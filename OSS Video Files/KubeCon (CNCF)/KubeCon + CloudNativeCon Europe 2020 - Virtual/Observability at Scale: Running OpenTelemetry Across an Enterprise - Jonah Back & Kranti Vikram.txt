Title: Observability at Scale: Running OpenTelemetry Across an Enterprise - Jonah Back & Kranti Vikram
Publication date: 2020-08-28
Playlist: KubeCon + CloudNativeCon Europe 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming events: EnvoyCon Virtual on October 15 and KubeCon + CloudNativeCon North America 2020 Virtual from November 17-20. Learn more at https://kubecon.io. The conferences feature presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.  

Observability at Scale: Running OpenTelemetry Across an Enterprise - Jonah Back & Kranti Vikram, Intuit 

Observability has been a huge topic of interest in the software industry over the last few years. One of the major components in observability is distributed tracing. Tools like Jaeger, Zipkin, OpenCensus, and OpenTelemetry have made it really easy to get started. Less easy, however, is getting your tracing infrastructure to a place where it can be fully leveraged across hundreds, if not thousands, of services.  This talk will cover Intuit's experience deploying tracing infrastructure using Kubernetes, Jaeger, and OpenTelemetry. It will cover a few key areas in Intuit's journey to running a highly available, multi-region tracing solution.  1) Scaling ElasticSearch to support 500M+ traces per day. 2) Secure, automated on-boarding of OpenTelemetry agents to central collectors 3) Leveraging open-source libraries to provide high quality trace data, enhanced with domain-specific attributes

https://sched.co/Zeqy
Captions: 
	00:00:00,080 --> 00:00:03,520
hello everyone good evening my name is

00:00:02,800 --> 00:00:05,279
jonah back

00:00:03,520 --> 00:00:06,960
i'm a principal software engineer at

00:00:05,279 --> 00:00:08,960
intuit and presenting

00:00:06,960 --> 00:00:10,559
with me today is chronti vikram a staff

00:00:08,960 --> 00:00:11,920
software engineer at intuit

00:00:10,559 --> 00:00:13,840
and today we're going to be talking

00:00:11,920 --> 00:00:15,200
about observability at scale

00:00:13,840 --> 00:00:17,279
running open telemetry across an

00:00:15,200 --> 00:00:19,199
enterprise a look at

00:00:17,279 --> 00:00:20,960
into its journey of adopting open

00:00:19,199 --> 00:00:21,760
telemetry to power our distributed

00:00:20,960 --> 00:00:25,680
tracing

00:00:21,760 --> 00:00:27,279
and other observability solutions

00:00:25,680 --> 00:00:30,000
first let's take a look at the agenda

00:00:27,279 --> 00:00:31,119
for today's talk

00:00:30,000 --> 00:00:33,120
first we're going to be talking about

00:00:31,119 --> 00:00:34,719
the journey into its head towards

00:00:33,120 --> 00:00:36,160
adopting open telemetry and distributed

00:00:34,719 --> 00:00:37,520
tracing

00:00:36,160 --> 00:00:39,440
you know we're really going to talk

00:00:37,520 --> 00:00:42,000
about kind of the state we were in about

00:00:39,440 --> 00:00:43,680
18 to 24 months ago

00:00:42,000 --> 00:00:46,480
the problems that our engineers and our

00:00:43,680 --> 00:00:49,520
on-call engineers were facing

00:00:46,480 --> 00:00:51,680
the solutions

00:00:49,520 --> 00:00:53,920
that they were using and really how

00:00:51,680 --> 00:00:56,960
adopting tools like open telemetry

00:00:53,920 --> 00:00:59,520
jaeger and other vendored tools

00:00:56,960 --> 00:01:01,120
has really enabled intuit to uh solve

00:00:59,520 --> 00:01:03,440
some of those problems

00:01:01,120 --> 00:01:05,199
from there we'll go into collection and

00:01:03,440 --> 00:01:07,439
instrumentation of trace data

00:01:05,199 --> 00:01:08,400
so we'll talk about how we've managed to

00:01:07,439 --> 00:01:10,240
collect

00:01:08,400 --> 00:01:11,520
all of the data from the services

00:01:10,240 --> 00:01:13,280
running it into it

00:01:11,520 --> 00:01:15,040
and how we instrumented those services

00:01:13,280 --> 00:01:17,840
in a pretty easy way

00:01:15,040 --> 00:01:19,040
so that they could collect trace data

00:01:17,840 --> 00:01:21,040
then we'll talk about

00:01:19,040 --> 00:01:22,720
some of the processors we've built on

00:01:21,040 --> 00:01:25,360
top of open telemetry

00:01:22,720 --> 00:01:27,600
specifically how we are able to extract

00:01:25,360 --> 00:01:29,680
metrics from our trace data

00:01:27,600 --> 00:01:30,640
and finally we'll talk about tracing's

00:01:29,680 --> 00:01:33,840
role in reducing

00:01:30,640 --> 00:01:35,200
mttr and mgtd that's mean time to repair

00:01:33,840 --> 00:01:38,720
and mean time to detect

00:01:35,200 --> 00:01:41,040
for our services okay so first

00:01:38,720 --> 00:01:44,720
let's look at kind of the state into it

00:01:41,040 --> 00:01:44,720
within 18 to 24 months ago

00:01:44,960 --> 00:01:48,560
we were in a state 18 to 24 months ago

00:01:46,880 --> 00:01:51,040
where uh not too

00:01:48,560 --> 00:01:52,159
far from where we are today but we had

00:01:51,040 --> 00:01:54,399
services running

00:01:52,159 --> 00:01:55,600
in a huge polygon environment you know

00:01:54,399 --> 00:01:59,119
we had

00:01:55,600 --> 00:02:02,240
teams running services in kubernetes uh

00:01:59,119 --> 00:02:04,799
in aws we had teams running just on bare

00:02:02,240 --> 00:02:06,719
metal ec2 instances we had teams running

00:02:04,799 --> 00:02:08,640
serverless functions we had teams

00:02:06,719 --> 00:02:11,440
running in an on-prem data center

00:02:08,640 --> 00:02:12,480
and we had uh a variety of tools that

00:02:11,440 --> 00:02:13,520
these services and teams were

00:02:12,480 --> 00:02:16,720
interacting with you know

00:02:13,520 --> 00:02:20,879
we had an istio service mesh we had uh

00:02:16,720 --> 00:02:24,239
dynamodb resources rds resources really

00:02:20,879 --> 00:02:26,080
a really truly polyglot stack and

00:02:24,239 --> 00:02:28,000
our engineers uh well while this gave

00:02:26,080 --> 00:02:30,319
them a lot of flexibility

00:02:28,000 --> 00:02:31,200
we started to notice some pretty common

00:02:30,319 --> 00:02:34,480
problems across

00:02:31,200 --> 00:02:35,599
teams one of the biggest ones being that

00:02:34,480 --> 00:02:37,920
when teams were talking and

00:02:35,599 --> 00:02:40,400
communicating with other services uh

00:02:37,920 --> 00:02:42,000
troubleshooting uh both latency issues

00:02:40,400 --> 00:02:45,040
and air issues became

00:02:42,000 --> 00:02:46,400
really difficult you know we had several

00:02:45,040 --> 00:02:47,120
incidents where teams would jump on a

00:02:46,400 --> 00:02:49,760
call

00:02:47,120 --> 00:02:51,599
and it was basically like domino's

00:02:49,760 --> 00:02:53,680
trying to page the right on-call team

00:02:51,599 --> 00:02:55,440
team one would notice latency they would

00:02:53,680 --> 00:02:57,920
get uh you know on onto

00:02:55,440 --> 00:02:59,360
some kind of uh virtual call and then

00:02:57,920 --> 00:03:01,200
they would realize oh no it's team two

00:02:59,360 --> 00:03:02,560
and then we would page team two

00:03:01,200 --> 00:03:04,640
and then pinch would be like oh no it's

00:03:02,560 --> 00:03:06,480
hit team three and we just keep paging

00:03:04,640 --> 00:03:08,159
teams down the stack until we realized

00:03:06,480 --> 00:03:09,280
where the root cause was because teams

00:03:08,159 --> 00:03:12,640
didn't have visibility

00:03:09,280 --> 00:03:13,760
past their own services and the reason

00:03:12,640 --> 00:03:14,879
they didn't have this visibility is

00:03:13,760 --> 00:03:18,080
because we really didn't have

00:03:14,879 --> 00:03:19,680
the tool chain to actually identify what

00:03:18,080 --> 00:03:20,400
these dependencies were and how far they

00:03:19,680 --> 00:03:23,840
went

00:03:20,400 --> 00:03:24,319
we had a very basic tracing mechanism we

00:03:23,840 --> 00:03:26,319
had a

00:03:24,319 --> 00:03:28,480
a header called intuit underscore to

00:03:26,319 --> 00:03:30,799
yeti which was intuitive

00:03:28,480 --> 00:03:32,000
which services generally propagated not

00:03:30,799 --> 00:03:32,799
100 coverage but

00:03:32,000 --> 00:03:35,200
you know they were pretty good about

00:03:32,799 --> 00:03:35,760
propagating but this header really

00:03:35,200 --> 00:03:37,840
didn't

00:03:35,760 --> 00:03:39,440
uh tell us anything about the

00:03:37,840 --> 00:03:40,319
relationships between services other

00:03:39,440 --> 00:03:42,480
than that

00:03:40,319 --> 00:03:44,239
uh they both had a common api call you

00:03:42,480 --> 00:03:46,640
know the only way you could use this

00:03:44,239 --> 00:03:48,319
header was searching in our log storage

00:03:46,640 --> 00:03:50,159
and even then we had you had to search

00:03:48,319 --> 00:03:52,319
across three or four different

00:03:50,159 --> 00:03:53,599
uh log indexes to actually be able to

00:03:52,319 --> 00:03:55,360
use that data

00:03:53,599 --> 00:03:58,000
and that data still wasn't you know it

00:03:55,360 --> 00:04:00,239
didn't solve for problems like

00:03:58,000 --> 00:04:01,040
asynchronous communication over a kafka

00:04:00,239 --> 00:04:03,439
broker

00:04:01,040 --> 00:04:04,799
or i didn't solve for clock ski between

00:04:03,439 --> 00:04:06,959
services and it didn't have

00:04:04,799 --> 00:04:07,920
information about like parent-child

00:04:06,959 --> 00:04:10,000
relationships

00:04:07,920 --> 00:04:11,840
so it was really really hard to use that

00:04:10,000 --> 00:04:15,840
header to actually understand what

00:04:11,840 --> 00:04:15,840
dependencies between services were

00:04:17,120 --> 00:04:20,479
you know this diagram really just shows

00:04:18,880 --> 00:04:21,680
kind of that that state

00:04:20,479 --> 00:04:24,160
where we have all of these different

00:04:21,680 --> 00:04:26,240
application instances you know uh maybe

00:04:24,160 --> 00:04:28,320
central services or services specific to

00:04:26,240 --> 00:04:30,560
like our quickbooks platform uh

00:04:28,320 --> 00:04:31,840
all pushing data to different log

00:04:30,560 --> 00:04:33,840
indexer and search tools

00:04:31,840 --> 00:04:35,040
you know so the shared services would

00:04:33,840 --> 00:04:38,080
have their own instance of

00:04:35,040 --> 00:04:39,120
a log indexer the quickbooks services

00:04:38,080 --> 00:04:40,560
would have their own

00:04:39,120 --> 00:04:42,720
instances of a log indexer and these

00:04:40,560 --> 00:04:44,240
instances weren't talking to each other

00:04:42,720 --> 00:04:46,880
and it's the same story for like apm

00:04:44,240 --> 00:04:49,280
tools so you know intuit

00:04:46,880 --> 00:04:50,639
leverages a tool called appdynamics and

00:04:49,280 --> 00:04:51,040
it was sort of the same story where

00:04:50,639 --> 00:04:53,199
these

00:04:51,040 --> 00:04:54,400
uh some of our services were pushed to

00:04:53,199 --> 00:04:57,040
one instance

00:04:54,400 --> 00:04:58,080
uh on the app dynamics task platform and

00:04:57,040 --> 00:04:59,120
other services would push through a

00:04:58,080 --> 00:05:00,479
second instance

00:04:59,120 --> 00:05:02,400
and these instances couldn't talk to

00:05:00,479 --> 00:05:04,639
each other so none of this correlation

00:05:02,400 --> 00:05:07,120
data between services was actually

00:05:04,639 --> 00:05:08,560
visible in a core like single place

00:05:07,120 --> 00:05:09,440
developers had to go through multiple

00:05:08,560 --> 00:05:11,120
tools

00:05:09,440 --> 00:05:13,440
you know multiple log instances multiple

00:05:11,120 --> 00:05:14,960
apm tools really to get a sense of the

00:05:13,440 --> 00:05:16,400
the system why health they couldn't just

00:05:14,960 --> 00:05:19,440
go into one place and say

00:05:16,400 --> 00:05:20,720
hey so this request this

00:05:19,440 --> 00:05:22,160
these are all the services involved

00:05:20,720 --> 00:05:23,280
these are the latency times that each of

00:05:22,160 --> 00:05:26,479
them spent

00:05:23,280 --> 00:05:28,560
and it was a real problem right

00:05:26,479 --> 00:05:30,000
engineers couldn't troubleshoot issues

00:05:28,560 --> 00:05:31,440
in just a minute or two it often would

00:05:30,000 --> 00:05:34,479
take 20 30 minutes

00:05:31,440 --> 00:05:37,440
just to understand how one certain api

00:05:34,479 --> 00:05:39,919
or request was working across the system

00:05:37,440 --> 00:05:41,759
and some other problems that kind of uh

00:05:39,919 --> 00:05:43,120
you know in the same area

00:05:41,759 --> 00:05:45,840
were that we didn't have a standard

00:05:43,120 --> 00:05:47,199
trace log format uh that intuit tid was

00:05:45,840 --> 00:05:48,960
really the only thing that was powering

00:05:47,199 --> 00:05:51,039
it and nobody interrupted it you know

00:05:48,960 --> 00:05:52,560
app dynamics didn't understand it

00:05:51,039 --> 00:05:54,240
new relic wouldn't understand none of

00:05:52,560 --> 00:05:54,880
these solutions would go out there and

00:05:54,240 --> 00:05:57,520
understand

00:05:54,880 --> 00:05:59,759
what that header meant we didn't have a

00:05:57,520 --> 00:06:01,840
single tracing solution across into it

00:05:59,759 --> 00:06:02,960
some teams had looked at different

00:06:01,840 --> 00:06:04,560
standards you know within

00:06:02,960 --> 00:06:06,960
small groups teams might be using

00:06:04,560 --> 00:06:08,400
something like a zipkin to actually

00:06:06,960 --> 00:06:10,160
trace the requests across their three or

00:06:08,400 --> 00:06:11,600
four services but across the company

00:06:10,160 --> 00:06:13,520
there is no single place that you could

00:06:11,600 --> 00:06:15,440
go and actually

00:06:13,520 --> 00:06:17,039
visualize a trace across all intuit

00:06:15,440 --> 00:06:20,560
services involved in the request

00:06:17,039 --> 00:06:22,560
which was a huge problem um

00:06:20,560 --> 00:06:23,600
and so you know with these problems we

00:06:22,560 --> 00:06:25,280
had a

00:06:23,600 --> 00:06:26,319
we had a team get together and really

00:06:25,280 --> 00:06:28,639
think about like what is the right

00:06:26,319 --> 00:06:31,600
solution we we looked at the

00:06:28,639 --> 00:06:32,880
uh industry where they were at uh there

00:06:31,600 --> 00:06:34,720
were a couple standards at the time that

00:06:32,880 --> 00:06:36,720
were competing there was open tracing

00:06:34,720 --> 00:06:38,240
there was open census both in the

00:06:36,720 --> 00:06:39,440
distributed tracing space both had their

00:06:38,240 --> 00:06:40,160
own standards in terms of how to

00:06:39,440 --> 00:06:43,520
propagate

00:06:40,160 --> 00:06:46,000
data and we we looked at

00:06:43,520 --> 00:06:46,960
first attempting to build our own

00:06:46,000 --> 00:06:48,319
tracing library

00:06:46,960 --> 00:06:52,000
that kind of leverage some of those

00:06:48,319 --> 00:06:52,000
standards to propagate information

00:06:52,960 --> 00:06:55,919
now before we get into that though i

00:06:54,560 --> 00:06:57,520
want to talk about the role of

00:06:55,919 --> 00:06:59,280
observability

00:06:57,520 --> 00:07:00,400
in reducing those outages that we were

00:06:59,280 --> 00:07:01,759
talking about or those incidents that we

00:07:00,400 --> 00:07:04,080
were talking about

00:07:01,759 --> 00:07:04,800
intuit you know as a sas provider has a

00:07:04,080 --> 00:07:06,639
pretty

00:07:04,800 --> 00:07:08,639
high standard for how much time that we

00:07:06,639 --> 00:07:11,680
want to spend during an incident down

00:07:08,639 --> 00:07:14,720
obviously as as little as possible

00:07:11,680 --> 00:07:17,120
but in this case we have a budget

00:07:14,720 --> 00:07:18,000
max across all services of around 22

00:07:17,120 --> 00:07:21,039
minutes per month

00:07:18,000 --> 00:07:23,280
of downtime at most and

00:07:21,039 --> 00:07:25,840
you know one of the things to highlight

00:07:23,280 --> 00:07:27,280
here is that that budget isn't just for

00:07:25,840 --> 00:07:29,360
time to repair things but it also

00:07:27,280 --> 00:07:29,919
includes time waiting for like time to

00:07:29,360 --> 00:07:31,520
detect as

00:07:29,919 --> 00:07:33,360
well waiting for alerts to go off

00:07:31,520 --> 00:07:35,520
waiting for the on-call to join an

00:07:33,360 --> 00:07:37,520
incident call and page other people

00:07:35,520 --> 00:07:38,639
so when we think about it at 22 minutes

00:07:37,520 --> 00:07:42,400
is really

00:07:38,639 --> 00:07:45,199
almost no time at all and so

00:07:42,400 --> 00:07:46,479
if we can bring that any tooling that we

00:07:45,199 --> 00:07:49,120
can get that can help us

00:07:46,479 --> 00:07:49,840
improve the time to both detect and time

00:07:49,120 --> 00:07:52,400
to recover

00:07:49,840 --> 00:07:53,840
uh on an incident is uh something that

00:07:52,400 --> 00:07:54,319
intro is very interested in and that's

00:07:53,840 --> 00:07:56,000
why we

00:07:54,319 --> 00:07:57,919
started looking at observability tools

00:07:56,000 --> 00:07:58,639
uh and in the distributed tracing space

00:07:57,919 --> 00:08:01,039
in general

00:07:58,639 --> 00:08:02,000
was to solve both uh better mean to dime

00:08:01,039 --> 00:08:04,639
to detect

00:08:02,000 --> 00:08:05,199
and also help our engineers uh recover

00:08:04,639 --> 00:08:08,080
faster

00:08:05,199 --> 00:08:10,479
and be able to isolate the problem

00:08:08,080 --> 00:08:10,479
faster

00:08:11,840 --> 00:08:14,879
okay so let's go into distributed

00:08:14,319 --> 00:08:17,120
tracing

00:08:14,879 --> 00:08:18,639
uh and kind of how that first

00:08:17,120 --> 00:08:19,520
distributed tracing library that intuit

00:08:18,639 --> 00:08:22,240
worked on

00:08:19,520 --> 00:08:23,599
um got us from a not so great state to

00:08:22,240 --> 00:08:25,039
an okay state

00:08:23,599 --> 00:08:27,440
uh so if we look at the diagram at the

00:08:25,039 --> 00:08:28,479
left here we're basically just showing a

00:08:27,440 --> 00:08:31,520
service diagram

00:08:28,479 --> 00:08:33,360
of 10 or 15 requests where

00:08:31,520 --> 00:08:34,640
it shows all of these requests coming in

00:08:33,360 --> 00:08:35,839
all these requests bouncing across

00:08:34,640 --> 00:08:37,519
different services

00:08:35,839 --> 00:08:39,440
and it's really hard to follow which

00:08:37,519 --> 00:08:40,240
requests are actually going to which

00:08:39,440 --> 00:08:41,760
service

00:08:40,240 --> 00:08:43,200
and this diagram is pretty

00:08:41,760 --> 00:08:44,800
representative how it felt to be an

00:08:43,200 --> 00:08:46,800
intuit engineer at the time

00:08:44,800 --> 00:08:49,279
um where you saw these requests come in

00:08:46,800 --> 00:08:50,800
you had you know you had the links here

00:08:49,279 --> 00:08:53,040
but it was really hard to follow the

00:08:50,800 --> 00:08:54,240
path uh between these requests to

00:08:53,040 --> 00:08:57,120
actually understand

00:08:54,240 --> 00:08:58,800
the dependencies in these services so

00:08:57,120 --> 00:09:01,360
when we rolled out our first tracing

00:08:58,800 --> 00:09:02,720
library which was basically just a

00:09:01,360 --> 00:09:05,920
library that extended

00:09:02,720 --> 00:09:08,800
the intuit tid header and added some

00:09:05,920 --> 00:09:09,760
spam headers as well a stand being just

00:09:08,800 --> 00:09:13,839
you know a compo

00:09:09,760 --> 00:09:15,279
one part of a larger trace um

00:09:13,839 --> 00:09:17,120
we started to see that we could actually

00:09:15,279 --> 00:09:18,480
identify uh requests

00:09:17,120 --> 00:09:20,160
throughout the system and see where they

00:09:18,480 --> 00:09:21,600
started see where they ended and it's a

00:09:20,160 --> 00:09:23,040
little bit easier to

00:09:21,600 --> 00:09:24,240
search through the noise you know we

00:09:23,040 --> 00:09:26,959
could see the the path that an

00:09:24,240 --> 00:09:29,440
individual request was taking

00:09:26,959 --> 00:09:30,959
and as we noticed this as we discovered

00:09:29,440 --> 00:09:31,760
that yes distributed tracing like

00:09:30,959 --> 00:09:33,839
actually

00:09:31,760 --> 00:09:36,000
is going to be very valuable to intuit

00:09:33,839 --> 00:09:37,519
uh we realized that we should

00:09:36,000 --> 00:09:41,040
invest heavily in it and actually start

00:09:37,519 --> 00:09:41,040
to adopt open standards instead

00:09:42,640 --> 00:09:46,000
okay uh quick highlight of some of the

00:09:44,800 --> 00:09:48,080
terminology i'm

00:09:46,000 --> 00:09:49,760
mentioning here in terms of how

00:09:48,080 --> 00:09:50,480
distributed tracing systems generally

00:09:49,760 --> 00:09:52,399
work

00:09:50,480 --> 00:09:53,519
there's two key concepts to understand

00:09:52,399 --> 00:09:55,600
here

00:09:53,519 --> 00:09:57,120
there's the concept of a trace so the

00:09:55,600 --> 00:10:00,160
trace in this diagram is

00:09:57,120 --> 00:10:02,240
every basically everything under this a

00:10:00,160 --> 00:10:04,160
block and including this a block

00:10:02,240 --> 00:10:05,680
and all of those individual objects are

00:10:04,160 --> 00:10:07,360
called spans

00:10:05,680 --> 00:10:08,800
and those fans are usually just single

00:10:07,360 --> 00:10:11,200
operations in

00:10:08,800 --> 00:10:12,320
that are part of a trace so if you want

00:10:11,200 --> 00:10:14,079
to think about it in terms of like an

00:10:12,320 --> 00:10:15,200
api call let's say i'm making an api

00:10:14,079 --> 00:10:19,120
call to the hacker news

00:10:15,200 --> 00:10:20,560
api um that entire the single api call

00:10:19,120 --> 00:10:21,680
and everything that happened in it would

00:10:20,560 --> 00:10:24,160
be a trace

00:10:21,680 --> 00:10:24,880
uh each individual operation in that

00:10:24,160 --> 00:10:27,519
call so

00:10:24,880 --> 00:10:29,839
any uh api gateway operations any

00:10:27,519 --> 00:10:32,079
database operations any business logic

00:10:29,839 --> 00:10:32,959
all of those would be individual spans

00:10:32,079 --> 00:10:36,640
that ladder up

00:10:32,959 --> 00:10:36,640
to serving that api call

00:10:37,040 --> 00:10:40,240
and so the way that these tools are

00:10:39,600 --> 00:10:43,120
actually

00:10:40,240 --> 00:10:44,399
and these models are actually used is

00:10:43,120 --> 00:10:46,160
that you know you have a request come

00:10:44,399 --> 00:10:47,839
into a microservice

00:10:46,160 --> 00:10:50,240
that request is going to have some kind

00:10:47,839 --> 00:10:52,640
of trace data on it you know maybe a

00:10:50,240 --> 00:10:54,399
trace header or maybe nothing at all

00:10:52,640 --> 00:10:56,480
that microservice when it calls other

00:10:54,399 --> 00:10:58,079
services or does other operations

00:10:56,480 --> 00:11:00,000
is going to create those new span

00:10:58,079 --> 00:11:01,200
objects that we talked about

00:11:00,000 --> 00:11:02,720
and those span objects are going to

00:11:01,200 --> 00:11:03,600
interact with some kind of local tracing

00:11:02,720 --> 00:11:06,720
api

00:11:03,600 --> 00:11:10,480
so in our case it's an open tracing api

00:11:06,720 --> 00:11:11,839
in the jvm typically that tracing api

00:11:10,480 --> 00:11:13,760
is going to be configured to talk to

00:11:11,839 --> 00:11:15,680
some tracing library

00:11:13,760 --> 00:11:16,959
or tracing collector and then that

00:11:15,680 --> 00:11:18,800
tracing collector is going to send data

00:11:16,959 --> 00:11:19,839
to a trace storage layer

00:11:18,800 --> 00:11:21,600
and this is generally how you're going

00:11:19,839 --> 00:11:22,560
to see most tracing architectures work

00:11:21,600 --> 00:11:23,440
at a high level

00:11:22,560 --> 00:11:26,160
is that they're going to have some kind

00:11:23,440 --> 00:11:29,200
of instrumentation on your app servers

00:11:26,160 --> 00:11:32,079
that instrumentation is going to help uh

00:11:29,200 --> 00:11:33,279
facilitate the propagation of the

00:11:32,079 --> 00:11:35,360
required headers like

00:11:33,279 --> 00:11:36,480
trace ids fan ids and then any like

00:11:35,360 --> 00:11:38,240
baggage

00:11:36,480 --> 00:11:39,680
between systems and then all of these

00:11:38,240 --> 00:11:40,720
systems are going to independently

00:11:39,680 --> 00:11:42,399
report

00:11:40,720 --> 00:11:43,839
their view of the trace so the spans

00:11:42,399 --> 00:11:45,839
that they generate

00:11:43,839 --> 00:11:47,040
to that tracing api that they have which

00:11:45,839 --> 00:11:48,079
is going to kind of send it to a

00:11:47,040 --> 00:11:51,040
collector which will

00:11:48,079 --> 00:11:52,720
store it and normalize it so that when

00:11:51,040 --> 00:11:54,880
somebody wants to come and view it

00:11:52,720 --> 00:11:56,000
they're able to use some trace

00:11:54,880 --> 00:11:58,720
visualization tool like

00:11:56,000 --> 00:12:00,480
jager or zipkin or or any of the the

00:11:58,720 --> 00:12:02,240
third-party vendors out there

00:12:00,480 --> 00:12:03,839
to actually reconstruct the full view of

00:12:02,240 --> 00:12:06,160
the trace and then do

00:12:03,839 --> 00:12:07,600
maybe analytics uh in terms of like

00:12:06,160 --> 00:12:10,320
generating metrics or things like that

00:12:07,600 --> 00:12:12,480
on it

00:12:10,320 --> 00:12:14,240
so we look at how we set this

00:12:12,480 --> 00:12:15,440
architecture up into it it basically

00:12:14,240 --> 00:12:16,800
follows that guide that we looked at

00:12:15,440 --> 00:12:20,560
before

00:12:16,800 --> 00:12:21,839
so intuit today has two primary ways of

00:12:20,560 --> 00:12:24,959
running services

00:12:21,839 --> 00:12:27,279
we have services that run on kubernetes

00:12:24,959 --> 00:12:29,360
so they you know they'll have multiple

00:12:27,279 --> 00:12:31,200
pods running at kubernetes cluster

00:12:29,360 --> 00:12:33,440
or we have services running on just pure

00:12:31,200 --> 00:12:36,240
ec2 on aws so they'll you know

00:12:33,440 --> 00:12:36,959
they'll have a ami that they use they

00:12:36,240 --> 00:12:38,720
will

00:12:36,959 --> 00:12:42,160
start their server and they might have

00:12:38,720 --> 00:12:44,720
one or several daemons running on that

00:12:42,160 --> 00:12:46,560
and so for both of those cases today we

00:12:44,720 --> 00:12:48,240
run an open telemetry agent

00:12:46,560 --> 00:12:50,720
which is just you know basically a

00:12:48,240 --> 00:12:52,639
configured container

00:12:50,720 --> 00:12:54,240
using the open source open telemetry

00:12:52,639 --> 00:12:57,760
agent container

00:12:54,240 --> 00:13:00,160
that has configuration to send to a aws

00:12:57,760 --> 00:13:00,800
private link endpoint so every time a

00:13:00,160 --> 00:13:02,720
new

00:13:00,800 --> 00:13:05,040
node joins our kubernetes cluster we've

00:13:02,720 --> 00:13:07,440
got a kubernetes daemon set in place

00:13:05,040 --> 00:13:08,800
that will automatically provision an

00:13:07,440 --> 00:13:10,079
open selemetry agent

00:13:08,800 --> 00:13:12,639
that open telemetry agent is

00:13:10,079 --> 00:13:17,360
automatically configured to use

00:13:12,639 --> 00:13:18,959
grpc with an aws private link

00:13:17,360 --> 00:13:20,959
endpoint that's automatically configured

00:13:18,959 --> 00:13:22,079
in that account to talk to a central

00:13:20,959 --> 00:13:23,440
collector tier

00:13:22,079 --> 00:13:25,760
and it's the same thing on our ec2

00:13:23,440 --> 00:13:27,760
instances today when you

00:13:25,760 --> 00:13:29,600
provision a new ec2 instance using the

00:13:27,760 --> 00:13:31,120
intuit base ami

00:13:29,600 --> 00:13:33,440
there's a daemon already pre-configured

00:13:31,120 --> 00:13:35,360
you just have to run like service.start

00:13:33,440 --> 00:13:37,040
on that daemon and that daemon is

00:13:35,360 --> 00:13:38,720
basically going to auto discover that

00:13:37,040 --> 00:13:41,760
aws privately gun point

00:13:38,720 --> 00:13:42,880
and again it's going to use grpc to

00:13:41,760 --> 00:13:44,959
communicate with

00:13:42,880 --> 00:13:47,120
from the agent to the open telemetry

00:13:44,959 --> 00:13:48,399
collector node

00:13:47,120 --> 00:13:51,279
now those collector nodes are typically

00:13:48,399 --> 00:13:52,720
running uh in the same aws region so if

00:13:51,279 --> 00:13:55,680
you're running in the us west two

00:13:52,720 --> 00:13:58,320
region there will be a uh collector tier

00:13:55,680 --> 00:14:00,320
in us west too to receive this uh

00:13:58,320 --> 00:14:01,680
that trace data and the same goes for

00:14:00,320 --> 00:14:02,720
any other region that into it operates

00:14:01,680 --> 00:14:05,279
in so

00:14:02,720 --> 00:14:06,639
any major aws region we have a collector

00:14:05,279 --> 00:14:10,079
tier set up to kind of

00:14:06,639 --> 00:14:13,279
collect all of that data and from there

00:14:10,079 --> 00:14:14,800
that open telemetry collector tier uses

00:14:13,279 --> 00:14:17,680
grpc and envoy

00:14:14,800 --> 00:14:18,800
to basically push that data to a jager

00:14:17,680 --> 00:14:21,440
collector tier

00:14:18,800 --> 00:14:22,160
initially it was a separate tier but the

00:14:21,440 --> 00:14:23,440
jaeger

00:14:22,160 --> 00:14:25,920
team has actually been working to

00:14:23,440 --> 00:14:27,279
consolidate a lot of that into the open

00:14:25,920 --> 00:14:29,839
telemetry

00:14:27,279 --> 00:14:32,160
you know community as well in terms of

00:14:29,839 --> 00:14:35,680
extra receivers extra processors

00:14:32,160 --> 00:14:37,279
extra uh collector extra exporters

00:14:35,680 --> 00:14:39,680
and so that's actually really becoming a

00:14:37,279 --> 00:14:42,399
consolidated tier today

00:14:39,680 --> 00:14:44,240
and then from there data goes to the

00:14:42,399 --> 00:14:46,959
trace storage system in our case

00:14:44,240 --> 00:14:48,399
aws elasticsearch and from there it goes

00:14:46,959 --> 00:14:51,279
to a jager

00:14:48,399 --> 00:14:52,480
user interface where engineers can

00:14:51,279 --> 00:14:55,199
search for traces

00:14:52,480 --> 00:14:56,079
based on different tags based on the

00:14:55,199 --> 00:14:58,480
trace id

00:14:56,079 --> 00:15:01,680
things like that and we'll give them a

00:14:58,480 --> 00:15:03,199
pretty solid view of the trace

00:15:01,680 --> 00:15:05,680
and with that i'm going to hand it over

00:15:03,199 --> 00:15:05,680
to chronti

00:15:06,560 --> 00:15:11,440
thank you jonah as jana mentioned in one

00:15:09,519 --> 00:15:13,120
of his initial slide

00:15:11,440 --> 00:15:14,480
for intuit product to serve a single

00:15:13,120 --> 00:15:17,040
user or

00:15:14,480 --> 00:15:18,880
api request hundreds of services are

00:15:17,040 --> 00:15:20,320
invoked behind the scene to compose its

00:15:18,880 --> 00:15:21,760
response

00:15:20,320 --> 00:15:23,360
these services are often owned by

00:15:21,760 --> 00:15:25,279
different teams

00:15:23,360 --> 00:15:26,639
or are different art from different

00:15:25,279 --> 00:15:28,720
functional groups

00:15:26,639 --> 00:15:29,680
so when a customer complains about a

00:15:28,720 --> 00:15:32,639
slowness

00:15:29,680 --> 00:15:33,199
in your application or an alarm is

00:15:32,639 --> 00:15:35,279
triggered

00:15:33,199 --> 00:15:37,519
for the same there are some common

00:15:35,279 --> 00:15:40,639
questions that pop up

00:15:37,519 --> 00:15:43,680
which flow or which request

00:15:40,639 --> 00:15:46,320
had issues is one of my components

00:15:43,680 --> 00:15:47,360
having an issue or is it one of the

00:15:46,320 --> 00:15:49,839
dependent service

00:15:47,360 --> 00:15:51,680
that is having an issue is the dependent

00:15:49,839 --> 00:15:53,839
service throwing an error

00:15:51,680 --> 00:15:55,360
or is it the high latency of the

00:15:53,839 --> 00:15:56,880
dependent service that's causing this

00:15:55,360 --> 00:15:59,839
repercussion

00:15:56,880 --> 00:16:01,279
typically in such cases what do we do we

00:15:59,839 --> 00:16:02,560
take the time window

00:16:01,279 --> 00:16:05,120
and dig through the logs to find

00:16:02,560 --> 00:16:06,560
something useful we also pull in teams

00:16:05,120 --> 00:16:08,880
from different services

00:16:06,560 --> 00:16:10,720
to troubleshoot this issue this process

00:16:08,880 --> 00:16:13,839
is very time consuming

00:16:10,720 --> 00:16:15,600
involves many people and often we don't

00:16:13,839 --> 00:16:17,680
find the root cause of the issue

00:16:15,600 --> 00:16:22,800
so whom do we blame if you don't find

00:16:17,680 --> 00:16:24,480
the root cause we blame it on the

00:16:22,800 --> 00:16:26,560
network

00:16:24,480 --> 00:16:28,000
distributed tracing helps in addressing

00:16:26,560 --> 00:16:30,959
this issue

00:16:28,000 --> 00:16:32,079
it is a process of tracking a request

00:16:30,959 --> 00:16:33,759
end to end

00:16:32,079 --> 00:16:37,839
as it traverses through multiple

00:16:33,759 --> 00:16:37,839
services in near real time

00:16:38,880 --> 00:16:43,440
so let's see it in action we have three

00:16:42,160 --> 00:16:46,000
micro services

00:16:43,440 --> 00:16:47,519
handling a retail order all these three

00:16:46,000 --> 00:16:49,839
micro services

00:16:47,519 --> 00:16:52,880
are instrumented with open tracing or

00:16:49,839 --> 00:16:56,320
open telemetry libraries

00:16:52,880 --> 00:16:56,320
when a retail order is placed

00:16:57,360 --> 00:17:04,640
a trace context gets created and also

00:17:00,720 --> 00:17:06,799
generates a server span which would have

00:17:04,640 --> 00:17:08,240
trace id to identify a distributed

00:17:06,799 --> 00:17:11,280
transaction

00:17:08,240 --> 00:17:13,520
a span id to identify the transaction

00:17:11,280 --> 00:17:16,160
flow within the order service

00:17:13,520 --> 00:17:18,319
a parent span id to identify the span

00:17:16,160 --> 00:17:20,959
that initiated this transaction

00:17:18,319 --> 00:17:22,559
since order service is a service or the

00:17:20,959 --> 00:17:24,959
first service

00:17:22,559 --> 00:17:26,880
receiving the request the parent id

00:17:24,959 --> 00:17:29,919
would be null

00:17:26,880 --> 00:17:33,440
now when a downstream call is made

00:17:29,919 --> 00:17:36,559
a similar span gets created but

00:17:33,440 --> 00:17:40,000
with the parent id being same as the

00:17:36,559 --> 00:17:40,000
span id of the order service

00:17:40,320 --> 00:17:44,400
the same happens with the billing

00:17:42,000 --> 00:17:44,400
service

00:17:45,039 --> 00:17:48,240
now when the billing service responds

00:17:47,840 --> 00:17:51,280
back

00:17:48,240 --> 00:17:53,440
to the allocation services request a

00:17:51,280 --> 00:17:56,080
client span gets created

00:17:53,440 --> 00:17:57,760
so these fans continue to get created in

00:17:56,080 --> 00:18:00,880
this fashion

00:17:57,760 --> 00:18:02,400
so once we have all these fans we could

00:18:00,880 --> 00:18:04,080
definitely visualize this

00:18:02,400 --> 00:18:05,840
in a timeline view with a parental

00:18:04,080 --> 00:18:08,720
relationship and

00:18:05,840 --> 00:18:09,200
the spans have rich information such as

00:18:08,720 --> 00:18:12,880
the

00:18:09,200 --> 00:18:15,039
status quo of the request the latency

00:18:12,880 --> 00:18:15,919
and some custom tags that would help to

00:18:15,039 --> 00:18:19,840
filter these

00:18:15,919 --> 00:18:19,840
spans from the play store

00:18:20,960 --> 00:18:26,000
so these fans later get shipped to the

00:18:24,000 --> 00:18:29,840
open telemetry collector

00:18:26,000 --> 00:18:29,840
and they end up in the case store

00:18:31,120 --> 00:18:36,320
so we we can build a lot of analytics

00:18:34,400 --> 00:18:40,840
and visualization

00:18:36,320 --> 00:18:43,360
pieces to get insights from this store

00:18:40,840 --> 00:18:46,320
data

00:18:43,360 --> 00:18:48,000
so one key thing to note here is that

00:18:46,320 --> 00:18:50,000
open telemetry collector

00:18:48,000 --> 00:18:52,480
becomes our tracing back end abstraction

00:18:50,000 --> 00:18:53,600
layer it can accept spans in different

00:18:52,480 --> 00:18:55,840
formats

00:18:53,600 --> 00:18:58,240
and it can spit out spans in a desired

00:18:55,840 --> 00:18:58,240
format

00:19:01,520 --> 00:19:05,440
let's look at metrics there are a lot of

00:19:03,919 --> 00:19:07,840
different types of metrics

00:19:05,440 --> 00:19:09,360
that you can capture for your system

00:19:07,840 --> 00:19:12,799
starting with

00:19:09,360 --> 00:19:16,240
system resource metrics cpu memory

00:19:12,799 --> 00:19:17,520
for your node there are a bunch of aws

00:19:16,240 --> 00:19:19,919
resource metrics

00:19:17,520 --> 00:19:20,799
cloud watch metrics coming from

00:19:19,919 --> 00:19:24,559
elasticsearch

00:19:20,799 --> 00:19:27,919
dynamodb rds serverless

00:19:24,559 --> 00:19:29,440
so on and so forth and we also have

00:19:27,919 --> 00:19:31,679
transaction metrics

00:19:29,440 --> 00:19:32,640
these are the metrics that give you

00:19:31,679 --> 00:19:36,240
insights on

00:19:32,640 --> 00:19:38,799
the api's request rate error rate

00:19:36,240 --> 00:19:39,440
duration or at the service level as well

00:19:38,799 --> 00:19:42,080
so these are

00:19:39,440 --> 00:19:44,000
the transaction metrics today we will

00:19:42,080 --> 00:19:47,280
talk about red metrics

00:19:44,000 --> 00:19:53,840
the problem we had and how we solved it

00:19:47,280 --> 00:19:53,840
using the screening pipeline

00:19:54,640 --> 00:19:59,840
so in the initial solution services

00:19:57,679 --> 00:20:01,200
were pushing logs with latency and

00:19:59,840 --> 00:20:03,600
status quo

00:20:01,200 --> 00:20:05,280
we had scheduled queries on top of our

00:20:03,600 --> 00:20:08,080
log management system

00:20:05,280 --> 00:20:09,039
queries used to run every minute and

00:20:08,080 --> 00:20:12,400
push those to

00:20:09,039 --> 00:20:15,280
s3 bucket and we had a lambda

00:20:12,400 --> 00:20:16,720
which pushed the data to our time series

00:20:15,280 --> 00:20:19,200
everybody liked it

00:20:16,720 --> 00:20:21,919
and started pushing more and more data

00:20:19,200 --> 00:20:24,240
we soon had thousands of queries running

00:20:21,919 --> 00:20:25,360
and competing for each other we had

00:20:24,240 --> 00:20:28,559
delayed in

00:20:25,360 --> 00:20:29,280
we had delays and metrics we made

00:20:28,559 --> 00:20:31,919
queries

00:20:29,280 --> 00:20:33,120
run every 10 minutes to mitigate this

00:20:31,919 --> 00:20:36,400
issue

00:20:33,120 --> 00:20:38,320
we found that traces or spans had these

00:20:36,400 --> 00:20:40,159
details

00:20:38,320 --> 00:20:41,360
the details that i am talking about is

00:20:40,159 --> 00:20:45,600
the status quo

00:20:41,360 --> 00:20:47,600
the response time these uh

00:20:45,600 --> 00:20:48,799
service name the api name so on and so

00:20:47,600 --> 00:20:51,840
on

00:20:48,799 --> 00:20:54,000
each span had these details in a

00:20:51,840 --> 00:20:55,679
consistent format

00:20:54,000 --> 00:20:57,120
we knew that we could take advantage of

00:20:55,679 --> 00:20:59,440
this

00:20:57,120 --> 00:21:00,480
so we enhanced the open telemetry

00:20:59,440 --> 00:21:02,960
collector

00:21:00,480 --> 00:21:03,919
to extract metrics from spans as they

00:21:02,960 --> 00:21:07,120
flow through the

00:21:03,919 --> 00:21:08,880
collector to the tracing packet so the

00:21:07,120 --> 00:21:11,760
metrics extracted by

00:21:08,880 --> 00:21:13,360
the open telemetry collector were then

00:21:11,760 --> 00:21:15,919
streamed to matrix aggregator

00:21:13,360 --> 00:21:16,480
to summarize the metrics and then push

00:21:15,919 --> 00:21:19,520
them to

00:21:16,480 --> 00:21:19,919
a time series database this brought in

00:21:19,520 --> 00:21:22,000
our

00:21:19,919 --> 00:21:24,400
this brought in couple of advantages or

00:21:22,000 --> 00:21:26,320
benefits

00:21:24,400 --> 00:21:28,400
definitely one of the benefit was the

00:21:26,320 --> 00:21:30,799
metrics were available

00:21:28,400 --> 00:21:31,919
was in less than a minute when compared

00:21:30,799 --> 00:21:34,880
with 10 minutes

00:21:31,919 --> 00:21:34,880
to the earlier solution

00:21:35,200 --> 00:21:41,600
so the top three graphs represent

00:21:39,039 --> 00:21:43,520
the data coming out of our previous log

00:21:41,600 --> 00:21:45,440
base pipeline

00:21:43,520 --> 00:21:48,559
bottom three show data coming from our

00:21:45,440 --> 00:21:51,840
observability pipeline

00:21:48,559 --> 00:21:55,120
first on the top left most graph

00:21:51,840 --> 00:21:57,440
shows tps numbers

00:21:55,120 --> 00:21:58,640
shows dots that are separated by 10

00:21:57,440 --> 00:22:02,720
minutes or more

00:21:58,640 --> 00:22:05,360
or more than that on the bottom you see

00:22:02,720 --> 00:22:08,000
it's coming every minute or even every

00:22:05,360 --> 00:22:08,000
30 seconds

00:22:08,559 --> 00:22:12,720
second graph shows the error rate you

00:22:11,600 --> 00:22:14,960
can see

00:22:12,720 --> 00:22:17,919
dotted lines that means we did not get

00:22:14,960 --> 00:22:22,240
any data at that period of time

00:22:17,919 --> 00:22:26,159
that means we were flying blind

00:22:22,240 --> 00:22:29,360
during that period of time

00:22:26,159 --> 00:22:30,320
whereas our new observability pipeline

00:22:29,360 --> 00:22:33,039
is showing

00:22:30,320 --> 00:22:34,080
continuous metrics definitely we saw 10

00:22:33,039 --> 00:22:36,480
minutes reduction

00:22:34,080 --> 00:22:41,840
in our mean time to detect because of

00:22:36,480 --> 00:22:41,840
this new pipeline

00:22:46,880 --> 00:22:51,440
so let us get into the benefits of

00:22:48,320 --> 00:22:51,440
traces and phrase methods

00:22:51,600 --> 00:22:57,600
topology view helps in quickly

00:22:54,320 --> 00:23:01,600
identifying the faulty service

00:22:57,600 --> 00:23:01,600
autocorrelation helps to identify

00:23:02,320 --> 00:23:07,200
the impacted api and root cause quickly

00:23:07,360 --> 00:23:13,440
not just for identifying issues

00:23:10,400 --> 00:23:15,039
traces and trace metrics also help in

00:23:13,440 --> 00:23:18,320
running performance analysis on

00:23:15,039 --> 00:23:18,320
pre-product product planners

00:23:18,960 --> 00:23:23,440
so we could get we could analyze the

00:23:21,760 --> 00:23:26,320
aggregated tps

00:23:23,440 --> 00:23:28,159
tp metrics at service and api level

00:23:26,320 --> 00:23:30,400
enabling comparison between

00:23:28,159 --> 00:23:32,720
current and fast baselines help in

00:23:30,400 --> 00:23:37,840
deploying deployment regressions

00:23:32,720 --> 00:23:37,840
and canary rollouts

00:23:39,440 --> 00:23:43,200

YouTube URL: https://www.youtube.com/watch?v=Dswj-nPy_Fs


