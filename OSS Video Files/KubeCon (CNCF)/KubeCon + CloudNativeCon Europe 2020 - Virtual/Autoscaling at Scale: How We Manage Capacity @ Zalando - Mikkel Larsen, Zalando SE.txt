Title: Autoscaling at Scale: How We Manage Capacity @ Zalando - Mikkel Larsen, Zalando SE
Publication date: 2020-08-27
Playlist: KubeCon + CloudNativeCon Europe 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming events: EnvoyCon Virtual on October 15 and KubeCon + CloudNativeCon North America 2020 Virtual from November 17-20. Learn more at https://kubecon.io. The conferences feature presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.  

Autoscaling at Scale: How We Manage Capacity @ Zalando - Mikkel Larsen, Zalando SE 

As of October 2019 Zalando has ~140 Kubernetes clusters varying in size from 5 to 400 nodes. This talk goes over how different applications both stateful and stateless are autoscaled using the HPA(Horizontal Pod Autoscaler) and VPA(Vertical Pod Autoscaler) and also the cluster using the Cluster Autoscaler. More specifically the talk explains the limitations and workarounds to each of these scaling strategies and also the techniques used to monitor so that applications always have sufficient capacity. It also discusses common pitfalls while scaling with these controllers and finally concludes with proposed enhancements to these controllers which would make autoscaling more reliable and efficient.

https://sched.co/ZelM
Captions: 
	00:00:00,080 --> 00:00:04,080
hello everyone and welcome to this talk

00:00:02,159 --> 00:00:06,080
on auto scaling at scale and how we

00:00:04,080 --> 00:00:08,240
manage capacity at salando

00:00:06,080 --> 00:00:10,320
my name is miguel larsen i'm a software

00:00:08,240 --> 00:00:11,840
engineer at zalando and i work in the

00:00:10,320 --> 00:00:14,480
cloud infrastructure team

00:00:11,840 --> 00:00:17,039
which was responsible for managing our

00:00:14,480 --> 00:00:19,279
companies infrastructure

00:00:17,039 --> 00:00:20,240
salando is the leading online fashion

00:00:19,279 --> 00:00:22,560
retailer

00:00:20,240 --> 00:00:23,519
in europe and we operate in 17 countries

00:00:22,560 --> 00:00:26,000
across europe

00:00:23,519 --> 00:00:27,279
we have 11 team we have an 11

00:00:26,000 --> 00:00:29,279
fulfillment centers

00:00:27,279 --> 00:00:30,480
the warehouses where we ship out all the

00:00:29,279 --> 00:00:33,120
fashion articles

00:00:30,480 --> 00:00:35,120
we have 32 million active customers and

00:00:33,120 --> 00:00:38,640
a lot of visitors every month

00:00:35,120 --> 00:00:40,640
and in order to handle this size on the

00:00:38,640 --> 00:00:44,399
on the technical infrastructure side

00:00:40,640 --> 00:00:45,039
we currently have 150 kubernetes

00:00:44,399 --> 00:00:47,280
clusters

00:00:45,039 --> 00:00:48,559
running in aws half of them are

00:00:47,280 --> 00:00:51,600
production and the other half

00:00:48,559 --> 00:00:54,000
test clusters we have around

00:00:51,600 --> 00:00:55,120
4 000 a little bit more than 4 000

00:00:54,000 --> 00:00:58,320
services where

00:00:55,120 --> 00:00:59,039
85 is running in kubernetes and we aim

00:00:58,320 --> 00:01:00,960
to

00:00:59,039 --> 00:01:02,399
increase this to 95 percent of all

00:01:00,960 --> 00:01:04,640
services

00:01:02,399 --> 00:01:06,240
and because of zelando operates only

00:01:04,640 --> 00:01:09,439
europe we also have a certain

00:01:06,240 --> 00:01:10,640
traffic pattern where we see less load

00:01:09,439 --> 00:01:12,640
in the nights because

00:01:10,640 --> 00:01:13,680
there's less people active on this

00:01:12,640 --> 00:01:16,880
website

00:01:13,680 --> 00:01:18,560
and therefore usual scaling

00:01:16,880 --> 00:01:20,640
in in some of the bigger kubernetes

00:01:18,560 --> 00:01:23,920
clusters is between 50 nodes

00:01:20,640 --> 00:01:25,040
in the night and 350 on the on the peak

00:01:23,920 --> 00:01:27,360
hours of the day

00:01:25,040 --> 00:01:29,119
this is not for all the clusters but it

00:01:27,360 --> 00:01:29,920
happens for the for the bigger ones that

00:01:29,119 --> 00:01:33,200
see so much

00:01:29,920 --> 00:01:33,200
uh scaling difference

00:01:34,079 --> 00:01:38,320
and how we manage scaling from the user

00:01:36,880 --> 00:01:39,840
point of view so the users of the

00:01:38,320 --> 00:01:42,000
kubernetes infrastructure the

00:01:39,840 --> 00:01:42,960
the teams developing services for

00:01:42,000 --> 00:01:45,280
solando

00:01:42,960 --> 00:01:47,040
is through horizontal part auto scaling

00:01:45,280 --> 00:01:50,240
this is kind of the interface that we

00:01:47,040 --> 00:01:51,840
give to users and we use the horizontal

00:01:50,240 --> 00:01:52,880
part autoscaler the official one from

00:01:51,840 --> 00:01:54,880
kubernetes

00:01:52,880 --> 00:01:56,640
and the best way to explain this one is

00:01:54,880 --> 00:01:58,960
to look at the algorithm

00:01:56,640 --> 00:02:01,119
the main idea is that it wants to

00:01:58,960 --> 00:02:01,920
calculate a number of replicas or a

00:02:01,119 --> 00:02:03,600
number of

00:02:01,920 --> 00:02:05,040
pots or number of instances of your

00:02:03,600 --> 00:02:07,680
application that should run

00:02:05,040 --> 00:02:08,959
given a certain amount of given the

00:02:07,680 --> 00:02:12,319
current metrics that is

00:02:08,959 --> 00:02:14,319
observed so the calculation is that it

00:02:12,319 --> 00:02:16,160
takes the current number of replicas and

00:02:14,319 --> 00:02:18,000
multiplies it with the current metric

00:02:16,160 --> 00:02:19,760
over the desired metric

00:02:18,000 --> 00:02:21,599
and the current metric is what is

00:02:19,760 --> 00:02:22,959
currently being observed of the running

00:02:21,599 --> 00:02:25,760
parts so it could be

00:02:22,959 --> 00:02:28,640
the cpu usage across all the parts and

00:02:25,760 --> 00:02:30,560
the desired metric is what you

00:02:28,640 --> 00:02:32,319
have defined for your application this

00:02:30,560 --> 00:02:33,760
in the scaling configuration so you

00:02:32,319 --> 00:02:36,959
might say that

00:02:33,760 --> 00:02:40,000
you want your service to have 50

00:02:36,959 --> 00:02:42,720
cpu load cpu load per per

00:02:40,000 --> 00:02:43,440
instance per pot and then you configure

00:02:42,720 --> 00:02:46,560
00:02:43,440 --> 00:02:49,920
average utilization and then if the

00:02:46,560 --> 00:02:51,519
if the current metric is reported to be

00:02:49,920 --> 00:02:53,040
higher than this then it means that it

00:02:51,519 --> 00:02:54,239
needs to scale out and if it's lower

00:02:53,040 --> 00:02:58,480
than this then it

00:02:54,239 --> 00:03:00,959
needs to scale in um or scale down

00:02:58,480 --> 00:03:03,040
and one thing to also note about the

00:03:00,959 --> 00:03:04,000
algorithm is that it uses a seal and

00:03:03,040 --> 00:03:08,480
this is to ensure

00:03:04,000 --> 00:03:08,480
that it it it calculates

00:03:08,560 --> 00:03:12,640
a bit more than needed in some cases so

00:03:11,760 --> 00:03:14,959
it's better to

00:03:12,640 --> 00:03:16,000
over shoot the target than to undershoot

00:03:14,959 --> 00:03:17,680
the target

00:03:16,000 --> 00:03:19,440
because you rather run with a little bit

00:03:17,680 --> 00:03:21,840
extra capacity than to run

00:03:19,440 --> 00:03:22,480
with too few so that's why it uses the

00:03:21,840 --> 00:03:26,239
seal to

00:03:22,480 --> 00:03:28,640
to ensure that um the official hpa

00:03:26,239 --> 00:03:30,480
supports cpu and memory out of the box

00:03:28,640 --> 00:03:32,000
as metric types and then it also

00:03:30,480 --> 00:03:35,760
supports custom or external

00:03:32,000 --> 00:03:37,760
metrics and because the cpu memory is

00:03:35,760 --> 00:03:39,920
not enough in our case because we have a

00:03:37,760 --> 00:03:41,440
lot of different applications and a lot

00:03:39,920 --> 00:03:43,680
of different use cases

00:03:41,440 --> 00:03:45,280
we have developed something we call the

00:03:43,680 --> 00:03:48,560
cube metrics adapter

00:03:45,280 --> 00:03:49,760
which is a component that can interface

00:03:48,560 --> 00:03:52,560
with the hpa

00:03:49,760 --> 00:03:53,519
and provide custom metrics so the matrix

00:03:52,560 --> 00:03:56,239
we provide there

00:03:53,519 --> 00:03:57,760
is amazon sqs so you can scale your

00:03:56,239 --> 00:04:01,599
application based on

00:03:57,760 --> 00:04:03,200
sqsq length so if you have some job that

00:04:01,599 --> 00:04:05,920
reads from a queue you can scale out the

00:04:03,200 --> 00:04:07,840
number of workers based on that

00:04:05,920 --> 00:04:09,280
we also support ingress requests per

00:04:07,840 --> 00:04:11,840
second metric

00:04:09,280 --> 00:04:12,959
to scale based on the http traffic for

00:04:11,840 --> 00:04:16,320
for ecp

00:04:12,959 --> 00:04:18,400
apis and services and we also have a

00:04:16,320 --> 00:04:20,400
generic prometheus query interface so

00:04:18,400 --> 00:04:21,440
you can define any promises query and

00:04:20,400 --> 00:04:24,880
scale based on

00:04:21,440 --> 00:04:26,560
that you can also define json metrics so

00:04:24,880 --> 00:04:28,560
if you have a service that just returns

00:04:26,560 --> 00:04:31,680
metrics in some json format

00:04:28,560 --> 00:04:32,720
you can use this to to scale on and then

00:04:31,680 --> 00:04:35,120
we also have

00:04:32,720 --> 00:04:36,720
segment checks which is our internal

00:04:35,120 --> 00:04:39,759
monitoring solution

00:04:36,720 --> 00:04:42,080
where you can also get metrics in some

00:04:39,759 --> 00:04:45,120
different formats we also support

00:04:42,080 --> 00:04:47,040
and we lastly also support influx db

00:04:45,120 --> 00:04:49,759
queries we don't use influx db in

00:04:47,040 --> 00:04:52,960
solando but this was contributed

00:04:49,759 --> 00:04:54,880
to us from outside and and the cube

00:04:52,960 --> 00:04:56,880
matrix adapter is an open source project

00:04:54,880 --> 00:05:00,479
available and

00:04:56,880 --> 00:05:02,400
also has users outside of celando so

00:05:00,479 --> 00:05:04,960
yeah you can take a look at the link i

00:05:02,400 --> 00:05:07,840
put here in the slide

00:05:04,960 --> 00:05:09,840
and just to show how it it looks like

00:05:07,840 --> 00:05:10,639
with an example here is where we we

00:05:09,840 --> 00:05:12,560
scale on

00:05:10,639 --> 00:05:13,840
requests per second so this is one of

00:05:12,560 --> 00:05:16,000
our services

00:05:13,840 --> 00:05:17,680
where you see the orange which is the

00:05:16,000 --> 00:05:19,600
request per second and you see the

00:05:17,680 --> 00:05:21,120
the black line which is the number of

00:05:19,600 --> 00:05:24,320
pots and this

00:05:21,120 --> 00:05:24,639
shows clearly that like it's useful to

00:05:24,320 --> 00:05:26,160
have

00:05:24,639 --> 00:05:29,039
something that scales based on the

00:05:26,160 --> 00:05:29,039
amount of traffic

00:05:30,320 --> 00:05:34,400
um of course there are also some

00:05:32,160 --> 00:05:37,440
challenges using hpa

00:05:34,400 --> 00:05:40,080
and some limitations that

00:05:37,440 --> 00:05:42,639
is either not supported yet or supported

00:05:40,080 --> 00:05:44,800
very in very recent kubernetes versions

00:05:42,639 --> 00:05:48,000
so one thing is scaling behavior which

00:05:44,800 --> 00:05:50,320
was unsealed kubernetes 118

00:05:48,000 --> 00:05:52,160
a cluster-wide setting so you couldn't

00:05:50,320 --> 00:05:55,120
do it per application

00:05:52,160 --> 00:05:56,960
um and another thing is that parts with

00:05:55,120 --> 00:05:59,199
multiple containers inside are not

00:05:56,960 --> 00:06:01,600
handled variable by the auto scaler

00:05:59,199 --> 00:06:03,039
and i want to give a shout out to arjun

00:06:01,600 --> 00:06:04,720
nike who

00:06:03,039 --> 00:06:06,720
was in our team before and that doesn't

00:06:04,720 --> 00:06:08,720
work at slander anymore but while he was

00:06:06,720 --> 00:06:10,639
here he did a lot of contribution

00:06:08,720 --> 00:06:13,600
upstream to kubernetes around

00:06:10,639 --> 00:06:15,280
the hpa and he's working on the on the

00:06:13,600 --> 00:06:17,360
first issue and he's currently working

00:06:15,280 --> 00:06:19,039
on the second issue which is not yet in

00:06:17,360 --> 00:06:20,319
kubernetes but hopefully we will see it

00:06:19,039 --> 00:06:24,319
soon

00:06:20,319 --> 00:06:27,120
as a fix for for multiple containers

00:06:24,319 --> 00:06:27,680
and the the scaling behavior issue or

00:06:27,120 --> 00:06:30,319
the

00:06:27,680 --> 00:06:30,880
the changes that were done in in 118 is

00:06:30,319 --> 00:06:34,240
that

00:06:30,880 --> 00:06:35,919
until 117 and including 117 you

00:06:34,240 --> 00:06:38,000
just had some settings you could set

00:06:35,919 --> 00:06:40,400
cluster wide so

00:06:38,000 --> 00:06:42,800
namely this downscaling stabilization is

00:06:40,400 --> 00:06:46,160
interesting this means how

00:06:42,800 --> 00:06:49,039
how fast a service will scale down

00:06:46,160 --> 00:06:50,800
depending on the metrics and normally it

00:06:49,039 --> 00:06:51,919
was just five minutes so no matter what

00:06:50,800 --> 00:06:53,280
kind of service you have

00:06:51,919 --> 00:06:55,840
you just have to rely on these five

00:06:53,280 --> 00:06:57,280
minutes but from kubernetes 118 you can

00:06:55,840 --> 00:06:59,520
configure these

00:06:57,280 --> 00:07:01,680
individually per auto scaling

00:06:59,520 --> 00:07:04,000
configuration so per application

00:07:01,680 --> 00:07:05,440
so for instance you could do here's and

00:07:04,000 --> 00:07:09,039
in the example where you set up

00:07:05,440 --> 00:07:11,360
a lower scaling window from 5 minutes to

00:07:09,039 --> 00:07:13,120
60 seconds and this would mean that it

00:07:11,360 --> 00:07:14,880
would scale down faster you could also

00:07:13,120 --> 00:07:17,520
increase it if you want to be more

00:07:14,880 --> 00:07:19,280
conservative with scaling down and

00:07:17,520 --> 00:07:23,360
another thing you can do is use

00:07:19,280 --> 00:07:26,400
policies which is behavior policies

00:07:23,360 --> 00:07:27,680
which is a way to say if you are

00:07:26,400 --> 00:07:29,520
supposed to scale down

00:07:27,680 --> 00:07:31,199
let's say the hpa calculate to scale

00:07:29,520 --> 00:07:33,120
down by 10 parts

00:07:31,199 --> 00:07:34,400
then you can say that it should only

00:07:33,120 --> 00:07:37,840
scale down 10

00:07:34,400 --> 00:07:39,440
at a time so one part at a time

00:07:37,840 --> 00:07:41,599
and this way it doesn't just drop 10

00:07:39,440 --> 00:07:44,479
parts at once but it

00:07:41,599 --> 00:07:45,520
drops the the ten parts more slowly one

00:07:44,479 --> 00:07:47,759
one by one

00:07:45,520 --> 00:07:48,960
so it's a way to to reduce the scaling

00:07:47,759 --> 00:07:51,039
in case

00:07:48,960 --> 00:07:52,479
um it would have been too much so it

00:07:51,039 --> 00:07:54,479
could be that you have

00:07:52,479 --> 00:07:55,759
a not consistent load on your system and

00:07:54,479 --> 00:07:58,800
you don't want to just scale

00:07:55,759 --> 00:08:00,560
too aggressively down and these

00:07:58,800 --> 00:08:02,400
behaviors also work on the

00:08:00,560 --> 00:08:04,160
on the other direction so when scaling

00:08:02,400 --> 00:08:05,840
up you can also tweak this

00:08:04,160 --> 00:08:08,800
so it's very useful for having this

00:08:05,840 --> 00:08:10,840
pre-application where you have different

00:08:08,800 --> 00:08:13,840
different behaviors that you need to

00:08:10,840 --> 00:08:15,919
handle

00:08:13,840 --> 00:08:17,680
regarding the multi container parts

00:08:15,919 --> 00:08:19,680
issue

00:08:17,680 --> 00:08:21,280
here's an example of how it how it

00:08:19,680 --> 00:08:23,039
currently works so you can understand

00:08:21,280 --> 00:08:25,039
what the problem is

00:08:23,039 --> 00:08:26,080
imagine you have a pot with two

00:08:25,039 --> 00:08:28,479
containers inside

00:08:26,080 --> 00:08:31,680
and they each request some in this

00:08:28,479 --> 00:08:35,519
example they request 250 millicourse

00:08:31,680 --> 00:08:37,919
cpu and they one uses 200 millicourse

00:08:35,519 --> 00:08:39,599
and the other uses 50 millicourse

00:08:37,919 --> 00:08:41,519
and you could think of this as the green

00:08:39,599 --> 00:08:43,440
one is the main application

00:08:41,519 --> 00:08:45,600
in your in your pot and the other one is

00:08:43,440 --> 00:08:48,399
a side car it could be like a

00:08:45,600 --> 00:08:49,920
um like a stereo sidecar for a service

00:08:48,399 --> 00:08:51,839
mesh or something like this or it could

00:08:49,920 --> 00:08:52,800
be something that ships locks or

00:08:51,839 --> 00:08:54,880
whatever

00:08:52,800 --> 00:08:56,160
the main point is that they you might

00:08:54,880 --> 00:08:57,680
have one application

00:08:56,160 --> 00:08:59,680
or one container which is the main

00:08:57,680 --> 00:09:01,600
container that you want to scale on

00:08:59,680 --> 00:09:03,920
um and the other one is kind of

00:09:01,600 --> 00:09:05,040
secondary but the way it works is that

00:09:03,920 --> 00:09:08,000
it takes

00:09:05,040 --> 00:09:09,680
the sum of all the containers so instead

00:09:08,000 --> 00:09:11,920
of treating them individually

00:09:09,680 --> 00:09:14,080
it just take the sum and the sum of

00:09:11,920 --> 00:09:17,279
usage and the sum of

00:09:14,080 --> 00:09:19,600
requested cpu and then

00:09:17,279 --> 00:09:21,200
instead of having 80 for the for the

00:09:19,600 --> 00:09:24,320
main container the

00:09:21,200 --> 00:09:27,920
the result of the hpa is that it sees

00:09:24,320 --> 00:09:30,480
50 utilization across all the containers

00:09:27,920 --> 00:09:31,200
so not necessarily wrong the calculation

00:09:30,480 --> 00:09:32,959
but it's not

00:09:31,200 --> 00:09:34,320
really helpful for what you want because

00:09:32,959 --> 00:09:35,920
if you want to scale

00:09:34,320 --> 00:09:38,720
your main application or your main

00:09:35,920 --> 00:09:41,360
container when it's at 50 percent

00:09:38,720 --> 00:09:44,560
and it's as at 80 percent in this

00:09:41,360 --> 00:09:47,519
example then it wouldn't scale up

00:09:44,560 --> 00:09:49,040
so this is this is the problem and the

00:09:47,519 --> 00:09:51,600
solution for this problem that is

00:09:49,040 --> 00:09:54,240
proposed and currently being implemented

00:09:51,600 --> 00:09:54,880
is to extend the way you define metrics

00:09:54,240 --> 00:09:58,000
so

00:09:54,880 --> 00:09:58,560
in the in the past or currently you have

00:09:58,000 --> 00:10:00,880
just

00:09:58,560 --> 00:10:03,200
you can you can specify a type resource

00:10:00,880 --> 00:10:04,000
and it could be cpu and then you specify

00:10:03,200 --> 00:10:07,519
the

00:10:04,000 --> 00:10:09,680
the average utilization and this is this

00:10:07,519 --> 00:10:11,360
across all the containers but in the

00:10:09,680 --> 00:10:13,360
future you would be able to specify

00:10:11,360 --> 00:10:14,480
another metric type which is container

00:10:13,360 --> 00:10:16,399
resource

00:10:14,480 --> 00:10:17,680
and here you can specify which container

00:10:16,399 --> 00:10:19,440
that you want to

00:10:17,680 --> 00:10:21,200
scale based on so you can also add

00:10:19,440 --> 00:10:23,120
multiple containers and scale on all of

00:10:21,200 --> 00:10:24,800
them with different strategies

00:10:23,120 --> 00:10:27,440
but the main point is that you can

00:10:24,800 --> 00:10:31,200
select the one that you most care about

00:10:27,440 --> 00:10:33,360
and that mostly yeah explains how your

00:10:31,200 --> 00:10:35,360
application is scaling

00:10:33,360 --> 00:10:36,640
so this will be very very useful when

00:10:35,360 --> 00:10:40,560
this lens unfortunately

00:10:36,640 --> 00:10:43,360
it is what not in in 119 of kubernetes

00:10:40,560 --> 00:10:45,120
but hopefully we will see it in 120.

00:10:43,360 --> 00:10:46,800
definitely looking forward to this one

00:10:45,120 --> 00:10:49,120
and this is also adrian that is working

00:10:46,800 --> 00:10:49,120
on that

00:10:50,160 --> 00:10:54,720
now now we talked about the the

00:10:52,880 --> 00:10:55,920
horizontal part autoscaling this is what

00:10:54,720 --> 00:10:57,920
our users do

00:10:55,920 --> 00:10:59,920
but it doesn't make sense to scale the

00:10:57,920 --> 00:11:00,800
parts horizontally unless the cluster

00:10:59,920 --> 00:11:03,440
also scales

00:11:00,800 --> 00:11:04,720
underneath and for this we have cluster

00:11:03,440 --> 00:11:07,680
autoscaling

00:11:04,720 --> 00:11:11,120
and there we use the official cluster

00:11:07,680 --> 00:11:11,120
autoscaler for kubernetes

00:11:11,360 --> 00:11:18,160
just to explain how it works is that

00:11:14,480 --> 00:11:19,120
normally you would deploy auto scaling

00:11:18,160 --> 00:11:21,440
groups with your

00:11:19,120 --> 00:11:23,680
with your instances so this is running

00:11:21,440 --> 00:11:25,600
in aws as an example it also supports

00:11:23,680 --> 00:11:27,920
dke and other cloud providers

00:11:25,600 --> 00:11:29,680
but in aws you have autoscaling groups

00:11:27,920 --> 00:11:32,720
and you place your nodes

00:11:29,680 --> 00:11:35,440
or your instances in those and then

00:11:32,720 --> 00:11:37,360
once there is a pending pot the

00:11:35,440 --> 00:11:39,279
autoscaler will

00:11:37,360 --> 00:11:41,360
first see if it can fit on an existing

00:11:39,279 --> 00:11:44,480
node if not it will try to

00:11:41,360 --> 00:11:47,680
scale up in node in in one of these

00:11:44,480 --> 00:11:48,800
availability zones and normally for

00:11:47,680 --> 00:11:50,720
parts that don't have so much

00:11:48,800 --> 00:11:51,519
requirements they can run in any of the

00:11:50,720 --> 00:11:54,720
zones

00:11:51,519 --> 00:11:56,480
but if a pod needs a volume it has is

00:11:54,720 --> 00:11:58,560
usually attached to a single zone and

00:11:56,480 --> 00:11:58,959
then the auto scaler needs to pick the

00:11:58,560 --> 00:12:02,079
right

00:11:58,959 --> 00:12:04,480
zone so in this example it would be zone

00:12:02,079 --> 00:12:06,560
c that it has to pick

00:12:04,480 --> 00:12:09,120
and to manage this on our side we have

00:12:06,560 --> 00:12:10,880
some abstraction on top of these

00:12:09,120 --> 00:12:13,920
all scaling groups that we call node

00:12:10,880 --> 00:12:17,440
pools and a node pool is basically

00:12:13,920 --> 00:12:17,440
a way to define um

00:12:17,920 --> 00:12:23,519
like one type of instance and so on that

00:12:21,519 --> 00:12:24,320
is mapping to multiple auto scaling

00:12:23,519 --> 00:12:26,480
groups

00:12:24,320 --> 00:12:28,560
so by default we met two three

00:12:26,480 --> 00:12:31,680
autoscaling groups but we can also

00:12:28,560 --> 00:12:33,360
reduce it to less if we want to

00:12:31,680 --> 00:12:34,959
have a certain need for that but

00:12:33,360 --> 00:12:35,440
basically you define a node pool you

00:12:34,959 --> 00:12:37,680
give it

00:12:35,440 --> 00:12:39,519
you then you get all the scaling groups

00:12:37,680 --> 00:12:42,000
with the same instance types

00:12:39,519 --> 00:12:43,120
the same note uh labels and tens if you

00:12:42,000 --> 00:12:46,560
want to have special

00:12:43,120 --> 00:12:49,200
scaling or scheduling logic there

00:12:46,560 --> 00:12:51,120
and the same min and max size but the

00:12:49,200 --> 00:12:53,760
difference is that

00:12:51,120 --> 00:12:54,560
they are in different zones so if we

00:12:53,760 --> 00:12:56,959
look this from

00:12:54,560 --> 00:12:58,240
from how we define it we basically have

00:12:56,959 --> 00:13:00,399
a list of

00:12:58,240 --> 00:13:02,639
node pools for each of our clusters we

00:13:00,399 --> 00:13:04,800
we can give them a name like default

00:13:02,639 --> 00:13:07,279
define the ins the instance types to be

00:13:04,800 --> 00:13:08,639
used and then set a min and max

00:13:07,279 --> 00:13:10,800
and then we also have something we call

00:13:08,639 --> 00:13:12,959
config items which is a way to

00:13:10,800 --> 00:13:14,800
either add labels obtains or set other

00:13:12,959 --> 00:13:15,680
special configurations for the node

00:13:14,800 --> 00:13:18,560
pools

00:13:15,680 --> 00:13:20,560
um so one example could be that we have

00:13:18,560 --> 00:13:21,600
a custom node pool which has a certain

00:13:20,560 --> 00:13:24,320
instance type with

00:13:21,600 --> 00:13:26,240
local ssd storage and then we will label

00:13:24,320 --> 00:13:29,120
and change these nodes with

00:13:26,240 --> 00:13:30,880
a dedicated storage label such that the

00:13:29,120 --> 00:13:33,360
parts that really need to run this and

00:13:30,880 --> 00:13:34,079
using local ssd can target those in

00:13:33,360 --> 00:13:37,519
there

00:13:34,079 --> 00:13:40,720
in the node selector or the toleration

00:13:37,519 --> 00:13:41,680
um this is how we operate like how we

00:13:40,720 --> 00:13:43,519
handle that

00:13:41,680 --> 00:13:45,839
there are special cases for different

00:13:43,519 --> 00:13:47,440
parts so it could also be a cpu node we

00:13:45,839 --> 00:13:49,600
would label in a certain way

00:13:47,440 --> 00:13:50,720
so only parts that actually need to use

00:13:49,600 --> 00:13:53,970
the cpu

00:13:50,720 --> 00:13:57,039
gpu i mean would land on this

00:13:53,970 --> 00:13:57,039
[Music]

00:13:57,360 --> 00:14:03,760
and because the cluster order scalar

00:14:01,360 --> 00:14:05,519
by default doesn't do everything we want

00:14:03,760 --> 00:14:08,000
and doesn't do everything we want

00:14:05,519 --> 00:14:09,839
as well as we want we have made some

00:14:08,000 --> 00:14:10,959
changes to the official and we run a

00:14:09,839 --> 00:14:13,360
fork of it

00:14:10,959 --> 00:14:15,120
um and the changes we have worked on is

00:14:13,360 --> 00:14:17,519
to do more robust template node

00:14:15,120 --> 00:14:20,000
generation so a template node is

00:14:17,519 --> 00:14:21,040
that there's a pot that is pending and

00:14:20,000 --> 00:14:22,959
the auto scaler

00:14:21,040 --> 00:14:24,800
tries to calculate what kind of

00:14:22,959 --> 00:14:26,560
instances need to create or what kind of

00:14:24,800 --> 00:14:30,320
kubernetes node it would need

00:14:26,560 --> 00:14:32,320
to satisfy the pot and um

00:14:30,320 --> 00:14:34,160
in the in the official auto scale it's

00:14:32,320 --> 00:14:36,160
not so easy if you don't have any

00:14:34,160 --> 00:14:37,680
existing nodes because it relies on the

00:14:36,160 --> 00:14:40,000
existing nodes in the cluster

00:14:37,680 --> 00:14:40,959
but if you're scaling from zero you need

00:14:40,000 --> 00:14:43,040
a way to

00:14:40,959 --> 00:14:44,000
predict how the node will look like once

00:14:43,040 --> 00:14:45,360
it's created

00:14:44,000 --> 00:14:47,600
and this we have made some changes

00:14:45,360 --> 00:14:49,839
around that and we also added support

00:14:47,600 --> 00:14:52,720
for multiple instance types on aws

00:14:49,839 --> 00:14:53,600
um this is mainly to handle spot where

00:14:52,720 --> 00:14:55,199
you can

00:14:53,600 --> 00:14:56,800
create auto scaling groups that have

00:14:55,199 --> 00:15:00,240
different instance types

00:14:56,800 --> 00:15:03,199
and then aws spot can select which

00:15:00,240 --> 00:15:04,240
which is the best for the time so if

00:15:03,199 --> 00:15:06,000
there's a certain

00:15:04,240 --> 00:15:07,519
shortage in one instance type it will

00:15:06,000 --> 00:15:08,240
select another instance type and give

00:15:07,519 --> 00:15:11,920
you

00:15:08,240 --> 00:15:13,279
um additionally we have added a custom

00:15:11,920 --> 00:15:16,320
customizable backup

00:15:13,279 --> 00:15:18,480
settings which is needed when you

00:15:16,320 --> 00:15:20,480
are trying to scale up auto scanning

00:15:18,480 --> 00:15:24,000
group and it fails for whatever reason

00:15:20,480 --> 00:15:25,760
we hit many issues where aws simply

00:15:24,000 --> 00:15:26,000
don't don't have enough of the instance

00:15:25,760 --> 00:15:28,160
type

00:15:26,000 --> 00:15:30,720
that we want and then the auto scaler

00:15:28,160 --> 00:15:32,399
has to fall back to another kind of auto

00:15:30,720 --> 00:15:34,079
scaling group

00:15:32,399 --> 00:15:36,720
which has a different instance type that

00:15:34,079 --> 00:15:38,480
can be provided at this time

00:15:36,720 --> 00:15:40,560
and we want this to be as fast as

00:15:38,480 --> 00:15:43,120
possible so we don't have parts pending

00:15:40,560 --> 00:15:46,000
for too long time

00:15:43,120 --> 00:15:48,000
and last thing we did is we added some

00:15:46,000 --> 00:15:50,160
priority based expander

00:15:48,000 --> 00:15:52,240
expander is a name used in the

00:15:50,160 --> 00:15:55,120
autoscaler for

00:15:52,240 --> 00:15:56,639
logic that can kind of figure out which

00:15:55,120 --> 00:16:00,000
node to pick which is the

00:15:56,639 --> 00:16:02,560
best one to pick and we added um

00:16:00,000 --> 00:16:04,480
a simple one which uses priorities and

00:16:02,560 --> 00:16:07,120
the way it works is that

00:16:04,480 --> 00:16:08,320
we have here again the the node pool

00:16:07,120 --> 00:16:10,079
overview but

00:16:08,320 --> 00:16:11,600
you can see the config item that has

00:16:10,079 --> 00:16:13,519
priorities defined

00:16:11,600 --> 00:16:15,839
and this way we can define the default

00:16:13,519 --> 00:16:18,079
null pool with the priority zero

00:16:15,839 --> 00:16:19,759
and then we can define fallback node

00:16:18,079 --> 00:16:22,480
pools of different

00:16:19,759 --> 00:16:24,079
instance types to have things to fall

00:16:22,480 --> 00:16:27,680
back to in case we

00:16:24,079 --> 00:16:29,920
cannot provide the aws doesn't have the

00:16:27,680 --> 00:16:31,040
the one that we want by default and this

00:16:29,920 --> 00:16:33,920
can both work as

00:16:31,040 --> 00:16:34,959
just fall back so going back to an m4

00:16:33,920 --> 00:16:36,959
large instead of an

00:16:34,959 --> 00:16:38,639
m5 light so going back to an older

00:16:36,959 --> 00:16:40,480
generation instance

00:16:38,639 --> 00:16:42,480
but it can also work in the way that we

00:16:40,480 --> 00:16:45,120
always choose the smallest instance type

00:16:42,480 --> 00:16:48,399
unless the parts request

00:16:45,120 --> 00:16:50,639
a bigger size than if the pod has to run

00:16:48,399 --> 00:16:53,680
an m5x large because of the request

00:16:50,639 --> 00:16:54,160
then it would would land on this instead

00:16:53,680 --> 00:16:57,040
of

00:16:54,160 --> 00:16:57,600
instead of a smaller one and for spot

00:16:57,040 --> 00:16:59,920
pools

00:16:57,600 --> 00:17:01,839
just as an example we can define like i

00:16:59,920 --> 00:17:03,600
said multiple instance types

00:17:01,839 --> 00:17:06,240
and then we don't know which of them we

00:17:03,600 --> 00:17:08,319
get but whatever aws picks as the best

00:17:06,240 --> 00:17:12,079
one for the time in terms of

00:17:08,319 --> 00:17:14,880
getting uh getting taken out of spot

00:17:12,079 --> 00:17:16,240
to terminate it earlier and for this we

00:17:14,880 --> 00:17:18,880
can lose

00:17:16,240 --> 00:17:19,919
we also add a label so you can you you

00:17:18,880 --> 00:17:21,679
can select and

00:17:19,919 --> 00:17:25,360
actively opt into running on spot

00:17:21,679 --> 00:17:25,360
instances if you want to do this

00:17:26,959 --> 00:17:31,600
of course the auto scale also have some

00:17:28,960 --> 00:17:34,880
limits um

00:17:31,600 --> 00:17:35,679
and are not the not the auto scale

00:17:34,880 --> 00:17:38,960
itself but

00:17:35,679 --> 00:17:40,720
once you figure solve all these problems

00:17:38,960 --> 00:17:43,039
of the auto scaler so it can scale out

00:17:40,720 --> 00:17:44,799
and pick new instances and so on

00:17:43,039 --> 00:17:46,559
then what you start to do is you start

00:17:44,799 --> 00:17:49,280
to hit limits

00:17:46,559 --> 00:17:51,360
and it's we found that it's very easy to

00:17:49,280 --> 00:17:53,200
hit the default aws limits on

00:17:51,360 --> 00:17:54,640
on the number of instances that you can

00:17:53,200 --> 00:17:57,679
have in your account

00:17:54,640 --> 00:18:00,400
um and we also hit

00:17:57,679 --> 00:18:02,160
limits in the networking layout so for

00:18:00,400 --> 00:18:04,160
the aws limits

00:18:02,160 --> 00:18:05,679
basically every account that is created

00:18:04,160 --> 00:18:07,280
has a certain default

00:18:05,679 --> 00:18:09,039
set of limiters so you cannot just

00:18:07,280 --> 00:18:10,400
create an account and create millions of

00:18:09,039 --> 00:18:12,720
instances

00:18:10,400 --> 00:18:14,720
and then in order to get this limit

00:18:12,720 --> 00:18:16,480
increased you need to open a support

00:18:14,720 --> 00:18:18,559
request to aws

00:18:16,480 --> 00:18:20,320
and this became very time consuming

00:18:18,559 --> 00:18:22,640
because we had to do this

00:18:20,320 --> 00:18:23,760
for many accounts many clusters and we

00:18:22,640 --> 00:18:25,360
had to do this for

00:18:23,760 --> 00:18:27,120
all of these different node pools that

00:18:25,360 --> 00:18:29,760
we create um

00:18:27,120 --> 00:18:30,880
so what we did is that we have a cron

00:18:29,760 --> 00:18:33,600
job that just

00:18:30,880 --> 00:18:34,400
looks at how much we want and then it

00:18:33,600 --> 00:18:36,320
automatically

00:18:34,400 --> 00:18:37,919
creates the the support request rate of

00:18:36,320 --> 00:18:39,600
this and bumps the limits

00:18:37,919 --> 00:18:42,720
so this helps us a lot so we don't have

00:18:39,600 --> 00:18:45,039
to to do this ourselves

00:18:42,720 --> 00:18:46,240
and regarding the limit on the network

00:18:45,039 --> 00:18:48,160
um

00:18:46,240 --> 00:18:49,919
this is basically this depends a little

00:18:48,160 --> 00:18:50,720
bit on the network layout you have and

00:18:49,919 --> 00:18:52,880
how you

00:18:50,720 --> 00:18:54,080
what kind of network interface you're

00:18:52,880 --> 00:18:56,480
using kubernetes

00:18:54,080 --> 00:18:57,440
but in our setup we picked for all our

00:18:56,480 --> 00:19:00,960
clusters

00:18:57,440 --> 00:19:04,880
slash 16 uh cider which gives us

00:19:00,960 --> 00:19:08,000
around 65 000 addresses and

00:19:04,880 --> 00:19:09,440
with the default um configuration per

00:19:08,000 --> 00:19:13,360
node you have

00:19:09,440 --> 00:19:16,240
24 which gives you 256

00:19:13,360 --> 00:19:17,720
addresses per node and this also means

00:19:16,240 --> 00:19:20,480
that you can maximum have

00:19:17,720 --> 00:19:22,080
256 nodes because otherwise the address

00:19:20,480 --> 00:19:24,960
space is used up

00:19:22,080 --> 00:19:27,840
so the first thing we hit was this limit

00:19:24,960 --> 00:19:28,799
that we couldn't create more than 256

00:19:27,840 --> 00:19:32,240
nodes

00:19:28,799 --> 00:19:34,160
and then we found a way to

00:19:32,240 --> 00:19:35,840
we basically figured out that the way to

00:19:34,160 --> 00:19:38,640
deal with this is to

00:19:35,840 --> 00:19:39,679
just increase the side or lower the

00:19:38,640 --> 00:19:43,440
lower the side

00:19:39,679 --> 00:19:46,640
address space to slash 15 per node

00:19:43,440 --> 00:19:50,160
because then you have only 128

00:19:46,640 --> 00:19:54,080
addresses per node but you get 512

00:19:50,160 --> 00:19:55,760
nodes and you can go further and select

00:19:54,080 --> 00:19:58,400
less addresses per node and then get

00:19:55,760 --> 00:20:01,919
more nodes in total

00:19:58,400 --> 00:20:04,480
the lowest we have is slash 26 because

00:20:01,919 --> 00:20:06,000
once you go to slash 27 you have so few

00:20:04,480 --> 00:20:09,360
addresses per node

00:20:06,000 --> 00:20:10,799
that it becomes hard to schedule any

00:20:09,360 --> 00:20:12,960
pots on the nodes

00:20:10,799 --> 00:20:14,480
because usually kubernetes expects that

00:20:12,960 --> 00:20:15,200
you have at least a double amount of

00:20:14,480 --> 00:20:18,559
averages

00:20:15,200 --> 00:20:20,960
as as much minimum number of pots

00:20:18,559 --> 00:20:22,799
and this is to ensure that it can easily

00:20:20,960 --> 00:20:24,240
shuffle around parts without giving them

00:20:22,799 --> 00:20:25,840
the same addresses

00:20:24,240 --> 00:20:28,159
as a previous part because then you

00:20:25,840 --> 00:20:31,039
could be sending traffic to

00:20:28,159 --> 00:20:32,720
to the wrong application basically um

00:20:31,039 --> 00:20:34,000
and one thing also to notice that the

00:20:32,720 --> 00:20:37,039
cubelet has a limit

00:20:34,000 --> 00:20:37,679
of 110 parts by default so even with

00:20:37,039 --> 00:20:41,760
this

00:20:37,679 --> 00:20:44,880
slash 24 where you have technically 256

00:20:41,760 --> 00:20:45,919
addresses you can only schedule 110

00:20:44,880 --> 00:20:47,520
parts

00:20:45,919 --> 00:20:50,559
this also means that if you want to

00:20:47,520 --> 00:20:53,039
lower this uh cider to slash 25

00:20:50,559 --> 00:20:54,559
or less 26 then you also need to change

00:20:53,039 --> 00:20:56,799
the flag on the cubelet

00:20:54,559 --> 00:20:58,880
to have less spots because otherwise you

00:20:56,799 --> 00:21:00,159
can schedule more than you actually have

00:20:58,880 --> 00:21:02,799
addresses for

00:21:00,159 --> 00:21:04,320
so this is something to look out for and

00:21:02,799 --> 00:21:06,480
we have basically

00:21:04,320 --> 00:21:07,919
configured this that we just have per

00:21:06,480 --> 00:21:10,480
cluster we can configure this

00:21:07,919 --> 00:21:13,360
the node sider and then we automatically

00:21:10,480 --> 00:21:16,000
calculate how many parts per node and

00:21:13,360 --> 00:21:17,760
uh yeah but the limit we have right now

00:21:16,000 --> 00:21:20,960
is thousand parts with this current

00:21:17,760 --> 00:21:23,280
uh setup so if we want to go

00:21:20,960 --> 00:21:25,120
further in the in the future more nodes

00:21:23,280 --> 00:21:28,080
per cluster then we need to

00:21:25,120 --> 00:21:29,919
to change how the network setup is um so

00:21:28,080 --> 00:21:31,600
if you're creating a new cluster and you

00:21:29,919 --> 00:21:34,640
expect to have a lot of nodes then it's

00:21:31,600 --> 00:21:34,640
something to be aware of

00:21:35,840 --> 00:21:39,039
and of course once you fix all of these

00:21:38,000 --> 00:21:41,600
limit problems

00:21:39,039 --> 00:21:42,880
and you allow your users to scale up to

00:21:41,600 --> 00:21:45,360
a thousand nodes

00:21:42,880 --> 00:21:46,880
then they will also do it so they will

00:21:45,360 --> 00:21:47,600
make sure to create enough parts to

00:21:46,880 --> 00:21:50,320
scale out

00:21:47,600 --> 00:21:51,919
and this is just an example of this and

00:21:50,320 --> 00:21:54,240
by scaling up the

00:21:51,919 --> 00:21:55,440
cluster this is one thing this is

00:21:54,240 --> 00:21:57,760
actually fine

00:21:55,440 --> 00:21:59,600
but it obviously also puts some loads on

00:21:57,760 --> 00:22:02,720
the control plane

00:21:59,600 --> 00:22:04,240
so on the api server and and so on and

00:22:02,720 --> 00:22:06,480
this is just an example of

00:22:04,240 --> 00:22:08,799
yeah when we skipped a thousand then our

00:22:06,480 --> 00:22:10,559
latency goes from

00:22:08,799 --> 00:22:13,120
so little we cannot even see it here

00:22:10,559 --> 00:22:16,080
until uh several seconds

00:22:13,120 --> 00:22:17,200
in latency and yeah this is obviously

00:22:16,080 --> 00:22:19,200
not ideal because

00:22:17,200 --> 00:22:21,600
every time you do keep yourself get pots

00:22:19,200 --> 00:22:23,360
or any controller that needs to list all

00:22:21,600 --> 00:22:25,120
the products in the cluster will

00:22:23,360 --> 00:22:26,880
either time out or it will take much

00:22:25,120 --> 00:22:30,000
longer than than

00:22:26,880 --> 00:22:31,919
normally um

00:22:30,000 --> 00:22:33,840
so in order to handle this we have

00:22:31,919 --> 00:22:35,840
something we call the control plane auto

00:22:33,840 --> 00:22:38,960
scaler which is something that we

00:22:35,840 --> 00:22:40,720
develop for our needs and

00:22:38,960 --> 00:22:42,159
the way it works is that it scales

00:22:40,720 --> 00:22:44,799
vertically the parts

00:22:42,159 --> 00:22:45,760
the the the control plane nodes so they

00:22:44,799 --> 00:22:47,679
will just run on

00:22:45,760 --> 00:22:49,679
ec2 instances and they are scaled

00:22:47,679 --> 00:22:51,360
vertically the reason we scale

00:22:49,679 --> 00:22:54,720
vertically and not horizontally

00:22:51,360 --> 00:22:56,799
is that what you want to scale is that a

00:22:54,720 --> 00:22:58,000
single instance can read all the parts

00:22:56,799 --> 00:23:01,039
from it cd

00:22:58,000 --> 00:23:04,320
and and send it via the api server so

00:23:01,039 --> 00:23:06,159
having many small instances would not

00:23:04,320 --> 00:23:08,880
actually solve the problem because

00:23:06,159 --> 00:23:10,000
then do just have many small instances

00:23:08,880 --> 00:23:12,880
that need to

00:23:10,000 --> 00:23:14,640
read a lot of data and what you want is

00:23:12,880 --> 00:23:18,640
bigger instances that can lead

00:23:14,640 --> 00:23:19,440
a lot of data faster so we already run

00:23:18,640 --> 00:23:21,440
with two

00:23:19,440 --> 00:23:23,360
nodes at minimum to have highly

00:23:21,440 --> 00:23:25,360
available but we don't scale it

00:23:23,360 --> 00:23:26,480
further horizontally instead we scale it

00:23:25,360 --> 00:23:29,120
vertically

00:23:26,480 --> 00:23:30,720
and we basically look at cpu load as an

00:23:29,120 --> 00:23:33,520
indicator for scaling

00:23:30,720 --> 00:23:35,520
um it could also be interesting to look

00:23:33,520 --> 00:23:36,000
at memory but we found that cpu is

00:23:35,520 --> 00:23:38,240
enough to

00:23:36,000 --> 00:23:39,280
indicate whether we need to pick a

00:23:38,240 --> 00:23:41,600
bigger instance

00:23:39,280 --> 00:23:43,360
um and the way we scale this just by

00:23:41,600 --> 00:23:45,360
changing the instance type so

00:23:43,360 --> 00:23:47,360
we basically sort all the instance types

00:23:45,360 --> 00:23:50,320
available in in aws

00:23:47,360 --> 00:23:52,480
by vcpu and memory and then we exclude

00:23:50,320 --> 00:23:53,279
some like we don't include gpu nodes and

00:23:52,480 --> 00:23:57,840
so on

00:23:53,279 --> 00:23:59,679
but then if if we see the cpu load is

00:23:57,840 --> 00:24:00,880
is high at a certain level that we want

00:23:59,679 --> 00:24:04,320
to scale out

00:24:00,880 --> 00:24:06,480
then we just or this

00:24:04,320 --> 00:24:08,080
control plane auto scaler will pick the

00:24:06,480 --> 00:24:11,679
next instance type

00:24:08,080 --> 00:24:14,400
um sorted by by vcpu and memory

00:24:11,679 --> 00:24:15,440
and one of the most important things is

00:24:14,400 --> 00:24:17,840
that this

00:24:15,440 --> 00:24:19,600
automates a previous manual task because

00:24:17,840 --> 00:24:20,480
before we just had alerts that would

00:24:19,600 --> 00:24:22,720
tell us

00:24:20,480 --> 00:24:24,240
the api server is on the high load and

00:24:22,720 --> 00:24:26,000
then we would go and

00:24:24,240 --> 00:24:27,840
figure out okay this is running this

00:24:26,000 --> 00:24:28,400
instance type it's changing to a bigger

00:24:27,840 --> 00:24:30,799
one

00:24:28,400 --> 00:24:32,799
and now we have automated this which

00:24:30,799 --> 00:24:36,320
makes it much easier to manage

00:24:32,799 --> 00:24:38,240
uh 150 clusters and it's not only good

00:24:36,320 --> 00:24:40,080
for scaling out so it's getting bigger

00:24:38,240 --> 00:24:41,200
it's also good for scaling down again

00:24:40,080 --> 00:24:43,200
once the

00:24:41,200 --> 00:24:44,960
the cluster is under less load because

00:24:43,200 --> 00:24:46,880
you don't want to just leave

00:24:44,960 --> 00:24:48,720
running with the very big instances that

00:24:46,880 --> 00:24:50,640
are much more expensive so

00:24:48,720 --> 00:24:52,720
this helps us both on cost saving and

00:24:50,640 --> 00:24:55,679
also on on the manual work that we would

00:24:52,720 --> 00:24:55,679
have to do in the past

00:24:56,640 --> 00:25:00,880
along with the vertical scaling of the

00:24:58,640 --> 00:25:03,200
control plane we also have vertical pod

00:25:00,880 --> 00:25:05,279
auto scaling

00:25:03,200 --> 00:25:08,640
and here we use the vertical pod auto

00:25:05,279 --> 00:25:10,240
scaler also from kubernetes

00:25:08,640 --> 00:25:12,320
and the way it works is that it scales

00:25:10,240 --> 00:25:14,080
parts vertically by scale by changing

00:25:12,320 --> 00:25:16,400
the request and limits so unlike

00:25:14,080 --> 00:25:17,919
our control plane autoscaler it doesn't

00:25:16,400 --> 00:25:19,840
change instance types

00:25:17,919 --> 00:25:22,080
but it changes how much request limits

00:25:19,840 --> 00:25:24,320
that you define on the parts

00:25:22,080 --> 00:25:26,480
um it scales only based on cpu and

00:25:24,320 --> 00:25:27,039
memory so there's no custom metrics like

00:25:26,480 --> 00:25:30,559
the

00:25:27,039 --> 00:25:31,279
hpa but this is also usually what you

00:25:30,559 --> 00:25:34,880
want

00:25:31,279 --> 00:25:36,640
cpu and memory and it's useful for

00:25:34,880 --> 00:25:38,720
components that scale vertically with

00:25:36,640 --> 00:25:40,000
the size of the cluster so example is

00:25:38,720 --> 00:25:43,520
like prometheus

00:25:40,000 --> 00:25:45,440
you want to have more execute more

00:25:43,520 --> 00:25:47,919
you want to collect more more metrics

00:25:45,440 --> 00:25:49,520
and maybe execute more more queries

00:25:47,919 --> 00:25:51,200
depending on how many how big your

00:25:49,520 --> 00:25:52,320
cluster is and how many resources you

00:25:51,200 --> 00:25:54,880
have in the cluster

00:25:52,320 --> 00:25:56,480
and you cannot just scale prometheus

00:25:54,880 --> 00:25:58,400
horizontally you need to scale the

00:25:56,480 --> 00:26:01,440
individual instances

00:25:58,400 --> 00:26:03,760
to bigger sizes so vertically um

00:26:01,440 --> 00:26:05,279
another example is ingress controller so

00:26:03,760 --> 00:26:06,880
if you

00:26:05,279 --> 00:26:08,880
you have just a single controller

00:26:06,880 --> 00:26:11,279
running in your cluster and if you have

00:26:08,880 --> 00:26:13,360
more ingresses uh then you also

00:26:11,279 --> 00:26:15,120
need to scale it vertically because it

00:26:13,360 --> 00:26:17,279
needs to download more

00:26:15,120 --> 00:26:18,799
uh ingress resources and keep them in

00:26:17,279 --> 00:26:21,600
memory for some time

00:26:18,799 --> 00:26:23,200
um similar externally and this is

00:26:21,600 --> 00:26:24,559
another example where it also scales

00:26:23,200 --> 00:26:26,320
with the number of ingress

00:26:24,559 --> 00:26:28,480
or service resources and you have any

00:26:26,320 --> 00:26:29,200
cluster so the bigger the cluster

00:26:28,480 --> 00:26:30,960
usually the

00:26:29,200 --> 00:26:32,880
the bigger they need to scale and for

00:26:30,960 --> 00:26:33,840
this the the vertical part autoscaler is

00:26:32,880 --> 00:26:37,120
very helpful to

00:26:33,840 --> 00:26:40,080
to manage this and

00:26:37,120 --> 00:26:42,480
this is how it looks like so uh here's

00:26:40,080 --> 00:26:43,440
an example of a prometheus in one of our

00:26:42,480 --> 00:26:46,159
clusters

00:26:43,440 --> 00:26:48,000
and the orange line is the request limit

00:26:46,159 --> 00:26:50,000
so we always use the same request and

00:26:48,000 --> 00:26:50,559
limit for memories so we cannot overcome

00:26:50,000 --> 00:26:52,880
it

00:26:50,559 --> 00:26:54,559
um but it basically shows that at some

00:26:52,880 --> 00:26:56,159
point it scales a bit down and then it

00:26:54,559 --> 00:26:58,159
scales up again

00:26:56,159 --> 00:26:59,840
when it sees that the usage which is the

00:26:58,159 --> 00:27:02,799
blue and the purple

00:26:59,840 --> 00:27:03,200
goes up and down over time um and this

00:27:02,799 --> 00:27:05,600
is

00:27:03,200 --> 00:27:06,960
like how it should work and and how we

00:27:05,600 --> 00:27:09,200
can

00:27:06,960 --> 00:27:11,200
kind of also save cost so we don't have

00:27:09,200 --> 00:27:14,400
to run with a very high

00:27:11,200 --> 00:27:16,400
uh memory uh request for all of the

00:27:14,400 --> 00:27:20,399
clusters but it depends on the cluster

00:27:16,400 --> 00:27:21,919
size and also how much it's scaling

00:27:20,399 --> 00:27:24,240
of course there's also failure modes

00:27:21,919 --> 00:27:27,039
with the vpa it's also

00:27:24,240 --> 00:27:27,840
um i don't think it's that widely used

00:27:27,039 --> 00:27:30,480
yet

00:27:27,840 --> 00:27:32,080
and there's some things that are also a

00:27:30,480 --> 00:27:35,360
bit troubling for us

00:27:32,080 --> 00:27:35,840
um so one thing that we have seen a lot

00:27:35,360 --> 00:27:39,360
is that

00:27:35,840 --> 00:27:41,679
it it hasn't picked a certain

00:27:39,360 --> 00:27:43,600
amount of resources and then over time

00:27:41,679 --> 00:27:45,760
it attempts to

00:27:43,600 --> 00:27:47,520
scale this down when it sees that these

00:27:45,760 --> 00:27:49,440
resources are not being used

00:27:47,520 --> 00:27:51,679
so this is also what it's supposed to do

00:27:49,440 --> 00:27:52,880
but sometimes it scales too far down as

00:27:51,679 --> 00:27:54,799
this example

00:27:52,880 --> 00:27:57,520
but then it quickly tries to scale up

00:27:54,799 --> 00:28:01,200
again and pick a little bit higher value

00:27:57,520 --> 00:28:03,440
um yeah but in other cases

00:28:01,200 --> 00:28:05,679
it can also scale down too aggressively

00:28:03,440 --> 00:28:06,880
and then it can take longer so there's

00:28:05,679 --> 00:28:10,240
more steps in order

00:28:06,880 --> 00:28:12,240
for it to uh to recover and and scale to

00:28:10,240 --> 00:28:15,440
a high enough size that

00:28:12,240 --> 00:28:18,480
uh yeah that the application won't get

00:28:15,440 --> 00:28:21,760
out of memory killed

00:28:18,480 --> 00:28:23,520
so for this we also have a fork of the

00:28:21,760 --> 00:28:24,880
vertical powder scaler similar to the

00:28:23,520 --> 00:28:27,360
cluster autoscaler

00:28:24,880 --> 00:28:28,960
and in our fork we have worked a lot of

00:28:27,360 --> 00:28:32,320
improving the

00:28:28,960 --> 00:28:34,720
oil and kill handling so whenever a pot

00:28:32,320 --> 00:28:37,120
runs out of memory what we wanted to do

00:28:34,720 --> 00:28:39,279
is to recover as quickly as possible so

00:28:37,120 --> 00:28:42,159
we want the bpa to scale out

00:28:39,279 --> 00:28:42,559
the resources higher and higher and we

00:28:42,159 --> 00:28:44,559
want

00:28:42,559 --> 00:28:46,399
we made a bunch of changes to make it as

00:28:44,559 --> 00:28:47,120
quick as possible there's still work to

00:28:46,399 --> 00:28:48,799
do

00:28:47,120 --> 00:28:50,880
because there's still situations where

00:28:48,799 --> 00:28:53,279
it can where it can take too long to

00:28:50,880 --> 00:28:56,480
recover a promisius instance that

00:28:53,279 --> 00:28:58,159
that ran out of memory and we also did

00:28:56,480 --> 00:29:00,159
some other small improvements of

00:28:58,159 --> 00:29:01,200
reducing the memory uses of the vpa

00:29:00,159 --> 00:29:03,600
components

00:29:01,200 --> 00:29:04,640
and also which way it's actually uh

00:29:03,600 --> 00:29:06,960
upstreamed

00:29:04,640 --> 00:29:08,399
and then we also added timeouts and so

00:29:06,960 --> 00:29:10,880
on for the mission record

00:29:08,399 --> 00:29:14,240
book which is part of the vpa so just

00:29:10,880 --> 00:29:15,679
small changes we have a link for the

00:29:14,240 --> 00:29:18,720
thought for all the changes we made if

00:29:15,679 --> 00:29:18,720
you are interested in that

00:29:19,760 --> 00:29:23,600
and this is pretty much the talk i put

00:29:22,399 --> 00:29:26,640
here some links to the

00:29:23,600 --> 00:29:28,559
open source things that we have done

00:29:26,640 --> 00:29:30,240
which i talked about in the talk so we

00:29:28,559 --> 00:29:32,640
have the cube matrix adapter which

00:29:30,240 --> 00:29:33,279
is yeah it's an open source project and

00:29:32,640 --> 00:29:36,320
as you saw

00:29:33,279 --> 00:29:39,360
someone also contributed

00:29:36,320 --> 00:29:41,120
influx db queries for it and

00:29:39,360 --> 00:29:42,399
you can also contribute other things if

00:29:41,120 --> 00:29:44,399
you are interested

00:29:42,399 --> 00:29:46,399
and we have a link to the cluster

00:29:44,399 --> 00:29:48,240
autoscaler fork and to the vertical port

00:29:46,399 --> 00:29:50,320
autoscaler fork

00:29:48,240 --> 00:29:52,000
and then to the two kubernetes

00:29:50,320 --> 00:29:54,480
enhancements proposals

00:29:52,000 --> 00:29:56,480
around the hpa so the one that is

00:29:54,480 --> 00:29:59,520
already in 118 about

00:29:56,480 --> 00:30:01,120
scale up and down velocity and scaling

00:29:59,520 --> 00:30:03,679
behavior configuration

00:30:01,120 --> 00:30:05,440
and then one that is about the container

00:30:03,679 --> 00:30:07,520
resource auto scaling so

00:30:05,440 --> 00:30:14,240
handling contained are pots with

00:30:07,520 --> 00:30:14,240

YouTube URL: https://www.youtube.com/watch?v=XTUsVK9F_Ds


