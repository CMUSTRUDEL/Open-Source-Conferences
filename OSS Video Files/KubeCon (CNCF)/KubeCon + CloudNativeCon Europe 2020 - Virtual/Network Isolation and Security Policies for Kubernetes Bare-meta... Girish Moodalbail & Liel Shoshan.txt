Title: Network Isolation and Security Policies for Kubernetes Bare-meta... Girish Moodalbail & Liel Shoshan
Publication date: 2020-08-28
Playlist: KubeCon + CloudNativeCon Europe 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming events: EnvoyCon Virtual on October 15 and KubeCon + CloudNativeCon North America 2020 Virtual from November 17-20. Learn more at https://kubecon.io. The conferences feature presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.  

Network Isolation and Security Policies for Kubernetes Bare-metal Nodes - Girish Moodalbail, NVIDIA & Liel Shoshan, Mellanox 

Running Kubernetes at scale in a multi-tenant Cloud requires strong network isolation and flexible stateful security policy enforcement for the bare-metal nodes used for both the tenant K8s clusters as well as in the Cloud control plane. Such isolation and security needs to be implemented in a way that consumes as little host resources as possible, while being immune to potentially malicious host root user. Additionally, the preferred implementation needs to be compatible with a high-performance (offloaded) K8s CNI. This presentation provides an overview of such an implementation for Software Defined (SDN) K8s node networking, based on Open Virtual Network (OVN) and Open vSwitch (OVS)) and offloaded to “bump-in-the-wire” Smart NICs.

https://sched.co/Zetj
Captions: 
	00:00:00,160 --> 00:00:03,439
hello everyone my name is girish moral

00:00:01,839 --> 00:00:07,440
bell my role at

00:00:03,439 --> 00:00:09,360
nvidia is to design develop and manage

00:00:07,440 --> 00:00:10,800
sdn networking for gp accelerated

00:00:09,360 --> 00:00:11,759
compute the computer itself can be

00:00:10,800 --> 00:00:14,480
virtual machines

00:00:11,759 --> 00:00:16,720
bare metal servers kubernetes pods and

00:00:14,480 --> 00:00:16,960
each of them having nvidia gpu networks

00:00:16,720 --> 00:00:19,600
and

00:00:16,960 --> 00:00:21,279
gpu cards in them i'm also upstream

00:00:19,600 --> 00:00:23,039
contributor and maintainer for oven

00:00:21,279 --> 00:00:25,519
kubernetes cni project

00:00:23,039 --> 00:00:26,800
i deal with the ovs in uh in my

00:00:25,519 --> 00:00:31,840
day-to-day work

00:00:26,800 --> 00:00:31,840
yeah you want to go introduce yourself

00:00:31,920 --> 00:00:36,239
hi my name is leo i'm a software

00:00:34,640 --> 00:00:39,120
architect at melanox

00:00:36,239 --> 00:00:41,120
now nvidia working on cloud

00:00:39,120 --> 00:00:44,640
accelerations

00:00:41,120 --> 00:00:47,920
kubernetes integration and

00:00:44,640 --> 00:00:49,520
switching offloads okay great

00:00:47,920 --> 00:00:50,960
so we're going to talk about how to

00:00:49,520 --> 00:00:53,440
secure bare metal nodes

00:00:50,960 --> 00:00:55,760
um that form the kubernetes cluster

00:00:53,440 --> 00:00:58,399
using oven logical constructs

00:00:55,760 --> 00:01:00,000
and bumping the wire smart nick and in

00:00:58,399 --> 00:01:02,800
our case it's going to be melanox

00:01:00,000 --> 00:01:04,239
bluefield network adapter so this is

00:01:02,800 --> 00:01:07,520
going to be the

00:01:04,239 --> 00:01:08,799
flow of our talk today um we're going to

00:01:07,520 --> 00:01:10,799
kind of

00:01:08,799 --> 00:01:12,720
explain as to why deployers are running

00:01:10,799 --> 00:01:15,920
kubernetes cluster on vms

00:01:12,720 --> 00:01:17,360
why they should be running the cluster

00:01:15,920 --> 00:01:20,720
on bare metal nodes

00:01:17,360 --> 00:01:22,799
we'll have a quick overview of oven and

00:01:20,720 --> 00:01:23,920
we'll see how one control plane runs on

00:01:22,799 --> 00:01:26,400
the smart nic

00:01:23,920 --> 00:01:27,439
how the data path for one gets offloaded

00:01:26,400 --> 00:01:30,320
to the hardware

00:01:27,439 --> 00:01:33,360
and we'll also kind of look at the other

00:01:30,320 --> 00:01:36,400
smart nic advantages

00:01:33,360 --> 00:01:38,320
so deployers have been um

00:01:36,400 --> 00:01:40,400
building kubernetes customizing vms as

00:01:38,320 --> 00:01:41,920
nodes and in this model

00:01:40,400 --> 00:01:43,280
they actually take an application

00:01:41,920 --> 00:01:44,079
containerize it they run the

00:01:43,280 --> 00:01:45,920
containerized

00:01:44,079 --> 00:01:47,680
application inside a part the part

00:01:45,920 --> 00:01:49,920
itself is running inside a vm

00:01:47,680 --> 00:01:51,200
and the vm itself is running on the bare

00:01:49,920 --> 00:01:54,399
metal box

00:01:51,200 --> 00:01:56,880
um so there is several layer of uh

00:01:54,399 --> 00:01:59,200
fencing if you will and and the whole

00:01:56,880 --> 00:02:02,479
reason here is uh they want to

00:01:59,200 --> 00:02:05,040
protect the data center from uh from

00:02:02,479 --> 00:02:06,159
application which could which can be

00:02:05,040 --> 00:02:07,840
compromised

00:02:06,159 --> 00:02:09,280
so the reason they do this fencing is

00:02:07,840 --> 00:02:10,640
because they're not confident about the

00:02:09,280 --> 00:02:12,640
container security

00:02:10,640 --> 00:02:14,080
and the hope is that if an attacker

00:02:12,640 --> 00:02:16,319
breaks out of the container

00:02:14,080 --> 00:02:17,440
they're still trapped inside the vm and

00:02:16,319 --> 00:02:19,040
then they don't act

00:02:17,440 --> 00:02:20,640
they don't have access to the bare metal

00:02:19,040 --> 00:02:23,840
node and therefore

00:02:20,640 --> 00:02:24,560
they cannot cause any harm to the data

00:02:23,840 --> 00:02:28,319
centers

00:02:24,560 --> 00:02:30,400
itself however um this comes with a huge

00:02:28,319 --> 00:02:32,319
performance cost for the application

00:02:30,400 --> 00:02:34,640
a lot of the hardware resources like the

00:02:32,319 --> 00:02:36,080
neck the gpu has to be virtualized

00:02:34,640 --> 00:02:38,480
and this virtualized thing has to be

00:02:36,080 --> 00:02:39,120
passed down to the vm and the vms the

00:02:38,480 --> 00:02:41,280
next

00:02:39,120 --> 00:02:43,760
pass down to the container there's also

00:02:41,280 --> 00:02:46,720
an hypervisor uh hold it

00:02:43,760 --> 00:02:47,280
for the application performance so the

00:02:46,720 --> 00:02:49,280
thing is

00:02:47,280 --> 00:02:50,800
uh there's a trade-off the trade-off

00:02:49,280 --> 00:02:53,120
between

00:02:50,800 --> 00:02:54,720
performance and security and violence

00:02:53,120 --> 00:02:58,319
and isolation

00:02:54,720 --> 00:03:01,360
so the deployers kind of tend to err

00:02:58,319 --> 00:03:03,120
on the side of security and they don't

00:03:01,360 --> 00:03:04,640
kind of focus more on the performance

00:03:03,120 --> 00:03:07,840
because security is uh

00:03:04,640 --> 00:03:11,440
of paramount importance

00:03:07,840 --> 00:03:13,440
so that need not however be the case um

00:03:11,440 --> 00:03:15,840
with a help from bumping the wire

00:03:13,440 --> 00:03:18,480
spartnic and

00:03:15,840 --> 00:03:20,000
distributed sdn control plane it's

00:03:18,480 --> 00:03:22,080
possible that we can get

00:03:20,000 --> 00:03:23,840
both performance as well as security and

00:03:22,080 --> 00:03:26,480
isolation

00:03:23,840 --> 00:03:27,519
and this kind of moves the zone the

00:03:26,480 --> 00:03:29,920
trust zone

00:03:27,519 --> 00:03:31,760
from the hypervisor onto the spartanic

00:03:29,920 --> 00:03:32,080
hardware and we're going to explain how

00:03:31,760 --> 00:03:33,680
this

00:03:32,080 --> 00:03:36,000
bump in the wire sparknet and the

00:03:33,680 --> 00:03:39,760
distributed sdn control plane

00:03:36,000 --> 00:03:42,640
enables uh achieving both performance

00:03:39,760 --> 00:03:44,159
at the at uh and not at the same time

00:03:42,640 --> 00:03:47,840
losing security in isolation for the

00:03:44,159 --> 00:03:47,840
bare metal nodes

00:03:48,879 --> 00:03:52,319
so um so where we are going with this is

00:03:51,200 --> 00:03:54,560
that um

00:03:52,319 --> 00:03:56,319
so with the smart nick and bom and the

00:03:54,560 --> 00:03:57,120
distributed control plane it is possible

00:03:56,319 --> 00:04:00,000
to

00:03:57,120 --> 00:04:01,200
push all of the axles and the firewall

00:04:00,000 --> 00:04:04,400
rules if you will

00:04:01,200 --> 00:04:06,000
uh to the smart mic itself um

00:04:04,400 --> 00:04:07,680
the parts running on the bare metal

00:04:06,000 --> 00:04:08,720
they're totally unaware of this bump in

00:04:07,680 --> 00:04:11,120
the wire

00:04:08,720 --> 00:04:11,920
all the kubernetes network policies uh

00:04:11,120 --> 00:04:15,280
for the pod

00:04:11,920 --> 00:04:19,199
are implemented on the spartnic

00:04:15,280 --> 00:04:22,720
and then the accols for the host itself

00:04:19,199 --> 00:04:24,639
is implemented on the spartanic so if uh

00:04:22,720 --> 00:04:25,840
if an attacker kind of jumps out of the

00:04:24,639 --> 00:04:28,479
pod and is now

00:04:25,840 --> 00:04:29,840
on the bare metal host they cannot flush

00:04:28,479 --> 00:04:32,720
ip table rules

00:04:29,840 --> 00:04:34,240
or the nat rules or any host based

00:04:32,720 --> 00:04:35,759
firewall rules because all of the

00:04:34,240 --> 00:04:39,040
firewall rules are

00:04:35,759 --> 00:04:40,960
on the smart neck which is acts as a

00:04:39,040 --> 00:04:44,080
bump in the wire thing

00:04:40,960 --> 00:04:45,120
so in that way uh you you can secure the

00:04:44,080 --> 00:04:48,320
bare metal uh

00:04:45,120 --> 00:04:50,400
nodes so oven

00:04:48,320 --> 00:04:51,440
the distributed sdn control plane and

00:04:50,400 --> 00:04:54,720
the smart

00:04:51,440 --> 00:04:56,960
kind of join hands here together to kind

00:04:54,720 --> 00:04:59,759
of provide the bare metal uh network

00:04:56,960 --> 00:05:02,880
security and isolation

00:04:59,759 --> 00:05:04,560
so one is open source uh project um it's

00:05:02,880 --> 00:05:06,240
a distributed sdn control plane

00:05:04,560 --> 00:05:08,560
uh that provides network virtualization

00:05:06,240 --> 00:05:11,360
solution it's developed by the open

00:05:08,560 --> 00:05:12,800
vswitch community the github url here

00:05:11,360 --> 00:05:14,960
is where you can find the repo for the

00:05:12,800 --> 00:05:17,039
project um

00:05:14,960 --> 00:05:19,120
the thing with avon is that the whole

00:05:17,039 --> 00:05:22,080
data path for r1 can be offloaded

00:05:19,120 --> 00:05:24,160
with the smellinox nick so obviously

00:05:22,080 --> 00:05:27,120
with the offloads you get the reduce

00:05:24,160 --> 00:05:28,240
utilization and the cpu can be used for

00:05:27,120 --> 00:05:30,639
applications

00:05:28,240 --> 00:05:31,680
and and in addition we also get

00:05:30,639 --> 00:05:36,000
increased throughput

00:05:31,680 --> 00:05:38,240
um one can be better explained um

00:05:36,000 --> 00:05:39,520
with the exam you know with an example

00:05:38,240 --> 00:05:42,240
and a diagram and that's what we're

00:05:39,520 --> 00:05:42,240
going to do next

00:05:43,440 --> 00:05:47,840
so in this diagram you have um the two

00:05:46,240 --> 00:05:48,800
things right you have a logical topology

00:05:47,840 --> 00:05:50,720
to your left

00:05:48,800 --> 00:05:52,479
and then you have and we have realized

00:05:50,720 --> 00:05:53,759
that logical topology on the two bare

00:05:52,479 --> 00:05:56,960
metal nodes

00:05:53,759 --> 00:05:58,960
um so on the logical topology

00:05:56,960 --> 00:06:00,160
it's a very simple logical topology it's

00:05:58,960 --> 00:06:02,560
a logical router

00:06:00,160 --> 00:06:05,280
with two logical switches connected to

00:06:02,560 --> 00:06:05,840
it and each logical switch has a logical

00:06:05,280 --> 00:06:10,080
port

00:06:05,840 --> 00:06:10,639
attached to it so logical router is a l3

00:06:10,080 --> 00:06:14,400
entity

00:06:10,639 --> 00:06:18,160
it provides routing ecmp nat

00:06:14,400 --> 00:06:19,440
load balancing policy based routing

00:06:18,160 --> 00:06:21,520
logical switcher on the other hand

00:06:19,440 --> 00:06:23,520
provides uh support for icos

00:06:21,520 --> 00:06:25,440
support for thcp it also provides

00:06:23,520 --> 00:06:28,639
support for load balancing

00:06:25,440 --> 00:06:30,720
and dns the logical port captures the

00:06:28,639 --> 00:06:33,199
mac address and ip address

00:06:30,720 --> 00:06:34,240
uh it captures it it's where we apply

00:06:33,199 --> 00:06:37,039
axles

00:06:34,240 --> 00:06:38,160
and then also it provides anti-spoofing

00:06:37,039 --> 00:06:41,199
support

00:06:38,160 --> 00:06:43,360
um so the thing is the logical port

00:06:41,199 --> 00:06:44,400
maps to a kubernetes pod network

00:06:43,360 --> 00:06:47,680
interface

00:06:44,400 --> 00:06:49,360
so the logical port is an api to control

00:06:47,680 --> 00:06:52,960
the parts interface

00:06:49,360 --> 00:06:55,599
attributes like so you kind of apply uh

00:06:52,960 --> 00:06:56,000
policies on the logical port it kind of

00:06:55,599 --> 00:06:59,120
uh

00:06:56,000 --> 00:07:02,160
reflects on the the on the physical

00:06:59,120 --> 00:07:06,400
on the pod interface on the

00:07:02,160 --> 00:07:09,039
on the pod so um

00:07:06,400 --> 00:07:09,440
um so on the right side what we have a

00:07:09,039 --> 00:07:12,319
two

00:07:09,440 --> 00:07:14,080
bare metal notes um on these notes you

00:07:12,319 --> 00:07:15,440
have a obs integration bridge called

00:07:14,080 --> 00:07:18,240
brnd

00:07:15,440 --> 00:07:20,080
this integration bridge implements the

00:07:18,240 --> 00:07:22,479
whole logical pipeline

00:07:20,080 --> 00:07:23,599
the whole oven logical topology using

00:07:22,479 --> 00:07:26,960
your obs

00:07:23,599 --> 00:07:29,759
pipeline using open flow tables and

00:07:26,960 --> 00:07:30,479
flows in those tables so then on this

00:07:29,759 --> 00:07:32,880
particular

00:07:30,479 --> 00:07:33,759
uh in this particular diagram here on

00:07:32,880 --> 00:07:35,919
node one

00:07:33,759 --> 00:07:37,280
a pod zero got scheduled on node two

00:07:35,919 --> 00:07:38,720
part one got scheduled

00:07:37,280 --> 00:07:40,639
port zero is connected to the

00:07:38,720 --> 00:07:44,160
integration bridge through a

00:07:40,639 --> 00:07:46,240
ovs internal port p0 and part 1 is

00:07:44,160 --> 00:07:47,280
connected to the obvious integration

00:07:46,240 --> 00:07:50,319
bridge to the

00:07:47,280 --> 00:07:50,960
internal port p1 so what we do here is

00:07:50,319 --> 00:07:53,919
that

00:07:50,960 --> 00:07:54,960
we associate this physical port with a

00:07:53,919 --> 00:07:57,120
logical port

00:07:54,960 --> 00:07:58,720
and that process is called port binding

00:07:57,120 --> 00:07:59,360
and this is one of the important aspects

00:07:58,720 --> 00:08:02,400
of

00:07:59,360 --> 00:08:05,440
oven what this port binding does is

00:08:02,400 --> 00:08:08,800
it tells you the physical location of a

00:08:05,440 --> 00:08:12,319
port for a given logical port so

00:08:08,800 --> 00:08:16,400
with that physical location known now

00:08:12,319 --> 00:08:18,319
the the oven control plane knows

00:08:16,400 --> 00:08:20,000
where to send the packet to for a given

00:08:18,319 --> 00:08:22,479
part in this example when a

00:08:20,000 --> 00:08:25,120
when pod zero wants to talk to part one

00:08:22,479 --> 00:08:27,840
when the packet arrives on the br end

00:08:25,120 --> 00:08:28,560
and this br implements the oven logical

00:08:27,840 --> 00:08:30,879
topology

00:08:28,560 --> 00:08:32,959
through open flow tables it already

00:08:30,879 --> 00:08:33,680
knows that to send the packet to part

00:08:32,959 --> 00:08:36,080
one

00:08:33,680 --> 00:08:37,680
it has to just send it to the node two's

00:08:36,080 --> 00:08:39,919
uh vtep ip address

00:08:37,680 --> 00:08:42,560
and then using open flow flows it just

00:08:39,919 --> 00:08:45,760
forwards the packet to that next hop

00:08:42,560 --> 00:08:49,519
so the whole logical topology is

00:08:45,760 --> 00:08:52,640
realized uh on this integration bridge

00:08:49,519 --> 00:08:56,000
um and then the the physical

00:08:52,640 --> 00:08:57,600
or the port binding thing enables that

00:08:56,000 --> 00:08:59,760
end to end connectivity between the

00:08:57,600 --> 00:09:02,720
between the ports

00:08:59,760 --> 00:09:04,399
so aman has a collection of daemons that

00:09:02,720 --> 00:09:05,920
work together to push this logical

00:09:04,399 --> 00:09:07,920
topology information

00:09:05,920 --> 00:09:09,279
and the physical location of this ports

00:09:07,920 --> 00:09:11,440
to each node

00:09:09,279 --> 00:09:13,760
and and then each node's integration

00:09:11,440 --> 00:09:16,800
bridge

00:09:13,760 --> 00:09:18,320
so in this slide we basically this slide

00:09:16,800 --> 00:09:20,480
basically talks about the

00:09:18,320 --> 00:09:22,399
oven communities project one kubernetes

00:09:20,480 --> 00:09:25,279
is an open source project

00:09:22,399 --> 00:09:26,000
contributed or worked on by the oven

00:09:25,279 --> 00:09:29,839
community

00:09:26,000 --> 00:09:32,080
the github url for the same uh he's here

00:09:29,839 --> 00:09:35,839
as shown in the slide here uh it

00:09:32,080 --> 00:09:38,480
implements container networking spec

00:09:35,839 --> 00:09:39,200
um it basically it's a layered

00:09:38,480 --> 00:09:41,440
architecture

00:09:39,200 --> 00:09:42,720
at the higher layer you have one

00:09:41,440 --> 00:09:44,720
kubernetes cni

00:09:42,720 --> 00:09:46,800
and which in turn depends on the r1 and

00:09:44,720 --> 00:09:48,880
i1 in turn depends on obs

00:09:46,800 --> 00:09:50,240
so the cna itself is implemented as a

00:09:48,880 --> 00:09:52,399
client server

00:09:50,240 --> 00:09:53,839
architecture it's basically acquitted is

00:09:52,399 --> 00:09:57,600
watcher and it has such

00:09:53,839 --> 00:09:59,920
several set of parts um it kind of

00:09:57,600 --> 00:10:00,880
takes the kubernetes resources and then

00:09:59,920 --> 00:10:02,959
it maps

00:10:00,880 --> 00:10:05,120
into open resources for example

00:10:02,959 --> 00:10:06,240
kubernetes network policy is mapped into

00:10:05,120 --> 00:10:08,560
our accounts

00:10:06,240 --> 00:10:10,000
communities cluster services is mapped

00:10:08,560 --> 00:10:12,480
into one load balancer

00:10:10,000 --> 00:10:13,839
kubernetes parts is mapped into uh

00:10:12,480 --> 00:10:15,760
logical switch ports

00:10:13,839 --> 00:10:16,959
and kubernetes node is mapped into

00:10:15,760 --> 00:10:19,680
logical switches

00:10:16,959 --> 00:10:20,480
so it watches on these cubities api

00:10:19,680 --> 00:10:23,360
resources

00:10:20,480 --> 00:10:24,320
as and when these api objects appear it

00:10:23,360 --> 00:10:28,640
kind of translates

00:10:24,320 --> 00:10:32,079
us into oven resources

00:10:28,640 --> 00:10:34,640
so the oven control plane um

00:10:32,079 --> 00:10:35,360
kind of uh acts on those oven resources

00:10:34,640 --> 00:10:38,160
that create

00:10:35,360 --> 00:10:39,120
that that was created by the one cni and

00:10:38,160 --> 00:10:41,920
then converts

00:10:39,120 --> 00:10:43,760
that into our logical flows and these

00:10:41,920 --> 00:10:46,880
logical force are in turn

00:10:43,760 --> 00:10:49,279
translated into ovs open flow rules on

00:10:46,880 --> 00:10:50,720
each of the uh node that forms the

00:10:49,279 --> 00:10:53,680
qubits cluster

00:10:50,720 --> 00:10:54,240
so on the diagram to the right you see

00:10:53,680 --> 00:10:56,160
that

00:10:54,240 --> 00:10:58,079
you kind of explain all the components

00:10:56,160 --> 00:11:01,519
running at various layers

00:10:58,079 --> 00:11:03,200
or on the components of various layers

00:11:01,519 --> 00:11:04,560
running on various nodes in the

00:11:03,200 --> 00:11:06,959
kubernetes cluster

00:11:04,560 --> 00:11:08,399
so the whole point here is that to show

00:11:06,959 --> 00:11:10,959
um

00:11:08,399 --> 00:11:11,920
that on some of the one control plane

00:11:10,959 --> 00:11:14,160
components they run

00:11:11,920 --> 00:11:15,680
on the nodes and when we move to the

00:11:14,160 --> 00:11:17,519
smart nic model

00:11:15,680 --> 00:11:20,640
these control plane components will end

00:11:17,519 --> 00:11:23,279
up running on the smart nic itself

00:11:20,640 --> 00:11:24,800
so in here you can see that on the host

00:11:23,279 --> 00:11:26,880
we had ovs bridge

00:11:24,800 --> 00:11:28,880
and the amount controller which kind of

00:11:26,880 --> 00:11:30,959
was it

00:11:28,880 --> 00:11:33,120
adding open flow flows into the obvious

00:11:30,959 --> 00:11:35,200
bridge uh and the one cni come

00:11:33,120 --> 00:11:37,360
components itself running on the host

00:11:35,200 --> 00:11:38,399
but when we move to the smartening model

00:11:37,360 --> 00:11:40,480
uh the

00:11:38,399 --> 00:11:42,240
the entire sdn control plane the

00:11:40,480 --> 00:11:42,959
distributed ascent control plane starts

00:11:42,240 --> 00:11:45,920
running

00:11:42,959 --> 00:11:46,959
on the sr on on the smart nick itself

00:11:45,920 --> 00:11:49,120
the smart nik

00:11:46,959 --> 00:11:50,639
comes with the arm core it has its own

00:11:49,120 --> 00:11:53,040
os running

00:11:50,639 --> 00:11:54,560
on the encore and then all of these

00:11:53,040 --> 00:11:57,440
control plane components

00:11:54,560 --> 00:11:57,920
move to that arm core obviously this

00:11:57,440 --> 00:11:59,279
kind of

00:11:57,920 --> 00:12:02,800
provides increased performance and

00:11:59,279 --> 00:12:02,800
isolation for the bare metal nodes

00:12:03,760 --> 00:12:09,920
so this is uh this diagram kind of

00:12:07,279 --> 00:12:11,120
dives deep into how things look like on

00:12:09,920 --> 00:12:13,360
the smart mic

00:12:11,120 --> 00:12:14,959
so on the smart nic itself we have one

00:12:13,360 --> 00:12:15,360
control plane components running which

00:12:14,959 --> 00:12:17,760
is our

00:12:15,360 --> 00:12:19,440
controller and we have our obvious

00:12:17,760 --> 00:12:21,360
components running which is obvious we

00:12:19,440 --> 00:12:23,440
switch t and obstb server

00:12:21,360 --> 00:12:24,959
and on the one cni side we have an agent

00:12:23,440 --> 00:12:28,160
running which acts as a

00:12:24,959 --> 00:12:28,880
kubernetes watcher uh trying to watch on

00:12:28,160 --> 00:12:31,360
certain

00:12:28,880 --> 00:12:33,040
kubernetes resources and then acting on

00:12:31,360 --> 00:12:36,240
those resources uh

00:12:33,040 --> 00:12:38,320
lifecycle the amount controller takes

00:12:36,240 --> 00:12:40,160
the logical flows from the oven and then

00:12:38,320 --> 00:12:41,519
translates into the open flow tables and

00:12:40,160 --> 00:12:43,839
open flow flows

00:12:41,519 --> 00:12:45,200
um and then kind of creates the data

00:12:43,839 --> 00:12:48,160
path for the pawn

00:12:45,200 --> 00:12:49,120
on the bare metal host itself we have

00:12:48,160 --> 00:12:51,040
multus to

00:12:49,120 --> 00:12:53,120
orchestrate multiple interfaces into the

00:12:51,040 --> 00:12:56,000
pod and we have a very

00:12:53,120 --> 00:12:57,279
basic cni that takes the next available

00:12:56,000 --> 00:12:59,839
vf from the pf

00:12:57,279 --> 00:13:01,120
and then provides it into the pod itself

00:12:59,839 --> 00:13:03,680
for every vf

00:13:01,120 --> 00:13:05,360
in the host we have a corresponding

00:13:03,680 --> 00:13:07,600
representer

00:13:05,360 --> 00:13:09,839
um we have a corresponding control

00:13:07,600 --> 00:13:11,600
entity on the blue field itself

00:13:09,839 --> 00:13:14,399
we call it as a representer it

00:13:11,600 --> 00:13:16,240
represents the vf on the host

00:13:14,399 --> 00:13:17,839
and we add these representers to the

00:13:16,240 --> 00:13:21,519
integration bridge

00:13:17,839 --> 00:13:23,600
and this representer maps

00:13:21,519 --> 00:13:24,560
to the logical switch port in the oven

00:13:23,600 --> 00:13:28,800
topology

00:13:24,560 --> 00:13:32,399
so this is the port binding aspect of

00:13:28,800 --> 00:13:35,440
the oven functionality so

00:13:32,399 --> 00:13:37,040
when a representative04 gets added to

00:13:35,440 --> 00:13:40,160
the pr end we

00:13:37,040 --> 00:13:41,839
we kind of annotate represented 0 4

00:13:40,160 --> 00:13:43,360
to tell that it belongs to certain

00:13:41,839 --> 00:13:45,199
logical port so now

00:13:43,360 --> 00:13:46,959
anything anytime we change the logical

00:13:45,199 --> 00:13:49,279
ports attributes

00:13:46,959 --> 00:13:50,880
the representers attributes change

00:13:49,279 --> 00:13:54,480
through open flow flows

00:13:50,880 --> 00:13:56,480
and therefore um the vf2 that is

00:13:54,480 --> 00:13:59,360
attached to representer04

00:13:56,480 --> 00:14:00,079
it will see that the changes all of

00:13:59,360 --> 00:14:03,680
these things

00:14:00,079 --> 00:14:06,079
are being done on the next hop

00:14:03,680 --> 00:14:07,199
you know next stop it acts like a next

00:14:06,079 --> 00:14:09,279
stop switch

00:14:07,199 --> 00:14:10,320
it's like a bump in the wire so all the

00:14:09,279 --> 00:14:12,720
attributes of

00:14:10,320 --> 00:14:14,079
the logical port gets reflected on this

00:14:12,720 --> 00:14:18,480
representer and then

00:14:14,079 --> 00:14:20,320
it gets reflected on the vf itself um

00:14:18,480 --> 00:14:22,880
so that's pretty much what this slide

00:14:20,320 --> 00:14:22,880
talks to

00:14:22,959 --> 00:14:29,600
so the this this slide basically

00:14:26,720 --> 00:14:31,440
captures how we can use our apples to

00:14:29,600 --> 00:14:34,959
kind of

00:14:31,440 --> 00:14:38,000
add axles at a different priorities

00:14:34,959 --> 00:14:40,480
and therefore override the axles that

00:14:38,000 --> 00:14:41,600
the tenants themselves configure so in

00:14:40,480 --> 00:14:44,720
here um

00:14:41,600 --> 00:14:46,160
avon eccles priority range from zero all

00:14:44,720 --> 00:14:47,199
the way up to thirty two thousand seven

00:14:46,160 --> 00:14:49,199
sixty seven

00:14:47,199 --> 00:14:51,120
so higher the priority of the accol

00:14:49,199 --> 00:14:53,680
higher will be its precedence

00:14:51,120 --> 00:14:55,600
so in the design here we have all the

00:14:53,680 --> 00:14:57,519
tenant accols being represented

00:14:55,600 --> 00:14:58,880
uh with priorities less than ten

00:14:57,519 --> 00:15:00,720
thousand however

00:14:58,880 --> 00:15:02,800
all the cloud provider cycles are at the

00:15:00,720 --> 00:15:03,440
priority more than ten thousand so

00:15:02,800 --> 00:15:06,639
therefore

00:15:03,440 --> 00:15:09,279
their uh precedence will be higher

00:15:06,639 --> 00:15:10,000
as compared to the tenant tackles so

00:15:09,279 --> 00:15:12,959
whenever

00:15:10,000 --> 00:15:14,240
uh the kubernetes network policies are

00:15:12,959 --> 00:15:16,399
translated to

00:15:14,240 --> 00:15:17,440
urban apples they get uh priorities in

00:15:16,399 --> 00:15:19,120
this range

00:15:17,440 --> 00:15:21,279
the cloud providers they can come in on

00:15:19,120 --> 00:15:24,720
the side and then

00:15:21,279 --> 00:15:28,160
add their axles at a higher level

00:15:24,720 --> 00:15:30,160
so that you know we

00:15:28,160 --> 00:15:32,240
they can kind of make sure that the pink

00:15:30,160 --> 00:15:34,399
tenant here does not

00:15:32,240 --> 00:15:35,600
uh somehow talk into the blue tenant

00:15:34,399 --> 00:15:37,279
because

00:15:35,600 --> 00:15:39,519
the cloud provider have added ankles to

00:15:37,279 --> 00:15:40,560
make sure that such a communication is

00:15:39,519 --> 00:15:43,040
not possible

00:15:40,560 --> 00:15:44,480
in fact the cloud projects can add

00:15:43,040 --> 00:15:47,360
accols to their own

00:15:44,480 --> 00:15:48,240
infrastructure services to make sure

00:15:47,360 --> 00:15:51,040
that

00:15:48,240 --> 00:15:52,959
the the tenants host or the tenants part

00:15:51,040 --> 00:15:54,720
do not

00:15:52,959 --> 00:15:56,320
talk to infrastructure services that

00:15:54,720 --> 00:15:59,040
they're not supposed to talk to

00:15:56,320 --> 00:15:59,839
so in this way using our priority

00:15:59,040 --> 00:16:03,120
accounts

00:15:59,839 --> 00:16:04,160
we kind of make sure we provide that

00:16:03,120 --> 00:16:06,000
isolation

00:16:04,160 --> 00:16:07,759
of between the bare metal host and the

00:16:06,000 --> 00:16:08,880
data center services and the data center

00:16:07,759 --> 00:16:11,920
itself

00:16:08,880 --> 00:16:14,079
and all of this echoes the flows is all

00:16:11,920 --> 00:16:16,639
offloaded onto the integration bridge

00:16:14,079 --> 00:16:20,399
and leah is going to talk to it uh

00:16:16,639 --> 00:16:23,360
in the next set of slides so then

00:16:20,399 --> 00:16:24,079
so in here what we have done is um we

00:16:23,360 --> 00:16:27,279
have two

00:16:24,079 --> 00:16:30,880
different bare metal servers um uh

00:16:27,279 --> 00:16:33,360
and the bare metal itself is uh

00:16:30,880 --> 00:16:34,000
part of ammon logical topology and then

00:16:33,360 --> 00:16:35,920
the

00:16:34,000 --> 00:16:37,600
kubernetes cluster also is part of one

00:16:35,920 --> 00:16:41,120
logical topology so

00:16:37,600 --> 00:16:43,600
uh the paths are kind of connected

00:16:41,120 --> 00:16:45,759
together within our logical topology

00:16:43,600 --> 00:16:48,000
orchestrated by the urban cni

00:16:45,759 --> 00:16:49,839
on the other hand the bare metal servers

00:16:48,000 --> 00:16:50,560
themselves are part of our logical

00:16:49,839 --> 00:16:53,040
topology

00:16:50,560 --> 00:16:54,800
and then the interconnection between

00:16:53,040 --> 00:16:58,240
them is provided by our

00:16:54,800 --> 00:16:58,959
so in here uh the parts if they want to

00:16:58,240 --> 00:17:01,040
talk

00:16:58,959 --> 00:17:03,440
east-west traffic between the parts is

00:17:01,040 --> 00:17:06,880
represented through the green color

00:17:03,440 --> 00:17:08,480
it's a overlay traffic um and

00:17:06,880 --> 00:17:10,240
the network policies for this overlay

00:17:08,480 --> 00:17:12,480
traffic is defined uh

00:17:10,240 --> 00:17:13,520
by the communities network policies on

00:17:12,480 --> 00:17:15,199
the other hand

00:17:13,520 --> 00:17:16,720
the e-space traffic between the bare

00:17:15,199 --> 00:17:19,120
metal server itself

00:17:16,720 --> 00:17:20,000
is achieved through the oven logical

00:17:19,120 --> 00:17:23,280
topology

00:17:20,000 --> 00:17:25,760
and uh um using an overlay

00:17:23,280 --> 00:17:26,319
and and then we'll have one apples

00:17:25,760 --> 00:17:27,919
define

00:17:26,319 --> 00:17:30,799
which bare metal hose can talk to each

00:17:27,919 --> 00:17:32,240
other and which bare metal hose cannot

00:17:30,799 --> 00:17:34,000
talk to each other this is defined by

00:17:32,240 --> 00:17:37,280
the one actuals

00:17:34,000 --> 00:17:39,039
see both the the green overlay and the

00:17:37,280 --> 00:17:41,039
orange overlay

00:17:39,039 --> 00:17:43,120
they are transported over the blue

00:17:41,039 --> 00:17:45,440
underlay this forms the underlay

00:17:43,120 --> 00:17:46,960
where the blue fields are all in blue

00:17:45,440 --> 00:17:50,559
fields in the data center

00:17:46,960 --> 00:17:53,600
they're all interconnected together and

00:17:50,559 --> 00:17:56,559
they form the underlay over which

00:17:53,600 --> 00:17:57,760
the geneva tunnel traffic is transported

00:17:56,559 --> 00:18:00,320
between the pods

00:17:57,760 --> 00:18:01,039
and between the bare metal server itself

00:18:00,320 --> 00:18:03,760
um

00:18:01,039 --> 00:18:05,200
so the red underlay network here is for

00:18:03,760 --> 00:18:06,960
access to the internet

00:18:05,200 --> 00:18:08,720
if these spots want to access to the

00:18:06,960 --> 00:18:10,640
internet they kind of exit

00:18:08,720 --> 00:18:12,320
out of the tunnel or they exit out of

00:18:10,640 --> 00:18:15,600
the oven logical topology

00:18:12,320 --> 00:18:15,919
through a oven gateway router where they

00:18:15,600 --> 00:18:18,400
get

00:18:15,919 --> 00:18:19,200
source knighted into the blue fit ip and

00:18:18,400 --> 00:18:21,919
then now

00:18:19,200 --> 00:18:23,039
they're on the red underlay and heading

00:18:21,919 --> 00:18:26,160
towards the internet

00:18:23,039 --> 00:18:27,919
the same thing is true for the bare

00:18:26,160 --> 00:18:28,720
metal server host trying to access

00:18:27,919 --> 00:18:30,320
internet

00:18:28,720 --> 00:18:32,160
so they exit out of their logical

00:18:30,320 --> 00:18:34,880
topology um

00:18:32,160 --> 00:18:35,520
through a gateway router uh oven gateway

00:18:34,880 --> 00:18:38,960
router

00:18:35,520 --> 00:18:41,120
where the source netting happens

00:18:38,960 --> 00:18:43,120
the logical topology ip is so snatched

00:18:41,120 --> 00:18:45,200
to the bluefield

00:18:43,120 --> 00:18:46,799
underlay ip and then the packet head

00:18:45,200 --> 00:18:49,840
towards the internet

00:18:46,799 --> 00:18:52,960
um so that's this diagram so

00:18:49,840 --> 00:18:57,520
here what we're capturing is how

00:18:52,960 --> 00:19:00,480
one can provide a multi-tenancy model

00:18:57,520 --> 00:19:01,520
uh in a dc using oven distributed

00:19:00,480 --> 00:19:04,640
control plane

00:19:01,520 --> 00:19:05,679
and a smart nic in it so in here we have

00:19:04,640 --> 00:19:08,320
two tenants

00:19:05,679 --> 00:19:09,120
um a blue tenant and a ping tenant the

00:19:08,320 --> 00:19:12,480
blue tenant

00:19:09,120 --> 00:19:14,080
um uh has a bunch of bare metal servers

00:19:12,480 --> 00:19:14,640
which are all interconnected together

00:19:14,080 --> 00:19:17,600
using

00:19:14,640 --> 00:19:18,559
our logical topology on top of this bare

00:19:17,600 --> 00:19:21,280
metal servers

00:19:18,559 --> 00:19:24,000
they're running kubernetes cluster uh

00:19:21,280 --> 00:19:27,120
which is orchestrated by the urban cni

00:19:24,000 --> 00:19:28,960
and the the cluster

00:19:27,120 --> 00:19:30,880
the qubit is cluster itself is it is in

00:19:28,960 --> 00:19:33,679
its own logical topology

00:19:30,880 --> 00:19:35,280
um similarly here for the ping tenant

00:19:33,679 --> 00:19:36,400
they have a bunch of bare metal servers

00:19:35,280 --> 00:19:38,480
in the dc

00:19:36,400 --> 00:19:39,840
that was assigned to them and the bare

00:19:38,480 --> 00:19:41,840
metal servers themselves are

00:19:39,840 --> 00:19:44,320
connected together using a logical

00:19:41,840 --> 00:19:47,200
switch and a logical router

00:19:44,320 --> 00:19:47,760
um and then on those bare metal servers

00:19:47,200 --> 00:19:49,919
they have

00:19:47,760 --> 00:19:51,440
their own kubernetes cluster running

00:19:49,919 --> 00:19:52,400
which is again orchestrated throughout

00:19:51,440 --> 00:19:54,240
cni

00:19:52,400 --> 00:19:55,840
as you can see they all use this

00:19:54,240 --> 00:19:58,640
overlapping ips

00:19:55,840 --> 00:20:01,200
and um and this is all made possible

00:19:58,640 --> 00:20:04,320
because of the oven virtual topology

00:20:01,200 --> 00:20:05,520
and they all exit out to the underlay

00:20:04,320 --> 00:20:06,400
and then finally they reach out to the

00:20:05,520 --> 00:20:09,120
internet

00:20:06,400 --> 00:20:10,080
um the cloud providers can apply their

00:20:09,120 --> 00:20:14,880
axles

00:20:10,080 --> 00:20:16,159
on the bare metal um on the smart neck

00:20:14,880 --> 00:20:18,000
to kind of make sure that the blue

00:20:16,159 --> 00:20:19,840
tenant and the ping tenant are

00:20:18,000 --> 00:20:21,600
isolated from each other and they're

00:20:19,840 --> 00:20:24,559
also isolated from

00:20:21,600 --> 00:20:27,200
the cloud provider services itself and

00:20:24,559 --> 00:20:29,919
the next slide kind of talks to that

00:20:27,200 --> 00:20:31,600
so in here we show a dc networking with

00:20:29,919 --> 00:20:34,640
uh

00:20:31,600 --> 00:20:37,679
thor's and l2 within the tors and

00:20:34,640 --> 00:20:40,240
everything across the tarsus l3

00:20:37,679 --> 00:20:41,120
so here you see a blue tenon has a bunch

00:20:40,240 --> 00:20:42,559
of

00:20:41,120 --> 00:20:44,559
bare metal nodes assigned to it there's

00:20:42,559 --> 00:20:46,559
a four bare metal node assigned to it

00:20:44,559 --> 00:20:48,080
and each of the brain metal node has a

00:20:46,559 --> 00:20:51,600
blue field adapter

00:20:48,080 --> 00:20:54,880
uh assigned to it um and then these

00:20:51,600 --> 00:20:55,440
four bare metals they form a vtep

00:20:54,880 --> 00:20:57,360
network

00:20:55,440 --> 00:20:58,559
amongst themselves there's a concept of

00:20:57,360 --> 00:21:02,000
trusted zones

00:20:58,559 --> 00:21:04,159
so you create uh this island of bare

00:21:02,000 --> 00:21:07,840
metal servers with bluefield on them

00:21:04,159 --> 00:21:11,919
and these uh the the geneva traffic

00:21:07,840 --> 00:21:14,799
um or the oven traffic only kind of uh

00:21:11,919 --> 00:21:16,000
happens or occurs between these set of

00:21:14,799 --> 00:21:18,159
bare metal nodes

00:21:16,000 --> 00:21:19,760
um on top of this bare metal node is

00:21:18,159 --> 00:21:22,640
where we build the

00:21:19,760 --> 00:21:23,840
kubernetes cluster and then they kind of

00:21:22,640 --> 00:21:27,600
live within this island

00:21:23,840 --> 00:21:30,159
similarly the pink tenant also has

00:21:27,600 --> 00:21:30,960
its own island with a set of bare metal

00:21:30,159 --> 00:21:32,720
servers with

00:21:30,960 --> 00:21:34,000
each of them having a blue field

00:21:32,720 --> 00:21:36,640
associated with it

00:21:34,000 --> 00:21:37,919
and they have their own logical topology

00:21:36,640 --> 00:21:40,960
and

00:21:37,919 --> 00:21:41,919
the tunnel is formed only between these

00:21:40,960 --> 00:21:44,159
four

00:21:41,919 --> 00:21:46,559
pink bare metals so in that way there is

00:21:44,159 --> 00:21:48,880
no interconnect at all between

00:21:46,559 --> 00:21:50,400
the blue island and the pink island and

00:21:48,880 --> 00:21:52,400
then in the same way

00:21:50,400 --> 00:21:54,640
the cloud provider services the cloud

00:21:52,400 --> 00:21:57,440
provider can provide their services

00:21:54,640 --> 00:21:59,440
either on the physical network directly

00:21:57,440 --> 00:22:00,720
or they can provide it on the oven logic

00:21:59,440 --> 00:22:03,840
network itself

00:22:00,720 --> 00:22:05,360
um so in that way a blue tenant can

00:22:03,840 --> 00:22:05,840
access some of the services provided by

00:22:05,360 --> 00:22:08,159
the

00:22:05,840 --> 00:22:10,159
provider provided by the cloud provider

00:22:08,159 --> 00:22:12,640
um

00:22:10,159 --> 00:22:14,559
through the logical topology or directly

00:22:12,640 --> 00:22:15,440
they exit out to the underlay and on the

00:22:14,559 --> 00:22:17,520
underlay

00:22:15,440 --> 00:22:21,120
they kind of connect to the services

00:22:17,520 --> 00:22:26,400
running on the physical network itself

00:22:21,120 --> 00:22:30,080
uh next slide uh leal can you proceed

00:22:26,400 --> 00:22:30,720
sure thanks greg so uh in addition to

00:22:30,080 --> 00:22:33,440
the

00:22:30,720 --> 00:22:34,080
security enhancement and isolation this

00:22:33,440 --> 00:22:38,159
solution

00:22:34,080 --> 00:22:41,200
also enjoys uh datapass hardware offload

00:22:38,159 --> 00:22:42,240
this is done using the asub square

00:22:41,200 --> 00:22:44,320
framework

00:22:42,240 --> 00:22:46,159
by melanox accelerated switching and

00:22:44,320 --> 00:22:48,080
packet processing

00:22:46,159 --> 00:22:49,440
it's a software and hardware integrated

00:22:48,080 --> 00:22:52,159
framework

00:22:49,440 --> 00:22:54,159
which utilizes melanox nix to accelerate

00:22:52,159 --> 00:22:56,480
and offload the data plan

00:22:54,159 --> 00:22:57,760
while maintaining the control plane in

00:22:56,480 --> 00:23:00,159
software

00:22:57,760 --> 00:23:01,440
so it minimizes the needed changes in

00:23:00,159 --> 00:23:05,440
kubernetes cni

00:23:01,440 --> 00:23:09,360
or other sdns the solution supports

00:23:05,440 --> 00:23:11,679
different configuration srv and virteo

00:23:09,360 --> 00:23:14,159
and it's available both upstream and

00:23:11,679 --> 00:23:14,159
inbox

00:23:20,640 --> 00:23:25,039
it consists of three capabilities

00:23:23,280 --> 00:23:27,840
classification offload

00:23:25,039 --> 00:23:28,400
action of load and data bus offload

00:23:27,840 --> 00:23:31,440
which are

00:23:28,400 --> 00:23:37,840
required for uh

00:23:31,440 --> 00:23:37,840
switching for switch data pass offload

00:23:40,960 --> 00:23:48,840
so as girish explained we worked with

00:23:44,400 --> 00:23:51,919
oven control plan which is based on ovs

00:23:48,840 --> 00:23:53,039
switch ovr switch is the most popular

00:23:51,919 --> 00:23:54,960
virtual switch

00:23:53,039 --> 00:23:56,480
is a flow based one with many

00:23:54,960 --> 00:23:59,200
capabilities

00:23:56,480 --> 00:23:59,919
uh among which are geneve connection

00:23:59,200 --> 00:24:03,360
tracking

00:23:59,919 --> 00:24:05,520
mirroring and more and

00:24:03,360 --> 00:24:07,279
uh there are many multiple control

00:24:05,520 --> 00:24:11,440
planes that are built on top of it

00:24:07,279 --> 00:24:15,840
uh ovn is an example for that

00:24:11,440 --> 00:24:18,880
it has a ova it has a user space

00:24:15,840 --> 00:24:20,880
module and a credit space module and in

00:24:18,880 --> 00:24:24,880
the regular configuration

00:24:20,880 --> 00:24:33,840
each pod is assigned a vth

00:24:24,880 --> 00:24:33,840
a pervert interface

00:24:34,640 --> 00:24:40,720
so in the traditional model

00:24:37,679 --> 00:24:41,919
the first packet of a flow will go up to

00:24:40,720 --> 00:24:45,360
the user space

00:24:41,919 --> 00:24:48,080
there it will be processed and sent

00:24:45,360 --> 00:24:49,200
and then suitable rule will be inserted

00:24:48,080 --> 00:24:51,760
to the kernel

00:24:49,200 --> 00:24:53,440
so the next packets of the same flow

00:24:51,760 --> 00:24:56,960
will go directly through the kernel

00:24:53,440 --> 00:25:00,559
without going through the user space

00:24:56,960 --> 00:25:03,279
with connectex hardware offload the suit

00:25:00,559 --> 00:25:04,240
this rule would not only be inserted to

00:25:03,279 --> 00:25:07,440
the kernel but

00:25:04,240 --> 00:25:10,640
also to the hardware to the connected

00:25:07,440 --> 00:25:14,000
embedded switch in this case

00:25:10,640 --> 00:25:16,960
the next packets of a flow will not

00:25:14,000 --> 00:25:18,000
go to the cpu at all and will just be

00:25:16,960 --> 00:25:21,360
processed

00:25:18,000 --> 00:25:21,360
in the how hardware

00:25:23,650 --> 00:25:29,919
[Music]

00:25:26,480 --> 00:25:30,400
so this way we keep the first packet

00:25:29,919 --> 00:25:34,240
miss

00:25:30,400 --> 00:25:36,880
architecture by using an additional

00:25:34,240 --> 00:25:38,000
hardware layer which is the embedded

00:25:36,880 --> 00:25:41,520
switch

00:25:38,000 --> 00:25:46,640
which is located on the neck each pod

00:25:41,520 --> 00:25:49,279
is assigned an srv virtual function

00:25:46,640 --> 00:25:50,480
and while the controller or the user

00:25:49,279 --> 00:25:52,880
inserts the

00:25:50,480 --> 00:25:53,760
the policies by using obvious open flow

00:25:52,880 --> 00:25:55,760
rules

00:25:53,760 --> 00:25:57,919
these rules are offloaded to the

00:25:55,760 --> 00:26:01,600
hardware in a transparent way

00:25:57,919 --> 00:26:01,600
using linux dc rules

00:26:03,919 --> 00:26:06,960
packets are being processed by the nic

00:26:06,320 --> 00:26:10,400
and

00:26:06,960 --> 00:26:11,840
therefore there is no host cpu

00:26:10,400 --> 00:26:14,960
consumption

00:26:11,840 --> 00:26:18,159
and we increase throughput and

00:26:14,960 --> 00:26:18,159
decrease latency

00:26:19,120 --> 00:26:22,000
drastically

00:26:22,960 --> 00:26:27,440
the ace of square has many benefits it

00:26:25,200 --> 00:26:29,600
achieves uncompromised performance it

00:26:27,440 --> 00:26:32,640
saves cpu

00:26:29,600 --> 00:26:35,200
using the smart nic you can achieve also

00:26:32,640 --> 00:26:37,679
full isolation and it facilitates the

00:26:35,200 --> 00:26:41,200
solution for both bare metal

00:26:37,679 --> 00:26:41,200
and virtualized clouds

00:26:50,480 --> 00:26:55,679
so if for virtualized cloud

00:26:53,679 --> 00:26:57,279
that both network virtualization and

00:26:55,679 --> 00:26:59,279
storage virtualization

00:26:57,279 --> 00:27:02,159
were traditionally running in the

00:26:59,279 --> 00:27:02,159
hypervisor

00:27:02,880 --> 00:27:10,159
using uh connectex

00:27:06,159 --> 00:27:12,559
we can offload the network data plane

00:27:10,159 --> 00:27:14,320
by by leveraging the asub square

00:27:12,559 --> 00:27:16,480
framework

00:27:14,320 --> 00:27:19,120
when we use bluefield the smart

00:27:16,480 --> 00:27:22,399
technique as a bump in a wire

00:27:19,120 --> 00:27:24,720
also the control plane of the switch

00:27:22,399 --> 00:27:25,440
can run on the smartening on the arm

00:27:24,720 --> 00:27:27,520
cores

00:27:25,440 --> 00:27:30,320
as well as the storage virtualization

00:27:27,520 --> 00:27:33,440
and other security services

00:27:30,320 --> 00:27:35,840
this is also true for bare metal service

00:27:33,440 --> 00:27:35,840
servers

00:27:42,000 --> 00:27:48,320
so i'll summarize bare metal clouds

00:27:45,360 --> 00:27:51,100
raises the demand for network isolation

00:27:48,320 --> 00:27:52,240
since the host is no longer trusted

00:27:51,100 --> 00:27:54,559
[Music]

00:27:52,240 --> 00:27:56,960
network resolution can be achieved by

00:27:54,559 --> 00:28:00,399
using a dedicated hardware

00:27:56,960 --> 00:28:03,440
which is which which sits

00:28:00,399 --> 00:28:05,919
in front of the in part of the host

00:28:03,440 --> 00:28:07,360
we use bluefield smartnic as a bumper in

00:28:05,919 --> 00:28:10,080
the wire

00:28:07,360 --> 00:28:10,720
to run ovn control plane in a secured

00:28:10,080 --> 00:28:13,840
way

00:28:10,720 --> 00:28:16,640
we're running both the virtual switch

00:28:13,840 --> 00:28:18,960
and the utilities configuring it on the

00:28:16,640 --> 00:28:18,960
neck

00:28:20,399 --> 00:28:24,720
we also leveraged the s of square

00:28:22,480 --> 00:28:26,799
framework in order to achieve data pass

00:28:24,720 --> 00:28:29,840
hardware offload

00:28:26,799 --> 00:28:32,080
which resulted in bus drastic reduction

00:28:29,840 --> 00:28:35,360
in host cpu consumption

00:28:32,080 --> 00:28:38,080
and enhanced performance

00:28:35,360 --> 00:28:38,080

YouTube URL: https://www.youtube.com/watch?v=AjEgSUN1aYw


