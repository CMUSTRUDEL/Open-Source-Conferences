Title: Are You Sure About Your Mesh Performance? - Otto van der Schaaf, Sunku Ranganath & Mrittika Ganguli
Publication date: 2021-05-05
Playlist: ServiceMeshCon EU 2021
Description: 
	Donâ€™t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

Are You Sure About Your Mesh Performance? Details matter! - Otto van der Schaaf, Red Hat, Sunku Ranganath & Mrittika Ganguli, Intel Corporation

Service Mesh performance characterization has been an elusive aspect of understanding the impact of mesh in production. While few studies have been published, mesh performance generally is highly influenced by runtime environment, hardware settings, test tool & methodology used to benchmark. Based on various tests performed on Envoy, this presentation aims to shed light on: -Performance characterization methodology of Envoy for deterministic throughput & latency -Gaps in benchmark tools - disconnect between L2 to L7 optimizations for load generation & features WIP to address these gaps, e.g. with Nighthawk -Common pitfalls in measurements -Usual culprits for lack of consistency in benchmarks -Impact of scaling Envoy on latency & hardware utilization -Share benchmark results & common bottlenecks using Envoy sandboxes -Customizing Envoy for telco grade performance with hardware offloads
Captions: 
	00:00:00,480 --> 00:00:04,400
hi everyone i'm sun kuranganath i'm a

00:00:02,480 --> 00:00:06,960
network software engineer in intel

00:00:04,400 --> 00:00:08,559
working on enabling service mesh for 5g

00:00:06,960 --> 00:00:10,800
and edge environments

00:00:08,559 --> 00:00:12,799
topic today i'm presenting title are you

00:00:10,800 --> 00:00:15,679
sure about your mesh performance

00:00:12,799 --> 00:00:16,560
details matter along with myrthika and

00:00:15,679 --> 00:00:19,600
otto

00:00:16,560 --> 00:00:21,680
here's the legal disclaimer high level

00:00:19,600 --> 00:00:23,920
agenda we start off looking at some of

00:00:21,680 --> 00:00:26,640
the important aspects to keep in mind

00:00:23,920 --> 00:00:27,359
while measuring your mesh deployment we

00:00:26,640 --> 00:00:29,599
then share

00:00:27,359 --> 00:00:31,199
the some of the results and observations

00:00:29,599 --> 00:00:32,960
uh with the performance studies we have

00:00:31,199 --> 00:00:34,000
done on bare metal and virtualized

00:00:32,960 --> 00:00:35,680
environment

00:00:34,000 --> 00:00:38,079
otto would then talk about some of the

00:00:35,680 --> 00:00:40,399
benchmarking pitfalls around tooling

00:00:38,079 --> 00:00:41,440
and test environment and method i would

00:00:40,399 --> 00:00:44,399
end the talk

00:00:41,440 --> 00:00:45,760
sharing some of the ongoing work so if

00:00:44,399 --> 00:00:47,520
you're someone looking to understand

00:00:45,760 --> 00:00:49,200
your mesh deployment

00:00:47,520 --> 00:00:51,120
you would sort of setting up your

00:00:49,200 --> 00:00:51,760
cluster your service mesh application

00:00:51,120 --> 00:00:54,480
under test

00:00:51,760 --> 00:00:56,399
pick a benchmarking tool configure your

00:00:54,480 --> 00:00:59,520
transactions per second

00:00:56,399 --> 00:01:02,160
or protocol of interest test duration

00:00:59,520 --> 00:01:03,520
connections then run the test and

00:01:02,160 --> 00:01:05,040
capture the results

00:01:03,520 --> 00:01:07,200
however if you closely look at this

00:01:05,040 --> 00:01:09,760
picture what's missing

00:01:07,200 --> 00:01:12,400
right so turns out quite a lot for the

00:01:09,760 --> 00:01:14,960
purposes of this talk let's assume

00:01:12,400 --> 00:01:16,080
east-west traffic is trafficked between

00:01:14,960 --> 00:01:18,560
microservices

00:01:16,080 --> 00:01:19,360
either in the same pod or across the

00:01:18,560 --> 00:01:21,119
pods

00:01:19,360 --> 00:01:23,280
either on the same virtual machine or

00:01:21,119 --> 00:01:25,840
across different virtual machines

00:01:23,280 --> 00:01:27,360
or across different hosts and not so

00:01:25,840 --> 00:01:29,759
traffic is something

00:01:27,360 --> 00:01:30,880
traffic coming in and out of a specific

00:01:29,759 --> 00:01:33,439
host

00:01:30,880 --> 00:01:34,400
so depending on how you configure the

00:01:33,439 --> 00:01:37,439
number of hops

00:01:34,400 --> 00:01:39,920
between your source and destination

00:01:37,439 --> 00:01:41,520
that your mesh performance has a huge

00:01:39,920 --> 00:01:43,759
impact for example your

00:01:41,520 --> 00:01:45,280
load balancer your api gateways ingress

00:01:43,759 --> 00:01:47,600
controllers firewall

00:01:45,280 --> 00:01:48,320
all of these add a good amount of

00:01:47,600 --> 00:01:50,240
latency

00:01:48,320 --> 00:01:52,000
uh impacting overall performance of your

00:01:50,240 --> 00:01:53,680
mesh not just that

00:01:52,000 --> 00:01:55,439
that's your hardware settings related to

00:01:53,680 --> 00:01:58,000
your bias settings

00:01:55,439 --> 00:01:59,119
power management new awareness type

00:01:58,000 --> 00:02:01,439
accelerators

00:01:59,119 --> 00:02:03,360
or your operating system settings type

00:02:01,439 --> 00:02:04,960
of networking stack you would use kernel

00:02:03,360 --> 00:02:06,719
versus user plane stack

00:02:04,960 --> 00:02:09,119
all the type of tuning you would do

00:02:06,719 --> 00:02:11,440
across your l2 and three layer

00:02:09,119 --> 00:02:12,560
right and your load generator settings

00:02:11,440 --> 00:02:14,800
all of these have

00:02:12,560 --> 00:02:16,000
a good amount of impact on your mesh

00:02:14,800 --> 00:02:18,879
performance

00:02:16,000 --> 00:02:19,760
so for the ultimately what's important

00:02:18,879 --> 00:02:22,640
is to have a

00:02:19,760 --> 00:02:23,840
method to have a consistent results

00:02:22,640 --> 00:02:26,400
across

00:02:23,840 --> 00:02:28,720
a repeated number of uh test cycles

00:02:26,400 --> 00:02:31,920
right so that's the crucial part

00:02:28,720 --> 00:02:33,200
so we started off looking at performance

00:02:31,920 --> 00:02:35,280
of envoy

00:02:33,200 --> 00:02:37,440
leveraging the unvoiced friend proxy

00:02:35,280 --> 00:02:38,319
sandbox available part of the online

00:02:37,440 --> 00:02:39,920
source code

00:02:38,319 --> 00:02:43,040
well if you're someone new looking to

00:02:39,920 --> 00:02:46,080
understand service mesh with envoy

00:02:43,040 --> 00:02:47,280
front pro the sandboxes provided are

00:02:46,080 --> 00:02:50,000
really great way to

00:02:47,280 --> 00:02:52,080
understand which is where i was uh just

00:02:50,000 --> 00:02:54,879
a few short months ago

00:02:52,080 --> 00:02:56,560
so in this example front proxy uh

00:02:54,879 --> 00:02:59,360
sandbox provides a simple

00:02:56,560 --> 00:02:59,760
enviro acting as a load balancer at the

00:02:59,360 --> 00:03:02,000
front

00:02:59,760 --> 00:03:03,200
uh service saying two services as i was

00:03:02,000 --> 00:03:05,920
one service two

00:03:03,200 --> 00:03:06,400
each of them having a simple flask app

00:03:05,920 --> 00:03:09,200
and

00:03:06,400 --> 00:03:10,800
on y has a cycle process in a in a

00:03:09,200 --> 00:03:12,959
docker container

00:03:10,800 --> 00:03:14,720
so type of test we do is scale the

00:03:12,959 --> 00:03:17,760
number of service one end points from

00:03:14,720 --> 00:03:18,000
either anywhere from one to 100 uh look

00:03:17,760 --> 00:03:20,879
at

00:03:18,000 --> 00:03:22,159
the amount of transactions resolved

00:03:20,879 --> 00:03:24,560
clear latency

00:03:22,159 --> 00:03:25,519
uh change the traffic generator settings

00:03:24,560 --> 00:03:28,720
so ultimately

00:03:25,519 --> 00:03:30,879
what we're trying to understand is

00:03:28,720 --> 00:03:32,319
performance impact with scaling the

00:03:30,879 --> 00:03:35,120
number of course

00:03:32,319 --> 00:03:37,120
or a number of connections uh along with

00:03:35,120 --> 00:03:40,480
uh scaling the number of flasks or

00:03:37,120 --> 00:03:41,680
online instances so to give you an idea

00:03:40,480 --> 00:03:44,080
of the results

00:03:41,680 --> 00:03:46,000
so if you look at here uh on the x-axis

00:03:44,080 --> 00:03:48,720
you have a number of on

00:03:46,000 --> 00:03:50,560
app or on y uh microservices from

00:03:48,720 --> 00:03:52,879
anywhere from two to hundred

00:03:50,560 --> 00:03:54,239
and y axis here uh one side you have the

00:03:52,879 --> 00:03:56,400
transactions result

00:03:54,239 --> 00:03:58,159
other side you have latency you could

00:03:56,400 --> 00:04:00,799
see from the bar graphs of

00:03:58,159 --> 00:04:01,680
with the one core versus two versus four

00:04:00,799 --> 00:04:03,599
course

00:04:01,680 --> 00:04:05,439
especially you have when you have a

00:04:03,599 --> 00:04:08,799
higher container count

00:04:05,439 --> 00:04:11,920
about hundred um at a thousand

00:04:08,799 --> 00:04:13,439
dps input i could see with four course

00:04:11,920 --> 00:04:15,200
all the thousands have been successfully

00:04:13,439 --> 00:04:18,639
resolved versus one core

00:04:15,200 --> 00:04:21,519
where not even reached to the 200 dps

00:04:18,639 --> 00:04:23,360
and your p99 latency has a huge impact

00:04:21,519 --> 00:04:25,120
of 4.5 times

00:04:23,360 --> 00:04:27,199
when you have four cores available and

00:04:25,120 --> 00:04:29,199
keeping it steady

00:04:27,199 --> 00:04:31,680
and when you scale it across the entire

00:04:29,199 --> 00:04:34,880
socket with 48 cores

00:04:31,680 --> 00:04:36,400
you could see a 10 000 tps input you

00:04:34,880 --> 00:04:39,759
could see it could achieve

00:04:36,400 --> 00:04:43,360
well over 7000 dps with the whole socket

00:04:39,759 --> 00:04:44,800
versus four cores which is about 200 in

00:04:43,360 --> 00:04:47,280
this case

00:04:44,800 --> 00:04:48,800
at 10 000 dps input and with 64

00:04:47,280 --> 00:04:51,840
connections

00:04:48,800 --> 00:04:53,440
versus your tail latencies uh where

00:04:51,840 --> 00:04:56,560
they've been study

00:04:53,440 --> 00:04:58,639
across the 10 000 dps input versus the

00:04:56,560 --> 00:04:59,360
four core case where your latency goes

00:04:58,639 --> 00:05:02,880
well over

00:04:59,360 --> 00:05:06,320
seconds one second uh if you look at the

00:05:02,880 --> 00:05:10,000
cpu utilization across the is four cores

00:05:06,320 --> 00:05:13,280
and when still cross 1000 tps input rate

00:05:10,000 --> 00:05:15,680
your cpu utilization uh for your flask

00:05:13,280 --> 00:05:19,039
cap and cycle proxy goes

00:05:15,680 --> 00:05:20,560
close to 100 right anything over 1000

00:05:19,039 --> 00:05:23,360
right so reaching 100

00:05:20,560 --> 00:05:24,000
which is which is a lot so essentially

00:05:23,360 --> 00:05:27,199
we found

00:05:24,000 --> 00:05:27,919
that number of cores have a significant

00:05:27,199 --> 00:05:30,880
impact

00:05:27,919 --> 00:05:32,960
on overall mesh performance uh isolated

00:05:30,880 --> 00:05:34,960
course and core pinning

00:05:32,960 --> 00:05:37,039
are necessarily helpful which is what

00:05:34,960 --> 00:05:39,840
some of the telco deployments do

00:05:37,039 --> 00:05:40,800
i isolate the cores and open the

00:05:39,840 --> 00:05:42,400
microservices

00:05:40,800 --> 00:05:44,240
right so it's not necessarily helpful

00:05:42,400 --> 00:05:47,199
and we found that we could do a

00:05:44,240 --> 00:05:48,400
quite a lot of optimizations to achieve

00:05:47,199 --> 00:05:51,120
better performance

00:05:48,400 --> 00:05:52,000
even compared to what's here so here's

00:05:51,120 --> 00:05:54,880
an example of

00:05:52,000 --> 00:05:56,800
a telco deployment where you would have

00:05:54,880 --> 00:06:00,000
a kubernetes in a virtual machine

00:05:56,800 --> 00:06:02,400
either in a

00:06:00,000 --> 00:06:03,919
same host different host right so in

00:06:02,400 --> 00:06:06,720
this example we did

00:06:03,919 --> 00:06:07,120
uh master uh communities master invert

00:06:06,720 --> 00:06:09,280
one

00:06:07,120 --> 00:06:10,800
vm and one host and kubernetes work on a

00:06:09,280 --> 00:06:13,520
different host

00:06:10,800 --> 00:06:14,639
and we look at calico and sto with their

00:06:13,520 --> 00:06:17,680
defaults

00:06:14,639 --> 00:06:19,360
um and on the worker host um for the

00:06:17,680 --> 00:06:21,759
data plane

00:06:19,360 --> 00:06:24,319
we'd use obs dpdk right so some of the

00:06:21,759 --> 00:06:25,680
top coated plywoods use either obs ppdk

00:06:24,319 --> 00:06:28,400
or sioe

00:06:25,680 --> 00:06:29,840
uh type of scenarios in this example we

00:06:28,400 --> 00:06:33,440
have two polema drivers

00:06:29,840 --> 00:06:36,240
um servicing the data plane traffic

00:06:33,440 --> 00:06:38,479
and the idea is to understand the impact

00:06:36,240 --> 00:06:41,600
of istio in this type of deployment

00:06:38,479 --> 00:06:44,080
right and ford io client would

00:06:41,600 --> 00:06:45,680
uh talk to nginx web server running in a

00:06:44,080 --> 00:06:47,759
pod along with

00:06:45,680 --> 00:06:48,800
on voice site called proxy i have two

00:06:47,759 --> 00:06:51,840
configurations here

00:06:48,800 --> 00:06:54,479
for ford io client running as a

00:06:51,840 --> 00:06:56,160
process outside of kubernetes cluster

00:06:54,479 --> 00:06:59,120
either within

00:06:56,160 --> 00:07:00,160
the kubernetes master vm or on the bare

00:06:59,120 --> 00:07:02,560
metal host

00:07:00,160 --> 00:07:03,199
and the idea is to understand the knots

00:07:02,560 --> 00:07:06,560
of traffic

00:07:03,199 --> 00:07:09,440
going in and out of this host

00:07:06,560 --> 00:07:10,400
so to give you a brief uh view of the

00:07:09,440 --> 00:07:12,960
results

00:07:10,400 --> 00:07:14,800
all right so for the vm to vm case where

00:07:12,960 --> 00:07:17,120
ford io and vm versus

00:07:14,800 --> 00:07:18,960
engine export another vm under two

00:07:17,120 --> 00:07:22,240
different hosts you could see

00:07:18,960 --> 00:07:22,960
adding istio adds about three times the

00:07:22,240 --> 00:07:26,080
latency

00:07:22,960 --> 00:07:29,199
across different tps rate and

00:07:26,080 --> 00:07:31,759
um at about 10 000 dps where without

00:07:29,199 --> 00:07:35,039
steel you could resolve 10 000 dps

00:07:31,759 --> 00:07:36,639
it still adds in about 46 performance

00:07:35,039 --> 00:07:40,319
degradation

00:07:36,639 --> 00:07:43,280
and with respect to the case where

00:07:40,319 --> 00:07:45,360
ford io is on the master host reaching

00:07:43,280 --> 00:07:48,319
the engine x bar in the vm

00:07:45,360 --> 00:07:50,319
istio adds about four times the latency

00:07:48,319 --> 00:07:52,879
and about 11 11

00:07:50,319 --> 00:07:53,680
performance degradation about 10 000 dps

00:07:52,879 --> 00:07:55,440
rate

00:07:53,680 --> 00:07:58,319
in hyper traffic you could see uh

00:07:55,440 --> 00:08:01,280
anywhere from 32 to 60

00:07:58,319 --> 00:08:02,639
depending on the case right um so

00:08:01,280 --> 00:08:04,560
essentially what we found

00:08:02,639 --> 00:08:05,680
is um there's a good amount of

00:08:04,560 --> 00:08:07,759
performance impact

00:08:05,680 --> 00:08:08,720
adding istio in a virtualized

00:08:07,759 --> 00:08:11,759
environment

00:08:08,720 --> 00:08:14,479
and we could see um the

00:08:11,759 --> 00:08:16,240
across the stack starting from cpus in

00:08:14,479 --> 00:08:18,639
this case we have 10

00:08:16,240 --> 00:08:19,440
and of course made available for vm

00:08:18,639 --> 00:08:22,160
right so

00:08:19,440 --> 00:08:23,280
a cpu number of course type of course

00:08:22,160 --> 00:08:26,400
calico config

00:08:23,280 --> 00:08:29,199
or respective mtu connection rate

00:08:26,400 --> 00:08:30,000
uh right or your on y config with

00:08:29,199 --> 00:08:31,919
respect to

00:08:30,000 --> 00:08:33,360
number of course versus number of online

00:08:31,919 --> 00:08:34,560
worker threats right so there are a lot

00:08:33,360 --> 00:08:36,240
of variables

00:08:34,560 --> 00:08:37,599
across the stack that we could look into

00:08:36,240 --> 00:08:41,680
or tune into

00:08:37,599 --> 00:08:43,360
a few observations so hardware tuning

00:08:41,680 --> 00:08:45,279
a lot of things could be done for

00:08:43,360 --> 00:08:47,279
example for our management policies

00:08:45,279 --> 00:08:48,720
uh simple thing to do is turn off all

00:08:47,279 --> 00:08:51,680
power management options

00:08:48,720 --> 00:08:52,800
however we found and depending on the

00:08:51,680 --> 00:08:53,360
peace states and c-state conflict

00:08:52,800 --> 00:08:55,839
conflict

00:08:53,360 --> 00:08:57,440
have a good impact on dail latencies

00:08:55,839 --> 00:09:00,320
enabling hyper threading

00:08:57,440 --> 00:09:01,519
improves performance core isolation

00:09:00,320 --> 00:09:04,000
isn't necessarily

00:09:01,519 --> 00:09:05,440
helpful for example some of the cases we

00:09:04,000 --> 00:09:07,760
described earlier

00:09:05,440 --> 00:09:08,959
with bare metal tests right impact of

00:09:07,760 --> 00:09:12,640
tuning a vm for new

00:09:08,959 --> 00:09:15,279
locality or pinning queue threads

00:09:12,640 --> 00:09:17,440
on on the isolated course have a as an

00:09:15,279 --> 00:09:20,640
impact on tail latencies

00:09:17,440 --> 00:09:23,279
you could do save cpu cycles

00:09:20,640 --> 00:09:25,360
when your cpu is idling or offload

00:09:23,279 --> 00:09:27,360
crypto operations for example leveraging

00:09:25,360 --> 00:09:29,680
intel quick assist technology

00:09:27,360 --> 00:09:30,880
or add vectorized code as another

00:09:29,680 --> 00:09:32,800
solution

00:09:30,880 --> 00:09:34,640
on the other side from a load generator

00:09:32,800 --> 00:09:37,440
perspective quite a few things too

00:09:34,640 --> 00:09:38,880
as it adds its own latency leveraging

00:09:37,440 --> 00:09:41,839
kernel networking stack

00:09:38,880 --> 00:09:42,320
right so the latency has a huge impact

00:09:41,839 --> 00:09:44,880
um

00:09:42,320 --> 00:09:45,760
depending on the backup algorithm used

00:09:44,880 --> 00:09:46,800
or

00:09:45,760 --> 00:09:48,880
and we find that some of the

00:09:46,800 --> 00:09:50,000
optimizations could be done as respite

00:09:48,880 --> 00:09:51,839
resources

00:09:50,000 --> 00:09:53,760
cpu or hardware tuning micro

00:09:51,839 --> 00:09:56,160
architectural analysis

00:09:53,760 --> 00:09:57,760
metrics paid fee based feedback loop

00:09:56,160 --> 00:10:00,720
which is how some of the

00:09:57,760 --> 00:10:01,279
hardware traffic generators do otto

00:10:00,720 --> 00:10:05,440
would

00:10:01,279 --> 00:10:09,519
touch upon some of these in his part

00:10:05,440 --> 00:10:13,440
also we found some of the l2 l3

00:10:09,519 --> 00:10:16,480
traffic generators leverage rfc 2544

00:10:13,440 --> 00:10:18,079
but for l7 benchmarking we haven't found

00:10:16,480 --> 00:10:19,600
loads in later leveraging in

00:10:18,079 --> 00:10:22,079
standardized way

00:10:19,600 --> 00:10:23,600
right so we could see overall um

00:10:22,079 --> 00:10:26,160
optimizations could be done

00:10:23,600 --> 00:10:26,880
from a load generator perspective across

00:10:26,160 --> 00:10:30,480
the stack

00:10:26,880 --> 00:10:31,440
etc in fact um we've been discussing a

00:10:30,480 --> 00:10:34,560
lot of this work

00:10:31,440 --> 00:10:37,040
as part of cncf service mesh

00:10:34,560 --> 00:10:38,240
working group which is part of a sig

00:10:37,040 --> 00:10:41,360
network

00:10:38,240 --> 00:10:41,839
and the the project we've been talking

00:10:41,360 --> 00:10:43,839
about

00:10:41,839 --> 00:10:44,880
a lot of these aspects is service mesh

00:10:43,839 --> 00:10:48,079
performance

00:10:44,880 --> 00:10:49,519
in fact this project has been submitted

00:10:48,079 --> 00:10:52,000
to cncf

00:10:49,519 --> 00:10:54,240
to be considered as a sandbox project

00:10:52,000 --> 00:10:55,040
essentially smb provides a standardized

00:10:54,240 --> 00:10:57,360
way of

00:10:55,040 --> 00:10:58,959
running these mesh performance

00:10:57,360 --> 00:11:02,000
measurements

00:10:58,959 --> 00:11:04,560
looking at the vendor neutral way of uh

00:11:02,000 --> 00:11:05,279
specifying mesh deployment patterns your

00:11:04,560 --> 00:11:07,760
environment

00:11:05,279 --> 00:11:08,720
infrastructure capturing a test results

00:11:07,760 --> 00:11:10,720
essentially in a

00:11:08,720 --> 00:11:11,920
automated way for example leveraging

00:11:10,720 --> 00:11:15,360
measuring

00:11:11,920 --> 00:11:17,680
right and so uh this is where uh

00:11:15,360 --> 00:11:19,760
in fact i met otto for the first time

00:11:17,680 --> 00:11:22,720
and we had a quite a few

00:11:19,760 --> 00:11:23,519
fruit fruitful conversations after that

00:11:22,720 --> 00:11:29,839
so with that

00:11:23,519 --> 00:11:29,839
uh pass it on to otto

00:11:32,640 --> 00:11:37,839
hello my name is otto van der schaff and

00:11:35,519 --> 00:11:41,040
i work as a principal envoy engineer at

00:11:37,839 --> 00:11:43,519
red hat with a focus on performance

00:11:41,040 --> 00:11:44,640
i am an active contributor to n5 proxy

00:11:43,519 --> 00:11:46,880
slash nighthawk

00:11:44,640 --> 00:11:48,079
a layer 7 performance characterization

00:11:46,880 --> 00:11:50,240
tool

00:11:48,079 --> 00:11:52,160
in this presentation i will share some

00:11:50,240 --> 00:11:54,079
of the pitfalls and observations i have

00:11:52,160 --> 00:11:57,839
run into while measuring performance of

00:11:54,079 --> 00:11:57,839
proxies and meshes

00:11:58,480 --> 00:12:02,240
to start out i will dive into low

00:12:00,399 --> 00:12:04,240
generator tooling for a bit

00:12:02,240 --> 00:12:05,519
and dig into some of the divergences

00:12:04,240 --> 00:12:07,440
between them

00:12:05,519 --> 00:12:09,279
to illustrate it is nice to visit the

00:12:07,440 --> 00:12:10,800
performance issue that was at some point

00:12:09,279 --> 00:12:13,200
reported in the open source

00:12:10,800 --> 00:12:16,240
envoy repository which compared nfoi

00:12:13,200 --> 00:12:19,600
proxy to aj proxy

00:12:16,240 --> 00:12:21,519
in that test wrk2 was used to compare

00:12:19,600 --> 00:12:23,760
latencies across the two proxies

00:12:21,519 --> 00:12:25,920
and envoy seemed to add two or three

00:12:23,760 --> 00:12:28,639
times as much latency

00:12:25,920 --> 00:12:30,320
at some point i figured that sanity

00:12:28,639 --> 00:12:32,880
checking these lit numbers was where i

00:12:30,320 --> 00:12:32,880
warranted

00:12:33,120 --> 00:12:37,920
so i set up a reproduction and next to

00:12:35,440 --> 00:12:40,000
wrk i used ford io and nighthawk to

00:12:37,920 --> 00:12:42,399
measure latencies

00:12:40,000 --> 00:12:44,560
while doing so wrk reproduced mean

00:12:42,399 --> 00:12:47,440
latencies in the right ballpark

00:12:44,560 --> 00:12:48,639
but fortio reported latencies about

00:12:47,440 --> 00:12:50,639
twice as high

00:12:48,639 --> 00:12:53,279
and nighthawk would in turn report

00:12:50,639 --> 00:12:56,399
latencies in order of magnitude lower

00:12:53,279 --> 00:12:58,079
they couldn't all possibly be right

00:12:56,399 --> 00:12:59,760
in the discussion that fallout we

00:12:58,079 --> 00:13:01,440
learned that the reported measurements

00:12:59,760 --> 00:13:02,720
had been obtained by executing on a

00:13:01,440 --> 00:13:05,040
virtual machine

00:13:02,720 --> 00:13:06,720
whereas my reproduction was on a tuned

00:13:05,040 --> 00:13:08,720
bare metal machine

00:13:06,720 --> 00:13:10,399
still that did not explain the

00:13:08,720 --> 00:13:13,360
divergence between nighthawk

00:13:10,399 --> 00:13:14,639
and the other tools to understand what

00:13:13,360 --> 00:13:16,480
happened there

00:13:14,639 --> 00:13:18,399
we need to dig into the subtleties

00:13:16,480 --> 00:13:21,839
around request release timings and

00:13:18,399 --> 00:13:21,839
connection reuse

00:13:23,440 --> 00:13:26,959
let's take a look at different

00:13:25,040 --> 00:13:27,360
strategies that a load generator could

00:13:26,959 --> 00:13:29,200
use

00:13:27,360 --> 00:13:31,519
when releasing requests in terms of

00:13:29,200 --> 00:13:33,920
timings and connections

00:13:31,519 --> 00:13:34,800
in the display diagram on the vertical

00:13:33,920 --> 00:13:36,480
axis

00:13:34,800 --> 00:13:37,839
four rows are representing four

00:13:36,480 --> 00:13:41,040
connections

00:13:37,839 --> 00:13:43,199
the x-axis represents time

00:13:41,040 --> 00:13:44,800
we're shooting for four queries per

00:13:43,199 --> 00:13:46,880
second

00:13:44,800 --> 00:13:48,880
as you can see this diagram shows a

00:13:46,880 --> 00:13:50,240
perfectly timed request release each

00:13:48,880 --> 00:13:52,079
quarter of a second

00:13:50,240 --> 00:13:54,720
balancing over all the connections that

00:13:52,079 --> 00:13:54,720
are available

00:13:55,360 --> 00:13:59,600
the same connection and queries per

00:13:57,120 --> 00:14:01,279
second parameterization can be expressed

00:13:59,600 --> 00:14:03,440
in alternative ways

00:14:01,279 --> 00:14:05,600
for example a load gen might depend on

00:14:03,440 --> 00:14:08,000
internal or external http libraries

00:14:05,600 --> 00:14:09,680
which will not prefetch connections

00:14:08,000 --> 00:14:11,760
this could in turn mean that only a

00:14:09,680 --> 00:14:14,560
single connection is actually involved

00:14:11,760 --> 00:14:17,120
where one might not easily think

00:14:14,560 --> 00:14:19,040
that four would have been involved the

00:14:17,120 --> 00:14:21,600
specified amount of connection

00:14:19,040 --> 00:14:24,240
serves as a maximum in this case but not

00:14:21,600 --> 00:14:24,240
as a minimum

00:14:24,800 --> 00:14:29,760
a load generator may also rely on a pool

00:14:27,120 --> 00:14:32,079
with prefetching capabilities

00:14:29,760 --> 00:14:33,680
when testing low latency replies four

00:14:32,079 --> 00:14:34,480
connections will be created in this

00:14:33,680 --> 00:14:36,320
scenario

00:14:34,480 --> 00:14:38,399
but only one will be used for sending

00:14:36,320 --> 00:14:40,160
requests

00:14:38,399 --> 00:14:42,079
connection pools tend to use the most

00:14:40,160 --> 00:14:44,560
recently used strategy when picking a

00:14:42,079 --> 00:14:46,320
free connection from the available ones

00:14:44,560 --> 00:14:48,800
depending on the specifics of what is

00:14:46,320 --> 00:14:50,079
being tested a single or a few large

00:14:48,800 --> 00:14:53,120
pools behaving like this

00:14:50,079 --> 00:14:54,880
may or may not be desirable

00:14:53,120 --> 00:14:57,040
when realistically trying to simulate

00:14:54,880 --> 00:14:59,040
browsers it is probably better to have

00:14:57,040 --> 00:15:00,079
lots of small pool instances that behave

00:14:59,040 --> 00:15:02,880
like this

00:15:00,079 --> 00:15:04,560
when simulating a downstream proxy one

00:15:02,880 --> 00:15:08,639
or only a large few

00:15:04,560 --> 00:15:08,639
of these pools may reflect reality

00:15:10,880 --> 00:15:15,199
so this diagram visualizes yet another

00:15:13,600 --> 00:15:18,560
way of releasing your request

00:15:15,199 --> 00:15:19,440
as observed in the wild timing request

00:15:18,560 --> 00:15:21,279
releases

00:15:19,440 --> 00:15:23,519
would occur in a bit of a synchronized

00:15:21,279 --> 00:15:25,040
fashion across connections

00:15:23,519 --> 00:15:26,880
note that this still reflects four

00:15:25,040 --> 00:15:30,079
queries per second but it may not be

00:15:26,880 --> 00:15:30,079
what someone has in mind

00:15:31,600 --> 00:15:35,920
now in the previous diagrams we had to

00:15:33,759 --> 00:15:37,600
request reply pairs fast enough to never

00:15:35,920 --> 00:15:40,399
have connections saturated

00:15:37,600 --> 00:15:42,639
when it is time to release a new request

00:15:40,399 --> 00:15:44,560
this diagram visualizes a situation

00:15:42,639 --> 00:15:47,120
where all connections are busy and it is

00:15:44,560 --> 00:15:48,800
time to issue a new request

00:15:47,120 --> 00:15:50,720
load generators diverge here and how

00:15:48,800 --> 00:15:52,639
they handle this

00:15:50,720 --> 00:15:54,639
closed loop load generators may just

00:15:52,639 --> 00:15:56,720
block and release when a connection

00:15:54,639 --> 00:15:59,040
frees up

00:15:56,720 --> 00:16:00,959
some track time being blocked others

00:15:59,040 --> 00:16:04,160
will try to use math to correct for

00:16:00,959 --> 00:16:06,320
missed requests in histogram output

00:16:04,160 --> 00:16:09,440
open loop generators may report

00:16:06,320 --> 00:16:11,519
connection and stream overflows

00:16:09,440 --> 00:16:12,800
regardless of closed or open loop

00:16:11,519 --> 00:16:16,000
methodology

00:16:12,800 --> 00:16:16,000
some may allow queuing

00:16:16,160 --> 00:16:19,279
what happens with the request release

00:16:17,920 --> 00:16:21,759
base in ferris two

00:16:19,279 --> 00:16:23,519
a lotion may or may not try to make up

00:16:21,759 --> 00:16:25,440
for missed request releases by

00:16:23,519 --> 00:16:28,959
increasing the pacing when possible

00:16:25,440 --> 00:16:28,959
depending on implementation

00:16:29,600 --> 00:16:33,120
here's an example of nighthawk warning

00:16:31,440 --> 00:16:34,160
about time spent being blocked on

00:16:33,120 --> 00:16:36,399
resources

00:16:34,160 --> 00:16:37,920
connections in this case when it ran in

00:16:36,399 --> 00:16:39,920
closed loop mode

00:16:37,920 --> 00:16:41,920
we advise to consider results as invalid

00:16:39,920 --> 00:16:43,839
when significant blocking is observed as

00:16:41,920 --> 00:16:46,639
latency histograms will be useless for

00:16:43,839 --> 00:16:48,639
most purposes

00:16:46,639 --> 00:16:50,880
the next slide shows that reporting pool

00:16:48,639 --> 00:16:52,800
overflows in open loop mode

00:16:50,880 --> 00:16:54,639
likewise when significant connection or

00:16:52,800 --> 00:16:56,560
stream overflows are detected

00:16:54,639 --> 00:16:58,399
those numbers can be checked as part of

00:16:56,560 --> 00:17:01,360
verifying test expectations through

00:16:58,399 --> 00:17:01,360
structured output

00:17:03,120 --> 00:17:06,400
so let's dig into two load generators

00:17:05,199 --> 00:17:08,959
for a bit

00:17:06,400 --> 00:17:10,559
um let me try to explain this plot for a

00:17:08,959 --> 00:17:13,280
bit

00:17:10,559 --> 00:17:14,959
on the vertical axis bottom to top we

00:17:13,280 --> 00:17:19,199
have fortio and wrk

00:17:14,959 --> 00:17:22,000
ramping up from 50 to 4000 qps

00:17:19,199 --> 00:17:24,400
this execution used a single connection

00:17:22,000 --> 00:17:26,720
and was assigned a single cpu

00:17:24,400 --> 00:17:27,439
we plot a series of p50 latency

00:17:26,720 --> 00:17:29,600
measurements

00:17:27,439 --> 00:17:32,000
on the horizontal axis to get a sense of

00:17:29,600 --> 00:17:34,640
the numbers and the spread

00:17:32,000 --> 00:17:35,840
what stood out here is that wrk

00:17:34,640 --> 00:17:37,679
generally reports an

00:17:35,840 --> 00:17:39,039
order of magnitude lower latencies

00:17:37,679 --> 00:17:40,720
compared to fortio

00:17:39,039 --> 00:17:44,160
and that its stability across test

00:17:40,720 --> 00:17:46,559
executions was much tighter

00:17:44,160 --> 00:17:48,559
the next slide is similar but uses 200

00:17:46,559 --> 00:17:51,039
connections and 4 cpus

00:17:48,559 --> 00:17:53,039
and it tracks p99.9 which is a little

00:17:51,039 --> 00:17:56,160
bit more ambitious

00:17:53,039 --> 00:17:59,919
what seems to be a quirk in wrk emerges

00:17:56,160 --> 00:18:00,960
the 50 and 250 qp interference yield no

00:17:59,919 --> 00:18:05,520
output

00:18:00,960 --> 00:18:05,520
and the 500 qps one looks pretty weird

00:18:08,160 --> 00:18:11,760
here we show the same plots but now from

00:18:10,080 --> 00:18:14,480
envoy's access logs which served as a

00:18:11,760 --> 00:18:17,200
control measurement in the experiment

00:18:14,480 --> 00:18:18,160
so wrk was sending traffic where we

00:18:17,200 --> 00:18:19,520
missed the output

00:18:18,160 --> 00:18:22,000
and it doesn't look that out of the

00:18:19,520 --> 00:18:24,559
ordinary

00:18:22,000 --> 00:18:26,000
manual investigation of request arrival

00:18:24,559 --> 00:18:27,760
timings at the test subject

00:18:26,000 --> 00:18:30,880
showed that 4 tile would yield traffic

00:18:27,760 --> 00:18:32,799
that was bursty in comparison to wrk

00:18:30,880 --> 00:18:34,640
this turned out to be a dominant factor

00:18:32,799 --> 00:18:36,160
in the observed divergence in measured

00:18:34,640 --> 00:18:38,559
latencies

00:18:36,160 --> 00:18:40,640
later on a jitter flag was added to fort

00:18:38,559 --> 00:18:44,880
io to desynchronize request release

00:18:40,640 --> 00:18:44,880
timings which reduce divergence

00:18:45,039 --> 00:18:49,039
this may suffice until one wants to

00:18:47,360 --> 00:18:50,960
define tests in terms of random

00:18:49,039 --> 00:18:52,400
distributions around request release

00:18:50,960 --> 00:18:54,559
timings

00:18:52,400 --> 00:18:57,120
anyway the learning here is to know your

00:18:54,559 --> 00:18:58,640
tools and have well-defined tests

00:18:57,120 --> 00:19:00,240
different tools come with different

00:18:58,640 --> 00:19:03,200
weaknesses strengths and implied

00:19:00,240 --> 00:19:03,200
characteristics

00:19:04,000 --> 00:19:09,440
some other gotchas observed in the wild

00:19:07,440 --> 00:19:10,960
latency measurement tooling may diverge

00:19:09,440 --> 00:19:14,240
when it comes to handling in-flight

00:19:10,960 --> 00:19:16,640
requests post-execution

00:19:14,240 --> 00:19:19,520
some tools may include status and oral

00:19:16,640 --> 00:19:21,440
latency in their reporting

00:19:19,520 --> 00:19:23,840
also the math around tracking rolling

00:19:21,440 --> 00:19:25,600
standard deviations and histograms isn't

00:19:23,840 --> 00:19:28,880
trivial

00:19:25,600 --> 00:19:30,720
floating point math is used here and it

00:19:28,880 --> 00:19:32,960
may be subject to a phenomena called

00:19:30,720 --> 00:19:34,480
catastrophical cancellation when using a

00:19:32,960 --> 00:19:36,640
naive approach

00:19:34,480 --> 00:19:37,520
and through that tooling can produce

00:19:36,640 --> 00:19:41,760
distorted

00:19:37,520 --> 00:19:43,679
numbers when comparing baselines

00:19:41,760 --> 00:19:45,200
of different servers and meshes or even

00:19:43,679 --> 00:19:47,039
across load generators

00:19:45,200 --> 00:19:48,480
it is good to be aware that diverging

00:19:47,039 --> 00:19:50,720
tls cipher send or

00:19:48,480 --> 00:19:52,720
session reuse characteristic

00:19:50,720 --> 00:19:54,880
characteristics and setups

00:19:52,720 --> 00:19:58,240
may translate into significant latest

00:19:54,880 --> 00:19:58,240
latency divergences

00:20:02,960 --> 00:20:07,280
so when it comes to reporting results it

00:20:05,440 --> 00:20:09,440
is common for a tooling to produce a

00:20:07,280 --> 00:20:11,520
single histogram

00:20:09,440 --> 00:20:13,360
sometimes however being able to dive a

00:20:11,520 --> 00:20:15,520
bit deeper is useful

00:20:13,360 --> 00:20:18,240
this example shows a single globally

00:20:15,520 --> 00:20:20,480
aggregated histogram on the left

00:20:18,240 --> 00:20:21,919
it is under its underlying data comes

00:20:20,480 --> 00:20:23,200
from measurements across multiple

00:20:21,919 --> 00:20:25,520
distinct workers

00:20:23,200 --> 00:20:27,360
which run on different threads the

00:20:25,520 --> 00:20:29,280
workers all use their own connection

00:20:27,360 --> 00:20:31,360
pool

00:20:29,280 --> 00:20:33,120
the plot shows a broken knee which is

00:20:31,360 --> 00:20:35,120
kind of odd

00:20:33,120 --> 00:20:36,960
on the right we visualize the output

00:20:35,120 --> 00:20:39,280
once more but now we also include the

00:20:36,960 --> 00:20:41,919
perverter plots

00:20:39,280 --> 00:20:43,760
those all look as regular latency plots

00:20:41,919 --> 00:20:48,720
and now it's much easier to see how the

00:20:43,760 --> 00:20:50,640
aggregated view obtained its odd shape

00:20:48,720 --> 00:20:53,039
the plots displayed in the previous

00:20:50,640 --> 00:20:54,880
slide often point to a phenomena called

00:20:53,039 --> 00:20:57,039
backhand hold spotting

00:20:54,880 --> 00:20:58,240
this diagram tries to visualize the

00:20:57,039 --> 00:21:00,080
phenomena

00:20:58,240 --> 00:21:01,840
the connections from the load generator

00:21:00,080 --> 00:21:04,640
didn't end up in a balanced way at the

00:21:01,840 --> 00:21:06,799
test subject's processing capacities

00:21:04,640 --> 00:21:09,200
one listener has three connections while

00:21:06,799 --> 00:21:11,600
the other one has just a single one

00:21:09,200 --> 00:21:13,440
in perfect keeper life tests that may

00:21:11,600 --> 00:21:14,640
not out this may not automatically

00:21:13,440 --> 00:21:16,559
rebalance

00:21:14,640 --> 00:21:18,799
and in setups with chain proxies like

00:21:16,559 --> 00:21:20,240
meshes this may even cascade through the

00:21:18,799 --> 00:21:22,320
system

00:21:20,240 --> 00:21:23,360
fortunately envoy has some mitigations

00:21:22,320 --> 00:21:25,280
throughout this today

00:21:23,360 --> 00:21:27,440
but your mileage may vary depending on

00:21:25,280 --> 00:21:29,200
the technology you are running tests

00:21:27,440 --> 00:21:31,440
against

00:21:29,200 --> 00:21:33,200
unless you are specifically testing this

00:21:31,440 --> 00:21:35,120
aspect of a system

00:21:33,200 --> 00:21:37,280
it might be good to explicitly limit

00:21:35,120 --> 00:21:38,240
connection reuse to allow for balancing

00:21:37,280 --> 00:21:41,280
opportunities

00:21:38,240 --> 00:21:41,280
when running into this

00:21:42,320 --> 00:21:45,520
here's a list of things that may

00:21:43,760 --> 00:21:46,960
complicate reasoning when measuring

00:21:45,520 --> 00:21:49,200
latency

00:21:46,960 --> 00:21:51,200
noise enablers in virtualized setups may

00:21:49,200 --> 00:21:54,159
sometimes cause instability between

00:21:51,200 --> 00:21:56,240
results of distinct test executions

00:21:54,159 --> 00:21:58,720
competing processes may introduce

00:21:56,240 --> 00:22:00,720
introduce noise too

00:21:58,720 --> 00:22:02,159
if you are after getting things as easy

00:22:00,720 --> 00:22:03,679
to reason about as possible

00:22:02,159 --> 00:22:06,240
and obtaining the most predictable

00:22:03,679 --> 00:22:07,679
results the cpu frequency changes and

00:22:06,240 --> 00:22:08,640
hyper trading may not be your best

00:22:07,679 --> 00:22:11,360
friends

00:22:08,640 --> 00:22:12,720
especially when testing surface meshes

00:22:11,360 --> 00:22:15,039
in my experience

00:22:12,720 --> 00:22:16,799
experience it is good to start out small

00:22:15,039 --> 00:22:18,720
and simple and build from there when

00:22:16,799 --> 00:22:21,200
trying to load test large and complex

00:22:18,720 --> 00:22:21,200
meshes

00:22:21,760 --> 00:22:25,840
load generators aside following the

00:22:23,760 --> 00:22:27,760
earlier observations it might be good to

00:22:25,840 --> 00:22:29,679
define load tests a bit more narrowly

00:22:27,760 --> 00:22:31,440
than just queries per second over a

00:22:29,679 --> 00:22:33,440
number of connections

00:22:31,440 --> 00:22:35,120
there seem to be many implicit choices

00:22:33,440 --> 00:22:35,840
across load generators and from the

00:22:35,120 --> 00:22:39,679
outside

00:22:35,840 --> 00:22:42,080
it's not always obvious what those are

00:22:39,679 --> 00:22:44,240
talking about test definitions google

00:22:42,080 --> 00:22:46,480
recently contributed an adaptive load

00:22:44,240 --> 00:22:48,400
controller feature to nighthawk

00:22:46,480 --> 00:22:50,559
that feature makes it easy to perform

00:22:48,400 --> 00:22:52,400
sla-based testing to get answers to

00:22:50,559 --> 00:22:54,320
questions like

00:22:52,400 --> 00:22:55,440
how many queries per second can my

00:22:54,320 --> 00:22:57,600
system sustain

00:22:55,440 --> 00:22:58,640
given these resource constraints under

00:22:57,600 --> 00:23:01,679
set latency

00:22:58,640 --> 00:23:03,360
while observing at most x errors the

00:23:01,679 --> 00:23:05,600
adaptive load controller will then

00:23:03,360 --> 00:23:07,679
repeatedly re-parameterize test

00:23:05,600 --> 00:23:11,520
execution to see conversions on the

00:23:07,679 --> 00:23:11,520
optimum qps that fits this

00:23:12,640 --> 00:23:16,000
so this concludes my part of the

00:23:14,320 --> 00:23:20,640
presentation

00:23:16,000 --> 00:23:20,640
thank you for listening ritika off to

00:23:20,840 --> 00:23:24,720
you

00:23:22,480 --> 00:23:26,320
thanks auro as a continuation of the

00:23:24,720 --> 00:23:28,080
bottleneck analysis we would like to

00:23:26,320 --> 00:23:30,720
propose some next steps

00:23:28,080 --> 00:23:33,200
let's look at the ingress connections on

00:23:30,720 --> 00:23:35,200
the left hand side of the diagram here

00:23:33,200 --> 00:23:36,880
that can be distributed among multiple

00:23:35,200 --> 00:23:40,240
worker threats

00:23:36,880 --> 00:23:42,400
the label one so worker threads

00:23:40,240 --> 00:23:43,520
that are waiting for traffic versus

00:23:42,400 --> 00:23:45,279
threads that have

00:23:43,520 --> 00:23:47,840
a traffic to microservices that are

00:23:45,279 --> 00:23:49,679
active but disbalanced

00:23:47,840 --> 00:23:51,039
look at the bypassing of some of the

00:23:49,679 --> 00:23:54,240
linux scheduling here

00:23:51,039 --> 00:23:57,279
and instead use score based

00:23:54,240 --> 00:23:59,440
balancing so multiple cores being used

00:23:57,279 --> 00:24:00,640
to balance out multiple worker threads

00:23:59,440 --> 00:24:03,679
being scheduled

00:24:00,640 --> 00:24:05,919
maybe have a priority among them

00:24:03,679 --> 00:24:08,480
the other bottleneck to analyze is that

00:24:05,919 --> 00:24:11,200
associated with memory copies

00:24:08,480 --> 00:24:13,440
between proxies proxies two micro

00:24:11,200 --> 00:24:15,760
services labeled as two here

00:24:13,440 --> 00:24:16,960
and how do we accelerate some of those

00:24:15,760 --> 00:24:19,600
bottlenecks

00:24:16,960 --> 00:24:20,480
for example dma copies using vectorized

00:24:19,600 --> 00:24:24,480
instructions

00:24:20,480 --> 00:24:26,720
like avx 512 for small memory copies

00:24:24,480 --> 00:24:30,080
larger copies uh slowing down frame

00:24:26,720 --> 00:24:32,799
processing or tcp text stacked flows

00:24:30,080 --> 00:24:35,120
that can be offloaded using dma offload

00:24:32,799 --> 00:24:37,760
engines in the cpu

00:24:35,120 --> 00:24:38,559
directly receiving the traffic packet

00:24:37,760 --> 00:24:42,080
from the nic

00:24:38,559 --> 00:24:45,440
to a cpu port at the proxy as shown in

00:24:42,080 --> 00:24:48,559
the right hand side of the file here

00:24:45,440 --> 00:24:50,960
and then using uh

00:24:48,559 --> 00:24:52,000
you know bypassing the kernel and then

00:24:50,960 --> 00:24:54,400
using

00:24:52,000 --> 00:24:55,440
an acceleration once you receive the

00:24:54,400 --> 00:24:57,520
packet

00:24:55,440 --> 00:24:59,919
how do you use dma engine to offload

00:24:57,520 --> 00:25:03,200
some of the copies that are happening

00:24:59,919 --> 00:25:06,320
from the proxy to the microservices

00:25:03,200 --> 00:25:10,080
so in summary um achieving a qps of

00:25:06,320 --> 00:25:11,760
more than 50k and latency of

00:25:10,080 --> 00:25:14,400
less than five milliseconds in the

00:25:11,760 --> 00:25:17,840
current setup is one of our goals

00:25:14,400 --> 00:25:19,760
this performance within a cni plus proxy

00:25:17,840 --> 00:25:22,320
in a virtualized environment is a

00:25:19,760 --> 00:25:25,039
formidable goal we are targeting

00:25:22,320 --> 00:25:26,240
if you want to join some of our analysis

00:25:25,039 --> 00:25:28,240
let us know

00:25:26,240 --> 00:25:29,919
connect with us and we should be able to

00:25:28,240 --> 00:25:34,159
achieve some of these goals

00:25:29,919 --> 00:25:34,159
and present in a future session thank

00:25:35,159 --> 00:25:38,159

YouTube URL: https://www.youtube.com/watch?v=RXmgOsSxrSg


