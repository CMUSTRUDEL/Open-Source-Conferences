Title: Multi-Cluster Service Mesh With Istio - Zhonghu Xu, Huawei
Publication date: 2021-05-05
Playlist: ServiceMeshCon EU 2021
Description: 
	Donâ€™t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

Multi-Cluster Service Mesh With Istio - Zhonghu Xu, Huawei

In order to achieve high availability and scalability, more and more users are going to deploy applications across multiple clusters. Even these clusters are distributed in different regions, how could services discovery and communicate with each other become a big challenge for users. In this talk, Zhonghu will show you how Istio can handle this tricky issue for us. First, he will talk about the different kinds of multi-cluster models and what are the best scenarios they can address. Then, Zhonghu will illustrate the design philosophy of how Istio discovers services across clusters and how Istio simplifies service management. And at last, Zhonghu will conclude some points that can be improved in the future.
Captions: 
	00:00:00,080 --> 00:00:07,520
hello all right my name is

00:00:04,240 --> 00:00:08,320
today i will introduce you multi-cluster

00:00:07,520 --> 00:00:11,920
service mesh

00:00:08,320 --> 00:00:14,960
with these two before we get started

00:00:11,920 --> 00:00:18,960
let me first introduce myself

00:00:14,960 --> 00:00:21,600
i'm enthusiastic

00:00:18,960 --> 00:00:22,600
and works on research source software

00:00:21,600 --> 00:00:26,480
since

00:00:22,600 --> 00:00:30,160
2017 i have joined the sql committee

00:00:26,480 --> 00:00:31,199
since 2018 and the focus on networking

00:00:30,160 --> 00:00:34,480
since then

00:00:31,199 --> 00:00:38,719
and now i used to call methanol and

00:00:34,480 --> 00:00:41,200
one of the top contributors i'm also a

00:00:38,719 --> 00:00:41,760
school student committee member as the

00:00:41,200 --> 00:00:44,879
author

00:00:41,760 --> 00:00:45,360
of our native search mesh is too the

00:00:44,879 --> 00:00:49,200
best

00:00:45,360 --> 00:00:49,200
selling service match book in china

00:00:50,480 --> 00:00:56,960
let's see today's agenda

00:00:55,039 --> 00:00:58,399
there are four parts that i will

00:00:56,960 --> 00:01:02,160
introduce today

00:00:58,399 --> 00:01:05,360
the first part is use case of multicast

00:01:02,160 --> 00:01:09,439
in this part i will show

00:01:05,360 --> 00:01:12,159
you why we need multi-cluster

00:01:09,439 --> 00:01:14,799
and the second part is the challenge of

00:01:12,159 --> 00:01:17,759
multi-cluster

00:01:14,799 --> 00:01:18,240
and there are several challenges here i

00:01:17,759 --> 00:01:21,360
will

00:01:18,240 --> 00:01:22,159
show you the third part is different

00:01:21,360 --> 00:01:25,600
multi-class

00:01:22,159 --> 00:01:29,360
patterns this part is the most

00:01:25,600 --> 00:01:32,159
important part of today's topic

00:01:29,360 --> 00:01:32,799
the last part i will introduce you

00:01:32,159 --> 00:01:37,920
future

00:01:32,799 --> 00:01:37,920
evolution of multi-cluster in history

00:01:38,400 --> 00:01:42,799
let's see the first part why do we need

00:01:40,799 --> 00:01:45,600
multicast

00:01:42,799 --> 00:01:46,640
multicluster is a subjective strategy

00:01:45,600 --> 00:01:50,159
for deploying

00:01:46,640 --> 00:01:52,640
an application on or across multiple

00:01:50,159 --> 00:01:53,600
kubernetes clusters with the goal of

00:01:52,640 --> 00:01:57,920
improving

00:01:53,600 --> 00:02:00,960
availability isolation and scalability

00:01:57,920 --> 00:02:03,920
multi-cluster can be important to ensure

00:02:00,960 --> 00:02:05,119
computer planes with different and

00:02:03,920 --> 00:02:08,239
conflicting

00:02:05,119 --> 00:02:09,679
regulations as individual clusters can

00:02:08,239 --> 00:02:13,599
be adopted to

00:02:09,679 --> 00:02:14,319
comply with geographic or certification

00:02:13,599 --> 00:02:17,040
specific

00:02:14,319 --> 00:02:18,319
regulations the speed and safety of

00:02:17,040 --> 00:02:21,599
software

00:02:18,319 --> 00:02:25,120
delivered can also be increased within

00:02:21,599 --> 00:02:30,239
individual development teams deploying

00:02:25,120 --> 00:02:30,239
applications to isolated clusters

00:02:30,319 --> 00:02:36,319
and selectively exposing which

00:02:33,360 --> 00:02:37,920
services are available for testing and

00:02:36,319 --> 00:02:40,720
release

00:02:37,920 --> 00:02:42,319
the first part the first point is

00:02:40,720 --> 00:02:44,400
availability

00:02:42,319 --> 00:02:45,519
multi-clusters can spin across

00:02:44,400 --> 00:02:49,200
multi-regions

00:02:45,519 --> 00:02:53,040
or even multi-vendors users can rapidly

00:02:49,200 --> 00:02:55,599
replicate applications in each cluster

00:02:53,040 --> 00:02:56,160
thus even when one cluster is totally

00:02:55,599 --> 00:02:59,599
down

00:02:56,160 --> 00:03:02,640
the traffic can fail over to a healthy

00:02:59,599 --> 00:03:03,599
remote cluster and it has no influence

00:03:02,640 --> 00:03:07,120
on the system

00:03:03,599 --> 00:03:11,040
availability the second one is

00:03:07,120 --> 00:03:14,239
performance for internet users

00:03:11,040 --> 00:03:15,120
they can access the servers nearest to

00:03:14,239 --> 00:03:19,440
them

00:03:15,120 --> 00:03:22,480
and the latency is the smallest

00:03:19,440 --> 00:03:25,680
strong isolation guarantees

00:03:22,480 --> 00:03:28,480
simplicity key operational processes

00:03:25,680 --> 00:03:29,440
such as cluster and application of

00:03:28,480 --> 00:03:32,879
greece

00:03:29,440 --> 00:03:37,680
moreover isolation can reduce

00:03:32,879 --> 00:03:40,560
the blaster radius of a cluster outage

00:03:37,680 --> 00:03:42,000
organizations with a strong tendency

00:03:40,560 --> 00:03:44,959
isolation

00:03:42,000 --> 00:03:45,360
requirements convert each tenant to

00:03:44,959 --> 00:03:48,480
their

00:03:45,360 --> 00:03:52,080
individual cluster scalability

00:03:48,480 --> 00:03:55,840
one kubernetes cluster can take charge

00:03:52,080 --> 00:03:59,519
of 5000 nodes at the most

00:03:55,840 --> 00:04:02,560
which is not enough for 100

00:03:59,519 --> 00:04:02,560
100 kilo

00:04:02,720 --> 00:04:09,680
100 000 replicas

00:04:05,760 --> 00:04:12,879
application so it's not enough for

00:04:09,680 --> 00:04:16,400
their application level

00:04:12,879 --> 00:04:19,759
so they need multi-clusters

00:04:16,400 --> 00:04:20,479
the last one is the cost a multi-class

00:04:19,759 --> 00:04:23,600
strategy

00:04:20,479 --> 00:04:26,400
strategy in enables your

00:04:23,600 --> 00:04:27,680
organization to shift workloads between

00:04:26,400 --> 00:04:30,960
different communities

00:04:27,680 --> 00:04:33,919
vendors to take advantage of

00:04:30,960 --> 00:04:34,400
new capabilities and pricing offered by

00:04:33,919 --> 00:04:36,880
different

00:04:34,400 --> 00:04:36,880
vendors

00:04:38,400 --> 00:04:44,720
okay let's see the second part the

00:04:41,040 --> 00:04:44,720
challenges for multicast

00:04:45,440 --> 00:04:52,000
there are four points i listed here

00:04:48,960 --> 00:04:55,280
the first one is service discover

00:04:52,000 --> 00:04:58,000
discovery for native

00:04:55,280 --> 00:04:59,600
kubernetes there is no way to do service

00:04:58,000 --> 00:05:02,960
discovery

00:04:59,600 --> 00:05:05,440
for remote clusters

00:05:02,960 --> 00:05:06,320
we have to make use of external service

00:05:05,440 --> 00:05:09,840
registries

00:05:06,320 --> 00:05:11,520
like people for example on the picture

00:05:09,840 --> 00:05:14,720
on the right picture

00:05:11,520 --> 00:05:18,639
the cluster one clients

00:05:14,720 --> 00:05:23,360
can not discovery discover the

00:05:18,639 --> 00:05:26,240
class services from the other clusters

00:05:23,360 --> 00:05:26,880
and the second one is the dns dns

00:05:26,240 --> 00:05:30,560
reserve

00:05:26,880 --> 00:05:34,479
reserve the remote service

00:05:30,560 --> 00:05:37,680
domains are resolvable in local cluster

00:05:34,479 --> 00:05:41,919
because in native kubernetes the

00:05:37,680 --> 00:05:46,560
kubernetes is responsible for dns

00:05:41,919 --> 00:05:49,600
reserve but here it has no information

00:05:46,560 --> 00:05:51,440
of the other clusters service and any

00:05:49,600 --> 00:05:54,720
points

00:05:51,440 --> 00:05:58,160
so for our service survives across much

00:05:54,720 --> 00:05:58,960
clusters it is resolvable the service is

00:05:58,160 --> 00:06:02,720
hard to

00:05:58,960 --> 00:06:06,000
access because the kinetics improvement

00:06:02,720 --> 00:06:08,319
service accessed by our iv walls

00:06:06,000 --> 00:06:10,000
which only handles local cluster service

00:06:08,319 --> 00:06:14,639
and voice

00:06:10,000 --> 00:06:17,280
the third one is load balancer policy

00:06:14,639 --> 00:06:19,039
the load balancing is run rubbing for

00:06:17,280 --> 00:06:21,440
kubernetes

00:06:19,039 --> 00:06:22,880
av testing and the canary release are

00:06:21,440 --> 00:06:26,240
hard to implement

00:06:22,880 --> 00:06:29,520
employment the

00:06:26,240 --> 00:06:32,000
fourth one is a lack of security

00:06:29,520 --> 00:06:33,759
it is very dangerous to talk with each

00:06:32,000 --> 00:06:37,120
other with plain text

00:06:33,759 --> 00:06:41,280
it's best especially

00:06:37,120 --> 00:06:41,280
sorry especially through

00:06:41,919 --> 00:06:47,360
especially through the public network

00:06:47,600 --> 00:06:54,800
okay let's see the next part

00:06:50,800 --> 00:06:58,720
what is sq uh from the

00:06:54,800 --> 00:07:03,840
is your official document website

00:06:58,720 --> 00:07:03,840
we can say the water is joyous

00:07:04,000 --> 00:07:10,000
before we start that part let's show

00:07:07,120 --> 00:07:11,759
kubernetes kubernetes is a platform for

00:07:10,000 --> 00:07:14,080
medication deployment

00:07:11,759 --> 00:07:15,199
and operation and also provides some

00:07:14,080 --> 00:07:18,000
capability

00:07:15,199 --> 00:07:18,880
on service discovery and load balance

00:07:18,000 --> 00:07:22,400
however

00:07:18,880 --> 00:07:25,680
its q is totally service or oriented

00:07:22,400 --> 00:07:28,639
and is very good supplement for managers

00:07:25,680 --> 00:07:29,280
in service management it is very

00:07:28,639 --> 00:07:33,599
friendly

00:07:29,280 --> 00:07:37,120
to both developers and operators

00:07:33,599 --> 00:07:40,639
from the slide here

00:07:37,120 --> 00:07:44,000
it still provides four major

00:07:40,639 --> 00:07:46,479
function the first one is connect it

00:07:44,000 --> 00:07:47,039
intelligently controls the flow of

00:07:46,479 --> 00:07:51,759
traffic

00:07:47,039 --> 00:07:51,759
and api calls between services

00:07:53,599 --> 00:08:00,080
so with it we can do some

00:07:56,639 --> 00:08:05,440
more advanced traffic

00:08:00,080 --> 00:08:08,240
management like uh blue green

00:08:05,440 --> 00:08:09,120
current deployment a red black

00:08:08,240 --> 00:08:12,400
development

00:08:09,120 --> 00:08:13,759
and a clan radius release penalty

00:08:12,400 --> 00:08:17,440
release

00:08:13,759 --> 00:08:20,680
the second part is secure

00:08:17,440 --> 00:08:24,000
it sql provides the

00:08:20,680 --> 00:08:27,599
automatically chairs with each

00:08:24,000 --> 00:08:28,720
each api call so by default the service

00:08:27,599 --> 00:08:33,759
to service

00:08:28,720 --> 00:08:33,759
traffic is encrypted with qrs

00:08:34,399 --> 00:08:42,159
and the third party is the control

00:08:38,320 --> 00:08:44,640
with the issue of authorization policy

00:08:42,159 --> 00:08:45,920
authentication policy it still can

00:08:44,640 --> 00:08:49,040
control

00:08:45,920 --> 00:08:52,399
the policy the

00:08:49,040 --> 00:08:55,680
fourth part is i think is the most

00:08:52,399 --> 00:08:59,760
important one it provides the over

00:08:55,680 --> 00:09:02,800
the ability such as access log

00:08:59,760 --> 00:09:06,839
monitoring and also assets

00:09:02,800 --> 00:09:09,839
uh also distributed

00:09:06,839 --> 00:09:09,839
transit

00:09:10,560 --> 00:09:14,240
let's say multi-class match

00:09:14,880 --> 00:09:20,720
before we get started

00:09:18,480 --> 00:09:21,760
in production with multi-cluster surface

00:09:20,720 --> 00:09:25,120
match

00:09:21,760 --> 00:09:27,680
we should consider the following three

00:09:25,120 --> 00:09:27,680
questions

00:09:29,040 --> 00:09:35,680
compared with multi-clusters

00:09:33,120 --> 00:09:37,680
which provides a blackboard service

00:09:35,680 --> 00:09:40,480
boarding single machine is aware of all

00:09:37,680 --> 00:09:40,480
the clusters

00:09:41,040 --> 00:09:47,360
services and can do more advanced

00:09:44,320 --> 00:09:50,720
localizing like location

00:09:47,360 --> 00:09:53,839
aware routing and failover

00:09:50,720 --> 00:09:54,640
so the first quick question the answer

00:09:53,839 --> 00:09:59,200
is

00:09:54,640 --> 00:09:59,200
where should the two choose single mesh

00:09:59,600 --> 00:10:03,680
the second question is single network or

00:10:01,920 --> 00:10:07,440
different networks

00:10:03,680 --> 00:10:09,920
it still uses a simplified definition

00:10:07,440 --> 00:10:11,200
of network to refer to workload

00:10:09,920 --> 00:10:14,720
instances

00:10:11,200 --> 00:10:16,839
that have direct reachability for

00:10:14,720 --> 00:10:19,760
example by default all workload

00:10:16,839 --> 00:10:22,800
instances in a single cluster

00:10:19,760 --> 00:10:22,800
on the same network

00:10:23,360 --> 00:10:27,120
many production systems require multiple

00:10:26,240 --> 00:10:29,600
networks

00:10:27,120 --> 00:10:30,640
or thumbnails for isolation and higher

00:10:29,600 --> 00:10:33,920
availability

00:10:30,640 --> 00:10:36,880
is about spring expanding a

00:10:33,920 --> 00:10:39,279
service mesh over a variety of network

00:10:36,880 --> 00:10:42,320
topologies

00:10:39,279 --> 00:10:45,120
so the second answer for the second

00:10:42,320 --> 00:10:48,880
question is depends on the

00:10:45,120 --> 00:10:51,120
production environment we have

00:10:48,880 --> 00:10:53,760
the third one is single or multiple

00:10:51,120 --> 00:10:56,399
control plans

00:10:53,760 --> 00:10:57,760
single plane is easier while multi

00:10:56,399 --> 00:10:59,839
replicated

00:10:57,760 --> 00:11:01,200
control planes in each cluster may

00:10:59,839 --> 00:11:06,560
provide

00:11:01,200 --> 00:11:06,560
more availabilities we can depend on our

00:11:06,640 --> 00:11:15,839
real scenery

00:11:16,240 --> 00:11:24,880
okay let's see the next part dns result

00:11:19,519 --> 00:11:24,880
before 1.1.8

00:11:25,120 --> 00:11:34,160
let's see the picture 1.8

00:11:30,320 --> 00:11:37,600
the dns result

00:11:34,160 --> 00:11:41,440
is via the kubernetes and the

00:11:37,600 --> 00:11:45,040
sql codens for services

00:11:41,440 --> 00:11:48,240
for native communities services which is

00:11:45,040 --> 00:11:52,880
resolved by the audience

00:11:48,240 --> 00:11:56,399
and for the remote cluster services

00:11:52,880 --> 00:12:01,600
which is resolved by a separate plugin

00:11:56,399 --> 00:12:04,800
it's your code yes the audience is a

00:12:01,600 --> 00:12:10,240
is an upstream dance

00:12:04,800 --> 00:12:13,200
for the audience

00:12:10,240 --> 00:12:14,800
uh in native kubernetes it still use the

00:12:13,200 --> 00:12:18,320
virtual ip

00:12:14,800 --> 00:12:18,800
returned by the dns lookup to load

00:12:18,320 --> 00:12:22,720
balance

00:12:18,800 --> 00:12:25,600
across the list of active endpoints

00:12:22,720 --> 00:12:26,480
for the requested services taking into

00:12:25,600 --> 00:12:29,519
account

00:12:26,480 --> 00:12:32,320
any configured routing rules it still

00:12:29,519 --> 00:12:33,200
uses other community services or

00:12:32,320 --> 00:12:36,320
endpoints

00:12:33,200 --> 00:12:38,639
or is to service entry to configure

00:12:36,320 --> 00:12:41,839
its internal mapping of hostname to

00:12:38,639 --> 00:12:41,839
workload ip address

00:12:42,959 --> 00:12:46,240
to ensure that dns lookup success you

00:12:45,839 --> 00:12:49,440
must

00:12:46,240 --> 00:12:53,040
deploy a connected service to

00:12:49,440 --> 00:12:55,440
each cluster that consumes that service

00:12:53,040 --> 00:12:56,399
this includes that regardless of where

00:12:55,440 --> 00:12:59,279
the record

00:12:56,399 --> 00:12:59,839
request origin is you will pass dns

00:12:59,279 --> 00:13:02,480
lookup

00:12:59,839 --> 00:13:03,360
and will be handed to issue for proper

00:13:02,480 --> 00:13:06,240
routing

00:13:03,360 --> 00:13:07,440
this can also be achieved with sql

00:13:06,240 --> 00:13:10,079
service entry

00:13:07,440 --> 00:13:10,959
rather than communicating service

00:13:10,079 --> 00:13:13,440
however

00:13:10,959 --> 00:13:15,440
a service entry doesn't confirm

00:13:13,440 --> 00:13:18,959
configure the kubernetes dns

00:13:15,440 --> 00:13:22,000
server this means that dns will be

00:13:18,959 --> 00:13:25,200
configured either manually or with

00:13:22,000 --> 00:13:28,720
automatic automated tooling such as

00:13:25,200 --> 00:13:28,720
sql code ds plugin

00:13:29,920 --> 00:13:37,040
let's see the dns resolve after after

00:13:33,440 --> 00:13:37,040
release 1.8

00:13:38,320 --> 00:13:43,839
after being 1.8 released dns proxy is

00:13:42,880 --> 00:13:48,240
introduced

00:13:43,839 --> 00:13:50,240
not only for multi clusters but also for

00:13:48,240 --> 00:13:52,560
virtual machines the as a result

00:13:50,240 --> 00:13:56,160
previous obviously we need an

00:13:52,560 --> 00:13:58,800
additional components sql code yes

00:13:56,160 --> 00:13:59,839
it is not very friendly it requires

00:13:58,800 --> 00:14:02,880
service interest

00:13:59,839 --> 00:14:06,560
created seo is extends

00:14:02,880 --> 00:14:10,320
a new kind of x guest

00:14:06,560 --> 00:14:13,760
nds and yes the full name is

00:14:10,320 --> 00:14:17,440
named this limitable discovery service

00:14:13,760 --> 00:14:20,240
to facilitate the dns reserve nds is

00:14:17,440 --> 00:14:20,720
used by the dns proxy to fetch the

00:14:20,240 --> 00:14:23,839
answer

00:14:20,720 --> 00:14:27,440
name tables from sql control plan

00:14:23,839 --> 00:14:31,519
and then it builds dns lookup cable

00:14:27,440 --> 00:14:34,079
to service mds results from reload

00:14:31,519 --> 00:14:35,040
from local application service the name

00:14:34,079 --> 00:14:38,000
table contains

00:14:35,040 --> 00:14:39,199
awesome services across clusters for

00:14:38,000 --> 00:14:42,000
community service

00:14:39,199 --> 00:14:44,240
the address is class type e for the

00:14:42,000 --> 00:14:49,360
other service entry defined service

00:14:44,240 --> 00:14:49,360
the address is author allocated

00:14:50,480 --> 00:14:58,480
this address is a kind of clustering

00:14:53,760 --> 00:14:59,680
address okay let's see the picture this

00:14:58,480 --> 00:15:04,079
is a works

00:14:59,680 --> 00:15:04,079
workflow of the dns reserve

00:15:04,240 --> 00:15:10,399
firstly the the dns

00:15:07,360 --> 00:15:11,440
requester is forwarded to the dns

00:15:10,399 --> 00:15:14,880
processing

00:15:11,440 --> 00:15:19,600
the drs proceed returns a dns

00:15:14,880 --> 00:15:23,440
response if it caches the dns

00:15:19,600 --> 00:15:26,720
name table otherwise it will afford the

00:15:23,440 --> 00:15:29,600
dns request to the kubernetes

00:15:26,720 --> 00:15:30,240
and then maybe the audience can follow

00:15:29,600 --> 00:15:33,440
the

00:15:30,240 --> 00:15:36,720
dns request to external

00:15:33,440 --> 00:15:36,720
upstream yes

00:15:36,959 --> 00:15:43,199
single network let's say single

00:15:40,079 --> 00:15:45,680
network for single network

00:15:43,199 --> 00:15:47,519
all cluster rest resides in a same

00:15:45,680 --> 00:15:50,639
network and outputs from past

00:15:47,519 --> 00:15:54,000
one can touch to pause from class 2

00:15:50,639 --> 00:15:56,320
directly such no gateway is needed

00:15:54,000 --> 00:15:57,360
and the close-class communication will

00:15:56,320 --> 00:16:01,600
not increase

00:15:57,360 --> 00:16:05,199
latest in most cases this is not common

00:16:01,600 --> 00:16:09,440
this is the pros proof is

00:16:05,199 --> 00:16:12,480
low latency for easter eggs to traffic

00:16:09,440 --> 00:16:15,519
as a gateway is not needed

00:16:12,480 --> 00:16:18,720
the cons one is the

00:16:15,519 --> 00:16:21,519
complexity need an additional tool to

00:16:18,720 --> 00:16:25,040
build flight and network

00:16:21,519 --> 00:16:25,040
second one is security

00:16:25,360 --> 00:16:30,399
it is not secure as all the workloads

00:16:27,680 --> 00:16:33,199
are using a single network

00:16:30,399 --> 00:16:35,279
the third one is no operation for the

00:16:33,199 --> 00:16:39,680
service iqr3

00:16:35,279 --> 00:16:43,440
ranges that means that the cluster

00:16:39,680 --> 00:16:46,480
the class 1 and the cluster 2

00:16:43,440 --> 00:16:50,560
cannot have overlapping service

00:16:46,480 --> 00:16:52,959
id address or port iplan for the ipl

00:16:50,560 --> 00:16:52,959
address

00:16:55,759 --> 00:16:59,040
different networks so the mesh kinds

00:16:58,160 --> 00:17:01,920
brings

00:16:59,040 --> 00:17:04,319
seven different networks each cluster

00:17:01,920 --> 00:17:07,760
resides in a network product can not

00:17:04,319 --> 00:17:11,039
talk directly to the other ports in

00:17:07,760 --> 00:17:13,839
another cluster this provides

00:17:11,039 --> 00:17:14,640
better isolation each cluster is

00:17:13,839 --> 00:17:18,400
independent

00:17:14,640 --> 00:17:20,799
independent undergrowth

00:17:18,400 --> 00:17:23,199
class service access must through uh

00:17:20,799 --> 00:17:25,919
easter western giveaway

00:17:23,199 --> 00:17:27,760
so the challenge is is a cross-cluster

00:17:25,919 --> 00:17:30,960
service communication

00:17:27,760 --> 00:17:36,400
actually it requires users to gather it

00:17:30,960 --> 00:17:41,039
and it works in tis or pass through mode

00:17:36,400 --> 00:17:41,039
so let's see the pros and cons

00:17:42,480 --> 00:17:48,559
yeah this is good for scaling of network

00:17:46,559 --> 00:17:51,600
addresses

00:17:48,559 --> 00:17:54,559
the concepts is the load balancing

00:17:51,600 --> 00:18:01,840
across multi-cluster as well as

00:17:54,559 --> 00:18:01,840
a single cluster

00:18:02,400 --> 00:18:06,320
but different network get away

00:18:07,360 --> 00:18:13,520
the first one is the stabilizer horizon

00:18:10,480 --> 00:18:14,799
is it s endpoints from different network

00:18:13,520 --> 00:18:17,520
cannot be accessed

00:18:14,799 --> 00:18:19,039
directly masked through a gateway and

00:18:17,520 --> 00:18:22,480
the gateway is accessible

00:18:19,039 --> 00:18:26,640
from other networks so it is hard

00:18:22,480 --> 00:18:30,240
it is issue d who has to convert

00:18:26,640 --> 00:18:33,520
the endpoint address to gateway address

00:18:30,240 --> 00:18:36,960
gui works in alt password mode

00:18:33,520 --> 00:18:40,080
and with sni cluster network filter

00:18:36,960 --> 00:18:42,720
applied on the listener invoice

00:18:40,080 --> 00:18:44,960
listings on the same port and follows

00:18:42,720 --> 00:18:47,679
different

00:18:44,960 --> 00:18:48,240
for different service to service course

00:18:47,679 --> 00:18:51,200
by the

00:18:48,240 --> 00:18:53,760
sni it brings with this it requires the

00:18:51,200 --> 00:18:57,280
closed cluster communication must be

00:18:53,760 --> 00:19:00,400
ts encrypted the sni cluster network

00:18:57,280 --> 00:19:01,360
filter works by setting the upstream

00:19:00,400 --> 00:19:04,480
cluster name

00:19:01,360 --> 00:19:11,600
through the sci field passed from the

00:19:04,480 --> 00:19:14,080
tis handshake

00:19:11,600 --> 00:19:16,960
let's see the single network primary

00:19:14,080 --> 00:19:16,960
remote model

00:19:17,600 --> 00:19:23,679
from the picture we can say that

00:19:20,799 --> 00:19:24,400
this is the class 1 and class two

00:19:23,679 --> 00:19:27,440
classes

00:19:24,400 --> 00:19:30,960
and class to rest are the two clusters

00:19:27,440 --> 00:19:34,320
but they are in the same network

00:19:30,960 --> 00:19:38,640
so service a can talk to

00:19:34,320 --> 00:19:42,080
service b across clusters

00:19:38,640 --> 00:19:44,559
and from the picture we can say that

00:19:42,080 --> 00:19:45,280
there is only one control plane resides

00:19:44,559 --> 00:19:48,640
in

00:19:45,280 --> 00:19:48,640
cluster west

00:19:50,000 --> 00:19:57,440
so this is the primary remote but

00:19:54,400 --> 00:20:00,720
the service discovery

00:19:57,440 --> 00:20:04,000
is done by the sqd is to the

00:20:00,720 --> 00:20:04,320
list to watching the services and points

00:20:04,000 --> 00:20:08,480
from

00:20:04,320 --> 00:20:12,000
all the clusters within the meshes

00:20:08,480 --> 00:20:12,960
the configure discovery it is also the

00:20:12,000 --> 00:20:16,640
sqd

00:20:12,960 --> 00:20:19,120
who is to watches custom resource

00:20:16,640 --> 00:20:20,480
like photo service designation rules

00:20:19,120 --> 00:20:23,679
from private

00:20:20,480 --> 00:20:27,039
primary from

00:20:23,679 --> 00:20:29,280
only primary cluster

00:20:27,039 --> 00:20:30,159
now this is how the single network

00:20:29,280 --> 00:20:34,000
primary

00:20:30,159 --> 00:20:37,840
remote pattern works

00:20:34,000 --> 00:20:41,039
this is a single network multi-primary

00:20:37,840 --> 00:20:43,760
model in this model

00:20:41,039 --> 00:20:46,559
the eco control plan is deployed into

00:20:43,760 --> 00:20:46,559
every cluster

00:20:47,120 --> 00:20:49,840
sorry

00:20:51,679 --> 00:20:55,360
okay the service discovery is like looks

00:20:54,880 --> 00:20:59,039
like

00:20:55,360 --> 00:21:03,200
the same as the single network

00:20:59,039 --> 00:21:06,080
is 2d in each cluster discharges service

00:21:03,200 --> 00:21:09,360
endpoints from all the clusters

00:21:06,080 --> 00:21:12,720
further configure discovery is to

00:21:09,360 --> 00:21:16,559
see in each cluster's list which is

00:21:12,720 --> 00:21:17,120
local or the local cluster customer

00:21:16,559 --> 00:21:21,280
resource

00:21:17,120 --> 00:21:21,280
like virtual service that is usual

00:21:22,559 --> 00:21:29,120
sidecast that means a local

00:21:25,760 --> 00:21:32,799
cluster sidecast can only

00:21:29,120 --> 00:21:35,679
can connect to the local institute

00:21:32,799 --> 00:21:36,960
and the local cluster study is

00:21:35,679 --> 00:21:42,320
responsible

00:21:36,960 --> 00:21:45,280
for pushing x gas to the side class

00:21:42,320 --> 00:21:46,320
this pattern provides more availability

00:21:45,280 --> 00:21:49,679
in the

00:21:46,320 --> 00:21:52,720
resilience because one cluster

00:21:49,679 --> 00:21:56,320
when one cluster fails

00:21:52,720 --> 00:21:59,600
or when one cluster when single

00:21:56,320 --> 00:22:01,840
control plane fields the other clusters

00:21:59,600 --> 00:22:01,840
can

00:22:01,919 --> 00:22:05,039
work as well

00:22:06,240 --> 00:22:13,679
let's see the gateway for the different

00:22:09,280 --> 00:22:16,720
networks ahead

00:22:13,679 --> 00:22:19,679
yeah how can eastern discover recovery

00:22:16,720 --> 00:22:20,720
discovers the east west gateway and is

00:22:19,679 --> 00:22:23,760
requested to do

00:22:20,720 --> 00:22:27,440
also to do auto split

00:22:23,760 --> 00:22:30,559
rising eds this is by the

00:22:27,440 --> 00:22:30,559
name space labels

00:22:30,880 --> 00:22:34,799
the name space labels topology dot sql

00:22:34,400 --> 00:22:39,919
dot

00:22:34,799 --> 00:22:42,240
io network tells is to the local cluster

00:22:39,919 --> 00:22:45,520
is in network one

00:22:42,240 --> 00:22:48,559
and the service labels topology

00:22:45,520 --> 00:22:51,840
dot institute dot io

00:22:48,559 --> 00:22:55,840
slash network tells xqd

00:22:51,840 --> 00:22:59,120
the service in is network

00:22:55,840 --> 00:23:04,000
the the service is network

00:22:59,120 --> 00:23:07,679
is resized residing in network one

00:23:04,000 --> 00:23:11,600
it is the east western gateway service

00:23:07,679 --> 00:23:14,880
so we can get the service ingress ip

00:23:11,600 --> 00:23:17,919
or external ib address to

00:23:14,880 --> 00:23:23,280
get the east west gateway address for

00:23:17,919 --> 00:23:26,240
network one

00:23:23,280 --> 00:23:28,480
and such the ucld can convert the

00:23:26,240 --> 00:23:34,320
endpoints

00:23:28,480 --> 00:23:34,320
for network one with the gateway address

00:23:34,960 --> 00:23:40,080
so what can be improved

00:23:41,039 --> 00:23:47,840
yeah multi-cluster is looks very well

00:23:44,559 --> 00:23:52,799
for and it is already can be used

00:23:47,840 --> 00:23:56,640
in production but there are several

00:23:52,799 --> 00:23:59,200
companies we have to face too

00:23:56,640 --> 00:24:00,640
the first one is better load balancing

00:23:59,200 --> 00:24:04,400
across cluster

00:24:00,640 --> 00:24:05,120
from the right side picture we can see

00:24:04,400 --> 00:24:08,159
that

00:24:05,120 --> 00:24:08,960
green cluster wine service want to talk

00:24:08,159 --> 00:24:12,159
to class

00:24:08,960 --> 00:24:12,159
2 services

00:24:12,240 --> 00:24:15,600
at the easter easter white western

00:24:14,480 --> 00:24:18,720
gallery

00:24:15,600 --> 00:24:22,000
as a tcp proxy

00:24:18,720 --> 00:24:27,200
actually it is a tis proxy

00:24:22,000 --> 00:24:27,200
and the sessions decay between the

00:24:27,360 --> 00:24:34,720
client service and the server space

00:24:30,960 --> 00:24:38,400
is not every article

00:24:34,720 --> 00:24:41,200
because the easter egg

00:24:38,400 --> 00:24:43,279
getaway works on around rubbing load

00:24:41,200 --> 00:24:46,720
balancing

00:24:43,279 --> 00:24:49,200
policy so there is no setting

00:24:46,720 --> 00:24:51,600
session sticky between source and

00:24:49,200 --> 00:24:55,120
desktops

00:24:51,600 --> 00:24:59,679
and this is easier in

00:24:55,120 --> 00:25:02,320
github we can see more details there

00:24:59,679 --> 00:25:02,880
the second one is the holiday service

00:25:02,320 --> 00:25:06,080
does not

00:25:02,880 --> 00:25:10,799
work well that means

00:25:06,080 --> 00:25:14,640
when we want to access a healthy service

00:25:10,799 --> 00:25:18,720
to like it's the right

00:25:14,640 --> 00:25:22,320
picture if we want to access

00:25:18,720 --> 00:25:23,760
from the class to one to a remote

00:25:22,320 --> 00:25:28,080
headlight service

00:25:23,760 --> 00:25:32,080
that is deployed in class two

00:25:28,080 --> 00:25:35,360
it is not possible now because

00:25:32,080 --> 00:25:38,799
that's the hiding service instance

00:25:35,360 --> 00:25:42,720
dns result resulted to the portal

00:25:38,799 --> 00:25:46,159
called address as the cluster type

00:25:42,720 --> 00:25:49,360
is using the original desktop

00:25:46,159 --> 00:25:52,559
so the if we

00:25:49,360 --> 00:25:53,120
access the port ip address from the

00:25:52,559 --> 00:25:56,559
local

00:25:53,120 --> 00:25:59,919
cluster cost one here

00:25:56,559 --> 00:26:01,120
the traffic is blind text and it cannot

00:25:59,919 --> 00:26:05,679
be processed

00:26:01,120 --> 00:26:05,679
proxied by the green

00:26:06,080 --> 00:26:12,720
the third one is the cluster client

00:26:09,200 --> 00:26:16,640
must send ts

00:26:12,720 --> 00:26:20,159
rightly so if we want to

00:26:16,640 --> 00:26:24,000
make the healthy service works firstly

00:26:20,159 --> 00:26:27,279
we should make the easter

00:26:24,000 --> 00:26:30,559
west gateway worker

00:26:27,279 --> 00:26:33,919
for plant text not only for trs

00:26:30,559 --> 00:26:37,679
encrypt traffic the last one

00:26:33,919 --> 00:26:41,039
is a network or class to aware

00:26:37,679 --> 00:26:45,200
load balancing because for multi

00:26:41,039 --> 00:26:48,799
multi networks multi-cluster model

00:26:45,200 --> 00:26:52,000
the traffic is funded by the

00:26:48,799 --> 00:26:55,360
east west gateway so in order

00:26:52,000 --> 00:26:58,720
to reduce so this increase is

00:26:55,360 --> 00:27:02,880
the service to service licensing

00:26:58,720 --> 00:27:05,600
because the trafficker is through

00:27:02,880 --> 00:27:07,840
an additional piece to us to get away

00:27:05,600 --> 00:27:07,840
hope

00:27:08,400 --> 00:27:16,159
so we should increase

00:27:11,919 --> 00:27:19,279
improve the performance by

00:27:16,159 --> 00:27:20,960
enable bias supporting network or

00:27:19,279 --> 00:27:24,000
cluster aware localities

00:27:20,960 --> 00:27:27,039
so with network

00:27:24,000 --> 00:27:30,559
aware messaging the traffic from

00:27:27,039 --> 00:27:33,840
local cluster will

00:27:30,559 --> 00:27:34,799
forward it to the lower class services

00:27:33,840 --> 00:27:38,320
firstly

00:27:34,799 --> 00:27:40,080
and then when when the local cluster

00:27:38,320 --> 00:27:42,559
service is

00:27:40,080 --> 00:27:43,520
filled down the traffic will fail over

00:27:42,559 --> 00:27:46,840
to the

00:27:43,520 --> 00:27:49,840
remote cluster or remote network

00:27:46,840 --> 00:27:49,840
clusters

00:27:50,480 --> 00:27:54,799
okay thank you this is all the all

00:27:53,360 --> 00:27:57,360
today's

00:27:54,799 --> 00:27:58,440
topic thank you everyone thank you for

00:27:57,360 --> 00:28:01,440
your time

00:27:58,440 --> 00:28:01,440

YouTube URL: https://www.youtube.com/watch?v=aO6syFca6BE


