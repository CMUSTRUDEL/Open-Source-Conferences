Title: Rapid Experimentation Simplified with Linkerd - Alexander Simon Jones, DevOps Institute Ambassador
Publication date: 2021-05-05
Playlist: ServiceMeshCon EU 2021
Description: 
	Donâ€™t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

Rapid Experimentation Simplified with Linkerd - Alexander Simon Jones, DevOps Institute Ambassador

The cloud engineering team of a multinational financial services corporation used Linkerd to introduce rapid experimentation.
It enabled them to create more resilient services and comparatively test changes. In this session, we'll explore how they did it and lessons learned.
With the proliferation of microservices architectures, developers need to rapidly set up prod-like dev environments locally.
Fortunately, local Kubernetes clusters to model high-scale, complex architecture within an enterprise are increasingly common.
This enables engineers to work with local abstractions of much larger systems.

Within this paradigm, service meshes also need representation locally as it has intrinsic differences to running a vanilla network layer within Kubernetes.
To speed up and ease experimentation, the service mesh must be simple and easy to use. A positive developer experience is key.
Captions: 
	00:00:00,160 --> 00:00:04,400
hello service mesh con europe 2021

00:00:02,879 --> 00:00:05,279
welcome to my talk about rapid

00:00:04,400 --> 00:00:07,919
experimentation

00:00:05,279 --> 00:00:10,080
simplify linkedin my name is alex jones

00:00:07,919 --> 00:00:11,679
i'm the principal engineer at sivo

00:00:10,080 --> 00:00:13,759
sivo is a cloud computing company

00:00:11,679 --> 00:00:15,360
focusing on k3s kubernetes

00:00:13,759 --> 00:00:17,359
and really driving home developer

00:00:15,360 --> 00:00:18,800
experience as a first class citizen

00:00:17,359 --> 00:00:21,520
in my former life i've worked at

00:00:18,800 --> 00:00:22,160
microsoft bskyb jpmorgan american

00:00:21,520 --> 00:00:24,400
express

00:00:22,160 --> 00:00:25,599
to name a few and really the financial

00:00:24,400 --> 00:00:26,880
services industry is what i'm going to

00:00:25,599 --> 00:00:28,720
focus on today because

00:00:26,880 --> 00:00:30,480
they have a lot of problems that are

00:00:28,720 --> 00:00:32,800
really compounded by the fact that

00:00:30,480 --> 00:00:33,600
delivering versions of application

00:00:32,800 --> 00:00:36,000
software

00:00:33,600 --> 00:00:37,040
is very slow and on top of that

00:00:36,000 --> 00:00:39,600
comparing the changes

00:00:37,040 --> 00:00:41,680
is very difficult and so this is why the

00:00:39,600 --> 00:00:42,719
talk of experimentation is so pertinent

00:00:41,680 --> 00:00:43,440
right now because a lot of these

00:00:42,719 --> 00:00:44,800
businesses

00:00:43,440 --> 00:00:46,960
are going through a transformational

00:00:44,800 --> 00:00:47,680
process where they're trying to enable

00:00:46,960 --> 00:00:49,600
engineers

00:00:47,680 --> 00:00:50,879
to have the tools they require to be

00:00:49,600 --> 00:00:51,600
able to perform these low-cost

00:00:50,879 --> 00:00:53,440
experiments

00:00:51,600 --> 00:00:54,640
to determine whether the feature change

00:00:53,440 --> 00:00:57,120
is going to cause an application

00:00:54,640 --> 00:00:58,320
impact right so the agenda for today is

00:00:57,120 --> 00:01:00,640
we're going to talk about

00:00:58,320 --> 00:01:01,440
why is there a need for experimentation

00:01:00,640 --> 00:01:03,920
why do

00:01:01,440 --> 00:01:05,760
firms invest in tooling and why are

00:01:03,920 --> 00:01:06,560
things like linker d becoming extremely

00:01:05,760 --> 00:01:09,439
exciting

00:01:06,560 --> 00:01:11,280
for very low bar to entry ways of

00:01:09,439 --> 00:01:12,640
measuring those experiments and testing

00:01:11,280 --> 00:01:14,320
hypotheses

00:01:12,640 --> 00:01:15,759
the apparatus of that experimentation

00:01:14,320 --> 00:01:16,640
the technical side of how is this

00:01:15,759 --> 00:01:18,640
implemented

00:01:16,640 --> 00:01:20,080
how difficult is it to use and what are

00:01:18,640 --> 00:01:20,560
the kind of things i can do you know is

00:01:20,080 --> 00:01:22,240
that a b

00:01:20,560 --> 00:01:23,840
testing is that chaos testing is that

00:01:22,240 --> 00:01:26,400
gonna be canary testing

00:01:23,840 --> 00:01:28,000
and thirdly what is the implication of

00:01:26,400 --> 00:01:31,520
lowering the bar to entry

00:01:28,000 --> 00:01:34,159
on this kind of form of experimentation

00:01:31,520 --> 00:01:35,439
and so before we go any further it's

00:01:34,159 --> 00:01:36,799
really important to set the scene in

00:01:35,439 --> 00:01:38,079
terms of why is there a need for

00:01:36,799 --> 00:01:40,079
experimentation

00:01:38,079 --> 00:01:42,079
well if we think about this as a simple

00:01:40,079 --> 00:01:43,759
example here you have a v 1.1 all the

00:01:42,079 --> 00:01:44,799
way to 1.4

00:01:43,759 --> 00:01:46,720
let's say that you're a product

00:01:44,799 --> 00:01:48,079
engineering team and you are rolling

00:01:46,720 --> 00:01:49,280
down the tracks building out these

00:01:48,079 --> 00:01:51,520
version changes now

00:01:49,280 --> 00:01:52,320
what happens along here is that we start

00:01:51,520 --> 00:01:54,560
to see

00:01:52,320 --> 00:01:56,240
our sre team are telling us that hey the

00:01:54,560 --> 00:01:57,759
latency of your application is

00:01:56,240 --> 00:02:00,079
increasing over time

00:01:57,759 --> 00:02:01,840
this is a very coarse-grained approach

00:02:00,079 --> 00:02:02,799
of understanding the infrastructure

00:02:01,840 --> 00:02:05,200
footprint

00:02:02,799 --> 00:02:06,159
impact of our application right whether

00:02:05,200 --> 00:02:08,000
that's compute

00:02:06,159 --> 00:02:09,280
whether that's manifested as iops or

00:02:08,000 --> 00:02:11,599
some other

00:02:09,280 --> 00:02:12,480
signal we are making a change in the

00:02:11,599 --> 00:02:15,120
environment

00:02:12,480 --> 00:02:17,440
which is going to cause additional

00:02:15,120 --> 00:02:19,360
outcomes over time in terms of how that

00:02:17,440 --> 00:02:20,560
microservice or application interacts

00:02:19,360 --> 00:02:22,400
with other systems

00:02:20,560 --> 00:02:24,239
and so therefore it makes a lot of

00:02:22,400 --> 00:02:25,840
common sense that we want to measure

00:02:24,239 --> 00:02:27,840
scientifically what is the delta of

00:02:25,840 --> 00:02:28,959
change not just in code but also in

00:02:27,840 --> 00:02:30,560
performance right

00:02:28,959 --> 00:02:32,239
and so i think about this in terms of

00:02:30,560 --> 00:02:32,959
reduced signals as well and i'll come on

00:02:32,239 --> 00:02:35,920
to describe

00:02:32,959 --> 00:02:37,200
what i mean by that in a moment a

00:02:35,920 --> 00:02:39,599
secondary example

00:02:37,200 --> 00:02:40,400
is how does that application or service

00:02:39,599 --> 00:02:42,480
interact

00:02:40,400 --> 00:02:44,160
in a complex environment if we're

00:02:42,480 --> 00:02:45,040
changing the version of several micro

00:02:44,160 --> 00:02:46,720
services

00:02:45,040 --> 00:02:49,040
how do we know scientifically what the

00:02:46,720 --> 00:02:51,440
change will be to a queuing mechanism

00:02:49,040 --> 00:02:52,480
and how do we understand the knock-on

00:02:51,440 --> 00:02:54,000
implication

00:02:52,480 --> 00:02:56,640
and i think this shows also that there's

00:02:54,000 --> 00:02:59,519
a real need to be able to

00:02:56,640 --> 00:03:01,040
not only inject faults but to understand

00:02:59,519 --> 00:03:02,480
when there's service degradation how

00:03:01,040 --> 00:03:04,640
does the environment perform

00:03:02,480 --> 00:03:06,560
there's often a joke that you know the

00:03:04,640 --> 00:03:07,440
disaster recovery plan is nothing like

00:03:06,560 --> 00:03:08,720
what it looks like

00:03:07,440 --> 00:03:10,800
when you actually have to perform it

00:03:08,720 --> 00:03:11,120
that's because that it's so high cost in

00:03:10,800 --> 00:03:12,879
many

00:03:11,120 --> 00:03:14,640
organizations to actually perform a dr

00:03:12,879 --> 00:03:15,920
exercise that many of these things are

00:03:14,640 --> 00:03:19,360
just existential

00:03:15,920 --> 00:03:20,319
and aren't really testable another part

00:03:19,360 --> 00:03:21,680
of this as well

00:03:20,319 --> 00:03:23,440
and illustrated by the previous two

00:03:21,680 --> 00:03:24,080
examples is that a b testing should be

00:03:23,440 --> 00:03:25,840
easy right

00:03:24,080 --> 00:03:27,599
this idea of having some change and

00:03:25,840 --> 00:03:29,120
being able to test it should be easy

00:03:27,599 --> 00:03:31,040
the problem with this sort of faz based

00:03:29,120 --> 00:03:32,400
example here is that between these two

00:03:31,040 --> 00:03:34,319
functions

00:03:32,400 --> 00:03:35,840
essentially performing a failover right

00:03:34,319 --> 00:03:38,959
i have to test the new

00:03:35,840 --> 00:03:39,599
optimized database table against a new

00:03:38,959 --> 00:03:41,200
function

00:03:39,599 --> 00:03:42,560
and then fail back again and this could

00:03:41,200 --> 00:03:44,080
even be a code change right this could

00:03:42,560 --> 00:03:45,519
be an optimization in the code not

00:03:44,080 --> 00:03:47,440
necessarily the database

00:03:45,519 --> 00:03:48,879
but the idea is you have to manually

00:03:47,440 --> 00:03:49,280
change the service whether that's

00:03:48,879 --> 00:03:51,120
through

00:03:49,280 --> 00:03:52,319
automated deployment config or some

00:03:51,120 --> 00:03:53,439
other human activity

00:03:52,319 --> 00:03:55,920
it's not something you run

00:03:53,439 --> 00:03:57,439
simultaneously equally this becomes even

00:03:55,920 --> 00:04:00,400
more compounded when you want to run

00:03:57,439 --> 00:04:01,680
say 20 to 30 faz changes with many small

00:04:00,400 --> 00:04:03,439
nuances between them

00:04:01,680 --> 00:04:05,599
so a b testing needs to be easy and this

00:04:03,439 --> 00:04:07,840
is a really really difficult problem to

00:04:05,599 --> 00:04:12,159
solve for at enterprise in a safe

00:04:07,840 --> 00:04:13,760
and scalable way but wait a minute right

00:04:12,159 --> 00:04:15,040
many people will tell me hey well in our

00:04:13,760 --> 00:04:16,799
environment we can deploy multiple

00:04:15,040 --> 00:04:17,519
versions we have no problems whatsoever

00:04:16,799 --> 00:04:20,239
we can have

00:04:17,519 --> 00:04:22,400
you know 1.1 1.2 et cetera on branches

00:04:20,239 --> 00:04:23,919
prs yada yada that's fine

00:04:22,400 --> 00:04:26,240
but let's really break that down right

00:04:23,919 --> 00:04:27,040
so you have your micro service alpha v1

00:04:26,240 --> 00:04:28,880
and v2

00:04:27,040 --> 00:04:30,639
the delta change is just the code and we

00:04:28,880 --> 00:04:32,400
go through the typical kind of

00:04:30,639 --> 00:04:33,840
routine of you know committing that code

00:04:32,400 --> 00:04:35,840
change we deploy that

00:04:33,840 --> 00:04:36,960
through a pipeline we get a new replica

00:04:35,840 --> 00:04:39,680
set

00:04:36,960 --> 00:04:41,600
uh yeah that's great but how do we then

00:04:39,680 --> 00:04:43,040
see what changes there are between these

00:04:41,600 --> 00:04:44,960
we have to do some sort of activity

00:04:43,040 --> 00:04:47,120
where we observe the prior state

00:04:44,960 --> 00:04:48,639
and then look at the new state and so we

00:04:47,120 --> 00:04:50,080
can look at a pattern of change

00:04:48,639 --> 00:04:51,680
and determine that there's been a

00:04:50,080 --> 00:04:52,960
regression okay then

00:04:51,680 --> 00:04:54,800
if there's a regression we have to go

00:04:52,960 --> 00:04:56,000
back to the drawing board and we have to

00:04:54,800 --> 00:04:58,639
figure out what that regression

00:04:56,000 --> 00:05:00,560
is that could be latency that could be

00:04:58,639 --> 00:05:01,759
saturation some other penalty we have to

00:05:00,560 --> 00:05:03,120
pay

00:05:01,759 --> 00:05:05,440
the point i'm trying to illustrate here

00:05:03,120 --> 00:05:06,479
is that the cycle is fairly long and it

00:05:05,440 --> 00:05:08,400
also means that

00:05:06,479 --> 00:05:09,520
it's extremely arduous to do across

00:05:08,400 --> 00:05:11,840
multiple versions

00:05:09,520 --> 00:05:14,000
because that's just comparing v1 to v2

00:05:11,840 --> 00:05:17,440
we should be able to compare v1 to v3

00:05:14,000 --> 00:05:19,280
and v3 to v2 simultaneously

00:05:17,440 --> 00:05:21,280
so the challenge is probably quite clear

00:05:19,280 --> 00:05:23,759
by now in that it's expensive

00:05:21,280 --> 00:05:24,400
not only in in monetary terms but in

00:05:23,759 --> 00:05:26,240
time

00:05:24,400 --> 00:05:28,720
to promote changes to a new environment

00:05:26,240 --> 00:05:30,479
especially within financial institutions

00:05:28,720 --> 00:05:31,759
multi-stage multi-dependency chain of

00:05:30,479 --> 00:05:34,000
promotion is

00:05:31,759 --> 00:05:35,759
a really big overhead to have to bear to

00:05:34,000 --> 00:05:37,600
test what i would call a micro version

00:05:35,759 --> 00:05:39,759
right like a small bet

00:05:37,600 --> 00:05:40,720
observability of these small bets has to

00:05:39,759 --> 00:05:42,720
be targeted

00:05:40,720 --> 00:05:44,160
a lot of the labeling systems that you

00:05:42,720 --> 00:05:46,400
get out of the box or in these

00:05:44,160 --> 00:05:48,240
organizations aren't dynamic enough to

00:05:46,400 --> 00:05:48,639
be able to determine these micro changes

00:05:48,240 --> 00:05:50,880
now

00:05:48,639 --> 00:05:51,759
whether that's a suffix on a version

00:05:50,880 --> 00:05:53,680
whether that is

00:05:51,759 --> 00:05:55,280
a shar on an image we need a way of

00:05:53,680 --> 00:05:56,000
being able to pin the difference between

00:05:55,280 --> 00:05:58,720
changes

00:05:56,000 --> 00:06:00,720
and then to measure those over time and

00:05:58,720 --> 00:06:02,479
with all that said it probably really

00:06:00,720 --> 00:06:04,080
makes it clear that this is complex

00:06:02,479 --> 00:06:05,520
right this this is a complex and often

00:06:04,080 --> 00:06:07,600
impractical

00:06:05,520 --> 00:06:08,639
set of ideas to try and bring across to

00:06:07,600 --> 00:06:09,919
an organization where

00:06:08,639 --> 00:06:11,440
they don't know where to start right

00:06:09,919 --> 00:06:12,319
there might be an application team who

00:06:11,440 --> 00:06:13,919
are delivering

00:06:12,319 --> 00:06:15,840
20 different micro services and each one

00:06:13,919 --> 00:06:16,800
of those micro services might have a

00:06:15,840 --> 00:06:18,479
bunch of branches

00:06:16,800 --> 00:06:20,000
with a bunch of changes in each branch

00:06:18,479 --> 00:06:22,080
how do you determine

00:06:20,000 --> 00:06:25,120
which branch is introducing a regression

00:06:22,080 --> 00:06:27,440
in terms of infrastructure performance

00:06:25,120 --> 00:06:28,160
so let's take a step back and just think

00:06:27,440 --> 00:06:30,960
about

00:06:28,160 --> 00:06:32,479
distillating those requirements here's

00:06:30,960 --> 00:06:34,080
your typical

00:06:32,479 --> 00:06:35,759
infrastructure architecture right you've

00:06:34,080 --> 00:06:37,199
got an application that has a

00:06:35,759 --> 00:06:38,560
microservice and a queue

00:06:37,199 --> 00:06:40,960
and it might create something in a

00:06:38,560 --> 00:06:42,080
database it stands to reason that we

00:06:40,960 --> 00:06:44,160
should be able to test

00:06:42,080 --> 00:06:45,199
in real time an alternative vision to

00:06:44,160 --> 00:06:46,880
this architecture

00:06:45,199 --> 00:06:48,319
and in this case it's direct calling the

00:06:46,880 --> 00:06:50,720
database right so

00:06:48,319 --> 00:06:52,160
this is fairly well known and well

00:06:50,720 --> 00:06:54,639
trodden this kind of path but we

00:06:52,160 --> 00:06:56,639
find it difficult to do because it's

00:06:54,639 --> 00:06:58,400
hard to be able to tell the api gateway

00:06:56,639 --> 00:07:00,479
to send data to both of these without a

00:06:58,400 --> 00:07:02,319
code change in the gateway

00:07:00,479 --> 00:07:03,840
and again that is introducing more

00:07:02,319 --> 00:07:05,520
change that means that there are more

00:07:03,840 --> 00:07:07,759
unknowns to measure

00:07:05,520 --> 00:07:09,199
equally we should be able to understand

00:07:07,759 --> 00:07:09,919
what happens if this service starts

00:07:09,199 --> 00:07:11,520
faulting

00:07:09,919 --> 00:07:12,960
without actually having to codify a

00:07:11,520 --> 00:07:15,680
fault into the service

00:07:12,960 --> 00:07:17,680
and so therefore there needs to be a way

00:07:15,680 --> 00:07:20,240
thinking back to our disaster recovery

00:07:17,680 --> 00:07:21,039
illustration how we can start to bring

00:07:20,240 --> 00:07:23,360
failures

00:07:21,039 --> 00:07:25,599
and chaos into the system and build more

00:07:23,360 --> 00:07:28,720
resilient systems

00:07:25,599 --> 00:07:30,080
and lastly observing the difference

00:07:28,720 --> 00:07:32,160
across generations

00:07:30,080 --> 00:07:33,520
is really paramount to this succeeding

00:07:32,160 --> 00:07:35,120
we can do all this stuff but if we can't

00:07:33,520 --> 00:07:35,840
observe it in a way that isn't super

00:07:35,120 --> 00:07:38,639
coarse grain

00:07:35,840 --> 00:07:39,680
then it's pointless how do we bring rate

00:07:38,639 --> 00:07:41,840
errors duration

00:07:39,680 --> 00:07:43,759
utilization saturation signals all to

00:07:41,840 --> 00:07:44,560
the table and say between the version

00:07:43,759 --> 00:07:46,800
one and three

00:07:44,560 --> 00:07:47,840
there's a massive regression right these

00:07:46,800 --> 00:07:50,639
are things that we need to be able to

00:07:47,840 --> 00:07:52,160
understand and understand how to measure

00:07:50,639 --> 00:07:53,680
so that brings me to the apparatus of

00:07:52,160 --> 00:07:55,680
this experimentation

00:07:53,680 --> 00:07:56,879
and really after looking at a lot of

00:07:55,680 --> 00:07:58,560
different solutions

00:07:56,879 --> 00:07:59,919
what we settled on time and time again

00:07:58,560 --> 00:08:02,080
was linker d

00:07:59,919 --> 00:08:03,759
the two key tenets of this traffic

00:08:02,080 --> 00:08:06,160
splitting and observability

00:08:03,759 --> 00:08:07,680
both of which are underpinned by super

00:08:06,160 --> 00:08:10,080
easy to use dx

00:08:07,680 --> 00:08:12,080
that time and time again have saved us a

00:08:10,080 --> 00:08:14,319
ton of effort by just working out of the

00:08:12,080 --> 00:08:16,639
box

00:08:14,319 --> 00:08:17,919
when we think about especially how these

00:08:16,639 --> 00:08:19,599
things work

00:08:17,919 --> 00:08:21,120
it reminds me that there's a lot of

00:08:19,599 --> 00:08:24,560
effort that's been put in at the

00:08:21,120 --> 00:08:25,440
smi spec level from the cnc f6 who are

00:08:24,560 --> 00:08:26,639
caring about

00:08:25,440 --> 00:08:28,800
the future of these kind of

00:08:26,639 --> 00:08:30,400
implementations and how the end users

00:08:28,800 --> 00:08:31,919
are going to work with them and that's

00:08:30,400 --> 00:08:32,719
very much appreciated because when we

00:08:31,919 --> 00:08:34,959
look at

00:08:32,719 --> 00:08:36,640
the customer resource definition for how

00:08:34,959 --> 00:08:38,320
a traffic split should work

00:08:36,640 --> 00:08:40,000
it's super easy to understand that in

00:08:38,320 --> 00:08:42,719
this example there's 90

00:08:40,000 --> 00:08:43,680
traffic balance to the v1 versus 10 on

00:08:42,719 --> 00:08:46,000
the v2

00:08:43,680 --> 00:08:47,519
equally the alpha v4 of traffic

00:08:46,000 --> 00:08:48,880
splitting is taking us in a direction we

00:08:47,519 --> 00:08:50,320
can start to perform front-end

00:08:48,880 --> 00:08:52,800
application testing

00:08:50,320 --> 00:08:54,560
inside of the mesh and that's exciting

00:08:52,800 --> 00:08:56,000
because we can start to define headers

00:08:54,560 --> 00:08:57,920
that we care about and in this example

00:08:56,000 --> 00:09:00,399
it's a user agent of firefox

00:08:57,920 --> 00:09:01,519
so super exciting future going forward

00:09:00,399 --> 00:09:04,000
for enabling

00:09:01,519 --> 00:09:06,560
a b testing within linkedin and other

00:09:04,000 --> 00:09:06,560
smi

00:09:06,800 --> 00:09:10,320
when we think about another big feature

00:09:08,480 --> 00:09:11,200
of linkedin and this idea of traffic

00:09:10,320 --> 00:09:12,560
splitting

00:09:11,200 --> 00:09:13,839
it's about visualizing that data and

00:09:12,560 --> 00:09:14,399
about developer experience and i

00:09:13,839 --> 00:09:15,440
mentioned

00:09:14,399 --> 00:09:17,760
two or three times already that

00:09:15,440 --> 00:09:19,040
developer experience is super important

00:09:17,760 --> 00:09:20,000
because when you have five or six

00:09:19,040 --> 00:09:21,440
hundred teams

00:09:20,000 --> 00:09:23,040
and you amplify that by the amount of

00:09:21,440 --> 00:09:24,480
developers on that as teams and they're

00:09:23,040 --> 00:09:25,760
all trying to work with mesh

00:09:24,480 --> 00:09:28,240
their level of experience is going to

00:09:25,760 --> 00:09:30,560
vary vastly and so when you have

00:09:28,240 --> 00:09:32,320
super crisp dashboarding and

00:09:30,560 --> 00:09:34,560
visualizations of what's going on

00:09:32,320 --> 00:09:36,080
then it makes everyone's lives easier

00:09:34,560 --> 00:09:36,880
and this is great because you can double

00:09:36,080 --> 00:09:38,240
click into this

00:09:36,880 --> 00:09:39,600
so that if you are an engineer that

00:09:38,240 --> 00:09:41,040
wants to understand like hey what's

00:09:39,600 --> 00:09:42,880
going on with the response codes

00:09:41,040 --> 00:09:44,480
what's going on with like the internal

00:09:42,880 --> 00:09:47,680
host headers you can do that

00:09:44,480 --> 00:09:48,880
within the mesh within the kd equally

00:09:47,680 --> 00:09:50,720
for sres

00:09:48,880 --> 00:09:52,720
there's that super deeply ingrained

00:09:50,720 --> 00:09:54,320
prometheus and grafana installation that

00:09:52,720 --> 00:09:54,800
lets you be a bit more scientific over

00:09:54,320 --> 00:09:56,640
time

00:09:54,800 --> 00:09:58,080
it's one thing just to deploy a service

00:09:56,640 --> 00:09:58,880
and say yeah okay it's introduced some

00:09:58,080 --> 00:10:01,279
latency

00:09:58,880 --> 00:10:02,720
it's another thing to then start pushing

00:10:01,279 --> 00:10:04,399
a load into that service and looking at

00:10:02,720 --> 00:10:06,560
how it performs um

00:10:04,399 --> 00:10:08,240
you know compared to its prior

00:10:06,560 --> 00:10:10,800
generations

00:10:08,240 --> 00:10:11,360
so let's let's look at a small demo now

00:10:10,800 --> 00:10:14,399
so

00:10:11,360 --> 00:10:16,800
i've got this uh linkedin demo

00:10:14,399 --> 00:10:18,640
repository right here i've got a client

00:10:16,800 --> 00:10:22,320
that calls a version

00:10:18,640 --> 00:10:24,480
and that version is the one two or three

00:10:22,320 --> 00:10:25,760
right and so what this client does is it

00:10:24,480 --> 00:10:28,079
creates a user

00:10:25,760 --> 00:10:29,200
uh the user will then just sit in memory

00:10:28,079 --> 00:10:29,760
now the difference between one two and

00:10:29,200 --> 00:10:31,360
three

00:10:29,760 --> 00:10:33,279
is pretty small but i'll show you so

00:10:31,360 --> 00:10:34,000
version two i've changed the swagger

00:10:33,279 --> 00:10:35,519
spec

00:10:34,000 --> 00:10:37,120
and this is so that we can represent

00:10:35,519 --> 00:10:37,680
some code change that an engineer might

00:10:37,120 --> 00:10:39,760
make

00:10:37,680 --> 00:10:41,360
and in this case it's adding a required

00:10:39,760 --> 00:10:43,040
food preference field now

00:10:41,360 --> 00:10:44,959
my client is super dumb and all it's

00:10:43,040 --> 00:10:46,160
doing is hitting with the default user

00:10:44,959 --> 00:10:47,920
field so what's going to happen

00:10:46,160 --> 00:10:49,600
is i'm going to get a 422 because it's

00:10:47,920 --> 00:10:51,519
going to say hey i can't process this i

00:10:49,600 --> 00:10:54,079
don't understand where's my food field

00:10:51,519 --> 00:10:54,880
equally in the open api v3 what's going

00:10:54,079 --> 00:10:57,200
to happen

00:10:54,880 --> 00:10:58,480
is that i've introduced some latency

00:10:57,200 --> 00:10:59,760
right i've just put some times dot

00:10:58,480 --> 00:11:02,079
sleeps all over the code

00:10:59,760 --> 00:11:03,760
so that we can sort of emulate what

00:11:02,079 --> 00:11:06,160
would happen if there was a real

00:11:03,760 --> 00:11:07,600
real sort of regression introduced into

00:11:06,160 --> 00:11:09,839
that service

00:11:07,600 --> 00:11:11,839
and what brings this all together is the

00:11:09,839 --> 00:11:14,320
traffic split so the default behavior of

00:11:11,839 --> 00:11:15,680
the open api client is to hit this v1

00:11:14,320 --> 00:11:17,440
but what we're saying now is hey

00:11:15,680 --> 00:11:20,000
actually i want to balance equally

00:11:17,440 --> 00:11:20,560
between the v2 and the v3 so let's go

00:11:20,000 --> 00:11:23,360
ahead

00:11:20,560 --> 00:11:23,360
and deploy that

00:11:25,360 --> 00:11:29,600
cool so i want to visualize this

00:11:27,440 --> 00:11:32,720
naturally so let's go link to the

00:11:29,600 --> 00:11:34,640
there's dashboard and if we go into here

00:11:32,720 --> 00:11:35,920
we can see the default behavior as

00:11:34,640 --> 00:11:37,839
expected open

00:11:35,920 --> 00:11:40,160
open api client is hitting the v1 back

00:11:37,839 --> 00:11:41,440
end we can see though that the traffic

00:11:40,160 --> 00:11:42,959
splits has come online

00:11:41,440 --> 00:11:44,800
and we have some prior data for the

00:11:42,959 --> 00:11:45,279
existing service and what will soon

00:11:44,800 --> 00:11:47,760
happen

00:11:45,279 --> 00:11:49,120
is as the live data starts to come

00:11:47,760 --> 00:11:50,399
through from the new routes

00:11:49,120 --> 00:11:51,920
we'll see these fields start to get

00:11:50,399 --> 00:11:52,639
populated so you can see ones come up

00:11:51,920 --> 00:11:54,639
right there

00:11:52,639 --> 00:11:56,079
and what's really really exciting and

00:11:54,639 --> 00:11:57,200
useful about this is that it just

00:11:56,079 --> 00:11:59,120
works right there's no additional

00:11:57,200 --> 00:12:01,360
configuration it was just what you saw i

00:11:59,120 --> 00:12:03,519
just applied that crd and away we go

00:12:01,360 --> 00:12:04,959
and now what i'm interested to see is is

00:12:03,519 --> 00:12:05,680
there going to be more latency on this

00:12:04,959 --> 00:12:07,279
route because

00:12:05,680 --> 00:12:09,519
as we saw i've added in all sorts of

00:12:07,279 --> 00:12:10,000
sleeps and we've seen that user creation

00:12:09,519 --> 00:12:12,000
as it gets

00:12:10,000 --> 00:12:13,680
round robin between these should start

00:12:12,000 --> 00:12:16,399
to get slower on this route and

00:12:13,680 --> 00:12:19,279
hey presto we can see almost 10 seconds

00:12:16,399 --> 00:12:20,800
at the p99 on this route coming through

00:12:19,279 --> 00:12:22,480
in addition to this i could say okay

00:12:20,800 --> 00:12:23,760
well what's the what's the history of

00:12:22,480 --> 00:12:25,839
this v3 api

00:12:23,760 --> 00:12:27,360
and again clicking through to grafana

00:12:25,839 --> 00:12:29,040
it's great i can look and see

00:12:27,360 --> 00:12:30,639
oh you know what i can see over the past

00:12:29,040 --> 00:12:32,720
couple of minutes this is

00:12:30,639 --> 00:12:35,040
um you know something that i've known to

00:12:32,720 --> 00:12:36,240
be a behavioral change in this service

00:12:35,040 --> 00:12:37,040
or this is something that's absolutely

00:12:36,240 --> 00:12:38,639
fine

00:12:37,040 --> 00:12:41,040
i love the idea that you can combine

00:12:38,639 --> 00:12:42,639
this with existing dashboards right so

00:12:41,040 --> 00:12:43,279
if i'm working in a compute constrained

00:12:42,639 --> 00:12:44,959
environment

00:12:43,279 --> 00:12:46,959
and i'm deploying a hundred different

00:12:44,959 --> 00:12:48,160
micro bets i can have a default

00:12:46,959 --> 00:12:50,720
dashboard that i add in

00:12:48,160 --> 00:12:52,560
that focuses purely on compute so that

00:12:50,720 --> 00:12:54,480
when i have all these different services

00:12:52,560 --> 00:12:55,760
and if i refresh now i should see them

00:12:54,480 --> 00:12:56,880
um i can tell

00:12:55,760 --> 00:12:58,959
which one is going to be the most

00:12:56,880 --> 00:13:00,160
performant change right and so these

00:12:58,959 --> 00:13:01,600
three could represent three different

00:13:00,160 --> 00:13:03,440
hashing algorithms you know it could be

00:13:01,600 --> 00:13:07,360
anything that you want to try to test

00:13:03,440 --> 00:13:09,600
in a very low barter entryway

00:13:07,360 --> 00:13:10,800
so when we come back to thinking about

00:13:09,600 --> 00:13:12,000
what is this actually done in terms of

00:13:10,800 --> 00:13:15,279
lowering the bar

00:13:12,000 --> 00:13:16,480
to test a hypothesis well you know we're

00:13:15,279 --> 00:13:18,160
essentially creating an experiment

00:13:16,480 --> 00:13:19,680
factory we're creating the ability for

00:13:18,160 --> 00:13:20,959
developers on their local machine in

00:13:19,680 --> 00:13:21,760
their lower environments wherever they

00:13:20,959 --> 00:13:24,000
might be

00:13:21,760 --> 00:13:25,440
to put a load of different small changes

00:13:24,000 --> 00:13:26,000
out there and test the ones that work

00:13:25,440 --> 00:13:27,920
the best

00:13:26,000 --> 00:13:29,839
and when you start cascading this to not

00:13:27,920 --> 00:13:31,120
just a single micro service but multiple

00:13:29,839 --> 00:13:33,360
micro services

00:13:31,120 --> 00:13:34,160
then it gets really exciting because at

00:13:33,360 --> 00:13:36,480
that point

00:13:34,160 --> 00:13:37,360
you can start combining it with chaos

00:13:36,480 --> 00:13:39,839
testing

00:13:37,360 --> 00:13:40,800
so it's almost like an evolutionary

00:13:39,839 --> 00:13:42,639
darwinism

00:13:40,800 --> 00:13:44,560
of microservices where you're starting

00:13:42,639 --> 00:13:46,959
to see which one survives the best

00:13:44,560 --> 00:13:49,199
so in this example let's say alpha v1

00:13:46,959 --> 00:13:50,720
and v2 actually have different roots

00:13:49,199 --> 00:13:52,320
but what we can see is that the

00:13:50,720 --> 00:13:54,399
implication of those roots

00:13:52,320 --> 00:13:55,680
can be quite significant in terms of

00:13:54,399 --> 00:13:57,199
which is more resilient

00:13:55,680 --> 00:13:58,800
and so we might find that you know what

00:13:57,199 --> 00:14:00,959
actually alpha v2

00:13:58,800 --> 00:14:02,880
if we're knocking it down it impacts the

00:14:00,959 --> 00:14:03,680
the beta v1 service in a way we just

00:14:02,880 --> 00:14:05,920
didn't know

00:14:03,680 --> 00:14:07,120
was possible and that is the power of

00:14:05,920 --> 00:14:08,240
traffic splitting

00:14:07,120 --> 00:14:10,399
and with the observability that's

00:14:08,240 --> 00:14:11,120
afforded to you by linkid you can do a b

00:14:10,399 --> 00:14:12,560
testing

00:14:11,120 --> 00:14:14,800
chaos testing and you can do canary

00:14:12,560 --> 00:14:16,160
releases as well and we've found time

00:14:14,800 --> 00:14:17,199
and time again that developers find it

00:14:16,160 --> 00:14:18,560
so easy to use

00:14:17,199 --> 00:14:20,560
they're starting to chain this stuff in

00:14:18,560 --> 00:14:22,160
ways we didn't even think was possible

00:14:20,560 --> 00:14:24,399
and there's affording a lot more

00:14:22,160 --> 00:14:25,600
resilience at the product engineering

00:14:24,399 --> 00:14:27,040
level

00:14:25,600 --> 00:14:28,880
this means that the candidate releases

00:14:27,040 --> 00:14:29,680
that actually go forward to production

00:14:28,880 --> 00:14:32,480
environments

00:14:29,680 --> 00:14:34,720
are innately more stable because there's

00:14:32,480 --> 00:14:36,560
been a consideration around the signals

00:14:34,720 --> 00:14:37,760
of the infrastructure uh to which

00:14:36,560 --> 00:14:39,040
they're measuring now

00:14:37,760 --> 00:14:40,639
obviously there might be the argument of

00:14:39,040 --> 00:14:41,680
hey well you know deploying to a lower

00:14:40,639 --> 00:14:43,440
environment isn't completely

00:14:41,680 --> 00:14:44,880
representative of a higher environment

00:14:43,440 --> 00:14:46,720
well that's where the adventure gets

00:14:44,880 --> 00:14:48,160
really really exciting because you can

00:14:46,720 --> 00:14:49,600
start dipping into those higher

00:14:48,160 --> 00:14:51,199
environments especially if you federate

00:14:49,600 --> 00:14:53,680
your mesh across clusters so

00:14:51,199 --> 00:14:54,480
there is a strategy to scale this out to

00:14:53,680 --> 00:14:57,600
however

00:14:54,480 --> 00:14:59,279
wide your risk appetite is

00:14:57,600 --> 00:15:00,720
emboldening engineers i think really is

00:14:59,279 --> 00:15:03,279
the crux here and i think

00:15:00,720 --> 00:15:03,760
i've said throughout this talk that it's

00:15:03,279 --> 00:15:05,279
about

00:15:03,760 --> 00:15:07,120
trying to perform experiments but

00:15:05,279 --> 00:15:08,880
ultimately those experiments are about

00:15:07,120 --> 00:15:10,720
tiptoeing the path that helps to keep

00:15:08,880 --> 00:15:12,800
our service stability the highest and

00:15:10,720 --> 00:15:14,720
rather than have sres after the fact

00:15:12,800 --> 00:15:15,920
coming to the sr um to the product

00:15:14,720 --> 00:15:17,440
engineering team and say

00:15:15,920 --> 00:15:19,199
hey you know you've got a degradation

00:15:17,440 --> 00:15:21,120
here it's about building that

00:15:19,199 --> 00:15:23,120
cultural change in so that people care

00:15:21,120 --> 00:15:25,519
about making those resilient services

00:15:23,120 --> 00:15:27,040
and so the behavior that i expect that

00:15:25,519 --> 00:15:29,360
will start to be driven from this

00:15:27,040 --> 00:15:30,959
is that sres and product engineers are

00:15:29,360 --> 00:15:31,360
starting to work at the same kind of

00:15:30,959 --> 00:15:32,720
level

00:15:31,360 --> 00:15:34,480
you know they're starting to think hey

00:15:32,720 --> 00:15:35,680
let's bring in our qa friends and we'll

00:15:34,480 --> 00:15:37,680
all start to build

00:15:35,680 --> 00:15:39,279
these traffic spending methodologies so

00:15:37,680 --> 00:15:40,639
that we can start testing these micro

00:15:39,279 --> 00:15:42,240
services and really thinking about how

00:15:40,639 --> 00:15:43,759
can we break this right and that's kind

00:15:42,240 --> 00:15:45,759
of the hat you need to wear is

00:15:43,759 --> 00:15:46,880
how can i break this micro service

00:15:45,759 --> 00:15:49,440
architecture because

00:15:46,880 --> 00:15:50,320
ultimately the the best genetic

00:15:49,440 --> 00:15:52,560
variation

00:15:50,320 --> 00:15:55,040
at the end of it will survive uh for the

00:15:52,560 --> 00:15:56,320
longest period of time

00:15:55,040 --> 00:15:57,680
i feel like there's a super bright

00:15:56,320 --> 00:15:59,120
future for this sort of stuff and i hope

00:15:57,680 --> 00:16:00,720
that you've enjoyed this talk

00:15:59,120 --> 00:16:02,240
there's so many questions that i have

00:16:00,720 --> 00:16:03,120
yet to answer and so many things we can

00:16:02,240 --> 00:16:04,880
get to talk about

00:16:03,120 --> 00:16:06,639
and if anybody wants to follow me

00:16:04,880 --> 00:16:07,440
offline we can talk and chat about all

00:16:06,639 --> 00:16:09,440
this stuff but

00:16:07,440 --> 00:16:11,360
ultimately watch this space because

00:16:09,440 --> 00:16:13,759
canary releasing a b testing

00:16:11,360 --> 00:16:15,600
and chaos testing are all completely

00:16:13,759 --> 00:16:17,759
possible within lincoln d and are

00:16:15,600 --> 00:16:19,519
all being done today in hundreds of

00:16:17,759 --> 00:16:23,120
companies with a lot of success

00:16:19,519 --> 00:16:23,120

YouTube URL: https://www.youtube.com/watch?v=EB6QWpIYMSA


