Title: Tap Tap, Debugging an App with your Service Mesh - Jason Morgan, Buoyant
Publication date: 2021-05-05
Playlist: ServiceMeshCon EU 2021
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

Tap Tap, Debugging an App with your Service Mesh - Jason Morgan, Buoyant

Microservices are great for all kinds of reasons but troubleshooting issues in a distributed system is rarely easy. Add to that that getting multiple independent teams to agree to use a standard set of metrics or debugging strategies is really hard. Issues often turn into a blame game where DNS, Kubernetes, the network, or the mesh all take a turn. With Linkerd’s built in ability to tap into, and analyze traffic we can quickly identify and isolate problems. We can do that with zero code changes, without needing app teams to expose their own metrics or become experts in Kubernetes or the mesh. When a problem pops up Linkerd users can rely on the mesh as a single source of truth to help quickly identify issues and drive down MTTR.
Captions: 
	00:00:00,080 --> 00:00:04,000
hey folks hello and welcome to

00:00:02,800 --> 00:00:07,359
servicemeshcon

00:00:04,000 --> 00:00:08,960
eu today we're going to be talking about

00:00:07,359 --> 00:00:10,719
debugging an application with your

00:00:08,960 --> 00:00:14,719
service mesh

00:00:10,719 --> 00:00:17,520
tap tap uh my name is jason morgan

00:00:14,719 --> 00:00:19,279
this person here also that person there

00:00:17,520 --> 00:00:22,960
i am a technical evangelist

00:00:19,279 --> 00:00:25,760
over at point that means it is my job to

00:00:22,960 --> 00:00:27,920
talk to folks about the lingerie project

00:00:25,760 --> 00:00:28,400
encourage them to use it and evaluate it

00:00:27,920 --> 00:00:29,920
and help

00:00:28,400 --> 00:00:33,120
folks as they move from development

00:00:29,920 --> 00:00:35,760
through to production with linker d

00:00:33,120 --> 00:00:36,480
you can find me on twitter at r jason

00:00:35,760 --> 00:00:38,559
morgan

00:00:36,480 --> 00:00:41,040
you can find me on github at jason

00:00:38,559 --> 00:00:41,280
morgan and you can find me on the linker

00:00:41,040 --> 00:00:44,559
d

00:00:41,280 --> 00:00:46,879
slack at jmo

00:00:44,559 --> 00:00:48,480
so this is about the end of our slides

00:00:46,879 --> 00:00:52,000
for today we're going to do everything

00:00:48,480 --> 00:00:53,199
as a live demo or as close to a live

00:00:52,000 --> 00:00:56,160
demo as we can get

00:00:53,199 --> 00:00:57,039
considering the circumstances so let's

00:00:56,160 --> 00:01:00,000
talk about

00:00:57,039 --> 00:01:02,000
what it is we're gonna talk about um

00:01:00,000 --> 00:01:03,600
i've got two applications that are

00:01:02,000 --> 00:01:05,920
running in my kubernetes cluster

00:01:03,600 --> 00:01:07,439
that are having some issues and we want

00:01:05,920 --> 00:01:09,520
to diagnose

00:01:07,439 --> 00:01:11,040
and try and do what we can to remedy

00:01:09,520 --> 00:01:12,400
those problems

00:01:11,040 --> 00:01:14,799
with our mesh so i've got one

00:01:12,400 --> 00:01:17,119
application which is a web front end

00:01:14,799 --> 00:01:19,200
backed by two grpc services and another

00:01:17,119 --> 00:01:22,560
that is a series of web applications

00:01:19,200 --> 00:01:26,240
talking to each other over rest

00:01:22,560 --> 00:01:28,159
emoji vote is is the grpc application

00:01:26,240 --> 00:01:30,079
so what it gives us is a web front end

00:01:28,159 --> 00:01:32,079
that displays a number of emojis

00:01:30,079 --> 00:01:33,680
allows us to vote on them view the

00:01:32,079 --> 00:01:37,200
leaderboard and see

00:01:33,680 --> 00:01:40,240
what the what the current state is of uh

00:01:37,200 --> 00:01:41,840
of the voting and it you know it seems

00:01:40,240 --> 00:01:43,040
to be working as designed but we're

00:01:41,840 --> 00:01:44,240
having a problem with it i'm getting

00:01:43,040 --> 00:01:47,040
reports from my users

00:01:44,240 --> 00:01:49,280
and i want to fix it so let's go ahead

00:01:47,040 --> 00:01:52,640
and take a look at the service map

00:01:49,280 --> 00:01:54,000
for emoji vote so what i have here is a

00:01:52,640 --> 00:01:56,640
vote bot which generates some

00:01:54,000 --> 00:01:58,640
traffic it talks to our web service and

00:01:56,640 --> 00:02:00,479
then the web service makes a grpc call

00:01:58,640 --> 00:02:01,600
over to voting or emoji

00:02:00,479 --> 00:02:03,680
and to be clear i'm not getting that

00:02:01,600 --> 00:02:05,520
grpc thing from looking at this graph

00:02:03,680 --> 00:02:09,280
i just knew that already because i work

00:02:05,520 --> 00:02:12,319
with the emoji vote app a lot

00:02:09,280 --> 00:02:13,280
so i want to debug and i want to get

00:02:12,319 --> 00:02:14,319
things going so the first thing i'm

00:02:13,280 --> 00:02:16,000
going to do is just talk to my

00:02:14,319 --> 00:02:17,440
kubernetes cluster ensure that i can

00:02:16,000 --> 00:02:20,640
communicate and that things are

00:02:17,440 --> 00:02:22,160
are running as as expected so i can ask

00:02:20,640 --> 00:02:24,640
about the nodes i can see that i've got

00:02:22,160 --> 00:02:26,400
my three nodes in my k3s cluster this

00:02:24,640 --> 00:02:28,080
k3s plus they're provided by the good

00:02:26,400 --> 00:02:29,440
folks over at sivo

00:02:28,080 --> 00:02:31,599
who are actually going to be announcing

00:02:29,440 --> 00:02:34,640
something here at cubecon so please stay

00:02:31,599 --> 00:02:34,640
tuned for that

00:02:35,120 --> 00:02:38,400
uh now that we've checked on our nodes

00:02:37,360 --> 00:02:41,519
we're going to just

00:02:38,400 --> 00:02:43,440
check on the state of our mesh so

00:02:41,519 --> 00:02:45,200
the linker dcli bundles in this check

00:02:43,440 --> 00:02:46,800
command which will check on the health

00:02:45,200 --> 00:02:47,519
of the control plane and all of its

00:02:46,800 --> 00:02:49,599
components

00:02:47,519 --> 00:02:51,280
and it will also look at any installed

00:02:49,599 --> 00:02:52,800
linker d extensions and it will run

00:02:51,280 --> 00:02:54,720
their health checks as well

00:02:52,800 --> 00:02:56,080
so we see that the results are green

00:02:54,720 --> 00:02:59,120
check mark mark

00:02:56,080 --> 00:03:00,720
for both which sure seems good to me so

00:02:59,120 --> 00:03:04,800
we're going to move on and actually

00:03:00,720 --> 00:03:06,959
troubleshoot our application um

00:03:04,800 --> 00:03:09,120
so the first thing i want to do to

00:03:06,959 --> 00:03:12,159
troubleshoot my app is just take a look

00:03:09,120 --> 00:03:13,200
at the namespaces in the cluster and see

00:03:12,159 --> 00:03:14,879
what does linkerd

00:03:13,200 --> 00:03:16,480
see in terms of the golden metrics for

00:03:14,879 --> 00:03:19,040
those namespaces

00:03:16,480 --> 00:03:19,760
golden metrics being you know success

00:03:19,040 --> 00:03:22,800
rate

00:03:19,760 --> 00:03:25,200
uh volume of requests and then um

00:03:22,800 --> 00:03:26,560
and then latency right so we can see

00:03:25,200 --> 00:03:28,640
we've got a bunch of namespaces here

00:03:26,560 --> 00:03:30,480
pod info is seeing a ton of requests but

00:03:28,640 --> 00:03:31,440
is responding great and has a 100

00:03:30,480 --> 00:03:33,760
success rate

00:03:31,440 --> 00:03:35,840
linker d viz and liquor d dashboard also

00:03:33,760 --> 00:03:37,920
both have 100 success rate

00:03:35,840 --> 00:03:39,599
one nice thing with linker d the control

00:03:37,920 --> 00:03:40,480
plane components are also part of the

00:03:39,599 --> 00:03:42,239
data plane

00:03:40,480 --> 00:03:43,680
so we can use the same debugging

00:03:42,239 --> 00:03:45,120
techniques we're going to use today on

00:03:43,680 --> 00:03:47,120
our various applications

00:03:45,120 --> 00:03:49,760
to check on the health of our mesh or

00:03:47,120 --> 00:03:52,159
debug issues should we run into anything

00:03:49,760 --> 00:03:53,120
now we have emoji vote and books app

00:03:52,159 --> 00:03:56,239
that are both seeing

00:03:53,120 --> 00:03:58,400
a sub 100 success rate so let's dive

00:03:56,239 --> 00:04:01,519
into emoji vote and see if we can't

00:03:58,400 --> 00:04:03,120
can't get this problem fixed so now that

00:04:01,519 --> 00:04:04,000
we know we have two namespaces with

00:04:03,120 --> 00:04:06,720
issues right

00:04:04,000 --> 00:04:08,879
um books app and emoji vote let's look

00:04:06,720 --> 00:04:10,959
at the deployments in emoji vote

00:04:08,879 --> 00:04:12,480
and see what their their relative health

00:04:10,959 --> 00:04:14,239
is

00:04:12,480 --> 00:04:15,840
so i can see right away votebot and

00:04:14,239 --> 00:04:18,160
emoji seem to be

00:04:15,840 --> 00:04:19,040
succeeding all the time have low latency

00:04:18,160 --> 00:04:21,519
that's great

00:04:19,040 --> 00:04:22,880
but web and voting are both reporting

00:04:21,519 --> 00:04:24,639
sound problems

00:04:22,880 --> 00:04:26,560
so let's dive in a little bit further so

00:04:24,639 --> 00:04:28,160
now that we've got the statistics on the

00:04:26,560 --> 00:04:30,320
deployment objects

00:04:28,160 --> 00:04:31,680
overall we can actually dive into each

00:04:30,320 --> 00:04:34,080
one of those deployments

00:04:31,680 --> 00:04:35,840
and see what live calls are going in and

00:04:34,080 --> 00:04:37,280
what can they tell us about the health

00:04:35,840 --> 00:04:39,280
of the application

00:04:37,280 --> 00:04:41,280
now to be clear i haven't instrumented

00:04:39,280 --> 00:04:43,440
anything inside these apps these are

00:04:41,280 --> 00:04:45,520
normal grpc apps i don't have tracing

00:04:43,440 --> 00:04:47,759
enabled or any anything in particular

00:04:45,520 --> 00:04:49,199
this is what the linker d proxy is able

00:04:47,759 --> 00:04:51,440
to tell me about

00:04:49,199 --> 00:04:53,520
my application traffic natively with no

00:04:51,440 --> 00:04:56,000
configuration

00:04:53,520 --> 00:04:56,960
so i can see what pod a given request is

00:04:56,000 --> 00:04:59,600
coming from

00:04:56,960 --> 00:05:00,639
what pod it's going to the method as

00:04:59,600 --> 00:05:03,360
well as the path

00:05:00,639 --> 00:05:03,680
that's being that's being called right

00:05:03,360 --> 00:05:06,080
and

00:05:03,680 --> 00:05:07,520
and the number of requests coming in so

00:05:06,080 --> 00:05:10,160
i can see right away that

00:05:07,520 --> 00:05:11,919
web here my web pod is talking to emoji

00:05:10,160 --> 00:05:12,720
and those should all be successful we

00:05:11,919 --> 00:05:15,199
can see the call

00:05:12,720 --> 00:05:16,479
as list all and find by short code if we

00:05:15,199 --> 00:05:19,039
scroll over to the right we see that

00:05:16,479 --> 00:05:21,120
those are both 100 successful

00:05:19,039 --> 00:05:23,199
great in line with our expectations

00:05:21,120 --> 00:05:26,320
based on the statistics we've seen

00:05:23,199 --> 00:05:27,440
votebot talk in a web has two calls api

00:05:26,320 --> 00:05:30,400
list and api

00:05:27,440 --> 00:05:31,600
vote so let's see how they're doing api

00:05:30,400 --> 00:05:35,520
list is at 100

00:05:31,600 --> 00:05:37,840
but api vote is at around 85 to 90

00:05:35,520 --> 00:05:38,720
so it's not succeeding all the way and

00:05:37,840 --> 00:05:40,800
then webb

00:05:38,720 --> 00:05:42,000
is talking to voting a lot right and

00:05:40,800 --> 00:05:45,039
it's talking to voting

00:05:42,000 --> 00:05:47,759
and it's hitting these individual vote

00:05:45,039 --> 00:05:49,199
these individual vote urls and let's

00:05:47,759 --> 00:05:50,960
look at their success rate so it looks

00:05:49,199 --> 00:05:52,960
like it's good for most things

00:05:50,960 --> 00:05:54,880
but vote donut right so donut what

00:05:52,960 --> 00:05:56,400
should be our most popular emoji

00:05:54,880 --> 00:05:59,120
is actually seeing a zero percent

00:05:56,400 --> 00:05:59,120
success rate

00:05:59,840 --> 00:06:03,840
great is actually seeing a zero percent

00:06:02,479 --> 00:06:07,199
success rate

00:06:03,840 --> 00:06:10,720
uh so let's go

00:06:07,199 --> 00:06:11,360
on um so we probably have we probably

00:06:10,720 --> 00:06:14,080
have our

00:06:11,360 --> 00:06:15,759
our problem solved already or diagnosed

00:06:14,080 --> 00:06:18,479
and what we'd like to do now

00:06:15,759 --> 00:06:20,160
is just just triangulate a little bit

00:06:18,479 --> 00:06:23,280
let's go check out the voting service

00:06:20,160 --> 00:06:23,280
see what it reports

00:06:24,479 --> 00:06:27,600
so i do a top on that on that voting

00:06:26,800 --> 00:06:29,520
deployment

00:06:27,600 --> 00:06:31,199
right now i can see that all the calls

00:06:29,520 --> 00:06:32,720
to voting like we saw in our traffic map

00:06:31,199 --> 00:06:33,600
are actually coming from the web service

00:06:32,720 --> 00:06:36,400
over to voting

00:06:33,600 --> 00:06:39,120
we see the the api calls that they're or

00:06:36,400 --> 00:06:42,319
the paths they're taking inside the api

00:06:39,120 --> 00:06:44,000
and we can see we can see how many uh

00:06:42,319 --> 00:06:45,840
and we can look at the success rate so

00:06:44,000 --> 00:06:46,479
so far everything seems to be going

00:06:45,840 --> 00:06:48,080
great

00:06:46,479 --> 00:06:50,240
right we don't have we don't have any

00:06:48,080 --> 00:06:53,520
failures but let's give it a minute and

00:06:50,240 --> 00:06:53,520
see what else pops

00:07:00,840 --> 00:07:05,840
up

00:07:02,319 --> 00:07:08,000
all right uh thank you

00:07:05,840 --> 00:07:09,840
uh we have vote donut coming in starting

00:07:08,000 --> 00:07:11,919
to receive some some traffic

00:07:09,840 --> 00:07:14,800
and we see that it's actually hitting a

00:07:11,919 --> 00:07:17,280
a zero percent success rate

00:07:14,800 --> 00:07:19,440
so right here let's go ahead and let's

00:07:17,280 --> 00:07:21,919
go ahead and check on that right

00:07:19,440 --> 00:07:24,960
i can go over to my emoji vote app i can

00:07:21,919 --> 00:07:27,039
click on donut i see that i've got a 404

00:07:24,960 --> 00:07:29,039
so we have a real problem so i have more

00:07:27,039 --> 00:07:30,400
than enough to package over

00:07:29,039 --> 00:07:34,080
to my development team so they know

00:07:30,400 --> 00:07:34,080
where to begin looking for the problem

00:07:34,880 --> 00:07:39,759
great so we've got that but let's let's

00:07:38,080 --> 00:07:41,599
continue to dive a little deeper let's

00:07:39,759 --> 00:07:43,280
see if we can't

00:07:41,599 --> 00:07:46,639
if we can't grab some of that live

00:07:43,280 --> 00:07:49,360
traffic over to this this voting service

00:07:46,639 --> 00:07:50,319
and see what we get now we've got the

00:07:49,360 --> 00:07:54,240
linker d

00:07:50,319 --> 00:07:56,479
vis tap command here and tap is going to

00:07:54,240 --> 00:07:57,360
snoop in on the calls between the two

00:07:56,479 --> 00:07:59,919
proxies

00:07:57,360 --> 00:08:01,599
and get a bunch of metadata around them

00:07:59,919 --> 00:08:04,400
so that we can we can see the current

00:08:01,599 --> 00:08:07,599
state of our environment

00:08:04,400 --> 00:08:08,319
uh so we we run our command and we can

00:08:07,599 --> 00:08:10,800
see that

00:08:08,319 --> 00:08:12,400
you know we have calls like this vote

00:08:10,800 --> 00:08:14,479
for running man it's

00:08:12,400 --> 00:08:16,080
indicates that it's an mtls call which

00:08:14,479 --> 00:08:18,000
is the default for linker d

00:08:16,080 --> 00:08:20,160
when you install you get mtls between

00:08:18,000 --> 00:08:22,400
all uh all services

00:08:20,160 --> 00:08:24,560
uh we have a status code of 200 which is

00:08:22,400 --> 00:08:29,199
great and a drpc status

00:08:24,560 --> 00:08:33,599
okay so let's look for donut

00:08:29,199 --> 00:08:33,599
donut see if i type that right

00:08:34,000 --> 00:08:38,240
scroll around here there we go we've got

00:08:36,159 --> 00:08:41,279
a donut call here so

00:08:38,240 --> 00:08:43,680
now i've got a path of emoji v1

00:08:41,279 --> 00:08:45,360
uh voting service vote donut and we can

00:08:43,680 --> 00:08:48,399
see that while we still have that

00:08:45,360 --> 00:08:50,480
tls is true and the status code 200 we

00:08:48,399 --> 00:08:51,760
also get our grpc status of unknown

00:08:50,480 --> 00:08:55,279
which is actually a grpc

00:08:51,760 --> 00:08:56,399
error okay well the nice thing that we

00:08:55,279 --> 00:08:59,200
got out of that

00:08:56,399 --> 00:08:59,680
out of that last call is the particular

00:08:59,200 --> 00:09:02,800
path

00:08:59,680 --> 00:09:05,680
to look at for voting donut errors

00:09:02,800 --> 00:09:06,080
so we we check out vote donut and let's

00:09:05,680 --> 00:09:07,600
just

00:09:06,080 --> 00:09:08,640
let's again let's update the path that

00:09:07,600 --> 00:09:09,519
we're calling to instead of just the

00:09:08,640 --> 00:09:13,600
base url

00:09:09,519 --> 00:09:13,600
to the full voting service vote donut

00:09:14,000 --> 00:09:17,920
see what what all calls come in now this

00:09:16,080 --> 00:09:19,839
is going to populate as soon as

00:09:17,920 --> 00:09:22,080
as soon as our traffic generator votes

00:09:19,839 --> 00:09:22,880
for donut or i can actually pop over to

00:09:22,080 --> 00:09:27,040
my web app

00:09:22,880 --> 00:09:27,040
and try voting for the donut again

00:09:29,279 --> 00:09:31,660
great so we see some of that traffic

00:09:31,120 --> 00:09:33,839
coming in

00:09:31,660 --> 00:09:35,600
[Music]

00:09:33,839 --> 00:09:37,839
we see some of that traffic coming in we

00:09:35,600 --> 00:09:40,560
have still the tls true

00:09:37,839 --> 00:09:41,680
sasco 200 and that grpc unknown so i

00:09:40,560 --> 00:09:44,240
could save this off

00:09:41,680 --> 00:09:46,080
and pass it to my developers which would

00:09:44,240 --> 00:09:46,880
be handy for them but we can actually

00:09:46,080 --> 00:09:49,200
take

00:09:46,880 --> 00:09:50,320
we can actually get way more detailed

00:09:49,200 --> 00:09:52,320
information

00:09:50,320 --> 00:09:53,760
so that same tap command right i'm going

00:09:52,320 --> 00:09:55,440
to run it again but i'm going to change

00:09:53,760 --> 00:09:58,000
the output format from

00:09:55,440 --> 00:09:59,519
just that default terminal to a full

00:09:58,000 --> 00:10:01,839
json output

00:09:59,519 --> 00:10:02,880
so we'll see we'll see the same basic

00:10:01,839 --> 00:10:04,800
thing but with

00:10:02,880 --> 00:10:06,640
a ton more detail and we could save this

00:10:04,800 --> 00:10:08,720
off bundle it with

00:10:06,640 --> 00:10:10,079
you know with the message we send to our

00:10:08,720 --> 00:10:13,279
developers on the

00:10:10,079 --> 00:10:15,440
voting service and let them let them

00:10:13,279 --> 00:10:17,519
start solving the problem

00:10:15,440 --> 00:10:19,600
so here right we see the we see the

00:10:17,519 --> 00:10:21,519
output of one of these calls right now

00:10:19,600 --> 00:10:22,800
is jason so i've got a bunch of metadata

00:10:21,519 --> 00:10:25,680
about the source

00:10:22,800 --> 00:10:27,120
the destination uh you know the the

00:10:25,680 --> 00:10:29,920
request information

00:10:27,120 --> 00:10:30,720
and all the headers involved in this

00:10:29,920 --> 00:10:34,640
call

00:10:30,720 --> 00:10:36,720
all right great i can also if i look

00:10:34,640 --> 00:10:38,160
down a little bit further

00:10:36,720 --> 00:10:40,160
i can look at the headers for another

00:10:38,160 --> 00:10:42,640
request and see that you know we've got

00:10:40,160 --> 00:10:45,680
our grpc status and we've got our grpc

00:10:42,640 --> 00:10:48,000
error message right there cool so

00:10:45,680 --> 00:10:49,440
save that send it to my devs i feel like

00:10:48,000 --> 00:10:52,959
i've discharged my duty

00:10:49,440 --> 00:10:53,760
to the team in terms of in terms of

00:10:52,959 --> 00:10:55,360
debugging

00:10:53,760 --> 00:11:00,160
emoji photo and now it's on the

00:10:55,360 --> 00:11:03,279
developers to actually get it fixed

00:11:00,160 --> 00:11:04,959
uh so that's been great right but this

00:11:03,279 --> 00:11:06,800
this troubleshooting process actually

00:11:04,959 --> 00:11:08,320
took longer than it needed to take right

00:11:06,800 --> 00:11:09,680
because i was looking around i was

00:11:08,320 --> 00:11:10,880
waiting for things to come up

00:11:09,680 --> 00:11:13,360
you all saw when i went to voting

00:11:10,880 --> 00:11:16,160
service i didn't see anything about

00:11:13,360 --> 00:11:16,640
the donut until a fair bit of time in

00:11:16,160 --> 00:11:18,480
right

00:11:16,640 --> 00:11:20,160
so i had to do that kind of aggregation

00:11:18,480 --> 00:11:22,959
myself um

00:11:20,160 --> 00:11:24,800
so one of the things that's really nice

00:11:22,959 --> 00:11:25,839
about linker d is you install it and you

00:11:24,800 --> 00:11:27,360
don't have to

00:11:25,839 --> 00:11:29,519
you don't have to build or configure a

00:11:27,360 --> 00:11:31,279
bunch of custom resource definitions

00:11:29,519 --> 00:11:33,279
in order to make the mesh work you get

00:11:31,279 --> 00:11:36,560
all the value with a very simple install

00:11:33,279 --> 00:11:38,160
and and inject on your applications

00:11:36,560 --> 00:11:39,839
but they're one of the two custom

00:11:38,160 --> 00:11:40,640
resource definitions that linker d does

00:11:39,839 --> 00:11:43,200
use

00:11:40,640 --> 00:11:44,800
is this service profile so let me just

00:11:43,200 --> 00:11:45,519
create this and we'll talk about what we

00:11:44,800 --> 00:11:49,120
did

00:11:45,519 --> 00:11:50,320
so i use the linkery cli to look at the

00:11:49,120 --> 00:11:53,279
proto file

00:11:50,320 --> 00:11:54,000
for my emoji service so emoji is a grpc

00:11:53,279 --> 00:11:56,079
service

00:11:54,000 --> 00:11:57,440
it uses these protobufs and we can

00:11:56,079 --> 00:12:00,160
actually look at them

00:11:57,440 --> 00:12:02,399
and see what are the actual valid calls

00:12:00,160 --> 00:12:04,480
for this service

00:12:02,399 --> 00:12:05,440
and we get we get this service profile

00:12:04,480 --> 00:12:07,519
object

00:12:05,440 --> 00:12:08,560
which will allow linkerd to do some more

00:12:07,519 --> 00:12:10,639
intelligent stuff

00:12:08,560 --> 00:12:12,160
with the traffic for this application

00:12:10,639 --> 00:12:14,480
and it will be able to do things like

00:12:12,160 --> 00:12:15,680
collect and maintain the data about the

00:12:14,480 --> 00:12:17,519
given routes

00:12:15,680 --> 00:12:19,360
right there inside the service so that

00:12:17,519 --> 00:12:20,480
we can more quickly debug issues like

00:12:19,360 --> 00:12:22,320
this when they come up

00:12:20,480 --> 00:12:25,040
so i'm going to go ahead and create and

00:12:22,320 --> 00:12:25,760
apply a service profile for the emoji

00:12:25,040 --> 00:12:27,600
service

00:12:25,760 --> 00:12:29,120
i'm going to do the same thing for the

00:12:27,600 --> 00:12:32,000
voting service again

00:12:29,120 --> 00:12:34,240
just using that just using that protobuf

00:12:32,000 --> 00:12:36,800
file

00:12:34,240 --> 00:12:38,240
and once those are done i'm about ready

00:12:36,800 --> 00:12:39,920
but there's a third service that i care

00:12:38,240 --> 00:12:41,040
about here which is the web front end

00:12:39,920 --> 00:12:42,880
now the web front end

00:12:41,040 --> 00:12:44,560
is not it's just the rest service so i

00:12:42,880 --> 00:12:46,000
don't have a grpc file to work with or a

00:12:44,560 --> 00:12:49,120
profile to work with

00:12:46,000 --> 00:12:50,959
and i also don't have a swagger file so

00:12:49,120 --> 00:12:52,639
i need to actually just take a look so i

00:12:50,959 --> 00:12:54,959
can either write

00:12:52,639 --> 00:12:56,560
write my own service profile based on

00:12:54,959 --> 00:12:59,279
what i know of the application

00:12:56,560 --> 00:13:00,160
or i can also use linkerd's tap

00:12:59,279 --> 00:13:02,560
functionality

00:13:00,160 --> 00:13:04,240
to in to to watch the traffic that's

00:13:02,560 --> 00:13:06,880
coming to this web service

00:13:04,240 --> 00:13:08,480
and and decide how to build a service

00:13:06,880 --> 00:13:10,079
profile based on what we see

00:13:08,480 --> 00:13:13,120
so for the next 10 seconds we're going

00:13:10,079 --> 00:13:16,240
to watch the web service

00:13:13,120 --> 00:13:20,639
see what comes in and auto generate a a

00:13:16,240 --> 00:13:23,519
service profile

00:13:20,639 --> 00:13:25,360
so we have we have our new web service

00:13:23,519 --> 00:13:27,600
uh service profile created

00:13:25,360 --> 00:13:29,360
great we're gonna i just give myself a

00:13:27,600 --> 00:13:31,040
reminder to go hop over to the dashboard

00:13:29,360 --> 00:13:32,320
and we're gonna take a look at how we

00:13:31,040 --> 00:13:35,040
can debug this

00:13:32,320 --> 00:13:35,920
even faster if we were using service

00:13:35,040 --> 00:13:38,079
profiles

00:13:35,920 --> 00:13:39,839
so again looking at the emoji photo name

00:13:38,079 --> 00:13:42,959
space i can click on web

00:13:39,839 --> 00:13:44,160
i can get my route metrics you know i

00:13:42,959 --> 00:13:47,040
can see that

00:13:44,160 --> 00:13:48,320
you know as we're coming in api list is

00:13:47,040 --> 00:13:49,839
staying at 100

00:13:48,320 --> 00:13:51,360
api vote now again it's only been a

00:13:49,839 --> 00:13:52,639
couple seconds but over time

00:13:51,360 --> 00:13:54,720
we're going to see some of those failed

00:13:52,639 --> 00:13:56,880
transactions come in and this

00:13:54,720 --> 00:13:58,000
this api vote service is going to start

00:13:56,880 --> 00:13:59,920
to degrade

00:13:58,000 --> 00:14:01,600
right and if i'd left this running for

00:13:59,920 --> 00:14:04,800
hours it would be a lot more obvious

00:14:01,600 --> 00:14:06,560
at what the failure rate actually is um

00:14:04,800 --> 00:14:08,399
so again gives us an indication that

00:14:06,560 --> 00:14:10,240
we've got a problem with

00:14:08,399 --> 00:14:12,639
with voting so we look at our voting

00:14:10,240 --> 00:14:12,639
service

00:14:13,920 --> 00:14:18,000
and again right instead of waiting for a

00:14:16,399 --> 00:14:19,680
vote donut call to come in

00:14:18,000 --> 00:14:21,120
right which it did right away but you

00:14:19,680 --> 00:14:21,519
know you saw it doesn't necessarily do

00:14:21,120 --> 00:14:23,680
that

00:14:21,519 --> 00:14:25,279
we can look at our route metrics filter

00:14:23,680 --> 00:14:26,880
on the success rate so now imagine the

00:14:25,279 --> 00:14:28,959
situation i've been paged

00:14:26,880 --> 00:14:30,000
because there's an issue i come in it's

00:14:28,959 --> 00:14:32,399
it's some amount of time

00:14:30,000 --> 00:14:34,240
later and i can just immediately look

00:14:32,399 --> 00:14:35,120
for the pass inside my api and see

00:14:34,240 --> 00:14:37,279
what's

00:14:35,120 --> 00:14:38,800
what is either responding slowly or is

00:14:37,279 --> 00:14:41,199
starting to see errors

00:14:38,800 --> 00:14:42,160
and then and then immediately go direct

00:14:41,199 --> 00:14:44,639
direct the

00:14:42,160 --> 00:14:46,399
the right uh the right ticket to the

00:14:44,639 --> 00:14:49,440
right team so that we can get

00:14:46,399 --> 00:14:50,240
get this thing fixed so that is emoji

00:14:49,440 --> 00:14:51,920
vote

00:14:50,240 --> 00:14:53,120
right so that's that's as far as we're

00:14:51,920 --> 00:14:56,000
gonna go because we have some

00:14:53,120 --> 00:14:57,760
fundamental problem with our application

00:14:56,000 --> 00:14:59,600
but now let's change scenarios a little

00:14:57,760 --> 00:15:00,399
bit let's talk about not a grpc

00:14:59,600 --> 00:15:01,920
application

00:15:00,399 --> 00:15:03,680
but instead an all rest-based

00:15:01,920 --> 00:15:04,639
application so we've got our buoyant

00:15:03,680 --> 00:15:07,120
books app

00:15:04,639 --> 00:15:08,240
and and books let's take a look at that

00:15:07,120 --> 00:15:10,560
inside that terminal

00:15:08,240 --> 00:15:11,600
we're inside the dashboard real quick

00:15:10,560 --> 00:15:14,079
books is

00:15:11,600 --> 00:15:15,920
is a bit more of a complicated service

00:15:14,079 --> 00:15:18,399
let's give this a little refresh

00:15:15,920 --> 00:15:20,160
um books is a bit more of a complicated

00:15:18,399 --> 00:15:21,760
service we have a traffic generator just

00:15:20,160 --> 00:15:22,800
like we did with emoji boat and three

00:15:21,760 --> 00:15:25,120
services

00:15:22,800 --> 00:15:26,800
but web app is talking to both authors

00:15:25,120 --> 00:15:28,639
and books and books and authors are

00:15:26,800 --> 00:15:30,800
talking to each other so there's some

00:15:28,639 --> 00:15:31,920
additional dependency there and my

00:15:30,800 --> 00:15:33,920
failures with

00:15:31,920 --> 00:15:35,600
with books are more intermittent so it's

00:15:33,920 --> 00:15:37,519
a little bit harder to

00:15:35,600 --> 00:15:41,759
to get a sense of really where my

00:15:37,519 --> 00:15:44,240
problem is now luckily in this case

00:15:41,759 --> 00:15:46,240
i've already got i've i've gotten i've

00:15:44,240 --> 00:15:47,839
had the profile set up this entire time

00:15:46,240 --> 00:15:49,440
so we're going to just use

00:15:47,839 --> 00:15:50,959
these service profiles because these are

00:15:49,440 --> 00:15:53,759
all rest apis with

00:15:50,959 --> 00:15:54,240
swagger files we're going to use those

00:15:53,759 --> 00:15:56,720
those

00:15:54,240 --> 00:15:57,839
profiles to get a sense of where our

00:15:56,720 --> 00:16:00,079
problem is and we're actually going to

00:15:57,839 --> 00:16:02,460
do a little bit to resolve it before

00:16:00,079 --> 00:16:03,680
our devs have to get involved

00:16:02,460 --> 00:16:05,920
[Music]

00:16:03,680 --> 00:16:07,199
so we start we just look at the routes

00:16:05,920 --> 00:16:08,399
for the web app service

00:16:07,199 --> 00:16:09,839
right because that's kind of our entry

00:16:08,399 --> 00:16:12,240
point to this app so let's look at

00:16:09,839 --> 00:16:13,600
what's going on here

00:16:12,240 --> 00:16:15,360
we can see a bunch of the calls are

00:16:13,600 --> 00:16:17,600
seeing 100 success rate but

00:16:15,360 --> 00:16:19,440
post the books and post the books

00:16:17,600 --> 00:16:22,480
editing and individual id

00:16:19,440 --> 00:16:25,040
uh are problematic okay so

00:16:22,480 --> 00:16:27,120
let's uh let's see if we can't get this

00:16:25,040 --> 00:16:28,639
a bit more triangulated i'm gonna see

00:16:27,120 --> 00:16:30,480
how is web app doing in its

00:16:28,639 --> 00:16:33,040
conversations to both authors

00:16:30,480 --> 00:16:33,040
and books

00:16:33,600 --> 00:16:36,720
so when i look at you know from this web

00:16:36,320 --> 00:16:39,120
app

00:16:36,720 --> 00:16:40,560
over to authors let's see let's see how

00:16:39,120 --> 00:16:43,759
how the routes look

00:16:40,560 --> 00:16:46,399
looks like every single call from

00:16:43,759 --> 00:16:48,079
from the web app over to authors is

00:16:46,399 --> 00:16:51,120
succeeding 100 of time

00:16:48,079 --> 00:16:52,480
so we're pretty good on that route great

00:16:51,120 --> 00:16:54,560
but we still have problems with that app

00:16:52,480 --> 00:16:56,880
so clearly it's a problem with web app

00:16:54,560 --> 00:16:58,880
talking to books

00:16:56,880 --> 00:17:00,519
so we look from web app to books yep

00:16:58,880 --> 00:17:03,279
there it is uh

00:17:00,519 --> 00:17:06,000
postbooks.json and post book or sorry

00:17:03,279 --> 00:17:08,000
put on a particular book id dot jason

00:17:06,000 --> 00:17:09,600
is failing somewhere around 50 of the

00:17:08,000 --> 00:17:12,400
time so

00:17:09,600 --> 00:17:13,520
we're already getting a lot closer to to

00:17:12,400 --> 00:17:15,679
our root cause

00:17:13,520 --> 00:17:17,919
right and our goal here is to drive down

00:17:15,679 --> 00:17:19,520
that that meantime the detection on the

00:17:17,919 --> 00:17:21,520
problem so that we can get this resolved

00:17:19,520 --> 00:17:24,000
as quickly as possible

00:17:21,520 --> 00:17:25,360
uh now i'm gonna look uh now that we've

00:17:24,000 --> 00:17:27,679
seen seen this we also have that

00:17:25,360 --> 00:17:30,080
dependency between books app and authors

00:17:27,679 --> 00:17:32,080
so let's just let's just go in and check

00:17:30,080 --> 00:17:34,240
how what is books talking to

00:17:32,080 --> 00:17:36,320
authors about and how are those requests

00:17:34,240 --> 00:17:39,280
doing

00:17:36,320 --> 00:17:40,720
so now we look all books app is doing is

00:17:39,280 --> 00:17:44,000
a head request

00:17:40,720 --> 00:17:46,080
on authors by a particular id and we're

00:17:44,000 --> 00:17:49,360
seeing that right about 50

00:17:46,080 --> 00:17:52,240
successful so should we clearly have

00:17:49,360 --> 00:17:53,679
we clearly have an issue and if we look

00:17:52,240 --> 00:17:56,640
right and that was that was

00:17:53,679 --> 00:17:57,600
quick to diagnose with uh that was very

00:17:56,640 --> 00:18:00,799
quick to diagnose

00:17:57,600 --> 00:18:02,240
with um with the routes in place but

00:18:00,799 --> 00:18:04,320
you'll see it's a little bit harder when

00:18:02,240 --> 00:18:06,880
we look at the author service

00:18:04,320 --> 00:18:06,880
directly

00:18:08,000 --> 00:18:11,600
right so we look at the live calls for

00:18:09,520 --> 00:18:13,200
the author service and we can see that

00:18:11,600 --> 00:18:15,600
that sometimes we're seeing

00:18:13,200 --> 00:18:17,679
we're seeing failures but it's not it's

00:18:15,600 --> 00:18:20,240
not in a way that's clearly aggregated

00:18:17,679 --> 00:18:21,360
right i've got a particular author id

00:18:20,240 --> 00:18:23,840
that's seeing some

00:18:21,360 --> 00:18:24,720
some failures and and some that are

00:18:23,840 --> 00:18:26,559
succeeding

00:18:24,720 --> 00:18:28,799
right but when i when i view it from a

00:18:26,559 --> 00:18:31,120
route perspective where linker d's been

00:18:28,799 --> 00:18:33,200
been aggregating the traffic based on

00:18:31,120 --> 00:18:33,919
the the api calls defined in the swagger

00:18:33,200 --> 00:18:36,799
dock

00:18:33,919 --> 00:18:38,240
i see that you know the particular head

00:18:36,799 --> 00:18:41,200
the particular head request to

00:18:38,240 --> 00:18:43,120
any author id is failing about 50 of the

00:18:41,200 --> 00:18:45,440
time so i've got

00:18:43,120 --> 00:18:46,480
like i've got the i've got the problem

00:18:45,440 --> 00:18:50,000
identified

00:18:46,480 --> 00:18:52,000
right and and that's good and

00:18:50,000 --> 00:18:53,840
and so we're we're there right i can go

00:18:52,000 --> 00:18:55,919
ahead and alert my

00:18:53,840 --> 00:18:57,200
my authors folks that this head request

00:18:55,919 --> 00:18:59,039
is failing half the time they need to

00:18:57,200 --> 00:19:00,000
look at at the code that's responsible

00:18:59,039 --> 00:19:02,320
for that

00:19:00,000 --> 00:19:03,760
response but i can also because it's

00:19:02,320 --> 00:19:05,280
failing about half the time and

00:19:03,760 --> 00:19:07,760
succeeding about half the time

00:19:05,280 --> 00:19:08,720
i can actually use my mesh to solve some

00:19:07,760 --> 00:19:10,480
of this problem

00:19:08,720 --> 00:19:12,080
so we're going to look at that service

00:19:10,480 --> 00:19:13,600
profile for the author service and we're

00:19:12,080 --> 00:19:15,440
going to we're going to change it

00:19:13,600 --> 00:19:17,360
so that we we fix the problem for

00:19:15,440 --> 00:19:19,760
tonight and they're able to take time

00:19:17,360 --> 00:19:21,840
and deal with it in the morning

00:19:19,760 --> 00:19:23,120
so our service profile has the various

00:19:21,840 --> 00:19:24,480
routes for this service

00:19:23,120 --> 00:19:26,640
and we're going to go into this head

00:19:24,480 --> 00:19:30,799
request that we saw before so head

00:19:26,640 --> 00:19:32,799
authors id dot json is failing

00:19:30,799 --> 00:19:34,640
we're just going to go in here and we

00:19:32,799 --> 00:19:36,640
know that it's safe to retry this call

00:19:34,640 --> 00:19:40,240
so we're going to add a field that says

00:19:36,640 --> 00:19:42,880
is retriable is retriable

00:19:40,240 --> 00:19:45,039
true i'm going to hope that i type that

00:19:42,880 --> 00:19:47,280
correctly

00:19:45,039 --> 00:19:48,480
and and so we're telling the mesh hey

00:19:47,280 --> 00:19:51,120
when you see these calls

00:19:48,480 --> 00:19:52,960
proxy go ahead and just retry it so no

00:19:51,120 --> 00:19:55,600
app logic has changed

00:19:52,960 --> 00:19:57,280
no you know no no fundamental issues to

00:19:55,600 --> 00:19:58,400
the code or no no changes of the code

00:19:57,280 --> 00:20:00,080
have been created

00:19:58,400 --> 00:20:02,080
right but instead i'm letting the mesh

00:20:00,080 --> 00:20:03,679
take responsibility for

00:20:02,080 --> 00:20:05,600
trying to solve some of this problem and

00:20:03,679 --> 00:20:08,880
now we look at the routes

00:20:05,600 --> 00:20:10,480
from from books to authors

00:20:08,880 --> 00:20:12,480
right and we can see that the success

00:20:10,480 --> 00:20:14,159
rate while the actual success rate is

00:20:12,480 --> 00:20:15,919
staying right around 50 percent

00:20:14,159 --> 00:20:17,200
the effective success rate is going to

00:20:15,919 --> 00:20:18,480
steadily climb

00:20:17,200 --> 00:20:21,760
and it's going to climb all the way up

00:20:18,480 --> 00:20:23,679
to 100 because it through those retries

00:20:21,760 --> 00:20:25,360
it's going to it's going to succeed

00:20:23,679 --> 00:20:27,440
eventually we're also going to see our

00:20:25,360 --> 00:20:28,480
latency go up but as long as it stays

00:20:27,440 --> 00:20:30,400
within reason

00:20:28,480 --> 00:20:31,760
you know it's overnight we'll we'll page

00:20:30,400 --> 00:20:33,520
them first thing in the morning

00:20:31,760 --> 00:20:35,520
they can respond to it in a timely

00:20:33,520 --> 00:20:38,960
fashion

00:20:35,520 --> 00:20:38,960
so we'll break out of this

00:20:39,760 --> 00:20:43,679
and again now i can look at the routes

00:20:41,919 --> 00:20:45,440
from from the web app

00:20:43,679 --> 00:20:48,080
to books where we originally saw the

00:20:45,440 --> 00:20:50,720
issue and see what this looks like and

00:20:48,080 --> 00:20:53,600
now our success rate has gone up to

00:20:50,720 --> 00:20:54,159
gone up to 100 and we're feeling good

00:20:53,600 --> 00:20:57,360
about

00:20:54,159 --> 00:21:03,840
about that result so

00:20:57,360 --> 00:21:05,600
let's pop back into the slides

00:21:03,840 --> 00:21:07,120
let's pop back into our slides and

00:21:05,600 --> 00:21:09,840
that's the end of my talk

00:21:07,120 --> 00:21:10,480
thank you so much for staying to listen

00:21:09,840 --> 00:21:12,159
uh

00:21:10,480 --> 00:21:13,760
like i said earlier you can find me on

00:21:12,159 --> 00:21:15,280
twitter at rj's morgan

00:21:13,760 --> 00:21:16,799
if for some reason you want to see my

00:21:15,280 --> 00:21:17,440
github contributions you can find me

00:21:16,799 --> 00:21:19,200
here

00:21:17,440 --> 00:21:20,559
and i'd love to hear from you over on

00:21:19,200 --> 00:21:23,080
the linkerity slack at

00:21:20,559 --> 00:21:26,080
jmo thanks so much and have a great day

00:21:23,080 --> 00:21:26,080

YouTube URL: https://www.youtube.com/watch?v=YJ8zP-lqB5E


