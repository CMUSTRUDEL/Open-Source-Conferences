Title: Administering Multi Cluster Service Meshes Securely - Eric Murphy & Eitan Yarmush, Solo.io
Publication date: 2021-05-05
Playlist: ServiceMeshCon EU 2021
Description: 
	Donâ€™t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

Administering Multi Cluster Service Meshes Securely - Eric Murphy & Eitan Yarmush, Solo.io

The majority of existing multi-cluster service mesh architectures require the distribution of Kubernetes API credentials (kubeconfigs) across clusters, typically by provisioning a service account in the local cluster and copying its access token to a process running in a remote cluster. This architecture requires that credentials for the Kubernetes API be shared with entities outside the cluster, exposing it to attack. Furthermore, scalability limits of the Kubernetes API Server make it less than ideal to serve an unbounded number of potential remote clients managing configuration and sharing access to a cluster. This talk will explore the downside of existing approaches in this model and propose a new approach based on a client-server management architecture inspired by Envoy which does not require sharing sensitive Kubernetes credentials with remote clusters.
Captions: 
	00:00:00,080 --> 00:00:04,640
all right hello and welcome to

00:00:02,879 --> 00:00:05,359
administering multi-cluster service

00:00:04,640 --> 00:00:08,240
meshes

00:00:05,359 --> 00:00:08,559
securely i'm eitan yarmisch an architect

00:00:08,240 --> 00:00:12,240
at

00:00:08,559 --> 00:00:12,719
solo i o i'm eric murphy i'm a field

00:00:12,240 --> 00:00:16,400
engineer

00:00:12,719 --> 00:00:18,400
also at seoul io

00:00:16,400 --> 00:00:19,920
so before we jump in let's just quickly

00:00:18,400 --> 00:00:23,119
go over what we're going to

00:00:19,920 --> 00:00:25,279
discuss in this talk first

00:00:23,119 --> 00:00:27,840
we're going to review multi-cluster

00:00:25,279 --> 00:00:29,679
service mesh and how it has evolved

00:00:27,840 --> 00:00:31,039
first focusing on single cluster service

00:00:29,679 --> 00:00:33,760
mesh and

00:00:31,039 --> 00:00:34,480
jumping off from there next we're going

00:00:33,760 --> 00:00:37,040
to discuss

00:00:34,480 --> 00:00:38,000
the limitations and security issues with

00:00:37,040 --> 00:00:40,800
the current

00:00:38,000 --> 00:00:41,440
approach third we're going to explain

00:00:40,800 --> 00:00:44,320
how

00:00:41,440 --> 00:00:45,200
we at soloyo solved those problems and

00:00:44,320 --> 00:00:47,200
then fourth

00:00:45,200 --> 00:00:48,480
we're going to give an example or demo

00:00:47,200 --> 00:00:52,160
of how to deploy

00:00:48,480 --> 00:00:52,160
securely using git ops

00:00:52,879 --> 00:00:57,120
so before we jump into multi-cluster

00:00:54,719 --> 00:00:59,760
service mesh let's just quickly

00:00:57,120 --> 00:01:01,440
take a look back at single cluster

00:00:59,760 --> 00:01:03,760
service mesh

00:01:01,440 --> 00:01:05,600
so this is a very simple single cluster

00:01:03,760 --> 00:01:08,080
service mesh

00:01:05,600 --> 00:01:09,360
example we have a couple of different

00:01:08,080 --> 00:01:12,400
workloads

00:01:09,360 --> 00:01:13,200
running in cluster as well as our

00:01:12,400 --> 00:01:15,920
service mesh

00:01:13,200 --> 00:01:17,200
in this case the service mesh is istio

00:01:15,920 --> 00:01:20,320
but that is not

00:01:17,200 --> 00:01:22,640
necessary for this so

00:01:20,320 --> 00:01:23,360
as you can see all of the all the

00:01:22,640 --> 00:01:25,759
workloads

00:01:23,360 --> 00:01:26,560
including the istio control plane are

00:01:25,759 --> 00:01:30,479
communicating

00:01:26,560 --> 00:01:32,479
directly with the kubernetes api server

00:01:30,479 --> 00:01:34,479
in the case of the control plane it

00:01:32,479 --> 00:01:38,320
communicates with the api server

00:01:34,479 --> 00:01:39,439
to get the endpoint information for all

00:01:38,320 --> 00:01:42,240
the workloads

00:01:39,439 --> 00:01:43,040
as well as service info and other

00:01:42,240 --> 00:01:45,920
metadata

00:01:43,040 --> 00:01:45,920
about the cluster

00:01:46,240 --> 00:01:50,159
so let's just quickly review the single

00:01:48,960 --> 00:01:52,560
cluster scenario

00:01:50,159 --> 00:01:54,640
and why it had what the benefits are and

00:01:52,560 --> 00:01:56,640
why it's more simplistic

00:01:54,640 --> 00:01:59,280
so the control plane and the data plane

00:01:56,640 --> 00:02:01,360
both live on the same cluster

00:01:59,280 --> 00:02:03,360
authentication with the api server is

00:02:01,360 --> 00:02:05,920
handled automatically by

00:02:03,360 --> 00:02:06,719
kubernetes so the service account tokens

00:02:05,920 --> 00:02:10,399
are generated

00:02:06,719 --> 00:02:13,040
and mounted automatically and the tokens

00:02:10,399 --> 00:02:14,239
remain safely in kubernetes more on this

00:02:13,040 --> 00:02:18,400
later

00:02:14,239 --> 00:02:20,800
most likely no need to revoke those

00:02:18,400 --> 00:02:21,520
and lastly configuration can be easily

00:02:20,800 --> 00:02:24,800
housed

00:02:21,520 --> 00:02:28,720
in a single repository leading to simple

00:02:24,800 --> 00:02:28,720
easy to understand git ops workflows

00:02:29,200 --> 00:02:33,040
so now that we've quickly discussed a

00:02:31,840 --> 00:02:36,160
single cluster

00:02:33,040 --> 00:02:38,480
istio scenario let's evolve let's

00:02:36,160 --> 00:02:39,599
jump that up to a multi-cluster scenario

00:02:38,480 --> 00:02:41,760
and begin to explain

00:02:39,599 --> 00:02:42,959
the complexities and security

00:02:41,760 --> 00:02:46,160
implications

00:02:42,959 --> 00:02:48,480
involved so now

00:02:46,160 --> 00:02:50,480
as you can see we've taken our single

00:02:48,480 --> 00:02:53,760
cluster diagram and

00:02:50,480 --> 00:02:56,239
multiplied it by five now

00:02:53,760 --> 00:02:57,120
in this scenario all of the control

00:02:56,239 --> 00:03:00,000
planes

00:02:57,120 --> 00:03:00,480
still uh need access to the cluster that

00:03:00,000 --> 00:03:04,080
they are

00:03:00,480 --> 00:03:05,760
running on um but

00:03:04,080 --> 00:03:08,159
in order for a multi-cluster surface

00:03:05,760 --> 00:03:10,480
mesh to be useful

00:03:08,159 --> 00:03:11,599
the istio control planes also need to be

00:03:10,480 --> 00:03:14,720
able to read

00:03:11,599 --> 00:03:17,599
and write to the other clusters

00:03:14,720 --> 00:03:18,239
so what does that look like uh for us

00:03:17,599 --> 00:03:21,680
well

00:03:18,239 --> 00:03:23,920
we've quickly drawn up an example so now

00:03:21,680 --> 00:03:25,599
we can see all of the istio control

00:03:23,920 --> 00:03:28,640
planes have access

00:03:25,599 --> 00:03:31,760
to every other cluster's

00:03:28,640 --> 00:03:35,040
kubernetes api server

00:03:31,760 --> 00:03:37,040
so what does this mean in practice well

00:03:35,040 --> 00:03:38,560
in practice this means that every

00:03:37,040 --> 00:03:41,599
control plane

00:03:38,560 --> 00:03:45,120
needs to have uh credentials

00:03:41,599 --> 00:03:47,760
to access those other clusters so

00:03:45,120 --> 00:03:50,480
as we mentioned earlier the the

00:03:47,760 --> 00:03:54,000
kubernetes credentials need to leave

00:03:50,480 --> 00:03:56,560
the kubernetes uh the single cluster

00:03:54,000 --> 00:03:56,560
boundary

00:03:57,040 --> 00:04:00,640
so multi-cluster service mesh the

00:03:59,280 --> 00:04:03,360
control plane and the data plan are

00:04:00,640 --> 00:04:05,280
spread out across many clusters

00:04:03,360 --> 00:04:07,519
each control plane needs access to each

00:04:05,280 --> 00:04:10,879
other cluster that means we have the an

00:04:07,519 --> 00:04:14,400
order n squared connections where n

00:04:10,879 --> 00:04:16,479
is the number of clusters

00:04:14,400 --> 00:04:17,680
authentication is not handled

00:04:16,479 --> 00:04:19,759
automatically for our

00:04:17,680 --> 00:04:21,199
remote control planes so service

00:04:19,759 --> 00:04:23,840
accounts must be

00:04:21,199 --> 00:04:25,280
created coupe configs containing the

00:04:23,840 --> 00:04:27,199
service account token must be

00:04:25,280 --> 00:04:29,120
distributed to each cluster

00:04:27,199 --> 00:04:30,240
now these coup configs which must leave

00:04:29,120 --> 00:04:32,080
cluster borders

00:04:30,240 --> 00:04:34,400
give potentially dangerous levels of

00:04:32,080 --> 00:04:36,160
access to said cluster

00:04:34,400 --> 00:04:38,800
if any kubeconfig credential were to

00:04:36,160 --> 00:04:39,759
leak replacing it and redistributing the

00:04:38,800 --> 00:04:43,040
credentials is a

00:04:39,759 --> 00:04:45,759
convoluted process keeping all of these

00:04:43,040 --> 00:04:47,840
coupe credentials in sync across all the

00:04:45,759 --> 00:04:50,320
different clusters

00:04:47,840 --> 00:04:51,680
is very difficult and not easily done

00:04:50,320 --> 00:04:54,240
using git ops

00:04:51,680 --> 00:04:56,080
so and on top of this in deployments

00:04:54,240 --> 00:04:58,240
with many clusters

00:04:56,080 --> 00:04:59,280
potentially many control planes are

00:04:58,240 --> 00:05:01,680
performing i o

00:04:59,280 --> 00:05:03,919
on each api server which can slow it

00:05:01,680 --> 00:05:03,919
down

00:05:04,560 --> 00:05:10,720
so the approach that i just mentioned

00:05:07,759 --> 00:05:12,080
with order n squared connections that's

00:05:10,720 --> 00:05:15,280
one way of doing it that's the

00:05:12,080 --> 00:05:16,080
that's that was the uh istio way of

00:05:15,280 --> 00:05:19,039
doing it

00:05:16,080 --> 00:05:19,840
um beforehand but a new way is starting

00:05:19,039 --> 00:05:22,000
to emerge

00:05:19,840 --> 00:05:24,000
which we think is a lot better and

00:05:22,000 --> 00:05:27,600
that's the external control point

00:05:24,000 --> 00:05:30,800
so in this scenario istio is running

00:05:27,600 --> 00:05:34,400
or the service mesh is running on a

00:05:30,800 --> 00:05:37,440
management cluster and managing

00:05:34,400 --> 00:05:39,840
a bunch of clusters on its own and in

00:05:37,440 --> 00:05:39,840
this way

00:05:42,240 --> 00:05:45,520
there doesn't need to be a control plane

00:05:44,320 --> 00:05:47,680
on each cluster

00:05:45,520 --> 00:05:49,280
that needs access to each and every

00:05:47,680 --> 00:05:51,120
other cluster

00:05:49,280 --> 00:05:52,400
so there are of course downsides to this

00:05:51,120 --> 00:05:55,840
scenario

00:05:52,400 --> 00:05:57,199
such as if the management cluster were

00:05:55,840 --> 00:05:59,440
to go down

00:05:57,199 --> 00:06:00,479
then the control plane would also go

00:05:59,440 --> 00:06:02,639
down so

00:06:00,479 --> 00:06:04,639
it is important to build in redundancy

00:06:02,639 --> 00:06:08,800
for the management cluster

00:06:04,639 --> 00:06:12,240
but you also reduce the number of

00:06:08,800 --> 00:06:14,720
places that need the kube credentials

00:06:12,240 --> 00:06:17,039
as well as reducing the load on the api

00:06:14,720 --> 00:06:19,039
server

00:06:17,039 --> 00:06:20,400
so the external control plane the

00:06:19,039 --> 00:06:22,240
control planes are now on a single

00:06:20,400 --> 00:06:23,280
cluster and the data planes are still

00:06:22,240 --> 00:06:25,360
spread out

00:06:23,280 --> 00:06:26,720
now we like to call this the push model

00:06:25,360 --> 00:06:29,039
of

00:06:26,720 --> 00:06:30,960
configuration there's one centralized

00:06:29,039 --> 00:06:32,479
control plane with access to all data

00:06:30,960 --> 00:06:35,680
plane clusters so that's order

00:06:32,479 --> 00:06:37,280
n connections and

00:06:35,680 --> 00:06:39,199
the centralized control plane will be

00:06:37,280 --> 00:06:42,319
pushing config out

00:06:39,199 --> 00:06:43,840
to the remote clusters each control

00:06:42,319 --> 00:06:46,639
plane will still need access

00:06:43,840 --> 00:06:47,520
to the api server of each node or remote

00:06:46,639 --> 00:06:49,039
cluster

00:06:47,520 --> 00:06:51,199
and although the number of network edges

00:06:49,039 --> 00:06:52,639
has shrunk the same problems from the

00:06:51,199 --> 00:06:55,360
previous model still exist

00:06:52,639 --> 00:06:56,639
albeit on a smaller scale still hard to

00:06:55,360 --> 00:06:58,560
distribute the

00:06:56,639 --> 00:07:02,639
credentials and the leaked credentials

00:06:58,560 --> 00:07:04,080
can give bad actors outsized power

00:07:02,639 --> 00:07:05,840
so what are our concerns our main

00:07:04,080 --> 00:07:07,199
concerns about administering

00:07:05,840 --> 00:07:10,560
multi-cluster seo

00:07:07,199 --> 00:07:11,680
or service mesh securely one difficult

00:07:10,560 --> 00:07:13,520
to store and sync

00:07:11,680 --> 00:07:15,599
all of the coupe credentials using

00:07:13,520 --> 00:07:17,599
traditional git ops workflows

00:07:15,599 --> 00:07:19,440
and two giving a cluster read write

00:07:17,599 --> 00:07:20,560
access to many other clusters leaves a

00:07:19,440 --> 00:07:24,639
potentially large

00:07:20,560 --> 00:07:25,759
security threat so how can we get around

00:07:24,639 --> 00:07:29,440
some of these issues

00:07:25,759 --> 00:07:33,440
well the way that we thought about it is

00:07:29,440 --> 00:07:36,319
something called a pull-based method of

00:07:33,440 --> 00:07:37,199
config so if you notice in the previous

00:07:36,319 --> 00:07:40,240
diagram

00:07:37,199 --> 00:07:40,639
all of the arrows were pointing out to

00:07:40,240 --> 00:07:43,759
the

00:07:40,639 --> 00:07:46,240
to the remote clusters whereas now

00:07:43,759 --> 00:07:47,599
all the arrows are pointing in at the

00:07:46,240 --> 00:07:49,759
management cluster

00:07:47,599 --> 00:07:51,840
so it's still an external control plane

00:07:49,759 --> 00:07:54,400
but it's pull base this time

00:07:51,840 --> 00:07:56,840
so this so now all of the remote

00:07:54,400 --> 00:07:59,680
clusters are actually creating the

00:07:56,840 --> 00:08:01,919
connections to the management plane

00:07:59,680 --> 00:08:03,280
so the management plane doesn't need to

00:08:01,919 --> 00:08:07,039
have access

00:08:03,280 --> 00:08:07,039
to any of the remote clusters

00:08:07,280 --> 00:08:12,639
and neither does the service mesh

00:08:09,919 --> 00:08:15,840
running there

00:08:12,639 --> 00:08:17,199
so what benefits does this bring us well

00:08:15,840 --> 00:08:19,599
this pull model of

00:08:17,199 --> 00:08:20,319
configuration as i mentioned there's one

00:08:19,599 --> 00:08:22,000
centralized

00:08:20,319 --> 00:08:23,759
control plane which need act which needs

00:08:22,000 --> 00:08:26,879
access to zero

00:08:23,759 --> 00:08:28,720
data plane clusters all the data plane

00:08:26,879 --> 00:08:31,520
clusters connect to the control plane

00:08:28,720 --> 00:08:32,000
cluster and receive updates that way the

00:08:31,520 --> 00:08:34,159
config

00:08:32,000 --> 00:08:35,680
updates function similarly to the envoy

00:08:34,159 --> 00:08:37,279
xds protocol

00:08:35,680 --> 00:08:38,839
for those familiar and for those

00:08:37,279 --> 00:08:40,320
unfamiliar we will go over that in a

00:08:38,839 --> 00:08:42,159
minute

00:08:40,320 --> 00:08:43,680
and linked credentials no longer give

00:08:42,159 --> 00:08:47,120
the same direct control

00:08:43,680 --> 00:08:48,959
over the api server furthermore

00:08:47,120 --> 00:08:52,640
no kubernetes credentials need to be

00:08:48,959 --> 00:08:52,640
shared outside of the cluster borders

00:08:52,839 --> 00:08:57,360
and since we are no longer distributing

00:08:55,680 --> 00:09:00,399
these credentials

00:08:57,360 --> 00:09:01,360
the the remote components we are

00:09:00,399 --> 00:09:04,000
distributing

00:09:01,360 --> 00:09:06,000
can be easily stored and distributed via

00:09:04,000 --> 00:09:09,200
githubs

00:09:06,000 --> 00:09:11,920
so how does this really work well

00:09:09,200 --> 00:09:13,040
we took heavy inspiration from envoys

00:09:11,920 --> 00:09:14,959
xds protocol

00:09:13,040 --> 00:09:16,480
and the kubernetes bootstrap token which

00:09:14,959 --> 00:09:18,800
i will explain now

00:09:16,480 --> 00:09:19,839
but those links are available from the

00:09:18,800 --> 00:09:23,279
slide

00:09:19,839 --> 00:09:27,040
the way that the xds protocol works is

00:09:23,279 --> 00:09:28,959
essentially envoy makes a connection

00:09:27,040 --> 00:09:30,720
to the control plane or the management

00:09:28,959 --> 00:09:33,600
server and says

00:09:30,720 --> 00:09:34,880
hey i want some config updates and the

00:09:33,600 --> 00:09:37,600
management server says

00:09:34,880 --> 00:09:38,160
all right great i've got that for you

00:09:37,600 --> 00:09:41,680
and so

00:09:38,160 --> 00:09:42,640
in that way each remote instance each

00:09:41,680 --> 00:09:46,480
envoy

00:09:42,640 --> 00:09:49,519
is responsible for creating the

00:09:46,480 --> 00:09:51,360
the connection and then the

00:09:49,519 --> 00:09:52,880
management server is responsible for

00:09:51,360 --> 00:09:56,480
updating envoy

00:09:52,880 --> 00:09:58,959
with the config when necessary

00:09:56,480 --> 00:10:00,560
and the kubernetes bootstrap token is a

00:09:58,959 --> 00:10:04,240
way of verifying

00:10:00,560 --> 00:10:07,519
the identity of different callers

00:10:04,240 --> 00:10:10,560
so more on that in a second

00:10:07,519 --> 00:10:11,120
so how does it really work well in this

00:10:10,560 --> 00:10:13,680
case

00:10:11,120 --> 00:10:15,839
the server would live in is our

00:10:13,680 --> 00:10:16,320
management plane that is the component

00:10:15,839 --> 00:10:19,200
living

00:10:16,320 --> 00:10:19,680
in the in the management cluster and the

00:10:19,200 --> 00:10:21,839
agent

00:10:19,680 --> 00:10:24,079
and the agent is the component that gets

00:10:21,839 --> 00:10:27,279
deployed to each remote cluster

00:10:24,079 --> 00:10:30,560
or each data plane cluster

00:10:27,279 --> 00:10:31,920
so we start off with a ca or signing

00:10:30,560 --> 00:10:33,760
cert

00:10:31,920 --> 00:10:35,920
on the server and a server cert on both

00:10:33,760 --> 00:10:37,600
the agent and the server

00:10:35,920 --> 00:10:40,160
and the server cert will be used to

00:10:37,600 --> 00:10:42,399
establish the initial tls

00:10:40,160 --> 00:10:43,519
connection in addition we distribute a

00:10:42,399 --> 00:10:46,079
bootstrap token to

00:10:43,519 --> 00:10:48,000
both clusters and this bootstrap token

00:10:46,079 --> 00:10:50,399
will be used in the initial

00:10:48,000 --> 00:10:51,279
request with the token so that the

00:10:50,399 --> 00:10:55,120
server

00:10:51,279 --> 00:10:56,800
knows to trust the agent and then can

00:10:55,120 --> 00:10:58,720
and then once it knows it can trust the

00:10:56,800 --> 00:11:01,760
agent it will respond

00:10:58,720 --> 00:11:04,959
with its mtls cert and it's with the

00:11:01,760 --> 00:11:08,720
identity and this mtls cert is

00:11:04,959 --> 00:11:11,120
how the server knows what data to send

00:11:08,720 --> 00:11:12,839
back to the agent so once that identity

00:11:11,120 --> 00:11:15,360
has been

00:11:12,839 --> 00:11:18,399
uh provided

00:11:15,360 --> 00:11:22,000
then we can open our uh our

00:11:18,399 --> 00:11:24,720
config streams so the first one we have

00:11:22,000 --> 00:11:25,760
is a remote resource stream and this

00:11:24,720 --> 00:11:27,600
will stream

00:11:25,760 --> 00:11:29,680
the resources from the dataplane

00:11:27,600 --> 00:11:31,839
clusters from the agent to the server

00:11:29,680 --> 00:11:33,680
so that it can make decisions about the

00:11:31,839 --> 00:11:35,839
system

00:11:33,680 --> 00:11:36,720
and then the second one will stream the

00:11:35,839 --> 00:11:39,040
mesh config

00:11:36,720 --> 00:11:39,760
or the config needed to run the data

00:11:39,040 --> 00:11:43,839
planes

00:11:39,760 --> 00:11:43,839
to the agent clusters

00:11:44,000 --> 00:11:47,920
so can the pull model really solve all

00:11:45,839 --> 00:11:51,040
of our problems short answer

00:11:47,920 --> 00:11:52,000
no but it does alleviate the two main

00:11:51,040 --> 00:11:55,360
issues that we

00:11:52,000 --> 00:11:57,040
began to discuss a few slides back

00:11:55,360 --> 00:11:58,800
the first being giving a cluster read

00:11:57,040 --> 00:12:00,399
write access to many other clusters

00:11:58,800 --> 00:12:03,120
leaves a potentially large

00:12:00,399 --> 00:12:05,360
security threat well since our control

00:12:03,120 --> 00:12:06,880
planes no longer need direct access to

00:12:05,360 --> 00:12:08,560
remote api servers

00:12:06,880 --> 00:12:10,959
the kubernetes credentials no longer

00:12:08,560 --> 00:12:12,880
need to leave cluster boundaries

00:12:10,959 --> 00:12:14,160
this reduces the chance that a bad actor

00:12:12,880 --> 00:12:16,880
gets elevated access

00:12:14,160 --> 00:12:17,920
to the system and reduces the load on

00:12:16,880 --> 00:12:21,120
the api server

00:12:17,920 --> 00:12:22,000
by reducing the number of clients the

00:12:21,120 --> 00:12:24,240
second

00:12:22,000 --> 00:12:25,120
difficult to store and sync all these

00:12:24,240 --> 00:12:27,839
cube credentials

00:12:25,120 --> 00:12:29,600
using github's workflows well luckily

00:12:27,839 --> 00:12:31,360
there's no longer any need to store or

00:12:29,600 --> 00:12:33,519
sync these coup credentials

00:12:31,360 --> 00:12:34,720
uh on top of which the the components

00:12:33,519 --> 00:12:37,680
that do need to be synced

00:12:34,720 --> 00:12:39,680
mainly the agent and the token can

00:12:37,680 --> 00:12:42,800
easily be integrated with get ops

00:12:39,680 --> 00:12:43,760
see part two of the talk so what were

00:12:42,800 --> 00:12:45,519
those question marks that we were

00:12:43,760 --> 00:12:47,519
talking about earlier well it just so

00:12:45,519 --> 00:12:49,279
happens that we at solo i o have created

00:12:47,519 --> 00:12:51,519
a product called glue mesh

00:12:49,279 --> 00:12:52,560
which can which can handle this but more

00:12:51,519 --> 00:12:54,959
on that later

00:12:52,560 --> 00:12:57,279
for now i'm going to kick it over to

00:12:54,959 --> 00:12:57,279
eric

00:12:58,240 --> 00:13:01,360
thank you eton so next i want to talk

00:13:00,480 --> 00:13:04,480
about

00:13:01,360 --> 00:13:06,480
how you can deploy glue mesh securely

00:13:04,480 --> 00:13:10,240
across multiple clusters

00:13:06,480 --> 00:13:10,240
using git ops and argo cd

00:13:11,839 --> 00:13:15,120
so i have a demo i want to show you

00:13:13,360 --> 00:13:16,959
today and in that demo i have two

00:13:15,120 --> 00:13:18,720
clusters running i have a management

00:13:16,959 --> 00:13:20,399
cluster and on that management cluster i

00:13:18,720 --> 00:13:23,040
have argo cd deployed

00:13:20,399 --> 00:13:24,160
i'm going to use argo cd to deploy

00:13:23,040 --> 00:13:26,800
configuration

00:13:24,160 --> 00:13:28,720
from a git repository and automate the

00:13:26,800 --> 00:13:30,000
deployment of the glue mesh management

00:13:28,720 --> 00:13:32,240
plane

00:13:30,000 --> 00:13:34,079
on the remote cluster i have a separate

00:13:32,240 --> 00:13:35,920
instance of argo cd running and i'm

00:13:34,079 --> 00:13:39,760
going to use configuration there

00:13:35,920 --> 00:13:41,600
to deploy the agent automatically

00:13:39,760 --> 00:13:42,959
so there are multiple benefits here to

00:13:41,600 --> 00:13:44,959
this approach where you can take

00:13:42,959 --> 00:13:47,040
configuration as code you can store

00:13:44,959 --> 00:13:49,360
everything in your git repository

00:13:47,040 --> 00:13:50,240
and you can have a repeatable deployment

00:13:49,360 --> 00:13:52,399
process

00:13:50,240 --> 00:13:53,600
for both the glue mesh management plane

00:13:52,399 --> 00:13:55,440
and the agent

00:13:53,600 --> 00:13:57,680
and this allows you to automatically

00:13:55,440 --> 00:13:58,959
update your glue mesh deployments with

00:13:57,680 --> 00:14:02,079
that configuration

00:13:58,959 --> 00:14:03,680
with argo cd additionally if you need to

00:14:02,079 --> 00:14:06,480
spin up new clusters

00:14:03,680 --> 00:14:07,920
and deploy new meshes you can do it in

00:14:06,480 --> 00:14:10,800
an automated way

00:14:07,920 --> 00:14:13,040
more quickly and more effectively

00:14:10,800 --> 00:14:13,920
additionally in a disaster recovery

00:14:13,040 --> 00:14:15,600
scenario

00:14:13,920 --> 00:14:17,600
you want to be able to replace your

00:14:15,600 --> 00:14:18,399
clusters and your meshes as quickly as

00:14:17,600 --> 00:14:20,639
possible

00:14:18,399 --> 00:14:24,240
and you can do that with this approach

00:14:20,639 --> 00:14:28,800
even including that management cluster

00:14:24,240 --> 00:14:30,560
okay so now let's jump into the demo

00:14:28,800 --> 00:14:32,399
so what i'm going to show here in this

00:14:30,560 --> 00:14:33,360
demo is actually how to physically

00:14:32,399 --> 00:14:36,880
deploy

00:14:33,360 --> 00:14:38,440
glue mesh using argo cd okay

00:14:36,880 --> 00:14:40,720
and so to do that i have some

00:14:38,440 --> 00:14:42,240
configurations uh

00:14:40,720 --> 00:14:43,680
checked into my git repository

00:14:42,240 --> 00:14:45,040
everything is deployed from a git

00:14:43,680 --> 00:14:48,000
repository

00:14:45,040 --> 00:14:49,120
using argo cd and in that git repository

00:14:48,000 --> 00:14:52,480
i have crds

00:14:49,120 --> 00:14:52,880
that are specific to argo cd and so this

00:14:52,480 --> 00:14:55,680
is an

00:14:52,880 --> 00:14:58,560
application crt and inside of that

00:14:55,680 --> 00:15:02,079
application crd you can actually define

00:14:58,560 --> 00:15:04,880
values for a helm chart so behind the

00:15:02,079 --> 00:15:07,519
scenes argo cd is actually using helm

00:15:04,880 --> 00:15:08,639
to deploy the glue mesh management plane

00:15:07,519 --> 00:15:11,120
and the agent

00:15:08,639 --> 00:15:12,480
so in this case i'm using the glue mesh

00:15:11,120 --> 00:15:16,079
enterprise chart

00:15:12,480 --> 00:15:19,199
right and i'm setting a license key

00:15:16,079 --> 00:15:20,240
and some other values such as self sign

00:15:19,199 --> 00:15:21,600
equals false

00:15:20,240 --> 00:15:23,360
that means i'm providing the

00:15:21,600 --> 00:15:25,839
certificates in advance

00:15:23,360 --> 00:15:26,480
of the installation i'm not depending on

00:15:25,839 --> 00:15:30,480
glue mesh

00:15:26,480 --> 00:15:32,160
to generate certificates on its own okay

00:15:30,480 --> 00:15:34,160
and then if i go over to this other

00:15:32,160 --> 00:15:37,920
application crd i can see

00:15:34,160 --> 00:15:40,160
the helm chart for deploying the agent

00:15:37,920 --> 00:15:41,519
and so here the chart is enterprise

00:15:40,160 --> 00:15:44,160
agent

00:15:41,519 --> 00:15:46,079
and i'm giving it a server address for

00:15:44,160 --> 00:15:48,639
the management cluster i'm giving it

00:15:46,079 --> 00:15:49,600
an authority for the certificates that

00:15:48,639 --> 00:15:51,600
are used

00:15:49,600 --> 00:15:55,360
and i'm giving it a cluster name which

00:15:51,600 --> 00:15:57,199
is simply remote cluster okay

00:15:55,360 --> 00:15:59,120
so now let me walk you through the steps

00:15:57,199 --> 00:16:02,079
for actually conducting the

00:15:59,120 --> 00:16:03,040
deployment with argo cd okay so first

00:16:02,079 --> 00:16:05,759
i'm going to switch

00:16:03,040 --> 00:16:08,320
my cubecodal context over to the

00:16:05,759 --> 00:16:10,560
management cluster

00:16:08,320 --> 00:16:14,639
next i'm going to create a new namespace

00:16:10,560 --> 00:16:14,639
for glue mesh on that management cluster

00:16:15,040 --> 00:16:19,279
next i'm actually going to kick off the

00:16:17,360 --> 00:16:21,600
deployment of

00:16:19,279 --> 00:16:24,160
the glue mesh management plane using

00:16:21,600 --> 00:16:26,240
argo cd and i can do that by simply

00:16:24,160 --> 00:16:29,519
doing a cube cuddle apply on that

00:16:26,240 --> 00:16:29,519
application crd

00:16:30,240 --> 00:16:35,839
okay so that's done so what i can do is

00:16:32,959 --> 00:16:37,360
go over here and log into the argo cd

00:16:35,839 --> 00:16:41,360
deployment

00:16:37,360 --> 00:16:43,519
on my management cluster and you can see

00:16:41,360 --> 00:16:45,440
how quickly that was actually deployed

00:16:43,519 --> 00:16:47,120
everything is synced and healthy already

00:16:45,440 --> 00:16:48,800
in a matter of seconds

00:16:47,120 --> 00:16:50,560
and if i click in here i can see

00:16:48,800 --> 00:16:53,839
everything that was deployed

00:16:50,560 --> 00:16:55,199
for the glue mesh management plane okay

00:16:53,839 --> 00:16:57,279
so you can see there's

00:16:55,199 --> 00:16:59,920
a lot of different things deployed here

00:16:57,279 --> 00:17:01,519
right everything is green

00:16:59,920 --> 00:17:03,199
everything is deployed everything is

00:17:01,519 --> 00:17:04,319
healthy include all the pods for the

00:17:03,199 --> 00:17:07,919
dashboard

00:17:04,319 --> 00:17:10,480
the enterprise networking and so

00:17:07,919 --> 00:17:11,600
that all looks good as you can imagine

00:17:10,480 --> 00:17:13,360
in a glue mesh

00:17:11,600 --> 00:17:15,120
the management plan is you know a little

00:17:13,360 --> 00:17:18,240
bit complicated because

00:17:15,120 --> 00:17:19,439
it does manage multiple clusters with

00:17:18,240 --> 00:17:21,280
multiple meshes

00:17:19,439 --> 00:17:22,720
and so there are a lot of capabilities

00:17:21,280 --> 00:17:24,480
built into it

00:17:22,720 --> 00:17:26,559
then if i step out here and look at the

00:17:24,480 --> 00:17:27,039
rest of my application deployment i can

00:17:26,559 --> 00:17:29,280
see

00:17:27,039 --> 00:17:30,640
configurations for the glue mesh

00:17:29,280 --> 00:17:34,160
management plane

00:17:30,640 --> 00:17:36,000
so here i can see that there is a

00:17:34,160 --> 00:17:38,480
writ certificate that is stored in a

00:17:36,000 --> 00:17:41,600
secret i can see that there

00:17:38,480 --> 00:17:44,720
is a tls certificate for

00:17:41,600 --> 00:17:46,240
securing connection between the

00:17:44,720 --> 00:17:48,000
remote cluster and the management

00:17:46,240 --> 00:17:49,919
cluster there is a

00:17:48,000 --> 00:17:51,919
signing certificate also stored in the

00:17:49,919 --> 00:17:54,320
secret and that allows

00:17:51,919 --> 00:17:54,960
the glue mesh management plane to

00:17:54,320 --> 00:17:57,919
generate

00:17:54,960 --> 00:17:59,520
new certificates that can be passed to

00:17:57,919 --> 00:18:02,960
remote clusters

00:17:59,520 --> 00:18:06,000
to establish an mtls secure connection

00:18:02,960 --> 00:18:08,960
okay also there

00:18:06,000 --> 00:18:11,440
is a token that is created and i

00:18:08,960 --> 00:18:14,640
provided this token in advance

00:18:11,440 --> 00:18:18,480
so that token is deployed with

00:18:14,640 --> 00:18:20,799
the glue mesh agent and that is used

00:18:18,480 --> 00:18:22,480
for making that initial connection from

00:18:20,799 --> 00:18:25,039
the remote cluster

00:18:22,480 --> 00:18:27,360
to the management cluster as that was

00:18:25,039 --> 00:18:30,240
discussed previously

00:18:27,360 --> 00:18:31,840
finally i have a configuration called

00:18:30,240 --> 00:18:33,840
kubernetes cluster

00:18:31,840 --> 00:18:35,280
and that simply defines the remote

00:18:33,840 --> 00:18:38,880
cluster that

00:18:35,280 --> 00:18:40,640
is uh configured to be registered

00:18:38,880 --> 00:18:42,880
by the agent and i'll walk through how

00:18:40,640 --> 00:18:45,679
that works okay

00:18:42,880 --> 00:18:47,120
so let me go over here to my console and

00:18:45,679 --> 00:18:51,520
i'm actually going to

00:18:47,120 --> 00:18:54,559
kick off the mesh cuddle command

00:18:51,520 --> 00:18:57,280
and so that will allow me to see

00:18:54,559 --> 00:18:58,960
what is currently um running what is

00:18:57,280 --> 00:19:01,440
currently registered

00:18:58,960 --> 00:19:02,960
um in the glue mesh management plane

00:19:01,440 --> 00:19:04,880
okay so

00:19:02,960 --> 00:19:07,440
as i mentioned before there is a

00:19:04,880 --> 00:19:09,919
kubernetes cluster crd

00:19:07,440 --> 00:19:11,360
you can see that uh the cluster is there

00:19:09,919 --> 00:19:14,240
right

00:19:11,360 --> 00:19:15,840
but if i go in here there are no meshes

00:19:14,240 --> 00:19:18,000
that were discovered right

00:19:15,840 --> 00:19:19,440
and that is because the agent has not

00:19:18,000 --> 00:19:21,200
yet completed

00:19:19,440 --> 00:19:23,280
registration with the glue mesh

00:19:21,200 --> 00:19:24,880
management plane okay

00:19:23,280 --> 00:19:27,440
and so that is going to be the next part

00:19:24,880 --> 00:19:27,440
of the demo

00:19:27,760 --> 00:19:32,640
okay so let me go back over here and so

00:19:30,960 --> 00:19:35,280
what i'm going to do is i'm going to

00:19:32,640 --> 00:19:39,760
change my cube cuddle context

00:19:35,280 --> 00:19:42,240
to b for the remote cluster

00:19:39,760 --> 00:19:45,200
i'm going to also create a glue mesh

00:19:42,240 --> 00:19:45,200
namespace there

00:19:45,360 --> 00:19:50,320
and i'm going to deploy that other

00:19:47,919 --> 00:19:52,240
application crd specific to the glue

00:19:50,320 --> 00:19:55,440
mesh agent okay

00:19:52,240 --> 00:19:58,480
so let me paste that in there

00:19:55,440 --> 00:19:58,880
there we go all right so now let me go

00:19:58,480 --> 00:20:01,600
over

00:19:58,880 --> 00:20:03,600
and log into the other instance of vargo

00:20:01,600 --> 00:20:05,520
cd this is not the same argo cd this is

00:20:03,600 --> 00:20:06,400
the one deployed on the remote cluster

00:20:05,520 --> 00:20:09,440
okay

00:20:06,400 --> 00:20:10,880
so i'm going to click sign in here and

00:20:09,440 --> 00:20:14,159
here you can see that

00:20:10,880 --> 00:20:17,200
um everything that is needed for

00:20:14,159 --> 00:20:19,039
the glue mesh agent is deployed right

00:20:17,200 --> 00:20:20,880
so that also happened very quickly in a

00:20:19,039 --> 00:20:21,679
matter of seconds you can see that

00:20:20,880 --> 00:20:23,919
there's a

00:20:21,679 --> 00:20:24,960
enterprise agent pod that's running

00:20:23,919 --> 00:20:27,760
right here

00:20:24,960 --> 00:20:29,600
so that is green that is healthy so

00:20:27,760 --> 00:20:31,919
everything looks good for the

00:20:29,600 --> 00:20:33,679
glue mesh agent deployment then if i go

00:20:31,919 --> 00:20:34,720
over here and look at the rest of my

00:20:33,679 --> 00:20:37,120
deployment

00:20:34,720 --> 00:20:38,320
i can see the actual configurations that

00:20:37,120 --> 00:20:41,120
are residing in that

00:20:38,320 --> 00:20:41,520
git repository deployed by glue mesh

00:20:41,120 --> 00:20:44,559
okay

00:20:41,520 --> 00:20:47,520
so if you look here

00:20:44,559 --> 00:20:50,080
um there is a secret that is deployed

00:20:47,520 --> 00:20:52,400
with that wrist certificate right

00:20:50,080 --> 00:20:53,200
and there's also that identity token

00:20:52,400 --> 00:20:56,559
secret

00:20:53,200 --> 00:20:57,360
so that is the same token that was

00:20:56,559 --> 00:21:00,000
deployed

00:20:57,360 --> 00:21:00,720
on the management cluster and once again

00:21:00,000 --> 00:21:03,600
that token

00:21:00,720 --> 00:21:05,679
is used to authenticate the remote

00:21:03,600 --> 00:21:08,559
cluster with the management cluster

00:21:05,679 --> 00:21:10,240
and establish that initial connection

00:21:08,559 --> 00:21:13,200
with the management plane

00:21:10,240 --> 00:21:14,480
for glue mesh okay and when that happens

00:21:13,200 --> 00:21:17,280
it receives back an

00:21:14,480 --> 00:21:18,240
mtls certificate and a new connection is

00:21:17,280 --> 00:21:21,760
established

00:21:18,240 --> 00:21:25,760
so there is a fully secure connection

00:21:21,760 --> 00:21:27,280
for the glue mesh agent okay

00:21:25,760 --> 00:21:28,960
and one thing i want to call out here

00:21:27,280 --> 00:21:31,039
you know just for demo purposes i'm

00:21:28,960 --> 00:21:32,720
putting the certificates

00:21:31,039 --> 00:21:34,640
into git you know for the risk

00:21:32,720 --> 00:21:35,520
certificate that is not the best

00:21:34,640 --> 00:21:38,240
practice

00:21:35,520 --> 00:21:40,080
instead you could use um remote secrets

00:21:38,240 --> 00:21:41,760
you could integrate with hashicorp vault

00:21:40,080 --> 00:21:43,600
you can integrate with your

00:21:41,760 --> 00:21:45,760
cloud provider with their certificate

00:21:43,600 --> 00:21:47,280
management services

00:21:45,760 --> 00:21:50,240
so once again that's just for demo

00:21:47,280 --> 00:21:52,480
purposes okay

00:21:50,240 --> 00:21:54,640
um and by the way um you know all this

00:21:52,480 --> 00:21:56,640
is stored in in the git repository as i

00:21:54,640 --> 00:21:58,400
mentioned so if you go in here

00:21:56,640 --> 00:22:00,799
you know you can look and you can see

00:21:58,400 --> 00:22:03,520
you know these configurations for

00:22:00,799 --> 00:22:04,080
the glue mesh agent you can see like the

00:22:03,520 --> 00:22:07,200
token

00:22:04,080 --> 00:22:07,600
the token value right here right you can

00:22:07,200 --> 00:22:10,799
see

00:22:07,600 --> 00:22:12,000
the um the root tls secret you can see

00:22:10,799 --> 00:22:16,000
the ca cert right

00:22:12,000 --> 00:22:17,520
here right so all that is deployed using

00:22:16,000 --> 00:22:21,360
that that get ops approach

00:22:17,520 --> 00:22:22,400
okay okay so since we deployed that

00:22:21,360 --> 00:22:24,400
agent

00:22:22,400 --> 00:22:26,080
uh it should be registered so let's go

00:22:24,400 --> 00:22:29,600
back and look at the glue

00:22:26,080 --> 00:22:32,960
mesh console again and you can see

00:22:29,600 --> 00:22:34,720
now that under meshes right here

00:22:32,960 --> 00:22:36,880
i actually have a mesh that's registered

00:22:34,720 --> 00:22:38,880
right it's this sdod

00:22:36,880 --> 00:22:40,080
istio system remote cluster right here

00:22:38,880 --> 00:22:43,440
right

00:22:40,080 --> 00:22:44,400
and the mesh health is green and i can

00:22:43,440 --> 00:22:46,799
see that

00:22:44,400 --> 00:22:48,640
there are some destinations that were

00:22:46,799 --> 00:22:49,440
automatically discovered on that remote

00:22:48,640 --> 00:22:51,440
cluster

00:22:49,440 --> 00:22:53,440
keep in mind this glue mesh console is

00:22:51,440 --> 00:22:56,720
running on the management cluster

00:22:53,440 --> 00:22:58,080
so if i go in here and look at the mesh

00:22:56,720 --> 00:23:00,799
details

00:22:58,080 --> 00:23:02,799
i can see that there is a destination

00:23:00,799 --> 00:23:04,880
for the pet store application i deployed

00:23:02,799 --> 00:23:07,919
that pet store application

00:23:04,880 --> 00:23:08,400
in advance of this demo it is part of

00:23:07,919 --> 00:23:11,440
that

00:23:08,400 --> 00:23:12,640
service mesh uh offered by istio on that

00:23:11,440 --> 00:23:14,559
cluster

00:23:12,640 --> 00:23:16,000
uh the side cars were automatically

00:23:14,559 --> 00:23:19,120
ingest injected

00:23:16,000 --> 00:23:23,760
into the pods and so that was

00:23:19,120 --> 00:23:24,960
um how the pet store was automatically

00:23:23,760 --> 00:23:28,640
discovered

00:23:24,960 --> 00:23:28,640
uh by glue mesh okay

00:23:28,880 --> 00:23:34,840
so that wraps up the demo so let me go

00:23:32,159 --> 00:23:36,799
back to the slides and just finish up

00:23:34,840 --> 00:23:39,840
here

00:23:36,799 --> 00:23:42,640
so glue mesh is a really awesome

00:23:39,840 --> 00:23:44,880
product for providing that multi-cluster

00:23:42,640 --> 00:23:48,240
multi-mesh management right

00:23:44,880 --> 00:23:51,200
but solo offers other products that

00:23:48,240 --> 00:23:52,400
integrate with kubernetes integrate with

00:23:51,200 --> 00:23:54,799
service mesh

00:23:52,400 --> 00:23:56,000
so we actually have an api gateway

00:23:54,799 --> 00:23:59,760
product called glue

00:23:56,000 --> 00:24:01,760
edge that offers integrations with istio

00:23:59,760 --> 00:24:03,279
we also have something called the glue

00:24:01,760 --> 00:24:06,799
portal so that provides

00:24:03,279 --> 00:24:09,919
a developer portal a user interface

00:24:06,799 --> 00:24:11,440
an api specification discovery that

00:24:09,919 --> 00:24:14,960
integrates with glue edge

00:24:11,440 --> 00:24:17,360
and also istio ingress and finally we

00:24:14,960 --> 00:24:19,360
offer something called

00:24:17,360 --> 00:24:20,400
webassemblyhub so that is a place where

00:24:19,360 --> 00:24:23,279
you can

00:24:20,400 --> 00:24:25,440
build webassembly extensions for the

00:24:23,279 --> 00:24:28,000
envoy proxy you can publish them

00:24:25,440 --> 00:24:30,960
you can deploy them it's compatible both

00:24:28,000 --> 00:24:32,960
with istio and glue edge

00:24:30,960 --> 00:24:35,120
so with that i thank you for watching

00:24:32,960 --> 00:24:40,400
today and if you have any questions

00:24:35,120 --> 00:24:40,400

YouTube URL: https://www.youtube.com/watch?v=6tf69XWBTEg


