Title: Challenges and Learnings: Building gRPC Python AsyncIO Stack - Lidi Zheng, Google
Publication date: 2020-08-01
Playlist: gRPC Conf 2020
Description: 
	Challenges and Learnings: Building gRPC Python AsyncIO Stack - Lidi Zheng, Google 

One of Python 3â€™s most significant improvements is AsyncIO -- the official asynchronous library. The split Python asynchronous world now shares a common destination. Although this feature has been asked to integrate with gRPC back in 2015, it never really happened until now. In this session, we will discuss the challenges of integrating an asynchronous paradigm with gRPC Core, the designs that are tailored to the AsyncIO world, and, most importantly, the collaborations from the open-source community.
Captions: 
	00:00:00,240 --> 00:00:04,560
so hello everyone today i'm going to

00:00:02,560 --> 00:00:07,120
share my challenges and learnings

00:00:04,560 --> 00:00:08,240
from building the grpc python asynchio

00:00:07,120 --> 00:00:10,000
stack

00:00:08,240 --> 00:00:11,920
so today we are going to cover four

00:00:10,000 --> 00:00:14,240
topics the first is about

00:00:11,920 --> 00:00:16,560
python's asynchronous story the second

00:00:14,240 --> 00:00:17,039
is about the collaboration between jfc

00:00:16,560 --> 00:00:19,840
team

00:00:17,039 --> 00:00:20,640
and the community third is about some

00:00:19,840 --> 00:00:23,840
highlights

00:00:20,640 --> 00:00:25,279
of the asynchial api and finally some

00:00:23,840 --> 00:00:29,199
implementation challenges

00:00:25,279 --> 00:00:32,239
i've i personally encountered

00:00:29,199 --> 00:00:33,360
so for many of you may know japanese has

00:00:32,239 --> 00:00:35,600
a lot of stacks

00:00:33,360 --> 00:00:37,120
we support 14 different languages and

00:00:35,600 --> 00:00:40,399
some of them even have

00:00:37,120 --> 00:00:42,559
more than one implementations and

00:00:40,399 --> 00:00:44,000
some information share a majority of

00:00:42,559 --> 00:00:49,039
copas but some of them

00:00:44,000 --> 00:00:52,079
are very different programming paradigm

00:00:49,039 --> 00:00:55,600
so why do we want to build the new stack

00:00:52,079 --> 00:00:58,800
if there are already many stacks existed

00:00:55,600 --> 00:01:01,680
is this is because pythons fragmented

00:00:58,800 --> 00:01:03,120
asynchronous story and as you may know

00:01:01,680 --> 00:01:05,360
python have many different

00:01:03,120 --> 00:01:07,600
uh asynchronous libraries for example

00:01:05,360 --> 00:01:10,560
like g band twisted tornado

00:01:07,600 --> 00:01:12,479
and they can split into two different

00:01:10,560 --> 00:01:13,040
kinds the first kind is they do monkey

00:01:12,479 --> 00:01:15,119
patching

00:01:13,040 --> 00:01:16,560
they patch all the standard libraries of

00:01:15,119 --> 00:01:18,240
python

00:01:16,560 --> 00:01:20,080
and the second one is providing an

00:01:18,240 --> 00:01:21,840
alternative event loop so you can

00:01:20,080 --> 00:01:25,040
register your own coding

00:01:21,840 --> 00:01:27,040
into it but the problem is if

00:01:25,040 --> 00:01:29,119
you are using more than one asynchronous

00:01:27,040 --> 00:01:30,240
library it may very likely to cause a

00:01:29,119 --> 00:01:33,439
deadlock

00:01:30,240 --> 00:01:36,400
and each one of them have many different

00:01:33,439 --> 00:01:40,560
programming paradigm so it's very hard

00:01:36,400 --> 00:01:43,040
to switch from one to the other luckily

00:01:40,560 --> 00:01:44,079
the python maintainers also realize this

00:01:43,040 --> 00:01:46,720
is a problem

00:01:44,079 --> 00:01:47,680
and they did so they developed the

00:01:46,720 --> 00:01:49,759
official

00:01:47,680 --> 00:01:51,280
asynchronous library which is named

00:01:49,759 --> 00:01:54,640
async io

00:01:51,280 --> 00:01:55,439
and it introduces the new keywords async

00:01:54,640 --> 00:01:58,799
and await

00:01:55,439 --> 00:02:02,079
just like javascript and hd shop and

00:01:58,799 --> 00:02:03,840
all and all the asymus libraries we saw

00:02:02,079 --> 00:02:05,680
before they are trying to be compatible

00:02:03,840 --> 00:02:08,160
with this new official

00:02:05,680 --> 00:02:08,720
asynchronous library and on the right

00:02:08,160 --> 00:02:11,520
side

00:02:08,720 --> 00:02:13,920
is a short snippet showing how to view

00:02:11,520 --> 00:02:16,560
how to open up a tcp connection

00:02:13,920 --> 00:02:17,520
and how to read write from it using the

00:02:16,560 --> 00:02:20,959
asynchronous

00:02:17,520 --> 00:02:24,319
using the async io semantic

00:02:20,959 --> 00:02:28,160
well um all of this is great

00:02:24,319 --> 00:02:30,400
and one of our grpc team member posted

00:02:28,160 --> 00:02:31,840
a comment saying we are going to build

00:02:30,400 --> 00:02:34,560
this thing

00:02:31,840 --> 00:02:35,360
and this comment god is the most is one

00:02:34,560 --> 00:02:39,360
of the most

00:02:35,360 --> 00:02:41,760
uploaded comments in entire drives repo

00:02:39,360 --> 00:02:44,319
however despite there is a high demand

00:02:41,760 --> 00:02:47,680
and all those benefits we talked about

00:02:44,319 --> 00:02:50,239
it never happened due to lack of

00:02:47,680 --> 00:02:53,120
engineering resources so

00:02:50,239 --> 00:02:54,319
it so the async io api never got

00:02:53,120 --> 00:02:58,239
prioritized

00:02:54,319 --> 00:03:01,760
before helping him

00:02:58,239 --> 00:03:05,440
appeared so an engineer named

00:03:01,760 --> 00:03:08,560
paul frixis and he was from sky scanner

00:03:05,440 --> 00:03:11,680
he sent us an email saying okay

00:03:08,560 --> 00:03:15,200
now we have now i had built

00:03:11,680 --> 00:03:18,319
a pro proof of concept of

00:03:15,200 --> 00:03:20,480
a grpc async io native driver and he's

00:03:18,319 --> 00:03:23,599
asking for collaboration

00:03:20,480 --> 00:03:25,040
from the grcc team and after our first

00:03:23,599 --> 00:03:27,120
meeting

00:03:25,040 --> 00:03:29,840
they promised to contribute four

00:03:27,120 --> 00:03:32,400
part-time suites

00:03:29,840 --> 00:03:34,879
and which solved which gives the project

00:03:32,400 --> 00:03:38,560
all the engineering resources it needed

00:03:34,879 --> 00:03:41,120
and then we can get finally get started

00:03:38,560 --> 00:03:42,000
and as time goes by more and more

00:03:41,120 --> 00:03:44,720
companies

00:03:42,000 --> 00:03:45,599
join the effort including some engineers

00:03:44,720 --> 00:03:47,519
from dropbox

00:03:45,599 --> 00:03:48,799
and some from uber and many other

00:03:47,519 --> 00:03:51,200
community members

00:03:48,799 --> 00:03:53,120
they're not only contributing to code

00:03:51,200 --> 00:03:57,920
but also to design itself

00:03:53,120 --> 00:03:57,920
and commenting about the directions and

00:03:59,439 --> 00:04:04,400
company about the directions so

00:04:02,879 --> 00:04:06,959
that as you can see there are many

00:04:04,400 --> 00:04:09,439
comments on the design dog

00:04:06,959 --> 00:04:10,799
this on the design dock itself and all

00:04:09,439 --> 00:04:12,799
other threads

00:04:10,799 --> 00:04:15,360
all of these flashes always call it

00:04:12,799 --> 00:04:18,639
alternative so it might not be as hard

00:04:15,360 --> 00:04:22,000
to read as as

00:04:18,639 --> 00:04:23,680
expected expected so here i want to say

00:04:22,000 --> 00:04:24,720
thank you to all the people who have

00:04:23,680 --> 00:04:28,479
contributed

00:04:24,720 --> 00:04:30,160
to this project um i know this may sound

00:04:28,479 --> 00:04:32,560
like an ending but i'm just getting

00:04:30,160 --> 00:04:32,560
started

00:04:33,520 --> 00:04:39,360
so next let me try to sell the async io

00:04:37,199 --> 00:04:42,000
api through some of these highlights

00:04:39,360 --> 00:04:43,360
the async api was tailored to the async

00:04:42,000 --> 00:04:46,000
io semantic

00:04:43,360 --> 00:04:47,040
and it has redesigned entire public

00:04:46,000 --> 00:04:50,080
interface

00:04:47,040 --> 00:04:53,759
it includes 23 new public classes

00:04:50,080 --> 00:04:56,320
64 methods or functions all the io

00:04:53,759 --> 00:04:58,479
operations now is labeled with async

00:04:56,320 --> 00:05:00,000
and is now type annotated so it's more

00:04:58,479 --> 00:05:03,440
friendly to new project

00:05:00,000 --> 00:05:06,560
to larger projects on the right side

00:05:03,440 --> 00:05:07,600
is a short example about how to use the

00:05:06,560 --> 00:05:11,039
async io

00:05:07,600 --> 00:05:13,680
api as you can see we can use

00:05:11,039 --> 00:05:14,639
a single wave to create a channel and

00:05:13,680 --> 00:05:17,600
use a weight

00:05:14,639 --> 00:05:18,880
to wait for an rpc to finish on the

00:05:17,600 --> 00:05:21,520
lower part

00:05:18,880 --> 00:05:22,479
you can define asynchronous master

00:05:21,520 --> 00:05:24,960
handlers through

00:05:22,479 --> 00:05:27,919
async defined and finally you can start

00:05:24,960 --> 00:05:31,280
the server in asynchronous way

00:05:27,919 --> 00:05:32,720
it only not only introduced jpc python

00:05:31,280 --> 00:05:34,960
into the async io world

00:05:32,720 --> 00:05:38,160
it also solves other problems for

00:05:34,960 --> 00:05:39,199
example the thread exhaustion issue so

00:05:38,160 --> 00:05:42,400
in current api

00:05:39,199 --> 00:05:44,479
if you are trying to build a if truck

00:05:42,400 --> 00:05:45,039
you're trying to initialize a grpc

00:05:44,479 --> 00:05:46,960
server

00:05:45,039 --> 00:05:48,479
you have to provide a thread pull

00:05:46,960 --> 00:05:50,400
executor which

00:05:48,479 --> 00:05:52,160
currently requires you to specify a

00:05:50,400 --> 00:05:56,319
maximum worker

00:05:52,160 --> 00:05:58,560
however if the worker number is limited

00:05:56,319 --> 00:06:01,039
there might be a threat exhaustion issue

00:05:58,560 --> 00:06:02,160
because each rpc consumes at least one

00:06:01,039 --> 00:06:04,080
thread

00:06:02,160 --> 00:06:06,080
and if you have 10 loan running

00:06:04,080 --> 00:06:08,560
streaming rpc or you have to

00:06:06,080 --> 00:06:10,400
slow user rpc which means the entire

00:06:08,560 --> 00:06:12,880
server will deadlock

00:06:10,400 --> 00:06:12,880
okay

00:06:13,600 --> 00:06:17,280
luckily this problem would will be no

00:06:16,240 --> 00:06:20,960
longer existed

00:06:17,280 --> 00:06:23,600
with async io api and moreover

00:06:20,960 --> 00:06:25,840
we're trying to unify the core entrance

00:06:23,600 --> 00:06:28,240
for the on the client side

00:06:25,840 --> 00:06:29,520
currently there's three way to call to

00:06:28,240 --> 00:06:31,840
invoke an rpc

00:06:29,520 --> 00:06:34,319
you can invoke it directly invoke it

00:06:31,840 --> 00:06:37,840
with a waste core

00:06:34,319 --> 00:06:40,639
method invoke it with a future method

00:06:37,840 --> 00:06:41,759
it creates some confusion for our users

00:06:40,639 --> 00:06:44,800
about how to use it

00:06:41,759 --> 00:06:48,000
and when to use it well in the new api

00:06:44,800 --> 00:06:51,120
there is only one user a unified

00:06:48,000 --> 00:06:54,080
way to invoke it

00:06:51,120 --> 00:06:55,199
secondly is about the streaming costs so

00:06:54,080 --> 00:06:57,599
in current api

00:06:55,199 --> 00:06:58,240
as you can see there are a lot of boy

00:06:57,599 --> 00:07:00,880
played

00:06:58,240 --> 00:07:03,199
if you are trying to send a message

00:07:00,880 --> 00:07:04,479
depending on the response you received

00:07:03,199 --> 00:07:07,120
from the server

00:07:04,479 --> 00:07:08,720
and the sending logic is before the

00:07:07,120 --> 00:07:10,479
invocation of the rpc

00:07:08,720 --> 00:07:12,160
and the receiving logic is down below so

00:07:10,479 --> 00:07:15,599
you have this

00:07:12,160 --> 00:07:17,840
conflict of reach of logic flow here

00:07:15,599 --> 00:07:20,160
well on the other hand on the new async

00:07:17,840 --> 00:07:20,639
api we're trying to introduce a read

00:07:20,160 --> 00:07:23,280
write

00:07:20,639 --> 00:07:23,840
so you can really write from a stream

00:07:23,280 --> 00:07:27,120
just

00:07:23,840 --> 00:07:29,680
like many other grpc languages design

00:07:27,120 --> 00:07:30,160
and but you can also use a very pythonic

00:07:29,680 --> 00:07:33,039
way

00:07:30,160 --> 00:07:33,919
of trying trying to iterate through each

00:07:33,039 --> 00:07:37,280
response

00:07:33,919 --> 00:07:37,280
in an rpc

00:07:37,919 --> 00:07:42,840
and finally this part may get into a

00:07:40,960 --> 00:07:45,840
little bit

00:07:42,840 --> 00:07:49,199
technical

00:07:45,840 --> 00:07:50,240
during building this uh during building

00:07:49,199 --> 00:07:54,639
this

00:07:50,240 --> 00:07:57,440
188 io api we encountered some challenge

00:07:54,639 --> 00:08:00,639
the first one is about the numbering

00:07:57,440 --> 00:08:03,759
non-blocking io itself

00:08:00,639 --> 00:08:07,199
the so if you if a

00:08:03,759 --> 00:08:09,840
application trying to run any logic

00:08:07,199 --> 00:08:12,960
that is blocking for example reading a

00:08:09,840 --> 00:08:16,400
file or writing to a network socket

00:08:12,960 --> 00:08:19,919
it might deadlock the entire event loop

00:08:16,400 --> 00:08:22,400
and here and luckily here is how a jrpg

00:08:19,919 --> 00:08:26,000
core organizes io operations

00:08:22,400 --> 00:08:29,520
so it has an io manager inside the uh

00:08:26,000 --> 00:08:32,800
jftc core and the abstract all the

00:08:29,520 --> 00:08:35,519
lower uh the the system level

00:08:32,800 --> 00:08:37,039
io operations into several categories

00:08:35,519 --> 00:08:40,320
and allows people to

00:08:37,039 --> 00:08:43,919
to provide their own so our solution

00:08:40,320 --> 00:08:48,080
is that we can data io manager

00:08:43,919 --> 00:08:52,640
calling back into the python space

00:08:48,080 --> 00:08:52,640
calling back sorry my screen just logged

00:08:53,600 --> 00:09:01,120
according to the the

00:08:57,040 --> 00:09:03,519
custom i o manager and

00:09:01,120 --> 00:09:07,279
however this method it was deprecated

00:09:03,519 --> 00:09:10,399
and not recommended to use anymore

00:09:07,279 --> 00:09:12,959
so do not follow this pattern so despite

00:09:10,399 --> 00:09:13,600
the last solution that is working it may

00:09:12,959 --> 00:09:16,080
dialogue

00:09:13,600 --> 00:09:17,839
if the application is trying to fork or

00:09:16,080 --> 00:09:21,279
using multiple event loop

00:09:17,839 --> 00:09:24,720
or using multiple threads running an rpc

00:09:21,279 --> 00:09:27,360
the root cause is because the python's

00:09:24,720 --> 00:09:27,360
three enchants

00:09:27,680 --> 00:09:31,839
so on the right side is a diagram

00:09:29,600 --> 00:09:34,080
showing how this problem occurred

00:09:31,839 --> 00:09:36,320
in straight a a python application is

00:09:34,080 --> 00:09:39,360
trying to invoke a grpc core

00:09:36,320 --> 00:09:40,480
method and the javascript acquired a

00:09:39,360 --> 00:09:43,760
certain mutex

00:09:40,480 --> 00:09:46,800
and calling back into python space

00:09:43,760 --> 00:09:50,000
however the python space required a gear

00:09:46,800 --> 00:09:53,200
the the global interpreter log and

00:09:50,000 --> 00:09:55,200
on the right side however it cannot do

00:09:53,200 --> 00:09:55,920
so because thready already acquired the

00:09:55,200 --> 00:09:59,040
deal

00:09:55,920 --> 00:10:01,600
and but but the threat be

00:09:59,040 --> 00:10:02,839
can doesn't want to yield the gear

00:10:01,600 --> 00:10:07,040
because

00:10:02,839 --> 00:10:09,600
um because it was trying to

00:10:07,040 --> 00:10:10,959
call another jfc called api which

00:10:09,600 --> 00:10:14,880
requires the mutex

00:10:10,959 --> 00:10:18,320
x again so it enters into a deadlock

00:10:14,880 --> 00:10:21,440
the solution is easy which is

00:10:18,320 --> 00:10:22,320
uh posting a polar thread in in the as a

00:10:21,440 --> 00:10:25,519
middleman which

00:10:22,320 --> 00:10:28,720
involves the japanese core api

00:10:25,519 --> 00:10:29,519
to fetch the events from the nfc core

00:10:28,720 --> 00:10:32,880
space

00:10:29,519 --> 00:10:34,240
and then sending the events to the async

00:10:32,880 --> 00:10:37,760
io event loop saying

00:10:34,240 --> 00:10:39,040
hey this is a there is a i o events for

00:10:37,760 --> 00:10:42,160
example a

00:10:39,040 --> 00:10:46,399
new message arrived or a dns

00:10:42,160 --> 00:10:50,160
result resolutions said succeed

00:10:46,399 --> 00:10:53,200
in this way we keep the python

00:10:50,160 --> 00:10:58,079
objects in the python space and

00:10:53,200 --> 00:10:58,079
there's no longer any deadlock issues

00:10:58,160 --> 00:11:01,920
however you introduce a performance

00:11:00,959 --> 00:11:04,800
regression

00:11:01,920 --> 00:11:05,279
which means uh which as many of you may

00:11:04,800 --> 00:11:08,640
know

00:11:05,279 --> 00:11:10,800
um because of the existence of geo

00:11:08,640 --> 00:11:13,120
mos threading basically means small log

00:11:10,800 --> 00:11:16,000
contention

00:11:13,120 --> 00:11:17,279
lower in lower part there is a latency

00:11:16,000 --> 00:11:21,440
distribution

00:11:17,279 --> 00:11:26,079
which means uh if the latency

00:11:21,440 --> 00:11:29,839
oh in low part is a latency distribution

00:11:26,079 --> 00:11:29,839
which means um

00:11:31,279 --> 00:11:34,959
so which shows the more the more thread

00:11:34,320 --> 00:11:38,640
you're using

00:11:34,959 --> 00:11:38,640
the higher the latency will be

00:11:39,120 --> 00:11:42,240
let's take another look at the previous

00:11:41,279 --> 00:11:45,680
solution

00:11:42,240 --> 00:11:47,680
so the polar strand was skill protected

00:11:45,680 --> 00:11:51,680
hence that you have to jump between the

00:11:47,680 --> 00:11:51,680
ac ion thread and the polar thread

00:11:51,760 --> 00:11:56,959
so how the the solution to solve this is

00:11:55,680 --> 00:11:59,760
uh straightforward

00:11:56,959 --> 00:12:01,360
so we can make the polar thread only

00:11:59,760 --> 00:12:05,519
running in the sizing

00:12:01,360 --> 00:12:08,079
the reason in sizing it doesn't require

00:12:05,519 --> 00:12:08,560
and he releases the gills so it doesn't

00:12:08,079 --> 00:12:11,680
need

00:12:08,560 --> 00:12:12,959
any access to any python object but

00:12:11,680 --> 00:12:15,600
instead

00:12:12,959 --> 00:12:16,560
when you receive a core event you are

00:12:15,600 --> 00:12:18,959
trying to send

00:12:16,560 --> 00:12:20,480
a circuit a socket right into

00:12:18,959 --> 00:12:22,639
asynchronous space

00:12:20,480 --> 00:12:23,600
and the azure io event will receive the

00:12:22,639 --> 00:12:25,760
socket right

00:12:23,600 --> 00:12:28,160
and being wake up and check the c clock

00:12:25,760 --> 00:12:32,639
fast queue to see if

00:12:28,160 --> 00:12:35,600
to to process this core event

00:12:32,639 --> 00:12:36,720
and finally as as a result we can see

00:12:35,600 --> 00:12:40,000
the benchmark

00:12:36,720 --> 00:12:40,800
or between the current api and the async

00:12:40,000 --> 00:12:44,560
api

00:12:40,800 --> 00:12:46,800
and the cpl plus uh api

00:12:44,560 --> 00:12:48,959
and the cpanel api the red the red one

00:12:46,800 --> 00:12:49,760
is the current api the blue one is the

00:12:48,959 --> 00:12:54,399
async

00:12:49,760 --> 00:12:58,800
i o api the gray one is the cpr plus api

00:12:54,399 --> 00:13:01,440
so c5 as you can see um the 18ml api

00:12:58,800 --> 00:13:04,000
reaches around 50 of the per core

00:13:01,440 --> 00:13:06,800
performance of cpr plus

00:13:04,000 --> 00:13:09,519
and is two times to 28 times better than

00:13:06,800 --> 00:13:09,519
the sync stack

00:13:11,040 --> 00:13:14,959
finally is an upstate test update about

00:13:13,680 --> 00:13:18,079
the jpc

00:13:14,959 --> 00:13:19,440
async io stack so it has it have been

00:13:18,079 --> 00:13:21,519
released

00:13:19,440 --> 00:13:22,560
it has been releasing as experimental

00:13:21,519 --> 00:13:25,440
api since

00:13:22,560 --> 00:13:27,600
version 1.25 and it passes all the

00:13:25,440 --> 00:13:29,360
internal tests which means

00:13:27,600 --> 00:13:30,959
it can communicate well with all the

00:13:29,360 --> 00:13:33,839
historical version of

00:13:30,959 --> 00:13:36,000
jrpc and other languages other jrpg

00:13:33,839 --> 00:13:39,120
languages

00:13:36,000 --> 00:13:41,279
and is now feature complete with

00:13:39,120 --> 00:13:42,560
sync stack and we are trying to

00:13:41,279 --> 00:13:45,600
integrate it with

00:13:42,560 --> 00:13:47,839
a google cloud platform clients

00:13:45,600 --> 00:13:50,240
and they are expected some of them

00:13:47,839 --> 00:13:53,279
expected to release in quarter 3

00:13:50,240 --> 00:13:56,639
of 2020 and you can find api reference

00:13:53,279 --> 00:14:01,760
on jrpgio for more information

00:13:56,639 --> 00:14:01,760

YouTube URL: https://www.youtube.com/watch?v=SDOzb1tt0jU


