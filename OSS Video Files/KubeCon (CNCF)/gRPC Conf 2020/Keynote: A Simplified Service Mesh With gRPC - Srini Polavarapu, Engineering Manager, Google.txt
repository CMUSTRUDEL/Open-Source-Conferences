Title: Keynote: A Simplified Service Mesh With gRPC - Srini Polavarapu, Engineering Manager, Google
Publication date: 2020-08-01
Playlist: gRPC Conf 2020
Description: 
	Keynote: A Simplified Service Mesh With gRPC - Srini Polavarapu, Engineering Manager, Google 

gRPC is a popular choice for building microservices based applications but it does not have any native service mesh features such as service discovery, load balancing, application security and observability. A service mesh typically provides such features via sidecar proxies deployed alongside the microservices. In this session, we will introduce the new exciting development of service mesh features in gRPC and show how to bring the benefits of a service mesh to your gRPC applications without the complexity of managing sidecar proxies. We will also talk about the architecture and show how you can go from a proxy based service mesh to a proxy-less service mesh with gRPC.
Captions: 
	00:00:00,080 --> 00:00:03,280
hello and thank you for joining this

00:00:01,920 --> 00:00:06,240
presentation

00:00:03,280 --> 00:00:08,160
my name is srini polovarpu i am an

00:00:06,240 --> 00:00:11,040
engineering manager at google

00:00:08,160 --> 00:00:12,960
and i work on grpc and service mesh

00:00:11,040 --> 00:00:14,799
technologies

00:00:12,960 --> 00:00:16,400
so today i will be presenting the

00:00:14,799 --> 00:00:19,279
exciting new development

00:00:16,400 --> 00:00:21,199
happening in grpc in the area of service

00:00:19,279 --> 00:00:24,640
mesh technologies

00:00:21,199 --> 00:00:24,640
so let's dive right into it

00:00:25,359 --> 00:00:31,760
so i'll start by saying that grp season

00:00:28,720 --> 00:00:34,719
is an awesome rpc framework it is

00:00:31,760 --> 00:00:36,640
a high performance open source framework

00:00:34,719 --> 00:00:40,719
with a ton of features

00:00:36,640 --> 00:00:43,360
and it has very high industry adoption

00:00:40,719 --> 00:00:44,960
it has a sophisticated networking stack

00:00:43,360 --> 00:00:46,640
that enables

00:00:44,960 --> 00:00:48,320
connection management bi-directional

00:00:46,640 --> 00:00:51,520
streaming flow control

00:00:48,320 --> 00:00:52,879
and many more features it is available

00:00:51,520 --> 00:00:56,640
in many languages

00:00:52,879 --> 00:01:00,000
and works great with protocol buffers

00:00:56,640 --> 00:01:01,359
so all this makes grpc an awesome

00:01:00,000 --> 00:01:05,120
framework for building

00:01:01,359 --> 00:01:05,120
microservices based applications

00:01:07,280 --> 00:01:11,119
now i will also say that service meshes

00:01:09,920 --> 00:01:13,760
are cool

00:01:11,119 --> 00:01:15,759
as you know microservices is the name of

00:01:13,760 --> 00:01:17,360
the game when you want to build

00:01:15,759 --> 00:01:18,799
scalable and highly available

00:01:17,360 --> 00:01:22,080
applications

00:01:18,799 --> 00:01:23,360
so it is also known as service oriented

00:01:22,080 --> 00:01:26,000
architecture

00:01:23,360 --> 00:01:26,640
where a large application is divided

00:01:26,000 --> 00:01:28,799
into

00:01:26,640 --> 00:01:30,640
many small services that talk to each

00:01:28,799 --> 00:01:34,000
other over the network

00:01:30,640 --> 00:01:36,240
so this provides great agility in

00:01:34,000 --> 00:01:38,640
developing and deploying parts of the

00:01:36,240 --> 00:01:41,119
application independently

00:01:38,640 --> 00:01:42,799
so but it comes with its own challenges

00:01:41,119 --> 00:01:46,000
and complexities like

00:01:42,799 --> 00:01:47,600
how to discover services um how requests

00:01:46,000 --> 00:01:50,880
are load balanced to

00:01:47,600 --> 00:01:51,600
many replicas of a service or how to

00:01:50,880 --> 00:01:55,600
secure

00:01:51,600 --> 00:01:56,079
the mesh or get insights into any issues

00:01:55,600 --> 00:01:59,119
or

00:01:56,079 --> 00:01:59,119
bottlenecks in the mesh

00:01:59,680 --> 00:02:02,880
so these are common problems um on which

00:02:02,640 --> 00:02:04,399
an

00:02:02,880 --> 00:02:06,960
application developer should not be

00:02:04,399 --> 00:02:10,000
wasting their time and that's where a

00:02:06,960 --> 00:02:10,800
service mesh is very useful to solve

00:02:10,000 --> 00:02:14,000
these issues

00:02:10,800 --> 00:02:16,560
seamlessly so as an example um

00:02:14,000 --> 00:02:17,440
check out istio which is an open source

00:02:16,560 --> 00:02:20,400
service mesh

00:02:17,440 --> 00:02:23,840
solution and that is gaining a lot of

00:02:20,400 --> 00:02:23,840
popularity these days

00:02:24,640 --> 00:02:30,640
um okay but where does grpc stand

00:02:28,560 --> 00:02:32,160
with respect to service mesh

00:02:30,640 --> 00:02:35,680
technologies

00:02:32,160 --> 00:02:36,959
so out of the box grpc comes with a dns

00:02:35,680 --> 00:02:40,080
resolver

00:02:36,959 --> 00:02:42,080
and a very simple pick first or round

00:02:40,080 --> 00:02:45,599
robin load balancing

00:02:42,080 --> 00:02:47,200
so to do anything fancier um you have to

00:02:45,599 --> 00:02:48,959
write your own resolver and load

00:02:47,200 --> 00:02:52,000
balancing plugins

00:02:48,959 --> 00:02:53,200
right the other option is to implement a

00:02:52,000 --> 00:02:56,720
controller

00:02:53,200 --> 00:02:59,440
that talks grpc lb protocol

00:02:56,720 --> 00:03:00,560
and we also call it leukocyte load

00:02:59,440 --> 00:03:02,959
balancer

00:03:00,560 --> 00:03:04,239
and all the smartness is built into this

00:03:02,959 --> 00:03:07,920
controller

00:03:04,239 --> 00:03:11,280
which provides a flat list of end points

00:03:07,920 --> 00:03:13,599
for a given service to a grpc client

00:03:11,280 --> 00:03:14,959
which then does simple round robin load

00:03:13,599 --> 00:03:17,840
balancing

00:03:14,959 --> 00:03:20,000
right but unfortunately there is no open

00:03:17,840 --> 00:03:22,000
source implementation available

00:03:20,000 --> 00:03:23,440
so you will have to build your own such

00:03:22,000 --> 00:03:26,480
controller

00:03:23,440 --> 00:03:28,640
so also this protocol is very basic

00:03:26,480 --> 00:03:29,840
and i would say it won't get you

00:03:28,640 --> 00:03:33,040
anywhere close to

00:03:29,840 --> 00:03:36,400
what a service mesh provides

00:03:33,040 --> 00:03:39,120
so instead of evolving this protocol

00:03:36,400 --> 00:03:42,000
it made sense to integrate with existing

00:03:39,120 --> 00:03:42,000
mesh technologies

00:03:43,599 --> 00:03:50,879
so before i talk about that

00:03:47,760 --> 00:03:51,840
let's see how a grpc application is

00:03:50,879 --> 00:03:55,840
deployed in a

00:03:51,840 --> 00:03:56,879
service mesh today in a commonly used

00:03:55,840 --> 00:04:00,000
model

00:03:56,879 --> 00:04:02,640
and like any other application the grpc

00:04:00,000 --> 00:04:04,159
application is deployed along with a

00:04:02,640 --> 00:04:07,200
sidecar proxy

00:04:04,159 --> 00:04:08,560
that intercepts the request the sidecar

00:04:07,200 --> 00:04:10,319
proxy

00:04:08,560 --> 00:04:11,840
gets service mesh policies from the

00:04:10,319 --> 00:04:13,360
control plane

00:04:11,840 --> 00:04:15,519
and apply them on the intercepted

00:04:13,360 --> 00:04:17,680
requests so there is one

00:04:15,519 --> 00:04:18,880
sidecar proxy on each end of the

00:04:17,680 --> 00:04:21,120
connection

00:04:18,880 --> 00:04:22,880
providing client-side and server-side

00:04:21,120 --> 00:04:26,080
mesh functionality

00:04:22,880 --> 00:04:29,199
right so like any other application

00:04:26,080 --> 00:04:31,680
a grpc app client would do a

00:04:29,199 --> 00:04:32,960
dns lookup of the service it is trying

00:04:31,680 --> 00:04:35,120
to connect to

00:04:32,960 --> 00:04:37,520
and open one connection to the virtual

00:04:35,120 --> 00:04:40,160
ip of the service

00:04:37,520 --> 00:04:42,479
so as you can see much of the

00:04:40,160 --> 00:04:45,360
sophistication in the grpg stack

00:04:42,479 --> 00:04:48,400
is not being used the sidecar proxy

00:04:45,360 --> 00:04:48,400
takes care of everything

00:04:48,720 --> 00:04:56,080
okay um with service mesh integration

00:04:52,560 --> 00:04:57,600
um a grpc application would then

00:04:56,080 --> 00:04:59,280
directly talk to the service mesh

00:04:57,600 --> 00:05:02,639
control plane

00:04:59,280 --> 00:05:06,479
now this looks similar to the look aside

00:05:02,639 --> 00:05:09,919
grpc lb solution that i showed earlier

00:05:06,479 --> 00:05:11,919
but here grpc would be talking a much

00:05:09,919 --> 00:05:14,880
broader set of

00:05:11,919 --> 00:05:16,720
data plane apis that enable supporting a

00:05:14,880 --> 00:05:19,280
large number of features

00:05:16,720 --> 00:05:20,240
available in a service mesh right so the

00:05:19,280 --> 00:05:22,720
key thing to

00:05:20,240 --> 00:05:24,000
note here is that we won't need a

00:05:22,720 --> 00:05:27,039
sidecar proxy in the

00:05:24,000 --> 00:05:29,440
middle so as you can see in the previous

00:05:27,039 --> 00:05:31,360
um diagram there is a proxy on both

00:05:29,440 --> 00:05:33,840
sides of the connection

00:05:31,360 --> 00:05:35,280
and here we don't have it and that's why

00:05:33,840 --> 00:05:38,400
we are calling it proxy less

00:05:35,280 --> 00:05:41,680
grpc service mesh

00:05:38,400 --> 00:05:45,039
so now the question is

00:05:41,680 --> 00:05:47,280
which service mesh to use um so

00:05:45,039 --> 00:05:49,440
it is not about picking a particular

00:05:47,280 --> 00:05:52,960
service mesh solution

00:05:49,440 --> 00:05:54,960
the lessons we learned from clb protocol

00:05:52,960 --> 00:05:57,520
showed us that picking the right data

00:05:54,960 --> 00:06:01,520
plane api is the most important thing

00:05:57,520 --> 00:06:04,160
so data plane apis are used to exchange

00:06:01,520 --> 00:06:04,639
configuration data between the control

00:06:04,160 --> 00:06:06,560
plane

00:06:04,639 --> 00:06:08,639
and the data plane components like the

00:06:06,560 --> 00:06:11,360
proxies

00:06:08,639 --> 00:06:14,240
in contrast control plane apis are used

00:06:11,360 --> 00:06:16,880
to program the control plane itself

00:06:14,240 --> 00:06:18,880
with information like what are the

00:06:16,880 --> 00:06:21,199
services in the mesh

00:06:18,880 --> 00:06:23,280
what are the routing policies and what

00:06:21,199 --> 00:06:24,639
is the current state of the mesh

00:06:23,280 --> 00:06:26,639
like which services are up which

00:06:24,639 --> 00:06:30,319
services are down and so on

00:06:26,639 --> 00:06:33,840
right so if we pick a widely used

00:06:30,319 --> 00:06:36,000
data plane api then there will be

00:06:33,840 --> 00:06:38,479
good choice of control planes to use

00:06:36,000 --> 00:06:42,560
with your grpc applications

00:06:38,479 --> 00:06:45,199
so obviously the api has to be open

00:06:42,560 --> 00:06:47,120
and needs to have a strong community

00:06:45,199 --> 00:06:50,080
support

00:06:47,120 --> 00:06:51,360
also it should fit the current grpc

00:06:50,080 --> 00:06:54,000
architecture

00:06:51,360 --> 00:06:54,639
and align with our vision of no vendor

00:06:54,000 --> 00:06:57,759
lock-in

00:06:54,639 --> 00:06:57,759
right so

00:06:57,840 --> 00:07:04,160
um xts apis fit the bill perfectly and

00:07:02,080 --> 00:07:06,880
these apis were developed for

00:07:04,160 --> 00:07:08,160
onboard proxy which is a very popular

00:07:06,880 --> 00:07:11,360
proxy used in

00:07:08,160 --> 00:07:14,479
many service mesh solutions like istio

00:07:11,360 --> 00:07:15,280
and because of that xts apis have become

00:07:14,479 --> 00:07:19,039
a

00:07:15,280 --> 00:07:21,759
defect to standard for data plane apis

00:07:19,039 --> 00:07:23,840
and there is also an effort going on to

00:07:21,759 --> 00:07:26,960
evolve these apis

00:07:23,840 --> 00:07:31,840
into universal data plane apis for

00:07:26,960 --> 00:07:31,840
layer 4 to 7 data plane configuration

00:07:34,960 --> 00:07:42,000
so i'll give you a quick overview of

00:07:38,319 --> 00:07:43,919
what xts apis are so

00:07:42,000 --> 00:07:46,000
these are a collection of apis to

00:07:43,919 --> 00:07:49,520
discover various resources

00:07:46,000 --> 00:07:50,479
in a service mesh hence the name xds

00:07:49,520 --> 00:07:53,199
where

00:07:50,479 --> 00:07:53,759
x stands for the type of resource like a

00:07:53,199 --> 00:07:57,280
listener

00:07:53,759 --> 00:07:59,680
a route a cluster or an endpoint

00:07:57,280 --> 00:08:01,520
and and there is also a lot more

00:07:59,680 --> 00:08:03,680
text-based apis than

00:08:01,520 --> 00:08:06,000
what i'm showing in this diagram but

00:08:03,680 --> 00:08:08,560
i'll keep it simple here

00:08:06,000 --> 00:08:10,000
so you have a virtual service in a

00:08:08,560 --> 00:08:13,039
service mesh

00:08:10,000 --> 00:08:14,080
which is assigned a virtual ip and and

00:08:13,039 --> 00:08:16,560
ports

00:08:14,080 --> 00:08:18,479
on which the service listens and that is

00:08:16,560 --> 00:08:21,039
called a whip

00:08:18,479 --> 00:08:22,800
and when an application wants to send

00:08:21,039 --> 00:08:25,440
request to a service

00:08:22,800 --> 00:08:27,120
it does dns lookup and sends the request

00:08:25,440 --> 00:08:29,680
to the virtual ip

00:08:27,120 --> 00:08:33,039
which is the web here and this request

00:08:29,680 --> 00:08:35,919
is then intercepted by the sidecar proxy

00:08:33,039 --> 00:08:36,800
um so now the proxy needs to know uh

00:08:35,919 --> 00:08:39,599
about such

00:08:36,800 --> 00:08:40,560
virtual ips in the service mesh so it

00:08:39,599 --> 00:08:42,959
can intercept such

00:08:40,560 --> 00:08:43,680
traffic and apply the policies of those

00:08:42,959 --> 00:08:46,240
services

00:08:43,680 --> 00:08:48,800
correctly right um so that's where the

00:08:46,240 --> 00:08:50,640
listener discovery request is used

00:08:48,800 --> 00:08:52,480
to get the information about the virtual

00:08:50,640 --> 00:08:55,200
ips

00:08:52,480 --> 00:08:56,959
so now after intercepting the request

00:08:55,200 --> 00:08:59,760
the proxy needs to know

00:08:56,959 --> 00:09:01,279
how to route the traffic and what

00:08:59,760 --> 00:09:04,080
policies to apply

00:09:01,279 --> 00:09:04,560
so this information is obtained using

00:09:04,080 --> 00:09:08,000
the

00:09:04,560 --> 00:09:11,120
route discovery service the country

00:09:08,000 --> 00:09:14,959
may include various rules like

00:09:11,120 --> 00:09:17,680
match on header values um or a path

00:09:14,959 --> 00:09:19,600
and then route to a particular cluster

00:09:17,680 --> 00:09:23,279
and apply a particular timeout

00:09:19,600 --> 00:09:25,839
or maybe insert a header right

00:09:23,279 --> 00:09:26,399
so once the proxy knows which cluster

00:09:25,839 --> 00:09:29,600
the

00:09:26,399 --> 00:09:31,440
request needs to be sent to it then uses

00:09:29,600 --> 00:09:34,720
cluster discovery service

00:09:31,440 --> 00:09:36,720
to get cluster information like what is

00:09:34,720 --> 00:09:37,519
the load balancing policies and what are

00:09:36,720 --> 00:09:40,800
the

00:09:37,519 --> 00:09:44,160
security policies if any right so

00:09:40,800 --> 00:09:46,399
a cluster um is basically

00:09:44,160 --> 00:09:48,160
the implementation of your service in

00:09:46,399 --> 00:09:50,480
the form of end points

00:09:48,160 --> 00:09:51,519
right so so the cluster is made up of

00:09:50,480 --> 00:09:54,560
many endpoints

00:09:51,519 --> 00:09:57,440
um that are the actual service instances

00:09:54,560 --> 00:09:59,040
and these endpoints could be spread

00:09:57,440 --> 00:10:02,000
across different zones on

00:09:59,040 --> 00:10:02,800
and regions which are known as

00:10:02,000 --> 00:10:06,560
localities

00:10:02,800 --> 00:10:07,360
in in xds terms and that information is

00:10:06,560 --> 00:10:10,399
obtained via

00:10:07,360 --> 00:10:14,079
the endpoint discovery service so

00:10:10,399 --> 00:10:14,800
as you can see um xts apis map very well

00:10:14,079 --> 00:10:16,720
to

00:10:14,800 --> 00:10:19,839
service mesh concepts that we are

00:10:16,720 --> 00:10:19,839
familiar with right

00:10:20,720 --> 00:10:28,000
so this is how the

00:10:24,079 --> 00:10:31,360
implementation of xts in grpc looks like

00:10:28,000 --> 00:10:34,880
it is not as scary as it looks

00:10:31,360 --> 00:10:38,000
it is a very modular design that maps

00:10:34,880 --> 00:10:41,200
the concepts of xts apis and integrates

00:10:38,000 --> 00:10:43,760
well with the grpc client channel design

00:10:41,200 --> 00:10:44,959
so there's a common xts client

00:10:43,760 --> 00:10:48,480
implementation

00:10:44,959 --> 00:10:50,320
on the left that talks to the xts server

00:10:48,480 --> 00:10:52,880
so when you create a client channel in

00:10:50,320 --> 00:10:55,600
grpc to connect to a service

00:10:52,880 --> 00:10:56,800
you need to use the xds scheme instead

00:10:55,600 --> 00:11:00,240
of the default

00:10:56,800 --> 00:11:01,200
dns scheme and this scheme tells the

00:11:00,240 --> 00:11:04,240
client channel

00:11:01,200 --> 00:11:07,360
that an xts resolver needs to be created

00:11:04,240 --> 00:11:09,360
in order to resolve the service name and

00:11:07,360 --> 00:11:10,800
and then it sends an lds request to

00:11:09,360 --> 00:11:13,200
resolve the name

00:11:10,800 --> 00:11:14,560
so if the service exists then it uses

00:11:13,200 --> 00:11:17,120
rds

00:11:14,560 --> 00:11:18,880
to get the routing rules associated with

00:11:17,120 --> 00:11:22,160
that service

00:11:18,880 --> 00:11:24,720
so a routing lb policy is then created

00:11:22,160 --> 00:11:25,600
to match the rpcs against the match

00:11:24,720 --> 00:11:27,680
rules

00:11:25,600 --> 00:11:29,040
right so so based on the route action of

00:11:27,680 --> 00:11:32,000
the match rule

00:11:29,040 --> 00:11:34,800
a traffic splitting policy or a a single

00:11:32,000 --> 00:11:37,760
cluster load balancing policy is created

00:11:34,800 --> 00:11:40,560
and as i said earlier a cluster is made

00:11:37,760 --> 00:11:43,120
of end points in different localities

00:11:40,560 --> 00:11:44,240
so the rest of the policies in the chain

00:11:43,120 --> 00:11:46,320
map to picking

00:11:44,240 --> 00:11:47,519
localities and then end points within

00:11:46,320 --> 00:11:51,040
those localities

00:11:47,519 --> 00:11:53,600
right so the common xts client instance

00:11:51,040 --> 00:11:54,480
is responsible for talking to the xts

00:11:53,600 --> 00:11:57,839
server

00:11:54,480 --> 00:11:58,800
um and and then give the discovery

00:11:57,839 --> 00:12:01,040
service data

00:11:58,800 --> 00:12:03,920
to the resolver and the load balancing

00:12:01,040 --> 00:12:06,800
plugins as needed

00:12:03,920 --> 00:12:07,920
so the xts client keeps a bidirectional

00:12:06,800 --> 00:12:10,240
grpc stream

00:12:07,920 --> 00:12:11,920
open with the xds server so that the

00:12:10,240 --> 00:12:14,399
server can push

00:12:11,920 --> 00:12:15,920
configuration and state changes in real

00:12:14,399 --> 00:12:18,959
time

00:12:15,920 --> 00:12:21,760
so also the grpc client reports

00:12:18,959 --> 00:12:23,519
some metrics to the xds server via the

00:12:21,760 --> 00:12:27,120
load reporting service

00:12:23,519 --> 00:12:29,680
which is also part of the xds apis so

00:12:27,120 --> 00:12:30,720
i will not go into more details here but

00:12:29,680 --> 00:12:32,639
feel free to

00:12:30,720 --> 00:12:33,760
look up the design docs if you are

00:12:32,639 --> 00:12:39,040
interested

00:12:33,760 --> 00:12:43,040
the links are provided in the last slide

00:12:39,040 --> 00:12:45,680
okay so how do you enable xts

00:12:43,040 --> 00:12:46,880
in your grpc applications and go proxy

00:12:45,680 --> 00:12:50,079
less

00:12:46,880 --> 00:12:53,200
it is quite easy instead of using the

00:12:50,079 --> 00:12:56,079
default scheme when you create a channel

00:12:53,200 --> 00:12:57,760
um just use the new xds scheme that

00:12:56,079 --> 00:13:00,160
tells the client channel

00:12:57,760 --> 00:13:01,519
to start an xts resolver instead of a

00:13:00,160 --> 00:13:04,399
dns resolver

00:13:01,519 --> 00:13:05,760
right so in this example you can see how

00:13:04,399 --> 00:13:08,800
xds scheme is used

00:13:05,760 --> 00:13:10,160
to connect to a service called foo dot

00:13:08,800 --> 00:13:12,959
my service

00:13:10,160 --> 00:13:15,600
right but then how does a grpc

00:13:12,959 --> 00:13:18,880
application know

00:13:15,600 --> 00:13:20,880
which xts server to talk to so the

00:13:18,880 --> 00:13:21,600
address and the credentials of the xts

00:13:20,880 --> 00:13:23,760
server

00:13:21,600 --> 00:13:24,639
needs to be provided via a bootstrap

00:13:23,760 --> 00:13:26,959
file in the

00:13:24,639 --> 00:13:28,800
local file system and the location of

00:13:26,959 --> 00:13:32,079
the file is provided via

00:13:28,800 --> 00:13:35,680
an environment variable called grpc

00:13:32,079 --> 00:13:36,880
xts bootstrap so the bootstrap can

00:13:35,680 --> 00:13:39,680
contain

00:13:36,880 --> 00:13:41,519
some other information such as a unique

00:13:39,680 --> 00:13:45,120
client id

00:13:41,519 --> 00:13:47,600
its locality and any other data that is

00:13:45,120 --> 00:13:49,120
relevant to a particular xds server

00:13:47,600 --> 00:13:52,000
implementation

00:13:49,120 --> 00:13:54,000
such as a project number or a or a

00:13:52,000 --> 00:13:56,240
network number

00:13:54,000 --> 00:13:56,240
so

00:13:57,040 --> 00:14:03,680
and that's it and it is that simple

00:14:00,480 --> 00:14:06,720
you can pick xts or dns resolver scheme

00:14:03,680 --> 00:14:09,040
on a per channel basis so this means

00:14:06,720 --> 00:14:10,480
you can connect to some services via a

00:14:09,040 --> 00:14:12,880
proxy

00:14:10,480 --> 00:14:15,040
while connecting to some other services

00:14:12,880 --> 00:14:18,079
directly without a proxy

00:14:15,040 --> 00:14:18,720
so this makes it easy to slowly migrate

00:14:18,079 --> 00:14:23,040
to

00:14:18,720 --> 00:14:23,040
proxy-less grpc right

00:14:23,680 --> 00:14:27,760
so so what are the advantages of going

00:14:25,920 --> 00:14:31,360
proxy-less

00:14:27,760 --> 00:14:33,920
so well you get a better qps and

00:14:31,360 --> 00:14:35,920
lower latency because you have two less

00:14:33,920 --> 00:14:37,600
proxies in the path

00:14:35,920 --> 00:14:39,120
and one on the client side and one on

00:14:37,600 --> 00:14:42,079
the server side right

00:14:39,120 --> 00:14:43,279
and as the size of your service mesh

00:14:42,079 --> 00:14:45,519
grows

00:14:43,279 --> 00:14:47,120
it becomes less efficient and scalable

00:14:45,519 --> 00:14:51,120
to have a proxy

00:14:47,120 --> 00:14:53,440
with each service and proxies require

00:14:51,120 --> 00:14:56,480
some performance tuning to avoid

00:14:53,440 --> 00:14:59,120
impedance mismatch with the applications

00:14:56,480 --> 00:15:01,199
so you could have a fast application

00:14:59,120 --> 00:15:03,680
bottlenecked by a slow proxy

00:15:01,199 --> 00:15:04,959
right so and also a sidecar proxy

00:15:03,680 --> 00:15:08,000
requires a

00:15:04,959 --> 00:15:10,160
complex set of ip filter rules

00:15:08,000 --> 00:15:11,120
for intercepting traffic that are not

00:15:10,160 --> 00:15:14,399
needed in a

00:15:11,120 --> 00:15:18,639
proxy list deployment and because the

00:15:14,399 --> 00:15:20,079
grpc uses xts to resolve a service name

00:15:18,639 --> 00:15:24,320
you can even get away without

00:15:20,079 --> 00:15:26,800
configuring dns entries for the services

00:15:24,320 --> 00:15:28,880
also proxies are additional binaries

00:15:26,800 --> 00:15:31,600
running in your service mesh

00:15:28,880 --> 00:15:32,720
so there is an overhead of life cycle

00:15:31,600 --> 00:15:36,160
management

00:15:32,720 --> 00:15:39,839
like deploying upgrading health checking

00:15:36,160 --> 00:15:41,839
and dealing with security issues

00:15:39,839 --> 00:15:43,199
and that also makes it harder to deploy

00:15:41,839 --> 00:15:46,560
in vms

00:15:43,199 --> 00:15:48,959
compared to parts so

00:15:46,560 --> 00:15:49,759
with proxies grpc you don't have these

00:15:48,959 --> 00:15:52,240
issues

00:15:49,759 --> 00:15:54,000
which makes it easier to migrate your

00:15:52,240 --> 00:15:57,360
grpc applications to a service

00:15:54,000 --> 00:16:00,720
mesh and many service mesh solutions

00:15:57,360 --> 00:16:01,360
use xts apis so there is a great value

00:16:00,720 --> 00:16:05,199
add

00:16:01,360 --> 00:16:08,000
if you are already invested in grpc

00:16:05,199 --> 00:16:09,680
or if you are thinking of adopting it so

00:16:08,000 --> 00:16:12,320
this kind of proxy less model

00:16:09,680 --> 00:16:15,519
is used at many companies to build

00:16:12,320 --> 00:16:19,279
global scale services

00:16:15,519 --> 00:16:22,320
so so what are the downsides um

00:16:19,279 --> 00:16:23,360
so mature proxies like envoy are very

00:16:22,320 --> 00:16:25,600
feature rich

00:16:23,360 --> 00:16:27,519
and there is a large ecosystem of

00:16:25,600 --> 00:16:29,680
third-party filters

00:16:27,519 --> 00:16:31,600
and tools especially in the area of

00:16:29,680 --> 00:16:33,680
observability

00:16:31,600 --> 00:16:35,519
but we are actively working on adding

00:16:33,680 --> 00:16:39,040
more features in grpc

00:16:35,519 --> 00:16:42,880
to close the feature gap

00:16:39,040 --> 00:16:46,320
so note that grpc provides extensibility

00:16:42,880 --> 00:16:48,160
with interceptors and there is a good

00:16:46,320 --> 00:16:51,519
open sensors and open tracing

00:16:48,160 --> 00:16:53,920
integration for observability

00:16:51,519 --> 00:16:54,800
the other downside is to get xts

00:16:53,920 --> 00:16:58,320
functionality

00:16:54,800 --> 00:16:58,959
you will need to be able to upgrade grpc

00:16:58,320 --> 00:17:01,839
version

00:16:58,959 --> 00:17:03,759
and rebuild your applications so this is

00:17:01,839 --> 00:17:05,679
usually not an issue

00:17:03,759 --> 00:17:08,319
because developers building

00:17:05,679 --> 00:17:10,799
microservices based applications

00:17:08,319 --> 00:17:12,559
typically have a good ci cd

00:17:10,799 --> 00:17:15,600
infrastructure

00:17:12,559 --> 00:17:16,720
also you can continue to use proxies

00:17:15,600 --> 00:17:19,439
with applications

00:17:16,720 --> 00:17:20,959
that cannot be upgraded because as i

00:17:19,439 --> 00:17:24,000
talked earlier

00:17:20,959 --> 00:17:26,640
it is easy to mix and match proxy and

00:17:24,000 --> 00:17:29,200
proxies deployments

00:17:26,640 --> 00:17:30,960
so the language support may be an issue

00:17:29,200 --> 00:17:34,240
for you

00:17:30,960 --> 00:17:38,240
we already support c plus plus java

00:17:34,240 --> 00:17:41,520
go python php uh ruby and c-sharp

00:17:38,240 --> 00:17:44,720
um any grpc implementation that wraps

00:17:41,520 --> 00:17:47,679
c core uh for example python

00:17:44,720 --> 00:17:48,559
gets xtr support for free so such an

00:17:47,679 --> 00:17:53,039
implementation

00:17:48,559 --> 00:17:56,160
just needs to upgrade its core version

00:17:53,039 --> 00:17:59,600
so so where are we now

00:17:56,160 --> 00:18:02,880
um so in the past year

00:17:59,600 --> 00:18:04,320
the grpc team at google has been working

00:18:02,880 --> 00:18:06,880
hard to make

00:18:04,320 --> 00:18:08,559
proxy less service measure reality right

00:18:06,880 --> 00:18:11,760
and i'm happy to say that

00:18:08,559 --> 00:18:14,720
we recently released a 1.30 version

00:18:11,760 --> 00:18:15,600
that supports xts protocol for service

00:18:14,720 --> 00:18:18,559
discovery and

00:18:15,600 --> 00:18:20,880
endpoint load balancing we are also

00:18:18,559 --> 00:18:22,880
making good progress on adding

00:18:20,880 --> 00:18:24,000
route matching based on path and header

00:18:22,880 --> 00:18:26,840
values

00:18:24,000 --> 00:18:28,080
and splitting traffic to multiple

00:18:26,840 --> 00:18:30,320
clusters

00:18:28,080 --> 00:18:31,360
there are several other features in the

00:18:30,320 --> 00:18:34,160
design phase

00:18:31,360 --> 00:18:35,200
and we are migrating from xts version 2

00:18:34,160 --> 00:18:37,919
to version 3

00:18:35,200 --> 00:18:39,600
sometime this year so you can watch the

00:18:37,919 --> 00:18:43,840
progress on github

00:18:39,600 --> 00:18:43,840
and you are very welcome to contribute

00:18:44,080 --> 00:18:50,320
so with this i will

00:18:47,200 --> 00:18:51,200
switch to a demo of xts functionality

00:18:50,320 --> 00:18:54,400
using

00:18:51,200 --> 00:18:55,280
grpc hello world application i'll be

00:18:54,400 --> 00:18:57,360
using

00:18:55,280 --> 00:18:58,640
google cloud managed service mesh

00:18:57,360 --> 00:19:01,039
controller called

00:18:58,640 --> 00:19:03,600
traffic director that speaks xds

00:19:01,039 --> 00:19:03,600
protocol

00:19:04,559 --> 00:19:09,679
i will now do a quick demo using traffic

00:19:06,720 --> 00:19:12,320
director which is a google cloud managed

00:19:09,679 --> 00:19:14,000
service mesh control plane i have

00:19:12,320 --> 00:19:16,640
pre-configured a service called hello

00:19:14,000 --> 00:19:18,480
world gce in traffic director

00:19:16,640 --> 00:19:19,679
and i have also configured a cluster for

00:19:18,480 --> 00:19:21,360
this service

00:19:19,679 --> 00:19:22,960
whose backend instances are in two

00:19:21,360 --> 00:19:25,039
regions

00:19:22,960 --> 00:19:26,160
the top left two windows are instances

00:19:25,039 --> 00:19:28,320
in the central region

00:19:26,160 --> 00:19:30,080
and the bottom two are instances in the

00:19:28,320 --> 00:19:32,880
west region

00:19:30,080 --> 00:19:34,880
the client is in the central region and

00:19:32,880 --> 00:19:36,840
it is using the xds scheme

00:19:34,880 --> 00:19:38,480
to connect to the hello world gce

00:19:36,840 --> 00:19:41,520
service

00:19:38,480 --> 00:19:44,000
since traffic director also speaks xds

00:19:41,520 --> 00:19:45,760
the grpc client is able to get the

00:19:44,000 --> 00:19:47,760
cluster and backend information from

00:19:45,760 --> 00:19:49,760
traffic director

00:19:47,760 --> 00:19:51,440
and traffic director also provides

00:19:49,760 --> 00:19:53,280
global load balancing

00:19:51,440 --> 00:19:56,000
such that the requests are sent to the

00:19:53,280 --> 00:19:58,799
closest zone

00:19:56,000 --> 00:20:00,559
you can see that the client is now

00:19:58,799 --> 00:20:02,799
sending requests to back-ends

00:20:00,559 --> 00:20:07,840
in the central zone as that is the

00:20:02,799 --> 00:20:07,840
closest zone

00:20:08,480 --> 00:20:12,960
now i will stop the back ends in the

00:20:11,120 --> 00:20:16,159
central zone

00:20:12,960 --> 00:20:18,880
and you will see that the requests have

00:20:16,159 --> 00:20:22,480
immediately failed over to

00:20:18,880 --> 00:20:22,480
the back ends in the west zone

00:20:22,640 --> 00:20:25,919
the failover to another region is quite

00:20:24,559 --> 00:20:27,840
fast

00:20:25,919 --> 00:20:30,400
this is because the client received

00:20:27,840 --> 00:20:32,799
assignments for all the zones

00:20:30,400 --> 00:20:36,559
but with different priorities the

00:20:32,799 --> 00:20:39,039
central zone was in the highest priority

00:20:36,559 --> 00:20:40,080
because it was closest to the client and

00:20:39,039 --> 00:20:41,760
once it failed

00:20:40,080 --> 00:20:44,880
the client immediately switched to the

00:20:41,760 --> 00:20:47,760
next priority zone

00:20:44,880 --> 00:20:49,280
now i will restart the service in the

00:20:47,760 --> 00:20:53,120
central zone

00:20:49,280 --> 00:20:56,640
and we will see that in a few seconds

00:20:53,120 --> 00:20:59,360
the request will fail back to the

00:20:56,640 --> 00:20:59,360
central zone

00:21:00,480 --> 00:21:05,039
i have configured grpc health health

00:21:03,520 --> 00:21:07,520
check services here

00:21:05,039 --> 00:21:09,120
to check the health of the backends the

00:21:07,520 --> 00:21:09,919
traffic director looks at the health of

00:21:09,120 --> 00:21:12,480
the backends

00:21:09,919 --> 00:21:14,799
before including them in the eds

00:21:12,480 --> 00:21:17,120
response

00:21:14,799 --> 00:21:19,120
so here now you can see that the

00:21:17,120 --> 00:21:22,400
requests have started

00:21:19,120 --> 00:21:25,760
coming back to the central zone

00:21:22,400 --> 00:21:31,840
and now the client is

00:21:25,760 --> 00:21:31,840
not sending any request to the west zone

00:21:32,960 --> 00:21:36,159
okay i would also like to quickly show

00:21:35,360 --> 00:21:40,559
the

00:21:36,159 --> 00:21:43,679
bootstrap file that this client is using

00:21:40,559 --> 00:21:44,880
this is a sample bootstrap file you can

00:21:43,679 --> 00:21:47,919
see that

00:21:44,880 --> 00:21:50,880
there is an address to the xds server

00:21:47,919 --> 00:21:52,880
and there is some node id which is

00:21:50,880 --> 00:21:54,559
basically a unique string that you can

00:21:52,880 --> 00:21:58,240
set to any value

00:21:54,559 --> 00:22:00,080
and there is also a metadata which the

00:21:58,240 --> 00:22:02,960
xts server uses to

00:22:00,080 --> 00:22:04,080
identify the client for example the

00:22:02,960 --> 00:22:07,200
project number

00:22:04,080 --> 00:22:10,559
and the network name

00:22:07,200 --> 00:22:13,280
and that concludes this demo

00:22:10,559 --> 00:22:14,080
so uh that brings us to the end of this

00:22:13,280 --> 00:22:17,520
presentation

00:22:14,080 --> 00:22:19,919
um these are some useful links

00:22:17,520 --> 00:22:22,000
uh if you have any questions or feedback

00:22:19,919 --> 00:22:23,039
uh please use the link provided here to

00:22:22,000 --> 00:22:25,280
reach me

00:22:23,039 --> 00:22:31,760
um thank you for listening and we now

00:22:25,280 --> 00:22:31,760

YouTube URL: https://www.youtube.com/watch?v=9alMEeTxsMA


