Title: Live Panel: OpenTelemetry Metrics Deep Dive
Publication date: 2020-11-24
Playlist: OpenTelemetry Community Day North America 2020
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon Europe 2021 Virtual from May 4–7, 2021. Learn more at https://kubecon.io. The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

Live Panel: OpenTelemetry Metrics Deep Dive 

This panel will be a deep dive into the OpenTelemetry Metrics specification, a discussion on why it differs from past metrics APIs, and where we expect it to go in the future - bring your questions for Q&A!"
Captions: 
	00:00:00,320 --> 00:00:03,280
all right well i would like to begin

00:00:02,080 --> 00:00:06,000
this session

00:00:03,280 --> 00:00:07,680
um and the goal here is to do a deep

00:00:06,000 --> 00:00:08,880
dive into the open telemetry metrics

00:00:07,680 --> 00:00:11,200
system

00:00:08,880 --> 00:00:12,320
um i was asked to give this talk uh by

00:00:11,200 --> 00:00:13,840
the organizers

00:00:12,320 --> 00:00:16,080
and they suggested that it would be a

00:00:13,840 --> 00:00:17,119
good idea to have a conversation a year

00:00:16,080 --> 00:00:20,640
ago we did this

00:00:17,119 --> 00:00:21,119
um earlier on in our open telemetry

00:00:20,640 --> 00:00:23,600
project

00:00:21,119 --> 00:00:24,800
and had a similar presentation here

00:00:23,600 --> 00:00:27,119
where

00:00:24,800 --> 00:00:28,720
liz from honeycomb spoke with me for an

00:00:27,119 --> 00:00:30,880
hour about the metrics api and it was

00:00:28,720 --> 00:00:32,960
really helpful to have

00:00:30,880 --> 00:00:34,160
outsiders and insiders talking with me

00:00:32,960 --> 00:00:35,680
about it as we went

00:00:34,160 --> 00:00:38,160
so we're going to do that again and i

00:00:35,680 --> 00:00:41,600
have with me shelby and justin

00:00:38,160 --> 00:00:44,000
shelby is at honeycomb and has agreed to

00:00:41,600 --> 00:00:45,280
be an eager participant in this session

00:00:44,000 --> 00:00:47,039
and then justin

00:00:45,280 --> 00:00:48,960
is at new relic and has been one of the

00:00:47,039 --> 00:00:51,120
contributors especially working on our

00:00:48,960 --> 00:00:54,879
semantic conventions for the

00:00:51,120 --> 00:00:56,559
open telemetry system okay

00:00:54,879 --> 00:00:58,320
i probably should introduce myself i'm

00:00:56,559 --> 00:01:00,399
an engineer at lightstep

00:00:58,320 --> 00:01:03,280
i've been working in observability for i

00:01:00,399 --> 00:01:06,159
think about 15 years

00:01:03,280 --> 00:01:07,360
before lightstep i was at google and

00:01:06,159 --> 00:01:09,760
have been involved in like

00:01:07,360 --> 00:01:10,640
tracing metrics and logging for a long

00:01:09,760 --> 00:01:12,799
time

00:01:10,640 --> 00:01:14,159
okay so the outline is there are three

00:01:12,799 --> 00:01:15,920
parts here um

00:01:14,159 --> 00:01:17,360
going to start with what we were trying

00:01:15,920 --> 00:01:18,799
to achieve and what we're after

00:01:17,360 --> 00:01:20,799
in this project because we're not done

00:01:18,799 --> 00:01:22,080
it's it's underway um

00:01:20,799 --> 00:01:24,000
and we'll talk about the sort of

00:01:22,080 --> 00:01:26,880
timeline we that we think we have

00:01:24,000 --> 00:01:28,240
um the bulk of this sort of slide deck

00:01:26,880 --> 00:01:29,759
is talking about data model

00:01:28,240 --> 00:01:31,280
and the things we discovered when we

00:01:29,759 --> 00:01:34,400
tried to combine

00:01:31,280 --> 00:01:35,280
a prometheus system with a statistic

00:01:34,400 --> 00:01:37,920
system

00:01:35,280 --> 00:01:39,200
with a tracing system what happened and

00:01:37,920 --> 00:01:41,280
then lastly we're going to talk about

00:01:39,200 --> 00:01:42,720
some of the ways you can configure this

00:01:41,280 --> 00:01:44,720
thing that we've built the open

00:01:42,720 --> 00:01:46,560
telemetry metrics which includes both

00:01:44,720 --> 00:01:48,000
sdks and the collector

00:01:46,560 --> 00:01:51,439
and all the various protocols that we

00:01:48,000 --> 00:01:54,880
have at our disposal

00:01:51,439 --> 00:01:55,920
okay apologize for anybody visually

00:01:54,880 --> 00:01:57,680
impaired i'm

00:01:55,920 --> 00:01:59,200
better at drawing slides than i am at

00:01:57,680 --> 00:02:00,399
making them on the computer so i've

00:01:59,200 --> 00:02:04,240
scanned a bunch of um

00:02:00,399 --> 00:02:06,799
hand drawings for this uh deck and um

00:02:04,240 --> 00:02:08,640
hopefully uh you can follow along and

00:02:06,799 --> 00:02:10,479
read my writing

00:02:08,640 --> 00:02:12,480
um i'm trying to sort of start by saying

00:02:10,479 --> 00:02:14,400
who this is for i want to attract

00:02:12,480 --> 00:02:15,520
like an audience to talk about metrics

00:02:14,400 --> 00:02:16,879
now so i think we can

00:02:15,520 --> 00:02:18,959
basically agree that there are many

00:02:16,879 --> 00:02:21,520
different participants in the in the

00:02:18,959 --> 00:02:23,440
in this type of community we have people

00:02:21,520 --> 00:02:25,440
who are trying to configure telemetry in

00:02:23,440 --> 00:02:25,920
diagnostics for their platforms for

00:02:25,440 --> 00:02:27,599
their

00:02:25,920 --> 00:02:29,440
for their for their companies or their

00:02:27,599 --> 00:02:30,879
where they work there are engineers who

00:02:29,440 --> 00:02:32,480
are actually writing instrumentation and

00:02:30,879 --> 00:02:34,080
software at those companies trying to

00:02:32,480 --> 00:02:35,760
make more observability and better

00:02:34,080 --> 00:02:37,040
diagnostics for themselves

00:02:35,760 --> 00:02:38,239
and then there are actually users who

00:02:37,040 --> 00:02:39,040
are trying to understand why is my

00:02:38,239 --> 00:02:41,200
system broken

00:02:39,040 --> 00:02:42,720
why can't i you know log into my my

00:02:41,200 --> 00:02:44,480
machine or whatever

00:02:42,720 --> 00:02:46,160
so these three different groups have

00:02:44,480 --> 00:02:48,720
kind of different perspectives

00:02:46,160 --> 00:02:50,000
on the problem space and um we end up

00:02:48,720 --> 00:02:52,480
thinking about what each of them

00:02:50,000 --> 00:02:54,080
wants as we um talk through this problem

00:02:52,480 --> 00:02:57,519
space

00:02:54,080 --> 00:02:59,040
okay ultimately the goal is to get some

00:02:57,519 --> 00:03:00,560
dashboards or some other form of

00:02:59,040 --> 00:03:01,760
alerting or monitoring on your data and

00:03:00,560 --> 00:03:04,319
that looks something like this drawing

00:03:01,760 --> 00:03:06,720
that i made here on the right um

00:03:04,319 --> 00:03:08,560
this next diagram comes straight out of

00:03:06,720 --> 00:03:10,159
the open telemetry library guidelines

00:03:08,560 --> 00:03:11,760
this was put together in the very early

00:03:10,159 --> 00:03:12,720
days of the project basically seeing

00:03:11,760 --> 00:03:15,519
what we're after

00:03:12,720 --> 00:03:16,800
as far as the project as a whole one of

00:03:15,519 --> 00:03:17,120
the things that we wanted to do is build

00:03:16,800 --> 00:03:19,360
a

00:03:17,120 --> 00:03:21,760
neutral system so that you could as a as

00:03:19,360 --> 00:03:23,040
a developer decide to use open telemetry

00:03:21,760 --> 00:03:26,000
without locking yourself

00:03:23,040 --> 00:03:26,879
into somebody's sdk or some some vendors

00:03:26,000 --> 00:03:28,799
system

00:03:26,879 --> 00:03:31,599
so what that means is that we've created

00:03:28,799 --> 00:03:34,159
an api separation from the sdk

00:03:31,599 --> 00:03:35,040
it means our interfaces are decoupled

00:03:34,159 --> 00:03:36,799
from our

00:03:35,040 --> 00:03:39,120
implementations and this means that you

00:03:36,799 --> 00:03:41,920
can swap in another sdk

00:03:39,120 --> 00:03:42,560
or some alternate implementation later

00:03:41,920 --> 00:03:45,599
um

00:03:42,560 --> 00:03:47,120
so the the diagram has here your your

00:03:45,599 --> 00:03:50,000
application code running

00:03:47,120 --> 00:03:52,720
sorry um and then it goes into this sort

00:03:50,000 --> 00:03:54,799
of green box which combines both the api

00:03:52,720 --> 00:03:56,400
that's the spec that we've put together

00:03:54,799 --> 00:03:58,879
for how you interact with metrics

00:03:56,400 --> 00:04:00,319
as well as the sdk which is the default

00:03:58,879 --> 00:04:01,599
implementation that all the open

00:04:00,319 --> 00:04:03,760
telemetry

00:04:01,599 --> 00:04:05,439
libraries are going to include and

00:04:03,760 --> 00:04:06,879
together this should provide you a high

00:04:05,439 --> 00:04:09,040
performance

00:04:06,879 --> 00:04:10,720
pipeline and you will then configure an

00:04:09,040 --> 00:04:13,200
exporter for the protocol that you want

00:04:10,720 --> 00:04:16,959
so that you can expose your data in

00:04:13,200 --> 00:04:20,160
to the system that you want to

00:04:16,959 --> 00:04:21,840
so josh is how much how much more effort

00:04:20,160 --> 00:04:23,600
is it to use something

00:04:21,840 --> 00:04:25,840
um to use the open telemetry setup

00:04:23,600 --> 00:04:28,320
versus some of the vendor neutral

00:04:25,840 --> 00:04:29,199
um integration or the vendor-specific

00:04:28,320 --> 00:04:32,639
integrations that

00:04:29,199 --> 00:04:35,360
people may be used to um

00:04:32,639 --> 00:04:37,280
i'm not exactly sure which vendor

00:04:35,360 --> 00:04:40,720
integrations you might be thinking of

00:04:37,280 --> 00:04:42,880
um so some

00:04:40,720 --> 00:04:44,479
say sdks come in with a bunch of

00:04:42,880 --> 00:04:46,240
built-in metrics for say your platform

00:04:44,479 --> 00:04:47,360
so you start using this library and you

00:04:46,240 --> 00:04:48,800
get host metrics by

00:04:47,360 --> 00:04:51,040
out of the box you get kubernetes

00:04:48,800 --> 00:04:52,880
metrics out of the box um

00:04:51,040 --> 00:04:54,800
this is something that the open geometry

00:04:52,880 --> 00:04:56,479
system will include um so

00:04:54,800 --> 00:04:58,000
automatic metrics as much as possible

00:04:56,479 --> 00:05:00,479
are going to be included for you

00:04:58,000 --> 00:05:01,280
and um i've really structured this talk

00:05:00,479 --> 00:05:02,960
cut to leave

00:05:01,280 --> 00:05:04,479
sort of the details that you as a

00:05:02,960 --> 00:05:05,680
programmer might want to know to write

00:05:04,479 --> 00:05:07,280
your own metrics

00:05:05,680 --> 00:05:08,800
it's really not the most important part

00:05:07,280 --> 00:05:09,919
of this topic this talk because what

00:05:08,800 --> 00:05:11,600
we're really trying to do is help you

00:05:09,919 --> 00:05:12,320
set up an ecosystem help you set up a

00:05:11,600 --> 00:05:14,240
collector

00:05:12,320 --> 00:05:16,240
and an export pipeline very few

00:05:14,240 --> 00:05:17,199
engineers actually write custom metrics

00:05:16,240 --> 00:05:20,160
and so that's sort of the least

00:05:17,199 --> 00:05:23,280
important part of the top talk here

00:05:20,160 --> 00:05:26,479
gotcha thank you all right

00:05:23,280 --> 00:05:28,400
so what's next here um

00:05:26,479 --> 00:05:30,240
uh one of the major requirements here is

00:05:28,400 --> 00:05:32,400
that we are an open source project

00:05:30,240 --> 00:05:33,680
and and this at least at least from my

00:05:32,400 --> 00:05:35,199
perspective was one of the biggest

00:05:33,680 --> 00:05:36,800
challenges of joining an open source

00:05:35,199 --> 00:05:37,840
project is now you don't work for a

00:05:36,800 --> 00:05:40,880
company anymore you

00:05:37,840 --> 00:05:43,039
kind of have your own um we have the

00:05:40,880 --> 00:05:46,560
community to think about first and so

00:05:43,039 --> 00:05:48,400
this has been um a project that

00:05:46,560 --> 00:05:49,840
that moves at the pace of the community

00:05:48,400 --> 00:05:50,639
because we're getting what the community

00:05:49,840 --> 00:05:52,720
wants so

00:05:50,639 --> 00:05:54,960
i've drawn a picture here of a kind of a

00:05:52,720 --> 00:05:56,800
system that basically is a collection

00:05:54,960 --> 00:05:58,560
pipeline for both tracing and metrics

00:05:56,800 --> 00:06:01,120
data and we're trying to show that

00:05:58,560 --> 00:06:02,880
this collection infrastructure is going

00:06:01,120 --> 00:06:04,160
to work with all the open source systems

00:06:02,880 --> 00:06:07,840
that you're already using because that

00:06:04,160 --> 00:06:07,840
was one of our priorities

00:06:10,400 --> 00:06:15,360
okay um one of the sources of our sort

00:06:13,600 --> 00:06:17,360
of most sophisticated requirements came

00:06:15,360 --> 00:06:18,639
from open census the open census system

00:06:17,360 --> 00:06:20,000
is really what gave us the open

00:06:18,639 --> 00:06:23,360
telemetry project

00:06:20,000 --> 00:06:25,520
combined with open tracing so

00:06:23,360 --> 00:06:27,440
the the open tracing side of that

00:06:25,520 --> 00:06:29,919
combination gave us that requirement

00:06:27,440 --> 00:06:32,000
about sdk separation from the api

00:06:29,919 --> 00:06:33,919
what opencensus gave us was the

00:06:32,000 --> 00:06:35,039
requirement for a very high performance

00:06:33,919 --> 00:06:37,759
sdk

00:06:35,039 --> 00:06:38,800
and the requirement to have that sdk be

00:06:37,759 --> 00:06:40,319
configurable

00:06:38,800 --> 00:06:42,000
and in metrics what that meant was the

00:06:40,319 --> 00:06:43,120
ability to choose which metrics are

00:06:42,000 --> 00:06:44,639
going to be exported

00:06:43,120 --> 00:06:46,639
to choose which dimensions are going to

00:06:44,639 --> 00:06:47,600
be exported and they call that the views

00:06:46,639 --> 00:06:49,840
api

00:06:47,600 --> 00:06:51,440
so all those requirements were given to

00:06:49,840 --> 00:06:52,639
us sort of at the starting point

00:06:51,440 --> 00:06:55,120
we need to have a better way to

00:06:52,639 --> 00:06:58,800
configure which what happens

00:06:55,120 --> 00:06:58,800
when you use a metrics api

00:07:00,479 --> 00:07:03,759
next it's not just about how do you

00:07:02,560 --> 00:07:05,840
record numbers

00:07:03,759 --> 00:07:07,680
um we need to make sure that the entire

00:07:05,840 --> 00:07:09,039
problem is being solved so in addition

00:07:07,680 --> 00:07:12,560
to you creating your own

00:07:09,039 --> 00:07:14,400
application metrics you have plugins for

00:07:12,560 --> 00:07:16,000
your host metrics you have plugins for

00:07:14,400 --> 00:07:18,240
your kubernetes metrics

00:07:16,000 --> 00:07:19,440
you have instrumentation for tracing

00:07:18,240 --> 00:07:21,840
that is shared

00:07:19,440 --> 00:07:23,120
has shared resource attributes for your

00:07:21,840 --> 00:07:25,360
metrics and so

00:07:23,120 --> 00:07:27,440
we want to have specifications that tell

00:07:25,360 --> 00:07:30,000
us exactly how you should label data

00:07:27,440 --> 00:07:31,199
in a standard way so that users will be

00:07:30,000 --> 00:07:32,560
able to find it and understand what

00:07:31,199 --> 00:07:34,479
they're looking at

00:07:32,560 --> 00:07:36,319
and so this is really a task that

00:07:34,479 --> 00:07:38,639
bridges technical stuff

00:07:36,319 --> 00:07:40,560
with kind of language stuff and

00:07:38,639 --> 00:07:42,080
understanding and questions about how we

00:07:40,560 --> 00:07:45,039
talk and what names we use

00:07:42,080 --> 00:07:46,639
so this is an interesting sort of uh

00:07:45,039 --> 00:07:47,280
corner of the specification which is

00:07:46,639 --> 00:07:49,360
really about

00:07:47,280 --> 00:07:51,919
language and terminology and

00:07:49,360 --> 00:07:51,919
understanding

00:07:52,960 --> 00:07:57,280
um before i talk about data model and

00:07:55,680 --> 00:07:59,440
what you can do with this i want to

00:07:57,280 --> 00:08:00,400
be honest with you about the current

00:07:59,440 --> 00:08:03,360
timeline we

00:08:00,400 --> 00:08:05,039
are um right now the open telemetry

00:08:03,360 --> 00:08:07,680
project is trying to finish

00:08:05,039 --> 00:08:09,120
the tracing spec as and and and freeze

00:08:07,680 --> 00:08:10,160
that and make sure that we have a stable

00:08:09,120 --> 00:08:12,400
tracing environment

00:08:10,160 --> 00:08:14,000
soon because of that attention being

00:08:12,400 --> 00:08:15,599
given to trace the metrics project has

00:08:14,000 --> 00:08:16,879
been slowed down a little bit

00:08:15,599 --> 00:08:18,960
so we're kind of like putting our

00:08:16,879 --> 00:08:21,199
attention to tracing while we uh

00:08:18,960 --> 00:08:22,720
and waiting to finish metrics um but

00:08:21,199 --> 00:08:24,800
there is also just a large number of

00:08:22,720 --> 00:08:27,199
moving parts in this problem space

00:08:24,800 --> 00:08:28,479
we have specifications which um we've

00:08:27,199 --> 00:08:29,919
we've definitely finished the

00:08:28,479 --> 00:08:32,399
the for the most part we finished our

00:08:29,919 --> 00:08:33,680
api spec but the sdk specification is

00:08:32,399 --> 00:08:34,080
probably going to be the last thing to

00:08:33,680 --> 00:08:35,919
finish

00:08:34,080 --> 00:08:37,200
because we have to actually implement

00:08:35,919 --> 00:08:38,000
this sdk in a bunch of different

00:08:37,200 --> 00:08:39,599
languages

00:08:38,000 --> 00:08:41,279
and figure out what else needs to be

00:08:39,599 --> 00:08:44,000
answered but as far as

00:08:41,279 --> 00:08:44,880
um sort of the collector support that's

00:08:44,000 --> 00:08:46,160
been

00:08:44,880 --> 00:08:48,160
sort of ahead of schedule because we

00:08:46,160 --> 00:08:49,519
have the open census collector already

00:08:48,160 --> 00:08:51,200
and a number of the integrations and the

00:08:49,519 --> 00:08:53,200
receivers and exporters that you've

00:08:51,200 --> 00:08:54,320
heard about today were already in place

00:08:53,200 --> 00:08:56,320
for metrics so

00:08:54,320 --> 00:08:57,519
that's that's a little bit ahead and

00:08:56,320 --> 00:08:58,880
then

00:08:57,519 --> 00:09:00,240
there's a few kind of lingering

00:08:58,880 --> 00:09:01,440
questions about data model and the

00:09:00,240 --> 00:09:03,440
protocol that

00:09:01,440 --> 00:09:05,040
um are things that we may want to solve

00:09:03,440 --> 00:09:06,720
in the distant future

00:09:05,040 --> 00:09:08,480
we may not need to at all those are sort

00:09:06,720 --> 00:09:10,080
of open questions um

00:09:08,480 --> 00:09:11,839
things uh questions about sort of the

00:09:10,080 --> 00:09:13,120
obscure corners of the world where you

00:09:11,839 --> 00:09:14,320
might want to use the protocol for

00:09:13,120 --> 00:09:15,519
something different

00:09:14,320 --> 00:09:17,440
but for the most part we're getting

00:09:15,519 --> 00:09:18,959
close and we think that by the second

00:09:17,440 --> 00:09:20,000
first half of next year you'll be able

00:09:18,959 --> 00:09:23,200
to use this

00:09:20,000 --> 00:09:24,800
for real awesome

00:09:23,200 --> 00:09:26,640
okay i'm gonna start talking about data

00:09:24,800 --> 00:09:28,640
model now

00:09:26,640 --> 00:09:30,640
um before you do we have a couple

00:09:28,640 --> 00:09:33,680
questions in the chat that i wanted to

00:09:30,640 --> 00:09:35,760
share so um raj has a question does the

00:09:33,680 --> 00:09:37,360
sdk capture raw data

00:09:35,760 --> 00:09:41,120
at a time stamp or is there data

00:09:37,360 --> 00:09:43,200
aggregation processing in the sdk

00:09:41,120 --> 00:09:45,360
right so i i do plan to talk a little

00:09:43,200 --> 00:09:46,480
bit about that later there is definitely

00:09:45,360 --> 00:09:47,920
that's one of those performance

00:09:46,480 --> 00:09:48,720
requirements that we got from open

00:09:47,920 --> 00:09:50,160
census

00:09:48,720 --> 00:09:51,839
we need to have a high performance

00:09:50,160 --> 00:09:53,760
metrics library and we know that

00:09:51,839 --> 00:09:55,519
the sort of gold standard at the the

00:09:53,760 --> 00:09:56,160
point when we started was prometheus

00:09:55,519 --> 00:09:58,160
libraries

00:09:56,160 --> 00:09:59,200
so prometheus libraries are organized in

00:09:58,160 --> 00:10:01,440
such a way that

00:09:59,200 --> 00:10:03,040
your updates are very fast mainly

00:10:01,440 --> 00:10:04,160
because you're actually pinning memory

00:10:03,040 --> 00:10:05,519
to keep

00:10:04,160 --> 00:10:07,680
you know a variable in place that you

00:10:05,519 --> 00:10:09,839
can update quickly

00:10:07,680 --> 00:10:10,800
so we're going to talk later on this

00:10:09,839 --> 00:10:12,399
talk about

00:10:10,800 --> 00:10:14,079
something called aggregation temporality

00:10:12,399 --> 00:10:15,920
which lets us move that memory

00:10:14,079 --> 00:10:17,760
in our system but we're still going to

00:10:15,920 --> 00:10:20,640
be able to aggregate in the process

00:10:17,760 --> 00:10:22,160
over short windows of time and output

00:10:20,640 --> 00:10:25,440
data that's been aggregated

00:10:22,160 --> 00:10:25,440
before it leaves the process

00:10:25,519 --> 00:10:29,440
cool and yeah we'll go into that later

00:10:28,399 --> 00:10:31,920
in your presentation

00:10:29,440 --> 00:10:33,760
and then the other question um that's

00:10:31,920 --> 00:10:36,320
been coming up a lot today

00:10:33,760 --> 00:10:38,079
um i'm not sure if you're planning to

00:10:36,320 --> 00:10:40,640
talk about this a little bit more but

00:10:38,079 --> 00:10:41,600
um the relationship between openmetrics

00:10:40,640 --> 00:10:45,120
and hotel

00:10:41,600 --> 00:10:46,959
or otlp yes that is absolutely going to

00:10:45,120 --> 00:10:48,480
be part of this presentation it's one of

00:10:46,959 --> 00:10:50,079
the bigger questions that i have to

00:10:48,480 --> 00:10:51,040
answer and i hope that i will be able to

00:10:50,079 --> 00:10:52,720
answer that by the end of this

00:10:51,040 --> 00:10:54,959
presentation

00:10:52,720 --> 00:10:55,760
great then i will get out of your way

00:10:54,959 --> 00:10:58,320
cool

00:10:55,760 --> 00:11:00,320
all right so i just want to start with

00:10:58,320 --> 00:11:01,839
data model like this is a

00:11:00,320 --> 00:11:03,920
like what are we doing this is about

00:11:01,839 --> 00:11:05,120
metrics so i kind of want to bring us

00:11:03,920 --> 00:11:07,360
all back to the top

00:11:05,120 --> 00:11:09,760
level which is we're trying to look at

00:11:07,360 --> 00:11:11,440
some numbers probably in a visual way

00:11:09,760 --> 00:11:13,279
we may also be alerting and monitoring

00:11:11,440 --> 00:11:15,680
on them but the classic application for

00:11:13,279 --> 00:11:17,519
metrics is to create some charts

00:11:15,680 --> 00:11:19,120
and put them on a dashboard so i want to

00:11:17,519 --> 00:11:20,000
talk through the different kinds of

00:11:19,120 --> 00:11:22,160
charts

00:11:20,000 --> 00:11:23,920
and dashboarding facilities that are

00:11:22,160 --> 00:11:25,839
commonly available through metrics

00:11:23,920 --> 00:11:27,519
because there are different data types

00:11:25,839 --> 00:11:29,440
here so

00:11:27,519 --> 00:11:30,640
um the next few slides are going to talk

00:11:29,440 --> 00:11:32,720
about your sort of

00:11:30,640 --> 00:11:36,160
typical charts that you might get out of

00:11:32,720 --> 00:11:38,160
a metric system that's what we're after

00:11:36,160 --> 00:11:39,680
so first here is a count time series

00:11:38,160 --> 00:11:41,279
what i'm calling a count time series is

00:11:39,680 --> 00:11:43,839
just that there's some counter

00:11:41,279 --> 00:11:45,360
and it's it's cumulative in the sense

00:11:43,839 --> 00:11:46,480
that i've begin this counter at the

00:11:45,360 --> 00:11:48,560
start of my process

00:11:46,480 --> 00:11:50,639
and i may add to it i may subtract to it

00:11:48,560 --> 00:11:54,079
we haven't talked about monotonicity yet

00:11:50,639 --> 00:11:54,720
but over time that total is kept as a

00:11:54,079 --> 00:11:56,959
sum

00:11:54,720 --> 00:11:58,160
and whatever value i'm looking at as a

00:11:56,959 --> 00:12:01,120
function of time

00:11:58,160 --> 00:12:03,040
is the total cumulative count for that

00:12:01,120 --> 00:12:04,240
metric

00:12:03,040 --> 00:12:06,560
this is going to come up a lot

00:12:04,240 --> 00:12:08,800
throughout this talk i use the

00:12:06,560 --> 00:12:10,000
we use the terms cumulative and delta

00:12:08,800 --> 00:12:11,920
i'm going to use the greek letter

00:12:10,000 --> 00:12:14,880
sigma to refer to cumulative since it's

00:12:11,920 --> 00:12:17,200
a mathematical association we all have

00:12:14,880 --> 00:12:19,120
and then you can imagine looking at the

00:12:17,200 --> 00:12:21,839
same data

00:12:19,120 --> 00:12:22,320
as a rate now this is one of the big

00:12:21,839 --> 00:12:24,160
deals

00:12:22,320 --> 00:12:26,480
here in metrics is that you can often

00:12:24,160 --> 00:12:27,519
represent count or some data as either a

00:12:26,480 --> 00:12:29,600
rate or a total

00:12:27,519 --> 00:12:30,880
and that's this comes this is one of the

00:12:29,600 --> 00:12:31,920
complications that we're going to have

00:12:30,880 --> 00:12:33,120
to deal with

00:12:31,920 --> 00:12:34,959
um because they have different

00:12:33,120 --> 00:12:36,160
properties um but they're roughly

00:12:34,959 --> 00:12:38,800
speaking equivalent

00:12:36,160 --> 00:12:39,839
so when we talk about showing a rate it

00:12:38,800 --> 00:12:41,680
is um

00:12:39,839 --> 00:12:44,079
going to be associated with what we call

00:12:41,680 --> 00:12:44,480
deltas you're reporting the change in

00:12:44,079 --> 00:12:46,079
some

00:12:44,480 --> 00:12:48,560
quantity and then you're going to plot

00:12:46,079 --> 00:12:50,480
the change so this change could be

00:12:48,560 --> 00:12:52,399
um can is a number that can rise and

00:12:50,480 --> 00:12:54,320
fall and if the number uh

00:12:52,399 --> 00:12:56,399
the count is not monotonic this change

00:12:54,320 --> 00:12:58,560
could actually be negative

00:12:56,399 --> 00:13:01,040
so this is another way to talk about

00:12:58,560 --> 00:13:03,519
reporting some data

00:13:01,040 --> 00:13:05,440
um we also have this notion of a gauge

00:13:03,519 --> 00:13:06,480
in the kind of traditional metrics

00:13:05,440 --> 00:13:08,240
interfaces

00:13:06,480 --> 00:13:10,720
and the data model here is a little

00:13:08,240 --> 00:13:13,120
different from um

00:13:10,720 --> 00:13:15,200
sort of well what i'm trying to show

00:13:13,120 --> 00:13:17,600
with this visualization diagram here

00:13:15,200 --> 00:13:18,560
is that you may set a gauge many times

00:13:17,600 --> 00:13:20,240
during an interval

00:13:18,560 --> 00:13:22,320
but what we commonly report when we're

00:13:20,240 --> 00:13:23,200
talking about gauges is the last value

00:13:22,320 --> 00:13:26,320
that was set

00:13:23,200 --> 00:13:28,480
so although you have many points

00:13:26,320 --> 00:13:30,480
visualized in this in this graph only

00:13:28,480 --> 00:13:30,959
the blue colored dots are the ones that

00:13:30,480 --> 00:13:33,279
are going to be

00:13:30,959 --> 00:13:34,079
actually reported so this is called last

00:13:33,279 --> 00:13:37,120
value

00:13:34,079 --> 00:13:38,720
reporting we call this a gauge

00:13:37,120 --> 00:13:40,399
and it's interesting because we don't

00:13:38,720 --> 00:13:42,399
actually value or

00:13:40,399 --> 00:13:44,000
keep information about every point in

00:13:42,399 --> 00:13:45,199
some sense the number of points is

00:13:44,000 --> 00:13:46,959
irrelevant here

00:13:45,199 --> 00:13:49,199
all we want to know is that there's a

00:13:46,959 --> 00:13:49,680
signal we can evaluate it at a point in

00:13:49,199 --> 00:13:52,160
time

00:13:49,680 --> 00:13:54,079
and we often record just one value this

00:13:52,160 --> 00:13:56,240
is a relatively inexpensive type of

00:13:54,079 --> 00:13:57,680
aggregation

00:13:56,240 --> 00:13:59,360
now we also have this thing we call

00:13:57,680 --> 00:14:00,399
histograms or sometimes we call them

00:13:59,360 --> 00:14:02,320
distributions

00:14:00,399 --> 00:14:03,519
the idea is that here you have these

00:14:02,320 --> 00:14:05,040
individual measurements

00:14:03,519 --> 00:14:07,040
and instead of just recording say the

00:14:05,040 --> 00:14:08,720
last value of one of those measurements

00:14:07,040 --> 00:14:10,800
we're going to somehow capture more

00:14:08,720 --> 00:14:12,399
we're going to capture both account

00:14:10,800 --> 00:14:14,639
and the value so that we can speak

00:14:12,399 --> 00:14:16,800
independently about the count

00:14:14,639 --> 00:14:18,560
and about the values so what i've drawn

00:14:16,800 --> 00:14:21,199
here now is a distribution

00:14:18,560 --> 00:14:23,360
on the top i've got red purple and blue

00:14:21,199 --> 00:14:26,399
showing you quantiles so this might be

00:14:23,360 --> 00:14:28,160
p99 p50 p10 telling you where the

00:14:26,399 --> 00:14:29,040
distribution of latency or some other

00:14:28,160 --> 00:14:31,519
value in your

00:14:29,040 --> 00:14:32,399
system is and then on the bottom i have

00:14:31,519 --> 00:14:34,560
a rate

00:14:32,399 --> 00:14:36,639
plot which is showing for the same data

00:14:34,560 --> 00:14:39,120
set how many points were there

00:14:36,639 --> 00:14:41,360
per unit of time so a histogram is sort

00:14:39,120 --> 00:14:43,600
of the most expensive type of metric

00:14:41,360 --> 00:14:45,680
data that we report because it includes

00:14:43,600 --> 00:14:48,560
two independent pieces of information

00:14:45,680 --> 00:14:49,920
and we often use it to summarize a whole

00:14:48,560 --> 00:14:51,120
distribution

00:14:49,920 --> 00:14:53,519
so these tend to be a little bit more

00:14:51,120 --> 00:14:55,440
expensive

00:14:53,519 --> 00:14:56,800
that the goal of those last four pages

00:14:55,440 --> 00:14:58,480
there was to show you that

00:14:56,800 --> 00:15:00,000
there's something familiar and common

00:14:58,480 --> 00:15:01,760
that we are after even though this data

00:15:00,000 --> 00:15:04,959
model may feel confusing after

00:15:01,760 --> 00:15:08,240
we start talking about it for a bit

00:15:04,959 --> 00:15:10,160
now as far as what's different in open

00:15:08,240 --> 00:15:11,920
metrics and open telemetry this is the

00:15:10,160 --> 00:15:13,519
this is where it starts there's two

00:15:11,920 --> 00:15:14,800
pieces of the data model that are not

00:15:13,519 --> 00:15:17,920
present

00:15:14,800 --> 00:15:19,760
in an open telemetry um system

00:15:17,920 --> 00:15:21,440
and i just want to say it's not because

00:15:19,760 --> 00:15:23,440
they're missing it's because when you're

00:15:21,440 --> 00:15:26,000
pulling data in a metric system

00:15:23,440 --> 00:15:27,920
you don't need these features so the

00:15:26,000 --> 00:15:29,759
first thing is resources

00:15:27,920 --> 00:15:32,320
resources are a concept for these key

00:15:29,759 --> 00:15:34,480
value attributes that we attach to our

00:15:32,320 --> 00:15:36,240
diagnostics or our instrument or our

00:15:34,480 --> 00:15:39,199
observability data it's not just

00:15:36,240 --> 00:15:40,480
metrics this is also for spans and logs

00:15:39,199 --> 00:15:42,240
so

00:15:40,480 --> 00:15:43,600
i use the term attributes for this key

00:15:42,240 --> 00:15:45,440
value association

00:15:43,600 --> 00:15:47,519
and the difference between what we've

00:15:45,440 --> 00:15:48,639
got in openometry and what you have and

00:15:47,519 --> 00:15:50,880
say openmetrics

00:15:48,639 --> 00:15:51,680
is that this concept or resource is in

00:15:50,880 --> 00:15:53,680
the protocol

00:15:51,680 --> 00:15:55,279
so that when you report a batch of

00:15:53,680 --> 00:15:56,720
metrics data you're going to have one

00:15:55,279 --> 00:15:57,600
section for resources

00:15:56,720 --> 00:16:00,720
and then you're going to have a lot of

00:15:57,600 --> 00:16:04,079
metrics data inside that encapsulation

00:16:00,720 --> 00:16:06,560
and so this allows us to begin creating

00:16:04,079 --> 00:16:08,800
common uh coordinates and telemetry

00:16:06,560 --> 00:16:09,199
between traces and and metrics because

00:16:08,800 --> 00:16:11,600
we have

00:16:09,199 --> 00:16:13,120
standard resources and we can configure

00:16:11,600 --> 00:16:15,920
these export pipelines

00:16:13,120 --> 00:16:16,639
to attach standard resources to both our

00:16:15,920 --> 00:16:18,320
traces

00:16:16,639 --> 00:16:20,720
and our metrics as they pass through the

00:16:18,320 --> 00:16:22,639
export pipeline so having resources is a

00:16:20,720 --> 00:16:23,920
first class concept

00:16:22,639 --> 00:16:25,759
and then of course we have resource

00:16:23,920 --> 00:16:26,399
conventions for how you should name your

00:16:25,759 --> 00:16:28,399
resources

00:16:26,399 --> 00:16:30,639
i've shown one example here which is for

00:16:28,399 --> 00:16:32,160
the service attribute namespace

00:16:30,639 --> 00:16:34,079
you've got service name service

00:16:32,160 --> 00:16:35,199
namespace service instance id and

00:16:34,079 --> 00:16:37,360
service version

00:16:35,199 --> 00:16:39,199
we are suggesting that these sorts of

00:16:37,360 --> 00:16:39,759
attributes may be included with metric

00:16:39,199 --> 00:16:42,000
data

00:16:39,759 --> 00:16:43,440
and then some point later you may have

00:16:42,000 --> 00:16:45,199
to export them into a system that

00:16:43,440 --> 00:16:46,160
doesn't have this concept of resources

00:16:45,199 --> 00:16:47,920
and we're going to have to talk about

00:16:46,160 --> 00:16:51,199
whether those resources are

00:16:47,920 --> 00:16:51,680
or are not attached as metric attributes

00:16:51,199 --> 00:16:52,959
or

00:16:51,680 --> 00:16:54,560
sometimes these are called labels and

00:16:52,959 --> 00:16:56,880
tags we have a terminology problem there

00:16:54,560 --> 00:16:58,880
and try to avoid it for this talk

00:16:56,880 --> 00:17:00,399
so so the first thing we're adding

00:16:58,880 --> 00:17:01,759
that's different from openmetrics is the

00:17:00,399 --> 00:17:03,279
concept of resources

00:17:01,759 --> 00:17:05,120
and they say that you don't need these

00:17:03,279 --> 00:17:07,039
in a pull-based system because if you

00:17:05,120 --> 00:17:09,039
think about how prometheus works

00:17:07,039 --> 00:17:11,360
you scrape the target the target gives

00:17:09,039 --> 00:17:13,280
you all of its own metric data

00:17:11,360 --> 00:17:15,199
including its own metric labels but it

00:17:13,280 --> 00:17:17,120
doesn't know its own resources

00:17:15,199 --> 00:17:18,319
the whole point of scraping and a

00:17:17,120 --> 00:17:20,720
pull-based system

00:17:18,319 --> 00:17:21,520
is that you discover the targets you

00:17:20,720 --> 00:17:23,439
scrape them

00:17:21,520 --> 00:17:25,919
and you know what their resources are so

00:17:23,439 --> 00:17:27,360
it's the person that pulls that data

00:17:25,919 --> 00:17:29,280
who's responsible for attaching

00:17:27,360 --> 00:17:31,600
resources and that's why you didn't need

00:17:29,280 --> 00:17:33,760
it in open metrics

00:17:31,600 --> 00:17:35,679
the other thing that's different in open

00:17:33,760 --> 00:17:37,840
telemetry compared with openmetrics

00:17:35,679 --> 00:17:38,720
is this notion that we have support for

00:17:37,840 --> 00:17:41,039
temporality

00:17:38,720 --> 00:17:43,679
the word temporality is one that we um

00:17:41,039 --> 00:17:45,360
sort of made up it is a real word but

00:17:43,679 --> 00:17:47,600
we're using it to refer to this

00:17:45,360 --> 00:17:49,440
distinction between delta and cumulative

00:17:47,600 --> 00:17:51,280
reporting

00:17:49,440 --> 00:17:53,280
so um and i want to make a distinction

00:17:51,280 --> 00:17:55,280
between because we use this word in two

00:17:53,280 --> 00:17:56,480
senses here it's pretty important to the

00:17:55,280 --> 00:17:59,200
design of the whole system

00:17:56,480 --> 00:18:00,720
so bear with me the idea that there's

00:17:59,200 --> 00:18:02,720
something called temporality

00:18:00,720 --> 00:18:03,919
is trying to describe a relationship

00:18:02,720 --> 00:18:07,200
with time

00:18:03,919 --> 00:18:07,760
and when we report um metrics data they

00:18:07,200 --> 00:18:09,520
may be

00:18:07,760 --> 00:18:11,200
cumulative or they may be delta

00:18:09,520 --> 00:18:12,160
generally speaking deltas mean that

00:18:11,200 --> 00:18:14,320
you're reporting

00:18:12,160 --> 00:18:15,440
differences since the last report and

00:18:14,320 --> 00:18:17,679
every report

00:18:15,440 --> 00:18:18,960
is independent and does not build on the

00:18:17,679 --> 00:18:21,679
previous report

00:18:18,960 --> 00:18:22,960
we need to um so we're reporting changes

00:18:21,679 --> 00:18:24,400
one after the other

00:18:22,960 --> 00:18:26,240
in cumulative reporting we're going to

00:18:24,400 --> 00:18:27,360
repeat report some total since the

00:18:26,240 --> 00:18:28,799
beginning of time

00:18:27,360 --> 00:18:30,960
and we're going to keep re-reporting

00:18:28,799 --> 00:18:36,000
that total since the beginning of time

00:18:30,960 --> 00:18:36,000
okay why are we talking about this well

00:18:36,480 --> 00:18:39,679
there's something here called instrument

00:18:38,320 --> 00:18:41,440
temporality which is

00:18:39,679 --> 00:18:43,440
what type of numbers does this

00:18:41,440 --> 00:18:44,080
instrument deal with and that's an api

00:18:43,440 --> 00:18:45,520
question

00:18:44,080 --> 00:18:47,520
and there's something called aggregation

00:18:45,520 --> 00:18:49,200
temporality which is what is in the

00:18:47,520 --> 00:18:50,640
protocol that i'm putting through my

00:18:49,200 --> 00:18:52,400
pipeline

00:18:50,640 --> 00:18:54,799
okay this is confusing i have another

00:18:52,400 --> 00:18:57,280
slide on it so why do we care

00:18:54,799 --> 00:18:58,240
well the this choice is entirely about

00:18:57,280 --> 00:19:00,720
keeping state

00:18:58,240 --> 00:19:02,480
and stateful interactions betw out of

00:19:00,720 --> 00:19:05,200
the sdk and the api

00:19:02,480 --> 00:19:06,960
for open telemetry so if you think about

00:19:05,200 --> 00:19:09,120
reporting say a total uh

00:19:06,960 --> 00:19:10,000
your current memory usage you don't want

00:19:09,120 --> 00:19:12,160
to report your current

00:19:10,000 --> 00:19:14,240
memory usage as a delta because that

00:19:12,160 --> 00:19:14,960
would require you to remember the last

00:19:14,240 --> 00:19:16,480
value that

00:19:14,960 --> 00:19:18,640
you reported for your current memory

00:19:16,480 --> 00:19:20,160
usage so therefore we find it's better

00:19:18,640 --> 00:19:21,440
if you're reporting something like a

00:19:20,160 --> 00:19:24,000
current memory usage

00:19:21,440 --> 00:19:25,039
to report a current total and in the

00:19:24,000 --> 00:19:27,120
system that we're going to have

00:19:25,039 --> 00:19:28,640
if you're reporting information from a

00:19:27,120 --> 00:19:29,120
callback you're generally going to

00:19:28,640 --> 00:19:31,600
report

00:19:29,120 --> 00:19:32,960
totals if you're reporting something in

00:19:31,600 --> 00:19:34,960
a mean line code

00:19:32,960 --> 00:19:37,120
as part of a transaction or a request

00:19:34,960 --> 00:19:38,960
you're going to report deltas

00:19:37,120 --> 00:19:40,240
so from the start we have this built

00:19:38,960 --> 00:19:41,919
into our instruments

00:19:40,240 --> 00:19:43,760
and it's because we think it is more

00:19:41,919 --> 00:19:45,760
convenient to talk about deltas

00:19:43,760 --> 00:19:47,919
in some contexts and more convenient to

00:19:45,760 --> 00:19:48,799
talk about totals in other contexts it

00:19:47,919 --> 00:19:52,240
keeps state

00:19:48,799 --> 00:19:54,480
out now the point of creating

00:19:52,240 --> 00:19:55,360
this concept in aggregation this

00:19:54,480 --> 00:19:56,720
instrument

00:19:55,360 --> 00:19:58,799
sorry the point of creating instrument

00:19:56,720 --> 00:20:01,280
temporality is that we can adjust

00:19:58,799 --> 00:20:03,280
aggregation temporality so what goes in

00:20:01,280 --> 00:20:04,880
is not necessarily what comes out

00:20:03,280 --> 00:20:07,039
and it turns out that if we have deltas

00:20:04,880 --> 00:20:07,440
coming in and we want cumulatives coming

00:20:07,039 --> 00:20:09,919
out

00:20:07,440 --> 00:20:11,440
we're going to put memory so this is why

00:20:09,919 --> 00:20:13,760
we have these questions

00:20:11,440 --> 00:20:15,200
there's a trade-off trade-off is between

00:20:13,760 --> 00:20:19,280
reliability

00:20:15,200 --> 00:20:20,480
and memory costs

00:20:19,280 --> 00:20:23,760
uh there's going to be more on this

00:20:20,480 --> 00:20:27,520
topic we will keep talking about this

00:20:23,760 --> 00:20:29,039
so um i need to so we're sort of

00:20:27,520 --> 00:20:30,559
we're still talking about data model but

00:20:29,039 --> 00:20:32,720
we're trying now to integrate

00:20:30,559 --> 00:20:34,559
the concepts from prometheus and the

00:20:32,720 --> 00:20:36,240
concepts from statsd

00:20:34,559 --> 00:20:37,679
because one of those systems uses

00:20:36,240 --> 00:20:39,120
cumulatives and one of those systems

00:20:37,679 --> 00:20:43,440
uses deltas

00:20:39,120 --> 00:20:45,360
so however in the api for both of those

00:20:43,440 --> 00:20:47,520
systems counters use deltas

00:20:45,360 --> 00:20:49,039
so both prometheus and stats you expect

00:20:47,520 --> 00:20:51,760
when you're using a counter

00:20:49,039 --> 00:20:52,880
that you will tell it a delta however

00:20:51,760 --> 00:20:54,799
when prometheus

00:20:52,880 --> 00:20:56,799
writes data to its spread ahead log or

00:20:54,799 --> 00:21:00,559
exports data through its remote right

00:20:56,799 --> 00:21:03,280
it's sending you cumulatives and so

00:21:00,559 --> 00:21:05,039
this is why prometheus has trouble with

00:21:03,280 --> 00:21:07,120
cardinality it requires you to keep

00:21:05,039 --> 00:21:08,640
memory inside the client library for

00:21:07,120 --> 00:21:10,720
every

00:21:08,640 --> 00:21:14,159
counter you've ever used because it has

00:21:10,720 --> 00:21:17,200
to track the cumulative value

00:21:14,159 --> 00:21:20,159
in open telemetry we are giving you

00:21:17,200 --> 00:21:21,440
these instruments that have delta

00:21:20,159 --> 00:21:23,600
instrument temporality

00:21:21,440 --> 00:21:24,960
counters and this this instrument that

00:21:23,600 --> 00:21:28,240
we call up down counter

00:21:24,960 --> 00:21:30,640
are for for inputting changes or deltas

00:21:28,240 --> 00:21:32,320
to a sum so these are just like the

00:21:30,640 --> 00:21:33,440
prometheus concepts these are just like

00:21:32,320 --> 00:21:35,280
the statsd concept

00:21:33,440 --> 00:21:36,559
except that we've given you two ones for

00:21:35,280 --> 00:21:40,559
monotonic counters

00:21:36,559 --> 00:21:43,280
and one's for non-monotonic counters

00:21:40,559 --> 00:21:45,440
this is actually the simple case these

00:21:43,280 --> 00:21:49,520
are deltas

00:21:45,440 --> 00:21:51,039
now when we talk about gauges

00:21:49,520 --> 00:21:52,320
i mentioned earlier in some of this

00:21:51,039 --> 00:21:54,159
breakout sessions that there's a little

00:21:52,320 --> 00:21:56,640
bit of a terminology problem

00:21:54,159 --> 00:21:58,320
and the i don't want to go too close to

00:21:56,640 --> 00:21:59,919
the details here because i will lose you

00:21:58,320 --> 00:22:02,159
and you'll get bored but basically

00:21:59,919 --> 00:22:04,000
prometheus and statsy use the term gage

00:22:02,159 --> 00:22:06,159
in slightly different ways and i've

00:22:04,000 --> 00:22:07,360
copied a definition out of wikipedia for

00:22:06,159 --> 00:22:09,200
the word gage itself

00:22:07,360 --> 00:22:10,960
turns out that the actual word has two

00:22:09,200 --> 00:22:13,200
meanings in our actual engineering

00:22:10,960 --> 00:22:14,000
practice this is a confusing term to

00:22:13,200 --> 00:22:15,440
begin with

00:22:14,000 --> 00:22:16,799
and it turns out because there's it's

00:22:15,440 --> 00:22:17,440
used differently in prometheus and

00:22:16,799 --> 00:22:19,679
statsd

00:22:17,440 --> 00:22:21,840
it's even more confusing we're trying

00:22:19,679 --> 00:22:23,600
not to use it

00:22:21,840 --> 00:22:26,080
so the thing that you're using a gauge

00:22:23,600 --> 00:22:29,200
for however is a real application

00:22:26,080 --> 00:22:31,520
in prometheus and statsd gauge got used

00:22:29,200 --> 00:22:33,440
whenever we wanted an individual

00:22:31,520 --> 00:22:34,640
measurement but not the cost of a

00:22:33,440 --> 00:22:37,600
histogram

00:22:34,640 --> 00:22:38,720
so open telemetry um created a new

00:22:37,600 --> 00:22:42,159
distinction to

00:22:38,720 --> 00:22:42,559
cover this gauge use case and it's when

00:22:42,159 --> 00:22:45,440
you're

00:22:42,559 --> 00:22:46,880
recording an individual measurement so

00:22:45,440 --> 00:22:48,320
i'm going to talk more about what an

00:22:46,880 --> 00:22:50,799
individual measurement means

00:22:48,320 --> 00:22:51,520
but the point is that if you take a

00:22:50,799 --> 00:22:54,720
measurement

00:22:51,520 --> 00:22:57,440
and there's something um so you

00:22:54,720 --> 00:22:58,000
say suppose you measure a latency you

00:22:57,440 --> 00:22:59,919
would never

00:22:58,000 --> 00:23:02,080
add that measurement of latency to

00:22:59,919 --> 00:23:04,080
another measurement of latency

00:23:02,080 --> 00:23:05,600
just for the sake of adding those two

00:23:04,080 --> 00:23:07,360
numbers together

00:23:05,600 --> 00:23:09,039
that is an individual measurement and

00:23:07,360 --> 00:23:10,000
you're interested in knowing individual

00:23:09,039 --> 00:23:12,799
latencies

00:23:10,000 --> 00:23:14,640
so there's some so um when you when you

00:23:12,799 --> 00:23:15,840
use this value recorder instrument

00:23:14,640 --> 00:23:17,919
you're going to be computing a

00:23:15,840 --> 00:23:19,760
distribution as opposed to computing a

00:23:17,919 --> 00:23:22,400
sum which is a much simpler operation

00:23:19,760 --> 00:23:24,400
and much cheaper operation

00:23:22,400 --> 00:23:25,840
um i'm trying to explain why gauge

00:23:24,400 --> 00:23:27,200
doesn't exist and i think i'm getting a

00:23:25,840 --> 00:23:28,799
little confused or

00:23:27,200 --> 00:23:30,640
i'm not sure that people are following

00:23:28,799 --> 00:23:33,200
you at me at this time

00:23:30,640 --> 00:23:34,720
because gage is a very confusing concept

00:23:33,200 --> 00:23:36,640
but what i'm trying to say here is that

00:23:34,720 --> 00:23:39,520
we've replaced the concept of a gauge

00:23:36,640 --> 00:23:40,000
instrument with two new instruments one

00:23:39,520 --> 00:23:41,679
is

00:23:40,000 --> 00:23:43,440
going to be called value recorder and

00:23:41,679 --> 00:23:45,360
one is called value observer

00:23:43,440 --> 00:23:46,960
we use value recorder to record

00:23:45,360 --> 00:23:50,000
individual measurements

00:23:46,960 --> 00:23:52,400
we use value observer to witness

00:23:50,000 --> 00:23:55,919
an individual measurement such as a

00:23:52,400 --> 00:23:55,919
callback observing a total

00:23:57,520 --> 00:24:02,400
i'm afraid this slide is not going off

00:23:59,440 --> 00:24:02,400
but i'm going to keep going

00:24:02,880 --> 00:24:06,320
the problem i'm trying to explain with

00:24:04,240 --> 00:24:08,320
gage is that sometimes they got used to

00:24:06,320 --> 00:24:10,640
record sums that were cumulative

00:24:08,320 --> 00:24:12,559
or sums that were not monotonic and

00:24:10,640 --> 00:24:14,880
sometimes they were used to record

00:24:12,559 --> 00:24:16,559
sort of um other measurements such as a

00:24:14,880 --> 00:24:18,480
temperature or a latency which is not

00:24:16,559 --> 00:24:21,679
something you ordinarily add together

00:24:18,480 --> 00:24:23,919
so the point is gauges got used

00:24:21,679 --> 00:24:24,720
for several different things in each of

00:24:23,919 --> 00:24:26,880
these systems

00:24:24,720 --> 00:24:28,720
and we are trying to provide you

00:24:26,880 --> 00:24:29,520
instruments that get used for exactly

00:24:28,720 --> 00:24:32,080
one thing

00:24:29,520 --> 00:24:33,679
if you're observing a sum or you're

00:24:32,080 --> 00:24:36,640
observing

00:24:33,679 --> 00:24:37,919
a non-monotonic sum instead of using a

00:24:36,640 --> 00:24:39,679
gauge you're going to use one of these

00:24:37,919 --> 00:24:41,679
new instruments called some observer

00:24:39,679 --> 00:24:42,960
or up down some observer and these are

00:24:41,679 --> 00:24:44,840
instruments that have

00:24:42,960 --> 00:24:47,600
so-called cumulative instrument

00:24:44,840 --> 00:24:48,720
temporality

00:24:47,600 --> 00:24:51,200
we're going to keep talking about this

00:24:48,720 --> 00:24:54,480
because you yes please shall we

00:24:51,200 --> 00:24:55,440
go back one slide um so when you say

00:24:54,480 --> 00:24:57,520
monotonic

00:24:55,440 --> 00:25:00,320
just because i'm i might have missed

00:24:57,520 --> 00:25:01,760
that meaning at one point that's like a

00:25:00,320 --> 00:25:03,919
something that's unrelated to the

00:25:01,760 --> 00:25:06,960
previous value right

00:25:03,919 --> 00:25:08,400
it's i may have confused you and i would

00:25:06,960 --> 00:25:11,039
love to just

00:25:08,400 --> 00:25:12,640
completely reiterate this so i've used

00:25:11,039 --> 00:25:14,799
term deltas and cumulatives

00:25:12,640 --> 00:25:16,559
like you just described this is about

00:25:14,799 --> 00:25:18,240
the relationship between

00:25:16,559 --> 00:25:19,919
a prior interval and the current

00:25:18,240 --> 00:25:23,520
interval um

00:25:19,919 --> 00:25:25,679
as far as reporting count some values

00:25:23,520 --> 00:25:27,200
the the the question about monotonicity

00:25:25,679 --> 00:25:28,880
is really about whether the

00:25:27,200 --> 00:25:30,720
function is always rising or always

00:25:28,880 --> 00:25:33,039
following for example um

00:25:30,720 --> 00:25:34,720
i don't so the distinction is between

00:25:33,039 --> 00:25:37,039
sums that rise and fall versus

00:25:34,720 --> 00:25:39,120
sums that only rise and when when we

00:25:37,039 --> 00:25:41,039
have a sum that only rises

00:25:39,120 --> 00:25:42,159
more often we're interested in showing

00:25:41,039 --> 00:25:43,760
that as a rate

00:25:42,159 --> 00:25:45,440
but when we have a sum that rises and

00:25:43,760 --> 00:25:46,880
falls more often we are interested in

00:25:45,440 --> 00:25:48,080
showing that as a count

00:25:46,880 --> 00:25:50,000
and so you're probably familiar with

00:25:48,080 --> 00:25:50,799
metrics interfaces having the ability to

00:25:50,000 --> 00:25:52,240
choose

00:25:50,799 --> 00:25:54,000
whether you're talking about a rate or a

00:25:52,240 --> 00:25:55,440
count this is this is

00:25:54,000 --> 00:25:59,120
something that we're actually encoding

00:25:55,440 --> 00:26:02,640
in these instruments for open telemetry

00:25:59,120 --> 00:26:05,600
so i can imagine like for um in a very

00:26:02,640 --> 00:26:06,880
lay example right age is always in

00:26:05,600 --> 00:26:08,720
relation to the previous year you can

00:26:06,880 --> 00:26:11,120
always just say like age plus one

00:26:08,720 --> 00:26:12,720
right and that's my birthday it's

00:26:11,120 --> 00:26:14,480
related to the previous year

00:26:12,720 --> 00:26:16,080
but for like temperature it's completely

00:26:14,480 --> 00:26:17,120
unrelated to the past yesterday's

00:26:16,080 --> 00:26:19,520
temperature

00:26:17,120 --> 00:26:21,200
um you might you might say like you

00:26:19,520 --> 00:26:22,640
might your google alert says like oh

00:26:21,200 --> 00:26:23,120
it's going to be seven degrees colder

00:26:22,640 --> 00:26:25,039
today

00:26:23,120 --> 00:26:27,039
but usually you just want the individual

00:26:25,039 --> 00:26:28,480
like today's temperature

00:26:27,039 --> 00:26:30,640
um yeah that's like that's a good

00:26:28,480 --> 00:26:32,320
observation you don't ordinarily monitor

00:26:30,640 --> 00:26:33,679
changes in temperature you monitor

00:26:32,320 --> 00:26:37,440
absolute temperatures

00:26:33,679 --> 00:26:40,720
but you might monitor something like

00:26:37,440 --> 00:26:43,039
that that is um being counted as a rate

00:26:40,720 --> 00:26:46,799
and then you might monitor that

00:26:43,039 --> 00:26:46,799
as a change so um

00:26:47,120 --> 00:26:52,880
yeah okay i i like using the

00:26:50,480 --> 00:26:55,039
very human examples but um it's it's

00:26:52,880 --> 00:26:58,159
also helpful to have them i let

00:26:55,039 --> 00:27:01,440
your other examples of things like um

00:26:58,159 --> 00:27:03,120
um cp usage right where you always want

00:27:01,440 --> 00:27:04,320
that at the particular moment you don't

00:27:03,120 --> 00:27:05,600
care how

00:27:04,320 --> 00:27:08,159
you don't usually care how much that

00:27:05,600 --> 00:27:10,559
relates to like five minutes ago

00:27:08,159 --> 00:27:11,919
yeah i i like your example of using age

00:27:10,559 --> 00:27:12,400
though it's a great one so you could

00:27:11,919 --> 00:27:14,640
imagine

00:27:12,400 --> 00:27:15,600
two ways of of monitoring you an age of

00:27:14,640 --> 00:27:17,520
a person

00:27:15,600 --> 00:27:19,679
if you're if you use a counter then

00:27:17,520 --> 00:27:21,360
every year you increment it by one

00:27:19,679 --> 00:27:23,360
if you're using a sum observer then

00:27:21,360 --> 00:27:25,200
every year you output your current age

00:27:23,360 --> 00:27:26,720
and that's always one more than the last

00:27:25,200 --> 00:27:28,399
year so and

00:27:26,720 --> 00:27:30,640
we could talk more about the differences

00:27:28,399 --> 00:27:32,320
um we will talk more about the

00:27:30,640 --> 00:27:34,320
differences as one of the key aspects of

00:27:32,320 --> 00:27:37,840
this design

00:27:34,320 --> 00:27:40,880
gotcha josh that's super helpful

00:27:37,840 --> 00:27:42,159
justin please would you explain a little

00:27:40,880 --> 00:27:44,960
bit about why

00:27:42,159 --> 00:27:47,279
an instrumentation author shouldn't just

00:27:44,960 --> 00:27:49,279
always use the up down sum observer

00:27:47,279 --> 00:27:50,720
when you could use it to record age as

00:27:49,279 --> 00:27:54,000
well right is there a good reason to

00:27:50,720 --> 00:27:57,520
only use the sum observer in that case

00:27:54,000 --> 00:27:59,200
yes and i i'm i promise i tried to put a

00:27:57,520 --> 00:28:02,480
aside about this actually it's the next

00:27:59,200 --> 00:28:04,399
one um so

00:28:02,480 --> 00:28:05,919
there has been this debate and i promise

00:28:04,399 --> 00:28:07,840
you we are going to spend more time

00:28:05,919 --> 00:28:09,360
updating our specs to try and clarify

00:28:07,840 --> 00:28:13,200
this issue um

00:28:09,360 --> 00:28:15,600
so uh i i'm now regretting that i

00:28:13,200 --> 00:28:16,960
that i didn't include more detail

00:28:15,600 --> 00:28:17,840
already on the instruments that we've

00:28:16,960 --> 00:28:20,240
designed we've got

00:28:17,840 --> 00:28:21,760
six instruments and and some of this is

00:28:20,240 --> 00:28:24,000
what we're talking about right now

00:28:21,760 --> 00:28:26,000
um but but but when we talk about

00:28:24,000 --> 00:28:28,159
semantic kind there really are

00:28:26,000 --> 00:28:29,039
two to my knowledge and in the technical

00:28:28,159 --> 00:28:30,640
committee we are

00:28:29,039 --> 00:28:32,240
in the sig we've been talking about the

00:28:30,640 --> 00:28:33,679
words adding versus grouping

00:28:32,240 --> 00:28:35,279
what we're trying to distinguish is

00:28:33,679 --> 00:28:38,640
between things that you add

00:28:35,279 --> 00:28:39,679
or things that you average um and things

00:28:38,640 --> 00:28:42,640
that you add

00:28:39,679 --> 00:28:43,360
are are interesting let's suppose you're

00:28:42,640 --> 00:28:45,440
sampling

00:28:43,360 --> 00:28:46,559
so if i have a bunch of observations of

00:28:45,440 --> 00:28:48,320
things that i add

00:28:46,559 --> 00:28:50,080
well the larger numbers are more

00:28:48,320 --> 00:28:50,960
interesting because they contribute more

00:28:50,080 --> 00:28:53,440
to a sum

00:28:50,960 --> 00:28:54,000
so if i'm say sampling numbers and i

00:28:53,440 --> 00:28:55,679
know that

00:28:54,000 --> 00:28:57,520
there are things i care about the sum

00:28:55,679 --> 00:28:59,520
well then i put weight on those

00:28:57,520 --> 00:29:01,120
the higher numbers get more weight when

00:28:59,520 --> 00:29:03,039
i'm sampling or down

00:29:01,120 --> 00:29:04,480
or any kind of aggregation that i'm

00:29:03,039 --> 00:29:07,039
doing where where sum

00:29:04,480 --> 00:29:08,640
is the the property i'm after then i

00:29:07,039 --> 00:29:11,279
want to know that it's a sum

00:29:08,640 --> 00:29:13,440
whereas if i'm doing something like down

00:29:11,279 --> 00:29:15,840
sampling or reducing dimensionality of

00:29:13,440 --> 00:29:16,559
one of these other types like where

00:29:15,840 --> 00:29:18,320
traditionally

00:29:16,559 --> 00:29:21,200
we have used a gauge or where you use

00:29:18,320 --> 00:29:23,520
now a value recorder or a value observer

00:29:21,200 --> 00:29:25,440
then there's there's no just there's no

00:29:23,520 --> 00:29:26,080
in difference in importance between a

00:29:25,440 --> 00:29:28,240
small number

00:29:26,080 --> 00:29:30,000
and a big number you have a zero latency

00:29:28,240 --> 00:29:31,760
that's a significant measurement

00:29:30,000 --> 00:29:33,200
it's no more or less significant than

00:29:31,760 --> 00:29:34,880
100 second latency

00:29:33,200 --> 00:29:36,640
so when you're talking about these

00:29:34,880 --> 00:29:37,440
individual values or individual

00:29:36,640 --> 00:29:38,960
measurements

00:29:37,440 --> 00:29:41,279
you want to think of that as a different

00:29:38,960 --> 00:29:42,559
semantic type than it is than say a

00:29:41,279 --> 00:29:44,480
an increment where you're just going to

00:29:42,559 --> 00:29:47,760
contribute to a sum because

00:29:44,480 --> 00:29:47,760
bigger increments matter more

00:29:48,720 --> 00:29:52,399
this is um a little we're sort of

00:29:50,799 --> 00:29:53,679
straining into the theory of measurement

00:29:52,399 --> 00:29:55,679
and and i should have put i could have

00:29:53,679 --> 00:29:58,799
put some slides in here about that but

00:29:55,679 --> 00:30:01,600
we do in sort of in statistics or in in

00:29:58,799 --> 00:30:03,440
math we talk about scale for the numbers

00:30:01,600 --> 00:30:06,000
so you can talk about ratio scale

00:30:03,440 --> 00:30:07,520
and interval scale and logarithm scale

00:30:06,000 --> 00:30:09,360
um

00:30:07,520 --> 00:30:11,120
you can answer the question in those

00:30:09,360 --> 00:30:14,559
terms as well but i'd rather do it

00:30:11,120 --> 00:30:14,559
with the way i just did

00:30:19,520 --> 00:30:22,880
so um i just discussed roughly what was

00:30:22,399 --> 00:30:25,600
on this

00:30:22,880 --> 00:30:26,399
slide is to say that we are interested

00:30:25,600 --> 00:30:28,399
in keeping

00:30:26,399 --> 00:30:29,440
the semantic type when we have when we

00:30:28,399 --> 00:30:31,120
have that information

00:30:29,440 --> 00:30:32,960
and this choice justin's question the

00:30:31,120 --> 00:30:35,919
choice between value recorder

00:30:32,960 --> 00:30:37,679
and or sorry value observer and and up

00:30:35,919 --> 00:30:39,360
down some observer

00:30:37,679 --> 00:30:40,799
though those they're very close but

00:30:39,360 --> 00:30:41,360
conceptually we want them to be

00:30:40,799 --> 00:30:43,600
different

00:30:41,360 --> 00:30:44,720
and i promise you we'll keep writing to

00:30:43,600 --> 00:30:47,039
try and clarify this

00:30:44,720 --> 00:30:47,039
point

00:30:47,919 --> 00:30:52,080
okay um well i finished my section on

00:30:51,279 --> 00:30:54,559
data model

00:30:52,080 --> 00:30:55,840
um so that's good we made it through and

00:30:54,559 --> 00:30:59,120
you asked the same question

00:30:55,840 --> 00:31:00,799
right as i was reaching my last slide um

00:30:59,120 --> 00:31:02,159
okay so for the rest of this talk we are

00:31:00,799 --> 00:31:03,600
going to go through some of the ways

00:31:02,159 --> 00:31:06,240
that you can configure

00:31:03,600 --> 00:31:07,519
export for your metrics data especially

00:31:06,240 --> 00:31:11,200
talking about how we control

00:31:07,519 --> 00:31:14,159
costs okay so um

00:31:11,200 --> 00:31:15,760
in not particularly great order here we

00:31:14,159 --> 00:31:17,519
have a few features that are sort of new

00:31:15,760 --> 00:31:19,279
and interesting for open telemetry

00:31:17,519 --> 00:31:20,640
one of the things that's been i think

00:31:19,279 --> 00:31:23,039
holding back the industry

00:31:20,640 --> 00:31:25,279
uh generally speaking is that we need

00:31:23,039 --> 00:31:27,519
variable balanced histograms or we need

00:31:25,279 --> 00:31:28,480
histograms that can support high

00:31:27,519 --> 00:31:30,399
resolution

00:31:28,480 --> 00:31:31,600
and be relatively compressed various

00:31:30,399 --> 00:31:33,360
ways of saying the same thing

00:31:31,600 --> 00:31:35,279
sometimes we call these sketches

00:31:33,360 --> 00:31:36,880
histograms by however you

00:31:35,279 --> 00:31:38,640
formulate them are approximate

00:31:36,880 --> 00:31:39,360
representations of a distribution and we

00:31:38,640 --> 00:31:42,240
want to get

00:31:39,360 --> 00:31:43,279
better at this so um i've named some

00:31:42,240 --> 00:31:45,360
algorithms that are

00:31:43,279 --> 00:31:47,039
that you that are used for this we are

00:31:45,360 --> 00:31:48,240
currently negotiating and working out

00:31:47,039 --> 00:31:51,679
some of the protocol

00:31:48,240 --> 00:31:53,679
decisions that we want here and we are

00:31:51,679 --> 00:31:56,799
looking to standardize on one of these

00:31:53,679 --> 00:31:59,919
algorithms as a sort of recommended

00:31:56,799 --> 00:32:00,640
approach so open geometry is going to

00:31:59,919 --> 00:32:02,960
offer you

00:32:00,640 --> 00:32:04,240
essentially better histograms than than

00:32:02,960 --> 00:32:05,039
you've been getting out of the box for

00:32:04,240 --> 00:32:06,880
some of the

00:32:05,039 --> 00:32:09,279
metric systems that you're interested

00:32:06,880 --> 00:32:14,559
that you've been using

00:32:09,279 --> 00:32:16,960
dd sketch is the leading contender

00:32:14,559 --> 00:32:18,960
um a lot of the discussion that we've

00:32:16,960 --> 00:32:21,679
had about temporality in the past

00:32:18,960 --> 00:32:22,880
15 minutes ultimately comes down to cost

00:32:21,679 --> 00:32:25,600
and cardinality

00:32:22,880 --> 00:32:26,640
so what we have a situation where we

00:32:25,600 --> 00:32:28,399
expect that users

00:32:26,640 --> 00:32:29,760
or developers are going to be writing

00:32:28,399 --> 00:32:30,799
instrumentation and putting metric

00:32:29,760 --> 00:32:33,039
labels together

00:32:30,799 --> 00:32:33,919
that are sort of that they may not

00:32:33,039 --> 00:32:35,760
actually know

00:32:33,919 --> 00:32:37,440
what are the useful labels or what the

00:32:35,760 --> 00:32:38,559
cost tolerances of the people running

00:32:37,440 --> 00:32:40,080
that code are going to be

00:32:38,559 --> 00:32:41,679
so that you may end up as a situation

00:32:40,080 --> 00:32:43,760
where you're generating more labels than

00:32:41,679 --> 00:32:46,000
you actually need

00:32:43,760 --> 00:32:47,360
so what do you do with that situation

00:32:46,000 --> 00:32:47,840
two things that we know how to do that

00:32:47,360 --> 00:32:50,000
we're

00:32:47,840 --> 00:32:52,080
including in part as as as part of the

00:32:50,000 --> 00:32:53,840
open telemetry metric system

00:32:52,080 --> 00:32:55,279
one is that we have the ability to do

00:32:53,840 --> 00:32:58,000
in-process aggregation

00:32:55,279 --> 00:33:00,159
so these these sdks are relatively

00:32:58,000 --> 00:33:02,080
sophisticated they include

00:33:00,159 --> 00:33:04,080
the ability to coalesce events that

00:33:02,080 --> 00:33:06,159
happen over a short interval of time

00:33:04,080 --> 00:33:07,840
and then if we want to do label label

00:33:06,159 --> 00:33:09,760
reduction we can actually erase those

00:33:07,840 --> 00:33:11,440
labels and just aggregate those values

00:33:09,760 --> 00:33:14,399
together so that we get fewer time

00:33:11,440 --> 00:33:16,320
series with more aggregation happening

00:33:14,399 --> 00:33:17,679
so this is a facility that the sdks

00:33:16,320 --> 00:33:18,559
include and it's something that we have

00:33:17,679 --> 00:33:19,760
to configure

00:33:18,559 --> 00:33:21,840
so later on we'll talk about how to

00:33:19,760 --> 00:33:24,559
configure that but for now the sdks

00:33:21,840 --> 00:33:26,159
include this mechanism the other thing

00:33:24,559 --> 00:33:27,200
that we can do to control cost is the

00:33:26,159 --> 00:33:28,640
reason why i've been talking about

00:33:27,200 --> 00:33:30,960
temporality so much

00:33:28,640 --> 00:33:32,080
so in a statsd system you may have been

00:33:30,960 --> 00:33:34,480
used to cardinality

00:33:32,080 --> 00:33:36,399
being okay because every at every flush

00:33:34,480 --> 00:33:37,440
interval you can completely forget what

00:33:36,399 --> 00:33:39,600
you've been reporting

00:33:37,440 --> 00:33:42,080
and just begin accumulating new state

00:33:39,600 --> 00:33:44,399
whereas in a prometheus system

00:33:42,080 --> 00:33:46,320
if you use high cardinality those labels

00:33:44,399 --> 00:33:48,240
are stuck in memory for a very long time

00:33:46,320 --> 00:33:50,080
so a stateless export pipeline is one

00:33:48,240 --> 00:33:52,480
where you force the use

00:33:50,080 --> 00:33:53,360
of delta export of delta aggregation

00:33:52,480 --> 00:33:55,440
temporality

00:33:53,360 --> 00:33:57,840
in order to allow yourself to flesh out

00:33:55,440 --> 00:33:57,840
memory

00:34:00,320 --> 00:34:05,440
so let's look at some diagrams this here

00:34:03,519 --> 00:34:08,320
is a diagram of a standalone

00:34:05,440 --> 00:34:10,240
sdk running open telemetry metrics so

00:34:08,320 --> 00:34:12,560
you've got your application code running

00:34:10,240 --> 00:34:14,079
you have a runtime metrics

00:34:12,560 --> 00:34:15,359
instrumentation package running that's

00:34:14,079 --> 00:34:18,159
telling you garbage question

00:34:15,359 --> 00:34:19,760
statistics maybe you have a host metrics

00:34:18,159 --> 00:34:21,919
instrumentation package that's running

00:34:19,760 --> 00:34:23,679
that may be telling you cpu usage and

00:34:21,919 --> 00:34:26,879
memory usage and so on

00:34:23,679 --> 00:34:28,240
you have the open telemetry api beneath

00:34:26,879 --> 00:34:30,159
that you have the sdk

00:34:28,240 --> 00:34:31,919
the sdk has a frontline component we

00:34:30,159 --> 00:34:34,800
call the accumulator which builds up

00:34:31,919 --> 00:34:37,040
short-term state so over one interval

00:34:34,800 --> 00:34:39,280
whether that's a second or a minute or

00:34:37,040 --> 00:34:40,960
ten minutes could be anything you build

00:34:39,280 --> 00:34:42,639
up a record of everything that's

00:34:40,960 --> 00:34:44,800
happened during that period of time

00:34:42,639 --> 00:34:46,800
and then every once in a while the sdk

00:34:44,800 --> 00:34:49,919
flushes its state

00:34:46,800 --> 00:34:52,639
i have this orange box here with a

00:34:49,919 --> 00:34:54,240
diagram saying here is where we do delta

00:34:52,639 --> 00:34:56,639
to cumulative conversion

00:34:54,240 --> 00:34:57,359
so this orange box represents a

00:34:56,639 --> 00:35:00,160
long-term

00:34:57,359 --> 00:35:02,000
memory commitment in order to implement

00:35:00,160 --> 00:35:03,760
cumulative export for your metrics

00:35:02,000 --> 00:35:05,040
you have to put memory somewhere and in

00:35:03,760 --> 00:35:06,960
this configuration

00:35:05,040 --> 00:35:09,040
this standalone configuration you just

00:35:06,960 --> 00:35:11,119
put that directly in your sdk

00:35:09,040 --> 00:35:12,720
so this is a configuration where you

00:35:11,119 --> 00:35:13,119
don't want to use a lot of cardinality

00:35:12,720 --> 00:35:14,720
because

00:35:13,119 --> 00:35:16,320
it's going to sit in memory for a long

00:35:14,720 --> 00:35:18,240
time but

00:35:16,320 --> 00:35:20,400
this is a this is a configuration that's

00:35:18,240 --> 00:35:22,320
very compatible with downstream systems

00:35:20,400 --> 00:35:24,560
you can blindly send this to an endpoint

00:35:22,320 --> 00:35:26,880
say a collector and it can export

00:35:24,560 --> 00:35:28,000
to prometheus because you put the data

00:35:26,880 --> 00:35:31,280
into cumulative form

00:35:28,000 --> 00:35:32,880
before you send it now

00:35:31,280 --> 00:35:34,480
the problem is you have to put a lot of

00:35:32,880 --> 00:35:35,839
memory in your application for this and

00:35:34,480 --> 00:35:36,720
you have to hold on to that memory for a

00:35:35,839 --> 00:35:38,000
long time

00:35:36,720 --> 00:35:40,000
and it prevents you from using high

00:35:38,000 --> 00:35:42,880
cardinality metrics

00:35:40,000 --> 00:35:44,400
so we have this other option to

00:35:42,880 --> 00:35:45,119
configure an export pipeline that is

00:35:44,400 --> 00:35:47,040
stateless

00:35:45,119 --> 00:35:48,480
and the way we do this is by making sure

00:35:47,040 --> 00:35:50,320
that the

00:35:48,480 --> 00:35:52,000
aggregation temporality matches

00:35:50,320 --> 00:35:53,839
instrument temporality if you're putting

00:35:52,000 --> 00:35:55,680
deltas in you give deltas out

00:35:53,839 --> 00:35:57,359
if you're putting cumulatives in you put

00:35:55,680 --> 00:35:58,560
cumulatives out and this way you have no

00:35:57,359 --> 00:36:00,400
memory requirements

00:35:58,560 --> 00:36:02,320
so in this diagram it's the same as the

00:36:00,400 --> 00:36:03,760
diagram before but now there's no

00:36:02,320 --> 00:36:05,520
long-term memory commitment and there's

00:36:03,760 --> 00:36:07,040
no orange box there's nothing happening

00:36:05,520 --> 00:36:08,400
in that box it's just passing straight

00:36:07,040 --> 00:36:10,560
through

00:36:08,400 --> 00:36:11,599
so this is going to cost you less and if

00:36:10,560 --> 00:36:14,720
you have an otlp

00:36:11,599 --> 00:36:16,880
endpoint that supports deltas natively

00:36:14,720 --> 00:36:17,920
this is actually a good configuration

00:36:16,880 --> 00:36:19,680
and um

00:36:17,920 --> 00:36:21,760
i want to say if anyone's listening here

00:36:19,680 --> 00:36:24,000
if you're the author of an open source

00:36:21,760 --> 00:36:26,160
backend for metrics data this is an

00:36:24,000 --> 00:36:27,680
opportunity a big opportunity to accept

00:36:26,160 --> 00:36:29,680
native otlp data

00:36:27,680 --> 00:36:32,560
because it will allow the clients to

00:36:29,680 --> 00:36:35,040
begin configuring themselves statelessly

00:36:32,560 --> 00:36:38,400
and this is going to be open the door to

00:36:35,040 --> 00:36:41,200
high cardinality metrics

00:36:38,400 --> 00:36:42,400
so that was a sort of complicated uh

00:36:41,200 --> 00:36:43,359
configuration here because it is

00:36:42,400 --> 00:36:44,560
standalone you've got

00:36:43,359 --> 00:36:46,000
several things running inside your

00:36:44,560 --> 00:36:46,960
process and all those things running

00:36:46,000 --> 00:36:49,520
inside your process

00:36:46,960 --> 00:36:51,119
add up to risk and cost so there's

00:36:49,520 --> 00:36:51,760
another way you can configure this and

00:36:51,119 --> 00:36:54,720
this is

00:36:51,760 --> 00:36:56,480
using the collector as an agent so i

00:36:54,720 --> 00:36:57,200
might want to export cumulative metrics

00:36:56,480 --> 00:36:59,200
because

00:36:57,200 --> 00:37:01,760
that makes the downstream system happy

00:36:59,200 --> 00:37:03,520
so i can for example write to prometheus

00:37:01,760 --> 00:37:05,280
but somewhere in my pipeline i have to

00:37:03,520 --> 00:37:07,280
configure this point where i convert

00:37:05,280 --> 00:37:08,480
deltas to cumulatives which leads to a

00:37:07,280 --> 00:37:10,720
long-term memory

00:37:08,480 --> 00:37:11,520
requirement so in this configuration i

00:37:10,720 --> 00:37:14,000
have an

00:37:11,520 --> 00:37:15,280
application running inside of a host but

00:37:14,000 --> 00:37:18,240
it's it is a stateless

00:37:15,280 --> 00:37:20,079
exporter so it is it is outputting otlp

00:37:18,240 --> 00:37:23,200
with aggregation temporality

00:37:20,079 --> 00:37:26,000
to require no memory the hotel collector

00:37:23,200 --> 00:37:28,160
is running on the same host it is able

00:37:26,000 --> 00:37:28,800
to implement that delta to cumulative

00:37:28,160 --> 00:37:31,760
conversion

00:37:28,800 --> 00:37:32,720
and then you can output otlp converted

00:37:31,760 --> 00:37:34,880
to cumulative

00:37:32,720 --> 00:37:36,079
from your host but your application is

00:37:34,880 --> 00:37:39,200
still running statelessly

00:37:36,079 --> 00:37:41,520
so this is a way that we can reduce risk

00:37:39,200 --> 00:37:43,359
on the application itself if you were

00:37:41,520 --> 00:37:44,079
accidentally to put high cardinality

00:37:43,359 --> 00:37:46,400
metrics

00:37:44,079 --> 00:37:47,200
in this configuration it'll crash the

00:37:46,400 --> 00:37:49,920
collector

00:37:47,200 --> 00:37:51,520
not your application actually the

00:37:49,920 --> 00:37:53,760
collector is probably written to handle

00:37:51,520 --> 00:37:56,880
that better than your application is

00:37:53,760 --> 00:37:58,160
so so that's one configuration of course

00:37:56,880 --> 00:37:59,440
there is still a long term state

00:37:58,160 --> 00:38:01,359
requirement here we've just

00:37:59,440 --> 00:38:03,440
moved it from the application to the

00:38:01,359 --> 00:38:05,920
collector

00:38:03,440 --> 00:38:07,680
so there's another configuration this is

00:38:05,920 --> 00:38:09,599
where we talk about just a standalone

00:38:07,680 --> 00:38:11,200
prometheus exporter suppose you have an

00:38:09,599 --> 00:38:11,920
existing prometheus setup you just want

00:38:11,200 --> 00:38:14,480
to add one

00:38:11,920 --> 00:38:16,320
new new target so in this situation it's

00:38:14,480 --> 00:38:17,520
roughly the same as that configuration

00:38:16,320 --> 00:38:20,160
that i showed you before

00:38:17,520 --> 00:38:22,160
i'm running my runtime metrics plugin

00:38:20,160 --> 00:38:24,000
tells me gc stats i'm running my host

00:38:22,160 --> 00:38:26,160
metrics which means i don't need to run

00:38:24,000 --> 00:38:26,560
some other prometheus tool to export

00:38:26,160 --> 00:38:30,079
node

00:38:26,560 --> 00:38:31,839
statistics um i've got my api sdk

00:38:30,079 --> 00:38:34,079
i'm still doing that delta cumulative

00:38:31,839 --> 00:38:35,839
conversion because i must for prometheus

00:38:34,079 --> 00:38:37,680
and then i can either pull that data

00:38:35,839 --> 00:38:38,160
using openmetrics or i can push that

00:38:37,680 --> 00:38:40,640
data

00:38:38,160 --> 00:38:42,000
using a prometheus remote write exporter

00:38:40,640 --> 00:38:44,000
in both of these cases

00:38:42,000 --> 00:38:45,760
i'm writing to prometheus in both these

00:38:44,000 --> 00:38:46,400
cases i have to do a delta to cumulative

00:38:45,760 --> 00:38:49,680
conversion

00:38:46,400 --> 00:38:51,440
because again prometheus requires it

00:38:49,680 --> 00:38:53,440
but i could also do that statelessly

00:38:51,440 --> 00:38:54,240
again same almost the same diagram as

00:38:53,440 --> 00:38:56,320
before

00:38:54,240 --> 00:38:57,680
i've got a stateless application i'm

00:38:56,320 --> 00:38:58,720
sending through a stateless export

00:38:57,680 --> 00:39:00,240
pipeline

00:38:58,720 --> 00:39:02,720
i'm receiving that at the collector

00:39:00,240 --> 00:39:05,520
agent i then can do delta to cumulative

00:39:02,720 --> 00:39:07,280
conversion but now i'm going to use the

00:39:05,520 --> 00:39:09,839
prometheus remote right

00:39:07,280 --> 00:39:11,040
because there's some impedance mismatch

00:39:09,839 --> 00:39:12,960
using open telemetry

00:39:11,040 --> 00:39:14,240
using openmetrics to pull from a

00:39:12,960 --> 00:39:15,920
collector we're not going to do that

00:39:14,240 --> 00:39:18,720
right away

00:39:15,920 --> 00:39:20,079
um so this is a way you can export to

00:39:18,720 --> 00:39:21,680
prometheus through a stateless

00:39:20,079 --> 00:39:24,320
in-process exporter and a

00:39:21,680 --> 00:39:25,440
local agent that's not the only way you

00:39:24,320 --> 00:39:27,839
can configure

00:39:25,440 --> 00:39:29,280
export for open telemetry metrics i want

00:39:27,839 --> 00:39:30,640
to do a little review on

00:39:29,280 --> 00:39:33,040
some of the stuff that we just talked

00:39:30,640 --> 00:39:35,359
about there's sort of two

00:39:33,040 --> 00:39:36,480
uh ways that you might to two ends of

00:39:35,359 --> 00:39:38,560
the spectrum that you might configure

00:39:36,480 --> 00:39:40,400
your metrics export

00:39:38,560 --> 00:39:42,720
you can do either push and pull or you

00:39:40,400 --> 00:39:44,720
can do stateless or cumulative

00:39:42,720 --> 00:39:46,480
but we generally pair the pull model

00:39:44,720 --> 00:39:48,160
with cumulative which is the prometheus

00:39:46,480 --> 00:39:49,280
model and that's the default for open

00:39:48,160 --> 00:39:51,440
telemetry

00:39:49,280 --> 00:39:53,280
and we all um and and that has

00:39:51,440 --> 00:39:54,480
properties are good for prometheus

00:39:53,280 --> 00:39:56,400
you're automatically going to work with

00:39:54,480 --> 00:39:59,280
prometheus even if you have a collector

00:39:56,400 --> 00:39:59,920
in the loop you get easier reliability

00:39:59,280 --> 00:40:02,400
because

00:39:59,920 --> 00:40:03,680
when you drop a record or a data point

00:40:02,400 --> 00:40:05,920
in prometheus

00:40:03,680 --> 00:40:07,520
when you drop a cumulative data point it

00:40:05,920 --> 00:40:08,160
gets smoothed out by the next data point

00:40:07,520 --> 00:40:11,359
that arrives

00:40:08,160 --> 00:40:12,960
so loss loss of data is pretty okay in a

00:40:11,359 --> 00:40:15,200
prometheus system

00:40:12,960 --> 00:40:16,880
um but high cardinality is pretty much

00:40:15,200 --> 00:40:18,720
not allowed and it will blow you up if

00:40:16,880 --> 00:40:22,079
you do it by accident

00:40:18,720 --> 00:40:24,319
so in this other side here we've got

00:40:22,079 --> 00:40:25,119
this premie this this push model and

00:40:24,319 --> 00:40:27,440
this stateless

00:40:25,119 --> 00:40:29,119
export model so the one thing that's

00:40:27,440 --> 00:40:29,599
going for it is it's exactly the same as

00:40:29,119 --> 00:40:31,200
we're doing

00:40:29,599 --> 00:40:32,960
already for traces if you're going to

00:40:31,200 --> 00:40:33,680
set up an export pipeline for your span

00:40:32,960 --> 00:40:35,680
data

00:40:33,680 --> 00:40:37,680
you might want to set up the same exact

00:40:35,680 --> 00:40:39,520
topology for your metrics data

00:40:37,680 --> 00:40:41,200
so the idea that you're going to attach

00:40:39,520 --> 00:40:42,240
resources as you collect data through

00:40:41,200 --> 00:40:44,240
your infrastructure

00:40:42,240 --> 00:40:47,119
it fits very well it matches trace

00:40:44,240 --> 00:40:49,119
collection however

00:40:47,119 --> 00:40:50,400
it means you're using delta delta

00:40:49,119 --> 00:40:51,760
aggregation temporality

00:40:50,400 --> 00:40:53,520
it means that you have to be a little

00:40:51,760 --> 00:40:54,560
bit you have to work a lot harder for

00:40:53,520 --> 00:40:57,440
reliability

00:40:54,560 --> 00:40:59,599
so you have to avoid drop packets means

00:40:57,440 --> 00:41:01,119
loss of data and replayed packets means

00:40:59,599 --> 00:41:02,880
double counted data so

00:41:01,119 --> 00:41:06,400
we have to be careful to avoid dropping

00:41:02,880 --> 00:41:09,200
data but we also have to avoid replay

00:41:06,400 --> 00:41:10,160
but the upside of this configuration is

00:41:09,200 --> 00:41:12,480
that

00:41:10,160 --> 00:41:14,240
that high cardinality is no longer a

00:41:12,480 --> 00:41:16,079
cost inside the process and high

00:41:14,240 --> 00:41:17,040
cardinality if the downstream system

00:41:16,079 --> 00:41:18,800
supports it

00:41:17,040 --> 00:41:20,960
is going to be okay and actually could

00:41:18,800 --> 00:41:22,720
be good for the user

00:41:20,960 --> 00:41:24,319
i didn't cover this com this sort of

00:41:22,720 --> 00:41:25,920
worst and best of both cases which would

00:41:24,319 --> 00:41:28,319
be a push cumulative model

00:41:25,920 --> 00:41:29,680
that is definitely a valid configuration

00:41:28,319 --> 00:41:31,200
it's exactly what you want if you're

00:41:29,680 --> 00:41:34,480
going to send prometheus data

00:41:31,200 --> 00:41:37,599
through a collector so

00:41:34,480 --> 00:41:39,520
um these are choices um currently

00:41:37,599 --> 00:41:41,520
we're we're the default for open

00:41:39,520 --> 00:41:43,440
telemetry is cumulative because it's the

00:41:41,520 --> 00:41:45,359
most compatible but it does mean that

00:41:43,440 --> 00:41:47,200
basically we're giving you prometheus

00:41:45,359 --> 00:41:49,440
status quo you're gonna be

00:41:47,200 --> 00:41:51,760
keeping memory for your cardinality

00:41:49,440 --> 00:41:53,040
until we reconfigure your system to use

00:41:51,760 --> 00:41:56,800
this push stateless

00:41:53,040 --> 00:41:58,880
export strategy hey josh

00:41:56,800 --> 00:42:00,160
yeah question came in in our chat about

00:41:58,880 --> 00:42:02,000
what things exactly do we need to

00:42:00,160 --> 00:42:05,119
configure to get to that stateless

00:42:02,000 --> 00:42:07,280
model you're describing um

00:42:05,119 --> 00:42:08,960
good question um so we need some more

00:42:07,280 --> 00:42:12,960
collector development there is

00:42:08,960 --> 00:42:17,119
not an actual accumulated cumulative

00:42:12,960 --> 00:42:20,319
processing stage today it's imaginary

00:42:17,119 --> 00:42:22,079
um the reference implementation for the

00:42:20,319 --> 00:42:23,680
otel metrics sdk which is the one that

00:42:22,079 --> 00:42:25,839
i've been developing in go

00:42:23,680 --> 00:42:27,920
does have this ability to select the

00:42:25,839 --> 00:42:28,720
stateless versus cumulative mode today

00:42:27,920 --> 00:42:31,040
so

00:42:28,720 --> 00:42:32,800
the the sdk support has been specced out

00:42:31,040 --> 00:42:33,920
the reference implementation does exist

00:42:32,800 --> 00:42:37,520
it works

00:42:33,920 --> 00:42:38,480
but as far as um a collector support

00:42:37,520 --> 00:42:40,400
it's not there yet

00:42:38,480 --> 00:42:42,079
if you had a vendor that was supporting

00:42:40,400 --> 00:42:44,560
both forms of otlp

00:42:42,079 --> 00:42:47,760
today you could get that from at least

00:42:44,560 --> 00:42:47,760
from the reference implementation

00:42:50,000 --> 00:42:53,760
hope that answers the question um i'm

00:42:52,319 --> 00:42:55,760
very excited about this because

00:42:53,760 --> 00:42:57,119
as a user from a statistic background i

00:42:55,760 --> 00:42:58,960
was used to having a little bit more

00:42:57,119 --> 00:43:00,160
cardinality than i'm able to get from a

00:42:58,960 --> 00:43:02,560
prometheus

00:43:00,160 --> 00:43:04,160
configuration so i would like to push

00:43:02,560 --> 00:43:06,160
more hype more cardinality

00:43:04,160 --> 00:43:07,440
on the metrics world until it's a real

00:43:06,160 --> 00:43:08,880
problem and i'm hoping i think there's a

00:43:07,440 --> 00:43:10,880
good opportunity for

00:43:08,880 --> 00:43:12,880
data scientists to come help us with

00:43:10,880 --> 00:43:14,640
actual cardinality control mechanisms

00:43:12,880 --> 00:43:16,160
like we can down sample to control

00:43:14,640 --> 00:43:20,079
cardinality

00:43:16,160 --> 00:43:21,839
for example i think that's actually a

00:43:20,079 --> 00:43:24,400
really exciting topic but i won't

00:43:21,839 --> 00:43:24,400
say no more

00:43:25,040 --> 00:43:28,319
um i think we're almost at the end here

00:43:27,119 --> 00:43:29,920
of my slide deck

00:43:28,319 --> 00:43:31,359
um i have a few more diagrams just to

00:43:29,920 --> 00:43:32,720
kind of give you a greater picture of

00:43:31,359 --> 00:43:33,920
all the ways you might configure open

00:43:32,720 --> 00:43:35,599
telemetry metrics

00:43:33,920 --> 00:43:37,359
um this is an example of a sort of

00:43:35,599 --> 00:43:38,160
kubernetes deployment where you've got a

00:43:37,359 --> 00:43:40,000
node

00:43:38,160 --> 00:43:41,680
you've got a daemon set collector

00:43:40,000 --> 00:43:44,240
running on every node

00:43:41,680 --> 00:43:45,119
you've configured your receivers for

00:43:44,240 --> 00:43:48,000
openmetrics

00:43:45,119 --> 00:43:49,760
for statsd for otlp you've got the

00:43:48,000 --> 00:43:52,319
kubernetes receiver built in so you're

00:43:49,760 --> 00:43:54,240
getting the kubernetes state metrics

00:43:52,319 --> 00:43:55,680
you've got a host metrics running on the

00:43:54,240 --> 00:43:57,599
collector so you don't have to run those

00:43:55,680 --> 00:43:59,599
host metrics on every machine

00:43:57,599 --> 00:44:01,680
running on the pod or on the node for

00:43:59,599 --> 00:44:04,160
example so these are just a number of

00:44:01,680 --> 00:44:05,440
sort of this is the scale of these

00:44:04,160 --> 00:44:06,480
installations is now good we're going to

00:44:05,440 --> 00:44:08,560
be able to

00:44:06,480 --> 00:44:10,160
to talk about one collector per node and

00:44:08,560 --> 00:44:11,680
a bunch of different targets per node

00:44:10,160 --> 00:44:13,280
and so on

00:44:11,680 --> 00:44:16,560
and this is just part of this the plan

00:44:13,280 --> 00:44:16,560
for open telemetry collector

00:44:16,720 --> 00:44:21,520
um definitely part of that resource

00:44:20,160 --> 00:44:22,480
model is that we're going to implement

00:44:21,520 --> 00:44:24,000
hierarchical

00:44:22,480 --> 00:44:25,280
collections so that you have a node

00:44:24,000 --> 00:44:25,920
collector that's collecting all the

00:44:25,280 --> 00:44:27,440
resources

00:44:25,920 --> 00:44:29,040
all the metrics locally and it's going

00:44:27,440 --> 00:44:30,160
to pass that to a regional collector

00:44:29,040 --> 00:44:30,880
that's going to attach all of your

00:44:30,160 --> 00:44:33,040
regional

00:44:30,880 --> 00:44:35,520
resource attributes and it might pass to

00:44:33,040 --> 00:44:37,680
a global or other levels of hierarchy

00:44:35,520 --> 00:44:39,839
that you can use to organize your

00:44:37,680 --> 00:44:41,200
metrics data

00:44:39,839 --> 00:44:43,680
this is something that you might be able

00:44:41,200 --> 00:44:44,079
to to bypass certain prometheus features

00:44:43,680 --> 00:44:45,440
with

00:44:44,079 --> 00:44:46,960
if you're using prometheus you might be

00:44:45,440 --> 00:44:49,280
using recording rules to get some more

00:44:46,960 --> 00:44:51,200
functionality but we can just talk about

00:44:49,280 --> 00:44:52,720
an expert pipeline that aggregates

00:44:51,200 --> 00:44:53,359
everything together into one place

00:44:52,720 --> 00:44:55,359
without

00:44:53,359 --> 00:44:56,720
that type of recording rule

00:44:55,359 --> 00:44:59,200
functionality

00:44:56,720 --> 00:44:59,839
which is like a right time aggregation

00:44:59,200 --> 00:45:02,319
all right

00:44:59,839 --> 00:45:02,880
there's one more i saved it for last

00:45:02,319 --> 00:45:04,000
i've

00:45:02,880 --> 00:45:05,440
discovered that there are a lot of

00:45:04,000 --> 00:45:06,960
prometheus users out there who have

00:45:05,440 --> 00:45:09,040
invested a tremendous amount

00:45:06,960 --> 00:45:10,640
in their configuration so prometheus

00:45:09,040 --> 00:45:12,000
does a lot of things and it's going to

00:45:10,640 --> 00:45:15,280
be difficult to replace

00:45:12,000 --> 00:45:16,319
all of that functionality so we were

00:45:15,280 --> 00:45:18,800
looking for ways to help

00:45:16,319 --> 00:45:22,240
prometheus users get on board with open

00:45:18,800 --> 00:45:24,400
telemetry and begin using otlp

00:45:22,240 --> 00:45:25,839
so i found this thing um that the

00:45:24,400 --> 00:45:27,920
stackdriver

00:45:25,839 --> 00:45:30,720
group had done a couple years ago it is

00:45:27,920 --> 00:45:33,200
a stackdriver prometheus sidecar

00:45:30,720 --> 00:45:33,760
i haven't talked about this much but

00:45:33,200 --> 00:45:35,920
today

00:45:33,760 --> 00:45:37,760
uh it's open source now um we opened up

00:45:35,920 --> 00:45:39,760
the public the repository publicly

00:45:37,760 --> 00:45:41,680
this is a sidecar for prometheus

00:45:39,760 --> 00:45:43,839
basically lets you read prometheus data

00:45:41,680 --> 00:45:46,079
and send otlp

00:45:43,839 --> 00:45:47,599
um this i see as a short-term solution

00:45:46,079 --> 00:45:48,640
because ultimately the prometheus

00:45:47,599 --> 00:45:50,720
project will add

00:45:48,640 --> 00:45:52,880
metadata to its remote right protocol

00:45:50,720 --> 00:45:56,160
and then it will be able to

00:45:52,880 --> 00:45:58,000
um replace this sidecar so today you

00:45:56,160 --> 00:46:00,079
cannot send prometheus data directly out

00:45:58,000 --> 00:46:00,560
of prometheus into otlp without a tool

00:46:00,079 --> 00:46:02,319
like this

00:46:00,560 --> 00:46:05,119
but hopefully in the future that

00:46:02,319 --> 00:46:08,319
restriction will go away

00:46:05,119 --> 00:46:10,000
i put a few links to this this code base

00:46:08,319 --> 00:46:11,440
as well as a couple of the prometheus

00:46:10,000 --> 00:46:12,800
issues that are currently trying to

00:46:11,440 --> 00:46:14,800
address this shortcoming

00:46:12,800 --> 00:46:15,839
so that hopefully we can retire this

00:46:14,800 --> 00:46:18,079
code in the future

00:46:15,839 --> 00:46:19,839
but meanwhile prometheus users should be

00:46:18,079 --> 00:46:22,319
able to try their open telemetry

00:46:19,839 --> 00:46:23,280
metrics system and this should also help

00:46:22,319 --> 00:46:25,440
us migrate

00:46:23,280 --> 00:46:28,000
from prometheus onto open telemetry in a

00:46:25,440 --> 00:46:29,599
gradual way

00:46:28,000 --> 00:46:31,200
now i think i've actually reached the

00:46:29,599 --> 00:46:42,480
end of the talk and i'd love to have

00:46:31,200 --> 00:46:45,040
questions and discussion from the group

00:46:42,480 --> 00:46:47,359
josh are you expecting questions on this

00:46:45,040 --> 00:46:49,760
on the zoom chat or we can ask them just

00:46:47,359 --> 00:46:51,280
like uh since i've been presenting i

00:46:49,760 --> 00:46:53,200
wasn't reading zoom chat but i'm gonna

00:46:51,280 --> 00:46:56,720
unpresent

00:46:53,200 --> 00:46:58,720
um and and then we can all talk

00:46:56,720 --> 00:47:00,560
and i'll read the chat have a quick one

00:46:58,720 --> 00:47:02,640
please so this morning we were talking

00:47:00,560 --> 00:47:03,920
about traces and context and context

00:47:02,640 --> 00:47:05,760
propagation

00:47:03,920 --> 00:47:07,119
and this this afternoon we are talking

00:47:05,760 --> 00:47:10,160
about metrics

00:47:07,119 --> 00:47:13,280
and labels and tags so do we

00:47:10,160 --> 00:47:16,000
lose the context of traces when we

00:47:13,280 --> 00:47:17,040
handle open telemetry matrix or is this

00:47:16,000 --> 00:47:20,640
is there a way

00:47:17,040 --> 00:47:21,359
when we can link or correlate the traces

00:47:20,640 --> 00:47:24,800
context

00:47:21,359 --> 00:47:26,800
with the metrics labels

00:47:24,800 --> 00:47:28,960
right um first i always want to point

00:47:26,800 --> 00:47:30,960
out there's been a bit of terminology

00:47:28,960 --> 00:47:32,240
um debate and we we haven't actually

00:47:30,960 --> 00:47:35,040
settled it so the term

00:47:32,240 --> 00:47:36,480
attribute the term label and term tag

00:47:35,040 --> 00:47:38,160
are almost synonymous

00:47:36,480 --> 00:47:39,599
and used interchangeably here i hope

00:47:38,160 --> 00:47:41,119
that's not the question you're actually

00:47:39,599 --> 00:47:42,559
asking

00:47:41,119 --> 00:47:44,400
i believe you're asking about how we

00:47:42,559 --> 00:47:47,520
talk about distributed context

00:47:44,400 --> 00:47:50,559
and getting

00:47:47,520 --> 00:47:53,280
those attributes that come from say

00:47:50,559 --> 00:47:54,319
distributed actors in your system onto

00:47:53,280 --> 00:47:55,760
your metrics

00:47:54,319 --> 00:47:57,119
this is something that i mentioned in

00:47:55,760 --> 00:47:57,680
one of the breakout sessions earlier

00:47:57,119 --> 00:47:59,920
today

00:47:57,680 --> 00:48:01,040
was the sort of driving i think a

00:47:59,920 --> 00:48:03,359
driving

00:48:01,040 --> 00:48:04,839
motivator for the census design and part

00:48:03,359 --> 00:48:07,440
of the sort of

00:48:04,839 --> 00:48:07,920
the the heritage of open telemetry at

00:48:07,440 --> 00:48:10,720
this point

00:48:07,920 --> 00:48:12,240
is that that we were a bit based off of

00:48:10,720 --> 00:48:15,119
open census metrics

00:48:12,240 --> 00:48:17,680
so openstack just metrics did include a

00:48:15,119 --> 00:48:17,680
way to

00:48:18,839 --> 00:48:24,000
um

00:48:20,960 --> 00:48:28,160
open senses did include a way

00:48:24,000 --> 00:48:29,920
to um get your distributed context

00:48:28,160 --> 00:48:31,359
attributes into your metrics it's

00:48:29,920 --> 00:48:32,880
something that you have to configure

00:48:31,359 --> 00:48:35,119
we're talking about the terminology for

00:48:32,880 --> 00:48:37,680
this we've we've got something called an

00:48:35,119 --> 00:48:39,119
an enhancer at this point which is a a

00:48:37,680 --> 00:48:41,440
hook that you can use

00:48:39,119 --> 00:48:43,119
to pull attributes out of your context

00:48:41,440 --> 00:48:44,319
and put them into your metric system i

00:48:43,119 --> 00:48:44,880
hope that answers the question we're

00:48:44,319 --> 00:48:47,599
still

00:48:44,880 --> 00:48:49,520
talking terminology okay i think maybe

00:48:47,599 --> 00:48:51,920
there are a few pieces there

00:48:49,520 --> 00:48:53,119
one of them is that we can pull uh key

00:48:51,920 --> 00:48:54,800
values from

00:48:53,119 --> 00:48:56,960
your baggage from that correlation

00:48:54,800 --> 00:48:58,400
context and put them onto metrics

00:48:56,960 --> 00:49:00,000
another one i think is just built into

00:48:58,400 --> 00:49:01,599
the semantic conventions of our metrics

00:49:00,000 --> 00:49:03,119
we are trying to design the semantic

00:49:01,599 --> 00:49:04,720
conventions

00:49:03,119 --> 00:49:06,319
so that you can make correlations

00:49:04,720 --> 00:49:08,800
between those metrics and the traces

00:49:06,319 --> 00:49:11,119
that might be representing the same

00:49:08,800 --> 00:49:12,800
timed operations and then the final

00:49:11,119 --> 00:49:14,559
thing that is maybe the most exciting in

00:49:12,800 --> 00:49:16,800
my opinion is the concept of

00:49:14,559 --> 00:49:18,400
exemplars that go on metrics and what

00:49:16,800 --> 00:49:22,160
this is is like each

00:49:18,400 --> 00:49:23,760
metric will include just like a list of

00:49:22,160 --> 00:49:29,200
a few data points a few

00:49:23,760 --> 00:49:30,640
spans that are examples of that metric

00:49:29,200 --> 00:49:32,079
which then there's like a very strong

00:49:30,640 --> 00:49:32,400
correlation between those fans and then

00:49:32,079 --> 00:49:35,359
those

00:49:32,400 --> 00:49:35,359
that specific metric

00:49:36,079 --> 00:49:41,200
yeah so to make that clear we have ways

00:49:39,520 --> 00:49:43,520
at least planned out or specked out in

00:49:41,200 --> 00:49:45,680
protocol to get data from your

00:49:43,520 --> 00:49:47,440
contacts including both those attributes

00:49:45,680 --> 00:49:48,000
as well as your spam id and your trace

00:49:47,440 --> 00:49:52,079
id

00:49:48,000 --> 00:49:52,079
and to get that associated with metrics

00:49:52,480 --> 00:49:56,000
fantastic cool thank you very much

00:49:56,640 --> 00:50:01,440
um so we were planning to take a

00:49:59,520 --> 00:50:04,720
five-minute break before the next

00:50:01,440 --> 00:50:08,240
session um but if there are any other

00:50:04,720 --> 00:50:11,280
um questions you can take them into

00:50:08,240 --> 00:50:14,319
the hotel community slack channel

00:50:11,280 --> 00:50:16,000
um thank you so much josh justin

00:50:14,319 --> 00:50:17,359
for for presenting and sharing your

00:50:16,000 --> 00:50:19,599
expertise um

00:50:17,359 --> 00:50:21,359
i i shared my notes in the chat i'll

00:50:19,599 --> 00:50:22,160
share them in in the slack channel as

00:50:21,359 --> 00:50:25,359
well um

00:50:22,160 --> 00:50:27,599
i got a lot out of this

00:50:25,359 --> 00:50:29,040
thank you shelby um i'd be happy to stay

00:50:27,599 --> 00:50:30,319
on for a little bit i do see the

00:50:29,040 --> 00:50:31,839
questions now when i was presenting i

00:50:30,319 --> 00:50:32,800
couldn't see it i'm not sure how i could

00:50:31,839 --> 00:50:34,720
have fixed that but

00:50:32,800 --> 00:50:37,520
there's one here for example i can see

00:50:34,720 --> 00:50:40,160
about um motel policy on discovery

00:50:37,520 --> 00:50:42,079
when using open metrics approach that is

00:50:40,160 --> 00:50:43,440
really one of the greatest questions and

00:50:42,079 --> 00:50:43,839
it's not one that we're really answering

00:50:43,440 --> 00:50:46,240
here

00:50:43,839 --> 00:50:48,319
openmetrics or sorry the prometheus

00:50:46,240 --> 00:50:50,079
system has a great big piece of code

00:50:48,319 --> 00:50:51,920
that does service discovery

00:50:50,079 --> 00:50:54,240
and right now there's an ongoing

00:50:51,920 --> 00:50:57,440
discussion about how to

00:50:54,240 --> 00:50:58,960
either emulate or simplify or replace

00:50:57,440 --> 00:51:00,000
such functionality in the hotel

00:50:58,960 --> 00:51:01,680
collector world

00:51:00,000 --> 00:51:03,599
and the current state of affairs is

00:51:01,680 --> 00:51:06,160
actually not so good we've linked in a

00:51:03,599 --> 00:51:07,760
huge dependency on the prometheus system

00:51:06,160 --> 00:51:09,520
and are actually using that service

00:51:07,760 --> 00:51:11,520
discovery configuration

00:51:09,520 --> 00:51:13,200
which has caused some friction and some

00:51:11,520 --> 00:51:16,559
dependency bloat

00:51:13,200 --> 00:51:17,520
um we are working on that bowden would

00:51:16,559 --> 00:51:22,640
you like to speak

00:51:17,520 --> 00:51:27,119
anybody want to speak on that

00:51:22,640 --> 00:51:27,119
something that needs to happen yeah

00:51:28,800 --> 00:51:34,079
um i i actually have uh a dream

00:51:32,000 --> 00:51:36,400
here that somebody will take the

00:51:34,079 --> 00:51:38,079
prometheus service discovery code

00:51:36,400 --> 00:51:39,680
factor it out of prometheus and create a

00:51:38,079 --> 00:51:41,200
first class service called service

00:51:39,680 --> 00:51:42,800
discovery service

00:51:41,200 --> 00:51:44,160
and then we can have an hotel collector

00:51:42,800 --> 00:51:45,760
plugin which will reach out to the

00:51:44,160 --> 00:51:47,440
service discovery service

00:51:45,760 --> 00:51:49,359
with some sort of sharding information

00:51:47,440 --> 00:51:50,000
and get a list of sharded targets for it

00:51:49,359 --> 00:51:52,960
to scrape so

00:51:50,000 --> 00:51:55,040
that we can then rather than linking in

00:51:52,960 --> 00:51:56,880
a huge dependency on prometheus just

00:51:55,040 --> 00:51:58,559
actually call out to a service that does

00:51:56,880 --> 00:52:00,079
the same stuff

00:51:58,559 --> 00:52:03,200
and then the prometheus service

00:52:00,079 --> 00:52:06,240
discovery will become its own thing

00:52:03,200 --> 00:52:08,720
awesome answer thank you

00:52:06,240 --> 00:52:10,079
also uh it was mentioned on the slack

00:52:08,720 --> 00:52:12,079
that

00:52:10,079 --> 00:52:13,520
this is a break of five minutes before

00:52:12,079 --> 00:52:16,319
the next session so

00:52:13,520 --> 00:52:17,200
everyone who is willing to chat more

00:52:16,319 --> 00:52:21,599
feel free to

00:52:17,200 --> 00:52:21,599
but otherwise it's considered a break

00:52:22,480 --> 00:52:26,640
service discovery service discovery

00:52:24,240 --> 00:52:28,400
service oh geez

00:52:26,640 --> 00:52:33,200
all right everyone thank you i'll be in

00:52:28,400 --> 00:52:33,200

YouTube URL: https://www.youtube.com/watch?v=L-Ss8PtWlRA


