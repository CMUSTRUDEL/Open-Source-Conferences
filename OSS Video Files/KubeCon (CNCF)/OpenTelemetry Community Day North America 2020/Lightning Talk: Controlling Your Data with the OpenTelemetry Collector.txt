Title: Lightning Talk: Controlling Your Data with the OpenTelemetry Collector
Publication date: 2020-11-24
Playlist: OpenTelemetry Community Day North America 2020
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon Europe 2021 Virtual from May 4–7, 2021. Learn more at https://kubecon.io. The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects. 

Lightning Talk: Controlling Your Data with the OpenTelemetry Collector

How do people change telemetry data to support use-cases including PII redaction, downsampling, custom tags and fixing instrumentation flaws? The OpenTelemetry Collector! Telemetry data contains a lot of valuable information, but sometimes that data may or may not contain what people want. Modifying telemetry data by updating instrumentation often proves challenging due in part to application impact. The OpenTelemetry project solves this problem by providing a pluggable architecture in the OpenTelemetry Collector. Learn how others have configured it to support a variety of different use-cases.  PII redaction Downsampling Custom tags Renaming spans Fixing instrumentation flaws"
Captions: 
	00:00:00,080 --> 00:00:03,840
hi my name is steve flanders and i'm a

00:00:02,240 --> 00:00:05,920
director of engineering at splunk

00:00:03,840 --> 00:00:06,960
responsible for observability getting

00:00:05,920 --> 00:00:08,720
data in

00:00:06,960 --> 00:00:10,160
my team and i work extensively on the

00:00:08,720 --> 00:00:11,840
open telemetry project

00:00:10,160 --> 00:00:15,280
and i'm excited to join you today to

00:00:11,840 --> 00:00:16,720
talk about the open telemetry collector

00:00:15,280 --> 00:00:18,160
for those of you not familiar with the

00:00:16,720 --> 00:00:18,960
collector i'll start with a quick

00:00:18,160 --> 00:00:21,039
introduction

00:00:18,960 --> 00:00:22,000
before showing how you can use it to

00:00:21,039 --> 00:00:25,680
really control

00:00:22,000 --> 00:00:28,240
all your data so what is the open

00:00:25,680 --> 00:00:29,840
telemetry collector

00:00:28,240 --> 00:00:31,599
the collector is a vendor agnostic

00:00:29,840 --> 00:00:32,239
implementation that really allows you to

00:00:31,599 --> 00:00:35,040
receive

00:00:32,239 --> 00:00:36,719
process and export any telemetry data in

00:00:35,040 --> 00:00:38,399
a seamless way

00:00:36,719 --> 00:00:40,399
it provides a single binary that can be

00:00:38,399 --> 00:00:43,040
deployed in a variety of different ways

00:00:40,399 --> 00:00:45,280
including as an agent on your hosts or

00:00:43,040 --> 00:00:47,600
as a stand-alone gateway

00:00:45,280 --> 00:00:50,480
and it's the default destination for the

00:00:47,600 --> 00:00:52,160
open telemetry client libraries

00:00:50,480 --> 00:00:53,280
let me walk you through how it looks

00:00:52,160 --> 00:00:54,559
from a reference architecture

00:00:53,280 --> 00:00:56,480
perspective

00:00:54,559 --> 00:00:58,000
let's assume you have one or more hosts

00:00:56,480 --> 00:01:00,320
each host containing one or more

00:00:58,000 --> 00:01:02,000
applications and you want to send your

00:01:00,320 --> 00:01:03,600
telemetry data to one or more different

00:01:02,000 --> 00:01:05,040
backends

00:01:03,600 --> 00:01:06,720
typically how you get started is you

00:01:05,040 --> 00:01:07,119
would deploy the open symmetry collector

00:01:06,720 --> 00:01:09,360
as an

00:01:07,119 --> 00:01:11,760
agent on your hosts for example in

00:01:09,360 --> 00:01:13,920
kubernetes this could be a daemon set

00:01:11,760 --> 00:01:14,880
this would allow the collector to go

00:01:13,920 --> 00:01:18,320
ahead and

00:01:14,880 --> 00:01:20,479
gather metrics from the host itself and

00:01:18,320 --> 00:01:22,960
it provides a destination where if you

00:01:20,479 --> 00:01:24,799
deploy the open telemetry client library

00:01:22,960 --> 00:01:26,400
it will be able to send metrics and

00:01:24,799 --> 00:01:27,759
traces from your application to the

00:01:26,400 --> 00:01:29,600
collector itself

00:01:27,759 --> 00:01:31,680
note that the collector also supports

00:01:29,600 --> 00:01:33,040
popular open source client libraries

00:01:31,680 --> 00:01:36,720
including prometheus

00:01:33,040 --> 00:01:38,560
zipkin and jaeger in addition you may

00:01:36,720 --> 00:01:41,360
also want to deploy the collector as a

00:01:38,560 --> 00:01:43,119
standalone gateway service

00:01:41,360 --> 00:01:45,439
the open telemetry collector is made up

00:01:43,119 --> 00:01:47,439
of variety of components

00:01:45,439 --> 00:01:49,280
first we have receivers which are used

00:01:47,439 --> 00:01:52,799
to get data into the collector

00:01:49,280 --> 00:01:54,399
these can be either push or pull based

00:01:52,799 --> 00:01:56,000
on the other end we have exporters how

00:01:54,399 --> 00:01:59,680
you get data out of the system

00:01:56,000 --> 00:02:01,200
again these can be push or pull based

00:01:59,680 --> 00:02:03,040
in between you have the notion of

00:02:01,200 --> 00:02:05,680
processors which are used

00:02:03,040 --> 00:02:08,160
to massage the data as it moves through

00:02:05,680 --> 00:02:09,280
the collector

00:02:08,160 --> 00:02:10,959
now for each of these different

00:02:09,280 --> 00:02:12,319
components you can build what are called

00:02:10,959 --> 00:02:14,560
pipelines

00:02:12,319 --> 00:02:16,160
for example maybe i have a pipeline

00:02:14,560 --> 00:02:18,480
where i have an otp

00:02:16,160 --> 00:02:20,080
receiver otlp is the default protocol

00:02:18,480 --> 00:02:23,040
used by open telemetry

00:02:20,080 --> 00:02:23,440
that passes through a batch processor

00:02:23,040 --> 00:02:25,840
and

00:02:23,440 --> 00:02:27,840
also a kubernetes tagger processor

00:02:25,840 --> 00:02:30,879
before being exported

00:02:27,840 --> 00:02:32,400
out through a jager destination

00:02:30,879 --> 00:02:34,959
in this case you can see that the

00:02:32,400 --> 00:02:37,519
converter is translating from one format

00:02:34,959 --> 00:02:39,120
to another

00:02:37,519 --> 00:02:41,120
you may also have a second pipeline

00:02:39,120 --> 00:02:42,000
where again you are leveraging the otp

00:02:41,120 --> 00:02:43,760
receiver

00:02:42,000 --> 00:02:45,760
this time with a separate batch

00:02:43,760 --> 00:02:47,280
processor in kubernetes tagger

00:02:45,760 --> 00:02:49,360
and this time you're sending it to two

00:02:47,280 --> 00:02:52,640
different destinations otlp

00:02:49,360 --> 00:02:55,040
and prometheus as you can see pipelines

00:02:52,640 --> 00:02:57,280
give you flexibility and choice

00:02:55,040 --> 00:02:58,879
when it comes to telemetry data at the

00:02:57,280 --> 00:03:00,480
end of the day it's your data

00:02:58,879 --> 00:03:02,080
so you need to make sure that the

00:03:00,480 --> 00:03:04,640
components that are available

00:03:02,080 --> 00:03:06,560
meet your specific requirements the

00:03:04,640 --> 00:03:08,560
collector provides flexibility and

00:03:06,560 --> 00:03:09,760
choice through configuration

00:03:08,560 --> 00:03:12,000
and there are a variety of different

00:03:09,760 --> 00:03:13,840
ways you can leverage it to control your

00:03:12,000 --> 00:03:16,720
data

00:03:13,840 --> 00:03:17,840
first let's talk about pii reduction

00:03:16,720 --> 00:03:20,000
there are a few different types of

00:03:17,840 --> 00:03:21,680
scenarios where this may be applicable

00:03:20,000 --> 00:03:23,440
most commonly we have things like

00:03:21,680 --> 00:03:24,959
database queries which can contain

00:03:23,440 --> 00:03:27,200
sensitive information

00:03:24,959 --> 00:03:29,280
but this could also apply to application

00:03:27,200 --> 00:03:30,799
specific metadata

00:03:29,280 --> 00:03:33,519
in the example on the left we're doing

00:03:30,799 --> 00:03:35,599
two things first we're saying that if we

00:03:33,519 --> 00:03:38,480
notice that a span has an attribute with

00:03:35,599 --> 00:03:40,400
a key of pii and a value of false

00:03:38,480 --> 00:03:43,120
then we don't actually want to go any

00:03:40,400 --> 00:03:45,680
further with this attributes processor

00:03:43,120 --> 00:03:47,200
otherwise we want to look for the key of

00:03:45,680 --> 00:03:49,360
database dot statement

00:03:47,200 --> 00:03:51,120
and if we find it we want to hash that

00:03:49,360 --> 00:03:52,319
value

00:03:51,120 --> 00:03:54,720
on the right hand side we have a

00:03:52,319 --> 00:03:56,560
different example here we're looking for

00:03:54,720 --> 00:03:59,200
spans where the service is called

00:03:56,560 --> 00:04:02,239
sell on amazon and we're looking for an

00:03:59,200 --> 00:04:04,400
attribute of job underscore args

00:04:02,239 --> 00:04:05,519
if we find it we will delete the job

00:04:04,400 --> 00:04:08,640
underscore args

00:04:05,519 --> 00:04:11,360
attribute next we have sampling which is

00:04:08,640 --> 00:04:13,760
typically used to control costs

00:04:11,360 --> 00:04:14,480
in this example we're doing two things

00:04:13,760 --> 00:04:16,320
first

00:04:14,480 --> 00:04:18,639
we're looking for all spans where the

00:04:16,320 --> 00:04:20,160
service is data hyphen platform

00:04:18,639 --> 00:04:22,079
and we're adding an attribute of

00:04:20,160 --> 00:04:22,560
sampling priority setting it to a

00:04:22,079 --> 00:04:24,400
hundred

00:04:22,560 --> 00:04:25,759
to ensure that these spans are always

00:04:24,400 --> 00:04:27,360
sampled

00:04:25,759 --> 00:04:29,360
secondarily we're enabling the

00:04:27,360 --> 00:04:31,840
probabilistic sampler and setting the

00:04:29,360 --> 00:04:33,680
sampling percentage to 15 percent

00:04:31,840 --> 00:04:35,280
this applies to all data processing

00:04:33,680 --> 00:04:38,800
through the system but note that the

00:04:35,280 --> 00:04:41,520
sampling priority takes precedence

00:04:38,800 --> 00:04:44,560
data enrichment is used to add metadata

00:04:41,520 --> 00:04:47,360
or to add resource information

00:04:44,560 --> 00:04:47,680
for example we can add an attribute

00:04:47,360 --> 00:04:50,240
called

00:04:47,680 --> 00:04:51,840
environment with a value of production

00:04:50,240 --> 00:04:55,759
for all spans that pass

00:04:51,840 --> 00:04:57,680
through this collector in addition we've

00:04:55,759 --> 00:04:59,919
enabled the kubernetes tagger

00:04:57,680 --> 00:05:01,360
which allows us to add information such

00:04:59,919 --> 00:05:03,680
as the pod

00:05:01,360 --> 00:05:05,680
deployment namespace information

00:05:03,680 --> 00:05:07,039
directly from kubernetes

00:05:05,680 --> 00:05:09,440
this is possible when running the

00:05:07,039 --> 00:05:11,600
collector as an agent on hosts

00:05:09,440 --> 00:05:13,360
and is something that an agent is in the

00:05:11,600 --> 00:05:15,120
best position to go ahead and add

00:05:13,360 --> 00:05:16,720
given that the application may or may

00:05:15,120 --> 00:05:20,479
not be aware of this

00:05:16,720 --> 00:05:21,840
metadata next let's talk about reducing

00:05:20,479 --> 00:05:23,600
cardinality

00:05:21,840 --> 00:05:25,120
you may come across services that are

00:05:23,600 --> 00:05:27,919
instrumented in such a way

00:05:25,120 --> 00:05:29,680
that the span name values return a large

00:05:27,919 --> 00:05:32,320
number of unique values

00:05:29,680 --> 00:05:34,400
resulting in high cardinality this can

00:05:32,320 --> 00:05:35,840
be overcome using the span processor in

00:05:34,400 --> 00:05:38,240
the collector

00:05:35,840 --> 00:05:39,520
for example we can include certain

00:05:38,240 --> 00:05:41,520
service names

00:05:39,520 --> 00:05:43,360
and we can write rules to actually

00:05:41,520 --> 00:05:45,840
normalize the span names

00:05:43,360 --> 00:05:47,840
thus removing the cardinality here you

00:05:45,840 --> 00:05:49,840
can see an example with core dns where

00:05:47,840 --> 00:05:52,479
the service instance and request name

00:05:49,840 --> 00:05:54,400
are being normalized finally

00:05:52,479 --> 00:05:55,520
let's talk about fixing instrumentation

00:05:54,400 --> 00:05:57,440
flaws

00:05:55,520 --> 00:05:59,600
it's much harder to actually change and

00:05:57,440 --> 00:06:02,000
modify code instrumentation than it is

00:05:59,600 --> 00:06:04,639
to update collector configuration

00:06:02,000 --> 00:06:06,560
so the span processor also allows us to

00:06:04,639 --> 00:06:08,960
override the span name given other

00:06:06,560 --> 00:06:10,639
attributes on the span itself

00:06:08,960 --> 00:06:12,800
for example here you can see that we're

00:06:10,639 --> 00:06:15,440
looking for the rails.controller

00:06:12,800 --> 00:06:16,319
and rails.action attributes we're

00:06:15,440 --> 00:06:18,720
separating them

00:06:16,319 --> 00:06:20,319
by a period and we're using them to

00:06:18,720 --> 00:06:22,720
rename the spans

00:06:20,319 --> 00:06:24,880
thus providing value where value may not

00:06:22,720 --> 00:06:27,199
already exist

00:06:24,880 --> 00:06:29,199
as i mentioned for each of the different

00:06:27,199 --> 00:06:29,919
processors receivers and exporters that

00:06:29,199 --> 00:06:31,840
you have

00:06:29,919 --> 00:06:33,120
you want to define one or more different

00:06:31,840 --> 00:06:35,039
pipelines

00:06:33,120 --> 00:06:37,360
pipelines are actually defined per data

00:06:35,039 --> 00:06:38,400
source today traces and metrics are

00:06:37,360 --> 00:06:41,440
fully supported

00:06:38,400 --> 00:06:43,360
and initial log support does exist the

00:06:41,440 --> 00:06:45,199
order in which you define the processors

00:06:43,360 --> 00:06:46,479
actually matters

00:06:45,199 --> 00:06:48,400
the great thing about the collector is

00:06:46,479 --> 00:06:50,319
that it handles translation for you

00:06:48,400 --> 00:06:52,400
this means you can receive in one format

00:06:50,319 --> 00:06:53,919
while exporting in a different format

00:06:52,400 --> 00:06:55,599
thus providing a vendor agnostic

00:06:53,919 --> 00:06:57,360
solution

00:06:55,599 --> 00:06:58,960
now that you know different ways in

00:06:57,360 --> 00:06:59,759
which you can control your data with the

00:06:58,960 --> 00:07:01,520
collector

00:06:59,759 --> 00:07:02,880
i want to actually show you live with a

00:07:01,520 --> 00:07:04,800
demo

00:07:02,880 --> 00:07:07,360
for this demo i'm going to start with a

00:07:04,800 --> 00:07:09,919
base yaml file for the collector

00:07:07,360 --> 00:07:12,560
this will allow me to receive span data

00:07:09,919 --> 00:07:15,120
in either otop or zip code format

00:07:12,560 --> 00:07:17,599
and export that data via a logging

00:07:15,120 --> 00:07:19,280
exporter so we can see it locally on the

00:07:17,599 --> 00:07:22,319
screen here

00:07:19,280 --> 00:07:24,800
let's go ahead and run this docker

00:07:22,319 --> 00:07:24,800
container

00:07:24,960 --> 00:07:30,000
then i've actually constructed a json

00:07:27,680 --> 00:07:33,199
payload that consists of a single

00:07:30,000 --> 00:07:33,680
span trace which i can then send via

00:07:33,199 --> 00:07:35,840
curl

00:07:33,680 --> 00:07:38,000
to the collector and we can go ahead and

00:07:35,840 --> 00:07:40,080
check the output

00:07:38,000 --> 00:07:41,919
now above here you can see that we did

00:07:40,080 --> 00:07:44,960
in fact receive a span

00:07:41,919 --> 00:07:45,680
i span with a service name of api it

00:07:44,960 --> 00:07:49,039
looks like it

00:07:45,680 --> 00:07:50,720
is a server span kind and it has a

00:07:49,039 --> 00:07:52,879
variety of different metadata for

00:07:50,720 --> 00:07:55,280
example you'll notice the http

00:07:52,879 --> 00:07:56,879
underscore response code which was 201

00:07:55,280 --> 00:07:59,680
in this case

00:07:56,879 --> 00:08:00,720
it also has some interesting additional

00:07:59,680 --> 00:08:02,960
metadata

00:08:00,720 --> 00:08:05,440
this looks like it might be a social

00:08:02,960 --> 00:08:06,800
security number which would be pii

00:08:05,440 --> 00:08:09,440
and definitely not something i'd want to

00:08:06,800 --> 00:08:12,080
attach to my spans

00:08:09,440 --> 00:08:12,560
i can also see an email address here and

00:08:12,080 --> 00:08:14,400
well

00:08:12,560 --> 00:08:16,400
that might be sensitive and maybe i'm

00:08:14,400 --> 00:08:17,360
not that comfortable sending that data

00:08:16,400 --> 00:08:20,080
either

00:08:17,360 --> 00:08:21,919
so what can i do about this let's go

00:08:20,080 --> 00:08:22,720
ahead and go back to the configuration

00:08:21,919 --> 00:08:24,479
here

00:08:22,720 --> 00:08:26,080
you'll notice that i'm using an

00:08:24,479 --> 00:08:27,759
attributes processor

00:08:26,080 --> 00:08:30,000
and it's currently configured to go

00:08:27,759 --> 00:08:31,599
ahead and add some metadata in fact it

00:08:30,000 --> 00:08:34,959
adds an environment tag

00:08:31,599 --> 00:08:37,039
where the value is set to panic clinic

00:08:34,959 --> 00:08:39,039
well i can actually use this attributes

00:08:37,039 --> 00:08:39,760
processor to do a variety of different

00:08:39,039 --> 00:08:41,839
operations

00:08:39,760 --> 00:08:44,000
like for example that social security

00:08:41,839 --> 00:08:45,680
number really needs to be removed

00:08:44,000 --> 00:08:47,040
so let's go ahead and use the delete

00:08:45,680 --> 00:08:50,160
action

00:08:47,040 --> 00:08:52,000
and i saw the email address too

00:08:50,160 --> 00:08:53,600
and that's a little bit sensitive so

00:08:52,000 --> 00:08:56,800
maybe i'll go ahead and

00:08:53,600 --> 00:08:58,320
hash that value so we'll just make those

00:08:56,800 --> 00:09:01,440
two changes we will

00:08:58,320 --> 00:09:01,839
fire the collector back up and go ahead

00:09:01,440 --> 00:09:05,120
and

00:09:01,839 --> 00:09:06,240
send the exact same data again and let's

00:09:05,120 --> 00:09:09,360
see what we get

00:09:06,240 --> 00:09:11,440
this time so i still see i get an

00:09:09,360 --> 00:09:12,160
environment pet clinic that's been added

00:09:11,440 --> 00:09:15,360
so that's

00:09:12,160 --> 00:09:17,680
great i can see that the email address

00:09:15,360 --> 00:09:18,560
is now hashed so i don't really know

00:09:17,680 --> 00:09:20,640
what the value

00:09:18,560 --> 00:09:22,399
is but i have a unique identifier that i

00:09:20,640 --> 00:09:23,920
could then query in the back end that i

00:09:22,399 --> 00:09:27,200
send this data to

00:09:23,920 --> 00:09:28,000
and i no longer see a social security

00:09:27,200 --> 00:09:31,600
number so

00:09:28,000 --> 00:09:33,440
that's that's great as you can see

00:09:31,600 --> 00:09:35,279
the collector makes it really easy to

00:09:33,440 --> 00:09:36,480
control your data

00:09:35,279 --> 00:09:39,120
we hope you'll come take a look at the

00:09:36,480 --> 00:09:41,519
project join the conversation on getter

00:09:39,120 --> 00:09:42,880
or prs are definitely welcome so take a

00:09:41,519 --> 00:09:46,080
look at the good first issue

00:09:42,880 --> 00:09:46,080
and help wanted labels

00:09:46,160 --> 00:09:51,680
thanks so much for joining and i hope

00:09:47,600 --> 00:09:51,680

YouTube URL: https://www.youtube.com/watch?v=9Sfm70u7yLo


