Title: Migrating unicorn company to real-time stream processing
Publication date: 2021-06-01
Playlist: KCD Africa
Description: 
	Every company is a data company nowadays. Ruslan will present how he has migrated Bolt from batch data workloads and synchronous processing to real-time data streaming and asynchronous processing.
He will talk about the problems he faced along the way and the lessons team has learned from this migration. Ruslan will also focus on the unleashed value real-time data provided to the company.
Captions: 
	00:00:00,719 --> 00:00:03,679
hello everyone thank you very much for

00:00:02,399 --> 00:00:05,520
joining this presentation

00:00:03,679 --> 00:00:08,240
hope you'll find it useful my name is

00:00:05,520 --> 00:00:10,559
roslan i work as data architect at bolt

00:00:08,240 --> 00:00:11,360
and today i will tell you our experience

00:00:10,559 --> 00:00:14,000
migrating

00:00:11,360 --> 00:00:16,880
unicorn size company from batch to real

00:00:14,000 --> 00:00:18,720
time stream processing

00:00:16,880 --> 00:00:19,920
i guess most of you are fairly familiar

00:00:18,720 --> 00:00:21,359
with bolt we are

00:00:19,920 --> 00:00:23,439
leading micro mobility and

00:00:21,359 --> 00:00:24,960
transportation platform in europe and

00:00:23,439 --> 00:00:27,439
africa

00:00:24,960 --> 00:00:29,199
this slide shows the volume of money

00:00:27,439 --> 00:00:30,240
raised by different players in the

00:00:29,199 --> 00:00:32,880
market

00:00:30,240 --> 00:00:34,160
it can be a bit outdated but it conveys

00:00:32,880 --> 00:00:36,880
very important meaning

00:00:34,160 --> 00:00:37,920
it basically shows how important for the

00:00:36,880 --> 00:00:42,480
success

00:00:37,920 --> 00:00:42,480
is to be efficient in today's business

00:00:43,120 --> 00:00:46,800
these are the topics we will cover in

00:00:44,640 --> 00:00:47,440
our presentation today we will start

00:00:46,800 --> 00:00:50,719
with

00:00:47,440 --> 00:00:51,199
understanding which requirements users

00:00:50,719 --> 00:00:54,719
have

00:00:51,199 --> 00:00:55,199
towards data nowadays then we will talk

00:00:54,719 --> 00:00:57,680
about

00:00:55,199 --> 00:00:59,920
why would companies want to adopt stream

00:00:57,680 --> 00:01:03,039
processing and migrate away from batch

00:00:59,920 --> 00:01:06,159
towards stream then we will

00:01:03,039 --> 00:01:08,720
discuss where to get the events

00:01:06,159 --> 00:01:10,880
to process with stream processing then

00:01:08,720 --> 00:01:12,720
we will discuss the actual internals

00:01:10,880 --> 00:01:14,400
of the stream processing and we will

00:01:12,720 --> 00:01:15,119
wrap it up with discussing which

00:01:14,400 --> 00:01:17,680
problems

00:01:15,119 --> 00:01:20,560
we have experienced along our way and

00:01:17,680 --> 00:01:23,119
which lessons we have learned

00:01:20,560 --> 00:01:24,479
so let's start with understanding what

00:01:23,119 --> 00:01:28,400
kind of requirements

00:01:24,479 --> 00:01:30,320
users have towards the data

00:01:28,400 --> 00:01:32,159
data can originate in many different

00:01:30,320 --> 00:01:34,560
sources those can be

00:01:32,159 --> 00:01:36,000
backend microservices front-end

00:01:34,560 --> 00:01:40,079
microservices

00:01:36,000 --> 00:01:43,600
databases be it sql database or nosql

00:01:40,079 --> 00:01:46,560
database some files it can be even be

00:01:43,600 --> 00:01:48,000
user input after all but first and

00:01:46,560 --> 00:01:50,799
foremost requirements

00:01:48,000 --> 00:01:52,960
which users have towards data is that

00:01:50,799 --> 00:01:55,600
data has to be consistent across

00:01:52,960 --> 00:01:57,360
different systems and services users

00:01:55,600 --> 00:01:59,920
will not tolerate if they see

00:01:57,360 --> 00:02:00,399
one number in one place and another

00:01:59,920 --> 00:02:03,439
number

00:02:00,399 --> 00:02:06,000
somewhere else second requirement

00:02:03,439 --> 00:02:08,239
is that users don't actually care where

00:02:06,000 --> 00:02:12,080
this data has originated from

00:02:08,239 --> 00:02:15,280
be it structured source or database

00:02:12,080 --> 00:02:18,480
or unstructured they want to have easy

00:02:15,280 --> 00:02:20,800
way to query the data and to get

00:02:18,480 --> 00:02:24,560
they want to have easy way to get to all

00:02:20,800 --> 00:02:28,000
of its fields and all of its values

00:02:24,560 --> 00:02:30,000
all of us live in the era of big data

00:02:28,000 --> 00:02:32,319
meaning that volumes of the data we have

00:02:30,000 --> 00:02:35,440
to process and store and handle

00:02:32,319 --> 00:02:37,280
only grows over the time but

00:02:35,440 --> 00:02:39,440
users don't actually care about that

00:02:37,280 --> 00:02:42,800
they want to have access to both

00:02:39,440 --> 00:02:45,519
current data or the fresh data and all

00:02:42,800 --> 00:02:47,840
the historical data

00:02:45,519 --> 00:02:49,760
and last but not the least is actually

00:02:47,840 --> 00:02:52,879
the speed of data delivery

00:02:49,760 --> 00:02:54,640
users usually don't want to wait for the

00:02:52,879 --> 00:02:56,480
data to get

00:02:54,640 --> 00:02:57,760
to propagate through the systems or to

00:02:56,480 --> 00:03:00,159
get delivered to the

00:02:57,760 --> 00:03:04,400
final system they want to have access to

00:03:00,159 --> 00:03:06,080
it the fastest the better

00:03:04,400 --> 00:03:07,760
now that we know which requirements

00:03:06,080 --> 00:03:10,159
people have towards the data

00:03:07,760 --> 00:03:10,800
let's discuss why would any company want

00:03:10,159 --> 00:03:14,000
to actually

00:03:10,800 --> 00:03:16,080
adopt the stream processing and remember

00:03:14,000 --> 00:03:18,239
we have talked briefly about

00:03:16,080 --> 00:03:19,200
the importance of being efficient in

00:03:18,239 --> 00:03:21,599
today's business

00:03:19,200 --> 00:03:23,440
and that's exactly the point of this

00:03:21,599 --> 00:03:27,040
slide

00:03:23,440 --> 00:03:28,959
com any company would prefer to adopt

00:03:27,040 --> 00:03:32,000
stream processing because it actually

00:03:28,959 --> 00:03:33,920
puts data in to the motion

00:03:32,000 --> 00:03:35,280
it allows data to flow throughout the

00:03:33,920 --> 00:03:38,720
systems and

00:03:35,280 --> 00:03:39,360
it means that company can develop

00:03:38,720 --> 00:03:42,959
so-called

00:03:39,360 --> 00:03:42,959
reactive microservices

00:03:45,040 --> 00:03:48,400
those are the services which actually um

00:03:47,840 --> 00:03:51,440
react

00:03:48,400 --> 00:03:53,760
or respond to some kind of change in

00:03:51,440 --> 00:03:54,480
some state or to some kind of event

00:03:53,760 --> 00:03:56,159
imagine

00:03:54,480 --> 00:03:57,760
you are buying something on amazon when

00:03:56,159 --> 00:04:01,280
you click this

00:03:57,760 --> 00:04:03,360
buy button many actions have to happen

00:04:01,280 --> 00:04:04,799
like invoice has to be generated and

00:04:03,360 --> 00:04:08,000
sent to your email

00:04:04,799 --> 00:04:09,280
the inventory on the warehouse has to be

00:04:08,000 --> 00:04:11,439
reserved

00:04:09,280 --> 00:04:12,879
the shipment or delivery process has to

00:04:11,439 --> 00:04:15,360
get initiated

00:04:12,879 --> 00:04:17,199
and all of those events are triggered by

00:04:15,360 --> 00:04:19,840
one simple click

00:04:17,199 --> 00:04:20,400
and adopting the stream processing

00:04:19,840 --> 00:04:22,400
actually

00:04:20,400 --> 00:04:25,040
allows to do it and it also allows you

00:04:22,400 --> 00:04:28,000
to detect all those changes in real time

00:04:25,040 --> 00:04:28,960
allowing you to make real time decisions

00:04:28,000 --> 00:04:33,440
on top of your

00:04:28,960 --> 00:04:35,199
data it also allows you

00:04:33,440 --> 00:04:37,120
i guess most of you are fairly familiar

00:04:35,199 --> 00:04:41,040
with the concept of etl

00:04:37,120 --> 00:04:42,320
when you basically process big volumes

00:04:41,040 --> 00:04:46,080
of data in a

00:04:42,320 --> 00:04:46,479
batch mode and stream processing allows

00:04:46,080 --> 00:04:49,840
you

00:04:46,479 --> 00:04:52,080
to spread the load of those etls

00:04:49,840 --> 00:04:52,880
throughout the some throughout some time

00:04:52,080 --> 00:04:55,120
period like

00:04:52,880 --> 00:04:56,800
day or week for example basically it

00:04:55,120 --> 00:04:59,440
means that you don't have to

00:04:56,800 --> 00:05:01,440
run the heavy etl job once a day or once

00:04:59,440 --> 00:05:03,199
a week to process all the data for the

00:05:01,440 --> 00:05:07,520
past period

00:05:03,199 --> 00:05:07,520
and you can actually incrementally

00:05:07,600 --> 00:05:12,160
process every single new piece of data

00:05:10,479 --> 00:05:13,680
throughout the day or throughout the

00:05:12,160 --> 00:05:16,000
week in real time

00:05:13,680 --> 00:05:17,199
and adoption stream processing actually

00:05:16,000 --> 00:05:20,000
allows you to build

00:05:17,199 --> 00:05:20,479
the services and architectures which are

00:05:20,000 --> 00:05:24,479
much

00:05:20,479 --> 00:05:27,199
more scalable so now that we know

00:05:24,479 --> 00:05:28,720
why actually would you want to adopt the

00:05:27,199 --> 00:05:32,240
stream processing

00:05:28,720 --> 00:05:34,720
or events in motion

00:05:32,240 --> 00:05:37,120
let's talk about where actually can you

00:05:34,720 --> 00:05:40,639
get those events from

00:05:37,120 --> 00:05:43,759
we all live in the era of microservices

00:05:40,639 --> 00:05:46,639
and they have plenty of

00:05:43,759 --> 00:05:48,400
different benefits like for example

00:05:46,639 --> 00:05:51,039
easiness of deploy

00:05:48,400 --> 00:05:52,880
or maybe being decoupled one from

00:05:51,039 --> 00:05:57,120
another

00:05:52,880 --> 00:06:01,520
but they also come at some cost

00:05:57,120 --> 00:06:04,720
and usually that's how it looks in your

00:06:01,520 --> 00:06:05,280
production environment and want it or

00:06:04,720 --> 00:06:07,280
not

00:06:05,280 --> 00:06:08,720
at some point you end up in a zoo where

00:06:07,280 --> 00:06:09,600
you have different plenty of micro

00:06:08,720 --> 00:06:12,800
services

00:06:09,600 --> 00:06:15,759
talking to each other and

00:06:12,800 --> 00:06:18,080
good luck deriving some minion from this

00:06:15,759 --> 00:06:20,639
house

00:06:18,080 --> 00:06:22,319
but one of the important features

00:06:20,639 --> 00:06:23,360
feature of the microservices is that

00:06:22,319 --> 00:06:25,759
they don't actually leave

00:06:23,360 --> 00:06:28,479
isolated from each other and they need

00:06:25,759 --> 00:06:32,080
to communicate or talk to each other

00:06:28,479 --> 00:06:33,120
to basically signal to other services

00:06:32,080 --> 00:06:36,479
that something has

00:06:33,120 --> 00:06:37,280
actually happened and believe it or not

00:06:36,479 --> 00:06:39,840
but

00:06:37,280 --> 00:06:41,039
it may sound easy but it's actually one

00:06:39,840 --> 00:06:44,400
of the hardest problems

00:06:41,039 --> 00:06:48,080
in modern software engineering

00:06:44,400 --> 00:06:50,479
because you can never actually

00:06:48,080 --> 00:06:51,280
be sure that some communication between

00:06:50,479 --> 00:06:53,759
the services

00:06:51,280 --> 00:06:54,560
has happened because services can fail

00:06:53,759 --> 00:06:57,120
or maybe

00:06:54,560 --> 00:06:57,840
event or the communication this the

00:06:57,120 --> 00:07:00,960
request

00:06:57,840 --> 00:07:03,599
can get lost and you will never know

00:07:00,960 --> 00:07:06,240
about that

00:07:03,599 --> 00:07:08,479
unfortunately right now there is no

00:07:06,240 --> 00:07:10,800
engineer friendly or easy way

00:07:08,479 --> 00:07:12,479
to guarantee that some communication

00:07:10,800 --> 00:07:14,800
between the services has happened

00:07:12,479 --> 00:07:16,160
and those of you who have worked with

00:07:14,800 --> 00:07:18,479
similar setups

00:07:16,160 --> 00:07:19,759
they know that usually people solve it

00:07:18,479 --> 00:07:20,880
with some kind of distributed

00:07:19,759 --> 00:07:23,759
transactions

00:07:20,880 --> 00:07:24,479
or state machines and they also know how

00:07:23,759 --> 00:07:28,479
painful

00:07:24,479 --> 00:07:30,160
it is and when we at bolt

00:07:28,479 --> 00:07:32,319
thought about all right how would we

00:07:30,160 --> 00:07:34,639
tackle this this problem

00:07:32,319 --> 00:07:36,720
how can we migrate towards stream

00:07:34,639 --> 00:07:38,240
processing where do we get events from

00:07:36,720 --> 00:07:40,160
we started thinking from the other

00:07:38,240 --> 00:07:44,319
direction we use

00:07:40,160 --> 00:07:46,080
mysql as our relational database for all

00:07:44,319 --> 00:07:49,199
our microservices

00:07:46,080 --> 00:07:51,120
and those of you who know who have

00:07:49,199 --> 00:07:52,080
worked with databases they might know

00:07:51,120 --> 00:07:54,479
that

00:07:52,080 --> 00:07:55,520
most of the modern databases they have

00:07:54,479 --> 00:07:58,639
the so-called

00:07:55,520 --> 00:08:00,240
commit log under the hood basically it

00:07:58,639 --> 00:08:03,120
is the log of all the changes

00:08:00,240 --> 00:08:03,360
which database has applied it is used as

00:08:03,120 --> 00:08:06,479
an

00:08:03,360 --> 00:08:08,800
internal tool for the database to

00:08:06,479 --> 00:08:11,120
guarantee data consistency

00:08:08,800 --> 00:08:12,800
and not many people know that actually

00:08:11,120 --> 00:08:15,280
this log it can be read

00:08:12,800 --> 00:08:17,680
by other applications and that's exactly

00:08:15,280 --> 00:08:20,800
the way which we decided to go at bolt

00:08:17,680 --> 00:08:22,479
but now comes the question all right if

00:08:20,800 --> 00:08:24,960
we read this log

00:08:22,479 --> 00:08:26,720
the data which we extract from it well

00:08:24,960 --> 00:08:29,280
it has to be stored somewhere right

00:08:26,720 --> 00:08:30,240
so that later on it can be processed by

00:08:29,280 --> 00:08:33,360
someone else

00:08:30,240 --> 00:08:34,320
or maybe uh even better it can it should

00:08:33,360 --> 00:08:36,560
be processed

00:08:34,320 --> 00:08:38,640
it should be able to be processed by

00:08:36,560 --> 00:08:41,039
many different consumers

00:08:38,640 --> 00:08:41,919
one of the crucial requirements which we

00:08:41,039 --> 00:08:44,159
have agreed on

00:08:41,919 --> 00:08:45,440
when we started developing this whole

00:08:44,159 --> 00:08:47,760
pipeline was that

00:08:45,440 --> 00:08:49,760
well it has to be scalable because

00:08:47,760 --> 00:08:51,519
company grows year over year we launch

00:08:49,760 --> 00:08:55,040
new cities new countries

00:08:51,519 --> 00:08:58,800
we provide rides to more and more

00:08:55,040 --> 00:09:01,600
customers and all of us agreed that

00:08:58,800 --> 00:09:02,640
it has to be scalable but any time when

00:09:01,600 --> 00:09:05,760
someone says

00:09:02,640 --> 00:09:07,920
to me anything about scalability

00:09:05,760 --> 00:09:08,959
there is one piece of software one

00:09:07,920 --> 00:09:11,040
system which

00:09:08,959 --> 00:09:12,800
comes to my mind straight away and

00:09:11,040 --> 00:09:14,800
that's apache kafka

00:09:12,800 --> 00:09:17,040
for those of you who might who might not

00:09:14,800 --> 00:09:19,920
be familiar with apache kafka

00:09:17,040 --> 00:09:21,440
it is the messaging system which was

00:09:19,920 --> 00:09:25,040
developed at linkedin

00:09:21,440 --> 00:09:28,240
and later on it was open sourced

00:09:25,040 --> 00:09:30,000
it provides plenty of beautiful

00:09:28,240 --> 00:09:30,959
guarantees to the engineers and its

00:09:30,000 --> 00:09:34,000
users

00:09:30,959 --> 00:09:37,120
but most importantly it has really high

00:09:34,000 --> 00:09:38,640
events throughput and it is horizontally

00:09:37,120 --> 00:09:41,760
scalable

00:09:38,640 --> 00:09:41,760
one of my favorite

00:09:46,560 --> 00:09:49,920
features which apache kafka provides is

00:09:48,959 --> 00:09:52,000
actually

00:09:49,920 --> 00:09:53,440
write once read many semantics it means

00:09:52,000 --> 00:09:56,720
that you can send

00:09:53,440 --> 00:09:59,360
or write one event to it which can be

00:09:56,720 --> 00:10:00,320
processed by number of independent

00:09:59,360 --> 00:10:02,640
consumers

00:10:00,320 --> 00:10:04,240
at their own pace at different points in

00:10:02,640 --> 00:10:06,160
time in the future

00:10:04,240 --> 00:10:08,160
there is no need to duplicate this same

00:10:06,160 --> 00:10:09,040
event so that it can be processed by

00:10:08,160 --> 00:10:12,320
many consumers

00:10:09,040 --> 00:10:12,560
you can write it once and then process

00:10:12,320 --> 00:10:16,640
it

00:10:12,560 --> 00:10:19,760
as many times as you want so

00:10:16,640 --> 00:10:20,320
now that we understand from where can we

00:10:19,760 --> 00:10:24,000
get

00:10:20,320 --> 00:10:26,880
can we get those events from and that we

00:10:24,000 --> 00:10:28,000
persist them to apache kafka now comes

00:10:26,880 --> 00:10:31,279
the part of

00:10:28,000 --> 00:10:32,320
stream processing let's deep dive into

00:10:31,279 --> 00:10:33,760
it

00:10:32,320 --> 00:10:35,760
right now on the market there are plenty

00:10:33,760 --> 00:10:38,000
of different frameworks and libraries

00:10:35,760 --> 00:10:38,959
which are actually doing stream

00:10:38,000 --> 00:10:41,519
processing

00:10:38,959 --> 00:10:42,959
so it's up to you to decide whichever

00:10:41,519 --> 00:10:46,720
one of those you would like to

00:10:42,959 --> 00:10:49,760
to use but before maybe

00:10:46,720 --> 00:10:50,720
discussing them let's uh define what

00:10:49,760 --> 00:10:53,839
actually a stream

00:10:50,720 --> 00:10:56,079
is and stream has two important

00:10:53,839 --> 00:10:58,800
characteristics first of all it is

00:10:56,079 --> 00:10:59,600
unbounded flow of data it basically

00:10:58,800 --> 00:11:02,480
means that

00:10:59,600 --> 00:11:03,920
you are getting the events right now you

00:11:02,480 --> 00:11:06,160
will get them in the future and

00:11:03,920 --> 00:11:07,519
there will be no end to that you cannot

00:11:06,160 --> 00:11:10,399
say that at some point

00:11:07,519 --> 00:11:11,040
they will stop it is unbounded you will

00:11:10,399 --> 00:11:13,360
simply

00:11:11,040 --> 00:11:14,880
keep getting them continuously and

00:11:13,360 --> 00:11:18,160
second important characteristic

00:11:14,880 --> 00:11:21,440
is that it is happening in real time

00:11:18,160 --> 00:11:23,120
you don't have to do it once an hour

00:11:21,440 --> 00:11:26,839
once a day once a week

00:11:23,120 --> 00:11:28,959
it is happening at any given point in

00:11:26,839 --> 00:11:31,279
time

00:11:28,959 --> 00:11:32,800
there are two types of stream whenever

00:11:31,279 --> 00:11:35,120
we are talking about

00:11:32,800 --> 00:11:35,839
the uh the stream processing as a

00:11:35,120 --> 00:11:37,839
framework

00:11:35,839 --> 00:11:39,519
there are two types of stream processors

00:11:37,839 --> 00:11:41,920
which you have to very clearly

00:11:39,519 --> 00:11:43,600
differentiate between first one are so

00:11:41,920 --> 00:11:46,399
called stateless processors

00:11:43,600 --> 00:11:47,519
and they process every single event

00:11:46,399 --> 00:11:49,760
independently of

00:11:47,519 --> 00:11:52,880
all the previous events they don't have

00:11:49,760 --> 00:11:55,519
any history whatsoever

00:11:52,880 --> 00:11:57,120
examples of such stream processors can

00:11:55,519 --> 00:11:59,440
be for example some

00:11:57,120 --> 00:12:00,639
transformations or maybe extracting some

00:11:59,440 --> 00:12:02,720
field from event

00:12:00,639 --> 00:12:03,920
or maybe repacking the event into

00:12:02,720 --> 00:12:07,200
different form

00:12:03,920 --> 00:12:09,040
something which you can do for every

00:12:07,200 --> 00:12:11,519
event separately and second one

00:12:09,040 --> 00:12:13,760
is so-called stateful stream processors

00:12:11,519 --> 00:12:16,639
and

00:12:13,760 --> 00:12:17,519
they whenever they process any new event

00:12:16,639 --> 00:12:19,519
they also

00:12:17,519 --> 00:12:20,959
keep all the history of previous events

00:12:19,519 --> 00:12:22,639
which they have processed

00:12:20,959 --> 00:12:25,440
examples here can be for example

00:12:22,639 --> 00:12:28,160
calculating some aggregations

00:12:25,440 --> 00:12:28,160
or maybe

00:12:29,760 --> 00:12:34,399
doing some filtering based on the

00:12:32,000 --> 00:12:38,000
previous history

00:12:34,399 --> 00:12:41,200
and so on like i said there are many

00:12:38,000 --> 00:12:42,320
different stream processing frameworks

00:12:41,200 --> 00:12:46,399
on the market

00:12:42,320 --> 00:12:48,160
but there are two which i would like to

00:12:46,399 --> 00:12:49,519
specifically highlight in this

00:12:48,160 --> 00:12:51,920
presentation

00:12:49,519 --> 00:12:53,360
because they are native to the kafka

00:12:51,920 --> 00:12:56,399
ecosystem and first one

00:12:53,360 --> 00:12:58,079
is called k-sql

00:12:56,399 --> 00:13:00,560
it allows you to do the stream

00:12:58,079 --> 00:13:01,040
processing in a syntaxes which is very

00:13:00,560 --> 00:13:03,760
very

00:13:01,040 --> 00:13:05,760
sql-like which is very familiar to the

00:13:03,760 --> 00:13:08,240
engineers

00:13:05,760 --> 00:13:11,040
with such an easy sql like statement you

00:13:08,240 --> 00:13:14,320
can actually define

00:13:11,040 --> 00:13:17,040
which events from which topic you want

00:13:14,320 --> 00:13:17,040
to process

00:13:17,600 --> 00:13:20,880
which fields you want to extract from

00:13:19,839 --> 00:13:22,480
them and to

00:13:20,880 --> 00:13:24,240
which destination topic you want to

00:13:22,480 --> 00:13:28,160
persist them

00:13:24,240 --> 00:13:31,120
second one k sql is a good tool for

00:13:28,160 --> 00:13:31,920
let's call them lightweight stream

00:13:31,120 --> 00:13:33,839
processing

00:13:31,920 --> 00:13:35,680
but sometimes you would want you might

00:13:33,839 --> 00:13:39,040
want to do the um

00:13:35,680 --> 00:13:42,079
uh some very sophisticated

00:13:39,040 --> 00:13:44,320
uh logic it might be for example

00:13:42,079 --> 00:13:45,839
um something related to your business

00:13:44,320 --> 00:13:48,560
logic when you need to i don't know do

00:13:45,839 --> 00:13:51,519
some kind of very specific

00:13:48,560 --> 00:13:52,000
aggregations or transformations and so

00:13:51,519 --> 00:13:54,959
on

00:13:52,000 --> 00:13:55,279
and in this case kafka system provides

00:13:54,959 --> 00:13:57,920
you

00:13:55,279 --> 00:13:59,839
with the kafka streams library it allows

00:13:57,920 --> 00:14:01,360
you to define the stream processing

00:13:59,839 --> 00:14:04,800
tasks

00:14:01,360 --> 00:14:05,600
as a java applications so whatever you

00:14:04,800 --> 00:14:08,560
can express

00:14:05,600 --> 00:14:10,720
in your java code you can do with your

00:14:08,560 --> 00:14:12,720
stream processing

00:14:10,720 --> 00:14:13,839
very good news is that both of the

00:14:12,720 --> 00:14:17,440
frameworks

00:14:13,839 --> 00:14:17,760
are can be actually extended if you need

00:14:17,440 --> 00:14:19,199
to

00:14:17,760 --> 00:14:20,880
implement something which is very very

00:14:19,199 --> 00:14:23,680
much tailored towards your

00:14:20,880 --> 00:14:25,440
business you can write the so-called

00:14:23,680 --> 00:14:28,639
user-defined functions

00:14:25,440 --> 00:14:32,160
basically you define the

00:14:28,639 --> 00:14:34,320
java function and it can be whatever

00:14:32,160 --> 00:14:35,440
like aggregation or it can do some

00:14:34,320 --> 00:14:38,880
business logic

00:14:35,440 --> 00:14:41,920
and then you can call this function boss

00:14:38,880 --> 00:14:42,800
from the k sql like any sql function

00:14:41,920 --> 00:14:46,399
here

00:14:42,800 --> 00:14:49,440
or from your kafka streams application

00:14:46,399 --> 00:14:51,600
and it can be everything it can

00:14:49,440 --> 00:14:54,000
like it's already said be something very

00:14:51,600 --> 00:14:57,279
much related towards your business logic

00:14:54,000 --> 00:14:58,000
or it can also be some kind of fancy

00:14:57,279 --> 00:15:00,720
machine learning

00:14:58,000 --> 00:15:03,120
and stuff and it can be used for fraud

00:15:00,720 --> 00:15:06,160
prevention for anomaly detection

00:15:03,120 --> 00:15:09,199
anything so

00:15:06,160 --> 00:15:10,480
that's the setup which we have adopted

00:15:09,199 --> 00:15:12,880
at bolt

00:15:10,480 --> 00:15:13,600
we ingest data from all the source

00:15:12,880 --> 00:15:16,160
databases

00:15:13,600 --> 00:15:17,839
we persist them into the kafka brokers

00:15:16,160 --> 00:15:20,560
we do the stream processing with the

00:15:17,839 --> 00:15:22,800
libraries i have mentioned

00:15:20,560 --> 00:15:23,760
and also persist the results to kafka

00:15:22,800 --> 00:15:26,079
and then

00:15:23,760 --> 00:15:27,360
those results are consumed by number of

00:15:26,079 --> 00:15:30,560
different consumers

00:15:27,360 --> 00:15:31,040
we store some events to our data lake we

00:15:30,560 --> 00:15:33,519
also

00:15:31,040 --> 00:15:34,880
allow backend micro services to consume

00:15:33,519 --> 00:15:38,000
those events

00:15:34,880 --> 00:15:41,120
and do the business logic and decision

00:15:38,000 --> 00:15:43,759
making on top of them

00:15:41,120 --> 00:15:44,160
which problems have encountered during

00:15:43,759 --> 00:15:47,839
our

00:15:44,160 --> 00:15:48,800
throughout our way when i started this

00:15:47,839 --> 00:15:52,800
whole project

00:15:48,800 --> 00:15:56,480
at that point there was no single

00:15:52,800 --> 00:15:58,480
managed kafka offering on the market

00:15:56,480 --> 00:16:01,040
as of now there are already few of them

00:15:58,480 --> 00:16:04,880
available but there was no at that time

00:16:01,040 --> 00:16:07,920
and if in case you are

00:16:04,880 --> 00:16:09,360
planning to adopt kafka i actually would

00:16:07,920 --> 00:16:12,560
advise you to go with

00:16:09,360 --> 00:16:16,079
managed cloud offering because managing

00:16:12,560 --> 00:16:20,079
setting up and managing kafka clusters

00:16:16,079 --> 00:16:20,079
is not the easiest task believe me

00:16:21,199 --> 00:16:25,519
that was it's not actually a problem but

00:16:23,920 --> 00:16:28,720
that was like

00:16:25,519 --> 00:16:29,519
one uh relatively big challenge and

00:16:28,720 --> 00:16:32,639
obstacle we

00:16:29,519 --> 00:16:36,320
which we had to overcome

00:16:32,639 --> 00:16:37,680
second problem is actually

00:16:36,320 --> 00:16:39,519
whenever you're working with stream

00:16:37,680 --> 00:16:42,079
processing you can be getting

00:16:39,519 --> 00:16:43,839
hundreds of thousands events per second

00:16:42,079 --> 00:16:44,880
and if someone comes to you and says

00:16:43,839 --> 00:16:46,480
like hey

00:16:44,880 --> 00:16:48,959
you know this number is not actually

00:16:46,480 --> 00:16:52,560
correct good luck debugging it

00:16:48,959 --> 00:16:56,160
because it's really really hard to find

00:16:52,560 --> 00:17:00,079
where actually this error is happening

00:16:56,160 --> 00:17:02,000
next issue is that unfortunately

00:17:00,079 --> 00:17:03,680
but currently most of the stream

00:17:02,000 --> 00:17:08,959
processing frameworks

00:17:03,680 --> 00:17:10,959
they are jvm oriented mostly

00:17:08,959 --> 00:17:12,400
so want it or not but you would need to

00:17:10,959 --> 00:17:16,839
use either java

00:17:12,400 --> 00:17:18,880
scala or some other jvm friendly

00:17:16,839 --> 00:17:22,079
language

00:17:18,880 --> 00:17:24,400
next issue which you have to understand

00:17:22,079 --> 00:17:24,400
is that

00:17:26,000 --> 00:17:29,280
let's say one of the big advantages of

00:17:28,480 --> 00:17:31,440
kafka

00:17:29,280 --> 00:17:32,880
is that it actually also keeps the

00:17:31,440 --> 00:17:34,640
history of the events

00:17:32,880 --> 00:17:36,320
it is configurable you can set it up to

00:17:34,640 --> 00:17:39,120
one day one week or months

00:17:36,320 --> 00:17:39,679
one year whatever and it actually allows

00:17:39,120 --> 00:17:41,600
you to

00:17:39,679 --> 00:17:44,080
replace some data from the past let's

00:17:41,600 --> 00:17:46,400
say you have released some new logic

00:17:44,080 --> 00:17:47,520
and later on a few days after that you

00:17:46,400 --> 00:17:50,799
have realized that

00:17:47,520 --> 00:17:54,160
there was a mistake there but

00:17:50,799 --> 00:17:57,760
and now you can replay this um

00:17:54,160 --> 00:18:00,880
history replay those events and correct

00:17:57,760 --> 00:18:02,160
your uh make some adjustments towards

00:18:00,880 --> 00:18:04,160
your business logic

00:18:02,160 --> 00:18:06,160
but you should never think that you have

00:18:04,160 --> 00:18:06,880
to replay only some specific piece of

00:18:06,160 --> 00:18:09,840
data

00:18:06,880 --> 00:18:11,520
because like i said stream is unbounded

00:18:09,840 --> 00:18:14,000
flow of data so you should

00:18:11,520 --> 00:18:14,880
always think of it in the following way

00:18:14,000 --> 00:18:17,120
you should

00:18:14,880 --> 00:18:17,919
that you start processing data from some

00:18:17,120 --> 00:18:22,160
point

00:18:17,919 --> 00:18:22,160
and you keep doing that afterwards

00:18:22,480 --> 00:18:26,320
and other problems which we have seen

00:18:24,160 --> 00:18:30,000
along the way are the um

00:18:26,320 --> 00:18:32,160
let's say for example data deduplication

00:18:30,000 --> 00:18:33,840
network is not reliable different

00:18:32,160 --> 00:18:36,880
services can fail

00:18:33,840 --> 00:18:38,799
and eventually it would lead to

00:18:36,880 --> 00:18:41,840
duplicating some of the events

00:18:38,799 --> 00:18:44,799
and so it is important so your consumers

00:18:41,840 --> 00:18:48,080
are prepared for that

00:18:44,799 --> 00:18:50,240
next one is types in compatibility

00:18:48,080 --> 00:18:51,760
whenever you integrate many different

00:18:50,240 --> 00:18:53,600
systems

00:18:51,760 --> 00:18:55,600
and make them talk to each other you

00:18:53,600 --> 00:18:58,080
have to be very very careful

00:18:55,600 --> 00:18:58,640
so that they process all the data and

00:18:58,080 --> 00:19:01,919
types

00:18:58,640 --> 00:19:03,039
the same way and last but not the least

00:19:01,919 --> 00:19:05,440
like i said if

00:19:03,039 --> 00:19:06,799
you want to start sourcing the events

00:19:05,440 --> 00:19:09,120
from your databases

00:19:06,799 --> 00:19:11,440
you should also think about how do you

00:19:09,120 --> 00:19:13,039
handle database schema migrations like

00:19:11,440 --> 00:19:15,200
for example

00:19:13,039 --> 00:19:19,280
adding some fields to the table or maybe

00:19:15,200 --> 00:19:19,280
changing the type or creating new tables

00:19:20,480 --> 00:19:25,520
that's all i wanted to talk about today

00:19:23,120 --> 00:19:27,280
thank you very much for your attention

00:19:25,520 --> 00:19:32,400
if you have any questions feel free to

00:19:27,280 --> 00:19:32,400

YouTube URL: https://www.youtube.com/watch?v=Id2G3yilXiU


