Title: Whatever Can Go Wrong, Will Go Wrong – Rook Ceph and Storage Failures - Sagy Volkov, Red Hat
Publication date: 2020-11-23
Playlist: KubeCon + CloudNativeCon North America 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon Europe 2021 Virtual from May 4–7, 2021. Learn more at https://kubecon.io. The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects. 

Whatever Can Go Wrong, Will Go Wrong – Rook/Ceph and Storage Failures - Sagy Volkov, Red Hat 

Imagine running a 200-node Kubernetes cluster, and suddenly you lost a node or even a ToR switch. What is the state of your persistent storage that your application relies on? How can you make sure your storage is always available? How can you time and plan how long it takes for your storage to get back to 100% resiliency? In this presentation we’ll go over the basics of storage demands (RPO/RTO), How different types of replications in Ceph impact our recovery time, and how components failure such as drive, node or cluster determine how long we are at risk. We'll include a live demo of a Rook/Ceph recovery process from a failed component. We'll show what components of Rook are recreated, how Ceph behaves during components/pods recreation, and what is the impact on the application while these failures occur (In our case the application will be MariaDB). 

https://sched.co/ekFH
Captions: 
	00:00:01,360 --> 00:00:06,160
hi my name is sagi volkov

00:00:03,280 --> 00:00:07,520
i work for redhat as a storage

00:00:06,160 --> 00:00:10,000
performance instigator

00:00:07,520 --> 00:00:11,440
in the cloud storage and data services

00:00:10,000 --> 00:00:14,080
business unit

00:00:11,440 --> 00:00:16,480
welcome to my cubecon session whatever

00:00:14,080 --> 00:00:19,520
can go wrong will go wrong ru concept

00:00:16,480 --> 00:00:19,520
and storage failures

00:00:26,320 --> 00:00:30,720
in our session agenda i'll talk a little

00:00:29,119 --> 00:00:33,760
bit about um

00:00:30,720 --> 00:00:35,920
kind of a storage introduction um i'll

00:00:33,760 --> 00:00:37,600
explain a few resiliency terms that are

00:00:35,920 --> 00:00:40,719
commonly used in the

00:00:37,600 --> 00:00:42,719
uh storage industry we'll talk about

00:00:40,719 --> 00:00:45,039
cipher storage provider rook as a

00:00:42,719 --> 00:00:46,239
storage orchestrator or manager of fan

00:00:45,039 --> 00:00:48,800
storage

00:00:46,239 --> 00:00:50,239
and we'll talk about failures um

00:00:48,800 --> 00:00:51,760
supposed to be a live demo it's going to

00:00:50,239 --> 00:00:56,640
be a recorded demo

00:00:51,760 --> 00:00:56,640
and i'll leave time for questions in the

00:00:56,840 --> 00:00:59,840
end

00:01:00,079 --> 00:01:04,400
so storage we all uh you need it we all

00:01:03,199 --> 00:01:06,159
use it every day

00:01:04,400 --> 00:01:08,560
it might be ephemeral or it might be

00:01:06,159 --> 00:01:10,720
persistent uh of course the discussion

00:01:08,560 --> 00:01:11,520
today is just on the persistent storage

00:01:10,720 --> 00:01:14,400
in a

00:01:11,520 --> 00:01:15,759
kubernetes let's do a little bit of an

00:01:14,400 --> 00:01:18,799
overview of

00:01:15,759 --> 00:01:19,520
storage up until like 10 years ago what

00:01:18,799 --> 00:01:22,640
we used to

00:01:19,520 --> 00:01:25,360
have is storage arrays this giant's

00:01:22,640 --> 00:01:26,159
giant bare metal arrays of many drives

00:01:25,360 --> 00:01:29,360
in it

00:01:26,159 --> 00:01:33,920
and they would uh connect uh into

00:01:29,360 --> 00:01:37,600
bare metal nodes or or virtual machine

00:01:33,920 --> 00:01:40,640
not a lot of dynamic in a location of a

00:01:37,600 --> 00:01:43,840
volumes and about 10 years ago

00:01:40,640 --> 00:01:47,520
software defined storage or sds uh

00:01:43,840 --> 00:01:50,960
started to be a big we start to see the

00:01:47,520 --> 00:01:54,479
uh the ability to have uh servers

00:01:50,960 --> 00:01:56,159
have uh the ability to run some sort of

00:01:54,479 --> 00:01:59,119
a software that provides

00:01:56,159 --> 00:02:00,320
storage to a cluster of servers or set

00:01:59,119 --> 00:02:04,159
of servers

00:02:00,320 --> 00:02:08,239
uh in a manner that it was still

00:02:04,159 --> 00:02:10,879
a stable resilient and redundant

00:02:08,239 --> 00:02:12,560
software-defined storage is probably the

00:02:10,879 --> 00:02:15,520
most common

00:02:12,560 --> 00:02:16,800
storage used in kubernetes nowadays you

00:02:15,520 --> 00:02:20,720
can use it in a

00:02:16,800 --> 00:02:22,640
converge manner where we have

00:02:20,720 --> 00:02:24,080
our nodes running the software defined

00:02:22,640 --> 00:02:27,040
storage and our

00:02:24,080 --> 00:02:28,800
pods and you can also use it in

00:02:27,040 --> 00:02:29,920
non-converge manner where we have some

00:02:28,800 --> 00:02:31,760
nodes running

00:02:29,920 --> 00:02:33,280
the pods the application ports and some

00:02:31,760 --> 00:02:38,480
nodes are just running

00:02:33,280 --> 00:02:38,480
uh the software defined storage pods

00:02:42,319 --> 00:02:46,239
so let's talk about a few terms that are

00:02:44,319 --> 00:02:49,519
widely used in the storage world

00:02:46,239 --> 00:02:51,519
of course they are also used in a lot of

00:02:49,519 --> 00:02:53,760
processes in the technology world and

00:02:51,519 --> 00:02:57,120
outside of the technology world

00:02:53,760 --> 00:03:00,159
availability is basically your uptime

00:02:57,120 --> 00:03:02,400
in the storage domain you're gonna hear

00:03:00,159 --> 00:03:03,760
a lot about the number of nines data

00:03:02,400 --> 00:03:09,120
storage

00:03:03,760 --> 00:03:09,120
solution has that's basically

00:03:10,280 --> 00:03:15,360
99.99 something

00:03:12,000 --> 00:03:16,959
nines which basically impact the

00:03:15,360 --> 00:03:18,959
availability of your storage the number

00:03:16,959 --> 00:03:22,080
of nines after the dot

00:03:18,959 --> 00:03:25,840
uh six nines is equivalent

00:03:22,080 --> 00:03:29,040
um to 30 once 31 seconds of

00:03:25,840 --> 00:03:30,080
downtime in one year five nines is about

00:03:29,040 --> 00:03:32,400
five minutes

00:03:30,080 --> 00:03:33,519
four nines is about one hour and three

00:03:32,400 --> 00:03:36,640
nines about

00:03:33,519 --> 00:03:40,720
nine hours of downtime in

00:03:36,640 --> 00:03:43,440
one year durability is

00:03:40,720 --> 00:03:44,560
uh like the name basically how durable

00:03:43,440 --> 00:03:47,280
is your storage

00:03:44,560 --> 00:03:48,080
uh how good is it is in in in keeping

00:03:47,280 --> 00:03:50,480
the data

00:03:48,080 --> 00:03:52,080
the data might not be accessible all the

00:03:50,480 --> 00:03:54,799
time

00:03:52,080 --> 00:03:55,840
but it is still intact so for example

00:03:54,799 --> 00:04:00,799
you lost a

00:03:55,840 --> 00:04:04,080
a data center um the data still resides

00:04:00,799 --> 00:04:06,480
or you or you hope still resides on your

00:04:04,080 --> 00:04:07,120
storage solution when the data setter

00:04:06,480 --> 00:04:09,519
comes back

00:04:07,120 --> 00:04:11,680
up the data is still there uh

00:04:09,519 --> 00:04:14,159
reliability and resiliency

00:04:11,680 --> 00:04:15,120
two terms that we are going to be

00:04:14,159 --> 00:04:18,160
focusing uh

00:04:15,120 --> 00:04:20,799
more here um basically a probability

00:04:18,160 --> 00:04:22,400
you know of how good you design your

00:04:20,799 --> 00:04:25,840
storage solution

00:04:22,400 --> 00:04:28,240
and and basically it gets impact by

00:04:25,840 --> 00:04:29,360
uh the ability of your storage solution

00:04:28,240 --> 00:04:32,240
to heal itself

00:04:29,360 --> 00:04:34,639
so again we lost a storage device or

00:04:32,240 --> 00:04:35,680
maybe we lost a node with a few storage

00:04:34,639 --> 00:04:39,120
devices

00:04:35,680 --> 00:04:41,040
uh or a data center or a switch um it's

00:04:39,120 --> 00:04:45,040
the ability for the storage solution to

00:04:41,040 --> 00:04:47,120
heal itself and to continue and provide

00:04:45,040 --> 00:04:50,000
the services the data services that it

00:04:47,120 --> 00:04:54,000
needs to provide

00:04:50,000 --> 00:04:56,880
in this diagram i'm trying to kind of

00:04:54,000 --> 00:04:57,919
explain these terms with a couple of

00:04:56,880 --> 00:04:59,440
processes

00:04:57,919 --> 00:05:01,600
of course we want availability and

00:04:59,440 --> 00:05:03,840
durability all the time

00:05:01,600 --> 00:05:05,520
throughout all the processes we have

00:05:03,840 --> 00:05:08,000
process z that starts

00:05:05,520 --> 00:05:08,880
and in the end of the process basically

00:05:08,000 --> 00:05:11,199
inserts

00:05:08,880 --> 00:05:12,639
each process inserts 10 rows into some

00:05:11,199 --> 00:05:15,120
sort of a database

00:05:12,639 --> 00:05:15,840
if the process ran 100 times we're

00:05:15,120 --> 00:05:18,479
supposed to

00:05:15,840 --> 00:05:19,520
see in the end a thousand rows in the

00:05:18,479 --> 00:05:21,280
database

00:05:19,520 --> 00:05:23,199
uh if we're seeing that that's a

00:05:21,280 --> 00:05:26,560
basically 100 availability

00:05:23,199 --> 00:05:28,160
if we're seeing 950 it's basically a 95

00:05:26,560 --> 00:05:30,639
percent availability

00:05:28,160 --> 00:05:32,160
uh of course throughout the process once

00:05:30,639 --> 00:05:34,639
we insert the data

00:05:32,160 --> 00:05:35,600
we want the data to be durable we always

00:05:34,639 --> 00:05:40,639
want to be able to

00:05:35,600 --> 00:05:42,560
query it uh some in some cases

00:05:40,639 --> 00:05:44,400
in the storage domain there's a term

00:05:42,560 --> 00:05:46,160
called retention policy

00:05:44,400 --> 00:05:47,840
maybe you only need it for one night

00:05:46,160 --> 00:05:50,720
maybe you only need the data for

00:05:47,840 --> 00:05:51,440
one year maybe just for 10 minutes but

00:05:50,720 --> 00:05:54,000
as long

00:05:51,440 --> 00:05:55,199
as uh for the duration of your retention

00:05:54,000 --> 00:05:58,479
policy that you'd

00:05:55,199 --> 00:06:01,520
define for that a piece of data

00:05:58,479 --> 00:06:03,680
uh you need to be able to uh to to have

00:06:01,520 --> 00:06:06,880
durability 100 durability

00:06:03,680 --> 00:06:09,360
throughout this retention time

00:06:06,880 --> 00:06:10,560
and then you know once process z ends

00:06:09,360 --> 00:06:13,759
process y

00:06:10,560 --> 00:06:17,120
starts to read input from process z

00:06:13,759 --> 00:06:20,000
and and and run as some other

00:06:17,120 --> 00:06:22,720
processes as well throughout all this

00:06:20,000 --> 00:06:25,440
time reliability and resiliency

00:06:22,720 --> 00:06:27,919
is a must again you lost the node you

00:06:25,440 --> 00:06:30,800
lost the device you lost the switch

00:06:27,919 --> 00:06:31,440
we want to be able to continue and run

00:06:30,800 --> 00:06:34,720
process

00:06:31,440 --> 00:06:36,160
z and run process y and complete them

00:06:34,720 --> 00:06:39,120
successfully

00:06:36,160 --> 00:06:40,479
that's basically how reliable our our

00:06:39,120 --> 00:06:43,520
stored solution

00:06:40,479 --> 00:06:44,479
and even and if we are able to complete

00:06:43,520 --> 00:06:48,000
these processes

00:06:44,479 --> 00:06:52,160
while losing some uh

00:06:48,000 --> 00:06:57,440
some sort of a data storage capabilities

00:06:52,160 --> 00:06:57,440
that shows up how good our resiliency is

00:06:59,360 --> 00:07:02,880
so let's continue with a couple of more

00:07:01,199 --> 00:07:07,280
terms mttr

00:07:02,880 --> 00:07:10,319
or mean time to repair a very common

00:07:07,280 --> 00:07:13,520
used terminology that

00:07:10,319 --> 00:07:15,919
is directly connected to your

00:07:13,520 --> 00:07:18,400
resiliency you had a problem we talked

00:07:15,919 --> 00:07:20,880
about it you lost a storage device

00:07:18,400 --> 00:07:22,479
you know it's usually either a drive or

00:07:20,880 --> 00:07:27,120
flash device or

00:07:22,479 --> 00:07:30,240
or an array of storage or switch

00:07:27,120 --> 00:07:32,479
and how easy your storage solution is

00:07:30,240 --> 00:07:35,919
actually

00:07:32,479 --> 00:07:39,759
repairing itself yeah when the storage

00:07:35,919 --> 00:07:43,360
defined software world um

00:07:39,759 --> 00:07:45,440
you can sometimes define decides between

00:07:43,360 --> 00:07:47,039
having two copies of your data versus

00:07:45,440 --> 00:07:48,639
three copies of the data

00:07:47,039 --> 00:07:50,639
for example if you have two copies of

00:07:48,639 --> 00:07:53,199
your data and for some reason

00:07:50,639 --> 00:07:54,000
you lost some devices and you now only

00:07:53,199 --> 00:07:58,240
have

00:07:54,000 --> 00:08:00,720
valid um one copy of the data

00:07:58,240 --> 00:08:02,879
until you actually repair and go back

00:08:00,720 --> 00:08:06,479
into 100 resiliency 100

00:08:02,879 --> 00:08:07,360
two copies you are basically exposing

00:08:06,479 --> 00:08:10,800
yourself

00:08:07,360 --> 00:08:14,160
uh to data unavailability if you lost

00:08:10,800 --> 00:08:15,039
more devices and now your only copy that

00:08:14,160 --> 00:08:18,400
exists

00:08:15,039 --> 00:08:22,240
is also not available so mttr

00:08:18,400 --> 00:08:26,400
is very important for your resiliency

00:08:22,240 --> 00:08:28,319
um in the sds world um

00:08:26,400 --> 00:08:30,240
we talked a little bit about converge

00:08:28,319 --> 00:08:35,120
and infrastructure

00:08:30,240 --> 00:08:38,880
versus a non-converge converge can be

00:08:35,120 --> 00:08:41,039
more more concerned in how you design

00:08:38,880 --> 00:08:41,919
and how you spread your data versus when

00:08:41,039 --> 00:08:45,600
you separate

00:08:41,919 --> 00:08:49,680
actually compute in storage in

00:08:45,600 --> 00:08:53,279
nodes mesh of course especially in sds

00:08:49,680 --> 00:08:54,000
is a big help software defined storage

00:08:53,279 --> 00:08:57,279
solutions like

00:08:54,000 --> 00:08:59,519
ceph thrive on mesh thrive on having

00:08:57,279 --> 00:09:01,200
a lot of nodes with a lot of devices the

00:08:59,519 --> 00:09:03,519
more devices you have

00:09:01,200 --> 00:09:09,839
the faster you can actually recover from

00:09:03,519 --> 00:09:09,839
a lost of a device

00:09:15,920 --> 00:09:22,000
another term is mtbf meantime between

00:09:18,959 --> 00:09:24,480
failures uh basically everything comes

00:09:22,000 --> 00:09:25,760
down into the quality of the devices

00:09:24,480 --> 00:09:29,839
that you use

00:09:25,760 --> 00:09:32,880
uh to store uh to store your data

00:09:29,839 --> 00:09:35,839
um so this measurement is basically

00:09:32,880 --> 00:09:37,760
um the between time from the last

00:09:35,839 --> 00:09:41,040
failure to the next failure

00:09:37,760 --> 00:09:44,000
of a device or or a storage system

00:09:41,040 --> 00:09:45,279
uh just for you know example uh an

00:09:44,000 --> 00:09:48,480
enterprise grade

00:09:45,279 --> 00:09:51,680
drive have about 800 000

00:09:48,480 --> 00:09:54,800
hours of mtbf which is roughly

00:09:51,680 --> 00:09:57,440
uh 90 years sounds a lot

00:09:54,800 --> 00:09:58,720
but then if your storage solution have

00:09:57,440 --> 00:10:00,800
90 drives

00:09:58,720 --> 00:10:02,320
in that example that basically means

00:10:00,800 --> 00:10:03,279
that you're going to have one failure

00:10:02,320 --> 00:10:05,440
every year

00:10:03,279 --> 00:10:06,399
if your stored solution have or data

00:10:05,440 --> 00:10:09,760
center

00:10:06,399 --> 00:10:11,920
uh have 900 drives basically means that

00:10:09,760 --> 00:10:12,800
you're going to have a a failure every

00:10:11,920 --> 00:10:16,160
five weeks

00:10:12,800 --> 00:10:19,600
in one of the drives one of the 900

00:10:16,160 --> 00:10:23,040
drives so you need a very good um

00:10:19,600 --> 00:10:26,000
software above the devices to actually

00:10:23,040 --> 00:10:27,920
uh make sure that you still have

00:10:26,000 --> 00:10:31,920
availability that's where the

00:10:27,920 --> 00:10:34,320
two copies of the data are three copies

00:10:31,920 --> 00:10:36,240
erasure coding of spreading the data

00:10:34,320 --> 00:10:38,560
across all the devices

00:10:36,240 --> 00:10:41,120
uh that's where all these things come

00:10:38,560 --> 00:10:41,120
into play

00:10:42,240 --> 00:10:46,880
so last couple of terms rto recovery

00:10:44,959 --> 00:10:48,800
time objective basically how long can

00:10:46,880 --> 00:10:49,440
your process or your company or data

00:10:48,800 --> 00:10:52,079
center

00:10:49,440 --> 00:10:53,839
can survive without access to data

00:10:52,079 --> 00:10:56,079
usually these

00:10:53,839 --> 00:10:57,120
your rto needs to be as short as

00:10:56,079 --> 00:11:00,240
possible

00:10:57,120 --> 00:11:04,399
you define this usually by

00:11:00,240 --> 00:11:07,519
by tiers in in enterprise organization

00:11:04,399 --> 00:11:11,040
some applications cannot have

00:11:07,519 --> 00:11:14,240
a no access for example

00:11:11,040 --> 00:11:17,040
no more than five minutes in one year or

00:11:14,240 --> 00:11:19,279
30 seconds in one year other

00:11:17,040 --> 00:11:21,040
applications are in lower tiers and

00:11:19,279 --> 00:11:22,880
yeah it's fine for them to not have

00:11:21,040 --> 00:11:25,920
access to the data for you know

00:11:22,880 --> 00:11:30,240
one day in a year rpo

00:11:25,920 --> 00:11:32,800
is your recovery point objective

00:11:30,240 --> 00:11:34,160
besides counting on your storage

00:11:32,800 --> 00:11:36,640
solution to

00:11:34,160 --> 00:11:38,079
maintain all the copies and run all the

00:11:36,640 --> 00:11:40,959
time and provide adequate

00:11:38,079 --> 00:11:41,279
performance you should always thrive to

00:11:40,959 --> 00:11:43,839
do

00:11:41,279 --> 00:11:45,200
also run backups outside of this uh

00:11:43,839 --> 00:11:48,640
storage solution

00:11:45,200 --> 00:11:52,000
and this basically uh uh impact your uh

00:11:48,640 --> 00:11:53,440
rpo uh how many backups you have in case

00:11:52,000 --> 00:11:55,600
you lost everything in your

00:11:53,440 --> 00:11:57,760
storage solution will basically impact

00:11:55,600 --> 00:12:00,240
your recovery point object

00:11:57,760 --> 00:12:01,279
objective if you're on a backup every uh

00:12:00,240 --> 00:12:05,200
one hour

00:12:01,279 --> 00:12:07,680
that means that uh your rpo is of

00:12:05,200 --> 00:12:07,680
one hour

00:12:08,240 --> 00:12:12,160
all right so now let's continue and

00:12:10,399 --> 00:12:15,839
start to talk about seven rook

00:12:12,160 --> 00:12:18,560
um ceph is a unified storage system

00:12:15,839 --> 00:12:20,320
uh software defined storage it's been

00:12:18,560 --> 00:12:22,240
around for a long time

00:12:20,320 --> 00:12:23,360
it provides basically the ability to

00:12:22,240 --> 00:12:26,560
have object

00:12:23,360 --> 00:12:29,200
and block a and file

00:12:26,560 --> 00:12:30,480
uh file from sffs which is basically in

00:12:29,200 --> 00:12:31,760
the kubernetes world will be the

00:12:30,480 --> 00:12:34,959
equivalent of

00:12:31,760 --> 00:12:38,079
ada rwx uh it's a shareable size

00:12:34,959 --> 00:12:41,120
file system rbd is a

00:12:38,079 --> 00:12:43,360
a a pure block device and rgw is the

00:12:41,120 --> 00:12:46,959
ability to interact with object

00:12:43,360 --> 00:12:50,160
using s3 for example

00:12:46,959 --> 00:12:53,279
there are a few components uh for

00:12:50,160 --> 00:12:54,399
uh for ceph and the cephmon or the

00:12:53,279 --> 00:12:58,160
monitor

00:12:54,399 --> 00:13:00,720
um is basically uh um

00:12:58,160 --> 00:13:02,079
the the coordination or took the

00:13:00,720 --> 00:13:06,160
coordinator of

00:13:02,079 --> 00:13:09,360
uh all data and components in

00:13:06,160 --> 00:13:11,680
in the cluster uh it's using a paxos

00:13:09,360 --> 00:13:13,120
to keep itself alive there's minimum of

00:13:11,680 --> 00:13:15,760
three of these

00:13:13,120 --> 00:13:16,399
processes in a cluster and you can go up

00:13:15,760 --> 00:13:20,000
to

00:13:16,399 --> 00:13:23,120
seven the manager mainly concentrate on

00:13:20,000 --> 00:13:26,320
on real-time metrics um and

00:13:23,120 --> 00:13:27,040
other management functions that are for

00:13:26,320 --> 00:13:29,440
the cfa

00:13:27,040 --> 00:13:31,040
cluster there's one active and you can

00:13:29,440 --> 00:13:34,399
go to have one active and one

00:13:31,040 --> 00:13:35,040
standby sephora is the easy uh are very

00:13:34,399 --> 00:13:37,440
important

00:13:35,040 --> 00:13:38,720
processes these are basically the uh i

00:13:37,440 --> 00:13:42,800
process that

00:13:38,720 --> 00:13:45,920
attached into a a a device

00:13:42,800 --> 00:13:49,600
that can provide storage like an lgd sdd

00:13:45,920 --> 00:13:52,320
nvme any type of a block device

00:13:49,600 --> 00:13:53,120
and then in return the ceph cluster

00:13:52,320 --> 00:13:56,000
aggregates

00:13:53,120 --> 00:13:56,639
all these devices create pools out of

00:13:56,000 --> 00:13:59,920
them

00:13:56,639 --> 00:14:02,160
and able to provide uh the storage

00:13:59,920 --> 00:14:03,120
capabilities for their clients in our

00:14:02,160 --> 00:14:06,399
case

00:14:03,120 --> 00:14:10,639
are kubernetes and pvcs

00:14:06,399 --> 00:14:13,279
whether it's an object a block or

00:14:10,639 --> 00:14:13,279
or file

00:14:13,920 --> 00:14:17,360
a couple of other processes that are not

00:14:15,680 --> 00:14:20,399
showing up here on the

00:14:17,360 --> 00:14:21,040
presentation rgw is basically what

00:14:20,399 --> 00:14:25,360
allows

00:14:21,040 --> 00:14:27,839
uh for object storage access um

00:14:25,360 --> 00:14:29,600
usually starts with one process and if

00:14:27,839 --> 00:14:30,399
you need to scale up to you know

00:14:29,600 --> 00:14:32,639
millions of

00:14:30,399 --> 00:14:34,000
accesses or millions of object accesses

00:14:32,639 --> 00:14:36,560
you can add more

00:14:34,000 --> 00:14:38,639
mds is the ability basically to access

00:14:36,560 --> 00:14:41,120
sffs

00:14:38,639 --> 00:14:42,800
you start with two of these processes

00:14:41,120 --> 00:14:47,040
one is

00:14:42,800 --> 00:14:47,040
active and one in a standby uh mode

00:14:47,120 --> 00:14:53,920
and now let's talk about uh our rook uh

00:14:50,959 --> 00:14:55,760
um the rook is a basically a storage

00:14:53,920 --> 00:14:58,720
manager for kubernetes

00:14:55,760 --> 00:15:00,959
in our case we use it with ceph i like

00:14:58,720 --> 00:15:03,680
to call it the orchestrator of

00:15:00,959 --> 00:15:04,959
all things as you can see on these

00:15:03,680 --> 00:15:08,160
slides

00:15:04,959 --> 00:15:09,279
anything blue is kind of more on the

00:15:08,160 --> 00:15:11,920
rook level

00:15:09,279 --> 00:15:14,320
we have the rook and ceph operator

00:15:11,920 --> 00:15:16,959
support that runs in one of the nodes

00:15:14,320 --> 00:15:17,600
um we have discovery pods that runs on

00:15:16,959 --> 00:15:20,720
any

00:15:17,600 --> 00:15:22,079
uh node that we would like to uh that

00:15:20,720 --> 00:15:24,639
will provide

00:15:22,079 --> 00:15:25,519
storage that have like block devices

00:15:24,639 --> 00:15:28,079
that cephei

00:15:25,519 --> 00:15:28,560
the ceph cluster is going to uh to use

00:15:28,079 --> 00:15:30,880
you can

00:15:28,560 --> 00:15:31,920
you configure you can configure these by

00:15:30,880 --> 00:15:34,320
uh labels

00:15:31,920 --> 00:15:35,440
and things and things like that the

00:15:34,320 --> 00:15:38,720
cfcsi

00:15:35,440 --> 00:15:42,160
uh level are basically the attacher

00:15:38,720 --> 00:15:45,199
uh and detach attach pods

00:15:42,160 --> 00:15:48,560
that exist on the in every nodes

00:15:45,199 --> 00:15:52,079
whether it's for a block or for cfs

00:15:48,560 --> 00:15:56,000
um and then you have the seth demons

00:15:52,079 --> 00:15:58,000
that rook is controlling um of course

00:15:56,000 --> 00:15:59,360
each of these things are are basically

00:15:58,000 --> 00:16:02,800
uh pods

00:15:59,360 --> 00:16:05,680
you have uh mon pods uh that runs

00:16:02,800 --> 00:16:06,320
on uh you have three minimum of three m1

00:16:05,680 --> 00:16:08,959
pods

00:16:06,320 --> 00:16:10,560
so on the next node uh that we don't uh

00:16:08,959 --> 00:16:13,360
show up in this slide

00:16:10,560 --> 00:16:14,240
we don't necessarily have a mon pod um

00:16:13,360 --> 00:16:16,639
osd's

00:16:14,240 --> 00:16:17,680
will actually uh we will have osds

00:16:16,639 --> 00:16:21,519
wherever

00:16:17,680 --> 00:16:24,560
we will have storage that we consume

00:16:21,519 --> 00:16:26,000
on these nodes and we talked about rgw's

00:16:24,560 --> 00:16:26,639
we've talked about the manager having

00:16:26,000 --> 00:16:30,000
one of them

00:16:26,639 --> 00:16:32,560
mds um there's a pod here for a

00:16:30,000 --> 00:16:34,800
mirroring of rbd to a different

00:16:32,560 --> 00:16:36,079
rook surf cluster on a completely

00:16:34,800 --> 00:16:38,639
different kubernetes

00:16:36,079 --> 00:16:40,480
cluster and crash collector in case

00:16:38,639 --> 00:16:44,800
things go bad and we need to uh

00:16:40,480 --> 00:16:44,800
collect all kinds of logs

00:16:45,360 --> 00:16:49,120
so let's talk about the rook and safe

00:16:47,040 --> 00:16:51,759
resiliency in kubernetes

00:16:49,120 --> 00:16:53,279
as discussed previously every ceph

00:16:51,759 --> 00:16:56,959
process is basically

00:16:53,279 --> 00:16:58,240
a pod so we want to understand and what

00:16:56,959 --> 00:17:01,120
happens when a pod

00:16:58,240 --> 00:17:02,240
fails going back to all the terminology

00:17:01,120 --> 00:17:05,919
that we use

00:17:02,240 --> 00:17:08,480
of course this impacts our resiliency um

00:17:05,919 --> 00:17:09,600
so you know the mons are uh very

00:17:08,480 --> 00:17:11,600
important

00:17:09,600 --> 00:17:13,839
processes that's why we have three of

00:17:11,600 --> 00:17:17,839
them that spread over uh

00:17:13,839 --> 00:17:21,360
three nodes or more uh the mds which

00:17:17,839 --> 00:17:24,480
again provides access to uh cfs we have

00:17:21,360 --> 00:17:27,199
a minimum of two uh one is in an active

00:17:24,480 --> 00:17:28,000
active mode and one in standby uh mode

00:17:27,199 --> 00:17:30,640
um

00:17:28,000 --> 00:17:31,520
none of the mons or the mgl the manager

00:17:30,640 --> 00:17:34,160
processes

00:17:31,520 --> 00:17:34,720
uh of ceph are actually in the data path

00:17:34,160 --> 00:17:37,760
when we

00:17:34,720 --> 00:17:41,039
uh access it using a

00:17:37,760 --> 00:17:44,799
a rbd for example uh

00:17:41,039 --> 00:17:49,280
you know we talked about uh uh rpos and

00:17:44,799 --> 00:17:51,840
and and backup and and and durability

00:17:49,280 --> 00:17:53,039
if you have two data centers you have

00:17:51,840 --> 00:17:56,400
two kubernetes

00:17:53,039 --> 00:17:59,600
uh uh clusters you can

00:17:56,400 --> 00:18:01,840
use uh self built in replication uh

00:17:59,600 --> 00:18:03,600
such as the self-rbd mirroring to

00:18:01,840 --> 00:18:06,640
basically mirror a pv

00:18:03,600 --> 00:18:10,080
a pv uh in a certain pool from

00:18:06,640 --> 00:18:11,039
one safe cluster in kubernetes cluster a

00:18:10,080 --> 00:18:13,600
to

00:18:11,039 --> 00:18:14,960
a rook safe cluster in kubernetes

00:18:13,600 --> 00:18:18,000
cluster a b

00:18:14,960 --> 00:18:19,280
um ceph also offer uh the same

00:18:18,000 --> 00:18:22,640
capabilities for

00:18:19,280 --> 00:18:26,000
uh object storage it is ga in

00:18:22,640 --> 00:18:31,360
the self project it's still uh not

00:18:26,000 --> 00:18:34,480
a ga in rook it will be ga in the next

00:18:31,360 --> 00:18:34,480
one or two releases

00:18:35,600 --> 00:18:40,880
so let's talk about um some demos that

00:18:39,039 --> 00:18:42,440
i want to show we're going to look at a

00:18:40,880 --> 00:18:46,000
few scenarios

00:18:42,440 --> 00:18:50,240
um basically of osd pods

00:18:46,000 --> 00:18:52,559
failures and how rook and chef behave

00:18:50,240 --> 00:18:53,600
what we're basically going to do is

00:18:52,559 --> 00:18:57,280
constantly

00:18:53,600 --> 00:18:57,919
create stress on the storage solution in

00:18:57,280 --> 00:19:01,600
our case

00:18:57,919 --> 00:19:02,160
rook and ceph i and then one scenario

00:19:01,600 --> 00:19:06,400
will be

00:19:02,160 --> 00:19:07,120
um when a a developer or an admin or

00:19:06,400 --> 00:19:09,120
person

00:19:07,120 --> 00:19:10,559
delete by mistake in osd pod what

00:19:09,120 --> 00:19:12,880
happens

00:19:10,559 --> 00:19:13,600
what happens when you reboot a node or

00:19:12,880 --> 00:19:16,720
have

00:19:13,600 --> 00:19:20,640
osds osd pods on it

00:19:16,720 --> 00:19:24,080
remember osd is the process that consume

00:19:20,640 --> 00:19:29,200
a blocked device on a node and then uh

00:19:24,080 --> 00:19:32,720
is part of the whole cf a cluster

00:19:29,200 --> 00:19:34,720
that then provides a storage to our

00:19:32,720 --> 00:19:36,840
kubernetes application we're also going

00:19:34,720 --> 00:19:39,840
to look at what happens when we

00:19:36,840 --> 00:19:40,320
lose one of the devices that the nosd

00:19:39,840 --> 00:19:43,520
pods

00:19:40,320 --> 00:19:46,880
uh is using um i've

00:19:43,520 --> 00:19:49,919
all this demo is running on aws very

00:19:46,880 --> 00:19:51,600
small clusters like three masters

00:19:49,919 --> 00:19:53,039
it's gonna be three nodes that are going

00:19:51,600 --> 00:19:53,840
to run rook and safe and provide the

00:19:53,039 --> 00:19:56,080
storage

00:19:53,840 --> 00:19:56,880
and three nodes that are going to run

00:19:56,080 --> 00:19:59,200
and

00:19:56,880 --> 00:20:00,880
applications uh that will consume and

00:19:59,200 --> 00:20:04,320
stress the storage

00:20:00,880 --> 00:20:04,720
uh i'm using a a project named sherlock

00:20:04,320 --> 00:20:07,039
which i

00:20:04,720 --> 00:20:09,520
started a couple of months ago it's

00:20:07,039 --> 00:20:12,640
basically a

00:20:09,520 --> 00:20:16,880
project that aims to

00:20:12,640 --> 00:20:21,280
check and test and stress uh performance

00:20:16,880 --> 00:20:24,240
on all sorts of stored solution

00:20:21,280 --> 00:20:24,480
in the kubernetes domain so you can run

00:20:24,240 --> 00:20:26,880
it

00:20:24,480 --> 00:20:28,240
not only on rook and surf but on any

00:20:26,880 --> 00:20:31,840
other asds

00:20:28,240 --> 00:20:33,679
provider i also collect the statistical

00:20:31,840 --> 00:20:36,159
information from

00:20:33,679 --> 00:20:38,000
the actual nodes that provide the

00:20:36,159 --> 00:20:38,960
storage and the nodes that are consumed

00:20:38,000 --> 00:20:42,559
with storage

00:20:38,960 --> 00:20:46,799
so you can take a look at that as well

00:20:42,559 --> 00:20:46,799
and let's move into the demos

00:20:47,760 --> 00:20:54,159
all right so let's start the first demo

00:20:51,200 --> 00:20:55,679
i've kind of split my uh terminal into

00:20:54,159 --> 00:20:58,720
four sections hopefully it's

00:20:55,679 --> 00:21:02,159
clear on the top right

00:20:58,720 --> 00:21:05,280
uh section i have a ceph command

00:21:02,159 --> 00:21:08,400
called zephosd3 that is

00:21:05,280 --> 00:21:09,520
constantly running using a watch um it

00:21:08,400 --> 00:21:12,880
basically shows

00:21:09,520 --> 00:21:12,880
the ceph cluster

00:21:12,960 --> 00:21:17,360
osd3 as the command says we can see that

00:21:15,760 --> 00:21:19,760
we have three nodes that

00:21:17,360 --> 00:21:20,720
provide the storage on each of the nodes

00:21:19,760 --> 00:21:24,799
we have

00:21:20,720 --> 00:21:27,919
uh two osds that

00:21:24,799 --> 00:21:30,559
consume two ssd devices

00:21:27,919 --> 00:21:31,919
on them and provide it back into the

00:21:30,559 --> 00:21:34,559
safe cluster

00:21:31,919 --> 00:21:35,360
on the top left i have a cube cuddle

00:21:34,559 --> 00:21:38,480
command

00:21:35,360 --> 00:21:39,200
looking at uh the pods that exist on the

00:21:38,480 --> 00:21:43,520
rooks f

00:21:39,200 --> 00:21:46,799
namespace i am only grabbing for the

00:21:43,520 --> 00:21:50,880
osd pods these are the ones that we are

00:21:46,799 --> 00:21:54,559
interested in as you can see we have

00:21:50,880 --> 00:21:54,559
six osds on the

00:21:54,799 --> 00:22:00,159
top right and we have six osd pods on

00:21:58,080 --> 00:22:03,280
the top left

00:22:00,159 --> 00:22:06,880
i have a project called mysql

00:22:03,280 --> 00:22:10,840
it has 12 databases uh

00:22:06,880 --> 00:22:13,919
12 mysql pods are running on these

00:22:10,840 --> 00:22:17,600
three nodes

00:22:13,919 --> 00:22:21,679
each mysql database is

00:22:17,600 --> 00:22:25,679
using 100 gigabyte pvc i also have

00:22:21,679 --> 00:22:29,039
12 syspent jobs

00:22:25,679 --> 00:22:32,720
suspect uh pods running uh on a

00:22:29,039 --> 00:22:35,520
on these 12 mysql databases and

00:22:32,720 --> 00:22:36,480
um this is done of course using the

00:22:35,520 --> 00:22:38,640
sherlock

00:22:36,480 --> 00:22:39,520
github project that i've previously

00:22:38,640 --> 00:22:42,480
mentioned

00:22:39,520 --> 00:22:44,000
i'm just going to pick one of these uh

00:22:42,480 --> 00:22:47,440
you know one of these nodes

00:22:44,000 --> 00:22:52,159
one of these mysql jobs

00:22:47,440 --> 00:22:55,840
and i'm going to basically

00:22:52,159 --> 00:22:58,559
constantly monitor the logs of

00:22:55,840 --> 00:22:59,919
this uh job as you can see it's a

00:22:58,559 --> 00:23:03,200
typical

00:22:59,919 --> 00:23:04,720
uh suspension output every 10 seconds

00:23:03,200 --> 00:23:08,000
shows like the number of transactions

00:23:04,720 --> 00:23:11,039
per second uh latency and

00:23:08,000 --> 00:23:12,400
things like that um

00:23:11,039 --> 00:23:14,240
what i'm going to do is i'm going to

00:23:12,400 --> 00:23:17,280
pick one of these

00:23:14,240 --> 00:23:20,480
uh um basically uh

00:23:17,280 --> 00:23:24,080
uh osd pods and um

00:23:20,480 --> 00:23:27,679
i will uh delete uh one of them

00:23:24,080 --> 00:23:30,799
um in this case i'm just going to pick a

00:23:27,679 --> 00:23:34,880
osd 3 and

00:23:30,799 --> 00:23:38,400
what is interesting to monitor

00:23:34,880 --> 00:23:41,840
is to look at

00:23:38,400 --> 00:23:44,320
the top left

00:23:41,840 --> 00:23:46,080
of what happens to the pod i kill it if

00:23:44,320 --> 00:23:49,120
a new one comes in

00:23:46,080 --> 00:23:52,480
and on the top right

00:23:49,120 --> 00:23:55,520
um since we are looking at the osd 3

00:23:52,480 --> 00:23:59,120
that's osd 3 right here what is

00:23:55,520 --> 00:24:01,039
a what is happening to the the status of

00:23:59,120 --> 00:24:04,400
that

00:24:01,039 --> 00:24:07,600
osd component is it staying up

00:24:04,400 --> 00:24:08,080
for how long there's another uh command

00:24:07,600 --> 00:24:10,720
here

00:24:08,080 --> 00:24:11,520
uh from ceph that basically dumping all

00:24:10,720 --> 00:24:14,640
the

00:24:11,520 --> 00:24:16,080
uh pg's or placement groups that and the

00:24:14,640 --> 00:24:18,240
cf cluster has

00:24:16,080 --> 00:24:19,279
as you can see now we have 81 pages and

00:24:18,240 --> 00:24:22,640
they're all at the uh

00:24:19,279 --> 00:24:25,520
all at an active and clean if seth uh

00:24:22,640 --> 00:24:26,400
uh see or uh uh that something is wrong

00:24:25,520 --> 00:24:29,760
with one of the

00:24:26,400 --> 00:24:30,320
osds uh maybe we need to uh recover one

00:24:29,760 --> 00:24:33,840
of

00:24:30,320 --> 00:24:36,480
those these you're gonna see that on

00:24:33,840 --> 00:24:36,960
on this uh dump command all right so

00:24:36,480 --> 00:24:40,240
let's

00:24:36,960 --> 00:24:40,799
uh dump let's delete the pod and see

00:24:40,240 --> 00:24:43,279
what's

00:24:40,799 --> 00:24:43,279
happening

00:24:43,840 --> 00:24:49,520
so the pod is a deleted you can see it's

00:24:47,440 --> 00:24:51,840
terminated you can see that there's

00:24:49,520 --> 00:24:55,039
already a new

00:24:51,840 --> 00:24:56,240
osd three pod started and it's already

00:24:55,039 --> 00:24:59,520
up and running

00:24:56,240 --> 00:25:03,039
um osd 3 was marked here as down

00:24:59,520 --> 00:25:06,320
for uh uh you know one or two seconds

00:25:03,039 --> 00:25:09,120
and as you can see from the uh

00:25:06,320 --> 00:25:09,520
dumping of the pages in here that seph

00:25:09,120 --> 00:25:13,840
is

00:25:09,520 --> 00:25:16,400
uh uh working in making sure that

00:25:13,840 --> 00:25:17,600
the new osd pod that is using basically

00:25:16,400 --> 00:25:20,720
the same device

00:25:17,600 --> 00:25:24,320
as previously has a all

00:25:20,720 --> 00:25:28,080
the right placement groups and

00:25:24,320 --> 00:25:30,320
all the pages are clean and as you can

00:25:28,080 --> 00:25:31,440
see all the placement groups are clean

00:25:30,320 --> 00:25:34,480
and as you can see

00:25:31,440 --> 00:25:37,919
everything went back into uh um

00:25:34,480 --> 00:25:39,600
uh normal state in in terms of uh how

00:25:37,919 --> 00:25:43,279
cfa behaves

00:25:39,600 --> 00:25:47,760
you can also see how in the output of

00:25:43,279 --> 00:25:51,120
sysbench um you can see that

00:25:47,760 --> 00:25:54,840
right here you can see how there was

00:25:51,120 --> 00:25:57,440
a little bit of a drop in the

00:25:54,840 --> 00:26:00,559
iops uh that did

00:25:57,440 --> 00:26:03,679
this cisband job was running and that is

00:26:00,559 --> 00:26:04,240
uh of course acceptable there was some

00:26:03,679 --> 00:26:07,919
kind of

00:26:04,240 --> 00:26:09,360
uh um an eye operation that internally

00:26:07,919 --> 00:26:12,480
ceph

00:26:09,360 --> 00:26:14,320
was doing this database uh

00:26:12,480 --> 00:26:15,760
might have been reading from uh

00:26:14,320 --> 00:26:18,960
placement groups that was

00:26:15,760 --> 00:26:22,000
on the osd that i have deleted

00:26:18,960 --> 00:26:23,120
so for a brief second it had to get the

00:26:22,000 --> 00:26:26,880
pages from

00:26:23,120 --> 00:26:29,679
um a different osd that's uh

00:26:26,880 --> 00:26:31,520
that's the job of the sf months to

00:26:29,679 --> 00:26:35,520
basically tell all the clients

00:26:31,520 --> 00:26:38,960
uh to get these pages from a a different

00:26:35,520 --> 00:26:42,960
location and that's the end of this

00:26:38,960 --> 00:26:42,960
uh first uh demo

00:26:43,360 --> 00:26:49,120
all right so let's uh do our second demo

00:26:46,960 --> 00:26:50,240
same setup what i'm going to do is

00:26:49,120 --> 00:26:53,279
actually

00:26:50,240 --> 00:26:57,520
now reboot one of the

00:26:53,279 --> 00:27:00,720
nodes uh that are running rook and ceph

00:26:57,520 --> 00:27:03,279
so which basically means that uh

00:27:00,720 --> 00:27:04,559
since each node have two osds and we

00:27:03,279 --> 00:27:08,000
have total of six

00:27:04,559 --> 00:27:12,000
osds we are basically taking down uh

00:27:08,000 --> 00:27:15,120
a third of our uh storage

00:27:12,000 --> 00:27:17,360
um for uh for a brief time i'm gonna

00:27:15,120 --> 00:27:17,360
pick

00:27:17,440 --> 00:27:24,000
this node

00:27:20,559 --> 00:27:35,120
let me start monitoring

00:27:24,000 --> 00:27:40,159
one of the suspense pods

00:27:35,120 --> 00:27:40,159
243 number eight

00:27:42,000 --> 00:27:48,399
and i'm going to uh reboot uh

00:27:45,120 --> 00:27:51,840
this node and on the bottom

00:27:48,399 --> 00:27:55,600
left i'm going to look at the uh the

00:27:51,840 --> 00:27:58,399
node status uh since this is aws

00:27:55,600 --> 00:28:00,480
things are probably uh you know

00:27:58,399 --> 00:28:02,080
rebooting so fast i'm not sure how much

00:28:00,480 --> 00:28:03,760
of a

00:28:02,080 --> 00:28:05,360
cube cattle is actually going to capture

00:28:03,760 --> 00:28:07,279
that the nodes are being down

00:28:05,360 --> 00:28:09,039
but as you can see on the bottom on the

00:28:07,279 --> 00:28:12,159
top right

00:28:09,039 --> 00:28:15,200
ceph does it definitely starts to see

00:28:12,159 --> 00:28:16,480
some rpgs that are not available it's

00:28:15,200 --> 00:28:20,320
marking the osds

00:28:16,480 --> 00:28:23,679
as down starting to

00:28:20,320 --> 00:28:25,760
make sure that it's

00:28:23,679 --> 00:28:26,880
understand where it needs to move these

00:28:25,760 --> 00:28:29,520
pages

00:28:26,880 --> 00:28:30,399
that were primary on these nodes and now

00:28:29,520 --> 00:28:33,279
on the

00:28:30,399 --> 00:28:35,120
top left you can see that we have two

00:28:33,279 --> 00:28:38,799
new osd ports

00:28:35,120 --> 00:28:40,960
starting and once they are going to be

00:28:38,799 --> 00:28:42,000
up and running on the top right we're

00:28:40,960 --> 00:28:45,679
going to see

00:28:42,000 --> 00:28:48,720
uh that um osd one and four

00:28:45,679 --> 00:28:50,399
they are now marked as up and peering

00:28:48,720 --> 00:28:51,600
process is starting in ceph and it's

00:28:50,399 --> 00:28:54,960
starting to

00:28:51,600 --> 00:28:58,000
move whatever pages it needs back

00:28:54,960 --> 00:29:03,600
into uh into those

00:28:58,000 --> 00:29:03,600
uh new sds that just uh uh came up

00:29:03,760 --> 00:29:10,080
now again this is a third of uh

00:29:06,799 --> 00:29:13,600
the storage and as you can see

00:29:10,080 --> 00:29:16,720
um we are uh uh looking at

00:29:13,600 --> 00:29:20,000
the logs of uh sysbench um

00:29:16,720 --> 00:29:23,039
there was a um a brief time that

00:29:20,000 --> 00:29:23,760
um we didn't see any uh transaction

00:29:23,039 --> 00:29:27,279
transaction

00:29:23,760 --> 00:29:29,039
were were basically posed or have a

00:29:27,279 --> 00:29:33,039
higher latencies

00:29:29,039 --> 00:29:33,039
but now everything continues

00:29:33,919 --> 00:29:40,960
so these were basically the two demos um

00:29:37,440 --> 00:29:44,640
replacing new devices for

00:29:40,960 --> 00:29:48,240
osds is basically the same thing as the

00:29:44,640 --> 00:29:50,480
last demo that i've shown ceph is going

00:29:48,240 --> 00:29:54,240
to

00:29:50,480 --> 00:29:57,679
have these new devices part of the osds

00:29:54,240 --> 00:30:00,159
and then move all the placement groups

00:29:57,679 --> 00:30:02,399
into or re-spread the placement groups

00:30:00,159 --> 00:30:07,039
using these devices as well

00:30:02,399 --> 00:30:10,000
i will now answer any questions that

00:30:07,039 --> 00:30:14,480
you guys might have and thanks for

00:30:10,000 --> 00:30:14,480

YouTube URL: https://www.youtube.com/watch?v=e0_M1Keed6M


