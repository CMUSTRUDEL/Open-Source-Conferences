Title: Intro & Deep Dive: Kubernetes Data Protection WG - Xing Yang, VMware & Xiangqian Yu, Google
Publication date: 2020-11-23
Playlist: KubeCon + CloudNativeCon North America 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon Europe 2021 Virtual from May 4–7, 2021. Learn more at https://kubecon.io. The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

Intro & Deep Dive: Kubernetes Data Protection WG - Xing Yang, VMware & Xiangqian Yu, Google  

Data Protection WG in Kubernetes was formed following discussions at KubeCon in San Diego. This is a Working Group dedicated to promoting data protection support in Kubernetes, identifying missing functionality and working together across multiple SIGs to design features to enable data protection in Kubernetes. In this session, the co-chairs of this WG will explain the motivation behind the formation of this WG, the charter of this WG, who are involved, what is the current state of data protection in Kubernetes and where it is heading in the future. They will also talk about how interested parties (including backup and storage vendors, application developers, and end users, etc.) can join this WG and contribute to this effort. Details of the WG can be found here: https://github.com/kubernetes/community/tree/master/wg-data-protection. 

https://sched.co/ekGO
Captions: 
	00:00:02,320 --> 00:00:06,240
hello everyone

00:00:03,760 --> 00:00:07,279
today chen chen and i will give a

00:00:06,240 --> 00:00:09,280
presentation

00:00:07,279 --> 00:00:12,160
about the data protection working group

00:00:09,280 --> 00:00:12,160
in kubernetes

00:00:14,880 --> 00:00:19,279
my name is xin yang i work at vmware in

00:00:17,520 --> 00:00:22,240
the cloud storage team

00:00:19,279 --> 00:00:24,320
i'm a co-chair of kubernetes sig storage

00:00:22,240 --> 00:00:25,039
i also co-lead the data protection

00:00:24,320 --> 00:00:28,800
working group

00:00:25,039 --> 00:00:30,960
with xiangtian hello everyone

00:00:28,800 --> 00:00:31,920
this is jean-chan i am a software

00:00:30,960 --> 00:00:34,800
engineer

00:00:31,920 --> 00:00:36,559
from the cloud department i work with

00:00:34,800 --> 00:00:41,280
shin heavy on

00:00:36,559 --> 00:00:41,280
providing data protection working group

00:00:42,840 --> 00:00:45,840
support

00:00:46,239 --> 00:00:50,800
here is today's agenda first we will

00:00:48,879 --> 00:00:52,079
talk about the history behind this data

00:00:50,800 --> 00:00:54,399
protection group

00:00:52,079 --> 00:00:56,160
who are involved why we need data

00:00:54,399 --> 00:00:58,320
protection in kubernetes

00:00:56,160 --> 00:01:00,160
we will talk about the charter the data

00:00:58,320 --> 00:01:02,239
protection definition

00:01:00,160 --> 00:01:03,280
what other existing building blocks in

00:01:02,239 --> 00:01:06,159
kubernetes

00:01:03,280 --> 00:01:07,600
and what are still missing and what we

00:01:06,159 --> 00:01:10,400
are working on in order to

00:01:07,600 --> 00:01:10,880
fill those gaps and finally we'll talk

00:01:10,400 --> 00:01:13,840
about

00:01:10,880 --> 00:01:13,840
how to get involved

00:01:16,080 --> 00:01:21,600
in kubernetes only snapshot was

00:01:18,479 --> 00:01:25,360
introduced as an ara feature in 1.12

00:01:21,600 --> 00:01:28,720
and was promoted to beta in 1.17 release

00:01:25,360 --> 00:01:31,119
and it is targeting ga in 1.20

00:01:28,720 --> 00:01:33,759
this allows us to backup and restore a

00:01:31,119 --> 00:01:36,960
volume based on volume snapshots

00:01:33,759 --> 00:01:39,280
however many things are still missing

00:01:36,960 --> 00:01:40,159
at cubecom in san diego at the end of

00:01:39,280 --> 00:01:43,040
last year

00:01:40,159 --> 00:01:44,079
we discussed about this and decided that

00:01:43,040 --> 00:01:47,759
we should form a

00:01:44,079 --> 00:01:49,360
working group to focus on this area

00:01:47,759 --> 00:01:51,600
the wahine group was formally

00:01:49,360 --> 00:01:53,920
established in january this year

00:01:51,600 --> 00:01:54,640
and we have been holding bi-weekly

00:01:53,920 --> 00:01:57,200
meetings

00:01:54,640 --> 00:01:57,200
since then

00:01:58,880 --> 00:02:03,759
as shown here many companies have been

00:02:01,759 --> 00:02:06,560
supporting this working group

00:02:03,759 --> 00:02:08,399
both backup vendors and storage vendors

00:02:06,560 --> 00:02:11,200
have been participating

00:02:08,399 --> 00:02:11,200
in the working group

00:02:13,200 --> 00:02:18,560
i will talk about why we need data

00:02:15,599 --> 00:02:21,840
protection in kubernetes

00:02:18,560 --> 00:02:22,560
applications have been around forever

00:02:21,840 --> 00:02:25,040
however

00:02:22,560 --> 00:02:26,319
the architecture of applications has

00:02:25,040 --> 00:02:29,360
changed drastically

00:02:26,319 --> 00:02:30,080
or gradually over time with the

00:02:29,360 --> 00:02:32,879
transition

00:02:30,080 --> 00:02:34,400
from traditional applications to cloud

00:02:32,879 --> 00:02:37,920
native applications

00:02:34,400 --> 00:02:41,519
the architecture is completely different

00:02:37,920 --> 00:02:42,160
to its predecessors so we need a new way

00:02:41,519 --> 00:02:43,760
to do

00:02:42,160 --> 00:02:45,920
data production for cloud native

00:02:43,760 --> 00:02:48,800
applications

00:02:45,920 --> 00:02:50,560
when container residation was taken off

00:02:48,800 --> 00:02:53,519
it was speculated

00:02:50,560 --> 00:02:55,200
that container applications would not

00:02:53,519 --> 00:02:58,480
need data protection

00:02:55,200 --> 00:03:02,000
because they are stateless

00:02:58,480 --> 00:03:03,280
but that has changed a lot of stable

00:03:02,000 --> 00:03:07,280
applications are being

00:03:03,280 --> 00:03:09,760
run within kubernetes these days

00:03:07,280 --> 00:03:13,200
kubernetes stateful applications use

00:03:09,760 --> 00:03:15,120
persistent volumes to stay their data

00:03:13,200 --> 00:03:17,280
a persistent walling has an independent

00:03:15,120 --> 00:03:20,959
lifecycle from the pod

00:03:17,280 --> 00:03:22,879
that is consuming it so that data can be

00:03:20,959 --> 00:03:25,200
preserved on the underlying storage

00:03:22,879 --> 00:03:28,319
system even if the part goes away

00:03:25,200 --> 00:03:29,360
however what if the underlying volume on

00:03:28,319 --> 00:03:32,080
the 3d system

00:03:29,360 --> 00:03:33,840
gets corrupted for some reason what if

00:03:32,080 --> 00:03:35,840
the underlying storage system is

00:03:33,840 --> 00:03:38,319
struggling by a disaster

00:03:35,840 --> 00:03:40,000
when that happens even data stored on

00:03:38,319 --> 00:03:43,599
the persistent recordings will be

00:03:40,000 --> 00:03:46,080
gone to prevent data loss from happening

00:03:43,599 --> 00:03:47,280
we need to find a way to protect the

00:03:46,080 --> 00:03:49,680
data stored

00:03:47,280 --> 00:03:52,959
in the persistent volumes used by the

00:03:49,680 --> 00:03:55,599
kubernetes day for applications

00:03:52,959 --> 00:03:56,159
although how to provision persistent

00:03:55,599 --> 00:03:58,159
volumes

00:03:56,159 --> 00:04:00,799
is well known it is still a challenge to

00:03:58,159 --> 00:04:03,040
protect your workloads in kubernetes

00:04:00,799 --> 00:04:04,560
this is a problem that this data

00:04:03,040 --> 00:04:07,599
protection one group

00:04:04,560 --> 00:04:07,599
wants to solve

00:04:09,040 --> 00:04:15,680
this is our charter this working group

00:04:13,439 --> 00:04:16,959
is formed so that we can have a cross

00:04:15,680 --> 00:04:18,479
seek collaboration

00:04:16,959 --> 00:04:20,880
to figure out what are the missing

00:04:18,479 --> 00:04:21,440
functionalities and work together to

00:04:20,880 --> 00:04:24,320
design

00:04:21,440 --> 00:04:26,639
features in order to provide support for

00:04:24,320 --> 00:04:28,800
data protection in kubernetes

00:04:26,639 --> 00:04:30,080
sponsoring six for the swing group are

00:04:28,800 --> 00:04:33,600
six f's and

00:04:30,080 --> 00:04:36,080
six storage next xiang chen

00:04:33,600 --> 00:04:38,639
is going to talk about data protection

00:04:36,080 --> 00:04:38,639
definition

00:04:39,360 --> 00:04:43,280
thank you shane exercise please uh as

00:04:42,560 --> 00:04:46,000
soon as they

00:04:43,280 --> 00:04:46,800
stated before more and more stateful

00:04:46,000 --> 00:04:48,479
workload

00:04:46,800 --> 00:04:50,160
started to move into kubernetes

00:04:48,479 --> 00:04:53,360
environment

00:04:50,160 --> 00:04:56,560
we see we observe the strong desire

00:04:53,360 --> 00:05:00,160
of protecting the state 4 applications

00:04:56,560 --> 00:05:02,479
in kubernetes context the main purpose

00:05:00,160 --> 00:05:03,600
of course for that protection is to

00:05:02,479 --> 00:05:05,919
ensure

00:05:03,600 --> 00:05:07,280
those stateful applications of stateful

00:05:05,919 --> 00:05:10,160
workloads

00:05:07,280 --> 00:05:12,240
can be restored to a previously

00:05:10,160 --> 00:05:13,440
preserved state at any given point of

00:05:12,240 --> 00:05:14,880
time

00:05:13,440 --> 00:05:16,800
especially in the cases like data

00:05:14,880 --> 00:05:21,280
corruption or dr

00:05:16,800 --> 00:05:24,720
etc etc in kubernetes context

00:05:21,280 --> 00:05:26,720
we mainly target we mainly discuss or

00:05:24,720 --> 00:05:29,520
target two types of entities

00:05:26,720 --> 00:05:31,360
one is the resource the api resources

00:05:29,520 --> 00:05:34,639
and the other will be data

00:05:31,360 --> 00:05:37,680
that uses those on position volumes

00:05:34,639 --> 00:05:40,880
this itself is a very complicated

00:05:37,680 --> 00:05:44,320
and we're a layered problem uh

00:05:40,880 --> 00:05:45,759
so far there are a couple of approaches

00:05:44,320 --> 00:05:49,280
of doing that

00:05:45,759 --> 00:05:50,840
i'll go through this later next slice

00:05:49,280 --> 00:05:54,479
please

00:05:50,840 --> 00:05:57,600
shine part of our charter

00:05:54,479 --> 00:06:01,360
is to define what are the

00:05:57,600 --> 00:06:04,720
kubernetes native constructs to enable

00:06:01,360 --> 00:06:06,560
backup and query for different levels

00:06:04,720 --> 00:06:08,160
we are not at the position to provide

00:06:06,560 --> 00:06:09,919
end-to-end solution

00:06:08,160 --> 00:06:11,280
to protect protect every single

00:06:09,919 --> 00:06:14,639
application

00:06:11,280 --> 00:06:15,360
but we do want to provide common modules

00:06:14,639 --> 00:06:19,680
that

00:06:15,360 --> 00:06:21,919
backup vendors or users can easily use

00:06:19,680 --> 00:06:23,600
those includes persistent level

00:06:21,919 --> 00:06:26,240
persistent volume level

00:06:23,600 --> 00:06:28,720
on the volume snapshot when backup and

00:06:26,240 --> 00:06:29,360
how to restore from a rehydrated volume

00:06:28,720 --> 00:06:33,520
from those

00:06:29,360 --> 00:06:36,960
central backups an application level

00:06:33,520 --> 00:06:39,280
how do you know which resources belong

00:06:36,960 --> 00:06:41,199
api resources belongs to a specific

00:06:39,280 --> 00:06:43,440
application

00:06:41,199 --> 00:06:45,199
how do you consistently acquires and

00:06:43,440 --> 00:06:47,039
acquires an application so that

00:06:45,199 --> 00:06:48,560
application consistent snapshot can be

00:06:47,039 --> 00:06:51,680
taken

00:06:48,560 --> 00:06:54,560
and finally the cluster level

00:06:51,680 --> 00:06:54,560
exercise base

00:06:54,960 --> 00:07:00,840
as of today we observe

00:06:58,000 --> 00:07:02,000
these backup workflows in kubernetes

00:07:00,840 --> 00:07:05,280
contexts

00:07:02,000 --> 00:07:08,720
user first of all starts a backup

00:07:05,280 --> 00:07:12,160
so it actually goes for two steps at two

00:07:08,720 --> 00:07:13,840
stage steps the first step is to collect

00:07:12,160 --> 00:07:17,440
all the kubernetes resources

00:07:13,840 --> 00:07:19,280
the backup into some external repository

00:07:17,440 --> 00:07:22,639
backup repository

00:07:19,280 --> 00:07:25,840
and the other one is the data backup

00:07:22,639 --> 00:07:27,759
and in the data backup is that there are

00:07:25,840 --> 00:07:31,440
two models

00:07:27,759 --> 00:07:32,720
uh right now we observed one so-called

00:07:31,440 --> 00:07:36,880
application

00:07:32,720 --> 00:07:40,400
data.dump for applications like mysql

00:07:36,880 --> 00:07:41,759
they already have a native support of

00:07:40,400 --> 00:07:45,759
dumping

00:07:41,759 --> 00:07:49,120
the snapshot data into a file and then

00:07:45,759 --> 00:07:51,520
some ex extra

00:07:49,120 --> 00:07:54,479
components can pick up those dumps and

00:07:51,520 --> 00:07:57,599
put it in the in the backup repository

00:07:54,479 --> 00:08:00,720
and the other way is that the

00:07:57,599 --> 00:08:02,960
we call it controller coordinated and

00:08:00,720 --> 00:08:06,400
that way the applications do not

00:08:02,960 --> 00:08:09,680
have a negative data dump mechanism

00:08:06,400 --> 00:08:10,080
so uh but by the concludes itself and it

00:08:09,680 --> 00:08:12,639
can

00:08:10,080 --> 00:08:13,599
unquest itself so the controller we're

00:08:12,639 --> 00:08:16,160
calling uh

00:08:13,599 --> 00:08:16,879
try to first try to quiz the application

00:08:16,160 --> 00:08:19,520
and create

00:08:16,879 --> 00:08:20,160
volume snapshots for all the volumes

00:08:19,520 --> 00:08:22,400
that

00:08:20,160 --> 00:08:24,000
the applications use and unclears the

00:08:22,400 --> 00:08:25,360
applications the application can start

00:08:24,000 --> 00:08:28,080
serving again

00:08:25,360 --> 00:08:28,400
and after that william can volume data

00:08:28,080 --> 00:08:31,120
or

00:08:28,400 --> 00:08:33,760
snapshot data can be exposed into some

00:08:31,120 --> 00:08:35,839
external backup

00:08:33,760 --> 00:08:38,640
on the reverse side the restore workflow

00:08:35,839 --> 00:08:38,640
next size please

00:08:39,680 --> 00:08:45,600
a user starts a restore

00:08:42,800 --> 00:08:46,959
import the backup into the cluster and

00:08:45,600 --> 00:08:50,480
then firstly

00:08:46,959 --> 00:08:54,080
we store the kubernetes resources

00:08:50,480 --> 00:08:56,480
but pvp pvcs needs to be handled

00:08:54,080 --> 00:08:58,399
especially because there are many

00:08:56,480 --> 00:09:00,640
dependencies

00:08:58,399 --> 00:09:02,399
uh in the native way mapped to the

00:09:00,640 --> 00:09:05,600
previous mentioned native wave

00:09:02,399 --> 00:09:09,440
as well uh the application

00:09:05,600 --> 00:09:10,000
can have native data can restore from a

00:09:09,440 --> 00:09:13,600
native

00:09:10,000 --> 00:09:16,320
data dump uh in the non-native way

00:09:13,600 --> 00:09:18,800
uh just rehydrates pvcs from one

00:09:16,320 --> 00:09:22,240
snapshot and volume back

00:09:18,800 --> 00:09:24,320
so we have these workflows what are the

00:09:22,240 --> 00:09:26,480
mixes without building blocks

00:09:24,320 --> 00:09:28,399
to support these workflows is what we

00:09:26,480 --> 00:09:31,680
want to answer in this working group

00:09:28,399 --> 00:09:35,200
exercise please see so what are there

00:09:31,680 --> 00:09:35,200
at this moment existing bed

00:09:35,600 --> 00:09:38,959
in the application from applications

00:09:38,080 --> 00:09:41,760
perspective

00:09:38,959 --> 00:09:43,200
we have workload apis stack process

00:09:41,760 --> 00:09:45,760
deployment etc

00:09:43,200 --> 00:09:48,480
an application crb and those are

00:09:45,760 --> 00:09:50,959
high-level constructs that

00:09:48,480 --> 00:09:54,560
groups a set of kubernetes resources

00:09:50,959 --> 00:09:56,560
together to form your application

00:09:54,560 --> 00:09:58,080
the other in the storage layer we have

00:09:56,560 --> 00:10:00,800
one snapshot

00:09:58,080 --> 00:10:01,680
which is built on top of pvc uh can take

00:10:00,800 --> 00:10:04,800
the

00:10:01,680 --> 00:10:07,440
point in time snapshot of a volume

00:10:04,800 --> 00:10:08,399
so how does this building blocks fit

00:10:07,440 --> 00:10:11,200
into the picture

00:10:08,399 --> 00:10:11,200
next slides please

00:10:11,760 --> 00:10:15,760
next slide yes thanks uh the workload

00:10:14,880 --> 00:10:18,480
api

00:10:15,760 --> 00:10:20,480
and the stick apps application crd can

00:10:18,480 --> 00:10:21,760
fit into the kubernetes resource backup

00:10:20,480 --> 00:10:24,320
process

00:10:21,760 --> 00:10:26,000
with that it provides convenient way to

00:10:24,320 --> 00:10:30,320
group kubernetes resources in

00:10:26,000 --> 00:10:30,800
together next slides please let's take a

00:10:30,320 --> 00:10:34,800
look at

00:10:30,800 --> 00:10:37,839
in how exactly

00:10:34,800 --> 00:10:39,839
look like for the application crb the

00:10:37,839 --> 00:10:42,480
application crd is nothing but

00:10:39,839 --> 00:10:44,800
providing an api for managing

00:10:42,480 --> 00:10:46,640
applications in kubernetes

00:10:44,800 --> 00:10:49,440
it aggregates individual kubernetes

00:10:46,640 --> 00:10:52,480
components for example in this example

00:10:49,440 --> 00:10:55,519
it's an application crd for mongodb

00:10:52,480 --> 00:10:57,600
it contains a service a stateful cell

00:10:55,519 --> 00:10:59,279
and maybe some secrets underneath for

00:10:57,600 --> 00:11:02,480
the yaml file

00:10:59,279 --> 00:11:05,839
the key is that with this single cr

00:11:02,480 --> 00:11:10,079
you can tell which resources belongs to

00:11:05,839 --> 00:11:10,079
this mongodb in the kubernetes world

00:11:10,399 --> 00:11:15,360
now let's take a look at how one

00:11:12,079 --> 00:11:18,800
snapshot fits in the picture

00:11:15,360 --> 00:11:22,480
so next slides question

00:11:18,800 --> 00:11:25,760
in one but in the backup workflow

00:11:22,480 --> 00:11:28,640
one snapshot can be used

00:11:25,760 --> 00:11:29,040
in the controller coordinated workflow

00:11:28,640 --> 00:11:30,880
where

00:11:29,040 --> 00:11:32,640
one snapshot will be created how the

00:11:30,880 --> 00:11:35,440
application is quieted

00:11:32,640 --> 00:11:36,480
next slidesplies in the restore workflow

00:11:35,440 --> 00:11:39,600
worm snapshot

00:11:36,480 --> 00:11:42,079
can be used to rehydrate pvc from them

00:11:39,600 --> 00:11:44,399
and restore the workload into the

00:11:42,079 --> 00:11:47,200
previous state

00:11:44,399 --> 00:11:48,560
uh one snapshot has been moving to beta

00:11:47,200 --> 00:11:51,760
since night 119

00:11:48,560 --> 00:11:55,519
next slide speech and we plan to

00:11:51,760 --> 00:11:59,200
move the future to ga in 120.

00:11:55,519 --> 00:12:00,639
uh validation webhook has been added in

00:11:59,200 --> 00:12:04,079
version 119

00:12:00,639 --> 00:12:06,399
and we are right now working on enhance

00:12:04,079 --> 00:12:07,440
the observability around the controllers

00:12:06,399 --> 00:12:10,880
and adding more

00:12:07,440 --> 00:12:13,760
end-to-end tests mostly stress testers

00:12:10,880 --> 00:12:15,760
so i'm looking forward to the ga of this

00:12:13,760 --> 00:12:18,880
feature

00:12:15,760 --> 00:12:22,000
so uh we're talking about existing

00:12:18,880 --> 00:12:23,760
so what i'm missing there next size

00:12:22,000 --> 00:12:26,000
please

00:12:23,760 --> 00:12:26,959
we're missing a whole lot and this is

00:12:26,000 --> 00:12:29,600
not yet

00:12:26,959 --> 00:12:30,560
a complete list yet going back up

00:12:29,600 --> 00:12:34,480
repository

00:12:30,560 --> 00:12:36,800
choirs and then quiet hooks etc etc

00:12:34,480 --> 00:12:39,839
if you take a full picture of this next

00:12:36,800 --> 00:12:42,000
size piece

00:12:39,839 --> 00:12:42,880
all these green boxes are currently

00:12:42,000 --> 00:12:45,920
existing

00:12:42,880 --> 00:12:47,120
the yellow box is working working in

00:12:45,920 --> 00:12:50,000
progress

00:12:47,120 --> 00:12:50,399
and the orange boxes are not there yet

00:12:50,000 --> 00:12:52,560
so

00:12:50,399 --> 00:12:54,160
they've those components application

00:12:52,560 --> 00:12:56,959
backup can be used to group

00:12:54,160 --> 00:12:58,480
the resources and data according to data

00:12:56,959 --> 00:13:00,399
backup as well

00:12:58,480 --> 00:13:02,320
a container notify feeds in multiple

00:13:00,399 --> 00:13:05,440
scenarios

00:13:02,320 --> 00:13:06,880
uh in this backup workflow next slice

00:13:05,440 --> 00:13:10,240
please

00:13:06,880 --> 00:13:12,639
in the restore workflow it is similar

00:13:10,240 --> 00:13:14,480
so i will not spend too much time on

00:13:12,639 --> 00:13:16,800
this hourly sheen to go through

00:13:14,480 --> 00:13:18,480
all these orange and yellow boxes in the

00:13:16,800 --> 00:13:22,000
next couple sites

00:13:18,480 --> 00:13:22,880
thanks shin it's all yours thanks juan

00:13:22,000 --> 00:13:25,519
chen

00:13:22,880 --> 00:13:27,279
i'm going to explain in more details on

00:13:25,519 --> 00:13:28,639
why we think all of us are missing

00:13:27,279 --> 00:13:33,920
building blocks

00:13:28,639 --> 00:13:34,639
and what we are planning to do with them

00:13:33,920 --> 00:13:36,079
the

00:13:34,639 --> 00:13:38,000
first missing building block we

00:13:36,079 --> 00:13:40,160
identified is volume backup

00:13:38,000 --> 00:13:42,959
we need this because we need to extract

00:13:40,160 --> 00:13:45,199
data to a secondary storage

00:13:42,959 --> 00:13:46,079
we've already got a volume snapshot api

00:13:45,199 --> 00:13:48,720
but there's no

00:13:46,079 --> 00:13:50,079
explicit definition made in the design

00:13:48,720 --> 00:13:52,639
to have snapshots

00:13:50,079 --> 00:13:55,040
stored on a different backup device

00:13:52,639 --> 00:13:57,519
separate from the primary storage

00:13:55,040 --> 00:13:58,079
for some cloud providers a snapshot is

00:13:57,519 --> 00:14:00,240
actually

00:13:58,079 --> 00:14:02,160
a backup that is uploaded to you and

00:14:00,240 --> 00:14:04,639
object store in cloud

00:14:02,160 --> 00:14:07,199
however for most other 3d vendors a

00:14:04,639 --> 00:14:09,680
snapshot is locally stored alongside

00:14:07,199 --> 00:14:10,320
the volume on the primary storage

00:14:09,680 --> 00:14:13,120
therefore

00:14:10,320 --> 00:14:15,199
it is impossible to design a portable

00:14:13,120 --> 00:14:18,399
data protection policy to support

00:14:15,199 --> 00:14:19,600
all storage vendors without a voting

00:14:18,399 --> 00:14:22,720
backup api

00:14:19,600 --> 00:14:24,399
the alternative is for backup vendors to

00:14:22,720 --> 00:14:27,279
have two solutions

00:14:24,399 --> 00:14:28,720
for 3d systems that upload snapshots to

00:14:27,279 --> 00:14:31,440
object store automatically

00:14:28,720 --> 00:14:32,800
a snapshot is a backup for storage

00:14:31,440 --> 00:14:35,600
systems that only take

00:14:32,800 --> 00:14:37,120
local snapshots use volume snapshot api

00:14:35,600 --> 00:14:39,440
to take the snapshot

00:14:37,120 --> 00:14:40,320
and then have a data mover to upload

00:14:39,440 --> 00:14:43,600
snapshot to

00:14:40,320 --> 00:14:45,279
a backup device we just started

00:14:43,600 --> 00:14:47,519
discussions about those in the working

00:14:45,279 --> 00:14:47,519
group

00:14:47,680 --> 00:14:51,440
so let's take a look of this diagram

00:14:50,000 --> 00:14:54,320
voting backup is

00:14:51,440 --> 00:14:54,639
next to what in snapshot here we put it

00:14:54,320 --> 00:14:57,279
in an

00:14:54,639 --> 00:14:59,360
orange box to indicate that it is a

00:14:57,279 --> 00:15:01,440
missing kubernetes component

00:14:59,360 --> 00:15:05,120
we have started discussions about it but

00:15:01,440 --> 00:15:05,120
there's no concrete design yet

00:15:05,920 --> 00:15:10,320
the next one is cbt and changed file

00:15:09,199 --> 00:15:12,959
list

00:15:10,320 --> 00:15:13,839
without cpt and change file list backup

00:15:12,959 --> 00:15:16,720
vendors

00:15:13,839 --> 00:15:17,040
have to do full backups all the time

00:15:16,720 --> 00:15:19,920
this

00:15:17,040 --> 00:15:20,959
is not space efficient takes longer to

00:15:19,920 --> 00:15:24,639
complete

00:15:20,959 --> 00:15:25,120
and miss more bandwidth another use case

00:15:24,639 --> 00:15:27,279
is

00:15:25,120 --> 00:15:29,360
snapshot-based replication where you

00:15:27,279 --> 00:15:31,440
take snapshots periodically

00:15:29,360 --> 00:15:34,639
and replicate it to another site for

00:15:31,440 --> 00:15:37,839
disaster recovery purpose

00:15:34,639 --> 00:15:40,720
so what are the alternatives without cvt

00:15:37,839 --> 00:15:41,199
we can either do full backups or call

00:15:40,720 --> 00:15:45,360
each

00:15:41,199 --> 00:15:48,959
storage api individually to retrieve cpt

00:15:45,360 --> 00:15:51,199
which is highly inefficient we just

00:15:48,959 --> 00:15:55,600
started a discussion about this in the

00:15:51,199 --> 00:15:59,680
working group

00:15:55,600 --> 00:16:02,480
the next one that we think are a missing

00:15:59,680 --> 00:16:04,480
building block is the backup repository

00:16:02,480 --> 00:16:06,399
backup repository is a location or

00:16:04,480 --> 00:16:10,880
report to store data

00:16:06,399 --> 00:16:13,920
this can be a object store in the cloud

00:16:10,880 --> 00:16:16,720
an on-prem storage location or

00:16:13,920 --> 00:16:18,480
some angular fast-based solutions there

00:16:16,720 --> 00:16:19,839
are two types of data to be backed up

00:16:18,480 --> 00:16:22,800
that we need at

00:16:19,839 --> 00:16:24,639
restore time the first one is kubernetes

00:16:22,800 --> 00:16:27,440
cluster metadata

00:16:24,639 --> 00:16:28,480
the second one is local snapshot data we

00:16:27,440 --> 00:16:32,079
need to back them up

00:16:28,480 --> 00:16:34,480
and store them in a backup repository

00:16:32,079 --> 00:16:35,120
currently there is a proposal for object

00:16:34,480 --> 00:16:37,680
store

00:16:35,120 --> 00:16:38,160
backup repository that is the proposal

00:16:37,680 --> 00:16:41,920
for

00:16:38,160 --> 00:16:45,040
object bucket provisioning or cosy

00:16:41,920 --> 00:16:46,240
this proposes object storage kubernetes

00:16:45,040 --> 00:16:48,480
apis

00:16:46,240 --> 00:16:50,079
to support orchestration of object store

00:16:48,480 --> 00:16:52,560
operations for kubernetes

00:16:50,079 --> 00:16:53,759
workloads therefore bring in object

00:16:52,560 --> 00:16:56,800
storage as the first

00:16:53,759 --> 00:17:00,000
class citizen in kubernetes just like

00:16:56,800 --> 00:17:00,800
file and block storage it also

00:17:00,000 --> 00:17:02,800
introduces

00:17:00,800 --> 00:17:06,000
container object storage interface or

00:17:02,800 --> 00:17:09,839
cosy as a set of grpc interfaces

00:17:06,000 --> 00:17:13,120
for provisioning object stores

00:17:09,839 --> 00:17:13,839
kubernetes cozy is already a sub project

00:17:13,120 --> 00:17:16,400
in

00:17:13,839 --> 00:17:18,319
six storage cab was merged as

00:17:16,400 --> 00:17:21,280
provisional in 1.20

00:17:18,319 --> 00:17:22,640
and the plan is to do prototyping we

00:17:21,280 --> 00:17:25,520
already have github repos

00:17:22,640 --> 00:17:27,439
created for this there is also a session

00:17:25,520 --> 00:17:32,320
about cozy at kubecon

00:17:27,439 --> 00:17:35,280
check it out if you are interested

00:17:32,320 --> 00:17:37,200
so let's see where cozy is in this

00:17:35,280 --> 00:17:39,760
diagram

00:17:37,200 --> 00:17:41,679
cozy is in a yellow box indicating that

00:17:39,760 --> 00:17:43,120
this is a working progress kubernetes

00:17:41,679 --> 00:17:45,200
component

00:17:43,120 --> 00:17:46,240
this is an object store backup

00:17:45,200 --> 00:17:48,720
repository

00:17:46,240 --> 00:17:51,600
it can be used to export backup and

00:17:48,720 --> 00:17:51,600
store the data

00:17:52,000 --> 00:17:57,200
now let's take a look of the restore

00:17:54,880 --> 00:18:01,760
cozy is used to import backup data at

00:17:57,200 --> 00:18:05,919
the restore time

00:18:01,760 --> 00:18:08,720
the next one is generic data populator

00:18:05,919 --> 00:18:09,760
currently we can only create a pvc for

00:18:08,720 --> 00:18:12,640
another pvc

00:18:09,760 --> 00:18:13,760
or a volume snapshot but what if the

00:18:12,640 --> 00:18:16,640
backup data

00:18:13,760 --> 00:18:18,480
is stored in a backup repository such as

00:18:16,640 --> 00:18:20,320
an object store

00:18:18,480 --> 00:18:22,559
the generic data populated feature

00:18:20,320 --> 00:18:25,039
allows us to provision a pvc

00:18:22,559 --> 00:18:26,960
from an external data source such as a

00:18:25,039 --> 00:18:29,280
backup repository

00:18:26,960 --> 00:18:30,880
in addition it allows us to dynamically

00:18:29,280 --> 00:18:32,880
provision a pvc

00:18:30,880 --> 00:18:33,919
having data populated from that backup

00:18:32,880 --> 00:18:36,320
repository

00:18:33,919 --> 00:18:37,440
and honor the wait for first consumer

00:18:36,320 --> 00:18:40,960
volume binding mode

00:18:37,440 --> 00:18:42,080
during restore to ensure that volume is

00:18:40,960 --> 00:18:45,120
placed at the right

00:18:42,080 --> 00:18:45,600
node where the part is scheduled there

00:18:45,120 --> 00:18:47,919
is an

00:18:45,600 --> 00:18:49,600
any warning data source alpha feature

00:18:47,919 --> 00:18:53,280
gate which was introduced in

00:18:49,600 --> 00:18:56,080
1.18 in 1.20 the plan is to do

00:18:53,280 --> 00:18:57,360
the design and prototyping for generic

00:18:56,080 --> 00:19:01,600
data populator

00:18:57,360 --> 00:19:03,280
implementation now let's take a look at

00:19:01,600 --> 00:19:05,840
the diagram

00:19:03,280 --> 00:19:08,320
we can see that generic data populator

00:19:05,840 --> 00:19:10,160
is needed at restore time

00:19:08,320 --> 00:19:12,640
generic data populator is in a yellow

00:19:10,160 --> 00:19:14,480
box indicating it is a working progress

00:19:12,640 --> 00:19:17,280
kubernetes component

00:19:14,480 --> 00:19:18,880
it is used to rehydrate pvc from a

00:19:17,280 --> 00:19:21,760
backup repository

00:19:18,880 --> 00:19:21,760
during restore

00:19:23,039 --> 00:19:29,919
next one is uh coins and unquest hooks

00:19:28,320 --> 00:19:32,160
we need this hooks to require

00:19:29,919 --> 00:19:32,400
supplication before taking a snapshot

00:19:32,160 --> 00:19:34,960
and

00:19:32,400 --> 00:19:36,000
encourage afterwards to ensure

00:19:34,960 --> 00:19:39,520
application

00:19:36,000 --> 00:19:41,360
consistency we investigated how coast

00:19:39,520 --> 00:19:42,480
enquires works in different types of

00:19:41,360 --> 00:19:44,160
workloads

00:19:42,480 --> 00:19:46,640
we looked at the relational databases

00:19:44,160 --> 00:19:49,600
such as mysql that provides a command

00:19:46,640 --> 00:19:51,440
to flash tables with readlock so we can

00:19:49,600 --> 00:19:54,240
use that to do requires

00:19:51,440 --> 00:19:54,559
we looked at time series databases such

00:19:54,240 --> 00:19:58,240
as

00:19:54,559 --> 00:20:01,280
new db prometheus influx db

00:19:58,240 --> 00:20:04,320
which do not have a explicit

00:20:01,280 --> 00:20:09,039
quiz command but there is a cri that

00:20:04,320 --> 00:20:11,840
supports consistent backups and restores

00:20:09,039 --> 00:20:12,559
key value store we also take a look of

00:20:11,840 --> 00:20:15,600
the

00:20:12,559 --> 00:20:18,400
example etc

00:20:15,600 --> 00:20:19,120
that provides a command to backup and

00:20:18,400 --> 00:20:21,840
restore

00:20:19,120 --> 00:20:23,760
but it did it does not have a explicit

00:20:21,840 --> 00:20:27,039
quest command

00:20:23,760 --> 00:20:30,180
we also look at message queues such as

00:20:27,039 --> 00:20:31,440
kafka for example

00:20:30,180 --> 00:20:34,480
[Music]

00:20:31,440 --> 00:20:36,880
kafka is designed for uh

00:20:34,480 --> 00:20:38,799
for tolerance we can do backup and

00:20:36,880 --> 00:20:42,080
restoring kubernetes with best

00:20:38,799 --> 00:20:45,760
effort but there are many issues

00:20:42,080 --> 00:20:48,799
um partitioning maybe rebalancing

00:20:45,760 --> 00:20:49,840
kafka well broker is offline and that

00:20:48,799 --> 00:20:53,280
might cause

00:20:49,840 --> 00:20:55,600
data loss we also look at distributed

00:20:53,280 --> 00:20:58,000
databases such as mongodb

00:20:55,600 --> 00:20:58,799
that provides a command to flush all

00:20:58,000 --> 00:21:02,159
pending right

00:20:58,799 --> 00:21:04,720
operations to disk unlocks mongodb

00:21:02,159 --> 00:21:05,520
instance against writes so we can use

00:21:04,720 --> 00:21:08,000
that command

00:21:05,520 --> 00:21:09,760
to do quiets but we need to keep in mind

00:21:08,000 --> 00:21:12,080
that the process to backup

00:21:09,760 --> 00:21:14,000
non-sharded versus sharded mobile db

00:21:12,080 --> 00:21:15,840
databases are different

00:21:14,000 --> 00:21:17,200
we don't have time to go over all the

00:21:15,840 --> 00:21:19,200
findings here but we will

00:21:17,200 --> 00:21:20,720
include them in the white paper that we

00:21:19,200 --> 00:21:23,120
are working on

00:21:20,720 --> 00:21:24,320
we want to design a generic mechanism to

00:21:23,120 --> 00:21:27,280
draw commands

00:21:24,320 --> 00:21:27,760
in containers but we want to mention

00:21:27,280 --> 00:21:30,320
that

00:21:27,760 --> 00:21:32,000
application specific semantics is out of

00:21:30,320 --> 00:21:34,400
scope

00:21:32,000 --> 00:21:36,159
we currently have a proposal called

00:21:34,400 --> 00:21:40,880
content notifier

00:21:36,159 --> 00:21:43,120
copy submitted and it has been reviewed

00:21:40,880 --> 00:21:45,280
let's take a look at the diagram here

00:21:43,120 --> 00:21:46,000
container notifier is many used at

00:21:45,280 --> 00:21:48,880
backup time

00:21:46,000 --> 00:21:51,200
to do choirs before taking snapshot and

00:21:48,880 --> 00:21:53,360
encourage afterwards

00:21:51,200 --> 00:21:58,000
and this this is also a working progress

00:21:53,360 --> 00:22:00,159
kubernetes component

00:21:58,000 --> 00:22:01,840
the next one is consistent group

00:22:00,159 --> 00:22:04,400
snapshot

00:22:01,840 --> 00:22:06,480
so we talked about the container notify

00:22:04,400 --> 00:22:09,440
proposal which tries to

00:22:06,480 --> 00:22:11,360
ensure application consistency what if

00:22:09,440 --> 00:22:13,760
we can't quite ask the education

00:22:11,360 --> 00:22:15,760
or if the application inquires is too

00:22:13,760 --> 00:22:17,919
expensive so you want to do it

00:22:15,760 --> 00:22:19,679
less frequently but still want to be

00:22:17,919 --> 00:22:21,840
able to do a

00:22:19,679 --> 00:22:23,200
crash consistent snapshot more

00:22:21,840 --> 00:22:26,000
frequently

00:22:23,200 --> 00:22:27,760
also an application may require the

00:22:26,000 --> 00:22:30,320
snapshots from multiple volumes to be

00:22:27,760 --> 00:22:32,720
taken at the same point in time

00:22:30,320 --> 00:22:34,559
so that's when consistent group snapshot

00:22:32,720 --> 00:22:36,880
comes into the picture

00:22:34,559 --> 00:22:37,760
there is a cap on one group and group

00:22:36,880 --> 00:22:40,000
snapshot

00:22:37,760 --> 00:22:41,919
it proposes to introduce a new voting

00:22:40,000 --> 00:22:42,799
group crd that groups multiple volumes

00:22:41,919 --> 00:22:45,039
together

00:22:42,799 --> 00:22:45,919
and the new group snapshot crd that

00:22:45,039 --> 00:22:48,559
supports

00:22:45,919 --> 00:22:48,960
taking a snapshot of all volumes in the

00:22:48,559 --> 00:22:52,080
group

00:22:48,960 --> 00:22:55,120
to ensure right order consistency

00:22:52,080 --> 00:22:58,400
the cap is being reviewed

00:22:55,120 --> 00:22:59,280
let's take a look at the diagram here uh

00:22:58,400 --> 00:23:01,200
we don't have

00:22:59,280 --> 00:23:04,400
container notifier to requires here but

00:23:01,200 --> 00:23:06,559
we have a consistent group snapshot

00:23:04,400 --> 00:23:08,000
that facilitates the creation of a

00:23:06,559 --> 00:23:10,880
snapshot of multiple

00:23:08,000 --> 00:23:14,320
volumes in the same group to ensure

00:23:10,880 --> 00:23:14,320
right order consistency

00:23:16,159 --> 00:23:20,559
next one is application snapshot and

00:23:18,480 --> 00:23:22,559
backup

00:23:20,559 --> 00:23:23,760
we have snapshot apis for individual

00:23:22,559 --> 00:23:25,760
volumes but

00:23:23,760 --> 00:23:27,200
what about protecting a stable

00:23:25,760 --> 00:23:30,640
application

00:23:27,200 --> 00:23:33,600
there is a cap submitted that proposes

00:23:30,640 --> 00:23:35,120
a kubernetes api that defines the notion

00:23:33,600 --> 00:23:37,280
of stable applications

00:23:35,120 --> 00:23:39,120
and defines healthy run operations among

00:23:37,280 --> 00:23:42,400
those safe applications

00:23:39,120 --> 00:23:44,559
such as snapshot backup and restore

00:23:42,400 --> 00:23:46,720
this is still at a very early design

00:23:44,559 --> 00:23:49,760
stage

00:23:46,720 --> 00:23:51,840
as shown in this diagram for backup

00:23:49,760 --> 00:23:53,440
application backup handles the backup of

00:23:51,840 --> 00:23:56,480
a safe application

00:23:53,440 --> 00:24:00,240
it can leverage a container

00:23:56,480 --> 00:24:05,840
notifier to do coils and use cozy

00:24:00,240 --> 00:24:09,520
as a backup repository

00:24:05,840 --> 00:24:12,000
and similarly

00:24:09,520 --> 00:24:14,000
we can have application restore that

00:24:12,000 --> 00:24:17,200
handles the restore

00:24:14,000 --> 00:24:19,840
of a stable application

00:24:17,200 --> 00:24:21,679
so those are all the missing building

00:24:19,840 --> 00:24:23,039
blocks that we have identified and are

00:24:21,679 --> 00:24:26,240
working on

00:24:23,039 --> 00:24:28,480
we hope eventually we can turn

00:24:26,240 --> 00:24:29,919
all of this yellow and orange boxes into

00:24:28,480 --> 00:24:32,080
green ones

00:24:29,919 --> 00:24:33,200
and when that happens someday in the

00:24:32,080 --> 00:24:36,799
future

00:24:33,200 --> 00:24:39,840
our mission is accomplished

00:24:36,799 --> 00:24:43,039
all right next i'm going to talk about

00:24:39,840 --> 00:24:43,039
how to get involved

00:24:44,000 --> 00:24:48,159
as discussed in previous slides this

00:24:47,039 --> 00:24:50,559
working group

00:24:48,159 --> 00:24:51,679
is working on identifying missing

00:24:50,559 --> 00:24:53,679
functionalities

00:24:51,679 --> 00:24:55,120
in supporting data protection in

00:24:53,679 --> 00:24:56,960
kubernetes

00:24:55,120 --> 00:24:58,880
and trying to figure out how to fill

00:24:56,960 --> 00:25:01,600
those gaps

00:24:58,880 --> 00:25:04,080
we have bi-weekly meetings on wednesdays

00:25:01,600 --> 00:25:05,840
at 9am pacific time

00:25:04,080 --> 00:25:07,760
if you are interested in joining the

00:25:05,840 --> 00:25:10,880
discussions you are welcome

00:25:07,760 --> 00:25:11,679
to join our meetings we also have a

00:25:10,880 --> 00:25:15,039
mailing list

00:25:11,679 --> 00:25:17,679
and a slack channel as shown here

00:25:15,039 --> 00:25:20,159
this is the end of the presentation

00:25:17,679 --> 00:25:22,320
thank you all for attending the session

00:25:20,159 --> 00:25:23,520
if you have any questions please don't

00:25:22,320 --> 00:25:28,400
hesitate to

00:25:23,520 --> 00:25:28,400

YouTube URL: https://www.youtube.com/watch?v=g8HEQnLVo04


