Title: Design Patterns for Extendable, Scalable K8s Extensions - Max Smythe, Google & Rita Zhang, Microsoft
Publication date: 2020-11-23
Playlist: KubeCon + CloudNativeCon North America 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon Europe 2021 Virtual from May 4–7, 2021. Learn more at https://kubecon.io. The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects. 

Design Patterns for Extendable, Scalable K8s Extensions - Max Smythe, Google & Rita Zhang, Microsoft 

OPA Gatekeeper is a customizable Kubernetes admission webhook that helps enforce policies and strengthen governance. The Gatekeeper project is capable of dynamically creating, managing and destroying new custom resources which are used to customize the webhook. This unique model has led to some interesting design patterns. In this talk, Gatekeeper's maintainters explore the CRDs-that-create-CRDs and leaderless horizontal scalability design patterns that allowed us to create an extendable, scalable Kubernetes extension. 

https://sched.co/ekBY
Captions: 
	00:00:00,399 --> 00:00:04,319
hello everyone welcome to design

00:00:02,399 --> 00:00:06,000
patterns for extensible scalable

00:00:04,319 --> 00:00:08,320
kubernetes extensions

00:00:06,000 --> 00:00:09,599
i'm rita jang i'm a software engineer at

00:00:08,320 --> 00:00:12,080
microsoft

00:00:09,599 --> 00:00:13,519
and i'm max smith a software engineer at

00:00:12,080 --> 00:00:15,599
google

00:00:13,519 --> 00:00:16,960
we are both maintainers of the

00:00:15,599 --> 00:00:19,920
gatekeeper project

00:00:16,960 --> 00:00:21,680
and today we wanted to take a look at

00:00:19,920 --> 00:00:23,840
some of the design patterns underlying

00:00:21,680 --> 00:00:24,640
gatekeeper and how they allowed us to

00:00:23,840 --> 00:00:27,279
create

00:00:24,640 --> 00:00:28,400
an extensible scalable kubernetes

00:00:27,279 --> 00:00:30,080
extension

00:00:28,400 --> 00:00:32,399
because there are a lot of topics to

00:00:30,080 --> 00:00:35,920
cover this talk will mostly cover

00:00:32,399 --> 00:00:37,200
high-level concepts but first what is

00:00:35,920 --> 00:00:40,000
gatekeeper

00:00:37,200 --> 00:00:41,760
gatekeeper is a customizable kubernetes

00:00:40,000 --> 00:00:43,520
mission web hook that helps

00:00:41,760 --> 00:00:46,239
enforce policies and strength and

00:00:43,520 --> 00:00:48,480
governance let's take a look

00:00:46,239 --> 00:00:51,840
at the way it does this before we dig

00:00:48,480 --> 00:00:51,840
into how the pieces work

00:00:52,000 --> 00:00:56,640
gatekeeper supports both audit time and

00:00:55,199 --> 00:00:58,719
admission time checks

00:00:56,640 --> 00:01:00,559
but today we're going to focus on

00:00:58,719 --> 00:01:02,640
admission time specifically

00:01:00,559 --> 00:01:03,680
serving the kubernetes admission web

00:01:02,640 --> 00:01:06,880
hook

00:01:03,680 --> 00:01:08,799
this diagram is a high level model

00:01:06,880 --> 00:01:10,080
of how gatekeeper serves admission

00:01:08,799 --> 00:01:12,479
requests

00:01:10,080 --> 00:01:14,560
when something makes a request to the

00:01:12,479 --> 00:01:17,759
api server so for example

00:01:14,560 --> 00:01:18,400
a user making a cube cuddle request the

00:01:17,759 --> 00:01:21,200
api

00:01:18,400 --> 00:01:22,560
server receives that request and sends

00:01:21,200 --> 00:01:25,759
an admission request

00:01:22,560 --> 00:01:28,720
to gatekeepers validating webhook it's

00:01:25,759 --> 00:01:30,240
asking gatekeeper hey should i allow

00:01:28,720 --> 00:01:32,720
this request to proceed

00:01:30,240 --> 00:01:33,520
right and gatekeeper must answer either

00:01:32,720 --> 00:01:37,360
yes

00:01:33,520 --> 00:01:39,680
or no and to do this gatekeeper consults

00:01:37,360 --> 00:01:42,159
its policy configuration which is the

00:01:39,680 --> 00:01:43,439
large gray box at the bottom of the

00:01:42,159 --> 00:01:45,439
slide

00:01:43,439 --> 00:01:46,720
the green objects that are labeled

00:01:45,439 --> 00:01:49,040
constraint

00:01:46,720 --> 00:01:50,640
tell the web hook what checks an admin

00:01:49,040 --> 00:01:53,680
wants to perform

00:01:50,640 --> 00:01:55,119
and constraints rely on templates which

00:01:53,680 --> 00:01:57,520
are the blue documents

00:01:55,119 --> 00:01:58,880
to tell the system exactly how to

00:01:57,520 --> 00:02:02,000
perform the check

00:01:58,880 --> 00:02:04,640
for example they might say labels live

00:02:02,000 --> 00:02:05,200
in metadata.labels so that's where you

00:02:04,640 --> 00:02:08,640
should

00:02:05,200 --> 00:02:10,879
validate labels

00:02:08,640 --> 00:02:12,640
the notions of gatekeeper the web hook

00:02:10,879 --> 00:02:15,280
constraints and templates

00:02:12,640 --> 00:02:16,000
underlie gatekeepers philosophy that

00:02:15,280 --> 00:02:18,480
policy

00:02:16,000 --> 00:02:20,319
is a team effort separating these roles

00:02:18,480 --> 00:02:22,640
into discrete areas

00:02:20,319 --> 00:02:24,800
allows for specialization where work in

00:02:22,640 --> 00:02:27,520
each area benefits the others

00:02:24,800 --> 00:02:29,360
increasing its scale for example a

00:02:27,520 --> 00:02:31,760
change to gatekeepers platform has the

00:02:29,360 --> 00:02:34,000
potential of increasing the utility

00:02:31,760 --> 00:02:35,280
of all templates across the board with

00:02:34,000 --> 00:02:38,879
zero extra work

00:02:35,280 --> 00:02:41,840
on the template authors part

00:02:38,879 --> 00:02:43,760
we rely on duct typing to give us this

00:02:41,840 --> 00:02:45,599
separation of concerns

00:02:43,760 --> 00:02:47,040
so let's take a brief look at duct

00:02:45,599 --> 00:02:48,560
typing how it helps

00:02:47,040 --> 00:02:51,760
and some of the patterns we found

00:02:48,560 --> 00:02:54,319
helpful in our implementation

00:02:51,760 --> 00:02:55,360
duct typing has been covered in previous

00:02:54,319 --> 00:02:57,599
cube cons

00:02:55,360 --> 00:02:58,879
so we won't get into the details but

00:02:57,599 --> 00:03:00,879
generally speaking

00:02:58,879 --> 00:03:01,920
if it walks like a duck and talks like a

00:03:00,879 --> 00:03:04,400
duck a system

00:03:01,920 --> 00:03:05,360
should treat it like a duck this is

00:03:04,400 --> 00:03:07,599
similar to

00:03:05,360 --> 00:03:08,959
polymorphism in object-oriented

00:03:07,599 --> 00:03:11,599
programming

00:03:08,959 --> 00:03:12,480
our ducts are constraints our goal for

00:03:11,599 --> 00:03:14,720
ammons

00:03:12,480 --> 00:03:16,000
is that they're able to define different

00:03:14,720 --> 00:03:18,480
types of constraints

00:03:16,000 --> 00:03:19,440
that share universal behavior for

00:03:18,480 --> 00:03:21,760
instance

00:03:19,440 --> 00:03:24,239
all constraints can use label selectors

00:03:21,760 --> 00:03:25,920
we do this by creating a parent class

00:03:24,239 --> 00:03:28,239
for the constraint that implements

00:03:25,920 --> 00:03:30,000
universal behaviors

00:03:28,239 --> 00:03:32,400
template authors are able to create

00:03:30,000 --> 00:03:35,280
subclasses by injecting

00:03:32,400 --> 00:03:36,080
their enforcement logic and its function

00:03:35,280 --> 00:03:37,840
signature

00:03:36,080 --> 00:03:39,920
by declaring a constraint template

00:03:37,840 --> 00:03:40,560
resource the constraint template is

00:03:39,920 --> 00:03:43,120
combined

00:03:40,560 --> 00:03:44,799
with this universal behavior to create a

00:03:43,120 --> 00:03:46,560
new constrained kind

00:03:44,799 --> 00:03:47,840
how does this look inside gatekeepers

00:03:46,560 --> 00:03:49,519
code

00:03:47,840 --> 00:03:51,280
when gatekeeper receives an admission

00:03:49,519 --> 00:03:53,599
request from the api server

00:03:51,280 --> 00:03:54,720
the first thing it does is find the

00:03:53,599 --> 00:03:57,120
matching kind

00:03:54,720 --> 00:03:58,239
that define the matching constraints

00:03:57,120 --> 00:04:01,040
using the match

00:03:58,239 --> 00:04:01,599
criteria as defined by the admin it then

00:04:01,040 --> 00:04:04,000
passes

00:04:01,599 --> 00:04:05,519
off execution to the template logic

00:04:04,000 --> 00:04:07,040
backing each constraint

00:04:05,519 --> 00:04:09,760
which tells gatekeeper whether the

00:04:07,040 --> 00:04:12,080
constraint was violated or not

00:04:09,760 --> 00:04:12,799
gatekeeper then aggregates the results

00:04:12,080 --> 00:04:14,959
and uses

00:04:12,799 --> 00:04:19,040
enforcement actions on the constraint to

00:04:14,959 --> 00:04:22,560
tell the api server how to proceed

00:04:19,040 --> 00:04:24,720
now constraint templates are crds

00:04:22,560 --> 00:04:26,560
and they create constraints which are

00:04:24,720 --> 00:04:28,880
also crds

00:04:26,560 --> 00:04:29,759
so really we have created a system where

00:04:28,880 --> 00:04:32,880
crds

00:04:29,759 --> 00:04:35,600
can create crds and

00:04:32,880 --> 00:04:37,360
there are a few challenges with this

00:04:35,600 --> 00:04:40,320
paradigm

00:04:37,360 --> 00:04:42,800
first we need to have a generic

00:04:40,320 --> 00:04:45,280
controller to handle constraints

00:04:42,800 --> 00:04:47,360
right and we wrote ours using

00:04:45,280 --> 00:04:49,919
unstructured resources

00:04:47,360 --> 00:04:52,000
and wherever we needed to have strongly

00:04:49,919 --> 00:04:55,199
typed sub schemas

00:04:52,000 --> 00:04:57,520
uh you know for things like status we

00:04:55,199 --> 00:04:58,320
serialize the json sub-tree we're

00:04:57,520 --> 00:05:01,440
interested in

00:04:58,320 --> 00:05:04,160
and then re-deserialize it into a

00:05:01,440 --> 00:05:07,199
strongly typed length struct

00:05:04,160 --> 00:05:10,639
now we also need a way to reliably merge

00:05:07,199 --> 00:05:11,360
partial json schemas for the template

00:05:10,639 --> 00:05:14,080
arguments

00:05:11,360 --> 00:05:16,560
and the universal behavior into a fully

00:05:14,080 --> 00:05:19,120
realized constraint schema

00:05:16,560 --> 00:05:21,360
and keeping the two different sub

00:05:19,120 --> 00:05:25,039
schemas like the template schema

00:05:21,360 --> 00:05:26,000
in a special route avoids the

00:05:25,039 --> 00:05:29,120
possibility

00:05:26,000 --> 00:05:32,400
of collisions when the there's

00:05:29,120 --> 00:05:34,880
overlap between the two schemas the most

00:05:32,400 --> 00:05:36,080
difficult problem is handling dynamic

00:05:34,880 --> 00:05:38,560
watches

00:05:36,080 --> 00:05:40,560
originally we did this by creating a sub

00:05:38,560 --> 00:05:42,720
manager that would restart

00:05:40,560 --> 00:05:43,759
every time the set of watched resources

00:05:42,720 --> 00:05:46,000
changed

00:05:43,759 --> 00:05:47,280
and there were two big problems with

00:05:46,000 --> 00:05:49,840
this approach

00:05:47,280 --> 00:05:51,520
the first was that we had inefficient

00:05:49,840 --> 00:05:54,160
memory usage

00:05:51,520 --> 00:05:55,039
because the controller runtimes watch

00:05:54,160 --> 00:05:58,240
cache

00:05:55,039 --> 00:06:01,360
was duplicated across the two managers

00:05:58,240 --> 00:06:04,080
and a larger problem for users was

00:06:01,360 --> 00:06:04,960
that we needed to use finalizers so that

00:06:04,080 --> 00:06:07,120
we could catch

00:06:04,960 --> 00:06:10,560
delete events that may have been missed

00:06:07,120 --> 00:06:12,880
because the sub manager was restarting

00:06:10,560 --> 00:06:14,639
and we've since been able to move on

00:06:12,880 --> 00:06:17,680
from this specific model

00:06:14,639 --> 00:06:18,639
because oren shamron wrote a dynamic

00:06:17,680 --> 00:06:21,039
watcher that

00:06:18,639 --> 00:06:23,520
allows us to change the set of watched

00:06:21,039 --> 00:06:26,400
resources without restarts

00:06:23,520 --> 00:06:29,120
cache duplication or of course

00:06:26,400 --> 00:06:29,120
finalizers

00:06:29,759 --> 00:06:34,160
let's zoom in a bit and discuss how we

00:06:32,639 --> 00:06:36,800
interact with this dynamic

00:06:34,160 --> 00:06:38,319
watch manager gatekeeper has two sets of

00:06:36,800 --> 00:06:40,479
dynamic watches

00:06:38,319 --> 00:06:42,400
one for constraints and the other to

00:06:40,479 --> 00:06:45,919
sync arbitrary data

00:06:42,400 --> 00:06:48,240
for each one of these we have a main

00:06:45,919 --> 00:06:50,160
controller and a dynamic controller

00:06:48,240 --> 00:06:52,400
the main controller is responsible for

00:06:50,160 --> 00:06:52,880
telling the dynamic controller what to

00:06:52,400 --> 00:06:55,120
watch

00:06:52,880 --> 00:06:56,880
the dynamic controller is a generic

00:06:55,120 --> 00:06:59,919
controller that can understand

00:06:56,880 --> 00:07:02,560
a duct type resource we have two

00:06:59,919 --> 00:07:03,360
main controllers one watches constrain

00:07:02,560 --> 00:07:04,880
templates

00:07:03,360 --> 00:07:07,680
it tells the watch manager which

00:07:04,880 --> 00:07:10,400
constrained kinds are available to watch

00:07:07,680 --> 00:07:11,360
the other watch the other one watches

00:07:10,400 --> 00:07:13,919
the config

00:07:11,360 --> 00:07:14,800
resource as that's where users tell

00:07:13,919 --> 00:07:18,000
gatekeeper

00:07:14,800 --> 00:07:20,400
that they want to sync those resources

00:07:18,000 --> 00:07:21,039
there is a potential problem here what

00:07:20,400 --> 00:07:22,960
if two

00:07:21,039 --> 00:07:24,720
main controllers are watching the same

00:07:22,960 --> 00:07:27,599
resource and one stops

00:07:24,720 --> 00:07:29,759
how do we prevent one dynamic watch from

00:07:27,599 --> 00:07:32,000
interfering with another

00:07:29,759 --> 00:07:34,960
we developed the registrar pattern to

00:07:32,000 --> 00:07:37,680
provide isolation

00:07:34,960 --> 00:07:38,880
here's an example of how a main

00:07:37,680 --> 00:07:42,400
controller uses

00:07:38,880 --> 00:07:44,720
the registrar first it would request a

00:07:42,400 --> 00:07:47,280
new registrar from the watch manager

00:07:44,720 --> 00:07:48,240
by providing the controller name and a

00:07:47,280 --> 00:07:50,400
channel by which

00:07:48,240 --> 00:07:51,919
watch events will be sent to the dynamic

00:07:50,400 --> 00:07:54,240
controller

00:07:51,919 --> 00:07:55,520
and when the main controller wants to

00:07:54,240 --> 00:07:57,440
add or remove

00:07:55,520 --> 00:07:59,599
a group version kind from the dynamic

00:07:57,440 --> 00:08:04,080
controller it simply calls

00:07:59,599 --> 00:08:04,080
add watch or remove watch

00:08:05,759 --> 00:08:08,879
each register is namespace to the

00:08:07,919 --> 00:08:11,120
controller

00:08:08,879 --> 00:08:12,240
and is capable of adding or removing an

00:08:11,120 --> 00:08:15,759
intent to watch

00:08:12,240 --> 00:08:17,199
a gvk or replacing the set of watch dvks

00:08:15,759 --> 00:08:19,280
altogether

00:08:17,199 --> 00:08:20,800
the watch manager can then take the

00:08:19,280 --> 00:08:23,360
union of all the intents

00:08:20,800 --> 00:08:25,039
across all registers to figure out what

00:08:23,360 --> 00:08:27,520
resources to watch

00:08:25,039 --> 00:08:29,039
by adding a layer of indirection and

00:08:27,520 --> 00:08:31,599
name spacing intent

00:08:29,039 --> 00:08:33,599
we have made it significantly easier to

00:08:31,599 --> 00:08:35,120
write multiple dynamic controllers that

00:08:33,599 --> 00:08:38,560
watch potentially

00:08:35,120 --> 00:08:42,159
overlapping sets of resources

00:08:38,560 --> 00:08:44,399
now we've touched a lot on meta topics

00:08:42,159 --> 00:08:47,600
right controllers that control

00:08:44,399 --> 00:08:49,519
controllers crds that create crds

00:08:47,600 --> 00:08:51,200
you know ducts that walk like

00:08:49,519 --> 00:08:53,200
constraints

00:08:51,200 --> 00:08:55,200
we could take this process to the

00:08:53,200 --> 00:08:56,320
logical conclusion right we probably

00:08:55,200 --> 00:09:01,440
should

00:08:56,320 --> 00:09:01,440
we should go full meta

00:09:01,519 --> 00:09:05,360
all right so let's take a look at the

00:09:04,080 --> 00:09:08,480
policy enforcement

00:09:05,360 --> 00:09:11,680
as a phenomenon right it follows

00:09:08,480 --> 00:09:13,760
a pretty standard pattern generally

00:09:11,680 --> 00:09:14,959
it's just looking at an object and

00:09:13,760 --> 00:09:18,640
returning a yep

00:09:14,959 --> 00:09:22,560
that looks good or no this is not good

00:09:18,640 --> 00:09:25,760
right and is there any reason that

00:09:22,560 --> 00:09:26,959
this must be done as uh kubernetes

00:09:25,760 --> 00:09:28,959
admission controller

00:09:26,959 --> 00:09:32,320
right do the resources even have to be

00:09:28,959 --> 00:09:32,320
kubernetes resources

00:09:32,560 --> 00:09:38,480
this is probably not true if you walk

00:09:35,839 --> 00:09:41,440
one rung up the abstraction ladder and

00:09:38,480 --> 00:09:42,959
duct type the decision process itself

00:09:41,440 --> 00:09:46,080
right so let's see what that

00:09:42,959 --> 00:09:48,560
might look like so

00:09:46,080 --> 00:09:49,600
here we have the constraint framework

00:09:48,560 --> 00:09:52,160
constraint framework

00:09:49,600 --> 00:09:52,959
is the library that underlies gatekeeper

00:09:52,160 --> 00:09:55,600
it coordinates

00:09:52,959 --> 00:09:56,959
all of the doctyping logic we've covered

00:09:55,600 --> 00:09:58,560
so far

00:09:56,959 --> 00:10:00,640
it provides the execution flow

00:09:58,560 --> 00:10:02,079
gatekeeper uses to render a decision to

00:10:00,640 --> 00:10:04,560
the api server

00:10:02,079 --> 00:10:05,200
it also provides two abstractions that

00:10:04,560 --> 00:10:07,200
allow

00:10:05,200 --> 00:10:08,320
us to define constrained templates and

00:10:07,200 --> 00:10:12,160
constraints

00:10:08,320 --> 00:10:12,160
enforcement points and targets

00:10:12,240 --> 00:10:17,200
there are a few critical behaviors the

00:10:14,880 --> 00:10:19,040
constraint slash template abstraction

00:10:17,200 --> 00:10:21,680
relies on

00:10:19,040 --> 00:10:22,959
some kind of match criteria schema and

00:10:21,680 --> 00:10:25,200
logic

00:10:22,959 --> 00:10:28,079
enforcement actions to tell the system

00:10:25,200 --> 00:10:29,680
what to do when a constraint is unhappy

00:10:28,079 --> 00:10:33,839
and an interface on which the

00:10:29,680 --> 00:10:33,839
enforcement logic can rely

00:10:34,000 --> 00:10:39,040
here we have the target a target

00:10:36,560 --> 00:10:41,279
abstracts the notion of a platform

00:10:39,040 --> 00:10:42,959
what do objects look like how are

00:10:41,279 --> 00:10:46,720
policies bound to them

00:10:42,959 --> 00:10:48,959
what request metadata do i have

00:10:46,720 --> 00:10:50,720
targets give us our match criteria

00:10:48,959 --> 00:10:53,200
schema and logic

00:10:50,720 --> 00:10:55,279
such as what a label selector looks like

00:10:53,200 --> 00:10:57,680
and how to test if a label selector

00:10:55,279 --> 00:10:57,680
matches

00:10:58,240 --> 00:11:01,760
targets also provide constrained

00:10:59,920 --> 00:11:02,560
template authors with the information

00:11:01,760 --> 00:11:05,600
they need

00:11:02,560 --> 00:11:07,920
to evaluate a request like

00:11:05,600 --> 00:11:08,800
what does the object i'm validating look

00:11:07,920 --> 00:11:11,200
like

00:11:08,800 --> 00:11:14,880
what kind of request metadata do i have

00:11:11,200 --> 00:11:17,760
such as requesting user

00:11:14,880 --> 00:11:19,440
an enforcement point is the system that

00:11:17,760 --> 00:11:22,480
asks for a policy check

00:11:19,440 --> 00:11:24,959
and knows what to do with any violations

00:11:22,480 --> 00:11:26,320
gatekeepers web hook is an example of an

00:11:24,959 --> 00:11:28,560
enforcement point

00:11:26,320 --> 00:11:31,440
gatekeeper's audit process is another

00:11:28,560 --> 00:11:31,440
enforcement point

00:11:31,760 --> 00:11:35,760
putting all of this together gives us a

00:11:34,160 --> 00:11:38,880
model for abstracting

00:11:35,760 --> 00:11:40,560
policy enforcement itself we can use the

00:11:38,880 --> 00:11:42,000
higher order abstractions of the

00:11:40,560 --> 00:11:45,040
constraint framework

00:11:42,000 --> 00:11:47,120
targets and enforcement points

00:11:45,040 --> 00:11:48,160
to take the notion of constraints and

00:11:47,120 --> 00:11:52,000
templates to

00:11:48,160 --> 00:11:54,720
other venues for example

00:11:52,000 --> 00:11:56,639
we have gatekeeper of course which uses

00:11:54,720 --> 00:11:59,519
the constraint framework to serve

00:11:56,639 --> 00:12:02,160
validating web hooks and for audit

00:11:59,519 --> 00:12:04,000
capped provides a docker image that can

00:12:02,160 --> 00:12:04,959
be used to validate kubernetes

00:12:04,000 --> 00:12:09,279
configurations

00:12:04,959 --> 00:12:12,000
at rest or as part of a ci cd pipeline

00:12:09,279 --> 00:12:13,279
a cloud config validator wraps the

00:12:12,000 --> 00:12:15,200
constraint framework

00:12:13,279 --> 00:12:18,240
and a target that understands google

00:12:15,200 --> 00:12:18,959
cloud it creates a library that has been

00:12:18,240 --> 00:12:22,480
used for

00:12:18,959 --> 00:12:23,279
a few things it can validate gcp

00:12:22,480 --> 00:12:26,320
resources

00:12:23,279 --> 00:12:29,360
as part of a for seti server deployment

00:12:26,320 --> 00:12:30,240
it can validate gcp resources or

00:12:29,360 --> 00:12:33,839
snapshots

00:12:30,240 --> 00:12:38,079
at rest using cfd scorecard

00:12:33,839 --> 00:12:41,120
and also it can validate terraform plans

00:12:38,079 --> 00:12:42,959
by a project called terraform validator

00:12:41,120 --> 00:12:44,720
now abstracting constraints and

00:12:42,959 --> 00:12:46,720
templates into a library

00:12:44,720 --> 00:12:48,800
has made it faster to bring them to

00:12:46,720 --> 00:12:50,240
other platforms and other policy

00:12:48,800 --> 00:12:53,279
enforcement points which

00:12:50,240 --> 00:12:54,560
allows devs to give users a consistent

00:12:53,279 --> 00:12:58,000
experience

00:12:54,560 --> 00:13:01,360
and helps users execute the same policy

00:12:58,000 --> 00:13:04,000
in many places so duct typing

00:13:01,360 --> 00:13:05,279
has allowed us to bring kate's style

00:13:04,000 --> 00:13:07,760
policies

00:13:05,279 --> 00:13:09,519
outside of the kubernetes cluster

00:13:07,760 --> 00:13:12,959
expanding the potential for

00:13:09,519 --> 00:13:15,360
cool user experiences like rejecting bad

00:13:12,959 --> 00:13:16,480
commits to a ci cd pipeline at the

00:13:15,360 --> 00:13:21,200
pre-submit stage

00:13:16,480 --> 00:13:23,600
which provides defense in depth

00:13:21,200 --> 00:13:24,639
all right now we're going to take a 90

00:13:23,600 --> 00:13:26,720
degree turn

00:13:24,639 --> 00:13:27,920
and talk a bit about gatekeepers

00:13:26,720 --> 00:13:29,839
infrastructure

00:13:27,920 --> 00:13:32,240
let's talk about gatekeeper as a web

00:13:29,839 --> 00:13:35,680
troller

00:13:32,240 --> 00:13:38,800
so what do we mean by web troller

00:13:35,680 --> 00:13:41,839
well gatekeeper is both a web hook

00:13:38,800 --> 00:13:43,199
and a controller at the same time and

00:13:41,839 --> 00:13:46,000
these two things are

00:13:43,199 --> 00:13:47,600
usually very different beasts right when

00:13:46,000 --> 00:13:49,920
we think about web hooks

00:13:47,600 --> 00:13:50,959
well web hooks main job is to serve

00:13:49,920 --> 00:13:53,040
requests

00:13:50,959 --> 00:13:54,000
that means they need to be responsive

00:13:53,040 --> 00:13:57,440
and therefore

00:13:54,000 --> 00:14:00,160
generally intolerant of downtime and

00:13:57,440 --> 00:14:01,600
web hooks also scale their availability

00:14:00,160 --> 00:14:05,199
and serving capacity

00:14:01,600 --> 00:14:07,680
by increasing the number of serving pots

00:14:05,199 --> 00:14:08,639
and each of these pods are peers which

00:14:07,680 --> 00:14:11,360
means there's no

00:14:08,639 --> 00:14:13,360
leader or follower relationship and

00:14:11,360 --> 00:14:16,720
everything is a flat hierarchy where

00:14:13,360 --> 00:14:20,320
every pod is equally able to serve

00:14:16,720 --> 00:14:23,680
any given request controllers

00:14:20,320 --> 00:14:24,880
on the other hand observe and reconcile

00:14:23,680 --> 00:14:26,800
resources

00:14:24,880 --> 00:14:29,199
and come together to create an

00:14:26,800 --> 00:14:31,760
eventually consistent system

00:14:29,199 --> 00:14:32,720
because controllers are background

00:14:31,760 --> 00:14:35,360
processes

00:14:32,720 --> 00:14:36,959
they are a little more downtime tolerant

00:14:35,360 --> 00:14:37,600
because the system only needs to

00:14:36,959 --> 00:14:40,720
converge

00:14:37,600 --> 00:14:44,079
in a reasonable amount of time and

00:14:40,720 --> 00:14:47,199
controllers are generally singletons

00:14:44,079 --> 00:14:49,279
they can use leader election but that

00:14:47,199 --> 00:14:52,000
doesn't really add capacity

00:14:49,279 --> 00:14:52,880
what we do there is extra pods and

00:14:52,000 --> 00:14:55,120
leader election

00:14:52,880 --> 00:14:57,440
usually approve availability by having

00:14:55,120 --> 00:15:00,480
hot standbys that can take over

00:14:57,440 --> 00:15:04,639
if the leader becomes unavailable

00:15:00,480 --> 00:15:07,199
now because gatekeeper is a web hook

00:15:04,639 --> 00:15:09,120
that serves results based off of

00:15:07,199 --> 00:15:11,440
observed resources

00:15:09,120 --> 00:15:12,720
it is a little bit web hook and a little

00:15:11,440 --> 00:15:16,399
bit controller

00:15:12,720 --> 00:15:16,399
therefore web troller

00:15:16,480 --> 00:15:19,760
because of this tension between how web

00:15:18,480 --> 00:15:22,720
plugs and controllers

00:15:19,760 --> 00:15:24,480
usually scale it seems like they may be

00:15:22,720 --> 00:15:26,800
incompatible models

00:15:24,480 --> 00:15:27,839
this apparent conflict can be resolved

00:15:26,800 --> 00:15:30,480
by observing

00:15:27,839 --> 00:15:32,480
that idempotent controller processes

00:15:30,480 --> 00:15:34,959
don't need to be singletons

00:15:32,480 --> 00:15:36,240
if more than one controller is watching

00:15:34,959 --> 00:15:38,800
the same resource

00:15:36,240 --> 00:15:40,399
and they both agree on the end state the

00:15:38,800 --> 00:15:43,680
first controller to write

00:15:40,399 --> 00:15:45,920
the output will win the right other

00:15:43,680 --> 00:15:46,800
controllers will either have not yet

00:15:45,920 --> 00:15:48,800
processed

00:15:46,800 --> 00:15:50,000
that resource or will have their rights

00:15:48,800 --> 00:15:53,040
rejected

00:15:50,000 --> 00:15:54,959
on retry those controllers will see the

00:15:53,040 --> 00:15:56,560
correct state and will not attempt to

00:15:54,959 --> 00:15:58,639
reconcile further

00:15:56,560 --> 00:16:00,079
this is similar to how kubernetes leader

00:15:58,639 --> 00:16:02,480
election works

00:16:00,079 --> 00:16:04,320
this can lead to some extra traffic but

00:16:02,480 --> 00:16:06,639
only when controllers need to write a

00:16:04,320 --> 00:16:06,639
change

00:16:07,199 --> 00:16:10,959
so with this observation we can create

00:16:10,000 --> 00:16:13,839
web trollers

00:16:10,959 --> 00:16:16,240
that leverage what we call leaderless

00:16:13,839 --> 00:16:18,480
horizontal scalability

00:16:16,240 --> 00:16:19,440
in this model a web troller scales

00:16:18,480 --> 00:16:22,959
horizontally

00:16:19,440 --> 00:16:26,320
like a normal web hook right controllers

00:16:22,959 --> 00:16:28,320
are ident and they use this

00:16:26,320 --> 00:16:29,759
first right wins model that we just

00:16:28,320 --> 00:16:33,199
talked about

00:16:29,759 --> 00:16:34,800
and each pod manages its own internal

00:16:33,199 --> 00:16:37,759
cache of constraints

00:16:34,800 --> 00:16:38,480
templates and data which makes every pod

00:16:37,759 --> 00:16:41,759
appear of

00:16:38,480 --> 00:16:44,639
every other pod now there are some

00:16:41,759 --> 00:16:46,000
limitations this model imposes on us

00:16:44,639 --> 00:16:49,120
right we need to be sure

00:16:46,000 --> 00:16:53,279
that non-idempotent operations like

00:16:49,120 --> 00:16:55,519
audit run in a separate singleton pod

00:16:53,279 --> 00:16:57,519
we also should avoid scaling right

00:16:55,519 --> 00:16:58,880
contention quadratically with the number

00:16:57,519 --> 00:17:01,680
of pods which

00:16:58,880 --> 00:17:04,799
can happen if each pod needs to write

00:17:01,680 --> 00:17:07,120
its own pod specific state

00:17:04,799 --> 00:17:08,880
one thing we need to be sure about also

00:17:07,120 --> 00:17:11,679
is that our controllers

00:17:08,880 --> 00:17:12,959
are side effect free as there's no

00:17:11,679 --> 00:17:16,799
guarantee that

00:17:12,959 --> 00:17:16,799
side effects are ident

00:17:16,880 --> 00:17:25,839
that was easy multiple parts on now what

00:17:20,640 --> 00:17:28,079
i think are we done can can we go home

00:17:25,839 --> 00:17:30,000
not quite people probably want to know

00:17:28,079 --> 00:17:32,240
the status of their policies

00:17:30,000 --> 00:17:33,200
whether they're enforced or not this is

00:17:32,240 --> 00:17:37,039
hard because

00:17:33,200 --> 00:17:40,880
kubernetes is eventually consistent

00:17:37,039 --> 00:17:41,760
so multiple pods means multiple possible

00:17:40,880 --> 00:17:45,200
enforcers

00:17:41,760 --> 00:17:46,960
and policy is only as strong as its

00:17:45,200 --> 00:17:50,160
weakest link

00:17:46,960 --> 00:17:52,000
if we have three web hook pods to

00:17:50,160 --> 00:17:55,280
enforcing a new policy

00:17:52,000 --> 00:17:57,360
one not and an api server

00:17:55,280 --> 00:17:58,720
that's going to choose a web hook bob

00:17:57,360 --> 00:18:02,080
randomly

00:17:58,720 --> 00:18:05,280
the policy only really has a 66 percent

00:18:02,080 --> 00:18:06,960
percent chance of being enforced and

00:18:05,280 --> 00:18:08,880
in order to reason about whether a

00:18:06,960 --> 00:18:11,440
constraint is enforced therefore

00:18:08,880 --> 00:18:14,240
we need to know whether it is recognized

00:18:11,440 --> 00:18:16,880
by all pods

00:18:14,240 --> 00:18:18,640
so to do this we implemented a bipod

00:18:16,880 --> 00:18:21,280
status sub-resource

00:18:18,640 --> 00:18:23,120
this resource tells us which pods have

00:18:21,280 --> 00:18:26,559
ingested a given resource

00:18:23,120 --> 00:18:29,679
and what roles like audit or web hook

00:18:26,559 --> 00:18:32,160
those pots perform we also track the

00:18:29,679 --> 00:18:34,080
observed generation of the resource

00:18:32,160 --> 00:18:36,480
to make sure each pod is enforcing the

00:18:34,080 --> 00:18:39,280
most current generation

00:18:36,480 --> 00:18:40,000
the uid of the object to detect if they

00:18:39,280 --> 00:18:41,919
if we are

00:18:40,000 --> 00:18:43,919
actually seeing the status of a deleted

00:18:41,919 --> 00:18:47,280
object that has since been

00:18:43,919 --> 00:18:48,000
recreated and any errors a pod may have

00:18:47,280 --> 00:18:51,120
encountered

00:18:48,000 --> 00:18:51,120
ingesting the resource

00:18:51,360 --> 00:18:55,520
we now have more information about the

00:18:54,240 --> 00:18:58,240
state of the system

00:18:55,520 --> 00:19:00,559
but how do we interpret it

00:18:58,240 --> 00:19:04,240
pessimistically of course

00:19:00,559 --> 00:19:06,559
so if a pod's entry is missing

00:19:04,240 --> 00:19:08,720
we assume that that pod has not yet

00:19:06,559 --> 00:19:12,400
ingested the resource

00:19:08,720 --> 00:19:14,240
if a resources deletion timestamp is set

00:19:12,400 --> 00:19:16,320
we assume that pods have already

00:19:14,240 --> 00:19:19,039
processed that delete

00:19:16,320 --> 00:19:20,320
and if the number of pods reporting

00:19:19,039 --> 00:19:22,799
status

00:19:20,320 --> 00:19:24,400
is equal to the number of pods serving

00:19:22,799 --> 00:19:27,039
the web hook

00:19:24,400 --> 00:19:28,240
well then we can assume that all pods

00:19:27,039 --> 00:19:30,799
have ingested

00:19:28,240 --> 00:19:33,440
and are enforcing that particular

00:19:30,799 --> 00:19:33,440
constraint

00:19:33,600 --> 00:19:37,360
so in order to make these observations

00:19:36,240 --> 00:19:39,360
more meaningful

00:19:37,360 --> 00:19:41,039
we need to enforce some invariants in

00:19:39,360 --> 00:19:44,080
our project

00:19:41,039 --> 00:19:47,360
one if we expect to have n pots

00:19:44,080 --> 00:19:49,520
we must never have n plus one pot

00:19:47,360 --> 00:19:51,919
otherwise counting in observations

00:19:49,520 --> 00:19:53,520
leaves the possibility that there is

00:19:51,919 --> 00:19:56,640
still one pod that has not

00:19:53,520 --> 00:19:59,360
observed a new resource two

00:19:56,640 --> 00:20:01,039
pods cannot serve until they have blue

00:19:59,360 --> 00:20:03,600
strapped all resources

00:20:01,039 --> 00:20:04,320
present during startup otherwise a new

00:20:03,600 --> 00:20:06,960
pod would

00:20:04,320 --> 00:20:07,760
put the assumption that n observations

00:20:06,960 --> 00:20:10,880
means

00:20:07,760 --> 00:20:14,400
in enforcing pots into question

00:20:10,880 --> 00:20:15,280
and lastly we must design our resources

00:20:14,400 --> 00:20:17,440
such that a

00:20:15,280 --> 00:20:18,480
missing resource has a known impact on

00:20:17,440 --> 00:20:20,960
the system

00:20:18,480 --> 00:20:22,880
in this case a missing constraint means

00:20:20,960 --> 00:20:26,480
that policy is enforced

00:20:22,880 --> 00:20:28,799
more loosely than it would otherwise be

00:20:26,480 --> 00:20:30,799
we should also know here that writing

00:20:28,799 --> 00:20:34,000
referential constraints

00:20:30,799 --> 00:20:36,880
which rely on cash data potentially

00:20:34,000 --> 00:20:38,640
violates this principle for one thing

00:20:36,880 --> 00:20:41,360
whether data has been cached

00:20:38,640 --> 00:20:44,080
is unreported for another it's

00:20:41,360 --> 00:20:45,919
impossible to know the significance of

00:20:44,080 --> 00:20:47,600
missing data without knowing the

00:20:45,919 --> 00:20:50,960
specifics of a constraint

00:20:47,600 --> 00:20:53,440
logic the use cases for cache data

00:20:50,960 --> 00:20:55,679
are too valuable to ignore but it's

00:20:53,440 --> 00:20:58,320
worth calling out that such

00:20:55,679 --> 00:21:02,080
templates necessarily have imperfect

00:20:58,320 --> 00:21:04,480
enforcement at the webhook level

00:21:02,080 --> 00:21:06,799
there are two potential problems with

00:21:04,480 --> 00:21:09,679
reporting bipod status

00:21:06,799 --> 00:21:10,559
which are right amplification and the

00:21:09,679 --> 00:21:14,000
possibility of

00:21:10,559 --> 00:21:14,720
zombie status right amplification can

00:21:14,000 --> 00:21:16,799
occur when

00:21:14,720 --> 00:21:18,400
all pods want to write status at the

00:21:16,799 --> 00:21:20,799
same time

00:21:18,400 --> 00:21:21,760
because of kubernetes optimus optimistic

00:21:20,799 --> 00:21:24,400
concurrency

00:21:21,760 --> 00:21:25,760
each pod may try to write the status at

00:21:24,400 --> 00:21:28,640
the same time

00:21:25,760 --> 00:21:30,080
and one will win leaving the other pods

00:21:28,640 --> 00:21:32,559
to retry

00:21:30,080 --> 00:21:34,960
which means that in the worst case there

00:21:32,559 --> 00:21:38,559
will be n squared total write requests

00:21:34,960 --> 00:21:41,840
where n is equal to the number of pods

00:21:38,559 --> 00:21:44,159
zombie status can occur if removed pods

00:21:41,840 --> 00:21:45,840
don't clean up their status

00:21:44,159 --> 00:21:47,280
and so you'll start seeing that old

00:21:45,840 --> 00:21:50,559
status and it'll just

00:21:47,280 --> 00:21:51,440
be there perpetually gatekeeper solves

00:21:50,559 --> 00:21:54,799
these problems

00:21:51,440 --> 00:21:57,919
with a layer of indirection we create

00:21:54,799 --> 00:21:59,120
a separate constraint pod status

00:21:57,919 --> 00:22:01,600
resource

00:21:59,120 --> 00:22:02,320
which is uh there where there's one of

00:22:01,600 --> 00:22:05,360
them per

00:22:02,320 --> 00:22:08,080
unique pod slash constraint tuple

00:22:05,360 --> 00:22:10,559
and each pod will write to its own

00:22:08,080 --> 00:22:12,880
constraint pod status resource

00:22:10,559 --> 00:22:14,880
there's then a singleton controller

00:22:12,880 --> 00:22:17,280
called the status controller

00:22:14,880 --> 00:22:18,080
that aggregates these statuses and

00:22:17,280 --> 00:22:21,440
writes them

00:22:18,080 --> 00:22:23,840
to the constraints this model

00:22:21,440 --> 00:22:25,520
lowers the worst case scaling of write

00:22:23,840 --> 00:22:28,720
requests from quadratic

00:22:25,520 --> 00:22:29,760
to linear and because each constraint

00:22:28,720 --> 00:22:32,960
pod status

00:22:29,760 --> 00:22:33,520
is owned by the corresponding pod we can

00:22:32,960 --> 00:22:35,600
rely

00:22:33,520 --> 00:22:37,919
on kubernetes garbage collection to

00:22:35,600 --> 00:22:41,760
clean up any zombie data that's left by

00:22:37,919 --> 00:22:43,600
old pots

00:22:41,760 --> 00:22:45,840
and that's how we implemented web

00:22:43,600 --> 00:22:48,400
troller all the way from scaling

00:22:45,840 --> 00:22:50,240
web hook parts horizontally to managing

00:22:48,400 --> 00:22:53,280
how they report status

00:22:50,240 --> 00:22:55,840
let's take a look at how the reliability

00:22:53,280 --> 00:22:57,679
and performance of the system may scale

00:22:55,840 --> 00:22:59,760
with the number of pots

00:22:57,679 --> 00:23:00,880
if we assume one serving pot is

00:22:59,760 --> 00:23:03,760
sufficient to serve

00:23:00,880 --> 00:23:06,240
all inbound traffic and that each pot

00:23:03,760 --> 00:23:09,039
fails independently of every other pod

00:23:06,240 --> 00:23:10,240
the probability of downtime decreases

00:23:09,039 --> 00:23:12,720
exponentially

00:23:10,240 --> 00:23:13,760
with the number of running pods on the

00:23:12,720 --> 00:23:16,159
other hand

00:23:13,760 --> 00:23:17,360
because we only consider a constraint as

00:23:16,159 --> 00:23:20,400
being enforced when

00:23:17,360 --> 00:23:22,559
all pots have observed it we lengthen

00:23:20,400 --> 00:23:24,159
the mean time to enforcement as reported

00:23:22,559 --> 00:23:26,000
by the system as a whole

00:23:24,159 --> 00:23:28,080
unfortunately without knowing the exact

00:23:26,000 --> 00:23:32,559
distribution of ingestion times

00:23:28,080 --> 00:23:35,039
it's hard to say how by how much

00:23:32,559 --> 00:23:36,799
to sum up we have covered design

00:23:35,039 --> 00:23:38,240
patterns that have helped us create the

00:23:36,799 --> 00:23:40,960
web troller model which

00:23:38,240 --> 00:23:42,400
include doctyping and various patterns

00:23:40,960 --> 00:23:45,039
we found helpful there

00:23:42,400 --> 00:23:45,679
along with infrastructure and interface

00:23:45,039 --> 00:23:47,840
development

00:23:45,679 --> 00:23:49,679
that helps us both serve and reason

00:23:47,840 --> 00:23:51,760
about our system

00:23:49,679 --> 00:23:53,919
so hopefully some of these patterns will

00:23:51,760 --> 00:23:56,880
be useful to you in your own projects

00:23:53,919 --> 00:24:00,720
and if they do solve an issue for you we

00:23:56,880 --> 00:24:00,720
would definitely love to hear about it

00:24:00,799 --> 00:24:04,000
and with that thank you for attending

00:24:02,799 --> 00:24:06,240
this session

00:24:04,000 --> 00:24:07,120
we just want to thank the gatekeeper

00:24:06,240 --> 00:24:09,520
community

00:24:07,120 --> 00:24:10,480
all the users all the contributors for

00:24:09,520 --> 00:24:12,240
your feedback

00:24:10,480 --> 00:24:13,600
and all the feature requests to make

00:24:12,240 --> 00:24:16,240
gatekeeper

00:24:13,600 --> 00:24:17,039
the project it is today and we also want

00:24:16,240 --> 00:24:18,960
to send our

00:24:17,039 --> 00:24:20,480
thank you to the cube builder controller

00:24:18,960 --> 00:24:22,080
runtime community

00:24:20,480 --> 00:24:23,679
for all the awesome work that has

00:24:22,080 --> 00:24:26,400
bootstrapped the gatekeeper

00:24:23,679 --> 00:24:28,080
project and last but not least our

00:24:26,400 --> 00:24:29,440
wonderful audience for attending this

00:24:28,080 --> 00:24:33,840
session

00:24:29,440 --> 00:24:33,840

YouTube URL: https://www.youtube.com/watch?v=Xwd1Syf-G6E


