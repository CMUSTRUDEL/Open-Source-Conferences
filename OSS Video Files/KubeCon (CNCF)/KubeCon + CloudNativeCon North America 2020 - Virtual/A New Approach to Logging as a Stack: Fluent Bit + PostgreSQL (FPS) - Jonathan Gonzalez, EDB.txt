Title: A New Approach to Logging as a Stack: Fluent Bit + PostgreSQL (FPS) - Jonathan Gonzalez, EDB
Publication date: 2020-11-23
Playlist: KubeCon + CloudNativeCon North America 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon Europe 2021 Virtual from May 4–7, 2021. Learn more at https://kubecon.io. The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects. 

A New Approach to Logging as a Stack: Fluent Bit + PostgreSQL (FPS) - Jonathan Gonzalez, EDB 

Logging at scale is a very interesting challenge, and having the right open source stack is mandatory. There are many ways to solve the log collection, processing and aggregation problem, but when it comes to perform data analysis once the data has been centralized, it's really easy to face latencies and get general problems when indexing our data that don't comes with a fixed structure. In this session, we will share our experience of implementing a new stack that we called FPS: Fluent Bit + PostgreSQL. The open source database PostgreSQL offers a very interesting storage engine for non-columnar structured documents: JSONB (binary JSON) which have shown to be a very efficient and scalable implementation to store trillions of record logs and provide fast results when performing queries. 

https://sched.co/ekD0
Captions: 
	00:00:02,800 --> 00:00:06,319
hello everyone

00:00:03,760 --> 00:00:07,600
i'm jonathan gonzalez from chile and i'm

00:00:06,319 --> 00:00:09,360
currently the maintainer of the

00:00:07,600 --> 00:00:11,679
post resql output plugin for flow and

00:00:09,360 --> 00:00:12,639
beat in the last ngo for my life i've

00:00:11,679 --> 00:00:14,719
been working with

00:00:12,639 --> 00:00:17,359
data processing and log processing for

00:00:14,719 --> 00:00:20,640
reports stats analysis and everything

00:00:17,359 --> 00:00:22,320
infosec reporting even so this time i'm

00:00:20,640 --> 00:00:25,359
going to talk about the

00:00:22,320 --> 00:00:27,519
output plane for fluent beat

00:00:25,359 --> 00:00:29,039
but what does it mean this means that

00:00:27,519 --> 00:00:31,359
i'm going to explain you a lot of stuff

00:00:29,039 --> 00:00:33,120
related to how to process the locks

00:00:31,359 --> 00:00:35,280
on the flow and bit size and push the

00:00:33,120 --> 00:00:38,239
locks inside pull the sql

00:00:35,280 --> 00:00:39,920
so let's start the talk one of the

00:00:38,239 --> 00:00:42,879
motivation to pay

00:00:39,920 --> 00:00:43,280
start this plugin and a year and a half

00:00:42,879 --> 00:00:45,120
ago

00:00:43,280 --> 00:00:46,879
it was to play with the post sql type

00:00:45,120 --> 00:00:49,200
json b which is apparently

00:00:46,879 --> 00:00:50,879
representation of javascript object

00:00:49,200 --> 00:00:53,680
notation

00:00:50,879 --> 00:00:54,239
we needed to create basically to process

00:00:53,680 --> 00:00:57,039
uh

00:00:54,239 --> 00:00:58,239
years of logs and find one line between

00:00:57,039 --> 00:01:01,440
millions of files

00:00:58,239 --> 00:01:02,160
wasn't that easy at that time we needed

00:01:01,440 --> 00:01:05,360
to generate

00:01:02,160 --> 00:01:07,119
users report infosys reports for

00:01:05,360 --> 00:01:09,520
i don't know administration management

00:01:07,119 --> 00:01:11,600
stuff like that

00:01:09,520 --> 00:01:13,600
and maybe create some start from

00:01:11,600 --> 00:01:15,439
starting from these logs

00:01:13,600 --> 00:01:17,040
like i don't know in summer we have a

00:01:15,439 --> 00:01:20,080
lot of visits on this site or

00:01:17,040 --> 00:01:23,600
this summer users started to log in

00:01:20,080 --> 00:01:25,200
more stuff like that people can ask

00:01:23,600 --> 00:01:28,560
questions

00:01:25,200 --> 00:01:31,280
in the beginning the initial idea was uh

00:01:28,560 --> 00:01:32,320
to follow this simple process fluid bit

00:01:31,280 --> 00:01:35,119
collect data

00:01:32,320 --> 00:01:36,079
collect logs from different sources

00:01:35,119 --> 00:01:38,720
using different

00:01:36,079 --> 00:01:40,320
input plugins and send them directly

00:01:38,720 --> 00:01:42,399
into poster sql

00:01:40,320 --> 00:01:43,920
into the table and from there you will

00:01:42,399 --> 00:01:46,159
be installing logs

00:01:43,920 --> 00:01:47,280
but with time and talking with my

00:01:46,159 --> 00:01:49,680
colleagues

00:01:47,280 --> 00:01:50,560
we finally decided that a best idea

00:01:49,680 --> 00:01:53,680
would be

00:01:50,560 --> 00:01:56,079
keep fluent bit sending logs into post

00:01:53,680 --> 00:01:56,799
sql but first process them using pl's

00:01:56,079 --> 00:02:00,079
sql

00:01:56,799 --> 00:02:01,920
language this will be allow us to

00:02:00,079 --> 00:02:04,719
separate files in between different

00:02:01,920 --> 00:02:08,560
tables or maybe purposes in something

00:02:04,719 --> 00:02:12,239
or even just create some rules

00:02:08,560 --> 00:02:17,440
we have the logs inside our sql

00:02:12,239 --> 00:02:19,920
or in our case inside our pl sql

00:02:17,440 --> 00:02:21,680
from it will send the data as the data

00:02:19,920 --> 00:02:23,200
logs as json objects

00:02:21,680 --> 00:02:24,720
which mean that they will have a big

00:02:23,200 --> 00:02:28,160
json plus

00:02:24,720 --> 00:02:31,120
some data in our case it will contain uh

00:02:28,160 --> 00:02:33,280
times a timestamp and attack plus the

00:02:31,120 --> 00:02:34,720
json object with everything

00:02:33,280 --> 00:02:36,800
in the main table we will sort the raw

00:02:34,720 --> 00:02:39,840
data into one main table

00:02:36,800 --> 00:02:41,040
meaning it's the table option in the in

00:02:39,840 --> 00:02:44,720
the plugin

00:02:41,040 --> 00:02:46,720
and we decided to process with using pl

00:02:44,720 --> 00:02:50,640
sql to process and split the data into

00:02:46,720 --> 00:02:53,599
many tables one table or just

00:02:50,640 --> 00:02:55,360
split the data for the table or using

00:02:53,599 --> 00:02:58,480
store them in the same table that's

00:02:55,360 --> 00:03:01,760
not an issue so we can use

00:02:58,480 --> 00:03:05,120
any object inside

00:03:01,760 --> 00:03:07,120
any field inside the json object to

00:03:05,120 --> 00:03:08,959
decide when to store the data where to

00:03:07,120 --> 00:03:12,319
store it uh split it by

00:03:08,959 --> 00:03:14,640
using some special handle

00:03:12,319 --> 00:03:15,519
or anything like that so you will take

00:03:14,640 --> 00:03:19,040
the decision

00:03:15,519 --> 00:03:23,519
based on the json object

00:03:19,040 --> 00:03:25,599
but what is json b or json on pos sql

00:03:23,519 --> 00:03:27,120
well json is just javascript object

00:03:25,599 --> 00:03:30,640
notation

00:03:27,120 --> 00:03:32,640
in in in our pos sql there's a

00:03:30,640 --> 00:03:36,080
secure technical report that you can

00:03:32,640 --> 00:03:36,080
check out on the slide right here

00:03:36,159 --> 00:03:42,560
template which is the implementation of

00:03:39,120 --> 00:03:42,560
json on post sql

00:03:42,799 --> 00:03:48,319
the idea is you can have json or json b

00:03:46,319 --> 00:03:50,640
but we use json b because you support

00:03:48,319 --> 00:03:51,840
indexing which means that it's easy to

00:03:50,640 --> 00:03:55,760
create the data

00:03:51,840 --> 00:03:59,439
and keep it indexed because we know that

00:03:55,760 --> 00:04:02,480
index create faster queries right

00:03:59,439 --> 00:04:04,000
and it's easy to query json is just you

00:04:02,480 --> 00:04:06,480
can select data

00:04:04,000 --> 00:04:07,360
and a field inside in this case date

00:04:06,480 --> 00:04:10,560
from the table

00:04:07,360 --> 00:04:12,799
and you will get the data but

00:04:10,560 --> 00:04:14,000
one of the biggest reason is to use json

00:04:12,799 --> 00:04:15,680
is that it is really

00:04:14,000 --> 00:04:17,040
really easy to export and using other

00:04:15,680 --> 00:04:20,239
apps in this case

00:04:17,040 --> 00:04:22,720
uh we can export uh json

00:04:20,239 --> 00:04:23,600
to any option or application we want to

00:04:22,720 --> 00:04:26,000
process data

00:04:23,600 --> 00:04:27,759
or to process logs from time to time or

00:04:26,000 --> 00:04:30,479
anything

00:04:27,759 --> 00:04:32,560
how we can configure this uh plane the

00:04:30,479 --> 00:04:34,880
main configuration you will need

00:04:32,560 --> 00:04:36,240
obviously the host because you will may

00:04:34,880 --> 00:04:37,120
not have password sql running on

00:04:36,240 --> 00:04:39,520
localhost

00:04:37,120 --> 00:04:40,400
and the port people will usually don't

00:04:39,520 --> 00:04:43,440
change the port but

00:04:40,400 --> 00:04:44,479
maybe you will need to change it uh user

00:04:43,440 --> 00:04:46,160
and password

00:04:44,479 --> 00:04:48,639
you can put it on the configuration but

00:04:46,160 --> 00:04:50,960
we strongly recommend use pgpas file

00:04:48,639 --> 00:04:52,080
because it's the best option and secure

00:04:50,960 --> 00:04:54,160
for in this case

00:04:52,080 --> 00:04:56,240
the database and a table the table will

00:04:54,160 --> 00:04:58,000
be created if it's not exist

00:04:56,240 --> 00:04:59,759
on the plugin when it's a startup and

00:04:58,000 --> 00:05:02,320
connects to pulse sql so

00:04:59,759 --> 00:05:04,720
you don't need to create it and we

00:05:02,320 --> 00:05:07,759
support codextv using codecrossdb

00:05:04,720 --> 00:05:09,919
equal true this is a codecrdb support

00:05:07,759 --> 00:05:12,720
postsql protocol

00:05:09,919 --> 00:05:14,880
but some queries don't they don't

00:05:12,720 --> 00:05:15,199
support some functions and some queries

00:05:14,880 --> 00:05:17,440
may

00:05:15,199 --> 00:05:18,639
change so we created some special option

00:05:17,440 --> 00:05:20,639
for it

00:05:18,639 --> 00:05:21,919
there's a full list of options for the

00:05:20,639 --> 00:05:23,919
plugin some

00:05:21,919 --> 00:05:27,840
interesting some are not you can test

00:05:23,919 --> 00:05:31,039
them and please report any back on it

00:05:27,840 --> 00:05:32,240
so how to query the data the data as we

00:05:31,039 --> 00:05:36,400
said before is

00:05:32,240 --> 00:05:40,560
stored on json with two fields stack

00:05:36,400 --> 00:05:44,639
time and data containing the json object

00:05:40,560 --> 00:05:48,080
here is an example of data of one record

00:05:44,639 --> 00:05:50,560
from fluent bit using the tag cpu.0

00:05:48,080 --> 00:05:54,560
which is just using the

00:05:50,560 --> 00:05:57,440
input plugging from fluid bit cpu

00:05:54,560 --> 00:05:58,960
and limit into one you will see the data

00:05:57,440 --> 00:06:02,000
over there

00:05:58,960 --> 00:06:03,759
it's nice easy and simple

00:06:02,000 --> 00:06:06,400
but in this case with apache loss you

00:06:03,759 --> 00:06:09,280
can see something else more interesting

00:06:06,400 --> 00:06:10,400
in this case we use the tag to separate

00:06:09,280 --> 00:06:13,520
the logs

00:06:10,400 --> 00:06:15,600
tag apache and you will see that we

00:06:13,520 --> 00:06:18,160
contain the code date everything that

00:06:15,600 --> 00:06:20,800
comes using the apache 2 parser

00:06:18,160 --> 00:06:22,080
from fluency this is what's using the

00:06:20,800 --> 00:06:25,440
tail plugin into

00:06:22,080 --> 00:06:26,240
some processing apache locks but gears

00:06:25,440 --> 00:06:27,919
of logs

00:06:26,240 --> 00:06:30,319
we needed to analyze three years of

00:06:27,919 --> 00:06:31,199
apache logs which is a really really big

00:06:30,319 --> 00:06:33,919
tax

00:06:31,199 --> 00:06:34,560
because we have around one terabyte of

00:06:33,919 --> 00:06:36,160
data

00:06:34,560 --> 00:06:38,080
and we needed to process it in just two

00:06:36,160 --> 00:06:39,919
weeks so for this

00:06:38,080 --> 00:06:42,400
we decided to use the tail plugin which

00:06:39,919 --> 00:06:44,080
needed some updated and we added some

00:06:42,400 --> 00:06:47,600
options into the documentation

00:06:44,080 --> 00:06:48,160
to process the pos sql uh the apache

00:06:47,600 --> 00:06:50,800
locks

00:06:48,160 --> 00:06:52,880
into post sql we use the pl sql language

00:06:50,800 --> 00:06:53,680
the processing speed data into different

00:06:52,880 --> 00:06:56,240
tables

00:06:53,680 --> 00:06:57,520
which because otherwise we will have

00:06:56,240 --> 00:07:00,400
billions of rows inside

00:06:57,520 --> 00:07:02,160
just one table but in our case we use

00:07:00,400 --> 00:07:04,960
partition tables per month

00:07:02,160 --> 00:07:05,280
which allow us to create proper indexes

00:07:04,960 --> 00:07:08,240
and

00:07:05,280 --> 00:07:08,560
query just one amount and this query for

00:07:08,240 --> 00:07:10,800
month

00:07:08,560 --> 00:07:12,240
took less than one second which is

00:07:10,800 --> 00:07:14,319
really really amazing because in other

00:07:12,240 --> 00:07:14,880
databases store logs this will obviously

00:07:14,319 --> 00:07:16,960
create

00:07:14,880 --> 00:07:19,039
more than one second because we have a

00:07:16,960 --> 00:07:22,000
lot of data so let's deploy

00:07:19,039 --> 00:07:24,000
uh inside kubernetes our fluent bit plus

00:07:22,000 --> 00:07:26,160
sql output plane we will provide some

00:07:24,000 --> 00:07:28,000
url on github which is open source

00:07:26,160 --> 00:07:29,680
and you will be able to use customize to

00:07:28,000 --> 00:07:32,560
deploy inside so

00:07:29,680 --> 00:07:34,479
let's have some fun in the final

00:07:32,560 --> 00:07:36,960
configuration you will be able to see

00:07:34,479 --> 00:07:39,039
the simple fluid bit configuration with

00:07:36,960 --> 00:07:42,840
the include statement to add

00:07:39,039 --> 00:07:45,360
the conf files into the front bit

00:07:42,840 --> 00:07:49,520
configuration

00:07:45,360 --> 00:07:51,759
and in the file pause sql customization

00:07:49,520 --> 00:07:52,639
is just a conflict map generator that

00:07:51,759 --> 00:07:55,360
will

00:07:52,639 --> 00:07:56,879
use the merge behavior to add output

00:07:55,360 --> 00:08:00,400
postgresql.com file

00:07:56,879 --> 00:08:03,680
into the fluent bit config map

00:08:00,400 --> 00:08:05,440
and in the output postsql.conf

00:08:03,680 --> 00:08:06,879
file you will be able to see the host

00:08:05,440 --> 00:08:09,280
which is out of

00:08:06,879 --> 00:08:10,400
kubernetes deployment using a dummy

00:08:09,280 --> 00:08:14,000
password

00:08:10,400 --> 00:08:15,440
and the user fluent bit

00:08:14,000 --> 00:08:16,879
for the database fluent bit and

00:08:15,440 --> 00:08:17,680
tablespoon bit that will match

00:08:16,879 --> 00:08:20,800
everything

00:08:17,680 --> 00:08:20,800
to the output plugin

00:08:22,160 --> 00:08:28,080
okay that was easy but let's use some

00:08:25,199 --> 00:08:30,000
pl sql language because we want all the

00:08:28,080 --> 00:08:33,200
data in a separate table

00:08:30,000 --> 00:08:36,640
because obviously we get

00:08:33,200 --> 00:08:37,839
logs from many places so let's partition

00:08:36,640 --> 00:08:39,519
that table

00:08:37,839 --> 00:08:41,919
because maybe in the future we will need

00:08:39,519 --> 00:08:45,200
partition by month

00:08:41,919 --> 00:08:46,800
year weeks or days and use

00:08:45,200 --> 00:08:48,880
some condition to fulfill the empty

00:08:46,800 --> 00:08:52,240
fields so

00:08:48,880 --> 00:08:52,240
let's see how this will work

00:08:53,600 --> 00:08:57,920
in our example code we will first create

00:08:56,000 --> 00:08:59,680
our table with all the data we want to

00:08:57,920 --> 00:09:01,600
store with a partition option

00:08:59,680 --> 00:09:05,680
that will be the useful in case we went

00:09:01,600 --> 00:09:05,680
about to partition our table later

00:09:05,760 --> 00:09:09,040
it's important that our table is owned

00:09:08,160 --> 00:09:12,800
by the fluenb

00:09:09,040 --> 00:09:14,959
user site is the one inserting the data

00:09:12,800 --> 00:09:15,839
after that we create a default partition

00:09:14,959 --> 00:09:18,800
will be the one

00:09:15,839 --> 00:09:22,560
by default then we can create more

00:09:18,800 --> 00:09:24,959
partition after if we want

00:09:22,560 --> 00:09:28,720
we now create our function that will do

00:09:24,959 --> 00:09:30,560
the final insert into the table

00:09:28,720 --> 00:09:32,560
it's important that if the json object

00:09:30,560 --> 00:09:34,160
doesn't come with a kubernetes object we

00:09:32,560 --> 00:09:36,480
can discard that row

00:09:34,160 --> 00:09:38,080
and return the full row since we want to

00:09:36,480 --> 00:09:40,959
store the data even if it's not a

00:09:38,080 --> 00:09:40,959
kubernetes object

00:09:42,160 --> 00:09:47,040
then we go and start with the insert

00:09:44,320 --> 00:09:48,800
into our table

00:09:47,040 --> 00:09:50,160
it's important to notice that we want a

00:09:48,800 --> 00:09:52,000
timestamp type

00:09:50,160 --> 00:09:53,760
and make sure it will be like that for

00:09:52,000 --> 00:09:57,279
that we use the default column

00:09:53,760 --> 00:10:00,560
that comes with the plugin then we can

00:09:57,279 --> 00:10:03,760
insert the data we want in our

00:10:00,560 --> 00:10:07,920
case it will contain container image

00:10:03,760 --> 00:10:10,160
container name namespace and host

00:10:07,920 --> 00:10:12,160
we will add the labels and annotation

00:10:10,160 --> 00:10:14,160
just to prove that we can add json data

00:10:12,160 --> 00:10:16,560
too

00:10:14,160 --> 00:10:18,240
we return an old value because we don't

00:10:16,560 --> 00:10:20,880
want to store the data in the default

00:10:18,240 --> 00:10:23,920
table but the one we decided

00:10:20,880 --> 00:10:23,920
in our trigger

00:10:24,240 --> 00:10:29,680
let's drop the trigger if exist and

00:10:26,240 --> 00:10:31,680
create our trigger for our default table

00:10:29,680 --> 00:10:32,720
this trigger will be executed for each

00:10:31,680 --> 00:10:47,839
row inserted

00:10:32,720 --> 00:10:47,839
in the table

00:10:59,279 --> 00:11:02,959
so for the record we will let the

00:11:00,880 --> 00:11:04,399
functions the pl sql function on the

00:11:02,959 --> 00:11:06,399
slide so you can take

00:11:04,399 --> 00:11:08,720
take them and use them later as an

00:11:06,399 --> 00:11:10,000
example but you can see here that we can

00:11:08,720 --> 00:11:12,160
query the data using

00:11:10,000 --> 00:11:14,000
select distance based on the container

00:11:12,160 --> 00:11:15,600
field in the kubernetes log

00:11:14,000 --> 00:11:18,160
and we will see that we have five

00:11:15,600 --> 00:11:18,720
records but there's one with an empty

00:11:18,160 --> 00:11:20,720
field

00:11:18,720 --> 00:11:21,839
maybe we need to look into this later

00:11:20,720 --> 00:11:23,680
and see that

00:11:21,839 --> 00:11:25,519
some objects will not come with the

00:11:23,680 --> 00:11:26,720
container name or something like that

00:11:25,519 --> 00:11:29,440
maybe that's a bug

00:11:26,720 --> 00:11:32,000
but you can take a look and that will be

00:11:29,440 --> 00:11:32,000
your task

00:11:34,160 --> 00:11:38,079
but appeal sql function can be more

00:11:36,399 --> 00:11:40,720
complicated because sometimes

00:11:38,079 --> 00:11:42,240
we may want to send the data into more

00:11:40,720 --> 00:11:45,440
than one just table

00:11:42,240 --> 00:11:48,560
okay so let's see a function that can

00:11:45,440 --> 00:11:50,560
speed the data depending on the tag if

00:11:48,560 --> 00:11:51,680
the input doesn't match attack send it

00:11:50,560 --> 00:11:56,399
to a default table

00:11:51,680 --> 00:11:58,880
so we may be ending having three tables

00:11:56,399 --> 00:12:00,959
so the following function will show you

00:11:58,880 --> 00:12:04,000
how to split between

00:12:00,959 --> 00:12:05,680
tags used with apache and tags

00:12:04,000 --> 00:12:07,120
or the data that comes with kubernetes

00:12:05,680 --> 00:12:10,000
inside

00:12:07,120 --> 00:12:10,800
you can use this function as example for

00:12:10,000 --> 00:12:14,320
later

00:12:10,800 --> 00:12:15,600
but uh go and have some fun because

00:12:14,320 --> 00:12:17,279
there's a lot of options you can use

00:12:15,600 --> 00:12:20,560
here to split your data

00:12:17,279 --> 00:12:20,560
and query it later

00:12:21,600 --> 00:12:24,720
we will see some this is a pre-recorded

00:12:23,680 --> 00:12:26,959
video and you will see

00:12:24,720 --> 00:12:28,399
some examples like this one where in all

00:12:26,959 --> 00:12:30,480
the

00:12:28,399 --> 00:12:31,760
fields on kubernetes log table which is

00:12:30,480 --> 00:12:33,920
not ideal

00:12:31,760 --> 00:12:36,399
but we can select distinct containers

00:12:33,920 --> 00:12:37,680
image from the kubernetes logs

00:12:36,399 --> 00:12:41,440
in this case we will select the

00:12:37,680 --> 00:12:41,440
container image and count them

00:12:50,399 --> 00:12:56,079
obviously we need to group by a field

00:12:53,519 --> 00:12:57,839
in this case container image and see

00:12:56,079 --> 00:13:00,160
that we have a lot of container

00:12:57,839 --> 00:13:00,959
but maybe we want to know what if we

00:13:00,160 --> 00:13:03,440
split the

00:13:00,959 --> 00:13:04,800
containers and count them by host that's

00:13:03,440 --> 00:13:07,600
easy just add cost

00:13:04,800 --> 00:13:08,480
and group by host then we have all the

00:13:07,600 --> 00:13:11,839
containers

00:13:08,480 --> 00:13:15,760
split them using host so

00:13:11,839 --> 00:13:18,880
yeah this is a nice and

00:13:15,760 --> 00:13:21,839
this is cool but let's see something

00:13:18,880 --> 00:13:21,839
else

00:13:35,519 --> 00:13:38,800
in this case we are going to select all

00:13:37,920 --> 00:13:40,720
the fields

00:13:38,800 --> 00:13:42,720
from the kubernetes logs where the

00:13:40,720 --> 00:13:44,800
container image is fluent bit

00:13:42,720 --> 00:13:45,760
okay there's a lot of data maybe that's

00:13:44,800 --> 00:13:49,120
not so useful

00:13:45,760 --> 00:13:52,320
you know uh well

00:13:49,120 --> 00:13:54,480
but you can see it's sql so

00:13:52,320 --> 00:13:56,000
let's uh do something different what

00:13:54,480 --> 00:13:59,040
about distinct host

00:13:56,000 --> 00:14:00,839
and count all the

00:13:59,040 --> 00:14:03,839
fluent bit image running on different

00:14:00,839 --> 00:14:03,839
hosts

00:14:06,320 --> 00:14:13,120
okay we have some good numbers here

00:14:09,680 --> 00:14:17,839
i will ask why we have 13

00:14:13,120 --> 00:14:17,839
just on master 02 but that's up

00:14:20,639 --> 00:14:23,760
in our in the following case we will see

00:14:22,720 --> 00:14:26,959
some uh

00:14:23,760 --> 00:14:28,160
kubernetes law and split all the things

00:14:26,959 --> 00:14:30,240
that is running on

00:14:28,160 --> 00:14:32,880
master zero one with the continued image

00:14:30,240 --> 00:14:32,880
fluent bit

00:14:35,680 --> 00:14:42,480
okay that's a lot of information again

00:14:39,360 --> 00:14:44,560
but maybe we need to

00:14:42,480 --> 00:14:46,320
split it that'll work later so let's see

00:14:44,560 --> 00:14:48,160
something else what about the apache

00:14:46,320 --> 00:14:50,480
logs

00:14:48,160 --> 00:14:52,480
as you can see the example we used just

00:14:50,480 --> 00:14:53,600
for field host pattern code plus the

00:14:52,480 --> 00:14:56,480
timestamp

00:14:53,600 --> 00:14:57,440
which is a lot of data maybe not so much

00:14:56,480 --> 00:14:59,760
data but

00:14:57,440 --> 00:15:02,000
it will contain different fields so as

00:14:59,760 --> 00:15:05,680
you can see there's a lot of data but

00:15:02,000 --> 00:15:13,839
not so useful let's group by uh the path

00:15:05,680 --> 00:15:13,839
and count them

00:15:17,600 --> 00:15:24,959
okay this is a lot of data here

00:15:21,600 --> 00:15:28,240
not too much data doesn't say anything

00:15:24,959 --> 00:15:28,240
let's do something different

00:15:32,160 --> 00:15:41,440
what about the adding the host but

00:15:36,720 --> 00:15:44,079
yeah let's work with the host

00:15:41,440 --> 00:15:44,639
okay what now we have the host but still

00:15:44,079 --> 00:15:48,240
not

00:15:44,639 --> 00:15:48,240
so much information useful

00:15:50,880 --> 00:15:57,040
so let's use something to

00:15:54,000 --> 00:15:57,839
split the data but about but like i

00:15:57,040 --> 00:16:00,320
don't know

00:15:57,839 --> 00:16:00,320
wordpress

00:16:07,040 --> 00:16:10,480
there's a lot of data with wordpress

00:16:12,880 --> 00:16:16,720
well this is something that will happen

00:16:14,880 --> 00:16:19,920
but we want everything that

00:16:16,720 --> 00:16:21,680
the code is 200 no

00:16:19,920 --> 00:16:23,680
that's a lot of 200 as you can see this

00:16:21,680 --> 00:16:26,240
is a wordpress

00:16:23,680 --> 00:16:26,240
running there

00:16:28,320 --> 00:16:34,000
so let's see everything that it's not

00:16:30,560 --> 00:16:34,000
found okay this is more nice

00:16:34,240 --> 00:16:40,320
but you know it's a patch locks

00:16:37,920 --> 00:16:41,279
you can you will have a lot of data here

00:16:40,320 --> 00:16:51,839
so let's see some

00:16:41,279 --> 00:16:51,839
other examples with different queries

00:16:53,199 --> 00:16:59,600
but let's see now what is in the default

00:16:57,199 --> 00:17:00,480
table through a bit oh there's a lot of

00:16:59,600 --> 00:17:03,040
records

00:17:00,480 --> 00:17:06,000
okay let's see the different text we

00:17:03,040 --> 00:17:06,000
have we got there

00:17:06,079 --> 00:17:09,120
and as you can see we have apache log

00:17:07,760 --> 00:17:12,720
that maybe didn't fit

00:17:09,120 --> 00:17:15,520
on the filter and the cpu

00:17:12,720 --> 00:17:16,839
so let's see all the data that is has

00:17:15,520 --> 00:17:19,839
the

00:17:16,839 --> 00:17:19,839
cpu

00:17:28,079 --> 00:17:32,160
okay you will see that there's a lot of

00:17:30,240 --> 00:17:34,880
data that didn't fit in our

00:17:32,160 --> 00:17:36,000
functions so it was stored on the

00:17:34,880 --> 00:17:37,760
default table

00:17:36,000 --> 00:17:39,039
that was the sense of having a default

00:17:37,760 --> 00:17:42,000
table because you

00:17:39,039 --> 00:17:42,720
may not want to lose any data at any

00:17:42,000 --> 00:17:46,880
moment

00:17:42,720 --> 00:17:48,080
of your process so let's see some data

00:17:46,880 --> 00:17:50,640
for example data

00:17:48,080 --> 00:17:51,200
for the cpu as you can see it's just

00:17:50,640 --> 00:18:00,160
like

00:17:51,200 --> 00:18:02,880
everything on fluid

00:18:00,160 --> 00:18:03,520
as you'll see we can do a lot of stuff

00:18:02,880 --> 00:18:05,919
with

00:18:03,520 --> 00:18:07,120
this plugin and once the data is on

00:18:05,919 --> 00:18:09,280
pulse sql

00:18:07,120 --> 00:18:11,520
but there's some ideas you may want to

00:18:09,280 --> 00:18:14,000
experiment right now

00:18:11,520 --> 00:18:14,799
like graph the data using rafana which

00:18:14,000 --> 00:18:17,840
is a

00:18:14,799 --> 00:18:18,480
very useful tool to create graphs split

00:18:17,840 --> 00:18:21,360
the data

00:18:18,480 --> 00:18:21,919
per weeks not just month or maybe per

00:18:21,360 --> 00:18:24,799
year

00:18:21,919 --> 00:18:25,280
if you have that amount of time maybe a

00:18:24,799 --> 00:18:28,480
day

00:18:25,280 --> 00:18:31,919
would be useful to start testing stuff

00:18:28,480 --> 00:18:32,640
create some script to out for automatics

00:18:31,919 --> 00:18:34,799
reports

00:18:32,640 --> 00:18:37,840
per month or per year that's also some

00:18:34,799 --> 00:18:40,160
idea you can get from here

00:18:37,840 --> 00:18:41,360
maybe there's something else you can add

00:18:40,160 --> 00:18:44,960
in the ideas

00:18:41,360 --> 00:18:47,039
so please drop me a message right here

00:18:44,960 --> 00:18:48,799
after this talk you are now able to use

00:18:47,039 --> 00:18:50,720
fluid plus positive square in its

00:18:48,799 --> 00:18:53,120
capabilities

00:18:50,720 --> 00:18:54,559
we are now going to add ssl connection

00:18:53,120 --> 00:18:56,720
support for pulse sql

00:18:54,559 --> 00:18:58,000
and schema support this will mean that

00:18:56,720 --> 00:19:01,039
you will be able to

00:18:58,000 --> 00:19:02,320
use ssl to connect to password sql in

00:19:01,039 --> 00:19:03,440
the meantime we hope to get some

00:19:02,320 --> 00:19:06,160
feedback from the users

00:19:03,440 --> 00:19:07,200
you and any kind of idea to add to the

00:19:06,160 --> 00:19:09,919
plugin

00:19:07,200 --> 00:19:17,760
so please contact me on twitter and

00:19:09,919 --> 00:19:17,760

YouTube URL: https://www.youtube.com/watch?v=pJtma71yAOg


