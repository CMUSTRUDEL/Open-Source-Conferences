Title: Kata Containers Performance Evaluation and Optimization on Arm64 - Jia He, Arm
Publication date: 2020-11-23
Playlist: KubeCon + CloudNativeCon North America 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon Europe 2021 Virtual from May 4–7, 2021. Learn more at https://kubecon.io. The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects. 

Kata Containers Performance Evaluation and Optimization on Arm64 - Jia He, Arm 

Kata Containers builds extremely lightweight virtual machines that seamlessly plug into the containers ecosystem. It is a multi-architecture project which has been supported on X86, aarch64, ppc, s390. Jia He will introduce current status of kata containers on aarch64, focusing on the performance evaluation on aarch64. Includes: 1. metrics report: boot time, memory footprint, network and storage io. 2. Real-life test case includes nginx, redis and tensorflow 3. Kata containers vs runc containers Besides, Jia He will also introduce what Arm container team has done to optimize the performance: 1. virtiofs enablement and bugfix 2. nvdimm support and bugfix 3. kvm ptp enablement 4. Misc performance optimization 

https://sched.co/ekFW
Captions: 
	00:00:01,839 --> 00:00:04,560
hello everyone good morning good

00:00:03,840 --> 00:00:08,960
afternoon

00:00:04,560 --> 00:00:11,920
and good evening thanks for attending my

00:00:08,960 --> 00:00:11,920
topic today

00:00:13,440 --> 00:00:20,080
today my topic is about containers

00:00:16,960 --> 00:00:25,279
performance evaluation and optimization

00:00:20,080 --> 00:00:28,320
of arm 64. my name is justin kerr

00:00:25,279 --> 00:00:31,359
i mainly focus focus on the

00:00:28,320 --> 00:00:34,640
container virtualization technology

00:00:31,359 --> 00:00:34,640
on arm 64.

00:00:40,239 --> 00:00:46,320
here is today's agenda

00:00:43,280 --> 00:00:49,680
firstly i will give you a

00:00:46,320 --> 00:00:53,280
broad brief introduction about

00:00:49,680 --> 00:00:53,280
what is cut container

00:00:54,480 --> 00:01:01,840
then i will give you the status update

00:00:58,480 --> 00:01:04,320
on arm 64. that is what we have done

00:01:01,840 --> 00:01:04,320
so far

00:01:06,640 --> 00:01:10,020
then i will go through the performance

00:01:08,880 --> 00:01:11,600
evaluation

00:01:10,020 --> 00:01:15,920
[Music]

00:01:11,600 --> 00:01:18,960
from several aspects

00:01:15,920 --> 00:01:22,560
at last i will introduce to

00:01:18,960 --> 00:01:27,840
the two simple users

00:01:22,560 --> 00:01:27,840
user stories in real cases

00:01:34,560 --> 00:01:40,159
if you have used the dot container maybe

00:01:38,079 --> 00:01:43,520
you have thought about

00:01:40,159 --> 00:01:47,840
the question how to make

00:01:43,520 --> 00:01:51,040
the container more secure

00:01:47,840 --> 00:01:55,439
you can drop some

00:01:51,040 --> 00:01:59,040
linux capabilities such such as

00:01:55,439 --> 00:02:03,439
make you you can mount the root fence

00:01:59,040 --> 00:02:06,479
with read-only mode you can use

00:02:03,439 --> 00:02:09,920
sc linux and app armor

00:02:06,479 --> 00:02:10,800
to protect the container and also you

00:02:09,920 --> 00:02:13,920
can use

00:02:10,800 --> 00:02:15,840
the sitcom to allow or disallow some

00:02:13,920 --> 00:02:20,080
system calls

00:02:15,840 --> 00:02:23,840
but the more you add the sick

00:02:20,080 --> 00:02:23,840
the sick the middle layer will be

00:02:24,080 --> 00:02:29,040
and you will get more performance

00:02:26,480 --> 00:02:29,040
overhead

00:02:29,920 --> 00:02:38,160
so the

00:02:33,920 --> 00:02:41,280
actually the container is a combination

00:02:38,160 --> 00:02:44,319
or trade-off between

00:02:41,280 --> 00:02:46,959
the virtual machine and the

00:02:44,319 --> 00:02:46,959
container

00:02:48,319 --> 00:02:55,280
it is compatible with oci runtime

00:02:51,840 --> 00:02:59,200
spec therefore works

00:02:55,280 --> 00:03:02,319
seamlessly with the dock engine

00:02:59,200 --> 00:03:06,560
besides it also supports

00:03:02,319 --> 00:03:11,920
the kubernetes and cii

00:03:06,560 --> 00:03:15,239
through the ciio and the css container d

00:03:11,920 --> 00:03:18,480
in other words you can choose

00:03:15,239 --> 00:03:18,879
transparently choose selected between

00:03:18,480 --> 00:03:22,640
the

00:03:18,879 --> 00:03:22,640
launchy and the cut container

00:03:26,000 --> 00:03:32,319
the this is the

00:03:29,840 --> 00:03:35,040
project containers architecture design

00:03:32,319 --> 00:03:35,040
flowchart

00:03:35,760 --> 00:03:42,959
the cutter agent is a process

00:03:38,959 --> 00:03:46,400
surrounding in the guest as a supervisor

00:03:42,959 --> 00:03:49,440
for managing the containers and

00:03:46,400 --> 00:03:52,080
the process running within those

00:03:49,440 --> 00:03:52,080
containers

00:03:53,760 --> 00:04:00,319
the cutter proxy offers

00:03:56,879 --> 00:04:04,319
access to the vm cutter agent

00:04:00,319 --> 00:04:07,680
to both custom and catalog time

00:04:04,319 --> 00:04:11,680
its main role is to route all the i o

00:04:07,680 --> 00:04:15,439
string streams and signals

00:04:11,680 --> 00:04:20,320
it connects to the cutter agent on a

00:04:15,439 --> 00:04:20,320
unix domain socket ievsoc

00:04:20,639 --> 00:04:27,440
cutter proxy uses earmarks to

00:04:23,919 --> 00:04:30,639
multiplex gipc connections

00:04:27,440 --> 00:04:30,639
on its connection

00:04:32,400 --> 00:04:40,840
the shim process

00:04:35,520 --> 00:04:44,160
runs in the in the host environment

00:04:40,840 --> 00:04:44,880
handling standard io and the signals on

00:04:44,160 --> 00:04:48,080
behalf

00:04:44,880 --> 00:04:52,720
of containers process

00:04:48,080 --> 00:04:52,720
which is inside guests

00:04:55,120 --> 00:04:57,840
but

00:04:58,720 --> 00:05:04,639
after the the cut container 2.0

00:05:02,160 --> 00:05:07,759
some parts of this pro chart will be

00:05:04,639 --> 00:05:07,759
changed a little bit

00:05:08,880 --> 00:05:17,840
so this is what what

00:05:13,120 --> 00:05:24,320
the new items in cutter 2.0

00:05:17,840 --> 00:05:28,560
for example cii i contain a dcm v2

00:05:24,320 --> 00:05:31,680
and it will redu reduce many uh

00:05:28,560 --> 00:05:35,440
many parts of the the

00:05:31,680 --> 00:05:35,440
architectural design flow chart

00:05:35,600 --> 00:05:46,479
and it's the

00:05:39,600 --> 00:05:46,479
roster agent to replace the golan agent

00:05:46,960 --> 00:05:54,720
and it's used the ttr pc

00:05:51,199 --> 00:05:58,000
is a tiny jipc

00:05:54,720 --> 00:06:01,440
implementation to replace the original

00:05:58,000 --> 00:06:06,800
jipc library choose

00:06:01,440 --> 00:06:11,440
vsoc to replace the virtual i o cloud

00:06:06,800 --> 00:06:15,759
and by default

00:06:11,440 --> 00:06:20,240
the cloud hypervisor will replace

00:06:15,759 --> 00:06:23,740
the cumule hybrid

00:06:20,240 --> 00:06:26,809
also supports the guest c group v2

00:06:23,740 --> 00:06:26,809
[Music]

00:06:31,840 --> 00:06:40,560
and what's the status

00:06:35,440 --> 00:06:44,160
of container on 64.

00:06:40,560 --> 00:06:49,840
totally speaking it can run

00:06:44,160 --> 00:06:49,840
can be run smoothly on 64.

00:06:50,319 --> 00:06:55,280
you can you can install the cut

00:06:53,440 --> 00:06:59,440
container

00:06:55,280 --> 00:07:03,039
on arm64 by two ways

00:06:59,440 --> 00:07:06,160
firstly is the snap install and secondly

00:07:03,039 --> 00:07:09,039
you can build it from

00:07:06,160 --> 00:07:09,039
the source code

00:07:10,720 --> 00:07:17,199
to run the container

00:07:14,160 --> 00:07:20,880
you can use the ctr command line

00:07:17,199 --> 00:07:24,720
here is the examples uh you can

00:07:20,880 --> 00:07:27,250
use the ctr image pool to put

00:07:24,720 --> 00:07:28,880
the container image and

00:07:27,250 --> 00:07:33,599
[Music]

00:07:28,880 --> 00:07:38,000
run the simple application

00:07:33,599 --> 00:07:38,000
by using the ctr rom

00:07:40,319 --> 00:07:48,000
also you can run the container on

00:07:44,879 --> 00:07:52,879
a raspberry pi 4

00:07:48,000 --> 00:07:52,879
platform with minor changes

00:07:57,360 --> 00:08:01,440
here is the future comparison result

00:08:00,879 --> 00:08:05,919
between

00:08:01,440 --> 00:08:10,120
arm64 and the xcsx86

00:08:05,919 --> 00:08:13,199
as for the hypervisor support

00:08:10,120 --> 00:08:16,479
x86 supports

00:08:13,199 --> 00:08:20,080
cumule by cracker class hypervisor

00:08:16,479 --> 00:08:24,840
and micro but

00:08:20,080 --> 00:08:28,160
a chrome is not supported on arm

00:08:24,840 --> 00:08:32,080
64. the cut container will

00:08:28,160 --> 00:08:35,279
use will let the hypervisor to create

00:08:32,080 --> 00:08:38,479
a medium device

00:08:35,279 --> 00:08:42,240
and then mount it as the

00:08:38,479 --> 00:08:42,240
guest root fs

00:08:43,279 --> 00:08:51,920
it can speed up the boot time

00:08:46,959 --> 00:08:51,920
and share the

00:08:52,720 --> 00:08:56,399
the guest root fs

00:08:58,160 --> 00:09:06,080
and the virtual io fs feature is

00:09:02,720 --> 00:09:09,839
used in

00:09:06,080 --> 00:09:09,839
the container root effects

00:09:12,320 --> 00:09:18,240
which i uh which i will introduce the in

00:09:15,360 --> 00:09:23,040
later slides

00:09:18,240 --> 00:09:26,640
the altruistic mark for the arm 64

00:09:23,040 --> 00:09:29,760
kumu here is the upstream

00:09:26,640 --> 00:09:33,920
kumu had supported this feature

00:09:29,760 --> 00:09:37,920
but we also need some time to introduce

00:09:33,920 --> 00:09:41,760
it on arm 64

00:09:37,920 --> 00:09:41,760
for the container

00:09:42,240 --> 00:09:45,279
next the

00:09:45,519 --> 00:09:53,200
the vm template is a useful technology

00:09:48,880 --> 00:09:53,200
to speed up the boot time

00:09:55,120 --> 00:10:03,680
this feature is supported on both

00:09:59,839 --> 00:10:08,160
and also the raster agent is supported

00:10:03,680 --> 00:10:12,320
on both the memory hot plug

00:10:08,160 --> 00:10:15,519
we the upstream had us supported on 64

00:10:12,320 --> 00:10:20,720
but we were introduced

00:10:15,519 --> 00:10:20,720
in in the future the vm

00:10:21,920 --> 00:10:29,279
vcpu hot plug someone

00:10:25,600 --> 00:10:30,720
had sub uh posted a patch series to

00:10:29,279 --> 00:10:35,760
support it

00:10:30,720 --> 00:10:39,440
in in both cumule and kernel community

00:10:35,760 --> 00:10:42,800
but i haven't got

00:10:39,440 --> 00:10:46,079
merged yet also

00:10:42,800 --> 00:10:51,600
the for the next kvm kvm

00:10:46,079 --> 00:10:51,600
feature i'm 64 kvm

00:10:52,560 --> 00:10:59,680
the community has discussed

00:10:56,160 --> 00:11:02,720
it's one huge one big

00:10:59,680 --> 00:11:05,839
series and after that after

00:11:02,720 --> 00:11:09,440
the community merge them we can you

00:11:05,839 --> 00:11:15,839
we can introduce the into on

00:11:09,440 --> 00:11:15,839
64 for cut container

00:11:19,040 --> 00:11:25,920
the virtual ios is

00:11:22,399 --> 00:11:29,200
a shared file system that

00:11:25,920 --> 00:11:32,560
lets virtual machine access

00:11:29,200 --> 00:11:36,560
a directory tree on the host

00:11:32,560 --> 00:11:40,000
unlike the existing approaches

00:11:36,560 --> 00:11:40,880
it is designed to offer local file

00:11:40,000 --> 00:11:44,640
system

00:11:40,880 --> 00:11:44,640
semantics and performance

00:11:47,040 --> 00:11:55,440
the the low level of the virtual fs

00:11:50,160 --> 00:11:58,639
is a fuse implementation

00:11:55,440 --> 00:12:00,160
they have used protocol is not based on

00:11:58,639 --> 00:12:04,480
the network

00:12:00,160 --> 00:12:08,399
protocol it means more faster

00:12:04,480 --> 00:12:11,680
faster performance and

00:12:08,399 --> 00:12:16,000
better posix semantics

00:12:11,680 --> 00:12:19,440
capabilities and

00:12:16,000 --> 00:12:23,519
there is a independent virtual ios

00:12:19,440 --> 00:12:27,040
demo process more secure

00:12:23,519 --> 00:12:30,000
and needed to maintain the

00:12:27,040 --> 00:12:30,000
device mapper

00:12:30,420 --> 00:12:33,549
[Music]

00:12:34,880 --> 00:12:41,519
and uh

00:12:38,000 --> 00:12:44,800
there is a text mode you can

00:12:41,519 --> 00:12:48,079
uh with this with this

00:12:44,800 --> 00:12:52,800
text mode enabled the host and

00:12:48,079 --> 00:12:57,600
the guest can share the memory

00:12:52,800 --> 00:13:00,639
and improve the performance

00:12:57,600 --> 00:13:03,839
and you can also bypass the guest

00:13:00,639 --> 00:13:07,839
page cache avoided

00:13:03,839 --> 00:13:07,839
an accessory vm exit

00:13:08,959 --> 00:13:16,079
and the the back end is in user space

00:13:13,440 --> 00:13:16,079
which is

00:13:16,959 --> 00:13:21,720
convenient for for for the user to

00:13:20,880 --> 00:13:24,869
further too

00:13:21,720 --> 00:13:24,869
[Music]

00:13:28,079 --> 00:13:36,480
but we once observed that

00:13:31,920 --> 00:13:39,680
the virtual frs will increase the system

00:13:36,480 --> 00:13:45,440
system level memory footprint

00:13:39,680 --> 00:13:45,440
because you know it uses additional

00:13:46,160 --> 00:13:55,279
shared pages which disallows

00:13:50,880 --> 00:13:55,279
the ksm to merge the pages

00:14:00,959 --> 00:14:08,079
the inner internal tests showed

00:14:04,000 --> 00:14:11,279
that it will significa significantly

00:14:08,079 --> 00:14:17,519
improve the file system performance in

00:14:11,279 --> 00:14:21,120
qatar compared with a virtual io9 p

00:14:17,519 --> 00:14:25,440
from this chart we use the

00:14:21,120 --> 00:14:28,839
the file read write test

00:14:25,440 --> 00:14:32,000
the data improves the increase

00:14:28,839 --> 00:14:36,240
about 10 to 20

00:14:32,000 --> 00:14:36,240
times in different cases

00:14:40,240 --> 00:14:46,720
this is the the what we have done

00:14:44,480 --> 00:14:48,959
for the functional future is the

00:14:46,720 --> 00:14:52,160
development

00:14:48,959 --> 00:14:55,279
and firstly we enabled the

00:14:52,160 --> 00:14:58,639
runtime and the rust agent

00:14:55,279 --> 00:15:02,800
on arm 64 and we maintain the

00:14:58,639 --> 00:15:06,639
the ci tester subsystem

00:15:02,800 --> 00:15:10,399
and we enable the firecracker

00:15:06,639 --> 00:15:13,839
and even the cloud hypervisor

00:15:10,399 --> 00:15:17,360
on especially uh

00:15:13,839 --> 00:15:21,600
we cloud hypervisor is another

00:15:17,360 --> 00:15:25,040
uh individual independent

00:15:21,600 --> 00:15:28,160
ripple we develop enable

00:15:25,040 --> 00:15:31,360
the cloud private from scratch

00:15:28,160 --> 00:15:31,360
on 64.

00:15:31,839 --> 00:15:41,759
we also finished the kubernetes

00:15:35,519 --> 00:15:45,120
integration test with cutter container

00:15:41,759 --> 00:15:48,800
there's a to-do list

00:15:45,120 --> 00:15:49,519
in the future for us to do such as the

00:15:48,800 --> 00:15:52,639
memory

00:15:49,519 --> 00:15:55,079
and the vcpu hot block and

00:15:52,639 --> 00:15:56,170
the important next state of

00:15:55,079 --> 00:15:59,290
virtualization

00:15:56,170 --> 00:15:59,290
[Music]

00:16:03,759 --> 00:16:09,920
so to summarize the performance

00:16:07,440 --> 00:16:12,000
comparison between different

00:16:09,920 --> 00:16:16,079
architectures

00:16:12,000 --> 00:16:19,839
we choose some important

00:16:16,079 --> 00:16:24,959
aspects such as the boot time the binary

00:16:19,839 --> 00:16:24,959
code size and then memory footprint

00:16:27,759 --> 00:16:32,720
here is the the hardware or software

00:16:30,480 --> 00:16:36,000
setup in for

00:16:32,720 --> 00:16:38,800
the host the guest the cumule and the

00:16:36,000 --> 00:16:38,800
cutter version

00:16:46,000 --> 00:16:53,920
this is the evaluation for both

00:16:50,800 --> 00:16:57,759
i once started

00:16:53,920 --> 00:17:00,880
the run a simple

00:16:57,759 --> 00:17:05,039
container application and

00:17:00,880 --> 00:17:08,079
you entered the

00:17:05,039 --> 00:17:11,360
hello world and exit at once

00:17:08,079 --> 00:17:14,640
for 10 100 times and

00:17:11,360 --> 00:17:16,079
then calculate the average data for both

00:17:14,640 --> 00:17:19,679
time

00:17:16,079 --> 00:17:22,799
the exit x-axis is the put

00:17:19,679 --> 00:17:24,160
point time difference to the starting

00:17:22,799 --> 00:17:27,310
point

00:17:24,160 --> 00:17:29,919
the y-axis is the

00:17:27,310 --> 00:17:33,840
[Music]

00:17:29,919 --> 00:17:33,840
mini-second time unit

00:17:34,320 --> 00:17:40,880
we observed that

00:17:37,760 --> 00:17:44,640
the most gap is between the vm started

00:17:40,880 --> 00:17:48,799
and the agent started i the code put

00:17:44,640 --> 00:17:48,799
time and accumulable time

00:17:49,600 --> 00:17:53,679
and the boot time might be a little

00:17:52,720 --> 00:18:01,840
different

00:17:53,679 --> 00:18:01,840
between different configuration

00:18:02,400 --> 00:18:05,760
the total boot time of the guest kernel

00:18:05,120 --> 00:18:09,520
is not

00:18:05,760 --> 00:18:09,520
so so

00:18:09,760 --> 00:18:16,400
so long but we still found something

00:18:13,200 --> 00:18:20,880
to optimize we

00:18:16,400 --> 00:18:26,480
reduced the printable time from

00:18:20,880 --> 00:18:30,480
maybe 117 milliseconds

00:18:26,480 --> 00:18:30,480
to 81 milliseconds

00:18:31,760 --> 00:18:39,679
another tunable is the

00:18:35,440 --> 00:18:43,600
systemd service but given that

00:18:39,679 --> 00:18:47,039
most of the district enable system d

00:18:43,600 --> 00:18:52,400
we didn't remove the system d as

00:18:47,039 --> 00:18:56,000
ipyle did from this chart

00:18:52,400 --> 00:18:56,799
we concluded that a boot time gap on arm

00:18:56,000 --> 00:19:00,000
00:18:56,799 --> 00:19:02,799
compared with x86 is not the guest

00:19:00,000 --> 00:19:02,799
kernel boot

00:19:08,640 --> 00:19:15,600
this is the tuning items

00:19:11,919 --> 00:19:20,799
and for example we disable

00:19:15,600 --> 00:19:25,039
the pmu initialization

00:19:20,799 --> 00:19:28,320
if the user doesn't want to use it

00:19:25,039 --> 00:19:31,360
and by default the scarcity scale mode

00:19:28,320 --> 00:19:35,440
is synchronization mode

00:19:31,360 --> 00:19:35,440
we can set it to now

00:19:36,400 --> 00:19:43,679
and also we by default uh arm 64

00:19:40,240 --> 00:19:46,880
will create about

00:19:43,679 --> 00:19:50,080
30 32 vm

00:19:46,880 --> 00:19:53,679
uh virtual mmi all devices

00:19:50,080 --> 00:19:56,160
even the user doesn't

00:19:53,679 --> 00:19:56,160
use it

00:19:57,039 --> 00:20:04,400
we can disable it by default

00:20:00,799 --> 00:20:04,400
in the kernel configuration

00:20:05,600 --> 00:20:12,880
there is a one another

00:20:09,360 --> 00:20:16,559
way to speed up the

00:20:12,880 --> 00:20:20,799
boot time it is more aggressive that is

00:20:16,559 --> 00:20:24,960
vm template

00:20:20,799 --> 00:20:28,240
a vm template is a new feature that

00:20:24,960 --> 00:20:33,120
enables the new

00:20:28,240 --> 00:20:36,240
vm creation using a cloning technique

00:20:33,120 --> 00:20:37,760
when it is enabled the new uh virtual

00:20:36,240 --> 00:20:40,960
machine

00:20:37,760 --> 00:20:46,720
virtual machine is created by

00:20:40,960 --> 00:20:50,159
cloning from a pre-created template

00:20:46,720 --> 00:20:51,360
and they will share the same uh initial

00:20:50,159 --> 00:20:55,280
mfs

00:20:51,360 --> 00:20:58,480
kernel and uh

00:20:55,280 --> 00:21:02,000
the and the agent memory

00:20:58,480 --> 00:21:05,679
in rhythm read-only mode it's just

00:21:02,000 --> 00:21:05,679
like a process folk

00:21:07,840 --> 00:21:14,400
it is expected that

00:21:10,960 --> 00:21:19,120
kumu doesn't write anything to the guest

00:21:14,400 --> 00:21:22,960
ram until the virtual machine started

00:21:19,120 --> 00:21:28,080
but it does in

00:21:22,960 --> 00:21:28,080
m64 qmil so

00:21:28,159 --> 00:21:35,600
there's very exception on 64

00:21:32,080 --> 00:21:37,760
when when we enable the vm template

00:21:35,600 --> 00:21:40,799
feature

00:21:37,760 --> 00:21:44,880
actually the room block dtp

00:21:40,799 --> 00:21:48,799
will be filled filled into ram

00:21:44,880 --> 00:21:52,640
during a room reset

00:21:48,799 --> 00:21:53,120
in in common case the room filling seems

00:21:52,640 --> 00:21:55,760
to be

00:21:53,120 --> 00:21:56,799
not required since all the data have

00:21:55,760 --> 00:22:00,400
been stored

00:21:56,799 --> 00:22:05,520
in memory backend

00:22:00,400 --> 00:22:05,520
already so we

00:22:06,000 --> 00:22:11,200
we bypass we actually we bypass that

00:22:09,120 --> 00:22:16,080
process and

00:22:11,200 --> 00:22:27,840
make mediator that make the vm template

00:22:16,080 --> 00:22:27,840
enabled on arm 64.

00:22:29,840 --> 00:22:38,320
this is the the binary size performance

00:22:33,679 --> 00:22:41,440
comparison qatar will

00:22:38,320 --> 00:22:45,520
start start a guest

00:22:41,440 --> 00:22:49,280
with limited memory or cpu resource

00:22:45,520 --> 00:22:53,280
hence reducing the binary size

00:22:49,280 --> 00:22:58,000
code size is a tuning aspect

00:22:53,280 --> 00:23:01,840
from this chart we reduce final size

00:22:58,000 --> 00:23:06,559
for qatar binary about

00:23:01,840 --> 00:23:06,559
20 to 30

00:23:06,840 --> 00:23:09,840
percent

00:23:12,960 --> 00:23:20,880
we also tuned the the the vmm

00:23:17,200 --> 00:23:24,240
currently only the cumule

00:23:20,880 --> 00:23:27,360
by customizing the configuration

00:23:24,240 --> 00:23:30,640
and stripping the binary

00:23:27,360 --> 00:23:35,120
binary size the binary size

00:23:30,640 --> 00:23:38,240
was reduced by about 20 percent

00:23:35,120 --> 00:23:41,279
with configuration tuning

00:23:38,240 --> 00:23:45,440
you know we can cut off

00:23:41,279 --> 00:23:45,660
all the unnecessary device creation

00:23:45,440 --> 00:23:48,809
and

00:23:45,660 --> 00:23:48,809
[Music]

00:23:49,279 --> 00:24:05,840
by by stripping we can reduce

00:23:52,880 --> 00:24:05,840
more 60 percent for the code size

00:24:07,279 --> 00:24:15,360
this is the memory footprint uh

00:24:10,320 --> 00:24:15,360
perform for the performance comparison

00:24:16,159 --> 00:24:23,919
you can see from this chart

00:24:19,919 --> 00:24:27,039
uh the possible reason why the

00:24:23,919 --> 00:24:29,039
virtual memory footprint of cumula on

00:24:27,039 --> 00:24:33,120
arm 64

00:24:29,039 --> 00:24:38,159
is bigger than that on x86

00:24:33,120 --> 00:24:42,240
is the firmware devices creation

00:24:38,159 --> 00:24:47,840
there are two p flats devices

00:24:42,240 --> 00:24:47,840
takes about 28 128

00:24:48,880 --> 00:24:54,960
it will be created unconditionally

00:24:58,480 --> 00:25:07,360
the resident memory uh

00:25:01,600 --> 00:25:11,600
a physical memory of arm 64.

00:25:07,360 --> 00:25:15,840
the cumulative footprint is as much as

00:25:11,600 --> 00:25:19,600
or even better than than it

00:25:15,840 --> 00:25:19,600
on x86

00:25:21,200 --> 00:25:29,840
so this is the comparison without

00:25:33,520 --> 00:25:41,440
this is the the resident

00:25:36,559 --> 00:25:41,440
memory summary for the

00:25:41,520 --> 00:25:46,000
for the size com comparison

00:25:47,919 --> 00:25:55,200
sorry for the the goal agent

00:25:50,960 --> 00:25:57,840
and cutter go wrong time

00:25:55,200 --> 00:25:57,840
process

00:26:00,159 --> 00:26:11,840
the size of two

00:26:03,440 --> 00:26:11,840
architecture is closed or even the same

00:26:13,760 --> 00:26:20,880
this is the the net network

00:26:18,240 --> 00:26:20,880
throughput

00:26:23,520 --> 00:26:32,480
you know the bbr and cubic

00:26:26,799 --> 00:26:32,480
are different congestion algorithm

00:26:32,720 --> 00:26:36,559
these are two tcp

00:26:36,840 --> 00:26:44,159
uh these two different

00:26:39,679 --> 00:26:47,360
congressional congestion algorithm

00:26:44,159 --> 00:26:50,640
for the tcp internal

00:26:47,360 --> 00:26:54,799
qatar will choose vbr as the default

00:26:50,640 --> 00:26:59,039
but from the test result shows that

00:26:54,799 --> 00:27:03,440
in the at least in the local

00:26:59,039 --> 00:27:07,039
and in the local

00:27:03,440 --> 00:27:10,159
lab environment the bbi has

00:27:07,039 --> 00:27:10,159
lower performance

00:27:10,320 --> 00:27:13,279
than the cubic

00:27:14,960 --> 00:27:22,640
here is the test result by

00:27:18,080 --> 00:27:27,600
changing the algorithm

00:27:22,640 --> 00:27:31,279
we we improve we increased the

00:27:27,600 --> 00:27:35,679
throughput from almost

00:27:31,279 --> 00:27:38,720
nearly 11 gigabytes

00:27:35,679 --> 00:27:41,520
to 15

00:27:38,720 --> 00:27:41,520
gigabytes

00:27:45,919 --> 00:27:50,960
this is the performance to the items

00:27:48,640 --> 00:27:54,640
what we have done

00:27:50,960 --> 00:27:58,159
enable the vm template the

00:27:54,640 --> 00:28:02,399
retirement fs decks

00:27:58,159 --> 00:28:05,760
and the persistent memory support

00:28:02,399 --> 00:28:08,799
and they we changed

00:28:05,760 --> 00:28:17,840
the algorithm from

00:28:08,799 --> 00:28:17,840
from bbr to cubic

00:28:19,679 --> 00:28:26,559
uh these are

00:28:23,520 --> 00:28:30,000
finally i will introduce to

00:28:26,559 --> 00:28:33,039
two user cases you know by two

00:28:30,000 --> 00:28:36,559
uh is a dominant chinese

00:28:33,039 --> 00:28:40,480
search engine operator

00:28:36,559 --> 00:28:43,120
in its ai cloud

00:28:40,480 --> 00:28:44,080
the by two air cloud is a complex

00:28:43,120 --> 00:28:47,120
network

00:28:44,080 --> 00:28:51,440
with huge amounts of traffic and

00:28:47,120 --> 00:28:55,440
complicate deployment scenarios

00:28:51,440 --> 00:28:58,559
the peak traffic is about 1 billion

00:28:55,440 --> 00:29:01,840
page page views

00:28:58,559 --> 00:29:06,320
per day and 50

00:29:01,840 --> 00:29:10,159
000 containers for a single tenant

00:29:06,320 --> 00:29:13,840
so i do choose to use cutter containers

00:29:10,159 --> 00:29:16,559
after doing extensive research on secure

00:29:13,840 --> 00:29:20,399
container technologies

00:29:16,559 --> 00:29:23,440
and determining that cutter containers

00:29:20,399 --> 00:29:27,520
is a highly secure and

00:29:23,440 --> 00:29:30,720
practical containers technology

00:29:27,520 --> 00:29:33,840
besides as one of the

00:29:30,720 --> 00:29:37,279
important founders and

00:29:33,840 --> 00:29:40,320
maintainers alibaba use

00:29:37,279 --> 00:29:44,000
uses cut containers in its

00:29:40,320 --> 00:29:46,640
ecs experimental instance

00:29:44,000 --> 00:29:49,600
plus kubernetes and serverless

00:29:46,640 --> 00:29:49,600
infrastructure

00:29:52,399 --> 00:30:02,399
so that's all for today's

00:29:56,080 --> 00:30:02,399

YouTube URL: https://www.youtube.com/watch?v=jiCGbH8RC_I


