Title: Rook: Intro and Ceph Deep Dive - Blaine Gardner, Alexander Trost, & Travis Nielsen, Sébastien Han
Publication date: 2020-11-23
Playlist: KubeCon + CloudNativeCon North America 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon Europe 2021 Virtual from May 4–7, 2021. Learn more at https://kubecon.io. The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects. 

Rook: Intro and Ceph Deep Dive - Blaine Gardner, SUSE, Alexander Trost, Cloudical, & Travis Nielsen, Sébastien Han, Red Hat 

The Rook project will be introduced to attendees of all levels and experience. Rook is an open source cloud-native storage orchestrator for Kubernetes, providing the platform, framework, and support for a diverse set of storage solutions to natively integrate with cloud-native environments. Rook turns storage software into self-managing, self-scaling, and self-healing storage services. The benefits and use cases of Rook will be explored along with an overview of each of the Rook storage providers: Ceph, Cassandra, NFS, EdgeFS, YugabyteDB, and CockroachDB. In the latter half of the talk, a deep-dive will be presented for the Ceph storage provider. Rook is run today in many production environments, providing a stable storage platform for your data. The architecture and recent improvements will be show how Rook provides the Ceph management layer for production environments. Rook was accepted as the first storage project hosted by the Cloud Native Computing Foundation in January 2018. 

https://sched.co/ekIQ
Captions: 
	00:00:00,480 --> 00:00:04,640
all right hello everyone uh welcome to

00:00:02,320 --> 00:00:08,080
the rook intro and theft deep dive talk

00:00:04,640 --> 00:00:10,080
uh for kubecon north america uh

00:00:08,080 --> 00:00:12,639
here we are virtual again happy to be

00:00:10,080 --> 00:00:13,599
with you i am travis nielsen uh one of

00:00:12,639 --> 00:00:16,960
the maintainers

00:00:13,599 --> 00:00:19,680
on the rook project since it's beginning

00:00:16,960 --> 00:00:21,760
i work for red hat and happy to be here

00:00:19,680 --> 00:00:24,480
with sebastian

00:00:21,760 --> 00:00:26,160
hello virtual kubecon i'm sebastian i

00:00:24,480 --> 00:00:27,119
work with travis as one of the rook

00:00:26,160 --> 00:00:30,080
maintainers

00:00:27,119 --> 00:00:31,039
and also work for red hat thanks and

00:00:30,080 --> 00:00:32,559
also

00:00:31,039 --> 00:00:35,040
to answer questions we'll have blaine

00:00:32,559 --> 00:00:37,040
gardner and alexander trust

00:00:35,040 --> 00:00:39,200
other rook maintainers all right first

00:00:37,040 --> 00:00:42,239
big announcement rook is graduated

00:00:39,200 --> 00:00:43,680
we are very excited about this

00:00:42,239 --> 00:00:45,440
we've been working hard on this for the

00:00:43,680 --> 00:00:48,480
last few years

00:00:45,440 --> 00:00:51,920
just a little history here so rook did

00:00:48,480 --> 00:00:52,640
get initially uh inducted as a sandbox

00:00:51,920 --> 00:00:54,399
project

00:00:52,640 --> 00:00:55,920
back almost three years ago in january

00:00:54,399 --> 00:00:57,840
00:00:55,920 --> 00:01:00,399
we progressed to incubation in september

00:00:57,840 --> 00:01:04,080
of that same year and then graduated

00:01:00,399 --> 00:01:06,240
now in october uh just

00:01:04,080 --> 00:01:08,000
just this last month so thank you a big

00:01:06,240 --> 00:01:10,240
thank you to the rook community

00:01:08,000 --> 00:01:11,439
cncf committee rook maintainers and

00:01:10,240 --> 00:01:13,840
everyone involved

00:01:11,439 --> 00:01:15,119
it has been a wonderful journey and

00:01:13,840 --> 00:01:17,520
we're grateful your support

00:01:15,119 --> 00:01:18,560
for your support and look forward to

00:01:17,520 --> 00:01:22,240
continued

00:01:18,560 --> 00:01:24,840
growth in the project with that support

00:01:22,240 --> 00:01:26,080
so what does this mean to get to

00:01:24,840 --> 00:01:28,240
graduated

00:01:26,080 --> 00:01:29,680
um it really means that we have a lot of

00:01:28,240 --> 00:01:32,560
large-scale production

00:01:29,680 --> 00:01:34,079
deployments out there we know people are

00:01:32,560 --> 00:01:36,479
using it

00:01:34,079 --> 00:01:38,079
to store their their critical data in

00:01:36,479 --> 00:01:41,119
production environments

00:01:38,079 --> 00:01:42,079
and this is that's a great um evidence

00:01:41,119 --> 00:01:43,759
of that

00:01:42,079 --> 00:01:45,360
we went through a security review by the

00:01:43,759 --> 00:01:49,040
third party

00:01:45,360 --> 00:01:51,680
that cncf helped us coordinate and

00:01:49,040 --> 00:01:53,439
maybe the third point is that really

00:01:51,680 --> 00:01:56,719
brooke is an open project

00:01:53,439 --> 00:01:58,479
it's it's not only open source but

00:01:56,719 --> 00:02:00,719
the governance around the project says

00:01:58,479 --> 00:02:02,719
that hey this is this is vendor neutral

00:02:00,719 --> 00:02:05,600
we have multiple companies contributing

00:02:02,719 --> 00:02:06,399
and expected to contribute so that it

00:02:05,600 --> 00:02:09,039
stays

00:02:06,399 --> 00:02:10,160
remains a project that is useful to the

00:02:09,039 --> 00:02:14,319
whole community

00:02:10,160 --> 00:02:16,560
and and not vendor driven so

00:02:14,319 --> 00:02:18,080
again thanks to for all your support uh

00:02:16,560 --> 00:02:19,599
we've had a great journey to get here

00:02:18,080 --> 00:02:21,360
all right let's get started on

00:02:19,599 --> 00:02:23,360
you know what what are we talking about

00:02:21,360 --> 00:02:26,319
what is rook what does it address

00:02:23,360 --> 00:02:28,879
what what are issues with storage and

00:02:26,319 --> 00:02:30,959
kubernetes

00:02:28,879 --> 00:02:31,920
well kubernetes is traditionally a

00:02:30,959 --> 00:02:35,040
platform

00:02:31,920 --> 00:02:35,760
of course to manage distributed apps and

00:02:35,040 --> 00:02:38,640
those apps

00:02:35,760 --> 00:02:39,680
have been stateless for the most part if

00:02:38,640 --> 00:02:42,160
you want storage

00:02:39,680 --> 00:02:43,440
you rely on x storage external to the

00:02:42,160 --> 00:02:45,360
cluster

00:02:43,440 --> 00:02:48,319
storage that's not portable you have to

00:02:45,360 --> 00:02:50,400
figure out how to deploy it

00:02:48,319 --> 00:02:51,760
you have maybe you have a reliance on a

00:02:50,400 --> 00:02:55,120
cloud provider to

00:02:51,760 --> 00:02:56,640
to give you that managed storage uh

00:02:55,120 --> 00:02:58,239
and quickly you get into a vendor

00:02:56,640 --> 00:03:01,360
lock-in if you're using

00:02:58,239 --> 00:03:02,400
cloud storage if you're managing that

00:03:01,360 --> 00:03:04,560
storage

00:03:02,400 --> 00:03:06,480
okay you have to you know day two

00:03:04,560 --> 00:03:07,840
operations at once you've set it up

00:03:06,480 --> 00:03:09,120
you've got to upgrade it you've got to

00:03:07,840 --> 00:03:12,480
manage it

00:03:09,120 --> 00:03:13,040
make sure it stays reliable so all of

00:03:12,480 --> 00:03:16,239
those

00:03:13,040 --> 00:03:17,920
are challenges that that everyone has

00:03:16,239 --> 00:03:20,239
with any stateful application in

00:03:17,920 --> 00:03:21,599
kubernetes so here's where rook comes in

00:03:20,239 --> 00:03:23,280
we saw this as a real challenge with

00:03:21,599 --> 00:03:25,440
kubernetes and we wanted to

00:03:23,280 --> 00:03:26,400
address it and kind of solve this

00:03:25,440 --> 00:03:29,440
problem

00:03:26,400 --> 00:03:30,879
for the community so rook really does

00:03:29,440 --> 00:03:34,080
have the goal of making storage

00:03:30,879 --> 00:03:36,000
available inside your kubernetes cluster

00:03:34,080 --> 00:03:37,680
it's not something external it's

00:03:36,000 --> 00:03:40,560
something that's

00:03:37,680 --> 00:03:42,640
deployed as any other application in

00:03:40,560 --> 00:03:44,640
your kubernetes cluster

00:03:42,640 --> 00:03:46,400
it's managed with kubernetes operators

00:03:44,640 --> 00:03:49,840
and crds

00:03:46,400 --> 00:03:54,159
just like any other application

00:03:49,840 --> 00:03:56,720
and because it's driven with operators

00:03:54,159 --> 00:03:57,280
the deployment of of the storage

00:03:56,720 --> 00:03:59,840
platform

00:03:57,280 --> 00:04:00,560
is automated so deploying it configuring

00:03:59,840 --> 00:04:03,200
it

00:04:00,560 --> 00:04:04,239
upgrading it you you get to decide how

00:04:03,200 --> 00:04:06,799
it's deployed

00:04:04,239 --> 00:04:07,760
uh through the crds and other settings

00:04:06,799 --> 00:04:09,840
but

00:04:07,760 --> 00:04:11,519
ultimately rook manages all of the

00:04:09,840 --> 00:04:13,599
headaches of

00:04:11,519 --> 00:04:15,280
of that day two management well day one

00:04:13,599 --> 00:04:17,919
as well to get it installed

00:04:15,280 --> 00:04:18,880
and once you have deployed rook now you

00:04:17,919 --> 00:04:21,359
can consume

00:04:18,880 --> 00:04:22,720
the storage just like any other

00:04:21,359 --> 00:04:24,800
kubernetes

00:04:22,720 --> 00:04:26,240
storage with storage classes with

00:04:24,800 --> 00:04:28,960
persistent volume claims

00:04:26,240 --> 00:04:31,280
so bring the storage platform inside

00:04:28,960 --> 00:04:33,199
kubernetes is the point

00:04:31,280 --> 00:04:35,199
uh rook is open source with an apache

00:04:33,199 --> 00:04:37,680
200 license

00:04:35,199 --> 00:04:38,560
so it is so what storage are we talking

00:04:37,680 --> 00:04:40,240
about here

00:04:38,560 --> 00:04:41,600
uh we have several different storage

00:04:40,240 --> 00:04:44,880
providers in rook

00:04:41,600 --> 00:04:45,360
at different levels of progression so

00:04:44,880 --> 00:04:48,320
seth

00:04:45,360 --> 00:04:49,280
is our stable storage provider it's been

00:04:48,320 --> 00:04:51,040
in since the

00:04:49,280 --> 00:04:52,560
start of the project since then we've

00:04:51,040 --> 00:04:55,120
added other

00:04:52,560 --> 00:04:56,880
storage providers as well to expand you

00:04:55,120 --> 00:05:01,280
know the the storage

00:04:56,880 --> 00:05:03,520
platform nfs cassandra uydb cockroachdb

00:05:01,280 --> 00:05:05,039
these are still in alpha and uh we're

00:05:03,520 --> 00:05:05,840
always looking for community involvement

00:05:05,039 --> 00:05:09,600
to help

00:05:05,840 --> 00:05:10,960
grow those uh we do have one

00:05:09,600 --> 00:05:13,039
storage provider that is deprecated so

00:05:10,960 --> 00:05:14,479
edge of s the

00:05:13,039 --> 00:05:16,880
the owners of edge of fest are working

00:05:14,479 --> 00:05:19,440
on a replacement for it

00:05:16,880 --> 00:05:20,240
um the timeline of that i i'm not aware

00:05:19,440 --> 00:05:21,919
of but

00:05:20,240 --> 00:05:23,280
that's why it's deprecated since

00:05:21,919 --> 00:05:26,240
something new will be coming

00:05:23,280 --> 00:05:26,720
all right so kind of overall project you

00:05:26,240 --> 00:05:29,440
know

00:05:26,720 --> 00:05:30,320
how big is rook where is it so version

00:05:29,440 --> 00:05:33,039
1.5

00:05:30,320 --> 00:05:34,240
is our latest release at least it will

00:05:33,039 --> 00:05:38,160
be as of

00:05:34,240 --> 00:05:40,080
tubecon november 2020 we will have that

00:05:38,160 --> 00:05:42,240
release out

00:05:40,080 --> 00:05:43,280
you know as far as how popular project

00:05:42,240 --> 00:05:45,840
is you know

00:05:43,280 --> 00:05:47,759
all the github stars downloads going on

00:05:45,840 --> 00:05:50,800
200 million downloads now

00:05:47,759 --> 00:05:52,880
for our containers uh

00:05:50,800 --> 00:05:54,720
about 300 contributors now on on the

00:05:52,880 --> 00:05:56,880
github project

00:05:54,720 --> 00:05:57,919
and as mentioned we are a graduated

00:05:56,880 --> 00:06:00,639
project

00:05:57,919 --> 00:06:02,080
so thanks again to the the community

00:06:00,639 --> 00:06:02,800
these are just a few of the stats that

00:06:02,080 --> 00:06:07,039
show

00:06:02,800 --> 00:06:09,039
how how much it is growing okay so rook

00:06:07,039 --> 00:06:10,400
and sef if we're getting into the ceph

00:06:09,039 --> 00:06:12,960
portion of this

00:06:10,400 --> 00:06:14,000
what does that mean to bring ceph into

00:06:12,960 --> 00:06:16,080
kubernetes

00:06:14,000 --> 00:06:18,479
that's what rick will do rook will bring

00:06:16,080 --> 00:06:20,960
the ceph storage layer into kubernetes

00:06:18,479 --> 00:06:22,479
and make it work well so what is steph

00:06:20,960 --> 00:06:25,360
if you're not familiar with it seth

00:06:22,479 --> 00:06:26,080
is another open source project but it is

00:06:25,360 --> 00:06:28,400
a

00:06:26,080 --> 00:06:29,199
software defined storage solution it

00:06:28,400 --> 00:06:32,560
provides

00:06:29,199 --> 00:06:35,680
block shared file system and object

00:06:32,560 --> 00:06:36,240
storage that is s3 compliant so these

00:06:35,680 --> 00:06:39,039
are the

00:06:36,240 --> 00:06:40,080
really the three building blocks of any

00:06:39,039 --> 00:06:41,840
storage system

00:06:40,080 --> 00:06:43,600
that you'll find out there and ceph has

00:06:41,840 --> 00:06:47,039
them all in one

00:06:43,600 --> 00:06:48,639
in one system no need for deploying one

00:06:47,039 --> 00:06:51,840
solution for block and another one

00:06:48,639 --> 00:06:53,599
for object or file system the ceph

00:06:51,840 --> 00:06:56,319
provides all of those

00:06:53,599 --> 00:06:57,440
types of storage together so early on

00:06:56,319 --> 00:07:00,080
the project we really

00:06:57,440 --> 00:07:01,680
saw ceph as being a reliable platform

00:07:00,080 --> 00:07:03,520
it's been around for a long time

00:07:01,680 --> 00:07:05,120
we just wanted to bring it to kubernetes

00:07:03,520 --> 00:07:08,319
so how did we bring it to kubernetes

00:07:05,120 --> 00:07:11,360
um think of it as having three different

00:07:08,319 --> 00:07:14,800
layers in the system where rook

00:07:11,360 --> 00:07:16,960
is really the operator in kubernetes

00:07:14,800 --> 00:07:19,199
that owns the management of ceph

00:07:16,960 --> 00:07:20,720
so the deployment of it the upgrading of

00:07:19,199 --> 00:07:24,160
stuff like rook

00:07:20,720 --> 00:07:25,599
manages the configuration of stuff

00:07:24,160 --> 00:07:27,680
if you're familiar with staff you know

00:07:25,599 --> 00:07:31,199
that it is kind of

00:07:27,680 --> 00:07:33,840
a complicated system to deploy

00:07:31,199 --> 00:07:35,120
as distributed storage so rook takes a

00:07:33,840 --> 00:07:37,199
lot of that

00:07:35,120 --> 00:07:39,280
complexity out of it and manages it for

00:07:37,199 --> 00:07:43,039
you second layer now we've got

00:07:39,280 --> 00:07:44,800
the ceph csi driver so this csi driver

00:07:43,039 --> 00:07:46,639
just like any other storage platform

00:07:44,800 --> 00:07:49,120
kubernetes will provision

00:07:46,639 --> 00:07:50,080
and then mount the storage into your app

00:07:49,120 --> 00:07:52,879
pod

00:07:50,080 --> 00:07:53,599
so this is how you consume this the ceph

00:07:52,879 --> 00:07:55,360
storage

00:07:53,599 --> 00:07:57,280
once you have rook installed and then

00:07:55,360 --> 00:07:59,360
finally the third layer

00:07:57,280 --> 00:08:01,919
is the data layer so when you are

00:07:59,360 --> 00:08:05,199
reading and writing data to the cluster

00:08:01,919 --> 00:08:08,240
ceph is purely the one that is

00:08:05,199 --> 00:08:11,280
responding to that that layer and

00:08:08,240 --> 00:08:12,240
there is no rook management code in the

00:08:11,280 --> 00:08:14,560
data path

00:08:12,240 --> 00:08:16,240
it's just ceph at the data layer so you

00:08:14,560 --> 00:08:19,440
really get

00:08:16,240 --> 00:08:21,120
performance high performance storage

00:08:19,440 --> 00:08:22,400
at that layer all right so let's look at

00:08:21,120 --> 00:08:22,960
some pictures of what these layers look

00:08:22,400 --> 00:08:25,599
like

00:08:22,960 --> 00:08:27,440
so for the first layer with rook so this

00:08:25,599 --> 00:08:28,240
this is a picture trying to show that

00:08:27,440 --> 00:08:30,400
okay we've got

00:08:28,240 --> 00:08:31,599
a single cluster with three nodes so

00:08:30,400 --> 00:08:34,800
each of these black boxes

00:08:31,599 --> 00:08:37,440
is is a node in your kubernetes cluster

00:08:34,800 --> 00:08:39,360
and each of these nodes is running pods

00:08:37,440 --> 00:08:42,399
that run different types of demons

00:08:39,360 --> 00:08:43,039
so in the center center node here we've

00:08:42,399 --> 00:08:46,160
got the rook

00:08:43,039 --> 00:08:48,000
operator pod and of course that

00:08:46,160 --> 00:08:49,760
that's the management layer that's going

00:08:48,000 --> 00:08:53,279
to manage everything else

00:08:49,760 --> 00:08:57,120
so this operator pod is is going to

00:08:53,279 --> 00:09:00,240
deploy ceph and and the csi driver

00:08:57,120 --> 00:09:02,720
depending on what settings and how

00:09:00,240 --> 00:09:04,000
how you tell it to be configured okay so

00:09:02,720 --> 00:09:07,279
the these blue

00:09:04,000 --> 00:09:09,600
pods are really

00:09:07,279 --> 00:09:10,560
core rook pods that are providing the

00:09:09,600 --> 00:09:14,240
management layer

00:09:10,560 --> 00:09:17,279
the discovery pod is discovering what

00:09:14,240 --> 00:09:21,519
storage is available on each node so you

00:09:17,279 --> 00:09:24,160
um so rook knows how to deploy the

00:09:21,519 --> 00:09:24,720
sep demons the green pods are csi

00:09:24,160 --> 00:09:26,880
drivers

00:09:24,720 --> 00:09:28,080
so on each node you need a csi driver

00:09:26,880 --> 00:09:30,880
that will

00:09:28,080 --> 00:09:32,720
mount mount the storage and then the red

00:09:30,880 --> 00:09:33,279
pods are all the ceph demons so there

00:09:32,720 --> 00:09:36,959
are

00:09:33,279 --> 00:09:40,320
quite a number of seth demons that

00:09:36,959 --> 00:09:41,760
provide that data layer the mons and the

00:09:40,320 --> 00:09:45,519
osds

00:09:41,760 --> 00:09:46,959
are backed by local storage on the node

00:09:45,519 --> 00:09:48,800
so it's very important for sef to have

00:09:46,959 --> 00:09:52,240
that local storage anyway rook

00:09:48,800 --> 00:09:54,000
deploys all these pods for you and does

00:09:52,240 --> 00:09:56,880
that management at the

00:09:54,000 --> 00:09:58,880
kubernetes resource layer creating

00:09:56,880 --> 00:10:01,279
deployments pod services

00:09:58,880 --> 00:10:02,800
okay so now at the next layer we got csi

00:10:01,279 --> 00:10:06,560
provisioning

00:10:02,800 --> 00:10:10,240
so this is uh this is a picture of how

00:10:06,560 --> 00:10:12,399
your application can request storage

00:10:10,240 --> 00:10:14,000
from the csi driver so if we start on

00:10:12,399 --> 00:10:16,079
the left here we've got an application

00:10:14,000 --> 00:10:18,880
that needs block storage

00:10:16,079 --> 00:10:19,760
okay so you create a persistent volume

00:10:18,880 --> 00:10:23,120
claim

00:10:19,760 --> 00:10:23,920
uh usually with uh read write once

00:10:23,120 --> 00:10:26,720
access

00:10:23,920 --> 00:10:28,560
okay so that when you create that claim

00:10:26,720 --> 00:10:30,399
the the request goes to the

00:10:28,560 --> 00:10:33,120
storage class which is which uses

00:10:30,399 --> 00:10:36,959
cephrbd that rook sets up for you

00:10:33,120 --> 00:10:40,000
and then the csi driver will mount that

00:10:36,959 --> 00:10:40,399
that's rbd storage into your application

00:10:40,000 --> 00:10:41,760
pod

00:10:40,399 --> 00:10:44,000
something very similar happens for

00:10:41,760 --> 00:10:46,640
shared file system where you've got two

00:10:44,000 --> 00:10:47,279
applications who need to share the file

00:10:46,640 --> 00:10:49,360
system

00:10:47,279 --> 00:10:51,120
so they both have their persistent

00:10:49,360 --> 00:10:52,640
volume claims

00:10:51,120 --> 00:10:55,200
and the request goes to the storage

00:10:52,640 --> 00:10:57,040
class and then this csr driver

00:10:55,200 --> 00:10:58,880
for the shared file system mounts that

00:10:57,040 --> 00:10:59,600
storage okay now the third type of

00:10:58,880 --> 00:11:02,000
storage

00:10:59,600 --> 00:11:03,279
we've got with ceph is object with the

00:11:02,000 --> 00:11:06,399
s3 input

00:11:03,279 --> 00:11:08,959
now this this is different but we

00:11:06,399 --> 00:11:10,160
have exposed it in a way that will be

00:11:08,959 --> 00:11:13,680
consistent

00:11:10,160 --> 00:11:15,440
with as consistent as it can be with

00:11:13,680 --> 00:11:19,519
block and file storage

00:11:15,440 --> 00:11:21,040
okay so the if you want object storage

00:11:19,519 --> 00:11:22,959
really what you want is a bucket so you

00:11:21,040 --> 00:11:25,600
can rewrite objects to the bucket

00:11:22,959 --> 00:11:26,560
and so we create a bucket claim which

00:11:25,600 --> 00:11:29,440
requests

00:11:26,560 --> 00:11:31,680
that storage from the storage class

00:11:29,440 --> 00:11:34,079
crafted for object storage

00:11:31,680 --> 00:11:34,720
and we have a bucket provisioner then

00:11:34,079 --> 00:11:38,240
that

00:11:34,720 --> 00:11:39,600
actually creates a bucket in enceph the

00:11:38,240 --> 00:11:40,720
sept optic storage

00:11:39,600 --> 00:11:42,560
and provides that back to the

00:11:40,720 --> 00:11:44,240
applications all right so that gives us

00:11:42,560 --> 00:11:46,000
all three types of storage

00:11:44,240 --> 00:11:47,360
at the provisioning now what does it

00:11:46,000 --> 00:11:48,959
look like at the data path

00:11:47,360 --> 00:11:51,040
you've installed rook at layer one

00:11:48,959 --> 00:11:54,160
you've got the csi driver with your

00:11:51,040 --> 00:11:56,320
storage mounted layer two now

00:11:54,160 --> 00:11:57,279
your application just needs to write and

00:11:56,320 --> 00:11:59,519
read data

00:11:57,279 --> 00:12:00,480
to the to the cluster so again your

00:11:59,519 --> 00:12:03,120
application

00:12:00,480 --> 00:12:04,240
it's already got the volume mounted and

00:12:03,120 --> 00:12:06,880
there is

00:12:04,240 --> 00:12:07,440
um the sef rbd kernel driver then that

00:12:06,880 --> 00:12:09,360
will

00:12:07,440 --> 00:12:11,200
take care of writing to the ceph cluster

00:12:09,360 --> 00:12:12,800
for you and that driver will

00:12:11,200 --> 00:12:14,880
it knows how to connect to the different

00:12:12,800 --> 00:12:15,600
ceph demons the mons and osds and

00:12:14,880 --> 00:12:17,600
manager

00:12:15,600 --> 00:12:18,720
you know to make that that write that

00:12:17,600 --> 00:12:20,959
data to the cluster

00:12:18,720 --> 00:12:23,760
again at the file system file system

00:12:20,959 --> 00:12:26,320
layer connect to the mds daemon in ceph

00:12:23,760 --> 00:12:27,680
which manages the you know the locks and

00:12:26,320 --> 00:12:30,000
the file system

00:12:27,680 --> 00:12:32,800
semantics and the s3 client then

00:12:30,000 --> 00:12:36,480
connects to the rdw

00:12:32,800 --> 00:12:38,880
endpoint which then

00:12:36,480 --> 00:12:39,519
writes the objects into the ceph cluster

00:12:38,880 --> 00:12:42,399
hopefully

00:12:39,519 --> 00:12:43,040
we're good there after layer three that

00:12:42,399 --> 00:12:45,680
you've

00:12:43,040 --> 00:12:46,480
set up rook and now you can write to the

00:12:45,680 --> 00:12:48,880
cluster

00:12:46,480 --> 00:12:50,639
so what does it take to get started this

00:12:48,880 --> 00:12:53,760
is you know i'm sure it sounds like

00:12:50,639 --> 00:12:55,200
a complex system but really to get

00:12:53,760 --> 00:12:57,120
started with rook we've tried to make it

00:12:55,200 --> 00:13:00,160
as simple as possible

00:12:57,120 --> 00:13:02,959
and installing ceph has never been

00:13:00,160 --> 00:13:03,519
this simple really there are three steps

00:13:02,959 --> 00:13:06,480
for

00:13:03,519 --> 00:13:08,720
for installing sap through okay the

00:13:06,480 --> 00:13:10,720
first thing is well you need to

00:13:08,720 --> 00:13:12,000
set up authorization or rbac in

00:13:10,720 --> 00:13:14,880
kubernetes so you say

00:13:12,000 --> 00:13:16,320
cuddle create common.yaml okay next you

00:13:14,880 --> 00:13:18,800
need to run the operator

00:13:16,320 --> 00:13:20,480
and so you create the operator that gets

00:13:18,800 --> 00:13:22,880
the rook operator running but

00:13:20,480 --> 00:13:24,399
now the operator you have to tell it how

00:13:22,880 --> 00:13:25,760
to deploy ceph

00:13:24,399 --> 00:13:28,480
so now we've got what we call the

00:13:25,760 --> 00:13:29,839
cluster that cluster cr

00:13:28,480 --> 00:13:32,240
and you can see a picture of that on the

00:13:29,839 --> 00:13:33,839
right here where you tell rook how to

00:13:32,240 --> 00:13:35,680
deploy stuff you tell it well what

00:13:33,839 --> 00:13:38,000
version of stuff do you want to deploy

00:13:35,680 --> 00:13:40,720
now do you want ceph octopus or do you

00:13:38,000 --> 00:13:42,800
want which is v15 or do you want a

00:13:40,720 --> 00:13:44,480
an earlier version seth nautilus which

00:13:42,800 --> 00:13:46,800
would be v14

00:13:44,480 --> 00:13:48,079
we all and you can choose what do you

00:13:46,800 --> 00:13:49,920
want the latest version

00:13:48,079 --> 00:13:51,440
or a previous version of stuff so you

00:13:49,920 --> 00:13:54,959
can choose when to

00:13:51,440 --> 00:13:57,519
update the the ceph version

00:13:54,959 --> 00:13:58,800
and then some other there are many many

00:13:57,519 --> 00:13:59,760
settings here we're just showing a few

00:13:58,800 --> 00:14:02,000
of them

00:13:59,760 --> 00:14:03,680
how many mods how to deploy the storage

00:14:02,000 --> 00:14:05,519
that is the basic cluster

00:14:03,680 --> 00:14:06,720
once you've created these three these

00:14:05,519 --> 00:14:09,279
three manifests

00:14:06,720 --> 00:14:10,480
you'll have a basic ceph cluster up and

00:14:09,279 --> 00:14:12,639
running in rook

00:14:10,480 --> 00:14:14,639
as a side note there's also a helm chart

00:14:12,639 --> 00:14:16,800
we have that will simplify

00:14:14,639 --> 00:14:17,760
these first two steps and we're working

00:14:16,800 --> 00:14:21,040
on health charts

00:14:17,760 --> 00:14:23,279
for uh our other crs going forward

00:14:21,040 --> 00:14:24,399
let's assume rook is set up now with sef

00:14:23,279 --> 00:14:27,440
what does it take to

00:14:24,399 --> 00:14:28,240
consume that storage uh just like any

00:14:27,440 --> 00:14:30,800
other

00:14:28,240 --> 00:14:32,399
storage platform first of all the admin

00:14:30,800 --> 00:14:33,760
has to create the storage class

00:14:32,399 --> 00:14:35,839
after they have increased the storage

00:14:33,760 --> 00:14:38,160
class the application

00:14:35,839 --> 00:14:40,079
request that storage was with the pvc

00:14:38,160 --> 00:14:41,279
and then you create your application pod

00:14:40,079 --> 00:14:43,760
that

00:14:41,279 --> 00:14:45,120
that will mount that that volume so here

00:14:43,760 --> 00:14:46,000
on the right we've just got an example

00:14:45,120 --> 00:14:50,079
pod

00:14:46,000 --> 00:14:52,480
that shows this the pvc

00:14:50,079 --> 00:14:53,360
that's going to be mounted into this

00:14:52,480 --> 00:14:55,839
this demo

00:14:53,360 --> 00:14:56,959
web server and really this if you've

00:14:55,839 --> 00:14:59,519
done any storage

00:14:56,959 --> 00:15:00,320
and kubernetes before this should be

00:14:59,519 --> 00:15:02,480
very familiar

00:15:00,320 --> 00:15:04,639
just the same pattern that used to plug

00:15:02,480 --> 00:15:06,639
in any other storage provider

00:15:04,639 --> 00:15:08,240
outside of rook either okay and now i'll

00:15:06,639 --> 00:15:10,560
hand off to sebastian

00:15:08,240 --> 00:15:11,600
to talk more about key features and

00:15:10,560 --> 00:15:15,120
everything and stuff

00:15:11,600 --> 00:15:18,720
okay thanks travis now let's dive into

00:15:15,120 --> 00:15:22,079
some of the key features of rook

00:15:18,720 --> 00:15:23,680
so first environments ruksef is capable

00:15:22,079 --> 00:15:26,880
of deploying

00:15:23,680 --> 00:15:27,839
ceph permanent environments so bring

00:15:26,880 --> 00:15:30,399
your own hardware

00:15:27,839 --> 00:15:31,680
if you have your own infrastructure and

00:15:30,399 --> 00:15:35,120
you want to work on premise

00:15:31,680 --> 00:15:38,079
then this scenario is for you

00:15:35,120 --> 00:15:39,120
we also support deploying rooksef into

00:15:38,079 --> 00:15:42,399
the cloud

00:15:39,120 --> 00:15:43,519
with various cloud providers so if you

00:15:42,399 --> 00:15:46,240
already

00:15:43,519 --> 00:15:47,360
if your entire infrastructure relies on

00:15:46,240 --> 00:15:50,480
the cloud and

00:15:47,360 --> 00:15:54,480
everything is running there already then

00:15:50,480 --> 00:15:56,880
you can consume storage through rook

00:15:54,480 --> 00:15:57,839
inside inside the cloud environment but

00:15:56,880 --> 00:16:00,079
first

00:15:57,839 --> 00:16:01,360
you might you might ask yourself why

00:16:00,079 --> 00:16:03,519
would you run

00:16:01,360 --> 00:16:05,600
ceph in the cloud so basically it's all

00:16:03,519 --> 00:16:07,920
about consistency

00:16:05,600 --> 00:16:09,920
because kubernetes has been adopted by

00:16:07,920 --> 00:16:12,800
all the major cloud providers

00:16:09,920 --> 00:16:14,959
today then it is very easy to run any

00:16:12,800 --> 00:16:17,519
cloud native application as part of

00:16:14,959 --> 00:16:19,440
any cloud providers available out there

00:16:17,519 --> 00:16:22,639
and also there are a bunch of

00:16:19,440 --> 00:16:24,639
limitations shortcomings coming from the

00:16:22,639 --> 00:16:26,639
cloud providers

00:16:24,639 --> 00:16:28,720
for instance the limitation of pvs you

00:16:26,639 --> 00:16:30,320
can attach to a given node

00:16:28,720 --> 00:16:32,320
let's assume that we are in a cloud

00:16:30,320 --> 00:16:33,759
environment then obviously we have

00:16:32,320 --> 00:16:35,440
hypervisors and

00:16:33,759 --> 00:16:38,240
those hypervisors typically have a

00:16:35,440 --> 00:16:40,880
limitation in a number of disks

00:16:38,240 --> 00:16:41,839
that can be attached to a given virtual

00:16:40,880 --> 00:16:45,040
machine

00:16:41,839 --> 00:16:47,440
and typically that limit is around 30

00:16:45,040 --> 00:16:49,519
which means that if you're looking at

00:16:47,440 --> 00:16:51,600
providing dynamic provisioning

00:16:49,519 --> 00:16:53,519
to applications then you might be

00:16:51,600 --> 00:16:55,680
limited by

00:16:53,519 --> 00:16:57,759
the number of vms times the number of

00:16:55,680 --> 00:17:01,440
pvs you can attach to it

00:16:57,759 --> 00:17:03,120
if you do run rook in the cloud though

00:17:01,440 --> 00:17:05,280
rook will be running inside virtual

00:17:03,120 --> 00:17:06,319
machines and because we will be

00:17:05,280 --> 00:17:09,280
attaching

00:17:06,319 --> 00:17:11,520
devices then we use our own technology

00:17:09,280 --> 00:17:14,559
to provide dynamic provisioning

00:17:11,520 --> 00:17:15,199
then we can scale up to thousands and

00:17:14,559 --> 00:17:18,240
thousands

00:17:15,199 --> 00:17:21,360
of of pvs instead of that 30

00:17:18,240 --> 00:17:22,480
limitation by virtual machine and not

00:17:21,360 --> 00:17:24,640
only we

00:17:22,480 --> 00:17:26,799
bypass that limitation but also because

00:17:24,640 --> 00:17:29,840
we are aggregating storage

00:17:26,799 --> 00:17:31,760
then we're providing much much better

00:17:29,840 --> 00:17:33,919
performance overall

00:17:31,760 --> 00:17:36,320
if you were to use a single disk which

00:17:33,919 --> 00:17:38,320
is typically attached to

00:17:36,320 --> 00:17:40,720
a storage class which essentially

00:17:38,320 --> 00:17:44,160
represents the flavor of storage

00:17:40,720 --> 00:17:44,640
then that flavor would have iops as well

00:17:44,160 --> 00:17:47,280
as

00:17:44,640 --> 00:17:49,360
throughput limitations so for a given

00:17:47,280 --> 00:17:52,640
application you will be limited to

00:17:49,360 --> 00:17:54,559
what that flavor can can provide or if

00:17:52,640 --> 00:17:57,039
you use rook self because as i said

00:17:54,559 --> 00:17:58,240
again we are aggregating many pvs then

00:17:57,039 --> 00:18:00,720
we're providing much

00:17:58,240 --> 00:18:02,000
much better performance out of the box

00:18:00,720 --> 00:18:05,280
we have configurable

00:18:02,000 --> 00:18:06,559
clustered topologies seth is really

00:18:05,280 --> 00:18:10,080
amazing as

00:18:06,559 --> 00:18:13,520
being topology award seth

00:18:10,080 --> 00:18:15,919
knows and because it's being told to

00:18:13,520 --> 00:18:18,000
where it is running in what kind of

00:18:15,919 --> 00:18:20,880
topology it is running on

00:18:18,000 --> 00:18:22,000
rook can support that so it really

00:18:20,880 --> 00:18:25,760
allows you to

00:18:22,000 --> 00:18:27,760
deploy a cluster and define your what

00:18:25,760 --> 00:18:30,640
your topology is

00:18:27,760 --> 00:18:32,400
so if you run ceph in rook step inside a

00:18:30,640 --> 00:18:33,679
data center and then you have multiple

00:18:32,400 --> 00:18:35,679
rooms

00:18:33,679 --> 00:18:36,880
multiple racks and multiple nodes then

00:18:35,679 --> 00:18:39,280
we can easily build

00:18:36,880 --> 00:18:40,160
such topology by assigning labels to

00:18:39,280 --> 00:18:42,080
nodes

00:18:40,160 --> 00:18:43,760
and all of that will be reflected as

00:18:42,080 --> 00:18:44,720
part of step by doing so you're

00:18:43,760 --> 00:18:48,080
increasing

00:18:44,720 --> 00:18:49,039
data availability and resiliency because

00:18:48,080 --> 00:18:51,360
you're spreading it

00:18:49,039 --> 00:18:53,120
across different field domains and again

00:18:51,360 --> 00:18:56,400
those domains can be

00:18:53,120 --> 00:18:58,720
nodes racks rooms and even

00:18:56,400 --> 00:18:59,840
across zones between different data

00:18:58,720 --> 00:19:00,960
centers if you want to stretch your

00:18:59,840 --> 00:19:03,360
cluster

00:19:00,960 --> 00:19:05,440
safe csi drivers so from the very

00:19:03,360 --> 00:19:07,919
beginning rook

00:19:05,440 --> 00:19:08,720
once csi was released and the spec was

00:19:07,919 --> 00:19:11,200
released

00:19:08,720 --> 00:19:12,240
we started to collaborate the cfcsi team

00:19:11,200 --> 00:19:14,080
to integrate as

00:19:12,240 --> 00:19:16,559
much as possible so just like travis

00:19:14,080 --> 00:19:18,440
mentioned we support a large variety of

00:19:16,559 --> 00:19:21,440
dynamic provisioning modes

00:19:18,440 --> 00:19:24,080
rwx and rocks

00:19:21,440 --> 00:19:26,160
and this for both block and file system

00:19:24,080 --> 00:19:29,360
interfaces

00:19:26,160 --> 00:19:31,840
rook the latest release of rook 1 5

00:19:29,360 --> 00:19:34,240
ships with the latest version of the

00:19:31,840 --> 00:19:36,960
system csi driver

00:19:34,240 --> 00:19:38,960
which now supports volume expansion and

00:19:36,960 --> 00:19:42,880
as a better feature supports

00:19:38,960 --> 00:19:45,360
snapshots and clones we still do support

00:19:42,880 --> 00:19:46,320
the flex driver but it has really

00:19:45,360 --> 00:19:48,559
limited support

00:19:46,320 --> 00:19:49,679
and we really encourage everybody to

00:19:48,559 --> 00:19:52,640
switch to

00:19:49,679 --> 00:19:54,400
using the staff csi spec upgrading is

00:19:52,640 --> 00:19:57,840
automated

00:19:54,400 --> 00:20:00,000
typically upgrades are some kind of a

00:19:57,840 --> 00:20:03,200
tedious process

00:20:00,000 --> 00:20:03,840
and it is really making administrators

00:20:03,200 --> 00:20:05,919
nervous

00:20:03,840 --> 00:20:07,679
especially when it comes to storage you

00:20:05,919 --> 00:20:10,000
always be wondering okay

00:20:07,679 --> 00:20:11,360
if something goes wrong will i be losing

00:20:10,000 --> 00:20:13,919
data

00:20:11,360 --> 00:20:15,039
first this cannot really happen with sev

00:20:13,919 --> 00:20:17,200
and also

00:20:15,039 --> 00:20:18,960
one of the good things about surf is

00:20:17,200 --> 00:20:20,960
from the very beginning it has proven to

00:20:18,960 --> 00:20:23,360
be super robust at performing upgrades

00:20:20,960 --> 00:20:25,919
and it's one of the few

00:20:23,360 --> 00:20:27,200
storage solution not to say software

00:20:25,919 --> 00:20:31,200
most of the time

00:20:27,200 --> 00:20:34,720
that can upgrade in a rolling fashion

00:20:31,200 --> 00:20:36,159
with no downtime and yeah while

00:20:34,720 --> 00:20:38,960
providing a service

00:20:36,159 --> 00:20:40,960
so seth is already really good at it and

00:20:38,960 --> 00:20:43,840
with rook we really took that

00:20:40,960 --> 00:20:44,559
to the next we really really took that

00:20:43,840 --> 00:20:47,280
further

00:20:44,559 --> 00:20:48,080
to the next level by aggregating

00:20:47,280 --> 00:20:49,679
collecting

00:20:48,080 --> 00:20:51,679
all the operational knowledge that is

00:20:49,679 --> 00:20:53,840
needed to do upgrades

00:20:51,679 --> 00:20:54,799
and we embedded all of that logic into

00:20:53,840 --> 00:20:57,360
rook

00:20:54,799 --> 00:20:59,360
so to a network it is really easy as

00:20:57,360 --> 00:21:02,400
just changing the image in the spec

00:20:59,360 --> 00:21:04,720
of the deployment file of the operator

00:21:02,400 --> 00:21:06,880
to update to the step cluster then it's

00:21:04,720 --> 00:21:09,919
as easy as changing the

00:21:06,880 --> 00:21:12,240
self image version into the cluster cr

00:21:09,919 --> 00:21:13,760
and rook will handle all the details

00:21:12,240 --> 00:21:16,000
when it comes to the upgrade

00:21:13,760 --> 00:21:16,799
internally going notes by nodes and

00:21:16,000 --> 00:21:18,720
making sure

00:21:16,799 --> 00:21:21,200
they're all recovering and being

00:21:18,720 --> 00:21:24,720
upgraded and backfilling properly

00:21:21,200 --> 00:21:26,320
and then go to the other one so upgrades

00:21:24,720 --> 00:21:28,080
made easy basically

00:21:26,320 --> 00:21:30,400
x no cluster connection so this is a

00:21:28,080 --> 00:21:34,720
really popular scenario

00:21:30,400 --> 00:21:36,240
and we have introduced it in the 1.3

00:21:34,720 --> 00:21:38,080
release cycle

00:21:36,240 --> 00:21:40,640
and the the use case is quite simple

00:21:38,080 --> 00:21:42,880
because not everything is about

00:21:40,640 --> 00:21:44,320
green field environments you already

00:21:42,880 --> 00:21:47,360
have

00:21:44,320 --> 00:21:49,440
clusters out there up and running

00:21:47,360 --> 00:21:51,280
which are maybe standalone clusters

00:21:49,440 --> 00:21:53,280
providing object storage

00:21:51,280 --> 00:21:54,880
or you might have subclusters providing

00:21:53,280 --> 00:21:57,039
block for

00:21:54,880 --> 00:21:59,120
let's say openstack for example you're

00:21:57,039 --> 00:22:01,520
ready to run applications

00:21:59,120 --> 00:22:02,720
on kubernetes but you're not you're not

00:22:01,520 --> 00:22:04,880
really ready to

00:22:02,720 --> 00:22:07,760
move all of that storage because it is

00:22:04,880 --> 00:22:10,720
difficult and it's challenging to do

00:22:07,760 --> 00:22:13,280
to two kubernetes so you want to keep

00:22:10,720 --> 00:22:16,320
your cluster which was deployed by

00:22:13,280 --> 00:22:18,080
whatever tool can be can be defensible

00:22:16,320 --> 00:22:21,120
can be rook as well if you want

00:22:18,080 --> 00:22:23,039
the print the goal is to

00:22:21,120 --> 00:22:24,159
from your kubernetes cluster you deploy

00:22:23,039 --> 00:22:27,200
rook

00:22:24,159 --> 00:22:29,280
and then with a few details you connect

00:22:27,200 --> 00:22:30,799
to the external cluster and then once

00:22:29,280 --> 00:22:32,480
the connection is established we're

00:22:30,799 --> 00:22:34,480
really into this consumer producer

00:22:32,480 --> 00:22:36,159
relationship where rook is at this point

00:22:34,480 --> 00:22:38,320
only consuming the external

00:22:36,159 --> 00:22:40,559
storage getting all the connection

00:22:38,320 --> 00:22:41,919
details and passing them to csi

00:22:40,559 --> 00:22:43,440
so that we can provide persistent

00:22:41,919 --> 00:22:45,120
storage to containers but that is the

00:22:43,440 --> 00:22:48,159
only thing we do we don't

00:22:45,120 --> 00:22:50,240
get into the business of managing nodes

00:22:48,159 --> 00:22:52,159
and things like this when we are in

00:22:50,240 --> 00:22:53,679
external mode so we just consume the

00:22:52,159 --> 00:22:55,760
storage that is

00:22:53,679 --> 00:22:57,440
available to us object packet

00:22:55,760 --> 00:23:00,000
provisioning

00:22:57,440 --> 00:23:01,840
so just like travis mentioned earlier

00:23:00,000 --> 00:23:03,440
when we were showing the topology of the

00:23:01,840 --> 00:23:05,360
storage interfaces are supported by

00:23:03,440 --> 00:23:08,320
rooksef

00:23:05,360 --> 00:23:09,200
we mentioned the object bucket claim and

00:23:08,320 --> 00:23:12,320
again

00:23:09,200 --> 00:23:14,080
the this is quite simple as a user

00:23:12,320 --> 00:23:16,720
the only thing i care about is because

00:23:14,080 --> 00:23:17,520
i'm developing an s3 compliant

00:23:16,720 --> 00:23:19,600
application

00:23:17,520 --> 00:23:21,600
i just want to have a bucket with a few

00:23:19,600 --> 00:23:22,960
policies on that bucket

00:23:21,600 --> 00:23:24,799
and i want to be able to consume that

00:23:22,960 --> 00:23:27,360
bucket storing data and

00:23:24,799 --> 00:23:28,000
retrieving data and that's all i care

00:23:27,360 --> 00:23:31,280
about

00:23:28,000 --> 00:23:33,440
so in a similar fashion of claiming for

00:23:31,280 --> 00:23:34,559
block or file then i want to claim for a

00:23:33,440 --> 00:23:37,919
bucket

00:23:34,559 --> 00:23:39,760
and for that we have embedded for

00:23:37,919 --> 00:23:42,000
quite a while now the lead bucket

00:23:39,760 --> 00:23:45,200
provisioner

00:23:42,000 --> 00:23:46,559
but because we we really wanted to have

00:23:45,200 --> 00:23:49,840
it in a more

00:23:46,559 --> 00:23:52,880
native slash native kubernetes way

00:23:49,840 --> 00:23:55,600
there was enough upstream to

00:23:52,880 --> 00:23:57,120
implement a cozy interface which is

00:23:55,600 --> 00:24:00,480
similar to csi

00:23:57,120 --> 00:24:01,120
but for container object and it is

00:24:00,480 --> 00:24:04,720
really

00:24:01,120 --> 00:24:06,799
the csi equivalent of object

00:24:04,720 --> 00:24:09,200
so the capped kubernetes announcement

00:24:06,799 --> 00:24:12,240
proposal was merged upstream

00:24:09,200 --> 00:24:14,080
and now we will be able to

00:24:12,240 --> 00:24:15,360
vendors will be able to integrate their

00:24:14,080 --> 00:24:18,000
object solution

00:24:15,360 --> 00:24:19,200
through a native kubernetes interface

00:24:18,000 --> 00:24:22,799
which is really great

00:24:19,200 --> 00:24:25,600
so we just released rook one five

00:24:22,799 --> 00:24:26,480
and we're super excited because we have

00:24:25,600 --> 00:24:29,600
a bunch of

00:24:26,480 --> 00:24:32,640
features and it's probably one of the

00:24:29,600 --> 00:24:37,360
most feature-rich release we have

00:24:32,640 --> 00:24:39,840
ever delivered so first is

00:24:37,360 --> 00:24:41,039
encryption with kms support during the

00:24:39,840 --> 00:24:44,799
wonderful cycle we

00:24:41,039 --> 00:24:47,279
introduced encryption for osds on pvcs

00:24:44,799 --> 00:24:48,880
and we were storing encryption keys into

00:24:47,279 --> 00:24:50,640
kubernetes secrets so we all know that

00:24:48,880 --> 00:24:52,559
kubernetes secrets are not so

00:24:50,640 --> 00:24:54,240
secret and secure because they're just

00:24:52,559 --> 00:24:57,520
merely a

00:24:54,240 --> 00:25:00,799
a base 64 hash value of of your value

00:24:57,520 --> 00:25:02,880
to move that to the next level we have

00:25:00,799 --> 00:25:05,520
introduced support for

00:25:02,880 --> 00:25:07,200
kms and the first one we have introduced

00:25:05,520 --> 00:25:11,520
is hashicorp vault

00:25:07,200 --> 00:25:14,000
which is really popular and now we

00:25:11,520 --> 00:25:15,840
not only deploy encrypted osds but we

00:25:14,000 --> 00:25:19,279
store those secret keys

00:25:15,840 --> 00:25:22,159
inside vault and vault

00:25:19,279 --> 00:25:23,360
as a key management service will manage

00:25:22,159 --> 00:25:25,279
all the keys for us

00:25:23,360 --> 00:25:27,279
and in this really secure fashion in a

00:25:25,279 --> 00:25:29,919
really secure environment

00:25:27,279 --> 00:25:30,960
all the connections to vault are tls

00:25:29,919 --> 00:25:33,440
encrypted

00:25:30,960 --> 00:25:34,799
and today we only support the token

00:25:33,440 --> 00:25:36,400
based authentication

00:25:34,799 --> 00:25:38,159
you know in the future we are planning

00:25:36,400 --> 00:25:41,039
on supporting the

00:25:38,159 --> 00:25:42,400
kubernetes native authentication and

00:25:41,039 --> 00:25:45,360
obviously

00:25:42,400 --> 00:25:46,880
much more kms as well mirroring of block

00:25:45,360 --> 00:25:49,760
data

00:25:46,880 --> 00:25:51,279
this one is really interesting because

00:25:49,760 --> 00:25:53,360
we have been having

00:25:51,279 --> 00:25:54,880
support for bootstrapping rbd mirror

00:25:53,360 --> 00:25:56,880
demons

00:25:54,880 --> 00:25:58,559
for a while but we never got into the

00:25:56,880 --> 00:26:00,799
business of configuring mirroring

00:25:58,559 --> 00:26:05,360
between two sites automatically

00:26:00,799 --> 00:26:08,880
and now it's possible with rook one five

00:26:05,360 --> 00:26:12,799
just to get a a step back a little bit

00:26:08,880 --> 00:26:16,080
seth by design is strongly consistent

00:26:12,799 --> 00:26:17,039
meaning that whenever a client writes a

00:26:16,080 --> 00:26:20,240
data

00:26:17,039 --> 00:26:22,000
then it has to wait for

00:26:20,240 --> 00:26:24,559
all the replicas to be written so that

00:26:22,000 --> 00:26:28,159
the writing is acknowledged

00:26:24,559 --> 00:26:31,679
and because to that it makes

00:26:28,159 --> 00:26:33,360
difficult to stretch stuff between

00:26:31,679 --> 00:26:35,279
regions for example because of the

00:26:33,360 --> 00:26:36,960
latencies and that won't be really

00:26:35,279 --> 00:26:40,559
practical

00:26:36,960 --> 00:26:44,240
so as as a solution for that

00:26:40,559 --> 00:26:46,400
we the rook team has built the arbiter

00:26:44,240 --> 00:26:48,960
the rbd mirror daemon so that we can

00:26:46,400 --> 00:26:52,960
replicate block devices asynchronously

00:26:48,960 --> 00:26:56,320
between clusters now you can have

00:26:52,960 --> 00:26:59,279
two kubernetes environments and

00:26:56,320 --> 00:27:02,640
connect each other's and all the blocked

00:26:59,279 --> 00:27:04,720
devices so the pvcs will be replicated

00:27:02,640 --> 00:27:07,600
obviously this works along with with

00:27:04,720 --> 00:27:11,120
steps csi

00:27:07,600 --> 00:27:13,279
and and work is uh is still still moving

00:27:11,120 --> 00:27:14,799
ahead to to improve that support so now

00:27:13,279 --> 00:27:18,480
we do support automating

00:27:14,799 --> 00:27:21,360
automatic configuration of of peers

00:27:18,480 --> 00:27:22,399
and where pr is basically a site last

00:27:21,360 --> 00:27:25,360
but not least

00:27:22,399 --> 00:27:27,039
the the stretch cluster accumulates uh

00:27:25,360 --> 00:27:30,159
the stretch kubernetes cluster

00:27:27,039 --> 00:27:31,120
which is essentially one cluster that is

00:27:30,159 --> 00:27:32,799
stretched

00:27:31,120 --> 00:27:34,799
one kubernetes crystal is stretched

00:27:32,799 --> 00:27:37,360
across different zones

00:27:34,799 --> 00:27:39,600
so the use case typically is that i have

00:27:37,360 --> 00:27:41,279
two data centers but i only have two

00:27:39,600 --> 00:27:43,760
i don't have three data centers which

00:27:41,279 --> 00:27:47,039
will be really ideal

00:27:43,760 --> 00:27:50,240
but i only have two and because ceph is

00:27:47,039 --> 00:27:53,279
a quorum based election based

00:27:50,240 --> 00:27:55,600
we need to have some

00:27:53,279 --> 00:27:56,559
quorum resolution when we do certain

00:27:55,600 --> 00:27:58,480
operations

00:27:56,559 --> 00:28:00,399
in order to maintain the stability of

00:27:58,480 --> 00:28:02,640
the cluster

00:28:00,399 --> 00:28:04,480
so if we had three sides and it would be

00:28:02,640 --> 00:28:07,200
really easy we would have

00:28:04,480 --> 00:28:08,159
each one on one side and done but

00:28:07,200 --> 00:28:11,679
because i only have

00:28:08,159 --> 00:28:14,399
two and my first site is really limited

00:28:11,679 --> 00:28:18,080
it can be a vm running on the cloud or

00:28:14,399 --> 00:28:21,360
or it can be in the admin

00:28:18,080 --> 00:28:24,640
desk something like this we

00:28:21,360 --> 00:28:28,640
have to add a solution for sev to

00:28:24,640 --> 00:28:31,200
work a bit smoothly

00:28:28,640 --> 00:28:32,320
with latencies with higher latencies

00:28:31,200 --> 00:28:35,440
basically

00:28:32,320 --> 00:28:37,520
and we implemented this new mode uh in

00:28:35,440 --> 00:28:40,399
rook so now you can

00:28:37,520 --> 00:28:41,520
have a stretched kubernetes cluster in

00:28:40,399 --> 00:28:44,880
an orbital zone

00:28:41,520 --> 00:28:47,120
which will only contain one month to

00:28:44,880 --> 00:28:48,240
resolve quorum and act as a member of

00:28:47,120 --> 00:28:51,120
the quran

00:28:48,240 --> 00:28:53,120
and perform elections and pursue

00:28:51,120 --> 00:28:55,520
decision making

00:28:53,120 --> 00:28:56,480
all of the other zones will have storage

00:28:55,520 --> 00:28:59,360
available

00:28:56,480 --> 00:29:00,880
and i guess that's it for today so it is

00:28:59,360 --> 00:29:03,679
really easy to get involved

00:29:00,880 --> 00:29:04,640
as soon as you reach rio then you will

00:29:03,679 --> 00:29:07,679
have

00:29:04,640 --> 00:29:10,960
links to everywhere basically um

00:29:07,679 --> 00:29:13,120
the doc the slack channel and

00:29:10,960 --> 00:29:15,200
how to contribute uh obviously on on

00:29:13,120 --> 00:29:17,679
github thank you for

00:29:15,200 --> 00:29:18,320
your attention uh thanks for being here

00:29:17,679 --> 00:29:20,480
today

00:29:18,320 --> 00:29:21,520
it was a pleasure talking with everybody

00:29:20,480 --> 00:29:23,120
and uh

00:29:21,520 --> 00:29:24,960
hopefully we will see everybody in

00:29:23,120 --> 00:29:28,159
person next time have a good

00:29:24,960 --> 00:29:29,760
day and stay safe bye-bye yes thank you

00:29:28,159 --> 00:29:31,200
it's been great to be with you hopefully

00:29:29,760 --> 00:29:32,480
you've learned a bit a little bit about

00:29:31,200 --> 00:29:34,720
rook and we hope to

00:29:32,480 --> 00:29:36,360
to see you in the rook slack or other

00:29:34,720 --> 00:29:39,360
community

00:29:36,360 --> 00:29:39,360

YouTube URL: https://www.youtube.com/watch?v=aO-n4FuOU2w


