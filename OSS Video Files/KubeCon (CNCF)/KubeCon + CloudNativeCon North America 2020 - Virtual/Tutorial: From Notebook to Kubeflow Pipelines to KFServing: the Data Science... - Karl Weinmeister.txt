Title: Tutorial: From Notebook to Kubeflow Pipelines to KFServing: the Data Science... - Karl Weinmeister
Publication date: 2020-11-23
Playlist: KubeCon + CloudNativeCon North America 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon Europe 2021 Virtual from May 4–7, 2021. Learn more at https://kubecon.io. The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects. 

Tutorial: From Notebook to Kubeflow Pipelines to KFServing: the Data Science Odyssey - Karl Weinmeister, Google & Stefano Fioravanzo, Arrikto 

A hands-on lab driven tutorial to show Data Scientists and ML Engineers alike how to turbocharge your Kubeflow efforts. In this session you will learn how to quickly build, tune, and execute complex Kubeflow workflows - as well as how to work faster using Kale to automate much of your work. Learn how to rapidly automate Kubeflow: - Deploy a Jupyter Notebook as a Kubeflow pipeline using Kale - Optimize your model training using Katib for hyperparameter tuning - Serve your model with KFServing - Run thousands of runs with caching and garbage collection - Track and reproduce pipeline steps along with their state and artifacts Data Scientists benefit from an intuitive GUI that automates and hides all of the underlying infrastructure and SDK requirements. ML Engineers can use the reproducible, automated workflows as a scaffold to quickly move to even more advanced tuning and model building. 

https://sched.co/ekFQ
Captions: 
	00:00:02,000 --> 00:00:06,000
hi my name is carl weinmeister

00:00:04,400 --> 00:00:07,440
and i'm joined today by stefano

00:00:06,000 --> 00:00:09,120
firavonzo

00:00:07,440 --> 00:00:10,639
we're going to talk about the data

00:00:09,120 --> 00:00:12,960
science odyssey

00:00:10,639 --> 00:00:14,160
from a notebook all the way through to

00:00:12,960 --> 00:00:16,320
serving a model

00:00:14,160 --> 00:00:18,720
and everything in between we're going to

00:00:16,320 --> 00:00:20,560
cover a complete data science workflow

00:00:18,720 --> 00:00:22,720
and introduce several products that help

00:00:20,560 --> 00:00:26,000
along the way

00:00:22,720 --> 00:00:27,039
let's get started the primary use case

00:00:26,000 --> 00:00:30,160
we're going to cover

00:00:27,039 --> 00:00:32,239
is hyper parameter optimization which is

00:00:30,160 --> 00:00:34,160
the ability to search across a wide

00:00:32,239 --> 00:00:36,719
variety of parameters

00:00:34,160 --> 00:00:39,120
to find the parameters that will create

00:00:36,719 --> 00:00:41,120
the best model

00:00:39,120 --> 00:00:43,360
in general in this session we're going

00:00:41,120 --> 00:00:45,280
to learn about simplifying

00:00:43,360 --> 00:00:47,440
your hyper parameter tuning as well as

00:00:45,280 --> 00:00:49,440
your serving workflows with intuitive

00:00:47,440 --> 00:00:51,680
uis

00:00:49,440 --> 00:00:53,280
the benefit is that you will accelerate

00:00:51,680 --> 00:00:56,320
your time to production

00:00:53,280 --> 00:00:59,120
with reduced training time and reduced

00:00:56,320 --> 00:00:59,680
time to build a process you're going to

00:00:59,120 --> 00:01:01,680
get to

00:00:59,680 --> 00:01:03,039
the answers that you want faster into

00:01:01,680 --> 00:01:04,799
deployment faster

00:01:03,039 --> 00:01:07,439
and finally we're going to cover

00:01:04,799 --> 00:01:09,280
collaboration so reducing friction

00:01:07,439 --> 00:01:13,280
between the teams that build the model

00:01:09,280 --> 00:01:16,159
and the teams that deploy the model

00:01:13,280 --> 00:01:17,840
first what's kubeflow so kubflow is an

00:01:16,159 --> 00:01:22,080
open source project

00:01:17,840 --> 00:01:25,600
aimed at making ml deployments simple

00:01:22,080 --> 00:01:27,920
portable across on-prem multiple cloud

00:01:25,600 --> 00:01:29,759
environments and because it takes

00:01:27,920 --> 00:01:32,400
advantage of kubernetes

00:01:29,759 --> 00:01:33,840
scalable so that you can run distributed

00:01:32,400 --> 00:01:35,680
training jobs

00:01:33,840 --> 00:01:38,000
and so that you can serve your models in

00:01:35,680 --> 00:01:40,000
a way that can

00:01:38,000 --> 00:01:42,400
handle the wide variety of use that you

00:01:40,000 --> 00:01:42,400
might have

00:01:43,680 --> 00:01:46,960
the use cases we're going to cover today

00:01:45,920 --> 00:01:50,000
i cover

00:01:46,960 --> 00:01:52,000
quite a variety the first is

00:01:50,000 --> 00:01:53,200
that we're going to cover a complex ml

00:01:52,000 --> 00:01:55,600
system at scale

00:01:53,200 --> 00:01:57,360
so we're going to cover more than what

00:01:55,600 --> 00:02:00,000
they might call a toy data set

00:01:57,360 --> 00:02:01,200
where it's small it fits in one virtual

00:02:00,000 --> 00:02:02,159
machine we're going to talk about

00:02:01,200 --> 00:02:04,719
handling

00:02:02,159 --> 00:02:07,360
a more complex scenario we're going to

00:02:04,719 --> 00:02:10,319
talk about rapid experimentation

00:02:07,360 --> 00:02:11,200
hyper hyperparameter tuning seeing how

00:02:10,319 --> 00:02:13,040
all this can

00:02:11,200 --> 00:02:14,319
work in both hybrid and multi-cloud

00:02:13,040 --> 00:02:16,400
workloads

00:02:14,319 --> 00:02:18,160
and finally you might see how these

00:02:16,400 --> 00:02:20,080
traditional concepts and software

00:02:18,160 --> 00:02:20,959
development of continuous integration

00:02:20,080 --> 00:02:22,400
and deployment

00:02:20,959 --> 00:02:24,160
might apply in the machine learning

00:02:22,400 --> 00:02:26,640
world

00:02:24,160 --> 00:02:28,239
if you think about it there's a lot of

00:02:26,640 --> 00:02:31,760
similarities there

00:02:28,239 --> 00:02:35,200
concepts like testing the input data

00:02:31,760 --> 00:02:37,040
testing the output of your model

00:02:35,200 --> 00:02:38,319
making sure there's not a regression so

00:02:37,040 --> 00:02:40,319
that your model

00:02:38,319 --> 00:02:41,440
is better than it was before if you want

00:02:40,319 --> 00:02:44,720
to deploy it

00:02:41,440 --> 00:02:48,080
all of these things you can codify into

00:02:44,720 --> 00:02:51,840
a pipeline that's reproducible and

00:02:48,080 --> 00:02:51,840
definable within your notebook

00:02:52,640 --> 00:02:56,239
so the kubeflow platform has multiple

00:02:55,680 --> 00:02:59,920
layers

00:02:56,239 --> 00:03:02,159
let's walk through each of those first

00:02:59,920 --> 00:03:04,080
you can bring whatever framework that

00:03:02,159 --> 00:03:06,000
you're comfortable with

00:03:04,080 --> 00:03:08,000
whatever machine learning library you've

00:03:06,000 --> 00:03:10,159
learned or want to learn

00:03:08,000 --> 00:03:11,120
you can use on kubeflow there are a

00:03:10,159 --> 00:03:13,920
variety

00:03:11,120 --> 00:03:15,599
of what we call operators that allow you

00:03:13,920 --> 00:03:19,840
to run your training jobs

00:03:15,599 --> 00:03:19,840
and serve your models

00:03:20,720 --> 00:03:25,360
in this layer we see the heart of

00:03:22,159 --> 00:03:29,440
kubeflow which is a set of components

00:03:25,360 --> 00:03:31,760
for capabilities like notebooks

00:03:29,440 --> 00:03:33,920
kale which is going to help convert the

00:03:31,760 --> 00:03:35,599
notebook into a pipeline

00:03:33,920 --> 00:03:37,760
the pipeline itself which helps to

00:03:35,599 --> 00:03:40,159
orchestrate your workflow

00:03:37,760 --> 00:03:41,920
hyperparameter tuning which is called

00:03:40,159 --> 00:03:43,440
cotton

00:03:41,920 --> 00:03:45,360
and several other components we won't

00:03:43,440 --> 00:03:47,120
get into all of these right now

00:03:45,360 --> 00:03:49,120
you also see a variety of serving

00:03:47,120 --> 00:03:50,879
components as well as capabilities for

00:03:49,120 --> 00:03:52,480
logging and monitoring

00:03:50,879 --> 00:03:55,200
the whole stack of things that are

00:03:52,480 --> 00:03:55,200
necessary

00:03:55,760 --> 00:03:59,920
and all of this is built on top of

00:03:57,519 --> 00:04:01,280
kubernetes so that you're able to run

00:03:59,920 --> 00:04:04,560
your workload

00:04:01,280 --> 00:04:06,560
across multiple environments and the way

00:04:04,560 --> 00:04:07,840
that kubeflow is designed

00:04:06,560 --> 00:04:11,599
it will come naturally if you're

00:04:07,840 --> 00:04:11,599
comfortable with kubernetes concepts

00:04:12,560 --> 00:04:15,920
all right let's walk through the

00:04:14,239 --> 00:04:17,359
workflow we're going to discuss in more

00:04:15,920 --> 00:04:19,759
detail today

00:04:17,359 --> 00:04:21,040
so the first step is in the jupyter

00:04:19,759 --> 00:04:22,479
notebook itself

00:04:21,040 --> 00:04:24,880
which is available in the kubeflow

00:04:22,479 --> 00:04:26,320
environment in this notebook you're

00:04:24,880 --> 00:04:28,880
going to start with

00:04:26,320 --> 00:04:29,840
identifying the problem and some data

00:04:28,880 --> 00:04:32,560
exploration

00:04:29,840 --> 00:04:35,040
and analysis next you choose an

00:04:32,560 --> 00:04:38,240
algorithm code your model

00:04:35,040 --> 00:04:41,120
the next step is experimentation

00:04:38,240 --> 00:04:42,800
and model training we're going to tune

00:04:41,120 --> 00:04:44,320
the model parameters

00:04:42,800 --> 00:04:45,840
and then finally we'll be able to serve

00:04:44,320 --> 00:04:49,360
it and that will take us all the way

00:04:45,840 --> 00:04:52,400
through the key steps of the process

00:04:49,360 --> 00:04:56,560
let's look at how kubeflow works so

00:04:52,400 --> 00:04:58,960
let's start with the user interface

00:04:56,560 --> 00:05:00,240
a picture of the user interface you see

00:04:58,960 --> 00:05:02,560
on the left side

00:05:00,240 --> 00:05:03,280
each of the key capabilities from

00:05:02,560 --> 00:05:05,440
notebooks

00:05:03,280 --> 00:05:06,560
pipelines experiments and so forth are

00:05:05,440 --> 00:05:08,639
available to you

00:05:06,560 --> 00:05:09,680
you also have a dashboard that shows up

00:05:08,639 --> 00:05:12,000
where you see

00:05:09,680 --> 00:05:13,759
recent assets you've worked with and

00:05:12,000 --> 00:05:16,880
shortcuts to common

00:05:13,759 --> 00:05:16,880
things that you might need to do

00:05:17,440 --> 00:05:21,600
in addition to the user interface the

00:05:19,759 --> 00:05:23,039
command line is another way to interact

00:05:21,600 --> 00:05:25,840
with kubeflow

00:05:23,039 --> 00:05:27,680
so they're actually two command lines

00:05:25,840 --> 00:05:31,360
that you might want to use

00:05:27,680 --> 00:05:33,759
so first there's the standard

00:05:31,360 --> 00:05:34,880
kubernetes command line that allows you

00:05:33,759 --> 00:05:37,759
to do things

00:05:34,880 --> 00:05:39,039
like check the status of a trading job

00:05:37,759 --> 00:05:40,560
and check the

00:05:39,039 --> 00:05:42,720
pods that are running maybe get some

00:05:40,560 --> 00:05:45,680
logs things of that nature

00:05:42,720 --> 00:05:46,720
you also have the kubeflow command line

00:05:45,680 --> 00:05:49,600
utility

00:05:46,720 --> 00:05:49,919
which allows you to take templates that

00:05:49,600 --> 00:05:53,840
are

00:05:49,919 --> 00:05:56,880
yaml files and apply them to customize

00:05:53,840 --> 00:05:56,880
your configuration

00:05:57,759 --> 00:06:01,759
finally there are apis and sdks so if

00:06:00,080 --> 00:06:03,759
you're going to build a model

00:06:01,759 --> 00:06:05,840
pipeline you're going to create a

00:06:03,759 --> 00:06:08,560
hyperparameter tuning job

00:06:05,840 --> 00:06:11,199
these capabilities are available through

00:06:08,560 --> 00:06:11,199
an sdk

00:06:11,919 --> 00:06:15,199
and the final thing i'm going to mention

00:06:13,759 --> 00:06:19,039
before i turn it over to step

00:06:15,199 --> 00:06:23,199
note is that

00:06:19,039 --> 00:06:25,360
ml code is one small part of the process

00:06:23,199 --> 00:06:27,280
what we're seeing here in this diagram

00:06:25,360 --> 00:06:28,720
is from a famous paper about hidden

00:06:27,280 --> 00:06:29,360
technical debt of machine learning

00:06:28,720 --> 00:06:32,000
systems

00:06:29,360 --> 00:06:33,919
and what i want to point out is that

00:06:32,000 --> 00:06:36,400
these are things that you need to do

00:06:33,919 --> 00:06:39,039
when your code goes into production

00:06:36,400 --> 00:06:39,759
it's inevitable you will need to think

00:06:39,039 --> 00:06:43,199
about

00:06:39,759 --> 00:06:46,240
monitoring logging testing

00:06:43,199 --> 00:06:48,400
managing your resources making sure that

00:06:46,240 --> 00:06:50,479
you have a reproducible environment so

00:06:48,400 --> 00:06:52,720
you'll have to build and manage that

00:06:50,479 --> 00:06:54,000
by working with the infrastructure that

00:06:52,720 --> 00:06:56,000
we're talking about today

00:06:54,000 --> 00:06:57,120
you're able to work on top of that and

00:06:56,000 --> 00:06:59,759
leverage it

00:06:57,120 --> 00:07:00,720
so that you could focus on the data

00:06:59,759 --> 00:07:02,720
science

00:07:00,720 --> 00:07:03,840
as well as putting together a process in

00:07:02,720 --> 00:07:06,880
a pipeline

00:07:03,840 --> 00:07:08,720
that matches the goals of your business

00:07:06,880 --> 00:07:11,360
or your project

00:07:08,720 --> 00:07:11,919
and you can leverage all the great work

00:07:11,360 --> 00:07:14,720
that's been

00:07:11,919 --> 00:07:16,880
happening in this project so with that

00:07:14,720 --> 00:07:18,160
i'm going to turn it over to stefan

00:07:16,880 --> 00:07:20,000
thank you carla for the great

00:07:18,160 --> 00:07:22,319
introduction to kubeflow

00:07:20,000 --> 00:07:24,240
now that you have a general idea of the

00:07:22,319 --> 00:07:25,919
architecture of qflo and all of the

00:07:24,240 --> 00:07:27,759
components that it provides

00:07:25,919 --> 00:07:30,000
you might wonder how you can actually

00:07:27,759 --> 00:07:32,479
use it to develop and then

00:07:30,000 --> 00:07:33,599
continually improve and validate machine

00:07:32,479 --> 00:07:36,960
learning models

00:07:33,599 --> 00:07:40,000
using jupyter notebooks kale

00:07:36,960 --> 00:07:42,000
kilflow pipelines and brock so this is

00:07:40,000 --> 00:07:45,039
exactly what we're going to do

00:07:42,000 --> 00:07:46,879
in this tutorial but first let's have

00:07:45,039 --> 00:07:48,639
a high level overview over the

00:07:46,879 --> 00:07:49,520
end-to-end workflow that we are going to

00:07:48,639 --> 00:07:53,199
implement

00:07:49,520 --> 00:07:55,360
so first that the end user you

00:07:53,199 --> 00:07:57,199
will start from a jupiter notebook so a

00:07:55,360 --> 00:07:59,680
local environment

00:07:57,199 --> 00:08:01,840
where you can develop your models or

00:07:59,680 --> 00:08:03,759
your training algorithms etc

00:08:01,840 --> 00:08:05,199
and then once you're done all you need

00:08:03,759 --> 00:08:09,039
to do is to

00:08:05,199 --> 00:08:11,360
use scale to annotate notebook cells

00:08:09,039 --> 00:08:12,319
to convert this notebook into a scalable

00:08:11,360 --> 00:08:15,440
pipeline

00:08:12,319 --> 00:08:18,560
and afterwards you can

00:08:15,440 --> 00:08:22,800
spin up a hyper parameter tuning job

00:08:18,560 --> 00:08:26,000
to run hundreds or thousands

00:08:22,800 --> 00:08:29,039
of parallel pipelines once

00:08:26,000 --> 00:08:33,279
that's done you can use again kale

00:08:29,039 --> 00:08:36,640
to select the best model out of this

00:08:33,279 --> 00:08:38,240
massive workload and then with a very

00:08:36,640 --> 00:08:41,680
convenient api

00:08:38,240 --> 00:08:45,040
serve this model for everyone to use

00:08:41,680 --> 00:08:48,080
each step of the way is tracked by mlmd

00:08:45,040 --> 00:08:51,360
so you can have an end-to-end lineage

00:08:48,080 --> 00:08:52,800
of the workflow also each step is backed

00:08:51,360 --> 00:08:55,680
by pvcs

00:08:52,800 --> 00:08:56,640
and rocks rock takes a snapshot of these

00:08:55,680 --> 00:08:58,959
pvcs

00:08:56,640 --> 00:09:01,519
to have a complete time machine of your

00:08:58,959 --> 00:09:01,519
workflow

00:09:02,480 --> 00:09:06,240
this is our agenda we'll start from

00:09:04,800 --> 00:09:09,600
deployment kf

00:09:06,240 --> 00:09:11,839
so our local our development environment

00:09:09,600 --> 00:09:12,800
and then you'll learn how to convert

00:09:11,839 --> 00:09:15,279
notebooks

00:09:12,800 --> 00:09:16,560
to pipelines and then how to scale up

00:09:15,279 --> 00:09:18,640
this workflow

00:09:16,560 --> 00:09:20,560
with the car tip and hyperparameter

00:09:18,640 --> 00:09:24,399
tuning and afterwards

00:09:20,560 --> 00:09:27,600
how to serve the best model so check out

00:09:24,399 --> 00:09:29,360
this url which will redirect you to a

00:09:27,600 --> 00:09:32,399
code lab where you can follow

00:09:29,360 --> 00:09:35,440
all of the steps of this tutorial

00:09:32,399 --> 00:09:35,440
at your own pace

00:09:35,680 --> 00:09:41,600
let's start by deploying mini kf mini kf

00:09:39,200 --> 00:09:42,640
is our own portable and opinionated

00:09:41,600 --> 00:09:45,760
kubeflow

00:09:42,640 --> 00:09:48,880
distribution so mini kf runs

00:09:45,760 --> 00:09:52,800
seamlessly on gcp or on your laptop or

00:09:48,880 --> 00:09:52,800
on any on-premises infrastructure

00:09:52,880 --> 00:09:57,680
mini kf is a single node vm is a single

00:09:56,160 --> 00:10:01,519
node kubernetes vm

00:09:57,680 --> 00:10:04,560
that runs um kuber that runs cube flow

00:10:01,519 --> 00:10:06,640
alongside rocks data management platform

00:10:04,560 --> 00:10:07,600
it is super easy to deploy and in just

00:10:06,640 --> 00:10:10,800
15 minutes

00:10:07,600 --> 00:10:11,440
you have everything ready to go let's

00:10:10,800 --> 00:10:14,480
see how

00:10:11,440 --> 00:10:17,760
easy it is to deploy mini kf on gcp

00:10:14,480 --> 00:10:20,720
so i'm in the console of my gcp

00:10:17,760 --> 00:10:22,079
project and what i need to do to create

00:10:20,720 --> 00:10:25,519
a new mini kf

00:10:22,079 --> 00:10:28,959
is to head over the marketplace

00:10:25,519 --> 00:10:33,279
so i'm clicking on marketplace

00:10:28,959 --> 00:10:38,160
and then search for mini kf

00:10:33,279 --> 00:10:38,160
here it is let's click

00:10:38,720 --> 00:10:41,760
and launch

00:10:43,279 --> 00:10:47,839
all i need to do is to provide a name

00:10:50,720 --> 00:10:56,880
and that's it after clicking deploy

00:10:55,279 --> 00:10:58,880
the deployment procedure will take

00:10:56,880 --> 00:11:01,200
roughly 15 minutes

00:10:58,880 --> 00:11:03,440
and you will be able to monitor all of

00:11:01,200 --> 00:11:07,200
the resources that we are deploying

00:11:03,440 --> 00:11:10,000
inside the virtual machine

00:11:07,200 --> 00:11:11,839
for now i'm just going to use a mini kf

00:11:10,000 --> 00:11:13,760
i have already deployed

00:11:11,839 --> 00:11:15,440
so once you start the deployment

00:11:13,760 --> 00:11:18,160
procedure you will see this page

00:11:15,440 --> 00:11:19,440
and once it's complete this url will be

00:11:18,160 --> 00:11:22,000
available to you

00:11:19,440 --> 00:11:23,760
so i'm copying the password of my newly

00:11:22,000 --> 00:11:27,040
generated user

00:11:23,760 --> 00:11:29,839
i'm clicking on this link and i will be

00:11:27,040 --> 00:11:29,839
redirected

00:11:30,160 --> 00:11:33,600
to my q flow home page

00:11:34,000 --> 00:11:38,480
so now that our mini kf is ready to use

00:11:36,640 --> 00:11:40,959
let's start with the fun part

00:11:38,480 --> 00:11:42,560
that is how to convert the notebook into

00:11:40,959 --> 00:11:44,480
a pipeline

00:11:42,560 --> 00:11:46,079
but first let's talk a little bit about

00:11:44,480 --> 00:11:48,160
google pipelines

00:11:46,079 --> 00:11:50,639
pipelines is the one of the most

00:11:48,160 --> 00:11:52,160
important components in the qfl platform

00:11:50,639 --> 00:11:53,920
this is because data science is

00:11:52,160 --> 00:11:56,320
inherently a pipeline process

00:11:53,920 --> 00:11:58,320
right you always go through the same

00:11:56,320 --> 00:12:00,639
repeated steps whether they be

00:11:58,320 --> 00:12:02,320
data processing data transformation

00:12:00,639 --> 00:12:03,519
model training and then serving and so

00:12:02,320 --> 00:12:05,200
on and so forth

00:12:03,519 --> 00:12:07,600
so in this tutorial we'll try to

00:12:05,200 --> 00:12:09,920
simplify as much as possible the

00:12:07,600 --> 00:12:10,959
deployment and creation of this kind of

00:12:09,920 --> 00:12:12,800
workflows

00:12:10,959 --> 00:12:14,880
and how to make them completely

00:12:12,800 --> 00:12:18,320
reproducible we'll do this

00:12:14,880 --> 00:12:20,079
with kale and rock now converting a

00:12:18,320 --> 00:12:20,959
notebook into a pipeline has several

00:12:20,079 --> 00:12:24,399
benefits

00:12:20,959 --> 00:12:26,959
first of all the notebook allows us to

00:12:24,399 --> 00:12:30,240
clearly define what is the structure

00:12:26,959 --> 00:12:31,839
of the resulting pipeline and since we

00:12:30,240 --> 00:12:35,040
have multiple cells

00:12:31,839 --> 00:12:36,320
we can easily parallelize and isolate

00:12:35,040 --> 00:12:38,720
them

00:12:36,320 --> 00:12:40,079
also once you have a pipeline you can

00:12:38,720 --> 00:12:42,320
apply data versioning

00:12:40,079 --> 00:12:43,920
and even different hardware requirements

00:12:42,320 --> 00:12:46,399
for running them like running

00:12:43,920 --> 00:12:47,279
the training step in gpu and data

00:12:46,399 --> 00:12:50,000
processing

00:12:47,279 --> 00:12:50,000
in a cpu

00:12:51,360 --> 00:12:55,519
let's look at how the workflow changes

00:12:54,160 --> 00:12:58,399
when you apply

00:12:55,519 --> 00:12:59,839
when you use klm rock so before that

00:12:58,399 --> 00:13:01,040
what you would need to do to create a

00:12:59,839 --> 00:13:02,959
kubeflow pipeline

00:13:01,040 --> 00:13:04,720
is to write your machine learning code

00:13:02,959 --> 00:13:08,240
maybe you test it locally

00:13:04,720 --> 00:13:11,760
and then you would need to write some

00:13:08,240 --> 00:13:13,519
specific dsl code to construct and

00:13:11,760 --> 00:13:16,560
define the pipeline

00:13:13,519 --> 00:13:18,000
and then build the docker images to

00:13:16,560 --> 00:13:20,399
package your code and all of your

00:13:18,000 --> 00:13:21,680
dependencies and then build and upload

00:13:20,399 --> 00:13:25,200
the pipeline

00:13:21,680 --> 00:13:27,040
once you run it if you bump into any bug

00:13:25,200 --> 00:13:28,959
or if you want to amend your code in any

00:13:27,040 --> 00:13:29,839
way you would need to go all the way

00:13:28,959 --> 00:13:32,160
back to building

00:13:29,839 --> 00:13:33,360
new docker images and then to redeploy

00:13:32,160 --> 00:13:36,639
the pipeline

00:13:33,360 --> 00:13:39,199
but now with kale and rock this workflow

00:13:36,639 --> 00:13:40,160
gets dramatically simplified because you

00:13:39,199 --> 00:13:43,440
just need

00:13:40,160 --> 00:13:44,639
to develop on your notebook tag your

00:13:43,440 --> 00:13:46,639
notebook cells

00:13:44,639 --> 00:13:48,720
with a very simple ui that you'll see

00:13:46,639 --> 00:13:50,639
later and then run the pipeline with the

00:13:48,720 --> 00:13:52,240
click of a button without any docker

00:13:50,639 --> 00:13:55,360
image building

00:13:52,240 --> 00:13:58,800
and you can imagine how this

00:13:55,360 --> 00:14:00,959
workflow dramatically improves um

00:13:58,800 --> 00:14:03,839
your your speed to production and

00:14:00,959 --> 00:14:03,839
iteration

00:14:04,720 --> 00:14:09,600
let's also talk about beta management

00:14:06,959 --> 00:14:13,760
and how rock integrates with kubeflow

00:14:09,600 --> 00:14:16,160
so this is a tfx paper from 2017

00:14:13,760 --> 00:14:17,040
that talks about a high-level component

00:14:16,160 --> 00:14:20,320
of view

00:14:17,040 --> 00:14:21,760
of a machine learning platform so we can

00:14:20,320 --> 00:14:24,160
see here

00:14:21,760 --> 00:14:26,480
where the tfx components the machine

00:14:24,160 --> 00:14:30,000
learning libraries fit in the

00:14:26,480 --> 00:14:32,240
overall platform and in our case this

00:14:30,000 --> 00:14:33,519
is where kubeflow comes in to provide

00:14:32,240 --> 00:14:36,720
this kind of components

00:14:33,519 --> 00:14:37,600
in a containerized way kubeflow also

00:14:36,720 --> 00:14:39,920
provides

00:14:37,600 --> 00:14:41,519
the integrated front-end to manage

00:14:39,920 --> 00:14:44,160
deploy and monitor

00:14:41,519 --> 00:14:45,199
all of the various applications all of

00:14:44,160 --> 00:14:49,120
this

00:14:45,199 --> 00:14:51,279
is orchestrated by kubernetes now

00:14:49,120 --> 00:14:52,959
when you write write a pipeline or a

00:14:51,279 --> 00:14:53,920
machine application on top of cube

00:14:52,959 --> 00:14:56,480
though

00:14:53,920 --> 00:14:57,279
you need to you need some storage so in

00:14:56,480 --> 00:14:59,519
general

00:14:57,279 --> 00:15:01,760
you would write a pipeline that is

00:14:59,519 --> 00:15:03,360
specific that interacts specifically

00:15:01,760 --> 00:15:06,399
with the

00:15:03,360 --> 00:15:09,440
vendor api based on the kind of storage

00:15:06,399 --> 00:15:13,279
technology that you're using so

00:15:09,440 --> 00:15:16,240
ericto comes in and provides a general

00:15:13,279 --> 00:15:18,480
purpose storage layer so that you can

00:15:16,240 --> 00:15:20,079
write pipelines that are not specific

00:15:18,480 --> 00:15:21,600
that don't have to interact with a

00:15:20,079 --> 00:15:24,000
specific storage api

00:15:21,600 --> 00:15:25,040
but can just access super fast local

00:15:24,000 --> 00:15:28,320
storage

00:15:25,040 --> 00:15:28,800
how do we do that well we've extended q

00:15:28,320 --> 00:15:33,360
flow

00:15:28,800 --> 00:15:36,320
to b data ware and specifically to use

00:15:33,360 --> 00:15:38,800
kubernetes primitives pvcs persistent

00:15:36,320 --> 00:15:41,519
volume claims in all of its applications

00:15:38,800 --> 00:15:43,279
we do this by integrating with the

00:15:41,519 --> 00:15:45,279
container storage interface

00:15:43,279 --> 00:15:46,560
and sitting on the side of the critical

00:15:45,279 --> 00:15:49,199
io path

00:15:46,560 --> 00:15:50,240
in this way you can read and write data

00:15:49,199 --> 00:15:53,920
super fast

00:15:50,240 --> 00:15:55,360
from local from local data from local

00:15:53,920 --> 00:15:58,720
volumes

00:15:55,360 --> 00:16:02,000
and in the meanwhile every rock

00:15:58,720 --> 00:16:04,320
can take snapshots of your volumes

00:16:02,000 --> 00:16:05,759
immutable snapshots and shared them

00:16:04,320 --> 00:16:09,040
across locations

00:16:05,759 --> 00:16:09,920
via an object store so you can reproduce

00:16:09,040 --> 00:16:13,440
your environment

00:16:09,920 --> 00:16:16,399
or share it with your colleagues

00:16:13,440 --> 00:16:18,399
okay so now we are ready to put our

00:16:16,399 --> 00:16:20,880
hands on a notebook and convert it to a

00:16:18,399 --> 00:16:23,440
pipeline

00:16:20,880 --> 00:16:24,959
this is kubeflow's central dashboard the

00:16:23,440 --> 00:16:26,160
place where all of the keyplus

00:16:24,959 --> 00:16:28,160
components

00:16:26,160 --> 00:16:30,079
come together and where i can navigate

00:16:28,160 --> 00:16:32,800
between them as you can see

00:16:30,079 --> 00:16:33,680
on my left i have notebooks volumes

00:16:32,800 --> 00:16:36,160
models

00:16:33,680 --> 00:16:37,120
snapshot pipelines and then experiments

00:16:36,160 --> 00:16:40,880
and runs

00:16:37,120 --> 00:16:43,360
where we group together objects coming

00:16:40,880 --> 00:16:44,800
from different applications that belong

00:16:43,360 --> 00:16:48,000
to the same place

00:16:44,800 --> 00:16:50,959
like pipelines experiments or

00:16:48,000 --> 00:16:52,000
hyper parameter tuning experiments as a

00:16:50,959 --> 00:16:53,920
data scientist

00:16:52,000 --> 00:16:56,240
the first thing i want to do is to

00:16:53,920 --> 00:16:57,839
create my own development environment

00:16:56,240 --> 00:17:00,240
so i'm going to create a new notebook

00:16:57,839 --> 00:17:00,240
server

00:17:03,519 --> 00:17:06,559
and it is a matter just of just a matter

00:17:06,160 --> 00:17:09,839
of

00:17:06,559 --> 00:17:10,480
um assigning a name and in a matter of

00:17:09,839 --> 00:17:13,760
seconds

00:17:10,480 --> 00:17:19,839
i'll have a fully uh full

00:17:13,760 --> 00:17:19,839
jupiter lab environment ready to use

00:17:45,520 --> 00:17:52,000
great so the first thing i want to do

00:17:48,799 --> 00:17:55,760
is to clone the cable repository to pull

00:17:52,000 --> 00:17:55,760
the example that we're going to run

00:17:58,000 --> 00:18:05,840
so git clone http

00:18:01,600 --> 00:18:05,840
github q flow

00:18:06,559 --> 00:18:10,960
kale kale

00:18:12,840 --> 00:18:15,840
great

00:18:16,240 --> 00:18:20,640
so i get into the kale folder the

00:18:18,799 --> 00:18:23,039
examples folder

00:18:20,640 --> 00:18:28,720
and then the open vaccine kaggle

00:18:23,039 --> 00:18:31,440
competition folder

00:18:28,720 --> 00:18:33,440
so we created this notebook to work on

00:18:31,440 --> 00:18:35,360
the open boxing kegel challenge

00:18:33,440 --> 00:18:37,120
as we wanted to tackle the real world

00:18:35,360 --> 00:18:39,919
problem the challenge

00:18:37,120 --> 00:18:42,000
is about trying to locate the weak spots

00:18:39,919 --> 00:18:45,280
of a messenger rna structure

00:18:42,000 --> 00:18:47,440
to help create a stable vaccine

00:18:45,280 --> 00:18:48,720
this notebook contains a typical data

00:18:47,440 --> 00:18:52,559
science pipeline

00:18:48,720 --> 00:18:57,840
starting from data processing to

00:18:52,559 --> 00:18:57,840
model training and then evaluation

00:18:57,919 --> 00:19:01,120
i won't go too much into details of what

00:19:00,720 --> 00:19:02,960
this

00:19:01,120 --> 00:19:04,240
notebook is actually doing because we

00:19:02,960 --> 00:19:06,160
don't care right now

00:19:04,240 --> 00:19:08,400
but you will be able to play with it

00:19:06,160 --> 00:19:10,640
after the demo

00:19:08,400 --> 00:19:11,760
so the first thing i want to do is to

00:19:10,640 --> 00:19:15,280
verify that i

00:19:11,760 --> 00:19:18,480
have all my dependencies available

00:19:15,280 --> 00:19:19,919
so let's run the imports

00:19:18,480 --> 00:19:25,840
apparently i need to install some

00:19:19,919 --> 00:19:25,840
libraries so let's do just that

00:19:28,080 --> 00:19:31,520
notice how i am installing libraries

00:19:30,640 --> 00:19:36,559
here

00:19:31,520 --> 00:19:36,559
on the fly and you will see later

00:19:37,600 --> 00:19:40,480
what this means

00:19:40,640 --> 00:19:46,799
okay so now everything should be

00:19:43,760 --> 00:19:46,799
running smoothly

00:19:48,480 --> 00:19:54,000
now since i know my notebook is working

00:19:52,080 --> 00:19:55,200
i want to convert it to a pipeline with

00:19:54,000 --> 00:19:57,440
kale

00:19:55,200 --> 00:19:59,840
so i'll head over here on the left on

00:19:57,440 --> 00:20:02,080
the kale panel and enable it

00:19:59,840 --> 00:20:04,400
you can see a bunch of colors and badges

00:20:02,080 --> 00:20:06,640
pop up in the notebook

00:20:04,400 --> 00:20:09,200
this is scale showing you how the

00:20:06,640 --> 00:20:13,679
notebook has been annotated

00:20:09,200 --> 00:20:16,080
you can use kale's note annotations tool

00:20:13,679 --> 00:20:17,440
to actually change these annotations

00:20:16,080 --> 00:20:20,080
create new ones

00:20:17,440 --> 00:20:21,280
so each cell can be annotated with the

00:20:20,080 --> 00:20:24,000
name

00:20:21,280 --> 00:20:26,240
of a corresponding pipeline step and its

00:20:24,000 --> 00:20:28,720
dependencies

00:20:26,240 --> 00:20:30,240
for example here the processing data

00:20:28,720 --> 00:20:33,280
step depends

00:20:30,240 --> 00:20:34,960
on the load data step all the cells

00:20:33,280 --> 00:20:38,080
belonging

00:20:34,960 --> 00:20:40,480
having the same color will

00:20:38,080 --> 00:20:42,400
eventually belong to the same pipeline

00:20:40,480 --> 00:20:44,559
step

00:20:42,400 --> 00:20:46,840
that's all we need and if i click the

00:20:44,559 --> 00:20:49,840
compile and run button

00:20:46,840 --> 00:20:49,840
ko

00:20:50,559 --> 00:20:58,400
analyzes the notebook validates it

00:20:54,320 --> 00:21:01,120
and then starts to take a rock snapshot

00:20:58,400 --> 00:21:02,960
of the current environment to completely

00:21:01,120 --> 00:21:03,600
reproduce the environment we are

00:21:02,960 --> 00:21:07,200
developing

00:21:03,600 --> 00:21:10,000
on so since i

00:21:07,200 --> 00:21:11,679
just installed on the fly some python

00:21:10,000 --> 00:21:14,400
libraries

00:21:11,679 --> 00:21:16,080
my pipeline will run regardless

00:21:14,400 --> 00:21:19,280
seamlessly without having

00:21:16,080 --> 00:21:20,320
to build new docker images thanks to

00:21:19,280 --> 00:21:23,440
snapchat

00:21:20,320 --> 00:21:26,080
to rock snapshots

00:21:23,440 --> 00:21:27,840
then i can follow these deep links to

00:21:26,080 --> 00:21:32,240
see what is going on

00:21:27,840 --> 00:21:32,240
if i click here i'm redirected

00:21:34,000 --> 00:21:37,600
to the kubeflow pipelines run page where

00:21:37,039 --> 00:21:41,520
i can

00:21:37,600 --> 00:21:43,840
see the new run that starts

00:21:41,520 --> 00:21:45,039
since this run will take a few minutes

00:21:43,840 --> 00:21:48,000
to complete

00:21:45,039 --> 00:21:49,440
i will head over to an experiment i run

00:21:48,000 --> 00:21:53,280
previously

00:21:49,440 --> 00:21:56,000
and show you the resulting pipeline

00:21:53,280 --> 00:21:57,039
after a few minutes of computation as

00:21:56,000 --> 00:22:00,799
you can see

00:21:57,039 --> 00:22:04,480
we have a four step pipeline

00:22:00,799 --> 00:22:07,039
each step corresponds to an annotation

00:22:04,480 --> 00:22:09,440
in the in the original notebook so

00:22:07,039 --> 00:22:13,039
multiple cells have been packaged

00:22:09,440 --> 00:22:15,120
inside this step by kl which is running

00:22:13,039 --> 00:22:16,640
in this in the exact same original

00:22:15,120 --> 00:22:19,440
environment with all of your

00:22:16,640 --> 00:22:19,440
dependencies

00:22:20,080 --> 00:22:24,320
also k creates machine learning metadata

00:22:23,200 --> 00:22:27,360
execution

00:22:24,320 --> 00:22:27,919
for each pipeline step this allows us to

00:22:27,360 --> 00:22:29,840
track

00:22:27,919 --> 00:22:32,480
and link other entities that either

00:22:29,840 --> 00:22:35,919
belong or relate to the step itself

00:22:32,480 --> 00:22:37,919
some examples are the parent kfp run

00:22:35,919 --> 00:22:40,000
and artifacts that are consumed and

00:22:37,919 --> 00:22:43,280
produced by the step

00:22:40,000 --> 00:22:46,640
we can see them by clicking here

00:22:43,280 --> 00:22:50,320
and navigating so as you can see this

00:22:46,640 --> 00:22:51,919
is an mlmd execution that was created by

00:22:50,320 --> 00:22:54,159
kale

00:22:51,919 --> 00:22:56,480
i can click here to go to the specific

00:22:54,159 --> 00:22:56,480
page

00:22:56,559 --> 00:23:02,880
and notice for example how the run id

00:22:59,600 --> 00:23:06,880
so the kfprl id the parent

00:23:02,880 --> 00:23:09,440
of this step execution is clickable

00:23:06,880 --> 00:23:11,039
this is because we are standardizing on

00:23:09,440 --> 00:23:13,600
using global uris

00:23:11,039 --> 00:23:14,880
to reference and link resources across

00:23:13,600 --> 00:23:17,280
qfl applications

00:23:14,880 --> 00:23:18,159
and we are extending the queue flow uis

00:23:17,280 --> 00:23:21,120
to interpret

00:23:18,159 --> 00:23:23,039
these unique identifiers irrespectively

00:23:21,120 --> 00:23:26,320
to where the uis actually live

00:23:23,039 --> 00:23:36,159
so clicking here will redirect me

00:23:26,320 --> 00:23:38,320
to the original run page

00:23:36,159 --> 00:23:41,039
likewise going back to the execution

00:23:38,320 --> 00:23:46,880
page i can scroll down

00:23:41,039 --> 00:23:50,000
and see that we took a rock snapshot

00:23:46,880 --> 00:23:54,000
at the end of this step so

00:23:50,000 --> 00:23:56,559
we are linking with kale this snapshot

00:23:54,000 --> 00:24:00,000
to the step execution and we can even

00:23:56,559 --> 00:24:00,000
navigate to the rock ui

00:24:00,640 --> 00:24:04,720
now that we have converted the notebook

00:24:03,120 --> 00:24:07,120
into a single round pipeline

00:24:04,720 --> 00:24:10,320
we want to scale this up and optimize

00:24:07,120 --> 00:24:13,520
our model with hyper-parameter tuning

00:24:10,320 --> 00:24:15,039
so how do we do that well we could start

00:24:13,520 --> 00:24:17,520
tinkering manually

00:24:15,039 --> 00:24:18,240
with the parameters in the notebook and

00:24:17,520 --> 00:24:21,360
run

00:24:18,240 --> 00:24:23,360
manually multiple pipelines like

00:24:21,360 --> 00:24:24,400
just like we did now and then compare

00:24:23,360 --> 00:24:27,520
the metrics and

00:24:24,400 --> 00:24:29,919
choose the best model or we could use

00:24:27,520 --> 00:24:33,279
catib to automate this process

00:24:29,919 --> 00:24:36,320
so katib is the official q-flow

00:24:33,279 --> 00:24:38,960
hyper-parameter tuner uh it supports

00:24:36,320 --> 00:24:40,159
several machine learning frameworks from

00:24:38,960 --> 00:24:43,360
tensorflow

00:24:40,159 --> 00:24:44,320
mxnet by torch and others it is very

00:24:43,360 --> 00:24:46,559
flexible

00:24:44,320 --> 00:24:48,159
and we've integrated it with kail to run

00:24:46,559 --> 00:24:51,520
it from a notebook

00:24:48,159 --> 00:24:52,960
so what we can do is we can go back to

00:24:51,520 --> 00:24:56,080
the notebook

00:24:52,960 --> 00:24:56,960
configure inputs and outputs in a

00:24:56,080 --> 00:24:59,440
pipeline

00:24:56,960 --> 00:25:00,640
so we can create a pipeline that accepts

00:24:59,440 --> 00:25:03,760
input parameters

00:25:00,640 --> 00:25:07,039
and produces metrics that katip can use

00:25:03,760 --> 00:25:10,400
to optimize over the resulting pipelines

00:25:07,039 --> 00:25:13,120
and then still from the notebook select

00:25:10,400 --> 00:25:14,799
the input hyperparameter tuning space

00:25:13,120 --> 00:25:18,000
the search algorithms

00:25:14,799 --> 00:25:19,840
and the goal afterwards

00:25:18,000 --> 00:25:21,679
just with the click of a button directly

00:25:19,840 --> 00:25:22,320
from the notebook we can create and

00:25:21,679 --> 00:25:25,279
submit

00:25:22,320 --> 00:25:25,279
the new cutie job

00:25:25,600 --> 00:25:32,320
let's see how we can do that

00:25:29,120 --> 00:25:34,880
we are back to our original notebook

00:25:32,320 --> 00:25:36,080
so the previous pipeline completed

00:25:34,880 --> 00:25:39,120
successfully

00:25:36,080 --> 00:25:40,880
and now we want to optimize it with

00:25:39,120 --> 00:25:43,600
hyper parameter tuning

00:25:40,880 --> 00:25:44,799
as we said before we need two things a

00:25:43,600 --> 00:25:49,279
pipeline with

00:25:44,799 --> 00:25:52,480
inputs and outputs in q flow pipelines

00:25:49,279 --> 00:25:53,360
it's possible to create pipelines that

00:25:52,480 --> 00:25:56,159
have

00:25:53,360 --> 00:25:56,799
input parameters so that can vary

00:25:56,159 --> 00:26:01,440
between

00:25:56,799 --> 00:26:04,000
pipeline runs and output metrics

00:26:01,440 --> 00:26:04,720
with scale creating such a pipeline is

00:26:04,000 --> 00:26:07,039
very easy

00:26:04,720 --> 00:26:07,919
because you just need to create a

00:26:07,039 --> 00:26:11,120
notebook cell

00:26:07,919 --> 00:26:13,120
with some variables assignment and then

00:26:11,120 --> 00:26:14,240
annotate it with the pipeline parameters

00:26:13,120 --> 00:26:16,320
annotation

00:26:14,240 --> 00:26:17,840
and with this kale will make sure that

00:26:16,320 --> 00:26:19,200
the resulting pipeline will be

00:26:17,840 --> 00:26:22,159
parameterized

00:26:19,200 --> 00:26:22,159
with these values

00:26:22,400 --> 00:26:28,880
and then to create a pipeline metric

00:26:25,600 --> 00:26:31,520
all i need to do is to select

00:26:28,880 --> 00:26:33,679
to choose which variable i want to

00:26:31,520 --> 00:26:34,320
basically print to output from my

00:26:33,679 --> 00:26:36,799
pipeline

00:26:34,320 --> 00:26:39,200
in this case i'll i'm choosing the

00:26:36,799 --> 00:26:42,880
validation loss here produced by

00:26:39,200 --> 00:26:45,919
my training procedure and then

00:26:42,880 --> 00:26:49,520
on the bottom of the notebook

00:26:45,919 --> 00:26:52,720
i can just print

00:26:49,520 --> 00:26:55,600
the validation loss and

00:26:52,720 --> 00:26:57,360
annotated this cell with the pipeline

00:26:55,600 --> 00:26:58,720
metrics annotation

00:26:57,360 --> 00:27:00,640
and this will make sure that the

00:26:58,720 --> 00:27:03,679
pipeline will output

00:27:00,640 --> 00:27:05,840
a qft pipeline metric

00:27:03,679 --> 00:27:08,320
now if i want to start the hyper

00:27:05,840 --> 00:27:11,200
parameter tuning job to spin up

00:27:08,320 --> 00:27:13,200
hundreds of pipelines all i need to do

00:27:11,200 --> 00:27:16,320
is to enable this toggle

00:27:13,200 --> 00:27:20,240
and open up the scatter dialog

00:27:16,320 --> 00:27:22,880
that k provides you can notice how

00:27:20,240 --> 00:27:24,480
kale already recognized all of the

00:27:22,880 --> 00:27:28,320
variables that we've tagged

00:27:24,480 --> 00:27:31,679
with the pipeline parameters annotation

00:27:28,320 --> 00:27:32,960
we have already pre-filled this dialog

00:27:31,679 --> 00:27:35,360
before

00:27:32,960 --> 00:27:36,159
but you could choose between ranges and

00:27:35,360 --> 00:27:39,279
lists

00:27:36,159 --> 00:27:41,919
however you wanted in all of the input

00:27:39,279 --> 00:27:41,919
parameters

00:27:42,720 --> 00:27:46,399
also you can choose between various

00:27:44,880 --> 00:27:48,640
search algorithms

00:27:46,399 --> 00:27:50,159
and then the search objective that in

00:27:48,640 --> 00:27:53,520
this case it's just

00:27:50,159 --> 00:27:56,799
the validation loss we chose before

00:27:53,520 --> 00:27:56,799
and we want to minimize it

00:27:57,520 --> 00:28:02,480
so just like that we've defined a

00:28:00,000 --> 00:28:04,960
hyperparameter tuning job and by

00:28:02,480 --> 00:28:05,600
clicking the compile and run button kale

00:28:04,960 --> 00:28:08,640
will

00:28:05,600 --> 00:28:10,960
basically again validate the notebook

00:28:08,640 --> 00:28:12,159
convert it and build it into a queue

00:28:10,960 --> 00:28:14,960
flow pipeline

00:28:12,159 --> 00:28:16,080
rog is taking now a snapshot of the

00:28:14,960 --> 00:28:19,360
current environment

00:28:16,080 --> 00:28:20,559
to reproduce the current state in the

00:28:19,360 --> 00:28:24,240
pipelines

00:28:20,559 --> 00:28:27,760
and then kale also creates and submits

00:28:24,240 --> 00:28:31,039
a new cartibate experiment where each

00:28:27,760 --> 00:28:34,399
cutting trial will correspond to

00:28:31,039 --> 00:28:37,679
a pipeline run also

00:28:34,399 --> 00:28:40,559
we can see here a live view over

00:28:37,679 --> 00:28:42,080
the current state of the katib

00:28:40,559 --> 00:28:46,320
experiment

00:28:42,080 --> 00:28:50,000
so as soon as new trials and new rounds

00:28:46,320 --> 00:28:55,279
pop up and then complete you can

00:28:50,000 --> 00:28:55,279
monitor here directly from the notebook

00:28:55,440 --> 00:29:02,080
now i can also click in this link and

00:28:58,480 --> 00:29:02,080
navigate to the katib ui

00:29:06,640 --> 00:29:11,200
this is a catibui we have built from

00:29:09,840 --> 00:29:14,080
scratch

00:29:11,200 --> 00:29:15,440
following the design patterns that you

00:29:14,080 --> 00:29:18,720
may find already

00:29:15,440 --> 00:29:21,360
in other kubeflow applications

00:29:18,720 --> 00:29:22,320
we've we've improved over uh the

00:29:21,360 --> 00:29:25,520
existing

00:29:22,320 --> 00:29:28,240
catbuy to show much more

00:29:25,520 --> 00:29:29,440
and much more detailed information over

00:29:28,240 --> 00:29:34,000
the status

00:29:29,440 --> 00:29:36,640
and various details of the experiments

00:29:34,000 --> 00:29:37,840
since this experiment will take a lot of

00:29:36,640 --> 00:29:41,679
time to run

00:29:37,840 --> 00:29:44,240
let me show you something i run before

00:29:41,679 --> 00:29:45,279
so i'm going here on the left selecting

00:29:44,240 --> 00:29:48,320
experiments and then

00:29:45,279 --> 00:29:49,679
hp tuning to go to the home page of our

00:29:48,320 --> 00:29:53,200
new catib ui

00:29:49,679 --> 00:29:56,080
as you can see i have my new

00:29:53,200 --> 00:29:59,200
experiment running and i can also see

00:29:56,080 --> 00:30:02,640
something around before with 50 trials

00:29:59,200 --> 00:30:05,919
and at a glance i can also see what was

00:30:02,640 --> 00:30:07,919
the best metric

00:30:05,919 --> 00:30:09,279
and the corresponding configure input

00:30:07,919 --> 00:30:12,720
configuration

00:30:09,279 --> 00:30:12,720
let's go in and see

00:30:13,440 --> 00:30:18,880
so our new ui also exposes

00:30:16,480 --> 00:30:20,559
the state of the entire experiment with

00:30:18,880 --> 00:30:23,520
this nice graph

00:30:20,559 --> 00:30:25,440
where you can see color coded all of the

00:30:23,520 --> 00:30:29,279
triads that have executed and

00:30:25,440 --> 00:30:31,200
their parameter configurations

00:30:29,279 --> 00:30:33,919
this plot is interactive so that you can

00:30:31,200 --> 00:30:37,200
also basically

00:30:33,919 --> 00:30:39,440
explore how the various parameter

00:30:37,200 --> 00:30:42,480
configurations

00:30:39,440 --> 00:30:42,480
behaved and

00:30:43,279 --> 00:30:53,840
how they they basically

00:30:48,480 --> 00:30:56,080
influence the output of the experiment

00:30:53,840 --> 00:30:57,200
you can see at a glance what was the

00:30:56,080 --> 00:31:00,960
best

00:30:57,200 --> 00:31:02,640
trial configuration what's the current

00:31:00,960 --> 00:31:05,679
state of the experiment

00:31:02,640 --> 00:31:08,559
and then a list of all the trials

00:31:05,679 --> 00:31:09,679
and when you hover on a row you see the

00:31:08,559 --> 00:31:12,960
specific

00:31:09,679 --> 00:31:17,279
trial and its configuration

00:31:12,960 --> 00:31:20,960
in the graph by scrolling down

00:31:17,279 --> 00:31:25,440
we can see at a glance also which one

00:31:20,960 --> 00:31:25,440
was the best trial this one highlighted

00:31:28,640 --> 00:31:35,840
here it is if i click on

00:31:33,039 --> 00:31:39,120
this pipeline icon here on the right i

00:31:35,840 --> 00:31:39,120
will be redirected

00:31:39,919 --> 00:31:44,720
to the corresponding q flow pipeline run

00:31:42,880 --> 00:31:47,440
this is because each trial

00:31:44,720 --> 00:31:48,399
corresponds to a specific pipeline run

00:31:47,440 --> 00:31:51,120
so we want

00:31:48,399 --> 00:31:52,159
you to be able to navigate between uis

00:31:51,120 --> 00:31:56,559
seamlessly

00:31:52,159 --> 00:31:56,559
and link together all of the entities

00:31:57,279 --> 00:32:04,799
if i click on config here i can i also

00:32:00,559 --> 00:32:08,080
have some new category related entries

00:32:04,799 --> 00:32:11,279
and navigate back to the original

00:32:08,080 --> 00:32:14,399
katib experiment so you

00:32:11,279 --> 00:32:16,720
always know where you are and how

00:32:14,399 --> 00:32:17,679
specific objects and entities across

00:32:16,720 --> 00:32:20,080
cube flow

00:32:17,679 --> 00:32:21,039
link together let's go back to the

00:32:20,080 --> 00:32:25,120
pipeline

00:32:21,039 --> 00:32:28,720
so this is the pipeline that performed

00:32:25,120 --> 00:32:31,760
best in my original experiment

00:32:28,720 --> 00:32:33,200
you can also see that there are these

00:32:31,760 --> 00:32:35,519
two icons

00:32:33,200 --> 00:32:36,559
this means that these two steps have

00:32:35,519 --> 00:32:38,559
been cached

00:32:36,559 --> 00:32:40,159
in fact when running a cutting

00:32:38,559 --> 00:32:43,679
experiment not all

00:32:40,159 --> 00:32:46,720
not all steps need to be rerun across

00:32:43,679 --> 00:32:49,039
across pipelines the first step in this

00:32:46,720 --> 00:32:50,240
pipeline that consumes the input

00:32:49,039 --> 00:32:52,799
parameters

00:32:50,240 --> 00:32:55,039
is model training so we actually don't

00:32:52,799 --> 00:32:55,679
need to reload and reprocess the same

00:32:55,039 --> 00:32:58,000
data

00:32:55,679 --> 00:32:59,039
over and over again so with rock and

00:32:58,000 --> 00:33:01,279
pvcs

00:32:59,039 --> 00:33:02,240
we're actually just skipping these

00:33:01,279 --> 00:33:05,679
executions

00:33:02,240 --> 00:33:08,720
and starting from here from

00:33:05,679 --> 00:33:09,440
from a process data snapshot and let's

00:33:08,720 --> 00:33:12,640
actually go

00:33:09,440 --> 00:33:16,720
see visually using

00:33:12,640 --> 00:33:19,760
mlmd since scale is logging

00:33:16,720 --> 00:33:23,039
input and output artifacts for each step

00:33:19,760 --> 00:33:25,200
we can go look here at the artifacts

00:33:23,039 --> 00:33:27,840
that the specific rock artifacts that

00:33:25,200 --> 00:33:31,279
are associated to this step

00:33:27,840 --> 00:33:34,320
so by clicking here i can navigate

00:33:31,279 --> 00:33:35,760
to the rock snapshot artifact saved into

00:33:34,320 --> 00:33:39,840
mlmd

00:33:35,760 --> 00:33:39,840
then to the lineage explorer

00:33:40,240 --> 00:33:44,960
and here this nice visualization allows

00:33:43,360 --> 00:33:48,240
me to understand

00:33:44,960 --> 00:33:49,600
that this rock snapshot artifact was

00:33:48,240 --> 00:33:53,279
produced

00:33:49,600 --> 00:33:56,320
by many many steps and consumed

00:33:53,279 --> 00:33:58,880
by many others this clearly means

00:33:56,320 --> 00:34:00,159
that all of these pipelines are using

00:33:58,880 --> 00:34:03,360
the same step

00:34:00,159 --> 00:34:05,519
and so they were being cached

00:34:03,360 --> 00:34:07,039
now that we've run a massive hyper

00:34:05,519 --> 00:34:09,919
parameter tuning job

00:34:07,039 --> 00:34:10,240
we want to take the best model and serve

00:34:09,919 --> 00:34:14,159
it

00:34:10,240 --> 00:34:16,159
with kf serving kf serving is kubeflow's

00:34:14,159 --> 00:34:18,960
component for serving models into

00:34:16,159 --> 00:34:21,520
production it is based on k native

00:34:18,960 --> 00:34:23,280
and it allows serverless inferencing on

00:34:21,520 --> 00:34:25,919
kubernetes

00:34:23,280 --> 00:34:27,440
so serving provides several abstractions

00:34:25,919 --> 00:34:28,320
on top of various machine learning

00:34:27,440 --> 00:34:31,119
frameworks

00:34:28,320 --> 00:34:33,119
and provides quite a few features like

00:34:31,119 --> 00:34:36,800
canary deployments and scale to zero

00:34:33,119 --> 00:34:38,879
and much more so what we want to do

00:34:36,800 --> 00:34:40,560
is to select the best trial of the

00:34:38,879 --> 00:34:43,359
previous cutting experiment

00:34:40,560 --> 00:34:44,159
and restore a notebook out of a snapshot

00:34:43,359 --> 00:34:47,760
of that

00:34:44,159 --> 00:34:51,119
of that pipeline so in this way

00:34:47,760 --> 00:34:53,919
we'll use rock to restore the notebook

00:34:51,119 --> 00:34:56,800
from the best trained model and have the

00:34:53,919 --> 00:34:58,800
model directly into notebook memory

00:34:56,800 --> 00:35:01,280
then we'll use a very convenient kale

00:34:58,800 --> 00:35:03,599
api to serve this model

00:35:01,280 --> 00:35:04,400
in general creating inference services

00:35:03,599 --> 00:35:07,760
is quite a bit

00:35:04,400 --> 00:35:10,800
tedious as you can as you need to

00:35:07,760 --> 00:35:13,040
submit new crs

00:35:10,800 --> 00:35:15,200
and maybe even build docker images if

00:35:13,040 --> 00:35:17,040
you're using pre-processing transformers

00:35:15,200 --> 00:35:19,280
you will see how this all becomes much

00:35:17,040 --> 00:35:22,320
much easier with kale

00:35:19,280 --> 00:35:22,320
let's see how it's done

00:35:22,400 --> 00:35:26,800
i'm back to the previous hyperparameter

00:35:24,800 --> 00:35:30,560
tuning experiment

00:35:26,800 --> 00:35:33,520
so i want to select the best trial

00:35:30,560 --> 00:35:35,040
and then get the best model out of it

00:35:33,520 --> 00:35:38,720
and serve it

00:35:35,040 --> 00:35:44,400
so to do that i will first navigate

00:35:38,720 --> 00:35:47,920
to the corresponding pipeline

00:35:44,400 --> 00:35:50,240
and choose

00:35:47,920 --> 00:35:54,640
the last step in the pipeline this is

00:35:50,240 --> 00:35:54,640
because i want to restore a notebook

00:35:55,200 --> 00:36:00,560
with the state after the model has been

00:35:58,160 --> 00:36:03,200
trained

00:36:00,560 --> 00:36:04,160
so i'm heading over to visualizations

00:36:03,200 --> 00:36:08,079
where

00:36:04,160 --> 00:36:09,839
kale has produced a bunch of artifacts

00:36:08,079 --> 00:36:12,800
so here i can see an artifact

00:36:09,839 --> 00:36:15,839
corresponding to the first snapshot

00:36:12,800 --> 00:36:18,960
taken before the step execution

00:36:15,839 --> 00:36:19,920
and then another artifact corresponding

00:36:18,960 --> 00:36:22,320
to

00:36:19,920 --> 00:36:23,760
a snapshot taken after the step

00:36:22,320 --> 00:36:26,800
execution

00:36:23,760 --> 00:36:27,920
let's take the first one so i'll open

00:36:26,800 --> 00:36:32,800
this link

00:36:27,920 --> 00:36:36,560
which will redirect me to the rock ui

00:36:32,800 --> 00:36:40,000
and specifically to this snapshot

00:36:36,560 --> 00:36:43,280
page i'll copy this link

00:36:40,000 --> 00:36:45,839
which i can take back

00:36:43,280 --> 00:36:48,079
to central dashboard i'll open up

00:36:45,839 --> 00:36:53,359
notebooks

00:36:48,079 --> 00:36:56,960
new server and i can copy here

00:36:53,359 --> 00:36:58,240
um this special url which will make sure

00:36:56,960 --> 00:37:01,280
that my notebook

00:36:58,240 --> 00:37:06,560
is restored from this snapshot

00:37:01,280 --> 00:37:06,560
let's call this actually let's call this

00:37:06,839 --> 00:37:09,839
serving

00:37:13,920 --> 00:37:17,440
now that my notebook is up when i click

00:37:16,400 --> 00:37:20,480
on connect

00:37:17,440 --> 00:37:24,320
something interesting happens so

00:37:20,480 --> 00:37:28,079
kale notices that we are restoring an

00:37:24,320 --> 00:37:28,800
a notebook from a snapshot so what it

00:37:28,079 --> 00:37:32,480
will do

00:37:28,800 --> 00:37:35,200
is it will automatically open up

00:37:32,480 --> 00:37:36,160
the original notebook and start

00:37:35,200 --> 00:37:39,599
restoring

00:37:36,160 --> 00:37:43,200
a marshaling data uh so that

00:37:39,599 --> 00:37:43,839
the current in memory state is exactly

00:37:43,200 --> 00:37:47,040
the one

00:37:43,839 --> 00:37:48,800
that i would have found um at that

00:37:47,040 --> 00:37:52,400
specific point in time

00:37:48,800 --> 00:37:55,839
in the in the pipeline execution

00:37:52,400 --> 00:37:58,960
this means that i will find my model

00:37:55,839 --> 00:37:59,760
in memory i will have in my python

00:37:58,960 --> 00:38:02,880
memory

00:37:59,760 --> 00:38:04,079
the best model trained out of the hyper

00:38:02,880 --> 00:38:07,760
parameter tuning

00:38:04,079 --> 00:38:10,880
experiment that's it

00:38:07,760 --> 00:38:14,320
kale has completely completed the

00:38:10,880 --> 00:38:14,320
resuming of the notebook

00:38:15,520 --> 00:38:22,240
i can create a new cell here and verify

00:38:19,520 --> 00:38:24,079
that model actually is here i haven't

00:38:22,240 --> 00:38:26,160
done anything i just created a new

00:38:24,079 --> 00:38:29,760
notebook out of a snapshot and here it

00:38:26,160 --> 00:38:29,760
is my model in memory

00:38:30,240 --> 00:38:35,599
okay so

00:38:33,440 --> 00:38:36,800
now i have my best model here in the

00:38:35,599 --> 00:38:40,480
notebook

00:38:36,800 --> 00:38:42,320
and i want to serve it so kale provides

00:38:40,480 --> 00:38:45,680
a very simple and convenient

00:38:42,320 --> 00:38:48,880
api to serve the model

00:38:45,680 --> 00:38:52,960
let's see so i can import kale

00:38:48,880 --> 00:38:52,960
dot common dot servo tills

00:38:53,040 --> 00:39:01,119
import serve and now i want to use this

00:38:58,839 --> 00:39:04,240
function

00:39:01,119 --> 00:39:05,119
to to serve my model and it's just as

00:39:04,240 --> 00:39:09,599
easy

00:39:05,119 --> 00:39:13,440
as saying serve model

00:39:09,599 --> 00:39:13,440
but then i also want to

00:39:13,520 --> 00:39:18,160
to pass some pre-processing function

00:39:16,000 --> 00:39:21,520
since kf serving supports

00:39:18,160 --> 00:39:23,520
both predictors and transformers we have

00:39:21,520 --> 00:39:24,800
a very handy function here in the

00:39:23,520 --> 00:39:28,000
notebook

00:39:24,800 --> 00:39:30,960
which pre-processes features for us so

00:39:28,000 --> 00:39:34,640
we can then pass just raw data

00:39:30,960 --> 00:39:38,240
to the corresponding model server let me

00:39:34,640 --> 00:39:41,440
go here and redefine

00:39:38,240 --> 00:39:44,640
the function and the tokenizer

00:39:41,440 --> 00:39:44,640
this function uses

00:39:48,640 --> 00:39:53,599
so i'm back down to the bottom of the

00:39:50,320 --> 00:39:56,480
notebook and i want to tell my api

00:39:53,599 --> 00:39:56,960
that the corresponding model server will

00:39:56,480 --> 00:40:02,160
have

00:39:56,960 --> 00:40:02,160
also to create a transformer

00:40:02,240 --> 00:40:12,160
which will need to execute these

00:40:07,040 --> 00:40:12,160
process features function and

00:40:13,119 --> 00:40:21,680
i will need to pass as well

00:40:17,040 --> 00:40:24,079
my tokenizer so that kale

00:40:21,680 --> 00:40:24,720
knows that it needs to package this

00:40:24,079 --> 00:40:28,160
object

00:40:24,720 --> 00:40:28,160
alongside this function

00:40:29,839 --> 00:40:33,520
so now that i'm running this we can see

00:40:32,560 --> 00:40:36,400
that ko

00:40:33,520 --> 00:40:38,160
recognizes the type of the model it

00:40:36,400 --> 00:40:41,359
dumps it

00:40:38,160 --> 00:40:44,800
it saves it in a specific format

00:40:41,359 --> 00:40:45,520
and after this it starts it will start

00:40:44,800 --> 00:40:49,119
taking

00:40:45,520 --> 00:40:53,839
a rock snapshot and afterwards creating

00:40:49,119 --> 00:40:53,839
a new inference service

00:41:18,839 --> 00:41:21,839
okay

00:41:34,880 --> 00:41:38,319
and now that the inference service is

00:41:36,720 --> 00:41:41,440
ready we can

00:41:38,319 --> 00:41:45,040
print this

00:41:41,440 --> 00:41:48,240
object to see where it is

00:41:45,040 --> 00:41:49,920
served and by clicking here we can

00:41:48,240 --> 00:41:54,480
navigate

00:41:49,920 --> 00:41:57,680
to our new models ui so we've built

00:41:54,480 --> 00:42:00,880
a brand new ui to expose

00:41:57,680 --> 00:42:01,599
the entire state of kf serving to

00:42:00,880 --> 00:42:04,079
monitor and

00:42:01,599 --> 00:42:05,839
expose all of the inference services

00:42:04,079 --> 00:42:09,280
that you may deploy

00:42:05,839 --> 00:42:13,520
as you can see here i have an overview

00:42:09,280 --> 00:42:15,520
over where this model is exposed

00:42:13,520 --> 00:42:16,960
the fact that i deployed both a

00:42:15,520 --> 00:42:20,079
predictor and

00:42:16,960 --> 00:42:24,000
a transformer and

00:42:20,079 --> 00:42:26,640
the entire state i can have

00:42:24,000 --> 00:42:27,680
more details here about the various

00:42:26,640 --> 00:42:30,319
resources

00:42:27,680 --> 00:42:32,319
the fact that we are using a pvc where

00:42:30,319 --> 00:42:35,760
it is mounted

00:42:32,319 --> 00:42:39,200
and then we can also have a look

00:42:35,760 --> 00:42:42,160
at the various metrics exposed by

00:42:39,200 --> 00:42:43,599
the specific pods that are running one

00:42:42,160 --> 00:42:47,440
for the predictor

00:42:43,599 --> 00:42:47,440
and one for the transformer

00:42:49,280 --> 00:42:56,839
these are live updating metrics

00:42:53,119 --> 00:42:59,839
that come from the underlying k native

00:42:56,839 --> 00:42:59,839
resources

00:43:00,240 --> 00:43:05,119
we can also see logs these are live logs

00:43:03,680 --> 00:43:07,440
coming from the predictor and the

00:43:05,119 --> 00:43:09,040
transformer and we'll see how this

00:43:07,440 --> 00:43:11,520
update once we

00:43:09,040 --> 00:43:13,839
send the prediction and also the yaml

00:43:11,520 --> 00:43:13,839
definition

00:43:14,400 --> 00:43:19,839
and to have an overview of all the

00:43:16,240 --> 00:43:22,480
models we have a nice home page

00:43:19,839 --> 00:43:23,839
where we can have a list we can see a

00:43:22,480 --> 00:43:26,000
list with a summary

00:43:23,839 --> 00:43:28,480
of all our of all our running model

00:43:26,000 --> 00:43:28,480
servers

00:43:29,280 --> 00:43:37,680
okay so let's go back to the notebook

00:43:33,280 --> 00:43:41,119
and actually send try to hit this model

00:43:37,680 --> 00:43:42,240
and get a prediction so i would want to

00:43:41,119 --> 00:43:44,480
create

00:43:42,240 --> 00:43:44,480
a

00:43:47,680 --> 00:43:51,680
a data some a data structure to to send

00:43:50,880 --> 00:43:55,040
to them

00:43:51,680 --> 00:43:57,520
to my model server for this

00:43:55,040 --> 00:43:59,040
i'm creating a dictionary with an

00:43:57,520 --> 00:44:02,720
instances key

00:43:59,040 --> 00:44:06,400
which is a standard way to send

00:44:02,720 --> 00:44:10,319
data to a tensorflow server

00:44:06,400 --> 00:44:13,839
and i'm going to pass our unprocessed

00:44:10,319 --> 00:44:17,200
ex public test data

00:44:13,839 --> 00:44:19,200
that's this is some this is data that

00:44:17,200 --> 00:44:21,040
we've defined before in the notebook

00:44:19,200 --> 00:44:22,560
and that was actually restored

00:44:21,040 --> 00:44:25,920
automatically

00:44:22,560 --> 00:44:28,960
by ko so this is our data

00:44:25,920 --> 00:44:32,400
something that we will want to

00:44:28,960 --> 00:44:36,560
to send and then

00:44:32,400 --> 00:44:39,280
to send a request to our model server

00:44:36,560 --> 00:44:40,960
it's just as easy as saying kf serving

00:44:39,280 --> 00:44:43,440
dot predict

00:44:40,960 --> 00:44:43,440
data

00:44:45,200 --> 00:44:52,640
actually it was kf server

00:44:48,960 --> 00:44:56,079
okay so sending a request

00:44:52,640 --> 00:44:58,720
and let's head here to the logs page

00:44:56,079 --> 00:44:59,760
and you can see live that our

00:44:58,720 --> 00:45:02,079
transformer

00:44:59,760 --> 00:45:03,599
is actually getting some input data and

00:45:02,079 --> 00:45:05,920
process data

00:45:03,599 --> 00:45:06,720
and then kale is doing some magic and

00:45:05,920 --> 00:45:10,640
then

00:45:06,720 --> 00:45:14,079
the data is processed

00:45:10,640 --> 00:45:14,560
all these happened without having to

00:45:14,079 --> 00:45:17,599
build

00:45:14,560 --> 00:45:18,079
any kind of docker image so kale was

00:45:17,599 --> 00:45:21,839
able

00:45:18,079 --> 00:45:24,720
to detect parse and package

00:45:21,839 --> 00:45:26,319
the input processing function and its

00:45:24,720 --> 00:45:30,839
related assets

00:45:26,319 --> 00:45:32,000
create a new transformer initialize it

00:45:30,839 --> 00:45:35,839
here

00:45:32,000 --> 00:45:39,680
then use it to actually

00:45:35,839 --> 00:45:47,680
process the raw data to be passed

00:45:39,680 --> 00:45:51,839
to the to the predictor

00:45:47,680 --> 00:45:55,280
and here i can print my predictions

00:45:51,839 --> 00:45:56,319
it should be quite a big response so

00:45:55,280 --> 00:45:59,599
let's give

00:45:56,319 --> 00:46:02,240
jupiter lab a bit of time to process it

00:45:59,599 --> 00:46:02,240
here it is

00:46:02,640 --> 00:46:08,160
that was the last part of our tutorial

00:46:05,760 --> 00:46:09,680
so now it's time to summarize what we've

00:46:08,160 --> 00:46:12,880
learned today

00:46:09,680 --> 00:46:13,520
so you've learned you have learned how

00:46:12,880 --> 00:46:16,960
to

00:46:13,520 --> 00:46:17,839
install and deploy mini kf in a super

00:46:16,960 --> 00:46:21,520
easy way

00:46:17,839 --> 00:46:22,319
and in no time and then how to use a

00:46:21,520 --> 00:46:25,119
notebook

00:46:22,319 --> 00:46:26,720
to build some machine learning model and

00:46:25,119 --> 00:46:29,520
then using ko

00:46:26,720 --> 00:46:31,040
annotate and deployed and converted to a

00:46:29,520 --> 00:46:34,720
scalable pipeline

00:46:31,040 --> 00:46:37,520
afterwards how to use scale to

00:46:34,720 --> 00:46:39,040
even scale more this workload using

00:46:37,520 --> 00:46:42,000
hyper parameter tuning

00:46:39,040 --> 00:46:44,319
and run tens or hundreds of pipelines

00:46:42,000 --> 00:46:47,760
using caching

00:46:44,319 --> 00:46:51,200
afterwards you've learned how using rock

00:46:47,760 --> 00:46:51,920
and the snapshots that rock takes to

00:46:51,200 --> 00:46:55,040
reproduce

00:46:51,920 --> 00:46:57,119
every step of the way you can restore

00:46:55,040 --> 00:46:58,079
a notebook from a pipeline to its

00:46:57,119 --> 00:47:00,319
previous state

00:46:58,079 --> 00:47:02,640
so in this case we took the best model

00:47:00,319 --> 00:47:03,680
trained out of the hyperparameter tuning

00:47:02,640 --> 00:47:06,160
job

00:47:03,680 --> 00:47:08,000
to find the model ready to use in the

00:47:06,160 --> 00:47:11,359
notebook's memory

00:47:08,000 --> 00:47:12,800
afterwards we used a convenient to use

00:47:11,359 --> 00:47:14,800
scale api

00:47:12,800 --> 00:47:18,880
to serve this model directly from the

00:47:14,800 --> 00:47:22,400
notebook to make it production ready

00:47:18,880 --> 00:47:23,599
all of this workflow was backed by mlmd

00:47:22,400 --> 00:47:26,640
so that you could have

00:47:23,599 --> 00:47:29,760
a complete lineage of the work

00:47:26,640 --> 00:47:33,280
so we've also seen some cool new uis

00:47:29,760 --> 00:47:36,800
like the cate bui and the models ui

00:47:33,280 --> 00:47:38,880
these uis are not open source yet

00:47:36,800 --> 00:47:39,839
but will work in the next weeks and

00:47:38,880 --> 00:47:41,520
months

00:47:39,839 --> 00:47:44,079
to make sure that they will find their

00:47:41,520 --> 00:47:46,800
way into upstream cube flow

00:47:44,079 --> 00:47:47,359
also note that this is a pre-recording

00:47:46,800 --> 00:47:49,599
and

00:47:47,359 --> 00:47:51,520
we are still heavily working on this

00:47:49,599 --> 00:47:52,079
workflow so by the time you see this

00:47:51,520 --> 00:47:54,400
video

00:47:52,079 --> 00:47:57,520
some of the features might change or

00:47:54,400 --> 00:47:57,520
might see improvements

00:47:57,599 --> 00:48:02,079
this is just a small sample of community

00:48:00,319 --> 00:48:04,559
contributions that we've done at

00:48:02,079 --> 00:48:05,359
richto we are investing a lot of time

00:48:04,559 --> 00:48:08,400
and effort

00:48:05,359 --> 00:48:09,839
into making kubeflow a great platform

00:48:08,400 --> 00:48:12,720
for machine learning

00:48:09,839 --> 00:48:13,599
so from jupiter manager to volume

00:48:12,720 --> 00:48:16,880
support

00:48:13,599 --> 00:48:20,240
mini kf and authorization manifest

00:48:16,880 --> 00:48:24,000
installations across the board

00:48:20,240 --> 00:48:27,119
now qflow is a

00:48:24,000 --> 00:48:27,520
big and vibrant community backed by many

00:48:27,119 --> 00:48:29,839
both

00:48:27,520 --> 00:48:31,839
large and small companies so if you feel

00:48:29,839 --> 00:48:35,200
like you want to join as a developer

00:48:31,839 --> 00:48:38,000
or as an end user here is a list of

00:48:35,200 --> 00:48:40,000
channels and public places that you can

00:48:38,000 --> 00:48:43,520
join

00:48:40,000 --> 00:48:46,319
and talk with us now

00:48:43,520 --> 00:48:47,359
um all all the new things that you've

00:48:46,319 --> 00:48:50,480
seen today

00:48:47,359 --> 00:48:51,359
are the the product of the joint effort

00:48:50,480 --> 00:48:54,000
of our

00:48:51,359 --> 00:48:56,000
internal team at a richto so i want to

00:48:54,000 --> 00:48:58,240
really thank all of my colleagues

00:48:56,000 --> 00:48:59,680
who have contributed tons and tons of

00:48:58,240 --> 00:49:03,040
work to this

00:48:59,680 --> 00:49:06,960
so from ilias dimitris kimonos apostolos

00:49:03,040 --> 00:49:09,200
constantinos and chris really thank you

00:49:06,960 --> 00:49:11,599
and thanks to you for sticking with us

00:49:09,200 --> 00:49:13,839
until the end of this tutorial

00:49:11,599 --> 00:49:17,839
so now we are ready to answer all of

00:49:13,839 --> 00:49:17,839

YouTube URL: https://www.youtube.com/watch?v=VDINH5WkBhA


