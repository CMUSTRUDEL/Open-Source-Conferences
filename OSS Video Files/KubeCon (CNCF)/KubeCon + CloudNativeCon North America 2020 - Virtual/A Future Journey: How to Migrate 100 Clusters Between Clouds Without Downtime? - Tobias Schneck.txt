Title: A Future Journey: How to Migrate 100 Clusters Between Clouds Without Downtime? - Tobias Schneck
Publication date: 2020-11-23
Playlist: KubeCon + CloudNativeCon North America 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon Europe 2021 Virtual from May 4–7, 2021. Learn more at https://kubecon.io. The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects. 

A Future Journey: How to Migrate 100 Clusters Between Clouds Without Downtime? - Tobias Schneck & Manuel Stößel, Kubermatic 

Have you ever thought about migrating your Kubernetes clusters to another cloud provider to save costs? Yes? We too! Join us on an interactive journey to discover the main challenges of live migration at scale of etcd’s, traffic routing and application workloads from one cloud to another. The talk will discuss the current state of the technical concept, known problems and insides of the already proven migration steps for stateless workload.  As part of the journey, we’ll see the differences between migrating one or one hundred clusters with productive workloads; What parts can be automated? What steps may need to be manual? Let’s see how an automated solution could look like in the future and what steps are missing. 

https://sched.co/ekDU
Captions: 
	00:00:01,280 --> 00:00:06,720
hello and welcome to our talk

00:00:03,679 --> 00:00:10,240
uh how to migrate 100 clusters between

00:00:06,720 --> 00:00:11,200
clouds without uh downtime my name is

00:00:10,240 --> 00:00:13,759
manuel struss

00:00:11,200 --> 00:00:15,599
i'm a systems architect and tech lead at

00:00:13,759 --> 00:00:18,000
guru matic

00:00:15,599 --> 00:00:19,119
we're doing a whole bunch of kubernetes

00:00:18,000 --> 00:00:22,320
and cloud native

00:00:19,119 --> 00:00:24,400
consulting and we're developing the

00:00:22,320 --> 00:00:24,720
kubernetes kubernetes platform as well

00:00:24,400 --> 00:00:28,480
as

00:00:24,720 --> 00:00:31,119
coupon our kubernetes management tools

00:00:28,480 --> 00:00:32,880
with me today is to be a schneck um head

00:00:31,119 --> 00:00:34,719
of professional services at problematic

00:00:32,880 --> 00:00:36,160
and he's gonna tell you a few words

00:00:34,719 --> 00:00:38,879
about himself now

00:00:36,160 --> 00:00:39,600
yes thanks manuel um yeah my name is

00:00:38,879 --> 00:00:41,760
tobias

00:00:39,600 --> 00:00:44,160
i'm already working two and a half years

00:00:41,760 --> 00:00:45,520
for lutze and mostly

00:00:44,160 --> 00:00:47,680
responsible for professional

00:00:45,520 --> 00:00:50,000
professional service where we discovered

00:00:47,680 --> 00:00:52,640
a way how to migrate clusters

00:00:50,000 --> 00:00:54,079
so uh this was a fancy idea and now we

00:00:52,640 --> 00:00:57,280
want to present you

00:00:54,079 --> 00:00:59,520
how far we came to the journey and um so

00:00:57,280 --> 00:01:02,559
heading back to manuel

00:00:59,520 --> 00:01:04,960
all right thanks um right

00:01:02,559 --> 00:01:07,840
why would you actually want to migrate

00:01:04,960 --> 00:01:09,360
uh classes between cloud providers

00:01:07,840 --> 00:01:11,439
and there are actually a couple of

00:01:09,360 --> 00:01:14,720
reasons for that um

00:01:11,439 --> 00:01:16,159
on the more businessy side of things um

00:01:14,720 --> 00:01:18,400
you might have better contract

00:01:16,159 --> 00:01:21,840
conditions at another cloud provider

00:01:18,400 --> 00:01:24,799
so you would be able to save cost um

00:01:21,840 --> 00:01:25,439
there could be the need to migrate data

00:01:24,799 --> 00:01:28,960
centers

00:01:25,439 --> 00:01:29,920
to a different hosting provider or cloud

00:01:28,960 --> 00:01:32,960
provider

00:01:29,920 --> 00:01:35,680
from a logistic point of view to

00:01:32,960 --> 00:01:38,320
like a legal point of view or you're

00:01:35,680 --> 00:01:40,479
driven by a multi-cloud strategy

00:01:38,320 --> 00:01:42,079
and you want to decrease your dependency

00:01:40,479 --> 00:01:44,399
on one

00:01:42,079 --> 00:01:45,600
single uh existing cloud provider and

00:01:44,399 --> 00:01:48,560
and expand

00:01:45,600 --> 00:01:50,399
out to other providers there are also

00:01:48,560 --> 00:01:53,439
some technical reasons for that

00:01:50,399 --> 00:01:55,200
um so again more logistical kind of

00:01:53,439 --> 00:01:57,200
reason might be that you

00:01:55,200 --> 00:01:58,880
have a location migration of a data

00:01:57,200 --> 00:02:01,280
center

00:01:58,880 --> 00:02:02,719
or you might want to migrate to another

00:02:01,280 --> 00:02:06,079
network segment

00:02:02,719 --> 00:02:09,520
for separation of concerns or

00:02:06,079 --> 00:02:12,560
other reasons um

00:02:09,520 --> 00:02:15,440
you might be um adapting

00:02:12,560 --> 00:02:16,560
um improvements in your on-prem and

00:02:15,440 --> 00:02:19,120
cloud environments

00:02:16,560 --> 00:02:20,640
um add a new provider that you want to

00:02:19,120 --> 00:02:24,800
use like new features

00:02:20,640 --> 00:02:26,959
new different infrastructure

00:02:24,800 --> 00:02:28,319
technologies you want to use at a new

00:02:26,959 --> 00:02:31,040
provider

00:02:28,319 --> 00:02:32,640
or you're bound to some constraints when

00:02:31,040 --> 00:02:35,680
it comes to data location

00:02:32,640 --> 00:02:37,680
uh of a certain um

00:02:35,680 --> 00:02:39,519
services you're running maybe on some

00:02:37,680 --> 00:02:42,080
cloud offered services

00:02:39,519 --> 00:02:43,440
for example where you run your machine

00:02:42,080 --> 00:02:44,560
learning and where you have your machine

00:02:43,440 --> 00:02:48,560
learning data

00:02:44,560 --> 00:02:51,840
or some gdpr compliance

00:02:48,560 --> 00:02:51,840
needs that you need to fulfill

00:02:53,040 --> 00:02:56,400
so what are the main challenging

00:02:55,280 --> 00:02:59,760
challenges around

00:02:56,400 --> 00:03:01,440
moving to another cloud provider

00:02:59,760 --> 00:03:02,879
kubernetes itself abstracts

00:03:01,440 --> 00:03:05,440
infrastructure but

00:03:02,879 --> 00:03:06,959
it does have several kind of

00:03:05,440 --> 00:03:09,280
dependencies nonetheless

00:03:06,959 --> 00:03:11,519
right um so it does consume

00:03:09,280 --> 00:03:13,120
infrastructure resources for example the

00:03:11,519 --> 00:03:14,400
virtual machines where the cluster

00:03:13,120 --> 00:03:18,000
itself runs

00:03:14,400 --> 00:03:20,560
on it uses and consumes the network

00:03:18,000 --> 00:03:22,480
provided right the ip address space

00:03:20,560 --> 00:03:24,799
routing and firewalling rules

00:03:22,480 --> 00:03:25,920
management of ingress and egress traffic

00:03:24,799 --> 00:03:28,799
and also

00:03:25,920 --> 00:03:31,200
dns as always and external storage

00:03:28,799 --> 00:03:31,200
systems

00:03:31,360 --> 00:03:35,120
then there are kubernetes components

00:03:33,599 --> 00:03:38,799
that are actually dependent on

00:03:35,120 --> 00:03:40,720
a certain cloud provider um in

00:03:38,799 --> 00:03:42,239
most of that is the cloud controller

00:03:40,720 --> 00:03:44,000
manager um that

00:03:42,239 --> 00:03:45,920
contains the node controller for

00:03:44,000 --> 00:03:48,239
updating kubernetes nodes the service

00:03:45,920 --> 00:03:50,560
controller which

00:03:48,239 --> 00:03:52,400
translates a service type load balancer

00:03:50,560 --> 00:03:54,159
within kubernetes to an actual cloud

00:03:52,400 --> 00:03:55,519
load balancer in the cloud provider's

00:03:54,159 --> 00:03:57,360
environment

00:03:55,519 --> 00:03:58,799
the route controller which is

00:03:57,360 --> 00:04:01,200
responsible for setting up network

00:03:58,799 --> 00:04:02,799
routes in the cloud provider's network

00:04:01,200 --> 00:04:04,400
but there are also things like storage

00:04:02,799 --> 00:04:07,280
classes that map

00:04:04,400 --> 00:04:08,400
to cloud provider specific storage

00:04:07,280 --> 00:04:11,040
offerings

00:04:08,400 --> 00:04:12,879
and sometimes the overlay network you

00:04:11,040 --> 00:04:16,480
use has some dependency on

00:04:12,879 --> 00:04:16,480
a cloud provider as well

00:04:16,720 --> 00:04:21,120
and just to remind ourselves a quick

00:04:20,239 --> 00:04:24,160
overview

00:04:21,120 --> 00:04:24,960
of the components of kubernetes the

00:04:24,160 --> 00:04:27,440
central one

00:04:24,960 --> 00:04:28,160
being the api server which kind of

00:04:27,440 --> 00:04:31,040
handles

00:04:28,160 --> 00:04:31,919
all the changes in state of the cluster

00:04:31,040 --> 00:04:34,400
itself

00:04:31,919 --> 00:04:35,120
and the worker nodes with the cubelet

00:04:34,400 --> 00:04:38,400
which

00:04:35,120 --> 00:04:40,160
runs the actual workload that that users

00:04:38,400 --> 00:04:41,840
run on top of that cluster

00:04:40,160 --> 00:04:43,680
and between the api server and the

00:04:41,840 --> 00:04:47,120
cubelet there has to be a two-way

00:04:43,680 --> 00:04:49,520
um communication happening um and

00:04:47,120 --> 00:04:51,440
um between the cubelets as well um i

00:04:49,520 --> 00:04:53,840
mean between the worker nodes as well

00:04:51,440 --> 00:04:56,720
and that is one main challenges where

00:04:53,840 --> 00:05:00,560
we're kind of solving today

00:04:56,720 --> 00:05:04,960
to to enable this ways of communication

00:05:00,560 --> 00:05:08,240
and that directly leads us

00:05:04,960 --> 00:05:09,120
to our actual dependencies when we

00:05:08,240 --> 00:05:11,360
migrate

00:05:09,120 --> 00:05:12,639
so for us the application workload has

00:05:11,360 --> 00:05:15,759
the highest priority

00:05:12,639 --> 00:05:17,120
but we need to ensure for that to be the

00:05:15,759 --> 00:05:18,800
case we need to ensure

00:05:17,120 --> 00:05:21,440
fundamental networking rules that

00:05:18,800 --> 00:05:23,440
kubernetes expect to be in place

00:05:21,440 --> 00:05:26,080
those rules are that all containers

00:05:23,440 --> 00:05:30,639
within a pod can communicate

00:05:26,080 --> 00:05:32,960
unimpededly on layer 4 so tcp udp

00:05:30,639 --> 00:05:35,600
all pods can communicate with all other

00:05:32,960 --> 00:05:37,680
pods within the cluster without netting

00:05:35,600 --> 00:05:39,120
and all nodes can communicate with all

00:05:37,680 --> 00:05:42,160
parts and vice versa

00:05:39,120 --> 00:05:43,440
also without netting and the final one

00:05:42,160 --> 00:05:46,639
the ip that the pod

00:05:43,440 --> 00:05:50,160
sees itself is the same ip that others

00:05:46,639 --> 00:05:51,039
see the part as and this is really

00:05:50,160 --> 00:05:53,600
important to keep

00:05:51,039 --> 00:05:55,759
up so that the actual networking between

00:05:53,600 --> 00:05:58,160
parts and applications works

00:05:55,759 --> 00:05:59,280
um we have to have some external

00:05:58,160 --> 00:06:01,759
dependencies

00:05:59,280 --> 00:06:03,840
that need to be reachable right like

00:06:01,759 --> 00:06:05,440
externally routed ips for

00:06:03,840 --> 00:06:06,880
load balancers and notepad services

00:06:05,440 --> 00:06:10,400
within the cluster

00:06:06,880 --> 00:06:13,600
and dns names need to be reachable

00:06:10,400 --> 00:06:16,000
resolvable right and storage

00:06:13,600 --> 00:06:18,000
um some applications might have to have

00:06:16,000 --> 00:06:20,400
state that needs to be migrated without

00:06:18,000 --> 00:06:23,919
data loss

00:06:20,400 --> 00:06:27,360
so when we look at that now um

00:06:23,919 --> 00:06:31,039
at a level of like scale

00:06:27,360 --> 00:06:32,960
of hundreds of clusters maybe we look at

00:06:31,039 --> 00:06:34,160
large organizations running a whole

00:06:32,960 --> 00:06:36,160
bunch of classes in

00:06:34,160 --> 00:06:37,440
different location for different

00:06:36,160 --> 00:06:40,880
organizational units

00:06:37,440 --> 00:06:44,400
in different time zones right and

00:06:40,880 --> 00:06:46,720
for those users cluster users

00:06:44,400 --> 00:06:48,000
the cluster itself is just the service

00:06:46,720 --> 00:06:50,720
that they consume

00:06:48,000 --> 00:06:52,560
right um and that means that the cluster

00:06:50,720 --> 00:06:54,560
connection and secrets so the actual

00:06:52,560 --> 00:06:56,240
interface that the user has to the

00:06:54,560 --> 00:06:58,319
cluster doesn't uh

00:06:56,240 --> 00:07:00,639
is not allowed to change right otherwise

00:06:58,319 --> 00:07:03,840
it would impede the actual service that

00:07:00,639 --> 00:07:03,840
those users consume

00:07:04,000 --> 00:07:07,919
um so how do we solve for that um the

00:07:07,039 --> 00:07:10,000
status quo is

00:07:07,919 --> 00:07:11,840
uh will have a multi-cloud setup with

00:07:10,000 --> 00:07:13,919
the kubernetes kubernetes platform

00:07:11,840 --> 00:07:15,360
our open source uh kubernetes management

00:07:13,919 --> 00:07:17,280
platform

00:07:15,360 --> 00:07:19,280
and it has a concept of a seed cluster

00:07:17,280 --> 00:07:21,199
that holds the containerized control

00:07:19,280 --> 00:07:23,120
plane of the user clusters the user

00:07:21,199 --> 00:07:24,880
clusters being the class kubernetes

00:07:23,120 --> 00:07:28,479
clusters that are managed by

00:07:24,880 --> 00:07:31,520
kkp the worker nodes itself

00:07:28,479 --> 00:07:33,440
are uh via the kubomatic machine

00:07:31,520 --> 00:07:36,639
controller which is a cluster api

00:07:33,440 --> 00:07:39,759
conform operator that

00:07:36,639 --> 00:07:43,840
translates a machine deployment object

00:07:39,759 --> 00:07:44,720
into actual machines vms on cloud

00:07:43,840 --> 00:07:47,440
providers

00:07:44,720 --> 00:07:49,520
and we'll use canal as our default

00:07:47,440 --> 00:07:51,919
overlay networking which is effectively

00:07:49,520 --> 00:07:53,599
flannel um the exlan overlay networking

00:07:51,919 --> 00:07:57,120
with a calico

00:07:53,599 --> 00:07:59,759
network policy plugin and the target is

00:07:57,120 --> 00:08:01,440
that we migrate the user and c cluster

00:07:59,759 --> 00:08:03,120
control planes and worker nodes to a

00:08:01,440 --> 00:08:04,639
different cloud provider

00:08:03,120 --> 00:08:06,960
uh we'll keep all the external

00:08:04,639 --> 00:08:09,039
clustering points stable that means the

00:08:06,960 --> 00:08:10,879
control plane kubernetes api server

00:08:09,039 --> 00:08:13,039
endpoints and the actual

00:08:10,879 --> 00:08:15,840
application endpoints being the dns and

00:08:13,039 --> 00:08:17,520
ingress routine

00:08:15,840 --> 00:08:19,440
out of scope for now is the storage

00:08:17,520 --> 00:08:20,879
replication um

00:08:19,440 --> 00:08:22,639
the assumption is that the actual

00:08:20,879 --> 00:08:23,440
application layer manages the storage

00:08:22,639 --> 00:08:25,520
replication

00:08:23,440 --> 00:08:26,560
like at cd which is a feature that we

00:08:25,520 --> 00:08:28,879
will use

00:08:26,560 --> 00:08:31,120
to migrate the user cluster control

00:08:28,879 --> 00:08:31,120
plane

00:08:31,280 --> 00:08:35,680
so how does it look um we have a

00:08:33,440 --> 00:08:36,560
kubomatic installation that has a seat

00:08:35,680 --> 00:08:38,719
cluster

00:08:36,560 --> 00:08:39,599
which is also just a kubernetes cluster

00:08:38,719 --> 00:08:42,240
running in

00:08:39,599 --> 00:08:43,599
the google cloud and it hosts a couple

00:08:42,240 --> 00:08:46,320
of user clusters

00:08:43,599 --> 00:08:48,320
and we'll have a look at the vsphere

00:08:46,320 --> 00:08:49,519
user cluster that runs worker nodes on a

00:08:48,320 --> 00:08:51,200
vsphere cluster

00:08:49,519 --> 00:08:53,040
and we want to move all of that over to

00:08:51,200 --> 00:08:55,279
aws because

00:08:53,040 --> 00:08:57,120
uh for whatever reason we want to run on

00:08:55,279 --> 00:08:59,519
aws

00:08:57,120 --> 00:09:00,399
um and just some recommended

00:08:59,519 --> 00:09:04,000
prerequisites

00:09:00,399 --> 00:09:05,360
uh uh doing that uh in production uh

00:09:04,000 --> 00:09:08,160
you'll have to announce a maintenance

00:09:05,360 --> 00:09:10,959
window and block cluster updates so that

00:09:08,160 --> 00:09:13,360
those doesn't don't interfere with the

00:09:10,959 --> 00:09:15,200
actual migration process

00:09:13,360 --> 00:09:17,279
we'll have to ensure that our backup and

00:09:15,200 --> 00:09:18,800
recovery procedure for the seat and user

00:09:17,279 --> 00:09:20,560
clusters but also for the application

00:09:18,800 --> 00:09:23,120
workloads

00:09:20,560 --> 00:09:23,839
works is tested and and proven to be

00:09:23,120 --> 00:09:27,120
working

00:09:23,839 --> 00:09:30,880
right um we'll should create

00:09:27,120 --> 00:09:33,120
a target cloud cluster as a reference in

00:09:30,880 --> 00:09:35,440
our case an aws cluster just so

00:09:33,120 --> 00:09:36,880
that we can uh copy and paste some stuff

00:09:35,440 --> 00:09:40,000
over

00:09:36,880 --> 00:09:42,320
and we'll have to ensure that we

00:09:40,000 --> 00:09:44,720
actually control the dns entries and be

00:09:42,320 --> 00:09:46,320
able to to switch over dns entries to

00:09:44,720 --> 00:09:48,320
the new cloud endpoints

00:09:46,320 --> 00:09:51,360
once we migrated the workload over to

00:09:48,320 --> 00:09:54,160
the new cloud provider

00:09:51,360 --> 00:09:55,519
yeah and now we'll look at the actual

00:09:54,160 --> 00:09:57,760
solution approach and

00:09:55,519 --> 00:10:00,320
toby is going to give us a quick demo on

00:09:57,760 --> 00:10:05,040
how that works

00:10:00,320 --> 00:10:08,800
okay down over to me segs manually um

00:10:05,040 --> 00:10:09,680
share my screen hopefully you can see it

00:10:08,800 --> 00:10:13,680
now

00:10:09,680 --> 00:10:16,959
um and yes so

00:10:13,680 --> 00:10:18,959
what is the solution approach um so

00:10:16,959 --> 00:10:20,079
first of all we want to migrate the user

00:10:18,959 --> 00:10:23,360
cluster workers

00:10:20,079 --> 00:10:26,560
so in this case um

00:10:23,360 --> 00:10:26,959
we we want to migrate it and we want to

00:10:26,560 --> 00:10:29,440
have

00:10:26,959 --> 00:10:30,480
uh new workers in the target cloud how

00:10:29,440 --> 00:10:32,399
we can reach that

00:10:30,480 --> 00:10:33,680
so we're using the so-called machine

00:10:32,399 --> 00:10:36,480
controller

00:10:33,680 --> 00:10:38,640
in kubernetes and keyboard and access

00:10:36,480 --> 00:10:40,720
controller can create

00:10:38,640 --> 00:10:42,000
workers based on crds what's called

00:10:40,720 --> 00:10:44,079
machine deployment

00:10:42,000 --> 00:10:45,519
so machine deployment is similar like

00:10:44,079 --> 00:10:48,240
the

00:10:45,519 --> 00:10:49,279
deployment of pots it's a deployment of

00:10:48,240 --> 00:10:52,320
machines

00:10:49,279 --> 00:10:53,839
and if we like change that machine

00:10:52,320 --> 00:10:55,760
controller and give them a new

00:10:53,839 --> 00:10:57,839
specification we can create the machines

00:10:55,760 --> 00:11:00,800
in the new cloud

00:10:57,839 --> 00:11:02,000
what is needed to ensure the traffic is

00:11:00,800 --> 00:11:06,160
reachable we need to

00:11:02,000 --> 00:11:08,320
somehow need a way how to communicate

00:11:06,160 --> 00:11:09,760
between ports and ports and nodes to

00:11:08,320 --> 00:11:12,480
nodes for that we

00:11:09,760 --> 00:11:13,200
create a vpn overlay by a daemon set and

00:11:12,480 --> 00:11:17,120
route and

00:11:13,200 --> 00:11:18,640
traffic of the cni uh in our case kennel

00:11:17,120 --> 00:11:20,320
through the vpn network

00:11:18,640 --> 00:11:22,560
and let's ensure that we have maybe

00:11:20,320 --> 00:11:25,440
different network segmentations

00:11:22,560 --> 00:11:27,600
but still can talk to each other if we

00:11:25,440 --> 00:11:29,040
have this client to client vpn traffic

00:11:27,600 --> 00:11:31,839
enabled

00:11:29,040 --> 00:11:33,360
and uh at least we should ensure the

00:11:31,839 --> 00:11:36,160
reachability that means

00:11:33,360 --> 00:11:36,480
that we try as long as possible to keep

00:11:36,160 --> 00:11:39,279
the

00:11:36,480 --> 00:11:39,680
increase endpoint stable to trans and

00:11:39,279 --> 00:11:43,519
then

00:11:39,680 --> 00:11:45,519
transfer the workload to the new cloud

00:11:43,519 --> 00:11:46,880
and after the workload is the new cloud

00:11:45,519 --> 00:11:50,079
we delete the

00:11:46,880 --> 00:11:53,200
connectivity okay how

00:11:50,079 --> 00:11:54,880
sus uh scan looks like so we have the c

00:11:53,200 --> 00:11:57,279
cluster what host in the containerize

00:11:54,880 --> 00:12:00,639
control plane here we have

00:11:57,279 --> 00:12:03,040
some um controllers and we have the

00:12:00,639 --> 00:12:03,680
kubernetes containerized control plane

00:12:03,040 --> 00:12:06,639
we have

00:12:03,680 --> 00:12:07,760
their default vpn server this vpn server

00:12:06,639 --> 00:12:11,040
is used for

00:12:07,760 --> 00:12:12,560
vpn traffic between this control plane

00:12:11,040 --> 00:12:14,800
and the workers attributic

00:12:12,560 --> 00:12:15,920
so that workers can connect with the

00:12:14,800 --> 00:12:18,160
control plane

00:12:15,920 --> 00:12:19,440
and your control plane can back uh

00:12:18,160 --> 00:12:22,000
tunnel the

00:12:19,440 --> 00:12:25,040
cube control locks and exec cores over

00:12:22,000 --> 00:12:25,040
this vpn tunnel

00:12:25,120 --> 00:12:28,560
also we have a machine controller which

00:12:26,800 --> 00:12:30,959
is configured to place on

00:12:28,560 --> 00:12:33,360
machines on the vsphere cloud and we

00:12:30,959 --> 00:12:36,079
have between the v3 workers we have an

00:12:33,360 --> 00:12:38,399
overlay what's based on kennel and we

00:12:36,079 --> 00:12:42,399
have a metal ap service what

00:12:38,399 --> 00:12:45,440
uh creates then the inbound

00:12:42,399 --> 00:12:47,600
traffic load balancer to the dedicated

00:12:45,440 --> 00:12:49,680
application ports

00:12:47,600 --> 00:12:51,760
first step how to migrate is now to

00:12:49,680 --> 00:12:54,399
deploy a vpn daemon said

00:12:51,760 --> 00:12:55,040
this vpn daemon set ensures that we have

00:12:54,399 --> 00:12:57,680
an

00:12:55,040 --> 00:12:58,720
uh vpn client at every worker node this

00:12:57,680 --> 00:13:00,959
opens in the

00:12:58,720 --> 00:13:04,800
working out the new interface and we

00:13:00,959 --> 00:13:08,160
route the traffic from the vpn interface

00:13:04,800 --> 00:13:11,839
through the vpn interface uh our kennel

00:13:08,160 --> 00:13:13,519
to our dedicated overlay so and for that

00:13:11,839 --> 00:13:15,680
we need to pause the cluster because

00:13:13,519 --> 00:13:17,279
our cluster is controlled by a cluster

00:13:15,680 --> 00:13:18,160
controller and this cluster controller

00:13:17,279 --> 00:13:21,279
would then

00:13:18,160 --> 00:13:24,560
reconcile uh the machine

00:13:21,279 --> 00:13:26,399
deployments and uh the vpn servers so to

00:13:24,560 --> 00:13:28,079
ensure that this does not happen we

00:13:26,399 --> 00:13:31,440
pause the cluster to make

00:13:28,079 --> 00:13:34,880
some patches there next step is um

00:13:31,440 --> 00:13:37,519
after we

00:13:34,880 --> 00:13:38,480
have the credentials for the new cloud

00:13:37,519 --> 00:13:41,120
uh adopted

00:13:38,480 --> 00:13:42,399
we can update the cluster spec and

00:13:41,120 --> 00:13:45,120
specificate

00:13:42,399 --> 00:13:46,880
the new aws cloud then the machine

00:13:45,120 --> 00:13:48,399
controller gets updated and we get a new

00:13:46,880 --> 00:13:51,920
machine controller instance with

00:13:48,399 --> 00:13:54,639
co and now talk to aws and the

00:13:51,920 --> 00:13:56,000
new nodes get created and also joins our

00:13:54,639 --> 00:13:58,880
vpn network

00:13:56,000 --> 00:14:00,399
and the join so also the kennel overlay

00:13:58,880 --> 00:14:02,800
routing

00:14:00,399 --> 00:14:05,199
the cloud controller now ensures that we

00:14:02,800 --> 00:14:08,160
also have a new aws lb

00:14:05,199 --> 00:14:09,680
what get created chronolysis llb is not

00:14:08,160 --> 00:14:12,320
routed but anyway

00:14:09,680 --> 00:14:14,800
traffic goes from here to over the metal

00:14:12,320 --> 00:14:17,839
ap service to the dedicated workload

00:14:14,800 --> 00:14:20,399
after this this happens we can

00:14:17,839 --> 00:14:21,279
remove the workload from the old uh

00:14:20,399 --> 00:14:23,360
workers

00:14:21,279 --> 00:14:24,399
and move the workload to the new

00:14:23,360 --> 00:14:27,360
applications

00:14:24,399 --> 00:14:27,680
after this is done we can also re-name

00:14:27,360 --> 00:14:31,680
the

00:14:27,680 --> 00:14:33,920
dns names to the new cloud load balancer

00:14:31,680 --> 00:14:37,120
and ensure that we have now may create

00:14:33,920 --> 00:14:39,760
traffic to the new cloud

00:14:37,120 --> 00:14:40,480
at least then clean up the old resources

00:14:39,760 --> 00:14:43,360
and we

00:14:40,480 --> 00:14:44,000
removed we not needed any more vpn

00:14:43,360 --> 00:14:46,880
overlay because

00:14:44,000 --> 00:14:48,480
this two workers can now talk to each

00:14:46,880 --> 00:14:51,920
other with eth

00:14:48,480 --> 00:14:55,199
interface and that's how we migrated it

00:14:51,920 --> 00:14:57,199
so to give you a short in a

00:14:55,199 --> 00:14:58,560
short look what already is happening we

00:14:57,199 --> 00:15:01,360
create some demo

00:14:58,560 --> 00:15:02,079
be aware that uh currently this project

00:15:01,360 --> 00:15:04,160
is like not

00:15:02,079 --> 00:15:05,519
fully finished so it's more a proof of

00:15:04,160 --> 00:15:08,880
concept state

00:15:05,519 --> 00:15:11,440
so as we see here we have here

00:15:08,880 --> 00:15:12,160
a app or what is our reference echo

00:15:11,440 --> 00:15:14,000
service

00:15:12,160 --> 00:15:15,680
and this is deployed on the cluster

00:15:14,000 --> 00:15:18,240
let's take a look in our

00:15:15,680 --> 00:15:19,279
chiromatic control plane and in this

00:15:18,240 --> 00:15:22,560
control plane

00:15:19,279 --> 00:15:26,720
we have um the dedicated

00:15:22,560 --> 00:15:28,480
clusters here so first we have here this

00:15:26,720 --> 00:15:31,680
cube called migrate cluster what we want

00:15:28,480 --> 00:15:35,120
to migrate here we see

00:15:31,680 --> 00:15:37,199
this cluster contains the containerized

00:15:35,120 --> 00:15:38,399
control planes a machine deployment of

00:15:37,199 --> 00:15:41,680
two nodes

00:15:38,399 --> 00:15:44,959
and a so-called echo service engine x

00:15:41,680 --> 00:15:48,880
and uh metal ap this metal ap points to

00:15:44,959 --> 00:15:51,920
the peer queries of the vsphere and

00:15:48,880 --> 00:15:54,880
deploys our echo services so

00:15:51,920 --> 00:15:57,839
if we go to the vsphere we see here

00:15:54,880 --> 00:16:03,040
under the cluster id

00:15:57,839 --> 00:16:03,040
what is here kbhc

00:16:03,680 --> 00:16:10,560
we see here the little machines running

00:16:07,519 --> 00:16:13,600
and this one we want now to move to

00:16:10,560 --> 00:16:16,800
aws first step

00:16:13,600 --> 00:16:20,800
what we already did is that we

00:16:16,800 --> 00:16:21,600
deployed a vpn so we patched our vpn

00:16:20,800 --> 00:16:23,920
server

00:16:21,600 --> 00:16:24,880
and for that we see that we have a

00:16:23,920 --> 00:16:28,240
cluster spec

00:16:24,880 --> 00:16:30,000
and we have in so we see

00:16:28,240 --> 00:16:31,759
here also in our c cluster that

00:16:30,000 --> 00:16:34,399
everything what you see in cubic is also

00:16:31,759 --> 00:16:37,440
represented as a cluster crt

00:16:34,399 --> 00:16:40,240
in the in the cluster namespace

00:16:37,440 --> 00:16:41,199
here sysnamespace we see the control

00:16:40,240 --> 00:16:44,160
planes and

00:16:41,199 --> 00:16:45,040
here you see that we have like the vpn

00:16:44,160 --> 00:16:48,399
server

00:16:45,040 --> 00:16:50,240
running and we have api server and all

00:16:48,399 --> 00:16:52,800
other components running

00:16:50,240 --> 00:16:53,440
if we connect here to the user cluster

00:16:52,800 --> 00:16:56,320
so

00:16:53,440 --> 00:16:58,480
let's take this shortcut yes i want to

00:16:56,320 --> 00:17:01,759
go to the cubicle cluster

00:16:58,480 --> 00:17:04,480
um we see a bunch of containers here

00:17:01,759 --> 00:17:05,120
and we see here that we have our echo

00:17:04,480 --> 00:17:07,600
service

00:17:05,120 --> 00:17:10,959
we have our ancient x we have our kennel

00:17:07,600 --> 00:17:13,679
we have our vpn client

00:17:10,959 --> 00:17:14,720
and that's now we want what we want to

00:17:13,679 --> 00:17:18,079
migrate

00:17:14,720 --> 00:17:21,280
so yeah first step is the vpn

00:17:18,079 --> 00:17:24,000
is already there so we can now deploy um

00:17:21,280 --> 00:17:25,679
our control plane and migrate it what is

00:17:24,000 --> 00:17:26,559
i think the most interesting part of the

00:17:25,679 --> 00:17:29,760
whole thing

00:17:26,559 --> 00:17:32,160
so we can then say okay here our

00:17:29,760 --> 00:17:32,880
update target cloud script what does it

00:17:32,160 --> 00:17:36,880
do

00:17:32,880 --> 00:17:39,360
so we have here um yeah we first we need

00:17:36,880 --> 00:17:39,360
somehow

00:17:40,480 --> 00:17:46,880
cluster id and

00:17:44,000 --> 00:17:47,360
we need a project id the project id we

00:17:46,880 --> 00:17:51,360
find

00:17:47,360 --> 00:17:53,840
here in our hubometic um

00:17:51,360 --> 00:17:55,120
url here to approach it actually we can

00:17:53,840 --> 00:17:58,320
copy it

00:17:55,120 --> 00:17:58,320
and we can here

00:17:58,559 --> 00:18:02,559
place it so what is the first step we

00:18:01,120 --> 00:18:05,360
create in backup

00:18:02,559 --> 00:18:06,559
that's the backup of the specification

00:18:05,360 --> 00:18:09,679
of this cluster

00:18:06,559 --> 00:18:13,840
so yeah i want to create it and

00:18:09,679 --> 00:18:17,200
as next step i want to pause the cluster

00:18:13,840 --> 00:18:19,600
and yes after i pause the cluster

00:18:17,200 --> 00:18:20,880
the controller does not care anymore so

00:18:19,600 --> 00:18:23,679
now i can safely

00:18:20,880 --> 00:18:25,280
update my cloud provider so yes i want

00:18:23,679 --> 00:18:27,280
to patch the cloud provider so

00:18:25,280 --> 00:18:28,480
i create a new cluster the ammo what we

00:18:27,280 --> 00:18:31,840
now can take

00:18:28,480 --> 00:18:35,520
a look on so if we go no to my

00:18:31,840 --> 00:18:40,559
files i see here uh that i have here

00:18:35,520 --> 00:18:40,559
under control plane somehow a new file

00:18:40,960 --> 00:18:44,240
uh where um is

00:18:45,120 --> 00:18:51,599
should be here let's see

00:18:48,550 --> 00:18:51,599
[Music]

00:18:54,840 --> 00:19:00,640
um okay

00:18:56,640 --> 00:19:04,559
um yeah here we are now we have

00:19:00,640 --> 00:19:07,280
um this one backup clusters yamo

00:19:04,559 --> 00:19:08,880
and uh the petchamu so let's see what's

00:19:07,280 --> 00:19:11,679
the difference is

00:19:08,880 --> 00:19:13,760
so yeah so currently that's our cluster

00:19:11,679 --> 00:19:14,720
crd on the left we have the backup

00:19:13,760 --> 00:19:18,080
cluster

00:19:14,720 --> 00:19:18,640
spec and we have here our api server

00:19:18,080 --> 00:19:20,559
token

00:19:18,640 --> 00:19:21,760
and so on and we have to finalize us

00:19:20,559 --> 00:19:24,240
what clean up

00:19:21,760 --> 00:19:24,880
our cloud when we delete the clusters

00:19:24,240 --> 00:19:27,840
and that's

00:19:24,880 --> 00:19:29,440
now the important stuff we have here the

00:19:27,840 --> 00:19:31,039
vsphere

00:19:29,440 --> 00:19:33,120
configuration what reference the

00:19:31,039 --> 00:19:35,360
credential and

00:19:33,120 --> 00:19:36,240
the folder and so on and that's

00:19:35,360 --> 00:19:39,440
something what

00:19:36,240 --> 00:19:43,039
we now remove as well as status fields

00:19:39,440 --> 00:19:44,240
and we apply this change now to our

00:19:43,039 --> 00:19:48,480
cluster

00:19:44,240 --> 00:19:51,520
the first step to remove the credentials

00:19:48,480 --> 00:19:52,000
this is needed so let's apply that and

00:19:51,520 --> 00:19:54,799
see what

00:19:52,000 --> 00:19:56,720
happens so we see now we have configured

00:19:54,799 --> 00:19:58,880
and now to start reconciling we

00:19:56,720 --> 00:20:01,039
need to compose the cluster that the

00:19:58,880 --> 00:20:02,159
cluster controller can take care about

00:20:01,039 --> 00:20:05,039
the change

00:20:02,159 --> 00:20:06,480
and yeah let's see what now is happening

00:20:05,039 --> 00:20:09,919
so we have now the

00:20:06,480 --> 00:20:12,559
cloud spec what get recreated by the

00:20:09,919 --> 00:20:15,440
cloud controller so hopefully everything

00:20:12,559 --> 00:20:17,280
works well and we see that the cluster

00:20:15,440 --> 00:20:19,360
can now reconcile

00:20:17,280 --> 00:20:20,880
yeah we see now we're getting the new

00:20:19,360 --> 00:20:23,679
object we have the

00:20:20,880 --> 00:20:25,280
vsphere is empty okay this looks good

00:20:23,679 --> 00:20:28,320
and we can now watch

00:20:25,280 --> 00:20:30,640
uh that hopefully yeah we see the api

00:20:28,320 --> 00:20:33,679
server is reconciling to

00:20:30,640 --> 00:20:36,320
now an empty cloud provider okay cool

00:20:33,679 --> 00:20:37,440
that was the first step so now as next

00:20:36,320 --> 00:20:40,400
step

00:20:37,440 --> 00:20:42,000
we want to change to to our aws so let's

00:20:40,400 --> 00:20:44,720
go out of this view

00:20:42,000 --> 00:20:45,679
and let's pause the cluster again like

00:20:44,720 --> 00:20:47,600
currently

00:20:45,679 --> 00:20:48,960
uh depending on the chromatic controller

00:20:47,600 --> 00:20:51,360
we need this two-step

00:20:48,960 --> 00:20:52,720
upgrades uh because we are just a cube

00:20:51,360 --> 00:20:55,360
control client and not

00:20:52,720 --> 00:20:56,480
operator and and now we can patch the

00:20:55,360 --> 00:20:59,840
cloud provider

00:20:56,480 --> 00:21:02,640
so what happens now um yes

00:20:59,840 --> 00:21:03,919
i want to patch it yes i'd want a new

00:21:02,640 --> 00:21:05,840
secret so there we

00:21:03,919 --> 00:21:07,039
create a new secret reference for the

00:21:05,840 --> 00:21:10,320
aws

00:21:07,039 --> 00:21:12,880
credentials and then we

00:21:10,320 --> 00:21:15,039
created a new cluster patch file again

00:21:12,880 --> 00:21:15,679
so let's also see here what's different

00:21:15,039 --> 00:21:18,799
now

00:21:15,679 --> 00:21:21,200
so we see now we have uh changed

00:21:18,799 --> 00:21:23,200
basically removed the finalizers because

00:21:21,200 --> 00:21:26,559
this are not valid anymore

00:21:23,200 --> 00:21:30,159
we added a annotation here

00:21:26,559 --> 00:21:30,880
to the chromatic avws region we patched

00:21:30,159 --> 00:21:34,159
cloud

00:21:30,880 --> 00:21:35,600
spec aws with the credentials and we

00:21:34,159 --> 00:21:38,080
have a migration

00:21:35,600 --> 00:21:39,760
a new data center so that's the new

00:21:38,080 --> 00:21:41,600
migration data center where we want to

00:21:39,760 --> 00:21:45,200
migrate it

00:21:41,600 --> 00:21:48,320
after we now make that pause to

00:21:45,200 --> 00:21:51,760
force uh the controller try to reconcile

00:21:48,320 --> 00:21:53,919
where uh new aws

00:21:51,760 --> 00:21:56,559
should place there and the nice thing is

00:21:53,919 --> 00:22:00,080
therefore cubematic creates now a new

00:21:56,559 --> 00:22:02,880
uh group a new

00:22:00,080 --> 00:22:04,240
roles and that's hopefully what's now

00:22:02,880 --> 00:22:08,080
happening

00:22:04,240 --> 00:22:11,440
so let's see so let's patch that cluster

00:22:08,080 --> 00:22:14,240
and um configured

00:22:11,440 --> 00:22:14,799
and yeah now let's on pause cluster and

00:22:14,240 --> 00:22:18,000
see

00:22:14,799 --> 00:22:22,080
what's happening happening good

00:22:18,000 --> 00:22:23,120
so okay so what we see now here we can

00:22:22,080 --> 00:22:26,960
reconcile

00:22:23,120 --> 00:22:29,280
you see we get now here also aws

00:22:26,960 --> 00:22:31,280
data back from the chromatic controller

00:22:29,280 --> 00:22:31,760
and see what we created new security

00:22:31,280 --> 00:22:34,799
group

00:22:31,760 --> 00:22:36,960
and the instance profile um

00:22:34,799 --> 00:22:38,320
cool so let's see what happens with my

00:22:36,960 --> 00:22:40,159
components

00:22:38,320 --> 00:22:42,559
we're now seeing yeah we have a

00:22:40,159 --> 00:22:44,480
restarting api server again

00:22:42,559 --> 00:22:46,240
uh it started now with the new club

00:22:44,480 --> 00:22:49,120
prevent credentials

00:22:46,240 --> 00:22:51,200
so let's try to find a little bit out

00:22:49,120 --> 00:22:54,480
what's happening here so we have

00:22:51,200 --> 00:22:57,760
here um deployment

00:22:54,480 --> 00:22:59,280
so and let's take a look into oh it's

00:22:57,760 --> 00:23:02,400
the wrong

00:22:59,280 --> 00:23:05,840
side let's see what we are

00:23:02,400 --> 00:23:09,840
have placed there so can you get

00:23:05,840 --> 00:23:09,840
explain deployment

00:23:10,880 --> 00:23:17,760
for sure i need to write cube config

00:23:14,480 --> 00:23:20,799
and now we should go to the api

00:23:17,760 --> 00:23:25,039
server of our cluster and see that here

00:23:20,799 --> 00:23:29,120
we have specified hopefully now the

00:23:25,039 --> 00:23:32,559
uh cloud provider uh this

00:23:29,120 --> 00:23:35,520
here um our new

00:23:32,559 --> 00:23:36,960
club provider aws and now that's

00:23:35,520 --> 00:23:38,400
reconciling take place

00:23:36,960 --> 00:23:40,960
and we also have a new machine

00:23:38,400 --> 00:23:41,840
controller that now is able to talk with

00:23:40,960 --> 00:23:44,799
aws

00:23:41,840 --> 00:23:45,760
so what we can now create is the new aws

00:23:44,799 --> 00:23:48,840
workers

00:23:45,760 --> 00:23:50,240
so let's go back into the user cluster

00:23:48,840 --> 00:23:53,600
so um

00:23:50,240 --> 00:23:54,720
yeah here go to this qr migration

00:23:53,600 --> 00:23:58,240
cluster

00:23:54,720 --> 00:24:01,360
and first um yeah pause

00:23:58,240 --> 00:24:04,720
all sheep deployments to be sure

00:24:01,360 --> 00:24:08,159
so uh worker

00:24:04,720 --> 00:24:08,159
machine deployment pause

00:24:10,480 --> 00:24:18,400
um that we don't upgrade this

00:24:13,840 --> 00:24:22,320
all machines and yes pause

00:24:18,400 --> 00:24:26,000
done and then we can now create our new

00:24:22,320 --> 00:24:29,360
aws workers therefore we need uh

00:24:26,000 --> 00:24:32,720
first a few inputs so here we

00:24:29,360 --> 00:24:34,400
need to specify the cluster id

00:24:32,720 --> 00:24:36,000
the instance profile in the security

00:24:34,400 --> 00:24:39,360
group but chromatic had

00:24:36,000 --> 00:24:42,880
created automatically so here yeah

00:24:39,360 --> 00:24:42,880
i want to see the metadatas

00:24:43,279 --> 00:24:49,840
we have here the cluster id so

00:24:46,320 --> 00:24:53,520
let's change that one here

00:24:49,840 --> 00:24:55,600
we have the aws instance profile

00:24:53,520 --> 00:24:56,640
what have we created what should be that

00:24:55,600 --> 00:25:00,880
one

00:24:56,640 --> 00:25:05,520
and we have the aws security group

00:25:00,880 --> 00:25:05,520
okay so let's create that one

00:25:05,760 --> 00:25:14,320
save it and then running the script

00:25:09,840 --> 00:25:17,760
deploy so now hopefully fingers crossed

00:25:14,320 --> 00:25:20,400
we grab new aws workers so

00:25:17,760 --> 00:25:22,159
yes i want to create one here we see

00:25:20,400 --> 00:25:23,039
that's now rendered in in the machine

00:25:22,159 --> 00:25:26,240
deployment

00:25:23,039 --> 00:25:29,679
we see our security group we see that we

00:25:26,240 --> 00:25:34,000
want to have a t3 medium and

00:25:29,679 --> 00:25:37,919
yeah that in the usb best one see so

00:25:34,000 --> 00:25:37,919
okay then let's deploy it

00:25:38,880 --> 00:25:44,880
so fingers cross yes as

00:25:42,000 --> 00:25:47,679
has been created so i want to watch the

00:25:44,880 --> 00:25:49,520
creation yes i want to see that

00:25:47,679 --> 00:25:50,880
and you now see what the machine

00:25:49,520 --> 00:25:53,440
deployment we have a new machine

00:25:50,880 --> 00:25:56,240
deployment with a boot target aws

00:25:53,440 --> 00:25:56,640
what creates two new machines and uh

00:25:56,240 --> 00:26:00,480
what

00:25:56,640 --> 00:26:03,120
get provision now in atl yes um

00:26:00,480 --> 00:26:04,559
so let's see uh our workload is still

00:26:03,120 --> 00:26:07,360
running as we see

00:26:04,559 --> 00:26:10,080
uh we have here our target cluster we

00:26:07,360 --> 00:26:12,480
see also here we have the new ubit

00:26:10,080 --> 00:26:14,159
would retarget aws node group let's now

00:26:12,480 --> 00:26:18,159
get muted

00:26:14,159 --> 00:26:19,600
and yeah let's go to aws and see what's

00:26:18,159 --> 00:26:22,020
have been created

00:26:19,600 --> 00:26:23,200
so here

00:26:22,020 --> 00:26:26,559
[Music]

00:26:23,200 --> 00:26:31,760
hopefully the aws console is

00:26:26,559 --> 00:26:31,760
fast enough uh we can now go to the

00:26:32,240 --> 00:26:35,919
ec2 in sensors and should see that we

00:26:35,440 --> 00:26:41,440
can

00:26:35,919 --> 00:26:41,440
have booted now new to instances

00:26:42,480 --> 00:26:54,480
and let's see

00:26:52,240 --> 00:26:56,880
yeah we see here initializing so that's

00:26:54,480 --> 00:26:58,640
the new two machines what we created

00:26:56,880 --> 00:27:00,559
if we take a look here we also see the

00:26:58,640 --> 00:27:03,760
tag that's the cluster

00:27:00,559 --> 00:27:06,400
for what we have and for that so

00:27:03,760 --> 00:27:07,039
we have created new machines so let's

00:27:06,400 --> 00:27:11,360
wait

00:27:07,039 --> 00:27:12,720
until the points uh say i get booted

00:27:11,360 --> 00:27:14,799
in the meantime we can take a look in

00:27:12,720 --> 00:27:18,240
the load pencil so hopefully

00:27:14,799 --> 00:27:21,520
um we also have a load balancer created

00:27:18,240 --> 00:27:23,279
that's what aws uh uh cloud controller

00:27:21,520 --> 00:27:24,159
manager will create because we already

00:27:23,279 --> 00:27:25,760
have a

00:27:24,159 --> 00:27:27,919
service type load balancer in the

00:27:25,760 --> 00:27:30,559
cluster and that aws

00:27:27,919 --> 00:27:31,600
takes over and creates also low bands on

00:27:30,559 --> 00:27:35,279
their side

00:27:31,600 --> 00:27:38,480
so let's see that's

00:27:35,279 --> 00:27:41,679
not the right one that's the right one

00:27:38,480 --> 00:27:42,399
um here we see there's a class created

00:27:41,679 --> 00:27:44,559
but

00:27:42,399 --> 00:27:46,799
we don't have any instances here because

00:27:44,559 --> 00:27:49,760
yeah the instances get now booted

00:27:46,799 --> 00:27:50,880
so let's hopefully let's go back and see

00:27:49,760 --> 00:27:54,240
how fast they are

00:27:50,880 --> 00:27:56,880
coming up okay

00:27:54,240 --> 00:27:58,159
we now see that we get a new node that's

00:27:56,880 --> 00:28:00,880
not ready but we can

00:27:58,159 --> 00:28:01,840
already connect to him so let's try to

00:28:00,880 --> 00:28:05,520
ssh

00:28:01,840 --> 00:28:09,200
into one adapter aws node

00:28:05,520 --> 00:28:12,559
and let's go here for that kind

00:28:09,200 --> 00:28:16,159
and see what's happening there yes i

00:28:12,559 --> 00:28:19,279
want to connect

00:28:16,159 --> 00:28:22,640
and yeah we see here that we have

00:28:19,279 --> 00:28:25,760
here uh a flannel route

00:28:22,640 --> 00:28:28,320
and we will soon have a cube

00:28:25,760 --> 00:28:29,200
or a interface for the vpn server as

00:28:28,320 --> 00:28:32,000
soon as that's

00:28:29,200 --> 00:28:32,960
started here we go here we see that

00:28:32,000 --> 00:28:34,960
interface

00:28:32,960 --> 00:28:36,720
and we can now try to get the

00:28:34,960 --> 00:28:38,399
interconnection between one cloud to

00:28:36,720 --> 00:28:42,000
another so we i

00:28:38,399 --> 00:28:45,120
then connect here to the on-premise node

00:28:42,000 --> 00:28:48,880
on the down and um

00:28:45,120 --> 00:28:52,399
let's connect to that one ubuntu

00:28:48,880 --> 00:28:55,760
and test if we are now can talk

00:28:52,399 --> 00:28:59,039
between clouds and

00:28:55,760 --> 00:29:03,200
here as well i have ip address

00:28:59,039 --> 00:29:07,200
for the cube interface what is

00:29:03,200 --> 00:29:13,840
10 20 42 and let's try

00:29:07,200 --> 00:29:13,840
if we can bring it from our aws node

00:29:20,960 --> 00:29:24,320
okay now we see that we get a connection

00:29:23,520 --> 00:29:29,039
here from

00:29:24,320 --> 00:29:32,799
the club node to the on-premise node

00:29:29,039 --> 00:29:33,279
good cool so next step what we now need

00:29:32,799 --> 00:29:37,919
to do

00:29:33,279 --> 00:29:42,080
is to to migrate our workload

00:29:37,919 --> 00:29:44,480
okay cool then let's go back here

00:29:42,080 --> 00:29:44,480
and

00:29:49,039 --> 00:29:56,000
here as well and try to migrate

00:29:52,960 --> 00:29:59,200
to the workload so we have now

00:29:56,000 --> 00:30:00,559
switched here to the user cluster to see

00:29:59,200 --> 00:30:03,039
what's happening

00:30:00,559 --> 00:30:03,039
and

00:30:04,720 --> 00:30:10,880
see take a look in the echo server

00:30:08,240 --> 00:30:10,880
namespace

00:30:11,279 --> 00:30:13,679
sorry

00:30:15,600 --> 00:30:20,399
um we see now here okay the echo cell is

00:30:18,080 --> 00:30:23,679
deployed on the migration vsphere node

00:30:20,399 --> 00:30:26,880
we want to now to roll out the new

00:30:23,679 --> 00:30:30,080
workload to the new cloud so let's

00:30:26,880 --> 00:30:33,679
try to quarten the notes

00:30:30,080 --> 00:30:36,720
according the notes because

00:30:33,679 --> 00:30:39,919
that new workload should not anymore go

00:30:36,720 --> 00:30:39,919
to this old nodes

00:30:41,200 --> 00:30:47,440
good that's corton and

00:30:44,240 --> 00:30:49,200
we have the another node to curtain

00:30:47,440 --> 00:30:51,840
so that means that the node should be

00:30:49,200 --> 00:30:54,960
now marked as not schedule

00:30:51,840 --> 00:30:58,720
able yes so scheduling disabled perfect

00:30:54,960 --> 00:31:01,760
so we can now uh use the cubecontrol

00:30:58,720 --> 00:31:04,640
rollout restart feature to now restart

00:31:01,760 --> 00:31:06,880
our deployment of the echo server in the

00:31:04,640 --> 00:31:07,600
namespace ico server to trigger like a

00:31:06,880 --> 00:31:09,919
row in the

00:31:07,600 --> 00:31:10,799
release of the echo server without any

00:31:09,919 --> 00:31:13,760
change

00:31:10,799 --> 00:31:14,880
so uh yeah let's trigger that one and

00:31:13,760 --> 00:31:17,519
see what's happening

00:31:14,880 --> 00:31:18,320
in the down we see okay that's still we

00:31:17,519 --> 00:31:21,120
have uh

00:31:18,320 --> 00:31:23,279
the application up and running and yes

00:31:21,120 --> 00:31:24,159
so now we see the container get created

00:31:23,279 --> 00:31:26,960
in the new cloud

00:31:24,159 --> 00:31:28,000
so first success so hopefully this will

00:31:26,960 --> 00:31:31,039
now work

00:31:28,000 --> 00:31:32,000
um we see that now a new container is

00:31:31,039 --> 00:31:35,120
running on the new

00:31:32,000 --> 00:31:38,720
aws workers and we can now terminate

00:31:35,120 --> 00:31:39,200
the old one so that's now going step by

00:31:38,720 --> 00:31:42,080
step

00:31:39,200 --> 00:31:43,360
and we have three new workers and you

00:31:42,080 --> 00:31:45,919
see

00:31:43,360 --> 00:31:48,240
the service is still reachable and we

00:31:45,919 --> 00:31:52,880
can't get now hopefully also if we

00:31:48,240 --> 00:31:58,159
go to the browser the

00:31:52,880 --> 00:31:58,159
we can see here detail that we get back

00:31:58,240 --> 00:32:05,360
from the aws node to cause

00:32:01,840 --> 00:32:08,399
so we have uh where is the host name

00:32:05,360 --> 00:32:10,640
no it's just the host name um

00:32:08,399 --> 00:32:12,080
we see that the yeah the workers are

00:32:10,640 --> 00:32:15,279
running in a new cloud

00:32:12,080 --> 00:32:16,960
and uh is still reachable through our

00:32:15,279 --> 00:32:20,799
old endpoint

00:32:16,960 --> 00:32:23,120
cool so that seems to work um like now

00:32:20,799 --> 00:32:25,200
that the next step would be to migrate

00:32:23,120 --> 00:32:28,399
all other workload to the new cloud

00:32:25,200 --> 00:32:31,519
remove the load balancer and then using

00:32:28,399 --> 00:32:33,440
the new dns name so um

00:32:31,519 --> 00:32:35,519
we can quickly try if the new low

00:32:33,440 --> 00:32:38,159
balance is now listening

00:32:35,519 --> 00:32:40,320
so yeah we have here the new instances

00:32:38,159 --> 00:32:42,000
and maybe the dns name is already

00:32:40,320 --> 00:32:45,200
propagated

00:32:42,000 --> 00:32:48,640
to see if this is working we can see

00:32:45,200 --> 00:32:50,240
okay let's change that to the new dns

00:32:48,640 --> 00:32:54,880
name

00:32:50,240 --> 00:32:57,919
and no so dns is not

00:32:54,880 --> 00:33:02,720
there so yeah good

00:32:57,919 --> 00:33:05,840
then finally migrated to the new cloud

00:33:02,720 --> 00:33:08,880
so how are the next step looks like

00:33:05,840 --> 00:33:10,960
so um that's uh

00:33:08,880 --> 00:33:12,880
how we can create the migration uh the

00:33:10,960 --> 00:33:14,720
worker user clusters

00:33:12,880 --> 00:33:16,000
and to move completely to the another

00:33:14,720 --> 00:33:18,960
cloud we need now to

00:33:16,000 --> 00:33:20,720
migrate also the c cluster for that how

00:33:18,960 --> 00:33:23,600
we can achieve that is the same way

00:33:20,720 --> 00:33:24,080
we're reusing the same principle so here

00:33:23,600 --> 00:33:26,159
we

00:33:24,080 --> 00:33:27,360
we have the workers now in the new cloud

00:33:26,159 --> 00:33:30,320
and now we need

00:33:27,360 --> 00:33:31,360
to make migrate control plane the good

00:33:30,320 --> 00:33:34,480
thing is on that

00:33:31,360 --> 00:33:35,440
in any case the workers workload is

00:33:34,480 --> 00:33:38,000
still safe because

00:33:35,440 --> 00:33:39,200
it's already in the new cloud so we can

00:33:38,000 --> 00:33:42,880
migrate it even

00:33:39,200 --> 00:33:44,640
if we may break it and the only thing

00:33:42,880 --> 00:33:47,279
what would maybe not work is an

00:33:44,640 --> 00:33:49,279
upgrade of the kubernetes but still the

00:33:47,279 --> 00:33:51,440
workload is safe

00:33:49,279 --> 00:33:52,720
so how we migrate the c clusters first

00:33:51,440 --> 00:33:55,679
we create new

00:33:52,720 --> 00:33:57,919
master nodes for the seat masters so

00:33:55,679 --> 00:34:00,880
that means we creating new

00:33:57,919 --> 00:34:02,000
nodes putting a new capabilities apis

00:34:00,880 --> 00:34:05,039
load balance on it

00:34:02,000 --> 00:34:07,039
update api endpoints because this

00:34:05,039 --> 00:34:08,079
api endpoints are stable in the qubit in

00:34:07,039 --> 00:34:11,440
this cluster

00:34:08,079 --> 00:34:14,639
and then we um block uh

00:34:11,440 --> 00:34:17,679
for sure the c cluster for upgrades so

00:34:14,639 --> 00:34:20,000
that we don't anything yeah uh

00:34:17,679 --> 00:34:20,879
trigger unexpected characteristics

00:34:20,000 --> 00:34:23,839
during this

00:34:20,879 --> 00:34:25,520
migration and then um yeah we migrating

00:34:23,839 --> 00:34:27,679
the user control planes

00:34:25,520 --> 00:34:28,879
uh in the same way as migrated the

00:34:27,679 --> 00:34:32,639
workload now at the

00:34:28,879 --> 00:34:33,280
user clusters so we moved the scds to

00:34:32,639 --> 00:34:36,079
the new

00:34:33,280 --> 00:34:37,280
cloud and using sct quorum for the data

00:34:36,079 --> 00:34:40,320
migration

00:34:37,280 --> 00:34:42,240
and um yeah for sure we should have uh

00:34:40,320 --> 00:34:43,760
backups and recovery for all the

00:34:42,240 --> 00:34:45,919
components

00:34:43,760 --> 00:34:47,520
how does this look like for example here

00:34:45,919 --> 00:34:50,079
we have our seed

00:34:47,520 --> 00:34:50,639
jns for our own api server what's

00:34:50,079 --> 00:34:53,280
projecting

00:34:50,639 --> 00:34:54,320
these remasters there we have our now

00:34:53,280 --> 00:34:57,119
our aws

00:34:54,320 --> 00:34:58,560
clusters hosted the control plane and we

00:34:57,119 --> 00:35:00,480
have the workers who are hosting

00:34:58,560 --> 00:35:03,599
security voice control plane

00:35:00,480 --> 00:35:04,160
every single gcp then as fast first step

00:35:03,599 --> 00:35:07,359
we would

00:35:04,160 --> 00:35:08,880
again place the vpn that time in the c

00:35:07,359 --> 00:35:11,680
cluster

00:35:08,880 --> 00:35:12,720
connecting a reworker node and

00:35:11,680 --> 00:35:16,560
masternodes

00:35:12,720 --> 00:35:19,040
then uh create a quorum with five

00:35:16,560 --> 00:35:19,920
hcd for the masternodes that ensures

00:35:19,040 --> 00:35:23,599
that the new

00:35:19,920 --> 00:35:26,720
two lcds get the data replicated from

00:35:23,599 --> 00:35:29,599
gcp nodes to the aws nodes then

00:35:26,720 --> 00:35:30,960
as next step if we have achieved that we

00:35:29,599 --> 00:35:33,520
can remove

00:35:30,960 --> 00:35:35,520
two gcp workers and still have the

00:35:33,520 --> 00:35:36,000
quorum of three so three or five if

00:35:35,520 --> 00:35:38,560
enough

00:35:36,000 --> 00:35:39,119
is enough for the quorum and we can

00:35:38,560 --> 00:35:42,560
therefore

00:35:39,119 --> 00:35:43,599
also change now the uh the the dns name

00:35:42,560 --> 00:35:47,119
to the new

00:35:43,599 --> 00:35:51,119
aws club provider load balancer

00:35:47,119 --> 00:35:55,280
so there now we can then remove the

00:35:51,119 --> 00:35:58,880
quorum also from five to

00:35:55,280 --> 00:36:02,000
three and we have a stable control plane

00:35:58,880 --> 00:36:05,599
um for sure also in this project

00:36:02,000 --> 00:36:08,400
procedure uh we need then to clean up

00:36:05,599 --> 00:36:10,320
remove the old gsp node again create a

00:36:08,400 --> 00:36:13,440
new idea by s master node

00:36:10,320 --> 00:36:14,400
and then we have three lcds uh healthy

00:36:13,440 --> 00:36:17,760
in the new cloud

00:36:14,400 --> 00:36:20,720
migrated in the same way

00:36:17,760 --> 00:36:22,400
as we migrated to user clusters workers

00:36:20,720 --> 00:36:26,160
we now need to migrate

00:36:22,400 --> 00:36:26,880
the c-cluster workers for that um we

00:36:26,160 --> 00:36:31,040
here can

00:36:26,880 --> 00:36:33,760
change the um vpn again

00:36:31,040 --> 00:36:34,480
create a new worker nodes and then in

00:36:33,760 --> 00:36:37,680
this kind

00:36:34,480 --> 00:36:41,040
also changing here at the cloud provider

00:36:37,680 --> 00:36:44,160
here that it's pointing now to aws

00:36:41,040 --> 00:36:46,880
after this is happened we can increase

00:36:44,160 --> 00:36:49,760
here uh in chromatic settings the

00:36:46,880 --> 00:36:51,920
default value of entity replicas to five

00:36:49,760 --> 00:36:53,680
that means that the programmatic user

00:36:51,920 --> 00:36:56,320
cluster controller manager

00:36:53,680 --> 00:36:57,040
creates two new entities for every user

00:36:56,320 --> 00:36:59,520
cluster

00:36:57,040 --> 00:37:01,599
that's insurers in the same ways as me

00:36:59,520 --> 00:37:03,680
migrated the lcd on the top

00:37:01,599 --> 00:37:05,520
it's migrating also for the for the

00:37:03,680 --> 00:37:08,800
users clusters so

00:37:05,520 --> 00:37:11,839
i have now a quorum of five

00:37:08,800 --> 00:37:13,520
it is running but still pointing to the

00:37:11,839 --> 00:37:17,040
old dns

00:37:13,520 --> 00:37:20,320
but between clouds as next step um

00:37:17,040 --> 00:37:22,800
we now creating a new cloud balance

00:37:20,320 --> 00:37:24,320
load balancer and rename the white card

00:37:22,800 --> 00:37:26,960
so that's the connection

00:37:24,320 --> 00:37:28,560
from the user cluster workers what we

00:37:26,960 --> 00:37:32,240
already migrated to the new

00:37:28,560 --> 00:37:35,599
cloud load balancer and

00:37:32,240 --> 00:37:39,119
yes then we switch over and we

00:37:35,599 --> 00:37:40,720
adding one new aws node replacing all

00:37:39,119 --> 00:37:43,520
gcp nodes so we have then

00:37:40,720 --> 00:37:45,280
a quorum of three entities per user

00:37:43,520 --> 00:37:47,520
cluster on the new cloud

00:37:45,280 --> 00:37:48,800
that is the most reasonable target so

00:37:47,520 --> 00:37:51,920
now we are safe

00:37:48,800 --> 00:37:53,520
and we can remove the old missingno

00:37:51,920 --> 00:37:56,720
working roads

00:37:53,520 --> 00:37:59,359
uh clean up the old cloud resources

00:37:56,720 --> 00:38:00,079
that means we scale down to lcd level

00:37:59,359 --> 00:38:04,079
three

00:38:00,079 --> 00:38:07,119
we um remove then the

00:38:04,079 --> 00:38:09,359
vpn overlay and um

00:38:07,119 --> 00:38:11,280
we now have the kernel away what's

00:38:09,359 --> 00:38:14,880
routing between the

00:38:11,280 --> 00:38:17,520
workers again between eth0

00:38:14,880 --> 00:38:19,040
so then we have successfully migrated

00:38:17,520 --> 00:38:22,240
every user cluster into

00:38:19,040 --> 00:38:23,040
seed cluster to the new cloud so that is

00:38:22,240 --> 00:38:25,920
the approach

00:38:23,040 --> 00:38:28,800
what we think are is possible to migrate

00:38:25,920 --> 00:38:32,000
really 100 clusters without downtime

00:38:28,800 --> 00:38:32,400
and in a scalable way so for sure we are

00:38:32,000 --> 00:38:34,240
not

00:38:32,400 --> 00:38:36,480
right now that we just press a button

00:38:34,240 --> 00:38:39,839
but that's our future target

00:38:36,480 --> 00:38:42,640
um so we want to automate also

00:38:39,839 --> 00:38:43,760
um the clean approach procedure and for

00:38:42,640 --> 00:38:45,440
sure if you want to

00:38:43,760 --> 00:38:47,839
really make it scalable we need to write

00:38:45,440 --> 00:38:48,560
an operator currently we did everything

00:38:47,839 --> 00:38:52,240
by a

00:38:48,560 --> 00:38:55,680
yeah hacky help uh um bash script

00:38:52,240 --> 00:38:57,200
um and to not like yeah doing that by

00:38:55,680 --> 00:38:58,880
hand the health checks we need to have

00:38:57,200 --> 00:39:01,280
an operator who check the house check

00:38:58,880 --> 00:39:03,040
the conditions and have also some repair

00:39:01,280 --> 00:39:05,200
options and retry options

00:39:03,040 --> 00:39:06,880
but therefore the do we need this

00:39:05,200 --> 00:39:08,960
reconsidering pattern match

00:39:06,880 --> 00:39:10,480
in the same way as is match for our

00:39:08,960 --> 00:39:13,119
cloud migration where the

00:39:10,480 --> 00:39:14,160
chromatic controller creates the new

00:39:13,119 --> 00:39:16,720
cloud

00:39:14,160 --> 00:39:18,800
provider and so on we can use the

00:39:16,720 --> 00:39:21,119
reconciling also for the migration

00:39:18,800 --> 00:39:22,400
and using okay we have a new target

00:39:21,119 --> 00:39:25,280
cloud

00:39:22,400 --> 00:39:27,520
operator take care about that matching

00:39:25,280 --> 00:39:30,400
the old state to the new state

00:39:27,520 --> 00:39:32,400
um technical we also need to stabilize

00:39:30,400 --> 00:39:35,200
the vpn connection so

00:39:32,400 --> 00:39:36,640
um right now we have only one vpn server

00:39:35,200 --> 00:39:39,359
this could be maybe

00:39:36,640 --> 00:39:41,359
on a bottleneck on bigger clusters maybe

00:39:39,359 --> 00:39:45,359
we should deploy multiple vpn

00:39:41,359 --> 00:39:48,400
clusters and we have uh like

00:39:45,359 --> 00:39:50,960
maybe uh have also more soft switch

00:39:48,400 --> 00:39:52,800
between vpn and host networking overlay

00:39:50,960 --> 00:39:54,079
that's something what we need to explore

00:39:52,800 --> 00:39:56,880
more um

00:39:54,079 --> 00:39:59,280
and maybe their wire card can be an

00:39:56,880 --> 00:40:02,560
alternative as a vpn connection

00:39:59,280 --> 00:40:05,280
what could help maybe on the setup

00:40:02,560 --> 00:40:06,160
okay um one more detail here currently

00:40:05,280 --> 00:40:09,760
we tested it

00:40:06,160 --> 00:40:10,319
in a 1.17 seat cluster where the manage

00:40:09,760 --> 00:40:12,560
fields

00:40:10,319 --> 00:40:14,480
feature is not included somehow this

00:40:12,560 --> 00:40:17,440
makes a little bit of trouble between

00:40:14,480 --> 00:40:18,079
okay you have an like patch by cube

00:40:17,440 --> 00:40:19,280
control

00:40:18,079 --> 00:40:21,359
and you have an operator what

00:40:19,280 --> 00:40:23,680
reconsidering the fields so that's

00:40:21,359 --> 00:40:25,119
why currently i'm a little bit limited

00:40:23,680 --> 00:40:27,599
on the 117

00:40:25,119 --> 00:40:31,359
seed clusters as you saw in the demo uh

00:40:27,599 --> 00:40:34,640
the user cluster can be on 118.

00:40:31,359 --> 00:40:35,280
cool then um yeah thanks for your

00:40:34,640 --> 00:40:38,079
attention

00:40:35,280 --> 00:40:38,800
uh we are happy to answer any questions

00:40:38,079 --> 00:40:41,119
so

00:40:38,800 --> 00:40:42,720
feel free to reach out now and yeah

00:40:41,119 --> 00:40:45,520
thanks to listening

00:40:42,720 --> 00:40:47,599
and yeah manuel a few last words from

00:40:45,520 --> 00:40:50,480
your side

00:40:47,599 --> 00:40:53,200
uh no that was amazing thank you so much

00:40:50,480 --> 00:40:56,640
and we're open to questions now

00:40:53,200 --> 00:41:00,319
okay thanks a lot and looking forward to

00:40:56,640 --> 00:41:00,319

YouTube URL: https://www.youtube.com/watch?v=dCWIU7JeOH0


