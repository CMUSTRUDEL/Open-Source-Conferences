Title: Automatically Making Dashboards Load 100X Faster - Shreyas Srivatsan, Chronosphere
Publication date: 2020-11-23
Playlist: KubeCon + CloudNativeCon North America 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon Europe 2021 Virtual from May 4–7, 2021. Learn more at https://kubecon.io. The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects. 

Automatically Making Dashboards Load 100X Faster - Shreyas Srivatsan, Chronosphere 

High cardinality metrics often cause alerts and dashboards to time out when they try to fetch too much data. Prometheus provides recording rules to speed up queries by pre-generating the queries, however, they have to be configured manually and require reconfiguring alerts and dashboards to point to the recorded series. The performance degradation often happens as new metrics are introduced with more instances or deploys and a working query may break all of a sudden. In this talk, we will show you how slow queries can be preemptively detected and automatically sped up without any manual reconfiguration. We detail two approaches to achieving automated speed ups - one that is based on recording rules and the other is based on the M3 Aggregation tier. We will compare and contrast both approaches and show examples of how one can leverage either open source method to achieve the same results. 

https://sched.co/ekCu
Captions: 
	00:00:01,680 --> 00:00:05,440
hi good morning good afternoon good

00:00:03,360 --> 00:00:09,200
evening depending on where you are

00:00:05,440 --> 00:00:11,440
today we're going to talk about um

00:00:09,200 --> 00:00:12,639
making dashboards like automatically

00:00:11,440 --> 00:00:14,559
faster

00:00:12,639 --> 00:00:16,480
uh using some tools we already know

00:00:14,559 --> 00:00:18,000
about like recording rules and also

00:00:16,480 --> 00:00:20,160
we're going to dive into kind of the m3

00:00:18,000 --> 00:00:23,439
aggregation tier

00:00:20,160 --> 00:00:24,640
uh my name is shreyas uh i'm a software

00:00:23,439 --> 00:00:26,480
engineer at chronosphere

00:00:24,640 --> 00:00:28,720
uh we're working on building a hosted

00:00:26,480 --> 00:00:30,960
metrics and monitoring platform

00:00:28,720 --> 00:00:32,719
targeting large-scale high-throughput

00:00:30,960 --> 00:00:35,280
use cases

00:00:32,719 --> 00:00:37,200
um i'm interested in all things

00:00:35,280 --> 00:00:39,760
observability

00:00:37,200 --> 00:00:41,680
primarily kind of focusing on making use

00:00:39,760 --> 00:00:44,800
of metrics and traces to solve like

00:00:41,680 --> 00:00:46,800
interesting problems uh prior to this

00:00:44,800 --> 00:00:47,840
i was on the observable team at uber

00:00:46,800 --> 00:00:50,160
working on kind of

00:00:47,840 --> 00:00:52,320
like building out and scaling the

00:00:50,160 --> 00:00:54,719
alerting platform there

00:00:52,320 --> 00:00:56,160
the agenda for today is kind of setting

00:00:54,719 --> 00:00:57,840
up the problem

00:00:56,160 --> 00:00:59,520
uh looking at kind of like high

00:00:57,840 --> 00:01:01,039
cardinality metrics and why they're a

00:00:59,520 --> 00:01:03,039
challenge

00:01:01,039 --> 00:01:04,479
um then we're going to talk about kind

00:01:03,039 --> 00:01:06,159
of aggregating metrics

00:01:04,479 --> 00:01:08,000
using recording rules and the m3

00:01:06,159 --> 00:01:11,600
aggregation here and kind of

00:01:08,000 --> 00:01:14,400
like how that is uh yeah how we can kind

00:01:11,600 --> 00:01:16,400
of make dashboards faster using that

00:01:14,400 --> 00:01:18,000
then we kind of talk about how we make

00:01:16,400 --> 00:01:19,759
things easy to use

00:01:18,000 --> 00:01:22,960
uh and finally we kind of round out with

00:01:19,759 --> 00:01:25,439
the demo and kind of a q a

00:01:22,960 --> 00:01:26,080
so to set up the problem for most of

00:01:25,439 --> 00:01:28,640
this

00:01:26,080 --> 00:01:30,079
talk we're going to take c advisor as an

00:01:28,640 --> 00:01:31,759
example

00:01:30,079 --> 00:01:34,400
which provides essentially resource

00:01:31,759 --> 00:01:37,600
usage and performance metrics

00:01:34,400 --> 00:01:38,079
of running containers this dashboard

00:01:37,600 --> 00:01:39,920
taken

00:01:38,079 --> 00:01:41,439
is kind of the c advisor dashboard from

00:01:39,920 --> 00:01:44,479
grafana

00:01:41,439 --> 00:01:46,799
as you can see it has some amount of

00:01:44,479 --> 00:01:47,680
aggregated information and some amount

00:01:46,799 --> 00:01:49,520
of like

00:01:47,680 --> 00:01:51,200
individual kind of like pod level

00:01:49,520 --> 00:01:53,520
information

00:01:51,200 --> 00:01:54,320
so it has like it's tracking 5000

00:01:53,520 --> 00:01:56,159
containers

00:01:54,320 --> 00:01:58,640
you have some aggregate sets like cpu

00:01:56,159 --> 00:02:02,079
usage network traffic

00:01:58,640 --> 00:02:04,399
etc you also have like per pod stats

00:02:02,079 --> 00:02:07,280
like cpu usage and memory usage

00:02:04,399 --> 00:02:08,959
which are useful so the aggregate stats

00:02:07,280 --> 00:02:09,599
kind of give you an idea that something

00:02:08,959 --> 00:02:13,040
is wrong

00:02:09,599 --> 00:02:16,560
and then the more like per pod stats

00:02:13,040 --> 00:02:19,840
kind of help your root cause better um

00:02:16,560 --> 00:02:21,760
so this has like tons of dimensions uh

00:02:19,840 --> 00:02:24,640
because it wants to kind of slice and

00:02:21,760 --> 00:02:26,319
dice metrics in different forms

00:02:24,640 --> 00:02:28,640
so what do these dimensions actually do

00:02:26,319 --> 00:02:31,519
if you consider a simple metric

00:02:28,640 --> 00:02:33,920
kind of cpu usage you end up having like

00:02:31,519 --> 00:02:35,519
16 000 series and it takes 20 seconds to

00:02:33,920 --> 00:02:38,640
actually query this information

00:02:35,519 --> 00:02:41,760
like which is really slow

00:02:38,640 --> 00:02:43,360
this is because like you can see the so

00:02:41,760 --> 00:02:45,680
many different

00:02:43,360 --> 00:02:48,720
tags on it like there's a part id

00:02:45,680 --> 00:02:48,720
there's an instance

00:02:48,800 --> 00:02:53,440
there's the image and so on so forth

00:02:53,840 --> 00:02:57,200
if you only consider the two labels that

00:02:55,680 --> 00:02:59,680
we're interested in and we actually

00:02:57,200 --> 00:03:01,440
aggregate this using a recording rule

00:02:59,680 --> 00:03:03,920
then this query becomes a way more

00:03:01,440 --> 00:03:05,440
manageable like 230 series and just

00:03:03,920 --> 00:03:07,680
takes like half a second to actually

00:03:05,440 --> 00:03:10,560
like get back information

00:03:07,680 --> 00:03:13,760
uh and in most cases this is actually

00:03:10,560 --> 00:03:13,760
all the information we want

00:03:13,840 --> 00:03:17,440
now the reason for getting so many

00:03:16,560 --> 00:03:19,680
series here

00:03:17,440 --> 00:03:21,440
is like even though it's kind of like

00:03:19,680 --> 00:03:23,280
containing it's kind of something is

00:03:21,440 --> 00:03:26,239
tracking one instance

00:03:23,280 --> 00:03:28,239
as things get restarted like as the pod

00:03:26,239 --> 00:03:29,440
gets we started reschedule like a new

00:03:28,239 --> 00:03:30,799
time series shows up

00:03:29,440 --> 00:03:32,959
so there are more things actually

00:03:30,799 --> 00:03:35,440
getting stored in the time series

00:03:32,959 --> 00:03:36,959
database which don't have data currently

00:03:35,440 --> 00:03:39,280
but when you actually like query for

00:03:36,959 --> 00:03:40,879
like 30 minutes or like a few hours of

00:03:39,280 --> 00:03:43,840
data you end up pulling time series

00:03:40,879 --> 00:03:46,480
which should not exist

00:03:43,840 --> 00:03:47,680
so it basically results in like slowing

00:03:46,480 --> 00:03:49,599
dashboards and

00:03:47,680 --> 00:03:53,120
dashboards which kind of like constantly

00:03:49,599 --> 00:03:54,799
decreasing like degrade over time

00:03:53,120 --> 00:03:56,239
as mentioned this is because kind of the

00:03:54,799 --> 00:03:57,439
cardinality of dimensions keep

00:03:56,239 --> 00:03:59,519
increasing

00:03:57,439 --> 00:04:00,640
as you add new instances roll out new

00:03:59,519 --> 00:04:03,040
images

00:04:00,640 --> 00:04:04,640
like things basically yeah we keep

00:04:03,040 --> 00:04:06,400
adding dimensions and things keep

00:04:04,640 --> 00:04:09,840
slowing up

00:04:06,400 --> 00:04:11,760
okay so this basically results in kind

00:04:09,840 --> 00:04:13,599
of slower dashboards

00:04:11,760 --> 00:04:15,280
slower dashboards results and kind of

00:04:13,599 --> 00:04:18,720
browser locking up

00:04:15,280 --> 00:04:21,840
and a bad user experience

00:04:18,720 --> 00:04:23,680
so when an engineer notices this the

00:04:21,840 --> 00:04:26,320
dashboard needs to be optimized

00:04:23,680 --> 00:04:28,000
like how we actually kind of go up go

00:04:26,320 --> 00:04:29,520
about doing this

00:04:28,000 --> 00:04:31,520
the first step is kind of figuring out

00:04:29,520 --> 00:04:33,680
which queries are the culprit

00:04:31,520 --> 00:04:35,280
you inspect the requests you could

00:04:33,680 --> 00:04:37,199
inspect the request from a dashboard to

00:04:35,280 --> 00:04:38,479
look for slow queries

00:04:37,199 --> 00:04:40,240
you could do this in kind of chrome

00:04:38,479 --> 00:04:42,639
inspector

00:04:40,240 --> 00:04:44,400
you could use the prometheus query log

00:04:42,639 --> 00:04:47,600
uh to do this

00:04:44,400 --> 00:04:48,880
and kind of associate back and yeah

00:04:47,600 --> 00:04:51,120
and try to associate back to the

00:04:48,880 --> 00:04:52,720
dashboard but this is difficult

00:04:51,120 --> 00:04:54,240
because the prometheus query log has

00:04:52,720 --> 00:04:55,360
like all the queries running in the

00:04:54,240 --> 00:04:58,479
system

00:04:55,360 --> 00:05:01,759
and the dashboard itself like you may

00:04:58,479 --> 00:05:01,759
have many many dashboards

00:05:02,160 --> 00:05:05,680
but given that we figure out kind of

00:05:03,759 --> 00:05:07,280
which are the culprit you probably want

00:05:05,680 --> 00:05:08,240
to do some form of pre-aggregation of

00:05:07,280 --> 00:05:11,199
these metrics to

00:05:08,240 --> 00:05:12,800
make these queries faster and over the

00:05:11,199 --> 00:05:13,280
next few slides we will kind of talk

00:05:12,800 --> 00:05:16,320
about

00:05:13,280 --> 00:05:18,320
a few of these like pre-aggregation

00:05:16,320 --> 00:05:20,160
like a few ways we can actually

00:05:18,320 --> 00:05:21,520
pre-aggregate things

00:05:20,160 --> 00:05:23,520
uh the first thing i want to talk about

00:05:21,520 --> 00:05:25,280
is recording rules this is something

00:05:23,520 --> 00:05:27,520
prometheus provides support for

00:05:25,280 --> 00:05:29,280
and kind of they're like widely used for

00:05:27,520 --> 00:05:30,720
this purpose

00:05:29,280 --> 00:05:32,720
they basically allow kind of

00:05:30,720 --> 00:05:34,560
pre-computing queries and

00:05:32,720 --> 00:05:36,720
storing back aggregate time series to

00:05:34,560 --> 00:05:38,320
the tsdb

00:05:36,720 --> 00:05:40,240
once you do that you essentially instead

00:05:38,320 --> 00:05:42,080
of making the query

00:05:40,240 --> 00:05:43,919
which kind of looks at all the series

00:05:42,080 --> 00:05:46,400
the dashboard is just making a query

00:05:43,919 --> 00:05:50,479
which is like picking back one series

00:05:46,400 --> 00:05:53,199
um so it's like super fast

00:05:50,479 --> 00:05:54,400
and the caveat is that now the dashboard

00:05:53,199 --> 00:05:55,759
now has to go and

00:05:54,400 --> 00:05:57,360
you need to point it back to the

00:05:55,759 --> 00:05:58,639
pre-computer time series you need to go

00:05:57,360 --> 00:05:59,280
and like change the dashboard to

00:05:58,639 --> 00:06:02,240
actually

00:05:59,280 --> 00:06:04,479
access the real-time series recording

00:06:02,240 --> 00:06:06,400
rule looks like something like the below

00:06:04,479 --> 00:06:08,880
you have a record which is basically the

00:06:06,400 --> 00:06:12,080
new time series you're storing stuff in

00:06:08,880 --> 00:06:14,319
um and then the expression which is like

00:06:12,080 --> 00:06:16,720
whatever prom q and expression you want

00:06:14,319 --> 00:06:20,319
to get executed so you can put anything

00:06:16,720 --> 00:06:20,319
as complicated as you want here

00:06:20,560 --> 00:06:24,240
and as mentioned the record is

00:06:21,759 --> 00:06:27,600
essentially a new time series

00:06:24,240 --> 00:06:28,639
um so to basically create recording

00:06:27,600 --> 00:06:29,680
rules you need to know what to

00:06:28,639 --> 00:06:31,759
pre-compute

00:06:29,680 --> 00:06:33,520
so we figure out the background queries

00:06:31,759 --> 00:06:35,840
by doing some formatting analysis on the

00:06:33,520 --> 00:06:35,840
dashboard

00:06:36,160 --> 00:06:39,840
then you basically go configure these

00:06:37,759 --> 00:06:42,319
recording rules

00:06:39,840 --> 00:06:44,160
and you can go and change the dashboard

00:06:42,319 --> 00:06:45,919
people kind of query the recording rule

00:06:44,160 --> 00:06:48,639
metrics

00:06:45,919 --> 00:06:49,360
instead of the underlying metrics so

00:06:48,639 --> 00:06:52,080
it's a

00:06:49,360 --> 00:06:55,039
real long manual process let's say

00:06:52,080 --> 00:06:55,039
you've actually done this

00:06:55,120 --> 00:06:58,880
what happens when one of these metrics

00:06:56,720 --> 00:07:01,759
changes like the

00:06:58,880 --> 00:07:02,400
query for the recording rule has changed

00:07:01,759 --> 00:07:05,280
or

00:07:02,400 --> 00:07:06,800
a new panel becomes slow you basically

00:07:05,280 --> 00:07:07,840
have to repeat this process all over

00:07:06,800 --> 00:07:09,440
again

00:07:07,840 --> 00:07:12,400
and kind of do the same manual

00:07:09,440 --> 00:07:14,080
processing again and again

00:07:12,400 --> 00:07:16,800
the second thing is recording rules are

00:07:14,080 --> 00:07:19,039
very expensive

00:07:16,800 --> 00:07:21,280
they basically execute and pre-compute

00:07:19,039 --> 00:07:24,560
the score at regular intervals

00:07:21,280 --> 00:07:27,199
so a query accessing many time series

00:07:24,560 --> 00:07:28,319
can get expensive very quickly

00:07:27,199 --> 00:07:29,599
especially when kind of the

00:07:28,319 --> 00:07:33,520
dimensionality of

00:07:29,599 --> 00:07:33,520
the underlying metrics keep increasing

00:07:33,919 --> 00:07:39,039
and given that these kind of run

00:07:37,199 --> 00:07:41,440
alongside other queries in the system

00:07:39,039 --> 00:07:42,800
like dashboards and alerts

00:07:41,440 --> 00:07:45,840
they have the potential to kind of

00:07:42,800 --> 00:07:45,840
orient the query engine

00:07:45,919 --> 00:07:50,560
so those are kind of a couple of like

00:07:48,479 --> 00:07:52,479
really bad

00:07:50,560 --> 00:07:54,560
aspects of kind of using recording rules

00:07:52,479 --> 00:07:58,479
and which we need to kind of be

00:07:54,560 --> 00:07:59,360
careful about but as mentioned before

00:07:58,479 --> 00:08:00,879
sometimes for

00:07:59,360 --> 00:08:02,800
underlying metrics we don't actually

00:08:00,879 --> 00:08:04,000
need all these dimensions so it would be

00:08:02,800 --> 00:08:05,599
nice if we can actually

00:08:04,000 --> 00:08:07,919
get rid of these completely and not

00:08:05,599 --> 00:08:09,039
store them what we care about is just

00:08:07,919 --> 00:08:10,720
the aggregate

00:08:09,039 --> 00:08:13,440
and we don't really care about kind of

00:08:10,720 --> 00:08:15,599
the underlying metrics themselves

00:08:13,440 --> 00:08:17,360
so would be great if we can do that and

00:08:15,599 --> 00:08:18,479
that's kind of where the m2 aggregation

00:08:17,360 --> 00:08:22,560
tier comes in

00:08:18,479 --> 00:08:24,879
so m3 is a remote storage for prometheus

00:08:22,560 --> 00:08:26,240
and the empty aggregation here allows us

00:08:24,879 --> 00:08:28,479
to kind of move

00:08:26,240 --> 00:08:30,960
the expensive recording rule computation

00:08:28,479 --> 00:08:33,680
to streaming aggregation

00:08:30,960 --> 00:08:35,519
so when prometheus remote write comes

00:08:33,680 --> 00:08:37,519
into m3

00:08:35,519 --> 00:08:39,519
it sees that some metrics need

00:08:37,519 --> 00:08:41,760
aggregation and kind of it forwards it

00:08:39,519 --> 00:08:43,519
to the m3 aggregation here

00:08:41,760 --> 00:08:45,600
the empty aggregation tier basically

00:08:43,519 --> 00:08:49,760
knows

00:08:45,600 --> 00:08:51,680
what rules to apply on what metrics

00:08:49,760 --> 00:08:53,200
it applies those aggregations and then

00:08:51,680 --> 00:08:54,560
sends it back to the coordinator to

00:08:53,200 --> 00:08:58,320
actually like persist it

00:08:54,560 --> 00:09:00,320
into long-term storage in m3db

00:08:58,320 --> 00:09:02,000
so the aggregator basically allows down

00:09:00,320 --> 00:09:04,160
sampling dropping

00:09:02,000 --> 00:09:05,680
or aggregating metrics prior to

00:09:04,160 --> 00:09:07,200
persisting them to the time series

00:09:05,680 --> 00:09:10,720
database

00:09:07,200 --> 00:09:12,320
uh the aggregator supports kind of two

00:09:10,720 --> 00:09:14,080
different types of rules

00:09:12,320 --> 00:09:16,560
one of them is called roll-up rules

00:09:14,080 --> 00:09:19,680
which allow aggregating these metrics

00:09:16,560 --> 00:09:21,600
uh then second is mapping rules which

00:09:19,680 --> 00:09:23,519
basically are kind of dropping random

00:09:21,600 --> 00:09:25,200
metrics

00:09:23,519 --> 00:09:27,040
uh we're going to first we're going to

00:09:25,200 --> 00:09:30,080
talk about both of these but first we're

00:09:27,040 --> 00:09:33,600
going to dive into roll-up rules

00:09:30,080 --> 00:09:34,839
roll-up rules are a way to aggregate

00:09:33,600 --> 00:09:37,360
metrics

00:09:34,839 --> 00:09:40,959
so they basically have a series of

00:09:37,360 --> 00:09:44,480
transforms which are applied in order

00:09:40,959 --> 00:09:45,710
uh to kind of change and generate a new

00:09:44,480 --> 00:09:46,880
metric

00:09:45,710 --> 00:09:48,959
[Music]

00:09:46,880 --> 00:09:50,800
and the metrics kind of applied to

00:09:48,959 --> 00:09:52,560
depend on kind of what filter

00:09:50,800 --> 00:09:54,640
actually matches it also has something

00:09:52,560 --> 00:09:57,440
called a storage policy which

00:09:54,640 --> 00:09:58,399
basically determines like where like

00:09:57,440 --> 00:10:00,800
what

00:09:58,399 --> 00:10:03,600
like where to store the generated time

00:10:00,800 --> 00:10:05,360
series in m3db

00:10:03,600 --> 00:10:07,120
we're talking about kind of one by one

00:10:05,360 --> 00:10:07,839
uh the first step is kind of what we

00:10:07,120 --> 00:10:10,560
call

00:10:07,839 --> 00:10:12,000
like a transform an increase or a delta

00:10:10,560 --> 00:10:13,680
transform

00:10:12,000 --> 00:10:14,959
uh the underlying metrics which come

00:10:13,680 --> 00:10:16,800
from prometheus are kind of

00:10:14,959 --> 00:10:20,160
monotonically increasing

00:10:16,800 --> 00:10:21,200
like counters so we can't really

00:10:20,160 --> 00:10:24,079
aggregate them as

00:10:21,200 --> 00:10:25,680
is so we need to apply the equivalent of

00:10:24,079 --> 00:10:26,959
kind of the prometheus rate function

00:10:25,680 --> 00:10:30,399
first

00:10:26,959 --> 00:10:31,440
so that's exactly what the increased

00:10:30,399 --> 00:10:33,200
transform is

00:10:31,440 --> 00:10:35,440
essentially applies the prometheus rate

00:10:33,200 --> 00:10:36,880
function gets diffs between successive

00:10:35,440 --> 00:10:39,120
data points

00:10:36,880 --> 00:10:40,640
and kind of generates a new time series

00:10:39,120 --> 00:10:43,200
for kind of use

00:10:40,640 --> 00:10:44,720
by the next level of the transform the

00:10:43,200 --> 00:10:47,360
next level of transform

00:10:44,720 --> 00:10:48,160
is called the rollup it essentially sums

00:10:47,360 --> 00:10:50,079
the deltas

00:10:48,160 --> 00:10:51,200
by the unique dimensions specified in

00:10:50,079 --> 00:10:52,560
the group by

00:10:51,200 --> 00:10:54,959
in this case we're only interested in

00:10:52,560 --> 00:10:57,760
container name and namespace so it does

00:10:54,959 --> 00:10:58,399
a sum by container container namespace

00:10:57,760 --> 00:11:01,519
in

00:10:58,399 --> 00:11:04,720
like fromql language

00:11:01,519 --> 00:11:07,200
and it stores the generated thing

00:11:04,720 --> 00:11:08,480
as a different metric name now the

00:11:07,200 --> 00:11:12,160
metric name

00:11:08,480 --> 00:11:15,040
is interesting uh if you're actually

00:11:12,160 --> 00:11:17,760
using a mapping rule to

00:11:15,040 --> 00:11:18,720
drop the original metrics then we can

00:11:17,760 --> 00:11:21,760
basically

00:11:18,720 --> 00:11:22,880
store the rolled up metric as with the

00:11:21,760 --> 00:11:27,279
same metric name

00:11:22,880 --> 00:11:29,519
as like as the original metric

00:11:27,279 --> 00:11:31,360
if you're not planning to kind of drop

00:11:29,519 --> 00:11:35,040
the underlying metrics then this metric

00:11:31,360 --> 00:11:37,360
needs to be different just like it's in

00:11:35,040 --> 00:11:38,560
in the recording rule case but the

00:11:37,360 --> 00:11:40,160
advantage of actually

00:11:38,560 --> 00:11:42,320
dropping the metrics and storing the

00:11:40,160 --> 00:11:44,320
metrics using the same name

00:11:42,320 --> 00:11:46,800
is that dashboards alerts or anything

00:11:44,320 --> 00:11:49,680
which is actually querying these metrics

00:11:46,800 --> 00:11:50,240
now just query the aggregate rather than

00:11:49,680 --> 00:11:52,800
the

00:11:50,240 --> 00:11:55,600
high dimensional metric so that's

00:11:52,800 --> 00:11:57,120
something to keep in mind

00:11:55,600 --> 00:11:58,560
now finally like after we've rolled

00:11:57,120 --> 00:11:59,440
things up and we've kind of reduced the

00:11:58,560 --> 00:12:02,399
dimensionality

00:11:59,440 --> 00:12:04,639
we want to store this back into the tstb

00:12:02,399 --> 00:12:07,519
so we need to go back from

00:12:04,639 --> 00:12:08,800
dealing with deltas to actual kind of

00:12:07,519 --> 00:12:10,800
accumulatively

00:12:08,800 --> 00:12:12,639
like increasing like monotonically

00:12:10,800 --> 00:12:15,120
increasing time series

00:12:12,639 --> 00:12:16,720
so we apply kind of a cumulative add

00:12:15,120 --> 00:12:19,680
operation

00:12:16,720 --> 00:12:20,320
uh for each of the metrics uh to kind of

00:12:19,680 --> 00:12:22,399
get the

00:12:20,320 --> 00:12:25,440
aggregated like monotonically increasing

00:12:22,399 --> 00:12:27,440
time series

00:12:25,440 --> 00:12:29,519
this is basically sent to m3db

00:12:27,440 --> 00:12:30,639
namespaces identified kind of by the

00:12:29,519 --> 00:12:34,560
storage policies

00:12:30,639 --> 00:12:37,120
and things get stored there

00:12:34,560 --> 00:12:38,000
now as mentioned before like if you

00:12:37,120 --> 00:12:40,560
actually want to go

00:12:38,000 --> 00:12:42,240
and store the metric kind of with the

00:12:40,560 --> 00:12:45,279
same name as the original

00:12:42,240 --> 00:12:47,519
like original metric then we need a way

00:12:45,279 --> 00:12:50,000
to kind of drop the

00:12:47,519 --> 00:12:51,120
like the raw like high dimensional

00:12:50,000 --> 00:12:53,040
metrics

00:12:51,120 --> 00:12:54,160
uh this is where kind of mapping rules

00:12:53,040 --> 00:12:56,639
come into play

00:12:54,160 --> 00:12:58,399
so mapping rules also have a filter uh

00:12:56,639 --> 00:12:59,279
which basically says oh these are the

00:12:58,399 --> 00:13:02,480
metrics that

00:12:59,279 --> 00:13:04,800
i'm actually interested in um

00:13:02,480 --> 00:13:06,000
match these metrics and just drop them

00:13:04,800 --> 00:13:08,079
do not store them

00:13:06,000 --> 00:13:09,760
so the roll-up rules would still apply

00:13:08,079 --> 00:13:11,760
but they won't actually get stored to

00:13:09,760 --> 00:13:13,360
the tsdb

00:13:11,760 --> 00:13:15,360
so the combination of roll-up rules and

00:13:13,360 --> 00:13:18,480
mapping rules uh

00:13:15,360 --> 00:13:19,360
we can get like aggregation like really

00:13:18,480 --> 00:13:22,639
easily

00:13:19,360 --> 00:13:24,639
like at ingestion time uh so to

00:13:22,639 --> 00:13:26,480
summarize kind of the aggregation here

00:13:24,639 --> 00:13:29,440
uh it basically allows for kind of

00:13:26,480 --> 00:13:32,079
ingestion time streaming aggregation

00:13:29,440 --> 00:13:33,920
uh the metrics can be aggregated or

00:13:32,079 --> 00:13:35,680
rolled up based on like whatever rules

00:13:33,920 --> 00:13:38,959
you provide

00:13:35,680 --> 00:13:41,440
different functions are possible

00:13:38,959 --> 00:13:42,800
and i think the key thing really is that

00:13:41,440 --> 00:13:45,279
there's a way to kind of

00:13:42,800 --> 00:13:46,880
like drop the raw metrics based on these

00:13:45,279 --> 00:13:50,079
matching filters

00:13:46,880 --> 00:13:51,279
which gives us the like the great

00:13:50,079 --> 00:13:53,199
characteristic that

00:13:51,279 --> 00:13:57,040
dashboards and alerts and different

00:13:53,199 --> 00:13:59,519
queries could automatically sped up

00:13:57,040 --> 00:14:00,320
uh so we really have like two ways of

00:13:59,519 --> 00:14:02,480
doing things

00:14:00,320 --> 00:14:04,079
uh you just want to quickly like go over

00:14:02,480 --> 00:14:05,680
pros and cons of recording rules and

00:14:04,079 --> 00:14:08,000
roll-up rules

00:14:05,680 --> 00:14:09,279
so recording rules are generally like

00:14:08,000 --> 00:14:12,560
their general purpose

00:14:09,279 --> 00:14:14,959
and they support full prom ql so uh

00:14:12,560 --> 00:14:16,079
if you have this like super complicated

00:14:14,959 --> 00:14:18,320
like expensive query

00:14:16,079 --> 00:14:21,920
which we want to speed up then recording

00:14:18,320 --> 00:14:21,920
rules are probably the way to go

00:14:22,000 --> 00:14:25,680
the caveat is that they're expensive

00:14:24,320 --> 00:14:27,040
because they run against the regular

00:14:25,680 --> 00:14:28,839
query engine

00:14:27,040 --> 00:14:30,399
and they also kind of affect other

00:14:28,839 --> 00:14:33,519
queries

00:14:30,399 --> 00:14:35,519
so you're actually crying like if you

00:14:33,519 --> 00:14:38,399
have a recording rule going

00:14:35,519 --> 00:14:40,959
and like aggregating across 20 000 time

00:14:38,399 --> 00:14:42,880
series recording rule every minute or

00:14:40,959 --> 00:14:44,720
every interval goes and queries those 20

00:14:42,880 --> 00:14:47,440
000 time series and does aggregates and

00:14:44,720 --> 00:14:47,440
stores them back

00:14:48,160 --> 00:14:52,320
and the third kind of yeah i guess

00:14:50,880 --> 00:14:54,000
bigger point is that

00:14:52,320 --> 00:14:55,600
all the data needs to be stored so

00:14:54,000 --> 00:14:56,839
there's a very high storage cost you

00:14:55,600 --> 00:15:00,079
cannot

00:14:56,839 --> 00:15:02,160
um like with recording rule you will

00:15:00,079 --> 00:15:04,480
store the low dimensionality

00:15:02,160 --> 00:15:06,160
information and high dimension and

00:15:04,480 --> 00:15:07,920
aggregate information at the same

00:15:06,160 --> 00:15:10,560
like high dimensionality information and

00:15:07,920 --> 00:15:12,399
aggregate information at the same time

00:15:10,560 --> 00:15:14,480
which also means that kind of your

00:15:12,399 --> 00:15:14,959
dashboards and queries and alerts need

00:15:14,480 --> 00:15:18,399
to

00:15:14,959 --> 00:15:20,880
be modified to actually like hit the

00:15:18,399 --> 00:15:21,680
uh hit the aggregated metric instead of

00:15:20,880 --> 00:15:25,760
the

00:15:21,680 --> 00:15:28,480
like the original metrics

00:15:25,760 --> 00:15:30,160
rules because of the way they happen at

00:15:28,480 --> 00:15:32,480
ingestion time are much more efficient

00:15:30,160 --> 00:15:32,480
to run

00:15:32,560 --> 00:15:36,079
we have the option of only destroying

00:15:34,320 --> 00:15:38,800
the aggregates we need and dropping

00:15:36,079 --> 00:15:40,959
other series

00:15:38,800 --> 00:15:42,560
and if we go the route of only storing

00:15:40,959 --> 00:15:43,360
the aggregates then you kind of get

00:15:42,560 --> 00:15:46,800
automatic

00:15:43,360 --> 00:15:48,399
query speed up uh because aggregate ends

00:15:46,800 --> 00:15:51,680
up having the same metric name

00:15:48,399 --> 00:15:53,839
as the result matrix the biggest kind of

00:15:51,680 --> 00:15:54,800
caveat with roll-up rules is they don't

00:15:53,839 --> 00:15:58,639
support full prom

00:15:54,800 --> 00:16:00,959
ql but rather like specific aggregates

00:15:58,639 --> 00:16:02,880
we are we have been kind of adding more

00:16:00,959 --> 00:16:05,759
and more of these

00:16:02,880 --> 00:16:06,160
uh aggregates aggregate functions over

00:16:05,759 --> 00:16:09,360
time

00:16:06,160 --> 00:16:11,040
but it's unlikely that recording like

00:16:09,360 --> 00:16:13,680
grow up rules would support everything

00:16:11,040 --> 00:16:16,000
that recordings would support

00:16:13,680 --> 00:16:16,959
so now that we have these like two

00:16:16,000 --> 00:16:18,720
things

00:16:16,959 --> 00:16:20,240
like two ways of kind of aggregating

00:16:18,720 --> 00:16:23,839
things like

00:16:20,240 --> 00:16:27,360
how do we kind of make it easy uh

00:16:23,839 --> 00:16:30,240
to do these aggregations so we're gonna

00:16:27,360 --> 00:16:31,759
talk about kind of uh two we're gonna

00:16:30,240 --> 00:16:32,880
talk about kind of a couple of different

00:16:31,759 --> 00:16:36,560
things

00:16:32,880 --> 00:16:37,920
uh around like uh

00:16:36,560 --> 00:16:39,920
so we're going to talk about this tool

00:16:37,920 --> 00:16:41,360
which we call the high cardinality

00:16:39,920 --> 00:16:44,880
analyzer

00:16:41,360 --> 00:16:46,720
which makes it easy um

00:16:44,880 --> 00:16:49,120
yeah which basically makes it easy to go

00:16:46,720 --> 00:16:51,839
and like

00:16:49,120 --> 00:16:51,839
do this

00:16:52,320 --> 00:16:55,519
yeah which basically makes it easy to

00:16:53,839 --> 00:16:58,639
kind of go and do this analysis

00:16:55,519 --> 00:17:01,199
and kind of like create these recording

00:16:58,639 --> 00:17:05,120
rules and roll up rules

00:17:01,199 --> 00:17:09,039
uh so i'm gonna share screen again here

00:17:05,120 --> 00:17:10,160
so this is actually available as a tool

00:17:09,039 --> 00:17:12,959
on github

00:17:10,160 --> 00:17:16,400
the link to this is posted sometime like

00:17:12,959 --> 00:17:18,640
in the later part of the

00:17:16,400 --> 00:17:19,600
yeah in the latter part of the talk so

00:17:18,640 --> 00:17:22,000
we have

00:17:19,600 --> 00:17:23,280
um it we're basically going to make use

00:17:22,000 --> 00:17:25,439
of the prometheus

00:17:23,280 --> 00:17:26,559
like query logs and we're going to

00:17:25,439 --> 00:17:28,319
analyze them

00:17:26,559 --> 00:17:30,640
and we're going to do like some

00:17:28,319 --> 00:17:32,640
operations to actually go and like

00:17:30,640 --> 00:17:34,240
speed things up the purpose of this

00:17:32,640 --> 00:17:36,960
example

00:17:34,240 --> 00:17:37,919
i emitted some metrics locally just to

00:17:36,960 --> 00:17:40,960
kind of get

00:17:37,919 --> 00:17:42,720
like some basic information uh

00:17:40,960 --> 00:17:44,720
for the purposes of this demo so it's

00:17:42,720 --> 00:17:46,960
not really high cardinality information

00:17:44,720 --> 00:17:48,480
with too many series it's just a handful

00:17:46,960 --> 00:17:52,080
of queries

00:17:48,480 --> 00:17:52,559
um so what does the query log actually

00:17:52,080 --> 00:17:55,679
have

00:17:52,559 --> 00:17:57,840
it has some query information here

00:17:55,679 --> 00:17:59,600
of what the exact query was when it

00:17:57,840 --> 00:18:01,440
started

00:17:59,600 --> 00:18:02,880
and then different stats like how long

00:18:01,440 --> 00:18:05,039
it took to eval

00:18:02,880 --> 00:18:06,080
how long it took to sort like that query

00:18:05,039 --> 00:18:09,360
preparation time

00:18:06,080 --> 00:18:13,440
any inner evaluation and so on

00:18:09,360 --> 00:18:16,880
um so kind of just to run this

00:18:13,440 --> 00:18:19,039
uh we have like this high chronic

00:18:16,880 --> 00:18:21,200
analyzer we point it to a sample query

00:18:19,039 --> 00:18:23,039
log

00:18:21,200 --> 00:18:24,640
we give it some target saying i'm only

00:18:23,039 --> 00:18:26,840
interested in queries which have kind of

00:18:24,640 --> 00:18:28,000
a minimum query time of like 0.01

00:18:26,840 --> 00:18:31,840
seconds

00:18:28,000 --> 00:18:34,000
because it's a smaller example

00:18:31,840 --> 00:18:35,840
and then we have kind of oh i want to

00:18:34,000 --> 00:18:38,720
only like filter out queries which

00:18:35,840 --> 00:18:40,240
happen at least two times so this

00:18:38,720 --> 00:18:43,919
actually shows you

00:18:40,240 --> 00:18:46,880
some like some options here uh

00:18:43,919 --> 00:18:48,559
then the query log like the analyzer has

00:18:46,880 --> 00:18:50,400
this way of like generating recording

00:18:48,559 --> 00:18:53,919
rules for these queries that

00:18:50,400 --> 00:18:56,320
we identified uh and we actually see

00:18:53,919 --> 00:18:57,760
recording rules here

00:18:56,320 --> 00:18:59,600
and it has another mode where you kind

00:18:57,760 --> 00:19:02,640
of can generate roll-up rules

00:18:59,600 --> 00:19:03,440
and generate mapping rules so what

00:19:02,640 --> 00:19:07,039
exactly

00:19:03,440 --> 00:19:10,400
is this like analyzer doing so let's

00:19:07,039 --> 00:19:12,559
jump back to the slides kind of talk

00:19:10,400 --> 00:19:15,600
about that

00:19:12,559 --> 00:19:17,280
so analyzer like primarily kind of uses

00:19:15,600 --> 00:19:19,520
the prometheus query logs

00:19:17,280 --> 00:19:22,720
it logs all of these queries kind of

00:19:19,520 --> 00:19:25,360
like run by the engine

00:19:22,720 --> 00:19:26,160
and the query log has information about

00:19:25,360 --> 00:19:30,000
kind of where

00:19:26,160 --> 00:19:32,000
time was spent in the query so analyzer

00:19:30,000 --> 00:19:33,039
is kind of an offline process a

00:19:32,000 --> 00:19:34,720
standalone tool

00:19:33,039 --> 00:19:36,720
to generate recording android roll-up

00:19:34,720 --> 00:19:38,080
rules um

00:19:36,720 --> 00:19:39,760
as mentioned it uses the prometheus

00:19:38,080 --> 00:19:41,120
square log to find good candidates for

00:19:39,760 --> 00:19:43,840
aggregation

00:19:41,120 --> 00:19:45,360
uh and it provides some recommendations

00:19:43,840 --> 00:19:47,919
for recording rules

00:19:45,360 --> 00:19:51,039
or kind of m3 aggregator roll up and

00:19:47,919 --> 00:19:54,080
mapping rules to create

00:19:51,039 --> 00:19:55,600
and speed up expensive queries uh so it

00:19:54,080 --> 00:19:57,760
just provides recommendations and then

00:19:55,600 --> 00:19:59,520
users kind of have the option of taking

00:19:57,760 --> 00:20:02,159
these recommendations and going and

00:19:59,520 --> 00:20:04,840
creating the actual rules themselves

00:20:02,159 --> 00:20:07,440
uh then kind of speeding up the

00:20:04,840 --> 00:20:10,960
dashboards so a few steps

00:20:07,440 --> 00:20:13,120
like we would do is you basically

00:20:10,960 --> 00:20:14,880
have like days worth of prometheus query

00:20:13,120 --> 00:20:16,559
logs

00:20:14,880 --> 00:20:18,320
so we have enough information to know

00:20:16,559 --> 00:20:19,360
like which are the repeated queries

00:20:18,320 --> 00:20:21,039
which

00:20:19,360 --> 00:20:22,799
repeated expensive queries which are

00:20:21,039 --> 00:20:24,720
being run on the system

00:20:22,799 --> 00:20:27,200
so we want to kind of find these most

00:20:24,720 --> 00:20:30,640
commonly hit expensive queries

00:20:27,200 --> 00:20:32,720
uh once we kind of get information about

00:20:30,640 --> 00:20:34,159
that we want to kind of check the cost

00:20:32,720 --> 00:20:36,720
of these queries are due to number of

00:20:34,159 --> 00:20:38,080
series so the cost of a query could be

00:20:36,720 --> 00:20:38,799
due to multiple different reasons it

00:20:38,080 --> 00:20:40,640
could just be

00:20:38,799 --> 00:20:42,159
because it's very important like large

00:20:40,640 --> 00:20:45,520
chunks of data

00:20:42,159 --> 00:20:47,200
uh large chunks of like a few series

00:20:45,520 --> 00:20:48,559
essentially over like a huge like

00:20:47,200 --> 00:20:50,960
sequence of time

00:20:48,559 --> 00:20:52,480
or it could be like creating many many

00:20:50,960 --> 00:20:57,840
series of data

00:20:52,480 --> 00:21:00,400
uh like yeah many series of data

00:20:57,840 --> 00:21:01,840
and the cost like even when it queries

00:21:00,400 --> 00:21:03,520
many series of data if it's actually

00:21:01,840 --> 00:21:05,360
returning all the data then you can't

00:21:03,520 --> 00:21:08,480
really speed things up

00:21:05,360 --> 00:21:10,480
but if it's

00:21:08,480 --> 00:21:11,919
actually acquiring many underlying

00:21:10,480 --> 00:21:14,640
series from the

00:21:11,919 --> 00:21:15,600
tstb and then it's kind of aggregating

00:21:14,640 --> 00:21:18,400
them together

00:21:15,600 --> 00:21:19,760
into like a fewer set of series like

00:21:18,400 --> 00:21:21,919
those are kind of the

00:21:19,760 --> 00:21:23,520
candidates like candidate queries we're

00:21:21,919 --> 00:21:25,679
actually looking for

00:21:23,520 --> 00:21:27,760
so we want to basically look at kind of

00:21:25,679 --> 00:21:30,320
like

00:21:27,760 --> 00:21:31,280
the cardinality of the queries which are

00:21:30,320 --> 00:21:34,080
actually being run

00:21:31,280 --> 00:21:36,720
and kind of use that and only optimize

00:21:34,080 --> 00:21:36,720
those queries

00:21:37,039 --> 00:21:44,320
so once it kind of identifies these

00:21:40,799 --> 00:21:47,840
these queries which actually need to be

00:21:44,320 --> 00:21:49,919
need to be need to be sped up uh

00:21:47,840 --> 00:21:52,240
we provide proposals for kind of

00:21:49,919 --> 00:21:55,280
recording and roll-up rules to create

00:21:52,240 --> 00:21:57,360
um and then users are kind of like free

00:21:55,280 --> 00:21:59,840
to go and like configure these rules as

00:21:57,360 --> 00:21:59,840
necessary

00:22:00,640 --> 00:22:04,640
if the user kind of goes and creates

00:22:02,080 --> 00:22:06,159
recording rules then dashboard and other

00:22:04,640 --> 00:22:08,720
places where these

00:22:06,159 --> 00:22:10,480
rules are being applied need to be

00:22:08,720 --> 00:22:12,159
changed

00:22:10,480 --> 00:22:14,400
if we're talking about kind of roll-up

00:22:12,159 --> 00:22:17,039
rules then

00:22:14,400 --> 00:22:18,000
and the user is already gone and like

00:22:17,039 --> 00:22:19,360
the user also

00:22:18,000 --> 00:22:21,919
said that they want to kind of drop the

00:22:19,360 --> 00:22:24,559
underlying metrics then the queries will

00:22:21,919 --> 00:22:25,919
kind of get sped up automatically

00:22:24,559 --> 00:22:28,240
as the query actually captures the

00:22:25,919 --> 00:22:30,159
aggregated metric um

00:22:28,240 --> 00:22:31,600
in case the roll-up rule is not dropping

00:22:30,159 --> 00:22:34,880
the underlying series

00:22:31,600 --> 00:22:36,320
then like um then you'd

00:22:34,880 --> 00:22:38,320
like recording rules you'll have to go

00:22:36,320 --> 00:22:40,720
and kind of like change

00:22:38,320 --> 00:22:42,480
uh change dashboards and other places

00:22:40,720 --> 00:22:44,799
where things have run

00:22:42,480 --> 00:22:46,880
uh so we built this tool um it's

00:22:44,799 --> 00:22:47,840
available open source the link to this

00:22:46,880 --> 00:22:51,760
is in the next

00:22:47,840 --> 00:22:54,640
like in like the last last slide

00:22:51,760 --> 00:22:54,960
um so yeah we encourage you to go and

00:22:54,640 --> 00:22:58,240
like

00:22:54,960 --> 00:23:00,320
try this out and let us know kind of how

00:22:58,240 --> 00:23:03,360
this works out for you

00:23:00,320 --> 00:23:07,679
uh so thank you so much

00:23:03,360 --> 00:23:09,280
um we're open to questions now

00:23:07,679 --> 00:23:11,039
if you want to kind of know more about

00:23:09,280 --> 00:23:12,799
kind of the m3 aggregation tier like

00:23:11,039 --> 00:23:14,880
reach out do reach out to us on the m3

00:23:12,799 --> 00:23:16,320
slack channel

00:23:14,880 --> 00:23:20,320
and there's a link to kind of the high

00:23:16,320 --> 00:23:20,320
cardinality analyzer available here

00:23:20,720 --> 00:23:24,000
yeah there's a link to that on an

00:23:22,000 --> 00:23:25,280
analyzer available for you to kind of

00:23:24,000 --> 00:23:28,320
try out

00:23:25,280 --> 00:23:28,320

YouTube URL: https://www.youtube.com/watch?v=wLSFPujH6lk


