Title: Managing Cloud Native Artifacts for Large Scale Kubernetes Cluster - Henry Zhang & Mingming Pei
Publication date: 2020-11-23
Playlist: KubeCon + CloudNativeCon North America 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon Europe 2021 Virtual from May 4–7, 2021. Learn more at https://kubecon.io. The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects. 

Managing Cloud Native Artifacts for Large Scale Kubernetes Cluster - Henry Zhang, VMware & Mingming Pei, Netease 

When managing artifacts like container images and Helm charts for cloud native apps, users often face challenges such as efficiently publishing applications to Kubernetes cluster at scale, enforcing access control, identifying image vulnerabilities and backing up cloud native artifacts.  In this talk, we introduce real-world use cases for managing images and Helm charts in CI/CD pipeline and production environment. The cases are summarized from production users of Kubernetes, including these aspects:  1. Setting up a highly available and scale-out registry service for a Kubernetes cluster 2. Synchronizing images and Helm charts of cloud native applications across multiple cloud environments or data centers 3. Preventing images with vulnerabilities from getting into production 4. Using P2P approach to efficiently distribute images in a large cluster 5. Integrated management in CI/CD  

https://sched.co/etWq
Captions: 
	00:00:00,960 --> 00:00:07,200
hello everyone welcome to our session

00:00:04,560 --> 00:00:11,200
managing cloud-native artifacts for

00:00:07,200 --> 00:00:11,200
large-scale kubernetes clusters

00:00:11,759 --> 00:00:17,520
i'm henry zhang technical director of

00:00:14,639 --> 00:00:19,600
cloud native lab we are with china r d

00:00:17,520 --> 00:00:21,119
i'm the creator and maintainer of

00:00:19,600 --> 00:00:24,320
harvard

00:00:21,119 --> 00:00:25,359
my interest is in cloud computing ai

00:00:24,320 --> 00:00:28,800
machine learning

00:00:25,359 --> 00:00:29,519
and blockchain today with me is min wing

00:00:28,800 --> 00:00:32,719
pay

00:00:29,519 --> 00:00:34,320
architect from netease he's responsible

00:00:32,719 --> 00:00:37,520
for ching zhou cloud native

00:00:34,320 --> 00:00:40,559
that off platform he's also a

00:00:37,520 --> 00:00:40,559
harbor maintainer

00:00:41,680 --> 00:00:47,360
here is today's agenda we first

00:00:44,960 --> 00:00:49,760
talk about the two aspects of managing

00:00:47,360 --> 00:00:52,640
cloud native applications

00:00:49,760 --> 00:00:55,360
then we introduce how to use harbor to

00:00:52,640 --> 00:00:58,079
manage cloud-native artifacts

00:00:55,360 --> 00:00:58,960
next we'll go through the case study of

00:00:58,079 --> 00:01:01,920
netease

00:00:58,960 --> 00:01:02,960
and how they manage artifacts for

00:01:01,920 --> 00:01:07,360
large-scale

00:01:02,960 --> 00:01:07,360
kubernetes cluster using harbour

00:01:09,200 --> 00:01:12,560
we all know that cloud-native

00:01:11,520 --> 00:01:15,040
technologies

00:01:12,560 --> 00:01:17,119
become increasingly important to build

00:01:15,040 --> 00:01:19,439
modern applications

00:01:17,119 --> 00:01:22,080
usually there are two aspects in

00:01:19,439 --> 00:01:25,360
managing cloud-native applications

00:01:22,080 --> 00:01:26,640
the first is the dynamic part that is

00:01:25,360 --> 00:01:29,119
the run time

00:01:26,640 --> 00:01:29,840
how the applications run and how they

00:01:29,119 --> 00:01:35,360
are

00:01:29,840 --> 00:01:35,360
scaled up monitored backed up and so on

00:01:35,520 --> 00:01:42,720
the second step is the static part

00:01:39,200 --> 00:01:43,759
that is artifacts when applications are

00:01:42,720 --> 00:01:46,720
not running

00:01:43,759 --> 00:01:47,600
they usually reside on storage device as

00:01:46,720 --> 00:01:51,119
files

00:01:47,600 --> 00:01:51,119
or some kind of artifacts

00:01:51,439 --> 00:01:55,200
the most common and important cognitive

00:01:54,240 --> 00:01:58,320
artifacts

00:01:55,200 --> 00:02:00,799
are container images ham trusts

00:01:58,320 --> 00:02:02,320
and so on but usually there could be

00:02:00,799 --> 00:02:04,000
more like cnav

00:02:02,320 --> 00:02:09,280
and so on to be managed in a

00:02:04,000 --> 00:02:11,840
cloud-native environment

00:02:09,280 --> 00:02:13,120
given the importance of artifacts we

00:02:11,840 --> 00:02:15,520
need to efficiently

00:02:13,120 --> 00:02:18,640
and securely manage artifacts when

00:02:15,520 --> 00:02:22,160
operating a cloud-native platform

00:02:18,640 --> 00:02:25,040
and however a graduated cncf project

00:02:22,160 --> 00:02:27,360
is designed to perform the tasks of

00:02:25,040 --> 00:02:30,720
managing cloud-native artifacts

00:02:27,360 --> 00:02:31,519
it supports oci artifacts like docker

00:02:30,720 --> 00:02:35,040
image

00:02:31,519 --> 00:02:37,519
ham charge cnap

00:02:35,040 --> 00:02:39,680
open policy agents singularities and so

00:02:37,519 --> 00:02:39,680
on

00:02:39,840 --> 00:02:46,640
in addition harvard provides a bunch of

00:02:43,440 --> 00:02:49,599
features on artifacts management such as

00:02:46,640 --> 00:02:51,760
rbac low base access control image

00:02:49,599 --> 00:02:56,640
isolation by project

00:02:51,760 --> 00:03:00,239
image retention and immutable images

00:02:56,640 --> 00:03:00,239
we will cover some of them shortly

00:03:02,800 --> 00:03:06,400
the first artifact management feature

00:03:05,040 --> 00:03:10,480
i'd like to talk about

00:03:06,400 --> 00:03:14,319
is replication this feature was created

00:03:10,480 --> 00:03:17,040
in harvest early version 0.3

00:03:14,319 --> 00:03:18,720
it allows two hub instances to

00:03:17,040 --> 00:03:21,760
synchronize the images

00:03:18,720 --> 00:03:23,200
from one to the other because the

00:03:21,760 --> 00:03:25,120
replication tasks

00:03:23,200 --> 00:03:26,400
are carried out automatically and

00:03:25,120 --> 00:03:28,799
reliably

00:03:26,400 --> 00:03:31,680
users love and apply this feature in

00:03:28,799 --> 00:03:34,799
many scenarios

00:03:31,680 --> 00:03:36,879
in the latest release of harvard it can

00:03:34,799 --> 00:03:40,000
support replications of artifacts

00:03:36,879 --> 00:03:43,200
across multiple cloud environments

00:03:40,000 --> 00:03:45,680
with various registry and services

00:03:43,200 --> 00:03:47,040
we list some refugee services here such

00:03:45,680 --> 00:03:50,799
as docker hub

00:03:47,040 --> 00:03:54,640
refugee service in public cloud like aws

00:03:50,799 --> 00:03:56,959
google azure article and so on

00:03:54,640 --> 00:03:58,080
it is very simple to move your artifacts

00:03:56,959 --> 00:04:00,480
between different

00:03:58,080 --> 00:04:03,760
environments this will help a lot when

00:04:00,480 --> 00:04:03,760
managing your artifacts

00:04:05,200 --> 00:04:08,319
suppose you are running a large cluster

00:04:07,200 --> 00:04:11,280
with many

00:04:08,319 --> 00:04:13,280
nodes if all those cool image from the

00:04:11,280 --> 00:04:14,959
public registry services

00:04:13,280 --> 00:04:17,600
it will take up a lot of networking

00:04:14,959 --> 00:04:19,199
bandwidth to download the same images

00:04:17,600 --> 00:04:22,560
again and again

00:04:19,199 --> 00:04:25,199
it is obvious not optimal

00:04:22,560 --> 00:04:26,080
moreover not all nodes within

00:04:25,199 --> 00:04:28,080
organizations

00:04:26,080 --> 00:04:29,440
are allowed to connect to the external

00:04:28,080 --> 00:04:32,160
registry

00:04:29,440 --> 00:04:34,320
so harvard provides a feature called

00:04:32,160 --> 00:04:36,960
proxy cache

00:04:34,320 --> 00:04:38,400
this feature has been requested by many

00:04:36,960 --> 00:04:41,840
community users

00:04:38,400 --> 00:04:43,120
for quite some time it is released in

00:04:41,840 --> 00:04:45,520
harbor 2.1

00:04:43,120 --> 00:04:46,720
recently it is a special kind of

00:04:45,520 --> 00:04:49,360
hardware project

00:04:46,720 --> 00:04:51,600
which can hold catch the images and

00:04:49,360 --> 00:04:54,000
serve them locally

00:04:51,600 --> 00:04:55,120
it saves external networking bandwidth

00:04:54,000 --> 00:04:58,560
and speed up the

00:04:55,120 --> 00:05:00,800
local distribution of images

00:04:58,560 --> 00:05:01,759
under the hood it leverages replication

00:05:00,800 --> 00:05:04,160
capability

00:05:01,759 --> 00:05:05,120
of hardware to pull images from remote

00:05:04,160 --> 00:05:08,080
sources

00:05:05,120 --> 00:05:08,960
when they are not available locally

00:05:08,080 --> 00:05:11,440
because

00:05:08,960 --> 00:05:12,720
cached images are stored under a

00:05:11,440 --> 00:05:14,639
hardware project

00:05:12,720 --> 00:05:15,759
all the project related features in

00:05:14,639 --> 00:05:19,440
hardware such as

00:05:15,759 --> 00:05:20,160
quota scanning immutable tag can all be

00:05:19,440 --> 00:05:25,840
applied to

00:05:20,160 --> 00:05:25,840
cached images

00:05:26,080 --> 00:05:30,240
for any registered service in production

00:05:28,960 --> 00:05:33,600
high availability

00:05:30,240 --> 00:05:36,240
and scalability must be considered

00:05:33,600 --> 00:05:38,320
there are many ways to achieve ha of a

00:05:36,240 --> 00:05:41,600
registry service

00:05:38,320 --> 00:05:44,320
i just introduced some principles here

00:05:41,600 --> 00:05:44,960
which can be the guidelines for users to

00:05:44,320 --> 00:05:48,880
implement

00:05:44,960 --> 00:05:50,720
a production registry the hardware core

00:05:48,880 --> 00:05:53,520
services

00:05:50,720 --> 00:05:55,440
components are stainless which means

00:05:53,520 --> 00:05:58,160
that they can be scaled out

00:05:55,440 --> 00:05:59,600
by running multiple instances of each

00:05:58,160 --> 00:06:02,800
component

00:05:59,600 --> 00:06:04,840
the key here is to set up h a for

00:06:02,800 --> 00:06:08,960
persistent services like

00:06:04,840 --> 00:06:12,000
postgresql radish and shared storage

00:06:08,960 --> 00:06:14,880
there are many existing solutions for

00:06:12,000 --> 00:06:15,759
these ha services just through one of

00:06:14,880 --> 00:06:20,960
them

00:06:15,759 --> 00:06:22,880
that fits your environment

00:06:20,960 --> 00:06:24,000
if you have multiple data centers or

00:06:22,880 --> 00:06:27,039
cloud environments

00:06:24,000 --> 00:06:27,759
that's running your applications you can

00:06:27,039 --> 00:06:30,479
establish

00:06:27,759 --> 00:06:32,080
an ha harvard instance in each data

00:06:30,479 --> 00:06:35,120
center and environment

00:06:32,080 --> 00:06:37,680
so that they can back up each other

00:06:35,120 --> 00:06:39,280
the replication policy can be configured

00:06:37,680 --> 00:06:43,039
to synchronized

00:06:39,280 --> 00:06:46,960
artifacts between two environments also

00:06:43,039 --> 00:06:48,960
postgresql and radius can be

00:06:46,960 --> 00:06:50,240
cap in sync by using some kind of

00:06:48,960 --> 00:06:53,599
synchronization

00:06:50,240 --> 00:06:55,520
software or mechanism

00:06:53,599 --> 00:06:56,880
by adding load balancers in front of

00:06:55,520 --> 00:06:59,520
hardware services

00:06:56,880 --> 00:07:01,840
it provides high availability with an

00:06:59,520 --> 00:07:04,880
active standby configuration

00:07:01,840 --> 00:07:06,160
across two environments this creates

00:07:04,880 --> 00:07:09,360
additional protection

00:07:06,160 --> 00:07:09,840
for retrofit service if one data center

00:07:09,360 --> 00:07:13,120
or

00:07:09,840 --> 00:07:16,960
one environment goes down the other

00:07:13,120 --> 00:07:19,599
can go live to continue the service

00:07:16,960 --> 00:07:21,280
one thing to know is that the

00:07:19,599 --> 00:07:22,160
propagation delay of artifact

00:07:21,280 --> 00:07:24,080
replication

00:07:22,160 --> 00:07:25,919
between two data centers or two

00:07:24,080 --> 00:07:27,199
environments should be taken into

00:07:25,919 --> 00:07:30,720
consideration

00:07:27,199 --> 00:07:33,520
and when implementing such a solution

00:07:30,720 --> 00:07:34,800
that is to say artifacts stored in two

00:07:33,520 --> 00:07:36,720
data centers

00:07:34,800 --> 00:07:39,599
could be different when artifacts are

00:07:36,720 --> 00:07:39,599
being replicated

00:07:40,080 --> 00:07:44,400
when artifacts are deleted from a

00:07:41,919 --> 00:07:47,599
registry their storage space

00:07:44,400 --> 00:07:49,919
need to be released and retained

00:07:47,599 --> 00:07:51,280
this process is called garbage

00:07:49,919 --> 00:07:53,520
collection

00:07:51,280 --> 00:07:54,800
system administrators should perform

00:07:53,520 --> 00:07:57,520
garbage collection

00:07:54,800 --> 00:07:58,000
periodically to ensure the system does

00:07:57,520 --> 00:08:01,039
not run

00:07:58,000 --> 00:08:04,160
out of storage space

00:08:01,039 --> 00:08:05,280
in the latest harbor 2.1 release garbage

00:08:04,160 --> 00:08:07,759
collection

00:08:05,280 --> 00:08:09,120
feature is improved and can be performed

00:08:07,759 --> 00:08:13,120
without any impact

00:08:09,120 --> 00:08:15,599
of image pushing pooling and deletion

00:08:13,120 --> 00:08:16,240
this feature allows harbor to keep

00:08:15,599 --> 00:08:19,199
providing

00:08:16,240 --> 00:08:19,840
artifact service while doing backhand

00:08:19,199 --> 00:08:23,680
garbage

00:08:19,840 --> 00:08:24,879
cleanup this is very crucial for a

00:08:23,680 --> 00:08:28,960
production system

00:08:24,879 --> 00:08:28,960
to be up and running continuously

00:08:30,879 --> 00:08:35,680
when publishing a new version of the

00:08:34,000 --> 00:08:38,080
application to a cluster

00:08:35,680 --> 00:08:39,519
we need to send the artifacts to every

00:08:38,080 --> 00:08:42,479
node

00:08:39,519 --> 00:08:43,200
a large scale kubernetes cluster it is a

00:08:42,479 --> 00:08:45,920
challenging

00:08:43,200 --> 00:08:46,399
task to distribute artifacts to all

00:08:45,920 --> 00:08:50,000
nodes

00:08:46,399 --> 00:08:53,360
within a short time frame if there is

00:08:50,000 --> 00:08:54,959
only one registry instance servicing the

00:08:53,360 --> 00:08:58,000
entire cluster

00:08:54,959 --> 00:08:59,519
the registry instantly become the

00:08:58,000 --> 00:09:01,360
becomes the bottleneck of the

00:08:59,519 --> 00:09:03,760
distribution

00:09:01,360 --> 00:09:05,600
so to address this distribution problem

00:09:03,760 --> 00:09:07,760
in a large cluster

00:09:05,600 --> 00:09:09,040
peer-to-peer distribution dispute

00:09:07,760 --> 00:09:11,920
disputing approach

00:09:09,040 --> 00:09:13,680
seems a feasible solution hubble can

00:09:11,920 --> 00:09:16,800
leverage the capabilities

00:09:13,680 --> 00:09:20,839
of p2p engines like dragonfly

00:09:16,800 --> 00:09:22,160
and kraken to accelerate the artifacts

00:09:20,839 --> 00:09:24,720
distribution

00:09:22,160 --> 00:09:25,519
what harvard does is to preheat the p2p

00:09:24,720 --> 00:09:29,200
network

00:09:25,519 --> 00:09:30,880
or one of the cluster the idea is to

00:09:29,200 --> 00:09:32,240
distribute the artifacts to the

00:09:30,880 --> 00:09:34,080
peer-to-peer network

00:09:32,240 --> 00:09:35,760
before the requests of the artifacts

00:09:34,080 --> 00:09:38,480
arrive

00:09:35,760 --> 00:09:40,640
when the actual request comes in the

00:09:38,480 --> 00:09:42,080
content is ready for distribution within

00:09:40,640 --> 00:09:45,680
the p2p network

00:09:42,080 --> 00:09:48,480
and can be transferred right away

00:09:45,680 --> 00:09:50,240
in a case study of netease we can see

00:09:48,480 --> 00:09:53,120
that the p2p approach

00:09:50,240 --> 00:09:54,320
improves performance significantly in a

00:09:53,120 --> 00:09:57,760
large cluster

00:09:54,320 --> 00:09:57,760
of many kubernetes nodes

00:09:59,600 --> 00:10:03,360
harbor can work with multiple kubernetes

00:10:01,760 --> 00:10:05,680
clusters as well

00:10:03,360 --> 00:10:06,800
by setting up proper pre-heating

00:10:05,680 --> 00:10:09,839
policies

00:10:06,800 --> 00:10:11,040
harbor can send artifacts to each p2p

00:10:09,839 --> 00:10:13,040
cluster

00:10:11,040 --> 00:10:19,839
and make them ready for subsequent

00:10:13,040 --> 00:10:22,320
artifact distribution

00:10:19,839 --> 00:10:24,240
when administrators manage artifacts

00:10:22,320 --> 00:10:26,880
security is one thing

00:10:24,240 --> 00:10:28,399
they need to deal with harvard can help

00:10:26,880 --> 00:10:31,680
scan the content against

00:10:28,399 --> 00:10:32,720
publicly known cbe databases based on

00:10:31,680 --> 00:10:34,959
the

00:10:32,720 --> 00:10:36,320
scanning result hubble reports

00:10:34,959 --> 00:10:38,640
vulnerabilities

00:10:36,320 --> 00:10:41,440
found in the artifacts so that

00:10:38,640 --> 00:10:44,000
administrators can take proper action

00:10:41,440 --> 00:10:46,320
such as patching the image to remediate

00:10:44,000 --> 00:10:48,800
the vulnerability

00:10:46,320 --> 00:10:49,760
this is not only crucial in a production

00:10:48,800 --> 00:10:53,120
environment

00:10:49,760 --> 00:10:56,480
it can be used in ci pipeline during

00:10:53,120 --> 00:10:59,040
development phases to ensure all images

00:10:56,480 --> 00:11:01,839
created do not contain severe

00:10:59,040 --> 00:11:01,839
vulnerability

00:11:04,000 --> 00:11:07,680
in addition to vulnerability scanning

00:11:06,160 --> 00:11:11,360
harbor can block

00:11:07,680 --> 00:11:15,279
pull requests if the image vulnerability

00:11:11,360 --> 00:11:18,560
exists a certain threshold level

00:11:15,279 --> 00:11:21,760
also other features like content trust

00:11:18,560 --> 00:11:23,760
can ensure a prominence of the artifacts

00:11:21,760 --> 00:11:25,360
and vulnerability scanning can be

00:11:23,760 --> 00:11:27,600
triggered automatically

00:11:25,360 --> 00:11:29,839
when an artifact has been pushed to the

00:11:27,600 --> 00:11:32,320
registry

00:11:29,839 --> 00:11:32,880
if you want to allow some cves to exist

00:11:32,320 --> 00:11:36,800
in the

00:11:32,880 --> 00:11:39,040
images for example if they are not very

00:11:36,800 --> 00:11:40,240
critical or if you know it is critical

00:11:39,040 --> 00:11:42,720
but you want to

00:11:40,240 --> 00:11:43,279
make them uh available for a while in

00:11:42,720 --> 00:11:45,279
your

00:11:43,279 --> 00:11:46,800
enterprise or in your organizations you

00:11:45,279 --> 00:11:49,839
can set the exception

00:11:46,800 --> 00:11:49,839
in the allowed list

00:11:51,680 --> 00:11:56,320
from time to time user may then have

00:11:53,760 --> 00:11:57,839
their own types of artifacts if these

00:11:56,320 --> 00:12:01,040
artifacts follow the oci

00:11:57,839 --> 00:12:02,880
specs they can be managed and visualized

00:12:01,040 --> 00:12:05,760
by harbor

00:12:02,880 --> 00:12:07,279
the late this version of harbour extend

00:12:05,760 --> 00:12:09,680
the function of the default artifacts

00:12:07,279 --> 00:12:11,680
processor

00:12:09,680 --> 00:12:13,279
users can define their own artifacts

00:12:11,680 --> 00:12:16,240
format and media types

00:12:13,279 --> 00:12:17,120
by following the oci specs then the

00:12:16,240 --> 00:12:22,240
artifacts

00:12:17,120 --> 00:12:25,760
can be pushed to or put from harbor

00:12:22,240 --> 00:12:28,959
a benefit of storing artifacts in harbor

00:12:25,760 --> 00:12:32,000
is that artifacts can be

00:12:28,959 --> 00:12:33,279
treated the same as container images and

00:12:32,000 --> 00:12:36,399
can be replicated

00:12:33,279 --> 00:12:39,839
to other places or enforced by

00:12:36,399 --> 00:12:42,399
road based access control

00:12:39,839 --> 00:12:45,519
and you also get our other features free

00:12:42,399 --> 00:12:48,240
from using harbor

00:12:45,519 --> 00:12:48,880
we have already seen partners utilizing

00:12:48,240 --> 00:12:51,839
harper

00:12:48,880 --> 00:12:52,880
for storing machine learning models as

00:12:51,839 --> 00:12:56,959
artifacts

00:12:52,880 --> 00:12:56,959
and reduce the operational capacities

00:12:57,279 --> 00:13:00,639
there are many more to talk about

00:12:58,880 --> 00:13:01,680
harvest capability of artifact

00:13:00,639 --> 00:13:04,880
management

00:13:01,680 --> 00:13:07,519
but i'd like to pass it to

00:13:04,880 --> 00:13:08,880
ming ming for sharing his experience in

00:13:07,519 --> 00:13:11,360
artifact management in

00:13:08,880 --> 00:13:11,360
netease

00:13:13,120 --> 00:13:19,279
thank you hurry i'm mimi from the taste

00:13:16,959 --> 00:13:20,399
today i will introduce how we manage our

00:13:19,279 --> 00:13:24,320
adapters

00:13:20,399 --> 00:13:25,760
in the case now the container technology

00:13:24,320 --> 00:13:29,839
is widely used

00:13:25,760 --> 00:13:33,839
in that is we use container image

00:13:29,839 --> 00:13:37,680
chart plate bundle etc as

00:13:33,839 --> 00:13:41,040
advectors githubs is also

00:13:37,680 --> 00:13:44,880
used in production and we use hub

00:13:41,040 --> 00:13:49,680
as the repository of cloud native

00:13:44,880 --> 00:13:53,279
artifactors and now there are lots of

00:13:49,680 --> 00:13:56,959
kubernetes clusters in natives and

00:13:53,279 --> 00:14:01,040
5 000 plus nodes in larger ones

00:13:56,959 --> 00:14:03,519
we have more than 20 half instances

00:14:01,040 --> 00:14:04,800
and the largest hub instance manages

00:14:03,519 --> 00:14:07,760
about 100

00:14:04,800 --> 00:14:07,760
000 images

00:14:08,959 --> 00:14:16,399
let's take a look at our architecture

00:14:12,399 --> 00:14:16,720
we developed two services to manage the

00:14:16,399 --> 00:14:19,600
hub

00:14:16,720 --> 00:14:20,639
instance is and in the communities

00:14:19,600 --> 00:14:24,480
clusters

00:14:20,639 --> 00:14:28,000
and we have the flexibility to combine

00:14:24,480 --> 00:14:30,800
the relationships of these instances

00:14:28,000 --> 00:14:32,000
at the same time accessing them by the

00:14:30,800 --> 00:14:35,440
authority system

00:14:32,000 --> 00:14:38,959
of netiz cloud native platform

00:14:35,440 --> 00:14:42,560
has been established in addition

00:14:38,959 --> 00:14:45,519
we integrated our native object storage

00:14:42,560 --> 00:14:48,320
as backhand for high availability and

00:14:45,519 --> 00:14:48,320
high performance

00:14:52,560 --> 00:14:59,600
how do we make hubble

00:14:56,320 --> 00:15:00,880
highly available in the case and first

00:14:59,600 --> 00:15:04,959
we address harvest

00:15:00,880 --> 00:15:08,560
high availability of fire storage in qs

00:15:04,959 --> 00:15:11,760
object storage and local fire storage

00:15:08,560 --> 00:15:14,240
we use async you know a fire

00:15:11,760 --> 00:15:15,519
synchronized tool to synchronize the

00:15:14,240 --> 00:15:20,399
data when using

00:15:15,519 --> 00:15:24,399
lock fire system uh for externally

00:15:20,399 --> 00:15:27,440
uh dependent uh highly high available

00:15:24,399 --> 00:15:30,480
availability we have used

00:15:27,440 --> 00:15:32,399
the open source project storage to

00:15:30,480 --> 00:15:35,920
adjust the high availability

00:15:32,399 --> 00:15:39,600
of first grade circle and

00:15:35,920 --> 00:15:43,360
the high proxy approach to adjust the

00:15:39,600 --> 00:15:45,680
high availability of radius

00:15:43,360 --> 00:15:46,720
which you can find a detailed

00:15:45,680 --> 00:15:50,160
description

00:15:46,720 --> 00:15:53,199
in hubble community in addition

00:15:50,160 --> 00:15:56,320
harvest monitoring mainly armed at the

00:15:53,199 --> 00:15:59,279
thing is enhanced and

00:15:56,320 --> 00:15:59,920
the monitoring clamps of the things such

00:15:59,279 --> 00:16:03,040
as

00:15:59,920 --> 00:16:09,759
replication failure and pdp dispatching

00:16:03,040 --> 00:16:12,320
failure achieved through matrix of

00:16:09,759 --> 00:16:12,320
messiest

00:16:14,240 --> 00:16:18,560
this is how we approach the

00:16:16,959 --> 00:16:21,839
multi-environment

00:16:18,560 --> 00:16:22,480
management of ad factors our first do

00:16:21,839 --> 00:16:25,519
packaging

00:16:22,480 --> 00:16:29,040
and complete all kinds of tests

00:16:25,519 --> 00:16:30,079
in the testing environment and then we

00:16:29,040 --> 00:16:35,440
tag the image

00:16:30,079 --> 00:16:39,360
with release to cheap remote replication

00:16:35,440 --> 00:16:42,000
the image will be replicated to the

00:16:39,360 --> 00:16:45,120
production environment and then intrigue

00:16:42,000 --> 00:16:45,120
the online deployment

00:16:48,839 --> 00:16:51,839
um

00:16:53,120 --> 00:17:00,320
for larger scale distribution of the

00:16:56,639 --> 00:17:04,880
images we are facing two problems

00:17:00,320 --> 00:17:08,400
uh one is the throughput

00:17:04,880 --> 00:17:12,000
pressure in register service and next

00:17:08,400 --> 00:17:15,600
is a network

00:17:12,000 --> 00:17:18,959
plan wise pressure in backhand storage

00:17:15,600 --> 00:17:22,799
and we realized p2p distribution by

00:17:18,959 --> 00:17:25,919
integrating integrate integrating

00:17:22,799 --> 00:17:30,080
hub and kraken through the

00:17:25,919 --> 00:17:33,039
mode of sharing registry server

00:17:30,080 --> 00:17:34,080
the complete design can be found in the

00:17:33,039 --> 00:17:37,360
community of

00:17:34,080 --> 00:17:40,559
krakow we finally achieved the growth of

00:17:37,360 --> 00:17:44,960
over 5000 concurrent pros and

00:17:40,559 --> 00:17:47,840
10 gigabytes plus image distribution

00:17:44,960 --> 00:17:47,840
acceleration

00:17:49,120 --> 00:17:56,080
and here i would like to introduce some

00:17:52,799 --> 00:17:59,280
features of p2p distribution

00:17:56,080 --> 00:18:04,640
of clocken this is the test that

00:17:59,280 --> 00:18:04,640
we met you can see from the table

00:18:06,880 --> 00:18:12,799
the bandwidth limit of p2p distribution

00:18:10,000 --> 00:18:13,440
is configurable and from table you can

00:18:12,799 --> 00:18:16,720
see

00:18:13,440 --> 00:18:19,520
more layers and smaller layer size

00:18:16,720 --> 00:18:19,520
cause lower

00:18:20,000 --> 00:18:26,720
utilized utilization of bandwidth

00:18:23,679 --> 00:18:29,600
it means the distribution will be slower

00:18:26,720 --> 00:18:30,880
and the number of prp peers brings

00:18:29,600 --> 00:18:33,919
little impact

00:18:30,880 --> 00:18:38,160
to their distribution performance

00:18:33,919 --> 00:18:40,960
max 15 kilos

00:18:38,160 --> 00:18:42,080
appears supported officially and it

00:18:40,960 --> 00:18:46,320
works well

00:18:42,080 --> 00:18:46,320
in our product environment now

00:18:47,360 --> 00:18:55,440
we also pay attention to the safety of

00:18:51,440 --> 00:18:58,559
artifactors artifacts are scanned

00:18:55,440 --> 00:19:02,320
after packing the emitted immediately

00:18:58,559 --> 00:19:05,600
there are also two types of quality gas

00:19:02,320 --> 00:19:09,440
the pipeline get and the dispatch get

00:19:05,600 --> 00:19:11,760
a lost asset and there will be

00:19:09,440 --> 00:19:13,280
go through to determine whether the eye

00:19:11,760 --> 00:19:16,720
factors should be

00:19:13,280 --> 00:19:19,280
prevented from being used in production

00:19:16,720 --> 00:19:19,280
environment

00:19:20,480 --> 00:19:27,840
and finally let's

00:19:24,320 --> 00:19:31,280
look at how we manage the

00:19:27,840 --> 00:19:35,360
factors in crcd first we create

00:19:31,280 --> 00:19:39,039
our theact process with application

00:19:35,360 --> 00:19:39,840
center you can see the full flow of our

00:19:39,039 --> 00:19:43,280
crcd

00:19:39,840 --> 00:19:46,559
and also i have introduced some related

00:19:43,280 --> 00:19:50,320
processes earlier and

00:19:46,559 --> 00:19:50,320
besides that we

00:19:51,200 --> 00:19:54,559
following functions are also contained

00:19:53,360 --> 00:19:59,120
in csd

00:19:54,559 --> 00:20:02,320
stages artifactual version management

00:19:59,120 --> 00:20:03,360
are vector security and cd triggered by

00:20:02,320 --> 00:20:07,200
image pushing

00:20:03,360 --> 00:20:11,200
webhook crcd stages

00:20:07,200 --> 00:20:13,840
are connected in serious slow code and

00:20:11,200 --> 00:20:13,840
defectors

00:20:15,039 --> 00:20:18,559
you can see from this picture and this

00:20:17,840 --> 00:20:23,200
is

00:20:18,559 --> 00:20:26,480
uh the full crcd flow in our company

00:20:23,200 --> 00:20:29,600
and that's

00:20:26,480 --> 00:20:32,000
of my share thank you dr

00:20:29,600 --> 00:20:32,000
conrad

00:20:33,280 --> 00:20:37,200
thanks mimi i want to summarize a little

00:20:35,919 --> 00:20:39,679
bit here

00:20:37,200 --> 00:20:40,640
so artifact management is an important

00:20:39,679 --> 00:20:43,120
aspect

00:20:40,640 --> 00:20:44,559
of operations in a cloud-native

00:20:43,120 --> 00:20:47,440
environment

00:20:44,559 --> 00:20:48,000
registry is an ideal place for

00:20:47,440 --> 00:20:51,600
performing

00:20:48,000 --> 00:20:54,720
the management tasks of artifacts

00:20:51,600 --> 00:20:57,760
harper can be your choice of powerful

00:20:54,720 --> 00:20:58,559
tools for managing your artifacts hubble

00:20:57,760 --> 00:21:00,799
can

00:20:58,559 --> 00:21:02,640
provide high-variability and scalability

00:21:00,799 --> 00:21:05,120
for registry service

00:21:02,640 --> 00:21:06,799
and have replication proxy caching

00:21:05,120 --> 00:21:09,440
non-blocking gc and so on

00:21:06,799 --> 00:21:11,679
a whole bunch of powerful features that

00:21:09,440 --> 00:21:13,600
you can consider and leverage

00:21:11,679 --> 00:21:15,360
um as you can see in the case study of

00:21:13,600 --> 00:21:17,280
landis they use harvard

00:21:15,360 --> 00:21:18,559
for the large-scale kubernetes cluster

00:21:17,280 --> 00:21:21,679
in the for the

00:21:18,559 --> 00:21:25,039
artifacts management lastly

00:21:21,679 --> 00:21:27,760
i wanted to introduce our new book um

00:21:25,039 --> 00:21:29,039
of harbor it is the first book on harbor

00:21:27,760 --> 00:21:31,200
in the world

00:21:29,039 --> 00:21:32,720
also by the harbors maintainer and

00:21:31,200 --> 00:21:34,480
contributors

00:21:32,720 --> 00:21:36,400
if you are interested in the content

00:21:34,480 --> 00:21:42,080
please take a look

00:21:36,400 --> 00:21:42,080

YouTube URL: https://www.youtube.com/watch?v=BNQHowtj2dY


