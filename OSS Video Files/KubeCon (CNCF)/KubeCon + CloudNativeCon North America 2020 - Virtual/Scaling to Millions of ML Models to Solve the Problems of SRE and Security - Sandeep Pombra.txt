Title: Scaling to Millions of ML Models to Solve the Problems of SRE and Security - Sandeep Pombra
Publication date: 2020-11-23
Playlist: KubeCon + CloudNativeCon North America 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon Europe 2021 Virtual from May 4–7, 2021. Learn more at https://kubecon.io. The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects. 

Scaling to Millions of ML Models to Solve the Problems of SRE and Security - Sandeep Pombra & Jakub Pavlik, Volterra 

This talk describes how to scale to millions of ML models operating on petabytes of operational and user data that is used to improve the efficacy of SRE teams and security of end users’ application services. These models are used to improve zero trust security framework and infrastructure diagnosis -- based on machine learning, anomaly detection and time series analysis. A production deployment that delivers these large numbers of models combines many open source technologies such as Kubernetes, Prometheus, Cortex, Apache Spark and Apache Arrow. In this talk, we will describe the key challenges that we had to solve when implementing machine learning and anomaly detection on K8s nodes and Envoy-based service mesh. These challenges include collecting data from hundreds of thousands of nodes, high cardinality of models, and distributing the inference models down to each of the K8s nodes. 

https://sched.co/ekCH
Captions: 
	00:00:02,080 --> 00:00:05,839
okay so

00:00:03,199 --> 00:00:06,960
uh hello everyone uh welcome to our

00:00:05,839 --> 00:00:09,120
presentation

00:00:06,960 --> 00:00:10,240
my name is jacob pavlik and i am

00:00:09,120 --> 00:00:12,240
director

00:00:10,240 --> 00:00:13,840
of engineering at volterra and i am

00:00:12,240 --> 00:00:17,680
responsible for

00:00:13,840 --> 00:00:19,520
leading the sari team great thanks jakub

00:00:17,680 --> 00:00:21,520
my name is sandeep pombra and i'm the

00:00:19,520 --> 00:00:23,199
head of data science and i'm responsible

00:00:21,520 --> 00:00:23,920
for doing all the machine learning data

00:00:23,199 --> 00:00:26,960
science

00:00:23,920 --> 00:00:30,480
and data modeling at uh

00:00:26,960 --> 00:00:33,600
volterra okay so

00:00:30,480 --> 00:00:35,440
today uh we are excited to uh

00:00:33,600 --> 00:00:37,360
get our session to kubecon and we would

00:00:35,440 --> 00:00:40,480
like to share with you

00:00:37,360 --> 00:00:41,200
our story and journey how we get to the

00:00:40,480 --> 00:00:44,239
scaling

00:00:41,200 --> 00:00:47,120
to the million machine learning models

00:00:44,239 --> 00:00:48,559
and how we are using kubernetes apache

00:00:47,120 --> 00:00:51,600
spark and apache hero

00:00:48,559 --> 00:00:54,480
to work with all those data

00:00:51,600 --> 00:00:58,079
and what great features we are able to

00:00:54,480 --> 00:01:01,520
get from them

00:00:58,079 --> 00:01:04,320
so a little bit about agenda uh

00:01:01,520 --> 00:01:06,080
first of all uh i will take you through

00:01:04,320 --> 00:01:07,280
quick overview of what is volterra

00:01:06,080 --> 00:01:09,920
because uh

00:01:07,280 --> 00:01:11,200
not everybody probably is familiar what

00:01:09,920 --> 00:01:14,320
we are doing

00:01:11,200 --> 00:01:15,439
then sandeep will explain our machine

00:01:14,320 --> 00:01:18,880
learning functions

00:01:15,439 --> 00:01:22,240
and our model explosion

00:01:18,880 --> 00:01:25,439
uh and the problems what we were facing

00:01:22,240 --> 00:01:27,040
then uh i will take you through machine

00:01:25,439 --> 00:01:27,680
learning infrastructure evolution

00:01:27,040 --> 00:01:30,880
journey

00:01:27,680 --> 00:01:34,479
and how we continuously improve

00:01:30,880 --> 00:01:36,960
our large infrastructure and then sunday

00:01:34,479 --> 00:01:38,880
will take you to all model scaling

00:01:36,960 --> 00:01:41,840
challenges

00:01:38,880 --> 00:01:42,320
now uh to begin with i uh pick up this

00:01:41,840 --> 00:01:44,560
slide

00:01:42,320 --> 00:01:45,840
uh don't worry it is not uh so much

00:01:44,560 --> 00:01:48,159
vendor stuff

00:01:45,840 --> 00:01:49,520
uh but i wanted to show you what is

00:01:48,159 --> 00:01:52,640
basically behind

00:01:49,520 --> 00:01:54,479
so we at the volterra we built uh

00:01:52,640 --> 00:01:57,520
distributed cloud services

00:01:54,479 --> 00:01:58,399
wherever your apps and data need them so

00:01:57,520 --> 00:02:01,600
it can be

00:01:58,399 --> 00:02:04,719
public cloud it can be private cloud

00:02:01,600 --> 00:02:09,200
physical edge nomadic edge or our

00:02:04,719 --> 00:02:12,400
global backbone we focus on providing

00:02:09,200 --> 00:02:14,720
distributed network services and

00:02:12,400 --> 00:02:16,000
distributed infrastructure for the

00:02:14,720 --> 00:02:19,599
application

00:02:16,000 --> 00:02:22,160
so this is how looks our normal slide

00:02:19,599 --> 00:02:23,280
but from the our engineering side and

00:02:22,160 --> 00:02:26,560
where you can see

00:02:23,280 --> 00:02:30,800
the behind and what is running so for

00:02:26,560 --> 00:02:32,720
us it is basically kubernetes everywhere

00:02:30,800 --> 00:02:34,080
and all those locations and all those

00:02:32,720 --> 00:02:37,280
sites means um

00:02:34,080 --> 00:02:40,959
kubernetes site the kubernetes sites and

00:02:37,280 --> 00:02:41,920
it also means a lot of locks and metrics

00:02:40,959 --> 00:02:45,440
and data

00:02:41,920 --> 00:02:48,720
which we are able to pull from

00:02:45,440 --> 00:02:51,920
and then analyze them and provide

00:02:48,720 --> 00:02:53,200
a great features for our customers and

00:02:51,920 --> 00:02:56,959
also for us

00:02:53,200 --> 00:02:56,959
to be able to operate

00:02:59,760 --> 00:03:03,200
okay thanks jakub so i'm going to talk a

00:03:02,159 --> 00:03:05,519
little bit about our

00:03:03,200 --> 00:03:06,800
machine learning applications as jakub

00:03:05,519 --> 00:03:07,760
described you know we have a very

00:03:06,800 --> 00:03:10,480
complex

00:03:07,760 --> 00:03:11,519
distributed microservices multi-cloud

00:03:10,480 --> 00:03:13,840
environment

00:03:11,519 --> 00:03:15,440
and this uh entails a lot of different

00:03:13,840 --> 00:03:17,680
machine learning functions

00:03:15,440 --> 00:03:18,879
first of all we need to provide a very

00:03:17,680 --> 00:03:21,599
sophisticated

00:03:18,879 --> 00:03:23,599
web application firewall function and

00:03:21,599 --> 00:03:26,000
typical rule-based maps

00:03:23,599 --> 00:03:26,879
are not sophisticated enough to handle

00:03:26,000 --> 00:03:29,280
zero

00:03:26,879 --> 00:03:30,799
uh zero-day attacks um so we need to

00:03:29,280 --> 00:03:32,879
provide

00:03:30,799 --> 00:03:34,560
the most uh you know sophisticated

00:03:32,879 --> 00:03:35,040
machine learning algorithms that allow

00:03:34,560 --> 00:03:37,120
us to

00:03:35,040 --> 00:03:39,280
handle that uh besides that's an

00:03:37,120 --> 00:03:42,000
important part of an application for us

00:03:39,280 --> 00:03:43,440
is understanding how the apis within the

00:03:42,000 --> 00:03:45,840
application are working

00:03:43,440 --> 00:03:47,280
uh so we use machine learning to to

00:03:45,840 --> 00:03:50,400
discover apis

00:03:47,280 --> 00:03:52,879
and basically uh compress the whole

00:03:50,400 --> 00:03:54,959
you know application into a bunch of

00:03:52,879 --> 00:03:57,120
different api endpoints

00:03:54,959 --> 00:03:58,400
then we do a lot of different types of

00:03:57,120 --> 00:04:00,560
anomaly detection

00:03:58,400 --> 00:04:01,519
to again detect bot attacks and

00:04:00,560 --> 00:04:03,760
different types of

00:04:01,519 --> 00:04:05,120
ddos attacks uh we do time series

00:04:03,760 --> 00:04:06,879
anomaly detection

00:04:05,120 --> 00:04:08,640
and we also do per request anomaly

00:04:06,879 --> 00:04:10,879
detection and we also do

00:04:08,640 --> 00:04:12,480
user behavior analysis to understand if

00:04:10,879 --> 00:04:14,720
there's any malicious users as well as

00:04:12,480 --> 00:04:16,959
to understand the application better

00:04:14,720 --> 00:04:18,239
so as you can see from this picture uh

00:04:16,959 --> 00:04:19,359
there is a lot of different types of

00:04:18,239 --> 00:04:20,799
machine learning functions they are

00:04:19,359 --> 00:04:22,880
divided between

00:04:20,799 --> 00:04:24,080
the learning core which is the training

00:04:22,880 --> 00:04:25,600
and um

00:04:24,080 --> 00:04:27,360
and the inference engines which run on

00:04:25,600 --> 00:04:29,840
the edge and

00:04:27,360 --> 00:04:31,759
the learning core actually is a global

00:04:29,840 --> 00:04:34,720
learning area where we

00:04:31,759 --> 00:04:35,199
do all our training and as you will see

00:04:34,720 --> 00:04:38,240
in the

00:04:35,199 --> 00:04:40,160
next few slides that to do this we have

00:04:38,240 --> 00:04:42,800
to

00:04:40,160 --> 00:04:43,280
really manage a very massive scale so

00:04:42,800 --> 00:04:47,280
we'll

00:04:43,280 --> 00:04:47,280
talk about this more as we go forward

00:04:49,600 --> 00:04:57,680
okay so before we get to

00:04:53,759 --> 00:05:01,199
external models let me explain you

00:04:57,680 --> 00:05:01,600
uh the scale and how we collect metrics

00:05:01,199 --> 00:05:04,800
and

00:05:01,600 --> 00:05:07,120
locks in our infrastructure so we have

00:05:04,800 --> 00:05:08,720
uh three type of sites we have a

00:05:07,120 --> 00:05:12,080
customer edges

00:05:08,720 --> 00:05:14,639
which are available in thousands

00:05:12,080 --> 00:05:17,280
right so there can be thousands and

00:05:14,639 --> 00:05:19,360
hundreds of thousands of the sites

00:05:17,280 --> 00:05:20,479
then we have something what we call

00:05:19,360 --> 00:05:22,479
regional edges

00:05:20,479 --> 00:05:25,440
which are our point of presence and our

00:05:22,479 --> 00:05:28,639
global backbone those are tense today

00:05:25,440 --> 00:05:30,960
and then we have a global controller

00:05:28,639 --> 00:05:32,639
uh which is today distributed across

00:05:30,960 --> 00:05:34,960
three regions

00:05:32,639 --> 00:05:36,400
and this is the place where we are doing

00:05:34,960 --> 00:05:39,520
uh

00:05:36,400 --> 00:05:41,360
data analysis so the way how it works is

00:05:39,520 --> 00:05:44,800
that we have a prometheus in

00:05:41,360 --> 00:05:48,160
each of every site which scrapes

00:05:44,800 --> 00:05:49,199
local kubernetes workload as well as

00:05:48,160 --> 00:05:52,560
nodes

00:05:49,199 --> 00:05:55,759
and then we are doing from the connected

00:05:52,560 --> 00:05:58,400
regional edges uh prometheus federation

00:05:55,759 --> 00:06:00,560
with the metrics white listings and we

00:05:58,400 --> 00:06:02,319
are scraping only certain metrics which

00:06:00,560 --> 00:06:03,360
we are particularly interested in and

00:06:02,319 --> 00:06:05,600
which we

00:06:03,360 --> 00:06:07,120
want to work with so not basically

00:06:05,600 --> 00:06:10,560
everything

00:06:07,120 --> 00:06:13,840
and we use a remote write which gets

00:06:10,560 --> 00:06:17,039
right data into cortex as our

00:06:13,840 --> 00:06:19,280
long-term storage for all those kind of

00:06:17,039 --> 00:06:21,759
metrics

00:06:19,280 --> 00:06:22,880
original edge prometheus this prometai

00:06:21,759 --> 00:06:25,680
also

00:06:22,880 --> 00:06:27,360
send alerts and produce alerts to to our

00:06:25,680 --> 00:06:30,639
uh alert manager

00:06:27,360 --> 00:06:31,919
on the lock side uh today we are using

00:06:30,639 --> 00:06:35,919
fluent bit

00:06:31,919 --> 00:06:36,880
who captures all the lock messages from

00:06:35,919 --> 00:06:39,280
our services

00:06:36,880 --> 00:06:41,199
and from the third party services they

00:06:39,280 --> 00:06:44,479
are forwarded to

00:06:41,199 --> 00:06:47,680
uh aggregation fluency and fluent

00:06:44,479 --> 00:06:50,400
bits demons in the

00:06:47,680 --> 00:06:50,720
pops and original edges and from there

00:06:50,400 --> 00:06:53,599
we

00:06:50,720 --> 00:06:54,240
write into two places today so we write

00:06:53,599 --> 00:06:59,280
into

00:06:54,240 --> 00:07:02,479
a elastic search and we also write into

00:06:59,280 --> 00:07:05,599
aws ss3 and

00:07:02,479 --> 00:07:08,720
those apis are then available for

00:07:05,599 --> 00:07:11,840
our uh metric data analysis

00:07:08,720 --> 00:07:14,319
uh even data analysis and other services

00:07:11,840 --> 00:07:19,440
which sunday will be

00:07:14,319 --> 00:07:19,440
explaining on the future slides

00:07:20,880 --> 00:07:26,479
yeah so as yakub just uh

00:07:24,160 --> 00:07:27,919
showed that we have a very complex

00:07:26,479 --> 00:07:29,680
architecture with a lot of different

00:07:27,919 --> 00:07:31,680
types of data ingestion

00:07:29,680 --> 00:07:33,199
and we also have when we are deploying

00:07:31,680 --> 00:07:34,960
applications we are typically deploying

00:07:33,199 --> 00:07:38,400
very complex applications

00:07:34,960 --> 00:07:38,800
and these applications uh uh basically

00:07:38,400 --> 00:07:41,039
under

00:07:38,800 --> 00:07:42,800
under underlying these applications are

00:07:41,039 --> 00:07:44,879
a lot of different dimensions

00:07:42,800 --> 00:07:47,680
um basically we have application virtual

00:07:44,879 --> 00:07:49,199
hosts source sites destination sites

00:07:47,680 --> 00:07:50,400
and one of the things we found when

00:07:49,199 --> 00:07:50,960
we're doing all our machine learning

00:07:50,400 --> 00:07:53,680
modeling

00:07:50,960 --> 00:07:54,479
is that every application every customer

00:07:53,680 --> 00:07:57,360
every

00:07:54,479 --> 00:07:58,319
geography has its own characteristics so

00:07:57,360 --> 00:08:01,759
it's very difficult

00:07:58,319 --> 00:08:02,400
to develop a universal model for all of

00:08:01,759 --> 00:08:04,240
these

00:08:02,400 --> 00:08:05,680
uh so basically to get the best

00:08:04,240 --> 00:08:06,960
performance in terms of our machine

00:08:05,680 --> 00:08:09,199
learning accuracy

00:08:06,960 --> 00:08:10,879
we have to develop models for each

00:08:09,199 --> 00:08:12,639
across each of the dimension

00:08:10,879 --> 00:08:14,720
and as you can see uh basically doing

00:08:12,639 --> 00:08:17,680
that kind of a combination

00:08:14,720 --> 00:08:19,680
just by a multiplicative logic leads to

00:08:17,680 --> 00:08:23,120
a very large cardinality of models

00:08:19,680 --> 00:08:25,599
and in our case we were looking at

00:08:23,120 --> 00:08:26,560
in certain cases some of the models for

00:08:25,599 --> 00:08:28,319
example for the time

00:08:26,560 --> 00:08:31,280
series we're getting into millions of

00:08:28,319 --> 00:08:33,919
time series um so basically we need to

00:08:31,280 --> 00:08:35,360
figure out a way to scale these models

00:08:33,919 --> 00:08:36,159
um so initially when we started this

00:08:35,360 --> 00:08:37,360
project

00:08:36,159 --> 00:08:38,719
you know obviously we were trying to get

00:08:37,360 --> 00:08:39,599
the machine learning and our algorithms

00:08:38,719 --> 00:08:40,880
working

00:08:39,599 --> 00:08:43,279
so we were running these models in a

00:08:40,880 --> 00:08:45,839
single instance in a serialized manner

00:08:43,279 --> 00:08:47,440
and what that did was basically it took

00:08:45,839 --> 00:08:49,360
a very long time to run

00:08:47,440 --> 00:08:51,519
and uh obviously you know when we are

00:08:49,360 --> 00:08:52,800
training and doing inference and scoring

00:08:51,519 --> 00:08:54,800
based on these models

00:08:52,800 --> 00:08:56,880
if the model takes several hours to run

00:08:54,800 --> 00:08:57,760
and the the model itself becomes

00:08:56,880 --> 00:08:59,200
obsolete

00:08:57,760 --> 00:09:01,200
and definitely it's not something that

00:08:59,200 --> 00:09:03,040
was sustainable uh so

00:09:01,200 --> 00:09:04,399
um now yakub is going to talk about a

00:09:03,040 --> 00:09:05,440
little bit about how our infrastructure

00:09:04,399 --> 00:09:09,600
was also

00:09:05,440 --> 00:09:13,360
struggling to meet this need

00:09:09,600 --> 00:09:15,200
so when we started

00:09:13,360 --> 00:09:17,600
we actually and this is the

00:09:15,200 --> 00:09:19,760
infrastructure picture

00:09:17,600 --> 00:09:20,880
which is looking now on the global

00:09:19,760 --> 00:09:24,160
controller as i

00:09:20,880 --> 00:09:27,279
introduced in the previous slide so

00:09:24,160 --> 00:09:29,760
we started with the more regions but

00:09:27,279 --> 00:09:30,560
this picture cover one of the region

00:09:29,760 --> 00:09:34,000
where

00:09:30,560 --> 00:09:38,080
we provided elasticsearch

00:09:34,000 --> 00:09:40,160
cortex and aws free api and we run

00:09:38,080 --> 00:09:44,160
everything as an eks

00:09:40,160 --> 00:09:47,360
cluster in inside of the eks cluster

00:09:44,160 --> 00:09:51,519
and it was basically single cluster

00:09:47,360 --> 00:09:55,120
running continuous jobs

00:09:51,519 --> 00:09:59,839
and the issue was that parallelism

00:09:55,120 --> 00:10:05,760
and also inefficient

00:09:59,839 --> 00:10:08,240
cpu ram usage and

00:10:05,760 --> 00:10:10,959
we the issue was that we had to resize

00:10:08,240 --> 00:10:15,040
to bigger and bigger

00:10:10,959 --> 00:10:18,240
vms and flavors and

00:10:15,040 --> 00:10:20,399
it was not quite efficient because some

00:10:18,240 --> 00:10:21,920
because those jobs running few hours and

00:10:20,399 --> 00:10:24,720
then then they don't run

00:10:21,920 --> 00:10:25,600
many hours so you need to find kind of

00:10:24,720 --> 00:10:29,120
balance so

00:10:25,600 --> 00:10:31,120
it was not really cost efficient and

00:10:29,120 --> 00:10:34,240
resources efficient

00:10:31,120 --> 00:10:36,959
and therefore

00:10:34,240 --> 00:10:38,800
we wanted to find a different way or

00:10:36,959 --> 00:10:40,160
take a look on the different perspective

00:10:38,800 --> 00:10:43,279
how we can handle this

00:10:40,160 --> 00:10:47,839
very large data says ingestion

00:10:43,279 --> 00:10:47,839
and the training

00:10:48,880 --> 00:10:52,640
yeah so um so as yakub mentioned you

00:10:51,680 --> 00:10:54,320
know we were

00:10:52,640 --> 00:10:55,760
coming running into a lot of bottlenecks

00:10:54,320 --> 00:10:58,079
in our infrastructure

00:10:55,760 --> 00:10:59,279
so obviously you know uh from a machine

00:10:58,079 --> 00:11:02,000
learning model

00:10:59,279 --> 00:11:03,760
scaling the obvious approach is to run

00:11:02,000 --> 00:11:05,519
several models in parallel

00:11:03,760 --> 00:11:07,519
and you know this can be done in various

00:11:05,519 --> 00:11:10,079
ways you know we could use das

00:11:07,519 --> 00:11:11,920
or job labor a variety of python because

00:11:10,079 --> 00:11:14,000
most of our code was in python

00:11:11,920 --> 00:11:15,680
but we wanted something which was much

00:11:14,000 --> 00:11:18,720
more easier to maintain

00:11:15,680 --> 00:11:20,320
easier to scale easier to manage uh so

00:11:18,720 --> 00:11:23,120
we wanted to combine

00:11:20,320 --> 00:11:24,079
uh the best of scaling horizontal

00:11:23,120 --> 00:11:27,279
scaling

00:11:24,079 --> 00:11:28,240
as well as having the ability to auto

00:11:27,279 --> 00:11:30,320
scale

00:11:28,240 --> 00:11:31,279
scale to different customers do

00:11:30,320 --> 00:11:33,760
automation as

00:11:31,279 --> 00:11:35,519
yakub mentioned with ci cd and minimize

00:11:33,760 --> 00:11:38,079
our infrastructure management

00:11:35,519 --> 00:11:38,880
and we also wanted to be able to have a

00:11:38,079 --> 00:11:41,440
very

00:11:38,880 --> 00:11:43,279
universal way of our data ingestion so

00:11:41,440 --> 00:11:45,680
that we could easily do secure

00:11:43,279 --> 00:11:46,959
and seamless data ingestion uh so for

00:11:45,680 --> 00:11:51,360
all these reasons

00:11:46,959 --> 00:11:51,360
uh we decided that we wanted to um

00:11:52,480 --> 00:11:55,920
i think it's not allowing me to go to

00:11:55,360 --> 00:11:59,839
the

00:11:55,920 --> 00:12:00,240
next slide okay there it is sorry about

00:11:59,839 --> 00:12:02,160
that

00:12:00,240 --> 00:12:03,760
um yeah so basically to do this we

00:12:02,160 --> 00:12:06,720
wanted to combine the best of

00:12:03,760 --> 00:12:08,240
uh spark scaling and parallelization and

00:12:06,720 --> 00:12:10,880
kubernetes infrastructure

00:12:08,240 --> 00:12:12,000
so we use spark uh basically as a

00:12:10,880 --> 00:12:14,560
horizontal scaling

00:12:12,000 --> 00:12:16,240
and uh kubernetes as a distributed data

00:12:14,560 --> 00:12:19,680
ingestion architecture with uh

00:12:16,240 --> 00:12:21,760
integrated ci cd um so uh we wanted to

00:12:19,680 --> 00:12:23,440
have a faster time to market uh so

00:12:21,760 --> 00:12:26,160
rather than creating our own

00:12:23,440 --> 00:12:27,040
open source spark engine we decided to

00:12:26,160 --> 00:12:31,279
leverage

00:12:27,040 --> 00:12:31,279
the sas technology from databricks

00:12:36,000 --> 00:12:42,000
yes so when vista

00:12:39,440 --> 00:12:45,040
let me talk about a little bit about how

00:12:42,000 --> 00:12:48,560
we actually integrated data breaks

00:12:45,040 --> 00:12:51,760
and uh what we had to do

00:12:48,560 --> 00:12:54,800
so if you look at the

00:12:51,760 --> 00:12:57,760
standard databricks uh integration

00:12:54,800 --> 00:12:58,320
basically they calculate that you will

00:12:57,760 --> 00:13:02,320
give them

00:12:58,320 --> 00:13:05,360
access to the full uh aws vpc

00:13:02,320 --> 00:13:09,120
and they can provision the vms and jobs

00:13:05,360 --> 00:13:13,200
as they need and you do aws

00:13:09,120 --> 00:13:15,360
peering with your uh vpc where you have

00:13:13,200 --> 00:13:16,399
your data in our case our global control

00:13:15,360 --> 00:13:19,600
with cortex

00:13:16,399 --> 00:13:20,639
uh prometheus and elasticsearch api so

00:13:19,600 --> 00:13:24,160
the pro

00:13:20,639 --> 00:13:27,600
the problem what we find here was that

00:13:24,160 --> 00:13:30,800
we didn't want to give them access to

00:13:27,600 --> 00:13:34,079
uh our vpc so we created

00:13:30,800 --> 00:13:36,800
dedicated instance uh dedicated

00:13:34,079 --> 00:13:37,920
account but we still didn't want to use

00:13:36,800 --> 00:13:40,240
just the p ring

00:13:37,920 --> 00:13:41,839
because uh we wanted to have a

00:13:40,240 --> 00:13:44,079
visibility on

00:13:41,839 --> 00:13:45,360
what is flowing set detail firewall

00:13:44,079 --> 00:13:48,720
rules

00:13:45,360 --> 00:13:50,800
and uh make sure that uh it cannot get

00:13:48,720 --> 00:13:54,160
breached and they cannot

00:13:50,800 --> 00:13:58,320
access our core infrastructure

00:13:54,160 --> 00:14:00,720
uh and they don't they don't use our

00:13:58,320 --> 00:14:01,680
cas and our certificate so we wanted to

00:14:00,720 --> 00:14:04,639
relay

00:14:01,680 --> 00:14:05,760
isolate them and vpc peering was not

00:14:04,639 --> 00:14:08,560
good enough

00:14:05,760 --> 00:14:09,279
so we come up with the idea that we can

00:14:08,560 --> 00:14:12,320
actually

00:14:09,279 --> 00:14:15,440
leverage our own technology and

00:14:12,320 --> 00:14:18,800
make it better and therefore

00:14:15,440 --> 00:14:22,480
we worked with this design so

00:14:18,800 --> 00:14:25,760
we took we let run our eks

00:14:22,480 --> 00:14:29,199
and our existing vpc as is

00:14:25,760 --> 00:14:32,880
and we just create a dedicated account

00:14:29,199 --> 00:14:36,880
uh for data breaks learning jobs

00:14:32,880 --> 00:14:41,040
and then we launched our

00:14:36,880 --> 00:14:43,920
ingress egress gateway

00:14:41,040 --> 00:14:45,360
uh called voltmesh which basically allow

00:14:43,920 --> 00:14:48,079
us to

00:14:45,360 --> 00:14:49,839
get connectivity for only a particular

00:14:48,079 --> 00:14:52,560
api

00:14:49,839 --> 00:14:54,639
which we need in this case it's a

00:14:52,560 --> 00:14:57,839
prometheus

00:14:54,639 --> 00:14:58,720
cortex and elasticsearch and we just

00:14:57,839 --> 00:15:01,839
advertise

00:14:58,720 --> 00:15:04,079
only those apis with uh

00:15:01,839 --> 00:15:05,519
different certificates and different uh

00:15:04,079 --> 00:15:08,560
authority

00:15:05,519 --> 00:15:13,040
for the data breaks which allow us to

00:15:08,560 --> 00:15:16,639
really provide granular api

00:15:13,040 --> 00:15:20,480
filtering and service policies and

00:15:16,639 --> 00:15:23,360
allows to volterral learning services

00:15:20,480 --> 00:15:24,560
such as api discovery time series

00:15:23,360 --> 00:15:27,760
anomaly detection

00:15:24,560 --> 00:15:29,920
prerequisite detection request data

00:15:27,760 --> 00:15:32,880
analysis or user behavioral analysis

00:15:29,920 --> 00:15:34,000
to run and consume and produce metrics

00:15:32,880 --> 00:15:38,480
backs to

00:15:34,000 --> 00:15:38,480
our infrastructure without any

00:15:39,519 --> 00:15:46,160
security breaching or direct access to

00:15:42,959 --> 00:15:48,800
core volterra services

00:15:46,160 --> 00:15:51,920
and of course this help us also to

00:15:48,800 --> 00:15:51,920
improve our own

00:15:52,839 --> 00:15:58,079
technology

00:15:55,839 --> 00:15:59,199
okay great thanks jacob so um now i

00:15:58,079 --> 00:16:01,920
think the rest of the

00:15:59,199 --> 00:16:02,720
talk i will focus on how we use spark to

00:16:01,920 --> 00:16:06,160
paralyze

00:16:02,720 --> 00:16:07,680
our models and uh basically um

00:16:06,160 --> 00:16:09,279
sorry i can go to the previous slide

00:16:07,680 --> 00:16:11,519
yeah thank you um

00:16:09,279 --> 00:16:12,800
so the way spark works is you know spark

00:16:11,519 --> 00:16:14,880
relies very much on running the

00:16:12,800 --> 00:16:17,920
functions on the driver itself

00:16:14,880 --> 00:16:20,720
and then uh running a bunch of executors

00:16:17,920 --> 00:16:21,199
in parallel and typically that can be

00:16:20,720 --> 00:16:24,639
done

00:16:21,199 --> 00:16:26,160
by creating either data frames or rdds

00:16:24,639 --> 00:16:26,880
which are resilient distributed data

00:16:26,160 --> 00:16:29,440
sets

00:16:26,880 --> 00:16:30,160
uh which are collection of data frames

00:16:29,440 --> 00:16:32,320
or data

00:16:30,160 --> 00:16:34,240
modules that run on different executors

00:16:32,320 --> 00:16:37,839
so the idea is basically

00:16:34,240 --> 00:16:39,920
um you know you take some huge data

00:16:37,839 --> 00:16:41,360
and then you split it into different

00:16:39,920 --> 00:16:43,759
executors and actually

00:16:41,360 --> 00:16:45,600
if your executors are multi-core you can

00:16:43,759 --> 00:16:48,160
even go and split them into multi-core

00:16:45,600 --> 00:16:50,240
so for example if we have four executors

00:16:48,160 --> 00:16:51,839
uh with four cores so we could paralyze

00:16:50,240 --> 00:16:54,320
by a factor of 16.

00:16:51,839 --> 00:16:56,560
uh so the first approach we took uh was

00:16:54,320 --> 00:16:58,639
for the kinds of scaling where basically

00:16:56,560 --> 00:17:01,360
we were going to ingest the data

00:16:58,639 --> 00:17:02,399
and we were going to also extend the

00:17:01,360 --> 00:17:04,959
models

00:17:02,399 --> 00:17:05,919
into the various interfaces that yakub

00:17:04,959 --> 00:17:07,679
talked about

00:17:05,919 --> 00:17:09,439
and for that uh we came up with a very

00:17:07,679 --> 00:17:11,919
simple scheme where basically

00:17:09,439 --> 00:17:13,280
we took our dimensions like applications

00:17:11,919 --> 00:17:16,959
namespaces

00:17:13,280 --> 00:17:19,199
and accreted uh uh what we call is a you

00:17:16,959 --> 00:17:22,559
know a panda's data frame out of it

00:17:19,199 --> 00:17:23,679
and uh then we did uh you converted that

00:17:22,559 --> 00:17:26,720
data frame into

00:17:23,679 --> 00:17:27,199
an rdd and then uh basically what we can

00:17:26,720 --> 00:17:29,679
do

00:17:27,199 --> 00:17:33,840
is do a map which is basically applying

00:17:29,679 --> 00:17:33,840
any function

00:17:36,559 --> 00:17:40,720
this allows us to run a function

00:17:38,799 --> 00:17:42,880
obviously the input output of this is

00:17:40,720 --> 00:17:44,840
more symbolic where basically we want to

00:17:42,880 --> 00:17:47,840
make sure that

00:17:44,840 --> 00:17:50,640
your function is executed uh

00:17:47,840 --> 00:17:52,000
but the actual uh core functionality uh

00:17:50,640 --> 00:17:54,640
can be very complicated

00:17:52,000 --> 00:17:55,440
um so if you look at this um quotes and

00:17:54,640 --> 00:17:57,600
snippet

00:17:55,440 --> 00:17:59,520
uh i can basically explain a little bit

00:17:57,600 --> 00:18:01,840
further how we did this

00:17:59,520 --> 00:18:02,960
so basically uh spark has um two kinds

00:18:01,840 --> 00:18:04,960
of operations there is the

00:18:02,960 --> 00:18:06,799
transformations like the map

00:18:04,960 --> 00:18:08,720
and which is apply of a function and

00:18:06,799 --> 00:18:10,160
then there is basically actions which

00:18:08,720 --> 00:18:12,720
actually execute the thing

00:18:10,160 --> 00:18:13,360
uh like collect uh or count and things

00:18:12,720 --> 00:18:15,600
like that or

00:18:13,360 --> 00:18:17,200
other kinds of aggregations so in this

00:18:15,600 --> 00:18:20,559
case you can see

00:18:17,200 --> 00:18:22,240
if we define this outer function

00:18:20,559 --> 00:18:24,000
which is basically a standard python

00:18:22,240 --> 00:18:27,520
function we get

00:18:24,000 --> 00:18:29,840
our pandas frame which has all the keys

00:18:27,520 --> 00:18:30,559
that we're going to use for mapping uh

00:18:29,840 --> 00:18:34,480
we create

00:18:30,559 --> 00:18:38,000
a spark data frame uh based on the panda

00:18:34,480 --> 00:18:38,640
frame and uh then basically we make sure

00:18:38,000 --> 00:18:42,000
that

00:18:38,640 --> 00:18:44,480
we have the the

00:18:42,000 --> 00:18:46,640
actual uh mapping function which is

00:18:44,480 --> 00:18:48,559
embedded within this function so we can

00:18:46,640 --> 00:18:50,559
use any of these variables like variable

00:18:48,559 --> 00:18:52,320
one variable two inside this function

00:18:50,559 --> 00:18:55,679
and these will be automatically uh

00:18:52,320 --> 00:18:59,039
exported uh to each of the

00:18:55,679 --> 00:19:01,840
and uh so pretty much this function

00:18:59,039 --> 00:19:03,280
is uh we we take the data frame we map

00:19:01,840 --> 00:19:06,400
we convert into rdd

00:19:03,280 --> 00:19:07,919
and we map this function uh this

00:19:06,400 --> 00:19:09,600
therefore it applies this function to

00:19:07,919 --> 00:19:12,080
every row of this

00:19:09,600 --> 00:19:13,200
rdd which is the original part of frame

00:19:12,080 --> 00:19:14,799
and

00:19:13,200 --> 00:19:16,320
and then we do a collection to make sure

00:19:14,799 --> 00:19:16,799
that this function is actually executed

00:19:16,320 --> 00:19:19,360
because

00:19:16,799 --> 00:19:20,640
spark has a lazy execution where it will

00:19:19,360 --> 00:19:22,720
only execute the function

00:19:20,640 --> 00:19:24,160
when they actually do the collecting but

00:19:22,720 --> 00:19:25,679
one thing i wanted to point out is that

00:19:24,160 --> 00:19:26,240
this model function which i have not

00:19:25,679 --> 00:19:28,720
really

00:19:26,240 --> 00:19:30,480
uh talked to described here can be a

00:19:28,720 --> 00:19:32,400
very complicated function

00:19:30,480 --> 00:19:33,760
uh and we can pass a lot of different

00:19:32,400 --> 00:19:35,919
types of objects to it

00:19:33,760 --> 00:19:37,360
uh without any problem and this will all

00:19:35,919 --> 00:19:41,520
be done seamlessly

00:19:37,360 --> 00:19:43,919
and this automatically uh scales the

00:19:41,520 --> 00:19:45,679
the the function into various parallel

00:19:43,919 --> 00:19:47,280
um components

00:19:45,679 --> 00:19:48,799
um so this is actually a very cool

00:19:47,280 --> 00:19:50,559
approach and this works very well in

00:19:48,799 --> 00:19:52,240
scaling when you know we do everything

00:19:50,559 --> 00:19:54,799
within the function

00:19:52,240 --> 00:19:56,160
but there are other instances where

00:19:54,799 --> 00:19:57,840
basically we have a much more

00:19:56,160 --> 00:19:59,440
complicated data frame

00:19:57,840 --> 00:20:02,000
sorry let me see how do i go to the next

00:19:59,440 --> 00:20:02,000
slide here

00:20:02,159 --> 00:20:07,760
could you go to the next slide i'm not

00:20:04,320 --> 00:20:11,440
able to do that for some reason

00:20:07,760 --> 00:20:13,760
uh can we go to the next slide oh

00:20:11,440 --> 00:20:16,159
okay thank you so much yeah so there is

00:20:13,760 --> 00:20:16,720
a a lot of situations where basically

00:20:16,159 --> 00:20:19,120
our

00:20:16,720 --> 00:20:20,000
data set is already available it's it's

00:20:19,120 --> 00:20:22,480
a complex data

00:20:20,000 --> 00:20:23,280
frame and not every single column within

00:20:22,480 --> 00:20:25,200
the data set

00:20:23,280 --> 00:20:27,360
is are the keys there's a lot of actual

00:20:25,200 --> 00:20:28,240
data uh there's also things where we

00:20:27,360 --> 00:20:30,320
want to actually

00:20:28,240 --> 00:20:31,679
um you know get data out of our

00:20:30,320 --> 00:20:32,880
functions which are much more

00:20:31,679 --> 00:20:35,440
sophisticated

00:20:32,880 --> 00:20:36,320
uh so we cannot use that simple approach

00:20:35,440 --> 00:20:39,120
in that case

00:20:36,320 --> 00:20:40,159
and in that case what we have to do is

00:20:39,120 --> 00:20:40,640
we have to come up with a different

00:20:40,159 --> 00:20:42,960
approach

00:20:40,640 --> 00:20:45,440
and we decided to use an approach which

00:20:42,960 --> 00:20:48,240
uses the conjunction of pandas udf

00:20:45,440 --> 00:20:50,640
with apache arrow and we'll talk about

00:20:48,240 --> 00:20:53,440
apache arrow um in the next slide but

00:20:50,640 --> 00:20:55,200
basically the idea is uds basically

00:20:53,440 --> 00:20:58,240
means a user defined function

00:20:55,200 --> 00:20:59,679
and typically in python uh when you do

00:20:58,240 --> 00:21:02,880
python udfs

00:20:59,679 --> 00:21:05,600
that is basically takes every uh row

00:21:02,880 --> 00:21:07,120
of the data frame and converts and runs

00:21:05,600 --> 00:21:08,320
the function every row and that's that's

00:21:07,120 --> 00:21:10,960
very inefficient

00:21:08,320 --> 00:21:11,760
uh so we wanted to use pandas udfs which

00:21:10,960 --> 00:21:14,320
actually

00:21:11,760 --> 00:21:14,880
work in a much more vectorized fashion

00:21:14,320 --> 00:21:16,400
and

00:21:14,880 --> 00:21:18,799
can increase the performance by up to

00:21:16,400 --> 00:21:20,880
100 times on on python udfs

00:21:18,799 --> 00:21:23,039
and obviously since uh we are going to

00:21:20,880 --> 00:21:24,000
do our models across a lot of different

00:21:23,039 --> 00:21:25,440
dimensions

00:21:24,000 --> 00:21:27,520
uh we want to use something called a

00:21:25,440 --> 00:21:30,559
group map pandas udfs

00:21:27,520 --> 00:21:32,880
uh which allow us to take a

00:21:30,559 --> 00:21:34,240
you know a group by approach uh to split

00:21:32,880 --> 00:21:37,440
apply combine these

00:21:34,240 --> 00:21:38,159
udfs and uh do these functions uh in a

00:21:37,440 --> 00:21:41,919
much more

00:21:38,159 --> 00:21:43,760
seamless fashion um so

00:21:41,919 --> 00:21:45,600
i'll talk a little bit more about apache

00:21:43,760 --> 00:21:49,200
arrow um yeah

00:21:45,600 --> 00:21:51,280
so um so when we uh do this kind of um

00:21:49,200 --> 00:21:52,400
uh go from like you know basically spark

00:21:51,280 --> 00:21:54,799
which is running in uh

00:21:52,400 --> 00:21:56,080
like a java virtual machine and go into

00:21:54,799 --> 00:21:58,799
a python pandas

00:21:56,080 --> 00:21:59,679
api uh there's a lot of serialization

00:21:58,799 --> 00:22:02,720
involved

00:21:59,679 --> 00:22:04,320
if you don't use any apache arrow and

00:22:02,720 --> 00:22:06,240
that civilization can take a lot of time

00:22:04,320 --> 00:22:08,880
and it's very efficient inefficient

00:22:06,240 --> 00:22:10,000
because it works row by row but with

00:22:08,880 --> 00:22:13,120
apache arrow

00:22:10,000 --> 00:22:15,440
we can use a corner uh way to

00:22:13,120 --> 00:22:16,240
uh to basically send this data from uh

00:22:15,440 --> 00:22:20,080
spark to

00:22:16,240 --> 00:22:21,360
uh to python api and to a pandas api and

00:22:20,080 --> 00:22:23,360
this corner is

00:22:21,360 --> 00:22:25,760
data is very efficient because it takes

00:22:23,360 --> 00:22:28,640
advantage of all

00:22:25,760 --> 00:22:29,919
the the sim the architecture of all the

00:22:28,640 --> 00:22:31,919
modern cpus

00:22:29,919 --> 00:22:34,000
and this allows a very efficient way to

00:22:31,919 --> 00:22:34,880
uh to get this data in a very vectorized

00:22:34,000 --> 00:22:37,520
fashion

00:22:34,880 --> 00:22:38,159
uh from uh from basically the spark data

00:22:37,520 --> 00:22:41,280
frame

00:22:38,159 --> 00:22:42,240
uh to the pandas api and uh so this is

00:22:41,280 --> 00:22:44,000
essential in

00:22:42,240 --> 00:22:45,440
getting the performance and the cost

00:22:44,000 --> 00:22:48,480
reduction that we need

00:22:45,440 --> 00:22:50,559
um so i will um uh

00:22:48,480 --> 00:22:51,840
talk a little bit more about the the

00:22:50,559 --> 00:22:54,240
group pandas

00:22:51,840 --> 00:22:56,320
api um so as i said you know basically

00:22:54,240 --> 00:22:57,120
the idea is we have an original data

00:22:56,320 --> 00:22:59,760
frame

00:22:57,120 --> 00:23:00,960
which is consists of all our data and a

00:22:59,760 --> 00:23:03,200
bunch of keys

00:23:00,960 --> 00:23:04,799
and we use those keys to group these

00:23:03,200 --> 00:23:08,480
data frames into

00:23:04,799 --> 00:23:10,240
different groups and these groups then

00:23:08,480 --> 00:23:11,760
go into a pandas function

00:23:10,240 --> 00:23:13,840
and the function is applied to each

00:23:11,760 --> 00:23:14,640
group separately and then the result of

00:23:13,840 --> 00:23:17,679
these groups

00:23:14,640 --> 00:23:20,159
is a pandas output and that

00:23:17,679 --> 00:23:21,679
allows us to get basically a very

00:23:20,159 --> 00:23:24,080
efficient way to do

00:23:21,679 --> 00:23:24,799
this this function so basically the idea

00:23:24,080 --> 00:23:26,880
is

00:23:24,799 --> 00:23:28,880
we are kind of doing the python function

00:23:26,880 --> 00:23:31,280
as a python api

00:23:28,880 --> 00:23:32,880
but we are using these underlying

00:23:31,280 --> 00:23:34,720
technologies like apache arrow and

00:23:32,880 --> 00:23:34,960
pandas udf to do it in a very efficient

00:23:34,720 --> 00:23:38,320
and

00:23:34,960 --> 00:23:41,760
very parallel fashion um so what i'll do

00:23:38,320 --> 00:23:44,880
is i will go through a code snippet

00:23:41,760 --> 00:23:45,520
that actually uh explains how how we do

00:23:44,880 --> 00:23:48,480
this

00:23:45,520 --> 00:23:50,080
uh in a little more detail so uh

00:23:48,480 --> 00:23:52,640
basically if you look at this python

00:23:50,080 --> 00:23:53,200
code um there is a very it's a very

00:23:52,640 --> 00:23:56,720
simple

00:23:53,200 --> 00:23:59,440
um instance of how to run uh several

00:23:56,720 --> 00:24:01,039
uh models of a random forest um

00:23:59,440 --> 00:24:02,480
scikit-learn random forest record

00:24:01,039 --> 00:24:04,240
regressor in parallel

00:24:02,480 --> 00:24:05,840
uh so that would be a good example of

00:24:04,240 --> 00:24:07,840
some of the models we use

00:24:05,840 --> 00:24:09,840
uh so the way we do that is first we

00:24:07,840 --> 00:24:12,159
define a schema

00:24:09,840 --> 00:24:13,679
uh which basically defines what what

00:24:12,159 --> 00:24:14,880
kind of output we're going to have in

00:24:13,679 --> 00:24:15,679
this case we're doing something very

00:24:14,880 --> 00:24:18,720
simple

00:24:15,679 --> 00:24:20,000
uh we have basically our group id which

00:24:18,720 --> 00:24:22,320
is the key we

00:24:20,000 --> 00:24:23,840
do the parallelization by and then we

00:24:22,320 --> 00:24:24,240
have the model string which basically

00:24:23,840 --> 00:24:27,279
just

00:24:24,240 --> 00:24:28,960
uh gives you the model uh file name

00:24:27,279 --> 00:24:31,039
but this this schema can be very

00:24:28,960 --> 00:24:32,880
complicated and we can do

00:24:31,039 --> 00:24:35,360
uh you know complete pandas data frame

00:24:32,880 --> 00:24:38,159
with all of different types of uh uh

00:24:35,360 --> 00:24:39,760
objects in it uh to really uh return a

00:24:38,159 --> 00:24:42,240
very uh you know very uh

00:24:39,760 --> 00:24:44,400
complete data frame uh so in this case

00:24:42,240 --> 00:24:46,960
um you know we have to use a decorator

00:24:44,400 --> 00:24:48,159
uh to to basically instantiate this

00:24:46,960 --> 00:24:50,640
pandas udf

00:24:48,159 --> 00:24:52,320
and we are doing a group map udf uh so

00:24:50,640 --> 00:24:53,919
once we do that then everything else is

00:24:52,320 --> 00:24:56,080
pretty straightforward this is your a

00:24:53,919 --> 00:24:59,120
regular uh python function

00:24:56,080 --> 00:25:02,320
uh that basically has a pandas

00:24:59,120 --> 00:25:03,039
data frame as an input the group id is

00:25:02,320 --> 00:25:04,880
basically

00:25:03,039 --> 00:25:07,440
something we pass as part of this data

00:25:04,880 --> 00:25:10,320
frame and that allows us to identify

00:25:07,440 --> 00:25:11,919
this specific group and then you know

00:25:10,320 --> 00:25:12,400
within the data frame we can have a lot

00:25:11,919 --> 00:25:14,240
of different

00:25:12,400 --> 00:25:16,080
columns and in this case we have three

00:25:14,240 --> 00:25:19,440
columns which has

00:25:16,080 --> 00:25:21,039
the two features and the label and so by

00:25:19,440 --> 00:25:22,480
extracting those columns

00:25:21,039 --> 00:25:25,120
uh we can run the random forest

00:25:22,480 --> 00:25:26,240
regressor on it and then we basically

00:25:25,120 --> 00:25:28,960
can do a pickle

00:25:26,240 --> 00:25:29,919
uh dump of the model and we can

00:25:28,960 --> 00:25:32,159
basically pass

00:25:29,919 --> 00:25:33,279
the the model and the group id back and

00:25:32,159 --> 00:25:36,000
this way

00:25:33,279 --> 00:25:36,880
this whole function runs in parallel

00:25:36,000 --> 00:25:40,480
across several

00:25:36,880 --> 00:25:41,120
executors and the way we instantiate

00:25:40,480 --> 00:25:44,080
this function

00:25:41,120 --> 00:25:45,600
is first we basically enable arrows so

00:25:44,080 --> 00:25:48,720
the serialization is

00:25:45,600 --> 00:25:50,000
is very fast and is vectorized then we

00:25:48,720 --> 00:25:51,919
have our original

00:25:50,000 --> 00:25:54,240
pandas data frame that contains the

00:25:51,919 --> 00:25:55,279
actual data we convert that into a spark

00:25:54,240 --> 00:25:58,480
data frame

00:25:55,279 --> 00:26:00,159
and then we apply the the group by

00:25:58,480 --> 00:26:02,159
model on it i'm sorry group by pandas

00:26:00,159 --> 00:26:04,559
udf on it and

00:26:02,159 --> 00:26:06,080
then once we apply this as as i

00:26:04,559 --> 00:26:07,200
mentioned before spark has lazy

00:26:06,080 --> 00:26:08,880
execution

00:26:07,200 --> 00:26:10,799
so we have to convert this back to

00:26:08,880 --> 00:26:11,679
pandas and this way we get our results

00:26:10,799 --> 00:26:14,400
back

00:26:11,679 --> 00:26:14,880
so it's a really simple way uh with just

00:26:14,400 --> 00:26:16,799
using

00:26:14,880 --> 00:26:18,320
underlying python functions and doing

00:26:16,799 --> 00:26:19,919
parallelization spark

00:26:18,320 --> 00:26:22,080
without getting into too much of the

00:26:19,919 --> 00:26:23,840
nitty gritty of a spark

00:26:22,080 --> 00:26:25,760
so this actually demonstrates you know

00:26:23,840 --> 00:26:28,799
the two ways we have paralyzed

00:26:25,760 --> 00:26:29,520
and been able to scale our models um to

00:26:28,799 --> 00:26:33,039
this um

00:26:29,520 --> 00:26:33,760
this level uh so so that actually

00:26:33,039 --> 00:26:37,200
concludes

00:26:33,760 --> 00:26:38,960
um our presentation uh we have a few

00:26:37,200 --> 00:26:40,480
concluding remarks we can talk about

00:26:38,960 --> 00:26:43,520
those

00:26:40,480 --> 00:26:46,720
so basically you know we presented a

00:26:43,520 --> 00:26:50,400
a way to take the best of kubernetes

00:26:46,720 --> 00:26:52,400
and best of spark to do a scaling

00:26:50,400 --> 00:26:53,679
with end-to-end automation and security

00:26:52,400 --> 00:26:56,799
and ci cd

00:26:53,679 --> 00:26:59,200
uh to basically prioritize our

00:26:56,799 --> 00:27:00,480
models and scale them to a very high

00:26:59,200 --> 00:27:02,159
cardinality

00:27:00,480 --> 00:27:04,159
we used a very unique architecture as

00:27:02,159 --> 00:27:05,840
jakub described

00:27:04,159 --> 00:27:07,600
it's very unique because we embedding

00:27:05,840 --> 00:27:09,840
spark into a micro service within

00:27:07,600 --> 00:27:10,240
kubernetes i think this is a very novel

00:27:09,840 --> 00:27:13,760
but

00:27:10,240 --> 00:27:15,600
uh but very but very simple uh

00:27:13,760 --> 00:27:17,440
uh so obviously you know the one

00:27:15,600 --> 00:27:18,399
advantage we have with spark as yakub

00:27:17,440 --> 00:27:20,320
mentioned was

00:27:18,399 --> 00:27:21,440
you know when we run these models uh we

00:27:20,320 --> 00:27:24,720
can run them

00:27:21,440 --> 00:27:27,760
as basically uh clusters that we create

00:27:24,720 --> 00:27:30,000
and uh terminate once the the actual

00:27:27,760 --> 00:27:32,240
uh job or the training is over that way

00:27:30,000 --> 00:27:32,640
we save a lot of resource courses i'm

00:27:32,240 --> 00:27:35,760
sorry

00:27:32,640 --> 00:27:37,520
resource costs and then as we

00:27:35,760 --> 00:27:39,520
evolve with more models and more

00:27:37,520 --> 00:27:41,200
applications it's very easy for us to

00:27:39,520 --> 00:27:43,760
integrate those into

00:27:41,200 --> 00:27:45,200
into our current infrastructure and now

00:27:43,760 --> 00:27:46,799
you know in terms of scaling

00:27:45,200 --> 00:27:48,799
we're also looking at other dimensions

00:27:46,799 --> 00:27:50,240
of scalings uh that we can do with it

00:27:48,799 --> 00:27:52,880
within this architecture

00:27:50,240 --> 00:27:54,720
uh which is beyond uh just the the

00:27:52,880 --> 00:27:56,240
models itself uh we're looking at

00:27:54,720 --> 00:27:58,159
you know when our application is very

00:27:56,240 --> 00:28:00,159
big and has a lot of different

00:27:58,159 --> 00:28:01,200
complexity we can basically scale within

00:28:00,159 --> 00:28:03,440
the application

00:28:01,200 --> 00:28:05,200
uh with the data itself and then we're

00:28:03,440 --> 00:28:06,880
also looking at some very sophisticated

00:28:05,200 --> 00:28:08,720
deep learning models which you know

00:28:06,880 --> 00:28:10,399
which tend to be very complex

00:28:08,720 --> 00:28:12,320
and we're looking at how we can leverage

00:28:10,399 --> 00:28:14,080
this architecture to even

00:28:12,320 --> 00:28:15,600
scale within those models and

00:28:14,080 --> 00:28:16,720
parallelize those models

00:28:15,600 --> 00:28:19,200
so those are some of the things we are

00:28:16,720 --> 00:28:22,240
going to be doing in the in the future

00:28:19,200 --> 00:28:25,310
great thank you okay uh

00:28:22,240 --> 00:28:26,559
thanks sandeep thanks uh thank you uh

00:28:25,310 --> 00:28:31,279
[Music]

00:28:26,559 --> 00:28:31,279

YouTube URL: https://www.youtube.com/watch?v=6jBFHXsfrx0


