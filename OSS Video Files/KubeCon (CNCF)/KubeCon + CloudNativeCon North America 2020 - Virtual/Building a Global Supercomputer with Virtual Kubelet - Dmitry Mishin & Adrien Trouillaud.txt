Title: Building a Global Supercomputer with Virtual Kubelet - Dmitry Mishin & Adrien Trouillaud
Publication date: 2020-11-23
Playlist: KubeCon + CloudNativeCon North America 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon Europe 2021 Virtual from May 4–7, 2021. Learn more at https://kubecon.io. The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects. 

Building a Global Supercomputer with Virtual Kubelet - Dmitry Mishin, University of California San Diego & Adrien Trouillaud, Admiralty 

Nautilus is a global Kubernetes cluster, product of the Pacific Research Platform (PRP) project at The University of California San Diego (UCSD) and many collaborating campuses. It aggregates compute resources from around the world. While that's impressive, there are issues that a single cluster cannot solve: in this case, decentralized governance. PRP participants delegate the control of some of their compute resources to UCSD. That wouldn't scale to a global network of independent institutions. So PRP, along with peer projects nationwide, decided to adopt Admiralty's open-source multi-cluster scheduler, which allows for decentralized control plane topologies. Dmitry will demo the federation of Nautilus with the clusters of other large scientific networks; Adrien will explain how Admiralty implemented Virtual Kubelet and the Kubernetes scheduler framework to make this possible. 

https://sched.co/ekBh
Captions: 
	00:00:01,599 --> 00:00:06,399
hello everyone this is the talk

00:00:03,439 --> 00:00:07,520
on federation in kubernetes i'm dmitry

00:00:06,399 --> 00:00:11,599
mission i work for

00:00:07,520 --> 00:00:11,599
university of california san diego

00:00:12,400 --> 00:00:16,160
this talk will have two parts in part

00:00:14,639 --> 00:00:19,520
one i will describe

00:00:16,160 --> 00:00:20,640
our cluster kubernetes cluster how we

00:00:19,520 --> 00:00:22,720
use admiralty

00:00:20,640 --> 00:00:24,720
how our project came to the current

00:00:22,720 --> 00:00:27,840
state it's in

00:00:24,720 --> 00:00:28,640
and in the second part uh adrian the

00:00:27,840 --> 00:00:32,000
creator of

00:00:28,640 --> 00:00:32,880
admiralty will describe how admiralty is

00:00:32,000 --> 00:00:35,200
working

00:00:32,880 --> 00:00:38,079
also we will have the demo on how

00:00:35,200 --> 00:00:38,079
admiralty works

00:00:40,320 --> 00:00:44,960
so our project is called prp pacific

00:00:43,440 --> 00:00:48,640
research platform

00:00:44,960 --> 00:00:51,840
and it grew from a project

00:00:48,640 --> 00:00:55,280
measuring network performance so the

00:00:51,840 --> 00:00:55,280
initial problem was that

00:00:55,760 --> 00:01:00,000
university of california san diego had

00:00:57,920 --> 00:01:02,719
multiple campuses connected with

00:01:00,000 --> 00:01:03,440
what's called science dmz a scientific

00:01:02,719 --> 00:01:05,920
network

00:01:03,440 --> 00:01:07,840
uh 10 to 100 gigabits per second so

00:01:05,920 --> 00:01:10,720
pretty fast

00:01:07,840 --> 00:01:12,479
and sometimes this some segments of the

00:01:10,720 --> 00:01:14,799
network doesn't perform

00:01:12,479 --> 00:01:15,600
the way they're supposed to and prp was

00:01:14,799 --> 00:01:17,840
measuring

00:01:15,600 --> 00:01:20,560
uh network performance by sending test

00:01:17,840 --> 00:01:22,560
data between those locations

00:01:20,560 --> 00:01:24,720
uh that was done manually and at some

00:01:22,560 --> 00:01:27,680
point uh there were just

00:01:24,720 --> 00:01:28,960
more nodes that prp could handle

00:01:27,680 --> 00:01:30,799
manually

00:01:28,960 --> 00:01:33,119
and there was an idea to deploy

00:01:30,799 --> 00:01:36,880
kubernetes on top so that kubernetes

00:01:33,119 --> 00:01:41,040
would orchestrate those measurement

00:01:36,880 --> 00:01:42,960
software uh once we deployed kubernetes

00:01:41,040 --> 00:01:45,920
we realized that now we have

00:01:42,960 --> 00:01:48,240
a cluster of nodes uh we just

00:01:45,920 --> 00:01:50,720
occasionally ascend data but

00:01:48,240 --> 00:01:51,840
mostly city idle and we started

00:01:50,720 --> 00:01:55,520
providing

00:01:51,840 --> 00:01:58,719
those hardware resources to scientists

00:01:55,520 --> 00:02:00,560
uh to just do their computations and we

00:01:58,719 --> 00:02:02,880
saw exponential growth of

00:02:00,560 --> 00:02:04,240
our cluster because it was really easy

00:02:02,880 --> 00:02:09,759
to onboard

00:02:04,240 --> 00:02:12,640
our our system uh bring in the container

00:02:09,759 --> 00:02:13,680
run it and get free access to the

00:02:12,640 --> 00:02:16,879
resources for

00:02:13,680 --> 00:02:20,800
scientific use we call this cluster

00:02:16,879 --> 00:02:23,760
nautilus at some point

00:02:20,800 --> 00:02:24,400
more projects started donating hardware

00:02:23,760 --> 00:02:27,520
to us

00:02:24,400 --> 00:02:30,640
uh we partnered with internet too

00:02:27,520 --> 00:02:34,239
open science grid donated some notes uh

00:02:30,640 --> 00:02:39,360
more networks were coming

00:02:34,239 --> 00:02:42,879
to us and this is how

00:02:39,360 --> 00:02:45,760
now it looks in us

00:02:42,879 --> 00:02:46,959
so we have a scenic network in

00:02:45,760 --> 00:02:51,040
california

00:02:46,959 --> 00:02:53,280
providing us the internet access

00:02:51,040 --> 00:02:54,319
and there is a bunch of regional

00:02:53,280 --> 00:02:57,040
networks that

00:02:54,319 --> 00:02:59,280
also donate hardware to our cluster so

00:02:57,040 --> 00:03:01,920
basically we have national

00:02:59,280 --> 00:03:02,560
kubernetes cluster aggregating a bunch

00:03:01,920 --> 00:03:05,920
of

00:03:02,560 --> 00:03:09,360
computing resources

00:03:05,920 --> 00:03:10,319
at some point we started bringing in gpu

00:03:09,360 --> 00:03:13,680
nodes

00:03:10,319 --> 00:03:17,200
storage nodes and so on so now

00:03:13,680 --> 00:03:19,040
our cluster is pretty big and

00:03:17,200 --> 00:03:22,000
at some point it started growing

00:03:19,040 --> 00:03:25,120
internationally

00:03:22,000 --> 00:03:26,480
so an international scale uh we have

00:03:25,120 --> 00:03:29,680
notes in europe

00:03:26,480 --> 00:03:34,239
in asia and pacific region region

00:03:29,680 --> 00:03:36,879
we have known in singapore korea guam

00:03:34,239 --> 00:03:36,879
australia

00:03:40,000 --> 00:03:44,400
and if we're talking about nodes

00:03:42,879 --> 00:03:48,799
themselves those are

00:03:44,400 --> 00:03:52,560
called fiona nodes flash io network

00:03:48,799 --> 00:03:53,519
appliances so those nodes are optimized

00:03:52,560 --> 00:03:56,080
for

00:03:53,519 --> 00:03:57,680
fast networks so they can actually

00:03:56,080 --> 00:04:01,120
leverage those

00:03:57,680 --> 00:04:04,319
speeds and some nodes can fit up to

00:04:01,120 --> 00:04:07,519
8 gaming gpus

00:04:04,319 --> 00:04:08,319
and some nodes have a bunch of hard disk

00:04:07,519 --> 00:04:15,840
which we use

00:04:08,319 --> 00:04:15,840
for storage for ourselves cluster

00:04:16,239 --> 00:04:23,440
so when we talk about federation

00:04:19,519 --> 00:04:27,840
our current state is that we have

00:04:23,440 --> 00:04:30,880
our prp notice cluster

00:04:27,840 --> 00:04:31,520
national for us and some international

00:04:30,880 --> 00:04:34,800
nodes

00:04:31,520 --> 00:04:38,520
um now it has 7000

00:04:34,800 --> 00:04:41,680
cpu cores more than 500 gpus

00:04:38,520 --> 00:04:45,199
2.5 petabytes of

00:04:41,680 --> 00:04:46,000
of self storage additional to that we

00:04:45,199 --> 00:04:48,800
deployed

00:04:46,000 --> 00:04:50,880
a couple of smaller clusters we have a

00:04:48,800 --> 00:04:53,440
development k3s arm

00:04:50,880 --> 00:04:55,680
cluster we have development windows

00:04:53,440 --> 00:04:58,880
kubernetes cluster

00:04:55,680 --> 00:05:02,960
agonist gpu cluster that's for

00:04:58,880 --> 00:05:06,080
gaming and 3d visualization and

00:05:02,960 --> 00:05:09,600
we spawn to aws

00:05:06,080 --> 00:05:12,160
cloud so all those clusters

00:05:09,600 --> 00:05:15,360
are separate it's really hard to

00:05:12,160 --> 00:05:19,199
integrate all that hardware in one

00:05:15,360 --> 00:05:22,400
and we need a

00:05:19,199 --> 00:05:24,880
federation to work with them

00:05:22,400 --> 00:05:26,160
also we partnered with several other

00:05:24,880 --> 00:05:28,320
smaller clusters

00:05:26,160 --> 00:05:29,520
and there is a federation already

00:05:28,320 --> 00:05:32,000
established between

00:05:29,520 --> 00:05:32,960
those locations so basically all

00:05:32,000 --> 00:05:35,280
clusters can

00:05:32,960 --> 00:05:36,240
finally talk to each other but are

00:05:35,280 --> 00:05:38,560
controlled by

00:05:36,240 --> 00:05:40,479
different organizations and different

00:05:38,560 --> 00:05:43,440
people

00:05:40,479 --> 00:05:44,080
that's the huge benefit of admiralty

00:05:43,440 --> 00:05:47,039
because

00:05:44,080 --> 00:05:47,840
all our other projects are seeing

00:05:47,039 --> 00:05:50,720
federation

00:05:47,840 --> 00:05:51,280
as one cluster controlling others it's

00:05:50,720 --> 00:05:55,120
not

00:05:51,280 --> 00:05:58,560
one level federation between them

00:05:55,120 --> 00:06:02,960
and what we are looking forward to is

00:05:58,560 --> 00:06:06,319
when expense supercomputer is deployed

00:06:02,960 --> 00:06:09,520
it's also going to have the kubernetes

00:06:06,319 --> 00:06:12,000
cluster as a part of it and admiralty

00:06:09,520 --> 00:06:12,720
will be used to federate expense with

00:06:12,000 --> 00:06:16,240
notos

00:06:12,720 --> 00:06:20,000
and other clusters and that will

00:06:16,240 --> 00:06:23,039
bring a theoretical expansion

00:06:20,000 --> 00:06:24,720
to 100 000 cores

00:06:23,039 --> 00:06:26,560
of course we will not get all of them

00:06:24,720 --> 00:06:29,840
but we're

00:06:26,560 --> 00:06:34,560
that's a pretty big increase in

00:06:29,840 --> 00:06:36,319
what we federate with so how

00:06:34,560 --> 00:06:37,919
can we use resources between those

00:06:36,319 --> 00:06:41,520
clusters

00:06:37,919 --> 00:06:42,639
uh as i said uh most of these clusters

00:06:41,520 --> 00:06:45,360
belong to different

00:06:42,639 --> 00:06:46,000
organizations who want to set their own

00:06:45,360 --> 00:06:49,120
rules so

00:06:46,000 --> 00:06:53,440
we cannot federation cannot dictate

00:06:49,120 --> 00:06:56,800
uh policies for running stuff

00:06:53,440 --> 00:06:59,199
in different clusters and scientists who

00:06:56,800 --> 00:07:00,319
use those clusters usually have access

00:06:59,199 --> 00:07:03,120
to only one

00:07:00,319 --> 00:07:04,319
or several namespaces they again don't

00:07:03,120 --> 00:07:07,440
control the whole

00:07:04,319 --> 00:07:10,720
cluster and so federation should

00:07:07,440 --> 00:07:13,280
uh allow working on namespaces level

00:07:10,720 --> 00:07:15,039
when people don't have to bug the admin

00:07:13,280 --> 00:07:18,400
to federate but they can

00:07:15,039 --> 00:07:22,400
establish federations between the

00:07:18,400 --> 00:07:22,400
piece of the cluster that they control

00:07:24,000 --> 00:07:30,319
additional to those small clusters

00:07:27,280 --> 00:07:33,599
which are on-prem as i said we

00:07:30,319 --> 00:07:36,639
federate with clouds uh amazon

00:07:33,599 --> 00:07:38,880
microsoft azure google cloud

00:07:36,639 --> 00:07:40,560
if we need some resources from those

00:07:38,880 --> 00:07:42,880
clusters if we have some

00:07:40,560 --> 00:07:44,639
big workloads or we want specialized

00:07:42,880 --> 00:07:46,879
hardware

00:07:44,639 --> 00:07:48,720
we can always federate with those by

00:07:46,879 --> 00:07:50,800
simply running

00:07:48,720 --> 00:07:54,240
some temporary cluster in those and then

00:07:50,800 --> 00:07:54,240
establishing the federation

00:07:55,360 --> 00:07:59,840
so how do we use federation now

00:08:00,639 --> 00:08:07,039
gitlab ci is what we started with

00:08:04,000 --> 00:08:10,479
it's super convenient to like when we

00:08:07,039 --> 00:08:14,080
need to build a container on some

00:08:10,479 --> 00:08:17,360
specialized hardware like arm or windows

00:08:14,080 --> 00:08:19,360
uh instead of manually creating a gitlab

00:08:17,360 --> 00:08:21,680
runner somewhere else or

00:08:19,360 --> 00:08:23,599
trying to join this node in our big

00:08:21,680 --> 00:08:26,639
cluster we just federate

00:08:23,599 --> 00:08:29,039
and that will be in the demo so now we

00:08:26,639 --> 00:08:30,400
feather it with windows and arm and our

00:08:29,039 --> 00:08:33,440
gitlab

00:08:30,400 --> 00:08:34,240
by just tagging the job can build the

00:08:33,440 --> 00:08:37,360
container

00:08:34,240 --> 00:08:40,880
in some other cluster and this container

00:08:37,360 --> 00:08:43,039
will be stored in gitlab registry

00:08:40,880 --> 00:08:45,760
network monitoring right now we have a

00:08:43,039 --> 00:08:49,040
bunch of monitoring ports in our

00:08:45,760 --> 00:08:52,000
large cluster but the goal is that

00:08:49,040 --> 00:08:53,200
any other cluster that wants to have

00:08:52,000 --> 00:08:56,000
automated

00:08:53,200 --> 00:08:56,480
monitoring can just join our federation

00:08:56,000 --> 00:09:00,160
and get

00:08:56,480 --> 00:09:00,160
all the monitoring pods

00:09:00,800 --> 00:09:06,560
uh jobs bursting if some cluster has

00:09:04,000 --> 00:09:07,440
some unused resources and another one is

00:09:06,560 --> 00:09:09,600
overloaded

00:09:07,440 --> 00:09:10,800
it's always useful to just burst to

00:09:09,600 --> 00:09:14,320
another cluster

00:09:10,800 --> 00:09:15,519
and clusters can share their computing

00:09:14,320 --> 00:09:18,480
resources without

00:09:15,519 --> 00:09:21,120
actually reattaching nodes so that's

00:09:18,480 --> 00:09:24,320
super convenient

00:09:21,120 --> 00:09:25,040
medical data uses a big one uh some data

00:09:24,320 --> 00:09:28,240
can

00:09:25,040 --> 00:09:31,360
is highly protected it cannot leave

00:09:28,240 --> 00:09:34,800
some cluster but this data can

00:09:31,360 --> 00:09:36,959
be used for uh some computation and then

00:09:34,800 --> 00:09:38,560
results of this computation can be

00:09:36,959 --> 00:09:42,000
shared

00:09:38,560 --> 00:09:46,080
when it's anonymized so federation

00:09:42,000 --> 00:09:49,839
again can let you spawn your

00:09:46,080 --> 00:09:52,000
computing job in another cluster

00:09:49,839 --> 00:09:52,880
get some product anonymize it get it

00:09:52,000 --> 00:09:55,839
back

00:09:52,880 --> 00:09:56,560
so this is super convenient special

00:09:55,839 --> 00:09:58,880
devices

00:09:56,560 --> 00:09:59,839
internet of things some devices are

00:09:58,880 --> 00:10:03,040
really small

00:09:59,839 --> 00:10:04,160
and tiny and they can't just join a big

00:10:03,040 --> 00:10:06,800
cluster and

00:10:04,160 --> 00:10:09,440
just monitoring pots will kill it so

00:10:06,800 --> 00:10:13,120
again this is super useful to federate

00:10:09,440 --> 00:10:14,640
and some iot devices are again

00:10:13,120 --> 00:10:18,000
controlled by

00:10:14,640 --> 00:10:20,800
some other people and

00:10:18,000 --> 00:10:22,240
federation helps establish those

00:10:20,800 --> 00:10:24,720
connections without

00:10:22,240 --> 00:10:28,000
getting or requiring access to the

00:10:24,720 --> 00:10:28,000
specialized hardware

00:10:28,959 --> 00:10:32,880
uh future of federation we are now

00:10:32,160 --> 00:10:36,399
working

00:10:32,880 --> 00:10:40,079
on the project that will allow us to

00:10:36,399 --> 00:10:41,040
uh create on-demand player to a fast

00:10:40,079 --> 00:10:43,839
pass

00:10:41,040 --> 00:10:45,600
around the world so there is a that's a

00:10:43,839 --> 00:10:48,880
project with surfnet

00:10:45,600 --> 00:10:52,640
it's called nsa autogoal

00:10:48,880 --> 00:10:56,240
and this project already can

00:10:52,640 --> 00:10:59,200
create a layer two connections and

00:10:56,240 --> 00:11:00,079
tear them down on request and what we

00:10:59,200 --> 00:11:03,279
are writing now

00:11:00,079 --> 00:11:06,480
is a special crd and operator

00:11:03,279 --> 00:11:08,640
which will allow pods that spawn in some

00:11:06,480 --> 00:11:10,480
remote location to create a path for

00:11:08,640 --> 00:11:11,519
example to storage in some other

00:11:10,480 --> 00:11:13,760
location

00:11:11,519 --> 00:11:14,959
and this path will exist while the pod

00:11:13,760 --> 00:11:18,800
is running

00:11:14,959 --> 00:11:22,000
and then this path will will disappear

00:11:18,800 --> 00:11:24,320
and federation will help us to

00:11:22,000 --> 00:11:25,279
run this across several kubernetes

00:11:24,320 --> 00:11:27,040
clusters

00:11:25,279 --> 00:11:29,040
so if some cluster wants to talk to

00:11:27,040 --> 00:11:30,800
another cluster it just creates a layer

00:11:29,040 --> 00:11:33,839
2 connection

00:11:30,800 --> 00:11:36,399
and that will be controlled by standard

00:11:33,839 --> 00:11:38,480
kubernetes api so this is

00:11:36,399 --> 00:11:42,079
a really great project that we're

00:11:38,480 --> 00:11:45,519
working on

00:11:42,079 --> 00:11:48,160
is the demo on federation in this demo i

00:11:45,519 --> 00:11:49,680
will show how federation works in our

00:11:48,160 --> 00:11:51,600
production cluster

00:11:49,680 --> 00:11:52,800
i will not go through details of

00:11:51,600 --> 00:11:55,279
establishing

00:11:52,800 --> 00:11:56,160
the connection it's all covered in

00:11:55,279 --> 00:11:57,839
documentation

00:11:56,160 --> 00:11:59,680
and this is supposed to be the short

00:11:57,839 --> 00:12:02,560
demo

00:11:59,680 --> 00:12:04,480
so now we're looking at the nodes in our

00:12:02,560 --> 00:12:06,079
production cluster and we see that the

00:12:04,480 --> 00:12:09,680
first node

00:12:06,079 --> 00:12:11,760
has the role of cluster and master

00:12:09,680 --> 00:12:13,519
this is virtual node created by

00:12:11,760 --> 00:12:16,560
admiralty so basically

00:12:13,519 --> 00:12:18,880
this represents the whole remote cluster

00:12:16,560 --> 00:12:20,160
and federated parts will run in this

00:12:18,880 --> 00:12:23,920
node

00:12:20,160 --> 00:12:25,600
while the actual node is below it in

00:12:23,920 --> 00:12:29,279
this demo i will show how

00:12:25,600 --> 00:12:33,839
our gitlab can spawn parts in

00:12:29,279 --> 00:12:37,120
other clusters and get results from them

00:12:33,839 --> 00:12:40,160
so here is the jobs

00:12:37,120 --> 00:12:40,880
table of our gitlab the these jobs are

00:12:40,160 --> 00:12:44,480
built in

00:12:40,880 --> 00:12:45,680
container and putting it in gitlab

00:12:44,480 --> 00:12:48,959
registry

00:12:45,680 --> 00:12:49,760
but while uh most projects are just

00:12:48,959 --> 00:12:52,480
building

00:12:49,760 --> 00:12:53,120
uh containers in our regular cluster

00:12:52,480 --> 00:12:56,639
this

00:12:53,120 --> 00:13:00,560
uh project is setting up this

00:12:56,639 --> 00:13:03,360
architecture arm 64 tag

00:13:00,560 --> 00:13:04,079
on the jobs which make them go to

00:13:03,360 --> 00:13:08,000
special

00:13:04,079 --> 00:13:08,000
runner in git lab

00:13:09,920 --> 00:13:17,600
let's look at our gitlab

00:13:13,839 --> 00:13:18,160
namespace so in this namespace we will

00:13:17,600 --> 00:13:21,600
see that

00:13:18,160 --> 00:13:24,240
it's labeled with multi-cluster

00:13:21,600 --> 00:13:25,120
scheduler enabled this will tell

00:13:24,240 --> 00:13:27,760
admiralty

00:13:25,120 --> 00:13:29,120
to look at this namespace and wait for

00:13:27,760 --> 00:13:32,959
parts that we mark

00:13:29,120 --> 00:13:33,600
as federated if we go in this namespace

00:13:32,959 --> 00:13:36,560
we will see

00:13:33,600 --> 00:13:37,839
several runners running already on

00:13:36,560 --> 00:13:41,600
regular

00:13:37,839 --> 00:13:43,440
notes if you look at deployments

00:13:41,600 --> 00:13:47,120
there is already deployment that i

00:13:43,440 --> 00:13:50,000
created called gitlab render federated

00:13:47,120 --> 00:13:50,000
we can look at it

00:13:52,000 --> 00:13:58,399
we will see that in template for parts

00:13:55,440 --> 00:13:58,959
it has the multi-cluster administrator

00:13:58,399 --> 00:14:02,160
elect

00:13:58,959 --> 00:14:04,720
annotation so this again will

00:14:02,160 --> 00:14:06,000
tell admiralty that parts from this

00:14:04,720 --> 00:14:08,240
deployment should be

00:14:06,000 --> 00:14:11,120
federated so admiralty will decide where

00:14:08,240 --> 00:14:11,120
to send them to

00:14:12,079 --> 00:14:18,160
let's scale this deployment

00:14:15,680 --> 00:14:18,160
to one

00:14:20,079 --> 00:14:30,240
and we will see that this new pod

00:14:24,480 --> 00:14:32,880
is scheduled to run on this virtual node

00:14:30,240 --> 00:14:33,440
so if you look at all uh ports all of

00:14:32,880 --> 00:14:37,360
them are

00:14:33,440 --> 00:14:40,480
in our cluster but this one part is

00:14:37,360 --> 00:14:40,480
running somewhere else

00:14:42,399 --> 00:14:48,959
we can look at our arm

00:14:46,399 --> 00:14:48,959
cluster

00:14:49,839 --> 00:14:55,199
again this is gitlab namespace but this

00:14:52,320 --> 00:14:59,120
is now remote cluster federated with our

00:14:55,199 --> 00:15:01,120
large one and this is the node

00:14:59,120 --> 00:15:02,880
this part is running on so we see that

00:15:01,120 --> 00:15:06,160
gitlab runner

00:15:02,880 --> 00:15:09,279
actually started on remote node

00:15:06,160 --> 00:15:10,880
and in our large cluster we only have

00:15:09,279 --> 00:15:15,199
the proxy for this

00:15:10,880 --> 00:15:17,839
pod let's switch back to our

00:15:15,199 --> 00:15:17,839
main quest

00:15:20,800 --> 00:15:26,720
so this runner already registered

00:15:23,440 --> 00:15:29,600
automatically in gitlab and if we rerun

00:15:26,720 --> 00:15:29,600
one of the jobs

00:15:31,519 --> 00:15:35,839
the runner will run it remotely

00:15:36,560 --> 00:15:43,600
so if we go again to remote cluster

00:15:40,480 --> 00:15:43,600
we'll see that new

00:15:44,720 --> 00:15:48,560
job started

00:15:51,360 --> 00:15:57,279
and it just finished this is pretty

00:15:54,959 --> 00:15:57,279
quick

00:15:57,440 --> 00:16:03,839
and the result of this job went into

00:16:00,720 --> 00:16:07,360
the local container

00:16:03,839 --> 00:16:09,680
registry in our gitlab so this completed

00:16:07,360 --> 00:16:13,279
successfully

00:16:09,680 --> 00:16:16,160
this allows gitlab to run

00:16:13,279 --> 00:16:16,800
runners in federated clusters without

00:16:16,160 --> 00:16:20,000
manually

00:16:16,800 --> 00:16:23,839
setting them up and control them from

00:16:20,000 --> 00:16:23,839
one single location

00:16:26,720 --> 00:16:30,560
hi my name is adrian i'm the ceo of

00:16:28,880 --> 00:16:31,519
admiralty admiralty is the company

00:16:30,560 --> 00:16:34,720
behind the open source

00:16:31,519 --> 00:16:36,000
project of the same name admiralty is

00:16:34,720 --> 00:16:38,720
what makes the decentralized

00:16:36,000 --> 00:16:40,399
federation presented by dimitri possible

00:16:38,720 --> 00:16:42,160
it's a multi-class control plane

00:16:40,399 --> 00:16:44,720
that uses common kubernetes extension

00:16:42,160 --> 00:16:46,399
patterns this talk will focus on

00:16:44,720 --> 00:16:48,000
virtual cubelet and the scheduler

00:16:46,399 --> 00:16:50,079
framework

00:16:48,000 --> 00:16:51,680
what is virtual cubelet this is a

00:16:50,079 --> 00:16:56,000
screenshot from the virtual cube

00:16:51,680 --> 00:16:58,480
website so like the kubernetes cubelet

00:16:56,000 --> 00:17:00,240
that runs on each node a virtual cubelet

00:16:58,480 --> 00:17:02,399
instance presents itself

00:17:00,240 --> 00:17:04,319
as a virtual node but instead of running

00:17:02,399 --> 00:17:07,760
containers on a local vm

00:17:04,319 --> 00:17:10,319
it runs them on a remote system

00:17:07,760 --> 00:17:10,799
and there are 11 known providers to the

00:17:10,319 --> 00:17:13,919
state

00:17:10,799 --> 00:17:15,760
including admiralty in the case of

00:17:13,919 --> 00:17:19,199
admiralty virtual cubelets

00:17:15,760 --> 00:17:20,559
represent remote clusters to help us

00:17:19,199 --> 00:17:22,319
understand the concepts that i'll

00:17:20,559 --> 00:17:24,559
explain later let's consider this

00:17:22,319 --> 00:17:28,160
example use case where a user submits

00:17:24,559 --> 00:17:30,320
jobs in their own cluster cluster a

00:17:28,160 --> 00:17:33,360
but containers run in other clusters b

00:17:30,320 --> 00:17:35,200
and c and so in cluster a

00:17:33,360 --> 00:17:36,559
there are virtual nodes that represent

00:17:35,200 --> 00:17:39,039
the other clusters

00:17:36,559 --> 00:17:40,000
the bots created by the kubernetes job

00:17:39,039 --> 00:17:42,000
controller

00:17:40,000 --> 00:17:46,240
are mutated by admiralty submission

00:17:42,000 --> 00:17:48,240
controller into what we call proxy pods

00:17:46,240 --> 00:17:50,080
we call them like that because they

00:17:48,240 --> 00:17:51,600
represent other pods

00:17:50,080 --> 00:17:53,840
not because they are proxy in the

00:17:51,600 --> 00:17:56,080
networking sense

00:17:53,840 --> 00:17:58,000
admiralty's proxy pod scheduler

00:17:56,080 --> 00:17:58,960
schedules those spots to the virtual

00:17:58,000 --> 00:18:02,400
nodes

00:17:58,960 --> 00:18:05,760
and creates candidate pods

00:18:02,400 --> 00:18:07,679
in the other clusters some of those

00:18:05,760 --> 00:18:11,280
candidates become delegate plots

00:18:07,679 --> 00:18:13,760
that are bound to real nodes

00:18:11,280 --> 00:18:14,960
those real nodes could actually be other

00:18:13,760 --> 00:18:16,960
virtual nodes

00:18:14,960 --> 00:18:19,520
you can imagine several levels of

00:18:16,960 --> 00:18:19,520
inception

00:18:19,840 --> 00:18:24,000
admiralty also includes a bunch of

00:18:21,360 --> 00:18:25,919
controllers to update bot statuses

00:18:24,000 --> 00:18:28,240
make config maps secrets and other

00:18:25,919 --> 00:18:30,880
dependencies follow pods

00:18:28,240 --> 00:18:32,640
notice that cluster a needs to talk to

00:18:30,880 --> 00:18:33,840
clusters b and c and we'll see different

00:18:32,640 --> 00:18:37,280
ways to do that

00:18:33,840 --> 00:18:37,840
in a minute let's go back to virtual

00:18:37,280 --> 00:18:41,039
cubelet

00:18:37,840 --> 00:18:42,160
and how admiralty implements it virtual

00:18:41,039 --> 00:18:45,200
cubelet has

00:18:42,160 --> 00:18:47,760
four main responsibilities

00:18:45,200 --> 00:18:49,280
the first responsibility is to register

00:18:47,760 --> 00:18:51,919
a node object

00:18:49,280 --> 00:18:55,039
admiralty creates virtual nodes based on

00:18:51,919 --> 00:18:58,480
user created targets and cluster targets

00:18:55,039 --> 00:19:01,600
those are custom resource definitions

00:18:58,480 --> 00:19:02,480
that basically uh give a name to a

00:19:01,600 --> 00:19:05,039
virtual node

00:19:02,480 --> 00:19:06,720
and refer to a secret that will be used

00:19:05,039 --> 00:19:09,760
to talk to the corresponding

00:19:06,720 --> 00:19:13,520
target cluster the second responsibility

00:19:09,760 --> 00:19:14,080
is heartbeat kubernetes needs to know

00:19:13,520 --> 00:19:17,600
that a

00:19:14,080 --> 00:19:20,720
node is healthy at regular intervals

00:19:17,600 --> 00:19:22,960
otherwise it'll evict the pods

00:19:20,720 --> 00:19:24,320
if the node is not responding so the

00:19:22,960 --> 00:19:27,840
virtual cubelet

00:19:24,320 --> 00:19:28,799
in admiralty um checks the health of the

00:19:27,840 --> 00:19:31,840
target clusters and

00:19:28,799 --> 00:19:34,480
updates the condition of the node

00:19:31,840 --> 00:19:36,400
the third and maybe the most important

00:19:34,480 --> 00:19:39,440
responsibility is to handle

00:19:36,400 --> 00:19:42,880
pods so to run the actual containers

00:19:39,440 --> 00:19:43,919
somewhere um and in admiralty this is

00:19:42,880 --> 00:19:46,880
most of the logic

00:19:43,919 --> 00:19:47,760
with multi-cluster scheduling status

00:19:46,880 --> 00:19:49,760
feedback

00:19:47,760 --> 00:19:51,360
cross cluster garbage collection and so

00:19:49,760 --> 00:19:53,840
on

00:19:51,360 --> 00:19:54,880
know of course there are some uh uh

00:19:53,840 --> 00:19:58,080
other features

00:19:54,880 --> 00:20:01,039
like uh handling logs requests and

00:19:58,080 --> 00:20:02,559
exact requests and in admiralty's case

00:20:01,039 --> 00:20:03,360
it's very simple we just forward those

00:20:02,559 --> 00:20:07,840
requests

00:20:03,360 --> 00:20:07,840
to the target clusters kubernetes api

00:20:08,159 --> 00:20:16,080
three last rows require two configs

00:20:12,159 --> 00:20:16,080
to call the target clusters

00:20:18,080 --> 00:20:22,400
so let's talk about that in admiralty

00:20:20,480 --> 00:20:23,440
clusters are connected in the control

00:20:22,400 --> 00:20:26,960
plane

00:20:23,440 --> 00:20:28,640
in one to one relationships

00:20:26,960 --> 00:20:30,480
and we say that the source and the

00:20:28,640 --> 00:20:32,159
target cluster are connected

00:20:30,480 --> 00:20:34,000
when controllers in the source cluster

00:20:32,159 --> 00:20:37,360
can call the kubernetes api server

00:20:34,000 --> 00:20:39,440
of the target cluster for that

00:20:37,360 --> 00:20:40,400
we need three ingredients routing

00:20:39,440 --> 00:20:43,600
authentication and

00:20:40,400 --> 00:20:48,080
authorization routing may require

00:20:43,600 --> 00:20:52,080
a vpn for a tunnel if the clusters uh on

00:20:48,080 --> 00:20:55,280
public or if they're in different vpcs

00:20:52,080 --> 00:20:57,520
authorization the last one

00:20:55,280 --> 00:20:59,760
is uh is quite straightforward uh he

00:20:57,520 --> 00:21:01,120
uses object resources

00:20:59,760 --> 00:21:03,360
in the target cluster and that's very

00:21:01,120 --> 00:21:06,480
important to dimitri and

00:21:03,360 --> 00:21:08,240
his colleagues and partners they want

00:21:06,480 --> 00:21:11,520
cluster admins to be in control of who

00:21:08,240 --> 00:21:11,520
can do what in their clusters

00:21:11,600 --> 00:21:17,840
and authentication can be done using

00:21:14,880 --> 00:21:17,840
different methods

00:21:18,960 --> 00:21:22,320
the the nice thing about having

00:21:20,480 --> 00:21:24,400
one-to-one

00:21:22,320 --> 00:21:25,760
relationships or connect cluster

00:21:24,400 --> 00:21:28,159
connections is that

00:21:25,760 --> 00:21:30,400
you can uh build any kind of topology

00:21:28,159 --> 00:21:32,240
cluster topology with those as long as

00:21:30,400 --> 00:21:34,480
it's a directed graph

00:21:32,240 --> 00:21:36,159
it's valid and so the first one that

00:21:34,480 --> 00:21:37,760
comes to mind is probably a management

00:21:36,159 --> 00:21:38,559
cluster talking too many workload

00:21:37,760 --> 00:21:41,360
clusters

00:21:38,559 --> 00:21:43,440
or you can do cloud bursting where a

00:21:41,360 --> 00:21:45,840
cluster is its own

00:21:43,440 --> 00:21:47,360
target in the case of the research

00:21:45,840 --> 00:21:50,240
platform presented by dmitry

00:21:47,360 --> 00:21:50,960
we have a decentralized federation where

00:21:50,240 --> 00:21:58,240
you know any

00:21:50,960 --> 00:21:59,840
there's no uh no leader

00:21:58,240 --> 00:22:02,400
i said i would talk about cross-cluster

00:21:59,840 --> 00:22:05,520
authentication this could

00:22:02,400 --> 00:22:08,240
be a talk um a full talk

00:22:05,520 --> 00:22:10,320
um just by itself so i'll go very

00:22:08,240 --> 00:22:13,120
quickly

00:22:10,320 --> 00:22:14,080
uh the the the simplest way to uh to

00:22:13,120 --> 00:22:16,240
achieve it

00:22:14,080 --> 00:22:18,159
is to uh take the service account token

00:22:16,240 --> 00:22:20,960
from target cluster

00:22:18,159 --> 00:22:22,480
and export it and save it in the source

00:22:20,960 --> 00:22:24,320
cluster

00:22:22,480 --> 00:22:26,080
you could do the same thing with the

00:22:24,320 --> 00:22:27,280
certificates api

00:22:26,080 --> 00:22:29,440
instead of a token you would have a

00:22:27,280 --> 00:22:30,720
certificate the problem with those two

00:22:29,440 --> 00:22:34,080
methods is that

00:22:30,720 --> 00:22:35,360
you're using the target clusters as an

00:22:34,080 --> 00:22:37,360
identity provider

00:22:35,360 --> 00:22:39,039
and not the source cluster so you need

00:22:37,360 --> 00:22:41,440
to

00:22:39,039 --> 00:22:43,440
distribute and rotate the secrets

00:22:41,440 --> 00:22:45,440
yourself

00:22:43,440 --> 00:22:46,559
with other methods you can use an

00:22:45,440 --> 00:22:49,200
identity provider

00:22:46,559 --> 00:22:51,440
available in the source cluster to get

00:22:49,200 --> 00:22:52,400
your tokens or certificates and present

00:22:51,440 --> 00:22:55,440
those

00:22:52,400 --> 00:22:57,919
to the target cluster

00:22:55,440 --> 00:22:59,520
and that target cluster has to be able

00:22:57,919 --> 00:23:02,080
to recognize them

00:22:59,520 --> 00:23:03,280
and so if you're in the cloud you can

00:23:02,080 --> 00:23:05,919
use

00:23:03,280 --> 00:23:09,120
um cloud like a kubernetes service

00:23:05,919 --> 00:23:12,720
account in the in a source cluster

00:23:09,120 --> 00:23:15,679
can impersonate uh like an aws

00:23:12,720 --> 00:23:16,320
uh i am role or google cloud service

00:23:15,679 --> 00:23:18,559
account

00:23:16,320 --> 00:23:20,400
azure search principle however you name

00:23:18,559 --> 00:23:24,960
workload identities

00:23:20,400 --> 00:23:27,520
or machine identities and

00:23:24,960 --> 00:23:29,280
and then use that to connect to the

00:23:27,520 --> 00:23:32,480
target cluster

00:23:29,280 --> 00:23:34,320
if you're in control of the master nodes

00:23:32,480 --> 00:23:35,679
and you can change the api server flags

00:23:34,320 --> 00:23:37,200
that's great because then you can use

00:23:35,679 --> 00:23:40,559
web hook token authentication

00:23:37,200 --> 00:23:43,919
authenticating proxy if

00:23:40,559 --> 00:23:48,080
uh you need a kind of a a solution

00:23:43,919 --> 00:23:51,600
that uh a one size fits all

00:23:48,080 --> 00:23:54,720
is uh you can use an impersonating proxy

00:23:51,600 --> 00:23:58,960
that uses kubernetes impersonation

00:23:54,720 --> 00:24:02,000
to uh authenticate and

00:23:58,960 --> 00:24:04,640
uh you can check out the kubo ibc proxy

00:24:02,000 --> 00:24:06,720
um project as a prime example and

00:24:04,640 --> 00:24:09,919
admiralty cloud uses the same

00:24:06,720 --> 00:24:09,919
print the same concept

00:24:13,440 --> 00:24:19,840
admiralty has two schedulers

00:24:16,880 --> 00:24:20,880
the proxy scheduler and the candidate

00:24:19,840 --> 00:24:23,120
scheduler

00:24:20,880 --> 00:24:24,720
proxy scheduler handles proxy paws on

00:24:23,120 --> 00:24:28,480
the source cluster side

00:24:24,720 --> 00:24:30,840
and the candidate scheduler handles the

00:24:28,480 --> 00:24:32,559
candidate pods created by the proxy

00:24:30,840 --> 00:24:34,720
scheduler

00:24:32,559 --> 00:24:37,760
and they're both built upon the

00:24:34,720 --> 00:24:37,760
scheduler framework

00:24:39,279 --> 00:24:42,960
the scheduled framework is is great it's

00:24:41,919 --> 00:24:46,480
a set of

00:24:42,960 --> 00:24:48,080
go language interfaces

00:24:46,480 --> 00:24:49,679
that allows you to build your own

00:24:48,080 --> 00:24:52,000
scheduler

00:24:49,679 --> 00:24:52,880
while retaining all the features of the

00:24:52,000 --> 00:24:56,000
standard

00:24:52,880 --> 00:24:58,960
kubernetes scheduler and adding yours

00:24:56,000 --> 00:25:01,520
at various extension points in the

00:24:58,960 --> 00:25:04,799
scheduling cycle and the binding cycle

00:25:01,520 --> 00:25:08,159
and so uh the dots here

00:25:04,799 --> 00:25:11,279
mark where uh our like multi

00:25:08,159 --> 00:25:16,080
two schedulers extend

00:25:11,279 --> 00:25:19,120
the the kubernetes scheduler

00:25:16,080 --> 00:25:22,000
so you can for example add some

00:25:19,120 --> 00:25:22,640
filters like how to filter nodes or you

00:25:22,000 --> 00:25:24,799
can wait

00:25:22,640 --> 00:25:27,520
before actually binding to a node and

00:25:24,799 --> 00:25:30,000
this is useful

00:25:27,520 --> 00:25:31,360
in admiralty's multi-cluster scaling

00:25:30,000 --> 00:25:35,039
algorithm because the

00:25:31,360 --> 00:25:38,880
two clusters talk to each other um

00:25:35,039 --> 00:25:43,600
using annotations on uh pod chaperones

00:25:38,880 --> 00:25:46,880
let's uh let's see uh how this works

00:25:43,600 --> 00:25:48,720
so in this uh sequence diagram i have

00:25:46,880 --> 00:25:51,039
the source cluster on one side

00:25:48,720 --> 00:25:54,000
and very started clusters on the other

00:25:51,039 --> 00:25:57,520
side that have the same components

00:25:54,000 --> 00:25:58,559
with different timelines when a source

00:25:57,520 --> 00:26:02,080
spot is created

00:25:58,559 --> 00:26:04,400
and annotated to use admiralty as a

00:26:02,080 --> 00:26:07,440
multi-cluster scheduler

00:26:04,400 --> 00:26:08,720
it is mutated we need to make a few

00:26:07,440 --> 00:26:11,840
changes to the pod

00:26:08,720 --> 00:26:14,880
like a scalar name change the

00:26:11,840 --> 00:26:16,799
scheduling constraints so that the pod

00:26:14,880 --> 00:26:18,400
can tolerate the virtual node

00:26:16,799 --> 00:26:21,600
we save the original scheduling

00:26:18,400 --> 00:26:23,440
constraints for later

00:26:21,600 --> 00:26:24,720
we add a finalizer for cross cluster

00:26:23,440 --> 00:26:26,320
gabbage collection

00:26:24,720 --> 00:26:28,159
different things check the documentation

00:26:26,320 --> 00:26:32,240
for details

00:26:28,159 --> 00:26:35,200
while the proxy part is being scheduled

00:26:32,240 --> 00:26:35,679
and the virtual nodes that it tolerates

00:26:35,200 --> 00:26:38,880
are

00:26:35,679 --> 00:26:40,720
filtered pod chaperones

00:26:38,880 --> 00:26:42,960
are created in the target clusters that

00:26:40,720 --> 00:26:47,120
correspond to those ritual nodes

00:26:42,960 --> 00:26:51,039
a part chaperone has two purposes

00:26:47,120 --> 00:26:54,000
it is the vehicle of annotations um

00:26:51,039 --> 00:26:55,279
both schedulers annotate the chaperone

00:26:54,000 --> 00:26:58,720
to communicate

00:26:55,279 --> 00:27:00,720
if they annotated one of the other pod

00:26:58,720 --> 00:27:02,320
instead that would invalidate the cache

00:27:00,720 --> 00:27:04,880
of the scheduler so

00:27:02,320 --> 00:27:06,559
it's best to use uh another component

00:27:04,880 --> 00:27:10,000
another object for that

00:27:06,559 --> 00:27:11,919
but also maybe mostly

00:27:10,000 --> 00:27:14,240
most important from a feature

00:27:11,919 --> 00:27:17,120
perspective

00:27:14,240 --> 00:27:18,720
if the source to target cluster

00:27:17,120 --> 00:27:21,360
connection is interrupted

00:27:18,720 --> 00:27:22,960
and the a pod has been running for a

00:27:21,360 --> 00:27:24,799
while a candidate part has been

00:27:22,960 --> 00:27:26,559
sorry a delegate pod has been running

00:27:24,799 --> 00:27:29,200
for a while but

00:27:26,559 --> 00:27:31,039
is evicted because a node that runs on a

00:27:29,200 --> 00:27:34,240
real node that it runs on is

00:27:31,039 --> 00:27:37,760
um being coordinated for example

00:27:34,240 --> 00:27:39,440
or stop responding we need a way to

00:27:37,760 --> 00:27:42,080
recreate the pod

00:27:39,440 --> 00:27:43,600
and uh so so we need a local controller

00:27:42,080 --> 00:27:45,279
and that's the pod chaperone

00:27:43,600 --> 00:27:46,720
part chaperone is just a pot template

00:27:45,279 --> 00:27:49,760
that creates the pawn

00:27:46,720 --> 00:27:51,919
all right enough about the chaperone so

00:27:49,760 --> 00:27:54,880
each part chaperone creates a pot that

00:27:51,919 --> 00:27:58,240
looks exactly like the pod chaperone

00:27:54,880 --> 00:28:00,960
and uh it includes

00:27:58,240 --> 00:28:02,640
the scheduling constraints that were

00:28:00,960 --> 00:28:06,399
saved

00:28:02,640 --> 00:28:09,919
in the in annotations on the proxy pod

00:28:06,399 --> 00:28:10,640
so that we can ensure that the intention

00:28:09,919 --> 00:28:14,240
of the user

00:28:10,640 --> 00:28:15,039
to schedule on a node that had gpu for

00:28:14,240 --> 00:28:18,399
example

00:28:15,039 --> 00:28:21,840
is met at the

00:28:18,399 --> 00:28:24,320
real node level in the target clusters

00:28:21,840 --> 00:28:26,720
and so for example here only two of the

00:28:24,320 --> 00:28:30,000
target clusters out of three

00:28:26,720 --> 00:28:32,240
can find a node for the candidates

00:28:30,000 --> 00:28:34,559
their respective candidates so they

00:28:32,240 --> 00:28:36,799
annotate

00:28:34,559 --> 00:28:38,480
the uh the podca prom the proxy

00:28:36,799 --> 00:28:41,200
scheduler sees that

00:28:38,480 --> 00:28:42,720
selects one of the two using some

00:28:41,200 --> 00:28:45,919
topology spread constraint

00:28:42,720 --> 00:28:48,799
all other other proxy

00:28:45,919 --> 00:28:50,880
pod level scaling constraints that you

00:28:48,799 --> 00:28:54,799
can add

00:28:50,880 --> 00:28:56,799
and when one is when the highest scoring

00:28:54,799 --> 00:28:58,480
node is selected the point shot

00:28:56,799 --> 00:29:00,000
chaperone in the corresponding target

00:28:58,480 --> 00:29:03,279
cluster

00:29:00,000 --> 00:29:04,960
is annotated again this time to signal

00:29:03,279 --> 00:29:06,159
the candidate scheduler that it is

00:29:04,960 --> 00:29:09,840
allowed

00:29:06,159 --> 00:29:13,840
to bind the candidate pod

00:29:09,840 --> 00:29:13,840
that becomes a delegate pod

00:29:13,919 --> 00:29:19,840
the proxy scheduler sees that

00:29:16,960 --> 00:29:20,240
binds to finally binds the proxy pod and

00:29:19,840 --> 00:29:23,760
the

00:29:20,240 --> 00:29:25,440
other candidate pods are deleted

00:29:23,760 --> 00:29:27,840
via the pot chaperones and garbage

00:29:25,440 --> 00:29:27,840
collection

00:29:28,640 --> 00:29:33,520
in summary it is possible to build a

00:29:31,600 --> 00:29:36,559
global kubernetes cluster

00:29:33,520 --> 00:29:39,919
you just need super fast networks and

00:29:36,559 --> 00:29:40,799
some custom built notes but even then

00:29:39,919 --> 00:29:42,320
sritri

00:29:40,799 --> 00:29:44,399
colleagues and partners found reasons to

00:29:42,320 --> 00:29:47,279
use multiple clusters

00:29:44,399 --> 00:29:48,080
the federation around oculus has over 10

00:29:47,279 --> 00:29:50,240
000 cores

00:29:48,080 --> 00:29:52,480
currently and will soon be expanded by

00:29:50,240 --> 00:29:55,360
an order of magnitude with the addition

00:29:52,480 --> 00:29:56,880
of the expanse of supercomputer

00:29:55,360 --> 00:29:59,200
federation uses

00:29:56,880 --> 00:30:00,000
admiralty which itself uses virtual

00:29:59,200 --> 00:30:03,279
cubelet

00:30:00,000 --> 00:30:04,320
and the scheduler framework

00:30:03,279 --> 00:30:06,240
if you're interested if you're

00:30:04,320 --> 00:30:08,480
interested in joining the research

00:30:06,240 --> 00:30:10,320
platform around nautilus

00:30:08,480 --> 00:30:11,919
contact them and if you want to build

00:30:10,320 --> 00:30:16,480
your own federation

00:30:11,919 --> 00:30:16,480

YouTube URL: https://www.youtube.com/watch?v=8jf7AHCxBmA


