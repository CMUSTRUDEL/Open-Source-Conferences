Title: High Performance KubeVirt in Action - Huamin Chen, Red Hat & Marcin Franczyk, Kubermatic
Publication date: 2020-11-23
Playlist: KubeCon + CloudNativeCon North America 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon Europe 2021 Virtual from May 4–7, 2021. Learn more at https://kubecon.io. The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects. 

High Performance KubeVirt in Action - Huamin Chen, Red Hat & Marcin Franczyk, Kubermatic 

This talk details a real world solution design of a high performance KubeVirt for running mission critical enterprise workload. KubeVirt brings Cloud Native Virtual Machine management to Kubernetes. It unifies workload orchestration across Containers, Virtual Machine, as well as Serverless. Solution designs around KubeVirt, however, are sparse as of date, especially in networking and storage. On the other hand, workloads running on Virtual Machines often demand high performance and isolation enhancement. In this talk, Red Hat and Kubermatic jointly share with the community their customer engagement experiences of building a high performance KubeVirt environment by integrating with Gardener and multiple CNCF projects, including Kubernetes, KubeVirt, Rook, and Kubernetes Network Plumbing Working Group. 

https://sched.co/ekDs
Captions: 
	00:00:01,439 --> 00:00:07,120
greetings it is our pleasure

00:00:03,919 --> 00:00:10,480
to meet each other at this cubecon 2000

00:00:07,120 --> 00:00:16,640
virtual event my name is juan chen

00:00:10,480 --> 00:00:18,960
i work as a office of cto as a redhead

00:00:16,640 --> 00:00:20,720
and my name is martin francik where i

00:00:18,960 --> 00:00:24,480
work as a software engineer

00:00:20,720 --> 00:00:27,599
at kubernetes this talk

00:00:24,480 --> 00:00:30,000
is organized in this way first we go

00:00:27,599 --> 00:00:33,040
through the cube verse project

00:00:30,000 --> 00:00:35,280
kuvroz is the cncf project

00:00:33,040 --> 00:00:36,239
that manages the virtual machines the

00:00:35,280 --> 00:00:40,079
same way as

00:00:36,239 --> 00:00:43,600
kubernetes thus for containers

00:00:40,079 --> 00:00:46,320
then we introduce gardener projects

00:00:43,600 --> 00:00:47,200
gardener is the open source project

00:00:46,320 --> 00:00:50,160
that's managed

00:00:47,200 --> 00:00:51,039
kubernetes clusters life cycle and our

00:00:50,160 --> 00:00:54,559
offers

00:00:51,039 --> 00:00:57,760
they run an a2 operation it runs

00:00:54,559 --> 00:01:00,719
on a number of infrastructure platforms

00:00:57,760 --> 00:01:01,359
including pure words we explain how

00:01:00,719 --> 00:01:04,640
kubrick's

00:01:01,359 --> 00:01:07,040
features can help gardeners to deploy

00:01:04,640 --> 00:01:09,040
high performance and highly secured

00:01:07,040 --> 00:01:11,520
kubernetes clusters

00:01:09,040 --> 00:01:12,080
we focus mostly on the networking and

00:01:11,520 --> 00:01:15,280
storage

00:01:12,080 --> 00:01:18,560
features that we how to use

00:01:15,280 --> 00:01:21,600
motors cni for high performance and

00:01:18,560 --> 00:01:24,720
fully isolated networking configuration

00:01:21,600 --> 00:01:27,840
and how to use data volume and clone

00:01:24,720 --> 00:01:32,000
to accelerate virtual machine deployment

00:01:27,840 --> 00:01:32,720
at scale ship first joined the cncf

00:01:32,000 --> 00:01:35,840
sandbox

00:01:32,720 --> 00:01:38,799
over a year ago in 2019

00:01:35,840 --> 00:01:40,400
it enables workloads that run instead of

00:01:38,799 --> 00:01:42,960
vms to be deployed

00:01:40,400 --> 00:01:45,360
on the same kubernetes cluster as

00:01:42,960 --> 00:01:48,560
containers are running

00:01:45,360 --> 00:01:52,799
it's used some of the kubernetes native

00:01:48,560 --> 00:01:56,079
objects such as positions volume claims

00:01:52,799 --> 00:01:58,399
and resources claims it provides a

00:01:56,079 --> 00:02:00,640
convenient way to describe

00:01:58,399 --> 00:02:01,840
virtual machine configurations and their

00:02:00,640 --> 00:02:04,240
state

00:02:01,840 --> 00:02:05,840
in this example a virtual machine that

00:02:04,240 --> 00:02:08,239
consists of virtual

00:02:05,840 --> 00:02:09,119
natural hemispheres and the virtual

00:02:08,239 --> 00:02:12,480
disks

00:02:09,119 --> 00:02:14,000
are described as devices api inside of

00:02:12,480 --> 00:02:16,959
the virtual machine

00:02:14,000 --> 00:02:17,840
specifically the interfaces api

00:02:16,959 --> 00:02:20,319
describes

00:02:17,840 --> 00:02:22,239
how the networks are configured which

00:02:20,319 --> 00:02:24,800
network they are attached to whether

00:02:22,239 --> 00:02:26,239
that is the kubernetes part network or

00:02:24,800 --> 00:02:28,640
additional networks

00:02:26,239 --> 00:02:30,480
thus are defined the network attachment

00:02:28,640 --> 00:02:33,200
definition

00:02:30,480 --> 00:02:33,680
the virtual disks are described in a

00:02:33,200 --> 00:02:37,360
diff

00:02:33,680 --> 00:02:40,640
api the information includes such as

00:02:37,360 --> 00:02:41,360
the size of the disk whether the disk is

00:02:40,640 --> 00:02:44,720
the

00:02:41,360 --> 00:02:48,000
data disk or cloudiness disk

00:02:44,720 --> 00:02:48,720
and specifically the disk can be refers

00:02:48,000 --> 00:02:51,760
to

00:02:48,720 --> 00:02:57,040
our existing virtual

00:02:51,760 --> 00:02:57,040
persistent uh volume claims

00:02:58,000 --> 00:03:01,519
in addition to the virtual machine apis

00:03:00,720 --> 00:03:05,599
qrs

00:03:01,519 --> 00:03:08,080
also provide apis to manage the

00:03:05,599 --> 00:03:08,800
virtual machine templates but data

00:03:08,080 --> 00:03:12,239
volumes

00:03:08,800 --> 00:03:12,959
and virtual machine states queue first

00:03:12,239 --> 00:03:15,760
can be

00:03:12,959 --> 00:03:16,640
conveniently installed on openshift and

00:03:15,760 --> 00:03:19,599
kubernetes

00:03:16,640 --> 00:03:21,519
as operator that is available as

00:03:19,599 --> 00:03:23,440
operator hub

00:03:21,519 --> 00:03:25,120
gardner is an open source project that

00:03:23,440 --> 00:03:26,720
delivers fully managed kubernetes

00:03:25,120 --> 00:03:28,480
clusters at scale

00:03:26,720 --> 00:03:31,360
in simple words image that you can

00:03:28,480 --> 00:03:33,840
create new kubernetes instances as spots

00:03:31,360 --> 00:03:36,480
we can call it kubernetes as a service

00:03:33,840 --> 00:03:39,200
let's take a closer look at architecture

00:03:36,480 --> 00:03:41,040
gardener runs on kubernetes we can split

00:03:39,200 --> 00:03:42,080
the diagram you see into three separate

00:03:41,040 --> 00:03:44,879
clusters

00:03:42,080 --> 00:03:46,159
the first square on the very left side

00:03:44,879 --> 00:03:48,159
is the garden cluster

00:03:46,159 --> 00:03:49,840
where all core components run like

00:03:48,159 --> 00:03:52,720
gardener api server

00:03:49,840 --> 00:03:53,840
gardener controller manager and gardener

00:03:52,720 --> 00:03:55,680
scheduler

00:03:53,840 --> 00:03:58,319
the second one in the middle the seed

00:03:55,680 --> 00:04:00,239
cluster as you remember i said gardener

00:03:58,319 --> 00:04:02,480
is capable of creating kubernetes

00:04:00,239 --> 00:04:04,560
clusters as spots thus the role of the

00:04:02,480 --> 00:04:06,879
seed is to run control planes of freshly

00:04:04,560 --> 00:04:08,799
created kubernetes instances

00:04:06,879 --> 00:04:10,239
each seat must have a running component

00:04:08,799 --> 00:04:12,159
called garden led

00:04:10,239 --> 00:04:14,159
as you can observe there is an analogy

00:04:12,159 --> 00:04:16,400
to kubernetes core components

00:04:14,159 --> 00:04:17,359
garden led to cubelet gardener api

00:04:16,400 --> 00:04:19,040
server

00:04:17,359 --> 00:04:20,560
controller manager and scheduler to

00:04:19,040 --> 00:04:23,840
kubernetes ones

00:04:20,560 --> 00:04:26,320
last but not least the shoot cluster

00:04:23,840 --> 00:04:26,880
on the right side where all worker nodes

00:04:26,320 --> 00:04:28,960
run

00:04:26,880 --> 00:04:30,880
the cluster itself must be created on

00:04:28,960 --> 00:04:32,000
one of the following cloud providers

00:04:30,880 --> 00:04:34,560
listed below

00:04:32,000 --> 00:04:36,320
each provider has a gardener extension

00:04:34,560 --> 00:04:37,199
that knows how to start a new worker

00:04:36,320 --> 00:04:39,680
node

00:04:37,199 --> 00:04:41,199
to summarize this a newly created

00:04:39,680 --> 00:04:43,440
kubernetes instance

00:04:41,199 --> 00:04:44,880
is stretched between seat where control

00:04:43,440 --> 00:04:46,800
plane is present

00:04:44,880 --> 00:04:49,120
and shoot where worker nodes are

00:04:46,800 --> 00:04:51,600
available communication between them

00:04:49,120 --> 00:04:54,400
takes place via load balancer services

00:04:51,600 --> 00:04:56,639
and vpn connection recently we added the

00:04:54,400 --> 00:04:58,720
keyword extension that allows customers

00:04:56,639 --> 00:05:01,520
to use any bare metal environment

00:04:58,720 --> 00:05:03,360
that supports linux kvm and allows to

00:05:01,520 --> 00:05:04,400
start many kubernetes clusters on

00:05:03,360 --> 00:05:06,400
premise

00:05:04,400 --> 00:05:08,960
i must admit that i find this use case

00:05:06,400 --> 00:05:10,880
very interesting as we run cube vert vms

00:05:08,960 --> 00:05:12,400
as worker nodes it brings some

00:05:10,880 --> 00:05:15,840
challenges and part of them we will

00:05:12,400 --> 00:05:15,840
describe in the following slides

00:05:16,000 --> 00:05:22,080
mouse's cli is a meta cri

00:05:19,440 --> 00:05:23,199
that allows kubernetes parts or cube

00:05:22,080 --> 00:05:25,840
versus vms

00:05:23,199 --> 00:05:28,560
to attach to a different network other

00:05:25,840 --> 00:05:30,800
than the default spot network

00:05:28,560 --> 00:05:32,880
in order to use modules you have to

00:05:30,800 --> 00:05:36,000
install the necessary dimmer set and

00:05:32,880 --> 00:05:38,639
crds in openshift

00:05:36,000 --> 00:05:40,240
these are all pre-installed for you so

00:05:38,639 --> 00:05:43,280
in this example

00:05:40,240 --> 00:05:47,199
we create a linux bridge based

00:05:43,280 --> 00:05:48,479
um ipvlan enabled network attachments

00:05:47,199 --> 00:05:50,960
definition

00:05:48,479 --> 00:05:52,400
so there are multiple types of autos

00:05:50,960 --> 00:05:55,199
water sunrise

00:05:52,400 --> 00:05:56,319
so in this example we use bridge in that

00:05:55,199 --> 00:05:59,360
example you can see

00:05:56,319 --> 00:06:02,479
macvillan and an ipv

00:05:59,360 --> 00:06:03,919
so the bridge has to exist first if you

00:06:02,479 --> 00:06:06,560
do not have it you have to

00:06:03,919 --> 00:06:07,440
do using some other automation

00:06:06,560 --> 00:06:09,600
mechanisms

00:06:07,440 --> 00:06:10,639
to create the bridge on the nodes as you

00:06:09,600 --> 00:06:14,000
want the

00:06:10,639 --> 00:06:17,680
attachments happen the linux bridge

00:06:14,000 --> 00:06:19,680
uses ipvlan tag1234

00:06:17,680 --> 00:06:21,520
on environment especially when you are

00:06:19,680 --> 00:06:25,039
running on public clouds

00:06:21,520 --> 00:06:28,880
you may not have access to native vlan

00:06:25,039 --> 00:06:31,120
so you can create tunneling mechanisms

00:06:28,880 --> 00:06:31,919
and on top of the tunneling device like

00:06:31,120 --> 00:06:35,039
for example

00:06:31,919 --> 00:06:38,000
vxlan you create the bridge and then

00:06:35,039 --> 00:06:42,400
using the vlans on top of the tunneling

00:06:38,000 --> 00:06:45,840
device the ip address management

00:06:42,400 --> 00:06:49,360
used in this example is a well about

00:06:45,840 --> 00:06:52,080
which allows you to manage ip addresses

00:06:49,360 --> 00:06:53,599
across the nodes globally there are

00:06:52,080 --> 00:06:56,560
other types of

00:06:53,599 --> 00:06:58,319
ipl address management such as static

00:06:56,560 --> 00:07:01,599
which only manage the specific

00:06:58,319 --> 00:07:04,960
ipl addresses along cost local

00:07:01,599 --> 00:07:08,880
which is limited to per host

00:07:04,960 --> 00:07:08,880
that vms or parts are running

00:07:09,840 --> 00:07:13,840
converse use motors for a number of use

00:07:12,720 --> 00:07:17,120
cases

00:07:13,840 --> 00:07:18,479
so in this use case we want to separate

00:07:17,120 --> 00:07:21,919
the networks

00:07:18,479 --> 00:07:24,880
so the virtual machines can use the

00:07:21,919 --> 00:07:27,680
defaults a part network to access

00:07:24,880 --> 00:07:30,880
kubernetes native or services such as

00:07:27,680 --> 00:07:34,560
api server and using another

00:07:30,880 --> 00:07:37,440
other network for data management

00:07:34,560 --> 00:07:38,319
so by separating the two networks the

00:07:37,440 --> 00:07:40,720
vms

00:07:38,319 --> 00:07:41,840
can ensure that the traffic from each of

00:07:40,720 --> 00:07:44,800
the network will not

00:07:41,840 --> 00:07:46,080
interfere with each other for such

00:07:44,800 --> 00:07:49,520
configuration

00:07:46,080 --> 00:07:50,800
we first defined a network attachment

00:07:49,520 --> 00:07:54,240
definition

00:07:50,800 --> 00:07:55,599
that describes how the network interface

00:07:54,240 --> 00:07:59,280
is constructed

00:07:55,599 --> 00:08:02,319
and how ip management is done

00:07:59,280 --> 00:08:05,120
the network attachments definition is

00:08:02,319 --> 00:08:05,440
referenced in you know virtual machines

00:08:05,120 --> 00:08:08,479
or

00:08:05,440 --> 00:08:12,560
networks api on the right side

00:08:08,479 --> 00:08:15,759
of the yaml the network apis

00:08:12,560 --> 00:08:19,360
supplies a number of types the part

00:08:15,759 --> 00:08:22,960
type is a kubernetes port network

00:08:19,360 --> 00:08:26,160
the modus type is a reference to

00:08:22,960 --> 00:08:28,639
the network attachments definition uses

00:08:26,160 --> 00:08:32,640
the format as the namespace slash

00:08:28,639 --> 00:08:36,240
the name this also supports a number of

00:08:32,640 --> 00:08:36,959
attachment mechanisms such as bridge as

00:08:36,240 --> 00:08:40,479
well as the

00:08:36,959 --> 00:08:42,159
srlv once this yammer is

00:08:40,479 --> 00:08:44,959
applied the virtual machine general is

00:08:42,159 --> 00:08:47,920
applied in the background the first

00:08:44,959 --> 00:08:48,800
launcher part will create a necessary

00:08:47,920 --> 00:08:51,279
networking

00:08:48,800 --> 00:08:52,720
and accurate new environments to

00:08:51,279 --> 00:08:57,200
configure the vm

00:08:52,720 --> 00:09:00,320
so you should adjust runs as desired

00:08:57,200 --> 00:09:01,040
allowing the vm to access both the plus

00:09:00,320 --> 00:09:04,240
network

00:09:01,040 --> 00:09:07,200
and other networks is great for the vms

00:09:04,240 --> 00:09:08,000
to access to access all types of

00:09:07,200 --> 00:09:09,839
services

00:09:08,000 --> 00:09:11,040
provided by kubernetes and other

00:09:09,839 --> 00:09:13,760
networks

00:09:11,040 --> 00:09:16,640
but for security sensitive environments

00:09:13,760 --> 00:09:19,360
you do not want the vms to be exposed

00:09:16,640 --> 00:09:20,880
on the part network because if you can

00:09:19,360 --> 00:09:23,839
pee in or just

00:09:20,880 --> 00:09:25,680
attack of vms without proper

00:09:23,839 --> 00:09:28,399
authorization

00:09:25,680 --> 00:09:30,720
network policy is surely going to help

00:09:28,399 --> 00:09:33,839
but it's not efficient

00:09:30,720 --> 00:09:36,959
so we have another configuration for

00:09:33,839 --> 00:09:38,000
full isolation while the vms is

00:09:36,959 --> 00:09:41,200
completely

00:09:38,000 --> 00:09:43,040
disconnected from the pod network so

00:09:41,200 --> 00:09:44,320
if you look at the yellows and the

00:09:43,040 --> 00:09:46,880
configurations

00:09:44,320 --> 00:09:47,839
the only difference is that in the

00:09:46,880 --> 00:09:50,640
virtual machine

00:09:47,839 --> 00:09:52,720
networks api the path network is

00:09:50,640 --> 00:09:55,839
completely removed

00:09:52,720 --> 00:09:59,519
only the modus network stays

00:09:55,839 --> 00:10:03,040
in this configuration when the vm starts

00:09:59,519 --> 00:10:06,399
it loses access to the part network

00:10:03,040 --> 00:10:08,480
so the any network traffic from the

00:10:06,399 --> 00:10:12,480
cubenet spot network will not

00:10:08,480 --> 00:10:15,440
reach the vm this ensures that the vm

00:10:12,480 --> 00:10:16,240
is in full isolation using its own view

00:10:15,440 --> 00:10:19,279
length

00:10:16,240 --> 00:10:21,040
and this also saves the ip address from

00:10:19,279 --> 00:10:23,920
the part network that is also

00:10:21,040 --> 00:10:26,079
efficient we were going to see the both

00:10:23,920 --> 00:10:29,360
configurations used in different use

00:10:26,079 --> 00:10:32,320
cases in gardner

00:10:29,360 --> 00:10:33,279
with last information in mind now look

00:10:32,320 --> 00:10:36,160
at some of the

00:10:33,279 --> 00:10:37,279
converse networking configuration use

00:10:36,160 --> 00:10:39,440
cases

00:10:37,279 --> 00:10:40,959
the first is high-performance networking

00:10:39,440 --> 00:10:43,279
configuration

00:10:40,959 --> 00:10:44,320
as you know the virtual machines are

00:10:43,279 --> 00:10:47,519
often

00:10:44,320 --> 00:10:50,560
worked with cloud native storage

00:10:47,519 --> 00:10:51,839
that's a provisioned by rook rook is

00:10:50,560 --> 00:10:55,040
another cncf

00:10:51,839 --> 00:10:58,240
project has recently graduated

00:10:55,040 --> 00:10:59,519
it's a storage backend operator that's

00:10:58,240 --> 00:11:02,560
provisions

00:10:59,519 --> 00:11:06,000
saf hfs nfs

00:11:02,560 --> 00:11:07,040
as well as apache cassandra one of the

00:11:06,000 --> 00:11:10,079
recent features

00:11:07,040 --> 00:11:14,320
in rook is that it allows

00:11:10,079 --> 00:11:16,480
a safe cluster to use surface network

00:11:14,320 --> 00:11:17,839
defined by network attachments

00:11:16,480 --> 00:11:21,760
definition

00:11:17,839 --> 00:11:24,079
so that the front-end the public network

00:11:21,760 --> 00:11:24,800
that the south clients are interacting

00:11:24,079 --> 00:11:27,680
with

00:11:24,800 --> 00:11:28,000
has the best possible bandwidth and

00:11:27,680 --> 00:11:30,959
while

00:11:28,000 --> 00:11:33,360
the uh the cluster network where the

00:11:30,959 --> 00:11:35,360
osds are communicating with each other

00:11:33,360 --> 00:11:36,320
to share for the data to balance the

00:11:35,360 --> 00:11:38,000
data

00:11:36,320 --> 00:11:39,519
they are separated from the public

00:11:38,000 --> 00:11:42,800
network

00:11:39,519 --> 00:11:45,839
so in this use case the vms

00:11:42,800 --> 00:11:48,640
and the staff public network

00:11:45,839 --> 00:11:50,160
reference the same network attachment

00:11:48,640 --> 00:11:53,760
definition

00:11:50,160 --> 00:11:56,480
such as their traffic will be isolated

00:11:53,760 --> 00:11:57,519
in the same network in this

00:11:56,480 --> 00:12:00,880
configuration

00:11:57,519 --> 00:12:03,360
they can assure that the traffic is not

00:12:00,880 --> 00:12:04,079
interacting with other networks and the

00:12:03,360 --> 00:12:08,320
performance

00:12:04,079 --> 00:12:11,360
is guaranteed on a separate occasion

00:12:08,320 --> 00:12:15,839
what a full isolation configuration

00:12:11,360 --> 00:12:19,040
the vms are protected in their own vlans

00:12:15,839 --> 00:12:19,839
in this configuration we use linux

00:12:19,040 --> 00:12:23,360
bridge

00:12:19,839 --> 00:12:26,320
as the connectivity mechanisms

00:12:23,360 --> 00:12:28,320
in network attachments definition reach

00:12:26,320 --> 00:12:31,440
the vlan number

00:12:28,320 --> 00:12:34,560
embedded in the definition

00:12:31,440 --> 00:12:38,800
again for use for environments

00:12:34,560 --> 00:12:42,160
let's do not support native vlan text

00:12:38,800 --> 00:12:45,519
you can build up a tunneling

00:12:42,160 --> 00:12:48,720
ip service underneath using

00:12:45,519 --> 00:12:52,079
vxlens and creates a bridge device

00:12:48,720 --> 00:12:55,200
on top of the vxlan so the vxlan will

00:12:52,079 --> 00:12:56,959
emulate a switch trunk that will just

00:12:55,200 --> 00:13:00,480
tunnel all the vlan

00:12:56,959 --> 00:13:03,839
between different endpoints

00:13:00,480 --> 00:13:06,880
so this is uh one of the configurations

00:13:03,839 --> 00:13:09,519
you can reference for network

00:13:06,880 --> 00:13:09,519
isolation

00:13:09,920 --> 00:13:13,200
now let's put our learnings into

00:13:12,240 --> 00:13:15,600
practice

00:13:13,200 --> 00:13:16,399
we will use gardener and the kubernetes

00:13:15,600 --> 00:13:19,440
integration

00:13:16,399 --> 00:13:22,399
as a case study to show that

00:13:19,440 --> 00:13:23,920
we how we can use the two configurations

00:13:22,399 --> 00:13:26,720
of motorcycling line

00:13:23,920 --> 00:13:27,760
to provide the best performance as well

00:13:26,720 --> 00:13:31,120
as the best

00:13:27,760 --> 00:13:34,320
isolation so recorders garden

00:13:31,120 --> 00:13:37,120
project provides kubernetes as a service

00:13:34,320 --> 00:13:37,839
it's a provisions kubernetes clusters on

00:13:37,120 --> 00:13:41,120
a number of

00:13:37,839 --> 00:13:43,680
infrastructure platforms but it does so

00:13:41,120 --> 00:13:46,000
by abstracting the underlying resources

00:13:43,680 --> 00:13:46,800
including the cloud environment the

00:13:46,000 --> 00:13:49,839
infrastructure

00:13:46,800 --> 00:13:52,320
environment operating system network

00:13:49,839 --> 00:13:53,680
and worknose management and such and

00:13:52,320 --> 00:13:57,160
such

00:13:53,680 --> 00:13:59,760
this abstraction programmatically and

00:13:57,160 --> 00:14:01,040
administratively allows government to be

00:13:59,760 --> 00:14:04,160
extended to different

00:14:01,040 --> 00:14:07,680
platforms so the kubernetes

00:14:04,160 --> 00:14:08,079
cluster work now created by gardener is

00:14:07,680 --> 00:14:10,959
called

00:14:08,079 --> 00:14:12,480
shoot so we have two shoes clusters in

00:14:10,959 --> 00:14:14,959
this example

00:14:12,480 --> 00:14:16,800
the blue shoes cluster consists of

00:14:14,959 --> 00:14:19,680
infrastructure config

00:14:16,800 --> 00:14:20,320
that has two networks one is the shield

00:14:19,680 --> 00:14:23,760
storage

00:14:20,320 --> 00:14:27,519
washer network in this case the

00:14:23,760 --> 00:14:29,519
shell networks refers to the network

00:14:27,519 --> 00:14:32,800
attachments definition

00:14:29,519 --> 00:14:34,000
created by rook for steph's public

00:14:32,800 --> 00:14:37,360
network

00:14:34,000 --> 00:14:40,160
so the reference here will be the self

00:14:37,360 --> 00:14:40,639
reference use uh use the name space rook

00:14:40,160 --> 00:14:43,600
staff

00:14:40,639 --> 00:14:46,399
and name staff we are refers to our

00:14:43,600 --> 00:14:49,519
existing network attachment definition

00:14:46,399 --> 00:14:51,680
there are vms that will use this uh

00:14:49,519 --> 00:14:53,040
rough network attachment definition to

00:14:51,680 --> 00:14:56,639
attach to the same

00:14:53,040 --> 00:14:57,839
uh cluster staff the tenants network on

00:14:56,639 --> 00:15:00,959
the other hand

00:14:57,839 --> 00:15:01,920
provides a set of information that will

00:15:00,959 --> 00:15:03,839
be used

00:15:01,920 --> 00:15:06,240
to create a network attachment

00:15:03,839 --> 00:15:08,399
definition on the fly

00:15:06,240 --> 00:15:10,560
so this is its own privacy network

00:15:08,399 --> 00:15:13,360
attachment definition

00:15:10,560 --> 00:15:15,120
thus will must be used by other clusters

00:15:13,360 --> 00:15:18,320
to have another layer for

00:15:15,120 --> 00:15:19,600
isolation the nasa attach depth also

00:15:18,320 --> 00:15:22,399
uses a vlan

00:15:19,600 --> 00:15:23,600
one two three four one two three four to

00:15:22,399 --> 00:15:26,320
ensure that

00:15:23,600 --> 00:15:29,040
the wheel and the vms are running in

00:15:26,320 --> 00:15:32,079
isolated vlans that are not

00:15:29,040 --> 00:15:34,240
accessible by other vms on the different

00:15:32,079 --> 00:15:36,560
attachments

00:15:34,240 --> 00:15:39,120
the second excuse cluster the green

00:15:36,560 --> 00:15:42,240
cluster of the gracious cluster

00:15:39,120 --> 00:15:44,880
also refers to the same theft

00:15:42,240 --> 00:15:46,240
storage network for high performance

00:15:44,880 --> 00:15:50,320
purposes

00:15:46,240 --> 00:15:53,839
this creates its own nice tennis network

00:15:50,320 --> 00:15:56,000
by providing a different set of

00:15:53,839 --> 00:15:57,839
configuration for network attachments

00:15:56,000 --> 00:16:00,079
definition

00:15:57,839 --> 00:16:01,120
the vlan configuration in this case is

00:16:00,079 --> 00:16:04,399
different it's

00:16:01,120 --> 00:16:06,959
two three four five so by having two

00:16:04,399 --> 00:16:08,079
network attachment definition and two

00:16:06,959 --> 00:16:11,440
vlans

00:16:08,079 --> 00:16:14,240
the green network and blue networks

00:16:11,440 --> 00:16:15,519
are separated physically and

00:16:14,240 --> 00:16:19,120
symmetrically

00:16:15,519 --> 00:16:23,079
as a different kubernetes cluster

00:16:19,120 --> 00:16:26,399
so this just shows the case that the

00:16:23,079 --> 00:16:29,360
multi-modal configuration used by

00:16:26,399 --> 00:16:31,680
q verse can provide a flexibility for

00:16:29,360 --> 00:16:34,000
different use cases

00:16:31,680 --> 00:16:34,959
before i start talking about data volume

00:16:34,000 --> 00:16:36,880
and clone

00:16:34,959 --> 00:16:39,920
i must say a few words about the project

00:16:36,880 --> 00:16:42,160
behind the containerized data importer

00:16:39,920 --> 00:16:43,360
it's among one of my favorite add-ons in

00:16:42,160 --> 00:16:45,040
kubernetes

00:16:43,360 --> 00:16:46,399
the primary goal is to provide a

00:16:45,040 --> 00:16:49,440
declarative way

00:16:46,399 --> 00:16:51,600
to build virtual machine disks however

00:16:49,440 --> 00:16:53,120
you can use it as well to initialize

00:16:51,600 --> 00:16:54,959
kubernetes volumes

00:16:53,120 --> 00:16:56,320
with some data out of the keyword

00:16:54,959 --> 00:16:58,240
context

00:16:56,320 --> 00:17:00,240
cdi includes a custom resource

00:16:58,240 --> 00:17:01,519
definition that provides a data volume

00:17:00,240 --> 00:17:03,519
object type

00:17:01,519 --> 00:17:05,439
those objects are an abstraction on top

00:17:03,519 --> 00:17:07,679
of persistent volume claims

00:17:05,439 --> 00:17:10,319
and are very helpful in terms of data

00:17:07,679 --> 00:17:12,400
imports and uploads onto pvc

00:17:10,319 --> 00:17:14,480
this is a way to automate virtual

00:17:12,400 --> 00:17:16,880
machine disks management

00:17:14,480 --> 00:17:19,039
without that you would have to prepare a

00:17:16,880 --> 00:17:20,240
pvc that contains the disk image

00:17:19,039 --> 00:17:22,240
yourself

00:17:20,240 --> 00:17:23,919
one of the good outcomes of cdi

00:17:22,240 --> 00:17:26,000
integration with cubeverd

00:17:23,919 --> 00:17:28,240
is that you can specify a concrete data

00:17:26,000 --> 00:17:29,760
volume which is tied to the virtual

00:17:28,240 --> 00:17:32,559
machine lifecycle

00:17:29,760 --> 00:17:34,080
you can do it over data volume template

00:17:32,559 --> 00:17:36,720
in the vm spec

00:17:34,080 --> 00:17:38,400
when you delete the virtual machine the

00:17:36,720 --> 00:17:40,799
data volume would be destroyed too

00:17:38,400 --> 00:17:43,120
together with the provision storage

00:17:40,799 --> 00:17:45,360
so there is no requirement for the user

00:17:43,120 --> 00:17:47,600
to take care of the cleanup

00:17:45,360 --> 00:17:49,919
on the right side of the slide we can

00:17:47,600 --> 00:17:51,919
see a sample data volume defined

00:17:49,919 --> 00:17:54,160
there we have two interesting parts

00:17:51,919 --> 00:17:55,840
source and pvc

00:17:54,160 --> 00:17:57,520
the source determines if data is

00:17:55,840 --> 00:17:59,360
supposed to be cloned from another

00:17:57,520 --> 00:18:02,160
persistent volume claim

00:17:59,360 --> 00:18:04,559
or download it from some address in that

00:18:02,160 --> 00:18:05,360
case in the source we would have http

00:18:04,559 --> 00:18:08,720
options

00:18:05,360 --> 00:18:09,280
with url value below there is a pvc

00:18:08,720 --> 00:18:11,600
section

00:18:09,280 --> 00:18:12,559
as the name indicates it's about pvc

00:18:11,600 --> 00:18:14,240
settings

00:18:12,559 --> 00:18:16,400
like what kind of storage class you want

00:18:14,240 --> 00:18:17,120
to use and how big storage you want to

00:18:16,400 --> 00:18:19,840
have

00:18:17,120 --> 00:18:21,280
so having all these details in place

00:18:19,840 --> 00:18:24,000
data volume would create

00:18:21,280 --> 00:18:24,799
a new persistent volume clone for our

00:18:24,000 --> 00:18:27,600
disk

00:18:24,799 --> 00:18:28,160
the add-on makes our lives much easier i

00:18:27,600 --> 00:18:30,320
encourage

00:18:28,160 --> 00:18:31,520
everybody to check it themselves if you

00:18:30,320 --> 00:18:33,280
haven't yet

00:18:31,520 --> 00:18:35,760
i'm going to show the possible user

00:18:33,280 --> 00:18:38,559
strategies in the next slide

00:18:35,760 --> 00:18:40,640
virtual machines images management here

00:18:38,559 --> 00:18:42,480
we can see two diagrams that show two

00:18:40,640 --> 00:18:43,360
different strategies of data volume

00:18:42,480 --> 00:18:46,240
usage

00:18:43,360 --> 00:18:48,000
ad-hoc and pre-allocated data volumes

00:18:46,240 --> 00:18:50,480
for now let's focus on the first one the

00:18:48,000 --> 00:18:52,559
ad-hoc approach on the left side

00:18:50,480 --> 00:18:54,400
as i mentioned in the previous slide we

00:18:52,559 --> 00:18:56,640
can download data directly from the

00:18:54,400 --> 00:18:59,360
internet by changing source

00:18:56,640 --> 00:19:01,440
from persistent volume claim to http

00:18:59,360 --> 00:19:04,160
with an appropriate url

00:19:01,440 --> 00:19:06,559
so now considering the scenario where we

00:19:04,160 --> 00:19:08,559
type data volume to virtual machine

00:19:06,559 --> 00:19:10,160
by using data volume template in the

00:19:08,559 --> 00:19:11,760
spec means

00:19:10,160 --> 00:19:14,720
each time we create a new virtual

00:19:11,760 --> 00:19:16,559
machine each time we download the disk

00:19:14,720 --> 00:19:19,600
this can cause many problems if you want

00:19:16,559 --> 00:19:22,000
to schedule hundreds or thousands of vms

00:19:19,600 --> 00:19:23,200
first you can have performance issues

00:19:22,000 --> 00:19:25,600
you might have

00:19:23,200 --> 00:19:26,960
quite slow network connection or some

00:19:25,600 --> 00:19:29,600
red limitation

00:19:26,960 --> 00:19:31,679
even heavy network workload let's say

00:19:29,600 --> 00:19:33,039
you must download four gigabyte disks

00:19:31,679 --> 00:19:36,080
for each vm

00:19:33,039 --> 00:19:37,760
and you run many of those second

00:19:36,080 --> 00:19:40,640
the endpoint you are using might

00:19:37,760 --> 00:19:42,400
disappear causing reliability issues

00:19:40,640 --> 00:19:44,480
somebody could say that instead of

00:19:42,400 --> 00:19:46,640
downloading stuff from the internet

00:19:44,480 --> 00:19:48,320
you could create let's say a local http

00:19:46,640 --> 00:19:50,559
server i don't know

00:19:48,320 --> 00:19:51,440
maybe using nginx and serve images from

00:19:50,559 --> 00:19:54,320
there

00:19:51,440 --> 00:19:55,039
yes you could but that doesn't solve all

00:19:54,320 --> 00:19:57,840
problems

00:19:55,039 --> 00:19:58,480
i mentioned in 100 percent and you still

00:19:57,840 --> 00:20:02,559
can cause

00:19:58,480 --> 00:20:04,320
network latency on your cluster now

00:20:02,559 --> 00:20:06,400
let's focus on the pre-located data

00:20:04,320 --> 00:20:07,919
volume as it helps to get rid of

00:20:06,400 --> 00:20:09,919
problems i spoke

00:20:07,919 --> 00:20:11,600
the diagram on the right slide the

00:20:09,919 --> 00:20:14,080
preallocated data volume

00:20:11,600 --> 00:20:14,799
that is visible in the middle is a

00:20:14,080 --> 00:20:17,039
source for

00:20:14,799 --> 00:20:18,480
other data volumes you download the disk

00:20:17,039 --> 00:20:20,400
image only once

00:20:18,480 --> 00:20:21,919
and you use the volume as a source for

00:20:20,400 --> 00:20:24,640
other virtual machines

00:20:21,919 --> 00:20:26,320
you want to bring to life this strategy

00:20:24,640 --> 00:20:29,039
we follow in the gardener cube

00:20:26,320 --> 00:20:31,600
extension implementation we get rid of

00:20:29,039 --> 00:20:34,320
problems i described previously

00:20:31,600 --> 00:20:36,240
we reduce network latency data volume

00:20:34,320 --> 00:20:37,919
supports two types of cloning

00:20:36,240 --> 00:20:39,360
and i'm going to describe this in the

00:20:37,919 --> 00:20:42,240
next slide

00:20:39,360 --> 00:20:43,280
we have two ways of data volume cloning

00:20:42,240 --> 00:20:46,000
host assisted

00:20:43,280 --> 00:20:46,720
and smart cloning the first split on the

00:20:46,000 --> 00:20:48,400
left side

00:20:46,720 --> 00:20:50,799
shows a diagram of the traditional

00:20:48,400 --> 00:20:54,080
cloning of persistent volume claim

00:20:50,799 --> 00:20:56,159
that is host assisted it means there is

00:20:54,080 --> 00:20:58,960
a data stream from source pvc

00:20:56,159 --> 00:20:59,840
to target pvc the entire volume is being

00:20:58,960 --> 00:21:02,400
copied

00:20:59,840 --> 00:21:04,320
it is a traditional heavyweight approach

00:21:02,400 --> 00:21:05,440
it is not necessarily needed to waste

00:21:04,320 --> 00:21:08,080
storage space

00:21:05,440 --> 00:21:10,159
and put some additional load on disks

00:21:08,080 --> 00:21:12,080
just to copy the whole image

00:21:10,159 --> 00:21:14,159
on the right side i show another

00:21:12,080 --> 00:21:16,640
strategy of volume cloning

00:21:14,159 --> 00:21:19,200
it is a feature called smart clone this

00:21:16,640 --> 00:21:21,039
is being executed when it's possible

00:21:19,200 --> 00:21:22,559
in order to improve the performance of

00:21:21,039 --> 00:21:24,960
the cloning process

00:21:22,559 --> 00:21:26,720
the containerized data importer team

00:21:24,960 --> 00:21:29,360
introduced smart cloning

00:21:26,720 --> 00:21:31,440
that uses volume snapshot of course to

00:21:29,360 --> 00:21:32,320
use such a feature the csi plugin which

00:21:31,440 --> 00:21:35,280
you use

00:21:32,320 --> 00:21:37,600
must support snapshots the yam structure

00:21:35,280 --> 00:21:39,760
of the data volume is still the same

00:21:37,600 --> 00:21:41,679
the data volume automatically checks if

00:21:39,760 --> 00:21:43,840
smart cloning is possible

00:21:41,679 --> 00:21:45,280
if yes then create a snapshot of the

00:21:43,840 --> 00:21:47,679
source pvc

00:21:45,280 --> 00:21:50,159
next the snapshot is being used to bring

00:21:47,679 --> 00:21:52,640
a new persistent volume claim

00:21:50,159 --> 00:21:54,080
finally the snapshot itself is going to

00:21:52,640 --> 00:21:55,679
be deleted

00:21:54,080 --> 00:21:58,080
additionally if you want to use the

00:21:55,679 --> 00:22:00,640
feature the requirement is to make

00:21:58,080 --> 00:22:02,559
sure that source pvc is in the same name

00:22:00,640 --> 00:22:04,880
space as the target pvc

00:22:02,559 --> 00:22:06,400
the source and target pvcs are in the

00:22:04,880 --> 00:22:08,960
same storage class

00:22:06,400 --> 00:22:09,440
and finally there must be a snapshot

00:22:08,960 --> 00:22:11,919
class

00:22:09,440 --> 00:22:13,840
associated with the storage class

00:22:11,919 --> 00:22:15,440
depending on the storage backend you can

00:22:13,840 --> 00:22:17,120
really benefit from this

00:22:15,440 --> 00:22:19,200
especially when snapshots are

00:22:17,120 --> 00:22:21,919
implemented as copy on right

00:22:19,200 --> 00:22:23,760
which is often on the next slide i'm

00:22:21,919 --> 00:22:24,799
going to show you in detail how we deal

00:22:23,760 --> 00:22:27,200
with disks

00:22:24,799 --> 00:22:29,039
and volumes management in the gardener

00:22:27,200 --> 00:22:30,799
cube vert extension

00:22:29,039 --> 00:22:32,400
each shoot cluster that i was talking

00:22:30,799 --> 00:22:35,039
about at the beginning

00:22:32,400 --> 00:22:36,320
is defined as a custom resource object

00:22:35,039 --> 00:22:38,320
you can see an example

00:22:36,320 --> 00:22:39,520
on the left side the should yaml

00:22:38,320 --> 00:22:41,600
definition

00:22:39,520 --> 00:22:44,000
as you can see there is a worker section

00:22:41,600 --> 00:22:47,120
where users can define volumes

00:22:44,000 --> 00:22:49,679
the first one the volume object spec it

00:22:47,120 --> 00:22:52,240
defines a root disk of virtual machine

00:22:49,679 --> 00:22:55,120
and the second one data volumes which

00:22:52,240 --> 00:22:57,120
are blank additional data volumes disks

00:22:55,120 --> 00:22:58,880
the worker pool definition is processed

00:22:57,120 --> 00:23:01,520
by the worker controller

00:22:58,880 --> 00:23:03,679
in the keyword extension based on that

00:23:01,520 --> 00:23:05,520
an appropriate specification of virtual

00:23:03,679 --> 00:23:07,440
machine is prepared

00:23:05,520 --> 00:23:09,679
the size parameter is an obvious

00:23:07,440 --> 00:23:11,600
indication of the storage size

00:23:09,679 --> 00:23:13,280
and the type determines which storage

00:23:11,600 --> 00:23:15,280
class should be used

00:23:13,280 --> 00:23:16,880
somebody could ask why didn't you name

00:23:15,280 --> 00:23:19,280
the parameter storage class

00:23:16,880 --> 00:23:21,440
instead of type we didn't because

00:23:19,280 --> 00:23:24,960
gardener supports many cloud providers

00:23:21,440 --> 00:23:25,679
and this is the api then following the

00:23:24,960 --> 00:23:27,679
arrows

00:23:25,679 --> 00:23:29,679
we can see that volumes are translated

00:23:27,679 --> 00:23:32,080
to data volume templates

00:23:29,679 --> 00:23:34,080
so data volume is closely related to the

00:23:32,080 --> 00:23:36,000
virtual machine lifecycle

00:23:34,080 --> 00:23:38,640
by default the extension uses a

00:23:36,000 --> 00:23:40,559
prelocated data volume approach

00:23:38,640 --> 00:23:43,120
which means that the first virtual

00:23:40,559 --> 00:23:44,799
machine created will download the image

00:23:43,120 --> 00:23:47,840
and other virtual machines from the

00:23:44,799 --> 00:23:49,840
worker pool will clone the volume

00:23:47,840 --> 00:23:52,240
we use blank data volumes to provide

00:23:49,840 --> 00:23:54,159
additional disk and storage space

00:23:52,240 --> 00:23:56,559
these are completely blank disks and

00:23:54,159 --> 00:23:58,799
it's up to the user to handle them

00:23:56,559 --> 00:24:00,720
one way would be to run cloud init and

00:23:58,799 --> 00:24:03,279
format the disk according to the

00:24:00,720 --> 00:24:05,440
to their needs although i must admit

00:24:03,279 --> 00:24:07,120
that this is not yet possible to pass

00:24:05,440 --> 00:24:08,960
users cloud init scripts in the

00:24:07,120 --> 00:24:10,799
extension the reason

00:24:08,960 --> 00:24:13,039
is that we are using cloud init to join

00:24:10,799 --> 00:24:14,640
nodes and we still have to decide how we

00:24:13,039 --> 00:24:16,000
want to deal with the custom part of

00:24:14,640 --> 00:24:17,840
cloud unit

00:24:16,000 --> 00:24:20,159
but for sure at some point this feature

00:24:17,840 --> 00:24:22,400
will land in the extension

00:24:20,159 --> 00:24:24,559
on that slide i'm going to talk about

00:24:22,400 --> 00:24:27,279
highly isolated clusters

00:24:24,559 --> 00:24:29,200
and how to upload disk images there it

00:24:27,279 --> 00:24:31,360
happens that customers demand highly

00:24:29,200 --> 00:24:33,600
isolated environments

00:24:31,360 --> 00:24:35,120
not so long ago at cubermatik we had

00:24:33,600 --> 00:24:37,440
such a case

00:24:35,120 --> 00:24:39,360
and environment where we could not pull

00:24:37,440 --> 00:24:40,720
keyword images directly from the

00:24:39,360 --> 00:24:42,799
internet

00:24:40,720 --> 00:24:44,559
there was only a bastion host where we

00:24:42,799 --> 00:24:47,600
could access a cluster

00:24:44,559 --> 00:24:48,640
and expose a few services outside but

00:24:47,600 --> 00:24:50,720
still

00:24:48,640 --> 00:24:52,240
those services were only visible to the

00:24:50,720 --> 00:24:55,120
bastion host

00:24:52,240 --> 00:24:56,799
each bot couldn't reach the internet

00:24:55,120 --> 00:24:58,080
somehow we had to provide virtual

00:24:56,799 --> 00:25:00,640
machine images

00:24:58,080 --> 00:25:02,000
to the cluster we could create a local

00:25:00,640 --> 00:25:04,799
image repository

00:25:02,000 --> 00:25:07,200
however with containerized data importer

00:25:04,799 --> 00:25:09,679
it is quite easy to upload image

00:25:07,200 --> 00:25:11,919
we can benefit from that and prepare

00:25:09,679 --> 00:25:12,960
data volume which will be a source for

00:25:11,919 --> 00:25:16,400
others

00:25:12,960 --> 00:25:19,200
the upload is done over cd8 upload proxy

00:25:16,400 --> 00:25:20,159
first users have to expose upload proxy

00:25:19,200 --> 00:25:22,559
service

00:25:20,159 --> 00:25:24,000
and it must be accessible from outside

00:25:22,559 --> 00:25:26,640
the cluster

00:25:24,000 --> 00:25:28,080
next it is required to create a data

00:25:26,640 --> 00:25:31,360
volume with source set to

00:25:28,080 --> 00:25:34,000
upload similar like on the slide

00:25:31,360 --> 00:25:34,720
and finally we must request an upload

00:25:34,000 --> 00:25:36,720
token

00:25:34,720 --> 00:25:38,640
that can be done over upload token

00:25:36,720 --> 00:25:40,720
request custom resource

00:25:38,640 --> 00:25:42,640
the crd of the object comes from the

00:25:40,720 --> 00:25:45,120
project itself

00:25:42,640 --> 00:25:47,039
when all dimension steps are complete

00:25:45,120 --> 00:25:49,279
it's time to upload an image

00:25:47,039 --> 00:25:51,679
you can obtain the generated token from

00:25:49,279 --> 00:25:53,919
the status section of the cr

00:25:51,679 --> 00:25:55,360
then you have to pass the token to an

00:25:53,919 --> 00:25:58,240
http request

00:25:55,360 --> 00:25:59,279
with a disk image there are two ways of

00:25:58,240 --> 00:26:01,919
upload

00:25:59,279 --> 00:26:03,600
synchronous and asynchronous the

00:26:01,919 --> 00:26:06,320
synchronous connection can be closed

00:26:03,600 --> 00:26:08,880
unexpectedly because of the conversion

00:26:06,320 --> 00:26:11,840
or resizing process that can take some

00:26:08,880 --> 00:26:14,080
time with an unsynchronous approach

00:26:11,840 --> 00:26:16,559
the connection will be closed as soon as

00:26:14,080 --> 00:26:19,360
the disk image has been transmitted

00:26:16,559 --> 00:26:21,520
however you have to check yourself if

00:26:19,360 --> 00:26:22,880
the process of resizing and conversion

00:26:21,520 --> 00:26:25,279
is finished

00:26:22,880 --> 00:26:27,200
for more details and examples i

00:26:25,279 --> 00:26:29,360
encourage you to visit containerized

00:26:27,200 --> 00:26:31,120
data importer github page

00:26:29,360 --> 00:26:33,279
it's a very nice project which solves

00:26:31,120 --> 00:26:35,120
many issues regarding volumes and disks

00:26:33,279 --> 00:26:37,440
management

00:26:35,120 --> 00:26:39,679
this concludes our talk we are so

00:26:37,440 --> 00:26:43,039
excited to share with you

00:26:39,679 --> 00:26:45,279
the optimizations in converse using

00:26:43,039 --> 00:26:46,880
monitors and beta volumes for high

00:26:45,279 --> 00:26:48,480
performance and high security

00:26:46,880 --> 00:26:50,320
configurations

00:26:48,480 --> 00:26:52,640
there are even more features in cubic

00:26:50,320 --> 00:26:55,200
verse that's including the cpu

00:26:52,640 --> 00:26:57,039
memory storage and networks we are

00:26:55,200 --> 00:27:00,880
looking forward to share with you

00:26:57,039 --> 00:27:00,880

YouTube URL: https://www.youtube.com/watch?v=_dyUGDhCb6U


