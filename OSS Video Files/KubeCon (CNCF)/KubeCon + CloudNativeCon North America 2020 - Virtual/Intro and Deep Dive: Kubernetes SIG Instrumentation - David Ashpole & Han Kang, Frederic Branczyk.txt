Title: Intro and Deep Dive: Kubernetes SIG Instrumentation - David Ashpole & Han Kang, Frederic Branczyk
Publication date: 2020-11-23
Playlist: KubeCon + CloudNativeCon North America 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon Europe 2021 Virtual from May 4–7, 2021. Learn more at https://kubecon.io. The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects. 

Intro and Deep Dive: Kubernetes SIG Instrumentation - David Ashpole & Han Kang, Google, Frederic Branczyk, Independent, & Elana Hashman 

Kubernetes SIG Instrumentation is responsible for ensuring high quality and consistent instrumentation across the Kubernetes project. We will begin with an introductory overview of the efforts the SIG Instrumentation has worked on in the past and is currently working on. This deep dive session will go in detail currently ongoing efforts happening within SIG Instrumentation to share with the audience concrete pieces of work to encourage future collaboration. Software engineering and operations are both disciplines practiced in SIG Instrumentation, and any experience will help the special interest group's mission. Join this session to learn how to get involved in SIG Instrumentation to make instrumentation even better! 

https://sched.co/ekHG
Captions: 
	00:00:00,719 --> 00:00:03,679
hello everyone and welcome to the

00:00:02,080 --> 00:00:06,160
kubernetes sig instrumentation

00:00:03,679 --> 00:00:08,800
introduction and deep dive session i'm

00:00:06,160 --> 00:00:11,040
david ashbal from google

00:00:08,800 --> 00:00:13,599
hey i'm frederick i'm the ceo and

00:00:11,040 --> 00:00:15,839
founder of polar signals

00:00:13,599 --> 00:00:17,760
i'm alana hashman i currently work as a

00:00:15,839 --> 00:00:19,439
principal site reliability engineer at

00:00:17,760 --> 00:00:21,920
red hat and i'm one of the co-chairs of

00:00:19,439 --> 00:00:25,039
sing instrumentation

00:00:21,920 --> 00:00:26,400
hey i'm han i am also a co-chair of sig

00:00:25,039 --> 00:00:31,359
instrumentation i also work

00:00:26,400 --> 00:00:33,360
at google with david nashville

00:00:31,359 --> 00:00:34,960
great so i'm going to kick us off and

00:00:33,360 --> 00:00:37,040
this is what we're going to cover in

00:00:34,960 --> 00:00:39,680
today's maintainer track session

00:00:37,040 --> 00:00:41,520
uh so i'm going to introduce uh what

00:00:39,680 --> 00:00:42,960
does sig instrumentation do what are we

00:00:41,520 --> 00:00:44,879
responsible for

00:00:42,960 --> 00:00:47,039
and then we'll get into some of our

00:00:44,879 --> 00:00:48,960
current activities status updates on

00:00:47,039 --> 00:00:50,000
what we're doing for the various sig

00:00:48,960 --> 00:00:53,039
components

00:00:50,000 --> 00:00:54,719
metrics logs and events traces and

00:00:53,039 --> 00:00:55,920
what's going on with all of our sig sub

00:00:54,719 --> 00:00:58,079
projects

00:00:55,920 --> 00:00:59,280
and in each of these sections one of my

00:00:58,079 --> 00:01:02,320
co-leads will

00:00:59,280 --> 00:01:05,119
introduce the topic and what's going on

00:01:02,320 --> 00:01:06,159
uh then i will close out our talk with

00:01:05,119 --> 00:01:08,000
some resources

00:01:06,159 --> 00:01:09,280
on how to get involved in the sig how to

00:01:08,000 --> 00:01:10,799
contribute uh

00:01:09,280 --> 00:01:12,640
where to find us in the various

00:01:10,799 --> 00:01:15,200
kubernetes online spaces

00:01:12,640 --> 00:01:17,520
and also share some links to related

00:01:15,200 --> 00:01:17,520
talks

00:01:17,840 --> 00:01:21,680
so what does thing instrumentation do

00:01:20,640 --> 00:01:24,479
our charter

00:01:21,680 --> 00:01:25,119
and you can find these slides on the

00:01:24,479 --> 00:01:27,439
schedule

00:01:25,119 --> 00:01:29,759
so you can click all of these links if

00:01:27,439 --> 00:01:31,680
you want to get a chance to drill down

00:01:29,759 --> 00:01:33,759
but our charter in the kubernetes

00:01:31,680 --> 00:01:36,240
community repo says that we cover

00:01:33,759 --> 00:01:36,799
best practices for cluster observability

00:01:36,240 --> 00:01:39,439
across

00:01:36,799 --> 00:01:42,000
all kubernetes components and as well we

00:01:39,439 --> 00:01:44,880
develop all of these relevant components

00:01:42,000 --> 00:01:46,960
so uh in summary uh i like to think of

00:01:44,880 --> 00:01:49,759
this as working on metrics

00:01:46,960 --> 00:01:50,240
logs and events uh and traces which are

00:01:49,759 --> 00:01:52,479
sort of

00:01:50,240 --> 00:01:54,159
are various pillars of observability

00:01:52,479 --> 00:01:56,000
within the project

00:01:54,159 --> 00:01:57,920
we're also responsible for a number of

00:01:56,000 --> 00:01:59,040
sub-projects again covered in our

00:01:57,920 --> 00:02:00,640
charter

00:01:59,040 --> 00:02:02,079
but some of those that you might be most

00:02:00,640 --> 00:02:05,759
familiar with include

00:02:02,079 --> 00:02:09,280
cube state metrics k log metric server

00:02:05,759 --> 00:02:12,800
and many more

00:02:09,280 --> 00:02:13,840
so how do we do it our sig activities

00:02:12,800 --> 00:02:15,840
usually involve

00:02:13,840 --> 00:02:17,599
triaging and fixing relevant

00:02:15,840 --> 00:02:20,000
instrumentation issues

00:02:17,599 --> 00:02:21,520
reviewing all code changes for metrics

00:02:20,000 --> 00:02:24,319
dot go files

00:02:21,520 --> 00:02:26,160
developing new features and enhancements

00:02:24,319 --> 00:02:28,640
and maintaining all of our various sub

00:02:26,160 --> 00:02:28,640
projects

00:02:30,000 --> 00:02:36,400
hey elena what do you call a chart

00:02:31,760 --> 00:02:40,080
without any underlying metric data

00:02:36,400 --> 00:02:42,640
i don't know what do you call it han

00:02:40,080 --> 00:02:42,640
pointless

00:02:43,920 --> 00:02:48,080
you gotta have the drum beat uh so in

00:02:47,040 --> 00:02:49,920
sick instrumentation

00:02:48,080 --> 00:02:52,720
uh you'll know me either as the metrics

00:02:49,920 --> 00:02:54,080
guy or the guy who likes terrible puns

00:02:52,720 --> 00:02:58,480
but you're not going to hear me tell a

00:02:54,080 --> 00:03:01,040
pun about insects because they bug me

00:02:58,480 --> 00:03:01,599
all right that's the last one uh jokes

00:03:01,040 --> 00:03:03,840
aside

00:03:01,599 --> 00:03:05,360
in order to understand how metrics works

00:03:03,840 --> 00:03:06,879
in kubernetes we're going to have to

00:03:05,360 --> 00:03:08,080
know a little bit about kubernetes

00:03:06,879 --> 00:03:10,080
though

00:03:08,080 --> 00:03:11,120
just very briefly kubernetes pretty

00:03:10,080 --> 00:03:14,480
complicated

00:03:11,120 --> 00:03:16,840
stack and it has a lot of despair

00:03:14,480 --> 00:03:18,080
components and it gets even more

00:03:16,840 --> 00:03:21,680
complicated

00:03:18,080 --> 00:03:23,360
because these despair components

00:03:21,680 --> 00:03:24,799
have quite a large number of

00:03:23,360 --> 00:03:26,000
interactions and this can make it

00:03:24,799 --> 00:03:29,440
difficult to tell

00:03:26,000 --> 00:03:31,519
if and when something is going wrong

00:03:29,440 --> 00:03:32,560
and this is basically where metrics come

00:03:31,519 --> 00:03:35,440
in

00:03:32,560 --> 00:03:35,920
and in case we instrument our binaries

00:03:35,440 --> 00:03:38,080
using

00:03:35,920 --> 00:03:38,959
prometheus clients which can look

00:03:38,080 --> 00:03:41,360
something

00:03:38,959 --> 00:03:41,360
like this

00:03:42,640 --> 00:03:45,760
in general you're going to have a

00:03:44,080 --> 00:03:49,599
component that you want to instrument

00:03:45,760 --> 00:03:52,560
some bit of software and the software

00:03:49,599 --> 00:03:55,200
exposes a simple http endpoint

00:03:52,560 --> 00:03:58,239
conventionally slash metrics

00:03:55,200 --> 00:03:59,120
which is then scraped by some monitoring

00:03:58,239 --> 00:04:01,599
agent and

00:03:59,120 --> 00:04:02,400
it into a time series database or back

00:04:01,599 --> 00:04:04,879
end

00:04:02,400 --> 00:04:06,239
and uh for those of you who are already

00:04:04,879 --> 00:04:09,599
familiar with previous

00:04:06,239 --> 00:04:10,879
metrics this may seem simple and a bit

00:04:09,599 --> 00:04:13,599
boring

00:04:10,879 --> 00:04:14,640
and yeah it is simple enough we have

00:04:13,599 --> 00:04:16,959
some software

00:04:14,640 --> 00:04:20,160
and we want to measure stuff i mean what

00:04:16,959 --> 00:04:20,160
could possibly go wrong

00:04:26,560 --> 00:04:34,000
so uh well it turns out quite a lot

00:04:29,840 --> 00:04:34,000
uh metrics can become memory leaks

00:04:34,479 --> 00:04:43,840
uh typically through unbounded

00:04:36,960 --> 00:04:43,840
personality and label values

00:04:44,800 --> 00:04:52,000
even worse we can have unbounded

00:04:48,560 --> 00:04:52,000
interpolated metric names

00:04:53,120 --> 00:04:59,600
and we can get any of these leaks in any

00:04:56,160 --> 00:04:59,600
of the kubernetes binaries

00:05:01,280 --> 00:05:07,360
these issues can be latent and they can

00:05:03,440 --> 00:05:07,360
manifest through innocuous underlying

00:05:08,840 --> 00:05:12,080
changes

00:05:10,000 --> 00:05:13,039
sometimes our metrics just don't do the

00:05:12,080 --> 00:05:16,800
job properly

00:05:13,039 --> 00:05:18,560
like we have inadequate bucket sizes um

00:05:16,800 --> 00:05:20,240
which don't make any sense so if you

00:05:18,560 --> 00:05:22,240
have two buckets for latency metrics

00:05:20,240 --> 00:05:26,960
obviously you're not going to get

00:05:22,240 --> 00:05:26,960
super helpful latency uh data

00:05:29,280 --> 00:05:32,320
sometimes the metric says it's actually

00:05:31,199 --> 00:05:34,240
admitting seconds

00:05:32,320 --> 00:05:39,840
but the metric unit it's actually

00:05:34,240 --> 00:05:41,680
emitting is in microseconds

00:05:39,840 --> 00:05:43,600
so that brings us to the stuff that we

00:05:41,680 --> 00:05:46,639
do uh involving metrics

00:05:43,600 --> 00:05:49,680
in signal instrumentation um

00:05:46,639 --> 00:05:50,720
first and foremost uh due to all of

00:05:49,680 --> 00:05:53,440
these things that we've

00:05:50,720 --> 00:05:54,320
encountered in kubernetes we overhauled

00:05:53,440 --> 00:05:56,880
our metrics

00:05:54,320 --> 00:05:58,000
this has landed in ga recently if you

00:05:56,880 --> 00:06:02,639
want to see the cap

00:05:58,000 --> 00:06:02,639
it's uh up here

00:06:03,840 --> 00:06:07,280
and this fixed a bunch of inconsistent

00:06:06,720 --> 00:06:09,440
and

00:06:07,280 --> 00:06:10,880
broken broken metrics across uh

00:06:09,440 --> 00:06:13,039
kubernetes

00:06:10,880 --> 00:06:14,479
but in doing so we changed the api and

00:06:13,039 --> 00:06:15,759
this caused issues for people who are

00:06:14,479 --> 00:06:19,199
ingesting our

00:06:15,759 --> 00:06:21,520
older and broken metrics uh

00:06:19,199 --> 00:06:23,520
and so in order to offset this we

00:06:21,520 --> 00:06:26,240
implemented a stability framework

00:06:23,520 --> 00:06:27,600
so that uh people who are ingesting

00:06:26,240 --> 00:06:30,960
kubernetes metrics

00:06:27,600 --> 00:06:34,720
can uh rely on them with

00:06:30,960 --> 00:06:38,479
uh a proper deprecation policy

00:06:34,720 --> 00:06:43,199
and this landed in beta and we have

00:06:38,479 --> 00:06:46,880
slated work to land this into ga in 121.

00:06:43,199 --> 00:06:49,199
uh we have

00:06:46,880 --> 00:06:50,720
not only do we focus on broken metrics

00:06:49,199 --> 00:06:53,759
but we also focus on

00:06:50,720 --> 00:06:55,520
uh improving existing metrics and on the

00:06:53,759 --> 00:06:56,880
road to alpha we have pod resource

00:06:55,520 --> 00:07:00,080
metrics which is slated

00:06:56,880 --> 00:07:00,960
to land in 120 and dynamic cardinality

00:07:00,080 --> 00:07:05,280
enforcement

00:07:00,960 --> 00:07:05,280
uh slated to land in 121

00:07:05,440 --> 00:07:12,240
but our work doesn't just involve

00:07:09,120 --> 00:07:14,240
fixing metrics and improving and

00:07:12,240 --> 00:07:15,840
iterating over existing metrics in the

00:07:14,240 --> 00:07:17,680
in the queue binders we also want to

00:07:15,840 --> 00:07:19,039
make it easier for people to debug

00:07:17,680 --> 00:07:21,120
kubernetes clusters

00:07:19,039 --> 00:07:22,639
and in order to do that uh we wrote this

00:07:21,120 --> 00:07:24,960
tool called promptq

00:07:22,639 --> 00:07:26,319
which is basically a in-memory

00:07:24,960 --> 00:07:29,120
prometheus client

00:07:26,319 --> 00:07:31,440
running in your cli to help you debug

00:07:29,120 --> 00:07:33,199
native prometheus endpoints

00:07:31,440 --> 00:07:35,599
and we have a link if you want to check

00:07:33,199 --> 00:07:35,599
that out

00:07:37,120 --> 00:07:40,160
and that's it for me

00:07:43,120 --> 00:07:46,160
cool thanks han that was really

00:07:44,639 --> 00:07:50,160
interesting

00:07:46,160 --> 00:07:50,160
so now let's talk about logs and events

00:07:53,039 --> 00:07:56,879
i'll start with events so events in

00:07:54,639 --> 00:07:59,840
kubernetes are

00:07:56,879 --> 00:08:01,199
the way that users tend to interact with

00:07:59,840 --> 00:08:02,160
kubernetes objects when they're first

00:08:01,199 --> 00:08:03,440
getting started

00:08:02,160 --> 00:08:06,800
as well as when they're just trying to

00:08:03,440 --> 00:08:06,800
figure out what the heck's been going on

00:08:07,360 --> 00:08:11,039
to think about it in terms of telemetry

00:08:09,199 --> 00:08:13,599
it's essentially like writing

00:08:11,039 --> 00:08:14,160
a structured log message to the api

00:08:13,599 --> 00:08:16,319
server

00:08:14,160 --> 00:08:18,160
that kubert that uh users are then

00:08:16,319 --> 00:08:21,280
allowed to query and show up and

00:08:18,160 --> 00:08:24,720
shows up in things like cube control top

00:08:21,280 --> 00:08:25,759
almost three years ago we

00:08:24,720 --> 00:08:28,479
found that we were having some

00:08:25,759 --> 00:08:30,800
scalability issues with events because

00:08:28,479 --> 00:08:32,479
they can be quite spammy if something is

00:08:30,800 --> 00:08:35,839
crash looping for example

00:08:32,479 --> 00:08:38,800
that will emit a lot of events

00:08:35,839 --> 00:08:40,000
and so way way way back then they came

00:08:38,800 --> 00:08:42,399
up with a plan for how to

00:08:40,000 --> 00:08:45,440
change the event object in order to make

00:08:42,399 --> 00:08:47,200
it scale better

00:08:45,440 --> 00:08:48,560
and add a little bit more structure to

00:08:47,200 --> 00:08:50,959
it

00:08:48,560 --> 00:08:54,480
and now finally three years later we've

00:08:50,959 --> 00:08:57,839
graduated this new events api to ga

00:08:54,480 --> 00:09:00,800
and it's currently in use in many

00:08:57,839 --> 00:09:02,080
components in the kubernetes ecosystem

00:09:00,800 --> 00:09:05,920
so

00:09:02,080 --> 00:09:05,920
great job we move something to ga

00:09:06,640 --> 00:09:11,200
for logging this is probably the most

00:09:09,600 --> 00:09:13,519
simple form of telemetry right we're

00:09:11,200 --> 00:09:14,800
just writing things to files so

00:09:13,519 --> 00:09:16,720
what the heck could we improve with

00:09:14,800 --> 00:09:20,000
logging well it turns out that

00:09:16,720 --> 00:09:23,120
oftentimes you want to know which

00:09:20,000 --> 00:09:24,959
things are being referenced in a log

00:09:23,120 --> 00:09:26,320
message for example if i have a log and

00:09:24,959 --> 00:09:28,160
it's about a pod

00:09:26,320 --> 00:09:30,160
it would be nice to represent fields

00:09:28,160 --> 00:09:32,800
from the pod such as the pod name

00:09:30,160 --> 00:09:33,920
in a standardized way so that i can if

00:09:32,800 --> 00:09:37,360
i'm a login gesture

00:09:33,920 --> 00:09:38,959
then i'm able to take those

00:09:37,360 --> 00:09:40,880
and search potentially over those

00:09:38,959 --> 00:09:43,760
attributes in my back end

00:09:40,880 --> 00:09:45,360
so sig instrumentation worked on

00:09:43,760 --> 00:09:48,160
structured logging

00:09:45,360 --> 00:09:49,360
and the method we chose to introduce

00:09:48,160 --> 00:09:51,839
this is to introduce

00:09:49,360 --> 00:09:53,440
new methods into the k log library

00:09:51,839 --> 00:09:55,760
you'll notice that the

00:09:53,440 --> 00:09:57,120
info and error methods here are appended

00:09:55,760 --> 00:10:00,080
with an s

00:09:57,120 --> 00:10:00,800
instead of the usual f for format and

00:10:00,080 --> 00:10:02,640
what this does

00:10:00,800 --> 00:10:05,040
is it allows you to set a message for

00:10:02,640 --> 00:10:08,640
your log line and then to add in

00:10:05,040 --> 00:10:11,680
key value pairs one after the other so

00:10:08,640 --> 00:10:13,600
for example the key might be pod

00:10:11,680 --> 00:10:15,680
and the name or and the value might be

00:10:13,600 --> 00:10:18,320
the name of the pod

00:10:15,680 --> 00:10:20,160
this is alpha in 1.19 and it can be

00:10:18,320 --> 00:10:22,560
enabled on any component

00:10:20,160 --> 00:10:24,880
in kubernetes using the logging format

00:10:22,560 --> 00:10:24,880
flag

00:10:25,760 --> 00:10:30,560
and because there are certain log

00:10:28,720 --> 00:10:32,480
messages that people really care about

00:10:30,560 --> 00:10:32,800
potentially some stuff in the api server

00:10:32,480 --> 00:10:35,920
or

00:10:32,800 --> 00:10:37,600
cubelet we're starting with those log

00:10:35,920 --> 00:10:39,760
messages that people

00:10:37,600 --> 00:10:41,760
find most impactful and if you're

00:10:39,760 --> 00:10:44,720
curious about the details feel free

00:10:41,760 --> 00:10:47,839
to check out the the structured logging

00:10:44,720 --> 00:10:47,839
blog post

00:10:49,200 --> 00:10:53,440
the next set of improvements we've been

00:10:51,120 --> 00:10:55,760
working on for logging is related to

00:10:53,440 --> 00:10:58,800
logging security

00:10:55,760 --> 00:11:01,680
so as a general rule it's

00:10:58,800 --> 00:11:02,640
a very bad thing to log credentials or

00:11:01,680 --> 00:11:05,680
other secrets

00:11:02,640 --> 00:11:08,399
into kubernetes logs and

00:11:05,680 --> 00:11:09,440
sadly this has happened more than once

00:11:08,399 --> 00:11:12,560
in the recent past

00:11:09,440 --> 00:11:14,000
and came up in a recent security review

00:11:12,560 --> 00:11:15,839
by a third party

00:11:14,000 --> 00:11:17,600
so we knew that we had to do something

00:11:15,839 --> 00:11:20,320
about it

00:11:17,600 --> 00:11:21,920
and we are taking two approaches in the

00:11:20,320 --> 00:11:24,560
1.20 released

00:11:21,920 --> 00:11:26,320
the first is dynamic standardization of

00:11:24,560 --> 00:11:28,320
logs meaning

00:11:26,320 --> 00:11:29,839
when your kubernetes component is

00:11:28,320 --> 00:11:31,760
running in your cluster

00:11:29,839 --> 00:11:32,959
and tries to log something that we think

00:11:31,760 --> 00:11:36,000
is bad

00:11:32,959 --> 00:11:37,760
um that log message will be uh

00:11:36,000 --> 00:11:39,600
blocked or otherwise modified so it

00:11:37,760 --> 00:11:42,079
doesn't contain the

00:11:39,600 --> 00:11:43,519
the secret information so that's that's

00:11:42,079 --> 00:11:45,360
one method

00:11:43,519 --> 00:11:47,440
and another method that we're going to

00:11:45,360 --> 00:11:50,399
be applying in 1.20

00:11:47,440 --> 00:11:51,920
is static checking and this is mostly

00:11:50,399 --> 00:11:53,519
during development so

00:11:51,920 --> 00:11:56,160
you can think of this as we're basically

00:11:53,519 --> 00:11:58,560
adding pre-submits and ede tests

00:11:56,160 --> 00:11:59,600
that go through or that run kubernetes

00:11:58,560 --> 00:12:01,920
components

00:11:59,600 --> 00:12:04,639
or know that statically analyze all of

00:12:01,920 --> 00:12:08,480
our controller binaries

00:12:04,639 --> 00:12:11,600
and look for points in which the

00:12:08,480 --> 00:12:15,200
secret information uh could be

00:12:11,600 --> 00:12:17,360
logged using the k log libraries

00:12:15,200 --> 00:12:20,079
so we we try and programmatically figure

00:12:17,360 --> 00:12:21,760
out where that could be happening

00:12:20,079 --> 00:12:23,680
and this can be enabled with the logging

00:12:21,760 --> 00:12:24,480
standardization flag i believe that's

00:12:23,680 --> 00:12:27,839
for

00:12:24,480 --> 00:12:29,279
dynamic standardization and you can see

00:12:27,839 --> 00:12:32,399
more information

00:12:29,279 --> 00:12:32,399
at the linked cap here

00:12:33,760 --> 00:12:37,600
now let's talk about traces traces are

00:12:36,320 --> 00:12:39,920
exciting and new

00:12:37,600 --> 00:12:41,279
in fact this is the first time that

00:12:39,920 --> 00:12:42,639
kubernetes is doing anything with

00:12:41,279 --> 00:12:44,480
tracing at all

00:12:42,639 --> 00:12:46,079
so what are we tackling first well we

00:12:44,480 --> 00:12:49,680
decided to start with the

00:12:46,079 --> 00:12:52,560
simple um and straightforward use

00:12:49,680 --> 00:12:53,760
with tracing api server requests the api

00:12:52,560 --> 00:12:56,560
server is

00:12:53,760 --> 00:12:58,399
a big http server that runs at the heart

00:12:56,560 --> 00:13:00,320
of a kubernetes cluster

00:12:58,399 --> 00:13:01,440
and it would be really useful to know

00:13:00,320 --> 00:13:04,240
how long

00:13:01,440 --> 00:13:05,440
different requests take and especially

00:13:04,240 --> 00:13:07,040
for those ones that

00:13:05,440 --> 00:13:08,880
don't behave as we expected maybe

00:13:07,040 --> 00:13:10,480
they're too slow

00:13:08,880 --> 00:13:12,639
we'd like to be able to see detailed

00:13:10,480 --> 00:13:14,639
information about how that request

00:13:12,639 --> 00:13:18,399
passed through the api server

00:13:14,639 --> 00:13:21,600
and on to other clients such as fcd

00:13:18,399 --> 00:13:23,360
so in 1.20 we're going to be adding

00:13:21,600 --> 00:13:24,959
distributed tracing to the api server

00:13:23,360 --> 00:13:26,639
using open telemetry

00:13:24,959 --> 00:13:28,320
and you can enable it by specifying a

00:13:26,639 --> 00:13:29,360
configuration file with the open

00:13:28,320 --> 00:13:32,399
telemetry config

00:13:29,360 --> 00:13:34,880
file flag if you'd like to read more

00:13:32,399 --> 00:13:38,399
about it you can look at the

00:13:34,880 --> 00:13:38,399
cap which is linked below

00:13:41,839 --> 00:13:46,880
all right thank you david for

00:13:44,959 --> 00:13:48,560
sharing all this awesome work i'm

00:13:46,880 --> 00:13:50,639
definitely super excited about the

00:13:48,560 --> 00:13:52,880
tracing work that's happening

00:13:50,639 --> 00:13:54,480
unfortunately i don't have a great joke

00:13:52,880 --> 00:13:56,959
like han to start out with

00:13:54,480 --> 00:13:58,000
so let's just take it away and talk

00:13:56,959 --> 00:14:02,240
about some of our sub

00:13:58,000 --> 00:14:05,440
projects next slide

00:14:02,240 --> 00:14:08,959
um so we have uh three primary

00:14:05,440 --> 00:14:10,480
um sub projects um which are

00:14:08,959 --> 00:14:12,000
actually we have a couple more but these

00:14:10,480 --> 00:14:14,000
are this is the selection that we want

00:14:12,000 --> 00:14:16,560
to talk about this time

00:14:14,000 --> 00:14:18,959
uh this time around um the first one

00:14:16,560 --> 00:14:21,839
being coop state metrics which i believe

00:14:18,959 --> 00:14:22,880
may be the oldest sub-project of stick

00:14:21,839 --> 00:14:26,320
instrumentation

00:14:22,880 --> 00:14:29,760
it is actually under the kubernetes or

00:14:26,320 --> 00:14:31,600
that's how old it is um

00:14:29,760 --> 00:14:32,959
i'm saying that because things don't get

00:14:31,600 --> 00:14:34,240
submitted to the kubernetes arc

00:14:32,959 --> 00:14:37,360
generally anymore

00:14:34,240 --> 00:14:38,880
um then the other one that we

00:14:37,360 --> 00:14:41,440
that we're talking about today is the

00:14:38,880 --> 00:14:43,680
metric server and we'll see what that is

00:14:41,440 --> 00:14:45,760
um a little bit later and then the

00:14:43,680 --> 00:14:48,959
kubernetes prometheus adapter

00:14:45,760 --> 00:14:51,199
which we'll also see okay

00:14:48,959 --> 00:14:53,440
so first off coupe state metrics i think

00:14:51,199 --> 00:14:58,000
this is a really exciting component

00:14:53,440 --> 00:15:00,320
and this really originated from a need

00:14:58,000 --> 00:15:01,519
where we were talking to a bunch of

00:15:00,320 --> 00:15:04,240
people in the

00:15:01,519 --> 00:15:06,480
kubernetes ecosystem who were also using

00:15:04,240 --> 00:15:09,440
prometheus at the time

00:15:06,480 --> 00:15:10,079
and the gap was kind of people were

00:15:09,440 --> 00:15:12,320
saying

00:15:10,079 --> 00:15:13,600
well prometheus is great and kubernetes

00:15:12,320 --> 00:15:16,000
is great but when i

00:15:13,600 --> 00:15:18,160
actually troubleshoot my applications i

00:15:16,000 --> 00:15:20,959
still drop down into coupe ctl

00:15:18,160 --> 00:15:22,000
and query things it would be really

00:15:20,959 --> 00:15:24,160
handy if i had

00:15:22,000 --> 00:15:26,240
a lot of this information queriable in

00:15:24,160 --> 00:15:26,959
prometheus i can alert on it i can do

00:15:26,240 --> 00:15:29,120
all these

00:15:26,959 --> 00:15:30,720
automated workflows with it and so

00:15:29,120 --> 00:15:32,160
that's kind of where coupe state metrics

00:15:30,720 --> 00:15:34,720
was born

00:15:32,160 --> 00:15:35,920
and kind of the philosophy that we have

00:15:34,720 --> 00:15:39,360
with kubeseconds

00:15:35,920 --> 00:15:42,399
is that anything that can be a

00:15:39,360 --> 00:15:45,440
metric in a kubernetes api object so

00:15:42,399 --> 00:15:48,399
pods deployments staple sets you can

00:15:45,440 --> 00:15:49,519
pretty much find metrics about any api

00:15:48,399 --> 00:15:52,320
object that is

00:15:49,519 --> 00:15:53,440
available in kubernetes as metrics and

00:15:52,320 --> 00:15:55,440
coupe state metrics

00:15:53,440 --> 00:15:56,800
and we kind of take everything that can

00:15:55,440 --> 00:16:00,160
possibly be a metric

00:15:56,800 --> 00:16:02,160
and convert it into prometheus um

00:16:00,160 --> 00:16:04,000
into the prometheus exposition format

00:16:02,160 --> 00:16:05,199
and then whenever prometheus comes

00:16:04,000 --> 00:16:08,959
around

00:16:05,199 --> 00:16:10,959
it just scrapes scrapes this output

00:16:08,959 --> 00:16:13,519
output

00:16:10,959 --> 00:16:14,720
and ingests it and then we can do really

00:16:13,519 --> 00:16:16,480
exciting things like

00:16:14,720 --> 00:16:18,880
what we have on the slides here where we

00:16:16,480 --> 00:16:21,279
can say

00:16:18,880 --> 00:16:22,880
well i have some expected number of

00:16:21,279 --> 00:16:25,519
replicas and i have an

00:16:22,880 --> 00:16:27,120
actual number of replicas of my

00:16:25,519 --> 00:16:29,040
deployment and if these are

00:16:27,120 --> 00:16:30,480
not the same then obviously something's

00:16:29,040 --> 00:16:32,399
not going

00:16:30,480 --> 00:16:33,519
the way it should be going and we can

00:16:32,399 --> 00:16:35,759
write

00:16:33,519 --> 00:16:37,199
pretty sophisticated and really

00:16:35,759 --> 00:16:38,720
incredible alerting routes that have

00:16:37,199 --> 00:16:40,880
definitely helped me

00:16:38,720 --> 00:16:42,079
run applications on top of kubernetes

00:16:40,880 --> 00:16:45,440
numerous times

00:16:42,079 --> 00:16:47,920
so this is extremely helpful

00:16:45,440 --> 00:16:49,279
and one exciting thing about coop state

00:16:47,920 --> 00:16:52,959
metrics is that we're

00:16:49,279 --> 00:16:53,360
actually just spent well actually over a

00:16:52,959 --> 00:16:56,320
year

00:16:53,360 --> 00:16:56,880
almost cleaning up the entire code base

00:16:56,320 --> 00:16:59,199
um

00:16:56,880 --> 00:17:00,560
and we've started doing a couple of

00:16:59,199 --> 00:17:02,880
pre-releases of a

00:17:00,560 --> 00:17:03,759
of a new major version of this this

00:17:02,880 --> 00:17:07,280
project

00:17:03,759 --> 00:17:10,640
so please go ahead and check this out

00:17:07,280 --> 00:17:13,360
try it out run it on your clusters uh

00:17:10,640 --> 00:17:14,480
give us feedback both in terms of

00:17:13,360 --> 00:17:17,199
performance

00:17:14,480 --> 00:17:19,039
um as well as obviously whether things

00:17:17,199 --> 00:17:20,880
still work that they used to work

00:17:19,039 --> 00:17:22,640
if you if you may already be running

00:17:20,880 --> 00:17:26,079
group state metrics

00:17:22,640 --> 00:17:27,360
um and the last thing that i think

00:17:26,079 --> 00:17:29,520
i want to mention about coop state

00:17:27,360 --> 00:17:32,000
metrics that's kind of unique about it

00:17:29,520 --> 00:17:34,559
is we've done a number of really

00:17:32,000 --> 00:17:36,720
incredible performance improvements

00:17:34,559 --> 00:17:39,600
where crusade matrix doesn't actually

00:17:36,720 --> 00:17:42,240
use the normal prometheus client

00:17:39,600 --> 00:17:43,600
because um because of the nature of what

00:17:42,240 --> 00:17:45,039
group state metrics does it kind of

00:17:43,600 --> 00:17:48,400
converts every api

00:17:45,039 --> 00:17:51,120
object to a metric it tends to

00:17:48,400 --> 00:17:52,000
get tends to have really huge metrics

00:17:51,120 --> 00:17:55,039
output

00:17:52,000 --> 00:17:57,600
so i'm talking megabytes of slash

00:17:55,039 --> 00:18:01,760
metrics and points

00:17:57,600 --> 00:18:05,600
and so that is a different dimension

00:18:01,760 --> 00:18:06,400
of even writing bytes out to an http

00:18:05,600 --> 00:18:08,080
request

00:18:06,400 --> 00:18:09,919
so if you're interested in kind of

00:18:08,080 --> 00:18:13,039
performance work there's

00:18:09,919 --> 00:18:14,880
lots still possible in cubesate metrics

00:18:13,039 --> 00:18:16,640
so get involved into this project if

00:18:14,880 --> 00:18:17,360
these are things that are appealing to

00:18:16,640 --> 00:18:20,080
you

00:18:17,360 --> 00:18:22,720
um but we've already done a really

00:18:20,080 --> 00:18:25,679
incredible job i think we used to have

00:18:22,720 --> 00:18:27,200
tens of seconds of latency with really

00:18:25,679 --> 00:18:30,160
huge clusters and we've brought that

00:18:27,200 --> 00:18:32,080
down to a handful of seconds

00:18:30,160 --> 00:18:33,919
in really really huge clusters so i

00:18:32,080 --> 00:18:34,320
think we've done a pretty good job but

00:18:33,919 --> 00:18:36,720
there's

00:18:34,320 --> 00:18:37,360
always room for improvement so yeah

00:18:36,720 --> 00:18:40,720
that's

00:18:37,360 --> 00:18:44,720
what i have to say about cubesat metrics

00:18:40,720 --> 00:18:47,200
so going on to the metric server

00:18:44,720 --> 00:18:48,960
you may some people may not be aware of

00:18:47,200 --> 00:18:51,520
this component but you've

00:18:48,960 --> 00:18:52,559
almost certainly used it or another

00:18:51,520 --> 00:18:56,400
variation of the

00:18:52,559 --> 00:18:58,960
resource metrics api because

00:18:56,400 --> 00:18:59,840
the resource matrix api is essentially

00:18:58,960 --> 00:19:03,200
the

00:18:59,840 --> 00:19:06,720
generic description of an api

00:19:03,200 --> 00:19:07,760
that can be used to request cpu and

00:19:06,720 --> 00:19:10,880
memory usage

00:19:07,760 --> 00:19:12,720
of pods containers and nodes so if

00:19:10,880 --> 00:19:15,919
you've used kubectl pod

00:19:12,720 --> 00:19:18,160
you've actually indirectly used this api

00:19:15,919 --> 00:19:20,640
because coop ctl pod essentially

00:19:18,160 --> 00:19:23,280
requests a resource metrics api

00:19:20,640 --> 00:19:24,320
implementation and the metric server

00:19:23,280 --> 00:19:27,360
happens to be

00:19:24,320 --> 00:19:29,120
kind of the what we say the the default

00:19:27,360 --> 00:19:30,799
implementation of the resource metrics

00:19:29,120 --> 00:19:34,799
api

00:19:30,799 --> 00:19:38,080
and uh this this api can also be used to

00:19:34,799 --> 00:19:42,080
auto scale your deployments

00:19:38,080 --> 00:19:45,440
on kubernetes with resource metrics so

00:19:42,080 --> 00:19:48,559
as i said cpu or memory and the

00:19:45,440 --> 00:19:52,400
varying kind of

00:19:48,559 --> 00:19:53,760
usages that you uh that you can kind of

00:19:52,400 --> 00:19:56,960
can configure

00:19:53,760 --> 00:19:59,440
um and the

00:19:56,960 --> 00:20:00,080
the thing to kind of uh keep in mind

00:19:59,440 --> 00:20:03,039
here this

00:20:00,080 --> 00:20:04,480
component essentially works very similar

00:20:03,039 --> 00:20:07,919
to prometheus but it's kind

00:20:04,480 --> 00:20:09,760
of a very narrow scope so it also goes

00:20:07,919 --> 00:20:12,480
around

00:20:09,760 --> 00:20:14,240
each individual kubelet and collects all

00:20:12,480 --> 00:20:16,159
of these metrics by pulling them i

00:20:14,240 --> 00:20:18,559
believe every minute

00:20:16,159 --> 00:20:20,240
and then holds the state in memory and

00:20:18,559 --> 00:20:21,760
then whenever there's a request

00:20:20,240 --> 00:20:24,960
let's say from group c tail top for

00:20:21,760 --> 00:20:27,840
example um then it presents the

00:20:24,960 --> 00:20:29,679
information that has been requested but

00:20:27,840 --> 00:20:30,159
this component is kind of intentionally

00:20:29,679 --> 00:20:33,520
very

00:20:30,159 --> 00:20:36,240
very narrow so that we can

00:20:33,520 --> 00:20:38,080
we can have very crisp expectations on

00:20:36,240 --> 00:20:39,200
the scalability requirements and stuff

00:20:38,080 --> 00:20:42,000
like that

00:20:39,200 --> 00:20:43,840
so this is one possible implementation

00:20:42,000 --> 00:20:47,919
of the resource matrix api

00:20:43,840 --> 00:20:48,640
and then we have another one next slide

00:20:47,919 --> 00:20:50,799
which is

00:20:48,640 --> 00:20:52,559
um actually something that we've we're

00:20:50,799 --> 00:20:53,679
just a project that we're just starting

00:20:52,559 --> 00:20:57,120
to adopt

00:20:53,679 --> 00:21:00,080
so as as of this recording uh

00:20:57,120 --> 00:21:00,559
the adoption of this project hasn't

00:21:00,080 --> 00:21:02,640
actually

00:21:00,559 --> 00:21:04,880
entirely gone through but i expect that

00:21:02,640 --> 00:21:06,159
over the next couple of weeks this will

00:21:04,880 --> 00:21:08,960
probably happen

00:21:06,159 --> 00:21:11,440
it has already been accepted by the

00:21:08,960 --> 00:21:14,159
basic instrumentation as a whole

00:21:11,440 --> 00:21:15,600
we just need to figure out some people

00:21:14,159 --> 00:21:18,960
assigning the

00:21:15,600 --> 00:21:22,159
cla but essentially what this is

00:21:18,960 --> 00:21:23,919
is much like the metric server is the

00:21:22,159 --> 00:21:25,200
default implementation of the resource

00:21:23,919 --> 00:21:27,120
metrics api

00:21:25,200 --> 00:21:29,679
the prometheus kubernetes prometheus

00:21:27,120 --> 00:21:31,679
adapter is essentially

00:21:29,679 --> 00:21:33,840
an implementation of the resource

00:21:31,679 --> 00:21:35,280
metrics api as well as the custom and

00:21:33,840 --> 00:21:38,240
external metrics apis

00:21:35,280 --> 00:21:41,200
which are also generic descriptions of

00:21:38,240 --> 00:21:41,200
metrix apis

00:21:42,000 --> 00:21:46,400
and this one as the name already says is

00:21:44,880 --> 00:21:49,679
backed by prometheus

00:21:46,400 --> 00:21:51,440
and why this is useful is because if you

00:21:49,679 --> 00:21:53,440
already have prometheus collecting these

00:21:51,440 --> 00:21:55,200
metrics anyways

00:21:53,440 --> 00:21:57,679
you might as well use prometheus to

00:21:55,200 --> 00:22:00,080
present these metrics to your users or

00:21:57,679 --> 00:22:02,400
use it for auto scaling purposes right

00:22:00,080 --> 00:22:04,720
um that way you don't have to have this

00:22:02,400 --> 00:22:08,159
extra process running in your cluster

00:22:04,720 --> 00:22:10,640
that uses uh memory and cpu

00:22:08,159 --> 00:22:12,159
to essentially collect the same things

00:22:10,640 --> 00:22:14,240
this is really only if you're

00:22:12,159 --> 00:22:16,480
already running prometheus anyways i

00:22:14,240 --> 00:22:18,640
would i would say that

00:22:16,480 --> 00:22:19,520
if you're not using prometheus this is

00:22:18,640 --> 00:22:21,360
probably uh

00:22:19,520 --> 00:22:23,200
too complicated of a setup just to get

00:22:21,360 --> 00:22:25,360
the resource metrics api

00:22:23,200 --> 00:22:26,960
in that case the metric server is

00:22:25,360 --> 00:22:28,640
probably the better choice but if

00:22:26,960 --> 00:22:30,320
prometheus is already your choice

00:22:28,640 --> 00:22:32,240
monitoring system of choice

00:22:30,320 --> 00:22:34,000
um i would highly recommend you giving

00:22:32,240 --> 00:22:36,880
this a try

00:22:34,000 --> 00:22:37,360
so that's all the sub project selections

00:22:36,880 --> 00:22:40,480
that

00:22:37,360 --> 00:22:43,760
we wanted to share today and now

00:22:40,480 --> 00:22:45,039
back to elena thanks so much frederick

00:22:43,760 --> 00:22:47,840
that was awesome

00:22:45,039 --> 00:22:49,120
uh so uh you've heard a bunch about our

00:22:47,840 --> 00:22:51,200
sig activities

00:22:49,120 --> 00:22:54,640
and uh what we're working on how can you

00:22:51,200 --> 00:22:56,799
get involved in that

00:22:54,640 --> 00:22:57,760
so uh first thing is if you're

00:22:56,799 --> 00:22:59,840
interested in getting

00:22:57,760 --> 00:23:02,080
involved attend our sig meetings that's

00:22:59,840 --> 00:23:03,600
the best way to get an idea of what's

00:23:02,080 --> 00:23:05,840
happening with the sig

00:23:03,600 --> 00:23:06,880
uh what sorts of things you can work on

00:23:05,840 --> 00:23:09,600
what sorts of

00:23:06,880 --> 00:23:10,400
projects are looking for contributors

00:23:09,600 --> 00:23:12,080
and

00:23:10,400 --> 00:23:13,280
get to know all the various people

00:23:12,080 --> 00:23:14,720
working on the various different

00:23:13,280 --> 00:23:17,200
components

00:23:14,720 --> 00:23:18,240
you can also start participating in

00:23:17,200 --> 00:23:21,360
reviews and

00:23:18,240 --> 00:23:22,080
issues and documentation all of those

00:23:21,360 --> 00:23:24,400
things were

00:23:22,080 --> 00:23:25,760
uh happy to accept new contributors and

00:23:24,400 --> 00:23:26,799
you don't need to ask for permission you

00:23:25,760 --> 00:23:29,760
can just jump in

00:23:26,799 --> 00:23:31,200
and we will take a look uh in terms of

00:23:29,760 --> 00:23:33,520
specific projects

00:23:31,200 --> 00:23:35,280
uh cube state metrics is explicitly

00:23:33,520 --> 00:23:36,000
seeking new contributors and you can

00:23:35,280 --> 00:23:38,000
reach out to

00:23:36,000 --> 00:23:39,200
lilly if you're interested in working on

00:23:38,000 --> 00:23:41,279
that

00:23:39,200 --> 00:23:42,640
both metric server and the structured

00:23:41,279 --> 00:23:45,039
logging implementation

00:23:42,640 --> 00:23:46,960
are seeking contributors and if you're

00:23:45,039 --> 00:23:47,679
interested in working on those you can

00:23:46,960 --> 00:23:51,440
contact

00:23:47,679 --> 00:23:52,480
merrick and prom q is also seeking new

00:23:51,440 --> 00:23:54,960
contributors

00:23:52,480 --> 00:23:57,039
so if you want to work on prom q which

00:23:54,960 --> 00:23:57,840
han introduced earlier you can reach out

00:23:57,039 --> 00:24:01,600
to him

00:23:57,840 --> 00:24:01,600
or sally or yuchen

00:24:02,559 --> 00:24:07,279
so how can you find us uh we have

00:24:05,120 --> 00:24:07,760
regular sig meetings effectively once a

00:24:07,279 --> 00:24:09,679
week

00:24:07,760 --> 00:24:12,159
we have two alternating bi-weekly

00:24:09,679 --> 00:24:16,080
meetings so our regular sig meeting

00:24:12,159 --> 00:24:18,240
is on thursdays at 9 30 a.m pacific time

00:24:16,080 --> 00:24:20,480
uh and that alternates every other week

00:24:18,240 --> 00:24:22,400
with a triage meeting where we go over

00:24:20,480 --> 00:24:24,720
our pr and issue backlog

00:24:22,400 --> 00:24:26,320
and those are on wednesdays at 9 00 a.m

00:24:24,720 --> 00:24:29,200
pacific time

00:24:26,320 --> 00:24:30,000
if you want to find more information or

00:24:29,200 --> 00:24:33,120
reach out to

00:24:30,000 --> 00:24:34,559
various folks in the sieg you can visit

00:24:33,120 --> 00:24:37,440
our slack channel

00:24:34,559 --> 00:24:39,200
pound sig instrumentation you can join

00:24:37,440 --> 00:24:41,120
our mailing list which is a google group

00:24:39,200 --> 00:24:43,679
which will also give you right access to

00:24:41,120 --> 00:24:46,240
the meeting agendas linked above

00:24:43,679 --> 00:24:47,039
and if you need to know who's in charge

00:24:46,240 --> 00:24:48,720
of what

00:24:47,039 --> 00:24:50,960
just repeating again the chairs of the

00:24:48,720 --> 00:24:53,120
sig are myself and han

00:24:50,960 --> 00:24:55,520
and the tech leads are frederick and

00:24:53,120 --> 00:24:55,520
david

00:24:55,919 --> 00:24:59,200
the last thing that i wanted to mention

00:24:57,600 --> 00:25:01,440
before we close out the talk is some

00:24:59,200 --> 00:25:02,720
other relevant talks from this cubecon

00:25:01,440 --> 00:25:04,000
that we want to give some brief

00:25:02,720 --> 00:25:05,679
shoutouts to

00:25:04,000 --> 00:25:07,200
that go into a little bit more depth in

00:25:05,679 --> 00:25:07,840
terms of some of the things we cover

00:25:07,200 --> 00:25:10,240
today

00:25:07,840 --> 00:25:12,240
there is an entire talk discussing the

00:25:10,240 --> 00:25:14,400
structured logging implementation in

00:25:12,240 --> 00:25:16,400
kubernetes 119

00:25:14,400 --> 00:25:17,440
so highly recommend you give a look at

00:25:16,400 --> 00:25:19,120
that recording

00:25:17,440 --> 00:25:20,960
if you're interested in what's going on

00:25:19,120 --> 00:25:22,960
with structured logging that's been at

00:25:20,960 --> 00:25:23,919
least a two year long effort so i'm very

00:25:22,960 --> 00:25:26,840
excited to see

00:25:23,919 --> 00:25:28,640
uh that land and hopefully soon become

00:25:26,840 --> 00:25:31,760
ga and as well

00:25:28,640 --> 00:25:33,919
the cncf sig observability intro

00:25:31,760 --> 00:25:36,080
and deep dive talk is scheduled i think

00:25:33,919 --> 00:25:37,679
at the same time as this talk

00:25:36,080 --> 00:25:39,279
so if you're watching this one i

00:25:37,679 --> 00:25:40,000
recommend you take a look at that if you

00:25:39,279 --> 00:25:41,919
want to

00:25:40,000 --> 00:25:43,200
look at observability things in the

00:25:41,919 --> 00:25:46,799
wider cloud native

00:25:43,200 --> 00:25:48,640
ecosystem there are of course many other

00:25:46,799 --> 00:25:50,480
related talks i don't want to talk about

00:25:48,640 --> 00:25:52,000
every single talk at kubecon

00:25:50,480 --> 00:25:54,640
but you can check out both the

00:25:52,000 --> 00:25:56,559
observability and maintainer tracks

00:25:54,640 --> 00:25:59,840
for more talks on kubernetes

00:25:56,559 --> 00:26:02,240
observability and instrumentation

00:25:59,840 --> 00:26:05,840
and thanks so much for joining us i hope

00:26:02,240 --> 00:26:05,840

YouTube URL: https://www.youtube.com/watch?v=NzoG--2UqEk


