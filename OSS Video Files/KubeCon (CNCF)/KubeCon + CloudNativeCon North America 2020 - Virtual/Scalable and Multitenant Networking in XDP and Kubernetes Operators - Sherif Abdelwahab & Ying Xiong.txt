Title: Scalable and Multitenant Networking in XDP and Kubernetes Operators - Sherif Abdelwahab & Ying Xiong
Publication date: 2020-11-23
Playlist: KubeCon + CloudNativeCon North America 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon Europe 2021 Virtual from May 4–7, 2021. Learn more at https://kubecon.io. The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects. 

Scalable and Multitenant Networking in XDP and Kubernetes Operators - Sherif Abdelwahab & Ying Xiong, Futurewei 

Enabling multi-tenant and extensible networking in Kubernetes is of paramount importance for cloud providers requires traffic isolation across tenants. By introducing a new data-plane built using XDP and inter-working custom resource operators, we will discuss how to enable multi-tenant networking at scale without compromising packet processing performance. Sherif and Ying will discuss the use of cases of networking isolation and fast provisioning within the Kubernetes environment for large scale cloud. They will share their experience and design techniques building a networking solution for multi-tenant networks from the ground-up using XDP, CRDs, and Kubernetes Operators. They will conclude with a demo. 

https://sched.co/ekCi
Captions: 
	00:00:02,240 --> 00:00:06,560
hello everyone

00:00:03,280 --> 00:00:08,880
welcome to coop count 2020 and welcome

00:00:06,560 --> 00:00:10,559
to the session of multitendon networking

00:00:08,880 --> 00:00:12,960
for kubernetes

00:00:10,559 --> 00:00:14,400
my name is incheon i'm with virtual

00:00:12,960 --> 00:00:17,359
technologies

00:00:14,400 --> 00:00:17,680
in this talk today giraffe who is now

00:00:17,359 --> 00:00:20,640
with

00:00:17,680 --> 00:00:23,199
microsoft he and i will present a

00:00:20,640 --> 00:00:26,800
multi-tenant scalable network solution

00:00:23,199 --> 00:00:27,840
for kubernetes we hope you will enjoy

00:00:26,800 --> 00:00:30,160
the talk

00:00:27,840 --> 00:00:31,519
and and the end of the session hopefully

00:00:30,160 --> 00:00:33,760
you'll learn something

00:00:31,519 --> 00:00:34,559
and interested in contributing to this

00:00:33,760 --> 00:00:38,880
project

00:00:34,559 --> 00:00:38,880
or building your own network solutions

00:00:38,960 --> 00:00:44,399
with that let's get started this is the

00:00:42,239 --> 00:00:46,719
agenda for today's talk

00:00:44,399 --> 00:00:48,399
i will give a introduction and some

00:00:46,719 --> 00:00:50,399
background on the project

00:00:48,399 --> 00:00:51,760
why we are doing this and what's the

00:00:50,399 --> 00:00:53,600
context

00:00:51,760 --> 00:00:55,760
i will also talking about current

00:00:53,600 --> 00:00:58,320
network model in kubernetes

00:00:55,760 --> 00:00:59,280
and introduce you the high level design

00:00:58,320 --> 00:01:02,320
of our new

00:00:59,280 --> 00:01:04,640
multi-tenant model for kubernetes

00:01:02,320 --> 00:01:06,159
we have a forked version of kubernetes

00:01:04,640 --> 00:01:08,000
called actors

00:01:06,159 --> 00:01:09,439
you can consider actors as a

00:01:08,000 --> 00:01:12,479
multi-tenant

00:01:09,439 --> 00:01:14,640
kubernetes i will introduce actors more

00:01:12,479 --> 00:01:17,520
in next slides

00:01:14,640 --> 00:01:17,840
after that sheriff will give you how

00:01:17,520 --> 00:01:19,840
will

00:01:17,840 --> 00:01:21,040
implement the multi-tender network model

00:01:19,840 --> 00:01:24,400
in kubernetes

00:01:21,040 --> 00:01:26,000
and introduce you the misa project misa

00:01:24,400 --> 00:01:28,880
is a virtual network solution

00:01:26,000 --> 00:01:29,360
that has both control plane and a data

00:01:28,880 --> 00:01:33,439
plane

00:01:29,360 --> 00:01:33,439
based on the xdp technology

00:01:34,240 --> 00:01:38,159
so misa project is a part of an umbrella

00:01:37,280 --> 00:01:41,600
project called

00:01:38,159 --> 00:01:44,320
centaurus centerra's project includes

00:01:41,600 --> 00:01:44,880
two independent projects one is called

00:01:44,320 --> 00:01:48,560
actors

00:01:44,880 --> 00:01:49,520
one is called visa misa again is what we

00:01:48,560 --> 00:01:52,799
discussed today

00:01:49,520 --> 00:01:54,320
in this talk however i'd like to briefly

00:01:52,799 --> 00:01:57,200
introduce actress project

00:01:54,320 --> 00:01:58,719
first so that you understand some of the

00:01:57,200 --> 00:02:01,759
contents

00:01:58,719 --> 00:02:02,079
as i mentioned earlier actors project is

00:02:01,759 --> 00:02:04,079
a

00:02:02,079 --> 00:02:06,640
focused version of kubernetes with a

00:02:04,079 --> 00:02:10,239
major change in design

00:02:06,640 --> 00:02:13,120
there are three goals for actors project

00:02:10,239 --> 00:02:13,920
first we want to unify vmware campaign

00:02:13,120 --> 00:02:16,720
orchestration

00:02:13,920 --> 00:02:18,480
and the runtime for that we explain

00:02:16,720 --> 00:02:20,640
product path definition

00:02:18,480 --> 00:02:21,760
and then make the runtime agent which is

00:02:20,640 --> 00:02:24,319
kubernetes

00:02:21,760 --> 00:02:26,720
to be a unified agent that supports both

00:02:24,319 --> 00:02:29,760
vm and containers

00:02:26,720 --> 00:02:34,239
second we want kubernetes to support

00:02:29,760 --> 00:02:36,959
from 50k to 100k nodes in a cluster

00:02:34,239 --> 00:02:37,760
for that we have to partition kubernetes

00:02:36,959 --> 00:02:40,160
components

00:02:37,760 --> 00:02:40,959
such as api server schedulers

00:02:40,160 --> 00:02:44,879
controllers and

00:02:40,959 --> 00:02:48,239
etcd third we want to build a true

00:02:44,879 --> 00:02:50,720
multi-tenant platform so we design a

00:02:48,239 --> 00:02:53,840
multi-tenant solution for kubernetes

00:02:50,720 --> 00:02:57,360
including a new tendon object

00:02:53,840 --> 00:02:57,680
and a new network model so we believe

00:02:57,360 --> 00:03:01,200
with

00:02:57,680 --> 00:03:03,680
those three goals octos takes

00:03:01,200 --> 00:03:04,239
kubernetes to the next level make it a

00:03:03,680 --> 00:03:07,599
true

00:03:04,239 --> 00:03:07,599
cloud infrastructure platform

00:03:08,480 --> 00:03:13,519
so back to the main topic we have today

00:03:12,000 --> 00:03:15,440
which is mesa

00:03:13,519 --> 00:03:19,280
meteor network solution is trying to

00:03:15,440 --> 00:03:21,840
address the following problems

00:03:19,280 --> 00:03:23,760
and first we want to provide a virtual

00:03:21,840 --> 00:03:26,400
network solution for kubernetes

00:03:23,760 --> 00:03:27,599
so that kubernetes apart that from

00:03:26,400 --> 00:03:30,000
different tendons

00:03:27,599 --> 00:03:31,519
that will reside in a different virtual

00:03:30,000 --> 00:03:34,159
network

00:03:31,519 --> 00:03:36,159
second we want to address the problem of

00:03:34,159 --> 00:03:38,799
fast provisioning

00:03:36,159 --> 00:03:39,519
of network resources for parts it's

00:03:38,799 --> 00:03:42,080
basically

00:03:39,519 --> 00:03:42,959
how to get a pilot ready as quick as

00:03:42,080 --> 00:03:46,000
possible

00:03:42,959 --> 00:03:48,319
from network perspective third

00:03:46,000 --> 00:03:49,440
we want to have a scalable virtual

00:03:48,319 --> 00:03:52,159
network solution

00:03:49,440 --> 00:03:53,200
to support the networking within a

00:03:52,159 --> 00:03:57,040
cluster of

00:03:53,200 --> 00:03:58,840
more than 100k hosts and that's the

00:03:57,040 --> 00:04:01,840
problems we try to solve from misa

00:03:58,840 --> 00:04:01,840
perspective

00:04:02,480 --> 00:04:08,319
as most of most of you already know that

00:04:05,920 --> 00:04:09,599
current network model for in kubernetes

00:04:08,319 --> 00:04:11,840
is a flat model

00:04:09,599 --> 00:04:13,760
it's a single address space and they

00:04:11,840 --> 00:04:16,959
share the single dnas

00:04:13,760 --> 00:04:18,239
and by default every part or containers

00:04:16,959 --> 00:04:20,560
can communicate

00:04:18,239 --> 00:04:22,000
with every other part of containers in

00:04:20,560 --> 00:04:24,639
the clusters

00:04:22,000 --> 00:04:27,600
so by default there is no water tendon

00:04:24,639 --> 00:04:30,160
for network perspective

00:04:27,600 --> 00:04:30,880
kubernetes introduce network policy to

00:04:30,160 --> 00:04:34,240
isolate

00:04:30,880 --> 00:04:37,440
container apart each other however

00:04:34,240 --> 00:04:39,759
network policy is not secure or as not

00:04:37,440 --> 00:04:40,639
strong isolated as virtual network can

00:04:39,759 --> 00:04:43,440
do

00:04:40,639 --> 00:04:44,400
for example network policy does not

00:04:43,440 --> 00:04:47,120
prevent

00:04:44,400 --> 00:04:48,560
packet sleeping put in somewhere where

00:04:47,120 --> 00:04:50,960
the traffic is passing by

00:04:48,560 --> 00:04:52,160
and extract the information intensity of

00:04:50,960 --> 00:04:55,280
the information or data

00:04:52,160 --> 00:04:57,840
out of the packet additionally some of

00:04:55,280 --> 00:04:59,520
network policies are implemented based

00:04:57,840 --> 00:05:02,560
on unix feature

00:04:59,520 --> 00:05:04,000
called net filter which uses ip table

00:05:02,560 --> 00:05:06,720
rules

00:05:04,000 --> 00:05:07,680
in reality epi table rules could get

00:05:06,720 --> 00:05:09,840
huge

00:05:07,680 --> 00:05:11,039
and then cause overhead and increased

00:05:09,840 --> 00:05:14,080
network of network

00:05:11,039 --> 00:05:19,759
latency not a security issue but

00:05:14,080 --> 00:05:19,759
is a not desirable solutions we want

00:05:22,560 --> 00:05:26,720
so in actors we introduce a network

00:05:25,759 --> 00:05:30,400
object

00:05:26,720 --> 00:05:33,759
a new crd object that represents a vpc

00:05:30,400 --> 00:05:35,680
or subnet each part to be created

00:05:33,759 --> 00:05:38,400
has to be associated with a network

00:05:35,680 --> 00:05:41,520
object and each never object

00:05:38,400 --> 00:05:43,520
has its own ip address space so

00:05:41,520 --> 00:05:45,120
paths or containers created in a

00:05:43,520 --> 00:05:47,520
different network

00:05:45,120 --> 00:05:48,800
are naturally isolated with the network

00:05:47,520 --> 00:05:51,919
boundary

00:05:48,800 --> 00:05:53,360
as you can see from this diagram under

00:05:51,919 --> 00:05:56,479
slice

00:05:53,360 --> 00:05:58,800
now within each network you can still

00:05:56,479 --> 00:06:00,000
use network policies to manage network

00:05:58,800 --> 00:06:03,919
security

00:06:00,000 --> 00:06:06,080
within a single attendant

00:06:03,919 --> 00:06:07,440
the new navigator objects will introduce

00:06:06,080 --> 00:06:10,479
is the abstraction

00:06:07,440 --> 00:06:12,319
of network resources but not actual

00:06:10,479 --> 00:06:14,960
implementation

00:06:12,319 --> 00:06:17,120
someone still needs to actually create

00:06:14,960 --> 00:06:19,759
vpcs create a subnet

00:06:17,120 --> 00:06:22,000
and manage the api address spaces and

00:06:19,759 --> 00:06:25,840
the road network traffic

00:06:22,000 --> 00:06:27,600
this is where misa comes into play

00:06:25,840 --> 00:06:29,919
misa is one of the network

00:06:27,600 --> 00:06:30,400
implementation for multi-tenant network

00:06:29,919 --> 00:06:34,160
model

00:06:30,400 --> 00:06:37,840
in kubernetes so now i will pass

00:06:34,160 --> 00:06:38,160
on to cherev he will present the detail

00:06:37,840 --> 00:06:43,120
of

00:06:38,160 --> 00:06:43,120
misa design and implementation thank you

00:06:43,840 --> 00:06:48,880
hi everyone this is sharif i led the

00:06:47,120 --> 00:06:50,479
development of the mizar project

00:06:48,880 --> 00:06:52,319
and i am now a software engineer with

00:06:50,479 --> 00:06:54,720
microsoft

00:06:52,319 --> 00:06:56,880
we built mizor from the ground up to

00:06:54,720 --> 00:06:58,400
accelerate pulse network provisioning at

00:06:56,880 --> 00:07:01,440
scale

00:06:58,400 --> 00:07:02,080
actually to rethink cloud networking in

00:07:01,440 --> 00:07:05,280
missouri

00:07:02,080 --> 00:07:06,080
all together to build it in the exact

00:07:05,280 --> 00:07:08,240
same way

00:07:06,080 --> 00:07:10,639
as we build distributed systems in the

00:07:08,240 --> 00:07:11,440
cloud to make cloud networks simple to

00:07:10,639 --> 00:07:14,960
understand

00:07:11,440 --> 00:07:16,000
and simpler to operate in the rest of

00:07:14,960 --> 00:07:17,680
this talk

00:07:16,000 --> 00:07:20,240
i will walk you through our thought

00:07:17,680 --> 00:07:22,720
process and how mazar works

00:07:20,240 --> 00:07:24,000
from a high level missouri consists of a

00:07:22,720 --> 00:07:27,199
crd operators

00:07:24,000 --> 00:07:27,680
a demon and a cni the operators the

00:07:27,199 --> 00:07:30,319
demon

00:07:27,680 --> 00:07:32,319
and the cni are mazar's management plan

00:07:30,319 --> 00:07:35,520
components

00:07:32,319 --> 00:07:36,319
the demon exposes a grpc interface for

00:07:35,520 --> 00:07:40,000
the operator

00:07:36,319 --> 00:07:42,639
and the c9 we eliminate any api from the

00:07:40,000 --> 00:07:44,960
worker nodes to the api server

00:07:42,639 --> 00:07:45,840
this prevents operators failures to

00:07:44,960 --> 00:07:49,919
amplify

00:07:45,840 --> 00:07:49,919
as we add more workers to the cluster

00:07:51,199 --> 00:07:55,520
on the met on the data plan side mizar

00:07:53,840 --> 00:07:58,639
consists of a set of multiple

00:07:55,520 --> 00:08:01,039
xdp programs the processed nodes packets

00:07:58,639 --> 00:08:04,080
i will detail exactly how the xdb

00:08:01,039 --> 00:08:06,479
program processes the packets later

00:08:04,080 --> 00:08:08,080
in this architecture we rethink the data

00:08:06,479 --> 00:08:10,560
plan programming model

00:08:08,080 --> 00:08:11,759
to scale the management plan accelerate

00:08:10,560 --> 00:08:13,840
pub provisioning

00:08:11,759 --> 00:08:15,039
and develop customized logic for network

00:08:13,840 --> 00:08:18,479
services

00:08:15,039 --> 00:08:20,560
as a result in mazar enables scalable

00:08:18,479 --> 00:08:23,840
and multi-tenant

00:08:20,560 --> 00:08:23,840
kubernetes networking

00:08:24,400 --> 00:08:28,479
before i detail how misar works i would

00:08:26,879 --> 00:08:31,039
like to discuss the limitations of

00:08:28,479 --> 00:08:33,039
flow-based network programming

00:08:31,039 --> 00:08:34,479
load-based network programming is the de

00:08:33,039 --> 00:08:36,320
facto programming model

00:08:34,479 --> 00:08:37,839
in virtual switches including open

00:08:36,320 --> 00:08:41,680
v-switch

00:08:37,839 --> 00:08:44,959
i will take ovn and ovs as an example

00:08:41,680 --> 00:08:45,920
ovn uses the concept of logical ports to

00:08:44,959 --> 00:08:49,360
create a large

00:08:45,920 --> 00:08:52,480
logical switch that spans multiple hosts

00:08:49,360 --> 00:08:55,440
with this model creating 10 000 logical

00:08:52,480 --> 00:08:57,040
ports generates more than 40 000 support

00:08:55,440 --> 00:08:58,959
bindings

00:08:57,040 --> 00:09:00,880
the logical switch approach does not

00:08:58,959 --> 00:09:01,839
scale as we increase the number of

00:09:00,880 --> 00:09:05,200
worker nodes

00:09:01,839 --> 00:09:07,040
of a cluster moreover

00:09:05,200 --> 00:09:08,560
during flow programming it's not

00:09:07,040 --> 00:09:10,880
uncommon to observe an

00:09:08,560 --> 00:09:13,279
increase in the cpu utilization during

00:09:10,880 --> 00:09:15,200
flow parsing

00:09:13,279 --> 00:09:17,360
with the logical switch architecture the

00:09:15,200 --> 00:09:18,640
time to provision network resources for

00:09:17,360 --> 00:09:20,720
each new container

00:09:18,640 --> 00:09:22,560
depends on the number of containers that

00:09:20,720 --> 00:09:24,560
already exist in the system and the

00:09:22,560 --> 00:09:26,959
number of worker nodes of the cluster

00:09:24,560 --> 00:09:27,680
so clearly the logical switch approach

00:09:26,959 --> 00:09:30,240
restricts

00:09:27,680 --> 00:09:32,080
scale and is not suitable for dynamic

00:09:30,240 --> 00:09:33,360
cloud applications that have a short

00:09:32,080 --> 00:09:36,720
lifetime span

00:09:33,360 --> 00:09:38,560
like serverless

00:09:36,720 --> 00:09:39,920
with the limitations of flow programming

00:09:38,560 --> 00:09:41,920
model in mind

00:09:39,920 --> 00:09:43,440
we redesigned the host networking in

00:09:41,920 --> 00:09:47,040
mizor to interconnect

00:09:43,440 --> 00:09:49,839
containers only with xdb programs

00:09:47,040 --> 00:09:51,920
and to do this we attach an xdp program

00:09:49,839 --> 00:09:52,959
on each physical interface of a worker

00:09:51,920 --> 00:09:56,720
node

00:09:52,959 --> 00:09:58,880
we name this program transit xdp

00:09:56,720 --> 00:10:02,160
this program processes all english

00:09:58,880 --> 00:10:04,959
packets to the worker nodes

00:10:02,160 --> 00:10:05,360
we also attach another xtp program on

00:10:04,959 --> 00:10:07,760
the v

00:10:05,360 --> 00:10:10,079
if peer connecting a container in the

00:10:07,760 --> 00:10:13,519
root namespace

00:10:10,079 --> 00:10:16,079
we call this program the transit agent

00:10:13,519 --> 00:10:17,279
the transit agent attaches to the v if

00:10:16,079 --> 00:10:19,279
pier to process

00:10:17,279 --> 00:10:21,200
all the egress traffic from each

00:10:19,279 --> 00:10:23,760
container

00:10:21,200 --> 00:10:25,120
from a management plan perspective all

00:10:23,760 --> 00:10:28,399
what we needed to do next

00:10:25,120 --> 00:10:32,240
is to expose the eppf user space api

00:10:28,399 --> 00:10:35,200
as grpc interfaces the operator

00:10:32,240 --> 00:10:39,279
programs logical functions of the xdb

00:10:35,200 --> 00:10:41,519
program through these rpc interfaces

00:10:39,279 --> 00:10:42,560
to understand the rule of virtual

00:10:41,519 --> 00:10:44,720
function

00:10:42,560 --> 00:10:46,160
we need to look into the new network

00:10:44,720 --> 00:10:49,279
organization of kubernetes

00:10:46,160 --> 00:10:51,200
that mizar enables we extended

00:10:49,279 --> 00:10:53,760
kubernetes with two resources that we

00:10:51,200 --> 00:10:54,800
typically find in any multicannon cloud

00:10:53,760 --> 00:10:57,760
system

00:10:54,800 --> 00:11:00,160
virtual private clouds vpcs and subnets

00:10:57,760 --> 00:11:02,160
within the vpcs

00:11:00,160 --> 00:11:04,560
creating vpcs and subnets is

00:11:02,160 --> 00:11:06,160
straightforward in kubernetes with crds

00:11:04,560 --> 00:11:08,240
and operators

00:11:06,160 --> 00:11:09,440
on the data plan we introduced new

00:11:08,240 --> 00:11:12,560
logical functions

00:11:09,440 --> 00:11:14,560
within the xdb programs the first

00:11:12,560 --> 00:11:18,240
logical function is bouncers

00:11:14,560 --> 00:11:21,760
within the network scope and the second

00:11:18,240 --> 00:11:24,240
are dividers within the vpc scope

00:11:21,760 --> 00:11:26,480
unlike logical routers or switches the

00:11:24,240 --> 00:11:29,040
bouncers and dividers are in-network

00:11:26,480 --> 00:11:31,279
distributed hash tables

00:11:29,040 --> 00:11:34,000
and i will detail exactly how they work

00:11:31,279 --> 00:11:36,320
in the next few slides

00:11:34,000 --> 00:11:38,640
bouncers and dividers are the logical

00:11:36,320 --> 00:11:40,839
functions that make up the vpcs

00:11:38,640 --> 00:11:42,560
to isolate pulse traffic for

00:11:40,839 --> 00:11:44,640
multi-tenant and allow

00:11:42,560 --> 00:11:47,279
tenants to reuse the same network

00:11:44,640 --> 00:11:47,279
address space

00:11:50,000 --> 00:11:55,440
the user creates a vpc like any object

00:11:52,720 --> 00:11:57,839
in kubernetes as a yaml file

00:11:55,440 --> 00:11:58,480
the user specifies the side range of the

00:11:57,839 --> 00:12:01,600
vpc

00:11:58,480 --> 00:12:03,519
and the number of vpc dividers

00:12:01,600 --> 00:12:05,440
because the divider is a distributed

00:12:03,519 --> 00:12:07,360
hash table we can provide

00:12:05,440 --> 00:12:11,839
any number of dividers in the object

00:12:07,360 --> 00:12:14,399
definition with one being the default

00:12:11,839 --> 00:12:17,040
when the vpc operator receives the vpc

00:12:14,399 --> 00:12:20,000
object it schedules the vpc divider on

00:12:17,040 --> 00:12:21,839
one of the worker nodes of the cluster

00:12:20,000 --> 00:12:23,279
scheduling the divider does not mean

00:12:21,839 --> 00:12:25,920
that the operator runs

00:12:23,279 --> 00:12:27,200
any new code on the node all what the

00:12:25,920 --> 00:12:29,279
operator does

00:12:27,200 --> 00:12:32,480
is labeling the selected host as a

00:12:29,279 --> 00:12:35,120
divider for the vpc

00:12:32,480 --> 00:12:35,600
the kubernetes operator also assigns a

00:12:35,120 --> 00:12:38,160
unique

00:12:35,600 --> 00:12:39,279
identifier for the vpc that the data

00:12:38,160 --> 00:12:42,000
plan uses

00:12:39,279 --> 00:12:45,600
to separate traffic which is known as

00:12:42,000 --> 00:12:45,600
the virtual network identifier

00:12:47,680 --> 00:12:51,600
after creating a vpc the user now

00:12:50,480 --> 00:12:55,200
creates a subnet

00:12:51,600 --> 00:12:57,440
without within the vpc we also provide

00:12:55,200 --> 00:12:58,399
any number of pouncers to create for

00:12:57,440 --> 00:13:01,279
each network

00:12:58,399 --> 00:13:03,360
with one being the default when the

00:13:01,279 --> 00:13:04,079
network operator receives the subnet

00:13:03,360 --> 00:13:06,160
object

00:13:04,079 --> 00:13:08,800
it schedules the bouncers on some of the

00:13:06,160 --> 00:13:10,800
worker nodes of the cluster

00:13:08,800 --> 00:13:12,399
scheduling the bouncer involves two

00:13:10,800 --> 00:13:14,720
actions first

00:13:12,399 --> 00:13:16,560
the operator labels the host as bouncers

00:13:14,720 --> 00:13:19,120
on the management plan

00:13:16,560 --> 00:13:22,000
second it programs the divider's worker

00:13:19,120 --> 00:13:24,720
node through rpc calls

00:13:22,000 --> 00:13:26,320
the rpc call simply populates an eppf

00:13:24,720 --> 00:13:28,800
map in the transit xdb

00:13:26,320 --> 00:13:30,160
program within the network within the

00:13:28,800 --> 00:13:32,480
vpc

00:13:30,160 --> 00:13:34,880
and the ip addresses of the host that

00:13:32,480 --> 00:13:38,240
are bouncers to these networks

00:13:34,880 --> 00:13:41,600
in this example net1 has a bouncer 1

00:13:38,240 --> 00:13:42,079
and net 2 has a bouncer 2 and both of

00:13:41,600 --> 00:13:44,560
them

00:13:42,079 --> 00:13:47,360
are populated in the eppf map of the

00:13:44,560 --> 00:13:47,360
divider host

00:13:47,519 --> 00:13:51,600
now comes the interesting part where the

00:13:49,839 --> 00:13:52,399
user creates a pod within a

00:13:51,600 --> 00:13:55,199
multi-network

00:13:52,399 --> 00:13:56,720
a multi-tenant network natively in

00:13:55,199 --> 00:14:00,160
kubernetes and similar to what you

00:13:56,720 --> 00:14:02,720
typically find in any cloud system

00:14:00,160 --> 00:14:05,040
to do that we use annotations of the

00:14:02,720 --> 00:14:08,079
pulled object to specify the vpc

00:14:05,040 --> 00:14:08,800
and the subnet of the pod arctos

00:14:08,079 --> 00:14:11,600
controller

00:14:08,800 --> 00:14:14,720
adds the network and neck annotations

00:14:11,600 --> 00:14:16,480
that i'm showing in these slides

00:14:14,720 --> 00:14:18,160
the missile operator uses these

00:14:16,480 --> 00:14:20,160
annotations to

00:14:18,160 --> 00:14:22,880
provision the pod within the requested

00:14:20,160 --> 00:14:24,320
vpc and network boundary

00:14:22,880 --> 00:14:26,720
the missouri operator provisioned the

00:14:24,320 --> 00:14:28,160
network resources with a constant number

00:14:26,720 --> 00:14:31,680
of rpc calls

00:14:28,160 --> 00:14:32,880
typically too the number of rpc calls

00:14:31,680 --> 00:14:34,639
does not depend

00:14:32,880 --> 00:14:37,680
on the number of worker nodes in the

00:14:34,639 --> 00:14:39,120
cluster or the pulse already provisioned

00:14:37,680 --> 00:14:42,160
this is what allows the network

00:14:39,120 --> 00:14:44,079
provisioning to scale

00:14:42,160 --> 00:14:46,320
the operator makes one call to the

00:14:44,079 --> 00:14:48,720
bouncer host of the subnet

00:14:46,320 --> 00:14:50,320
and this call effectively adds an entry

00:14:48,720 --> 00:14:52,560
in one ebpf map

00:14:50,320 --> 00:14:54,320
with the ip address of the node hosting

00:14:52,560 --> 00:14:56,880
the pod

00:14:54,320 --> 00:14:58,320
the other code provisions the vif peer

00:14:56,880 --> 00:15:01,440
interface for the pod

00:14:58,320 --> 00:15:04,560
and make it ready for the c9 to consume

00:15:01,440 --> 00:15:05,519
internally when the cni adds the network

00:15:04,560 --> 00:15:07,519
interface

00:15:05,519 --> 00:15:10,480
it makes a local call to the misar

00:15:07,519 --> 00:15:12,959
daemon to consume that interface

00:15:10,480 --> 00:15:14,880
this design significantly simplifies the

00:15:12,959 --> 00:15:17,440
cni

00:15:14,880 --> 00:15:18,240
when the cni adds an interface it only

00:15:17,440 --> 00:15:20,560
consumes

00:15:18,240 --> 00:15:23,440
the interface that is already created by

00:15:20,560 --> 00:15:23,440
the missouri demon

00:15:24,240 --> 00:15:28,560
the effect of this provisioning workflow

00:15:26,639 --> 00:15:30,320
is significantly better scale

00:15:28,560 --> 00:15:33,680
and significantly better time to

00:15:30,320 --> 00:15:35,759
provision networking for the pub

00:15:33,680 --> 00:15:37,759
the time to provision network resources

00:15:35,759 --> 00:15:40,000
for the pod is now constant

00:15:37,759 --> 00:15:41,759
and independent on the number of worker

00:15:40,000 --> 00:15:43,519
nodes of the cluster

00:15:41,759 --> 00:15:45,600
or even the number of pods already

00:15:43,519 --> 00:15:48,079
provisioned in the cluster

00:15:45,600 --> 00:15:50,000
compare this to obvious which does not

00:15:48,079 --> 00:15:52,000
scale well as we add more nodes in the

00:15:50,000 --> 00:15:55,120
cluster or if the number of poles

00:15:52,000 --> 00:15:57,120
existing already increases

00:15:55,120 --> 00:15:58,720
up until this point i describe the

00:15:57,120 --> 00:16:00,800
management plan operations

00:15:58,720 --> 00:16:02,079
and in the next few slides i will

00:16:00,800 --> 00:16:04,639
describe in detail

00:16:02,079 --> 00:16:07,040
how the xdb program on holds process

00:16:04,639 --> 00:16:07,040
packets

00:16:07,440 --> 00:16:12,800
consider the case in which a pod with ip

00:16:10,000 --> 00:16:15,040
address 1001 on host a

00:16:12,800 --> 00:16:16,720
sends a packet to a pod with an ip

00:16:15,040 --> 00:16:20,079
address 1002

00:16:16,720 --> 00:16:22,560
on host b an xdp program

00:16:20,079 --> 00:16:23,120
intercepts the outgoing packet from the

00:16:22,560 --> 00:16:25,759
pod

00:16:23,120 --> 00:16:27,759
when the vehicle receives it in the root

00:16:25,759 --> 00:16:30,480
name space

00:16:27,759 --> 00:16:32,800
the xdp program simply looks up a static

00:16:30,480 --> 00:16:34,800
configuration in an eppf map

00:16:32,800 --> 00:16:36,639
and encapsulates the packet into a

00:16:34,800 --> 00:16:38,959
geneve packet

00:16:36,639 --> 00:16:40,560
it also assigns the virtual network

00:16:38,959 --> 00:16:43,920
identifier of the vpc

00:16:40,560 --> 00:16:46,240
in the geneve header several tenants

00:16:43,920 --> 00:16:48,480
still use the same address space of the

00:16:46,240 --> 00:16:52,240
vpc where the network distinguishes

00:16:48,480 --> 00:16:54,079
traffic within a vpc by the v9i field

00:16:52,240 --> 00:16:56,800
the only information available to the

00:16:54,079 --> 00:16:58,959
transit agent at this stage is the ip

00:16:56,800 --> 00:17:01,920
address of the bouncer

00:16:58,959 --> 00:17:04,079
so it sends the packets to the bouncer

00:17:01,920 --> 00:17:06,959
by redirecting it for transmission

00:17:04,079 --> 00:17:08,959
on the physical interface when the

00:17:06,959 --> 00:17:11,520
bouncer receives the packet

00:17:08,959 --> 00:17:14,160
that an xdp program is first to process

00:17:11,520 --> 00:17:16,240
it on the bouncer host

00:17:14,160 --> 00:17:19,439
the xdb program looks up the inner

00:17:16,240 --> 00:17:22,240
destination address of an eppf map

00:17:19,439 --> 00:17:23,839
then it rewrite the outer destination ip

00:17:22,240 --> 00:17:25,600
address to host c

00:17:23,839 --> 00:17:29,200
which is the worker node running the

00:17:25,600 --> 00:17:31,440
destination pod 1002

00:17:29,200 --> 00:17:34,000
when the packet arrives at host c the

00:17:31,440 --> 00:17:36,559
xdp program decapsulates the packet

00:17:34,000 --> 00:17:38,559
and redirected to the vehicle of the pod

00:17:36,559 --> 00:17:40,880
to receive it

00:17:38,559 --> 00:17:41,919
this approach greatly simplifies the

00:17:40,880 --> 00:17:44,720
prop provisioning

00:17:41,919 --> 00:17:46,160
but it has a serious drawback all the

00:17:44,720 --> 00:17:50,080
packets now traverse

00:17:46,160 --> 00:17:52,160
one extra hop to reach their destination

00:17:50,080 --> 00:17:55,200
i will now describe how we solve this

00:17:52,160 --> 00:17:55,200
entirely in xdp

00:17:55,679 --> 00:17:59,120
overcome the extra hub problem we

00:17:57,760 --> 00:18:01,360
modified the xdb

00:17:59,120 --> 00:18:02,559
program running on the bouncer host to

00:18:01,360 --> 00:18:05,520
respond to arp

00:18:02,559 --> 00:18:07,600
queries since we already have the pulse

00:18:05,520 --> 00:18:08,640
ip and mac address configured by the

00:18:07,600 --> 00:18:10,480
missouri operator

00:18:08,640 --> 00:18:12,240
when we provision the pulse network so

00:18:10,480 --> 00:18:15,520
it makes sense to respond to our

00:18:12,240 --> 00:18:17,679
queries at this stage

00:18:15,520 --> 00:18:19,679
when the pod at host a sends an arp

00:18:17,679 --> 00:18:21,600
query the bouncer responds with the mac

00:18:19,679 --> 00:18:24,240
address of 1002

00:18:21,600 --> 00:18:25,280
but it does not only respond to art

00:18:24,240 --> 00:18:27,760
queries

00:18:25,280 --> 00:18:29,760
the bouncer also adds a geneve option in

00:18:27,760 --> 00:18:31,840
the outer packet to tell the transit

00:18:29,760 --> 00:18:34,880
agent the 1002

00:18:31,840 --> 00:18:36,640
is hosted at house c when host a

00:18:34,880 --> 00:18:39,440
receives the arp reply

00:18:36,640 --> 00:18:40,320
it extracts the geneve option and adds

00:18:39,440 --> 00:18:44,400
the host mapping

00:18:40,320 --> 00:18:47,520
information in its ebpf map

00:18:44,400 --> 00:18:49,200
now the transit agent of 1001

00:18:47,520 --> 00:18:50,880
sends packets directly to the

00:18:49,200 --> 00:18:52,960
destination fault

00:18:50,880 --> 00:18:55,360
and this direct communication happens

00:18:52,960 --> 00:18:57,039
from the very first packet of the flow

00:18:55,360 --> 00:18:59,280
and it remains throughout the pods

00:18:57,039 --> 00:19:00,880
lifetime

00:18:59,280 --> 00:19:02,799
there is no more there is one more

00:19:00,880 --> 00:19:05,039
detail here

00:19:02,799 --> 00:19:07,120
when the transit agent sends a packet

00:19:05,039 --> 00:19:09,760
directly to the destination pod

00:19:07,120 --> 00:19:11,360
it toggles one pit in a geneve option to

00:19:09,760 --> 00:19:13,600
tell the destination host

00:19:11,360 --> 00:19:15,200
that the packet is sent directly from

00:19:13,600 --> 00:19:18,400
the source spot worker nodes

00:19:15,200 --> 00:19:19,520
and not from the bouncer this allows the

00:19:18,400 --> 00:19:21,919
transit xdp

00:19:19,520 --> 00:19:24,480
at host c to also return package

00:19:21,919 --> 00:19:26,480
directly to the source pod

00:19:24,480 --> 00:19:28,160
and this simple mechanism allows all

00:19:26,480 --> 00:19:30,720
flows in the cluster to be direct

00:19:28,160 --> 00:19:32,320
without traversing the bouncer

00:19:30,720 --> 00:19:34,480
and at the same time allows the

00:19:32,320 --> 00:19:37,520
management plan to provision the network

00:19:34,480 --> 00:19:38,080
by only making few rpc calls to a couple

00:19:37,520 --> 00:19:40,400
of hosts

00:19:38,080 --> 00:19:42,240
in the cluster not all the hosts in the

00:19:40,400 --> 00:19:43,919
customer

00:19:42,240 --> 00:19:46,480
and if you think about the role of the

00:19:43,919 --> 00:19:49,200
bouncer now comparing to it to a logical

00:19:46,480 --> 00:19:51,440
switch or a logical router in ovn

00:19:49,200 --> 00:19:54,400
it is an in-network as the in-controller

00:19:51,440 --> 00:19:56,960
rather than a virtual switch

00:19:54,400 --> 00:19:59,120
it's like a microservice in the network

00:19:56,960 --> 00:20:02,640
that prove it that provides distributed

00:19:59,120 --> 00:20:04,559
functions to the endpoints

00:20:02,640 --> 00:20:06,159
we take this observation to extend

00:20:04,559 --> 00:20:08,159
missouri functionality beyond

00:20:06,159 --> 00:20:09,600
providing simple connectivity between

00:20:08,159 --> 00:20:11,679
the bots

00:20:09,600 --> 00:20:12,720
essentially we extended the balancer

00:20:11,679 --> 00:20:16,320
functionality

00:20:12,720 --> 00:20:18,080
to implement kupernet's services as well

00:20:16,320 --> 00:20:21,919
this is best to be explained by an

00:20:18,080 --> 00:20:24,559
example consider the 1001 pod

00:20:21,919 --> 00:20:25,480
went with sending packets to one to the

00:20:24,559 --> 00:20:29,600
00:20:25,480 --> 00:20:30,080
1680 service the transit agent xdp

00:20:29,600 --> 00:20:32,799
program

00:20:30,080 --> 00:20:34,000
first processed the packet which knows

00:20:32,799 --> 00:20:35,919
nothing about the network

00:20:34,000 --> 00:20:38,000
except sending the packet to the bouncer

00:20:35,919 --> 00:20:39,840
at host b

00:20:38,000 --> 00:20:41,679
when the bouncer receives the packet it

00:20:39,840 --> 00:20:42,799
looks up the destination ip address of

00:20:41,679 --> 00:20:46,640
the inner packet

00:20:42,799 --> 00:20:48,320
and determines it is for a service ip

00:20:46,640 --> 00:20:50,240
there are several decisions that the

00:20:48,320 --> 00:20:51,760
bouncer can make right now

00:20:50,240 --> 00:20:53,679
including rewriting the inner

00:20:51,760 --> 00:20:55,760
destination ip address and sending the

00:20:53,679 --> 00:20:57,440
packet to a back-end pod

00:20:55,760 --> 00:20:59,280
like any conventional net or load

00:20:57,440 --> 00:21:02,000
balancer device

00:20:59,280 --> 00:21:03,600
but i will describe a different approach

00:21:02,000 --> 00:21:07,120
the bouncer instead

00:21:03,600 --> 00:21:09,919
has the geneve option of its decision

00:21:07,120 --> 00:21:11,600
it instructs the pulse transit agent how

00:21:09,919 --> 00:21:13,120
the service should modify the inner

00:21:11,600 --> 00:21:15,520
packet

00:21:13,120 --> 00:21:17,919
in the clone example the modification

00:21:15,520 --> 00:21:19,760
option says to rewrite the service ip

00:21:17,919 --> 00:21:22,880
address to 1004

00:21:19,760 --> 00:21:24,720
and port 80. then the bouncer returns

00:21:22,880 --> 00:21:25,120
the packet to the sender host which is

00:21:24,720 --> 00:21:28,799
host

00:21:25,120 --> 00:21:29,679
a the transit xdp program on the

00:21:28,799 --> 00:21:32,400
client's host

00:21:29,679 --> 00:21:34,720
add this information option in any in an

00:21:32,400 --> 00:21:37,360
eppf map entry

00:21:34,720 --> 00:21:37,840
then it rewrites the packet according to

00:21:37,360 --> 00:21:39,600
the

00:21:37,840 --> 00:21:41,520
accordingly and rescinds the packet

00:21:39,600 --> 00:21:43,520
again but this time

00:21:41,520 --> 00:21:44,840
not to the service ip but at the back

00:21:43,520 --> 00:21:49,200
end pod

00:21:44,840 --> 00:21:51,280
n004 from this point forward the transit

00:21:49,200 --> 00:21:52,080
agent sends all the packets for this

00:21:51,280 --> 00:21:54,559
flow

00:21:52,080 --> 00:21:56,000
to the back end pod and again without

00:21:54,559 --> 00:22:00,720
going through the bouncers

00:21:56,000 --> 00:22:00,720
ip tables or any other intermediate step

00:22:01,360 --> 00:22:06,880
this powerful concept enabled by xdb

00:22:04,480 --> 00:22:09,440
and kubernetes operators allow us to

00:22:06,880 --> 00:22:11,520
scale the services and replace qproxy

00:22:09,440 --> 00:22:13,520
without compromising the advantage of

00:22:11,520 --> 00:22:16,320
direct communication between pulse over

00:22:13,520 --> 00:22:17,840
service ips

00:22:16,320 --> 00:22:20,080
missouri scales out the number of

00:22:17,840 --> 00:22:22,000
bouncers and dividers in the network to

00:22:20,080 --> 00:22:23,039
become a distributed in-network

00:22:22,000 --> 00:22:26,640
controller

00:22:23,039 --> 00:22:27,440
that serves any traffic the transit

00:22:26,640 --> 00:22:30,159
agent also

00:22:27,440 --> 00:22:31,760
implements a load balancing function on

00:22:30,159 --> 00:22:35,679
the outer ip header

00:22:31,760 --> 00:22:37,600
to rebalance the traffic at 2d bouncers

00:22:35,679 --> 00:22:39,039
but we typically find that a single

00:22:37,600 --> 00:22:42,080
bouncer is enough

00:22:39,039 --> 00:22:45,280
in most cases as it only processes arp

00:22:42,080 --> 00:22:47,679
queries and the first packets of flows

00:22:45,280 --> 00:22:49,360
with this i have provided an overview of

00:22:47,679 --> 00:22:52,400
miser and there is a lot of

00:22:49,360 --> 00:22:54,480
a lot more to it in the probe map and i

00:22:52,400 --> 00:22:55,919
conclude this talk and we will now play

00:22:54,480 --> 00:22:58,559
a recorded demo

00:22:55,919 --> 00:23:00,799
before moving to the q a thank you very

00:22:58,559 --> 00:23:00,799
much

00:23:06,080 --> 00:23:09,200
hi for this demo today we have a three

00:23:07,840 --> 00:23:12,480
node kubernetes cluster

00:23:09,200 --> 00:23:14,080
using kind with mesar installed miser is

00:23:12,480 --> 00:23:14,400
installed on this cluster via a daemon

00:23:14,080 --> 00:23:17,200
set

00:23:14,400 --> 00:23:18,880
and an operator appointment we bootstrap

00:23:17,200 --> 00:23:20,720
the cluster with a default vpc

00:23:18,880 --> 00:23:22,320
and network each with their own divider

00:23:20,720 --> 00:23:24,480
and bouncer

00:23:22,320 --> 00:23:27,200
here we use misar's simple endpoints to

00:23:24,480 --> 00:23:27,200
deploy pods

00:23:27,840 --> 00:23:32,240
now on each node we see that an xv

00:23:30,000 --> 00:23:33,760
program is loaded on the main interface

00:23:32,240 --> 00:23:37,440
and on the v device in the root name

00:23:33,760 --> 00:23:40,000
space we load the agent xp program

00:23:37,440 --> 00:23:43,120
next we demonstrate ping on the two

00:23:40,000 --> 00:23:43,120
recently created pods

00:23:46,480 --> 00:23:50,799
next we will create another ppc with two

00:23:49,120 --> 00:23:54,400
subnets

00:23:50,799 --> 00:23:57,919
the vpc has two dividers and here each

00:23:54,400 --> 00:23:57,919
of its subnets will have a single

00:24:02,840 --> 00:24:05,840
bouncer

00:24:09,840 --> 00:24:13,039
now for each of these subnets we will

00:24:11,600 --> 00:24:19,840
create an endpoint

00:24:13,039 --> 00:24:19,840
or a pod

00:24:34,400 --> 00:24:38,880
now with these two recently created pods

00:24:36,240 --> 00:24:40,720
we will demonstrate cross net ping

00:24:38,880 --> 00:24:47,840
and here to demonstrate our isolation we

00:24:40,720 --> 00:24:47,840
will try to ping across vpcs

00:24:50,480 --> 00:25:01,520
now we will demonstrate our operator

00:24:52,320 --> 00:25:03,760
provisioning 40 endpoints

00:25:01,520 --> 00:25:06,240
regardless of the existing number of

00:25:03,760 --> 00:25:08,240
endpoints or pods on the system

00:25:06,240 --> 00:25:09,600
all subsequent endpoints are provisioned

00:25:08,240 --> 00:25:15,840
at a constant time

00:25:09,600 --> 00:25:15,840
of about 0.35 seconds

00:25:22,159 --> 00:25:27,520
now in the next section we demonstrate

00:25:24,240 --> 00:25:29,360
intra and inter network direct path

00:25:27,520 --> 00:25:30,880
intern network direct path only the

00:25:29,360 --> 00:25:32,880
first art packet goes through the

00:25:30,880 --> 00:25:34,799
bouncer

00:25:32,880 --> 00:25:36,159
once both sides have cached the endpoint

00:25:34,799 --> 00:25:38,159
host information

00:25:36,159 --> 00:25:43,840
any traffic thereafter will only flow

00:25:38,159 --> 00:25:43,840
between the two endpoint hosts

00:25:46,559 --> 00:25:50,159
for inter-network direct path the first

00:25:49,360 --> 00:25:51,679
arp packet

00:25:50,159 --> 00:25:53,919
goes through the divider and both

00:25:51,679 --> 00:25:56,640
bouncers

00:25:53,919 --> 00:25:57,120
here the divider and the two endpoint

00:25:56,640 --> 00:26:11,840
hosts

00:25:57,120 --> 00:26:11,840
must cache the host's information

00:26:19,520 --> 00:26:22,720
once all three have cached the endpoint

00:26:21,200 --> 00:26:24,400
host information

00:26:22,720 --> 00:26:26,080
any traffic thereafter will flow between

00:26:24,400 --> 00:26:30,000
the two endpoint hosts

00:26:26,080 --> 00:26:30,000
and the divider as an intermediary

00:26:35,679 --> 00:26:39,120
finally in this part of the demo we

00:26:38,240 --> 00:26:41,679
demonstrate using

00:26:39,120 --> 00:26:53,840
misar scale endpoint as a replacement

00:26:41,679 --> 00:26:53,840
for the community's cluster ip service

00:26:57,520 --> 00:27:07,840
when a service is created miser creates

00:27:00,000 --> 00:27:07,840
a corresponding scaled endpoint

00:27:12,960 --> 00:27:16,880
here we label the two recently created

00:27:15,039 --> 00:27:18,880
pods

00:27:16,880 --> 00:27:21,840
to add them as back ends for the scaled

00:27:18,880 --> 00:27:21,840
endpoint

00:27:33,760 --> 00:27:37,360
now for this demonstration we will curl

00:27:35,679 --> 00:27:38,960
the surface from our third pod

00:27:37,360 --> 00:27:40,880
and in the reply we will see that the

00:27:38,960 --> 00:27:43,440
surface replies with the pod name of one

00:27:40,880 --> 00:27:49,840
of the back ends

00:27:43,440 --> 00:27:49,840
here both pod 1 and pod 2 reply to curl

00:27:52,720 --> 00:27:56,880
we can also ping the service this is

00:27:55,440 --> 00:28:03,760
possible because of the current

00:27:56,880 --> 00:28:03,760

YouTube URL: https://www.youtube.com/watch?v=gptklnLaqMA


