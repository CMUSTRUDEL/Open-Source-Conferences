Title: Changing Oil for a Fast Running (Side) Car Quickly and Safely - Fuyuan Bie
Publication date: 2020-10-21
Playlist: EnvoyCon 2020 - Virtual
Description: 
	Changing Oil for a Fast Running (Side) Car Quickly and Safely - Fuyuan Bie

While we all want our features in production ASAP, safety - on the other hand - is the last thing we should sacrifice. At Pinterest, mesh configuration story looks like this: - thousands clusters - under 25 minutes to fully deploy through all stages and availability zones serially - 0 incidents during xDS v3 migration - every change is validated individually - full visibility into change and client history Thanks to comprehensive pre-deploy validation, holistic health checks and a specially designed feedback channel based on xDS, configuration roll-out is safe and yet very fast. The machinery carries every config change to VMs, dockerized hosts, and k8s automatically. Should an issue happen, it can spot the problem within a minute. In this talk, we will share the architecture, design considerations, good practices, and lessons learned along our path towards configuration nirvana.
Captions: 
	00:00:00,560 --> 00:00:06,000
hi everyone welcome to this talk

00:00:03,600 --> 00:00:06,720
i'm very excited to be here to talk

00:00:06,000 --> 00:00:09,440
about

00:00:06,720 --> 00:00:10,639
how we change oil for fast running

00:00:09,440 --> 00:00:14,719
inside the car

00:00:10,639 --> 00:00:18,240
um quickly and safely at pinterest

00:00:14,719 --> 00:00:21,600
so i'm today's uh mechanic

00:00:18,240 --> 00:00:22,480
my name is fuyuan and uh i'm part of the

00:00:21,600 --> 00:00:26,880
traffic team

00:00:22,480 --> 00:00:29,279
at pinterest infrastructure engineering

00:00:26,880 --> 00:00:30,240
so today we are going to start with a

00:00:29,279 --> 00:00:33,760
look at

00:00:30,240 --> 00:00:36,239
uh avoid pinterest briefly

00:00:33,760 --> 00:00:38,399
and follow that or look at the

00:00:36,239 --> 00:00:41,920
configuration layout

00:00:38,399 --> 00:00:42,399
story he add pinterest the challenges we

00:00:41,920 --> 00:00:46,640
faced

00:00:42,399 --> 00:00:49,039
and the solutions we used and at the end

00:00:46,640 --> 00:00:49,840
we'll share some best practices we

00:00:49,039 --> 00:00:51,920
applied

00:00:49,840 --> 00:00:54,239
and the lessons we have learned during

00:00:51,920 --> 00:00:56,559
this journey

00:00:54,239 --> 00:01:00,079
so at pinterest we have been using

00:00:56,559 --> 00:01:03,520
hawaii started around four years ago

00:01:00,079 --> 00:01:07,360
and we use it very widely

00:01:03,520 --> 00:01:09,439
and deeply when you when user traffic

00:01:07,360 --> 00:01:14,000
comes into pinterest

00:01:09,439 --> 00:01:17,680
it firstly hit he'd always an agile boy

00:01:14,000 --> 00:01:21,360
from there trafficker that will get

00:01:17,680 --> 00:01:24,720
routed to our mesh mesh envoy

00:01:21,360 --> 00:01:26,240
and starting from there we the traffic

00:01:24,720 --> 00:01:29,600
that allowed routed to different

00:01:26,240 --> 00:01:33,280
services and it did not stop there

00:01:29,600 --> 00:01:36,880
we we also use our at the storage layer

00:01:33,280 --> 00:01:40,720
for example we use alloy if

00:01:36,880 --> 00:01:41,119
we uh install our bicycle so that we can

00:01:40,720 --> 00:01:45,680
get

00:01:41,119 --> 00:01:49,280
mutual trs between services and bicycle

00:01:45,680 --> 00:01:52,479
and at the edge only

00:01:49,280 --> 00:01:56,399
we deal we are processing

00:01:52,479 --> 00:01:56,399
millions rps per seconds

00:01:57,280 --> 00:02:03,439
and this is the architecture of our mesh

00:02:00,399 --> 00:02:04,479
at the very top is the tower tower is

00:02:03,439 --> 00:02:08,319
the central

00:02:04,479 --> 00:02:11,520
lyso control plane officer service mesh

00:02:08,319 --> 00:02:14,400
when a user land a config into

00:02:11,520 --> 00:02:15,280
the gate report jenkins will upload the

00:02:14,400 --> 00:02:17,840
config

00:02:15,280 --> 00:02:19,040
in into the centralized control plan and

00:02:17,840 --> 00:02:22,400
it will be processed

00:02:19,040 --> 00:02:24,800
in zookeeper and behind

00:02:22,400 --> 00:02:25,920
that the centralized control plan we

00:02:24,800 --> 00:02:30,319
install

00:02:25,920 --> 00:02:33,440
a agent on every host it's called beacon

00:02:30,319 --> 00:02:36,480
beacon and their tower they are

00:02:33,440 --> 00:02:39,680
between buick and harvard is a

00:02:36,480 --> 00:02:41,280
simple interrupt but a very robust grpc

00:02:39,680 --> 00:02:45,040
streaming protocol

00:02:41,280 --> 00:02:49,200
and it's a generic config distribution

00:02:45,040 --> 00:02:52,239
protocol between them and we

00:02:49,200 --> 00:02:55,680
will leave the worsening

00:02:52,239 --> 00:02:58,560
and other complexities

00:02:55,680 --> 00:02:59,760
to the last mile so the beacon will be

00:02:58,560 --> 00:03:03,519
the

00:02:59,760 --> 00:03:06,400
agent dealing with xcs services

00:03:03,519 --> 00:03:08,959
and they will distribute the coffee

00:03:06,400 --> 00:03:11,440
through xcs to outboy

00:03:08,959 --> 00:03:12,080
and to build seating side by side of his

00:03:11,440 --> 00:03:15,440
armoire

00:03:12,080 --> 00:03:18,000
we have sds sidecar and opi agent

00:03:15,440 --> 00:03:21,599
to the austin to help our way to do

00:03:18,000 --> 00:03:25,440
authentication under authorization

00:03:21,599 --> 00:03:26,000
we have an alloy deployed on both easy

00:03:25,440 --> 00:03:29,120
to

00:03:26,000 --> 00:03:31,200
vm and kubernetes

00:03:29,120 --> 00:03:32,879
this cross platform we have one

00:03:31,200 --> 00:03:36,400
centralized control plan for

00:03:32,879 --> 00:03:36,400
both both platform

00:03:38,319 --> 00:03:41,760
compared to binary changes config

00:03:41,200 --> 00:03:45,760
changes

00:03:41,760 --> 00:03:48,799
the more complicated why because we have

00:03:45,760 --> 00:03:51,519
thousands of clusters some are small

00:03:48,799 --> 00:03:51,519
some are huge

00:03:53,040 --> 00:03:59,280
pinterest was created 10 years ago and

00:03:56,640 --> 00:04:01,840
since then service was added gradually

00:03:59,280 --> 00:04:05,040
so we got more and more services

00:04:01,840 --> 00:04:06,400
and every service has one or more

00:04:05,040 --> 00:04:10,080
clusters

00:04:06,400 --> 00:04:13,120
and to make it make it more complicated

00:04:10,080 --> 00:04:16,560
we give every developer a easy

00:04:13,120 --> 00:04:18,799
instance as dell box

00:04:16,560 --> 00:04:20,160
i read that we treat our dev box a

00:04:18,799 --> 00:04:22,639
cluster

00:04:20,160 --> 00:04:23,600
that is how we get thousands clusters

00:04:22,639 --> 00:04:26,639
and

00:04:23,600 --> 00:04:30,320
some of the clusters are small

00:04:26,639 --> 00:04:33,680
only one listeners one

00:04:30,320 --> 00:04:37,040
or two maybe three upstreams

00:04:33,680 --> 00:04:40,080
some are huge the i had i just

00:04:37,040 --> 00:04:43,360
look at the largest cluster we have

00:04:40,080 --> 00:04:46,880
it has about 300 000 lines of

00:04:43,360 --> 00:04:50,320
config and the config dump

00:04:46,880 --> 00:04:53,600
was more than seven megabytes

00:04:50,320 --> 00:04:54,639
and we will on average we go out tens of

00:04:53,600 --> 00:04:58,000
changes

00:04:54,639 --> 00:04:58,720
every day and we deploy it we need to

00:04:58,000 --> 00:05:02,560
deploy them

00:04:58,720 --> 00:05:06,320
first and then you come to the

00:05:02,560 --> 00:05:08,960
question if our problem happen to config

00:05:06,320 --> 00:05:10,160
how do we root cause the problem problem

00:05:08,960 --> 00:05:12,560
quickly

00:05:10,160 --> 00:05:13,600
and it was after you relocate the

00:05:12,560 --> 00:05:16,320
problem

00:05:13,600 --> 00:05:16,800
what do you do should it support your

00:05:16,320 --> 00:05:22,160
back

00:05:16,800 --> 00:05:22,160
or should we mind it forward fix

00:05:22,240 --> 00:05:29,280
so think about this scenario is like

00:05:26,000 --> 00:05:30,240
you are driving outside a car and your

00:05:29,280 --> 00:05:34,320
customer is

00:05:30,240 --> 00:05:37,919
enjoying the ride and this

00:05:34,320 --> 00:05:38,400
it it's perfect and uh but deep in your

00:05:37,919 --> 00:05:41,440
heart

00:05:38,400 --> 00:05:44,800
you know you are due on oil change

00:05:41,440 --> 00:05:45,680
and you don't want your customer to be a

00:05:44,800 --> 00:05:47,919
happy

00:05:45,680 --> 00:05:49,360
how do you do that how do you do the oil

00:05:47,919 --> 00:05:52,639
change while you also because

00:05:49,360 --> 00:05:53,280
the car is still running so here are the

00:05:52,639 --> 00:05:55,680
pinches

00:05:53,280 --> 00:05:57,039
we basically tackle this problem in

00:05:55,680 --> 00:06:00,479
three

00:05:57,039 --> 00:06:03,919
from three perspectives the first one

00:06:00,479 --> 00:06:06,400
is configuration as code so called

00:06:03,919 --> 00:06:07,039
four configurations we may make sure

00:06:06,400 --> 00:06:09,919
they are

00:06:07,039 --> 00:06:10,800
typed they are tested they are

00:06:09,919 --> 00:06:13,600
rationalized

00:06:10,800 --> 00:06:13,840
and they are deployable the second one

00:06:13,600 --> 00:06:16,800
is

00:06:13,840 --> 00:06:18,319
we emphasize strong infrastructure

00:06:16,800 --> 00:06:21,039
governance

00:06:18,319 --> 00:06:23,280
and third one is the subway state we use

00:06:21,039 --> 00:06:26,960
the stator configure ruling out

00:06:23,280 --> 00:06:30,080
it's safeguarded by a near real-time

00:06:26,960 --> 00:06:32,160
feedback loop and a holistic health

00:06:30,080 --> 00:06:35,680
check

00:06:32,160 --> 00:06:39,440
so configure obviously written as code

00:06:35,680 --> 00:06:40,639
typed why white go how we make the

00:06:39,440 --> 00:06:43,759
configuration type

00:06:40,639 --> 00:06:45,600
we use the ginger template

00:06:43,759 --> 00:06:48,400
all the configurations are based on a

00:06:45,600 --> 00:06:50,479
set of agenda templates that we provide

00:06:48,400 --> 00:06:52,800
and when the user write their

00:06:50,479 --> 00:06:55,520
configuration they need to compile

00:06:52,800 --> 00:06:58,160
and configurations are materialized at

00:06:55,520 --> 00:07:00,800
compile time

00:06:58,160 --> 00:07:01,520
and it's also versionized when the

00:07:00,800 --> 00:07:04,400
configural

00:07:01,520 --> 00:07:06,160
the ground shoes is in kit it keeps the

00:07:04,400 --> 00:07:08,560
rail with the version and the

00:07:06,160 --> 00:07:09,759
centralized when the zinc winching is

00:07:08,560 --> 00:07:12,160
copied the configuration

00:07:09,759 --> 00:07:12,960
into the tower into the centralized

00:07:12,160 --> 00:07:14,880
control plate

00:07:12,960 --> 00:07:16,639
it's at add version there so the

00:07:14,880 --> 00:07:18,880
configuration stored in sync to a

00:07:16,639 --> 00:07:20,240
centralized control plate is also

00:07:18,880 --> 00:07:23,160
organized

00:07:20,240 --> 00:07:24,319
and is tested we run the unit has a

00:07:23,160 --> 00:07:26,960
comprehensive

00:07:24,319 --> 00:07:28,240
set of unit tests before and after

00:07:26,960 --> 00:07:31,360
landing

00:07:28,240 --> 00:07:32,400
so when we we the tests are

00:07:31,360 --> 00:07:34,960
comprehensive

00:07:32,400 --> 00:07:35,440
if we test the listeners every listener

00:07:34,960 --> 00:07:38,560
has

00:07:35,440 --> 00:07:39,440
must have have valid route every route

00:07:38,560 --> 00:07:42,880
need to

00:07:39,440 --> 00:07:43,440
have valid valid upstream if a really if

00:07:42,880 --> 00:07:46,639
a

00:07:43,440 --> 00:07:49,440
in upstream require tls

00:07:46,639 --> 00:07:50,800
the downstream must give the correct

00:07:49,440 --> 00:07:53,919
configuration they must

00:07:50,800 --> 00:07:56,639
match also like a poor conflict

00:07:53,919 --> 00:07:58,000
these kind of things are detected during

00:07:56,639 --> 00:08:01,039
build time

00:07:58,000 --> 00:08:02,560
and also it is deployable uh if we have

00:08:01,039 --> 00:08:05,919
a pipeline deploy

00:08:02,560 --> 00:08:09,120
uh deploy changes

00:08:05,919 --> 00:08:12,240
after apr landed until it

00:08:09,120 --> 00:08:14,479
does that for every pr every company

00:08:12,240 --> 00:08:17,680
changes

00:08:14,479 --> 00:08:20,479
so infrastructure governance from

00:08:17,680 --> 00:08:23,840
perform called mesh perspective

00:08:20,479 --> 00:08:24,479
what is basically two questions first

00:08:23,840 --> 00:08:27,039
one

00:08:24,479 --> 00:08:28,960
where should i where should the mesh

00:08:27,039 --> 00:08:31,759
configurations go

00:08:28,960 --> 00:08:32,080
should it be in a centralized ripple

00:08:31,759 --> 00:08:35,200
this

00:08:32,080 --> 00:08:38,159
of this of course has the proton counts

00:08:35,200 --> 00:08:40,320
pros of a synchronized ripple is

00:08:38,159 --> 00:08:42,640
consistent you get consistency

00:08:40,320 --> 00:08:44,240
you have a quick turnaround for

00:08:42,640 --> 00:08:47,600
horizontal changes

00:08:44,240 --> 00:08:50,080
or by horizontal changes i mean

00:08:47,600 --> 00:08:52,320
you will make a change which applies to

00:08:50,080 --> 00:08:54,959
multiple clusters

00:08:52,320 --> 00:08:55,920
and of course it also have counts user

00:08:54,959 --> 00:08:58,240
experience may

00:08:55,920 --> 00:09:00,640
not be ideal if you are a service owner

00:08:58,240 --> 00:09:02,240
you most likely would like to have your

00:09:00,640 --> 00:09:05,279
configurations

00:09:02,240 --> 00:09:06,399
in your own repo instead of making a

00:09:05,279 --> 00:09:10,320
change to another

00:09:06,399 --> 00:09:14,480
centralized ripple or should we seek

00:09:10,320 --> 00:09:17,440
is through the sit next to each service

00:09:14,480 --> 00:09:19,600
pros users have better control of their

00:09:17,440 --> 00:09:21,279
their conflict they just need to

00:09:19,600 --> 00:09:23,600
modify that configuration in their repo

00:09:21,279 --> 00:09:26,959
is that go to your ripple

00:09:23,600 --> 00:09:29,760
however this there's another big

00:09:26,959 --> 00:09:31,440
conference limitations you when you are

00:09:29,760 --> 00:09:32,800
your configuration with a mesh

00:09:31,440 --> 00:09:36,080
configuration scattered

00:09:32,800 --> 00:09:37,839
in multiple places the build cost think

00:09:36,080 --> 00:09:38,320
about the build cost when you build the

00:09:37,839 --> 00:09:40,720
company

00:09:38,320 --> 00:09:42,800
how do you build how do you sure make

00:09:40,720 --> 00:09:43,440
sure that everyone every company needs

00:09:42,800 --> 00:09:46,640
to be

00:09:43,440 --> 00:09:49,200
built is built and another

00:09:46,640 --> 00:09:50,320
question is actually more severe

00:09:49,200 --> 00:09:53,600
question is

00:09:50,320 --> 00:09:55,920
if a critical problem is spotted like a

00:09:53,600 --> 00:09:57,279
security bug that needs to change the

00:09:55,920 --> 00:10:00,399
config immediately

00:09:57,279 --> 00:10:01,440
what do we do you have hundreds of maybe

00:10:00,399 --> 00:10:03,600
hundreds per

00:10:01,440 --> 00:10:04,959
report what will you do how do you fix

00:10:03,600 --> 00:10:05,680
the problem how do you make sure that

00:10:04,959 --> 00:10:07,200
you cover

00:10:05,680 --> 00:10:09,040
everything and you are not missing

00:10:07,200 --> 00:10:12,320
anything

00:10:09,040 --> 00:10:14,800
so based on that the choice we made

00:10:12,320 --> 00:10:16,399
is we use a centralized group everyone

00:10:14,800 --> 00:10:18,720
checking their configuration into that

00:10:16,399 --> 00:10:21,519
circularized ripple

00:10:18,720 --> 00:10:22,240
another question i will with regard to

00:10:21,519 --> 00:10:25,440
infrared

00:10:22,240 --> 00:10:28,480
or governance is who owns what

00:10:25,440 --> 00:10:28,959
mesh is big no matter how many measures

00:10:28,480 --> 00:10:32,000
you have

00:10:28,959 --> 00:10:34,079
you probably have one how many

00:10:32,000 --> 00:10:35,200
do you sell you always have this

00:10:34,079 --> 00:10:38,560
required problem

00:10:35,200 --> 00:10:42,320
whole consequent at pinterest we have

00:10:38,560 --> 00:10:45,519
three personas infrastructure engine

00:10:42,320 --> 00:10:46,399
security engine and service owner each

00:10:45,519 --> 00:10:49,680
of them

00:10:46,399 --> 00:10:53,200
own different things of the mesh

00:10:49,680 --> 00:10:56,720
as you as are listed here

00:10:53,200 --> 00:10:59,920
so how do we fit those personas

00:10:56,720 --> 00:11:01,200
into the service mesh world let's start

00:10:59,920 --> 00:11:04,399
with simple

00:11:01,200 --> 00:11:08,000
a service a and service b and

00:11:04,399 --> 00:11:09,200
service a talk to service b in this case

00:11:08,000 --> 00:11:12,079
we call service a

00:11:09,200 --> 00:11:13,360
a downstream so it's b not upstream

00:11:12,079 --> 00:11:16,800
right

00:11:13,360 --> 00:11:21,200
and instead of treat each service i.e

00:11:16,800 --> 00:11:24,000
monolithic we cut them into three pieces

00:11:21,200 --> 00:11:26,000
well ingredient eating grass egress and

00:11:24,000 --> 00:11:29,120
the service itself

00:11:26,000 --> 00:11:30,800
then we fit each persona's

00:11:29,120 --> 00:11:34,399
responsibilities

00:11:30,800 --> 00:11:34,399
into each piece

00:11:35,120 --> 00:11:42,240
infrastructure engineering this is

00:11:38,320 --> 00:11:45,600
this persona on the mesh

00:11:42,240 --> 00:11:48,320
infrastructure here example

00:11:45,600 --> 00:11:49,680
the infrastructure engineer define

00:11:48,320 --> 00:11:53,600
well-known ports

00:11:49,680 --> 00:11:58,480
like ingress https port

00:11:53,600 --> 00:12:01,600
must be on port 8443

00:11:58,480 --> 00:12:05,360
then security engine engineering

00:12:01,600 --> 00:12:08,560
security engineer with otl tris

00:12:05,360 --> 00:12:09,279
authorization author authentication and

00:12:08,560 --> 00:12:11,279
they are both

00:12:09,279 --> 00:12:13,680
deal worthy controversy ingress and

00:12:11,279 --> 00:12:13,680
egress

00:12:13,760 --> 00:12:17,839
the last sorry zone service owner of

00:12:16,800 --> 00:12:20,399
course

00:12:17,839 --> 00:12:21,680
on the server on the business logic in

00:12:20,399 --> 00:12:24,560
their service

00:12:21,680 --> 00:12:26,880
and made like a routine in there from

00:12:24,560 --> 00:12:30,000
the ingress side

00:12:26,880 --> 00:12:33,519
however this is the most

00:12:30,000 --> 00:12:36,480
important part think about your your

00:12:33,519 --> 00:12:38,000
simple see yourself as the owner of

00:12:36,480 --> 00:12:40,480
service b

00:12:38,000 --> 00:12:42,160
another another service owner want to

00:12:40,480 --> 00:12:45,519
call your service

00:12:42,160 --> 00:12:49,440
and you want to re-limit that service

00:12:45,519 --> 00:12:52,560
that re-limited config is part of a

00:12:49,440 --> 00:12:54,000
service a but given by a service bus

00:12:52,560 --> 00:12:57,360
owner

00:12:54,000 --> 00:13:00,399
this scenario in this case

00:12:57,360 --> 00:13:03,760
make it complicated like say

00:13:00,399 --> 00:13:07,920
once the result your upstream owner

00:13:03,760 --> 00:13:11,279
contributed to downstream configuration

00:13:07,920 --> 00:13:14,639
think about that it's the right

00:13:11,279 --> 00:13:16,720
thing to do and it's it it's beyond

00:13:14,639 --> 00:13:17,600
those traditional definition of service

00:13:16,720 --> 00:13:20,720
owner a

00:13:17,600 --> 00:13:23,680
of service and service owner b owns the

00:13:20,720 --> 00:13:24,720
things of their service totally that's

00:13:23,680 --> 00:13:28,079
not right this is

00:13:24,720 --> 00:13:29,839
different this is how a house mesh

00:13:28,079 --> 00:13:33,120
configuration should be like

00:13:29,839 --> 00:13:35,839
look like and

00:13:33,120 --> 00:13:36,320
with that if you look at service service

00:13:35,839 --> 00:13:40,320
as an

00:13:36,320 --> 00:13:42,160
owner of a service fee from ibra side

00:13:40,320 --> 00:13:44,000
he barely has have something to

00:13:42,160 --> 00:13:47,440
configure

00:13:44,000 --> 00:13:51,440
probably just some declaration of

00:13:47,440 --> 00:13:51,440
upstreams you you need to talk to

00:13:52,160 --> 00:13:59,360
that stage drawing out

00:13:56,160 --> 00:14:00,399
i think is reduced but my cell conflict

00:13:59,360 --> 00:14:03,360
got landed

00:14:00,399 --> 00:14:04,399
it's as we saw we started to push that

00:14:03,360 --> 00:14:07,680
config

00:14:04,399 --> 00:14:11,120
into a pilot through our pipeline

00:14:07,680 --> 00:14:12,320
so jenkins dropped he started running

00:14:11,120 --> 00:14:15,680
unit tests

00:14:12,320 --> 00:14:18,320
so although unit has run before pr land

00:14:15,680 --> 00:14:19,199
however after a pr land we still run it

00:14:18,320 --> 00:14:21,360
because

00:14:19,199 --> 00:14:23,040
you could have a merge problem cause of

00:14:21,360 --> 00:14:26,240
configuration failure

00:14:23,040 --> 00:14:28,959
and once that is finished we

00:14:26,240 --> 00:14:29,519
the difference job will literally the

00:14:28,959 --> 00:14:31,600
pipeline

00:14:29,519 --> 00:14:33,600
will upload the configuration into tower

00:14:31,600 --> 00:14:35,360
the centralized control plate

00:14:33,600 --> 00:14:36,800
and then it will deploy the

00:14:35,360 --> 00:14:40,720
configuration into latest

00:14:36,800 --> 00:14:43,199
stage immediately and

00:14:40,720 --> 00:14:44,320
every conflict exchange will start a

00:14:43,199 --> 00:14:46,560
pipeline right

00:14:44,320 --> 00:14:48,079
and every pipeline rank will deploy the

00:14:46,560 --> 00:14:51,680
configuration into latest

00:14:48,079 --> 00:14:54,800
stage automatically it takes about

00:14:51,680 --> 00:14:58,800
three minutes after config is landed

00:14:54,800 --> 00:15:01,360
till it gets activated in latest stage

00:14:58,800 --> 00:15:03,120
and after that finished i will see there

00:15:01,360 --> 00:15:06,720
i send that notification to

00:15:03,120 --> 00:15:10,160
the on-call engineer and a pt4 approval

00:15:06,720 --> 00:15:11,040
and if the outcome engineer were there

00:15:10,160 --> 00:15:13,839
approaches

00:15:11,040 --> 00:15:16,160
of a click approve button it will go to

00:15:13,839 --> 00:15:19,839
the next stage which is the canary

00:15:16,160 --> 00:15:23,199
and the dev app those stages

00:15:19,839 --> 00:15:23,519
still activate the new configuration in

00:15:23,199 --> 00:15:27,279
the

00:15:23,519 --> 00:15:30,880
stage and listen for several minutes

00:15:27,279 --> 00:15:34,240
and keep holding the the service status

00:15:30,880 --> 00:15:36,880
and if that there is no problem it will

00:15:34,240 --> 00:15:38,839
consider the canary stage got passed and

00:15:36,880 --> 00:15:40,399
it was in another notification for

00:15:38,839 --> 00:15:42,959
approval

00:15:40,399 --> 00:15:44,399
once they are approved it will go to go

00:15:42,959 --> 00:15:47,440
next or go to

00:15:44,399 --> 00:15:48,160
product stage in first stage we do

00:15:47,440 --> 00:15:51,120
region

00:15:48,160 --> 00:15:52,240
scene parallel so every however we see

00:15:51,120 --> 00:15:55,279
each region

00:15:52,240 --> 00:15:58,880
we do ability availability zone

00:15:55,279 --> 00:16:02,079
after availability so every time we just

00:15:58,880 --> 00:16:05,360
deploy one availability zone and

00:16:02,079 --> 00:16:06,079
if there's something similar happened or

00:16:05,360 --> 00:16:11,120
stopped there

00:16:06,079 --> 00:16:11,120
invest it it is a canadian ruling out

00:16:12,000 --> 00:16:17,040
so we use a rule out configuration how

00:16:14,560 --> 00:16:19,920
do we know there is a problem

00:16:17,040 --> 00:16:20,240
so we have we we detect problems through

00:16:19,920 --> 00:16:23,600
a

00:16:20,240 --> 00:16:25,120
near real-time feedback loop and this is

00:16:23,600 --> 00:16:27,600
the definition of

00:16:25,120 --> 00:16:29,360
negative of negative feedback loop from

00:16:27,600 --> 00:16:32,399
wikipedia i

00:16:29,360 --> 00:16:35,440
don't want to repeat but it's basically

00:16:32,399 --> 00:16:36,959
like you take the output of your

00:16:35,440 --> 00:16:40,399
amplifier

00:16:36,959 --> 00:16:42,720
apply some algorithm and convert the

00:16:40,399 --> 00:16:44,240
part of the output and fit it into the

00:16:42,720 --> 00:16:47,600
input and

00:16:44,240 --> 00:16:51,600
through that you keep tuning your your

00:16:47,600 --> 00:16:54,160
system to get your ideal output

00:16:51,600 --> 00:16:56,880
how do you up how do we apply that to

00:16:54,160 --> 00:16:56,880
our mesh

00:16:56,959 --> 00:17:00,959
and the feedback part is basically used

00:17:00,160 --> 00:17:04,480
to detect

00:17:00,959 --> 00:17:08,799
problem and what problem do we detect

00:17:04,480 --> 00:17:12,079
we detect our avoid reject configs

00:17:08,799 --> 00:17:15,600
basically this is through xcs protocol

00:17:12,079 --> 00:17:18,319
and also we have a health check

00:17:15,600 --> 00:17:20,400
failures like data plan for the it

00:17:18,319 --> 00:17:21,679
detected failures input data plane and

00:17:20,400 --> 00:17:24,160
control plane

00:17:21,679 --> 00:17:25,039
for a data plane side service may be

00:17:24,160 --> 00:17:27,839
unhealthy

00:17:25,039 --> 00:17:28,240
from the control plane side maybe the

00:17:27,839 --> 00:17:30,240
maybe

00:17:28,240 --> 00:17:31,520
kind of no config available for that

00:17:30,240 --> 00:17:34,320
specific

00:17:31,520 --> 00:17:36,000
uh cluster or maybe mesh infrastructure

00:17:34,320 --> 00:17:39,520
infrastructure is not ready

00:17:36,000 --> 00:17:41,520
or maybe listener are not up in the old

00:17:39,520 --> 00:17:43,200
or in other cases when you send out a

00:17:41,520 --> 00:17:45,200
configuration and it

00:17:43,200 --> 00:17:47,360
triggers some bug and avoid just the

00:17:45,200 --> 00:17:50,640
poom disappeared

00:17:47,360 --> 00:17:53,280
that we lose clients and the last of

00:17:50,640 --> 00:17:53,919
our last of our check we do is version

00:17:53,280 --> 00:17:56,000
mismatch

00:17:53,919 --> 00:17:57,520
you send out the configuration to our

00:17:56,000 --> 00:18:00,960
say hey hey

00:17:57,520 --> 00:18:04,160
alloy on this cluster please use

00:18:00,960 --> 00:18:07,360
um version 100 and

00:18:04,160 --> 00:18:09,760
then you you see you observe oh

00:18:07,360 --> 00:18:10,559
some of them or all of them are actually

00:18:09,760 --> 00:18:13,919
running

00:18:10,559 --> 00:18:14,640
on that new on old build which is like a

00:18:13,919 --> 00:18:16,400
00:18:14,640 --> 00:18:20,240
and then you know oh there's a problem

00:18:16,400 --> 00:18:20,240
because it's not on the right version

00:18:21,200 --> 00:18:26,000
so the first thing uh

00:18:24,400 --> 00:18:28,240
the first failure that we did have is

00:18:26,000 --> 00:18:32,240
the xds are resource

00:18:28,240 --> 00:18:35,360
acceptance or rejection so

00:18:32,240 --> 00:18:38,480
with it within xds3 or protocol

00:18:35,360 --> 00:18:41,039
it's about it's about a real required

00:18:38,480 --> 00:18:44,400
discovery request and response

00:18:41,039 --> 00:18:47,840
from this blog because i'll control the

00:18:44,400 --> 00:18:49,840
config pipeline send a new configuration

00:18:47,840 --> 00:18:51,919
of a new version of the of the

00:18:49,840 --> 00:18:56,720
configuration to

00:18:51,919 --> 00:18:58,400
the tower and uh tower this

00:18:56,720 --> 00:19:01,120
sends this new car a new version to

00:18:58,400 --> 00:19:03,600
beacon which is the unhost agent

00:19:01,120 --> 00:19:04,559
and then that host agent convert this

00:19:03,600 --> 00:19:07,679
new version

00:19:04,559 --> 00:19:08,559
to actually a as xcs response and send

00:19:07,679 --> 00:19:11,600
it to our

00:19:08,559 --> 00:19:12,559
give it to that one and we look get the

00:19:11,600 --> 00:19:15,200
version

00:19:12,559 --> 00:19:16,320
and then do a bunch of checks and find

00:19:15,200 --> 00:19:19,280
it

00:19:16,320 --> 00:19:20,000
i i'm okay with this conflict and i

00:19:19,280 --> 00:19:22,799
accept it

00:19:20,000 --> 00:19:24,240
then it will use the discovery request

00:19:22,799 --> 00:19:27,360
to act

00:19:24,240 --> 00:19:29,919
there is a the the newers and then

00:19:27,360 --> 00:19:31,039
this will be reportedly be aggregated in

00:19:29,919 --> 00:19:32,720
beacon

00:19:31,039 --> 00:19:34,799
and the beacon will aggregate the

00:19:32,720 --> 00:19:37,760
repository report back to the

00:19:34,799 --> 00:19:38,960
tower so so the pipeline will keep

00:19:37,760 --> 00:19:40,480
current tower to see

00:19:38,960 --> 00:19:42,880
oh is there any fulfilling there

00:19:40,480 --> 00:19:46,000
anything so another case is

00:19:42,880 --> 00:19:48,160
for example the new version contains or

00:19:46,000 --> 00:19:51,200
require a new secret

00:19:48,160 --> 00:19:54,720
the sacred however wasn't granted to the

00:19:51,200 --> 00:19:55,120
process on that of our way so unlock

00:19:54,720 --> 00:19:57,440
will

00:19:55,120 --> 00:19:58,240
gather the new configuration and see oh

00:19:57,440 --> 00:20:01,520
i need a new

00:19:58,240 --> 00:20:02,640
new new secret the work ask sds for the

00:20:01,520 --> 00:20:06,000
new secret and

00:20:02,640 --> 00:20:09,039
access reject the sdsu account

00:20:06,000 --> 00:20:12,240
the request and i will report an

00:20:09,039 --> 00:20:14,960
arrow to beacon and the beacon was was

00:20:12,240 --> 00:20:17,360
eventually send this aggregated result

00:20:14,960 --> 00:20:19,919
to tower tower or see oh okay here

00:20:17,360 --> 00:20:21,679
mark mark the the cluster see this

00:20:19,919 --> 00:20:24,559
calendar has this problem

00:20:21,679 --> 00:20:25,919
and it will similar the arrow to to uh

00:20:24,559 --> 00:20:28,000
to yellow key

00:20:25,919 --> 00:20:30,080
and at the same time the both the

00:20:28,000 --> 00:20:30,400
configured pipeline will see this arrow

00:20:30,080 --> 00:20:34,080
and

00:20:30,400 --> 00:20:37,280
fill the deployment

00:20:34,080 --> 00:20:40,080
we do this at a cluster block cluster

00:20:37,280 --> 00:20:40,640
so any cluster has a configuration

00:20:40,080 --> 00:20:44,320
problem

00:20:40,640 --> 00:20:47,280
will be detected and will be reflected

00:20:44,320 --> 00:20:50,159
on configuration pipeline

00:20:47,280 --> 00:20:50,640
so another part is the holistic health

00:20:50,159 --> 00:20:54,080
check

00:20:50,640 --> 00:20:55,840
we household check listener and

00:20:54,080 --> 00:20:57,600
we health check the coffee of the

00:20:55,840 --> 00:21:00,080
configuration uh

00:20:57,600 --> 00:21:01,520
about bobby child child called

00:21:00,080 --> 00:21:04,640
controversy

00:21:01,520 --> 00:21:07,280
for like the host agent sds or

00:21:04,640 --> 00:21:08,559
agent and they also check listeners at

00:21:07,280 --> 00:21:12,400
both l7 and

00:21:08,559 --> 00:21:13,520
l4 and as we implement that as that

00:21:12,400 --> 00:21:17,360
script so that

00:21:13,520 --> 00:21:21,919
each platform can invoke it uh easily as

00:21:17,360 --> 00:21:24,240
it does not not depend on any platform

00:21:21,919 --> 00:21:26,000
so real-time feedback the last thing is

00:21:24,240 --> 00:21:28,640
the confidence

00:21:26,000 --> 00:21:30,240
so when how does the pipeline build the

00:21:28,640 --> 00:21:33,039
confidence it's built on

00:21:30,240 --> 00:21:33,679
it's on top of confidence so on our way

00:21:33,039 --> 00:21:35,760
report

00:21:33,679 --> 00:21:37,360
to peak every 30 seconds speaking to

00:21:35,760 --> 00:21:39,679
report the centralized country plan

00:21:37,360 --> 00:21:42,080
which is the tower every 30 seconds

00:21:39,679 --> 00:21:43,679
so in total after one minute you should

00:21:42,080 --> 00:21:47,360
be able to see the

00:21:43,679 --> 00:21:48,799
see the third last result after you send

00:21:47,360 --> 00:21:51,520
out

00:21:48,799 --> 00:21:52,400
the car new configuration so starting

00:21:51,520 --> 00:21:54,480
from the for

00:21:52,400 --> 00:21:55,600
t plus one minute you start building

00:21:54,480 --> 00:21:57,919
confidence

00:21:55,600 --> 00:21:59,039
and after another minute which is the t

00:21:57,919 --> 00:22:01,280
plus two minutes

00:21:59,039 --> 00:22:02,880
they will start they will look at the

00:22:01,280 --> 00:22:05,039
confidence say oh if i get

00:22:02,880 --> 00:22:06,159
high confidence you will see oh every

00:22:05,039 --> 00:22:08,799
four hour four

00:22:06,159 --> 00:22:10,240
if this configuration ruled out touch

00:22:08,799 --> 00:22:14,159
1000 clusters

00:22:10,240 --> 00:22:16,799
and all of them has more than 90 percent

00:22:14,159 --> 00:22:17,440
acceptance rate then we got enough

00:22:16,799 --> 00:22:20,240
confidence

00:22:17,440 --> 00:22:21,679
and then let's exit early without have

00:22:20,240 --> 00:22:23,840
to wait

00:22:21,679 --> 00:22:25,440
just go to let's go just move to our

00:22:23,840 --> 00:22:28,880
next stage

00:22:25,440 --> 00:22:30,720
or if it did not get

00:22:28,880 --> 00:22:32,320
enough confidence just keep waiting

00:22:30,720 --> 00:22:33,120
people keep looking keep looking

00:22:32,320 --> 00:22:36,159
completely

00:22:33,120 --> 00:22:40,159
and after five minutes it will come out

00:22:36,159 --> 00:22:42,720
and it will fail and the world cup

00:22:40,159 --> 00:22:43,360
contact the on-call engineer to take a

00:22:42,720 --> 00:22:46,960
look what's

00:22:43,360 --> 00:22:50,720
going on so

00:22:46,960 --> 00:22:53,760
with regards to the visibility

00:22:50,720 --> 00:22:56,320
since everyone uses slack we

00:22:53,760 --> 00:22:56,799
send a select message on every ad on

00:22:56,320 --> 00:23:01,600
every

00:22:56,799 --> 00:23:03,679
deployment activity to our channel

00:23:01,600 --> 00:23:05,840
from the right side you can see that so

00:23:03,679 --> 00:23:08,559
the mesh configuration was ruled out

00:23:05,840 --> 00:23:09,200
and there is notification got sent out

00:23:08,559 --> 00:23:11,760
and

00:23:09,200 --> 00:23:13,360
after it finished the latest stage it

00:23:11,760 --> 00:23:16,799
sent another notification

00:23:13,360 --> 00:23:17,440
see oh please approve this change to

00:23:16,799 --> 00:23:20,320
develop

00:23:17,440 --> 00:23:21,039
and canary and as you can see someone

00:23:20,320 --> 00:23:24,400
clicked up

00:23:21,039 --> 00:23:27,280
accept and it moved to pro pro

00:23:24,400 --> 00:23:27,840
and it has been very wise though it

00:23:27,280 --> 00:23:29,360
takes

00:23:27,840 --> 00:23:32,480
several minutes before it's actually

00:23:29,360 --> 00:23:35,440
finished about actually 10 minutes maybe

00:23:32,480 --> 00:23:36,960
the finished prod and it sends cinecom

00:23:35,440 --> 00:23:40,320
congratulations the app

00:23:36,960 --> 00:23:42,240
everything was good and on the left side

00:23:40,320 --> 00:23:44,640
you can see that

00:23:42,240 --> 00:23:45,360
the configuration actually had a problem

00:23:44,640 --> 00:23:47,760
and that

00:23:45,360 --> 00:23:49,120
prop pipeline detected that problem and

00:23:47,760 --> 00:23:51,679
some notification

00:23:49,120 --> 00:23:53,120
see there is the arrow pressing masking

00:23:51,679 --> 00:23:56,799
and it has that

00:23:53,120 --> 00:23:58,400
eok link so that the angle engineer can

00:23:56,799 --> 00:24:00,880
click that and at that

00:23:58,400 --> 00:24:04,000
link and see what the detail works see

00:24:00,880 --> 00:24:07,840
the detailed arrow

00:24:04,000 --> 00:24:08,880
so in summary we have a thousand

00:24:07,840 --> 00:24:13,120
clusters

00:24:08,880 --> 00:24:16,799
and uh which serves millions of

00:24:13,120 --> 00:24:19,679
uh rps at the edge side and

00:24:16,799 --> 00:24:20,159
from the change landing time to its

00:24:19,679 --> 00:24:23,200
fully

00:24:20,159 --> 00:24:26,400
activity production uh if everything

00:24:23,200 --> 00:24:29,039
goes well it's less than 30 minutes

00:24:26,400 --> 00:24:30,320
and we had we just finished the week two

00:24:29,039 --> 00:24:33,120
two week three uh

00:24:30,320 --> 00:24:33,440
api and configuration migration and we

00:24:33,120 --> 00:24:35,520
had

00:24:33,440 --> 00:24:37,679
zero incident thanks to the

00:24:35,520 --> 00:24:41,600
configuration pipeline

00:24:37,679 --> 00:24:45,520
and that for the near real-time

00:24:41,600 --> 00:24:47,679
feedback loop so

00:24:45,520 --> 00:24:48,799
because we have built the configuration

00:24:47,679 --> 00:24:51,840
the control plane

00:24:48,799 --> 00:24:53,360
to be a generic resource distribution

00:24:51,840 --> 00:24:56,400
service

00:24:53,360 --> 00:25:00,080
instead of just for xds

00:24:56,400 --> 00:25:02,240
um other teams and uh from other

00:25:00,080 --> 00:25:03,679
operations our organizations are seeing

00:25:02,240 --> 00:25:06,799
value of this

00:25:03,679 --> 00:25:09,120
this system and they are moving

00:25:06,799 --> 00:25:11,440
they're moving their existing service

00:25:09,120 --> 00:25:14,320
their their configuration into this

00:25:11,440 --> 00:25:16,640
the mesh control plan so that they

00:25:14,320 --> 00:25:17,440
because they also want their super easy

00:25:16,640 --> 00:25:20,880
validation

00:25:17,440 --> 00:25:20,880
for easy protection

00:25:21,440 --> 00:25:26,400
so last some best practice and some

00:25:24,080 --> 00:25:28,960
lessons we learned

00:25:26,400 --> 00:25:31,039
first infrared infrastructure governance

00:25:28,960 --> 00:25:32,720
is crucial this is very important for

00:25:31,039 --> 00:25:35,919
our success

00:25:32,720 --> 00:25:36,720
so let's admit today service match is

00:25:35,919 --> 00:25:39,840
still

00:25:36,720 --> 00:25:41,760
a quick evolving world therefore

00:25:39,840 --> 00:25:42,880
your changes are established most of the

00:25:41,760 --> 00:25:46,240
time

00:25:42,880 --> 00:25:46,880
and two years ago we have been talking

00:25:46,240 --> 00:25:50,320
about

00:25:46,880 --> 00:25:53,679
xds xds v2 last year we talked about

00:25:50,320 --> 00:25:57,120
v3 we talked about ui uh

00:25:53,679 --> 00:25:57,520
universal data plan api and this year if

00:25:57,120 --> 00:26:01,440
look

00:25:57,520 --> 00:26:05,919
where if you look at xts

00:26:01,440 --> 00:26:08,720
you your udp is already everywhere

00:26:05,919 --> 00:26:09,440
and because because of that we need to

00:26:08,720 --> 00:26:12,720
define

00:26:09,440 --> 00:26:13,360
clear boundary between each row between

00:26:12,720 --> 00:26:16,640
each team

00:26:13,360 --> 00:26:19,360
and who is what otherwise

00:26:16,640 --> 00:26:20,640
you will run into fragmentation that's

00:26:19,360 --> 00:26:24,400
the garment

00:26:20,640 --> 00:26:27,440
make your mesh unmanageable someday

00:26:24,400 --> 00:26:29,679
and that's i cannot amplify amorphous

00:26:27,440 --> 00:26:33,679
side this enough

00:26:29,679 --> 00:26:37,200
and you will know and the next one is

00:26:33,679 --> 00:26:39,440
mtrs it's your good friend if it's not

00:26:37,200 --> 00:26:42,480
the best

00:26:39,440 --> 00:26:45,039
we have 99 problem with

00:26:42,480 --> 00:26:46,080
our control plan which delay the eds

00:26:45,039 --> 00:26:48,240
update

00:26:46,080 --> 00:26:50,480
make sure this made certain endpoint

00:26:48,240 --> 00:26:53,039
data stored

00:26:50,480 --> 00:26:53,760
and because of that if we did not have x

00:26:53,039 --> 00:26:57,200
there i

00:26:53,760 --> 00:26:59,840
did not have mtrs we would have

00:26:57,200 --> 00:27:00,480
accessory because the requests are out

00:26:59,840 --> 00:27:04,000
routed

00:27:00,480 --> 00:27:06,240
to incorrect incorrect service

00:27:04,000 --> 00:27:08,240
however because we because we added

00:27:06,240 --> 00:27:11,360
mutual trs

00:27:08,240 --> 00:27:14,799
so the connections was blocked at the

00:27:11,360 --> 00:27:17,120
layer four so layer seven even

00:27:14,799 --> 00:27:18,720
did not even get a chance to talk to the

00:27:17,120 --> 00:27:21,360
wrong endpoints

00:27:18,720 --> 00:27:22,720
we did not have had a cell 0 because of

00:27:21,360 --> 00:27:25,279
the big card because of the

00:27:22,720 --> 00:27:25,919
the bug is that the only thing we saw

00:27:25,279 --> 00:27:29,600
was

00:27:25,919 --> 00:27:33,440
some a slightly increased latency

00:27:29,600 --> 00:27:36,480
at the api service other than that

00:27:33,440 --> 00:27:39,840
no problem so it actually helped

00:27:36,480 --> 00:27:43,360
us with i7 it's not just

00:27:39,840 --> 00:27:43,360
for authentication

00:27:43,760 --> 00:27:49,919
another one is rtds rtds

00:27:46,799 --> 00:27:51,279
is very powerful and sometimes it's very

00:27:49,919 --> 00:27:54,960
dangerous

00:27:51,279 --> 00:27:57,279
so here is a sad story we added a global

00:27:54,960 --> 00:28:00,399
layer

00:27:57,279 --> 00:28:02,320
for our kit for our envoy

00:28:00,399 --> 00:28:04,320
and we saw that it would be a good idea

00:28:02,320 --> 00:28:07,360
if we were to draw our sun

00:28:04,320 --> 00:28:11,760
throughout something globally

00:28:07,360 --> 00:28:13,679
and someday one of our developers

00:28:11,760 --> 00:28:15,520
want to change something in the global

00:28:13,679 --> 00:28:18,559
entry global layer

00:28:15,520 --> 00:28:19,039
and he typed in a command like command

00:28:18,559 --> 00:28:22,720
line

00:28:19,039 --> 00:28:27,200
and however he forgot to quote that

00:28:22,720 --> 00:28:29,919
json stream in a quote well could pair

00:28:27,200 --> 00:28:30,960
and this sent a more formatted json to

00:28:29,919 --> 00:28:34,480
avoid

00:28:30,960 --> 00:28:37,600
within two seconds every envoy

00:28:34,480 --> 00:28:40,720
of pinterest got this wrong value within

00:28:37,600 --> 00:28:43,919
two seconds and sadly

00:28:40,720 --> 00:28:47,279
there is a bug in hawaii which cause

00:28:43,919 --> 00:28:49,760
which caused a catastrophic

00:28:47,279 --> 00:28:50,880
failure cause our trafficker backs a

00:28:49,760 --> 00:28:54,720
backtracking

00:28:50,880 --> 00:28:57,840
bug which caused our out of my memory

00:28:54,720 --> 00:28:59,039
within one minute so how my crap crashes

00:28:57,840 --> 00:29:01,840
because of our oom

00:28:59,039 --> 00:29:02,480
and they restarted and they'll start or

00:29:01,840 --> 00:29:05,360
work for

00:29:02,480 --> 00:29:06,000
maybe 30 minutes 30 seconds then crash

00:29:05,360 --> 00:29:09,840
again

00:29:06,000 --> 00:29:13,279
so i crashed every minute and it took us

00:29:09,840 --> 00:29:16,880
a lot of effort and almost 30 minutes

00:29:13,279 --> 00:29:17,760
to recover every alloy so it's a sad

00:29:16,880 --> 00:29:21,279
story

00:29:17,760 --> 00:29:24,640
the lesson learned here is

00:29:21,279 --> 00:29:25,360
rtds is powerful but you feel not

00:29:24,640 --> 00:29:28,480
necessary

00:29:25,360 --> 00:29:28,799
don't use a global layer actually after

00:29:28,480 --> 00:29:31,919
this

00:29:28,799 --> 00:29:32,480
incident we disabled the global layer

00:29:31,919 --> 00:29:37,760
and

00:29:32,480 --> 00:29:37,760

YouTube URL: https://www.youtube.com/watch?v=YBnR09EEYMU


