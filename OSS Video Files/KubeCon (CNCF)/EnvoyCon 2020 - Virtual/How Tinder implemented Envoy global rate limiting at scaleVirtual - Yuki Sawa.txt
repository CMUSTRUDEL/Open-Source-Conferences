Title: How Tinder implemented Envoy global rate limiting at scaleVirtual - Yuki Sawa
Publication date: 2020-10-21
Playlist: EnvoyCon 2020 - Virtual
Description: 
	How Tinder implemented Envoy global rate limiting at scale
Virtual - Yuki Sawa

Tinder recently completed a migration to Envoy based service mesh in their Kubernetes based infrastructure. A big win was moving rate limiting logic out of the application and into the network layer by leveraging Envoy's powerful global rate limiting capabilities. Previously implementations relied on home-grown code inside the application or features built into proxies like Nginx â€” which were difficult to maintain and did not offer the configurability and observability of Envoy. This talk covers how Envoy global rate limiting works at Tinder, how we migrated to it, and what steps were taken to ensure it performs at scale. We'll also discuss the unique rate limiting features available in Envoy, how to configure it and how we extended upon it.
Captions: 
	00:00:01,599 --> 00:00:04,960
hi i'm yuki and today i'll be talking

00:00:03,360 --> 00:00:06,720
about how tinder implemented envoy

00:00:04,960 --> 00:00:08,559
global rate limiting at scale

00:00:06,720 --> 00:00:10,080
i'm a software engineer at tinder in the

00:00:08,559 --> 00:00:12,000
cloud infrastructure team

00:00:10,080 --> 00:00:13,759
and in my day-to-day i work on

00:00:12,000 --> 00:00:16,080
kubernetes and envoy

00:00:13,759 --> 00:00:18,560
so in particular i've implemented the

00:00:16,080 --> 00:00:20,880
xts control plane for our service mesh

00:00:18,560 --> 00:00:22,720
which all our envoys talk to and today

00:00:20,880 --> 00:00:24,640
i'm going to be going into detail about

00:00:22,720 --> 00:00:27,599
the rate limiting platform i built there

00:00:24,640 --> 00:00:28,960
which is also based on envoy

00:00:27,599 --> 00:00:30,960
so in summary i'll be talking about

00:00:28,960 --> 00:00:32,800
these things first of all our service

00:00:30,960 --> 00:00:35,200
mesh journey how we implemented

00:00:32,800 --> 00:00:36,880
envoy across all our services the

00:00:35,200 --> 00:00:38,320
problems with our previous rate limiting

00:00:36,880 --> 00:00:40,800
implementations

00:00:38,320 --> 00:00:43,200
envoy rate milling why it's good how

00:00:40,800 --> 00:00:45,039
envoy rate limiting works in detail how

00:00:43,200 --> 00:00:46,960
tinder's rate limiting

00:00:45,039 --> 00:00:49,039
works how we've extended upon it and

00:00:46,960 --> 00:00:51,039
just some tips on if you want to try it

00:00:49,039 --> 00:00:53,280
out yourself

00:00:51,039 --> 00:00:55,360
so tinder's service mesh journey we were

00:00:53,280 --> 00:00:58,800
fully on kubernetes before starting

00:00:55,360 --> 00:00:59,120
a service method migration so we started

00:00:58,800 --> 00:01:01,120
the

00:00:59,120 --> 00:01:02,559
service mesh migration about a year ago

00:01:01,120 --> 00:01:05,119
in 2019 it took about

00:01:02,559 --> 00:01:05,680
a full year for all our services fully

00:01:05,119 --> 00:01:07,840
to be on

00:01:05,680 --> 00:01:09,600
envoy it was definitely a very big

00:01:07,840 --> 00:01:12,479
effort took many people in the

00:01:09,600 --> 00:01:14,799
organization to achieve it and

00:01:12,479 --> 00:01:15,600
today about 1.6 million requests per

00:01:14,799 --> 00:01:17,920
second

00:01:15,600 --> 00:01:18,640
are meshed you know in our service mesh

00:01:17,920 --> 00:01:21,040
at peak

00:01:18,640 --> 00:01:23,439
and there's about 200 000 containers in

00:01:21,040 --> 00:01:25,439
our service mesh

00:01:23,439 --> 00:01:26,799
so this is just a chart of how our

00:01:25,439 --> 00:01:29,200
infrastructure works

00:01:26,799 --> 00:01:30,400
on the left is a user it's using the

00:01:29,200 --> 00:01:32,560
tinder app

00:01:30,400 --> 00:01:35,119
the request gets routed to one of our

00:01:32,560 --> 00:01:38,400
three clusters so we have a cluster per

00:01:35,119 --> 00:01:40,320
availability zone in aws there's dns

00:01:38,400 --> 00:01:41,759
round robin that routes the request to

00:01:40,320 --> 00:01:44,479
one of these clusters

00:01:41,759 --> 00:01:45,920
and each cluster is isolated so they do

00:01:44,479 --> 00:01:47,840
not talk to each other

00:01:45,920 --> 00:01:48,960
this gives us a lot of like benefits in

00:01:47,840 --> 00:01:52,000
terms of performance

00:01:48,960 --> 00:01:53,119
as well as like traffic bandwidth cost

00:01:52,000 --> 00:01:54,880
savings

00:01:53,119 --> 00:01:57,520
we have an ingress layer which used to

00:01:54,880 --> 00:01:59,600
be nginx but now we might get the envoy

00:01:57,520 --> 00:02:01,360
and once the request makes it into a

00:01:59,600 --> 00:02:04,320
kubernetes cluster of course

00:02:01,360 --> 00:02:06,399
the request is routed to the correct

00:02:04,320 --> 00:02:10,000
service

00:02:06,399 --> 00:02:12,160
so we tried out envoy because

00:02:10,000 --> 00:02:13,599
initially the default kubernetes elb

00:02:12,160 --> 00:02:16,879
routing was not that great

00:02:13,599 --> 00:02:18,959
it was very uneven and resulted in uh

00:02:16,879 --> 00:02:21,200
hotpot issues where pods some pods got a

00:02:18,959 --> 00:02:22,720
lot more requests than others

00:02:21,200 --> 00:02:25,120
but eventually you know we started

00:02:22,720 --> 00:02:27,840
trying out a lot of the other great

00:02:25,120 --> 00:02:29,599
features envoy has like these requests

00:02:27,840 --> 00:02:31,680
routing first of all right

00:02:29,599 --> 00:02:32,800
uh there's retries circuit breakers

00:02:31,680 --> 00:02:35,120
timeouts that you

00:02:32,800 --> 00:02:37,040
essentially get for free now you're not

00:02:35,120 --> 00:02:40,080
having to do all this network

00:02:37,040 --> 00:02:42,000
automation inside the application it's

00:02:40,080 --> 00:02:44,239
all on the envoy network layer

00:02:42,000 --> 00:02:45,519
now we're exploring like the redis full

00:02:44,239 --> 00:02:48,080
red filter

00:02:45,519 --> 00:02:50,160
dynamodb filter where you're proxying

00:02:48,080 --> 00:02:52,400
database requests through envoy

00:02:50,160 --> 00:02:53,440
that way you get like metrics for free

00:02:52,400 --> 00:02:55,760
uh now we're

00:02:53,440 --> 00:02:56,640
uh of course we're doing observability

00:02:55,760 --> 00:02:59,440
where

00:02:56,640 --> 00:03:01,680
all our envoy metrics are being scraped

00:02:59,440 --> 00:03:04,560
through prometheus and then charted on

00:03:01,680 --> 00:03:05,680
grafana so previous rate limiting

00:03:04,560 --> 00:03:08,720
implementations

00:03:05,680 --> 00:03:10,879
a tinder used nginx at the ingress layer

00:03:08,720 --> 00:03:12,480
problem with this is uh there was really

00:03:10,879 --> 00:03:14,239
no visibility you would need to like

00:03:12,480 --> 00:03:15,840
tell logs to figure out if the rate

00:03:14,239 --> 00:03:17,599
limiting was even working or you know

00:03:15,840 --> 00:03:20,400
what's being rate limited

00:03:17,599 --> 00:03:21,200
um so that's not ideal it was all done

00:03:20,400 --> 00:03:23,280
locally

00:03:21,200 --> 00:03:24,879
as in it was based on the number of

00:03:23,280 --> 00:03:27,440
requests uh

00:03:24,879 --> 00:03:28,879
certain nginx hosts saw not on the on

00:03:27,440 --> 00:03:30,560
the global request count

00:03:28,879 --> 00:03:31,920
it was also very difficult to update the

00:03:30,560 --> 00:03:32,319
rate limits because you need to you know

00:03:31,920 --> 00:03:36,080
roll

00:03:32,319 --> 00:03:37,840
all the nginx hosts

00:03:36,080 --> 00:03:40,159
and then secondly there were

00:03:37,840 --> 00:03:41,920
implementations inside the application

00:03:40,159 --> 00:03:44,000
so a lot of teams would like roll their

00:03:41,920 --> 00:03:45,440
own rate limiting code

00:03:44,000 --> 00:03:47,440
so there's a lot of different

00:03:45,440 --> 00:03:50,239
implementations across the org

00:03:47,440 --> 00:03:51,280
there's little visibility into how they

00:03:50,239 --> 00:03:53,280
were working

00:03:51,280 --> 00:03:55,840
or how they were configured and often

00:03:53,280 --> 00:03:57,040
there was redundant infrastructure

00:03:55,840 --> 00:03:58,959
you know because a lot of teams would

00:03:57,040 --> 00:04:01,280
use reddish caches for the rate limiting

00:03:58,959 --> 00:04:05,040
and there were duplicates of those

00:04:01,280 --> 00:04:07,360
basically serving the same purpose so

00:04:05,040 --> 00:04:09,360
why is envoy rate limiting good we were

00:04:07,360 --> 00:04:11,840
able to move all the rate limiting logic

00:04:09,360 --> 00:04:14,080
to the envoy layer so we have a uniform

00:04:11,840 --> 00:04:14,640
implementation across the company we

00:04:14,080 --> 00:04:17,840
have

00:04:14,640 --> 00:04:20,320
global rate limiting where the rate

00:04:17,840 --> 00:04:22,000
limit is based on a global request count

00:04:20,320 --> 00:04:23,680
and this is really important for our

00:04:22,000 --> 00:04:26,160
cluster per ac model

00:04:23,680 --> 00:04:27,120
it's got granular configuration such as

00:04:26,160 --> 00:04:28,880
you can

00:04:27,120 --> 00:04:31,520
rate limit on multiple headers or one

00:04:28,880 --> 00:04:33,520
header or you know even no headers

00:04:31,520 --> 00:04:34,720
you have monitoring and visibility

00:04:33,520 --> 00:04:37,919
through the prometheus

00:04:34,720 --> 00:04:39,600
metrics it offers and overall we're able

00:04:37,919 --> 00:04:42,880
to prevent system failure

00:04:39,600 --> 00:04:45,040
patch concurrency issues and due to

00:04:42,880 --> 00:04:46,639
you know stopping like spot traffic we

00:04:45,040 --> 00:04:49,280
have a lot of cost savings

00:04:46,639 --> 00:04:52,320
and right now it polices about you know

00:04:49,280 --> 00:04:53,840
200 000 requests per second at tinder

00:04:52,320 --> 00:04:56,880
so this is a chart of onward rate

00:04:53,840 --> 00:04:58,720
limiting it's a request flow of

00:04:56,880 --> 00:05:00,639
service a making a request to service b

00:04:58,720 --> 00:05:01,680
so let's start at one a makes a request

00:05:00,639 --> 00:05:04,479
to b

00:05:01,680 --> 00:05:06,720
and service a has an envoy sidecar right

00:05:04,479 --> 00:05:08,000
so the envoy sidecar asks the rate limit

00:05:06,720 --> 00:05:10,240
service

00:05:08,000 --> 00:05:11,600
should we rate limit this request or not

00:05:10,240 --> 00:05:14,400
if the answer is yes

00:05:11,600 --> 00:05:15,280
a 429 status code is returned to the

00:05:14,400 --> 00:05:18,320
service a

00:05:15,280 --> 00:05:18,800
container otherwise the request is let

00:05:18,320 --> 00:05:22,160
through

00:05:18,800 --> 00:05:24,240
and a successfully makes a request to b

00:05:22,160 --> 00:05:26,800
and you can notice here that on the very

00:05:24,240 --> 00:05:29,280
right the rate limit service is

00:05:26,800 --> 00:05:32,880
uh storing all the request count

00:05:29,280 --> 00:05:32,880
information in a redis cache

00:05:33,360 --> 00:05:36,800
okay so let's talk about the rate limit

00:05:34,800 --> 00:05:38,720
service it is a go project

00:05:36,800 --> 00:05:40,400
and deploy it in kubernetes pause with

00:05:38,720 --> 00:05:43,600
an envoy site card

00:05:40,400 --> 00:05:46,400
and this way we can proxy requests

00:05:43,600 --> 00:05:48,000
to redis through envoy which gives us

00:05:46,400 --> 00:05:49,919
metrics for free

00:05:48,000 --> 00:05:51,520
as well as being able to tweak some

00:05:49,919 --> 00:05:53,360
redis connection settings

00:05:51,520 --> 00:05:55,600
we have two separate rate limiting

00:05:53,360 --> 00:05:58,319
clusters one for internal routes

00:05:55,600 --> 00:05:59,759
another for external routes and then you

00:05:58,319 --> 00:06:01,360
might be wondering what happens if the

00:05:59,759 --> 00:06:03,120
rate from the service is down or if it's

00:06:01,360 --> 00:06:04,880
slowing down

00:06:03,120 --> 00:06:06,479
there is a 20 millisecond default

00:06:04,880 --> 00:06:08,319
timeout to request

00:06:06,479 --> 00:06:09,680
to the rate limit service if it is

00:06:08,319 --> 00:06:12,639
exceeded there is a fail

00:06:09,680 --> 00:06:14,160
open meaning that the requests will be

00:06:12,639 --> 00:06:16,560
allowed through by default

00:06:14,160 --> 00:06:17,759
and what's nice is that just by updating

00:06:16,560 --> 00:06:20,479
the rate limit

00:06:17,759 --> 00:06:22,720
service comic map it is hot reloaded so

00:06:20,479 --> 00:06:24,639
if you're changing from say like

00:06:22,720 --> 00:06:26,479
uh rate limit from five requests per

00:06:24,639 --> 00:06:27,840
second to 10 requests per second just

00:06:26,479 --> 00:06:30,880
update the config map

00:06:27,840 --> 00:06:32,960
and it will reload automatically so

00:06:30,880 --> 00:06:34,800
let's look at the configuration

00:06:32,960 --> 00:06:37,039
it's important to note all the envoy

00:06:34,800 --> 00:06:40,160
configuration is done in the caller

00:06:37,039 --> 00:06:42,960
so here with this route you're attaching

00:06:40,160 --> 00:06:45,840
a descriptor key called foo

00:06:42,960 --> 00:06:47,759
if that request has a header name of foo

00:06:45,840 --> 00:06:52,000
if it has a request with a header

00:06:47,759 --> 00:06:53,919
called foo that is and then

00:06:52,000 --> 00:06:55,120
with that descriptor it is sent to the

00:06:53,919 --> 00:06:56,639
rate limit service and here's the

00:06:55,120 --> 00:06:57,840
corresponding weight limit service

00:06:56,639 --> 00:06:59,680
config

00:06:57,840 --> 00:07:01,360
you can see here that it has a nested

00:06:59,680 --> 00:07:04,000
structure which allows for pretty

00:07:01,360 --> 00:07:06,960
complex uh rate limiting logic

00:07:04,000 --> 00:07:08,000
and then here for the food key there is

00:07:06,960 --> 00:07:10,000
a corresponding

00:07:08,000 --> 00:07:11,199
two weeks two requests per minute

00:07:10,000 --> 00:07:12,639
specified so

00:07:11,199 --> 00:07:17,120
in the rate limit service config is

00:07:12,639 --> 00:07:18,880
where you define the thresholds

00:07:17,120 --> 00:07:20,240
so let's talk about one use case at

00:07:18,880 --> 00:07:22,880
tinder which is

00:07:20,240 --> 00:07:24,639
sms request rate limiting so what

00:07:22,880 --> 00:07:25,759
happens is we have a lot of bots

00:07:24,639 --> 00:07:27,919
requesting a lot of

00:07:25,759 --> 00:07:29,360
sms codes which is very expensive

00:07:27,919 --> 00:07:30,960
because twilios is

00:07:29,360 --> 00:07:33,120
twilio is expensive which is our

00:07:30,960 --> 00:07:35,440
third-party sms provider

00:07:33,120 --> 00:07:36,720
and initially we did have this lately

00:07:35,440 --> 00:07:38,960
winning built into

00:07:36,720 --> 00:07:40,639
our applications but it was brittle hard

00:07:38,960 --> 00:07:42,000
to update every time you want to add a

00:07:40,639 --> 00:07:43,120
new rate limit you'd have to write

00:07:42,000 --> 00:07:45,599
additional code

00:07:43,120 --> 00:07:47,199
was not ideal so we migrated all of it

00:07:45,599 --> 00:07:49,840
to envoy which

00:07:47,199 --> 00:07:52,000
had uh millions of dollars in savings

00:07:49,840 --> 00:07:55,440
and we had these very adaptive

00:07:52,000 --> 00:07:57,360
rate limits so for each ip it had a per

00:07:55,440 --> 00:07:59,520
day and a per second quota

00:07:57,360 --> 00:08:00,400
and we could rate limit on a combination

00:07:59,520 --> 00:08:03,520
of headers

00:08:00,400 --> 00:08:07,440
so if one user had an ip of one one one

00:08:03,520 --> 00:08:09,280
one the country is us devices ios

00:08:07,440 --> 00:08:10,639
they would have like a five request per

00:08:09,280 --> 00:08:12,639
second rate limit

00:08:10,639 --> 00:08:14,879
while if they had a different ip address

00:08:12,639 --> 00:08:18,160
their country is in japan the devices

00:08:14,879 --> 00:08:20,319
is a web they would have a lower rate

00:08:18,160 --> 00:08:23,199
limit

00:08:20,319 --> 00:08:24,879
so on top of this we also built in a

00:08:23,199 --> 00:08:26,960
analytics module

00:08:24,879 --> 00:08:28,800
into the rate limit service so every

00:08:26,960 --> 00:08:30,879
time there is a rate limit event it is

00:08:28,800 --> 00:08:33,279
sent to s3 for processing

00:08:30,879 --> 00:08:35,200
and long term storage so what we can do

00:08:33,279 --> 00:08:38,000
is automated block listing

00:08:35,200 --> 00:08:39,039
so if like a user was rate limited three

00:08:38,000 --> 00:08:41,120
days in a row

00:08:39,039 --> 00:08:42,240
we may ban them we may ban them for like

00:08:41,120 --> 00:08:44,560
30 days

00:08:42,240 --> 00:08:45,279
uh and now we can also analyze long-term

00:08:44,560 --> 00:08:47,680
behavior

00:08:45,279 --> 00:08:48,560
we can you know say you know how did the

00:08:47,680 --> 00:08:50,560
rate limiting

00:08:48,560 --> 00:08:52,240
perform like 90 days ago who got rate

00:08:50,560 --> 00:08:53,519
limited on what routes

00:08:52,240 --> 00:08:55,360
now we're also starting to be like

00:08:53,519 --> 00:08:58,160
machine learning on this data

00:08:55,360 --> 00:09:00,160
to enrich our existing bot detection so

00:08:58,160 --> 00:09:03,680
we're finding a lot of value in this

00:09:00,160 --> 00:09:06,560
weight limiting data so this is just a

00:09:03,680 --> 00:09:06,959
cool little chart i made so i got all

00:09:06,560 --> 00:09:09,040
the

00:09:06,959 --> 00:09:11,120
requests that got rate limited got their

00:09:09,040 --> 00:09:12,480
geo locational data and put it on this

00:09:11,120 --> 00:09:14,240
world map

00:09:12,480 --> 00:09:16,240
you can see that there's really bots

00:09:14,240 --> 00:09:19,200
coming from everywhere

00:09:16,240 --> 00:09:20,560
particularly around where aws data

00:09:19,200 --> 00:09:22,800
centers are located

00:09:20,560 --> 00:09:24,480
as you can see there's a couple big dots

00:09:22,800 --> 00:09:27,680
in virginia where

00:09:24,480 --> 00:09:30,399
aws usc 1 is

00:09:27,680 --> 00:09:31,120
and then just lastly some tips i would

00:09:30,399 --> 00:09:33,279
scale your

00:09:31,120 --> 00:09:35,200
infrastructure so that your p99 9

00:09:33,279 --> 00:09:37,360
latency to the rate limit service is

00:09:35,200 --> 00:09:38,800
20 milliseconds and not much more than

00:09:37,360 --> 00:09:40,880
that otherwise you're going to have a

00:09:38,800 --> 00:09:42,880
lot of fail opens

00:09:40,880 --> 00:09:45,440
5 000 quests per second per rate in the

00:09:42,880 --> 00:09:47,440
service pod is a pretty good benchmark

00:09:45,440 --> 00:09:50,240
i would use the envoy redis filter to

00:09:47,440 --> 00:09:52,080
make your life a lot easier

00:09:50,240 --> 00:09:53,760
particularly because this gives you

00:09:52,080 --> 00:09:56,240
redis performance metrics

00:09:53,760 --> 00:09:57,040
and how redis performs is going to

00:09:56,240 --> 00:10:00,000
affect how

00:09:57,040 --> 00:10:01,839
your p99 latency is so if you want to

00:10:00,000 --> 00:10:02,640
just try it out on your laptop clone

00:10:01,839 --> 00:10:05,519
this repo

00:10:02,640 --> 00:10:07,600
run this docker compose command and you

00:10:05,519 --> 00:10:11,040
can just curl your local host

00:10:07,600 --> 00:10:14,320
and you should get a 429 response

00:10:11,040 --> 00:10:16,480
like you see here on the bottom right

00:10:14,320 --> 00:10:18,079
thank you that was all shoot me some

00:10:16,480 --> 00:10:21,600
questions on twitter if you got any

00:10:18,079 --> 00:10:21,600

YouTube URL: https://www.youtube.com/watch?v=2EKU8zCQAow


