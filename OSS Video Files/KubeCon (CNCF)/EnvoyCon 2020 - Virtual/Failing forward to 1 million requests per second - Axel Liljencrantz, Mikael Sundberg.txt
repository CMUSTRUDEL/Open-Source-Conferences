Title: Failing forward to 1 million requests per second - Axel Liljencrantz, Mikael Sundberg
Publication date: 2020-10-21
Playlist: EnvoyCon 2020 - Virtual
Description: 
	Failing forward to 1 million requests per second - Axel Liljencrantz, Mikael Sundberg

Many companies claim to have a work culture that celebrates failures, but few companies have tested that claim as thoroughly as Spotify did during our migration to Envoy. Come hear war stories of trying, failing, and failing some more with Envoy, and learn how to make sure you learn something new every time you fail.
Captions: 
	00:00:00,960 --> 00:00:07,120
hi my name is axel

00:00:03,600 --> 00:00:10,320
and my name is mike and we are here

00:00:07,120 --> 00:00:13,920
to tell you how to successfully fail at

00:00:10,320 --> 00:00:17,119
life the

00:00:13,920 --> 00:00:20,480
baseless claim that a company

00:00:17,119 --> 00:00:22,240
celebrates failure is becoming as common

00:00:20,480 --> 00:00:25,119
for trope in modern tech

00:00:22,240 --> 00:00:26,560
as have you tried turning it off and on

00:00:25,119 --> 00:00:31,199
again

00:00:26,560 --> 00:00:35,120
it turns out that repeated failure

00:00:31,199 --> 00:00:39,360
is not enough to ensure future success

00:00:35,120 --> 00:00:42,879
and in practice celebrating failure

00:00:39,360 --> 00:00:45,120
often amounts to not firing people who

00:00:42,879 --> 00:00:48,480
are bad at their job

00:00:45,120 --> 00:00:52,079
as such thinking about how to fail

00:00:48,480 --> 00:00:55,360
forward that is fail in a way

00:00:52,079 --> 00:00:56,399
that moves you meaningfully towards your

00:00:55,360 --> 00:00:59,760
goal

00:00:56,399 --> 00:01:03,280
is relevant this talk

00:00:59,760 --> 00:01:04,159
is about how we failed five consecutive

00:01:03,280 --> 00:01:08,159
deploys

00:01:04,159 --> 00:01:12,080
at spotify and why we're

00:01:08,159 --> 00:01:15,600
reasonably proud about it

00:01:12,080 --> 00:01:18,000
first a little background at spotify

00:01:15,600 --> 00:01:20,000
we have a google load balancer in front

00:01:18,000 --> 00:01:22,080
of our microservices

00:01:20,000 --> 00:01:24,479
and for a long time we've been wanting

00:01:22,080 --> 00:01:26,640
to run envoy as an http proxy at the

00:01:24,479 --> 00:01:28,479
perimeter of our back ends

00:01:26,640 --> 00:01:30,640
in addition to having a unified

00:01:28,479 --> 00:01:32,479
perimeter there is a long laundry list

00:01:30,640 --> 00:01:33,520
of futures we want to get out of this

00:01:32,479 --> 00:01:35,920
setup

00:01:33,520 --> 00:01:36,720
common metrics authentication rate

00:01:35,920 --> 00:01:39,759
limiting

00:01:36,720 --> 00:01:43,920
client ip lookups access logs

00:01:39,759 --> 00:01:45,520
and so on and as you might know

00:01:43,920 --> 00:01:48,159
envoy doesn't actually do all of those

00:01:45,520 --> 00:01:49,360
things our desired android setup

00:01:48,159 --> 00:01:51,920
contains a docker

00:01:49,360 --> 00:01:52,720
sidecar that runs a second service

00:01:51,920 --> 00:01:55,520
implementing

00:01:52,720 --> 00:01:56,880
authentication guip lookups and smarter

00:01:55,520 --> 00:01:59,040
things

00:01:56,880 --> 00:02:00,719
envoy will call this service for most

00:01:59,040 --> 00:02:04,560
incoming requests

00:02:00,719 --> 00:02:04,560
using the rtc extension

00:02:05,280 --> 00:02:10,399
we started this journey over a year ago

00:02:08,479 --> 00:02:12,239
by adding our first services behind

00:02:10,399 --> 00:02:14,800
envoy we have then

00:02:12,239 --> 00:02:15,599
gradually added more and more traffic

00:02:14,800 --> 00:02:18,160
and now

00:02:15,599 --> 00:02:20,879
it's time for the biggest deployment the

00:02:18,160 --> 00:02:23,440
http traffic from the actual spotify

00:02:20,879 --> 00:02:26,959
clients

00:02:23,440 --> 00:02:30,239
but before such an important move

00:02:26,959 --> 00:02:33,680
we wanted to do a fair bit of testing

00:02:30,239 --> 00:02:38,080
to build confidence that this would

00:02:33,680 --> 00:02:40,720
actually work we created a test setup

00:02:38,080 --> 00:02:41,440
that resembled the production setup as

00:02:40,720 --> 00:02:44,879
much

00:02:41,440 --> 00:02:48,319
as possible instead of a real client

00:02:44,879 --> 00:02:51,440
we use the wrk2 tool which does

00:02:48,319 --> 00:02:51,760
open loop testing that is it allows us

00:02:51,440 --> 00:02:54,959
to

00:02:51,760 --> 00:02:57,519
set the desired request rate instead of

00:02:54,959 --> 00:03:01,120
just trying to fully saturate the system

00:02:57,519 --> 00:03:03,519
this is almost always the correct way to

00:03:01,120 --> 00:03:06,319
load test the system

00:03:03,519 --> 00:03:07,120
our test setup uses the same load

00:03:06,319 --> 00:03:10,159
balancer

00:03:07,120 --> 00:03:10,959
as production an identically configured

00:03:10,159 --> 00:03:14,560
cluster

00:03:10,959 --> 00:03:17,760
but with only one host and various

00:03:14,560 --> 00:03:21,360
different core counts on that host

00:03:17,760 --> 00:03:22,080
finally our test used a single upstream

00:03:21,360 --> 00:03:25,599
service

00:03:22,080 --> 00:03:28,959
named no op no up is a service

00:03:25,599 --> 00:03:29,760
whose reply time status code and payload

00:03:28,959 --> 00:03:32,959
size

00:03:29,760 --> 00:03:35,360
can all be configured on each incoming

00:03:32,959 --> 00:03:38,879
request

00:03:35,360 --> 00:03:41,519
so what did we find

00:03:38,879 --> 00:03:42,000
first of all regardless of number of

00:03:41,519 --> 00:03:45,120
cores

00:03:42,000 --> 00:03:46,000
on the host metrics propagation always

00:03:45,120 --> 00:03:50,239
uses one

00:03:46,000 --> 00:03:51,840
full core secondly a few configuration

00:03:50,239 --> 00:03:54,560
bottlenecks were found

00:03:51,840 --> 00:03:55,680
the biggest one was the http thread pool

00:03:54,560 --> 00:03:59,519
in our off-c

00:03:55,680 --> 00:04:04,080
sidecar we tried eight

00:03:59,519 --> 00:04:07,599
32 and 64 core hosts and 32 cores

00:04:04,080 --> 00:04:10,239
offered the best throughput per core

00:04:07,599 --> 00:04:11,599
we saw some failure rate elevation on

00:04:10,239 --> 00:04:15,840
slow requests

00:04:11,599 --> 00:04:19,199
but we didn't investigate this further

00:04:15,840 --> 00:04:22,560
and finally we could see that tls

00:04:19,199 --> 00:04:26,080
used a bit more cpu than we expected

00:04:22,560 --> 00:04:30,080
in the flame graphs good enough

00:04:26,080 --> 00:04:33,120
let's go and we did and thanks to that

00:04:30,080 --> 00:04:36,479
we have this amazing news article

00:04:33,120 --> 00:04:39,280
and needless to say we had to roll back

00:04:36,479 --> 00:04:39,280
fairly quickly

00:04:39,919 --> 00:04:46,479
what was going on well it turns out

00:04:43,440 --> 00:04:48,960
that envoy's circuit breaker which

00:04:46,479 --> 00:04:49,680
is really just an outstanding request

00:04:48,960 --> 00:04:53,040
limit

00:04:49,680 --> 00:04:54,800
had triggered it has a default limit of

00:04:53,040 --> 00:04:58,400
1000 requests

00:04:54,800 --> 00:05:01,919
which we had not changed

00:04:58,400 --> 00:05:04,639
handling 30 to 30 000 rps

00:05:01,919 --> 00:05:05,600
per host means that average latency on

00:05:04,639 --> 00:05:09,199
your request

00:05:05,600 --> 00:05:11,919
must be lower than 30 milliseconds

00:05:09,199 --> 00:05:13,039
we had in fact checked that the median

00:05:11,919 --> 00:05:17,280
latency

00:05:13,039 --> 00:05:21,759
was much lower than that but as usual

00:05:17,280 --> 00:05:25,120
the long tail ruins everything

00:05:21,759 --> 00:05:28,400
so we had failed to check the right

00:05:25,120 --> 00:05:32,880
metric for this situation which is

00:05:28,400 --> 00:05:38,639
the average because

00:05:32,880 --> 00:05:40,720
onward doesn't report the average

00:05:38,639 --> 00:05:42,960
but the good news is we could

00:05:40,720 --> 00:05:44,720
successfully reproduce the problem in

00:05:42,960 --> 00:05:47,680
our test environment

00:05:44,720 --> 00:05:50,479
and we could validate that it went away

00:05:47,680 --> 00:05:53,199
when we adjusted the limit

00:05:50,479 --> 00:05:54,400
so we adjusted the production circuit

00:05:53,199 --> 00:05:57,360
breaker settings

00:05:54,400 --> 00:05:59,120
and we tried again and this time it

00:05:57,360 --> 00:06:01,840
worked

00:05:59,120 --> 00:06:01,840
for a few minutes

00:06:02,560 --> 00:06:08,240
it started out fine but as time went on

00:06:05,919 --> 00:06:11,120
we got more and more errors

00:06:08,240 --> 00:06:13,840
we noted this amazing graph showing the

00:06:11,120 --> 00:06:16,400
number of requests for each host

00:06:13,840 --> 00:06:18,720
basically the load balancer seems to

00:06:16,400 --> 00:06:22,000
throw all it can at a random host

00:06:18,720 --> 00:06:24,800
until it gets overloaded then it throw

00:06:22,000 --> 00:06:28,000
the load away at the other hosts instead

00:06:24,800 --> 00:06:31,199
spiraling into more and more 500 hours

00:06:28,000 --> 00:06:31,199
from overloaded hosts

00:06:31,440 --> 00:06:35,039
we adjusted our environment to be

00:06:33,440 --> 00:06:38,080
protected production

00:06:35,039 --> 00:06:40,240
by increasing its size from one to ten

00:06:38,080 --> 00:06:41,759
we could then see the same problem in

00:06:40,240 --> 00:06:44,560
our test environment

00:06:41,759 --> 00:06:46,720
and after some testing we figured that

00:06:44,560 --> 00:06:49,599
if we tell the load balancer to target

00:06:46,720 --> 00:06:52,560
15 000 requests per second for each host

00:06:49,599 --> 00:06:52,560
everything looks fine

00:06:52,880 --> 00:06:57,440
we had assumed that a single node test

00:06:55,440 --> 00:07:00,080
cluster would be fine

00:06:57,440 --> 00:07:02,000
looking back it feels pretty naive to

00:07:00,080 --> 00:07:04,560
use a single node

00:07:02,000 --> 00:07:05,840
but it's always easier when you have all

00:07:04,560 --> 00:07:07,599
the answers

00:07:05,840 --> 00:07:10,400
we didn't know that the load balancer

00:07:07,599 --> 00:07:13,039
considered a single node cluster special

00:07:10,400 --> 00:07:13,840
so we did fail to make most to make our

00:07:13,039 --> 00:07:16,800
test setup

00:07:13,840 --> 00:07:19,360
similar enough to production to save

00:07:16,800 --> 00:07:19,360
some money

00:07:19,599 --> 00:07:24,160
and slot link to 15 000 requests stop

00:07:22,720 --> 00:07:27,360
the flapping

00:07:24,160 --> 00:07:30,319
but we have poor research usage and

00:07:27,360 --> 00:07:33,840
an elevated failure rate clearly

00:07:30,319 --> 00:07:33,840
something still wasn't right

00:07:34,080 --> 00:07:40,240
but we didn't know where so

00:07:38,080 --> 00:07:41,199
we tried to isolate the different parts

00:07:40,240 --> 00:07:44,800
of the system

00:07:41,199 --> 00:07:46,960
to locate the bottleneck we started with

00:07:44,800 --> 00:07:50,319
the previously mentioned off z

00:07:46,960 --> 00:07:52,639
sidecar we disable it and

00:07:50,319 --> 00:07:54,000
rps went from fifteen thousand to twenty

00:07:52,639 --> 00:07:56,160
thousand

00:07:54,000 --> 00:07:58,479
this is expected since the number of

00:07:56,160 --> 00:08:01,599
messages that are processed by the host

00:07:58,479 --> 00:08:04,479
goes down significantly and

00:08:01,599 --> 00:08:05,360
also cpu usage during these load tests

00:08:04,479 --> 00:08:09,199
still stayed

00:08:05,360 --> 00:08:12,960
well below 50 percent so

00:08:09,199 --> 00:08:16,000
that was not the limiting factor

00:08:12,960 --> 00:08:17,120
next we turned our eyes to the no op

00:08:16,000 --> 00:08:20,960
service

00:08:17,120 --> 00:08:23,199
this fake service runs in kubernetes

00:08:20,960 --> 00:08:24,879
we did some quick profiling and found

00:08:23,199 --> 00:08:28,639
that each replica

00:08:24,879 --> 00:08:31,599
can handle 23 000 rps

00:08:28,639 --> 00:08:32,640
it is auto scaled with a maximum of 100

00:08:31,599 --> 00:08:35,279
replicas

00:08:32,640 --> 00:08:35,839
that means it can handle roughly 2.3

00:08:35,279 --> 00:08:41,839
million

00:08:35,839 --> 00:08:45,440
rps once again not the limiting factor

00:08:41,839 --> 00:08:49,519
most envoy users use the http 2 stack

00:08:45,440 --> 00:08:54,080
but envoy and our upstream uses

00:08:49,519 --> 00:08:58,000
http 1.1 perhaps the http 1.1 stack

00:08:54,080 --> 00:09:00,320
is somehow less scalable we ran a test

00:08:58,000 --> 00:09:02,240
where envoy directly responds to all

00:09:00,320 --> 00:09:06,080
requests

00:09:02,240 --> 00:09:08,160
thereby bypassing any http 1.1 stack

00:09:06,080 --> 00:09:09,920
and we found that we could handle only

00:09:08,160 --> 00:09:13,920
30 000 rps

00:09:09,920 --> 00:09:13,920
with a 10 cpu usage

00:09:14,320 --> 00:09:16,720
why

00:09:17,600 --> 00:09:22,000
this is the low point it is the part of

00:09:20,480 --> 00:09:25,519
the hero's journey

00:09:22,000 --> 00:09:27,120
known as the abyss it is

00:09:25,519 --> 00:09:29,680
where we considered giving up on

00:09:27,120 --> 00:09:31,040
software development and finding a brand

00:09:29,680 --> 00:09:34,839
new career

00:09:31,040 --> 00:09:37,839
one that makes sense like

00:09:34,839 --> 00:09:37,839
carpentry

00:09:38,240 --> 00:09:42,560
but instead we started looking at the

00:09:41,360 --> 00:09:45,839
number of connections

00:09:42,560 --> 00:09:48,000
between our load balancer and nyhosts

00:09:45,839 --> 00:09:50,320
and found that we have about 13 thousand

00:09:48,000 --> 00:09:53,519
connection to east host

00:09:50,320 --> 00:09:54,399
that's a pretty high number and someone

00:09:53,519 --> 00:09:58,080
pointed out

00:09:54,399 --> 00:10:00,399
that the buffer size is one megabyte

00:09:58,080 --> 00:10:01,760
and with some math you get a total

00:10:00,399 --> 00:10:04,320
buffer size of

00:10:01,760 --> 00:10:07,040
13 gigabytes that's quite a lot of

00:10:04,320 --> 00:10:09,440
buffering for ny to do

00:10:07,040 --> 00:10:12,079
so we tried to decrease it to 32

00:10:09,440 --> 00:10:15,120
kilobytes for each connection

00:10:12,079 --> 00:10:16,800
and our request per second increased

00:10:15,120 --> 00:10:20,640
from 30 to 60 000

00:10:16,800 --> 00:10:22,399
on direct responses we did try

00:10:20,640 --> 00:10:24,000
to tweak similar settings like the

00:10:22,399 --> 00:10:27,279
number of concurrent streams

00:10:24,000 --> 00:10:27,920
and window sizes but we didn't find

00:10:27,279 --> 00:10:32,720
anything

00:10:27,920 --> 00:10:35,120
we thought were worth changing

00:10:32,720 --> 00:10:36,079
as soon as we hit 15 000 requests per

00:10:35,120 --> 00:10:39,920
second

00:10:36,079 --> 00:10:39,920
latency started to increase

00:10:40,000 --> 00:10:43,440
this did not happen if we removed the

00:10:42,079 --> 00:10:46,880
rtc decorator

00:10:43,440 --> 00:10:48,800
from the from the request path

00:10:46,880 --> 00:10:50,560
to check if it was the service that was

00:10:48,800 --> 00:10:52,640
slow replaced it

00:10:50,560 --> 00:10:55,040
with a service that immediately returned

00:10:52,640 --> 00:10:57,760
to 100 okay

00:10:55,040 --> 00:11:01,839
performance was still bad and we only

00:10:57,760 --> 00:11:01,839
got 15 000 requests per second

00:11:02,720 --> 00:11:06,880
clearly we have isolated an issue in the

00:11:05,760 --> 00:11:10,240
communication between

00:11:06,880 --> 00:11:13,600
envoy and the otc sidecar

00:11:10,240 --> 00:11:16,880
this was narrow enough

00:11:13,600 --> 00:11:18,320
for a teammate to realize

00:11:16,880 --> 00:11:20,560
that we have previously touched the

00:11:18,320 --> 00:11:23,760
network configuration on this cluster

00:11:20,560 --> 00:11:24,640
and sure enough we were using docker

00:11:23,760 --> 00:11:26,839
network bridge

00:11:24,640 --> 00:11:28,399
instead of the much faster loopback

00:11:26,839 --> 00:11:31,120
device

00:11:28,399 --> 00:11:32,480
throughput increased to 30 000 requests

00:11:31,120 --> 00:11:36,160
per second

00:11:32,480 --> 00:11:39,680
but why didn't we see this earlier

00:11:36,160 --> 00:11:41,200
it does only increase latency so badly

00:11:39,680 --> 00:11:42,720
that the load balancer started

00:11:41,200 --> 00:11:46,880
considering the hose dead

00:11:42,720 --> 00:11:46,880
it didn't actually limit the throughput

00:11:47,440 --> 00:11:51,440
finally we have reached the end of our

00:11:50,399 --> 00:11:55,200
journey

00:11:51,440 --> 00:11:58,240
everything worked we hid in production

00:11:55,200 --> 00:12:02,160
and everything is fine

00:11:58,240 --> 00:12:04,560
for a few minutes then once again the

00:12:02,160 --> 00:12:08,839
error rate started to creep up

00:12:04,560 --> 00:12:10,880
and rps went down to the same old 15 000

00:12:08,839 --> 00:12:13,279
rps

00:12:10,880 --> 00:12:14,880
we decided at this point to drill down

00:12:13,279 --> 00:12:15,600
to the various thread pools on the

00:12:14,880 --> 00:12:18,639
system

00:12:15,600 --> 00:12:21,440
to see if any of them were overloaded

00:12:18,639 --> 00:12:22,880
what we found instead was that the main

00:12:21,440 --> 00:12:25,760
envoy worker pool

00:12:22,880 --> 00:12:28,720
was extremely unevenly loaded some

00:12:25,760 --> 00:12:32,079
threads were pegged at 100 percent

00:12:28,720 --> 00:12:34,079
others were doing nothing we assumed

00:12:32,079 --> 00:12:37,279
that this was a locking problem and we

00:12:34,079 --> 00:12:40,320
started to work on profiling envoy

00:12:37,279 --> 00:12:42,560
that is until someone noticed that the

00:12:40,320 --> 00:12:43,519
number of open connections to each

00:12:42,560 --> 00:12:47,600
worker thread

00:12:43,519 --> 00:12:51,120
was actually similarly lopsided

00:12:47,600 --> 00:12:51,680
so why were some worker threads

00:12:51,120 --> 00:12:54,560
receiving

00:12:51,680 --> 00:12:56,320
all of the traffic and others none we

00:12:54,560 --> 00:12:57,600
could not reproduce this problem in our

00:12:56,320 --> 00:13:00,399
test environment

00:12:57,600 --> 00:13:01,200
which meant that we were flying blind we

00:13:00,399 --> 00:13:03,839
decided

00:13:01,200 --> 00:13:04,399
to reach out to the envoy community as

00:13:03,839 --> 00:13:08,000
well

00:13:04,399 --> 00:13:08,959
as our cloud provider google we got a

00:13:08,000 --> 00:13:12,639
suggestion

00:13:08,959 --> 00:13:16,160
from both in the form of harvey touch

00:13:12,639 --> 00:13:17,120
so reuse port this configuration option

00:13:16,160 --> 00:13:20,240
in envoy is

00:13:17,120 --> 00:13:22,800
described as such this makes

00:13:20,240 --> 00:13:23,839
inbound connections distribute among

00:13:22,800 --> 00:13:27,040
worker threads

00:13:23,839 --> 00:13:30,079
roughly evenly in cases where there

00:13:27,040 --> 00:13:33,519
are a high number of connections

00:13:30,079 --> 00:13:36,320
which begs the question when would you

00:13:33,519 --> 00:13:39,120
not want connections evenly distributed

00:13:36,320 --> 00:13:39,120
among workers

00:13:39,440 --> 00:13:44,079
anyway it worked

00:13:44,320 --> 00:13:50,399
but why couldn't we reproduce this

00:13:47,600 --> 00:13:53,680
problem in testing

00:13:50,399 --> 00:13:56,399
it turns out that load started out

00:13:53,680 --> 00:13:58,639
pretty evenly distributed and then

00:13:56,399 --> 00:14:02,480
slowly diverges

00:13:58,639 --> 00:14:03,760
our test cluster was either reconfigured

00:14:02,480 --> 00:14:06,480
often enough

00:14:03,760 --> 00:14:07,519
or saw long enough breaks with no

00:14:06,480 --> 00:14:10,639
traffic

00:14:07,519 --> 00:14:11,920
that things reset themselves whereas our

00:14:10,639 --> 00:14:16,079
production traffic

00:14:11,920 --> 00:14:19,279
cluster was always loaded

00:14:16,079 --> 00:14:21,680
so this is the end of our journey

00:14:19,279 --> 00:14:23,360
we have now had four months without any

00:14:21,680 --> 00:14:26,320
major problems

00:14:23,360 --> 00:14:28,160
and to get more certainty we did a

00:14:26,320 --> 00:14:29,440
successful regional failover test where

00:14:28,160 --> 00:14:31,279
we killed one region

00:14:29,440 --> 00:14:34,480
and let all that traffic go to our other

00:14:31,279 --> 00:14:36,720
regions and it just worked

00:14:34,480 --> 00:14:38,880
so we have started doing fun things like

00:14:36,720 --> 00:14:40,160
upgrading to the version 3 of the xts

00:14:38,880 --> 00:14:42,480
api

00:14:40,160 --> 00:14:46,079
adding rate limiting and looking at

00:14:42,480 --> 00:14:46,079
course configuration for our clients

00:14:46,639 --> 00:14:50,160
and we did take it slow by rolling out

00:14:49,519 --> 00:14:53,440
gradual

00:14:50,160 --> 00:14:55,839
over an entire year and we did spend a

00:14:53,440 --> 00:14:58,959
full week of performance testing before

00:14:55,839 --> 00:15:01,360
our last and final deployment and still

00:14:58,959 --> 00:15:04,079
we failed to identify five major

00:15:01,360 --> 00:15:06,800
scalability bottlenecks

00:15:04,079 --> 00:15:08,000
maybe spending an hour looking at all

00:15:06,800 --> 00:15:10,800
the available metrics

00:15:08,000 --> 00:15:11,920
while testing our setup might have

00:15:10,800 --> 00:15:15,279
actually identified

00:15:11,920 --> 00:15:18,720
a few of these problems but probably not

00:15:15,279 --> 00:15:20,399
all of them looking back this journey

00:15:18,720 --> 00:15:22,480
was a lot of fun

00:15:20,399 --> 00:15:25,040
even though it didn't always feel like

00:15:22,480 --> 00:15:29,199
that while it was ongoing

00:15:25,040 --> 00:15:32,240
and we for sure did learn a lot

00:15:29,199 --> 00:15:33,360
so some suggestions we thought we would

00:15:32,240 --> 00:15:35,279
share

00:15:33,360 --> 00:15:36,639
they would most likely have helped us so

00:15:35,279 --> 00:15:40,320
maybe they can help

00:15:36,639 --> 00:15:42,720
someone else make the default cue size

00:15:40,320 --> 00:15:44,160
per core so you don't have to remember

00:15:42,720 --> 00:15:45,199
to change it when you change your

00:15:44,160 --> 00:15:48,399
machine type

00:15:45,199 --> 00:15:51,680
to have a different number of cores

00:15:48,399 --> 00:15:53,920
make so reuse port default

00:15:51,680 --> 00:15:55,759
we know this has some performance costs

00:15:53,920 --> 00:15:58,079
to low traffic servers

00:15:55,759 --> 00:16:00,800
but we figure efficiency is more

00:15:58,079 --> 00:16:02,560
important on high traffic servers

00:16:00,800 --> 00:16:04,240
another alternative would be to

00:16:02,560 --> 00:16:06,480
highlight it in the

00:16:04,240 --> 00:16:08,320
best practices guide for android as an

00:16:06,480 --> 00:16:11,040
edge proxy

00:16:08,320 --> 00:16:12,399
and last add average latency to the

00:16:11,040 --> 00:16:14,800
histograms

00:16:12,399 --> 00:16:16,000
we know averages can be overused and

00:16:14,800 --> 00:16:18,000
misguiding

00:16:16,000 --> 00:16:19,120
but when doing math on connection

00:16:18,000 --> 00:16:22,160
settings it can be

00:16:19,120 --> 00:16:27,360
very helpful

00:16:22,160 --> 00:16:31,120
okay so how do you fail at life

00:16:27,360 --> 00:16:34,880
by planning for it assume

00:16:31,120 --> 00:16:38,079
that you will fail because you will

00:16:34,880 --> 00:16:41,759
try to think ahead to when you will fail

00:16:38,079 --> 00:16:45,199
and try to think of what you need to do

00:16:41,759 --> 00:16:47,040
next and make sure that you have the

00:16:45,199 --> 00:16:50,720
tools at your disposal

00:16:47,040 --> 00:16:54,079
to do just that this

00:16:50,720 --> 00:16:57,279
often means having the right metrics

00:16:54,079 --> 00:17:00,720
next do your best to reproduce

00:16:57,279 --> 00:17:02,079
all problems outside of the production

00:17:00,720 --> 00:17:04,880
environment

00:17:02,079 --> 00:17:06,000
not only does doing so give you much

00:17:04,880 --> 00:17:08,319
more opportunity

00:17:06,000 --> 00:17:09,839
to see what happens in various related

00:17:08,319 --> 00:17:13,120
failure scenarios

00:17:09,839 --> 00:17:13,120
the act of crafting

00:17:13,280 --> 00:17:16,880
a test environment often shows you blind

00:17:16,240 --> 00:17:20,720
spots

00:17:16,880 --> 00:17:24,880
you didn't know you had and finally

00:17:20,720 --> 00:17:27,439
communicate ask for help

00:17:24,880 --> 00:17:30,559
broadcast your shortcomings to anyone

00:17:27,439 --> 00:17:33,600
who can be made to listen

00:17:30,559 --> 00:17:37,280
like you even if

00:17:33,600 --> 00:17:40,880
your mistakes are embarrassingly dumb

00:17:37,280 --> 00:17:44,160
like ours keep talking

00:17:40,880 --> 00:17:45,039
maybe some of those mistakes can be

00:17:44,160 --> 00:17:48,160
prevented

00:17:45,039 --> 00:17:51,360
through code changes and if not

00:17:48,160 --> 00:17:54,799
at least more people will know

00:17:51,360 --> 00:17:54,799
about the common pitfalls

00:17:54,960 --> 00:18:01,840
hello everyone are there any questions

00:17:58,799 --> 00:18:01,840
in here

00:18:07,360 --> 00:18:12,720
thank you for all of the feedback and

00:18:10,160 --> 00:18:15,840
the thumbs up and whatnot

00:18:12,720 --> 00:18:17,039
let's see uh have you guys looked at

00:18:15,840 --> 00:18:20,799
enabling

00:18:17,039 --> 00:18:22,080
exact balancer on the listener

00:18:20,799 --> 00:18:24,080
i'm gonna let you handle that one

00:18:22,080 --> 00:18:27,120
because i don't know i i

00:18:24,080 --> 00:18:29,280
don't actually know what that is that

00:18:27,120 --> 00:18:32,559
was what i was too ashamed to admit

00:18:29,280 --> 00:18:34,160
so i'm not ashamed of things like that

00:18:32,559 --> 00:18:36,799
uh i have no idea i will look it up

00:18:34,160 --> 00:18:36,799
thanks for a tip

00:18:39,440 --> 00:18:44,240
uh question about if http 1.1 issue was

00:18:42,880 --> 00:18:48,320
identified

00:18:44,240 --> 00:18:50,880
so there was no http 1.1

00:18:48,320 --> 00:18:52,160
issue that was a suspicion that we had

00:18:50,880 --> 00:18:55,520
that maybe

00:18:52,160 --> 00:18:57,280
the http 1.1 stack was slower or less

00:18:55,520 --> 00:18:59,520
battle tested or less scalable or

00:18:57,280 --> 00:19:00,320
something like that and that turned out

00:18:59,520 --> 00:19:04,000
to be

00:19:00,320 --> 00:19:07,840
wrong uh we are still using http

00:19:04,000 --> 00:19:09,520
2 from envoy to the load balancer

00:19:07,840 --> 00:19:11,520
from from the load balancer to envoy

00:19:09,520 --> 00:19:14,160
obviously and then from envoy

00:19:11,520 --> 00:19:15,360
to our microservices we're talking http

00:19:14,160 --> 00:19:24,000
1.1

00:19:15,360 --> 00:19:26,640
and they both seem to perform just fine

00:19:24,000 --> 00:19:27,520
uh running into very similar problems at

00:19:26,640 --> 00:19:31,120
twitter

00:19:27,520 --> 00:19:33,919
i think overall

00:19:31,120 --> 00:19:34,880
i would expect people that have very

00:19:33,919 --> 00:19:38,240
large

00:19:34,880 --> 00:19:41,679
request volumes to have similar issues

00:19:38,240 --> 00:19:44,799
and i think uh

00:19:41,679 --> 00:19:48,559
like there is a very good start of

00:19:44,799 --> 00:19:49,840
how to put how to make uh online http

00:19:48,559 --> 00:19:53,120
proxy for a large

00:19:49,840 --> 00:19:54,080
organization in the docs for envoy but i

00:19:53,120 --> 00:19:57,280
think there are

00:19:54,080 --> 00:19:59,679
there are opportunities to improve

00:19:57,280 --> 00:20:02,000
the configuration as well as improve

00:19:59,679 --> 00:20:05,200
that documentation to make

00:20:02,000 --> 00:20:08,960
life even easier for a

00:20:05,200 --> 00:20:11,919
large uh installations

00:20:08,960 --> 00:20:11,919
yeah definitely

00:20:16,000 --> 00:20:20,240
ah it's another way of forcing

00:20:17,600 --> 00:20:22,480
connection balancing well

00:20:20,240 --> 00:20:23,520
then we should look into and see if it

00:20:22,480 --> 00:20:26,799
works

00:20:23,520 --> 00:20:30,320
better or worse thanks for the tip yeah

00:20:26,799 --> 00:20:34,000
and yes maxim in our load testing we got

00:20:30,320 --> 00:20:35,520
about 1000 requests per second per core

00:20:34,000 --> 00:20:38,000
i think in production we get a little

00:20:35,520 --> 00:20:38,000
bit less

00:20:43,039 --> 00:20:50,880
and matt klein asks why are you using

00:20:46,320 --> 00:20:54,159
http 1.1 to the back ends versus 2

00:20:50,880 --> 00:20:58,159
and the answer to that is

00:20:54,159 --> 00:21:02,080
mostly legacy so spotify

00:20:58,159 --> 00:21:04,960
has a very old network stack it's almost

00:21:02,080 --> 00:21:06,240
it's about a decade old we implemented

00:21:04,960 --> 00:21:10,400
our own

00:21:06,240 --> 00:21:12,720
uh transport layer instead of uh http

00:21:10,400 --> 00:21:15,039
because we had a lot of scalability

00:21:12,720 --> 00:21:18,880
problems with http

00:21:15,039 --> 00:21:21,520
this uh transport layer called hermes

00:21:18,880 --> 00:21:22,559
is basically very similar in most ways

00:21:21,520 --> 00:21:25,679
to http

00:21:22,559 --> 00:21:26,799
2. it solves the same problems in mostly

00:21:25,679 --> 00:21:30,080
the same way

00:21:26,799 --> 00:21:33,360
and it tries to be very http like

00:21:30,080 --> 00:21:37,919
in its api but it is

00:21:33,360 --> 00:21:41,200
uh older than http 2. we started work

00:21:37,919 --> 00:21:43,360
slightly after well like slightly before

00:21:41,200 --> 00:21:45,440
google started talking about speedy

00:21:43,360 --> 00:21:47,840
publicly

00:21:45,440 --> 00:21:49,919
and we are still transitioning away from

00:21:47,840 --> 00:21:52,400
this internal hermes protocol

00:21:49,919 --> 00:21:53,840
and what we have today for our hermes

00:21:52,400 --> 00:21:57,039
based services

00:21:53,840 --> 00:22:00,320
is a uh

00:21:57,039 --> 00:22:03,600
like library that you can use to accept

00:22:00,320 --> 00:22:06,080
http traffic as if it was hermes traffic

00:22:03,600 --> 00:22:06,960
and we are instead moving to internally

00:22:06,080 --> 00:22:10,480
use

00:22:06,960 --> 00:22:13,919
uh http 2 and grpc and then

00:22:10,480 --> 00:22:16,799
in the future hopefully http 3 and so on

00:22:13,919 --> 00:22:19,280
like modernizing our stack but we're not

00:22:16,799 --> 00:22:19,280
there yet

00:22:21,919 --> 00:22:27,840
and christopher we're six people

00:22:25,440 --> 00:22:27,840
i believe

00:22:28,880 --> 00:22:35,120
yeah something like that yes

00:22:33,039 --> 00:22:37,840
and the other people are more competent

00:22:35,120 --> 00:22:41,840
than me and ex

00:22:37,840 --> 00:22:41,840
that's why they kicked me out yes

00:22:45,280 --> 00:22:50,559
and louise i'm not sure how many

00:22:47,520 --> 00:22:53,919
requests per connection we had

00:22:50,559 --> 00:22:56,080
if you ask me on slack i can i can check

00:22:53,919 --> 00:22:56,080
it

00:23:02,480 --> 00:23:06,480
so with regards to much in the way of

00:23:05,200 --> 00:23:09,760
filters

00:23:06,480 --> 00:23:12,799
we are using a few filters to filter out

00:23:09,760 --> 00:23:14,400
uh users who are not allowed on some

00:23:12,799 --> 00:23:17,600
resources and so on

00:23:14,400 --> 00:23:18,400
but the big thing that reduces our

00:23:17,600 --> 00:23:22,159
efficiency

00:23:18,400 --> 00:23:26,000
i would say is that we are running

00:23:22,159 --> 00:23:28,720
both the uh both envoy itself

00:23:26,000 --> 00:23:29,840
and this decorator side car which is

00:23:28,720 --> 00:23:33,120
implemented as

00:23:29,840 --> 00:23:36,320
an ext off c filter on

00:23:33,120 --> 00:23:38,640
those on the same 32 core machine so the

00:23:36,320 --> 00:23:42,080
three resource hogs on the machine

00:23:38,640 --> 00:23:43,120
is envoy itself which uses like half the

00:23:42,080 --> 00:23:45,760
cpu

00:23:43,120 --> 00:23:48,159
and then the sidecar which uses slightly

00:23:45,760 --> 00:23:51,120
less but still a significant amount

00:23:48,159 --> 00:23:54,240
and lastly also the metrics propagation

00:23:51,120 --> 00:23:57,039
which uses about 1 out of 32 cores

00:23:54,240 --> 00:23:59,120
so all three of those are running on

00:23:57,039 --> 00:24:02,159
every single envoy host

00:23:59,120 --> 00:24:03,200
so and also that means that you're

00:24:02,159 --> 00:24:06,559
getting

00:24:03,200 --> 00:24:08,400
you get a message in to envoy and then

00:24:06,559 --> 00:24:09,679
it's passed out from envoy to the other

00:24:08,400 --> 00:24:11,360
service and then back

00:24:09,679 --> 00:24:13,120
and then to the next and then you get

00:24:11,360 --> 00:24:15,360
the replying so like

00:24:13,120 --> 00:24:16,720
there are six message passing steps or

00:24:15,360 --> 00:24:19,120
something like that

00:24:16,720 --> 00:24:20,480
uh not just the four that you would

00:24:19,120 --> 00:24:22,400
expect

00:24:20,480 --> 00:24:23,679
my math is probably wrong but something

00:24:22,400 --> 00:24:26,159
along those lines

00:24:23,679 --> 00:24:28,559
yeah and also the our decorator is in

00:24:26,159 --> 00:24:32,320
java so we have some garbage collection

00:24:28,559 --> 00:24:35,360
fun things

00:24:32,320 --> 00:24:38,559
and replacing it with filters i don't

00:24:35,360 --> 00:24:38,559
think we have talked about that

00:24:38,960 --> 00:24:43,200
and i'm not sure why we decided to go

00:24:41,679 --> 00:24:45,360
with a

00:24:43,200 --> 00:24:47,120
side car that was before i joined the

00:24:45,360 --> 00:24:50,480
team actually

00:24:47,120 --> 00:24:55,120
that decision is over a year old

00:24:50,480 --> 00:24:58,960
i i was very interested to hear

00:24:55,120 --> 00:25:01,440
the uh talk like one of the starting

00:24:58,960 --> 00:25:05,360
talks about using webassembly to make

00:25:01,440 --> 00:25:10,000
your own custom filters in envoy

00:25:05,360 --> 00:25:10,000
that could definitely be useful for us

00:25:10,240 --> 00:25:14,799
we did not want to write our own c

00:25:12,799 --> 00:25:18,559
c-plus plus filters because

00:25:14,799 --> 00:25:20,640
we as a company have too few developers

00:25:18,559 --> 00:25:24,320
who are super comfortable with

00:25:20,640 --> 00:25:28,000
c plus plus and then it becomes a

00:25:24,320 --> 00:25:31,039
like who owns its uh problem problem

00:25:28,000 --> 00:25:33,600
whereas we have lots of java devs but

00:25:31,039 --> 00:25:34,159
webassembly might help out with that we

00:25:33,600 --> 00:25:36,559
don't know

00:25:34,159 --> 00:25:36,559
we'll see

00:25:39,919 --> 00:25:43,039
but overall i agree that the sidecar

00:25:42,400 --> 00:25:46,080
solution

00:25:43,039 --> 00:25:48,080
feels like probably not what we want to

00:25:46,080 --> 00:25:50,880
do long term

00:25:48,080 --> 00:25:57,840
yeah and maxine we're running on

00:25:50,880 --> 00:25:57,840
managing infrastructure

00:26:01,039 --> 00:26:08,400
i think we answered all of the questions

00:26:04,640 --> 00:26:11,039
if someone has a question that they um

00:26:08,400 --> 00:26:12,000
posted that we didn't answer it's not

00:26:11,039 --> 00:26:14,000
because we hate you

00:26:12,000 --> 00:26:16,080
it's because we missed it so please feel

00:26:14,000 --> 00:26:19,200
free to repost it in that case

00:26:16,080 --> 00:26:20,400
yeah or ask on the elmo slack at least

00:26:19,200 --> 00:26:21,200
i'm there i'm not sure if you are

00:26:20,400 --> 00:26:26,640
excellent

00:26:21,200 --> 00:26:26,640
i actually am fantastic i know

00:26:29,440 --> 00:26:37,440
well that seems to be it

00:26:34,799 --> 00:26:39,600
sure does thanks a lot everyone for

00:26:37,440 --> 00:26:43,919
listening

00:26:39,600 --> 00:26:47,039
this was uh this was great i will now

00:26:43,919 --> 00:26:50,320
disconnect and uh go say hi

00:26:47,039 --> 00:26:51,520
to the third uh talker from this

00:26:50,320 --> 00:26:54,799
conference

00:26:51,520 --> 00:26:58,559
titus so bye

00:26:54,799 --> 00:26:58,559

YouTube URL: https://www.youtube.com/watch?v=2EU7kFxl6RI


