Title: Improving performance of RPCs with envoy at Wikimedia - Giuseppe Lavagetto
Publication date: 2020-10-21
Playlist: EnvoyCon 2020 - Virtual
Description: 
	Improving performance of RPCs with envoy at Wikimedia - Giuseppe Lavagetto

Performance of remote procedure calls between services depend on a lot of factors, but when you start doing RPCs over a high latency network and/or using TLS (so when you have to perform RPCs across different datacenters, for example), the cost of establishing a connection is very steep. This is particularly problematic for environment which don't support persistent connection pools - one notable example being the PHP language, that we use to run MediaWiki. This talk will go through how Wikimedia introduced envoy in its mixed on-prem/kubernetes environment, and how that allowed to improve the performance, reliability and observability of its stack. Particular focus will be put on: the performance effects for our PHP applications running at scale, the operational problems adopting envoy allowed solving, and the challenges introduced by moving to use it.
Captions: 
	00:00:00,960 --> 00:00:04,560
hello everyone today i'm here to talk to

00:00:03,760 --> 00:00:06,399
you about

00:00:04,560 --> 00:00:08,320
how we improve the performance of

00:00:06,399 --> 00:00:12,160
service-to-service interactions using

00:00:08,320 --> 00:00:12,160
envoy at the wikimedia foundation

00:00:12,559 --> 00:00:17,199
now today i'll start with uh introducing

00:00:15,759 --> 00:00:20,560
you to our infrastructures

00:00:17,199 --> 00:00:24,160
explain why and why it made sense in

00:00:20,560 --> 00:00:26,080
that context i'll

00:00:24,160 --> 00:00:27,840
explain briefly how the transition went

00:00:26,080 --> 00:00:30,160
that we went through the tradition of

00:00:27,840 --> 00:00:32,239
introducing environment production and

00:00:30,160 --> 00:00:34,399
finally i'll focus on what we gain from

00:00:32,239 --> 00:00:34,399
this

00:00:34,640 --> 00:00:39,040
let's start with the introduction who we

00:00:36,800 --> 00:00:41,120
are the wikimedia foundation is the

00:00:39,040 --> 00:00:42,960
nonprofit organization

00:00:41,120 --> 00:00:44,879
that runs the infrastructure that

00:00:42,960 --> 00:00:46,879
supports wikipedia

00:00:44,879 --> 00:00:49,440
and its sister projects like quick

00:00:46,879 --> 00:00:53,120
scenario or wikidata

00:00:49,440 --> 00:00:56,879
we do quite a lot of traffic monthly

00:00:53,120 --> 00:01:01,039
we get one 21 billion page views

00:00:56,879 --> 00:01:01,039
this is the data from last august

00:01:01,440 --> 00:01:06,240
now how is our infrastructure structured

00:01:04,559 --> 00:01:08,640
we have five data centers

00:01:06,240 --> 00:01:10,720
three of which are just caching pops so

00:01:08,640 --> 00:01:11,439
they are just points of presence that

00:01:10,720 --> 00:01:15,200
only

00:01:11,439 --> 00:01:18,400
hosts the caching layer

00:01:15,200 --> 00:01:21,600
while the two main data centers

00:01:18,400 --> 00:01:24,080
are here in blue and green and are both

00:01:21,600 --> 00:01:25,920
located in the united states

00:01:24,080 --> 00:01:28,960
one in virginia and the other in dallas

00:01:25,920 --> 00:01:33,439
and they host our wall

00:01:28,960 --> 00:01:36,640
application stack now what happens when

00:01:33,439 --> 00:01:40,720
somebody makes a request let's say from

00:01:36,640 --> 00:01:41,759
africa somebody connects to wikipedia

00:01:40,720 --> 00:01:44,159
they will be

00:01:41,759 --> 00:01:47,439
directed by geodns to the nearest point

00:01:44,159 --> 00:01:50,880
of presence which is esam's

00:01:47,439 --> 00:01:54,320
if visitor is

00:01:50,880 --> 00:01:56,399
logged in or if a page is not present

00:01:54,320 --> 00:01:58,560
their traffic will be sent to the

00:01:56,399 --> 00:02:00,240
application layer data centers and the

00:01:58,560 --> 00:02:03,439
response will be fetched back from there

00:02:00,240 --> 00:02:05,920
computed and fetched back from there

00:02:03,439 --> 00:02:07,680
if a user is surfing anonymously and the

00:02:05,920 --> 00:02:11,200
page has been seen

00:02:07,680 --> 00:02:11,200
in the last day

00:02:11,360 --> 00:02:16,160
then they will get directly response

00:02:12,959 --> 00:02:18,959
from the caching data center

00:02:16,160 --> 00:02:21,040
now our main data centers that run the

00:02:18,959 --> 00:02:24,480
application stack

00:02:21,040 --> 00:02:27,520
our system is a mix

00:02:24,480 --> 00:02:28,800
of some applications micro services

00:02:27,520 --> 00:02:31,599
running on kubernetes

00:02:28,800 --> 00:02:33,840
and some stuff still to be moved into

00:02:31,599 --> 00:02:36,480
kubernetes from our legacy environment

00:02:33,840 --> 00:02:40,160
which is basically physical hosts

00:02:36,480 --> 00:02:45,120
um one of these applications is our uh

00:02:40,160 --> 00:02:47,120
de facto old monolith which is mediawiki

00:02:45,120 --> 00:02:48,160
the other peculiarity of mediawiki is

00:02:47,120 --> 00:02:51,680
that while

00:02:48,160 --> 00:02:53,519
most of the other applications can run

00:02:51,680 --> 00:02:55,840
can serve traffic from both that

00:02:53,519 --> 00:02:58,080
the main data centers media wiki is

00:02:55,840 --> 00:03:01,680
active passive meaning it can only serve

00:02:58,080 --> 00:03:05,280
traffic from one data center at a time

00:03:01,680 --> 00:03:07,440
this means that services

00:03:05,280 --> 00:03:09,040
from one data center might fetch stuff

00:03:07,440 --> 00:03:10,640
from the wiki api

00:03:09,040 --> 00:03:12,239
and the other data center but also that

00:03:10,640 --> 00:03:14,000
media wiki might need to connect

00:03:12,239 --> 00:03:16,000
data stores or other services in both

00:03:14,000 --> 00:03:19,280
data centers

00:03:16,000 --> 00:03:21,440
as an example um let's say

00:03:19,280 --> 00:03:22,560
that when you get we get an edit we need

00:03:21,440 --> 00:03:25,280
to to notify

00:03:22,560 --> 00:03:25,599
our elastic search clusters that power

00:03:25,280 --> 00:03:27,840
the

00:03:25,599 --> 00:03:31,280
search box on wikipedia about the fact

00:03:27,840 --> 00:03:33,760
that that article has been modified

00:03:31,280 --> 00:03:35,680
now we have independent clusters in the

00:03:33,760 --> 00:03:37,040
two data centers so we need to send the

00:03:35,680 --> 00:03:39,440
update to both

00:03:37,040 --> 00:03:40,159
and this means that some of the traffic

00:03:39,440 --> 00:03:43,200
will go

00:03:40,159 --> 00:03:46,959
across data center the

00:03:43,200 --> 00:03:50,000
arrows in red here and this means that

00:03:46,959 --> 00:03:52,159
while we encrypted a long time ago the

00:03:50,000 --> 00:03:54,159
traffic between the edges vacation pops

00:03:52,159 --> 00:03:56,239
and the main data centers

00:03:54,159 --> 00:03:57,840
we didn't encrypt this traffic with

00:03:56,239 --> 00:03:58,799
traffic between applications which means

00:03:57,840 --> 00:04:01,120
that

00:03:58,799 --> 00:04:03,280
this traffic crossed at the center was

00:04:01,120 --> 00:04:06,560
going in the clear

00:04:03,280 --> 00:04:08,319
now uh if the last 10 years taught us

00:04:06,560 --> 00:04:10,239
anything is that if you run more than

00:04:08,319 --> 00:04:12,080
one data center you really want the

00:04:10,239 --> 00:04:13,519
communications across data centers to be

00:04:12,080 --> 00:04:16,160
encrypted

00:04:13,519 --> 00:04:16,799
because that makes the life of state

00:04:16,160 --> 00:04:19,120
nation

00:04:16,799 --> 00:04:19,840
actors harder when they want to snoop on

00:04:19,120 --> 00:04:23,120
the data

00:04:19,840 --> 00:04:25,440
of your users and

00:04:23,120 --> 00:04:27,199
so we need to introduce tls in front of

00:04:25,440 --> 00:04:29,280
all of our application

00:04:27,199 --> 00:04:30,400
now we could have asked every

00:04:29,280 --> 00:04:34,080
development team

00:04:30,400 --> 00:04:34,960
to develop to add tls termination to

00:04:34,080 --> 00:04:37,199
their application

00:04:34,960 --> 00:04:38,800
but that would have meant asking all

00:04:37,199 --> 00:04:42,320
these teams to become

00:04:38,800 --> 00:04:43,360
somewhat experts in configuring a tls

00:04:42,320 --> 00:04:45,840
stack

00:04:43,360 --> 00:04:47,680
and at the same time it would it meant

00:04:45,840 --> 00:04:49,120
that we would have to track the security

00:04:47,680 --> 00:04:51,520
defects in multiple

00:04:49,120 --> 00:04:53,520
application stacks so we decided pretty

00:04:51,520 --> 00:04:56,560
early we're going to install

00:04:53,520 --> 00:04:58,720
a tls termination sidecar

00:04:56,560 --> 00:05:00,800
now we choose envoy for that function

00:04:58,720 --> 00:05:04,000
for a series of reasons the first one

00:05:00,800 --> 00:05:08,560
is envoys not open core as some

00:05:04,000 --> 00:05:08,560
other tls terminating proxy

00:05:08,639 --> 00:05:11,680
then there was reason of performance

00:05:10,400 --> 00:05:14,800
we've seen reports

00:05:11,680 --> 00:05:16,240
that envoy is blessingly fast and i told

00:05:14,800 --> 00:05:18,960
you before

00:05:16,240 --> 00:05:21,600
our logged-in users always get sent back

00:05:18,960 --> 00:05:24,080
to the main data centers

00:05:21,600 --> 00:05:25,680
now the logged-in users are typically

00:05:24,080 --> 00:05:28,880
the editors the people that

00:05:25,680 --> 00:05:31,680
add infor add content to the wikis

00:05:28,880 --> 00:05:33,440
and these are in some ways our most

00:05:31,680 --> 00:05:35,120
valued users

00:05:33,440 --> 00:05:37,039
because we are the ones that build the

00:05:35,120 --> 00:05:39,440
projects that make them successful

00:05:37,039 --> 00:05:40,880
and these users already pay a price for

00:05:39,440 --> 00:05:43,360
their loyalty

00:05:40,880 --> 00:05:45,120
by always being sent to the main data

00:05:43,360 --> 00:05:46,720
centers the reason for that is that

00:05:45,120 --> 00:05:48,800
when you're logged in you can change the

00:05:46,720 --> 00:05:50,000
interface the appearance of the site so

00:05:48,800 --> 00:05:54,080
we can't just

00:05:50,000 --> 00:05:56,319
send you a test copy now we didn't want

00:05:54,080 --> 00:05:58,400
our introduction of encryption between

00:05:56,319 --> 00:06:01,039
services to

00:05:58,400 --> 00:06:02,560
add another penalty that was used and

00:06:01,039 --> 00:06:06,160
apart from this

00:06:02,560 --> 00:06:07,840
envoy has been designed to be

00:06:06,160 --> 00:06:10,319
the perfect service to service

00:06:07,840 --> 00:06:13,440
middleware so it has a series

00:06:10,319 --> 00:06:15,120
of characteristics that we really wanted

00:06:13,440 --> 00:06:17,520
to add one of which is

00:06:15,120 --> 00:06:18,479
its observability features it adds

00:06:17,520 --> 00:06:22,000
telemetry

00:06:18,479 --> 00:06:24,479
within all services it gives you the

00:06:22,000 --> 00:06:27,680
ability to emit tracing

00:06:24,479 --> 00:06:29,680
data it has some

00:06:27,680 --> 00:06:30,960
additional things that you want when you

00:06:29,680 --> 00:06:33,280
build a

00:06:30,960 --> 00:06:34,080
true microservices infrastructure like

00:06:33,280 --> 00:06:36,319
rate limiting

00:06:34,080 --> 00:06:37,919
and circuit breaking built into the

00:06:36,319 --> 00:06:40,720
proxy so that you can have

00:06:37,919 --> 00:06:44,319
a common implementation across services

00:06:40,720 --> 00:06:46,240
and finally it's very easy to configure

00:06:44,319 --> 00:06:47,680
yeah i know i was joking and this is

00:06:46,240 --> 00:06:49,199
kind of a cheap shot

00:06:47,680 --> 00:06:50,880
but there's a reason why i'm naming

00:06:49,199 --> 00:06:52,639
configuration and it will be clear in a

00:06:50,880 --> 00:06:54,720
couple of slides

00:06:52,639 --> 00:06:56,000
so let's move we understood why which is

00:06:54,720 --> 00:06:58,400
envoy let's move to

00:06:56,000 --> 00:06:59,840
how we did the transition and again

00:06:58,400 --> 00:07:02,880
wikipedias

00:06:59,840 --> 00:07:03,840
are very high traffic so we run a very

00:07:02,880 --> 00:07:07,120
large website

00:07:03,840 --> 00:07:08,319
with a lot of edge cases and a ton of

00:07:07,120 --> 00:07:11,440
traffic

00:07:08,319 --> 00:07:13,120
uh if one if working at the foundation

00:07:11,440 --> 00:07:14,720
for six years told me anything is that

00:07:13,120 --> 00:07:16,880
there was always an edge case so we

00:07:14,720 --> 00:07:19,520
wanted to proceed with

00:07:16,880 --> 00:07:20,720
a certain level of caution and that

00:07:19,520 --> 00:07:22,800
meant that we

00:07:20,720 --> 00:07:24,000
divided the transition into phases first

00:07:22,800 --> 00:07:26,319
we introduced uh

00:07:24,000 --> 00:07:28,840
tls termination in front of all services

00:07:26,319 --> 00:07:31,120
and we made the other services called

00:07:28,840 --> 00:07:31,599
mbhtps and then only in the second

00:07:31,120 --> 00:07:33,680
moment

00:07:31,599 --> 00:07:35,360
we started introducing configuration in

00:07:33,680 --> 00:07:38,000
envoy to be able to act

00:07:35,360 --> 00:07:39,680
as a service proxy recruit service proxy

00:07:38,000 --> 00:07:42,840
and

00:07:39,680 --> 00:07:44,240
reconfigure the applications to use it

00:07:42,840 --> 00:07:46,639
progressively

00:07:44,240 --> 00:07:48,000
now the problem that we faced during the

00:07:46,639 --> 00:07:49,840
transition the biggest problem that we

00:07:48,000 --> 00:07:52,319
faced during the transition is that the

00:07:49,840 --> 00:07:53,759
configuration of envoy is very complex

00:07:52,319 --> 00:07:55,520
it's well documented by the

00:07:53,759 --> 00:07:58,960
there is a steep learning curve and we

00:07:55,520 --> 00:08:00,960
didn't feel that everybody

00:07:58,960 --> 00:08:02,720
in the team needed to become an expert

00:08:00,960 --> 00:08:04,639
in envoy

00:08:02,720 --> 00:08:06,400
also we had the problem that we had two

00:08:04,639 --> 00:08:07,919
different templating engines one for

00:08:06,400 --> 00:08:11,039
kubernetes that's elm

00:08:07,919 --> 00:08:12,639
and one for the legacy environment

00:08:11,039 --> 00:08:14,479
that's puppet so

00:08:12,639 --> 00:08:16,080
we needed to have basically a common

00:08:14,479 --> 00:08:18,800
template with just

00:08:16,080 --> 00:08:19,840
where we sadly had to implement just the

00:08:18,800 --> 00:08:21,919
templating uh

00:08:19,840 --> 00:08:23,680
primitives but we wanted to have the

00:08:21,919 --> 00:08:27,039
same data structure

00:08:23,680 --> 00:08:27,840
defining listeners and clusters for

00:08:27,039 --> 00:08:30,479
envoy

00:08:27,840 --> 00:08:32,000
to be used both by kubernetes and the

00:08:30,479 --> 00:08:33,919
legacy environment

00:08:32,000 --> 00:08:35,919
and we wanted this to be simple and easy

00:08:33,919 --> 00:08:36,719
to understand to any sre even if they

00:08:35,919 --> 00:08:39,120
have

00:08:36,719 --> 00:08:40,159
no prior experience with envoy we came

00:08:39,120 --> 00:08:42,800
up with a simple

00:08:40,159 --> 00:08:45,200
yaml data structure here is an example

00:08:42,800 --> 00:08:48,560
that's taken from our configuration

00:08:45,200 --> 00:08:50,160
um this example doesn't have all the

00:08:48,560 --> 00:08:50,560
keys that you can define but this is a

00:08:50,160 --> 00:08:52,640
good

00:08:50,560 --> 00:08:54,880
portion of them and basically our design

00:08:52,640 --> 00:08:57,120
goal was any sre

00:08:54,880 --> 00:08:59,360
that is 15 lines of documentation and is

00:08:57,120 --> 00:09:02,399
able to add a basic listener

00:08:59,360 --> 00:09:03,519
to the envoy configuration the other

00:09:02,399 --> 00:09:06,880
goal that we had

00:09:03,519 --> 00:09:08,720
was for this to be boring and when i say

00:09:06,880 --> 00:09:09,839
boring i mean that we didn't want to

00:09:08,720 --> 00:09:13,120
have surprises

00:09:09,839 --> 00:09:16,160
once you have envoy mediating all your

00:09:13,120 --> 00:09:18,080
http traffic between microservices

00:09:16,160 --> 00:09:19,760
changing something in its configuration

00:09:18,080 --> 00:09:23,760
becomes really scary because the

00:09:19,760 --> 00:09:26,800
blast radius is huge i want to

00:09:23,760 --> 00:09:28,959
make a shout out to the android apps for

00:09:26,800 --> 00:09:30,320
adding mood validate to the server so

00:09:28,959 --> 00:09:32,560
that it allowed us to

00:09:30,320 --> 00:09:34,240
easily catch any errors that we were

00:09:32,560 --> 00:09:36,399
introducing in

00:09:34,240 --> 00:09:38,880
the configuration directly in the

00:09:36,399 --> 00:09:41,040
continuous integration environment

00:09:38,880 --> 00:09:43,120
and some investment in making your

00:09:41,040 --> 00:09:45,279
configuration

00:09:43,120 --> 00:09:47,120
continuous integration environment check

00:09:45,279 --> 00:09:49,200
with the configuration is doing what

00:09:47,120 --> 00:09:51,839
it expects to do is really something you

00:09:49,200 --> 00:09:51,839
should invest in

00:09:52,880 --> 00:09:57,839
now all transitions that

00:09:56,240 --> 00:09:59,440
happen in real life happen with some

00:09:57,839 --> 00:10:01,519
struggles i want to name some

00:09:59,440 --> 00:10:02,560
of the struggles we had and the first

00:10:01,519 --> 00:10:05,760
and foremost was

00:10:02,560 --> 00:10:07,760
we use a level four load balancing

00:10:05,760 --> 00:10:09,200
which means that we balance connections

00:10:07,760 --> 00:10:12,480
ip connections and not

00:10:09,200 --> 00:10:14,160
requests now envoy tries to funnel as

00:10:12,480 --> 00:10:16,000
many requests as possible

00:10:14,160 --> 00:10:17,519
as possible across the same connection

00:10:16,000 --> 00:10:19,600
that's one of his strengths

00:10:17,519 --> 00:10:21,760
one of the reasons that we made big

00:10:19,600 --> 00:10:24,079
gains later

00:10:21,760 --> 00:10:25,360
but this also means that it makes it

00:10:24,079 --> 00:10:27,519
very bad to be

00:10:25,360 --> 00:10:28,800
load balanced through a connection based

00:10:27,519 --> 00:10:30,640
load balancer

00:10:28,800 --> 00:10:32,399
because you can send one million

00:10:30,640 --> 00:10:33,920
requests across one connection

00:10:32,399 --> 00:10:35,839
and three connect three requests across

00:10:33,920 --> 00:10:37,360
another connection you've balanced these

00:10:35,839 --> 00:10:38,800
connections across the back ends but you

00:10:37,360 --> 00:10:41,519
didn't balance the requests which is

00:10:38,800 --> 00:10:43,519
what you really want to balance

00:10:41,519 --> 00:10:45,040
so what we did was just to limit the

00:10:43,519 --> 00:10:47,279
number of requests that you can send

00:10:45,040 --> 00:10:48,720
over one single connection to 1000 by

00:10:47,279 --> 00:10:51,279
default

00:10:48,720 --> 00:10:54,000
and that was enough to make the problems

00:10:51,279 --> 00:10:57,360
that i was naming or basically go away

00:10:54,000 --> 00:11:01,040
we also had another problem which was

00:10:57,360 --> 00:11:02,720
we did see um especially

00:11:01,040 --> 00:11:04,720
for the applications that run at high

00:11:02,720 --> 00:11:06,720
scales or thousands and thousands of

00:11:04,720 --> 00:11:10,160
requests per second

00:11:06,720 --> 00:11:11,360
but we had some mysterious connection

00:11:10,160 --> 00:11:14,720
timeouts

00:11:11,360 --> 00:11:16,959
connection failures sorry happening

00:11:14,720 --> 00:11:18,560
from time to time and we treat it back

00:11:16,959 --> 00:11:20,480
to the fact that application servers

00:11:18,560 --> 00:11:24,000
typically

00:11:20,480 --> 00:11:25,920
define them they keep a lifetime out for

00:11:24,000 --> 00:11:27,360
http connections so that they can

00:11:25,920 --> 00:11:30,240
basically

00:11:27,360 --> 00:11:31,440
kill a connection that's kept alive by

00:11:30,240 --> 00:11:33,600
the client

00:11:31,440 --> 00:11:35,120
if the client doesn't send any data out

00:11:33,600 --> 00:11:36,640
of that connection for more than n

00:11:35,120 --> 00:11:39,440
seconds

00:11:36,640 --> 00:11:40,399
and uh turned out that basically we had

00:11:39,440 --> 00:11:42,399
to uh

00:11:40,399 --> 00:11:44,160
account for vetting and void as well let

00:11:42,399 --> 00:11:46,079
me go back to the example i made before

00:11:44,160 --> 00:11:47,760
here we defined the keep alive which

00:11:46,079 --> 00:11:51,920
becomes an idle timeout in

00:11:47,760 --> 00:11:54,399
invoice peak of 4.5 seconds because this

00:11:51,920 --> 00:11:56,320
application and then gate analytics is

00:11:54,399 --> 00:11:57,360
another js application and node.js by

00:11:56,320 --> 00:12:01,040
default

00:11:57,360 --> 00:12:03,839
um as an idle timeout of five seconds

00:12:01,040 --> 00:12:05,920
so just keep that value a bit smaller in

00:12:03,839 --> 00:12:06,399
on the invoice side than on the server

00:12:05,920 --> 00:12:09,360
side

00:12:06,399 --> 00:12:10,880
and all these errors that we had go away

00:12:09,360 --> 00:12:13,360
and finally

00:12:10,880 --> 00:12:14,160
since we didn't we choose to not go with

00:12:13,360 --> 00:12:17,040
the

00:12:14,160 --> 00:12:18,959
sdoa and just make all routing through

00:12:17,040 --> 00:12:22,720
envoy transparent to the application

00:12:18,959 --> 00:12:24,800
by using iptable sorceries

00:12:22,720 --> 00:12:26,639
and we decided to actually go and change

00:12:24,800 --> 00:12:29,279
the configuration of the application to

00:12:26,639 --> 00:12:31,120
uh find a request for envoy also because

00:12:29,279 --> 00:12:34,160
we this way we could just switch

00:12:31,120 --> 00:12:35,600
one back end at a time if we need to uh

00:12:34,160 --> 00:12:37,360
well the problem is that sometimes the

00:12:35,600 --> 00:12:40,959
same configuration key

00:12:37,360 --> 00:12:41,680
is used for finding the upstream servers

00:12:40,959 --> 00:12:44,800
to call

00:12:41,680 --> 00:12:47,920
and also to output some data to the user

00:12:44,800 --> 00:12:52,079
like a csv url for css

00:12:47,920 --> 00:12:53,120
and thus when you use localhost 5603 in

00:12:52,079 --> 00:12:56,959
a configuration

00:12:53,120 --> 00:12:58,560
for our mobile application service

00:12:56,959 --> 00:13:00,399
you might break mobile wikipedia like

00:12:58,560 --> 00:13:03,600
you did

00:13:00,399 --> 00:13:05,279
the point is just it's not always

00:13:03,600 --> 00:13:08,639
cost free to change the configuration of

00:13:05,279 --> 00:13:10,720
a service to point to localhost

00:13:08,639 --> 00:13:12,000
but let's go to the back to the good

00:13:10,720 --> 00:13:13,760
news as i said

00:13:12,000 --> 00:13:16,560
before one of the reasons we choose

00:13:13,760 --> 00:13:20,000
envoy4 was performance

00:13:16,560 --> 00:13:22,079
now we knew performance is great

00:13:20,000 --> 00:13:23,839
in envoy but what we didn't expect is

00:13:22,079 --> 00:13:26,480
that we would actually

00:13:23,839 --> 00:13:28,000
improve the performance of our stack by

00:13:26,480 --> 00:13:32,399
introducing envoy

00:13:28,000 --> 00:13:34,639
and the reason for that is php

00:13:32,399 --> 00:13:35,920
now it's easy to dunk on php but it's

00:13:34,639 --> 00:13:38,720
also undeniable

00:13:35,920 --> 00:13:39,440
uh that for all its flows it's very

00:13:38,720 --> 00:13:42,720
successful

00:13:39,440 --> 00:13:45,360
it's used to run some of the largest

00:13:42,720 --> 00:13:48,160
website investment sites in the world

00:13:45,360 --> 00:13:50,160
and we create one of the creators of

00:13:48,160 --> 00:13:52,320
hhvm keith adams

00:13:50,160 --> 00:13:54,480
uh as written has argued that the reason

00:13:52,320 --> 00:13:57,120
for success for php's success

00:13:54,480 --> 00:13:58,639
is its scoping rules which means the

00:13:57,120 --> 00:14:01,360
scope of any

00:13:58,639 --> 00:14:02,320
of the execution of a php script in a

00:14:01,360 --> 00:14:05,760
web server

00:14:02,320 --> 00:14:07,519
is the web request so at the start of a

00:14:05,760 --> 00:14:08,320
web request you start with basically an

00:14:07,519 --> 00:14:10,800
empty scope

00:14:08,320 --> 00:14:11,440
you have nothing besides some globals

00:14:10,800 --> 00:14:14,959
and the

00:14:11,440 --> 00:14:16,480
request variables and when you have to

00:14:14,959 --> 00:14:17,600
build anything you have to allocate

00:14:16,480 --> 00:14:20,560
memory you have to

00:14:17,600 --> 00:14:22,079
um build anything you need to make all

00:14:20,560 --> 00:14:24,079
the connections you need and

00:14:22,079 --> 00:14:25,120
at the end of the request everything the

00:14:24,079 --> 00:14:26,959
memory you are located the file

00:14:25,120 --> 00:14:28,079
descriptors is open everything is run

00:14:26,959 --> 00:14:31,440
away

00:14:28,079 --> 00:14:32,480
you can see how this makes it very very

00:14:31,440 --> 00:14:35,199
easy to write

00:14:32,480 --> 00:14:36,880
a web application php without having to

00:14:35,199 --> 00:14:40,560
worry about memory leaks or

00:14:36,880 --> 00:14:41,199
such stuff at the same time this gives

00:14:40,560 --> 00:14:44,480
you another

00:14:41,199 --> 00:14:46,480
unique advantage which is serving

00:14:44,480 --> 00:14:47,440
requests concurrently in php is

00:14:46,480 --> 00:14:49,040
incredibly easy

00:14:47,440 --> 00:14:51,040
you have to do nothing to be able to do

00:14:49,040 --> 00:14:52,639
that because every request is atomic by

00:14:51,040 --> 00:14:54,720
default

00:14:52,639 --> 00:14:56,160
so there's it's shared is sure nothing

00:14:54,720 --> 00:14:57,760
architecture where

00:14:56,160 --> 00:15:00,880
you can run things in parallel as much

00:14:57,760 --> 00:15:02,880
as you as you want

00:15:00,880 --> 00:15:04,800
this is an approximation forgive me if

00:15:02,880 --> 00:15:07,040
you know php better you were probably

00:15:04,800 --> 00:15:08,880
saying well actually at this point but

00:15:07,040 --> 00:15:09,760
for this the sake of the argument let's

00:15:08,880 --> 00:15:12,560
just assume that

00:15:09,760 --> 00:15:14,880
nothing is shared between requests this

00:15:12,560 --> 00:15:17,519
means that also you can share things

00:15:14,880 --> 00:15:19,839
like connection pools

00:15:17,519 --> 00:15:21,120
and this means that whenever your php

00:15:19,839 --> 00:15:23,440
application has to call

00:15:21,120 --> 00:15:24,959
other services it needs to create a new

00:15:23,440 --> 00:15:27,040
connection for every request it needs to

00:15:24,959 --> 00:15:29,759
make

00:15:27,040 --> 00:15:30,959
and this is a cost but this cost is even

00:15:29,759 --> 00:15:34,720
bigger

00:15:30,959 --> 00:15:37,120
if your connection is using tls because

00:15:34,720 --> 00:15:38,959
when you use tls you have at least two

00:15:37,120 --> 00:15:40,800
additional round trips to account for

00:15:38,959 --> 00:15:43,120
i'm saying at least because it depends

00:15:40,800 --> 00:15:44,560
on a series of factors but it's at least

00:15:43,120 --> 00:15:47,279
two round trips

00:15:44,560 --> 00:15:47,680
it's one if you get two tls 1.3 but good

00:15:47,279 --> 00:15:52,320
luck

00:15:47,680 --> 00:15:54,720
using tls 1.3 from php

00:15:52,320 --> 00:15:55,759
then there's the cost of establishing

00:15:54,720 --> 00:15:57,199
actually with

00:15:55,759 --> 00:15:59,680
this connection from the computational

00:15:57,199 --> 00:16:02,399
point of view you have to exchange

00:15:59,680 --> 00:16:03,279
a certificate uh you have to exchange

00:16:02,399 --> 00:16:05,120
secrets

00:16:03,279 --> 00:16:06,560
you have to recreate for every request

00:16:05,120 --> 00:16:09,519
the session tickets

00:16:06,560 --> 00:16:10,399
on the server side because as we said

00:16:09,519 --> 00:16:13,519
shared nothing

00:16:10,399 --> 00:16:14,880
on the client side so what we expect is

00:16:13,519 --> 00:16:17,120
basically that this will be

00:16:14,880 --> 00:16:18,079
introduced higher cpu usage higher

00:16:17,120 --> 00:16:19,920
network usage

00:16:18,079 --> 00:16:21,920
and in the case you have latency over

00:16:19,920 --> 00:16:24,000
your network also latency to the

00:16:21,920 --> 00:16:27,199
applications

00:16:24,000 --> 00:16:28,000
now i said before it's a bit more

00:16:27,199 --> 00:16:31,680
complex than

00:16:28,000 --> 00:16:34,720
saying share nothing that's because

00:16:31,680 --> 00:16:35,440
php extensions are written in c and they

00:16:34,720 --> 00:16:38,399
can

00:16:35,440 --> 00:16:40,240
bypass the shared nothing uh behavior of

00:16:38,399 --> 00:16:42,320
php and in fact

00:16:40,240 --> 00:16:44,079
kulex securely extension the cure

00:16:42,320 --> 00:16:45,759
extension which is what everybody uses

00:16:44,079 --> 00:16:48,800
to make remote dhtp calls

00:16:45,759 --> 00:16:51,279
in hhvm allows you to predefine

00:16:48,800 --> 00:16:53,360
shared connection pools for specific

00:16:51,279 --> 00:16:56,720
host names remote test names

00:16:53,360 --> 00:16:58,560
in zen php which is what we are using

00:16:56,720 --> 00:17:01,440
this is not possible at all there is no

00:16:58,560 --> 00:17:04,160
way to do that so we knew

00:17:01,440 --> 00:17:07,439
that introducing tls especially for

00:17:04,160 --> 00:17:10,160
requests that went cross data center

00:17:07,439 --> 00:17:11,679
who made us pay a big price and we

00:17:10,160 --> 00:17:13,679
wanted to test how much

00:17:11,679 --> 00:17:15,520
and to do that we just did this very

00:17:13,679 --> 00:17:16,959
very small benchmark of cross dc

00:17:15,520 --> 00:17:18,079
performance in our production

00:17:16,959 --> 00:17:19,919
environment

00:17:18,079 --> 00:17:21,120
we just wrote a small script that would

00:17:19,919 --> 00:17:24,240
call will fetch

00:17:21,120 --> 00:17:26,000
using the php call extension the elastic

00:17:24,240 --> 00:17:26,480
search burner page from the other data

00:17:26,000 --> 00:17:29,760
center

00:17:26,480 --> 00:17:32,400
which is 35 milliseconds more or less

00:17:29,760 --> 00:17:33,919
from triple way

00:17:32,400 --> 00:17:35,919
and then we call this script with a

00:17:33,919 --> 00:17:37,760
concurrency of 100 under three different

00:17:35,919 --> 00:17:40,720
conditions in one case we pointed the

00:17:37,760 --> 00:17:43,520
script to fetch the data from

00:17:40,720 --> 00:17:46,080
directly from elasticsearch but using

00:17:43,520 --> 00:17:48,320
the http endpoint so no encryption we

00:17:46,080 --> 00:17:51,600
got 720 requests per second

00:17:48,320 --> 00:17:52,720
of throughput then we configured it to

00:17:51,600 --> 00:17:56,720
connect to

00:17:52,720 --> 00:18:00,240
electric search directly but using tls

00:17:56,720 --> 00:18:04,080
and the performance was severely

00:18:00,240 --> 00:18:05,840
reduced and finally we configured envoy

00:18:04,080 --> 00:18:08,640
and the same machine

00:18:05,840 --> 00:18:09,360
to manage connections to elasticsearch

00:18:08,640 --> 00:18:11,120
and we made

00:18:09,360 --> 00:18:12,960
the php script connect to envoy and

00:18:11,120 --> 00:18:14,960
localhost

00:18:12,960 --> 00:18:16,240
so the connections were encrypted but

00:18:14,960 --> 00:18:18,559
mediated by envoy

00:18:16,240 --> 00:18:19,679
and in this case we obtained 1050

00:18:18,559 --> 00:18:22,080
requests per second

00:18:19,679 --> 00:18:23,679
which is more than double that we got

00:18:22,080 --> 00:18:25,679
with the recalls and

00:18:23,679 --> 00:18:27,600
much much better even the direct calls

00:18:25,679 --> 00:18:30,559
with no encryption

00:18:27,600 --> 00:18:31,919
now take these numbers with um some

00:18:30,559 --> 00:18:35,679
discretion

00:18:31,919 --> 00:18:37,840
there is a large sharer bars like

00:18:35,679 --> 00:18:39,440
across those numbers but still it's

00:18:37,840 --> 00:18:42,000
under 20 throughput gain

00:18:39,440 --> 00:18:43,600
when we use envoy for tls calls to a

00:18:42,000 --> 00:18:45,840
remote data center

00:18:43,600 --> 00:18:47,520
okay this would solve our problem we

00:18:45,840 --> 00:18:48,720
want to call encrypted across data

00:18:47,520 --> 00:18:51,360
centers and we don't want to lose

00:18:48,720 --> 00:18:53,039
performance

00:18:51,360 --> 00:18:54,640
which meant that we started the

00:18:53,039 --> 00:18:56,160
migration to

00:18:54,640 --> 00:18:57,919
a phase two of a migration of

00:18:56,160 --> 00:19:00,080
introducing noise and middleware

00:18:57,919 --> 00:19:02,400
from our biggest application which is

00:19:00,080 --> 00:19:05,520
using php which is mediawiki

00:19:02,400 --> 00:19:06,720
instead of starting like it's customary

00:19:05,520 --> 00:19:10,720
for an sre

00:19:06,720 --> 00:19:13,760
from a smallish service that we don't

00:19:10,720 --> 00:19:13,760
worry too much about

00:19:13,840 --> 00:19:19,039
the game that we were seeing in front of

00:19:16,559 --> 00:19:20,559
us was too large to ignore so we started

00:19:19,039 --> 00:19:23,840
from there

00:19:20,559 --> 00:19:25,919
and let's see what happened so we said

00:19:23,840 --> 00:19:28,320
before we expect to get

00:19:25,919 --> 00:19:29,440
gains of latency mainly across data

00:19:28,320 --> 00:19:32,160
centers

00:19:29,440 --> 00:19:33,520
we expect to get a reduction in the cpu

00:19:32,160 --> 00:19:35,360
and network use

00:19:33,520 --> 00:19:36,640
let's see what happened when we

00:19:35,360 --> 00:19:39,600
transitioned the

00:19:36,640 --> 00:19:41,200
the or through the use of envoy process

00:19:39,600 --> 00:19:44,480
calling session store from

00:19:41,200 --> 00:19:44,880
session store uh you can guess from the

00:19:44,480 --> 00:19:46,720
name

00:19:44,880 --> 00:19:48,000
is just a small gulang application that

00:19:46,720 --> 00:19:51,200
is used to manage

00:19:48,000 --> 00:19:53,360
user sessions and to provide

00:19:51,200 --> 00:19:54,880
that data to other services uh first of

00:19:53,360 --> 00:19:55,600
all media wiki at the time of the

00:19:54,880 --> 00:19:58,480
transition

00:19:55,600 --> 00:20:01,679
uh just about one quarter of a week of a

00:19:58,480 --> 00:20:05,200
tweaky traffic was using session store

00:20:01,679 --> 00:20:06,400
so he was doing 4 500 requests per

00:20:05,200 --> 00:20:09,760
second to

00:20:06,400 --> 00:20:11,280
a session store and still at the moment

00:20:09,760 --> 00:20:12,799
of the transition you can see here that

00:20:11,280 --> 00:20:16,400
the cpu usage

00:20:12,799 --> 00:20:19,520
from all the pods of session store

00:20:16,400 --> 00:20:20,640
went from 2.5 seconds per second so 2.5

00:20:19,520 --> 00:20:24,159
cpus

00:20:20,640 --> 00:20:26,080
to about 0.7 cpus

00:20:24,159 --> 00:20:28,480
this is amazing but let's see what

00:20:26,080 --> 00:20:30,640
happened with the network

00:20:28,480 --> 00:20:32,320
uh so for the network effect was even

00:20:30,640 --> 00:20:35,280
more unexpected

00:20:32,320 --> 00:20:37,760
what we saw was a big drop in the number

00:20:35,280 --> 00:20:39,520
of bytes exchange and we expected that

00:20:37,760 --> 00:20:41,760
but we also saw that the difference

00:20:39,520 --> 00:20:44,880
between transmitted and received data

00:20:41,760 --> 00:20:48,159
basically uh disappeared because we

00:20:44,880 --> 00:20:49,760
weren't transmitting continuously the

00:20:48,159 --> 00:20:52,000
tls certificate 4

00:20:49,760 --> 00:20:54,799
500 times per second but just a few

00:20:52,000 --> 00:20:54,799
times per minute

00:20:55,360 --> 00:20:59,919
okay this is all looks very good right

00:20:58,240 --> 00:21:01,919
on paper but

00:20:59,919 --> 00:21:03,280
what about the latency which is what we

00:21:01,919 --> 00:21:04,880
really care about because that's what

00:21:03,280 --> 00:21:07,520
the users see

00:21:04,880 --> 00:21:09,280
and the latency of both the services and

00:21:07,520 --> 00:21:13,200
media was reduced significantly

00:21:09,280 --> 00:21:16,400
let's see how so this is

00:21:13,200 --> 00:21:18,880
the latency buckets traced for the

00:21:16,400 --> 00:21:19,919
session store service you can see here

00:21:18,880 --> 00:21:22,400
below that the

00:21:19,919 --> 00:21:23,280
green line which is requests that be

00:21:22,400 --> 00:21:25,760
served in

00:21:23,280 --> 00:21:27,200
less than one millisecond doubled

00:21:25,760 --> 00:21:27,520
basically at the time of the transition

00:21:27,200 --> 00:21:29,520
at

00:21:27,520 --> 00:21:31,840
the same time the number of requests

00:21:29,520 --> 00:21:35,280
that took over 10 milliseconds

00:21:31,840 --> 00:21:37,039
almost disappeared so good effects on

00:21:35,280 --> 00:21:39,360
the latency of

00:21:37,039 --> 00:21:40,880
uh session store let's see if it had any

00:21:39,360 --> 00:21:43,039
effect on mediawiki let's

00:21:40,880 --> 00:21:44,320
always remember it was about one-fourth

00:21:43,039 --> 00:21:46,880
of the traffic

00:21:44,320 --> 00:21:47,600
that was affected by the transition

00:21:46,880 --> 00:21:49,679
because

00:21:47,600 --> 00:21:51,760
only one-fourth of the traffic was

00:21:49,679 --> 00:21:54,080
served by was using session store at the

00:21:51,760 --> 00:21:54,080
time

00:21:54,320 --> 00:21:58,240
and at the time of the transition you

00:21:55,840 --> 00:21:59,840
can see here that the number of requests

00:21:58,240 --> 00:22:00,880
the percentage of requests that

00:21:59,840 --> 00:22:03,280
mediawiki was

00:22:00,880 --> 00:22:04,480
responding to and that's when 100

00:22:03,280 --> 00:22:08,480
milliseconds

00:22:04,480 --> 00:22:11,600
went from about 21 20 between 20 and 22

00:22:08,480 --> 00:22:13,840
it went to be between 27 and 25

00:22:11,600 --> 00:22:15,200
so a five percent the increase in the

00:22:13,840 --> 00:22:18,960
number of responses that took

00:22:15,200 --> 00:22:20,720
less than 100 milliseconds not super

00:22:18,960 --> 00:22:21,280
shocking as a result but still pretty

00:22:20,720 --> 00:22:23,039
impressive

00:22:21,280 --> 00:22:27,039
if you keep in mind that we did no

00:22:23,039 --> 00:22:30,240
optimizations at the application layer

00:22:27,039 --> 00:22:31,120
now this game is the random volume and i

00:22:30,240 --> 00:22:33,120
want to

00:22:31,120 --> 00:22:34,799
show it to you with a couple of more

00:22:33,120 --> 00:22:37,120
graphs this is um

00:22:34,799 --> 00:22:39,360
of two different deployments of the same

00:22:37,120 --> 00:22:42,320
application it's a rest gateway to a to

00:22:39,360 --> 00:22:44,480
basically uh one getting two thousand

00:22:42,320 --> 00:22:47,120
requests per second from mediawiki

00:22:44,480 --> 00:22:49,840
and in this case recipe where the usage

00:22:47,120 --> 00:22:52,480
reduction is about 25 percent

00:22:49,840 --> 00:22:54,080
and one where it gets 13 000 requests

00:22:52,480 --> 00:22:56,559
per second and in this case the cpu

00:22:54,080 --> 00:22:58,559
research reduction was 40

00:22:56,559 --> 00:23:00,240
so you have to keep in mind that how

00:22:58,559 --> 00:23:02,159
much you gain by introducing

00:23:00,240 --> 00:23:04,159
um something that does persistent

00:23:02,159 --> 00:23:05,840
connections on behalf of php depends on

00:23:04,159 --> 00:23:08,559
the volume of course

00:23:05,840 --> 00:23:11,120
that you make if you're in in the range

00:23:08,559 --> 00:23:12,880
of making hundreds of calls per second

00:23:11,120 --> 00:23:14,159
probably this is not very important for

00:23:12,880 --> 00:23:16,000
you

00:23:14,159 --> 00:23:17,679
but still there are other things that we

00:23:16,000 --> 00:23:23,280
gained that are appliable

00:23:17,679 --> 00:23:26,799
even to a small scale um

00:23:23,280 --> 00:23:28,159
and one is as i told you before php runs

00:23:26,799 --> 00:23:30,400
active passive so

00:23:28,159 --> 00:23:32,159
at some point at the start of september

00:23:30,400 --> 00:23:32,799
we switched which data center we were

00:23:32,159 --> 00:23:34,880
serving

00:23:32,799 --> 00:23:36,400
media wiki from because we want from

00:23:34,880 --> 00:23:38,480
time to time to

00:23:36,400 --> 00:23:39,520
verify that the infrastructure is sound

00:23:38,480 --> 00:23:42,960
and working

00:23:39,520 --> 00:23:45,600
healthy in both dataset everything went

00:23:42,960 --> 00:23:48,840
good but overnight we noticed that the

00:23:45,600 --> 00:23:50,320
save times reported from end users

00:23:48,840 --> 00:23:53,520
doubled

00:23:50,320 --> 00:23:56,559
for selling edits now normally you

00:23:53,520 --> 00:23:58,960
have no idea where to look because it's

00:23:56,559 --> 00:24:01,200
anything everything basically is working

00:23:58,960 --> 00:24:03,440
fresh for media wiki all of it's stuck

00:24:01,200 --> 00:24:05,679
so you have to figure out which part is

00:24:03,440 --> 00:24:08,000
not working as expected

00:24:05,679 --> 00:24:09,919
thank to using envoy who were able to

00:24:08,000 --> 00:24:11,919
pinpoint within minutes

00:24:09,919 --> 00:24:14,000
where the problem was it was an upstream

00:24:11,919 --> 00:24:17,919
service called envoy

00:24:14,000 --> 00:24:19,039
and to find the solution this was a huge

00:24:17,919 --> 00:24:21,279
advantage compared to

00:24:19,039 --> 00:24:22,480
anything that we had before also in

00:24:21,279 --> 00:24:25,279
addition to that

00:24:22,480 --> 00:24:27,679
circuit breaking in and void is amazing

00:24:25,279 --> 00:24:30,240
even in its default configuration

00:24:27,679 --> 00:24:32,559
um we have some services that media wiki

00:24:30,240 --> 00:24:36,000
calls basically on every request

00:24:32,559 --> 00:24:38,000
and if one of the service works too slow

00:24:36,000 --> 00:24:39,760
uh traditionally would have basically

00:24:38,000 --> 00:24:43,279
all the requests

00:24:39,760 --> 00:24:46,080
piling up in php waiting for a response

00:24:43,279 --> 00:24:50,159
from this remote service because we

00:24:46,080 --> 00:24:50,159
couldn't be too aggressive with timeouts

00:24:50,400 --> 00:24:54,000
but introducing envoy even if it's

00:24:52,320 --> 00:24:57,679
default configuration envoy

00:24:54,000 --> 00:24:59,600
would detect that too many timeouts were

00:24:57,679 --> 00:25:00,000
received from the remote service and

00:24:59,600 --> 00:25:02,000
start

00:25:00,000 --> 00:25:03,360
considering it unhealthy and just

00:25:02,000 --> 00:25:05,200
returning fast

00:25:03,360 --> 00:25:07,360
errors to media wiki which means that

00:25:05,200 --> 00:25:09,200
mediawiki was still able to operate

00:25:07,360 --> 00:25:11,440
even though it was in a degraded state

00:25:09,200 --> 00:25:13,039
it was maybe not reporting statistics

00:25:11,440 --> 00:25:14,640
about visits

00:25:13,039 --> 00:25:16,720
but still it was serving pages to the

00:25:14,640 --> 00:25:20,159
users which is what we care most

00:25:16,720 --> 00:25:21,760
mostly about finally just one notice we

00:25:20,159 --> 00:25:23,919
i want to say that we still have some

00:25:21,760 --> 00:25:25,279
scratches that's normal that's expected

00:25:23,919 --> 00:25:27,039
when you're transitioning to using

00:25:25,279 --> 00:25:28,720
something completely new

00:25:27,039 --> 00:25:30,159
it will take time to earn all of them

00:25:28,720 --> 00:25:34,960
out but overall

00:25:30,159 --> 00:25:37,279
i think this was an unmitigated success

00:25:34,960 --> 00:25:38,799
uh by using envoy we were able to

00:25:37,279 --> 00:25:40,799
encrypt all the communications between

00:25:38,799 --> 00:25:42,159
our services to add a lot of

00:25:40,799 --> 00:25:45,200
observability to

00:25:42,159 --> 00:25:46,000
them and we were able to grammatically

00:25:45,200 --> 00:25:48,080
improve

00:25:46,000 --> 00:25:49,120
the performance of our php applications

00:25:48,080 --> 00:25:51,760
and really anyone

00:25:49,120 --> 00:25:53,360
running a php application at scale

00:25:51,760 --> 00:25:56,080
should think of

00:25:53,360 --> 00:25:56,799
doing something like this well thank you

00:25:56,080 --> 00:25:59,120
for your time

00:25:56,799 --> 00:26:00,559
i hope i didn't bore you too much uh i

00:25:59,120 --> 00:26:02,000
just want to point out that you can see

00:26:00,559 --> 00:26:04,159
all of our dashboards that

00:26:02,000 --> 00:26:07,279
the graphs that i showed you uh are

00:26:04,159 --> 00:26:09,200
taken from at grafanathwithmedia.org

00:26:07,279 --> 00:26:11,520
and you can talk to us on freenode on

00:26:09,200 --> 00:26:14,720
our xenon 3 node

00:26:11,520 --> 00:26:15,200
and my team is angry thank you very with

00:26:14,720 --> 00:26:18,240
this

00:26:15,200 --> 00:26:24,159
final blameless plug i just

00:26:18,240 --> 00:26:26,799
thank you all for listening goodbye

00:26:24,159 --> 00:26:27,760
hello everyone uh i hope you can hear me

00:26:26,799 --> 00:26:30,799
so first of all

00:26:27,760 --> 00:26:32,480
yes jake uh is asking uh when i'm saying

00:26:30,799 --> 00:26:34,480
circuit breaking i'm talking about the

00:26:32,480 --> 00:26:36,559
concept of circuit breaking and in the

00:26:34,480 --> 00:26:37,520
case of envoy in terms of envoys

00:26:36,559 --> 00:26:41,279
terminology it's

00:26:37,520 --> 00:26:42,559
outlier detection in this case yes sorry

00:26:41,279 --> 00:26:44,400
i should have been i should have been

00:26:42,559 --> 00:26:46,480
clearer i realized why i was listening

00:26:44,400 --> 00:26:49,600
now

00:26:46,480 --> 00:26:51,120
so i think i answered in the chat

00:26:49,600 --> 00:26:52,000
already because i knew that i was

00:26:51,120 --> 00:26:55,039
running a bit

00:26:52,000 --> 00:26:56,159
late with the time um so i see that

00:26:55,039 --> 00:26:58,240
derek was asking

00:26:56,159 --> 00:26:59,679
uh if the ultrastarter works differently

00:26:58,240 --> 00:27:02,960
than swapping config files

00:26:59,679 --> 00:27:04,159
we wanted to use the um the hotel

00:27:02,960 --> 00:27:05,279
starter anyways

00:27:04,159 --> 00:27:07,120
for things that are not running

00:27:05,279 --> 00:27:07,679
kubernetes because we don't want to we

00:27:07,120 --> 00:27:09,840
want to

00:27:07,679 --> 00:27:11,919
racially serve all the requests whenever

00:27:09,840 --> 00:27:14,720
we have to restart and buy

00:27:11,919 --> 00:27:16,640
so at that point it was natural to just

00:27:14,720 --> 00:27:18,159
use it just changing files

00:27:16,640 --> 00:27:20,399
we have a procedure to build the files

00:27:18,159 --> 00:27:24,000
basically on yeah puppet and

00:27:20,399 --> 00:27:28,880
and deploy them the configuration files

00:27:24,000 --> 00:27:32,320
and then just we just send a

00:27:28,880 --> 00:27:33,039
sector with a cd agp to uh together

00:27:32,320 --> 00:27:37,520
starter and

00:27:33,039 --> 00:27:37,520
just starts again and any

00:27:37,600 --> 00:27:42,720
a new envoy uh

00:27:41,039 --> 00:27:44,320
to run the new configuration also from

00:27:42,720 --> 00:27:45,600
what i remember if the configuration is

00:27:44,320 --> 00:27:47,840
wrong

00:27:45,600 --> 00:27:49,679
the old version should keep working from

00:27:47,840 --> 00:27:50,480
what i remember but maybe it changed

00:27:49,679 --> 00:27:52,960
over

00:27:50,480 --> 00:27:52,960
last year

00:27:59,200 --> 00:28:02,240
okay i didn't see other questions if i

00:28:01,360 --> 00:28:17,840
missed any

00:28:02,240 --> 00:28:17,840
please just tell me ask them again

00:28:27,360 --> 00:28:30,720
some time she loading failures during

00:28:29,600 --> 00:28:35,279
hot restart

00:28:30,720 --> 00:28:35,279
uh no interesting

00:28:37,679 --> 00:28:41,120
i didn't we didn't experience that but

00:28:39,679 --> 00:28:42,720
probably we're not running

00:28:41,120 --> 00:28:43,840
we're not running and boy at the edge

00:28:42,720 --> 00:28:45,760
we're just running it between the

00:28:43,840 --> 00:28:48,320
services internally so

00:28:45,760 --> 00:28:50,000
every envoy is really doing the amount

00:28:48,320 --> 00:28:52,080
of requests that probably could trigger

00:28:50,000 --> 00:28:55,360
that

00:28:52,080 --> 00:28:56,080
uh every invoice running at pretty low

00:28:55,360 --> 00:28:58,480
scale if you

00:28:56,080 --> 00:29:00,480
if you want right running some maybe a

00:28:58,480 --> 00:29:03,039
thousand requests per second overall

00:29:00,480 --> 00:29:03,039
a peak

00:29:10,960 --> 00:29:16,039
okay thank you very much for listening

00:29:13,919 --> 00:29:19,039
have a nice rest of the

00:29:16,039 --> 00:29:19,039

YouTube URL: https://www.youtube.com/watch?v=uuYkOK3qyxA


