Title: How Niantic switched Pokémon GO to use Envoy - Renana Yacobi
Publication date: 2020-10-21
Playlist: EnvoyCon 2020 - Virtual
Description: 
	How Niantic switched Pokémon GO to use Envoy - Renana Yacobi

Niantic are the creators of Pokemon GO. As one of the world's most popular mobile games, Niantic needs to serve ##'s of players all across the world, concurrently, necessitating the need of a truly planet-scale solution. In this presentation, Rennana Yacobi, Server Core Infrastructure Lead, explains why Niantic made the transition from NGINX to Envoy, starting with the most important question of ‘Why Envoy?’, then reviewing Niantic’s journey with extending Envoy to support our proprietary protocol which includes websockets and player leasing, using xDS to minimize disruptions when scaling, load testing to ensure Envoy can handle millions of QPS at Pokemon GO's scale, where things fall apart, all the way until consolidating everything to the final launch.
Captions: 
	00:00:01,520 --> 00:00:05,759
hi everybody and welcome to how niantic

00:00:03,679 --> 00:00:07,919
switch pokemon go to using boy my name

00:00:05,759 --> 00:00:10,960
is lana nayakovi and i'm a server core

00:00:07,919 --> 00:00:13,599
infrastructure tech lead at niantic

00:00:10,960 --> 00:00:14,639
who are we niantic is a leading ar

00:00:13,599 --> 00:00:16,560
technology company

00:00:14,639 --> 00:00:18,000
focused on exploring new applications

00:00:16,560 --> 00:00:20,080
for advanced hardware

00:00:18,000 --> 00:00:22,000
wearable devices in particular that

00:00:20,080 --> 00:00:23,840
marry the digital world and the physical

00:00:22,000 --> 00:00:26,240
world

00:00:23,840 --> 00:00:27,920
our mission is based on three key

00:00:26,240 --> 00:00:31,359
product principles

00:00:27,920 --> 00:00:33,760
exploration and discovery of new places

00:00:31,359 --> 00:00:36,640
exercise and real world social

00:00:33,760 --> 00:00:38,960
interactions with other people

00:00:36,640 --> 00:00:41,280
this mission has been our compass it's

00:00:38,960 --> 00:00:44,960
paved the way for our consumer ar

00:00:41,280 --> 00:00:46,879
games ingress pokemon go harry potter

00:00:44,960 --> 00:00:48,160
and that's what we are arguably best

00:00:46,879 --> 00:00:50,559
known for

00:00:48,160 --> 00:00:52,480
but we're a lot a whole lot more than

00:00:50,559 --> 00:00:55,039
that underpinning these games

00:00:52,480 --> 00:00:55,760
is a powerful technology platform the

00:00:55,039 --> 00:00:58,160
niantic

00:00:55,760 --> 00:00:58,879
real world platform which we've been

00:00:58,160 --> 00:01:01,199
investing in

00:00:58,879 --> 00:01:03,039
and evolving from the very beginning

00:01:01,199 --> 00:01:03,680
niantic real world platform also

00:01:03,039 --> 00:01:06,479
supports

00:01:03,680 --> 00:01:08,880
social features mapping and advanced ar

00:01:06,479 --> 00:01:12,000
all the components needed to develop and

00:01:08,880 --> 00:01:14,560
publish a real-world ar game

00:01:12,000 --> 00:01:16,880
as our platform grew to offer additional

00:01:14,560 --> 00:01:19,040
components and new games were published

00:01:16,880 --> 00:01:21,840
we needed to adjust infrastructure to

00:01:19,040 --> 00:01:27,680
support the new workloads and complexity

00:01:21,840 --> 00:01:30,479
let's review our journey

00:01:27,680 --> 00:01:32,159
our journey took two parallel routes one

00:01:30,479 --> 00:01:33,360
for unifying our services service

00:01:32,159 --> 00:01:35,200
connectivity

00:01:33,360 --> 00:01:37,119
and the other to simplify and support

00:01:35,200 --> 00:01:38,159
our edge proxy let me start with the

00:01:37,119 --> 00:01:39,840
simpler one

00:01:38,159 --> 00:01:41,840
how we unified our services service

00:01:39,840 --> 00:01:43,840
connectivity and secured our backend

00:01:41,840 --> 00:01:45,680
services

00:01:43,840 --> 00:01:48,320
in the beginning our configuration was

00:01:45,680 --> 00:01:51,040
simple engines deploys our edge proxy

00:01:48,320 --> 00:01:53,040
routing traffic to the game servers

00:01:51,040 --> 00:01:54,640
but as new services were developed and

00:01:53,040 --> 00:01:56,719
deployed different routing

00:01:54,640 --> 00:01:57,520
configurations were created and defined

00:01:56,719 --> 00:02:00,399
by the new

00:01:57,520 --> 00:02:01,920
by the dev teams most connections were

00:02:00,399 --> 00:02:03,840
passing through the front door using

00:02:01,920 --> 00:02:05,840
regional ips hoping not to hit the

00:02:03,840 --> 00:02:08,239
public internet

00:02:05,840 --> 00:02:10,479
others were using external services or

00:02:08,239 --> 00:02:12,640
infrastructure like vpc peering

00:02:10,479 --> 00:02:14,720
which still required a gateway to be

00:02:12,640 --> 00:02:17,200
defined in the cluster

00:02:14,720 --> 00:02:19,840
and we even had services sending data to

00:02:17,200 --> 00:02:21,680
dataflow to update the game's data

00:02:19,840 --> 00:02:23,840
this created several problems that we

00:02:21,680 --> 00:02:25,920
needed to address

00:02:23,840 --> 00:02:27,440
this problem included the need for

00:02:25,920 --> 00:02:29,680
developers to add security and

00:02:27,440 --> 00:02:31,680
instrumentation to code

00:02:29,680 --> 00:02:33,519
our inability to configure routes

00:02:31,680 --> 00:02:35,599
dynamically

00:02:33,519 --> 00:02:37,599
lack of observability and alerting if

00:02:35,599 --> 00:02:40,400
instrumentation wasn't added to code

00:02:37,599 --> 00:02:42,720
are the services indeed connected and

00:02:40,400 --> 00:02:44,959
lastly the need for user is joining the

00:02:42,720 --> 00:02:47,040
team to learn all these technologies

00:02:44,959 --> 00:02:51,200
but have very little control over them

00:02:47,040 --> 00:02:53,440
resulting in maintenance mode

00:02:51,200 --> 00:02:55,440
as a result we focused our efforts in

00:02:53,440 --> 00:02:57,360
finding a solution that will isolate the

00:02:55,440 --> 00:02:59,680
connectivity configuration

00:02:57,360 --> 00:03:02,159
from the application and will will

00:02:59,680 --> 00:03:05,840
enable us to address the rest

00:03:02,159 --> 00:03:05,840
of the issues

00:03:10,080 --> 00:03:16,400
we decided to adopt in voip plus fire

00:03:13,200 --> 00:03:19,120
for our cross-gcp's communication

00:03:16,400 --> 00:03:20,159
the flexibility provided by despair

00:03:19,120 --> 00:03:22,640
enabled us

00:03:20,159 --> 00:03:24,400
to support our different models from

00:03:22,640 --> 00:03:27,920
duplex mode to either pull

00:03:24,400 --> 00:03:30,640
or push add unified security with

00:03:27,920 --> 00:03:32,799
federation for games and services

00:03:30,640 --> 00:03:35,120
by creating a pool of gateways we were

00:03:32,799 --> 00:03:36,799
also able to shift all services from

00:03:35,120 --> 00:03:38,959
using the edge proxy

00:03:36,799 --> 00:03:41,440
to using their dedicated more secure

00:03:38,959 --> 00:03:43,680
endpoints

00:03:41,440 --> 00:03:44,720
so we named our new project service

00:03:43,680 --> 00:03:47,120
gateway

00:03:44,720 --> 00:03:49,200
mainly to this distinguish it from

00:03:47,120 --> 00:03:51,040
service mesh that became a synonym for

00:03:49,200 --> 00:03:53,840
istio in the company

00:03:51,040 --> 00:03:55,519
caused a lot of confusion what we loved

00:03:53,840 --> 00:03:57,599
about this approach was that there was

00:03:55,519 --> 00:04:00,400
no need for application code changes and

00:03:57,599 --> 00:04:02,400
we only had to do cleanup in addition

00:04:00,400 --> 00:04:04,000
the solution was cloud native and ensure

00:04:02,400 --> 00:04:06,159
high availability

00:04:04,000 --> 00:04:07,760
we used gradle scripts with helm charts

00:04:06,159 --> 00:04:09,040
to deploy and configure and voice

00:04:07,760 --> 00:04:11,280
inspire

00:04:09,040 --> 00:04:13,360
and our measure performance was one

00:04:11,280 --> 00:04:14,480
milliseconds of added latency with max

00:04:13,360 --> 00:04:17,120
measure load of

00:04:14,480 --> 00:04:18,079
20 000 requests per second per one

00:04:17,120 --> 00:04:22,320
virtual

00:04:18,079 --> 00:04:22,320
cpu which was pretty good

00:04:22,560 --> 00:04:26,479
one question you are probably wondering

00:04:24,320 --> 00:04:28,240
about was why we didn't adopt service

00:04:26,479 --> 00:04:30,479
mesh implementation for orchestration

00:04:28,240 --> 00:04:34,160
and management for the mesh

00:04:30,479 --> 00:04:37,280
in one word maturity when looking at

00:04:34,160 --> 00:04:39,520
cmcf landscape map under service mesh

00:04:37,280 --> 00:04:41,360
these are the graduated projects these

00:04:39,520 --> 00:04:43,680
are the incubating projects

00:04:41,360 --> 00:04:45,280
but most of the projects are still in

00:04:43,680 --> 00:04:48,320
send back stage

00:04:45,280 --> 00:04:50,080
or not yet added to the map

00:04:48,320 --> 00:04:52,800
when we started our journey we

00:04:50,080 --> 00:04:54,080
investigated using istio but decided to

00:04:52,800 --> 00:04:56,320
hold off for now

00:04:54,080 --> 00:04:58,960
due to complex configuration and

00:04:56,320 --> 00:05:02,080
breaking changes between versions

00:04:58,960 --> 00:05:04,000
looking at our next steps with a future

00:05:02,080 --> 00:05:06,639
of increasing numbers of back-end

00:05:04,000 --> 00:05:08,400
services and games it is obvious for us

00:05:06,639 --> 00:05:10,800
that we will need to find a solution for

00:05:08,400 --> 00:05:12,560
orchestration and management at scale

00:05:10,800 --> 00:05:14,639
what we liked about shifting to envoy

00:05:12,560 --> 00:05:15,600
was the most of the tools under

00:05:14,639 --> 00:05:17,759
development

00:05:15,600 --> 00:05:18,880
we're using envoy as the proxy so

00:05:17,759 --> 00:05:21,039
migrating to them

00:05:18,880 --> 00:05:22,320
should be an incremental step and we

00:05:21,039 --> 00:05:24,639
still had the option

00:05:22,320 --> 00:05:27,440
of using xts in the meantime as our

00:05:24,639 --> 00:05:27,440
management tool

00:05:27,919 --> 00:05:33,199
let's review now our more interesting

00:05:30,080 --> 00:05:35,840
journey with envoy as an edge proxy

00:05:33,199 --> 00:05:37,440
i mentioned before that our game cluster

00:05:35,840 --> 00:05:39,520
deployment is simple

00:05:37,440 --> 00:05:40,880
but while the deployment is simple there

00:05:39,520 --> 00:05:42,639
were several things you wanted to

00:05:40,880 --> 00:05:44,560
address

00:05:42,639 --> 00:05:46,000
improving user experience when scaling

00:05:44,560 --> 00:05:46,800
was one of the goals and i will

00:05:46,000 --> 00:05:48,960
elaborate

00:05:46,800 --> 00:05:51,280
more about what discovers and why in a

00:05:48,960 --> 00:05:52,960
minute but besides this issue we also

00:05:51,280 --> 00:05:54,000
wanted to improve our monitoring and

00:05:52,960 --> 00:05:55,919
alerts

00:05:54,000 --> 00:05:58,000
unify as much as possible our tools and

00:05:55,919 --> 00:05:58,960
data plane and lastly support the

00:05:58,000 --> 00:06:01,039
company's goal

00:05:58,960 --> 00:06:07,120
in enabling external developers to

00:06:01,039 --> 00:06:09,039
create new games without exposing rrep

00:06:07,120 --> 00:06:11,680
i would like to focus for a moment on

00:06:09,039 --> 00:06:13,919
the goal of improving user experience

00:06:11,680 --> 00:06:15,840
when scaling mainly because this part of

00:06:13,919 --> 00:06:18,800
our our architecture

00:06:15,840 --> 00:06:20,560
caused us some trouble here the big

00:06:18,800 --> 00:06:22,720
question of course is why scaling

00:06:20,560 --> 00:06:25,039
created bad user experience

00:06:22,720 --> 00:06:28,479
problem lies in the proxy configuration

00:06:25,039 --> 00:06:30,639
and config map limitations

00:06:28,479 --> 00:06:31,600
our proxy configuration is tied very

00:06:30,639 --> 00:06:35,280
much into our

00:06:31,600 --> 00:06:37,520
ip for every incoming request the proxy

00:06:35,280 --> 00:06:40,800
is checking if the request url

00:06:37,520 --> 00:06:43,120
has a prefix of an opaque id if one is

00:06:40,800 --> 00:06:44,160
found the proxy matches that id to a

00:06:43,120 --> 00:06:46,840
game server

00:06:44,160 --> 00:06:48,240
removes the prefix and route the request

00:06:46,840 --> 00:06:50,800
accordingly

00:06:48,240 --> 00:06:53,199
we crest without id a round rubbing

00:06:50,800 --> 00:06:54,960
between the available game servers

00:06:53,199 --> 00:06:57,120
we refer to this specific part of the

00:06:54,960 --> 00:07:00,000
configuration as the catch-all

00:06:57,120 --> 00:07:02,000
in addition our protocol supports http

00:07:00,000 --> 00:07:04,720
and websockets requests

00:07:02,000 --> 00:07:07,199
as a result our configuration file can't

00:07:04,720 --> 00:07:10,400
fit into kubernetes config map

00:07:07,199 --> 00:07:12,400
and is deployed inside the proxy image

00:07:10,400 --> 00:07:14,800
this problem is true for engines and

00:07:12,400 --> 00:07:14,800
envoy

00:07:15,280 --> 00:07:19,120
looking on the process of scaling up we

00:07:17,840 --> 00:07:20,720
first increase the number of game

00:07:19,120 --> 00:07:23,680
servers replica

00:07:20,720 --> 00:07:25,759
when all instances are happen healthy we

00:07:23,680 --> 00:07:28,800
update the configuration of the routes

00:07:25,759 --> 00:07:31,280
leaving the catch all the same this

00:07:28,800 --> 00:07:32,639
creating a new image and update the

00:07:31,280 --> 00:07:34,960
deployment

00:07:32,639 --> 00:07:36,560
this step can be disruptive to clients

00:07:34,960 --> 00:07:38,479
that are currently connected through

00:07:36,560 --> 00:07:40,560
websockets or pending a response from

00:07:38,479 --> 00:07:42,880
the server

00:07:40,560 --> 00:07:44,479
when all pods are updated and healthy we

00:07:42,880 --> 00:07:46,319
update the backend with the number of

00:07:44,479 --> 00:07:48,400
available game servers

00:07:46,319 --> 00:07:51,039
and update again the proxy configuration

00:07:48,400 --> 00:07:53,680
creating a second wave of disruptions

00:07:51,039 --> 00:07:56,639
scaling down process is very similar

00:07:53,680 --> 00:07:59,360
only in reverse order

00:07:56,639 --> 00:08:01,360
well next yes to the rescue using a

00:07:59,360 --> 00:08:03,280
custom xds operator we were able to

00:08:01,360 --> 00:08:05,919
scale up and down the clusters without

00:08:03,280 --> 00:08:08,400
the need to restart or redeploy them

00:08:05,919 --> 00:08:10,560
the operator receives the requested

00:08:08,400 --> 00:08:11,680
cluster size through a kubernetes custom

00:08:10,560 --> 00:08:13,680
resource

00:08:11,680 --> 00:08:15,039
update the configuration and pass it to

00:08:13,680 --> 00:08:16,800
envoy

00:08:15,039 --> 00:08:18,639
we are currently in the process of

00:08:16,800 --> 00:08:20,400
deploying this new operator

00:08:18,639 --> 00:08:22,000
our next steps after verifying the

00:08:20,400 --> 00:08:23,039
operator is stable and working as

00:08:22,000 --> 00:08:24,960
expected

00:08:23,039 --> 00:08:27,120
will be to integrate it with the game

00:08:24,960 --> 00:08:30,080
server existing drain feature for a

00:08:27,120 --> 00:08:34,320
seamless scale experience

00:08:30,080 --> 00:08:34,320
this overview our new deployment

00:08:35,680 --> 00:08:40,159
after verifying we can achieve our goals

00:08:38,080 --> 00:08:41,680
using envoy we started to plan and

00:08:40,159 --> 00:08:43,360
execute the migration from engine

00:08:41,680 --> 00:08:45,279
standpoint

00:08:43,360 --> 00:08:47,440
our high level plan included running

00:08:45,279 --> 00:08:49,120
load testing to verify performance and

00:08:47,440 --> 00:08:51,839
provisioning of envoy

00:08:49,120 --> 00:08:52,480
test immigration scenario run books and

00:08:51,839 --> 00:08:54,959
roll back

00:08:52,480 --> 00:08:57,120
in the lotus environment deploy the new

00:08:54,959 --> 00:08:58,480
envoy instances and shift traffic from

00:08:57,120 --> 00:09:01,920
engines to endpoint

00:08:58,480 --> 00:09:04,160
sounds easy right we started with load

00:09:01,920 --> 00:09:05,600
testing using a pokemon go load testing

00:09:04,160 --> 00:09:07,600
setup and runners

00:09:05,600 --> 00:09:08,959
we used the latest invoice version which

00:09:07,600 --> 00:09:11,680
was 112

00:09:08,959 --> 00:09:12,880
and started with a basic environment of

00:09:11,680 --> 00:09:15,920
x game servers

00:09:12,880 --> 00:09:18,000
x caching services one proxy

00:09:15,920 --> 00:09:19,760
and a load of hundreds of thousands of

00:09:18,000 --> 00:09:22,720
requests per second

00:09:19,760 --> 00:09:24,320
the result looks great except some pesky

00:09:22,720 --> 00:09:26,959
five or three errors that surface from

00:09:24,320 --> 00:09:26,959
time to time

00:09:27,760 --> 00:09:31,120
we had to make a couple of changes to

00:09:29,600 --> 00:09:33,920
our production environment to be able to

00:09:31,120 --> 00:09:36,399
shift the traffic from engines to enjoy

00:09:33,920 --> 00:09:38,160
first change was to separate the traffic

00:09:36,399 --> 00:09:40,240
the public traffic from the backend

00:09:38,160 --> 00:09:42,080
services i mentioned before

00:09:40,240 --> 00:09:44,080
since we didn't onboard the service

00:09:42,080 --> 00:09:45,760
gateway yet we took the approach of

00:09:44,080 --> 00:09:48,320
simply duplicating the engine's

00:09:45,760 --> 00:09:50,320
configuration and creating a dedicated

00:09:48,320 --> 00:09:52,560
pool for them

00:09:50,320 --> 00:09:54,560
we then had to replace the existing

00:09:52,560 --> 00:09:57,279
external load balancer service with a

00:09:54,560 --> 00:10:00,000
node port with external traffic policy

00:09:57,279 --> 00:10:01,839
set to local instead of cluster this

00:10:00,000 --> 00:10:02,880
allowed us to control the traffic and

00:10:01,839 --> 00:10:04,880
monitor both

00:10:02,880 --> 00:10:08,399
proxy pools while serving on the same

00:10:04,880 --> 00:10:10,880
port which is a gclp limitation

00:10:08,399 --> 00:10:13,040
we also switched from old replication

00:10:10,880 --> 00:10:14,800
controller to a deployment object

00:10:13,040 --> 00:10:18,000
which gave us an opportunity to do a

00:10:14,800 --> 00:10:20,880
simplified dry run of the traffic shift

00:10:18,000 --> 00:10:22,000
lastly we switch from utilization based

00:10:20,880 --> 00:10:24,240
load balancing

00:10:22,000 --> 00:10:26,399
to count based load balancing due to

00:10:24,240 --> 00:10:27,600
difference in resources between engines

00:10:26,399 --> 00:10:29,680
and envoy

00:10:27,600 --> 00:10:30,720
we copied the already prepared grafano

00:10:29,680 --> 00:10:35,519
dashboards

00:10:30,720 --> 00:10:37,200
and created a new poll for dingboyflake

00:10:35,519 --> 00:10:38,959
one of the features we couldn't find a

00:10:37,200 --> 00:10:40,160
replacement for by the way is the

00:10:38,959 --> 00:10:41,839
ipdenylist

00:10:40,160 --> 00:10:44,160
and we had to replace it with the google

00:10:41,839 --> 00:10:45,600
cloud armor if there is an existing

00:10:44,160 --> 00:10:47,440
invoice filter that provides this

00:10:45,600 --> 00:10:48,959
functionality we would love to hear

00:10:47,440 --> 00:10:50,959
about it but if not

00:10:48,959 --> 00:10:52,720
we will probably write a custom filter

00:10:50,959 --> 00:10:54,560
to provide this functionality

00:10:52,720 --> 00:10:56,800
we're becoming very good in writing

00:10:54,560 --> 00:10:56,800
those

00:10:57,040 --> 00:11:00,800
well the big day arrived and we started

00:10:59,120 --> 00:11:01,600
shifting the traffic from engines to

00:11:00,800 --> 00:11:04,079
invoice well

00:11:01,600 --> 00:11:04,880
we started started deploying the traffic

00:11:04,079 --> 00:11:06,640
shift plan

00:11:04,880 --> 00:11:08,800
was to first increase the number of

00:11:06,640 --> 00:11:10,880
envoy replicas in the pool

00:11:08,800 --> 00:11:12,880
when reaching full capacity decrease the

00:11:10,880 --> 00:11:15,519
number of engines replica

00:11:12,880 --> 00:11:17,760
until hitting zero we started with

00:11:15,519 --> 00:11:20,399
deploying a single invoice instance

00:11:17,760 --> 00:11:21,680
it was there for a couple of minutes

00:11:20,399 --> 00:11:25,200
when we started seeing an

00:11:21,680 --> 00:11:27,920
increase in five or three hours and then

00:11:25,200 --> 00:11:29,839
it crashed we immediately scale down to

00:11:27,920 --> 00:11:32,560
zero

00:11:29,839 --> 00:11:34,240
this is just a short version but after

00:11:32,560 --> 00:11:36,079
several days of investigations and

00:11:34,240 --> 00:11:37,279
digging up in envoy and gclp

00:11:36,079 --> 00:11:40,079
documentation

00:11:37,279 --> 00:11:42,160
we found the following results the main

00:11:40,079 --> 00:11:44,320
reason for the invoice crash was because

00:11:42,160 --> 00:11:47,519
we were hitting it with 20

00:11:44,320 --> 00:11:49,440
times the expected load and even more

00:11:47,519 --> 00:11:51,440
it was already deployed on a pod with

00:11:49,440 --> 00:11:52,880
limited resources which might have been

00:11:51,440 --> 00:11:54,959
a contributor factor

00:11:52,880 --> 00:11:56,079
but unfortunately after correcting the

00:11:54,959 --> 00:11:58,720
traffic distribution

00:11:56,079 --> 00:12:01,200
to 2000 requests per second we never

00:11:58,720 --> 00:12:03,839
encountered this problem again

00:12:01,200 --> 00:12:05,440
the 503 errors were easier to understand

00:12:03,839 --> 00:12:07,600
but harder to find

00:12:05,440 --> 00:12:09,760
as they were a result of a discrepancy

00:12:07,600 --> 00:12:11,440
in the timeout for idle connections

00:12:09,760 --> 00:12:13,760
causing gclb to return

00:12:11,440 --> 00:12:15,519
503s for connections that were

00:12:13,760 --> 00:12:17,920
terminated by envoy

00:12:15,519 --> 00:12:21,200
fixing this part decreased the 503's

00:12:17,920 --> 00:12:21,200
errors to a normal level

00:12:22,240 --> 00:12:27,040
as it usually happens we were ready for

00:12:24,320 --> 00:12:29,040
second try only two weeks later

00:12:27,040 --> 00:12:30,880
we fixed the configuration scale it up

00:12:29,040 --> 00:12:32,399
to one instance and started monitoring

00:12:30,880 --> 00:12:34,480
the service

00:12:32,399 --> 00:12:36,320
it was happily running for a day with no

00:12:34,480 --> 00:12:38,639
problems so we decided we are ready for

00:12:36,320 --> 00:12:41,360
the next step

00:12:38,639 --> 00:12:42,839
and scale it up to join voice instances

00:12:41,360 --> 00:12:45,920
and we enter the new era of

00:12:42,839 --> 00:12:48,079
multi-instances in production

00:12:45,920 --> 00:12:50,160
over the next two weeks we slowly slowly

00:12:48,079 --> 00:12:52,160
scaled up the envoy fleet from two to

00:12:50,160 --> 00:12:53,839
five from five to fifty percent

00:12:52,160 --> 00:12:55,519
and so on until we hit one hundred

00:12:53,839 --> 00:12:57,200
percent

00:12:55,519 --> 00:12:58,560
when hitting one hundred percent of it

00:12:57,200 --> 00:13:00,800
void deployments and after

00:12:58,560 --> 00:13:01,920
verifying all is okay or at least that's

00:13:00,800 --> 00:13:04,880
what we thought

00:13:01,920 --> 00:13:06,720
we started scaling down the engine split

00:13:04,880 --> 00:13:08,880
parallel to this migration we had new

00:13:06,720 --> 00:13:09,680
features and services also added to the

00:13:08,880 --> 00:13:11,680
environment

00:13:09,680 --> 00:13:13,040
creating problems of their own so when

00:13:11,680 --> 00:13:15,040
problems started hitting us

00:13:13,040 --> 00:13:17,440
we didn't recognize what is the source

00:13:15,040 --> 00:13:20,320
of them

00:13:17,440 --> 00:13:21,839
everything was looking good except

00:13:20,320 --> 00:13:24,160
occasional 503

00:13:21,839 --> 00:13:26,560
errors and we also noticed an increase

00:13:24,160 --> 00:13:29,279
in nx domains received from code dns

00:13:26,560 --> 00:13:31,920
that started when we deployed envoy we

00:13:29,279 --> 00:13:34,079
had no success tuning tube dns and had

00:13:31,920 --> 00:13:36,240
little information to understand the

00:13:34,079 --> 00:13:38,240
source of the problem

00:13:36,240 --> 00:13:41,279
like any other team we were busy and

00:13:38,240 --> 00:13:43,519
with little information or access

00:13:41,279 --> 00:13:44,399
this turned to be the status quo for

00:13:43,519 --> 00:13:47,760
four months

00:13:44,399 --> 00:13:50,240
until goldfaced arrived

00:13:47,760 --> 00:13:52,320
if you're not aware go fest is our

00:13:50,240 --> 00:13:54,160
biggest event of the year

00:13:52,320 --> 00:13:55,920
when starting to scale the number of

00:13:54,160 --> 00:13:56,399
game servers in preparation for the

00:13:55,920 --> 00:13:58,560
event

00:13:56,399 --> 00:13:59,440
we started receiving reports of an

00:13:58,560 --> 00:14:02,160
increase

00:13:59,440 --> 00:14:03,760
in five or three errors this happened

00:14:02,160 --> 00:14:07,360
immediately after deploying

00:14:03,760 --> 00:14:09,279
envoy images with the new configuration

00:14:07,360 --> 00:14:12,240
if you remember step two in the scallop

00:14:09,279 --> 00:14:16,160
scenario that was before

00:14:12,240 --> 00:14:18,079
we even tried to scaling void but hoping

00:14:16,160 --> 00:14:20,240
this might be related to load

00:14:18,079 --> 00:14:21,440
with interesting void fleet size we

00:14:20,240 --> 00:14:24,720
didn't know that this

00:14:21,440 --> 00:14:25,920
actually made things worse we also

00:14:24,720 --> 00:14:28,079
started noticing

00:14:25,920 --> 00:14:29,440
an increase in player complaints on the

00:14:28,079 --> 00:14:32,399
down detector

00:14:29,440 --> 00:14:33,360
we investigated that configuration wrong

00:14:32,399 --> 00:14:36,880
health check

00:14:33,360 --> 00:14:39,920
everything but found no clue except

00:14:36,880 --> 00:14:40,880
an interesting decrease in coop dns

00:14:39,920 --> 00:14:43,120
errors

00:14:40,880 --> 00:14:45,680
that part confused us more but then we

00:14:43,120 --> 00:14:48,320
found a hint

00:14:45,680 --> 00:14:48,880
and just a reminder our deployed void

00:14:48,320 --> 00:14:55,360
version

00:14:48,880 --> 00:14:55,360
was 112 and this was fixed only in 114

00:14:55,760 --> 00:14:59,519
so given the timeline and the importance

00:14:58,079 --> 00:15:01,680
of go fest the company

00:14:59,519 --> 00:15:04,079
to the company engines was deployed to

00:15:01,680 --> 00:15:06,560
replace envoy and go fast fast with no

00:15:04,079 --> 00:15:08,240
issues from the edge proxy and we went

00:15:06,560 --> 00:15:12,639
back to our low test environment to try

00:15:08,240 --> 00:15:14,639
and figure out how to solve this problem

00:15:12,639 --> 00:15:16,720
back to our low test environment we

00:15:14,639 --> 00:15:18,000
started with reconfiguring it to match

00:15:16,720 --> 00:15:20,320
production

00:15:18,000 --> 00:15:22,800
we scale it up to much one tenth of

00:15:20,320 --> 00:15:25,120
production and configure to have

00:15:22,800 --> 00:15:27,360
x10 services pointing to the game

00:15:25,120 --> 00:15:29,040
servers in help to recreate the

00:15:27,360 --> 00:15:30,160
conditions that caused the problem with

00:15:29,040 --> 00:15:32,959
sound production

00:15:30,160 --> 00:15:34,320
it was easy we got the ns errors already

00:15:32,959 --> 00:15:37,519
when hitting one

00:15:34,320 --> 00:15:39,440
30s of the production environment

00:15:37,519 --> 00:15:41,120
at that point i already knew what was

00:15:39,440 --> 00:15:44,000
the problem and maybe some of you are

00:15:41,120 --> 00:15:44,000
already guessing it

00:15:44,240 --> 00:15:48,320
it was our configuration that caused the

00:15:46,560 --> 00:15:51,519
loading dns queries

00:15:48,320 --> 00:15:54,240
our stateful back-end configuration was

00:15:51,519 --> 00:15:56,880
causing the proxies to query two dns

00:15:54,240 --> 00:15:58,000
four times for each game server now

00:15:56,880 --> 00:16:00,320
multiply that by

00:15:58,000 --> 00:16:01,600
the number of proxies add to that the

00:16:00,320 --> 00:16:04,560
number of game servers

00:16:01,600 --> 00:16:05,839
and the number becomes too high a

00:16:04,560 --> 00:16:08,240
solution we tested

00:16:05,839 --> 00:16:09,680
was google cloud new dns caching

00:16:08,240 --> 00:16:12,959
operator which made the

00:16:09,680 --> 00:16:16,639
errors disappear completely but will

00:16:12,959 --> 00:16:18,720
require an update to the jke version

00:16:16,639 --> 00:16:20,800
so my personal learnings are stop

00:16:18,720 --> 00:16:22,399
ignoring intermittent error

00:16:20,800 --> 00:16:24,639
testing production like environment

00:16:22,399 --> 00:16:27,040
instead of a low test environment

00:16:24,639 --> 00:16:29,360
continue evolving this traffic shift

00:16:27,040 --> 00:16:31,920
scenario to support future software and

00:16:29,360 --> 00:16:37,120
infrastructure upgrade just a reminder

00:16:31,920 --> 00:16:39,199
we will need to upgrade our jke version

00:16:37,120 --> 00:16:41,279
one last part of envoy i would like to

00:16:39,199 --> 00:16:44,079
talk about is our work to extend

00:16:41,279 --> 00:16:45,839
of extending envoy mainly to provide

00:16:44,079 --> 00:16:46,720
external developers access to our

00:16:45,839 --> 00:16:50,480
platform

00:16:46,720 --> 00:16:52,320
without exposing our ip

00:16:50,480 --> 00:16:54,639
unfortunately i can't share a lot about

00:16:52,320 --> 00:16:56,480
this work i can tell you that included

00:16:54,639 --> 00:16:58,000
request validation for http and

00:16:56,480 --> 00:17:00,079
websockets requests

00:16:58,000 --> 00:17:02,639
shifting the responsibility to block

00:17:00,079 --> 00:17:05,360
this traffic already in the proxy

00:17:02,639 --> 00:17:07,520
it also manipulates the incoming traffic

00:17:05,360 --> 00:17:09,520
based on the received message

00:17:07,520 --> 00:17:11,520
and provide us with additional metrics

00:17:09,520 --> 00:17:13,760
related to the games

00:17:11,520 --> 00:17:15,600
part of this includes a websocket custom

00:17:13,760 --> 00:17:17,919
filter that we wonder if

00:17:15,600 --> 00:17:19,360
it shouldn't be part of the platform

00:17:17,919 --> 00:17:23,919
even for the purpose of

00:17:19,360 --> 00:17:23,919
upgrading it to a grpc at the proxy

00:17:24,160 --> 00:17:28,880
and this is truly how our deployment

00:17:26,400 --> 00:17:28,880
looks like

00:17:29,840 --> 00:17:33,440
i would like to summarize this stock

00:17:31,360 --> 00:17:35,440
with a wine voice slide

00:17:33,440 --> 00:17:37,440
well we chose envoy for the better

00:17:35,440 --> 00:17:39,039
observability in monitoring

00:17:37,440 --> 00:17:41,280
service to service proxy the

00:17:39,039 --> 00:17:42,720
configuration the better scale

00:17:41,280 --> 00:17:46,160
experience with xds

00:17:42,720 --> 00:17:47,760
and finally the extendability

00:17:46,160 --> 00:17:49,840
thank you for listening and i would like

00:17:47,760 --> 00:17:51,360
to thank everyone in niantic that helped

00:17:49,840 --> 00:17:53,039
me in this work

00:17:51,360 --> 00:17:55,600
and for those of you that find this

00:17:53,039 --> 00:17:58,400
interesting and would like to learn more

00:17:55,600 --> 00:18:00,720
we are hiring feel free to reach to me

00:17:58,400 --> 00:18:02,640
and check out our career website

00:18:00,720 --> 00:18:06,799
thank you and i will be very happy to

00:18:02,640 --> 00:18:06,799
answer now any question you might have

00:18:07,679 --> 00:18:10,960
hi everybody i hope you hear me and to

00:18:10,400 --> 00:18:13,679
me

00:18:10,960 --> 00:18:14,080
um and i will be very happy to answer

00:18:13,679 --> 00:18:16,480
any

00:18:14,080 --> 00:18:17,520
of your questions that were in the uh

00:18:16,480 --> 00:18:20,640
chat

00:18:17,520 --> 00:18:22,320
um so uh

00:18:20,640 --> 00:18:23,840
first question did you observe any

00:18:22,320 --> 00:18:25,840
issues when moving from engines to

00:18:23,840 --> 00:18:27,360
devoid in terms of how the downstream

00:18:25,840 --> 00:18:30,320
connections are

00:18:27,360 --> 00:18:30,880
maintained by engines versus envoy and i

00:18:30,320 --> 00:18:33,760
actually

00:18:30,880 --> 00:18:34,799
asked here um continuing questions

00:18:33,760 --> 00:18:38,160
regarding uh

00:18:34,799 --> 00:18:41,200
was it something specific so um as

00:18:38,160 --> 00:18:42,640
as sweeney that trinity uh mentioned we

00:18:41,200 --> 00:18:44,720
have seen an issue where

00:18:42,640 --> 00:18:45,679
void retains high throughput connections

00:18:44,720 --> 00:18:47,760
constantly

00:18:45,679 --> 00:18:49,600
whereas engines releases them priority

00:18:47,760 --> 00:18:50,880
did you see such issue during your

00:18:49,600 --> 00:18:52,880
migration

00:18:50,880 --> 00:18:54,480
well uh we didn't see this as an issue

00:18:52,880 --> 00:18:57,600
um this was

00:18:54,480 --> 00:18:59,200
part of the behavior we already knew

00:18:57,600 --> 00:19:02,080
that this part of your boy

00:18:59,200 --> 00:19:03,039
um as i mentioned the main difference

00:19:02,080 --> 00:19:05,840
that we

00:19:03,039 --> 00:19:07,280
had between envoy and engines was

00:19:05,840 --> 00:19:10,559
actually around the dns

00:19:07,280 --> 00:19:14,240
um and the dns query and jenks

00:19:10,559 --> 00:19:17,600
are not um queering the dns and such

00:19:14,240 --> 00:19:20,160
uh so frequently which was actually um

00:19:17,600 --> 00:19:21,440
made us had some made us do some work

00:19:20,160 --> 00:19:23,280
models in our environments

00:19:21,440 --> 00:19:25,840
with and we didn't want we can now

00:19:23,280 --> 00:19:28,559
remove them but

00:19:25,840 --> 00:19:29,760
we actually um the only issue that we

00:19:28,559 --> 00:19:33,440
had was with the

00:19:29,760 --> 00:19:36,480
number of dns scores that we had um

00:19:33,440 --> 00:19:38,640
second question was um can you elaborate

00:19:36,480 --> 00:19:41,200
on switching from utilization based on

00:19:38,640 --> 00:19:43,520
bouncing to count based load balancing

00:19:41,200 --> 00:19:44,720
does count based load balancing refers

00:19:43,520 --> 00:19:48,240
to a number of

00:19:44,720 --> 00:19:51,679
incoming requests um so i

00:19:48,240 --> 00:19:53,679
forward this question to our sres and um

00:19:51,679 --> 00:19:54,720
part of our team that is also was

00:19:53,679 --> 00:19:57,760
working on that

00:19:54,720 --> 00:20:00,720
um i i believe this is a gclb

00:19:57,760 --> 00:20:01,200
uh property but i might be wrong here

00:20:00,720 --> 00:20:05,039
and

00:20:01,200 --> 00:20:06,880
um if we were mainly um

00:20:05,039 --> 00:20:08,159
we're mainly just working on yes the

00:20:06,880 --> 00:20:11,039
incoming requests so

00:20:08,159 --> 00:20:12,000
ensure that they're being um correctly

00:20:11,039 --> 00:20:15,120
um

00:20:12,000 --> 00:20:16,159
load balance um between different envoy

00:20:15,120 --> 00:20:20,240
instances

00:20:16,159 --> 00:20:22,880
of the backend services and um

00:20:20,240 --> 00:20:24,400
thank you for the um ip deny list we

00:20:22,880 --> 00:20:26,320
will definitely look into that i think

00:20:24,400 --> 00:20:28,080
we looked at that in the past but

00:20:26,320 --> 00:20:30,080
if you notice from the presentation we

00:20:28,080 --> 00:20:33,280
started our journey a year ago

00:20:30,080 --> 00:20:34,000
uh when it was in void 12 and in 112 and

00:20:33,280 --> 00:20:37,840
it was

00:20:34,000 --> 00:20:39,440
um envoy evolved a lot since then

00:20:37,840 --> 00:20:42,559
there are a lot of new things that we

00:20:39,440 --> 00:20:48,320
are using um

00:20:42,559 --> 00:20:52,080
so let me get back um

00:20:48,320 --> 00:20:52,080
for another questions

00:21:01,120 --> 00:21:05,840
so i get a lot of uh feedback career

00:21:03,520 --> 00:21:08,559
around what i should do look for the ip

00:21:05,840 --> 00:21:10,559
uh deny um thank you i'll definitely

00:21:08,559 --> 00:21:11,600
look into them it's really interesting i

00:21:10,559 --> 00:21:17,840
heard that our back

00:21:11,600 --> 00:21:17,840
can help with that um

00:21:19,280 --> 00:21:22,880
well uh rubens if you'll tell me which

00:21:21,440 --> 00:21:25,120
slide maybe i can share uh

00:21:22,880 --> 00:21:26,880
the presentation but i think everything

00:21:25,120 --> 00:21:29,280
will be online as soon

00:21:26,880 --> 00:21:31,200
and you are always welcome to uh contact

00:21:29,280 --> 00:21:34,320
me through the stack i'm on the

00:21:31,200 --> 00:21:37,520
voice live channels um or through uh

00:21:34,320 --> 00:21:38,320
linkedin or my email and thank you

00:21:37,520 --> 00:21:41,760
everybody

00:21:38,320 --> 00:21:42,400
um somebody asked about the control

00:21:41,760 --> 00:21:44,640
plane and i

00:21:42,400 --> 00:21:45,840
missed the question so we are using

00:21:44,640 --> 00:21:49,840
control plane

00:21:45,840 --> 00:21:53,280
um that was uh one of our moves

00:21:49,840 --> 00:21:53,919
to make the scale better um we are

00:21:53,280 --> 00:21:56,559
actually

00:21:53,919 --> 00:21:57,360
now going um to start deploying it

00:21:56,559 --> 00:22:00,000
everywhere

00:21:57,360 --> 00:22:01,840
we haven't worked it's definitely going

00:22:00,000 --> 00:22:04,400
to improve our scale

00:22:01,840 --> 00:22:04,400
experience

00:22:10,240 --> 00:22:14,320
and by the way for the control plane

00:22:11,919 --> 00:22:17,360
we're using of course lds and cds

00:22:14,320 --> 00:22:19,600
um because our listeners uh path

00:22:17,360 --> 00:22:23,600
matchings and clusters

00:22:19,600 --> 00:22:23,600
are really coupled into one to one

00:22:25,679 --> 00:22:29,360
well we are no longer using engines

00:22:28,559 --> 00:22:32,799
itself

00:22:29,360 --> 00:22:34,559
um in most of our places and the one

00:22:32,799 --> 00:22:41,840
that we are using i believe we're using

00:22:34,559 --> 00:22:41,840
the open source

00:22:51,120 --> 00:22:59,360
well i'm very happy to hear and help um

00:22:55,440 --> 00:23:00,159
and we um we're very happy to share from

00:22:59,360 --> 00:23:01,679
our experience

00:23:00,159 --> 00:23:03,760
uh one of the things as i mentioned by

00:23:01,679 --> 00:23:04,880
the way uh we did have to write a custom

00:23:03,760 --> 00:23:07,440
filter for

00:23:04,880 --> 00:23:09,200
uh web sockets which was a very

00:23:07,440 --> 00:23:12,799
interesting experience

00:23:09,200 --> 00:23:16,080
actually and we are probably um

00:23:12,799 --> 00:23:17,679
we'll be um publishing it

00:23:16,080 --> 00:23:19,200
in a couple of months we need to pass a

00:23:17,679 --> 00:23:21,440
legal process for that

00:23:19,200 --> 00:23:23,840
um so anybody everybody will be able to

00:23:21,440 --> 00:23:23,840
use it

00:23:25,280 --> 00:23:33,120
so um the number of rps per core on

00:23:28,960 --> 00:23:36,000
our envoy um i do have it somewhere

00:23:33,120 --> 00:23:37,760
but i don't really remember um and i

00:23:36,000 --> 00:23:41,200
don't want just to throw numbers

00:23:37,760 --> 00:23:43,039
so uh mikhail if you can um

00:23:41,200 --> 00:23:45,919
just contact mrs tank i will be very

00:23:43,039 --> 00:23:45,919
happy to answer that

00:23:47,440 --> 00:23:51,360
and um yeah thank you everybody uh if

00:23:50,000 --> 00:23:52,640
there are any more questions we have a

00:23:51,360 --> 00:23:56,080
couple more minutes

00:23:52,640 --> 00:24:00,159
but um if not then we have

00:23:56,080 --> 00:24:00,159
six minutes for refreshing

00:24:00,480 --> 00:24:05,840
thank you

00:24:08,080 --> 00:24:12,000
engines features which we could not

00:24:09,840 --> 00:24:14,640
replace with invoice was vip denials

00:24:12,000 --> 00:24:15,440
that was mainly we actually uh gained a

00:24:14,640 --> 00:24:19,679
lot of

00:24:15,440 --> 00:24:23,279
things moving to enjoy um

00:24:19,679 --> 00:24:25,600
and uh we we

00:24:23,279 --> 00:24:27,520
really enjoyed doing this move so no

00:24:25,600 --> 00:24:29,679
actually it was the other way around

00:24:27,520 --> 00:24:33,520
except that the nightly switch i believe

00:24:29,679 --> 00:24:37,679
you will find a solution very fast

00:24:33,520 --> 00:24:37,679

YouTube URL: https://www.youtube.com/watch?v=dUck3tBrfYQ


