Title: Multi-Cluster & Multi-Cloud Service Mesh with CNCF’s Kuma and Envoy - Marco Palladino, Kong
Publication date: 2020-11-25
Playlist: ServiceMeshCon North America 2020
Description: 
	Multi-Cluster & Multi-Cloud Service Mesh with CNCF’s Kuma and Envoy - Marco Palladino, Kong

Learn how to run a distributed Envoy-based service mesh on multiple Kubernetes clusters and multiple clouds in just a few steps with Kuma, a CNCF project. In this session, we'll be firing up Kubernetes clusters in multiple regions to demonstrate how we can secure, route, connect and observe service connectivity in a distributed service mesh.   In this session, we will learn to:  - Use Kuma’s multi-zone deployment to spin up a multi-cluster and multi-region service mesh. - Leverage the global/remote control separation to scale reliability with HA.  - Use the built-in service discovery and ingress capability for out of the box service connectivity across multiple zones, clusters and regions.  - Use Kuma’s policy to determine the behavior of traffic across different clusters, like Traffic Route, mTLS, Traffic Permission and so on.
Captions: 
	00:00:00,000 --> 00:00:04,319
welcome everybody to this live

00:00:02,320 --> 00:00:06,399
demonstration of cuba today i'm very

00:00:04,319 --> 00:00:07,200
excited to be presenting to you this

00:00:06,399 --> 00:00:08,960
session

00:00:07,200 --> 00:00:11,280
because we're going to be seeing the

00:00:08,960 --> 00:00:13,280
results of many months of work

00:00:11,280 --> 00:00:15,280
with the community and with our users

00:00:13,280 --> 00:00:16,560
into building the best multi-cluster and

00:00:15,280 --> 00:00:19,039
multi-cloud support

00:00:16,560 --> 00:00:20,640
that any service mesh has ever seen my

00:00:19,039 --> 00:00:24,080
name is marco palladino

00:00:20,640 --> 00:00:26,480
and i am the cto and co-founder of kong

00:00:24,080 --> 00:00:28,400
and kong really was the organization

00:00:26,480 --> 00:00:31,039
that first created kuma

00:00:28,400 --> 00:00:32,640
and then decided to donate cuma to the

00:00:31,039 --> 00:00:35,280
cncf foundation

00:00:32,640 --> 00:00:36,079
today available as a sandbox project

00:00:35,280 --> 00:00:39,280
which means that

00:00:36,079 --> 00:00:41,920
kuma can be used with the same openness

00:00:39,280 --> 00:00:42,320
the same neutrality the same governance

00:00:41,920 --> 00:00:45,520
as

00:00:42,320 --> 00:00:49,200
any other cncf project

00:00:45,520 --> 00:00:51,199
when we first looked at creating a

00:00:49,200 --> 00:00:53,520
service mesh integration for our

00:00:51,199 --> 00:00:56,480
enterprise customers at cong

00:00:53,520 --> 00:00:59,120
we didn't want to build queue we really

00:00:56,480 --> 00:01:00,559
wanted to leverage existing service

00:00:59,120 --> 00:01:02,800
meshes out there

00:01:00,559 --> 00:01:04,640
that were available in the industry so

00:01:02,800 --> 00:01:05,360
that we could package those service

00:01:04,640 --> 00:01:08,000
meshes

00:01:05,360 --> 00:01:09,360
into a solution that we could ship to

00:01:08,000 --> 00:01:12,240
our users

00:01:09,360 --> 00:01:13,439
and and none of the service meshes out

00:01:12,240 --> 00:01:16,560
there worked for us and

00:01:13,439 --> 00:01:18,320
let me tell you why we wanted something

00:01:16,560 --> 00:01:18,960
that was easy to use that was simple to

00:01:18,320 --> 00:01:21,280
deploy

00:01:18,960 --> 00:01:23,920
simple to scale but for many users

00:01:21,280 --> 00:01:26,799
service mesh has been a very complicated

00:01:23,920 --> 00:01:27,759
technology to deploy and to use at scale

00:01:26,799 --> 00:01:29,920
in production

00:01:27,759 --> 00:01:31,840
we needed something that would support

00:01:29,920 --> 00:01:33,040
not just kubernetes deployments but

00:01:31,840 --> 00:01:35,280
could also support

00:01:33,040 --> 00:01:37,119
virtual machines or support other

00:01:35,280 --> 00:01:38,560
containerized environments that were not

00:01:37,119 --> 00:01:42,159
kubernetes for example

00:01:38,560 --> 00:01:44,079
aws fargate or ecs and um

00:01:42,159 --> 00:01:45,520
and none of the service meshes out there

00:01:44,079 --> 00:01:47,439
allowed us to do that

00:01:45,520 --> 00:01:48,640
we needed something that would allow us

00:01:47,439 --> 00:01:52,000
to support

00:01:48,640 --> 00:01:52,799
multiple uh zones in a distributed

00:01:52,000 --> 00:01:55,680
service mesh

00:01:52,799 --> 00:01:58,479
across multiple clouds multiple clusters

00:01:55,680 --> 00:02:01,040
across a hybrid of vms and kubernetes

00:01:58,479 --> 00:02:01,680
and no service mesh was allowing us to

00:02:01,040 --> 00:02:03,759
do that

00:02:01,680 --> 00:02:05,840
so we decided to build cuba we built

00:02:03,759 --> 00:02:08,160
kuma and we have donated kuma

00:02:05,840 --> 00:02:10,160
making it the first based service mesh

00:02:08,160 --> 00:02:13,120
to ever be donated and accepted

00:02:10,160 --> 00:02:14,879
into into the foundation cuma it is

00:02:13,120 --> 00:02:16,080
built on top of envoy we're strong

00:02:14,879 --> 00:02:18,959
believers of envoy

00:02:16,080 --> 00:02:21,040
as a networking technology for our data

00:02:18,959 --> 00:02:23,440
plane proxies and because of the very

00:02:21,040 --> 00:02:27,040
unique set of features that qm has built

00:02:23,440 --> 00:02:29,200
over the past years we are looking at

00:02:27,040 --> 00:02:30,879
an incredible growth in the community

00:02:29,200 --> 00:02:31,280
when it comes to community adoption as

00:02:30,879 --> 00:02:34,160
well

00:02:31,280 --> 00:02:36,239
as mission critical enterprise adoption

00:02:34,160 --> 00:02:38,720
in mission critical use cases

00:02:36,239 --> 00:02:41,040
where kuma it is being used today to

00:02:38,720 --> 00:02:44,319
support a service mesh for the entire

00:02:41,040 --> 00:02:46,640
organization kuma it is

00:02:44,319 --> 00:02:48,080
something that has been designed for the

00:02:46,640 --> 00:02:49,680
enterprise architect

00:02:48,080 --> 00:02:51,120
we want to make sure that as the

00:02:49,680 --> 00:02:53,360
application teams are building more and

00:02:51,120 --> 00:02:55,120
more services more and more applications

00:02:53,360 --> 00:02:57,040
they do not manage the network that

00:02:55,120 --> 00:03:00,239
comes as part of the infrastructure

00:02:57,040 --> 00:03:02,840
and so today kuma it is being used by

00:03:00,239 --> 00:03:04,720
central teams architecture teams to

00:03:02,840 --> 00:03:07,280
provide under

00:03:04,720 --> 00:03:08,080
the uh under the hood connectivity via a

00:03:07,280 --> 00:03:11,200
service mesh

00:03:08,080 --> 00:03:13,280
across any environment any cloud any

00:03:11,200 --> 00:03:15,680
architecture that the application teams

00:03:13,280 --> 00:03:17,920
are using for their applications

00:03:15,680 --> 00:03:19,519
so qma primarily has been used for you

00:03:17,920 --> 00:03:20,400
know for many different use cases but we

00:03:19,519 --> 00:03:22,080
can sum them up

00:03:20,400 --> 00:03:23,440
in the following ones so has been

00:03:22,080 --> 00:03:25,200
used to enable

00:03:23,440 --> 00:03:26,560
service connectivity across all the

00:03:25,200 --> 00:03:28,400
services that we're building

00:03:26,560 --> 00:03:30,560
to discover the services and make sure

00:03:28,400 --> 00:03:32,400
that the traffic among them is reliable

00:03:30,560 --> 00:03:34,319
it has been used for one click zero

00:03:32,400 --> 00:03:37,040
trust security model enablement

00:03:34,319 --> 00:03:38,080
in order to be able to create security

00:03:37,040 --> 00:03:39,920
um

00:03:38,080 --> 00:03:41,280
uh enhanced security across all the

00:03:39,920 --> 00:03:41,760
workloads as well as being able to

00:03:41,280 --> 00:03:43,680
rotate

00:03:41,760 --> 00:03:44,799
the data plan proxy certificates in an

00:03:43,680 --> 00:03:46,319
automated way

00:03:44,799 --> 00:03:48,080
and then of course the more services we

00:03:46,319 --> 00:03:50,159
have the more traffic we have

00:03:48,080 --> 00:03:51,200
the bigger is the requirement for strong

00:03:50,159 --> 00:03:53,599
observability

00:03:51,200 --> 00:03:55,599
and so is being used today to

00:03:53,599 --> 00:03:56,480
capture those traces those logs those

00:03:55,599 --> 00:03:59,120
metrics

00:03:56,480 --> 00:04:01,519
and and either visualize them on zipkin

00:03:59,120 --> 00:04:04,720
or jager or splunk or logstash

00:04:01,519 --> 00:04:08,080
or via the out of the box dashboards

00:04:04,720 --> 00:04:10,319
on grafana that the project provides

00:04:08,080 --> 00:04:12,560
and from a ten thousand feet standpoint

00:04:10,319 --> 00:04:14,879
kuma is a control plane that implements

00:04:12,560 --> 00:04:16,560
the xds api it can run on kubernetes and

00:04:14,879 --> 00:04:18,639
vms it can run in a single and

00:04:16,560 --> 00:04:20,400
multi-zone capacity it is built for the

00:04:18,639 --> 00:04:22,479
enterprise architect that must

00:04:20,400 --> 00:04:24,080
support the entire organization and

00:04:22,479 --> 00:04:27,120
because it's part of cncf

00:04:24,080 --> 00:04:29,120
it is a vendor neutral technology

00:04:27,120 --> 00:04:30,880
kuma provides a very unique set of

00:04:29,120 --> 00:04:32,880
features uh that we've built

00:04:30,880 --> 00:04:34,400
because we couldn't find them elsewhere

00:04:32,880 --> 00:04:36,720
and so qma supports

00:04:34,400 --> 00:04:38,000
a multiple multiple virtual meshes

00:04:36,720 --> 00:04:39,919
multi-tenancy

00:04:38,000 --> 00:04:41,199
for each team or each application that

00:04:39,919 --> 00:04:43,199
we want to support

00:04:41,199 --> 00:04:44,120
in order to reduce the team coordination

00:04:43,199 --> 00:04:45,840
as well as improve the

00:04:44,120 --> 00:04:46,400
compartmentalization of our service

00:04:45,840 --> 00:04:48,160
mentions

00:04:46,400 --> 00:04:49,440
so we can deploy comma once and create

00:04:48,160 --> 00:04:51,600
as many meshes as we want

00:04:49,440 --> 00:04:52,720
as opposed as opposed to creating one

00:04:51,600 --> 00:04:55,759
service mesh

00:04:52,720 --> 00:04:57,759
per application or per team it is

00:04:55,759 --> 00:04:59,600
universal it runs on kubernetes in the

00:04:57,759 --> 00:05:01,280
ends it supports custom attributes that

00:04:59,600 --> 00:05:03,120
we can use for our policies

00:05:01,280 --> 00:05:05,680
for example to keep traffic within a

00:05:03,120 --> 00:05:07,759
country and it supports the best

00:05:05,680 --> 00:05:08,880
built-in multi-zone connectivity that

00:05:07,759 --> 00:05:12,000
we're going to be seeing

00:05:08,880 --> 00:05:12,720
seeing live in a demo today when it

00:05:12,000 --> 00:05:14,880
comes to

00:05:12,720 --> 00:05:16,720
service mesh in general service mesh is

00:05:14,880 --> 00:05:18,160
important because it centralizes how we

00:05:16,720 --> 00:05:19,520
manage connectivity which is going to be

00:05:18,160 --> 00:05:21,280
one of the most important things we need

00:05:19,520 --> 00:05:22,320
to manage as we get more distributed and

00:05:21,280 --> 00:05:24,080
more decoupled

00:05:22,320 --> 00:05:25,600
but it also makes the application things

00:05:24,080 --> 00:05:27,039
more efficient because they don't have

00:05:25,600 --> 00:05:28,240
to reinvent and rebuild

00:05:27,039 --> 00:05:30,800
all the things that the service mesh

00:05:28,240 --> 00:05:32,800
provides uh as well as it's built on top

00:05:30,800 --> 00:05:34,800
of envoy and we are strong believers of

00:05:32,800 --> 00:05:37,199
envoy like i said we can leverage

00:05:34,800 --> 00:05:38,160
all the envoy functionality inside of

00:05:37,199 --> 00:05:40,320
human

00:05:38,160 --> 00:05:41,360
we can implement zero trust security in

00:05:40,320 --> 00:05:44,479
one click

00:05:41,360 --> 00:05:46,400
by using the mutual tls and traffic

00:05:44,479 --> 00:05:47,039
permission policies that kuma offers out

00:05:46,400 --> 00:05:49,520
of the box

00:05:47,039 --> 00:05:51,840
as well as like i said we can integrate

00:05:49,520 --> 00:05:53,520
cuma with any sorts of observability

00:05:51,840 --> 00:05:55,280
tooling we may be using today

00:05:53,520 --> 00:05:57,199
as well as using what cuba provides out

00:05:55,280 --> 00:05:59,120
of the box

00:05:57,199 --> 00:06:00,720
now of course as one of the biggest

00:05:59,120 --> 00:06:02,720
reasons for using a service mesh

00:06:00,720 --> 00:06:04,319
it's also to make sure we can implement

00:06:02,720 --> 00:06:04,960
the blue green deployments and canary

00:06:04,319 --> 00:06:06,960
releases

00:06:04,960 --> 00:06:08,319
and you know traffic shifting across

00:06:06,960 --> 00:06:10,960
different data centers

00:06:08,319 --> 00:06:12,479
and qma can do it can do all of it with

00:06:10,960 --> 00:06:13,039
the routing capabilities that it

00:06:12,479 --> 00:06:15,280
provides

00:06:13,039 --> 00:06:17,440
and in the demo today we're going to be

00:06:15,280 --> 00:06:18,639
seeing it live across multiple clouds

00:06:17,440 --> 00:06:22,960
multiple regions

00:06:18,639 --> 00:06:26,080
across vms and containers simultaneously

00:06:22,960 --> 00:06:26,479
when it comes to uh deploying qma there

00:06:26,080 --> 00:06:28,400
is

00:06:26,479 --> 00:06:30,319
two different ways we can run the

00:06:28,400 --> 00:06:32,240
project we can run it in a single zone

00:06:30,319 --> 00:06:33,440
or standalone mode or we can run it in a

00:06:32,240 --> 00:06:35,440
multi-zone mode

00:06:33,440 --> 00:06:36,639
multi-zone really it is what makes qma

00:06:35,440 --> 00:06:38,960
very um

00:06:36,639 --> 00:06:39,759
interesting uh in in an enterprise

00:06:38,960 --> 00:06:42,160
organization

00:06:39,759 --> 00:06:44,080
so when running a service mesh across

00:06:42,160 --> 00:06:45,120
multiple zones multiple platforms

00:06:44,080 --> 00:06:47,199
multiple clouds

00:06:45,120 --> 00:06:48,639
multiple clusters there's two main

00:06:47,199 --> 00:06:50,319
challenges we have to solve

00:06:48,639 --> 00:06:52,240
propagating the service mesh policies

00:06:50,319 --> 00:06:54,000
across each zone as well as enabling

00:06:52,240 --> 00:06:56,880
cross zone connectivity

00:06:54,000 --> 00:06:58,639
uh from one zone to another and kuma

00:06:56,880 --> 00:07:00,479
automates both of these problems by

00:06:58,639 --> 00:07:02,720
providing a global control plane

00:07:00,479 --> 00:07:05,120
and remote control plane separation to

00:07:02,720 --> 00:07:07,520
automatically synchronize the policies

00:07:05,120 --> 00:07:09,280
uh the global control plane is the entry

00:07:07,520 --> 00:07:10,160
point for setting all the service mesh

00:07:09,280 --> 00:07:11,440
resources

00:07:10,160 --> 00:07:13,440
and those resources will be

00:07:11,440 --> 00:07:14,160
automatically propagated to the remote

00:07:13,440 --> 00:07:15,520
ones

00:07:14,160 --> 00:07:17,440
whereas we're going to be using the

00:07:15,520 --> 00:07:18,880
built-in service discovery and the human

00:07:17,440 --> 00:07:20,800
ingress that comes out of the box

00:07:18,880 --> 00:07:21,919
to enable cross-zone connectivity from

00:07:20,800 --> 00:07:24,160
one zone to another

00:07:21,919 --> 00:07:26,240
even if one zone is vms and another zone

00:07:24,160 --> 00:07:28,160
is kubernetes or one zone is in one

00:07:26,240 --> 00:07:29,680
cloud one region and another zone is a

00:07:28,160 --> 00:07:32,240
physical data center

00:07:29,680 --> 00:07:33,840
kuma makes no assumptions as to where

00:07:32,240 --> 00:07:35,039
we're running the service mesh with the

00:07:33,840 --> 00:07:37,840
goal of supporting

00:07:35,039 --> 00:07:39,520
every workload in the organization and

00:07:37,840 --> 00:07:41,759
of course it provides a gui out of the

00:07:39,520 --> 00:07:42,319
box it provides a cli out of the box an

00:07:41,759 --> 00:07:44,639
api

00:07:42,319 --> 00:07:46,240
out of the box and i'm very excited

00:07:44,639 --> 00:07:48,639
about all the things that we're building

00:07:46,240 --> 00:07:52,479
when it comes to improving how the users

00:07:48,639 --> 00:07:52,479
are interacting with the service mesh

00:07:52,800 --> 00:07:56,560
in cuma we deploy a service mesh and

00:07:54,720 --> 00:07:58,000
then we can apply policies on top of our

00:07:56,560 --> 00:07:59,759
workloads policies like traffic

00:07:58,000 --> 00:08:01,440
route mutual tls permissions health

00:07:59,759 --> 00:08:02,160
check circuit breakers and so on and so

00:08:01,440 --> 00:08:04,479
forth

00:08:02,160 --> 00:08:06,560
um as well as kuma it is being used

00:08:04,479 --> 00:08:08,240
today to accelerate the transition to

00:08:06,560 --> 00:08:10,479
kubernetes by

00:08:08,240 --> 00:08:12,240
allowing to support simultaneously

00:08:10,479 --> 00:08:13,039
virtual machine based zones with

00:08:12,240 --> 00:08:14,879
kubernetes

00:08:13,039 --> 00:08:16,879
zones and then determining with the

00:08:14,879 --> 00:08:18,319
traffic routing rules how much traffic

00:08:16,879 --> 00:08:20,160
should go to the vm based

00:08:18,319 --> 00:08:23,280
version of a service as opposed to the

00:08:20,160 --> 00:08:26,400
container based version of a service

00:08:23,280 --> 00:08:29,680
including environments like aws

00:08:26,400 --> 00:08:31,599
fargate as well as ecs which typically

00:08:29,680 --> 00:08:32,800
other service meshes do not support and

00:08:31,599 --> 00:08:35,680
this is a function

00:08:32,800 --> 00:08:36,479
of the uni the the universality that

00:08:35,680 --> 00:08:40,000
we've built

00:08:36,479 --> 00:08:41,039
inside of the project and of course uh

00:08:40,000 --> 00:08:43,519
it integrates

00:08:41,039 --> 00:08:45,200
with existing gateway technologies uh

00:08:43,519 --> 00:08:46,959
for example at the edge at the edge you

00:08:45,200 --> 00:08:49,760
know service mesh is not applicable

00:08:46,959 --> 00:08:51,040
if we want to enable our apis to be

00:08:49,760 --> 00:08:52,880
consumed by a client

00:08:51,040 --> 00:08:54,640
that we do not that's outside of the

00:08:52,880 --> 00:08:56,080
organization because we cannot force a

00:08:54,640 --> 00:08:57,680
cycle deployment to them

00:08:56,080 --> 00:08:59,200
and we don't want their sidecar to talk

00:08:57,680 --> 00:09:00,480
to our control plane and so we can

00:08:59,200 --> 00:09:02,000
integrate with gateways

00:09:00,480 --> 00:09:03,680
which can become the ingress and the

00:09:02,000 --> 00:09:05,760
egress of the service mesh

00:09:03,680 --> 00:09:07,440
and we have that native full stack

00:09:05,760 --> 00:09:08,399
end-to-end integration built into the

00:09:07,440 --> 00:09:10,480
product and so

00:09:08,399 --> 00:09:12,240
in the product you can see gateway data

00:09:10,480 --> 00:09:14,640
planes that can be assigned

00:09:12,240 --> 00:09:16,160
in order to either support edge

00:09:14,640 --> 00:09:18,160
requirements with an ecosystem of

00:09:16,160 --> 00:09:19,600
partners or mobile apps as well as

00:09:18,160 --> 00:09:21,200
inside of the organization

00:09:19,600 --> 00:09:23,279
to enable different teams to talk to

00:09:21,200 --> 00:09:24,800
each other via an abstraction layer

00:09:23,279 --> 00:09:26,959
provided by a gateway

00:09:24,800 --> 00:09:29,200
so let's not spend any more time talking

00:09:26,959 --> 00:09:33,200
about all the things that kuma can do

00:09:29,200 --> 00:09:35,760
let's watch them live in production

00:09:33,200 --> 00:09:37,360
so i'm going to be pulling up my uh

00:09:35,760 --> 00:09:38,080
infrastructure right now so i can show

00:09:37,360 --> 00:09:40,640
you

00:09:38,080 --> 00:09:42,720
uh what what kuma can do so first and

00:09:40,640 --> 00:09:45,279
foremost i am running kuma

00:09:42,720 --> 00:09:46,399
right now in a multi-zone deployment

00:09:45,279 --> 00:09:49,040
that spans across

00:09:46,399 --> 00:09:51,920
both ec2 and virtual machines as well as

00:09:49,040 --> 00:09:53,839
kubernetes clusters on gcp on gk

00:09:51,920 --> 00:09:55,600
so we're going to be seeing here that on

00:09:53,839 --> 00:09:58,480
gke we have cuba east

00:09:55,600 --> 00:09:59,120
kuma west these are my east and west uh

00:09:58,480 --> 00:10:01,760
zones

00:09:59,120 --> 00:10:03,440
on gke as well as there is a global zone

00:10:01,760 --> 00:10:05,120
for our global control plane

00:10:03,440 --> 00:10:06,560
and then we do also have the remote

00:10:05,120 --> 00:10:09,519
control plane and the ingress being

00:10:06,560 --> 00:10:11,760
deployed as a virtual machine on ec2

00:10:09,519 --> 00:10:13,839
now in order to show you this demo i

00:10:11,760 --> 00:10:17,200
have built a very simple application

00:10:13,839 --> 00:10:18,959
that basically integrates uh shows a

00:10:17,200 --> 00:10:21,760
front end that allows us to

00:10:18,959 --> 00:10:22,880
increment a counter on ratings and so uh

00:10:21,760 --> 00:10:25,839
i was loading

00:10:22,880 --> 00:10:26,399
my applications from ec2 and um as you

00:10:25,839 --> 00:10:28,640
can see

00:10:26,399 --> 00:10:30,399
if i press the increment button it will

00:10:28,640 --> 00:10:33,120
increment the counter

00:10:30,399 --> 00:10:33,839
in in a specific zone and so this is the

00:10:33,120 --> 00:10:37,040
zone

00:10:33,839 --> 00:10:39,360
where redis lives and so different radis

00:10:37,040 --> 00:10:40,959
instances in different zones may end up

00:10:39,360 --> 00:10:41,680
having different counters depending on

00:10:40,959 --> 00:10:44,480
how often

00:10:41,680 --> 00:10:46,560
we increment that counter um you know i

00:10:44,480 --> 00:10:48,240
can generate some traffic here if i want

00:10:46,560 --> 00:10:49,600
but most importantly let's go to the

00:10:48,240 --> 00:10:53,200
global control plane

00:10:49,600 --> 00:10:56,240
and see uh what we have running live

00:10:53,200 --> 00:10:56,720
into our mesh so first and foremost if

00:10:56,240 --> 00:10:59,440
we

00:10:56,720 --> 00:11:00,640
explore the namespaces that we have

00:10:59,440 --> 00:11:01,920
running we see that there is akuma

00:11:00,640 --> 00:11:05,360
system namespace

00:11:01,920 --> 00:11:08,000
and if i look into the uh into the pods

00:11:05,360 --> 00:11:10,240
of this kuma system namespace uh we see

00:11:08,000 --> 00:11:13,440
that there is one common control plane

00:11:10,240 --> 00:11:14,480
uh pod uh and a service really that we

00:11:13,440 --> 00:11:16,079
can then access

00:11:14,480 --> 00:11:19,120
so i'm going to be port forwarding the

00:11:16,079 --> 00:11:19,120
kuma control plane

00:11:19,519 --> 00:11:23,440
service from the qma system namespace so

00:11:22,480 --> 00:11:25,360
we can access

00:11:23,440 --> 00:11:27,200
it now the gui that i'm going to be

00:11:25,360 --> 00:11:29,440
showing you it is built on top of the

00:11:27,200 --> 00:11:31,680
same restful api that you can integrate

00:11:29,440 --> 00:11:32,880
with your own automation so this is this

00:11:31,680 --> 00:11:34,560
is the restful api

00:11:32,880 --> 00:11:36,240
you know if i go on slash meshes i can

00:11:34,560 --> 00:11:38,640
explore the meshes and

00:11:36,240 --> 00:11:40,240
i can see the resources of the mesh but

00:11:38,640 --> 00:11:42,160
if i go on slash gui

00:11:40,240 --> 00:11:44,160
i can see the gui that's being presented

00:11:42,160 --> 00:11:45,519
to me uh and it's built on top of the

00:11:44,160 --> 00:11:48,560
api i just showed you

00:11:45,519 --> 00:11:50,160
so this gui right now is showing us all

00:11:48,560 --> 00:11:51,760
the resources that we have in the mesh

00:11:50,160 --> 00:11:54,079
so we have two different zones

00:11:51,760 --> 00:11:55,839
one it's on aws on virtual machines

00:11:54,079 --> 00:11:58,000
another one it's on kubernetes east

00:11:55,839 --> 00:11:59,519
another one it's on kubernetes west

00:11:58,000 --> 00:12:01,279
and then we have a series of data planes

00:11:59,519 --> 00:12:03,120
that we have running um

00:12:01,279 --> 00:12:05,120
where we can see that there is a radius

00:12:03,120 --> 00:12:05,760
running on aws the one that i've just

00:12:05,120 --> 00:12:07,120
showed you

00:12:05,760 --> 00:12:08,800
as well there is a demo app running on

00:12:07,120 --> 00:12:10,560
nws on virtual machines

00:12:08,800 --> 00:12:12,000
but we also have the demo app running on

00:12:10,560 --> 00:12:14,160
gke east and we have

00:12:12,000 --> 00:12:15,120
uh red is running on gk east and west

00:12:14,160 --> 00:12:16,880
and so on

00:12:15,120 --> 00:12:19,279
we also have the three ingresses that

00:12:16,880 --> 00:12:22,399
allow us to enable

00:12:19,279 --> 00:12:24,320
cross zone communication

00:12:22,399 --> 00:12:25,600
out of the box and we have one for each

00:12:24,320 --> 00:12:28,000
zone for aws

00:12:25,600 --> 00:12:28,800
for gke st and for gke west you know

00:12:28,000 --> 00:12:30,800
when it comes to queue

00:12:28,800 --> 00:12:31,920
itself we can go on cumulative we can go

00:12:30,800 --> 00:12:33,360
on slash install

00:12:31,920 --> 00:12:34,560
and we can see all the different

00:12:33,360 --> 00:12:35,440
installation methods that we're

00:12:34,560 --> 00:12:38,079
supporting

00:12:35,440 --> 00:12:39,519
uh we are about to release qmo 1.0 and

00:12:38,079 --> 00:12:40,800
we're also going to be introducing

00:12:39,519 --> 00:12:42,480
support for windows

00:12:40,800 --> 00:12:45,440
which is now being released in an alpha

00:12:42,480 --> 00:12:47,440
version in android proxy

00:12:45,440 --> 00:12:48,959
all right so let's go ahead and uh and

00:12:47,440 --> 00:12:50,800
start using the mesh so right now the

00:12:48,959 --> 00:12:52,560
mesh is not really doing anything

00:12:50,800 --> 00:12:54,399
besides having one virtual mesh called

00:12:52,560 --> 00:12:55,920
the default everything is being disabled

00:12:54,399 --> 00:12:58,160
there is no resources

00:12:55,920 --> 00:12:59,200
uh all i have is the data planes being

00:12:58,160 --> 00:13:00,639
registered to it

00:12:59,200 --> 00:13:02,800
but there is nothing really going on

00:13:00,639 --> 00:13:04,399
here so let's go ahead and make this

00:13:02,800 --> 00:13:07,360
service mesh a little bit more

00:13:04,399 --> 00:13:09,920
interesting so let's for example enable

00:13:07,360 --> 00:13:12,079
zero trust security by implementing the

00:13:09,920 --> 00:13:13,600
enabling the mutual kls policy

00:13:12,079 --> 00:13:15,360
when it comes to mutual teles there is

00:13:13,600 --> 00:13:17,040
different certificate authorities that

00:13:15,360 --> 00:13:18,720
we can choose we can choose the built-in

00:13:17,040 --> 00:13:21,040
certificate authority that will

00:13:18,720 --> 00:13:22,240
automatically generate a ca for us we

00:13:21,040 --> 00:13:25,200
can provide our own

00:13:22,240 --> 00:13:26,639
root certificate and key in order to

00:13:25,200 --> 00:13:28,720
apply these resources

00:13:26,639 --> 00:13:30,720
uh it is very simple uh we can use on

00:13:28,720 --> 00:13:33,200
kubernetes we can use cube cattle

00:13:30,720 --> 00:13:33,760
to effectively uh update our default

00:13:33,200 --> 00:13:37,040
mesh

00:13:33,760 --> 00:13:37,839
to enable mutual tls with a a built-in

00:13:37,040 --> 00:13:40,240
backend

00:13:37,839 --> 00:13:42,079
that does a certificate rotation for our

00:13:40,240 --> 00:13:44,240
data plane proxies every day

00:13:42,079 --> 00:13:45,440
but if you were to be running these on

00:13:44,240 --> 00:13:47,120
virtual machines

00:13:45,440 --> 00:13:49,199
we could use a very similar yaml

00:13:47,120 --> 00:13:51,440
declarative config but instead of cube

00:13:49,199 --> 00:13:53,920
cattle we would be using cumacado

00:13:51,440 --> 00:13:55,360
so this is truly a universal service

00:13:53,920 --> 00:13:57,920
mesh that can support

00:13:55,360 --> 00:13:59,519
all kinds of environments but because

00:13:57,920 --> 00:14:03,279
we're running on kubernetes

00:13:59,519 --> 00:14:05,600
i'm going to be using this policy

00:14:03,279 --> 00:14:07,279
to change the state of my mesh so if i

00:14:05,600 --> 00:14:10,880
go back to my

00:14:07,279 --> 00:14:11,600
um if i go to my editor i am going to be

00:14:10,880 --> 00:14:14,000
running

00:14:11,600 --> 00:14:14,959
the command that we're going to be

00:14:14,000 --> 00:14:19,120
executing

00:14:14,959 --> 00:14:22,240
to that we're going to be executing

00:14:19,120 --> 00:14:25,040
to uh change the state of our default

00:14:22,240 --> 00:14:28,000
mesh by enabling mutual tls

00:14:25,040 --> 00:14:30,480
and if i do this mutual tls will be now

00:14:28,000 --> 00:14:33,600
enabled for every service in this mesh

00:14:30,480 --> 00:14:36,240
and by default without having

00:14:33,600 --> 00:14:38,000
a traffic permission which is another

00:14:36,240 --> 00:14:40,160
policy we need to add

00:14:38,000 --> 00:14:42,560
without having this our traffic will

00:14:40,160 --> 00:14:44,399
stop working and i'll show you this

00:14:42,560 --> 00:14:45,760
so if i go here and i open a new

00:14:44,399 --> 00:14:49,839
terminal and i

00:14:45,760 --> 00:14:52,320
apply i apply my

00:14:49,839 --> 00:14:53,279
resource on kubernetes on the global

00:14:52,320 --> 00:14:55,199
control plane

00:14:53,279 --> 00:14:57,120
we can see that the traffic will stop

00:14:55,199 --> 00:14:59,440
working and that is because we have

00:14:57,120 --> 00:15:01,680
enabled zero trust security and we must

00:14:59,440 --> 00:15:04,240
have an explicit traffic permission

00:15:01,680 --> 00:15:05,519
to determine what services can consume

00:15:04,240 --> 00:15:07,360
other services

00:15:05,519 --> 00:15:09,440
and so the traffic permission is a

00:15:07,360 --> 00:15:10,959
another resource that kuma provides

00:15:09,440 --> 00:15:12,639
and it allows us to determine what

00:15:10,959 --> 00:15:14,399
source of traffic you can consume what

00:15:12,639 --> 00:15:15,120
destination as you can see here we can

00:15:14,399 --> 00:15:18,000
use

00:15:15,120 --> 00:15:20,000
attributes that are being associated to

00:15:18,000 --> 00:15:21,279
every workload in qma and these

00:15:20,000 --> 00:15:24,399
attributes are

00:15:21,279 --> 00:15:25,680
attributes that we can customize

00:15:24,399 --> 00:15:27,519
some of them are also being auto

00:15:25,680 --> 00:15:28,880
generated but we can find them from the

00:15:27,519 --> 00:15:31,519
gui or from the cli

00:15:28,880 --> 00:15:32,320
or the api and we can see them here so

00:15:31,519 --> 00:15:34,880
anyways by

00:15:32,320 --> 00:15:35,920
enabling uh every service to consume

00:15:34,880 --> 00:15:38,160
every other service

00:15:35,920 --> 00:15:39,759
we are effectively re-enabling all the

00:15:38,160 --> 00:15:43,040
traffic to flow again

00:15:39,759 --> 00:15:44,720
now by default whenever we re-enable

00:15:43,040 --> 00:15:48,160
this traffic

00:15:44,720 --> 00:15:50,720
the traffic will go and flow

00:15:48,160 --> 00:15:52,399
across every zone that we support which

00:15:50,720 --> 00:15:54,000
means the kubernetes zones as well as

00:15:52,399 --> 00:15:55,600
the amazon zone

00:15:54,000 --> 00:15:58,079
we can limit that and change that

00:15:55,600 --> 00:16:00,560
behavior by using a traffic routing rule

00:15:58,079 --> 00:16:02,399
but if i do this now we can see that not

00:16:00,560 --> 00:16:03,920
only the traffic is being resumed

00:16:02,399 --> 00:16:06,240
but the traffic is also going to be

00:16:03,920 --> 00:16:08,480
flowing from one zone to another

00:16:06,240 --> 00:16:10,160
what you're looking at right now it's a

00:16:08,480 --> 00:16:12,399
multi-zone deployment running on

00:16:10,160 --> 00:16:14,880
multiple kubernetes clusters running on

00:16:12,399 --> 00:16:16,560
virtual machines on different clouds in

00:16:14,880 --> 00:16:18,480
different world regions

00:16:16,560 --> 00:16:20,320
with zero trust security enabled with

00:16:18,480 --> 00:16:22,480
traffic permission acl enabled

00:16:20,320 --> 00:16:23,440
and the traffic is flowing from one zone

00:16:22,480 --> 00:16:25,759
to another

00:16:23,440 --> 00:16:27,120
out of the box automatically discovered

00:16:25,759 --> 00:16:28,399
automatically secured

00:16:27,120 --> 00:16:30,320
of course the counter is going to be

00:16:28,399 --> 00:16:33,199
different depending on what

00:16:30,320 --> 00:16:34,399
radius instance in what in in you know

00:16:33,199 --> 00:16:36,880
where we're hitting

00:16:34,399 --> 00:16:38,880
in in the specific zone that's being

00:16:36,880 --> 00:16:40,240
visualized down here

00:16:38,880 --> 00:16:42,800
but now let's say that we want to change

00:16:40,240 --> 00:16:46,240
this we want to force traffic to go

00:16:42,800 --> 00:16:47,279
to specific zones that would be very

00:16:46,240 --> 00:16:49,680
easy to do by

00:16:47,279 --> 00:16:51,360
using the traffic routing policy which

00:16:49,680 --> 00:16:52,399
allows us to determine how we want the

00:16:51,360 --> 00:16:55,120
traffic to flow

00:16:52,399 --> 00:16:57,759
from from one zone to another and so i'm

00:16:55,120 --> 00:17:00,560
going to be pulling up my

00:16:57,759 --> 00:17:01,600
editor again and we can determine for

00:17:00,560 --> 00:17:03,680
example that

00:17:01,600 --> 00:17:04,640
we can use the attributes again so we

00:17:03,680 --> 00:17:07,839
can say that

00:17:04,640 --> 00:17:10,400
i want all traffic from this service

00:17:07,839 --> 00:17:12,000
generated by this service going to this

00:17:10,400 --> 00:17:12,959
other service so all the traffic

00:17:12,000 --> 00:17:16,079
generated by the

00:17:12,959 --> 00:17:20,000
demo app going to redis to go

00:17:16,079 --> 00:17:22,240
you know all of it all of it to go to

00:17:20,000 --> 00:17:23,520
redis in a specific zone so let's say

00:17:22,240 --> 00:17:26,240
that we want

00:17:23,520 --> 00:17:28,160
this zone to be oh we want all the

00:17:26,240 --> 00:17:30,080
traffic to go to gk east

00:17:28,160 --> 00:17:33,679
for example so we can create this

00:17:30,080 --> 00:17:36,640
traffic routing rule we can

00:17:33,679 --> 00:17:38,400
we can apply it on kubernetes but i'm

00:17:36,640 --> 00:17:39,840
going to be doing this next to my app so

00:17:38,400 --> 00:17:42,400
we can see what happens

00:17:39,840 --> 00:17:44,160
i'm now applying like i've done before

00:17:42,400 --> 00:17:44,960
i'm applying this resource on the global

00:17:44,160 --> 00:17:46,400
control plane

00:17:44,960 --> 00:17:47,919
the global control plane is now

00:17:46,400 --> 00:17:49,760
automatically synchronizing this

00:17:47,919 --> 00:17:52,960
resource across all the remotes

00:17:49,760 --> 00:17:55,840
so that we can put this uh new effect

00:17:52,960 --> 00:17:56,720
new new uh change into effect and so if

00:17:55,840 --> 00:17:58,400
i do this

00:17:56,720 --> 00:17:59,919
we can see how the traffic is now gonna

00:17:58,400 --> 00:18:01,840
be forced into gk

00:17:59,919 --> 00:18:03,760
east but let's say that i want a little

00:18:01,840 --> 00:18:06,080
bit of amazon a little bit of gcp

00:18:03,760 --> 00:18:07,200
that's great um i can i can go back to

00:18:06,080 --> 00:18:09,440
my

00:18:07,200 --> 00:18:11,200
i can go back to my configuration i can

00:18:09,440 --> 00:18:15,679
add

00:18:11,200 --> 00:18:18,240
another um i can split the weight uh

00:18:15,679 --> 00:18:21,280
in the following way and uh i can say

00:18:18,240 --> 00:18:24,559
that a little bit of traffic goes on aws

00:18:21,280 --> 00:18:26,640
and a little bit of traffic goes on gcp

00:18:24,559 --> 00:18:28,720
so right now we can see that

00:18:26,640 --> 00:18:31,679
approximately half of it will go in aws

00:18:28,720 --> 00:18:33,120
and half of it will go on gk east i can

00:18:31,679 --> 00:18:35,919
update my resource

00:18:33,120 --> 00:18:37,440
we can pull up our application again and

00:18:35,919 --> 00:18:39,120
if i do this

00:18:37,440 --> 00:18:40,720
we're going to be seeing the traffic

00:18:39,120 --> 00:18:41,280
going a little bit on the amps a little

00:18:40,720 --> 00:18:43,600
bit on

00:18:41,280 --> 00:18:45,280
containers a little bit on one cloud a

00:18:43,600 --> 00:18:47,520
little bit on another cloud

00:18:45,280 --> 00:18:48,400
it's that easy to run a distributed

00:18:47,520 --> 00:18:52,880
multi-cloud

00:18:48,400 --> 00:18:55,520
multi-cluster service mesh with cuma

00:18:52,880 --> 00:18:57,280
now um you know obviously this is a very

00:18:55,520 --> 00:18:58,799
simple demo that demonstrates zero trial

00:18:57,280 --> 00:19:00,720
security demonstrates our traffic

00:18:58,799 --> 00:19:02,000
permissions but there's a lot more to it

00:19:00,720 --> 00:19:04,080
and uh you can

00:19:02,000 --> 00:19:05,840
explore the policies that we have here

00:19:04,080 --> 00:19:07,600
as well as you can use any filter that

00:19:05,840 --> 00:19:08,960
amway provides with the proxy template

00:19:07,600 --> 00:19:10,080
policy as well as you can see the

00:19:08,960 --> 00:19:13,280
metrics the traces

00:19:10,080 --> 00:19:13,280
and so on and so forth

00:19:14,160 --> 00:19:20,400
now this is how easy it is

00:19:17,200 --> 00:19:22,000
to use the project as uh as one of the

00:19:20,400 --> 00:19:23,120
most important announcements that i

00:19:22,000 --> 00:19:25,120
would like to make

00:19:23,120 --> 00:19:27,200
we have release we have released quinoa

00:19:25,120 --> 00:19:28,480
1.0 which brings so much more in

00:19:27,200 --> 00:19:30,240
performance improvements and

00:19:28,480 --> 00:19:33,280
improvements in this multi-zone

00:19:30,240 --> 00:19:34,320
deployment to to the project and this is

00:19:33,280 --> 00:19:38,000
available

00:19:34,320 --> 00:19:39,840
on the website today on you can download

00:19:38,000 --> 00:19:40,960
and use it and push it in production

00:19:39,840 --> 00:19:44,880
live

00:19:40,960 --> 00:19:46,960
so we have seen today what qma is

00:19:44,880 --> 00:19:48,720
what kuma can do why is very

00:19:46,960 --> 00:19:51,200
different from other service meshes

00:19:48,720 --> 00:19:53,280
how simple it is to run how distributed

00:19:51,200 --> 00:19:54,799
it can be in a multi-zone capacity and

00:19:53,280 --> 00:19:56,240
we've seen a live demo

00:19:54,799 --> 00:19:57,840
of probably one of the most complex

00:19:56,240 --> 00:19:58,160
service mesh deployments anybody could

00:19:57,840 --> 00:20:00,960
run

00:19:58,160 --> 00:20:02,080
multi-cloud multi-cluster hybrid vms and

00:20:00,960 --> 00:20:03,760
containers

00:20:02,080 --> 00:20:05,120
so thank you so much you can check out

00:20:03,760 --> 00:20:07,120
 at cum.io

00:20:05,120 --> 00:20:08,720
install and you can chat with the

00:20:07,120 --> 00:20:10,640
community on slack

00:20:08,720 --> 00:20:12,480
by going on queue mode your slash

00:20:10,640 --> 00:20:13,520
community so thank you so much and i

00:20:12,480 --> 00:20:16,880
hope you've enjoyed

00:20:13,520 --> 00:20:16,880

YouTube URL: https://www.youtube.com/watch?v=HIzuS3pmXjY


