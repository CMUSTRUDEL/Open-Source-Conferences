Title: Running Machine Learning Workloads on a Service Mesh
Publication date: 2020-11-25
Playlist: ServiceMeshCon North America 2020
Description: 
	Running Machine Learning Workloads on a Service Mesh

Data security is one of the key pillars to ensure successful operationalization of machine learning workloads. A service mesh can help build capabilities around mTLS, authorization checks combined with some other goodies to add security, resilience and observability to existing services and applications. JupyterHub is one of the most popular open source tools of choice for teams running machine learning environments. There has been a lot of demand in the community to add support for running JupyterHub with a service mesh on Kubernetes. This talk would cover the journey of adding Istio ServiceMesh support to JupyterHub, the roadblocks, the troubleshooting journey and how Istio makes operating and securing machine learning workloads easier despite the heterogeneous nature of tools that the data scientists use. This combined with network policies and other security best practices for running workloads on Kubernetes makes for a great operational and usability combo.
Captions: 
	00:00:00,080 --> 00:00:03,919
hello and welcome today we'll be talking

00:00:02,960 --> 00:00:05,680
about

00:00:03,919 --> 00:00:07,680
running machine learning workloads on a

00:00:05,680 --> 00:00:10,480
service mesh

00:00:07,680 --> 00:00:12,160
my name is har simran singh man i work

00:00:10,480 --> 00:00:13,280
on the machine learning engineering team

00:00:12,160 --> 00:00:16,240
here at splunk

00:00:13,280 --> 00:00:17,359
i'm based out of vancouver canada my

00:00:16,240 --> 00:00:19,680
interests include

00:00:17,359 --> 00:00:21,199
distributed systems i like the

00:00:19,680 --> 00:00:21,840
scalability aspect of distributed

00:00:21,199 --> 00:00:23,119
systems

00:00:21,840 --> 00:00:25,680
i have some experience working on

00:00:23,119 --> 00:00:26,880
multi-cloud as well as security in the

00:00:25,680 --> 00:00:29,519
cloud

00:00:26,880 --> 00:00:30,400
i try and contribute to open source as

00:00:29,519 --> 00:00:32,320
and when i can

00:00:30,400 --> 00:00:34,079
i try and use open source components in

00:00:32,320 --> 00:00:37,120
my day to day job

00:00:34,079 --> 00:00:38,320
to solve everyday problems as well you

00:00:37,120 --> 00:00:42,000
can find me on

00:00:38,320 --> 00:00:45,280
github linkedin twitter or medium

00:00:42,000 --> 00:00:48,079
with my username brandman

00:00:45,280 --> 00:00:48,879
today we'll start by going over some

00:00:48,079 --> 00:00:51,120
background

00:00:48,879 --> 00:00:53,680
on on what kind of problems we are

00:00:51,120 --> 00:00:56,320
trying to solve here at splunk

00:00:53,680 --> 00:00:58,960
and how service mesh help us achieve

00:00:56,320 --> 00:01:01,600
some of our goals

00:00:58,960 --> 00:01:02,640
better and at scale we'll also talk

00:01:01,600 --> 00:01:04,239
about the journey

00:01:02,640 --> 00:01:07,360
and that we went through the transition

00:01:04,239 --> 00:01:09,840
that we had adopting service mesh into

00:01:07,360 --> 00:01:11,600
into our workflows and the benefits that

00:01:09,840 --> 00:01:13,439
we got as a result

00:01:11,600 --> 00:01:15,759
and we'll also go over some of the

00:01:13,439 --> 00:01:17,119
obstacles and challenges that we

00:01:15,759 --> 00:01:19,759
that we run into while doing the

00:01:17,119 --> 00:01:22,640
implementation i'm sure many of you

00:01:19,759 --> 00:01:23,040
would have run into similar problems so

00:01:22,640 --> 00:01:25,040
so

00:01:23,040 --> 00:01:27,280
we will see some techniques that we used

00:01:25,040 --> 00:01:30,240
and and i hope that

00:01:27,280 --> 00:01:31,840
it will be helpful for for many of you

00:01:30,240 --> 00:01:34,400
in other scenarios

00:01:31,840 --> 00:01:36,640
when you try and move existing workloads

00:01:34,400 --> 00:01:38,880
onto a service mesh

00:01:36,640 --> 00:01:41,439
to give you some background about a year

00:01:38,880 --> 00:01:44,079
ago we decided to re-envision our

00:01:41,439 --> 00:01:46,479
machine learning offering here at splunk

00:01:44,079 --> 00:01:48,079
splunk has grown massively over the last

00:01:46,479 --> 00:01:50,560
few years

00:01:48,079 --> 00:01:53,680
and the customer demands for more

00:01:50,560 --> 00:01:56,719
intelligent systems have grown as well

00:01:53,680 --> 00:01:59,119
to cater to those demands we we decided

00:01:56,719 --> 00:02:00,320
to re-envision our machine learning

00:01:59,119 --> 00:02:03,200
offering

00:02:00,320 --> 00:02:04,799
and we had a few clear goals for the

00:02:03,200 --> 00:02:07,119
project

00:02:04,799 --> 00:02:09,440
first was we wanted to run at splunk

00:02:07,119 --> 00:02:11,120
scale

00:02:09,440 --> 00:02:12,480
for those of you who are using splunk

00:02:11,120 --> 00:02:14,400
systems you would know

00:02:12,480 --> 00:02:17,680
the kind of scale that splunk operates

00:02:14,400 --> 00:02:20,879
at so so we wanted our machine learning

00:02:17,680 --> 00:02:22,160
offering to be at par and be able to

00:02:20,879 --> 00:02:24,400
handle

00:02:22,160 --> 00:02:25,920
data that uh thrown at the splunk

00:02:24,400 --> 00:02:28,400
systems

00:02:25,920 --> 00:02:29,440
we wanted to build a system to allow the

00:02:28,400 --> 00:02:32,480
data scientist

00:02:29,440 --> 00:02:34,319
to come and experiment with the data

00:02:32,480 --> 00:02:35,680
build their machine learning models and

00:02:34,319 --> 00:02:36,959
then

00:02:35,680 --> 00:02:39,760
we also wanted to have them

00:02:36,959 --> 00:02:41,519
operationalize them so we wanted to

00:02:39,760 --> 00:02:43,680
take these models and deploy those

00:02:41,519 --> 00:02:46,000
models in production

00:02:43,680 --> 00:02:48,000
and and then we also wanted to build the

00:02:46,000 --> 00:02:49,040
system which has a low barrier to

00:02:48,000 --> 00:02:51,200
adoption

00:02:49,040 --> 00:02:53,040
we did not want to start from scratch

00:02:51,200 --> 00:02:54,239
and and create yet another machine

00:02:53,040 --> 00:02:56,480
learning system

00:02:54,239 --> 00:02:58,080
we wanted to explore and see what people

00:02:56,480 --> 00:02:59,599
are comfortable with what kind of tools

00:02:58,080 --> 00:03:02,879
they are already using

00:02:59,599 --> 00:03:04,800
so we can we can build upon those tools

00:03:02,879 --> 00:03:05,920
so while while doing our research we

00:03:04,800 --> 00:03:08,800
looked at a few

00:03:05,920 --> 00:03:10,480
options in the open source world and and

00:03:08,800 --> 00:03:13,680
we narrowed down our focus

00:03:10,480 --> 00:03:16,560
onto onto jupiter notebooks

00:03:13,680 --> 00:03:19,200
they came across as a very convenient

00:03:16,560 --> 00:03:20,959
way for for data scientists to use

00:03:19,200 --> 00:03:24,159
to experiment with machine learning

00:03:20,959 --> 00:03:26,319
algorithms to try out a few things to do

00:03:24,159 --> 00:03:27,760
do a different kind of data manipulation

00:03:26,319 --> 00:03:31,360
and preparation

00:03:27,760 --> 00:03:34,080
for machine learning and and all of that

00:03:31,360 --> 00:03:34,640
so we decided to take take that as the

00:03:34,080 --> 00:03:37,360
base

00:03:34,640 --> 00:03:39,120
and and build our offering around that

00:03:37,360 --> 00:03:41,280
project

00:03:39,120 --> 00:03:42,239
before we jump into like where we are

00:03:41,280 --> 00:03:44,400
here at splunk

00:03:42,239 --> 00:03:46,879
let's start off by going over a simple

00:03:44,400 --> 00:03:49,599
definition of machine learning

00:03:46,879 --> 00:03:51,680
so i stole this from wikipedia it says

00:03:49,599 --> 00:03:55,040
it's the study of computer algorithms

00:03:51,680 --> 00:03:56,239
that learn through experience and and

00:03:55,040 --> 00:03:59,680
what it really translates

00:03:56,239 --> 00:04:00,799
into is is this so we start by defining

00:03:59,680 --> 00:04:02,959
a problem

00:04:00,799 --> 00:04:05,040
and then we source some data for which

00:04:02,959 --> 00:04:08,080
we're trying to solve this problem

00:04:05,040 --> 00:04:10,720
we'll massage this data a bit and then

00:04:08,080 --> 00:04:12,400
run some algorithms on it that's that's

00:04:10,720 --> 00:04:14,159
the modeling part

00:04:12,400 --> 00:04:16,320
and once once we get the model as a

00:04:14,159 --> 00:04:16,799
result we would do some evaluations on

00:04:16,320 --> 00:04:19,759
it

00:04:16,799 --> 00:04:21,440
at this point we might decide to go back

00:04:19,759 --> 00:04:24,880
maybe rework the data

00:04:21,440 --> 00:04:25,759
then retrain the model so so so once we

00:04:24,880 --> 00:04:28,880
are satisfied

00:04:25,759 --> 00:04:31,120
we will take this model and deploy it on

00:04:28,880 --> 00:04:32,080
on to production where this model can

00:04:31,120 --> 00:04:34,080
now do

00:04:32,080 --> 00:04:35,759
do really smart things with the data it

00:04:34,080 --> 00:04:37,360
has not seen before

00:04:35,759 --> 00:04:39,680
so what does it really look like in

00:04:37,360 --> 00:04:42,160
practice

00:04:39,680 --> 00:04:43,440
here at splunk we got a polyglot when we

00:04:42,160 --> 00:04:44,240
tried to build our machine learning

00:04:43,440 --> 00:04:46,000
environment

00:04:44,240 --> 00:04:47,840
we started building it around jupiter

00:04:46,000 --> 00:04:49,600
notebooks but soon we were writing

00:04:47,840 --> 00:04:52,639
services in scala

00:04:49,600 --> 00:04:53,919
we had components in r a lot of golang

00:04:52,639 --> 00:04:56,240
services

00:04:53,919 --> 00:04:58,720
python is heavily used for the

00:04:56,240 --> 00:05:00,000
experimentation side as well as for data

00:04:58,720 --> 00:05:02,960
preparation

00:05:00,000 --> 00:05:05,120
and we use apache link to parallelize

00:05:02,960 --> 00:05:07,360
our data processing jobs

00:05:05,120 --> 00:05:08,639
we run on top of kubernetes using an hto

00:05:07,360 --> 00:05:10,479
service mesh

00:05:08,639 --> 00:05:12,160
and all of this is closely tied to

00:05:10,479 --> 00:05:14,960
splunk which is our

00:05:12,160 --> 00:05:16,000
central data repository so we have data

00:05:14,960 --> 00:05:17,919
flowing

00:05:16,000 --> 00:05:19,039
between all of these different

00:05:17,919 --> 00:05:21,360
components

00:05:19,039 --> 00:05:22,160
and and we we use our machine learning

00:05:21,360 --> 00:05:25,520
models

00:05:22,160 --> 00:05:27,120
on top of this data but this wasn't

00:05:25,520 --> 00:05:29,919
always like this

00:05:27,120 --> 00:05:31,600
we started off with a very simple

00:05:29,919 --> 00:05:34,320
jupyter hub deployment

00:05:31,600 --> 00:05:34,639
so jupiter hub is a project that helps

00:05:34,320 --> 00:05:38,639
run

00:05:34,639 --> 00:05:40,720
jupyter notebooks on kubernetes it has

00:05:38,639 --> 00:05:42,400
two main components one is a proxy the

00:05:40,720 --> 00:05:43,840
other one is the hub

00:05:42,400 --> 00:05:47,280
hub takes care of things like

00:05:43,840 --> 00:05:49,600
authentication and handling user request

00:05:47,280 --> 00:05:50,479
proxy takes care of routing traffic to

00:05:49,600 --> 00:05:53,360
the hub

00:05:50,479 --> 00:05:56,319
or to the user notebooks depending upon

00:05:53,360 --> 00:05:58,160
where the request needs to be routed to

00:05:56,319 --> 00:05:59,360
in order to better understand jupiter

00:05:58,160 --> 00:06:01,520
hub

00:05:59,360 --> 00:06:03,520
let's do a deployment on a kubernetes

00:06:01,520 --> 00:06:04,479
cluster i have a kubernetes cluster

00:06:03,520 --> 00:06:07,199
running

00:06:04,479 --> 00:06:08,800
that we can use to deploy jupiter hub

00:06:07,199 --> 00:06:09,759
let's look at different components of

00:06:08,800 --> 00:06:13,120
jupiter hub

00:06:09,759 --> 00:06:13,120
that we'll be deploying today

00:06:13,280 --> 00:06:18,080
this is a project that helps deploy

00:06:15,520 --> 00:06:19,919
jupiter hub you can find this on

00:06:18,080 --> 00:06:21,840
on my github links would be shared in

00:06:19,919 --> 00:06:24,000
the slide

00:06:21,840 --> 00:06:26,319
like you saw in the diagram we have two

00:06:24,000 --> 00:06:29,120
components a hub and a proxy

00:06:26,319 --> 00:06:30,639
we have an additional component here

00:06:29,120 --> 00:06:32,000
called namespace

00:06:30,639 --> 00:06:33,759
that would create the kubernetes

00:06:32,000 --> 00:06:35,280
namespace for us

00:06:33,759 --> 00:06:37,600
let's look at all the components that

00:06:35,280 --> 00:06:40,319
we're going to deploy to the cluster

00:06:37,600 --> 00:06:41,600
we have a config map a deployment a

00:06:40,319 --> 00:06:44,160
secret

00:06:41,600 --> 00:06:46,160
some are back related components for hub

00:06:44,160 --> 00:06:47,680
for proxy we have a couple of services

00:06:46,160 --> 00:06:49,199
one called the public service other one

00:06:47,680 --> 00:06:52,479
called the api

00:06:49,199 --> 00:06:55,039
and a deployment component let's do the

00:06:52,479 --> 00:06:55,039
deployment

00:07:01,599 --> 00:07:05,520
as you can see all these components are

00:07:03,840 --> 00:07:06,880
being created in the cluster

00:07:05,520 --> 00:07:10,319
let's take a look at all the new

00:07:06,880 --> 00:07:10,319
components that we just created

00:07:11,919 --> 00:07:17,520
so here you see we have three services

00:07:15,360 --> 00:07:19,440
two parts a couple of deployments and

00:07:17,520 --> 00:07:20,880
they're applica sets

00:07:19,440 --> 00:07:23,039
let's see what it looks like in the

00:07:20,880 --> 00:07:24,319
browser for that i'll need to put

00:07:23,039 --> 00:07:27,599
forward to

00:07:24,319 --> 00:07:30,639
to the proxy public service

00:07:27,599 --> 00:07:33,360
so that we can access this in the

00:07:30,639 --> 00:07:33,360
in the browser

00:07:36,160 --> 00:07:39,199
if i go to the browser and and open up

00:07:38,160 --> 00:07:42,319
this url now

00:07:39,199 --> 00:07:42,880
i should see a login page and i can i

00:07:42,319 --> 00:07:47,440
can log

00:07:42,880 --> 00:07:50,319
in here and you'll see that

00:07:47,440 --> 00:07:52,960
a new pod is being spawned and we get a

00:07:50,319 --> 00:07:55,520
jupyter notebook

00:07:52,960 --> 00:07:56,879
in the back end let's see what it looks

00:07:55,520 --> 00:07:58,560
like

00:07:56,879 --> 00:08:01,039
so let me see all the resources once

00:07:58,560 --> 00:08:04,240
again now as you see

00:08:01,039 --> 00:08:07,280
there's an additional user part this is

00:08:04,240 --> 00:08:07,840
this is what is responsible for serving

00:08:07,280 --> 00:08:11,120
that

00:08:07,840 --> 00:08:13,280
notebook ui in the browser

00:08:11,120 --> 00:08:14,720
all the requests that that you see in

00:08:13,280 --> 00:08:15,520
the browser are being routed through

00:08:14,720 --> 00:08:18,720
this

00:08:15,520 --> 00:08:19,919
proxy public when i did the login it

00:08:18,720 --> 00:08:22,639
went

00:08:19,919 --> 00:08:24,560
to this hub service and and once the

00:08:22,639 --> 00:08:26,879
user's notebook was

00:08:24,560 --> 00:08:29,680
was created all the requests started

00:08:26,879 --> 00:08:31,520
going directly to the user's part

00:08:29,680 --> 00:08:33,200
i hope that the demo makes understanding

00:08:31,520 --> 00:08:34,880
this diagram a little easier

00:08:33,200 --> 00:08:36,560
but you all must be wondering by now

00:08:34,880 --> 00:08:38,399
what about service mesh

00:08:36,560 --> 00:08:40,000
that's the exact question the splunk

00:08:38,399 --> 00:08:41,680
security team asked us

00:08:40,000 --> 00:08:44,320
when we tried to move this service to

00:08:41,680 --> 00:08:46,800
production all cloud services in splunk

00:08:44,320 --> 00:08:50,720
are required to ensure

00:08:46,800 --> 00:08:53,120
end-to-end network traffic encryption

00:08:50,720 --> 00:08:55,519
and for us this was the foray into

00:08:53,120 --> 00:08:57,600
service mesh

00:08:55,519 --> 00:08:58,959
so service mesh as described by easter

00:08:57,600 --> 00:09:01,040
docks here

00:08:58,959 --> 00:09:02,320
is used to understand the network of

00:09:01,040 --> 00:09:04,640
microservices

00:09:02,320 --> 00:09:06,959
and how they interact amongst themselves

00:09:04,640 --> 00:09:10,320
and how the traffic flows

00:09:06,959 --> 00:09:13,200
between one service and the other

00:09:10,320 --> 00:09:15,040
and when we got this requirement the

00:09:13,200 --> 00:09:16,959
first thing that we did was to

00:09:15,040 --> 00:09:18,399
go out to the community and see what

00:09:16,959 --> 00:09:20,800
options we have and

00:09:18,399 --> 00:09:22,000
this is what we found so there was an

00:09:20,800 --> 00:09:24,959
open request on

00:09:22,000 --> 00:09:26,080
the jupiter hub kubernetes community to

00:09:24,959 --> 00:09:27,920
support

00:09:26,080 --> 00:09:29,680
securing connections between different

00:09:27,920 --> 00:09:32,240
jupiter hub components

00:09:29,680 --> 00:09:34,959
using istio and that's exactly the kind

00:09:32,240 --> 00:09:37,040
of problem we had on our hands

00:09:34,959 --> 00:09:38,080
i did some reading and i thought i was

00:09:37,040 --> 00:09:40,399
ready

00:09:38,080 --> 00:09:42,480
to enable sto on the namespace and

00:09:40,399 --> 00:09:46,000
things should just magically work

00:09:42,480 --> 00:09:48,720
but our life as engineers is not so easy

00:09:46,000 --> 00:09:51,040
everything broke when i enabled sto on

00:09:48,720 --> 00:09:53,040
the namespace so i had to learn how to

00:09:51,040 --> 00:09:54,959
troubleshoot the service mesh

00:09:53,040 --> 00:09:57,200
every time i would solve one problem i

00:09:54,959 --> 00:09:59,279
would turn into the next roadblock

00:09:57,200 --> 00:10:02,320
but slowly all the pieces were coming

00:09:59,279 --> 00:10:04,160
together to get a working application

00:10:02,320 --> 00:10:05,440
this this is what the application

00:10:04,160 --> 00:10:07,600
evolved towards

00:10:05,440 --> 00:10:08,959
so there were a few issues the moment we

00:10:07,600 --> 00:10:12,160
enabled istio

00:10:08,959 --> 00:10:15,200
on the namespace the first issue was

00:10:12,160 --> 00:10:16,160
the proxy itself it couldn't route the

00:10:15,200 --> 00:10:19,360
traffic anymore

00:10:16,160 --> 00:10:20,800
to the hub or to the user pods because

00:10:19,360 --> 00:10:22,640
there was some header mismatch

00:10:20,800 --> 00:10:24,160
on the outgoing request which istio

00:10:22,640 --> 00:10:27,200
sidecars didn't like

00:10:24,160 --> 00:10:29,120
and they would simply drop this request

00:10:27,200 --> 00:10:31,360
thinking that there is no destination

00:10:29,120 --> 00:10:33,360
route for this request

00:10:31,360 --> 00:10:35,519
i fixed this by rewriting the proxy to

00:10:33,360 --> 00:10:38,720
override the host headers appropriately

00:10:35,519 --> 00:10:40,240
and things seemed to work the next issue

00:10:38,720 --> 00:10:43,279
we ran into was

00:10:40,240 --> 00:10:46,000
the moment a user would log in and

00:10:43,279 --> 00:10:47,600
try to spawn a new notebook pod for

00:10:46,000 --> 00:10:50,800
themselves

00:10:47,600 --> 00:10:53,839
it wouldn't work and the reason was that

00:10:50,800 --> 00:10:56,480
hub was not able to connect to this pod

00:10:53,839 --> 00:10:57,760
to see if it was alive or not and this

00:10:56,480 --> 00:11:00,079
caused

00:10:57,760 --> 00:11:01,120
it to think that the pods were not being

00:11:00,079 --> 00:11:04,240
spawned

00:11:01,120 --> 00:11:05,839
and it got stuck in an infinite loop

00:11:04,240 --> 00:11:07,360
to mitigate this we introduced a

00:11:05,839 --> 00:11:10,320
kubernetes service to front

00:11:07,360 --> 00:11:11,360
all the pods this required patching the

00:11:10,320 --> 00:11:13,839
hub itself

00:11:11,360 --> 00:11:14,880
so all the traffic from the hub to the

00:11:13,839 --> 00:11:18,079
user pods

00:11:14,880 --> 00:11:19,279
would flow by kubernetes services we had

00:11:18,079 --> 00:11:21,680
a working version

00:11:19,279 --> 00:11:22,720
and we deployed it and gave access to

00:11:21,680 --> 00:11:24,320
our users

00:11:22,720 --> 00:11:27,120
everything worked fine for the first few

00:11:24,320 --> 00:11:29,839
days but as the number of users grew on

00:11:27,120 --> 00:11:31,600
the system and the usage went up

00:11:29,839 --> 00:11:34,079
users started complaining about

00:11:31,600 --> 00:11:36,560
connectivity issues to the service

00:11:34,079 --> 00:11:38,560
we were getting reports of more and more

00:11:36,560 --> 00:11:40,640
of issues like these

00:11:38,560 --> 00:11:42,720
when i did the troubleshooting i found

00:11:40,640 --> 00:11:45,920
that the root cause for all these issues

00:11:42,720 --> 00:11:47,519
was the proxy itself it was not able to

00:11:45,920 --> 00:11:50,000
handle so many user requests

00:11:47,519 --> 00:11:51,839
and was failing to process them and

00:11:50,000 --> 00:11:52,240
because of all the hacks that we had

00:11:51,839 --> 00:11:54,959
done

00:11:52,240 --> 00:11:57,600
to get the system to work it was not

00:11:54,959 --> 00:11:59,839
possible to add more of these proxy pods

00:11:57,600 --> 00:12:01,760
to handle more traffic

00:11:59,839 --> 00:12:02,880
so we had to go back to the design board

00:12:01,760 --> 00:12:05,120
once again

00:12:02,880 --> 00:12:06,880
and this time when doing the analysis we

00:12:05,120 --> 00:12:08,240
looked at the service mesh for

00:12:06,880 --> 00:12:10,320
scalability needs

00:12:08,240 --> 00:12:11,360
so we ended up removing this proxy all

00:12:10,320 --> 00:12:14,399
together

00:12:11,360 --> 00:12:15,360
and replacing it with one simple api so

00:12:14,399 --> 00:12:17,120
all the traffic

00:12:15,360 --> 00:12:18,399
would now come in through an istio

00:12:17,120 --> 00:12:21,360
gateway

00:12:18,399 --> 00:12:23,279
that would be routed to the user pod or

00:12:21,360 --> 00:12:24,079
to the hub itself based on the kind of

00:12:23,279 --> 00:12:25,920
request

00:12:24,079 --> 00:12:28,399
we introduced a new service which was

00:12:25,920 --> 00:12:30,800
compliant to the hub proxy api

00:12:28,399 --> 00:12:32,399
to configure istio and all of this was

00:12:30,800 --> 00:12:33,600
done in a couple of days

00:12:32,399 --> 00:12:36,399
i don't think this would have been

00:12:33,600 --> 00:12:38,240
possible had we not relied on

00:12:36,399 --> 00:12:40,079
what the community has built in terms of

00:12:38,240 --> 00:12:42,480
service mesh all the cool features that

00:12:40,079 --> 00:12:44,720
it ships with

00:12:42,480 --> 00:12:46,000
so this is what it really came down to

00:12:44,720 --> 00:12:49,120
in the end

00:12:46,000 --> 00:12:51,519
when hub starts it registered itself

00:12:49,120 --> 00:12:53,120
as one of the routes on the istio

00:12:51,519 --> 00:12:55,120
gateway

00:12:53,120 --> 00:12:57,440
to handle all the traffic coming to

00:12:55,120 --> 00:12:59,760
jupiter hub

00:12:57,440 --> 00:13:00,800
once this route is ready users can now

00:12:59,760 --> 00:13:03,360
come in and

00:13:00,800 --> 00:13:05,120
and start requesting their server parts

00:13:03,360 --> 00:13:08,320
when a new request comes in

00:13:05,120 --> 00:13:10,399
to register a server pod hub would go

00:13:08,320 --> 00:13:11,600
create the user part and also create the

00:13:10,399 --> 00:13:14,079
corresponding

00:13:11,600 --> 00:13:15,440
kubernetes service and once it knows

00:13:14,079 --> 00:13:18,480
that it can talk to this

00:13:15,440 --> 00:13:21,760
user part via that service

00:13:18,480 --> 00:13:24,160
it goes to the proxy api which is the

00:13:21,760 --> 00:13:27,600
jupiter hub steer proxy in this case

00:13:24,160 --> 00:13:29,279
to register a new route once this route

00:13:27,600 --> 00:13:31,040
is created and is ready to use

00:13:29,279 --> 00:13:32,560
the route is returned to hub which then

00:13:31,040 --> 00:13:35,680
redirects the user

00:13:32,560 --> 00:13:37,440
on to the user port

00:13:35,680 --> 00:13:39,120
so the architecture is very similar to

00:13:37,440 --> 00:13:41,839
what it was before

00:13:39,120 --> 00:13:44,160
except that we replaced a few components

00:13:41,839 --> 00:13:46,880
and we got the system working on

00:13:44,160 --> 00:13:49,360
on the service mesh so we got mutual tls

00:13:46,880 --> 00:13:50,880
so far and we got the scale that we

00:13:49,360 --> 00:13:54,079
needed

00:13:50,880 --> 00:13:55,279
let's see all of this in action

00:13:54,079 --> 00:13:57,199
we're going to go back to the

00:13:55,279 --> 00:14:00,399
application that we deployed earlier

00:13:57,199 --> 00:14:01,360
and enable hto on the namespace before

00:14:00,399 --> 00:14:03,279
we do that

00:14:01,360 --> 00:14:04,480
let's take a look at all the changes

00:14:03,279 --> 00:14:07,360
that we'll have to make

00:14:04,480 --> 00:14:08,320
in order to get this going quebec diff

00:14:07,360 --> 00:14:12,720
allows us to see

00:14:08,320 --> 00:14:12,720
all the changes before we deploy them

00:14:12,880 --> 00:14:17,760
so here you can see we have a few

00:14:16,079 --> 00:14:19,360
components that would be changed when we

00:14:17,760 --> 00:14:21,440
do the deployment

00:14:19,360 --> 00:14:23,040
as we talked about earlier we no longer

00:14:21,440 --> 00:14:25,760
need the proxy

00:14:23,040 --> 00:14:28,160
public component as istio gateway would

00:14:25,760 --> 00:14:30,800
take care of routing the traffic now

00:14:28,160 --> 00:14:32,320
we have some changes to the port for

00:14:30,800 --> 00:14:34,480
proxy api

00:14:32,320 --> 00:14:38,160
this is just to satisfy the port

00:14:34,480 --> 00:14:38,160
requirements for the new service

00:14:38,639 --> 00:14:45,040
then we are also updating hub with some

00:14:42,000 --> 00:14:47,199
of the routing information to tell it

00:14:45,040 --> 00:14:48,160
how the traffic is being routed to it

00:14:47,199 --> 00:14:50,000
and

00:14:48,160 --> 00:14:52,320
and how it should behave with respect to

00:14:50,000 --> 00:14:54,320
the incoming request

00:14:52,320 --> 00:14:55,680
on the proxy itself we are passing in

00:14:54,320 --> 00:14:57,680
some additional configuration we are

00:14:55,680 --> 00:15:00,160
passing in the name of a new

00:14:57,680 --> 00:15:01,199
istio gateway as well as the namespace

00:15:00,160 --> 00:15:02,639
where

00:15:01,199 --> 00:15:05,040
the new virtual services would be

00:15:02,639 --> 00:15:07,600
created and some other configuration

00:15:05,040 --> 00:15:07,600
that we need

00:15:07,839 --> 00:15:13,120
so we also update the proxy command

00:15:10,880 --> 00:15:15,120
to be the starting point of the new

00:15:13,120 --> 00:15:16,959
application instead of the old

00:15:15,120 --> 00:15:18,800
proxy that came bundled with jupiter

00:15:16,959 --> 00:15:20,240
operationally

00:15:18,800 --> 00:15:22,800
we're also able to make some additional

00:15:20,240 --> 00:15:25,040
changes like how we restart the pods and

00:15:22,800 --> 00:15:27,519
things like that which were not possible

00:15:25,040 --> 00:15:30,399
before we moved our implementation using

00:15:27,519 --> 00:15:30,399
the new component

00:15:30,480 --> 00:15:35,600
we are deploying an istio gateway

00:15:34,560 --> 00:15:38,160
component

00:15:35,600 --> 00:15:40,160
so this is to to intercept all the

00:15:38,160 --> 00:15:43,120
requests coming in

00:15:40,160 --> 00:15:45,279
to the gateway and to route them

00:15:43,120 --> 00:15:46,800
appropriately using the virtual services

00:15:45,279 --> 00:15:49,199
and the destination host that we'll

00:15:46,800 --> 00:15:52,560
configure on the services

00:15:49,199 --> 00:15:54,240
using the new proxy api

00:15:52,560 --> 00:15:55,759
we're also deploying a peer

00:15:54,240 --> 00:15:58,000
authentication

00:15:55,759 --> 00:15:58,880
component to the namespace to ensure

00:15:58,000 --> 00:16:01,680
that

00:15:58,880 --> 00:16:02,720
all components they have mutual tls

00:16:01,680 --> 00:16:04,560
enabled

00:16:02,720 --> 00:16:07,199
and we are using it in the strict mode

00:16:04,560 --> 00:16:10,000
which means that all components must use

00:16:07,199 --> 00:16:11,519
mtls when talking to each other the last

00:16:10,000 --> 00:16:13,920
change that we are making

00:16:11,519 --> 00:16:15,759
is to enable istio injection on the

00:16:13,920 --> 00:16:18,720
namespace

00:16:15,759 --> 00:16:20,399
this would mean that all the pods that

00:16:18,720 --> 00:16:21,199
would start in the namespace would now

00:16:20,399 --> 00:16:24,320
have

00:16:21,199 --> 00:16:37,839
the side cars injected into them

00:16:24,320 --> 00:16:37,839
let's go ahead and deploy this change

00:16:39,440 --> 00:16:44,079
so all these components are now updated

00:16:42,000 --> 00:16:46,079
as you can see we deleted one service

00:16:44,079 --> 00:16:48,399
and we created a few

00:16:46,079 --> 00:16:50,800
new components and some other components

00:16:48,399 --> 00:16:52,959
were updated

00:16:50,800 --> 00:16:55,839
let's take a look at all the components

00:16:52,959 --> 00:16:55,839
that we have here

00:16:59,839 --> 00:17:02,880
as you can see our pods are now running

00:17:02,240 --> 00:17:05,120
with

00:17:02,880 --> 00:17:07,439
a 2x2 what this means is that two

00:17:05,120 --> 00:17:10,240
containers running inside every pod

00:17:07,439 --> 00:17:12,240
one of them being the actual application

00:17:10,240 --> 00:17:13,839
part the other one is the istio sidecar

00:17:12,240 --> 00:17:17,039
that takes care of

00:17:13,839 --> 00:17:19,120
all the networking bits on this pod we

00:17:17,039 --> 00:17:21,520
also removed one of the services

00:17:19,120 --> 00:17:23,760
and there are no additional components

00:17:21,520 --> 00:17:25,439
as of now

00:17:23,760 --> 00:17:27,039
let's also take a look at any virtual

00:17:25,439 --> 00:17:28,000
services that might have been created as

00:17:27,039 --> 00:17:29,760
a result

00:17:28,000 --> 00:17:31,600
as we see here there is one virtual

00:17:29,760 --> 00:17:33,440
service already created

00:17:31,600 --> 00:17:34,720
and if you remember from the diagram

00:17:33,440 --> 00:17:37,039
when hub starts

00:17:34,720 --> 00:17:39,840
it would register itself as the

00:17:37,039 --> 00:17:41,919
destination for all the traffic

00:17:39,840 --> 00:17:43,280
coming to that route so let's see what

00:17:41,919 --> 00:17:47,760
this what this rule

00:17:43,280 --> 00:17:49,919
looks like we describe this

00:17:47,760 --> 00:17:51,840
virtual service we see that the

00:17:49,919 --> 00:17:54,799
destination for all these services

00:17:51,840 --> 00:17:57,679
is the hub service and we are routing

00:17:54,799 --> 00:18:00,480
all that all the paths on this route

00:17:57,679 --> 00:18:03,360
let's go to the browser and try to use

00:18:00,480 --> 00:18:03,360
jupyter hub now

00:18:08,000 --> 00:18:11,760
we didn't need to do any port forwarding

00:18:09,840 --> 00:18:13,760
because we are directly talking to the

00:18:11,760 --> 00:18:16,799
istio gateway which is running

00:18:13,760 --> 00:18:18,640
at localhost id in this case and i can

00:18:16,799 --> 00:18:22,000
log in

00:18:18,640 --> 00:18:22,960
and you should see the service being

00:18:22,000 --> 00:18:27,600
spawned

00:18:22,960 --> 00:18:29,039
and so the user gets their notebook once

00:18:27,600 --> 00:18:31,200
again

00:18:29,039 --> 00:18:32,720
let's go back and take a look at what's

00:18:31,200 --> 00:18:34,480
going on under the hood

00:18:32,720 --> 00:18:37,919
let's take a look at all the services

00:18:34,480 --> 00:18:37,919
that are now running in the namespace

00:18:38,799 --> 00:18:42,000
we have the new user part running as you

00:18:41,440 --> 00:18:43,840
can see this

00:18:42,000 --> 00:18:45,120
also has two containers one of them is

00:18:43,840 --> 00:18:46,960
the hto sidecar

00:18:45,120 --> 00:18:48,160
the other one being the notebook server

00:18:46,960 --> 00:18:50,240
itself

00:18:48,160 --> 00:18:51,520
and we also have a new service now to

00:18:50,240 --> 00:18:54,559
front the

00:18:51,520 --> 00:18:57,120
user part so every time hub would spawn

00:18:54,559 --> 00:18:58,720
a new user part it will also spawn a

00:18:57,120 --> 00:19:02,160
corresponding service

00:18:58,720 --> 00:19:04,320
this allows this this hub pod

00:19:02,160 --> 00:19:05,600
to talk to this user part and ensure

00:19:04,320 --> 00:19:07,840
that it's up and running

00:19:05,600 --> 00:19:09,600
before it starts configuring the network

00:19:07,840 --> 00:19:11,840
on the hto gateway

00:19:09,600 --> 00:19:12,720
using this proxy which is now just the

00:19:11,840 --> 00:19:14,720
api

00:19:12,720 --> 00:19:16,640
that listens to hub and would make a

00:19:14,720 --> 00:19:20,160
call to the kubernetes api

00:19:16,640 --> 00:19:22,320
to create new istio virtual services

00:19:20,160 --> 00:19:23,760
we can also take a look at the virtual

00:19:22,320 --> 00:19:25,520
services

00:19:23,760 --> 00:19:28,640
as we can see here there is a new istio

00:19:25,520 --> 00:19:32,320
virtual service that was created

00:19:28,640 --> 00:19:32,320
let's try and describe the service

00:19:34,559 --> 00:19:38,240
this looks very similar to the service

00:19:36,320 --> 00:19:39,840
that we saw earlier

00:19:38,240 --> 00:19:41,440
the only difference here is the uri

00:19:39,840 --> 00:19:44,320
prefix

00:19:41,440 --> 00:19:44,960
and the destination so when a request

00:19:44,320 --> 00:19:48,559
comes in

00:19:44,960 --> 00:19:51,039
for slash user slash username it will

00:19:48,559 --> 00:19:52,080
read out the traffic appropriately to

00:19:51,039 --> 00:19:54,080
the

00:19:52,080 --> 00:19:56,080
user service which is running under

00:19:54,080 --> 00:19:59,760
jupyter dash username

00:19:56,080 --> 00:19:59,760
as we saw in the list of components

00:19:59,919 --> 00:20:03,919
so what this allows us to do is to

00:20:01,600 --> 00:20:05,600
delegate all the network scalability

00:20:03,919 --> 00:20:07,600
needs to

00:20:05,600 --> 00:20:09,120
we are able to use the side cars for

00:20:07,600 --> 00:20:11,200
mutual tls

00:20:09,120 --> 00:20:12,240
whenever one component talks to other

00:20:11,200 --> 00:20:14,000
component

00:20:12,240 --> 00:20:16,400
all the traffic is encrypted between

00:20:14,000 --> 00:20:17,360
them and we are able to use istio

00:20:16,400 --> 00:20:19,440
gateway

00:20:17,360 --> 00:20:20,400
and the concept of virtual services

00:20:19,440 --> 00:20:23,280
configured with

00:20:20,400 --> 00:20:25,600
appropriate destination host to route

00:20:23,280 --> 00:20:27,600
the traffic

00:20:25,600 --> 00:20:30,000
this allows us to run our services at

00:20:27,600 --> 00:20:32,000
scale by delegating the network

00:20:30,000 --> 00:20:32,880
scalability needs to the hto service

00:20:32,000 --> 00:20:35,360
mesh

00:20:32,880 --> 00:20:37,039
before we go back to the slides let's

00:20:35,360 --> 00:20:40,720
quickly verify that our traffic is

00:20:37,039 --> 00:20:40,720
indeed routed through the sidecars

00:20:46,400 --> 00:20:49,600
we are going to tail the logs for the

00:20:48,799 --> 00:20:53,600
istio

00:20:49,600 --> 00:20:53,600
proxy container inside the pod

00:20:54,080 --> 00:20:57,840
as you can see here the sidecar

00:20:55,760 --> 00:20:58,799
intercepts the outgoing request from

00:20:57,840 --> 00:21:00,640
this part

00:20:58,799 --> 00:21:02,880
so this part is continuously sending

00:21:00,640 --> 00:21:05,280
heartbeats to the hub service

00:21:02,880 --> 00:21:08,000
and all those events are locked here in

00:21:05,280 --> 00:21:08,000
the sidecar

00:21:09,039 --> 00:21:12,960
at this point we were really comfortable

00:21:10,880 --> 00:21:14,799
working with the service mesh

00:21:12,960 --> 00:21:16,640
we were now looking at ways of using the

00:21:14,799 --> 00:21:17,760
service mesh to solve some of the other

00:21:16,640 --> 00:21:21,200
problems

00:21:17,760 --> 00:21:24,000
that we had in our system and this is

00:21:21,200 --> 00:21:26,000
where compliance and audit comes in

00:21:24,000 --> 00:21:29,200
so once we had the service running for a

00:21:26,000 --> 00:21:31,600
while and we got it on mutual tls

00:21:29,200 --> 00:21:33,280
we wanted to make sure that we don't

00:21:31,600 --> 00:21:35,440
regress on these things

00:21:33,280 --> 00:21:36,799
so we need to put in the checks to

00:21:35,440 --> 00:21:39,200
ensure

00:21:36,799 --> 00:21:40,559
that our services are compliant and stay

00:21:39,200 --> 00:21:43,760
compliant

00:21:40,559 --> 00:21:45,679
this is a prometheus metric for hto

00:21:43,760 --> 00:21:47,200
that we are continuously collecting from

00:21:45,679 --> 00:21:50,159
the system

00:21:47,200 --> 00:21:51,919
which tells us the state of mutual tls

00:21:50,159 --> 00:21:54,880
here you can see the connection security

00:21:51,919 --> 00:21:57,360
policy is set to mutual tls

00:21:54,880 --> 00:21:58,159
and we can set up our alerts on this to

00:21:57,360 --> 00:22:01,760
ensure that

00:21:58,159 --> 00:22:03,760
this policy is enforced all the time

00:22:01,760 --> 00:22:06,640
we can look at the services that are

00:22:03,760 --> 00:22:08,880
involved and also where this

00:22:06,640 --> 00:22:10,080
metric is being collected it's important

00:22:08,880 --> 00:22:12,640
here to know that the connection

00:22:10,080 --> 00:22:14,480
security policy is reported

00:22:12,640 --> 00:22:16,080
correctly on the reporter type

00:22:14,480 --> 00:22:17,679
destination

00:22:16,080 --> 00:22:20,080
for every request there is a source and

00:22:17,679 --> 00:22:20,799
a destination if you if you look at the

00:22:20,080 --> 00:22:23,200
request that

00:22:20,799 --> 00:22:25,120
originates from a source where the

00:22:23,200 --> 00:22:27,919
reporter is equal to source

00:22:25,120 --> 00:22:29,760
you might not see the correct connection

00:22:27,919 --> 00:22:32,159
policy because at that time

00:22:29,760 --> 00:22:33,200
it's not known whether the connection

00:22:32,159 --> 00:22:35,120
would be

00:22:33,200 --> 00:22:36,640
a mutual tls connection or not so it's

00:22:35,120 --> 00:22:40,000
always beneficial to look at the

00:22:36,640 --> 00:22:41,760
reported destination

00:22:40,000 --> 00:22:43,360
had started this project with a very

00:22:41,760 --> 00:22:46,400
small team

00:22:43,360 --> 00:22:48,480
but as the project scope expanded more

00:22:46,400 --> 00:22:50,720
and more engineers joined the team

00:22:48,480 --> 00:22:52,480
we had the need to be able to iterate

00:22:50,720 --> 00:22:54,480
fast on the product

00:22:52,480 --> 00:22:56,240
this meant that we needed to deploy and

00:22:54,480 --> 00:22:57,760
we needed to deploy fast

00:22:56,240 --> 00:22:59,120
and allow engineers to test their

00:22:57,760 --> 00:23:00,240
features in a production-like

00:22:59,120 --> 00:23:02,240
environment

00:23:00,240 --> 00:23:04,640
and also to be able to demo these

00:23:02,240 --> 00:23:07,039
features to the stakeholders

00:23:04,640 --> 00:23:09,440
we again used features from the service

00:23:07,039 --> 00:23:11,039
mesh to build out these capabilities we

00:23:09,440 --> 00:23:12,960
extended

00:23:11,039 --> 00:23:15,360
the features that were built around hdr

00:23:12,960 --> 00:23:17,919
virtual services and destination host

00:23:15,360 --> 00:23:18,880
to allow selective routing on deployment

00:23:17,919 --> 00:23:21,039
so this

00:23:18,880 --> 00:23:23,039
allowed every engineer to to deploy

00:23:21,039 --> 00:23:25,600
their component and test them

00:23:23,039 --> 00:23:27,360
in a production like setting service

00:23:25,600 --> 00:23:28,000
mesh also gives you additional benefits

00:23:27,360 --> 00:23:31,200
around

00:23:28,000 --> 00:23:33,520
things like rate limiting if you recall

00:23:31,200 --> 00:23:36,000
from the diagram that we shared earlier

00:23:33,520 --> 00:23:38,960
on the polyglot of services that we

00:23:36,000 --> 00:23:39,679
have it's really hard to go and

00:23:38,960 --> 00:23:42,400
implement

00:23:39,679 --> 00:23:43,200
the same features like rate limiting

00:23:42,400 --> 00:23:45,760
retries

00:23:43,200 --> 00:23:46,320
on each and every service with service

00:23:45,760 --> 00:23:48,880
mesh

00:23:46,320 --> 00:23:50,400
you can retry failed network requests

00:23:48,880 --> 00:23:53,919
you can put rate limits

00:23:50,400 --> 00:23:57,039
so that one service does not abuse

00:23:53,919 --> 00:23:58,080
the system and you can also do

00:23:57,039 --> 00:24:02,159
interesting things like

00:23:58,080 --> 00:24:05,039
fault tolerant this this helps you

00:24:02,159 --> 00:24:07,440
inject faults into into the system and

00:24:05,039 --> 00:24:10,320
see how your system behaves

00:24:07,440 --> 00:24:10,720
so all these when combined together

00:24:10,320 --> 00:24:13,279
makes

00:24:10,720 --> 00:24:14,559
for a really pleasant engineering

00:24:13,279 --> 00:24:16,000
experience because you

00:24:14,559 --> 00:24:18,880
you are focused on delivering the

00:24:16,000 --> 00:24:22,159
features and not worried about

00:24:18,880 --> 00:24:25,279
building a lot of infra that we can

00:24:22,159 --> 00:24:27,840
build using a service mesh

00:24:25,279 --> 00:24:30,400
and another important gain for using a

00:24:27,840 --> 00:24:32,400
service mesh is around telemetry

00:24:30,400 --> 00:24:34,159
so as all the traffic is now flowing

00:24:32,400 --> 00:24:37,279
through the service mesh

00:24:34,159 --> 00:24:38,640
we can easily see what's going on in the

00:24:37,279 --> 00:24:40,240
system

00:24:38,640 --> 00:24:41,760
so you we can see which service is

00:24:40,240 --> 00:24:43,840
talking to which other service

00:24:41,760 --> 00:24:46,720
we can see how many requests our service

00:24:43,840 --> 00:24:48,559
is making how many requests are failing

00:24:46,720 --> 00:24:51,039
how big of a data we are transferring

00:24:48,559 --> 00:24:52,880
and things like that

00:24:51,039 --> 00:24:54,960
while moving to a service mesh wasn't

00:24:52,880 --> 00:24:56,720
our first choice

00:24:54,960 --> 00:24:59,520
it all turned out to be a blessing in

00:24:56,720 --> 00:25:02,159
disguise in the end

00:24:59,520 --> 00:25:03,600
in conclusion i would like to say that a

00:25:02,159 --> 00:25:06,000
polyglot environment

00:25:03,600 --> 00:25:07,120
needs a service mesh the learnings that

00:25:06,000 --> 00:25:09,520
you have from

00:25:07,120 --> 00:25:11,600
putting one service on a service mesh

00:25:09,520 --> 00:25:14,080
are transferable to all other services

00:25:11,600 --> 00:25:15,760
that you're running in the system

00:25:14,080 --> 00:25:17,919
although troubleshooting can be a little

00:25:15,760 --> 00:25:20,320
tricky and you can run into some really

00:25:17,919 --> 00:25:21,840
challenging scenarios with the network

00:25:20,320 --> 00:25:24,000
once you are passed it

00:25:21,840 --> 00:25:28,000
operating your services at scale in a

00:25:24,000 --> 00:25:29,520
secure manner becomes really really easy

00:25:28,000 --> 00:25:31,520
you are not running a service mesh in

00:25:29,520 --> 00:25:33,360
your kubernetes cluster

00:25:31,520 --> 00:25:35,120
i strongly encourage you to give it a

00:25:33,360 --> 00:25:37,200
try

00:25:35,120 --> 00:25:38,880
it can help ensure that your engineers

00:25:37,200 --> 00:25:40,799
are focused on building the features

00:25:38,880 --> 00:25:43,600
that your customers want

00:25:40,799 --> 00:25:45,840
and also you have a very good

00:25:43,600 --> 00:25:47,600
understanding of your network security

00:25:45,840 --> 00:25:49,840
as well as you are able to scale your

00:25:47,600 --> 00:25:52,080
network if you are interested to learn

00:25:49,840 --> 00:25:54,000
more about the nitty gritties of

00:25:52,080 --> 00:25:56,000
how to troubleshoot and store service

00:25:54,000 --> 00:25:59,039
mesh i encourage you to check

00:25:56,000 --> 00:26:02,080
out this repository on github

00:25:59,039 --> 00:26:03,679
which has links to a blog detailing all

00:26:02,080 --> 00:26:04,720
the steps that we went through while

00:26:03,679 --> 00:26:07,840
troubleshooting

00:26:04,720 --> 00:26:09,520
the challenges that we run into

00:26:07,840 --> 00:26:11,760
i'm sure you'll be able to use some of

00:26:09,520 --> 00:26:13,760
these learnings in ensuring you're able

00:26:11,760 --> 00:26:14,799
to move your workloads onto a service

00:26:13,760 --> 00:26:17,919
mesh

00:26:14,799 --> 00:26:17,919

YouTube URL: https://www.youtube.com/watch?v=pYUXeFVjZ1o


