Title: Multi(Control Plane Network Mesh)??: A Practical MultiCluster... Nicholas Nellis & Vikas Choudhary
Publication date: 2020-11-25
Playlist: ServiceMeshCon North America 2020
Description: 
	Multi(Control Plane/Network/Mesh)??: A Practical MultiCluster Deployment - Nicholas Nellis & Vikas Choudhary, Tetrate

While Working with several traditional customers spanning defense, finance, etc., we found that the service mesh multicluster models that exist today are completely unusable from an enterprise point of view. They are designed with the network administrator in mind, focusing on how to connect two clusters, and not on how developers across teams like to consume the services exposed by other teams. The multicluster models that app teams want, turned out to be dramatically simpler than the ones out there today. This talk discusses our experiences working with these teams, our learnings from how they built out an API-centric multicluster model and what we as a community of (mostly) infrastructure developers should do to better support the application teams
Captions: 
	00:00:00,000 --> 00:00:03,760
hello everyone this is vikas and nick

00:00:02,240 --> 00:00:06,960
from tetrad

00:00:03,760 --> 00:00:09,920
at data we work at stu and other

00:00:06,960 --> 00:00:11,759
service smash technologies around stu in

00:00:09,920 --> 00:00:15,120
this talk

00:00:11,759 --> 00:00:17,440
we are going to discuss and

00:00:15,120 --> 00:00:18,720
share our experiences and learnings that

00:00:17,440 --> 00:00:20,480
we gained

00:00:18,720 --> 00:00:22,880
while working with some of our large

00:00:20,480 --> 00:00:24,640
enterprise customers

00:00:22,880 --> 00:00:27,279
trying to help them in magifying their

00:00:24,640 --> 00:00:29,119
multi-cluster installations

00:00:27,279 --> 00:00:30,640
this stock is roughly divided into four

00:00:29,119 --> 00:00:32,640
sections

00:00:30,640 --> 00:00:34,480
in the first section we will discuss

00:00:32,640 --> 00:00:36,640
about the complexity and the

00:00:34,480 --> 00:00:39,840
manageability issues in the

00:00:36,640 --> 00:00:43,360
large scale multi-cluster installations

00:00:39,840 --> 00:00:45,840
then assuming that service smash and

00:00:43,360 --> 00:00:49,440
histo is going to help in managing these

00:00:45,840 --> 00:00:51,520
problems we will see the gap and pain

00:00:49,440 --> 00:00:53,360
points if we try to follow

00:00:51,520 --> 00:00:55,520
this tube community suggested

00:00:53,360 --> 00:00:57,900
multi-cluster installation approaches

00:00:55,520 --> 00:00:59,840
for overlaying steel mesh

00:00:57,900 --> 00:01:03,280
[Music]

00:00:59,840 --> 00:01:05,600
installations and then we will discuss

00:01:03,280 --> 00:01:08,640
in the next two sections

00:01:05,600 --> 00:01:12,000
our our approach which is a bit

00:01:08,640 --> 00:01:14,479
more practical and and

00:01:12,000 --> 00:01:15,280
and how it is addressing the pain points

00:01:14,479 --> 00:01:18,000
and the gaps

00:01:15,280 --> 00:01:21,040
that we found in the community suggested

00:01:18,000 --> 00:01:25,520
installation approaches

00:01:21,040 --> 00:01:27,119
so next uh passing over to nick to start

00:01:25,520 --> 00:01:28,880
with the problems of the multi-cluster

00:01:27,119 --> 00:01:31,840
installations

00:01:28,880 --> 00:01:32,560
thank you yeah congratulations everybody

00:01:31,840 --> 00:01:34,799
you've been

00:01:32,560 --> 00:01:36,880
you've made it to multi-cluster you've

00:01:34,799 --> 00:01:40,159
successfully scaled your application

00:01:36,880 --> 00:01:43,439
from a single cluster to multiple

00:01:40,159 --> 00:01:44,720
but you're looking at back and your slas

00:01:43,439 --> 00:01:46,799
haven't improved

00:01:44,720 --> 00:01:48,799
um you broke a bunch of security rules

00:01:46,799 --> 00:01:49,920
in doing so um so that you could get the

00:01:48,799 --> 00:01:52,159
networking

00:01:49,920 --> 00:01:53,840
to connect properly and then you're

00:01:52,159 --> 00:01:55,520
realizing that management has got a lot

00:01:53,840 --> 00:01:57,439
more difficult

00:01:55,520 --> 00:01:58,719
it seems like the problems that you're

00:01:57,439 --> 00:02:01,119
facing

00:01:58,719 --> 00:02:02,399
are bigger than the ones you were doing

00:02:01,119 --> 00:02:05,600
earlier so

00:02:02,399 --> 00:02:07,280
trading off a better sla

00:02:05,600 --> 00:02:08,959
for more problems didn't really seem

00:02:07,280 --> 00:02:10,239
advantageous to you and so we're going

00:02:08,959 --> 00:02:12,080
to walk through some of these problems

00:02:10,239 --> 00:02:12,879
that a lot of our customers have early

00:02:12,080 --> 00:02:16,480
on when they're

00:02:12,879 --> 00:02:18,319
adopting the multi-cluster model

00:02:16,480 --> 00:02:21,120
the first one that we want to talk about

00:02:18,319 --> 00:02:23,280
is the this is a typical deployment

00:02:21,120 --> 00:02:26,000
that we that you would see when you go

00:02:23,280 --> 00:02:29,360
from a one to multi-cluster

00:02:26,000 --> 00:02:31,519
environment typically you have a

00:02:29,360 --> 00:02:32,800
low balancer within the cluster um

00:02:31,519 --> 00:02:34,879
engine x ingress is

00:02:32,800 --> 00:02:36,480
like the default one that most you're

00:02:34,879 --> 00:02:38,879
probably familiar with

00:02:36,480 --> 00:02:40,560
and then we connect all of those

00:02:38,879 --> 00:02:42,400
clusters and do the routing with a

00:02:40,560 --> 00:02:44,319
tier one which we call tier one low

00:02:42,400 --> 00:02:46,800
balance here which is typically your

00:02:44,319 --> 00:02:48,720
your cloud load balancer and that's so

00:02:46,800 --> 00:02:51,360
you have a single point of entry and can

00:02:48,720 --> 00:02:51,840
route between multiple clusters but

00:02:51,360 --> 00:02:53,680
you're

00:02:51,840 --> 00:02:55,760
you're team one you've you've been

00:02:53,680 --> 00:02:56,640
tasked to move your microservice from

00:02:55,760 --> 00:02:59,040
one cluster

00:02:56,640 --> 00:03:00,720
into a multicultural environment now um

00:02:59,040 --> 00:03:01,440
you're you're gonna follow the coattails

00:03:00,720 --> 00:03:02,879
of team two

00:03:01,440 --> 00:03:04,640
who's already been doing it for a little

00:03:02,879 --> 00:03:06,159
while and so you're going to

00:03:04,640 --> 00:03:08,720
take your application and stick it

00:03:06,159 --> 00:03:09,120
behind that same default low balancer

00:03:08,720 --> 00:03:12,560
that

00:03:09,120 --> 00:03:15,040
that team won or team 2 has been using

00:03:12,560 --> 00:03:17,680
but also you start to know you have more

00:03:15,040 --> 00:03:20,159
outages than you have before

00:03:17,680 --> 00:03:21,760
you've bound yourself now to the

00:03:20,159 --> 00:03:24,080
failures of t2

00:03:21,760 --> 00:03:25,360
uh team two's errors have now become

00:03:24,080 --> 00:03:27,280
your problem

00:03:25,360 --> 00:03:28,959
because when they have problems their

00:03:27,280 --> 00:03:30,480
low bounces they cause the low balance

00:03:28,959 --> 00:03:32,959
to be removed from the pool

00:03:30,480 --> 00:03:33,680
which is now causing outages for you and

00:03:32,959 --> 00:03:36,879
so

00:03:33,680 --> 00:03:37,920
there's all there's cases where going

00:03:36,879 --> 00:03:39,680
multi-cluster

00:03:37,920 --> 00:03:41,680
doesn't actually make anything better

00:03:39,680 --> 00:03:44,000
and so we're gonna have some solutions

00:03:41,680 --> 00:03:46,959
that will address these problems in a

00:03:44,000 --> 00:03:50,239
very practical way

00:03:46,959 --> 00:03:53,439
secondly when you go to this

00:03:50,239 --> 00:03:56,560
typical multi-cluster routing

00:03:53,439 --> 00:03:58,400
you're you don't have a

00:03:56,560 --> 00:03:59,599
good way of being able to tell your

00:03:58,400 --> 00:04:02,319
microservice

00:03:59,599 --> 00:04:03,680
how to locally address services if

00:04:02,319 --> 00:04:04,799
they're like within your cluster or

00:04:03,680 --> 00:04:08,239
within your region

00:04:04,799 --> 00:04:09,920
and so to get the aha

00:04:08,239 --> 00:04:11,920
availability you're going to have to

00:04:09,920 --> 00:04:14,879
call up outside of your

00:04:11,920 --> 00:04:16,880
cluster to that tier one low balancer to

00:04:14,879 --> 00:04:19,359
get

00:04:16,880 --> 00:04:20,560
access to the other micro service that

00:04:19,359 --> 00:04:21,600
your service that you're trying to

00:04:20,560 --> 00:04:24,960
access

00:04:21,600 --> 00:04:26,080
and so if you typically only have like

00:04:24,960 --> 00:04:28,880
two options either you

00:04:26,080 --> 00:04:30,320
route externally to that low balancer or

00:04:28,880 --> 00:04:32,320
directly internally to that

00:04:30,320 --> 00:04:33,520
that cluster that you're on but if you

00:04:32,320 --> 00:04:36,160
wrote locally

00:04:33,520 --> 00:04:36,800
and that that pod or that service is

00:04:36,160 --> 00:04:40,000
down

00:04:36,800 --> 00:04:40,000
you have no failover

00:04:40,479 --> 00:04:44,400
and finally as your multi-cluster

00:04:43,040 --> 00:04:46,800
architecture grows

00:04:44,400 --> 00:04:48,240
you can get some really complex

00:04:46,800 --> 00:04:51,520
dependency chains

00:04:48,240 --> 00:04:52,320
um you it's be very difficult for you to

00:04:51,520 --> 00:04:54,720
trace

00:04:52,320 --> 00:04:56,800
all the all of the consumers that depend

00:04:54,720 --> 00:05:00,400
on you and your functionality

00:04:56,800 --> 00:05:02,880
and so without service mesh

00:05:00,400 --> 00:05:03,680
you'll find these problems are very

00:05:02,880 --> 00:05:05,199
prevalent

00:05:03,680 --> 00:05:07,280
um and so then you'll be looking for

00:05:05,199 --> 00:05:09,199
that solution which you know service

00:05:07,280 --> 00:05:12,400
mess seems to be the one that'll

00:05:09,199 --> 00:05:14,720
that'll solve these problems for you

00:05:12,400 --> 00:05:15,919
and so as you're adopting service mesh

00:05:14,720 --> 00:05:17,360
there's a lot of questions that our

00:05:15,919 --> 00:05:18,960
customers are asking

00:05:17,360 --> 00:05:21,120
and how do you go about doing it in the

00:05:18,960 --> 00:05:23,120
correct way you know am i adding more

00:05:21,120 --> 00:05:24,960
complexity to my architecture

00:05:23,120 --> 00:05:26,400
when multi-clusters already made it

00:05:24,960 --> 00:05:29,440
complex

00:05:26,400 --> 00:05:31,600
i chose istio and now when i upgraded it

00:05:29,440 --> 00:05:33,120
i've lost connectivity to everything and

00:05:31,600 --> 00:05:34,639
so we're having larger production

00:05:33,120 --> 00:05:36,240
outages

00:05:34,639 --> 00:05:37,759
do i need to re-architect my

00:05:36,240 --> 00:05:40,160
multi-cluster

00:05:37,759 --> 00:05:41,840
up so that i can adopt service mesh and

00:05:40,160 --> 00:05:46,160
so a lot of these questions

00:05:41,840 --> 00:05:46,160
um we'll answer later in the slides

00:05:46,880 --> 00:05:52,240
and i'll pass over to the cost to start

00:05:48,479 --> 00:05:55,759
with the istio solution

00:05:52,240 --> 00:05:59,199
uh so yeah there's no denying that

00:05:55,759 --> 00:06:02,479
uh istio is super powerful it can

00:05:59,199 --> 00:06:05,759
it's you can help in in

00:06:02,479 --> 00:06:08,479
in in is you can help in

00:06:05,759 --> 00:06:10,479
managing these complexities of

00:06:08,479 --> 00:06:13,580
multi-cluster installations

00:06:10,479 --> 00:06:14,800
but but how exactly i am going to

00:06:13,580 --> 00:06:16,800
[Music]

00:06:14,800 --> 00:06:18,000
overlays to measure multi-cluster

00:06:16,800 --> 00:06:20,560
installations

00:06:18,000 --> 00:06:21,039
because there are different teams

00:06:20,560 --> 00:06:24,000
different

00:06:21,039 --> 00:06:25,039
different personas different uh

00:06:24,000 --> 00:06:28,400
administrative

00:06:25,039 --> 00:06:31,759
uh boundaries over these clusters so how

00:06:28,400 --> 00:06:31,759
how one is going to do that

00:06:32,840 --> 00:06:39,520
so we'll start with taking a look over

00:06:36,400 --> 00:06:42,400
what the steel community

00:06:39,520 --> 00:06:43,680
documentation has been suggesting so at

00:06:42,400 --> 00:06:46,560
the moment there are

00:06:43,680 --> 00:06:46,960
in the latest release which is 1.7 there

00:06:46,560 --> 00:06:48,560
are

00:06:46,960 --> 00:06:50,800
two multi-cluster installation

00:06:48,560 --> 00:06:52,160
approaches the one is replicated control

00:06:50,800 --> 00:06:55,199
planes and another one is

00:06:52,160 --> 00:06:57,280
shared control plane because of the

00:06:55,199 --> 00:07:00,840
resiliency requirements

00:06:57,280 --> 00:07:04,479
we can rule out the shared control plane

00:07:00,840 --> 00:07:08,400
approach and let's take a deeper look at

00:07:04,479 --> 00:07:08,400
the replicated control planes approach

00:07:09,360 --> 00:07:14,840
so how this works is that in your

00:07:12,479 --> 00:07:18,319
clusters each cluster has

00:07:14,840 --> 00:07:21,919
independent seo control plane running

00:07:18,319 --> 00:07:24,400
and the and the ca

00:07:21,919 --> 00:07:25,919
of each of the issues control plane on

00:07:24,400 --> 00:07:28,639
these clusters

00:07:25,919 --> 00:07:30,400
is configured with the intermediate cs

00:07:28,639 --> 00:07:33,599
which are generated from the

00:07:30,400 --> 00:07:35,440
same shared root ca

00:07:33,599 --> 00:07:38,400
the services which are the shared

00:07:35,440 --> 00:07:41,039
services and are supposed to be

00:07:38,400 --> 00:07:43,039
accessed from the remote clusters are

00:07:41,039 --> 00:07:45,280
exposed through service entries

00:07:43,039 --> 00:07:46,720
for example here the service who which

00:07:45,280 --> 00:07:48,720
is in the cluster 2

00:07:46,720 --> 00:07:50,400
is exposed on the cluster 1 using a

00:07:48,720 --> 00:07:52,639
service entry

00:07:50,400 --> 00:07:54,479
and the host name in the service entry

00:07:52,639 --> 00:07:57,199
has

00:07:54,479 --> 00:07:58,000
this dot global prefix added to the name

00:07:57,199 --> 00:08:01,199
and namespace

00:07:58,000 --> 00:08:03,440
of the service and the endpoints

00:08:01,199 --> 00:08:04,240
of the service entry points to the

00:08:03,440 --> 00:08:08,400
gateways

00:08:04,240 --> 00:08:11,280
of the remote clusters at runtime

00:08:08,400 --> 00:08:12,319
it looks something like this when a

00:08:11,280 --> 00:08:15,360
client

00:08:12,319 --> 00:08:19,199
makes a request it goes to kubernetes

00:08:15,360 --> 00:08:21,280
a dns server kubernetes kubernetes dns

00:08:19,199 --> 00:08:25,840
we have to run with a

00:08:21,280 --> 00:08:28,800
core dns plugin which is shipped by

00:08:25,840 --> 00:08:30,080
all so all the dot global prefixed

00:08:28,800 --> 00:08:33,279
hostname queries

00:08:30,080 --> 00:08:35,599
um kubernetes dns pass these

00:08:33,279 --> 00:08:38,320
queries to core dns and poor dns

00:08:35,599 --> 00:08:38,320
resolves these

00:08:38,640 --> 00:08:43,760
these hostname dns queries with the

00:08:41,279 --> 00:08:47,360
virtual id of the service entry

00:08:43,760 --> 00:08:49,200
and using this virtual ip on the sidecar

00:08:47,360 --> 00:08:52,240
picks up the listener

00:08:49,200 --> 00:08:54,160
and there the listener has the

00:08:52,240 --> 00:08:56,480
is pointing to the remote gateway where

00:08:54,160 --> 00:08:58,000
the actual the services the back-end

00:08:56,480 --> 00:08:59,680
service is running

00:08:58,000 --> 00:09:02,080
and this way the request reaches there

00:08:59,680 --> 00:09:05,440
is cluster tools gateway

00:09:02,080 --> 00:09:06,160
and sni host it still has the hostname

00:09:05,440 --> 00:09:08,880
which has got

00:09:06,160 --> 00:09:10,560
global prefix so it won't make to the

00:09:08,880 --> 00:09:12,480
actual backend service

00:09:10,560 --> 00:09:13,839
so there a an online filter is

00:09:12,480 --> 00:09:16,720
configured

00:09:13,839 --> 00:09:18,480
to to translate this top global from the

00:09:16,720 --> 00:09:22,320
si host to the

00:09:18,480 --> 00:09:26,160
service store cluster total local so now

00:09:22,320 --> 00:09:28,880
um this gateway on the cluster 2

00:09:26,160 --> 00:09:30,080
can finally forward the request to the

00:09:28,880 --> 00:09:34,560
actual service

00:09:30,080 --> 00:09:37,680
implementation but

00:09:34,560 --> 00:09:38,880
can we can we use this in our production

00:09:37,680 --> 00:09:43,600
environments

00:09:38,880 --> 00:09:46,000
unfortunately not and why not is because

00:09:43,600 --> 00:09:48,080
it it doesn't support locality aware

00:09:46,000 --> 00:09:51,920
routing out of the box

00:09:48,080 --> 00:09:53,519
we cannot simply if if we have um

00:09:51,920 --> 00:09:56,560
if we have a local instance of the

00:09:53,519 --> 00:09:59,440
service running in cluster

00:09:56,560 --> 00:10:01,360
ideally it is desired that the for that

00:09:59,440 --> 00:10:03,519
the first priority should be

00:10:01,360 --> 00:10:05,200
uh the request should be served by the

00:10:03,519 --> 00:10:08,240
local instance

00:10:05,200 --> 00:10:10,320
so we should be able to somehow

00:10:08,240 --> 00:10:11,680
the local instance should be part of the

00:10:10,320 --> 00:10:13,279
load balancer pool

00:10:11,680 --> 00:10:15,440
but we cannot simply here in this

00:10:13,279 --> 00:10:17,519
approach we cannot simply put the

00:10:15,440 --> 00:10:19,440
cluster ip in the service entry

00:10:17,519 --> 00:10:21,120
endpoints and the reason is the same

00:10:19,440 --> 00:10:23,040
this dot global thing

00:10:21,120 --> 00:10:24,399
we can fix this we can make it correct

00:10:23,040 --> 00:10:28,000
by adding

00:10:24,399 --> 00:10:29,360
um virtual service to rewrite this host

00:10:28,000 --> 00:10:32,079
name

00:10:29,360 --> 00:10:34,480
converting this door global to canonical

00:10:32,079 --> 00:10:37,040
name of the service in the local

00:10:34,480 --> 00:10:37,519
local cluster but this problem is not

00:10:37,040 --> 00:10:40,079
the

00:10:37,519 --> 00:10:41,519
only problem there is there are much

00:10:40,079 --> 00:10:43,839
bigger problems than this in this

00:10:41,519 --> 00:10:47,279
approach

00:10:43,839 --> 00:10:50,480
the so so the host names

00:10:47,279 --> 00:10:51,519
in the clients uh are directly depending

00:10:50,480 --> 00:10:53,600
on the

00:10:51,519 --> 00:10:54,720
actual back-end implementation of the

00:10:53,600 --> 00:10:56,800
services

00:10:54,720 --> 00:10:58,000
so let me explain you with this example

00:10:56,800 --> 00:11:00,000
here the

00:10:58,000 --> 00:11:02,160
actual service is foo which is running

00:11:00,000 --> 00:11:03,120
in the funnest name space in the cluster

00:11:02,160 --> 00:11:05,040
too

00:11:03,120 --> 00:11:07,279
so the the clients on the remote

00:11:05,040 --> 00:11:10,079
clusters here at the cluster one

00:11:07,279 --> 00:11:11,760
should use the host name uh which is the

00:11:10,079 --> 00:11:14,240
service name actual service name

00:11:11,760 --> 00:11:15,680
dot service name space namespace.global

00:11:14,240 --> 00:11:18,880
and this is pretty bad

00:11:15,680 --> 00:11:20,079
because if there are n number of

00:11:18,880 --> 00:11:23,680
clusters

00:11:20,079 --> 00:11:27,440
serving this uh service serving this

00:11:23,680 --> 00:11:29,920
api serving this service then

00:11:27,440 --> 00:11:31,680
the owner of these services cannot

00:11:29,920 --> 00:11:32,880
change the backend implementation of the

00:11:31,680 --> 00:11:34,560
service at all

00:11:32,880 --> 00:11:37,760
because the clients will get broken in

00:11:34,560 --> 00:11:37,760
that case so

00:11:38,800 --> 00:11:42,000
in other words we can say there is no

00:11:40,480 --> 00:11:43,519
abstraction the

00:11:42,000 --> 00:11:45,120
clients are directly depending on the

00:11:43,519 --> 00:11:48,640
actual backend implementation

00:11:45,120 --> 00:11:52,079
and there is no very straightforward

00:11:48,640 --> 00:11:52,079
locality aware routing as well

00:11:52,160 --> 00:11:56,560
these points are already acknowledged by

00:11:54,320 --> 00:11:58,079
the history community as well and

00:11:56,560 --> 00:12:00,079
we also part of this community and we

00:11:58,079 --> 00:12:01,839
have been working to come up with better

00:12:00,079 --> 00:12:05,360
installation approaches

00:12:01,839 --> 00:12:07,839
so in the 1.8 uh we have this

00:12:05,360 --> 00:12:10,480
a new approach and and this replicated

00:12:07,839 --> 00:12:11,760
control plane is not there anymore in

00:12:10,480 --> 00:12:14,240
the coming releases

00:12:11,760 --> 00:12:15,600
but so in this approach for example

00:12:14,240 --> 00:12:18,240
which is coming

00:12:15,600 --> 00:12:19,760
here each here also we have a stereo

00:12:18,240 --> 00:12:20,720
control plane running in each of the

00:12:19,760 --> 00:12:23,519
clusters

00:12:20,720 --> 00:12:24,880
and what change here is this that the

00:12:23,519 --> 00:12:26,959
study

00:12:24,880 --> 00:12:29,839
of each cluster is watching the api

00:12:26,959 --> 00:12:32,320
servers of all the remote clusters

00:12:29,839 --> 00:12:34,240
so this solves the locality the aware

00:12:32,320 --> 00:12:38,079
routing problem because

00:12:34,240 --> 00:12:40,320
uh because the end points it has

00:12:38,079 --> 00:12:42,240
because it has all the local endpoints

00:12:40,320 --> 00:12:43,360
and the remote endpoint in the same lb

00:12:42,240 --> 00:12:46,480
pool

00:12:43,360 --> 00:12:48,480
but the problem of the direct dependency

00:12:46,480 --> 00:12:50,320
on the canonical name of the service and

00:12:48,480 --> 00:12:52,880
namespace is still there

00:12:50,320 --> 00:12:54,639
still there is no abstraction and plus

00:12:52,880 --> 00:12:56,480
the scalability issues are there and the

00:12:54,639 --> 00:12:57,920
security concerns are there

00:12:56,480 --> 00:12:59,839
because these clusters are owned by

00:12:57,920 --> 00:13:01,519
different teams and they may not want to

00:12:59,839 --> 00:13:04,480
expose all their

00:13:01,519 --> 00:13:07,600
internal implementation of the services

00:13:04,480 --> 00:13:10,560
to other teams

00:13:07,600 --> 00:13:12,240
so that's where we we had to come up

00:13:10,560 --> 00:13:13,519
with a different approach

00:13:12,240 --> 00:13:15,680
to meet the requirements of our

00:13:13,519 --> 00:13:16,240
customers and now i'm passing over to

00:13:15,680 --> 00:13:19,920
nick

00:13:16,240 --> 00:13:21,600
to explain our approach yeah so we we

00:13:19,920 --> 00:13:24,959
took a look at this um

00:13:21,600 --> 00:13:26,800
and looked at our customers problems and

00:13:24,959 --> 00:13:28,560
our customers have many many clusters

00:13:26,800 --> 00:13:33,200
that they want to connect together

00:13:28,560 --> 00:13:36,959
um and span multiple clouds

00:13:33,200 --> 00:13:39,199
and so we we wanted an approach that was

00:13:36,959 --> 00:13:40,320
somewhat simplistic and practical to

00:13:39,199 --> 00:13:42,560
their use cases

00:13:40,320 --> 00:13:44,639
and it didn't require a lot of um

00:13:42,560 --> 00:13:45,920
architectural core potential changes

00:13:44,639 --> 00:13:49,040
within their environments

00:13:45,920 --> 00:13:50,800
to adopt this service mesh architecture

00:13:49,040 --> 00:13:52,639
um and then we also want to put a little

00:13:50,800 --> 00:13:53,839
a little bit of the application

00:13:52,639 --> 00:13:58,320
developer

00:13:53,839 --> 00:14:00,240
in mind when we when we deploy these

00:13:58,320 --> 00:14:02,480
and so what kind of developer mindset

00:14:00,240 --> 00:14:04,079
did we try to assume

00:14:02,480 --> 00:14:05,680
when we were coming up with this

00:14:04,079 --> 00:14:08,959
solution well

00:14:05,680 --> 00:14:11,680
developers just want to consume

00:14:08,959 --> 00:14:14,880
applications as sas products even

00:14:11,680 --> 00:14:18,079
internally they want to access your api

00:14:14,880 --> 00:14:20,639
that assume that it's ha

00:14:18,079 --> 00:14:21,199
and that you're you're routing locally

00:14:20,639 --> 00:14:23,519
or

00:14:21,199 --> 00:14:25,199
you know efficiently and so they don't

00:14:23,519 --> 00:14:27,279
really necessarily care where your

00:14:25,199 --> 00:14:28,480
your service is hosted it's that they

00:14:27,279 --> 00:14:31,199
can just reach it

00:14:28,480 --> 00:14:32,480
easily they want to spend more time on

00:14:31,199 --> 00:14:33,680
implementing the features

00:14:32,480 --> 00:14:35,839
that are really going to drive that

00:14:33,680 --> 00:14:38,160
product

00:14:35,839 --> 00:14:40,639
rather than spending time on a lot of

00:14:38,160 --> 00:14:43,199
the implementation details

00:14:40,639 --> 00:14:44,079
with auth routing networking and a lot

00:14:43,199 --> 00:14:48,079
of stuff that comes with

00:14:44,079 --> 00:14:49,760
service mesh or or even without

00:14:48,079 --> 00:14:51,279
and they want to be able to advertise

00:14:49,760 --> 00:14:53,279
their own products

00:14:51,279 --> 00:14:55,120
effectively and easily to either

00:14:53,279 --> 00:14:57,360
external customers or to other teams

00:14:55,120 --> 00:14:59,440
within their organization

00:14:57,360 --> 00:15:00,639
and so with that in mind we came up with

00:14:59,440 --> 00:15:03,440
a

00:15:00,639 --> 00:15:05,279
much more simplistic approach to uh

00:15:03,440 --> 00:15:08,000
initial installation

00:15:05,279 --> 00:15:09,600
and that's by deploying istio

00:15:08,000 --> 00:15:13,120
essentially isolated

00:15:09,600 --> 00:15:16,399
per cluster we deploy the control plane

00:15:13,120 --> 00:15:18,399
on every cluster we scope that control

00:15:16,399 --> 00:15:19,839
plane to only know about services within

00:15:18,399 --> 00:15:22,240
the cluster that it's

00:15:19,839 --> 00:15:24,000
residing and so we have a locally scoped

00:15:22,240 --> 00:15:25,760
mesh

00:15:24,000 --> 00:15:27,360
we say that you should manage these

00:15:25,760 --> 00:15:29,440
control planes externally

00:15:27,360 --> 00:15:30,800
and you can do so with a number of tools

00:15:29,440 --> 00:15:34,320
um ci

00:15:30,800 --> 00:15:36,240
tools github and so

00:15:34,320 --> 00:15:38,079
that'll take away the the managing

00:15:36,240 --> 00:15:39,600
everything individually

00:15:38,079 --> 00:15:41,120
and then we the final point is we really

00:15:39,600 --> 00:15:42,320
want you to embrace gateways

00:15:41,120 --> 00:15:46,800
a little bit differently than you are

00:15:42,320 --> 00:15:46,800
today and expand upon the use of those

00:15:47,360 --> 00:15:51,600
but why so why do we want to do separate

00:15:50,399 --> 00:15:54,000
pistol control points

00:15:51,600 --> 00:15:54,959
well we really want to align the failure

00:15:54,000 --> 00:15:57,279
domains

00:15:54,959 --> 00:15:58,079
of your application to that cluster and

00:15:57,279 --> 00:16:02,079
so

00:15:58,079 --> 00:16:03,519
if you push bad configs or you upgrade

00:16:02,079 --> 00:16:05,199
istio incorrectly

00:16:03,519 --> 00:16:06,639
and it causes an outage it is now

00:16:05,199 --> 00:16:07,440
localized to that cluster that you were

00:16:06,639 --> 00:16:09,120
working

00:16:07,440 --> 00:16:10,480
and so it gives you a lot more control

00:16:09,120 --> 00:16:13,759
over not

00:16:10,480 --> 00:16:16,000
bringing down your entire environment

00:16:13,759 --> 00:16:17,279
and it aligns that issue control plane

00:16:16,000 --> 00:16:19,279
with the underlying cluster that it's

00:16:17,279 --> 00:16:21,600
running out so networking and

00:16:19,279 --> 00:16:22,320
node management stuff like that and so

00:16:21,600 --> 00:16:23,920
it's a lot

00:16:22,320 --> 00:16:25,519
a lot more effective for our customers

00:16:23,920 --> 00:16:28,399
to manage them these days

00:16:25,519 --> 00:16:29,680
and ca especially for outages um and

00:16:28,399 --> 00:16:31,440
then it also allows you to do safer

00:16:29,680 --> 00:16:32,639
upgrades so as you upgrade issue you

00:16:31,440 --> 00:16:35,680
might be able to pick a

00:16:32,639 --> 00:16:36,000
cluster that has less usage update that

00:16:35,680 --> 00:16:37,279
one

00:16:36,000 --> 00:16:39,759
if it goes successfully then you can

00:16:37,279 --> 00:16:42,320
roll it out to other clusters

00:16:39,759 --> 00:16:44,160
and this this would differ from the

00:16:42,320 --> 00:16:48,880
shared control plane

00:16:44,160 --> 00:16:48,880
that was one of the seo recommendations

00:16:49,519 --> 00:16:55,440
um but we have a problem we can't then

00:16:52,800 --> 00:16:56,560
applications cannot see up services in

00:16:55,440 --> 00:16:59,920
other clusters

00:16:56,560 --> 00:17:03,759
and so we need to adjust this problem

00:16:59,920 --> 00:17:05,839
and so the way that we

00:17:03,759 --> 00:17:07,199
uh want you to go about doing this is by

00:17:05,839 --> 00:17:09,439
embracing gateways

00:17:07,199 --> 00:17:11,520
and gateways are just low balancers that

00:17:09,439 --> 00:17:13,919
are fronting your applications

00:17:11,520 --> 00:17:16,000
um but we want you to use them on a

00:17:13,919 --> 00:17:18,400
product focused

00:17:16,000 --> 00:17:20,079
architecture and so currently you're

00:17:18,400 --> 00:17:20,799
probably using the engine x ingress

00:17:20,079 --> 00:17:22,799
gateway

00:17:20,799 --> 00:17:24,959
or you're if you're using istio the

00:17:22,799 --> 00:17:27,199
defaulting gas gateway and

00:17:24,959 --> 00:17:28,480
istio system but we want you to kind of

00:17:27,199 --> 00:17:30,960
get rid of those and move

00:17:28,480 --> 00:17:32,880
to this product focus database and so in

00:17:30,960 --> 00:17:34,880
istio that's really easy

00:17:32,880 --> 00:17:36,400
you can just stand up any number of

00:17:34,880 --> 00:17:39,200
ingress gateways

00:17:36,400 --> 00:17:40,480
and then determine the services that are

00:17:39,200 --> 00:17:43,039
are behind it

00:17:40,480 --> 00:17:44,080
or upstream from it and so these product

00:17:43,039 --> 00:17:46,960
focus gateways then

00:17:44,080 --> 00:17:47,760
allow you to control the micro services

00:17:46,960 --> 00:17:50,320
behind it

00:17:47,760 --> 00:17:52,720
as if there are one api and so you can

00:17:50,320 --> 00:17:56,080
expose your api

00:17:52,720 --> 00:17:58,080
uh internally or externally

00:17:56,080 --> 00:17:59,120
via this gateway and now you've aligned

00:17:58,080 --> 00:18:01,600
the failure domain

00:17:59,120 --> 00:18:02,880
of this gateway with the product that

00:18:01,600 --> 00:18:04,720
that it's supporting

00:18:02,880 --> 00:18:06,080
and so you're not you won't have the

00:18:04,720 --> 00:18:08,720
shared gateway problem

00:18:06,080 --> 00:18:09,600
where other products could bring you

00:18:08,720 --> 00:18:11,919
down

00:18:09,600 --> 00:18:12,960
um and it just requires you to stand up

00:18:11,919 --> 00:18:14,880
more gateways

00:18:12,960 --> 00:18:16,720
and then you'll be able to talk across

00:18:14,880 --> 00:18:18,240
cluster a lot easier and we'll explain

00:18:16,720 --> 00:18:21,600
why in a sec

00:18:18,240 --> 00:18:21,600
um then you can also push

00:18:21,760 --> 00:18:25,520
your off authentication and

00:18:23,840 --> 00:18:26,320
authorization circuit breaking up into

00:18:25,520 --> 00:18:27,919
this gateway

00:18:26,320 --> 00:18:29,679
and then tune it specifically for your

00:18:27,919 --> 00:18:31,520
applications that are behind it

00:18:29,679 --> 00:18:35,280
and so it's a really purpose-built

00:18:31,520 --> 00:18:35,280
gateway for your product

00:18:35,919 --> 00:18:41,520
so in a in a cluster

00:18:39,360 --> 00:18:43,360
this is kind of what you would imagine

00:18:41,520 --> 00:18:46,400
logically what is happening

00:18:43,360 --> 00:18:49,440
from using a gateway the consumer name

00:18:46,400 --> 00:18:50,960
space on the right here

00:18:49,440 --> 00:18:53,039
is not related to the payments api

00:18:50,960 --> 00:18:56,160
product but it does consume that api

00:18:53,039 --> 00:18:57,520
and so logically you should be consuming

00:18:56,160 --> 00:18:59,679
it at the gateway level

00:18:57,520 --> 00:19:01,039
that allows you to scale microservices

00:18:59,679 --> 00:19:03,120
add functionality

00:19:01,039 --> 00:19:04,320
behind it without interruption of that

00:19:03,120 --> 00:19:07,200
service

00:19:04,320 --> 00:19:08,880
and so when when we go to a

00:19:07,200 --> 00:19:12,320
multi-cluster environment

00:19:08,880 --> 00:19:15,120
this becomes really easy we

00:19:12,320 --> 00:19:17,120
can then just replicate that payments

00:19:15,120 --> 00:19:19,280
namespace into another cluster with that

00:19:17,120 --> 00:19:22,400
gateway

00:19:19,280 --> 00:19:24,400
in as a whole package and then

00:19:22,400 --> 00:19:26,160
the consumer namespace now has multiple

00:19:24,400 --> 00:19:30,240
endpoints to reach you at

00:19:26,160 --> 00:19:31,840
and so we we add those those hosts to

00:19:30,240 --> 00:19:33,280
that consumer namespace so now it has

00:19:31,840 --> 00:19:36,320
two options

00:19:33,280 --> 00:19:39,840
so making your your payments api

00:19:36,320 --> 00:19:41,840
highly available

00:19:39,840 --> 00:19:44,080
but we didn't have to stop there we can

00:19:41,840 --> 00:19:45,360
say we can do some more intelligent

00:19:44,080 --> 00:19:48,960
routing that's saying

00:19:45,360 --> 00:19:51,200
if your consumer is in the same

00:19:48,960 --> 00:19:52,080
cluster as the gateway that it's

00:19:51,200 --> 00:19:55,280
consuming

00:19:52,080 --> 00:19:57,039
and that gateway is closer than say an

00:19:55,280 --> 00:20:00,880
external gateway in another cluster

00:19:57,039 --> 00:20:02,960
let's prefer routing locally first

00:20:00,880 --> 00:20:04,720
and so if you're in the same cluster we

00:20:02,960 --> 00:20:05,600
can actually use the sidecar to act as

00:20:04,720 --> 00:20:08,000
the

00:20:05,600 --> 00:20:09,840
gateway and so in this example this is

00:20:08,000 --> 00:20:11,760
what we're doing we're using the sidecar

00:20:09,840 --> 00:20:14,480
to act as the gateway to those

00:20:11,760 --> 00:20:17,600
microservices to access them directly

00:20:14,480 --> 00:20:18,320
but in the case of failure or outage we

00:20:17,600 --> 00:20:21,760
can actually

00:20:18,320 --> 00:20:24,000
reroute requests over to the cluster 2

00:20:21,760 --> 00:20:25,600
in the u.s west region

00:20:24,000 --> 00:20:27,760
so you still get that highly available

00:20:25,600 --> 00:20:30,799
but now we've added a component

00:20:27,760 --> 00:20:33,919
of this intelligent local aware

00:20:30,799 --> 00:20:34,559
routing this is really cost effective

00:20:33,919 --> 00:20:36,400
for

00:20:34,559 --> 00:20:38,080
our larger clients that have high

00:20:36,400 --> 00:20:39,840
volumes of traffic and

00:20:38,080 --> 00:20:42,559
egress and they're paying a lot for

00:20:39,840 --> 00:20:42,559
egress data

00:20:44,320 --> 00:20:48,080
so why should you embrace gateways more

00:20:46,480 --> 00:20:50,799
than you are today

00:20:48,080 --> 00:20:52,320
it gives you a way to abstract your apis

00:20:50,799 --> 00:20:55,200
and your microservices

00:20:52,320 --> 00:20:56,559
and with a product spin on a product

00:20:55,200 --> 00:20:59,760
focus

00:20:56,559 --> 00:21:02,080
is really architected for growing those

00:20:59,760 --> 00:21:05,280
in a multi-cluster environment it's more

00:21:02,080 --> 00:21:09,200
of a copy paste than a one-off or

00:21:05,280 --> 00:21:09,200
trying to figure out who owns the local

00:21:10,880 --> 00:21:17,440
who owns the local ingress gateway so

00:21:14,559 --> 00:21:19,520
you can attach your resources to it

00:21:17,440 --> 00:21:22,080
and then it aligns a lot more with the

00:21:19,520 --> 00:21:24,400
non-mesh architectures that exist today

00:21:22,080 --> 00:21:26,240
they're very low balancer centric and so

00:21:24,400 --> 00:21:28,640
gateway

00:21:26,240 --> 00:21:30,480
really fits that low balance and we're

00:21:28,640 --> 00:21:31,120
just empowering the gateway to be a lot

00:21:30,480 --> 00:21:33,440
more

00:21:31,120 --> 00:21:34,559
intelligent about the traffic that it's

00:21:33,440 --> 00:21:38,880
not that it's

00:21:34,559 --> 00:21:41,039
routing so if we put this all together

00:21:38,880 --> 00:21:42,559
and what we're doing at tetra is we're

00:21:41,039 --> 00:21:45,520
making that gateway discovery

00:21:42,559 --> 00:21:46,799
automated and so from any number of

00:21:45,520 --> 00:21:48,799
clusters that you

00:21:46,799 --> 00:21:50,159
you can stand up the gateways will

00:21:48,799 --> 00:21:52,640
automatically be discoverable

00:21:50,159 --> 00:21:54,400
in all other clusters so that means we

00:21:52,640 --> 00:21:55,919
don't need to know about all the other

00:21:54,400 --> 00:21:57,679
services

00:21:55,919 --> 00:21:59,039
microservices running in other clusters

00:21:57,679 --> 00:21:59,440
we just need to know where those gateway

00:21:59,039 --> 00:22:03,120
in

00:21:59,440 --> 00:22:04,480
ingress gateways are we are making sure

00:22:03,120 --> 00:22:05,120
that when you run to those other

00:22:04,480 --> 00:22:06,880
gateways

00:22:05,120 --> 00:22:08,400
outside of your own cluster that it's

00:22:06,880 --> 00:22:11,760
encrypted and it's

00:22:08,400 --> 00:22:14,400
authorized and then we're also

00:22:11,760 --> 00:22:14,880
implementing locality-based routing so

00:22:14,400 --> 00:22:17,039
to

00:22:14,880 --> 00:22:18,880
improve your cost savings and then

00:22:17,039 --> 00:22:22,159
failover

00:22:18,880 --> 00:22:23,520
to failover effectively and then finally

00:22:22,159 --> 00:22:26,159
we're using um

00:22:23,520 --> 00:22:27,600
a newer technology seo which is mesh dns

00:22:26,159 --> 00:22:30,159
but it allows you to

00:22:27,600 --> 00:22:31,760
eliminate the need for that name space

00:22:30,159 --> 00:22:33,600
routing that the cost was

00:22:31,760 --> 00:22:35,039
talking about earlier you can

00:22:33,600 --> 00:22:39,280
essentially use your own

00:22:35,039 --> 00:22:40,640
abstracted dns for these gateways

00:22:39,280 --> 00:22:43,039
to more represent the product that

00:22:40,640 --> 00:22:45,200
you're offering

00:22:43,039 --> 00:22:46,480
um so hand over across to talk a little

00:22:45,200 --> 00:22:47,679
bit more about how we're doing some of

00:22:46,480 --> 00:22:51,440
this

00:22:47,679 --> 00:22:56,000
uh routing and gateway management

00:22:51,440 --> 00:22:59,120
thanks nate all right so

00:22:56,000 --> 00:23:02,880
so from the service owner point of view

00:22:59,120 --> 00:23:06,159
exposing a service to the external user

00:23:02,880 --> 00:23:08,000
has the same security concerns if the

00:23:06,159 --> 00:23:10,640
if the intention is to expose the

00:23:08,000 --> 00:23:12,799
service within the mesh

00:23:10,640 --> 00:23:14,720
but to the remote clusters which are

00:23:12,799 --> 00:23:18,159
sitting across the public internet

00:23:14,720 --> 00:23:20,240
but essentially whether it so even the

00:23:18,159 --> 00:23:21,919
even the request is coming from within

00:23:20,240 --> 00:23:23,520
the mesh but it is coming through the

00:23:21,919 --> 00:23:26,640
public internet

00:23:23,520 --> 00:23:30,880
so in so

00:23:26,640 --> 00:23:34,159
with this point we

00:23:30,880 --> 00:23:38,000
in our model the service owner

00:23:34,159 --> 00:23:42,159
is supposed to just expose the service

00:23:38,000 --> 00:23:45,919
to the external world over the chosen

00:23:42,159 --> 00:23:48,720
gateway port and

00:23:45,919 --> 00:23:50,080
the user is supposed to configure the

00:23:48,720 --> 00:23:52,559
whatever security measures

00:23:50,080 --> 00:23:54,640
authentication and authorization

00:23:52,559 --> 00:23:55,760
he or she feels comfortable with to deal

00:23:54,640 --> 00:23:59,840
with the

00:23:55,760 --> 00:23:59,840
public internet uh security concerns

00:24:00,240 --> 00:24:07,520
and what we do programmatically is that

00:24:04,320 --> 00:24:09,440
the the the gateway hosts

00:24:07,520 --> 00:24:11,120
the apis which are exposed to the

00:24:09,440 --> 00:24:14,799
external world we auto

00:24:11,120 --> 00:24:17,919
automatically expose these apis

00:24:14,799 --> 00:24:19,520
within the mesh to be consumed from the

00:24:17,919 --> 00:24:22,400
remote clusters

00:24:19,520 --> 00:24:24,080
so what we do basically is that and this

00:24:22,400 --> 00:24:28,240
this one five four four three

00:24:24,080 --> 00:24:32,000
is a reserved port for the stu mt ls

00:24:28,240 --> 00:24:34,480
we we we discovered what the

00:24:32,000 --> 00:24:36,880
apis are exposed to the external world

00:24:34,480 --> 00:24:40,240
and we expose the same apis

00:24:36,880 --> 00:24:42,720
over htmtrs on this reserved port

00:24:40,240 --> 00:24:44,159
for the east-west traffic and

00:24:42,720 --> 00:24:47,360
additionally

00:24:44,159 --> 00:24:49,760
in addition to the mdls we apply

00:24:47,360 --> 00:24:52,000
all the uh authentication and

00:24:49,760 --> 00:24:55,039
authorization configurations

00:24:52,000 --> 00:24:56,720
which are uh which are which the

00:24:55,039 --> 00:24:58,400
user has configured for the external

00:24:56,720 --> 00:25:01,120
traffic we apply those on

00:24:58,400 --> 00:25:02,080
uh on this one five four four three port

00:25:01,120 --> 00:25:06,400
as well

00:25:02,080 --> 00:25:06,400
so it is like double secure

00:25:09,840 --> 00:25:14,159
in addition to that now this is a very

00:25:12,360 --> 00:25:17,039
important slide

00:25:14,159 --> 00:25:17,760
uh we create a local service entry as

00:25:17,039 --> 00:25:19,919
well

00:25:17,760 --> 00:25:21,840
and the point to focus here is that the

00:25:19,919 --> 00:25:24,799
host name in the

00:25:21,840 --> 00:25:27,039
the host name in the service for example

00:25:24,799 --> 00:25:30,320
here peer.example.com

00:25:27,039 --> 00:25:32,799
is same the same as the host which is

00:25:30,320 --> 00:25:36,799
exposed to be consumed by external world

00:25:32,799 --> 00:25:40,080
over 944 report what this means is that

00:25:36,799 --> 00:25:43,200
whether the client of the service

00:25:40,080 --> 00:25:45,200
is within this same cluster or in some

00:25:43,200 --> 00:25:48,159
other remote cluster

00:25:45,200 --> 00:25:49,279
or some external user all are consuming

00:25:48,159 --> 00:25:51,640
this service

00:25:49,279 --> 00:25:54,640
with the same abstracted api

00:25:51,640 --> 00:25:54,640
pay.example.com

00:25:54,840 --> 00:25:59,200
and the

00:25:56,960 --> 00:26:00,000
the remote instances and the localist

00:25:59,200 --> 00:26:02,480
instance

00:26:00,000 --> 00:26:05,360
are behind the same are part of the same

00:26:02,480 --> 00:26:05,360
load balancer pool

00:26:06,400 --> 00:26:11,360
so and and and just one more

00:26:09,840 --> 00:26:14,480
implementation detail here

00:26:11,360 --> 00:26:15,919
to achieve this the end point in this

00:26:14,480 --> 00:26:20,320
service entry

00:26:15,919 --> 00:26:20,320
is the local kubernetes cluster id

00:26:22,880 --> 00:26:27,919
and now this is how it will look uh if

00:26:26,000 --> 00:26:28,960
there are more than one clusters where

00:26:27,919 --> 00:26:31,520
this

00:26:28,960 --> 00:26:32,720
uh back-end service has instances

00:26:31,520 --> 00:26:35,679
running

00:26:32,720 --> 00:26:36,240
so this service entry in the local

00:26:35,679 --> 00:26:39,200
cluster

00:26:36,240 --> 00:26:41,039
has two endpoints the one is the cluster

00:26:39,200 --> 00:26:44,159
ip of the

00:26:41,039 --> 00:26:44,480
local service instance and this another

00:26:44,159 --> 00:26:47,600
one

00:26:44,480 --> 00:26:50,720
the in the green car this is uh this

00:26:47,600 --> 00:26:53,600
is the gateway ip address of the

00:26:50,720 --> 00:26:56,720
remote cluster where one more instance

00:26:53,600 --> 00:26:56,720
of the service is running

00:27:01,919 --> 00:27:08,080
and now here now we are going to see

00:27:05,679 --> 00:27:09,919
at run time when the traffic when the

00:27:08,080 --> 00:27:12,880
request flows from the client

00:27:09,919 --> 00:27:13,279
how it how how load balancing happens

00:27:12,880 --> 00:27:16,880
and

00:27:13,279 --> 00:27:20,080
how it reaches the destination endpoint

00:27:16,880 --> 00:27:22,960
destination service so

00:27:20,080 --> 00:27:24,720
first of all on when the client makes a

00:27:22,960 --> 00:27:25,360
request the client makes a request to

00:27:24,720 --> 00:27:29,200
the

00:27:25,360 --> 00:27:32,480
abstracted api paid.example.com

00:27:29,200 --> 00:27:35,520
whether it is for the local links so

00:27:32,480 --> 00:27:37,440
the client uh did not bother about where

00:27:35,520 --> 00:27:39,279
the service instance is running locally

00:27:37,440 --> 00:27:42,559
or globally or wherever it is

00:27:39,279 --> 00:27:45,279
so abstracted api is used by the clients

00:27:42,559 --> 00:27:46,240
this goes uh to the sidecar and in the

00:27:45,279 --> 00:27:49,279
sidecar

00:27:46,240 --> 00:27:53,440
we are using um a feature

00:27:49,279 --> 00:27:57,200
where uh the where the proxy hijacks the

00:27:53,440 --> 00:28:02,559
dns queries and it caches the dns

00:27:57,200 --> 00:28:04,960
and uh and and resolves the resolves the

00:28:02,559 --> 00:28:09,279
dns query

00:28:04,960 --> 00:28:09,279
with the virtual ip of the service entry

00:28:10,960 --> 00:28:17,039
and then after the dns query gets

00:28:14,840 --> 00:28:21,760
resolved

00:28:17,039 --> 00:28:24,720
then that request starts from the

00:28:21,760 --> 00:28:27,440
from the client container with the with

00:28:24,720 --> 00:28:31,039
the virtual id of the service entry

00:28:27,440 --> 00:28:32,720
and now the site in the sidecar

00:28:31,039 --> 00:28:34,080
matching with the virtual ip the

00:28:32,720 --> 00:28:37,600
listener is picked up

00:28:34,080 --> 00:28:40,399
the listener has um the endpoints to the

00:28:37,600 --> 00:28:42,159
local instance as well as to the remote

00:28:40,399 --> 00:28:44,080
instances

00:28:42,159 --> 00:28:45,600
and because of the locality aware

00:28:44,080 --> 00:28:49,120
routing

00:28:45,600 --> 00:28:51,039
in ideal case the local instances are

00:28:49,120 --> 00:28:54,640
picked up

00:28:51,039 --> 00:28:57,360
and if there is no local instance

00:28:54,640 --> 00:28:58,320
or the local instance is in failure mode

00:28:57,360 --> 00:29:00,799
then side cal

00:28:58,320 --> 00:29:02,799
will route the request to the remote

00:29:00,799 --> 00:29:05,840
gateways

00:29:02,799 --> 00:29:09,039
and when it reaches the gateway

00:29:05,840 --> 00:29:11,120
the request is over the htm tls

00:29:09,039 --> 00:29:14,159
plus additionally the gateway will

00:29:11,120 --> 00:29:16,559
authorize the request using the

00:29:14,159 --> 00:29:18,559
using the extra authentication and

00:29:16,559 --> 00:29:21,200
authorization configurations which

00:29:18,559 --> 00:29:21,919
the user service owner has configured

00:29:21,200 --> 00:29:25,279
for the

00:29:21,919 --> 00:29:29,039
external uh external world so

00:29:25,279 --> 00:29:32,080
here it can be

00:29:29,039 --> 00:29:33,520
any external oauth service which will

00:29:32,080 --> 00:29:36,080
authorize the request

00:29:33,520 --> 00:29:37,840
in addition to htmtls and if everything

00:29:36,080 --> 00:29:40,080
passes

00:29:37,840 --> 00:29:42,080
then the request is gets routed to the

00:29:40,080 --> 00:29:46,399
actual instance

00:29:42,080 --> 00:29:49,360
so just to summarize here um

00:29:46,399 --> 00:29:50,799
here we are using a abstracted api so

00:29:49,360 --> 00:29:53,120
there is no dependency

00:29:50,799 --> 00:29:55,279
directly on the backend service and the

00:29:53,120 --> 00:29:59,039
secondly

00:29:55,279 --> 00:30:00,399
the what api is the service owner wants

00:29:59,039 --> 00:30:03,520
to expose

00:30:00,399 --> 00:30:06,799
he has full control on

00:30:03,520 --> 00:30:09,039
on the expo on the exposure of the apis

00:30:06,799 --> 00:30:10,799
for example like whatever api he wants

00:30:09,039 --> 00:30:12,159
to expose to the

00:30:10,799 --> 00:30:13,919
consumed within the mesh or to the

00:30:12,159 --> 00:30:15,679
external world

00:30:13,919 --> 00:30:18,399
the service owner will expose this on

00:30:15,679 --> 00:30:21,600
the gateway it is not that

00:30:18,399 --> 00:30:22,880
the whole cluster is being washed upon

00:30:21,600 --> 00:30:26,159
by the remote

00:30:22,880 --> 00:30:28,799
remote clusters remote

00:30:26,159 --> 00:30:29,600
studies so because of the time

00:30:28,799 --> 00:30:31,760
constraint

00:30:29,600 --> 00:30:33,360
we could cover this much only though we

00:30:31,760 --> 00:30:37,279
are doing much more interesting

00:30:33,360 --> 00:30:40,000
stuff we are doing um

00:30:37,279 --> 00:30:41,360
multi-tier we are handling multi-tier

00:30:40,000 --> 00:30:44,480
architectures as well

00:30:41,360 --> 00:30:48,240
we are handling where the clusters

00:30:44,480 --> 00:30:50,559
are in the um the clusters are in

00:30:48,240 --> 00:30:52,720
are part of the different vpcs which are

00:30:50,559 --> 00:30:56,240
not directly connected but talking

00:30:52,720 --> 00:30:58,080
through a shared vpcs so we would really

00:30:56,240 --> 00:30:59,919
like to explain all these things but

00:30:58,080 --> 00:31:01,919
because of time constraints we cannot

00:30:59,919 --> 00:31:04,159
talk over this so

00:31:01,919 --> 00:31:05,519
we can take these things offline if you

00:31:04,159 --> 00:31:07,120
are interested in knowing more about

00:31:05,519 --> 00:31:10,480
this

00:31:07,120 --> 00:31:13,919
and i think that's pretty much all

00:31:10,480 --> 00:31:13,919

YouTube URL: https://www.youtube.com/watch?v=0s0tmDpnm5w


