Title: LTD20-101 ML and AI at the edge with Google Coral, EdgeTPU and Cloud Platform
Publication date: 2020-03-25
Playlist: Linaro Tech Days 2020
Description: 
	Description:
Join this session to see how you can run ML models on edge devices. Including - creating Tensorflow Lite models in the cloud or on the device, running inference on devices with Coral ARM platform and the Google EdgeTPU ML accelerator, and thoughts on managing the edge ML model lifecycle

Session Speakers
Markku Lepisto - Head of IoT/ML Solutions, APAC, Google Cloud (Google Cloud)

Markku is a Cloud Solutions Architect at Google. He has worked in cloud computing for the past 10 years. Before joining Google, Markku led the cloud architecture of Nokia Siemens Networks’ global business units, and covered APAC as a technology evangelist at Amazon Web Services. He works out of Tokyo, and helps developers and customers across APAC understand how to best use and architect solutions for Google Cloud. Markku is the IoT/ML solutions lead for APAC for Google Cloud.



Tamas Daranyi - Product Manager, Google

Tamas is a product manager for Google's Cloud AI and IoT products with a special interest on edge solutions. 



Find the presentation for this session on connect.linaro.org:
https://connect.linaro.org/resources/ltd20/ltd20-101/
Captions: 
	00:00:03,610 --> 00:00:10,150
so let me get going without any further

00:00:07,420 --> 00:00:12,870
ado so first of all Thank You Leonardo

00:00:10,150 --> 00:00:15,250
for organizing this and inviting us and

00:00:12,870 --> 00:00:17,460
I hope that everybody is enjoying the

00:00:15,250 --> 00:00:20,079
day and I hope that everybody is safe I

00:00:17,460 --> 00:00:22,119
think this is a wonderful way of still

00:00:20,079 --> 00:00:24,759
continuing to do this to have some fun

00:00:22,119 --> 00:00:28,150
together and geek out in technology even

00:00:24,759 --> 00:00:30,669
during these challenging times so my

00:00:28,150 --> 00:00:32,800
name is Marco that's my face there

00:00:30,669 --> 00:00:35,170
I'm a Solutions Architect at Google

00:00:32,800 --> 00:00:38,519
cloud I focus in IOT and machine

00:00:35,170 --> 00:00:41,199
learning and AI I'm based in Tokyo and

00:00:38,519 --> 00:00:46,030
today I'd like to show you about machine

00:00:41,199 --> 00:00:47,440
learning and AI at the ADEs if you're

00:00:46,030 --> 00:00:49,059
wondering about the accent you're

00:00:47,440 --> 00:00:51,129
hearing it's because I'm originally from

00:00:49,059 --> 00:00:53,230
Finland even though I know now in Tokyo

00:00:51,129 --> 00:00:56,559
and the accent is called rally English

00:00:53,230 --> 00:00:58,269
so there's some historical backgrounds

00:00:56,559 --> 00:01:00,940
there if people got familiar with the

00:00:58,269 --> 00:01:02,999
accent with some rarely legends that's

00:01:00,940 --> 00:01:06,400
not me though but some of my compatriots

00:01:02,999 --> 00:01:08,920
now at Google cloud so I've been in

00:01:06,400 --> 00:01:12,250
cloud computing about more than 10 years

00:01:08,920 --> 00:01:14,350
now and I'm at Google cloud now my role

00:01:12,250 --> 00:01:17,670
here is a Solutions Architect so I'll do

00:01:14,350 --> 00:01:20,050
my best to help our customers and

00:01:17,670 --> 00:01:21,460
communities anybody who's interested in

00:01:20,050 --> 00:01:23,530
building something useful and

00:01:21,460 --> 00:01:26,290
interesting using Google services

00:01:23,530 --> 00:01:28,150
open-source and other products putting

00:01:26,290 --> 00:01:31,870
them together in a hopefully meaningful

00:01:28,150 --> 00:01:34,530
meaningful way but to Kay today I would

00:01:31,870 --> 00:01:36,880
like to talk to you about AI and

00:01:34,530 --> 00:01:39,010
specifically how we could leverage AI

00:01:36,880 --> 00:01:43,120
technologies at the eighth's meaning

00:01:39,010 --> 00:01:44,950
with physical world devices and before

00:01:43,120 --> 00:01:46,120
we start deep diving into that I'd like

00:01:44,950 --> 00:01:50,040
to show you a couple of things about

00:01:46,120 --> 00:01:53,800
google's AI principle so sometime ago

00:01:50,040 --> 00:01:56,620
Google published these AR principles so

00:01:53,800 --> 00:01:59,260
whatever we do as Google related to AI

00:01:56,620 --> 00:02:00,880
we will follow these principles and we

00:01:59,260 --> 00:02:04,690
are hoping that also other people and

00:02:00,880 --> 00:02:06,100
organizations follow similar principles

00:02:04,690 --> 00:02:09,459
because we if you believe these are

00:02:06,100 --> 00:02:11,410
useful going forward so they need to be

00:02:09,459 --> 00:02:14,140
socially beneficial whatever you do with

00:02:11,410 --> 00:02:17,170
AI you should not have any unfair bias

00:02:14,140 --> 00:02:17,440
that's not that take some attention so

00:02:17,170 --> 00:02:20,260
when

00:02:17,440 --> 00:02:22,600
use dates up when you have raw dates and

00:02:20,260 --> 00:02:25,510
then you want to use AI to get value out

00:02:22,600 --> 00:02:29,740
of the data excuse me even though a is

00:02:25,510 --> 00:02:32,290
often a black box you need to understand

00:02:29,740 --> 00:02:34,270
why the AI model is doing whatever it's

00:02:32,290 --> 00:02:36,400
doing why it's giving certain results

00:02:34,270 --> 00:02:40,330
it's very important so that there's

00:02:36,400 --> 00:02:42,070
fairness equality etcetera if those if

00:02:40,330 --> 00:02:45,130
and when those results are used for any

00:02:42,070 --> 00:02:47,020
decision-making of course they are CP

00:02:45,130 --> 00:02:49,870
used with safety in mind and

00:02:47,020 --> 00:02:53,950
accountability people's privacy should

00:02:49,870 --> 00:02:55,510
be should be honored of course it's good

00:02:53,950 --> 00:02:57,580
if we have high standards of scientific

00:02:55,510 --> 00:03:00,550
excellence and of course if Google and

00:02:57,580 --> 00:03:02,500
our communities at lots can also help

00:03:00,550 --> 00:03:04,690
improve and raise this scientific

00:03:02,500 --> 00:03:07,360
excellence and then it should be

00:03:04,690 --> 00:03:10,590
available for everyone so as much as

00:03:07,360 --> 00:03:14,440
possible AI should be used then to

00:03:10,590 --> 00:03:16,120
democratize extracting value from data

00:03:14,440 --> 00:03:17,800
using these new programming

00:03:16,120 --> 00:03:21,730
methodologies that are called machine

00:03:17,800 --> 00:03:24,310
learning and AI so what is AI well its

00:03:21,730 --> 00:03:26,709
many different things but at its core AI

00:03:24,310 --> 00:03:29,709
means that you don't anymore program the

00:03:26,709 --> 00:03:32,739
software with classical programming

00:03:29,709 --> 00:03:36,280
logic classical logic being if my data

00:03:32,739 --> 00:03:38,080
looks like this then do that else do

00:03:36,280 --> 00:03:39,940
this or you have a case statement that

00:03:38,080 --> 00:03:42,880
doesn't really work anymore bits

00:03:39,940 --> 00:03:45,130
massive amounts of data with very

00:03:42,880 --> 00:03:47,860
unstructured data like video streams and

00:03:45,130 --> 00:03:51,250
images and etc so it's very difficult

00:03:47,860 --> 00:03:54,370
for humans to anymore program this kind

00:03:51,250 --> 00:03:57,090
of procedural logic to process that kind

00:03:54,370 --> 00:03:59,980
of data machine learning and AI is

00:03:57,090 --> 00:04:02,290
better for that kind of random data if

00:03:59,980 --> 00:04:04,600
you will because they're the computers

00:04:02,290 --> 00:04:08,350
are learning in a similar manner as

00:04:04,600 --> 00:04:11,260
humans learn the AI models or neural

00:04:08,350 --> 00:04:13,330
networks are exposed to data they are

00:04:11,260 --> 00:04:15,910
exposed to training images they expose

00:04:13,330 --> 00:04:18,609
the training data and then the neural

00:04:15,910 --> 00:04:20,739
network basically enforces any patterns

00:04:18,609 --> 00:04:24,010
that it finds in the data and quite

00:04:20,739 --> 00:04:26,890
often the neural network can actually

00:04:24,010 --> 00:04:28,690
see patterns in the data that humans

00:04:26,890 --> 00:04:30,849
just cannot see we cannot see the

00:04:28,690 --> 00:04:34,689
patterns in this randomness of

00:04:30,849 --> 00:04:37,539
binary numbers etc but at its core AI

00:04:34,689 --> 00:04:40,029
means that you train the neural network

00:04:37,539 --> 00:04:42,009
with example data so these are pictures

00:04:40,029 --> 00:04:45,219
of cats these are pictures of dogs etc

00:04:42,009 --> 00:04:48,669
or these are pictures of puppies and

00:04:45,219 --> 00:04:50,619
these are pictures of muffins so after a

00:04:48,669 --> 00:04:52,419
while when you give hundreds then

00:04:50,619 --> 00:04:55,979
thousands maybe even millions of

00:04:52,419 --> 00:05:00,270
examples the neural network will enforce

00:04:55,979 --> 00:05:02,709
the learnings and it will then learn to

00:05:00,270 --> 00:05:05,349
distinguish or in this case classify

00:05:02,709 --> 00:05:08,740
between pictures of puppies and pictures

00:05:05,349 --> 00:05:10,979
of muffins and you don't need to figure

00:05:08,740 --> 00:05:13,479
out how to program the model to do that

00:05:10,979 --> 00:05:17,339
it gets even more difficult when you

00:05:13,479 --> 00:05:21,039
start to compare some duckies and mops

00:05:17,339 --> 00:05:24,069
nowadays it is definitely possible to

00:05:21,039 --> 00:05:26,709
train neural networks and AI models that

00:05:24,069 --> 00:05:28,749
are significantly faster than humans of

00:05:26,709 --> 00:05:31,959
course at classifying or detecting

00:05:28,749 --> 00:05:36,069
objects in the data and also the

00:05:31,959 --> 00:05:39,309
accuracy is significantly higher at the

00:05:36,069 --> 00:05:41,919
end of the day we have data scientists

00:05:39,309 --> 00:05:44,800
we have people with PhDs in statistical

00:05:41,919 --> 00:05:47,919
analysis I'm not one of those so we have

00:05:44,800 --> 00:05:50,039
the experts who can build machine

00:05:47,919 --> 00:05:52,629
learning models by hand the hard way

00:05:50,039 --> 00:05:54,509
using frameworks like tennis offload at

00:05:52,629 --> 00:05:58,269
Google open sourced and many other ones

00:05:54,509 --> 00:06:00,789
however we believe that as technology

00:05:58,269 --> 00:06:03,729
and the software ecosystem and hardware

00:06:00,789 --> 00:06:05,589
everything advances you don't

00:06:03,729 --> 00:06:07,929
necessarily need to know how a microwave

00:06:05,589 --> 00:06:09,639
oven works you just need to put your

00:06:07,929 --> 00:06:11,979
food in the oven and press a button and

00:06:09,639 --> 00:06:15,279
trust that the oven is heating up your

00:06:11,979 --> 00:06:19,329
food are not exploding it so going

00:06:15,279 --> 00:06:21,789
forward for most people concerned you

00:06:19,329 --> 00:06:24,969
don't necessarily want to know how to

00:06:21,789 --> 00:06:27,909
create a neural network you just want to

00:06:24,969 --> 00:06:30,009
enjoy having neural networks that are

00:06:27,909 --> 00:06:34,029
helping with your data and giving you

00:06:30,009 --> 00:06:36,819
reasonable responses so if you will the

00:06:34,029 --> 00:06:38,889
trend is towards moving to give me

00:06:36,819 --> 00:06:41,439
machine learning please by pressing a

00:06:38,889 --> 00:06:44,590
button if you will so machine learning

00:06:41,439 --> 00:06:47,590
as a service easy machine learning easy

00:06:44,590 --> 00:06:50,140
ái makes it much more accessible and

00:06:47,590 --> 00:06:52,090
also democratizes the creation of

00:06:50,140 --> 00:06:55,870
machine learning models which means that

00:06:52,090 --> 00:06:58,060
more people can use it one way Google

00:06:55,870 --> 00:06:59,530
helps to do this is with a choral

00:06:58,060 --> 00:07:02,140
platform and this is something we

00:06:59,530 --> 00:07:05,830
launched almost exactly one year ago at

00:07:02,140 --> 00:07:09,070
Google i/o conference last year so what

00:07:05,830 --> 00:07:10,990
is coral coral isn't is an open-source

00:07:09,070 --> 00:07:16,180
ecosystem by Google where we have

00:07:10,990 --> 00:07:18,700
hardware software and development

00:07:16,180 --> 00:07:21,370
ecosystems that help you develop machine

00:07:18,700 --> 00:07:26,830
learning models and AI and then run them

00:07:21,370 --> 00:07:30,610
on your hardware and why is running AI

00:07:26,830 --> 00:07:33,400
own hardware beneficial or necessary why

00:07:30,610 --> 00:07:35,590
do we need its AI when we have massive

00:07:33,400 --> 00:07:38,710
clouds like Google has we have massive

00:07:35,590 --> 00:07:40,930
data centers producing Google services

00:07:38,710 --> 00:07:42,490
and you can also enjoy using Google's

00:07:40,930 --> 00:07:44,290
data centers through Google cloud

00:07:42,490 --> 00:07:46,720
platform now of course that's very

00:07:44,290 --> 00:07:49,840
useful and that's very common but in

00:07:46,720 --> 00:07:52,720
some cases you will want to run AI on

00:07:49,840 --> 00:07:54,730
your hardware at the EDS there are many

00:07:52,720 --> 00:07:57,430
reasons to do that one of them is

00:07:54,730 --> 00:08:00,910
privacy depending on the type of data

00:07:57,430 --> 00:08:02,410
maybe some medical data etc or depending

00:08:00,910 --> 00:08:05,550
on your location where you are your

00:08:02,410 --> 00:08:09,580
legislation your regulatory compliance

00:08:05,550 --> 00:08:11,800
you may not always be able to upload or

00:08:09,580 --> 00:08:14,260
transfer your raw data up to the cloud

00:08:11,800 --> 00:08:16,390
because that means it may leave your

00:08:14,260 --> 00:08:19,420
country or even it may leave your

00:08:16,390 --> 00:08:21,310
premises so sometimes for privacy

00:08:19,420 --> 00:08:26,010
reasons you need to process the data

00:08:21,310 --> 00:08:29,050
with your own devices at your premise

00:08:26,010 --> 00:08:31,050
latency is another consideration even

00:08:29,050 --> 00:08:33,040
though the cloud is relatively near

00:08:31,050 --> 00:08:34,630
especially if the data centers are in

00:08:33,040 --> 00:08:36,670
the same country or nearby countries

00:08:34,630 --> 00:08:38,650
there are some milliseconds or some 10s

00:08:36,670 --> 00:08:40,810
of milliseconds to the cloud data

00:08:38,650 --> 00:08:42,760
centers and services there sometimes

00:08:40,810 --> 00:08:45,730
those tens of milliseconds maybe two

00:08:42,760 --> 00:08:48,310
months if for example use AI with

00:08:45,730 --> 00:08:50,290
self-driving cars or some traffic

00:08:48,310 --> 00:08:52,810
monitoring or some other situation where

00:08:50,290 --> 00:08:56,020
you really need to have millisecond

00:08:52,810 --> 00:08:57,970
response to anything that you detect

00:08:56,020 --> 00:08:59,949
especially if you

00:08:57,970 --> 00:09:02,829
and safety or other kind of safety

00:08:59,949 --> 00:09:04,959
implications are involved so if you

00:09:02,829 --> 00:09:07,060
consider a self-driving car that is

00:09:04,959 --> 00:09:13,509
driving 65 miles per hour which is about

00:09:07,060 --> 00:09:19,000
100 km/h 100 milliseconds is about 10

00:09:13,509 --> 00:09:22,779
meters 10 feet no 3 meters 1 foot is

00:09:19,000 --> 00:09:24,339
near 3 meters us measurements Imperial

00:09:22,779 --> 00:09:27,279
there but you get the idea

00:09:24,339 --> 00:09:29,829
the first that the object moves the more

00:09:27,279 --> 00:09:31,930
critical latency become so for example

00:09:29,829 --> 00:09:35,620
for self-driving cars and similar use

00:09:31,930 --> 00:09:38,829
cases you need to run the AR model on

00:09:35,620 --> 00:09:41,319
the EDS on the device another

00:09:38,829 --> 00:09:43,180
consideration is bandwidth more and more

00:09:41,319 --> 00:09:45,660
you have high resolution cameras with

00:09:43,180 --> 00:09:48,100
high resolution images even video stream

00:09:45,660 --> 00:09:50,589
transferring that video stream to

00:09:48,100 --> 00:09:53,110
another data center from a device can be

00:09:50,589 --> 00:09:54,879
very costly and you may you know that if

00:09:53,110 --> 00:09:58,120
you have a mobile device that connects a

00:09:54,879 --> 00:10:02,559
may be disconnected at some at times so

00:09:58,120 --> 00:10:04,779
you may not want to do this offline you

00:10:02,559 --> 00:10:06,819
may want to have the AR model still

00:10:04,779 --> 00:10:09,370
running even if you lose internet

00:10:06,819 --> 00:10:13,240
connections for products on inference

00:10:09,370 --> 00:10:14,709
and then of course cost so sometimes

00:10:13,240 --> 00:10:16,600
especially for the internet connection

00:10:14,709 --> 00:10:19,240
and large data volumes high resolution

00:10:16,600 --> 00:10:20,589
data it may be cheaper simply to run the

00:10:19,240 --> 00:10:22,540
machine learning inference on your

00:10:20,589 --> 00:10:26,230
device without any data transmission or

00:10:22,540 --> 00:10:29,680
storage costs so in order to actually

00:10:26,230 --> 00:10:32,470
accelerate running AI models on your

00:10:29,680 --> 00:10:35,379
devices Google has built some specific

00:10:32,470 --> 00:10:37,809
hardware you may have heard of TP use or

00:10:35,379 --> 00:10:39,639
Google thinness of processing units they

00:10:37,809 --> 00:10:41,559
are specialized chips that we are

00:10:39,639 --> 00:10:43,959
running in our data centers to run

00:10:41,559 --> 00:10:45,819
machine learning models in Google data

00:10:43,959 --> 00:10:49,209
centers both for Google services like

00:10:45,819 --> 00:10:51,939
Google photos YouTube etc basically all

00:10:49,209 --> 00:10:53,709
the Google services are running AI to

00:10:51,939 --> 00:10:56,410
improve the customer feeds as and make

00:10:53,709 --> 00:10:58,809
the service better and then you can also

00:10:56,410 --> 00:11:01,569
run AI bit Google services like Google

00:10:58,809 --> 00:11:03,850
photos if you upload your pictures to

00:11:01,569 --> 00:11:06,160
Google photos then you can do human

00:11:03,850 --> 00:11:08,559
language searches like show me a picture

00:11:06,160 --> 00:11:11,510
of myself and my wife or girlfriend at a

00:11:08,559 --> 00:11:14,840
beach in Malaysia

00:11:11,510 --> 00:11:18,320
then you can use AI to basically query

00:11:14,840 --> 00:11:20,780
for example your images now this kind of

00:11:18,320 --> 00:11:23,180
AI actions are then running on these

00:11:20,780 --> 00:11:25,310
lots DP use in the Google Data Centers

00:11:23,180 --> 00:11:26,570
now those are only in the data centers

00:11:25,310 --> 00:11:29,330
they are very big they are very

00:11:26,570 --> 00:11:31,370
expensive if they require cooling etc so

00:11:29,330 --> 00:11:33,560
that's not really useful with something

00:11:31,370 --> 00:11:36,740
like IOT devices that need to be small

00:11:33,560 --> 00:11:39,620
cheap low-power that's why the Google

00:11:36,740 --> 00:11:42,770
coral platform introduced the HDPE you

00:11:39,620 --> 00:11:45,770
so it's a small little tensorflow

00:11:42,770 --> 00:11:47,390
processing unit so it's a small ASIC

00:11:45,770 --> 00:11:49,960
that Google has designed and built

00:11:47,390 --> 00:11:53,380
specifically for running tensorflow

00:11:49,960 --> 00:11:55,910
machine learning models on your device

00:11:53,380 --> 00:11:57,770
the first version which is here of

00:11:55,910 --> 00:11:59,360
course we have a roadmap of next

00:11:57,770 --> 00:12:01,340
generation versions the first person

00:11:59,360 --> 00:12:03,770
that's commercially available now for

00:12:01,340 --> 00:12:06,560
anybody to try and use has 4 trillion

00:12:03,770 --> 00:12:09,110
operations per second at a maximum power

00:12:06,560 --> 00:12:10,850
utilization of 2 watts and it's running

00:12:09,110 --> 00:12:13,820
quantized machine learning models

00:12:10,850 --> 00:12:16,490
meaning that instead of floating point

00:12:13,820 --> 00:12:19,490
tensor flow models you quantize the

00:12:16,490 --> 00:12:22,490
model to integer integer calculations

00:12:19,490 --> 00:12:24,620
which means that it slider on for the

00:12:22,490 --> 00:12:29,150
small chip it's more efficient and it's

00:12:24,620 --> 00:12:31,550
faster and then you can embed this HDPE

00:12:29,150 --> 00:12:34,190
you on your hardware if you want and to

00:12:31,550 --> 00:12:36,260
start prototyping Google coral has some

00:12:34,190 --> 00:12:38,510
prototyping hardware that you can use

00:12:36,260 --> 00:12:40,610
for prototyping and developing these

00:12:38,510 --> 00:12:43,070
tensor flow models and running them on

00:12:40,610 --> 00:12:45,260
the HDPE you one of the hardware is here

00:12:43,070 --> 00:12:47,570
and today in my presentation I will be

00:12:45,260 --> 00:12:50,450
using this Google coral development

00:12:47,570 --> 00:12:53,390
board with some live demos where we run

00:12:50,450 --> 00:12:55,460
machine learning and AI on that HDPE

00:12:53,390 --> 00:12:58,970
chip using this port that you see on the

00:12:55,460 --> 00:13:01,460
screen it's here on my table now if

00:12:58,970 --> 00:13:03,710
you're interested in prototyping with

00:13:01,460 --> 00:13:06,410
Google coral you can order these

00:13:03,710 --> 00:13:08,480
development ports or PCI Express

00:13:06,410 --> 00:13:11,630
accelerators that have the same HDPE you

00:13:08,480 --> 00:13:14,990
inside this is a USB 3 stick version and

00:13:11,630 --> 00:13:16,880
then a system on a module the one on the

00:13:14,990 --> 00:13:19,760
left the development board is something

00:13:16,880 --> 00:13:21,710
similar to a Raspberry Pi 4 it's not a

00:13:19,760 --> 00:13:24,680
Raspberry Pi 4 but it's similar to that

00:13:21,710 --> 00:13:25,070
it's designed by Google but the hardware

00:13:24,680 --> 00:13:27,920
space

00:13:25,070 --> 00:13:32,960
so very similar to Raspberry Pi for it

00:13:27,920 --> 00:13:35,930
has an NXP IMX quad-core cortex a53 arm

00:13:32,960 --> 00:13:38,270
CPU it's running Debian it's running a

00:13:35,930 --> 00:13:41,410
derivative of Debian called Mendel OS

00:13:38,270 --> 00:13:43,730
and it has the Google it's TPU

00:13:41,410 --> 00:13:46,520
coprocessor for running machine learning

00:13:43,730 --> 00:13:48,320
models next to the CPU so the machine

00:13:46,520 --> 00:13:51,830
learning model we run and run at the TPU

00:13:48,320 --> 00:13:54,230
and the CPU the arm CPU runs the Debian

00:13:51,830 --> 00:13:56,060
operating system then there's a

00:13:54,230 --> 00:13:58,190
cryptographic processor from microchip

00:13:56,060 --> 00:14:00,710
that you can use to run any

00:13:58,190 --> 00:14:02,360
cryptographic operations and for example

00:14:00,710 --> 00:14:05,210
store your private keys for cloud

00:14:02,360 --> 00:14:09,260
connectivity in this secure trusted

00:14:05,210 --> 00:14:12,260
platform module it has the latest Wi-Fi

00:14:09,260 --> 00:14:14,780
connectivity Gigabit Ethernet it has the

00:14:12,260 --> 00:14:16,640
same 40 pin GPIO connector as a

00:14:14,780 --> 00:14:21,080
Raspberry Pi so you can easily attach

00:14:16,640 --> 00:14:24,050
some other peripherals their USB C HDMI

00:14:21,080 --> 00:14:26,270
etc so consider these ports to be a

00:14:24,050 --> 00:14:28,550
prototyping port if you want to start

00:14:26,270 --> 00:14:31,700
running machine on your models own DX

00:14:28,550 --> 00:14:34,670
using the HDPE accelerator the other

00:14:31,700 --> 00:14:37,760
option then is this USB stick so it has

00:14:34,670 --> 00:14:39,650
a USBC connector with USB 3 so you can

00:14:37,760 --> 00:14:41,930
plug it into any for example Linux

00:14:39,650 --> 00:14:44,690
device and simply start running machine

00:14:41,930 --> 00:14:48,350
any models on the HDPE you own this USB

00:14:44,690 --> 00:14:50,480
stick now if you want to try running

00:14:48,350 --> 00:14:54,950
machine learning models on your hardware

00:14:50,480 --> 00:14:58,250
using the HDPE you basically we have

00:14:54,950 --> 00:14:59,930
provided a software tool chain now

00:14:58,250 --> 00:15:03,140
there's a lot of services that Google

00:14:59,930 --> 00:15:05,120
cloud has to make it easy to develop

00:15:03,140 --> 00:15:08,390
machine learning models like Auto ml

00:15:05,120 --> 00:15:09,320
more about that soon or you can develop

00:15:08,390 --> 00:15:11,630
your tensorflow

00:15:09,320 --> 00:15:14,810
machine learning models by hand if you

00:15:11,630 --> 00:15:17,210
are data scientist then what this tool

00:15:14,810 --> 00:15:20,150
chain allows you to do is to go from

00:15:17,210 --> 00:15:22,850
your familiar tensorflow models and then

00:15:20,150 --> 00:15:25,820
translate or convert those to change the

00:15:22,850 --> 00:15:28,340
follow light tensorflow light is then

00:15:25,820 --> 00:15:30,670
running on this edge stable that is then

00:15:28,340 --> 00:15:33,080
the quantized slightly smaller model

00:15:30,670 --> 00:15:34,550
with similar accuracy as the

00:15:33,080 --> 00:15:37,190
floating-point full tensorflow

00:15:34,550 --> 00:15:38,960
model then

00:15:37,190 --> 00:15:41,960
you run this compiled to terms of low

00:15:38,960 --> 00:15:44,480
light model on the HDPE you a software

00:15:41,960 --> 00:15:47,690
stack then provides the compiler and

00:15:44,480 --> 00:15:50,570
then C++ API for communicating with the

00:15:47,690 --> 00:15:52,610
HDPE hardware running and managed by

00:15:50,570 --> 00:15:56,180
this Debian Linux operating system and

00:15:52,610 --> 00:15:59,720
then we have also api's an SDKs for C

00:15:56,180 --> 00:16:03,950
C++ and Python that all developers can

00:15:59,720 --> 00:16:06,740
easily then run applications imported

00:16:03,950 --> 00:16:09,380
the HDP libraries own for example their

00:16:06,740 --> 00:16:11,180
Python code and then very easily run

00:16:09,380 --> 00:16:13,690
their machine learning model inference

00:16:11,180 --> 00:16:15,730
against the HDP with their model and

00:16:13,690 --> 00:16:18,530
that's what I would like to show you now

00:16:15,730 --> 00:16:21,620
basically a sort example of how you can

00:16:18,530 --> 00:16:24,490
run your machine on a model on your

00:16:21,620 --> 00:16:29,030
device using the HDPE you and Python

00:16:24,490 --> 00:16:31,940
libraries in this case so first you

00:16:29,030 --> 00:16:34,700
initiate an engine so you create a

00:16:31,940 --> 00:16:40,070
detection engine object using our Python

00:16:34,700 --> 00:16:41,810
SDK and then for example if you want to

00:16:40,070 --> 00:16:43,760
do image classification an object

00:16:41,810 --> 00:16:46,940
detection you open an image just

00:16:43,760 --> 00:16:48,770
standard Python here and then with one

00:16:46,940 --> 00:16:51,460
line of code you can run inference so

00:16:48,770 --> 00:16:53,900
basically you call the engine object and

00:16:51,460 --> 00:16:56,380
they will have to it will have different

00:16:53,900 --> 00:17:00,110
methods for example one method will be

00:16:56,380 --> 00:17:02,540
detect something with image and then you

00:17:00,110 --> 00:17:05,930
call this method you give it your image

00:17:02,540 --> 00:17:07,760
image object as the raw data and then

00:17:05,930 --> 00:17:10,010
you tell earlier when you create the

00:17:07,760 --> 00:17:12,230
engine what is the model that you wanna

00:17:10,010 --> 00:17:14,150
run and what is the labels file the

00:17:12,230 --> 00:17:15,680
model is the tensorflow light model the

00:17:14,150 --> 00:17:18,410
machine learning neural network

00:17:15,680 --> 00:17:21,500
optimized for the HDP you and the labels

00:17:18,410 --> 00:17:23,870
file contains the labels that you're

00:17:21,500 --> 00:17:28,370
classifying like cats and dogs and mopes

00:17:23,870 --> 00:17:30,890
and bananas and oranges etc and then

00:17:28,370 --> 00:17:33,320
it's simply a one line of code that

00:17:30,890 --> 00:17:35,480
python client that is highlighted on the

00:17:33,320 --> 00:17:40,160
bottom here then communicates with the

00:17:35,480 --> 00:17:42,650
SDPO processor runs the model with that

00:17:40,160 --> 00:17:44,810
emits your raw image and gets back a

00:17:42,650 --> 00:17:47,390
response this is the machine learning

00:17:44,810 --> 00:17:50,510
inference response and then you simply

00:17:47,390 --> 00:17:50,960
in your Python code in this case you

00:17:50,510 --> 00:17:53,419
invest

00:17:50,960 --> 00:17:54,890
to get the response and for example if

00:17:53,419 --> 00:17:56,659
you're doing object detection in an

00:17:54,890 --> 00:17:58,429
image you will get the bounding box

00:17:56,659 --> 00:18:03,440
coordinates and then you can draw a box

00:17:58,429 --> 00:18:05,510
around the object that was detected so

00:18:03,440 --> 00:18:07,940
that was just a quick code example but

00:18:05,510 --> 00:18:10,010
now rather than boring you with slides

00:18:07,940 --> 00:18:12,169
and boring you with code examples I

00:18:10,010 --> 00:18:14,720
believe some of you may want to see what

00:18:12,169 --> 00:18:17,830
this actually looks like so why don't we

00:18:14,720 --> 00:18:19,730
now start using the Google coral

00:18:17,830 --> 00:18:21,740
development port that I have on the

00:18:19,730 --> 00:18:23,510
table here I have two cameras connected

00:18:21,740 --> 00:18:25,610
to it one of them is here next to my

00:18:23,510 --> 00:18:27,950
webcam there's another webcam connected

00:18:25,610 --> 00:18:30,950
to the board and we could run a model

00:18:27,950 --> 00:18:33,919
that is detecting persons in real time

00:18:30,950 --> 00:18:37,610
in this video image that'll be me and

00:18:33,919 --> 00:18:40,039
then applying privacy protection so for

00:18:37,610 --> 00:18:42,200
example obfuscating or protecting the

00:18:40,039 --> 00:18:44,809
identity of detected people so for

00:18:42,200 --> 00:18:47,929
example let's say that you have a venue

00:18:44,809 --> 00:18:49,850
and you have a maximum capacity of 70

00:18:47,929 --> 00:18:51,260
people just a throw number you would

00:18:49,850 --> 00:18:53,539
like to know how many people there are

00:18:51,260 --> 00:18:56,240
in this room so that you are within your

00:18:53,539 --> 00:18:59,149
safety and legal limits so you could for

00:18:56,240 --> 00:19:02,390
example have a camera monitoring taking

00:18:59,149 --> 00:19:04,520
live feed of this venue but then

00:19:02,390 --> 00:19:06,200
detecting people and counting how many

00:19:04,520 --> 00:19:08,750
people there are to know that you are

00:19:06,200 --> 00:19:11,690
within your safety and legal limitations

00:19:08,750 --> 00:19:13,940
but not exposing anybody's privacy not

00:19:11,690 --> 00:19:15,529
violated violating anyone's privacy so

00:19:13,940 --> 00:19:19,070
you could for example obfuscate the

00:19:15,529 --> 00:19:20,659
people remove their identity so let's

00:19:19,070 --> 00:19:23,330
see how we could do this using the

00:19:20,659 --> 00:19:25,970
precompiled PostNet model so PostNet is

00:19:23,330 --> 00:19:29,539
one of the open-source precompiled

00:19:25,970 --> 00:19:32,510
models so let's run that model on the

00:19:29,539 --> 00:19:35,240
HDP oh so you can see here on the screen

00:19:32,510 --> 00:19:38,360
that on my right hand corner here top

00:19:35,240 --> 00:19:40,399
right corner we have a shell and that is

00:19:38,360 --> 00:19:42,320
connected to the Google coral

00:19:40,399 --> 00:19:47,090
development port you can see that it's

00:19:42,320 --> 00:19:51,529
running debian 10 and now if I run the

00:19:47,090 --> 00:19:55,760
example application in Python we run the

00:19:51,529 --> 00:19:59,600
mobile net PostNet model and we execute

00:19:55,760 --> 00:20:01,940
it on the Google HDB you and this screen

00:19:59,600 --> 00:20:04,320
here is the HDMI output of the

00:20:01,940 --> 00:20:07,289
development port so let me star

00:20:04,320 --> 00:20:10,649
running this model in 720p resolution

00:20:07,289 --> 00:20:13,559
now and let's see what happens so now

00:20:10,649 --> 00:20:15,809
you should see me well you don't see me

00:20:13,559 --> 00:20:18,830
you see the obfuscated version of me and

00:20:15,809 --> 00:20:21,480
I do a very awkward engineer dance here

00:20:18,830 --> 00:20:25,620
you can see that the PostNet model is

00:20:21,480 --> 00:20:28,350
depicting a human that would be me but

00:20:25,620 --> 00:20:31,470
in real time the hdbo model is then

00:20:28,350 --> 00:20:33,600
sailing the application what are the

00:20:31,470 --> 00:20:35,730
boundaries of the human and then the

00:20:33,600 --> 00:20:38,279
Python application is simply painting

00:20:35,730 --> 00:20:39,960
over the human with red color red pixels

00:20:38,279 --> 00:20:42,450
in this case to protect the privacy of

00:20:39,960 --> 00:20:45,269
the person and on the top of the screen

00:20:42,450 --> 00:20:48,240
if I can put somewhere there you can see

00:20:45,269 --> 00:20:50,250
current occupancy one so currently the

00:20:48,240 --> 00:20:53,070
machine learning model is saying that in

00:20:50,250 --> 00:20:56,490
this room there's one person and we also

00:20:53,070 --> 00:20:58,259
able to see their pose if that's useful

00:20:56,490 --> 00:20:59,990
but the main point here would be

00:20:58,259 --> 00:21:02,039
counting the number of people and then

00:20:59,990 --> 00:21:04,169
protecting their privacy with

00:21:02,039 --> 00:21:06,779
obfuscation and that's happening in real

00:21:04,169 --> 00:21:09,299
time and the overall processing time

00:21:06,779 --> 00:21:11,370
here is now about 40 milliseconds and we

00:21:09,299 --> 00:21:15,179
are running about 25 frames per second

00:21:11,370 --> 00:21:21,750
and the row resolution of the camera is

00:21:15,179 --> 00:21:25,590
now 720p so 1280 x 720 ok that was the

00:21:21,750 --> 00:21:28,559
first demo let's go back to the slice so

00:21:25,590 --> 00:21:30,929
that was the Google coral it's TPU and

00:21:28,559 --> 00:21:33,720
development port running the post net

00:21:30,929 --> 00:21:36,059
model and that was a precompiled model

00:21:33,720 --> 00:21:38,490
now some of you may say well that's fine

00:21:36,059 --> 00:21:40,500
we have a lot of precompiled models how

00:21:38,490 --> 00:21:43,139
about custom models for my business

00:21:40,500 --> 00:21:45,659
problem I need a custom model yes I

00:21:43,139 --> 00:21:49,320
fully understand that's most likely the

00:21:45,659 --> 00:21:52,889
case and I personally I shouldn't have

00:21:49,320 --> 00:21:55,080
favorites but I kind of have a favorite

00:21:52,889 --> 00:21:58,529
in the Google cloud service portfolio

00:21:55,080 --> 00:22:02,370
and that's this one Auto ml because I'm

00:21:58,529 --> 00:22:04,620
not a data scientist I don't know how to

00:22:02,370 --> 00:22:06,629
create neural networks by hand it's been

00:22:04,620 --> 00:22:09,240
on my to-do list for a couple of years

00:22:06,629 --> 00:22:12,149
now but because of order and mail I've

00:22:09,240 --> 00:22:13,860
been a little bit lazy I don't have to

00:22:12,149 --> 00:22:17,460
know how to make a machine learning

00:22:13,860 --> 00:22:18,150
model I just use auto ml to create a

00:22:17,460 --> 00:22:20,760
machine learning

00:22:18,150 --> 00:22:23,970
for me so now I'd like to show you how

00:22:20,760 --> 00:22:27,510
also ml can help you democratize make it

00:22:23,970 --> 00:22:31,230
very easy very fast to create custom

00:22:27,510 --> 00:22:33,570
machine learning models with Auto ml the

00:22:31,230 --> 00:22:36,330
funny thing is that we have machine

00:22:33,570 --> 00:22:38,010
learning models creating machine

00:22:36,330 --> 00:22:40,680
learning models because we have come to

00:22:38,010 --> 00:22:43,500
the point now in technology that machine

00:22:40,680 --> 00:22:45,810
learning is better than humans at

00:22:43,500 --> 00:22:47,520
creating other machine learning so

00:22:45,810 --> 00:22:50,100
basically what happens is that with Auto

00:22:47,520 --> 00:22:52,320
ml you give the system your training

00:22:50,100 --> 00:22:54,150
data for example you give pictures of

00:22:52,320 --> 00:22:56,820
your puppies and your muffins you say

00:22:54,150 --> 00:23:00,390
these are images of puppies and these

00:22:56,820 --> 00:23:02,250
are images of muffins figure it out you

00:23:00,390 --> 00:23:05,700
press a button in order ml that's called

00:23:02,250 --> 00:23:07,980
train and what auto ml will does it is

00:23:05,700 --> 00:23:09,210
that on the left side of the screen it

00:23:07,980 --> 00:23:09,930
will create something called the

00:23:09,210 --> 00:23:12,020
metaluna

00:23:09,930 --> 00:23:16,140
it will create a machine learning model

00:23:12,020 --> 00:23:18,210
which goal is to create another machine

00:23:16,140 --> 00:23:20,570
learning model that is really good at

00:23:18,210 --> 00:23:23,940
classifying between puppies and muffins

00:23:20,570 --> 00:23:25,800
so what auto ml will do is it will try

00:23:23,940 --> 00:23:27,690
so many different things that humans

00:23:25,800 --> 00:23:29,940
cannot keep up with that it will try

00:23:27,690 --> 00:23:32,010
different models it will do the hyper

00:23:29,940 --> 00:23:36,090
parameter tuning and optimization and

00:23:32,010 --> 00:23:39,360
selecting and ultimately it will train

00:23:36,090 --> 00:23:42,120
and identify and select the best

00:23:39,360 --> 00:23:44,520
performing model and based on our

00:23:42,120 --> 00:23:48,140
testing the model quality will be better

00:23:44,520 --> 00:23:51,150
than that done by humans in most cases

00:23:48,140 --> 00:23:53,850
so let's see what how we how we could

00:23:51,150 --> 00:23:57,990
use auto ml so I live in Japan I'm a big

00:23:53,850 --> 00:24:00,300
fan of sushi but sometimes I forget what

00:23:57,990 --> 00:24:02,850
is this sushi but you see am I looking

00:24:00,300 --> 00:24:05,340
at so I was wondering could I use cloud

00:24:02,850 --> 00:24:06,900
or 2 ml to create a custom machine

00:24:05,340 --> 00:24:10,410
learning model to help me identify

00:24:06,900 --> 00:24:12,560
different different types of sushi so

00:24:10,410 --> 00:24:15,390
what I did was I bought some sushi

00:24:12,560 --> 00:24:18,270
bought some little toys you sees that

00:24:15,390 --> 00:24:20,280
don't go bad so quickly you can see them

00:24:18,270 --> 00:24:22,800
here on the screen this is the console

00:24:20,280 --> 00:24:25,220
of cloud or 2 ml I took pictures of

00:24:22,800 --> 00:24:28,050
these little Susie's with my phone I

00:24:25,220 --> 00:24:29,850
took something like 300 images of each

00:24:28,050 --> 00:24:31,860
of these Suzy's when you press the phone

00:24:29,850 --> 00:24:33,540
camera shutter button and you

00:24:31,860 --> 00:24:35,460
keep it down with most force it will

00:24:33,540 --> 00:24:38,540
just take endless pictures right so I

00:24:35,460 --> 00:24:41,100
took these endless images of my sushis I

00:24:38,540 --> 00:24:43,350
uploaded them to the service and I said

00:24:41,100 --> 00:24:45,510
that this folder is full of hippie

00:24:43,350 --> 00:24:47,760
nigiri sushi photos and this folder is

00:24:45,510 --> 00:24:50,970
full of ikana getting different types of

00:24:47,760 --> 00:24:52,679
sushi and then in Auto ml all you need

00:24:50,970 --> 00:24:55,890
to do is to press the button that says

00:24:52,679 --> 00:24:58,740
train you go for coffee or your you go

00:24:55,890 --> 00:25:00,299
for tea you come back and the service

00:24:58,740 --> 00:25:02,460
will create a custom machine learning

00:25:00,299 --> 00:25:04,830
model for you that is able to classify

00:25:02,460 --> 00:25:07,679
in this case between different types of

00:25:04,830 --> 00:25:10,559
Suzy your Suzy so a custom model for you

00:25:07,679 --> 00:25:13,530
and then how you can actually use this

00:25:10,559 --> 00:25:14,160
in the big picture is that you go from

00:25:13,530 --> 00:25:16,799
raw data

00:25:14,160 --> 00:25:19,590
everybody has raw data and most people

00:25:16,799 --> 00:25:22,290
have and most organizations have more

00:25:19,590 --> 00:25:24,150
data that they can handle now one way

00:25:22,290 --> 00:25:27,090
you can actually extract value from this

00:25:24,150 --> 00:25:30,540
data is with AI and machine learning but

00:25:27,090 --> 00:25:32,070
you probably need a custom model so then

00:25:30,540 --> 00:25:35,040
what you can do is you discover your

00:25:32,070 --> 00:25:38,130
data you dump it into cloud or 2 ml and

00:25:35,040 --> 00:25:39,540
then you classify it you say these are

00:25:38,130 --> 00:25:41,760
puppies these are muffins

00:25:39,540 --> 00:25:43,740
these are fraudulent credit card

00:25:41,760 --> 00:25:45,900
transactions these are good credit card

00:25:43,740 --> 00:25:49,530
transactions you get the idea you give

00:25:45,900 --> 00:25:52,230
it examples and then if necessary you

00:25:49,530 --> 00:25:54,270
can use also human labeling services if

00:25:52,230 --> 00:25:56,669
you have a lot of images for example

00:25:54,270 --> 00:25:59,820
that are not labeled they are services

00:25:56,669 --> 00:26:02,970
that you can scale with human workers or

00:25:59,820 --> 00:26:05,880
machine workers excuse me to label that

00:26:02,970 --> 00:26:08,580
data once you have a lot of examples of

00:26:05,880 --> 00:26:11,520
training data what is a lot well based

00:26:08,580 --> 00:26:13,650
on my personal testing and prototyping

00:26:11,520 --> 00:26:16,140
if you have a at least a hundred

00:26:13,650 --> 00:26:17,850
examples of something and a hundred

00:26:16,140 --> 00:26:20,940
examples of something else that's a good

00:26:17,850 --> 00:26:22,710
starting points a thousand examples of

00:26:20,940 --> 00:26:26,730
something and something else is even

00:26:22,710 --> 00:26:30,720
better so with machine learning training

00:26:26,730 --> 00:26:32,130
data more data is good better data is

00:26:30,720 --> 00:26:35,370
good so if you can have high quality

00:26:32,130 --> 00:26:38,580
data and a lot of high quality data

00:26:35,370 --> 00:26:41,880
the end result machine learning model is

00:26:38,580 --> 00:26:43,410
even better but then you don't need to

00:26:41,880 --> 00:26:45,270
know how to create a machine learning

00:26:43,410 --> 00:26:47,400
model how to create a

00:26:45,270 --> 00:26:49,050
neural network you let Otto Amell handle

00:26:47,400 --> 00:26:51,690
that it does the feature engineering

00:26:49,050 --> 00:26:54,320
model selection hyper parameter tuning

00:26:51,690 --> 00:26:56,850
and it will speed out your custom model

00:26:54,320 --> 00:27:00,780
you can do the testing in the cloud to

00:26:56,850 --> 00:27:02,640
verify that it looks okay or and or you

00:27:00,780 --> 00:27:05,790
can export the model and for example

00:27:02,640 --> 00:27:08,790
then run it on the Google coral hardware

00:27:05,790 --> 00:27:11,160
we have services like cloud IOT core if

00:27:08,790 --> 00:27:14,250
necessary for transmitting data easily

00:27:11,160 --> 00:27:16,730
between devices and the cloud using

00:27:14,250 --> 00:27:19,050
constraint and network connections

00:27:16,730 --> 00:27:20,670
ultimately you transfer this model file

00:27:19,050 --> 00:27:22,950
which is a couple of kilobytes a few

00:27:20,670 --> 00:27:25,500
kilobytes it's small to your coral

00:27:22,950 --> 00:27:27,510
device with the HDP you or any other

00:27:25,500 --> 00:27:29,520
server which has th deep you connect it

00:27:27,510 --> 00:27:30,960
then you run the model you run the

00:27:29,520 --> 00:27:34,290
machine learning inference at the edge

00:27:30,960 --> 00:27:36,630
and then what you can do is you collect

00:27:34,290 --> 00:27:39,180
the results and then you iterate just

00:27:36,630 --> 00:27:41,340
like with any software you do iterative

00:27:39,180 --> 00:27:43,890
agile development of your machine on

00:27:41,340 --> 00:27:45,740
your model so you can do retraining of

00:27:43,890 --> 00:27:49,020
the model learning from mistakes

00:27:45,740 --> 00:27:51,240
continuously improving the model anyway

00:27:49,020 --> 00:27:55,680
let's now see this demo of this SUSE

00:27:51,240 --> 00:27:57,030
model so let's see if auto ml can help

00:27:55,680 --> 00:27:59,190
us create a machine learning model that

00:27:57,030 --> 00:28:01,710
can classify between mice you see and

00:27:59,190 --> 00:28:05,040
then if we can run this model at the

00:28:01,710 --> 00:28:07,640
edge using Google HDPE or accelerator so

00:28:05,040 --> 00:28:10,260
let's go back to my machine here and

00:28:07,640 --> 00:28:13,590
what I would like to show you first is

00:28:10,260 --> 00:28:15,600
the cloud or two ml user interface so we

00:28:13,590 --> 00:28:19,170
are here in Google cloud platform the

00:28:15,600 --> 00:28:20,580
service is called Bisson and this one

00:28:19,170 --> 00:28:23,400
has a couple of options we have

00:28:20,580 --> 00:28:25,020
pre-built cloud vision API bits

00:28:23,400 --> 00:28:28,470
google has trained with millions of

00:28:25,020 --> 00:28:31,710
images to detect and classify standard

00:28:28,470 --> 00:28:33,570
objects like cars and cats but if you

00:28:31,710 --> 00:28:37,110
need to classify between your two

00:28:33,570 --> 00:28:39,000
different cats bopi and skippy then the

00:28:37,110 --> 00:28:41,430
standard model will not know this you

00:28:39,000 --> 00:28:43,470
will need to create a custom model or if

00:28:41,430 --> 00:28:45,570
you want to classify between your

00:28:43,470 --> 00:28:48,180
different Susi types you will need a

00:28:45,570 --> 00:28:52,710
custom model so this is what I have done

00:28:48,180 --> 00:28:54,780
you can see these examples of images I

00:28:52,710 --> 00:28:56,340
took of my Susi you can see that they

00:28:54,780 --> 00:28:58,140
have different labels I have something

00:28:56,340 --> 00:28:58,900
like six different seven different

00:28:58,140 --> 00:29:01,660
Suseela

00:28:58,900 --> 00:29:03,490
here then you can press train once you

00:29:01,660 --> 00:29:06,040
have the training data in the service

00:29:03,490 --> 00:29:08,410
you can click train new model go for

00:29:06,040 --> 00:29:10,720
coffee the system will send you an email

00:29:08,410 --> 00:29:13,780
when it's done creating the model and

00:29:10,720 --> 00:29:16,120
then you can evaluate the model now in

00:29:13,780 --> 00:29:18,610
this case my confusion matrix or model

00:29:16,120 --> 00:29:20,320
evaluation is pretty positive because at

00:29:18,610 --> 00:29:23,260
least based on the testing that the

00:29:20,320 --> 00:29:27,160
service has done it was correct in all

00:29:23,260 --> 00:29:29,140
cases of detecting so when the system

00:29:27,160 --> 00:29:33,670
itself finished creating the model auto

00:29:29,140 --> 00:29:35,950
ml tested the model against the portion

00:29:33,670 --> 00:29:37,630
of the training images so portion of the

00:29:35,950 --> 00:29:39,880
images that I gave to the service were

00:29:37,630 --> 00:29:41,830
used for training the model and a

00:29:39,880 --> 00:29:44,830
portion of the images were then used for

00:29:41,830 --> 00:29:47,290
testing for verifying is this juicy that

00:29:44,830 --> 00:29:48,100
is supposed to be a pinnacle a really a

00:29:47,290 --> 00:29:50,200
pinnacle

00:29:48,100 --> 00:29:52,090
so then these are the testing results

00:29:50,200 --> 00:29:53,920
and at least in this case the model was

00:29:52,090 --> 00:29:56,080
very successful because it didn't make

00:29:53,920 --> 00:29:58,420
any mistakes that depends on your data

00:29:56,080 --> 00:30:00,010
and there are ways to improve the model

00:29:58,420 --> 00:30:03,340
I will show you some of those later

00:30:00,010 --> 00:30:06,370
today anyway we have to model here and

00:30:03,340 --> 00:30:08,560
then what you can do is you can export

00:30:06,370 --> 00:30:10,300
the model you can run it in the cloud

00:30:08,560 --> 00:30:12,760
you can send your data to the cloud and

00:30:10,300 --> 00:30:14,560
just get the service result back but if

00:30:12,760 --> 00:30:17,320
you need to run the model on your device

00:30:14,560 --> 00:30:20,080
you can then export the model as a

00:30:17,320 --> 00:30:22,630
tensorflow light file or tensorflow

00:30:20,080 --> 00:30:25,420
javascript file which is easy to run on

00:30:22,630 --> 00:30:28,530
mobile applications or for example core

00:30:25,420 --> 00:30:30,970
ml or container which is very popular or

00:30:28,530 --> 00:30:33,610
simply you can ask the service to give

00:30:30,970 --> 00:30:35,440
you a tennis a floor light file that is

00:30:33,610 --> 00:30:38,680
optimized for running on the Google

00:30:35,440 --> 00:30:40,240
coral HD pu accelerator so then you can

00:30:38,680 --> 00:30:43,330
just export the file to your cloud

00:30:40,240 --> 00:30:46,300
storage and copy it from there to your

00:30:43,330 --> 00:30:48,910
hardware so this is what I have done so

00:30:46,300 --> 00:30:52,930
the next thing we could do is I could

00:30:48,910 --> 00:30:56,020
show you that on my on my machine here

00:30:52,930 --> 00:30:59,800
on my kokoro development board i have

00:30:56,020 --> 00:31:03,340
the SUSE HD PU model tears of low flight

00:30:59,800 --> 00:31:06,910
file on my choral development port so we

00:31:03,340 --> 00:31:08,770
could use this model and then run live

00:31:06,910 --> 00:31:11,230
inference so let's see what that looks

00:31:08,770 --> 00:31:12,429
like so I will start a Python

00:31:11,230 --> 00:31:14,649
application on the coral

00:31:12,429 --> 00:31:17,499
development port and as you can see here

00:31:14,649 --> 00:31:20,919
we will load this custom SUSE model and

00:31:17,499 --> 00:31:25,450
the sushi labels file and let's see what

00:31:20,919 --> 00:31:27,749
happens we go back to the HDMI output so

00:31:25,450 --> 00:31:30,700
now you can see the live picture of the

00:31:27,749 --> 00:31:33,519
coral development port camera feed and

00:31:30,700 --> 00:31:36,159
on the bottom right-hand side of the

00:31:33,519 --> 00:31:38,350
corner you can see the current detected

00:31:36,159 --> 00:31:40,960
label or classified label ikura

00:31:38,350 --> 00:31:44,950
gong-gong means equalize cell Monroe and

00:31:40,960 --> 00:31:47,289
Google is like battleship so the system

00:31:44,950 --> 00:31:49,330
or the model is currently correctly

00:31:47,289 --> 00:31:52,330
classifying that this is indeed ikura

00:31:49,330 --> 00:31:56,820
Gungan if i change it this is atomic

00:31:52,330 --> 00:31:58,860
onigiri now you can see that yes the

00:31:56,820 --> 00:32:00,750
machine learning model is telling us

00:31:58,860 --> 00:32:04,260
that that is in beta mechanically let's

00:32:00,750 --> 00:32:05,700
try a B now that's a B nakiri so look at

00:32:04,260 --> 00:32:08,670
the bottom right yes that's correct

00:32:05,700 --> 00:32:10,590
let's put tamago back there yes Tom

00:32:08,670 --> 00:32:13,590
akane Kira tomorrow is air can Nicky

00:32:10,590 --> 00:32:16,500
raise this shoe see on the bottom here

00:32:13,590 --> 00:32:18,420
and then please take a look at the

00:32:16,500 --> 00:32:21,240
bottom left side of the screen here

00:32:18,420 --> 00:32:23,520
the inference time is the milliseconds

00:32:21,240 --> 00:32:25,350
that the google correlates DPO was

00:32:23,520 --> 00:32:27,300
running inference so that is the machine

00:32:25,350 --> 00:32:29,550
learning inference time on th TPU so

00:32:27,300 --> 00:32:31,980
it's something like 4 milliseconds this

00:32:29,550 --> 00:32:34,320
is now running at 720p resolution and

00:32:31,980 --> 00:32:37,560
you can see that the frames per second

00:32:34,320 --> 00:32:40,590
is currently about 220 230 frames per

00:32:37,560 --> 00:32:44,250
second so definitely it's fast enough

00:32:40,590 --> 00:32:47,610
for real-time inference using the HDPE

00:32:44,250 --> 00:32:52,410
you with high resolution source video

00:32:47,610 --> 00:32:56,160
feed in this case okay so that was the

00:32:52,410 --> 00:32:58,830
sushi model and here we can see the auto

00:32:56,160 --> 00:33:02,070
ml console so it's this model that auto

00:32:58,830 --> 00:33:04,590
and well created for us all right let's

00:33:02,070 --> 00:33:06,990
move on so that was the second demo

00:33:04,590 --> 00:33:10,050
running at CPU custom model created by

00:33:06,990 --> 00:33:12,630
Auto ml now somebody will say yeah demos

00:33:10,050 --> 00:33:14,550
are cool technology demos smoke and

00:33:12,630 --> 00:33:17,430
mirrors forever we live in the real

00:33:14,550 --> 00:33:17,940
world how about the real world yes I

00:33:17,430 --> 00:33:21,630
hear you

00:33:17,940 --> 00:33:25,110
so what I'd like to show you next is an

00:33:21,630 --> 00:33:27,690
actual customer case so I was working in

00:33:25,110 --> 00:33:31,140
this customer case as a team member with

00:33:27,690 --> 00:33:33,270
Kyocera Kyocera is a company in Japan it

00:33:31,140 --> 00:33:35,300
the word Kyocera comes from Kyoto

00:33:33,270 --> 00:33:39,270
ceramics so it's a traditional company

00:33:35,300 --> 00:33:41,010
well-respected company in Japan focusing

00:33:39,270 --> 00:33:44,220
in ceramics they make for example nice

00:33:41,010 --> 00:33:47,130
ceramic knives and Kyocera are producing

00:33:44,220 --> 00:33:49,920
small and medium pets manufacturing for

00:33:47,130 --> 00:33:53,430
other companies and they wanted to see

00:33:49,920 --> 00:33:56,190
if Google technologies Google AI could

00:33:53,430 --> 00:33:58,020
help them with quality control on their

00:33:56,190 --> 00:34:01,110
manufacturing lines basically visually

00:33:58,020 --> 00:34:04,530
inspecting objects what you can see here

00:34:01,110 --> 00:34:07,740
on the right side oh it's an animated

00:34:04,530 --> 00:34:09,149
gif of the axial RAW images from the

00:34:07,740 --> 00:34:10,679
Kyocera production line

00:34:09,149 --> 00:34:11,760
so you can see here the

00:34:10,679 --> 00:34:13,589
these are the objects they're

00:34:11,760 --> 00:34:17,039
manufacturing so there's a camera on top

00:34:13,589 --> 00:34:19,619
of the production line the conveyor belt

00:34:17,039 --> 00:34:21,389
and the object in the middle is one of

00:34:19,619 --> 00:34:24,299
those objects they're manufacturing you

00:34:21,389 --> 00:34:26,490
can also see personally other objects on

00:34:24,299 --> 00:34:29,460
the same tray but what we are interested

00:34:26,490 --> 00:34:32,279
in doing here is detecting any faults

00:34:29,460 --> 00:34:35,609
any issues with the center object there

00:34:32,279 --> 00:34:37,799
and these are actually examples of

00:34:35,609 --> 00:34:39,869
broken objects we know they are broken

00:34:37,799 --> 00:34:42,539
they Kyocera told us that these are

00:34:39,869 --> 00:34:44,549
examples of broken Center object so you

00:34:42,539 --> 00:34:47,250
may see some little blips like black

00:34:44,549 --> 00:34:49,500
dots sometimes on this end object those

00:34:47,250 --> 00:34:53,460
are the faults the defects that we want

00:34:49,500 --> 00:34:54,450
to find with with AI now for machine

00:34:53,460 --> 00:34:57,329
learning there's a couple of challenges

00:34:54,450 --> 00:34:59,430
here the first challenge is that there's

00:34:57,329 --> 00:35:01,349
multiple objects visible we assume you

00:34:59,430 --> 00:35:03,569
should be focusing on the center object

00:35:01,349 --> 00:35:07,349
then the same two object is moving

00:35:03,569 --> 00:35:10,700
around and the defects are really small

00:35:07,349 --> 00:35:13,619
little blips in this moving object so

00:35:10,700 --> 00:35:15,900
there's a lot of confusing signals in

00:35:13,619 --> 00:35:17,760
these raw images for the neural network

00:35:15,900 --> 00:35:20,160
so the neural network doesn't know where

00:35:17,760 --> 00:35:22,589
to look so basically we have a problem

00:35:20,160 --> 00:35:25,770
of signal-to-noise ratio being wrong

00:35:22,589 --> 00:35:29,010
here the signal of the actual faults is

00:35:25,770 --> 00:35:30,960
very small but the signal of the object

00:35:29,010 --> 00:35:34,829
jumping around in the middle is very

00:35:30,960 --> 00:35:37,470
strong so as a result when I tried using

00:35:34,829 --> 00:35:39,589
auto ml first with these raw images that

00:35:37,470 --> 00:35:42,450
you saw in the previous Bates the

00:35:39,589 --> 00:35:44,369
worst-case quality worst case scenario

00:35:42,450 --> 00:35:47,910
of the machine learning model success

00:35:44,369 --> 00:35:50,369
was 14% so that's not good at all so

00:35:47,910 --> 00:35:52,440
definitely we needed to do something to

00:35:50,369 --> 00:35:55,349
get the model quality better than 14%

00:35:52,440 --> 00:35:58,020
it's not usable so what we tried to do

00:35:55,349 --> 00:36:00,839
was a bit of data pre-processing so

00:35:58,020 --> 00:36:03,240
before we give the data to cloud auto ml

00:36:00,839 --> 00:36:07,920
as training data we want it to clean up

00:36:03,240 --> 00:36:09,990
and help manipulate the training data so

00:36:07,920 --> 00:36:12,180
that the neural network can focus on

00:36:09,990 --> 00:36:16,200
finding the up the defects in the

00:36:12,180 --> 00:36:19,319
objects so the first step we did was to

00:36:16,200 --> 00:36:21,750
cut or crop the D row image and cut away

00:36:19,319 --> 00:36:24,570
if you will the exterior objects that

00:36:21,750 --> 00:36:26,280
are not relevant here by the way

00:36:24,570 --> 00:36:28,050
here in the highlight you can see one of

00:36:26,280 --> 00:36:30,330
those defects oh it's really small there

00:36:28,050 --> 00:36:33,360
so that's an example of a defect that we

00:36:30,330 --> 00:36:35,370
should find with the AI model the second

00:36:33,360 --> 00:36:37,620
step was that I wanted to remove the

00:36:35,370 --> 00:36:39,990
background pixels from the image I

00:36:37,620 --> 00:36:43,290
wanted to basically turn the background

00:36:39,990 --> 00:36:45,300
pixels black or remove them so that the

00:36:43,290 --> 00:36:47,580
only pixels remaining in the image

00:36:45,300 --> 00:36:50,280
belong to the object we are trying to

00:36:47,580 --> 00:36:53,220
detect so I applied some image filters

00:36:50,280 --> 00:36:55,590
and I got rid of the background next I

00:36:53,220 --> 00:36:58,470
applied another Python image filter that

00:36:55,590 --> 00:37:01,380
gave us a point cloud of the objects it

00:36:58,470 --> 00:37:05,160
sees but those are not lines those are

00:37:01,380 --> 00:37:07,650
just dots next step I applied something

00:37:05,160 --> 00:37:10,500
called Hough transform which is then

00:37:07,650 --> 00:37:12,990
looking at this point cloud of EDS in

00:37:10,500 --> 00:37:16,110
the object and detecting if they are

00:37:12,990 --> 00:37:19,440
continuous lines why do I do this

00:37:16,110 --> 00:37:22,290
because I want to know is the object

00:37:19,440 --> 00:37:23,970
rotate it in some angle and where is the

00:37:22,290 --> 00:37:26,520
objects or two things where is the

00:37:23,970 --> 00:37:28,680
object and is it rotated at an angle

00:37:26,520 --> 00:37:30,930
with this hope transform beginning at

00:37:28,680 --> 00:37:34,920
the lines and then I can calculate the

00:37:30,930 --> 00:37:38,100
the rotational angle of these lines and

00:37:34,920 --> 00:37:40,620
then I can simply counter rotate the

00:37:38,100 --> 00:37:43,680
object back so that those lines are

00:37:40,620 --> 00:37:45,840
horizontal and vertical zero degrees so

00:37:43,680 --> 00:37:49,070
we can use these detected object lines

00:37:45,840 --> 00:37:53,370
to rotate the object back horizontal

00:37:49,070 --> 00:37:56,520
then I run one more round of filtering

00:37:53,370 --> 00:38:00,240
getting the image 80s and then I can

00:37:56,520 --> 00:38:03,570
crop on the bounding box of the rotated

00:38:00,240 --> 00:38:06,690
found object as a result after this

00:38:03,570 --> 00:38:08,690
pre-processing this is what the training

00:38:06,690 --> 00:38:11,730
images look like we get rid of the

00:38:08,690 --> 00:38:14,420
confusing background the object is not

00:38:11,730 --> 00:38:17,640
jumping around anymore and it's

00:38:14,420 --> 00:38:19,650
stationary so now on the only changes

00:38:17,640 --> 00:38:22,620
between the images should be those

00:38:19,650 --> 00:38:25,320
defects and as a result when we then

00:38:22,620 --> 00:38:28,410
gave these images you can see a sample

00:38:25,320 --> 00:38:30,720
of these processed training images that

00:38:28,410 --> 00:38:34,560
we gave to order ml now the model

00:38:30,720 --> 00:38:38,099
quality was 95 and 98 percent successful

00:38:34,560 --> 00:38:41,369
at detecting good and broken objects

00:38:38,099 --> 00:38:45,089
and the HDP you performance test test

00:38:41,369 --> 00:38:47,489
was 8.9 milliseconds on average so 95

00:38:45,089 --> 00:38:50,640
minimum 95% accuracy at nine

00:38:47,489 --> 00:38:52,140
milliseconds latency so now with the

00:38:50,640 --> 00:38:53,970
remaining couple of minutes I would like

00:38:52,140 --> 00:38:57,660
to show your live demo of exactly this

00:38:53,970 --> 00:39:00,479
the Kyocera customer case so here on

00:38:57,660 --> 00:39:05,640
this bottom window here bottom right

00:39:00,479 --> 00:39:08,069
hand side corner so we can run the

00:39:05,640 --> 00:39:09,839
pre-processing first so I'd like to show

00:39:08,069 --> 00:39:11,489
you this is one of the raw images from

00:39:09,839 --> 00:39:12,989
the chaos of here's our products online

00:39:11,489 --> 00:39:15,660
you can see the object in the middle

00:39:12,989 --> 00:39:18,479
it's rotated at an angle there's a lot

00:39:15,660 --> 00:39:19,619
of confusing stuff on the outside so

00:39:18,479 --> 00:39:23,279
let's see how this pre-processing

00:39:19,619 --> 00:39:24,630
pipeline could help focus the dates are

00:39:23,279 --> 00:39:27,450
a little bit better so I run this

00:39:24,630 --> 00:39:30,509
pre-processing pipeline and you can see

00:39:27,450 --> 00:39:32,430
that we wrote an output file you can see

00:39:30,509 --> 00:39:34,170
the this was run in debug mode so you

00:39:32,430 --> 00:39:37,229
can see the interim steps here like

00:39:34,170 --> 00:39:40,739
removing the background doing the hoff

00:39:37,229 --> 00:39:44,849
transform finding the lines rotating the

00:39:40,739 --> 00:39:49,200
object and then finally this is the end

00:39:44,849 --> 00:39:52,380
result the focused rotated fixed object

00:39:49,200 --> 00:39:54,329
when we gave these two auto ml it gave

00:39:52,380 --> 00:39:59,729
us this high quality machine learning

00:39:54,329 --> 00:40:03,719
model and then on the coral board what

00:39:59,729 --> 00:40:06,059
we can do is that we can run a test so I

00:40:03,719 --> 00:40:10,319
have the model here created by Auto ml

00:40:06,059 --> 00:40:14,479
with these pre processed images you can

00:40:10,319 --> 00:40:17,579
see here STM is this Kyocera model and

00:40:14,479 --> 00:40:20,969
what I'd like to do now is to run a

00:40:17,579 --> 00:40:24,569
performance and quality test using this

00:40:20,969 --> 00:40:26,670
Kyocera Auto ml credit model against

00:40:24,569 --> 00:40:28,979
example images from the Kyocera

00:40:26,670 --> 00:40:31,170
production line let's first run this

00:40:28,979 --> 00:40:35,519
machine learning model against known to

00:40:31,170 --> 00:40:37,799
be good examples so all the images in

00:40:35,519 --> 00:40:39,989
this next folder that I run are known to

00:40:37,799 --> 00:40:41,579
be good so let's see what the machine

00:40:39,989 --> 00:40:44,069
learning model is telling us so now one

00:40:41,579 --> 00:40:46,440
by one we are processing these raw

00:40:44,069 --> 00:40:50,339
images you can see the image name on the

00:40:46,440 --> 00:40:51,690
left and then the second column here you

00:40:50,339 --> 00:40:54,390
can see okay

00:40:51,690 --> 00:40:56,880
is the machine learning inference result

00:40:54,390 --> 00:40:59,220
or classification result so it's saying

00:40:56,880 --> 00:41:02,760
in all these cases that this object is

00:40:59,220 --> 00:41:05,280
okay however I want to find a mistake

00:41:02,760 --> 00:41:07,020
here not so you can see the highlight

00:41:05,280 --> 00:41:09,600
but I highlight one line here in the

00:41:07,020 --> 00:41:10,740
output here we have one example where

00:41:09,600 --> 00:41:12,720
the machine learning model made a

00:41:10,740 --> 00:41:15,390
mistake it's it that this is a broken

00:41:12,720 --> 00:41:16,950
object when we no it's not because kids

00:41:15,390 --> 00:41:19,980
are not told us that these are examples

00:41:16,950 --> 00:41:22,470
of good objects however when we look at

00:41:19,980 --> 00:41:24,870
this mistake machine learning is not

00:41:22,470 --> 00:41:26,940
black and white it's not usually 100%

00:41:24,870 --> 00:41:28,530
than zero here we have a mistake but

00:41:26,940 --> 00:41:30,300
when you look at the next column that's

00:41:28,530 --> 00:41:32,610
the confidence with machine learning

00:41:30,300 --> 00:41:34,850
when you call inference you get a

00:41:32,610 --> 00:41:37,440
confidence you get a label for example

00:41:34,850 --> 00:41:40,380
not good in this case and then you get a

00:41:37,440 --> 00:41:42,780
mess in learning confidence here it's

00:41:40,380 --> 00:41:44,430
only 50% so it's very low confidence if

00:41:42,780 --> 00:41:48,060
you look at the other results you can

00:41:44,430 --> 00:41:49,800
see 84 percent 88 percent 83 percent so

00:41:48,060 --> 00:41:51,780
what you could do based on your testing

00:41:49,800 --> 00:41:54,630
of course on your situation is have a

00:41:51,780 --> 00:41:56,730
tress hold that I don't trust results

00:41:54,630 --> 00:41:58,650
that are lower than 80 percent for

00:41:56,730 --> 00:42:00,120
example what do you do with those

00:41:58,650 --> 00:42:02,700
results that are lower than 80 percent

00:42:00,120 --> 00:42:04,950
you put them aside you investigate them

00:42:02,700 --> 00:42:06,930
and you probably use them to train a new

00:42:04,950 --> 00:42:09,600
model until your model gets better and

00:42:06,930 --> 00:42:12,900
better so I just wanted to show you a

00:42:09,600 --> 00:42:16,170
mistake as well next very quickly let's

00:42:12,900 --> 00:42:19,610
run the same model against all the known

00:42:16,170 --> 00:42:22,320
to be bad components broken components

00:42:19,610 --> 00:42:24,480
so now you can see that in most cases

00:42:22,320 --> 00:42:27,060
the model is saying not good not good

00:42:24,480 --> 00:42:30,360
not good so that's correct they are not

00:42:27,060 --> 00:42:32,490
good but actually the last example was

00:42:30,360 --> 00:42:33,900
again a mistake the model said that yeah

00:42:32,490 --> 00:42:35,940
that's a good component when we know

00:42:33,900 --> 00:42:39,450
it's not but again the confidence was

00:42:35,940 --> 00:42:42,260
low at only 50 percent so that was an

00:42:39,450 --> 00:42:44,130
actual test of the Kyocera

00:42:42,260 --> 00:42:47,370
manufacturing visual inspection model

00:42:44,130 --> 00:42:48,840
against both good and bad objects and

00:42:47,370 --> 00:42:52,590
you can see that in most cases the

00:42:48,840 --> 00:42:56,220
machinery model was correct in 95 to 98%

00:42:52,590 --> 00:42:57,960
of the times there we go

00:42:56,220 --> 00:42:59,520
I really hope that

00:42:57,960 --> 00:43:02,580
and just to show you what we actually

00:42:59,520 --> 00:43:04,170
ran here we ran the model on this Google

00:43:02,580 --> 00:43:06,150
Chrome development board it's a

00:43:04,170 --> 00:43:10,050
prototyping port under this heat sink

00:43:06,150 --> 00:43:12,330
there is the hdbo processor we executed

00:43:10,050 --> 00:43:13,950
the pre processing with the CPU so for

00:43:12,330 --> 00:43:16,320
example if you run it on the port you

00:43:13,950 --> 00:43:20,070
can use the arm CPU to do the image

00:43:16,320 --> 00:43:24,210
pre-processing and then you can use this

00:43:20,070 --> 00:43:26,850
HDB you then to run the image that you

00:43:24,210 --> 00:43:29,820
want to classify you run the interest on

00:43:26,850 --> 00:43:32,100
the HDP you accelerator and you run the

00:43:29,820 --> 00:43:34,230
neural network inside the HDP processor

00:43:32,100 --> 00:43:36,450
and you get a classification result in

00:43:34,230 --> 00:43:39,090
this case in the demo that you saw just

00:43:36,450 --> 00:43:42,030
now you simply get a label ok or not

00:43:39,090 --> 00:43:45,120
good so that's what the machine learning

00:43:42,030 --> 00:43:49,160
model will tell you that's all I had

00:43:45,120 --> 00:43:51,960
today except one more final thing

00:43:49,160 --> 00:43:53,670
somebody will remind me again that the

00:43:51,960 --> 00:43:55,980
eye it's good but the model is not

00:43:53,670 --> 00:43:58,830
static so model is software that's right

00:43:55,980 --> 00:44:00,810
machine learning is also software just

00:43:58,830 --> 00:44:02,400
like any other software so you need to

00:44:00,810 --> 00:44:04,050
consider things like lifecycle

00:44:02,400 --> 00:44:06,990
management you need to consider

00:44:04,050 --> 00:44:08,550
deployment of your model at scale after

00:44:06,990 --> 00:44:10,200
prototyping and testing if you go to

00:44:08,550 --> 00:44:12,510
products on depending on your use case

00:44:10,200 --> 00:44:14,610
you may have hundreds thousands or even

00:44:12,510 --> 00:44:16,620
millions of devices out there in the

00:44:14,610 --> 00:44:18,840
field all running may be different

00:44:16,620 --> 00:44:20,250
versions of machine learning models so

00:44:18,840 --> 00:44:22,460
you need to know what is out there at

00:44:20,250 --> 00:44:24,780
the visibility and you need to have

00:44:22,460 --> 00:44:28,620
efficient and reliable deployment

00:44:24,780 --> 00:44:30,270
mechanisms at scale then to manage this

00:44:28,620 --> 00:44:32,910
fleet of devices and machine learning

00:44:30,270 --> 00:44:35,460
models so then you then you get into a

00:44:32,910 --> 00:44:37,860
topic called ml ups so that's one new

00:44:35,460 --> 00:44:41,790
thing in the world of IT now you need to

00:44:37,860 --> 00:44:43,920
have email apps basically having CI CD

00:44:41,790 --> 00:44:46,830
pipelines continuous integration and

00:44:43,920 --> 00:44:49,560
deployment at cell development both

00:44:46,830 --> 00:44:51,300
machine learning models collecting also

00:44:49,560 --> 00:44:53,670
the inference results from testing and

00:44:51,300 --> 00:44:56,100
even from production if you can using

00:44:53,670 --> 00:44:58,500
that data to train new models test the

00:44:56,100 --> 00:45:00,900
new models against old models replaced

00:44:58,500 --> 00:45:03,390
with the pay tomorrow and continuously

00:45:00,900 --> 00:45:07,320
iterate until your model is exceeding

00:45:03,390 --> 00:45:10,299
your business business goals and giving

00:45:07,320 --> 00:45:12,549
you the result that you need there

00:45:10,299 --> 00:45:14,469
go I really hope that you enjoyed the so

00:45:12,549 --> 00:45:16,539
that's my Twitter handle they're

00:45:14,469 --> 00:45:20,380
slightly too long due to my Finnish name

00:45:16,539 --> 00:45:22,869
rally English thank you very much okay

00:45:20,380 --> 00:45:26,140
Thank You Marco we actually have a

00:45:22,869 --> 00:45:28,299
question here in the chat so sure as

00:45:26,140 --> 00:45:30,400
we've got some time left

00:45:28,299 --> 00:45:32,859
the question goes is it possible to

00:45:30,400 --> 00:45:39,759
integrate google quarrel features in

00:45:32,859 --> 00:45:43,380
your project OS I haven't personally

00:45:39,759 --> 00:45:46,809
used Yocto so we have Mendel on the

00:45:43,380 --> 00:45:49,150
Google choral boards but however you can

00:45:46,809 --> 00:45:52,119
take the hdbo in different form factors

00:45:49,150 --> 00:45:55,809
like system module or PCI Express etc

00:45:52,119 --> 00:45:57,950
and integrate it to linux server

00:45:55,809 --> 00:46:00,650
absolutely it should be possible York

00:45:57,950 --> 00:46:03,559
though is just yet another Linux so the

00:46:00,650 --> 00:46:05,869
question is do we have precompiled

00:46:03,559 --> 00:46:08,240
packages you know Linux is package

00:46:05,869 --> 00:46:11,289
management right Debian has easy up get

00:46:08,240 --> 00:46:14,450
packets management whether we have Yocto

00:46:11,289 --> 00:46:16,430
packets ease with the SDKs already or

00:46:14,450 --> 00:46:18,529
not or whether we need to compile those

00:46:16,430 --> 00:46:19,970
from source code that I don't know our

00:46:18,529 --> 00:46:23,089
choral product line would know

00:46:19,970 --> 00:46:25,730
definitely it will work but we may need

00:46:23,089 --> 00:46:28,220
to do this binary compilation for York

00:46:25,730 --> 00:46:29,690
the first but if there's people who need

00:46:28,220 --> 00:46:32,119
that for York to please let us know you

00:46:29,690 --> 00:46:34,819
see my Twitter handle there my email is

00:46:32,119 --> 00:46:36,589
Marco at Google my first first name are

00:46:34,819 --> 00:46:38,329
quite google.com so please let me know

00:46:36,589 --> 00:46:42,710
and I can connect you to those people at

00:46:38,329 --> 00:46:44,779
Carlton okay and we've got a few more

00:46:42,710 --> 00:46:47,119
comments here so the next question is

00:46:44,779 --> 00:46:50,119
what are the plans for making the

00:46:47,119 --> 00:46:53,319
corrals bootloader ebbr compliant for

00:46:50,119 --> 00:46:53,319
booting other os's

00:46:54,279 --> 00:46:59,900
well the choral development port is a

00:46:57,440 --> 00:47:02,720
playground machine so it has the Mendel

00:46:59,900 --> 00:47:05,299
operating system it it comes from the

00:47:02,720 --> 00:47:06,920
Google product line so we maintain the

00:47:05,299 --> 00:47:09,349
operating system in mates we've built

00:47:06,920 --> 00:47:10,970
the bootloader the optimized mendel OS

00:47:09,349 --> 00:47:13,730
which already has all the packet sees

00:47:10,970 --> 00:47:16,609
and drivers for its DP etc so that's an

00:47:13,730 --> 00:47:19,460
easy way but if you want to customize

00:47:16,609 --> 00:47:21,380
things you can it is a standard Linux

00:47:19,460 --> 00:47:23,420
server running arm so all these things

00:47:21,380 --> 00:47:25,220
can be absolutely customized at the

00:47:23,420 --> 00:47:27,140
point of the core o development board is

00:47:25,220 --> 00:47:28,970
to give you a really easy prototyping

00:47:27,140 --> 00:47:30,319
platform but you can't just take the

00:47:28,970 --> 00:47:32,150
it's deep you in many different form

00:47:30,319 --> 00:47:34,940
factors and into credit on your hardware

00:47:32,150 --> 00:47:37,220
so that's that's actually likely going

00:47:34,940 --> 00:47:39,230
to be the products and methods so this

00:47:37,220 --> 00:47:40,670
development board is not probably going

00:47:39,230 --> 00:47:44,720
to be in products and it's more for

00:47:40,670 --> 00:47:46,099
testing a prototyping okay great and I

00:47:44,720 --> 00:47:46,849
think we've got time for one more

00:47:46,099 --> 00:47:49,700
question

00:47:46,849 --> 00:47:52,069
so are there things and Mendel that it

00:47:49,700 --> 00:47:55,670
would be useful to upstream into Debian

00:47:52,069 --> 00:47:57,770
or is that already happening that's a

00:47:55,670 --> 00:47:59,920
good question I don't know how the folks

00:47:57,770 --> 00:48:02,720
are going back and forth so Mendel is a

00:47:59,920 --> 00:48:04,130
derivative of Debian so Mendel is part

00:48:02,720 --> 00:48:06,500
of the Debian family you know there's

00:48:04,130 --> 00:48:08,240
mint and Ubuntu somebody may not like me

00:48:06,500 --> 00:48:09,230
saying that but those Debian is the

00:48:08,240 --> 00:48:11,480
father or mother

00:48:09,230 --> 00:48:15,609
of all those right so Mendel is a Debian

00:48:11,480 --> 00:48:18,410
based distribution whether the teams are

00:48:15,609 --> 00:48:19,760
contributing back to the main I don't

00:48:18,410 --> 00:48:22,000
know that's a good question but

00:48:19,760 --> 00:48:24,609
definitely it's an open source

00:48:22,000 --> 00:48:26,840
derivative of Debian but how the

00:48:24,609 --> 00:48:30,109
contributions go I personally haven't

00:48:26,840 --> 00:48:33,830
checked it okay and sorry one more

00:48:30,109 --> 00:48:36,560
question we've got two minutes so what's

00:48:33,830 --> 00:48:40,030
the FPS of the camera that coral can

00:48:36,560 --> 00:48:40,030
process process

00:48:40,180 --> 00:48:47,660
well a coral has its own camera it's

00:48:45,560 --> 00:48:49,490
similar to the Raspberry Pi camera it's

00:48:47,660 --> 00:48:51,020
got the similar kind of connector it's

00:48:49,490 --> 00:48:52,900
not the Raspberry Pi camera because it

00:48:51,020 --> 00:48:55,609
does order for goes to PI camera doesn't

00:48:52,900 --> 00:48:58,400
so in this case what I did personally is

00:48:55,609 --> 00:49:00,080
that I used two coral AI camera which is

00:48:58,400 --> 00:49:02,180
the small camera module that was for the

00:49:00,080 --> 00:49:05,390
Suzy demo and then I had a logic eight

00:49:02,180 --> 00:49:08,060
Logitech webcam connected over USB for

00:49:05,390 --> 00:49:11,630
the PostNet where you saw me so I had

00:49:08,060 --> 00:49:13,250
two different cameras there so basically

00:49:11,630 --> 00:49:15,619
it's just a matter of the camera module

00:49:13,250 --> 00:49:18,109
use the interface you use to talk to the

00:49:15,619 --> 00:49:20,500
camera and the bandwidth available there

00:49:18,109 --> 00:49:22,970
so there's no one answer for that but

00:49:20,500 --> 00:49:26,920
definitely I have seen like in this demo

00:49:22,970 --> 00:49:29,660
that you saw it was running 720p with

00:49:26,920 --> 00:49:32,000
raw video feed from the camera and it

00:49:29,660 --> 00:49:33,470
was running about 240 frames per second

00:49:32,000 --> 00:49:37,609
I don't know what is the maximum but

00:49:33,470 --> 00:49:39,920
it's very high hey great well thank you

00:49:37,609 --> 00:49:41,680
so much Michael that was really

00:49:39,920 --> 00:49:45,530
interesting and lots of good questions

00:49:41,680 --> 00:49:48,530
and as you said if anyone else has any

00:49:45,530 --> 00:49:53,030
other questions you saw Marcos Twitter

00:49:48,530 --> 00:49:55,190
handle and sorry you do you want to add

00:49:53,030 --> 00:49:56,750
your email address in the chat in case

00:49:55,190 --> 00:49:58,340
there are any other questions if you're

00:49:56,750 --> 00:50:06,140
happy to know I'm supposed to give me

00:49:58,340 --> 00:50:09,380
just a second okay oh yeah mark what

00:50:06,140 --> 00:50:12,440
google.com so just ping me and then I

00:50:09,380 --> 00:50:14,780
will be a bad router running missing

00:50:12,440 --> 00:50:15,530
sister coral team after that then I'll

00:50:14,780 --> 00:50:19,190
do my best

00:50:15,530 --> 00:50:21,560
great thank you so much Mike oh and yeah

00:50:19,190 --> 00:50:24,460
thank you thank you very much thank you

00:50:21,560 --> 00:50:24,460

YouTube URL: https://www.youtube.com/watch?v=ntynHzycZok


