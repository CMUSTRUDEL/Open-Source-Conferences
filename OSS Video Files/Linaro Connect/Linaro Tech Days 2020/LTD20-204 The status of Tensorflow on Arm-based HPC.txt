Title: LTD20-204 The status of Tensorflow on Arm-based HPC
Publication date: 2020-04-01
Playlist: Linaro Tech Days 2020
Description: 
	Description: 

A review of the availability for use of Tensorflow within the HPC environment

Session Speakers
Paul Isaac's
Director (LDCG, HPC-SIG) (Linaro Limited)

Technical Lead for HPC-SIG within LDCG.
 30+ years international infrastructure architecture experience from Smart NICs to HPC and software development. 



You can view this sessions presentation here:
https://connect.linaro.org/resources/ltd20/ltd20-204/
Captions: 
	00:00:00,490 --> 00:00:08,389
okay welcome to the status of tensorflow

00:00:04,250 --> 00:00:11,120
on on-base HPC now what we're going to

00:00:08,389 --> 00:00:14,119
do is not cover the detailed workflow

00:00:11,120 --> 00:00:16,369
what tensorflow is don't give you an

00:00:14,119 --> 00:00:18,619
overview of that but specifically how

00:00:16,369 --> 00:00:25,519
we're making progress in running

00:00:18,619 --> 00:00:29,929
tensorflow in a non based environment so

00:00:25,519 --> 00:00:33,920
we're also looking to encode tensorflow

00:00:29,929 --> 00:00:39,050
on AR 64 in such way that it is

00:00:33,920 --> 00:00:41,989
optimized for the various is a so

00:00:39,050 --> 00:00:47,180
whether it's whether your silicon is

00:00:41,989 --> 00:00:50,629
running 8.1 or 8.2 and potentially 8.3

00:00:47,180 --> 00:00:52,399
soon because some people maybe we want

00:00:50,629 --> 00:00:57,440
to ensure that each of the commands

00:00:52,399 --> 00:00:59,929
which are Hardware benefiting benefiting

00:00:57,440 --> 00:01:02,149
by the new hardware that they are

00:00:59,929 --> 00:01:04,790
incorporated into tensorflow

00:01:02,149 --> 00:01:07,310
optimizations but if other than just

00:01:04,790 --> 00:01:10,670
leave it at the library we're also going

00:01:07,310 --> 00:01:13,970
to see how we can use tensorflow

00:01:10,670 --> 00:01:17,570
in a real world application such as an

00:01:13,970 --> 00:01:20,420
artificial intelligence solution so then

00:01:17,570 --> 00:01:22,759
this is the work that is ongoing

00:01:20,420 --> 00:01:25,909
currently in a group called AI on

00:01:22,759 --> 00:01:29,780
servers and that's a collaboration with

00:01:25,909 --> 00:01:35,229
our other project members called AI /ml

00:01:29,780 --> 00:01:37,700
group as part of the overall the Linares

00:01:35,229 --> 00:01:41,390
project collaborative approach to

00:01:37,700 --> 00:01:45,409
artificial intelligence so currently

00:01:41,390 --> 00:01:46,009
tensorflow does compile and run on a 64

00:01:45,409 --> 00:01:48,680
hardware

00:01:46,009 --> 00:01:51,939
so copies sold okay we can go getting

00:01:48,680 --> 00:01:55,990
assaulted actually it's not quite right

00:01:51,939 --> 00:02:02,469
it does compile if you've got specific

00:01:55,990 --> 00:02:05,750
versions of libraries so the underlying

00:02:02,469 --> 00:02:07,540
dependencies is if they are there at the

00:02:05,750 --> 00:02:11,000
moment you start modifying those

00:02:07,540 --> 00:02:13,670
dependent dependencies then the specific

00:02:11,000 --> 00:02:18,110
version of tensorflow

00:02:13,670 --> 00:02:21,650
may or may not compile easily so what do

00:02:18,110 --> 00:02:23,690
I mean by that well listen let's just

00:02:21,650 --> 00:02:25,280
point out what tense employee is anyway

00:02:23,690 --> 00:02:28,910
I said I wasn't going to go into the

00:02:25,280 --> 00:02:31,520
absolute details but it's a method of

00:02:28,910 --> 00:02:34,880
creating from scratch machine learn

00:02:31,520 --> 00:02:38,450
models so you may hear about edge

00:02:34,880 --> 00:02:41,630
devices using machine learning models

00:02:38,450 --> 00:02:45,769
where do those models come from well

00:02:41,630 --> 00:02:49,130
they created in training sessions model

00:02:45,769 --> 00:02:51,410
creation sessions using frameworks such

00:02:49,130 --> 00:02:57,519
as tend to flow others could be

00:02:51,410 --> 00:03:01,790
Pytor dnl or maybe their hand coded but

00:02:57,519 --> 00:03:03,739
the difficulty is that you basically not

00:03:01,790 --> 00:03:06,080
just need to be a data scientist you

00:03:03,739 --> 00:03:11,540
also need to be a software engineer to

00:03:06,080 --> 00:03:14,450
encode the training of these models so

00:03:11,540 --> 00:03:16,489
it's quite complicated now we know

00:03:14,450 --> 00:03:21,019
Google is trying to address this with

00:03:16,489 --> 00:03:24,410
Google Cloud functionality and what

00:03:21,019 --> 00:03:28,209
they're doing is automating as much as

00:03:24,410 --> 00:03:31,900
possible the process of selecting data

00:03:28,209 --> 00:03:37,430
refining the data training the data and

00:03:31,900 --> 00:03:42,430
optimizing the resultant model so then

00:03:37,430 --> 00:03:45,709
that's fine when you're using generic

00:03:42,430 --> 00:03:48,530
models generic environments what happens

00:03:45,709 --> 00:03:51,350
if you've got a bespoke solution you

00:03:48,530 --> 00:03:55,760
need to encode well it's it's back you

00:03:51,350 --> 00:03:58,100
know back to hand coding either Python

00:03:55,760 --> 00:04:03,950
or C++ calling to the relevant libraries

00:03:58,100 --> 00:04:07,609
to do so now an extra tool which you may

00:04:03,950 --> 00:04:09,049
require is if you've got the data coming

00:04:07,609 --> 00:04:12,260
in to your data set

00:04:09,049 --> 00:04:15,160
not an ideal format then rather than

00:04:12,260 --> 00:04:18,769
hand coding every single line

00:04:15,160 --> 00:04:22,570
concatenation or string job is required

00:04:18,769 --> 00:04:27,520
then there's a tool called Kerris and

00:04:22,570 --> 00:04:32,480
caris can help to align the data

00:04:27,520 --> 00:04:35,800
into regular text formats which is more

00:04:32,480 --> 00:04:39,950
easily consumed by tensorflow

00:04:35,800 --> 00:04:42,920
now tensorflow generates models now

00:04:39,950 --> 00:04:47,740
those models could be several layers

00:04:42,920 --> 00:04:51,470
deep or include several neuron or nodes

00:04:47,740 --> 00:04:54,560
within each layer but how many do you

00:04:51,470 --> 00:04:57,700
actually pick is a randomly selected

00:04:54,560 --> 00:05:01,760
such as five layers with six nodes each

00:04:57,700 --> 00:05:07,000
and me maybe it's six thousand nodes

00:05:01,760 --> 00:05:09,920
with ten layers to try and figure out

00:05:07,000 --> 00:05:13,750
what would be the best approach isn't

00:05:09,920 --> 00:05:16,760
always easily done by hand and so

00:05:13,750 --> 00:05:18,950
looking at looking at ways that this

00:05:16,760 --> 00:05:22,250
could be done there is a we can follow

00:05:18,950 --> 00:05:25,010
Auto ml principles and there is a turn

00:05:22,250 --> 00:05:27,620
an application called caress tuna which

00:05:25,010 --> 00:05:30,110
is for high parameterization tuning and

00:05:27,620 --> 00:05:32,740
what that does it specifically loads

00:05:30,110 --> 00:05:35,090
into the brook layers or number of nodes

00:05:32,740 --> 00:05:38,780
within each layer that is required

00:05:35,090 --> 00:05:41,840
within given Rangers now doesn't that

00:05:38,780 --> 00:05:44,540
sounds great but all we've we started

00:05:41,840 --> 00:05:48,950
out with a title about 10th floor agency

00:05:44,540 --> 00:05:51,590
well why do we need to HPC well if you

00:05:48,950 --> 00:05:56,419
start to think it's the number of

00:05:51,590 --> 00:05:58,970
possibilities of layers nodes that could

00:05:56,419 --> 00:06:00,919
be part of a particular model plus the

00:05:58,970 --> 00:06:04,610
size of the data sets that you're likely

00:06:00,919 --> 00:06:07,300
to be crunching to actually define the

00:06:04,610 --> 00:06:09,950
model then you're going to need a

00:06:07,300 --> 00:06:13,400
high-performance computing system to be

00:06:09,950 --> 00:06:16,430
able to do that training so the number

00:06:13,400 --> 00:06:19,790
of iterations when you're tuning don't

00:06:16,430 --> 00:06:23,660
grow significantly so this is why we

00:06:19,790 --> 00:06:26,990
need tensorflow and the other frameworks

00:06:23,660 --> 00:06:30,050
to be available within the HPC

00:06:26,990 --> 00:06:33,890
environment now we know they are in the

00:06:30,050 --> 00:06:36,470
x86 but we're in an ARM based conference

00:06:33,890 --> 00:06:40,630
so we'd like them to also be first-class

00:06:36,470 --> 00:06:40,630
citizens in the AR at 64

00:06:41,400 --> 00:06:46,749
so the particular versions we've been

00:06:43,689 --> 00:06:51,789
looking at so the last diversion at

00:06:46,749 --> 00:06:53,620
version 1.1 515 was released in the

00:06:51,789 --> 00:06:56,680
middle of last year and there will not

00:06:53,620 --> 00:07:00,039
be any further version one being

00:06:56,680 --> 00:07:04,300
released and as of earlier this month

00:07:00,039 --> 00:07:06,610
version 2.2 has been released and we

00:07:04,300 --> 00:07:09,039
understand that there is some conversion

00:07:06,610 --> 00:07:13,270
issues going from version one to version

00:07:09,039 --> 00:07:15,939
two now as of a few days ago with the

00:07:13,270 --> 00:07:19,569
tensor flow depth conference there was

00:07:15,939 --> 00:07:21,639
an implication that - - will actually be

00:07:19,569 --> 00:07:25,749
a little bit more backward compatible to

00:07:21,639 --> 00:07:28,960
the existing core i5 libraries so we

00:07:25,749 --> 00:07:30,909
need to investigate how how more easily

00:07:28,960 --> 00:07:33,399
it allows you to do the translation

00:07:30,909 --> 00:07:36,039
between the old versions and the reason

00:07:33,399 --> 00:07:39,669
for doing so is at the end of this year

00:07:36,039 --> 00:07:44,830
Google will actually stop training jobs

00:07:39,669 --> 00:07:47,289
on version 1.15 in essence forcing you

00:07:44,830 --> 00:07:49,479
over to the version two which can be a

00:07:47,289 --> 00:07:52,449
good thing because moving over to

00:07:49,479 --> 00:07:55,749
whatever the latest and greatest is can

00:07:52,449 --> 00:07:57,399
offer benefits now we have to say there

00:07:55,749 --> 00:07:59,919
highlight that there has been a little

00:07:57,399 --> 00:08:02,080
bit of reluctance initially moving from

00:07:59,919 --> 00:08:05,649
version one to version two simply

00:08:02,080 --> 00:08:09,219
because of benchmarking now benchmarking

00:08:05,649 --> 00:08:11,379
if you want to sing the the the praises

00:08:09,219 --> 00:08:14,050
that the particular system we've got you

00:08:11,379 --> 00:08:16,779
want the best performing benchmark but

00:08:14,050 --> 00:08:21,189
apparently in version 1 the benchmarks

00:08:16,779 --> 00:08:23,169
work better in version than version 2 so

00:08:21,189 --> 00:08:24,909
you can understand the reluctance to not

00:08:23,169 --> 00:08:27,669
want to do the latest benchmarks

00:08:24,909 --> 00:08:28,990
highlighting version 2 performance this

00:08:27,669 --> 00:08:31,479
is something that we just need to

00:08:28,990 --> 00:08:33,459
encourage the transition over and where

00:08:31,479 --> 00:08:35,260
there are optimization issues then

00:08:33,459 --> 00:08:40,810
that's where we need to look in into

00:08:35,260 --> 00:08:44,610
them now I think we want to make arm 64

00:08:40,810 --> 00:08:47,050
the first-class citizen or tensorflow

00:08:44,610 --> 00:08:49,720
because if you actually look in the

00:08:47,050 --> 00:08:54,279
tense at community builds then you'll

00:08:49,720 --> 00:08:57,910
find quite surprising results

00:08:54,279 --> 00:09:02,249
so in the official barrels the only time

00:08:57,910 --> 00:09:04,839
on 64 is mentioned is as raspberry pi's

00:09:02,249 --> 00:09:07,980
we started out this conversation by

00:09:04,839 --> 00:09:10,930
saying let's put tons of low on HP see

00:09:07,980 --> 00:09:14,050
don't quite think of raspberry pi is up

00:09:10,930 --> 00:09:17,290
to the i/o characteristics or

00:09:14,050 --> 00:09:20,439
high-performance computing but there's

00:09:17,290 --> 00:09:24,399
there it's it's a server function I'll

00:09:20,439 --> 00:09:25,749
be it a very small device so we don't

00:09:24,399 --> 00:09:27,879
mind it being on there

00:09:25,749 --> 00:09:31,600
it's just that we'd also like it it was

00:09:27,879 --> 00:09:37,059
like thunder x2 to be on there maybe a

00:09:31,600 --> 00:09:40,720
64 FX on there as well so rather than

00:09:37,059 --> 00:09:43,959
just adjust the particular chips it's

00:09:40,720 --> 00:09:46,120
the entire systems over the last few a

00:09:43,959 --> 00:09:49,059
couple of weeks we've had announcements

00:09:46,120 --> 00:09:53,019
at thunder X 3 and then the impaired

00:09:49,059 --> 00:09:56,019
ultra so the are the ultra being 80 core

00:09:53,019 --> 00:09:59,439
in the thunder X leading 96 core with

00:09:56,019 --> 00:10:03,759
multi-threading so these are significant

00:09:59,439 --> 00:10:06,430
devices significant silicon that can be

00:10:03,759 --> 00:10:07,600
put in services and scaled to high

00:10:06,430 --> 00:10:11,850
performance computing

00:10:07,600 --> 00:10:14,829
levels so it makes sense that we try and

00:10:11,850 --> 00:10:16,779
apply some pressure tor nicely to Google

00:10:14,829 --> 00:10:20,680
to say come on can we have with the

00:10:16,779 --> 00:10:24,430
official build speeds fault intensive

00:10:20,680 --> 00:10:26,410
low on arm 64 now of course sometimes

00:10:24,430 --> 00:10:29,319
we're driven by community rather than

00:10:26,410 --> 00:10:32,620
just the official builds but if we look

00:10:29,319 --> 00:10:35,860
at the community built status then we

00:10:32,620 --> 00:10:40,420
see and these welcomed GPU we see

00:10:35,860 --> 00:10:44,949
peepees the PowerPC GPUs and CPUs and

00:10:40,420 --> 00:10:51,160
we've seen Red Hat Linux version 2.6 on

00:10:44,949 --> 00:10:53,319
C community commute in x86 because it's

00:10:51,160 --> 00:10:56,889
a little odd we see Red Hat just at the

00:10:53,319 --> 00:11:00,490
bottom there simply because if we just

00:10:56,889 --> 00:11:02,110
go back you know you'll notice that what

00:11:00,490 --> 00:11:02,949
you're not seeing it just says

00:11:02,110 --> 00:11:07,790
generically

00:11:02,949 --> 00:11:11,210
the next CPU GPU oh if you look at

00:11:07,790 --> 00:11:12,850
any of the recipes Google searches on

00:11:11,210 --> 00:11:15,320
how to build tents afloat

00:11:12,850 --> 00:11:20,240
more than likely you're going to find

00:11:15,320 --> 00:11:22,310
that the Debian builds specifically on

00:11:20,240 --> 00:11:24,140
the raspberry PI's within their debian

00:11:22,310 --> 00:11:26,320
builds what

00:11:24,140 --> 00:11:30,200
Lenora likes to do is encourage

00:11:26,320 --> 00:11:32,510
destroyed Gnostics well of course we

00:11:30,200 --> 00:11:35,360
accept Red Hat is it's very much a

00:11:32,510 --> 00:11:36,890
member of our own it's great news we

00:11:35,360 --> 00:11:39,770
want to make sure that what we develop

00:11:36,890 --> 00:11:43,010
applies to all major distributions as

00:11:39,770 --> 00:11:45,290
well so we like to encourage recipes

00:11:43,010 --> 00:11:49,760
which allow Debian RedHat

00:11:45,290 --> 00:11:53,780
soozee scenarios to be listed on these

00:11:49,760 --> 00:11:57,200
are the build status so let's see if

00:11:53,780 --> 00:12:02,810
through encouragement we can increase

00:11:57,200 --> 00:12:05,870
the number of items on its list but we

00:12:02,810 --> 00:12:08,650
are limited in what versions are

00:12:05,870 --> 00:12:12,410
required because 10 tensorflow

00:12:08,650 --> 00:12:16,370
requires the build system from Google

00:12:12,410 --> 00:12:20,480
called basil and basil has its own

00:12:16,370 --> 00:12:22,610
dependencies currently the latest

00:12:20,480 --> 00:12:25,460
shipping version is at least two point

00:12:22,610 --> 00:12:29,270
two zero however we are using something

00:12:25,460 --> 00:12:32,870
like zero point eight nine and a release

00:12:29,270 --> 00:12:37,940
level simply to try to get tensorflow to

00:12:32,870 --> 00:12:40,480
build a out 64 so a zero release you can

00:12:37,940 --> 00:12:43,190
imagine that there's going to be some

00:12:40,480 --> 00:12:46,430
difficulties or some features which are

00:12:43,190 --> 00:12:50,660
not carried through to the later to the

00:12:46,430 --> 00:12:53,840
latest solutions so we'd like to sort of

00:12:50,660 --> 00:12:57,710
work with the community so that we can

00:12:53,840 --> 00:13:02,300
fix all the issues between what between

00:12:57,710 --> 00:13:04,670
what is preventing basil to be loaded so

00:13:02,300 --> 00:13:07,130
we asked the community to get in contact

00:13:04,670 --> 00:13:08,960
with us and assist but in the tensorflow

00:13:07,130 --> 00:13:13,330
builds itself and the underlying

00:13:08,960 --> 00:13:13,330
dependency of basil basil

00:13:14,650 --> 00:13:18,500
so the aptly

00:13:16,310 --> 00:13:21,770
occasion of what we're looking for for

00:13:18,500 --> 00:13:26,180
tensorflow so we have a group called

00:13:21,770 --> 00:13:28,550
alien service within the HPC group and

00:13:26,180 --> 00:13:32,230
say we collaboratively go with the edge

00:13:28,550 --> 00:13:37,130
device team that are looking at AI /ml

00:13:32,230 --> 00:13:39,610
and we look at more than one framework

00:13:37,130 --> 00:13:42,410
so we have tensorflow and pi torch and

00:13:39,610 --> 00:13:45,589
we're also looking at P&L and the

00:13:42,410 --> 00:13:50,300
integration of with the above frameworks

00:13:45,589 --> 00:13:54,650
as well so we're focused on the data

00:13:50,300 --> 00:13:57,529
center director of the LD CG or data

00:13:54,650 --> 00:14:01,400
center account group and we'd like to

00:13:57,529 --> 00:14:04,910
see our high performing on 64 devices

00:14:01,400 --> 00:14:06,650
doing the training primarily they kind

00:14:04,910 --> 00:14:09,320
of costume in fencing but if you're out

00:14:06,650 --> 00:14:12,410
at the edge and you're unable to carry a

00:14:09,320 --> 00:14:14,630
to use a co chasity around with you and

00:14:12,410 --> 00:14:16,130
of course you want the edge devices the

00:14:14,630 --> 00:14:19,270
smaller device installation you do the

00:14:16,130 --> 00:14:22,550
influencing so we look at the

00:14:19,270 --> 00:14:26,560
reinforcement training to make sure that

00:14:22,550 --> 00:14:29,540
what is being modeled is effective and

00:14:26,560 --> 00:14:31,250
if it's not then we feedback into a

00:14:29,540 --> 00:14:36,190
feedback loop an iterative for you

00:14:31,250 --> 00:14:39,890
having two to two new models effectively

00:14:36,190 --> 00:14:42,050
and we're looking some beyond ml as well

00:14:39,890 --> 00:14:46,210
minute so we understand ten suppose

00:14:42,050 --> 00:14:50,480
primary machine learning is a subset of

00:14:46,210 --> 00:14:52,100
AI it's not a super set error AI and so

00:14:50,480 --> 00:14:56,770
we're looking at the broader range

00:14:52,100 --> 00:14:59,480
aspect of AI on service of which ml

00:14:56,770 --> 00:15:04,190
intensively would then be an operational

00:14:59,480 --> 00:15:07,150
part I'd now we have some resources

00:15:04,190 --> 00:15:09,910
available and we encourage community

00:15:07,150 --> 00:15:11,120
assistance as well to optimize

00:15:09,910 --> 00:15:13,520
tensorflow

00:15:11,120 --> 00:15:17,600
and the various libraries for sve and

00:15:13,520 --> 00:15:22,339
the upcoming sp2 both are supported in

00:15:17,600 --> 00:15:27,910
the GCC and LLVM papyrus already so you

00:15:22,339 --> 00:15:29,400
don't actually need hardware to to run

00:15:27,910 --> 00:15:31,260
these

00:15:29,400 --> 00:15:35,070
particular versions you can use software

00:15:31,260 --> 00:15:37,410
emulation models too so that we have the

00:15:35,070 --> 00:15:41,190
software available when the silicon is

00:15:37,410 --> 00:15:43,820
also available and the key difference

00:15:41,190 --> 00:15:48,990
also for AI on servers is the online

00:15:43,820 --> 00:15:51,510
dynamic approach so conventional models

00:15:48,990 --> 00:15:55,760
are trained by throwing a whole load of

00:15:51,510 --> 00:15:58,650
data at the training environment and

00:15:55,760 --> 00:16:01,440
then working on the model once the model

00:15:58,650 --> 00:16:03,630
is done you take modeling you use it but

00:16:01,440 --> 00:16:05,010
if the diff and if the data structure

00:16:03,630 --> 00:16:07,350
changes you have to throw away that

00:16:05,010 --> 00:16:10,080
model and start with a whole new data

00:16:07,350 --> 00:16:12,450
set we are completely trained to

00:16:10,080 --> 00:16:15,480
generate a new bot now each new model

00:16:12,450 --> 00:16:17,400
could take hours or days or even weeks

00:16:15,480 --> 00:16:21,060
if you're talking about billions of

00:16:17,400 --> 00:16:23,790
layers or billions of data items in

00:16:21,060 --> 00:16:25,650
which to calculate and then this is also

00:16:23,790 --> 00:16:28,680
why we need to hide you can see or

00:16:25,650 --> 00:16:31,080
exascale computing even for the very

00:16:28,680 --> 00:16:34,290
large data sets but rarely of interest

00:16:31,080 --> 00:16:37,380
is the online dynamic nature of the

00:16:34,290 --> 00:16:41,070
potential for AI learning so you

00:16:37,380 --> 00:16:45,390
actually report bag such as say a reward

00:16:41,070 --> 00:16:48,570
mechanism if something is identified it

00:16:45,390 --> 00:16:51,660
correctly versus a negative reward

00:16:48,570 --> 00:16:53,760
system if something is I did incorrectly

00:16:51,660 --> 00:16:58,290
and so that over a period of time you

00:16:53,760 --> 00:17:02,610
can tune the intelligence that is being

00:16:58,290 --> 00:17:04,050
utilized now that we don't just look at

00:17:02,610 --> 00:17:07,380
frameworks we look at underlying

00:17:04,050 --> 00:17:09,180
libraries and we look at open CV open CL

00:17:07,380 --> 00:17:11,280
and they open RP each of those need

00:17:09,180 --> 00:17:14,250
further optimization to make use of

00:17:11,280 --> 00:17:16,380
stimuli architectures such as the

00:17:14,250 --> 00:17:21,180
earlier versions of neon and then

00:17:16,380 --> 00:17:24,050
subsequently SPM sp2 so with again like

00:17:21,180 --> 00:17:26,550
community support from that product

00:17:24,050 --> 00:17:30,200
these are the sort of environments that

00:17:26,550 --> 00:17:33,350
we're looking for training methods so

00:17:30,200 --> 00:17:35,490
this is where we can apply tensor flow

00:17:33,350 --> 00:17:37,380
in straightforward image of

00:17:35,490 --> 00:17:40,620
classification we've heard about the 11

00:17:37,380 --> 00:17:43,280
million cats to identify a cap where as

00:17:40,620 --> 00:17:46,250
a child will be shown maybe one

00:17:43,280 --> 00:17:48,020
over a period of hard time and realized

00:17:46,250 --> 00:17:51,260
that the period of time that it's called

00:17:48,020 --> 00:17:53,990
a cat so there's a differentiation being

00:17:51,260 --> 00:17:58,700
between massive datasets and learning

00:17:53,990 --> 00:18:01,970
like child object detection we we see

00:17:58,700 --> 00:18:05,450
the automotive industry during collision

00:18:01,970 --> 00:18:08,150
avoidance systems where they know

00:18:05,450 --> 00:18:10,700
they've got models which identified what

00:18:08,150 --> 00:18:14,570
cars look like or pedestrians cyclists

00:18:10,700 --> 00:18:18,050
etc but we're looking to see if we can

00:18:14,570 --> 00:18:23,720
get some tends to floating move beyond

00:18:18,050 --> 00:18:26,270
just those basic object detections a

00:18:23,720 --> 00:18:29,000
more intuitive way because what happens

00:18:26,270 --> 00:18:30,830
if a new car design is brought out that

00:18:29,000 --> 00:18:32,510
doesn't look like previous car designs

00:18:30,830 --> 00:18:35,420
well you have to go back to the drawing

00:18:32,510 --> 00:18:38,540
board and do a whole new set of data to

00:18:35,420 --> 00:18:41,660
recreate the model and yet a human's

00:18:38,540 --> 00:18:45,080
observation of a new car would think

00:18:41,660 --> 00:18:48,310
okay it's new but overall it

00:18:45,080 --> 00:18:51,590
fundamentally it looks like a car and so

00:18:48,310 --> 00:18:54,190
this is the area between pure

00:18:51,590 --> 00:18:56,570
computation and a bit of intuition or

00:18:54,190 --> 00:18:58,580
understanding so at the bottom of the

00:18:56,570 --> 00:19:00,350
screen you see perception understanding

00:18:58,580 --> 00:19:02,930
discovering prediction creation these

00:19:00,350 --> 00:19:05,870
are the approaches as to where we want

00:19:02,930 --> 00:19:10,850
to really push the underlying libraries

00:19:05,870 --> 00:19:13,820
and if we can get them to not just rely

00:19:10,850 --> 00:19:16,340
on a PCs but maybe the HPC is in the

00:19:13,820 --> 00:19:20,300
cloud and the edge devices are doing the

00:19:16,340 --> 00:19:22,460
latency-sensitive solutions but then the

00:19:20,300 --> 00:19:25,490
edge device is referring back to the

00:19:22,460 --> 00:19:29,210
cloud to receive updates and in fact

00:19:25,490 --> 00:19:32,480
it's a company I think Nuala they offer

00:19:29,210 --> 00:19:35,300
a cloud solution talking to edge devices

00:19:32,480 --> 00:19:38,150
which do pretty much that the edge

00:19:35,300 --> 00:19:40,460
devices feedback changes to the model

00:19:38,150 --> 00:19:42,770
the model is recalculated in the cloud

00:19:40,460 --> 00:19:49,700
and then the count three downloads by

00:19:42,770 --> 00:19:52,510
the updated model that's we don't really

00:19:49,700 --> 00:19:54,559
to test all these things out in the open

00:19:52,510 --> 00:19:56,389
environment it could be a little

00:19:54,559 --> 00:20:01,789
dangerous having a robot running around

00:19:56,389 --> 00:20:06,669
a roadway so we like test environments

00:20:01,789 --> 00:20:10,190
so here's a list of five different Aoi

00:20:06,669 --> 00:20:12,500
environments the deep mind labs Google

00:20:10,190 --> 00:20:15,830
research is a habitat from Facebook and

00:20:12,500 --> 00:20:17,990
videos Issac SDK coincidental I call

00:20:15,830 --> 00:20:22,070
Isaac but nothing - nothing to do with

00:20:17,990 --> 00:20:25,929
me and the cyber biotics we lots so

00:20:22,070 --> 00:20:29,330
we're looking at proving the

00:20:25,929 --> 00:20:32,450
optimization festival of our library but

00:20:29,330 --> 00:20:34,940
then the application of the library how

00:20:32,450 --> 00:20:38,389
it performs in these test environments

00:20:34,940 --> 00:20:41,659
and so if you're looking to actually

00:20:38,389 --> 00:20:43,840
test any of your own codes we can look

00:20:41,659 --> 00:20:48,679
at hosting some of these potential

00:20:43,840 --> 00:20:52,490
environments and we host these things in

00:20:48,679 --> 00:20:56,750
our colocation facility currently in

00:20:52,490 --> 00:21:01,250
London and some elements of it in the US

00:20:56,750 --> 00:21:03,590
as well and we encourage developers who

00:21:01,250 --> 00:21:08,179
want to try their solutions on arm

00:21:03,590 --> 00:21:11,029
hardware arm server based hardware to

00:21:08,179 --> 00:21:15,139
get in contact and then you'll be able

00:21:11,029 --> 00:21:20,480
to test your solutions on the large HPC

00:21:15,139 --> 00:21:23,120
environment typically were known to give

00:21:20,480 --> 00:21:25,549
an example of a large hey pH protein

00:21:23,120 --> 00:21:30,649
which is our well Sandia National Labs

00:21:25,549 --> 00:21:33,799
they have the Astra solution and then we

00:21:30,649 --> 00:21:36,110
have forgot a running the a 64 FX which

00:21:33,799 --> 00:21:39,919
is currently being constructed in Japan

00:21:36,110 --> 00:21:42,039
through Fujitsu and hopefully we will

00:21:39,919 --> 00:21:45,019
get access to those as well

00:21:42,039 --> 00:21:49,460
so where are we tending so just as a

00:21:45,019 --> 00:21:51,799
sort of a wrap up so we're currently

00:21:49,460 --> 00:21:54,049
reconfiguring our Hardware in the

00:21:51,799 --> 00:21:57,830
colocation facility we're looking to

00:21:54,049 --> 00:21:59,990
have a truly scalable environment so we

00:21:57,830 --> 00:22:02,390
can go from a small number of nodes to a

00:21:59,990 --> 00:22:04,590
large number of nodes depending on

00:22:02,390 --> 00:22:06,929
depending on the build scenario that's

00:22:04,590 --> 00:22:12,690
required and then scale it back down to

00:22:06,929 --> 00:22:16,080
be efficient use of energy electricity

00:22:12,690 --> 00:22:19,110
and arm is helping by providing some of

00:22:16,080 --> 00:22:23,100
the build recipes for tensorflow and ml

00:22:19,110 --> 00:22:26,429
perf so we work alongside all our

00:22:23,100 --> 00:22:29,159
members and their member engineers so

00:22:26,429 --> 00:22:32,940
that it's not just Leonora coming up

00:22:29,159 --> 00:22:35,610
with the software or they build

00:22:32,940 --> 00:22:39,570
scenarios that we actually encourage a

00:22:35,610 --> 00:22:42,600
collaborative environment and we all

00:22:39,570 --> 00:22:46,559
obviously recognize where the sauce the

00:22:42,600 --> 00:22:49,710
solution came from so we are currently

00:22:46,559 --> 00:22:52,860
reviewing the intensive low continuous

00:22:49,710 --> 00:22:54,870
integration solution to be fair we've

00:22:52,860 --> 00:22:57,630
got a couple of issues with Python 3 in

00:22:54,870 --> 00:23:00,390
the build but that's ongoing however you

00:22:57,630 --> 00:23:03,570
can see I'll get her page if you'd like

00:23:00,390 --> 00:23:05,610
to contribute it's made reference to a

00:23:03,570 --> 00:23:07,409
github page on a previous slide also for

00:23:05,610 --> 00:23:10,500
the AR servers in case you've got some

00:23:07,409 --> 00:23:14,100
examples that you'd like to upload and

00:23:10,500 --> 00:23:17,130
what we're currently being doing also is

00:23:14,100 --> 00:23:19,559
just making sure the opencv works out 64

00:23:17,130 --> 00:23:22,200
which is very very easily done because

00:23:19,559 --> 00:23:25,250
it even works on the Raspberry Pi but

00:23:22,200 --> 00:23:28,049
you know we'd like to scale it up

00:23:25,250 --> 00:23:31,669
optimization and we still have there

00:23:28,049 --> 00:23:35,700
plenty of things to do so OpenCL OpenMP

00:23:31,669 --> 00:23:39,600
required we've got some test platforms

00:23:35,700 --> 00:23:43,940
for the new neo verse and one solutions

00:23:39,600 --> 00:23:47,510
which will be the is a version 8.2

00:23:43,940 --> 00:23:53,490
whereas the current shipping hardware is

00:23:47,510 --> 00:23:56,010
8.1 so it's available and then we do

00:23:53,490 --> 00:23:59,490
plan to test containerization so that we

00:23:56,010 --> 00:24:02,520
can improve the scalability they build

00:23:59,490 --> 00:24:04,620
scenarios so with that if there's any

00:24:02,520 --> 00:24:07,200
final questions over them now please

00:24:04,620 --> 00:24:12,539
feel free to contact us on our servers

00:24:07,200 --> 00:24:15,170
at Leonora dog or so offer to submit on

00:24:12,539 --> 00:24:17,960
the github page any of the examples

00:24:15,170 --> 00:24:20,840
is there a tensorflow 2x package or

00:24:17,960 --> 00:24:25,060
docker image available now to run on arm

00:24:20,840 --> 00:24:27,440
64 and where can I get it for I

00:24:25,060 --> 00:24:30,620
personally don't have the docker and

00:24:27,440 --> 00:24:34,100
docker images available I know that

00:24:30,620 --> 00:24:38,480
NVIDIA has been container izing elements

00:24:34,100 --> 00:24:40,940
of it but it's something that we do want

00:24:38,480 --> 00:24:45,190
to get into but we don't have available

00:24:40,940 --> 00:24:45,190

YouTube URL: https://www.youtube.com/watch?v=iP-syf5lhBQ


