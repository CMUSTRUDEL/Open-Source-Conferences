Title: LTD20-303 Recent developments in I O control and testing
Publication date: 2020-04-01
Playlist: Linaro Tech Days 2020
Description: 
	Description:

This session is about the last main developments on I/O control and I/O testing, in the upstream Linux kernel. In more detail, we will cover: the adoption of the BFQ I/O scheduler in new systems, the new developments in the comparison among BFQ and the new Linux I/O controllers, and the last developments in I/O testing within the LKFT project.

Session Speakers
Anders Roxell
Software engineer (Linaro)

Versatile R&amp;D professional with 6 years experience in software development for embedded systems and their special requirements, with a large knowledge base of how to troubleshoot complex real time systems. The technical competence is built mostly upon R&amp;D within embedded systems in both telecommunication (e.g. base stations, media gateways) and automotive systems (e.g. engine-, gearbox-platforms).
Paolo Valente
Assistant professor (Linaro)


Paolo Valente is an Assistant Professor of Computer Science at the University of Modena and Reggio Emilia, Italy, and a collaborator of the Linaro engineering organization. Paolo's main activities focus on scheduling algorithms for storage devices, transmission links and CPUs. In this respect, Paolo is the author of the last version of the BFQ I/O scheduler. BFQ entered the Linux kernel from 4.12, providing unprecedented low-latency and fairness guarantees. As for transmission links, Paolo is one of the authors of the QFQ packet scheduler, which has been in the Linux kernel until 3.7, after that it has been replaced by QFQ+, a faster variant defined and implemented by Paolo himself. Finally, Paolo has also defined and implemented other algorithms, part of which are now in FreeBSD, and has provided new theoretic results on multiprocessor scheduling.


You can view this sessions presentation here:
https://connect.linaro.org/resources/ltd20/ltd20-303/
Captions: 
	00:00:00,030 --> 00:00:08,510
okay okay so hi everybody

00:00:03,360 --> 00:00:12,179
and I hope you you are good in this

00:00:08,510 --> 00:00:16,400
tough period I'm about to share my

00:00:12,179 --> 00:00:20,300
screen so this is a joint presentation

00:00:16,400 --> 00:00:24,060
made of two parts two sessions the first

00:00:20,300 --> 00:00:25,920
given by by me and it is about your

00:00:24,060 --> 00:00:28,859
control the other one is given by under

00:00:25,920 --> 00:00:34,620
sense about Colonel testing still

00:00:28,859 --> 00:00:37,680
related to with IO control my part is

00:00:34,620 --> 00:00:40,140
longer I've tried to keep it as short as

00:00:37,680 --> 00:00:42,629
possible I'll skip some slides if

00:00:40,140 --> 00:00:47,910
something is not clear just stop me or

00:00:42,629 --> 00:00:51,989
ask me after we have finished of course

00:00:47,910 --> 00:00:54,930
NP if you stop me so as for my sub

00:00:51,989 --> 00:00:56,969
session and also mice obsession is made

00:00:54,930 --> 00:00:59,699
of two parts in the first part I'll tell

00:00:56,969 --> 00:01:01,680
you the state of affairs so the state of

00:00:59,699 --> 00:01:03,870
the heart and I will talk about these

00:01:01,680 --> 00:01:08,460
two new are your controllers that are

00:01:03,870 --> 00:01:11,130
now available in Linux and I will report

00:01:08,460 --> 00:01:14,490
on what happened it happened that I have

00:01:11,130 --> 00:01:17,600
compared them will be FQ and I have seen

00:01:14,490 --> 00:01:20,759
that results are extremely bad because

00:01:17,600 --> 00:01:22,560
just trying some common workloads I've

00:01:20,759 --> 00:01:25,229
seen that this cat this new i/o

00:01:22,560 --> 00:01:27,600
controllers just fail they fail to

00:01:25,229 --> 00:01:32,100
control bandwidth they fail to reach

00:01:27,600 --> 00:01:35,340
high truth but what is missing in the

00:01:32,100 --> 00:01:37,860
picture is the reason why these

00:01:35,340 --> 00:01:40,229
controllers fail so the main goal of

00:01:37,860 --> 00:01:43,110
this presentation is answering this

00:01:40,229 --> 00:01:46,700
question why do these new controllers

00:01:43,110 --> 00:01:51,049
fail I'll answer with than an artist

00:01:46,700 --> 00:01:56,369
somehow an in-depth analysis on these

00:01:51,049 --> 00:01:59,340
controllers so what is the use case of

00:01:56,369 --> 00:02:01,320
interest for these controllers the use

00:01:59,340 --> 00:02:03,180
case is when you have multiple entities

00:02:01,320 --> 00:02:06,020
that are competing for some shared

00:02:03,180 --> 00:02:08,580
storage examples of entities are

00:02:06,020 --> 00:02:12,030
processes groups containers little

00:02:08,580 --> 00:02:13,970
machine any active entity that may

00:02:12,030 --> 00:02:17,240
compete for a store

00:02:13,970 --> 00:02:21,820
and the goal is to guarantee to each of

00:02:17,240 --> 00:02:25,760
these entities and bounded latency and

00:02:21,820 --> 00:02:28,160
over at least a minimum bandwidth of

00:02:25,760 --> 00:02:31,610
course we are talking about a year so

00:02:28,160 --> 00:02:36,800
our latency high bandwidth in this

00:02:31,610 --> 00:02:41,290
respect the typical mainstream solutions

00:02:36,800 --> 00:02:44,630
are extremely inefficient because they

00:02:41,290 --> 00:02:46,940
wastes almost all the speed of the

00:02:44,630 --> 00:02:48,980
drives up to 90 percent of the speed of

00:02:46,940 --> 00:02:55,510
these drives you can find all details on

00:02:48,980 --> 00:02:58,900
in this article published on Lennar blog

00:02:55,510 --> 00:03:02,960
there is a solution to this problem

00:02:58,900 --> 00:03:05,930
DBF qio scheduler dust face this problem

00:03:02,960 --> 00:03:09,260
and actually reduces this waste of speed

00:03:05,930 --> 00:03:14,000
to actually practically nothing no no no

00:03:09,260 --> 00:03:17,990
ways but but if you for a series of

00:03:14,000 --> 00:03:23,500
reasons is not yet a common solution so

00:03:17,990 --> 00:03:26,900
in this situation Facebook people have

00:03:23,500 --> 00:03:31,160
Facebook developers have contributed

00:03:26,900 --> 00:03:33,980
with to new and your controllers the i/o

00:03:31,160 --> 00:03:36,040
latency controller and diario cost

00:03:33,980 --> 00:03:40,010
controller the first controller

00:03:36,040 --> 00:03:45,190
associate a target latency with every

00:03:40,010 --> 00:03:49,790
group and then its photos group so has

00:03:45,190 --> 00:03:52,760
so that the target latency of each group

00:03:49,790 --> 00:03:54,860
is met so you assign a latency to every

00:03:52,760 --> 00:03:56,930
group the dunfin see that you desire to

00:03:54,860 --> 00:04:00,230
satisfy for that group and the

00:03:56,930 --> 00:04:03,290
controller tries to satisfy that latency

00:04:00,230 --> 00:04:06,890
target dear your controller on the other

00:04:03,290 --> 00:04:09,980
hand associate a wait with each group

00:04:06,890 --> 00:04:11,810
and in again it rotors group because it

00:04:09,980 --> 00:04:14,989
works with the same identical mechanism

00:04:11,810 --> 00:04:17,750
both controllers work by trotting IO but

00:04:14,989 --> 00:04:22,010
in this case the Trotters group so has

00:04:17,750 --> 00:04:24,400
to let each group get a fraction of the

00:04:22,010 --> 00:04:27,430
bandwidth of the total bandwidth

00:04:24,400 --> 00:04:30,009
fraction proportional to the way

00:04:27,430 --> 00:04:33,160
of the group so the higher the wait the

00:04:30,009 --> 00:04:36,759
higher the bandwidth in addition this

00:04:33,160 --> 00:04:40,990
bandwidth is inversely proportional to

00:04:36,759 --> 00:04:43,660
the cost of yoyo of that group cost in

00:04:40,990 --> 00:04:44,620
terms of time how long it takes to do

00:04:43,660 --> 00:04:47,590
that are you

00:04:44,620 --> 00:04:50,080
so the more costly a yogic rupees the

00:04:47,590 --> 00:04:53,199
lower bandwidth that group gets the idea

00:04:50,080 --> 00:04:56,169
is to avoid that groups doing very

00:04:53,199 --> 00:04:59,350
costly high or kill the total bandwidth

00:04:56,169 --> 00:05:02,979
of a tribe with this scheme you can also

00:04:59,350 --> 00:05:05,470
provide latency guarantees indirectly by

00:05:02,979 --> 00:05:09,870
giving the right bandwidth to a certain

00:05:05,470 --> 00:05:16,330
group you also guarantee hit a certain

00:05:09,870 --> 00:05:21,070
latency now how do these new proposals

00:05:16,330 --> 00:05:23,889
compare will be at cue well bsq does

00:05:21,070 --> 00:05:26,050
exactly the same thing as the i/o cost

00:05:23,889 --> 00:05:28,090
controller because it associates a

00:05:26,050 --> 00:05:30,099
weight with each group or even each

00:05:28,090 --> 00:05:33,430
process but this is not that relevant

00:05:30,099 --> 00:05:36,280
and then the mechanism is different

00:05:33,430 --> 00:05:38,680
because bfq does not rot or you but it's

00:05:36,280 --> 00:05:40,720
scheduled Co so it decides the order in

00:05:38,680 --> 00:05:44,699
which I always to be served and the

00:05:40,720 --> 00:05:47,110
scheduler also has such that each group

00:05:44,699 --> 00:05:49,240
gets a fraction of the bandwidth

00:05:47,110 --> 00:05:53,190
proportion to its weight it's the same

00:05:49,240 --> 00:05:58,000
as with your cost your host controller

00:05:53,190 --> 00:06:01,360
the to avoid that slow while you're also

00:05:58,000 --> 00:06:04,210
in this case kills through boots lower

00:06:01,360 --> 00:06:06,159
your gets guarantees in the time domain

00:06:04,210 --> 00:06:07,870
or is this detailed but I don't want to

00:06:06,159 --> 00:06:12,430
steal your time on low-level details

00:06:07,870 --> 00:06:14,680
about bfq and to finish about vfq as we

00:06:12,430 --> 00:06:17,380
do course you can have latency

00:06:14,680 --> 00:06:20,800
guarantees indirectly by setting

00:06:17,380 --> 00:06:23,530
bandwidth guarantees so as I hope you

00:06:20,800 --> 00:06:27,729
have seen the goals of these new

00:06:23,530 --> 00:06:30,580
controllers are the same as bfq latency

00:06:27,729 --> 00:06:33,039
and bandwidth yet no documentation is

00:06:30,580 --> 00:06:35,770
available on when one should use these

00:06:33,039 --> 00:06:40,029
controllers or and when one should use

00:06:35,770 --> 00:06:41,330
bfq so this motivated me to compare this

00:06:40,029 --> 00:06:44,629
controllers with

00:06:41,330 --> 00:06:47,150
to find out when to use one when the

00:06:44,629 --> 00:06:51,680
other what is the performance of these

00:06:47,150 --> 00:06:56,539
controllers and compared with bfq I made

00:06:51,680 --> 00:06:59,919
this comparison I am for example you can

00:06:56,539 --> 00:07:07,280
find Marie sat in my presentation for

00:06:59,919 --> 00:07:10,580
last connector and I measured the

00:07:07,280 --> 00:07:12,530
ability to guarantee bandwidth to groups

00:07:10,580 --> 00:07:15,379
and latency to groups and at the same

00:07:12,530 --> 00:07:17,509
time to reach high a true high Tata true

00:07:15,379 --> 00:07:18,289
boot what is what was the result what is

00:07:17,509 --> 00:07:20,960
the result

00:07:18,289 --> 00:07:25,639
if you outperforming these controllers

00:07:20,960 --> 00:07:29,750
for example on an SSD will be F Q I so

00:07:25,639 --> 00:07:33,379
agree I record it if up to 47 times more

00:07:29,750 --> 00:07:37,150
bandwidth than with are your cost and up

00:07:33,379 --> 00:07:42,400
to three times more total to boot so

00:07:37,150 --> 00:07:47,300
really upper outperform at them so why

00:07:42,400 --> 00:07:49,759
well it happened be closed for some

00:07:47,300 --> 00:07:52,159
workloads this new controller simply

00:07:49,759 --> 00:07:54,289
failed to controller you

00:07:52,159 --> 00:07:56,750
they fail to provide the expected

00:07:54,289 --> 00:07:59,810
guarantees for latency mandated they

00:07:56,750 --> 00:08:03,560
fail to reach a high throughput okay

00:07:59,810 --> 00:08:05,090
they failed but why did they fail so

00:08:03,560 --> 00:08:08,029
this is the goal of this presentation

00:08:05,090 --> 00:08:12,680
telling you what I found out about the

00:08:08,029 --> 00:08:15,440
why I will focus only on now your cost

00:08:12,680 --> 00:08:18,409
because is more complex and accurate

00:08:15,440 --> 00:08:23,000
than dilated so my result cost apply

00:08:18,409 --> 00:08:25,699
also to our latency icon there I will

00:08:23,000 --> 00:08:27,740
show me only three of the workloads for

00:08:25,699 --> 00:08:30,319
which are your cost rails on the drive

00:08:27,740 --> 00:08:32,930
that you can see on the slide you can

00:08:30,319 --> 00:08:35,029
find a full description of the plots

00:08:32,930 --> 00:08:37,690
that I'm going to show you and full

00:08:35,029 --> 00:08:42,500
results once again in my previous

00:08:37,690 --> 00:08:47,209
presentation so here's my first example

00:08:42,500 --> 00:08:51,680
let me tell you what is on this on this

00:08:47,209 --> 00:08:55,080
plot so in this block I'm showing true

00:08:51,680 --> 00:08:58,230
boots for work

00:08:55,080 --> 00:09:01,890
that are generated by two types of

00:08:58,230 --> 00:09:04,830
sources the first source is a group that

00:09:01,890 --> 00:09:07,890
I call target and this is our probe

00:09:04,830 --> 00:09:09,720
group the one that we want to use the

00:09:07,890 --> 00:09:11,790
bend we want to use to measure the

00:09:09,720 --> 00:09:15,000
bandwidth or the latency that can be

00:09:11,790 --> 00:09:18,200
guaranteed to a generic group for any

00:09:15,000 --> 00:09:20,760
workload the other groups are the

00:09:18,200 --> 00:09:24,270
interferers their goal is just to

00:09:20,760 --> 00:09:30,420
generate your to interfere with the area

00:09:24,270 --> 00:09:32,970
of the target and here you can see that

00:09:30,420 --> 00:09:35,610
what the interferers do so there are

00:09:32,970 --> 00:09:37,680
several workloads you can see that in

00:09:35,610 --> 00:09:40,860
the first subplot so the first workload

00:09:37,680 --> 00:09:43,470
interferes to just random reads second

00:09:40,860 --> 00:09:45,840
subplots random writes again random

00:09:43,470 --> 00:09:47,880
reads random writes and then there are

00:09:45,840 --> 00:09:50,310
the different different possible

00:09:47,880 --> 00:09:52,740
workloads for the target random reads

00:09:50,310 --> 00:09:56,190
random read sequential read sequentially

00:09:52,740 --> 00:10:00,000
so the various combination in this set

00:09:56,190 --> 00:10:02,460
of workloads this is just one of the

00:10:00,000 --> 00:10:07,710
many plots that I made so in this case

00:10:02,460 --> 00:10:13,320
and comparing basically interferers that

00:10:07,710 --> 00:10:17,280
do random i/o against a target that may

00:10:13,320 --> 00:10:21,450
do either random or sequential i/o just

00:10:17,280 --> 00:10:24,960
reads okay so what does do this plot

00:10:21,450 --> 00:10:27,210
show they show both the true put the

00:10:24,960 --> 00:10:31,550
average true during the test of the

00:10:27,210 --> 00:10:33,780
target it's the bar in pink and the

00:10:31,550 --> 00:10:37,200
commute the cumulative average

00:10:33,780 --> 00:10:41,730
throughput of all the interfere as it is

00:10:37,200 --> 00:10:45,450
the bar in Sian finally as a reference

00:10:41,730 --> 00:10:48,690
there is the total true boot that is

00:10:45,450 --> 00:10:51,120
reached if there is no control on a yo

00:10:48,690 --> 00:10:53,880
usually in that case the to boot is

00:10:51,120 --> 00:10:56,580
higher because the tribe is free to do

00:10:53,880 --> 00:11:00,000
any optimization impossible optimization

00:10:56,580 --> 00:11:03,420
to get the maximum possible truth of

00:11:00,000 --> 00:11:07,620
course the sum of the heads of these

00:11:03,420 --> 00:11:10,009
bars gives the total truth finally what

00:11:07,620 --> 00:11:13,350
else there

00:11:10,009 --> 00:11:16,680
the schedulers under test the two

00:11:13,350 --> 00:11:20,040
solutions under test one solution is the

00:11:16,680 --> 00:11:21,779
cost the I do cost control array and the

00:11:20,040 --> 00:11:26,779
a/o cost controller implements an i/o

00:11:21,779 --> 00:11:31,079
policy that is called cost so the first

00:11:26,779 --> 00:11:36,029
burst in each subplot report the results

00:11:31,079 --> 00:11:39,420
for the cost policy you can see both

00:11:36,029 --> 00:11:41,999
policy and scheduler in case of the cost

00:11:39,420 --> 00:11:44,399
controller the cost policy the scheduler

00:11:41,999 --> 00:11:47,730
is known because audio control is in the

00:11:44,399 --> 00:11:51,749
hands of the controller then there is

00:11:47,730 --> 00:11:53,699
pfq as scheduler and the policy is the

00:11:51,749 --> 00:11:59,189
proportional share policy which is

00:11:53,699 --> 00:12:01,800
exactly the policy implemented by bfq so

00:11:59,189 --> 00:12:05,720
what do we see in this plot here we see

00:12:01,800 --> 00:12:08,730
jets with cost as your controller the

00:12:05,720 --> 00:12:11,639
target eats all the bandwidth and the

00:12:08,730 --> 00:12:13,709
interference don't get anything consider

00:12:11,639 --> 00:12:17,610
that interference if end member well are

00:12:13,709 --> 00:12:20,279
at least seven or even fourteen so all

00:12:17,610 --> 00:12:23,959
these interference altogether gets

00:12:20,279 --> 00:12:27,029
almost no bandwidth so total failure in

00:12:23,959 --> 00:12:30,509
providing reasonable bandwidth

00:12:27,029 --> 00:12:34,259
guarantees will be a few you see that

00:12:30,509 --> 00:12:37,290
the bandwidth is somehow distributed

00:12:34,259 --> 00:12:39,329
between the target and the controller's

00:12:37,290 --> 00:12:44,879
are in the interferes but I don't want

00:12:39,329 --> 00:12:49,649
to go into excessive debt on this so I

00:12:44,879 --> 00:12:56,100
hope these plots are clear if they are

00:12:49,649 --> 00:12:58,110
not just tell me now if it okay the

00:12:56,100 --> 00:13:01,980
other two were close of interests are

00:12:58,110 --> 00:13:04,230
these ones in this case the failure is

00:13:01,980 --> 00:13:06,600
about the total throughput as you can

00:13:04,230 --> 00:13:08,879
see the sum of the heights of the bars

00:13:06,600 --> 00:13:12,929
in case of I your cost is much lower

00:13:08,879 --> 00:13:16,110
than the case with bfq it's extremely

00:13:12,929 --> 00:13:18,600
lower so there is a high loss of true

00:13:16,110 --> 00:13:20,910
boot both in this case where there are

00:13:18,600 --> 00:13:22,590
sequential readers against the a

00:13:20,910 --> 00:13:25,230
sequential reader is

00:13:22,590 --> 00:13:27,360
target and inkay indicates where there

00:13:25,230 --> 00:13:29,580
are sequential writers as interference

00:13:27,360 --> 00:13:31,500
and the sequential reader s target so

00:13:29,580 --> 00:13:33,690
this is the second type of field so

00:13:31,500 --> 00:13:35,760
first type failure in guarantee in

00:13:33,690 --> 00:13:37,529
bandwidth to the interference in this

00:13:35,760 --> 00:13:39,690
case but there are other example where

00:13:37,529 --> 00:13:42,170
the failure is for the target and in

00:13:39,690 --> 00:13:45,210
this case failure to reach a hydro boot

00:13:42,170 --> 00:13:47,460
okay so this is the problem so I started

00:13:45,210 --> 00:13:50,670
analyzing it to do that I made a patch

00:13:47,460 --> 00:13:56,880
to track to trace what happens inside

00:13:50,670 --> 00:13:59,130
are your cost and what I found the root

00:13:56,880 --> 00:14:01,680
cause of for all this failure is this

00:13:59,130 --> 00:14:04,260
one try save a very complex transfer

00:14:01,680 --> 00:14:07,560
function because they have multiple

00:14:04,260 --> 00:14:09,839
challenge they have pipelines inside the

00:14:07,560 --> 00:14:11,880
channels they do striping there is

00:14:09,839 --> 00:14:13,890
parallelism that depends on locality

00:14:11,880 --> 00:14:16,710
there is interference among workers

00:14:13,890 --> 00:14:19,500
inside the driver is read ahead request

00:14:16,710 --> 00:14:22,740
reordering garbage collection wearing

00:14:19,500 --> 00:14:24,960
and a lot of complex stuff so for this

00:14:22,740 --> 00:14:27,780
reason the parameters of the transfer

00:14:24,960 --> 00:14:30,480
function of these drives are no linear

00:14:27,780 --> 00:14:39,270
they vary with time they vary with

00:14:30,480 --> 00:14:44,730
workloads and transfer functions the

00:14:39,270 --> 00:14:48,960
function that tells us the output so

00:14:44,730 --> 00:14:51,360
what happens which are your requests are

00:14:48,960 --> 00:14:54,750
said and when as a function of the

00:14:51,360 --> 00:14:59,010
inputs G notice that TV dispatch to the

00:14:54,750 --> 00:15:01,380
drive so complex transfer functions and

00:14:59,010 --> 00:15:04,680
the problem is dead are you the cause

00:15:01,380 --> 00:15:08,520
and your latency control are you exactly

00:15:04,680 --> 00:15:11,520
true these parameters in particular are

00:15:08,520 --> 00:15:14,250
your costs are the parameters used by

00:15:11,520 --> 00:15:16,650
the local controller so the across

00:15:14,250 --> 00:15:19,140
controller uses the i/o costs the cost

00:15:16,650 --> 00:15:22,860
of each i/o as a parameter to control

00:15:19,140 --> 00:15:29,010
your and uses device saturation so how

00:15:22,860 --> 00:15:31,140
loaded the device is and these are

00:15:29,010 --> 00:15:33,720
exactly the parameters that are not

00:15:31,140 --> 00:15:35,920
linear that very time that's hard to

00:15:33,720 --> 00:15:37,630
know precisely so

00:15:35,920 --> 00:15:40,060
depending on the actual transfer

00:15:37,630 --> 00:15:43,029
function it may be hard for the

00:15:40,060 --> 00:15:45,540
controller to actually control the

00:15:43,029 --> 00:15:50,980
bandwidth of the groups and to boost the

00:15:45,540 --> 00:15:55,690
truth so I I know that these details are

00:15:50,980 --> 00:15:58,810
maybe somehow not so apparent

00:15:55,690 --> 00:16:01,810
the problem beneath all these details

00:15:58,810 --> 00:16:04,180
may be not so apparent so I will go for

00:16:01,810 --> 00:16:08,350
a real-life example that maybe will help

00:16:04,180 --> 00:16:11,139
us grasp the essence of the problem then

00:16:08,350 --> 00:16:20,620
I will apply this real-life example to

00:16:11,139 --> 00:16:23,829
the failure of our cost so it has been

00:16:20,620 --> 00:16:27,220
boots put little or through the knocker

00:16:23,829 --> 00:16:31,389
in having stable and easy to control

00:16:27,220 --> 00:16:34,389
system for heating or water and example

00:16:31,389 --> 00:16:36,730
is unfortunately in my hand my house so

00:16:34,389 --> 00:16:41,170
suppose did you want to take a shower in

00:16:36,730 --> 00:16:44,079
such a building getting the shower

00:16:41,170 --> 00:16:45,519
temperature right may be a problem in

00:16:44,079 --> 00:16:47,800
this building

00:16:45,519 --> 00:16:50,980
because when you rotate the knob of the

00:16:47,800 --> 00:16:54,519
shower you stimulate in a nonlinear way

00:16:50,980 --> 00:16:57,790
and no linear system and it is a system

00:16:54,519 --> 00:17:00,760
that reacts with time varying delays so

00:16:57,790 --> 00:17:04,449
here is what happens if you don't know

00:17:00,760 --> 00:17:09,000
these parameters well enough you may

00:17:04,449 --> 00:17:12,130
fail to control the temperature

00:17:09,000 --> 00:17:14,709
successfully because you may get to hot

00:17:12,130 --> 00:17:18,790
water to cold water until maybe you

00:17:14,709 --> 00:17:22,809
learn how to rotate the node but even

00:17:18,790 --> 00:17:24,819
after you learn that you need it so you

00:17:22,809 --> 00:17:33,280
have controlling your temperature there

00:17:24,819 --> 00:17:36,970
may still change this parameter so you

00:17:33,280 --> 00:17:40,840
get burned again even if you didn't

00:17:36,970 --> 00:17:45,880
touch anything so I hope that this

00:17:40,840 --> 00:17:48,910
example gives you good insights on the

00:17:45,880 --> 00:17:50,680
actual problem so for IO cost guarantee

00:17:48,910 --> 00:17:53,170
in controlling a also guaranteeing

00:17:50,680 --> 00:17:55,570
fairness and reaching high true beauties

00:17:53,170 --> 00:17:58,240
as difficult as getting the shower

00:17:55,570 --> 00:18:01,690
temperature right in in my example

00:17:58,240 --> 00:18:03,520
building this area now what we'll focus

00:18:01,690 --> 00:18:05,980
and they will show you some details only

00:18:03,520 --> 00:18:08,170
for one failure the to failure if you

00:18:05,980 --> 00:18:12,010
want your extra slides about the other

00:18:08,170 --> 00:18:16,150
favor let me tell you very very

00:18:12,010 --> 00:18:19,330
succinctly the emitters used by your

00:18:16,150 --> 00:18:21,430
hospital I will meters so you cost us

00:18:19,330 --> 00:18:23,800
the true go through a parameter named V

00:18:21,430 --> 00:18:27,490
rate which is computed as a function of

00:18:23,800 --> 00:18:29,530
the busy level of the drive that is how

00:18:27,490 --> 00:18:31,900
busy the drive is and as a function of

00:18:29,530 --> 00:18:35,080
how many groups are lagging behind their

00:18:31,900 --> 00:18:38,320
target service and finally as a function

00:18:35,080 --> 00:18:40,960
of the device saturation computed

00:18:38,320 --> 00:18:43,270
through latency of i/o but I don't want

00:18:40,960 --> 00:18:46,060
to go again not to give you too many

00:18:43,270 --> 00:18:47,820
details but this busy level and number

00:18:46,060 --> 00:18:51,190
of groups lagging are a function of the

00:18:47,820 --> 00:18:53,650
estimated service received by troop so

00:18:51,190 --> 00:18:56,200
they are a function of your costs but

00:18:53,650 --> 00:18:57,970
are your costs are subject to high

00:18:56,200 --> 00:19:00,700
imprecision for all the reasons that

00:18:57,970 --> 00:19:04,480
I've already told you and they are time

00:19:00,700 --> 00:19:07,240
variable so this is what happens this is

00:19:04,480 --> 00:19:09,910
a trace of what happens in the failure

00:19:07,240 --> 00:19:11,680
where there are only reads so the tube

00:19:09,910 --> 00:19:14,500
failure we don't only reads that I

00:19:11,680 --> 00:19:18,640
showed you you can see that the red line

00:19:14,500 --> 00:19:21,790
is the v-ray so how the controller sets

00:19:18,640 --> 00:19:24,130
the speed of the drive and as you can

00:19:21,790 --> 00:19:26,590
see at the beginning everything is okay

00:19:24,130 --> 00:19:28,060
but then there is this drop here and the

00:19:26,590 --> 00:19:31,150
drop happens because as you can see

00:19:28,060 --> 00:19:33,490
there is this peak in the busy level for

00:19:31,150 --> 00:19:35,950
some reason the controller believes that

00:19:33,490 --> 00:19:38,500
the drivers become suddenly extremely

00:19:35,950 --> 00:19:40,990
busy which is completely false so it

00:19:38,500 --> 00:19:43,750
drops the berate so it just slowdowns

00:19:40,990 --> 00:19:46,180
the drive for no reason and then it

00:19:43,750 --> 00:19:48,430
keeps it very low then there is some

00:19:46,180 --> 00:19:51,190
other fluctuation where it seems to

00:19:48,430 --> 00:19:53,800
realize that actually the busy lever is

00:19:51,190 --> 00:19:55,870
lower but then when it tries to increase

00:19:53,800 --> 00:19:58,960
the rate again it Peaks so the village

00:19:55,870 --> 00:20:02,659
goes down one then again here a very

00:19:58,960 --> 00:20:04,999
high peak even lower we

00:20:02,659 --> 00:20:07,729
then it tries to increase the gain the

00:20:04,999 --> 00:20:09,709
rates but there is a high number that

00:20:07,729 --> 00:20:13,549
you can see with the green here at high

00:20:09,709 --> 00:20:15,889
number of groups lagging behind their

00:20:13,549 --> 00:20:18,200
target service and in this case it's

00:20:15,889 --> 00:20:21,619
harder because of the algorithm to

00:20:18,200 --> 00:20:24,409
increase B rate quickly so they had the

00:20:21,619 --> 00:20:26,899
ultimate result is that on average the

00:20:24,409 --> 00:20:28,669
throughput is extremely low for no

00:20:26,899 --> 00:20:32,059
reason the drug is completely

00:20:28,669 --> 00:20:33,979
underutilized so that's the reason for

00:20:32,059 --> 00:20:35,869
the failure similar failure with the

00:20:33,979 --> 00:20:37,909
rights but I don't want to steal time to

00:20:35,869 --> 00:20:42,379
ender so believe me this is the same

00:20:37,909 --> 00:20:44,629
problem with different cures so to wrap

00:20:42,379 --> 00:20:47,119
up are you questioning your latency for

00:20:44,629 --> 00:20:50,089
sure have been crafted very stiffly and

00:20:47,119 --> 00:20:52,190
they've been tuned to the temperature

00:20:50,089 --> 00:20:54,379
rights on the machines of the orders of

00:20:52,190 --> 00:20:59,299
dis controllers but if the transfer

00:20:54,379 --> 00:21:01,279
function varies then these it may be

00:20:59,299 --> 00:21:04,219
hard or even impossible for this

00:21:01,279 --> 00:21:07,339
controller to to win to control uh-oh

00:21:04,219 --> 00:21:09,950
why does B if you make it I will leave

00:21:07,339 --> 00:21:11,239
this on Hector slides that I don't want

00:21:09,950 --> 00:21:13,070
to comment on this if you want I will

00:21:11,239 --> 00:21:14,539
tell you now I don't want to steal time

00:21:13,070 --> 00:21:17,299
- understood

00:21:14,539 --> 00:21:23,200
for me this is all this is all for my

00:21:17,299 --> 00:21:30,979
part and I hand it to 2-under so let me

00:21:23,200 --> 00:21:34,459
switch back to fullscreen and stop

00:21:30,979 --> 00:21:37,549
sharing that's ok Carlo antes can just

00:21:34,459 --> 00:21:39,609
stop sharing straight ok thank you thank

00:21:37,549 --> 00:21:39,609
you

00:22:01,919 --> 00:22:19,149
effort how we try to test powders stuff

00:22:10,479 --> 00:22:24,039
and we use I'm working for a product

00:22:19,149 --> 00:22:27,100
called el Quixote and we today we only

00:22:24,039 --> 00:22:33,849
do functional testing and powders being

00:22:27,100 --> 00:22:37,870
my guinea pig for benchmark tests and as

00:22:33,849 --> 00:22:41,320
public pointed out we found

00:22:37,870 --> 00:22:47,789
I found some issues and to present and

00:22:41,320 --> 00:22:52,999
to find

00:22:47,789 --> 00:23:00,109
versions with some endless loops how to

00:22:52,999 --> 00:23:04,919
how to do this first we think we need or

00:23:00,109 --> 00:23:10,889
I'm gonna say I'm gonna make this really

00:23:04,919 --> 00:23:15,389
short instead and do ask do we need

00:23:10,889 --> 00:23:18,149
can we roll on any platform and do the

00:23:15,389 --> 00:23:22,830
test or do we need to D do we need to

00:23:18,149 --> 00:23:28,679
have one hardware and one peripheral and

00:23:22,830 --> 00:23:31,649
run that over and over again and or can

00:23:28,679 --> 00:23:38,669
we have a pool of devices and

00:23:31,649 --> 00:23:41,070
peripherals to run and that's that

00:23:38,669 --> 00:23:43,859
depends on benchmarks where what

00:23:41,070 --> 00:23:48,809
benchmark what we want benchmark and for

00:23:43,859 --> 00:23:55,679
browsers a sweet we think we can use a

00:23:48,809 --> 00:23:57,979
pool of targets and it's a regression if

00:23:55,679 --> 00:23:57,979
it's

00:23:59,590 --> 00:24:04,770
all them 10%

00:24:05,169 --> 00:24:16,610
that's what we gonna start with at least

00:24:08,710 --> 00:24:21,799
and we we will be reusing we will use a

00:24:16,610 --> 00:24:25,760
tool called squad client to do demos

00:24:21,799 --> 00:24:30,460
reporting at them as well but I think

00:24:25,760 --> 00:24:30,460

YouTube URL: https://www.youtube.com/watch?v=Zt5zmYHcx9k


