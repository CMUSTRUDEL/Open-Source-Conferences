Title: BUD17-111 A Scalable Software Scheduler
Publication date: 2017-03-11
Playlist: Linaro Connect Budapest 2017
Description: 
	"Session ID: BUD17-111
Session Name: A Scalable Software Scheduler - BUD17-111
Speaker: 
Track: LNG


★ Session Summary ★
High performance implementation of scheduling algorithm
---------------------------------------------------
★ Resources ★
Event Page: http://connect.linaro.org/resource/bud17/bud17-111/
Presentation: 
Video: https://youtu.be/5YQ_Ua3XT-0
 ---------------------------------------------------

★ Event Details ★
Linaro Connect Budapest 2017 (BUD17)
6-10 March 2017
Corinthia Hotel, Budapest,
Erzsébet krt. 43-49,
1073 Hungary

---------------------------------------------------
Keyword: Scalable, Software, Scheduler, LNG, networking
http://www.linaro.org
http://connect.linaro.org
---------------------------------------------------
Follow us on Social Media
https://www.facebook.com/LinaroOrg
https://twitter.com/linaroorg
https://www.youtube.com/user/linaroorg?sub_confirmation=1
https://www.linkedin.com/company/1026961"
Captions: 
	00:00:00,130 --> 00:00:04,069
[Music]

00:00:08,910 --> 00:00:15,559
[Music]

00:00:11,170 --> 00:00:17,480
okay welcome my name is Ola little I

00:00:15,559 --> 00:00:21,080
work for me I think most of you who know

00:00:17,480 --> 00:00:23,509
me already I'm here to talk about the

00:00:21,080 --> 00:00:25,430
scalable software scheduler actually I'm

00:00:23,509 --> 00:00:28,040
here to talk about scalable software in

00:00:25,430 --> 00:00:30,470
general we're just using the scheduler

00:00:28,040 --> 00:00:34,850
as an example I thought that would be

00:00:30,470 --> 00:00:37,880
more of generic interest so I didn't

00:00:34,850 --> 00:00:39,320
really know the audience it seems to be

00:00:37,880 --> 00:00:41,420
a lot of familiar faces but let's start

00:00:39,320 --> 00:00:43,550
with what does a scheduler do

00:00:41,420 --> 00:00:45,739
so this scheduler is a work scheduler it

00:00:43,550 --> 00:00:48,829
schedules events such as packets

00:00:45,739 --> 00:00:52,280
timeouts notifications from queues to

00:00:48,829 --> 00:00:52,910
threads and since there are often many

00:00:52,280 --> 00:00:55,359
threads

00:00:52,910 --> 00:00:58,010
that's an interesting problem but this

00:00:55,359 --> 00:01:00,739
work scheduler it provides parallelism

00:00:58,010 --> 00:01:04,640
so you can use multiple CPUs efficiently

00:01:00,739 --> 00:01:07,430
and the idea is also to avoid single

00:01:04,640 --> 00:01:09,560
threaded bottlenecks and avoid singer

00:01:07,430 --> 00:01:11,090
threaded performance requirements that

00:01:09,560 --> 00:01:13,280
you might have when you're if you're not

00:01:11,090 --> 00:01:16,820
using a scheduler based program model

00:01:13,280 --> 00:01:18,740
and open data plane work schedule also

00:01:16,820 --> 00:01:20,780
provides some additional features such

00:01:18,740 --> 00:01:23,930
as order restoration and mutual

00:01:20,780 --> 00:01:28,070
exclusion you cannot just do unlimited

00:01:23,930 --> 00:01:30,649
parallel processing the results might

00:01:28,070 --> 00:01:32,540
not be exactly what you want if Fred's

00:01:30,649 --> 00:01:35,360
justcome process packets in parallel

00:01:32,540 --> 00:01:36,680
without synchronization so the work

00:01:35,360 --> 00:01:38,450
scheduler if you have a dedicated

00:01:36,680 --> 00:01:41,180
networking as you see you might have a

00:01:38,450 --> 00:01:47,390
work schedule in hardware that's a

00:01:41,180 --> 00:01:50,390
common feature if you don't have a

00:01:47,390 --> 00:01:57,250
dedicated networking as to see that's

00:01:50,390 --> 00:01:59,329
where this comes in so the reason why I

00:01:57,250 --> 00:02:01,640
designed this scalable software

00:01:59,329 --> 00:02:03,469
scheduler is is to also support those

00:02:01,640 --> 00:02:06,950
platforms that lack a hardware scheduler

00:02:03,469 --> 00:02:09,050
and such such pastro

00:02:06,950 --> 00:02:12,320
forms typically server platforms and if

00:02:09,050 --> 00:02:14,150
you look at current and future on base

00:02:12,320 --> 00:02:18,940
server platforms they have a lot of CPUs

00:02:14,150 --> 00:02:22,490
48 to 64 CPUs is seems to be the norm

00:02:18,940 --> 00:02:23,120
that's a lot of CPUs to get to a keep

00:02:22,490 --> 00:02:28,400
fed

00:02:23,120 --> 00:02:30,440
and we want to promote the unified

00:02:28,400 --> 00:02:35,390
program model on both server and

00:02:30,440 --> 00:02:37,400
networking platforms it's otherwise you

00:02:35,390 --> 00:02:39,069
might up and end up in a situation where

00:02:37,400 --> 00:02:42,650
you use like it scheduler based

00:02:39,069 --> 00:02:44,540
programming module when you run on a

00:02:42,650 --> 00:02:47,810
networking associate but if you run your

00:02:44,540 --> 00:02:49,819
application on a server as you see you

00:02:47,810 --> 00:02:51,560
might use a very different simplified

00:02:49,819 --> 00:02:54,049
programming model and you can't just

00:02:51,560 --> 00:02:56,120
port your application recompile it for

00:02:54,049 --> 00:02:58,040
the networking platform basically how to

00:02:56,120 --> 00:02:59,959
rewrite large parts of your application

00:02:58,040 --> 00:03:02,239
if you have different program models so

00:02:59,959 --> 00:03:04,700
the purpose here is to provide the same

00:03:02,239 --> 00:03:07,879
programming model efficiently on both

00:03:04,700 --> 00:03:13,730
platforms that lack and have hardware

00:03:07,879 --> 00:03:19,519
schedulers or have schedulers and some

00:03:13,730 --> 00:03:21,260
additional reasons for designing and

00:03:19,519 --> 00:03:24,379
researching schedules of the scheduler

00:03:21,260 --> 00:03:26,810
is actually to to solve interesting

00:03:24,379 --> 00:03:28,730
problems and to understand what those

00:03:26,810 --> 00:03:32,109
problems what are required from their

00:03:28,730 --> 00:03:35,420
solutions so in the process of designing

00:03:32,109 --> 00:03:37,790
this Cannibals of the scheduler I have

00:03:35,420 --> 00:03:39,260
actually invented a number of other

00:03:37,790 --> 00:03:42,139
interesting solutions such as no

00:03:39,260 --> 00:03:46,130
blocking ring buffers and block free no

00:03:42,139 --> 00:03:47,599
blocking order restoration designs this

00:03:46,130 --> 00:03:49,220
is this leads to a lot of fun

00:03:47,599 --> 00:03:55,549
programming that's always the best

00:03:49,220 --> 00:03:57,079
projects what limits multi-threaded

00:03:55,549 --> 00:03:59,959
scalability why can't you just throw

00:03:57,079 --> 00:04:03,950
these 64 CPUs on to your problem and it

00:03:59,959 --> 00:04:05,230
just runs 64 times as fast you have a

00:04:03,950 --> 00:04:12,680
lot of shared data in your application

00:04:05,230 --> 00:04:15,380
likely and if you protect those critical

00:04:12,680 --> 00:04:19,940
regions with locks in order to get muted

00:04:15,380 --> 00:04:21,979
solution that creates a single threaded

00:04:19,940 --> 00:04:23,870
bottleneck because only one thread at a

00:04:21,979 --> 00:04:27,050
time can execute this a critical section

00:04:23,870 --> 00:04:29,210
that's protected by the lock you often

00:04:27,050 --> 00:04:34,219
have the ordering requirements in

00:04:29,210 --> 00:04:36,169
different operations or functions for

00:04:34,219 --> 00:04:37,070
instance one example here is the ring

00:04:36,169 --> 00:04:38,360
buffer which is

00:04:37,070 --> 00:04:41,030
one way to implement the

00:04:38,360 --> 00:04:44,270
producer-consumer cue the ring buffer

00:04:41,030 --> 00:04:46,610
update has a number of steps there is an

00:04:44,270 --> 00:04:48,410
acquire step where you require parts

00:04:46,610 --> 00:04:54,880
slots in your ring buffer and there is a

00:04:48,410 --> 00:04:54,880
release step and these two steps must be

00:04:55,900 --> 00:05:01,490
executed in order so to say that you

00:04:58,700 --> 00:05:03,680
need a thread needs to release ring

00:05:01,490 --> 00:05:06,170
buffer updates in in the order that

00:05:03,680 --> 00:05:08,660
these slots were acquired otherwise the

00:05:06,170 --> 00:05:10,250
consumer would get an improper view so

00:05:08,660 --> 00:05:13,820
this creates ordering problems we have

00:05:10,250 --> 00:05:17,080
multiple threads that operate on the

00:05:13,820 --> 00:05:19,810
same ring buffer and there are also

00:05:17,080 --> 00:05:23,090
situations where you you might have

00:05:19,810 --> 00:05:24,710
independent or you might have atomic

00:05:23,090 --> 00:05:27,620
operations that have dependencies you

00:05:24,710 --> 00:05:31,970
want this to appear in a more or less

00:05:27,620 --> 00:05:35,590
atomic way if if you perform one

00:05:31,970 --> 00:05:39,410
operation which is itself perhaps a a

00:05:35,590 --> 00:05:42,140
multi thread safe operation then you

00:05:39,410 --> 00:05:45,080
also want to combine that with a

00:05:42,140 --> 00:05:47,930
separate dependent operation that needs

00:05:45,080 --> 00:05:50,330
to be performed together and this I call

00:05:47,930 --> 00:05:50,720
this atomicity it's it's a very loaded

00:05:50,330 --> 00:05:53,120
word

00:05:50,720 --> 00:06:00,260
it could mean many things but these

00:05:53,120 --> 00:06:02,180
different issues with software effects

00:06:00,260 --> 00:06:05,750
your multi-threaded scalability and if

00:06:02,180 --> 00:06:08,330
you look at the diagram here using the

00:06:05,750 --> 00:06:11,570
lock based but it's similar for all of

00:06:08,330 --> 00:06:13,790
these problems basically your system

00:06:11,570 --> 00:06:16,100
throughput gets limited by this critical

00:06:13,790 --> 00:06:18,800
section there's only one thread at a

00:06:16,100 --> 00:06:22,010
time which then hands over with some

00:06:18,800 --> 00:06:23,900
delay to the next thread and if with

00:06:22,010 --> 00:06:25,670
some probability the more friends you

00:06:23,900 --> 00:06:30,260
have the more threads will be a spin

00:06:25,670 --> 00:06:32,120
waiting to get its turn to enter this

00:06:30,260 --> 00:06:36,410
critical section or perform this update

00:06:32,120 --> 00:06:38,630
that's ordered so that's how software

00:06:36,410 --> 00:06:41,300
design can limit your multi-threaded

00:06:38,630 --> 00:06:44,360
scalability and these are not problems

00:06:41,300 --> 00:06:49,160
you see when you have just a few CPUs a

00:06:44,360 --> 00:06:50,810
few threats but these problems do

00:06:49,160 --> 00:06:53,150
manifest themselves I've been running

00:06:50,810 --> 00:06:57,260
like 16 CPU systems and already there

00:06:53,150 --> 00:06:59,080
this you can see how limitations in

00:06:57,260 --> 00:07:01,790
software such as these limitations

00:06:59,080 --> 00:07:05,740
affect the scalability achieved

00:07:01,790 --> 00:07:08,210
throughput very noticeable

00:07:05,740 --> 00:07:12,200
there are also hardware limitations that

00:07:08,210 --> 00:07:13,850
affect multi-threaded scalability and

00:07:12,200 --> 00:07:20,990
most of this has to do with the cache

00:07:13,850 --> 00:07:24,440
coherency that when I all the CPUs they

00:07:20,990 --> 00:07:27,440
have caches and these caches are kept

00:07:24,440 --> 00:07:30,820
coherent and unfortunately if a CPU is

00:07:27,440 --> 00:07:35,330
going to update something a location

00:07:30,820 --> 00:07:38,930
that modified line can only exist in one

00:07:35,330 --> 00:07:40,760
cache at a time the CPU needs to when he

00:07:38,930 --> 00:07:44,450
wants to write it needs to acquire the

00:07:40,760 --> 00:07:46,310
right to modify the cache line and all

00:07:44,450 --> 00:07:50,000
other copies are then invalidated and

00:07:46,310 --> 00:07:54,260
the CPU has the only instance of that

00:07:50,000 --> 00:07:57,979
cache line and this means there are

00:07:54,260 --> 00:07:59,150
cache coherency transactions is acquired

00:07:57,979 --> 00:08:02,210
the right to modify the cache line

00:07:59,150 --> 00:08:04,310
invalidate all other cop copies those

00:08:02,210 --> 00:08:06,320
are messages that are transferred over

00:08:04,310 --> 00:08:09,110
the interconnect to other caches in the

00:08:06,320 --> 00:08:10,880
system not all caches have to be

00:08:09,110 --> 00:08:12,470
associated with CPUs that's why we call

00:08:10,880 --> 00:08:14,690
them caches it's not necessarily the

00:08:12,470 --> 00:08:16,400
CPUs that are involved and these

00:08:14,690 --> 00:08:17,990
interconnect latencies because it's

00:08:16,400 --> 00:08:20,180
talking between cache is talking to in

00:08:17,990 --> 00:08:21,440
CPUs is not instantaneous that

00:08:20,180 --> 00:08:27,080
contributed to the scalability

00:08:21,440 --> 00:08:30,320
bottleneck us the this modified cache

00:08:27,080 --> 00:08:34,640
line is being transported around or the

00:08:30,320 --> 00:08:36,169
right to date is being granted the quest

00:08:34,640 --> 00:08:39,500
you can ask wise why don't you normally

00:08:36,169 --> 00:08:41,690
see this problem in normal thing a

00:08:39,500 --> 00:08:44,480
further code is because if when you

00:08:41,690 --> 00:08:46,760
write to a cache when you perform a

00:08:44,480 --> 00:08:49,190
normal write a normal store operation

00:08:46,760 --> 00:08:51,950
that complete in the background if you

00:08:49,190 --> 00:08:55,070
have a cache miss if if the CPU needs or

00:08:51,950 --> 00:08:57,380
the cache needs write permission to the

00:08:55,070 --> 00:08:59,209
cache line the software doesn't see that

00:08:57,380 --> 00:09:01,310
because it all happens in the background

00:08:59,209 --> 00:09:04,070
in the load store unit and in the cache

00:09:01,310 --> 00:09:04,530
of the CPU so there all the interconnect

00:09:04,070 --> 00:09:06,750
layton

00:09:04,530 --> 00:09:08,550
or hidden from software until you

00:09:06,750 --> 00:09:10,530
perform too many stores and you fill up

00:09:08,550 --> 00:09:15,240
your internal use but normally that's

00:09:10,530 --> 00:09:16,950
not a problem you also have some aspects

00:09:15,240 --> 00:09:19,110
of hardware that is of interest to

00:09:16,950 --> 00:09:21,750
multi-threaded programming you have

00:09:19,110 --> 00:09:23,580
atomic read-modify-write operations such

00:09:21,750 --> 00:09:25,860
as compare and swap they're basically

00:09:23,580 --> 00:09:28,680
synchronous operations that cannot

00:09:25,860 --> 00:09:33,870
complete in the background the reason is

00:09:28,680 --> 00:09:36,090
that they return a value that value may

00:09:33,870 --> 00:09:39,360
depend on for instance if the CPU

00:09:36,090 --> 00:09:42,030
managed to get this right access and and

00:09:39,360 --> 00:09:44,070
it also if you could perform a compare

00:09:42,030 --> 00:09:45,780
and swap maybe as I said is to compare

00:09:44,070 --> 00:09:48,120
maybe this compare doesn't succeed and

00:09:45,780 --> 00:09:52,500
the comparable corporation fails these

00:09:48,120 --> 00:09:54,420
aspects of these instructors makes it

00:09:52,500 --> 00:09:55,950
impossible to complete them in the

00:09:54,420 --> 00:09:58,200
background and hide the interconnect

00:09:55,950 --> 00:10:01,110
latencies instead basically there when

00:09:58,200 --> 00:10:03,210
it's time to retire this comparison of

00:10:01,110 --> 00:10:05,670
instruction or or math similar

00:10:03,210 --> 00:10:08,400
instructions knows to exclusive the CPU

00:10:05,670 --> 00:10:11,130
has to wait until it knows that the

00:10:08,400 --> 00:10:14,700
write actually succeeded and there are a

00:10:11,130 --> 00:10:16,740
number of them scenarios where it

00:10:14,700 --> 00:10:19,050
doesn't succeed and these status must

00:10:16,740 --> 00:10:23,970
then be returned to the application to

00:10:19,050 --> 00:10:26,490
software and it's many modern CPUs do a

00:10:23,970 --> 00:10:30,690
lot of speculation to to speed up things

00:10:26,490 --> 00:10:31,830
and to hide in this interconnect

00:10:30,690 --> 00:10:33,690
licenses for instance when you have

00:10:31,830 --> 00:10:35,250
cache misses but these instructions are

00:10:33,690 --> 00:10:37,310
more difficult to speculate they have

00:10:35,250 --> 00:10:41,370
global side effects conditional outcomes

00:10:37,310 --> 00:10:43,230
so atomic read-modify-write operations

00:10:41,370 --> 00:10:46,740
such as compare and swap such as load

00:10:43,230 --> 00:10:48,930
store exclusive they are slower much

00:10:46,740 --> 00:10:53,550
slower than other instructions much more

00:10:48,930 --> 00:10:56,730
of the latency in the system is it's

00:10:53,550 --> 00:11:01,080
exposed to software and not only exposed

00:10:56,730 --> 00:11:02,910
to software but becomes part of the

00:11:01,080 --> 00:11:08,340
critical there's this core to a critical

00:11:02,910 --> 00:11:10,440
region in the hardware phrases with it

00:11:08,340 --> 00:11:11,880
as using compare-and-swap as an example

00:11:10,440 --> 00:11:13,980
it's normally preceded by a low

00:11:11,880 --> 00:11:15,300
destruction you perform some computation

00:11:13,980 --> 00:11:18,180
based on the result of this load

00:11:15,300 --> 00:11:24,629
instruction and then you prefer

00:11:18,180 --> 00:11:26,579
perform the comparable operation and the

00:11:24,629 --> 00:11:28,470
latency of this critical section is

00:11:26,579 --> 00:11:30,449
affected by the interconnect latency

00:11:28,470 --> 00:11:31,529
because the load is just a load the

00:11:30,449 --> 00:11:35,040
compare and swap is when you actually

00:11:31,529 --> 00:11:37,620
want to write and that means that in

00:11:35,040 --> 00:11:39,839
this secret this critical section you

00:11:37,620 --> 00:11:45,089
have to wait for other caches to respond

00:11:39,839 --> 00:11:46,470
and if you have a lot long interconnect

00:11:45,089 --> 00:11:48,720
latencies that means your critical

00:11:46,470 --> 00:11:50,610
section is getting bigger so you also

00:11:48,720 --> 00:11:52,740
have critical sections as a part of the

00:11:50,610 --> 00:11:57,180
hardware design not only as part of

00:11:52,740 --> 00:12:01,079
software design and if you do not

00:11:57,180 --> 00:12:03,240
further programming you need to impose

00:12:01,079 --> 00:12:06,629
the right ordering of memory accesses

00:12:03,240 --> 00:12:08,519
you cannot just write data right to some

00:12:06,629 --> 00:12:11,819
flag and then expect consumers who did

00:12:08,519 --> 00:12:14,699
this data to to see the right values so

00:12:11,819 --> 00:12:16,559
you have there are different memory

00:12:14,699 --> 00:12:18,899
consistency models but the popular one

00:12:16,559 --> 00:12:22,939
is called weak it's called release

00:12:18,899 --> 00:12:24,540
consistency and it's it's suited for our

00:12:22,939 --> 00:12:26,189
architecture such as the ARM

00:12:24,540 --> 00:12:28,379
architecture with which has weak memory

00:12:26,189 --> 00:12:29,610
ordering and in the release consistence

00:12:28,379 --> 00:12:33,089
you have this acquire and release

00:12:29,610 --> 00:12:36,139
operations when you want to release some

00:12:33,089 --> 00:12:40,019
updates you perform your normal writes

00:12:36,139 --> 00:12:41,600
the releasing store to some flag or some

00:12:40,019 --> 00:12:44,220
other variable that is this

00:12:41,600 --> 00:12:49,319
synchronization point that has released

00:12:44,220 --> 00:12:52,829
ordering and this operation this store

00:12:49,319 --> 00:12:55,350
release what may have to stall the CPU

00:12:52,829 --> 00:12:57,360
to be sure that all preceding stores are

00:12:55,350 --> 00:12:59,600
now visible by all other agents all

00:12:57,360 --> 00:13:03,209
other caches and CPUs in the system and

00:12:59,600 --> 00:13:05,519
that have to wait for other agents on

00:13:03,209 --> 00:13:07,620
the intake under on this system on a

00:13:05,519 --> 00:13:13,319
shape on the interconnect that can be as

00:13:07,620 --> 00:13:16,139
no thing and correspondingly the the

00:13:13,319 --> 00:13:17,939
consumer of such an update has to

00:13:16,139 --> 00:13:22,079
perform a load acquire or some other

00:13:17,939 --> 00:13:26,129
acquire ordering operation to make be

00:13:22,079 --> 00:13:28,429
sure that loads from this that depends

00:13:26,129 --> 00:13:28,429
on this

00:13:28,680 --> 00:13:35,019
update don't see this that earlier

00:13:32,190 --> 00:13:39,970
incorrect data only sees the new updated

00:13:35,019 --> 00:13:43,649
data so this acquire ordering prevents

00:13:39,970 --> 00:13:46,990
the CPU from speculating loads from from

00:13:43,649 --> 00:13:48,550
from from accessing the caches earlier

00:13:46,990 --> 00:13:53,170
in in order to decrease cache miss

00:13:48,550 --> 00:13:58,630
latency so this inter core into Fred

00:13:53,170 --> 00:14:00,190
communication it's slower because of

00:13:58,630 --> 00:14:03,430
this you have to do this acquire and

00:14:00,190 --> 00:14:06,730
release or during memory accesses and

00:14:03,430 --> 00:14:08,230
the picture here shows the demo SC cache

00:14:06,730 --> 00:14:14,970
coherency protocol which is basically

00:14:08,230 --> 00:14:17,410
what the arm Amba specification uses and

00:14:14,970 --> 00:14:24,220
there are a number of states for a cache

00:14:17,410 --> 00:14:29,380
line it's interesting that in this where

00:14:24,220 --> 00:14:33,070
the protocol many CPUs can actually have

00:14:29,380 --> 00:14:35,230
a cache line in shared State but there

00:14:33,070 --> 00:14:37,240
can still also be one CPU that has that

00:14:35,230 --> 00:14:41,339
cache line in what is called their own

00:14:37,240 --> 00:14:42,910
state where it has to the right to

00:14:41,339 --> 00:14:47,529
modify the cache line

00:14:42,910 --> 00:14:50,550
this is like the a protocol that is this

00:14:47,529 --> 00:14:53,519
optimized for multiprocessor systems

00:14:50,550 --> 00:15:00,040
there are other cash currency protocols

00:14:53,519 --> 00:15:04,899
as well so now based on these hardware

00:15:00,040 --> 00:15:08,850
and software limitations I have written

00:15:04,899 --> 00:15:08,850
down some of the design rules that I

00:15:09,000 --> 00:15:15,420
derived from the way hardware software

00:15:12,209 --> 00:15:19,720
in order to achieve better scalability

00:15:15,420 --> 00:15:21,399
you want to separate variables you have

00:15:19,720 --> 00:15:24,670
some variables that you read and write

00:15:21,399 --> 00:15:26,350
to and you have other variables that

00:15:24,670 --> 00:15:29,050
might only be that might be read-only

00:15:26,350 --> 00:15:30,880
you only read from it often makes sense

00:15:29,050 --> 00:15:33,339
to put this into different cache lines

00:15:30,880 --> 00:15:36,970
so that the ones that are read-only can

00:15:33,339 --> 00:15:38,920
always be cached it also of course

00:15:36,970 --> 00:15:40,870
depends on the on the capacity of the

00:15:38,920 --> 00:15:43,570
cache because these

00:15:40,870 --> 00:15:45,670
variables that are written if some other

00:15:43,570 --> 00:15:47,529
CPU writes them they will be invalidated

00:15:45,670 --> 00:15:52,260
from your cache you don't want that

00:15:47,529 --> 00:15:55,960
invalidation to remove data that that

00:15:52,260 --> 00:16:00,339
you might be accessing that is actually

00:15:55,960 --> 00:16:02,440
not related so this is the default

00:16:00,339 --> 00:16:03,910
sharing problem that you put too many

00:16:02,440 --> 00:16:05,440
unrelated things in the same cache line

00:16:03,910 --> 00:16:07,990
or they might even be related in a way

00:16:05,440 --> 00:16:10,630
but you don't want your read-only data

00:16:07,990 --> 00:16:12,130
to be invalidated just because some

00:16:10,630 --> 00:16:15,339
other piece of data happens to be

00:16:12,130 --> 00:16:16,960
written on my conceptually they might be

00:16:15,339 --> 00:16:21,550
related but from a pure hardware

00:16:16,960 --> 00:16:24,190
perspective if you write one location

00:16:21,550 --> 00:16:29,440
you want to be able to have quick access

00:16:24,190 --> 00:16:31,779
to other locations of data there is also

00:16:29,440 --> 00:16:33,460
if you have multiple locations that you

00:16:31,779 --> 00:16:36,700
write to if if they're written at

00:16:33,460 --> 00:16:38,110
different times far away in time it

00:16:36,700 --> 00:16:40,330
makes sense to put them in different

00:16:38,110 --> 00:16:43,180
cache lines if you if you have two

00:16:40,330 --> 00:16:44,740
locations that you write to basically

00:16:43,180 --> 00:16:45,880
immediately after each other it makes

00:16:44,740 --> 00:16:47,230
sense to put them in the same cache line

00:16:45,880 --> 00:16:51,130
it's just one cache line you have to

00:16:47,230 --> 00:16:53,170
retrieve and when you perform the first

00:16:51,130 --> 00:16:54,820
write than the second write when the

00:16:53,170 --> 00:16:56,650
second write is performed that cache

00:16:54,820 --> 00:16:58,330
line is most likely in your cache

00:16:56,650 --> 00:17:02,560
because of the first write but the

00:16:58,330 --> 00:17:04,510
further way these two writes are the the

00:17:02,560 --> 00:17:06,490
larger the probability it's a beneficial

00:17:04,510 --> 00:17:08,439
to actually put these two locations in

00:17:06,490 --> 00:17:11,260
different cache lines your write to your

00:17:08,439 --> 00:17:15,790
first cache line which means that the

00:17:11,260 --> 00:17:17,800
CPU needs the access it needs to

00:17:15,790 --> 00:17:20,140
modified let's go back to this it needs

00:17:17,800 --> 00:17:26,980
to the modified the cache line in

00:17:20,140 --> 00:17:29,559
modified state and when it performs the

00:17:26,980 --> 00:17:31,420
write to the second location some other

00:17:29,559 --> 00:17:35,140
thread might already have written to

00:17:31,420 --> 00:17:37,390
that first location and invalidate that

00:17:35,140 --> 00:17:39,070
your copy of the cache line so then it's

00:17:37,390 --> 00:17:41,350
actually better for you to perform a

00:17:39,070 --> 00:17:42,820
right to a different cache line that

00:17:41,350 --> 00:17:44,080
increases the scalability now you have

00:17:42,820 --> 00:17:46,090
two different cache lines that can be

00:17:44,080 --> 00:17:47,500
written concurrently basically the more

00:17:46,090 --> 00:17:50,140
threads you have the more you want to

00:17:47,500 --> 00:17:51,910
spread out your rights if all your

00:17:50,140 --> 00:17:53,909
rights are to the same cache line that

00:17:51,910 --> 00:17:56,320
cache lines becomes a point of

00:17:53,909 --> 00:17:58,169
so spreading out is spreading out his

00:17:56,320 --> 00:18:02,769
right targets over multiple cache lines

00:17:58,169 --> 00:18:09,369
it's part of the design rules for

00:18:02,769 --> 00:18:13,509
scalability there are some code

00:18:09,369 --> 00:18:16,179
sequences where you conceptually you

00:18:13,509 --> 00:18:17,950
might read a cache line you might read a

00:18:16,179 --> 00:18:19,840
variable and then just write it

00:18:17,950 --> 00:18:21,909
immediately after and this is typically

00:18:19,840 --> 00:18:23,259
if you have a ticket look when you're

00:18:21,909 --> 00:18:25,509
releasing that ticket look you're just

00:18:23,259 --> 00:18:30,369
incrementing the value of the ticket

00:18:25,509 --> 00:18:34,090
look current counter and it's you often

00:18:30,369 --> 00:18:36,669
see code they do in this case it doesn't

00:18:34,090 --> 00:18:39,820
have to be an atomic increment it's just

00:18:36,669 --> 00:18:41,440
read add one right back but the problem

00:18:39,820 --> 00:18:43,509
here now

00:18:41,440 --> 00:18:47,999
when you perform a read the CPU needs to

00:18:43,509 --> 00:18:50,470
to fetch this cache line in shared mode

00:18:47,999 --> 00:18:52,389
otherwise you can't return the value of

00:18:50,470 --> 00:18:58,450
the load so that you can perform your

00:18:52,389 --> 00:18:59,980
addition and so that you're not hiding

00:18:58,450 --> 00:19:02,889
the latency of retrieving the cache line

00:18:59,980 --> 00:19:05,399
and also when you then perform your

00:19:02,889 --> 00:19:08,200
write to the cache line the CPU must

00:19:05,399 --> 00:19:11,100
acquire the write permission to this

00:19:08,200 --> 00:19:14,109
cache line and Andy validates any other

00:19:11,100 --> 00:19:16,389
copies of that cache line you're writing

00:19:14,109 --> 00:19:19,690
to so it's actually better just to in

00:19:16,389 --> 00:19:24,609
this case as a specific case you cache

00:19:19,690 --> 00:19:28,869
that value of of your ticket for

00:19:24,609 --> 00:19:30,669
instance and just perform a write of the

00:19:28,869 --> 00:19:33,399
new value without the without the

00:19:30,669 --> 00:19:35,019
previous load this means that the write

00:19:33,399 --> 00:19:37,960
can complete in the background you don't

00:19:35,019 --> 00:19:39,999
have actually to have to retrieve the

00:19:37,960 --> 00:19:42,580
cache line first in shared mode and then

00:19:39,999 --> 00:19:48,580
later get the write permission you can

00:19:42,580 --> 00:19:51,059
just acquire the cache line in in an

00:19:48,580 --> 00:19:53,549
exclusive or modified State

00:19:51,059 --> 00:19:58,269
this can also be done in the background

00:19:53,549 --> 00:20:01,690
so that's another small trick that you

00:19:58,269 --> 00:20:03,049
can avoid some of these cache coherency

00:20:01,690 --> 00:20:06,020
messages and

00:20:03,049 --> 00:20:13,100
the latency involved in the Koshka has a

00:20:06,020 --> 00:20:18,320
signaling there are related to this one

00:20:13,100 --> 00:20:20,029
if you is another small trick sometimes

00:20:18,320 --> 00:20:22,159
useful is actually the CPU can

00:20:20,029 --> 00:20:24,380
explicitly prefetch for store to fetch

00:20:22,159 --> 00:20:27,559
this cache line earlier if there is a

00:20:24,380 --> 00:20:29,510
potential for cache miss so if there is

00:20:27,559 --> 00:20:32,870
a because if you have a potential for

00:20:29,510 --> 00:20:34,250
cache miss the CPU normally if you

00:20:32,870 --> 00:20:36,080
perform a load and then you're later

00:20:34,250 --> 00:20:37,970
might want to store it you as I said you

00:20:36,080 --> 00:20:41,440
the CPU fetches the cache lining in

00:20:37,970 --> 00:20:45,429
shared mode and then it needs to to

00:20:41,440 --> 00:20:47,450
transfer that into the modified State

00:20:45,429 --> 00:20:48,860
but if you know we are going to write

00:20:47,450 --> 00:20:51,320
the cache line even if you come to read

00:20:48,860 --> 00:20:53,210
it first you can tell the CPU retrieve

00:20:51,320 --> 00:20:57,080
this cache line in modified state

00:20:53,210 --> 00:21:00,830
immediately so the when you actually

00:20:57,080 --> 00:21:02,720
perform the write that can be done

00:21:00,830 --> 00:21:04,010
without any latency that this decreases

00:21:02,720 --> 00:21:06,500
from a hardware perspective this

00:21:04,010 --> 00:21:09,559
decreases this critical section this is

00:21:06,500 --> 00:21:11,390
the time that the CPU owns this cache

00:21:09,559 --> 00:21:13,100
line before it can be passed on to

00:21:11,390 --> 00:21:14,059
someone else that wants to access and

00:21:13,100 --> 00:21:19,460
modify the cache line

00:21:14,059 --> 00:21:22,070
I mentioned this acquiring release

00:21:19,460 --> 00:21:27,230
orderings that you need when you

00:21:22,070 --> 00:21:32,720
transfer updates between CPUs you

00:21:27,230 --> 00:21:34,490
shouldn't use them more than needed in

00:21:32,720 --> 00:21:36,830
in some cases you can actually perform

00:21:34,490 --> 00:21:38,570
even if you perform an atomic update of

00:21:36,830 --> 00:21:41,539
some variable maybe this update is

00:21:38,570 --> 00:21:44,029
doesn't isn't associated with any kind

00:21:41,539 --> 00:21:46,570
of other memory updates then you can

00:21:44,029 --> 00:21:50,419
actually perform a relaxed operation

00:21:46,570 --> 00:21:53,600
which doesn't have this query release

00:21:50,419 --> 00:21:58,130
ordering of memory accesses and it

00:21:53,600 --> 00:22:00,320
generally is a lot faster this is what

00:21:58,130 --> 00:22:02,419
makes sense on arm CPUs where you have

00:22:00,320 --> 00:22:04,970
the weak memory model if you have any an

00:22:02,419 --> 00:22:08,690
x86 CPUs we're just more less strict

00:22:04,970 --> 00:22:12,649
memory ordering and all of these

00:22:08,690 --> 00:22:14,720
barriers are more or less no ops but I

00:22:12,649 --> 00:22:20,440
work for um I don't work for Intel so

00:22:14,720 --> 00:22:20,440
I'm telling you what's best for arm and

00:22:21,130 --> 00:22:25,130
again something that is different

00:22:23,299 --> 00:22:27,020
between put potentially between

00:22:25,130 --> 00:22:29,270
architectures but also within different

00:22:27,020 --> 00:22:32,779
architectures is how deterministic your

00:22:29,270 --> 00:22:34,309
hardware environment and also of course

00:22:32,779 --> 00:22:38,210
software environment but has a different

00:22:34,309 --> 00:22:42,559
thing in some of the SOC side be running

00:22:38,210 --> 00:22:44,360
on you perform a sequence of two threads

00:22:42,559 --> 00:22:46,370
perform the same sequence operations if

00:22:44,360 --> 00:22:49,100
one starts slightly before the other one

00:22:46,370 --> 00:22:51,649
they will end in the same order because

00:22:49,100 --> 00:22:53,360
they the hardware with memory accesses

00:22:51,649 --> 00:22:55,460
is very deterministic but on other

00:22:53,360 --> 00:22:57,679
systems it's not deterministic just

00:22:55,460 --> 00:22:59,539
because you have two CPUs that reach one

00:22:57,679 --> 00:23:00,950
point in the program in one order when

00:22:59,539 --> 00:23:04,970
they reach the next point in the program

00:23:00,950 --> 00:23:06,830
having done the same computations that

00:23:04,970 --> 00:23:09,010
that would be in a different order the

00:23:06,830 --> 00:23:11,690
hardware is not deterministic some some

00:23:09,010 --> 00:23:13,789
cache misses will take much longer for

00:23:11,690 --> 00:23:16,429
one CPU to serve them for the other cpus

00:23:13,789 --> 00:23:21,440
depending on where those cache misses

00:23:16,429 --> 00:23:23,000
are being served from and this creates

00:23:21,440 --> 00:23:28,940
problems when you have these for

00:23:23,000 --> 00:23:33,380
instance the ordering requirements from

00:23:28,940 --> 00:23:39,230
software because this creates a large

00:23:33,380 --> 00:23:42,139
probability that if you need to perform

00:23:39,230 --> 00:23:44,630
these operations in a certain order for

00:23:42,139 --> 00:23:46,340
instance to leave this ring buffer

00:23:44,630 --> 00:23:52,190
update for instance to release in a

00:23:46,340 --> 00:23:55,520
certain order if the second thread that

00:23:52,190 --> 00:23:57,230
supposedly was started later but might

00:23:55,520 --> 00:23:58,940
finish earlier but it's not allowed to

00:23:57,230 --> 00:24:00,500
finish earlier and what you what can you

00:23:58,940 --> 00:24:03,049
do you have to spin then you're just

00:24:00,500 --> 00:24:08,260
waiting wasting cycles spinning until

00:24:03,049 --> 00:24:12,440
you it's time for the CPU to finish this

00:24:08,260 --> 00:24:14,690
operation that it was performing and and

00:24:12,440 --> 00:24:17,510
we'd look based code this is even more

00:24:14,690 --> 00:24:20,710
of a problem because then the this lock

00:24:17,510 --> 00:24:20,710
based region might take

00:24:21,230 --> 00:24:25,160
much longer than you expected in certain

00:24:23,450 --> 00:24:27,590
cases it's not deterministic

00:24:25,160 --> 00:24:31,220
if the decreases determine decreases

00:24:27,590 --> 00:24:32,750
determinism also this in determinism in

00:24:31,220 --> 00:24:34,250
the hardware starts to affect the

00:24:32,750 --> 00:24:36,590
software it makes the software less

00:24:34,250 --> 00:24:39,290
deterministic and again if you have log

00:24:36,590 --> 00:24:41,570
based code all other threats that want

00:24:39,290 --> 00:24:44,800
to take the same lock will have to wait

00:24:41,570 --> 00:24:56,690
and you want to avoid those dependencies

00:24:44,800 --> 00:24:59,240
between threats so an important data

00:24:56,690 --> 00:25:03,310
type in open data plane or the event

00:24:59,240 --> 00:25:06,860
queues and we want a scalable event use

00:25:03,310 --> 00:25:09,620
if you profile some example you can see

00:25:06,860 --> 00:25:12,980
that it's a lot of time spent in event

00:25:09,620 --> 00:25:15,170
queues actually most of the time and I

00:25:12,980 --> 00:25:18,850
have implemented I'm using ring buffers

00:25:15,170 --> 00:25:21,350
which is a very common data type and

00:25:18,850 --> 00:25:26,810
with separate head and head pointers for

00:25:21,350 --> 00:25:29,480
producers and consumers and if you have

00:25:26,810 --> 00:25:32,330
this separate if when you combine the

00:25:29,480 --> 00:25:34,370
event queues with a scheduler if an

00:25:32,330 --> 00:25:36,740
event queues goes from empty turn to

00:25:34,370 --> 00:25:38,930
non-empty you want we want them to

00:25:36,740 --> 00:25:41,270
signal at this event queue is something

00:25:38,930 --> 00:25:43,880
that we can schedule from and when when

00:25:41,270 --> 00:25:46,100
a consumer DQ's from an event queue and

00:25:43,880 --> 00:25:49,370
that event here goes from non-empty to

00:25:46,100 --> 00:25:50,900
an empty state we want to sing you know

00:25:49,370 --> 00:25:53,840
that this event queue is not of interest

00:25:50,900 --> 00:25:55,540
to the scheduler anymore and we have

00:25:53,840 --> 00:25:58,160
something called scheduler cues for that

00:25:55,540 --> 00:25:59,390
but now we have the problem that we say

00:25:58,160 --> 00:26:01,250
we have this separate head and type

00:25:59,390 --> 00:26:04,610
pointers which is common DPD keyring

00:26:01,250 --> 00:26:06,650
buffers have the same a queue can also

00:26:04,610 --> 00:26:08,390
actually be both empty and non-empty at

00:26:06,650 --> 00:26:11,270
the same time and I call this the

00:26:08,390 --> 00:26:12,680
schrödinger's queue conundrum and it

00:26:11,270 --> 00:26:14,390
creates a problem because how can we

00:26:12,680 --> 00:26:16,880
tell the scheduler if this Q is of

00:26:14,390 --> 00:26:19,640
interest when it can be above empty and

00:26:16,880 --> 00:26:21,680
non-empty at the same time so we need to

00:26:19,640 --> 00:26:23,900
reintroduce some state that is shared

00:26:21,680 --> 00:26:25,520
between the peers on the consumer so

00:26:23,900 --> 00:26:27,920
that they have the same view of this

00:26:25,520 --> 00:26:31,370
queue state if it's empty or not empty

00:26:27,920 --> 00:26:34,340
and this state is a share counter and

00:26:31,370 --> 00:26:34,970
there is a ticket mechanism basically a

00:26:34,340 --> 00:26:42,280
sort of attic

00:26:34,970 --> 00:26:44,840
look and I have a diagram here of the

00:26:42,280 --> 00:26:47,539
the ring buffer but basically how you

00:26:44,840 --> 00:26:49,610
perform the different ring buffer NQ and

00:26:47,539 --> 00:26:51,620
EQ operations I don't really want to go

00:26:49,610 --> 00:26:54,679
into detail here about all the things

00:26:51,620 --> 00:26:57,320
you do but basically we had to produce

00:26:54,679 --> 00:26:59,659
aside here in purple you have head and

00:26:57,320 --> 00:27:01,520
tail pointers for the piercers for the

00:26:59,659 --> 00:27:03,980
consumer side in blue we have the head

00:27:01,520 --> 00:27:06,080
and tail pointers and we have some extra

00:27:03,980 --> 00:27:09,440
shared method eight are the number

00:27:06,080 --> 00:27:11,299
events and the ticket counter and so

00:27:09,440 --> 00:27:12,830
this is the metadata for the ring buffer

00:27:11,299 --> 00:27:19,010
and then you actually have the ring

00:27:12,830 --> 00:27:22,340
itself in the middle and depending on

00:27:19,010 --> 00:27:23,780
how you implement the updates here and

00:27:22,340 --> 00:27:25,789
how you put the data structures you

00:27:23,780 --> 00:27:33,110
actually get wrong different behavior

00:27:25,789 --> 00:27:34,909
so I had benched market I implement the

00:27:33,110 --> 00:27:36,679
dis ring buffer with different

00:27:34,909 --> 00:27:40,940
synchronization primitives I made a look

00:27:36,679 --> 00:27:44,059
based code in which the blue one and I

00:27:40,940 --> 00:27:47,480
have this the yellow the green and the

00:27:44,059 --> 00:27:50,120
purple door all these basically look

00:27:47,480 --> 00:27:52,880
free implementations with separate head

00:27:50,120 --> 00:27:54,890
and tail pointers the question is with

00:27:52,880 --> 00:27:57,110
the log based code the Lockean or the

00:27:54,890 --> 00:27:59,090
metadata sin 1 cache line with a lot

00:27:57,110 --> 00:28:00,549
less code again all that his head and

00:27:59,090 --> 00:28:02,710
tail pointers during the same cache line

00:28:00,549 --> 00:28:04,940
with this I call a split

00:28:02,710 --> 00:28:08,900
producer/consumer and split Kadir super

00:28:04,940 --> 00:28:11,299
Zoomer readwrite I put the the metadata

00:28:08,900 --> 00:28:14,140
in to cache lines and then in for cache

00:28:11,299 --> 00:28:16,850
lines and then I measure the number of

00:28:14,140 --> 00:28:19,340
the basically aggregated number of NQ

00:28:16,850 --> 00:28:22,190
and EQ operations and on the x-axis we

00:28:19,340 --> 00:28:25,850
have the number of threads so I go from

00:28:22,190 --> 00:28:28,010
from 1 to 16 threads and you can see

00:28:25,850 --> 00:28:29,240
that actually the best performance is

00:28:28,010 --> 00:28:31,070
when you only have one thread because

00:28:29,240 --> 00:28:35,480
then everything is in basically in your

00:28:31,070 --> 00:28:38,059
l1 cache but so performance degrades

00:28:35,480 --> 00:28:40,400
because you need to communicate over the

00:28:38,059 --> 00:28:42,380
interconnects the the system throughput

00:28:40,400 --> 00:28:44,570
is affected by the latencies of these

00:28:42,380 --> 00:28:46,549
Kashkari transactions but there is a

00:28:44,570 --> 00:28:47,820
rather big difference this both the log

00:28:46,549 --> 00:28:50,040
based code and

00:28:47,820 --> 00:28:52,350
the standard luckless implementation

00:28:50,040 --> 00:28:54,450
since they both put all that the

00:28:52,350 --> 00:29:00,990
metadata in the same cache line they

00:28:54,450 --> 00:29:03,570
have much lower performance and the the

00:29:00,990 --> 00:29:05,880
two implementations where the metadata

00:29:03,570 --> 00:29:08,270
split in two or even four cache lines

00:29:05,880 --> 00:29:11,490
the green and the purple they they get

00:29:08,270 --> 00:29:15,900
twice the throughput just because the

00:29:11,490 --> 00:29:17,670
more cache lines they have the to spread

00:29:15,900 --> 00:29:19,640
out your rights to the better your

00:29:17,670 --> 00:29:23,810
scalability because then threads can be

00:29:19,640 --> 00:29:29,760
can be writing to different cache lines

00:29:23,810 --> 00:29:34,860
concurrently so here you see that there

00:29:29,760 --> 00:29:38,580
is a my ring buffer throughput degrades

00:29:34,860 --> 00:29:40,970
quickly as the number of threads

00:29:38,580 --> 00:29:46,620
increases you can affect that

00:29:40,970 --> 00:29:48,360
performance how you with how you with

00:29:46,620 --> 00:29:54,360
your layout basically your metadata in

00:29:48,360 --> 00:29:55,590
different cache lines but again if you

00:29:54,360 --> 00:29:57,990
have too many Fred's the throughput

00:29:55,590 --> 00:29:59,850
decreases which is something the

00:29:57,990 --> 00:30:02,490
scalable scheduler wants to avoid so you

00:29:59,850 --> 00:30:04,170
don't want multiple threads accessing

00:30:02,490 --> 00:30:07,770
the same queue with the same ring buffer

00:30:04,170 --> 00:30:09,720
because that degrades your throughput so

00:30:07,770 --> 00:30:14,270
the solution here in the schedule is to

00:30:09,720 --> 00:30:14,270
use different scheduler queues you have

00:30:14,900 --> 00:30:19,980
the blue boxes here are these event

00:30:17,970 --> 00:30:22,110
queues non-empty event queues they are

00:30:19,980 --> 00:30:24,330
mapped to different scheduler queues in

00:30:22,110 --> 00:30:25,710
this case I have a scheduler group with

00:30:24,330 --> 00:30:28,830
two scheduler queues and have two

00:30:25,710 --> 00:30:31,350
threads and each thread has a list of

00:30:28,830 --> 00:30:36,240
scheduler key references there's green

00:30:31,350 --> 00:30:39,450
boxes and if you look at the green

00:30:36,240 --> 00:30:42,120
arrows you can see that the both threads

00:30:39,450 --> 00:30:44,960
refer to both scheduler queues but in

00:30:42,120 --> 00:30:47,340
different order the first thread p1 is

00:30:44,960 --> 00:30:49,170
first looking at the first scheduler Q

00:30:47,340 --> 00:30:51,000
and then if that is empty it's looking

00:30:49,170 --> 00:30:52,740
at the second scheduler queue the second

00:30:51,000 --> 00:30:56,520
thread is first looking at the second

00:30:52,740 --> 00:30:59,670
scheduler queue and if that one is empty

00:30:56,520 --> 00:31:01,410
if there are no eligible event queues it

00:30:59,670 --> 00:31:04,640
will be looking at the first

00:31:01,410 --> 00:31:08,570
like you so this is a way to spread out

00:31:04,640 --> 00:31:11,010
how the threads look at different

00:31:08,570 --> 00:31:13,890
eventually how they access this event

00:31:11,010 --> 00:31:15,680
kills this ring buffers to make sure

00:31:13,890 --> 00:31:19,500
they're actually accessing different

00:31:15,680 --> 00:31:22,980
event kills instead of accessing the

00:31:19,500 --> 00:31:26,730
same one so I will show how this affects

00:31:22,980 --> 00:31:29,370
scalability this is how I implement the

00:31:26,730 --> 00:31:33,150
schedule queues is not that important in

00:31:29,370 --> 00:31:36,860
this case what you have here on the

00:31:33,150 --> 00:31:39,810
x-axis is actually the number of this

00:31:36,860 --> 00:31:43,500
scheduler queues I have I think this

00:31:39,810 --> 00:31:48,030
this benchmark uses 64 event queues that

00:31:43,500 --> 00:31:50,670
are then mapped to from 1 to 16

00:31:48,030 --> 00:31:52,830
scheduler kills so when although event

00:31:50,670 --> 00:31:55,470
queues are mapped to the same scheduler

00:31:52,830 --> 00:31:58,410
queue and I have in this case I have 16

00:31:55,470 --> 00:32:02,460
threads that ask the scheduler for work

00:31:58,410 --> 00:32:05,640
and if these 16 threads have used the

00:32:02,460 --> 00:32:08,610
same scheduler queue they all will try

00:32:05,640 --> 00:32:13,340
to DQ from the same event queue from the

00:32:08,610 --> 00:32:17,250
same ring buffer and as we saw on that

00:32:13,340 --> 00:32:19,890
diagram performance did it wasn't very

00:32:17,250 --> 00:32:24,380
high then so but by increasing the

00:32:19,890 --> 00:32:24,380
number of the scheduler queues in

00:32:24,740 --> 00:32:29,610
basically we can get a linear

00:32:27,480 --> 00:32:32,520
performance increase the more we spread

00:32:29,610 --> 00:32:35,310
out how these threads in this case I

00:32:32,520 --> 00:32:37,050
have 16 threads all the time the more we

00:32:35,310 --> 00:32:39,690
spread them out that so that actually

00:32:37,050 --> 00:32:41,640
work on different event queues process

00:32:39,690 --> 00:32:43,830
different event queues the higher the

00:32:41,640 --> 00:32:47,940
fruit basically linear relationship

00:32:43,830 --> 00:32:51,120
between the fruit boot and not a number

00:32:47,940 --> 00:32:53,280
of scheduled accuse this is what I call

00:32:51,120 --> 00:32:57,420
X Factor how many scheduled accuse we

00:32:53,280 --> 00:32:59,100
spread out the event killzone so in in

00:32:57,420 --> 00:33:00,540
this example here I only had two frets

00:32:59,100 --> 00:33:04,310
and two scheduled accuse but I could

00:33:00,540 --> 00:33:04,310
have 16 threads here

00:33:04,470 --> 00:33:16,890
so for some examples how I applied these

00:33:14,000 --> 00:33:19,890
design rules in order to achieve

00:33:16,890 --> 00:33:21,780
scalability up to mine in this case

00:33:19,890 --> 00:33:30,200
basically linear scalability to up to 16

00:33:21,780 --> 00:33:33,510
threats by in it's not only but to

00:33:30,200 --> 00:33:37,740
basically to ensure that different

00:33:33,510 --> 00:33:39,360
course access different cache lines and

00:33:37,740 --> 00:33:40,590
rights to different cache lines because

00:33:39,360 --> 00:33:43,110
there's lots of write operations

00:33:40,590 --> 00:33:45,150
involved here they need to write to

00:33:43,110 --> 00:33:49,740
different cache lines otherwise you

00:33:45,150 --> 00:33:53,510
can't get linear scalability that's

00:33:49,740 --> 00:33:53,510
basically the one of the most important

00:33:54,110 --> 00:34:01,020
metrics for scalability is how well

00:33:57,990 --> 00:34:02,400
spread out your rights are if all rights

00:34:01,020 --> 00:34:07,050
go to the same cache line you have

00:34:02,400 --> 00:34:09,170
serious contention well that's all thank

00:34:07,050 --> 00:34:09,170
you

00:34:11,730 --> 00:34:16,030
[Applause]

00:34:42,039 --> 00:34:47,899
but in your case here if you have four

00:34:45,589 --> 00:34:49,569
rights to different cache lines that

00:34:47,899 --> 00:34:52,760
support to some kind critical section

00:34:49,569 --> 00:34:54,500
that is then released by a ticket look

00:34:52,760 --> 00:34:58,389
release it's the ticket lock release

00:34:54,500 --> 00:34:58,389
that has to store release operation

00:35:20,260 --> 00:35:29,080
I don't understand your case here

00:35:26,680 --> 00:35:33,790
because if you want these five stores to

00:35:29,080 --> 00:35:35,560
be visible or not visible and if this

00:35:33,790 --> 00:35:38,290
region is protected by ticket lock I

00:35:35,560 --> 00:35:41,710
assume they're only visible if a thread

00:35:38,290 --> 00:35:44,859
takes that ticket look so it's a ticket

00:35:41,710 --> 00:35:49,210
look that creates the atomic view of the

00:35:44,859 --> 00:35:54,000
data you're updating or are these other

00:35:49,210 --> 00:35:54,000
rights also separate atomic operations

00:36:01,670 --> 00:36:04,970
no but that's why you have this store

00:36:03,619 --> 00:36:07,010
release when you release the ticket

00:36:04,970 --> 00:36:10,510
that's what's ensures that they are

00:36:07,010 --> 00:36:10,510
visible when the ticket is released

00:36:28,859 --> 00:36:34,200
yeah but with x86 the stores will be

00:36:32,579 --> 00:36:37,229
visible in the order that they are

00:36:34,200 --> 00:36:39,150
performed which makes it possible to

00:36:37,229 --> 00:36:41,900
avoid barriers basically you don't need

00:36:39,150 --> 00:36:41,900
barriers in x86

00:37:17,219 --> 00:37:22,619
and I know I'm not the x86 expert so I

00:37:23,939 --> 00:37:29,769
don't know under which circumstances

00:37:25,989 --> 00:37:32,319
this ordering is not required on x86 but

00:37:29,769 --> 00:37:36,609
I thought that on x86 basically stores

00:37:32,319 --> 00:37:44,289
are become visible in the order in which

00:37:36,609 --> 00:37:46,660
they are executed but on arm I can

00:37:44,289 --> 00:37:49,479
assure that if you don't have the store

00:37:46,660 --> 00:37:52,390
release or you don't have a suitable D&B

00:37:49,479 --> 00:37:57,849
for instance the order of observability

00:37:52,390 --> 00:38:01,900
is whatever anything can happen but that

00:37:57,849 --> 00:38:04,269
also means you need the barriers and a

00:38:01,900 --> 00:38:06,369
day in order to impose this ordering of

00:38:04,269 --> 00:38:09,249
when the updates become visible it that

00:38:06,369 --> 00:38:12,249
that has some extra cost it's not really

00:38:09,249 --> 00:38:15,579
a scalability it's it depends on the

00:38:12,249 --> 00:38:17,140
situation if you just but what I was

00:38:15,579 --> 00:38:19,119
trying to say in some cases you are

00:38:17,140 --> 00:38:23,499
might birthday if you're updating for

00:38:19,119 --> 00:38:25,179
instance a some statistics counters you

00:38:23,499 --> 00:38:26,979
don't need a choir and release ordering

00:38:25,179 --> 00:38:31,239
that statistics counters just a separate

00:38:26,979 --> 00:38:33,609
piece of data so on on arm that counter

00:38:31,239 --> 00:38:42,209
updates you just be in a relaxed atomic

00:38:33,609 --> 00:38:42,209
operation no barriers needed

00:38:47,820 --> 00:38:53,610
someone will step back there no it's

00:38:51,520 --> 00:39:02,560
hard to see with these lights in my eyes

00:38:53,610 --> 00:39:05,730
more comments okay thank you

00:39:02,560 --> 00:39:05,730
[Applause]

00:39:05,960 --> 00:39:11,159

YouTube URL: https://www.youtube.com/watch?v=5YQ_Ua3XT-0


