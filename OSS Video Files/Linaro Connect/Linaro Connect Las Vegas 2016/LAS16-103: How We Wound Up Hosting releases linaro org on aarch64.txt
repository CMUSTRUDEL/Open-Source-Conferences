Title: LAS16-103: How We Wound Up Hosting releases linaro org on aarch64
Publication date: 2016-09-30
Playlist: Linaro Connect Las Vegas 2016
Description: 
	LAS16-103: How We Wound Up Hosting releases.linaro.org on aarch64
Speakers: Andy Doan
Date: September 26, 2016

★ Session Description ★
Discuss how the Systems team wound up hosting releases.linaro.org in the Austin Colo, how its holding up, and what other services might we “dogfood” in the future.

★ Resources ★
Etherpad: pad.linaro.org/p/las16-103
Presentations & Videos: http://connect.linaro.org/resource/las16/las16-103/

★ Event Details ★
Linaro Connect Las Vegas 2016 – #LAS16
September 26-30, 2016
http://www.linaro.org
http://connect.linaro.org
Captions: 
	00:00:06,710 --> 00:00:13,200
so my name is Andy don't I run the

00:00:10,200 --> 00:00:15,210
systems team at linaro Ben works with me

00:00:13,200 --> 00:00:18,359
also on we've got another guy named Paul

00:00:15,210 --> 00:00:20,400
and we're in charge of a bunch of

00:00:18,359 --> 00:00:24,590
different services that linaro runs like

00:00:20,400 --> 00:00:28,260
it the get servers bugzilla rci servers

00:00:24,590 --> 00:00:31,050
and we're also in charge of what we call

00:00:28,260 --> 00:00:33,500
publishing servers and the presentation

00:00:31,050 --> 00:00:36,510
was kind of a is a two-part thing we

00:00:33,500 --> 00:00:38,879
recently hit a big bug on releases that

00:00:36,510 --> 00:00:41,430
one heart out of work that we finally

00:00:38,879 --> 00:00:43,530
worked around in in the course of that

00:00:41,430 --> 00:00:46,500
we found ourselves running one of our

00:00:43,530 --> 00:00:50,129
most heavily used sites at linaro on arm

00:00:46,500 --> 00:00:51,870
hardware so the other part of this was I

00:00:50,129 --> 00:00:55,410
was kind of hoping to get some people to

00:00:51,870 --> 00:00:58,020
kind of have a more collaborative talk

00:00:55,410 --> 00:01:00,989
about different services linaro might be

00:00:58,020 --> 00:01:06,510
able to move to run on real arm hardware

00:01:00,989 --> 00:01:08,670
so just a little bit on what releases is

00:01:06,510 --> 00:01:11,159
we we call that one of our publishing

00:01:08,670 --> 00:01:14,549
servers the two others being snapshots

00:01:11,159 --> 00:01:18,570
Dalton r dot org and builds on 96 boards

00:01:14,549 --> 00:01:21,020
org which are basically it's a Django

00:01:18,570 --> 00:01:23,640
web app they share the same code and it

00:01:21,020 --> 00:01:26,130
serves releases it serves build

00:01:23,640 --> 00:01:29,579
artifacts that we have some are public

00:01:26,130 --> 00:01:31,049
some require EULA's to accept and some

00:01:29,579 --> 00:01:34,049
are private depending on who you're

00:01:31,049 --> 00:01:36,960
logged in as so what you'll see and

00:01:34,049 --> 00:01:39,450
every artifact linaro has ever created

00:01:36,960 --> 00:01:42,240
lives on releases so over time our

00:01:39,450 --> 00:01:45,360
storage requirements have grown pretty

00:01:42,240 --> 00:01:48,299
enormous and this was running an amazon

00:01:45,360 --> 00:01:51,899
and the data volume bills were going

00:01:48,299 --> 00:01:54,750
pretty high so guess about a year ago we

00:01:51,899 --> 00:01:57,799
switched to using Amazon's s3 to store

00:01:54,750 --> 00:02:01,979
our order our artifacts and s3 is

00:01:57,799 --> 00:02:04,619
Amazon's object storage system and for

00:02:01,979 --> 00:02:07,740
our needs is basically faster cheaper

00:02:04,619 --> 00:02:13,800
more reliable is pretty much better in

00:02:07,740 --> 00:02:17,550
every way and it's backed by a REST API

00:02:13,800 --> 00:02:20,520
to find out like what artifacts you have

00:02:17,550 --> 00:02:24,180
and it turned out that API is close

00:02:20,520 --> 00:02:26,070
enough to kind of look like it kind of

00:02:24,180 --> 00:02:27,540
has semantics that make it feel a little

00:02:26,070 --> 00:02:29,640
bit like dealing with a normal file

00:02:27,540 --> 00:02:31,500
system so we could kind of fit it into

00:02:29,640 --> 00:02:36,150
our publishing server code without too

00:02:31,500 --> 00:02:39,120
much pain and at the time I made a kind

00:02:36,150 --> 00:02:42,300
of a big technical decision that I did a

00:02:39,120 --> 00:02:44,430
lot of testing and I figured out that

00:02:42,300 --> 00:02:48,840
per user request we could just talk to

00:02:44,430 --> 00:02:51,720
to the s3 API and the it was basically a

00:02:48,840 --> 00:02:56,160
negligible performance impact on on our

00:02:51,720 --> 00:02:57,870
end users so it essentially sent me a

00:02:56,160 --> 00:03:00,810
little time I didn't have to worry about

00:02:57,870 --> 00:03:02,730
caching logic and we were a little bit

00:03:00,810 --> 00:03:04,770
nervous with caching logic because we

00:03:02,730 --> 00:03:07,260
have like these private files and stuff

00:03:04,770 --> 00:03:09,260
and you know if we accidentally serve

00:03:07,260 --> 00:03:13,920
the wrong file or wrong person it could

00:03:09,260 --> 00:03:17,880
give you a pain so this is kind of the

00:03:13,920 --> 00:03:20,880
timeline of events of it's a 10 month

00:03:17,880 --> 00:03:23,640
window of page load response times on

00:03:20,880 --> 00:03:25,980
our front page of our service and you

00:03:23,640 --> 00:03:28,530
can see when we deployed s3 pretty much

00:03:25,980 --> 00:03:30,300
a little over a year ago we had a small

00:03:28,530 --> 00:03:33,330
bump in page load times and that was

00:03:30,300 --> 00:03:35,400
what I had expected and essentially no

00:03:33,330 --> 00:03:38,480
one noticed it was pretty negligible and

00:03:35,400 --> 00:03:41,520
then time goes on about six months and

00:03:38,480 --> 00:03:44,820
you see just on a specific day

00:03:41,520 --> 00:03:46,830
everything went to crap and full

00:03:44,820 --> 00:03:49,110
disclosure we didn't totally we we

00:03:46,830 --> 00:03:50,970
weren't monitoring page load times on on

00:03:49,110 --> 00:03:52,950
this service we were monitoring more

00:03:50,970 --> 00:03:55,470
like is it offline or online kind of

00:03:52,950 --> 00:03:57,420
stuff and around the same time all of a

00:03:55,470 --> 00:04:00,600
sudden riku was probably the first

00:03:57,420 --> 00:04:04,320
people and the CI team started seeing a

00:04:00,600 --> 00:04:05,820
lot of just completely sporadic SSL kind

00:04:04,320 --> 00:04:09,090
of handshake failures when they were

00:04:05,820 --> 00:04:10,709
trying to download files so we kind of

00:04:09,090 --> 00:04:12,510
had all this thing all this stuff going

00:04:10,709 --> 00:04:15,930
on then you can see it in the far right

00:04:12,510 --> 00:04:20,160
like it got really bad with page load

00:04:15,930 --> 00:04:21,810
times then we kinda were trying to

00:04:20,160 --> 00:04:25,020
figure out what was going on I kind of

00:04:21,810 --> 00:04:26,360
had this uh I've always kind of had this

00:04:25,020 --> 00:04:27,979
fear of my mind I'm going to get a

00:04:26,360 --> 00:04:29,990
find a bug and not be able to fix it and

00:04:27,979 --> 00:04:31,879
I'm just going to get fired because I

00:04:29,990 --> 00:04:34,099
can't solve the bug and this will start

00:04:31,879 --> 00:04:37,310
to to feel like that at the time because

00:04:34,099 --> 00:04:39,199
it seemed like two issues it seemed like

00:04:37,310 --> 00:04:41,629
there was something wrong with ssl it

00:04:39,199 --> 00:04:44,930
also seemed like s3 had just gotten slow

00:04:41,629 --> 00:04:47,419
but we just we felt you had to be our

00:04:44,930 --> 00:04:49,520
call they couldn't just be SS LS to

00:04:47,419 --> 00:04:51,969
quality of a thing and apache and that's

00:04:49,520 --> 00:04:55,849
three like how could they be at fault so

00:04:51,969 --> 00:04:57,680
we need a way to debug things and we

00:04:55,849 --> 00:04:59,750
needed something with a really reliable

00:04:57,680 --> 00:05:03,740
internet connection and we needed it

00:04:59,750 --> 00:05:06,919
pretty fast and I happen to run the Colo

00:05:03,740 --> 00:05:10,129
in austin and i have access to lots of

00:05:06,919 --> 00:05:11,779
arm servers and i guess the one kind of

00:05:10,129 --> 00:05:13,699
silver lining for Ben and I in this

00:05:11,779 --> 00:05:16,610
moment when when things can't get any

00:05:13,699 --> 00:05:18,439
worse you have a lot of freedom to

00:05:16,610 --> 00:05:21,229
experiment so we finally just said you

00:05:18,439 --> 00:05:24,789
know let's just get one of these bare

00:05:21,229 --> 00:05:27,560
metal servers in the lab will throw on

00:05:24,789 --> 00:05:30,289
the host OS would put up on to Z niall

00:05:27,560 --> 00:05:34,400
and then we actually ran the releases

00:05:30,289 --> 00:05:36,080
within a Alexi container that was on

00:05:34,400 --> 00:05:38,900
trusty which is what we had our

00:05:36,080 --> 00:05:40,699
publishing service on at the time so it

00:05:38,900 --> 00:05:43,460
would kind of keep us on the same o us

00:05:40,699 --> 00:05:45,860
and help us start to debug like it's not

00:05:43,460 --> 00:05:51,289
on Amazon's network we try to figure out

00:05:45,860 --> 00:05:54,440
what's going on and another kind of side

00:05:51,289 --> 00:05:56,960
note to the way our releases server was

00:05:54,440 --> 00:05:59,419
working at this point we had a amazon

00:05:56,960 --> 00:06:01,310
has kind of a geographic dns resolution

00:05:59,419 --> 00:06:03,680
in front of it so if one of our servers

00:06:01,310 --> 00:06:05,779
goes offline it just kind of keeps

00:06:03,680 --> 00:06:10,400
sending stuff to another server that we

00:06:05,779 --> 00:06:12,110
had which was in asia but so it kind of

00:06:10,400 --> 00:06:14,599
sucked for everybody but we could keep

00:06:12,110 --> 00:06:17,839
running if this thing went you know went

00:06:14,599 --> 00:06:21,259
really bad for us but um anyways we put

00:06:17,839 --> 00:06:22,669
this thing into production start trying

00:06:21,259 --> 00:06:24,919
to figure out what was happening and it

00:06:22,669 --> 00:06:27,620
actually turned out that there were two

00:06:24,919 --> 00:06:30,560
bugs going on the biggest most

00:06:27,620 --> 00:06:32,089
noticeable was s3 just on that day that

00:06:30,560 --> 00:06:35,419
you saw in that timeline it just went

00:06:32,089 --> 00:06:38,599
for X slower its rest api just went to

00:06:35,419 --> 00:06:38,990
crap and what's really crazy to me i

00:06:38,599 --> 00:06:40,760
still

00:06:38,990 --> 00:06:42,320
can't wrap my mind around this but I

00:06:40,760 --> 00:06:45,590
mean Ben and I SAT there and we looked

00:06:42,320 --> 00:06:49,010
at it from the data center in Austin we

00:06:45,590 --> 00:06:52,250
could access s3 / AP is faster than we

00:06:49,010 --> 00:06:55,010
could with in amazon's own network so it

00:06:52,250 --> 00:06:57,440
was just kind of this insane thing so to

00:06:55,010 --> 00:06:59,570
deal with us three we just had to invent

00:06:57,440 --> 00:07:02,180
the cash that I've been trying to avoid

00:06:59,570 --> 00:07:04,970
all along and once we put that cash in

00:07:02,180 --> 00:07:06,620
and that solved our page load times but

00:07:04,970 --> 00:07:09,080
we still had the ssl issue which we

00:07:06,620 --> 00:07:11,180
finally hunted down on you found a I

00:07:09,080 --> 00:07:14,300
don't know like a three-year-old bug on

00:07:11,180 --> 00:07:16,340
launch pad that a bunch of different

00:07:14,300 --> 00:07:18,560
people within canonical had hit this

00:07:16,340 --> 00:07:21,290
like hosting their own web servers and

00:07:18,560 --> 00:07:23,270
essentially the Apache has different

00:07:21,290 --> 00:07:25,070
ways to handle connections and that kind

00:07:23,270 --> 00:07:29,180
of the default one is a threaded model

00:07:25,070 --> 00:07:31,820
and apparently the way I don't know if

00:07:29,180 --> 00:07:33,230
it's probably more patchy than live SSL

00:07:31,820 --> 00:07:35,180
but apparently they're not doing

00:07:33,230 --> 00:07:38,030
something that in a totally thread safe

00:07:35,180 --> 00:07:40,790
way and every once in a blue moon you

00:07:38,030 --> 00:07:41,990
would get some memory corruption and you

00:07:40,790 --> 00:07:45,650
would get this kind of SSL handshake

00:07:41,990 --> 00:07:48,860
failure so we literally just changed the

00:07:45,650 --> 00:07:52,550
apache connection handling thing to this

00:07:48,860 --> 00:07:55,250
mpm pre fork which is has multiple

00:07:52,550 --> 00:07:59,510
processes for a process per connection

00:07:55,250 --> 00:08:01,610
and for a site like ours it costs more

00:07:59,510 --> 00:08:04,990
memory but releases isn't you know

00:08:01,610 --> 00:08:07,550
google com so it wasn't prohibitive and

00:08:04,990 --> 00:08:10,400
once we did that like all our problems

00:08:07,550 --> 00:08:12,080
in a way so that really solved

00:08:10,400 --> 00:08:14,900
everything for us at that point and

00:08:12,080 --> 00:08:17,150
we've been running releases on arm

00:08:14,900 --> 00:08:19,520
hardware ever since and we've had no

00:08:17,150 --> 00:08:22,520
issues that the only down time we've had

00:08:19,520 --> 00:08:24,890
our colocation facility had a HVAC

00:08:22,520 --> 00:08:26,840
failure and it knocked out like three

00:08:24,890 --> 00:08:29,690
rows of servers one day and we were

00:08:26,840 --> 00:08:32,930
affected by that but releases itself it

00:08:29,690 --> 00:08:35,660
has run without any problems and I threw

00:08:32,930 --> 00:08:40,640
a picture of John on this because I hid

00:08:35,660 --> 00:08:42,950
in the early days of UEFI on unarm is to

00:08:40,640 --> 00:08:46,100
curse him very frequently because it

00:08:42,950 --> 00:08:47,100
just it wasn't working well and I like

00:08:46,100 --> 00:08:49,680
you boot

00:08:47,100 --> 00:08:51,870
I knew how to deal with that but when

00:08:49,680 --> 00:08:53,790
you when you have a big data center like

00:08:51,870 --> 00:08:56,850
I'm starting to get in Austin you've got

00:08:53,790 --> 00:08:59,670
multiple servers from multiple vendors

00:08:56,850 --> 00:09:03,240
you need them to be like cattle and and

00:08:59,670 --> 00:09:05,670
not like pets and UEFI and standards

00:09:03,240 --> 00:09:09,600
actually made that a lot easier for us

00:09:05,670 --> 00:09:11,370
to to just deal with servers so with

00:09:09,600 --> 00:09:12,899
that that's kind of the point I wanted

00:09:11,370 --> 00:09:15,899
to make this kind of interactive at this

00:09:12,899 --> 00:09:19,230
point get some opinions from people you

00:09:15,899 --> 00:09:21,750
know releases is one of our most heavily

00:09:19,230 --> 00:09:24,209
used sites in it works good but we want

00:09:21,750 --> 00:09:26,990
to run more stuff on our and we think

00:09:24,209 --> 00:09:32,009
it's important to dog through stuff and

00:09:26,990 --> 00:09:33,810
I kind of have two constraints on it as

00:09:32,009 --> 00:09:38,149
far as services I would want to do is

00:09:33,810 --> 00:09:40,620
yeah I'm up for hosting something that's

00:09:38,149 --> 00:09:42,810
essentially horizontally scalable and

00:09:40,620 --> 00:09:45,089
stateless like what releases as to where

00:09:42,810 --> 00:09:47,160
we can just put it in if it's not doing

00:09:45,089 --> 00:09:49,589
good no one notices except for us and we

00:09:47,160 --> 00:09:51,569
can analyze the failure and move on or

00:09:49,589 --> 00:09:53,100
the other one is just something that you

00:09:51,569 --> 00:09:55,350
know if it's up ninety-five percent of

00:09:53,100 --> 00:09:59,850
the time knowing no one cares about the

00:09:55,350 --> 00:10:02,670
other five percent so just throwing out

00:09:59,850 --> 00:10:07,050
two examples of services that might be

00:10:02,670 --> 00:10:10,230
able to do that is patches dahlin org is

00:10:07,050 --> 00:10:12,980
a service that gets used quite a bit but

00:10:10,230 --> 00:10:16,410
it's not mission-critical but it does

00:10:12,980 --> 00:10:18,930
use postgres pretty heavily so you get

00:10:16,410 --> 00:10:21,180
some kind of cool database usage on arm

00:10:18,930 --> 00:10:23,339
and we also use get pretty heavily on

00:10:21,180 --> 00:10:26,550
that so it'd be kind of an interesting

00:10:23,339 --> 00:10:28,589
service and if we want to really go for

00:10:26,550 --> 00:10:32,730
the gold we could put a new get mirror

00:10:28,589 --> 00:10:34,920
in that in the lab there and since it's

00:10:32,730 --> 00:10:38,490
done kind of by geographic DNS you know

00:10:34,920 --> 00:10:41,550
we have another one in our main gate

00:10:38,490 --> 00:10:43,769
servers in Virginia I guess so kind on

00:10:41,550 --> 00:10:46,490
it so you know most of the Southwest

00:10:43,769 --> 00:10:49,319
United States would go through the lab

00:10:46,490 --> 00:10:52,560
the one big thing that would stress this

00:10:49,319 --> 00:10:55,079
I was thinking about recently is all the

00:10:52,560 --> 00:10:58,630
arm builders that we have are in that

00:10:55,079 --> 00:11:02,820
Colo also so a lot of our CI loop

00:10:58,630 --> 00:11:09,490
up going through the server which

00:11:02,820 --> 00:11:12,550
there's good and bad to that right so I

00:11:09,490 --> 00:11:15,730
don't know if you guys want to like

00:11:12,550 --> 00:11:18,250
there's ever is there any service like

00:11:15,730 --> 00:11:21,030
you you think we could move to to arm

00:11:18,250 --> 00:11:21,030
rakuten

00:11:45,440 --> 00:11:52,079
yeah it's easier for us to deal with the

00:11:49,170 --> 00:11:55,769
systems because of her systems but I

00:11:52,079 --> 00:12:00,060
mean like repo Dalton re-org yeah i mean

00:11:55,769 --> 00:12:07,230
we could do that pretty easy you want to

00:12:00,060 --> 00:12:10,279
actually write that one down that one

00:12:07,230 --> 00:12:10,279
might be a little more difficult

00:12:22,060 --> 00:12:29,000
well I just reliability I mean if that

00:12:24,680 --> 00:12:31,329
goes down that's going to make people

00:12:29,000 --> 00:12:31,329
angry

00:12:49,279 --> 00:12:54,680
that's usually what makes this hard for

00:12:52,139 --> 00:12:54,680
everything

00:13:03,750 --> 00:13:11,730
I have a private branch the kind of does

00:13:09,150 --> 00:13:15,990
it the problem with repo doll in Arnold

00:13:11,730 --> 00:13:18,690
or that setup I forget the Debian pry a

00:13:15,990 --> 00:13:23,700
runs that kind of packages project and

00:13:18,690 --> 00:13:26,400
that's not very ansible friendly it make

00:13:23,700 --> 00:13:28,290
you you make change to a comp file and

00:13:26,400 --> 00:13:31,350
you have to kind of run a command and it

00:13:28,290 --> 00:13:33,600
applies changes to its hard to config

00:13:31,350 --> 00:13:36,750
manage boy that thing works it can be

00:13:33,600 --> 00:13:38,340
done but it but I mean it's one we can

00:13:36,750 --> 00:13:39,960
move over but like you said you have to

00:13:38,340 --> 00:13:42,770
root you have to remember to copy all

00:13:39,960 --> 00:13:42,770
the data over

00:13:55,459 --> 00:14:06,230
yeah now if that is is repo use for the

00:14:02,610 --> 00:14:06,230
CIA Lou pretty heavily right now

00:14:14,160 --> 00:14:19,290
okay that's not a bad idea

00:14:20,880 --> 00:14:25,680
and what's funny about this is I mean

00:14:22,920 --> 00:14:27,090
like right now releases is running on

00:14:25,680 --> 00:14:29,700
this one server but I've got it

00:14:27,090 --> 00:14:35,130
constrained to like two CPUs and two

00:14:29,700 --> 00:14:39,360
gigs of ram and in the server itself I

00:14:35,130 --> 00:14:42,300
think as you know 16 gigs are like it

00:14:39,360 --> 00:14:45,660
it's still basically a completely idle

00:14:42,300 --> 00:14:48,920
server so we could we can throw a few

00:14:45,660 --> 00:14:48,920
more containers on there

00:14:57,880 --> 00:15:02,490
so after repo what would you want to see

00:15:18,270 --> 00:15:27,520
yeah yeah we so we got releases on s3

00:15:23,230 --> 00:15:29,620
first we now have the 96 board server

00:15:27,520 --> 00:15:33,610
completely honest three so we can

00:15:29,620 --> 00:15:35,650
actually move it to this server now we

00:15:33,610 --> 00:15:37,630
been finished it like the week before

00:15:35,650 --> 00:15:43,480
connect and we're a little bit afraid to

00:15:37,630 --> 00:15:46,320
try to take that to me last week but we

00:15:43,480 --> 00:15:46,320
can move that one over

00:15:52,160 --> 00:16:13,800
yeah it barely works on on Intel yeah I

00:16:01,250 --> 00:16:21,180
agree so that's a good quick it's a

00:16:13,800 --> 00:16:23,340
little bit complicated because I I've

00:16:21,180 --> 00:16:26,090
talked to management before and one

00:16:23,340 --> 00:16:29,630
thing that Amazon does have very good

00:16:26,090 --> 00:16:31,800
characteristics for a company Ferb

00:16:29,630 --> 00:16:34,850
you're not having to manage your own

00:16:31,800 --> 00:16:39,540
hardware and things like that and I

00:16:34,850 --> 00:16:41,700
think management is somewhat reluctant

00:16:39,540 --> 00:16:44,370
to just completely move away from amazon

00:16:41,700 --> 00:16:46,200
we don't want it to it I'm the only guy

00:16:44,370 --> 00:16:48,360
in Austin that deals with this and I

00:16:46,200 --> 00:16:50,730
mean personally I don't want my my

00:16:48,360 --> 00:16:53,190
entire job to just become you know

00:16:50,730 --> 00:16:57,240
dealing with you know replacing old

00:16:53,190 --> 00:17:00,660
servers and it gets costly and in our

00:16:57,240 --> 00:17:03,030
the colo is actually it's expensive it's

00:17:00,660 --> 00:17:07,290
I think long run i believe it's cheaper

00:17:03,030 --> 00:17:09,690
than amazon yeah i mean this is just

00:17:07,290 --> 00:17:14,610
kind of my own little personal objective

00:17:09,690 --> 00:17:16,620
tangent here but i think like so I've

00:17:14,610 --> 00:17:19,140
been doing a lot of work on the linaro

00:17:16,620 --> 00:17:20,610
developer cloud as well and part of why

00:17:19,140 --> 00:17:22,290
i'm interested in that is I think

00:17:20,610 --> 00:17:25,110
there's this kind of interesting place

00:17:22,290 --> 00:17:28,200
now with arm you have so much compute

00:17:25,110 --> 00:17:30,660
density in energy efficiency which is

00:17:28,200 --> 00:17:32,550
also a major cost of Cola so this is

00:17:30,660 --> 00:17:34,770
kind of a funny side story of my other

00:17:32,550 --> 00:17:37,200
I'm on two tangents now but when we

00:17:34,770 --> 00:17:39,270
first wired up the we've got three racks

00:17:37,200 --> 00:17:41,010
of servers in our data center when we

00:17:39,270 --> 00:17:42,780
first wired them all up they actually

00:17:41,010 --> 00:17:44,940
called us one day on like hey is

00:17:42,780 --> 00:17:46,740
everything running ok I'm like yeah but

00:17:44,940 --> 00:17:50,280
you know what's the problem like well

00:17:46,740 --> 00:17:53,310
your your energy usage y'all just don't

00:17:50,280 --> 00:17:55,380
draw many amps or what we thought maybe

00:17:53,310 --> 00:17:57,030
there was like some power problem like

00:17:55,380 --> 00:17:59,140
no it's everything's up and running it's

00:17:57,030 --> 00:18:06,370
just you know it's arm hardware

00:17:59,140 --> 00:18:07,990
doesn't use much so I think that Amazon

00:18:06,370 --> 00:18:10,750
is really expensive I feel like they

00:18:07,990 --> 00:18:12,700
employ psychologist for their pricing

00:18:10,750 --> 00:18:15,220
model because you always you get really

00:18:12,700 --> 00:18:16,630
pissed off at I'm screw that we're going

00:18:15,220 --> 00:18:19,230
to do it ourselves and then you start

00:18:16,630 --> 00:18:21,970
looking around at other options yeah

00:18:19,230 --> 00:18:25,000
it's kind of expensive but you know if

00:18:21,970 --> 00:18:27,250
you could get like a really nice like

00:18:25,000 --> 00:18:29,200
set up with OpenStack and arm hardware I

00:18:27,250 --> 00:18:31,900
feel like you could just like get a rack

00:18:29,200 --> 00:18:34,510
of servers you know in some nice data

00:18:31,900 --> 00:18:36,690
facility I think you could make a pretty

00:18:34,510 --> 00:18:40,900
good pitch that it's cheaper than amazon

00:18:36,690 --> 00:18:42,670
it's more performant and I think you

00:18:40,900 --> 00:18:44,350
could just make a better business model

00:18:42,670 --> 00:18:47,680
that way I think that's a good I don't

00:18:44,350 --> 00:18:49,660
know if that's like linaro strategic

00:18:47,680 --> 00:18:51,670
direction or anything but to me I think

00:18:49,660 --> 00:18:54,790
it makes sense for hosting stuff but

00:18:51,670 --> 00:18:57,340
that said again are there's some

00:18:54,790 --> 00:18:59,320
nervousness about just getting too much

00:18:57,340 --> 00:19:02,620
into us putting everything on arm

00:18:59,320 --> 00:19:05,070
hardware and having to manage that pay

00:19:02,620 --> 00:19:05,070
for it ourselves

00:19:18,080 --> 00:19:26,640
yeah and I mean like with hardware

00:19:24,570 --> 00:19:28,800
changes and upgrades and stuff like that

00:19:26,640 --> 00:19:31,410
one of the things I spent a lot of time

00:19:28,800 --> 00:19:33,480
with with the developer cloud it is I've

00:19:31,410 --> 00:19:35,850
kind of recently gotten really

00:19:33,480 --> 00:19:39,050
interested in to being able to do live

00:19:35,850 --> 00:19:41,760
migration of VMs and containers because

00:19:39,050 --> 00:19:44,580
one problem i have it in the colo is

00:19:41,760 --> 00:19:46,050
like i'll need to upgrade a colonel on a

00:19:44,580 --> 00:19:48,390
host or something and i'll have like

00:19:46,050 --> 00:19:50,040
five tenants on it and i have to you

00:19:48,390 --> 00:19:52,140
know go email them and say hey is it

00:19:50,040 --> 00:19:53,910
okay if i bring your server down you

00:19:52,140 --> 00:19:55,590
know over the weekend and then bring

00:19:53,910 --> 00:19:57,900
everything back up for you it's just

00:19:55,590 --> 00:19:59,490
kind of a pain to deal with with the

00:19:57,900 --> 00:20:01,470
maintenance of that but when you start

00:19:59,490 --> 00:20:04,260
having things like live migration of

00:20:01,470 --> 00:20:06,630
services there's still the financial

00:20:04,260 --> 00:20:08,610
cost to doing you know hardware upgrades

00:20:06,630 --> 00:20:10,830
and stuff but you can start just moving

00:20:08,610 --> 00:20:13,470
people around and services with without

00:20:10,830 --> 00:20:16,050
anyone noticing and that stuff works

00:20:13,470 --> 00:20:18,510
right now an arm it actually works well

00:20:16,050 --> 00:20:20,490
there was it got cut out of the demo

00:20:18,510 --> 00:20:22,740
this morning but when you fire up a vm

00:20:20,490 --> 00:20:24,600
in our developer cloud you can like

00:20:22,740 --> 00:20:27,030
login to it have some interactive

00:20:24,600 --> 00:20:29,340
session going on and i can migrate you

00:20:27,030 --> 00:20:32,720
to another server and you won't you

00:20:29,340 --> 00:20:32,720
won't detect it from the command line

00:20:38,000 --> 00:20:42,080
so you just say move everything

00:21:28,050 --> 00:21:31,050
which

00:21:38,110 --> 00:21:45,350
yeah and I mean it's partly how we're

00:21:43,490 --> 00:21:49,910
kind of in this this mess in the first

00:21:45,350 --> 00:21:52,400
place it's like creating high V highly

00:21:49,910 --> 00:21:54,080
available services is expensive I mean

00:21:52,400 --> 00:21:55,429
like from an engineering standpoint it's

00:21:54,080 --> 00:21:57,799
hard to do in a lot of times it's just

00:21:55,429 --> 00:21:59,960
like the original release the server was

00:21:57,799 --> 00:22:02,510
just not built that way just use a local

00:21:59,960 --> 00:22:03,980
file system and it was easy enough and

00:22:02,510 --> 00:22:06,830
it's up ninety-nine percent of the time

00:22:03,980 --> 00:22:11,210
but at some point you've got to move

00:22:06,830 --> 00:22:13,790
past it and it's hard but it's not

00:22:11,210 --> 00:22:16,400
always worth it like you said out so

00:22:13,790 --> 00:22:18,350
it's down a little bit is that is it is

00:22:16,400 --> 00:22:22,090
ninety-five percent uptime better than

00:22:18,350 --> 00:22:26,200
you know $50,000 in engineering time to

00:22:22,090 --> 00:22:26,200
to make it highly available

00:22:36,740 --> 00:22:39,740
what

00:22:42,660 --> 00:22:47,530
there I mean they charge if we exceed

00:22:45,700 --> 00:22:53,500
some threshold but it's a pretty high

00:22:47,530 --> 00:22:56,650
threshold and I don't think that would

00:22:53,500 --> 00:22:58,210
be too big oh I mean if you think about

00:22:56,650 --> 00:23:00,130
a lot of a lot of the things we're

00:22:58,210 --> 00:23:02,320
talking about would wind up most of the

00:23:00,130 --> 00:23:05,320
traffic we would be like say we put a

00:23:02,320 --> 00:23:07,210
new get server in there most of the

00:23:05,320 --> 00:23:11,470
traffic would just be internal within a

00:23:07,210 --> 00:23:14,140
switch so it would might not even yeah

00:23:11,470 --> 00:23:17,380
maybe jump one switch but it's just like

00:23:14,140 --> 00:23:19,030
you know between our racks so I mean

00:23:17,380 --> 00:23:22,060
some ways that would be good for the CI

00:23:19,030 --> 00:23:25,000
loops right because you you're going to

00:23:22,060 --> 00:23:27,480
be get cloning at gigabit network speed

00:23:25,000 --> 00:23:27,480
and so

00:23:35,100 --> 00:23:43,770
so I mean that case we'd use less

00:23:40,840 --> 00:23:43,770
bandwidth I guess

00:23:52,700 --> 00:23:57,780
you mean like architecture specific

00:23:55,290 --> 00:24:00,380
things like in our deployment no I mean

00:23:57,780 --> 00:24:00,380
it just all

00:24:13,160 --> 00:24:22,470
the hardware we're on is actually

00:24:15,330 --> 00:24:26,420
production stuff we're actually mud very

00:24:22,470 --> 00:24:30,060
little hardware in the colo is is like

00:24:26,420 --> 00:24:32,790
reveille type stuff I not like it I know

00:24:30,060 --> 00:24:34,470
in Cambridge we've got it like in the

00:24:32,790 --> 00:24:36,810
lava validation lab they get a lot of

00:24:34,470 --> 00:24:38,760
this early stuff it's just the nature of

00:24:36,810 --> 00:24:43,730
how that works but most of the stuff

00:24:38,760 --> 00:24:43,730
I've gotten is like real stuff I'm him I

00:24:46,460 --> 00:24:52,920
was nervous just like when we were

00:24:49,920 --> 00:24:55,080
looking at that blog with releases with

00:24:52,920 --> 00:24:57,270
this you know we suspect there's

00:24:55,080 --> 00:24:58,890
something in Apache and maybe memory

00:24:57,270 --> 00:25:00,600
corruption I was a little bit or e well

00:24:58,890 --> 00:25:02,730
if we do this on arm are we just not

00:25:00,600 --> 00:25:05,760
going to see it for some so I mean this

00:25:02,730 --> 00:25:09,000
bug was Captain like once a week or

00:25:05,760 --> 00:25:10,560
something for you know a couple of

00:25:09,000 --> 00:25:12,570
hundred see I jobs I mean you know it's

00:25:10,560 --> 00:25:15,860
probably happening one in a thousand

00:25:12,570 --> 00:25:15,860
times people were access

00:25:21,060 --> 00:25:35,040
yeah yeah

00:25:48,600 --> 00:25:56,670
so been a nap talk those are a little

00:25:51,550 --> 00:25:56,670
more complex for for other reasons but

00:26:43,869 --> 00:26:51,249
he's a little bit embarrassing for the

00:26:46,429 --> 00:26:51,249
front pages did this go

00:27:22,770 --> 00:27:29,520
I i do think eventually we've got to

00:27:26,520 --> 00:27:33,030
move to me we've gotta move lennar dot

00:27:29,520 --> 00:27:36,270
org on to arm hardware I've just seems

00:27:33,030 --> 00:27:38,250
like there is the right thing to do but

00:27:36,270 --> 00:27:40,380
that one is been and I've talked about

00:27:38,250 --> 00:27:50,670
that one because I feel like that just

00:27:40,380 --> 00:27:52,640
needs to be done you know not really I

00:27:50,670 --> 00:28:00,600
mean I thought I talk about it some to

00:27:52,640 --> 00:28:02,100
to my boss but I think it something that

00:28:00,600 --> 00:28:04,170
we need to make something a little more

00:28:02,100 --> 00:28:06,930
formal but I think Bennett I need to

00:28:04,170 --> 00:28:10,790
sort out I'm not sure we know everything

00:28:06,930 --> 00:28:10,790
needs to be done right now and

00:28:25,020 --> 00:28:29,940
well in

00:29:13,880 --> 00:29:22,020
yeah in and actually there's some

00:29:19,980 --> 00:29:24,960
there's some loose ends we still need to

00:29:22,020 --> 00:29:27,360
tie up with like releases so at the time

00:29:24,960 --> 00:29:30,330
when I was doing that we we have an

00:29:27,360 --> 00:29:33,480
OpenStack deployment in Austin but it's

00:29:30,330 --> 00:29:36,180
a the first one we had done which we

00:29:33,480 --> 00:29:37,890
have we have tenants on I mean it was

00:29:36,180 --> 00:29:43,230
actually probably almost a hundred

00:29:37,890 --> 00:29:45,420
people using it right now but it's it's

00:29:43,230 --> 00:29:47,760
we're not actually route it's more of a

00:29:45,420 --> 00:29:53,010
glorified way to run VMs it's not really

00:29:47,760 --> 00:29:55,710
using OpenStack how we set it up and the

00:29:53,010 --> 00:29:57,330
next version we also deployed in a way

00:29:55,710 --> 00:30:00,870
we can't upgrade our way out of it

00:29:57,330 --> 00:30:03,300
essentially so I mean that the developer

00:30:00,870 --> 00:30:06,450
cloud b 2 i'm going to be deploying in

00:30:03,300 --> 00:30:08,190
austin and the upcoming month will just

00:30:06,450 --> 00:30:09,690
be completely new hardware and

00:30:08,190 --> 00:30:11,820
eventually we'll have to move tenants

00:30:09,690 --> 00:30:15,060
over kind of one by one to this new

00:30:11,820 --> 00:30:17,010
model but because of all this when i set

00:30:15,060 --> 00:30:18,840
up the releases server i just took a

00:30:17,010 --> 00:30:20,880
bare metal server in the lab and used

00:30:18,840 --> 00:30:23,660
containers and said well here's how I'm

00:30:20,880 --> 00:30:23,660
gonna do it and

00:30:27,010 --> 00:30:30,820
into the developer cloud

00:30:49,070 --> 00:30:56,190
and I guess we should talk some like if

00:30:54,119 --> 00:30:58,139
we move into OpenStack the way we've got

00:30:56,190 --> 00:31:00,509
to set up now you're running vm switch

00:30:58,139 --> 00:31:03,269
which is fine but I just feel like for

00:31:00,509 --> 00:31:04,710
like the services we're doing we could

00:31:03,269 --> 00:31:07,289
get a lot more you know basically

00:31:04,710 --> 00:31:09,629
compute density we can run a lot of

00:31:07,289 --> 00:31:11,879
stuff just on one server if we use

00:31:09,629 --> 00:31:13,950
containers we could run all the linaro

00:31:11,879 --> 00:31:16,529
publishing servers on one thing every

00:31:13,950 --> 00:31:18,659
repo delton we could basically put about

00:31:16,529 --> 00:31:22,799
you know six different service is on one

00:31:18,659 --> 00:31:26,279
arm hoaxed it would probably still be

00:31:22,799 --> 00:31:32,519
mostly idle whereas if we do 6pm it's

00:31:26,279 --> 00:31:35,639
going to start to but i haven't sorted

00:31:32,519 --> 00:31:37,879
out how to do that you can do containers

00:31:35,639 --> 00:31:43,109
and OpenStack but it's just not

00:31:37,879 --> 00:31:46,009
something I've looked into and I don't

00:31:43,109 --> 00:31:46,009
think that's really a

00:31:49,480 --> 00:32:00,380
well yeah I mean there's more so open

00:31:57,560 --> 00:32:02,570
OpenStack with themes you can you can

00:32:00,380 --> 00:32:08,750
overcome in on CPUs you can't overcome

00:32:02,570 --> 00:32:10,940
it on total Ram right you can't so if

00:32:08,750 --> 00:32:19,880
you've got you know 16 gigs of ram and

00:32:10,940 --> 00:32:22,160
you run now so I mean and it's fine you

00:32:19,880 --> 00:32:25,280
could still run a lot of VMs on a host

00:32:22,160 --> 00:32:28,360
it's just you don't get quite as much

00:32:25,280 --> 00:32:28,360
compute densities

00:32:38,490 --> 00:32:40,550

YouTube URL: https://www.youtube.com/watch?v=Y99xF_3mCvE


