Title: LAS16-311: Window Based Load Tracking WALT versus PELT utilization
Publication date: 2016-09-30
Playlist: Linaro Connect Las Vegas 2016
Description: 
	LAS16-311: Window Based Load Tracking (WALT) versus PELT utilization
Speakers: Vikram Mulukutla
Date: September 28, 2016

★ Session Description ★
Continuing the discussion from Linaro Connect Bangkok 2016, we discuss further experiments with Window Assisted Load Tracking (WALT, formerly WinLT) on other architectures such as x86 and with different workloads including laptop/desktop and server usecases. We propose that WALT is more accurate in tracking cpu/task utilization than PELT (which we believe is better at load rather than util tracking) and that better utilization estimation schemes can be built on top of WALT. Experiments use the new schedutil governor. Examination of individual use cases in detail as well as how to handle migration and potential caveats of a windowing scheme are discussed.

★ Resources ★
Etherpad: pad.linaro.org/p/las16-311
Presentations & Videos: http://connect.linaro.org/resource/las16/las16-311/

★ Event Details ★
Linaro Connect Las Vegas 2016 – #LAS16
September 26-30, 2016
http://www.linaro.org
http://connect.linaro.org
Captions: 
	00:00:08,300 --> 00:00:15,020
okay start my name is Vikram I'm from

00:00:12,929 --> 00:00:19,199
the Qualcomm Linux scheduler team

00:00:15,020 --> 00:00:22,470
presenting some data on Walt was spelled

00:00:19,199 --> 00:00:24,960
for CP utilization tracking on x86

00:00:22,470 --> 00:00:28,400
specifically this is part of an effort

00:00:24,960 --> 00:00:31,320
to get Walt upstream or to start

00:00:28,400 --> 00:00:33,059
conversation at least on lk ml and this

00:00:31,320 --> 00:00:36,390
is like first preview or a first step

00:00:33,059 --> 00:00:37,649
towards that sorry for my I heard my

00:00:36,390 --> 00:00:41,340
voice earlier and the mic it's pretty

00:00:37,649 --> 00:00:45,539
horrible so sorry about that this is the

00:00:41,340 --> 00:00:48,420
agenda I've assumed thats folks that

00:00:45,539 --> 00:00:51,390
have attended previous connects T mikkel

00:00:48,420 --> 00:00:53,399
presented some of my slides at Bangkok

00:00:51,390 --> 00:00:55,379
so if I'm assuming some level of

00:00:53,399 --> 00:00:58,410
familiarity with the Walton belt but I

00:00:55,379 --> 00:01:00,750
do have an introduction I'll describe

00:00:58,410 --> 00:01:05,129
what we've done for the RFC patch that

00:01:00,750 --> 00:01:08,820
we posted to ye as dev Srivatsa father

00:01:05,129 --> 00:01:11,430
gary was the original he had the

00:01:08,820 --> 00:01:14,100
original idea for Walt he basically

00:01:11,430 --> 00:01:16,939
helped clean up and reduce the pad set

00:01:14,100 --> 00:01:19,110
to just CP utilization tracking and

00:01:16,939 --> 00:01:22,979
we've removed the task utilization

00:01:19,110 --> 00:01:26,310
tracking for now and then up and then

00:01:22,979 --> 00:01:28,500
I'll present results on x86 I've used

00:01:26,310 --> 00:01:31,079
the faronics test suite results which i

00:01:28,500 --> 00:01:33,479
think is a nice segue into that because

00:01:31,079 --> 00:01:40,759
of steve Marcos experience we're trying

00:01:33,479 --> 00:01:45,390
to get a test suite going four-arm so

00:01:40,759 --> 00:01:49,009
belt was introduced by google paul

00:01:45,390 --> 00:01:52,020
turner in 2012 to address some of the

00:01:49,009 --> 00:01:54,619
load balancing and load tracking

00:01:52,020 --> 00:01:58,320
concerns that were present at that time

00:01:54,619 --> 00:02:01,049
it's it uses a geometric series to decay

00:01:58,320 --> 00:02:02,549
load and utilization some of the rework

00:02:01,049 --> 00:02:06,719
that's come into the kernel recently

00:02:02,549 --> 00:02:09,810
from you yang do from Intel makes it

00:02:06,719 --> 00:02:11,849
such that both blocked load and active

00:02:09,810 --> 00:02:14,280
load is tracked in the same util average

00:02:11,849 --> 00:02:17,879
statistic so it was separated but it's

00:02:14,280 --> 00:02:20,670
no longer that the belt math right now d

00:02:17,879 --> 00:02:22,560
caseload such that whatever load was

00:02:20,670 --> 00:02:24,599
seen 32 milliseconds ago contributes

00:02:22,560 --> 00:02:27,890
fifty percent towards the current

00:02:24,599 --> 00:02:31,110
average those are the two belt

00:02:27,890 --> 00:02:33,720
utilization statistics load average

00:02:31,110 --> 00:02:35,129
contribution of a i should say

00:02:33,720 --> 00:02:37,530
utilization their utilization average

00:02:35,129 --> 00:02:45,810
contribution of a single skate entity

00:02:37,530 --> 00:02:47,400
and averaged over the run queue this is

00:02:45,810 --> 00:02:48,989
a bit of a I'm sorry about these slides

00:02:47,400 --> 00:02:52,860
that are bit texty and have enough

00:02:48,989 --> 00:02:55,530
diagrams and there i guess but now this

00:02:52,860 --> 00:02:57,720
is a walt refresher for task utilization

00:02:55,530 --> 00:03:01,319
we keep track of n execution indoors for

00:02:57,720 --> 00:03:04,920
per task and we derive a demand

00:03:01,319 --> 00:03:07,590
attribute this this demand attribute is

00:03:04,920 --> 00:03:10,739
we impose a policy on it that's a

00:03:07,590 --> 00:03:12,300
tunable really where you can say look at

00:03:10,739 --> 00:03:15,030
the demand over the past five windows

00:03:12,300 --> 00:03:16,620
and take the average of that or look at

00:03:15,030 --> 00:03:18,629
the most recent window if that's how

00:03:16,620 --> 00:03:21,629
I've used that and those kind of

00:03:18,629 --> 00:03:24,030
policies one important thing to note

00:03:21,629 --> 00:03:25,920
about Walt is that the windows of

00:03:24,030 --> 00:03:28,440
observation are synchronized across CPUs

00:03:25,920 --> 00:03:30,959
which means that you need an appropriate

00:03:28,440 --> 00:03:35,790
time source to do that schedule a guided

00:03:30,959 --> 00:03:37,200
frequency which we do with Walt we

00:03:35,790 --> 00:03:39,150
basically I to get load from all tasks

00:03:37,200 --> 00:03:42,030
that ran in the most recently finished

00:03:39,150 --> 00:03:43,170
window so it's just the one single

00:03:42,030 --> 00:03:45,209
previous window that we're looking at

00:03:43,170 --> 00:03:47,580
for schedule a guided frequency and

00:03:45,209 --> 00:03:49,110
that's the statistic if you look in the

00:03:47,580 --> 00:03:52,500
code and the code is available both in

00:03:49,110 --> 00:03:55,170
the Android commentary as well as es dev

00:03:52,500 --> 00:04:01,439
you can find the can find the quartet

00:03:55,170 --> 00:04:03,209
that's the statistic that we use so we

00:04:01,439 --> 00:04:06,750
do have separate load tracking

00:04:03,209 --> 00:04:09,799
statistics for cpu versus task although

00:04:06,750 --> 00:04:13,110
we track the windows using the same code

00:04:09,799 --> 00:04:15,239
what ends up being sent to the governor

00:04:13,110 --> 00:04:17,880
versus what is used for task utilization

00:04:15,239 --> 00:04:19,590
is completely different and separate so

00:04:17,880 --> 00:04:21,650
these are the reasons why we do that we

00:04:19,590 --> 00:04:23,630
do account for wait time and task demand

00:04:21,650 --> 00:04:25,550
we don't do that for CPU

00:04:23,630 --> 00:04:27,880
I ization again that's it univille so if

00:04:25,550 --> 00:04:30,830
you want to play around with it you can

00:04:27,880 --> 00:04:33,500
we do believe that mobile workloads can

00:04:30,830 --> 00:04:36,320
benefit from the separation that's one

00:04:33,500 --> 00:04:37,910
of the reasons when you start a task you

00:04:36,320 --> 00:04:40,280
can actually modify task demand without

00:04:37,910 --> 00:04:42,010
having to affect the frequency so you

00:04:40,280 --> 00:04:48,350
can affect placement without affecting

00:04:42,010 --> 00:04:49,670
frequency like I said earlier these are

00:04:48,350 --> 00:04:53,000
some of the statistics maintained by

00:04:49,670 --> 00:04:54,440
Walt for the CPU or on cue that's the

00:04:53,000 --> 00:04:56,090
aggregate statistic the programmable

00:04:54,440 --> 00:04:57,710
summoned the corona balsamic earnable

00:04:56,090 --> 00:04:59,930
some is just the execution the current

00:04:57,710 --> 00:05:01,520
window so when you roll over you store

00:04:59,930 --> 00:05:04,840
that in prev runnable some and then you

00:05:01,520 --> 00:05:13,010
start over same thing goes for per task

00:05:04,840 --> 00:05:16,550
windows so this is basically a bit of

00:05:13,010 --> 00:05:19,850
history how we actually got here big

00:05:16,550 --> 00:05:22,760
little was we started working on big

00:05:19,850 --> 00:05:24,770
little a few years ago and see what

00:05:22,760 --> 00:05:26,630
science the muckle here decided there be

00:05:24,770 --> 00:05:28,640
a good idea to implement our own

00:05:26,630 --> 00:05:31,400
scheduler and see what's also decided

00:05:28,640 --> 00:05:35,390
that fear good idea to do window

00:05:31,400 --> 00:05:37,820
baseload tracking i would say that we

00:05:35,390 --> 00:05:39,590
don't actually have i don't remember if

00:05:37,820 --> 00:05:41,110
we have concrete data from those days as

00:05:39,590 --> 00:05:43,820
to why meditation where it seems to be

00:05:41,110 --> 00:05:45,940
seems to have been proven even three

00:05:43,820 --> 00:05:49,840
years later so i think it's a good one

00:05:45,940 --> 00:05:53,690
we've presented some prototype results

00:05:49,840 --> 00:05:57,530
comparing yes Walt es plus Walt versus

00:05:53,690 --> 00:05:59,990
es plus belt at linaro connect SFO last

00:05:57,530 --> 00:06:06,860
year and obviously it's been integrated

00:05:59,990 --> 00:06:09,290
into the android commentary this year so

00:06:06,860 --> 00:06:10,820
we've been asked obviously by various

00:06:09,290 --> 00:06:12,830
folks why we think it's better apart

00:06:10,820 --> 00:06:17,210
from the data itself the actual

00:06:12,830 --> 00:06:18,980
theoretical background 11 term that

00:06:17,210 --> 00:06:21,470
Watsa came up with which i think makes a

00:06:18,980 --> 00:06:23,780
lot of sense is task reclassification if

00:06:21,470 --> 00:06:25,310
a task has been you know a light task

00:06:23,780 --> 00:06:27,950
for a long time and suddenly it starts

00:06:25,310 --> 00:06:30,290
exhibiting heavy demand we believe that

00:06:27,950 --> 00:06:35,600
Walt is much faster at classifying it as

00:06:30,290 --> 00:06:36,980
a big task compared to belt leave when I

00:06:35,600 --> 00:06:39,650
say conservative approach in the sec

00:06:36,980 --> 00:06:42,560
point but I'm talking about how belt

00:06:39,650 --> 00:06:46,520
leaves the block utilization on the run

00:06:42,560 --> 00:06:48,770
queue and decays it slowly over time we

00:06:46,520 --> 00:06:51,170
believe that whereas Walt actually

00:06:48,770 --> 00:06:54,410
removes the Lord and restores it for

00:06:51,170 --> 00:06:56,780
past demand when the task is back on the

00:06:54,410 --> 00:06:58,760
run queue we believe that that is more

00:06:56,780 --> 00:07:02,780
appropriate for mobile was mobile use

00:06:58,760 --> 00:07:04,340
cases the math is simpler according to

00:07:02,780 --> 00:07:06,770
me at least I don't know if everybody

00:07:04,340 --> 00:07:08,540
agrees with that with Walt you do have

00:07:06,770 --> 00:07:11,240
to think a lot about window roll over

00:07:08,540 --> 00:07:13,670
and there is some complication that but

00:07:11,240 --> 00:07:15,260
at least it's not a geometric series you

00:07:13,670 --> 00:07:21,260
can add two numbers without thinking too

00:07:15,260 --> 00:07:22,790
much about it so like I said earlier

00:07:21,260 --> 00:07:25,340
this presentation is really about

00:07:22,790 --> 00:07:28,160
integrating the world frequency metric

00:07:25,340 --> 00:07:30,230
of the frequency statistic which scare

00:07:28,160 --> 00:07:32,060
do till I'm not it down some important

00:07:30,230 --> 00:07:33,770
points of a scheduled till most

00:07:32,060 --> 00:07:35,960
important being how it decides the next

00:07:33,770 --> 00:07:38,210
frequency there was some discussion of

00:07:35,960 --> 00:07:41,750
other formula upstream when the patch

00:07:38,210 --> 00:07:44,150
was merged so basically it's using an

00:07:41,750 --> 00:07:46,990
80% tipping point so you take the if

00:07:44,150 --> 00:07:49,880
your utilization is frequency invariant

00:07:46,990 --> 00:07:53,270
the Freak variable there is basically

00:07:49,880 --> 00:07:58,550
the max CP freak max frequency so you

00:07:53,270 --> 00:08:01,660
much by that x 1.25 and then multiply

00:07:58,550 --> 00:08:05,300
that by the utilization fraction

00:08:01,660 --> 00:08:06,500
schedule also supports atomic context

00:08:05,300 --> 00:08:08,120
freak switching I don't know if that's

00:08:06,500 --> 00:08:10,430
exactly the right way to do it what I

00:08:08,120 --> 00:08:11,720
really right way to term it what I

00:08:10,430 --> 00:08:12,920
really mean to say is that you can

00:08:11,720 --> 00:08:15,350
actually change frequency from the

00:08:12,920 --> 00:08:17,540
scheduler context if your hardware

00:08:15,350 --> 00:08:18,770
supports it without having to schedule a

00:08:17,540 --> 00:08:23,300
separate work queue or a separate

00:08:18,770 --> 00:08:26,540
context to so scared you tell frequency

00:08:23,300 --> 00:08:29,390
changes are triggered when the CFS fair

00:08:26,540 --> 00:08:31,250
utilization changes IE when it is

00:08:29,390 --> 00:08:35,270
decayed or during migration if the

00:08:31,250 --> 00:08:38,599
utilization is removed so what we've

00:08:35,270 --> 00:08:40,520
changed for Walt is basically you've

00:08:38,599 --> 00:08:42,680
tried to change nothing we don't want to

00:08:40,520 --> 00:08:44,720
add and think fancy no policies about

00:08:42,680 --> 00:08:46,730
what schedule is doing we just want to

00:08:44,720 --> 00:08:49,550
take the Walt utilization and provide it

00:08:46,730 --> 00:08:51,850
directly to schedule for a fair

00:08:49,550 --> 00:08:51,850
comparison

00:08:52,269 --> 00:08:58,670
so x86 does support atomic complex week

00:08:56,959 --> 00:09:01,310
switching frequency switching in

00:08:58,670 --> 00:09:04,310
hardware so we did want to retain that

00:09:01,310 --> 00:09:07,160
capability we didn't want Walt to make

00:09:04,310 --> 00:09:09,470
that not possible Walt used frequency

00:09:07,160 --> 00:09:12,139
transition notifiers and schedule it

00:09:09,470 --> 00:09:14,660
will basically prevent atomic switching

00:09:12,139 --> 00:09:16,279
if you have a fast switching that's why

00:09:14,660 --> 00:09:18,529
they call it they prevent fast switching

00:09:16,279 --> 00:09:22,879
if you have even a single transition

00:09:18,529 --> 00:09:25,180
notify registered so we remove that I

00:09:22,879 --> 00:09:28,220
also added another call site for the

00:09:25,180 --> 00:09:31,040
foreign working the frequency change and

00:09:28,220 --> 00:09:33,740
that's when the wall statistic changes I

00:09:31,040 --> 00:09:36,620
found that just doing it on the belt

00:09:33,740 --> 00:09:38,360
statistic when the pelt utilization

00:09:36,620 --> 00:09:47,269
average changes does not make sense for

00:09:38,360 --> 00:09:48,620
Walt so this is a simple example that

00:09:47,269 --> 00:09:53,029
are come up with again I wish it was

00:09:48,620 --> 00:09:54,860
graphical but bear with me this is an

00:09:53,029 --> 00:09:58,730
x86 machine which has a minimum

00:09:54,860 --> 00:10:03,470
frequency of 16 1.6 gigahertz and a max

00:09:58,730 --> 00:10:04,970
of 3.4 with 200 megawatt steps so if

00:10:03,470 --> 00:10:06,560
since the frequency tracking is

00:10:04,970 --> 00:10:09,769
invariant if you remember that formula

00:10:06,560 --> 00:10:11,930
we use the eighty percent tipping point

00:10:09,769 --> 00:10:14,720
so you basically take the max frequency

00:10:11,930 --> 00:10:16,699
add twenty-five percent to that and

00:10:14,720 --> 00:10:20,000
that's you're basically factor to which

00:10:16,699 --> 00:10:23,689
you multiply the utilization fraction so

00:10:20,000 --> 00:10:26,509
if you execute for a full window a 1600

00:10:23,689 --> 00:10:28,269
megahertz Walt will classify that as a

00:10:26,509 --> 00:10:32,630
forty six point nine percent task and

00:10:28,269 --> 00:10:34,279
then the next frequency is simply 2

00:10:32,630 --> 00:10:36,889
gigahertz by that equation and that's

00:10:34,279 --> 00:10:39,889
how it proceeds so within one to three

00:10:36,889 --> 00:10:43,040
windows you can hit your f max in this

00:10:39,889 --> 00:10:45,980
particular test case and I've used 10

00:10:43,040 --> 00:10:47,540
millisecond window here so for this

00:10:45,980 --> 00:10:49,639
example so basically that's 30

00:10:47,540 --> 00:10:52,639
milliseconds I mean that's also assuming

00:10:49,639 --> 00:10:54,079
that your frequency changes perfect and

00:10:52,639 --> 00:10:56,300
your windows line up perfectly with your

00:10:54,079 --> 00:10:57,860
load so there could be some skew there

00:10:56,300 --> 00:11:02,300
and you might have an extra window or

00:10:57,860 --> 00:11:05,319
two before you hit TF max mark run

00:11:02,300 --> 00:11:05,319
through a lot of slides of there

00:11:10,450 --> 00:11:14,480
yeah I we added that fault but I've

00:11:13,040 --> 00:11:18,130
tested with both with and without

00:11:14,480 --> 00:11:18,130
frequency weight invariance world

00:11:25,970 --> 00:11:32,779
so this is uh finally some graphical

00:11:29,249 --> 00:11:36,059
stuff so you don't fall asleep this is

00:11:32,779 --> 00:11:40,230
basically a trace that's loaded in the

00:11:36,059 --> 00:11:41,939
Chrome browser and I've compared to

00:11:40,230 --> 00:11:43,829
window sizes five milliseconds in 20

00:11:41,939 --> 00:11:48,329
milliseconds just to show how the ramp

00:11:43,829 --> 00:11:49,889
up and ramp down time changes the first

00:11:48,329 --> 00:11:51,509
one is obviously felt with the 32

00:11:49,889 --> 00:11:53,819
millisecond half life which I have not

00:11:51,509 --> 00:11:55,049
changed it should be interesting I

00:11:53,819 --> 00:11:57,600
supposed to compare to the 16

00:11:55,049 --> 00:12:00,420
millisecond or to try to change the belt

00:11:57,600 --> 00:12:04,259
half-life and make a comparison but this

00:12:00,420 --> 00:12:06,230
is what is so as you can see there is

00:12:04,259 --> 00:12:08,999
some skew it's not exactly three windows

00:12:06,230 --> 00:12:10,739
for when the window size is 20

00:12:08,999 --> 00:12:14,069
milliseconds they might be a little skew

00:12:10,739 --> 00:12:15,869
before you actually hit at max same goes

00:12:14,069 --> 00:12:17,730
for the five milliseconds I don't know

00:12:15,869 --> 00:12:18,869
if anybody will actually use 5

00:12:17,730 --> 00:12:22,379
millisecond window that's pretty

00:12:18,869 --> 00:12:24,179
aggressive but if you for some use case

00:12:22,379 --> 00:12:25,889
if you want to meet me to undermine

00:12:24,179 --> 00:12:38,970
performance for example I think it could

00:12:25,889 --> 00:12:41,429
make sense well actually if you look at

00:12:38,970 --> 00:12:42,869
it schedule + Pelt and schedule + vault

00:12:41,429 --> 00:12:44,610
with 20 millisecond window that is not

00:12:42,869 --> 00:12:47,420
that much of a difference simply because

00:12:44,610 --> 00:12:49,769
of how scared you till does the ramping

00:12:47,420 --> 00:12:51,990
what I guess the point of this slide is

00:12:49,769 --> 00:12:53,850
you can actually tune your ramp up time

00:12:51,990 --> 00:12:56,730
using the window size basically it's a

00:12:53,850 --> 00:12:58,499
very easy to use tunable for your use

00:12:56,730 --> 00:13:00,389
case way I don't really know what the

00:12:58,499 --> 00:13:02,480
right window size is for x86 was our use

00:13:00,389 --> 00:13:05,480
cases that's something that has to be

00:13:02,480 --> 00:13:05,480
investigated

00:13:12,540 --> 00:13:17,320
why is it a stepwise instead of going

00:13:14,950 --> 00:13:19,390
down all the way to zeros forward right

00:13:17,320 --> 00:13:22,120
so it depends on the windows so if the

00:13:19,390 --> 00:13:24,910
window is perfectly lined up such that

00:13:22,120 --> 00:13:27,250
your previous window was maximization

00:13:24,910 --> 00:13:28,600
and then one window later you had no

00:13:27,250 --> 00:13:30,490
utilization it would immediately drop

00:13:28,600 --> 00:13:32,080
the frequency but if your windows are

00:13:30,490 --> 00:13:34,930
not lined up perfectly it can take some

00:13:32,080 --> 00:13:41,529
time so it's just a matter of lining up

00:13:34,930 --> 00:13:42,339
the windows so basically if in that if

00:13:41,529 --> 00:13:45,010
you look in the second one right

00:13:42,339 --> 00:13:47,560
schedule + wall 20 milliseconds when the

00:13:45,010 --> 00:13:49,300
task stops executing you might not be

00:13:47,560 --> 00:13:50,890
the end of the window exactly it might

00:13:49,300 --> 00:14:00,339
be halfway through the window for

00:13:50,890 --> 00:14:02,080
example s so I have actually and so if

00:14:00,339 --> 00:14:04,150
you see those weird small lines there

00:14:02,080 --> 00:14:07,630
that's actually my fake task trying to

00:14:04,150 --> 00:14:09,790
trigger a CFS arc utilization change but

00:14:07,630 --> 00:14:12,310
it does takes longer always then

00:14:09,790 --> 00:14:13,660
volunteer that's in fact that's one of

00:14:12,310 --> 00:14:19,510
the significant differences that will

00:14:13,660 --> 00:14:22,930
highlight later so this is detailing

00:14:19,510 --> 00:14:24,310
some of the integration like I said

00:14:22,930 --> 00:14:26,920
earlier we've removed the task

00:14:24,310 --> 00:14:29,020
utilisation patch the reason why we did

00:14:26,920 --> 00:14:31,390
this is to not only reduce the bad size

00:14:29,020 --> 00:14:33,430
but to present something simple and

00:14:31,390 --> 00:14:34,720
something that upstream maintain us can

00:14:33,430 --> 00:14:37,870
just pick up and run if they really

00:14:34,720 --> 00:14:39,370
wanted to we send an email out to es dev

00:14:37,870 --> 00:14:41,110
when I believe the Atma just took out

00:14:39,370 --> 00:14:43,089
the patch ran it on his x86 machine

00:14:41,110 --> 00:14:44,890
almost the same day which is I think

00:14:43,089 --> 00:14:48,160
what we really want when we push this

00:14:44,890 --> 00:14:50,650
upstream like Steve Urkel said the arm

00:14:48,160 --> 00:14:55,150
targets are not very upstream ready for

00:14:50,650 --> 00:14:56,709
testing so so this is more of a

00:14:55,150 --> 00:15:01,270
demonstration of the code would look and

00:14:56,709 --> 00:15:02,800
how the stats accumulate if you it's

00:15:01,270 --> 00:15:04,510
basically what is there on the Android

00:15:02,800 --> 00:15:06,970
commentary minus the task utilization

00:15:04,510 --> 00:15:10,690
and some refactoring to make it more

00:15:06,970 --> 00:15:12,850
upstream amenable so that's what we do

00:15:10,690 --> 00:15:14,290
we basically just take the prev runnable

00:15:12,850 --> 00:15:17,170
some that's the aggregated cpu

00:15:14,290 --> 00:15:19,030
utilization in the previous window we

00:15:17,170 --> 00:15:21,310
directly pastor into schedule without

00:15:19,030 --> 00:15:26,140
any policy or without any massaging

00:15:21,310 --> 00:15:28,840
anything of that sort if so during

00:15:26,140 --> 00:15:32,200
migration Walt has a need right now to

00:15:28,840 --> 00:15:35,800
take both the source and destination RQ

00:15:32,200 --> 00:15:38,320
lock which is an expensive operation we

00:15:35,800 --> 00:15:41,290
did try to reduce the amount of code

00:15:38,320 --> 00:15:42,850
that executes under the lock to make

00:15:41,290 --> 00:15:45,550
that more friendly but obviously I think

00:15:42,850 --> 00:15:48,700
the consensus is that we need to not do

00:15:45,550 --> 00:15:50,440
that at all and just use a single lock

00:15:48,700 --> 00:15:53,380
and never basically take double our key

00:15:50,440 --> 00:15:54,880
lock to migrate statistics to migrate

00:15:53,380 --> 00:16:02,589
waltz 36 between source and destination

00:15:54,880 --> 00:16:04,870
run cues so I'd like to thank the guys

00:16:02,589 --> 00:16:07,330
and anybody else who's reviewed the

00:16:04,870 --> 00:16:09,190
patch on ES dev it's available for

00:16:07,330 --> 00:16:12,910
anybody else to review as well or test

00:16:09,190 --> 00:16:16,120
out so the main points that we want to

00:16:12,910 --> 00:16:18,330
fix is the double locking that's one

00:16:16,120 --> 00:16:20,830
sedation that's off the top of my head

00:16:18,330 --> 00:16:22,560
we also want to address the fact that

00:16:20,830 --> 00:16:25,330
not all architectures have synchronized

00:16:22,560 --> 00:16:28,060
clocks across CPUs we are using K time

00:16:25,330 --> 00:16:31,480
right now we want to use skate Clark and

00:16:28,060 --> 00:16:33,310
get rid of that requirement as well the

00:16:31,480 --> 00:16:36,040
third one should be easy enough to do we

00:16:33,310 --> 00:16:37,780
need to integrate with the arc freaking

00:16:36,040 --> 00:16:42,940
variance in arc efficiency invariant 80

00:16:37,780 --> 00:16:44,800
ice we also have so Walt is totally

00:16:42,940 --> 00:16:47,260
event-driven you basically update

00:16:44,800 --> 00:16:49,870
statistics when when the next task is

00:16:47,260 --> 00:16:52,000
selected in the previous task is taken

00:16:49,870 --> 00:16:53,650
off the run queue and we have like event

00:16:52,000 --> 00:16:56,680
codes and we want to get rid of those

00:16:53,650 --> 00:16:59,080
and just use the on our queue or the

00:16:56,680 --> 00:17:01,630
existing flags to that so three and four

00:16:59,080 --> 00:17:03,280
should be relatively easy to do too is a

00:17:01,630 --> 00:17:07,829
bit more long-term and one is an

00:17:03,280 --> 00:17:07,829
immediate requirement for upstream

00:17:11,480 --> 00:17:17,339
so this is some data on x86 I am

00:17:15,209 --> 00:17:21,420
personally very new and I think a lot of

00:17:17,339 --> 00:17:24,690
people are new to x86 here so honestly

00:17:21,420 --> 00:17:27,270
this is just preliminary data for power

00:17:24,690 --> 00:17:31,410
testing have used the repl counters is

00:17:27,270 --> 00:17:33,480
not actually measuring real power with

00:17:31,410 --> 00:17:37,440
hardware this is just the on-chip

00:17:33,480 --> 00:17:40,530
counters I've used the faronics test

00:17:37,440 --> 00:17:42,060
suite I found it to be some of the tests

00:17:40,530 --> 00:17:44,160
are kind of outdated they've been

00:17:42,060 --> 00:17:47,160
updated like two years ago or a year ago

00:17:44,160 --> 00:17:50,460
but overall I believe it's like one of

00:17:47,160 --> 00:17:52,230
the best mass testing frameworks have

00:17:50,460 --> 00:17:54,870
seen simply because it's very

00:17:52,230 --> 00:17:57,600
reproducible very easy to use the

00:17:54,870 --> 00:17:59,760
dependencies are very easy to install of

00:17:57,600 --> 00:18:04,920
course it's x86 so I guess they have

00:17:59,760 --> 00:18:06,600
that advantage but yeah I think the

00:18:04,920 --> 00:18:09,240
faronics test suite is something that we

00:18:06,600 --> 00:18:11,220
should probably try to imitate forearm

00:18:09,240 --> 00:18:13,620
as well I do believe they have some arm

00:18:11,220 --> 00:18:16,440
devices in there right now that they're

00:18:13,620 --> 00:18:21,810
testing but yeah it needs to be much

00:18:16,440 --> 00:18:24,420
more large-scale forearm upstream so we

00:18:21,810 --> 00:18:26,670
integrated Walt on for dot seven RC six

00:18:24,420 --> 00:18:29,370
I really need to update that that's kind

00:18:26,670 --> 00:18:30,870
of all right now the distribution that

00:18:29,370 --> 00:18:34,530
we use the linux distribution as you

00:18:30,870 --> 00:18:36,180
want to 1604 I just use that because

00:18:34,530 --> 00:18:40,980
that's what we use internally welcome as

00:18:36,180 --> 00:18:45,510
well so coming back to frequency

00:18:40,980 --> 00:18:49,380
invariants on x86 is pretty strange for

00:18:45,510 --> 00:18:50,730
someone has worked on arm a lot the

00:18:49,380 --> 00:18:52,290
hardware actually does a lot of work

00:18:50,730 --> 00:18:53,700
with respect to even if you use the

00:18:52,290 --> 00:18:55,110
software governors there's a lot of

00:18:53,700 --> 00:18:57,330
things going down and going on in

00:18:55,110 --> 00:18:58,860
hardware even though CPU freakazoid

00:18:57,330 --> 00:19:00,930
policy if you take an eight-core machine

00:18:58,860 --> 00:19:03,030
see you free has a separate policy for

00:19:00,930 --> 00:19:06,270
each CPU and you can actually have a

00:19:03,030 --> 00:19:07,800
different word the actual frequency that

00:19:06,270 --> 00:19:10,860
each CPU runs is basically the

00:19:07,800 --> 00:19:13,440
aggregated max of all the active CPUs

00:19:10,860 --> 00:19:15,060
would so if you have two CPUs and one

00:19:13,440 --> 00:19:17,850
cpu is worried for two gigahertz and

00:19:15,060 --> 00:19:20,160
goes to sleep the frequency is changed

00:19:17,850 --> 00:19:22,200
under software and since that software

00:19:20,160 --> 00:19:25,640
doesn't know about it to the other

00:19:22,200 --> 00:19:25,640
frequencies what if it was lower

00:19:26,299 --> 00:19:31,529
there's also something called turbo

00:19:28,169 --> 00:19:34,639
boost so depending number of idle CPUs

00:19:31,529 --> 00:19:38,369
you might actually overclock your cpu

00:19:34,639 --> 00:19:39,479
frequency something that's much which

00:19:38,369 --> 00:19:41,429
something that's actually higher than

00:19:39,479 --> 00:19:44,219
what's listed in the scaling available

00:19:41,429 --> 00:19:45,599
frequencies so if you have an extra six

00:19:44,219 --> 00:19:47,339
laptop right now and you cat you're

00:19:45,599 --> 00:19:48,899
scaling available frequencies you'll

00:19:47,339 --> 00:19:51,509
notice a very strange one at the top

00:19:48,899 --> 00:19:54,719
that's just one megahertz above the next

00:19:51,509 --> 00:19:56,819
closest one so it's 3.4 gigahertz and

00:19:54,719 --> 00:20:00,329
then it's 3.41 because for some reason

00:19:56,819 --> 00:20:02,519
that 3.41 allows the enabling of this

00:20:00,329 --> 00:20:04,499
turbo boost mode which can take your

00:20:02,519 --> 00:20:06,719
frequency up to something like 3.8

00:20:04,499 --> 00:20:11,119
because as well any reason I figured

00:20:06,719 --> 00:20:13,559
this out is because I ran like a simple

00:20:11,119 --> 00:20:16,109
sis bench and I found varying results

00:20:13,559 --> 00:20:21,599
depending on a number of idle course and

00:20:16,109 --> 00:20:24,659
then obviously the internet helped so

00:20:21,599 --> 00:20:27,389
when we did frequency invariance on with

00:20:24,659 --> 00:20:29,789
the vault patch I decided that we needed

00:20:27,389 --> 00:20:31,799
something more accurate so we use the

00:20:29,789 --> 00:20:35,129
Intel performance monitoring encounters

00:20:31,799 --> 00:20:37,109
and those are pretty well supported and

00:20:35,129 --> 00:20:40,229
there's a single instruction that you

00:20:37,109 --> 00:20:42,239
can use to read the cycle counter the

00:20:40,229 --> 00:20:44,159
problem there is that the one problem

00:20:42,239 --> 00:20:46,489
there is that you can't read or at least

00:20:44,159 --> 00:20:49,589
I haven't been able to read remote CPUs

00:20:46,489 --> 00:20:51,539
counter so I had to make sure that each

00:20:49,589 --> 00:20:53,759
CPU tries to keep its counter updated as

00:20:51,539 --> 00:20:55,829
much as possible that adds some overhead

00:20:53,759 --> 00:20:57,119
which I can try to reduce but I don't

00:20:55,829 --> 00:21:00,839
know if upstream even cares about this

00:20:57,119 --> 00:21:03,059
because it doesn't seem to affect test

00:21:00,839 --> 00:21:04,799
cases all that much but I do believe

00:21:03,059 --> 00:21:08,549
that if you go into things like frame

00:21:04,799 --> 00:21:12,959
Layton sees or what's the exact word for

00:21:08,549 --> 00:21:14,279
it percentiles yes then think those

00:21:12,959 --> 00:21:20,369
those kind of use cases should be

00:21:14,279 --> 00:21:21,629
affected by inaccurate load tracking so

00:21:20,369 --> 00:21:25,649
these are some actual results from

00:21:21,629 --> 00:21:27,329
faronics there's like a ton of tests and

00:21:25,649 --> 00:21:28,619
I have results for all of those but if I

00:21:27,329 --> 00:21:30,389
put them that nobody would remember a

00:21:28,619 --> 00:21:32,429
single one and I doubt you'll remember

00:21:30,389 --> 00:21:35,369
these as well because there the results

00:21:32,429 --> 00:21:36,550
have kind of the same for all these

00:21:35,369 --> 00:21:39,580
three combinations

00:21:36,550 --> 00:21:40,990
I've tried a different window sizes for

00:21:39,580 --> 00:21:42,640
Walt just to see if would make a

00:21:40,990 --> 00:21:45,250
difference but these are pretty CPU

00:21:42,640 --> 00:21:50,080
intensive use cases except for probably

00:21:45,250 --> 00:21:51,760
FFM for the mp4 encoding all of these

00:21:50,080 --> 00:21:54,550
show relatively the same performance

00:21:51,760 --> 00:21:56,880
with these governors and with Walt and

00:21:54,550 --> 00:21:56,880
belched

00:22:05,509 --> 00:22:12,109
so these are some of the graphics or

00:22:09,840 --> 00:22:16,799
gaming benchmarks which actually shows

00:22:12,109 --> 00:22:19,019
useful FPS information we did see some

00:22:16,799 --> 00:22:21,749
power savings with Walt on the open

00:22:19,019 --> 00:22:26,279
arena one not too much within variance

00:22:21,749 --> 00:22:28,710
but still consistently lesser power for

00:22:26,279 --> 00:22:31,919
unique engine valley which is much more

00:22:28,710 --> 00:22:35,220
complicated 3d scene with a lot more

00:22:31,919 --> 00:22:38,039
objects I guess a lot more polygons or

00:22:35,220 --> 00:22:40,169
triangles or how we want to put it the

00:22:38,039 --> 00:22:43,470
power was almost exactly the same and so

00:22:40,169 --> 00:22:47,129
was the FPS for tesseract the second one

00:22:43,470 --> 00:22:48,629
pelt Pelt frequency rate residency is

00:22:47,129 --> 00:22:51,479
considerably higher and the power is

00:22:48,629 --> 00:22:55,159
considerably worse so that's why you

00:22:51,479 --> 00:22:57,090
have this abnormally high FPS number I

00:22:55,159 --> 00:22:59,820
haven't been able to figure out how to

00:22:57,090 --> 00:23:02,940
restrict these FPSs to vsync once i

00:22:59,820 --> 00:23:07,590
switch to the nvidia driver for my

00:23:02,940 --> 00:23:09,809
machine when I use the open sources the

00:23:07,590 --> 00:23:12,629
upstream Nouveau driver for nvidia

00:23:09,809 --> 00:23:15,690
graphics cards the FPS numbers dropped

00:23:12,629 --> 00:23:18,840
radically I've just included the power

00:23:15,690 --> 00:23:21,869
numbers basically sure that it's just on

00:23:18,840 --> 00:23:23,700
demand keeping the frequency high but I

00:23:21,869 --> 00:23:25,499
don't think those are relevant so

00:23:23,700 --> 00:23:28,109
basically the takeaway from the slide is

00:23:25,499 --> 00:23:30,720
the FPS numbers are also more or less

00:23:28,109 --> 00:23:39,539
the same between these three

00:23:30,720 --> 00:23:42,570
configurations so this is some test

00:23:39,539 --> 00:23:46,889
cases that I came up it just to be able

00:23:42,570 --> 00:23:49,499
to measure power what I do is just start

00:23:46,889 --> 00:23:52,379
HTTP server on the local subnet which

00:23:49,499 --> 00:23:57,749
hosts these videos this is the Big Buck

00:23:52,379 --> 00:23:59,519
Bunny usual pretty standard video so and

00:23:57,749 --> 00:24:02,489
then the machine that's running schedule

00:23:59,519 --> 00:24:05,580
or any of these configurations loads up

00:24:02,489 --> 00:24:08,700
that video in a browser and plays about

00:24:05,580 --> 00:24:11,700
two minutes of video so this is testing

00:24:08,700 --> 00:24:16,320
put the network stack power as well as

00:24:11,700 --> 00:24:18,690
the video decoding power we see huge

00:24:16,320 --> 00:24:23,430
savings for 1080p with

00:24:18,690 --> 00:24:25,260
Walt I have not been able to get

00:24:23,430 --> 00:24:26,310
something like flame Layton sees or

00:24:25,260 --> 00:24:29,190
percentiles or something like that that

00:24:26,310 --> 00:24:31,100
we better statistic but there were no

00:24:29,190 --> 00:24:34,320
framedrops so I guess that's something

00:24:31,100 --> 00:24:37,050
but ideally to believe this data i would

00:24:34,320 --> 00:24:38,940
also want to look at rendering Layton

00:24:37,050 --> 00:24:42,630
sees or something that's that I can try

00:24:38,940 --> 00:24:45,020
to find on x86 when you switch to 4k

00:24:42,630 --> 00:24:47,970
videos the power becomes pretty intense

00:24:45,020 --> 00:24:50,310
and the differences between them between

00:24:47,970 --> 00:24:52,050
the three configurations disappear but

00:24:50,310 --> 00:24:53,850
yeah I fear if you watch a lot of

00:24:52,050 --> 00:24:55,050
YouTube I suggest you switch to scare

00:24:53,850 --> 00:25:00,750
you till to save some power on your

00:24:55,050 --> 00:25:03,840
laptops the other interesting note here

00:25:00,750 --> 00:25:05,490
is that Firefox seems to be way worse at

00:25:03,840 --> 00:25:09,000
power-saving than Chrome I have no idea

00:25:05,490 --> 00:25:11,700
why but i think it's just I don't know

00:25:09,000 --> 00:25:14,610
if it's using the same decoder as chrome

00:25:11,700 --> 00:25:16,770
is using so the takeaway from this slide

00:25:14,610 --> 00:25:19,320
is there is definitely potential for

00:25:16,770 --> 00:25:21,630
lower powerful Walt but we need more

00:25:19,320 --> 00:25:24,990
complete data to confirm this I'm trying

00:25:21,630 --> 00:25:26,700
to also look for more interactive test

00:25:24,990 --> 00:25:28,950
cases on x86 there are a couple of

00:25:26,700 --> 00:25:30,840
browser-based benchmarks there is in

00:25:28,950 --> 00:25:32,250
fact a chrome rendering latency

00:25:30,840 --> 00:25:34,830
benchmark but those do not give me

00:25:32,250 --> 00:25:37,350
consistent results so i have not include

00:25:34,830 --> 00:25:38,790
them but hopefully by the time lkm will

00:25:37,350 --> 00:25:41,600
comes around i can try to include

00:25:38,790 --> 00:25:45,110
something that concretely without doubt

00:25:41,600 --> 00:25:45,110
shows the difference

00:25:54,890 --> 00:26:01,020
forward window size was 20 milliseconds

00:25:58,950 --> 00:26:03,240
for on-demand I honestly don't remember

00:26:01,020 --> 00:26:06,150
it was just the stock thing that came

00:26:03,240 --> 00:26:13,470
with ubuntu so I can look that up and

00:26:06,150 --> 00:26:16,380
add that in so also did VLC video

00:26:13,470 --> 00:26:18,780
playback similar to the test previously

00:26:16,380 --> 00:26:20,330
but this just uses a local video file

00:26:18,780 --> 00:26:22,140
instead of streaming it or the network

00:26:20,330 --> 00:26:25,530
there was not that much of a difference

00:26:22,140 --> 00:26:27,210
between belt and Walt but again 40 frame

00:26:25,530 --> 00:26:30,210
drops scared you tell saves way more

00:26:27,210 --> 00:26:31,800
power than on demand so yeah if you

00:26:30,210 --> 00:26:34,920
watch lot of video you should switch to

00:26:31,800 --> 00:26:37,650
scare you did the interesting thing here

00:26:34,920 --> 00:26:40,950
is that I haven't included the 2160 or

00:26:37,650 --> 00:26:43,530
the 4k video because both schedule +

00:26:40,950 --> 00:26:46,680
Pelt and schedule + Walt actually start

00:26:43,530 --> 00:26:48,150
dropping frames which is which makes

00:26:46,680 --> 00:26:50,040
sense because on-demand just keeps the

00:26:48,150 --> 00:26:51,300
frequency high it has enough esterases

00:26:50,040 --> 00:26:53,610
to take care of this use case but I

00:26:51,300 --> 00:26:55,020
think this is a pretty important use

00:26:53,610 --> 00:26:56,970
case because it highlights a real

00:26:55,020 --> 00:26:59,460
difference between scheduled to land on

00:26:56,970 --> 00:27:01,380
demand and I'm trying to actually get

00:26:59,460 --> 00:27:03,060
I've sent an email and I'm trying to get

00:27:01,380 --> 00:27:11,640
the photonics guys to include something

00:27:03,060 --> 00:27:13,620
like this so the conclusions the main

00:27:11,640 --> 00:27:15,270
conclusion from the data itself is that

00:27:13,620 --> 00:27:19,050
we do believe that work / power is

00:27:15,270 --> 00:27:21,660
likely better but we need more data the

00:27:19,050 --> 00:27:24,180
real advantage of Walt is really going

00:27:21,660 --> 00:27:25,770
to be the task tracking rather than

00:27:24,180 --> 00:27:28,710
frequency guidance like I said this is

00:27:25,770 --> 00:27:30,270
just to make the bats it simple and have

00:27:28,710 --> 00:27:31,830
a working prototype that the upstream

00:27:30,270 --> 00:27:36,510
antennas can actually run and see how

00:27:31,830 --> 00:27:40,200
the window stats are accumulated we hope

00:27:36,510 --> 00:27:43,010
to post real data on a very pertinent

00:27:40,200 --> 00:27:45,390
arm device I would call it pretty soon

00:27:43,010 --> 00:27:51,420
and that should really clear any doubt

00:27:45,390 --> 00:27:53,460
about Walt was a spelt the actual next

00:27:51,420 --> 00:27:54,930
steps for the patch itself like I

00:27:53,460 --> 00:27:58,560
highlighted earlier is to eliminate

00:27:54,930 --> 00:28:02,670
double-locking integrate the invariants

00:27:58,560 --> 00:28:04,020
api's and attempt unsynchronized we want

00:28:02,670 --> 00:28:06,480
to do the double locking

00:28:04,020 --> 00:28:09,210
and the upstream invariants appears

00:28:06,480 --> 00:28:11,460
before LPC for sure we're trying hard to

00:28:09,210 --> 00:28:13,230
get to that point for attempting

00:28:11,460 --> 00:28:16,590
unsynchronized window patches i think

00:28:13,230 --> 00:28:21,570
that's we won't delay our posting just

00:28:16,590 --> 00:28:23,310
because of that so that's about it for

00:28:21,570 --> 00:28:36,170
me it's a pretty short presentation I

00:28:23,310 --> 00:28:39,420
guess any questions you got any data

00:28:36,170 --> 00:28:42,420
running the like rock band show this

00:28:39,420 --> 00:28:45,630
kind of bedrock doesn't mean it should

00:28:42,420 --> 00:28:47,850
show or not Oh breath in editing this

00:28:45,630 --> 00:28:49,530
team so I did not actually see a

00:28:47,850 --> 00:28:52,410
difference between when I ran / pinch

00:28:49,530 --> 00:28:53,460
and hack bench between the numbers which

00:28:52,410 --> 00:28:55,320
is why I didn't really put them up here

00:28:53,460 --> 00:28:58,110
because I wanted to show real world use

00:28:55,320 --> 00:29:02,370
cases but I will try to put that up for

00:28:58,110 --> 00:29:11,580
now when you post the bat-rope yeah I

00:29:02,370 --> 00:29:13,260
mean yeah holy SH reg'lar right yeah I

00:29:11,580 --> 00:29:16,920
did I think I'd end up of scared

00:29:13,260 --> 00:29:19,560
messaging thing and really not much of a

00:29:16,920 --> 00:29:21,350
difference one thing that we also want

00:29:19,560 --> 00:29:24,270
to do is actually instrument the Walt

00:29:21,350 --> 00:29:26,010
load tracking functions themselves like

00:29:24,270 --> 00:29:28,380
the math in the Walt and see how much of

00:29:26,010 --> 00:29:31,170
an over and it is versus the pelt update

00:29:28,380 --> 00:29:34,320
load average for example so even if you

00:29:31,170 --> 00:29:36,360
don't see an end difference in the

00:29:34,320 --> 00:29:37,770
overhead or the real world use cases you

00:29:36,360 --> 00:29:39,630
can actually slice to establish if there

00:29:37,770 --> 00:29:46,410
is some overhead that we're adding that

00:29:39,630 --> 00:29:49,890
way this is perhaps slightly off topic

00:29:46,410 --> 00:29:51,570
but you mentioned the fast switch right

00:29:49,890 --> 00:29:53,430
and how you guys did some modifications

00:29:51,570 --> 00:29:56,670
to wall to make sure you didn't have a

00:29:53,430 --> 00:29:58,230
regression there is there any data out

00:29:56,670 --> 00:29:59,970
there on how big a difference that

00:29:58,230 --> 00:30:01,740
really makes doing the transition /

00:29:59,970 --> 00:30:05,130
booth and the scheduler context for just

00:30:01,740 --> 00:30:07,080
spawning it out I have not seen data and

00:30:05,130 --> 00:30:09,750
have not measured it personally it was

00:30:07,080 --> 00:30:10,920
just from a theoretical point of view we

00:30:09,750 --> 00:30:12,750
want to make sure a low tech news

00:30:10,920 --> 00:30:15,230
accurate but we do intend to measure

00:30:12,750 --> 00:30:15,230
that as well

00:30:18,110 --> 00:30:22,920
one thing I cannot gotten on today from

00:30:20,850 --> 00:30:24,900
stima how was that I think when they

00:30:22,920 --> 00:30:26,220
don't do fast switching the thread

00:30:24,900 --> 00:30:28,890
that's running is a normal priority

00:30:26,220 --> 00:30:31,350
thread so it could be superseded by any

00:30:28,890 --> 00:30:33,840
other higher priority thread or ERT

00:30:31,350 --> 00:30:36,030
threads so that's going to add a lot of

00:30:33,840 --> 00:30:37,740
latency depending on the condition so

00:30:36,030 --> 00:30:39,600
maybe if you do make it our teeth read

00:30:37,740 --> 00:30:41,460
it might not be a significant difference

00:30:39,600 --> 00:30:43,680
but at least the way it is right now I

00:30:41,460 --> 00:30:45,360
would believe it'll be at least

00:30:43,680 --> 00:30:48,840
depending on this case it could be

00:30:45,360 --> 00:30:50,190
significant so yeah I think given that I

00:30:48,840 --> 00:30:51,870
don't know if there's any um device out

00:30:50,190 --> 00:30:53,790
there that has fast switching we really

00:30:51,870 --> 00:30:58,200
don't have data but I think it should

00:30:53,790 --> 00:31:01,710
make a difference for mobile uploads is

00:30:58,200 --> 00:31:05,520
there any data Apollo I already a test

00:31:01,710 --> 00:31:08,520
or a PP lunch oh do you say I or testing

00:31:05,520 --> 00:31:10,740
yes so I or testing at least from the

00:31:08,520 --> 00:31:14,430
faronics x86 benchmarks I did not see a

00:31:10,740 --> 00:31:17,220
difference but we will probably post

00:31:14,430 --> 00:31:19,200
will post data on the lamp device soon

00:31:17,220 --> 00:31:22,260
and then we should be able to see some

00:31:19,200 --> 00:31:25,680
differences there and you mentioned the

00:31:22,260 --> 00:31:28,260
white tie would not be count in the CPU

00:31:25,680 --> 00:31:33,750
time pathak hung in the testament right

00:31:28,260 --> 00:31:37,350
so it in color I OPC is the I OPC time I

00:31:33,750 --> 00:31:38,880
oh wait time yes yeah we do have a

00:31:37,350 --> 00:31:41,630
tunable that allows you to include I oh

00:31:38,880 --> 00:31:41,630
wait time as well

00:31:46,340 --> 00:31:52,740
they seem to have like some devices

00:31:49,680 --> 00:31:55,620
listed so I think they are trying at

00:31:52,740 --> 00:31:58,590
least but honestly if if we could get

00:31:55,620 --> 00:32:00,150
like an arm farm but all of us could

00:31:58,590 --> 00:32:03,030
just cue tests and run it and get

00:32:00,150 --> 00:32:13,020
results I'd be ideal and be a great

00:32:03,030 --> 00:32:14,340
world to live in now I think it's into

00:32:13,020 --> 00:32:15,720
everybody's advantage also if every

00:32:14,340 --> 00:32:18,690
she's really trying to converge upstream

00:32:15,720 --> 00:32:20,340
then OEMs and vendors and my company of

00:32:18,690 --> 00:32:31,440
course should be really interested in

00:32:20,340 --> 00:32:33,210
this just wanted to broach the topic so

00:32:31,440 --> 00:32:34,590
that others in the room can also because

00:32:33,210 --> 00:32:36,270
we kind of know this already right so

00:32:34,590 --> 00:32:39,950
you're building up to make a posting on

00:32:36,270 --> 00:32:42,090
Elk ml we're between arm linaro and

00:32:39,950 --> 00:32:44,940
Google all of us we want to try and help

00:32:42,090 --> 00:32:47,220
right so you mentioned next steps over

00:32:44,940 --> 00:32:49,260
here is your intention to close out all

00:32:47,220 --> 00:32:51,420
of those before you do a posting ahead

00:32:49,260 --> 00:32:53,280
of the LPC oh so like I said we want to

00:32:51,420 --> 00:32:56,070
close out the double locking thing okay

00:32:53,280 --> 00:32:57,570
want to close out the invariants api's I

00:32:56,070 --> 00:33:00,330
think those should be pretty simple

00:32:57,570 --> 00:33:01,830
enough to do but the unsynchronized

00:33:00,330 --> 00:33:03,960
clock sources is something that will

00:33:01,830 --> 00:33:05,700
have to I don't think it'd be able to

00:33:03,960 --> 00:33:07,170
get to it before finish it before that's

00:33:05,700 --> 00:33:09,360
okay right we can call that out as a

00:33:07,170 --> 00:33:12,480
to-do thing right here and in terms of

00:33:09,360 --> 00:33:14,940
the data I think we've got substantial

00:33:12,480 --> 00:33:17,880
later now right do we intend closing out

00:33:14,940 --> 00:33:20,490
with further analysis oh I do for

00:33:17,880 --> 00:33:24,440
boosting up I mean I do want to provide

00:33:20,490 --> 00:33:26,700
some real x86 defenses right because

00:33:24,440 --> 00:33:28,080
like we discussed right real-world use

00:33:26,700 --> 00:33:29,430
cases but something that you can

00:33:28,080 --> 00:33:32,640
actually run then a maintainer

00:33:29,430 --> 00:33:34,350
themselves can run that'd be ideal oh

00:33:32,640 --> 00:33:37,200
yeah but yeah we do have sufficient data

00:33:34,350 --> 00:33:39,350
on for example okay and finally for the

00:33:37,200 --> 00:33:42,390
benefit of others the intent is that

00:33:39,350 --> 00:33:44,910
basically vehicle to circle around what

00:33:42,390 --> 00:33:47,340
should be the text for the posting right

00:33:44,910 --> 00:33:49,460
in terms of its kind of a summary of

00:33:47,340 --> 00:33:51,720
your presentation right yeah it's like

00:33:49,460 --> 00:33:54,600
here's why volt is better kind of thing

00:33:51,720 --> 00:33:56,840
and here's some data to support our

00:33:54,600 --> 00:33:56,840
documentary

00:33:58,679 --> 00:34:03,580
I would also say that the for example I

00:34:01,360 --> 00:34:05,590
slides were you basically we're saying

00:34:03,580 --> 00:34:07,389
that like three windows you reach the

00:34:05,590 --> 00:34:10,599
max so it's kind of mathematical

00:34:07,389 --> 00:34:12,520
property of the world compared to bed so

00:34:10,599 --> 00:34:15,669
something like these like comparing

00:34:12,520 --> 00:34:33,849
mathematically world and belt on area

00:34:15,669 --> 00:34:37,899
unless simple example I think help yes

00:34:33,849 --> 00:34:39,429
we do the only thing is just how we

00:34:37,899 --> 00:34:42,280
release the data we got to do that

00:34:39,429 --> 00:34:43,690
pretty soon this is the Intel platform

00:34:42,280 --> 00:34:46,060
the reason it's Intel is because the

00:34:43,690 --> 00:34:47,980
upstream maintenance are all in working

00:34:46,060 --> 00:34:50,589
in Tel so I want them to be able to

00:34:47,980 --> 00:34:52,450
actually make this relevant to them but

00:34:50,589 --> 00:34:54,490
are obviously our strong suite is the

00:34:52,450 --> 00:35:09,099
mobile workloads arm data which we

00:34:54,490 --> 00:35:12,910
planted is pretty soon i think the cpu

00:35:09,099 --> 00:35:14,140
usage is pretty intensive yeah i think i

00:35:12,910 --> 00:35:15,910
don't know how much of the actual

00:35:14,140 --> 00:35:17,770
hardware is being used for the decoding

00:35:15,910 --> 00:35:19,839
but as far as i can look at the

00:35:17,770 --> 00:35:21,910
frequency residences and the busy time

00:35:19,839 --> 00:35:25,230
of the cpu it's pretty intensive

00:35:21,910 --> 00:35:25,230
compared to the 1080p case

00:35:31,630 --> 00:35:35,150
how costly do you think the frequency

00:35:33,829 --> 00:35:37,779
and variance feature is to add so i

00:35:35,150 --> 00:35:40,269
actually would find that useful as well

00:35:37,779 --> 00:35:43,849
for some list of them work done and yeah

00:35:40,269 --> 00:35:46,579
so right now it actually reduces the

00:35:43,849 --> 00:35:49,519
absolute values of the benchmarks quite

00:35:46,579 --> 00:35:50,599
a bit because I've done it because I

00:35:49,519 --> 00:35:51,680
mean it's a horrible implementation

00:35:50,599 --> 00:35:53,900
right now because I didn't care about

00:35:51,680 --> 00:35:56,000
absolute numbers what I've done is

00:35:53,900 --> 00:35:58,460
whenever the RQ clock is updated I've

00:35:56,000 --> 00:36:00,019
read this register which is insane so I

00:35:58,460 --> 00:36:02,509
think if you just keep it updated in the

00:36:00,019 --> 00:36:04,400
tick baths and the NQ DQ and a few other

00:36:02,509 --> 00:36:06,769
places I think the overhead should not

00:36:04,400 --> 00:36:09,349
be that much and the accuracy should be

00:36:06,769 --> 00:36:11,750
good enough is what I field but that

00:36:09,349 --> 00:36:14,240
needs to be evaluated okay and then as a

00:36:11,750 --> 00:36:15,950
follow-up to that I'm just sort of

00:36:14,240 --> 00:36:19,339
morbidly curious what what kind of

00:36:15,950 --> 00:36:22,460
reaction are you expecting to this when

00:36:19,339 --> 00:36:23,599
it hits the list yeah we've just wanted

00:36:22,460 --> 00:36:25,819
to replace this thing that's been

00:36:23,599 --> 00:36:27,170
working before yes so yeah I think the

00:36:25,819 --> 00:36:29,660
reaction is going to be pretty explosive

00:36:27,170 --> 00:36:31,250
maybe but if you build up enough if we

00:36:29,660 --> 00:36:33,859
have enough good enough theory and good

00:36:31,250 --> 00:36:37,640
enough data and we don't do something

00:36:33,859 --> 00:36:39,650
that immediately pisses them off then I

00:36:37,640 --> 00:36:41,390
think we should have something of a shot

00:36:39,650 --> 00:36:42,619
I do believe that obviously the first

00:36:41,390 --> 00:36:44,990
question is why can't you make belt

00:36:42,619 --> 00:36:47,750
better and there are everyone is trying

00:36:44,990 --> 00:36:49,369
to narrower trying to address that as

00:36:47,750 --> 00:36:51,319
well I guess what's your best guess if

00:36:49,369 --> 00:36:52,819
you had to put a timeline on how long

00:36:51,319 --> 00:36:54,799
you think it'll take to get this

00:36:52,819 --> 00:36:56,180
assuming you can get it in how long do

00:36:54,799 --> 00:37:02,509
you think that that will that battle

00:36:56,180 --> 00:37:05,799
will last honestly I have I don't have

00:37:02,509 --> 00:37:05,799
any experience to answer that question

00:37:07,960 --> 00:37:12,220
so you probably want to say the same

00:37:10,460 --> 00:37:15,799
thing will not say it anyways really I

00:37:12,220 --> 00:37:17,390
don't think we should necessarily set

00:37:15,799 --> 00:37:20,059
our ambition at replacing paired with

00:37:17,390 --> 00:37:21,950
world frankly right I think what we're

00:37:20,059 --> 00:37:23,900
really trying to do is start the

00:37:21,950 --> 00:37:27,470
conversation which results in belt being

00:37:23,900 --> 00:37:29,210
improved for mobile and I think you guys

00:37:27,470 --> 00:37:31,130
all agree that if that does happen which

00:37:29,210 --> 00:37:32,720
in itself is also quite a lofty ambition

00:37:31,130 --> 00:37:36,710
absolutely it'll be happy outcome for

00:37:32,720 --> 00:37:37,700
all of us right yeah yeah from a data

00:37:36,710 --> 00:37:39,619
point of view they're going to be very

00:37:37,700 --> 00:37:42,350
ambitious from a patch point of view are

00:37:39,619 --> 00:37:45,070
going to be very less empty yeah

00:37:42,350 --> 00:37:48,170
some what similar Mayor was actually

00:37:45,070 --> 00:37:51,020
when risk if you are going to put

00:37:48,170 --> 00:37:54,290
position DIFC like we want to replace

00:37:51,020 --> 00:37:58,010
belt or one DC maybe to be alongside

00:37:54,290 --> 00:37:59,810
belt that is kind of annoyed few so we

00:37:58,010 --> 00:38:02,420
have given that a bit of thought right

00:37:59,810 --> 00:38:05,060
now we want to we're not going to say

00:38:02,420 --> 00:38:06,890
either way explicitly which is going to

00:38:05,060 --> 00:38:08,240
say these are the mobile workers these

00:38:06,890 --> 00:38:10,820
are the data we do believe there are

00:38:08,240 --> 00:38:12,500
shortcomings with belt world is an

00:38:10,820 --> 00:38:14,720
alternative that you might want to

00:38:12,500 --> 00:38:16,130
consider and then we start the

00:38:14,720 --> 00:38:17,030
conversation like he said it's obviously

00:38:16,130 --> 00:38:24,620
about the starting the conversation

00:38:17,030 --> 00:38:28,520
think that's most important we could

00:38:24,620 --> 00:38:30,440
mention having good data and a decent

00:38:28,520 --> 00:38:32,690
amount of patch sets would be a good way

00:38:30,440 --> 00:38:35,090
to start moving the conversation but in

00:38:32,690 --> 00:38:38,150
equally critical part is maybe more

00:38:35,090 --> 00:38:40,790
armband us can speak up and say hey this

00:38:38,150 --> 00:38:42,440
would help us out we see the same I

00:38:40,790 --> 00:38:44,330
shear students are just being ha calm

00:38:42,440 --> 00:38:46,670
saying hey we hear these issues and

00:38:44,330 --> 00:38:52,550
that'd be a really good point to try to

00:38:46,670 --> 00:38:54,050
move the Sahara so perhaps you can in a

00:38:52,550 --> 00:38:56,690
public forum just say that yes I

00:38:54,050 --> 00:38:58,910
engineers will send her tested by for

00:38:56,690 --> 00:39:00,920
some of this stuff on el camaron you

00:38:58,910 --> 00:39:04,280
don't have to answer to that I'm just

00:39:00,920 --> 00:39:05,480
winding you up he's got a good point

00:39:04,280 --> 00:39:08,240
right i mean like right now it's

00:39:05,480 --> 00:39:10,370
Qualcomm and lin are on us so the other

00:39:08,240 --> 00:39:13,610
hand if and by the way there's earned

00:39:10,370 --> 00:39:15,920
there's a classic example of how you

00:39:13,610 --> 00:39:18,020
know one or two windows providing tested

00:39:15,920 --> 00:39:20,000
by zanele kml helltrain this is like

00:39:18,020 --> 00:39:21,800
last month when we are which pad set did

00:39:20,000 --> 00:39:24,080
we get an act from raining sauce and a

00:39:21,800 --> 00:39:25,580
knock from somebody else and that's it

00:39:24,080 --> 00:39:28,100
like in a couple of weeks the patch was

00:39:25,580 --> 00:39:29,300
merged right so okay that's another kind

00:39:28,100 --> 00:39:31,490
of projects i travel what I'm really

00:39:29,300 --> 00:39:33,020
saying is if you get more support as i

00:39:31,490 --> 00:39:36,290
said what i was saying makes a big

00:39:33,020 --> 00:39:39,820
difference sorry i used you as a cheap

00:39:36,290 --> 00:39:39,820
shorter target right

00:39:47,030 --> 00:39:52,250
okay I think we're done thank you

00:40:00,890 --> 00:40:02,950

YouTube URL: https://www.youtube.com/watch?v=hc0aufpfWvY


