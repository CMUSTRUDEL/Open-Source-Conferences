Title: YVR18- 331 MXNet and ONNX
Publication date: 2018-09-28
Playlist: Linaro Connect Vancouver 2018
Description: 
	TBD
Captions: 
	00:00:02,150 --> 00:00:07,160
[Music]

00:00:08,410 --> 00:00:14,089
so thanks everyone for joining in the

00:00:10,940 --> 00:00:16,520
session today so I'm Tom from AWS here

00:00:14,089 --> 00:00:18,530
in Vancouver so I firstly like to thank

00:00:16,520 --> 00:00:20,510
the conference organizers for picking

00:00:18,530 --> 00:00:22,220
such a great location that's one block

00:00:20,510 --> 00:00:23,360
away from my office so that kind of

00:00:22,220 --> 00:00:26,960
helped that was pretty handy again

00:00:23,360 --> 00:00:29,780
across and so AWS I work as an applied

00:00:26,960 --> 00:00:33,940
scientist and I help internal and

00:00:29,780 --> 00:00:36,140
external customers deploy MX net models

00:00:33,940 --> 00:00:38,420
so the subjects I'd like to talk about

00:00:36,140 --> 00:00:40,100
today is onyx which we've heard a few

00:00:38,420 --> 00:00:42,440
times mentioned in presentations and

00:00:40,100 --> 00:00:43,850
then taking onyx all the way to the edge

00:00:42,440 --> 00:00:46,910
and looking at edge deployments with

00:00:43,850 --> 00:00:48,620
onyx so a quick agenda what we're going

00:00:46,910 --> 00:00:50,960
to look at is just look a bit more

00:00:48,620 --> 00:00:54,260
detail into onyx we're gonna look at how

00:00:50,960 --> 00:00:56,690
to create onyx models or find them look

00:00:54,260 --> 00:00:58,579
at a visualization tool then look at

00:00:56,690 --> 00:01:00,530
deployment options and lastly just

00:00:58,579 --> 00:01:03,680
briefly talk about optimizations that we

00:01:00,530 --> 00:01:08,330
can do with these onyx models so what is

00:01:03,680 --> 00:01:11,080
onyx so onyx as we've heard is a common

00:01:08,330 --> 00:01:14,180
format for neural network models as

00:01:11,080 --> 00:01:16,070
every framework previously implemented

00:01:14,180 --> 00:01:18,950
their own model format so you'd get

00:01:16,070 --> 00:01:22,310
quite a large degree of locking to a

00:01:18,950 --> 00:01:24,200
framework and model researchers would

00:01:22,310 --> 00:01:25,670
have to pick a framework tool developers

00:01:24,200 --> 00:01:27,260
would have to pick one and you wouldn't

00:01:25,670 --> 00:01:29,420
get that cross compatibility across

00:01:27,260 --> 00:01:31,310
frameworks so having this standardized

00:01:29,420 --> 00:01:33,890
format is obviously beneficial it allows

00:01:31,310 --> 00:01:35,600
researchers to share their models means

00:01:33,890 --> 00:01:36,979
you can use a lot more of the tooling

00:01:35,600 --> 00:01:38,840
that's been provided around neural

00:01:36,979 --> 00:01:41,450
networks and exchange between the

00:01:38,840 --> 00:01:43,939
frameworks and so onyx is this

00:01:41,450 --> 00:01:48,049
interchange formats that stands for open

00:01:43,939 --> 00:01:51,170
neural network exchange format and the

00:01:48,049 --> 00:01:53,330
key data structure here is the graph so

00:01:51,170 --> 00:01:56,180
we're looking at a computational graph

00:01:53,330 --> 00:01:57,650
and the nodes of this graph are certain

00:01:56,180 --> 00:02:01,700
operators that are going to be applied

00:01:57,650 --> 00:02:03,950
to some input data in addition to having

00:02:01,700 --> 00:02:05,930
a set of operators that have been

00:02:03,950 --> 00:02:08,229
defined in onyx you have different types

00:02:05,930 --> 00:02:11,540
of metadata that you can attach to the

00:02:08,229 --> 00:02:13,730
operators and the graph as a whole and

00:02:11,540 --> 00:02:13,910
then the operators are implemented in

00:02:13,730 --> 00:02:17,330
the

00:02:13,910 --> 00:02:19,600
the frameworks so that's both from an

00:02:17,330 --> 00:02:21,980
import and export perspective you can

00:02:19,600 --> 00:02:23,660
depends on the framework some frameworks

00:02:21,980 --> 00:02:25,700
you just have the import capability or

00:02:23,660 --> 00:02:27,080
export capability MX net is one of the

00:02:25,700 --> 00:02:29,300
frameworks where you can go both ways

00:02:27,080 --> 00:02:31,130
you can train a model an export or you

00:02:29,300 --> 00:02:34,570
can just import a model and run it for

00:02:31,130 --> 00:02:37,160
inference and the different sets of

00:02:34,570 --> 00:02:38,390
software that interact with Onix so

00:02:37,160 --> 00:02:40,940
we've obviously what the frameworks that

00:02:38,390 --> 00:02:44,450
I mentioned then you've got the runtimes

00:02:40,940 --> 00:02:46,790
and compilers so we've got a Nvidia with

00:02:44,450 --> 00:02:49,910
10 to Rathi TVM as an example of a

00:02:46,790 --> 00:02:51,440
compiler and then lastly sort of some

00:02:49,910 --> 00:02:55,070
visualization tools so we're going to be

00:02:51,440 --> 00:02:56,420
looking at net run so this is just to

00:02:55,070 --> 00:02:59,360
highlight that there's a large number of

00:02:56,420 --> 00:03:01,400
partners that are supporting Onix so as

00:02:59,360 --> 00:03:03,470
a double yes we're supporting Onix we've

00:03:01,400 --> 00:03:06,290
got Facebook Microsoft and a huge number

00:03:03,470 --> 00:03:07,300
of others including arm so that's a good

00:03:06,290 --> 00:03:09,530
sign

00:03:07,300 --> 00:03:12,110
so let's start at the beginning where do

00:03:09,530 --> 00:03:16,220
we get onyx models from so a few

00:03:12,110 --> 00:03:18,140
different sources most likely starting

00:03:16,220 --> 00:03:20,060
point for a lot of users will be to go

00:03:18,140 --> 00:03:22,700
to a model zoo and there's actually a

00:03:20,060 --> 00:03:24,290
distinct onyx model zoo that has a

00:03:22,700 --> 00:03:26,780
number of pre trained models for a lot

00:03:24,290 --> 00:03:28,400
of different tasks and that's been added

00:03:26,780 --> 00:03:31,640
to all the time that's increasing quite

00:03:28,400 --> 00:03:33,980
a lot there's obviously model zoos for

00:03:31,640 --> 00:03:36,170
lots of different frameworks as well so

00:03:33,980 --> 00:03:38,450
maybe you found a certain computer

00:03:36,170 --> 00:03:41,270
vision model for a certain framework you

00:03:38,450 --> 00:03:45,140
can then use onyx to export that model

00:03:41,270 --> 00:03:47,450
and use the rest of your your software

00:03:45,140 --> 00:03:48,860
to process that model then on the other

00:03:47,450 --> 00:03:50,420
side of things if you've got more

00:03:48,860 --> 00:03:52,340
bespoke requirements you can obviously

00:03:50,420 --> 00:03:55,070
train your own neural network in all

00:03:52,340 --> 00:03:58,100
these frameworks and then export to onyx

00:03:55,070 --> 00:03:59,870
to use the whole process and then

00:03:58,100 --> 00:04:01,520
there's a middle ground which I think it

00:03:59,870 --> 00:04:04,220
will be quite common for a lot of users

00:04:01,520 --> 00:04:07,040
here is what's called fine-tuning so you

00:04:04,220 --> 00:04:08,900
take a model from the model zoo that's

00:04:07,040 --> 00:04:11,239
already been pre trained on a huge data

00:04:08,900 --> 00:04:14,150
set like image net for computer vision

00:04:11,239 --> 00:04:16,310
and then you have a smaller data set

00:04:14,150 --> 00:04:18,620
that you own that you can fine-tune the

00:04:16,310 --> 00:04:19,970
model to and so that's an example that

00:04:18,620 --> 00:04:23,330
I'd like to run through today of

00:04:19,970 --> 00:04:25,400
fine-tuning a computer vision model and

00:04:23,330 --> 00:04:27,410
to do that the two tools that I'll be

00:04:25,400 --> 00:04:30,560
talking about one is a

00:04:27,410 --> 00:04:32,990
TMX net which is a framework I thought

00:04:30,560 --> 00:04:35,630
I've been working with and we want to do

00:04:32,990 --> 00:04:38,210
this on AWS Sage Maker so let's just

00:04:35,630 --> 00:04:40,940
introduce those two a little bit later

00:04:38,210 --> 00:04:43,670
on before we look at any code so one

00:04:40,940 --> 00:04:44,900
consideration is the model that we're

00:04:43,670 --> 00:04:46,370
choosing for edge when we're looking

00:04:44,900 --> 00:04:47,390
through this model Zoo there's going to

00:04:46,370 --> 00:04:50,780
be lots and lots of different options

00:04:47,390 --> 00:04:52,070
for us so I'd just like to talk about a

00:04:50,780 --> 00:04:53,870
few of the trade-offs that we're going

00:04:52,070 --> 00:04:55,760
to have here so we obviously are

00:04:53,870 --> 00:04:59,900
interested in the model performance

00:04:55,760 --> 00:05:01,430
could be accuracy for your use case but

00:04:59,900 --> 00:05:03,230
there's trade-offs with the amount of

00:05:01,430 --> 00:05:05,150
computation that's required to gather

00:05:03,230 --> 00:05:09,170
accuracy and then the size of the model

00:05:05,150 --> 00:05:13,700
so a once upon a time state-of-the-art

00:05:09,170 --> 00:05:15,800
model was vgg 16 and a typical benchmark

00:05:13,700 --> 00:05:17,390
for this is its accuracy on image net

00:05:15,800 --> 00:05:19,700
which is an image classification task

00:05:17,390 --> 00:05:22,610
across a thousand different classes and

00:05:19,700 --> 00:05:25,370
so that achieved 71.5%

00:05:22,610 --> 00:05:28,250
but that came at the cost of a huge

00:05:25,370 --> 00:05:30,860
amount of computation and in terms of

00:05:28,250 --> 00:05:33,020
the size so there was 138 million

00:05:30,860 --> 00:05:34,550
parameters for that model but more

00:05:33,020 --> 00:05:37,340
recently there's been developments on

00:05:34,550 --> 00:05:40,190
much smaller models so one an example of

00:05:37,340 --> 00:05:42,800
that is the mobile net and you get

00:05:40,190 --> 00:05:45,020
roughly the same accuracy but for much

00:05:42,800 --> 00:05:49,940
much less on the computation and the

00:05:45,020 --> 00:05:51,710
size so here order of magnitude less

00:05:49,940 --> 00:05:53,750
parameters so it's going to be much much

00:05:51,710 --> 00:05:55,370
more efficient and then you'll see the

00:05:53,750 --> 00:05:57,500
likes of squeeze net models as well

00:05:55,370 --> 00:05:58,970
there's just a few other variants so

00:05:57,500 --> 00:06:02,060
that's what you should be looking more

00:05:58,970 --> 00:06:06,050
towards for edge devices so what makes

00:06:02,060 --> 00:06:08,330
these models special so it comes down to

00:06:06,050 --> 00:06:10,400
one of the convolutional layers so this

00:06:08,330 --> 00:06:12,470
is just one component where the saving

00:06:10,400 --> 00:06:15,050
is made with something called a depth

00:06:12,470 --> 00:06:16,850
wise separable convolution so I'm not

00:06:15,050 --> 00:06:19,760
going to run into too much the details

00:06:16,850 --> 00:06:22,850
behind this convolution but in a regular

00:06:19,760 --> 00:06:25,490
convolution you look across the spatial

00:06:22,850 --> 00:06:27,590
dimensions and all the input channels at

00:06:25,490 --> 00:06:29,270
the same time for a given output channel

00:06:27,590 --> 00:06:31,310
so here the kernel is being applied

00:06:29,270 --> 00:06:33,680
across there are three dimensions of an

00:06:31,310 --> 00:06:35,720
input but with a depth wise separable

00:06:33,680 --> 00:06:38,390
convolution you split this process out

00:06:35,720 --> 00:06:40,580
into two steps so the first step you

00:06:38,390 --> 00:06:42,949
just look across the spatial axes

00:06:40,580 --> 00:06:44,720
and you treat each channel separately so

00:06:42,949 --> 00:06:46,879
in this example we've got three input

00:06:44,720 --> 00:06:48,470
channels that means after the first step

00:06:46,879 --> 00:06:50,539
we've still got three input channels

00:06:48,470 --> 00:06:52,069
because we've done this spatial

00:06:50,539 --> 00:06:54,439
convolution across all the three

00:06:52,069 --> 00:06:56,539
channels independently and then the

00:06:54,439 --> 00:06:58,819
second step is we look across all the

00:06:56,539 --> 00:07:00,650
channels and we apply a one by one

00:06:58,819 --> 00:07:02,479
convolution so we're not looking at the

00:07:00,650 --> 00:07:05,060
spatial dimensions anymore and then we

00:07:02,479 --> 00:07:07,819
can change that to give us the number of

00:07:05,060 --> 00:07:10,219
output channels as we need so we get the

00:07:07,819 --> 00:07:12,889
same input and gives us the same output

00:07:10,219 --> 00:07:14,509
but in between we've saved on the number

00:07:12,889 --> 00:07:17,110
of parameters that we need from the both

00:07:14,509 --> 00:07:20,960
of those two kernels so we've gone from

00:07:17,110 --> 00:07:22,580
432 down to 75 and then if you look to

00:07:20,960 --> 00:07:23,930
the number of computations that need to

00:07:22,580 --> 00:07:25,460
be done we've cut that down

00:07:23,930 --> 00:07:27,919
significantly as well so we're looking

00:07:25,460 --> 00:07:31,879
in this case v les parameters and

00:07:27,919 --> 00:07:33,289
computations so for our mobile in that

00:07:31,879 --> 00:07:35,900
example now that we've said that that's

00:07:33,289 --> 00:07:38,240
a good model for edge just want to run

00:07:35,900 --> 00:07:42,379
through an example of fine tuning that

00:07:38,240 --> 00:07:45,319
on the Caltech 101 dataset so imagenet

00:07:42,379 --> 00:07:47,629
is well it was pre trained on from the

00:07:45,319 --> 00:07:49,159
onyx model zoo and then we can download

00:07:47,629 --> 00:07:51,710
the Caltech data set which has a

00:07:49,159 --> 00:07:54,440
different amount of classes and we can

00:07:51,710 --> 00:07:56,810
see how that performs so I'm just gonna

00:07:54,440 --> 00:08:01,159
hop across and actually try and dive

00:07:56,810 --> 00:08:03,529
into some code very quickly shall if you

00:08:01,159 --> 00:08:06,940
can't see the code that I'm about to

00:08:03,529 --> 00:08:12,860
show but I'm gonna zoom in a little bit

00:08:06,940 --> 00:08:19,190
okay so can everyone read that yep okay

00:08:12,860 --> 00:08:22,819
so this is an example of running MMX net

00:08:19,190 --> 00:08:24,139
on Archie sorry I'm just gonna jump back

00:08:22,819 --> 00:08:27,680
because I was going to give a quick

00:08:24,139 --> 00:08:29,360
introduction on MX net and sage maker

00:08:27,680 --> 00:08:33,229
which will then hopefully make this

00:08:29,360 --> 00:08:35,729
notebook make a little bit more sense so

00:08:33,229 --> 00:08:39,310
let's just talk to that now

00:08:35,729 --> 00:08:40,539
so yeah just wanted to introduce MX net

00:08:39,310 --> 00:08:45,370
a little bit more so this is the

00:08:40,539 --> 00:08:47,410
framework of choice by Amazon and one of

00:08:45,370 --> 00:08:50,620
the key things I'd like to talk about is

00:08:47,410 --> 00:08:54,580
the gluon API this is a high-level API

00:08:50,620 --> 00:08:56,710
for MX net and it it tackles the problem

00:08:54,580 --> 00:08:58,300
from an imperative approach so you can

00:08:56,710 --> 00:09:00,460
define the network as you run through

00:08:58,300 --> 00:09:02,020
code you don't have to create the symbol

00:09:00,460 --> 00:09:04,390
upfront beforehand

00:09:02,020 --> 00:09:06,510
so that gives you a lot of flexibility

00:09:04,390 --> 00:09:09,100
because you can create dynamic graphs

00:09:06,510 --> 00:09:10,930
the other benefit is that it makes the

00:09:09,100 --> 00:09:13,089
code a lot more debuggable so you can

00:09:10,930 --> 00:09:17,290
actually set breakpoints in your model

00:09:13,089 --> 00:09:19,480
and as you're adding operators to your

00:09:17,290 --> 00:09:21,100
graph you can actually inspect the

00:09:19,480 --> 00:09:24,730
values of the arrays the values of the

00:09:21,100 --> 00:09:26,140
kernels and see exactly why your model

00:09:24,730 --> 00:09:28,960
might not be working this is great if

00:09:26,140 --> 00:09:30,970
you're developing your own model and

00:09:28,960 --> 00:09:33,220
then with glue on you get the benefit of

00:09:30,970 --> 00:09:34,720
the symbolic graph optimizations as well

00:09:33,220 --> 00:09:37,180
because you can go through this process

00:09:34,720 --> 00:09:39,790
called hybridization that takes you from

00:09:37,180 --> 00:09:44,110
this imperative approach to the symbolic

00:09:39,790 --> 00:09:46,209
approach so little overview as to what

00:09:44,110 --> 00:09:48,010
gluon code will look like these are just

00:09:46,209 --> 00:09:50,230
I'm cooling out the main sections here

00:09:48,010 --> 00:09:52,810
so the first thing we need to do is

00:09:50,230 --> 00:09:55,540
obviously create our data so this is an

00:09:52,810 --> 00:09:58,630
example with CFR 10 but in the notebook

00:09:55,540 --> 00:10:00,310
will see the kel-tec they said we

00:09:58,630 --> 00:10:02,230
defined something called a data set

00:10:00,310 --> 00:10:05,500
which is responsible for delivering

00:10:02,230 --> 00:10:07,420
single samples or single images and then

00:10:05,500 --> 00:10:09,430
with your network training you often

00:10:07,420 --> 00:10:11,260
want to work with batches so we have a

00:10:09,430 --> 00:10:14,470
data loader that's responsible for

00:10:11,260 --> 00:10:17,410
batching all those samples and doing the

00:10:14,470 --> 00:10:19,330
shuffling and this is just across

00:10:17,410 --> 00:10:23,170
multiple processes with lots of

00:10:19,330 --> 00:10:25,450
different workers then we have the model

00:10:23,170 --> 00:10:27,640
definition so here we're using a model

00:10:25,450 --> 00:10:30,670
from model Zoo of glue on rather than

00:10:27,640 --> 00:10:32,290
onyx and then we want to define a loss

00:10:30,670 --> 00:10:34,810
because we want to optimize something

00:10:32,290 --> 00:10:36,580
and the way this optimization is done is

00:10:34,810 --> 00:10:38,170
handled by a trainer that's going to

00:10:36,580 --> 00:10:40,670
manage the parameters and work out how

00:10:38,170 --> 00:10:43,460
to do the optimizations

00:10:40,670 --> 00:10:46,730
and the last thing you'll see with gluon

00:10:43,460 --> 00:10:48,860
code is the training loop so we loop

00:10:46,730 --> 00:10:51,110
through all our data and for every batch

00:10:48,860 --> 00:10:53,210
that we get we do a forward pass

00:10:51,110 --> 00:10:55,700
so there we cool our network with some

00:10:53,210 --> 00:10:57,140
input data and then we call a backward

00:10:55,700 --> 00:10:58,970
pass which is going to compute all the

00:10:57,140 --> 00:11:00,980
gradients and then we do something

00:10:58,970 --> 00:11:03,050
called a trainer step which is going to

00:11:00,980 --> 00:11:04,670
use those gradients and work out how

00:11:03,050 --> 00:11:06,680
best to change all the parameters of a

00:11:04,670 --> 00:11:11,240
network so they're the key blocks that

00:11:06,680 --> 00:11:14,720
you're see with gluon code now AWS age

00:11:11,240 --> 00:11:16,640
maker is a really handy service of AWS

00:11:14,720 --> 00:11:18,890
if you want to get started training in

00:11:16,640 --> 00:11:21,950
your networks quickly so the first

00:11:18,890 --> 00:11:23,510
component to Sage maker is the notebooks

00:11:21,950 --> 00:11:25,640
and that's generally what we classed as

00:11:23,510 --> 00:11:28,370
the build stage so you can spin up a

00:11:25,640 --> 00:11:30,260
notebook instance that has Jupiter

00:11:28,370 --> 00:11:33,350
installed all the different frameworks

00:11:30,260 --> 00:11:35,320
and we'll have CUDA installed so you can

00:11:33,350 --> 00:11:37,760
use GPU training right out of the box

00:11:35,320 --> 00:11:39,260
then when you you built your model and

00:11:37,760 --> 00:11:41,660
you want to train it on a larger data

00:11:39,260 --> 00:11:46,100
set there's really great functionality

00:11:41,660 --> 00:11:47,690
for training distributed so they use the

00:11:46,100 --> 00:11:50,150
concept of containers we've got docker

00:11:47,690 --> 00:11:51,860
containers running on all these

00:11:50,150 --> 00:11:54,170
instances that are spun up for you so

00:11:51,860 --> 00:11:56,060
you don't have to handle the the server

00:11:54,170 --> 00:11:58,520
connections on how the weights are

00:11:56,060 --> 00:12:01,280
distributed and sent between the the

00:11:58,520 --> 00:12:03,560
instances there's a automatic model

00:12:01,280 --> 00:12:05,720
tuner that users are Bayesian

00:12:03,560 --> 00:12:07,580
optimization methods so you can select

00:12:05,720 --> 00:12:08,960
hyper parameters for these training jobs

00:12:07,580 --> 00:12:11,660
and then when you're happy with your

00:12:08,960 --> 00:12:14,690
train model there's a deploy method that

00:12:11,660 --> 00:12:16,160
will create an end point for you so that

00:12:14,690 --> 00:12:17,930
makes the process from build to

00:12:16,160 --> 00:12:21,170
deployment and much much easier when

00:12:17,930 --> 00:12:22,520
you're working on the cloud so now if we

00:12:21,170 --> 00:12:28,970
look at the notebook should make a

00:12:22,520 --> 00:12:31,430
little bit more sense okay so we've got

00:12:28,970 --> 00:12:33,700
the imports and the one thing I'd like

00:12:31,430 --> 00:12:37,520
to call out on the imports is onyx

00:12:33,700 --> 00:12:40,820
itself is a sub module of MX net so it's

00:12:37,520 --> 00:12:42,860
in the contrib package this part we're

00:12:40,820 --> 00:12:45,080
just downloading the model from the

00:12:42,860 --> 00:12:47,000
model zoo that is just a URL for this

00:12:45,080 --> 00:12:49,670
particular model and we do the extract

00:12:47,000 --> 00:12:52,790
of that file so now we have something

00:12:49,670 --> 00:12:54,000
that is f dot Onix format so this is our

00:12:52,790 --> 00:12:56,790
Onix file

00:12:54,000 --> 00:13:00,089
we download our data set just in this

00:12:56,790 --> 00:13:03,389
case Caltech 101 it's got a hundred and

00:13:00,089 --> 00:13:06,689
one classes I presume as so here's a

00:13:03,389 --> 00:13:09,149
variety of those classes the first clue

00:13:06,689 --> 00:13:12,180
on code we get to is creating the data

00:13:09,149 --> 00:13:13,949
sets and as a data set called image fold

00:13:12,180 --> 00:13:15,600
the data set where you just point it to

00:13:13,949 --> 00:13:17,850
a folder and that's going to work out

00:13:15,600 --> 00:13:20,430
what classes up and then this is easy to

00:13:17,850 --> 00:13:24,120
run so this would be an example of one

00:13:20,430 --> 00:13:25,889
of the images which is a motorbike now

00:13:24,120 --> 00:13:30,029
when it comes to fine-tuning the Onix

00:13:25,889 --> 00:13:31,949
model we can cool import model with the

00:13:30,029 --> 00:13:34,470
path to the Onix model and that's going

00:13:31,949 --> 00:13:36,740
to give us a MX net object so we've got

00:13:34,470 --> 00:13:40,170
an MX net symbol and we've got the

00:13:36,740 --> 00:13:42,540
parameters of that model as well when we

00:13:40,170 --> 00:13:46,620
think about fine-tuning it's quite often

00:13:42,540 --> 00:13:48,959
that you want to take the first layers

00:13:46,620 --> 00:13:51,240
but you don't want the last layer so

00:13:48,959 --> 00:13:52,980
here we've got code that will slice off

00:13:51,240 --> 00:13:54,629
the last layer which is usually a fully

00:13:52,980 --> 00:13:56,519
connected layer and that's much more

00:13:54,629 --> 00:13:58,860
dependent on the data set you're working

00:13:56,519 --> 00:14:01,050
with so because this is pre trained on

00:13:58,860 --> 00:14:03,449
imagenet that part has got a thousand

00:14:01,050 --> 00:14:04,949
classes but for Caltech 101 we want to

00:14:03,449 --> 00:14:07,319
change that because we've got a smaller

00:14:04,949 --> 00:14:11,399
number of classes so we slice off the

00:14:07,319 --> 00:14:14,519
last layer and then we create so that's

00:14:11,399 --> 00:14:16,500
a symbol block for the first part and we

00:14:14,519 --> 00:14:17,459
create our own dense layer so with glue

00:14:16,500 --> 00:14:20,939
on we've got a number of different

00:14:17,459 --> 00:14:23,430
neural network models and blocks they're

00:14:20,939 --> 00:14:25,410
called so here is a dense block and we

00:14:23,430 --> 00:14:28,230
specify the number of classes in Caltech

00:14:25,410 --> 00:14:30,990
we then initialize the parameters of

00:14:28,230 --> 00:14:33,930
this new layer and we just combined it

00:14:30,990 --> 00:14:35,790
with the start of the pre-training model

00:14:33,930 --> 00:14:38,730
so we create something called a hybrid

00:14:35,790 --> 00:14:40,709
sequential blog where we take the

00:14:38,730 --> 00:14:43,439
pre-trained and that then gets passed

00:14:40,709 --> 00:14:45,149
into our new dense layer this is just

00:14:43,439 --> 00:14:48,959
the training loop so we saw an example

00:14:45,149 --> 00:14:51,660
of that and now if we look at what the

00:14:48,959 --> 00:14:54,600
predictions would look like this is if

00:14:51,660 --> 00:14:57,240
we just took our pre trained model on

00:14:54,600 --> 00:14:59,189
imagenet and gave it some images from

00:14:57,240 --> 00:15:01,170
caltech so it would get the predictions

00:14:59,189 --> 00:15:03,360
wrong mostly because these images are

00:15:01,170 --> 00:15:06,990
not classes in the

00:15:03,360 --> 00:15:10,050
net data sir the the model was pre

00:15:06,990 --> 00:15:13,200
trained on but now if we fine-tune on

00:15:10,050 --> 00:15:14,399
our new Caltech data set the predictions

00:15:13,200 --> 00:15:15,480
from the model are much better and

00:15:14,399 --> 00:15:18,240
they're accurate because these images

00:15:15,480 --> 00:15:21,000
are of classes that we fine-tuned on

00:15:18,240 --> 00:15:24,149
and lastly now that we've pre trained

00:15:21,000 --> 00:15:27,390
our model we can go ahead and save it so

00:15:24,149 --> 00:15:30,480
the method for saving an MX net model is

00:15:27,390 --> 00:15:33,540
we can export our MX net model with the

00:15:30,480 --> 00:15:36,630
export method and define a path and then

00:15:33,540 --> 00:15:38,250
use onyx MX net when we export the model

00:15:36,630 --> 00:15:40,860
this is going to do the conversion from

00:15:38,250 --> 00:15:42,779
the MX net model to onyx and then it's

00:15:40,860 --> 00:15:44,220
just gonna give us that onyx file so

00:15:42,779 --> 00:15:46,649
we've gone through we've imported the

00:15:44,220 --> 00:15:50,720
onyx model done the fine-tuning on our

00:15:46,649 --> 00:15:50,720
own data set and now we've exported it

00:16:01,580 --> 00:16:06,529
okay just a quick mention of how to

00:16:03,410 --> 00:16:08,089
visualize these files so one of the

00:16:06,529 --> 00:16:11,330
tools that I often use is something

00:16:08,089 --> 00:16:13,459
called net run you can load in that onyx

00:16:11,330 --> 00:16:15,890
file and it will create a graphical

00:16:13,459 --> 00:16:17,420
representation or visual representation

00:16:15,890 --> 00:16:20,570
of that graph and you can click on

00:16:17,420 --> 00:16:22,550
individual nodes and inspect some of the

00:16:20,570 --> 00:16:25,399
parameters such as how many filters or

00:16:22,550 --> 00:16:26,360
combinations got what the inputs were so

00:16:25,399 --> 00:16:29,180
it's just a really good way of

00:16:26,360 --> 00:16:31,430
inspecting so now moving on to

00:16:29,180 --> 00:16:33,680
deployment so we've heard the reasons

00:16:31,430 --> 00:16:35,990
for and against cloud deployment and

00:16:33,680 --> 00:16:37,700
alleged of deployment if you're looking

00:16:35,990 --> 00:16:39,950
at cloud deployment then I mentioned

00:16:37,700 --> 00:16:43,910
sage maker as this deploy method once

00:16:39,950 --> 00:16:45,649
you've trained to get an endpoint also

00:16:43,910 --> 00:16:50,120
another option is to use a double yes

00:16:45,649 --> 00:16:53,690
far gate this is a service that manages

00:16:50,120 --> 00:16:55,579
deployments across containers and in the

00:16:53,690 --> 00:16:57,620
containers you could be running some

00:16:55,579 --> 00:17:00,260
kind of model server so a tensorflow

00:16:57,620 --> 00:17:01,730
it's got serving an MX net there's a

00:17:00,260 --> 00:17:03,529
model server for that too so that's some

00:17:01,730 --> 00:17:05,089
option that you could use but then

00:17:03,529 --> 00:17:07,309
obviously we've heard about all the

00:17:05,089 --> 00:17:10,189
benefits of edge deployment that's why

00:17:07,309 --> 00:17:12,400
we're here so obviously we're gonna get

00:17:10,189 --> 00:17:14,689
lower latency we save on bandwidth

00:17:12,400 --> 00:17:16,730
previously could be a concern so that's

00:17:14,689 --> 00:17:20,300
why we want to deploy an edge and the

00:17:16,730 --> 00:17:22,309
ability to operate offline is key so for

00:17:20,300 --> 00:17:23,959
that AWS provides a service called

00:17:22,309 --> 00:17:25,880
Greengrass and we'll take a quick look

00:17:23,959 --> 00:17:28,069
at that and then obviously you could be

00:17:25,880 --> 00:17:31,480
doing your own custom deployments here

00:17:28,069 --> 00:17:36,020
and building or infrastructure yourself

00:17:31,480 --> 00:17:38,720
so an overview of what Greengrass is the

00:17:36,020 --> 00:17:41,090
core component of Greengrass the OSI is

00:17:38,720 --> 00:17:43,400
something called a Greengrass group so

00:17:41,090 --> 00:17:45,380
this is a collection of devices one of

00:17:43,400 --> 00:17:48,380
which is designated as the Greengrass

00:17:45,380 --> 00:17:49,970
core and the Greengrass core manages a

00:17:48,380 --> 00:17:51,590
few different things the way it connects

00:17:49,970 --> 00:17:53,540
the cloud so it's managing the

00:17:51,590 --> 00:17:58,520
authentication and authorization of all

00:17:53,540 --> 00:18:00,860
the devices it's managing the shadowing

00:17:58,520 --> 00:18:02,390
so if this group falls off line it's

00:18:00,860 --> 00:18:04,700
gonna do the buffering of all the

00:18:02,390 --> 00:18:06,410
messages and the updates and it will

00:18:04,700 --> 00:18:09,130
manage over-the-air updates to all these

00:18:06,410 --> 00:18:09,130
devices too

00:18:11,270 --> 00:18:17,640
so for a deployment of an onyx model on

00:18:15,120 --> 00:18:19,740
green grass what components who we have

00:18:17,640 --> 00:18:22,200
to do so the first thing is the device

00:18:19,740 --> 00:18:25,010
itself and if you look at green grass

00:18:22,200 --> 00:18:28,020
they've got a number of pre-compiled

00:18:25,010 --> 00:18:29,490
frameworks for certain number of devices

00:18:28,020 --> 00:18:32,400
so if you're using Raspberry Pi for

00:18:29,490 --> 00:18:34,830
example you can download a pre-built

00:18:32,400 --> 00:18:36,840
version of Apache MX net and run that on

00:18:34,830 --> 00:18:39,060
your Raspberry Pi then you'll connect

00:18:36,840 --> 00:18:41,460
that to the green grass group and the

00:18:39,060 --> 00:18:43,770
next step is to define the resources so

00:18:41,460 --> 00:18:46,100
device resources are things like cameras

00:18:43,770 --> 00:18:48,420
or USBs that are on the devices and

00:18:46,100 --> 00:18:51,450
machine learning resources so this is

00:18:48,420 --> 00:18:54,030
where you'll take the onyx model package

00:18:51,450 --> 00:18:56,070
it up and compress it and then send it

00:18:54,030 --> 00:18:58,920
across to all the devices through this

00:18:56,070 --> 00:19:02,250
model resource next thing you probably

00:18:58,920 --> 00:19:04,620
you may have heard of AWS lambda which

00:19:02,250 --> 00:19:06,450
is the ability to run functions on the

00:19:04,620 --> 00:19:09,720
cloud and not manage and serve yourself

00:19:06,450 --> 00:19:12,240
well we take be lambda function and put

00:19:09,720 --> 00:19:14,280
that on the edge through Greengrass so

00:19:12,240 --> 00:19:16,350
you can define a lambda function in the

00:19:14,280 --> 00:19:17,940
same way and that will then interact

00:19:16,350 --> 00:19:20,100
with the resources that you've defined

00:19:17,940 --> 00:19:23,250
so it can access the device resources

00:19:20,100 --> 00:19:26,370
and has access to your onyx model as

00:19:23,250 --> 00:19:28,770
well and then if you look at the the

00:19:26,370 --> 00:19:30,360
package from the pre-built MX net you'll

00:19:28,770 --> 00:19:33,090
see example lambda functions of how to

00:19:30,360 --> 00:19:34,800
use that there and then optionally you

00:19:33,090 --> 00:19:36,690
can set up subscriptions to your

00:19:34,800 --> 00:19:38,880
Greengrass group so if you want to send

00:19:36,690 --> 00:19:40,650
the predictions back to the cloud that

00:19:38,880 --> 00:19:42,480
can be met handled automatically with

00:19:40,650 --> 00:19:43,860
subscriptions and you don't always have

00:19:42,480 --> 00:19:45,360
to have that connection to the cloud

00:19:43,860 --> 00:19:48,840
it's just going to batch things up and

00:19:45,360 --> 00:19:50,310
send them when it can ok the last thing

00:19:48,840 --> 00:19:52,290
I'd like to talk about and I realized

00:19:50,310 --> 00:19:54,210
there is a talk afterwards in a lot more

00:19:52,290 --> 00:19:56,880
detail about this but I want to give you

00:19:54,210 --> 00:20:00,690
an overview after how to optimize onyx

00:19:56,880 --> 00:20:04,080
models so the first thing is look

00:20:00,690 --> 00:20:07,380
towards 1/2 precision if possible so I

00:20:04,080 --> 00:20:09,960
think the Marly GPU has operations on

00:20:07,380 --> 00:20:11,660
float 16 so it depends on what framework

00:20:09,960 --> 00:20:16,080
you're using whether you can use these

00:20:11,660 --> 00:20:17,670
then quantization if you're looking at 8

00:20:16,080 --> 00:20:19,230
then you have to look towards

00:20:17,670 --> 00:20:20,880
calibration to make sure you're doing

00:20:19,230 --> 00:20:22,320
this properly and there's some

00:20:20,880 --> 00:20:24,090
experimental example

00:20:22,320 --> 00:20:27,360
and features in MX net to be doing this

00:20:24,090 --> 00:20:30,330
and then lastly you should be looking

00:20:27,360 --> 00:20:32,610
towards compiling your model and we've

00:20:30,330 --> 00:20:35,039
seen examples of that previously but the

00:20:32,610 --> 00:20:36,750
example I'd like to run through is TVN

00:20:35,039 --> 00:20:39,419
which we'll hear a lot more about in the

00:20:36,750 --> 00:20:41,789
next talk and so the main overview of

00:20:39,419 --> 00:20:43,710
what the TVM stack provides is you have

00:20:41,789 --> 00:20:46,649
the compiler element and then you have

00:20:43,710 --> 00:20:50,250
the runtime and then the compiler takes

00:20:46,649 --> 00:20:53,130
in various front-end models so you can

00:20:50,250 --> 00:20:55,740
use onyx directly that is one of the

00:20:53,130 --> 00:20:57,539
front ends for the TVM stack then it

00:20:55,740 --> 00:21:00,620
gets passed through something called NN

00:20:57,539 --> 00:21:03,929
VM which is going to handle the graph

00:21:00,620 --> 00:21:06,330
optimizations then T VM which will

00:21:03,929 --> 00:21:09,720
manage the tensor operations and you

00:21:06,330 --> 00:21:11,909
specifically target certain devices so

00:21:09,720 --> 00:21:14,850
here when you're doing the TVM step you

00:21:11,909 --> 00:21:16,620
can specify whether you want to use LLVM

00:21:14,850 --> 00:21:18,779
which would be used for arm devices or

00:21:16,620 --> 00:21:20,850
cooter if you're working with GPUs and

00:21:18,779 --> 00:21:22,500
video GPUs and the output of this

00:21:20,850 --> 00:21:24,870
compile stage will be a library that

00:21:22,500 --> 00:21:28,110
then you can run on the TVM runtime

00:21:24,870 --> 00:21:30,710
which is much much lighter I will give a

00:21:28,110 --> 00:21:34,049
good speed-up

00:21:30,710 --> 00:21:36,480
so just a quick overview as the the

00:21:34,049 --> 00:21:39,720
types of optimizations that the TVM

00:21:36,480 --> 00:21:42,450
stack will offer the N nvm part with is

00:21:39,720 --> 00:21:45,779
the graph optimizations so we can look

00:21:42,450 --> 00:21:47,460
at pruning and fusing and TBM offers

00:21:45,779 --> 00:21:49,169
toiling and vectorization and there's

00:21:47,460 --> 00:21:51,600
many more things that these do this is

00:21:49,169 --> 00:21:54,929
just two examples of each so pruning

00:21:51,600 --> 00:21:57,570
this is simplifying the graph for

00:21:54,929 --> 00:22:00,179
inference so while you're training it's

00:21:57,570 --> 00:22:01,980
it's useful to regularize your models to

00:22:00,179 --> 00:22:04,139
prevent it overfitting and using things

00:22:01,980 --> 00:22:05,970
like dropout but when you're looking at

00:22:04,139 --> 00:22:07,230
inference you don't need to do the

00:22:05,970 --> 00:22:08,850
dropout that's not something you want to

00:22:07,230 --> 00:22:10,909
apply and you don't have to check

00:22:08,850 --> 00:22:13,169
whether you're in training mode or in

00:22:10,909 --> 00:22:17,580
execution mode so it's best to just

00:22:13,169 --> 00:22:19,080
prune these nodes out fusing so if

00:22:17,580 --> 00:22:21,450
you've got a graph that has a number of

00:22:19,080 --> 00:22:24,059
operations or one after another often

00:22:21,450 --> 00:22:26,519
times you don't need to store the

00:22:24,059 --> 00:22:27,960
intermediate values from the convolution

00:22:26,519 --> 00:22:30,029
to the relu you can just apply the

00:22:27,960 --> 00:22:32,940
operation in a single step so these

00:22:30,029 --> 00:22:35,520
operations can get fused together which

00:22:32,940 --> 00:22:37,500
gives a much much better speed up

00:22:35,520 --> 00:22:39,930
and if it in the case of depth-wise

00:22:37,500 --> 00:22:42,150
convolution which we saw before it

00:22:39,930 --> 00:22:44,460
doubles the speed up it doubles the

00:22:42,150 --> 00:22:49,260
speed of the depth-wise convolution if

00:22:44,460 --> 00:22:52,650
use fusing with TVM and then there's

00:22:49,260 --> 00:22:54,270
also techniques called folding away if

00:22:52,650 --> 00:22:56,280
you've got convolution and batch norm

00:22:54,270 --> 00:22:59,430
together those operations can be applied

00:22:56,280 --> 00:23:01,950
together in the same step to now if we

00:22:59,430 --> 00:23:03,930
look across to the tensor optimizations

00:23:01,950 --> 00:23:05,610
which are much more specific to the

00:23:03,930 --> 00:23:09,060
device that you're going to be compiling

00:23:05,610 --> 00:23:11,970
for one example is tiling where you

00:23:09,060 --> 00:23:14,760
actually reshape your input in such a

00:23:11,970 --> 00:23:17,190
way to make better use of the cache so

00:23:14,760 --> 00:23:19,350
more of the the data that you're you're

00:23:17,190 --> 00:23:21,290
using either from the input data and the

00:23:19,350 --> 00:23:24,810
kernel could be used in the cache and

00:23:21,290 --> 00:23:26,520
vectorization where you don't want to

00:23:24,810 --> 00:23:27,990
apply the same operation to every

00:23:26,520 --> 00:23:29,880
element in your array you can just take

00:23:27,990 --> 00:23:32,070
the two arrays and do a single operation

00:23:29,880 --> 00:23:35,430
together these are just a few few quick

00:23:32,070 --> 00:23:38,190
examples and then with TVM is a great

00:23:35,430 --> 00:23:41,580
feature called Auto TVM which is going

00:23:38,190 --> 00:23:43,710
to actually work out the best levels or

00:23:41,580 --> 00:23:45,660
amounts of tiling and vectorization and

00:23:43,710 --> 00:23:48,240
parallelization to do for a given

00:23:45,660 --> 00:23:50,070
hardware so it's essentially hyper

00:23:48,240 --> 00:23:52,170
parameter search if you come at it from

00:23:50,070 --> 00:23:55,200
a modeling perspective on the the

00:23:52,170 --> 00:23:57,480
architecture so yeah just to wrap up in

00:23:55,200 --> 00:24:00,180
summary we looked at where you can find

00:23:57,480 --> 00:24:02,250
onyx models from the onyx model zoo and

00:24:00,180 --> 00:24:04,770
then an example of fine tuning with MX

00:24:02,250 --> 00:24:06,990
net we saw net Ron is good for

00:24:04,770 --> 00:24:09,000
visualizing the model Greengrass is an

00:24:06,990 --> 00:24:11,910
option if you're wanting to manage a

00:24:09,000 --> 00:24:14,400
large number of edge devices in a group

00:24:11,910 --> 00:24:15,360
and then TVM staff stack is something

00:24:14,400 --> 00:24:19,150
you can look at when you're optimizing

00:24:15,360 --> 00:24:24,100
your models all right thank you

00:24:19,150 --> 00:24:25,669
[Applause]

00:24:24,100 --> 00:24:29,950
thank you Tom

00:24:25,669 --> 00:24:29,950
is it fair if I start with a question

00:24:31,720 --> 00:24:39,740
every framework has its own models ooh

00:24:36,220 --> 00:24:43,070
yeah ten so floor has its own mx9 has

00:24:39,740 --> 00:24:45,950
bloomed caffee has its models ooh and

00:24:43,070 --> 00:24:50,720
you mentioned that onyx has its own

00:24:45,950 --> 00:24:53,960
models ooh as well easy a different one

00:24:50,720 --> 00:24:55,850
is it a conversion of the other tools

00:24:53,960 --> 00:24:58,639
how does it work

00:24:55,850 --> 00:25:01,970
so the aim with the onyx models there at

00:24:58,639 --> 00:25:04,340
least was to provide the code that was

00:25:01,970 --> 00:25:05,870
used to train the models in the models

00:25:04,340 --> 00:25:07,580
ooh so a lot of the other frameworks you

00:25:05,870 --> 00:25:09,500
have the pre trained model we don't know

00:25:07,580 --> 00:25:13,460
where it's come from so that's been one

00:25:09,500 --> 00:25:17,389
key thing with the onyx models ooh there

00:25:13,460 --> 00:25:19,460
hasn't been too much working converting

00:25:17,389 --> 00:25:21,679
the models that have been pre trained

00:25:19,460 --> 00:25:22,909
into onyx models ooh because of that

00:25:21,679 --> 00:25:25,759
reason so you wanted to have the

00:25:22,909 --> 00:25:27,860
training script but more work has been

00:25:25,759 --> 00:25:30,879
put in from a few different partners to

00:25:27,860 --> 00:25:30,879
build this model zoo out

00:25:40,960 --> 00:25:47,120
there is a deep lines device edge device

00:25:44,570 --> 00:25:51,230
that you gun in a corporation in the

00:25:47,120 --> 00:25:55,100
Intel could you please don't Lee explain

00:25:51,230 --> 00:25:57,320
how it works with like the details you

00:25:55,100 --> 00:25:58,970
shot here and are you able with that

00:25:57,320 --> 00:26:00,730
device to do the inference on the very

00:25:58,970 --> 00:26:03,710
device or you're doing that in the cloud

00:26:00,730 --> 00:26:06,080
so for deep lens yeah the inference is

00:26:03,710 --> 00:26:08,179
done on the device itself

00:26:06,080 --> 00:26:10,160
so you'll see through the idea of its

00:26:08,179 --> 00:26:11,450
console you'll be able to use pre

00:26:10,160 --> 00:26:14,179
trained models and do the fine-tuning

00:26:11,450 --> 00:26:17,630
step again but then it goes through a

00:26:14,179 --> 00:26:19,640
separate optimization step specifically

00:26:17,630 --> 00:26:21,830
for I think Intel have managed the

00:26:19,640 --> 00:26:23,150
optimization for that device because the

00:26:21,830 --> 00:26:27,140
thing is running an Intel Atom

00:26:23,150 --> 00:26:28,730
and so that speeds up the models huge

00:26:27,140 --> 00:26:30,410
amount and then you can run on the

00:26:28,730 --> 00:26:32,590
actual device itself not just in the

00:26:30,410 --> 00:26:32,590
cloud

00:26:36,150 --> 00:26:41,690
okay thank you Tom that was great thank

00:26:39,240 --> 00:26:41,690
you very much

00:26:41,880 --> 00:26:49,729
[Applause]

00:26:44,720 --> 00:26:49,729

YouTube URL: https://www.youtube.com/watch?v=BDWlIew5pfo


