Title: YVR18-338 Tensorflow for Arm devices
Publication date: 2018-09-28
Playlist: Linaro Connect Vancouver 2018
Description: 
	
Captions: 
	00:00:02,110 --> 00:00:07,120
[Music]

00:00:09,129 --> 00:00:16,539
so yeah we have a joke at Google that we

00:00:11,589 --> 00:00:18,990
may be solving AI but AV is it's gonna

00:00:16,539 --> 00:00:22,720
remain the the frontier for a long time

00:00:18,990 --> 00:00:23,080
so yeah thanks for joining me I'm Pete

00:00:22,720 --> 00:00:26,230
worden

00:00:23,080 --> 00:00:31,779
I'm the tech lead of the mobile and

00:00:26,230 --> 00:00:34,420
embedded tensorflow team and I'm kind of

00:00:31,779 --> 00:00:38,649
a last-minute addition but I am very

00:00:34,420 --> 00:00:42,660
excited to be here and what I'm gonna be

00:00:38,649 --> 00:00:46,030
talking about is where tensor flows been

00:00:42,660 --> 00:00:51,850
where it's going and where we're at now

00:00:46,030 --> 00:00:54,059
on arm devices so the first thing to

00:00:51,850 --> 00:00:57,600
know is that we really care about

00:00:54,059 --> 00:01:00,340
working well on all sorts of arm devices

00:00:57,600 --> 00:01:05,049
partly that's because we have a bunch of

00:01:00,340 --> 00:01:09,909
Google applications using sort of arm

00:01:05,049 --> 00:01:13,020
devices both on Android and for all

00:01:09,909 --> 00:01:16,979
sorts of devices things like Google home

00:01:13,020 --> 00:01:22,000
and also as tensorflow

00:01:16,979 --> 00:01:24,369
we really care about enabling people

00:01:22,000 --> 00:01:27,729
kind of across the world outside of

00:01:24,369 --> 00:01:29,979
Google to use machine learning to do all

00:01:27,729 --> 00:01:33,610
sorts of interesting things so we have a

00:01:29,979 --> 00:01:36,579
whole bunch of really fun external

00:01:33,610 --> 00:01:41,409
projects that I can talk about they're

00:01:36,579 --> 00:01:43,299
using tensorflow on arm and one of my

00:01:41,409 --> 00:01:47,200
favorites actually was from a couple of

00:01:43,299 --> 00:01:51,280
years ago a guy in San Francisco trained

00:01:47,200 --> 00:01:53,110
an image recognizer to recognize the

00:01:51,280 --> 00:01:55,350
traffic wardens the meter maids that

00:01:53,110 --> 00:01:58,990
drive around San Francisco streets and

00:01:55,350 --> 00:02:00,640
he put that model on a Raspberry Pi

00:01:58,990 --> 00:02:04,680
which he put in the back window of his

00:02:00,640 --> 00:02:08,170
car which then texted him when somebody

00:02:04,680 --> 00:02:09,580
came near his car that was recognized as

00:02:08,170 --> 00:02:12,490
a traffic warden so he can move it

00:02:09,580 --> 00:02:13,840
before he got a ticket I'm not sure

00:02:12,490 --> 00:02:17,290
that's an official thing

00:02:13,840 --> 00:02:18,700
we should be promoting but it's kind of

00:02:17,290 --> 00:02:20,769
the sort of thing that we would never

00:02:18,700 --> 00:02:25,180
thought of ourselves but what we think

00:02:20,769 --> 00:02:26,620
is a really interesting thing that if we

00:02:25,180 --> 00:02:28,510
can get stuff out into the world it can

00:02:26,620 --> 00:02:30,549
actually enable a whole bunch of really

00:02:28,510 --> 00:02:32,250
interesting use cases and these are the

00:02:30,549 --> 00:02:37,500
kind of use cases that only really

00:02:32,250 --> 00:02:44,880
exists in the edge in the arm ecosystem

00:02:37,500 --> 00:02:46,349
so where are we at the moment we have

00:02:44,880 --> 00:02:49,329
tensorflow

00:02:46,349 --> 00:02:51,790
and tens flow light I can talk a little

00:02:49,329 --> 00:02:57,540
bit about this sort of the history there

00:02:51,790 --> 00:03:02,190
but we have them running on iOS on

00:02:57,540 --> 00:03:06,730
Android devices and on the vast we PI

00:03:02,190 --> 00:03:13,030
and the vast we pi is especially

00:03:06,730 --> 00:03:14,560
exciting for us because we've managed to

00:03:13,030 --> 00:03:19,530
actually get it to the point where you

00:03:14,560 --> 00:03:23,139
can just automatically build and install

00:03:19,530 --> 00:03:27,160
these binaries on the Raspberry Pi to

00:03:23,139 --> 00:03:29,109
get the full tensor flow running through

00:03:27,160 --> 00:03:33,310
Python and you can just do a standard

00:03:29,109 --> 00:03:36,269
pip install and while we don't

00:03:33,310 --> 00:03:38,560
officially support it because we don't

00:03:36,269 --> 00:03:40,989
we don't have the continuous integration

00:03:38,560 --> 00:03:43,510
testing setup we have had success as

00:03:40,989 --> 00:03:46,419
well doing the same pipe installs on a

00:03:43,510 --> 00:03:50,919
bunch of other ARM based Linux systems

00:03:46,419 --> 00:03:53,709
so we've seen a lot of uptake of people

00:03:50,919 --> 00:03:56,769
actually running the full tensorflow on

00:03:53,709 --> 00:03:58,630
these arm devices like if you've got a

00:03:56,769 --> 00:04:02,530
big meaty arm device they were able to

00:03:58,630 --> 00:04:04,810
run Linux on we've hopefully giving you

00:04:02,530 --> 00:04:07,269
pretty much the same experience maybe a

00:04:04,810 --> 00:04:09,609
little bit slower depending on the clock

00:04:07,269 --> 00:04:11,500
rate of the processor but a very similar

00:04:09,609 --> 00:04:16,359
experience to what you get on a high-end

00:04:11,500 --> 00:04:18,519
cloud server now that is when you're

00:04:16,359 --> 00:04:21,579
looking at the full gamut of what

00:04:18,519 --> 00:04:25,539
tensorflow can do in terms of a Python

00:04:21,579 --> 00:04:26,710
interface in terms of training but we

00:04:25,539 --> 00:04:29,919
also

00:04:26,710 --> 00:04:33,370
have a strong need for hey we just want

00:04:29,919 --> 00:04:39,009
to run infants we don't want our Android

00:04:33,370 --> 00:04:41,620
binary iOS binary to get enormous and we

00:04:39,009 --> 00:04:44,080
want it to run as fast as possible

00:04:41,620 --> 00:04:46,000
and we're willing to kind of jump

00:04:44,080 --> 00:04:48,340
through a few more hoops to kind of

00:04:46,000 --> 00:04:51,880
optimize our graph after we've done the

00:04:48,340 --> 00:04:54,060
training and get to the point where you

00:04:51,880 --> 00:04:57,580
have something that's really streamlined

00:04:54,060 --> 00:05:02,289
so for that we actually have ten to flow

00:04:57,580 --> 00:05:05,349
light and that is a part of tensor flow

00:05:02,289 --> 00:05:11,800
that is pretty much exclusively designed

00:05:05,349 --> 00:05:16,509
for arm devices it's got a whole bunch

00:05:11,800 --> 00:05:22,380
of optimizations especially for arm we

00:05:16,509 --> 00:05:26,199
have the gem lo P library that we

00:05:22,380 --> 00:05:29,740
created I think about two or three years

00:05:26,199 --> 00:05:35,289
ago now and that is an open source

00:05:29,740 --> 00:05:38,860
library which aims to collect the very

00:05:35,289 --> 00:05:42,849
fastest implementations we can of the

00:05:38,860 --> 00:05:47,789
8-bit matrix-multiply operations that we

00:05:42,849 --> 00:05:50,979
rely on to get a lot of the speed out of

00:05:47,789 --> 00:05:52,180
sort of arm devices and edge devices in

00:05:50,979 --> 00:05:56,710
general there was nothing out there

00:05:52,180 --> 00:05:59,199
really offering this kind of fixed point

00:05:56,710 --> 00:06:01,479
low precision matrix-multiply that we

00:05:59,199 --> 00:06:02,949
needed so we kicked off an open source

00:06:01,479 --> 00:06:05,349
project and thankfully we've actually

00:06:02,949 --> 00:06:07,719
had a lot of contributions from a lot of

00:06:05,349 --> 00:06:09,969
people including arm to help us speed

00:06:07,719 --> 00:06:12,759
this up so we're putting a lot of

00:06:09,969 --> 00:06:17,620
engineering effort into making sure that

00:06:12,759 --> 00:06:18,909
we run as well as we can on arm for the

00:06:17,620 --> 00:06:21,340
floating-point side

00:06:18,909 --> 00:06:24,729
we're also heavy users and heavy

00:06:21,340 --> 00:06:28,300
contributors to the eigen open source

00:06:24,729 --> 00:06:31,180
project which is a numerical computing

00:06:28,300 --> 00:06:34,270
library which includes the bits that

00:06:31,180 --> 00:06:37,599
we're most interested in again the

00:06:34,270 --> 00:06:39,250
matrix multiply that's what we spend the

00:06:37,599 --> 00:06:39,620
bulk of our compute doing whether we're

00:06:39,250 --> 00:06:41,780
doing

00:06:39,620 --> 00:06:43,870
volution or fully connected layers that

00:06:41,780 --> 00:06:47,180
really compute-intensive parts are the

00:06:43,870 --> 00:06:49,130
matrix multiply sections so we have

00:06:47,180 --> 00:06:53,780
spent a bunch of time ourselves trying

00:06:49,130 --> 00:06:56,479
to optimize that stuff and we're also

00:06:53,780 --> 00:07:00,410
working very closely with the Android

00:06:56,479 --> 00:07:02,000
team you know obviously we we are a sort

00:07:00,410 --> 00:07:04,669
of you know independent of each other

00:07:02,000 --> 00:07:08,360
but we try and collaborate and cooperate

00:07:04,669 --> 00:07:11,540
as much as we can and they're currently

00:07:08,360 --> 00:07:13,060
using tense flow light implementations

00:07:11,540 --> 00:07:16,010
for their CPU

00:07:13,060 --> 00:07:18,350
fallback path and you'll see a lot of

00:07:16,010 --> 00:07:20,960
similarities between the tensorflow

00:07:18,350 --> 00:07:26,740
light operations that are supported and

00:07:20,960 --> 00:07:31,639
what's supported in Android nn API so

00:07:26,740 --> 00:07:36,860
where did all this come from so back in

00:07:31,639 --> 00:07:39,350
2013 I really wanted to get some deep

00:07:36,860 --> 00:07:43,789
learning image recognition stuff running

00:07:39,350 --> 00:07:48,680
on my phone when I was at my startup

00:07:43,789 --> 00:07:51,139
jetpack so I ended up coding up some

00:07:48,680 --> 00:07:54,110
libraries to run in that in that day and

00:07:51,139 --> 00:07:57,530
age that was sort of Alex net running

00:07:54,110 --> 00:08:01,250
locally on a device and I managed to

00:07:57,530 --> 00:08:05,860
port it over to iOS Android and PI but

00:08:01,250 --> 00:08:10,479
it was a very very simple framework

00:08:05,860 --> 00:08:14,060
Google acquired my startup jetpack and

00:08:10,479 --> 00:08:16,190
with with other people on the tensorflow

00:08:14,060 --> 00:08:21,050
team before it was launched I started

00:08:16,190 --> 00:08:23,500
working on the mobile support because

00:08:21,050 --> 00:08:27,139
from the very start tensorflow

00:08:23,500 --> 00:08:28,880
really cared about mobile we knew that

00:08:27,139 --> 00:08:31,220
this was going to be a key use case we

00:08:28,880 --> 00:08:34,159
knew from internal customers that people

00:08:31,220 --> 00:08:37,580
really wanted to create products that

00:08:34,159 --> 00:08:41,209
were using deep learning on mobile

00:08:37,580 --> 00:08:44,600
devices so we knew we had to come out of

00:08:41,209 --> 00:08:47,029
the gate with mobile support and we came

00:08:44,600 --> 00:08:49,940
out with Android support with a very

00:08:47,029 --> 00:08:51,960
initial release and we very quickly

00:08:49,940 --> 00:08:54,870
added iOS

00:08:51,960 --> 00:08:58,530
and lastly pi support in subsequent

00:08:54,870 --> 00:09:00,600
releases and we've been very focused on

00:08:58,530 --> 00:09:03,270
arm improvements it's been we've had a

00:09:00,600 --> 00:09:06,030
lot of pressure both internally and

00:09:03,270 --> 00:09:09,060
externally to keep working on this

00:09:06,030 --> 00:09:13,290
people really care about it so we've had

00:09:09,060 --> 00:09:19,800
a team that's still growing are working

00:09:13,290 --> 00:09:23,370
on improving this so what I want to talk

00:09:19,800 --> 00:09:25,740
about especially with you know the

00:09:23,370 --> 00:09:28,860
people here who are very engineering

00:09:25,740 --> 00:09:31,940
heavy is okay where are we going what

00:09:28,860 --> 00:09:31,940
are we going to be doing in the future

00:09:32,150 --> 00:09:39,720
the number one issue that we find our

00:09:36,300 --> 00:09:44,460
customers hit is usability they have

00:09:39,720 --> 00:09:46,380
models that they have created in you

00:09:44,460 --> 00:09:49,340
know tensorflow training framework and

00:09:46,380 --> 00:09:54,600
they want to get them onto mobile and

00:09:49,340 --> 00:09:57,350
it's a very hard process especially if

00:09:54,600 --> 00:10:04,680
you want to get down to things like

00:09:57,350 --> 00:10:05,100
8-bit and a lot of the toughness of that

00:10:04,680 --> 00:10:08,160
problem

00:10:05,100 --> 00:10:10,500
is that you need to often make changes

00:10:08,160 --> 00:10:13,080
or have some awareness during training

00:10:10,500 --> 00:10:15,120
like Rob was saying you need to to get

00:10:13,080 --> 00:10:16,910
best results for quantization you

00:10:15,120 --> 00:10:20,790
actually need to do quantization aware

00:10:16,910 --> 00:10:23,730
training and things like that so we're

00:10:20,790 --> 00:10:28,560
really focused on trying to make sure

00:10:23,730 --> 00:10:30,870
that we make the process as easy as

00:10:28,560 --> 00:10:32,280
possible for people so that more and

00:10:30,870 --> 00:10:33,780
more people can actually succeed in

00:10:32,280 --> 00:10:38,430
putting these machine learning

00:10:33,780 --> 00:10:42,000
applications on devices we see a big

00:10:38,430 --> 00:10:47,130
part of that as actually creating

00:10:42,000 --> 00:10:49,110
benchmarks which function as both kind

00:10:47,130 --> 00:10:52,770
of tutorials and examples for people to

00:10:49,110 --> 00:10:56,130
follow but also are very clear

00:10:52,770 --> 00:10:58,170
expressions of what our typical users

00:10:56,130 --> 00:11:01,950
requirements are both in terms of

00:10:58,170 --> 00:11:02,700
latency and crucially in terms of

00:11:01,950 --> 00:11:04,630
accuracy

00:11:02,700 --> 00:11:07,210
so having benchmarks

00:11:04,630 --> 00:11:10,510
things like ml perf that not only give

00:11:07,210 --> 00:11:13,570
you sort of speed numbers but also give

00:11:10,510 --> 00:11:16,030
you an estimate of the accuracy loss

00:11:13,570 --> 00:11:17,710
that you might encounter if you take

00:11:16,030 --> 00:11:20,680
particular you know things like

00:11:17,710 --> 00:11:27,190
quantization we see that as being really

00:11:20,680 --> 00:11:30,010
crucial to opening up this code and this

00:11:27,190 --> 00:11:33,850
project to a lot of collaboration if we

00:11:30,010 --> 00:11:37,000
can just hand people benchmarks and let

00:11:33,850 --> 00:11:38,710
them work independently we're safe in

00:11:37,000 --> 00:11:40,030
the knowledge that hey if they're

00:11:38,710 --> 00:11:41,980
actually passing this suite of

00:11:40,030 --> 00:11:45,220
benchmarks then the results are going to

00:11:41,980 --> 00:11:48,490
be good enough for our users

00:11:45,220 --> 00:11:51,820
then I'm hoping that really opens up a

00:11:48,490 --> 00:11:53,520
lot of collaboration possibilities we

00:11:51,820 --> 00:11:58,000
want to be working much more closely

00:11:53,520 --> 00:12:01,630
with like this growing field of people

00:11:58,000 --> 00:12:03,850
who are creating sort of new

00:12:01,630 --> 00:12:09,790
technologies or new platforms for

00:12:03,850 --> 00:12:11,470
running these kind of ml models and like

00:12:09,790 --> 00:12:14,710
I was saying we're very very keen to

00:12:11,470 --> 00:12:17,160
collaborate overall we've had a great

00:12:14,710 --> 00:12:21,750
experience with the arm compute library

00:12:17,160 --> 00:12:24,760
where we've been using some benchmarks

00:12:21,750 --> 00:12:27,310
to kind of go back and forth between our

00:12:24,760 --> 00:12:31,930
engineers and the arm engineers working

00:12:27,310 --> 00:12:34,840
on this and using those benchmarks we've

00:12:31,930 --> 00:12:38,710
actually been able to sort of narrow in

00:12:34,840 --> 00:12:40,540
on something that means that the arm

00:12:38,710 --> 00:12:43,030
compute library is shown significantly

00:12:40,540 --> 00:12:46,150
better performance than our own hand

00:12:43,030 --> 00:12:48,520
coded code so we're hoping to take

00:12:46,150 --> 00:12:50,440
advantage of that and integrate with the

00:12:48,520 --> 00:12:52,720
our tensor flow light with the arm

00:12:50,440 --> 00:12:56,410
compute library so that we can use that

00:12:52,720 --> 00:12:58,600
and really we don't see our core

00:12:56,410 --> 00:13:00,940
competency as knowing all the

00:12:58,600 --> 00:13:03,310
intricacies of particular platforms and

00:13:00,940 --> 00:13:07,110
particular processors and writing these

00:13:03,310 --> 00:13:09,400
low-level routines that's just been

00:13:07,110 --> 00:13:11,980
something we do because we didn't have

00:13:09,400 --> 00:13:14,320
anybody else who was kind of stepping up

00:13:11,980 --> 00:13:17,589
and doing that as the field is growing

00:13:14,320 --> 00:13:18,520
we're super happy to work with anybody

00:13:17,589 --> 00:13:21,820
who's at

00:13:18,520 --> 00:13:25,090
the an expert on these platforms and is

00:13:21,820 --> 00:13:27,400
able to offer really better performance

00:13:25,090 --> 00:13:32,410
than we can we're not at all bothered

00:13:27,400 --> 00:13:34,390
about using other people's code we would

00:13:32,410 --> 00:13:36,970
love people to be sort of contributing

00:13:34,390 --> 00:13:40,660
more to tensorflow intensive low light

00:13:36,970 --> 00:13:43,060
and through lower-level interfaces like

00:13:40,660 --> 00:13:46,300
gem low P we've had a pretty good track

00:13:43,060 --> 00:13:48,420
record of taking a lot of contributions

00:13:46,300 --> 00:13:53,860
from other people and we want to see

00:13:48,420 --> 00:13:56,050
more of that happening we do see a lot

00:13:53,860 --> 00:14:00,270
of excitement and a lot of growth

00:13:56,050 --> 00:14:05,590
happening around accelerators in general

00:14:00,270 --> 00:14:08,790
both GPUs custom accelerators we're

00:14:05,590 --> 00:14:14,410
working you may have seen the edge TPU

00:14:08,790 --> 00:14:18,030
which is going to be shipping on an ARM

00:14:14,410 --> 00:14:22,270
based connected to an ARM based platform

00:14:18,030 --> 00:14:27,010
we see a massive amount of growth in

00:14:22,270 --> 00:14:29,530
that area and we want to try and work as

00:14:27,010 --> 00:14:31,060
close as we can with people who are

00:14:29,530 --> 00:14:34,030
creating accelerators to make sure that

00:14:31,060 --> 00:14:37,060
our users have a really good experience

00:14:34,030 --> 00:14:39,070
what we've seen with kind of the the

00:14:37,060 --> 00:14:41,680
first generation of accelerators was if

00:14:39,070 --> 00:14:43,960
you're using the exact model that made

00:14:41,680 --> 00:14:46,750
it through the translation pipeline of

00:14:43,960 --> 00:14:49,630
the vendors tools you would get great

00:14:46,750 --> 00:14:52,210
performance but it was very very hard to

00:14:49,630 --> 00:14:54,610
take your own model bring your own model

00:14:52,210 --> 00:14:56,410
in and actually get it all the way

00:14:54,610 --> 00:15:00,250
through the tooling so we want to do

00:14:56,410 --> 00:15:04,630
whatever we can to help people solve

00:15:00,250 --> 00:15:06,130
that tooling problem the other thing

00:15:04,630 --> 00:15:07,780
that we're actually spending a lot of

00:15:06,130 --> 00:15:13,320
time on these days and we're seeing a

00:15:07,780 --> 00:15:21,330
lot of demand for is running models on

00:15:13,320 --> 00:15:24,070
microcontrollers we are using internally

00:15:21,330 --> 00:15:26,650
microcontrollers for a bunch of things

00:15:24,070 --> 00:15:30,790
everything ranging from kind of the okay

00:15:26,650 --> 00:15:32,410
google keyword detection which runs on

00:15:30,790 --> 00:15:35,360
always-on

00:15:32,410 --> 00:15:38,840
that are usually like DSPs or cortex-m

00:15:35,360 --> 00:15:39,890
type processors to things like the

00:15:38,840 --> 00:15:42,650
pixels

00:15:39,890 --> 00:15:46,040
background music detection which will

00:15:42,650 --> 00:15:48,860
actually if you have a pixel phone it

00:15:46,040 --> 00:15:52,880
will listen out for music and when it

00:15:48,860 --> 00:15:56,030
detects a a song it will just display on

00:15:52,880 --> 00:15:59,360
the sort of on the lock screen the name

00:15:56,030 --> 00:16:01,310
and the artist of the song and we think

00:15:59,360 --> 00:16:03,700
that that is just the sort of the

00:16:01,310 --> 00:16:06,950
beginning for these kind of like ambient

00:16:03,700 --> 00:16:11,840
applications of machine learning that

00:16:06,950 --> 00:16:14,170
have to run locally on device so we're

00:16:11,840 --> 00:16:17,210
putting a lot of engineering effort into

00:16:14,170 --> 00:16:20,480
trying to help people get up and running

00:16:17,210 --> 00:16:23,330
on these cortex you know especially

00:16:20,480 --> 00:16:25,660
cortex-m series we see as a real sweet

00:16:23,330 --> 00:16:33,110
spot for doing this kind of low-power

00:16:25,660 --> 00:16:36,230
compute so really what I want to get out

00:16:33,110 --> 00:16:38,840
of this conference is I want to hear

00:16:36,230 --> 00:16:41,830
from the people who are at the cutting

00:16:38,840 --> 00:16:44,210
edge of this work you know the engineers

00:16:41,830 --> 00:16:49,220
who are dealing with this day-to-day

00:16:44,210 --> 00:16:52,850
like what your requirements are what the

00:16:49,220 --> 00:16:57,410
best ways are for us to help and

00:16:52,850 --> 00:17:00,140
collaborate and get help from you to

00:16:57,410 --> 00:17:04,070
move this stuff forward so I wanted to

00:17:00,140 --> 00:17:07,339
leave a few minutes now for some

00:17:04,070 --> 00:17:10,070
questions and also if you want to come

00:17:07,339 --> 00:17:14,000
up afterwards I do have some tensorflow

00:17:10,070 --> 00:17:15,260
stickers so even if I'm not able to

00:17:14,000 --> 00:17:18,500
answer your question at least you'll get

00:17:15,260 --> 00:17:21,410
a sticker out of it so yeah if you want

00:17:18,500 --> 00:17:24,040
to jump in and sort of throw some

00:17:21,410 --> 00:17:24,040
questions at me

00:17:36,630 --> 00:17:45,179
so you the you did some integration by

00:17:40,080 --> 00:17:47,280
eigen and obviously arm compute and n

00:17:45,179 --> 00:17:52,490
gives you well the compute live because

00:17:47,280 --> 00:17:55,620
we're different pass through that is a

00:17:52,490 --> 00:17:59,400
would you make a decision either way at

00:17:55,620 --> 00:18:01,350
some point yeah so what we've found is

00:17:59,400 --> 00:18:03,780
that we're getting significantly better

00:18:01,350 --> 00:18:05,549
performance through arm compute library

00:18:03,780 --> 00:18:07,590
than we are through

00:18:05,549 --> 00:18:11,010
I like our existing eigen

00:18:07,590 --> 00:18:15,510
implementations part of that is because

00:18:11,010 --> 00:18:17,909
I ghen doesn't have any inherent notion

00:18:15,510 --> 00:18:20,120
of threading like if you want to use

00:18:17,909 --> 00:18:22,980
threading you need to use open MP and

00:18:20,120 --> 00:18:28,289
open MP is not very well supported on

00:18:22,980 --> 00:18:31,890
Android so having a framework that's

00:18:28,289 --> 00:18:33,750
actually able to make really like has

00:18:31,890 --> 00:18:36,809
kind of threading support and it's able

00:18:33,750 --> 00:18:40,049
to make really intelligent like parallel

00:18:36,809 --> 00:18:41,789
processing decisions built in I feel

00:18:40,049 --> 00:18:44,070
like is a big advantage of the arm

00:18:41,789 --> 00:18:45,299
compute library compared to sort of like

00:18:44,070 --> 00:18:48,059
I go and basically doesn't want the

00:18:45,299 --> 00:18:49,740
responsibility of kind of understanding

00:18:48,059 --> 00:18:52,320
how threads work across all of these

00:18:49,740 --> 00:18:53,940
different platforms so the fact that the

00:18:52,320 --> 00:18:57,510
arm compute library is able to I think

00:18:53,940 --> 00:19:01,020
is a big advantage and you know we're

00:18:57,510 --> 00:19:04,010
still using eigen in you know other

00:19:01,020 --> 00:19:07,020
situations for other things so you know

00:19:04,010 --> 00:19:08,880
the nice thing is that we do get the

00:19:07,020 --> 00:19:11,940
choice of sort of like picking and

00:19:08,880 --> 00:19:13,320
mixing you know these technologies and

00:19:11,940 --> 00:19:18,230
using the ones that make the most sense

00:19:13,320 --> 00:19:18,230
in the particular circumstances

00:19:22,560 --> 00:19:26,240
so I'm guessing you may have kind of

00:19:24,420 --> 00:19:28,350
answered this in the last question but

00:19:26,240 --> 00:19:30,300
if you're running on something like a

00:19:28,350 --> 00:19:33,210
raspberry pie you've got a quad-core a

00:19:30,300 --> 00:19:36,630
53 are you actually running a

00:19:33,210 --> 00:19:38,880
multi-threaded tend to blow at the tenth

00:19:36,630 --> 00:19:43,590
slow light layer or are you leaving the

00:19:38,880 --> 00:19:45,600
threading to the lower layers so what if

00:19:43,590 --> 00:19:50,010
you grab the current pip binary it's

00:19:45,600 --> 00:19:52,370
using a multi-threaded icon

00:19:50,010 --> 00:19:57,060
implementation that I hacked up myself

00:19:52,370 --> 00:20:00,300
and it's really it's it's really

00:19:57,060 --> 00:20:02,970
unpleasant there's a bunch of reasons

00:20:00,300 --> 00:20:04,890
one of them is that especially on

00:20:02,970 --> 00:20:07,020
Android wear we put a lot of our

00:20:04,890 --> 00:20:10,220
performance effort things like the CPU

00:20:07,020 --> 00:20:12,690
governor's really mess up your

00:20:10,220 --> 00:20:13,950
multi-threading strategy you really have

00:20:12,690 --> 00:20:16,770
to know what's happening in the system

00:20:13,950 --> 00:20:18,420
to make a good choice the best

00:20:16,770 --> 00:20:22,170
performance I found I was able to get

00:20:18,420 --> 00:20:27,150
was if I had like up to like several

00:20:22,170 --> 00:20:30,510
milliseconds of sort of busy waiting

00:20:27,150 --> 00:20:34,290
loops yeah I know everyone here's

00:20:30,510 --> 00:20:36,780
wincing because what would happen would

00:20:34,290 --> 00:20:39,980
be that as soon as you'd break up the

00:20:36,780 --> 00:20:42,720
like the matrix-multiply tiles into

00:20:39,980 --> 00:20:45,300
different chunks send each one of them

00:20:42,720 --> 00:20:48,840
to a different core as soon as one core

00:20:45,300 --> 00:20:50,880
finished it would get put to sleep even

00:20:48,840 --> 00:20:52,670
though the very next layer was then

00:20:50,880 --> 00:20:57,060
going to have to spin up that same core

00:20:52,670 --> 00:20:58,230
to you know do further you know so when

00:20:57,060 --> 00:21:00,030
we knew that there was going to be

00:20:58,230 --> 00:21:02,430
further work happening we would actually

00:21:00,030 --> 00:21:05,840
put in busy waiting loops just to kind

00:21:02,430 --> 00:21:08,070
of like trick the CPU governor sort of

00:21:05,840 --> 00:21:11,790
implementation and things to get the

00:21:08,070 --> 00:21:14,940
performance we needed so threading has

00:21:11,790 --> 00:21:18,300
really been one of our you know one of

00:21:14,940 --> 00:21:22,200
the things where we we just don't have

00:21:18,300 --> 00:21:23,970
the expertise we kind of get there by

00:21:22,200 --> 00:21:25,410
trial and error to figure out what we

00:21:23,970 --> 00:21:30,300
should be doing to get good performance

00:21:25,410 --> 00:21:32,610
but it's really really hard to figure

00:21:30,300 --> 00:21:35,970
out as an outsider whereas you know

00:21:32,610 --> 00:21:37,710
armed team has dealt with that in a lot

00:21:35,970 --> 00:21:40,110
of different circumstances they really

00:21:37,710 --> 00:21:41,730
have some great expertise and that's

00:21:40,110 --> 00:21:45,390
like a great example of where we're

00:21:41,730 --> 00:21:46,620
happy to have somebody else come in and

00:21:45,390 --> 00:21:51,750
sort of you know have solved that

00:21:46,620 --> 00:21:53,070
problem for us and so one follow-up

00:21:51,750 --> 00:21:57,570
you've obviously talked a lot about

00:21:53,070 --> 00:21:59,640
Android on PI and Android have you does

00:21:57,570 --> 00:22:03,540
the code work out of the box on Linux oh

00:21:59,640 --> 00:22:07,230
yeah yeah and actually our main focus on

00:22:03,540 --> 00:22:09,360
the Raspberry Pi is raspbian so yeah

00:22:07,230 --> 00:22:11,850
we're very we're very focused on Linux

00:22:09,360 --> 00:22:15,360
and we have a bunch of internal users

00:22:11,850 --> 00:22:20,340
who are using Linux on Android so yeah

00:22:15,360 --> 00:22:22,380
we really care about that hi I have one

00:22:20,340 --> 00:22:26,130
question so can you share a little bit

00:22:22,380 --> 00:22:30,540
about the state has Excel a and

00:22:26,130 --> 00:22:32,190
especially on the arm platforms yeah no

00:22:30,540 --> 00:22:37,490
that's a good question because Excel a

00:22:32,190 --> 00:22:42,120
is are like one of Google's compiler

00:22:37,490 --> 00:22:47,250
technologies it turns tensorflow graphs

00:22:42,120 --> 00:22:50,790
into an intermediate representation one

00:22:47,250 --> 00:22:55,830
of the you know the excel a has been

00:22:50,790 --> 00:23:00,140
largely focused on the cloud TPU so

00:22:55,830 --> 00:23:03,720
there hasn't been a very heavy focus on

00:23:00,140 --> 00:23:06,750
running on arm platforms and one of the

00:23:03,720 --> 00:23:09,720
things that we have found is that a lot

00:23:06,750 --> 00:23:11,040
of the performance that you get even

00:23:09,720 --> 00:23:13,260
when you're doing things like

00:23:11,040 --> 00:23:15,510
intermediate representations like Excel

00:23:13,260 --> 00:23:17,669
a actually comes from the sort of matrix

00:23:15,510 --> 00:23:26,130
multiply and the convolution primitives

00:23:17,669 --> 00:23:27,960
themselves so we have had we expect that

00:23:26,130 --> 00:23:30,299
there may be sort of further integration

00:23:27,960 --> 00:23:32,910
with Excel a in the future on arm but

00:23:30,299 --> 00:23:35,940
that's not our focus at the moment I

00:23:32,910 --> 00:23:38,970
guess is like we because we see a lot of

00:23:35,940 --> 00:23:40,710
we still see hand coding these kind of

00:23:38,970 --> 00:23:42,900
primitives to be the place where you

00:23:40,710 --> 00:23:44,520
really get performance and the the

00:23:42,900 --> 00:23:46,770
fusing x' and other things that you get

00:23:44,520 --> 00:23:48,510
out of compilers

00:23:46,770 --> 00:23:51,380
don't seem to make nearly as much of a

00:23:48,510 --> 00:23:51,380
dramatic difference

00:23:56,370 --> 00:24:04,710
I am I kind of work enough on governors

00:24:00,720 --> 00:24:06,240
and scheduling site to make a comment on

00:24:04,710 --> 00:24:07,710
your previous statement that you saw

00:24:06,240 --> 00:24:09,930
that the governors were kind of screwing

00:24:07,710 --> 00:24:11,850
things up for you have you tried using

00:24:09,930 --> 00:24:14,880
thread pools and sticking to using the

00:24:11,850 --> 00:24:16,770
same thread for because that would

00:24:14,880 --> 00:24:19,890
generally help because the governor's do

00:24:16,770 --> 00:24:21,480
try to remember what the thread did last

00:24:19,890 --> 00:24:24,360
time

00:24:21,480 --> 00:24:28,410
no no and I would love I would love help

00:24:24,360 --> 00:24:30,750
on that as well like any any sort of

00:24:28,410 --> 00:24:33,360
like debugging help we have and that's

00:24:30,750 --> 00:24:35,070
where I'm hoping that getting some of

00:24:33,360 --> 00:24:39,900
our latency benchmarks out into the

00:24:35,070 --> 00:24:42,750
world will help people point out where

00:24:39,900 --> 00:24:45,110
we're doing silly things and give people

00:24:42,750 --> 00:24:48,960
an easy way to kind of you know it's

00:24:45,110 --> 00:24:50,929
like dig into you know places where

00:24:48,960 --> 00:24:53,309
we're leaving performance on the floor

00:24:50,929 --> 00:24:54,960
because I'm sure that there's a lot of

00:24:53,309 --> 00:24:57,090
places where we're just not

00:24:54,960 --> 00:24:59,750
understanding the you know the

00:24:57,090 --> 00:25:02,100
intricacies of what we should be doing

00:24:59,750 --> 00:25:03,570
so that's one of the things we're really

00:25:02,100 --> 00:25:06,150
trying to do is open up the code base

00:25:03,570 --> 00:25:09,210
make it easier to hack on and easier to

00:25:06,150 --> 00:25:10,830
understand so that we hopefully pull in

00:25:09,210 --> 00:25:15,179
we've got a bunch of external

00:25:10,830 --> 00:25:16,200
contributors but we want more and so one

00:25:15,179 --> 00:25:18,240
of the areas where machine learning

00:25:16,200 --> 00:25:21,030
seems to be fairly promising is around

00:25:18,240 --> 00:25:22,860
scheduling so I'm wondering if you guys

00:25:21,030 --> 00:25:24,840
have looked into machine learning

00:25:22,860 --> 00:25:26,940
algorithms to understand when to ramp up

00:25:24,840 --> 00:25:31,350
cause and keep things active for

00:25:26,940 --> 00:25:35,390
differing workloads yeah and kind of

00:25:31,350 --> 00:25:39,270
related to that we actually have Android

00:25:35,390 --> 00:25:42,480
auto awesome battery which kind of

00:25:39,270 --> 00:25:45,050
actually looks at uses a machine

00:25:42,480 --> 00:25:51,210
learning model I think on Android Pi and

00:25:45,050 --> 00:25:53,400
actually optimizes we you know the

00:25:51,210 --> 00:25:55,260
priority of different applications and

00:25:53,400 --> 00:25:57,900
my kind of shuts down applications

00:25:55,260 --> 00:25:59,040
in order to extend battery life and I

00:25:57,900 --> 00:26:03,780
think there's been a significant

00:25:59,040 --> 00:26:06,510
percentage increase in battery life just

00:26:03,780 --> 00:26:09,180
from just from that so I'm actually

00:26:06,510 --> 00:26:11,820
really I'm really interested in

00:26:09,180 --> 00:26:15,690
seeing how those ML models can kind of

00:26:11,820 --> 00:26:17,730
make it into kind of some of those more

00:26:15,690 --> 00:26:19,680
tricky sort of scheduling decisions so I

00:26:17,730 --> 00:26:21,030
think that is a really promising area

00:26:19,680 --> 00:26:23,640
it's like anywhere where you've got

00:26:21,030 --> 00:26:25,290
arbitrary rules that you're just kind of

00:26:23,640 --> 00:26:27,120
you know having to make up yourself if

00:26:25,290 --> 00:26:29,670
you can get a machine learning model to

00:26:27,120 --> 00:26:34,410
learn them then there's often a pretty

00:26:29,670 --> 00:26:36,030
big win question this isn't quite armed

00:26:34,410 --> 00:26:38,400
but there's been a lot of attention on

00:26:36,030 --> 00:26:39,690
the Mobius neural compute stick as a

00:26:38,400 --> 00:26:41,670
parent I'm curious about your thoughts

00:26:39,690 --> 00:26:44,630
on running that in combination with the

00:26:41,670 --> 00:26:49,320
Raspberry Pi and denser flow no I love

00:26:44,630 --> 00:26:54,210
the myriad sort of my videos compute

00:26:49,320 --> 00:26:56,640
stick we actually have the AI Y sort of

00:26:54,210 --> 00:26:58,980
series of products where if you go to a

00:26:56,640 --> 00:27:02,340
Target store in the US you can actually

00:26:58,980 --> 00:27:05,330
get a sort of a foldable cardboard box

00:27:02,340 --> 00:27:08,970
with a Raspberry Pi and a nvidia s'

00:27:05,330 --> 00:27:13,280
stick in it as part of the aiy vision

00:27:08,970 --> 00:27:18,300
kit and you can run a bunch of models

00:27:13,280 --> 00:27:20,370
you know through that I think they're

00:27:18,300 --> 00:27:25,500
again one of the big challenges is

00:27:20,370 --> 00:27:28,320
usability it's currently quite hard to

00:27:25,500 --> 00:27:32,040
take an arbitrary model and run it

00:27:28,320 --> 00:27:37,710
through the conversion to get it running

00:27:32,040 --> 00:27:39,270
on the video stick and that's one of the

00:27:37,710 --> 00:27:41,040
experiences that we went through where

00:27:39,270 --> 00:27:43,590
we are like okay we really really need

00:27:41,040 --> 00:27:45,360
to get usability better and we need to

00:27:43,590 --> 00:27:48,810
collaborate more with people who are

00:27:45,360 --> 00:27:51,240
creating these you know these devices to

00:27:48,810 --> 00:27:52,050
make sure that our users are able to

00:27:51,240 --> 00:27:54,870
take tensorflow

00:27:52,050 --> 00:27:57,510
models and actually get them onto things

00:27:54,870 --> 00:27:58,680
like the video stick because it's it's

00:27:57,510 --> 00:28:02,310
great having something that's really

00:27:58,680 --> 00:28:05,070
fast but if you can't use it it's you

00:28:02,310 --> 00:28:07,320
know not that helpful Thank You Pete

00:28:05,070 --> 00:28:09,270
thank you very much thanks so much so

00:28:07,320 --> 00:28:12,330
the next speaker is Mark Charlevoix from

00:28:09,270 --> 00:28:14,020
Qualcomm please mark welcome

00:28:12,330 --> 00:28:17,440
you

00:28:14,020 --> 00:28:22,449
[Applause]

00:28:17,440 --> 00:28:22,449

YouTube URL: https://www.youtube.com/watch?v=xYtw7fN2C88


