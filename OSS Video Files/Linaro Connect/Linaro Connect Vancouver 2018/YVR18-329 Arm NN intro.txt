Title: YVR18-329 Arm NN intro
Publication date: 2018-09-28
Playlist: Linaro Connect Vancouver 2018
Description: 
	An overview of the Arm NN design and codebase, what interfaces exist and why, and how people are using it today. Weâ€™ll also discuss upcoming features that Arm are contributing to this codebase.
Captions: 
	00:00:02,150 --> 00:00:07,160
[Music]

00:00:08,259 --> 00:00:16,900
okay let's start it's time to start

00:00:11,329 --> 00:00:16,900
so please welcome Rob Elliott from arm

00:00:23,030 --> 00:00:30,050
[Applause]

00:00:26,770 --> 00:00:31,520
okay thank you enjoy so my name is Rob

00:00:30,050 --> 00:00:33,650
Elliott I'm the director of applied

00:00:31,520 --> 00:00:35,690
machine learning at arm what that means

00:00:33,650 --> 00:00:37,970
in practice is I look at our customers

00:00:35,690 --> 00:00:39,410
use cases our partners use cases how

00:00:37,970 --> 00:00:41,570
they decompose to run on our software

00:00:39,410 --> 00:00:43,670
stacks and our hardware and try and find

00:00:41,570 --> 00:00:47,120
gaps in that and improve the system as a

00:00:43,670 --> 00:00:50,120
whole and what I'm here to talk about

00:00:47,120 --> 00:00:52,730
today is a bit of a deeper dive into

00:00:50,120 --> 00:00:54,590
Arman n so hopefully all of you caught

00:00:52,730 --> 00:00:57,590
Gemma's keno earlier today where he

00:00:54,590 --> 00:01:00,590
spoke about what we're doing with Lynne

00:00:57,590 --> 00:01:02,360
ro so this donation of engineering

00:01:00,590 --> 00:01:04,580
effort this code base that we've been

00:01:02,360 --> 00:01:06,890
working on for a long time and some of

00:01:04,580 --> 00:01:08,690
our aspirations with that and so to what

00:01:06,890 --> 00:01:11,360
I'm going to talk about is in more

00:01:08,690 --> 00:01:14,120
practical terms what the arm and encode

00:01:11,360 --> 00:01:17,270
base is what interfaces there are and

00:01:14,120 --> 00:01:19,729
how people are using them what's our

00:01:17,270 --> 00:01:21,380
mannan doesn't seek to do so did the

00:01:19,729 --> 00:01:23,299
problem space that we've carved out as

00:01:21,380 --> 00:01:26,479
we see as appropriate for inference

00:01:23,299 --> 00:01:27,799
engines and then some ideas on areas for

00:01:26,479 --> 00:01:29,600
improvements and how you might get

00:01:27,799 --> 00:01:30,920
involved in the and working on the

00:01:29,600 --> 00:01:35,540
island end code base and working with

00:01:30,920 --> 00:01:39,260
arm and Lenara on this problem so first

00:01:35,540 --> 00:01:41,210
off you may this slide was also in James

00:01:39,260 --> 00:01:43,130
deck but I'll start to talk and

00:01:41,210 --> 00:01:46,909
decompose this into a bit more detail

00:01:43,130 --> 00:01:50,330
so what is Amon n so fundamentally it's

00:01:46,909 --> 00:01:53,509
a graphic consumption API and inference

00:01:50,330 --> 00:01:55,610
API that is a runtime that runs on

00:01:53,509 --> 00:01:58,549
Android and Linux on Android is

00:01:55,610 --> 00:02:00,439
integrated and Android n N and the N n

00:01:58,549 --> 00:02:02,390
API and under Linux it just runs

00:02:00,439 --> 00:02:03,470
directly connected and so there's a

00:02:02,390 --> 00:02:05,840
couple of different ways you can

00:02:03,470 --> 00:02:07,369
interact with this the first is

00:02:05,840 --> 00:02:09,200
connecting directly to the higher-level

00:02:07,369 --> 00:02:12,260
framework so we've got support for

00:02:09,200 --> 00:02:13,610
tensorflow cafe looking at support for

00:02:12,260 --> 00:02:15,260
pi torch

00:02:13,610 --> 00:02:17,900
we in the latest release we've got

00:02:15,260 --> 00:02:21,080
support for importing data from onyx as

00:02:17,900 --> 00:02:24,020
well the second way is integrating

00:02:21,080 --> 00:02:25,910
through an existing inference engine so

00:02:24,020 --> 00:02:27,730
as you probably know there are many

00:02:25,910 --> 00:02:31,190
different inference engines out there

00:02:27,730 --> 00:02:32,780
we're integrating in another code base

00:02:31,190 --> 00:02:34,790
that we've released publicly with the

00:02:32,780 --> 00:02:36,620
Android nn API so that's available on

00:02:34,790 --> 00:02:38,720
github today and then there are a number

00:02:36,620 --> 00:02:40,280
of other partner and customer inference

00:02:38,720 --> 00:02:44,060
engines that we've also integrated with

00:02:40,280 --> 00:02:46,880
as well and then the other way you would

00:02:44,060 --> 00:02:48,709
typically interact with this is we have

00:02:46,880 --> 00:02:52,280
the starting point of a device interface

00:02:48,709 --> 00:02:54,440
and that's really to open up and promote

00:02:52,280 --> 00:02:56,660
other people integrating their IP into

00:02:54,440 --> 00:03:00,020
the same stack and the idea with all of

00:02:56,660 --> 00:03:01,580
this as a whole is that as you get data

00:03:00,020 --> 00:03:03,440
coming in from all of those different

00:03:01,580 --> 00:03:05,750
machine learning frameworks we can get

00:03:03,440 --> 00:03:07,700
to this pinch point in the middle which

00:03:05,750 --> 00:03:11,450
supports all of those handles all of the

00:03:07,700 --> 00:03:13,130
tools and translations quantization that

00:03:11,450 --> 00:03:15,380
you need to do to these networks to get

00:03:13,130 --> 00:03:17,030
them to run efficiently and then helps

00:03:15,380 --> 00:03:19,010
you get that on to all of the bits of IP

00:03:17,030 --> 00:03:21,230
in the system and we've done that by

00:03:19,010 --> 00:03:23,720
optimized have optimizing heavily for

00:03:21,230 --> 00:03:25,040
the CPU GPU and our MPU and we're

00:03:23,720 --> 00:03:27,560
encouraging other people to get involved

00:03:25,040 --> 00:03:29,989
to do that for their IPs as well or just

00:03:27,560 --> 00:03:34,070
to reuse this code base on their arm CP

00:03:29,989 --> 00:03:39,200
or GPU enable platforms and so then to

00:03:34,070 --> 00:03:40,580
look at the Android side of this so this

00:03:39,200 --> 00:03:43,790
breaks down in in a number of different

00:03:40,580 --> 00:03:46,700
ways so within Android nn API there is a

00:03:43,790 --> 00:03:47,930
Hal that and that howl is used for

00:03:46,700 --> 00:03:51,440
abstraction and many different devices

00:03:47,930 --> 00:03:53,540
already some of you may be aware what we

00:03:51,440 --> 00:03:55,580
do with that is we have we have an

00:03:53,540 --> 00:03:57,350
implementation of our Arminian inference

00:03:55,580 --> 00:03:59,720
engine so that's the same code base that

00:03:57,350 --> 00:04:03,440
runs on Linux and that allows you to

00:03:59,720 --> 00:04:05,570
access the GPU or it allows you to

00:04:03,440 --> 00:04:07,970
access the MPU so we'll run one instance

00:04:05,570 --> 00:04:10,130
for each of those but we've also been

00:04:07,970 --> 00:04:13,190
doing a lot of work with Google over the

00:04:10,130 --> 00:04:14,570
past a year or two optimizing the CPU

00:04:13,190 --> 00:04:17,269
routines that are available in

00:04:14,570 --> 00:04:21,620
tensorflow light and it available in the

00:04:17,269 --> 00:04:24,560
Android and an API and so on the Android

00:04:21,620 --> 00:04:26,610
deployment of this you fall back to

00:04:24,560 --> 00:04:29,160
using the scheduler there is a

00:04:26,610 --> 00:04:30,480
within the Android and napi and you're

00:04:29,160 --> 00:04:33,570
just using the arm and any inference

00:04:30,480 --> 00:04:38,840
engine to translate those those networks

00:04:33,570 --> 00:04:42,000
to run on the GPU or the mpu and that's

00:04:38,840 --> 00:04:46,350
available open source today at the

00:04:42,000 --> 00:04:49,080
github link there and then moving on to

00:04:46,350 --> 00:04:52,230
talk about the deployment and the Linux

00:04:49,080 --> 00:04:53,790
or indeed Android as well so one of the

00:04:52,230 --> 00:04:57,120
things that we're finding is because

00:04:53,790 --> 00:04:59,130
Android releases are at a fairly low

00:04:57,120 --> 00:05:00,690
cadence we're finding a lot of new

00:04:59,130 --> 00:05:03,600
operators that people are interested in

00:05:00,690 --> 00:05:04,860
as well and so we try and run as fast as

00:05:03,600 --> 00:05:06,420
we can to keep up with all of the

00:05:04,860 --> 00:05:08,160
operators that people are interested in

00:05:06,420 --> 00:05:10,050
when they're processing these neural

00:05:08,160 --> 00:05:12,150
networks be that new ways of doing

00:05:10,050 --> 00:05:14,580
convolution convolution new activation

00:05:12,150 --> 00:05:16,800
functions and we expose that through

00:05:14,580 --> 00:05:18,870
this arm in an SDK and when you're

00:05:16,800 --> 00:05:20,910
running in this mode the almond n SDK is

00:05:18,870 --> 00:05:23,670
acting as the runtime and the scheduler

00:05:20,910 --> 00:05:27,330
as well and that can call through to the

00:05:23,670 --> 00:05:28,890
the CPU GPU or RMP you and then any

00:05:27,330 --> 00:05:31,050
other devices as well that you want to

00:05:28,890 --> 00:05:33,090
support in the system and this is the

00:05:31,050 --> 00:05:36,060
path through which we support tensorflow

00:05:33,090 --> 00:05:38,940
tensorflow light whereas with comparison

00:05:36,060 --> 00:05:41,790
on Android that's handled by Google and

00:05:38,940 --> 00:05:44,880
the Android ecosystem instead this is

00:05:41,790 --> 00:05:47,340
also available open source at the link

00:05:44,880 --> 00:05:50,130
here so you can go go and download that

00:05:47,340 --> 00:05:51,630
and try out today our last release which

00:05:50,130 --> 00:05:54,600
will in fact be the last release to this

00:05:51,630 --> 00:05:56,550
could get hub location happened about

00:05:54,600 --> 00:06:02,310
two weeks ago so you'll get the very

00:05:56,550 --> 00:06:05,100
latest code there and with the almond N

00:06:02,310 --> 00:06:07,170
SDK so that's the C++ 14 API for Linux

00:06:05,100 --> 00:06:09,150
on Android and as I say distributed on

00:06:07,170 --> 00:06:11,640
arm and then if you want to get a good

00:06:09,150 --> 00:06:13,920
overview of how the API works go and

00:06:11,640 --> 00:06:16,230
look at that header file or I'll also go

00:06:13,920 --> 00:06:20,880
through and talk through some of the

00:06:16,230 --> 00:06:22,680
details and what that interface is so on

00:06:20,880 --> 00:06:24,420
that initial slider talk through a bit

00:06:22,680 --> 00:06:26,970
of the on the framework integration

00:06:24,420 --> 00:06:29,040
options you have so so we've got support

00:06:26,970 --> 00:06:29,960
under Android nn and under Linux for

00:06:29,040 --> 00:06:32,640
tensorflow

00:06:29,960 --> 00:06:34,200
we're starting we've got support for

00:06:32,640 --> 00:06:36,330
cafe as well but we're starting to add

00:06:34,200 --> 00:06:39,689
support either for the frameworks

00:06:36,330 --> 00:06:42,449
directly like MX net or via onyx which

00:06:39,689 --> 00:06:44,099
is looking to standardize the conversion

00:06:42,449 --> 00:06:48,959
from multiple different frameworks to

00:06:44,099 --> 00:06:52,079
run on various do inference engines and

00:06:48,959 --> 00:06:54,119
over time our ambition here is to add

00:06:52,079 --> 00:06:55,409
support for more of these but more

00:06:54,119 --> 00:06:57,899
importantly I think particularly through

00:06:55,409 --> 00:07:00,329
the Lennar efforts start to standardize

00:06:57,899 --> 00:07:02,639
on an intermediate form that can be used

00:07:00,329 --> 00:07:04,860
by M and n can be used by other

00:07:02,639 --> 00:07:06,689
inference engines if they need to can be

00:07:04,860 --> 00:07:08,849
used by tools that are optimizing these

00:07:06,689 --> 00:07:10,679
graphs can be generated from a number of

00:07:08,849 --> 00:07:12,389
these different frameworks and whether

00:07:10,679 --> 00:07:14,219
that's onyx or some other intermediate

00:07:12,389 --> 00:07:15,869
form I think remains to be seen but

00:07:14,219 --> 00:07:17,610
getting to the point where you have

00:07:15,869 --> 00:07:20,219
compatibility from multiple different

00:07:17,610 --> 00:07:21,629
frameworks answering really that you

00:07:20,219 --> 00:07:24,059
know the fundamental question at the end

00:07:21,629 --> 00:07:26,159
of Chris's talk earlier was how do we

00:07:24,059 --> 00:07:28,889
solve this deployment problem this

00:07:26,159 --> 00:07:31,349
framework a runs on device a framework B

00:07:28,889 --> 00:07:32,819
runs on device B but framework B doesn't

00:07:31,349 --> 00:07:34,889
run on device a how do you solve that

00:07:32,819 --> 00:07:37,769
problem that's really at the heart of

00:07:34,889 --> 00:07:39,439
what we're trying to get to here so

00:07:37,769 --> 00:07:42,360
those are the various different

00:07:39,439 --> 00:07:44,550
framework integration options as to how

00:07:42,360 --> 00:07:46,860
Armen n works in a bit more detail as

00:07:44,550 --> 00:07:49,979
you break it down so the first thing it

00:07:46,860 --> 00:07:52,679
does is so everything is built on top of

00:07:49,979 --> 00:07:55,379
the graph builder API and fundamentally

00:07:52,679 --> 00:07:57,949
that means taking any one of these

00:07:55,379 --> 00:08:01,019
Network representation formats

00:07:57,949 --> 00:08:02,789
decomposing it through an importer and

00:08:01,019 --> 00:08:04,829
breaking it down and calling into the

00:08:02,789 --> 00:08:06,749
graph builder API that constructs this

00:08:04,829 --> 00:08:09,209
so again tensorflow

00:08:06,749 --> 00:08:10,739
TF like cafe you'll have a runtime

00:08:09,209 --> 00:08:14,759
component that's running on the edge

00:08:10,739 --> 00:08:16,679
device on current a-class platforms and

00:08:14,759 --> 00:08:18,089
it will break down that network

00:08:16,679 --> 00:08:20,429
architecture and call into the graph

00:08:18,089 --> 00:08:22,559
builder API to produce an internal

00:08:20,429 --> 00:08:24,749
representation and that's the same flow

00:08:22,559 --> 00:08:29,309
that we're using as we deploy on Android

00:08:24,749 --> 00:08:31,979
m and n as well and so once you've got

00:08:29,309 --> 00:08:33,629
this graphic builder API run and you've

00:08:31,979 --> 00:08:37,019
got the internal graph representation

00:08:33,629 --> 00:08:39,750
you then start to look at how you want

00:08:37,019 --> 00:08:41,639
to interact with this so we do have an

00:08:39,750 --> 00:08:43,139
internal fire representation but this is

00:08:41,639 --> 00:08:45,110
not one of the things that Arman and

00:08:43,139 --> 00:08:47,279
generally is trying to achieve

00:08:45,110 --> 00:08:48,959
generally the joke earlier that you know

00:08:47,279 --> 00:08:51,509
fwrite doesn't make a standard so we

00:08:48,959 --> 00:08:53,230
basically emit what our internal graph

00:08:51,509 --> 00:08:55,150
representation is just for

00:08:53,230 --> 00:08:58,630
storage and retrieval for various

00:08:55,150 --> 00:09:01,000
different reasons but the runtime graph

00:08:58,630 --> 00:09:03,400
representation is the common format

00:09:01,000 --> 00:09:06,940
within the Armen and software stack and

00:09:03,400 --> 00:09:08,170
that's used for you know both consuming

00:09:06,940 --> 00:09:09,970
of graphs in the first place and

00:09:08,170 --> 00:09:12,100
representing them but also running them

00:09:09,970 --> 00:09:13,660
through the optimizer and running them

00:09:12,100 --> 00:09:15,460
through the runtime to then decompose

00:09:13,660 --> 00:09:18,070
them and run them on on the device and

00:09:15,460 --> 00:09:21,130
then the optimization part of this looks

00:09:18,070 --> 00:09:22,660
at a few different things the first

00:09:21,130 --> 00:09:24,730
thing that happens is it runs through a

00:09:22,660 --> 00:09:26,860
scheduler to understand which pieces of

00:09:24,730 --> 00:09:29,920
ip are best for running this workload

00:09:26,860 --> 00:09:32,410
and where possible and tuning and fusing

00:09:29,920 --> 00:09:34,030
operations together it then runs through

00:09:32,410 --> 00:09:35,680
the graph optimizer that looks to

00:09:34,030 --> 00:09:37,750
simplify and you've got a number of

00:09:35,680 --> 00:09:39,640
different ways you can share your

00:09:37,750 --> 00:09:41,050
networks you can go either breadth-first

00:09:39,640 --> 00:09:43,180
across each layer of the network

00:09:41,050 --> 00:09:44,620
architecture or you can go depth-first

00:09:43,180 --> 00:09:46,930
through various different nodes that are

00:09:44,620 --> 00:09:48,250
being processed and so what that tries

00:09:46,930 --> 00:09:49,690
to do is find the best edge willing

00:09:48,250 --> 00:09:51,400
pattern statically that will give you

00:09:49,690 --> 00:09:53,080
the best performance on the target

00:09:51,400 --> 00:09:55,720
devices that you've selected or that the

00:09:53,080 --> 00:09:57,850
runtime is selected and then the third

00:09:55,720 --> 00:10:00,610
thing that happens our target

00:09:57,850 --> 00:10:02,680
optimizations and there are some basic

00:10:00,610 --> 00:10:05,170
ones there in the optimizer layer but

00:10:02,680 --> 00:10:07,510
more often than not we would expect then

00:10:05,170 --> 00:10:09,760
that this graph representation is passed

00:10:07,510 --> 00:10:11,710
into this runtime interface on the right

00:10:09,760 --> 00:10:14,260
here through into the plug-in device

00:10:11,710 --> 00:10:16,990
backends and then each of those can do

00:10:14,260 --> 00:10:19,390
their own optimizations so for example

00:10:16,990 --> 00:10:23,020
with our mpu it likes to take the full

00:10:19,390 --> 00:10:24,700
representation of the graph so it can it

00:10:23,020 --> 00:10:26,470
can change the scheduling order of the

00:10:24,700 --> 00:10:29,050
graph and keep all of the data in

00:10:26,470 --> 00:10:32,770
resident in local s realms to make more

00:10:29,050 --> 00:10:34,150
efficient use of our MTU to do that we

00:10:32,770 --> 00:10:36,670
need the full graph representation

00:10:34,150 --> 00:10:38,260
passed through so so we bypass some of

00:10:36,670 --> 00:10:40,420
the optimizations that happen in the

00:10:38,260 --> 00:10:41,890
generic optimizer at the top and we're

00:10:40,420 --> 00:10:43,120
finding lots of other partners who are

00:10:41,890 --> 00:10:45,310
looking at integrating their devices

00:10:43,120 --> 00:10:46,990
have these same kind of needs so that

00:10:45,310 --> 00:10:49,560
that's it's constructed to allow people

00:10:46,990 --> 00:10:52,180
to do that and the other thing that

00:10:49,560 --> 00:10:54,310
happens along the way is there's also a

00:10:52,180 --> 00:10:56,080
memory manager in there that handles the

00:10:54,310 --> 00:10:58,090
allocation of the major input and output

00:10:56,080 --> 00:10:59,830
buffers and any intermediate scratch

00:10:58,090 --> 00:11:02,050
memory that's needed for processing of

00:10:59,830 --> 00:11:04,660
the graph as a whole and any memory

00:11:02,050 --> 00:11:06,640
that's needed for passing operators from

00:11:04,660 --> 00:11:08,350
those stages of process

00:11:06,640 --> 00:11:09,730
from one device to another if the graph

00:11:08,350 --> 00:11:15,190
has been broken up to run on multiple

00:11:09,730 --> 00:11:18,010
devices so the optimizer in a bit more

00:11:15,190 --> 00:11:21,160
detail it takes in this initial I

00:11:18,010 --> 00:11:23,560
network representation of the network

00:11:21,160 --> 00:11:25,960
architecture that you're running does

00:11:23,560 --> 00:11:26,530
validation of the network you'd be

00:11:25,960 --> 00:11:28,300
surprised

00:11:26,530 --> 00:11:29,590
across all of the different machine

00:11:28,300 --> 00:11:31,900
learning frameworks and the tools that

00:11:29,590 --> 00:11:33,880
you can use to prepare data how often

00:11:31,900 --> 00:11:36,460
mistakes are introduced or redundancies

00:11:33,880 --> 00:11:38,320
are introduced like multiple transpose

00:11:36,460 --> 00:11:40,030
operations that effectively are and no

00:11:38,320 --> 00:11:42,940
are so it tries to work through those

00:11:40,030 --> 00:11:46,870
and tidy them up it then modifiers for

00:11:42,940 --> 00:11:48,880
correctness on the target so inserts so

00:11:46,870 --> 00:11:51,220
reinsert spree mute layers if you have

00:11:48,880 --> 00:11:53,440
an operator that only works in a certain

00:11:51,220 --> 00:11:56,590
orientation of input data so you may

00:11:53,440 --> 00:11:58,870
know the nhw C and CH w formats so

00:11:56,590 --> 00:12:01,210
different in memory representations it

00:11:58,870 --> 00:12:02,890
will handle those conversions inserts

00:12:01,210 --> 00:12:05,380
copies and flushes for transitioning

00:12:02,890 --> 00:12:07,990
between devices and then tries to modify

00:12:05,380 --> 00:12:10,510
for performance so fusing and removing

00:12:07,990 --> 00:12:12,790
redundant operations to make things more

00:12:10,510 --> 00:12:15,310
efficient and then that emits the

00:12:12,790 --> 00:12:17,050
optimized Network object that's an

00:12:15,310 --> 00:12:21,880
immutable object that then you can then

00:12:17,050 --> 00:12:23,620
run inference on and then in turn in

00:12:21,880 --> 00:12:25,630
terms of how that's actually run in

00:12:23,620 --> 00:12:27,940
practice so I'm gonna at the moment

00:12:25,630 --> 00:12:29,770
currently supports three backends in the

00:12:27,940 --> 00:12:31,300
code that we've released the first is

00:12:29,770 --> 00:12:34,000
this CPU reference implementation

00:12:31,300 --> 00:12:36,520
primarily for testing so if you're

00:12:34,000 --> 00:12:38,020
worried about bit bit exactness with any

00:12:36,520 --> 00:12:40,180
of one of the ML frameworks you can

00:12:38,020 --> 00:12:42,220
start to introduce operators with high

00:12:40,180 --> 00:12:43,740
accuracy there and then test the other

00:12:42,220 --> 00:12:47,590
backends that are implemented as well

00:12:43,740 --> 00:12:49,180
and we then also expose the CPU

00:12:47,590 --> 00:12:51,730
accelerated back-end and the GPU

00:12:49,180 --> 00:12:55,120
accelerated back-end both of which are

00:12:51,730 --> 00:12:56,500
using the compute library and and as Jim

00:12:55,120 --> 00:12:58,120
was saying earlier that's the area where

00:12:56,500 --> 00:13:00,670
we've spent a significant amount of our

00:12:58,120 --> 00:13:03,190
amount of our effort so the compute

00:13:00,670 --> 00:13:05,440
library is about two two and a half

00:13:03,190 --> 00:13:07,060
years now and has had ten or fifteen

00:13:05,440 --> 00:13:09,340
engineers working on it for that that

00:13:07,060 --> 00:13:11,110
full length of time working to really

00:13:09,340 --> 00:13:13,990
optimize all of the key routines for

00:13:11,110 --> 00:13:16,660
machine learning processing and so we've

00:13:13,990 --> 00:13:19,270
got that targeting neon we're using that

00:13:16,660 --> 00:13:20,480
to track all of the new CPU releases so

00:13:19,270 --> 00:13:23,000
as as we're in

00:13:20,480 --> 00:13:24,920
new vector capabilities in our CPUs and

00:13:23,000 --> 00:13:28,760
new versions of the architecture will

00:13:24,920 --> 00:13:32,600
also expose those as well and then it

00:13:28,760 --> 00:13:34,040
also targets the GPU via OpenCL and so

00:13:32,600 --> 00:13:37,100
you go through the normal open CL driver

00:13:34,040 --> 00:13:38,510
stack there the other area and

00:13:37,100 --> 00:13:40,070
surprisingly that we're working on is

00:13:38,510 --> 00:13:43,220
this internal back-end from machine

00:13:40,070 --> 00:13:45,649
learning processor and so this is a work

00:13:43,220 --> 00:13:47,240
in progress so we're revising the device

00:13:45,649 --> 00:13:49,310
interface at the moment and as part of

00:13:47,240 --> 00:13:50,779
this open sourcing and collaborative

00:13:49,310 --> 00:13:52,730
effort that we're undertaking with

00:13:50,779 --> 00:13:55,100
Lennar o we're going to start publishing

00:13:52,730 --> 00:13:57,050
all of those that documentation publicly

00:13:55,100 --> 00:13:58,730
so if you're interested in working on

00:13:57,050 --> 00:14:00,500
this device interface you can come and

00:13:58,730 --> 00:14:03,040
take a look or you can talk to me or

00:14:00,500 --> 00:14:05,029
Andrea about the machine learning

00:14:03,040 --> 00:14:08,060
special interest group and how you can

00:14:05,029 --> 00:14:09,649
get involved in all of this and then as

00:14:08,060 --> 00:14:11,810
I mentioned earlier the the deployment

00:14:09,649 --> 00:14:14,029
for Android nn currently targets the GPU

00:14:11,810 --> 00:14:15,410
acceleration only this is because you

00:14:14,029 --> 00:14:17,389
know as I said earlier we've worked with

00:14:15,410 --> 00:14:19,160
Google to optimize these routines so the

00:14:17,389 --> 00:14:21,350
CPU path goes through the standard

00:14:19,160 --> 00:14:23,360
tentative low light route and then the

00:14:21,350 --> 00:14:24,980
GPU is accelerated through the the

00:14:23,360 --> 00:14:29,209
arminana interface through the android

00:14:24,980 --> 00:14:31,279
now just to give you an idea of

00:14:29,209 --> 00:14:33,589
capabilities at the moment I won't go

00:14:31,279 --> 00:14:35,120
through all of these in detail but the

00:14:33,589 --> 00:14:36,980
major focus is at the moment our

00:14:35,120 --> 00:14:40,279
convolutional deconvolution or neural

00:14:36,980 --> 00:14:43,100
networks and then also lsdm and other RN

00:14:40,279 --> 00:14:45,800
ends for audio processing so all of the

00:14:43,100 --> 00:14:46,970
examples that Gemma was showing and a

00:14:45,800 --> 00:14:49,250
lot of the ones that Chris was showing

00:14:46,970 --> 00:14:52,670
earlier a majority of those are running

00:14:49,250 --> 00:14:53,959
there are obviously more experimental

00:14:52,670 --> 00:14:55,790
techniques that are coming along and

00:14:53,959 --> 00:14:57,949
we'll continue to add to this operator

00:14:55,790 --> 00:15:00,470
set that's where we are at the moment

00:14:57,949 --> 00:15:02,720
and then to put that in a bit more

00:15:00,470 --> 00:15:05,540
concrete terms if you if you know neural

00:15:02,720 --> 00:15:06,769
network architectures tall so we're

00:15:05,540 --> 00:15:09,190
looking at these convolutional neural

00:15:06,769 --> 00:15:12,230
networks that are either doing

00:15:09,190 --> 00:15:15,860
recognition tasks or doing image

00:15:12,230 --> 00:15:20,110
segmentation and and a number of other

00:15:15,860 --> 00:15:23,170
ones as well okay

00:15:20,110 --> 00:15:26,829
and so so that's what almond end does

00:15:23,170 --> 00:15:28,660
what what doesn't it do so we're really

00:15:26,829 --> 00:15:31,750
focusing our antenna on the inference

00:15:28,660 --> 00:15:33,459
and whilst we do have various stages in

00:15:31,750 --> 00:15:35,980
here that look at optimization of

00:15:33,459 --> 00:15:38,829
networks I think it's important to step

00:15:35,980 --> 00:15:40,690
back and look at how the the entirety of

00:15:38,829 --> 00:15:43,060
this this architecture should work from

00:15:40,690 --> 00:15:44,620
the frameworks all the way down one of

00:15:43,060 --> 00:15:46,839
the arguments you can reasonably make is

00:15:44,620 --> 00:15:48,160
that a lot of this processing is not

00:15:46,839 --> 00:15:48,610
something that should be happening at

00:15:48,160 --> 00:15:50,529
runtime

00:15:48,610 --> 00:15:51,910
it's something that should be happening

00:15:50,529 --> 00:15:53,890
statically in all of the tools that

00:15:51,910 --> 00:15:54,850
build up to this problem so whether

00:15:53,890 --> 00:15:56,769
that's in the machine learning

00:15:54,850 --> 00:15:58,420
frameworks at the top during training

00:15:56,769 --> 00:16:00,910
where you might want to train and run

00:15:58,420 --> 00:16:02,440
through quantization and use a real you

00:16:00,910 --> 00:16:04,600
know a retraining flow to increase

00:16:02,440 --> 00:16:06,490
accuracy or you might want to look at

00:16:04,600 --> 00:16:08,370
compilation technology to get a more

00:16:06,490 --> 00:16:12,600
efficient scheduling of what's being run

00:16:08,370 --> 00:16:14,320
you might want to do pruning and other

00:16:12,600 --> 00:16:15,700
simplifications of the network that

00:16:14,320 --> 00:16:18,010
increased performance on the target

00:16:15,700 --> 00:16:19,959
platform and then a lot of the graph

00:16:18,010 --> 00:16:22,120
cleaning and optimization that happens

00:16:19,959 --> 00:16:24,100
in the run time today is arguably better

00:16:22,120 --> 00:16:26,380
suited to be put into a tool that shared

00:16:24,100 --> 00:16:29,110
by multiple people so that's that's one

00:16:26,380 --> 00:16:30,579
of the areas where we think other tools

00:16:29,110 --> 00:16:32,320
should be handling this in the longer

00:16:30,579 --> 00:16:34,180
term and we need to understand how we

00:16:32,320 --> 00:16:36,130
decompose these problems so people can

00:16:34,180 --> 00:16:38,079
work on tools people can work on

00:16:36,130 --> 00:16:39,699
inference engines we have a common

00:16:38,079 --> 00:16:41,560
interchange between all of those and

00:16:39,699 --> 00:16:44,529
more people can be working together on

00:16:41,560 --> 00:16:46,449
this kind of problem the other the other

00:16:44,529 --> 00:16:48,880
area we haven't looked at and one of the

00:16:46,449 --> 00:16:50,949
areas I mentioned is on the flat file

00:16:48,880 --> 00:16:52,959
formats and the graph manipulation tools

00:16:50,949 --> 00:16:55,600
so one of the things you don't want to

00:16:52,959 --> 00:16:57,940
get to is a point where you you have

00:16:55,600 --> 00:17:02,339
these formats continuously changing and

00:16:57,940 --> 00:17:04,449
you you have to keep stepping back and

00:17:02,339 --> 00:17:06,220
reworking your support for these so if

00:17:04,449 --> 00:17:07,870
you're just tracking a machine learning

00:17:06,220 --> 00:17:09,819
framework its code base it's new

00:17:07,870 --> 00:17:11,650
operators those networks will change

00:17:09,819 --> 00:17:13,270
over time and those network

00:17:11,650 --> 00:17:15,370
representations will change over time so

00:17:13,270 --> 00:17:17,410
you have to keep keep chasing to catch

00:17:15,370 --> 00:17:19,419
up so moving to the point where we are

00:17:17,410 --> 00:17:22,209
we working with a common file format for

00:17:19,419 --> 00:17:23,620
all of this will be useful as well and

00:17:22,209 --> 00:17:26,809
then the other area we're not looking at

00:17:23,620 --> 00:17:30,919
is looking at the support in

00:17:26,809 --> 00:17:33,169
and armin em i should say is training so

00:17:30,919 --> 00:17:34,549
that's already supported by the overall

00:17:33,169 --> 00:17:35,659
machine learning you know machine

00:17:34,549 --> 00:17:37,580
learning frameworks that's where it

00:17:35,659 --> 00:17:39,070
always happens today one of the areas

00:17:37,580 --> 00:17:41,030
that will be interesting to look at is

00:17:39,070 --> 00:17:44,179
integrating these kinds of inference

00:17:41,030 --> 00:17:46,159
engines more closely into the machine

00:17:44,179 --> 00:17:47,990
learning frameworks such that you can

00:17:46,159 --> 00:17:49,640
use them as part of the feedback cycle

00:17:47,990 --> 00:17:51,320
when you're training your network but

00:17:49,640 --> 00:17:54,730
arm and then in and of itself isn't

00:17:51,320 --> 00:17:54,730
going to focus on on training

00:17:59,680 --> 00:18:06,620
and so just then to talk about some

00:18:04,120 --> 00:18:07,940
obvious next steps and some of the

00:18:06,620 --> 00:18:10,940
things that we hope you'll you'll start

00:18:07,940 --> 00:18:15,740
to work with us on the first are around

00:18:10,940 --> 00:18:17,890
optimizations so optimizations to

00:18:15,740 --> 00:18:20,090
advance the device interface to make

00:18:17,890 --> 00:18:22,850
interruptin inter operation between

00:18:20,090 --> 00:18:24,860
multiple devices more efficient working

00:18:22,850 --> 00:18:26,990
on zero copy semantics and how to handle

00:18:24,860 --> 00:18:29,810
the graph handoff between multiple

00:18:26,990 --> 00:18:32,980
devices adding an optimizing existing

00:18:29,810 --> 00:18:35,390
operators and adding new operators so

00:18:32,980 --> 00:18:36,830
new operators are coming along all the

00:18:35,390 --> 00:18:38,480
time new network architectures are

00:18:36,830 --> 00:18:40,160
coming along and keeping up with that

00:18:38,480 --> 00:18:41,720
across the device diverse range of

00:18:40,160 --> 00:18:43,640
markets that people are interested in is

00:18:41,720 --> 00:18:45,980
difficult so that's that's another area

00:18:43,640 --> 00:18:48,590
that needs some work and then continuing

00:18:45,980 --> 00:18:51,530
to improve the representations of

00:18:48,590 --> 00:18:53,180
networks and intermediate memories such

00:18:51,530 --> 00:18:55,430
that we get higher efficiency when we're

00:18:53,180 --> 00:18:58,400
running these networks the example there

00:18:55,430 --> 00:19:01,580
being an HWC support to get better

00:18:58,400 --> 00:19:03,350
bandwidth from main memory and so that

00:19:01,580 --> 00:19:07,250
that's on the optimization side and then

00:19:03,350 --> 00:19:09,160
in terms of new features so working with

00:19:07,250 --> 00:19:12,110
offline possible to some suitable

00:19:09,160 --> 00:19:13,940
intermediate form so one of the problems

00:19:12,110 --> 00:19:16,040
you have is if you try and optimize all

00:19:13,940 --> 00:19:17,450
the way down to your target if you don't

00:19:16,040 --> 00:19:20,720
know what your target is because you're

00:19:17,450 --> 00:19:22,370
running across multiple different end

00:19:20,720 --> 00:19:24,890
devices multiple different phones for

00:19:22,370 --> 00:19:26,900
example then you can't do that so

00:19:24,890 --> 00:19:30,140
finding better solutions to getting a

00:19:26,900 --> 00:19:31,490
more optimal pre-prepared network

00:19:30,140 --> 00:19:34,400
architecture for running on different

00:19:31,490 --> 00:19:37,820
devices is going to be important and

00:19:34,400 --> 00:19:40,340
then one one area that keeps cropping up

00:19:37,820 --> 00:19:43,400
but the solution to you hasn't been

00:19:40,340 --> 00:19:45,500
found is using just just classic

00:19:43,400 --> 00:19:46,160
compiler technology like LLVM to solve

00:19:45,500 --> 00:19:47,810
this problem

00:19:46,160 --> 00:19:50,960
rather than building on top of

00:19:47,810 --> 00:19:53,180
individual operators and then alongside

00:19:50,960 --> 00:19:56,180
those options which look at static

00:19:53,180 --> 00:19:59,210
scheduling and and improve improving the

00:19:56,180 --> 00:20:01,700
passing stages also looking at how in a

00:19:59,210 --> 00:20:03,350
heavily loaded dynamic system you better

00:20:01,700 --> 00:20:06,830
balance across all of the pieces of ip

00:20:03,350 --> 00:20:08,750
they're available when sometimes even

00:20:06,830 --> 00:20:10,640
though the best device for running a

00:20:08,750 --> 00:20:11,300
certain network is available in the

00:20:10,640 --> 00:20:12,740
platform

00:20:11,300 --> 00:20:14,720
maybe it's fully loaded with other

00:20:12,740 --> 00:20:17,000
tasks so you have to fall back to other

00:20:14,720 --> 00:20:22,130
devices and finding ways to handle that

00:20:17,000 --> 00:20:23,300
as well and then just to go into a

00:20:22,130 --> 00:20:26,390
little bit of detail on what the

00:20:23,300 --> 00:20:28,040
computer barriers so it's a set of

00:20:26,390 --> 00:20:29,570
optimized low-level functions for the

00:20:28,040 --> 00:20:32,270
CPM GPU

00:20:29,570 --> 00:20:36,350
covers both classic computer vision and

00:20:32,270 --> 00:20:38,360
machine learning functions what with the

00:20:36,350 --> 00:20:40,460
way we're using in practice is people

00:20:38,360 --> 00:20:44,090
are using a mixture of the compute

00:20:40,460 --> 00:20:46,429
library for for example hard + SVM and

00:20:44,090 --> 00:20:48,890
other initial image processing stages

00:20:46,429 --> 00:20:49,970
then combined with arm and then which

00:20:48,890 --> 00:20:51,770
cause the compute library for

00:20:49,970 --> 00:20:53,420
decomposing and managing the neural

00:20:51,770 --> 00:20:55,670
network architecture itself and so you

00:20:53,420 --> 00:20:58,130
can build up more complicated pipelines

00:20:55,670 --> 00:20:59,720
including computer vision or signal

00:20:58,130 --> 00:21:04,160
processing along with machine learning

00:20:59,720 --> 00:21:06,500
and that's also a available open source

00:21:04,160 --> 00:21:11,600
you can go to the link theater to get

00:21:06,500 --> 00:21:13,040
access to it the the important thing

00:21:11,600 --> 00:21:14,929
with a beauty like before us is it's

00:21:13,040 --> 00:21:16,340
where we consolidate all of our effort

00:21:14,929 --> 00:21:20,000
around supporting current and future

00:21:16,340 --> 00:21:23,780
architectures so we're supporting AR 32

00:21:20,000 --> 00:21:26,030
and 64 for gem and convolution of all

00:21:23,780 --> 00:21:27,620
those creation ala T's and as we move to

00:21:26,030 --> 00:21:29,720
newer architectures on the CPU and GPU

00:21:27,620 --> 00:21:34,130
are also extending support for that as

00:21:29,720 --> 00:21:36,050
well it's got support for this OpenCL

00:21:34,130 --> 00:21:37,910
auto tuner and and the intent there is

00:21:36,050 --> 00:21:39,500
that when you're running your network

00:21:37,910 --> 00:21:41,059
architecture depending on the input

00:21:39,500 --> 00:21:43,280
parameters for each of the layers you

00:21:41,059 --> 00:21:44,900
can have a very differing outputs in

00:21:43,280 --> 00:21:47,360
terms of performance that you get on the

00:21:44,900 --> 00:21:49,820
system so that's one of the you know a

00:21:47,360 --> 00:21:52,130
nascent static tool for analyzing how

00:21:49,820 --> 00:21:55,429
best to represent one stage of the

00:21:52,130 --> 00:21:56,720
network running on the GPU we will need

00:21:55,429 --> 00:21:58,610
more tools in that kind of space

00:21:56,720 --> 00:22:01,420
particularly for different architectures

00:21:58,610 --> 00:22:03,860
and different microarchitectures so

00:22:01,420 --> 00:22:06,500
something something we are working on at

00:22:03,860 --> 00:22:08,630
the moment and then alongside that so

00:22:06,500 --> 00:22:10,940
we've got FP 32 support we've also got

00:22:08,630 --> 00:22:12,260
intake quantized acceleration because

00:22:10,940 --> 00:22:14,090
for a lot of network architectures

00:22:12,260 --> 00:22:16,610
that's being that's proving to be the

00:22:14,090 --> 00:22:18,380
right route to take from optimizing all

00:22:16,610 --> 00:22:20,780
of this

00:22:18,380 --> 00:22:22,789
and then in terms of where we are with

00:22:20,780 --> 00:22:25,669
all of these this codebase so these are

00:22:22,789 --> 00:22:27,289
all open source and available now up

00:22:25,669 --> 00:22:29,390
until now these these have been pushed

00:22:27,289 --> 00:22:31,309
on quarterly cadence so every three

00:22:29,390 --> 00:22:34,299
months we make a new release with the

00:22:31,309 --> 00:22:36,650
1808 release being the latest of these

00:22:34,299 --> 00:22:39,410
as Jim mentioned this morning we're

00:22:36,650 --> 00:22:40,580
going to be moving a lot of this open to

00:22:39,410 --> 00:22:43,520
open source and collaborative

00:22:40,580 --> 00:22:45,980
development so all of these code bases

00:22:43,520 --> 00:22:48,350
will be mu sorry with the exception of

00:22:45,980 --> 00:22:50,000
Simpson and today we'll be moving over

00:22:48,350 --> 00:22:53,000
to be managed within the llanera

00:22:50,000 --> 00:22:55,159
infrastructure and we will be developing

00:22:53,000 --> 00:22:56,720
in the open having our design worker in

00:22:55,159 --> 00:23:01,640
the open so other people can see that

00:22:56,720 --> 00:23:05,570
and contribute as well and just finally

00:23:01,640 --> 00:23:07,730
to say so as JIT as Jim was saying with

00:23:05,570 --> 00:23:10,179
all of this it's all about coming

00:23:07,730 --> 00:23:12,590
together to work on this collaboratively

00:23:10,179 --> 00:23:16,010
we're doing what we can that we

00:23:12,590 --> 00:23:17,780
recognize even with the 50 hundred

00:23:16,010 --> 00:23:20,090
engineers that we're spending on the

00:23:17,780 --> 00:23:22,070
software an optimization in this space

00:23:20,090 --> 00:23:23,990
that's still not enough to solve all of

00:23:22,070 --> 00:23:26,210
the problems that people have all of the

00:23:23,990 --> 00:23:27,530
different network architectures all of

00:23:26,210 --> 00:23:30,650
the different platforms that people are

00:23:27,530 --> 00:23:32,990
running with hmm and so what we're

00:23:30,650 --> 00:23:35,480
really hoping is people who are experts

00:23:32,990 --> 00:23:37,390
in neural networks people who have

00:23:35,480 --> 00:23:40,460
experience from the past in compilers

00:23:37,390 --> 00:23:42,650
people who have worked on say

00:23:40,460 --> 00:23:44,179
heterogeneous yet heterogeneous

00:23:42,650 --> 00:23:46,610
scheduling or other problems like that

00:23:44,179 --> 00:23:48,559
in the past people who have worked on

00:23:46,610 --> 00:23:51,169
tooling for performance in other domains

00:23:48,559 --> 00:23:52,190
and people who have worked more

00:23:51,169 --> 00:23:54,700
generally with the machine learning

00:23:52,190 --> 00:23:57,409
frameworks all of those people are

00:23:54,700 --> 00:23:59,330
absolutely critical to making all of

00:23:57,409 --> 00:24:01,039
this work and through all of the

00:23:59,330 --> 00:24:02,900
different companies here we will have

00:24:01,039 --> 00:24:05,270
exposure to so many different use cases

00:24:02,900 --> 00:24:07,130
that people have today and being able to

00:24:05,270 --> 00:24:08,809
fold in all of that experience and at

00:24:07,130 --> 00:24:10,909
least the primitive operators that those

00:24:08,809 --> 00:24:12,530
different markets need is kind of where

00:24:10,909 --> 00:24:16,070
we want to get we'll get to you with all

00:24:12,530 --> 00:24:19,039
of this and so you can contribute code

00:24:16,070 --> 00:24:20,539
directly to our mannan today so you can

00:24:19,039 --> 00:24:23,539
even get it from github and start

00:24:20,539 --> 00:24:26,780
interacting with it and file issues and

00:24:23,539 --> 00:24:28,930
push features and as I say it will be

00:24:26,780 --> 00:24:30,160
hosted very soon by

00:24:28,930 --> 00:24:32,380
Lennar Oh on there Garrett

00:24:30,160 --> 00:24:34,150
infrastructure along with bug tracking

00:24:32,380 --> 00:24:36,070
and mailing list so you can even start

00:24:34,150 --> 00:24:45,340
to contribute through the Lonardo

00:24:36,070 --> 00:24:55,470
initiative as well okay thank you thank

00:24:45,340 --> 00:24:58,480
you rob any question thanks for the talk

00:24:55,470 --> 00:25:02,320
how well does the OpenCL

00:24:58,480 --> 00:25:04,000
I work with non Mali and cheap youth so

00:25:02,320 --> 00:25:05,950
they are people using like that or is it

00:25:04,000 --> 00:25:09,700
specially tuned for Mali or should it

00:25:05,950 --> 00:25:12,030
work so well so it is standard OpenCL

00:25:09,700 --> 00:25:14,620
code so it will work with other GPUs

00:25:12,030 --> 00:25:16,750
we've done a lot of micro architectural

00:25:14,620 --> 00:25:19,510
optimization to get the best performance

00:25:16,750 --> 00:25:22,540
on the Mali GPUs so I think typically

00:25:19,510 --> 00:25:23,770
what you find is the way we've optimized

00:25:22,540 --> 00:25:25,750
this we can get an order of magnitude

00:25:23,770 --> 00:25:27,970
difference in performance for individual

00:25:25,750 --> 00:25:28,840
operators on Mali and I would expect

00:25:27,970 --> 00:25:30,910
there to be very different

00:25:28,840 --> 00:25:33,280
characteristics across other GPUs

00:25:30,910 --> 00:25:36,280
so work would need to be done to improve

00:25:33,280 --> 00:25:38,680
the performance there it's an open

00:25:36,280 --> 00:25:41,170
question to me as to whether this

00:25:38,680 --> 00:25:43,330
optimization should be done in the

00:25:41,170 --> 00:25:45,640
computer berry or people already have

00:25:43,330 --> 00:25:47,530
their own accelerated primitive

00:25:45,640 --> 00:25:49,090
libraries for their GPUs all their other

00:25:47,530 --> 00:25:51,850
pieces of IP that they would prefer to

00:25:49,090 --> 00:25:53,380
use one of the many things that will be

00:25:51,850 --> 00:25:58,930
discretion discussing as part of the

00:25:53,380 --> 00:26:03,840
initiative we have time for one more

00:25:58,930 --> 00:26:03,840
question anybody else

00:26:08,529 --> 00:26:15,580
sorry a second question is there a clear

00:26:12,889 --> 00:26:18,559
performance difference between the

00:26:15,580 --> 00:26:22,309
Android stack where you stand roid one

00:26:18,559 --> 00:26:27,649
or where you directly go to the to the

00:26:22,309 --> 00:26:29,240
or mmmm so well so on this on the CPU

00:26:27,649 --> 00:26:30,830
side note because we've been working so

00:26:29,240 --> 00:26:31,970
closely with Google to make sure that

00:26:30,830 --> 00:26:34,429
gap doesn't exist

00:26:31,970 --> 00:26:35,990
and as we're optimizing we're pushing

00:26:34,429 --> 00:26:37,360
those changes through to the CPU

00:26:35,990 --> 00:26:39,320
back-end

00:26:37,360 --> 00:26:40,759
because of the bind raised interface

00:26:39,320 --> 00:26:43,730
there can be some overheads in going

00:26:40,759 --> 00:26:46,610
through that full stack down to save the

00:26:43,730 --> 00:26:49,129
GPU or MPU but we're continuing to work

00:26:46,610 --> 00:26:51,889
on that to reduce those overheads at the

00:26:49,129 --> 00:26:53,840
moment it's it's it's a concern but it's

00:26:51,889 --> 00:26:58,129
it's not a significant concern so maybe

00:26:53,840 --> 00:27:02,019
five percent approximately thank you rob

00:26:58,129 --> 00:27:02,019
excellent thank you thank you

00:27:03,000 --> 00:27:08,009

YouTube URL: https://www.youtube.com/watch?v=te-rJ5BVrtw


