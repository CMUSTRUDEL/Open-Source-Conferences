Title: YVR18-332 TVM compiler stack and ONNX support
Publication date: 2018-09-28
Playlist: Linaro Connect Vancouver 2018
Description: 
	As an open source deep learning compiler driven by the community, TVM is evolving quickly and well received by the industry. In this session, the architecture of the TVM stack will be introduced first, including some important features added recently such as AutoTVM and VTA (Versatile Tensor Accelerator) support. Then the build and deployment of deep learning models with TVM will be talked about, and ONNX (Open Neural Network eXchange format) is one of the model formats supported by TVM stack. Besides unified model format and operator definitions, ONNXIFI (ONNX Interface for Framework Integration) is another initiative from the ONNX community to define a cross-platform API, and how to fit TVM stack into ONNXIFI seems an interesting topic to discuss as well.
Captions: 
	00:00:02,110 --> 00:00:07,120
[Music]

00:00:08,050 --> 00:00:16,369
so hi everyone so I'm glad to be here so

00:00:13,309 --> 00:00:21,230
today I will do some introduction about

00:00:16,369 --> 00:00:22,130
the TVM kampala stack and how we can

00:00:21,230 --> 00:00:28,550
support Onix

00:00:22,130 --> 00:00:31,550
by GBM so I think a lot of people know

00:00:28,550 --> 00:00:33,530
TVM but I want to do some brief

00:00:31,550 --> 00:00:37,550
introduction here as well

00:00:33,530 --> 00:00:41,990
so originally tvam referred to that

00:00:37,550 --> 00:00:46,070
cancer is back and nm VM is a grass fire

00:00:41,990 --> 00:00:48,620
stack but recently the nvm the pottery

00:00:46,070 --> 00:00:54,550
in the github has been merged into the

00:00:48,620 --> 00:00:57,410
TVM little tree so TVM referred to a

00:00:54,550 --> 00:00:59,960
complete deep learning stack from the

00:00:57,410 --> 00:01:04,420
class level down to the cancer level now

00:00:59,960 --> 00:01:07,880
I think so TVM is a community project

00:01:04,420 --> 00:01:11,120
led by ten children from University of

00:01:07,880 --> 00:01:13,220
Washington and it there are a lot of

00:01:11,120 --> 00:01:14,360
contributions from companies like

00:01:13,220 --> 00:01:19,640
amazing

00:01:14,360 --> 00:01:21,190
WOWY entity lab mid mind and a bunch of

00:01:19,640 --> 00:01:23,570
university an organization like

00:01:21,190 --> 00:01:25,580
University of Washington Shanghai

00:01:23,570 --> 00:01:29,810
Jiaotong University Berkeley University

00:01:25,580 --> 00:01:35,150
Hong Kong UFT and some other one other

00:01:29,810 --> 00:01:40,400
aging such can such like a patch yeah so

00:01:35,150 --> 00:01:45,500
a mixed net version del upon nine fully

00:01:40,400 --> 00:01:49,100
adopted and nvm TVM framework so and we

00:01:45,500 --> 00:01:55,940
can see how it is used in a mixed net

00:01:49,100 --> 00:02:01,310
right in in the MX net so tree as a

00:01:55,940 --> 00:02:04,400
community project TVM is we are received

00:02:01,310 --> 00:02:08,209
by the community we can see that they

00:02:04,400 --> 00:02:10,319
are already already around 2000 stars in

00:02:08,209 --> 00:02:14,430
the github

00:02:10,319 --> 00:02:16,920
and a lot of contributors around a 100

00:02:14,430 --> 00:02:18,719
in the study and maybe the data is a

00:02:16,920 --> 00:02:21,870
little over they because and I prepare

00:02:18,719 --> 00:02:29,389
that a fly that maybe one week ago so it

00:02:21,870 --> 00:02:34,590
should be more okay so this diagram is

00:02:29,389 --> 00:02:36,569
about a GVM architecture so for deep

00:02:34,590 --> 00:02:39,209
learning we need to support different

00:02:36,569 --> 00:02:42,540
kinds of hardware's different

00:02:39,209 --> 00:02:46,700
heterogeneous accelerators here you can

00:02:42,540 --> 00:02:50,340
see that TVM can support a lot of

00:02:46,700 --> 00:02:52,829
hardware CPU GPU FPGA

00:02:50,340 --> 00:02:56,959
and there is some plan to enable ASIC

00:02:52,829 --> 00:03:00,030
support and there is an simulator for

00:02:56,959 --> 00:03:03,709
verify your hardware design so it is

00:03:00,030 --> 00:03:06,689
very cool and design accorded evita

00:03:03,709 --> 00:03:09,049
accelerate a Coppins open accelerator

00:03:06,689 --> 00:03:13,340
stack which I will have a page for

00:03:09,049 --> 00:03:19,259
introduction about this part so from OS

00:03:13,340 --> 00:03:23,879
part so you can use Windows Linux Mac OS

00:03:19,259 --> 00:03:25,650
for the host OS for the TVM development

00:03:23,879 --> 00:03:30,709
and for deployment

00:03:25,650 --> 00:03:37,939
you can we can support Android iOS and

00:03:30,709 --> 00:03:43,259
Linux right so moving up this is what we

00:03:37,939 --> 00:03:47,790
support in GBM right from up level we

00:03:43,259 --> 00:03:52,500
stuff called a various front-end MX net

00:03:47,790 --> 00:03:54,989
right onyx Kara's Auto ml tensorflow

00:03:52,500 --> 00:04:00,169
the last one is duck duck net which is

00:03:54,989 --> 00:04:05,939
recently added as a front end of the TVM

00:04:00,169 --> 00:04:10,439
crop support so move it down so the eye

00:04:05,939 --> 00:04:12,150
and grass I are called an nvm so an nvme

00:04:10,439 --> 00:04:16,019
is a computational class ir

00:04:12,150 --> 00:04:18,840
implementation so and then a bunch of

00:04:16,019 --> 00:04:20,789
grass optimization can be done in the

00:04:18,840 --> 00:04:22,290
cross level just at Tom just mentioned

00:04:20,789 --> 00:04:25,680
before in

00:04:22,290 --> 00:04:27,960
yeah so for example you can do some

00:04:25,680 --> 00:04:30,330
memory playing you can do some operate

00:04:27,960 --> 00:04:34,560
fusion you can do some data layout

00:04:30,330 --> 00:04:38,870
transformation and then the crops after

00:04:34,560 --> 00:04:41,100
the cross optimization you can do some

00:04:38,870 --> 00:04:43,950
tensile level or pretty level

00:04:41,100 --> 00:04:47,340
optimization scheduling right so there

00:04:43,950 --> 00:04:51,870
is a tensile expiration language defined

00:04:47,340 --> 00:04:55,710
in TVM implementation which is based on

00:04:51,870 --> 00:04:59,600
the hardy de ir which is also a very

00:04:55,710 --> 00:05:05,550
popular framework for image processing

00:04:59,600 --> 00:05:07,880
so and here we have a definition bout

00:05:05,550 --> 00:05:11,130
stereo stereo is some are

00:05:07,880 --> 00:05:14,520
transformations about the computation so

00:05:11,130 --> 00:05:19,920
a lot of a stereo primitive is defining

00:05:14,520 --> 00:05:22,560
TVM and we can do optimizations in the

00:05:19,920 --> 00:05:25,560
stereo level right for example the loop

00:05:22,560 --> 00:05:28,800
transformations the thread bindings the

00:05:25,560 --> 00:05:31,890
cache locality and a specific for GPU

00:05:28,800 --> 00:05:34,410
acceleration there is a sled cooperation

00:05:31,890 --> 00:05:37,740
controller ization and as such kind of a

00:05:34,410 --> 00:05:40,140
stereo primitive support so and another

00:05:37,740 --> 00:05:43,380
new feature added recently to the step

00:05:40,140 --> 00:05:47,120
in the auto t vm I think Tom also

00:05:43,380 --> 00:05:47,120
mentioned it a bit chatted before

00:05:49,439 --> 00:05:54,669
okay here I want to talk a little bit

00:05:52,509 --> 00:05:58,479
about as a diploma Canada miss applauded

00:05:54,669 --> 00:06:02,560
by TVM it is our PC remote deployment

00:05:58,479 --> 00:06:06,789
mechanism so you can't and compile the

00:06:02,560 --> 00:06:09,430
model on your host device or even in the

00:06:06,789 --> 00:06:11,110
cloud style right so it's just and you

00:06:09,430 --> 00:06:13,900
need adjust and the network access to

00:06:11,110 --> 00:06:16,599
access the target device and in device

00:06:13,900 --> 00:06:20,169
ID is there is a long time right you

00:06:16,599 --> 00:06:23,550
just along the output of the compile

00:06:20,169 --> 00:06:27,690
process we can see on the left that we

00:06:23,550 --> 00:06:30,370
the demo apply some code right way we

00:06:27,690 --> 00:06:33,159
imported onyx model right which is

00:06:30,370 --> 00:06:36,430
trained already from different

00:06:33,159 --> 00:06:39,250
frameworks right so we import a

00:06:36,430 --> 00:06:44,169
framework and transform it to the nvm

00:06:39,250 --> 00:06:47,259
graphs and call the LLVM compiler build

00:06:44,169 --> 00:06:52,800
to generate the execution level graph

00:06:47,259 --> 00:06:57,759
and the library the library actuary is

00:06:52,800 --> 00:07:00,909
some TVM modules TVM modules it contains

00:06:57,759 --> 00:07:06,009
some packed functions for calling in the

00:07:00,909 --> 00:07:08,469
runtime side on the target device so for

00:07:06,009 --> 00:07:10,990
the exit cash for the execution graph it

00:07:08,469 --> 00:07:15,490
has in JSON format right so here I just

00:07:10,990 --> 00:07:19,229
a small diagram here for the Jason

00:07:15,490 --> 00:07:22,449
specification so note input up input

00:07:19,229 --> 00:07:24,310
arguments all calls are defined right so

00:07:22,449 --> 00:07:26,740
in the node definition there should be

00:07:24,310 --> 00:07:30,099
operate imported tributes such kind of

00:07:26,740 --> 00:07:33,339
things so on the target device with

00:07:30,099 --> 00:07:36,789
output from the GBM compile process we

00:07:33,339 --> 00:07:40,479
can create a graph runtime from the

00:07:36,789 --> 00:07:44,759
execution graph the libraries and

00:07:40,479 --> 00:07:47,110
different contexts this context can be

00:07:44,759 --> 00:07:51,639
specific for different hardware for

00:07:47,110 --> 00:07:54,490
example CPU contacts right with VM it

00:07:51,639 --> 00:07:58,389
can be an open serial contact using

00:07:54,490 --> 00:08:02,020
Somali GPU it can be an FPGA contact

00:07:58,389 --> 00:08:03,249
using pink you hardware ISO yeah and

00:08:02,020 --> 00:08:05,960
then you can

00:08:03,249 --> 00:08:08,509
important important to specify imports

00:08:05,960 --> 00:08:11,569
to the longtime and a call the lung

00:08:08,509 --> 00:08:14,689
function to execute the prediction with

00:08:11,569 --> 00:08:21,469
their new data you get on the device

00:08:14,689 --> 00:08:22,340
side so I think the TVM runtime is good

00:08:21,469 --> 00:08:24,560
at that

00:08:22,340 --> 00:08:27,289
to reduce them up there binary size

00:08:24,560 --> 00:08:31,819
right and and reduce some runtime

00:08:27,289 --> 00:08:33,620
overhead for the graph model conversion

00:08:31,819 --> 00:08:36,560
such kind of things right I think it is

00:08:33,620 --> 00:08:43,130
very critical for devices with limited

00:08:36,560 --> 00:08:45,260
resources okay so here I will do some

00:08:43,130 --> 00:08:48,740
instruction about it Auto TV M which is

00:08:45,260 --> 00:08:51,490
on automated optimizations normally in

00:08:48,740 --> 00:08:54,579
the planning world we are using an

00:08:51,490 --> 00:08:58,639
handed optimization right so but it is

00:08:54,579 --> 00:08:59,660
it it required a lot of effort right and

00:08:58,639 --> 00:09:02,209
you needed to do a lot lot of

00:08:59,660 --> 00:09:05,329
experiments to find the best results so

00:09:02,209 --> 00:09:08,390
the TVM community community introduced

00:09:05,329 --> 00:09:11,600
the automatic way to use optimizations

00:09:08,390 --> 00:09:15,320
which can reduce a lot of a tedious work

00:09:11,600 --> 00:09:17,329
for developers right so machine learning

00:09:15,320 --> 00:09:20,089
based framework is designed to do the

00:09:17,329 --> 00:09:23,390
automatic automated optimization with an

00:09:20,089 --> 00:09:25,160
the main specific cost model so any

00:09:23,390 --> 00:09:27,709
chance for transfer lending can be used

00:09:25,160 --> 00:09:32,269
that she accelerates of optimizations

00:09:27,709 --> 00:09:34,760
for great workloads with a lot of change

00:09:32,269 --> 00:09:36,680
operators so to do the automated

00:09:34,760 --> 00:09:38,360
optimization there are many two steps

00:09:36,680 --> 00:09:42,350
one step is defining a search space

00:09:38,360 --> 00:09:45,560
right and then you can learn a search

00:09:42,350 --> 00:09:48,620
algorithm to explore the space to find

00:09:45,560 --> 00:09:51,110
the best config currently four kinds of

00:09:48,620 --> 00:09:53,899
tuners are supported already in in T VM

00:09:51,110 --> 00:09:56,779
running the random tuner in random order

00:09:53,899 --> 00:10:00,490
right create a search in Krita such

00:09:56,779 --> 00:10:02,060
order by GA tuner which is a genetic

00:10:00,490 --> 00:10:05,660
algorithm using

00:10:02,060 --> 00:10:09,019
okay and the most advanced to one here

00:10:05,660 --> 00:10:13,220
or with better performance in the tuners

00:10:09,019 --> 00:10:14,380
and isolated xgb tuner which are extreme

00:10:13,220 --> 00:10:17,800
gradient poor

00:10:14,380 --> 00:10:22,000
ting algorithm so I think it's also a

00:10:17,800 --> 00:10:25,990
pro open-source project from Eng I think

00:10:22,000 --> 00:10:28,360
if I remember correctly and with this

00:10:25,990 --> 00:10:30,910
automated optimization mechanism or the

00:10:28,360 --> 00:10:32,800
listing and hand optimization can be

00:10:30,910 --> 00:10:35,350
part of the search space so your

00:10:32,800 --> 00:10:37,990
previous work still can be leveraged to

00:10:35,350 --> 00:10:44,080
make sure that the optimization effort

00:10:37,990 --> 00:10:46,630
can be still used here is some data from

00:10:44,080 --> 00:10:49,150
the TVM community right so there is a

00:10:46,630 --> 00:10:52,060
paper called the title will learning to

00:10:49,150 --> 00:10:54,850
optimize tensor programs if you have

00:10:52,060 --> 00:10:58,630
injured you can read it at paper right

00:10:54,850 --> 00:11:00,130
so on the left it is and some comparison

00:10:58,630 --> 00:11:02,470
compare them between tens of row light

00:11:00,130 --> 00:11:06,730
and the auto TVM we can see that is a

00:11:02,470 --> 00:11:09,820
great speed up for the performance and

00:11:06,730 --> 00:11:12,730
on the right there is some comparison

00:11:09,820 --> 00:11:18,580
between computer library and auto TVN

00:11:12,730 --> 00:11:22,060
here version 18.3 is used for um

00:11:18,580 --> 00:11:24,430
computer library and I think we recent

00:11:22,060 --> 00:11:26,230
and rushing done computer liability I

00:11:24,430 --> 00:11:27,820
should have it they should have a better

00:11:26,230 --> 00:11:30,220
performance but this is just the first

00:11:27,820 --> 00:11:33,430
time our comparison so what we can also

00:11:30,220 --> 00:11:37,210
see is that auto TVM can provide some

00:11:33,430 --> 00:11:42,370
performance gain as using a computer

00:11:37,210 --> 00:11:45,490
library as a base baseline okay so this

00:11:42,370 --> 00:11:51,010
is a vita open accelerator step I think

00:11:45,490 --> 00:11:53,350
it is very helpful for people doing a

00:11:51,010 --> 00:11:55,900
holiday sign right if if they want to

00:11:53,350 --> 00:11:58,930
design some deep learning accelerator so

00:11:55,900 --> 00:12:01,870
for the Vita open accelerators that it

00:11:58,930 --> 00:12:04,240
is integrated with DBMS deck there will

00:12:01,870 --> 00:12:06,220
so there are major three parts one part

00:12:04,240 --> 00:12:08,620
a the beat meat heart just run time

00:12:06,220 --> 00:12:12,910
right use the up level

00:12:08,620 --> 00:12:16,210
stereo can cause the runtime to do to do

00:12:12,910 --> 00:12:19,240
the computation right nothing is the

00:12:16,210 --> 00:12:25,170
Vita I saw it defined several

00:12:19,240 --> 00:12:25,170
instructions for to do

00:12:25,720 --> 00:12:31,390
deep learning acceleration right so we

00:12:29,650 --> 00:12:34,810
can see that the Vita McMichael

00:12:31,390 --> 00:12:37,120
architecture there are some instruction

00:12:34,810 --> 00:12:39,580
fetch operation memory load and a memory

00:12:37,120 --> 00:12:41,260
store and on the core of the diagram

00:12:39,580 --> 00:12:44,800
because is that they either compute

00:12:41,260 --> 00:12:48,490
module which support a ru operations and

00:12:44,800 --> 00:12:54,490
the gem operations right so and as this

00:12:48,490 --> 00:12:58,120
this after the microarchitecture we can

00:12:54,490 --> 00:13:00,010
deploy it on the edge FPGA for example

00:12:58,120 --> 00:13:02,560
the pinko fpga from psych starting

00:13:00,010 --> 00:13:07,720
supporting it now and there's some plan

00:13:02,560 --> 00:13:11,710
to deploy it to the cloud FPGA instances

00:13:07,720 --> 00:13:15,820
from amazing and we can also deploy it

00:13:11,710 --> 00:13:22,480
to the simulator ok the whole process is

00:13:15,820 --> 00:13:26,440
that and we can use TVM to do schedule

00:13:22,480 --> 00:13:32,140
optimization and deploy it remotely by a

00:13:26,440 --> 00:13:34,780
VPS RPC and on the target device the RPC

00:13:32,140 --> 00:13:39,460
server can call in to the jet wrong time

00:13:34,780 --> 00:13:42,100
and the provide the vita stream on to

00:13:39,460 --> 00:13:45,339
the FPGA device i think it is very

00:13:42,100 --> 00:13:51,880
helpful to verify somehow a design and

00:13:45,339 --> 00:13:54,640
to do some early identification and and

00:13:51,880 --> 00:13:57,790
there is also one paper from the TVM

00:13:54,640 --> 00:14:03,730
community if you have interests you can

00:13:57,790 --> 00:14:09,730
also read about it so here we can see on

00:14:03,730 --> 00:14:11,290
the table the TVM public road map so for

00:14:09,730 --> 00:14:15,150
example there is some plan to enable

00:14:11,290 --> 00:14:19,650
8-bit support and even some lower bit

00:14:15,150 --> 00:14:23,410
precision support for bit even one bit

00:14:19,650 --> 00:14:27,459
and we very also some plan to enable

00:14:23,410 --> 00:14:29,980
graph level auto GBM or automated

00:14:27,459 --> 00:14:33,430
attuning so currently although TVM is

00:14:29,980 --> 00:14:37,690
just a 420 level so if you want to do it

00:14:33,430 --> 00:14:38,200
on a graph level right so there's also

00:14:37,690 --> 00:14:42,610
some time

00:14:38,200 --> 00:14:46,390
this part for beta there's some plan to

00:14:42,610 --> 00:14:51,700
enable the ultra nine sticks which is an

00:14:46,390 --> 00:14:56,410
which is an FPGA 96 port from Xilinx

00:14:51,700 --> 00:14:59,050
with as a lynx dinkle SOC and as they

00:14:56,410 --> 00:15:03,550
add some plan to enable amazing f1

00:14:59,050 --> 00:15:06,760
instance just as I mentioned so another

00:15:03,550 --> 00:15:09,850
in the important part for next algorithm

00:15:06,760 --> 00:15:14,260
is a high level III improvements

00:15:09,850 --> 00:15:16,900
previously we have mm VM so nvm v2 is in

00:15:14,260 --> 00:15:20,560
the plan it will called a relay so there

00:15:16,900 --> 00:15:25,000
is some discussions about it so because

00:15:20,560 --> 00:15:27,940
and with industry involvement we need to

00:15:25,000 --> 00:15:31,180
choose a pod control flows right and

00:15:27,940 --> 00:15:36,850
that improve the type systems right how

00:15:31,180 --> 00:15:40,300
to make the control make it a long time

00:15:36,850 --> 00:15:41,440
and a compile server system looked

00:15:40,300 --> 00:15:43,390
better

00:15:41,440 --> 00:15:51,430
this is also one important consideration

00:15:43,390 --> 00:15:55,810
about this new rule a design yeah okay

00:15:51,430 --> 00:15:59,320
for photonics I think and and it is also

00:15:55,810 --> 00:16:01,840
mentioned a lot in the previous session

00:15:59,320 --> 00:16:04,630
so I would just go through it quickly so

00:16:01,840 --> 00:16:07,900
onyx defines actually three major parts

00:16:04,630 --> 00:16:09,090
one part their computation graph model

00:16:07,900 --> 00:16:13,230
and other is seeing as that building

00:16:09,090 --> 00:16:15,370
operators and some standard database and

00:16:13,230 --> 00:16:18,880
but it is a phone you know dental work

00:16:15,370 --> 00:16:21,190
for onyx ml it is an crestcom machine

00:16:18,880 --> 00:16:28,230
learning extension not necessarily open

00:16:21,190 --> 00:16:31,300
neural networks so how to support

00:16:28,230 --> 00:16:34,240
explorers in GBM right only more onyx

00:16:31,300 --> 00:16:38,350
model defines the graph define the nodes

00:16:34,240 --> 00:16:41,110
we needed to import and do some offline

00:16:38,350 --> 00:16:45,460
conversion to the nm VM symbolic graph

00:16:41,110 --> 00:16:49,089
so there is an onyx front and

00:16:45,460 --> 00:16:54,489
implemented in DVM to support onyx model

00:16:49,089 --> 00:16:58,809
so I think there are some angles or

00:16:54,489 --> 00:17:01,509
factors were needed to think about to

00:16:58,809 --> 00:17:03,999
think about to support onyx model in GBM

00:17:01,509 --> 00:17:05,730
I think one one part one level is

00:17:03,999 --> 00:17:09,279
operator level right

00:17:05,730 --> 00:17:12,309
onyx already support defined a lot of

00:17:09,279 --> 00:17:15,220
operators so TVM there is also some

00:17:12,309 --> 00:17:18,939
definition or implementation so we

00:17:15,220 --> 00:17:21,159
probably need to think about the operate

00:17:18,939 --> 00:17:23,649
definitions well what operators are

00:17:21,159 --> 00:17:27,879
supporting onyx what our support by TVM

00:17:23,649 --> 00:17:30,730
so shall we support order that operator

00:17:27,879 --> 00:17:33,269
define about onyx in GBM this is a

00:17:30,730 --> 00:17:36,999
question to to to to to investigate and

00:17:33,269 --> 00:17:39,009
nothing is a model accuracy and

00:17:36,999 --> 00:17:41,200
efficiency for the model conversion

00:17:39,009 --> 00:17:43,450
right because some people don't open

00:17:41,200 --> 00:17:47,139
that and they'll run into a lot of

00:17:43,450 --> 00:17:54,519
problems before for convertor onyx model

00:17:47,139 --> 00:17:58,240
t vm graph so this is probably another

00:17:54,519 --> 00:18:00,399
thing we needed to check right yeah so

00:17:58,240 --> 00:18:02,590
so because and the offline conversion

00:18:00,399 --> 00:18:07,570
right there is some maybe mismatch or

00:18:02,590 --> 00:18:09,100
some point missed so yeah and we we can

00:18:07,570 --> 00:18:13,629
also think about the graph level

00:18:09,100 --> 00:18:16,299
optimization so for onyx there is onyx

00:18:13,629 --> 00:18:21,460
optimizer defined or implemented for

00:18:16,299 --> 00:18:23,889
some common level optimizations and in t

00:18:21,460 --> 00:18:28,090
vm side there is also some graph level

00:18:23,889 --> 00:18:31,269
optimizations implemented in vm so we

00:18:28,090 --> 00:18:34,779
can progress think about how how the

00:18:31,269 --> 00:18:39,129
graph level optimization can be handled

00:18:34,779 --> 00:18:42,159
shall we more did or we still should

00:18:39,129 --> 00:18:45,519
have some pass through pass through

00:18:42,159 --> 00:18:50,499
measure to just use the tvam cross

00:18:45,519 --> 00:18:52,990
optimization directory so somebody may

00:18:50,499 --> 00:18:54,820
ask why not use UNIX directory as a

00:18:52,990 --> 00:18:57,039
graph I are in a VM

00:18:54,820 --> 00:18:59,379
right because and they are pretty

00:18:57,039 --> 00:19:02,230
similar right so just they are both an

00:18:59,379 --> 00:19:05,290
compute computational graph I are

00:19:02,230 --> 00:19:08,800
so yeah it's good question I think so

00:19:05,290 --> 00:19:11,800
and there is also already there is also

00:19:08,800 --> 00:19:16,030
some discussion in the community about

00:19:11,800 --> 00:19:20,530
this mm-hmm it looks like the essay is

00:19:16,030 --> 00:19:25,600
still some limitations of onyx for a TV

00:19:20,530 --> 00:19:28,540
em so they still want to use relay at

00:19:25,600 --> 00:19:30,850
this moment so I think this is one thing

00:19:28,540 --> 00:19:34,240
we probably the only community and the

00:19:30,850 --> 00:19:36,550
TVM community can work together right

00:19:34,240 --> 00:19:40,750
for for the merging in the future I

00:19:36,550 --> 00:19:43,680
think otherwise and that we needed to do

00:19:40,750 --> 00:19:46,660
we still needed to do the graph our

00:19:43,680 --> 00:19:49,060
model conversion right it is not so

00:19:46,660 --> 00:19:51,520
efficient I think because you it will

00:19:49,060 --> 00:19:54,280
require some overhead at the wrong time

00:19:51,520 --> 00:19:57,720
right so yeah I think they say the one

00:19:54,280 --> 00:19:57,720
direction we can probably think about

00:19:58,770 --> 00:20:04,660
okay another thing I want to introduce

00:20:01,750 --> 00:20:06,100
is onyx V so onyx TV is the onyx

00:20:04,660 --> 00:20:10,030
interface for framework integration

00:20:06,100 --> 00:20:13,360
which is released in latest onyx version

00:20:10,030 --> 00:20:15,340
should be version 1.0 I think so

00:20:13,360 --> 00:20:17,230
onyx TV is a standard interface for

00:20:15,340 --> 00:20:19,480
neural network interface inference on

00:20:17,230 --> 00:20:21,520
different accelerators so it's the pod

00:20:19,480 --> 00:20:24,340
runtime discovery and a selection of

00:20:21,520 --> 00:20:27,070
extrication backends and for each

00:20:24,340 --> 00:20:32,020
back-end we can also do some longtime

00:20:27,070 --> 00:20:34,930
discovery of onyx operators and it can

00:20:32,020 --> 00:20:39,130
support otix model format and online

00:20:34,930 --> 00:20:41,440
model conversion for own expect and it

00:20:39,130 --> 00:20:44,680
is a combination with software layer and

00:20:41,440 --> 00:20:47,440
harbor device too long and onyx cross so

00:20:44,680 --> 00:20:49,690
the same software later can expose

00:20:47,440 --> 00:20:53,410
multiple back-end for example CP back

00:20:49,690 --> 00:20:54,720
end GPU back and DSP back end and there

00:20:53,410 --> 00:20:58,480
is also one

00:20:54,720 --> 00:21:02,410
satella GP has type a back-end define

00:20:58,480 --> 00:21:04,180
because maybe the backend want to handle

00:21:02,410 --> 00:21:08,050
the scheduling or dispatching to

00:21:04,180 --> 00:21:10,630
different devices by itself right so in

00:21:08,050 --> 00:21:13,650
this case you can use the hazard unit

00:21:10,630 --> 00:21:16,650
type so that you can do your own

00:21:13,650 --> 00:21:19,530
CPU platform level optimization

00:21:16,650 --> 00:21:25,680
including CPU GPU DSP MA and maybe MP

00:21:19,530 --> 00:21:27,780
you so so with onyx onyx TV and solid so

00:21:25,680 --> 00:21:29,820
we had clears of API that defined one

00:21:27,780 --> 00:21:32,220
type in for a one click through rates

00:21:29,820 --> 00:21:35,550
for the back end right get a back end

00:21:32,220 --> 00:21:37,050
IDE get a back end information right to

00:21:35,550 --> 00:21:38,790
do the initialization sometimes things

00:21:37,050 --> 00:21:40,530
and I think yes event

00:21:38,790 --> 00:21:43,440
eventually the you that to do the

00:21:40,530 --> 00:21:45,929
synchronization and an another

00:21:43,440 --> 00:21:48,900
categories is the cross right we need to

00:21:45,929 --> 00:21:54,090
you need to graph price set across I oh

00:21:48,900 --> 00:21:57,210
and along the grass okay so this is the

00:21:54,090 --> 00:22:01,170
diagram I just draw for discussion I

00:21:57,210 --> 00:22:03,929
think currently there is one glow back

00:22:01,170 --> 00:22:04,650
and implemented by Facebook already for

00:22:03,929 --> 00:22:06,450
onyx a fee

00:22:04,650 --> 00:22:09,210
I think it can be used at a cooler

00:22:06,450 --> 00:22:14,460
reference how we can support only recipe

00:22:09,210 --> 00:22:18,780
and integrate with uplevel applications

00:22:14,460 --> 00:22:22,020
and a playable frameworks right so here

00:22:18,780 --> 00:22:24,450
for other back-end I think it should be

00:22:22,020 --> 00:22:26,520
still open for discussion how we can

00:22:24,450 --> 00:22:31,110
support Alexa fee right

00:22:26,520 --> 00:22:34,740
probably we can add an TVM back-end

00:22:31,110 --> 00:22:37,290
using a lip onyx IPTV em library we can

00:22:34,740 --> 00:22:40,770
add and snappy backend we can add an

00:22:37,290 --> 00:22:43,260
onion back-end we can add and ten starts

00:22:40,770 --> 00:22:45,360
back-end I think it should be part of

00:22:43,260 --> 00:22:48,330
the future work and the community should

00:22:45,360 --> 00:22:52,490
think about how we can support it better

00:22:48,330 --> 00:22:55,980
to make it unify unified standard

00:22:52,490 --> 00:22:59,940
interface for all the ecosystem I think

00:22:55,980 --> 00:23:02,610
we as the ecosystem this is a very

00:22:59,940 --> 00:23:04,020
important thing to consider because each

00:23:02,610 --> 00:23:07,280
vendor may have their own specific

00:23:04,020 --> 00:23:11,700
requirements or they have some specific

00:23:07,280 --> 00:23:15,270
implementation and how to fit it into

00:23:11,700 --> 00:23:19,350
the diagram I think it is a topic we can

00:23:15,270 --> 00:23:23,340
discuss so specifically for TBM right so

00:23:19,350 --> 00:23:27,330
a TV em and for TV and RPC deploy right

00:23:23,340 --> 00:23:29,820
so we do the a ot ahead of time compiler

00:23:27,330 --> 00:23:33,899
right so how the compiler and the

00:23:29,820 --> 00:23:38,460
runtime mechanism can be supported by so

00:23:33,899 --> 00:23:41,700
you just start this mechanism right so I

00:23:38,460 --> 00:23:43,019
think this is and I still have no good

00:23:41,700 --> 00:23:45,830
idea about this part yet

00:23:43,019 --> 00:23:52,889
so if somebody have idea about this

00:23:45,830 --> 00:23:56,090
welcome for discussion ok I think that's

00:23:52,889 --> 00:24:01,399
all thanks everybody

00:23:56,090 --> 00:24:10,190
[Applause]

00:24:01,399 --> 00:24:13,639
any question for Jami in the auto TVM

00:24:10,190 --> 00:24:16,909
slide you mentioned that you could

00:24:13,639 --> 00:24:19,830
incorporate one of the hand-coded

00:24:16,909 --> 00:24:20,190
implementations as part of the search

00:24:19,830 --> 00:24:23,749
space

00:24:20,190 --> 00:24:26,489
what's the representation for that

00:24:23,749 --> 00:24:28,499
hand-coded rep implementation okay

00:24:26,489 --> 00:24:32,190
for example if you want to do the tiling

00:24:28,499 --> 00:24:34,940
right and you already have a hex hand

00:24:32,190 --> 00:24:39,929
optimization wheeze with an Pro for

00:24:34,940 --> 00:24:44,009
multiply 8 right so you can specify area

00:24:39,929 --> 00:24:46,379
to the list of a search space right for

00:24:44,009 --> 00:24:51,859
the such space so that that still

00:24:46,379 --> 00:24:51,859
represented in TVM yes yes yes yes

00:25:01,000 --> 00:25:09,059
so I'll ask a question so onyx initially

00:25:05,830 --> 00:25:12,669
we we understood that onyx is a

00:25:09,059 --> 00:25:16,630
description format and onyx if I is

00:25:12,669 --> 00:25:21,820
brother runtime api so does it mean that

00:25:16,630 --> 00:25:25,980
onyx is creating two kind of two

00:25:21,820 --> 00:25:30,220
different sub projects yeah and from our

00:25:25,980 --> 00:25:33,880
my understanding yes so the own X equal

00:25:30,220 --> 00:25:36,280
system is expanding not just the open

00:25:33,880 --> 00:25:39,010
exchange format right so because and

00:25:36,280 --> 00:25:41,020
they're asked to do some not not just as

00:25:39,010 --> 00:25:45,340
a model format right we still need to

00:25:41,020 --> 00:25:48,400
unify the API right so that uplevel

00:25:45,340 --> 00:25:51,760
applications can call the single

00:25:48,400 --> 00:25:55,600
standard api to do the runtime inference

00:25:51,760 --> 00:25:57,940
I think this is an also this it will

00:25:55,600 --> 00:26:02,080
also be very helpful for the developers

00:25:57,940 --> 00:26:06,909
in this area yeah thank you very much

00:26:02,080 --> 00:26:09,730
any any other question for Jami okay

00:26:06,909 --> 00:26:12,880
then I think we can pause for lunch and

00:26:09,730 --> 00:26:13,660
we will restart it - Thank You Jaime

00:26:12,880 --> 00:26:17,560
thank you

00:26:13,660 --> 00:26:22,569
[Applause]

00:26:17,560 --> 00:26:22,569

YouTube URL: https://www.youtube.com/watch?v=daYr4tpncFo


