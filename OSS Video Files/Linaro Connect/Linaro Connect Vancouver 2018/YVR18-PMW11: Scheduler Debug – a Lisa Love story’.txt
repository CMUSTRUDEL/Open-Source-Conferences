Title: YVR18-PMW11: Scheduler Debug – a Lisa Love story’
Publication date: 2018-10-04
Playlist: Linaro Connect Vancouver 2018
Description: 
	- Brief introduction to LISA (https://github.com/ARM-software/lisa)
- Using LISA for system analysis
  - For writing patches (misfit patch-set)
  - For platform tuning
- Testing with LISA
  - Scheduler "unit-tests"
  - Android power/perf testing (wltests)
Captions: 
	00:00:02,120 --> 00:00:07,130
[Music]

00:00:09,140 --> 00:00:15,360
hello everyone hi everyone I'm Val

00:00:13,530 --> 00:00:17,190
Lewton Schneider I'm working at armed

00:00:15,360 --> 00:00:19,560
and they'll be talking about Lisa today

00:00:17,190 --> 00:00:23,250
and so will Steven Yahoo who's hiding in

00:00:19,560 --> 00:00:25,080
the back so can I have two different

00:00:23,250 --> 00:00:28,890
perspectives on Lisa

00:00:25,080 --> 00:00:30,900
- mall the kernel hacker and Steven is

00:00:28,890 --> 00:00:32,579
more partner enablement so my site is

00:00:30,900 --> 00:00:33,989
more you have some patches that you're

00:00:32,579 --> 00:00:36,360
looking at essentially one to evaluate

00:00:33,989 --> 00:00:38,070
and Steven is more about you have a

00:00:36,360 --> 00:00:41,460
platform and you want to relate how you

00:00:38,070 --> 00:00:44,130
kernel is running on the platform so

00:00:41,460 --> 00:00:45,690
what we'll go through but I think a lot

00:00:44,130 --> 00:00:47,309
of people in this room know but Lisa is

00:00:45,690 --> 00:00:49,860
that it's bad name but I still go

00:00:47,309 --> 00:00:52,379
through a brief introduction and then

00:00:49,860 --> 00:00:55,260
two sides of Lisa one is using it for

00:00:52,379 --> 00:00:59,309
analysis and the other is using this

00:00:55,260 --> 00:01:01,980
analysis to do some automatic tests also

00:00:59,309 --> 00:01:04,109
have a chat about what's coming in Lisa

00:01:01,980 --> 00:01:05,459
and I put questions at the end but

00:01:04,109 --> 00:01:09,450
actually if you have questions at any

00:01:05,459 --> 00:01:11,899
time feel free to interrupt me right so

00:01:09,450 --> 00:01:15,569
what is Lisa Lisa is a bison project

00:01:11,899 --> 00:01:17,789
that we did look at arm and it it

00:01:15,569 --> 00:01:20,159
bundles the most of my some projects we

00:01:17,789 --> 00:01:22,920
have a Tom namely we have tail deep

00:01:20,159 --> 00:01:25,109
which is a device communication

00:01:22,920 --> 00:01:27,689
abstraction library so what this means

00:01:25,109 --> 00:01:31,169
is it's very easy to communicate with

00:01:27,689 --> 00:01:33,299
your target so if you have you hundred

00:01:31,169 --> 00:01:35,429
smartphone or your high key dev board

00:01:33,299 --> 00:01:36,899
you have the Python interface to

00:01:35,429 --> 00:01:40,679
communicate with it and it makes your

00:01:36,899 --> 00:01:43,679
life easier we also have trap e-fortress

00:01:40,679 --> 00:01:46,439
pair for trespassing so for analysis we

00:01:43,679 --> 00:01:49,380
mostly rely on trespassing so that can

00:01:46,439 --> 00:01:54,329
be F trace or systrace for the Chrome OS

00:01:49,380 --> 00:01:56,639
guys usually use that and yeah so we

00:01:54,329 --> 00:01:59,819
also have notebooks so what Arnold books

00:01:56,639 --> 00:02:02,039
there some sort of IDE that is hosted in

00:01:59,819 --> 00:02:04,109
your web browser and they're very

00:02:02,039 --> 00:02:07,559
convenient because then it's easy to

00:02:04,109 --> 00:02:09,119
plot some information in them and every

00:02:07,559 --> 00:02:10,920
image that you have in a notebook

00:02:09,119 --> 00:02:12,780
is packaged in the same file so you can

00:02:10,920 --> 00:02:13,670
just copy the notebook and set it to

00:02:12,780 --> 00:02:15,380
someone

00:02:13,670 --> 00:02:19,030
and they have all of the information so

00:02:15,380 --> 00:02:22,370
for collaboration it's it's quite nice

00:02:19,030 --> 00:02:23,750
so this is pretty much the same

00:02:22,370 --> 00:02:25,190
information as I've said but I didn't

00:02:23,750 --> 00:02:28,730
want to dump this carried a grandmother

00:02:25,190 --> 00:02:31,310
at once so this is pleading to Singh

00:02:28,730 --> 00:02:33,020
it's not like the shiny future that Joel

00:02:31,310 --> 00:02:34,700
presented we are still split where we do

00:02:33,020 --> 00:02:38,480
some work on our host and then we

00:02:34,700 --> 00:02:40,010
communicate with our target so all of

00:02:38,480 --> 00:02:42,830
the heavy lifting for the communication

00:02:40,010 --> 00:02:45,560
can be s SH ou LD be and that's handled

00:02:42,830 --> 00:02:47,810
through death lip and devil it also

00:02:45,560 --> 00:02:49,880
gives you some interfaces for

00:02:47,810 --> 00:02:53,060
simplifying things with a target so you

00:02:49,880 --> 00:02:55,130
are entirely free to interact with CFS

00:02:53,060 --> 00:02:57,230
or professor on your target and run some

00:02:55,130 --> 00:03:00,100
shell commands on it through that lip

00:02:57,230 --> 00:03:03,049
but you also have some Python interfaces

00:03:00,100 --> 00:03:05,299
just because it's more convenient so for

00:03:03,049 --> 00:03:09,230
instance for a phrase instead of

00:03:05,299 --> 00:03:11,450
manually fiddling with Tracy MD you have

00:03:09,230 --> 00:03:15,470
an interface where you just do start and

00:03:11,450 --> 00:03:18,500
stop and behind the scenes it will copy

00:03:15,470 --> 00:03:20,209
tracy md to the targets start the

00:03:18,500 --> 00:03:22,940
collection of the trace with the events

00:03:20,209 --> 00:03:24,260
you specified and will copy that back to

00:03:22,940 --> 00:03:28,579
your host and you have your trace ready

00:03:24,260 --> 00:03:31,910
to use and then as I said to actually

00:03:28,579 --> 00:03:34,609
use this trace we pass it to trappy that

00:03:31,910 --> 00:03:37,010
will turn into data frames so data

00:03:34,609 --> 00:03:39,350
frames are by some structure that's

00:03:37,010 --> 00:03:41,420
really convenient and really nice to use

00:03:39,350 --> 00:03:43,190
so you can do some sorting some

00:03:41,420 --> 00:03:45,980
filtering you can try to look for

00:03:43,190 --> 00:03:47,150
certain patterns in your data and that's

00:03:45,980 --> 00:03:50,600
the reason why you use them because

00:03:47,150 --> 00:03:51,920
they're quite powerful and so yeah as

00:03:50,600 --> 00:03:53,239
I've said in this hour once we have

00:03:51,920 --> 00:03:55,400
these data frames we can do some

00:03:53,239 --> 00:04:01,760
analysis run some tests on these traces

00:03:55,400 --> 00:04:03,709
and we have two books right so an

00:04:01,760 --> 00:04:09,380
example I'm gonna use is misfit patch

00:04:03,709 --> 00:04:11,889
set so once one has talked about it so

00:04:09,380 --> 00:04:14,780
I'll still give a very quick

00:04:11,889 --> 00:04:16,760
introduction in big label you have as

00:04:14,780 --> 00:04:19,430
metric CPU performance and so you need

00:04:16,760 --> 00:04:22,070
some sort of policy to decide what runs

00:04:19,430 --> 00:04:23,990
on pigs the way we're going was with the

00:04:22,070 --> 00:04:25,940
mission with bash that which was altered

00:04:23,990 --> 00:04:27,040
by Martin who's working in the same team

00:04:25,940 --> 00:04:29,440
as I

00:04:27,040 --> 00:04:32,230
and the rest of it is if you have a task

00:04:29,440 --> 00:04:36,730
that has a utilization greater than 80%

00:04:32,230 --> 00:04:39,700
so it's using at least 80% of its CPU

00:04:36,730 --> 00:04:42,360
time if its current CPUs time we try to

00:04:39,700 --> 00:04:44,980
move it to a higher capacity CPU it's

00:04:42,360 --> 00:04:46,960
the general idea is okay you have a CPU

00:04:44,980 --> 00:04:49,770
bound task and try to give it some more

00:04:46,960 --> 00:04:53,920
CPU done by moving into a bigger CPU

00:04:49,770 --> 00:04:56,070
it's been in Android for a few years but

00:04:53,920 --> 00:05:00,670
we still have a few issues with

00:04:56,070 --> 00:05:03,520
migration delays so this is my test case

00:05:00,670 --> 00:05:05,560
on the Heike 960 where you have four

00:05:03,520 --> 00:05:08,680
liters and four peaks and you just have

00:05:05,560 --> 00:05:12,610
a CPU hug running on a little and we

00:05:08,680 --> 00:05:14,530
want that to run on vmstat and rarely so

00:05:12,610 --> 00:05:18,130
it's not a little time otherwise people

00:05:14,530 --> 00:05:19,570
who have been complaining why before it

00:05:18,130 --> 00:05:22,240
can take a long time so in this case

00:05:19,570 --> 00:05:27,700
it's 350 milliseconds before it moves to

00:05:22,240 --> 00:05:30,460
a big CPU which is a lot of time this is

00:05:27,700 --> 00:05:32,620
a very simple at yep 100% tasks so

00:05:30,460 --> 00:05:36,250
whatever the cute ones I need to run

00:05:32,620 --> 00:05:41,770
forever but we want that run on big just

00:05:36,250 --> 00:05:44,170
do 350 if you look at over there

00:05:41,770 --> 00:05:47,110
the window is 450 but it could even

00:05:44,170 --> 00:05:54,340
higher ups in 800 milliseconds and that

00:05:47,110 --> 00:05:55,690
so so that's where I stepped in and try

00:05:54,340 --> 00:05:58,660
to look at where were squeezing to see

00:05:55,690 --> 00:06:00,280
lies and how we could fix them mostly

00:05:58,660 --> 00:06:03,430
involved a lot of tray sprinkle rather

00:06:00,280 --> 00:06:06,370
shadolla I managed to fix a few things

00:06:03,430 --> 00:06:09,670
and improve the batch set but there's

00:06:06,370 --> 00:06:11,430
one specific issue that I want to show

00:06:09,670 --> 00:06:13,930
in here where Alyssa was very useful

00:06:11,430 --> 00:06:16,780
that after having all of those

00:06:13,930 --> 00:06:20,650
improvements I still even more rarely

00:06:16,780 --> 00:06:23,020
had some long balance intervals some

00:06:20,650 --> 00:06:26,350
migration delays and I found out that it

00:06:23,020 --> 00:06:30,250
was cause to balance interval so this is

00:06:26,350 --> 00:06:31,600
used in particular balance when you are

00:06:30,250 --> 00:06:33,550
about to do a particular parents you

00:06:31,600 --> 00:06:36,340
look at when was the last time you did a

00:06:33,550 --> 00:06:38,310
load balance at the interval and that's

00:06:36,340 --> 00:06:41,610
your next time you're supposed to do

00:06:38,310 --> 00:06:43,530
balance but this ballast interval value

00:06:41,610 --> 00:06:45,930
can be increased when you have some

00:06:43,530 --> 00:06:47,760
tasks that are pinned to a CPU and the

00:06:45,930 --> 00:06:50,850
load balance fail to pull that load and

00:06:47,760 --> 00:06:53,250
so it's sort of a heuristic to say okay

00:06:50,850 --> 00:06:54,930
I know that in my current workload there

00:06:53,250 --> 00:06:56,940
are some ping tests that I can't move so

00:06:54,930 --> 00:06:59,070
let's just wait it out increase the

00:06:56,940 --> 00:07:06,419
balance delay so that next time I look I

00:06:59,070 --> 00:07:07,700
hope that it will be gone yeah so that

00:07:06,419 --> 00:07:14,760
you don't pull again while it's

00:07:07,700 --> 00:07:16,530
migrating yeah yeah but in that case

00:07:14,760 --> 00:07:19,760
that was not the actual balance fast

00:07:16,530 --> 00:07:19,760
that was causing issues

00:07:27,039 --> 00:07:31,749
so the thing is you use this balance

00:07:29,889 --> 00:07:33,639
interval to compute when's the next time

00:07:31,749 --> 00:07:35,740
you're supposed to do a load balance so

00:07:33,639 --> 00:07:37,689
since this value was very high we would

00:07:35,740 --> 00:07:39,969
block any load balancing and we'll we

00:07:37,689 --> 00:07:42,009
have gone over 30 lei that was imposed

00:07:39,969 --> 00:07:44,469
by state value so it's fluctuating the

00:07:42,009 --> 00:07:47,229
period at which you do load balances if

00:07:44,469 --> 00:07:57,550
it's very high you're only gonna do it

00:07:47,229 --> 00:08:01,389
suddenly I think that you can delay by

00:07:57,550 --> 00:08:03,279
something like 8 or 16 times so by

00:08:01,389 --> 00:08:07,809
default you have a short interval but

00:08:03,279 --> 00:08:11,229
you can increase by models 16 time by

00:08:07,809 --> 00:08:19,779
default it's the size of the just span

00:08:11,229 --> 00:08:22,889
of the log based on the factor in the

00:08:19,779 --> 00:08:27,219
load balancing 300 milliseconds us the

00:08:22,889 --> 00:08:28,930
belt itself would have taken I guess if

00:08:27,219 --> 00:08:31,589
it was starting from 0 it would have

00:08:28,930 --> 00:08:34,389
taken like a hundred and something

00:08:31,589 --> 00:08:36,550
that's also something that's also

00:08:34,389 --> 00:08:48,329
something but not for those delays there

00:08:36,550 --> 00:08:52,240
which is way over the parapet time it's

00:08:48,329 --> 00:08:58,000
100 millisecond you starting from 0 to

00:08:52,240 --> 00:09:03,819
reach 80% something right here yeah

00:08:58,000 --> 00:09:05,740
maybe yeah it's not the biggest part and

00:09:03,819 --> 00:09:07,690
so when you have traces like that trap

00:09:05,740 --> 00:09:08,980
you can also pass it and turn it into a

00:09:07,690 --> 00:09:10,300
data frame because as long as you have

00:09:08,980 --> 00:09:12,610
something that looks like a regular

00:09:10,300 --> 00:09:15,490
trace point you can pass it and have

00:09:12,610 --> 00:09:18,880
columns in your data frames and so what

00:09:15,490 --> 00:09:20,860
I've done is I've plotted it so I did

00:09:18,880 --> 00:09:23,110
some filtering to just get the value on

00:09:20,860 --> 00:09:25,660
the big CPU that I was interested in and

00:09:23,110 --> 00:09:29,639
at eye level and the interesting thing

00:09:25,660 --> 00:09:33,120
that I saw is this value gets very high

00:09:29,639 --> 00:09:37,029
before my test workload is even executed

00:09:33,120 --> 00:09:39,760
and that's actually quite nice because

00:09:37,029 --> 00:09:40,480
if I didn't have some way of plotting it

00:09:39,760 --> 00:09:43,029
or having

00:09:40,480 --> 00:09:44,829
overview of all of those values I would

00:09:43,029 --> 00:09:46,420
have been looking in this area and not

00:09:44,829 --> 00:09:48,430
understanding what was happening so I

00:09:46,420 --> 00:09:51,810
think that's a good example of being

00:09:48,430 --> 00:09:54,639
able to plot your traces it's quite nice

00:09:51,810 --> 00:09:58,389
so I had a look in the trace in this

00:09:54,639 --> 00:10:00,430
area and definitely was to blame

00:09:58,389 --> 00:10:05,079
so it was doing some setup on the target

00:10:00,430 --> 00:10:07,779
and it uses buzzy box but this fuzzy box

00:10:05,079 --> 00:10:10,300
red is actually pin to CPU zero and if

00:10:07,779 --> 00:10:12,519
the stars align and at this point you're

00:10:10,300 --> 00:10:14,769
trying to do a load balance you will try

00:10:12,519 --> 00:10:17,350
to pull this task if it's the busiest

00:10:14,769 --> 00:10:20,670
group that you think will fail and you

00:10:17,350 --> 00:10:20,670
will increase the load balance interval

00:10:21,180 --> 00:10:26,980
so yeah that's it for my first

00:10:23,829 --> 00:10:29,730
introduction to Lisa and Steven if you

00:10:26,980 --> 00:10:29,730
wanna talk about the rest

00:10:42,220 --> 00:10:48,070
no yeah we can discuss I I send a fix I

00:10:45,910 --> 00:10:50,140
have discovered that you have some case

00:10:48,070 --> 00:10:52,660
where you say we need to do some load

00:10:50,140 --> 00:10:55,150
balance and the imbalance value is 0 and

00:10:52,660 --> 00:10:59,080
in this case you run in the past with

00:10:55,150 --> 00:11:00,610
the lbf or pin logic whereas it's not

00:10:59,080 --> 00:11:05,080
because your PIN is just because you

00:11:00,610 --> 00:11:08,290
asked to migrate zero load and in the

00:11:05,080 --> 00:11:10,990
move task if the imbalance is zero you

00:11:08,290 --> 00:11:12,970
just return before cleaning that so in

00:11:10,990 --> 00:11:16,630
the load balance per se okay the the

00:11:12,970 --> 00:11:19,690
flag is set so it's a opening balance

00:11:16,630 --> 00:11:22,690
I remove the CPU and redo all the load

00:11:19,690 --> 00:11:31,120
balance stuff so there is a fix in

00:11:22,690 --> 00:11:36,250
Mainliner for that good afternoon i'm

00:11:31,120 --> 00:11:43,950
steven from china systems for support

00:11:36,250 --> 00:11:47,140
team a/e and focused on yes IJ and v8

00:11:43,950 --> 00:11:51,570
virtualization and work very closely

00:11:47,140 --> 00:11:51,570
with the Power Team

00:11:52,740 --> 00:12:05,010
Valentin's this example focus cannot

00:11:57,700 --> 00:12:10,060
hack my Lisa example the target or

00:12:05,010 --> 00:12:12,730
custom but it seems custom and my

00:12:10,060 --> 00:12:18,460
interest in AI and machine learning

00:12:12,730 --> 00:12:22,330
topic so beginning with the first first

00:12:18,460 --> 00:12:27,870
example it's a visualizing kernel

00:12:22,330 --> 00:12:27,870
function graph with in Lisa notebook

00:12:28,080 --> 00:12:37,570
this is show Lisa from work is a very

00:12:32,170 --> 00:12:40,780
flexible and extensible we can very it's

00:12:37,570 --> 00:12:47,260
very easy to add some external module

00:12:40,780 --> 00:12:50,640
and import in in the Lisa notebook in

00:12:47,260 --> 00:12:54,330
this example we needed to install the

00:12:50,640 --> 00:12:58,710
Python graph is module

00:12:54,330 --> 00:13:06,020
import in Lisa notebook it's a simple

00:12:58,710 --> 00:13:11,400
Python interface to for the choir fees

00:13:06,020 --> 00:13:15,620
to join the graph I think most of you

00:13:11,400 --> 00:13:20,120
may may be familiar with graphviz tour

00:13:15,620 --> 00:13:22,950
it's a visualization tools

00:13:20,120 --> 00:13:28,140
it's our open source with searching -

00:13:22,950 --> 00:13:32,000
and it defines simple text-based

00:13:28,140 --> 00:13:37,340
language taught language to describe the

00:13:32,000 --> 00:13:40,790
layout and structure of the graph and

00:13:37,340 --> 00:13:46,370
generate the graph based on a dot file

00:13:40,790 --> 00:13:52,980
so we need a pattern graph is module

00:13:46,370 --> 00:13:57,420
then we get the function trace based on

00:13:52,980 --> 00:14:02,040
the F phase which is a function graph

00:13:57,420 --> 00:14:06,630
tracer - we use the one life simple

00:14:02,040 --> 00:14:09,830
trace command to for grammar - if we if

00:14:06,630 --> 00:14:13,380
we want to get the try to wake up

00:14:09,830 --> 00:14:19,020
function call graph we just use this

00:14:13,380 --> 00:14:24,360
simple command right so we can get a C

00:14:19,020 --> 00:14:28,020
source code like call graph the first

00:14:24,360 --> 00:14:32,030
level function is try to wake up under

00:14:28,020 --> 00:14:41,570
the collie function using a pair of

00:14:32,030 --> 00:14:43,570
brace then next next level and so on so

00:14:41,570 --> 00:14:48,640
to

00:14:43,570 --> 00:14:51,970
visualize the call graph you set the

00:14:48,640 --> 00:14:52,860
function trace graph now nesting level

00:14:51,970 --> 00:14:55,300
as well right

00:14:52,860 --> 00:15:00,060
nesting level because it looks like it's

00:14:55,300 --> 00:15:00,060
nester only two level three I think

00:15:01,770 --> 00:15:05,320
because that's what I was going to ask

00:15:03,580 --> 00:15:08,410
you because if you don't do that then

00:15:05,320 --> 00:15:10,780
the overhead is going to be a lot maybe

00:15:08,410 --> 00:15:13,390
yeah maybe yeah

00:15:10,780 --> 00:15:15,760
the other problem is if you don't set

00:15:13,390 --> 00:15:17,860
filters then you're tracing every kernel

00:15:15,760 --> 00:15:18,970
function yeah yeah yeah all the time

00:15:17,860 --> 00:15:23,950
yeah

00:15:18,970 --> 00:15:27,970
this Bing lock or kind of lock yeah and

00:15:23,950 --> 00:15:32,340
the next step we need a pheasant script

00:15:27,970 --> 00:15:39,160
to pass a trace log from the root

00:15:32,340 --> 00:15:48,880
function and pass one when one line by

00:15:39,160 --> 00:15:55,630
one night to get a quarry and yeah yeah

00:15:48,880 --> 00:16:00,060
so I found a example in on github which

00:15:55,630 --> 00:16:02,860
I it's a Python module which name is a

00:16:00,060 --> 00:16:07,960
kernel visualization

00:16:02,860 --> 00:16:12,780
yeah it's based on the de três três

00:16:07,960 --> 00:16:17,350
logger so I check this hiding script and

00:16:12,780 --> 00:16:21,430
import in the little notebook to pass

00:16:17,350 --> 00:16:25,450
the F trace format yeah it's basically

00:16:21,430 --> 00:16:29,700
create a function called tree from the

00:16:25,450 --> 00:16:34,690
root node and quarry function Noda

00:16:29,700 --> 00:16:37,810
so after build a tree the trace tree we

00:16:34,690 --> 00:16:41,460
just traveled a treat to dump all other

00:16:37,810 --> 00:16:45,510
quarry to the dot fire to describe the

00:16:41,460 --> 00:16:51,520
describe their call graph

00:16:45,510 --> 00:16:55,990
and uh last step we use gravity person

00:16:51,520 --> 00:17:04,660
module just important to visualize the

00:16:55,990 --> 00:17:15,819
function call so her co graph is very

00:17:04,660 --> 00:17:19,360
big so it's just part of the graph it's

00:17:15,819 --> 00:17:29,080
a very useful to do some coded if tab to

00:17:19,360 --> 00:17:36,670
castle next example is KVM performance

00:17:29,080 --> 00:17:40,990
analysis on 39:16 board as the next

00:17:36,670 --> 00:17:45,490
topic scripture like trace command can

00:17:40,990 --> 00:17:47,530
call Python directly Chase Alana yeah so

00:17:45,490 --> 00:17:52,330
you can probably submit this to the

00:17:47,530 --> 00:17:56,110
trace command project it'll be useful

00:17:52,330 --> 00:18:01,990
for everyone you mean this yeah this is

00:17:56,110 --> 00:18:05,860
not book yeah do you need Lisa for this

00:18:01,990 --> 00:18:09,730
stuff to you okay so if it's just python

00:18:05,860 --> 00:18:11,800
and python libraries and other c

00:18:09,730 --> 00:18:14,530
libraries then you can just write a

00:18:11,800 --> 00:18:15,520
plug-in for trace command yeah that

00:18:14,530 --> 00:18:19,260
would be super useful

00:18:15,520 --> 00:18:22,450
I can't reformatted and notebooks as an

00:18:19,260 --> 00:18:28,480
upstream send a poor request

00:18:22,450 --> 00:18:31,650
yeah to trace command project yeah no

00:18:28,480 --> 00:18:34,600
it's just so I'm seeing like you you

00:18:31,650 --> 00:18:39,340
generated the function graph data right

00:18:34,600 --> 00:18:41,320
so when you generate it you can not

00:18:39,340 --> 00:18:43,690
least our trappy or any of that this is

00:18:41,320 --> 00:18:46,990
just you can you can ask trace command

00:18:43,690 --> 00:18:50,470
to call Python functions ok function is

00:18:46,990 --> 00:18:52,690
hit and then you can probably generate

00:18:50,470 --> 00:18:55,420
your dot file in that and then send that

00:18:52,690 --> 00:19:00,840
to graph is to get to get this

00:18:55,420 --> 00:19:03,700
I don't know if we enjoy the static

00:19:00,840 --> 00:19:06,940
abuse Americans support a Python script

00:19:03,700 --> 00:19:09,940
you can use the last talk that I the the

00:19:06,940 --> 00:19:11,710
topic of the last talk right

00:19:09,940 --> 00:19:18,430
you can run trace command you think

00:19:11,710 --> 00:19:20,860
Eddie you're not then the last okay yeah

00:19:18,430 --> 00:19:24,640
you can use Android app to Android them

00:19:20,860 --> 00:19:28,230
yeah okay to run Python trace command

00:19:24,640 --> 00:19:28,230
and yeah and that's one of the reasons

00:19:30,390 --> 00:19:41,850
for functional trace command

00:19:34,120 --> 00:19:47,580
yeah the next is a VM performance on

00:19:41,850 --> 00:19:51,910
hockey 916 I do this performance

00:19:47,580 --> 00:19:58,300
analysis because more and more partners

00:19:51,910 --> 00:20:03,510
ask for the v8 virtualization about the

00:19:58,300 --> 00:20:07,810
performance and implementation on

00:20:03,510 --> 00:20:12,430
platform so I did

00:20:07,810 --> 00:20:15,400
KVM test on the hockey 960 board so this

00:20:12,430 --> 00:20:22,120
is the architecture of

00:20:15,400 --> 00:20:29,760
KVM hiking hockey board so to bring up a

00:20:22,120 --> 00:20:34,480
VM we need to load a colonel from UEFI

00:20:29,760 --> 00:20:38,320
so you can put good at a year to to

00:20:34,480 --> 00:20:43,090
initialize hypervisor as a low-level low

00:20:38,320 --> 00:20:46,960
visor own at your tool saying we start a

00:20:43,090 --> 00:20:52,940
colonel hosted host corner and KVM

00:20:46,960 --> 00:21:01,039
module at a young one yeah for

00:20:52,940 --> 00:21:06,519
and it were created CPUs rather to run

00:21:01,039 --> 00:21:12,979
cast OS and enable stage 2 translation

00:21:06,519 --> 00:21:19,119
then jumping into the guest OS the right

00:21:12,979 --> 00:21:25,549
side of the guest OS and para

00:21:19,119 --> 00:21:29,330
virtualization driver in cast OS our

00:21:25,549 --> 00:21:40,659
co-worker with back-end driver in the

00:21:29,330 --> 00:21:44,590
host OS 2 to the device IO emulation I

00:21:40,659 --> 00:21:51,580
did some roughly test based on sis bench

00:21:44,590 --> 00:21:57,889
like ramp and we test CPU test and test

00:21:51,580 --> 00:22:02,019
we can and visualize tested out in the

00:21:57,889 --> 00:22:07,519
this a notebook we can see the result

00:22:02,019 --> 00:22:13,220
maybe it's a font is too small the right

00:22:07,519 --> 00:22:21,940
is suspension Ram test we can find the

00:22:13,220 --> 00:22:26,679
palapa is in cast OS and oranges host OS

00:22:21,940 --> 00:22:31,519
we can see that the ramp anyways is

00:22:26,679 --> 00:22:35,539
almost the same even guest OS in a

00:22:31,519 --> 00:22:41,629
border stage 2 translation it can be up

00:22:35,539 --> 00:22:46,249
to 24 translation table work if we for

00:22:41,629 --> 00:22:48,850
tre missing but the ramp anyways you

00:22:46,249 --> 00:22:54,160
know almost the same

00:22:48,850 --> 00:22:58,120
yeah and the cpu performance the cpu

00:22:54,160 --> 00:23:05,380
performance in guest OS and in host OS

00:22:58,120 --> 00:23:09,820
is very closely for the file IO tester

00:23:05,380 --> 00:23:14,740
we can find that her so L bandwidth in

00:23:09,820 --> 00:23:21,310
guest OS is significantly lower than in

00:23:14,740 --> 00:23:27,510
host OS yeah as this is the bottleneck

00:23:21,310 --> 00:23:34,310
on v8 virtualization I think back to the

00:23:27,510 --> 00:23:36,780
architecture yeah because each time we

00:23:34,310 --> 00:23:40,780
[Music]

00:23:36,780 --> 00:23:46,690
the front front end what should I watch

00:23:40,780 --> 00:23:51,340
your eyes virtual driver request IO it

00:23:46,690 --> 00:23:55,960
were a trap - yeah - then chapter house

00:23:51,340 --> 00:24:01,410
OS e r1 then house were did a real

00:23:55,960 --> 00:24:05,800
hardware IO and finish after finished

00:24:01,410 --> 00:24:11,920
our request it a trap two-year tools and

00:24:05,800 --> 00:24:19,450
switch - yeah one in cast OS so maybe it

00:24:11,920 --> 00:24:21,930
it will - - the latency so I already see

00:24:19,450 --> 00:24:21,930
ya

00:24:26,870 --> 00:24:35,880
this is the system core function

00:24:31,230 --> 00:24:41,720
profiling based on the reserve function

00:24:35,880 --> 00:24:44,970
profiling function we can easily

00:24:41,720 --> 00:24:49,220
visualize the system core in guest OS

00:24:44,970 --> 00:24:54,240
and in host OS we found the average

00:24:49,220 --> 00:24:59,310
system call like this system call back

00:24:54,240 --> 00:25:01,890
sis rewrite this open add and sis

00:24:59,310 --> 00:25:10,010
Clark a time the average execution time

00:25:01,890 --> 00:25:16,230
is long in guest OS then the host OS

00:25:10,010 --> 00:25:18,410
it's bring too much latency in the guest

00:25:16,230 --> 00:25:18,410
OS

00:25:26,520 --> 00:25:32,880
it's in cash is hot for for the suspense

00:25:30,940 --> 00:25:38,640
test

00:25:32,880 --> 00:25:44,890
yes the latency come from too much trip

00:25:38,640 --> 00:25:55,590
this is because Hockey Night six ball is

00:25:44,890 --> 00:26:02,530
mv8 if we have V 8.1 we actually

00:25:55,590 --> 00:26:08,280
extension cpu the host OS were wrong at

00:26:02,530 --> 00:26:15,840
year two it can reduce half of the trap

00:26:08,280 --> 00:26:15,840
between host OS and guest OS yeah

00:26:15,930 --> 00:26:24,990
country we have hired by the in yellow

00:26:19,810 --> 00:26:24,990
at year one under low visor at year true

00:26:25,980 --> 00:26:35,530
too much contact switch for the year two

00:26:32,230 --> 00:26:39,000
were self yawen context and switch guest

00:26:35,530 --> 00:26:39,000
OS to host OS

00:26:45,760 --> 00:26:51,309
ever

00:26:46,940 --> 00:26:51,309
aiyyo aiyyo bounded as his car

00:26:58,039 --> 00:27:09,080
yes it's called from year zero to y1 in

00:27:03,350 --> 00:27:14,240
guest OS saying SOS for example if we to

00:27:09,080 --> 00:27:21,110
the IO virtual disk IO request it were

00:27:14,240 --> 00:27:23,500
request yes the Ferranti front-end

00:27:21,110 --> 00:27:27,980
driver in guest OS

00:27:23,500 --> 00:27:30,860
atiawa the road of virtue IO interface

00:27:27,980 --> 00:27:37,730
to communicate to the back and the

00:27:30,860 --> 00:27:44,899
driver in the host OS ok at yawen but it

00:27:37,730 --> 00:27:51,730
it happened in one CPU because year year

00:27:44,899 --> 00:27:51,730
switch just can't wrap to current a CPU

00:27:51,850 --> 00:27:58,769
you know

00:27:54,840 --> 00:28:06,659
you can switch trap to another CPUs year

00:27:58,769 --> 00:28:10,110
too so it it must save the year one

00:28:06,659 --> 00:28:13,159
context other Khan has and then switch

00:28:10,110 --> 00:28:13,159
then switch back

00:28:20,390 --> 00:28:30,680
this this example is from from customer

00:28:27,050 --> 00:28:34,670
issue they reported on their too big to

00:28:30,680 --> 00:28:42,050
little platform the apostolate has

00:28:34,670 --> 00:28:47,810
always back to the last bigger CPU yeah

00:28:42,050 --> 00:28:54,380
so I did I create Lisa notebook to

00:28:47,810 --> 00:29:01,100
analysis this behavior it is based on

00:28:54,380 --> 00:29:06,680
the 4.9 Connor because for no latency

00:29:01,100 --> 00:29:12,830
sensitive tasks pass even even it is a

00:29:06,680 --> 00:29:19,400
postulate Husker fan West's FPT were

00:29:12,830 --> 00:29:25,010
direct max super-capacity CPU yeah so

00:29:19,400 --> 00:29:31,670
the max spare capacity means capacity

00:29:25,010 --> 00:29:39,710
original - new you turn for a postulate

00:29:31,670 --> 00:29:44,240
ask new III equals new to a select max

00:29:39,710 --> 00:29:48,340
value of a postulate ask utilization or

00:29:44,240 --> 00:29:54,950
the current CPU utilization plus task

00:29:48,340 --> 00:30:00,340
utilization so for this is our case

00:29:54,950 --> 00:30:05,410
assumption so for us more postilla tasks

00:30:00,340 --> 00:30:12,220
say the utilization for the task is 100

00:30:05,410 --> 00:30:12,220
so the positive the task duration is

00:30:14,330 --> 00:30:29,070
562 so it'll in FB T it will past all

00:30:22,200 --> 00:30:32,759
the small CPU then some max so target a

00:30:29,070 --> 00:30:43,009
Maxwell capacitive for the biggest view

00:30:32,759 --> 00:30:47,360
is 462 so it was select the last big CPU

00:30:43,009 --> 00:30:52,590
almost every time so it it tend to be

00:30:47,360 --> 00:30:54,889
fixed order to select much spare

00:30:52,590 --> 00:30:54,889
capacity

00:30:58,430 --> 00:31:09,290
so after the first task placed on the

00:31:04,580 --> 00:31:15,080
last CPU CPU three if a new task wake up

00:31:09,290 --> 00:31:17,960
with a little utilization in 10 and it

00:31:15,080 --> 00:31:24,340
is also a boost to the 5 percent 15

00:31:17,960 --> 00:31:29,900
percent task so the new utilization is

00:31:24,340 --> 00:31:34,640
517 it was selected to the max for spare

00:31:29,900 --> 00:31:36,370
capacity CPU it were a select CPU 3

00:31:34,640 --> 00:31:43,570
again

00:31:36,370 --> 00:31:53,660
yeah this is our assumption because for

00:31:43,570 --> 00:31:58,730
for the no sensitive tasks pass it tend

00:31:53,660 --> 00:32:07,000
to be pack the yes have packing Tasker

00:31:58,730 --> 00:32:07,000
strategy while it tried to spread a task

00:32:08,740 --> 00:32:15,430
if the CPU can running or a lower okichi

00:32:20,020 --> 00:32:28,050
so we created a Lisa a notebook to

00:32:22,830 --> 00:32:36,010
reproduce this case assumption and

00:32:28,050 --> 00:32:40,870
analysis at the salmon customer trace

00:32:36,010 --> 00:32:44,010
printer clay-like Valentin did and the

00:32:40,870 --> 00:32:50,860
trappy in Lisa Sheree model in Lisa

00:32:44,010 --> 00:32:54,150
Kelly's custom signal and Pilate a

00:32:50,860 --> 00:32:54,150
secure residency

00:32:59,320 --> 00:33:11,770
we also can create a feature - for them

00:33:03,520 --> 00:33:14,590
who we just want to check task 6 so we

00:33:11,770 --> 00:33:20,700
can use a filter to check the Husky 6

00:33:14,590 --> 00:33:27,550
new new utilization and the max fare

00:33:20,700 --> 00:33:35,130
capacity so Trust is GPU residency plot

00:33:27,550 --> 00:33:38,980
we can find our task five six seven

00:33:35,130 --> 00:33:41,970
where select the last because if you at

00:33:38,980 --> 00:33:41,970
first

00:33:47,330 --> 00:33:50,530
the CBO seven

00:33:51,460 --> 00:33:57,419
after a while it may be poor to other

00:33:54,519 --> 00:33:57,419
bigger CPU

00:33:59,770 --> 00:34:10,720
yeah next ultimate tester right so we've

00:34:08,889 --> 00:34:13,030
seen that with Lisa we have a few tools

00:34:10,720 --> 00:34:15,580
to do some analysis on the traces and

00:34:13,030 --> 00:34:17,560
the next logical thing to do was turn

00:34:15,580 --> 00:34:19,990
that into tests that then we can

00:34:17,560 --> 00:34:23,169
automate on our boards and try to see if

00:34:19,990 --> 00:34:26,590
things break or Patras do what we expect

00:34:23,169 --> 00:34:29,740
them to do so for the tests that we have

00:34:26,590 --> 00:34:32,050
in Mesa most of them use a tee up so

00:34:29,740 --> 00:34:34,360
we've already talked about that yep but

00:34:32,050 --> 00:34:37,720
just an easy way to create tasks with a

00:34:34,360 --> 00:34:40,570
given utilisation that we want so in

00:34:37,720 --> 00:34:44,200
terms of test that we have the main test

00:34:40,570 --> 00:34:45,879
should we have is about es so I'll go

00:34:44,200 --> 00:34:47,230
into details on this one on the next

00:34:45,879 --> 00:34:49,389
night which is really just making sure

00:34:47,230 --> 00:34:53,500
that es is doing what we expect which is

00:34:49,389 --> 00:34:56,379
reducing energy consumption we have a

00:34:53,500 --> 00:34:59,230
sanity test for CPU freak which has

00:34:56,379 --> 00:35:01,180
shown that it's quite useful so even if

00:34:59,230 --> 00:35:02,790
you have CPU freak and say you use the

00:35:01,180 --> 00:35:05,560
userspace governor and you try different

00:35:02,790 --> 00:35:07,530
frequencies and then the Sisyphus does

00:35:05,560 --> 00:35:09,520
say yes you have this frequency

00:35:07,530 --> 00:35:11,500
sometimes you actually need to check

00:35:09,520 --> 00:35:13,119
that in the hardware you have the

00:35:11,500 --> 00:35:15,250
frequency that you've requested because

00:35:13,119 --> 00:35:19,270
fences on the Heike 960 we had a broken

00:35:15,250 --> 00:35:21,580
CPU freak and realize that if you run

00:35:19,270 --> 00:35:23,290
this bench at every single OPP you

00:35:21,580 --> 00:35:24,850
always get the same score because the

00:35:23,290 --> 00:35:26,859
frequency is actually not changing so

00:35:24,850 --> 00:35:29,340
this is a sort of test it's quite useful

00:35:26,859 --> 00:35:33,130
to make sure you have something that's

00:35:29,340 --> 00:35:36,670
reasonably sane hot-plug is kind of the

00:35:33,130 --> 00:35:38,290
same kind of test where you plug in and

00:35:36,670 --> 00:35:41,790
out a lot of CPUs and then you try to

00:35:38,290 --> 00:35:45,280
see if you target survived the torture

00:35:41,790 --> 00:35:47,710
and again that is useful when you have

00:35:45,280 --> 00:35:50,470
some hot like help block problems and

00:35:47,710 --> 00:35:52,420
things like that it's a nice test and

00:35:50,470 --> 00:35:54,820
then we have some fusers so load

00:35:52,420 --> 00:35:56,770
tracking signals since we set up we can

00:35:54,820 --> 00:35:58,510
create a workload where we sort of know

00:35:56,770 --> 00:36:00,490
what utilization you should have we can

00:35:58,510 --> 00:36:02,680
try to then compare with the actual

00:36:00,490 --> 00:36:06,400
signals that we have and sort of get an

00:36:02,680 --> 00:36:08,170
idea if it's broken or not and since I

00:36:06,400 --> 00:36:09,670
had some workloads to make sure that

00:36:08,170 --> 00:36:12,030
misfit was working and turn that into

00:36:09,670 --> 00:36:12,030
tests

00:36:12,230 --> 00:36:17,940
right so for the es tests one that is

00:36:16,500 --> 00:36:22,710
actually for interesting is the test

00:36:17,940 --> 00:36:24,540
placement so with es we want to minimize

00:36:22,710 --> 00:36:27,810
the amount of energy we spend and so

00:36:24,540 --> 00:36:29,580
what we can do is since we know what

00:36:27,810 --> 00:36:32,970
kind of task composition we're creating

00:36:29,580 --> 00:36:35,130
we can compute an expected an optimal

00:36:32,970 --> 00:36:38,190
task placement so here for instance our

00:36:35,130 --> 00:36:40,260
workload is we have as many tasks as big

00:36:38,190 --> 00:36:42,510
CPUs so this is hiking in 60 so we have

00:36:40,260 --> 00:36:44,550
four tasks and then we just have two

00:36:42,510 --> 00:36:46,590
phases one phase where it's low

00:36:44,550 --> 00:36:49,080
utilization and the other phase where

00:36:46,590 --> 00:36:50,460
it's high utilization so in the first

00:36:49,080 --> 00:36:52,320
phase we expect it to run our hurdles

00:36:50,460 --> 00:36:54,150
and the second phase we expect it to run

00:36:52,320 --> 00:36:57,630
on the peaks and then we just repeat

00:36:54,150 --> 00:36:59,610
that one more time so since we have the

00:36:57,630 --> 00:37:02,400
energy model we can compute what the

00:36:59,610 --> 00:37:06,180
expected utilization on scape look like

00:37:02,400 --> 00:37:07,860
and then compute an estimated energy

00:37:06,180 --> 00:37:10,140
consumption so this is just using the

00:37:07,860 --> 00:37:12,810
energy model that we have for the target

00:37:10,140 --> 00:37:14,850
and then when we run this we can do

00:37:12,810 --> 00:37:16,380
exactly the same thing we can see okay

00:37:14,850 --> 00:37:18,420
this is where the tasks were placed we

00:37:16,380 --> 00:37:20,400
have the energy model and we can compute

00:37:18,420 --> 00:37:21,960
how much energy that would have cost it

00:37:20,400 --> 00:37:24,540
so there's no actual energy measurement

00:37:21,960 --> 00:37:27,090
but this is just estimated energy and

00:37:24,540 --> 00:37:29,160
then the test make sure that okay what

00:37:27,090 --> 00:37:31,350
energy we have we think we have spent

00:37:29,160 --> 00:37:33,330
here and what energy richness optimally

00:37:31,350 --> 00:37:39,990
used the difference must not be greater

00:37:33,330 --> 00:37:41,760
than 5% and when we run our tests this

00:37:39,990 --> 00:37:43,980
is working pretty well it's just so a

00:37:41,760 --> 00:37:48,030
good example that ETS is doing what we

00:37:43,980 --> 00:37:51,000
want and then as an example of how the

00:37:48,030 --> 00:37:54,150
API looks half of it is actually just

00:37:51,000 --> 00:37:56,550
figuring out what how many tasks you

00:37:54,150 --> 00:37:58,590
want and how much radiation you want but

00:37:56,550 --> 00:38:01,440
then the API is fairly simple with that

00:37:58,590 --> 00:38:03,690
you just say which utilization

00:38:01,440 --> 00:38:05,610
percentage you want and how many times

00:38:03,690 --> 00:38:07,770
you want to loop and I think that's a

00:38:05,610 --> 00:38:10,290
good thing about that you can create

00:38:07,770 --> 00:38:15,420
different workloads with different task

00:38:10,290 --> 00:38:18,960
compositions fairly simply another thing

00:38:15,420 --> 00:38:21,840
so every two weeks we assemble of the

00:38:18,960 --> 00:38:24,450
match sets we have four main line and we

00:38:21,840 --> 00:38:25,289
realize that on tip Chetco so this is in

00:38:24,450 --> 00:38:28,479
go

00:38:25,289 --> 00:38:30,369
work-in-progress tree in a way and we

00:38:28,479 --> 00:38:32,259
run all of our tests for two days and

00:38:30,369 --> 00:38:35,019
then we do a bit of statistical analysis

00:38:32,259 --> 00:38:38,170
and it tells us if we have some

00:38:35,019 --> 00:38:39,789
significant changes or not so this one

00:38:38,170 --> 00:38:42,519
was at the beginning for August this

00:38:39,789 --> 00:38:44,710
year and we can see for instance that we

00:38:42,519 --> 00:38:47,859
had one of them where we had significant

00:38:44,710 --> 00:38:49,630
regression and the p-value was really

00:38:47,859 --> 00:38:52,630
low so we are really sure that this is

00:38:49,630 --> 00:38:54,700
not just testing noise in that case

00:38:52,630 --> 00:38:58,119
actually if I remember correctly it was

00:38:54,700 --> 00:39:00,160
a system D that had created a lot of

00:38:58,119 --> 00:39:02,920
tasks and that was just corrupting the

00:39:00,160 --> 00:39:09,249
tests and influencing what we were what

00:39:02,920 --> 00:39:11,349
we were doing okay so this is for the

00:39:09,249 --> 00:39:14,349
Linux testing side of things

00:39:11,349 --> 00:39:23,259
Steven can take it over 400 and double

00:39:14,349 --> 00:39:26,950
test me again double test the new

00:39:23,259 --> 00:39:30,869
automated test framework for benchmark

00:39:26,950 --> 00:39:34,660
Linux schedule and a yes improvement

00:39:30,869 --> 00:39:39,069
it's based on Lisa and workload

00:39:34,660 --> 00:39:42,969
automation version 3 to around its

00:39:39,069 --> 00:39:46,059
protocol to run a batch of tests and use

00:39:42,969 --> 00:39:51,869
the Lisa notebook to analysis the

00:39:46,059 --> 00:39:59,440
results its intent to to the for

00:39:51,869 --> 00:40:01,900
scheduled evaluation for the yes for

00:39:59,440 --> 00:40:07,019
example for the parrot of years

00:40:01,900 --> 00:40:07,019
what comparison

00:40:08,310 --> 00:40:18,770
yes the debris test workflow is wrong

00:40:13,460 --> 00:40:21,930
the test command in the laser console

00:40:18,770 --> 00:40:28,860
saying it were auto builder the auto

00:40:21,930 --> 00:40:35,280
test and output - without directories

00:40:28,860 --> 00:40:39,060
and wrong several around the at last the

00:40:35,280 --> 00:40:45,180
post-processing listen notebook to do

00:40:39,060 --> 00:40:49,500
some data of theorization yeah it's very

00:40:45,180 --> 00:40:54,690
cool but some potala have their own auto

00:40:49,500 --> 00:40:58,440
test and auto builder system they may

00:40:54,690 --> 00:41:03,270
don't need our aware test auto builder

00:40:58,440 --> 00:41:10,580
and auto fresh just want auto test and

00:41:03,270 --> 00:41:16,050
the order for general evaluation so I

00:41:10,580 --> 00:41:20,340
sound trick for a partner to integrate

00:41:16,050 --> 00:41:26,810
the W attest to their Auto Bild auto

00:41:20,340 --> 00:41:31,860
test system by default W tests were

00:41:26,810 --> 00:41:37,530
compared a different test without based

00:41:31,860 --> 00:41:43,230
on the kernel version but some partners

00:41:37,530 --> 00:41:45,600
think they just want to some two

00:41:43,230 --> 00:41:51,920
different tests it was a different

00:41:45,600 --> 00:41:56,540
configure with the same kernel so

00:41:51,920 --> 00:42:02,270
besides the kernel washing the shower ID

00:41:56,540 --> 00:42:07,620
compare we can use we can classify the

00:42:02,270 --> 00:42:10,460
test with test Italian like to test well

00:42:07,620 --> 00:42:10,460
and it has to

00:42:10,560 --> 00:42:19,000
then in the schedule

00:42:15,040 --> 00:42:22,410
evaluation notebook it can compare a

00:42:19,000 --> 00:42:22,410
different test attack

00:42:25,460 --> 00:42:38,420
yes this is the example for cat vs what

00:42:32,079 --> 00:42:45,589
also use the best anchor in the world

00:42:38,420 --> 00:42:51,760
and parrot tag against the world tag to

00:42:45,589 --> 00:42:58,520
compare this is a boxplot for the

00:42:51,760 --> 00:43:03,920
comparison yeah it's just a trivial

00:42:58,520 --> 00:43:11,780
trick for the hot everywhere test next

00:43:03,920 --> 00:43:13,609
is the future of the Lisa right so for

00:43:11,780 --> 00:43:16,910
the future of this there's a few things

00:43:13,609 --> 00:43:19,700
that we're working on most importantly

00:43:16,910 --> 00:43:22,190
maybe pison sorry so a lot of things for

00:43:19,700 --> 00:43:25,400
passion to the support is getting near

00:43:22,190 --> 00:43:28,730
to 2020 so things like data frames and

00:43:25,400 --> 00:43:30,440
all of the scientific packages they're

00:43:28,730 --> 00:43:33,079
gonna end this report so now is a good

00:43:30,440 --> 00:43:36,170
time to switch over to Python 3

00:43:33,079 --> 00:43:37,730
you've seen the integration report with

00:43:36,170 --> 00:43:41,809
all of those statistics we're planning

00:43:37,730 --> 00:43:43,460
to make that public as well and we are

00:43:41,809 --> 00:43:45,049
going to switch the APR for the

00:43:43,460 --> 00:43:47,150
connector so the example I've given that

00:43:45,049 --> 00:43:48,980
was nice and short that's the new EPA

00:43:47,150 --> 00:43:50,780
because the old one was actually quite

00:43:48,980 --> 00:43:52,339
convoluted and if you wanted to go out

00:43:50,780 --> 00:43:56,030
of the beaten path it was a bit more

00:43:52,339 --> 00:43:58,549
complex so the reason for that is also

00:43:56,030 --> 00:44:00,559
that within these using Lisa for fuse

00:43:58,549 --> 00:44:02,210
and that we have a better idea of what

00:44:00,559 --> 00:44:04,130
we actually want to do so it's a good

00:44:02,210 --> 00:44:05,690
time for a redesign and at the same time

00:44:04,130 --> 00:44:09,440
we can try to make it a bit more ci

00:44:05,690 --> 00:44:12,500
friendly one thing I'm also insisting on

00:44:09,440 --> 00:44:16,420
is documentation and ease of use because

00:44:12,500 --> 00:44:18,829
is not really creating electrons and so

00:44:16,420 --> 00:44:20,270
we're making recommendation as good as

00:44:18,829 --> 00:44:21,559
it can be in the hopes that eventually

00:44:20,270 --> 00:44:26,210
some other people who can also

00:44:21,559 --> 00:44:28,450
contribute the test cases so in terms of

00:44:26,210 --> 00:44:30,859
roadmap this mostly started in August

00:44:28,450 --> 00:44:34,579
where we have so the test execution

00:44:30,859 --> 00:44:36,349
apiary walk and also what we use to

00:44:34,579 --> 00:44:38,230
actually run the test so have we

00:44:36,349 --> 00:44:42,070
collected all of our results and build

00:44:38,230 --> 00:44:43,869
statistics September was well it's still

00:44:42,070 --> 00:44:46,600
mostly about boating the tests we have a

00:44:43,869 --> 00:44:49,150
new API that's almost done and pythons

00:44:46,600 --> 00:44:50,680
free migration since Friday of last week

00:44:49,150 --> 00:44:54,070
we're actually running bison Serena on

00:44:50,680 --> 00:44:55,900
our internal branch next months that's

00:44:54,070 --> 00:44:58,180
where we're gonna use that more

00:44:55,900 --> 00:45:00,280
internally so that both developers and

00:44:58,180 --> 00:45:01,960
also see I use that so that we can try

00:45:00,280 --> 00:45:05,380
to iron out the remaining bugs we would

00:45:01,960 --> 00:45:08,109
have and in November I hopes would be to

00:45:05,380 --> 00:45:10,210
an external release so have a new branch

00:45:08,109 --> 00:45:12,369
from the official repository and after

00:45:10,210 --> 00:45:13,930
some period of time we I mean for six

00:45:12,369 --> 00:45:17,140
months we would switch the main Lisa

00:45:13,930 --> 00:45:19,420
branch to that new version and yeah of

00:45:17,140 --> 00:45:21,340
course one one thing we want is more

00:45:19,420 --> 00:45:24,670
tests that either we come up with so

00:45:21,340 --> 00:45:26,770
that some other people can contribute a

00:45:24,670 --> 00:45:28,780
small preview for people who want to

00:45:26,770 --> 00:45:31,000
really get that through slides and yeah

00:45:28,780 --> 00:45:36,190
that's it so if you have questions or

00:45:31,000 --> 00:45:38,200
otherwise I'm done what other things

00:45:36,190 --> 00:45:41,680
have been thinking about us you know

00:45:38,200 --> 00:45:49,119
trapeze kind of slow right with even

00:45:41,680 --> 00:45:52,390
with like decent-sized traces like weird

00:45:49,119 --> 00:45:55,000
like where's the bottleneck so it seems

00:45:52,390 --> 00:45:58,000
like you know I don't know about tandas

00:45:55,000 --> 00:46:00,550
internals but is there is you know so

00:45:58,000 --> 00:46:02,380
even if crappy was sailed written in C++

00:46:00,550 --> 00:46:06,580
or something and it was multi-threaded

00:46:02,380 --> 00:46:09,280
and doing parallel trace parsing and

00:46:06,580 --> 00:46:12,760
building data frames in parallel but I'm

00:46:09,280 --> 00:46:15,640
not sure too sure whether pandas is

00:46:12,760 --> 00:46:17,980
still the bottleneck no so appendixes

00:46:15,640 --> 00:46:19,690
actually very nicely optimized for large

00:46:17,980 --> 00:46:22,090
data sets I think the bowline like etre

00:46:19,690 --> 00:46:23,830
P is actually passing the trace because

00:46:22,090 --> 00:46:26,650
we're only doing that at once I think we

00:46:23,830 --> 00:46:29,109
have like a work in progress patch set

00:46:26,650 --> 00:46:30,850
to try and divide the file in multiple

00:46:29,109 --> 00:46:32,200
chunks and then do the processing on

00:46:30,850 --> 00:46:34,300
those and then you fit that in the same

00:46:32,200 --> 00:46:35,980
data frame but I don't think it's been

00:46:34,300 --> 00:46:37,570
touched in a while so we should revive

00:46:35,980 --> 00:46:39,420
that maybe if there isn't it's just that

00:46:37,570 --> 00:46:42,250
for now with the cache that you have

00:46:39,420 --> 00:46:43,840
usually you run your tests on the CI and

00:46:42,250 --> 00:46:45,609
then you clone that and you already have

00:46:43,840 --> 00:46:47,830
the cache that's being created and you

00:46:45,609 --> 00:46:49,330
really it's fast to load it again but if

00:46:47,830 --> 00:46:51,360
you actually have use cases where you

00:46:49,330 --> 00:46:54,830
trace is really huge

00:46:51,360 --> 00:46:57,420
that could be an incentive to do that

00:46:54,830 --> 00:47:00,210
idea we discussed before was to parse

00:46:57,420 --> 00:47:02,910
the binary directly from trace trace dot

00:47:00,210 --> 00:47:07,530
that from trace command instead of right

00:47:02,910 --> 00:47:11,370
now it it's all text-based right so you

00:47:07,530 --> 00:47:13,770
know there is lib trace command in trace

00:47:11,370 --> 00:47:17,940
command which you can use to write

00:47:13,770 --> 00:47:19,950
native code to to get access to the

00:47:17,940 --> 00:47:24,240
trace records directly instead of the

00:47:19,950 --> 00:47:31,290
text yes optimisation yeah that should

00:47:24,240 --> 00:47:33,500
speed it up to quite a bit thank you

00:47:31,290 --> 00:47:33,500
everyone

00:47:36,300 --> 00:47:44,460
[Applause]

00:47:39,450 --> 00:47:44,460

YouTube URL: https://www.youtube.com/watch?v=hbPe81OmGw0


