Title: YVR18-205:Evolution of Load tracking mechanism in Linux scheduler
Publication date: 2019-05-09
Playlist: Linaro Connect Vancouver 2018
Description: 
	For the last couple of years, the linux scheduler is trying to get an always better view of the utilization of the CPUs in order to optimize its resources. This presentation will show and explain the latest evolution of the load tracking mechanism, the rational behind these changes and which kind of benefit we can expect in linux system
Captions: 
	00:00:02,110 --> 00:00:07,120
[Music]

00:00:08,349 --> 00:00:16,430
hi everybody and thanks for joining this

00:00:12,259 --> 00:00:18,760
session maybe someone can close the door

00:00:16,430 --> 00:00:18,760
please

00:00:22,750 --> 00:00:30,970
thank you so the the topic the purpose

00:00:28,189 --> 00:00:33,260
of this session is just to give a

00:00:30,970 --> 00:00:35,570
presentation another view of all the

00:00:33,260 --> 00:00:39,170
evolution that that have been done in

00:00:35,570 --> 00:00:40,960
the lot tracking mechanism that is using

00:00:39,170 --> 00:00:43,370
the schedule of ties placement for

00:00:40,960 --> 00:00:45,350
detecting what the what is the

00:00:43,370 --> 00:00:47,090
utilization of the CPU and out to you

00:00:45,350 --> 00:00:52,940
how we are using that in the scheduler

00:00:47,090 --> 00:00:54,440
right now so yeah for the agenda I will

00:00:52,940 --> 00:00:56,690
start with an introduction I you don't

00:00:54,440 --> 00:00:58,190
know if everybody really know how the

00:00:56,690 --> 00:00:59,840
load track what is the load tracking

00:00:58,190 --> 00:01:03,110
mechanism the schedulers I will give a

00:00:59,840 --> 00:01:07,070
short explanation on how we are tracking

00:01:03,110 --> 00:01:09,710
the load and what does it mean and then

00:01:07,070 --> 00:01:13,280
I will go through all the change that

00:01:09,710 --> 00:01:15,800
have been pushed in main line from 4.9

00:01:13,280 --> 00:01:18,340
to the latest kernel that we have and

00:01:15,800 --> 00:01:22,070
which kind of problem we have solved and

00:01:18,340 --> 00:01:26,410
why that should be better for us and

00:01:22,070 --> 00:01:29,540
task placement and scheduling as a wall

00:01:26,410 --> 00:01:32,030
then I will mention a bit what are the

00:01:29,540 --> 00:01:34,940
menus usage of this tracking mechanism

00:01:32,030 --> 00:01:38,810
and also the next step that we can have

00:01:34,940 --> 00:01:43,130
a look at and if there is any good idea

00:01:38,810 --> 00:01:46,760
we can include that as well so let's

00:01:43,130 --> 00:01:51,560
start with the introduction so in the

00:01:46,760 --> 00:01:53,659
scheduler we have what we call the pair

00:01:51,560 --> 00:01:57,520
on TT load tracking mechanism so that's

00:01:53,659 --> 00:02:00,140
a way to track the load as a wide

00:01:57,520 --> 00:02:02,750
description of each entity that are

00:02:00,140 --> 00:02:05,060
scheduled on the CPU so by entity that

00:02:02,750 --> 00:02:08,509
can be either a task or a group of tasks

00:02:05,060 --> 00:02:13,010
and the goal is to evaluate how much CPU

00:02:08,509 --> 00:02:14,930
is needed and so to place them and also

00:02:13,010 --> 00:02:17,480
evaluate how much load it's used to

00:02:14,930 --> 00:02:21,610
balance between each CPU each task and

00:02:17,480 --> 00:02:27,290
give the fairness in the scheduling type

00:02:21,610 --> 00:02:29,150
so for that we have divided the time in

00:02:27,290 --> 00:02:32,720
segments of one millisecond in fact it's

00:02:29,150 --> 00:02:34,640
around one millisecond exactly 1,024

00:02:32,720 --> 00:02:37,310
microsecond just to make the computation

00:02:34,640 --> 00:02:40,970
easier because it's just some shift of

00:02:37,310 --> 00:02:43,900
the beat so that's more efficient for us

00:02:40,970 --> 00:02:46,250
and what we are using it typical

00:02:43,900 --> 00:02:50,750
geometric series where we are waiting

00:02:46,250 --> 00:02:53,090
the past times times times lot so that

00:02:50,750 --> 00:02:55,910
the more recent time slot have a bigger

00:02:53,090 --> 00:02:58,190
impact on the low tracking computation

00:02:55,910 --> 00:03:00,890
so we are very for reaching that in

00:02:58,190 --> 00:03:03,410
order to get an average value so this

00:03:00,890 --> 00:03:06,379
matrix Theory were using what we call

00:03:03,410 --> 00:03:10,459
the alpha tired which is 32 minutes ago

00:03:06,379 --> 00:03:13,549
so it's mean that utilization times not

00:03:10,459 --> 00:03:17,000
after 32 millisecond will wait for out

00:03:13,549 --> 00:03:20,720
of the total utilization so this have

00:03:17,000 --> 00:03:24,139
been used it's a bit a Pyrrhic in the

00:03:20,720 --> 00:03:28,400
sense that this has been selected to to

00:03:24,139 --> 00:03:30,709
be a good time window compared to the

00:03:28,400 --> 00:03:33,620
scheduling time on a system that is

00:03:30,709 --> 00:03:35,120
using something like a CPU so maybe this

00:03:33,620 --> 00:03:37,970
should be changed according if you have

00:03:35,120 --> 00:03:41,299
a larger or smaller system but for

00:03:37,970 --> 00:03:42,829
Knights ad coded for in order to be

00:03:41,299 --> 00:03:46,280
really efficient because we are calling

00:03:42,829 --> 00:03:48,319
that a lot of time there for each time

00:03:46,280 --> 00:03:53,450
certain scheduling time slice so we need

00:03:48,319 --> 00:03:55,579
to optimize everything and this load in

00:03:53,450 --> 00:03:57,290
this load we have three three metrics in

00:03:55,579 --> 00:03:59,299
fact is certainly one metric that it's

00:03:57,290 --> 00:04:00,620
in fact trimetric the first one is what

00:03:59,299 --> 00:04:03,230
we call the et average or the

00:04:00,620 --> 00:04:05,690
utilization it's read the running time

00:04:03,230 --> 00:04:07,910
of an identity how much time we are

00:04:05,690 --> 00:04:10,699
really running either because we want to

00:04:07,910 --> 00:04:13,579
run but yeah it's real running and not

00:04:10,699 --> 00:04:16,519
waiting for running the other one is the

00:04:13,579 --> 00:04:18,859
load average the load average it to be

00:04:16,519 --> 00:04:20,720
different in the sense that we are

00:04:18,859 --> 00:04:22,340
taking into account the waiting time

00:04:20,720 --> 00:04:23,450
when how much time you have you're

00:04:22,340 --> 00:04:25,820
waiting for

00:04:23,450 --> 00:04:29,180
CPU to compute what you have to do and

00:04:25,820 --> 00:04:31,670
we're also taking to account the

00:04:29,180 --> 00:04:33,710
priority of the task you know that to

00:04:31,670 --> 00:04:36,230
the goal is that if you have a higher

00:04:33,710 --> 00:04:38,570
priority so you need more running time

00:04:36,230 --> 00:04:42,980
or you expect more earnings and another

00:04:38,570 --> 00:04:45,110
you're just inflating your load and the

00:04:42,980 --> 00:04:48,680
last one which is the load the Renewable

00:04:45,110 --> 00:04:51,580
low develop average so this third metric

00:04:48,680 --> 00:04:55,520
is only used at the run queue level and

00:04:51,580 --> 00:04:58,280
it's mainly so this is just the sum of

00:04:55,520 --> 00:05:00,950
the load of the runnable tasks mainly

00:04:58,280 --> 00:05:01,940
because the load yeah maybe I should

00:05:00,950 --> 00:05:06,230
have mentioned that

00:05:01,940 --> 00:05:08,840
so the load average for the run queue is

00:05:06,230 --> 00:05:11,990
the sum of the load of each entity under

00:05:08,840 --> 00:05:14,270
under under NQ this entity can be

00:05:11,990 --> 00:05:18,830
running run queuing able run queue our

00:05:14,270 --> 00:05:21,080
weight or slipping slipping scale entity

00:05:18,830 --> 00:05:23,660
so the runnable part is just the sum of

00:05:21,080 --> 00:05:26,060
the rain able skid on TT just to give

00:05:23,660 --> 00:05:29,480
you what is currently trying to run on

00:05:26,060 --> 00:05:36,680
the CPU and give putting about all the

00:05:29,480 --> 00:05:40,700
sleeping tasks so that just describe

00:05:36,680 --> 00:05:42,290
what I was mentioning so the et leverage

00:05:40,700 --> 00:05:44,840
value chose the run part the load

00:05:42,290 --> 00:05:46,850
average include the waiting part and in

00:05:44,840 --> 00:05:52,400
the run queue so the run part is the sum

00:05:46,850 --> 00:05:56,480
of all the random thought so this give

00:05:52,400 --> 00:05:58,730
you a chart of the utilization so here

00:05:56,480 --> 00:06:03,950
we have the CPU utilization so it's

00:05:58,730 --> 00:06:06,980
increasing so I have just adjust the

00:06:03,950 --> 00:06:09,140
load tracking of this okay where we have

00:06:06,980 --> 00:06:11,540
two tasks which try to schedule to run

00:06:09,140 --> 00:06:14,690
at the same time so we have one way one

00:06:11,540 --> 00:06:16,820
task must wait whereas the other one can

00:06:14,690 --> 00:06:20,000
run directly and we have the run queue

00:06:16,820 --> 00:06:23,240
so this is the utilization which is the

00:06:20,000 --> 00:06:24,890
sum of both running together or you can

00:06:23,240 --> 00:06:26,860
see that there we have some blocked

00:06:24,890 --> 00:06:29,450
scenes that will come back a bit later

00:06:26,860 --> 00:06:31,460
so that's quite simple the goal is

00:06:29,450 --> 00:06:33,470
really to have a sum so when we migrate

00:06:31,460 --> 00:06:35,540
an entity from one CPU to another one

00:06:33,470 --> 00:06:38,480
everything stay

00:06:35,540 --> 00:06:40,340
container for the runnable it's a bit

00:06:38,480 --> 00:06:45,020
more difficult to see I hope that it's

00:06:40,340 --> 00:06:46,640
not too fuzzy but I have just make sure

00:06:45,020 --> 00:06:48,710
that the two tasks have a different nice

00:06:46,640 --> 00:06:50,240
priority and that make a huge difference

00:06:48,710 --> 00:06:52,520
for this for example we have a first

00:06:50,240 --> 00:06:54,710
test here which is quite low in the load

00:06:52,520 --> 00:06:58,250
and the other one which is much higher

00:06:54,710 --> 00:07:01,340
so that give him more runtime natural

00:06:58,250 --> 00:07:03,890
run time and on the run cube and then we

00:07:01,340 --> 00:07:05,600
are swimming them and also we have this

00:07:03,890 --> 00:07:08,810
renewable path that just give you for

00:07:05,600 --> 00:07:11,390
example that when both tasks are

00:07:08,810 --> 00:07:13,550
slipping the Renewable Road average is

00:07:11,390 --> 00:07:16,760
new whereas the load in itself state I

00:07:13,550 --> 00:07:18,800
to say that even if right now am idle I

00:07:16,760 --> 00:07:20,570
have a lot of tasks that I've run

00:07:18,800 --> 00:07:23,390
recently there so maybe they will come

00:07:20,570 --> 00:07:24,860
up soon so don't put too much don't put

00:07:23,390 --> 00:07:28,520
that much more tests there because

00:07:24,860 --> 00:07:34,010
otherwise I will have to to to find a

00:07:28,520 --> 00:07:36,800
solution to schedule everything so yeah

00:07:34,010 --> 00:07:40,340
that's for the that's for the

00:07:36,800 --> 00:07:42,380
introduction now I will go through all

00:07:40,340 --> 00:07:44,540
the evolutions so starting for the 4.9

00:07:42,380 --> 00:07:46,160
kernel I will explain all the changes

00:07:44,540 --> 00:07:51,890
that have been done until the latest

00:07:46,160 --> 00:07:54,380
kernel so this in this I'm describing

00:07:51,890 --> 00:07:56,780
how the load tracking mechanism was

00:07:54,380 --> 00:07:58,160
working on the 4.9 and all the problem

00:07:56,780 --> 00:08:03,250
that we were facing

00:07:58,160 --> 00:08:06,650
so one first thing was that once the CPU

00:08:03,250 --> 00:08:09,730
become idle its utilization and its load

00:08:06,650 --> 00:08:12,470
was told to the last running value and

00:08:09,730 --> 00:08:15,410
because it was told it's mean that the

00:08:12,470 --> 00:08:18,170
schedule scheduler level this CPU was

00:08:15,410 --> 00:08:18,620
seen as loaded even if nothing happened

00:08:18,170 --> 00:08:22,280
for a while

00:08:18,620 --> 00:08:24,680
so it was that was creating some wrong

00:08:22,280 --> 00:08:27,230
exception at the schedule Oliver you

00:08:24,680 --> 00:08:30,830
know where we should place one task or

00:08:27,230 --> 00:08:33,200
another the other one was there when a

00:08:30,830 --> 00:08:36,110
task was migrating from one CPU this is

00:08:33,200 --> 00:08:39,080
a green to another one in fact when the

00:08:36,110 --> 00:08:42,910
task was migrating its utilization was

00:08:39,080 --> 00:08:45,200
not migrating so the CPU could be idle

00:08:42,910 --> 00:08:47,300
with iodization

00:08:45,200 --> 00:08:48,570
because just because the task just

00:08:47,300 --> 00:08:51,990
migrated

00:08:48,570 --> 00:08:57,600
but not its utilization so which mean

00:08:51,990 --> 00:08:59,310
that we we had to wait for the load

00:08:57,600 --> 00:09:01,860
tracking mechanism to stabilize to the

00:08:59,310 --> 00:09:04,710
new state and because in the scheduler

00:09:01,860 --> 00:09:06,150
we are migrating a lot of time a lot we

00:09:04,710 --> 00:09:09,030
often by grating the task you know

00:09:06,150 --> 00:09:10,770
denied utilization of the CPU at the end

00:09:09,030 --> 00:09:13,320
your utilization or your load was not

00:09:10,770 --> 00:09:17,580
really accurate compared to your current

00:09:13,320 --> 00:09:20,640
state we had few others problem which

00:09:17,580 --> 00:09:23,370
was the stability for some small task

00:09:20,640 --> 00:09:28,230
the load was moving around without any

00:09:23,370 --> 00:09:31,080
good reason also am i mentioning the

00:09:28,230 --> 00:09:33,120
load sharing I don't know if all of you

00:09:31,080 --> 00:09:35,970
are aware of that but when you are using

00:09:33,120 --> 00:09:39,420
task group so the goal of the task group

00:09:35,970 --> 00:09:43,530
is to do a group of tasks and this group

00:09:39,420 --> 00:09:46,740
should not use mah CPU then if they were

00:09:43,530 --> 00:09:49,110
if it was only one single task so for

00:09:46,740 --> 00:09:52,410
that we have what we call a share so how

00:09:49,110 --> 00:09:53,790
much of this equivalent of one task we

00:09:52,410 --> 00:09:57,870
are sharing between the different thank

00:09:53,790 --> 00:10:01,080
you and so this should move according to

00:09:57,870 --> 00:10:04,770
which task we are running and it's an

00:10:01,080 --> 00:10:07,290
issue on queue and the last one was that

00:10:04,770 --> 00:10:09,750
so this is the frequency change that

00:10:07,290 --> 00:10:12,330
happening songs to the tracking so yeah

00:10:09,750 --> 00:10:15,660
we are using the utilization in order to

00:10:12,330 --> 00:10:19,950
set the frequency of the CPU and adapt

00:10:15,660 --> 00:10:22,590
that and the point was that when the

00:10:19,950 --> 00:10:25,230
task the when the task migrated was

00:10:22,590 --> 00:10:28,230
migrating the frequency was decreasing

00:10:25,230 --> 00:10:30,450
even if the the frequency was shared

00:10:28,230 --> 00:10:32,700
between CPU which mean that we have to

00:10:30,450 --> 00:10:34,740
wait for the new the utilization of the

00:10:32,700 --> 00:10:36,750
and the new CPU to increase slowly in

00:10:34,740 --> 00:10:40,410
order to increase the that the frequency

00:10:36,750 --> 00:10:42,360
which were just creating some some

00:10:40,410 --> 00:10:47,160
regression in performance without any

00:10:42,360 --> 00:10:49,140
good reason because the things that we

00:10:47,160 --> 00:10:50,550
have to compute was still there it was

00:10:49,140 --> 00:10:52,230
just because we have change for one CPU

00:10:50,550 --> 00:11:00,220
to another one

00:10:52,230 --> 00:11:03,820
so based on all this problem we have

00:11:00,220 --> 00:11:05,860
start to make some some changes so when

00:11:03,820 --> 00:11:07,090
I say we it's not only meet a lot of

00:11:05,860 --> 00:11:09,940
people have been involved in that I

00:11:07,090 --> 00:11:13,330
haven't put the authorship of all the

00:11:09,940 --> 00:11:16,720
budget but it's this have been done by a

00:11:13,330 --> 00:11:18,670
lot of different people so the first

00:11:16,720 --> 00:11:21,340
things that have been pushed is the

00:11:18,670 --> 00:11:22,660
propagation so when it has migrated we

00:11:21,340 --> 00:11:24,850
have start to migrate also its

00:11:22,660 --> 00:11:29,610
utilization so that the authorization of

00:11:24,850 --> 00:11:32,980
a CPU always reflect the condition that

00:11:29,610 --> 00:11:35,170
was the first things then we have also

00:11:32,980 --> 00:11:37,480
optimized algorithm because it can be

00:11:35,170 --> 00:11:42,670
called so the load tracking with the one

00:11:37,480 --> 00:11:48,220
millisecond time slice can be can be

00:11:42,670 --> 00:11:50,500
called several times per cheek it can be

00:11:48,220 --> 00:11:52,810
probably more than sometimes not at ten

00:11:50,500 --> 00:11:54,430
or twenty times per cheek so we can't

00:11:52,810 --> 00:11:57,640
afford to spend too much time doing that

00:11:54,430 --> 00:11:59,890
so this have been optimized we have also

00:11:57,640 --> 00:12:01,360
increased the accuracy for the small

00:11:59,890 --> 00:12:03,820
test that what was I was mentioning

00:12:01,360 --> 00:12:05,890
there the fact that when small task was

00:12:03,820 --> 00:12:07,750
running the task that was done in less

00:12:05,890 --> 00:12:10,660
than one millisecond for example you

00:12:07,750 --> 00:12:16,200
were seeing some situation without any

00:12:10,660 --> 00:12:20,440
good reason and then infer that certain

00:12:16,200 --> 00:12:22,450
yeah we have also taken improve also yes

00:12:20,440 --> 00:12:23,080
the fact where we were starting in time

00:12:22,450 --> 00:12:26,980
window

00:12:23,080 --> 00:12:30,550
now that removed that which mean that in

00:12:26,980 --> 00:12:35,590
4.14 we have we do the same kind of test

00:12:30,550 --> 00:12:38,260
to see what was still missing so you can

00:12:35,590 --> 00:12:40,690
see that we still have the stalled it it

00:12:38,260 --> 00:12:43,420
is exceptional value for idle CPU which

00:12:40,690 --> 00:12:48,160
mean that they are still seen as big

00:12:43,420 --> 00:12:50,320
tasks big CPU our Eevee CPU now we have

00:12:48,160 --> 00:12:54,880
the migration of the of the utilization

00:12:50,320 --> 00:12:56,560
in the load between CPU so which means

00:12:54,880 --> 00:12:59,810
that there is no more frequency change

00:12:56,560 --> 00:13:02,199
when we are migrating

00:12:59,810 --> 00:13:05,209
we still have a problem the sense that

00:13:02,199 --> 00:13:07,850
the frequency is increasing and when we

00:13:05,209 --> 00:13:09,230
are we were idle for a long time we are

00:13:07,850 --> 00:13:12,740
going back to low frequency and

00:13:09,230 --> 00:13:14,269
increasing so yeah we have mentioned

00:13:12,740 --> 00:13:16,459
that so the load sharing is still not

00:13:14,269 --> 00:13:18,370
yet accurate because when a new task is

00:13:16,459 --> 00:13:21,139
running in the same textbook the share

00:13:18,370 --> 00:13:24,470
shouldn't should be changed to take into

00:13:21,139 --> 00:13:27,160
whom this new running task but we have

00:13:24,470 --> 00:13:30,589
most stable tasks most of all

00:13:27,160 --> 00:13:37,279
utilization for small tasks so this was

00:13:30,589 --> 00:13:40,970
for the 4.14 so based on that we have

00:13:37,279 --> 00:13:42,620
modified the propagation mechanism in

00:13:40,970 --> 00:13:44,870
order to take into account what we call

00:13:42,620 --> 00:13:46,790
the runnable the fact that when we were

00:13:44,870 --> 00:13:49,009
migrating a task we also migrating the

00:13:46,790 --> 00:13:50,899
runnable and that will help us to update

00:13:49,009 --> 00:13:57,560
what we call the share between task

00:13:50,899 --> 00:14:01,240
group so that was main things this so I

00:13:57,560 --> 00:14:04,220
haven't reflected that that also in 4.16

00:14:01,240 --> 00:14:05,959
the utilization of deadlines kept class

00:14:04,220 --> 00:14:08,420
have been implemented so we were

00:14:05,959 --> 00:14:10,670
tracking we were able to take into

00:14:08,420 --> 00:14:13,790
account how much bandwidth is needed in

00:14:10,670 --> 00:14:16,819
a CPU because of deadlines tasks and

00:14:13,790 --> 00:14:20,559
also the invariants in the OPP selection

00:14:16,819 --> 00:14:23,540
have been implemented as well just too

00:14:20,559 --> 00:14:28,279
so for that is just that running for the

00:14:23,540 --> 00:14:29,959
line test running at lowest it doesn't

00:14:28,279 --> 00:14:32,329
make the same difference

00:14:29,959 --> 00:14:33,949
so we are taking that into account you

00:14:32,329 --> 00:14:37,730
know that when we are contained the

00:14:33,949 --> 00:14:40,699
running time of each task and if to know

00:14:37,730 --> 00:14:43,189
when we have to to block the task

00:14:40,699 --> 00:14:49,009
because it has used more time that

00:14:43,189 --> 00:14:51,730
expected what we have also done so yeah

00:14:49,009 --> 00:14:55,629
we have also decayed this blocked

00:14:51,730 --> 00:14:58,910
utilization for idle CPU the fact that

00:14:55,629 --> 00:15:05,420
some CPU which were idle for a long time

00:14:58,910 --> 00:15:07,790
can be seen as busy CPU the utilized

00:15:05,420 --> 00:15:10,009
also have been implemented in merge so

00:15:07,790 --> 00:15:13,180
the goal of this is to save the less the

00:15:10,009 --> 00:15:16,910
last highest utilization of

00:15:13,180 --> 00:15:19,550
before going to sleep so that when the

00:15:16,910 --> 00:15:22,400
tasks wake up instead of waiting for its

00:15:19,550 --> 00:15:24,410
utilization to to raise slowly we can

00:15:22,400 --> 00:15:26,840
anticipate to say okay the last time the

00:15:24,410 --> 00:15:29,300
task was running they reach this level

00:15:26,840 --> 00:15:30,680
of utilization so let's start to assume

00:15:29,300 --> 00:15:34,310
that he will reach the same value

00:15:30,680 --> 00:15:41,570
directly that will help us to prevent

00:15:34,310 --> 00:15:46,130
some poor people switching we have also

00:15:41,570 --> 00:15:49,070
implemented the tracking of utilization

00:15:46,130 --> 00:15:50,890
for dirty tasks so and for the deadlines

00:15:49,070 --> 00:15:53,990
that's another thing so the first one

00:15:50,890 --> 00:15:57,170
the utilization was computed based on

00:15:53,990 --> 00:15:59,150
the request set for each deadline test

00:15:57,170 --> 00:16:01,130
so the fact that you had your period and

00:15:59,150 --> 00:16:03,890
your running time and based on that you

00:16:01,130 --> 00:16:06,440
know emerged CPU is needed but we're

00:16:03,890 --> 00:16:11,750
also tracking that with the load

00:16:06,440 --> 00:16:14,780
tracking mechanism just to get to say

00:16:11,750 --> 00:16:18,260
that the main one main one interesting

00:16:14,780 --> 00:16:21,740
thing was to compute how much time was

00:16:18,260 --> 00:16:24,080
stolen by deadlines tasks and task to

00:16:21,740 --> 00:16:25,880
CFS does just because the CFS task is

00:16:24,080 --> 00:16:28,160
the lowest priority task so they just

00:16:25,880 --> 00:16:30,020
they can just use what is remaining and

00:16:28,160 --> 00:16:35,020
we need to take that into account when

00:16:30,020 --> 00:16:37,580
we are placing the task and we have also

00:16:35,020 --> 00:16:39,950
implemented the utilization tracking

00:16:37,580 --> 00:16:42,470
making irq tracking mechanism so we're

00:16:39,950 --> 00:16:45,170
now tracking the time span and the inter

00:16:42,470 --> 00:16:48,440
contexts as well and we are taking that

00:16:45,170 --> 00:16:51,920
into account for frequency scaling but

00:16:48,440 --> 00:16:54,800
where as it was not the case before do

00:16:51,920 --> 00:16:57,970
it normally the winter time is just not

00:16:54,800 --> 00:16:57,970
seen by the scheduler

00:17:00,030 --> 00:17:08,650
so this is the latest version of the

00:17:05,829 --> 00:17:10,959
load tracking in the scheduler so as you

00:17:08,650 --> 00:17:13,690
can see now we are starting directly at

00:17:10,959 --> 00:17:15,400
the final frequency songs to ET les so

00:17:13,690 --> 00:17:18,970
we can estimate what will be the final

00:17:15,400 --> 00:17:21,100
utilization so we don't slowly increase

00:17:18,970 --> 00:17:24,310
or decreasing we directly switch between

00:17:21,100 --> 00:17:25,870
we are we have some running tasks so we

00:17:24,310 --> 00:17:32,650
don't have any tasks and we can go back

00:17:25,870 --> 00:17:36,010
to the lower OPP we still have the load

00:17:32,650 --> 00:17:40,270
and the accession propagation we are now

00:17:36,010 --> 00:17:45,220
taking slowly the utilization of idle

00:17:40,270 --> 00:17:47,710
CPU and the shell so we can see that the

00:17:45,220 --> 00:17:50,470
load of a task when another task in that

00:17:47,710 --> 00:17:53,020
same task group is in Korea is running

00:17:50,470 --> 00:17:55,270
we are decreasing his load so that it

00:17:53,020 --> 00:17:59,260
will not take too much runtime compared

00:17:55,270 --> 00:18:02,250
to other task group I haven't seen that

00:17:59,260 --> 00:18:09,340
but also the renewable load is correctly

00:18:02,250 --> 00:18:10,660
computed now and so that's what that's

00:18:09,340 --> 00:18:12,550
where we are right now so I'm seeing

00:18:10,660 --> 00:18:14,940
much more stable and clean compared to

00:18:12,550 --> 00:18:19,000
the first one we come back for example

00:18:14,940 --> 00:18:20,500
to this version you can see that there

00:18:19,000 --> 00:18:22,840
were a lot of change a lot of

00:18:20,500 --> 00:18:27,850
instability in the in the in the metric

00:18:22,840 --> 00:18:31,470
that was creating some strange scheduler

00:18:27,850 --> 00:18:36,880
behavior or wrong displacement decision

00:18:31,470 --> 00:18:39,940
compared to what we have now there so

00:18:36,880 --> 00:18:42,880
that's all what we have done that just

00:18:39,940 --> 00:18:48,160
give you an idea of how much change have

00:18:42,880 --> 00:18:50,680
done since 4.9 I can only encourage you

00:18:48,160 --> 00:18:54,510
to use the latest version if you want to

00:18:50,680 --> 00:18:54,510
have a good scheduling

00:19:01,960 --> 00:19:06,669
that depends of when you mean what you

00:19:04,840 --> 00:19:09,370
mean die either normally if nothing is

00:19:06,669 --> 00:19:10,990
running that should be yes so yeah the

00:19:09,370 --> 00:19:17,770
question was what is the load and

00:19:10,990 --> 00:19:19,960
utilization when a CPU is idle oh how do

00:19:17,770 --> 00:19:22,779
you decay the decision and the load when

00:19:19,960 --> 00:19:25,029
all the CPUs are either so we are taking

00:19:22,779 --> 00:19:28,120
an advantage of different mechanism

00:19:25,029 --> 00:19:32,230
either so in the load in in the

00:19:28,120 --> 00:19:35,320
scheduler we are from time to time we

00:19:32,230 --> 00:19:38,799
are waking up an idle CPU to see if it

00:19:35,320 --> 00:19:40,000
should pull some tasks from other CPU so

00:19:38,799 --> 00:19:42,100
for example if you have two tasks

00:19:40,000 --> 00:19:44,770
running on one CPU we will wake up

00:19:42,100 --> 00:19:48,370
another CPU that will check if it was

00:19:44,770 --> 00:19:51,520
pulling a task and an either CPU or led

00:19:48,370 --> 00:19:53,770
the two tasks on the same CPU and when

00:19:51,520 --> 00:19:57,279
we wake up the DCP you we take advantage

00:19:53,770 --> 00:19:59,830
to also decades-- also when a CPU go

00:19:57,279 --> 00:20:03,399
idle we take advantage to update all the

00:19:59,830 --> 00:20:05,260
block load as well of all the CPU so the

00:20:03,399 --> 00:20:08,529
goal is to minimize we can't do that

00:20:05,260 --> 00:20:10,330
directly when we in the fat what we call

00:20:08,529 --> 00:20:11,740
the odd pass when we are scheduling

00:20:10,330 --> 00:20:16,080
tasks when we are switching tasks

00:20:11,740 --> 00:20:19,390
instead we are taking advantage of some

00:20:16,080 --> 00:20:22,210
event where we can afford to waste some

00:20:19,390 --> 00:20:24,820
time doing this so we are working up one

00:20:22,210 --> 00:20:26,409
most of the time were waking up one CPU

00:20:24,820 --> 00:20:31,990
that will do the job for all the idle

00:20:26,409 --> 00:20:34,659
CPU that's why you are seeing there so

00:20:31,990 --> 00:20:38,250
it's happening from time to time and I

00:20:34,659 --> 00:20:41,890
don't remember if it's 16 or 32

00:20:38,250 --> 00:20:45,580
millisecond the period where which ok so

00:20:41,890 --> 00:20:48,309
if the if all CPUs are I don't we are

00:20:45,580 --> 00:20:50,230
not updating nothing is updated but it's

00:20:48,309 --> 00:20:53,380
a bit wonder for the first CPU is waking

00:20:50,230 --> 00:20:57,220
up we update and then suddenly we have

00:20:53,380 --> 00:20:59,590
all the transition decaying if all CPU

00:20:57,220 --> 00:21:01,630
idle will not will not wake up for any

00:20:59,590 --> 00:21:04,600
thought there is no reason to wake up a

00:21:01,630 --> 00:21:06,010
CPU but yeah once the first CPU will

00:21:04,600 --> 00:21:09,039
wake up in this case we will stop to

00:21:06,010 --> 00:21:11,980
update everything ok and why there is a

00:21:09,039 --> 00:21:14,560
difference between a blocked either in

00:21:11,980 --> 00:21:19,240
our feelings to slice before

00:21:14,560 --> 00:21:21,100
yeah there is white block idol and DK

00:21:19,240 --> 00:21:24,070
block idol which is a difference between

00:21:21,100 --> 00:21:27,340
a decaying four block title and and not

00:21:24,070 --> 00:21:29,920
blocked either what is different here we

00:21:27,340 --> 00:21:32,920
have blocked Idol yeah what's the

00:21:29,920 --> 00:21:37,480
difference between decaying with blocked

00:21:32,920 --> 00:21:41,320
load and without block load know what I

00:21:37,480 --> 00:21:43,600
mean that there is that the load of Idol

00:21:41,320 --> 00:21:45,700
CPU is blocked to its last update and

00:21:43,600 --> 00:21:46,240
the goal is to decay that because we're

00:21:45,700 --> 00:21:49,510
Idol

00:21:46,240 --> 00:21:53,920
we will decay the value slowly otherwise

00:21:49,510 --> 00:21:56,650
when if the CPU are not idle for a long

00:21:53,920 --> 00:21:58,240
time we will have decided that will

00:21:56,650 --> 00:22:00,520
happen and the CBO's who will update

00:21:58,240 --> 00:22:02,290
that regularly on even if the CPUs

00:22:00,520 --> 00:22:04,780
running were updating the load in

00:22:02,290 --> 00:22:07,300
totalization regularly so this is really

00:22:04,780 --> 00:22:12,370
to decay this pot which is block by

00:22:07,300 --> 00:22:21,460
block is not the blocked it's not the is

00:22:12,370 --> 00:22:23,760
not a slot on iOS for example I think so

00:22:21,460 --> 00:22:23,760
right now

00:22:24,810 --> 00:22:31,300
the usage of pallet so it's mainly for

00:22:27,430 --> 00:22:34,690
test placements a load balance so the

00:22:31,300 --> 00:22:37,920
goal is to make sure that we are using

00:22:34,690 --> 00:22:40,570
all the capacity available in each CPU

00:22:37,920 --> 00:22:43,660
so we do utilization we can know if

00:22:40,570 --> 00:22:45,730
there is some non use part of the CPU

00:22:43,660 --> 00:22:47,830
and the load would help us to make some

00:22:45,730 --> 00:22:49,630
fair scheduling between tasks making

00:22:47,830 --> 00:22:56,830
sure that they will have the same amount

00:22:49,630 --> 00:22:59,500
of running time in average so yes that

00:22:56,830 --> 00:23:01,720
we have this pair capacity so when we

00:22:59,500 --> 00:23:06,630
are selecting a CPU will look for a CPU

00:23:01,720 --> 00:23:09,100
with the most available compute capacity

00:23:06,630 --> 00:23:11,470
and we're also using that for the

00:23:09,100 --> 00:23:13,450
schedule illu vanna which is a cpu frag

00:23:11,470 --> 00:23:16,510
governor which is linked directly with

00:23:13,450 --> 00:23:18,220
the scheduler so because the utilization

00:23:16,510 --> 00:23:20,020
because the utilization of attach

00:23:18,220 --> 00:23:23,200
megawatt with this task we can

00:23:20,020 --> 00:23:27,020
anticipate what will be the best OPP

00:23:23,200 --> 00:23:29,910
on the cpu when it does migrant

00:23:27,020 --> 00:23:31,740
and also we are using that to prevent

00:23:29,910 --> 00:23:35,220
some spurious frequency switch in the

00:23:31,740 --> 00:23:37,710
sense that for example right now in the

00:23:35,220 --> 00:23:40,770
mainline the goal is to run at maxsa P P

00:23:37,710 --> 00:23:43,370
when LT task is running and then select

00:23:40,770 --> 00:23:47,280
the best OPP when a CFS task is running

00:23:43,370 --> 00:23:51,180
but because the RT task

00:23:47,280 --> 00:23:53,550
priam the CFS task install some time the

00:23:51,180 --> 00:23:55,800
CFS task can be seen as not really busy

00:23:53,550 --> 00:23:59,520
just because it will use the remaining

00:23:55,800 --> 00:24:01,950
time in the CPU so we were seeing some

00:23:59,520 --> 00:24:04,430
OPP switch between when the T task was

00:24:01,950 --> 00:24:08,760
running we were running at maxsa PP and

00:24:04,430 --> 00:24:10,290
once the CFS task was running we were

00:24:08,760 --> 00:24:12,330
saying that this task for example was

00:24:10,290 --> 00:24:14,670
only using Alf of the CPU because only

00:24:12,330 --> 00:24:17,700
alpha to CP was remaining so we were

00:24:14,670 --> 00:24:20,040
decreasing the OPP whereas in fact the

00:24:17,700 --> 00:24:23,130
CP was reused it says that it was only

00:24:20,040 --> 00:24:25,140
after CPU was available so we are using

00:24:23,130 --> 00:24:26,460
that and there are tiller tracking to

00:24:25,140 --> 00:24:30,420
make sure that will stay the max

00:24:26,460 --> 00:24:33,810
frequency that the two main users that

00:24:30,420 --> 00:24:36,000
we have for now I don't know if there is

00:24:33,810 --> 00:24:38,070
any idea for new usage I mean you're

00:24:36,000 --> 00:24:42,390
more than welcome to try that or make

00:24:38,070 --> 00:24:45,660
some proposal and the next step for us

00:24:42,390 --> 00:24:47,340
so one next step is that we would like

00:24:45,660 --> 00:24:50,160
to win to add the thermal pressure in

00:24:47,340 --> 00:24:52,710
this tracking mechanism just because to

00:24:50,160 --> 00:24:54,780
see that like if you're when you have

00:24:52,710 --> 00:24:56,550
thermal mitigation you're just capping

00:24:54,780 --> 00:24:59,910
the max frequency so you are stealing

00:24:56,550 --> 00:25:01,710
some compute capacity and we want to

00:24:59,910 --> 00:25:04,200
take that into account so when the CPU

00:25:01,710 --> 00:25:07,020
is kept because of thermal mitigation

00:25:04,200 --> 00:25:08,820
we'll we'll see in that as a submitted

00:25:07,020 --> 00:25:12,750
ization that is no more available so we

00:25:08,820 --> 00:25:14,730
can take that when placing task we also

00:25:12,750 --> 00:25:15,360
we also want to update the scale

00:25:14,730 --> 00:25:17,220
invariance

00:25:15,360 --> 00:25:18,750
right now the utilization we have a

00:25:17,220 --> 00:25:20,910
limitation where we're computing the

00:25:18,750 --> 00:25:25,710
ization the initiation can go above the

00:25:20,910 --> 00:25:30,810
current operating point so that gives us

00:25:25,710 --> 00:25:33,450
some limitation and the last part is

00:25:30,810 --> 00:25:37,170
that right now we are using the time and

00:25:33,450 --> 00:25:39,750
the frequency to estimate the

00:25:37,170 --> 00:25:41,910
computation of a task and the goal would

00:25:39,750 --> 00:25:45,390
be to see how we can use some other

00:25:41,910 --> 00:25:47,850
control instead of just to have some

00:25:45,390 --> 00:25:50,820
more realistic value instead of

00:25:47,850 --> 00:25:53,190
emulating this the goal will be to use

00:25:50,820 --> 00:25:55,770
this the some real hardware contr why

00:25:53,190 --> 00:25:59,490
you can come any system any cycle have

00:25:55,770 --> 00:26:01,170
been very used by each task and maybe to

00:25:59,490 --> 00:26:04,170
make some difference between CPU we need

00:26:01,170 --> 00:26:07,200
a memory of indeed tasks I have already

00:26:04,170 --> 00:26:09,240
discussed that with some but yeah that

00:26:07,200 --> 00:26:12,720
that will be help us to know if a task

00:26:09,240 --> 00:26:15,170
is really really need some cycle or just

00:26:12,720 --> 00:26:15,170
need some time

00:26:16,340 --> 00:26:27,420
sorry just asking is there Hardware

00:26:23,640 --> 00:26:29,820
already out there next at four there is

00:26:27,420 --> 00:26:32,790
something but for example in Intel there

00:26:29,820 --> 00:26:34,020
is this a pair of em pair stuff and for

00:26:32,790 --> 00:26:37,550
example because of the current

00:26:34,020 --> 00:26:37,550
implementation of the skin invariance

00:26:39,590 --> 00:26:44,280
that creates some problem for them to

00:26:42,330 --> 00:26:45,600
use the current implementation so

00:26:44,280 --> 00:26:49,700
because they have some other work on

00:26:45,600 --> 00:26:52,050
third set and I think that that's all

00:26:49,700 --> 00:26:55,530
I'm not sure that we have time for one

00:26:52,050 --> 00:26:57,930
question nevertheless tomorrow there is

00:26:55,530 --> 00:27:01,770
a a concession on the same topic or

00:26:57,930 --> 00:27:04,650
where we can follow and go more in

00:27:01,770 --> 00:27:09,120
detail or these cues about any point

00:27:04,650 --> 00:27:10,770
that you want to to address yeah I was

00:27:09,120 --> 00:27:12,480
just curious you said you were gonna try

00:27:10,770 --> 00:27:14,760
removing the capping of the load

00:27:12,480 --> 00:27:16,020
tracking by the current frequency so you

00:27:14,760 --> 00:27:17,520
have any ideas how to do that because it

00:27:16,020 --> 00:27:18,750
seems like a tough problem right I mean

00:27:17,520 --> 00:27:21,420
if you're operating at a half of F Max

00:27:18,750 --> 00:27:23,760
and in saturates the CPU the task load

00:27:21,420 --> 00:27:25,290
how do you know you know what it's what

00:27:23,760 --> 00:27:29,060
it's headed to until you increase the

00:27:25,290 --> 00:27:29,060
operating point that the frequency

00:27:31,350 --> 00:27:36,030
yeah that one main problem right now the

00:27:34,040 --> 00:27:38,850
assumption that is done right now is

00:27:36,030 --> 00:27:42,120
that if you're reaching the current

00:27:38,850 --> 00:27:43,740
utilization the current capacity you

00:27:42,120 --> 00:27:46,470
will increase the frequency and by that

00:27:43,740 --> 00:27:47,820
you will go over this limitation but we

00:27:46,470 --> 00:27:50,790
still have some case where it's not

00:27:47,820 --> 00:27:52,170
working correctly so that's why we want

00:27:50,790 --> 00:27:54,000
to remove that and we just want to

00:27:52,170 --> 00:27:56,160
switch to another way instead of taking

00:27:54,000 --> 00:28:01,560
to con the current capacity we just want

00:27:56,160 --> 00:28:03,960
to scale the time use a different way

00:28:01,560 --> 00:28:06,390
right now we are scaling the utilization

00:28:03,960 --> 00:28:09,300
due the impact and instead we want to

00:28:06,390 --> 00:28:11,790
scale the time it's something similar to

00:28:09,300 --> 00:28:13,170
using a black contour where another way

00:28:11,790 --> 00:28:14,940
counter if you're pointing the number of

00:28:13,170 --> 00:28:17,640
CPU cycle running at a lower frequency

00:28:14,940 --> 00:28:20,250
will give you less cycle time cycle CPU

00:28:17,640 --> 00:28:22,080
cycle which is some kind of time cycle

00:28:20,250 --> 00:28:24,060
so we want to go in this that in this

00:28:22,080 --> 00:28:26,780
direction but we have some problem to

00:28:24,060 --> 00:28:26,780
fix before that

00:28:30,360 --> 00:28:33,480
thank you

00:28:35,570 --> 00:28:42,289
[Applause]

00:28:37,280 --> 00:28:42,289

YouTube URL: https://www.youtube.com/watch?v=2-KJsRPHfz8


