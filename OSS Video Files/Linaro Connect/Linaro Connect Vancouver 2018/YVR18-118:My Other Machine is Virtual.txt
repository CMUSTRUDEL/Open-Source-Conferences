Title: YVR18-118:My Other Machine is Virtual
Publication date: 2019-05-09
Playlist: Linaro Connect Vancouver 2018
Description: 
	When working with new architectures there is often a scramble for getting access to hardware. However hardware comes with its own problems - especially when it is new. It's hard to upgrade, hard to poke around inside and hard to experiment with.

This is an area where QEMU can help. Thanks to its cross-architecture emulation and ability to run full-system emulation it provides a platform for experimentation without the potential consequences of turning your new board into a brick.

This talk will start with an overview of QEMU and how various configurations can be setup. We'll then examine various features available that allow us to examine the run time behaviour of code inside QEMU as well as discuss some of its limitations. Finally we'll look at some experiments that would be hard to do with real hardware and what they can tell us about the code we are running.
Captions: 
	00:00:02,110 --> 00:00:07,120
[Music]

00:00:08,410 --> 00:00:14,389
all right can everyone hear me

00:00:11,139 --> 00:00:18,199
excellent right I'll try and zip through

00:00:14,389 --> 00:00:21,619
this quickly so just a quick

00:00:18,199 --> 00:00:24,199
introductions my name's Alex Bonet I'm a

00:00:21,619 --> 00:00:26,630
senior virtualization engineer with

00:00:24,199 --> 00:00:28,609
Lennar oh I mostly work on Chrome you

00:00:26,630 --> 00:00:30,500
and the bit of chrome you I work on the

00:00:28,609 --> 00:00:33,620
most is the TCG part which is what's

00:00:30,500 --> 00:00:37,520
used for emulation I've been known to do

00:00:33,620 --> 00:00:40,550
a bit of KBM stuff as well you can find

00:00:37,520 --> 00:00:43,370
me on the I on the lanai IRC channels as

00:00:40,550 --> 00:00:47,180
a JB Lennar oh and on the creme you IRC

00:00:43,370 --> 00:00:48,770
channel as SD Squad just a couple of

00:00:47,180 --> 00:00:51,110
notes on the code in this presentation

00:00:48,770 --> 00:00:53,270
all the analysis code is written in

00:00:51,110 --> 00:00:55,640
Python where you see these double

00:00:53,270 --> 00:00:57,530
Chevron things they're just no web

00:00:55,640 --> 00:00:59,450
inclusions so it's just so I can run the

00:00:57,530 --> 00:01:00,980
code and make sure it works but I've not

00:00:59,450 --> 00:01:03,350
included it in the presentation because

00:01:00,980 --> 00:01:04,610
generally boilerplate stuff but if you

00:01:03,350 --> 00:01:06,170
want the source code for these slides

00:01:04,610 --> 00:01:10,670
give us a shout and I can send them to

00:01:06,170 --> 00:01:13,450
you so why do we want to use virtual

00:01:10,670 --> 00:01:16,490
machines for doing experiments on code

00:01:13,450 --> 00:01:18,080
well the first reason is measurement

00:01:16,490 --> 00:01:20,270
measuring things on Hardware can be

00:01:18,080 --> 00:01:23,090
quite hard it generally involves

00:01:20,270 --> 00:01:26,090
attaching lots of wires it will also

00:01:23,090 --> 00:01:28,340
depend on having the information that

00:01:26,090 --> 00:01:29,900
you want exposed by the hardware and of

00:01:28,340 --> 00:01:32,630
course everything in software is easy

00:01:29,900 --> 00:01:35,750
because you can just simply rebuild it

00:01:32,630 --> 00:01:39,200
and add what you need the next reason is

00:01:35,750 --> 00:01:41,030
control when you're running a machine in

00:01:39,200 --> 00:01:43,159
a virtual environment you can completely

00:01:41,030 --> 00:01:45,920
control what that machine sees so you

00:01:43,159 --> 00:01:49,070
can decide what hardware it sees when it

00:01:45,920 --> 00:01:51,140
sees it you can even freeze time examine

00:01:49,070 --> 00:01:52,520
what's going on in advance time at a

00:01:51,140 --> 00:01:55,250
speed that's helpful to you

00:01:52,520 --> 00:01:58,820
you can even rewind time if you're being

00:01:55,250 --> 00:02:01,580
exceptionally clever the next reason is

00:01:58,820 --> 00:02:03,590
to aid analysis so if you're using

00:02:01,580 --> 00:02:05,360
Hardware tracing facilities you've often

00:02:03,590 --> 00:02:07,340
got limited buffers and which you can

00:02:05,360 --> 00:02:08,959
trace when you're doing things in

00:02:07,340 --> 00:02:10,209
software you're basically limited by the

00:02:08,959 --> 00:02:14,230
size of your

00:02:10,209 --> 00:02:17,620
disk so there are a number of example

00:02:14,230 --> 00:02:21,060
tools that exist in this space dynamite

00:02:17,620 --> 00:02:24,489
dynamo Rio is a BSD licensed tool kit

00:02:21,060 --> 00:02:26,620
Intel pin is their proprietary offering

00:02:24,489 --> 00:02:30,430
and the venerable Val grind also has a

00:02:26,620 --> 00:02:32,019
tools in interface however one thing all

00:02:30,430 --> 00:02:35,439
these tools have in common is their

00:02:32,019 --> 00:02:36,430
target user space programs so what about

00:02:35,439 --> 00:02:40,079
premium

00:02:36,430 --> 00:02:43,000
I mean who mu is a open source

00:02:40,079 --> 00:02:45,340
virtualizer most people will have come

00:02:43,000 --> 00:02:47,260
across chrome you using it for KVM but

00:02:45,340 --> 00:02:50,769
in this case what we're interested in it

00:02:47,260 --> 00:02:53,470
is in its emulation abilities so crema

00:02:50,769 --> 00:02:55,540
supports 21 guest architectures and it

00:02:53,470 --> 00:02:57,549
can run code for all of those guest

00:02:55,540 --> 00:02:58,930
architectures on any of its major

00:02:57,549 --> 00:03:03,370
supported architectures I think there's

00:02:58,930 --> 00:03:05,459
about seven backends including there are

00:03:03,370 --> 00:03:09,310
two modes in which the simulation runs

00:03:05,459 --> 00:03:11,560
the first is Linux user emulation so

00:03:09,310 --> 00:03:13,930
this is for running Linux user space

00:03:11,560 --> 00:03:17,229
apps and it's a pretty fast way of

00:03:13,930 --> 00:03:19,479
running them and the reason it's fast is

00:03:17,229 --> 00:03:22,000
because it only has to concentrate on

00:03:19,479 --> 00:03:25,030
translating the the user mode part of

00:03:22,000 --> 00:03:27,099
the code when these mode code makes the

00:03:25,030 --> 00:03:29,079
system call chrome you takes that system

00:03:27,099 --> 00:03:33,310
call lunges the parameters and then

00:03:29,079 --> 00:03:35,620
passes it down to the host system the

00:03:33,310 --> 00:03:37,479
next mode that you can run chrome you're

00:03:35,620 --> 00:03:39,729
in is what's known as full system

00:03:37,479 --> 00:03:42,159
emulation so here we're emulating a

00:03:39,729 --> 00:03:44,319
whole machine including hardware with

00:03:42,159 --> 00:03:46,239
peripherals and then you're running in

00:03:44,319 --> 00:03:47,919
that machine a guest kernel and the

00:03:46,239 --> 00:03:51,009
guest kernel has its own user space

00:03:47,919 --> 00:03:52,599
applications this is generally quite a

00:03:51,009 --> 00:03:54,099
lot slower than running in user mode

00:03:52,599 --> 00:03:56,229
because we also have to make sure that

00:03:54,099 --> 00:03:59,680
we're emulating things like the MMU

00:03:56,229 --> 00:04:00,909
properly so let's look at some simple

00:03:59,680 --> 00:04:03,970
tracing that we can do

00:04:00,909 --> 00:04:06,790
so the log subsystem of chrome you was

00:04:03,970 --> 00:04:08,139
basically designed as a developer tool

00:04:06,790 --> 00:04:09,549
so this is the sort of thing that Crimea

00:04:08,139 --> 00:04:12,609
developers use but you can do some

00:04:09,549 --> 00:04:15,609
interesting stuff with it so here we've

00:04:12,609 --> 00:04:17,590
got an example trace so the binary is

00:04:15,609 --> 00:04:19,719
one of our test binaries that we run

00:04:17,590 --> 00:04:21,969
just to verify that the TCG is working

00:04:19,719 --> 00:04:23,740
properly and it simply computes a sha-1

00:04:21,969 --> 00:04:25,120
it's a nice simple binary

00:04:23,740 --> 00:04:26,590
but it's a good test case to make sure

00:04:25,120 --> 00:04:30,789
your logic instructions are doing what

00:04:26,590 --> 00:04:32,770
you expect so the little D minus D CPU

00:04:30,789 --> 00:04:35,770
is telling it to dump the CPU state and

00:04:32,770 --> 00:04:38,530
the capital D is telling it what file we

00:04:35,770 --> 00:04:40,090
wanted to go into if we have a look at

00:04:38,530 --> 00:04:42,400
the trace file you'll see something like

00:04:40,090 --> 00:04:44,500
this so here we're dumping the complete

00:04:42,400 --> 00:04:46,539
state of the the register files all the

00:04:44,500 --> 00:04:49,060
general-purpose registers as well as a

00:04:46,539 --> 00:04:52,180
couple of bits about the the processor

00:04:49,060 --> 00:04:54,550
state you can expand this trace is

00:04:52,180 --> 00:04:59,229
willing to include FPU information and

00:04:54,550 --> 00:05:00,639
Cindy information if you want so let's

00:04:59,229 --> 00:05:03,460
look at what we've got in this trace are

00:05:00,639 --> 00:05:05,020
we've just generated now the size is a

00:05:03,460 --> 00:05:07,150
little bit of a lie because I'm running

00:05:05,020 --> 00:05:09,130
on ZFS here so it's reporting a smaller

00:05:07,150 --> 00:05:11,199
size in the actual file I think the

00:05:09,130 --> 00:05:14,110
total trace file according to LS is

00:05:11,199 --> 00:05:16,360
about half a megabyte for this run and

00:05:14,110 --> 00:05:19,930
you can see we've collected 799 program

00:05:16,360 --> 00:05:23,199
counters so let's do a little bit of

00:05:19,930 --> 00:05:24,490
analysis on those data points so the

00:05:23,199 --> 00:05:26,919
first thing we need to do is we need to

00:05:24,490 --> 00:05:29,500
get the data into something to do the

00:05:26,919 --> 00:05:31,440
analysis with so I'm using Python here

00:05:29,500 --> 00:05:34,150
and I'm particularly using the numpy

00:05:31,440 --> 00:05:36,130
Python package which is a package for

00:05:34,150 --> 00:05:39,280
dealing with large arrays of data and

00:05:36,130 --> 00:05:40,630
doing scientific analysis on it so the

00:05:39,280 --> 00:05:41,800
first bit here we're simply running a

00:05:40,630 --> 00:05:45,070
regular expression and collecting all

00:05:41,800 --> 00:05:46,870
the program counters then once we've got

00:05:45,070 --> 00:05:49,659
them in our numpy array we can start

00:05:46,870 --> 00:05:51,669
doing some basic analysis so I can see

00:05:49,659 --> 00:05:53,319
where the bottom program counter is

00:05:51,669 --> 00:05:56,830
where the top program counter is get an

00:05:53,319 --> 00:05:59,830
idea of where and where this code is

00:05:56,830 --> 00:06:01,659
running in the memory map and we can

00:05:59,830 --> 00:06:04,090
then start thinking about doing some

00:06:01,659 --> 00:06:06,370
simple statistical analysis so the bin

00:06:04,090 --> 00:06:08,069
count function here basically adds up

00:06:06,370 --> 00:06:11,500
the number of times a program counter

00:06:08,069 --> 00:06:12,940
appears and counts how many time each

00:06:11,500 --> 00:06:15,039
program counters a bit so you can see

00:06:12,940 --> 00:06:19,569
here most frequent program counter in

00:06:15,039 --> 00:06:22,780
this run was was it for one b612 times

00:06:19,569 --> 00:06:24,639
it's been seen some time but this is a

00:06:22,780 --> 00:06:28,930
little bit dry sometimes it makes sense

00:06:24,639 --> 00:06:30,669
to visualize this information so we can

00:06:28,930 --> 00:06:34,000
what I'm doing here is I'm preparing the

00:06:30,669 --> 00:06:36,070
data to plot it in a 2d heat map so the

00:06:34,000 --> 00:06:36,669
first bit I'm just simply making sure

00:06:36,070 --> 00:06:39,629
everything's and

00:06:36,669 --> 00:06:43,090
number of even number of pages around

00:06:39,629 --> 00:06:45,400
I'm doing reduction so I'm actually

00:06:43,090 --> 00:06:47,469
making each bucket cover about four

00:06:45,400 --> 00:06:48,999
instructions this is just to reduce the

00:06:47,469 --> 00:06:51,099
total number of data points that I have

00:06:48,999 --> 00:06:53,289
to plot in the graph and then finally

00:06:51,099 --> 00:06:56,529
I'm formatting this in a nice nice 2d

00:06:53,289 --> 00:06:58,960
grid and then I pass it on to none PI's

00:06:56,529 --> 00:07:00,819
matplotlib to plot it and that's a

00:06:58,960 --> 00:07:03,509
trivial piece of code and you get a

00:07:00,819 --> 00:07:05,620
result that looks something like this

00:07:03,509 --> 00:07:08,199
now the contrast isn't particularly good

00:07:05,620 --> 00:07:10,539
on on the projection but you can see the

00:07:08,199 --> 00:07:12,819
little white spots and that's where the

00:07:10,539 --> 00:07:14,439
hot code is and then everything gets

00:07:12,819 --> 00:07:17,349
preventing the progression of the darker

00:07:14,439 --> 00:07:19,000
the less hot code well that's something

00:07:17,349 --> 00:07:22,719
that spent wrong with this the execution

00:07:19,000 --> 00:07:25,060
count of 14 on the highest PC is not

00:07:22,719 --> 00:07:27,550
particularly high and so we need to have

00:07:25,060 --> 00:07:31,509
a little diversion it's the way the TCG

00:07:27,550 --> 00:07:34,479
runs so this is a basic example of the

00:07:31,509 --> 00:07:36,400
TCG run loop so the way premiere works

00:07:34,479 --> 00:07:38,740
is when we get to a particular program

00:07:36,400 --> 00:07:40,270
counter we look up in our translation

00:07:38,740 --> 00:07:43,210
cache to see if we've got a translated

00:07:40,270 --> 00:07:45,279
block to run it if we haven't where you

00:07:43,210 --> 00:07:47,080
need to generate that block and we

00:07:45,279 --> 00:07:50,589
insert it into our translated buffer and

00:07:47,080 --> 00:07:52,689
then come from our run loot and jump

00:07:50,589 --> 00:07:54,310
into the translated code buffer and then

00:07:52,689 --> 00:07:57,310
return so what we've actually done is

00:07:54,310 --> 00:08:00,219
we've measured code coverage for the

00:07:57,310 --> 00:08:02,469
program but we haven't X we haven't

00:08:00,219 --> 00:08:03,759
actually measured how frequently every

00:08:02,469 --> 00:08:05,560
single block is called and that's

00:08:03,759 --> 00:08:08,589
because we do a thing called block

00:08:05,560 --> 00:08:10,839
chaining so the coming out into the run

00:08:08,589 --> 00:08:13,659
loop is fairly expensive and most of the

00:08:10,839 --> 00:08:16,120
time it's unnecessary what we do is when

00:08:13,659 --> 00:08:18,159
we translate a new block and we find the

00:08:16,120 --> 00:08:20,979
two blocks jump from one to the other we

00:08:18,159 --> 00:08:22,240
patch it directly so we jump straight

00:08:20,979 --> 00:08:23,889
from block to block to block so we've

00:08:22,240 --> 00:08:25,750
reduced the number of times we come out

00:08:23,889 --> 00:08:31,319
into our run loop which is where the

00:08:25,750 --> 00:08:31,319
debugging print comes in fortunately the

00:08:31,800 --> 00:08:36,849
fortunately the block chaining mechanism

00:08:35,199 --> 00:08:38,140
has been something that's not being

00:08:36,849 --> 00:08:40,089
without bugs in the past so we have a

00:08:38,140 --> 00:08:42,399
useful flag that allows us to turn it

00:08:40,089 --> 00:08:44,709
off no chain and now you can see we've

00:08:42,399 --> 00:08:47,060
got a lot more useful data thirty-six

00:08:44,709 --> 00:08:50,060
Meg of trace data and

00:08:47,060 --> 00:08:52,340
nearly a million program counters so we

00:08:50,060 --> 00:08:54,590
can do some analysis on that that's more

00:08:52,340 --> 00:08:56,420
like it we're seeing a probe the program

00:08:54,590 --> 00:08:59,990
counter for and see four hundred

00:08:56,420 --> 00:09:02,330
sixty-four thousand hits that's what

00:08:59,990 --> 00:09:05,270
we're looking for so we can plot that in

00:09:02,330 --> 00:09:06,650
a heat map and it doesn't actually look

00:09:05,270 --> 00:09:10,630
particularly pretty but it does show you

00:09:06,650 --> 00:09:14,330
the white points the most executed code

00:09:10,630 --> 00:09:15,470
it actually works if we if we use a log

00:09:14,330 --> 00:09:17,779
scale we get a little bit more

00:09:15,470 --> 00:09:21,860
resolution which hopefully will show up

00:09:17,779 --> 00:09:25,670
on the projector but these program

00:09:21,860 --> 00:09:27,200
counters aren't exactly the every single

00:09:25,670 --> 00:09:29,180
program counter that's being executed

00:09:27,200 --> 00:09:32,620
they're actually the first program

00:09:29,180 --> 00:09:35,690
counter of each executed translated plot

00:09:32,620 --> 00:09:37,940
so what we could do is we could turn on

00:09:35,690 --> 00:09:39,350
single stepping in Crimea and single

00:09:37,940 --> 00:09:41,089
stepping just limits the number of

00:09:39,350 --> 00:09:44,150
instructions to each translated book

00:09:41,089 --> 00:09:46,460
that we run to a single instruction and

00:09:44,150 --> 00:09:48,920
we can collect our log that way and you

00:09:46,460 --> 00:09:49,990
will get you know space because the file

00:09:48,920 --> 00:09:52,400
is huge

00:09:49,990 --> 00:09:55,460
but actually you probably don't need

00:09:52,400 --> 00:09:57,740
this level of detail because each place

00:09:55,460 --> 00:09:59,030
each block that we translate is a whole

00:09:57,740 --> 00:10:01,700
group of instructions that's going to

00:09:59,030 --> 00:10:02,930
run one after another and it's only when

00:10:01,700 --> 00:10:04,339
we come to a decision point where we've

00:10:02,930 --> 00:10:06,230
got a conditional branch that we stopped

00:10:04,339 --> 00:10:08,000
our translations so in terms of working

00:10:06,230 --> 00:10:11,089
out where your code is going that should

00:10:08,000 --> 00:10:12,589
be enough it's one other thing we can do

00:10:11,089 --> 00:10:16,280
though if we're particularly interested

00:10:12,589 --> 00:10:18,410
in areas of the code is use a feature

00:10:16,280 --> 00:10:20,690
called D filter so all D filter does is

00:10:18,410 --> 00:10:23,000
it applies a filter to the program

00:10:20,690 --> 00:10:26,210
counter and limits the output to the log

00:10:23,000 --> 00:10:27,910
file if if the program counter isn't in

00:10:26,210 --> 00:10:30,440
the range that we specified the

00:10:27,910 --> 00:10:32,690
addresses here I've picked or actually

00:10:30,440 --> 00:10:34,610
the start and end and of course our one

00:10:32,690 --> 00:10:37,160
routines themselves because I'm really

00:10:34,610 --> 00:10:39,200
not interested in getting any execution

00:10:37,160 --> 00:10:42,110
profile going through G Lib C for

00:10:39,200 --> 00:10:43,790
printing the result at the end so we can

00:10:42,110 --> 00:10:46,190
plot that in a lot more compact heat

00:10:43,790 --> 00:10:48,230
heat map and you can see exactly where

00:10:46,190 --> 00:10:52,220
the hot points in the code are and where

00:10:48,230 --> 00:10:54,830
the you know the next hottest bits so

00:10:52,220 --> 00:10:57,620
just to summarize the log options you've

00:10:54,830 --> 00:11:00,829
got so D CPU will dump the

00:10:57,620 --> 00:11:03,260
the state of the CPU is quick you get

00:11:00,829 --> 00:11:04,490
relatively small trace files but what

00:11:03,260 --> 00:11:06,529
you're really covering here is

00:11:04,490 --> 00:11:09,200
translation so you it's it's good

00:11:06,529 --> 00:11:10,910
coverage for every piece of code that

00:11:09,200 --> 00:11:13,040
has been executed because we have to

00:11:10,910 --> 00:11:15,079
translate it at least once but it won't

00:11:13,040 --> 00:11:16,760
tell you how often it's executed if we

00:11:15,079 --> 00:11:18,290
want to see how often it's executed we

00:11:16,760 --> 00:11:20,839
need to turn on the know chain option

00:11:18,290 --> 00:11:22,490
and if you really want to Massacre your

00:11:20,839 --> 00:11:26,959
hard disk you can turn on single step

00:11:22,490 --> 00:11:30,079
and get huge trace files but that's just

00:11:26,959 --> 00:11:33,740
the built-in debugging support creamy

00:11:30,079 --> 00:11:36,769
also has a trace facility and in an

00:11:33,740 --> 00:11:39,500
internal trace point API so the trace

00:11:36,769 --> 00:11:42,139
point API is fairly simple but it allows

00:11:39,500 --> 00:11:44,570
you to export a lot of interesting data

00:11:42,139 --> 00:11:46,699
so the way you add an additional trace

00:11:44,570 --> 00:11:48,800
point to crime you is you simply have to

00:11:46,699 --> 00:11:52,160
define a prototype for the trace point

00:11:48,800 --> 00:11:54,500
which the type of information you've got

00:11:52,160 --> 00:11:56,240
and a little format string format string

00:11:54,500 --> 00:11:58,459
is actually used by the tools so when it

00:11:56,240 --> 00:12:00,560
renders the data afterwards and then you

00:11:58,459 --> 00:12:02,449
just make a call to the trace point in

00:12:00,560 --> 00:12:03,890
the code now trace points are fairly

00:12:02,449 --> 00:12:07,190
lightweight because it's a very well

00:12:03,890 --> 00:12:09,440
predicted branch and if the trace point

00:12:07,190 --> 00:12:12,620
is not enabled then it'll just run over

00:12:09,440 --> 00:12:15,680
it very quickly one advantage of the

00:12:12,620 --> 00:12:17,300
trace point API has is you can have

00:12:15,680 --> 00:12:20,810
multiple different types of outputs so

00:12:17,300 --> 00:12:22,519
the default is plain text and it will

00:12:20,810 --> 00:12:26,540
use the existing logging system and

00:12:22,519 --> 00:12:29,149
output a text rendition of the trace

00:12:26,540 --> 00:12:30,890
point we also have what we call simple

00:12:29,149 --> 00:12:32,690
which is basically a binary logging file

00:12:30,890 --> 00:12:37,010
but that's a lot better if you want to

00:12:32,690 --> 00:12:39,500
have compact data D trace an F trace

00:12:37,010 --> 00:12:41,440
these are probably not things you're

00:12:39,500 --> 00:12:43,970
going to use if you're just analyzing

00:12:41,440 --> 00:12:47,360
code in the emulator but they are quite

00:12:43,970 --> 00:12:50,149
useful for the KVM users because you can

00:12:47,360 --> 00:12:51,890
then run analysis on trace points in

00:12:50,149 --> 00:12:54,620
chrome you and correlate them with trace

00:12:51,890 --> 00:12:56,839
points in the kernel and finally we've

00:12:54,620 --> 00:12:59,690
got user space tracing and syslog also

00:12:56,839 --> 00:13:01,639
useful in some cases so why do we want

00:12:59,690 --> 00:13:03,380
to use the trace files or trace files

00:13:01,639 --> 00:13:05,959
are a lot more compact than the debug

00:13:03,380 --> 00:13:08,569
logging so it's going to be faster at

00:13:05,959 --> 00:13:10,010
the out point also you've got a lot more

00:13:08,569 --> 00:13:10,860
interesting trace points we've got trace

00:13:10,010 --> 00:13:13,230
points in

00:13:10,860 --> 00:13:15,180
pretty a lot of the major drivers as

00:13:13,230 --> 00:13:18,210
well as we've got integration into the

00:13:15,180 --> 00:13:20,280
code generator itself so if we're going

00:13:18,210 --> 00:13:23,940
to do the previous experiment but using

00:13:20,280 --> 00:13:26,550
trace points instead this is the example

00:13:23,940 --> 00:13:28,770
so again I have to enable no change but

00:13:26,550 --> 00:13:30,840
instead of tracing the CPU state I'm

00:13:28,770 --> 00:13:33,570
enabling the exact TB trace point so

00:13:30,840 --> 00:13:36,030
that is a trace point that runs before

00:13:33,570 --> 00:13:38,970
we execute every every translation block

00:13:36,030 --> 00:13:42,120
as you can see the final file is a lot

00:13:38,970 --> 00:13:44,310
lot smaller but let's look at some of

00:13:42,120 --> 00:13:47,400
the more interesting trace points so

00:13:44,310 --> 00:13:49,350
this is an example of a trace point that

00:13:47,400 --> 00:13:52,170
will trace all memory accesses run by

00:13:49,350 --> 00:13:55,440
the program so it's called guess I guess

00:13:52,170 --> 00:13:57,240
member for exec and it will be basically

00:13:55,440 --> 00:14:00,030
inserts a helper on every single load

00:13:57,240 --> 00:14:03,150
and store in the program and then logs

00:14:00,030 --> 00:14:05,250
that to the tracing facility so if we

00:14:03,150 --> 00:14:09,480
have just a look at the example trace

00:14:05,250 --> 00:14:11,040
file we get you can see for every trace

00:14:09,480 --> 00:14:12,750
point we've got a timestamp all trace

00:14:11,040 --> 00:14:16,230
points have a timestamp associated with

00:14:12,750 --> 00:14:18,990
them and we're tracing the program that

00:14:16,230 --> 00:14:20,340
appeared what CPU is running on and then

00:14:18,990 --> 00:14:23,520
the virtual address that we've just done

00:14:20,340 --> 00:14:25,170
an access to and the info at the end is

00:14:23,520 --> 00:14:27,540
a little encoded bit of information

00:14:25,170 --> 00:14:28,890
that's holding the size of the access

00:14:27,540 --> 00:14:31,860
whether it's big-endian little-endian

00:14:28,890 --> 00:14:35,910
that sort of stuff so we can do some

00:14:31,860 --> 00:14:38,550
analysis on this we can use the simple

00:14:35,910 --> 00:14:42,000
trace API so in the chromium source tree

00:14:38,550 --> 00:14:44,310
there's a Python simple trace class and

00:14:42,000 --> 00:14:46,800
we can build our own Class M analyzer

00:14:44,310 --> 00:14:49,560
based on that analyzer class and all you

00:14:46,800 --> 00:14:50,910
need to do is you need to add a function

00:14:49,560 --> 00:14:53,250
for each trace point that you want to

00:14:50,910 --> 00:14:57,090
analyze so all I'm doing here is for

00:14:53,250 --> 00:14:58,950
every guest mem trace point I'm making a

00:14:57,090 --> 00:15:02,040
record of the address and I'm keeping a

00:14:58,950 --> 00:15:04,890
track of what size is run so if I do

00:15:02,040 --> 00:15:07,770
that analysis from the program I've just

00:15:04,890 --> 00:15:10,020
run I can see exactly how many loads and

00:15:07,770 --> 00:15:11,970
stores it did in total and a breakdown

00:15:10,020 --> 00:15:15,840
of the sizes of those various loads and

00:15:11,970 --> 00:15:18,200
stores another useful thing you can do

00:15:15,840 --> 00:15:20,300
with queries you can do dynamic pricing

00:15:18,200 --> 00:15:22,640
so when you're running in system

00:15:20,300 --> 00:15:24,830
emulation mode you can use the

00:15:22,640 --> 00:15:26,030
machine monitor to interrogate the state

00:15:24,830 --> 00:15:27,620
of creme you and one of the things you

00:15:26,030 --> 00:15:29,780
can do is you can turn trace points on

00:15:27,620 --> 00:15:32,300
and off and you can also turn the

00:15:29,780 --> 00:15:34,700
tracing file on and off so the trace

00:15:32,300 --> 00:15:36,560
points I'm looking at here the block

00:15:34,700 --> 00:15:37,970
driver trace point so I'm looking at the

00:15:36,560 --> 00:15:42,710
number of bytes that are being written

00:15:37,970 --> 00:15:45,410
and read by Cremonese block driver so

00:15:42,710 --> 00:15:49,190
what I've actually done is I'm going to

00:15:45,410 --> 00:15:50,690
run several set generate several trace

00:15:49,190 --> 00:15:54,740
files when I'm doing various different

00:15:50,690 --> 00:15:56,360
things so here's the analyzer pretty

00:15:54,740 --> 00:15:58,640
much the same as the gasp murmur

00:15:56,360 --> 00:16:00,620
analyzer except on all I'm doing is I'm

00:15:58,640 --> 00:16:03,230
summing up the total number of bytes

00:16:00,620 --> 00:16:04,760
that I've read and written and the

00:16:03,230 --> 00:16:06,530
experiment that I've run is I've done a

00:16:04,760 --> 00:16:09,140
compiled of the cream.you source code

00:16:06,530 --> 00:16:11,360
just after I've booted up and I've set a

00:16:09,140 --> 00:16:13,430
new trace file after did my clean set a

00:16:11,360 --> 00:16:16,840
nice new trace file and then ran the

00:16:13,430 --> 00:16:20,000
compiled again so plotting that result

00:16:16,840 --> 00:16:21,320
it's a fairly simple code I'll just I'll

00:16:20,000 --> 00:16:23,480
skip over it because it's not

00:16:21,320 --> 00:16:26,660
particularly interesting but we get a

00:16:23,480 --> 00:16:28,880
nice graph like this so this is exactly

00:16:26,660 --> 00:16:30,380
what we'd expect to see in the cold case

00:16:28,880 --> 00:16:32,450
we spend quite a lot of time reading

00:16:30,380 --> 00:16:34,940
stuff off the disk and then when we

00:16:32,450 --> 00:16:36,110
recompile it with hot cache we don't

00:16:34,940 --> 00:16:40,610
read anything off the disk as it's

00:16:36,110 --> 00:16:43,370
already cached in in RAM there's a

00:16:40,610 --> 00:16:47,090
couple of things to mention about these

00:16:43,370 --> 00:16:49,250
sort of analyses we have there's timing

00:16:47,090 --> 00:16:51,230
issues when you're doing stuff with TCG

00:16:49,250 --> 00:16:53,240
so if your program is a straight run

00:16:51,230 --> 00:16:54,740
through use of space program it doesn't

00:16:53,240 --> 00:16:56,570
really matter but if you've got timers

00:16:54,740 --> 00:16:58,400
running and you've got some or an

00:16:56,570 --> 00:16:59,750
interrupt handler you need to be aware

00:16:58,400 --> 00:17:01,730
that the interrupt handler is going to

00:16:59,750 --> 00:17:06,020
run slower under emulation

00:17:01,730 --> 00:17:07,760
than it is on the real hardware but we

00:17:06,020 --> 00:17:11,959
do have a solution for this it's called

00:17:07,760 --> 00:17:14,089
I count the I count basically instead of

00:17:11,959 --> 00:17:15,440
reporting times the wall clock time of

00:17:14,089 --> 00:17:18,199
the host system that you're running on

00:17:15,440 --> 00:17:20,150
now reports time based on the number of

00:17:18,199 --> 00:17:22,699
instruction number of guest instructions

00:17:20,150 --> 00:17:24,260
you've executed now it's important to

00:17:22,699 --> 00:17:26,570
point out this isn't a cycle accurate

00:17:24,260 --> 00:17:29,110
emulation of the guest instructions but

00:17:26,570 --> 00:17:32,540
it does give you a deterministic way of

00:17:29,110 --> 00:17:34,880
running your code so in this case I

00:17:32,540 --> 00:17:37,360
think each instruction counts for

00:17:34,880 --> 00:17:41,300
nanosecond have elapsed virtual time

00:17:37,360 --> 00:17:42,710
I'll skip over this all right just what

00:17:41,300 --> 00:17:46,970
I quickly talk about some of the things

00:17:42,710 --> 00:17:49,550
that are in development running late so

00:17:46,970 --> 00:17:51,080
reverse debugging so this builds on our

00:17:49,550 --> 00:17:53,630
account infrastructure so without

00:17:51,080 --> 00:17:55,520
account assuming that all other things

00:17:53,630 --> 00:17:58,940
are equal you can get deterministic

00:17:55,520 --> 00:18:01,190
playback so you can record a trace with

00:17:58,940 --> 00:18:03,350
I can enable and then as long as you've

00:18:01,190 --> 00:18:05,120
enabled snapshotting on your disk you

00:18:03,350 --> 00:18:07,550
run it again you should get exactly the

00:18:05,120 --> 00:18:09,350
same execution because things will

00:18:07,550 --> 00:18:12,320
happen at exactly at the same time as

00:18:09,350 --> 00:18:14,180
far as the guest is concerned and

00:18:12,320 --> 00:18:15,650
building on that you could there's the

00:18:14,180 --> 00:18:18,190
record replayability so this is

00:18:15,650 --> 00:18:21,140
currently a patch on list and this

00:18:18,190 --> 00:18:23,900
extends our gdb stub to support the

00:18:21,140 --> 00:18:26,830
reverse execution primitives that gdb

00:18:23,900 --> 00:18:29,480
supports so you can do funky things like

00:18:26,830 --> 00:18:31,010
crash a bug and then rewind ten

00:18:29,480 --> 00:18:32,890
instructions and see exactly why you've

00:18:31,010 --> 00:18:35,390
got there

00:18:32,890 --> 00:18:38,450
another area of development is plugins

00:18:35,390 --> 00:18:41,570
so this is a taking a bit more time

00:18:38,450 --> 00:18:43,250
that's an RFC at the moment but we would

00:18:41,570 --> 00:18:45,950
like to do is support the ability to

00:18:43,250 --> 00:18:48,680
have more interesting experiments loaded

00:18:45,950 --> 00:18:50,480
as plugins into crÃ©me you but as with

00:18:48,680 --> 00:18:52,130
any plug-in API there's quite a lot of

00:18:50,480 --> 00:18:54,170
debate about exactly what the scope of

00:18:52,130 --> 00:18:56,240
that API to should be or what sort of

00:18:54,170 --> 00:18:59,830
guarantees we make about API stability

00:18:56,240 --> 00:19:03,830
so it's a it's an area of active debate

00:18:59,830 --> 00:19:07,940
I just want to mention our own work so

00:19:03,830 --> 00:19:09,650
there's a epic in JIRA called 35:2 and

00:19:07,940 --> 00:19:10,820
that's the creamy tooling ethic and

00:19:09,650 --> 00:19:12,290
there's a number of things that we're

00:19:10,820 --> 00:19:14,840
looking at improving in the way the

00:19:12,290 --> 00:19:16,280
creamy runs so better memory logging so

00:19:14,840 --> 00:19:18,710
the memory logging I pointed out before

00:19:16,280 --> 00:19:20,510
doesn't tell you anything about the

00:19:18,710 --> 00:19:20,870
values that were loaded and stored into

00:19:20,510 --> 00:19:24,770
memory

00:19:20,870 --> 00:19:26,480
I count for user mode there are some

00:19:24,770 --> 00:19:28,220
complications and this because if you're

00:19:26,480 --> 00:19:30,140
doing user mode emulation you're

00:19:28,220 --> 00:19:32,300
actually talking to the host system so

00:19:30,140 --> 00:19:33,530
yeah you'd have to take care with things

00:19:32,300 --> 00:19:37,880
like timers but it could be potentially

00:19:33,530 --> 00:19:39,410
useful block execution counts it's all

00:19:37,880 --> 00:19:42,580
very well counting up all the blocks

00:19:39,410 --> 00:19:45,120
that I know I started 15 minutes late

00:19:42,580 --> 00:19:46,800
it's I'm not with them

00:19:45,120 --> 00:19:48,810
so instead of collecting all this data

00:19:46,800 --> 00:19:49,980
in a massive log file KREM you'd be

00:19:48,810 --> 00:19:51,270
quite capable will actually just

00:19:49,980 --> 00:19:54,420
counting the number of times each block

00:19:51,270 --> 00:20:03,780
is executed and dumping that so in

00:19:54,420 --> 00:20:05,370
summary why right white creme you it's

00:20:03,780 --> 00:20:07,350
cross-platform so you can run a whole

00:20:05,370 --> 00:20:08,820
number of different guest architectures

00:20:07,350 --> 00:20:11,340
on any of our supported host

00:20:08,820 --> 00:20:13,020
architectures the fact that it supports

00:20:11,340 --> 00:20:15,210
system emulation allows you to do more

00:20:13,020 --> 00:20:19,050
interesting experiments about way things

00:20:15,210 --> 00:20:21,000
that like kernels work and it's also an

00:20:19,050 --> 00:20:24,120
open platform that's nice and open for

00:20:21,000 --> 00:20:26,220
hacking and experimenting tracing API

00:20:24,120 --> 00:20:28,260
use the tracing API it's better than

00:20:26,220 --> 00:20:30,360
using debug locked and it's an area of

00:20:28,260 --> 00:20:32,010
active development we haven't really got

00:20:30,360 --> 00:20:33,480
time for feedback please if you're

00:20:32,010 --> 00:20:35,160
interested in running any sort of

00:20:33,480 --> 00:20:37,740
experiments in chrome you come and grab

00:20:35,160 --> 00:20:39,480
me not after this talk but later in the

00:20:37,740 --> 00:20:41,490
week and tell me what sort of things

00:20:39,480 --> 00:20:43,080
you'd like to run and what sort of

00:20:41,490 --> 00:20:44,910
things you'd like to find out about how

00:20:43,080 --> 00:20:46,220
your code works okay

00:20:44,910 --> 00:20:50,200
thank you very much

00:20:46,220 --> 00:20:55,209
[Applause]

00:20:50,200 --> 00:20:55,209

YouTube URL: https://www.youtube.com/watch?v=R29LdOWJDb4


