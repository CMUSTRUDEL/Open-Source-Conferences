Title: LVC20 211 Next evolutions for Linux scheduler
Publication date: 2020-10-08
Playlist: Linaro Virtual Connect 2020
Description: 
	The scheduler has been the place of a lot of changes during the past releases with new interfaces to set properties of tasks and/or groups of tasks; Other evolution are ongoing and this session will go through the main changes merged during the past releases and the ongoing discussions for next changes.

https://connect.linaro.org/resources/lvc20/lvc20-211/
Captions: 
	00:00:00,160 --> 00:00:03,439
so yeah my name is van sangito i'm

00:00:01,839 --> 00:00:06,399
working for the

00:00:03,439 --> 00:00:08,559
kernel working group of linear role i'm

00:00:06,399 --> 00:00:11,759
involved in the scheduler

00:00:08,559 --> 00:00:15,440
and associated development so

00:00:11,759 --> 00:00:18,720
during this session my goal is to give a

00:00:15,440 --> 00:00:20,240
status on the late latest evolution

00:00:18,720 --> 00:00:22,560
our latest feature which have been

00:00:20,240 --> 00:00:24,480
merged in the scheduler

00:00:22,560 --> 00:00:26,960
and which are the next so it's a main

00:00:24,480 --> 00:00:26,960
topic

00:00:27,439 --> 00:00:31,760
which are in the discussion are which

00:00:29,679 --> 00:00:35,120
are

00:00:31,760 --> 00:00:37,280
studied for the the scheduler

00:00:35,120 --> 00:00:38,280
so yeah don't hesitate to ask questions

00:00:37,280 --> 00:00:39,600
i will start

00:00:38,280 --> 00:00:42,480
[Music]

00:00:39,600 --> 00:00:43,280
i will start with a short status and

00:00:42,480 --> 00:00:46,079
statistic

00:00:43,280 --> 00:00:47,280
so let me move to the first slide yeah

00:00:46,079 --> 00:00:49,600
that's some statistic

00:00:47,280 --> 00:00:51,280
i never did that before so and for this

00:00:49,600 --> 00:00:53,360
session i decided to get

00:00:51,280 --> 00:00:55,199
some kind of statistic around the the

00:00:53,360 --> 00:00:56,079
skate directory and the kernel to have

00:00:55,199 --> 00:00:58,800
an idea of

00:00:56,079 --> 00:01:00,480
what was going on that give you on the

00:00:58,800 --> 00:01:01,600
left side the number of commits which

00:01:00,480 --> 00:01:03,280
are merged

00:01:01,600 --> 00:01:05,680
in the skate directory for the last

00:01:03,280 --> 00:01:07,840
couple of

00:01:05,680 --> 00:01:09,600
of kernel readies and on the right side

00:01:07,840 --> 00:01:14,159
is the number of uh

00:01:09,600 --> 00:01:14,159
ad and delayed of line so

00:01:14,240 --> 00:01:18,880
as you can see we are around most of the

00:01:16,479 --> 00:01:20,560
time we are on between 25 and 50 patch

00:01:18,880 --> 00:01:24,640
which are merged

00:01:20,560 --> 00:01:27,600
so i have put some how it's a

00:01:24,640 --> 00:01:28,080
significant changes but uh most of the

00:01:27,600 --> 00:01:31,920
time

00:01:28,080 --> 00:01:31,920
even most of the time the

00:01:32,400 --> 00:01:38,000
the changes are really small and even

00:01:35,840 --> 00:01:39,600
and we can have so in some kernel

00:01:38,000 --> 00:01:42,560
version we can have a lot of small

00:01:39,600 --> 00:01:43,439
changes which make the number of commit

00:01:42,560 --> 00:01:46,720
going around

00:01:43,439 --> 00:01:50,399
75 but usually we don't have that much

00:01:46,720 --> 00:01:53,360
rework so i have put

00:01:50,399 --> 00:01:56,000
one which is under kernel v 3.3 where we

00:01:53,360 --> 00:01:58,000
have moved all the sched file in the dkt

00:01:56,000 --> 00:02:00,479
directory so that's why also on the code

00:01:58,000 --> 00:02:04,320
change i have removed that because

00:02:00,479 --> 00:02:04,320
the div was so used that it was just

00:02:04,799 --> 00:02:08,560
hiding all the other other changes for

00:02:07,360 --> 00:02:11,520
the

00:02:08,560 --> 00:02:12,160
release that happen after that so yeah

00:02:11,520 --> 00:02:13,840
the main

00:02:12,160 --> 00:02:15,760
real the main changes which have

00:02:13,840 --> 00:02:17,599
generated a lot of commit so it's mainly

00:02:15,760 --> 00:02:18,239
the rework of pneuma balancing which is

00:02:17,599 --> 00:02:22,319
now

00:02:18,239 --> 00:02:26,560
which was in 3.13 so it's a bit

00:02:22,319 --> 00:02:28,879
old now uh also when uh ingo has a

00:02:26,560 --> 00:02:29,920
rework all the the other file of the

00:02:28,879 --> 00:02:33,040
scheduler

00:02:29,920 --> 00:02:35,200
and cleared this and

00:02:33,040 --> 00:02:36,160
another big changes that have in the

00:02:35,200 --> 00:02:37,840
topology the

00:02:36,160 --> 00:02:39,280
cleanup in the topology underwear to

00:02:37,840 --> 00:02:42,000
create them and also the

00:02:39,280 --> 00:02:44,879
when we have integrated the line grab so

00:02:42,000 --> 00:02:47,440
grab its greedy reclamation

00:02:44,879 --> 00:02:49,120
it's a feature of a news bandwidth it's

00:02:47,440 --> 00:02:52,400
a feature of the line scheduler

00:02:49,120 --> 00:02:52,879
when you can use more than the bandwidth

00:02:52,400 --> 00:02:57,040
you have

00:02:52,879 --> 00:03:00,080
been committed to and as you can see

00:02:57,040 --> 00:03:02,840
the change is not that huge of a release

00:03:00,080 --> 00:03:05,440
i mean except from these few changes

00:03:02,840 --> 00:03:06,480
usually it's not that change even if you

00:03:05,440 --> 00:03:09,680
have several features

00:03:06,480 --> 00:03:12,319
which are match which we usually tend to

00:03:09,680 --> 00:03:13,920
to merge us feature a small feature step

00:03:12,319 --> 00:03:17,519
by step instead of doing

00:03:13,920 --> 00:03:18,080
um a big rework so that that was more

00:03:17,519 --> 00:03:21,440
for

00:03:18,080 --> 00:03:24,640
some kind of information about uh

00:03:21,440 --> 00:03:25,840
how the the changes are going on on the

00:03:24,640 --> 00:03:28,959
linux scheduler

00:03:25,840 --> 00:03:31,599
but so we have we have activity but

00:03:28,959 --> 00:03:32,640
yeah the change usually is not that huge

00:03:31,599 --> 00:03:36,000
that might change

00:03:32,640 --> 00:03:38,319
for the next release but usually that

00:03:36,000 --> 00:03:41,200
we tend to be cautious before uh

00:03:38,319 --> 00:03:41,200
changing everything

00:03:41,599 --> 00:03:44,879
so yeah i will go through the new

00:03:43,280 --> 00:03:47,200
feature which have been merged so

00:03:44,879 --> 00:03:50,400
since i've decided to start after the

00:03:47,200 --> 00:03:52,000
5.4 which was the last lts

00:03:50,400 --> 00:03:53,280
so that gives us a few number of

00:03:52,000 --> 00:03:54,400
features which have been merged so i

00:03:53,280 --> 00:03:56,799
have only focused on

00:03:54,400 --> 00:03:57,439
new feature and i have skipped

00:03:56,799 --> 00:04:01,280
everything

00:03:57,439 --> 00:04:02,400
related to the fix the improvement or

00:04:01,280 --> 00:04:05,680
the optimization

00:04:02,400 --> 00:04:08,560
because i mean we have that all the time

00:04:05,680 --> 00:04:11,360
and yeah the list would have been quite

00:04:08,560 --> 00:04:13,200
long so that's why

00:04:11,360 --> 00:04:15,040
so all the features that i mentioned

00:04:13,200 --> 00:04:16,320
there are not the only things that have

00:04:15,040 --> 00:04:18,959
been merged but are just

00:04:16,320 --> 00:04:20,160
new feature which which could impact the

00:04:18,959 --> 00:04:23,840
the way the scheduler

00:04:20,160 --> 00:04:26,320
is uh working so in the fight

00:04:23,840 --> 00:04:28,400
five maybe one of the main uh changes

00:04:26,320 --> 00:04:30,320
was the rework of the load balance

00:04:28,400 --> 00:04:32,960
which was going on for a while so the

00:04:30,320 --> 00:04:37,040
goal was to clean up and clarify the way

00:04:32,960 --> 00:04:40,880
we were balancing the task between each

00:04:37,040 --> 00:04:44,240
cpu and between each node so from a

00:04:40,880 --> 00:04:47,440
normal load balancer point of view uh

00:04:44,240 --> 00:04:49,199
the change in itself is is not that huge

00:04:47,440 --> 00:04:50,800
when you look at the number of lines

00:04:49,199 --> 00:04:52,800
which have been changed and

00:04:50,800 --> 00:04:54,080
the main goal was mainly to not regress

00:04:52,800 --> 00:04:57,520
everything which

00:04:54,080 --> 00:05:00,800
have been i think almost achieved

00:04:57,520 --> 00:05:03,520
and that that has also helped to clarify

00:05:00,800 --> 00:05:04,560
and simplify the the way we were

00:05:03,520 --> 00:05:07,280
selecting uh

00:05:04,560 --> 00:05:08,880
the task to migrate on other side so

00:05:07,280 --> 00:05:12,000
following this few other

00:05:08,880 --> 00:05:15,120
changes have been merged since 5.5

00:05:12,000 --> 00:05:17,680
and another one which was the

00:05:15,120 --> 00:05:20,400
in the ut list a way to to ramp up

00:05:17,680 --> 00:05:20,400
faster the

00:05:21,680 --> 00:05:26,160
estimated utilization of a cpu which

00:05:24,479 --> 00:05:28,320
then is directly used to select the

00:05:26,160 --> 00:05:31,360
frequency

00:05:28,320 --> 00:05:32,080
and the goal was to speed up frequency

00:05:31,360 --> 00:05:33,840
boost

00:05:32,080 --> 00:05:35,440
when new activity was going on or was

00:05:33,840 --> 00:05:38,880
increasing which

00:05:35,440 --> 00:05:41,039
was given some performance improvement

00:05:38,880 --> 00:05:42,320
and the last one was a new a new way to

00:05:41,039 --> 00:05:45,440
access the

00:05:42,320 --> 00:05:46,240
can recipes that so that's a way to get

00:05:45,440 --> 00:05:48,560
everything

00:05:46,240 --> 00:05:49,520
in once instead of accessing different

00:05:48,560 --> 00:05:52,800
the future

00:05:49,520 --> 00:05:54,080
field one by one on the next kernel

00:05:52,800 --> 00:05:56,720
really so we have more

00:05:54,080 --> 00:05:58,639
feature uh maybe one thing which is

00:05:56,720 --> 00:06:00,479
interesting is the arctic capacity

00:05:58,639 --> 00:06:01,199
awareness which have been merged in this

00:06:00,479 --> 00:06:04,080
so

00:06:01,199 --> 00:06:06,080
until now only the until before this

00:06:04,080 --> 00:06:10,000
version only the

00:06:06,080 --> 00:06:13,919
cfs was aware of the

00:06:10,000 --> 00:06:14,720
the capacity of different capacity in

00:06:13,919 --> 00:06:17,600
the system

00:06:14,720 --> 00:06:19,360
and with this starting from this kernel

00:06:17,600 --> 00:06:22,080
version so

00:06:19,360 --> 00:06:24,080
even the rt tasks are placed on the cpu

00:06:22,080 --> 00:06:27,680
depending of its capacity

00:06:24,080 --> 00:06:30,639
uh there were also other small uh so the

00:06:27,680 --> 00:06:32,160
the es also was using the the u-clamp in

00:06:30,639 --> 00:06:34,880
order to select the cpu

00:06:32,160 --> 00:06:38,240
and not only selecting the frequency

00:06:34,880 --> 00:06:40,960
which was a new feature that was enabled

00:06:38,240 --> 00:06:42,240
um a task to be put only always on big

00:06:40,960 --> 00:06:46,479
core for example

00:06:42,240 --> 00:06:48,479
instead of just increasing the frequency

00:06:46,479 --> 00:06:50,319
and also another feature which was

00:06:48,479 --> 00:06:53,039
interesting uh

00:06:50,319 --> 00:06:54,310
was about the scheduled task the goal

00:06:53,039 --> 00:06:55,599
was really to make them

00:06:54,310 --> 00:06:58,639
[Music]

00:06:55,599 --> 00:07:01,039
easily and aggressively preemptable

00:06:58,639 --> 00:07:02,800
by cfs tasks so the main goal of this is

00:07:01,039 --> 00:07:05,199
really to

00:07:02,800 --> 00:07:06,800
i would say to put some background task

00:07:05,199 --> 00:07:08,639
to to use the

00:07:06,800 --> 00:07:10,240
the spare cycle of a cpu but to not

00:07:08,639 --> 00:07:12,639
disturb your main your main

00:07:10,240 --> 00:07:14,160
your main load on your system so that

00:07:12,639 --> 00:07:16,960
can be either on

00:07:14,160 --> 00:07:19,199
on the server so you can use a this

00:07:16,960 --> 00:07:19,599
spare cycle to do some transcoding or

00:07:19,199 --> 00:07:21,840
what

00:07:19,599 --> 00:07:23,440
whatever you want or even on a mobile

00:07:21,840 --> 00:07:25,680
the goal might be to do some background

00:07:23,440 --> 00:07:27,919
activity here

00:07:25,680 --> 00:07:28,840
so but i'm not sure that it's using it

00:07:27,919 --> 00:07:31,840
on mobile

00:07:28,840 --> 00:07:31,840
side

00:07:32,000 --> 00:07:37,759
also there were ces another feature

00:07:35,520 --> 00:07:39,680
which is related to the pneuma

00:07:37,759 --> 00:07:41,759
that's a low small imbalance up to now

00:07:39,680 --> 00:07:43,520
we were trying to be fully

00:07:41,759 --> 00:07:45,840
balanced between the nut but in some

00:07:43,520 --> 00:07:45,840
case

00:07:46,000 --> 00:07:50,720
entering an even balance between the nut

00:07:49,440 --> 00:07:53,120
is not really useful

00:07:50,720 --> 00:07:54,160
especially when you have a lot of a

00:07:53,120 --> 00:07:56,639
spare cycle on

00:07:54,160 --> 00:07:59,199
each node that doesn't really impact

00:07:56,639 --> 00:08:02,479
your bandwidth or your throughput

00:07:59,199 --> 00:08:03,840
and that help to share the the data

00:08:02,479 --> 00:08:05,759
locality between the

00:08:03,840 --> 00:08:07,199
the tasks so that's why this have been

00:08:05,759 --> 00:08:10,560
introduced and

00:08:07,199 --> 00:08:12,000
this is then used to to improve this

00:08:10,560 --> 00:08:15,280
kind of imbalance

00:08:12,000 --> 00:08:17,120
to which up to which level we can allow

00:08:15,280 --> 00:08:17,919
an imbalance between two nodes so that's

00:08:17,120 --> 00:08:20,960
something

00:08:17,919 --> 00:08:23,039
which is not yet fully um

00:08:20,960 --> 00:08:24,000
we say optimize so right now it's quite

00:08:23,039 --> 00:08:26,960
conservative

00:08:24,000 --> 00:08:28,560
but the goal is that to improve this and

00:08:26,960 --> 00:08:31,360
to

00:08:28,560 --> 00:08:31,919
and maybe enforce the end goal will be

00:08:31,360 --> 00:08:34,800
to

00:08:31,919 --> 00:08:35,919
uh to to be able to keep a several tasks

00:08:34,800 --> 00:08:40,320
on the seminar

00:08:35,919 --> 00:08:41,839
to ensure throughput optimization

00:08:40,320 --> 00:08:43,680
the last one has been a small

00:08:41,839 --> 00:08:44,080
improvement and if i remember correctly

00:08:43,680 --> 00:08:47,440
it was

00:08:44,080 --> 00:08:50,640
especially for xfs file system

00:08:47,440 --> 00:08:52,880
and the goal is that when there is this

00:08:50,640 --> 00:08:54,240
use case where you are using a k thread

00:08:52,880 --> 00:08:57,360
which in fact

00:08:54,240 --> 00:08:59,519
so you have a main test that user case

00:08:57,360 --> 00:09:00,560
that send an event and then a case rate

00:08:59,519 --> 00:09:02,480
is working up

00:09:00,560 --> 00:09:04,880
to acknowledge the content and wake up

00:09:02,480 --> 00:09:08,560
then the main task

00:09:04,880 --> 00:09:11,360
and with this so before this patch

00:09:08,560 --> 00:09:12,160
we we can see that there were ping-pong

00:09:11,360 --> 00:09:14,800
between a

00:09:12,160 --> 00:09:16,640
several cpu because when the main task

00:09:14,800 --> 00:09:17,120
was waking up the case thread was

00:09:16,640 --> 00:09:20,640
running

00:09:17,120 --> 00:09:22,320
on its previous cpu which

00:09:20,640 --> 00:09:24,000
so we were selecting another one and

00:09:22,320 --> 00:09:24,800
then the case thread the new case thread

00:09:24,000 --> 00:09:28,000
was used

00:09:24,800 --> 00:09:30,160
on this new cpu and so on so now um when

00:09:28,000 --> 00:09:33,839
a case trade wake up another task and

00:09:30,160 --> 00:09:37,040
it is the only one running on the cpu

00:09:33,839 --> 00:09:40,160
the task the week the the working task

00:09:37,040 --> 00:09:42,560
stay on the same cpu uh

00:09:40,160 --> 00:09:43,200
we make the assumption that the case

00:09:42,560 --> 00:09:47,120
thread will

00:09:43,200 --> 00:09:49,360
slip close really soon and it was just

00:09:47,120 --> 00:09:51,360
after waking up the cpu so the goal is

00:09:49,360 --> 00:09:54,000
to improve the locality the data

00:09:51,360 --> 00:09:54,000
locality

00:09:56,000 --> 00:10:03,519
so this was for the 5.6 for the next one

00:09:59,600 --> 00:10:04,720
on 5.7 so yeah the frequency invariance

00:10:03,519 --> 00:10:08,720
has been merged for the

00:10:04,720 --> 00:10:10,640
intel for intel platform so

00:10:08,720 --> 00:10:12,480
it's already there it was already there

00:10:10,640 --> 00:10:15,040
for

00:10:12,480 --> 00:10:16,880
arm platform which are using a cpu flex

00:10:15,040 --> 00:10:18,959
dt

00:10:16,880 --> 00:10:21,279
driver which is the one used by most of

00:10:18,959 --> 00:10:24,800
the embedded system

00:10:21,279 --> 00:10:25,440
uh that also means that if you are using

00:10:24,800 --> 00:10:29,040
other

00:10:25,440 --> 00:10:30,399
cpu flag drivers like the cpu ppc on amp

00:10:29,040 --> 00:10:32,720
platform for example you don't have the

00:10:30,399 --> 00:10:34,880
frequency in variance

00:10:32,720 --> 00:10:35,839
but now there are some patches which are

00:10:34,880 --> 00:10:37,839
in the

00:10:35,839 --> 00:10:41,200
discussion and review to enable that

00:10:37,839 --> 00:10:41,200
also for the arm server

00:10:41,360 --> 00:10:46,880
also among the the change from in the

00:10:44,160 --> 00:10:50,800
statistic we have removed a

00:10:46,880 --> 00:10:52,640
renewable load average which was uh

00:10:50,800 --> 00:10:54,160
created based on the load of the task

00:10:52,640 --> 00:10:55,760
and the fact that it was run able by

00:10:54,160 --> 00:10:56,959
something much more simple which was the

00:10:55,760 --> 00:11:00,240
runnable average

00:10:56,959 --> 00:11:03,920
so it's only it's it's monitoring only

00:11:00,240 --> 00:11:05,839
how long you are ready to run and

00:11:03,920 --> 00:11:07,600
that include the running time but also

00:11:05,839 --> 00:11:10,720
your waiting time

00:11:07,600 --> 00:11:12,720
so this has simplified the

00:11:10,720 --> 00:11:14,399
the metric how we were computing the

00:11:12,720 --> 00:11:14,880
metric in the scheduler and especially

00:11:14,399 --> 00:11:18,160
the

00:11:14,880 --> 00:11:21,360
hot pass and this gave a bit a

00:11:18,160 --> 00:11:22,320
more stable value so another interesting

00:11:21,360 --> 00:11:23,680
feature

00:11:22,320 --> 00:11:25,440
for the scheduler was the thermal

00:11:23,680 --> 00:11:27,680
pressure where

00:11:25,440 --> 00:11:29,839
the thermal framework can give feedback

00:11:27,680 --> 00:11:30,800
about which part of the system is under

00:11:29,839 --> 00:11:36,320
pressure

00:11:30,800 --> 00:11:36,320
and and have a reduced compute capacity

00:11:37,440 --> 00:11:41,200
also i mean something interesting for

00:11:39,279 --> 00:11:43,440
the embedded system is the dynamic

00:11:41,200 --> 00:11:44,560
the support of dynamic for fast wake up

00:11:43,440 --> 00:11:46,399
pass

00:11:44,560 --> 00:11:47,760
to now there was it was not fully

00:11:46,399 --> 00:11:50,240
handled because

00:11:47,760 --> 00:11:52,320
on dynamic system arm dynamic system all

00:11:50,240 --> 00:11:52,880
the big and the little uh belong to the

00:11:52,320 --> 00:11:55,839
same

00:11:52,880 --> 00:11:57,519
last level of cache whereas the previous

00:11:55,839 --> 00:12:00,880
assumption for

00:11:57,519 --> 00:12:02,959
eternal system was that

00:12:00,880 --> 00:12:04,720
each big and little were belonging to

00:12:02,959 --> 00:12:08,000
different the last level of cash

00:12:04,720 --> 00:12:11,920
and then the the balancing was done at a

00:12:08,000 --> 00:12:13,120
upper level and then another thing that

00:12:11,920 --> 00:12:15,920
was interesting is the

00:12:13,120 --> 00:12:17,680
distribution task so yeah when some

00:12:15,920 --> 00:12:19,040
tasks are forking they are looking for

00:12:17,680 --> 00:12:20,959
the

00:12:19,040 --> 00:12:22,800
the first available cpu and the first

00:12:20,959 --> 00:12:24,800
idle cpu but in some case

00:12:22,800 --> 00:12:26,160
all the new forklift tasks were

00:12:24,800 --> 00:12:28,880
selecting the same

00:12:26,160 --> 00:12:30,320
the same cpu so some changes have been

00:12:28,880 --> 00:12:30,959
done to make sure that we'll use the

00:12:30,320 --> 00:12:35,760
next one

00:12:30,959 --> 00:12:35,760
each after each and every every changes

00:12:36,839 --> 00:12:42,399
so uh and then

00:12:39,040 --> 00:12:45,440
so on the 5.8 yeah there were

00:12:42,399 --> 00:12:48,399
few new features

00:12:45,440 --> 00:12:50,560
which are mainly uh supporting uh the

00:12:48,399 --> 00:12:53,200
case where the load of the task is new

00:12:50,560 --> 00:12:55,279
which can happen when you have some huge

00:12:53,200 --> 00:12:58,079
number of cpu or huge number

00:12:55,279 --> 00:12:58,079
or a deep

00:12:59,279 --> 00:13:02,399
c group topology and also the way we're

00:13:01,920 --> 00:13:04,320
using

00:13:02,399 --> 00:13:06,160
the runnable average but yeah this one

00:13:04,320 --> 00:13:06,720
was required from a new feature point of

00:13:06,160 --> 00:13:09,040
view

00:13:06,720 --> 00:13:11,440
there were a lot of fix and cleanup but

00:13:09,040 --> 00:13:14,480
no no real new feature

00:13:11,440 --> 00:13:18,079
and then the 5.9 which i think we are

00:13:14,480 --> 00:13:21,440
quite close to the rc7 now

00:13:18,079 --> 00:13:24,399
so uh we have the deadline

00:13:21,440 --> 00:13:25,360
so the deadline will have the capacity

00:13:24,399 --> 00:13:27,760
awareness

00:13:25,360 --> 00:13:30,480
it was the last scheduling class which

00:13:27,760 --> 00:13:32,079
was not capacity aware

00:13:30,480 --> 00:13:33,519
and also there is a remove that's an

00:13:32,079 --> 00:13:37,839
important thing also

00:13:33,519 --> 00:13:39,600
of the sketch set scheduler export so

00:13:37,839 --> 00:13:42,079
the goal is to prevent some modules to

00:13:39,600 --> 00:13:45,920
set their um

00:13:42,079 --> 00:13:48,320
rt priority because we consider that

00:13:45,920 --> 00:13:51,120
there is nowhere when you are selecting

00:13:48,320 --> 00:13:53,440
early priority this must be done

00:13:51,120 --> 00:13:54,639
having in mind the other artist right

00:13:53,440 --> 00:13:58,000
which are running with you

00:13:54,639 --> 00:13:58,000
which is not really possible

00:13:58,160 --> 00:14:01,760
from a module so these have been

00:13:59,839 --> 00:14:04,720
simplified and there is only some

00:14:01,760 --> 00:14:06,399
by default there is some fuel value and

00:14:04,720 --> 00:14:08,800
then it's up to the

00:14:06,399 --> 00:14:10,079
system admin tool to to set that to the

00:14:08,800 --> 00:14:12,000
right value

00:14:10,079 --> 00:14:14,160
another interesting one is the you clamp

00:14:12,000 --> 00:14:15,199
the the support of a global air t boost

00:14:14,160 --> 00:14:16,880
value

00:14:15,199 --> 00:14:19,920
so the goal is to be able to make sure

00:14:16,880 --> 00:14:23,040
that you will run at

00:14:19,920 --> 00:14:25,199
at the value at a cpu frag value for rt

00:14:23,040 --> 00:14:28,399
task and not only the max

00:14:25,199 --> 00:14:30,240
cpu frequency and also something

00:14:28,399 --> 00:14:33,760
interesting especially for server

00:14:30,240 --> 00:14:37,040
is the a better spread of default tasks

00:14:33,760 --> 00:14:40,160
and these have been done especially

00:14:37,040 --> 00:14:41,760
for a system using omp for example

00:14:40,160 --> 00:14:43,360
where you have a lot of tasks which are

00:14:41,760 --> 00:14:45,440
created and the goal is to spread them

00:14:43,360 --> 00:14:47,360
directly at fork instead of waiting them

00:14:45,440 --> 00:14:50,639
to start to be used

00:14:47,360 --> 00:14:52,240
before balancing them so these are the

00:14:50,639 --> 00:14:56,399
main features which

00:14:52,240 --> 00:14:58,399
have been merged then

00:14:56,399 --> 00:14:59,519
let's come to the new one which are in

00:14:58,399 --> 00:15:02,800
the discussion

00:14:59,519 --> 00:15:08,079
uh so i have selected

00:15:02,800 --> 00:15:09,839
six of them which different level of

00:15:08,079 --> 00:15:12,240
of readiness so the first one is the

00:15:09,839 --> 00:15:13,680
fairness of the cfs so this one i would

00:15:12,240 --> 00:15:15,760
say

00:15:13,680 --> 00:15:16,880
it's quite close to compilation the

00:15:15,760 --> 00:15:20,720
patch is there this

00:15:16,880 --> 00:15:21,680
has been tested and an act by some

00:15:20,720 --> 00:15:24,160
people

00:15:21,680 --> 00:15:27,519
so we had some problem of the fairness

00:15:24,160 --> 00:15:29,360
running time distribution

00:15:27,519 --> 00:15:31,440
on the on the scheduler depending of the

00:15:29,360 --> 00:15:34,399
topology and the number of tasks

00:15:31,440 --> 00:15:36,000
so we now have some patch which enable a

00:15:34,399 --> 00:15:39,120
bit of fairness

00:15:36,000 --> 00:15:42,240
distribution and this can be useful

00:15:39,120 --> 00:15:44,079
for example especially when you're

00:15:42,240 --> 00:15:46,720
starting several steps thread and you're

00:15:44,079 --> 00:15:50,000
waiting for the last one to finish

00:15:46,720 --> 00:15:52,399
before moving to the next step

00:15:50,000 --> 00:15:54,000
and in such case if this if you get less

00:15:52,399 --> 00:15:56,959
running time than others you

00:15:54,000 --> 00:16:00,160
will have to wait longer and this can

00:15:56,959 --> 00:16:02,079
decrease your your performance

00:16:00,160 --> 00:16:03,440
um another one which is a better

00:16:02,079 --> 00:16:05,279
collaboration between the

00:16:03,440 --> 00:16:06,639
the new mind the normal balancer so

00:16:05,279 --> 00:16:09,600
that's something

00:16:06,639 --> 00:16:11,519
several patcher going on um on the

00:16:09,600 --> 00:16:12,399
mailing list and we're also working on

00:16:11,519 --> 00:16:14,399
that

00:16:12,399 --> 00:16:15,839
so now that we have reached a level

00:16:14,399 --> 00:16:17,920
where the number balancer

00:16:15,839 --> 00:16:19,600
and the normal load balancer have the

00:16:17,920 --> 00:16:23,120
same

00:16:19,600 --> 00:16:25,440
view of the load of the system

00:16:23,120 --> 00:16:28,399
at least from a cpu load point of view

00:16:25,440 --> 00:16:31,759
and a cpu running time point of view

00:16:28,399 --> 00:16:33,040
and the goal now is ready to make sure

00:16:31,759 --> 00:16:36,800
that

00:16:33,040 --> 00:16:40,160
they are both looking on the same way

00:16:36,800 --> 00:16:42,720
when they're balancing the task and

00:16:40,160 --> 00:16:44,959
probably the the main the next main

00:16:42,720 --> 00:16:47,199
difficulties will be to make sure how to

00:16:44,959 --> 00:16:50,320
improve the throughput and how to

00:16:47,199 --> 00:16:53,920
would say load this imbalance you know

00:16:50,320 --> 00:16:56,320
from a lot point of view

00:16:53,920 --> 00:16:57,600
in order to improve data locality and

00:16:56,320 --> 00:17:00,240
throughput

00:16:57,600 --> 00:17:00,800
so that will be something uh we have

00:17:00,240 --> 00:17:03,199
some people

00:17:00,800 --> 00:17:04,559
in line who have started to work to look

00:17:03,199 --> 00:17:07,120
at that

00:17:04,559 --> 00:17:09,360
and we'll continue working on this and

00:17:07,120 --> 00:17:12,799
see how we can improve the

00:17:09,360 --> 00:17:13,120
the overall performance so but i know

00:17:12,799 --> 00:17:15,520
that

00:17:13,120 --> 00:17:16,319
uh several other people are working on

00:17:15,520 --> 00:17:19,520
this on

00:17:16,319 --> 00:17:21,520
on reddit on intel side and so on so i

00:17:19,520 --> 00:17:23,039
expect more patch to be available on the

00:17:21,520 --> 00:17:23,919
mailing list soon but that will be i

00:17:23,039 --> 00:17:27,280
would say

00:17:23,919 --> 00:17:30,720
probably the one of the next main

00:17:27,280 --> 00:17:34,960
area of changes in the scheduler

00:17:30,720 --> 00:17:34,960
from a lot a lot balance point of view

00:17:35,120 --> 00:17:39,280
uh i think yeah we'll speed up a bit so

00:17:38,960 --> 00:17:41,760
then

00:17:39,280 --> 00:17:42,720
another feature which which have been

00:17:41,760 --> 00:17:44,240
discussed and is

00:17:42,720 --> 00:17:46,080
still discussed for a while that's the

00:17:44,240 --> 00:17:50,080
car scheduling

00:17:46,080 --> 00:17:54,240
feature and the goal is mainly uh

00:17:50,080 --> 00:17:56,080
to so the first goal is to make sure

00:17:54,240 --> 00:17:59,120
that

00:17:56,080 --> 00:18:00,320
the task of the the thread that will run

00:17:59,120 --> 00:18:04,080
on the same

00:18:00,320 --> 00:18:06,880
c core cpu and the same core will trust

00:18:04,080 --> 00:18:09,440
each other in order to prevent some

00:18:06,880 --> 00:18:12,480
speculative

00:18:09,440 --> 00:18:16,160
attack and so on so that's

00:18:12,480 --> 00:18:19,760
mainly for for smt based system

00:18:16,160 --> 00:18:22,240
and this is based on uh you you

00:18:19,760 --> 00:18:23,840
you set a cookie for each stats which

00:18:22,240 --> 00:18:24,880
trust is total and the goal is to make

00:18:23,840 --> 00:18:28,799
sure that

00:18:24,880 --> 00:18:33,440
at each and any time

00:18:28,799 --> 00:18:36,720
on one car only thread that

00:18:33,440 --> 00:18:40,400
trust each other will run simultaneously

00:18:36,720 --> 00:18:41,440
and if we can't find enough straight in

00:18:40,400 --> 00:18:44,720
this case

00:18:41,440 --> 00:18:45,360
we will force one's cpu uh to be idle on

00:18:44,720 --> 00:18:47,520
the core

00:18:45,360 --> 00:18:48,720
while the other one is running a

00:18:47,520 --> 00:18:51,760
straight

00:18:48,720 --> 00:18:53,760
so this has a lot of impact uh in

00:18:51,760 --> 00:18:54,240
several areas so on the load balance on

00:18:53,760 --> 00:18:56,720
the

00:18:54,240 --> 00:18:57,679
on the test selection especially because

00:18:56,720 --> 00:19:00,720
now

00:18:57,679 --> 00:19:01,600
by default the scheduler is a per cpu or

00:19:00,720 --> 00:19:04,640
pair run queue

00:19:01,600 --> 00:19:06,960
mechanism and in this case you must also

00:19:04,640 --> 00:19:08,160
include a pair of cpu so you must

00:19:06,960 --> 00:19:10,799
look at several run queues

00:19:08,160 --> 00:19:14,080
simultaneously or you must

00:19:10,799 --> 00:19:18,160
coordinate which tasks are running on

00:19:14,080 --> 00:19:20,559
each cpus under all the cpu of a cars

00:19:18,160 --> 00:19:22,799
simultaneously which means a lot of luck

00:19:20,559 --> 00:19:26,880
to be used and so on so

00:19:22,799 --> 00:19:30,880
this has an impact on the performance

00:19:26,880 --> 00:19:33,919
and also um this is quite difficult to

00:19:30,880 --> 00:19:35,840
to keep some kind of fairness between

00:19:33,919 --> 00:19:37,120
the the cpu and the threat running on

00:19:35,840 --> 00:19:38,799
each cpu in a car

00:19:37,120 --> 00:19:41,039
because of this because you have to

00:19:38,799 --> 00:19:42,799
compare

00:19:41,039 --> 00:19:44,240
usually we are using the normally we are

00:19:42,799 --> 00:19:46,080
using the v run time

00:19:44,240 --> 00:19:47,679
to make a decision between which tests

00:19:46,080 --> 00:19:50,320
must run first

00:19:47,679 --> 00:19:50,880
and this variant time is only a per cpu

00:19:50,320 --> 00:19:54,799
per

00:19:50,880 --> 00:19:56,640
parent user so with

00:19:54,799 --> 00:19:57,840
with this scheduling decision at core

00:19:56,640 --> 00:19:59,760
level you have to compare

00:19:57,840 --> 00:20:00,880
the variant time between between run

00:19:59,760 --> 00:20:02,720
queue

00:20:00,880 --> 00:20:04,000
which is not something that is quite

00:20:02,720 --> 00:20:06,159
easy

00:20:04,000 --> 00:20:07,679
and also on the load balance you should

00:20:06,159 --> 00:20:08,400
also take into account the fact that

00:20:07,679 --> 00:20:10,880
something

00:20:08,400 --> 00:20:13,039
some tasks should not be long should not

00:20:10,880 --> 00:20:16,240
be a pulled

00:20:13,039 --> 00:20:20,720
on on a run queue if uh

00:20:16,240 --> 00:20:22,960
if some tasks without a cookie are there

00:20:20,720 --> 00:20:24,880
okay you have only five minutes if i'm

00:20:22,960 --> 00:20:27,919
not wrong so the latency nice

00:20:24,880 --> 00:20:30,400
also that's a new a new way to set uh

00:20:27,919 --> 00:20:31,039
more and more people are complaining

00:20:30,400 --> 00:20:33,760
about

00:20:31,039 --> 00:20:36,720
the fact that you can't ask you can't

00:20:33,760 --> 00:20:40,320
say if a task a cfs task

00:20:36,720 --> 00:20:42,480
is a latency sensitive or not and should

00:20:40,320 --> 00:20:42,480
be

00:20:43,280 --> 00:20:48,640
promoted in the in the when we are

00:20:45,679 --> 00:20:51,760
selecting for the next task

00:20:48,640 --> 00:20:54,559
to be running a cpu

00:20:51,760 --> 00:20:56,400
so the goal would be to create a new a

00:20:54,559 --> 00:20:59,200
new kind of a

00:20:56,400 --> 00:20:59,919
latency priority similarly to what we

00:20:59,200 --> 00:21:03,840
have for the

00:20:59,919 --> 00:21:06,240
niceness of a cfs task

00:21:03,840 --> 00:21:07,280
the the discussion is going on for for a

00:21:06,240 --> 00:21:09,520
while now

00:21:07,280 --> 00:21:11,120
and we have the main problem that we

00:21:09,520 --> 00:21:13,360
have right now so i think that

00:21:11,120 --> 00:21:15,440
now everybody agree between the use of

00:21:13,360 --> 00:21:19,840
the latency nice which will be

00:21:15,440 --> 00:21:23,120
between minus 19 to 20 minus 19 min

00:21:19,840 --> 00:21:25,840
that you you are uh you have some

00:21:23,120 --> 00:21:27,039
uh you're more sensible to latency and

00:21:25,840 --> 00:21:28,400
minus 20

00:21:27,039 --> 00:21:30,400
saying that you don't care about your

00:21:28,400 --> 00:21:33,520
scheduling latency

00:21:30,400 --> 00:21:34,080
now the discussion is mainly about what

00:21:33,520 --> 00:21:37,520
to do

00:21:34,080 --> 00:21:42,000
behind this kind of priority and

00:21:37,520 --> 00:21:44,720
so there are three four main

00:21:42,000 --> 00:21:46,000
main behavior that people want to put

00:21:44,720 --> 00:21:49,600
below that

00:21:46,000 --> 00:21:53,039
uh some are some time i would say a bit

00:21:49,600 --> 00:21:55,280
at the opposite direction so

00:21:53,039 --> 00:21:56,880
there is one which is mainly right now

00:21:55,280 --> 00:21:58,960
when it does wake up

00:21:56,880 --> 00:22:01,120
and when we are looking at uh if we can

00:21:58,960 --> 00:22:04,799
pre-empt the current one

00:22:01,120 --> 00:22:08,000
we are using uh some uh

00:22:04,799 --> 00:22:11,679
slice to to make the decision so once

00:22:08,000 --> 00:22:14,080
one idea is to use this to shorten

00:22:11,679 --> 00:22:15,600
the slice time before in order to be

00:22:14,080 --> 00:22:18,159
able to preempt

00:22:15,600 --> 00:22:19,520
the task with a higher latency nice

00:22:18,159 --> 00:22:22,159
value

00:22:19,520 --> 00:22:25,039
more easily and at the opposite to wait

00:22:22,159 --> 00:22:28,320
a bit if you have a latency knife

00:22:25,039 --> 00:22:30,080
latency nice value so this seems to be

00:22:28,320 --> 00:22:30,799
quite reasonable and in line with what

00:22:30,080 --> 00:22:34,799
that mean

00:22:30,799 --> 00:22:36,480
then we have another another behavior

00:22:34,799 --> 00:22:38,080
which is mainly for embedded system

00:22:36,480 --> 00:22:40,080
where you want to

00:22:38,080 --> 00:22:42,240
select for an idle cpu when you are

00:22:40,080 --> 00:22:44,480
latency nice sensitive

00:22:42,240 --> 00:22:46,320
so the goal is really to to run on that

00:22:44,480 --> 00:22:47,919
you have a small system usually eight

00:22:46,320 --> 00:22:50,159
core and you want to look for an idle

00:22:47,919 --> 00:22:52,400
cpu if any

00:22:50,159 --> 00:22:54,159
and i would say that at the opposite on

00:22:52,400 --> 00:22:57,120
some large system

00:22:54,159 --> 00:23:00,000
or you would like to not look at any

00:22:57,120 --> 00:23:02,240
other cpu but just say okay stay on

00:23:00,000 --> 00:23:03,840
remain on the same cpu because you know

00:23:02,240 --> 00:23:05,840
that

00:23:03,840 --> 00:23:07,600
you will be scheduled really soon and

00:23:05,840 --> 00:23:08,159
you will it will take more time to look

00:23:07,600 --> 00:23:10,080
for

00:23:08,159 --> 00:23:12,320
another cpu than just waiting for you to

00:23:10,080 --> 00:23:15,600
be uh scheduled

00:23:12,320 --> 00:23:17,520
so i expect the discussion to to move

00:23:15,600 --> 00:23:22,080
forward on this during the

00:23:17,520 --> 00:23:25,520
last couple of months in order to reach

00:23:22,080 --> 00:23:27,679
a decision and an agreement

00:23:25,520 --> 00:23:29,679
the last the next one is the proxy

00:23:27,679 --> 00:23:31,280
execution that's also something that is

00:23:29,679 --> 00:23:34,559
going on for a while

00:23:31,280 --> 00:23:38,000
and the main idea behind that so is

00:23:34,559 --> 00:23:41,600
that instead of uh on

00:23:38,000 --> 00:23:44,720
around a thread you have an execution

00:23:41,600 --> 00:23:48,559
context and

00:23:44,720 --> 00:23:49,520
the goal is that uh not only one thread

00:23:48,559 --> 00:23:50,960
but another

00:23:49,520 --> 00:23:52,880
several thread can use the same

00:23:50,960 --> 00:23:56,000
execution context to run

00:23:52,880 --> 00:23:57,360
and typically this is useful for i think

00:23:56,000 --> 00:23:58,400
that from the beginning it was for the

00:23:57,360 --> 00:24:01,200
deadline task

00:23:58,400 --> 00:24:02,720
and especially when you are deadline

00:24:01,200 --> 00:24:05,919
tests and you want to

00:24:02,720 --> 00:24:08,159
you take a mutex but this one is already

00:24:05,919 --> 00:24:09,600
already taken by a cfs test in this case

00:24:08,159 --> 00:24:10,960
you're blocked

00:24:09,600 --> 00:24:15,200
and you have to wait the other one to

00:24:10,960 --> 00:24:15,200
run but how do you really

00:24:15,279 --> 00:24:19,120
promote this because it's not only

00:24:17,440 --> 00:24:21,039
priority but you have to take into

00:24:19,120 --> 00:24:21,840
account this kind of bandwidths and so

00:24:21,039 --> 00:24:24,000
on

00:24:21,840 --> 00:24:26,400
so the goal would be to in this case to

00:24:24,000 --> 00:24:30,159
make the normal cfs task running

00:24:26,400 --> 00:24:32,080
in the deadline execution context

00:24:30,159 --> 00:24:34,000
so it will use the the runtime

00:24:32,080 --> 00:24:36,240
boundaries and so on

00:24:34,000 --> 00:24:38,960
so that will be useful for that and also

00:24:36,240 --> 00:24:42,880
maybe that can be useful for

00:24:38,960 --> 00:24:43,679
in order yet to to make some kind of uh

00:24:42,880 --> 00:24:47,840
to simplify

00:24:43,679 --> 00:24:47,840
the artist rustling stuff

00:24:48,159 --> 00:24:53,520
and order even the cfs wrestling stuff

00:24:51,360 --> 00:24:56,559
because you can encapsulate that in a

00:24:53,520 --> 00:24:59,919
deadline execution context

00:24:56,559 --> 00:25:01,679
so this one is not yet really mature but

00:24:59,919 --> 00:25:04,000
i think that's something that is used

00:25:01,679 --> 00:25:05,039
that would be quite useful for us that

00:25:04,000 --> 00:25:09,520
will simplify a lot

00:25:05,039 --> 00:25:11,279
the some priority inversion problem and

00:25:09,520 --> 00:25:14,320
so on

00:25:11,279 --> 00:25:16,640
the last one i've just added that

00:25:14,320 --> 00:25:17,840
because it's it's have a rise on the

00:25:16,640 --> 00:25:20,240
mailing list

00:25:17,840 --> 00:25:20,960
recently that's about this migrate

00:25:20,240 --> 00:25:22,960
disabled

00:25:20,960 --> 00:25:24,080
spot for the rt kernel so in order to

00:25:22,960 --> 00:25:27,360
have a full

00:25:24,080 --> 00:25:28,240
prime 30 kernel or some discussion to be

00:25:27,360 --> 00:25:30,159
able to

00:25:28,240 --> 00:25:31,679
disable the regret of a task and the

00:25:30,159 --> 00:25:33,520
main goal behind that

00:25:31,679 --> 00:25:35,600
is just to make sure that when a task is

00:25:33,520 --> 00:25:38,080
running or when you are in

00:25:35,600 --> 00:25:40,080
in some context you don't want to be

00:25:38,080 --> 00:25:42,320
migrated until the end of this section

00:25:40,080 --> 00:25:45,200
which can be critical

00:25:42,320 --> 00:25:48,080
but this allow also some other part to

00:25:45,200 --> 00:25:50,640
be preemptable

00:25:48,080 --> 00:25:53,679
i'm afraid that i'm going out of time

00:25:50,640 --> 00:25:56,400
we're already 26

00:25:53,679 --> 00:25:57,360
so i will not i will stop now maybe i

00:25:56,400 --> 00:26:00,559
can text

00:25:57,360 --> 00:26:01,600
few questions i will i can continue if

00:26:00,559 --> 00:26:04,960
there is any question

00:26:01,600 --> 00:26:04,960
i can continue on the slack

00:26:05,600 --> 00:26:11,360
interface yeah hold on someone someone

00:26:08,880 --> 00:26:11,360
raised height

00:26:19,600 --> 00:26:26,000
hi i have a question about the um

00:26:23,440 --> 00:26:26,799
the the new thing you mentioned new

00:26:26,000 --> 00:26:30,240
spreading

00:26:26,799 --> 00:26:33,679
in 5.9 what happens exactly

00:26:30,240 --> 00:26:36,720
you guarantee that if like if a

00:26:33,679 --> 00:26:38,159
thread does if you have 100 cores and if

00:26:36,720 --> 00:26:39,760
you do 100 news

00:26:38,159 --> 00:26:42,720
that they will all end up on different

00:26:39,760 --> 00:26:46,320
course yes that's the goal

00:26:42,720 --> 00:26:46,720
and uh and yeah that's really what we

00:26:46,320 --> 00:26:49,039
have to

00:26:46,720 --> 00:26:51,440
what we have tried to do and normally

00:26:49,039 --> 00:26:55,120
that's what is happening

00:26:51,440 --> 00:26:58,480
so often i see that um

00:26:55,120 --> 00:27:02,159
i mean it takes some time to do 100 news

00:26:58,480 --> 00:27:03,120
and in in those threads when they do

00:27:02,159 --> 00:27:04,880
their news

00:27:03,120 --> 00:27:06,640
they maybe they just do a little bit of

00:27:04,880 --> 00:27:08,960
initialization and then they block for

00:27:06,640 --> 00:27:12,159
the other news to be done

00:27:08,960 --> 00:27:12,799
and so then um you have maybe 10 news

00:27:12,159 --> 00:27:14,880
that happen

00:27:12,799 --> 00:27:16,240
and then they all start blocking and

00:27:14,880 --> 00:27:18,559
then the cores

00:27:16,240 --> 00:27:20,080
those 10 cores are going to look like

00:27:18,559 --> 00:27:22,399
they're idle

00:27:20,080 --> 00:27:24,240
so do you still keep moving them on to

00:27:22,399 --> 00:27:27,039
other cores even though some of them

00:27:24,240 --> 00:27:28,399
close by cores seem to be idle yeah

00:27:27,039 --> 00:27:31,520
that's what

00:27:28,399 --> 00:27:36,399
we are we have fixed with this uh

00:27:31,520 --> 00:27:36,399
typically this problem okay great thanks

00:27:42,399 --> 00:27:45,279
any other questions

00:27:45,919 --> 00:27:51,279
three minutes over but yeah we have five

00:27:48,960 --> 00:27:55,679
minutes buffer so if there uh let's take

00:27:51,279 --> 00:27:55,679
a last question if there is any

00:27:57,520 --> 00:28:00,159
okay

00:28:01,120 --> 00:28:04,480
no more questions thank you winter thank

00:28:04,080 --> 00:28:07,360
you

00:28:04,480 --> 00:28:07,360

YouTube URL: https://www.youtube.com/watch?v=SWvlXMaOQqE


