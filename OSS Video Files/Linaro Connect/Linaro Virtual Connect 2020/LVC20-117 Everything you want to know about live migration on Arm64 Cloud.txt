Title: LVC20-117 Everything you want to know about live migration on Arm64 Cloud
Publication date: 2020-10-09
Playlist: Linaro Virtual Connect 2020
Description: 
	Everything you want to know about live migration on Arm64 Cloud

Currently, one big gap between Arm64 and X86 cloud platforms is that X86 can provide a much better instance migration experience than the Arm64 platform. CPU comparison and CPU model capabilities have provided Arm64 VM with the ability to live migration among different hardware vendors. This function is the essential function of the data center. From the cloud management framework, we also need to consider the realization of supporting VM live migration.

In this session, we will talk about what we have done in the most widely used virtualization management tool - Libvirt to provide better live migration capabilities on Arm64 platform and also some details in the newest lightweight cloud management project such as Kubevirt.

With live migration support on Arm64, it can finally benefit the cloud ecosystem for large scale datacenter scenarios which may use different Arm64 CPU architectures and vendors.

For the presentation see: https://static.linaro.org/connect/lvc20/presentations/LVC20-117-0.pdf
Captions: 
	00:00:01,360 --> 00:00:06,000
hello everyone welcome to our session

00:00:03,760 --> 00:00:08,320
in this session i will talk about

00:00:06,000 --> 00:00:10,559
virtual machines live migration on arm

00:00:08,320 --> 00:00:12,719
cloud infrastructures

00:00:10,559 --> 00:00:14,160
this session will be divided into two

00:00:12,719 --> 00:00:16,160
parts the first

00:00:14,160 --> 00:00:18,000
in the first part i will talk about some

00:00:16,160 --> 00:00:21,119
background information

00:00:18,000 --> 00:00:25,039
and the main gap between migration on

00:00:21,119 --> 00:00:28,160
arm and x86 platform and what kind of

00:00:25,039 --> 00:00:28,880
improvements did we done in the past few

00:00:28,160 --> 00:00:32,399
months

00:00:28,880 --> 00:00:34,719
to close the gaps and in the second part

00:00:32,399 --> 00:00:38,239
kevin from linaro will go to details

00:00:34,719 --> 00:00:40,640
about lag migration process

00:00:38,239 --> 00:00:42,399
before we go to the contents please let

00:00:40,640 --> 00:00:45,600
me introduce myself

00:00:42,399 --> 00:00:48,320
my name is jen yu jung i'm from huawei

00:00:45,600 --> 00:00:51,600
i'm the contact person of arm open

00:00:48,320 --> 00:00:53,520
source ecosystem of our department

00:00:51,600 --> 00:00:54,879
i've been working on open source for

00:00:53,520 --> 00:00:57,440
about six years

00:00:54,879 --> 00:00:58,239
and here is a qr code for our slack

00:00:57,440 --> 00:01:01,359
channel

00:00:58,239 --> 00:01:03,359
we created for discussion about arm

00:01:01,359 --> 00:01:06,400
server ecosystem

00:01:03,359 --> 00:01:08,240
please join if you are interested okay

00:01:06,400 --> 00:01:10,240
so let's start

00:01:08,240 --> 00:01:11,360
the first thing first what is left

00:01:10,240 --> 00:01:13,680
migration

00:01:11,360 --> 00:01:15,360
in the cloud world migration normally

00:01:13,680 --> 00:01:17,600
means process of

00:01:15,360 --> 00:01:19,439
moving virtual machines of or

00:01:17,600 --> 00:01:20,640
application between different physical

00:01:19,439 --> 00:01:23,040
hosts

00:01:20,640 --> 00:01:23,840
like short in the graph on the right

00:01:23,040 --> 00:01:27,280
yellow

00:01:23,840 --> 00:01:28,080
vm will be migrated from host a to host

00:01:27,280 --> 00:01:32,000
b

00:01:28,080 --> 00:01:34,720
and live migration is one of the

00:01:32,000 --> 00:01:35,280
forms of migrations when migration

00:01:34,720 --> 00:01:37,840
process

00:01:35,280 --> 00:01:39,520
is performed on running virtual machines

00:01:37,840 --> 00:01:42,000
and applications

00:01:39,520 --> 00:01:43,680
and the process can be done without

00:01:42,000 --> 00:01:46,240
disturbing the workloads

00:01:43,680 --> 00:01:48,560
it can be called live migration

00:01:46,240 --> 00:01:49,439
migration and lab migrations are very

00:01:48,560 --> 00:01:52,799
important

00:01:49,439 --> 00:01:54,079
for cloud users so what are the typical

00:01:52,799 --> 00:01:56,880
uses use cases

00:01:54,079 --> 00:01:58,240
for live migrations here we have two

00:01:56,880 --> 00:02:01,439
examples

00:01:58,240 --> 00:02:02,640
the first example case is for hardware

00:02:01,439 --> 00:02:05,280
maintenance

00:02:02,640 --> 00:02:07,280
in production develop deployments the

00:02:05,280 --> 00:02:09,200
cloud provider will always have to

00:02:07,280 --> 00:02:12,640
perform hardware maintenance

00:02:09,200 --> 00:02:14,000
sometimes for example hardware and

00:02:12,640 --> 00:02:18,080
software upgrades

00:02:14,000 --> 00:02:20,560
are the replacement of broken hardware

00:02:18,080 --> 00:02:23,520
when performing the maintenance we do

00:02:20,560 --> 00:02:26,560
not want to disturb our cloud users

00:02:23,520 --> 00:02:27,440
because one of the reason people use

00:02:26,560 --> 00:02:31,120
cloud

00:02:27,440 --> 00:02:33,519
is to avoid hardware failures

00:02:31,120 --> 00:02:34,239
so the cloud provider will have to live

00:02:33,519 --> 00:02:37,519
migrate

00:02:34,239 --> 00:02:38,800
all the vms on host a to host b before

00:02:37,519 --> 00:02:42,000
hardware maintenance

00:02:38,800 --> 00:02:45,840
without disturbing the running vms and

00:02:42,000 --> 00:02:48,640
the applications inside inside them

00:02:45,840 --> 00:02:49,840
the second use case is very also very

00:02:48,640 --> 00:02:52,000
common

00:02:49,840 --> 00:02:55,200
live migration can be used to perform

00:02:52,000 --> 00:02:57,920
load balance on cloud infrastructures

00:02:55,200 --> 00:02:59,200
in the production deployments sometimes

00:02:57,920 --> 00:03:02,080
due to some reasons

00:02:59,200 --> 00:03:04,239
for example some vm's on host b got

00:03:02,080 --> 00:03:07,040
deleted or migrated to

00:03:04,239 --> 00:03:09,680
other hosts causing the workloads on

00:03:07,040 --> 00:03:13,280
different hosts unbalanced

00:03:09,680 --> 00:03:14,400
so the load on host ace is much higher

00:03:13,280 --> 00:03:17,519
than host b

00:03:14,400 --> 00:03:18,720
so as the cloud provider i may want to

00:03:17,519 --> 00:03:22,080
balance the load

00:03:18,720 --> 00:03:25,120
to give each vm better performance

00:03:22,080 --> 00:03:26,799
this will be also need the help of land

00:03:25,120 --> 00:03:29,280
migration

00:03:26,799 --> 00:03:31,040
also sometimes the cloud provider will

00:03:29,280 --> 00:03:33,599
want to do the offset

00:03:31,040 --> 00:03:34,720
to save power they will want to live

00:03:33,599 --> 00:03:38,159
migrate the

00:03:34,720 --> 00:03:42,239
last vm or hospi to host a and then

00:03:38,159 --> 00:03:44,640
turn hospi into sleep mode to save power

00:03:42,239 --> 00:03:45,840
all this is due to cloud providers

00:03:44,640 --> 00:03:49,360
dynamic resource

00:03:45,840 --> 00:03:50,080
location policy but it will need lab

00:03:49,360 --> 00:03:53,120
migration

00:03:50,080 --> 00:03:56,640
as the underneath technology

00:03:53,120 --> 00:03:57,840
and there are also many other cases for

00:03:56,640 --> 00:04:01,840
live migrations

00:03:57,840 --> 00:04:01,840
we will not cover them here

00:04:02,640 --> 00:04:07,840
so before we go to the comparison

00:04:04,879 --> 00:04:09,840
between arm and x86 platform

00:04:07,840 --> 00:04:12,400
i would like to go through a little bit

00:04:09,840 --> 00:04:15,360
about the moon process of typical

00:04:12,400 --> 00:04:17,040
migration process we will only look at

00:04:15,360 --> 00:04:19,440
some basic ideas here

00:04:17,040 --> 00:04:22,000
the implementation details will be

00:04:19,440 --> 00:04:26,160
discussed by kevin later

00:04:22,000 --> 00:04:28,720
so we take openstack as an example

00:04:26,160 --> 00:04:29,919
we will have a controller accepting

00:04:28,720 --> 00:04:32,880
users api

00:04:29,919 --> 00:04:35,520
calls and manage the cloud and the vms

00:04:32,880 --> 00:04:38,639
are running on compute host

00:04:35,520 --> 00:04:40,880
when user calls for live migration the

00:04:38,639 --> 00:04:41,440
controller will receive the call and

00:04:40,880 --> 00:04:43,759
perform

00:04:41,440 --> 00:04:45,840
pre-migration actions this field

00:04:43,759 --> 00:04:47,280
normally includes funding the suitable

00:04:45,840 --> 00:04:49,360
destination

00:04:47,280 --> 00:04:50,320
if the user did not provide a target

00:04:49,360 --> 00:04:52,720
destination

00:04:50,320 --> 00:04:53,840
the controller will calculate a suitable

00:04:52,720 --> 00:04:56,720
one

00:04:53,840 --> 00:04:57,600
if the user provides the destination

00:04:56,720 --> 00:04:59,600
host

00:04:57,600 --> 00:05:01,759
the controller will check if it is

00:04:59,600 --> 00:05:04,000
suitable after this check

00:05:01,759 --> 00:05:06,240
the controller will ask the source and

00:05:04,000 --> 00:05:07,840
host destination to start the migration

00:05:06,240 --> 00:05:10,240
process

00:05:07,840 --> 00:05:12,720
in this part we will mainly focus on the

00:05:10,240 --> 00:05:14,880
check actions

00:05:12,720 --> 00:05:17,440
we will compare the leeward codes on

00:05:14,880 --> 00:05:19,759
this liberty is one of the most widely

00:05:17,440 --> 00:05:20,800
used open source management toolsets for

00:05:19,759 --> 00:05:24,240
management

00:05:20,800 --> 00:05:27,680
platform virtualizations it is used

00:05:24,240 --> 00:05:31,520
in openstack overt and many other

00:05:27,680 --> 00:05:32,800
cloud platforms by comparing the

00:05:31,520 --> 00:05:36,160
leadboard code for

00:05:32,800 --> 00:05:40,880
arm and x86 driver since we

00:05:36,160 --> 00:05:44,960
have done some improvements in 6.4.0 so

00:05:40,880 --> 00:05:48,560
first look at the code prior to 6.4

00:05:44,960 --> 00:05:50,080
here to see the gaps we can easily see

00:05:48,560 --> 00:05:52,880
three gaps here

00:05:50,080 --> 00:05:56,520
the first one is general we can see that

00:05:52,880 --> 00:05:59,120
x86 driver has over

00:05:56,520 --> 00:06:03,520
3500 lines of codes

00:05:59,120 --> 00:06:06,160
and the arm driver only have 265

00:06:03,520 --> 00:06:06,720
columns of code and that's very huge

00:06:06,160 --> 00:06:09,039
difference

00:06:06,720 --> 00:06:10,880
we can easily see how big the gap is

00:06:09,039 --> 00:06:12,400
just by thinking about the code line

00:06:10,880 --> 00:06:14,560
difference

00:06:12,400 --> 00:06:15,680
the rest two are related to migration

00:06:14,560 --> 00:06:17,919
directly

00:06:15,680 --> 00:06:19,199
the second one is the missing of get

00:06:17,919 --> 00:06:21,199
host function

00:06:19,199 --> 00:06:23,120
this function will be used to provide

00:06:21,199 --> 00:06:25,520
host cpu details

00:06:23,120 --> 00:06:27,759
and this information will be used to

00:06:25,520 --> 00:06:28,319
track whether the host is suitable for

00:06:27,759 --> 00:06:31,680
vm

00:06:28,319 --> 00:06:34,240
migrations if this is missing

00:06:31,680 --> 00:06:36,639
then we cannot perform the check the

00:06:34,240 --> 00:06:39,360
third one is the cpu compare

00:06:36,639 --> 00:06:40,080
cpu compare function we can see that

00:06:39,360 --> 00:06:43,680
mdriver

00:06:40,080 --> 00:06:44,560
have a cpu compare function and it seems

00:06:43,680 --> 00:06:46,800
okay

00:06:44,560 --> 00:06:48,000
but when we check the details for this

00:06:46,800 --> 00:06:51,039
function we can see

00:06:48,000 --> 00:06:52,160
that in arm driver there are no actual

00:06:51,039 --> 00:06:55,360
checks

00:06:52,160 --> 00:06:55,840
identical was returned directly and this

00:06:55,360 --> 00:06:58,000
also

00:06:55,840 --> 00:06:59,759
proves that if there are no host

00:06:58,000 --> 00:07:02,880
information gathered

00:06:59,759 --> 00:07:04,000
the chair cannot be performed on the

00:07:02,880 --> 00:07:07,199
contrary

00:07:04,000 --> 00:07:09,840
in the x86 driver there are much more

00:07:07,199 --> 00:07:13,360
related lines of code and performed much

00:07:09,840 --> 00:07:13,360
more complicated checks

00:07:13,680 --> 00:07:16,960
if we cannot provide the checks in the

00:07:15,599 --> 00:07:20,160
code and

00:07:16,960 --> 00:07:22,800
return directly as

00:07:20,160 --> 00:07:23,199
the code does it now the upper layer

00:07:22,800 --> 00:07:26,880
code

00:07:23,199 --> 00:07:27,840
platforms in the upper layer cloud

00:07:26,880 --> 00:07:30,240
platforms

00:07:27,840 --> 00:07:32,800
the cloud provider will have to manually

00:07:30,240 --> 00:07:35,759
set rules for our compute host

00:07:32,800 --> 00:07:37,199
this will be a disaster for the large

00:07:35,759 --> 00:07:39,919
departments

00:07:37,199 --> 00:07:40,960
so we want to change the situation so we

00:07:39,919 --> 00:07:44,240
did some

00:07:40,960 --> 00:07:46,960
improvements the first one

00:07:44,240 --> 00:07:48,680
is has already been merged and released

00:07:46,960 --> 00:07:52,000
in libward

00:07:48,680 --> 00:07:55,039
6.4.0 we can now gather and report

00:07:52,000 --> 00:07:57,919
cpu information on arm platforms the

00:07:55,039 --> 00:07:58,720
information includes cpu architecture

00:07:57,919 --> 00:08:02,479
model

00:07:58,720 --> 00:08:03,440
vendor cpu flex and others and this will

00:08:02,479 --> 00:08:07,599
provide enough

00:08:03,440 --> 00:08:09,520
information for cpu checks in migrations

00:08:07,599 --> 00:08:12,639
the other one we are currently working

00:08:09,520 --> 00:08:15,919
on is to provide the cpu compare support

00:08:12,639 --> 00:08:18,000
in r in the first step we will compare

00:08:15,919 --> 00:08:20,080
the source host cpu info with the

00:08:18,000 --> 00:08:23,199
destination cpu info

00:08:20,080 --> 00:08:23,919
as in most real world deployments people

00:08:23,199 --> 00:08:26,960
use this

00:08:23,919 --> 00:08:30,080
host password mode for arm vms

00:08:26,960 --> 00:08:31,440
after this is done the left of cloud

00:08:30,080 --> 00:08:35,599
platter will be much

00:08:31,440 --> 00:08:37,279
easier okay so this is for our first

00:08:35,599 --> 00:08:39,599
part of the session

00:08:37,279 --> 00:08:41,200
kevin will give you more details about

00:08:39,599 --> 00:08:43,599
lab migration

00:08:41,200 --> 00:08:44,240
and how it is performed in the second

00:08:43,599 --> 00:08:46,640
part

00:08:44,240 --> 00:08:46,640
thank you

00:08:47,360 --> 00:08:53,120
okay thanks for everyone for

00:08:50,720 --> 00:08:54,240
at least continue listening our session

00:08:53,120 --> 00:08:56,880
and this thanks to

00:08:54,240 --> 00:08:58,000
you for talking about the lab migration

00:08:56,880 --> 00:09:01,519
in the level d

00:08:58,000 --> 00:09:03,920
side so next we will

00:09:01,519 --> 00:09:06,839
talk about the lab migration on the

00:09:03,920 --> 00:09:10,880
infrastructure at the service side

00:09:06,839 --> 00:09:13,200
so first to generally introduce myself

00:09:10,880 --> 00:09:14,480
my name is kevin john and i'm now

00:09:13,200 --> 00:09:17,680
focusing

00:09:14,480 --> 00:09:18,959
on the openstack and the kubernetes

00:09:17,680 --> 00:09:23,839
development

00:09:18,959 --> 00:09:23,839
and now i'm working at linaro ldcg

00:09:25,760 --> 00:09:33,120
okay so as we see in this picture

00:09:28,800 --> 00:09:36,000
so the lab migration functions

00:09:33,120 --> 00:09:36,399
are generally need to support both on

00:09:36,000 --> 00:09:40,399
the

00:09:36,399 --> 00:09:43,680
leverage and also need to on the

00:09:40,399 --> 00:09:48,080
have a wider side and also

00:09:43,680 --> 00:09:50,480
in the high level

00:09:48,080 --> 00:09:52,880
infrastructure as a service side for

00:09:50,480 --> 00:09:56,000
example the openstack

00:09:52,880 --> 00:09:58,640
or the word manager they also need to

00:09:56,000 --> 00:09:59,440
support the lab migration feature so

00:09:58,640 --> 00:10:02,640
here

00:09:59,440 --> 00:10:04,800
we will take the openstack as an example

00:10:02,640 --> 00:10:08,800
to elaborate the details

00:10:04,800 --> 00:10:12,399
because i did openstack is generally the

00:10:08,800 --> 00:10:13,600
open source infrastructure as a service

00:10:12,399 --> 00:10:17,040
framework

00:10:13,600 --> 00:10:20,240
now and it has achieved a lot of success

00:10:17,040 --> 00:10:24,160
in the private cloud area also

00:10:20,240 --> 00:10:27,440
in the m64 we only support the

00:10:24,160 --> 00:10:29,920
live word plus qmu kvm instance

00:10:27,440 --> 00:10:30,640
so we will talk our talk will be limited

00:10:29,920 --> 00:10:34,160
to these

00:10:30,640 --> 00:10:34,160
two a year so

00:10:34,320 --> 00:10:38,000
we can see the openstack has a compute

00:10:36,560 --> 00:10:42,560
project called nova

00:10:38,000 --> 00:10:46,000
it is served as a liver client and

00:10:42,560 --> 00:10:48,000
when migration we need to it need to

00:10:46,000 --> 00:10:49,920
control and monitor the whole migration

00:10:48,000 --> 00:10:50,800
process and also deal with the cpu

00:10:49,920 --> 00:10:54,959
features

00:10:50,800 --> 00:10:57,360
or deal with the network problem or the

00:10:54,959 --> 00:10:58,720
storage problems and to control the

00:10:57,360 --> 00:11:01,200
whole precise

00:10:58,720 --> 00:11:04,720
so now we will dig into this precise to

00:11:01,200 --> 00:11:07,760
see the mechanism of land migration

00:11:04,720 --> 00:11:10,640
for nova it has support three

00:11:07,760 --> 00:11:10,959
three method of migrate the first is we

00:11:10,640 --> 00:11:13,760
know

00:11:10,959 --> 00:11:15,279
it is a non-lab migration uh it is a

00:11:13,760 --> 00:11:18,399
code migration so

00:11:15,279 --> 00:11:21,680
will migrate the instance will be

00:11:18,399 --> 00:11:24,480
suspend for a while so

00:11:21,680 --> 00:11:25,360
and there are two method of lab

00:11:24,480 --> 00:11:27,920
migration

00:11:25,360 --> 00:11:29,519
the first is a block like migration and

00:11:27,920 --> 00:11:30,399
this the second one is the true land

00:11:29,519 --> 00:11:33,839
migration

00:11:30,399 --> 00:11:37,360
the difference from the from them are

00:11:33,839 --> 00:11:41,120
are the whether we have a shared

00:11:37,360 --> 00:11:43,839
shared backend storage for the instance

00:11:41,120 --> 00:11:46,800
so for example so in the in block lab

00:11:43,839 --> 00:11:50,800
migration the instance storage are not

00:11:46,800 --> 00:11:53,040
uh node have a shared volume

00:11:50,800 --> 00:11:54,480
so we need to copy the whole instance

00:11:53,040 --> 00:11:56,800
disk to from the

00:11:54,480 --> 00:11:57,680
source node to the to the destination

00:11:56,800 --> 00:11:59,600
node

00:11:57,680 --> 00:12:00,880
so it will take up a long time and

00:11:59,600 --> 00:12:03,680
return so

00:12:00,880 --> 00:12:04,720
on the real-life migration based on the

00:12:03,680 --> 00:12:07,760
shared

00:12:04,720 --> 00:12:10,320
shared volume or shared backend storage

00:12:07,760 --> 00:12:11,440
it requires both the diced and the

00:12:10,320 --> 00:12:13,920
source node

00:12:11,440 --> 00:12:15,040
can excise the instance disks for

00:12:13,920 --> 00:12:18,240
example we can use

00:12:15,040 --> 00:12:21,760
nfs or such as uh

00:12:18,240 --> 00:12:22,160
the external distribute system such as

00:12:21,760 --> 00:12:25,040
the

00:12:22,160 --> 00:12:25,920
scythe so for example in our linear

00:12:25,040 --> 00:12:29,120
development cloud

00:12:25,920 --> 00:12:32,800
we are using the site as the as the

00:12:29,120 --> 00:12:35,040
backend storage for the instance

00:12:32,800 --> 00:12:37,040
so the migration time will be reduced

00:12:35,040 --> 00:12:38,800
since the migration process only need to

00:12:37,040 --> 00:12:41,200
deal with the dirty memories

00:12:38,800 --> 00:12:43,519
and no need to change the the instance

00:12:41,200 --> 00:12:43,519
disk

00:12:44,000 --> 00:12:49,760
okay here we can see it's a

00:12:47,600 --> 00:12:50,639
there are two con two parts we need to

00:12:49,760 --> 00:12:53,040
consider

00:12:50,639 --> 00:12:55,040
first is the control pies and the other

00:12:53,040 --> 00:12:58,399
is the the data price

00:12:55,040 --> 00:13:01,440
we can see in this picture uh

00:12:58,399 --> 00:13:03,600
the control piles and data price

00:13:01,440 --> 00:13:04,639
the source node here we can see the

00:13:03,600 --> 00:13:07,760
picture here

00:13:04,639 --> 00:13:08,320
so the source node has a running library

00:13:07,760 --> 00:13:10,399
d

00:13:08,320 --> 00:13:11,440
and the the test nodes are also running

00:13:10,399 --> 00:13:13,200
library

00:13:11,440 --> 00:13:14,560
so both of them are controlled by

00:13:13,200 --> 00:13:17,200
openstack and

00:13:14,560 --> 00:13:18,639
both of them are served as a compute

00:13:17,200 --> 00:13:21,200
node

00:13:18,639 --> 00:13:23,040
so now the noaa has a client to talk

00:13:21,200 --> 00:13:23,519
with liberty to control the migration

00:13:23,040 --> 00:13:25,839
flow

00:13:23,519 --> 00:13:28,320
and we want to migrate the vm script

00:13:25,839 --> 00:13:30,560
from source node to desktop node

00:13:28,320 --> 00:13:31,519
in the control pass normal will serve as

00:13:30,560 --> 00:13:35,519
a client

00:13:31,519 --> 00:13:39,199
and to control the migration precise so

00:13:35,519 --> 00:13:40,160
in the control price now i will monitor

00:13:39,199 --> 00:13:42,639
and trigger

00:13:40,160 --> 00:13:44,800
all the migration process and in the

00:13:42,639 --> 00:13:45,519
database it has been separated to two

00:13:44,800 --> 00:13:47,519
methods

00:13:45,519 --> 00:13:49,279
so one is have a weather native to

00:13:47,519 --> 00:13:52,240
direct data transport

00:13:49,279 --> 00:13:52,959
so uh if we want to enable this we need

00:13:52,240 --> 00:13:56,560
to

00:13:52,959 --> 00:13:59,839
set the migrate flag to peer-to-peer so

00:13:56,560 --> 00:14:01,040
so if we want encryption it all depends

00:13:59,839 --> 00:14:02,959
on the hypervisor

00:14:01,040 --> 00:14:04,480
whether have a better support the

00:14:02,959 --> 00:14:06,800
encryption

00:14:04,480 --> 00:14:07,600
and the other is the liberal terminal so

00:14:06,800 --> 00:14:10,800
liberal d

00:14:07,600 --> 00:14:13,279
will uh establish a rpc

00:14:10,800 --> 00:14:14,480
between the source and host and it can

00:14:13,279 --> 00:14:17,199
support the encryption

00:14:14,480 --> 00:14:18,079
due to liberal features so if we want to

00:14:17,199 --> 00:14:20,399
enable this

00:14:18,079 --> 00:14:21,680
we need to cite the lab migration turn

00:14:20,399 --> 00:14:24,399
of d to true

00:14:21,680 --> 00:14:24,399
in the noaa

00:14:25,519 --> 00:14:29,040
okay next we can see it is a general

00:14:28,160 --> 00:14:32,160
five

00:14:29,040 --> 00:14:35,760
phrase for the loud migration so

00:14:32,160 --> 00:14:38,320
in the prey level we can see uh

00:14:35,760 --> 00:14:39,839
there is a active virtual machine a on

00:14:38,320 --> 00:14:42,079
the vertical node a

00:14:39,839 --> 00:14:42,880
so and we want to migrate the virtual

00:14:42,079 --> 00:14:47,360
machine

00:14:42,880 --> 00:14:50,320
a to another node and the scheduler has

00:14:47,360 --> 00:14:53,600
selected the node b as the land

00:14:50,320 --> 00:14:53,600
migration destination

00:14:53,760 --> 00:14:57,600
so in the reserve side

00:14:57,680 --> 00:15:01,120
it will call the method as a host b to

00:15:00,240 --> 00:15:04,000
confirm the

00:15:01,120 --> 00:15:06,639
b has available resources and then

00:15:04,000 --> 00:15:08,639
trigger the migration precise

00:15:06,639 --> 00:15:11,120
so there is one thing we need to

00:15:08,639 --> 00:15:14,240
consider is a keep the network info

00:15:11,120 --> 00:15:17,519
because we want to the vma hosting

00:15:14,240 --> 00:15:20,079
item node a and then migrate to the vm

00:15:17,519 --> 00:15:21,920
a hosting and node node b so the network

00:15:20,079 --> 00:15:24,720
info are the same

00:15:21,920 --> 00:15:26,639
if you need to keep the same network nic

00:15:24,720 --> 00:15:29,120
mac drives and the ip

00:15:26,639 --> 00:15:30,000
so from the source host we need to get

00:15:29,120 --> 00:15:32,880
the metadata

00:15:30,000 --> 00:15:34,079
and then on the dash node it will set up

00:15:32,880 --> 00:15:37,279
a virtual machine

00:15:34,079 --> 00:15:40,079
on the node b and plug it into the

00:15:37,279 --> 00:15:40,800
vr end bridge we know that the brand

00:15:40,079 --> 00:15:43,920
bridge is

00:15:40,800 --> 00:15:45,600
eternal obs bridge and after reserve we

00:15:43,920 --> 00:15:48,560
will have a new vm post

00:15:45,600 --> 00:15:48,560
on the node b

00:15:49,759 --> 00:15:56,880
next we will enter into the

00:15:54,160 --> 00:15:59,120
more complicated area we call the

00:15:56,880 --> 00:16:02,160
iterative pre-copy method

00:15:59,120 --> 00:16:06,480
so in in this phrase we know

00:16:02,160 --> 00:16:09,759
first we the hypervisor will mark

00:16:06,480 --> 00:16:12,959
all the memories on the bma

00:16:09,759 --> 00:16:14,480
as 13 memories and then it will

00:16:12,959 --> 00:16:17,839
iteratively copy

00:16:14,480 --> 00:16:21,199
it memory from the source node

00:16:17,839 --> 00:16:21,759
to the destination node and the liberty

00:16:21,199 --> 00:16:24,480
will

00:16:21,759 --> 00:16:25,440
and noah will monitor the ram during the

00:16:24,480 --> 00:16:27,440
copy process

00:16:25,440 --> 00:16:28,480
and then copy the data pages to the

00:16:27,440 --> 00:16:30,959
destination

00:16:28,480 --> 00:16:32,399
it will be our iterative method and

00:16:30,959 --> 00:16:35,839
doing the things

00:16:32,399 --> 00:16:37,040
iteratively so as we see and as we

00:16:35,839 --> 00:16:39,680
expect

00:16:37,040 --> 00:16:40,880
the dirt pages will be reduced during

00:16:39,680 --> 00:16:43,120
the coping

00:16:40,880 --> 00:16:44,079
and we know that the rhyme is changing

00:16:43,120 --> 00:16:46,320
every time

00:16:44,079 --> 00:16:47,199
so during the coping precise the

00:16:46,320 --> 00:16:49,279
original

00:16:47,199 --> 00:16:51,920
virtual machine also will be the

00:16:49,279 --> 00:16:54,880
memories in the original vm

00:16:51,920 --> 00:16:55,360
also will be changed and the it will be

00:16:54,880 --> 00:16:58,959
marked

00:16:55,360 --> 00:16:59,360
as as 30 pages so we cannot guarantee

00:16:58,959 --> 00:17:02,880
that

00:16:59,360 --> 00:17:04,640
in we can enter into a phrase that all

00:17:02,880 --> 00:17:08,000
memories are synced with

00:17:04,640 --> 00:17:11,360
the the destination virtual machine so

00:17:08,000 --> 00:17:14,640
we should set uh the dung time threshold

00:17:11,360 --> 00:17:17,520
so this thread shoot is a time period

00:17:14,640 --> 00:17:19,600
we will meet when suspends the vm and

00:17:17,520 --> 00:17:23,439
copy the last memories

00:17:19,600 --> 00:17:26,720
so when migration it's easy to calculate

00:17:23,439 --> 00:17:30,280
the the the time uh

00:17:26,720 --> 00:17:31,840
according to the remaining 30 pages and

00:17:30,280 --> 00:17:36,080
[Music]

00:17:31,840 --> 00:17:38,720
so which is very significant signals for

00:17:36,080 --> 00:17:39,440
for stopping the iterative precise

00:17:38,720 --> 00:17:42,400
because

00:17:39,440 --> 00:17:42,960
when the calculating time is less than

00:17:42,400 --> 00:17:46,000
the

00:17:42,960 --> 00:17:49,520
the stress root level will trigger the

00:17:46,000 --> 00:17:53,120
next phrase we call stop and copy phrase

00:17:49,520 --> 00:17:57,120
so so a nova has set up the

00:17:53,120 --> 00:18:00,240
downtime threshold

00:17:57,120 --> 00:18:01,039
so and when when the the downtime

00:18:00,240 --> 00:18:03,360
stretch should

00:18:01,039 --> 00:18:05,919
meet our knees the leader word will stop

00:18:03,360 --> 00:18:07,679
the vm and copy the remaining 30 pages

00:18:05,919 --> 00:18:10,960
to node b

00:18:07,679 --> 00:18:14,000
so next we will come to this the last

00:18:10,960 --> 00:18:16,400
two phrase we can see the max downtime

00:18:14,000 --> 00:18:17,919
that calculated is less than the low mag

00:18:16,400 --> 00:18:21,200
rate downtime flag

00:18:17,919 --> 00:18:24,640
level will suspend the vm and

00:18:21,200 --> 00:18:28,960
then copy the remaining 30 pages

00:18:24,640 --> 00:18:33,200
one time directly to the node b and then

00:18:28,960 --> 00:18:36,880
neutron will unplug the virtual machine

00:18:33,200 --> 00:18:40,320
interface from the vma and node a

00:18:36,880 --> 00:18:44,640
and also on the in host b

00:18:40,320 --> 00:18:46,799
because that's the primary host for vm

00:18:44,640 --> 00:18:48,320
uh and how to be it it has become the

00:18:46,799 --> 00:18:51,200
primary host

00:18:48,320 --> 00:18:53,440
for the vma and the neutron will plug

00:18:51,200 --> 00:18:55,280
into the virtual machine interface

00:18:53,440 --> 00:18:56,640
uh sorry uh plug into the virtual

00:18:55,280 --> 00:18:58,480
interface to

00:18:56,640 --> 00:19:00,880
the virtual machine newly virtual

00:18:58,480 --> 00:19:03,520
machine and then

00:19:00,880 --> 00:19:04,320
triggering the booting precise also

00:19:03,520 --> 00:19:07,440
there is a flag

00:19:04,320 --> 00:19:09,600
we can see it is called auto converge

00:19:07,440 --> 00:19:11,200
let's assume a high overload virtual

00:19:09,600 --> 00:19:14,000
machine whose memory

00:19:11,200 --> 00:19:14,720
is changing rapidly and take a pressure

00:19:14,000 --> 00:19:17,360
loading

00:19:14,720 --> 00:19:18,799
so when migration the dirty pages are

00:19:17,360 --> 00:19:21,679
hard to reduce

00:19:18,799 --> 00:19:23,360
so uh an induced our migration process

00:19:21,679 --> 00:19:26,559
lasting for very long time

00:19:23,360 --> 00:19:29,919
or maybe induce a loud migration field

00:19:26,559 --> 00:19:32,799
so we can choose this flag and if you

00:19:29,919 --> 00:19:33,520
change the virtual cpu to reduce the low

00:19:32,799 --> 00:19:35,919
pressure

00:19:33,520 --> 00:19:39,840
in order to reduce the newly generated

00:19:35,919 --> 00:19:39,840
data pages

00:19:41,840 --> 00:19:46,799
okay we can see uh the language the land

00:19:45,600 --> 00:19:48,799
migration

00:19:46,799 --> 00:19:50,160
things we need to consider is a cpu

00:19:48,799 --> 00:19:52,240
different cpu

00:19:50,160 --> 00:19:53,200
as shown you said before the migration

00:19:52,240 --> 00:19:56,640
from a to b

00:19:53,200 --> 00:19:59,919
is fine due to that the

00:19:56,640 --> 00:20:03,120
the cpu instructions are the

00:19:59,919 --> 00:20:06,320
subside of the node b but

00:20:03,120 --> 00:20:08,960
uh but

00:20:06,320 --> 00:20:09,840
if in the comparation from b to a is

00:20:08,960 --> 00:20:12,640
filled

00:20:09,840 --> 00:20:13,200
so this is a very important things we

00:20:12,640 --> 00:20:16,080
need to

00:20:13,200 --> 00:20:17,280
enhance in the library level and now in

00:20:16,080 --> 00:20:19,600
openstack side we just

00:20:17,280 --> 00:20:22,400
escape this check to make the lab

00:20:19,600 --> 00:20:22,400
migration happen

00:20:23,760 --> 00:20:27,039
the last things we need to consider is

00:20:25,919 --> 00:20:31,919
the numera

00:20:27,039 --> 00:20:31,919
as we know new methodology is a yeah so

00:20:32,000 --> 00:20:35,440
the newer topology is another things

00:20:34,640 --> 00:20:38,720
here

00:20:35,440 --> 00:20:40,320
we need to to triggering the land

00:20:38,720 --> 00:20:42,320
migration we need to consider in

00:20:40,320 --> 00:20:44,559
triggering the land migration

00:20:42,320 --> 00:20:46,720
uh the liberal and cumule are generally

00:20:44,559 --> 00:20:49,520
doing what nova tells them to do

00:20:46,720 --> 00:20:49,919
so actually in the old version of nova

00:20:49,520 --> 00:20:52,480
when

00:20:49,919 --> 00:20:54,080
they migrating the cpu painting

00:20:52,480 --> 00:20:56,400
information and the pneumatop

00:20:54,080 --> 00:20:57,600
will be kept from the source to desktop

00:20:56,400 --> 00:21:00,000
after lab migration

00:20:57,600 --> 00:21:00,880
so nova is generally the pneuma and

00:21:00,000 --> 00:21:04,080
non-viewer

00:21:00,880 --> 00:21:08,080
to some extent so as we see

00:21:04,080 --> 00:21:10,240
uh this is a new xml and it will be

00:21:08,080 --> 00:21:13,520
natively copied to the destination

00:21:10,240 --> 00:21:16,000
so that will induce a problem because uh

00:21:13,520 --> 00:21:17,039
we have ping this we have set the cpu

00:21:16,000 --> 00:21:19,600
pin name

00:21:17,039 --> 00:21:20,720
in the node a and the painting map will

00:21:19,600 --> 00:21:23,039
be namely

00:21:20,720 --> 00:21:23,919
copied to the destination node and any

00:21:23,039 --> 00:21:27,200
exchange

00:21:23,919 --> 00:21:30,080
any existing pinging mappings on the

00:21:27,200 --> 00:21:32,480
test node will be ignored so that will

00:21:30,080 --> 00:21:34,640
induce the problem

00:21:32,480 --> 00:21:36,720
because if there is another instance

00:21:34,640 --> 00:21:39,280
already existing on the destination

00:21:36,720 --> 00:21:40,960
so both instance vcpu has been pinged to

00:21:39,280 --> 00:21:44,400
the same basic view

00:21:40,960 --> 00:21:47,600
the same physics cpus

00:21:44,400 --> 00:21:50,080
so that will meet the performance issues

00:21:47,600 --> 00:21:50,720
so in order to solve this problem yeah

00:21:50,080 --> 00:21:53,280
nova has

00:21:50,720 --> 00:21:54,159
introduced a new number of rear line

00:21:53,280 --> 00:21:56,640
migration

00:21:54,159 --> 00:21:58,640
we can see a totally workflow so after

00:21:56,640 --> 00:22:01,840
the scheduler choose the destination

00:21:58,640 --> 00:22:04,480
it is before the pre migration

00:22:01,840 --> 00:22:06,400
yeah the normal conductor will check the

00:22:04,480 --> 00:22:08,640
migration in destination node

00:22:06,400 --> 00:22:10,080
and the source node will send the

00:22:08,640 --> 00:22:12,480
migration data plus

00:22:10,080 --> 00:22:14,640
the instance numera topology so this

00:22:12,480 --> 00:22:17,679
here the instance of newer topology

00:22:14,640 --> 00:22:20,559
is information from the source

00:22:17,679 --> 00:22:21,679
and to the destination node and in the

00:22:20,559 --> 00:22:23,520
destination node

00:22:21,679 --> 00:22:25,280
it has maintained a normal topology

00:22:23,520 --> 00:22:28,240
itself so

00:22:25,280 --> 00:22:28,799
after that noah will do the logical to

00:22:28,240 --> 00:22:32,080
feed

00:22:28,799 --> 00:22:34,080
the instance newman topology

00:22:32,080 --> 00:22:35,520
according to the new methodology in the

00:22:34,080 --> 00:22:38,720
destination node

00:22:35,520 --> 00:22:41,760
and after that uh it will check if it

00:22:38,720 --> 00:22:44,880
if the destination will meet the pro

00:22:41,760 --> 00:22:47,440
will meet the request so a field

00:22:44,880 --> 00:22:48,080
if we feel our allow migration will be

00:22:47,440 --> 00:22:51,840
filled

00:22:48,080 --> 00:22:53,520
and if succeed it will retain the new

00:22:51,840 --> 00:22:56,640
instance numa topology

00:22:53,520 --> 00:22:58,400
to the normal conductor so

00:22:56,640 --> 00:23:00,720
and after that the conductor will

00:22:58,400 --> 00:23:02,799
trigger the preload migrations

00:23:00,720 --> 00:23:03,760
with the newly faded instance noma

00:23:02,799 --> 00:23:06,240
topology

00:23:03,760 --> 00:23:08,320
to the source node and then the source

00:23:06,240 --> 00:23:09,039
node will use the newly instanced numa

00:23:08,320 --> 00:23:11,760
topology

00:23:09,039 --> 00:23:12,480
to call the label driver to generate the

00:23:11,760 --> 00:23:16,000
new

00:23:12,480 --> 00:23:17,280
new my example sorry the newma xml for

00:23:16,000 --> 00:23:19,840
the destination

00:23:17,280 --> 00:23:22,640
so the new my topology will be changed

00:23:19,840 --> 00:23:26,000
according to the newly destination node

00:23:22,640 --> 00:23:26,400
and it will make sure that it suits our

00:23:26,000 --> 00:23:28,240
need

00:23:26,400 --> 00:23:30,960
and didn't trigger the problem as we

00:23:28,240 --> 00:23:30,960
refer before

00:23:31,280 --> 00:23:37,840

YouTube URL: https://www.youtube.com/watch?v=aDzPHmk5yeY


