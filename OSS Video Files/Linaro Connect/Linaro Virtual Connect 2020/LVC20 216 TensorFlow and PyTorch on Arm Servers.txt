Title: LVC20 216 TensorFlow and PyTorch on Arm Servers
Publication date: 2020-10-08
Playlist: Linaro Virtual Connect 2020
Description: 
	Interested in running Machine Learning workloads on Arm servers? Do you want to make the ML software stack faster on Arm? In this talk, the speaker will talk about status, roadmap, and areas of improvement when using TensorFlow and PyTorch on Arm servers. The speaker will also cover key open source projects - Eigen, OneDNN, ArmCL that aid in improving framework performance

https://connect.linaro.org/resources/lvc20/lvc20-216/
Captions: 
	00:00:00,080 --> 00:00:04,400
today i'm going to talk about tensorflow

00:00:01,839 --> 00:00:08,240
and pytorch and arm servers

00:00:04,400 --> 00:00:11,759
the primary

00:00:08,240 --> 00:00:13,120
driver has been the availability of arm

00:00:11,759 --> 00:00:15,120
servers with ml

00:00:13,120 --> 00:00:16,720
features there are mn specific features

00:00:15,120 --> 00:00:17,920
that are available and

00:00:16,720 --> 00:00:19,760
there's a lot of interest in the

00:00:17,920 --> 00:00:21,840
community to run

00:00:19,760 --> 00:00:24,480
standard frameworks like tensorflow and

00:00:21,840 --> 00:00:27,680
by dodge so try and cover as much

00:00:24,480 --> 00:00:29,760
uh as we can in next 25 minutes i am

00:00:27,680 --> 00:00:31,920
ashok but i'm senior product manager at

00:00:29,760 --> 00:00:33,840
palm i look after

00:00:31,920 --> 00:00:35,440
many open source projects including the

00:00:33,840 --> 00:00:38,960
ml software stack

00:00:35,440 --> 00:00:40,719
tensorflow by dodge included so

00:00:38,960 --> 00:00:43,840
if you have any questions you can take

00:00:40,719 --> 00:00:45,920
it at the end when i look at this

00:00:43,840 --> 00:00:47,840
so what is the agenda for today uh we

00:00:45,920 --> 00:00:50,000
look at tensorflow and pi

00:00:47,840 --> 00:00:52,559
torch and arm servers i'm specifically

00:00:50,000 --> 00:00:54,000
going to look at cpu inference

00:00:52,559 --> 00:00:56,320
we're not going to cover the training

00:00:54,000 --> 00:00:58,399
use case today

00:00:56,320 --> 00:00:59,840
and within that cpu inference on armed

00:00:58,399 --> 00:01:01,199
servers we look at

00:00:59,840 --> 00:01:03,920
what's the end goal and what are you

00:01:01,199 --> 00:01:06,240
going to achieve as a community

00:01:03,920 --> 00:01:08,240
for tensorflow and by torch what is

00:01:06,240 --> 00:01:10,720
available today where are we

00:01:08,240 --> 00:01:12,400
and here i'm going to take the end user

00:01:10,720 --> 00:01:13,119
perspective in the sense that somebody

00:01:12,400 --> 00:01:15,520
wants to run

00:01:13,119 --> 00:01:17,280
machine learning on arm what is

00:01:15,520 --> 00:01:19,759
available for them

00:01:17,280 --> 00:01:20,479
and then we look at what's being done to

00:01:19,759 --> 00:01:23,360
improve

00:01:20,479 --> 00:01:24,159
two aspects one is usability how easy is

00:01:23,360 --> 00:01:26,560
it to

00:01:24,159 --> 00:01:27,680
just get on with what you want to do on

00:01:26,560 --> 00:01:29,840
arm

00:01:27,680 --> 00:01:31,200
and how do you make it go fast so these

00:01:29,840 --> 00:01:34,159
are kind of two different vectors

00:01:31,200 --> 00:01:34,960
and we'll try and do that finally how do

00:01:34,159 --> 00:01:38,560
you

00:01:34,960 --> 00:01:40,960
how does anybody uh get involved in

00:01:38,560 --> 00:01:43,600
making improvements to either usability

00:01:40,960 --> 00:01:45,759
or performance or anything else

00:01:43,600 --> 00:01:47,439
so there are options where you can get

00:01:45,759 --> 00:01:51,520
involved there's already a

00:01:47,439 --> 00:01:54,960
sizable community work so it'll be

00:01:51,520 --> 00:01:56,880
useful to have more and more community

00:01:54,960 --> 00:01:58,320
participation so what's not being

00:01:56,880 --> 00:01:59,920
covered we're not going to look at

00:01:58,320 --> 00:02:02,399
training

00:01:59,920 --> 00:02:04,159
we'll also not look at machine learning

00:02:02,399 --> 00:02:07,200
using arm plus gpu

00:02:04,159 --> 00:02:09,599
so this is being done by nvidia

00:02:07,200 --> 00:02:11,039
separately so they announced recently

00:02:09,599 --> 00:02:13,599
the entire stack

00:02:11,039 --> 00:02:15,680
being available for apple's gpu then gpu

00:02:13,599 --> 00:02:18,800
so we're not gonna look at that

00:02:15,680 --> 00:02:20,239
and finally uh i'm still working on the

00:02:18,800 --> 00:02:22,080
availability so i'm not gonna look

00:02:20,239 --> 00:02:24,319
for any specific benchmarks or

00:02:22,080 --> 00:02:27,360
performance comparisons against x86

00:02:24,319 --> 00:02:30,879
at this point okay

00:02:27,360 --> 00:02:31,360
all right moving further what is the

00:02:30,879 --> 00:02:34,080
goal

00:02:31,360 --> 00:02:35,599
um this is a loaded slide i'll take some

00:02:34,080 --> 00:02:38,720
time to explain

00:02:35,599 --> 00:02:42,160
why this is the case

00:02:38,720 --> 00:02:45,120
the main thing is

00:02:42,160 --> 00:02:46,480
we want ml inference solution for arm

00:02:45,120 --> 00:02:48,560
servers

00:02:46,480 --> 00:02:51,040
and we want that to use whatever mn

00:02:48,560 --> 00:02:52,959
specific cpu features that are available

00:02:51,040 --> 00:02:55,280
it should be easy to use and it should

00:02:52,959 --> 00:02:57,280
be as performant as it can depending on

00:02:55,280 --> 00:03:00,480
the hardware available

00:02:57,280 --> 00:03:02,560
now within easy to use typically

00:03:00,480 --> 00:03:04,000
easy to use would mean i can do a docker

00:03:02,560 --> 00:03:07,040
pull get a container

00:03:04,000 --> 00:03:08,720
image for example or i would do a pip

00:03:07,040 --> 00:03:10,319
install so i have a python package

00:03:08,720 --> 00:03:12,159
all i do is just pick and install and

00:03:10,319 --> 00:03:14,720
magically comes

00:03:12,159 --> 00:03:15,360
and this option is hopefully available

00:03:14,720 --> 00:03:17,920
for

00:03:15,360 --> 00:03:19,599
all the popular ml frameworks so we're

00:03:17,920 --> 00:03:20,239
looking at tensorflow and pytorch but

00:03:19,599 --> 00:03:23,360
there are other

00:03:20,239 --> 00:03:24,560
ml frameworks as well and the key thing

00:03:23,360 --> 00:03:26,560
is for them to support

00:03:24,560 --> 00:03:29,840
arm as a first class citizen in the

00:03:26,560 --> 00:03:31,599
sense on par with other architectures

00:03:29,840 --> 00:03:34,080
that's the ease of use so if you do pip

00:03:31,599 --> 00:03:35,760
install or docker pull then

00:03:34,080 --> 00:03:38,159
usually that's how the industry kind of

00:03:35,760 --> 00:03:41,519
works in this segment

00:03:38,159 --> 00:03:43,599
then performance and ability to

00:03:41,519 --> 00:03:44,560
use it depends on wide variety of

00:03:43,599 --> 00:03:47,440
inference workloads

00:03:44,560 --> 00:03:48,080
inference itself is kind of a broad

00:03:47,440 --> 00:03:49,920
domain

00:03:48,080 --> 00:03:51,519
so you can do image classification

00:03:49,920 --> 00:03:52,239
object detection i've just listed two of

00:03:51,519 --> 00:03:53,920
them

00:03:52,239 --> 00:03:57,120
but the list kind of goes on you can do

00:03:53,920 --> 00:03:59,120
so many different versions of them

00:03:57,120 --> 00:04:00,640
each of them have their own unique

00:03:59,120 --> 00:04:03,840
operators requirement

00:04:00,640 --> 00:04:04,720
and some unique characteristics so just

00:04:03,840 --> 00:04:06,560
because

00:04:04,720 --> 00:04:08,080
it is performant for image

00:04:06,560 --> 00:04:09,439
classification doesn't automatically

00:04:08,080 --> 00:04:12,239
mean that it will be performant for

00:04:09,439 --> 00:04:14,640
object detection for example

00:04:12,239 --> 00:04:15,439
and then we have the arm architecture

00:04:14,640 --> 00:04:19,519
features

00:04:15,439 --> 00:04:22,560
and here one of the

00:04:19,519 --> 00:04:24,639
defining features of latest arm-based

00:04:22,560 --> 00:04:28,880
servers is we have large number of cores

00:04:24,639 --> 00:04:31,919
so for example n1 ampere graviton

00:04:28,880 --> 00:04:35,919
of course there are more than 60 cores

00:04:31,919 --> 00:04:37,759
per sock we do have some

00:04:35,919 --> 00:04:40,479
ml specific features right now for

00:04:37,759 --> 00:04:44,320
example indeed fp16 is already available

00:04:40,479 --> 00:04:47,919
in universe n1 there are more coming

00:04:44,320 --> 00:04:50,160
like b float 16 and

00:04:47,919 --> 00:04:52,240
matrix multiplication s e they're all

00:04:50,160 --> 00:04:54,560
coming in the next generation

00:04:52,240 --> 00:04:57,600
of arm hardware so i've listed some of

00:04:54,560 --> 00:05:00,720
the arm hardware there so universe n1

00:04:57,600 --> 00:05:03,840
v1 which was the official name for

00:05:00,720 --> 00:05:04,320
what was called zeus before and n2 which

00:05:03,840 --> 00:05:06,880
is

00:05:04,320 --> 00:05:09,280
official name for perseus both of which

00:05:06,880 --> 00:05:11,039
were announced yesterday

00:05:09,280 --> 00:05:13,840
that's the arm course and then you have

00:05:11,039 --> 00:05:18,320
the partner course like fujitsu a64 fx

00:05:13,840 --> 00:05:21,520
which has an sve 512 big code

00:05:18,320 --> 00:05:24,560
right so first let's look at

00:05:21,520 --> 00:05:25,919
the usability angle right if you're an

00:05:24,560 --> 00:05:27,759
end user

00:05:25,919 --> 00:05:30,720
what how do you get a package or a

00:05:27,759 --> 00:05:30,720
docker image today

00:05:31,199 --> 00:05:35,919
so i'm going to split this into three

00:05:33,840 --> 00:05:36,639
areas first one is the python packages

00:05:35,919 --> 00:05:39,919
itself

00:05:36,639 --> 00:05:42,080
the goal here is how do we quickly get

00:05:39,919 --> 00:05:43,520
a python package or a tensorflow package

00:05:42,080 --> 00:05:46,000
from standard repo

00:05:43,520 --> 00:05:48,160
the standard repo could be pi by conda

00:05:46,000 --> 00:05:51,039
any of those

00:05:48,160 --> 00:05:51,360
the uh we it's it's a journey i think we

00:05:51,039 --> 00:05:52,880
are

00:05:51,360 --> 00:05:56,160
i'm listing out what is the current

00:05:52,880 --> 00:06:00,240
status today and what the next steps are

00:05:56,160 --> 00:06:03,520
so as of uh today this is september 2020

00:06:00,240 --> 00:06:05,840
tensorflow 1.15 and 2.3 package

00:06:03,520 --> 00:06:07,039
snapshots they're available in the

00:06:05,840 --> 00:06:11,039
linaro

00:06:07,039 --> 00:06:12,479
snapshot repo pytorch nightly

00:06:11,039 --> 00:06:14,720
is also available in the lenaro

00:06:12,479 --> 00:06:17,280
snapshots this is 1.7

00:06:14,720 --> 00:06:18,160
uh candidate that's available so you

00:06:17,280 --> 00:06:20,720
could use this

00:06:18,160 --> 00:06:22,160
and as far as we know they have been

00:06:20,720 --> 00:06:25,360
built with the best flags

00:06:22,160 --> 00:06:27,759
that are suited for arm so we'll

00:06:25,360 --> 00:06:28,560
quickly get started there what are the

00:06:27,759 --> 00:06:32,160
next steps

00:06:28,560 --> 00:06:34,000
um we have to get the pytorch snapshots

00:06:32,160 --> 00:06:37,600
first to 1.6 which is the last

00:06:34,000 --> 00:06:39,919
major release of my torch and thereafter

00:06:37,600 --> 00:06:42,400
get to a point where you don't have to

00:06:39,919 --> 00:06:42,960
go to the snapshot but you have a hosted

00:06:42,400 --> 00:06:46,000
area

00:06:42,960 --> 00:06:47,600
where you can just do maybe wget and

00:06:46,000 --> 00:06:50,639
then pip install

00:06:47,600 --> 00:06:51,039
for both tensorflow and pytorch finally

00:06:50,639 --> 00:06:53,520
the

00:06:51,039 --> 00:06:55,199
most important one which is to work with

00:06:53,520 --> 00:06:55,759
tensorflow and pytorch communities to

00:06:55,199 --> 00:06:58,800
provide

00:06:55,759 --> 00:07:01,520
ar 64 ready packages now you

00:06:58,800 --> 00:07:03,360
can do tensorflow and pi dots people

00:07:01,520 --> 00:07:05,520
installed today but the thing is

00:07:03,360 --> 00:07:07,199
because they have cc plus discord

00:07:05,520 --> 00:07:09,440
underneath them

00:07:07,199 --> 00:07:11,199
what happens is it gets compiled on your

00:07:09,440 --> 00:07:13,199
ar64 device which is not

00:07:11,199 --> 00:07:15,520
ideal you want them pre-compiled with

00:07:13,199 --> 00:07:18,880
all the standard flags and everything

00:07:15,520 --> 00:07:20,880
just want to install so that's the

00:07:18,880 --> 00:07:23,440
kind of the next step we are working the

00:07:20,880 --> 00:07:23,440
words there

00:07:23,840 --> 00:07:26,720
moving further

00:07:27,840 --> 00:07:31,360
the other option is the docker images

00:07:30,240 --> 00:07:34,560
now

00:07:31,360 --> 00:07:34,560
within docker images

00:07:35,039 --> 00:07:40,160
typical solution is you have a recipe

00:07:37,840 --> 00:07:43,680
that you can build your own docker image

00:07:40,160 --> 00:07:44,720
so we already have a repo that's hosted

00:07:43,680 --> 00:07:47,759
by arm

00:07:44,720 --> 00:07:49,919
which contains the recipe to build

00:07:47,759 --> 00:07:51,919
tensorflow for various combinations by

00:07:49,919 --> 00:07:54,800
torch for

00:07:51,919 --> 00:07:56,000
one of the combination right now and we

00:07:54,800 --> 00:07:59,599
have next steps

00:07:56,000 --> 00:08:02,800
where we will expand

00:07:59,599 --> 00:08:05,440
this to include newer releases and

00:08:02,800 --> 00:08:07,440
acl based configuration yeah so i'll

00:08:05,440 --> 00:08:08,800
talk about what acl is and what one dna

00:08:07,440 --> 00:08:11,919
is later

00:08:08,800 --> 00:08:15,680
so just quickly share

00:08:11,919 --> 00:08:18,000
the screen which contains

00:08:15,680 --> 00:08:19,919
the docker image then i'll explain the

00:08:18,000 --> 00:08:23,199
recipe so this is a tensorflow

00:08:19,919 --> 00:08:24,479
docker recipe and you can see that it

00:08:23,199 --> 00:08:27,039
can provide

00:08:24,479 --> 00:08:27,840
all kinds of combinations so you can

00:08:27,039 --> 00:08:30,479
build with

00:08:27,840 --> 00:08:32,640
uh with whether you want to build

00:08:30,479 --> 00:08:34,719
version one or version two

00:08:32,640 --> 00:08:36,000
uh there is different levels uh

00:08:34,719 --> 00:08:38,000
available then

00:08:36,000 --> 00:08:41,279
which target you want to build so neos

00:08:38,000 --> 00:08:44,240
n1 thunder x and so on generic and stuff

00:08:41,279 --> 00:08:45,440
so it's it's it's quite extensive and

00:08:44,240 --> 00:08:48,560
worst time

00:08:45,440 --> 00:08:53,040
so if you have a requirement for

00:08:48,560 --> 00:08:55,279
docker uh image then i would recommend

00:08:53,040 --> 00:08:56,720
using this recipe if you want some

00:08:55,279 --> 00:08:59,120
particular combination

00:08:56,720 --> 00:09:00,560
if you have any requests you can raise a

00:08:59,120 --> 00:09:05,920
issue in this

00:09:00,560 --> 00:09:05,920
github repo moving back

00:09:06,560 --> 00:09:12,640
so next dexter is a docker image now uh

00:09:09,839 --> 00:09:14,480
ideally for most common use cases you

00:09:12,640 --> 00:09:15,200
just want to do dockable you don't want

00:09:14,480 --> 00:09:17,839
to

00:09:15,200 --> 00:09:18,399
sit and build an image um because that's

00:09:17,839 --> 00:09:20,839
not

00:09:18,399 --> 00:09:22,320
most users do so they just wanted to

00:09:20,839 --> 00:09:25,600
outline

00:09:22,320 --> 00:09:26,480
here for other architectures it's either

00:09:25,600 --> 00:09:29,360
available

00:09:26,480 --> 00:09:30,880
with the framework group itself so for

00:09:29,360 --> 00:09:31,920
example google provides tensorflow

00:09:30,880 --> 00:09:36,480
docker images

00:09:31,920 --> 00:09:39,040
for x86 with lithium gpu and so on

00:09:36,480 --> 00:09:40,399
we are slowly getting there the first

00:09:39,040 --> 00:09:43,600
stage we are in right now

00:09:40,399 --> 00:09:46,000
is you have uh for a couple of options

00:09:43,600 --> 00:09:48,240
i'll explain those options later on

00:09:46,000 --> 00:09:50,959
for tensorflow and by torch we have it

00:09:48,240 --> 00:09:53,760
in the staging area now

00:09:50,959 --> 00:09:54,080
this staging area is a area which we use

00:09:53,760 --> 00:09:55,920
to

00:09:54,080 --> 00:09:58,399
host the images before they get pulled

00:09:55,920 --> 00:10:00,000
onto kind of the lenaro's docker hub

00:09:58,399 --> 00:10:02,079
repo

00:10:00,000 --> 00:10:04,399
eventually soon we'll get into the

00:10:02,079 --> 00:10:07,440
lenara docker hub reaper but

00:10:04,399 --> 00:10:10,160
if you are keen to use then the link is

00:10:07,440 --> 00:10:14,880
there you can go on

00:10:10,160 --> 00:10:17,760
take a look at those so if i quickly

00:10:14,880 --> 00:10:18,240
change and you can look at the this is

00:10:17,760 --> 00:10:19,839
the

00:10:18,240 --> 00:10:21,360
staging area i was talking about so you

00:10:19,839 --> 00:10:24,320
have different options

00:10:21,360 --> 00:10:25,600
you can build with ampl open blast icon

00:10:24,320 --> 00:10:26,959
and so on i'll explain these options

00:10:25,600 --> 00:10:28,480
later on but

00:10:26,959 --> 00:10:30,720
it's kind of available for you to use

00:10:28,480 --> 00:10:30,720
today

00:10:31,200 --> 00:10:34,640
all right what do we plan to do next uh

00:10:33,279 --> 00:10:37,440
whenever a new release comes like

00:10:34,640 --> 00:10:38,480
tensorflow 2.4 or 1.7 for pytorch we'll

00:10:37,440 --> 00:10:40,800
upgrade it

00:10:38,480 --> 00:10:42,959
uh we are in the process of adding the

00:10:40,800 --> 00:10:46,240
one dna on sale option i'll explain

00:10:42,959 --> 00:10:48,640
why that option is useful later we are

00:10:46,240 --> 00:10:50,800
also planning to add images for fujitsu

00:10:48,640 --> 00:10:54,640
a64 effects right now we only have

00:10:50,800 --> 00:10:58,079
uh um universe n1 that's in the plans

00:10:54,640 --> 00:11:00,560
working with linaro in this and

00:10:58,079 --> 00:11:03,440
we'll move the images from staging area

00:11:00,560 --> 00:11:06,000
to properly narrow docker repo

00:11:03,440 --> 00:11:07,680
which would then be supported by a

00:11:06,000 --> 00:11:09,680
combination of leonardo and army

00:11:07,680 --> 00:11:11,760
engineers

00:11:09,680 --> 00:11:14,240
finally i think the end goal like i said

00:11:11,760 --> 00:11:16,480
in the initial is we don't want to

00:11:14,240 --> 00:11:18,160
have a separate solution we want to be

00:11:16,480 --> 00:11:19,680
integrated into the default standard

00:11:18,160 --> 00:11:21,200
repo so we're kind of working with

00:11:19,680 --> 00:11:24,240
google when

00:11:21,200 --> 00:11:26,880
i touch community on that

00:11:24,240 --> 00:11:27,440
all right so this is about the ease of

00:11:26,880 --> 00:11:30,079
use

00:11:27,440 --> 00:11:31,760
so hopefully you can see that you can

00:11:30,079 --> 00:11:33,360
quickly get started on arm you have the

00:11:31,760 --> 00:11:36,000
docker images you have the

00:11:33,360 --> 00:11:36,399
packages if uh you're interested and you

00:11:36,000 --> 00:11:39,040
can get

00:11:36,399 --> 00:11:40,959
started with the latest version of

00:11:39,040 --> 00:11:42,640
tensorflow and pytorch

00:11:40,959 --> 00:11:44,240
i'll move track a little bit and i'm

00:11:42,640 --> 00:11:45,440
talk i'll talk about okay what are we

00:11:44,240 --> 00:11:46,959
doing to

00:11:45,440 --> 00:11:48,959
improve the performance we talked about

00:11:46,959 --> 00:11:50,880
the usability in the first section now

00:11:48,959 --> 00:11:54,240
we're going to talk about performance

00:11:50,880 --> 00:11:55,360
and performance using the features that

00:11:54,240 --> 00:11:58,959
are in on just

00:11:55,360 --> 00:11:58,959
which are useful to ml

00:11:59,040 --> 00:12:04,240
this is uh kind of a map of the

00:12:01,519 --> 00:12:06,399
different open source projects that

00:12:04,240 --> 00:12:08,320
help us with performance so you have the

00:12:06,399 --> 00:12:09,440
framework projects itself the tensorflow

00:12:08,320 --> 00:12:11,279
and pi dodge

00:12:09,440 --> 00:12:12,720
and you have the library projects i've

00:12:11,279 --> 00:12:15,200
listed four of them we'll talk

00:12:12,720 --> 00:12:16,480
talk about them a little bit um as we go

00:12:15,200 --> 00:12:19,360
along

00:12:16,480 --> 00:12:20,800
so the framework project itself is um

00:12:19,360 --> 00:12:23,040
yeah tensorflow

00:12:20,800 --> 00:12:24,560
it has multiple backends on x86 so you

00:12:23,040 --> 00:12:27,680
have the eigen backend

00:12:24,560 --> 00:12:28,959
for gebp is the default backend

00:12:27,680 --> 00:12:31,040
that this is the one which comes when

00:12:28,959 --> 00:12:31,920
google provides its tensorflow docker

00:12:31,040 --> 00:12:33,600
image

00:12:31,920 --> 00:12:35,680
and then there's this one dna back-end

00:12:33,600 --> 00:12:37,600
provided by intel

00:12:35,680 --> 00:12:39,519
which has many options like you can do a

00:12:37,600 --> 00:12:42,160
blast by intel and kl or

00:12:39,519 --> 00:12:44,160
you have direct kernels for editing

00:12:42,160 --> 00:12:46,079
which generates

00:12:44,160 --> 00:12:47,920
target specific kernel depending on what

00:12:46,079 --> 00:12:50,079
the hardware has

00:12:47,920 --> 00:12:52,079
on the pytorch again it has multiple

00:12:50,079 --> 00:12:54,480
back ends um

00:12:52,079 --> 00:12:56,959
nn attack was kind of the default back

00:12:54,480 --> 00:12:59,600
end but you can also use open blast and

00:12:56,959 --> 00:13:01,519
for intel uh these days one dna is

00:12:59,600 --> 00:13:03,440
integrated by default

00:13:01,519 --> 00:13:05,760
with again same features like glass and

00:13:03,440 --> 00:13:08,639
direct colors

00:13:05,760 --> 00:13:10,000
terms of libraries uh it's interesting

00:13:08,639 --> 00:13:11,360
i'll spend some time here because this

00:13:10,000 --> 00:13:14,399
has according to

00:13:11,360 --> 00:13:14,800
implications for performance eigen is

00:13:14,399 --> 00:13:16,480
the

00:13:14,800 --> 00:13:18,320
c plus plus template library that is

00:13:16,480 --> 00:13:20,079
used by tensorflow it has

00:13:18,320 --> 00:13:21,839
weight make two matrices and related

00:13:20,079 --> 00:13:22,639
algorithms now this is used for two

00:13:21,839 --> 00:13:24,880
reasons in

00:13:22,639 --> 00:13:27,120
tensorflow one just to represent

00:13:24,880 --> 00:13:29,920
internal data structures and operations

00:13:27,120 --> 00:13:31,600
so that is independent of what library

00:13:29,920 --> 00:13:34,399
used for acceleration

00:13:31,600 --> 00:13:36,880
but it also uses eigens gbp kernel as a

00:13:34,399 --> 00:13:39,120
depart cpu backend for fp32

00:13:36,880 --> 00:13:40,880
contraction so if you were to do a gem

00:13:39,120 --> 00:13:43,279
in tensorflow without having any

00:13:40,880 --> 00:13:47,199
specialized library or gpu

00:13:43,279 --> 00:13:49,120
it'll end up in this eigen gbb kernel

00:13:47,199 --> 00:13:50,480
we talked a little bit about one dnn

00:13:49,120 --> 00:13:52,160
this is intel's

00:13:50,480 --> 00:13:54,160
machine learning acceleration open

00:13:52,160 --> 00:13:55,440
source library it's integrated with all

00:13:54,160 --> 00:13:57,920
major frameworks

00:13:55,440 --> 00:13:58,480
we've been working with one dnn team to

00:13:57,920 --> 00:14:01,120
add

00:13:58,480 --> 00:14:02,320
ar 64 support we have an experimental

00:14:01,120 --> 00:14:04,560
support uh

00:14:02,320 --> 00:14:06,399
now i'll talk about it further in the in

00:14:04,560 --> 00:14:07,920
the coming slides

00:14:06,399 --> 00:14:09,519
arm compute library there have been many

00:14:07,920 --> 00:14:11,680
presentations today on

00:14:09,519 --> 00:14:14,160
arm compute library in rmn so this is

00:14:11,680 --> 00:14:17,440
our uh

00:14:14,160 --> 00:14:19,360
i think ml platform hosted

00:14:17,440 --> 00:14:20,639
arm contributes partners contribute it's

00:14:19,360 --> 00:14:22,240
an acceleration library

00:14:20,639 --> 00:14:24,079
it has high level operators like

00:14:22,240 --> 00:14:26,480
convolution real you

00:14:24,079 --> 00:14:28,880
name it a lot of lot of operators which

00:14:26,480 --> 00:14:31,199
can be used in one dnn

00:14:28,880 --> 00:14:32,639
so this is kind of important for service

00:14:31,199 --> 00:14:35,920
as well because

00:14:32,639 --> 00:14:37,839
uh we can use this existing investment

00:14:35,920 --> 00:14:40,240
by the economy ecosystem

00:14:37,839 --> 00:14:41,440
to make sure that the even server use

00:14:40,240 --> 00:14:42,880
cases are faster

00:14:41,440 --> 00:14:44,480
we don't have to reinvent the wheel in

00:14:42,880 --> 00:14:48,560
other words

00:14:44,480 --> 00:14:50,399
lastly the default solution before you

00:14:48,560 --> 00:14:51,839
go into specialized operator libraries

00:14:50,399 --> 00:14:53,760
like arm compute library

00:14:51,839 --> 00:14:55,519
is a blast and there there's an open

00:14:53,760 --> 00:14:56,880
source project which has the best

00:14:55,519 --> 00:14:58,639
performance

00:14:56,880 --> 00:15:00,240
amongst open source projects for glass

00:14:58,639 --> 00:15:02,000
is open flash which is

00:15:00,240 --> 00:15:06,320
most common and it's already integrated

00:15:02,000 --> 00:15:06,320
with fine touch and tensorflow

00:15:07,040 --> 00:15:10,720
all right quick introduction further

00:15:09,600 --> 00:15:14,079
details on

00:15:10,720 --> 00:15:16,240
arm compute library um it's typically

00:15:14,079 --> 00:15:19,360
used with armand but you can use it

00:15:16,240 --> 00:15:20,959
on top of uh on its own so that's how

00:15:19,360 --> 00:15:23,279
we are using it so and you have the

00:15:20,959 --> 00:15:25,680
cortex a cpu so this is the path

00:15:23,279 --> 00:15:27,440
we are more or less using for uh server

00:15:25,680 --> 00:15:30,639
work

00:15:27,440 --> 00:15:31,519
now how does this map to one dnn uh so

00:15:30,639 --> 00:15:34,720
one dnn

00:15:31,519 --> 00:15:35,920
is like an ml library it has many back

00:15:34,720 --> 00:15:37,839
end of its own

00:15:35,920 --> 00:15:39,440
so you can do a just a simple c plus

00:15:37,839 --> 00:15:40,560
plus reference code back in this is for

00:15:39,440 --> 00:15:42,639
correctness

00:15:40,560 --> 00:15:44,800
so there there's no libraries on either

00:15:42,639 --> 00:15:46,079
end but it also has a gem back in or a

00:15:44,800 --> 00:15:49,600
blast back in

00:15:46,079 --> 00:15:52,800
so there in intel land you have in klml

00:15:49,600 --> 00:15:54,720
on arm we have the rpl primitives but

00:15:52,800 --> 00:15:56,480
to get the best performance you ended up

00:15:54,720 --> 00:15:59,519
you end up using an intrinsic

00:15:56,480 --> 00:16:01,199
assembly so that's where we plan to use

00:15:59,519 --> 00:16:04,959
the arm compute library

00:16:01,199 --> 00:16:06,959
in one dna so to

00:16:04,959 --> 00:16:09,199
show in a diagrammatic form what this

00:16:06,959 --> 00:16:12,160
means for two different stacks

00:16:09,199 --> 00:16:12,880
for tensorflow the default option of

00:16:12,160 --> 00:16:14,639
eigen

00:16:12,880 --> 00:16:16,959
is already available today so you can

00:16:14,639 --> 00:16:18,160
see 2.3 and 2.2

00:16:16,959 --> 00:16:20,399
versions which were introduced in the

00:16:18,160 --> 00:16:22,240
last six months they both have well

00:16:20,399 --> 00:16:23,279
tested eigen implementations you can

00:16:22,240 --> 00:16:26,000
just use

00:16:23,279 --> 00:16:26,880
so the docker images and the packages i

00:16:26,000 --> 00:16:28,320
was talking about

00:16:26,880 --> 00:16:30,000
they come with these two options so you

00:16:28,320 --> 00:16:31,680
can use it with

00:16:30,000 --> 00:16:33,279
safety and this provides decent

00:16:31,680 --> 00:16:36,480
performance it's not super slow

00:16:33,279 --> 00:16:37,680
eigen gbp is a good project and it gives

00:16:36,480 --> 00:16:40,800
good performance it's not

00:16:37,680 --> 00:16:41,199
it's not super slow what we are working

00:16:40,800 --> 00:16:43,839
on

00:16:41,199 --> 00:16:45,360
is uh using the one dna back end that's

00:16:43,839 --> 00:16:47,519
available on x86

00:16:45,360 --> 00:16:49,360
we're adding an arm back into it and we

00:16:47,519 --> 00:16:50,000
have already added the arm performance

00:16:49,360 --> 00:16:52,959
libraries

00:16:50,000 --> 00:16:55,279
to it it will be part of the kind of the

00:16:52,959 --> 00:16:56,160
next release we expect in q4 of this

00:16:55,279 --> 00:16:58,639
year

00:16:56,160 --> 00:17:00,800
uh it's already part of one dnn 1.5

00:16:58,639 --> 00:17:01,360
release and we expect that to be part of

00:17:00,800 --> 00:17:03,920
the next

00:17:01,360 --> 00:17:04,720
2.4 times of your release there's a bit

00:17:03,920 --> 00:17:06,559
of lag of

00:17:04,720 --> 00:17:08,880
when we contribute to one dnn and then

00:17:06,559 --> 00:17:11,039
that gets picked up in tensorflow

00:17:08,880 --> 00:17:13,600
so this path is kind of already

00:17:11,039 --> 00:17:16,480
available and you can get the

00:17:13,600 --> 00:17:19,039
docker images today from the staging

00:17:16,480 --> 00:17:21,520
area that i mentioned earlier

00:17:19,039 --> 00:17:22,079
then we have the arm compute library

00:17:21,520 --> 00:17:25,600
option

00:17:22,079 --> 00:17:26,319
um we are aiming for like like the first

00:17:25,600 --> 00:17:30,000
release of

00:17:26,319 --> 00:17:31,840
next year of tensorflow and within that

00:17:30,000 --> 00:17:34,000
what it gives is it gives an intake and

00:17:31,840 --> 00:17:37,280
fp32 both options

00:17:34,000 --> 00:17:42,240
and as of

00:17:37,280 --> 00:17:44,320
this week we have got

00:17:42,240 --> 00:17:46,080
we have added support to one dnn to be

00:17:44,320 --> 00:17:49,679
able to call arm compute library

00:17:46,080 --> 00:17:50,559
operators for fp32 so the next release

00:17:49,679 --> 00:17:54,160
of one dnn

00:17:50,559 --> 00:17:57,360
1.7 will have the ability to call

00:17:54,160 --> 00:18:01,120
mcl or acl

00:17:57,360 --> 00:18:04,000
operators and we hope to

00:18:01,120 --> 00:18:06,480
improve that over next few months and by

00:18:04,000 --> 00:18:08,799
the time we get to end of the year

00:18:06,480 --> 00:18:10,240
we expect it to be much higher

00:18:08,799 --> 00:18:13,120
performing than

00:18:10,240 --> 00:18:13,120
where it is today

00:18:13,360 --> 00:18:18,720
on the pie tarts we are

00:18:17,200 --> 00:18:19,919
i mean there's a default open blast

00:18:18,720 --> 00:18:21,360
which is already available in the

00:18:19,919 --> 00:18:24,400
staging area that you can use

00:18:21,360 --> 00:18:26,799
it knows known to work well

00:18:24,400 --> 00:18:28,799
for 1.7 that's an upcoming release it'll

00:18:26,799 --> 00:18:30,880
still continue to be open blast back end

00:18:28,799 --> 00:18:32,960
uh we are working with the whatever we

00:18:30,880 --> 00:18:35,280
do for one dnn that will end up

00:18:32,960 --> 00:18:36,320
being integrated even with by torch

00:18:35,280 --> 00:18:39,280
stack

00:18:36,320 --> 00:18:40,880
so we expect that 1.8 release which is

00:18:39,280 --> 00:18:43,600
maybe end of the year

00:18:40,880 --> 00:18:44,559
uh will have an ability to just use one

00:18:43,600 --> 00:18:47,840
dnn

00:18:44,559 --> 00:18:50,880
and as and when both of these projects

00:18:47,840 --> 00:18:52,880
get support for one dnacl we'll also

00:18:50,880 --> 00:18:55,200
update the

00:18:52,880 --> 00:18:56,640
docker images for sure recipe docker

00:18:55,200 --> 00:18:58,480
images for sure

00:18:56,640 --> 00:19:00,640
and hopefully the python packages as

00:18:58,480 --> 00:19:00,640
well

00:19:01,200 --> 00:19:06,880
okay a quick update on the data type

00:19:04,559 --> 00:19:09,760
like i said there are many data types

00:19:06,880 --> 00:19:12,080
we are in parallel working on adding

00:19:09,760 --> 00:19:13,919
some of the data types as you can see

00:19:12,080 --> 00:19:15,799
the blast option is usually good for

00:19:13,919 --> 00:19:19,760
fp32 but when it comes to

00:19:15,799 --> 00:19:21,200
non-fb32 like fp16 data and float16

00:19:19,760 --> 00:19:24,000
it's not part of a standard blast

00:19:21,200 --> 00:19:27,919
interface so we plan to only do it

00:19:24,000 --> 00:19:30,240
for the acl paths uh so that will be

00:19:27,919 --> 00:19:31,280
intake the b float 16 is already part of

00:19:30,240 --> 00:19:32,880
acl

00:19:31,280 --> 00:19:35,600
as you would you would have seen in an

00:19:32,880 --> 00:19:37,600
earlier um

00:19:35,600 --> 00:19:39,600
is just the when it's integrated with

00:19:37,600 --> 00:19:41,360
tensorflow we need to ensure that it is

00:19:39,600 --> 00:19:43,520
able to use acl operators

00:19:41,360 --> 00:19:44,559
end to end and the other thing we

00:19:43,520 --> 00:19:47,120
typically do is

00:19:44,559 --> 00:19:47,840
uh work with acl team to improve the

00:19:47,120 --> 00:19:50,799
scaling

00:19:47,840 --> 00:19:52,720
because in edge devices you have maximum

00:19:50,799 --> 00:19:54,160
of eight to ten cores whereas in server

00:19:52,720 --> 00:19:55,600
you have a lot of course so

00:19:54,160 --> 00:19:57,919
you kind of work with them to ensure

00:19:55,600 --> 00:20:00,240
that happens

00:19:57,919 --> 00:20:01,600
all right so with that let me go to the

00:20:00,240 --> 00:20:05,280
final slide

00:20:01,600 --> 00:20:07,039
and this talks about

00:20:05,280 --> 00:20:09,120
how you can get involved machine

00:20:07,039 --> 00:20:11,919
learning alarm the

00:20:09,120 --> 00:20:13,039
easiest way is in user try the docker

00:20:11,919 --> 00:20:15,840
recipe and images

00:20:13,039 --> 00:20:17,360
images is a better place to start with

00:20:15,840 --> 00:20:18,480
if you are just using a standard

00:20:17,360 --> 00:20:20,400
combination

00:20:18,480 --> 00:20:21,840
but if you want to try a different

00:20:20,400 --> 00:20:23,840
combination then please

00:20:21,840 --> 00:20:25,840
start with the docker recipe if you have

00:20:23,840 --> 00:20:27,440
any issues you can raise

00:20:25,840 --> 00:20:29,360
any questions or issues you can raise it

00:20:27,440 --> 00:20:32,400
in that github repo

00:20:29,360 --> 00:20:35,360
arm compute library is our preferred

00:20:32,400 --> 00:20:36,720
acceleration library so anything if you

00:20:35,360 --> 00:20:38,080
want to contribute an arm compute

00:20:36,720 --> 00:20:41,200
library will end up

00:20:38,080 --> 00:20:45,120
helping servers as well as edge so um

00:20:41,200 --> 00:20:48,000
it's a lenoro hosted

00:20:45,120 --> 00:20:50,880
community project so any anybody is

00:20:48,000 --> 00:20:52,960
welcome to please work on that

00:20:50,880 --> 00:20:54,080
if you use docker images or recipes or

00:20:52,960 --> 00:20:55,679
python packages

00:20:54,080 --> 00:20:57,200
depending what performance you see for

00:20:55,679 --> 00:21:00,240
your application uh

00:20:57,200 --> 00:21:01,679
we are extremely keen to know how it's

00:21:00,240 --> 00:21:04,480
working for you

00:21:01,679 --> 00:21:07,200
this helps us focus where we spend our

00:21:04,480 --> 00:21:09,600
performance optimizations work

00:21:07,200 --> 00:21:11,360
and to get involved for all of this

00:21:09,600 --> 00:21:12,400
there's a weekly public meeting that i

00:21:11,360 --> 00:21:14,720
host

00:21:12,400 --> 00:21:15,919
and this happens in two different time

00:21:14,720 --> 00:21:19,120
zones

00:21:15,919 --> 00:21:23,120
every alternative weeks it happens in

00:21:19,120 --> 00:21:25,120
either apac asia pacific time zone or

00:21:23,120 --> 00:21:26,400
this week for example it will happen in

00:21:25,120 --> 00:21:29,440
u.s time zone

00:21:26,400 --> 00:21:31,679
so the details are in that link please

00:21:29,440 --> 00:21:35,360
look at that lincoln

00:21:31,679 --> 00:21:38,880
and we are ex will be excited to

00:21:35,360 --> 00:21:40,000
hear your views and what your concerns

00:21:38,880 --> 00:21:42,280
and

00:21:40,000 --> 00:21:45,280
co-work with you to move the ecosystem

00:21:42,280 --> 00:21:45,280

YouTube URL: https://www.youtube.com/watch?v=2pm5VDIyk4c


