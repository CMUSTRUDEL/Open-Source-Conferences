Title: LVC20-101 Make BigData fly on Arm64 - Apache Arrow by Yuqi Gu
Publication date: 2020-10-09
Playlist: Linaro Virtual Connect 2020
Description: 
	Make BigData fly on Arm64 - Apache Arrow
There are lots of data formats in the BigData world such as parquet file with Python(pandas), Spark dataframe, JSON, Avro, CSV, etc.

It would waste about 70-80% computation on data conversion and serialization/deserialization among different projects.

Apache Arrow addresses these issues and facilitates communication between many components with its high-speed in-memory representation for flat and hierarchical data. It would help to get 10-100x speedup on In-Memory analytics workloads.

Collaborating with Linaro LDCG, we validated Apache Arrow on Arm64 and delivered the Arm-related optimization for Arrow.

This session covered an overview of Apache Arrow, a brief introduction to Arrow optimization with Arm crypto and Neon extension and patches status submitted to the community. You will see the benchmark statistics results and how to take advantage of ARMv8 characteristics to make your data fly.

For the presentation see: https://static.linaro.org/connect/lvc20/presentations/LVC20-101-0.pdf
Captions: 
	00:00:00,080 --> 00:00:03,360
hello everyone and thanks for attending

00:00:02,399 --> 00:00:06,640
this session

00:00:03,360 --> 00:00:10,160
and my name is uichi from um

00:00:06,640 --> 00:00:13,599
i'm also the narrator me working in

00:00:10,160 --> 00:00:16,720
ldcg big data team and in this

00:00:13,599 --> 00:00:20,080
session i would like to

00:00:16,720 --> 00:00:20,800
talk about how to you know make big data

00:00:20,080 --> 00:00:24,240
fly

00:00:20,800 --> 00:00:27,439
on i'm 64. i also would

00:00:24,240 --> 00:00:29,119
like to introduce a project named apache

00:00:27,439 --> 00:00:31,679
aero to all of you

00:00:29,119 --> 00:00:32,320
and to share something and share what we

00:00:31,679 --> 00:00:36,480
have done

00:00:32,320 --> 00:00:39,040
on this project yeah and

00:00:36,480 --> 00:00:40,640
okay nowadays there are some problems

00:00:39,040 --> 00:00:43,680
you know and

00:00:40,640 --> 00:00:47,360
like reporting a new

00:00:43,680 --> 00:00:49,360
products forecasting threat detection

00:00:47,360 --> 00:00:50,640
machine learning and the freight

00:00:49,360 --> 00:00:53,440
prevention

00:00:50,640 --> 00:00:55,039
you know there are more demands for data

00:00:53,440 --> 00:00:58,559
are increasing

00:00:55,039 --> 00:01:02,399
dramatically so so then

00:00:58,559 --> 00:01:05,280
um modern and data analytics is

00:01:02,399 --> 00:01:06,400
a huge challenge for all of us and it's

00:01:05,280 --> 00:01:09,439
also a challenge

00:01:06,400 --> 00:01:13,280
to you know to make data fly

00:01:09,439 --> 00:01:15,200
um i'm 64. it's a

00:01:13,280 --> 00:01:16,320
you know in the data world in dictator

00:01:15,200 --> 00:01:18,960
world you know um

00:01:16,320 --> 00:01:19,600
there's some limits here you know and

00:01:18,960 --> 00:01:23,119
every

00:01:19,600 --> 00:01:27,840
every database and the language has to

00:01:23,119 --> 00:01:27,840
implement its own internal data format

00:01:28,000 --> 00:01:33,680
it generates a lot of uh web

00:01:31,040 --> 00:01:34,079
when you're moving data from one system

00:01:33,680 --> 00:01:37,200
to

00:01:34,079 --> 00:01:40,400
another access term involves cost

00:01:37,200 --> 00:01:43,920
costly you know serialization and

00:01:40,400 --> 00:01:47,920
just a serialization you know and it's

00:01:43,920 --> 00:01:52,240
almost always to you know uh 60 percent

00:01:47,920 --> 00:01:55,600
cpu waste on data copy and

00:01:52,240 --> 00:01:57,439
conversion disk primary and duplication

00:01:55,600 --> 00:02:00,320
is unnecessary

00:01:57,439 --> 00:02:01,680
in addition and being additional more

00:02:00,320 --> 00:02:06,320
algorithm

00:02:01,680 --> 00:02:08,959
also must be often be rewriting

00:02:06,320 --> 00:02:11,120
for each data format between you know

00:02:08,959 --> 00:02:15,280
these systems

00:02:11,120 --> 00:02:18,720
this is a very big problem for us

00:02:15,280 --> 00:02:22,080
and they ask how to take this

00:02:18,720 --> 00:02:25,360
problem and let's see and

00:02:22,080 --> 00:02:27,360
first you know we will we

00:02:25,360 --> 00:02:28,560
would like to put everything in memory

00:02:27,360 --> 00:02:32,720
you know in memory

00:02:28,560 --> 00:02:35,680
it's because you know the low speed disk

00:02:32,720 --> 00:02:39,280
is the bottleneck of the i o performance

00:02:35,680 --> 00:02:41,599
so we can take up advantage of the

00:02:39,280 --> 00:02:44,720
faster transaction and

00:02:41,599 --> 00:02:47,840
multi-use currency by leveraging

00:02:44,720 --> 00:02:49,120
in a memory database you know it will

00:02:47,840 --> 00:02:53,200
accelerate our

00:02:49,120 --> 00:02:56,640
performance and the other choice is a

00:02:53,200 --> 00:03:00,319
gpu in our gpu is the parallel computing

00:02:56,640 --> 00:03:01,360
architecture each gpu can accelerate our

00:03:00,319 --> 00:03:07,280
data science

00:03:01,360 --> 00:03:10,319
workflows and our in our data analytics

00:03:07,280 --> 00:03:13,840
and the third one is we

00:03:10,319 --> 00:03:14,879
can you know uh put you we can convert

00:03:13,840 --> 00:03:18,319
data to uh

00:03:14,879 --> 00:03:21,760
columnar data format you know the

00:03:18,319 --> 00:03:25,440
benefits of the columnar data formats

00:03:21,760 --> 00:03:26,239
are you know the cache pipeline and smit

00:03:25,440 --> 00:03:28,720
smmd

00:03:26,239 --> 00:03:29,840
means single instruction and multiple

00:03:28,720 --> 00:03:33,040
data

00:03:29,840 --> 00:03:36,159
and you know the climate and the column

00:03:33,040 --> 00:03:39,239
data also and can be you know

00:03:36,159 --> 00:03:44,159
used to gpu um

00:03:39,239 --> 00:03:46,799
vectorized processing and we also can

00:03:44,159 --> 00:03:48,480
you know leverage you know to distribute

00:03:46,799 --> 00:03:52,319
the systems we can

00:03:48,480 --> 00:03:55,360
take advantage of the scalability and

00:03:52,319 --> 00:03:58,400
in distributed system also this

00:03:55,360 --> 00:04:01,599
distributed system has you know faster

00:03:58,400 --> 00:04:04,879
calculation speed and also

00:04:01,599 --> 00:04:07,120
is the high for totterings okay

00:04:04,879 --> 00:04:07,120
so

00:04:08,239 --> 00:04:14,400
here we may this these four points

00:04:11,680 --> 00:04:15,280
mentioned above you know the solution to

00:04:14,400 --> 00:04:18,720
tackle these

00:04:15,280 --> 00:04:21,840
problems but the arrow is

00:04:18,720 --> 00:04:26,400
it's a you know the in-memory you know

00:04:21,840 --> 00:04:29,919
data format error also can be

00:04:26,400 --> 00:04:33,440
deployed as the distributed modes

00:04:29,919 --> 00:04:37,040
arrow also was integrated you know in

00:04:33,440 --> 00:04:40,080
some gpu manual architecture

00:04:37,040 --> 00:04:42,639
so arrow is to process

00:04:40,080 --> 00:04:44,960
and move data faster you know error is

00:04:42,639 --> 00:04:47,759
designed to make things faster

00:04:44,960 --> 00:04:50,400
it's a it focused on speeding

00:04:47,759 --> 00:04:53,680
communication between systems as well as

00:04:50,400 --> 00:04:57,840
processing with only one system

00:04:53,680 --> 00:04:57,840
that's very efficient

00:04:57,919 --> 00:05:02,639
um errol um specifies uh standard

00:05:01,280 --> 00:05:04,960
language

00:05:02,639 --> 00:05:05,759
you know it's the independent you know

00:05:04,960 --> 00:05:08,800
the

00:05:05,759 --> 00:05:12,080
columnar memory format for flatbed

00:05:08,800 --> 00:05:15,440
okay it air also provides the

00:05:12,080 --> 00:05:18,639
inter process communication

00:05:15,440 --> 00:05:22,000
uh i will also support um you mean

00:05:18,639 --> 00:05:25,280
zero copy streaming message and

00:05:22,000 --> 00:05:28,560
computational libraries um

00:05:25,280 --> 00:05:31,680
and arrow is you know organized

00:05:28,560 --> 00:05:35,039
i will design the columnar uh

00:05:31,680 --> 00:05:36,479
data format so the better the column

00:05:35,039 --> 00:05:39,600
data formats

00:05:36,479 --> 00:05:42,720
are about is it can take

00:05:39,600 --> 00:05:46,240
advantage of the hardware like the

00:05:42,720 --> 00:05:50,000
pipelining smit casual quality

00:05:46,240 --> 00:05:53,120
and to maximum you know cpu throughput

00:05:50,000 --> 00:05:56,160
and arrow arrows libraries

00:05:53,120 --> 00:05:57,199
implemented format and provide the

00:05:56,160 --> 00:06:00,560
building blocks

00:05:57,199 --> 00:06:04,720
including high performance analytics

00:06:00,560 --> 00:06:08,639
and many you know popular projects like

00:06:04,720 --> 00:06:11,280
spark and pocky pandas

00:06:08,639 --> 00:06:12,479
use arrow to ship columnar data

00:06:11,280 --> 00:06:17,039
efficiently

00:06:12,479 --> 00:06:20,560
or as the basis for analytic engines

00:06:17,039 --> 00:06:23,680
the libraries are available for now for

00:06:20,560 --> 00:06:27,360
available for c and c plus plus c

00:06:23,680 --> 00:06:30,960
sharp gold java and java graph

00:06:27,360 --> 00:06:34,560
and the rust you know

00:06:30,960 --> 00:06:37,440
arrow the columnar is fast

00:06:34,560 --> 00:06:38,880
arrow can enable you to work with data

00:06:37,440 --> 00:06:41,440
in aerocluma format

00:06:38,880 --> 00:06:44,000
in many languages to support many

00:06:41,440 --> 00:06:44,000
libraries

00:06:44,319 --> 00:06:52,720
okay and you can see the picture

00:06:49,120 --> 00:06:56,319
showing the size of our system

00:06:52,720 --> 00:06:59,520
utilize the same memory and format

00:06:56,319 --> 00:07:02,479
and no over not overhead for cross

00:06:59,520 --> 00:07:03,520
system communication it means zero copy

00:07:02,479 --> 00:07:07,360
data process

00:07:03,520 --> 00:07:10,639
exchanger you know metadata and

00:07:07,360 --> 00:07:13,680
points to aerodata

00:07:10,639 --> 00:07:15,919
arrays and arrow libraries

00:07:13,680 --> 00:07:17,400
also enable you know third party

00:07:15,919 --> 00:07:20,720
projects to work with

00:07:17,400 --> 00:07:23,759
outstate without have uh without

00:07:20,720 --> 00:07:26,960
having to implement the arrow

00:07:23,759 --> 00:07:30,160
klamena parameter format themselves it

00:07:26,960 --> 00:07:33,520
will help you know this

00:07:30,160 --> 00:07:36,560
third part libraries can be integrated

00:07:33,520 --> 00:07:36,560
in their projects

00:07:37,360 --> 00:07:44,400
okay arrow also simplifies the system

00:07:41,520 --> 00:07:45,759
architectures to you know improve

00:07:44,400 --> 00:07:49,360
improves

00:07:45,759 --> 00:07:52,919
uh interrupt improves and improved

00:07:49,360 --> 00:07:56,000
performance and also reduced ecosystems

00:07:52,919 --> 00:07:56,400
fragmentation it's easy to put a data

00:07:56,000 --> 00:08:00,240
set

00:07:56,400 --> 00:08:03,199
into our our language to system

00:08:00,240 --> 00:08:03,759
for machine learning in some use case

00:08:03,199 --> 00:08:06,560
without

00:08:03,759 --> 00:08:07,440
any overhead of the serialization the

00:08:06,560 --> 00:08:10,800
disability

00:08:07,440 --> 00:08:14,000
okay and you know the like this

00:08:10,800 --> 00:08:17,199
table shoes the pandas has its own

00:08:14,000 --> 00:08:20,639
data format a pi spark

00:08:17,199 --> 00:08:23,680
also has they all when they

00:08:20,639 --> 00:08:25,360
take we can leverage the arrow they all

00:08:23,680 --> 00:08:29,440
use the you know the

00:08:25,360 --> 00:08:33,039
same data representation

00:08:29,440 --> 00:08:36,320
we call it we call this format is

00:08:33,039 --> 00:08:39,360
arrow batch so you may

00:08:36,320 --> 00:08:43,120
ask what is the arrow badge

00:08:39,360 --> 00:08:46,320
and we can decide i will introduce to

00:08:43,120 --> 00:08:49,760
the arrow known for that yes okay

00:08:46,320 --> 00:08:54,000
and arrow economic format is also called

00:08:49,760 --> 00:08:56,880
its common message pattern and

00:08:54,000 --> 00:08:58,399
it is consisted of the you know the

00:08:56,880 --> 00:09:01,760
scheme integration

00:08:58,399 --> 00:09:03,680
block and the dictionary badge and the

00:09:01,760 --> 00:09:06,480
record badge

00:09:03,680 --> 00:09:07,279
for you know and this common message

00:09:06,480 --> 00:09:11,519
pattern

00:09:07,279 --> 00:09:14,560
we send a series of message

00:09:11,519 --> 00:09:16,640
you know the skim integration and one or

00:09:14,560 --> 00:09:19,519
more record patches

00:09:16,640 --> 00:09:19,839
and the schema integration is you know

00:09:19,519 --> 00:09:22,720
the

00:09:19,839 --> 00:09:23,920
is the logical discrimination of the

00:09:22,720 --> 00:09:27,839
structure

00:09:23,920 --> 00:09:30,959
and also is the identification of the

00:09:27,839 --> 00:09:34,800
dictionary encoded the nodes

00:09:30,959 --> 00:09:38,560
and dictionary patch is to contain some

00:09:34,800 --> 00:09:41,760
you know dictionary id and its values

00:09:38,560 --> 00:09:44,959
and the record batch is

00:09:41,760 --> 00:09:48,160
is to order the collection of the

00:09:44,959 --> 00:09:51,360
named arrow data arise

00:09:48,160 --> 00:09:55,760
okay and another format is the

00:09:51,360 --> 00:09:58,959
reading access file rename the file is

00:09:55,760 --> 00:10:02,000
consist of the you know record badge and

00:09:58,959 --> 00:10:03,120
the schema file layout schema file

00:10:02,000 --> 00:10:06,160
layout is

00:10:03,120 --> 00:10:09,360
the table schema and it contains

00:10:06,160 --> 00:10:12,399
it contains table schema and block

00:10:09,360 --> 00:10:16,000
location at the end of

00:10:12,399 --> 00:10:17,279
random access file it enables users to

00:10:16,000 --> 00:10:20,320
select

00:10:17,279 --> 00:10:22,800
any recorded batch or any color

00:10:20,320 --> 00:10:23,600
in the data set very cheaply and very

00:10:22,800 --> 00:10:26,480
quick

00:10:23,600 --> 00:10:26,480
efficiently

00:10:26,720 --> 00:10:34,240
okay this is this is the

00:10:30,480 --> 00:10:36,880
example of the kinema data uh

00:10:34,240 --> 00:10:37,760
this data is is one person okay it's

00:10:36,880 --> 00:10:40,640
some person's

00:10:37,760 --> 00:10:42,000
this name jack the rose and the age

00:10:40,640 --> 00:10:46,480
their phones

00:10:42,000 --> 00:10:49,920
in aerocram format they uh

00:10:46,480 --> 00:10:53,120
arrow put them together and put eight

00:10:49,920 --> 00:10:56,240
data together and put phones together

00:10:53,120 --> 00:10:59,519
as a different color you know

00:10:56,240 --> 00:11:03,279
just showing the slides you know and

00:10:59,519 --> 00:11:06,320
in nam chrom you know the offset of zero

00:11:03,279 --> 00:11:09,519
is point to jack and

00:11:06,320 --> 00:11:13,839
and by offset four and it's

00:11:09,519 --> 00:11:16,880
the point to a rose and the age is

00:11:13,839 --> 00:11:20,880
here the phones is here you know

00:11:16,880 --> 00:11:26,800
and they are very you know they are

00:11:20,880 --> 00:11:26,800
they are organized in columnar format

00:11:27,600 --> 00:11:33,839
um okay go back to uh

00:11:30,800 --> 00:11:37,040
arrow granite uh

00:11:33,839 --> 00:11:40,000
this is to show the details in the

00:11:37,040 --> 00:11:41,120
record batch okay in record in regular

00:11:40,000 --> 00:11:43,760
batch

00:11:41,120 --> 00:11:44,320
it is consists of the you know data

00:11:43,760 --> 00:11:47,760
header

00:11:44,320 --> 00:11:50,800
and and the name data

00:11:47,760 --> 00:11:54,560
followed by this header and the age

00:11:50,800 --> 00:11:57,920
and the phone okay each box is

00:11:54,560 --> 00:12:00,959
the contiguous memory and

00:11:57,920 --> 00:12:04,079
we can utilize the casual

00:12:00,959 --> 00:12:07,279
locality it's per it's

00:12:04,079 --> 00:12:10,399
pretty efficient efficiently

00:12:07,279 --> 00:12:13,440
for uh you know in the memory

00:12:10,399 --> 00:12:16,800
it's the

00:12:13,440 --> 00:12:20,160
continuous uh camera layout you know

00:12:16,800 --> 00:12:23,680
uh will leverage just smid

00:12:20,160 --> 00:12:27,040
modern processors you know the

00:12:23,680 --> 00:12:30,720
casual locality and it also can

00:12:27,040 --> 00:12:33,839
minimal just structure overhead and just

00:12:30,720 --> 00:12:37,200
to support you know the the

00:12:33,839 --> 00:12:40,160
constant battle excess or it also can

00:12:37,200 --> 00:12:41,519
operate directly on a columnar

00:12:40,160 --> 00:12:44,079
compressed data

00:12:41,519 --> 00:12:44,079
in our team

00:12:46,079 --> 00:12:51,920
okay it's in

00:12:49,360 --> 00:12:52,399
the next in this part and i would like

00:12:51,920 --> 00:12:55,600
to

00:12:52,399 --> 00:12:58,000
what we have done for apachero

00:12:55,600 --> 00:12:59,279
and the first two we are leveraging to

00:12:58,000 --> 00:13:02,399
leverage

00:12:59,279 --> 00:13:06,959
the cst parallel other reason

00:13:02,399 --> 00:13:12,320
to optimize the harsh computation always

00:13:06,959 --> 00:13:14,959
crc extraction and crypto exchanging

00:13:12,320 --> 00:13:17,200
you can see this table if it's charged

00:13:14,959 --> 00:13:20,320
and

00:13:17,200 --> 00:13:24,079
the hash function was only

00:13:20,320 --> 00:13:28,000
just uh implemented by uh software

00:13:24,079 --> 00:13:31,680
it gets the 0.9

00:13:28,000 --> 00:13:36,079
you know kilobytes per second throughout

00:13:31,680 --> 00:13:39,760
throughput and if we leverage

00:13:36,079 --> 00:13:43,959
um crc 32 instructions

00:13:39,760 --> 00:13:48,480
and crypto extension we can get

00:13:43,959 --> 00:13:51,519
8.8 you know kilobytes per second

00:13:48,480 --> 00:13:54,880
we get you know the knife or speed up

00:13:51,519 --> 00:13:58,000
the performance boost is we get

00:13:54,880 --> 00:14:01,360
the performance boost on

00:13:58,000 --> 00:14:04,720
ax64 the next

00:14:01,360 --> 00:14:06,800
we also have to leverage our new

00:14:04,720 --> 00:14:10,639
instructions to

00:14:06,800 --> 00:14:13,760
increment uh the function of the uh

00:14:10,639 --> 00:14:17,519
validating the acid communal string

00:14:13,760 --> 00:14:20,639
function for arrow and

00:14:17,519 --> 00:14:24,639
you know and as as you see in

00:14:20,639 --> 00:14:27,680
this slides and in a

00:14:24,639 --> 00:14:30,880
large asset string data science case

00:14:27,680 --> 00:14:34,240
in this case we got

00:14:30,880 --> 00:14:35,519
you you see the 29 gigabytes per second

00:14:34,240 --> 00:14:39,120
throughput

00:14:35,519 --> 00:14:40,639
um compared to just to a software

00:14:39,120 --> 00:14:44,160
implementation

00:14:40,639 --> 00:14:47,040
we got about a 25

00:14:44,160 --> 00:14:47,040
for speed up

00:14:48,480 --> 00:14:55,920
and we also add support to

00:14:52,240 --> 00:14:59,920
armor you know cpu info detection for r

00:14:55,920 --> 00:15:03,760
we also implement arrow memory benchmark

00:14:59,920 --> 00:15:07,440
for arm 64. we also fixed

00:15:03,760 --> 00:15:11,199
cpu info compiling issues

00:15:07,440 --> 00:15:14,240
and we also implemented arrow none

00:15:11,199 --> 00:15:17,760
acid validation function

00:15:14,240 --> 00:15:21,279
and also fixed some memory bench

00:15:17,760 --> 00:15:24,160
issues here and it was high

00:15:21,279 --> 00:15:25,199
he is the narrow member engineer from

00:15:24,160 --> 00:15:29,680
your team

00:15:25,199 --> 00:15:32,720
also made a huge contributions to

00:15:29,680 --> 00:15:33,360
aero community if you are interested

00:15:32,720 --> 00:15:36,880
into what

00:15:33,360 --> 00:15:40,399
we have done in detail you can see

00:15:36,880 --> 00:15:43,519
the dura tickets list listed in this

00:15:40,399 --> 00:15:45,839
slide and the ipo thai and me

00:15:43,519 --> 00:15:47,600
are the members of the contributors of

00:15:45,839 --> 00:15:51,279
the

00:15:47,600 --> 00:15:55,839
apache aero 1.0 released

00:15:51,279 --> 00:15:55,839
this is official released

00:15:56,000 --> 00:16:00,000
okay uh in just in this part i will show

00:15:59,040 --> 00:16:02,560
you to demo

00:16:00,000 --> 00:16:04,880
all of you to show the huge you know

00:16:02,560 --> 00:16:08,240
performance booster

00:16:04,880 --> 00:16:11,199
64. the first one is to

00:16:08,240 --> 00:16:13,120
be spinning up to pi spot it's apache

00:16:11,199 --> 00:16:16,639
arrow on

00:16:13,120 --> 00:16:20,240
64. you know uh in

00:16:16,639 --> 00:16:20,639
the functioning spark sql it will enable

00:16:20,240 --> 00:16:24,079
to

00:16:20,639 --> 00:16:27,920
you know the arrow by this uh

00:16:24,079 --> 00:16:31,680
this command and i it will

00:16:27,920 --> 00:16:32,560
convert to a convert the data framing

00:16:31,680 --> 00:16:36,560
spark to

00:16:32,560 --> 00:16:39,839
uh data framing impenders we can get

00:16:36,560 --> 00:16:44,240
about 14 or 44

00:16:39,839 --> 00:16:47,519
speed up okay let me show you

00:16:44,240 --> 00:16:48,079
let me show you the details and the

00:16:47,519 --> 00:16:53,519
first

00:16:48,079 --> 00:16:53,519
ever since this ah this is the r

00:16:54,560 --> 00:17:05,839
this is the arm okay and

00:16:57,600 --> 00:17:05,839
we round the pi spark here

00:17:09,360 --> 00:17:16,720
now first we import some

00:17:12,720 --> 00:17:20,400
facilities here see

00:17:16,720 --> 00:17:23,919
in the second we can create our

00:17:20,400 --> 00:17:26,559
data sets this is simple

00:17:23,919 --> 00:17:26,559
data sets

00:17:29,200 --> 00:17:37,679
okay let's let me show you

00:17:32,720 --> 00:17:41,440
the schema the data schema here

00:17:37,679 --> 00:17:44,320
let's print it's the id is so

00:17:41,440 --> 00:17:46,160
it's the wrong data type is the x is the

00:17:44,320 --> 00:17:49,520
double okay

00:17:46,160 --> 00:17:53,280
okay let's let me

00:17:49,520 --> 00:17:56,320
show you how long you know

00:17:53,280 --> 00:17:59,600
this data schema how long we

00:17:56,320 --> 00:18:03,600
how long will we take to convert this

00:17:59,600 --> 00:18:03,600
data to a panda data frame

00:18:03,760 --> 00:18:15,840
in this way sorry

00:18:15,919 --> 00:18:23,919
just wait a minute as about so i take

00:18:19,520 --> 00:18:23,919
about one minute

00:18:26,840 --> 00:18:31,200
yes

00:18:28,880 --> 00:18:31,200
okay

00:18:40,960 --> 00:18:45,200
you know it's take a long time to

00:18:43,039 --> 00:18:57,840
convert this data to

00:18:45,200 --> 00:18:57,840
pandas data frame

00:18:58,080 --> 00:19:06,960
okay it takes totally uh 46 seconds

00:19:03,760 --> 00:19:10,000
so now let's enable um

00:19:06,960 --> 00:19:12,559
arrow in the spark okay

00:19:10,000 --> 00:19:12,559
let's see

00:19:15,919 --> 00:19:18,320
sorry

00:19:22,880 --> 00:19:25,840
okay

00:19:27,840 --> 00:19:32,240
let's see again when we enable the arrow

00:19:31,600 --> 00:19:35,760
in

00:19:32,240 --> 00:19:39,360
spark how long will we take to convert

00:19:35,760 --> 00:19:42,160
this data

00:19:39,360 --> 00:19:43,200
okay let's just take only one second

00:19:42,160 --> 00:19:47,679
okay

00:19:43,200 --> 00:19:52,400
we get we get huge performance

00:19:47,679 --> 00:19:56,080
here by you know by being able to

00:19:52,400 --> 00:20:00,080
to spark being able to uh arrow in spark

00:19:56,080 --> 00:20:03,679
it's about just remained 40

00:20:00,080 --> 00:20:03,679
a foot speed up here

00:20:03,760 --> 00:20:12,720
okay the the next demo is

00:20:07,280 --> 00:20:12,720
streaming arrow performance on af64

00:20:12,960 --> 00:20:19,600
in this demo i will first we will first

00:20:16,480 --> 00:20:23,679
generate example dataset

00:20:19,600 --> 00:20:27,039
it's we will create a one gigabyte

00:20:23,679 --> 00:20:29,360
size totally and then we will

00:20:27,039 --> 00:20:30,080
construct and write this data to the

00:20:29,360 --> 00:20:33,120
memory

00:20:30,080 --> 00:20:33,520
and the same reason and data back to the

00:20:33,120 --> 00:20:36,640
back

00:20:33,520 --> 00:20:39,200
and to see how fast aero streaming

00:20:36,640 --> 00:20:39,200
performance

00:20:46,840 --> 00:20:49,840
okay

00:20:54,720 --> 00:20:57,039
okay

00:20:58,320 --> 00:21:04,240
let's see this um this demo script first

00:21:01,919 --> 00:21:04,240
okay

00:21:06,000 --> 00:21:13,520
okay in this demo i will create a

00:21:09,760 --> 00:21:17,120
gigabyte data size

00:21:13,520 --> 00:21:20,159
and i will also you know

00:21:17,120 --> 00:21:23,360
divide this this data into

00:21:20,159 --> 00:21:26,720
the different blocks means

00:21:23,360 --> 00:21:30,320
chunk size it's range to 16

00:21:26,720 --> 00:21:31,919
kilobytes to 16 megabytes and

00:21:30,320 --> 00:21:34,799
see the performance and to see the

00:21:31,919 --> 00:21:38,080
performance difference

00:21:34,799 --> 00:21:41,520
let's see it okay this is the

00:21:38,080 --> 00:21:42,840
you know the 16 kilobytes chunk we take

00:21:41,520 --> 00:21:47,280
this time we take

00:21:42,840 --> 00:21:50,559
about six seconds when the chunk size

00:21:47,280 --> 00:21:53,600
is you know the 16

00:21:50,559 --> 00:21:57,520
megabytes we just take

00:21:53,600 --> 00:22:01,520
only as 0.06

00:21:57,520 --> 00:22:05,840
seconds to you know to handle this

00:22:01,520 --> 00:22:05,840
conversion okay let's see it

00:22:08,640 --> 00:22:12,000
okay and as the you know as the

00:22:10,799 --> 00:22:15,039
streaming the chunk

00:22:12,000 --> 00:22:15,679
size grows smaller you know the the cost

00:22:15,039 --> 00:22:19,280
of the

00:22:15,679 --> 00:22:22,720
risk reconstructor uh contributes the

00:22:19,280 --> 00:22:25,600
climate pandas data frame increase okay

00:22:22,720 --> 00:22:26,240
because because of the because of the

00:22:25,600 --> 00:22:28,640
cash

00:22:26,240 --> 00:22:29,520
in efficiency you know memory access

00:22:28,640 --> 00:22:34,080
patterns

00:22:29,520 --> 00:22:37,840
so we can see that for the broadest

00:22:34,080 --> 00:22:40,400
16 megabytes you know

00:22:37,840 --> 00:22:42,799
just we can get the throughput about to

00:22:40,400 --> 00:22:46,559
22 gigabytes per second

00:22:42,799 --> 00:22:50,000
to restructure a one gigabyte

00:22:46,559 --> 00:22:54,880
data frame it's really

00:22:50,000 --> 00:22:57,679
efficient okay

00:22:54,880 --> 00:22:58,480
okay the last and at the last i would

00:22:57,679 --> 00:23:01,840
like to

00:22:58,480 --> 00:23:05,280
thanks ippo uh from foreign and

00:23:01,840 --> 00:23:06,080
also thanks to leonardo and dcg big data

00:23:05,280 --> 00:23:10,480
team

00:23:06,080 --> 00:23:10,480
ganesh and and

00:23:10,640 --> 00:23:17,039
it's all for my sharing today

00:23:13,919 --> 00:23:20,480
and if you have any questions please

00:23:17,039 --> 00:23:24,640
feel free to contact us by email

00:23:20,480 --> 00:23:24,640
post here okay

00:23:25,360 --> 00:23:30,640
thank you very much sets off from my

00:23:27,640 --> 00:23:30,640

YouTube URL: https://www.youtube.com/watch?v=lijl-w_4AC4


