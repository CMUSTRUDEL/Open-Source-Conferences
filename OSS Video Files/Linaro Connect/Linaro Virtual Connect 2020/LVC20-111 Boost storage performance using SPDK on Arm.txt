Title: LVC20-111 Boost storage performance using SPDK on Arm
Publication date: 2020-09-20
Playlist: Linaro Virtual Connect 2020
Description: 
	SPDK on Arm
Arm servers, an important player in data center technology, are well suited for storage workloads. It's critical to explore techniques to improve storage performance on Arm servers. This session shares practices to boost storage IO performance on Arm servers with SPDK, which get extremely high performance with techniques of running at user level, Poll Mode Drivers (PMDs) and shared-nothing design.

We have done much work to dig what SPDK brings for Arm servers, including using SPDK NVMe-over-RDMA to access remote NVMe devices, using SPDK OCF to enhance the low speed devices and SPDK Vhost to improve the IOs of virtual machines. And we collect many exciting profiling data to share. Besides, optimization has been made in base64, CRC and atomic operations utilizing Arm hardware features, which are also meaningful to other projects optimization on Arm.

For the presentation see: https://static.linaro.org/connect/lvc20/presentations/LVC20-111-0.pdf
Captions: 
	00:00:00,240 --> 00:00:07,120
hi i'm rachel from um

00:00:03,280 --> 00:00:10,240
my topic is embracing high performance

00:00:07,120 --> 00:00:14,920
storage with open arm

00:00:10,240 --> 00:00:16,640
it's about our practice of spdk on

00:00:14,920 --> 00:00:19,840
m64

00:00:16,640 --> 00:00:21,840
sbdk is the storage performance

00:00:19,840 --> 00:00:24,880
development kit

00:00:21,840 --> 00:00:28,560
it provides a set of tools and

00:00:24,880 --> 00:00:33,120
libraries to create high performance

00:00:28,560 --> 00:00:36,399
scalable user mode storage applications

00:00:33,120 --> 00:00:37,760
this picture shows the spd case

00:00:36,399 --> 00:00:40,879
architecture

00:00:37,760 --> 00:00:41,840
in many includes four layers from the

00:00:40,879 --> 00:00:45,120
bottom app

00:00:41,840 --> 00:00:48,320
it's the driver data including

00:00:45,120 --> 00:00:48,879
a mimi driver and so on and then the

00:00:48,320 --> 00:00:51,840
block

00:00:48,879 --> 00:00:51,840
storage layer

00:00:52,000 --> 00:00:58,239
it includes other

00:00:55,199 --> 00:01:02,079
maybe block device and linux

00:00:58,239 --> 00:01:06,240
aio product device io rim product device

00:01:02,079 --> 00:01:10,000
and many others and this layer is

00:01:06,240 --> 00:01:13,119
block storage service layer it provides

00:01:10,000 --> 00:01:16,400
several services for different purposes

00:01:13,119 --> 00:01:19,600
like logical volume red

00:01:16,400 --> 00:01:22,880
open channel and so on and the top layer

00:01:19,600 --> 00:01:26,159
is the storage protocol there

00:01:22,880 --> 00:01:30,640
including the mmeo fabrics the

00:01:26,159 --> 00:01:33,759
big host targets iscsi targets

00:01:30,640 --> 00:01:34,159
leds are capable of serving disks over

00:01:33,759 --> 00:01:38,000
the

00:01:34,159 --> 00:01:41,600
network or to other processes besides

00:01:38,000 --> 00:01:45,439
it implements a prop fs as

00:01:41,600 --> 00:01:45,439
a file storage service

00:01:47,600 --> 00:01:51,520
we know that a traditional kernel io

00:01:50,560 --> 00:01:55,280
stats bring

00:01:51,520 --> 00:01:58,000
upwares overhead due to content switch

00:01:55,280 --> 00:01:59,119
data copy interrupt resource

00:01:58,000 --> 00:02:03,119
synchronization

00:01:59,119 --> 00:02:06,399
and so on svdk can minimize it

00:02:03,119 --> 00:02:10,319
minimize the impact on cpu and

00:02:06,399 --> 00:02:14,160
memory bus cycles during i o processing

00:02:10,319 --> 00:02:16,640
mainly by three ways one is

00:02:14,160 --> 00:02:17,920
uses the user modes for storage

00:02:16,640 --> 00:02:21,760
applications

00:02:17,920 --> 00:02:25,040
rather than the kernel mode the devices

00:02:21,760 --> 00:02:28,480
let spdk test control

00:02:25,040 --> 00:02:31,920
are unbound from the kernel space driver

00:02:28,480 --> 00:02:32,640
and instead your io of vfio driver is

00:02:31,920 --> 00:02:36,160
used

00:02:32,640 --> 00:02:39,200
for spdk to operate devices

00:02:36,160 --> 00:02:43,599
directly from user space which thereby

00:02:39,200 --> 00:02:48,000
eliminates costly kernel content switch

00:02:43,599 --> 00:02:51,840
second is that spdk runs in a port mode

00:02:48,000 --> 00:02:55,920
instead of interrupt mode when

00:02:51,840 --> 00:03:00,239
spdk initializes on each call

00:02:55,920 --> 00:03:03,840
runs a slide is called a reactor

00:03:00,239 --> 00:03:07,920
and users can register different folders

00:03:03,840 --> 00:03:11,360
on this reactor some of these folders

00:03:07,920 --> 00:03:14,640
then pull the devices for completions

00:03:11,360 --> 00:03:15,040
instead of waiting for interrupts and

00:03:14,640 --> 00:03:18,239
this

00:03:15,040 --> 00:03:19,599
reduces the related interrupt handling

00:03:18,239 --> 00:03:23,120
overhead

00:03:19,599 --> 00:03:26,319
and the third one the shared nothing

00:03:23,120 --> 00:03:29,599
stride model on each

00:03:26,319 --> 00:03:33,280
slide operates on its own

00:03:29,599 --> 00:03:36,480
resources for and for threads

00:03:33,280 --> 00:03:39,519
communication spdk doesn't use

00:03:36,480 --> 00:03:42,799
a traditional lock mechanism but

00:03:39,519 --> 00:03:46,000
using event ring mechanism

00:03:42,799 --> 00:03:49,200
for a miami device a hardware queue

00:03:46,000 --> 00:03:52,879
is only ever assessed from one strike at

00:03:49,200 --> 00:03:56,720
a time spdk can send

00:03:52,879 --> 00:03:57,840
a request to the device from multiple

00:03:56,720 --> 00:04:01,439
threads

00:03:57,840 --> 00:04:03,760
in patterdale without logs

00:04:01,439 --> 00:04:05,200
it leads three key techniques let's

00:04:03,760 --> 00:04:08,159
bring spdks

00:04:05,200 --> 00:04:08,159
high performance

00:04:10,000 --> 00:04:17,040
and about sbdk we have contributed

00:04:13,599 --> 00:04:21,840
more than 50 pages to enable and

00:04:17,040 --> 00:04:26,240
optimize spdk on m64

00:04:21,840 --> 00:04:29,280
including the optimization is

00:04:26,240 --> 00:04:35,199
about a memory builder and

00:04:29,280 --> 00:04:37,759
best 64 and crc32 and the isa library

00:04:35,199 --> 00:04:38,639
and in the following slides we will walk

00:04:37,759 --> 00:04:42,400
through our

00:04:38,639 --> 00:04:43,360
practice about running spdk and rmeo

00:04:42,400 --> 00:04:48,160
fabrics

00:04:43,360 --> 00:04:51,280
and sbdk we host target on our servers

00:04:48,160 --> 00:04:54,479
and our implementation of sbdk csi

00:04:51,280 --> 00:04:57,840
to bring sbdk to

00:04:54,479 --> 00:04:57,840
container storage

00:05:01,759 --> 00:05:06,400
uh i first ever introduced the amio

00:05:04,880 --> 00:05:10,800
fabrics

00:05:06,400 --> 00:05:14,880
fme is an interface specification

00:05:10,800 --> 00:05:18,560
optimized for solid state storage

00:05:14,880 --> 00:05:21,600
for both client and enterprise storage

00:05:18,560 --> 00:05:24,800
systems utilizing their pci

00:05:21,600 --> 00:05:26,000
interface one of the main distinction

00:05:24,800 --> 00:05:29,680
between

00:05:26,000 --> 00:05:32,800
the me and amio fabrics

00:05:29,680 --> 00:05:35,680
is the transport mapping mechanism

00:05:32,800 --> 00:05:37,039
for sending and receiving commands or

00:05:35,680 --> 00:05:41,120
responses

00:05:37,039 --> 00:05:44,479
for local pcie for local access

00:05:41,120 --> 00:05:48,320
mme maps the commands and

00:05:44,479 --> 00:05:48,320
responses to the shared

00:05:48,960 --> 00:05:56,880
to the shared memory in the host

00:05:53,120 --> 00:05:59,919
over the pcie interface protocol

00:05:56,880 --> 00:06:01,840
while a memory over fabrics uses a

00:05:59,919 --> 00:06:04,960
message-based model

00:06:01,840 --> 00:06:05,600
for communication between a host and a

00:06:04,960 --> 00:06:09,919
target

00:06:05,600 --> 00:06:13,120
storage device and currently we have

00:06:09,919 --> 00:06:21,039
mme over rdma over tcp

00:06:13,120 --> 00:06:25,280
and over fiber channel

00:06:21,039 --> 00:06:28,319
for rdma transports rdma is a host

00:06:25,280 --> 00:06:30,240
of load host bypass technology that

00:06:28,319 --> 00:06:33,360
allows an application

00:06:30,240 --> 00:06:34,319
including the storage application to

00:06:33,360 --> 00:06:38,080
make date

00:06:34,319 --> 00:06:40,880
transferred directly to or from another

00:06:38,080 --> 00:06:41,600
application's memory space and it

00:06:40,880 --> 00:06:45,840
requires

00:06:41,600 --> 00:06:46,240
a spatial niche the rdma capable needs

00:06:45,840 --> 00:06:49,440
to

00:06:46,240 --> 00:06:50,000
manage reliable connections between the

00:06:49,440 --> 00:06:53,039
source

00:06:50,000 --> 00:06:57,360
and the destination and

00:06:53,039 --> 00:06:57,360
similar to the mme copiers

00:06:57,440 --> 00:07:05,280
applications communicate with the rdma

00:07:00,800 --> 00:07:09,199
nic using the dedicated qpr's

00:07:05,280 --> 00:07:12,880
including this uh sq pairs include

00:07:09,199 --> 00:07:16,479
the send q and the receive queue

00:07:12,880 --> 00:07:20,160
and also it requires the completion

00:07:16,479 --> 00:07:23,599
cues and for fme over rdma

00:07:20,160 --> 00:07:27,599
each only a me cube here a

00:07:23,599 --> 00:07:30,960
this is an eq here hm mmeq here is

00:07:27,599 --> 00:07:34,240
mapped to a rdma cube here

00:07:30,960 --> 00:07:38,319
and each set of the qps

00:07:34,240 --> 00:07:41,880
is aligned to a cpu car

00:07:38,319 --> 00:07:44,639
then mme commands are put in tulare

00:07:41,880 --> 00:07:48,400
rdmaqps and the things over

00:07:44,639 --> 00:07:48,400
the nick

00:07:52,240 --> 00:07:56,960
this chart is the comparison of the mme

00:07:55,360 --> 00:08:00,000
over rdma and

00:07:56,960 --> 00:08:03,199
local pcie on amp server we use

00:08:00,000 --> 00:08:07,120
a mme75 zero in

00:08:03,199 --> 00:08:10,479
target and uses a melanos nic

00:08:07,120 --> 00:08:11,120
the date is tested with a four kilobyte

00:08:10,479 --> 00:08:15,759
payload

00:08:11,120 --> 00:08:19,440
size at 128 kilo depth

00:08:15,759 --> 00:08:22,400
we can see there from this picture

00:08:19,440 --> 00:08:24,000
there is little uh change with the

00:08:22,400 --> 00:08:27,280
number of cpu car

00:08:24,000 --> 00:08:31,840
for in this platform one car can

00:08:27,280 --> 00:08:35,360
almost saturate this full mme ssd

00:08:31,840 --> 00:08:38,479
and except for the one car

00:08:35,360 --> 00:08:41,919
rent rate case in most cases

00:08:38,479 --> 00:08:47,360
the performance of a miami of rdma

00:08:41,919 --> 00:08:47,360
is close to the local pcie's performance

00:08:50,560 --> 00:08:56,880
and to use a memory over idma

00:08:54,320 --> 00:08:57,920
we said before spatial needs are

00:08:56,880 --> 00:09:01,920
required

00:08:57,920 --> 00:09:05,519
however me over tcp makes it possible

00:09:01,920 --> 00:09:09,200
to use memory fabrics across

00:09:05,519 --> 00:09:09,839
a standard ethernet network there is no

00:09:09,200 --> 00:09:12,959
need

00:09:09,839 --> 00:09:16,240
to make configuration changes

00:09:12,959 --> 00:09:19,440
or implement any spatial equipment

00:09:16,240 --> 00:09:22,720
when using a tcp transport each

00:09:19,440 --> 00:09:26,560
mmeq peers is mapped to

00:09:22,720 --> 00:09:29,760
a tcp connection and mme commands

00:09:26,560 --> 00:09:36,320
are in capsules and then sent

00:09:29,760 --> 00:09:39,360
over the standard tcpip sockets

00:09:36,320 --> 00:09:42,560
uh currently in spdk

00:09:39,360 --> 00:09:46,000
uh mme over tcp there are four types

00:09:42,560 --> 00:09:49,519
of circuit implementation

00:09:46,000 --> 00:09:53,120
the positive socket urine socket

00:09:49,519 --> 00:09:53,680
and vpp and c stars okay the positive

00:09:53,120 --> 00:09:57,040
circuit

00:09:53,680 --> 00:09:57,920
is the most stable one uh and it has no

00:09:57,040 --> 00:10:02,560
dependency

00:09:57,920 --> 00:10:06,000
on the kernel urine sockets

00:10:02,560 --> 00:10:07,760
leverages learn new linux asynchronous

00:10:06,000 --> 00:10:12,240
io interface

00:10:07,760 --> 00:10:15,360
called iou ring it's built around

00:10:12,240 --> 00:10:18,959
a ring buffer in memory shared

00:10:15,360 --> 00:10:22,320
between user space and the kernel

00:10:18,959 --> 00:10:25,920
and it allows the submission

00:10:22,320 --> 00:10:28,800
of operations and collecting the results

00:10:25,920 --> 00:10:29,839
without the need to call into the kernel

00:10:28,800 --> 00:10:33,600
space

00:10:29,839 --> 00:10:35,920
in many cases

00:10:33,600 --> 00:10:37,519
but it requires the kernel version newer

00:10:35,920 --> 00:10:41,519
than 5.4

00:10:37,519 --> 00:10:44,800
and 0.3 um bpp circuit vpp

00:10:41,519 --> 00:10:45,519
is the vector package processing it's a

00:10:44,800 --> 00:10:48,880
fast

00:10:45,519 --> 00:10:52,640
network that playing designed on

00:10:48,880 --> 00:10:56,240
top of dpdk

00:10:52,640 --> 00:11:00,880
to run the network workflows using

00:10:56,240 --> 00:11:04,640
vector package processing technology

00:11:00,880 --> 00:11:05,920
but the integration test of this pvp

00:11:04,640 --> 00:11:10,320
will be stopped in

00:11:05,920 --> 00:11:14,399
spdk 20.07

00:11:10,320 --> 00:11:17,440
due to the performance limitation

00:11:14,399 --> 00:11:20,800
and sistar sister is an event

00:11:17,440 --> 00:11:24,399
driving framework allowing you to

00:11:20,800 --> 00:11:27,839
write non-broken synchronous codes

00:11:24,399 --> 00:11:28,640
in a relatively straightforward manner

00:11:27,839 --> 00:11:30,959
some of

00:11:28,640 --> 00:11:32,480
the work of sister showcase has been

00:11:30,959 --> 00:11:37,360
done by still

00:11:32,480 --> 00:11:39,839
needs a further investigation

00:11:37,360 --> 00:11:40,560
currently we have tested the urine

00:11:39,839 --> 00:11:44,959
sockets

00:11:40,560 --> 00:11:44,959
and their positive circuits

00:11:47,200 --> 00:11:55,120
this data is from static tested from

00:11:51,680 --> 00:11:58,800
another type of mme ssd key

00:11:55,120 --> 00:12:01,839
forces double zero in the targets

00:11:58,800 --> 00:12:03,200
it's also tested with the four kilobyte

00:12:01,839 --> 00:12:06,639
payload size

00:12:03,200 --> 00:12:09,120
and 128 qth

00:12:06,639 --> 00:12:11,680
this is the comparison of the positive

00:12:09,120 --> 00:12:15,519
sockets and during sockets

00:12:11,680 --> 00:12:18,800
generally with the number of cpu cards

00:12:15,519 --> 00:12:22,000
the bandwidth is getting better and at

00:12:18,800 --> 00:12:25,839
about eight core it can reach the

00:12:22,000 --> 00:12:25,839
miami's full performance

00:12:26,959 --> 00:12:31,920
from comparison we can see that the

00:12:29,680 --> 00:12:35,040
performance of urine circuit

00:12:31,920 --> 00:12:36,839
is not obviously better than positive

00:12:35,040 --> 00:12:40,720
sockets

00:12:36,839 --> 00:12:44,240
for kernel io ring has three memos

00:12:40,720 --> 00:12:44,720
one is the interrupt modes and the other

00:12:44,240 --> 00:12:48,160
two

00:12:44,720 --> 00:12:48,880
is a pro mode uh from my viewer it's the

00:12:48,160 --> 00:12:52,079
pro mode

00:12:48,880 --> 00:12:56,480
light should have better performance

00:12:52,079 --> 00:12:59,920
but now spdk only supports urine's

00:12:56,480 --> 00:13:02,959
interrupt mode due to some limitation

00:12:59,920 --> 00:13:07,839
and the community is still doing

00:13:02,959 --> 00:13:07,839
optimization on this

00:13:10,639 --> 00:13:17,279
the above introduced is all about

00:13:13,760 --> 00:13:20,480
the sbd case of wemo fabrics

00:13:17,279 --> 00:13:24,079
is used to provide storage

00:13:20,480 --> 00:13:27,440
for remote clouds and spdk also

00:13:24,079 --> 00:13:31,040
can be used to provide bacon storage

00:13:27,440 --> 00:13:32,320
for virtual machines as pdk v host

00:13:31,040 --> 00:13:35,519
target

00:13:32,320 --> 00:13:38,720
is designed for this purpose

00:13:35,519 --> 00:13:39,120
before i introduce the rehost target i

00:13:38,720 --> 00:13:42,800
will

00:13:39,120 --> 00:13:46,160
explain some related concepts uh one is

00:13:42,800 --> 00:13:49,199
what i o as i write here

00:13:46,160 --> 00:13:50,720
versailles is the io virtualization

00:13:49,199 --> 00:13:54,320
specification

00:13:50,720 --> 00:13:57,920
it's a structure layer above a set

00:13:54,320 --> 00:14:02,079
of common emulated devices

00:13:57,920 --> 00:14:04,800
in a virtualized hypervisor

00:14:02,079 --> 00:14:04,800
like qmu

00:14:05,120 --> 00:14:09,040
and it provides a common mechanism and

00:14:08,079 --> 00:14:13,120
layouts

00:14:09,040 --> 00:14:15,360
for device discovery and configuration

00:14:13,120 --> 00:14:17,199
also provides common mechanisms for

00:14:15,360 --> 00:14:22,000
front ends driver

00:14:17,199 --> 00:14:22,000
and back-end driver to communicate

00:14:22,079 --> 00:14:29,440
and what i o of those parts of

00:14:25,600 --> 00:14:32,560
operations to host is called way host

00:14:29,440 --> 00:14:36,399
includes the v-host kernel and

00:14:32,560 --> 00:14:39,839
v-host user for the difference

00:14:36,399 --> 00:14:43,120
is for cumulative cumulative

00:14:39,839 --> 00:14:45,680
deals with dates with guest

00:14:43,120 --> 00:14:46,240
by four warehouse kernel it's the v host

00:14:45,680 --> 00:14:49,199
module

00:14:46,240 --> 00:14:51,440
in the kernel space transfer date uh

00:14:49,199 --> 00:14:54,160
visual guest

00:14:51,440 --> 00:14:55,360
and for warehouse user it's the v-host

00:14:54,160 --> 00:14:59,279
backhand

00:14:55,360 --> 00:15:02,959
in the user space let's transfer that

00:14:59,279 --> 00:15:03,360
with the gaster so we host user protocol

00:15:02,959 --> 00:15:07,760
makes

00:15:03,360 --> 00:15:10,800
it possible for guest to transfer

00:15:07,760 --> 00:15:15,600
to transfer io dates with a user

00:15:10,800 --> 00:15:15,600
space process like sbdk

00:15:15,760 --> 00:15:22,079
and we host user protocol defines two

00:15:18,560 --> 00:15:24,399
sides of the communication

00:15:22,079 --> 00:15:25,360
one side is master and the other is

00:15:24,399 --> 00:15:28,800
slave

00:15:25,360 --> 00:15:32,800
master is the application let's share

00:15:28,800 --> 00:15:35,519
is vote queue in our case is qmu

00:15:32,800 --> 00:15:36,720
and snap is the consumer of the watch

00:15:35,519 --> 00:15:39,440
queue

00:15:36,720 --> 00:15:42,079
in our case it's learned as pdk we host

00:15:39,440 --> 00:15:42,079
the targets

00:15:42,320 --> 00:15:45,759
in this picture we can see that the

00:15:44,720 --> 00:15:50,720
virtual machines

00:15:45,759 --> 00:15:51,759
shares add a huge page memory with spdk

00:15:50,720 --> 00:15:54,959
host

00:15:51,759 --> 00:15:58,639
and sbdkv host

00:15:54,959 --> 00:16:02,160
transfer that with the virtual machine

00:15:58,639 --> 00:16:03,120
through the vert queues in the shared

00:16:02,160 --> 00:16:07,120
memory

00:16:03,120 --> 00:16:09,839
and full control path unix domain socket

00:16:07,120 --> 00:16:13,600
is used to transfer control message

00:16:09,839 --> 00:16:18,320
between these two processes

00:16:13,600 --> 00:16:22,000
now as bdk vhost supports

00:16:18,320 --> 00:16:25,199
we host scarcity we host blog

00:16:22,000 --> 00:16:28,240
and they host away me

00:16:25,199 --> 00:16:29,519
and the away me the host of me is your

00:16:28,240 --> 00:16:33,199
experimental

00:16:29,519 --> 00:16:37,440
you can refer to spdk

00:16:33,199 --> 00:16:40,160
documents or the source code for details

00:16:37,440 --> 00:16:43,120
and this is the general introduction of

00:16:40,160 --> 00:16:47,519
how spdk provides

00:16:43,120 --> 00:16:51,600
storage for virtual machines

00:16:47,519 --> 00:16:57,120
and to bring sbdk to container storage

00:16:51,600 --> 00:16:57,120
we designed sbdk csi plugged in

00:17:00,000 --> 00:17:06,799
csi are the container storage

00:17:03,199 --> 00:17:09,919
interface it's a standard

00:17:06,799 --> 00:17:10,559
for exposing arbitrary block and file

00:17:09,919 --> 00:17:14,319
storage

00:17:10,559 --> 00:17:17,679
systems to containerized workloads

00:17:14,319 --> 00:17:21,039
on the container orchestration systems

00:17:17,679 --> 00:17:26,480
like kubernetes and it's a general

00:17:21,039 --> 00:17:26,480
protocol not for kubernetes only

00:17:26,559 --> 00:17:34,720
csi con consists of a

00:17:30,320 --> 00:17:38,559
controller driver and another drivers

00:17:34,720 --> 00:17:39,600
the controller driver is responsible

00:17:38,559 --> 00:17:42,960
talking

00:17:39,600 --> 00:17:46,400
to the service provider

00:17:42,960 --> 00:17:50,640
to create or delete the volumes

00:17:46,400 --> 00:17:54,559
and the node driver is responsible for

00:17:50,640 --> 00:17:58,320
mounting or amounting remote volumes

00:17:54,559 --> 00:18:01,679
to the host to localhost

00:17:58,320 --> 00:18:04,720
for example we can deploy a

00:18:01,679 --> 00:18:08,160
controller a driver on

00:18:04,720 --> 00:18:10,320
a container's master nodes container

00:18:08,160 --> 00:18:13,360
orchestrations master node

00:18:10,320 --> 00:18:16,799
here and

00:18:13,360 --> 00:18:18,160
a note driver on each of the worker

00:18:16,799 --> 00:18:22,000
nodes

00:18:18,160 --> 00:18:24,720
um fuller communication between the

00:18:22,000 --> 00:18:25,760
container orchestration and the csi

00:18:24,720 --> 00:18:30,480
drivers

00:18:25,760 --> 00:18:33,919
uh we use the rtc messages

00:18:30,480 --> 00:18:33,919
for this communication

00:18:37,520 --> 00:18:44,559
and for kubernetes kubernetes

00:18:40,799 --> 00:18:47,919
has supported the csi spec

00:18:44,559 --> 00:18:52,240
1. 1.0 from

00:18:47,919 --> 00:18:55,679
release 1.13 uh in this link

00:18:52,240 --> 00:18:56,000
there are a set of csi driver which can

00:18:55,679 --> 00:19:00,240
be

00:18:56,000 --> 00:19:04,000
used with kubernetes

00:19:00,240 --> 00:19:08,960
our new csi driver spdk csi

00:19:04,000 --> 00:19:12,400
aims to bring sbdk to kubernetes storage

00:19:08,960 --> 00:19:13,840
through a vimeo fabrics or the iscsi

00:19:12,400 --> 00:19:17,200
protocol

00:19:13,840 --> 00:19:20,640
and it should support dynamic volume

00:19:17,200 --> 00:19:24,440
provisioning and enables

00:19:20,640 --> 00:19:27,679
posts to use sbdk storage

00:19:24,440 --> 00:19:30,640
transparently the csi driver

00:19:27,679 --> 00:19:31,919
will be an executable wrapped inside a

00:19:30,640 --> 00:19:35,520
container

00:19:31,919 --> 00:19:39,760
to be deployed in kubernetes cluster

00:19:35,520 --> 00:19:43,520
and implement spdk storage class

00:19:39,760 --> 00:19:53,840
and this project is initiated by um

00:19:43,520 --> 00:19:53,840
it has been released in spdk 20.07

00:19:54,640 --> 00:20:03,520
this picture is the spdkc size overview

00:19:59,840 --> 00:20:06,799
or it may contains three parts

00:20:03,520 --> 00:20:09,840
on this part is the

00:20:06,799 --> 00:20:13,600
s kubernetes is a kubernetes cluster

00:20:09,840 --> 00:20:17,280
with one master it's here one master

00:20:13,600 --> 00:20:17,600
and the two volcanoes these work nose

00:20:17,280 --> 00:20:22,240
and

00:20:17,600 --> 00:20:25,440
list work node this work node

00:20:22,240 --> 00:20:30,000
runs a csi node driver

00:20:25,440 --> 00:20:33,840
and work part running the engines

00:20:30,000 --> 00:20:35,280
and this work node runs a csi controller

00:20:33,840 --> 00:20:38,559
driver

00:20:35,280 --> 00:20:41,760
and the csi rpc is synced

00:20:38,559 --> 00:20:49,440
from list master to

00:20:41,760 --> 00:20:52,880
list to a drive csi driver

00:20:49,440 --> 00:20:54,400
at least this part this part is a

00:20:52,880 --> 00:20:57,760
kubernetes cluster

00:20:54,400 --> 00:21:00,960
running some spdk nodes

00:20:57,760 --> 00:21:04,880
with umi devices and

00:21:00,960 --> 00:21:04,880
spdk software stake

00:21:06,799 --> 00:21:13,039
and this part is networking

00:21:09,919 --> 00:21:17,360
between the kubernetes cluster and

00:21:13,039 --> 00:21:21,440
sbdk storage cluster the csi

00:21:17,360 --> 00:21:25,919
uh controller driver this this driver

00:21:21,440 --> 00:21:29,520
talks to the spdks json rpc service

00:21:25,919 --> 00:21:33,039
through the controller network and

00:21:29,520 --> 00:21:33,760
the csi know the driver here the node

00:21:33,039 --> 00:21:39,679
driver

00:21:33,760 --> 00:21:42,159
connects to the sbdk mimi over target

00:21:39,679 --> 00:21:43,280
i mean all february targets through the

00:21:42,159 --> 00:21:46,799
high speed

00:21:43,280 --> 00:21:49,440
data network like rdma or

00:21:46,799 --> 00:21:49,440
tcp

00:21:49,919 --> 00:21:56,559
and the status of our sbd

00:21:52,960 --> 00:21:59,840
sbdk csi project is that

00:21:56,559 --> 00:22:03,280
mandatory csi functionalities

00:21:59,840 --> 00:22:06,159
are ready now and there are still lots

00:22:03,280 --> 00:22:06,159
of work to do

00:22:08,559 --> 00:22:17,120
so what's next sorry

00:22:12,880 --> 00:22:20,080
of either during

00:22:17,120 --> 00:22:20,559
the following times we were for maybe

00:22:20,080 --> 00:22:24,080
over

00:22:20,559 --> 00:22:26,799
february we were

00:22:24,080 --> 00:22:29,440
continuing to integrate and optimize

00:22:26,799 --> 00:22:32,960
spdk lme otcp

00:22:29,440 --> 00:22:36,559
with a mtcp us learner

00:22:32,960 --> 00:22:40,400
user space tcp stack

00:22:36,559 --> 00:22:42,000
and we will try to optimize spdk mmu

00:22:40,400 --> 00:22:46,400
otsp with their

00:22:42,000 --> 00:22:50,080
urine socket for spdk csi

00:22:46,400 --> 00:22:53,600
plugin will continue tests

00:22:50,080 --> 00:22:56,880
and improvements for production level

00:22:53,600 --> 00:23:00,240
quality and also we will introduce some

00:22:56,880 --> 00:23:04,240
new features like topology

00:23:00,240 --> 00:23:07,760
volume expansion snapshots and so on

00:23:04,240 --> 00:23:11,440
you can uh refer to this

00:23:07,760 --> 00:23:14,799
trello board uh for the details

00:23:11,440 --> 00:23:15,520
and we will also uh integrate advanced

00:23:14,799 --> 00:23:18,960
look

00:23:15,520 --> 00:23:23,840
to build a total solution of

00:23:18,960 --> 00:23:23,840
leveraging spdk in kubernetes

00:23:24,960 --> 00:23:31,440
and we welcome your contribution to

00:23:28,240 --> 00:23:34,799
sbdk csi project

00:23:31,440 --> 00:23:37,600
you can refer to the foreign links

00:23:34,799 --> 00:23:38,799
the following links to get more

00:23:37,600 --> 00:23:41,919
information

00:23:38,799 --> 00:23:41,919
and guidance

00:23:46,240 --> 00:23:53,039
let's all let i share with you

00:23:49,600 --> 00:23:56,159
if you have any questions please send me

00:23:53,039 --> 00:23:57,360
an email and i will respond as soon as

00:23:56,159 --> 00:24:01,039
possible

00:23:57,360 --> 00:24:01,039

YouTube URL: https://www.youtube.com/watch?v=WktmJdvFNmA


