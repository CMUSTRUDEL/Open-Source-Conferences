Title: LVC20 208 Arm NN   New features in 19 11 to 20 05 release
Publication date: 2020-10-08
Playlist: Linaro Virtual Connect 2020
Description: 
	Arm NN is an accelerated inference engine for Arm CPUs, GPUs, and NPUs.
It executes ML algorithms on-device in order to make predictions based on input data.
Arm NN enables efficient translation of existing neural network frameworks, such as TensorFlow Lite,
TensorFlow, ONNX, and Caffe, allowing them to run efficiently and without modification across Arm Cortex-A CPUs,
Mali GPUs, and Ethos-N NPUs. This presentation will provide details of the new features that have been added to Arm NN from the 19.11 to 20.05 release.

https://connect.linaro.org/resources/lvc20/lvc20-208/
Captions: 
	00:00:00,880 --> 00:00:04,799
so yes good morning or good evening

00:00:03,199 --> 00:00:05,279
depending on where you're watching this

00:00:04,799 --> 00:00:08,559
from

00:00:05,279 --> 00:00:09,760
uh my name's kevin may and i'm speaking

00:00:08,559 --> 00:00:12,080
to you live from

00:00:09,760 --> 00:00:13,840
galway city on the west coast of ireland

00:00:12,080 --> 00:00:16,800
where i have been

00:00:13,840 --> 00:00:17,119
a software engineer on the rmn team

00:00:16,800 --> 00:00:20,720
since

00:00:17,119 --> 00:00:24,400
uh 2018.

00:00:20,720 --> 00:00:26,880
so um

00:00:24,400 --> 00:00:28,400
quick introduction uh here's a quick

00:00:26,880 --> 00:00:29,679
rundown of what we'll be talking about

00:00:28,400 --> 00:00:32,559
i'm sure there's many

00:00:29,679 --> 00:00:33,600
of you who are familiar already with

00:00:32,559 --> 00:00:35,760
armanin maybe

00:00:33,600 --> 00:00:37,600
even you've made a pull request but

00:00:35,760 --> 00:00:39,520
equally there may be some of you

00:00:37,600 --> 00:00:40,960
who have little knowledge uh of what

00:00:39,520 --> 00:00:42,480
armanin is so

00:00:40,960 --> 00:00:44,559
i'm not going to make any assumptions

00:00:42,480 --> 00:00:47,840
and we'll start with an introduction

00:00:44,559 --> 00:00:48,879
to armanin what it is why you would want

00:00:47,840 --> 00:00:54,239
to use armanin

00:00:48,879 --> 00:00:56,960
and then a nice real-world rnn use case

00:00:54,239 --> 00:00:58,480
and then we'll get a look in detail at

00:00:56,960 --> 00:01:01,199
some of the new features

00:00:58,480 --> 00:01:01,760
introduced in the past few releases so

00:01:01,199 --> 00:01:05,040
pi r m

00:01:01,760 --> 00:01:05,439
n which is our new python api and we'll

00:01:05,040 --> 00:01:07,760
have

00:01:05,439 --> 00:01:10,080
a look at two new back-end features for

00:01:07,760 --> 00:01:12,640
power users of armanin

00:01:10,080 --> 00:01:14,479
and we'll have a look at some of the new

00:01:12,640 --> 00:01:16,880
divs type support we've added

00:01:14,479 --> 00:01:18,640
um and look at the in and driver

00:01:16,880 --> 00:01:21,520
operator support that we've added

00:01:18,640 --> 00:01:22,400
and if we have time then i'll uh give

00:01:21,520 --> 00:01:27,119
some information

00:01:22,400 --> 00:01:30,640
on how you can also contribute to rm m

00:01:27,119 --> 00:01:33,920
so what is r n n

00:01:30,640 --> 00:01:35,360
um it's an inference engine for edge

00:01:33,920 --> 00:01:36,400
machine learning so that's the first

00:01:35,360 --> 00:01:38,240
thing it's uh

00:01:36,400 --> 00:01:40,560
for inference only it doesn't concern

00:01:38,240 --> 00:01:43,680
itself for training

00:01:40,560 --> 00:01:45,680
and it bridges the gap between

00:01:43,680 --> 00:01:46,880
existing popular neural network

00:01:45,680 --> 00:01:50,240
frameworks so

00:01:46,880 --> 00:01:54,000
tflight onyx and

00:01:50,240 --> 00:01:56,640
high torch via onyx and cafe

00:01:54,000 --> 00:01:58,960
and a bridges gap between those and

00:01:56,640 --> 00:02:02,560
underlying ips so for example

00:01:58,960 --> 00:02:05,920
cortex ac to use our cpus

00:02:02,560 --> 00:02:08,959
and uh the mali gpus

00:02:05,920 --> 00:02:13,200
and also arms new ethos in

00:02:08,959 --> 00:02:14,319
npus in addition it uh provides in an

00:02:13,200 --> 00:02:17,520
api support so

00:02:14,319 --> 00:02:20,000
rmn interfaces with google's android

00:02:17,520 --> 00:02:20,640
and neural network api using the hal

00:02:20,000 --> 00:02:24,160
driver

00:02:20,640 --> 00:02:25,360
to target rmip and it's through the

00:02:24,160 --> 00:02:29,200
sufficient targeting

00:02:25,360 --> 00:02:31,680
of mip using arm compute library

00:02:29,200 --> 00:02:32,560
and the ethers in drivers track that

00:02:31,680 --> 00:02:35,120
armanine

00:02:32,560 --> 00:02:38,239
can deliver some real performance

00:02:35,120 --> 00:02:40,640
improvements which we'll see

00:02:38,239 --> 00:02:42,319
so uh here's a quick overview which i

00:02:40,640 --> 00:02:42,959
think is useful to give us an idea of

00:02:42,319 --> 00:02:46,319
where it

00:02:42,959 --> 00:02:47,280
uh sits in relation to high level neural

00:02:46,319 --> 00:02:50,319
network

00:02:47,280 --> 00:02:53,840
libraries and android applications so

00:02:50,319 --> 00:02:56,400
the blue components here are uh rmn

00:02:53,840 --> 00:02:57,040
which consists of the hal driver that

00:02:56,400 --> 00:03:01,040
provides

00:02:57,040 --> 00:03:04,879
access to rmn for android applications

00:03:01,040 --> 00:03:08,480
um and then the rmn sdk itself

00:03:04,879 --> 00:03:10,720
which provides the back ends here um

00:03:08,480 --> 00:03:12,800
for the lower level libraries and

00:03:10,720 --> 00:03:15,040
hardware drivers

00:03:12,800 --> 00:03:17,360
so the back ends that are by default

00:03:15,040 --> 00:03:18,000
added would be the neon back end and the

00:03:17,360 --> 00:03:21,360
open

00:03:18,000 --> 00:03:22,800
cl back end these interface with the arm

00:03:21,360 --> 00:03:25,120
compute library

00:03:22,800 --> 00:03:28,879
which provides access to the cortex-a

00:03:25,120 --> 00:03:28,879
cpus and the mali gpus

00:03:29,120 --> 00:03:34,000
and but we've also available to d uh to

00:03:32,480 --> 00:03:37,200
download in addition is an

00:03:34,000 --> 00:03:40,400
npu backend and that would be

00:03:37,200 --> 00:03:43,599
for the access to the ethos in

00:03:40,400 --> 00:03:47,280
processor um

00:03:43,599 --> 00:03:49,200
and of course if you are a third party

00:03:47,280 --> 00:03:52,239
partner you can also add your own

00:03:49,200 --> 00:03:53,120
uh back end for arm name via our

00:03:52,239 --> 00:03:56,400
plugable

00:03:53,120 --> 00:04:01,120
backend mechanism

00:03:56,400 --> 00:04:03,120
so i have added this slide because

00:04:01,120 --> 00:04:04,480
this shows the inner components of rmn

00:04:03,120 --> 00:04:06,560
and this will be useful

00:04:04,480 --> 00:04:08,879
when we talk about some of the new

00:04:06,560 --> 00:04:11,280
features just to put them into context

00:04:08,879 --> 00:04:12,560
so i'm using blue again to designate

00:04:11,280 --> 00:04:14,879
armanin components

00:04:12,560 --> 00:04:16,639
and we'll start here these are the three

00:04:14,879 --> 00:04:17,359
main flaws of when you'd be using arm

00:04:16,639 --> 00:04:20,720
and n

00:04:17,359 --> 00:04:23,360
if you look at the direct api here um

00:04:20,720 --> 00:04:24,400
this is how you would use rnn if you

00:04:23,360 --> 00:04:26,479
were to

00:04:24,400 --> 00:04:30,560
build up your neural network layer by

00:04:26,479 --> 00:04:33,680
layer yourself using our direct api

00:04:30,560 --> 00:04:36,560
but probably more likely uh is how you

00:04:33,680 --> 00:04:38,720
you would use our parsers uh in this

00:04:36,560 --> 00:04:41,600
black box here so we say the t of light

00:04:38,720 --> 00:04:44,960
parser the cafe parser the onyx parser

00:04:41,600 --> 00:04:46,400
or indeed the hull driver and

00:04:44,960 --> 00:04:48,240
in this case for example if you were to

00:04:46,400 --> 00:04:50,240
use the tia flight

00:04:48,240 --> 00:04:52,400
you would pass your tier flight model

00:04:50,240 --> 00:04:56,160
into the tf flight parser

00:04:52,400 --> 00:04:57,040
and the parser would construct a a

00:04:56,160 --> 00:05:00,240
network

00:04:57,040 --> 00:05:02,560
um for use within armanin and

00:05:00,240 --> 00:05:03,840
and take all the hard work out of it for

00:05:02,560 --> 00:05:07,680
you so you don't have to

00:05:03,840 --> 00:05:11,680
put it together yourself um once it's

00:05:07,680 --> 00:05:13,680
imported into the graph builder

00:05:11,680 --> 00:05:15,600
it's built up network and then it's

00:05:13,680 --> 00:05:19,280
optimized which is to say

00:05:15,600 --> 00:05:21,840
that the optimizer

00:05:19,280 --> 00:05:22,639
does some validation on the graph

00:05:21,840 --> 00:05:24,240
because

00:05:22,639 --> 00:05:25,759
i guess mistakes can be introduced

00:05:24,240 --> 00:05:29,199
sometimes perhaps there's

00:05:25,759 --> 00:05:31,039
redundancy like multiple transpose or

00:05:29,199 --> 00:05:33,520
reshape operations

00:05:31,039 --> 00:05:34,240
so these are all tidied up by our

00:05:33,520 --> 00:05:36,080
optimizer

00:05:34,240 --> 00:05:38,320
and that would be these types of graph

00:05:36,080 --> 00:05:41,919
optimizations here

00:05:38,320 --> 00:05:44,800
but it also modify the network for

00:05:41,919 --> 00:05:46,720
correctness for example perhaps you want

00:05:44,800 --> 00:05:47,520
to run it on a back end that would

00:05:46,720 --> 00:05:51,440
prefer

00:05:47,520 --> 00:05:54,800
to receive it in nchw format

00:05:51,440 --> 00:05:57,199
but you've given a model of nhwc and

00:05:54,800 --> 00:05:59,120
so the optimizer would add a permute

00:05:57,199 --> 00:06:02,400
layer and

00:05:59,120 --> 00:06:06,400
target the correct um

00:06:02,400 --> 00:06:08,960
back-end format

00:06:06,400 --> 00:06:09,919
and it also uh optimizes them for

00:06:08,960 --> 00:06:12,319
performance too

00:06:09,919 --> 00:06:13,120
so we'll see examples that later the

00:06:12,319 --> 00:06:16,720
scheduler

00:06:13,120 --> 00:06:19,600
is um when the optimizer schedules it

00:06:16,720 --> 00:06:22,080
assigns the layers in the network

00:06:19,600 --> 00:06:23,440
to the best available backend to run

00:06:22,080 --> 00:06:25,199
that layer on

00:06:23,440 --> 00:06:28,160
and here you can see our back-ends

00:06:25,199 --> 00:06:31,120
interfacing with the compute library

00:06:28,160 --> 00:06:32,560
and in the runtime so the runtime gets

00:06:31,120 --> 00:06:35,360
the optimize

00:06:32,560 --> 00:06:36,960
network and it executes it on a given

00:06:35,360 --> 00:06:40,080
backend

00:06:36,960 --> 00:06:43,039
right so that is

00:06:40,080 --> 00:06:44,160
what is armanin uh this is more why so

00:06:43,039 --> 00:06:46,720
i've already mentioned

00:06:44,160 --> 00:06:48,960
armanin's easy parsing of different

00:06:46,720 --> 00:06:51,199
frameworks which is a great feature

00:06:48,960 --> 00:06:53,520
and and that gives you access to the

00:06:51,199 --> 00:06:56,560
compute library to target cores such as

00:06:53,520 --> 00:06:59,599
cortex a as efficiently as possible

00:06:56,560 --> 00:07:01,919
and this slide shows

00:06:59,599 --> 00:07:03,919
these performance improvements and why

00:07:01,919 --> 00:07:07,280
they're a differentiator

00:07:03,919 --> 00:07:08,320
for armanin so on the left you'll see a

00:07:07,280 --> 00:07:09,599
number of different

00:07:08,320 --> 00:07:11,759
[Music]

00:07:09,599 --> 00:07:14,400
image recognition models such as mobile

00:07:11,759 --> 00:07:16,080
net inception etc

00:07:14,400 --> 00:07:17,840
and the one i want you to look at is

00:07:16,080 --> 00:07:22,240
actually the mobile net here uh

00:07:17,840 --> 00:07:25,840
for the cortex a you'll see a 4.2 times

00:07:22,240 --> 00:07:27,840
faster improvement of rmnn relative

00:07:25,840 --> 00:07:29,680
to up to six different industries

00:07:27,840 --> 00:07:31,440
software libraries

00:07:29,680 --> 00:07:33,759
and also the main here you'll see for

00:07:31,440 --> 00:07:34,560
both the cortex a and mali gpu you're

00:07:33,759 --> 00:07:36,479
looking at

00:07:34,560 --> 00:07:37,599
three times faster over three times

00:07:36,479 --> 00:07:40,720
faster improvement

00:07:37,599 --> 00:07:42,000
uh relative to six industry software

00:07:40,720 --> 00:07:45,599
libraries so

00:07:42,000 --> 00:07:48,720
uh armanin shows what's possible uh for

00:07:45,599 --> 00:07:51,840
inference uh inferior networks up to my

00:07:48,720 --> 00:07:55,199
station's honor mip

00:07:51,840 --> 00:07:57,599
and next

00:07:55,199 --> 00:07:58,879
is a real world use case and i'm really

00:07:57,599 --> 00:08:01,599
happy that i can

00:07:58,879 --> 00:08:02,160
uh got permission to show this because

00:08:01,599 --> 00:08:03,840
uh

00:08:02,160 --> 00:08:06,160
as a developer and airman it's always

00:08:03,840 --> 00:08:09,680
lovely to uh

00:08:06,160 --> 00:08:12,960
see it actually being used and the end

00:08:09,680 --> 00:08:16,240
products so actors no networks

00:08:12,960 --> 00:08:17,039
uh um transform cameras from being

00:08:16,240 --> 00:08:19,759
passive

00:08:17,039 --> 00:08:22,080
observers into active detection systems

00:08:19,759 --> 00:08:24,240
so their use cases would be for safety

00:08:22,080 --> 00:08:25,360
and security surveillance devices that

00:08:24,240 --> 00:08:28,560
kind of thing

00:08:25,360 --> 00:08:30,479
um so motion or intrusion

00:08:28,560 --> 00:08:32,240
boundary crossing we'll say in a train

00:08:30,479 --> 00:08:33,120
station so you're not too close to the

00:08:32,240 --> 00:08:35,360
tracks

00:08:33,120 --> 00:08:38,240
royce rain abandoned package face

00:08:35,360 --> 00:08:41,279
verification that kind of thing

00:08:38,240 --> 00:08:43,519
so they're the arcturus uh

00:08:41,279 --> 00:08:45,040
inference runtimes they're optimized for

00:08:43,519 --> 00:08:48,880
edge processing

00:08:45,040 --> 00:08:51,600
using armanin and uh tensorflow light

00:08:48,880 --> 00:08:54,880
um the types of models they use include

00:08:51,600 --> 00:08:58,399
yolo v3 and inception v4

00:08:54,880 --> 00:09:00,720
and l leveraging the arm v8 neon engine

00:08:58,399 --> 00:09:01,839
and utilizing the armanin inference

00:09:00,720 --> 00:09:03,760
network

00:09:01,839 --> 00:09:05,360
to accelerate performance of a neural

00:09:03,760 --> 00:09:08,800
network this allows

00:09:05,360 --> 00:09:10,240
archers to run detection on cortex a

00:09:08,800 --> 00:09:14,399
cores without the need

00:09:10,240 --> 00:09:16,959
for specialized ml hardware or a gpu

00:09:14,399 --> 00:09:17,600
and so for public safety system like

00:09:16,959 --> 00:09:20,080
this one

00:09:17,600 --> 00:09:22,640
edge processing improves the response

00:09:20,080 --> 00:09:25,360
time and the reliability by

00:09:22,640 --> 00:09:27,120
eliminating the need to ship pixel data

00:09:25,360 --> 00:09:30,399
from each camera across the network

00:09:27,120 --> 00:09:33,839
and up to the cloud and rmnn uh helps

00:09:30,399 --> 00:09:36,240
archer's networks realize that

00:09:33,839 --> 00:09:38,959
okay so let's look at some of the new

00:09:36,240 --> 00:09:42,240
features that we have added

00:09:38,959 --> 00:09:45,279
pi arm and and so

00:09:42,240 --> 00:09:46,080
pi r m n is a python extension for rmna

00:09:45,279 --> 00:09:49,680
and sdk

00:09:46,080 --> 00:09:50,080
um many people use python through neural

00:09:49,680 --> 00:09:52,560
network

00:09:50,080 --> 00:09:54,320
frameworks like pytarch and tensorflow

00:09:52,560 --> 00:09:58,320
and so i think offering

00:09:54,320 --> 00:09:59,440
a python api to rmn is a really nice

00:09:58,320 --> 00:10:02,320
feature

00:09:59,440 --> 00:10:02,800
um it's essentially a wrapper around the

00:10:02,320 --> 00:10:05,040
c

00:10:02,800 --> 00:10:06,959
plus interface and if you're familiar

00:10:05,040 --> 00:10:08,880
with our c plus plus api it's very

00:10:06,959 --> 00:10:11,279
similar to use

00:10:08,880 --> 00:10:13,040
so as such it doesn't actually implement

00:10:11,279 --> 00:10:15,279
any computational kernels

00:10:13,040 --> 00:10:16,959
uh just helper functions and it's built

00:10:15,279 --> 00:10:20,079
around the public headers

00:10:16,959 --> 00:10:21,680
of ironman in so all the operations are

00:10:20,079 --> 00:10:24,880
delegated to the rmnn

00:10:21,680 --> 00:10:26,800
library and pi r m n

00:10:24,880 --> 00:10:28,160
is integrated into armament's c make

00:10:26,800 --> 00:10:30,640
build system

00:10:28,160 --> 00:10:31,519
so you can build it that way or

00:10:30,640 --> 00:10:33,920
separately

00:10:31,519 --> 00:10:35,760
and i'm going to i'm not going to get

00:10:33,920 --> 00:10:37,120
into that here now but uh there is a

00:10:35,760 --> 00:10:40,640
talk after me

00:10:37,120 --> 00:10:43,360
from uh pavel mcinerney and uh

00:10:40,640 --> 00:10:45,440
he has far greater knowledge about pi

00:10:43,360 --> 00:10:47,680
armament so i suggest that you

00:10:45,440 --> 00:10:49,360
join that talk if you can i think it'll

00:10:47,680 --> 00:10:52,079
be very very interesting

00:10:49,360 --> 00:10:53,680
um what i will do is show some code

00:10:52,079 --> 00:10:56,240
example because this relates to the

00:10:53,680 --> 00:11:00,160
flows we were talking about earlier so

00:10:56,240 --> 00:11:03,920
you would um import your pi arm and in

00:11:00,160 --> 00:11:07,200
package as you would any other python

00:11:03,920 --> 00:11:08,399
package and and what you want to do is

00:11:07,200 --> 00:11:10,800
create a network

00:11:08,399 --> 00:11:12,399
from a model file and a parser of your

00:11:10,800 --> 00:11:13,600
choice so i'm going to use the ionix

00:11:12,399 --> 00:11:17,519
parser

00:11:13,600 --> 00:11:20,959
and passing in a mobile net v2 uh onix

00:11:17,519 --> 00:11:24,399
model file and from that

00:11:20,959 --> 00:11:26,320
i'm going to create the network and

00:11:24,399 --> 00:11:28,800
what i want to do next is optimize that

00:11:26,320 --> 00:11:31,600
network and

00:11:28,800 --> 00:11:32,399
run it in the uh run an inference from

00:11:31,600 --> 00:11:34,560
it in the

00:11:32,399 --> 00:11:35,680
uh runtime so we need to create a few

00:11:34,560 --> 00:11:38,480
things here

00:11:35,680 --> 00:11:40,560
uh first we'll create a runtime to do

00:11:38,480 --> 00:11:41,200
that we need to initialize it with some

00:11:40,560 --> 00:11:44,560
creation

00:11:41,200 --> 00:11:45,600
options in this example here i'm just

00:11:44,560 --> 00:11:47,839
using

00:11:45,600 --> 00:11:49,600
default creation options but we'll see

00:11:47,839 --> 00:11:52,480
in one of the other features

00:11:49,600 --> 00:11:54,880
how this can be used and what types of

00:11:52,480 --> 00:11:56,959
creation options can be added to it

00:11:54,880 --> 00:11:58,079
so initialize that get your runtime

00:11:56,959 --> 00:12:00,399
object

00:11:58,079 --> 00:12:02,160
you also need a list of prepared backend

00:12:00,399 --> 00:12:04,399
preferred backends

00:12:02,160 --> 00:12:07,120
so this is the order of which we would

00:12:04,399 --> 00:12:10,079
prefer these back-ends to be

00:12:07,120 --> 00:12:10,639
chosen and then you pass all of that

00:12:10,079 --> 00:12:12,959
into

00:12:10,639 --> 00:12:14,160
uh an optimized function so you pass

00:12:12,959 --> 00:12:17,120
your network

00:12:14,160 --> 00:12:18,560
your preferred backends your runtime get

00:12:17,120 --> 00:12:21,360
device spec and this will

00:12:18,560 --> 00:12:22,480
get the list of backends that are

00:12:21,360 --> 00:12:26,560
actually available

00:12:22,480 --> 00:12:29,760
on the device that you're running and

00:12:26,560 --> 00:12:32,160
you also pass in some optimizer options

00:12:29,760 --> 00:12:33,680
these are not to be confused with the

00:12:32,160 --> 00:12:35,120
creation options i talked about in the

00:12:33,680 --> 00:12:37,680
last slide these are

00:12:35,120 --> 00:12:39,519
optimizer options specific for the

00:12:37,680 --> 00:12:40,560
optimizer and we'll look at those again

00:12:39,519 --> 00:12:43,600
later as well

00:12:40,560 --> 00:12:46,639
so once you have your optimized network

00:12:43,600 --> 00:12:50,720
uh you can load that into your runtime

00:12:46,639 --> 00:12:54,000
and get a unique network id

00:12:50,720 --> 00:12:55,680
and finally uh you want to get your

00:12:54,000 --> 00:12:57,440
input tensors and output tensors from

00:12:55,680 --> 00:12:58,399
the input names and binding info from

00:12:57,440 --> 00:13:01,839
the model

00:12:58,399 --> 00:13:04,880
and you pass all of that into your

00:13:01,839 --> 00:13:06,079
in queue workload function uh on the

00:13:04,880 --> 00:13:08,079
runtime

00:13:06,079 --> 00:13:10,079
you get your network id the input

00:13:08,079 --> 00:13:14,240
tensors and output tensors

00:13:10,079 --> 00:13:17,120
and voila you run your inference

00:13:14,240 --> 00:13:18,720
so like i said there's a really good

00:13:17,120 --> 00:13:21,279
talk after me and there's also

00:13:18,720 --> 00:13:22,240
loads of more information on pi armanin

00:13:21,279 --> 00:13:26,160
in the readme

00:13:22,240 --> 00:13:27,680
and we also have uh examples in that

00:13:26,160 --> 00:13:30,000
example folder there

00:13:27,680 --> 00:13:31,040
it has an object detection sample

00:13:30,000 --> 00:13:32,880
application in it

00:13:31,040 --> 00:13:34,480
and a really detailed readme on how it

00:13:32,880 --> 00:13:38,160
all works

00:13:34,480 --> 00:13:41,839
okay so next feature is the armanin

00:13:38,160 --> 00:13:43,199
back-ends uh so

00:13:41,839 --> 00:13:46,079
we've already talked about this is kind

00:13:43,199 --> 00:13:48,160
of a summary a backend is an abstraction

00:13:46,079 --> 00:13:49,920
connecting layers of a graph to the

00:13:48,160 --> 00:13:52,880
underlying hardware

00:13:49,920 --> 00:13:53,920
hardware through a driver or compute

00:13:52,880 --> 00:13:57,519
engine

00:13:53,920 --> 00:13:59,519
so our existing backends by default

00:13:57,519 --> 00:14:00,959
uh when you download rmn you get the

00:13:59,519 --> 00:14:04,079
opencl

00:14:00,959 --> 00:14:06,880
which represents mali gpu and the neon

00:14:04,079 --> 00:14:09,600
which is back end which is cortex a cpu

00:14:06,880 --> 00:14:12,079
go to those using arm compute library

00:14:09,600 --> 00:14:14,720
we also by default include a reference

00:14:12,079 --> 00:14:16,240
back end this is purely for testing

00:14:14,720 --> 00:14:17,760
just to make sure everything works and

00:14:16,240 --> 00:14:20,639
you can run it on a device that doesn't

00:14:17,760 --> 00:14:23,519
have rmip

00:14:20,639 --> 00:14:24,720
and then like i said downloadable is an

00:14:23,519 --> 00:14:27,120
mpu backend

00:14:24,720 --> 00:14:29,199
uh which would be for accessing the

00:14:27,120 --> 00:14:32,240
ethos in processor

00:14:29,199 --> 00:14:34,560
so rmn allows adding

00:14:32,240 --> 00:14:36,240
custom back-ends through our pluggable

00:14:34,560 --> 00:14:38,959
back-end mechanism

00:14:36,240 --> 00:14:40,399
and you can also load custom back-ends

00:14:38,959 --> 00:14:44,079
at runtime through the

00:14:40,399 --> 00:14:44,800
dynamic back-end interface i've sent two

00:14:44,079 --> 00:14:45,920
links here

00:14:44,800 --> 00:14:47,360
because again we have a lot of

00:14:45,920 --> 00:14:48,399
information in the readme i could

00:14:47,360 --> 00:14:51,440
probably talk

00:14:48,399 --> 00:14:52,880
uh entire talk on both of those features

00:14:51,440 --> 00:14:55,360
but

00:14:52,880 --> 00:14:57,760
those links will help you out if you

00:14:55,360 --> 00:15:00,480
want more information on them

00:14:57,760 --> 00:15:00,959
so currently globally if you want to set

00:15:00,480 --> 00:15:02,399
the

00:15:00,959 --> 00:15:05,519
your preference for back-ends this is

00:15:02,399 --> 00:15:07,360
how you would do it um

00:15:05,519 --> 00:15:09,360
you give your list of preferred

00:15:07,360 --> 00:15:10,560
back-ends in this example here i have a

00:15:09,360 --> 00:15:13,440
custom backend

00:15:10,560 --> 00:15:14,560
and as my number one preference then a

00:15:13,440 --> 00:15:17,839
cpu acc

00:15:14,560 --> 00:15:20,880
and a gpu sec and when

00:15:17,839 --> 00:15:21,519
arm and then optimizes the execution of

00:15:20,880 --> 00:15:23,839
the graph

00:15:21,519 --> 00:15:26,639
it does so into sub graphs so in this

00:15:23,839 --> 00:15:28,880
example over here

00:15:26,639 --> 00:15:30,000
it has been optimized and layer 1 and

00:15:28,880 --> 00:15:32,480
layer 2

00:15:30,000 --> 00:15:33,920
are supported on my custom back end so

00:15:32,480 --> 00:15:38,160
they get

00:15:33,920 --> 00:15:40,720
put into a sub graph of their own

00:15:38,160 --> 00:15:41,759
layer 3 is not supported on custom

00:15:40,720 --> 00:15:44,959
backend so it gets

00:15:41,759 --> 00:15:47,199
put into a sub graph which is supported

00:15:44,959 --> 00:15:47,920
on the cpu scc it goes to the next back

00:15:47,199 --> 00:15:51,120
end

00:15:47,920 --> 00:15:53,680
and then layer 4 and layer 5 these again

00:15:51,120 --> 00:15:55,920
are supported on my custom back end so

00:15:53,680 --> 00:15:56,720
they get their own sub-graph and that

00:15:55,920 --> 00:15:59,519
and that

00:15:56,720 --> 00:16:00,240
is the order that is used all the way

00:15:59,519 --> 00:16:04,320
right through

00:16:00,240 --> 00:16:07,680
your model our new feature

00:16:04,320 --> 00:16:08,000
is for power users of rmn and what it

00:16:07,680 --> 00:16:11,120
does

00:16:08,000 --> 00:16:12,639
is you can specify your preference per

00:16:11,120 --> 00:16:15,920
layer so

00:16:12,639 --> 00:16:18,240
if you set uh this back-end hint it's

00:16:15,920 --> 00:16:18,800
treated as the first most preferred

00:16:18,240 --> 00:16:20,880
back-end

00:16:18,800 --> 00:16:23,680
select during the back-end selection

00:16:20,880 --> 00:16:27,120
phase of the optimizer

00:16:23,680 --> 00:16:27,759
and if it doesn't find support for the

00:16:27,120 --> 00:16:29,920
layer

00:16:27,759 --> 00:16:31,360
uh we used a normal fallback that you

00:16:29,920 --> 00:16:34,240
saw uh

00:16:31,360 --> 00:16:35,360
in the previous slide and this is how

00:16:34,240 --> 00:16:38,000
you would do it

00:16:35,360 --> 00:16:39,199
you add the following method to the i

00:16:38,000 --> 00:16:42,399
connectable layer

00:16:39,199 --> 00:16:47,440
interface so when i was talking about

00:16:42,399 --> 00:16:50,000
using our api the direct api

00:16:47,440 --> 00:16:51,839
to make layers and connect them together

00:16:50,000 --> 00:16:52,320
you'd use this object on eye connectable

00:16:51,839 --> 00:16:54,399
layer

00:16:52,320 --> 00:16:56,320
and we've added this uh back in

00:16:54,399 --> 00:16:58,720
selection hint so you can tell

00:16:56,320 --> 00:16:59,600
a specific layer as you're creating your

00:16:58,720 --> 00:17:02,480
model

00:16:59,600 --> 00:17:03,120
i want this one to run on this specific

00:17:02,480 --> 00:17:07,919
back-end

00:17:03,120 --> 00:17:08,400
id now there is a caveat is so if the

00:17:07,919 --> 00:17:12,959
user

00:17:08,400 --> 00:17:15,199
wants to load a model using the parsers

00:17:12,959 --> 00:17:16,799
there's no handles to the layers while

00:17:15,199 --> 00:17:18,000
the input network has been built so you

00:17:16,799 --> 00:17:20,480
cannot explicitly

00:17:18,000 --> 00:17:21,600
set that back-end hint if using the

00:17:20,480 --> 00:17:25,120
parser

00:17:21,600 --> 00:17:28,799
but we do provide an i-network

00:17:25,120 --> 00:17:31,120
accept uh layer visitor interface

00:17:28,799 --> 00:17:32,880
and uh with a little bit of c plus

00:17:31,120 --> 00:17:33,840
knowledge you can provide a custom

00:17:32,880 --> 00:17:35,840
visitor class

00:17:33,840 --> 00:17:38,000
which would identify layers in the graph

00:17:35,840 --> 00:17:41,760
in order to set the back-end hint

00:17:38,000 --> 00:17:44,840
based on your custom rules

00:17:41,760 --> 00:17:47,600
okay this is the second back-end

00:17:44,840 --> 00:17:48,559
feature so in the 2002 release we

00:17:47,600 --> 00:17:50,799
introduced a new

00:17:48,559 --> 00:17:53,039
api backend options for passing

00:17:50,799 --> 00:17:54,480
parameters directly to the back ends and

00:17:53,039 --> 00:17:57,280
the backend options

00:17:54,480 --> 00:17:57,679
are added during the initialization of

00:17:57,280 --> 00:18:01,679
the

00:17:57,679 --> 00:18:03,760
runtime so if you recall when the

00:18:01,679 --> 00:18:06,640
pyramid in example we had

00:18:03,760 --> 00:18:08,240
a creation options that we added when we

00:18:06,640 --> 00:18:08,960
initialized the runtime well the backend

00:18:08,240 --> 00:18:10,960
options

00:18:08,960 --> 00:18:12,320
are a member of those creation options

00:18:10,960 --> 00:18:15,120
you'll see that now in a minute

00:18:12,320 --> 00:18:16,000
i'm going to demonstrate using this new

00:18:15,120 --> 00:18:18,000
api

00:18:16,000 --> 00:18:19,440
with an existing feature we have called

00:18:18,000 --> 00:18:23,280
a gpu

00:18:19,440 --> 00:18:26,320
acc tuner that previously had its own

00:18:23,280 --> 00:18:29,039
api so

00:18:26,320 --> 00:18:30,720
the tuner is a module of arm compute

00:18:29,039 --> 00:18:31,919
library and it can improve the

00:18:30,720 --> 00:18:34,960
performance

00:18:31,919 --> 00:18:38,000
of the opencl kernels by tuning the

00:18:34,960 --> 00:18:41,360
local workgroup size and so

00:18:38,000 --> 00:18:45,120
you can give it a file and

00:18:41,360 --> 00:18:46,640
it will learn uh the correct values to

00:18:45,120 --> 00:18:47,840
write to the file so you can have a

00:18:46,640 --> 00:18:50,240
trade-off between

00:18:47,840 --> 00:18:52,160
the time taken to tune and the kernel

00:18:50,240 --> 00:18:56,240
execution time achieved

00:18:52,160 --> 00:18:59,600
um so here's an example

00:18:56,240 --> 00:19:01,280
i'm using c plus plus code here

00:18:59,600 --> 00:19:04,000
but you can see it's quite similar flow

00:19:01,280 --> 00:19:06,799
to the python um

00:19:04,000 --> 00:19:07,440
code so here i'm creating my creation

00:19:06,799 --> 00:19:10,720
options

00:19:07,440 --> 00:19:12,880
but before i initialize my runtime i'm

00:19:10,720 --> 00:19:14,799
accessing the back end options on it and

00:19:12,880 --> 00:19:18,960
i'm placing back into that

00:19:14,799 --> 00:19:22,480
my custom parameters for the gpu

00:19:18,960 --> 00:19:23,679
acc backend so here i am referencing gpu

00:19:22,480 --> 00:19:25,679
sec back end

00:19:23,679 --> 00:19:27,760
and these are my custom parameters so

00:19:25,679 --> 00:19:29,679
i've in this example i'm

00:19:27,760 --> 00:19:30,960
picking a tune in level of two and i'm

00:19:29,679 --> 00:19:34,320
also passing in my

00:19:30,960 --> 00:19:38,080
uh my file where i want to write that

00:19:34,320 --> 00:19:39,120
data to the tuning information

00:19:38,080 --> 00:19:41,679
and then it's just a matter of

00:19:39,120 --> 00:19:43,760
initializing your runtime you pass your

00:19:41,679 --> 00:19:45,039
your creation options in once that's

00:19:43,760 --> 00:19:48,799
been set

00:19:45,039 --> 00:19:51,120
um i have here as a note if you want to

00:19:48,799 --> 00:19:52,960
actually then execute that tuning data

00:19:51,120 --> 00:19:53,600
you would just start up with the tuning

00:19:52,960 --> 00:19:56,960
file

00:19:53,600 --> 00:20:00,480
specified and not the tuning level

00:19:56,960 --> 00:20:03,440
okay so the next

00:20:00,480 --> 00:20:04,559
uh feature is the data type support so

00:20:03,440 --> 00:20:07,440
in

00:20:04,559 --> 00:20:08,480
2005 we added some new data types and we

00:20:07,440 --> 00:20:10,960
renamed some

00:20:08,480 --> 00:20:11,679
uh data types so the existing uh data

00:20:10,960 --> 00:20:15,200
types we have

00:20:11,679 --> 00:20:18,000
had were quantized asm8 which has been

00:20:15,200 --> 00:20:22,000
renamed to qasim u8

00:20:18,000 --> 00:20:25,280
queue asymmetric unsigned 8-bit

00:20:22,000 --> 00:20:28,320
and quantized sem16 which would be q

00:20:25,280 --> 00:20:31,919
sim sixteen and then we added these

00:20:28,320 --> 00:20:35,280
two new data types q7 s8 qasim s8

00:20:31,919 --> 00:20:36,799
and then the b float 16. so let's have a

00:20:35,280 --> 00:20:40,000
look at those

00:20:36,799 --> 00:20:43,039
the q a sim s8 and the cusin

00:20:40,000 --> 00:20:47,600
s8 in the 2002 release

00:20:43,039 --> 00:20:50,000
we updated tflight to 1.15 and

00:20:47,600 --> 00:20:52,000
the new tensorflow lite uh 8-bit

00:20:50,000 --> 00:20:55,760
quantization specification

00:20:52,000 --> 00:20:57,039
uses end date instead of you and it to

00:20:55,760 --> 00:20:59,039
hold the payload

00:20:57,039 --> 00:21:00,640
so to support this we added these two

00:20:59,039 --> 00:21:03,280
new data types

00:21:00,640 --> 00:21:05,520
quantized asymmetric signed 8-bit

00:21:03,280 --> 00:21:10,159
integer type and the quantized

00:21:05,520 --> 00:21:10,159
symmetric signed 8-bit integer type

00:21:10,480 --> 00:21:14,559
asymmetric means the zero point can be

00:21:13,280 --> 00:21:18,400
anywhere within the

00:21:14,559 --> 00:21:21,440
signed into range so minus 128

00:21:18,400 --> 00:21:25,360
to 127 and activations would be

00:21:21,440 --> 00:21:26,880
uh asymmetric um and symmetric

00:21:25,360 --> 00:21:28,640
uh then would mean the zero point is

00:21:26,880 --> 00:21:30,960
equal to zero and weights

00:21:28,640 --> 00:21:31,760
are forced to have their zero point

00:21:30,960 --> 00:21:34,880
equal

00:21:31,760 --> 00:21:36,400
to zero so uh

00:21:34,880 --> 00:21:38,640
we provided support for this for

00:21:36,400 --> 00:21:40,320
tensorflow lite and we also updated the

00:21:38,640 --> 00:21:44,480
android n and driver

00:21:40,320 --> 00:21:47,520
to support the corresponding data types

00:21:44,480 --> 00:21:50,559
in those and we

00:21:47,520 --> 00:21:52,000
also added two these are two other

00:21:50,559 --> 00:21:56,320
programs that armanin

00:21:52,000 --> 00:21:58,000
comes with um the serializer a tool for

00:21:56,320 --> 00:22:01,039
serializing and deserializing

00:21:58,000 --> 00:22:03,360
armament and network to a stream and

00:22:01,039 --> 00:22:05,520
the quantizer a tool for converting a

00:22:03,360 --> 00:22:08,880
32-bit float network

00:22:05,520 --> 00:22:12,400
into a quantized asymmetric 8-bit

00:22:08,880 --> 00:22:12,799
or symmetric 16-bit network so we added

00:22:12,400 --> 00:22:16,240
the

00:22:12,799 --> 00:22:16,240
support for those as well to that

00:22:16,480 --> 00:22:24,000
uh data type support

00:22:20,799 --> 00:22:27,679
so uh the b float 16

00:22:24,000 --> 00:22:28,159
in 2005 we added a bfloat 16 data type

00:22:27,679 --> 00:22:32,320
support

00:22:28,159 --> 00:22:34,200
and what that is is bf16 provides an

00:22:32,320 --> 00:22:37,520
easy way to accelerate

00:22:34,200 --> 00:22:39,039
fp32 networks with a smaller loss in

00:22:37,520 --> 00:22:42,480
accuracy

00:22:39,039 --> 00:22:44,799
compared to fp16 so the benefits would

00:22:42,480 --> 00:22:47,520
include reduction of memory footprint

00:22:44,799 --> 00:22:48,400
and computation speed up compared to

00:22:47,520 --> 00:22:52,640
float

00:22:48,400 --> 00:22:56,640
fp32 but more accurate

00:22:52,640 --> 00:22:59,440
than fb16 so the

00:22:56,640 --> 00:23:01,840
existing fb16 turbo mode what it does is

00:22:59,440 --> 00:23:05,360
it adds a conversion layer

00:23:01,840 --> 00:23:06,000
before the inputs and are after the

00:23:05,360 --> 00:23:08,480
inputs and

00:23:06,000 --> 00:23:09,280
for the outputs so that the inputs and

00:23:08,480 --> 00:23:12,799
outputs are still

00:23:09,280 --> 00:23:14,000
fp32 and then any uh layers that can be

00:23:12,799 --> 00:23:17,039
supported within

00:23:14,000 --> 00:23:20,320
uh would be fb16 and

00:23:17,039 --> 00:23:23,600
so our second turbo mode does the same

00:23:20,320 --> 00:23:25,520
and this is how you engage that so uh

00:23:23,600 --> 00:23:27,840
you create your optimizer options

00:23:25,520 --> 00:23:29,760
again this is c plus plus but it looks

00:23:27,840 --> 00:23:31,440
quite familiar to the flow we saw in the

00:23:29,760 --> 00:23:35,360
python api

00:23:31,440 --> 00:23:38,400
um and you call the member on it here

00:23:35,360 --> 00:23:40,960
reduce fp32 to bf16

00:23:38,400 --> 00:23:41,600
and set that to true and then it's just

00:23:40,960 --> 00:23:45,279
a case

00:23:41,600 --> 00:23:47,520
of you passing your optimizer options

00:23:45,279 --> 00:23:48,400
uh into the optimize function as per

00:23:47,520 --> 00:23:53,279
usual

00:23:48,400 --> 00:23:53,279
and our optimizer takes care of the rest

00:23:54,559 --> 00:23:58,080
okay uh i'm getting very close to the

00:23:57,200 --> 00:24:03,120
end i see for

00:23:58,080 --> 00:24:05,360
time um i will do this one quickly

00:24:03,120 --> 00:24:06,640
so android nn operators were always

00:24:05,360 --> 00:24:09,120
adding them these are the ones that we

00:24:06,640 --> 00:24:13,919
added in halo 1.0 and 1.1 they're

00:24:09,120 --> 00:24:17,120
also available to house 1.2 and 1.3

00:24:13,919 --> 00:24:19,039
and here in orange you can see all of

00:24:17,120 --> 00:24:21,200
the new operators that we have added

00:24:19,039 --> 00:24:24,799
since the 1911 release

00:24:21,200 --> 00:24:27,840
so there are quite a few there and

00:24:24,799 --> 00:24:31,919
we have added not just to the hal 1.2

00:24:27,840 --> 00:24:36,159
but we have also added to the how hal

00:24:31,919 --> 00:24:36,159
1.3 um

00:24:36,880 --> 00:24:41,919
we've added elu and fantastically named

00:24:40,640 --> 00:24:44,320
hard swish

00:24:41,919 --> 00:24:45,279
which are activations with their own

00:24:44,320 --> 00:24:47,840
benefits

00:24:45,279 --> 00:24:50,159
and then some helper operators like phil

00:24:47,840 --> 00:24:53,440
which will fill a tensor with a scalar

00:24:50,159 --> 00:24:56,400
value and a rank which will report it as

00:24:53,440 --> 00:24:58,080
a number of dimensions of a tensor and

00:24:56,400 --> 00:25:00,960
then qlstm

00:24:58,080 --> 00:25:02,880
which is quantized long short-term

00:25:00,960 --> 00:25:06,480
memory used for speech

00:25:02,880 --> 00:25:06,799
uh recognition um so while we've added a

00:25:06,480 --> 00:25:09,520
lot

00:25:06,799 --> 00:25:10,400
there's there are always more to add so

00:25:09,520 --> 00:25:12,240
uh

00:25:10,400 --> 00:25:13,520
i'd like to finish then with these

00:25:12,240 --> 00:25:16,400
slides which would be

00:25:13,520 --> 00:25:16,640
if you would like to contribute or add

00:25:16,400 --> 00:25:19,919
an

00:25:16,640 --> 00:25:21,440
operator or indeed any other feature to

00:25:19,919 --> 00:25:25,200
armin in

00:25:21,440 --> 00:25:28,400
this is how you would do it so

00:25:25,200 --> 00:25:30,400
harmony is currently open source mit

00:25:28,400 --> 00:25:32,640
license

00:25:30,400 --> 00:25:34,400
it uses continuously integrated

00:25:32,640 --> 00:25:36,159
development model we undertook that when

00:25:34,400 --> 00:25:38,559
we were accepted as part

00:25:36,159 --> 00:25:40,799
of the official artificial intelligence

00:25:38,559 --> 00:25:43,200
initiative of linaro

00:25:40,799 --> 00:25:44,240
and uh every commit to master is

00:25:43,200 --> 00:25:47,520
publicly available

00:25:44,240 --> 00:25:50,720
on review.mltheplatform.org

00:25:47,520 --> 00:25:52,720
um we also mirror those on github the

00:25:50,720 --> 00:25:53,919
master and we make releases to github

00:25:52,720 --> 00:25:57,120
but the action is on

00:25:53,919 --> 00:26:00,480
ml platform girl so

00:25:57,120 --> 00:26:03,440
you can set up your github account

00:26:00,480 --> 00:26:04,080
and set up your name your email address

00:26:03,440 --> 00:26:06,720
and

00:26:04,080 --> 00:26:08,000
once you've patched for your push a

00:26:06,720 --> 00:26:10,960
patch for code

00:26:08,000 --> 00:26:11,600
you would sign off and uh the patch will

00:26:10,960 --> 00:26:13,600
appear on

00:26:11,600 --> 00:26:15,360
ml platform garrett here and more than

00:26:13,600 --> 00:26:19,279
likely somebody from arm and inn

00:26:15,360 --> 00:26:20,400
will review and uh once that gets merged

00:26:19,279 --> 00:26:23,760
that's available

00:26:20,400 --> 00:26:25,279
for everyone to take advantage of on the

00:26:23,760 --> 00:26:27,840
master

00:26:25,279 --> 00:26:30,960
so i think i may have gone quite close

00:26:27,840 --> 00:26:34,159
to time or if not over

00:26:30,960 --> 00:26:37,039
so i can leave it there and see

00:26:34,159 --> 00:26:37,039
if there are

00:26:37,279 --> 00:26:39,840
any questions

00:26:49,440 --> 00:26:52,559
there's one question kevin and i've

00:26:51,120 --> 00:26:56,000
moved over to

00:26:52,559 --> 00:26:59,760
um from

00:26:56,000 --> 00:26:59,760
zoom over to slack

00:27:00,799 --> 00:27:07,679
yes so annie any any plans

00:27:04,480 --> 00:27:09,039
for rmn integration within video

00:27:07,679 --> 00:27:11,200
hardware software

00:27:09,039 --> 00:27:12,799
um i'm not in a position to talk about

00:27:11,200 --> 00:27:16,480
that uh

00:27:12,799 --> 00:27:20,320
so unfortunately i am

00:27:16,480 --> 00:27:23,919
i can't answer it um i wish i could

00:27:20,320 --> 00:27:24,640
but i i am not aware of it are privy to

00:27:23,919 --> 00:27:29,520
it

00:27:24,640 --> 00:27:29,520

YouTube URL: https://www.youtube.com/watch?v=OyP1ktbucuE


