Title: LVC20 202 RoboCar Vision Perception Unit  Using Ultra96 Board and Autoware Stack
Publication date: 2020-10-07
Playlist: Linaro Virtual Connect 2020
Description: 
	Autonomous vehicles are becoming a part of normal life as companies, universities and foundations are heavily investing in projects to aid its research and development. One such initiative that has taken up wide acceptance by the automotive community is Autoware Foundation. This project supports self-driving mobility and has been adopted by over 100 companies and 40 vehicles. 96Boards, Autoware Foundation and its members (Xilinx, AutoCore) have teamed up to design an Autonomous driving solution using a customized Ultra96 Board and the Autoware stack. Using a distributed system design we will demonstrate some of the key autonomous driving features, which will also have the potential to be deployed as an ADAS module.

The talk will describe in detail the design and implementation of the vision control unit of RoboCar covering the hardware, software features and performance capabilities. The vision perception unit performs the main perception tasks in autonomous driving including object detection, traffic light detection and self-parking. The algorithms and models are open source and have been implemented using Xilinx FPGAs on the Ultra96 boards. The design of the functional nodes in the autonomous vehicle is distributed in nature with the nodes talking to each other over a Distributed Data Service layer as a messaging middleware and a real-time kernel to coordinate the actions. We also demonstrate the capability of Ultra96 MPSoC technology to handle multiple channels of LVDS real-time camera and the integration with the Lidar/Radar point cloud fusion to feed into the decision making unit of the overall system.

The presentation will also cover an Open source AI framework (XTA) used for object detection using Yolov3-tiny model. The details of image capture and algorithm processing of the vision perception pipeline will be presented along with the performance measurements in each phase of the pipeline. We will also be illustrating the ability of the stack to update the software components and designs through OTA. It is envisioned that the core AI engine will require regular updates with the latest training values; hence a built-in platform level mechanism supporting such capability is essential for real-world deployment. 

https://connect.linaro.org/resources/lvc20/lvc20-202/
Captions: 
	00:00:00,160 --> 00:00:06,640
so my name is ravi kumar sakravati i

00:00:03,280 --> 00:00:06,960
lead the open source stack development

00:00:06,640 --> 00:00:10,080
at

00:00:06,960 --> 00:00:13,280
xilinx and

00:00:10,080 --> 00:00:14,960
lately my focus has been more around ai

00:00:13,280 --> 00:00:18,480
acceleration stacks

00:00:14,960 --> 00:00:21,359
and automotive solutions also targeting

00:00:18,480 --> 00:00:23,359
autonomous driving so today in this

00:00:21,359 --> 00:00:25,279
presentation

00:00:23,359 --> 00:00:27,760
i will be discussing a vision control

00:00:25,279 --> 00:00:31,039
unit that we designed and developed

00:00:27,760 --> 00:00:34,160
working with lenaro and autocore

00:00:31,039 --> 00:00:35,760
and my my partner yang also i think he's

00:00:34,160 --> 00:00:38,320
in the meeting today

00:00:35,760 --> 00:00:39,840
we work together to build a solution so

00:00:38,320 --> 00:00:42,640
before we talk about the

00:00:39,840 --> 00:00:44,640
uh solution i just want to give a quick

00:00:42,640 --> 00:00:47,120
background on why this project started

00:00:44,640 --> 00:00:49,120
and what is the intent of this project

00:00:47,120 --> 00:00:51,360
so going to a quick introduction auto

00:00:49,120 --> 00:00:54,960
auto uh

00:00:51,360 --> 00:00:56,640
where foundation has been

00:00:54,960 --> 00:00:59,120
initiating and growing a lot of the auto

00:00:56,640 --> 00:01:00,079
web projects and this has been adopted

00:00:59,120 --> 00:01:02,800
by

00:01:00,079 --> 00:01:03,840
many companies around the world and over

00:01:02,800 --> 00:01:06,880
20 countries

00:01:03,840 --> 00:01:10,840
and it has been used in a

00:01:06,880 --> 00:01:14,320
variety of autonomous driving

00:01:10,840 --> 00:01:17,280
applications like robocars

00:01:14,320 --> 00:01:20,000
automatic automatic conveyors last mile

00:01:17,280 --> 00:01:23,600
delivery vehicles

00:01:20,000 --> 00:01:25,200
so autocore is one of the key

00:01:23,600 --> 00:01:26,880
members of ottawa foundation that has

00:01:25,200 --> 00:01:29,920
been

00:01:26,880 --> 00:01:30,880
really prophesizing this use of

00:01:29,920 --> 00:01:32,400
waterwear

00:01:30,880 --> 00:01:34,079
stack and they've been able to put

00:01:32,400 --> 00:01:35,520
together into an heterogeneous compute

00:01:34,079 --> 00:01:37,520
platform so this

00:01:35,520 --> 00:01:39,280
vision control unit that we have we're

00:01:37,520 --> 00:01:40,880
talking about today is part of that

00:01:39,280 --> 00:01:43,200
overall stack

00:01:40,880 --> 00:01:44,640
so if you look at the the bottom left

00:01:43,200 --> 00:01:47,759
side there's a diagram

00:01:44,640 --> 00:01:50,159
illustrating the uh the components of

00:01:47,759 --> 00:01:52,560
the autonomous

00:01:50,159 --> 00:01:54,399
driving stack so there's a sensing

00:01:52,560 --> 00:01:55,840
module there's a computing module in the

00:01:54,399 --> 00:01:58,960
actuation module

00:01:55,840 --> 00:02:00,799
the sensing module uses all kinds of

00:01:58,960 --> 00:02:03,119
cameras lidar radar

00:02:00,799 --> 00:02:04,079
uh all kinds of peripherals to capture

00:02:03,119 --> 00:02:06,960
data for the

00:02:04,079 --> 00:02:08,080
robocars and the compute module uses the

00:02:06,960 --> 00:02:11,280
data to

00:02:08,080 --> 00:02:11,920
to to make intelligent decisions for the

00:02:11,280 --> 00:02:13,680
car

00:02:11,920 --> 00:02:15,200
and the actuation module is really

00:02:13,680 --> 00:02:17,520
trying to

00:02:15,200 --> 00:02:18,239
to maneuver the car based on the nations

00:02:17,520 --> 00:02:20,879
uh

00:02:18,239 --> 00:02:21,280
generated from the computing module so

00:02:20,879 --> 00:02:23,920
the

00:02:21,280 --> 00:02:24,640
uh the presentation that we are doing is

00:02:23,920 --> 00:02:26,400
um

00:02:24,640 --> 00:02:28,160
focus around sensing and computing

00:02:26,400 --> 00:02:30,640
modules that we have

00:02:28,160 --> 00:02:32,080
we have designed and developed so to

00:02:30,640 --> 00:02:35,200
give you a big picture again

00:02:32,080 --> 00:02:37,200
um so there is uh there is a

00:02:35,200 --> 00:02:38,560
vision control unit and a perception

00:02:37,200 --> 00:02:40,959
control unit that is

00:02:38,560 --> 00:02:42,879
part of the uh autonomous driving car

00:02:40,959 --> 00:02:45,840
the vision control unit

00:02:42,879 --> 00:02:47,360
uses cameras to make intelligent visions

00:02:45,840 --> 00:02:49,200
for the car and it does

00:02:47,360 --> 00:02:51,120
all kinds of ai processing and all of

00:02:49,200 --> 00:02:53,120
that to be able to make those decisions

00:02:51,120 --> 00:02:55,120
the perception control unit relies on

00:02:53,120 --> 00:02:57,760
radar lidar information

00:02:55,120 --> 00:02:58,959
and together the data from the two is

00:02:57,760 --> 00:03:02,080
combined to make

00:02:58,959 --> 00:03:04,959
intelligent decisions and

00:03:02,080 --> 00:03:06,159
help maneuver the car so autocore has

00:03:04,959 --> 00:03:10,159
been one of the

00:03:06,159 --> 00:03:12,480
uh first to uh uh uh first to use 96

00:03:10,159 --> 00:03:14,239
boats to drive the mobility platform

00:03:12,480 --> 00:03:16,239
also built the first auto air reference

00:03:14,239 --> 00:03:17,920
platform and the first open source

00:03:16,239 --> 00:03:19,440
design and develop the first open source

00:03:17,920 --> 00:03:22,879
slam on slam

00:03:19,440 --> 00:03:23,599
algorithms on the platforms um so this

00:03:22,879 --> 00:03:26,879
actually this

00:03:23,599 --> 00:03:29,200
figure shows um the

00:03:26,879 --> 00:03:30,799
the various components or systems of the

00:03:29,200 --> 00:03:33,200
robocars

00:03:30,799 --> 00:03:34,959
and on the left side is the perception

00:03:33,200 --> 00:03:35,920
controller the right side is a mission

00:03:34,959 --> 00:03:38,560
control unit

00:03:35,920 --> 00:03:40,879
the perception control unit has

00:03:38,560 --> 00:03:44,239
connectivity to radar lidar

00:03:40,879 --> 00:03:45,599
ims gps sensors on the right side the

00:03:44,239 --> 00:03:48,720
vision control unit

00:03:45,599 --> 00:03:50,319
is connected to six cameras and the

00:03:48,720 --> 00:03:52,560
vision and the perception control you

00:03:50,319 --> 00:03:56,239
can talk to each other over ethernet

00:03:52,560 --> 00:03:58,080
and um and we'll discuss more in details

00:03:56,239 --> 00:04:01,200
how they talk to each other but

00:03:58,080 --> 00:04:02,799
essentially um the data gets passed

00:04:01,200 --> 00:04:05,519
between each other intelligent patients

00:04:02,799 --> 00:04:05,519
are made on the car

00:04:06,080 --> 00:04:14,799
so the uh the the whole setup has

00:04:10,799 --> 00:04:18,000
three different uh

00:04:14,799 --> 00:04:21,040
three different uh units of computation

00:04:18,000 --> 00:04:23,120
you're calling it p1p2p3 the

00:04:21,040 --> 00:04:24,320
the camera capture unit of the vision

00:04:23,120 --> 00:04:27,440
control unit

00:04:24,320 --> 00:04:30,720
does cnn processing for the overall car

00:04:27,440 --> 00:04:32,560
robocars does camera perception the pcu

00:04:30,720 --> 00:04:33,759
does localization and sensor fusion of

00:04:32,560 --> 00:04:36,639
the images

00:04:33,759 --> 00:04:38,960
and together they are combined and they

00:04:36,639 --> 00:04:41,840
send sent to the the p3

00:04:38,960 --> 00:04:44,160
computation unit that actually makes

00:04:41,840 --> 00:04:46,800
vehicle controllations

00:04:44,160 --> 00:04:49,440
for the stack uh in addition we also

00:04:46,800 --> 00:04:52,240
have connectivity here on the simulators

00:04:49,440 --> 00:04:54,800
that can so you can essentially develop

00:04:52,240 --> 00:04:58,800
algorithms on the simulator even before

00:04:54,800 --> 00:04:58,800
realizing this on the real platform

00:04:59,199 --> 00:05:06,400
um so the overall solution essentially

00:05:02,800 --> 00:05:08,080
has a pcu two pcus and one vision

00:05:06,400 --> 00:05:10,560
control unit

00:05:08,080 --> 00:05:11,919
um so let's talk about vision control

00:05:10,560 --> 00:05:14,479
units so essentially this isn't mission

00:05:11,919 --> 00:05:17,680
control unit uses cameras

00:05:14,479 --> 00:05:18,720
uh to sense the uh surrounding of the

00:05:17,680 --> 00:05:20,960
car

00:05:18,720 --> 00:05:22,560
and then make intelligence so before we

00:05:20,960 --> 00:05:23,600
get to the details so let's talk about

00:05:22,560 --> 00:05:26,400
requirements

00:05:23,600 --> 00:05:27,840
uh so the the vision control unit

00:05:26,400 --> 00:05:30,880
supports um

00:05:27,840 --> 00:05:32,560
needs to support uh four side cameras

00:05:30,880 --> 00:05:33,600
and the side cameras are essentially

00:05:32,560 --> 00:05:36,800
needed to

00:05:33,600 --> 00:05:38,240
get a surround view of the car and

00:05:36,800 --> 00:05:40,800
typically the

00:05:38,240 --> 00:05:42,479
solutions that the target is or the

00:05:40,800 --> 00:05:44,960
application the target is

00:05:42,479 --> 00:05:46,080
around the self parking and things like

00:05:44,960 --> 00:05:47,840
that nature

00:05:46,080 --> 00:05:49,759
and the two front facing cameras that

00:05:47,840 --> 00:05:51,120
capture the front view of the car and

00:05:49,759 --> 00:05:53,600
they're used for object detection and

00:05:51,120 --> 00:05:56,800
classification one of the requirements

00:05:53,600 --> 00:05:59,360
for us is to use open source stack

00:05:56,800 --> 00:06:00,479
and this is predominantly because most

00:05:59,360 --> 00:06:02,880
of the auto

00:06:00,479 --> 00:06:04,840
moto oems they want at least for the

00:06:02,880 --> 00:06:06,319
evaluation stack they want open source

00:06:04,840 --> 00:06:08,000
tax

00:06:06,319 --> 00:06:09,759
to be used and for production they

00:06:08,000 --> 00:06:11,440
wanted to be safety certified so most of

00:06:09,759 --> 00:06:14,639
the open source stacks are not really

00:06:11,440 --> 00:06:16,479
safety compliant but if you look here

00:06:14,639 --> 00:06:18,800
we are pretty much doing all of this on

00:06:16,479 --> 00:06:21,199
the open source stack

00:06:18,800 --> 00:06:22,080
and uh we're using v4l for capture

00:06:21,199 --> 00:06:24,080
pipeline

00:06:22,080 --> 00:06:25,680
we're using bunch of peripheral drivers

00:06:24,080 --> 00:06:29,039
open source drivers

00:06:25,680 --> 00:06:31,280
uh yokto linux kernel embedded stack

00:06:29,039 --> 00:06:32,960
we also have designed and developed uh

00:06:31,280 --> 00:06:34,319
open source ai stack called xt we'll

00:06:32,960 --> 00:06:36,479
talk about it later

00:06:34,319 --> 00:06:38,880
and the ros v2 middleware stack for

00:06:36,479 --> 00:06:41,840
communication between the processors

00:06:38,880 --> 00:06:44,400
crossing subsystems a performance

00:06:41,840 --> 00:06:45,759
requirements around 8 to 10 fps

00:06:44,400 --> 00:06:49,840
depending on the applications we are

00:06:45,759 --> 00:06:49,840
targeting and like any automotive

00:06:50,479 --> 00:06:53,759
application we are required to support

00:06:52,240 --> 00:06:55,280
the odr update

00:06:53,759 --> 00:06:57,120
in addition we are also required to

00:06:55,280 --> 00:07:00,240
support uh

00:06:57,120 --> 00:07:04,160
update of real runtime parameters

00:07:00,240 --> 00:07:04,160
for ai training

00:07:04,560 --> 00:07:10,000
so vision control unit um essentially

00:07:08,080 --> 00:07:12,240
the the the first thing that comes up in

00:07:10,000 --> 00:07:14,800
the vision controlling is a camera

00:07:12,240 --> 00:07:15,599
the camera is actually a sensor it has

00:07:14,800 --> 00:07:19,039
an isp

00:07:15,599 --> 00:07:20,560
and a serializer and the capture image

00:07:19,039 --> 00:07:22,080
is serialized and sent to the vision

00:07:20,560 --> 00:07:24,800
mezzanine board

00:07:22,080 --> 00:07:25,919
designed and developed by lenaro and

00:07:24,800 --> 00:07:28,240
this board

00:07:25,919 --> 00:07:30,319
actually deserializes the data and sends

00:07:28,240 --> 00:07:34,160
it over to

00:07:30,319 --> 00:07:37,599
the alternate six board

00:07:34,160 --> 00:07:38,319
and the ultra96 port actually dms the

00:07:37,599 --> 00:07:41,199
data to

00:07:38,319 --> 00:07:42,800
different regions of the memory so we

00:07:41,199 --> 00:07:46,479
are actually storing this data as

00:07:42,800 --> 00:07:48,560
a dma buff buffers

00:07:46,479 --> 00:07:50,319
and this will be used in the next stages

00:07:48,560 --> 00:07:52,240
of the pipeline processing

00:07:50,319 --> 00:07:53,759
the solution supports up to eight

00:07:52,240 --> 00:07:54,240
cameras in this picture we're showing

00:07:53,759 --> 00:07:57,360
four

00:07:54,240 --> 00:08:00,720
uh four links but it can support

00:07:57,360 --> 00:08:03,919
cameras so the

00:08:00,720 --> 00:08:06,080
image processing pipeline uses the the

00:08:03,919 --> 00:08:07,039
data captured from the dma buffers but

00:08:06,080 --> 00:08:10,240
it needs uh

00:08:07,039 --> 00:08:12,319
processing capability to compute uh

00:08:10,240 --> 00:08:13,840
the uh to do a lot of computation on

00:08:12,319 --> 00:08:15,840
this captured image

00:08:13,840 --> 00:08:16,960
so if you look at it and the the lowest

00:08:15,840 --> 00:08:20,720
end here we have

00:08:16,960 --> 00:08:21,759
um crossing back ins uh from xilinx mp

00:08:20,720 --> 00:08:24,879
socs

00:08:21,759 --> 00:08:27,440
so pl is the fpga uh

00:08:24,879 --> 00:08:28,479
fabric that supports compute

00:08:27,440 --> 00:08:30,720
acceleration

00:08:28,479 --> 00:08:32,800
the apus and rpus are the a53s and the

00:08:30,720 --> 00:08:35,360
r5s that we have in the um

00:08:32,800 --> 00:08:36,080
the zellings and psoc and the gpus of

00:08:35,360 --> 00:08:38,880
the malig

00:08:36,080 --> 00:08:39,680
molecule gpus the stack above it is the

00:08:38,880 --> 00:08:43,519
linux

00:08:39,680 --> 00:08:45,839
base stack we have the v4l2 for capture

00:08:43,519 --> 00:08:47,519
we have a heterogeneous scheduler

00:08:45,839 --> 00:08:48,080
designed and developed as a part of the

00:08:47,519 --> 00:08:50,640
xta

00:08:48,080 --> 00:08:52,160
ai engine stack and we'll talk about in

00:08:50,640 --> 00:08:55,440
the next few slides and how the

00:08:52,160 --> 00:08:57,920
heterogeneous scheduling can actually

00:08:55,440 --> 00:08:59,839
slice and dice a ai problem and

00:08:57,920 --> 00:09:02,560
orchestrate it across the different

00:08:59,839 --> 00:09:06,160
backend cores the open gles is

00:09:02,560 --> 00:09:09,920
predominantly used to accelerate

00:09:06,160 --> 00:09:11,440
the the image processing on the gpu

00:09:09,920 --> 00:09:14,080
if you look at the application stack it

00:09:11,440 --> 00:09:16,560
takes the data from the dma buffs

00:09:14,080 --> 00:09:18,320
and let's say you have four cameras you

00:09:16,560 --> 00:09:20,000
capture capture images for the four

00:09:18,320 --> 00:09:20,800
cameras it does the composition of the

00:09:20,000 --> 00:09:23,120
image

00:09:20,800 --> 00:09:25,279
it downscales the image and that's

00:09:23,120 --> 00:09:26,720
because the ai stacks under the certain

00:09:25,279 --> 00:09:28,800
resolution

00:09:26,720 --> 00:09:30,560
and then it normalizes the data and then

00:09:28,800 --> 00:09:32,000
it processes the back end so when it

00:09:30,560 --> 00:09:35,040
process the back end

00:09:32,000 --> 00:09:37,120
uh it does ai or the yolo v3 light or

00:09:35,040 --> 00:09:39,600
the object classification

00:09:37,120 --> 00:09:41,279
and the result is it it detects or it

00:09:39,600 --> 00:09:42,959
generates the bounding box for the

00:09:41,279 --> 00:09:43,519
region of interest or let's say if you

00:09:42,959 --> 00:09:46,560
are

00:09:43,519 --> 00:09:49,600
interested in detecting uh a car

00:09:46,560 --> 00:09:50,000
it cannot draws a bounding box around it

00:09:49,600 --> 00:09:51,760
or

00:09:50,000 --> 00:09:53,680
it generates the rear is generates the

00:09:51,760 --> 00:09:55,839
coordinates for the bonding box

00:09:53,680 --> 00:09:57,920
and that is faster gpu pipeline the gpu

00:09:55,839 --> 00:10:00,800
pipeline takes that and

00:09:57,920 --> 00:10:03,120
uh over the ros interface sends the data

00:10:00,800 --> 00:10:05,600
to the pcu the pcu is the perception

00:10:03,120 --> 00:10:08,399
controlling it takes the data from vcu

00:10:05,600 --> 00:10:12,079
and then it uses a sensor fusion data

00:10:08,399 --> 00:10:12,079
and it makes more intelligent decisions

00:10:12,320 --> 00:10:18,880
so xta is the uh the

00:10:16,320 --> 00:10:20,720
stack that we had designed for ai uh

00:10:18,880 --> 00:10:22,959
engine acceleration

00:10:20,720 --> 00:10:24,079
and uh one of the requirements that we

00:10:22,959 --> 00:10:26,240
had for

00:10:24,079 --> 00:10:27,680
doing the air acceleration was to be

00:10:26,240 --> 00:10:30,000
able to use all

00:10:27,680 --> 00:10:32,880
the heterogeneous hardware back-ends of

00:10:30,000 --> 00:10:34,640
designing csoc

00:10:32,880 --> 00:10:36,079
we are not aware of any stack that can

00:10:34,640 --> 00:10:38,240
use all the

00:10:36,079 --> 00:10:39,839
the compute capabilities of the mpsoc

00:10:38,240 --> 00:10:42,399
and there's a lot of reasons why we

00:10:39,839 --> 00:10:43,519
we came up with a stack but we there's

00:10:42,399 --> 00:10:46,079
one stack that

00:10:43,519 --> 00:10:48,079
we came across that was close enough to

00:10:46,079 --> 00:10:49,600
as a starting point and that was the dvm

00:10:48,079 --> 00:10:51,839
vta stack

00:10:49,600 --> 00:10:53,519
we extended the stack to support

00:10:51,839 --> 00:10:55,360
heterogeneous compute

00:10:53,519 --> 00:10:57,040
so and not only heterogeneous compute

00:10:55,360 --> 00:10:59,600
but also to be able to support

00:10:57,040 --> 00:11:00,720
pipelining and parallelism if i had a ai

00:10:59,600 --> 00:11:02,480
graph

00:11:00,720 --> 00:11:04,880
and i want to schedule it across every

00:11:02,480 --> 00:11:08,160
part of the back end here like fpga

00:11:04,880 --> 00:11:08,160
air cores r cores

00:11:08,399 --> 00:11:13,279
and the gpu i need a framework that can

00:11:11,440 --> 00:11:14,560
splice and dice the graph and schedule

00:11:13,279 --> 00:11:16,399
it

00:11:14,560 --> 00:11:18,079
and that was a key requirement to

00:11:16,399 --> 00:11:19,279
optimize performance

00:11:18,079 --> 00:11:21,200
and i also had it do it in a

00:11:19,279 --> 00:11:22,959
parallelized fashion so

00:11:21,200 --> 00:11:24,800
many of those frameworks they kind of

00:11:22,959 --> 00:11:26,000
execute on one core this and they send

00:11:24,800 --> 00:11:28,480
it to next core

00:11:26,000 --> 00:11:29,920
so we really wanted to pipeline it and

00:11:28,480 --> 00:11:32,160
paralyze it

00:11:29,920 --> 00:11:34,160
so we had to extend that capability we

00:11:32,160 --> 00:11:37,040
also wanted to make it configurable

00:11:34,160 --> 00:11:39,040
in the sense that if i uh if i didn't

00:11:37,040 --> 00:11:40,959
want to use like a r core

00:11:39,040 --> 00:11:42,560
um and if i wanted to use other three

00:11:40,959 --> 00:11:44,000
cores i wanted to be

00:11:42,560 --> 00:11:45,600
the architecture of the framework to

00:11:44,000 --> 00:11:46,800
give me the knobs to be able to do that

00:11:45,600 --> 00:11:49,519
so i can

00:11:46,800 --> 00:11:50,959
i can automatically orchestrate these

00:11:49,519 --> 00:11:54,000
graphs on these

00:11:50,959 --> 00:11:55,920
back-ends in addition we also wanted to

00:11:54,000 --> 00:11:58,000
be able to scale the processors in the

00:11:55,920 --> 00:11:59,519
sense that if i had a small fpga like

00:11:58,000 --> 00:12:01,920
ultra96 port

00:11:59,519 --> 00:12:04,399
i'd probably instantiate one gem code

00:12:01,920 --> 00:12:06,240
but if i had a larger fpga

00:12:04,399 --> 00:12:08,839
i could scale the performance by

00:12:06,240 --> 00:12:10,399
instantiate instantiating multiple gem

00:12:08,839 --> 00:12:13,120
cores

00:12:10,399 --> 00:12:13,920
and for that we wanted to make it very

00:12:13,120 --> 00:12:17,040
simple

00:12:13,920 --> 00:12:17,839
for the user to configure and configure

00:12:17,040 --> 00:12:21,519
these scores

00:12:17,839 --> 00:12:24,320
so that the stack automatically can

00:12:21,519 --> 00:12:25,519
splice the graph and and schedule it to

00:12:24,320 --> 00:12:28,639
various backends

00:12:25,519 --> 00:12:32,399
so one one other key requirement for

00:12:28,639 --> 00:12:33,360
um this ai framework is to be able to

00:12:32,399 --> 00:12:35,279
auto tune

00:12:33,360 --> 00:12:36,959
when you have multiple backends it

00:12:35,279 --> 00:12:38,399
really requires an expert level person

00:12:36,959 --> 00:12:41,440
to be able to

00:12:38,399 --> 00:12:43,839
to slice the graph and orchestrate it

00:12:41,440 --> 00:12:46,480
it is not a scalable solution and so we

00:12:43,839 --> 00:12:48,560
needed something that can be auto tuned

00:12:46,480 --> 00:12:50,320
auto tuning is a concept that's popular

00:12:48,560 --> 00:12:52,000
at an operator level

00:12:50,320 --> 00:12:54,160
supporting a specific back end but what

00:12:52,000 --> 00:12:55,200
we are supporting here is a graph level

00:12:54,160 --> 00:12:56,959
auto tuning

00:12:55,200 --> 00:12:58,959
or sub graph level auto tuning that can

00:12:56,959 --> 00:13:00,240
be orchestrated across multiple hardware

00:12:58,959 --> 00:13:02,639
back and forth

00:13:00,240 --> 00:13:04,639
so essentially it can splice the graph

00:13:02,639 --> 00:13:07,760
in the most optimal fashion

00:13:04,639 --> 00:13:08,480
and that can be uh scheduled in various

00:13:07,760 --> 00:13:10,160
back-ends

00:13:08,480 --> 00:13:12,320
for best performance so if you want to

00:13:10,160 --> 00:13:15,040
understand more about how we do this

00:13:12,320 --> 00:13:17,120
uh we have a presentation uh you know

00:13:15,040 --> 00:13:19,519
the ieee conferences next month

00:13:17,120 --> 00:13:20,720
we had uh we have a paper accepted in it

00:13:19,519 --> 00:13:24,399
so you are free to

00:13:20,720 --> 00:13:25,040
listen into it so at the nutshell the

00:13:24,399 --> 00:13:28,880
way the

00:13:25,040 --> 00:13:32,079
ai acceleration works is

00:13:28,880 --> 00:13:35,440
it uses a high level uh model like

00:13:32,079 --> 00:13:37,760
tensorflow it takes a graph uh

00:13:35,440 --> 00:13:39,600
if the xta has a compiler that passes

00:13:37,760 --> 00:13:43,279
the graph does a bunch of

00:13:39,600 --> 00:13:44,959
compute uh optimizations and then it

00:13:43,279 --> 00:13:47,120
come it converts it into an intermediate

00:13:44,959 --> 00:13:48,720
representation so we rely on tbm for

00:13:47,120 --> 00:13:51,440
some of the stuff

00:13:48,720 --> 00:13:52,399
and then once it does the intermediate

00:13:51,440 --> 00:13:54,720
representation

00:13:52,399 --> 00:13:56,800
we do a bunch of schedule optimizations

00:13:54,720 --> 00:13:58,639
especially around heterogeneity

00:13:56,800 --> 00:14:00,800
and that is where we are talking about

00:13:58,639 --> 00:14:04,000
the packing and splitting of graphs

00:14:00,800 --> 00:14:05,519
so if you think of a big graph we really

00:14:04,000 --> 00:14:08,880
want to splice it

00:14:05,519 --> 00:14:10,880
uh into multiple back ends uh so that

00:14:08,880 --> 00:14:12,160
it can be axial it can be pipelined and

00:14:10,880 --> 00:14:14,160
accelerated and

00:14:12,160 --> 00:14:16,399
parallelized so here if you look at it

00:14:14,160 --> 00:14:18,480
we are showing examples of

00:14:16,399 --> 00:14:20,160
fpga gp and arm instructions so the

00:14:18,480 --> 00:14:21,600
graph is essentially spread into three

00:14:20,160 --> 00:14:23,360
different back ends

00:14:21,600 --> 00:14:25,040
and we are generating binaries for each

00:14:23,360 --> 00:14:26,079
one of these back ends now these back

00:14:25,040 --> 00:14:27,760
ends binaries

00:14:26,079 --> 00:14:29,600
are then going to be used by runtime

00:14:27,760 --> 00:14:30,000
stack the jet compiler we talked about

00:14:29,600 --> 00:14:34,240
earlier

00:14:30,000 --> 00:14:34,240
that that can execute the graphs

00:14:34,800 --> 00:14:41,680
so this example shows

00:14:38,240 --> 00:14:44,800
how different images can be

00:14:41,680 --> 00:14:45,920
processed in a pipeline and parallel

00:14:44,800 --> 00:14:48,639
fashion

00:14:45,920 --> 00:14:49,440
um so essentially we break it down into

00:14:48,639 --> 00:14:51,199
four different

00:14:49,440 --> 00:14:52,639
three different stages pre-processing

00:14:51,199 --> 00:14:54,320
and the processing of image itself and

00:14:52,639 --> 00:14:56,000
the post-processing

00:14:54,320 --> 00:14:59,199
and if you look at the right side we're

00:14:56,000 --> 00:15:00,639
using all the heterogeneous scores

00:14:59,199 --> 00:15:03,199
not only the back ends but all the

00:15:00,639 --> 00:15:05,680
course that's available

00:15:03,199 --> 00:15:07,199
from the gpus the a53s have four four

00:15:05,680 --> 00:15:08,639
cores we're using all the four cores the

00:15:07,199 --> 00:15:10,639
fpga

00:15:08,639 --> 00:15:11,680
and r5s have two cores we're using all

00:15:10,639 --> 00:15:13,279
the cores

00:15:11,680 --> 00:15:15,360
and part of the reason we're doing this

00:15:13,279 --> 00:15:16,720
is we want to kind of load balance that

00:15:15,360 --> 00:15:19,199
across all the processors

00:15:16,720 --> 00:15:19,760
to be able to take best advantage of it

00:15:19,199 --> 00:15:21,760
and

00:15:19,760 --> 00:15:23,040
also want to make sure that we're not

00:15:21,760 --> 00:15:26,079
fully loading any

00:15:23,040 --> 00:15:28,240
cpu or the processing end

00:15:26,079 --> 00:15:32,000
so we could essentially throw more

00:15:28,240 --> 00:15:32,000
compute capabilities at it for future

00:15:32,240 --> 00:15:35,600
uh so this is some performance data here

00:15:34,639 --> 00:15:38,000
we are trying to

00:15:35,600 --> 00:15:39,759
get the best performance by trying to

00:15:38,000 --> 00:15:40,800
split the graph in a very optimal

00:15:39,759 --> 00:15:43,680
fashion

00:15:40,800 --> 00:15:45,839
um obviously uh compared to the existing

00:15:43,680 --> 00:15:48,160
frameworks that actually schedule the

00:15:45,839 --> 00:15:50,720
processing in a more sequential fashion

00:15:48,160 --> 00:15:52,160
doing it in a pipeline parallel process

00:15:50,720 --> 00:15:54,079
is obviously going to need a better

00:15:52,160 --> 00:15:55,440
performance and we are seeing very good

00:15:54,079 --> 00:15:59,199
performance results

00:15:55,440 --> 00:16:03,120
from this stack

00:15:59,199 --> 00:16:06,240
over the air update um so xilinx has

00:16:03,120 --> 00:16:07,279
uh had a flavor of linux called petrol

00:16:06,240 --> 00:16:09,519
linux that

00:16:07,279 --> 00:16:10,639
we we have been supporting for many

00:16:09,519 --> 00:16:14,160
years now

00:16:10,639 --> 00:16:19,040
and that stack um

00:16:14,160 --> 00:16:22,160
essentially supports uh

00:16:19,040 --> 00:16:23,600
runtime uh software component updates uh

00:16:22,160 --> 00:16:25,120
that's petal linux it's a flavor of

00:16:23,600 --> 00:16:26,240
linux that we have been using for some

00:16:25,120 --> 00:16:28,880
time now

00:16:26,240 --> 00:16:30,079
um and uh what we are doing is we are

00:16:28,880 --> 00:16:32,160
piggybacking on top of that

00:16:30,079 --> 00:16:35,600
infrastructure to be able to support

00:16:32,160 --> 00:16:39,360
update of both components uh update of

00:16:35,600 --> 00:16:43,040
runtime components kernels drivers

00:16:39,360 --> 00:16:44,560
even fpga designs and hardware designs

00:16:43,040 --> 00:16:47,199
in addition to that we're also trying to

00:16:44,560 --> 00:16:50,399
support the ability to be able to

00:16:47,199 --> 00:16:51,440
train ability to update the training

00:16:50,399 --> 00:16:54,240
parameters

00:16:51,440 --> 00:16:55,440
on the fly so it's actually a key uh

00:16:54,240 --> 00:16:58,560
requirement for

00:16:55,440 --> 00:17:00,959
uh because as you keep training

00:16:58,560 --> 00:17:04,000
and getting better optimized parameters

00:17:00,959 --> 00:17:07,760
you want to update it on the flight

00:17:04,000 --> 00:17:09,439
so this is actually showing um a demo of

00:17:07,760 --> 00:17:11,199
the solution we had put together

00:17:09,439 --> 00:17:12,799
this actually the solution actually uses

00:17:11,199 --> 00:17:15,120
four cameras

00:17:12,799 --> 00:17:16,400
and each camera is pointing to a certain

00:17:15,120 --> 00:17:20,400
region

00:17:16,400 --> 00:17:21,919
which has uh a car and the top left is

00:17:20,400 --> 00:17:24,079
actually a video clip

00:17:21,919 --> 00:17:26,240
and the the bottom two are static images

00:17:24,079 --> 00:17:27,039
the the right top is actually a non-car

00:17:26,240 --> 00:17:30,080
image

00:17:27,039 --> 00:17:31,840
so this uh actually um

00:17:30,080 --> 00:17:33,440
captures the image and we actually run

00:17:31,840 --> 00:17:34,240
yellow on each one of these camera

00:17:33,440 --> 00:17:36,320
images

00:17:34,240 --> 00:17:37,600
so you could configure the pipeline many

00:17:36,320 --> 00:17:39,360
different ways

00:17:37,600 --> 00:17:41,280
you could you could do a surround view

00:17:39,360 --> 00:17:42,720
of the four images and then apply yellow

00:17:41,280 --> 00:17:44,080
in this case we are applying yellow on

00:17:42,720 --> 00:17:45,600
each one of them

00:17:44,080 --> 00:17:47,039
we are seeing that the stack is able to

00:17:45,600 --> 00:17:48,400
scale up to those uh different

00:17:47,039 --> 00:17:52,720
requirements

00:17:48,400 --> 00:17:54,960
um so uh just to conclude

00:17:52,720 --> 00:17:57,039
we had a very good collaboration with uh

00:17:54,960 --> 00:17:57,919
leonardo autocore and xilinx to be able

00:17:57,039 --> 00:18:01,440
to put together

00:17:57,919 --> 00:18:04,960
automatic uh reference platform uh

00:18:01,440 --> 00:18:06,880
also i think this is one of the uh

00:18:04,960 --> 00:18:09,039
we have done a very good job i think in

00:18:06,880 --> 00:18:11,200
pulling together open source components

00:18:09,039 --> 00:18:13,440
and all the work that we have done uh we

00:18:11,200 --> 00:18:16,240
have uh upstream some of it and we are

00:18:13,440 --> 00:18:17,520
intending to upstream all of it uh in a

00:18:16,240 --> 00:18:18,720
phased approach

00:18:17,520 --> 00:18:20,160
um so that's been a very good

00:18:18,720 --> 00:18:22,320
collaboration but there's also been a

00:18:20,160 --> 00:18:25,679
lot of good collaboration with

00:18:22,320 --> 00:18:27,200
uh autoware foundation 96 boards

00:18:25,679 --> 00:18:28,880
and also the outer core foundation or

00:18:27,200 --> 00:18:31,200
auto core incorporation

00:18:28,880 --> 00:18:32,480
uh on the other parts of the autonomous

00:18:31,200 --> 00:18:34,400
driving stack especially around the

00:18:32,480 --> 00:18:38,400
perception control unit

00:18:34,400 --> 00:18:40,799
and the actuation control logic

00:18:38,400 --> 00:18:42,000
this solution actually uses alternate

00:18:40,799 --> 00:18:45,600
export this is a really low

00:18:42,000 --> 00:18:48,000
end board uh low cost board

00:18:45,600 --> 00:18:49,840
and also low end board but we are able

00:18:48,000 --> 00:18:53,200
to see good performance

00:18:49,840 --> 00:18:54,240
uh with that low end fpgas and not only

00:18:53,200 --> 00:18:56,320
that we think

00:18:54,240 --> 00:18:58,160
i believe that we have enough processing

00:18:56,320 --> 00:18:59,600
capability especially in the r5 cores

00:18:58,160 --> 00:19:00,400
there's enough processing power we

00:18:59,600 --> 00:19:03,360
haven't used

00:19:00,400 --> 00:19:05,280
much of the gpus as well and these can

00:19:03,360 --> 00:19:07,440
be used to extend the capabilities of

00:19:05,280 --> 00:19:10,240
the stack for

00:19:07,440 --> 00:19:13,280
other autonomous driving capabilities

00:19:10,240 --> 00:19:15,600
like localization and such

00:19:13,280 --> 00:19:25,840
so that concludes the presentations i am

00:19:15,600 --> 00:19:25,840
ready to take on any questions

00:19:38,080 --> 00:19:41,840
okay so let me see um

00:19:43,919 --> 00:19:47,679
so what are the main differences between

00:19:45,440 --> 00:19:50,960
the ottawa and apollo

00:19:47,679 --> 00:19:54,320
auto where apollo i i it's

00:19:50,960 --> 00:19:56,640
driven by baidu and um

00:19:54,320 --> 00:19:57,919
and auto west has us from lotto air

00:19:56,640 --> 00:20:01,039
foundation

00:19:57,919 --> 00:20:01,440
um apollo stack uh although they claim

00:20:01,039 --> 00:20:03,840
that

00:20:01,440 --> 00:20:05,760
uh it's uh open source i don't believe

00:20:03,840 --> 00:20:07,520
every aspect of that apollo styles open

00:20:05,760 --> 00:20:08,480
source and autoware is completely into

00:20:07,520 --> 00:20:12,320
an open source

00:20:08,480 --> 00:20:15,120
ottawa stack targets

00:20:12,320 --> 00:20:16,400
certain range of applications more like

00:20:15,120 --> 00:20:20,400
city driving

00:20:16,400 --> 00:20:23,679
kind of applications

00:20:20,400 --> 00:20:23,679
i hope that answers the question

00:20:28,400 --> 00:20:35,840
okay i think yang has already answered

00:20:30,320 --> 00:20:35,840
those questions here

00:20:39,919 --> 00:20:49,840
ok any other questions

00:21:10,000 --> 00:21:13,039
uh hi robbie can hear me yes i can hear

00:21:12,559 --> 00:21:15,919
you

00:21:13,039 --> 00:21:17,520
yeah i think there's a lot to is this is

00:21:15,919 --> 00:21:18,720
a lot of content that we condensed into

00:21:17,520 --> 00:21:21,679
a couple of slides

00:21:18,720 --> 00:21:22,080
so i thought people were gonna talk to

00:21:21,679 --> 00:21:24,559
that

00:21:22,080 --> 00:21:26,000
so just a proper question for christine

00:21:24,559 --> 00:21:27,840
so those slides

00:21:26,000 --> 00:21:30,480
a version of that will be available on

00:21:27,840 --> 00:21:32,960
the website right yes

00:21:30,480 --> 00:21:34,559
on the scad event site and then as well

00:21:32,960 --> 00:21:36,320
as the resource page

00:21:34,559 --> 00:21:38,000
okay yeah so once everybody is happy

00:21:36,320 --> 00:21:40,080
with that i think that so people

00:21:38,000 --> 00:21:41,120
will have more time to to look into

00:21:40,080 --> 00:21:42,720
because that then you know

00:21:41,120 --> 00:21:44,880
there's a lot of content actually you

00:21:42,720 --> 00:21:47,360
know jammed into a few slides here

00:21:44,880 --> 00:21:49,600
yeah yeah so i see a question uh

00:21:47,360 --> 00:21:52,000
regarding xta has been open source

00:21:49,600 --> 00:21:53,200
we have been uh open sourcing in phases

00:21:52,000 --> 00:21:57,440
so there's been

00:21:53,200 --> 00:22:00,000
uh upstream actually some of the um

00:21:57,440 --> 00:22:01,840
the there's been some parts of the logic

00:22:00,000 --> 00:22:05,039
that's already been upstream but

00:22:01,840 --> 00:22:07,919
some of the logic related to scheduling

00:22:05,039 --> 00:22:10,000
uh and hardware abstraction layers uh

00:22:07,919 --> 00:22:12,240
and graph splitting has not yet been

00:22:10,000 --> 00:22:14,480
uh upstream so we will be doing it all

00:22:12,240 --> 00:22:14,480
time

00:22:21,860 --> 00:22:27,280
[Music]

00:22:23,840 --> 00:22:30,400
so xta is both

00:22:27,280 --> 00:22:31,840
tbm and vta uh so um

00:22:30,400 --> 00:22:33,600
we are hoping to contribute so we

00:22:31,840 --> 00:22:37,120
already have contributed some things

00:22:33,600 --> 00:22:40,240
to our tvm but we are also hoping to

00:22:37,120 --> 00:22:42,080
add more uh the thing is the uh

00:22:40,240 --> 00:22:43,919
the amount of changes that's that we

00:22:42,080 --> 00:22:48,080
have made to support heterogeneity is

00:22:43,919 --> 00:22:49,919
quite a big uh quite a bit and

00:22:48,080 --> 00:22:55,840
we'll have to work with the maintainers

00:22:49,919 --> 00:22:55,840
there to see how we can upstream it

00:23:14,960 --> 00:23:18,400
just another note we have a slack

00:23:16,400 --> 00:23:20,840
channel for this session if anyone comes

00:23:18,400 --> 00:23:23,440
up with any questions later once they

00:23:20,840 --> 00:23:26,559
um have more time to think about it they

00:23:23,440 --> 00:23:26,559
can reach you on this left

00:23:34,840 --> 00:23:37,840
okay

00:24:01,840 --> 00:24:04,890
so this is the alternate port from

00:24:03,600 --> 00:24:06,080
mavnet

00:24:04,890 --> 00:24:19,840
[Music]

00:24:06,080 --> 00:24:19,840
that has the xilinx mpsocs

00:24:30,840 --> 00:24:33,840
my

00:24:40,880 --> 00:24:50,080
so the cameras that were used where the

00:24:44,960 --> 00:24:55,279
uh the on semi cameras right um

00:24:50,080 --> 00:24:55,279
and these are the jmsl cameras

00:24:56,080 --> 00:25:01,840
that were using the setup

00:25:12,840 --> 00:25:19,840
um

00:25:15,279 --> 00:25:22,320
okay so this next question is um

00:25:19,840 --> 00:25:25,440
was ai the only use case implemented or

00:25:22,320 --> 00:25:28,799
was navigation also implemented so ai

00:25:25,440 --> 00:25:32,000
um and then of course we are also doing

00:25:28,799 --> 00:25:36,320
image surround vision uh

00:25:32,000 --> 00:25:39,360
we're doing a bunch of scaling and other

00:25:36,320 --> 00:25:42,559
image processing on the gpu course

00:25:39,360 --> 00:25:43,679
and like i mentioned we want to extend

00:25:42,559 --> 00:25:45,600
it to

00:25:43,679 --> 00:25:46,799
more capabilities like localization and

00:25:45,600 --> 00:25:49,120
stuff so

00:25:46,799 --> 00:25:50,880
today that's done on the pcu but we

00:25:49,120 --> 00:25:56,799
would like to do that on the

00:25:50,880 --> 00:26:00,480
on the mps ocs uh

00:25:56,799 --> 00:26:04,400
do you rely on opencl for gpgpus

00:26:00,480 --> 00:26:04,880
um so the the molly is a molly 400 gpu

00:26:04,400 --> 00:26:08,240
course

00:26:04,880 --> 00:26:10,240
um we do not use open so

00:26:08,240 --> 00:26:13,600
we don't use opencl here it's just

00:26:10,240 --> 00:26:13,600
pretty much open gls

00:26:16,000 --> 00:26:20,400
all the opengs gls libraries

00:26:20,880 --> 00:26:25,840

YouTube URL: https://www.youtube.com/watch?v=GQ0FeHcFuwg


