Title: LVC20 210 Running accelerated Neural Networks using Python and Arm NN
Publication date: 2020-10-08
Playlist: Linaro Virtual Connect 2020
Description: 
	Arm NN is a neural network inference engine developed by Arm and the Linaro Machine Learning initiative connecting popular frameworks such as TensorFlow, TF Lite, ONNX, or Caffe with the CPU, GPU or NPU on your device using Arm NNâ€™s backends. The new Python interface called PyArmNN makes this even easier by enabling Python and all its modules.

The presentation takes you through the concepts of the Arm NN framework, dives deeper both into Python enablement, the usage, and into the different backends for acceleration on platforms such as Arm NEON, OpenCL, or having to develop a custom one.

https://connect.linaro.org/resources/lvc20/lvc20-210/
Captions: 
	00:00:00,480 --> 00:00:06,000
my name is pablo matsanauer and

00:00:03,760 --> 00:00:07,520
i'm a software engineer at nxp

00:00:06,000 --> 00:00:10,719
semiconductors

00:00:07,520 --> 00:00:14,080
located in the czech republic

00:00:10,719 --> 00:00:16,800
and i'm going to follow up on

00:00:14,080 --> 00:00:17,440
kevin's earlier presentation pretty much

00:00:16,800 --> 00:00:20,800
and

00:00:17,440 --> 00:00:21,840
talk about running accelerated neural

00:00:20,800 --> 00:00:25,920
networks

00:00:21,840 --> 00:00:27,199
using mainly pi armanin and give some

00:00:25,920 --> 00:00:31,840
examples

00:00:27,199 --> 00:00:31,840
around the new python interface

00:00:32,559 --> 00:00:35,840
okay so first of all i'm going to

00:00:34,079 --> 00:00:39,920
introduce armand so this is

00:00:35,840 --> 00:00:42,879
going to be somewhat a recap

00:00:39,920 --> 00:00:45,680
maybe introduce a little how it's

00:00:42,879 --> 00:00:48,800
interconnected with linaro

00:00:45,680 --> 00:00:49,200
afterwards i'm going to dive deeper into

00:00:48,800 --> 00:00:51,840
the

00:00:49,200 --> 00:00:53,680
python interface which was introduced in

00:00:51,840 --> 00:00:59,680
uh

00:00:53,680 --> 00:01:02,960
in the i can actually hide this

00:00:59,680 --> 00:01:04,000
okay uh yeah uh then afterwards i'm

00:01:02,960 --> 00:01:06,400
going to

00:01:04,000 --> 00:01:07,600
introduce more or dive deeper into the

00:01:06,400 --> 00:01:09,920
python interface

00:01:07,600 --> 00:01:11,119
which was introduced in the 2005 release

00:01:09,920 --> 00:01:14,400
that means around

00:01:11,119 --> 00:01:16,479
three months ago and explain

00:01:14,400 --> 00:01:18,479
more about how it was developed for

00:01:16,479 --> 00:01:21,920
those who are interested in

00:01:18,479 --> 00:01:24,080
python wrapping and then and at the end

00:01:21,920 --> 00:01:26,960
i'm going to talk about

00:01:24,080 --> 00:01:27,680
acceleration of neural networks and how

00:01:26,960 --> 00:01:30,000
to do that

00:01:27,680 --> 00:01:31,200
through through uh rmn and back-ends

00:01:30,000 --> 00:01:34,720
which are basically

00:01:31,200 --> 00:01:36,079
the enabler for that and so also explain

00:01:34,720 --> 00:01:38,000
a bit

00:01:36,079 --> 00:01:41,840
about how to develop your own

00:01:38,000 --> 00:01:41,840
third-party back-end

00:01:42,799 --> 00:01:47,280
okay so this is going to be a recap so

00:01:45,759 --> 00:01:49,360
what is our man

00:01:47,280 --> 00:01:51,439
uh armand is a middleware inference

00:01:49,360 --> 00:01:52,079
engine for machine learning on the edge

00:01:51,439 --> 00:01:55,200
so

00:01:52,079 --> 00:01:58,719
i'd like to stress the world the word

00:01:55,200 --> 00:02:01,759
middleware here because

00:01:58,719 --> 00:02:05,360
it connects it connects your

00:02:01,759 --> 00:02:06,240
high-level high-level machine learning

00:02:05,360 --> 00:02:11,920
framework

00:02:06,240 --> 00:02:14,720
such as tensorflow tensorflow lite o nx

00:02:11,920 --> 00:02:15,200
bionics i mean the onan x format so that

00:02:14,720 --> 00:02:18,879
can be

00:02:15,200 --> 00:02:22,480
uh exported for example through

00:02:18,879 --> 00:02:24,560
pytorch or mxnet or cafe so you have

00:02:22,480 --> 00:02:25,840
all these high-level frameworks you

00:02:24,560 --> 00:02:29,280
pretty much train your

00:02:25,840 --> 00:02:32,000
uh train your neural

00:02:29,280 --> 00:02:33,040
model there you export it to your

00:02:32,000 --> 00:02:36,400
preferred format

00:02:33,040 --> 00:02:39,120
so for example tensorflow lite is

00:02:36,400 --> 00:02:40,800
a very popular format used used on

00:02:39,120 --> 00:02:44,400
embedded lately

00:02:40,800 --> 00:02:47,040
and then you load it into rmn

00:02:44,400 --> 00:02:48,239
armand and optimize optimizes the model

00:02:47,040 --> 00:02:51,440
and

00:02:48,239 --> 00:02:55,360
delegates it to whatever software

00:02:51,440 --> 00:02:56,000
uh to whatever software at the opposite

00:02:55,360 --> 00:02:58,879
end so

00:02:56,000 --> 00:02:59,840
it can be our compute library which

00:02:58,879 --> 00:03:02,000
accelerates

00:02:59,840 --> 00:03:03,680
uh your model through the neon

00:03:02,000 --> 00:03:06,239
instruction set or

00:03:03,680 --> 00:03:07,840
through opencl it can be the ethos and

00:03:06,239 --> 00:03:12,080
npu driver

00:03:07,840 --> 00:03:15,519
it can be your own third-party driver

00:03:12,080 --> 00:03:17,360
well pretty much any software yeah so

00:03:15,519 --> 00:03:19,440
it doesn't even have to be directly a

00:03:17,360 --> 00:03:24,400
driver it can be

00:03:19,440 --> 00:03:24,400
some high level compute engine

00:03:25,519 --> 00:03:32,000
so that's basically where uh

00:03:28,959 --> 00:03:34,640
where armanine stands there is also

00:03:32,000 --> 00:03:35,519
an android hull interface available so

00:03:34,640 --> 00:03:39,280
if

00:03:35,519 --> 00:03:43,120
you want to use it on android you can

00:03:39,280 --> 00:03:45,200
uh it is actually optimize uh

00:03:43,120 --> 00:03:46,480
the benefits of using armand compared

00:03:45,200 --> 00:03:49,360
let's say to other

00:03:46,480 --> 00:03:51,000
frameworks uh would be mainly that it is

00:03:49,360 --> 00:03:53,280
optimized for

00:03:51,000 --> 00:03:54,080
cortex-a-cpus through the rm compute

00:03:53,280 --> 00:03:57,120
library

00:03:54,080 --> 00:03:58,879
it is optimized for the mali gpus again

00:03:57,120 --> 00:04:01,120
through the arm compute library but

00:03:58,879 --> 00:04:04,640
through the opencl interface

00:04:01,120 --> 00:04:07,680
so you can even use it on

00:04:04,640 --> 00:04:08,640
other devices it's the only thing you

00:04:07,680 --> 00:04:11,920
need is

00:04:08,640 --> 00:04:14,239
opencl and a few

00:04:11,920 --> 00:04:17,120
a few hard requirements for that it has

00:04:14,239 --> 00:04:19,919
to be open clo 1.1

00:04:17,120 --> 00:04:22,479
and you need a few extensions but i'm

00:04:19,919 --> 00:04:25,360
going to talk about it later

00:04:22,479 --> 00:04:27,440
it is also optimized for ethos and mpus

00:04:25,360 --> 00:04:28,080
actually the ethos and back-end is not

00:04:27,440 --> 00:04:32,240
included

00:04:28,080 --> 00:04:32,240
directly in the arma nano repository so

00:04:32,320 --> 00:04:36,880
if you want that and you have ethosan

00:04:35,280 --> 00:04:39,440
available on your device

00:04:36,880 --> 00:04:40,560
you have to pull it from a separate

00:04:39,440 --> 00:04:44,080
repository

00:04:40,560 --> 00:04:47,360
i have as well life links on other side

00:04:44,080 --> 00:04:51,400
slides and the last thing is

00:04:47,360 --> 00:04:54,400
optimized for is the idata x8

00:04:51,400 --> 00:04:57,160
microprocessors so

00:04:54,400 --> 00:04:58,320
those are a those are multimedia

00:04:57,160 --> 00:05:01,759
microprocessors

00:04:58,320 --> 00:05:03,280
for some are targeted for machine

00:05:01,759 --> 00:05:06,400
learning some are targeted for

00:05:03,280 --> 00:05:10,080
graphics or displays so these also

00:05:06,400 --> 00:05:13,199
have cortex-a cpus and

00:05:10,080 --> 00:05:16,160
they have a dedicated npu which

00:05:13,199 --> 00:05:17,919
has a third-party driver as i mentioned

00:05:16,160 --> 00:05:24,560
here available

00:05:17,919 --> 00:05:28,000
for armand so that enables acceleration

00:05:24,560 --> 00:05:31,199
okay so uh how it's um how is rmnn uh

00:05:28,000 --> 00:05:32,639
interconnected with lenaro so originally

00:05:31,199 --> 00:05:36,000
it was all managed by arm

00:05:32,639 --> 00:05:38,000
and then in uh mid 280 2018

00:05:36,000 --> 00:05:39,280
it was donated to the linaro ai

00:05:38,000 --> 00:05:42,000
initiative

00:05:39,280 --> 00:05:43,120
so around uh two years ago and since

00:05:42,000 --> 00:05:46,479
then it is still

00:05:43,120 --> 00:05:47,039
mostly developed by by arm there's quite

00:05:46,479 --> 00:05:50,479
a lot of

00:05:47,039 --> 00:05:53,600
developers dedicated to that so uh

00:05:50,479 --> 00:05:56,479
if you want to erase a pull request

00:05:53,600 --> 00:05:56,800
you can it's pretty easy everything is

00:05:56,479 --> 00:06:00,400
done

00:05:56,800 --> 00:06:04,160
on ml platform through

00:06:00,400 --> 00:06:06,560
through garrett so you just

00:06:04,160 --> 00:06:07,680
pull the codes edit it push it

00:06:06,560 --> 00:06:11,120
afterwards

00:06:07,680 --> 00:06:14,800
and the only thing you need apart

00:06:11,120 --> 00:06:17,039
from uh signing

00:06:14,800 --> 00:06:19,120
signing you contributed so apart from

00:06:17,039 --> 00:06:20,000
that you just need a plus two from one

00:06:19,120 --> 00:06:24,400
of the arm

00:06:20,000 --> 00:06:28,000
core developers and when that's done

00:06:24,400 --> 00:06:29,360
you you may have your you may have your

00:06:28,000 --> 00:06:33,520
contribution up in the

00:06:29,360 --> 00:06:35,600
master branch it is released quarterly

00:06:33,520 --> 00:06:37,600
on github and all the releases are

00:06:35,600 --> 00:06:38,639
actually synchronized with our compute

00:06:37,600 --> 00:06:42,000
library

00:06:38,639 --> 00:06:45,280
which is somewhat a hard dependency for

00:06:42,000 --> 00:06:48,000
armand because well without

00:06:45,280 --> 00:06:50,160
without the let's say base back ends

00:06:48,000 --> 00:06:54,080
opencl and neon

00:06:50,160 --> 00:06:56,400
it wouldn't be all that useful so uh

00:06:54,080 --> 00:06:57,360
though those two are those two are

00:06:56,400 --> 00:07:01,039
released

00:06:57,360 --> 00:07:03,120
in sync for any discussions

00:07:01,039 --> 00:07:04,960
report box or maybe even to start a

00:07:03,120 --> 00:07:08,880
discussion so i would

00:07:04,960 --> 00:07:11,919
i would suggest to use github and

00:07:08,880 --> 00:07:15,919
anyone anyone

00:07:11,919 --> 00:07:16,720
from arm or even from the community may

00:07:15,919 --> 00:07:20,080
may respond

00:07:16,720 --> 00:07:20,080
or respond to your requests

00:07:20,720 --> 00:07:28,160
so uh now something about pi armanin

00:07:24,880 --> 00:07:30,639
so pi armanin was the interface

00:07:28,160 --> 00:07:32,880
python interface which was added in the

00:07:30,639 --> 00:07:35,840
2005 release

00:07:32,880 --> 00:07:38,319
as mentioned by kevin it's it's only a

00:07:35,840 --> 00:07:42,080
wrapper interface

00:07:38,319 --> 00:07:44,479
around around the armanin dynamic

00:07:42,080 --> 00:07:48,240
library so on the background

00:07:44,479 --> 00:07:51,520
it runs all the c plus plus

00:07:48,240 --> 00:07:55,039
compute or it like uh

00:07:51,520 --> 00:07:57,759
it runs the dynamic library and

00:07:55,039 --> 00:07:58,720
on only on the front-end it exposes the

00:07:57,759 --> 00:08:04,400
python

00:07:58,720 --> 00:08:07,120
so there is no additional compute or

00:08:04,400 --> 00:08:08,160
there is very very little overhead done

00:08:07,120 --> 00:08:09,599
by python

00:08:08,160 --> 00:08:12,479
there are actually a few convenient

00:08:09,599 --> 00:08:13,120
functions i'm going to show a few for

00:08:12,479 --> 00:08:16,479
example

00:08:13,120 --> 00:08:19,840
coping copying data to uh

00:08:16,479 --> 00:08:21,520
numpy arrays but that's uh

00:08:19,840 --> 00:08:22,960
basically just a convenience for the

00:08:21,520 --> 00:08:26,479
developer it's

00:08:22,960 --> 00:08:28,560
they don't implement any compute

00:08:26,479 --> 00:08:30,400
the swig project was used to generate

00:08:28,560 --> 00:08:31,919
wrappers so i'm going to talk about that

00:08:30,400 --> 00:08:35,519
later

00:08:31,919 --> 00:08:36,800
and also in case someone would be

00:08:35,519 --> 00:08:40,240
interested in

00:08:36,800 --> 00:08:42,719
using legacy versions so using pi

00:08:40,240 --> 00:08:44,800
armanin uh before it was actually

00:08:42,719 --> 00:08:48,480
published in 2005

00:08:44,800 --> 00:08:51,760
there is a on github there is a

00:08:48,480 --> 00:08:55,760
project by nxp which

00:08:51,760 --> 00:09:00,399
provides a standalone stand

00:08:55,760 --> 00:09:02,800
extendable package using which you can

00:09:00,399 --> 00:09:04,560
build your wrappers the difference is

00:09:02,800 --> 00:09:05,120
that it is completely a standalone

00:09:04,560 --> 00:09:08,000
project

00:09:05,120 --> 00:09:09,440
it doesn't build armand on its own so

00:09:08,000 --> 00:09:13,680
you actually have to come with

00:09:09,440 --> 00:09:17,360
pre-compiled libraries and header files

00:09:13,680 --> 00:09:20,160
to which you want to link it to and

00:09:17,360 --> 00:09:21,120
then run either the standalone scripts

00:09:20,160 --> 00:09:24,959
or

00:09:21,120 --> 00:09:28,320
run cmake and it will do all the

00:09:24,959 --> 00:09:31,600
python magic and output either

00:09:28,320 --> 00:09:32,720
a binary package a wheel which is

00:09:31,600 --> 00:09:35,760
platform specific

00:09:32,720 --> 00:09:36,720
or a source package which is spot for

00:09:35,760 --> 00:09:40,000
independence

00:09:36,720 --> 00:09:43,120
the difference really is that

00:09:40,000 --> 00:09:44,880
if when you build a binary package

00:09:43,120 --> 00:09:46,640
it works on the machine you are building

00:09:44,880 --> 00:09:49,120
in on or

00:09:46,640 --> 00:09:51,519
if you are cross compiling it on the

00:09:49,120 --> 00:09:53,279
machine you are cross compiling it for

00:09:51,519 --> 00:09:55,519
and if you are building a source package

00:09:53,279 --> 00:09:58,560
it kind of only contains mostly the

00:09:55,519 --> 00:10:00,480
source codes so afterwards when you

00:09:58,560 --> 00:10:02,240
install it on your target machine it

00:10:00,480 --> 00:10:05,519
compiles there so

00:10:02,240 --> 00:10:09,040
you need to have a compiler like gcc or

00:10:05,519 --> 00:10:12,240
something like that available yeah

00:10:09,040 --> 00:10:15,040
so uh on the right uh

00:10:12,240 --> 00:10:15,680
you can see this is actually applicable

00:10:15,040 --> 00:10:18,480
for both

00:10:15,680 --> 00:10:18,959
for the standalone project and for arma

00:10:18,480 --> 00:10:21,279
nan

00:10:18,959 --> 00:10:23,279
how to build it so use either using

00:10:21,279 --> 00:10:24,800
cmake you just add one options whether

00:10:23,279 --> 00:10:28,079
you want to build the wheel

00:10:24,800 --> 00:10:29,200
and and or the source package you can

00:10:28,079 --> 00:10:31,279
build both

00:10:29,200 --> 00:10:34,160
uh the same for the standalone project

00:10:31,279 --> 00:10:37,680
or you can do it completely stand-alone

00:10:34,160 --> 00:10:40,720
so in the pi armand folder

00:10:37,680 --> 00:10:42,240
python pyraminan there are scripts

00:10:40,720 --> 00:10:45,519
available there

00:10:42,240 --> 00:10:48,720
and you may pretty much

00:10:45,519 --> 00:10:51,440
run them one by one and

00:10:48,720 --> 00:10:52,640
use exports along the way and it will do

00:10:51,440 --> 00:10:54,640
the same thing you have much more

00:10:52,640 --> 00:11:01,440
control over it

00:10:54,640 --> 00:11:04,800
but well you are doing it manually

00:11:01,440 --> 00:11:06,880
so now about python wrapping

00:11:04,800 --> 00:11:08,000
this might be interesting for those of

00:11:06,880 --> 00:11:11,440
you who are

00:11:08,000 --> 00:11:14,320
uh because generally

00:11:11,440 --> 00:11:15,440
there is a lot of c bus libraries

00:11:14,320 --> 00:11:20,000
available

00:11:15,440 --> 00:11:23,600
and it's nice to interface with python

00:11:20,000 --> 00:11:26,079
so the advantages of python write are

00:11:23,600 --> 00:11:26,800
that you just install your modules using

00:11:26,079 --> 00:11:31,040
pip

00:11:26,800 --> 00:11:34,399
you have all those uh math math modules

00:11:31,040 --> 00:11:34,399
analytics modules

00:11:34,560 --> 00:11:40,000
even like graphs matplotlib whatever

00:11:38,240 --> 00:11:41,600
and you don't really have those for c

00:11:40,000 --> 00:11:44,640
plus or at least it's

00:11:41,600 --> 00:11:49,519
it's a pain to integrate them into c

00:11:44,640 --> 00:11:52,720
plus so that's where python kicks in

00:11:49,519 --> 00:11:55,600
and if you have a c plus plus library

00:11:52,720 --> 00:11:58,000
which is very robust and high performing

00:11:55,600 --> 00:12:00,639
such as armand

00:11:58,000 --> 00:12:02,480
then it's nice to have a python

00:12:00,639 --> 00:12:04,240
interface for it

00:12:02,480 --> 00:12:05,839
one of the options to generate an

00:12:04,240 --> 00:12:08,720
interface is sweet there are

00:12:05,839 --> 00:12:10,000
of course other options depending on on

00:12:08,720 --> 00:12:12,720
mostly on the size of your

00:12:10,000 --> 00:12:14,560
project uh myself i have experience with

00:12:12,720 --> 00:12:17,680
boost python here

00:12:14,560 --> 00:12:19,519
so if i would compare the two uh

00:12:17,680 --> 00:12:20,800
i would suggest for example to use boost

00:12:19,519 --> 00:12:23,839
python with

00:12:20,800 --> 00:12:26,720
bigger projects yeah uh

00:12:23,839 --> 00:12:27,519
when you really have tons uh tons of

00:12:26,720 --> 00:12:29,360
code and

00:12:27,519 --> 00:12:31,440
you know that you will be doing tons of

00:12:29,360 --> 00:12:33,600
modifications to your python wrappers

00:12:31,440 --> 00:12:35,920
i would maybe go with boost python

00:12:33,600 --> 00:12:40,320
otherwise swake is also a

00:12:35,920 --> 00:12:40,880
nice option uh swik also allows to

00:12:40,320 --> 00:12:43,600
create an

00:12:40,880 --> 00:12:45,440
interface for other languages not only

00:12:43,600 --> 00:12:50,079
not only python but

00:12:45,440 --> 00:12:52,720
javascript perl php ruby

00:12:50,079 --> 00:12:54,079
a lot easier for ajs so it's even

00:12:52,720 --> 00:12:57,920
integrated into

00:12:54,079 --> 00:12:59,120
into uh standard python modules such as

00:12:57,920 --> 00:13:03,279
setup tools

00:12:59,120 --> 00:13:06,480
you have options there for smake so

00:13:03,279 --> 00:13:07,680
it's really no rocket science and

00:13:06,480 --> 00:13:11,600
there's tons of

00:13:07,680 --> 00:13:14,480
documentation there in order to

00:13:11,600 --> 00:13:16,320
write your python wrapper you basically

00:13:14,480 --> 00:13:19,360
need just two things

00:13:16,320 --> 00:13:22,160
the first is to

00:13:19,360 --> 00:13:23,440
expose the interface so you pretty much

00:13:22,160 --> 00:13:27,360
need to define

00:13:23,440 --> 00:13:30,800
what you want to export you do that

00:13:27,360 --> 00:13:33,920
using sweet template they have their own

00:13:30,800 --> 00:13:36,800
language the language language

00:13:33,920 --> 00:13:37,360
is described in the documentation so

00:13:36,800 --> 00:13:40,560
well

00:13:37,360 --> 00:13:44,320
you just have to go through that and

00:13:40,560 --> 00:13:47,279
define everything you want to

00:13:44,320 --> 00:13:48,480
you want to expose the basic thing

00:13:47,279 --> 00:13:51,920
really is

00:13:48,480 --> 00:13:53,600
just to include uh include header files

00:13:51,920 --> 00:13:56,320
yeah because it takes it takes the

00:13:53,600 --> 00:14:00,000
header files uh

00:13:56,320 --> 00:14:04,880
and uh based on the header files

00:14:00,000 --> 00:14:04,880
it generates the wrappers and

00:14:05,920 --> 00:14:12,959
yeah just like that

00:14:09,360 --> 00:14:15,199
and one thing you

00:14:12,959 --> 00:14:17,120
need to do a lot while developing uh

00:14:15,199 --> 00:14:19,279
python wrappers is for example memory

00:14:17,120 --> 00:14:22,320
management so

00:14:19,279 --> 00:14:23,839
that might get tricky because python

00:14:22,320 --> 00:14:27,760
uses garbage collection

00:14:23,839 --> 00:14:31,040
c plus plus well

00:14:27,760 --> 00:14:32,720
depends how you implemented it but

00:14:31,040 --> 00:14:36,079
it doesn't have a garbage collector you

00:14:32,720 --> 00:14:36,079
have smart pointers and

00:14:36,320 --> 00:14:40,959
all funky stuff but generally you need

00:14:39,440 --> 00:14:44,000
to be aware of

00:14:40,959 --> 00:14:46,959
how memory is handled and in case you

00:14:44,000 --> 00:14:48,800
know that if the python wrapper wouldn't

00:14:46,959 --> 00:14:49,360
would create a memory leak you actually

00:14:48,800 --> 00:14:53,199
have to

00:14:49,360 --> 00:14:56,240
take care of that in the python wrapper

00:14:53,199 --> 00:14:59,279
so that's one thing you need to do while

00:14:56,240 --> 00:15:02,000
extending the python interface

00:14:59,279 --> 00:15:03,440
uh another file you need to modify is

00:15:02,000 --> 00:15:06,320
the setup pi

00:15:03,440 --> 00:15:06,800
uh which is the file which builds your

00:15:06,320 --> 00:15:10,000
uh

00:15:06,800 --> 00:15:13,360
python package so either reveal or

00:15:10,000 --> 00:15:16,880
or your source package and

00:15:13,360 --> 00:15:18,240
you need to add a setup tools extension

00:15:16,880 --> 00:15:21,839
there

00:15:18,240 --> 00:15:24,079
where you specify templates

00:15:21,839 --> 00:15:27,279
which export the interface and you

00:15:24,079 --> 00:15:30,399
specify a dynamic library

00:15:27,279 --> 00:15:33,759
so then afterwards when you run the

00:15:30,399 --> 00:15:33,759
when you run the generation

00:15:34,720 --> 00:15:41,360
then it basically knows which

00:15:38,000 --> 00:15:44,399
which binding connects

00:15:41,360 --> 00:15:47,600
to which call in the

00:15:44,399 --> 00:15:47,600
in the dynamic library

00:15:48,240 --> 00:15:54,639
so that's what you specify there

00:15:51,600 --> 00:15:57,199
also when if you would be developing

00:15:54,639 --> 00:15:58,639
a python wrapper or specif specifically

00:15:57,199 --> 00:16:01,920
pyramin and here

00:15:58,639 --> 00:16:03,759
sphig4 was used there so that's

00:16:01,920 --> 00:16:06,320
typically not part of the

00:16:03,759 --> 00:16:07,600
debian packages which you can install

00:16:06,320 --> 00:16:10,240
there's week 3

00:16:07,600 --> 00:16:12,240
so you need to compile that from sources

00:16:10,240 --> 00:16:13,600
typically so for that there is speak

00:16:12,240 --> 00:16:16,480
executable

00:16:13,600 --> 00:16:20,560
option if you have multiple versions of

00:16:16,480 --> 00:16:22,720
zwick available on your computer so

00:16:20,560 --> 00:16:25,199
and it would for some reason it would

00:16:22,720 --> 00:16:25,199
conflict

00:16:25,360 --> 00:16:30,399
so uh here is an example a code example

00:16:29,519 --> 00:16:33,519
kevin already

00:16:30,399 --> 00:16:34,399
had a very similar one so i'm going to

00:16:33,519 --> 00:16:37,680
go through it

00:16:34,399 --> 00:16:38,880
very quickly the interface looks very

00:16:37,680 --> 00:16:42,399
similar to

00:16:38,880 --> 00:16:45,120
uh to the c plus plus api because it's

00:16:42,399 --> 00:16:45,680
generated from the header files so even

00:16:45,120 --> 00:16:47,759
the

00:16:45,680 --> 00:16:50,079
functions themselves they are called the

00:16:47,759 --> 00:16:52,639
same like create sensor from binary

00:16:50,079 --> 00:16:54,000
file it would be the same in the c plus

00:16:52,639 --> 00:16:56,000
plus interface

00:16:54,000 --> 00:16:57,440
so the difference here is that i'm using

00:16:56,000 --> 00:17:01,360
a tf light pass

00:16:57,440 --> 00:17:04,959
parser not the onx parser and

00:17:01,360 --> 00:17:07,520
i need to load my tflight model

00:17:04,959 --> 00:17:09,600
there might be slight differences in

00:17:07,520 --> 00:17:11,679
individual parsers for example the own x

00:17:09,600 --> 00:17:14,799
parser or

00:17:11,679 --> 00:17:17,360
cafe parser etc you need to specify

00:17:14,799 --> 00:17:20,000
input output sensors

00:17:17,360 --> 00:17:22,319
lite has this encoded within the model

00:17:20,000 --> 00:17:26,000
so you actually don't need to

00:17:22,319 --> 00:17:28,640
specify the tensors when when you run

00:17:26,000 --> 00:17:30,840
inference then you initialize the

00:17:28,640 --> 00:17:34,799
runtime

00:17:30,840 --> 00:17:36,720
and you can see uh afterwards there is

00:17:34,799 --> 00:17:38,640
preferred back-ends where you specify

00:17:36,720 --> 00:17:40,880
what back-ends you want to use

00:17:38,640 --> 00:17:43,360
so now uh that's something to remember

00:17:40,880 --> 00:17:46,400
you can use multiple backhands

00:17:43,360 --> 00:17:49,919
in arman where

00:17:46,400 --> 00:17:52,160
they they are being run depending

00:17:49,919 --> 00:17:53,679
on layer support so if something's not

00:17:52,160 --> 00:17:56,320
supported it will

00:17:53,679 --> 00:17:57,120
fall back to the uh to the next back end

00:17:56,320 --> 00:18:00,960
which

00:17:57,120 --> 00:18:01,440
is a nice feature and afterwards after

00:18:00,960 --> 00:18:05,200
you have

00:18:01,440 --> 00:18:06,720
all this set up uh rman you may run the

00:18:05,200 --> 00:18:10,559
optimize function

00:18:06,720 --> 00:18:13,760
what optimize does it basically

00:18:10,559 --> 00:18:18,160
checks all the back ends check all the

00:18:13,760 --> 00:18:21,200
layers optimizes optimizes the runtime

00:18:18,160 --> 00:18:24,240
and the backends themselves they create

00:18:21,200 --> 00:18:26,840
something called workloads those

00:18:24,240 --> 00:18:30,799
workloads they are like

00:18:26,840 --> 00:18:34,240
tiny well how to describe it like

00:18:30,799 --> 00:18:37,520
black boxes which

00:18:34,240 --> 00:18:40,240
take care of the

00:18:37,520 --> 00:18:41,120
layer runtime so if you have a layer

00:18:40,240 --> 00:18:43,600
it's

00:18:41,120 --> 00:18:45,440
might be for example convolution that's

00:18:43,600 --> 00:18:46,080
an operation right but the workload

00:18:45,440 --> 00:18:50,080
itself

00:18:46,080 --> 00:18:53,280
is uh is the virg horse for that so

00:18:50,080 --> 00:18:54,960
that's something which is created by the

00:18:53,280 --> 00:18:57,679
back ends and then afterwards all the

00:18:54,960 --> 00:18:59,520
afterwards when all the uh workloads are

00:18:57,679 --> 00:19:03,440
created they are chained together

00:18:59,520 --> 00:19:08,559
and armand runs inference

00:19:03,440 --> 00:19:11,919
uh inference on the chain of workloads

00:19:08,559 --> 00:19:13,280
okay so uh now on the second part of the

00:19:11,919 --> 00:19:16,960
code example

00:19:13,280 --> 00:19:19,280
uh you may load the image you may use

00:19:16,960 --> 00:19:22,840
because it's python it's really easy so

00:19:19,280 --> 00:19:26,160
you may use opencv for that

00:19:22,840 --> 00:19:29,120
or numpy to

00:19:26,160 --> 00:19:29,840
to generate a random random input or

00:19:29,120 --> 00:19:32,080
whatever

00:19:29,840 --> 00:19:33,200
you may use your favorite uh video

00:19:32,080 --> 00:19:36,840
library or

00:19:33,200 --> 00:19:40,400
interface with your uh let's say video

00:19:36,840 --> 00:19:43,679
camera through python interface so

00:19:40,400 --> 00:19:45,520
it's really nice to use python here

00:19:43,679 --> 00:19:47,840
you specify the input tensors and load

00:19:45,520 --> 00:19:49,039
the image into the input tensors

00:19:47,840 --> 00:19:52,160
there can of course there can be

00:19:49,039 --> 00:19:54,400
multiple inputs for a neural network

00:19:52,160 --> 00:19:55,840
you specify the output so again there

00:19:54,400 --> 00:19:58,400
can be multiple outputs

00:19:55,840 --> 00:19:59,600
you have to bind that and afterwards

00:19:58,400 --> 00:20:02,080
when you run inference

00:19:59,600 --> 00:20:03,120
you will find your result in in the

00:20:02,080 --> 00:20:06,080
output tensors

00:20:03,120 --> 00:20:07,840
you run inference that's the enqueue

00:20:06,080 --> 00:20:11,280
workload function

00:20:07,840 --> 00:20:13,440
and then there's a example which shows

00:20:11,280 --> 00:20:16,400
that there are a few convenient

00:20:13,440 --> 00:20:16,810
functions implemented in the python api

00:20:16,400 --> 00:20:18,159
so

00:20:16,810 --> 00:20:21,360
[Music]

00:20:18,159 --> 00:20:25,039
output sensors would normally be

00:20:21,360 --> 00:20:28,000
a very ugly tensor shape

00:20:25,039 --> 00:20:30,080
object which would be pretty annoying to

00:20:28,000 --> 00:20:33,360
work with so

00:20:30,080 --> 00:20:35,600
there is a function which which

00:20:33,360 --> 00:20:36,880
copies this to a numpy array so

00:20:35,600 --> 00:20:40,799
afterwards you can

00:20:36,880 --> 00:20:43,919
work with that and and

00:20:40,799 --> 00:20:45,360
post-process it for whatever purpose on

00:20:43,919 --> 00:20:48,640
the right there is

00:20:45,360 --> 00:20:51,919
a cat so typically what us

00:20:48,640 --> 00:20:55,120
ml engineers do we just detect

00:20:51,919 --> 00:20:58,240
uh cats all day sometimes dogs

00:20:55,120 --> 00:21:01,600
and this is an example

00:20:58,240 --> 00:21:03,760
from uh from the paya armanin repository

00:21:01,600 --> 00:21:07,039
or from the armenian repository

00:21:03,760 --> 00:21:10,559
uh for uh pi armand where

00:21:07,039 --> 00:21:13,840
uh if you run the example

00:21:10,559 --> 00:21:15,039
uh it will download a cat example from

00:21:13,840 --> 00:21:18,080
the internet

00:21:15,039 --> 00:21:20,080
and it will output something like below

00:21:18,080 --> 00:21:23,919
that it's a tabby

00:21:20,080 --> 00:21:24,400
and it pretty much looks like it at

00:21:23,919 --> 00:21:30,080
least

00:21:24,400 --> 00:21:33,280
based on the definition on wikipedia

00:21:30,080 --> 00:21:34,000
okay um so at the end now something

00:21:33,280 --> 00:21:37,039
about

00:21:34,000 --> 00:21:38,320
the armin and backhands so uh the

00:21:37,039 --> 00:21:40,799
backhands they are pretty much an

00:21:38,320 --> 00:21:42,080
abstraction which connects connect

00:21:40,799 --> 00:21:46,000
layers of your

00:21:42,080 --> 00:21:50,080
uh of your neural network to

00:21:46,000 --> 00:21:53,679
the the hardware on the opposite end so

00:21:50,080 --> 00:21:56,320
uh that is a

00:21:53,679 --> 00:21:57,520
there are two available or there are

00:21:56,320 --> 00:22:00,400
three available

00:21:57,520 --> 00:22:01,600
in the basic arm and repository in plus

00:22:00,400 --> 00:22:04,080
one

00:22:01,600 --> 00:22:05,600
and you also have other repositories

00:22:04,080 --> 00:22:08,880
with third party back-ends

00:22:05,600 --> 00:22:12,240
so the three are there is opencl neon

00:22:08,880 --> 00:22:14,480
and the reference backend opencl is the

00:22:12,240 --> 00:22:16,320
specialized backend through

00:22:14,480 --> 00:22:18,720
arm compute library which enables

00:22:16,320 --> 00:22:21,440
acceleration on multi gpus

00:22:18,720 --> 00:22:22,240
the neon backend again uses arm compute

00:22:21,440 --> 00:22:26,559
library

00:22:22,240 --> 00:22:28,640
but accelerates on anything cortex a

00:22:26,559 --> 00:22:30,159
then and then there is the reference

00:22:28,640 --> 00:22:33,520
backend which is pretty much used

00:22:30,159 --> 00:22:33,520
just for testing or

00:22:34,080 --> 00:22:41,120
testing and as i default fallback so if

00:22:37,520 --> 00:22:43,280
if you're if your operation wouldn't be

00:22:41,120 --> 00:22:44,640
implemented then it will fall back to

00:22:43,280 --> 00:22:46,720
that one

00:22:44,640 --> 00:22:48,240
and by plus one i meant the ethos and

00:22:46,720 --> 00:22:51,600
back end so

00:22:48,240 --> 00:22:53,280
there is a separate repository from arm

00:22:51,600 --> 00:22:56,640
which contains this back end and you

00:22:53,280 --> 00:22:59,120
have to integrate it separately

00:22:56,640 --> 00:23:01,039
it is also pretty easy to implement your

00:22:59,120 --> 00:23:03,919
own back-end so

00:23:01,039 --> 00:23:05,520
i'm going to talk about it very quickly

00:23:03,919 --> 00:23:09,679
in just a minute

00:23:05,520 --> 00:23:14,000
so just an example of the neon back-end

00:23:09,679 --> 00:23:16,559
so in order to run a model inference

00:23:14,000 --> 00:23:17,760
you need acceleration no question there

00:23:16,559 --> 00:23:21,039
really

00:23:17,760 --> 00:23:23,600
so if if you would run the

00:23:21,039 --> 00:23:24,559
reference backend which is a single

00:23:23,600 --> 00:23:26,960
threaded

00:23:24,559 --> 00:23:29,039
c plus plus implementation it would take

00:23:26,960 --> 00:23:32,240
77 milliseconds to

00:23:29,039 --> 00:23:34,240
run inference of a popular model called

00:23:32,240 --> 00:23:37,440
mobilenet v1

00:23:34,240 --> 00:23:40,559
whereas if we do that on cpu uh

00:23:37,440 --> 00:23:44,240
through the neon optimized backend it

00:23:40,559 --> 00:23:47,360
takes on four a 53 uh

00:23:44,240 --> 00:23:48,000
cores with neon it takes 93 milliseconds

00:23:47,360 --> 00:23:51,120
so that's

00:23:48,000 --> 00:23:56,240
800 times faster so really this is

00:23:51,120 --> 00:23:56,240
uh you cannot even compare those two

00:23:57,279 --> 00:24:01,120
okay uh so as i mentioned hybrid

00:24:00,240 --> 00:24:04,960
execution

00:24:01,120 --> 00:24:05,760
so hybrid execution is a feature of

00:24:04,960 --> 00:24:08,000
armand

00:24:05,760 --> 00:24:08,960
where because you have multiple backends

00:24:08,000 --> 00:24:12,880
available

00:24:08,960 --> 00:24:15,360
you are you are and you want to support

00:24:12,880 --> 00:24:18,559
all the operations and all the layers

00:24:15,360 --> 00:24:21,360
uh within your model it acts

00:24:18,559 --> 00:24:21,840
and you can specify multiple backends

00:24:21,360 --> 00:24:24,960
then

00:24:21,840 --> 00:24:26,720
you can run the whole model even if

00:24:24,960 --> 00:24:29,360
those layers are supported only

00:24:26,720 --> 00:24:31,279
partially so here is an example of the

00:24:29,360 --> 00:24:33,279
vsi npu backend which is

00:24:31,279 --> 00:24:35,039
an accelerated back-end for itemix id

00:24:33,279 --> 00:24:38,480
devices

00:24:35,039 --> 00:24:40,000
and this back-end

00:24:38,480 --> 00:24:42,080
it actually supports more operations

00:24:40,000 --> 00:24:43,840
than just convolution but

00:24:42,080 --> 00:24:45,279
here in the example it supports online

00:24:43,840 --> 00:24:47,279
convolution

00:24:45,279 --> 00:24:48,320
so because it's specified as the first

00:24:47,279 --> 00:24:51,120
backend

00:24:48,320 --> 00:24:52,640
your model will your back at the back

00:24:51,120 --> 00:24:54,720
end will create workloads

00:24:52,640 --> 00:24:56,400
just for convolution then after the

00:24:54,720 --> 00:24:58,240
words in the model there is average

00:24:56,400 --> 00:24:58,880
pooling layer and a fully connected

00:24:58,240 --> 00:25:02,080
layer

00:24:58,880 --> 00:25:02,640
so those are actually not imp those are

00:25:02,080 --> 00:25:05,440
not

00:25:02,640 --> 00:25:07,039
they actually are implemented but in

00:25:05,440 --> 00:25:08,640
this example they are not implemented in

00:25:07,039 --> 00:25:11,840
the back end

00:25:08,640 --> 00:25:15,440
and so it will be up to

00:25:11,840 --> 00:25:17,919
the neon back to create the workload

00:25:15,440 --> 00:25:19,279
there so when you run the whole

00:25:17,919 --> 00:25:23,039
inference

00:25:19,279 --> 00:25:25,600
it will run it will execute the all the

00:25:23,039 --> 00:25:26,840
workloads and first it will execute them

00:25:25,600 --> 00:25:29,600
on the

00:25:26,840 --> 00:25:32,559
idatamix8npu or

00:25:29,600 --> 00:25:33,360
it will execute the convolution there uh

00:25:32,559 --> 00:25:36,320
afterwards

00:25:33,360 --> 00:25:38,320
it will skip to average pooling layer uh

00:25:36,320 --> 00:25:39,600
full and the fully connected layer and

00:25:38,320 --> 00:25:43,840
it will run those

00:25:39,600 --> 00:25:43,840
on uh the neon cpu

00:25:45,679 --> 00:25:49,679
so yeah

00:25:50,320 --> 00:25:56,640
now i'm going to be pretty quick here

00:25:54,320 --> 00:25:58,240
so it's not really a problem to

00:25:56,640 --> 00:26:01,279
implement your own

00:25:58,240 --> 00:26:04,880
third-party back-end all you need

00:26:01,279 --> 00:26:07,200
is to implement the interface you need

00:26:04,880 --> 00:26:10,640
to implement individual workloads

00:26:07,200 --> 00:26:14,640
unit tests and of course

00:26:10,640 --> 00:26:17,760
make files to build your backend

00:26:14,640 --> 00:26:20,480
you have two options you can either

00:26:17,760 --> 00:26:21,840
create a dynamic back-end or a static

00:26:20,480 --> 00:26:23,600
back-end

00:26:21,840 --> 00:26:25,919
up to you what's more convenient dynamic

00:26:23,600 --> 00:26:27,360
is loaded during runtime static is

00:26:25,919 --> 00:26:30,559
directly compiled

00:26:27,360 --> 00:26:30,559
into the armament and lab

00:26:31,520 --> 00:26:35,440
yeah on the right you can see an example

00:26:33,440 --> 00:26:38,240
what needs to be implemented so

00:26:35,440 --> 00:26:39,360
header files with individual workloads

00:26:38,240 --> 00:26:41,600
tests

00:26:39,360 --> 00:26:43,520
and the interface the most important

00:26:41,600 --> 00:26:47,840
thing to implement here

00:26:43,520 --> 00:26:50,960
is pretty much the memory manager

00:26:47,840 --> 00:26:54,640
which takes care of

00:26:50,960 --> 00:26:58,559
memory management and memory allocations

00:26:54,640 --> 00:27:00,880
on for your device so

00:26:58,559 --> 00:27:02,159
now with the last slide i'm just going

00:27:00,880 --> 00:27:05,039
to introduce

00:27:02,159 --> 00:27:08,000
a third-party custom back-end so on the

00:27:05,039 --> 00:27:11,760
ideothemix 8 devices

00:27:08,000 --> 00:27:14,960
there is there is an npu accelerator

00:27:11,760 --> 00:27:18,000
it can be either either a gpu or

00:27:14,960 --> 00:27:19,440
it can be a dedicated npu in case of the

00:27:18,000 --> 00:27:24,320
idatamix 8m

00:27:19,440 --> 00:27:26,960
plus and the acceleration there is done

00:27:24,320 --> 00:27:28,880
by the nnrt software stack shown on the

00:27:26,960 --> 00:27:31,919
right so there is arman

00:27:28,880 --> 00:27:33,120
and there is the vsi npu backhand which

00:27:31,919 --> 00:27:36,559
is available

00:27:33,120 --> 00:27:38,799
on code our repository

00:27:36,559 --> 00:27:41,039
and below that there is a whole software

00:27:38,799 --> 00:27:45,840
stack consisting of

00:27:41,039 --> 00:27:48,480
an nrt of an rt dynamic library ovxlip

00:27:45,840 --> 00:27:49,760
dynamic library and on the bottom the

00:27:48,480 --> 00:27:54,000
openvx

00:27:49,760 --> 00:27:54,000
driver so

00:27:54,880 --> 00:27:58,679
that's something like a third-party

00:27:57,679 --> 00:28:02,480
backend

00:27:58,679 --> 00:28:06,320
and as you can see from the numbers here

00:28:02,480 --> 00:28:10,880
uh if you run inference on

00:28:06,320 --> 00:28:14,399
on well on the idatemx8m

00:28:10,880 --> 00:28:17,440
plus and then it takes

00:28:14,399 --> 00:28:19,120
either 93 milliseconds for the cpu

00:28:17,440 --> 00:28:20,960
or if you really use a dedicated

00:28:19,120 --> 00:28:24,559
accelerator it may take

00:28:20,960 --> 00:28:27,600
three milliseconds so that would be uh

00:28:24,559 --> 00:28:30,440
with full with full super layer support

00:28:27,600 --> 00:28:31,520
it would be like 330

00:28:30,440 --> 00:28:34,159
[Music]

00:28:31,520 --> 00:28:35,440
fps if you would be for example doing

00:28:34,159 --> 00:28:38,480
real-time

00:28:35,440 --> 00:28:41,200
real-time detection

00:28:38,480 --> 00:28:41,200
for camera

00:28:42,399 --> 00:28:48,799
okay so thank you uh

00:28:45,760 --> 00:28:51,120
that would be pretty much it so

00:28:48,799 --> 00:28:53,360
are there any questions i have that

00:28:51,120 --> 00:28:56,240
available so any guidance as to

00:28:53,360 --> 00:28:58,000
the impact of performance utilizing the

00:28:56,240 --> 00:29:01,440
python api

00:28:58,000 --> 00:29:04,000
versus the c plus plus api

00:29:01,440 --> 00:29:05,360
well uh i would say that there would be

00:29:04,000 --> 00:29:07,840
very little

00:29:05,360 --> 00:29:09,120
uh very little impact on performance

00:29:07,840 --> 00:29:10,720
because the python

00:29:09,120 --> 00:29:12,240
just delegates all the calls to the

00:29:10,720 --> 00:29:15,120
dynamic library so if

00:29:12,240 --> 00:29:15,520
if you are using the c plus plus api

00:29:15,120 --> 00:29:17,279
well

00:29:15,520 --> 00:29:19,039
uh you have that you have your

00:29:17,279 --> 00:29:21,600
application it still links

00:29:19,039 --> 00:29:22,880
to the dynamic library and then uses the

00:29:21,600 --> 00:29:24,399
dynamic library for

00:29:22,880 --> 00:29:26,240
all the calls whereas if you use the

00:29:24,399 --> 00:29:29,840
python api it does

00:29:26,240 --> 00:29:31,760
the same thing yeah the

00:29:29,840 --> 00:29:33,760
the difference is that there are only

00:29:31,760 --> 00:29:35,760
there is only an additional intermediate

00:29:33,760 --> 00:29:39,200
dynamic library

00:29:35,760 --> 00:29:42,240
which is generated by swag and it's

00:29:39,200 --> 00:29:46,240
so it runs through that and uh

00:29:42,240 --> 00:29:49,840
to the lip uh arm and an so

00:29:46,240 --> 00:29:51,760
so there is not much of an

00:29:49,840 --> 00:29:54,240
impact because well uh neural networks

00:29:51,760 --> 00:29:56,399
are pretty computationally intensive

00:29:54,240 --> 00:29:59,120
compared to something like python

00:29:56,399 --> 00:30:01,840
overhead so

00:29:59,120 --> 00:30:03,520
maybe for very little models you might

00:30:01,840 --> 00:30:07,120
see something but

00:30:03,520 --> 00:30:09,840
in terms of milliseconds

00:30:07,120 --> 00:30:11,360
we didn't really observe any performance

00:30:09,840 --> 00:30:15,360
impact of

00:30:11,360 --> 00:30:20,240
using either python or c plus plus apis

00:30:15,360 --> 00:30:20,240

YouTube URL: https://www.youtube.com/watch?v=QuNOaFLobSg


