Title: HKG18-312 - CMSIS-NN
Publication date: 2019-05-09
Playlist: Linaro Connect Hong Kong 2018
Description: 
	"Session ID: HKG18-312
Session Name: HKG18-312 - CMSIS-NN
Speaker: Rod Crawford
Track: LEG;Ecosystem Day


★ Session Summary ★
Deep learning algorithms are gaining popularity in IoT edge devices, because of their human-level accuracy in many applications, such as image classification and speech recognition. There is an increasing interest in deploying neural networks (NN) on always-on systems such as Arm Cortex-M microcontrollers. In this session, we introduce the challenges of deploying neural networks on microcontrollers with limited memory/compute resources and power budget. We introduce CMSIS-NN, optimized software kernels to enable deployment of neural networks on Cortex-M cores. We further present NN architecture exploration, using keyword spotting application as an example, to develop light-weight models suitable for resource constrained systems.
---------------------------------------------------
★ Resources ★
Event Page: http://connect.linaro.org/resource/hkg18/hkg18-312/
Presentation: http://connect.linaro.org.s3.amazonaws.com/hkg18/presentations/hkg18-312.pdf
Video: http://connect.linaro.org.s3.amazonaws.com/hkg18/videos/hkg18-312.mp4
 ---------------------------------------------------
★ Event Details ★
Linaro Connect Hong Kong 2018 (HKG18)
19-23 March 2018 
Regal Airport Hotel Hong Kong

---------------------------------------------------
Keyword: LEG;Ecosystem Day
'http://www.linaro.org'
'http://connect.linaro.org'
---------------------------------------------------
Follow us on Social Media
https://www.facebook.com/LinaroOrg
https://www.youtube.com/user/linaroorg?sub_confirmation=1
https://www.linkedin.com/company/1026961"
Captions: 
	00:00:01,620 --> 00:00:06,580
[Music]

00:00:14,370 --> 00:00:22,320
you have a challenge people are hungry

00:00:17,670 --> 00:00:33,610
keep them with us okay I'm hungry too

00:00:22,320 --> 00:00:36,390
let's see set a timer for 20 minutes so

00:00:33,610 --> 00:00:40,750
what I've just done there is actually

00:00:36,390 --> 00:00:41,500
run some machine learning actually on my

00:00:40,750 --> 00:00:45,190
watch

00:00:41,500 --> 00:00:47,440
to understand that sentence and set a

00:00:45,190 --> 00:00:51,130
set a timer that's really what this talk

00:00:47,440 --> 00:00:53,110
is all about so what I'm gonna talk to

00:00:51,130 --> 00:00:56,790
you about today about is what you can do

00:00:53,110 --> 00:01:00,040
in terms of running machine learning

00:00:56,790 --> 00:01:04,900
running machine learning on cortex-m

00:01:00,040 --> 00:01:06,600
microcontrollers so first off let's kind

00:01:04,900 --> 00:01:09,850
of go through a little bit of the

00:01:06,600 --> 00:01:12,880
terminology that I'm going to use so

00:01:09,850 --> 00:01:14,560
we've got some common ground so so what

00:01:12,880 --> 00:01:19,060
I'm really going to be looking at is is

00:01:14,560 --> 00:01:22,240
is machine learning the the third row of

00:01:19,060 --> 00:01:24,159
this third ring of this this onion here

00:01:22,240 --> 00:01:27,159
and what we're really talking about when

00:01:24,159 --> 00:01:29,610
we talk about machine learning is the

00:01:27,159 --> 00:01:34,210
construction of a mathematical model

00:01:29,610 --> 00:01:37,380
that you can train with a with a set of

00:01:34,210 --> 00:01:40,030
input data and you construct this models

00:01:37,380 --> 00:01:42,850
generally training it in the cloud

00:01:40,030 --> 00:01:45,610
rather than then at the edge but once

00:01:42,850 --> 00:01:47,950
you've got this model trained with with

00:01:45,610 --> 00:01:52,360
the data set then you can run this model

00:01:47,950 --> 00:01:54,340
against against a set of data and once

00:01:52,360 --> 00:01:57,810
you've run that model against a set of

00:01:54,340 --> 00:02:01,360
data you can then infer whether or not

00:01:57,810 --> 00:02:04,570
something is is true or false within a

00:02:01,360 --> 00:02:08,890
specific percentage of probability so

00:02:04,570 --> 00:02:11,200
for example you might take a data set

00:02:08,890 --> 00:02:14,290
that's a series of photographs of of

00:02:11,200 --> 00:02:16,330
animals and you might build a model that

00:02:14,290 --> 00:02:19,060
basically says whether they're a cat or

00:02:16,330 --> 00:02:21,280
a dog or a giraffe and then when you

00:02:19,060 --> 00:02:23,859
show a photograph to this model that

00:02:21,280 --> 00:02:24,969
you've trained up in this way it will

00:02:23,859 --> 00:02:26,850
actually give you a percentage

00:02:24,969 --> 00:02:29,370
probability of whether it's

00:02:26,850 --> 00:02:32,130
a cat or a dog or a giraffe or something

00:02:29,370 --> 00:02:34,880
it doesn't basically know so that's kind

00:02:32,130 --> 00:02:38,190
of what what what actually happens today

00:02:34,880 --> 00:02:40,230
but generally these models have been run

00:02:38,190 --> 00:02:42,240
up in the cloud they're trained in the

00:02:40,230 --> 00:02:44,490
cloud and they've run in the cloud but

00:02:42,240 --> 00:02:46,920
what we're going to look at is how we

00:02:44,490 --> 00:02:51,360
can actually make these models run on

00:02:46,920 --> 00:02:54,980
our wee tiny small cortex cause XM micro

00:02:51,360 --> 00:02:57,300
controller so why do we want to run

00:02:54,980 --> 00:03:00,900
machine learning at the edge and I think

00:02:57,300 --> 00:03:03,300
this comes back to the the slides and

00:03:00,900 --> 00:03:06,960
presentation that Mark Hamilton did

00:03:03,300 --> 00:03:09,090
yesterday in the keynote if you're

00:03:06,960 --> 00:03:12,570
uploading a lot of data into the cloud

00:03:09,090 --> 00:03:14,460
to do your inference you're going to use

00:03:12,570 --> 00:03:17,370
a lot of bandwidth and that's time

00:03:14,460 --> 00:03:21,450
consuming and it's also cost consuming

00:03:17,370 --> 00:03:24,240
as well running stuff in the cloud is

00:03:21,450 --> 00:03:26,280
great but it does tend to be rather

00:03:24,240 --> 00:03:27,690
rather power-hungry it would be great if

00:03:26,280 --> 00:03:29,670
we could run at the edge and we could

00:03:27,690 --> 00:03:32,430
maybe run these inference engines on

00:03:29,670 --> 00:03:33,930
batteries for example that would be

00:03:32,430 --> 00:03:37,410
great

00:03:33,930 --> 00:03:40,410
and latency is another issue if I say a

00:03:37,410 --> 00:03:42,150
sentence to my my assistant and I have

00:03:40,410 --> 00:03:44,040
to wait several seconds before it

00:03:42,150 --> 00:03:47,840
actually comes back with an answer

00:03:44,040 --> 00:03:51,360
that's that's also under undesirable and

00:03:47,840 --> 00:03:53,310
then reliability is another issue it's

00:03:51,360 --> 00:03:55,380
very much dependent on your connection

00:03:53,310 --> 00:03:56,880
into the cloud so if you're driving

00:03:55,380 --> 00:03:58,770
along in the desert and you suddenly

00:03:56,880 --> 00:04:01,740
lose connection with your cloud you'd be

00:03:58,770 --> 00:04:03,540
rather unhappy if your self-driving car

00:04:01,740 --> 00:04:06,930
just stops because it doesn't know where

00:04:03,540 --> 00:04:08,760
it's going and then lastly another

00:04:06,930 --> 00:04:13,020
reason for running all this stuff at the

00:04:08,760 --> 00:04:16,230
edge is security you want the data that

00:04:13,020 --> 00:04:19,140
you're using to feed your your inference

00:04:16,230 --> 00:04:20,820
engine preferably to actually reside on

00:04:19,140 --> 00:04:23,550
the device that you're running it or not

00:04:20,820 --> 00:04:26,130
go off into into the cloud and who knows

00:04:23,550 --> 00:04:29,670
where that data might go as we're seeing

00:04:26,130 --> 00:04:33,320
in the news today so what am I talking

00:04:29,670 --> 00:04:37,080
about in terms of running on cortex and

00:04:33,320 --> 00:04:39,030
so you know the the high-end of running

00:04:37,080 --> 00:04:39,400
at the edge you would have autonomous

00:04:39,030 --> 00:04:41,169
view

00:04:39,400 --> 00:04:44,110
calls you know with fairly large

00:04:41,169 --> 00:04:46,330
processes actually running in the car

00:04:44,110 --> 00:04:49,180
you know it's like a mini data center

00:04:46,330 --> 00:04:51,699
running in the boot of your car but what

00:04:49,180 --> 00:04:54,520
I'm really looking at is what I can do

00:04:51,699 --> 00:04:58,210
with with the cortex-m processor down at

00:04:54,520 --> 00:05:00,669
the tens of millions what way down there

00:04:58,210 --> 00:05:04,479
so in this domain I can do things like

00:05:00,669 --> 00:05:08,620
rudimentary image detection pattern

00:05:04,479 --> 00:05:12,100
training for senses so maybe sensors all

00:05:08,620 --> 00:05:14,860
fused into into an inference engine for

00:05:12,100 --> 00:05:17,860
looking at anomalies in industrial

00:05:14,860 --> 00:05:19,990
processes looking at movements around

00:05:17,860 --> 00:05:23,080
smart buildings things like that and

00:05:19,990 --> 00:05:25,690
then way down to the basic keyword

00:05:23,080 --> 00:05:28,360
detection where you actually want to be

00:05:25,690 --> 00:05:31,300
able to talk to things so today if you

00:05:28,360 --> 00:05:35,010
buy an Alexa device or something like

00:05:31,300 --> 00:05:38,500
that you're actually looking at a fairly

00:05:35,010 --> 00:05:40,539
heavyweight processor or if you buy a

00:05:38,500 --> 00:05:42,310
Google home you're looking at a fairly

00:05:40,539 --> 00:05:45,940
heavyweight processor but what's really

00:05:42,310 --> 00:05:48,340
desirable is to be able to reduce the

00:05:45,940 --> 00:05:50,410
cost of these processors get it down to

00:05:48,340 --> 00:05:52,690
something a lot smaller and a lot

00:05:50,410 --> 00:05:54,690
cheaper and then you can actually start

00:05:52,690 --> 00:05:58,150
talking to things like your light bulb

00:05:54,690 --> 00:06:00,400
or your light switch to turn things on

00:05:58,150 --> 00:06:03,639
and off and I think there's going to be

00:06:00,400 --> 00:06:08,530
a growth of these kind of platforms over

00:06:03,639 --> 00:06:11,409
the next couple of years it could be

00:06:08,530 --> 00:06:16,570
quite an explosive growth because we've

00:06:11,409 --> 00:06:19,479
all seen the the advent of wearables

00:06:16,570 --> 00:06:22,300
like this devices that go into your

00:06:19,479 --> 00:06:24,820
shoes all of that health tracking that

00:06:22,300 --> 00:06:28,030
kind of stuff so deploying machine

00:06:24,820 --> 00:06:30,639
learning into into these kind of devices

00:06:28,030 --> 00:06:34,090
is going to be really really big I think

00:06:30,639 --> 00:06:36,520
so so there's an explosive growth for

00:06:34,090 --> 00:06:39,669
machine learning I think on on small

00:06:36,520 --> 00:06:41,380
microcontrollers so for the rest of this

00:06:39,669 --> 00:06:44,310
presentation what I'm going to do is

00:06:41,380 --> 00:06:47,860
walk you through the process of

00:06:44,310 --> 00:06:49,300
deploying a neural net model on a

00:06:47,860 --> 00:06:53,680
cortex-m device

00:06:49,300 --> 00:06:56,530
so what we're going to look at is speech

00:06:53,680 --> 00:07:00,520
recognition so recognition of a small

00:06:56,530 --> 00:07:02,889
vocabulary that kind of thing we're

00:07:00,520 --> 00:07:07,300
going to develop a trained neural net

00:07:02,889 --> 00:07:11,080
model and will then deploy that neural

00:07:07,300 --> 00:07:13,569
net model onto a cortex end and will

00:07:11,080 --> 00:07:16,870
optimize it so as it runs really really

00:07:13,569 --> 00:07:21,639
efficiently so the steps for doing this

00:07:16,870 --> 00:07:23,199
involve deciding what kind of neuron

00:07:21,639 --> 00:07:27,250
that model you're going to use for your

00:07:23,199 --> 00:07:29,590
application domain translating that

00:07:27,250 --> 00:07:31,900
model once you've got it into something

00:07:29,590 --> 00:07:34,870
that can be deployed onto the

00:07:31,900 --> 00:07:36,969
microcontroller because large scale

00:07:34,870 --> 00:07:40,599
neuron that framework simply won't fit

00:07:36,969 --> 00:07:45,520
and then using a set of optimized

00:07:40,599 --> 00:07:49,060
functions to to allow you to run the

00:07:45,520 --> 00:07:52,930
model fast run the model small and run

00:07:49,060 --> 00:07:55,569
the model energy efficiently so let's

00:07:52,930 --> 00:08:01,360
have a look at the choice how would go

00:07:55,569 --> 00:08:04,590
out choosing a neural net model so for

00:08:01,360 --> 00:08:07,029
something like a vocabulary recognition

00:08:04,590 --> 00:08:10,120
keyword searching that kind of thing

00:08:07,029 --> 00:08:13,029
there are a number of neural nets that

00:08:10,120 --> 00:08:16,960
are neural net models that are actually

00:08:13,029 --> 00:08:20,050
applicable and when we look at the

00:08:16,960 --> 00:08:23,740
cortex family of processes we can

00:08:20,050 --> 00:08:26,889
basically classify them into small

00:08:23,740 --> 00:08:28,870
medium and large and the small scale

00:08:26,889 --> 00:08:29,979
ones are actually quite interesting

00:08:28,870 --> 00:08:32,969
there's quite a lot of these out there

00:08:29,979 --> 00:08:35,919
so so doing something on a reasonably

00:08:32,969 --> 00:08:39,789
low-cost device which has got a small

00:08:35,919 --> 00:08:41,890
memory footprint and a reasonably

00:08:39,789 --> 00:08:45,870
constrained performance envelope is

00:08:41,890 --> 00:08:45,870
really the area that we want to look at

00:08:47,620 --> 00:08:56,680
so up until recently we we saw that

00:08:54,130 --> 00:08:59,650
there were a range of different neural

00:08:56,680 --> 00:09:02,590
network models that were applicable in

00:08:59,650 --> 00:09:06,760
different different scales of operation

00:09:02,590 --> 00:09:09,100
versus versus memories so down at the

00:09:06,760 --> 00:09:12,760
we've got large medium and small here

00:09:09,100 --> 00:09:15,940
and then at the the small scale up until

00:09:12,760 --> 00:09:18,910
recently the large sorry the long shirt

00:09:15,940 --> 00:09:20,740
short-term memory model neural net

00:09:18,910 --> 00:09:23,920
seemed to be the most applicable and

00:09:20,740 --> 00:09:28,990
give you the best accuracy for memory

00:09:23,920 --> 00:09:32,770
size versus versus operations but we did

00:09:28,990 --> 00:09:36,610
our own research into this and we looked

00:09:32,770 --> 00:09:40,720
at a series of these different neural

00:09:36,610 --> 00:09:44,050
net models and what this basically shows

00:09:40,720 --> 00:09:47,170
is a three-dimensional map of the

00:09:44,050 --> 00:09:50,980
different models where we have accuracy

00:09:47,170 --> 00:09:53,020
on the the Y X Y axis and we have the

00:09:50,980 --> 00:09:57,220
number of operations it takes to get to

00:09:53,020 --> 00:09:59,140
an inference and the x axis but the the

00:09:57,220 --> 00:10:02,590
size of the bubble or the size of the

00:09:59,140 --> 00:10:04,900
blob represents the number of parameters

00:10:02,590 --> 00:10:07,660
in the model or that's actually the

00:10:04,900 --> 00:10:09,250
number of weights in the model and if

00:10:07,660 --> 00:10:10,960
you have a very large number of weights

00:10:09,250 --> 00:10:13,360
then of course your memory footprint

00:10:10,960 --> 00:10:16,090
goes up considerably so really we're

00:10:13,360 --> 00:10:19,480
looking at accuracy versus number of

00:10:16,090 --> 00:10:22,360
operations versus memory footprint and

00:10:19,480 --> 00:10:31,690
from this research we actually found

00:10:22,360 --> 00:10:36,010
that a convolutional or a trnn a

00:10:31,690 --> 00:10:38,410
convolutional got a look at my notes

00:10:36,010 --> 00:10:41,680
I think it's convolutional regressive

00:10:38,410 --> 00:10:44,020
neural net is actually the best the best

00:10:41,680 --> 00:10:47,280
fit for this because it actually has a

00:10:44,020 --> 00:10:50,110
really small number of operations

00:10:47,280 --> 00:10:53,770
relative to to any of the other networks

00:10:50,110 --> 00:10:55,620
but still gives you a low number of

00:10:53,770 --> 00:10:57,990
parameters and

00:10:55,620 --> 00:11:02,040
and a high level of accuracy so if we

00:10:57,990 --> 00:11:05,370
tabular eyes this what we find is the

00:11:02,040 --> 00:11:08,130
bottom two lines here of this table show

00:11:05,370 --> 00:11:10,650
that between depth wise separate we'll

00:11:08,130 --> 00:11:13,529
see it in and at CRNA and we actually

00:11:10,650 --> 00:11:15,810
have a trade off and that trade off is

00:11:13,529 --> 00:11:19,310
the number of operations to get to an

00:11:15,810 --> 00:11:22,320
inference versus the amount of memory so

00:11:19,310 --> 00:11:25,040
if you have a CR n n you're at about 80

00:11:22,320 --> 00:11:27,600
K bytes but you're only using three

00:11:25,040 --> 00:11:30,240
three million operations to get your

00:11:27,600 --> 00:11:32,490
inference whereas a DSC n n you're

00:11:30,240 --> 00:11:34,440
actually got half the memory footprint

00:11:32,490 --> 00:11:37,560
but you've nearly doubled the number of

00:11:34,440 --> 00:11:39,420
operations you want to do so it's kind

00:11:37,560 --> 00:11:42,150
of as we would say horses for courses

00:11:39,420 --> 00:11:44,279
you need to choose whether you want a

00:11:42,150 --> 00:11:47,880
lower memory footprint and a higher

00:11:44,279 --> 00:11:50,550
energy envelope or a larger memory

00:11:47,880 --> 00:11:52,529
footprint and a smaller energy envelope

00:11:50,550 --> 00:11:58,110
so it's all about energy versus cost

00:11:52,529 --> 00:12:00,870
really so we've talked about choice of

00:11:58,110 --> 00:12:03,180
models what I think we should do now is

00:12:00,870 --> 00:12:06,270
talk about how that model gets

00:12:03,180 --> 00:12:10,410
compressed down to an executable program

00:12:06,270 --> 00:12:14,760
that can run on a cortex processor so to

00:12:10,410 --> 00:12:17,730
do that I almost put together a series

00:12:14,760 --> 00:12:20,730
of tools to help you do this so one of

00:12:17,730 --> 00:12:24,660
the most important things to do when you

00:12:20,730 --> 00:12:27,990
when you want to deploy your model is to

00:12:24,660 --> 00:12:32,520
reduce the accuracy of the weights one

00:12:27,990 --> 00:12:34,980
of the interesting features of machine

00:12:32,520 --> 00:12:36,390
learning is that once you've got a

00:12:34,980 --> 00:12:38,100
trained modeling you've got it working

00:12:36,390 --> 00:12:41,790
and you've got the accuracy that you

00:12:38,100 --> 00:12:44,190
require you can actually reduce the

00:12:41,790 --> 00:12:46,410
floating-point accuracy of the weights

00:12:44,190 --> 00:12:50,520
that you use considerably and you can go

00:12:46,410 --> 00:12:54,690
from 32-bit floating-point right down to

00:12:50,520 --> 00:12:57,630
16 bit or even 8-bit fixed point for

00:12:54,690 --> 00:13:00,510
your weights and as this table shows as

00:12:57,630 --> 00:13:02,640
you go to an 8-bit quantized model in

00:13:00,510 --> 00:13:04,280
fact in the case of CRNA it's just

00:13:02,640 --> 00:13:06,990
slightly although it's in the noise

00:13:04,280 --> 00:13:08,140
slightly more accuracy than you you had

00:13:06,990 --> 00:13:11,260
was two 32-bit

00:13:08,140 --> 00:13:13,510
floating-point so quantizing things down

00:13:11,260 --> 00:13:16,000
can actually greatly reduce your memory

00:13:13,510 --> 00:13:22,870
footprint but still allows you to have

00:13:16,000 --> 00:13:26,680
the same level of accuracy so arm has

00:13:22,870 --> 00:13:32,250
put together a series of libraries and

00:13:26,680 --> 00:13:34,540
tools part of its project Trillium

00:13:32,250 --> 00:13:39,280
endeavour which was announced a few

00:13:34,540 --> 00:13:44,830
weeks ago and as part of that there is

00:13:39,280 --> 00:13:46,660
an SDK called the arm n n SDK and part

00:13:44,830 --> 00:13:50,110
of that will actually have the tools

00:13:46,660 --> 00:13:53,890
that you need to compress down compress

00:13:50,110 --> 00:13:57,760
down the the the network into into

00:13:53,890 --> 00:14:00,670
something that's executable so once

00:13:57,760 --> 00:14:03,640
you've got your executable program you

00:14:00,670 --> 00:14:07,090
also want to be able to hopefully run it

00:14:03,640 --> 00:14:09,190
efficiently on the core tech firm and so

00:14:07,090 --> 00:14:12,750
for that we actually introduced at the

00:14:09,190 --> 00:14:17,280
beginning of this year since it sent in

00:14:12,750 --> 00:14:20,160
so Simpson is an industrial standard

00:14:17,280 --> 00:14:23,410
there arm brought out a few years ago

00:14:20,160 --> 00:14:26,170
that enables if you write to the the

00:14:23,410 --> 00:14:29,980
Simpson API is it actually enables you

00:14:26,170 --> 00:14:32,200
to interplay and exchange different ARM

00:14:29,980 --> 00:14:34,390
based microcontrollers because you just

00:14:32,200 --> 00:14:36,340
write to the symphysis standard and if

00:14:34,390 --> 00:14:38,950
the Simpson and it is supported by the

00:14:36,340 --> 00:14:40,860
microcontroller then it then it will run

00:14:38,950 --> 00:14:43,300
the code will just run on any of those

00:14:40,860 --> 00:14:45,810
microcontrollers so as well as since

00:14:43,300 --> 00:14:48,520
this core depending what you want to do

00:14:45,810 --> 00:14:50,950
there are packs Simpsons packs as

00:14:48,520 --> 00:14:53,560
they're called for real-time operations

00:14:50,950 --> 00:14:56,170
like there's an artist there if you want

00:14:53,560 --> 00:14:58,960
to do DSP operations as a Simpsons pack

00:14:56,170 --> 00:15:02,770
for that and there's now a series of

00:14:58,960 --> 00:15:05,560
optimized operators for for neural nets

00:15:02,770 --> 00:15:08,380
called Simpson sent in and that's

00:15:05,560 --> 00:15:11,500
available free on the the arm github

00:15:08,380 --> 00:15:13,650
today one of the other things that we

00:15:11,500 --> 00:15:15,850
have is also the ability to measure

00:15:13,650 --> 00:15:18,190
energy consumption on the

00:15:15,850 --> 00:15:19,870
microcontroller so you can actually do

00:15:18,190 --> 00:15:20,819
some of the trade-offs I described

00:15:19,870 --> 00:15:24,720
earlier

00:15:20,819 --> 00:15:29,809
around memory footprint versus versus

00:15:24,720 --> 00:15:33,539
number of operations so since it's an N

00:15:29,809 --> 00:15:35,189
provides you with a set of optimized

00:15:33,539 --> 00:15:38,759
routines for some of the common

00:15:35,189 --> 00:15:41,789
operations that are done by inference

00:15:38,759 --> 00:15:43,979
engines that includes simply operations

00:15:41,789 --> 00:15:47,989
it provides tools for minimizing the

00:15:43,979 --> 00:15:52,410
memory footprint it also provides a

00:15:47,989 --> 00:15:55,619
series of data formats that you can

00:15:52,410 --> 00:15:57,989
basically relay out your your network in

00:15:55,619 --> 00:15:59,879
in a data layout that makes it much

00:15:57,989 --> 00:16:06,629
easier for the microcontroller to

00:15:59,879 --> 00:16:09,029
actually execute on so here's a little

00:16:06,629 --> 00:16:11,579
look at the kind of performance you can

00:16:09,029 --> 00:16:16,769
get performance improvement you can get

00:16:11,579 --> 00:16:21,179
if you use synthase name so you can get

00:16:16,769 --> 00:16:23,879
roughly using these these operations

00:16:21,179 --> 00:16:26,369
that are part of simply sent in you can

00:16:23,879 --> 00:16:29,399
actually improve the performance by

00:16:26,369 --> 00:16:31,679
nearly five percent but also a sort of

00:16:29,399 --> 00:16:34,289
by five times sorry not five percent

00:16:31,679 --> 00:16:37,259
five times but you can also improve the

00:16:34,289 --> 00:16:41,100
energy efficiency by nearly five times

00:16:37,259 --> 00:16:46,829
as well and so that's in in an MCU space

00:16:41,100 --> 00:16:50,249
is really very interesting okay so so

00:16:46,829 --> 00:16:55,649
lastly I wanted to talk a little bit

00:16:50,249 --> 00:16:59,009
about an example that is actually in the

00:16:55,649 --> 00:17:01,019
simplest code base and this is running

00:16:59,009 --> 00:17:03,749
on a cortex m7 it's running a little

00:17:01,019 --> 00:17:09,230
board called the open mV platform an

00:17:03,749 --> 00:17:11,789
open env has an SD micro nucleo

00:17:09,230 --> 00:17:15,809
microcontroller and a little camera on

00:17:11,789 --> 00:17:19,740
there so so what we've done is we've put

00:17:15,809 --> 00:17:23,039
in a neural net for recognizing

00:17:19,740 --> 00:17:25,949
different animals and we basically show

00:17:23,039 --> 00:17:29,460
show the camera images of the animals

00:17:25,949 --> 00:17:30,900
and it has a go so what I'm going to do

00:17:29,460 --> 00:17:33,480
is I've got a link

00:17:30,900 --> 00:17:39,680
here and if you guys can fire up the

00:17:33,480 --> 00:17:39,680
link see if that works

00:17:40,790 --> 00:17:54,270
okay you may need to switch to your

00:17:46,020 --> 00:17:56,070
browser yeah not going to work if it

00:17:54,270 --> 00:17:59,340
doesn't work I can show show you guys

00:17:56,070 --> 00:18:01,740
later but the nice thing about the open

00:17:59,340 --> 00:18:05,790
env platform is it's programmable in

00:18:01,740 --> 00:18:08,700
micro pison so for the applicational I

00:18:05,790 --> 00:18:08,880
see a bit of a browser yep yeah there we

00:18:08,700 --> 00:18:10,560
go

00:18:08,880 --> 00:18:19,260
if you just take that full screen I

00:18:10,560 --> 00:18:22,650
think we're good okay so what you can

00:18:19,260 --> 00:18:25,550
see here is on the left hand side is the

00:18:22,650 --> 00:18:28,620
microfiber applicator that categorizes

00:18:25,550 --> 00:18:31,950
the images to be the aeroplanes

00:18:28,620 --> 00:18:34,770
automobiles birds cats dogs frogs you

00:18:31,950 --> 00:18:36,660
know you name it and then on the right

00:18:34,770 --> 00:18:39,810
you see the image that's being shown to

00:18:36,660 --> 00:18:42,750
the camera there and in the bottom left

00:18:39,810 --> 00:18:45,870
you actually see what it actually thinks

00:18:42,750 --> 00:18:47,760
the image is so it doesn't get it right

00:18:45,870 --> 00:18:51,930
all the time occasionally it thinks a

00:18:47,760 --> 00:18:54,390
cat is a fish and occasionally it thinks

00:18:51,930 --> 00:18:56,970
a cat is a frog and stuff like that

00:18:54,390 --> 00:18:58,590
but you can actually see that a lot of

00:18:56,970 --> 00:19:01,410
the time it actually does get this right

00:18:58,590 --> 00:19:03,060
it's decided cats now a frog I think at

00:19:01,410 --> 00:19:06,630
this point because of the way it's

00:19:03,060 --> 00:19:08,340
sitting but this is just an example of

00:19:06,630 --> 00:19:11,370
sort of what you can get going

00:19:08,340 --> 00:19:14,520
using Simpsons ññ and the example code

00:19:11,370 --> 00:19:16,680
the the camera is running at about 4

00:19:14,520 --> 00:19:20,670
frames per second and if we actually

00:19:16,680 --> 00:19:22,620
switch that into monochrome then you'd

00:19:20,670 --> 00:19:24,690
actually get probably about 12 frames

00:19:22,620 --> 00:19:26,760
per second and what it Chrome's

00:19:24,690 --> 00:19:30,240
perfectly fine for surveillance cameras

00:19:26,760 --> 00:19:32,430
and stuff like that so I hope that kind

00:19:30,240 --> 00:19:35,450
of gives you a flavor we can probably go

00:19:32,430 --> 00:19:35,450
back to the slides

00:19:39,070 --> 00:19:44,120
so I hope that gives you a flavor of

00:19:41,000 --> 00:19:47,900
what you can actually do with with a

00:19:44,120 --> 00:19:50,000
cortex-m today off the shelf with with

00:19:47,900 --> 00:19:51,799
machine learning what we've kind of

00:19:50,000 --> 00:19:53,360
looked at is how you go about choosing

00:19:51,799 --> 00:19:56,510
the model for your particular

00:19:53,360 --> 00:19:58,909
application domain how you can basically

00:19:56,510 --> 00:20:04,370
use some of the tools that becoming

00:19:58,909 --> 00:20:08,120
available in the arm nn SDK to compress

00:20:04,370 --> 00:20:11,090
that down quantize your your weights and

00:20:08,120 --> 00:20:14,500
get the the thing down to a few tens of

00:20:11,090 --> 00:20:17,659
K and then how you can actually then

00:20:14,500 --> 00:20:20,210
bind that against the Simpson end

00:20:17,659 --> 00:20:22,490
library and actually get pretty good

00:20:20,210 --> 00:20:37,309
pretty good performance and pretty good

00:20:22,490 --> 00:20:44,240
accuracy thank you and I'm on time as

00:20:37,309 --> 00:20:46,159
well so just to make sure I understand

00:20:44,240 --> 00:20:50,570
it correctly you you mentioned you have

00:20:46,159 --> 00:20:53,270
a compiler that takes the model as input

00:20:50,570 --> 00:20:56,559
and it generated executable code for the

00:20:53,270 --> 00:21:00,110
cortex aim to run did I get that right

00:20:56,559 --> 00:21:02,059
not not entirely no we haven't we have a

00:21:00,110 --> 00:21:04,070
series of tools it's not it's not as

00:21:02,059 --> 00:21:06,679
easy as just taking a compiler and

00:21:04,070 --> 00:21:09,140
compressing it but there are a series of

00:21:06,679 --> 00:21:12,260
tools on that on the on the github that

00:21:09,140 --> 00:21:14,049
you can effectively do this it's more of

00:21:12,260 --> 00:21:16,580
a hand operation to get it down to that

00:21:14,049 --> 00:21:19,159
but the tools actually make it a lot

00:21:16,580 --> 00:21:22,970
easier ok so if I extrapolate a little

00:21:19,159 --> 00:21:26,450
bit that generated code could run other

00:21:22,970 --> 00:21:28,640
arm as well right absolutely

00:21:26,450 --> 00:21:31,159
it's just probably not the best most

00:21:28,640 --> 00:21:34,340
efficient I guess yeah I mean I think

00:21:31,159 --> 00:21:36,559
the operations that we've done in in the

00:21:34,340 --> 00:21:40,250
Simpson 10 library are specifically

00:21:36,559 --> 00:21:42,289
focused at the the cortex-m family but

00:21:40,250 --> 00:21:44,120
you might choose a different library to

00:21:42,289 --> 00:21:46,460
optimize it or if you're running on a

00:21:44,120 --> 00:21:50,430
cortex a you might might have a bit more

00:21:46,460 --> 00:21:54,180
Headroom yeah ok thanks

00:21:50,430 --> 00:21:55,890
and there will be the arm NN for example

00:21:54,180 --> 00:22:02,670
the set of libraries optimized for

00:21:55,890 --> 00:22:05,610
cortex a yes so for for the keyword

00:22:02,670 --> 00:22:07,980
recognition application typically how

00:22:05,610 --> 00:22:16,110
many keywords can be recognized in the

00:22:07,980 --> 00:22:17,790
cortex M with save ATK probably can't

00:22:16,110 --> 00:22:22,800
give you I can't give you the accuracy

00:22:17,790 --> 00:22:26,910
for 8k but what I can give you is is

00:22:22,800 --> 00:22:31,410
that you can recognize you know sort of

00:22:26,910 --> 00:22:33,540
hundreds to hundreds and above if you if

00:22:31,410 --> 00:22:37,410
you've got it if you know it's all about

00:22:33,540 --> 00:22:41,220
your memory really so there's a there's

00:22:37,410 --> 00:22:44,250
a website called Kegel which you're

00:22:41,220 --> 00:22:46,679
familiar with right so so there's a lot

00:22:44,250 --> 00:22:51,240
of activity on that and how big you can

00:22:46,679 --> 00:22:57,960
you could you can do your second

00:22:51,240 --> 00:23:00,030
question real quick so on the when when

00:22:57,960 --> 00:23:01,020
training neural nets and and running

00:23:00,030 --> 00:23:03,570
them you needed to a lot of

00:23:01,020 --> 00:23:06,150
pre-processing so do you have a library

00:23:03,570 --> 00:23:08,070
for some of those scalar is a ssin and

00:23:06,150 --> 00:23:09,929
normalization kind of operations you

00:23:08,070 --> 00:23:12,300
need to do to feed their own that with

00:23:09,929 --> 00:23:15,210
the sensor data I vote for the the

00:23:12,300 --> 00:23:16,980
training process Oh for the for the ring

00:23:15,210 --> 00:23:19,050
and running as well so you're getting

00:23:16,980 --> 00:23:21,540
raw sensor data and then you need to you

00:23:19,050 --> 00:23:23,550
need to compress it and do things before

00:23:21,540 --> 00:23:25,440
you give it to the neuron that is sort

00:23:23,550 --> 00:23:29,340
of the pre-processing straight across

00:23:25,440 --> 00:23:30,960
the sing stage that I think that's

00:23:29,340 --> 00:23:32,970
pretty much outside the scope of the

00:23:30,960 --> 00:23:34,980
synthesis enemy but yeah that would be

00:23:32,970 --> 00:23:37,710
that would be you know without fight for

00:23:34,980 --> 00:23:40,160
another another area sorry scikit-learn

00:23:37,710 --> 00:23:45,950
type functions just lots of stuff that

00:23:40,160 --> 00:23:45,950
needs to be in there right okay

00:23:48,140 --> 00:23:58,170
any other question then I have one okay

00:23:54,560 --> 00:24:02,280
the recent announcements about project

00:23:58,170 --> 00:24:04,590
trailers and the processors about object

00:24:02,280 --> 00:24:07,770
detection recognition by arm would those

00:24:04,590 --> 00:24:11,460
feet on the CMC CNN as well within

00:24:07,770 --> 00:24:14,250
cortex-m range for the the object

00:24:11,460 --> 00:24:17,280
detection processor is completely

00:24:14,250 --> 00:24:20,130
completely separate IP so since this

00:24:17,280 --> 00:24:22,880
antenna is is purely focused being able

00:24:20,130 --> 00:24:26,520
to run inference engines are on

00:24:22,880 --> 00:24:27,990
microcontrollers and sure you can do as

00:24:26,520 --> 00:24:32,280
we've seen there you can do object

00:24:27,990 --> 00:24:34,170
detection but thought for the the object

00:24:32,280 --> 00:24:36,030
detection process so that's a completely

00:24:34,170 --> 00:24:40,680
separate entity with its own own

00:24:36,030 --> 00:24:43,710
software libraries take your Road

00:24:40,680 --> 00:24:47,210
okay thank you we can break here have a

00:24:43,710 --> 00:24:47,210
great lunch everybody thank you

00:24:47,790 --> 00:24:51,270
[Applause]

00:24:56,020 --> 00:24:58,080

YouTube URL: https://www.youtube.com/watch?v=HZFmqeKEJwI


