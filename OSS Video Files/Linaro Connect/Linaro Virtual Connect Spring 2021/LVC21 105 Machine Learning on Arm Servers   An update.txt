Title: LVC21 105 Machine Learning on Arm Servers   An update
Publication date: 2021-04-15
Playlist: Linaro Virtual Connect Spring 2021
Description: 
	Interested in running Machine Learning workloads on Arm servers? Do you want to make the ML software stack faster on Arm? In this talk, the speaker will talk about the status, roadmap, and areas of improvement when using TensorFlow and PyTorch on Arm servers. The speaker will also cover key open source projects - Eigen, OneDNN, ACL that aid in improving framework performance
Captions: 
	00:00:04,560 --> 00:00:08,400
hello everybody this is

00:00:05,920 --> 00:00:09,040
uh ashok bhatt i'm senior product

00:00:08,400 --> 00:00:10,719
manager at

00:00:09,040 --> 00:00:12,400
arm i'm going to give a machine learning

00:00:10,719 --> 00:00:16,560
on arm servers and update

00:00:12,400 --> 00:00:18,960
in this particular session

00:00:16,560 --> 00:00:20,880
so just as a quick heads up what it is

00:00:18,960 --> 00:00:22,400
being covered and what's not

00:00:20,880 --> 00:00:25,439
primarily i'm going to talk about

00:00:22,400 --> 00:00:28,560
tensorflow and pytorch as frameworks

00:00:25,439 --> 00:00:30,560
and uh within that we'll talk about what

00:00:28,560 --> 00:00:33,120
is the end goal i mean why do we want

00:00:30,560 --> 00:00:35,040
tensorflow on pytorch and armed servers

00:00:33,120 --> 00:00:36,640
what's the end goal with respect to on

00:00:35,040 --> 00:00:38,960
cpu inference

00:00:36,640 --> 00:00:40,640
what's available today for end users and

00:00:38,960 --> 00:00:42,320
what's being worked on to improve

00:00:40,640 --> 00:00:44,239
usability and performance and how do you

00:00:42,320 --> 00:00:47,440
get involved okay

00:00:44,239 --> 00:00:49,840
what's not being covered is

00:00:47,440 --> 00:00:52,239
the training use case and kind of arm

00:00:49,840 --> 00:00:53,360
plus gpu which typically is the training

00:00:52,239 --> 00:00:54,879
use case

00:00:53,360 --> 00:00:56,879
and benchmarks and performance

00:00:54,879 --> 00:00:59,120
comparisons this is more of

00:00:56,879 --> 00:01:01,680
how do you get started on arm with

00:00:59,120 --> 00:01:03,600
machine learning rather than

00:01:01,680 --> 00:01:04,960
how do you make something absolutely

00:01:03,600 --> 00:01:08,880
fast and so on

00:01:04,960 --> 00:01:11,920
okay so on the

00:01:08,880 --> 00:01:12,960
cpu machine learning inference the

00:01:11,920 --> 00:01:16,400
primary goal

00:01:12,960 --> 00:01:17,360
we have is we need to get to where other

00:01:16,400 --> 00:01:21,200
architectures are

00:01:17,360 --> 00:01:24,240
so easy to use best-in-class performance

00:01:21,200 --> 00:01:26,640
inference solution on arm servers

00:01:24,240 --> 00:01:28,479
with ml specific cpu features now we'll

00:01:26,640 --> 00:01:29,439
go through one by one what each of them

00:01:28,479 --> 00:01:31,680
mean

00:01:29,439 --> 00:01:32,720
so when we say easy to use what it

00:01:31,680 --> 00:01:36,240
actually means is

00:01:32,720 --> 00:01:38,000
um typically a container image or python

00:01:36,240 --> 00:01:40,799
packages you should be able to do a pip

00:01:38,000 --> 00:01:42,720
install or it should be part of anaconda

00:01:40,799 --> 00:01:44,240
and when it comes to containers similar

00:01:42,720 --> 00:01:47,200
to what nvidia has

00:01:44,240 --> 00:01:49,920
in terms of the ngc you just can pull a

00:01:47,200 --> 00:01:52,720
container image and start using it

00:01:49,920 --> 00:01:54,320
that's the kind of easiest way for end

00:01:52,720 --> 00:01:56,320
users to get started

00:01:54,320 --> 00:01:58,240
and then you have the popular ml

00:01:56,320 --> 00:01:58,799
frameworks support arm as a first class

00:01:58,240 --> 00:02:02,640
citizen

00:01:58,799 --> 00:02:03,360
today all of them mostly use x86 as

00:02:02,640 --> 00:02:06,240
their

00:02:03,360 --> 00:02:07,040
primary cpu so the goal is to get them

00:02:06,240 --> 00:02:10,080
to support

00:02:07,040 --> 00:02:12,400
arm as well in addition to x86

00:02:10,080 --> 00:02:14,080
so that's the easy to use side and then

00:02:12,400 --> 00:02:16,319
you have wide variety of inference

00:02:14,080 --> 00:02:19,520
workloads and inference is a kind of a

00:02:16,319 --> 00:02:21,520
big continuum of workloads we've started

00:02:19,520 --> 00:02:22,239
with image classification but you can

00:02:21,520 --> 00:02:25,040
think of

00:02:22,239 --> 00:02:26,560
object detection language processing

00:02:25,040 --> 00:02:28,720
like bird models and so on

00:02:26,560 --> 00:02:30,400
so that's the wide variety of inference

00:02:28,720 --> 00:02:31,920
workloads

00:02:30,400 --> 00:02:34,080
and in terms of arm architecture

00:02:31,920 --> 00:02:35,599
features there are a bunch of

00:02:34,080 --> 00:02:38,160
arm architecture features which have

00:02:35,599 --> 00:02:40,239
been added with machine learning in mind

00:02:38,160 --> 00:02:41,040
so for example we have dot product in

00:02:40,239 --> 00:02:44,400
date

00:02:41,040 --> 00:02:46,640
option b float 16 was

00:02:44,400 --> 00:02:47,680
added and it's going to be available in

00:02:46,640 --> 00:02:50,720
zeus family

00:02:47,680 --> 00:02:52,640
and into family soon and we also have

00:02:50,720 --> 00:02:54,319
the matrix multiplier extension which

00:02:52,640 --> 00:02:57,760
was added

00:02:54,319 --> 00:03:00,080
in 8.6 if i remember correctly arm 8.6

00:02:57,760 --> 00:03:01,200
variant of the architecture and it is

00:03:00,080 --> 00:03:04,879
part of

00:03:01,200 --> 00:03:07,280
v1 and also n2

00:03:04,879 --> 00:03:08,720
that's going to come hopefully later

00:03:07,280 --> 00:03:11,760
this year

00:03:08,720 --> 00:03:12,879
sve also provides an advantage in the

00:03:11,760 --> 00:03:15,760
sense that

00:03:12,879 --> 00:03:17,200
zeus for example will have a 256 bit

00:03:15,760 --> 00:03:20,000
wide

00:03:17,200 --> 00:03:21,599
vector unit which helps typically in a

00:03:20,000 --> 00:03:23,760
machine learning operation

00:03:21,599 --> 00:03:25,200
okay so that's the arm architecture

00:03:23,760 --> 00:03:27,360
features that we are talking about

00:03:25,200 --> 00:03:30,879
and finally on the latest hardware so

00:03:27,360 --> 00:03:32,400
there's a newer family which is n1 n2 v1

00:03:30,879 --> 00:03:35,280
but we also have partner codes like

00:03:32,400 --> 00:03:37,760
fujitsu's a64fx

00:03:35,280 --> 00:03:38,640
so our overall goal is how do you get

00:03:37,760 --> 00:03:42,080
easy to use

00:03:38,640 --> 00:03:43,680
best-in-class high-performing influence

00:03:42,080 --> 00:03:45,120
solution for arm servers using all the

00:03:43,680 --> 00:03:48,720
cpu features we'll go through

00:03:45,120 --> 00:03:51,120
one by one in detail later

00:03:48,720 --> 00:03:52,640
so first let me just cover the packages

00:03:51,120 --> 00:03:54,239
and images story right the

00:03:52,640 --> 00:03:55,920
ease of use of getting the container

00:03:54,239 --> 00:03:58,560
images and

00:03:55,920 --> 00:03:58,959
on the packages so in terms of ready to

00:03:58,560 --> 00:04:02,799
use

00:03:58,959 --> 00:04:04,640
packages i'm happy to update you i mean

00:04:02,799 --> 00:04:06,879
last time when we talked about

00:04:04,640 --> 00:04:08,319
we didn't have any official release

00:04:06,879 --> 00:04:10,400
today

00:04:08,319 --> 00:04:12,480
tensorflow snapshot packages are

00:04:10,400 --> 00:04:16,239
available via linaro

00:04:12,480 --> 00:04:18,720
you can see 1.15 and 2.4 is available

00:04:16,239 --> 00:04:19,840
and in terms of pytorch we have official

00:04:18,720 --> 00:04:23,520
support now so

00:04:19,840 --> 00:04:26,720
if you go to pytorch official website

00:04:23,520 --> 00:04:29,840
and you can get pytosh 1.8

00:04:26,720 --> 00:04:32,160
binary wheel for arm and similarly you

00:04:29,840 --> 00:04:34,160
can provide the latest development wheel

00:04:32,160 --> 00:04:38,160
for pytorch so pytorch happens to be the

00:04:34,160 --> 00:04:42,240
first major framework to support er 64

00:04:38,160 --> 00:04:44,240
as a primary uh platform

00:04:42,240 --> 00:04:46,080
in terms of next steps there we still

00:04:44,240 --> 00:04:48,720
have to get to

00:04:46,080 --> 00:04:49,840
a position where tensorflow packages are

00:04:48,720 --> 00:04:52,720
available

00:04:49,840 --> 00:04:53,440
for end users outside of things like

00:04:52,720 --> 00:04:57,440
lenaro

00:04:53,440 --> 00:05:00,800
or community build so think of

00:04:57,440 --> 00:05:04,240
pip or conda those kind of areas

00:05:00,800 --> 00:05:08,160
how do we get the r64 packages

00:05:04,240 --> 00:05:08,840
okay so moving further on docker recipes

00:05:08,160 --> 00:05:13,039
now

00:05:08,840 --> 00:05:14,400
what the docker recipe project that we

00:05:13,039 --> 00:05:18,000
have

00:05:14,400 --> 00:05:19,840
is as it's a recipe

00:05:18,000 --> 00:05:21,919
to the docker container so if you want

00:05:19,840 --> 00:05:24,800
to create containers of your own

00:05:21,919 --> 00:05:26,160
with various options of kind of

00:05:24,800 --> 00:05:28,320
different back-ends

00:05:26,160 --> 00:05:30,880
then the docker recipes help you this is

00:05:28,320 --> 00:05:33,280
kind of arm-provided

00:05:30,880 --> 00:05:34,240
locations where we provide the latest

00:05:33,280 --> 00:05:36,800
and the greatest

00:05:34,240 --> 00:05:38,320
in terms of performance and enablement

00:05:36,800 --> 00:05:40,400
so you can see two

00:05:38,320 --> 00:05:41,759
particular directories within tool

00:05:40,400 --> 00:05:44,240
solutions hosted by

00:05:41,759 --> 00:05:45,360
arm one for tensorflow and one for

00:05:44,240 --> 00:05:46,800
pytorch

00:05:45,360 --> 00:05:49,280
right now they have slightly older

00:05:46,800 --> 00:05:52,400
versions of tensorflow and pytorch so

00:05:49,280 --> 00:05:57,240
for example 2.3 tensorflow and

00:05:52,400 --> 00:06:01,360
pytorch 1.6 soon we will upgrade them to

00:05:57,240 --> 00:06:03,199
1.8 in case of my torch and 2.4 or 2.5

00:06:01,360 --> 00:06:05,440
in case of tensorflow

00:06:03,199 --> 00:06:07,440
so that's kind of the next steps there

00:06:05,440 --> 00:06:10,000
the main thing as an end user

00:06:07,440 --> 00:06:11,680
is you have the docker images itself so

00:06:10,000 --> 00:06:13,120
last time when we spoke in september

00:06:11,680 --> 00:06:15,039
this was still a

00:06:13,120 --> 00:06:17,039
kind of a concept it was not available

00:06:15,039 --> 00:06:19,520
as part of lenovo docker hub

00:06:17,039 --> 00:06:20,319
i'm happy to provide you an update

00:06:19,520 --> 00:06:22,960
saying that

00:06:20,319 --> 00:06:23,840
tensorflow and pi torch both images are

00:06:22,960 --> 00:06:26,319
now available

00:06:23,840 --> 00:06:27,840
in lenora docker hub all you have to do

00:06:26,319 --> 00:06:30,080
is just do a

00:06:27,840 --> 00:06:31,120
docker pull and you get the tensorflow

00:06:30,080 --> 00:06:32,479
with all the

00:06:31,120 --> 00:06:35,440
different variants so these are

00:06:32,479 --> 00:06:38,479
available for newer sn1 right now

00:06:35,440 --> 00:06:41,520
we will try to upgrade them to have

00:06:38,479 --> 00:06:43,600
a 64 fx variants and so on in the

00:06:41,520 --> 00:06:47,280
upcoming future

00:06:43,600 --> 00:06:48,880
plus the obvious updating the images to

00:06:47,280 --> 00:06:52,319
the newer version like for example this

00:06:48,880 --> 00:06:54,319
is also a python 1.6 and 2.3 tensorflow

00:06:52,319 --> 00:06:56,240
we won't upgrade them to the later

00:06:54,319 --> 00:06:58,080
version

00:06:56,240 --> 00:06:59,599
so that's on the kind of general

00:06:58,080 --> 00:07:02,240
availability side

00:06:59,599 --> 00:07:04,400
quickly go to what does what does best

00:07:02,240 --> 00:07:08,800
in class performance mean for

00:07:04,400 --> 00:07:10,880
these projects machine learning for

00:07:08,800 --> 00:07:12,160
most purposes is collection of open

00:07:10,880 --> 00:07:13,440
source projects

00:07:12,160 --> 00:07:15,599
so at the highest level you have

00:07:13,440 --> 00:07:17,120
tensorflow and bytorch

00:07:15,599 --> 00:07:19,520
which are the frameworks and then there

00:07:17,120 --> 00:07:21,280
are a whole bunch of libraries which

00:07:19,520 --> 00:07:23,120
work underneath and i've listed the

00:07:21,280 --> 00:07:26,000
important ones but there are

00:07:23,120 --> 00:07:27,280
many many more libraries which are

00:07:26,000 --> 00:07:30,479
important to

00:07:27,280 --> 00:07:33,440
income when you think about performance

00:07:30,479 --> 00:07:35,199
the obvious ones tensorflow and pytorch

00:07:33,440 --> 00:07:36,720
uh tensorflow and python

00:07:35,199 --> 00:07:39,759
i think i guess their neck and neck in

00:07:36,720 --> 00:07:41,840
terms of machine learning framework

00:07:39,759 --> 00:07:42,800
uh depending on who you ask they will

00:07:41,840 --> 00:07:46,319
say one is more

00:07:42,800 --> 00:07:47,759
popular than the other and both of them

00:07:46,319 --> 00:07:49,360
are similar in the sense that they have

00:07:47,759 --> 00:07:50,639
multiple backends so for example

00:07:49,360 --> 00:07:53,759
tensorflow

00:07:50,639 --> 00:07:55,840
has eigengbp backend for

00:07:53,759 --> 00:07:57,280
fp32 so this is something that google

00:07:55,840 --> 00:08:00,240
teams themselves

00:07:57,280 --> 00:08:03,680
use and google along with intel support

00:08:00,240 --> 00:08:07,280
one dnn backend for x86 which has

00:08:03,680 --> 00:08:08,960
a blas fallback or you can have direct

00:08:07,280 --> 00:08:12,080
kernels so one dnn

00:08:08,960 --> 00:08:15,440
is part of the one api family for intel

00:08:12,080 --> 00:08:18,960
on the pi dot side on the x86 you have

00:08:15,440 --> 00:08:20,639
many many more um back-ends

00:08:18,960 --> 00:08:22,720
so specifically i want to call out the

00:08:20,639 --> 00:08:24,319
fb gem back-end so this is the back-end

00:08:22,720 --> 00:08:26,160
that gets used

00:08:24,319 --> 00:08:27,520
when you run when you run quantized

00:08:26,160 --> 00:08:29,759
inference

00:08:27,520 --> 00:08:30,720
and open blast is the glass back end

00:08:29,759 --> 00:08:32,560
that

00:08:30,720 --> 00:08:34,719
you fall back if you need some blast

00:08:32,560 --> 00:08:35,839
routines one dna has also been

00:08:34,719 --> 00:08:39,360
integrated but

00:08:35,839 --> 00:08:40,080
for example in date support the 8-bit

00:08:39,360 --> 00:08:42,240
integer support

00:08:40,080 --> 00:08:43,919
is not integrated yet so that is still

00:08:42,240 --> 00:08:46,640
pending whereas in the case of

00:08:43,919 --> 00:08:49,760
tensorflow it is already integrated

00:08:46,640 --> 00:08:50,240
now the pytorch also has back-ends like

00:08:49,760 --> 00:08:52,560
i said

00:08:50,240 --> 00:08:54,720
it supports arm right now as a

00:08:52,560 --> 00:08:57,279
first-class architecture

00:08:54,720 --> 00:08:58,480
and it has two back ends q and n pack

00:08:57,279 --> 00:09:00,880
happens to be the

00:08:58,480 --> 00:09:02,800
8-bit integer back-end and open blast is

00:09:00,880 --> 00:09:07,200
the back-end for

00:09:02,800 --> 00:09:09,600
the blas moving further

00:09:07,200 --> 00:09:11,360
the projects that make these ml

00:09:09,600 --> 00:09:14,560
frameworks go faster

00:09:11,360 --> 00:09:16,880
eigen is a general c plus plus template

00:09:14,560 --> 00:09:19,200
library it's a linear algebra library it

00:09:16,880 --> 00:09:21,040
provides

00:09:19,200 --> 00:09:23,600
basic building blocks like vectors

00:09:21,040 --> 00:09:26,240
matrices and related algorithms

00:09:23,600 --> 00:09:28,000
tensorflow heavily uses eigen for

00:09:26,240 --> 00:09:28,320
representing internal data structures

00:09:28,000 --> 00:09:31,440
and

00:09:28,320 --> 00:09:35,360
their operations uh it

00:09:31,440 --> 00:09:36,720
it's gbp so there's a gbp kernel which

00:09:35,360 --> 00:09:39,120
which provides the basic matrix

00:09:36,720 --> 00:09:40,720
multiplication it's a default cpu

00:09:39,120 --> 00:09:43,360
backend for fp32

00:09:40,720 --> 00:09:45,040
so if you're doing fp32 inference using

00:09:43,360 --> 00:09:48,399
tensorflow by default

00:09:45,040 --> 00:09:50,560
it goes to eigengbp kernel eigen as a

00:09:48,399 --> 00:09:52,399
project is important because

00:09:50,560 --> 00:09:54,399
it's not just a gbp kernel i mean

00:09:52,399 --> 00:09:56,959
tensorflow for all practical purposes

00:09:54,399 --> 00:09:59,680
uses it for representing vectors

00:09:56,959 --> 00:10:01,600
matrices and so on

00:09:59,680 --> 00:10:03,680
the other three libraries that i want to

00:10:01,600 --> 00:10:04,240
call out is the one dnn library so this

00:10:03,680 --> 00:10:06,800
is the

00:10:04,240 --> 00:10:08,399
intel's ml open source library it's an

00:10:06,800 --> 00:10:11,120
acceleration library

00:10:08,399 --> 00:10:12,320
it's not directly used by the end users

00:10:11,120 --> 00:10:14,320
mostly it's uh

00:10:12,320 --> 00:10:16,320
integrated with all the frameworks so

00:10:14,320 --> 00:10:16,959
you think of tensorflow pytorch and so

00:10:16,320 --> 00:10:19,920
on

00:10:16,959 --> 00:10:21,920
it's integrated with that it has support

00:10:19,920 --> 00:10:23,279
for ar64 right now it's stacked as

00:10:21,920 --> 00:10:25,920
experimental

00:10:23,279 --> 00:10:27,519
uh because we've just recently added

00:10:25,920 --> 00:10:28,640
both fujitsu and arm have been

00:10:27,519 --> 00:10:31,440
contributing

00:10:28,640 --> 00:10:32,880
to this project at this point in time

00:10:31,440 --> 00:10:35,519
arm compute library

00:10:32,880 --> 00:10:37,519
is arms open source acceleration library

00:10:35,519 --> 00:10:39,040
so this was started with edge and mobile

00:10:37,519 --> 00:10:41,920
use cases in mind

00:10:39,040 --> 00:10:43,920
but it has a lot of high level operators

00:10:41,920 --> 00:10:45,519
in the future slides you will see

00:10:43,920 --> 00:10:47,279
what exactly it has and what it doesn't

00:10:45,519 --> 00:10:51,680
and how it's being used in

00:10:47,279 --> 00:10:53,519
server open blast is quite a common

00:10:51,680 --> 00:10:55,200
open source blast back-end so some of

00:10:53,519 --> 00:10:58,320
the matrix operations that

00:10:55,200 --> 00:11:01,440
machine learning frameworks use

00:10:58,320 --> 00:11:03,920
they end up as the typical hpc

00:11:01,440 --> 00:11:05,680
style blast operations and open blast

00:11:03,920 --> 00:11:08,320
happens to be very high performing

00:11:05,680 --> 00:11:10,240
in the case of arm so that is another

00:11:08,320 --> 00:11:12,720
key project

00:11:10,240 --> 00:11:14,000
quickly jumping to arm compute library

00:11:12,720 --> 00:11:16,959
this is a library

00:11:14,000 --> 00:11:18,000
which is a permissive mit open source

00:11:16,959 --> 00:11:21,040
license

00:11:18,000 --> 00:11:22,079
it's hosted by mlplatform.org as i

00:11:21,040 --> 00:11:23,760
understand

00:11:22,079 --> 00:11:26,160
and it has a collection of low-level

00:11:23,760 --> 00:11:26,959
functions for arm cpu and gpu and i say

00:11:26,160 --> 00:11:29,279
gpu

00:11:26,959 --> 00:11:30,720
this is the gpus that are typically

00:11:29,279 --> 00:11:34,640
available in

00:11:30,720 --> 00:11:37,440
mobile devices the mali series of gpus

00:11:34,640 --> 00:11:39,120
and it is used to accelerate arm and n

00:11:37,440 --> 00:11:42,160
which is arms inference engine

00:11:39,120 --> 00:11:45,200
okay so this is the solution that we

00:11:42,160 --> 00:11:46,880
we had or we still have for kind of the

00:11:45,200 --> 00:11:49,920
edge use cases

00:11:46,880 --> 00:11:53,120
how we using it in

00:11:49,920 --> 00:11:54,639
cloud is let me show the tensorflow

00:11:53,120 --> 00:11:57,839
software stack

00:11:54,639 --> 00:11:59,440
is a quite an important diagram so let

00:11:57,839 --> 00:12:02,800
me explain this diagram

00:11:59,440 --> 00:12:06,800
so in a typical ml inference workload

00:12:02,800 --> 00:12:10,800
tensorflow by default uses the gray path

00:12:06,800 --> 00:12:13,040
which is the in eigengbp kind of path

00:12:10,800 --> 00:12:16,160
and you end up with the hardware i mean

00:12:13,040 --> 00:12:19,440
the same path is followed for even x86

00:12:16,160 --> 00:12:21,519
what we have done is we have integrated

00:12:19,440 --> 00:12:24,639
arm compute library

00:12:21,519 --> 00:12:27,200
into one dnn in the sense that when

00:12:24,639 --> 00:12:29,519
tensorflow calls into one dnn one dnn

00:12:27,200 --> 00:12:30,079
uses arm compute library operators which

00:12:29,519 --> 00:12:33,680
have been

00:12:30,079 --> 00:12:35,600
optimized for arm cortex a processor the

00:12:33,680 --> 00:12:37,440
64 bit arm processors

00:12:35,600 --> 00:12:39,760
so that's what we have done so in other

00:12:37,440 --> 00:12:43,200
words we have

00:12:39,760 --> 00:12:46,800
arm we have exploited we have

00:12:43,200 --> 00:12:47,279
used our investment in kind of mobile

00:12:46,800 --> 00:12:52,480
edge

00:12:47,279 --> 00:12:55,600
devices and use the compute library to

00:12:52,480 --> 00:12:57,440
make the one dnn and in in return

00:12:55,600 --> 00:12:59,440
tensorflow go fast

00:12:57,440 --> 00:13:00,560
so even though i don't i don't have

00:12:59,440 --> 00:13:03,920
numbers to share

00:13:00,560 --> 00:13:06,560
so i'm happy to say that this path the

00:13:03,920 --> 00:13:09,839
1dn and arm compute library path

00:13:06,560 --> 00:13:13,120
is faster than the default path by a

00:13:09,839 --> 00:13:16,160
reasonable margin right now and this is

00:13:13,120 --> 00:13:19,440
this is going to be

00:13:16,160 --> 00:13:21,279
hopefully default soon okay so in terms

00:13:19,440 --> 00:13:23,839
of timeline you can look at the

00:13:21,279 --> 00:13:25,920
kind of tensorflow timeline tensorflow

00:13:23,839 --> 00:13:28,560
2.4

00:13:25,920 --> 00:13:32,000
version which was released in december

00:13:28,560 --> 00:13:35,200
it only had eigen as a default path

00:13:32,000 --> 00:13:38,079
in 2.5 which is expected end of march or

00:13:35,200 --> 00:13:39,120
uh and it's it's expected in april

00:13:38,079 --> 00:13:41,839
mostly

00:13:39,120 --> 00:13:44,480
it will have the one dnn arm compute

00:13:41,839 --> 00:13:46,639
library fb32 path

00:13:44,480 --> 00:13:49,040
and in the future so this tensorflow

00:13:46,639 --> 00:13:52,399
roughly follows a quarterly cycle

00:13:49,040 --> 00:13:56,480
so we expect 2.6 and 2.7 to end up

00:13:52,399 --> 00:13:59,680
in q2 and q3 of this calendar year

00:13:56,480 --> 00:14:02,560
and within them we will

00:13:59,680 --> 00:14:05,680
enhance the performance in the case of

00:14:02,560 --> 00:14:07,920
2.6 and in case of 2.7 we expect

00:14:05,680 --> 00:14:09,279
also to add the b float 16. so i'll

00:14:07,920 --> 00:14:13,920
cover the weak load 16

00:14:09,279 --> 00:14:16,079
in more detail in the further slides

00:14:13,920 --> 00:14:18,720
so coming to pi torch this is where i

00:14:16,079 --> 00:14:21,760
was talking about how one dnn is quite

00:14:18,720 --> 00:14:21,760
good for uh

00:14:21,839 --> 00:14:26,720
multiple frameworks so you can see that

00:14:23,920 --> 00:14:28,959
in the default path pytorch goes via

00:14:26,720 --> 00:14:30,240
open blast or qn pack open blast for

00:14:28,959 --> 00:14:33,440
mostly fp32

00:14:30,240 --> 00:14:35,760
or operations where blast is involved

00:14:33,440 --> 00:14:36,880
whereas qn and pack is mainly used for

00:14:35,760 --> 00:14:40,079
eight bit integer

00:14:36,880 --> 00:14:42,000
operations and within one dnn

00:14:40,079 --> 00:14:44,800
again whatever work we have done is kind

00:14:42,000 --> 00:14:48,320
of integrated into arm compute library

00:14:44,800 --> 00:14:49,600
it did not get into the 1.8 release the

00:14:48,320 --> 00:14:51,360
one point rate release which got

00:14:49,600 --> 00:14:53,680
released earlier this month

00:14:51,360 --> 00:14:56,240
that was the first one which had ar 64

00:14:53,680 --> 00:14:58,800
support the 64-bit arm support

00:14:56,240 --> 00:15:00,959
we expect the arm compute library path

00:14:58,800 --> 00:15:03,440
to be integrated into 1.9

00:15:00,959 --> 00:15:04,720
which we expect in next three to four

00:15:03,440 --> 00:15:07,760
months time frame

00:15:04,720 --> 00:15:10,000
so roughly called it q2 calendar 21 but

00:15:07,760 --> 00:15:11,920
yeah next three to six months these are

00:15:10,000 --> 00:15:13,440
all arms estimates i mean we don't have

00:15:11,920 --> 00:15:15,279
an official

00:15:13,440 --> 00:15:16,800
roadmap from either the pytorch or the

00:15:15,279 --> 00:15:18,079
tensorflow team so this is arms

00:15:16,800 --> 00:15:19,760
estimates of

00:15:18,079 --> 00:15:22,000
when the next versions are likely to

00:15:19,760 --> 00:15:24,240
come in the future

00:15:22,000 --> 00:15:25,360
um don't know it's going to be called

00:15:24,240 --> 00:15:28,800
1.10

00:15:25,360 --> 00:15:33,040
or it'll be 2.0 whenever that is

00:15:28,800 --> 00:15:33,040
we expect also add b float 16.

00:15:33,519 --> 00:15:40,000
so finally on the data type

00:15:36,560 --> 00:15:42,320
support so this is the

00:15:40,000 --> 00:15:43,600
important bit you'll see in machine

00:15:42,320 --> 00:15:46,160
learning where

00:15:43,600 --> 00:15:48,000
you can have the models either in fp32

00:15:46,160 --> 00:15:52,399
or you can have it in in 16

00:15:48,000 --> 00:15:54,079
or you can have it in b float 16 and

00:15:52,399 --> 00:15:56,480
i mean it's all kinds of combinations

00:15:54,079 --> 00:15:59,199
are possible how do you support these

00:15:56,480 --> 00:16:00,320
and these can have a huge difference in

00:15:59,199 --> 00:16:02,800
terms of performance they can have a

00:16:00,320 --> 00:16:05,440
huge impact in terms of performance

00:16:02,800 --> 00:16:07,759
so let me talk through each of them so

00:16:05,440 --> 00:16:08,240
for fp32 which is a 32-bit floating

00:16:07,759 --> 00:16:10,800
point

00:16:08,240 --> 00:16:11,920
the most common widely used type

00:16:10,800 --> 00:16:14,560
tensorflow

00:16:11,920 --> 00:16:15,519
today uses eigen as a back-end so that

00:16:14,560 --> 00:16:19,440
is kind of

00:16:15,519 --> 00:16:21,120
available we can use it for 1dn acl

00:16:19,440 --> 00:16:24,720
back-end i expect

00:16:21,120 --> 00:16:25,440
tensorflow 2.5 to be the version which

00:16:24,720 --> 00:16:27,759
will have

00:16:25,440 --> 00:16:29,920
support for this fully it is already

00:16:27,759 --> 00:16:31,440
available via the docker recipes and

00:16:29,920 --> 00:16:33,839
docker images

00:16:31,440 --> 00:16:35,040
but if you want to really get it via the

00:16:33,839 --> 00:16:37,759
tensorflow

00:16:35,040 --> 00:16:40,880
official packages then official release

00:16:37,759 --> 00:16:43,759
then it's likely to be

00:16:40,880 --> 00:16:44,959
i mean april time frame that you should

00:16:43,759 --> 00:16:47,440
see this

00:16:44,959 --> 00:16:49,519
and the pi torque side fv32 is already

00:16:47,440 --> 00:16:52,000
supported using open blast

00:16:49,519 --> 00:16:52,880
the acl path will be enabled in the next

00:16:52,000 --> 00:16:56,480
quarter

00:16:52,880 --> 00:16:58,320
so expecting pytorch 1.9 to have support

00:16:56,480 --> 00:17:01,839
for this

00:16:58,320 --> 00:17:04,319
in terms of integer 8 now integer 8 is

00:17:01,839 --> 00:17:07,360
the quantization

00:17:04,319 --> 00:17:08,240
paths and the quantization path is

00:17:07,360 --> 00:17:10,799
different between

00:17:08,240 --> 00:17:12,400
arm and intel because of the hardware i

00:17:10,799 --> 00:17:14,000
mean quantization is quite hardware

00:17:12,400 --> 00:17:17,679
specific

00:17:14,000 --> 00:17:20,400
it is not universal like fp32

00:17:17,679 --> 00:17:21,360
so in the case of tensorflow today it

00:17:20,400 --> 00:17:25,679
supports tf

00:17:21,360 --> 00:17:28,319
light so tf light has been designed

00:17:25,679 --> 00:17:29,360
uh tf lights quantization has been

00:17:28,319 --> 00:17:31,919
designed with

00:17:29,360 --> 00:17:33,360
kind of arm in mind the edge that's the

00:17:31,919 --> 00:17:35,200
kind of default

00:17:33,360 --> 00:17:36,559
quantization support that is available

00:17:35,200 --> 00:17:38,880
in tensorflow

00:17:36,559 --> 00:17:40,799
that is available on arm and it works

00:17:38,880 --> 00:17:42,640
well you've tested it on

00:17:40,799 --> 00:17:44,799
on our side and it works well even on

00:17:42,640 --> 00:17:45,200
the server so even though tensorflow

00:17:44,799 --> 00:17:47,919
lite

00:17:45,200 --> 00:17:49,120
has been originally aimed at edge and

00:17:47,919 --> 00:17:51,520
mobile devices

00:17:49,120 --> 00:17:53,280
you if you take tensorflow lite engine

00:17:51,520 --> 00:17:54,640
and if you run it on a server we are

00:17:53,280 --> 00:17:56,880
seeing good scaling

00:17:54,640 --> 00:17:59,039
even in large core count like 48 cores

00:17:56,880 --> 00:18:01,840
and 64 cores and so on

00:17:59,039 --> 00:18:03,600
so um if you are interested in quantized

00:18:01,840 --> 00:18:05,600
running quantized models

00:18:03,600 --> 00:18:06,960
i would strongly encourage you to try tf

00:18:05,600 --> 00:18:10,400
light route it's quite

00:18:06,960 --> 00:18:13,919
it's quite powerful on the

00:18:10,400 --> 00:18:16,559
one dnn route one of the disadvantages

00:18:13,919 --> 00:18:19,360
of using one dnn is 1dn assumes

00:18:16,559 --> 00:18:22,400
quantization

00:18:19,360 --> 00:18:24,000
which is x86 friendly so we have

00:18:22,400 --> 00:18:27,200
integrated some operators

00:18:24,000 --> 00:18:28,880
into one dnn that make

00:18:27,200 --> 00:18:30,880
if you just take a model that has been

00:18:28,880 --> 00:18:32,080
previously quantized for x86 and if you

00:18:30,880 --> 00:18:35,039
try to run it

00:18:32,080 --> 00:18:36,720
on tensorflow with one dnn as a back end

00:18:35,039 --> 00:18:38,400
some of the operators will fall back to

00:18:36,720 --> 00:18:39,360
arm compute library and they'll go

00:18:38,400 --> 00:18:41,520
faster

00:18:39,360 --> 00:18:42,720
but a whole bunch of them don't have an

00:18:41,520 --> 00:18:45,760
implementation because

00:18:42,720 --> 00:18:46,320
it's quite x86 specific we plan to

00:18:45,760 --> 00:18:48,640
improve

00:18:46,320 --> 00:18:50,799
some of the performances of performance

00:18:48,640 --> 00:18:54,880
in the next quarter

00:18:50,799 --> 00:18:57,280
but broadly we would expect

00:18:54,880 --> 00:18:59,600
end users not to rely on the kind of 1d

00:18:57,280 --> 00:19:03,039
and end path for in date

00:18:59,600 --> 00:19:04,880
if you're doing 8-bit route then

00:19:03,039 --> 00:19:06,720
most likely you are going to use the tf

00:19:04,880 --> 00:19:09,520
lite or the qn pack

00:19:06,720 --> 00:19:10,640
as in the case of pytorch so with that

00:19:09,520 --> 00:19:13,600
in segway

00:19:10,640 --> 00:19:15,200
just going into pythos uh in pytorch

00:19:13,600 --> 00:19:16,240
there is q1 and pack which is the

00:19:15,200 --> 00:19:18,640
default backend

00:19:16,240 --> 00:19:20,400
introduced by facebook a couple of years

00:19:18,640 --> 00:19:22,720
ago and it it

00:19:20,400 --> 00:19:24,320
even though again like tf flight even

00:19:22,720 --> 00:19:26,320
though it's been originally written for

00:19:24,320 --> 00:19:28,080
mobile it scales really well for even

00:19:26,320 --> 00:19:29,360
server cores large number of cores we

00:19:28,080 --> 00:19:32,799
have tested it for 32

00:19:29,360 --> 00:19:36,240
48 64 and so on so

00:19:32,799 --> 00:19:39,440
we would recommend if you have

00:19:36,240 --> 00:19:42,640
a python fp32 model

00:19:39,440 --> 00:19:44,240
use the pytorch quantization scheme and

00:19:42,640 --> 00:19:46,000
you should be able to run it on top of q

00:19:44,240 --> 00:19:48,640
and pack if not

00:19:46,000 --> 00:19:50,160
raise an issue then we are monitoring

00:19:48,640 --> 00:19:53,440
python repo as well and

00:19:50,160 --> 00:19:56,559
python stream also potentially help

00:19:53,440 --> 00:19:58,160
in figuring out for the same reasons i

00:19:56,559 --> 00:20:02,320
was talking about tensorflow

00:19:58,160 --> 00:20:03,039
we don't plan to support intake via 1dn

00:20:02,320 --> 00:20:05,840
in route

00:20:03,039 --> 00:20:08,400
because one dnn happens to be quite x86

00:20:05,840 --> 00:20:11,520
quantization friendly

00:20:08,400 --> 00:20:15,520
on the b float 16 which is a new type uh

00:20:11,520 --> 00:20:17,120
it has the same kind of

00:20:15,520 --> 00:20:19,679
it's it's very very friendly if you're

00:20:17,120 --> 00:20:23,120
going from fp32 it's the brain float

00:20:19,679 --> 00:20:27,919
format originally developed or

00:20:23,120 --> 00:20:30,480
used by google team so tensorflow

00:20:27,919 --> 00:20:31,120
we don't have we don't have information

00:20:30,480 --> 00:20:32,799
yet on

00:20:31,120 --> 00:20:34,320
how tensorflow is going to support b

00:20:32,799 --> 00:20:37,440
float 16 models

00:20:34,320 --> 00:20:41,120
however we will use the b float 16

00:20:37,440 --> 00:20:43,120
model a b float 16 type to accelerate

00:20:41,120 --> 00:20:44,720
the fp32 model so if you use running

00:20:43,120 --> 00:20:47,919
fp32 models

00:20:44,720 --> 00:20:50,000
on top of one dnacl which is a typical

00:20:47,919 --> 00:20:53,360
common use case

00:20:50,000 --> 00:20:56,480
acl does allow you to run

00:20:53,360 --> 00:20:59,520
quantization convolution

00:20:56,480 --> 00:21:01,360
using b float16 so we will enable

00:20:59,520 --> 00:21:03,760
under the hood both in case of

00:21:01,360 --> 00:21:05,679
tensorflow and by torch

00:21:03,760 --> 00:21:08,480
using b float 16 under the hood to

00:21:05,679 --> 00:21:10,480
accelerate fp32 models

00:21:08,480 --> 00:21:11,520
there may be a case once frameworks

00:21:10,480 --> 00:21:14,480
themselves support

00:21:11,520 --> 00:21:15,120
uh b float 16 we might we'll allow also

00:21:14,480 --> 00:21:18,320
support the b

00:21:15,120 --> 00:21:20,720
float 16 model but for now the plan is

00:21:18,320 --> 00:21:21,919
to accelerate fe 32 models using b float

00:21:20,720 --> 00:21:24,559
16.

00:21:21,919 --> 00:21:26,960
both of that should come in the second

00:21:24,559 --> 00:21:29,679
half of this year

00:21:26,960 --> 00:21:31,600
okay so with that let me quickly

00:21:29,679 --> 00:21:34,000
summarize

00:21:31,600 --> 00:21:36,559
where we are and where we are going uh i

00:21:34,000 --> 00:21:40,480
would strongly encourage

00:21:36,559 --> 00:21:42,640
users to try their sap images and

00:21:40,480 --> 00:21:44,320
provide feedback to us please if you

00:21:42,640 --> 00:21:46,080
have any particular applications that

00:21:44,320 --> 00:21:48,480
you want to try and arm

00:21:46,080 --> 00:21:50,159
try the docker images and recipes the

00:21:48,480 --> 00:21:51,600
links that were provided in the slide

00:21:50,159 --> 00:21:54,559
deck

00:21:51,600 --> 00:21:56,400
and there is a weekly public meeting

00:21:54,559 --> 00:21:59,360
that anybody can join meeting

00:21:56,400 --> 00:21:59,760
the link is provided here that happens

00:21:59,360 --> 00:22:02,559
uh

00:21:59,760 --> 00:22:03,520
weekly in the sense it's a bi-weekly

00:22:02,559 --> 00:22:06,559
every two weeks

00:22:03,520 --> 00:22:09,120
we have one in asia specific asia apac

00:22:06,559 --> 00:22:10,400
specific time slot and the other one

00:22:09,120 --> 00:22:12,880
happens in the us

00:22:10,400 --> 00:22:15,120
time slot so you have kind of covering

00:22:12,880 --> 00:22:17,600
both regions you have a meeting

00:22:15,120 --> 00:22:18,400
please do join that meeting and there we

00:22:17,600 --> 00:22:20,640
can

00:22:18,400 --> 00:22:23,039
help you out with any particular issues

00:22:20,640 --> 00:22:26,480
that you're facing or not

00:22:23,039 --> 00:22:27,919
with that i'm open to any questions

00:22:26,480 --> 00:22:30,320
that was the end of my presentation

00:22:27,919 --> 00:22:30,320
thank you

00:22:38,000 --> 00:22:42,000
okay if there are no further questions

00:22:39,760 --> 00:22:45,520
thanks for attending today

00:22:42,000 --> 00:22:48,240
hope to give you a more exciting update

00:22:45,520 --> 00:22:48,240
in six months time

00:22:52,840 --> 00:22:55,840
bye

00:22:59,840 --> 00:23:02,000

YouTube URL: https://www.youtube.com/watch?v=qat7vpVHGkA


