Title: LVC21-211 Boosting Application Performance on Arm Data Centers
Publication date: 2021-04-15
Playlist: Linaro Virtual Connect Spring 2021
Description: 
	With more players join the game, the opensource software ecosystem have been greatly enhanced during last year, over 30 new top opensource projects have built their Arm CI and will continue to dev, build and test on Arm platform. 

And that brings us to the next step - How good did they run? Are they competitive with other platforms? Are there any general rules to improve their performance on Arm platform?

In this session, we will share ideas and best practices on how can we imporve application performance on Arm data centers with both software and hardware techniques. We will cover fields including BigData, Databases and general libraries like glibc, gzip etc.
Captions: 
	00:00:03,760 --> 00:00:06,960
hi

00:00:04,319 --> 00:00:07,919
welcome to this session my name is jen

00:00:06,960 --> 00:00:11,440
yui jung

00:00:07,919 --> 00:00:13,360
i'm from huawei in the past two years

00:00:11,440 --> 00:00:15,360
our team have been working on promoting

00:00:13,360 --> 00:00:18,160
um data centers in various open source

00:00:15,360 --> 00:00:20,640
communities and projects

00:00:18,160 --> 00:00:22,480
the next 20 minutes i would like to

00:00:20,640 --> 00:00:23,279
share some of the ideas and the

00:00:22,480 --> 00:00:27,439
experience

00:00:23,279 --> 00:00:30,240
from our previous work this will include

00:00:27,439 --> 00:00:32,399
how we enable and optimize open source

00:00:30,240 --> 00:00:36,000
projects on our platform

00:00:32,399 --> 00:00:37,920
i hope that will be useful for arm users

00:00:36,000 --> 00:00:39,200
the session will be divided into three

00:00:37,920 --> 00:00:41,680
parts

00:00:39,200 --> 00:00:43,600
in the first part of this session i

00:00:41,680 --> 00:00:47,760
would like to have a recap on

00:00:43,600 --> 00:00:50,239
rmcis for the open source communities

00:00:47,760 --> 00:00:51,440
in a typical open source development

00:00:50,239 --> 00:00:53,840
workflow

00:00:51,440 --> 00:00:56,000
the contributor wants to contribute some

00:00:53,840 --> 00:00:58,559
codes to a project

00:00:56,000 --> 00:01:01,039
he or she has to write the codes and

00:00:58,559 --> 00:01:02,559
then build and test the codes on his or

00:01:01,039 --> 00:01:06,479
her local development

00:01:02,559 --> 00:01:08,720
environment after everything is checked

00:01:06,479 --> 00:01:10,640
he or she can then push the code to the

00:01:08,720 --> 00:01:14,560
project repository

00:01:10,640 --> 00:01:16,880
for example on github and the codes will

00:01:14,560 --> 00:01:19,920
also be built and tested there

00:01:16,880 --> 00:01:22,799
and if everything works okay the code

00:01:19,920 --> 00:01:25,520
might be merged to the mainstream

00:01:22,799 --> 00:01:26,080
after a certain period of time the

00:01:25,520 --> 00:01:28,240
project

00:01:26,080 --> 00:01:30,880
might build and released a package for

00:01:28,240 --> 00:01:32,880
users who use directly

00:01:30,880 --> 00:01:36,560
all those tests in the workflow

00:01:32,880 --> 00:01:39,759
guarantees the quality of the project

00:01:36,560 --> 00:01:40,720
since x86 is the major product in the

00:01:39,759 --> 00:01:43,119
market

00:01:40,720 --> 00:01:44,320
and the continuously high quality

00:01:43,119 --> 00:01:48,000
contributions from

00:01:44,320 --> 00:01:50,240
x86 users and vendors for the majority

00:01:48,000 --> 00:01:52,479
of the open source projects

00:01:50,240 --> 00:01:54,159
the development workflow mentioned above

00:01:52,479 --> 00:01:58,719
were built on x86

00:01:54,159 --> 00:02:01,920
platforms so for x86 users

00:01:58,719 --> 00:02:02,880
the quality is good for sure but for arm

00:02:01,920 --> 00:02:06,000
users

00:02:02,880 --> 00:02:09,200
or users in any other platforms

00:02:06,000 --> 00:02:11,440
there might be some problems as the

00:02:09,200 --> 00:02:13,520
workflow does not provide build and test

00:02:11,440 --> 00:02:15,920
on those platforms

00:02:13,520 --> 00:02:17,680
in order to provide a quality assurance

00:02:15,920 --> 00:02:19,680
on our platform

00:02:17,680 --> 00:02:21,040
the first thing we should do is to

00:02:19,680 --> 00:02:23,120
propose at

00:02:21,040 --> 00:02:24,160
arm resources to the development

00:02:23,120 --> 00:02:26,720
workflow

00:02:24,160 --> 00:02:29,360
and run the same test to achieve the

00:02:26,720 --> 00:02:32,640
same quality for arm users

00:02:29,360 --> 00:02:34,080
and that's exactly what we did in the

00:02:32,640 --> 00:02:36,239
last two years

00:02:34,080 --> 00:02:39,280
our team and other contributors have

00:02:36,239 --> 00:02:41,120
enabled mcis in over 60

00:02:39,280 --> 00:02:42,879
famous open source projects and

00:02:41,120 --> 00:02:45,519
communities

00:02:42,879 --> 00:02:46,400
those projects covered areas including

00:02:45,519 --> 00:02:50,160
big data

00:02:46,400 --> 00:02:53,920
database cloud and cloud native storage

00:02:50,160 --> 00:02:56,239
vibe ai libraries and two chains

00:02:53,920 --> 00:02:58,640
in order to provide those information to

00:02:56,239 --> 00:03:00,959
our contributors who wants to know which

00:02:58,640 --> 00:03:03,680
projects have mci's

00:03:00,959 --> 00:03:04,000
i have made a landscape page for all the

00:03:03,680 --> 00:03:08,080
open

00:03:04,000 --> 00:03:10,879
source projects that we have enabled mci

00:03:08,080 --> 00:03:12,720
if you are interested you can go to the

00:03:10,879 --> 00:03:15,200
following address

00:03:12,720 --> 00:03:16,000
you can click the project's logo it and

00:03:15,200 --> 00:03:18,959
it will

00:03:16,000 --> 00:03:20,800
jump to the mci status page for the

00:03:18,959 --> 00:03:23,599
project

00:03:20,800 --> 00:03:24,400
and of course this may not be the full

00:03:23,599 --> 00:03:28,000
list

00:03:24,400 --> 00:03:30,000
so please contact me or file an issue

00:03:28,000 --> 00:03:31,680
if you want to add more projects to the

00:03:30,000 --> 00:03:34,159
landscape

00:03:31,680 --> 00:03:37,040
if you are also interested in promoting

00:03:34,159 --> 00:03:40,319
mcis to the open source communities

00:03:37,040 --> 00:03:42,480
and want to be part of it there are some

00:03:40,319 --> 00:03:44,480
choices to do it

00:03:42,480 --> 00:03:46,879
you can donate on virtual machines

00:03:44,480 --> 00:03:49,920
directly to the projects

00:03:46,879 --> 00:03:51,680
cloud providers like huawei and aws now

00:03:49,920 --> 00:03:54,879
provide powerful arm

00:03:51,680 --> 00:03:56,799
virtual machines you can also use

00:03:54,879 --> 00:04:00,159
commercial ci providers

00:03:56,799 --> 00:04:02,239
like the tribal ci they now have armed

00:04:00,159 --> 00:04:05,599
resources and many projects

00:04:02,239 --> 00:04:06,959
have already using it you can also

00:04:05,599 --> 00:04:09,200
cooperate with

00:04:06,959 --> 00:04:10,239
the narrow developer cloud or the open

00:04:09,200 --> 00:04:12,400
lab

00:04:10,239 --> 00:04:14,000
there are the communities that working

00:04:12,400 --> 00:04:17,120
on promoting mcis for

00:04:14,000 --> 00:04:20,639
a very long time okay

00:04:17,120 --> 00:04:22,079
that's all for the recap after enabling

00:04:20,639 --> 00:04:24,560
the projects

00:04:22,079 --> 00:04:27,360
we start to think about how to make them

00:04:24,560 --> 00:04:30,320
run better on our platform

00:04:27,360 --> 00:04:32,560
in the next part i'd like to share some

00:04:30,320 --> 00:04:34,960
ideas we have tried

00:04:32,560 --> 00:04:37,759
i will first share some optimization

00:04:34,960 --> 00:04:39,160
tricks for general arm hardware

00:04:37,759 --> 00:04:42,479
and then i will share some

00:04:39,160 --> 00:04:43,840
vendor-specific optimizations so let's

00:04:42,479 --> 00:04:46,000
start

00:04:43,840 --> 00:04:48,800
the first optimization i would like to

00:04:46,000 --> 00:04:52,000
share is about spin locks

00:04:48,800 --> 00:04:54,880
according to wikipedia a spin lock

00:04:52,000 --> 00:04:57,120
is a busy waiting lock which causes a

00:04:54,880 --> 00:04:58,560
threat trying to acquire it to simply

00:04:57,120 --> 00:05:01,280
wait in a loop

00:04:58,560 --> 00:05:03,039
while repeatedly checking if the lock is

00:05:01,280 --> 00:05:05,440
available

00:05:03,039 --> 00:05:06,160
it is widely used in multi-thread

00:05:05,440 --> 00:05:09,360
software

00:05:06,160 --> 00:05:10,880
like db and when come to the

00:05:09,360 --> 00:05:14,000
implementation

00:05:10,880 --> 00:05:18,560
there are two major forms tas and

00:05:14,000 --> 00:05:21,360
ces ts means test and set

00:05:18,560 --> 00:05:23,120
in this form instruction will write one

00:05:21,360 --> 00:05:26,560
to a memory location and

00:05:23,120 --> 00:05:29,759
return its old value as a single atomic

00:05:26,560 --> 00:05:33,120
operation on the other hand

00:05:29,759 --> 00:05:35,199
in ces the atomic

00:05:33,120 --> 00:05:38,240
instruction compared the content of a

00:05:35,199 --> 00:05:41,759
memory location with a given value

00:05:38,240 --> 00:05:44,160
and only if the they are the same

00:05:41,759 --> 00:05:46,639
modifies the content of that memory

00:05:44,160 --> 00:05:50,000
location for the new given value

00:05:46,639 --> 00:05:52,240
both tas and cas have building

00:05:50,000 --> 00:05:53,199
implementations in major programming

00:05:52,240 --> 00:05:56,800
language and

00:05:53,199 --> 00:06:00,160
compilers here we use gcc as an

00:05:56,800 --> 00:06:04,479
example the corresponding functions

00:06:00,160 --> 00:06:08,960
are atomic test and set for tas

00:06:04,479 --> 00:06:12,720
and atomic compare exchange n for ces

00:06:08,960 --> 00:06:16,639
from assembly we can see that in tas

00:06:12,720 --> 00:06:20,160
we write before compare but in ces

00:06:16,639 --> 00:06:20,720
we compare before rights and red is

00:06:20,160 --> 00:06:23,680
actually

00:06:20,720 --> 00:06:25,759
much more expensive than compare so

00:06:23,680 --> 00:06:28,960
technically ces can

00:06:25,759 --> 00:06:30,000
save more time so what can this benefit

00:06:28,960 --> 00:06:33,199
us

00:06:30,000 --> 00:06:36,720
as mentioned above in databases

00:06:33,199 --> 00:06:37,680
spin logs are widely used and impossibly

00:06:36,720 --> 00:06:41,120
circle

00:06:37,680 --> 00:06:42,880
we found that it uses tls for spin locks

00:06:41,120 --> 00:06:45,919
on on platform

00:06:42,880 --> 00:06:48,319
so we'll change it to cas spring

00:06:45,919 --> 00:06:50,800
performance improvements

00:06:48,319 --> 00:06:52,960
let's have a look on the charts the

00:06:50,800 --> 00:06:56,800
purple line shows the throughputs

00:06:52,960 --> 00:06:58,960
for the development head when we test it

00:06:56,800 --> 00:07:00,319
and the green line shows the throughput

00:06:58,960 --> 00:07:04,240
for using ces

00:07:00,319 --> 00:07:05,440
instead of ts we can see from the boost

00:07:04,240 --> 00:07:08,080
charts

00:07:05,440 --> 00:07:10,240
ces brings about 50 percent of

00:07:08,080 --> 00:07:12,800
performance improvement

00:07:10,240 --> 00:07:14,080
which is very huge if you are interested

00:07:12,800 --> 00:07:16,479
on details

00:07:14,080 --> 00:07:18,479
about this change please check the

00:07:16,479 --> 00:07:22,080
following mail list

00:07:18,479 --> 00:07:24,639
archive for more details there is one

00:07:22,080 --> 00:07:26,720
more optimization check for spin locks i

00:07:24,639 --> 00:07:29,680
would like to share

00:07:26,720 --> 00:07:31,919
when perform the loop or spin in order

00:07:29,680 --> 00:07:35,039
to avoid the infinite loop

00:07:31,919 --> 00:07:40,000
we normally add delay or number of loops

00:07:35,039 --> 00:07:43,280
to the implementation and the mara db

00:07:40,000 --> 00:07:44,000
in the original implementation there was

00:07:43,280 --> 00:07:47,360
a dummy

00:07:44,000 --> 00:07:50,080
compare and swap operation used as the

00:07:47,360 --> 00:07:53,120
delay of the loop

00:07:50,080 --> 00:07:56,319
this was tested in the old arm model and

00:07:53,120 --> 00:07:58,879
proved to have a better performance

00:07:56,319 --> 00:08:00,800
but with the innovation of arm hardware

00:07:58,879 --> 00:08:03,360
and instructions

00:08:00,800 --> 00:08:05,440
it turns out that omitting the dummy

00:08:03,360 --> 00:08:09,039
compare and swap operation

00:08:05,440 --> 00:08:12,800
and use memory barrier to omit compiler

00:08:09,039 --> 00:08:15,840
optimization can bring about 12 percent

00:08:12,800 --> 00:08:17,520
of performance improvement in newer our

00:08:15,840 --> 00:08:20,639
models

00:08:17,520 --> 00:08:21,840
i think this tells us that even if your

00:08:20,639 --> 00:08:24,800
code have been

00:08:21,840 --> 00:08:25,599
optimized for on before you might still

00:08:24,800 --> 00:08:28,720
have to have

00:08:25,599 --> 00:08:30,080
a test for the new models since new

00:08:28,720 --> 00:08:33,760
hardware might need

00:08:30,080 --> 00:08:36,320
new ways of automation if you are

00:08:33,760 --> 00:08:38,839
interested in this particular case

00:08:36,320 --> 00:08:41,039
please check the following link for more

00:08:38,839 --> 00:08:43,120
details

00:08:41,039 --> 00:08:44,959
the next trick i would like to share is

00:08:43,120 --> 00:08:47,200
more simpler

00:08:44,959 --> 00:08:48,640
i think everyone knows the memory copy

00:08:47,200 --> 00:08:51,600
function

00:08:48,640 --> 00:08:53,120
it is a c standard library used to copy

00:08:51,600 --> 00:08:54,959
content of data

00:08:53,120 --> 00:08:57,360
from the source memory to the

00:08:54,959 --> 00:09:00,880
destination memory

00:08:57,360 --> 00:09:02,880
it is used everywhere and for old codes

00:09:00,880 --> 00:09:06,399
with arm v7

00:09:02,880 --> 00:09:10,320
such as this example in mysql

00:09:06,399 --> 00:09:15,279
it copies with battles of four smv7

00:09:10,320 --> 00:09:18,480
is a 32-bit chip but for arm v8 hardware

00:09:15,279 --> 00:09:21,519
we should copy with battles of 8.

00:09:18,480 --> 00:09:24,320
there are 64-bits hardware

00:09:21,519 --> 00:09:27,120
and this will bring another 50

00:09:24,320 --> 00:09:28,959
performance improvement

00:09:27,120 --> 00:09:31,120
despite of tricks in programming

00:09:28,959 --> 00:09:33,600
languages and compilers

00:09:31,120 --> 00:09:34,240
arm cpu also comes with a lot of

00:09:33,600 --> 00:09:37,680
hardware

00:09:34,240 --> 00:09:39,120
solution techniques the first one i

00:09:37,680 --> 00:09:43,600
would like to talk about

00:09:39,120 --> 00:09:45,519
is the zrc32 crc stands for cyclic

00:09:43,600 --> 00:09:48,880
redundancy check

00:09:45,519 --> 00:09:50,880
it is used to detect accidental changes

00:09:48,880 --> 00:09:54,320
to raw data

00:09:50,880 --> 00:09:57,680
it is used by almost every software

00:09:54,320 --> 00:10:00,480
so accelerate it with hardware can bring

00:09:57,680 --> 00:10:04,640
a lot of benefits

00:10:00,480 --> 00:10:07,880
again we use mysql as an example

00:10:04,640 --> 00:10:11,200
mysql uses two kinds of crc

00:10:07,880 --> 00:10:12,399
crc32 for calculating table and bin log

00:10:11,200 --> 00:10:16,800
checksums

00:10:12,399 --> 00:10:19,920
and crc32 for innodb page saxon

00:10:16,800 --> 00:10:21,360
if your arm hardware can provide crc32

00:10:19,920 --> 00:10:24,959
acceleration

00:10:21,360 --> 00:10:28,320
you can use the arm c language extension

00:10:24,959 --> 00:10:31,680
to leverage the acceleration

00:10:28,320 --> 00:10:34,720
and it will bring a lot of benefits

00:10:31,680 --> 00:10:37,200
here is our test results

00:10:34,720 --> 00:10:37,839
we have a table with lines of records

00:10:37,200 --> 00:10:41,600
from

00:10:37,839 --> 00:10:43,120
100 000 to 15 million and we calculate

00:10:41,600 --> 00:10:46,160
the table check sum

00:10:43,120 --> 00:10:49,279
of them we can see that

00:10:46,160 --> 00:10:51,839
with hardware acceleration there will be

00:10:49,279 --> 00:10:55,760
at least 50 percent of performance

00:10:51,839 --> 00:10:58,880
improvement for more details on this

00:10:55,760 --> 00:11:01,279
you can check the following link

00:10:58,880 --> 00:11:02,399
the next hardware acceleration technique

00:11:01,279 --> 00:11:05,760
i want to talk about

00:11:02,399 --> 00:11:08,959
is noon or vectorization

00:11:05,760 --> 00:11:13,279
i will use the case of usual code to

00:11:08,959 --> 00:11:16,320
demonstrate how it will be used

00:11:13,279 --> 00:11:17,279
the usual code is a concept from coding

00:11:16,320 --> 00:11:20,800
theory

00:11:17,279 --> 00:11:24,640
it is the forward error correction code

00:11:20,800 --> 00:11:27,360
under the assumption of bit erasure

00:11:24,640 --> 00:11:29,040
which transforms a message of key symbol

00:11:27,360 --> 00:11:31,839
into a longer message

00:11:29,040 --> 00:11:34,480
with n symbols so that the original

00:11:31,839 --> 00:11:36,880
message can be recovered from a subset

00:11:34,480 --> 00:11:40,600
of the earned symbols

00:11:36,880 --> 00:11:44,800
and read solomon encoding is one of the

00:11:40,600 --> 00:11:44,800
implementations of a usual code

00:11:45,120 --> 00:11:52,399
usual code can be used in hadoop hdfs

00:11:49,040 --> 00:11:54,399
adobe hdfs by default replicates each

00:11:52,399 --> 00:11:57,040
block in 3 times

00:11:54,399 --> 00:11:59,600
so that it can provide simple and robust

00:11:57,040 --> 00:12:02,639
form of redundancy to shield against

00:11:59,600 --> 00:12:05,519
most failure scenarios it also

00:12:02,639 --> 00:12:07,040
eases scheduling compute tasks on local

00:12:05,519 --> 00:12:09,839
stores data blocks

00:12:07,040 --> 00:12:12,240
by providing multiple replicas of each

00:12:09,839 --> 00:12:15,519
block to choose from

00:12:12,240 --> 00:12:17,920
but it has a very big drawback as the

00:12:15,519 --> 00:12:20,240
divorce replication brings triple

00:12:17,920 --> 00:12:24,000
overhead in storage spaces and

00:12:20,240 --> 00:12:27,040
other resources and for data sets with

00:12:24,000 --> 00:12:28,079
relatively low i o activity the

00:12:27,040 --> 00:12:30,720
additional block

00:12:28,079 --> 00:12:32,320
replicas are rarely accessed during the

00:12:30,720 --> 00:12:34,880
normal operations

00:12:32,320 --> 00:12:36,320
but still consumes the same amount of

00:12:34,880 --> 00:12:38,720
storage spaces

00:12:36,320 --> 00:12:39,680
the usual code can actually solve this

00:12:38,720 --> 00:12:42,480
problem

00:12:39,680 --> 00:12:43,760
it can reduce the storage cost by about

00:12:42,480 --> 00:12:46,240
50 percent

00:12:43,760 --> 00:12:48,399
comparing to the three replication

00:12:46,240 --> 00:12:51,680
typical deployments

00:12:48,399 --> 00:12:54,160
hadoop3 has added this feature

00:12:51,680 --> 00:12:55,680
but using usual code means add more

00:12:54,160 --> 00:12:58,160
computing tasks

00:12:55,680 --> 00:13:00,800
and the usual code is actually a

00:12:58,160 --> 00:13:04,079
calculation that can be vectorized

00:13:00,800 --> 00:13:08,720
and that's what nuon is born for

00:13:04,079 --> 00:13:12,160
hadoop uses as a slash l for accelerated

00:13:08,720 --> 00:13:15,120
easy calculations ios is slash

00:13:12,160 --> 00:13:16,160
l is an optimized low level function

00:13:15,120 --> 00:13:19,279
targeting

00:13:16,160 --> 00:13:22,160
storage applications including

00:13:19,279 --> 00:13:23,279
easy calculations compression and

00:13:22,160 --> 00:13:27,279
decompressions

00:13:23,279 --> 00:13:28,320
and etc we have added new support for

00:13:27,279 --> 00:13:32,320
isa slash

00:13:28,320 --> 00:13:34,639
l in version 2.28

00:13:32,320 --> 00:13:37,680
so that it can leverage the hardware

00:13:34,639 --> 00:13:41,040
acceleration on our platform

00:13:37,680 --> 00:13:44,320
and the improvement is quite clear from

00:13:41,040 --> 00:13:45,360
the graph on the left we can see that s

00:13:44,320 --> 00:13:49,279
a slash l

00:13:45,360 --> 00:13:50,480
is brings 5 to 10 times faster than the

00:13:49,279 --> 00:13:53,760
pure software

00:13:50,480 --> 00:13:54,160
implementation the last trick i have

00:13:53,760 --> 00:13:56,560
here

00:13:54,160 --> 00:13:59,199
is just you simply use the newer version

00:13:56,560 --> 00:14:02,000
of programming languages

00:13:59,199 --> 00:14:03,839
after investigate a lot of applications

00:14:02,000 --> 00:14:06,959
in different areas

00:14:03,839 --> 00:14:09,120
we found that many applications

00:14:06,959 --> 00:14:11,120
the key to boost the performance

00:14:09,120 --> 00:14:12,480
sometimes go all the way down to the

00:14:11,120 --> 00:14:15,360
programming language

00:14:12,480 --> 00:14:16,959
itself especially for applications

00:14:15,360 --> 00:14:20,320
written in languages like

00:14:16,959 --> 00:14:22,800
java scala and exactly

00:14:20,320 --> 00:14:24,639
in those programming languages the

00:14:22,800 --> 00:14:27,680
programs runs in vm

00:14:24,639 --> 00:14:30,000
runtimes like gvm so

00:14:27,680 --> 00:14:30,800
there is a middle layer between the

00:14:30,000 --> 00:14:33,600
application

00:14:30,800 --> 00:14:34,720
and the hardware this is good to make

00:14:33,600 --> 00:14:37,519
applications

00:14:34,720 --> 00:14:38,480
more adaptive for different hardware but

00:14:37,519 --> 00:14:41,279
sometimes

00:14:38,480 --> 00:14:43,440
it blocks you from perform attenuation

00:14:41,279 --> 00:14:45,920
for specific hardware

00:14:43,440 --> 00:14:48,160
and sometimes in the newer version of

00:14:45,920 --> 00:14:50,480
the programming languages

00:14:48,160 --> 00:14:51,760
the new hardware platform have better

00:14:50,480 --> 00:14:55,279
support

00:14:51,760 --> 00:14:55,760
here i have a simple example after we

00:14:55,279 --> 00:14:58,800
added

00:14:55,760 --> 00:15:02,000
mcis to spark flink and hive

00:14:58,800 --> 00:15:04,320
we observe that among other tests

00:15:02,000 --> 00:15:05,519
some of the mathematical calculation

00:15:04,320 --> 00:15:10,639
tests have different

00:15:05,519 --> 00:15:13,360
results on arm and x86 hardware

00:15:10,639 --> 00:15:15,519
after looking into it we found out that

00:15:13,360 --> 00:15:17,920
the root cause for this is because the

00:15:15,519 --> 00:15:20,800
mathematical cognition library

00:15:17,920 --> 00:15:23,440
in java supports dynamic replacement of

00:15:20,800 --> 00:15:26,880
different hardware platform

00:15:23,440 --> 00:15:30,079
and in java 8 x86 has

00:15:26,880 --> 00:15:30,800
acceleration implementation which runs

00:15:30,079 --> 00:15:33,040
faster

00:15:30,800 --> 00:15:35,120
but the mathematical accuracy is not

00:15:33,040 --> 00:15:38,720
correct

00:15:35,120 --> 00:15:40,720
in java 11 on platform have also added

00:15:38,720 --> 00:15:43,199
the corresponding acceleration

00:15:40,720 --> 00:15:46,320
it provides both fast calculation while

00:15:43,199 --> 00:15:48,720
maintaining the mathematical accuracy

00:15:46,320 --> 00:15:50,800
so simply using the newer version of

00:15:48,720 --> 00:15:54,720
java in your application can provide

00:15:50,800 --> 00:15:56,000
better performance since different

00:15:54,720 --> 00:15:58,720
vendors normally add

00:15:56,000 --> 00:16:01,839
different features to their chips and

00:15:58,720 --> 00:16:04,240
those can boost your applications a lot

00:16:01,839 --> 00:16:06,560
so it could be good for you if you know

00:16:04,240 --> 00:16:09,040
some key features and how to use them

00:16:06,560 --> 00:16:10,959
of the chips you are using i'm using

00:16:09,040 --> 00:16:13,279
compound for daily work

00:16:10,959 --> 00:16:14,880
so here i will introduce some of the

00:16:13,279 --> 00:16:17,839
compound specific

00:16:14,880 --> 00:16:20,480
optimization tools the first one i have

00:16:17,839 --> 00:16:22,800
in mind is the vision jdk

00:16:20,480 --> 00:16:24,639
vision jdk is a high performance

00:16:22,800 --> 00:16:27,680
production ready downstream

00:16:24,639 --> 00:16:31,680
distribution of open jdk

00:16:27,680 --> 00:16:34,240
huawei has used it in over 500 products

00:16:31,680 --> 00:16:35,199
accumulating a large number of usage

00:16:34,240 --> 00:16:38,000
scenarios and

00:16:35,199 --> 00:16:39,759
problems and demands from the java

00:16:38,000 --> 00:16:41,680
developers

00:16:39,759 --> 00:16:44,160
they have solved many problems

00:16:41,680 --> 00:16:47,519
encountered in the actual operation

00:16:44,160 --> 00:16:50,320
of the business and the typical

00:16:47,519 --> 00:16:54,639
optimized for arm platforms

00:16:50,320 --> 00:16:56,639
it supports gtk8 and jdk 11.

00:16:54,639 --> 00:16:58,880
here i want to talk a little bit more

00:16:56,639 --> 00:17:02,560
about the vision jdk8

00:16:58,880 --> 00:17:04,959
it added few features to the openjdk8

00:17:02,560 --> 00:17:06,319
to provide better performance on our

00:17:04,959 --> 00:17:08,400
platform

00:17:06,319 --> 00:17:10,079
many open source projects are still

00:17:08,400 --> 00:17:13,919
using jdk8

00:17:10,079 --> 00:17:16,319
like hadoop so this might be very useful

00:17:13,919 --> 00:17:17,120
one of them is the application data

00:17:16,319 --> 00:17:20,480
sharing or

00:17:17,120 --> 00:17:22,480
app cds in open jdk

00:17:20,480 --> 00:17:24,160
this feature is only available in

00:17:22,480 --> 00:17:28,000
version 11.

00:17:24,160 --> 00:17:30,720
app cds is a gvm feature for startup

00:17:28,000 --> 00:17:31,600
accelerations and memory receiving the

00:17:30,720 --> 00:17:34,799
idea behind

00:17:31,600 --> 00:17:36,799
abb cds is to share once loaded classes

00:17:34,799 --> 00:17:40,000
between gvm instances

00:17:36,799 --> 00:17:42,000
on the same host this is actually very

00:17:40,000 --> 00:17:44,240
useful for running batch applications

00:17:42,000 --> 00:17:46,160
like hadoop mac reduce

00:17:44,240 --> 00:17:48,160
i have done some tests and from the

00:17:46,160 --> 00:17:50,080
results we can see that in typical

00:17:48,160 --> 00:17:54,000
pterosaur test case

00:17:50,080 --> 00:17:56,559
apbcds can actually provide up to 14

00:17:54,000 --> 00:18:00,240
improvement in performance depending on

00:17:56,559 --> 00:18:02,480
the cluster size

00:18:00,240 --> 00:18:04,400
the next one i want to talk about is the

00:18:02,480 --> 00:18:07,919
crimpling acceleration engine

00:18:04,400 --> 00:18:10,039
or kaeg it is a group of software

00:18:07,919 --> 00:18:11,679
libraries which provide performance

00:18:10,039 --> 00:18:15,039
competitiveness

00:18:11,679 --> 00:18:16,960
of a common software on quantum platform

00:18:15,039 --> 00:18:19,400
it includes hardware enabled

00:18:16,960 --> 00:18:22,400
accelerations for security

00:18:19,400 --> 00:18:24,480
authentications and compressions you can

00:18:22,400 --> 00:18:26,400
provide the following calculations

00:18:24,480 --> 00:18:28,080
i believe you are very familiar with

00:18:26,400 --> 00:18:30,640
those function names

00:18:28,080 --> 00:18:32,240
as they are very widely used in all

00:18:30,640 --> 00:18:35,440
kinds of applications

00:18:32,240 --> 00:18:35,919
so how does kae perform let's have a

00:18:35,440 --> 00:18:38,240
look

00:18:35,919 --> 00:18:38,960
at the comparison in the compression

00:18:38,240 --> 00:18:41,600
library

00:18:38,960 --> 00:18:42,080
from the test results we can see that

00:18:41,600 --> 00:18:45,200
using

00:18:42,080 --> 00:18:46,080
kae um compound platform can provide a

00:18:45,200 --> 00:18:48,400
very huge

00:18:46,080 --> 00:18:50,799
performance improvement in compression

00:18:48,400 --> 00:18:52,960
and decompression actions involved in

00:18:50,799 --> 00:18:54,559
your application will for sure bring a

00:18:52,960 --> 00:18:56,640
lot of benefits

00:18:54,559 --> 00:18:58,640
the last thing i want to talk about in

00:18:56,640 --> 00:19:01,039
this session is the compound spark

00:18:58,640 --> 00:19:04,480
machine learning algorithm library

00:19:01,039 --> 00:19:06,000
it is an accelerated library that can

00:19:04,480 --> 00:19:07,760
provide rich set of

00:19:06,000 --> 00:19:09,520
optimized tools for machine learning

00:19:07,760 --> 00:19:12,960
algorithms

00:19:09,520 --> 00:19:14,120
it is based on original apis from apache

00:19:12,960 --> 00:19:17,760
spark

00:19:14,120 --> 00:19:19,840
2.3.2 the accelerated library provides

00:19:17,760 --> 00:19:21,520
huge computational performance

00:19:19,840 --> 00:19:24,880
enhancement

00:19:21,520 --> 00:19:26,640
the supported algorithms are listed here

00:19:24,880 --> 00:19:29,039
i'm not going to go through all the

00:19:26,640 --> 00:19:31,440
details of those algorithms

00:19:29,039 --> 00:19:33,520
you can check on the official website

00:19:31,440 --> 00:19:35,600
let's jump right into the comparison

00:19:33,520 --> 00:19:38,640
from the test result provided

00:19:35,600 --> 00:19:40,799
by the official site of this project

00:19:38,640 --> 00:19:42,960
we can see that compound spark machine

00:19:40,799 --> 00:19:45,120
learning algorithm library

00:19:42,960 --> 00:19:46,880
can provide very huge performance

00:19:45,120 --> 00:19:49,520
improvements in many

00:19:46,880 --> 00:19:51,520
calculations users can consider using

00:19:49,520 --> 00:19:53,679
them in their applications

00:19:51,520 --> 00:19:54,720
so that's all the content for this

00:19:53,679 --> 00:19:58,840
session

00:19:54,720 --> 00:20:01,840
i hope it's useful thank you for your

00:19:58,840 --> 00:20:01,840

YouTube URL: https://www.youtube.com/watch?v=mN-cICluQc4


