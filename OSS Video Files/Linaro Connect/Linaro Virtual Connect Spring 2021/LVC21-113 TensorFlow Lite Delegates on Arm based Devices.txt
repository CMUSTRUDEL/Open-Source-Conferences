Title: LVC21-113 TensorFlow Lite Delegates on Arm based Devices
Publication date: 2021-04-15
Playlist: Linaro Virtual Connect Spring 2021
Description: 
	TensorFlow Lite is a popular open-source framework that enables Machine Learning on mobile, embedded, and IoT devices by default using the Arm NEON instruction set. The delegate mechanism takes TensorFlow Lite one step further and provides a mechanism to use on-device hardware accelerators such as the GPU, NPU, or Digital Signal Processor.

Session Detail:
The session focuses on TensorFlow Lite, and especially its delegate mechanism. Tensorflow Lite is a widely used open-source framework developed by Google for mobile, embedded, and IoT machine learning. By default, TensorFlow Lite kernel implementation is optimized using Arm NEON instruction set for Arm Cortex-A cores. The delegate mechanism provides the means to leverage the computational power of on-device accelerators such as the GPU, NPU, or DSP and offloads the execution of machine learning kernels to another framework, or an optimized kernel library. Such examples are the Arm NN delegate developed by Arm for the Linaro Machine Learning initiative, NN API delegate, or XNNPACK delegate.

We will briefly cover what is TensorFlow Lite and then dive deeper into the delegate mechanism. We will go over several examples of TF Lite delegates. The new Arm NN delegate, which was released in Arm NN 20.11 (an inference engine for machine learning developed by Linaro Machine Learning Initiative). The NN API delegate, which is one of the default delegates designed for Android and used for acceleration on the Arm-based NXP i.MX8 platform. We will also go over why should a developer consider implementing a custom delegate, how to do that. In the end, we will discuss delegate performance and operation support.

Flow:
1) Introduction to TF Lite
A brief introduction to Googleâ€™s embedded machine learning framework.
 
2) Introduction to Delegates
An introduction to the delegate mechanism, which offloads the execution of TF Lite machine learning kernels to another framework, driver, hardware, or other mechanisms, which enable acceleration.
 
3) NN API delegate
A default delegate integrated into TF Lite, which offloads the execution using Android NN API.
 
4) Arm NN delegate (new in Arm NN 20.11)
A newly implemented delegate in Arm NN 20.11, which offloads the execution to the Arm NN framework.
 
5) Performance benchmarking and operation support discussion
Options to benchmark the performance of the delegate. Handling of delegate unsupported operators and graph partitioning.
 
6) Implementing a custom delegate
Considerations why to implement a custom delegate, how to implement such a delegate, and a simple example.
Captions: 
	00:00:04,720 --> 00:00:08,400
hi

00:00:05,359 --> 00:00:09,920
so um my name is pablo matzenauer and

00:00:08,400 --> 00:00:13,920
i'm a software engineer

00:00:09,920 --> 00:00:17,920
at nxp semiconductors working in the

00:00:13,920 --> 00:00:21,439
software rnd group for machine learning

00:00:17,920 --> 00:00:22,960
and today i'm going to cover tensorflow

00:00:21,439 --> 00:00:26,480
lite delegates on

00:00:22,960 --> 00:00:26,480
arm based devices

00:00:26,640 --> 00:00:32,800
so first of all i'm going to talk about

00:00:29,920 --> 00:00:35,120
how to deploy a tensorflow lite model

00:00:32,800 --> 00:00:36,000
and afterwards we are going to dive

00:00:35,120 --> 00:00:38,960
deeper

00:00:36,000 --> 00:00:41,200
into the delegate mechanism uh introduce

00:00:38,960 --> 00:00:43,280
something called partitioning

00:00:41,200 --> 00:00:44,559
talk a little about implementing a

00:00:43,280 --> 00:00:47,520
custom delegate

00:00:44,559 --> 00:00:49,600
how to benchmark it how to check uh

00:00:47,520 --> 00:00:52,640
whether it works correctly

00:00:49,600 --> 00:00:55,680
and at the end we are going to dive

00:00:52,640 --> 00:00:58,960
deeper into two specific ones

00:00:55,680 --> 00:01:02,000
one being the nn api delegate and

00:00:58,960 --> 00:01:04,400
the second one being the newly

00:01:02,000 --> 00:01:05,840
introduced armin and delegate in the

00:01:04,400 --> 00:01:11,200
armanin framework

00:01:05,840 --> 00:01:11,200
and of course with some code samples

00:01:11,439 --> 00:01:16,720
so uh i guess

00:01:14,479 --> 00:01:18,080
most of the most of you here are already

00:01:16,720 --> 00:01:20,159
familiar with

00:01:18,080 --> 00:01:21,759
the tensorflow lite framework or

00:01:20,159 --> 00:01:23,759
tensorflow framework

00:01:21,759 --> 00:01:25,520
it's a popular machine learning

00:01:23,759 --> 00:01:28,720
framework developed

00:01:25,520 --> 00:01:31,840
originally by google and still

00:01:28,720 --> 00:01:33,600
mostly by google but there is a lot of

00:01:31,840 --> 00:01:37,200
contributors

00:01:33,600 --> 00:01:41,200
one of them being arm for example

00:01:37,200 --> 00:01:44,720
and tensorflow lite is a module

00:01:41,200 --> 00:01:48,079
within tensorflow which

00:01:44,720 --> 00:01:51,280
is there for inference on

00:01:48,079 --> 00:01:54,560
internet of things devices or embedded

00:01:51,280 --> 00:01:57,680
devices mostly

00:01:54,560 --> 00:02:00,799
those having for example cortex

00:01:57,680 --> 00:02:03,680
a course running linux

00:02:00,799 --> 00:02:04,479
but of course it may be might be a lot

00:02:03,680 --> 00:02:07,600
of

00:02:04,479 --> 00:02:07,600
different platforms

00:02:07,680 --> 00:02:11,039
one feature which i'm not going to cover

00:02:10,560 --> 00:02:14,400
uh

00:02:11,039 --> 00:02:18,000
this session is tensorflow micro

00:02:14,400 --> 00:02:21,440
uh which is a pretty newly introduced

00:02:18,000 --> 00:02:24,560
module for uh microcontrollers for so

00:02:21,440 --> 00:02:27,599
even smaller devices but it

00:02:24,560 --> 00:02:31,040
might be interesting for some of you to

00:02:27,599 --> 00:02:33,200
check it out so uh

00:02:31,040 --> 00:02:35,920
first of all uh when we deploy a

00:02:33,200 --> 00:02:36,800
tensorflow lite model we still need to

00:02:35,920 --> 00:02:39,519
start off

00:02:36,800 --> 00:02:42,080
with implementing or creating our model

00:02:39,519 --> 00:02:45,920
we typically do that on a pc

00:02:42,080 --> 00:02:49,040
using tensorflow python interface

00:02:45,920 --> 00:02:50,560
for those who are already familiar it's

00:02:49,040 --> 00:02:54,080
a

00:02:50,560 --> 00:02:56,160
well it might be a tricky process but

00:02:54,080 --> 00:02:57,599
it's not that hard you define your

00:02:56,160 --> 00:03:01,040
layers

00:02:57,599 --> 00:03:01,599
you produce a model you afterwards you

00:03:01,040 --> 00:03:04,800
may

00:03:01,599 --> 00:03:06,159
load the debates from a file or you may

00:03:04,800 --> 00:03:09,760
train it

00:03:06,159 --> 00:03:13,680
and there you go

00:03:09,760 --> 00:03:17,120
you you've got a model the next step

00:03:13,680 --> 00:03:20,480
is to take this model and load it

00:03:17,120 --> 00:03:22,879
into a tensorflow lite converter

00:03:20,480 --> 00:03:24,560
so a tensorflow lite converter is a

00:03:22,879 --> 00:03:27,680
module in

00:03:24,560 --> 00:03:32,319
tensorflow which converts

00:03:27,680 --> 00:03:34,799
the model into the flat buffers format

00:03:32,319 --> 00:03:35,920
again as i previously mentioned you may

00:03:34,799 --> 00:03:38,959
take

00:03:35,920 --> 00:03:40,560
already created model in the pro to

00:03:38,959 --> 00:03:43,920
buffer format

00:03:40,560 --> 00:03:47,040
in the keras format you may even feed it

00:03:43,920 --> 00:03:50,319
with low-level apis

00:03:47,040 --> 00:03:53,760
such as concrete functions

00:03:50,319 --> 00:03:58,799
and this then gets

00:03:53,760 --> 00:04:00,720
converted into flight buffers

00:03:58,799 --> 00:04:02,080
there is a couple of there is a couple

00:04:00,720 --> 00:04:05,840
of options

00:04:02,080 --> 00:04:06,319
which uh you or a couple of options

00:04:05,840 --> 00:04:09,120
which

00:04:06,319 --> 00:04:10,319
you may perform during this conversion

00:04:09,120 --> 00:04:13,760
uh

00:04:10,319 --> 00:04:15,040
the one of the most notable ones is

00:04:13,760 --> 00:04:18,000
quantization

00:04:15,040 --> 00:04:20,560
so by default uh your model when it gets

00:04:18,000 --> 00:04:25,199
converted to the tensorflow lite format

00:04:20,560 --> 00:04:25,199
uh it's kept in the 32 bits

00:04:26,080 --> 00:04:32,320
floating point uh

00:04:29,280 --> 00:04:35,199
now some some devices there

00:04:32,320 --> 00:04:35,840
they have hardware available which uh

00:04:35,199 --> 00:04:38,479
performs

00:04:35,840 --> 00:04:39,680
much better with uh integer operations

00:04:38,479 --> 00:04:43,199
typically 80-bit

00:04:39,680 --> 00:04:47,280
uh typically those are npus

00:04:43,199 --> 00:04:50,880
or you may also have a gpu which

00:04:47,280 --> 00:04:53,919
runs a lot faster with uh float

00:04:50,880 --> 00:04:56,880
with 16-bit float operations

00:04:53,919 --> 00:04:58,160
so uh those are uh there are actually

00:04:56,880 --> 00:05:00,320
several options

00:04:58,160 --> 00:05:02,240
uh for quantization which are available

00:05:00,320 --> 00:05:04,800
in the tensorflow lite converter

00:05:02,240 --> 00:05:05,360
uh one of them is the 16 bit floating

00:05:04,800 --> 00:05:09,680
point

00:05:05,360 --> 00:05:12,479
uh another one is the eight bits

00:05:09,680 --> 00:05:13,120
and there is also a new experimental

00:05:12,479 --> 00:05:16,960
feature

00:05:13,120 --> 00:05:20,880
which allows you to quantize your model

00:05:16,960 --> 00:05:24,000
into into a 8-bit model with

00:05:20,880 --> 00:05:26,720
16-bit activations so this

00:05:24,000 --> 00:05:27,680
might be useful in case uh you need a

00:05:26,720 --> 00:05:31,520
quantized

00:05:27,680 --> 00:05:35,520
model with slightly uh

00:05:31,520 --> 00:05:35,520
with slightly better accuracy

00:05:35,759 --> 00:05:41,919
so now after uh

00:05:38,800 --> 00:05:46,400
after we serialized our format into

00:05:41,919 --> 00:05:49,680
our model into the tensorflow

00:05:46,400 --> 00:05:53,280
flood buffers format we may run it

00:05:49,680 --> 00:05:55,360
using the interpreter one of the notable

00:05:53,280 --> 00:05:58,800
parameters which is passed when

00:05:55,360 --> 00:06:02,080
initializing the interpreter is the

00:05:58,800 --> 00:06:05,600
experimental delegates parameter so

00:06:02,080 --> 00:06:09,600
uh you might actually feed that one with

00:06:05,600 --> 00:06:13,039
the tensorflow like the delegate api or

00:06:09,600 --> 00:06:16,160
uh class and

00:06:13,039 --> 00:06:19,440
uh there is the api which

00:06:16,160 --> 00:06:21,120
handles uh when you handle sensor fluid

00:06:19,440 --> 00:06:23,600
light delegates

00:06:21,120 --> 00:06:24,400
and there is also the simple delegate

00:06:23,600 --> 00:06:26,400
api

00:06:24,400 --> 00:06:27,759
which is a wrapper around tensorflow

00:06:26,400 --> 00:06:30,639
lite delegates so

00:06:27,759 --> 00:06:32,000
uh it basically just simplifies the

00:06:30,639 --> 00:06:35,840
implementation and

00:06:32,000 --> 00:06:35,840
uh handles some other things

00:06:36,400 --> 00:06:40,880
or it handles some of the things that

00:06:38,240 --> 00:06:44,160
you don't have to

00:06:40,880 --> 00:06:49,440
and okay so

00:06:44,160 --> 00:06:52,479
after we feed our tensorflow lite model

00:06:49,440 --> 00:06:54,639
which runs inference already on your

00:06:52,479 --> 00:06:58,319
device

00:06:54,639 --> 00:07:01,680
so there are two options here

00:06:58,319 --> 00:07:05,440
how the interpreter can run the model

00:07:01,680 --> 00:07:07,840
it can be either using the default

00:07:05,440 --> 00:07:09,039
tflight implementation using the default

00:07:07,840 --> 00:07:11,919
kernels

00:07:09,039 --> 00:07:14,000
which are heavily optimized for the arm

00:07:11,919 --> 00:07:16,960
neon instruction set so

00:07:14,000 --> 00:07:17,680
if you want to run your model uh on the

00:07:16,960 --> 00:07:19,599
cpu

00:07:17,680 --> 00:07:21,440
on your device this is probably one of

00:07:19,599 --> 00:07:25,759
the best options

00:07:21,440 --> 00:07:29,039
to go with but

00:07:25,759 --> 00:07:31,199
typically hardware

00:07:29,039 --> 00:07:34,080
your hardware has other options

00:07:31,199 --> 00:07:37,440
available that may be a gpu that may be

00:07:34,080 --> 00:07:39,280
an npu that might be a digital signal

00:07:37,440 --> 00:07:41,599
processor

00:07:39,280 --> 00:07:42,720
so uh if you want to overflow the

00:07:41,599 --> 00:07:46,639
calculation or

00:07:42,720 --> 00:07:51,840
the computation to such device

00:07:46,639 --> 00:07:51,840
you need to use a delegate here

00:07:54,560 --> 00:08:02,080
so now tensorflow provides

00:07:58,720 --> 00:08:06,560
several already existing

00:08:02,080 --> 00:08:11,039
interfaces or implementations

00:08:06,560 --> 00:08:13,840
uh the first one is the an api delegate

00:08:11,039 --> 00:08:15,759
which offloads the uh execution to

00:08:13,840 --> 00:08:18,960
something called an api

00:08:15,759 --> 00:08:22,160
which is an interface targeted

00:08:18,960 --> 00:08:25,440
for android devices it's

00:08:22,160 --> 00:08:28,879
basically defines types and

00:08:25,440 --> 00:08:33,599
operations which it may then

00:08:28,879 --> 00:08:36,640
execute one thing to note here is that

00:08:33,599 --> 00:08:40,000
the nxp idotomix 8 microprocessors

00:08:36,640 --> 00:08:43,200
leverage this an api delegate

00:08:40,000 --> 00:08:46,640
in order uh to

00:08:43,200 --> 00:08:49,440
accelerate inference and

00:08:46,640 --> 00:08:50,240
typically uh what this this does is that

00:08:49,440 --> 00:08:53,440
it

00:08:50,240 --> 00:08:57,200
it delegates the execution of

00:08:53,440 --> 00:09:00,720
an api defined operations

00:08:57,200 --> 00:09:04,720
to the underlying driver which then

00:09:00,720 --> 00:09:06,959
executes the computation on whatever

00:09:04,720 --> 00:09:08,480
acceleration devices available this

00:09:06,959 --> 00:09:11,839
might be the gpu

00:09:08,480 --> 00:09:12,959
or it might also be a specialized npu in

00:09:11,839 --> 00:09:18,160
the case of

00:09:12,959 --> 00:09:21,440
idotamix 8 and plus microprocessors

00:09:18,160 --> 00:09:24,880
so then there is the excitement pack

00:09:21,440 --> 00:09:28,240
excellent pack is a package of

00:09:24,880 --> 00:09:29,200
kernels which is an alternative to the

00:09:28,240 --> 00:09:32,320
default

00:09:29,200 --> 00:09:36,080
cpu implementation it also executes

00:09:32,320 --> 00:09:39,279
your graph or your model on the cpu

00:09:36,080 --> 00:09:43,440
but well you may bench market see

00:09:39,279 --> 00:09:46,240
what performs better and

00:09:43,440 --> 00:09:47,120
then of course the more interesting part

00:09:46,240 --> 00:09:49,120
is the

00:09:47,120 --> 00:09:50,240
implement your own delegate class of

00:09:49,120 --> 00:09:54,160
delegates

00:09:50,240 --> 00:09:57,360
and those are custom delegates which

00:09:54,160 --> 00:10:01,120
don't live as a part of the

00:09:57,360 --> 00:10:04,800
tf flight repository

00:10:01,120 --> 00:10:08,399
one of them is the vx or openvxdelegate

00:10:04,800 --> 00:10:11,519
which we planned which we plan

00:10:08,399 --> 00:10:15,360
in 2021 to

00:10:11,519 --> 00:10:18,959
deploy for our linux idatemx8

00:10:15,360 --> 00:10:21,920
images but then there is also

00:10:18,959 --> 00:10:22,320
the armando delegate which you may find

00:10:21,920 --> 00:10:25,920
in

00:10:22,320 --> 00:10:29,200
the armanin repository

00:10:25,920 --> 00:10:32,720
already for uh three months

00:10:29,200 --> 00:10:37,519
under under the delegate folder and

00:10:32,720 --> 00:10:37,519
it kind of lifts its own life there

00:10:39,120 --> 00:10:46,079
so now before we

00:10:42,800 --> 00:10:49,600
run we run a delegate

00:10:46,079 --> 00:10:52,000
or run the competition on the delegate

00:10:49,600 --> 00:10:52,880
it's good to think about whether we even

00:10:52,000 --> 00:10:55,839
want to do that

00:10:52,880 --> 00:10:57,360
because well uh cpu might be better

00:10:55,839 --> 00:10:59,360
sometimes

00:10:57,360 --> 00:11:02,480
so the obvious advantages are

00:10:59,360 --> 00:11:06,560
performance and power consumption

00:11:02,480 --> 00:11:10,079
so typically a gpu or an npu

00:11:06,560 --> 00:11:12,560
might will perform a lot better

00:11:10,079 --> 00:11:14,320
than your cpu because it's of course

00:11:12,560 --> 00:11:17,760
specialized hardware

00:11:14,320 --> 00:11:20,959
uh it has in case of a gpu there is

00:11:17,760 --> 00:11:25,040
the parallel nature which uh

00:11:20,959 --> 00:11:26,839
computes uh which is suitable

00:11:25,040 --> 00:11:28,560
much more suitable for this kind of

00:11:26,839 --> 00:11:31,920
computation uh

00:11:28,560 --> 00:11:33,360
npus also typically fuse layers so in

00:11:31,920 --> 00:11:35,279
case you're for example running in a

00:11:33,360 --> 00:11:38,000
convolution operation

00:11:35,279 --> 00:11:40,320
uh afterwards you might have an

00:11:38,000 --> 00:11:44,160
excavation or a pooling layer

00:11:40,320 --> 00:11:48,640
some npus they fuse these together and

00:11:44,160 --> 00:11:51,920
may run it within a single cycle

00:11:48,640 --> 00:11:54,480
i also in the past i also ran into a

00:11:51,920 --> 00:11:55,040
into a use case where the customer he

00:11:54,480 --> 00:11:59,040
was

00:11:55,040 --> 00:12:02,240
using an algorithmics 8 microprocessor

00:11:59,040 --> 00:12:05,360
with well not really with a

00:12:02,240 --> 00:12:08,720
really low end gpu

00:12:05,360 --> 00:12:10,639
but his use case was that the cpu was

00:12:08,720 --> 00:12:14,000
already running at 100

00:12:10,639 --> 00:12:16,639
so you want to free it up a little so

00:12:14,000 --> 00:12:18,639
he actually offloaded uh offloaded the

00:12:16,639 --> 00:12:21,839
execution to the cpu

00:12:18,639 --> 00:12:25,279
uh where it actually took longer

00:12:21,839 --> 00:12:28,560
to execute inference maybe by 50 but

00:12:25,279 --> 00:12:29,839
he was able to use the cpu for something

00:12:28,560 --> 00:12:32,560
else

00:12:29,839 --> 00:12:33,839
now these are these are the advantages

00:12:32,560 --> 00:12:37,279
but

00:12:33,839 --> 00:12:40,800
you there might also be reasons why

00:12:37,279 --> 00:12:44,560
you might want to consider not

00:12:40,800 --> 00:12:46,880
running the delegate and

00:12:44,560 --> 00:12:47,920
that's in case of unsupported operations

00:12:46,880 --> 00:12:52,639
for example

00:12:47,920 --> 00:12:56,399
so when an operation is not supported

00:12:52,639 --> 00:12:59,519
uh tensorflow lite uh keeps

00:12:56,399 --> 00:13:02,639
the execution on the on the cpu

00:12:59,519 --> 00:13:06,160
and it does so by uh

00:13:02,639 --> 00:13:10,160
partitioning the graph prior to uh

00:13:06,160 --> 00:13:12,399
execution so uh

00:13:10,160 --> 00:13:13,760
now uh when when you have when your

00:13:12,399 --> 00:13:16,160
graph is partitioned

00:13:13,760 --> 00:13:17,440
a lot uh there's typically a lot of

00:13:16,160 --> 00:13:20,880
memory transfers

00:13:17,440 --> 00:13:22,959
and additional tensor or memory copies

00:13:20,880 --> 00:13:26,079
required so

00:13:22,959 --> 00:13:29,440
this might create quite an overhead

00:13:26,079 --> 00:13:33,120
so in case this happens and

00:13:29,440 --> 00:13:35,839
for example your npu is not suitable for

00:13:33,120 --> 00:13:38,079
for the model you are executing this

00:13:35,839 --> 00:13:40,800
might be

00:13:38,079 --> 00:13:42,880
it's typically not the case of

00:13:40,800 --> 00:13:46,000
classification use cases but

00:13:42,880 --> 00:13:49,440
suddenly when you are running audio

00:13:46,000 --> 00:13:51,839
models or segmentation models and

00:13:49,440 --> 00:13:53,680
stuff like that where you have more more

00:13:51,839 --> 00:13:56,480
specialized

00:13:53,680 --> 00:13:58,079
more specialized operations required and

00:13:56,480 --> 00:13:59,920
those typically won't be supported by

00:13:58,079 --> 00:14:02,560
your npu then

00:13:59,920 --> 00:14:06,160
you might not want to use a delegate and

00:14:02,560 --> 00:14:09,279
just run the model on a cpu

00:14:06,160 --> 00:14:12,480
now uh i mentioned

00:14:09,279 --> 00:14:12,959
delegate partitioning previously so this

00:14:12,480 --> 00:14:16,000
happens

00:14:12,959 --> 00:14:19,199
typically in a use case where

00:14:16,000 --> 00:14:22,560
some operations are not supported

00:14:19,199 --> 00:14:25,040
so here in our example we have four

00:14:22,560 --> 00:14:26,160
four types of operations we have

00:14:25,040 --> 00:14:29,360
convolution

00:14:26,160 --> 00:14:31,199
we have concatenation and we have max

00:14:29,360 --> 00:14:34,639
pooling and average pooling

00:14:31,199 --> 00:14:36,480
so uh in our case max pooling and

00:14:34,639 --> 00:14:38,959
average pooling are not supported

00:14:36,480 --> 00:14:39,839
on by our delegate but convolution and

00:14:38,959 --> 00:14:44,320
concatenation

00:14:39,839 --> 00:14:47,440
is so uh prior to uh running inference

00:14:44,320 --> 00:14:49,920
what happens is that tensorflow lite

00:14:47,440 --> 00:14:54,720
clusters the supported operations

00:14:49,920 --> 00:14:54,720
uh into a sub graph and

00:14:55,199 --> 00:15:00,880
then keeps the rest uh the rest

00:14:58,480 --> 00:15:01,839
is executed on the cpu and then there is

00:15:00,880 --> 00:15:06,399
the subgraph

00:15:01,839 --> 00:15:09,639
which gets copied to the

00:15:06,399 --> 00:15:13,279
well to the delegate

00:15:09,639 --> 00:15:16,639
and it's executed there

00:15:13,279 --> 00:15:20,000
now this might

00:15:16,639 --> 00:15:22,320
introduce several issues

00:15:20,000 --> 00:15:23,839
uh one of them is that you can imagine

00:15:22,320 --> 00:15:27,839
that in case for example you would have

00:15:23,839 --> 00:15:27,839
two types of operations here is

00:15:27,920 --> 00:15:31,040
example where we have convolution 2d

00:15:30,160 --> 00:15:34,000
which is

00:15:31,040 --> 00:15:34,880
supported by by the delegate but then we

00:15:34,000 --> 00:15:37,440
have

00:15:34,880 --> 00:15:38,720
depth device convolution 2d which is not

00:15:37,440 --> 00:15:42,560
so

00:15:38,720 --> 00:15:45,920
this would lead into uh partitioning

00:15:42,560 --> 00:15:46,959
uh the graph a lot and we want to

00:15:45,920 --> 00:15:50,000
refrain from that

00:15:46,959 --> 00:15:53,519
so uh there are two

00:15:50,000 --> 00:15:56,480
mechanis two parameters available in the

00:15:53,519 --> 00:15:57,839
simple delegate interface api uh one of

00:15:56,480 --> 00:16:00,079
them which is supported by

00:15:57,839 --> 00:16:01,680
most of the most of the already

00:16:00,079 --> 00:16:05,199
implemented delegates is the

00:16:01,680 --> 00:16:08,160
max delegated partitions so

00:16:05,199 --> 00:16:09,360
uh using that we may actually limit the

00:16:08,160 --> 00:16:12,320
number of

00:16:09,360 --> 00:16:13,040
uh partitions created typically for

00:16:12,320 --> 00:16:16,399
example

00:16:13,040 --> 00:16:16,880
uh you would set it to a value like two

00:16:16,399 --> 00:16:20,399
or

00:16:16,880 --> 00:16:24,240
three so that only the biggest chunks

00:16:20,399 --> 00:16:27,279
would get delegated but the smaller ones

00:16:24,240 --> 00:16:30,560
would not in in case

00:16:27,279 --> 00:16:31,360
you would run to into a similar use case

00:16:30,560 --> 00:16:34,399
like the one

00:16:31,360 --> 00:16:36,720
shown then there is a second option

00:16:34,399 --> 00:16:39,360
which is many notes per partition

00:16:36,720 --> 00:16:40,240
that one is actually implemented only in

00:16:39,360 --> 00:16:44,320
the

00:16:40,240 --> 00:16:46,560
hexagon dsp or the core ml delegate

00:16:44,320 --> 00:16:48,079
but it may be a good it might be a good

00:16:46,560 --> 00:16:52,959
option to implement it

00:16:48,079 --> 00:16:56,320
in your own delegate and that actually

00:16:52,959 --> 00:16:59,519
what it does is uh

00:16:56,320 --> 00:17:00,639
in case you don't reach the number of

00:16:59,519 --> 00:17:03,680
mugs delegated

00:17:00,639 --> 00:17:05,360
partitions but still have still have

00:17:03,680 --> 00:17:07,679
very small partitions for example

00:17:05,360 --> 00:17:11,120
containing only one or two operations

00:17:07,679 --> 00:17:15,039
then you may set the minimum size and

00:17:11,120 --> 00:17:18,640
only delegate those

00:17:15,039 --> 00:17:22,520
big chunks uh the reason

00:17:18,640 --> 00:17:26,720
actually to set uh set these values

00:17:22,520 --> 00:17:28,880
explicitly might is as i mentioned

00:17:26,720 --> 00:17:30,480
uh the memory transfers which are

00:17:28,880 --> 00:17:35,440
typically quite costly

00:17:30,480 --> 00:17:35,440
between a cpu and a gpu or an npu

00:17:36,320 --> 00:17:44,720
so in order to evaluate your

00:17:39,440 --> 00:17:44,720
delegate there are several options

00:17:45,039 --> 00:17:49,200
the simplest one or one of uh the

00:17:47,919 --> 00:17:52,400
simplest tools

00:17:49,200 --> 00:17:55,760
is the benchmark model application

00:17:52,400 --> 00:17:56,640
which is also in the uh in the

00:17:55,760 --> 00:18:00,400
tensorflow

00:17:56,640 --> 00:18:03,840
repository and

00:18:00,400 --> 00:18:07,760
it's basically a simple tool uh it

00:18:03,840 --> 00:18:11,280
runs inference for your model and

00:18:07,760 --> 00:18:12,160
enables the delegate and after that it

00:18:11,280 --> 00:18:15,039
outputs

00:18:12,160 --> 00:18:16,799
the number of cycles it outputs the

00:18:15,039 --> 00:18:19,840
memory footprint

00:18:16,799 --> 00:18:22,640
uh it outputs inference or

00:18:19,840 --> 00:18:23,200
the warm-up time so that the warm-up

00:18:22,640 --> 00:18:26,559
time that's

00:18:23,200 --> 00:18:29,679
um use case of

00:18:26,559 --> 00:18:33,280
uh of the nn api delegate

00:18:29,679 --> 00:18:36,559
and the idatomix 8n

00:18:33,280 --> 00:18:39,760
npu which does

00:18:36,559 --> 00:18:42,080
some let's say prior optimizations

00:18:39,760 --> 00:18:43,440
to the graph before running inference so

00:18:42,080 --> 00:18:46,400
the first inference cycle

00:18:43,440 --> 00:18:47,280
takes quite a long time for mobile net

00:18:46,400 --> 00:18:51,600
here

00:18:47,280 --> 00:18:54,799
it's it's six seconds but afterwards

00:18:51,600 --> 00:18:57,840
the inference itself or the

00:18:54,799 --> 00:19:03,360
individual iterations they take like

00:18:57,840 --> 00:19:03,360
uh three uh three microseconds

00:19:06,799 --> 00:19:13,280
and also in case you want to check uh

00:19:10,080 --> 00:19:14,799
the correctness of your model uh which

00:19:13,280 --> 00:19:18,960
typically involves

00:19:14,799 --> 00:19:22,400
uh which typically uh involves

00:19:18,960 --> 00:19:25,440
if you have an npu uh the computer

00:19:22,400 --> 00:19:27,919
it might improve the performance of

00:19:25,440 --> 00:19:28,799
running the model by actually computing

00:19:27,919 --> 00:19:32,480
in uh

00:19:28,799 --> 00:19:35,039
lower bit operations so uh in case this

00:19:32,480 --> 00:19:36,320
this becomes an issue uh you might use

00:19:35,039 --> 00:19:40,000
the inference

00:19:36,320 --> 00:19:44,559
div tool for that which is also

00:19:40,000 --> 00:19:47,520
in the tensorflow tensorflow repository

00:19:44,559 --> 00:19:48,400
now uh deeper into the into delegates

00:19:47,520 --> 00:19:51,360
itself

00:19:48,400 --> 00:19:52,480
so as i mentioned uh one of the popular

00:19:51,360 --> 00:19:56,160
delegates is

00:19:52,480 --> 00:19:59,679
the an api it

00:19:56,160 --> 00:20:03,600
defines it is defined by the android c

00:19:59,679 --> 00:20:07,200
api interface which unfortunately

00:20:03,600 --> 00:20:10,480
limits the operations only two

00:20:07,200 --> 00:20:14,000
16-bit floats 32-bit floats and then

00:20:10,480 --> 00:20:16,480
eight bits in dangers so

00:20:14,000 --> 00:20:18,720
it wouldn't be the delegate of choice in

00:20:16,480 --> 00:20:22,080
case you would want to run

00:20:18,720 --> 00:20:25,440
let's say the hybrid 16 bits 8 bits

00:20:22,080 --> 00:20:27,760
model or in case you would have a pure

00:20:25,440 --> 00:20:31,280
16 bit model

00:20:27,760 --> 00:20:34,080
on on the other hand it is designed

00:20:31,280 --> 00:20:35,520
to accelerate your model for android

00:20:34,080 --> 00:20:38,559
devices

00:20:35,520 --> 00:20:40,000
so if there is a gpu npu or a dsp

00:20:38,559 --> 00:20:43,600
available

00:20:40,000 --> 00:20:43,600
it might run there

00:20:43,679 --> 00:20:48,960
and that's also one of the reasons why

00:20:45,679 --> 00:20:52,400
it's the delegate of choice on idatemx8

00:20:48,960 --> 00:20:52,400
microprocessors currently

00:20:56,240 --> 00:21:02,320
to the next delegate so every linaro

00:20:59,520 --> 00:21:06,000
virtual client there is always a

00:21:02,320 --> 00:21:09,440
talk which involves arman so armanin is

00:21:06,000 --> 00:21:12,960
a inference

00:21:09,440 --> 00:21:13,600
framework original devop by arm but it

00:21:12,960 --> 00:21:17,120
was

00:21:13,600 --> 00:21:21,520
donated to the linaro ai initiative

00:21:17,120 --> 00:21:25,039
and the most notable feature is

00:21:21,520 --> 00:21:28,080
it takes uh inputs from all kinds of

00:21:25,039 --> 00:21:32,080
frameworks so tensorflow tensorflow lite

00:21:28,080 --> 00:21:35,840
uh on nx or cafe

00:21:32,080 --> 00:21:39,280
and it's able to run such model

00:21:35,840 --> 00:21:42,640
uh through

00:21:39,280 --> 00:21:45,360
through its engine and

00:21:42,640 --> 00:21:45,919
run it on all kinds of different back

00:21:45,360 --> 00:21:48,640
ends

00:21:45,919 --> 00:21:50,240
those might be cortex a cpus it might be

00:21:48,640 --> 00:21:53,280
a mali

00:21:50,240 --> 00:21:56,840
armali gpu it might be ethos

00:21:53,280 --> 00:21:58,080
and accelerators or there might be a

00:21:56,840 --> 00:22:02,799
dedicated

00:21:58,080 --> 00:22:06,000
back end for the idotemx8 processors

00:22:02,799 --> 00:22:09,120
um if you want to check this out so

00:22:06,000 --> 00:22:10,640
check out ml platform arc or the yarman

00:22:09,120 --> 00:22:12,559
and repository

00:22:10,640 --> 00:22:14,159
there is much more information which can

00:22:12,559 --> 00:22:17,679
be found

00:22:14,159 --> 00:22:20,159
regarding the framework there and

00:22:17,679 --> 00:22:20,799
the most important feature there is the

00:22:20,159 --> 00:22:24,080
armament

00:22:20,799 --> 00:22:24,799
delegate which was which was introduced

00:22:24,080 --> 00:22:27,679
in

00:22:24,799 --> 00:22:28,320
three months ago but it became much more

00:22:27,679 --> 00:22:31,360
major

00:22:28,320 --> 00:22:34,400
in the latest 2102 release

00:22:31,360 --> 00:22:36,240
uh see the docs how to how to integrate

00:22:34,400 --> 00:22:40,159
it or install it

00:22:36,240 --> 00:22:43,679
but as i mentioned

00:22:40,159 --> 00:22:45,760
the most notable feature is that

00:22:43,679 --> 00:22:47,919
if you are using this delegate you would

00:22:45,760 --> 00:22:51,840
be able to execute

00:22:47,919 --> 00:22:54,640
the execution on for example ethos and

00:22:51,840 --> 00:22:56,480
npu currently i'm not aware that there

00:22:54,640 --> 00:23:00,320
will be other options to

00:22:56,480 --> 00:23:03,039
execute on the itos accelerator using

00:23:00,320 --> 00:23:06,720
tensorflow lite

00:23:03,039 --> 00:23:10,159
also you may use the arm compute library

00:23:06,720 --> 00:23:14,480
to execute your graph on the

00:23:10,159 --> 00:23:18,159
mali gpu uh of course

00:23:14,480 --> 00:23:21,200
you uh there are there is the

00:23:18,159 --> 00:23:22,640
gpu delegate which is able to run opencl

00:23:21,200 --> 00:23:25,679
and

00:23:22,640 --> 00:23:29,360
also the default implementation

00:23:25,679 --> 00:23:32,880
accelerates out using arm neon so

00:23:29,360 --> 00:23:35,200
you would have to benchmark your model

00:23:32,880 --> 00:23:36,080
if you want to use the delegate for that

00:23:35,200 --> 00:23:37,919
but for example

00:23:36,080 --> 00:23:39,600
for ethos you don't have any other

00:23:37,919 --> 00:23:41,600
option

00:23:39,600 --> 00:23:43,440
the delegate lives its own life within

00:23:41,600 --> 00:23:47,679
the armament and repository so

00:23:43,440 --> 00:23:49,120
it's built it might be built together

00:23:47,679 --> 00:23:51,679
when you build the whole armament

00:23:49,120 --> 00:23:54,000
package or you may also build it

00:23:51,679 --> 00:23:57,039
completely separate separately

00:23:54,000 --> 00:24:00,960
it uses the cmake build system

00:23:57,039 --> 00:24:03,679
so in case you are familiar with that

00:24:00,960 --> 00:24:04,080
it's just running a single command and

00:24:03,679 --> 00:24:07,600
it will

00:24:04,080 --> 00:24:11,039
produce the libarman and delegate

00:24:07,600 --> 00:24:14,559
shared library which you may then attach

00:24:11,039 --> 00:24:18,159
to your program

00:24:14,559 --> 00:24:22,880
there are two interfaces here

00:24:18,159 --> 00:24:25,679
one of them is the python interface

00:24:22,880 --> 00:24:26,640
which i will quickly cover now and the

00:24:25,679 --> 00:24:29,919
second one is the c

00:24:26,640 --> 00:24:31,120
plus plus one they are they look very

00:24:29,919 --> 00:24:32,960
similar

00:24:31,120 --> 00:24:34,799
it's a matter of different programming

00:24:32,960 --> 00:24:37,919
language so

00:24:34,799 --> 00:24:41,440
here first of all you import your

00:24:37,919 --> 00:24:44,640
python modules you

00:24:41,440 --> 00:24:46,720
you import the tflight runtime which is

00:24:44,640 --> 00:24:51,200
a python wrapper

00:24:46,720 --> 00:24:51,200
for tf lite and

00:24:51,760 --> 00:24:58,400
then you go on you load your delegate

00:24:55,919 --> 00:25:00,960
you that's basically the most important

00:24:58,400 --> 00:25:02,240
thing here which attaches the shared

00:25:00,960 --> 00:25:05,120
library

00:25:02,240 --> 00:25:06,559
you specify the back-ends specify

00:25:05,120 --> 00:25:10,799
several other options

00:25:06,559 --> 00:25:14,400
and that's it

00:25:10,799 --> 00:25:16,320
afterwards you only run the interpreter

00:25:14,400 --> 00:25:17,520
uh where you set the experimental

00:25:16,320 --> 00:25:21,919
delegates uh

00:25:17,520 --> 00:25:24,320
delegate's parameter and

00:25:21,919 --> 00:25:25,360
then you may allocate tensors run

00:25:24,320 --> 00:25:29,600
inference

00:25:25,360 --> 00:25:32,960
print out the results and so on

00:25:29,600 --> 00:25:33,919
the c-plus bus api looks very much

00:25:32,960 --> 00:25:36,960
similar

00:25:33,919 --> 00:25:41,200
so again you specify the back-ends

00:25:36,960 --> 00:25:43,520
and create an instance

00:25:41,200 --> 00:25:46,400
afterwards you feed the interpreter

00:25:43,520 --> 00:25:49,440
using the modify graph delegate

00:25:46,400 --> 00:25:52,640
function with the

00:25:49,440 --> 00:25:55,200
nn delegate and

00:25:52,640 --> 00:25:56,559
afterwards you do pretty much the same

00:25:55,200 --> 00:25:59,279
stuff like we did

00:25:56,559 --> 00:25:59,279
in python

00:25:59,840 --> 00:26:06,559
okay so uh that will be it

00:26:03,120 --> 00:26:07,120
and some of the links for those of you

00:26:06,559 --> 00:26:09,360
who are

00:26:07,120 --> 00:26:11,200
interested to follow up so there is of

00:26:09,360 --> 00:26:14,799
course tensorflow

00:26:11,200 --> 00:26:16,640
there is the armand 2102

00:26:14,799 --> 00:26:17,840
with the tensorflow right delegate so

00:26:16,640 --> 00:26:21,600
for that one

00:26:17,840 --> 00:26:23,039
uh the best source of information is the

00:26:21,600 --> 00:26:26,880
github

00:26:23,039 --> 00:26:29,840
armand's github and

00:26:26,880 --> 00:26:31,440
then for uh nxp idiot makes eight

00:26:29,840 --> 00:26:34,559
microprocessors

00:26:31,440 --> 00:26:38,559
uh on code our nxp releases a modified

00:26:34,559 --> 00:26:42,640
version or a fork of tensorflow and

00:26:38,559 --> 00:26:46,080
also the an imx

00:26:42,640 --> 00:26:48,000
package which contains some interesting

00:26:46,080 --> 00:26:49,679
stuff such as the yanon api delegates

00:26:48,000 --> 00:26:54,080
source codes or

00:26:49,679 --> 00:26:58,480
and a backend for the armament framework

00:26:54,080 --> 00:27:07,840
so that's pretty much it and

00:26:58,480 --> 00:27:07,840
thank you for listening

00:27:12,520 --> 00:27:15,520

YouTube URL: https://www.youtube.com/watch?v=R48eNVs0rTQ


