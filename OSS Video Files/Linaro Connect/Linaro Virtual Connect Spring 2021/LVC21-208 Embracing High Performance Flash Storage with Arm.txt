Title: LVC21-208 Embracing High Performance Flash Storage with Arm
Publication date: 2021-04-15
Playlist: Linaro Virtual Connect Spring 2021
Description: 
	Solid-state storage media is in the process of taking over the data center, whose growing advantages in performance bring a major challenge that current storage software stack overhead becomes a major bottleneck for developing high performance storage applications.

Traditional kernel I/O stacks bring obvious overhead due to context switch, data copy, interrupt, resource synchronization and etc. SPDK minimizes the impact on CPU and Memory bus cycles during IO processing mainly by three ways. One is using the user-mode for storage applications rather than kernel mode. The devices that SPDK takes control are unbound from kernel space driver. And instead, UIO or VFIO driver is used for SPDK to operate devices directly from user space, which thereby eliminates costly kernel context switch. The second is that SPDK runs in a polled mode instead of interrupt mode. It polls devices for completions instead of waiting for interrupts, which reduces the related interrupt handling overhead. And the third one, the shared-nothing thread model. Separate queues exposed by a device like NVMe can be accessed without coordination. SPDK can send requests to the device from multiple threads of execution in parallel without locks for SPDK requires a hardware queue is only ever accessed from one thread at a time. 

Building upon such basis, SPDK further provides a full block stack as a user space library and provides NVMe-oF and vhost servers that are capable of serving disks over the network or to other processes. We can learn from these targets about how to implement a high performance storage target, or use them as the basis for production deployments. 

In this session, we will walk through our practice about running SPDK NVMe-over-TCP on Arm servers. We will show the IO performance and multi-dimentional comparison with the linux kernel driver. And we will show our tuning and optimization details to get performance improved.

The session will have this outline:
1.software framework and the key techniques of SPDK;
2.pratice on SPDK NVMe-over-TCP to accelerate accessing remote NVMe devices on Arm servers;
3.SPDK NVMe-over-TCP tuning and optimization on Arm servers.
Captions: 
	00:00:14,000 --> 00:00:18,000
hi

00:00:14,559 --> 00:00:20,720
i'm rachel from am my topic

00:00:18,000 --> 00:00:23,039
today is embrace high performance

00:00:20,720 --> 00:00:26,560
storage with open arm

00:00:23,039 --> 00:00:29,679
it's about our practice of spdk on

00:00:26,560 --> 00:00:32,880
m64 sbdk

00:00:29,679 --> 00:00:33,760
is the storage performance development

00:00:32,880 --> 00:00:36,399
kit

00:00:33,760 --> 00:00:37,520
it provides a set of tools and the

00:00:36,399 --> 00:00:40,559
libraries

00:00:37,520 --> 00:00:43,120
to create high-performance scalable user

00:00:40,559 --> 00:00:46,320
mode storage applications

00:00:43,120 --> 00:00:50,160
this feature is spdk's architecture

00:00:46,320 --> 00:00:53,199
many inclusive layers from the bottom up

00:00:50,160 --> 00:00:53,760
and the driver there including a mimi

00:00:53,199 --> 00:00:56,640
driver

00:00:53,760 --> 00:00:58,320
and so on and then the broad device

00:00:56,640 --> 00:01:02,160
layer

00:00:58,320 --> 00:01:02,160
in spda it provides

00:01:02,239 --> 00:01:09,920
several kinds of broad device like miami

00:01:05,840 --> 00:01:12,960
or device aio pro device and many others

00:01:09,920 --> 00:01:14,000
and then the broad storage service layer

00:01:12,960 --> 00:01:16,560
it provides

00:01:14,000 --> 00:01:18,080
several services for different purposes

00:01:16,560 --> 00:01:22,080
like logical volume

00:01:18,080 --> 00:01:24,640
red and so on and the top layer is

00:01:22,080 --> 00:01:26,400
a storage protocol layer including a

00:01:24,640 --> 00:01:30,000
memory or fabrics

00:01:26,400 --> 00:01:30,640
the host iscsi leds are capable of

00:01:30,000 --> 00:01:33,680
showing

00:01:30,640 --> 00:01:37,200
disks over the network or to

00:01:33,680 --> 00:01:37,600
other processes besides it implements

00:01:37,200 --> 00:01:42,960
pro

00:01:37,600 --> 00:01:45,200
fs as a file storage service

00:01:42,960 --> 00:01:46,320
we know that traditional kernel house

00:01:45,200 --> 00:01:49,040
desk bring

00:01:46,320 --> 00:01:49,920
upwards of overhead due to container

00:01:49,040 --> 00:01:52,840
switch

00:01:49,920 --> 00:01:54,079
data copy interrupt resource

00:01:52,840 --> 00:01:56,640
synchronization

00:01:54,079 --> 00:01:57,920
spdk can minimize

00:01:56,640 --> 00:02:01,119
[Music]

00:01:57,920 --> 00:02:04,000
the impact on cpu and memory bus cycles

00:02:01,119 --> 00:02:05,040
during our processing by manufacturing

00:02:04,000 --> 00:02:08,560
phase

00:02:05,040 --> 00:02:11,360
one as that it uses the user mode for

00:02:08,560 --> 00:02:12,160
storage applications rather than the

00:02:11,360 --> 00:02:14,959
kernel modes

00:02:12,160 --> 00:02:15,760
the devices that spk test control are

00:02:14,959 --> 00:02:19,200
unbound

00:02:15,760 --> 00:02:23,280
from kernel space driver and in-state

00:02:19,200 --> 00:02:26,560
uio or vfr drive is used but bdk2

00:02:23,280 --> 00:02:27,599
operates devices directly from user

00:02:26,560 --> 00:02:30,000
space

00:02:27,599 --> 00:02:31,280
which thereby eliminates cluster kernel

00:02:30,000 --> 00:02:34,879
content switch

00:02:31,280 --> 00:02:38,160
a second slash spdk runs in a port

00:02:34,879 --> 00:02:40,879
mode instead of interrupt mode

00:02:38,160 --> 00:02:41,360
what spdk initializes on each call

00:02:40,879 --> 00:02:44,879
around

00:02:41,360 --> 00:02:48,640
a thread which is called a reactor

00:02:44,879 --> 00:02:51,920
users can register different powers

00:02:48,640 --> 00:02:54,480
on this reactor

00:02:51,920 --> 00:02:55,920
some of these pullers then pull devices

00:02:54,480 --> 00:02:58,959
for completions

00:02:55,920 --> 00:03:01,599
instead of fighting for interrupts

00:02:58,959 --> 00:03:03,599
and this reduces the related interrupt

00:03:01,599 --> 00:03:06,959
handling overhead

00:03:03,599 --> 00:03:09,280
and the third one is the shadow

00:03:06,959 --> 00:03:11,599
nothing's right model

00:03:09,280 --> 00:03:13,120
each strike operates on its own

00:03:11,599 --> 00:03:16,159
resources

00:03:13,120 --> 00:03:19,200
full threat communications

00:03:16,159 --> 00:03:21,599
spdk doesn't use traditional lock

00:03:19,200 --> 00:03:26,480
mechanism

00:03:21,599 --> 00:03:26,480
but using event ring

00:03:27,760 --> 00:03:33,680
for a lm device a hardware queue

00:03:31,120 --> 00:03:34,799
is only ever assessed from one thread at

00:03:33,680 --> 00:03:38,720
a time

00:03:34,799 --> 00:03:41,840
so an sbdk can send

00:03:38,720 --> 00:03:42,640
requests to the device from multiple

00:03:41,840 --> 00:03:47,040
threads of

00:03:42,640 --> 00:03:50,080
execution in parallel without loss

00:03:47,040 --> 00:03:54,720
and if these three key

00:03:50,080 --> 00:03:58,319
techniques let's help to bring

00:03:54,720 --> 00:03:58,319
spdk's high performance

00:03:58,780 --> 00:04:03,120
[Music]

00:04:00,080 --> 00:04:05,840
about spdk we have contributed

00:04:03,120 --> 00:04:07,360
more than 70 patches to enable and

00:04:05,840 --> 00:04:11,120
optimize as mitigation

00:04:07,360 --> 00:04:13,120
and 64. and in the following slides

00:04:11,120 --> 00:04:15,840
we will walk through our practice about

00:04:13,120 --> 00:04:19,440
surrounding sbds mmv or fabrics

00:04:15,840 --> 00:04:22,479
on our servers in omeo fabrics there

00:04:19,440 --> 00:04:25,680
are different favorites like rtma tcp

00:04:22,479 --> 00:04:29,600
and fiber channel here i will many show

00:04:25,680 --> 00:04:32,400
our work on spda omg of gcp

00:04:29,600 --> 00:04:33,199
i will show the io performance stage and

00:04:32,400 --> 00:04:37,680
the tuning

00:04:33,199 --> 00:04:38,240
tone on m64 and our optimization on spda

00:04:37,680 --> 00:04:43,630
mme

00:04:38,240 --> 00:04:45,400
tcpl throughput

00:04:43,630 --> 00:04:49,840
[Music]

00:04:45,400 --> 00:04:53,040
um is an interface specification

00:04:49,840 --> 00:04:56,400
optimized for solid

00:04:53,040 --> 00:04:59,919
state storage for both client

00:04:56,400 --> 00:05:03,199
and enterprise storage systems

00:04:59,919 --> 00:05:06,560
utilizing the pcie

00:05:03,199 --> 00:05:07,600
interface protocol one of the main

00:05:06,560 --> 00:05:10,240
distinctions

00:05:07,600 --> 00:05:11,280
between amenity and maybe on favorites

00:05:10,240 --> 00:05:13,280
is that

00:05:11,280 --> 00:05:14,960
a trend is the transport mapping

00:05:13,280 --> 00:05:19,680
mechanism for sending

00:05:14,960 --> 00:05:22,479
and receiving commands or responses

00:05:19,680 --> 00:05:23,360
and full local access only match

00:05:22,479 --> 00:05:27,199
commands and

00:05:23,360 --> 00:05:30,960
responses to shard memory in the host

00:05:27,199 --> 00:05:34,560
over the pcie interface protocol while

00:05:30,960 --> 00:05:37,680
many of our fabrics uses

00:05:34,560 --> 00:05:38,720
a message based model for communication

00:05:37,680 --> 00:05:42,080
between a host

00:05:38,720 --> 00:05:45,520
and the target storage device

00:05:42,080 --> 00:05:49,680
currently we have a memory over rdma

00:05:45,520 --> 00:05:49,680
over tcp and a private channel

00:05:50,800 --> 00:05:57,039
the three key elements of ome is

00:05:53,840 --> 00:06:00,080
a submission queue communication

00:05:57,039 --> 00:06:03,759
completion queue and storage register

00:06:00,080 --> 00:06:06,800
a submission queue is a circular buffer

00:06:03,759 --> 00:06:09,360
used to hold miami command

00:06:06,800 --> 00:06:11,120
and condition here is to hold the

00:06:09,360 --> 00:06:14,319
commission status of

00:06:11,120 --> 00:06:16,960
each mma command so

00:06:14,319 --> 00:06:17,520
mission qs completion cube here can be

00:06:16,960 --> 00:06:20,960
set

00:06:17,520 --> 00:06:24,720
in the host memory are in only

00:06:20,960 --> 00:06:27,919
me but in most cases we set

00:06:24,720 --> 00:06:32,000
it in the host memory

00:06:27,919 --> 00:06:36,560
each entry in the submission queue

00:06:32,000 --> 00:06:39,280
is a 64 byte command

00:06:36,560 --> 00:06:39,759
physical locations in memory to use for

00:06:39,280 --> 00:06:42,800
dates

00:06:39,759 --> 00:06:47,280
transfers are specified

00:06:42,800 --> 00:06:51,199
using a prp physical region page

00:06:47,280 --> 00:06:54,319
entries or sgl as the scheduler

00:06:51,199 --> 00:06:57,919
get their list and

00:06:54,319 --> 00:07:03,840
trp and sgl are both structures

00:06:57,919 --> 00:07:03,840
to describe the memory footage

00:07:04,160 --> 00:07:10,639
in each system we have only

00:07:07,280 --> 00:07:12,319
one other main submission and

00:07:10,639 --> 00:07:16,560
combination killed here

00:07:12,319 --> 00:07:19,759
and there may be up to says t4

00:07:16,560 --> 00:07:23,520
kill bar okay setting for k

00:07:19,759 --> 00:07:23,520
minus one um

00:07:23,840 --> 00:07:31,840
ioq here two piers

00:07:32,639 --> 00:07:37,919
and mmu-tcp makes it possible to use a

00:07:36,240 --> 00:07:40,800
menu of fabric

00:07:37,919 --> 00:07:42,000
fabrics across a standard ethernet

00:07:40,800 --> 00:07:45,360
network

00:07:42,000 --> 00:07:46,000
there is no need to make configuration

00:07:45,360 --> 00:07:49,840
changes

00:07:46,000 --> 00:07:53,120
or implement any spatial equipment

00:07:49,840 --> 00:07:56,720
when using a tcp transports

00:07:53,120 --> 00:08:00,240
each mm cable are mapped to

00:07:56,720 --> 00:08:04,160
a tcp connection and then omeg commands

00:08:00,240 --> 00:08:07,759
and that are in tab suits as a

00:08:04,160 --> 00:08:13,840
proto protocol that you need called pdu

00:08:07,759 --> 00:08:13,840
and sent over standard tcp sockets

00:08:16,240 --> 00:08:21,360
on the one hand spdk photos almost tcps

00:08:20,319 --> 00:08:24,840
protocol

00:08:21,360 --> 00:08:28,479
to implement the connection

00:08:24,840 --> 00:08:31,199
initialization process a rewrite

00:08:28,479 --> 00:08:32,719
interaction process and so on on the

00:08:31,199 --> 00:08:36,399
other hand

00:08:32,719 --> 00:08:38,800
sbdk implements the qpr

00:08:36,399 --> 00:08:40,560
data handling process according to its

00:08:38,800 --> 00:08:43,760
own framework feature

00:08:40,560 --> 00:08:47,260
i mentioned that each mmeq peer is

00:08:43,760 --> 00:08:48,800
mapped to a tcp connection

00:08:47,260 --> 00:08:52,959
[Music]

00:08:48,800 --> 00:08:55,839
in sbdk target site spda will create

00:08:52,959 --> 00:08:56,320
a new socket for each coming connection

00:08:55,839 --> 00:08:59,360
then

00:08:56,320 --> 00:09:02,480
each socket is assigned to a socket

00:08:59,360 --> 00:09:05,680
group a socket group

00:09:02,480 --> 00:09:09,200
created on a cpu curve and four

00:09:05,680 --> 00:09:12,480
positive sockets epo is used to

00:09:09,200 --> 00:09:15,200
call the events of each socket in a

00:09:12,480 --> 00:09:15,200
socket group

00:09:17,200 --> 00:09:23,600
and once there is state in a socket

00:09:20,560 --> 00:09:26,959
begins to read the date from socket

00:09:23,600 --> 00:09:27,519
buffer and handle and according to the

00:09:26,959 --> 00:09:31,360
video

00:09:27,519 --> 00:09:38,160
type over to a pdu

00:09:31,360 --> 00:09:41,279
here and a mme tcp pdu contains

00:09:38,160 --> 00:09:45,839
the common header specific header

00:09:41,279 --> 00:09:48,640
padding panels and digest ratings

00:09:45,839 --> 00:09:49,760
in video handling stat machine it

00:09:48,640 --> 00:09:53,600
firstly hand

00:09:49,760 --> 00:09:56,800
the before state handles the camera head

00:09:53,600 --> 00:09:58,320
and then the specific header if there is

00:09:56,800 --> 00:10:02,640
no payload

00:09:58,320 --> 00:10:07,040
then uh is ready to handle an spu

00:10:02,640 --> 00:10:10,079
or if there is panels which may include

00:10:07,040 --> 00:10:14,880
include commands or dates it goes to

00:10:10,079 --> 00:10:14,880
the next stage to handle payload

00:10:16,839 --> 00:10:22,880
um we have

00:10:20,079 --> 00:10:24,079
said that spdk's design should be a

00:10:22,880 --> 00:10:27,040
synchronous

00:10:24,079 --> 00:10:29,920
and its data path is not free and it's a

00:10:27,040 --> 00:10:34,079
user's best storage protocol

00:10:29,920 --> 00:10:34,720
although in mma tcp the kernel tcp stack

00:10:34,079 --> 00:10:38,079
is not

00:10:34,720 --> 00:10:41,440
logless but spdk leverages

00:10:38,079 --> 00:10:44,839
some methods to use tcp stack

00:10:41,440 --> 00:10:46,399
more efficiently and for single source

00:10:44,839 --> 00:10:49,760
optimization

00:10:46,399 --> 00:10:53,440
one is that uses non-blocking io

00:10:49,760 --> 00:10:56,800
to avoid the strike to be brought

00:10:53,440 --> 00:10:58,640
while doing some operations like reading

00:10:56,800 --> 00:11:03,040
or writing sockets

00:10:58,640 --> 00:11:05,920
or this introduces a partial read

00:11:03,040 --> 00:11:07,440
and write problem for example during

00:11:05,920 --> 00:11:11,120
appropriate

00:11:07,440 --> 00:11:14,560
when the right way system call returns

00:11:11,120 --> 00:11:15,920
just a part of date is written into the

00:11:14,560 --> 00:11:19,120
tcp buffer

00:11:15,920 --> 00:11:22,240
and to solve this some extra

00:11:19,120 --> 00:11:23,680
phrases are introduced to mark how much

00:11:22,240 --> 00:11:27,040
of a request is sent

00:11:23,680 --> 00:11:30,399
or received and the second is that

00:11:27,040 --> 00:11:33,920
it introduces pipe buffer to

00:11:30,399 --> 00:11:38,160
read as much data as possible into

00:11:33,920 --> 00:11:41,519
buffer at a time to reduce this cost

00:11:38,160 --> 00:11:44,640
the third is it uses battery rights

00:11:41,519 --> 00:11:48,000
to reduce this cost which is done

00:11:44,640 --> 00:11:51,519
by merging multi right

00:11:48,000 --> 00:11:55,200
way into one and besides

00:11:51,519 --> 00:11:59,279
the send messages zero copy factor is

00:11:55,200 --> 00:12:02,880
leveraged to avoid user data is copied

00:11:59,279 --> 00:12:06,720
from user space to kernel tcp buffer

00:12:02,880 --> 00:12:11,040
when sending that which minimizes

00:12:06,720 --> 00:12:14,959
the cost of memory copy

00:12:11,040 --> 00:12:18,000
and for multi-surface optimization

00:12:14,959 --> 00:12:21,839
each socket is managed by only one

00:12:18,000 --> 00:12:25,440
sbdk stride to avoid completion

00:12:21,839 --> 00:12:29,120
competition and using usage evo

00:12:25,440 --> 00:12:30,079
to detect the events on a per group of

00:12:29,120 --> 00:12:33,760
sockets

00:12:30,079 --> 00:12:33,760
with just one c score

00:12:35,760 --> 00:12:43,279
and in urine sub implementation

00:12:39,600 --> 00:12:46,240
we can submit rights of operations of

00:12:43,279 --> 00:12:46,639
a group of circuits at a time but not

00:12:46,240 --> 00:12:53,839
call

00:12:46,639 --> 00:12:53,839
right way for each circuit

00:12:55,440 --> 00:13:02,240
and this is our testing environment

00:12:58,800 --> 00:13:06,079
we have um six

00:13:02,240 --> 00:13:10,079
piece four tss double zero

00:13:06,079 --> 00:13:13,120
of any ssds and one 100 gigabit

00:13:10,079 --> 00:13:15,839
and not sneak we use it

00:13:13,120 --> 00:13:17,360
four kilobytes kernel page size and vfl

00:13:15,839 --> 00:13:20,720
pci driver

00:13:17,360 --> 00:13:25,200
with io mmu through and

00:13:20,720 --> 00:13:29,040
make sure that there are enough

00:13:25,200 --> 00:13:34,480
huge pages on the newman nodes well

00:13:29,040 --> 00:13:39,040
spdk targets runs

00:13:34,480 --> 00:13:43,120
on this is the iops and ios reports with

00:13:39,040 --> 00:13:44,160
vocal bias payload size and 128

00:13:43,120 --> 00:13:48,000
kilotanks

00:13:44,160 --> 00:13:53,279
spdk targets runs on only eight cost

00:13:48,000 --> 00:13:53,279
and for render rate we can achieve about

00:13:53,360 --> 00:14:01,040
actually about 1.9 million lps with 24

00:13:57,839 --> 00:14:05,120
with 24 initiator cross and

00:14:01,040 --> 00:14:08,320
the right we can achieve uh

00:14:05,120 --> 00:14:11,680
0.9 million rps and for

00:14:08,320 --> 00:14:14,480
mixed rent rate and right

00:14:11,680 --> 00:14:16,160
with 70 percent rent rates you can

00:14:14,480 --> 00:14:19,440
achieve 1.4

00:14:16,160 --> 00:14:19,440
in your iops

00:14:21,519 --> 00:14:26,160
and on such basis we did sound tuning

00:14:24,160 --> 00:14:29,199
and optimization to

00:14:26,160 --> 00:14:30,959
optimize the performance we tuned the

00:14:29,199 --> 00:14:34,160
pci parameters

00:14:30,959 --> 00:14:37,440
and we i interrupt affinity and kernel

00:14:34,160 --> 00:14:37,440
tcp parameters

00:14:37,600 --> 00:14:44,320
for pcie setting except for

00:14:40,639 --> 00:14:48,160
pci wife and speed we can change

00:14:44,320 --> 00:14:51,600
pci's max payload size and mesh read

00:14:48,160 --> 00:14:54,959
request size it's up to them

00:14:51,600 --> 00:14:56,720
azure is up to the manufacturer to set a

00:14:54,959 --> 00:15:00,800
maximum

00:14:56,720 --> 00:15:00,800
pci package panel size

00:15:02,160 --> 00:15:08,739
supported by the pcie devices

00:15:05,590 --> 00:15:08,739
[Music]

00:15:09,279 --> 00:15:16,160
the max payload size

00:15:12,800 --> 00:15:17,120
limits the payload size when a pci

00:15:16,160 --> 00:15:20,399
device sends

00:15:17,120 --> 00:15:24,720
a package and mesh read request

00:15:20,399 --> 00:15:29,760
size limits the maximum bytes

00:15:24,720 --> 00:15:29,760
that's one rate or request can read

00:15:30,670 --> 00:15:33,760
[Music]

00:15:33,920 --> 00:15:40,560
for make for nick

00:15:37,120 --> 00:15:43,759
we can set next th and is

00:15:40,560 --> 00:15:47,279
q number and q depth and

00:15:43,759 --> 00:15:51,040
interrupt affinity uh in general we

00:15:47,279 --> 00:15:52,399
set a number of q equal to the number of

00:15:51,040 --> 00:15:55,199
cpu cost

00:15:52,399 --> 00:15:56,720
and start i interrupt balance services

00:15:55,199 --> 00:15:59,759
to make

00:15:56,720 --> 00:16:03,519
interrupts are balanced on each cpu

00:15:59,759 --> 00:16:07,600
but in our test setting only four

00:16:03,519 --> 00:16:08,320
right and tsq and the interrupts bound

00:16:07,600 --> 00:16:12,160
to full

00:16:08,320 --> 00:16:15,839
cost can get a better

00:16:12,160 --> 00:16:18,800
performance than binding interrupts to

00:16:15,839 --> 00:16:18,800
r8 cost

00:16:19,040 --> 00:16:22,240
and interrupt code lessons means we can

00:16:21,600 --> 00:16:25,600
set

00:16:22,240 --> 00:16:29,519
how long or after how many frames

00:16:25,600 --> 00:16:29,519
are received king

00:16:30,000 --> 00:16:34,560
they can generate and her art

00:16:34,839 --> 00:16:41,360
interrupts and

00:16:37,839 --> 00:16:45,199
from our test we don't use the

00:16:41,360 --> 00:16:48,720
original adaptive one we

00:16:45,199 --> 00:16:52,320
set the number

00:16:48,720 --> 00:16:55,600
the time according to our

00:16:52,320 --> 00:16:58,079
case and besides

00:16:55,600 --> 00:16:58,800
we can leverage the needs several

00:16:58,079 --> 00:17:04,000
offload

00:16:58,800 --> 00:17:07,439
features like tso gsr and gr

00:17:04,000 --> 00:17:10,480
it then it depends on the need to do the

00:17:07,439 --> 00:17:11,280
tcp package segmentation work and so

00:17:10,480 --> 00:17:13,839
much

00:17:11,280 --> 00:17:14,959
much received tcp package into the

00:17:13,839 --> 00:17:18,559
bigger one

00:17:14,959 --> 00:17:22,000
this can reduces

00:17:18,559 --> 00:17:22,000
some cpu of the head

00:17:22,240 --> 00:17:28,240
and focal tcp parameters we can stack

00:17:25,439 --> 00:17:29,679
the tcp buffer size according to gi

00:17:28,240 --> 00:17:32,960
applications

00:17:29,679 --> 00:17:34,640
and soft lensing is about how many

00:17:32,960 --> 00:17:39,280
packets or how

00:17:34,640 --> 00:17:42,880
long an api can call as high

00:17:39,280 --> 00:17:44,720
it's set by a net device budget and net

00:17:42,880 --> 00:17:49,440
device budget new seconds

00:17:44,720 --> 00:17:52,720
and device weight usually parameters

00:17:49,440 --> 00:17:56,320
is different from the hardware or

00:17:52,720 --> 00:17:56,320
interrupted correlations

00:17:56,640 --> 00:18:03,520
and the queuing discipline parameters

00:18:00,320 --> 00:18:04,240
set the traffic control at algorithm and

00:18:03,520 --> 00:18:09,120
learn

00:18:04,240 --> 00:18:12,320
related queue size a net device

00:18:09,120 --> 00:18:16,480
mesh by lock nut device back by lock

00:18:12,320 --> 00:18:20,160
and tcq banks are the kill size

00:18:16,480 --> 00:18:22,799
for example one way

00:18:20,160 --> 00:18:25,039
insert a new packet when inserting a new

00:18:22,799 --> 00:18:28,240
cab package into the backlog queue

00:18:25,039 --> 00:18:29,679
if the package in the back log queue

00:18:28,240 --> 00:18:33,600
have already

00:18:29,679 --> 00:18:38,000
reached the number of net devices by log

00:18:33,600 --> 00:18:38,000
the new package should be abandoned

00:18:41,280 --> 00:18:49,039
and besides boy tunes uh related spdk

00:18:44,960 --> 00:18:52,160
parameters to achieve a best iops

00:18:49,039 --> 00:18:55,440
you adjust list four of them

00:18:52,160 --> 00:18:58,000
encapsulate size and

00:18:55,440 --> 00:18:59,360
means how many days can be carried in a

00:18:58,000 --> 00:19:03,679
command capsule

00:18:59,360 --> 00:19:06,880
without its host needs to wait for

00:19:03,679 --> 00:19:09,919
an extra ready to transfer command

00:19:06,880 --> 00:19:10,400
sent from the control side and then

00:19:09,919 --> 00:19:14,080
sends

00:19:10,400 --> 00:19:14,799
the date to controller and c2hd success

00:19:14,080 --> 00:19:18,640
controls

00:19:14,799 --> 00:19:23,200
whether to send a response to host

00:19:18,640 --> 00:19:25,919
after controller things update to host

00:19:23,200 --> 00:19:27,840
and enable receive pipe controls whether

00:19:25,919 --> 00:19:31,200
to use the pipe buffer

00:19:27,840 --> 00:19:32,559
to reduce this cost and enable zero copy

00:19:31,200 --> 00:19:35,679
signs means whether

00:19:32,559 --> 00:19:36,400
to use the zero copy feature of send

00:19:35,679 --> 00:19:40,400
messages

00:19:36,400 --> 00:19:43,520
of the bill difficult this car

00:19:40,400 --> 00:19:46,559
and in addition to tuning the parameters

00:19:43,520 --> 00:19:49,840
we introduced some new features

00:19:46,559 --> 00:19:50,640
to optimize the performance this one is

00:19:49,840 --> 00:19:53,600
using

00:19:50,640 --> 00:19:54,640
the actual incoming cpu flag of

00:19:53,600 --> 00:19:57,919
socket

00:19:54,640 --> 00:20:01,120
we can get a cpu that is handling

00:19:57,919 --> 00:20:04,400
the dates of a socket in kernel space

00:20:01,120 --> 00:20:07,120
and then assign the 4-inch handle of

00:20:04,400 --> 00:20:08,080
this socket in user space to the

00:20:07,120 --> 00:20:11,440
expertise

00:20:08,080 --> 00:20:15,200
cpu which can provide optimal

00:20:11,440 --> 00:20:18,400
normal humor behavior and keep

00:20:15,200 --> 00:20:19,200
the cpu cache is hard and with this

00:20:18,400 --> 00:20:22,320
feature

00:20:19,200 --> 00:20:25,440
you can get about 111

00:20:22,320 --> 00:20:29,600
to 17 percent rent right performance

00:20:25,440 --> 00:20:32,640
boost for uh positive sockets and

00:20:29,600 --> 00:20:36,720
8 to 12 percent for

00:20:32,640 --> 00:20:36,720
during under test environment

00:20:37,440 --> 00:20:40,799
the detailed implementation is that we

00:20:40,400 --> 00:20:44,799
use

00:20:40,799 --> 00:20:48,400
a soft map to record the cpu id

00:20:44,799 --> 00:20:52,400
and designation group here when

00:20:48,400 --> 00:20:53,120
jane saw group initialization in each

00:20:52,400 --> 00:20:56,320
car

00:20:53,120 --> 00:20:57,600
the cpu id and sub group appear is in

00:20:56,320 --> 00:21:02,370
searches

00:20:57,600 --> 00:21:05,550
and then one a new connection counts

00:21:02,370 --> 00:21:05,550
[Music]

00:21:07,280 --> 00:21:11,520
when new connection comes we create a

00:21:10,640 --> 00:21:14,480
new socket

00:21:11,520 --> 00:21:16,080
for this connection and then get the

00:21:14,480 --> 00:21:19,440
xock incoming cpu

00:21:16,080 --> 00:21:19,440
id of this circuit

00:21:20,159 --> 00:21:27,200
if the exo-incoming cpu id heads

00:21:23,360 --> 00:21:28,960
which means defines an entry in the map

00:21:27,200 --> 00:21:31,360
then the circuit goes to the

00:21:28,960 --> 00:21:34,480
corresponding group to handle

00:21:31,360 --> 00:21:38,960
or if not it it gets

00:21:34,480 --> 00:21:42,559
a group by default round lobby method

00:21:38,960 --> 00:21:45,840
you can refer to the patch link here for

00:21:42,559 --> 00:21:45,840
for details

00:21:47,890 --> 00:21:53,280
[Music]

00:21:49,679 --> 00:21:56,840
and this optimization leverages

00:21:53,280 --> 00:22:01,440
the smooth batteries ground logging

00:21:56,840 --> 00:22:04,320
algorithm the default algorithm in spdk

00:22:01,440 --> 00:22:04,640
to assign the sockets to different cpus

00:22:04,320 --> 00:22:08,159
is

00:22:04,640 --> 00:22:10,840
run the lobby it means that each car

00:22:08,159 --> 00:22:12,080
deals with an average number of

00:22:10,840 --> 00:22:15,280
connections

00:22:12,080 --> 00:22:16,480
and but from our test with eight calls

00:22:15,280 --> 00:22:19,840
in total

00:22:16,480 --> 00:22:23,120
the performance based need interrupts

00:22:19,840 --> 00:22:27,200
on to full cores is better than

00:22:23,120 --> 00:22:31,280
interrupt bunch r8 cost and this means

00:22:27,200 --> 00:22:34,080
the full cost let's deal with interrupts

00:22:31,280 --> 00:22:36,640
have more cpu overhead than other

00:22:34,080 --> 00:22:39,919
forecasts

00:22:36,640 --> 00:22:40,559
so we expect the focal space lower

00:22:39,919 --> 00:22:43,600
internal

00:22:40,559 --> 00:22:44,960
overheads to deal to deal with small

00:22:43,600 --> 00:22:48,720
connections

00:22:44,960 --> 00:22:52,320
so we introduce a better ground rubbing

00:22:48,720 --> 00:22:54,480
and smooth vertical round robin

00:22:52,320 --> 00:22:57,360
not only ensure that distribute

00:22:54,480 --> 00:23:01,200
distribution is in line with the fact

00:22:57,360 --> 00:23:06,240
also make sure that the nodes

00:23:01,200 --> 00:23:06,240
are selected evenly in a period

00:23:06,720 --> 00:23:13,039
and this algorithm is not complicated

00:23:10,900 --> 00:23:17,520
[Music]

00:23:13,039 --> 00:23:20,320
for each node here up here we have two

00:23:17,520 --> 00:23:21,440
attributes a current byte which is

00:23:20,320 --> 00:23:25,600
initialized to

00:23:21,440 --> 00:23:26,640
zero and weight let's reset for this

00:23:25,600 --> 00:23:30,000
node

00:23:26,640 --> 00:23:31,600
on each node selection we increase the

00:23:30,000 --> 00:23:34,720
current weight of each node

00:23:31,600 --> 00:23:38,480
by its weight and then select

00:23:34,720 --> 00:23:41,520
the node with greatest current weight

00:23:38,480 --> 00:23:44,960
and reduce its current weight by

00:23:41,520 --> 00:23:49,039
sum of each nose

00:23:44,960 --> 00:23:51,440
weight and which this algorithm

00:23:49,039 --> 00:23:52,080
for example there are three nodes abc

00:23:51,440 --> 00:23:56,080
base

00:23:52,080 --> 00:23:58,799
weight five one one a final display

00:23:56,080 --> 00:24:00,159
distribution with smooth weighted round

00:23:58,799 --> 00:24:03,360
robin is

00:24:00,159 --> 00:24:07,559
a a b a c a a and

00:24:03,360 --> 00:24:10,559
not the original result is

00:24:07,559 --> 00:24:12,960
cbaa

00:24:10,559 --> 00:24:14,080
and you can refer to these two things to

00:24:12,960 --> 00:24:17,279
learn more about

00:24:14,080 --> 00:24:18,400
the algorithm and proof and using this

00:24:17,279 --> 00:24:22,559
algorithm we can

00:24:18,400 --> 00:24:25,679
get about eight to fourteen

00:24:22,559 --> 00:24:29,039
percent random rate lps improvement with

00:24:25,679 --> 00:24:29,039
a set of proper ways

00:24:30,840 --> 00:24:35,120
according uh this is the final

00:24:33,760 --> 00:24:38,720
performance

00:24:35,120 --> 00:24:40,320
the column with solid feeding is the

00:24:38,720 --> 00:24:42,960
original performance

00:24:40,320 --> 00:24:45,679
and the accordions risk of league line

00:24:42,960 --> 00:24:49,679
failing is the iops with tuning

00:24:45,679 --> 00:24:52,720
and optimization with 24 initiate course

00:24:49,679 --> 00:24:56,000
the mixed render rate and rendering

00:24:52,720 --> 00:25:00,159
performance increases from 1.4 million

00:24:56,000 --> 00:25:04,320
to 1.7 million lps

00:25:00,159 --> 00:25:07,840
and rent rights

00:25:04,320 --> 00:25:11,520
ilps bridge reaches 1 million and rent

00:25:07,840 --> 00:25:11,520
rift reaches 2 million

00:25:13,440 --> 00:25:19,760
and the foreign work we

00:25:16,640 --> 00:25:22,159
plan to continue to tuning spdk on eo

00:25:19,760 --> 00:25:26,000
fabrics on m64

00:25:22,159 --> 00:25:29,600
and also uh we'll do some organization

00:25:26,000 --> 00:25:43,120
with the sve yeah that's

00:25:29,600 --> 00:25:43,120

YouTube URL: https://www.youtube.com/watch?v=Pb-q3LnQgUI


