Title: BKK19-402 - Inferencing at the edge and Fragmentation Challenges
Publication date: 2019-04-11
Playlist: Linaro Connect Bangkok 2019
Description: 
	Abstract
As deep learning (DL) expands is application into ever more areas, DL at the edge has become an area of rapid innovation and has also become highly fragmented. This creates a challenge in the ecosystem for framework providers that want to take advantage of specialized hardware, and an equal challenge for SoC providers, or makers of DL accelerators that need to support various frameworks, customer innovations, device constraints, etc. This talk will explore what constitutes DL at the edge, it will highlight the recent trends in this area from runtimes and compilers, to model formats, and explore the challenges, and scalability needs of collaborative solutions.

Mark Charlebois
Mark is Director Engineering in Qualcomm Technologies Inc (QTI) in the Machine Learning Group. Currently he is focused on Neural Processing Runtime for Qualcomm SoCs, AI Benchmarking, and also serves as an open source Trusted Advisor for the MLG group. He has represented QTI on the Linux Foundation board, served on the Dronecode board and Core Infrastructure Initiative steering committee, and as the TSC lead for Dronecode. Mark also contributed code to the PX4 Open Source Flight Stack (http://github.com/PX4/Firmware), and to the LLVMLinux project with associated patches for the Linux kernel. He also helps support the Dragonboard developer platforms and has been working in embedded software for over 25 years. Mark has a BASc in Systems Design Engineering from the University of Waterloo, and a MASc in Engineering Science from Simon Frazer University.
Captions: 
	00:00:06,129 --> 00:00:12,709
Billee morning session I know some late

00:00:09,799 --> 00:00:14,929
nights for everybody so I work in the

00:00:12,709 --> 00:00:17,180
Qualcomm machine learning group on the

00:00:14,929 --> 00:00:20,060
Qualcomm Snapdragon neural processing

00:00:17,180 --> 00:00:21,200
engine and I've been looking at the

00:00:20,060 --> 00:00:23,509
fragmentation in the space of

00:00:21,200 --> 00:00:25,399
inferencing at the edge and wanted to do

00:00:23,509 --> 00:00:27,980
a little deeper dive and then thought I

00:00:25,399 --> 00:00:30,640
would give a talk on my findings and try

00:00:27,980 --> 00:00:33,020
to give an overview of the problem space

00:00:30,640 --> 00:00:35,809
so I'll talk about what I mean by

00:00:33,020 --> 00:00:38,620
inferencing at the edge and then look at

00:00:35,809 --> 00:00:41,900
some different deployment use cases and

00:00:38,620 --> 00:00:44,570
application development use cases talk

00:00:41,900 --> 00:00:47,090
about runtimes what they are give some

00:00:44,570 --> 00:00:49,220
examples and then go into graph

00:00:47,090 --> 00:00:50,960
compilers what I mean by that talk about

00:00:49,220 --> 00:00:55,160
different levels of intermediate

00:00:50,960 --> 00:00:56,690
representation and then how what's the

00:00:55,160 --> 00:00:58,760
possibility of leveraging runtimes

00:00:56,690 --> 00:01:00,290
inside of these graph compiler

00:00:58,760 --> 00:01:05,089
frameworks and then some of the

00:01:00,290 --> 00:01:08,630
conclusions that I came to so typically

00:01:05,089 --> 00:01:11,210
you'd have you know big servers with you

00:01:08,630 --> 00:01:13,159
know hulking NVIDIA GPUs and so forth in

00:01:11,210 --> 00:01:15,799
the cloud that you'd be designing and

00:01:13,159 --> 00:01:18,950
training models on with millions of

00:01:15,799 --> 00:01:20,869
images and then you create these image

00:01:18,950 --> 00:01:22,909
you create the models but then you want

00:01:20,869 --> 00:01:25,039
to deploy them at scale at the edge and

00:01:22,909 --> 00:01:27,920
the edge could be anything from an edge

00:01:25,039 --> 00:01:30,170
server to you know a microcontroller

00:01:27,920 --> 00:01:31,819
running something very simple taking you

00:01:30,170 --> 00:01:34,579
know one frame a second and processing

00:01:31,819 --> 00:01:36,679
it or listening for a simple keyword so

00:01:34,579 --> 00:01:41,209
there's a huge range of devices and use

00:01:36,679 --> 00:01:43,249
cases that constitute the edge but the

00:01:41,209 --> 00:01:46,609
the thing that that's really common with

00:01:43,249 --> 00:01:48,829
that is you know with the the end of

00:01:46,609 --> 00:01:50,810
Moore's law looking at how we can get

00:01:48,829 --> 00:01:52,459
more performance and for things like a

00:01:50,810 --> 00:01:54,079
neural network which takes advantage of

00:01:52,459 --> 00:01:56,359
specific hardware to run much more

00:01:54,079 --> 00:01:57,619
efficiently and faster there's a lot of

00:01:56,359 --> 00:01:59,659
new hardware being developed in this

00:01:57,619 --> 00:02:03,169
space and so that leads to an enormous

00:01:59,659 --> 00:02:04,880
diversity of hardware at the edge and

00:02:03,169 --> 00:02:07,399
there's new approaches like these

00:02:04,880 --> 00:02:10,399
compiler Tory model compilers that are

00:02:07,399 --> 00:02:12,349
trying to address the problem of how do

00:02:10,399 --> 00:02:17,060
you target all of these different

00:02:12,349 --> 00:02:19,160
devices optimally the the common part I

00:02:17,060 --> 00:02:20,750
guess is that all of the approaches have

00:02:19,160 --> 00:02:22,250
converters to convert these different

00:02:20,750 --> 00:02:23,690
graphs from the the various deep

00:02:22,250 --> 00:02:25,910
learning frameworks they have

00:02:23,690 --> 00:02:28,010
optimization passes you can target

00:02:25,910 --> 00:02:30,890
different runtimes so that a family of

00:02:28,010 --> 00:02:33,500
devices can be targeted by that runtime

00:02:30,890 --> 00:02:36,230
or you can have a ot compilers that

00:02:33,500 --> 00:02:40,310
generate code that can run optimally on

00:02:36,230 --> 00:02:41,870
a device so what are some of the

00:02:40,310 --> 00:02:44,360
deployment use cases that we're trying

00:02:41,870 --> 00:02:47,000
to solve here so in some cases you have

00:02:44,360 --> 00:02:49,340
a known model and say that you are

00:02:47,000 --> 00:02:50,870
making a benchmarking app and you want

00:02:49,340 --> 00:02:53,090
it to run in a variety of phones in the

00:02:50,870 --> 00:02:54,710
marketplace and so what you need to do

00:02:53,090 --> 00:02:57,350
to have it run optimally is to

00:02:54,710 --> 00:03:00,410
incorporate the SDKs from the various

00:02:57,350 --> 00:03:02,780
SOC providers so that you're running the

00:03:00,410 --> 00:03:04,160
application on the specific hardware

00:03:02,780 --> 00:03:06,200
accelerators when it's on that

00:03:04,160 --> 00:03:07,940
particular device and so you have to

00:03:06,200 --> 00:03:10,250
bundle all these things into the app or

00:03:07,940 --> 00:03:14,150
create separate applications that target

00:03:10,250 --> 00:03:15,710
those specific phones another use case

00:03:14,150 --> 00:03:17,300
would be where you know the model and

00:03:15,710 --> 00:03:20,090
you know the device so say that you're a

00:03:17,300 --> 00:03:22,010
phone manufacturer and you're creating a

00:03:20,090 --> 00:03:23,660
special case where you when you pull up

00:03:22,010 --> 00:03:25,580
the camera app it does some image

00:03:23,660 --> 00:03:27,170
recognition so that's a case where I

00:03:25,580 --> 00:03:31,190
know the model I know the device I can

00:03:27,170 --> 00:03:34,100
highly tailor that and then a third case

00:03:31,190 --> 00:03:36,920
would be I have a fleet of devices let's

00:03:34,100 --> 00:03:38,630
say Amazon echo family of devices for

00:03:36,920 --> 00:03:40,670
echo show to the echo device to the

00:03:38,630 --> 00:03:42,140
speaker and you've got a range of class

00:03:40,670 --> 00:03:45,350
of processors that may be in those

00:03:42,140 --> 00:03:47,090
things but you want to deploy a use case

00:03:45,350 --> 00:03:50,900
in a model that goes and scales across

00:03:47,090 --> 00:03:53,930
all of them so you design your software

00:03:50,900 --> 00:03:55,430
you create the different binaries that

00:03:53,930 --> 00:03:56,900
will run on the devices and then you

00:03:55,430 --> 00:03:58,940
have to deploy and manage those things

00:03:56,900 --> 00:04:04,820
across your fleet with different devices

00:03:58,940 --> 00:04:08,900
so if I'm a developer what are my

00:04:04,820 --> 00:04:10,010
challenges so as I mentioned before if

00:04:08,900 --> 00:04:11,510
I'm trying to make an app for the

00:04:10,010 --> 00:04:13,280
marketplace what I'm doing is I'm taking

00:04:11,510 --> 00:04:14,590
the different vendor runtimes and I'm

00:04:13,280 --> 00:04:16,940
incorporating those into my application

00:04:14,590 --> 00:04:19,880
so that they can run optimally on

00:04:16,940 --> 00:04:21,830
different devices and one of the things

00:04:19,880 --> 00:04:23,990
Google's done is they've extended the

00:04:21,830 --> 00:04:28,160
android framework to add something

00:04:23,990 --> 00:04:30,710
called in an api and that allows an app

00:04:28,160 --> 00:04:31,670
to be written once and use this api to

00:04:30,710 --> 00:04:34,250
run a neural

00:04:31,670 --> 00:04:36,530
work and then inside the particular

00:04:34,250 --> 00:04:38,540
device itself it implements a hal layer

00:04:36,530 --> 00:04:40,640
that has basically a runtime bundled

00:04:38,540 --> 00:04:42,800
into it that can run optimally on that

00:04:40,640 --> 00:04:44,660
hardware so now from the first case

00:04:42,800 --> 00:04:48,230
where I have to write two different SDKs

00:04:44,660 --> 00:04:50,420
I can now write to one SDK or one API

00:04:48,230 --> 00:04:52,880
and then it can still run optimally on

00:04:50,420 --> 00:04:54,770
each device the challenge is that that

00:04:52,880 --> 00:04:56,060
API is only available probably in less

00:04:54,770 --> 00:04:58,040
than 10% of phones that are out there

00:04:56,060 --> 00:04:59,900
and so as you still need two approaches

00:04:58,040 --> 00:05:03,110
that will work and scale across the

00:04:59,900 --> 00:05:05,180
phones that are available today the

00:05:03,110 --> 00:05:07,850
third case would be using a compiled

00:05:05,180 --> 00:05:10,010
model so when you use a compiled model

00:05:07,850 --> 00:05:11,570
you have the opportunity to take

00:05:10,010 --> 00:05:13,160
advantage of different opposite

00:05:11,570 --> 00:05:15,830
optimization opportunities that aren't

00:05:13,160 --> 00:05:18,890
available inside of runtimes and so you

00:05:15,830 --> 00:05:20,870
would generate a binary that is

00:05:18,890 --> 00:05:22,370
knowledgeable about what's the specific

00:05:20,870 --> 00:05:23,540
model and what are the parameters of the

00:05:22,370 --> 00:05:25,580
model and what are the constraints of

00:05:23,540 --> 00:05:27,350
the model and what is my constraints of

00:05:25,580 --> 00:05:29,170
the device and do I have accelerated

00:05:27,350 --> 00:05:34,420
memory that I could use

00:05:29,170 --> 00:05:37,610
do I have constraints for memory size

00:05:34,420 --> 00:05:40,910
what's the the speed at which I run what

00:05:37,610 --> 00:05:42,770
are the different hardware blocks that

00:05:40,910 --> 00:05:44,450
are on that specific device and then

00:05:42,770 --> 00:05:50,330
generate apps that can run for that

00:05:44,450 --> 00:05:52,400
specific device so I've talked about run

00:05:50,330 --> 00:05:53,690
times and I don't know how many people

00:05:52,400 --> 00:05:55,400
are familiar with what a runtime really

00:05:53,690 --> 00:05:58,210
is but I'll go into a few examples and

00:05:55,400 --> 00:06:00,790
show it in a little more detail

00:05:58,210 --> 00:06:03,020
basically what you have is you have a

00:06:00,790 --> 00:06:04,640
model that's been created by one of the

00:06:03,020 --> 00:06:06,950
different frameworks like tensorflow or

00:06:04,640 --> 00:06:10,100
say PI torch and then converted to onyx

00:06:06,950 --> 00:06:13,520
and you have a frozen model that then is

00:06:10,100 --> 00:06:15,860
incorporated into a runtime usually by a

00:06:13,520 --> 00:06:17,720
converter so in the case of the Qualcomm

00:06:15,860 --> 00:06:19,700
mineral processing SDK we would take

00:06:17,720 --> 00:06:21,920
converters that could take a tensor flow

00:06:19,700 --> 00:06:23,720
model or an Onix model and convert it

00:06:21,920 --> 00:06:25,850
into what we call a DLC which is a deep

00:06:23,720 --> 00:06:28,160
learning container and then you would

00:06:25,850 --> 00:06:30,830
write your app and bundle the runtime

00:06:28,160 --> 00:06:33,680
from the SDK inside your app and that

00:06:30,830 --> 00:06:37,220
can load the DLC and then dispatch the

00:06:33,680 --> 00:06:40,430
model across the CPU the GPU or the DSP

00:06:37,220 --> 00:06:41,420
using various acceleration methods to

00:06:40,430 --> 00:06:44,660
run

00:06:41,420 --> 00:06:49,150
using say neon for the CPU OpenCL for

00:06:44,660 --> 00:06:51,140
the GPU or HVX acceleration on the DSP

00:06:49,150 --> 00:06:53,900
alternatively you could write an

00:06:51,140 --> 00:06:56,000
application that uses TF light so you

00:06:53,900 --> 00:06:57,650
take a tensor flow model you convert it

00:06:56,000 --> 00:06:59,630
using the tensor flow light converter it

00:06:57,650 --> 00:07:01,220
would generate a dot TF light file and

00:06:59,630 --> 00:07:02,870
then you would create your app and

00:07:01,220 --> 00:07:05,600
bundle the TF library to have light

00:07:02,870 --> 00:07:06,740
libraries with it but TF light is trying

00:07:05,600 --> 00:07:09,320
to do something different it's not

00:07:06,740 --> 00:07:10,760
targeting a specific device and a

00:07:09,320 --> 00:07:12,980
specific hardware it's trying to scale

00:07:10,760 --> 00:07:15,020
across a range of devices so it uses

00:07:12,980 --> 00:07:19,370
generic interfaces it uses the generic

00:07:15,020 --> 00:07:22,820
CPU interfaces for arm to target all arm

00:07:19,370 --> 00:07:24,650
devices it uses OpenGL instead of OpenCL

00:07:22,820 --> 00:07:26,270
because in the case of Android Open CL

00:07:24,650 --> 00:07:28,850
is not an interface that's guaranteed to

00:07:26,270 --> 00:07:31,310
be present and so that can work across a

00:07:28,850 --> 00:07:33,380
variety of different GPUs and then they

00:07:31,310 --> 00:07:37,220
also support an an API which I mentioned

00:07:33,380 --> 00:07:38,570
earlier so in the case of n an API if

00:07:37,220 --> 00:07:42,050
you see that box at the bottom right

00:07:38,570 --> 00:07:44,450
over here and you look at the next slide

00:07:42,050 --> 00:07:47,690
so if I have that and I create my

00:07:44,450 --> 00:07:50,390
application I talked about earlier how

00:07:47,690 --> 00:07:52,610
when you use that an API it will run on

00:07:50,390 --> 00:07:53,930
the runtime so in the case of a qualcomm

00:07:52,610 --> 00:07:55,850
device it would reuse those same

00:07:53,930 --> 00:07:58,340
components that are in the qualcomm

00:07:55,850 --> 00:08:00,470
neural processing sdk and use them in

00:07:58,340 --> 00:08:05,660
the context of running on device when

00:08:00,470 --> 00:08:09,080
you're using android so another example

00:08:05,660 --> 00:08:11,810
of a runtime is arm and in an arm an n

00:08:09,080 --> 00:08:14,510
can directly read in fact certain model

00:08:11,810 --> 00:08:17,560
formats and then you would bundle the

00:08:14,510 --> 00:08:21,140
arm in n libraries with your app and

00:08:17,560 --> 00:08:27,740
they can target CPU across a range of

00:08:21,140 --> 00:08:30,140
different classes of arm SOC s it can

00:08:27,740 --> 00:08:32,360
target the Mallee GPU specifically in an

00:08:30,140 --> 00:08:34,670
optimal way but it also provides a

00:08:32,360 --> 00:08:37,100
generic plug-in framework so that if I

00:08:34,670 --> 00:08:39,260
wanted to port for instance the hexagon

00:08:37,100 --> 00:08:41,240
DSP back-end I could incorporate a

00:08:39,260 --> 00:08:44,180
runtime that could support that back-end

00:08:41,240 --> 00:08:46,040
or some other TPU or something else so

00:08:44,180 --> 00:08:51,920
it's a generic framework that can

00:08:46,040 --> 00:08:53,030
support a variety of hardware so you

00:08:51,920 --> 00:08:54,590
would think well there's all these

00:08:53,030 --> 00:08:56,930
different runtimes but the CP

00:08:54,590 --> 00:08:58,220
the CPU and clearly no one would reapply

00:08:56,930 --> 00:08:59,510
meant the same thing over and over again

00:08:58,220 --> 00:09:02,210
but you would be wrong

00:08:59,510 --> 00:09:04,730
and in fact all of the different

00:09:02,210 --> 00:09:08,000
runtimes generally do seem to redo these

00:09:04,730 --> 00:09:09,740
same things over and over and so they in

00:09:08,000 --> 00:09:11,630
the case of Arman n it's using the arm

00:09:09,740 --> 00:09:14,480
compute library to implement the CPU ops

00:09:11,630 --> 00:09:16,550
in the Qualcomm neural processing SDK we

00:09:14,480 --> 00:09:18,710
have our own internal math libraries

00:09:16,550 --> 00:09:19,850
that we use for acceleration for tensor

00:09:18,710 --> 00:09:22,220
flow light they've written their own

00:09:19,850 --> 00:09:24,020
custom kernels and for the Android and

00:09:22,220 --> 00:09:29,630
inhale they have again Rutan written

00:09:24,020 --> 00:09:31,700
different custom kernels okay so that's

00:09:29,630 --> 00:09:33,800
runtimes so what are the other

00:09:31,700 --> 00:09:36,770
approaches so graph compilers try to

00:09:33,800 --> 00:09:39,050
solve the problem of addressing the

00:09:36,770 --> 00:09:41,150
limitations of a runtime so a runtime

00:09:39,050 --> 00:09:43,220
has to support all models not knowing

00:09:41,150 --> 00:09:45,200
with the model is a priori it has to

00:09:43,220 --> 00:09:47,270
support a range of hardware devices so

00:09:45,200 --> 00:09:51,950
you don't have to have a runtime per

00:09:47,270 --> 00:09:53,750
version of the SOC for instance and and

00:09:51,950 --> 00:09:55,370
this is to step outside of that and

00:09:53,750 --> 00:09:57,020
think of how can they generate the most

00:09:55,370 --> 00:10:00,620
optimal binary that can run on a

00:09:57,020 --> 00:10:03,290
specific device so there's a bunch of

00:10:00,620 --> 00:10:05,240
different graph compilers onyx runtime

00:10:03,290 --> 00:10:06,890
is a runtime but in general it has a

00:10:05,240 --> 00:10:11,360
bunch of backends which in fact do

00:10:06,890 --> 00:10:13,850
compilation TVM is a project that was

00:10:11,360 --> 00:10:17,030
created a if you look at MX net MX net

00:10:13,850 --> 00:10:20,030
has a bunch of components for n n VM a

00:10:17,030 --> 00:10:21,710
relay t vm is a core part of that but t

00:10:20,030 --> 00:10:27,560
vm is also used in other projects as

00:10:21,710 --> 00:10:28,910
well n graph is from Intel and it I'll

00:10:27,560 --> 00:10:31,040
talk about it a little later and

00:10:28,910 --> 00:10:33,590
describe what its purposes it's again a

00:10:31,040 --> 00:10:36,730
little different than the others onic is

00:10:33,590 --> 00:10:40,100
a heterogeneous compiler that tries to

00:10:36,730 --> 00:10:40,730
optimize across a variety of different

00:10:40,100 --> 00:10:42,590
backends

00:10:40,730 --> 00:10:47,090
typically a lot of the compilers try to

00:10:42,590 --> 00:10:49,580
optimize for a specific hardware block

00:10:47,090 --> 00:10:52,250
but onic is tries to have metrics about

00:10:49,580 --> 00:10:55,370
what it would what could be optimum how

00:10:52,250 --> 00:10:57,500
do you optimize the entire system so

00:10:55,370 --> 00:11:01,040
that you're dispatching things across

00:10:57,500 --> 00:11:04,790
the different backends to optimally run

00:11:01,040 --> 00:11:06,380
that particular network xla is from

00:11:04,790 --> 00:11:07,680
google part of the tensor flow

00:11:06,380 --> 00:11:10,160
acceleration

00:11:07,680 --> 00:11:14,310
and glow is a project out of Facebook

00:11:10,160 --> 00:11:19,320
that is targeting again it can pilot

00:11:14,310 --> 00:11:21,360
framework using TVM so are using LVM in

00:11:19,320 --> 00:11:22,649
that case so what does it do what are

00:11:21,360 --> 00:11:25,230
these things doing so typically they

00:11:22,649 --> 00:11:27,110
take a deep learning framework and

00:11:25,230 --> 00:11:30,050
they'll run it through the compiler

00:11:27,110 --> 00:11:32,520
framework so the import the graph level

00:11:30,050 --> 00:11:34,620
information and then they're going to

00:11:32,520 --> 00:11:36,209
generate a series of binaries they're

00:11:34,620 --> 00:11:37,649
gonna break that graph up into pieces

00:11:36,209 --> 00:11:39,750
that can run on each of the different

00:11:37,649 --> 00:11:42,149
parts of the hardware and then you'll

00:11:39,750 --> 00:11:43,680
have save the CPU part that's generated

00:11:42,149 --> 00:11:45,450
it be the scheduler that can schedule

00:11:43,680 --> 00:11:47,399
and coordinate the dispatch of those

00:11:45,450 --> 00:11:49,610
things across the different pieces of

00:11:47,399 --> 00:11:49,610
hardware

00:11:49,790 --> 00:11:56,520
so I mentioned n graph was different so

00:11:52,770 --> 00:11:59,190
what n graph is is a it's kind of this

00:11:56,520 --> 00:12:00,510
connector to all things it wants to take

00:11:59,190 --> 00:12:02,670
all of these different deep learning

00:12:00,510 --> 00:12:04,380
frameworks and it has bridges to import

00:12:02,670 --> 00:12:06,839
the graph information that the graph

00:12:04,380 --> 00:12:09,110
level IR and then it has a plug-in

00:12:06,839 --> 00:12:11,970
architecture where you can plug in to

00:12:09,110 --> 00:12:13,200
what are called compiler backends and so

00:12:11,970 --> 00:12:15,180
you can have a compiler backend that

00:12:13,200 --> 00:12:18,390
generates something for the CPU and or

00:12:15,180 --> 00:12:20,940
generates something for the GPU or other

00:12:18,390 --> 00:12:23,339
different frameworks and so your plug-in

00:12:20,940 --> 00:12:24,839
can actually be another generic thing

00:12:23,339 --> 00:12:26,820
that can support other things and you

00:12:24,839 --> 00:12:29,040
end up with this layer cake of different

00:12:26,820 --> 00:12:31,770
pieces that form this chain of things to

00:12:29,040 --> 00:12:36,900
be able to finally generate optimal

00:12:31,770 --> 00:12:39,120
binary for your particular hardware this

00:12:36,900 --> 00:12:41,850
is a predominantly for Intel

00:12:39,120 --> 00:12:43,770
architecture and the CPU for instance

00:12:41,850 --> 00:12:49,560
only supports Intel in this particular

00:12:43,770 --> 00:12:51,300
case but what's common about all of

00:12:49,560 --> 00:12:53,490
these things even the run times really

00:12:51,300 --> 00:12:55,550
is that they have these four different

00:12:53,490 --> 00:12:58,380
levels of intermediate representations

00:12:55,550 --> 00:13:01,470
so you take the graph and you import the

00:12:58,380 --> 00:13:03,329
graph at the high level and at that high

00:13:01,470 --> 00:13:04,860
level what you're gonna do is partition

00:13:03,329 --> 00:13:06,649
the graph into the pieces that can run

00:13:04,860 --> 00:13:09,570
on the different parts of hardware

00:13:06,649 --> 00:13:12,329
you're going to try to optimize buffer

00:13:09,570 --> 00:13:14,700
reuse and things like that there's also

00:13:12,329 --> 00:13:17,690
different memory layouts sometimes you

00:13:14,700 --> 00:13:20,390
can have like a batch order and then

00:13:17,690 --> 00:13:22,100
the the height and then the width and

00:13:20,390 --> 00:13:23,750
the channel depth and that would be the

00:13:22,100 --> 00:13:25,130
format of the inputs you could have

00:13:23,750 --> 00:13:27,470
another one where you have the batch

00:13:25,130 --> 00:13:29,150
input and then the channels and then the

00:13:27,470 --> 00:13:32,170
height and the width and different

00:13:29,150 --> 00:13:35,120
frameworks use different layout formats

00:13:32,170 --> 00:13:38,210
you then go down to the operator level

00:13:35,120 --> 00:13:41,240
ir and now you can do optimizations that

00:13:38,210 --> 00:13:42,740
are specific to the hardware you want to

00:13:41,240 --> 00:13:44,660
do something which could use specific

00:13:42,740 --> 00:13:46,100
memory layout maybe you have fast memory

00:13:44,660 --> 00:13:48,200
available for that particular part of

00:13:46,100 --> 00:13:49,610
the hardware or you have something that

00:13:48,200 --> 00:13:53,960
can support different parallelization

00:13:49,610 --> 00:13:56,300
patterns and loop unrolling you then go

00:13:53,960 --> 00:13:58,670
down into the shader level or the ast

00:13:56,300 --> 00:14:00,050
level and there's this debate I guess

00:13:58,670 --> 00:14:01,070
we've had talking to other people who

00:14:00,050 --> 00:14:03,350
have given me some feedback of whether

00:14:01,070 --> 00:14:06,830
this is like a language specific thing

00:14:03,350 --> 00:14:08,420
like a C++ layer or a swift layer or

00:14:06,830 --> 00:14:12,770
something else but I think it's really

00:14:08,420 --> 00:14:14,990
just if you look at LVM it fits it's

00:14:12,770 --> 00:14:17,780
used a lot in these cases and that the

00:14:14,990 --> 00:14:20,390
sort of bit code layer is is where I

00:14:17,780 --> 00:14:23,110
would draw this there's a the glow

00:14:20,390 --> 00:14:27,410
project for instance has a way to

00:14:23,110 --> 00:14:29,120
generate the ast of the different

00:14:27,410 --> 00:14:31,730
kernels that are being run and inject

00:14:29,120 --> 00:14:34,310
precompiled bit code for a particular

00:14:31,730 --> 00:14:37,760
kernel into that stream and then combine

00:14:34,310 --> 00:14:39,410
them together then it's lowered again

00:14:37,760 --> 00:14:41,510
into an assembly level ir that would

00:14:39,410 --> 00:14:47,150
target a specific eisah like a hexagon

00:14:41,510 --> 00:14:48,980
ice ax or arm CPU so if I map the

00:14:47,150 --> 00:14:51,860
different projects into these four

00:14:48,980 --> 00:14:55,790
levels you'll see sort of where they

00:14:51,860 --> 00:14:57,470
fall roughly so if you look at an nvm or

00:14:55,790 --> 00:15:01,910
the next generation of that called relay

00:14:57,470 --> 00:15:03,350
that's part of the TVM project it's one

00:15:01,910 --> 00:15:05,240
of those things that imports a graph and

00:15:03,350 --> 00:15:08,000
handles manipulation of the graph at a

00:15:05,240 --> 00:15:11,360
high level an graph also takes in things

00:15:08,000 --> 00:15:13,340
at the top level graph Excel a has a

00:15:11,360 --> 00:15:16,100
front-end part that does that high level

00:15:13,340 --> 00:15:18,470
IR as does glow but then at the next

00:15:16,100 --> 00:15:20,180
level down you have TVM that starts to

00:15:18,470 --> 00:15:22,000
manage the operator levels and so

00:15:20,180 --> 00:15:25,400
there's an operator level IR interface

00:15:22,000 --> 00:15:28,970
platt ml has a similar interface cout

00:15:25,400 --> 00:15:30,930
DNN would be an equivalent for for the

00:15:28,970 --> 00:15:32,610
Nvidia stack

00:15:30,930 --> 00:15:35,130
but there's also a back-end for excel a

00:15:32,610 --> 00:15:37,500
that's an operator back-end agloe ir

00:15:35,130 --> 00:15:41,310
back-end and then many of these

00:15:37,500 --> 00:15:43,560
different frameworks then go to LVM to

00:15:41,310 --> 00:15:46,470
be able to do the cogeneration and that

00:15:43,560 --> 00:15:49,050
can generate OpenCL code there's code

00:15:46,470 --> 00:15:51,810
that can be created as well for again

00:15:49,050 --> 00:15:53,640
invidious tax and then that will be

00:15:51,810 --> 00:15:55,829
lowered again to the specific assembly

00:15:53,640 --> 00:15:56,339
for that the particular hardware being

00:15:55,829 --> 00:15:58,920
targeted

00:15:56,339 --> 00:16:05,760
whether it's arm or say hexagon or RP

00:15:58,920 --> 00:16:07,860
tx4 for GPU so now we have all these

00:16:05,760 --> 00:16:10,860
compiler frameworks but most of the SOC

00:16:07,860 --> 00:16:12,300
providers have run times and how can we

00:16:10,860 --> 00:16:15,750
leverage these things that we already

00:16:12,300 --> 00:16:18,720
have and try to help scale these

00:16:15,750 --> 00:16:20,600
compiler frameworks so the the problem

00:16:18,720 --> 00:16:23,040
for the compiler frameworks is they've

00:16:20,600 --> 00:16:24,870
they've been targeting a fairly narrow

00:16:23,040 --> 00:16:27,209
range of hardware but if they want to

00:16:24,870 --> 00:16:29,730
now go and deploy at the edge you've got

00:16:27,209 --> 00:16:32,490
this massive set of combinations that

00:16:29,730 --> 00:16:33,930
you have to support so for instance

00:16:32,490 --> 00:16:35,970
you've now got all these different

00:16:33,930 --> 00:16:39,510
frameworks that you can create models

00:16:35,970 --> 00:16:41,760
from either as tensorflow or cafe or MX

00:16:39,510 --> 00:16:44,010
net or pi torch and then you've got

00:16:41,760 --> 00:16:48,660
different runtimes or compilers that you

00:16:44,010 --> 00:16:50,640
could use to optimize the graph for for

00:16:48,660 --> 00:16:52,500
your particular application but then

00:16:50,640 --> 00:16:54,870
what what OS is it going to run on and

00:16:52,500 --> 00:16:56,640
then what specific hardware do I want to

00:16:54,870 --> 00:16:58,769
run on if I'm trying to optimize a bunch

00:16:56,640 --> 00:17:00,779
of networks to run on an SOC and target

00:16:58,769 --> 00:17:03,180
particular parts of the hardware for

00:17:00,779 --> 00:17:04,470
that particular application I got to

00:17:03,180 --> 00:17:06,660
manage and lay out all of these

00:17:04,470 --> 00:17:08,250
different pieces in it so if I'm an app

00:17:06,660 --> 00:17:10,140
developer I've got to say is there a

00:17:08,250 --> 00:17:12,839
flow that gets me where I want to go or

00:17:10,140 --> 00:17:14,610
if I'm an SOC provider it's how many of

00:17:12,839 --> 00:17:16,559
these frameworks can I support and how

00:17:14,610 --> 00:17:18,299
many runtimes or compilers can I support

00:17:16,559 --> 00:17:20,010
on how many different OSS

00:17:18,299 --> 00:17:22,290
across the different hardware that I

00:17:20,010 --> 00:17:24,750
have across all variants of the SOC S

00:17:22,290 --> 00:17:27,140
and it's enough to make your mind want

00:17:24,750 --> 00:17:27,140
to explode

00:17:31,100 --> 00:17:36,809
so this hardware diversity is a

00:17:34,080 --> 00:17:39,000
scalability issue the graph compilers

00:17:36,809 --> 00:17:41,250
can create things that are optimal for

00:17:39,000 --> 00:17:42,630
all the different devices but the cost

00:17:41,250 --> 00:17:43,950
of it is the complexity and the

00:17:42,630 --> 00:17:45,269
management of all the different

00:17:43,950 --> 00:17:46,860
compilers that you need to know and

00:17:45,269 --> 00:17:48,269
having descriptions of all of the

00:17:46,860 --> 00:17:50,100
devices you're trying to target and

00:17:48,269 --> 00:17:53,549
knowing there were specific constraints

00:17:50,100 --> 00:17:55,590
ahead of time so is there a way then to

00:17:53,549 --> 00:17:56,970
use a runtime for instance to try to

00:17:55,590 --> 00:17:59,399
collapse some of those things and

00:17:56,970 --> 00:18:01,110
achieve more scale or to use arm compute

00:17:59,399 --> 00:18:04,799
library for instance to scale across

00:18:01,110 --> 00:18:13,769
multiple ranges of devices and target

00:18:04,799 --> 00:18:15,539
more of these so after looking at all

00:18:13,769 --> 00:18:18,659
that things some of the conclusions I

00:18:15,539 --> 00:18:20,730
guess I came to I'll try summarize so

00:18:18,659 --> 00:18:23,490
generally we have framework models that

00:18:20,730 --> 00:18:26,809
can be converted to different containers

00:18:23,490 --> 00:18:30,539
or read directly into different runtimes

00:18:26,809 --> 00:18:34,649
or graph compiler and these all have

00:18:30,539 --> 00:18:36,990
some top-level ir that they ingest a a

00:18:34,649 --> 00:18:39,630
pre generated graph room that's been

00:18:36,990 --> 00:18:42,419
trained already but they also have these

00:18:39,630 --> 00:18:44,100
op level i ARS and so in the Qualcomm

00:18:42,419 --> 00:18:46,260
neural processing SDK there would be the

00:18:44,100 --> 00:18:48,720
hexagon and then back-end for instance

00:18:46,260 --> 00:18:51,080
for the DSP and an Armin and there's

00:18:48,720 --> 00:18:52,980
this whole back-end API that would allow

00:18:51,080 --> 00:18:55,559
multiple different vendors to be

00:18:52,980 --> 00:18:57,630
supported with a common API and in

00:18:55,559 --> 00:18:59,429
another runtime called T engine they

00:18:57,630 --> 00:19:02,309
have some component called a tianjin

00:18:59,429 --> 00:19:04,380
executor that is basically logically

00:19:02,309 --> 00:19:05,519
equivalent but not really separable so

00:19:04,380 --> 00:19:07,860
it's not like you can take this thing

00:19:05,519 --> 00:19:13,039
and really carve it out of the runtime

00:19:07,860 --> 00:19:15,809
it's an integral piece of it inside and

00:19:13,039 --> 00:19:19,350
so I guess what people have been looking

00:19:15,809 --> 00:19:21,059
at is we have something like MX net that

00:19:19,350 --> 00:19:23,549
goes and it reads the graph into this

00:19:21,059 --> 00:19:26,789
optimization Park I'll relay and then it

00:19:23,549 --> 00:19:29,309
lowers that into an op level I are at T

00:19:26,789 --> 00:19:34,519
VM and then that generates things for

00:19:29,309 --> 00:19:37,620
CPU for instance using LVM or could use

00:19:34,519 --> 00:19:43,110
something like a CL and inject that at

00:19:37,620 --> 00:19:45,410
the op layer and and generate GPU or CPU

00:19:43,110 --> 00:19:47,250
implementations of kernels at that level

00:19:45,410 --> 00:19:48,780
but other people have said well why

00:19:47,250 --> 00:19:52,110
can't we just plug the whole runtime in

00:19:48,780 --> 00:19:55,260
right there at this op level but the the

00:19:52,110 --> 00:19:57,450
problem is that a runtime takes things

00:19:55,260 --> 00:19:59,640
at a graph level and not an op level and

00:19:57,450 --> 00:20:01,710
so you have this mismatch of AP is a

00:19:59,640 --> 00:20:06,300
runtime isn't really meant to take

00:20:01,710 --> 00:20:08,010
things in at an OP level and so if

00:20:06,300 --> 00:20:09,720
you're gonna do that and you think of

00:20:08,010 --> 00:20:11,730
what you end up with at the end I'm

00:20:09,720 --> 00:20:12,990
gonna take em x net and I'm gonna run it

00:20:11,730 --> 00:20:14,760
through relay and then we're gonna run

00:20:12,990 --> 00:20:16,530
it through TVM and then I'm gonna plug

00:20:14,760 --> 00:20:18,720
in somehow to get back to the graph

00:20:16,530 --> 00:20:20,790
level ir and then I'm gonna generate

00:20:18,720 --> 00:20:24,180
code then I'm gonna generate a model

00:20:20,790 --> 00:20:26,970
that runs across the predefined CPU and

00:20:24,180 --> 00:20:29,160
GPU kernels why wouldn't I just use the

00:20:26,970 --> 00:20:31,470
convertor that I have that can convert

00:20:29,160 --> 00:20:33,330
an Onix model MX net to an Onix model

00:20:31,470 --> 00:20:34,860
then run it directly on the runtime and

00:20:33,330 --> 00:20:43,770
it seems like a much simpler path and I

00:20:34,860 --> 00:20:45,510
end up at the same place and this wasn't

00:20:43,770 --> 00:20:48,360
the initial focus of what I was looking

00:20:45,510 --> 00:20:50,550
at but if I think about inferencing at

00:20:48,360 --> 00:20:52,770
the edge for servers for arm servers I

00:20:50,550 --> 00:20:54,540
think one of the things to potentially

00:20:52,770 --> 00:20:56,550
consider is there's all this work that's

00:20:54,540 --> 00:20:57,750
been done inside of n graph to bridge

00:20:56,550 --> 00:21:00,210
all of these different frameworks

00:20:57,750 --> 00:21:02,280
already and that's the sort of value

00:21:00,210 --> 00:21:03,720
proposition of n graph is it saying you

00:21:02,280 --> 00:21:05,400
don't have to create all these

00:21:03,720 --> 00:21:07,320
converters the converters already exist

00:21:05,400 --> 00:21:09,810
all you have to do is implement these

00:21:07,320 --> 00:21:14,010
backend compilers so you could implement

00:21:09,810 --> 00:21:16,140
a CPU back-end that targets arm CPUs or

00:21:14,010 --> 00:21:17,880
an even potentially like a Mallee GPU or

00:21:16,140 --> 00:21:19,620
you could have an arm server with a

00:21:17,880 --> 00:21:21,210
custom accelerator and again you could

00:21:19,620 --> 00:21:23,030
just add this into the framework and

00:21:21,210 --> 00:21:26,840
you'd be able to benefit from all of the

00:21:23,030 --> 00:21:26,840
converters that exist in the top

00:21:32,940 --> 00:21:37,889
all right so what are some opportunities

00:21:35,909 --> 00:21:41,970
for addressing this graph level compiler

00:21:37,889 --> 00:21:43,710
rir fragmentation today each of the

00:21:41,970 --> 00:21:44,759
different compilers makes different

00:21:43,710 --> 00:21:46,979
trade-offs and they have different

00:21:44,759 --> 00:21:49,499
benefits but eventually hopefully

00:21:46,979 --> 00:21:51,509
there'll be some way to to grab

00:21:49,499 --> 00:21:53,399
components out of those and share them

00:21:51,509 --> 00:21:55,349
more collaboratively and today they do

00:21:53,399 --> 00:21:58,499
some of that already LLVM is used across

00:21:55,349 --> 00:22:01,259
many TVM used across many but hopefully

00:21:58,499 --> 00:22:03,539
there's things like code generation for

00:22:01,259 --> 00:22:06,210
open CLR stuff they can be more

00:22:03,539 --> 00:22:07,289
modularized and shared but there's

00:22:06,210 --> 00:22:08,789
really there's too many frameworks

00:22:07,289 --> 00:22:12,570
there's too many compilers

00:22:08,789 --> 00:22:14,159
there's too many formats for for an SOC

00:22:12,570 --> 00:22:17,519
vendor to try to address all of these

00:22:14,159 --> 00:22:19,649
for instance and for a a graph compiler

00:22:17,519 --> 00:22:22,259
creator there's too much disparate

00:22:19,649 --> 00:22:25,830
Hardware to try to support it all inside

00:22:22,259 --> 00:22:28,289
the framework so if I was looking how do

00:22:25,830 --> 00:22:31,409
I get the most scale to support this

00:22:28,289 --> 00:22:34,259
most of the frameworks can be imported

00:22:31,409 --> 00:22:36,379
through an onyx conversion tensorflow of

00:22:34,259 --> 00:22:40,109
being the exception to that and so

00:22:36,379 --> 00:22:42,479
potential has a very very large market

00:22:40,109 --> 00:22:43,440
share in the space as well so if I was

00:22:42,479 --> 00:22:46,739
to target tensorflow

00:22:43,440 --> 00:22:49,320
and onyx as formats I think that will

00:22:46,739 --> 00:22:52,529
give me the broadest scale for for edge

00:22:49,320 --> 00:22:55,259
inferencing but another opportunity

00:22:52,529 --> 00:22:57,359
would be if these compiler graph

00:22:55,259 --> 00:22:59,519
compiler frameworks supported a common

00:22:57,359 --> 00:23:02,210
back-end API something like arm and n

00:22:59,519 --> 00:23:05,909
that allows various vendors to plug in -

00:23:02,210 --> 00:23:08,940
then you'd have a scalable way to inject

00:23:05,909 --> 00:23:10,950
a runtime into these IRS and something

00:23:08,940 --> 00:23:16,289
like the TVM level they could then give

00:23:10,950 --> 00:23:19,440
you more scale and how could we address

00:23:16,289 --> 00:23:22,349
the cpu fragmentation so if I look at a

00:23:19,440 --> 00:23:25,080
CL if a CL became a Best of Breed

00:23:22,349 --> 00:23:27,929
implementation for CPU runtime because

00:23:25,080 --> 00:23:30,239
no one's winning any benchmarks on CPU

00:23:27,929 --> 00:23:32,249
inferencing it's it's usually done by

00:23:30,239 --> 00:23:33,659
hardware acceleration but the CPU is

00:23:32,249 --> 00:23:35,460
done for coordination is done for

00:23:33,659 --> 00:23:40,139
fallback and on certain devices like MC

00:23:35,460 --> 00:23:42,029
use it's done and CPU as well but having

00:23:40,139 --> 00:23:44,190
these things replicated in many places

00:23:42,029 --> 00:23:46,410
makes no sense and so if we could

00:23:44,190 --> 00:23:48,720
consolidate the TF light CPU runs

00:23:46,410 --> 00:23:50,910
and the Android and NCP you run time and

00:23:48,720 --> 00:23:52,530
the Armin and CPU back-end then that

00:23:50,910 --> 00:23:55,290
would create common code bases that we

00:23:52,530 --> 00:23:56,220
get all work on and that paves the way

00:23:55,290 --> 00:23:58,710
for others to follow

00:23:56,220 --> 00:24:01,170
like Tianjin or mace or Qualcomm neural

00:23:58,710 --> 00:24:04,650
processing SDK and utilize again those

00:24:01,170 --> 00:24:06,420
same implementations so I think that

00:24:04,650 --> 00:24:08,460
there are opportunities for

00:24:06,420 --> 00:24:10,470
collaboration and I'm hopeful in this

00:24:08,460 --> 00:24:11,970
space that we'll see more of that happen

00:24:10,470 --> 00:24:16,350
in the future and some of this

00:24:11,970 --> 00:24:17,730
fragmentation will go away so that's the

00:24:16,350 --> 00:24:21,500
end of my talk thank you all for showing

00:24:17,730 --> 00:24:21,500
up I appreciate your coming this road

00:24:23,360 --> 00:24:35,010
has any questions oh sorry one sec

00:24:33,780 --> 00:24:37,760
there's a microphone I'll give you the

00:24:35,010 --> 00:24:37,760
mic hang on semi

00:24:42,110 --> 00:24:49,770
so whether I gotta choose a full level I

00:24:46,200 --> 00:24:52,920
as you mentioned so if I remember

00:24:49,770 --> 00:24:56,760
correctly there was one proposal from

00:24:52,920 --> 00:24:58,320
Google called multi-level I uh called

00:24:56,760 --> 00:25:00,690
sorry I'll go back to that slide there

00:24:58,320 --> 00:25:03,540
yeah yeah multi-level I our

00:25:00,690 --> 00:25:05,130
infrastructure Google proposing some our

00:25:03,540 --> 00:25:08,610
mantra I'm not catching the first part

00:25:05,130 --> 00:25:11,400
of what level I are multi-level mal Iowa

00:25:08,610 --> 00:25:13,610
okay yeah I'm a lie our infrastructure

00:25:11,400 --> 00:25:15,450
I'm not sure if you have chance to

00:25:13,610 --> 00:25:16,950
that's the first time I've heard of it

00:25:15,450 --> 00:25:20,090
so I'll try to find out and look at it

00:25:16,950 --> 00:25:24,120
okay yeah I think that's probably just

00:25:20,090 --> 00:25:26,490
and good potentially a good proposal to

00:25:24,120 --> 00:25:32,940
address the fragmentation in the

00:25:26,490 --> 00:25:35,760
different compiler frameworks the whole

00:25:32,940 --> 00:25:37,710
stack hosts at multiple levels just as

00:25:35,760 --> 00:25:40,200
you stumble over here for the four

00:25:37,710 --> 00:25:42,630
levels of I ah I'm not quite sure about

00:25:40,200 --> 00:25:45,240
how Google handles this part but their

00:25:42,630 --> 00:25:48,360
purpose is to address the fragmentation

00:25:45,240 --> 00:25:50,070
is the area I think so yeah if we have

00:25:48,360 --> 00:25:54,530
chance to talk with Google about a part

00:25:50,070 --> 00:25:54,530
I think that will be helpful yeah

00:25:55,340 --> 00:26:08,880
okay so it's a graph level IR so you

00:26:04,710 --> 00:26:25,460
mean only there are no definition for

00:26:08,880 --> 00:26:32,990
the low level IRA sure sure thank you

00:26:25,460 --> 00:26:37,460
any other questions thank you very much

00:26:32,990 --> 00:26:37,460

YouTube URL: https://www.youtube.com/watch?v=qFIRDsSNQH4


