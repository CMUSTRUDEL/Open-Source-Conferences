Title: BKK19-121 - XDP for TSN
Publication date: 2019-04-11
Playlist: Linaro Connect Bangkok 2019
Description: 
	Session Resources

 
Abstract
ne of the challenging tasks of TSN (Time-Sensitive Networking) is itâ€™s low latency and bounded jitter strict requirements. Although XDP (eXpress Data Path) does not offer any jitter guarantees it offers significantly lower latency, by offloading traffic off the kernel and directly into user-space sockets (AF_XDP), compared to the linux kernel network stack. This talk is about a brief XDP introduction and the latency numbers we got on our initial tests.

Ilias Apalodimas / Tech Lead Linaro
Linux kernel developer with a taste for networking and performance

Ivan Khoronzhuk / software engineer Texas Instruments
Captions: 
	00:00:05,509 --> 00:00:09,970
hello everyone Amelia's with him and we

00:00:08,750 --> 00:00:13,090
have we've done some

00:00:09,970 --> 00:00:15,940
certain TSN and how we can use XDP to

00:00:13,090 --> 00:00:19,240
and leverage ectopy for that

00:00:15,940 --> 00:00:24,189
so we'd sure know of you knows what HDPE

00:00:19,240 --> 00:00:27,160
is right we do have no one of them knows

00:00:24,189 --> 00:00:29,439
what TSN is TSN is time-sensitive

00:00:27,160 --> 00:00:30,790
networking I'll do okay so I'll do the

00:00:29,439 --> 00:00:32,500
introduction TSN is time-sensitive

00:00:30,790 --> 00:00:35,140
networking and it's supposed to be a

00:00:32,500 --> 00:00:37,780
framework that we can use for packets

00:00:35,140 --> 00:00:41,200
that should be delivered in time and not

00:00:37,780 --> 00:00:43,570
out of a specific time schedule HDPE is

00:00:41,200 --> 00:00:45,040
an express date upon the kernel so it's

00:00:43,570 --> 00:00:47,140
a building block that you can use and

00:00:45,040 --> 00:00:49,510
build a bunch of applications on it it's

00:00:47,140 --> 00:00:52,330
mostly opera it operates on layer 2 and

00:00:49,510 --> 00:00:56,530
layer 3 only and you need to think of it

00:00:52,330 --> 00:00:58,780
as a as a kernel bypass it's not a

00:00:56,530 --> 00:01:00,850
secure bypass but a fast path in kernel

00:00:58,780 --> 00:01:02,770
one of the features that it offers is a

00:01:00,850 --> 00:01:06,549
cattle bypass but it's a feature it's

00:01:02,770 --> 00:01:08,860
not the complete set of it it operates

00:01:06,549 --> 00:01:10,329
on a driver basis so you need to edit

00:01:08,860 --> 00:01:13,180
the driver and you need to have a

00:01:10,329 --> 00:01:15,970
working driver for xdp in order to get

00:01:13,180 --> 00:01:17,230
the functionality and there's various

00:01:15,970 --> 00:01:18,670
modes that you can edit and there's

00:01:17,230 --> 00:01:21,730
various functionalities you can get

00:01:18,670 --> 00:01:23,350
we'll cover them in a bit the good thing

00:01:21,730 --> 00:01:24,970
is that there's a native xdp

00:01:23,350 --> 00:01:27,490
implementation which doesn't offer the

00:01:24,970 --> 00:01:29,500
sprint is the speedy Pro print but it

00:01:27,490 --> 00:01:31,030
does work on every working existing

00:01:29,500 --> 00:01:32,530
driver on the Linux kernel so if you

00:01:31,030 --> 00:01:34,690
want to experiment with it if you want

00:01:32,530 --> 00:01:38,979
to play around with it it's usable on

00:01:34,690 --> 00:01:40,510
any interface the last thing that you

00:01:38,979 --> 00:01:42,670
really need to figure out is that thing

00:01:40,510 --> 00:01:45,100
XDP as a building block it doesn't offer

00:01:42,670 --> 00:01:46,510
anything as a specific as the technology

00:01:45,100 --> 00:01:51,640
on its own but it's a great building

00:01:46,510 --> 00:01:53,049
block for doing a bunch of stuff so when

00:01:51,640 --> 00:01:54,840
you start x depending where you start to

00:01:53,049 --> 00:01:58,479
change the driver you have these actions

00:01:54,840 --> 00:02:00,460
XDP pass means send the send the packet

00:01:58,479 --> 00:02:02,770
back to the Linux networking stack for

00:02:00,460 --> 00:02:06,549
whatever processing the network stack

00:02:02,770 --> 00:02:08,530
was gonna make anyway so how this how

00:02:06,549 --> 00:02:10,840
xdp works is that you load the BPF

00:02:08,530 --> 00:02:13,720
program a program up vpf on the driver

00:02:10,840 --> 00:02:15,819
and depending on the outcome of the BPF

00:02:13,720 --> 00:02:18,250
program you might either get an x DP

00:02:15,819 --> 00:02:20,319
pass which means send the packet into

00:02:18,250 --> 00:02:22,030
the to the kernel you might get an extra

00:02:20,319 --> 00:02:23,080
big drop which means drop all the

00:02:22,030 --> 00:02:26,170
packets that

00:02:23,080 --> 00:02:28,540
not the BPF program you might get an XDP

00:02:26,170 --> 00:02:29,980
redirect and the redirect means redirect

00:02:28,540 --> 00:02:31,450
the packet on a different cpu or

00:02:29,980 --> 00:02:34,660
anything that your user space

00:02:31,450 --> 00:02:37,270
application does your BPF application

00:02:34,660 --> 00:02:39,220
does and you have xdp teks now xdp tix

00:02:37,270 --> 00:02:41,320
is a bit weird but essentially you send

00:02:39,220 --> 00:02:43,390
out the packet through the interface it

00:02:41,320 --> 00:02:45,820
came from so you can for example sweets

00:02:43,390 --> 00:02:47,230
what sweets the mug headers since the

00:02:45,820 --> 00:02:48,820
source and destination other sends us

00:02:47,230 --> 00:02:52,090
send it immediately out of the interface

00:02:48,820 --> 00:02:55,990
without paying the price of processing

00:02:52,090 --> 00:02:57,940
the packet in the Linux kernel so this

00:02:55,990 --> 00:03:00,220
is pretty much an overview of how XDP

00:02:57,940 --> 00:03:02,260
works if you have to network interfaces

00:03:00,220 --> 00:03:05,080
and a packet comes in from network

00:03:02,260 --> 00:03:06,730
interface one and the action is XD PTX

00:03:05,080 --> 00:03:11,760
the packet will be chained and it will

00:03:06,730 --> 00:03:14,500
go out through NIC interface one again

00:03:11,760 --> 00:03:17,080
if the packet if the action is an x DP

00:03:14,500 --> 00:03:20,860
pass it will go to the Linux networking

00:03:17,080 --> 00:03:23,350
stuff if it's a redirect there's a

00:03:20,860 --> 00:03:25,180
callback that we use which is called n

00:03:23,350 --> 00:03:26,890
do XD px meat and you can send the

00:03:25,180 --> 00:03:29,320
packet to a different network interface

00:03:26,890 --> 00:03:30,940
within the same horse without processing

00:03:29,320 --> 00:03:33,310
it from the Linux networks like that

00:03:30,940 --> 00:03:35,200
that's what speeds up things that but

00:03:33,310 --> 00:03:37,239
you don't go through the network stack

00:03:35,200 --> 00:03:38,980
you don't pay a bunch of price for

00:03:37,239 --> 00:03:41,019
traffic shaping or what IP tables

00:03:38,980 --> 00:03:42,880
whatever Linux is doing you're just

00:03:41,019 --> 00:03:46,870
redirecting it through a second

00:03:42,880 --> 00:03:50,320
interface now on 4.18 we get a new

00:03:46,870 --> 00:03:54,700
single which was called a FX DP now the

00:03:50,320 --> 00:03:58,600
a FX DP is an offload to user space that

00:03:54,700 --> 00:04:00,670
you can use you have the network x DP

00:03:58,600 --> 00:04:03,070
which we mentioned that if the driver

00:04:00,670 --> 00:04:05,400
doesn't have a DX DP support you can use

00:04:03,070 --> 00:04:09,340
it and test every functionality you want

00:04:05,400 --> 00:04:11,590
the obvious problem is that you don't

00:04:09,340 --> 00:04:13,480
get any speed improvement that x DP

00:04:11,590 --> 00:04:16,720
offers because you're using that

00:04:13,480 --> 00:04:19,959
standard linux network stock to emulate

00:04:16,720 --> 00:04:23,530
what x DP would do now the next step is

00:04:19,959 --> 00:04:25,690
negative x DP negative x DP offers a

00:04:23,530 --> 00:04:27,070
really good performance on our weeks but

00:04:25,690 --> 00:04:31,390
it doesn't change anything on the

00:04:27,070 --> 00:04:34,210
Teague's and then we have zero copy the

00:04:31,390 --> 00:04:36,700
zero copy mode is zero copy mode on a FX

00:04:34,210 --> 00:04:37,030
DP is essentially where you allocate the

00:04:36,700 --> 00:04:38,890
buff

00:04:37,030 --> 00:04:41,350
for the network interface card from the

00:04:38,890 --> 00:04:44,410
user space so since you upload all the

00:04:41,350 --> 00:04:46,300
traffic to user space you can you can

00:04:44,410 --> 00:04:48,420
use those buffers for either Eric's or

00:04:46,300 --> 00:04:51,100
ticks you don't have to copy the buffers

00:04:48,420 --> 00:04:52,870
XDP works on both directions and that's

00:04:51,100 --> 00:04:56,380
what you get that's where you get all

00:04:52,870 --> 00:04:58,060
the speed improvements for the zero copy

00:04:56,380 --> 00:05:02,110
AF xdp mode is the mode that works

00:04:58,060 --> 00:05:04,570
really really fast so compared to the

00:05:02,110 --> 00:05:06,610
previous slide this is where xdp fits in

00:05:04,570 --> 00:05:08,290
so when you have a packet on that that's

00:05:06,610 --> 00:05:10,840
been for the user space you can just

00:05:08,290 --> 00:05:12,640
send it directly on a socket and bypass

00:05:10,840 --> 00:05:15,220
the whole kernel Linux kernel Network

00:05:12,640 --> 00:05:16,540
stack now the xtp killer feature because

00:05:15,220 --> 00:05:18,130
there's been a bunch of application

00:05:16,540 --> 00:05:19,840
doing that this there's been DP decade

00:05:18,130 --> 00:05:21,670
has been Mugler from Google there's been

00:05:19,840 --> 00:05:23,980
a bunch of proposals doing that there's

00:05:21,670 --> 00:05:26,380
open load from solar flare now the

00:05:23,980 --> 00:05:27,970
killer feature is that this is ebps

00:05:26,380 --> 00:05:29,800
programmable so you can choose with

00:05:27,970 --> 00:05:31,120
packets to forward to user space you can

00:05:29,800 --> 00:05:33,610
choose which packets you will need to

00:05:31,120 --> 00:05:36,640
drop or redirect to the user space and

00:05:33,610 --> 00:05:38,680
you don't need any proprietary SDK or in

00:05:36,640 --> 00:05:40,450
user space SDK you just get the kernel

00:05:38,680 --> 00:05:43,230
if if it's supported by your driver

00:05:40,450 --> 00:05:43,230
you're good to go

00:05:43,410 --> 00:05:49,930
now why did we even consider

00:05:46,240 --> 00:05:52,170
xdp for TSN now as we discussed TSM is a

00:05:49,930 --> 00:05:54,520
time sensitive networking that needs a

00:05:52,170 --> 00:05:56,350
bounded latency and a boundary jitter

00:05:54,520 --> 00:05:58,150
you need to be able to determine both of

00:05:56,350 --> 00:06:02,800
them in order to have a time sensitive

00:05:58,150 --> 00:06:05,320
networking xdp can't guarantee jitter

00:06:02,800 --> 00:06:08,350
the only thing you can get out of xdp is

00:06:05,320 --> 00:06:09,880
that is that you get really low latency

00:06:08,350 --> 00:06:14,650
compared to what the linux net the

00:06:09,880 --> 00:06:15,940
network stack does so on previous ivan

00:06:14,650 --> 00:06:18,100
measurements that we had on previous

00:06:15,940 --> 00:06:22,390
connects about speeds on XDP we were

00:06:18,100 --> 00:06:25,090
getting like 60 to 70 micro seconds from

00:06:22,390 --> 00:06:27,400
kernel space to user space as a packet

00:06:25,090 --> 00:06:31,260
processing and now we're getting below

00:06:27,400 --> 00:06:33,610
10 microseconds just by using X DP n

00:06:31,260 --> 00:06:36,030
from the drive yeah from the hardware to

00:06:33,610 --> 00:06:39,280
user space will you'll explain this part

00:06:36,030 --> 00:06:41,050
but the initial latency measurements

00:06:39,280 --> 00:06:43,090
that we see compared to what the Linux

00:06:41,050 --> 00:06:45,460
Network stack does are really improved

00:06:43,090 --> 00:06:48,760
and this is really what's driving us to

00:06:45,460 --> 00:06:50,249
move forward the thing is that you can't

00:06:48,760 --> 00:06:52,139
guarantee Jeeter but there's

00:06:50,249 --> 00:06:55,079
tricks you can do to guarantee theater

00:06:52,139 --> 00:06:57,449
for example you can sip pin bar ACP use

00:06:55,079 --> 00:06:59,009
you can use you can isolate CPUs from

00:06:57,449 --> 00:07:01,109
the Linux kernel and you can run your

00:06:59,009 --> 00:07:04,469
user space applications on the isolated

00:07:01,109 --> 00:07:06,299
CPUs so there's there's currently there

00:07:04,469 --> 00:07:09,989
are some tricks you can do to try and

00:07:06,299 --> 00:07:13,139
guarantee deter but the still needs work

00:07:09,989 --> 00:07:15,239
to be done over there now the good part

00:07:13,139 --> 00:07:17,189
with xdp we mentioned that is that since

00:07:15,239 --> 00:07:19,349
it relies on vpf to figure out which

00:07:17,189 --> 00:07:21,119
packets it should offload which packets

00:07:19,349 --> 00:07:24,869
it should send back to user space or

00:07:21,119 --> 00:07:26,399
back to the kernel or redirect since

00:07:24,869 --> 00:07:28,110
this is programmable you can control

00:07:26,399 --> 00:07:31,889
your traffic really well because you can

00:07:28,110 --> 00:07:35,159
have your really sensitive traffic going

00:07:31,889 --> 00:07:38,119
out of directly to the user space and

00:07:35,159 --> 00:07:40,409
you can have your non sensitive traffic

00:07:38,119 --> 00:07:42,089
thrown back to the kernel so the kernel

00:07:40,409 --> 00:07:44,269
will do the processing for you you will

00:07:42,089 --> 00:07:47,729
do whatever you have to do and you have

00:07:44,269 --> 00:07:51,299
all the all the sensitive traffic

00:07:47,729 --> 00:07:52,679
offloaded the next thing is that it's

00:07:51,299 --> 00:07:54,389
currently designed to operate as a

00:07:52,679 --> 00:07:57,509
socket so you all the things that you

00:07:54,389 --> 00:07:59,219
know on sockets you open a socket and on

00:07:57,509 --> 00:08:01,049
that socket you just Lee receive layer

00:07:59,219 --> 00:08:02,969
two frames that's how the functionality

00:08:01,049 --> 00:08:06,349
works at the moment and this is pretty

00:08:02,969 --> 00:08:06,349
much standardized it's not gonna change

00:08:06,829 --> 00:08:12,360
we've also done some tests with user

00:08:09,659 --> 00:08:15,419
space libraries that do tcp/ip like LW

00:08:12,360 --> 00:08:18,479
AP v vp and this is working really well

00:08:15,419 --> 00:08:20,459
at the moment and we've always done at

00:08:18,479 --> 00:08:22,229
the moment is prototype work we haven't

00:08:20,459 --> 00:08:24,899
optimized the code to master still much

00:08:22,229 --> 00:08:30,629
work to be done but the first step seems

00:08:24,899 --> 00:08:33,000
to be really interesting okay we still

00:08:30,629 --> 00:08:35,610
have features merged for example as I as

00:08:33,000 --> 00:08:38,009
I told you in order to make F xtp work

00:08:35,610 --> 00:08:40,709
you need the BPF program loaded in the

00:08:38,009 --> 00:08:44,639
driver right now we only support a BPF

00:08:40,709 --> 00:08:46,290
program per network interface future

00:08:44,639 --> 00:08:49,769
work says that we're going to support a

00:08:46,290 --> 00:08:50,790
BPF program / q so for every hard work

00:08:49,769 --> 00:08:53,100
we needed to have you're going to have

00:08:50,790 --> 00:08:54,750
in BPF program doing the processing for

00:08:53,100 --> 00:08:58,170
you so this is going to get in verb or

00:08:54,750 --> 00:08:59,639
dragged around for the XTP work we did

00:08:58,170 --> 00:09:01,319
for the TSN work we did on the

00:08:59,639 --> 00:09:03,120
measurements we figured out that if you

00:09:01,319 --> 00:09:03,690
need really low latency you're going to

00:09:03,120 --> 00:09:05,340
have to do

00:09:03,690 --> 00:09:06,750
and you're probably gonna have to run

00:09:05,340 --> 00:09:10,650
with interrupts disabled and you're

00:09:06,750 --> 00:09:13,050
probably gonna have to try and avoid any

00:09:10,650 --> 00:09:14,700
MMI your access or any uncursed memory

00:09:13,050 --> 00:09:16,500
accesses do you need because you there

00:09:14,700 --> 00:09:17,850
are some graphs following up let's show

00:09:16,500 --> 00:09:20,010
the latency and you'll figure out that

00:09:17,850 --> 00:09:24,930
all this stuff for heating latency

00:09:20,010 --> 00:09:27,090
really really bad we unfortunately we

00:09:24,930 --> 00:09:29,610
don't have a driver to test zero copy on

00:09:27,090 --> 00:09:31,920
because zero copy is a bit tricky to get

00:09:29,610 --> 00:09:35,130
it correctly working at the moment and

00:09:31,920 --> 00:09:37,230
the only driver supporting it is the 10

00:09:35,130 --> 00:09:39,030
and 40 gigabit Intel network interfaces

00:09:37,230 --> 00:09:41,190
so there's no support for low-end

00:09:39,030 --> 00:09:43,320
drivers for it does the some support for

00:09:41,190 --> 00:09:50,750
low-end drivers on xdp only but without

00:09:43,320 --> 00:09:54,090
zero copy so how we did the measurements

00:09:50,750 --> 00:09:55,020
on the previous worker that we need when

00:09:54,090 --> 00:09:58,050
we try to measure

00:09:55,020 --> 00:09:59,700
skb latencies skb has a some method that

00:09:58,050 --> 00:10:00,270
skb is the socket buffer on the linux

00:09:59,700 --> 00:10:03,480
kernel

00:10:00,270 --> 00:10:05,040
so the skb has some mated data that

00:10:03,480 --> 00:10:06,660
already carried timestamps they carry

00:10:05,040 --> 00:10:08,610
Hardware timestamp so you can figure out

00:10:06,660 --> 00:10:10,470
that what the timestamp was when the

00:10:08,610 --> 00:10:12,120
packet entered the network interface and

00:10:10,470 --> 00:10:13,650
you can compare that to the time stop of

00:10:12,120 --> 00:10:15,870
the user space the moment you receive

00:10:13,650 --> 00:10:18,300
the packet so you know how much time the

00:10:15,870 --> 00:10:19,020
packet takes from the hardware to the

00:10:18,300 --> 00:10:20,910
user space

00:10:19,020 --> 00:10:22,950
unfortunately xtp doesn't have an

00:10:20,910 --> 00:10:26,640
in-store similar infrastructure at the

00:10:22,950 --> 00:10:29,280
moment but it it does have a header that

00:10:26,640 --> 00:10:30,900
we can piggyback because it's 32 unused

00:10:29,280 --> 00:10:33,060
bytes at the moment so we can use that

00:10:30,900 --> 00:10:34,830
Headroom to insert the timestamp so we

00:10:33,060 --> 00:10:36,330
pretty much have the same effect the bad

00:10:34,830 --> 00:10:38,940
part is that we have to hug the driver

00:10:36,330 --> 00:10:40,740
in order to get that so the idea is that

00:10:38,940 --> 00:10:42,210
you insert the timestamp on there and

00:10:40,740 --> 00:10:43,770
when the packet arrives to the user

00:10:42,210 --> 00:10:45,660
space you get the fresh timestamp you

00:10:43,770 --> 00:10:47,910
compare the two timestamps and you know

00:10:45,660 --> 00:10:53,400
how much time the packet took from the

00:10:47,910 --> 00:10:55,140
network card to the user space if the

00:10:53,400 --> 00:10:56,700
hardwood support timestamps this is

00:10:55,140 --> 00:10:58,230
great because you can you can find the

00:10:56,700 --> 00:10:59,640
latency of the full path so if you have

00:10:58,230 --> 00:11:01,380
hardware timestamps on the network

00:10:59,640 --> 00:11:03,300
interface you insert the hardware times

00:11:01,380 --> 00:11:05,130
them and you get the full path delay by

00:11:03,300 --> 00:11:07,230
full path we mean how much time the time

00:11:05,130 --> 00:11:10,020
took that the packet took from the

00:11:07,230 --> 00:11:12,089
hardware down to user space if your

00:11:10,020 --> 00:11:14,060
hardware doesn't support timestamps then

00:11:12,089 --> 00:11:15,740
you have to insert the software one

00:11:14,060 --> 00:11:19,400
and you're gonna get less accurate

00:11:15,740 --> 00:11:20,600
because you want counting the internal

00:11:19,400 --> 00:11:22,520
timing you won't come

00:11:20,600 --> 00:11:24,410
counting the software interrupt timing

00:11:22,520 --> 00:11:27,020
but you'll still get an approximation of

00:11:24,410 --> 00:11:31,580
how much time the network of the network

00:11:27,020 --> 00:11:34,780
start needs to process the packet so we

00:11:31,580 --> 00:11:37,010
need some initial tests on a cortex a53

00:11:34,780 --> 00:11:39,470
unfortunately the hardware we did the

00:11:37,010 --> 00:11:41,210
tests on didn't have any time stamps so

00:11:39,470 --> 00:11:43,340
we couldn't measure the full path delay

00:11:41,210 --> 00:11:46,610
that we mentioned we could only measure

00:11:43,340 --> 00:11:50,660
the delay from the network stack from

00:11:46,610 --> 00:11:53,420
the driver stack down to user space we

00:11:50,660 --> 00:11:55,670
did two tests the first one was 10,000

00:11:53,420 --> 00:11:58,310
packets sending 1,000 packets per second

00:11:55,670 --> 00:12:01,010
and the second one was 30,000 packets

00:11:58,310 --> 00:12:04,700
sending at at a rate of 5,000 packets

00:12:01,010 --> 00:12:06,950
per second the first one is with

00:12:04,700 --> 00:12:09,670
interface fully working so as you can

00:12:06,950 --> 00:12:13,220
see we got an average delay of thirty of

00:12:09,670 --> 00:12:15,670
three almost three microseconds and we

00:12:13,220 --> 00:12:18,560
had the peak delay of 16 microseconds

00:12:15,670 --> 00:12:21,170
the next the next entry you see when

00:12:18,560 --> 00:12:24,170
it's when it says polling mode it's a

00:12:21,170 --> 00:12:25,730
it's an mode that we disabled the MMI

00:12:24,170 --> 00:12:27,620
access from the hardware it was a

00:12:25,730 --> 00:12:29,000
hardware that we could do that it didn't

00:12:27,620 --> 00:12:31,420
hurt the general operation of the

00:12:29,000 --> 00:12:34,220
hardware and we could actually measure

00:12:31,420 --> 00:12:35,930
the difference that made so as you can

00:12:34,220 --> 00:12:38,150
see the latency dropped down by a lot

00:12:35,930 --> 00:12:40,340
the peak latency is random because you

00:12:38,150 --> 00:12:42,350
still have random jitter as we explained

00:12:40,340 --> 00:12:44,960
and we haven't figured that part out but

00:12:42,350 --> 00:12:46,940
the average latency went down by a

00:12:44,960 --> 00:12:49,190
microseconds just by disabling the MMI

00:12:46,940 --> 00:12:54,500
accesses and it was a single MMI access

00:12:49,190 --> 00:12:56,540
for for 16 packets and as you can see

00:12:54,500 --> 00:12:59,270
the results are similar to when we tried

00:12:56,540 --> 00:13:01,250
30,000 packets at the rate of 5,000

00:12:59,270 --> 00:13:03,530
packets per second and at the rate with

00:13:01,250 --> 00:13:07,100
everybody access is enabled and disabled

00:13:03,530 --> 00:13:11,090
again the reason the 30 thousand packets

00:13:07,100 --> 00:13:12,860
5 30,000 packet at a 5k rate rate limit

00:13:11,090 --> 00:13:14,810
was lower is that probably the classes

00:13:12,860 --> 00:13:18,830
were hotter at that point so you could

00:13:14,810 --> 00:13:20,450
process packets faster so this is a

00:13:18,830 --> 00:13:21,920
graph okay I don't know if this is if

00:13:20,450 --> 00:13:23,330
this is showing up this is time in

00:13:21,920 --> 00:13:25,730
nanoseconds that you see and this is the

00:13:23,330 --> 00:13:27,710
packets that you receive so this is

00:13:25,730 --> 00:13:30,890
10,000 packets on at once

00:13:27,710 --> 00:13:32,960
pockets rate the red dots that you see

00:13:30,890 --> 00:13:35,300
on the on the slide means that this is

00:13:32,960 --> 00:13:37,400
above five five microseconds the yellow

00:13:35,300 --> 00:13:38,840
ones are between five and two

00:13:37,400 --> 00:13:41,450
microseconds and everything that you see

00:13:38,840 --> 00:13:44,810
green is below two microseconds as you

00:13:41,450 --> 00:13:47,270
can see there's no green here and on

00:13:44,810 --> 00:13:50,150
this slide this is with the MMI access

00:13:47,270 --> 00:13:51,710
is disabled so the graph got a lot

00:13:50,150 --> 00:13:53,990
better here you still have some random

00:13:51,710 --> 00:13:56,120
red dots on the jumps and the latency

00:13:53,990 --> 00:13:59,410
but as you can see compared to the

00:13:56,120 --> 00:14:01,340
previous slide you do have many packets

00:13:59,410 --> 00:14:03,560
below two microseconds

00:14:01,340 --> 00:14:07,940
on the time that they need from the

00:14:03,560 --> 00:14:11,300
kernel space down to user space and this

00:14:07,940 --> 00:14:13,190
is the similar result on 30,000 packets

00:14:11,300 --> 00:14:15,890
at a rate of five thousand packets per

00:14:13,190 --> 00:14:17,600
second again as you can see there's no

00:14:15,890 --> 00:14:19,970
packet below two microseconds at the

00:14:17,600 --> 00:14:21,650
moment and the moment you disable them

00:14:19,970 --> 00:14:24,800
in my access is to occur at one caste

00:14:21,650 --> 00:14:30,380
memory there's a bunch of packets below

00:14:24,800 --> 00:14:31,660
two microseconds how much time do we

00:14:30,380 --> 00:14:40,310
have

00:14:31,660 --> 00:14:41,570
excuse me four minutes all right all

00:14:40,310 --> 00:14:45,380
right you can pick it up from now and we

00:14:41,570 --> 00:14:46,610
can have the questions okay yeah pick it

00:14:45,380 --> 00:14:56,150
up and we can leave the rest of the time

00:14:46,610 --> 00:14:59,330
for questions so how we actually do

00:14:56,150 --> 00:15:03,950
measurements using something like this

00:14:59,330 --> 00:15:06,170
test model when we have some board that

00:15:03,950 --> 00:15:12,430
has Hardware time stamping in our case

00:15:06,170 --> 00:15:15,500
it's a m57 2ti board it's cortex a15 and

00:15:12,430 --> 00:15:18,140
having those Hardware timestamp and

00:15:15,500 --> 00:15:20,600
software times that we can you know do

00:15:18,140 --> 00:15:23,300
the difference and calculate actually

00:15:20,600 --> 00:15:26,360
like it takes from in the driver itself

00:15:23,300 --> 00:15:30,620
and from the till the driver from the

00:15:26,360 --> 00:15:33,320
driver to the user space this is the

00:15:30,620 --> 00:15:35,510
basic scheme for this and usually I do

00:15:33,320 --> 00:15:37,670
measurements for mainline camera but

00:15:35,510 --> 00:15:40,640
this time just decided to

00:15:37,670 --> 00:15:44,150
do measurements 40-channel and close

00:15:40,640 --> 00:15:50,000
this one I found in TI it was 4.19 I can

00:15:44,150 --> 00:16:02,630
apply my FX DP changes from this and I'm

00:15:50,000 --> 00:16:08,270
do measurements so for comparison so for

00:16:02,630 --> 00:16:09,950
comparison I just might not not that you

00:16:08,270 --> 00:16:12,980
can know I have less latency usually

00:16:09,950 --> 00:16:16,150
than I have for this this one because

00:16:12,980 --> 00:16:20,230
it's not it's still migrating from lower

00:16:16,150 --> 00:16:24,320
numbers a week we we usually have about

00:16:20,230 --> 00:16:27,950
10 15 microsecond less for driver for

00:16:24,320 --> 00:16:30,470
this deck for FX DP stack I mean that is

00:16:27,950 --> 00:16:33,400
from the packet leaves drywall to the

00:16:30,470 --> 00:16:36,520
moment it to sit in the user space and

00:16:33,400 --> 00:16:38,900
in case and its software polling

00:16:36,520 --> 00:16:41,060
actually we do measurements not only for

00:16:38,900 --> 00:16:44,630
software pol that means that we're just

00:16:41,060 --> 00:16:48,760
constantly pulling hard work you so the

00:16:44,630 --> 00:16:51,350
most Hebraic you FX dpq we see you and

00:16:48,760 --> 00:16:54,470
also we have measurements when we when

00:16:51,350 --> 00:16:57,500
we use system poll I mean when we just

00:16:54,470 --> 00:17:02,750
wait one scheduler wake us and we can

00:16:57,500 --> 00:17:04,490
receive some packet and here for

00:17:02,750 --> 00:17:06,410
comparison because I failed GP circuit

00:17:04,490 --> 00:17:08,690
basically its raw socket type so we

00:17:06,410 --> 00:17:14,660
decided to do some comparison this RF

00:17:08,690 --> 00:17:17,900
packet sorta type and here only 128 PPS

00:17:14,660 --> 00:17:20,030
and one PPS it's no small rates because

00:17:17,900 --> 00:17:22,760
we we have actually measurements for

00:17:20,030 --> 00:17:24,310
higher rates but for TSN it's important

00:17:22,760 --> 00:17:28,730
also to have some measurements for

00:17:24,310 --> 00:17:33,070
follow rates like one PPS and 128 for

00:17:28,730 --> 00:17:37,520
these keys and it's where it depends on

00:17:33,070 --> 00:17:40,370
latencies latency is very depend on rate

00:17:37,520 --> 00:17:44,980
we use for this so for instance when we

00:17:40,370 --> 00:17:48,340
have I have x DP socket we have

00:17:44,980 --> 00:17:48,910
for tech latency about eight microsecond

00:17:48,340 --> 00:17:53,919
its

00:17:48,910 --> 00:17:56,130
cortex a15 not it's a 32-bit with 1v

00:17:53,919 --> 00:18:00,179
Gertz and they have two CPUs and

00:17:56,130 --> 00:18:04,540
increase priority for application I

00:18:00,179 --> 00:18:06,850
didn't do any CPU pinning or stuff like

00:18:04,540 --> 00:18:10,090
this because I have immediately a number

00:18:06,850 --> 00:18:14,650
of CPUs so we have eight eight

00:18:10,090 --> 00:18:17,679
microseconds for 128 PPS and just when

00:18:14,650 --> 00:18:20,230
for one PPS it's increased in two

00:18:17,679 --> 00:18:29,500
microseconds its average levels it's not

00:18:20,230 --> 00:18:32,919
big Levinson it's a plot section before

00:18:29,500 --> 00:18:42,190
driver and instead like to see what 100

00:18:32,919 --> 00:18:45,520
thousands packets with 128 PPS so

00:18:42,190 --> 00:18:48,549
actually yeah you can see that behaves

00:18:45,520 --> 00:18:51,220
we still have some peaks and that's

00:18:48,549 --> 00:18:53,890
because we know we didn't do you say

00:18:51,220 --> 00:18:56,260
Mississippi or pink and for the driver

00:18:53,890 --> 00:18:59,520
also have miss latency because we we

00:18:56,260 --> 00:19:03,700
can't assigned higher priority for

00:18:59,520 --> 00:19:08,770
software interrupt them then use the CPU

00:19:03,700 --> 00:19:14,650
for so for application itself so it

00:19:08,770 --> 00:19:21,790
software poll also we have dispersion of

00:19:14,650 --> 00:19:23,799
this software for complete latency is it

00:19:21,790 --> 00:19:25,600
means that from the very beginning like

00:19:23,799 --> 00:19:34,120
at arise from the interface and proceed

00:19:25,600 --> 00:19:36,970
in user space that's a similar one but

00:19:34,120 --> 00:19:43,080
for one PPS so we can see that it's

00:19:36,970 --> 00:19:47,890
higher for one who PS and it's yeah so

00:19:43,080 --> 00:19:49,360
more disparate on its own the great it's

00:19:47,890 --> 00:19:50,830
the same effect that we had on the

00:19:49,360 --> 00:19:52,899
previous card with the more the more

00:19:50,830 --> 00:19:55,419
packets you send the latency decreases

00:19:52,899 --> 00:19:56,980
because you keep the Casas hotter so how

00:19:55,419 --> 00:19:58,570
many packets per second you send does

00:19:56,980 --> 00:20:00,070
matter and latency at the moment and

00:19:58,570 --> 00:20:04,510
it's one of the things you will figure

00:20:00,070 --> 00:20:06,669
out if we can increase in 1,000 ppm in

00:20:04,510 --> 00:20:19,210
this case you can drop in 10

00:20:06,669 --> 00:20:21,549
microseconds up to 10 milliseconds what

00:20:19,210 --> 00:20:25,240
we can do the questions if you don't

00:20:21,549 --> 00:20:28,090
know second system for this is for

00:20:25,240 --> 00:20:31,870
system Bowl this is not software pole we

00:20:28,090 --> 00:20:33,760
just wait while while scheduler just you

00:20:31,870 --> 00:20:36,070
know wake up us and we can use your

00:20:33,760 --> 00:20:38,529
packet in this case we have about 20

00:20:36,070 --> 00:20:41,260
microseconds this is not very good on in

00:20:38,529 --> 00:20:44,760
this case and yeah so probably we need

00:20:41,260 --> 00:20:48,490
to do something pink and then purify

00:20:44,760 --> 00:20:53,470
once more so this is a dispersion for

00:20:48,490 --> 00:21:00,570
this one and just complete letting see

00:20:53,470 --> 00:21:00,570
yeah so right questions

00:21:02,480 --> 00:21:07,280
we have a microphone it's torn up and

00:21:18,070 --> 00:21:23,290
the you had the PC generating the

00:21:21,100 --> 00:21:27,640
packets yes have you considered the

00:21:23,290 --> 00:21:30,130
jitter yes well well there is dinner

00:21:27,640 --> 00:21:31,390
from there but the thing is that the

00:21:30,130 --> 00:21:33,070
jitter would matter on evens

00:21:31,390 --> 00:21:34,630
measurements because on the harder on

00:21:33,070 --> 00:21:36,820
the initial hardware that we tested we

00:21:34,630 --> 00:21:38,799
didn't have any timestamps so we didn't

00:21:36,820 --> 00:21:40,630
check the hardware timestamp from the

00:21:38,799 --> 00:21:42,460
network interface down to user space

00:21:40,630 --> 00:21:44,110
what we tested on the first result that

00:21:42,460 --> 00:21:46,120
the results were below two microseconds

00:21:44,110 --> 00:21:48,190
was the moment the packet is entering

00:21:46,120 --> 00:21:50,350
the software stack of the driver down to

00:21:48,190 --> 00:21:51,820
user space so the sender's eater doesn't

00:21:50,350 --> 00:21:55,809
matter in this case it would obviously

00:21:51,820 --> 00:21:57,160
matter or evils case where you is it

00:21:55,809 --> 00:21:58,809
doesn't matter on events case either

00:21:57,160 --> 00:22:00,910
because we're only measuring we're not

00:21:58,809 --> 00:22:03,520
measuring end to end latency we're only

00:22:00,910 --> 00:22:05,320
measuring from the network down to user

00:22:03,520 --> 00:22:07,150
space so it doesn't matter what Jeter

00:22:05,320 --> 00:22:08,710
the host introduces the remote was

00:22:07,150 --> 00:22:10,150
introducing the only thing that matters

00:22:08,710 --> 00:22:12,220
is how much time do we need to process

00:22:10,150 --> 00:22:15,010
the packet from the moment you get it

00:22:12,220 --> 00:22:17,620
down to your user space application okay

00:22:15,010 --> 00:22:19,660
so all these measurements yes this is

00:22:17,620 --> 00:22:22,660
for a single horse we have plans on

00:22:19,660 --> 00:22:25,900
doing across latency with obviously what

00:22:22,660 --> 00:22:27,580
you said will matter to in order to get

00:22:25,900 --> 00:22:36,730
end-to-end latencies but we haven't done

00:22:27,580 --> 00:22:38,140
this yet so what exactly do you mean by

00:22:36,730 --> 00:22:41,830
software pol

00:22:38,140 --> 00:22:43,870
how does it work okay what I'll do a bit

00:22:41,830 --> 00:22:46,030
more technical now what I what I meant

00:22:43,870 --> 00:22:47,980
about no MMI or access from the initial

00:22:46,030 --> 00:22:49,660
measurements is that you disable the MMI

00:22:47,980 --> 00:22:52,419
you access now the MMI you access on

00:22:49,660 --> 00:22:54,490
that hardware resetting the interrupt so

00:22:52,419 --> 00:22:56,919
what ended up what I ended up doing is

00:22:54,490 --> 00:22:59,919
that the the Linux net and network nappy

00:22:56,919 --> 00:23:03,490
was already always called there was

00:22:59,919 --> 00:23:05,500
always an interrupt running right but

00:23:03,490 --> 00:23:07,570
since I wasn't measuring network

00:23:05,500 --> 00:23:08,919
timestamps from the hardware because the

00:23:07,570 --> 00:23:10,929
hardware didn't support it I don't

00:23:08,919 --> 00:23:12,040
really care if I if I spam interrupts

00:23:10,929 --> 00:23:13,990
from the whole time just pick up a

00:23:12,040 --> 00:23:15,280
timestamp from the moment the packet

00:23:13,990 --> 00:23:18,520
starts getting processed

00:23:15,280 --> 00:23:20,350
down to user space so that's that's what

00:23:18,520 --> 00:23:22,090
we meant so the poling means that you

00:23:20,350 --> 00:23:23,860
you don't have any interruption able you

00:23:22,090 --> 00:23:25,480
just pull the queue all the time in

00:23:23,860 --> 00:23:28,410
order to get the packets on as fast as

00:23:25,480 --> 00:23:31,770
they arrive so you do sit in it

00:23:28,410 --> 00:23:35,850
user space level yes like you constantly

00:23:31,770 --> 00:23:41,430
yes exactly but it's not very poor I

00:23:35,850 --> 00:23:44,190
know essentially keeps CPU at the idea

00:23:41,430 --> 00:23:46,800
to minimize latency without paying

00:23:44,190 --> 00:23:48,900
attention to power it's a trial you have

00:23:46,800 --> 00:23:50,520
to figure out how much time how much the

00:23:48,900 --> 00:23:51,960
interrupt costs for you because it's a

00:23:50,520 --> 00:23:53,760
hardware interrupt and then you have to

00:23:51,960 --> 00:23:55,680
raise a soft i IQ on the kernel to get

00:23:53,760 --> 00:23:57,660
the nappy execution to run and all the

00:23:55,680 --> 00:23:59,580
things so you need to figure out what

00:23:57,660 --> 00:24:01,560
what's the latency of your application

00:23:59,580 --> 00:24:03,420
what are you looking for versus what you

00:24:01,560 --> 00:24:05,700
have so if you if you're looking for sub

00:24:03,420 --> 00:24:07,680
20 microseconds for example this might

00:24:05,700 --> 00:24:10,920
be doable with interrupts but if you're

00:24:07,680 --> 00:24:12,750
looking for some five microseconds with

00:24:10,920 --> 00:24:14,160
the current hardware this is not doable

00:24:12,750 --> 00:24:16,890
at least from what we've seen on the

00:24:14,160 --> 00:24:19,350
measurement essentially my question is

00:24:16,890 --> 00:24:22,290
this software pole mode is intended to

00:24:19,350 --> 00:24:24,750
be used in production or it's solely for

00:24:22,290 --> 00:24:26,190
measurement this is for measurement at

00:24:24,750 --> 00:24:28,740
the moment we don't have any production

00:24:26,190 --> 00:24:30,930
requirements for sub 5 microseconds so

00:24:28,740 --> 00:24:33,030
that the polling mode was just for the

00:24:30,930 --> 00:24:34,890
measurements that we did I don't know

00:24:33,030 --> 00:24:36,630
what's gonna end up doing on the on the

00:24:34,890 --> 00:24:39,990
industrial environment then you have a

00:24:36,630 --> 00:24:41,670
point because you so what 20% rise on

00:24:39,990 --> 00:24:45,660
the CPU temperature when we did Polina

00:24:41,670 --> 00:24:47,250
yeah so you do have a point but it all

00:24:45,660 --> 00:24:48,750
depends on what you're looking on the

00:24:47,250 --> 00:24:50,490
application if you're looking for really

00:24:48,750 --> 00:24:51,840
low latencies polling is the only way

00:24:50,490 --> 00:24:53,550
you can get it at the moment if you

00:24:51,840 --> 00:24:55,770
don't really care about ultra low

00:24:53,550 --> 00:24:57,780
latencies and you just care about I

00:24:55,770 --> 00:25:00,420
don't know sub 40 microsecond or sub 30

00:24:57,780 --> 00:25:03,120
or sub 20 this is probably doable with

00:25:00,420 --> 00:25:06,080
interrupt enable okay thank you I'm just

00:25:03,120 --> 00:25:10,230
thinking how to adopt this for Zephyr

00:25:06,080 --> 00:25:13,560
we can have the talk afternoon can you

00:25:10,230 --> 00:25:20,430
please open this slide with the plot for

00:25:13,560 --> 00:25:23,000
frames versus x so I noted that some you

00:25:20,430 --> 00:25:27,480
had a question about the nature of those

00:25:23,000 --> 00:25:31,140
outliners like some points like has much

00:25:27,480 --> 00:25:34,170
more latency the mother's right so what

00:25:31,140 --> 00:25:36,680
is the nature of those high latency

00:25:34,170 --> 00:25:36,680
points

00:25:36,790 --> 00:25:43,690
[Music]

00:25:38,560 --> 00:25:45,480
and we have two CPUs only and actually

00:25:43,690 --> 00:25:49,810
it's a memory right

00:25:45,480 --> 00:25:51,160
we mean we'll have you know Numa and we

00:25:49,810 --> 00:25:53,050
have some applications running in

00:25:51,160 --> 00:25:59,770
parallel that can accept the same memory

00:25:53,050 --> 00:26:03,490
and make this make this and also we have

00:25:59,770 --> 00:26:05,620
impact of scheduler okay so this is

00:26:03,490 --> 00:26:08,470
because of the scheduler yeah we can't

00:26:05,620 --> 00:26:11,440
avoid this in order to get there in case

00:26:08,470 --> 00:26:13,870
of this software Paul this is system for

00:26:11,440 --> 00:26:18,490
pay attention is not software for you

00:26:13,870 --> 00:26:20,500
not polling we just waiting on all in

00:26:18,490 --> 00:26:22,570
order to have the lowest latency Maxim

00:26:20,500 --> 00:26:25,330
had the suggesting for this he said do I

00:26:22,570 --> 00:26:27,670
know heard Colonel isolate the CPUs and

00:26:25,330 --> 00:26:29,080
pin down the CPU and pin down different

00:26:27,670 --> 00:26:30,520
CPUs you want for the user space

00:26:29,080 --> 00:26:32,500
application this is the probably gonna

00:26:30,520 --> 00:26:35,710
get ridiculous several methods for this

00:26:32,500 --> 00:26:37,570
you can you can kind of software poll do

00:26:35,710 --> 00:26:39,430
software polling application but it's

00:26:37,570 --> 00:26:42,910
done currently that your latency is

00:26:39,430 --> 00:26:45,460
exactly you know from the user from the

00:26:42,910 --> 00:26:47,830
driver to the user space you can you can

00:26:45,460 --> 00:26:49,810
have increased latency from the hardware

00:26:47,830 --> 00:26:53,380
received packet till the moment it

00:26:49,810 --> 00:26:56,530
leaves driver so we need to pull also

00:26:53,380 --> 00:26:59,380
software interrupt kind of pulling it

00:26:56,530 --> 00:27:01,720
nappy for this right and this is

00:26:59,380 --> 00:27:03,820
complete latency this is from the very

00:27:01,720 --> 00:27:05,830
beginning when packet received in the

00:27:03,820 --> 00:27:08,590
hardware I get hardware timestamp and in

00:27:05,830 --> 00:27:12,280
the software software timestamp in the

00:27:08,590 --> 00:27:19,690
real space so this is complete including

00:27:12,280 --> 00:27:22,270
this software even we're right we're out

00:27:19,690 --> 00:27:27,000
of time so if you have any questions you

00:27:22,270 --> 00:27:30,359
can find either me or even in thank you

00:27:27,000 --> 00:27:30,359

YouTube URL: https://www.youtube.com/watch?v=-HtDxpGYyrw


