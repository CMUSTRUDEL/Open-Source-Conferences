Title: BKK19-321: FPGAs for Highest Performance Inference
Publication date: 2019-04-05
Playlist: Linaro Connect Bangkok 2019
Description: 
	GPUs are often used to accelerate machine learning inference as they offer improvements in performance over standard processors. FPGAs, however, have unique capabilities that offer performance advantages over both CPUs and GPUs. This session will introduce those capabilities and explore some metrics.
Captions: 
	00:00:06,640 --> 00:00:10,780
welcome back from lunch my name is Craig

00:00:08,769 --> 00:00:13,180
Abramson I'm gonna do a session today

00:00:10,780 --> 00:00:15,820
called FPGAs for highest performance

00:00:13,180 --> 00:00:18,550
inference which is a neural net term

00:00:15,820 --> 00:00:20,439
I am from Xilinx I am based out of

00:00:18,550 --> 00:00:22,990
Longmont Colorado that's a picture from

00:00:20,439 --> 00:00:25,390
the parking lot of our building kind of

00:00:22,990 --> 00:00:26,769
nice I am a senior technical marketing

00:00:25,390 --> 00:00:28,269
engineer I've been that for a couple

00:00:26,769 --> 00:00:30,789
years prior to that I was a middle aged

00:00:28,269 --> 00:00:32,619
Technical Marketing engineer nothing

00:00:30,789 --> 00:00:34,750
okay I promise that's the only joke I

00:00:32,619 --> 00:00:36,699
will try and tell all right we're gonna

00:00:34,750 --> 00:00:40,090
talk about machine learning for the next

00:00:36,699 --> 00:00:42,280
25 minutes and here's how we're gonna do

00:00:40,090 --> 00:00:43,989
it we're going to talk about FPGAs just

00:00:42,280 --> 00:00:48,070
in general to kind of level set because

00:00:43,989 --> 00:00:50,050
I noticed here in this particular venue

00:00:48,070 --> 00:00:51,879
in this particular conference not

00:00:50,050 --> 00:00:53,289
everybody knows what an FPGA is so I'm

00:00:51,879 --> 00:00:54,670
gonna have a slide or two on that and

00:00:53,289 --> 00:00:56,949
then we're going to talk about things

00:00:54,670 --> 00:00:59,680
that influence the decision to use an

00:00:56,949 --> 00:01:01,180
FPGA and neural nets and then we will

00:00:59,680 --> 00:01:04,629
conclude with a conclusion because

00:01:01,180 --> 00:01:06,430
that's what you do all right quiz FPGA

00:01:04,629 --> 00:01:10,899
czar raise your hand if you think is a

00:01:06,430 --> 00:01:14,590
flexible parallel GPU accelerator nobody

00:01:10,899 --> 00:01:21,660
okay a functionally pervasive Giga speed

00:01:14,590 --> 00:01:26,259
ALU no field programmable gate array

00:01:21,660 --> 00:01:30,250
sweet and fast processor four new

00:01:26,259 --> 00:01:33,280
assemblers someday yeah right now it's a

00:01:30,250 --> 00:01:35,920
rather slow d it's a decelerate er four

00:01:33,280 --> 00:01:37,030
new assemblers to do assemblers all

00:01:35,920 --> 00:01:44,110
right so everybody kind of knows what an

00:01:37,030 --> 00:01:47,709
FPGA is what's because sometimes it's

00:01:44,110 --> 00:01:50,259
that sometimes it's that one okay so why

00:01:47,709 --> 00:01:52,569
might one use an FPGA they might use an

00:01:50,259 --> 00:01:55,630
FPGA as an alternative to an ASIC a

00:01:52,569 --> 00:01:59,140
dedicated chip why would you do that you

00:01:55,630 --> 00:02:01,330
get faster time-to-market so it takes a

00:01:59,140 --> 00:02:04,360
lot of time and money and energy to do a

00:02:01,330 --> 00:02:08,080
full custom chip FPGA is allow you to

00:02:04,360 --> 00:02:11,110
iterate and get to market faster it is a

00:02:08,080 --> 00:02:12,760
single chip solution versus having

00:02:11,110 --> 00:02:14,620
multiple different chips with different

00:02:12,760 --> 00:02:16,630
functions you can pull all of the

00:02:14,620 --> 00:02:18,910
different functions together and have a

00:02:16,630 --> 00:02:19,330
single chip which is a lower power often

00:02:18,910 --> 00:02:22,090
high

00:02:19,330 --> 00:02:23,470
reliability solution it is

00:02:22,090 --> 00:02:25,600
reprogrammable so you can be a very

00:02:23,470 --> 00:02:27,670
sloppy bad designer like I was before I

00:02:25,600 --> 00:02:30,160
went to marketing and that is a

00:02:27,670 --> 00:02:31,510
lower-risk solution oops there was a

00:02:30,160 --> 00:02:33,670
there's a fourth one and I think I got

00:02:31,510 --> 00:02:35,170
rid of it and I do have I have to cop to

00:02:33,670 --> 00:02:37,210
something I have a little bit of

00:02:35,170 --> 00:02:40,960
dyslexia and I thought this class went

00:02:37,210 --> 00:02:43,900
from 2 to 255 but it really goes to from

00:02:40,960 --> 00:02:45,670
2 to 225 so I'll opt about half of my

00:02:43,900 --> 00:02:47,710
slides out and I think subconsciously

00:02:45,670 --> 00:02:51,070
I'm racing to get through it and I also

00:02:47,710 --> 00:02:52,690
had an expresso so hopefully you'll be

00:02:51,070 --> 00:02:54,880
able to keep up tell me to slow the hell

00:02:52,690 --> 00:02:58,420
down if you need to that's ok

00:02:54,880 --> 00:03:01,180
in the FPGA world we do so ok we're

00:02:58,420 --> 00:03:04,360
going to switch gears here in the FPGA

00:03:01,180 --> 00:03:07,120
world we do inference rather than

00:03:04,360 --> 00:03:08,440
training some solutions that are doing

00:03:07,120 --> 00:03:10,030
neural nets in the world do both

00:03:08,440 --> 00:03:12,490
training and inference and what I like

00:03:10,030 --> 00:03:13,930
to do is take a second to kind of

00:03:12,490 --> 00:03:15,250
explain a little bit at a very high

00:03:13,930 --> 00:03:17,740
level the difference between training

00:03:15,250 --> 00:03:19,180
and inference if you don't know training

00:03:17,740 --> 00:03:20,590
in a neural net is where you start out

00:03:19,180 --> 00:03:22,240
with a neural network and imagine it's

00:03:20,590 --> 00:03:24,310
like a blank brain and you're trying to

00:03:22,240 --> 00:03:26,230
teach this thing what dogs look like you

00:03:24,310 --> 00:03:28,480
show it a zillion pictures of dogs and

00:03:26,230 --> 00:03:30,489
it eventually trains itself to recognize

00:03:28,480 --> 00:03:32,440
dogs and that takes a lot of math and a

00:03:30,489 --> 00:03:33,970
lot of energy and a lot of time but once

00:03:32,440 --> 00:03:36,760
you have that neural net and it's been

00:03:33,970 --> 00:03:38,410
trained to recognize dogs you can then

00:03:36,760 --> 00:03:40,030
figure out which parts of the neural net

00:03:38,410 --> 00:03:42,310
where important get rid of the things

00:03:40,030 --> 00:03:47,590
that weren't and then it becomes a much

00:03:42,310 --> 00:03:49,090
smaller job to infer what a dog is from

00:03:47,590 --> 00:03:51,970
all the training data you've done and

00:03:49,090 --> 00:03:54,610
you can then run a much more efficient

00:03:51,970 --> 00:03:57,489
lower power type implementation of this

00:03:54,610 --> 00:04:00,459
neural net and I like to use the example

00:03:57,489 --> 00:04:02,650
it's kind of like riding a bike so if

00:04:00,459 --> 00:04:04,660
you think back to when you were first

00:04:02,650 --> 00:04:06,519
learning to ride a bike you were

00:04:04,660 --> 00:04:08,860
throwing all of your attention at it

00:04:06,519 --> 00:04:10,030
you were paint you were making sure that

00:04:08,860 --> 00:04:11,410
you know you were balanced you were

00:04:10,030 --> 00:04:12,730
looking at the cars coming you were

00:04:11,410 --> 00:04:15,640
looking at the ground hoping you weren't

00:04:12,730 --> 00:04:17,709
gonna hit it you you were using a lot of

00:04:15,640 --> 00:04:20,890
your brain power to learn to ride the

00:04:17,709 --> 00:04:22,330
bike and you probably figured it out and

00:04:20,890 --> 00:04:23,890
now if you were to ride a bike

00:04:22,330 --> 00:04:26,350
you would not probably use very many

00:04:23,890 --> 00:04:27,820
neurons at all it's very similar in

00:04:26,350 --> 00:04:30,190
training and inference so that's my

00:04:27,820 --> 00:04:32,410
example of how to distinguish between

00:04:30,190 --> 00:04:32,680
training and inference and again in the

00:04:32,410 --> 00:04:35,560
F

00:04:32,680 --> 00:04:37,330
a world we do inference and we don't do

00:04:35,560 --> 00:04:38,770
training and there's like subtle

00:04:37,330 --> 00:04:40,259
exceptions to that that we're not going

00:04:38,770 --> 00:04:43,240
to cover in 25 minutes

00:04:40,259 --> 00:04:47,520
all right so one of the reasons FPGAs

00:04:43,240 --> 00:04:50,320
are useful in AI and an inference is

00:04:47,520 --> 00:04:53,530
it's solving a problem called the rate

00:04:50,320 --> 00:04:57,550
of AI innovation and what that means is

00:04:53,530 --> 00:05:01,210
AI is changing fast okay and to

00:04:57,550 --> 00:05:05,349
illustrate the point of why FPGAs makes

00:05:01,210 --> 00:05:07,509
sense in AI I'm going to do a little

00:05:05,349 --> 00:05:10,090
step back in time and think about PCI

00:05:07,509 --> 00:05:12,160
Express as it has rolled out over the

00:05:10,090 --> 00:05:16,120
years and years since like the year 2000

00:05:12,160 --> 00:05:17,770
etc and so I used to be a field

00:05:16,120 --> 00:05:20,199
applications engineer and I called on a

00:05:17,770 --> 00:05:22,930
test and measurement company okay that

00:05:20,199 --> 00:05:25,060
shall go unnamed and they made PCI

00:05:22,930 --> 00:05:29,860
Express analyzers now if you think about

00:05:25,060 --> 00:05:32,620
PCI Express the PCI Express standard

00:05:29,860 --> 00:05:34,030
spec is not ratified it's not ratified

00:05:32,620 --> 00:05:36,190
it's not ratified and then all of a

00:05:34,030 --> 00:05:37,900
sudden it is now all of this time when

00:05:36,190 --> 00:05:39,760
it's not ratified it's changing and

00:05:37,900 --> 00:05:41,710
thrashing a little bit but my test

00:05:39,760 --> 00:05:44,169
equipment company really wanted to come

00:05:41,710 --> 00:05:46,419
out with a product that could be a PCI

00:05:44,169 --> 00:05:48,460
Express analyzer at day one when that

00:05:46,419 --> 00:05:50,229
spec was available so instead of making

00:05:48,460 --> 00:05:52,630
a chip that they took their best guess

00:05:50,229 --> 00:05:55,300
on a custom chip that they took their

00:05:52,630 --> 00:05:57,610
best guess on on how PCI Express was

00:05:55,300 --> 00:05:59,949
going to work they used an FPGA instead

00:05:57,610 --> 00:06:01,180
so the whole time as the spec was

00:05:59,949 --> 00:06:05,169
changing they would track those changes

00:06:01,180 --> 00:06:07,900
and then on day one when the PCI Express

00:06:05,169 --> 00:06:10,210
spec was solid and ratified they had a

00:06:07,900 --> 00:06:13,229
product ready to go okay everybody kind

00:06:10,210 --> 00:06:16,659
of followed that that kind of thing so

00:06:13,229 --> 00:06:19,389
this is an example of neural nets and AI

00:06:16,659 --> 00:06:21,880
landscape it's changing a lot much like

00:06:19,389 --> 00:06:24,099
the PCI Express spec might have been

00:06:21,880 --> 00:06:25,810
changing over time so one of the big

00:06:24,099 --> 00:06:27,400
places it's changing right now this is

00:06:25,810 --> 00:06:28,870
just one example again it's a 25-minute

00:06:27,400 --> 00:06:31,360
talk so we can't go in a whole lot of

00:06:28,870 --> 00:06:35,500
detail but one of the places that it's

00:06:31,360 --> 00:06:38,320
changing is in the precision landscape

00:06:35,500 --> 00:06:40,990
so when you're doing training like I

00:06:38,320 --> 00:06:43,060
said you're doing training in like FP 32

00:06:40,990 --> 00:06:44,080
and very high dynamic range variables

00:06:43,060 --> 00:06:45,520
and things like that because you have to

00:06:44,080 --> 00:06:46,210
do a lot of math and accommodate for all

00:06:45,520 --> 00:06:48,940
the different thing

00:06:46,210 --> 00:06:51,160
but there's a lot of research going on

00:06:48,940 --> 00:06:53,289
that's trying to figure out in the

00:06:51,160 --> 00:06:56,560
inference realm whether you can get away

00:06:53,289 --> 00:06:58,090
with not FP 32 maybe a floating-point 16

00:06:56,560 --> 00:06:59,800
unit that's got less dynamic range or

00:06:58,090 --> 00:07:02,020
even integers and in fact that's a very

00:06:59,800 --> 00:07:05,500
active area of research and we take

00:07:02,020 --> 00:07:08,139
advantage of that my point being that as

00:07:05,500 --> 00:07:10,270
these algorithms change and the rent and

00:07:08,139 --> 00:07:12,699
the precision has to change we can

00:07:10,270 --> 00:07:15,280
accommodate that in an FPGA very very

00:07:12,699 --> 00:07:16,780
easily compared to some of the other

00:07:15,280 --> 00:07:18,849
alternatives like a GPU which I'll

00:07:16,780 --> 00:07:21,130
probably talk which I'll talk about more

00:07:18,849 --> 00:07:23,320
in further slides GPUs have a fairly

00:07:21,130 --> 00:07:25,810
fixed architecture so if you want to use

00:07:23,320 --> 00:07:27,070
a lower resolution you can but it

00:07:25,810 --> 00:07:29,770
affects your throughput in your

00:07:27,070 --> 00:07:32,139
performance of your of your design so

00:07:29,770 --> 00:07:34,570
inference moving to lower precision is

00:07:32,139 --> 00:07:36,130
one example of why FPGA is make a lot of

00:07:34,570 --> 00:07:38,800
sense when you're doing neural nets

00:07:36,130 --> 00:07:41,139
because the precision that you're going

00:07:38,800 --> 00:07:42,729
to need could very much change over time

00:07:41,139 --> 00:07:47,050
because there are so many changes going

00:07:42,729 --> 00:07:48,490
on in the neural net industry guys

00:07:47,050 --> 00:07:50,740
keeping up with me I'm kind of I feel

00:07:48,490 --> 00:07:52,360
like I'm talking fast but I'm also

00:07:50,740 --> 00:07:55,030
watching my clock here and it's ticking

00:07:52,360 --> 00:07:56,500
away okay so that was the fact that

00:07:55,030 --> 00:07:58,090
things are changing that's one of the

00:07:56,500 --> 00:08:00,070
reasons you might want to use an FPGA

00:07:58,090 --> 00:08:03,099
when you're doing a neural net the other

00:08:00,070 --> 00:08:08,050
one is performance at low latency FPGAs

00:08:03,099 --> 00:08:09,760
do very well let me restate that FPGA is

00:08:08,050 --> 00:08:12,820
implementing implementing neural Nets

00:08:09,760 --> 00:08:15,190
have very low latency and low latency is

00:08:12,820 --> 00:08:17,440
very important in a lot of neural net

00:08:15,190 --> 00:08:19,630
applications for instance let's say

00:08:17,440 --> 00:08:21,509
you're driving a car and you've got a

00:08:19,630 --> 00:08:26,409
bunch of cameras around your car and

00:08:21,509 --> 00:08:27,310
it's doing some obstacle avoidance kind

00:08:26,409 --> 00:08:31,090
of thing and maybe one of those

00:08:27,310 --> 00:08:35,200
obstacles was a pedestrian if that

00:08:31,090 --> 00:08:37,390
system has high latency you may you it

00:08:35,200 --> 00:08:39,640
will struggle it can struggle to

00:08:37,390 --> 00:08:41,289
identify the fact that there is a

00:08:39,640 --> 00:08:44,440
pedestrian there before you actually hit

00:08:41,289 --> 00:08:46,450
the pedestrian so latency is a very key

00:08:44,440 --> 00:08:50,579
concept looking at it differently

00:08:46,450 --> 00:08:53,020
latency is how long it takes from when

00:08:50,579 --> 00:08:56,680
information first enters the system to

00:08:53,020 --> 00:08:59,860
when you get useful information out so

00:08:56,680 --> 00:09:03,160
here's kind of a high-level roadmap

00:08:59,860 --> 00:09:07,360
looking thing that explains why a GPU

00:09:03,160 --> 00:09:11,560
type system has has latency associated

00:09:07,360 --> 00:09:14,709
with it so a GPU is used with a CPU and

00:09:11,560 --> 00:09:19,000
the way the whole thing works is the CPU

00:09:14,709 --> 00:09:20,860
has to pull CPU has got to pull

00:09:19,000 --> 00:09:21,880
instructions out of DRAM it's got to run

00:09:20,860 --> 00:09:23,440
it it's got to get it through the cache

00:09:21,880 --> 00:09:25,570
it's got to execute it's got to put

00:09:23,440 --> 00:09:28,120
together a kernel that it's going to

00:09:25,570 --> 00:09:30,519
then deploy over PCI Express or actually

00:09:28,120 --> 00:09:32,140
depending on whether it's a SOC or in a

00:09:30,519 --> 00:09:35,260
data center it's got to load the

00:09:32,140 --> 00:09:37,209
instruction in DRAM the DRAM the X the

00:09:35,260 --> 00:09:38,950
instruction execute and the instruction

00:09:37,209 --> 00:09:41,079
says all right so let's pull the image

00:09:38,950 --> 00:09:44,110
out of DRAM and load it into all of

00:09:41,079 --> 00:09:46,029
these a I'll use and all of that equals

00:09:44,110 --> 00:09:47,620
and it gotta load a lot of a I'll use to

00:09:46,029 --> 00:09:49,510
get the performance out of it all of

00:09:47,620 --> 00:09:51,070
that equals latency from the time where

00:09:49,510 --> 00:09:53,230
you start the execution all the way to

00:09:51,070 --> 00:09:54,850
the time where it's ready to operate on

00:09:53,230 --> 00:09:58,870
all of the data that's been loaded into

00:09:54,850 --> 00:10:00,190
the a I'll use in the GPU that's a good

00:09:58,870 --> 00:10:02,440
deal of latency and that's just

00:10:00,190 --> 00:10:04,779
something that's fundamental to a GPU

00:10:02,440 --> 00:10:07,959
architecture because if you think about

00:10:04,779 --> 00:10:10,740
GPUs you know their mission in life

00:10:07,959 --> 00:10:13,570
where they really started from was they

00:10:10,740 --> 00:10:15,339
were designed to operate on images which

00:10:13,570 --> 00:10:20,079
is a whole lot of pixels operating in

00:10:15,339 --> 00:10:22,060
parallel whereas you know that's that's

00:10:20,079 --> 00:10:25,959
great for images but it's not really

00:10:22,060 --> 00:10:31,180
great for getting low low latency on the

00:10:25,959 --> 00:10:33,339
FPGA side oh so the way GPUs work around

00:10:31,180 --> 00:10:37,209
that is they do a thing called batching

00:10:33,339 --> 00:10:39,070
so if you go ahead and you execute your

00:10:37,209 --> 00:10:41,380
instruction and you load one image into

00:10:39,070 --> 00:10:43,690
here you can do your part where you load

00:10:41,380 --> 00:10:45,490
the instruction move it over here it

00:10:43,690 --> 00:10:47,680
says hey get me an image it gets the

00:10:45,490 --> 00:10:49,660
image and then chunks it through that

00:10:47,680 --> 00:10:52,600
can happen in some amount of time and

00:10:49,660 --> 00:10:54,670
it's okay but what they really do to try

00:10:52,600 --> 00:10:56,470
and hide the latency is load as many

00:10:54,670 --> 00:10:57,970
images up as possible and they call that

00:10:56,470 --> 00:11:00,040
batching so if you're ever looking at

00:10:57,970 --> 00:11:03,690
specs you might see something that says

00:11:00,040 --> 00:11:07,209
how big the batch size was on the GPU so

00:11:03,690 --> 00:11:09,040
batching is another way of saying hey

00:11:07,209 --> 00:11:11,230
we're trying to hide the latency in this

00:11:09,040 --> 00:11:12,910
batching so we're setting it up to do

00:11:11,230 --> 00:11:13,180
it's kind of like a DMA transfer it's

00:11:12,910 --> 00:11:14,710
like

00:11:13,180 --> 00:11:16,780
well we're gonna transfer a whole lot of

00:11:14,710 --> 00:11:18,040
data and we'll suffer through the

00:11:16,780 --> 00:11:21,520
overhead at the beginning that's kind of

00:11:18,040 --> 00:11:23,110
what batching does on the FPGA side we

00:11:21,520 --> 00:11:25,570
kind of don't have that concept because

00:11:23,110 --> 00:11:28,080
we get that's its programmable hardware

00:11:25,570 --> 00:11:34,780
so we get to set that up however we want

00:11:28,080 --> 00:11:36,520
so what we do is we have the image or

00:11:34,780 --> 00:11:38,740
whatever data it is that you're going to

00:11:36,520 --> 00:11:40,960
be running through your neural net in

00:11:38,740 --> 00:11:44,770
external memory we pull it into memory

00:11:40,960 --> 00:11:46,720
that is local to your Hardware neural

00:11:44,770 --> 00:11:49,120
net accelerator and then we just kind of

00:11:46,720 --> 00:11:51,430
chunk it through so in FPGAs like I say

00:11:49,120 --> 00:11:54,010
we don't have a batching type of a

00:11:51,430 --> 00:11:56,370
problem we just run it through in a

00:11:54,010 --> 00:12:01,810
pipeline fashion so much much lower

00:11:56,370 --> 00:12:03,520
latency in that in that scenario so

00:12:01,810 --> 00:12:06,460
there's another thing we do and I talked

00:12:03,520 --> 00:12:09,010
about this before and that is once you

00:12:06,460 --> 00:12:12,040
have your trained neural net there's a

00:12:09,010 --> 00:12:14,110
lot of ways to make that Network simpler

00:12:12,040 --> 00:12:15,550
and a few of the things that we do are

00:12:14,110 --> 00:12:18,670
something called pruning and compression

00:12:15,550 --> 00:12:21,250
so if you imagine you've trained your

00:12:18,670 --> 00:12:22,480
neural net and it comes out and it looks

00:12:21,250 --> 00:12:23,740
like this it's a bunch of fully

00:12:22,480 --> 00:12:26,290
connected neurons as a bit of a

00:12:23,740 --> 00:12:27,940
simplification but a lot of full a lot

00:12:26,290 --> 00:12:29,520
of fully connected neurons is running

00:12:27,940 --> 00:12:32,050
floating point math and things like that

00:12:29,520 --> 00:12:33,790
what ends up happening when you have

00:12:32,050 --> 00:12:36,280
something like this and you're actually

00:12:33,790 --> 00:12:38,170
doing the the processing that is

00:12:36,280 --> 00:12:39,400
associated with the neural Nets you get

00:12:38,170 --> 00:12:40,900
a lot of matrices that are fully

00:12:39,400 --> 00:12:44,040
populated and there's a lot of work that

00:12:40,900 --> 00:12:46,840
has to go into doing matrix math and

00:12:44,040 --> 00:12:50,110
what we do is we go through and we

00:12:46,840 --> 00:12:50,590
analyze the matrices and the values and

00:12:50,110 --> 00:12:52,090
things like that

00:12:50,590 --> 00:12:55,870
and we do something called pruning we

00:12:52,090 --> 00:12:57,130
find out which neurons aren't really

00:12:55,870 --> 00:12:58,690
doing very much and we have to prune

00:12:57,130 --> 00:13:00,850
them out or change them from floating

00:12:58,690 --> 00:13:04,750
point to fixed point and change the or

00:13:00,850 --> 00:13:08,440
the precision and things like that and

00:13:04,750 --> 00:13:11,680
what you end up getting is a more

00:13:08,440 --> 00:13:14,140
sparsely populated matrix and that's an

00:13:11,680 --> 00:13:17,620
area where FPGAs can really excel as

00:13:14,140 --> 00:13:20,800
well so if we have a partially populated

00:13:17,620 --> 00:13:22,360
matrix there's things that we can do in

00:13:20,800 --> 00:13:24,220
our neural net accelerators that we've

00:13:22,360 --> 00:13:26,800
designed that allow you to get higher

00:13:24,220 --> 00:13:29,830
throughput so in the

00:13:26,800 --> 00:13:31,990
GPU world there are also ways to do

00:13:29,830 --> 00:13:34,830
sparsely-populated matrices but they

00:13:31,990 --> 00:13:37,390
don't actually result very much in

00:13:34,830 --> 00:13:38,529
substantial savings in performance it's

00:13:37,390 --> 00:13:39,670
still kind of going through it it's just

00:13:38,529 --> 00:13:44,170
handling the fact that it's sparsely

00:13:39,670 --> 00:13:46,570
populated so anyway so we figure out how

00:13:44,170 --> 00:13:47,950
to prune it here's what it looks like so

00:13:46,570 --> 00:13:50,050
you'll have things that are less

00:13:47,950 --> 00:13:54,130
connected and that just ends up equaling

00:13:50,050 --> 00:13:55,600
less math so let's see so pruning I

00:13:54,130 --> 00:13:57,370
think I covered all this smaller letter

00:13:55,600 --> 00:13:59,589
networks less capacity but having better

00:13:57,370 --> 00:14:02,500
yeah so yeah FPGAs were better with

00:13:59,589 --> 00:14:04,810
smart matrices single-chip yeah these

00:14:02,500 --> 00:14:07,120
are all the things I said and and we

00:14:04,810 --> 00:14:08,800
have tools that do this stuff for you so

00:14:07,120 --> 00:14:10,300
once you have your trained neural net

00:14:08,800 --> 00:14:13,690
you run it through our tools and it

00:14:10,300 --> 00:14:17,019
takes care that's not so another thing

00:14:13,690 --> 00:14:20,050
that we like to talk about in the FPGA

00:14:17,019 --> 00:14:23,110
world is benchmarks and things like that

00:14:20,050 --> 00:14:24,970
because if you look at published

00:14:23,110 --> 00:14:28,690
benchmarks and just look at the numbers

00:14:24,970 --> 00:14:32,800
on them on their own a lot of times

00:14:28,690 --> 00:14:34,570
it'll look like they like a GPU might

00:14:32,800 --> 00:14:36,310
have better performance than an FPGA

00:14:34,570 --> 00:14:38,680
but you kind of have to look at it in

00:14:36,310 --> 00:14:41,170
the context of how it's used a lot of

00:14:38,680 --> 00:14:43,209
the benchmarks that are out there are

00:14:41,170 --> 00:14:46,000
just looking at alright well we've got

00:14:43,209 --> 00:14:47,350
these cores in our GPU and each one is

00:14:46,000 --> 00:14:48,520
capable of a thing and we're just going

00:14:47,350 --> 00:14:50,380
to aggregate and we're going to come up

00:14:48,520 --> 00:14:51,610
with a number and that doesn't always

00:14:50,380 --> 00:14:52,510
tell the story because architecture

00:14:51,610 --> 00:14:55,540
really different things might be

00:14:52,510 --> 00:14:57,310
happening it there's there you can run

00:14:55,540 --> 00:15:01,690
into things with hell easily or hard

00:14:57,310 --> 00:15:03,760
data can be pulled through the GPU so if

00:15:01,690 --> 00:15:06,880
you look at this little thing here so

00:15:03,760 --> 00:15:10,089
you 200 and you 250 those are Xilinx

00:15:06,880 --> 00:15:14,260
boards actually and they have quoted

00:15:10,089 --> 00:15:17,140
numbers of 18 and 33 tera ops whereas

00:15:14,260 --> 00:15:19,540
the t4p 40 and p4 those are GPUs and

00:15:17,140 --> 00:15:22,180
they have quoted terry ops performance

00:15:19,540 --> 00:15:24,160
numbers much higher okay but if you

00:15:22,180 --> 00:15:25,930
actually run it through and I like this

00:15:24,160 --> 00:15:27,850
it says we've put it in focus and then

00:15:25,930 --> 00:15:29,949
we actually take it out of focus so I

00:15:27,850 --> 00:15:30,430
think that's misleading but I apologize

00:15:29,949 --> 00:15:33,220
for that

00:15:30,430 --> 00:15:35,500
so what happens like I said is these

00:15:33,220 --> 00:15:36,970
useable tops get limited by various

00:15:35,500 --> 00:15:38,829
conditions you know memory bottlenecks

00:15:36,970 --> 00:15:41,320
how the code is structured whether

00:15:38,829 --> 00:15:43,180
something else to stall or brand

00:15:41,320 --> 00:15:44,740
and then GPUs even do this thing called

00:15:43,180 --> 00:15:46,210
frequency throttling which is this

00:15:44,740 --> 00:15:47,890
fascinating thing where if the power

00:15:46,210 --> 00:15:49,480
gets too high it just kind of drops the

00:15:47,890 --> 00:15:51,870
clock down and you don't always even

00:15:49,480 --> 00:15:51,870
know about it

00:15:51,940 --> 00:15:55,720
so in addition to that other things

00:15:54,040 --> 00:15:58,750
affected like batch size like we talked

00:15:55,720 --> 00:16:00,430
about before whether there's a latency

00:15:58,750 --> 00:16:02,200
spec and the power which again goes back

00:16:00,430 --> 00:16:04,690
to this frequency throttling thing and

00:16:02,200 --> 00:16:05,950
different accuracy requirements so cut

00:16:04,690 --> 00:16:08,560
to the chase

00:16:05,950 --> 00:16:10,990
here's some real benchmarks this is a

00:16:08,560 --> 00:16:13,000
missed example it's Google net batch

00:16:10,990 --> 00:16:16,840
equals one because again we want to have

00:16:13,000 --> 00:16:19,630
the lowest latency solution and really

00:16:16,840 --> 00:16:21,700
the p4 and the p40 and uh this is kind

00:16:19,630 --> 00:16:24,910
of an estimate at this point but the p4

00:16:21,700 --> 00:16:26,740
and the p40 have a much lower throughput

00:16:24,910 --> 00:16:31,090
images per second again for the same

00:16:26,740 --> 00:16:33,340
batch size equals one whereas our u 200

00:16:31,090 --> 00:16:36,970
and u 250 zhiling solutions considerably

00:16:33,340 --> 00:16:39,780
higher so so the message here is and and

00:16:36,970 --> 00:16:42,160
this message actually flies in like all

00:16:39,780 --> 00:16:44,050
FPGA applications really have to look at

00:16:42,160 --> 00:16:46,690
the system and how its implemented in

00:16:44,050 --> 00:16:48,880
things like that so you know focus on

00:16:46,690 --> 00:16:52,620
the application level performance and

00:16:48,880 --> 00:16:52,620
that's where we typically do a good job

00:16:53,460 --> 00:16:59,410
here's another example where we have a

00:16:56,560 --> 00:17:00,760
speech to text conversion the neural net

00:16:59,410 --> 00:17:03,160
looks like this we got to see it you

00:17:00,760 --> 00:17:05,260
know we have these layers here each one

00:17:03,160 --> 00:17:07,960
of these layers got implemented and run

00:17:05,260 --> 00:17:10,750
through our tools run through our tools

00:17:07,960 --> 00:17:14,050
so we were able to do pruning as needed

00:17:10,750 --> 00:17:16,540
and things like that and if you ran this

00:17:14,050 --> 00:17:19,480
algorithm just in a CPU only pretty sure

00:17:16,540 --> 00:17:21,760
it was a Xeon it takes about that long

00:17:19,480 --> 00:17:24,940
CPU plus a GPU it drops down as you

00:17:21,760 --> 00:17:29,020
would expect but CPU again with this new

00:17:24,940 --> 00:17:33,040
200 board drops down quite a bit more so

00:17:29,020 --> 00:17:35,170
you can see an FPGA solution is about

00:17:33,040 --> 00:17:39,730
twice as fast in this particular neural

00:17:35,170 --> 00:17:41,800
net about twice as fast as what what a

00:17:39,730 --> 00:17:46,330
GPU is able to do and again batch equals

00:17:41,800 --> 00:17:48,340
one so that's a key thing for us Wow

00:17:46,330 --> 00:17:49,360
kind of ripping through this any

00:17:48,340 --> 00:17:53,700
questions yeah I was doing all right

00:17:49,360 --> 00:17:53,700
yeah sorry yeah wait wait wait wait

00:17:55,380 --> 00:18:00,490
when I last slide you were showing the

00:17:58,510 --> 00:18:02,230
time difference in it uh-huh

00:18:00,490 --> 00:18:03,760
but another thing that we interesting

00:18:02,230 --> 00:18:06,820
his show would be the power utilization

00:18:03,760 --> 00:18:09,940
perhaps because I mean it could be fully

00:18:06,820 --> 00:18:10,540
two milliseconds and it could be a lot

00:18:09,940 --> 00:18:13,660
of power

00:18:10,540 --> 00:18:15,370
yeah GPU that's and I don't know the

00:18:13,660 --> 00:18:16,990
answer for these these particulars it

00:18:15,370 --> 00:18:18,400
but you're exactly right that's just a

00:18:16,990 --> 00:18:29,380
suggestion for next week thank you guys

00:18:18,400 --> 00:18:36,010
thank you absolutely was there another

00:18:29,380 --> 00:18:40,060
question okay so another place where

00:18:36,010 --> 00:18:41,260
FPGAs do well is what what we call I was

00:18:40,060 --> 00:18:43,500
gonna say it's a marketing term but I'm

00:18:41,260 --> 00:18:45,640
a marketing guy so I shouldn't really

00:18:43,500 --> 00:18:48,340
badmouth myself um

00:18:45,640 --> 00:18:51,400
whole app acceleration really the the

00:18:48,340 --> 00:18:54,550
point there is the fact that the FPGA

00:18:51,400 --> 00:18:56,590
isn't just doing the neural net FPGAs

00:18:54,550 --> 00:18:58,120
are programmable hardware so let's say

00:18:56,590 --> 00:18:59,440
you've got some weird sensor that

00:18:58,120 --> 00:19:02,980
somebody designed that has a

00:18:59,440 --> 00:19:05,710
non-standard interface the FPGA can be

00:19:02,980 --> 00:19:08,440
modified to communicate with that sensor

00:19:05,710 --> 00:19:12,670
in a fairly straightforward fashion if

00:19:08,440 --> 00:19:14,800
you're an FPGA guy if you have you know

00:19:12,670 --> 00:19:16,420
it's an SOC so it's got five serial

00:19:14,800 --> 00:19:19,240
ports but you need twenty five for some

00:19:16,420 --> 00:19:21,700
reason an FPGA can very easily implement

00:19:19,240 --> 00:19:24,270
the rest of those serial ports so it's

00:19:21,700 --> 00:19:27,850
programmable hardware so the point is

00:19:24,270 --> 00:19:30,490
FPGAs do a great job of incorporating

00:19:27,850 --> 00:19:31,990
your whole application not just the

00:19:30,490 --> 00:19:33,640
neural net like we're talking about so

00:19:31,990 --> 00:19:35,350
you know always kind of think about that

00:19:33,640 --> 00:19:37,780
and that's really been our claim to fame

00:19:35,350 --> 00:19:39,070
we sometimes we call over the years I've

00:19:37,780 --> 00:19:41,500
no design makes a long time sometimes

00:19:39,070 --> 00:19:44,080
it's logic consolidation sometimes it's

00:19:41,500 --> 00:19:46,360
glue logic but the point is kind of any

00:19:44,080 --> 00:19:49,450
piece of hardware with with some

00:19:46,360 --> 00:19:51,130
exceptions can be integrated into the

00:19:49,450 --> 00:19:53,710
FPGA and you know it becomes a single

00:19:51,130 --> 00:19:55,720
chip solution so let's look at some

00:19:53,710 --> 00:19:58,390
examples of whole app acceleration in

00:19:55,720 --> 00:20:01,860
the context of a neural net type thing

00:19:58,390 --> 00:20:05,980
so this was a company called YY Inc and

00:20:01,860 --> 00:20:07,270
they had a product that would read in

00:20:05,980 --> 00:20:10,450
video do

00:20:07,270 --> 00:20:11,470
some analytics to it and they do some

00:20:10,450 --> 00:20:14,140
post-processing

00:20:11,470 --> 00:20:15,610
to that analytics and then encoded and

00:20:14,140 --> 00:20:17,350
you want to take you get anybody want to

00:20:15,610 --> 00:20:24,820
take a guess of what the real

00:20:17,350 --> 00:20:27,730
application of this was um okay so I

00:20:24,820 --> 00:20:30,130
think and I should know this but I think

00:20:27,730 --> 00:20:32,290
it's one of those deals where somebody

00:20:30,130 --> 00:20:34,150
uploads a video and it analyzes and it

00:20:32,290 --> 00:20:36,100
figures out where the face is and like

00:20:34,150 --> 00:20:38,560
puts cat ears on it or something like

00:20:36,100 --> 00:20:40,300
that I mean there's a lot of that that's

00:20:38,560 --> 00:20:41,710
an incredibly booming business right now

00:20:40,300 --> 00:20:45,100
I think that's kind of what's going on

00:20:41,710 --> 00:20:46,690
here anyway this company was growing

00:20:45,100 --> 00:20:52,210
quite a bit so they were able to do this

00:20:46,690 --> 00:20:54,160
capability and units of these servers so

00:20:52,210 --> 00:20:56,260
they were doing this with 30 of these

00:20:54,160 --> 00:20:59,320
five servers and these were all you know

00:20:56,260 --> 00:21:01,360
racks size 19-inch racks and they were

00:20:59,320 --> 00:21:02,410
trying to scale that and what was

00:21:01,360 --> 00:21:03,880
happening is they were blowing out of

00:21:02,410 --> 00:21:06,250
the power envelope for the particular

00:21:03,880 --> 00:21:08,440
you know for their facility so they came

00:21:06,250 --> 00:21:12,670
to as I links to find out if there is a

00:21:08,440 --> 00:21:14,320
way to get that functionality in a

00:21:12,670 --> 00:21:16,480
smaller envelope so they could so they

00:21:14,320 --> 00:21:18,910
could scale better so we threw some

00:21:16,480 --> 00:21:22,210
engineering at it and they ended up

00:21:18,910 --> 00:21:25,620
getting all of this capability down into

00:21:22,210 --> 00:21:29,770
a single rack unit so 48 of these uh

00:21:25,620 --> 00:21:32,080
Xilinx devices replacing that and the

00:21:29,770 --> 00:21:35,500
way it ended up coming out was a 10x

00:21:32,080 --> 00:21:39,510
energy improvement and 3.3 X performance

00:21:35,500 --> 00:21:41,830
improvement so better smaller faster I

00:21:39,510 --> 00:21:42,940
can't speak to cheaper or not but

00:21:41,830 --> 00:21:46,720
definitely better smaller and faster

00:21:42,940 --> 00:21:48,760
lower power so this is an example of a

00:21:46,720 --> 00:21:56,680
whole app acceleration that includes

00:21:48,760 --> 00:22:00,430
a neural net type of a future in it all

00:21:56,680 --> 00:22:03,480
right so there you go so we talked about

00:22:00,430 --> 00:22:06,520
why FPGA is why because you can

00:22:03,480 --> 00:22:09,940
integrate logic you can get a single

00:22:06,520 --> 00:22:11,800
bomb solution you can do things better

00:22:09,940 --> 00:22:15,790
at low latency and you can do your whole

00:22:11,800 --> 00:22:17,920
app acceleration and that's what FPGAs

00:22:15,790 --> 00:22:20,610
are good at is there any other questions

00:22:17,920 --> 00:22:20,610
or comments

00:22:24,830 --> 00:22:30,570
you talked about performance latency

00:22:28,440 --> 00:22:36,030
power performance what about price

00:22:30,570 --> 00:22:38,430
performance compared to GPRS I'm not

00:22:36,030 --> 00:22:42,000
gonna lie I'm not really prepared to

00:22:38,430 --> 00:22:46,440
talk about price I don't even always pay

00:22:42,000 --> 00:22:47,880
attention to that any other xylenes

00:22:46,440 --> 00:22:49,910
people in the room when I want to hit

00:22:47,880 --> 00:22:49,910
that

00:22:58,240 --> 00:23:05,149
yeah so sometimes it's cheaper just

00:23:02,480 --> 00:23:07,760
because of you know some people are more

00:23:05,149 --> 00:23:12,159
thinking about the capex versus the OPEC

00:23:07,760 --> 00:23:14,779
right when you have that kind of power

00:23:12,159 --> 00:23:17,720
savings than though you can save it on

00:23:14,779 --> 00:23:20,659
the OPEX but even the capex in in many

00:23:17,720 --> 00:23:22,309
places the cost in it should cost to

00:23:20,659 --> 00:23:25,279
report and so on but it really depends

00:23:22,309 --> 00:23:28,610
on the application GPUs are good in in

00:23:25,279 --> 00:23:31,070
some applications and and for other

00:23:28,610 --> 00:23:33,740
neural networks it's a big difference

00:23:31,070 --> 00:23:36,010
and fadh2 much much better so so it

00:23:33,740 --> 00:23:39,019
really depends on the application and

00:23:36,010 --> 00:23:42,169
and just to sort of build on that you

00:23:39,019 --> 00:23:45,350
know one of the advantages that FPGA is

00:23:42,169 --> 00:23:48,500
bring to the party versus a GPU is this

00:23:45,350 --> 00:23:50,419
concept of scalability FPGAs we have

00:23:48,500 --> 00:23:52,519
little guys that are like 20 bucks or 30

00:23:50,419 --> 00:23:56,139
bucks and we have big guys that are tens

00:23:52,519 --> 00:23:58,940
of thousands of dollars in the GPU world

00:23:56,139 --> 00:24:00,200
there's there's fewer price points

00:23:58,940 --> 00:24:02,750
because there's fewer devices I mean

00:24:00,200 --> 00:24:05,330
there's a fewer die basically for any

00:24:02,750 --> 00:24:08,419
one of our FPGA families and like today

00:24:05,330 --> 00:24:10,850
for instance there's probably nine or

00:24:08,419 --> 00:24:12,559
ten different families that you could

00:24:10,850 --> 00:24:14,899
design from each one of them probably

00:24:12,559 --> 00:24:18,200
has ten different devices in it so

00:24:14,899 --> 00:24:20,720
there's a pretty broad array of choices

00:24:18,200 --> 00:24:22,279
you can make of what fpga are using so

00:24:20,720 --> 00:24:24,260
you can hit a price point there's a

00:24:22,279 --> 00:24:25,580
perhaps a better chance that you'll be

00:24:24,260 --> 00:24:29,840
able to hit a price point that better

00:24:25,580 --> 00:24:33,279
matches your budget just because we have

00:24:29,840 --> 00:24:38,210
a range of devices if that makes sense

00:24:33,279 --> 00:24:42,019
uh-huh anybody else is we're at twenty

00:24:38,210 --> 00:24:44,889
four minutes and 40 seconds awesome

00:24:42,019 --> 00:24:44,889
thank you very much

00:24:45,970 --> 00:24:49,380

YouTube URL: https://www.youtube.com/watch?v=QRec8Hz6chY


