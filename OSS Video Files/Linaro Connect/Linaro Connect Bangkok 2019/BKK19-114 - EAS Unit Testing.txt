Title: BKK19-114 - EAS Unit Testing
Publication date: 2019-04-03
Playlist: Linaro Connect Bangkok 2019
Description: 
	For more information about Linaro Connect please visit: https://connect.linaro.org
Full session details and the presentation can be found at https://connect.linaro.org/resources/bkk19/bkk19-114/

Abstract
There is a lack of unit tests available for the scheduler, energy aware scheduling, and CPU frequency management. In this session a recent effort to expand the available tests will be described and discussed.

Steve Muckle
Steve Muckle works on Android kernel compliance testing and energy aware scheduling at Google. He formerly worked on energy aware scheduling at Linaro and Qualcomm.
Captions: 
	00:00:05,430 --> 00:00:10,320
okay so my name is Steve Michael I work

00:00:07,560 --> 00:00:12,059
at Google on Colonel compliance testing

00:00:10,320 --> 00:00:16,230
mostly but I dabble in scheduler stuff

00:00:12,059 --> 00:00:18,510
from my work in a past life so I decided

00:00:16,230 --> 00:00:21,119
to do some work on EAS unit testing it's

00:00:18,510 --> 00:00:22,740
an area that I see kind of as being

00:00:21,119 --> 00:00:24,390
underserved so I'm gonna talk a little

00:00:22,740 --> 00:00:26,400
bit about it some of that work I've done

00:00:24,390 --> 00:00:28,380
in the last six months or so so

00:00:26,400 --> 00:00:30,679
initially I'll just sort of give some

00:00:28,380 --> 00:00:33,510
motivation for that reasons why I think

00:00:30,679 --> 00:00:35,550
this is a problem I'll talk about the

00:00:33,510 --> 00:00:38,400
resources I see out existing tests in

00:00:35,550 --> 00:00:43,800
test Suites from a survey I did and then

00:00:38,400 --> 00:00:45,390
the work that I've done so thinking back

00:00:43,800 --> 00:00:47,640
to at least sort of the beginnings of

00:00:45,390 --> 00:00:49,560
Android CPU freq is a really common

00:00:47,640 --> 00:00:51,090
source of customization so I've seen

00:00:49,560 --> 00:00:53,490
that to say as sort of as long as I've

00:00:51,090 --> 00:00:55,710
been involved in Android folks either

00:00:53,490 --> 00:00:58,470
creating their own governor or modifying

00:00:55,710 --> 00:01:01,830
existing CPU free governor's the

00:00:58,470 --> 00:01:03,840
scheduler has become so especially with

00:01:01,830 --> 00:01:06,360
big.little it's really become a hotbed

00:01:03,840 --> 00:01:10,370
of customization you know google has an

00:01:06,360 --> 00:01:12,600
entirely custom load tracking system but

00:01:10,370 --> 00:01:14,700
vendors typically make all sorts of

00:01:12,600 --> 00:01:17,130
hacks throughout the scheduler either

00:01:14,700 --> 00:01:18,930
because of specific to their topology

00:01:17,130 --> 00:01:21,240
you know hardware specific things or

00:01:18,930 --> 00:01:23,670
just performance and power optimizations

00:01:21,240 --> 00:01:25,650
that they find they want to do and I

00:01:23,670 --> 00:01:29,010
don't so you know in as part of Google

00:01:25,650 --> 00:01:30,659
we would like to see these converge into

00:01:29,010 --> 00:01:32,520
sort of a single open source solution

00:01:30,659 --> 00:01:36,240
but I don't know whether how possible or

00:01:32,520 --> 00:01:37,950
feasible that that is and with dynamic I

00:01:36,240 --> 00:01:39,540
don't know if that's also going to make

00:01:37,950 --> 00:01:41,070
it harder if we're gonna see more sort

00:01:39,540 --> 00:01:46,890
of you know crazy custom hardware

00:01:41,070 --> 00:01:49,380
topologies so when people when vendors

00:01:46,890 --> 00:01:51,030
make modifications in these areas in my

00:01:49,380 --> 00:01:53,640
experience they typically focus on

00:01:51,030 --> 00:01:55,200
high-level testing so in the three major

00:01:53,640 --> 00:01:57,240
areas that I've listed here perf power

00:01:55,200 --> 00:01:59,100
and UX usually they're running you know

00:01:57,240 --> 00:02:01,020
for power testing they'll run a days of

00:01:59,100 --> 00:02:02,580
use type you know some mixture of use of

00:02:01,020 --> 00:02:05,130
high-level use cases and do power

00:02:02,580 --> 00:02:07,670
consumption for performance there's you

00:02:05,130 --> 00:02:10,409
know benchmarks like Geekbench or antutu

00:02:07,670 --> 00:02:13,620
and then you Excel they'll collect jank

00:02:10,409 --> 00:02:15,659
measurements or other high level numbers

00:02:13,620 --> 00:02:17,790
so what's missing here and my opinion

00:02:15,659 --> 00:02:19,709
are our low-level unit tests for the

00:02:17,790 --> 00:02:22,349
scheduler and for CPU freak

00:02:19,709 --> 00:02:23,670
so I'll give some examples here of so

00:02:22,349 --> 00:02:27,030
these are some issues that we've

00:02:23,670 --> 00:02:28,439
observed at Google actual bugs that we

00:02:27,030 --> 00:02:30,420
found and these are the sorts of things

00:02:28,439 --> 00:02:33,329
that I would like to see validated by

00:02:30,420 --> 00:02:35,159
unit testing I don't know that we really

00:02:33,329 --> 00:02:37,620
need to go in and discuss each one of

00:02:35,159 --> 00:02:39,420
these things in detail but you know

00:02:37,620 --> 00:02:42,810
governor responsiveness is kind of an

00:02:39,420 --> 00:02:45,000
obvious thing the wake-ups caused by

00:02:42,810 --> 00:02:47,370
schedule so if your CPU free governor is

00:02:45,000 --> 00:02:48,840
constantly threads in that governor

00:02:47,370 --> 00:02:52,019
constantly waking up this is obviously

00:02:48,840 --> 00:02:54,090
going to be a power problem the way that

00:02:52,019 --> 00:02:56,370
schedu till gov is is implemented so

00:02:54,090 --> 00:02:58,590
it's collecting CP utilization values

00:02:56,370 --> 00:03:00,959
from all the different CPUs however

00:02:58,590 --> 00:03:02,790
every CPU is not always updating those

00:03:00,959 --> 00:03:05,879
values so if a CPU goes and to seep some

00:03:02,790 --> 00:03:07,290
you know deep idle state it's its last

00:03:05,879 --> 00:03:08,790
utilization value is going to become

00:03:07,290 --> 00:03:09,870
stale and so there will become a point

00:03:08,790 --> 00:03:12,599
where you don't want to consider that

00:03:09,870 --> 00:03:15,239
anymore in your calculations there was

00:03:12,599 --> 00:03:17,819
an issue where the governor was not sort

00:03:15,239 --> 00:03:20,639
of ignoring stale values that obviously

00:03:17,819 --> 00:03:22,200
causes problems the wake up path in the

00:03:20,639 --> 00:03:23,790
scheduler so especially was big little

00:03:22,200 --> 00:03:25,590
now the wake up path is getting very

00:03:23,790 --> 00:03:28,620
complicated there's all sorts of knobs

00:03:25,590 --> 00:03:32,190
and it's very sensitive to things like

00:03:28,620 --> 00:03:33,959
what CPU you start the search for for

00:03:32,190 --> 00:03:35,579
the the best CPU for a particular task

00:03:33,959 --> 00:03:37,319
that's waking up where do you start that

00:03:35,579 --> 00:03:39,299
search in the CPUs that are available

00:03:37,319 --> 00:03:42,750
and so we saw like an issue I think

00:03:39,299 --> 00:03:44,129
where the start CPU was wrong and so you

00:03:42,750 --> 00:03:45,689
know this this is important for a hot

00:03:44,129 --> 00:03:47,459
path like that it's important to start

00:03:45,689 --> 00:03:50,099
at the right place both so that you

00:03:47,459 --> 00:03:52,169
Karev at the correct answer but also the

00:03:50,099 --> 00:03:55,349
latency for for that calculation to be

00:03:52,169 --> 00:03:57,810
done the sync flag we realize sync flag

00:03:55,349 --> 00:03:59,879
is very important in Android so look for

00:03:57,810 --> 00:04:01,439
binder this is a flag that you set like

00:03:59,879 --> 00:04:03,599
when a task is waking up if the same

00:04:01,439 --> 00:04:05,069
flag is set then the scheduler knows

00:04:03,599 --> 00:04:07,319
that that task is going to be going to

00:04:05,069 --> 00:04:09,720
sleep soon and so that the the waking

00:04:07,319 --> 00:04:12,120
task could be put on the same CPU it's

00:04:09,720 --> 00:04:13,349
an important performance enhancement and

00:04:12,120 --> 00:04:16,859
so this is something that really should

00:04:13,349 --> 00:04:18,090
be tested at least there should be a

00:04:16,859 --> 00:04:19,709
test available whether it can be a

00:04:18,090 --> 00:04:22,500
compliance thing or not it we have to

00:04:19,709 --> 00:04:24,300
figure that out but right now there is

00:04:22,500 --> 00:04:25,889
no verification of that and so it's just

00:04:24,300 --> 00:04:27,090
oh you know we're getting bad

00:04:25,889 --> 00:04:28,529
performance you know some vendor

00:04:27,090 --> 00:04:30,090
complains about their bad performance

00:04:28,529 --> 00:04:31,360
and you know a bunch of debugging later

00:04:30,090 --> 00:04:33,370
we find that

00:04:31,360 --> 00:04:34,509
they a change something in the scheduler

00:04:33,370 --> 00:04:37,840
and their sync flag is not behaving

00:04:34,509 --> 00:04:40,360
properly you over utilized flag is a

00:04:37,840 --> 00:04:42,099
part of EAS implementation that that was

00:04:40,360 --> 00:04:44,020
a more minor thing but but we found an

00:04:42,099 --> 00:04:46,150
issue with that and then again this is

00:04:44,020 --> 00:04:49,599
this is sort of low level scheduler

00:04:46,150 --> 00:04:51,580
details but the bias to the previous EP

00:04:49,599 --> 00:04:56,229
you of a task when it wakes up we found

00:04:51,580 --> 00:04:57,430
issues with that being incorrect so what

00:04:56,229 --> 00:04:59,349
are the existing resources out there

00:04:57,430 --> 00:05:00,639
that can help to verify these things so

00:04:59,349 --> 00:05:04,719
I looked at a few different test Suites

00:05:00,639 --> 00:05:08,169
so RT tests get this comes from the you

00:05:04,719 --> 00:05:09,849
know real-time Linux project most of

00:05:08,169 --> 00:05:12,039
these things are fairly high level in

00:05:09,849 --> 00:05:13,719
generic they're not you know deep sort

00:05:12,039 --> 00:05:16,270
of specific enough to target the kinds

00:05:13,719 --> 00:05:18,550
of issues that I was concerned about so

00:05:16,270 --> 00:05:20,080
you know like hack bench is just that's

00:05:18,550 --> 00:05:24,039
not even really a test it's just sort of

00:05:20,080 --> 00:05:26,229
a general stress generator yeah so these

00:05:24,039 --> 00:05:27,550
are not really getting at these so

00:05:26,229 --> 00:05:29,650
you've got some things concerned with

00:05:27,550 --> 00:05:31,750
signal latency or testing the latency of

00:05:29,650 --> 00:05:33,250
other scheduler primitives I mean

00:05:31,750 --> 00:05:34,870
they're useful and I do think that we

00:05:33,250 --> 00:05:37,270
should probably be seeing more vendors

00:05:34,870 --> 00:05:39,009
running these things but they don't

00:05:37,270 --> 00:05:41,460
completely fill the void that I was

00:05:39,009 --> 00:05:45,930
concerned about

00:05:41,460 --> 00:05:48,370
so LTP is kind of a similar thing it's

00:05:45,930 --> 00:05:51,550
probably more broad in terms of the

00:05:48,370 --> 00:05:53,669
things that it covers again so in at the

00:05:51,550 --> 00:05:56,529
top it has some things concerned with

00:05:53,669 --> 00:05:58,990
testing sort of high-level scheduler

00:05:56,529 --> 00:06:00,940
primitives there's a few things that

00:05:58,990 --> 00:06:03,129
verify specific bug fixes that were

00:06:00,940 --> 00:06:05,800
created so you know unit tests for

00:06:03,129 --> 00:06:08,860
particular bugs more general stress

00:06:05,800 --> 00:06:10,089
tests that don't actually return sort of

00:06:08,860 --> 00:06:11,229
a results you know this is just

00:06:10,089 --> 00:06:15,129
something you run and if things don't

00:06:11,229 --> 00:06:17,740
die then then you passed the biggest

00:06:15,129 --> 00:06:19,150
thing in LTP is this called testing so

00:06:17,740 --> 00:06:22,659
there's good coverage for all the

00:06:19,150 --> 00:06:24,039
syscalls so that's there and then the

00:06:22,659 --> 00:06:26,440
power management test those are very

00:06:24,039 --> 00:06:29,349
those are very broad so the basic CPU

00:06:26,440 --> 00:06:31,270
freak test that does things like just

00:06:29,349 --> 00:06:33,550
trying to go to every frequency that it

00:06:31,270 --> 00:06:36,009
finds in Sisyphus try to go to every

00:06:33,550 --> 00:06:39,190
governor confirm this and Sisyphus nodes

00:06:36,009 --> 00:06:41,709
are present so again not really the

00:06:39,190 --> 00:06:43,690
depth that I'm concerned about

00:06:41,709 --> 00:06:44,889
and then Kay self-test is pretty minimal

00:06:43,690 --> 00:06:46,360
I mean if

00:06:44,889 --> 00:06:47,379
whoever's familiar with K self-test it

00:06:46,360 --> 00:06:49,389
you know it's really just sort of a

00:06:47,379 --> 00:06:51,969
smattering of coverage across some

00:06:49,389 --> 00:06:53,800
random areas of the kernel there really

00:06:51,969 --> 00:06:56,139
is nothing for the scheduler itself it

00:06:53,800 --> 00:06:59,229
has some stuff for CPU freak but it's

00:06:56,139 --> 00:07:01,449
it's also fairly minimal again going to

00:06:59,229 --> 00:07:03,520
different governor's I think it's you

00:07:01,449 --> 00:07:06,520
know really designed to test the locking

00:07:03,520 --> 00:07:08,199
and the CP freak framework rather than

00:07:06,520 --> 00:07:12,729
going into detail of like a specific

00:07:08,199 --> 00:07:14,620
governor like schedu till so getting to

00:07:12,729 --> 00:07:15,520
oh and then sorry this last but

00:07:14,620 --> 00:07:17,590
certainly not least

00:07:15,520 --> 00:07:19,870
so Lisa so this is something that Chris

00:07:17,590 --> 00:07:22,479
could probably just talk in more detail

00:07:19,870 --> 00:07:23,770
about but this actually does I mean this

00:07:22,479 --> 00:07:25,990
goes to great lengths this is like

00:07:23,770 --> 00:07:27,699
purpose-built for exactly what I'm

00:07:25,990 --> 00:07:29,650
concerned about here so I mean it has

00:07:27,699 --> 00:07:31,449
great test for es placement load

00:07:29,650 --> 00:07:34,029
tracking so it's developing and

00:07:31,449 --> 00:07:36,819
maintained by arm really for this exact

00:07:34,029 --> 00:07:39,729
sort of thing lots of powerful analysis

00:07:36,819 --> 00:07:41,680
tools my personal issue with it

00:07:39,729 --> 00:07:43,210
and this is certainly a something for

00:07:41,680 --> 00:07:45,129
debate and would be a great topic for

00:07:43,210 --> 00:07:47,349
discussion here is well I guess whether

00:07:45,129 --> 00:07:50,110
it's too heavy weight to really fill the

00:07:47,349 --> 00:07:53,349
need that I see for sort of quick tests

00:07:50,110 --> 00:07:58,539
that can be run in a test suite that has

00:07:53,349 --> 00:08:01,240
broader exposure like LTP it's it's

00:07:58,539 --> 00:08:03,520
fairly large it's got multiple projects

00:08:01,240 --> 00:08:06,279
to to run the whole thing it's got an

00:08:03,520 --> 00:08:08,289
integrated test runner it's it's heavily

00:08:06,279 --> 00:08:10,360
reliant on Python so if you're you know

00:08:08,289 --> 00:08:12,789
like I'm not a real Python master I'm

00:08:10,360 --> 00:08:15,219
much more comfortable with C so that

00:08:12,789 --> 00:08:18,699
limits my ability to to use a lot of

00:08:15,219 --> 00:08:20,169
lisa's functionality and it has its own

00:08:18,699 --> 00:08:22,659
shell which is not really here there but

00:08:20,169 --> 00:08:26,680
it just kind of a you know comment on

00:08:22,659 --> 00:08:27,969
its size and complexity so but but this

00:08:26,680 --> 00:08:31,060
is kind of I think this is the big thing

00:08:27,969 --> 00:08:32,680
that if we are to create new tests this

00:08:31,060 --> 00:08:35,709
is sort of the gold standard I think in

00:08:32,680 --> 00:08:37,390
terms of for one thing making sure that

00:08:35,709 --> 00:08:39,699
arm is comfortable with with what we're

00:08:37,390 --> 00:08:42,579
creating and and reaching a parity of

00:08:39,699 --> 00:08:45,250
features and you know we want to make

00:08:42,579 --> 00:08:47,110
sure that tests give the same results in

00:08:45,250 --> 00:08:48,550
the case that we have tests that mirror

00:08:47,110 --> 00:08:52,899
what's and Lisa they obviously need to

00:08:48,550 --> 00:08:55,390
give the same result so what I've done

00:08:52,899 --> 00:08:57,790
is created a small initial set of C

00:08:55,390 --> 00:08:58,610
based tests my thinking was that these

00:08:57,790 --> 00:09:00,740
would go into

00:08:58,610 --> 00:09:04,579
TP so I've written them to be compliant

00:09:00,740 --> 00:09:07,370
with LTPS design standards it functions

00:09:04,579 --> 00:09:09,019
by enabling tracing and then parsing the

00:09:07,370 --> 00:09:11,000
trace so in order to parse the trace

00:09:09,019 --> 00:09:13,820
I've written my own trace parsing

00:09:11,000 --> 00:09:16,820
library which is kind of gross in the

00:09:13,820 --> 00:09:20,300
future I'd like to try and utilize so

00:09:16,820 --> 00:09:21,829
I'm aware of an effort with trace

00:09:20,300 --> 00:09:23,810
command to turn

00:09:21,829 --> 00:09:25,190
I think it's called Lib Lib trace I

00:09:23,810 --> 00:09:26,959
forget I think it's in my future work

00:09:25,190 --> 00:09:28,790
slide but they're trying to make a

00:09:26,959 --> 00:09:31,700
proper library out of that and so it

00:09:28,790 --> 00:09:33,740
would be nice to use that instead so

00:09:31,700 --> 00:09:36,410
this initial set of tests that I'm about

00:09:33,740 --> 00:09:38,660
to describe is merged into a osp's Fork

00:09:36,410 --> 00:09:41,480
of LTP so you can go and check it out

00:09:38,660 --> 00:09:44,510
the link to the commit is there and you

00:09:41,480 --> 00:09:46,519
can have a look at it and try it so the

00:09:44,510 --> 00:09:49,130
tests that are currently implemented so

00:09:46,519 --> 00:09:51,019
I just gratuitous gratuitously grabbed

00:09:49,130 --> 00:09:53,930
what was in Lisa for these and just

00:09:51,019 --> 00:09:58,420
wrote my own version of it in C so the

00:09:53,930 --> 00:10:01,700
EAS task plate placement tests are there

00:09:58,420 --> 00:10:03,290
they're all pretty self-explanatory and

00:10:01,700 --> 00:10:05,570
I'll talk more I guess about the

00:10:03,290 --> 00:10:07,279
functionality so I have some results for

00:10:05,570 --> 00:10:12,380
a couple of targets that I'll review in

00:10:07,279 --> 00:10:14,209
a moment so for schedule I have a test

00:10:12,380 --> 00:10:16,399
that plays with the boost values and

00:10:14,209 --> 00:10:18,170
then we can look at the utilization so

00:10:16,399 --> 00:10:20,750
this relies on an out of tree trace

00:10:18,170 --> 00:10:22,010
point I'm hoping actually that we can

00:10:20,750 --> 00:10:24,440
get that upstream I think it's a pretty

00:10:22,010 --> 00:10:25,699
non-controversial trace point but

00:10:24,440 --> 00:10:28,279
basically makes sure that the task

00:10:25,699 --> 00:10:31,160
utilization is consistent with the boost

00:10:28,279 --> 00:10:34,130
manipulations that we're doing and so

00:10:31,160 --> 00:10:36,290
this is a new test and then a couple of

00:10:34,130 --> 00:10:38,300
tests for the schedule tell governor so

00:10:36,290 --> 00:10:40,670
I starred these because these actually

00:10:38,300 --> 00:10:42,860
are applicable to two of the issues that

00:10:40,670 --> 00:10:45,290
we found you know within the last year

00:10:42,860 --> 00:10:47,560
or so so so this would check a couple of

00:10:45,290 --> 00:10:49,579
boxes for us in terms of known problems

00:10:47,560 --> 00:10:51,890
making sure that the latency of the

00:10:49,579 --> 00:10:54,019
governor is reasonable so this is you

00:10:51,890 --> 00:10:56,420
know like spinning up a CPU hog task

00:10:54,019 --> 00:10:57,649
making sure that the governor goes to F

00:10:56,420 --> 00:10:59,779
Max was in a reasonable amount of time

00:10:57,649 --> 00:11:02,120
and then go just making a small task

00:10:59,779 --> 00:11:04,190
make sure we go to F men and then

00:11:02,120 --> 00:11:06,199
checking those the number of wake ups

00:11:04,190 --> 00:11:09,319
from the governor threads to see that

00:11:06,199 --> 00:11:11,720
they're reasonable and then some generic

00:11:09,319 --> 00:11:13,939
generic scheduler tests you know

00:11:11,720 --> 00:11:17,930
stuff that's applicable to really any

00:11:13,939 --> 00:11:20,569
platform you know creating a handful of

00:11:17,930 --> 00:11:22,129
CFS tasks spinning into a single CPU you

00:11:20,569 --> 00:11:24,610
know with different priorities making

00:11:22,129 --> 00:11:27,769
sure that their bandwidth works out so

00:11:24,610 --> 00:11:29,060
you know if you if you look at fair

00:11:27,769 --> 00:11:31,129
Dossi or whatever you can figure out

00:11:29,060 --> 00:11:32,750
what the the bandwidth should be based

00:11:31,129 --> 00:11:34,329
on the different nice levels make sure

00:11:32,750 --> 00:11:37,420
they get the correct amount of runtime

00:11:34,329 --> 00:11:39,889
checking the deadlines task is actually

00:11:37,420 --> 00:11:42,230
execution correctness so both run time

00:11:39,889 --> 00:11:44,480
and deadlines I'm not aware so in the

00:11:42,230 --> 00:11:45,829
real-time test suite I didn't see

00:11:44,480 --> 00:11:46,939
anything for this I'm sure there's got

00:11:45,829 --> 00:11:49,329
to be something like this out there

00:11:46,939 --> 00:11:51,620
somewhere but I didn't see it

00:11:49,329 --> 00:11:53,449
scheduling latency there is a test in

00:11:51,620 --> 00:11:56,660
the real-time suite for RT but not for

00:11:53,449 --> 00:11:58,790
deadlines that I saw and then probably

00:11:56,660 --> 00:12:04,730
the correct priority scheduling for both

00:11:58,790 --> 00:12:06,889
FIFO and round-robin RT tasks okay so as

00:12:04,730 --> 00:12:09,170
I say I've run through all these tests

00:12:06,889 --> 00:12:11,300
on both pixel 3 with AOSP and then also

00:12:09,170 --> 00:12:20,750
the Heike 960 so I'm sorry there's a

00:12:11,300 --> 00:12:22,730
question yeah how did you take into

00:12:20,750 --> 00:12:24,889
account the C group impact on all your

00:12:22,730 --> 00:12:27,589
tests I mean running all these tests for

00:12:24,889 --> 00:12:30,550
at TCF as bandwidth allocation can be

00:12:27,589 --> 00:12:34,160
deeply impacted if you're using c group

00:12:30,550 --> 00:12:36,759
c FS bond with c group in your test are

00:12:34,160 --> 00:12:41,449
you using it oh it's just a plain route

00:12:36,759 --> 00:12:43,939
task running so for the like for the

00:12:41,449 --> 00:12:46,269
scheduled test where I play with the

00:12:43,939 --> 00:12:48,559
previous tests at least that you mention

00:12:46,269 --> 00:12:51,139
testing that the the test value it is

00:12:48,559 --> 00:12:54,050
affected by the CFS priority running in

00:12:51,139 --> 00:12:57,439
a C group at the root these tests should

00:12:54,050 --> 00:13:00,050
all be in the root yeah yeah cuz they're

00:12:57,439 --> 00:13:02,120
they're created just out of the shell

00:13:00,050 --> 00:13:09,170
and so on Android that should put them

00:13:02,120 --> 00:13:12,740
all in root C group so so yeah so these

00:13:09,170 --> 00:13:15,230
test results with pixel 3 so I guess a

00:13:12,740 --> 00:13:17,500
couple of things up front one thing is

00:13:15,230 --> 00:13:20,929
that a lot of these tests have

00:13:17,500 --> 00:13:22,910
thresholds you know the notion of you

00:13:20,929 --> 00:13:25,160
know what's passing for a latency for

00:13:22,910 --> 00:13:27,650
like up migration latency or

00:13:25,160 --> 00:13:29,660
you know the the percentage of so for

00:13:27,650 --> 00:13:32,000
like es one small task for example the

00:13:29,660 --> 00:13:34,100
the test criteria there is the task

00:13:32,000 --> 00:13:36,530
should run on the small CPUs of a big

00:13:34,100 --> 00:13:37,070
little topology for a certain percentage

00:13:36,530 --> 00:13:39,170
of time

00:13:37,070 --> 00:13:41,330
what is that percentage you know that

00:13:39,170 --> 00:13:42,590
that's going to be dependent on on who's

00:13:41,330 --> 00:13:44,540
running the test right and what your

00:13:42,590 --> 00:13:46,220
past criteria is so that that's

00:13:44,540 --> 00:13:47,750
something that needs to be arrived at so

00:13:46,220 --> 00:13:49,790
some of these failures are really just a

00:13:47,750 --> 00:13:51,380
matter of do we have the right you know

00:13:49,790 --> 00:13:53,450
thresholds for the test the percentage

00:13:51,380 --> 00:13:57,860
of correct task placement or the right

00:13:53,450 --> 00:14:00,740
you know latency thresholds so for pixel

00:13:57,860 --> 00:14:03,530
three the migration latency you know I'd

00:14:00,740 --> 00:14:08,060
set that to 100 milliseconds it's pixel

00:14:03,530 --> 00:14:11,480
3 is not meeting that so the es multiple

00:14:08,060 --> 00:14:12,710
to big test is failing consistently you

00:14:11,480 --> 00:14:15,680
know that that's probably a good thing

00:14:12,710 --> 00:14:18,410
for discussion I don't recall what

00:14:15,680 --> 00:14:21,170
happened with the big to small test

00:14:18,410 --> 00:14:23,480
small big toggle again it's a migration

00:14:21,170 --> 00:14:25,040
latency that's killing that so so the

00:14:23,480 --> 00:14:28,310
small the big transition is consistently

00:14:25,040 --> 00:14:30,740
happening slow and then es too big three

00:14:28,310 --> 00:14:32,750
small the small tasks are consistently

00:14:30,740 --> 00:14:34,400
running on big course and this is

00:14:32,750 --> 00:14:36,490
something that I see on hike you'll see

00:14:34,400 --> 00:14:39,020
it in the Heike 960 results as well and

00:14:36,490 --> 00:14:40,550
in discussion with arm that's something

00:14:39,020 --> 00:14:42,980
this so they've the arm folks have

00:14:40,550 --> 00:14:44,570
graciously played with this as well and

00:14:42,980 --> 00:14:46,100
given me some results and they're also

00:14:44,570 --> 00:14:51,410
seeing that so I think this would be a

00:14:46,100 --> 00:14:53,420
good thing to look into the latency so

00:14:51,410 --> 00:14:55,790
the governor test looked pretty good the

00:14:53,420 --> 00:14:57,440
scale yrs I think that's an issue again

00:14:55,790 --> 00:14:58,670
with the threshold so this is you know

00:14:57,440 --> 00:15:01,010
again we're playing with the boost

00:14:58,670 --> 00:15:02,390
tuning for a particular at SC group and

00:15:01,010 --> 00:15:04,250
then seeing what the bandwidths look

00:15:02,390 --> 00:15:06,980
like are the runtimes or I'm sorry the

00:15:04,250 --> 00:15:08,180
utilization values and you know there

00:15:06,980 --> 00:15:09,800
needs to be some slack in the

00:15:08,180 --> 00:15:15,020
utilization values and I'm not sure that

00:15:09,800 --> 00:15:16,610
that is that the slack is enough yeah

00:15:15,020 --> 00:15:18,530
most of these results looks pretty good

00:15:16,610 --> 00:15:21,650
I did see a couple of failures in the

00:15:18,530 --> 00:15:24,740
latency tests and that's something that

00:15:21,650 --> 00:15:26,930
might deserve some looking into where

00:15:24,740 --> 00:15:28,910
our latency to run a deadline or an RT

00:15:26,930 --> 00:15:31,520
task I think it got up to like 150

00:15:28,910 --> 00:15:33,470
microseconds whereas normally it's down

00:15:31,520 --> 00:15:35,270
under 20 so it seems like I don't know

00:15:33,470 --> 00:15:37,070
if it's an interrupt latency issue or

00:15:35,270 --> 00:15:39,220
what but it probably should be looked

00:15:37,070 --> 00:15:39,220
into

00:15:39,500 --> 00:15:47,210
so hiking on 60 basic one small task

00:15:44,089 --> 00:15:48,860
tests you know we're just I think so the

00:15:47,210 --> 00:15:50,839
the threshold for that is ninety percent

00:15:48,860 --> 00:15:53,120
is what I had set it at and we're not

00:15:50,839 --> 00:15:54,620
meeting that this is not a commercial

00:15:53,120 --> 00:15:56,060
you know one thing to keep in mind here

00:15:54,620 --> 00:15:58,700
I got of course hiking on sixty is not a

00:15:56,060 --> 00:15:59,720
commercially tuned platform right so so

00:15:58,700 --> 00:16:01,190
that's something to keep in mind with

00:15:59,720 --> 00:16:03,260
these numbers and are we gonna meet the

00:16:01,190 --> 00:16:04,370
same kind of thresholds that we can meet

00:16:03,260 --> 00:16:09,350
with a commercial target

00:16:04,370 --> 00:16:10,880
I don't know small the big not really

00:16:09,350 --> 00:16:14,000
passing so we're getting too much small

00:16:10,880 --> 00:16:17,180
task runtime on the big cores when it

00:16:14,000 --> 00:16:18,920
when the task is small again as I

00:16:17,180 --> 00:16:22,520
mentioned the two big three small test

00:16:18,920 --> 00:16:24,080
not not working very well and then the

00:16:22,520 --> 00:16:25,760
maximum up migration time we're seeing

00:16:24,080 --> 00:16:29,560
during the small the big toggle test is

00:16:25,760 --> 00:16:32,660
not is not good over 100 milliseconds

00:16:29,560 --> 00:16:34,279
the governor latency this is a case

00:16:32,660 --> 00:16:35,720
where I think our threshold is probably

00:16:34,279 --> 00:16:37,910
just not good our latency is right

00:16:35,720 --> 00:16:40,910
around 70 milliseconds on hiking 960 and

00:16:37,910 --> 00:16:42,529
so it's not passing all of the time we

00:16:40,910 --> 00:16:45,430
are seeing I'm seeing a lot of wake up's

00:16:42,529 --> 00:16:48,800
from the governor on hiking on 60 and

00:16:45,430 --> 00:16:51,770
then lastly the other major issue this

00:16:48,800 --> 00:16:53,270
SCAD latency dl1 on on 960 is looking

00:16:51,770 --> 00:16:55,459
pretty bad so the latency is

00:16:53,270 --> 00:16:57,980
consistently around 300 milliseconds or

00:16:55,459 --> 00:17:00,170
I'm sorry microseconds I ran this I ran

00:16:57,980 --> 00:17:02,000
this twice actually and saw the same

00:17:00,170 --> 00:17:03,620
behavior both times I did get one

00:17:02,000 --> 00:17:04,730
instance where it was down under 20 so

00:17:03,620 --> 00:17:06,470
this looks like there's some kind of

00:17:04,730 --> 00:17:12,439
system problem where our latency is not

00:17:06,470 --> 00:17:14,089
good so future plans I am hoping to get

00:17:12,439 --> 00:17:15,920
this upstream I mean I think you know

00:17:14,089 --> 00:17:18,189
there's there's a lot more work to do to

00:17:15,920 --> 00:17:20,360
make sure that this is supported on all

00:17:18,189 --> 00:17:22,910
topologies we don't want to put this

00:17:20,360 --> 00:17:25,880
into LTP if it's gonna cause you know

00:17:22,910 --> 00:17:26,059
confusion and be more trouble than it's

00:17:25,880 --> 00:17:28,670
worth

00:17:26,059 --> 00:17:30,530
as I just went through there's a number

00:17:28,670 --> 00:17:32,420
of existing failures you know this is

00:17:30,530 --> 00:17:34,670
already generating leads on potential

00:17:32,420 --> 00:17:35,780
bugs on a couple of platforms so there's

00:17:34,670 --> 00:17:38,210
already a good amount of work to do

00:17:35,780 --> 00:17:41,510
there there are some tests that are

00:17:38,210 --> 00:17:43,190
flaky some robustness issues the up

00:17:41,510 --> 00:17:46,280
streaming I'm sure is good so once once

00:17:43,190 --> 00:17:48,770
we're confident that this test suite you

00:17:46,280 --> 00:17:49,970
know has value and is accurate just

00:17:48,770 --> 00:17:51,650
going through the review process with

00:17:49,970 --> 00:17:51,980
the LTP folks I'm sure it will generate

00:17:51,650 --> 00:17:55,700
more

00:17:51,980 --> 00:17:57,830
clean up work and then there's you know

00:17:55,700 --> 00:18:00,500
out of all the existing problems I

00:17:57,830 --> 00:18:01,640
listed before I think there's only two

00:18:00,500 --> 00:18:03,590
of them that are addressed with the

00:18:01,640 --> 00:18:04,820
tests that I just mentioned and so you

00:18:03,590 --> 00:18:06,230
know there's still work especially the

00:18:04,820 --> 00:18:08,330
same flag we don't have a test for that

00:18:06,230 --> 00:18:10,310
and some of these things get pretty

00:18:08,330 --> 00:18:12,740
difficult to verify with the unit tests

00:18:10,310 --> 00:18:15,470
for example the same flag I have another

00:18:12,740 --> 00:18:18,290
test in progress for verifying the the

00:18:15,470 --> 00:18:19,910
stale CP utilization and the schedule

00:18:18,290 --> 00:18:23,990
total governor is ignored and that is

00:18:19,910 --> 00:18:25,610
not easy to implement and then ideally

00:18:23,990 --> 00:18:27,950
if we can get these tests reliable

00:18:25,610 --> 00:18:29,240
enough we'd love to get them into VTS

00:18:27,950 --> 00:18:32,270
and turn them into a compliance

00:18:29,240 --> 00:18:33,320
requirement for partners well we get

00:18:32,270 --> 00:18:34,580
there I don't know we'll have to see

00:18:33,320 --> 00:18:36,080
this is something that we really need

00:18:34,580 --> 00:18:37,940
more experience with on more platforms

00:18:36,080 --> 00:18:40,550
and see how reliable we can get these

00:18:37,940 --> 00:18:42,380
tests and then as I mentioned it be

00:18:40,550 --> 00:18:43,520
great to ditch the custom trace library

00:18:42,380 --> 00:18:48,350
because you know I don't want to have to

00:18:43,520 --> 00:18:50,770
maintain that so I think that's it we

00:18:48,350 --> 00:18:53,930
have a few minutes for any questions

00:18:50,770 --> 00:18:56,210
just have you tried to run your tests

00:18:53,930 --> 00:18:58,490
your LTP test on the mainline kernel now

00:18:56,210 --> 00:19:01,430
that we have this simple es patch it met

00:18:58,490 --> 00:19:01,910
so I believe the arm folks actually did

00:19:01,430 --> 00:19:03,950
that

00:19:01,910 --> 00:19:05,240
I don't recall the results offhand I do

00:19:03,950 --> 00:19:07,660
have them and I can share them with you

00:19:05,240 --> 00:19:07,660
afterwards

00:19:12,290 --> 00:19:16,490
I see one of the challenges for up

00:19:14,300 --> 00:19:18,320
streaming your code to LTP so you're

00:19:16,490 --> 00:19:20,390
specifying the latency depending on the

00:19:18,320 --> 00:19:21,830
hardware you are using so if it's going

00:19:20,390 --> 00:19:23,750
to be absolute thing then it has to be

00:19:21,830 --> 00:19:25,580
generic enough to identify the latency

00:19:23,750 --> 00:19:26,720
so I have seen these cases with the TC 2

00:19:25,580 --> 00:19:30,650
and you know when we are working with

00:19:26,720 --> 00:19:32,750
the jeune skilled test suit so these are

00:19:30,650 --> 00:19:34,760
configured per hardware to get the kind

00:19:32,750 --> 00:19:38,350
of a latency accordingly take additions

00:19:34,760 --> 00:19:40,940
to the verdict of a pass or failure yes

00:19:38,350 --> 00:19:42,590
so yeah there there is a clean-up item

00:19:40,940 --> 00:19:44,330
so right now as I mentioned the two

00:19:42,590 --> 00:19:46,130
thresholds for like a lot of the EES

00:19:44,330 --> 00:19:48,620
placement tests the so the placement

00:19:46,130 --> 00:19:50,810
thresholds and also like up or down

00:19:48,620 --> 00:19:52,160
migration latency thresholds right now

00:19:50,810 --> 00:19:53,870
those are just hard coded in the test

00:19:52,160 --> 00:19:55,280
but I mean at a minimum we can we can

00:19:53,870 --> 00:19:56,990
pass this and on the command line so

00:19:55,280 --> 00:20:00,380
folks can specify those in their own

00:19:56,990 --> 00:20:02,390
test scripts I would also think that we

00:20:00,380 --> 00:20:04,730
should be able to arrive on some kind of

00:20:02,390 --> 00:20:05,630
really high threshold it's like a basic

00:20:04,730 --> 00:20:08,360
minimum

00:20:05,630 --> 00:20:10,490
for a user experience you know I mean so

00:20:08,360 --> 00:20:11,990
for up migration latency for example if

00:20:10,490 --> 00:20:13,520
you have a small task that becomes large

00:20:11,990 --> 00:20:15,590
I would think

00:20:13,520 --> 00:20:16,850
200 milliseconds should be a pretty safe

00:20:15,590 --> 00:20:20,210
assumption that you should be able to

00:20:16,850 --> 00:20:21,680
get that onto the big CPU maybe not I

00:20:20,210 --> 00:20:24,080
don't know but I feel like we should be

00:20:21,680 --> 00:20:26,840
able to set some low thresholds that are

00:20:24,080 --> 00:20:28,370
as defaults and then be able to you know

00:20:26,840 --> 00:20:31,340
provide a way to override those if you

00:20:28,370 --> 00:20:32,660
want to tighten them up via the command

00:20:31,340 --> 00:20:35,810
line or whatever else these are specific

00:20:32,660 --> 00:20:37,520
to addicted addicted only some of them

00:20:35,810 --> 00:20:40,430
are so like the es placement task would

00:20:37,520 --> 00:20:43,010
be a test would be the general scheduler

00:20:40,430 --> 00:20:45,740
ones like deadlines runtime and

00:20:43,010 --> 00:20:47,600
deadlines verification those are more

00:20:45,740 --> 00:20:50,000
generic so you know I had a suite of a

00:20:47,600 --> 00:20:53,750
set of maybe six tests that apply to all

00:20:50,000 --> 00:20:55,670
all platforms there's work to I guess

00:20:53,750 --> 00:20:57,350
there's a few of them that will not run

00:20:55,670 --> 00:20:59,270
on an upstream kernel right now because

00:20:57,350 --> 00:21:02,570
they rely on out of tree EAS

00:20:59,270 --> 00:21:04,940
functionality I'm not sure the es ones

00:21:02,570 --> 00:21:08,000
also ideally they would be detecting the

00:21:04,940 --> 00:21:10,340
platform and return so like an LTP

00:21:08,000 --> 00:21:11,990
there's the T conf to skip you know the

00:21:10,340 --> 00:21:13,730
test they should do that obviously if

00:21:11,990 --> 00:21:15,230
there's functionality that's missing I

00:21:13,730 --> 00:21:16,580
doubt that they do that right now I

00:21:15,230 --> 00:21:18,200
don't remember off the top of my head

00:21:16,580 --> 00:21:21,290
but that's a cleanup that we'll need to

00:21:18,200 --> 00:21:26,930
do as part of up streaming so thank you

00:21:21,290 --> 00:21:28,990
sure any other questions I think we're

00:21:26,930 --> 00:21:33,740
done cool thank you everyone

00:21:28,990 --> 00:21:33,740

YouTube URL: https://www.youtube.com/watch?v=NIhugpiiQyU


