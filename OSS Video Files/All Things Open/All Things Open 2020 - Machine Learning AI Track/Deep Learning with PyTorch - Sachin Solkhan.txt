Title: Deep Learning with PyTorch - Sachin Solkhan
Publication date: 2020-12-14
Playlist: All Things Open 2020 - Machine Learning AI Track
Description: 
	Presented by: Sachin Solkhan, Fidelity Investments
Presented at All Things Open 2020 - Machine Learning/AI Track

Abstract: PyTorch is a open source, Python based machine and deep learning framework. It's easy-to-use API has made it a popular tool for building deep learning models. In this session, we will briefly learn basics of deep learning and focus on building a neural network using PyTorch. We will work through a Jupyter notebook to learn the steps to define, train and evaluate a convolutional neural network in PyTorch. Using a pre-trained neural network, we will then build a image classifier.
Captions: 
	00:00:05,279 --> 00:00:08,559
um

00:00:05,759 --> 00:00:09,200
welcome to deep learning with pytorch

00:00:08,559 --> 00:00:12,000
session

00:00:09,200 --> 00:00:14,400
um hope all of you are having a great

00:00:12,000 --> 00:00:16,800
time attending the conference

00:00:14,400 --> 00:00:18,960
my name is sachin sulkin i work at

00:00:16,800 --> 00:00:19,920
fidelity investments here in the rtp

00:00:18,960 --> 00:00:23,359
region in

00:00:19,920 --> 00:00:27,039
nc um uh i want to just

00:00:23,359 --> 00:00:28,960
briefly first go over kind of the agenda

00:00:27,039 --> 00:00:30,640
and the sequence of things

00:00:28,960 --> 00:00:33,440
that i will be talking and kind of

00:00:30,640 --> 00:00:36,160
sharing with you so first we'll just

00:00:33,440 --> 00:00:37,280
briefly uh talk about the python

00:00:36,160 --> 00:00:40,320
framework

00:00:37,280 --> 00:00:43,680
uh what is it why pie torch

00:00:40,320 --> 00:00:46,800
why has it become popular then we'll

00:00:43,680 --> 00:00:48,320
dive into the code of uh actually

00:00:46,800 --> 00:00:50,960
uh looking at the jupyter notebook

00:00:48,320 --> 00:00:51,760
looking at the pi touch code of building

00:00:50,960 --> 00:00:55,280
a

00:00:51,760 --> 00:00:57,120
neural network um uh we will i

00:00:55,280 --> 00:00:58,399
here will build a convolution neural

00:00:57,120 --> 00:01:00,399
network

00:00:58,399 --> 00:01:02,079
so we'll walk through that using the

00:01:00,399 --> 00:01:04,799
jupiter notebook

00:01:02,079 --> 00:01:05,199
and we'll use this convolutional network

00:01:04,799 --> 00:01:10,320
for

00:01:05,199 --> 00:01:13,200
image classification uh given the

00:01:10,320 --> 00:01:13,600
the virtual nature of this uh conference

00:01:13,200 --> 00:01:16,880
uh

00:01:13,600 --> 00:01:18,799
i i i really can't see any of you and it

00:01:16,880 --> 00:01:20,400
is really weird i have been presenting

00:01:18,799 --> 00:01:21,280
at this conference for the last few

00:01:20,400 --> 00:01:24,479
years and

00:01:21,280 --> 00:01:27,040
it is always an exciting time uh in

00:01:24,479 --> 00:01:28,000
this time around and fall uh just just

00:01:27,040 --> 00:01:29,840
that energy

00:01:28,000 --> 00:01:31,040
um that that you see here at the

00:01:29,840 --> 00:01:32,960
conference here

00:01:31,040 --> 00:01:34,079
uh i'm presenting there it's little

00:01:32,960 --> 00:01:36,159
weird right now

00:01:34,079 --> 00:01:39,040
uh that you can see me but i i don't i

00:01:36,159 --> 00:01:40,880
can't see any anybody else and you're

00:01:39,040 --> 00:01:42,880
how you're feeling and how things are

00:01:40,880 --> 00:01:46,560
going so

00:01:42,880 --> 00:01:50,079
please feel free as the moderator said

00:01:46,560 --> 00:01:52,840
use the chat session the q a session

00:01:50,079 --> 00:01:54,479
i'll be looking at it often through the

00:01:52,840 --> 00:01:57,840
presentation

00:01:54,479 --> 00:01:59,020
also will try to make it as much

00:01:57,840 --> 00:02:00,399
interactive as i can

00:01:59,020 --> 00:02:03,280
[Music]

00:02:00,399 --> 00:02:05,119
with using the raise your hand feature

00:02:03,280 --> 00:02:08,160
so we need your help there

00:02:05,119 --> 00:02:11,120
to to respond to my questions

00:02:08,160 --> 00:02:12,800
i also need your help to kind of disable

00:02:11,120 --> 00:02:14,080
your raise your hand

00:02:12,800 --> 00:02:15,840
as we are done with that kind of

00:02:14,080 --> 00:02:17,680
particular question because there's

00:02:15,840 --> 00:02:19,520
uh there's no way for me to kind of

00:02:17,680 --> 00:02:22,640
disable it for everybody so

00:02:19,520 --> 00:02:24,400
need some help there and i'm always

00:02:22,640 --> 00:02:25,360
available to also connect after the

00:02:24,400 --> 00:02:28,400
session

00:02:25,360 --> 00:02:32,000
uh my twitter handle is there and um

00:02:28,400 --> 00:02:36,160
on this on all the slides and so we can

00:02:32,000 --> 00:02:39,440
um we can also chat um via that

00:02:36,160 --> 00:02:39,840
so also as i um before i get into that

00:02:39,440 --> 00:02:42,640
just

00:02:39,840 --> 00:02:44,640
an obligatory comment um that all the

00:02:42,640 --> 00:02:46,160
content and comments here are mine not a

00:02:44,640 --> 00:02:49,120
reflection of

00:02:46,160 --> 00:02:51,840
the place where i work so just uh let's

00:02:49,120 --> 00:02:51,840
get into

00:02:54,239 --> 00:02:56,879
talking about

00:02:57,440 --> 00:03:05,440
by torch and before i get into

00:03:00,480 --> 00:03:07,440
the actual the basics of our python

00:03:05,440 --> 00:03:09,599
i wanted to just first share like few

00:03:07,440 --> 00:03:11,760
tips of my learnings

00:03:09,599 --> 00:03:14,239
as i learned pie torch or any of the

00:03:11,760 --> 00:03:17,280
other deep learning framework

00:03:14,239 --> 00:03:19,360
uh so we all know that uh ml

00:03:17,280 --> 00:03:21,280
is based on like the machine learning is

00:03:19,360 --> 00:03:23,920
based on linear algebra and

00:03:21,280 --> 00:03:25,040
probability calculus but you don't

00:03:23,920 --> 00:03:28,959
really need to know

00:03:25,040 --> 00:03:30,720
math and math is not going to help you

00:03:28,959 --> 00:03:31,440
choose the different types of algorithms

00:03:30,720 --> 00:03:34,959
you

00:03:31,440 --> 00:03:37,599
you still have to spend time in

00:03:34,959 --> 00:03:38,000
exploring and experimenting this so much

00:03:37,599 --> 00:03:41,040
of

00:03:38,000 --> 00:03:44,080
intuition that goes with

00:03:41,040 --> 00:03:46,000
creating a deep learning model or any

00:03:44,080 --> 00:03:49,360
any machine learning model

00:03:46,000 --> 00:03:51,440
so so the

00:03:49,360 --> 00:03:52,959
knowing everything about math is is not

00:03:51,440 --> 00:03:55,120
needed

00:03:52,959 --> 00:03:56,959
also some of these things get really

00:03:55,120 --> 00:03:58,159
very complex

00:03:56,959 --> 00:04:00,480
so you don't need to understand

00:03:58,159 --> 00:04:02,959
everything at the first go

00:04:00,480 --> 00:04:05,200
as you're going through the code that we

00:04:02,959 --> 00:04:07,280
that we will also see in in few minutes

00:04:05,200 --> 00:04:08,720
as i walk through the jupiter notebook

00:04:07,280 --> 00:04:11,040
uh you really need to

00:04:08,720 --> 00:04:12,000
at a high level understand kind of your

00:04:11,040 --> 00:04:15,200
the basics

00:04:12,000 --> 00:04:16,079
the input and the results that you see

00:04:15,200 --> 00:04:17,280
at the end

00:04:16,079 --> 00:04:19,040
and then you can then start

00:04:17,280 --> 00:04:22,239
experimenting with changing certain

00:04:19,040 --> 00:04:22,239
parameters etc

00:04:22,880 --> 00:04:29,919
also again it is not very

00:04:26,560 --> 00:04:30,240
much necessary to know the ins and outs

00:04:29,919 --> 00:04:32,720
and

00:04:30,240 --> 00:04:34,880
every detail about the particular

00:04:32,720 --> 00:04:36,639
algorithm

00:04:34,880 --> 00:04:38,880
i think it is more important to know

00:04:36,639 --> 00:04:41,600
kind of the the limitations

00:04:38,880 --> 00:04:42,080
of the algorithms more really know at a

00:04:41,600 --> 00:04:44,320
high level

00:04:42,080 --> 00:04:46,240
which is applicable where rather than

00:04:44,320 --> 00:04:49,280
like really understanding

00:04:46,240 --> 00:04:52,800
and deeply understanding the algorithms

00:04:49,280 --> 00:04:54,720
um and then you don't need to

00:04:52,800 --> 00:04:56,479
understand python python is obviously

00:04:54,720 --> 00:04:57,040
based on python you don't really need to

00:04:56,479 --> 00:04:58,720
know

00:04:57,040 --> 00:05:01,120
every time that obviously if you know

00:04:58,720 --> 00:05:02,720
that it's easier but

00:05:01,120 --> 00:05:05,199
if you know any other programming

00:05:02,720 --> 00:05:08,320
language it is good it is easier

00:05:05,199 --> 00:05:09,520
syntax of python is very intuitive many

00:05:08,320 --> 00:05:12,320
of you might have already

00:05:09,520 --> 00:05:13,680
been working on it so it might be um

00:05:12,320 --> 00:05:16,000
very familiar but

00:05:13,680 --> 00:05:16,720
you don't need to really know everything

00:05:16,000 --> 00:05:19,840
um

00:05:16,720 --> 00:05:22,000
or all the features of python

00:05:19,840 --> 00:05:23,039
and i always like i want to mention this

00:05:22,000 --> 00:05:27,120
is because

00:05:23,039 --> 00:05:28,720
i i feel um my journey of learning

00:05:27,120 --> 00:05:30,160
ai and machine learning and deep

00:05:28,720 --> 00:05:32,400
learning

00:05:30,160 --> 00:05:33,520
i was always hesitant uh learning more

00:05:32,400 --> 00:05:35,039
of it and

00:05:33,520 --> 00:05:36,880
it was all because of some of these

00:05:35,039 --> 00:05:39,360
things that were in my mind and were

00:05:36,880 --> 00:05:41,520
blocking me but once i started

00:05:39,360 --> 00:05:43,039
um just following some of the steps it

00:05:41,520 --> 00:05:44,880
was easier for me to

00:05:43,039 --> 00:05:46,320
learn the frame frameworks and most of

00:05:44,880 --> 00:05:48,800
these frameworks

00:05:46,320 --> 00:05:50,479
and and and how they work are very much

00:05:48,800 --> 00:05:52,800
similar so

00:05:50,479 --> 00:05:55,440
we will talk about by torch versus

00:05:52,800 --> 00:05:57,039
tensorflow keras um and kind of the

00:05:55,440 --> 00:05:58,720
distribution features between them but

00:05:57,039 --> 00:06:00,400
again it is they they follow

00:05:58,720 --> 00:06:02,240
very much similar pattern and you learn

00:06:00,400 --> 00:06:05,360
one you uh learn

00:06:02,240 --> 00:06:08,639
other ones also so getting on to

00:06:05,360 --> 00:06:11,919
the the the meat of this uh

00:06:08,639 --> 00:06:14,960
session so what what is pi my touch um

00:06:11,919 --> 00:06:16,639
uh i think with seeing so many of you

00:06:14,960 --> 00:06:19,840
attending this session i think

00:06:16,639 --> 00:06:21,360
uh it is you you all already know and

00:06:19,840 --> 00:06:24,560
that it is one of the

00:06:21,360 --> 00:06:28,080
most popular deep learning frameworks um

00:06:24,560 --> 00:06:31,360
it is uh that is increasingly used by

00:06:28,080 --> 00:06:34,720
a lot of ai researchers it has been

00:06:31,360 --> 00:06:34,720
it was released just

00:06:35,360 --> 00:06:39,600
in 2017 early 2017 and just in three

00:06:38,880 --> 00:06:42,560
years

00:06:39,600 --> 00:06:43,440
it has grown a lot in popularity and

00:06:42,560 --> 00:06:46,840
kind of

00:06:43,440 --> 00:06:48,000
uh matching trying to match uh with uh

00:06:46,840 --> 00:06:51,599
tensorflow

00:06:48,000 --> 00:06:53,440
so what is pytag

00:06:51,599 --> 00:06:55,759
open source python library for deep

00:06:53,440 --> 00:06:55,759
learning

00:06:56,240 --> 00:07:01,039
developed and maintained by facebook

00:06:59,360 --> 00:07:03,199
popularly used by academics and

00:07:01,039 --> 00:07:05,039
researchers that is where

00:07:03,199 --> 00:07:06,720
where it is getting that popularity and

00:07:05,039 --> 00:07:08,560
slowly uh getting into more

00:07:06,720 --> 00:07:12,160
productionalized

00:07:08,560 --> 00:07:14,400
kind of use cases um

00:07:12,160 --> 00:07:16,080
if i just had this picture of the

00:07:14,400 --> 00:07:19,360
popularity from the google search

00:07:16,080 --> 00:07:22,319
trends you can see that um the

00:07:19,360 --> 00:07:24,479
the blue is the tensorflow and the

00:07:22,319 --> 00:07:25,280
orange is the pie torch and you can see

00:07:24,479 --> 00:07:28,479
that

00:07:25,280 --> 00:07:29,919
as we come towards the june of 2020

00:07:28,479 --> 00:07:30,960
which is the like the last one that i

00:07:29,919 --> 00:07:34,000
have here

00:07:30,960 --> 00:07:35,520
um it has kind of started matching uh

00:07:34,000 --> 00:07:37,599
at least from the google search trends

00:07:35,520 --> 00:07:41,120
uh that often definitely is not

00:07:37,599 --> 00:07:43,599
the only means um of uh knowing the

00:07:41,120 --> 00:07:44,720
or understand assessing the framework

00:07:43,599 --> 00:07:48,160
but it kind of

00:07:44,720 --> 00:07:51,440
tells you a story about

00:07:48,160 --> 00:07:55,599
python it is primarily

00:07:51,440 --> 00:07:58,160
used for those use cases those ai

00:07:55,599 --> 00:08:00,479
machine learning use cases related to

00:07:58,160 --> 00:08:02,160
computer vision or natural language

00:08:00,479 --> 00:08:04,960
processing

00:08:02,160 --> 00:08:07,280
things so anything with image processing

00:08:04,960 --> 00:08:12,319
image classification which comes in

00:08:07,280 --> 00:08:15,280
under computer vision or your um

00:08:12,319 --> 00:08:15,280
understanding your

00:08:16,319 --> 00:08:21,120
basic english language text for example

00:08:19,199 --> 00:08:23,520
under natural language processing

00:08:21,120 --> 00:08:25,840
uh by one of the reasons why it has

00:08:23,520 --> 00:08:27,599
become popular

00:08:25,840 --> 00:08:30,479
as opposed to the other frameworks is

00:08:27,599 --> 00:08:33,279
because it is very flexible

00:08:30,479 --> 00:08:34,399
and we'll get into some of it in the

00:08:33,279 --> 00:08:37,440
next slide

00:08:34,399 --> 00:08:41,519
but there are

00:08:37,440 --> 00:08:42,800
the compared to something like keras

00:08:41,519 --> 00:08:45,680
which is really

00:08:42,800 --> 00:08:46,160
a very high level kind of uh framework

00:08:45,680 --> 00:08:48,880
um

00:08:46,160 --> 00:08:50,160
itouch is much more lower level but not

00:08:48,880 --> 00:08:53,760
as low level as

00:08:50,160 --> 00:08:54,800
for example tensorflow uh it is really

00:08:53,760 --> 00:08:57,760
um

00:08:54,800 --> 00:08:59,360
has uh good access very good uh

00:08:57,760 --> 00:09:01,519
debugging capabilities

00:08:59,360 --> 00:09:03,279
uh there are some features called like

00:09:01,519 --> 00:09:05,839
dynamic graphs and all which

00:09:03,279 --> 00:09:06,880
which makes it um much more appealing

00:09:05,839 --> 00:09:09,920
and flexible

00:09:06,880 --> 00:09:11,040
very well documented um keras for

00:09:09,920 --> 00:09:15,279
example is

00:09:11,040 --> 00:09:18,080
more suited for um that those

00:09:15,279 --> 00:09:19,600
developers or those who who want to

00:09:18,080 --> 00:09:20,240
really just want a plug-and-play

00:09:19,600 --> 00:09:24,000
framework

00:09:20,240 --> 00:09:25,839
don't want to re to know all the details

00:09:24,000 --> 00:09:29,200
all the lower level details

00:09:25,839 --> 00:09:31,519
um but as well by torch

00:09:29,200 --> 00:09:33,360
uh it gets into that lower level details

00:09:31,519 --> 00:09:34,560
and that's the cost you pay as opposed

00:09:33,360 --> 00:09:36,560
to killers

00:09:34,560 --> 00:09:38,880
uh and i'll get into the comparison

00:09:36,560 --> 00:09:39,680
between some of the frameworks um in in

00:09:38,880 --> 00:09:43,120
just

00:09:39,680 --> 00:09:44,160
in one of the slides uh later uh i think

00:09:43,120 --> 00:09:48,240
the next slide

00:09:44,160 --> 00:09:51,040
uh but just um

00:09:48,240 --> 00:09:51,760
just uh getting on to more about like

00:09:51,040 --> 00:09:55,120
why

00:09:51,760 --> 00:09:55,600
bite by torch right uh so it is python

00:09:55,120 --> 00:09:59,519
based

00:09:55,600 --> 00:10:02,480
um uh it is you you'll see

00:09:59,519 --> 00:10:04,399
called as very pythonic um the the

00:10:02,480 --> 00:10:05,200
libraries of my torch are similar to

00:10:04,399 --> 00:10:08,399
numpy

00:10:05,200 --> 00:10:09,120
uh though they are not there by torch by

00:10:08,399 --> 00:10:12,800
default

00:10:09,120 --> 00:10:15,279
uh user stencils um

00:10:12,800 --> 00:10:16,240
but very very much similar to numpy and

00:10:15,279 --> 00:10:19,839
very good

00:10:16,240 --> 00:10:23,519
support available to um

00:10:19,839 --> 00:10:27,120
to uh convert into numpy back and forth

00:10:23,519 --> 00:10:27,839
um uh and numpy is a is one of the very

00:10:27,120 --> 00:10:31,760
popular

00:10:27,839 --> 00:10:33,680
packages to that that is used heavily in

00:10:31,760 --> 00:10:35,680
machine learning algorithms or

00:10:33,680 --> 00:10:39,200
implementing machine learning

00:10:35,680 --> 00:10:40,800
um the python has many algorithms

00:10:39,200 --> 00:10:41,600
already implemented the whole point of

00:10:40,800 --> 00:10:43,120
all these

00:10:41,600 --> 00:10:45,600
deep learning frameworks is that they

00:10:43,120 --> 00:10:46,240
have these algorithms implemented and

00:10:45,600 --> 00:10:49,279
we'll see

00:10:46,240 --> 00:10:50,959
how easy it is to just generate

00:10:49,279 --> 00:10:52,320
like a convolutional neural network with

00:10:50,959 --> 00:10:54,560
just a few lines of code

00:10:52,320 --> 00:10:55,680
right that's the whole advantage

00:10:54,560 --> 00:10:58,480
advantage with

00:10:55,680 --> 00:10:58,480
this frameworks

00:10:58,800 --> 00:11:02,000
i it obviously is a deep learning

00:11:01,120 --> 00:11:05,440
framework

00:11:02,000 --> 00:11:06,800
uh it has gpu support um as opposed to

00:11:05,440 --> 00:11:08,800
something like if you just want to use

00:11:06,800 --> 00:11:11,519
python out of the box and numpy

00:11:08,800 --> 00:11:12,399
it does not have the gpu support uh as i

00:11:11,519 --> 00:11:14,240
mentioned

00:11:12,399 --> 00:11:15,519
earlier it's supposed dynamic

00:11:14,240 --> 00:11:18,399
computational graphs

00:11:15,519 --> 00:11:18,720
what it is that as we uh walk through

00:11:18,399 --> 00:11:22,560
the

00:11:18,720 --> 00:11:25,600
convolutional network kind of just the

00:11:22,560 --> 00:11:28,000
the flow there um you use uh that

00:11:25,600 --> 00:11:30,079
i can i can mention it there also that

00:11:28,000 --> 00:11:32,320
what happens is that

00:11:30,079 --> 00:11:33,839
in in in deep learning as you're going

00:11:32,320 --> 00:11:36,160
through

00:11:33,839 --> 00:11:37,120
your training data set and your training

00:11:36,160 --> 00:11:40,480
your your

00:11:37,120 --> 00:11:42,480
network the this

00:11:40,480 --> 00:11:44,160
behind the scenes these frameworks are

00:11:42,480 --> 00:11:46,399
really

00:11:44,160 --> 00:11:47,600
calculating and creating graphs and

00:11:46,399 --> 00:11:50,639
those graphs

00:11:47,600 --> 00:11:52,320
um are in in pi torch are dynamic as

00:11:50,639 --> 00:11:53,279
opposed to static and for example in

00:11:52,320 --> 00:11:54,800
tensorflow

00:11:53,279 --> 00:11:56,959
what that means is that it just gives

00:11:54,800 --> 00:11:59,040
you more flexibility as a developer

00:11:56,959 --> 00:12:02,000
to be able to manipulate those to be

00:11:59,040 --> 00:12:04,880
able to uh tweak those to be able to

00:12:02,000 --> 00:12:06,000
debug those uh it just is much more and

00:12:04,880 --> 00:12:10,240
that's where

00:12:06,000 --> 00:12:11,839
it is where the flexibility comes with

00:12:10,240 --> 00:12:13,519
with this dynamic computational graph

00:12:11,839 --> 00:12:17,360
you can in fact change the

00:12:13,519 --> 00:12:20,560
behavior as you run and and

00:12:17,360 --> 00:12:23,920
you you can do that without actually

00:12:20,560 --> 00:12:24,320
incurring much overhead so again very

00:12:23,920 --> 00:12:27,600
very

00:12:24,320 --> 00:12:29,680
useful when you are trying to train and

00:12:27,600 --> 00:12:32,399
you're trying to debug your

00:12:29,680 --> 00:12:34,480
um your your your new your neural

00:12:32,399 --> 00:12:37,519
network or your machine learning model

00:12:34,480 --> 00:12:38,800
and you it will help you create more it

00:12:37,519 --> 00:12:41,920
helps you create more

00:12:38,800 --> 00:12:45,920
sophisticated models with not much

00:12:41,920 --> 00:12:48,160
much effort and like most of the frame

00:12:45,920 --> 00:12:49,120
deep learning popular frameworks it has

00:12:48,160 --> 00:12:52,639
cloud support

00:12:49,120 --> 00:12:56,079
um so all the popular cloud providers

00:12:52,639 --> 00:12:58,399
provide that i'm going to use goggle

00:12:56,079 --> 00:12:59,200
and jupiter notebook and kaggle to to

00:12:58,399 --> 00:13:01,440
just

00:12:59,200 --> 00:13:03,360
walk through it and see and show you how

00:13:01,440 --> 00:13:07,279
easy it is but there is

00:13:03,360 --> 00:13:10,560
so many other other

00:13:07,279 --> 00:13:13,120
avenues also for you to quickly um

00:13:10,560 --> 00:13:14,480
play with my torch and then there is the

00:13:13,120 --> 00:13:17,920
open neural network

00:13:14,480 --> 00:13:20,639
um association and and kind of

00:13:17,920 --> 00:13:21,600
it supports that so what it means is

00:13:20,639 --> 00:13:24,079
that you are

00:13:21,600 --> 00:13:25,519
able to switch between different

00:13:24,079 --> 00:13:28,480
frameworks easily

00:13:25,519 --> 00:13:28,959
because of that support so before i get

00:13:28,480 --> 00:13:31,920
into

00:13:28,959 --> 00:13:33,120
like um just comparing pytorch with

00:13:31,920 --> 00:13:36,320
other frameworks

00:13:33,120 --> 00:13:39,839
i just wanted to

00:13:36,320 --> 00:13:41,839
just get a general idea of like

00:13:39,839 --> 00:13:43,040
what all frameworks have have people

00:13:41,839 --> 00:13:44,639
used

00:13:43,040 --> 00:13:46,880
machine learning frameworks if you want

00:13:44,639 --> 00:13:49,040
to just

00:13:46,880 --> 00:13:50,480
type it in the chat anything that you

00:13:49,040 --> 00:13:52,000
have played around

00:13:50,480 --> 00:14:03,839
any of the machine learning frameworks

00:13:52,000 --> 00:14:03,839
that you want to share

00:14:04,800 --> 00:14:10,959
yeah care is uh good any other ones yeah

00:14:08,240 --> 00:14:10,959
very popular

00:14:11,440 --> 00:14:15,279
google collab yeah i was going to

00:14:13,360 --> 00:14:19,120
mention google collab

00:14:15,279 --> 00:14:22,880
tensorflow tracks yeah all the

00:14:19,120 --> 00:14:26,000
all the familiar ones uh also just by

00:14:22,880 --> 00:14:31,199
show of hands um

00:14:26,000 --> 00:14:34,720
how many of you are um

00:14:31,199 --> 00:14:37,440
um have i have been have worked with

00:14:34,720 --> 00:14:39,760
deep learning and i have a good uh

00:14:37,440 --> 00:14:42,320
really solid understanding of

00:14:39,760 --> 00:14:44,320
deep learning frameworks and have i've

00:14:42,320 --> 00:14:47,519
created deep learning models

00:14:44,320 --> 00:14:47,519
um just show by hands

00:14:50,160 --> 00:14:57,760
okay kind of few um

00:14:54,480 --> 00:15:00,880
yeah around five uh

00:14:57,760 --> 00:15:01,920
probably less than five percent uh how

00:15:00,880 --> 00:15:04,880
many of you

00:15:01,920 --> 00:15:06,000
um have like a kind of a decent

00:15:04,880 --> 00:15:08,800
understanding

00:15:06,000 --> 00:15:13,839
uh of uh deep learning convolutional

00:15:08,800 --> 00:15:13,839
neural networks

00:15:18,959 --> 00:15:25,360
okay a little more probably

00:15:22,079 --> 00:15:26,079
maybe 10 12 and how many of you are like

00:15:25,360 --> 00:15:29,600
very new

00:15:26,079 --> 00:15:31,360
and um i just

00:15:29,600 --> 00:15:32,880
have probably read about it here and

00:15:31,360 --> 00:15:35,279
there but not really

00:15:32,880 --> 00:15:35,920
do not understand the theory and and

00:15:35,279 --> 00:15:41,839
kind of

00:15:35,920 --> 00:15:41,839
just how the modeling works

00:15:49,199 --> 00:15:56,639
okay not everybody is raising hands but

00:15:53,600 --> 00:15:58,000
i guess probably i will assume the rest

00:15:56,639 --> 00:16:00,399
of the ones are

00:15:58,000 --> 00:16:01,120
all new and i'm seeing also the chat

00:16:00,399 --> 00:16:02,720
posts um

00:16:01,120 --> 00:16:04,639
looking at different places here trying

00:16:02,720 --> 00:16:08,320
to gather information

00:16:04,639 --> 00:16:09,519
yeah cool thank you uh helpful so that

00:16:08,320 --> 00:16:11,279
it just helps me as i

00:16:09,519 --> 00:16:13,199
as i get into it and as i walk through

00:16:11,279 --> 00:16:17,360
it cool

00:16:13,199 --> 00:16:20,399
um so going to

00:16:17,360 --> 00:16:22,959
just talking about the different

00:16:20,399 --> 00:16:25,519
frameworks and and comparing it

00:16:22,959 --> 00:16:28,480
so i have just the the three most kind

00:16:25,519 --> 00:16:31,519
of popular frameworks that i have

00:16:28,480 --> 00:16:33,440
uh presented here and i have uh uh

00:16:31,519 --> 00:16:34,639
have picked it up from an article which

00:16:33,440 --> 00:16:37,759
have um

00:16:34,639 --> 00:16:40,720
referred here at the bottom so it was a

00:16:37,759 --> 00:16:40,720
very nice kind of

00:16:41,839 --> 00:16:47,040
way of comparing it uh from

00:16:44,959 --> 00:16:48,800
like at the top you see the api which

00:16:47,040 --> 00:16:49,600
means like do you have like easy apis

00:16:48,800 --> 00:16:52,160
available

00:16:49,600 --> 00:16:53,199
which is what like uh carers as well as

00:16:52,160 --> 00:16:55,600
tensorflow

00:16:53,199 --> 00:16:56,639
really excel apache really don't have

00:16:55,600 --> 00:17:00,320
that kind of like

00:16:56,639 --> 00:17:02,240
high level apis available um keras code

00:17:00,320 --> 00:17:04,000
if you just look at it you can it's like

00:17:02,240 --> 00:17:06,079
english you will just know it

00:17:04,000 --> 00:17:08,480
bye target gets into tensors and all in

00:17:06,079 --> 00:17:11,839
it it is not that easy to read

00:17:08,480 --> 00:17:14,559
um the the kind of use cases

00:17:11,839 --> 00:17:15,760
that uh pie dodge is more applicable are

00:17:14,559 --> 00:17:18,640
really more

00:17:15,760 --> 00:17:19,199
complex ones where there are a lot of

00:17:18,640 --> 00:17:21,280
large

00:17:19,199 --> 00:17:23,520
data sets you need high performance and

00:17:21,280 --> 00:17:25,439
all um

00:17:23,520 --> 00:17:27,600
which is the same which with tensorflow

00:17:25,439 --> 00:17:29,200
which by the way is the most popular and

00:17:27,600 --> 00:17:31,679
which is the most

00:17:29,200 --> 00:17:32,480
the oldest as well as um where most of

00:17:31,679 --> 00:17:35,760
the

00:17:32,480 --> 00:17:36,720
production kind of use cases have been

00:17:35,760 --> 00:17:39,280
deployed

00:17:36,720 --> 00:17:42,640
uh keras is obviously plug and play as i

00:17:39,280 --> 00:17:42,640
said so smaller data sets

00:17:42,799 --> 00:17:46,320
by touch as i mentioned good debugging

00:17:44,559 --> 00:17:49,840
capabilities which is one of the

00:17:46,320 --> 00:17:49,840
advantages over tensorflow

00:17:50,000 --> 00:17:53,600
uh it has all all the frameworks have

00:17:51,919 --> 00:17:55,120
pre-trained models so you really even

00:17:53,600 --> 00:17:56,880
don't even have to train the models you

00:17:55,120 --> 00:17:58,400
can just pick it and use it for your use

00:17:56,880 --> 00:18:01,039
case

00:17:58,400 --> 00:18:02,000
it is uh becoming more and more popular

00:18:01,039 --> 00:18:03,440
by torch

00:18:02,000 --> 00:18:05,039
as we speak i don't know if it is the

00:18:03,440 --> 00:18:08,320
third most popular

00:18:05,039 --> 00:18:11,440
the uh second most uh popular

00:18:08,320 --> 00:18:14,240
uh performance is high and

00:18:11,440 --> 00:18:15,600
the basic language that is written as is

00:18:14,240 --> 00:18:18,240
in lower but

00:18:15,600 --> 00:18:19,600
it it is exposed as python and you use

00:18:18,240 --> 00:18:21,360
python to write it

00:18:19,600 --> 00:18:23,039
so it just this i i really like this

00:18:21,360 --> 00:18:25,600
table because it kind of just

00:18:23,039 --> 00:18:27,200
tells you where pytorch fits with

00:18:25,600 --> 00:18:28,559
respect to all the other popular

00:18:27,200 --> 00:18:29,280
frameworks that are there and it's going

00:18:28,559 --> 00:18:32,000
to play out

00:18:29,280 --> 00:18:33,760
it's going to evolve and as as as we are

00:18:32,000 --> 00:18:36,400
seeing in the last couple of years

00:18:33,760 --> 00:18:38,000
uh most of the frameworks adopt the best

00:18:36,400 --> 00:18:40,799
practices and the most

00:18:38,000 --> 00:18:42,640
uh liked uh features of the other

00:18:40,799 --> 00:18:44,000
frameworks and within probably

00:18:42,640 --> 00:18:45,919
six months to a year you have other

00:18:44,000 --> 00:18:48,080
frameworks also catching up which

00:18:45,919 --> 00:18:49,120
i think tensorflow has now the the

00:18:48,080 --> 00:18:52,720
dynamic graph

00:18:49,120 --> 00:18:52,960
uh facility uh feature uh earlier it was

00:18:52,720 --> 00:18:54,720
a

00:18:52,960 --> 00:18:56,480
study graph but now it has the dynamic

00:18:54,720 --> 00:19:00,160
graph also

00:18:56,480 --> 00:19:03,440
okay so um now i'm getting to

00:19:00,160 --> 00:19:06,799
the the actual code and

00:19:03,440 --> 00:19:10,720
and building the neural network um

00:19:06,799 --> 00:19:13,360
so deep learning um

00:19:10,720 --> 00:19:14,880
like any other machine learning uh

00:19:13,360 --> 00:19:17,120
building model

00:19:14,880 --> 00:19:18,960
classic different steps uh from

00:19:17,120 --> 00:19:21,039
preparing the data defining the model

00:19:18,960 --> 00:19:22,400
training evaluating and then actually

00:19:21,039 --> 00:19:24,080
testing the model and making the

00:19:22,400 --> 00:19:26,880
productions right so

00:19:24,080 --> 00:19:29,440
i like to always think it that way i

00:19:26,880 --> 00:19:29,440
like to

00:19:29,520 --> 00:19:36,640
organize my code and my jupyter notebook

00:19:33,280 --> 00:19:37,440
also similarly that way and i have tried

00:19:36,640 --> 00:19:38,960
to organize

00:19:37,440 --> 00:19:40,799
the notebook that which i'm going to

00:19:38,960 --> 00:19:42,880
share with you in a minute

00:19:40,799 --> 00:19:43,919
that that way it is very clear and very

00:19:42,880 --> 00:19:45,679
easy to

00:19:43,919 --> 00:19:48,080
understand and obviously easy to share

00:19:45,679 --> 00:19:50,480
with your other

00:19:48,080 --> 00:19:53,840
co-workers that you would be working on

00:19:50,480 --> 00:19:53,840
for building this model

00:19:54,480 --> 00:19:58,240
any any questions before i move ahead

00:19:57,039 --> 00:20:07,840
and start

00:19:58,240 --> 00:20:07,840
sharing the code

00:20:11,200 --> 00:20:18,559
okay i will move on

00:20:14,640 --> 00:20:20,400
so jupyter notebook um so we

00:20:18,559 --> 00:20:22,080
as i mentioned that i'm going to uh

00:20:20,400 --> 00:20:24,799
hosted it on kaggle

00:20:22,080 --> 00:20:26,080
and kaggle provide gpu support um

00:20:24,799 --> 00:20:28,320
somebody on the chat also

00:20:26,080 --> 00:20:29,840
mentioned about google collab another

00:20:28,320 --> 00:20:32,880
tool that is available

00:20:29,840 --> 00:20:34,880
um it is very easy you cannot obviously

00:20:32,880 --> 00:20:35,919
use other the cloud providers also but

00:20:34,880 --> 00:20:38,640
this is

00:20:35,919 --> 00:20:39,039
just very simple plug and play any of

00:20:38,640 --> 00:20:41,760
these

00:20:39,039 --> 00:20:43,360
uh tools uh that are available online

00:20:41,760 --> 00:20:44,000
you don't need to install anything on

00:20:43,360 --> 00:20:46,480
your

00:20:44,000 --> 00:20:48,080
local machine and all to try it just to

00:20:46,480 --> 00:20:51,440
try out certain things

00:20:48,080 --> 00:20:52,720
and then you get free gpu uh to train

00:20:51,440 --> 00:20:55,280
your models

00:20:52,720 --> 00:20:55,840
which obviously makes uh training your

00:20:55,280 --> 00:20:57,600
models

00:20:55,840 --> 00:20:59,520
based obviously based on data side but

00:20:57,600 --> 00:21:01,760
data set but it helps you train your

00:20:59,520 --> 00:21:03,760
models

00:21:01,760 --> 00:21:04,960
in few minutes as opposed to hours if

00:21:03,760 --> 00:21:08,000
you um

00:21:04,960 --> 00:21:09,679
use cpu and as i'm saying that it

00:21:08,000 --> 00:21:12,559
obviously depends on the size of the

00:21:09,679 --> 00:21:15,200
data set so

00:21:12,559 --> 00:21:15,679
um i'm sorry i'm going to stop sharing

00:21:15,200 --> 00:21:18,720
this

00:21:15,679 --> 00:21:18,720
and then go to

00:21:18,799 --> 00:21:31,840
go and share the jupiter notebook so

00:21:20,559 --> 00:21:31,840
give me a second

00:21:34,480 --> 00:21:40,960
okay i think you all should see uh

00:21:37,840 --> 00:21:44,799
the the browser and the

00:21:40,960 --> 00:21:47,679
notebook here so

00:21:44,799 --> 00:21:48,880
um some other things here and this is as

00:21:47,679 --> 00:21:52,320
i mentioned is on

00:21:48,880 --> 00:21:56,000
goggles so let me

00:21:52,320 --> 00:21:59,600
just show you certain things here so um

00:21:56,000 --> 00:22:02,320
you have set it uh for python

00:21:59,600 --> 00:22:05,120
i have put in as accelerator as uh to

00:22:02,320 --> 00:22:08,159
your gpu it gives you some

00:22:05,120 --> 00:22:09,840
minimum number of hours um that's a

00:22:08,159 --> 00:22:10,400
quota that you're available over a

00:22:09,840 --> 00:22:14,240
certain

00:22:10,400 --> 00:22:15,600
number of days uh 42 hours

00:22:14,240 --> 00:22:17,520
and then there are certain other things

00:22:15,600 --> 00:22:20,880
that you need to

00:22:17,520 --> 00:22:25,280
enable for the code to run

00:22:20,880 --> 00:22:27,360
and there is this is all available under

00:22:25,280 --> 00:22:28,880
and under the settings if you're a

00:22:27,360 --> 00:22:30,559
kaggle account you can

00:22:28,880 --> 00:22:31,919
you you might already know where you you

00:22:30,559 --> 00:22:35,280
can go and

00:22:31,919 --> 00:22:37,679
check out um

00:22:35,280 --> 00:22:38,559
and here is where you can kind of change

00:22:37,679 --> 00:22:42,159
it

00:22:38,559 --> 00:22:43,120
to whether you use cpu whether g or your

00:22:42,159 --> 00:22:46,240
gpu

00:22:43,120 --> 00:22:48,559
or even go to tpu

00:22:46,240 --> 00:22:55,840
it shows also the folders at the top the

00:22:48,559 --> 00:22:55,840
input and the output folder um

00:22:58,559 --> 00:23:03,280
okay so i have already um just interest

00:23:02,159 --> 00:23:05,280
of time i have

00:23:03,280 --> 00:23:06,320
gone through and run this notebook so

00:23:05,280 --> 00:23:09,360
you i'll

00:23:06,320 --> 00:23:13,919
use that to talk through the

00:23:09,360 --> 00:23:16,240
uh the output um so uh i'm going to

00:23:13,919 --> 00:23:17,280
um talk through those five different

00:23:16,240 --> 00:23:20,400
steps that i

00:23:17,280 --> 00:23:23,520
uh showed you and for any

00:23:20,400 --> 00:23:25,679
machine learning model uh or any

00:23:23,520 --> 00:23:26,720
uh anytime you're you're you're building

00:23:25,679 --> 00:23:28,480
a model

00:23:26,720 --> 00:23:30,159
as as you know that the first and the

00:23:28,480 --> 00:23:33,760
foremost stuff is

00:23:30,159 --> 00:23:35,679
um getting your data set ready and

00:23:33,760 --> 00:23:39,520
that is the most time consuming time

00:23:35,679 --> 00:23:51,840
that's the most time that i have spent

00:23:39,520 --> 00:23:51,840
i'm just looking at was there a question

00:23:56,640 --> 00:24:00,720
okay if you have a question please type

00:23:58,799 --> 00:24:04,000
in

00:24:00,720 --> 00:24:06,320
um so um

00:24:04,000 --> 00:24:07,840
so what with the for this particular

00:24:06,320 --> 00:24:09,200
example what we are going to do is that

00:24:07,840 --> 00:24:12,480
we are going to use

00:24:09,200 --> 00:24:16,720
a um a

00:24:12,480 --> 00:24:16,720
c4 10 um

00:24:17,679 --> 00:24:20,880
i data set that is already available

00:24:19,679 --> 00:24:23,919
it's from the

00:24:20,880 --> 00:24:25,679
canadian canadian institute and it they

00:24:23,919 --> 00:24:29,120
have kind of tagged

00:24:25,679 --> 00:24:30,400
millions of images of different objects

00:24:29,120 --> 00:24:34,000
and c410

00:24:30,400 --> 00:24:37,600
is a is a small subset of that it has

00:24:34,000 --> 00:24:39,679
the 10 stands for the 10 um

00:24:37,600 --> 00:24:41,600
the 10 kind of classification so we are

00:24:39,679 --> 00:24:43,600
going to use those images and use a

00:24:41,600 --> 00:24:46,880
convolutional network to classify

00:24:43,600 --> 00:24:48,400
right so this is the c for 10 data set

00:24:46,880 --> 00:24:52,640
you can see that

00:24:48,400 --> 00:24:55,200
they're all in png format um there is

00:24:52,640 --> 00:24:56,400
these are the left hand side are the the

00:24:55,200 --> 00:24:59,360
10 kind of

00:24:56,400 --> 00:25:00,559
classification so given so we are going

00:24:59,360 --> 00:25:03,600
to train our

00:25:00,559 --> 00:25:07,200
cnn to use this data set

00:25:03,600 --> 00:25:10,320
uh which is around like 50 60 000 uh 10

00:25:07,200 --> 00:25:14,240
um 5000 images of each of these

00:25:10,320 --> 00:25:17,360
uh five to six thousand of each of these

00:25:14,240 --> 00:25:18,559
objects and then given a new object you

00:25:17,360 --> 00:25:20,960
should be able to

00:25:18,559 --> 00:25:21,679
find out yeah it should be able to

00:25:20,960 --> 00:25:24,080
detect

00:25:21,679 --> 00:25:25,200
whether it's a airplane or a horse for

00:25:24,080 --> 00:25:28,400
example

00:25:25,200 --> 00:25:29,360
so i already downloaded the data from

00:25:28,400 --> 00:25:31,679
the

00:25:29,360 --> 00:25:33,279
the website and it's in the zip file and

00:25:31,679 --> 00:25:36,080
i'll unzip it

00:25:33,279 --> 00:25:36,799
so first step is uh preparing the data

00:25:36,080 --> 00:25:38,960
um

00:25:36,799 --> 00:25:40,320
and as i mentioned right there are like

00:25:38,960 --> 00:25:44,880
uh

00:25:40,320 --> 00:25:45,760
50 000 to 60 000 images 50 000 are in

00:25:44,880 --> 00:25:49,120
the train and

00:25:45,760 --> 00:25:49,679
10 000 in the test region so their data

00:25:49,120 --> 00:25:52,960
set

00:25:49,679 --> 00:25:56,799
has two main folders train and test

00:25:52,960 --> 00:26:00,080
so this is all about extracting the data

00:25:56,799 --> 00:26:02,799
and then i'm because my hair output is

00:26:00,080 --> 00:26:03,360
for for the reason it is just spinning

00:26:02,799 --> 00:26:05,039
here

00:26:03,360 --> 00:26:06,720
so i thought let me just print out what

00:26:05,039 --> 00:26:09,520
are these and you can see

00:26:06,720 --> 00:26:10,720
all the different kind of the files that

00:26:09,520 --> 00:26:12,480
are there

00:26:10,720 --> 00:26:13,919
and then i have to play around to kind

00:26:12,480 --> 00:26:15,440
of just move

00:26:13,919 --> 00:26:17,520
some of the files and make some

00:26:15,440 --> 00:26:18,000
directories and all none of that is

00:26:17,520 --> 00:26:20,400
really

00:26:18,000 --> 00:26:21,679
important in this context just a lot of

00:26:20,400 --> 00:26:24,400
code

00:26:21,679 --> 00:26:25,840
to just make sure that i have everything

00:26:24,400 --> 00:26:28,240
kind of set properly

00:26:25,840 --> 00:26:29,039
and i'm just going to keep scrolling

00:26:28,240 --> 00:26:32,480
through those

00:26:29,039 --> 00:26:35,200
so i now have uh till now i have

00:26:32,480 --> 00:26:37,200
all three i have put in the training

00:26:35,200 --> 00:26:40,080
images in the right places

00:26:37,200 --> 00:26:41,679
and i just wanted to check hey like how

00:26:40,080 --> 00:26:43,120
many images are there for each one of

00:26:41,679 --> 00:26:46,000
them so as i mentioned

00:26:43,120 --> 00:26:48,159
total 50 000 images 5000 images of each

00:26:46,000 --> 00:26:51,600
one of them in the training data set

00:26:48,159 --> 00:26:52,480
same thing i do with the test and again

00:26:51,600 --> 00:26:54,880
not

00:26:52,480 --> 00:26:55,600
any important code here and the same

00:26:54,880 --> 00:26:59,360
thing that i

00:26:55,600 --> 00:27:02,080
uh print out and i have like

00:26:59,360 --> 00:27:03,440
a thousand images for again each of

00:27:02,080 --> 00:27:06,080
those in the test data set

00:27:03,440 --> 00:27:07,919
okay so it's all about just just getting

00:27:06,080 --> 00:27:10,960
the data in the right folders

00:27:07,919 --> 00:27:12,559
uh now it is in the right folders uh now

00:27:10,960 --> 00:27:15,840
we are using now all the pi

00:27:12,559 --> 00:27:17,679
by torch um uh features and frame

00:27:15,840 --> 00:27:19,919
and the framework whatever the frame

00:27:17,679 --> 00:27:21,919
title framework uh packages that it

00:27:19,919 --> 00:27:24,880
offers one of them is a torch version

00:27:21,919 --> 00:27:25,520
which is used for these kind of computer

00:27:24,880 --> 00:27:28,480
vision

00:27:25,520 --> 00:27:29,679
image classification kind of problems it

00:27:28,480 --> 00:27:33,200
provides a lot of

00:27:29,679 --> 00:27:36,320
cool utilities to load the images

00:27:33,200 --> 00:27:40,880
uh convert it into tensor etc

00:27:36,320 --> 00:27:44,320
i'm just checking the chat

00:27:40,880 --> 00:27:48,960
okay questions

00:27:44,320 --> 00:27:51,760
so um all the um so this kind of here

00:27:48,960 --> 00:27:52,799
you kind of load your training data set

00:27:51,760 --> 00:27:54,960
um

00:27:52,799 --> 00:27:56,080
all the images into into this data set

00:27:54,960 --> 00:27:59,440
variable

00:27:56,080 --> 00:28:01,440
and then uh every time you kind of

00:27:59,440 --> 00:28:02,480
before you start really getting into the

00:28:01,440 --> 00:28:05,760
depth of it

00:28:02,480 --> 00:28:08,640
uh into like creating the models and not

00:28:05,760 --> 00:28:11,120
make it uh just understand what is going

00:28:08,640 --> 00:28:11,600
on in in in the dataset play around with

00:28:11,120 --> 00:28:13,679
it

00:28:11,600 --> 00:28:15,120
and look at it so that's what i'm doing

00:28:13,679 --> 00:28:17,360
here understanding

00:28:15,120 --> 00:28:18,960
what are these data sets they are 32 by

00:28:17,360 --> 00:28:20,720
32 pixel images

00:28:18,960 --> 00:28:22,000
um and then the three stands for the

00:28:20,720 --> 00:28:24,480
channels for the red

00:28:22,000 --> 00:28:25,120
green blue rgb as you know in every

00:28:24,480 --> 00:28:29,039
image

00:28:25,120 --> 00:28:31,279
ultimately it's amazing matrix

00:28:29,039 --> 00:28:32,720
and those are the three levels of

00:28:31,279 --> 00:28:35,440
matrices for

00:28:32,720 --> 00:28:36,000
um each standing for the red green and

00:28:35,440 --> 00:28:38,000
blue

00:28:36,000 --> 00:28:40,080
because this is a color image and i kind

00:28:38,000 --> 00:28:43,039
of just print out it

00:28:40,080 --> 00:28:44,000
as i mentioned the default is all a

00:28:43,039 --> 00:28:46,559
tensor

00:28:44,000 --> 00:28:47,360
again nothing really to understand here

00:28:46,559 --> 00:28:49,840
i'm just

00:28:47,360 --> 00:28:51,760
really looking at it and confirming my

00:28:49,840 --> 00:28:54,799
understanding of this

00:28:51,760 --> 00:28:57,520
this data set and then

00:28:54,799 --> 00:28:58,480
just again if i look at it just plotting

00:28:57,520 --> 00:29:01,039
it using the

00:28:58,480 --> 00:29:01,919
matplotlib and just saying hey what is

00:29:01,039 --> 00:29:04,240
it

00:29:01,919 --> 00:29:05,679
this is aeroplane and you can see that

00:29:04,240 --> 00:29:09,120
this data set is not

00:29:05,679 --> 00:29:12,240
really that easy

00:29:09,120 --> 00:29:12,799
that it has this is not that something

00:29:12,240 --> 00:29:14,799
has

00:29:12,799 --> 00:29:15,840
happened with the resolution on my

00:29:14,799 --> 00:29:18,399
machine it is

00:29:15,840 --> 00:29:19,200
this image it is blurred because you are

00:29:18,399 --> 00:29:22,559
really

00:29:19,200 --> 00:29:25,039
um trying to uh train the model

00:29:22,559 --> 00:29:26,960
your and create a model uh to really

00:29:25,039 --> 00:29:27,919
understand like real world images which

00:29:26,960 --> 00:29:30,559
are not going to be all

00:29:27,919 --> 00:29:31,840
nice and crisp right so you can see the

00:29:30,559 --> 00:29:35,279
airplane like you can

00:29:31,840 --> 00:29:36,640
really just see um barely you can see

00:29:35,279 --> 00:29:39,919
another another image

00:29:36,640 --> 00:29:41,200
again just just understanding it now

00:29:39,919 --> 00:29:43,760
getting into

00:29:41,200 --> 00:29:45,200
like splitting the data sets right one

00:29:43,760 --> 00:29:48,240
of the most important

00:29:45,200 --> 00:29:51,360
uh step um is creating the training and

00:29:48,240 --> 00:29:54,399
validation set and usually 90 10

00:29:51,360 --> 00:29:58,080
percent uh you do um

00:29:54,399 --> 00:30:01,760
95 um 90

00:29:58,080 --> 00:30:05,520
percent for your training 10 for your

00:30:01,760 --> 00:30:09,440
validation so you use your training

00:30:05,520 --> 00:30:13,120
your data set and you split it

00:30:09,440 --> 00:30:15,440
so that as you build your model and you

00:30:13,120 --> 00:30:15,440
start

00:30:15,520 --> 00:30:21,120
training it you have some validation

00:30:18,720 --> 00:30:22,240
that happens along with that so that you

00:30:21,120 --> 00:30:24,880
kind of understand

00:30:22,240 --> 00:30:25,840
how your model is performing and whether

00:30:24,880 --> 00:30:28,240
it is really

00:30:25,840 --> 00:30:29,760
performing good uh how much accuracy it

00:30:28,240 --> 00:30:32,880
has before you

00:30:29,760 --> 00:30:36,640
apply it on a test data set so

00:30:32,880 --> 00:30:38,880
you split it 90 to 10 so 45 thousand for

00:30:36,640 --> 00:30:39,679
um your training and five thousand for

00:30:38,880 --> 00:30:42,960
your

00:30:39,679 --> 00:30:45,360
validation data set and um

00:30:42,960 --> 00:30:46,559
one of the things um another uh

00:30:45,360 --> 00:30:49,679
important things that you

00:30:46,559 --> 00:30:53,840
usually do building this models is that

00:30:49,679 --> 00:30:57,120
you use the concept of batches so you

00:30:53,840 --> 00:30:59,440
you create from your your

00:30:57,120 --> 00:31:01,519
data set the training data set you

00:30:59,440 --> 00:31:03,279
create different batches and i'm saying

00:31:01,519 --> 00:31:06,240
create 128 batches

00:31:03,279 --> 00:31:07,200
uh or a batch size each batch size of

00:31:06,240 --> 00:31:09,600
00:31:07,200 --> 00:31:10,880
so if i order 5000 divided by 128 would

00:31:09,600 --> 00:31:14,000
be the number of batches

00:31:10,880 --> 00:31:16,320
and that you do is just so that

00:31:14,000 --> 00:31:17,360
it is more efficient and you don't have

00:31:16,320 --> 00:31:19,440
too many things

00:31:17,360 --> 00:31:20,559
that you're holding in the memory um so

00:31:19,440 --> 00:31:22,799
when you're

00:31:20,559 --> 00:31:24,480
trying to train millions of images you

00:31:22,799 --> 00:31:26,480
don't want to load everything

00:31:24,480 --> 00:31:27,840
at the same time you put it in batches

00:31:26,480 --> 00:31:30,640
and kind of load it so

00:31:27,840 --> 00:31:32,240
that's what is happening here um there's

00:31:30,640 --> 00:31:35,279
a question so

00:31:32,240 --> 00:31:37,200
um so the question is can you please

00:31:35,279 --> 00:31:38,399
explain how defining model depending on

00:31:37,200 --> 00:31:42,000
the prepared

00:31:38,399 --> 00:31:44,159
uh data set so um i

00:31:42,000 --> 00:31:46,159
trying to understand more context but it

00:31:44,159 --> 00:31:49,120
is not um

00:31:46,159 --> 00:31:50,880
the uh the defining the model if you are

00:31:49,120 --> 00:31:52,880
like i'm going to go there and in just a

00:31:50,880 --> 00:31:53,679
few minutes of like convolutional neural

00:31:52,880 --> 00:31:56,960
network

00:31:53,679 --> 00:31:57,360
and it is not going to be um depending

00:31:56,960 --> 00:32:00,799
on

00:31:57,360 --> 00:32:04,240
the the data i think it is very uh com

00:32:00,799 --> 00:32:06,399
it is the same for um any type of

00:32:04,240 --> 00:32:08,000
neural network that you are defining so

00:32:06,399 --> 00:32:10,240
just going through

00:32:08,000 --> 00:32:12,159
uh this and seeing the batches okay this

00:32:10,240 --> 00:32:13,440
is my batch of 128 and

00:32:12,159 --> 00:32:15,279
you see all the different types of

00:32:13,440 --> 00:32:18,320
images it's all randomized

00:32:15,279 --> 00:32:20,480
um you you want to randomize that

00:32:18,320 --> 00:32:22,480
um and then now you start getting into

00:32:20,480 --> 00:32:24,320
the defining the cnn and the

00:32:22,480 --> 00:32:25,279
convolutional network right so and you

00:32:24,320 --> 00:32:28,159
can see

00:32:25,279 --> 00:32:30,240
um the different steps in the

00:32:28,159 --> 00:32:32,320
convolutional neural network and i know

00:32:30,240 --> 00:32:33,440
uh not everybody is familiar with it so

00:32:32,320 --> 00:32:36,799
i'm going to

00:32:33,440 --> 00:32:38,640
be like try as my best in the next 10 15

00:32:36,799 --> 00:32:41,519
minutes 15 minutes that i have

00:32:38,640 --> 00:32:42,880
to go through it i think it is more very

00:32:41,519 --> 00:32:45,679
important to kind of

00:32:42,880 --> 00:32:47,679
know at a high level uh but it is once

00:32:45,679 --> 00:32:50,799
you know it it is very intuitive and

00:32:47,679 --> 00:32:51,279
um you really don't need to uh you you

00:32:50,799 --> 00:32:54,799
you

00:32:51,279 --> 00:32:56,080
you um once you understand it

00:32:54,799 --> 00:32:58,640
intuitively it is very easy

00:32:56,080 --> 00:32:59,840
to then work with the code so first step

00:32:58,640 --> 00:33:00,799
is there are three steps in the

00:32:59,840 --> 00:33:03,039
convolutional

00:33:00,799 --> 00:33:05,200
neural network first is what is called

00:33:03,039 --> 00:33:08,720
the convolution right so this is really

00:33:05,200 --> 00:33:11,519
um i i have these images

00:33:08,720 --> 00:33:14,000
that i use this is kind of graphical

00:33:11,519 --> 00:33:17,519
dynamic images so it kind of shows you

00:33:14,000 --> 00:33:19,440
how you have a a

00:33:17,519 --> 00:33:21,760
you can think of this as an image which

00:33:19,440 --> 00:33:24,000
is just nothing but pixel

00:33:21,760 --> 00:33:25,120
different pixel values and then you kind

00:33:24,000 --> 00:33:28,399
of move

00:33:25,120 --> 00:33:31,919
a slider uh the 3x3 slider the

00:33:28,399 --> 00:33:32,960
represented by kind of yellow color that

00:33:31,919 --> 00:33:35,840
is sliding on it

00:33:32,960 --> 00:33:37,600
and you find the result what in real

00:33:35,840 --> 00:33:38,159
world what that means when it does is

00:33:37,600 --> 00:33:41,279
that

00:33:38,159 --> 00:33:42,799
it is really what it is doing is it is

00:33:41,279 --> 00:33:44,799
what it is trying to detect is the

00:33:42,799 --> 00:33:46,000
features of that image the features of

00:33:44,799 --> 00:33:48,080
the image could be

00:33:46,000 --> 00:33:49,519
the edge of an image but maybe there are

00:33:48,080 --> 00:33:52,559
curves in the image

00:33:49,519 --> 00:33:55,919
um all those things that kind of

00:33:52,559 --> 00:33:58,480
make that image uh there might be some

00:33:55,919 --> 00:33:59,120
elliptical shape in it that is what

00:33:58,480 --> 00:34:02,399
these

00:33:59,120 --> 00:34:04,320
uh convolutions do and that

00:34:02,399 --> 00:34:06,399
you kind of what it helps is you kind of

00:34:04,320 --> 00:34:10,079
start understanding what

00:34:06,399 --> 00:34:13,919
this image consists of so in real world

00:34:10,079 --> 00:34:17,119
if i look at this you can see that um

00:34:13,919 --> 00:34:19,679
there is this kind of downturn image and

00:34:17,119 --> 00:34:21,440
you have a slider here which if you can

00:34:19,679 --> 00:34:24,399
see the red one is kind of

00:34:21,440 --> 00:34:25,919
uh showing you one edge one tilted edge

00:34:24,399 --> 00:34:26,800
and the green one kind of the other

00:34:25,919 --> 00:34:29,599
tilted edge

00:34:26,800 --> 00:34:30,800
and as it goes over it is really

00:34:29,599 --> 00:34:35,440
understanding

00:34:30,800 --> 00:34:38,159
um those the the the those kind of

00:34:35,440 --> 00:34:39,040
vertical lines uh in the image so you

00:34:38,159 --> 00:34:41,520
can see

00:34:39,040 --> 00:34:42,320
the resultant image you are only seeing

00:34:41,520 --> 00:34:44,240
that

00:34:42,320 --> 00:34:45,440
those vertical lines and and not other

00:34:44,240 --> 00:34:48,720
things so what it is trying

00:34:45,440 --> 00:34:50,399
what it is helping you to do is that in

00:34:48,720 --> 00:34:52,639
in mathematical terms and

00:34:50,399 --> 00:34:55,440
for your computer and for your gpu it's

00:34:52,639 --> 00:34:58,240
just helping you to

00:34:55,440 --> 00:34:59,599
really understand image in terms of its

00:34:58,240 --> 00:35:02,000
spatial

00:34:59,599 --> 00:35:03,920
kind of configuration and kind of

00:35:02,000 --> 00:35:06,880
relationship

00:35:03,920 --> 00:35:06,880
so um

00:35:09,920 --> 00:35:15,680
so if i move to the next step

00:35:13,040 --> 00:35:17,520
and this is just another i'm just

00:35:15,680 --> 00:35:19,680
playing with the code and seeing

00:35:17,520 --> 00:35:21,760
how can i apply this kind of convolution

00:35:19,680 --> 00:35:24,640
and see what the printout is again

00:35:21,760 --> 00:35:25,599
not um nothing much to understand there

00:35:24,640 --> 00:35:29,040
here

00:35:25,599 --> 00:35:31,599
um the second other steps

00:35:29,040 --> 00:35:32,240
in in convolution is something called

00:35:31,599 --> 00:35:35,280
rectifier

00:35:32,240 --> 00:35:35,920
uh linear unit or value right so what it

00:35:35,280 --> 00:35:39,599
is it is

00:35:35,920 --> 00:35:41,599
it brings the uh non-linearity into cnn

00:35:39,599 --> 00:35:43,359
in the network right so what it does is

00:35:41,599 --> 00:35:45,520
that all negative values

00:35:43,359 --> 00:35:46,800
that you get from the calculations from

00:35:45,520 --> 00:35:49,920
your convolutions

00:35:46,800 --> 00:35:52,000
you use value function and

00:35:49,920 --> 00:35:54,320
anything that is negative you make it

00:35:52,000 --> 00:35:56,880
zero uh anything that is positive

00:35:54,320 --> 00:35:57,520
you keep it and and what it is it is

00:35:56,880 --> 00:35:59,599
just

00:35:57,520 --> 00:36:00,800
bringing brings and the non-linearity

00:35:59,599 --> 00:36:04,000
because if you

00:36:00,800 --> 00:36:06,160
uh if you don't do that then you what

00:36:04,000 --> 00:36:08,560
basically you are saying is that

00:36:06,160 --> 00:36:10,400
uh you you're telling your model and you

00:36:08,560 --> 00:36:11,040
the architecture of your neural network

00:36:10,400 --> 00:36:14,079
is that

00:36:11,040 --> 00:36:14,880
everything is linear your what your your

00:36:14,079 --> 00:36:16,960
input is

00:36:14,880 --> 00:36:18,400
the given and the output there is a

00:36:16,960 --> 00:36:19,920
complete linearity

00:36:18,400 --> 00:36:21,599
from the input output which is not

00:36:19,920 --> 00:36:23,440
always the case right so

00:36:21,599 --> 00:36:27,040
hence um you have to use something like

00:36:23,440 --> 00:36:30,079
radio and again the same thing um

00:36:27,040 --> 00:36:33,040
you can see that um whatever is the

00:36:30,079 --> 00:36:34,160
the negative is is removed after value

00:36:33,040 --> 00:36:36,320
and you only see

00:36:34,160 --> 00:36:37,280
the non-record and you can see how it

00:36:36,320 --> 00:36:39,520
changes and

00:36:37,280 --> 00:36:40,880
it shows the kind of again the spatial

00:36:39,520 --> 00:36:44,320
kind of configuration

00:36:40,880 --> 00:36:46,240
but not all the details uh

00:36:44,320 --> 00:36:49,359
the other thing is called spatial

00:36:46,240 --> 00:36:52,160
pooling what that does is

00:36:49,359 --> 00:36:52,800
um again what this is to simplify your

00:36:52,160 --> 00:36:56,400
model

00:36:52,800 --> 00:37:00,240
as you can imagine you have the 30 by

00:36:56,400 --> 00:37:02,480
2 by 32 pixel you multiply with the 3x3

00:37:00,240 --> 00:37:04,480
matrices your three channels you're

00:37:02,480 --> 00:37:06,720
going to get large set of numbers

00:37:04,480 --> 00:37:09,280
across all your data set right and you

00:37:06,720 --> 00:37:11,200
want it's going to bring in complexity

00:37:09,280 --> 00:37:12,560
so can you reduce that complexity so

00:37:11,200 --> 00:37:13,520
what it does is that you look at the

00:37:12,560 --> 00:37:16,160
matrix

00:37:13,520 --> 00:37:17,839
and you there are different ways you can

00:37:16,160 --> 00:37:19,200
just take the max values in this case

00:37:17,839 --> 00:37:21,599
the max value is 6

00:37:19,200 --> 00:37:23,200
in this part the max value is 8 three

00:37:21,599 --> 00:37:24,079
four etcetera you can even take the

00:37:23,200 --> 00:37:27,040
average

00:37:24,079 --> 00:37:28,240
uh what it is doing is that it you're

00:37:27,040 --> 00:37:31,280
losing some

00:37:28,240 --> 00:37:34,720
information but you are still keeping

00:37:31,280 --> 00:37:38,079
what is the most in in max case

00:37:34,720 --> 00:37:40,640
max pooling you uh strategy if you use

00:37:38,079 --> 00:37:41,359
you keep still keeping the the the top

00:37:40,640 --> 00:37:44,400
the

00:37:41,359 --> 00:37:45,280
the main kind of value there so if

00:37:44,400 --> 00:37:47,359
something is

00:37:45,280 --> 00:37:48,880
more darker you are going to probably in

00:37:47,359 --> 00:37:52,320
the image you are going to keep

00:37:48,880 --> 00:37:55,599
that rather than everything so um

00:37:52,320 --> 00:37:56,079
again the same feature if you look at

00:37:55,599 --> 00:37:58,640
this

00:37:56,079 --> 00:37:59,200
after you're done the relu you do the

00:37:58,640 --> 00:38:02,880
max

00:37:59,200 --> 00:38:07,119
or if you just did the average you can

00:38:02,880 --> 00:38:10,800
see what pixels you gather and

00:38:07,119 --> 00:38:12,880
all of this is again just to be able to

00:38:10,800 --> 00:38:14,880
make it simpler and easier for your

00:38:12,880 --> 00:38:18,400
calculations

00:38:14,880 --> 00:38:18,800
the fully connected layer what it is

00:38:18,400 --> 00:38:20,880
called

00:38:18,800 --> 00:38:22,480
as part of the cnn architecture is

00:38:20,880 --> 00:38:24,400
really um

00:38:22,480 --> 00:38:26,800
the you you have all these now high

00:38:24,400 --> 00:38:29,280
level features that you have calculated

00:38:26,800 --> 00:38:30,160
using your convolution max the neural

00:38:29,280 --> 00:38:32,640
network so

00:38:30,160 --> 00:38:33,920
if it is the image of a of a dog you

00:38:32,640 --> 00:38:35,520
kind of know

00:38:33,920 --> 00:38:37,440
of a cat you know there is there are

00:38:35,520 --> 00:38:39,520
some whiskers there is a

00:38:37,440 --> 00:38:40,480
particular type of nose and eyes and you

00:38:39,520 --> 00:38:43,440
kind of

00:38:40,480 --> 00:38:45,119
have that spatial understanding of it

00:38:43,440 --> 00:38:47,520
now you're trying to connect it

00:38:45,119 --> 00:38:49,200
all together to kind of recognize okay

00:38:47,520 --> 00:38:51,599
this is a cat versus

00:38:49,200 --> 00:38:53,280
a if this is kind of the spatial

00:38:51,599 --> 00:38:54,000
relationship then this is a dog and

00:38:53,280 --> 00:38:57,359
that's

00:38:54,000 --> 00:38:59,440
kind of what is called the fully trained

00:38:57,359 --> 00:39:00,400
connected network there are other

00:38:59,440 --> 00:39:03,440
concepts like

00:39:00,400 --> 00:39:05,040
dropout and all which are which gets

00:39:03,440 --> 00:39:05,760
into something called overfitting and

00:39:05,040 --> 00:39:08,079
all

00:39:05,760 --> 00:39:10,000
if i get time i'll get into it but i'm

00:39:08,079 --> 00:39:13,839
going to start showing some code

00:39:10,000 --> 00:39:16,079
here um i'm just checking it

00:39:13,839 --> 00:39:16,079
is a

00:39:18,800 --> 00:39:26,079
okay um so

00:39:21,839 --> 00:39:29,280
the code you can see it um i'm

00:39:26,079 --> 00:39:30,560
instantiating a neural network and i'm

00:39:29,280 --> 00:39:33,520
creating now

00:39:30,560 --> 00:39:34,720
a a say i'm using object-oriented

00:39:33,520 --> 00:39:37,119
programming here

00:39:34,720 --> 00:39:38,960
defining some classes for kind of

00:39:37,119 --> 00:39:40,560
validating and all those things just

00:39:38,960 --> 00:39:43,839
some helper functions

00:39:40,560 --> 00:39:45,200
but here is where is the class where i

00:39:43,839 --> 00:39:48,240
am defining my

00:39:45,200 --> 00:39:50,560
convolutional network what you see here

00:39:48,240 --> 00:39:52,560
is your convolutional network so what

00:39:50,560 --> 00:39:56,160
are 10 15 lines of code

00:39:52,560 --> 00:39:58,160
is cnn and so very easy to instantiate

00:39:56,160 --> 00:39:59,920
it and you can see that there are

00:39:58,160 --> 00:40:02,880
multiple layers so each

00:39:59,920 --> 00:40:03,760
convolutional network convolution what

00:40:02,880 --> 00:40:06,720
you see is the con

00:40:03,760 --> 00:40:07,839
2d is one of the layers and so what you

00:40:06,720 --> 00:40:10,160
do is that you

00:40:07,839 --> 00:40:11,520
kind of keep going through these

00:40:10,160 --> 00:40:14,480
different layers

00:40:11,520 --> 00:40:16,000
uh and intuitively or at a at a high

00:40:14,480 --> 00:40:17,200
abstraction level what that means is

00:40:16,000 --> 00:40:20,480
that you really

00:40:17,200 --> 00:40:24,079
each time you're passing your train

00:40:20,480 --> 00:40:26,560
your data set through that convolution

00:40:24,079 --> 00:40:27,440
you're identifying different features so

00:40:26,560 --> 00:40:28,800
the first

00:40:27,440 --> 00:40:30,960
uh when you pass through it you're

00:40:28,800 --> 00:40:31,839
probably maybe identifying hey are there

00:40:30,960 --> 00:40:34,640
like vertical

00:40:31,839 --> 00:40:35,599
edges hey when you pass it again you're

00:40:34,640 --> 00:40:37,520
going to okay

00:40:35,599 --> 00:40:38,720
i think i know kind of the vertical and

00:40:37,520 --> 00:40:42,240
horizontal edges

00:40:38,720 --> 00:40:45,440
i want to see uh are there curves here

00:40:42,240 --> 00:40:47,839
um are there some other kind of uh

00:40:45,440 --> 00:40:48,880
slanted lines and all those like i'm

00:40:47,839 --> 00:40:51,920
talking in terms of

00:40:48,880 --> 00:40:54,079
images uh and that is what each uh

00:40:51,920 --> 00:40:56,000
of this convolution network does and you

00:40:54,079 --> 00:40:58,000
you're going to the same step

00:40:56,000 --> 00:41:00,400
go through the conversion network make

00:40:58,000 --> 00:41:02,560
it non-linear using relu

00:41:00,400 --> 00:41:03,760
you do your max pooling to reduce your

00:41:02,560 --> 00:41:06,960
complexity

00:41:03,760 --> 00:41:10,400
and you can see that the the size of it

00:41:06,960 --> 00:41:13,839
uh size of it keeps increasing this

00:41:10,400 --> 00:41:17,520
it goes decreasing but size of it keeps

00:41:13,839 --> 00:41:17,520
the number of layers keeps increasing

00:41:19,440 --> 00:41:22,960
anybody wanted to mention anything

00:41:23,200 --> 00:41:29,599
okay so that is it and then you um

00:41:26,640 --> 00:41:30,079
uh kind of then tie it all together here

00:41:29,599 --> 00:41:32,560
the

00:41:30,079 --> 00:41:34,160
the fully connected network the 10 year

00:41:32,560 --> 00:41:37,200
stands for like the 10

00:41:34,160 --> 00:41:38,800
um different data sets that you have uh

00:41:37,200 --> 00:41:40,880
you instantiate the model

00:41:38,800 --> 00:41:42,880
uh here that this is this is the my

00:41:40,880 --> 00:41:46,560
class here which i have defined

00:41:42,880 --> 00:41:48,960
right and so i instantiate it

00:41:46,560 --> 00:41:51,680
and i just check what it uh print it out

00:41:48,960 --> 00:41:55,680
and it's the same thing that we just saw

00:41:51,680 --> 00:41:58,800
um and then i pass it past my training

00:41:55,680 --> 00:41:59,280
i train it and here is where i'm going

00:41:58,800 --> 00:42:02,560
through the

00:41:59,280 --> 00:42:04,079
different images i'm confirming that i

00:42:02,560 --> 00:42:06,480
have my gpu is set

00:42:04,079 --> 00:42:08,079
i just have some default code for that

00:42:06,480 --> 00:42:12,480
just to confirm that

00:42:08,079 --> 00:42:16,000
and say it says that i have my gpu here

00:42:12,480 --> 00:42:19,359
and i then start

00:42:16,000 --> 00:42:22,000
actually uh training the model um

00:42:19,359 --> 00:42:23,440
using by calling the train feature and

00:42:22,000 --> 00:42:26,880
then looking at the losses

00:42:23,440 --> 00:42:30,319
that the model has for each of the batch

00:42:26,880 --> 00:42:32,800
and then go and update uh

00:42:30,319 --> 00:42:34,400
the the values uh that that that are

00:42:32,800 --> 00:42:34,640
used in the convolutional neural network

00:42:34,400 --> 00:42:37,280
the

00:42:34,640 --> 00:42:39,040
matrices that you see in the convolution

00:42:37,280 --> 00:42:39,440
it starts with some default like one

00:42:39,040 --> 00:42:41,920
zero

00:42:39,440 --> 00:42:43,040
zero zero one zero and then it will keep

00:42:41,920 --> 00:42:46,640
updating it

00:42:43,040 --> 00:42:50,960
uh as it tries to optimize um

00:42:46,640 --> 00:42:50,960
to to the the actual

00:42:51,280 --> 00:42:54,319
thing the image that we are trying to

00:42:52,880 --> 00:42:56,560
identify uh

00:42:54,319 --> 00:42:57,839
and we are we are using this um the

00:42:56,560 --> 00:42:59,599
gradient descent

00:42:57,839 --> 00:43:01,200
again don't have time to go through that

00:42:59,599 --> 00:43:04,160
but it's a classic way

00:43:01,200 --> 00:43:05,119
uh to optimize a machine learning model

00:43:04,160 --> 00:43:08,319
um

00:43:05,119 --> 00:43:11,760
and once i have now uh trained it

00:43:08,319 --> 00:43:13,119
and i'm evaluating it i have

00:43:11,760 --> 00:43:14,800
sorry i have not yet trained it i've

00:43:13,119 --> 00:43:17,680
just defined it at the top

00:43:14,800 --> 00:43:18,400
uh without just training just by default

00:43:17,680 --> 00:43:22,000
values

00:43:18,400 --> 00:43:24,640
i have like uh just nine percent

00:43:22,000 --> 00:43:26,000
accuracy here so one in ten times it

00:43:24,640 --> 00:43:30,160
will identify

00:43:26,000 --> 00:43:33,599
but then i strain it

00:43:30,160 --> 00:43:36,640
the model and the

00:43:33,599 --> 00:43:37,119
epochs are the number of epics is 10

00:43:36,640 --> 00:43:38,960
each

00:43:37,119 --> 00:43:40,960
effect means it is going through the

00:43:38,960 --> 00:43:44,000
whole training data set

00:43:40,960 --> 00:43:45,839
every epic um uh so

00:43:44,000 --> 00:43:47,520
it goes through one data the whole

00:43:45,839 --> 00:43:51,040
training data set

00:43:47,520 --> 00:43:53,680
um modifying the values the accuracy is

00:43:51,040 --> 00:43:56,640
has jumped here just from 10 percent now

00:43:53,680 --> 00:44:00,079
to 48 percent

00:43:56,640 --> 00:44:02,800
we adjust the values it keeps on jumping

00:44:00,079 --> 00:44:03,440
and you go through that so multiple

00:44:02,800 --> 00:44:06,720
times

00:44:03,440 --> 00:44:09,520
and and there is no set value

00:44:06,720 --> 00:44:10,960
for epec it is really uh you really need

00:44:09,520 --> 00:44:13,839
to play around with it

00:44:10,960 --> 00:44:14,400
you can see here that at around like 9

00:44:13,839 --> 00:44:17,520
or 10

00:44:14,400 --> 00:44:18,400
it is already at max 76 that is all we

00:44:17,520 --> 00:44:20,160
can achieve

00:44:18,400 --> 00:44:21,680
with this neural network and i left it

00:44:20,160 --> 00:44:23,359
at that just to show that

00:44:21,680 --> 00:44:25,920
you need to play around with it i kind

00:44:23,359 --> 00:44:27,839
of uh drew the graph of it that it

00:44:25,920 --> 00:44:29,200
starts from like very low 10 percent

00:44:27,839 --> 00:44:33,680
goes to

00:44:29,200 --> 00:44:35,440
around 75 percent and and plateaus there

00:44:33,680 --> 00:44:37,040
and here is the training and the

00:44:35,440 --> 00:44:39,839
validation loss

00:44:37,040 --> 00:44:42,079
so that you can see here my training

00:44:39,839 --> 00:44:44,880
loss is continuously decreasing

00:44:42,079 --> 00:44:46,880
but my validation it decreases first and

00:44:44,880 --> 00:44:48,800
then it kind of plateaus and then starts

00:44:46,880 --> 00:44:51,040
in fact increasing

00:44:48,800 --> 00:44:52,560
this is what is called really under

00:44:51,040 --> 00:44:54,400
fitting which means what it is telling

00:44:52,560 --> 00:44:55,280
is that my model is good for my training

00:44:54,400 --> 00:44:57,839
data set

00:44:55,280 --> 00:44:59,280
which is not good for my validation data

00:44:57,839 --> 00:45:03,040
set and i need to

00:44:59,280 --> 00:45:04,880
continue to uh make it better

00:45:03,040 --> 00:45:06,079
and then i just test it with some

00:45:04,880 --> 00:45:08,720
different um

00:45:06,079 --> 00:45:09,680
just try it out and i tried with some

00:45:08,720 --> 00:45:11,839
then

00:45:09,680 --> 00:45:13,520
uh one of them for horse it is

00:45:11,839 --> 00:45:15,760
predicting as an airplane again it is

00:45:13,520 --> 00:45:18,240
only 75 percent accurate so it's never

00:45:15,760 --> 00:45:20,079
will not be accurate for everything but

00:45:18,240 --> 00:45:21,760
anyway i kind of

00:45:20,079 --> 00:45:24,560
went through the last section in the

00:45:21,760 --> 00:45:27,839
last 10 15 minutes but i hope i gave you

00:45:24,560 --> 00:45:31,119
enough idea about

00:45:27,839 --> 00:45:32,880
pie torch how it is used and how easy it

00:45:31,119 --> 00:45:34,720
is for you to play around with

00:45:32,880 --> 00:45:35,920
the available data sets and just within

00:45:34,720 --> 00:45:37,839
a few lines of code

00:45:35,920 --> 00:45:41,680
you are able to build a neural network

00:45:37,839 --> 00:45:41,680
and able to start predicting

00:45:41,760 --> 00:45:46,640
so that's all i have um given the time

00:45:44,839 --> 00:45:50,560
um

00:45:46,640 --> 00:45:52,960
so i'm just looking at the questions uh

00:45:50,560 --> 00:45:54,240
so one of the questions here is uh how

00:45:52,960 --> 00:45:57,119
do you decide which

00:45:54,240 --> 00:45:59,280
all functions like relu fully need to

00:45:57,119 --> 00:46:03,599
use

00:45:59,280 --> 00:46:05,760
anatram so that's uh is the standard

00:46:03,599 --> 00:46:07,680
convolution neural network architecture

00:46:05,760 --> 00:46:10,079
so as part of neural networks

00:46:07,680 --> 00:46:10,880
this is that's what i showed you that

00:46:10,079 --> 00:46:13,839
those ten

00:46:10,880 --> 00:46:15,760
lines and the different layers that is

00:46:13,839 --> 00:46:16,640
the default so you will always use that

00:46:15,760 --> 00:46:18,560
whenever you

00:46:16,640 --> 00:46:20,560
are using a deep learning network

00:46:18,560 --> 00:46:24,079
without that you will not even get

00:46:20,560 --> 00:46:25,599
even if uh even even a few percentage

00:46:24,079 --> 00:46:27,280
accurate accuracy

00:46:25,599 --> 00:46:35,760
uh even the ten percent we saw you will

00:46:27,280 --> 00:46:35,760

YouTube URL: https://www.youtube.com/watch?v=-4WSm-bCFI4


