Title: AI Bias Testing and Mitigation: How to Choose the Right Open Source Tools - Melinda Thielbar
Publication date: 2020-12-14
Playlist: All Things Open 2020 - Machine Learning AI Track
Description: 
	Presented by: Melinda Thielbar, Fidelity Investments
Presented at All Things Open 2020 - Machine Learning/AI Track

Abstract: Most AI researchers acknowledge that AI models can “learn” unwanted biases, but there is very little guidance on how to actually implement bias testing or what to do when bias is found. This talk will focus on how to choose the right bias tests for your model, and how to de-bias an AI model that fails those tests.  We’ll compare and contrast three major open source software packages for AI bias testing and mitigation: AIF 360 from IBM, Google’s What-If Tool, and Audit AI from Pymetrics.
Captions: 
	00:00:05,440 --> 00:00:10,000
thank you my name is melinda thialbar

00:00:07,440 --> 00:00:11,599
i work for fidelity investments but i am

00:00:10,000 --> 00:00:12,880
i'm speaking as a melinda thielbar

00:00:11,599 --> 00:00:14,799
private citizen

00:00:12,880 --> 00:00:16,960
and today we're going to talk about open

00:00:14,799 --> 00:00:18,400
source tools for ai bias detection and

00:00:16,960 --> 00:00:22,240
mitigation

00:00:18,400 --> 00:00:24,400
um it's traditional to start these talks

00:00:22,240 --> 00:00:26,880
by discussing your own biases

00:00:24,400 --> 00:00:27,920
i'm a phd station a statistician and a

00:00:26,880 --> 00:00:29,840
labor economist

00:00:27,920 --> 00:00:32,480
and what that means is i have some very

00:00:29,840 --> 00:00:35,600
strong fact-based opinions

00:00:32,480 --> 00:00:36,719
about how people should be treated both

00:00:35,600 --> 00:00:40,000
as consumers

00:00:36,719 --> 00:00:41,680
and as employees in our economy i'm also

00:00:40,000 --> 00:00:43,680
a software developer that means i have

00:00:41,680 --> 00:00:45,680
some insights into how to build software

00:00:43,680 --> 00:00:48,320
systems and the purpose of them

00:00:45,680 --> 00:00:50,239
my pronouns are she her and hers and i

00:00:48,320 --> 00:00:53,039
grew up on a farm in the midwest

00:00:50,239 --> 00:00:55,440
now what that now the traditional kind

00:00:53,039 --> 00:00:57,520
of vision of people who care about ai

00:00:55,440 --> 00:01:00,800
ethics and ai bias is this

00:00:57,520 --> 00:01:03,199
kind of urban liberal

00:01:00,800 --> 00:01:05,280
ivy league sort of person i guarantee

00:01:03,199 --> 00:01:07,520
you that was not how i was raised and

00:01:05,280 --> 00:01:10,159
that is not the world i come from

00:01:07,520 --> 00:01:11,200
but i am also an artist and one of the

00:01:10,159 --> 00:01:14,799
things that you learn

00:01:11,200 --> 00:01:16,720
in art is this idea of precognition it's

00:01:14,799 --> 00:01:18,640
what your brain decides these are

00:01:16,720 --> 00:01:19,439
decisions your brain makes before you're

00:01:18,640 --> 00:01:21,520
even aware

00:01:19,439 --> 00:01:24,000
that you're thinking about what you're

00:01:21,520 --> 00:01:25,600
seeing and so when i do talks like these

00:01:24,000 --> 00:01:28,080
i'm five foot tall

00:01:25,600 --> 00:01:30,240
i have giant blue eyes and sometimes i

00:01:28,080 --> 00:01:31,920
really feel like what people are seeing

00:01:30,240 --> 00:01:33,920
instead of the picture you had before

00:01:31,920 --> 00:01:36,079
it's more like this right

00:01:33,920 --> 00:01:37,920
um and i've dealt with that all my life

00:01:36,079 --> 00:01:40,720
and that does color my judgment

00:01:37,920 --> 00:01:41,680
just a tiny tiny bit okay what's our

00:01:40,720 --> 00:01:43,840
purpose today

00:01:41,680 --> 00:01:45,040
i want to get you started on your ai

00:01:43,840 --> 00:01:46,880
ethics journey

00:01:45,040 --> 00:01:48,079
we're going to talk a little bit about

00:01:46,880 --> 00:01:50,000
bias detection

00:01:48,079 --> 00:01:52,720
as a concept we're going to give some

00:01:50,000 --> 00:01:54,560
examples of some bias detection tools

00:01:52,720 --> 00:01:56,479
and what you should leave with is an

00:01:54,560 --> 00:01:59,280
idea of where you want to start

00:01:56,479 --> 00:02:02,159
depending on where you are kind of

00:01:59,280 --> 00:02:04,719
within your artificial intelligence

00:02:02,159 --> 00:02:06,240
journey as a business or an organization

00:02:04,719 --> 00:02:06,880
and then we'll talk a little bit about

00:02:06,240 --> 00:02:10,239
just

00:02:06,880 --> 00:02:13,760
ai ethics in general okay

00:02:10,239 --> 00:02:14,959
um software's a tool it's it's no more

00:02:13,760 --> 00:02:17,360
useful as a tool

00:02:14,959 --> 00:02:19,440
than a hammer is useful if you don't

00:02:17,360 --> 00:02:21,680
have any wood or nails or don't know how

00:02:19,440 --> 00:02:24,640
to swing a hammer

00:02:21,680 --> 00:02:26,480
my basic recommendations are you're

00:02:24,640 --> 00:02:30,080
going to look at three tools today

00:02:26,480 --> 00:02:32,760
audit ai gives you straightforward tests

00:02:30,080 --> 00:02:35,920
with minimum code

00:02:32,760 --> 00:02:38,080
aif360 is this comprehensive toolkit

00:02:35,920 --> 00:02:41,280
that's really meant to be stood up

00:02:38,080 --> 00:02:42,080
in a production setting when you're

00:02:41,280 --> 00:02:45,360
trying to

00:02:42,080 --> 00:02:47,280
automatically audit a lot of models

00:02:45,360 --> 00:02:48,400
and automatically de-bias a lot of

00:02:47,280 --> 00:02:50,480
models

00:02:48,400 --> 00:02:52,080
the what if tool from google which is

00:02:50,480 --> 00:02:54,879
the last one we'll discuss

00:02:52,080 --> 00:02:57,360
is an interactive tool it's really neat

00:02:54,879 --> 00:02:59,920
it's got a lot of wonderful features

00:02:57,360 --> 00:03:01,840
you can use it with any model you have

00:02:59,920 --> 00:03:04,319
it's made to work with tensorflow it's

00:03:01,840 --> 00:03:06,560
easiest with tensorflow

00:03:04,319 --> 00:03:07,360
but you can use it with just about

00:03:06,560 --> 00:03:09,360
anything

00:03:07,360 --> 00:03:10,959
but it's an interactive tool it's not

00:03:09,360 --> 00:03:12,720
meant to be stood up in a production

00:03:10,959 --> 00:03:16,840
environment and used as kind of a

00:03:12,720 --> 00:03:18,080
production test or an automated testing

00:03:16,840 --> 00:03:20,800
tool

00:03:18,080 --> 00:03:21,440
now let's talk a little bit about bias

00:03:20,800 --> 00:03:24,319
so

00:03:21,440 --> 00:03:24,879
the word bias a lot of people wishes

00:03:24,319 --> 00:03:27,040
wish

00:03:24,879 --> 00:03:27,920
a lot of people wish that we had chosen

00:03:27,040 --> 00:03:29,840
another term

00:03:27,920 --> 00:03:31,280
when we started talking about biased

00:03:29,840 --> 00:03:33,920
models

00:03:31,280 --> 00:03:34,560
bias is a time-honored word in machine

00:03:33,920 --> 00:03:37,680
learning

00:03:34,560 --> 00:03:38,640
and all it means is that two groups have

00:03:37,680 --> 00:03:41,360
different means

00:03:38,640 --> 00:03:42,319
or two groups have different counts

00:03:41,360 --> 00:03:45,440
that's what it means

00:03:42,319 --> 00:03:46,640
in machine learning when you apply the

00:03:45,440 --> 00:03:50,319
word bias

00:03:46,640 --> 00:03:52,319
to ai however when you say a model is

00:03:50,319 --> 00:03:55,519
biased you're saying that the model has

00:03:52,319 --> 00:03:56,000
systematic and repeatable errors that

00:03:55,519 --> 00:03:59,360
create

00:03:56,000 --> 00:04:01,680
unfair outcomes now think about that

00:03:59,360 --> 00:04:04,239
we're talking specifically about cases

00:04:01,680 --> 00:04:07,760
where bias is an error

00:04:04,239 --> 00:04:10,799
where it assigns people

00:04:07,760 --> 00:04:12,080
to a category not based on the things

00:04:10,799 --> 00:04:15,360
you're interested in

00:04:12,080 --> 00:04:18,239
but based on how they look um

00:04:15,360 --> 00:04:20,400
where they live information that

00:04:18,239 --> 00:04:23,360
actually technically isn't relevant

00:04:20,400 --> 00:04:24,160
to what you're trying to predict and if

00:04:23,360 --> 00:04:27,520
you think about

00:04:24,160 --> 00:04:30,160
model bias in that way a biased model

00:04:27,520 --> 00:04:32,000
is not doing what you want it to do it

00:04:30,160 --> 00:04:34,800
causes you to miss opportunities

00:04:32,000 --> 00:04:36,720
if it's a marketing model you're showing

00:04:34,800 --> 00:04:38,320
products to people who aren't interested

00:04:36,720 --> 00:04:40,160
and you're not showing products to

00:04:38,320 --> 00:04:43,360
people who are

00:04:40,160 --> 00:04:44,880
furthermore if people discover that your

00:04:43,360 --> 00:04:46,560
model is biased and

00:04:44,880 --> 00:04:48,240
probably the reason some of you have

00:04:46,560 --> 00:04:48,880
come to this talk today is because

00:04:48,240 --> 00:04:50,479
you've seen

00:04:48,880 --> 00:04:52,400
articles in the news where people have

00:04:50,479 --> 00:04:54,560
talked about bias models

00:04:52,400 --> 00:04:56,639
um and it's been a big reputational hit

00:04:54,560 --> 00:04:58,720
to a business people realize your

00:04:56,639 --> 00:04:59,919
model's bias that's a reputational risk

00:04:58,720 --> 00:05:03,120
to your company

00:04:59,919 --> 00:05:05,919
um and that is a risk that's a problem

00:05:03,120 --> 00:05:07,360
and so if you leave with nothing else

00:05:05,919 --> 00:05:09,680
today

00:05:07,360 --> 00:05:10,639
biased ai models are bad for your

00:05:09,680 --> 00:05:12,800
business

00:05:10,639 --> 00:05:13,759
if you're spending the money on an ai

00:05:12,800 --> 00:05:16,080
platform

00:05:13,759 --> 00:05:16,800
it's worth your time and effort to think

00:05:16,080 --> 00:05:19,039
about

00:05:16,800 --> 00:05:22,000
how those models might be biased and

00:05:19,039 --> 00:05:24,960
what you can do to find out

00:05:22,000 --> 00:05:26,720
all right um i am a phd statistician for

00:05:24,960 --> 00:05:28,560
those of you who like math

00:05:26,720 --> 00:05:30,400
this is for you um for those of you who

00:05:28,560 --> 00:05:31,600
don't i promise this is the only

00:05:30,400 --> 00:05:34,720
equation

00:05:31,600 --> 00:05:36,080
okay so where does bias come from this

00:05:34,720 --> 00:05:39,360
is what i think is the most

00:05:36,080 --> 00:05:41,600
common concept of bias

00:05:39,360 --> 00:05:43,199
you have what you're trying to predict

00:05:41,600 --> 00:05:46,479
that's your why

00:05:43,199 --> 00:05:48,560
you have your features that's your x

00:05:46,479 --> 00:05:50,000
and then you have a relationship between

00:05:48,560 --> 00:05:52,080
your features and your why

00:05:50,000 --> 00:05:53,680
okay and this is kind of the true

00:05:52,080 --> 00:05:55,600
relationship

00:05:53,680 --> 00:05:56,720
but when you get to the actual

00:05:55,600 --> 00:05:58,400
assignment

00:05:56,720 --> 00:06:01,120
when you actually start to assign the

00:05:58,400 --> 00:06:03,199
value of y there's this bias that comes

00:06:01,120 --> 00:06:06,960
in

00:06:03,199 --> 00:06:08,800
and it could be human bias it could be

00:06:06,960 --> 00:06:10,400
unconscious bias when people are

00:06:08,800 --> 00:06:12,960
labeling your training data

00:06:10,400 --> 00:06:13,600
it could be systematic bias like things

00:06:12,960 --> 00:06:15,440
like

00:06:13,600 --> 00:06:17,199
people who look like this don't shop in

00:06:15,440 --> 00:06:18,960
our stores well your stores

00:06:17,199 --> 00:06:20,479
aren't near any of the neighborhoods

00:06:18,960 --> 00:06:24,080
where those folks live

00:06:20,479 --> 00:06:27,199
and so it could be a systematic bias

00:06:24,080 --> 00:06:28,720
and what happens if the bias is large is

00:06:27,199 --> 00:06:29,600
that instead of learning this

00:06:28,720 --> 00:06:32,800
relationship

00:06:29,600 --> 00:06:34,240
f of x comma b equals y your artificial

00:06:32,800 --> 00:06:36,880
intelligence model

00:06:34,240 --> 00:06:39,280
learns how to distinguish between the

00:06:36,880 --> 00:06:41,919
people who are being biased against

00:06:39,280 --> 00:06:43,440
and the people who are not and so in the

00:06:41,919 --> 00:06:46,880
worst case of this

00:06:43,440 --> 00:06:48,240
you get this model that doesn't again

00:06:46,880 --> 00:06:51,520
actually predict what you want it to

00:06:48,240 --> 00:06:54,160
predict it predicts something else

00:06:51,520 --> 00:06:56,080
and that is why biases worth dealing

00:06:54,160 --> 00:06:59,599
with and that's why we're interested

00:06:56,080 --> 00:07:01,599
in detecting it okay so we're actually

00:06:59,599 --> 00:07:04,319
going to use simulated data

00:07:01,599 --> 00:07:06,240
from an experiment the example um is

00:07:04,319 --> 00:07:07,360
healthcare cost this is based on a real

00:07:06,240 --> 00:07:10,560
model

00:07:07,360 --> 00:07:13,520
and the intent of this model was

00:07:10,560 --> 00:07:14,080
was a good intent they said okay look

00:07:13,520 --> 00:07:16,400
we're

00:07:14,080 --> 00:07:17,440
a medical insurance company people who

00:07:16,400 --> 00:07:19,759
are sick or cost

00:07:17,440 --> 00:07:21,360
more so if we have people who are

00:07:19,759 --> 00:07:23,280
predicted to be high costs

00:07:21,360 --> 00:07:25,199
right so i've got this cost idea of

00:07:23,280 --> 00:07:27,759
people who are out on the tails

00:07:25,199 --> 00:07:30,720
the high tail for cost if you're

00:07:27,759 --> 00:07:31,120
predicted to be in that tail i am going

00:07:30,720 --> 00:07:32,880
to

00:07:31,120 --> 00:07:35,039
assign you to a program designed to

00:07:32,880 --> 00:07:37,680
improve your health

00:07:35,039 --> 00:07:39,919
but what's happening is the protected

00:07:37,680 --> 00:07:42,080
group

00:07:39,919 --> 00:07:43,280
has worse health outcomes right they're

00:07:42,080 --> 00:07:45,360
sicker

00:07:43,280 --> 00:07:47,039
but they're also lower cost and they're

00:07:45,360 --> 00:07:48,960
lower costs because they're not getting

00:07:47,039 --> 00:07:50,240
the care they need when they need it or

00:07:48,960 --> 00:07:52,160
they're lower costs because they're not

00:07:50,240 --> 00:07:53,919
getting the care they need not getting

00:07:52,160 --> 00:07:56,840
the care they need makes them sicker so

00:07:53,919 --> 00:07:58,080
it's actually a reverse causality

00:07:56,840 --> 00:08:00,560
situation

00:07:58,080 --> 00:08:02,479
um this really happened there's a link

00:08:00,560 --> 00:08:05,440
in the deck

00:08:02,479 --> 00:08:06,479
and we're going to talk about both how

00:08:05,440 --> 00:08:07,759
to detect this

00:08:06,479 --> 00:08:10,000
and we're going to talk about how to

00:08:07,759 --> 00:08:12,080
mitigate it in real life

00:08:10,000 --> 00:08:13,120
okay so what does bias testing

00:08:12,080 --> 00:08:15,759
accomplish

00:08:13,120 --> 00:08:17,680
the main purpose of a bias test is to

00:08:15,759 --> 00:08:21,680
inspect the model

00:08:17,680 --> 00:08:23,599
and see if it's predicting as well

00:08:21,680 --> 00:08:24,960
for the protected group and the

00:08:23,599 --> 00:08:27,840
non-protected group

00:08:24,960 --> 00:08:28,479
and who's in the protected group depends

00:08:27,840 --> 00:08:30,800
on

00:08:28,479 --> 00:08:31,520
your application so the classic

00:08:30,800 --> 00:08:35,360
protected

00:08:31,520 --> 00:08:36,640
cases are race gender and age

00:08:35,360 --> 00:08:39,120
those are the ones you hear about all

00:08:36,640 --> 00:08:40,159
the time there could be others depending

00:08:39,120 --> 00:08:41,519
on where you're located

00:08:40,159 --> 00:08:43,760
in the world there could be others

00:08:41,519 --> 00:08:45,680
depending on your particular business

00:08:43,760 --> 00:08:48,080
case

00:08:45,680 --> 00:08:49,839
but what a lot of these do then is they

00:08:48,080 --> 00:08:51,920
take something like the true positive

00:08:49,839 --> 00:08:53,920
rate or the false positive rate

00:08:51,920 --> 00:08:56,080
and they compare them for the protected

00:08:53,920 --> 00:08:57,839
group and the non-protected group

00:08:56,080 --> 00:08:59,279
and people with degrees of math have

00:08:57,839 --> 00:09:02,320
figured out what

00:08:59,279 --> 00:09:04,240
values for those comparisons mean that

00:09:02,320 --> 00:09:06,640
you have a biased model

00:09:04,240 --> 00:09:07,680
now i've spent a lot of time looking at

00:09:06,640 --> 00:09:09,519
these

00:09:07,680 --> 00:09:11,040
a lot of time kind of following the math

00:09:09,519 --> 00:09:12,160
of other people who've worked on this

00:09:11,040 --> 00:09:15,760
stuff

00:09:12,160 --> 00:09:17,760
and what i've come to realize is that

00:09:15,760 --> 00:09:20,320
there's a statistic called disparate

00:09:17,760 --> 00:09:22,080
impact which is literally just

00:09:20,320 --> 00:09:23,600
the percent of people in the protected

00:09:22,080 --> 00:09:25,760
group who are um

00:09:23,600 --> 00:09:26,640
assigned to the outcome divided by the

00:09:25,760 --> 00:09:28,399
percent of people

00:09:26,640 --> 00:09:30,480
in the non-protected group who are

00:09:28,399 --> 00:09:32,160
assigned to the positive case

00:09:30,480 --> 00:09:34,240
and when i first heard that i was like

00:09:32,160 --> 00:09:37,360
ah it's not very scientific

00:09:34,240 --> 00:09:39,760
this is by far right now

00:09:37,360 --> 00:09:41,040
my favorite statistic all right

00:09:39,760 --> 00:09:44,080
disparate impact

00:09:41,040 --> 00:09:44,640
is um it's unaffected by biases in your

00:09:44,080 --> 00:09:46,160
target

00:09:44,640 --> 00:09:47,839
because it only looks at the outcomes

00:09:46,160 --> 00:09:50,880
from your model um

00:09:47,839 --> 00:09:52,560
it is for a lot of hiring and lending it

00:09:50,880 --> 00:09:54,399
is the legal standard

00:09:52,560 --> 00:09:56,560
and it actually has this really nice

00:09:54,399 --> 00:09:58,240
really satisfying distribution so that

00:09:56,560 --> 00:09:59,440
you can look at your sample sizes and

00:09:58,240 --> 00:10:01,760
kind of make decisions

00:09:59,440 --> 00:10:03,360
about whether disparate impact is is

00:10:01,760 --> 00:10:04,959
working the way you expect it to work

00:10:03,360 --> 00:10:08,800
it's a very

00:10:04,959 --> 00:10:12,399
solid test so my recommendation

00:10:08,800 --> 00:10:14,320
is to always use desperate impact

00:10:12,399 --> 00:10:15,680
you're going to want some other bias

00:10:14,320 --> 00:10:17,440
tests as well

00:10:15,680 --> 00:10:19,200
all right to kind of get a get your arms

00:10:17,440 --> 00:10:23,440
around what your

00:10:19,200 --> 00:10:26,000
um particular model is doing pick

00:10:23,440 --> 00:10:27,519
the negative outcome and choose your

00:10:26,000 --> 00:10:30,880
bias tests accordingly

00:10:27,519 --> 00:10:32,000
right so if falsely assigning people to

00:10:30,880 --> 00:10:35,120
the negative case

00:10:32,000 --> 00:10:39,040
is really really bad use the bias tests

00:10:35,120 --> 00:10:41,200
that focus on the false negative rate

00:10:39,040 --> 00:10:43,120
if accidentally assigning people to the

00:10:41,200 --> 00:10:46,160
positive case is really really bad

00:10:43,120 --> 00:10:47,920
then take a look at the tests

00:10:46,160 --> 00:10:50,079
focus on the tests that have the false

00:10:47,920 --> 00:10:52,000
positive rate if you're just worried

00:10:50,079 --> 00:10:53,760
about predicting accurately so again the

00:10:52,000 --> 00:10:55,279
marketing model is the one that i tend

00:10:53,760 --> 00:10:57,040
to think about we do a lot of marketing

00:10:55,279 --> 00:10:59,279
models in our business

00:10:57,040 --> 00:11:00,480
um if you've got a marketing model you

00:10:59,279 --> 00:11:01,839
probably

00:11:00,480 --> 00:11:03,839
really just want to make sure that

00:11:01,839 --> 00:11:05,920
you're accurate everywhere

00:11:03,839 --> 00:11:07,680
and in that case things like average

00:11:05,920 --> 00:11:11,200
odds generalized entropy

00:11:07,680 --> 00:11:13,120
field index those are cases where

00:11:11,200 --> 00:11:14,320
you have the those are cases where

00:11:13,120 --> 00:11:15,600
they're looking at the overall

00:11:14,320 --> 00:11:18,720
predictive accuracy

00:11:15,600 --> 00:11:21,279
and that's where you want to focus

00:11:18,720 --> 00:11:22,959
um if you've never heard the words false

00:11:21,279 --> 00:11:24,720
negative rate and false positive rate

00:11:22,959 --> 00:11:27,920
this slide is for you

00:11:24,720 --> 00:11:29,040
um so you're true you have some sort of

00:11:27,920 --> 00:11:31,279
truth in your data set

00:11:29,040 --> 00:11:32,320
right that's how you're able to train it

00:11:31,279 --> 00:11:34,560
um and

00:11:32,320 --> 00:11:36,240
that's either positive or negative so

00:11:34,560 --> 00:11:37,839
for our healthcare example

00:11:36,240 --> 00:11:39,279
positive is being assigned to this

00:11:37,839 --> 00:11:41,120
special program

00:11:39,279 --> 00:11:43,519
negatives being kind of left on your own

00:11:41,120 --> 00:11:45,200
and we hope you stay healthy

00:11:43,519 --> 00:11:48,640
so you have people who are truly high

00:11:45,200 --> 00:11:50,880
cost they're in the positive case

00:11:48,640 --> 00:11:51,839
there and then you have your prediction

00:11:50,880 --> 00:11:53,839
from your model

00:11:51,839 --> 00:11:55,839
and so the predicted label again

00:11:53,839 --> 00:11:56,800
prediction says we predict you should be

00:11:55,839 --> 00:11:58,480
positive

00:11:56,800 --> 00:12:00,320
you should be assigned to this program

00:11:58,480 --> 00:12:01,760
or we predict you are in the negative

00:12:00,320 --> 00:12:03,680
case you shouldn't be assigned to this

00:12:01,760 --> 00:12:05,680
program

00:12:03,680 --> 00:12:07,279
if you're right that's a true pop if you

00:12:05,680 --> 00:12:08,560
predict someone to be in the positive

00:12:07,279 --> 00:12:10,720
case and they are

00:12:08,560 --> 00:12:12,720
that's a true positive if you predict

00:12:10,720 --> 00:12:14,800
someone to be in the negative case

00:12:12,720 --> 00:12:16,079
uh or in the positive case and they

00:12:14,800 --> 00:12:19,440
shouldn't be that's a

00:12:16,079 --> 00:12:20,959
false positive um

00:12:19,440 --> 00:12:22,320
and these words start to mean kind of

00:12:20,959 --> 00:12:23,519
what you think they mean right i said

00:12:22,320 --> 00:12:24,880
they were negative but they should have

00:12:23,519 --> 00:12:26,480
been positive they're in the false

00:12:24,880 --> 00:12:27,760
they're false negative i said they're

00:12:26,480 --> 00:12:28,639
negative and they're negative that's a

00:12:27,760 --> 00:12:31,519
true negative

00:12:28,639 --> 00:12:32,800
right um so again if you think of our

00:12:31,519 --> 00:12:35,600
health care model

00:12:32,800 --> 00:12:37,360
being assigned into this negative

00:12:35,600 --> 00:12:39,360
category being denied

00:12:37,360 --> 00:12:41,440
um the special program to improve your

00:12:39,360 --> 00:12:45,519
health incorrectly

00:12:41,440 --> 00:12:47,120
all right that's a that's a bad outcome

00:12:45,519 --> 00:12:49,440
all right and we probably want to check

00:12:47,120 --> 00:12:51,839
the false negative rate

00:12:49,440 --> 00:12:53,200
um there's a famous example and now i

00:12:51,839 --> 00:12:53,839
guess you could call it a classic

00:12:53,200 --> 00:12:56,079
example

00:12:53,839 --> 00:12:58,160
of a model that predicted who was at

00:12:56,079 --> 00:13:00,079
high risk to reoffend

00:12:58,160 --> 00:13:01,440
um so it was used for people who were up

00:13:00,079 --> 00:13:03,600
for parole

00:13:01,440 --> 00:13:06,079
and you were assigned parole based on

00:13:03,600 --> 00:13:07,680
your risk according to this model all

00:13:06,079 --> 00:13:09,519
right that's a case where we really want

00:13:07,680 --> 00:13:10,880
to check the false positive rate and we

00:13:09,519 --> 00:13:14,320
want to be sure that

00:13:10,880 --> 00:13:15,760
we're not falsely assigning people to

00:13:14,320 --> 00:13:18,800
high risk categories

00:13:15,760 --> 00:13:20,240
in other words denying them parole based

00:13:18,800 --> 00:13:22,160
on a model that doesn't predict

00:13:20,240 --> 00:13:24,320
accurately for them

00:13:22,160 --> 00:13:25,839
so this is a conversation that you have

00:13:24,320 --> 00:13:27,440
with your modeling team

00:13:25,839 --> 00:13:29,440
and the people who are going to use the

00:13:27,440 --> 00:13:31,200
outcome of the model

00:13:29,440 --> 00:13:32,639
which of these is the worst we're going

00:13:31,200 --> 00:13:34,000
to focus on those tests

00:13:32,639 --> 00:13:36,000
and we're always going to look at

00:13:34,000 --> 00:13:38,320
desperate impact that's my

00:13:36,000 --> 00:13:40,880
recommendation

00:13:38,320 --> 00:13:41,839
okay now i'm going to demonstrate these

00:13:40,880 --> 00:13:45,199
three tools

00:13:41,839 --> 00:13:46,399
for bias testing i put this slide in

00:13:45,199 --> 00:13:48,560
because i kind of want to give you an

00:13:46,399 --> 00:13:49,440
idea of the lift required for each of

00:13:48,560 --> 00:13:52,160
these

00:13:49,440 --> 00:13:53,440
again aif 360 is extraordinarily

00:13:52,160 --> 00:13:55,680
customizable

00:13:53,440 --> 00:13:57,279
it's really good if you're standing it

00:13:55,680 --> 00:13:58,959
up in production you're testing a bunch

00:13:57,279 --> 00:14:01,519
of models

00:13:58,959 --> 00:14:02,079
but if you just need a yes or no on a

00:14:01,519 --> 00:14:05,120
specific

00:14:02,079 --> 00:14:07,120
model it is kind of a heavy lift on the

00:14:05,120 --> 00:14:09,040
other hand audit ai

00:14:07,120 --> 00:14:11,360
there's a couple of packages it gives

00:14:09,040 --> 00:14:14,160
you a graph and that's all it does

00:14:11,360 --> 00:14:15,360
so there's not a ton of features but it

00:14:14,160 --> 00:14:18,079
does do what it does

00:14:15,360 --> 00:14:19,360
extremely well and very simply and then

00:14:18,079 --> 00:14:22,240
the what if tool

00:14:19,360 --> 00:14:24,240
is interactive it does take some setup

00:14:22,240 --> 00:14:27,040
it's a little easier in tensorflow

00:14:24,240 --> 00:14:27,360
obviously but it's still it's not it's

00:14:27,040 --> 00:14:30,240
not

00:14:27,360 --> 00:14:31,440
hard not that bad all right so let's

00:14:30,240 --> 00:14:33,199
take a look

00:14:31,440 --> 00:14:36,240
okay so you should be seeing my jupiter

00:14:33,199 --> 00:14:36,240
notebook at this point

00:14:36,720 --> 00:14:40,560
all right and what i'd like you to

00:14:39,600 --> 00:14:43,519
assume with me

00:14:40,560 --> 00:14:45,120
is that i've already built this model so

00:14:43,519 --> 00:14:47,920
i've built a model

00:14:45,120 --> 00:14:49,920
to predict cost so this is my truth here

00:14:47,920 --> 00:14:52,880
this is cost

00:14:49,920 --> 00:14:54,399
based on your health so this is a simple

00:14:52,880 --> 00:14:56,000
little linear regression like i said

00:14:54,399 --> 00:14:57,839
it's a toy example

00:14:56,000 --> 00:14:59,360
i'm going to use your health to predict

00:14:57,839 --> 00:15:01,279
how much you're going to cost

00:14:59,360 --> 00:15:03,519
and i've got a prediction this is my

00:15:01,279 --> 00:15:05,040
this is my validation data set

00:15:03,519 --> 00:15:06,720
that is sitting over here in this first

00:15:05,040 --> 00:15:08,480
column

00:15:06,720 --> 00:15:10,560
i also have this column called

00:15:08,480 --> 00:15:12,079
membership and membership tells me

00:15:10,560 --> 00:15:13,600
whether or not you're in the predictive

00:15:12,079 --> 00:15:17,279
group

00:15:13,600 --> 00:15:19,839
okay so i have your predicted cost

00:15:17,279 --> 00:15:21,120
based on your predicted cost i'm going

00:15:19,839 --> 00:15:23,360
to assign you

00:15:21,120 --> 00:15:24,880
either to the category where you get

00:15:23,360 --> 00:15:27,440
this special program

00:15:24,880 --> 00:15:28,959
or not okay that's my prediction and

00:15:27,440 --> 00:15:32,160
then i have the truth i

00:15:28,959 --> 00:15:34,560
and the truth is um anybody who was

00:15:32,160 --> 00:15:35,360
actually in the high cost group and high

00:15:34,560 --> 00:15:38,240
cost for this

00:15:35,360 --> 00:15:40,000
is upper 90th percentile so if you were

00:15:38,240 --> 00:15:42,480
one of the costliest people

00:15:40,000 --> 00:15:43,759
that's your true label and then i have

00:15:42,480 --> 00:15:46,959
your predicted label

00:15:43,759 --> 00:15:50,399
based on the prediction from the model

00:15:46,959 --> 00:15:52,560
okay all right so you can see me

00:15:50,399 --> 00:15:54,800
kind of kind of playing with the data

00:15:52,560 --> 00:15:58,720
taking a look at it

00:15:54,800 --> 00:16:01,600
these are my this is my test output

00:15:58,720 --> 00:16:02,560
and then i'm going to run audit ai um

00:16:01,600 --> 00:16:04,959
audit ai is

00:16:02,560 --> 00:16:06,880
a fairly straightforward install um it

00:16:04,959 --> 00:16:08,560
installed with with no problems on my

00:16:06,880 --> 00:16:10,240
machine

00:16:08,560 --> 00:16:12,800
it should work that way for yours as

00:16:10,240 --> 00:16:16,079
well pip install audit ai

00:16:12,800 --> 00:16:17,920
wait for a minute and i was done um

00:16:16,079 --> 00:16:19,839
i'm importing this one little function

00:16:17,920 --> 00:16:22,639
bias test check

00:16:19,839 --> 00:16:25,120
and what it wants to know is okay

00:16:22,639 --> 00:16:25,120
where's your

00:16:25,199 --> 00:16:28,639
where's your membership where's your

00:16:26,720 --> 00:16:32,720
protected group

00:16:28,639 --> 00:16:35,920
all right what's your prediction

00:16:32,720 --> 00:16:36,240
okay what's what's your predicted label

00:16:35,920 --> 00:16:39,920
um

00:16:36,240 --> 00:16:43,920
and what do you want me to call it okay

00:16:39,920 --> 00:16:46,720
and when i run it produces this

00:16:43,920 --> 00:16:48,399
small report it says okay well you're

00:16:46,720 --> 00:16:50,399
you're failing the four-fifths rule

00:16:48,399 --> 00:16:53,360
that's disparate impact

00:16:50,399 --> 00:16:55,199
um you're failing my fischer exact

00:16:53,360 --> 00:16:57,920
chi-squared and z-tests

00:16:55,199 --> 00:16:59,519
for equality of proportions and you're

00:16:57,920 --> 00:17:01,360
also failing according to the base

00:16:59,519 --> 00:17:04,720
factor

00:17:01,360 --> 00:17:06,959
so it's a straight thumbs up and thumbs

00:17:04,720 --> 00:17:09,679
down now i built this model to be biased

00:17:06,959 --> 00:17:11,360
so i am not shocked by this right but if

00:17:09,679 --> 00:17:14,559
you see that run a test

00:17:11,360 --> 00:17:16,400
on your audit program that's that's a

00:17:14,559 --> 00:17:18,240
really good indicator that you need to

00:17:16,400 --> 00:17:19,839
take a deeper look and see what you're

00:17:18,240 --> 00:17:22,959
doing

00:17:19,839 --> 00:17:25,360
okay um these

00:17:22,959 --> 00:17:26,240
fishers exact chi-squared z-tests and

00:17:25,360 --> 00:17:29,280
base factors

00:17:26,240 --> 00:17:30,080
these are actually all just what is the

00:17:29,280 --> 00:17:33,200
difference

00:17:30,080 --> 00:17:34,880
in the assignment for people in the

00:17:33,200 --> 00:17:35,440
protected group versus people in the

00:17:34,880 --> 00:17:37,280
knot

00:17:35,440 --> 00:17:39,840
so it's not even looking at model

00:17:37,280 --> 00:17:42,960
accuracy it's literally straight up

00:17:39,840 --> 00:17:43,919
looking at it's basically disparate

00:17:42,960 --> 00:17:46,960
impact

00:17:43,919 --> 00:17:50,480
tested four different ways

00:17:46,960 --> 00:17:53,039
these statistical tests um are based on

00:17:50,480 --> 00:17:54,640
p values um you've maybe you've heard of

00:17:53,039 --> 00:17:56,480
the great p-value controversy in

00:17:54,640 --> 00:17:58,240
statistics i try to stay out of the

00:17:56,480 --> 00:18:01,440
great statistician wars

00:17:58,240 --> 00:18:03,679
um but if you are from that tradition

00:18:01,440 --> 00:18:06,320
that light statistical tests and i'm

00:18:03,679 --> 00:18:07,280
of that tradition this is really really

00:18:06,320 --> 00:18:09,840
nice

00:18:07,280 --> 00:18:11,600
is it is it true or not are they are

00:18:09,840 --> 00:18:14,799
they the same the answer is no

00:18:11,600 --> 00:18:18,160
okay now we know to take a deeper look

00:18:14,799 --> 00:18:21,360
um audit ai will also give you a visual

00:18:18,160 --> 00:18:22,240
so keep in mind that i've got this

00:18:21,360 --> 00:18:25,440
prediction

00:18:22,240 --> 00:18:26,240
that comes from my data and for this

00:18:25,440 --> 00:18:28,160
example i

00:18:26,240 --> 00:18:30,480
arbitrarily said okay if you're in the

00:18:28,160 --> 00:18:32,160
top 10th percentile

00:18:30,480 --> 00:18:33,600
then i'm going to assign you to this

00:18:32,160 --> 00:18:37,600
category

00:18:33,600 --> 00:18:41,039
however i could pick

00:18:37,600 --> 00:18:43,039
any cutoff i want any cutoff that makes

00:18:41,039 --> 00:18:45,440
sense for me in my business

00:18:43,039 --> 00:18:46,799
and so audit ai says well look your bias

00:18:45,440 --> 00:18:49,200
tests are going to be different

00:18:46,799 --> 00:18:50,720
for these different thresholds i'm going

00:18:49,200 --> 00:18:54,480
to help you pick a threshold

00:18:50,720 --> 00:18:57,360
where the categorization is the same

00:18:54,480 --> 00:18:57,760
and so if you take a look at this run of

00:18:57,360 --> 00:19:02,080
this

00:18:57,760 --> 00:19:04,880
series of um pictures that i get

00:19:02,080 --> 00:19:05,760
um by using the plot threshold tests

00:19:04,880 --> 00:19:09,360
very similar

00:19:05,760 --> 00:19:11,919
syntax okay the x-axis

00:19:09,360 --> 00:19:12,799
is always the value of my predicted

00:19:11,919 --> 00:19:14,880
variable so

00:19:12,799 --> 00:19:16,480
in this case it's the predicted cost you

00:19:14,880 --> 00:19:18,320
cost me

00:19:16,480 --> 00:19:20,160
we'll say that's ten thousand dollars to

00:19:18,320 --> 00:19:21,919
eighty thousand dollars this year

00:19:20,160 --> 00:19:23,679
and i know there are probably people who

00:19:21,919 --> 00:19:26,400
are higher okay

00:19:23,679 --> 00:19:27,520
and it says well look base factor should

00:19:26,400 --> 00:19:31,440
be zero

00:19:27,520 --> 00:19:33,520
um if you use this four to six

00:19:31,440 --> 00:19:35,440
um again if you use this four to six cut

00:19:33,520 --> 00:19:36,400
off your your spike in high so you get

00:19:35,440 --> 00:19:39,840
big differences

00:19:36,400 --> 00:19:41,679
according to the bayes factor

00:19:39,840 --> 00:19:44,080
if you are from the tradition that likes

00:19:41,679 --> 00:19:46,400
p values instead of bayes factors

00:19:44,080 --> 00:19:48,000
you're pretty safe anywhere kind of in

00:19:46,400 --> 00:19:50,160
this middle or i'm sorry you're

00:19:48,000 --> 00:19:52,720
pretty safe if your cutoff is less than

00:19:50,160 --> 00:19:55,919
two or bigger than 6

00:19:52,720 --> 00:19:56,720
otherwise you're getting a distinctly

00:19:55,919 --> 00:20:00,080
different count

00:19:56,720 --> 00:20:02,480
by group

00:20:00,080 --> 00:20:04,400
and they helpfully print this red line

00:20:02,480 --> 00:20:07,840
below the red line

00:20:04,400 --> 00:20:10,559
is bad above the red line is good and

00:20:07,840 --> 00:20:12,240
in mathematical tradition unless it's

00:20:10,559 --> 00:20:14,960
the base factor

00:20:12,240 --> 00:20:16,159
okay click the red line above the red

00:20:14,960 --> 00:20:19,919
line

00:20:16,159 --> 00:20:23,679
okay so that's ai f3 or that's

00:20:19,919 --> 00:20:25,280
audit ai very straightforward yes no

00:20:23,679 --> 00:20:28,880
here's a cut off that'll keep you out of

00:20:25,280 --> 00:20:28,880
trouble go on with your day

00:20:29,280 --> 00:20:36,159
by contrast this is aif360

00:20:32,559 --> 00:20:38,559
so i'm importing some very specific um

00:20:36,159 --> 00:20:40,080
some very specific functions here there

00:20:38,559 --> 00:20:42,720
are a bunch of them

00:20:40,080 --> 00:20:44,559
which again the software is very full

00:20:42,720 --> 00:20:46,960
featured

00:20:44,559 --> 00:20:49,600
if i want bias test at all i've got my

00:20:46,960 --> 00:20:54,240
data set that has my output

00:20:49,600 --> 00:20:56,799
but i have to create an aif 360 data set

00:20:54,240 --> 00:20:58,000
so i'm actually generating this other

00:20:56,799 --> 00:21:00,960
data structure

00:20:58,000 --> 00:21:02,000
that has my original data in it but it

00:21:00,960 --> 00:21:04,559
also has these little

00:21:02,000 --> 00:21:05,360
decorators that tell it which column is

00:21:04,559 --> 00:21:08,240
the label

00:21:05,360 --> 00:21:08,559
which column is the protected group um

00:21:08,240 --> 00:21:11,520
and

00:21:08,559 --> 00:21:13,360
what value is the favorable label and so

00:21:11,520 --> 00:21:15,280
that's kind of nice if you're doing a

00:21:13,360 --> 00:21:16,559
lot of automated tests you give it the

00:21:15,280 --> 00:21:18,640
information one more time

00:21:16,559 --> 00:21:21,440
one time and then you can run a bunch of

00:21:18,640 --> 00:21:21,440
different tests

00:21:22,640 --> 00:21:25,840
however you also have to create this

00:21:24,880 --> 00:21:28,640
metric

00:21:25,840 --> 00:21:31,440
orage which lets you specify

00:21:28,640 --> 00:21:34,480
unprivileged groups and privilege groups

00:21:31,440 --> 00:21:36,559
if you program in python a lot this is a

00:21:34,480 --> 00:21:38,320
set of key value pairs right this is a

00:21:36,559 --> 00:21:41,360
dictionary

00:21:38,320 --> 00:21:44,000
and this is the name of the variable

00:21:41,360 --> 00:21:45,200
and the unprivileged group for that

00:21:44,000 --> 00:21:46,880
variable

00:21:45,200 --> 00:21:49,360
and you can have as many of these as you

00:21:46,880 --> 00:21:49,840
want so if you have to generate a lot of

00:21:49,360 --> 00:21:51,679
tests

00:21:49,840 --> 00:21:53,039
on a lot of different groups this is

00:21:51,679 --> 00:21:55,200
really convenient

00:21:53,039 --> 00:21:56,880
because they're all kind of tucked into

00:21:55,200 --> 00:22:01,200
this one function

00:21:56,880 --> 00:22:02,559
or you can create multiple metrics

00:22:01,200 --> 00:22:04,880
and again they're all tucked into one

00:22:02,559 --> 00:22:07,120
convenient function

00:22:04,880 --> 00:22:08,720
and so that's why i say this is really

00:22:07,120 --> 00:22:10,240
good for a production environment or

00:22:08,720 --> 00:22:12,320
where you're testing a lot of models or

00:22:10,240 --> 00:22:14,000
you have these really strict rules about

00:22:12,320 --> 00:22:15,679
what you need to test and what you

00:22:14,000 --> 00:22:17,440
what you must test and what you don't

00:22:15,679 --> 00:22:20,640
have to test

00:22:17,440 --> 00:22:22,240
okay so i create this metric that again

00:22:20,640 --> 00:22:24,640
gives it information on who's the

00:22:22,240 --> 00:22:27,039
privileged and unprivileged groups

00:22:24,640 --> 00:22:28,400
i create an explainer object based on

00:22:27,039 --> 00:22:31,039
the metric

00:22:28,400 --> 00:22:31,919
and then to get my disparate impact

00:22:31,039 --> 00:22:34,960
statistic

00:22:31,919 --> 00:22:37,120
i run the disparate impact method on the

00:22:34,960 --> 00:22:38,640
explainer

00:22:37,120 --> 00:22:41,760
one of the things i really love about

00:22:38,640 --> 00:22:44,880
this though is that for every statistic

00:22:41,760 --> 00:22:47,840
it generates i get

00:22:44,880 --> 00:22:49,120
a little explanation this is this is the

00:22:47,840 --> 00:22:50,880
statistic you asked for

00:22:49,120 --> 00:22:52,240
this is what it does and so if i'm

00:22:50,880 --> 00:22:55,200
printing this out for

00:22:52,240 --> 00:22:58,400
a user who maybe isn't an aia expert

00:22:55,200 --> 00:23:00,240
right or isn't an expert in ai bias

00:22:58,400 --> 00:23:05,039
they've got a reminder of what that

00:23:00,240 --> 00:23:05,039
means of what this specific test does

00:23:05,440 --> 00:23:09,360
all right so this is a this is a little

00:23:08,559 --> 00:23:13,120
bit more

00:23:09,360 --> 00:23:15,679
um of aif360 where i'm actually

00:23:13,120 --> 00:23:16,320
testing all of the different um i'm

00:23:15,679 --> 00:23:17,679
actually

00:23:16,320 --> 00:23:20,480
printing out a bunch of different

00:23:17,679 --> 00:23:20,480
statistics

00:23:21,600 --> 00:23:25,360
so unlike audited ai it's got all of

00:23:24,880 --> 00:23:27,679
them

00:23:25,360 --> 00:23:29,600
um if you have custom metrics if you've

00:23:27,679 --> 00:23:31,120
got a statistics team that has a favored

00:23:29,600 --> 00:23:33,120
metric that isn't in there

00:23:31,120 --> 00:23:34,960
it's pretty easy to add your own that's

00:23:33,120 --> 00:23:38,640
also a nice feature

00:23:34,960 --> 00:23:42,480
you can specify which ones you want

00:23:38,640 --> 00:23:43,600
um and like but like audit ai was

00:23:42,480 --> 00:23:46,400
showing you

00:23:43,600 --> 00:23:48,000
your um your metrics are going to change

00:23:46,400 --> 00:23:48,640
right so this is based on the decision

00:23:48,000 --> 00:23:52,559
you make

00:23:48,640 --> 00:23:54,400
based on the model the statistics change

00:23:52,559 --> 00:23:55,600
based on the cutoffs and one of the ways

00:23:54,400 --> 00:23:58,960
to mitigate bias

00:23:55,600 --> 00:24:00,559
is to actually change the cutoff and say

00:23:58,960 --> 00:24:02,840
okay i'm going to move the cut off

00:24:00,559 --> 00:24:04,000
somewhere where i don't get a bias

00:24:02,840 --> 00:24:07,039
decision

00:24:04,000 --> 00:24:11,039
and there's a lovely demonstration um in

00:24:07,039 --> 00:24:11,039
the aif360 documentation

00:24:11,520 --> 00:24:14,880
and then finally we come to wit widget

00:24:14,159 --> 00:24:17,360
and

00:24:14,880 --> 00:24:18,640
widget is is warming its way into my

00:24:17,360 --> 00:24:20,799
heart as one of my

00:24:18,640 --> 00:24:22,559
my favorite statistics are one of my

00:24:20,799 --> 00:24:25,760
favorite groups

00:24:22,559 --> 00:24:27,520
so i'm gonna kind of set this back to

00:24:25,760 --> 00:24:31,039
where it was

00:24:27,520 --> 00:24:35,120
okay all right so this is

00:24:31,039 --> 00:24:35,120
this is the whip widget um

00:24:35,360 --> 00:24:41,760
i've got and it's a little interactive

00:24:38,720 --> 00:24:44,559
um program that runs in your notebook

00:24:41,760 --> 00:24:46,080
so i gave it a data set i gave it some

00:24:44,559 --> 00:24:48,320
setup information

00:24:46,080 --> 00:24:51,120
and now it's producing graphs for me on

00:24:48,320 --> 00:24:54,080
the fly in interactive

00:24:51,120 --> 00:24:55,520
each ball in this little graph is a data

00:24:54,080 --> 00:24:57,679
point

00:24:55,520 --> 00:24:58,640
right now it's colored by who's in the

00:24:57,679 --> 00:25:00,480
protected group

00:24:58,640 --> 00:25:02,320
so i can see that my protected group and

00:25:00,480 --> 00:25:05,360
my non-protected group are about 50

00:25:02,320 --> 00:25:07,039
50. um

00:25:05,360 --> 00:25:08,880
i can change that and i say well i tell

00:25:07,039 --> 00:25:10,640
you what tell me

00:25:08,880 --> 00:25:12,000
tell me who's in my label group and you

00:25:10,640 --> 00:25:16,400
can see now

00:25:12,000 --> 00:25:19,120
oh okay well about 10 right

00:25:16,400 --> 00:25:19,919
of your of your group um you've got

00:25:19,120 --> 00:25:22,720
about 10

00:25:19,919 --> 00:25:24,400
people who are labeled to be one in the

00:25:22,720 --> 00:25:27,440
positive case

00:25:24,400 --> 00:25:28,880
for your outcome if i

00:25:27,440 --> 00:25:31,039
if i don't want to have to do that

00:25:28,880 --> 00:25:33,600
individually for everyone

00:25:31,039 --> 00:25:34,400
it has a really nice um tab called

00:25:33,600 --> 00:25:37,600
features

00:25:34,400 --> 00:25:40,320
where i can see that data set

00:25:37,600 --> 00:25:42,320
right this is stuff that i do every time

00:25:40,320 --> 00:25:44,559
i get a data set um

00:25:42,320 --> 00:25:45,760
and so i end up writing a lot of dot

00:25:44,559 --> 00:25:50,159
hist functions

00:25:45,760 --> 00:25:52,480
and a lot of code around um pi plot

00:25:50,159 --> 00:25:54,080
i love pi plot as much as anybody else i

00:25:52,480 --> 00:25:56,960
i love matplotlibe

00:25:54,080 --> 00:25:57,520
but gosh it's a speed up to just be able

00:25:56,960 --> 00:25:59,360
to say

00:25:57,520 --> 00:26:02,000
okay here's all the histograms here's

00:25:59,360 --> 00:26:04,480
cost here's health here's cost

00:26:02,000 --> 00:26:06,320
um excuse me here's who's in the

00:26:04,480 --> 00:26:08,159
protected group and here's who isn't

00:26:06,320 --> 00:26:10,159
this a sparse data set do i have a

00:26:08,159 --> 00:26:10,880
really sparse target do i need to worry

00:26:10,159 --> 00:26:13,440
about maybe

00:26:10,880 --> 00:26:15,679
upwaiting the number of people in the

00:26:13,440 --> 00:26:19,120
positive case i might

00:26:15,679 --> 00:26:22,960
okay and over here on my main

00:26:19,120 --> 00:26:24,640
tab that they call the data point editor

00:26:22,960 --> 00:26:27,520
okay i'm gonna use health to predict

00:26:24,640 --> 00:26:30,159
cost why don't i take a look at health

00:26:27,520 --> 00:26:33,360
versus cost i've colored these little

00:26:30,159 --> 00:26:35,120
points by the protected group

00:26:33,360 --> 00:26:36,400
or i'm sorry right now they're colored

00:26:35,120 --> 00:26:38,720
by the label

00:26:36,400 --> 00:26:40,000
and you can see okay yeah ten percent of

00:26:38,720 --> 00:26:42,960
cost is in fact

00:26:40,000 --> 00:26:45,919
labeled as being in the positive case

00:26:42,960 --> 00:26:49,360
let's see who's in the protected group

00:26:45,919 --> 00:26:52,480
okay right here no modeling

00:26:49,360 --> 00:26:55,760
no tests required

00:26:52,480 --> 00:27:00,400
all of my outliers on the low side

00:26:55,760 --> 00:27:00,400
of cost and the low side of health

00:27:01,120 --> 00:27:04,400
are in the protected group

00:27:05,440 --> 00:27:09,120
full stop right i don't i don't need

00:27:07,440 --> 00:27:10,880
something fancy to see that now this

00:27:09,120 --> 00:27:12,640
again is a toy example and a lot of

00:27:10,880 --> 00:27:15,200
these relationships are kind of

00:27:12,640 --> 00:27:18,559
intertwined and tangled up

00:27:15,200 --> 00:27:20,399
but i can see a lot of this

00:27:18,559 --> 00:27:21,679
using this interactive tool and to me

00:27:20,399 --> 00:27:24,480
that's the real power

00:27:21,679 --> 00:27:25,039
of the web widget you can get your bias

00:27:24,480 --> 00:27:26,559
tests

00:27:25,039 --> 00:27:30,799
pretty easily with another kind of

00:27:26,559 --> 00:27:33,120
software they have a bias testing

00:27:30,799 --> 00:27:34,000
um they have a bias testing tab where

00:27:33,120 --> 00:27:36,880
you can interrogate

00:27:34,000 --> 00:27:40,320
an existing model but where i tend to

00:27:36,880 --> 00:27:42,720
use this the heavy lifting on this

00:27:40,320 --> 00:27:46,640
pardon me is in the features and being

00:27:42,720 --> 00:27:46,640
able to just see the features quickly

00:27:47,200 --> 00:27:51,440
and look through the relationships

00:27:48,640 --> 00:27:51,440
interactively

00:27:52,640 --> 00:27:57,840
okay now how do you set this up again

00:27:55,919 --> 00:28:00,640
whip widget was a pretty easy install

00:27:57,840 --> 00:28:02,240
for me i do have a tensorflow installed

00:28:00,640 --> 00:28:05,440
on my machine

00:28:02,240 --> 00:28:05,440
pardon me for one second

00:28:14,880 --> 00:28:18,480
there we go um

00:28:18,559 --> 00:28:22,480
what i did to do my own configuration so

00:28:22,000 --> 00:28:24,880
what

00:28:22,480 --> 00:28:27,039
the widget wants is a configuration

00:28:24,880 --> 00:28:29,760
builder

00:28:27,039 --> 00:28:31,039
that gives you the data frame that

00:28:29,760 --> 00:28:34,080
you're going to work with

00:28:31,039 --> 00:28:36,000
and the names of the features it doesn't

00:28:34,080 --> 00:28:37,840
take a panda's data frame though you

00:28:36,000 --> 00:28:42,880
have to do something a little fancier

00:28:37,840 --> 00:28:45,760
so here i have my features data frame

00:28:42,880 --> 00:28:47,520
i had to grab the values of it and send

00:28:45,760 --> 00:28:50,720
it to a list

00:28:47,520 --> 00:28:51,360
um again if you're used to coding in

00:28:50,720 --> 00:28:54,000
python

00:28:51,360 --> 00:28:55,120
it's not that hard it's not bad but it

00:28:54,000 --> 00:28:56,799
was

00:28:55,120 --> 00:28:58,799
it took some time it took a little bit

00:28:56,799 --> 00:29:00,320
of energy hopefully i've done this

00:28:58,799 --> 00:29:02,320
once you've seen that it needs to be

00:29:00,320 --> 00:29:03,360
done and it's a little bit easier for

00:29:02,320 --> 00:29:05,600
you

00:29:03,360 --> 00:29:07,120
um all of these open source tools

00:29:05,600 --> 00:29:10,240
there's a little bit of fiddling while

00:29:07,120 --> 00:29:10,240
you figure out how they work

00:29:11,440 --> 00:29:18,480
and but once i hand it the configuration

00:29:14,480 --> 00:29:21,600
i just run with widget config builder

00:29:18,480 --> 00:29:24,960
and this little interactive system

00:29:21,600 --> 00:29:27,679
appears in my in my um

00:29:24,960 --> 00:29:28,720
jupiter notebook it's really nice and if

00:29:27,679 --> 00:29:31,760
you're working

00:29:28,720 --> 00:29:33,120
on google's cloud it's even nicer there

00:29:31,760 --> 00:29:33,840
are a lot more there are a lot more

00:29:33,120 --> 00:29:35,440
features

00:29:33,840 --> 00:29:37,120
and some of the things that are

00:29:35,440 --> 00:29:38,000
difficult about working with tensorflow

00:29:37,120 --> 00:29:39,679
models like

00:29:38,000 --> 00:29:41,440
having a hard time seeing why the model

00:29:39,679 --> 00:29:44,080
predicts the way it predicts

00:29:41,440 --> 00:29:46,320
are made easier with this tool so if

00:29:44,080 --> 00:29:49,679
tensorflow is your tool of choice

00:29:46,320 --> 00:29:53,039
wit widget is amazing if

00:29:49,679 --> 00:29:53,760
you're using other software and you want

00:29:53,039 --> 00:29:55,760
something

00:29:53,760 --> 00:29:57,200
so that you don't have to leave python

00:29:55,760 --> 00:29:59,840
but you can work interactively

00:29:57,200 --> 00:30:02,720
with widget is amazing it's got a lot of

00:29:59,840 --> 00:30:02,720
wonderful features

00:30:03,200 --> 00:30:08,000
okay and that's the basics of which ones

00:30:06,720 --> 00:30:10,559
you should use

00:30:08,000 --> 00:30:12,480
what do you do if you detect bias there

00:30:10,559 --> 00:30:14,080
is a software solution which is called

00:30:12,480 --> 00:30:17,520
bias mitigation

00:30:14,080 --> 00:30:19,600
um ibm has spent a lot of time and money

00:30:17,520 --> 00:30:22,640
figuring out how to implement a lot of

00:30:19,600 --> 00:30:24,559
these algorithms for bias mitigation

00:30:22,640 --> 00:30:27,840
they have some wonderful tutorials on it

00:30:24,559 --> 00:30:30,960
i do not have time for a full-fledged

00:30:27,840 --> 00:30:32,880
tutorial on it here but

00:30:30,960 --> 00:30:34,640
the demonstration is in the notebook and

00:30:32,880 --> 00:30:36,159
i'm happy to answer questions if you

00:30:34,640 --> 00:30:38,000
have some specific questions

00:30:36,159 --> 00:30:40,559
about bias detection please put them in

00:30:38,000 --> 00:30:40,559
the q a

00:30:40,960 --> 00:30:45,360
but what i have found and the reason i'm

00:30:43,039 --> 00:30:47,120
not doing a huge presentation on bias

00:30:45,360 --> 00:30:48,880
mitigation

00:30:47,120 --> 00:30:51,360
is because this really is a new field

00:30:48,880 --> 00:30:52,320
for us and so there's a lot of great

00:30:51,360 --> 00:30:53,840
thought

00:30:52,320 --> 00:30:55,840
there are a lot of interesting

00:30:53,840 --> 00:30:57,519
algorithms that work in some very

00:30:55,840 --> 00:30:58,960
specific cases

00:30:57,519 --> 00:31:01,919
but when it actually comes to

00:30:58,960 --> 00:31:05,360
implementation what i have seen

00:31:01,919 --> 00:31:07,760
is that detecting the bias is step one

00:31:05,360 --> 00:31:09,600
and mitigating the bias a lot of times

00:31:07,760 --> 00:31:11,120
means changing what you're modeling or

00:31:09,600 --> 00:31:13,120
how you're modeling

00:31:11,120 --> 00:31:14,159
um so what you're seeing on the screen

00:31:13,120 --> 00:31:16,960
is an example

00:31:14,159 --> 00:31:18,640
from the pulse algorithm and this is an

00:31:16,960 --> 00:31:20,320
algorithm that is supposed to take a

00:31:18,640 --> 00:31:23,360
pixelated image

00:31:20,320 --> 00:31:25,279
and turn it into a face

00:31:23,360 --> 00:31:26,480
give you an idea of the face of the

00:31:25,279 --> 00:31:29,840
person who

00:31:26,480 --> 00:31:29,840
was actually in that image

00:31:30,080 --> 00:31:34,880
they um didn't test it on the 44th

00:31:33,039 --> 00:31:37,440
president of the united states

00:31:34,880 --> 00:31:38,240
or if they did they didn't think it was

00:31:37,440 --> 00:31:41,440
a problem

00:31:38,240 --> 00:31:43,440
that it didn't work very well um and

00:31:41,440 --> 00:31:45,440
that particular algorithm as it was

00:31:43,440 --> 00:31:48,240
released into the wild

00:31:45,440 --> 00:31:49,360
worked very poorly for people of color

00:31:48,240 --> 00:31:51,760
if you had dark skin

00:31:49,360 --> 00:31:54,640
the algorithm could make some pretty

00:31:51,760 --> 00:31:58,640
pretty crazy assumptions about you

00:31:54,640 --> 00:31:59,519
um now one of the things that i

00:31:58,640 --> 00:32:00,960
personally love

00:31:59,519 --> 00:32:02,640
about what we're about the business

00:32:00,960 --> 00:32:05,679
we're in is that we can have

00:32:02,640 --> 00:32:08,000
rational conversations about this um

00:32:05,679 --> 00:32:09,760
one researcher just started the gradient

00:32:08,000 --> 00:32:12,000
descent for that algorithm

00:32:09,760 --> 00:32:13,600
in a different place and got a much

00:32:12,000 --> 00:32:15,360
better answer

00:32:13,600 --> 00:32:17,039
okay so it didn't mean they didn't have

00:32:15,360 --> 00:32:19,679
to do anything crazy with the data

00:32:17,039 --> 00:32:20,799
they didn't have to do anything crazy to

00:32:19,679 --> 00:32:23,600
anything they just

00:32:20,799 --> 00:32:24,880
had to try something different to get a

00:32:23,600 --> 00:32:27,279
better answer

00:32:24,880 --> 00:32:28,480
and in a lot of cases that is that is

00:32:27,279 --> 00:32:30,720
what you need to do

00:32:28,480 --> 00:32:31,760
when you find bias in your model because

00:32:30,720 --> 00:32:34,880
bias in your model

00:32:31,760 --> 00:32:36,159
a lot of times means you're not you're

00:32:34,880 --> 00:32:37,039
not modeling what you think you're

00:32:36,159 --> 00:32:40,720
modeling

00:32:37,039 --> 00:32:41,279
for our cost model and i simulated this

00:32:40,720 --> 00:32:43,039
data

00:32:41,279 --> 00:32:46,000
and when i saw the picture that you see

00:32:43,039 --> 00:32:50,159
on this screen i was surprised

00:32:46,000 --> 00:32:53,200
okay so i set this model up so that

00:32:50,159 --> 00:32:54,960
high cost is correlated with bad health

00:32:53,200 --> 00:32:56,559
and then you have this confounding

00:32:54,960 --> 00:32:59,519
factor which is

00:32:56,559 --> 00:33:00,320
um people of color tend to have lower

00:32:59,519 --> 00:33:04,159
costs

00:33:00,320 --> 00:33:04,720
and worse health when i set my training

00:33:04,159 --> 00:33:06,799
label

00:33:04,720 --> 00:33:09,120
it's exactly what you think it should be

00:33:06,799 --> 00:33:10,640
right it's the people on the upper tail

00:33:09,120 --> 00:33:11,519
of the cost distribution this is a

00:33:10,640 --> 00:33:14,640
histogram

00:33:11,519 --> 00:33:16,799
of cost that you're looking at and so

00:33:14,640 --> 00:33:19,120
you know most people are around cost

00:33:16,799 --> 00:33:20,880
four cost level four but you've got some

00:33:19,120 --> 00:33:23,679
people who are really high costs those

00:33:20,880 --> 00:33:25,440
are the people in the blue

00:33:23,679 --> 00:33:27,200
and i've been playing with this model

00:33:25,440 --> 00:33:29,679
for about a week before i thought well

00:33:27,200 --> 00:33:32,240
wait a minute why don't i do a histogram

00:33:29,679 --> 00:33:34,399
of health for the people who've been

00:33:32,240 --> 00:33:37,600
labeled to be high cost

00:33:34,399 --> 00:33:38,799
and see what i see now if you look at

00:33:37,600 --> 00:33:41,840
this

00:33:38,799 --> 00:33:43,760
yes bad health right so

00:33:41,840 --> 00:33:44,960
this negative on health means that

00:33:43,760 --> 00:33:46,880
you're unhealthy

00:33:44,960 --> 00:33:49,200
bad health is associated with higher

00:33:46,880 --> 00:33:52,640
costs this little blue

00:33:49,200 --> 00:33:54,880
um group is shifted compared to the

00:33:52,640 --> 00:33:58,960
orange group

00:33:54,880 --> 00:34:02,480
but they're also more spread out

00:33:58,960 --> 00:34:04,480
okay and they're not

00:34:02,480 --> 00:34:05,600
there are some people in that high cost

00:34:04,480 --> 00:34:08,879
group that are

00:34:05,600 --> 00:34:12,879
average health maybe even above average

00:34:08,879 --> 00:34:16,560
okay simulated data i simulated this

00:34:12,879 --> 00:34:18,639
and this is what happened all you had to

00:34:16,560 --> 00:34:20,159
do in this case was just inspect

00:34:18,639 --> 00:34:21,679
right the sickest people are not the

00:34:20,159 --> 00:34:24,800
most expensive

00:34:21,679 --> 00:34:26,320
and that was the case in the

00:34:24,800 --> 00:34:28,000
in the real life model as well the

00:34:26,320 --> 00:34:29,919
people who were costing the most weren't

00:34:28,000 --> 00:34:32,480
necessarily the sickest

00:34:29,919 --> 00:34:34,320
and so the model that they created based

00:34:32,480 --> 00:34:36,240
on health failed twice

00:34:34,320 --> 00:34:37,919
it didn't lower cost because it wasn't

00:34:36,240 --> 00:34:38,960
targeting the high cost people not

00:34:37,919 --> 00:34:41,839
really

00:34:38,960 --> 00:34:42,720
um it didn't and it didn't and it was

00:34:41,839 --> 00:34:45,280
biased

00:34:42,720 --> 00:34:46,320
right making the high cost people who

00:34:45,280 --> 00:34:48,240
were healthier

00:34:46,320 --> 00:34:49,760
even more healthy was not going to

00:34:48,240 --> 00:34:53,119
reduce your costs

00:34:49,760 --> 00:34:53,119
you needed a different approach

00:34:54,000 --> 00:34:57,040
that's what i like about the what if

00:34:55,359 --> 00:34:58,640
tool it gives you a lot of

00:34:57,040 --> 00:35:00,800
visibility into these simple

00:34:58,640 --> 00:35:03,760
relationships it keeps you from cutting

00:35:00,800 --> 00:35:08,440
your own foot off right

00:35:03,760 --> 00:35:11,680
and in general right software is a tool

00:35:08,440 --> 00:35:14,000
aif360 is a great tool for production

00:35:11,680 --> 00:35:16,160
the what if tool is a nice thumbs up

00:35:14,000 --> 00:35:18,640
thumbs down on your model

00:35:16,160 --> 00:35:20,079
or i'm sorry audit ai is a nice thumbs

00:35:18,640 --> 00:35:22,320
up thumbs down on your model

00:35:20,079 --> 00:35:24,400
and the what if tool is a great

00:35:22,320 --> 00:35:26,079
interactive visualization tool

00:35:24,400 --> 00:35:28,480
there's nothing to stop you from using

00:35:26,079 --> 00:35:28,480
all three

00:35:29,280 --> 00:35:32,400
but really what keeps you from running

00:35:31,200 --> 00:35:35,680
bias models

00:35:32,400 --> 00:35:38,320
is training your team

00:35:35,680 --> 00:35:40,000
training each other reading up on these

00:35:38,320 --> 00:35:40,800
case studies and really understanding

00:35:40,000 --> 00:35:43,359
the issues

00:35:40,800 --> 00:35:44,079
part of that is because this whole thing

00:35:43,359 --> 00:35:46,880
is a new

00:35:44,079 --> 00:35:48,560
field we don't have all the answers yet

00:35:46,880 --> 00:35:51,839
um but part of it is also

00:35:48,560 --> 00:35:53,839
this is this is not easy this is in some

00:35:51,839 --> 00:35:55,599
ways kind of a complex idea and it

00:35:53,839 --> 00:35:57,119
really does depend on your business case

00:35:55,599 --> 00:35:57,760
it depends on what you're doing with the

00:35:57,119 --> 00:35:59,200
model

00:35:57,760 --> 00:36:01,839
and how you're using it to make

00:35:59,200 --> 00:36:04,960
decisions these references

00:36:01,839 --> 00:36:06,560
are a great way to get started okay the

00:36:04,960 --> 00:36:10,079
montreal ai ethics

00:36:06,560 --> 00:36:13,200
institute um lighthouse 3 which is

00:36:10,079 --> 00:36:15,760
dedicated to education and training um

00:36:13,200 --> 00:36:16,240
there's a book called towards a code of

00:36:15,760 --> 00:36:18,800
ai

00:36:16,240 --> 00:36:20,240
ethics which is kind of which explores

00:36:18,800 --> 00:36:24,000
all of the different

00:36:20,240 --> 00:36:26,240
pieces of this including data use

00:36:24,000 --> 00:36:28,400
and the certified ethical emerging

00:36:26,240 --> 00:36:30,400
technologist professional training

00:36:28,400 --> 00:36:32,720
um that's available on coursera it's

00:36:30,400 --> 00:36:33,200
taught by a woman named renee cummings

00:36:32,720 --> 00:36:35,760
from

00:36:33,200 --> 00:36:36,960
um from and she's got an amazing

00:36:35,760 --> 00:36:40,079
background

00:36:36,960 --> 00:36:43,119
and it's it's a six

00:36:40,079 --> 00:36:45,920
course certification process

00:36:43,119 --> 00:36:47,119
that really helps people learn how to

00:36:45,920 --> 00:36:50,320
think about these issues

00:36:47,119 --> 00:36:52,240
and educates you on his

00:36:50,320 --> 00:36:54,560
on the historical context of the data we

00:36:52,240 --> 00:36:58,000
have and how to make good decisions

00:36:54,560 --> 00:36:58,000
based on the data we have

00:36:58,320 --> 00:37:01,440
and as always i recommend you read the

00:37:00,000 --> 00:37:03,839
docs the docs for

00:37:01,440 --> 00:37:05,040
each of these software algorithms are

00:37:03,839 --> 00:37:07,359
actually really really good

00:37:05,040 --> 00:37:09,119
they're a great guide hopefully i've

00:37:07,359 --> 00:37:11,599
simplified it for you given you a better

00:37:09,119 --> 00:37:14,320
idea of where to start

00:37:11,599 --> 00:37:14,720
and um if you have more questions we

00:37:14,320 --> 00:37:17,359
have

00:37:14,720 --> 00:37:18,480
about eight minutes for q a and i would

00:37:17,359 --> 00:37:21,440
love for you to use the q

00:37:18,480 --> 00:37:23,680
a tab we can also connect on linkedin

00:37:21,440 --> 00:37:31,839
and you can find me on github

00:37:23,680 --> 00:37:31,839

YouTube URL: https://www.youtube.com/watch?v=dwGFrHPp4OE


