Title: All Things Open 2015 | Brandon Philips - CoreOS | Application Containers Stack, RKT to Kubernetes
Publication date: 2015-11-19
Playlist: All Things Open 2015
Description: 
	All Things Open 2014 All Things Open 2015, October 19th and 20th, Raleigh NC. All Things Open 2015, October 19th and 20th, Raleigh NC.
Captions: 
	00:00:07,460 --> 00:00:13,769
alright well pretty much all the seats

00:00:11,880 --> 00:00:17,070
are filled up and I think we're about on

00:00:13,769 --> 00:00:20,400
time so I haven't analyzed wash I could

00:00:17,070 --> 00:00:24,570
be off all right well let's go ahead and

00:00:20,400 --> 00:00:26,070
get started so how many of you saw the

00:00:24,570 --> 00:00:29,460
morning keynote I was one of those

00:00:26,070 --> 00:00:32,279
speakers okay great well cruise through

00:00:29,460 --> 00:00:33,840
a lot of the high-level bits and we'll

00:00:32,279 --> 00:00:36,690
kind of spend more time on the technical

00:00:33,840 --> 00:00:38,670
details and then I have a bunch of like

00:00:36,690 --> 00:00:41,730
sort of choose your own adventure bonus

00:00:38,670 --> 00:00:43,410
stuff that we do at the end just kind of

00:00:41,730 --> 00:00:45,600
based on where your your interest lies

00:00:43,410 --> 00:00:47,629
it's a group or probably the interests

00:00:45,600 --> 00:00:50,610
of whoever raises their hand first

00:00:47,629 --> 00:00:54,890
inevitably so this is going to be

00:00:50,610 --> 00:00:56,969
talking about container orchestration so

00:00:54,890 --> 00:00:58,649
sort of like what we covered in the

00:00:56,969 --> 00:01:00,899
keynote was like containers are a way of

00:00:58,649 --> 00:01:03,930
packaging up applications this is more

00:01:00,899 --> 00:01:05,400
about how to actually run those and sort

00:01:03,930 --> 00:01:07,939
of the properties of systems that are

00:01:05,400 --> 00:01:12,119
building these distributed systems with

00:01:07,939 --> 00:01:15,030
containers so right I'm the CTO and

00:01:12,119 --> 00:01:16,619
co-founder of core OS systems engineer

00:01:15,030 --> 00:01:19,860
for a long time i worked at suzhou linux

00:01:16,619 --> 00:01:23,189
etc worked at Rackspace on distributed

00:01:19,860 --> 00:01:25,200
systems a combination of working on

00:01:23,189 --> 00:01:26,850
distributed cloudy stuff at Rackspace

00:01:25,200 --> 00:01:29,130
and working on low-level operating

00:01:26,850 --> 00:01:31,350
system stuff at souza means that i'm

00:01:29,130 --> 00:01:35,100
going to build a cloudy OS along with my

00:01:31,350 --> 00:01:37,439
co-founder alex Povey and so essentially

00:01:35,100 --> 00:01:41,820
what comes out of this is that we had

00:01:37,439 --> 00:01:43,649
the the engineering ability to build no

00:01:41,820 --> 00:01:46,430
s and then also it learned a bunch of

00:01:43,649 --> 00:01:49,439
paper cuts from deploying large

00:01:46,430 --> 00:01:51,600
infrastructures with at the time

00:01:49,439 --> 00:01:53,700
state-of-the-art operating systems so

00:01:51,600 --> 00:01:58,079
where we started was building core OS

00:01:53,700 --> 00:02:00,119
Linux thus the name of the company so

00:01:58,079 --> 00:02:02,729
during the demos I'm going to be using

00:02:00,119 --> 00:02:05,280
this core OS post-coup Brunetti setup

00:02:02,729 --> 00:02:08,310
that we have the docs are up on our

00:02:05,280 --> 00:02:09,929
website essentially Kouros calm and then

00:02:08,310 --> 00:02:11,830
click on the docks and then the second

00:02:09,929 --> 00:02:14,290
section there will be Cooper Nettie's

00:02:11,830 --> 00:02:17,770
the instructions i'll be using or unpaid

00:02:14,290 --> 00:02:20,110
rent but the instructions can be used

00:02:17,770 --> 00:02:23,740
against AWS if you want to charge

00:02:20,110 --> 00:02:26,200
against your employer's account or bare

00:02:23,740 --> 00:02:29,460
metal but definitely the quickest and

00:02:26,200 --> 00:02:32,230
easiest is local machine using vagrant

00:02:29,460 --> 00:02:35,730
also I'm going to be walking through

00:02:32,230 --> 00:02:38,650
some demos of stuff the easiest way to

00:02:35,730 --> 00:02:41,680
try those out I have a readme file in

00:02:38,650 --> 00:02:44,950
there along with the sample replication

00:02:41,680 --> 00:02:49,840
controllers and etc etc just at the top

00:02:44,950 --> 00:02:52,209
of that 2015 all things open so this is

00:02:49,840 --> 00:02:54,970
kind of covered in the keynote so what

00:02:52,209 --> 00:02:58,030
is core OS well core OS is Linux

00:02:54,970 --> 00:02:59,440
distribution and so what we wanted to do

00:02:58,030 --> 00:03:04,660
was create a really really hyper

00:02:59,440 --> 00:03:15,239
consistent Linux distribution for for

00:03:04,660 --> 00:03:17,950
anywhere that Linux could run Wow and so

00:03:15,239 --> 00:03:21,040
unsurprisingly core OS runs anywhere

00:03:17,950 --> 00:03:24,040
that Linux can run it's primarily if you

00:03:21,040 --> 00:03:26,800
want to you know try to think of it as

00:03:24,040 --> 00:03:31,360
through analogy it's a it's a kernel as

00:03:26,800 --> 00:03:33,010
a service it's a container hypervisor

00:03:31,360 --> 00:03:36,850
would be another way of thinking about

00:03:33,010 --> 00:03:39,370
core OS Linux is really minimal linux

00:03:36,850 --> 00:03:42,640
distro runs on the cloud providers and

00:03:39,370 --> 00:03:45,610
then also runs mysteriously on this

00:03:42,640 --> 00:03:47,769
stuff called bare bare metal some of you

00:03:45,610 --> 00:03:51,550
may have it most of us have gotten rid

00:03:47,769 --> 00:03:54,760
of it and we use Stratus topic computers

00:03:51,550 --> 00:03:59,489
now not made out of hardware at all it's

00:03:54,760 --> 00:04:01,330
kind of neat and then also we're a

00:03:59,489 --> 00:04:03,430
combination of a bunch of open source

00:04:01,330 --> 00:04:05,290
communities and projects at CD flannel

00:04:03,430 --> 00:04:08,970
rocket but we'll get it into those

00:04:05,290 --> 00:04:12,299
details in a bit large community of

00:04:08,970 --> 00:04:16,600
developers helping us out on this stuff

00:04:12,299 --> 00:04:20,200
and then we're also a company that's

00:04:16,600 --> 00:04:22,840
building enterprise software so if this

00:04:20,200 --> 00:04:24,039
sort of technology is interesting to you

00:04:22,840 --> 00:04:26,040
but you want to partner in deploying

00:04:24,039 --> 00:04:29,620
these open source technologies

00:04:26,040 --> 00:04:31,780
core OS tectonic is so initially soup to

00:04:29,620 --> 00:04:33,670
nuts platform from core OS Linux up to

00:04:31,780 --> 00:04:35,530
cure minetti's API with a lot of

00:04:33,670 --> 00:04:38,350
technology built-in that we built over

00:04:35,530 --> 00:04:41,340
the last few years one of those pieces

00:04:38,350 --> 00:04:43,270
of technology is Kwai or our key

00:04:41,340 --> 00:04:46,750
depending on where you're from in the

00:04:43,270 --> 00:04:50,620
world but that is a hosting and build

00:04:46,750 --> 00:04:53,380
system for containers so it's able to

00:04:50,620 --> 00:04:55,780
say take your darker file read it from

00:04:53,380 --> 00:04:59,440
your bitbucket build a container for you

00:04:55,780 --> 00:05:02,560
set up a CLS around it you have teams

00:04:59,440 --> 00:05:04,390
and groups and all that stuff so that's

00:05:02,560 --> 00:05:07,330
also something that's part of you know

00:05:04,390 --> 00:05:10,060
the overall tectonic offering so with

00:05:07,330 --> 00:05:12,310
the commercial break over why did we

00:05:10,060 --> 00:05:13,780
build this thing well during the keynote

00:05:12,310 --> 00:05:15,640
I talked about it but essentially this

00:05:13,780 --> 00:05:17,560
idea of a data center as a computer i

00:05:15,640 --> 00:05:20,560
add more machines i get more scale

00:05:17,560 --> 00:05:22,090
machines die i don't care because my

00:05:20,560 --> 00:05:23,550
application continues running and it's

00:05:22,090 --> 00:05:25,900
all about focusing on that application

00:05:23,550 --> 00:05:29,890
the application metrics and making sure

00:05:25,900 --> 00:05:31,180
the application continues to survive so

00:05:29,890 --> 00:05:32,919
we talked about this room the keynote

00:05:31,180 --> 00:05:34,210
too but it's about you as a software

00:05:32,919 --> 00:05:36,760
engineer taking your source code

00:05:34,210 --> 00:05:38,410
packaging it into containers the

00:05:36,760 --> 00:05:40,540
container is interesting because it

00:05:38,410 --> 00:05:43,360
packages everything away from the host

00:05:40,540 --> 00:05:45,400
operating system so you're everything

00:05:43,360 --> 00:05:49,030
required to run that app and a process

00:05:45,400 --> 00:05:50,740
and then we give it a name and then

00:05:49,030 --> 00:05:52,479
hopefully if we do this right we

00:05:50,740 --> 00:05:55,060
actually have signing so that when we

00:05:52,479 --> 00:05:56,530
deploy and provision our hardware we say

00:05:55,060 --> 00:06:00,100
these are the people that are allowed to

00:05:56,530 --> 00:06:03,220
deploy copies and run processes on our

00:06:00,100 --> 00:06:06,310
hosts as operations engineer this

00:06:03,220 --> 00:06:08,700
changes our jobs essentially what

00:06:06,310 --> 00:06:11,080
happens is instead of thinking about

00:06:08,700 --> 00:06:12,610
individual hosts we think about what our

00:06:11,080 --> 00:06:14,620
application needs and how many copies of

00:06:12,610 --> 00:06:17,560
our app we need running in order to

00:06:14,620 --> 00:06:25,360
serve traffic run our map produces

00:06:17,560 --> 00:06:27,940
whatever alright so that's the overall

00:06:25,360 --> 00:06:31,090
vision now how do we actually accomplish

00:06:27,940 --> 00:06:34,990
this what what needs to change and why

00:06:31,090 --> 00:06:37,150
do we start at the operating system how

00:06:34,990 --> 00:06:39,490
many people like really complex API

00:06:37,150 --> 00:06:45,490
surfaces like you're like soap and I

00:06:39,490 --> 00:06:47,500
Sinell okay so it's really difficult to

00:06:45,490 --> 00:06:50,080
maintain something that as a complex API

00:06:47,500 --> 00:06:52,750
service I joke about soap they're really

00:06:50,080 --> 00:06:54,729
great soap api's but soap made it so

00:06:52,750 --> 00:06:56,620
easy to just kind of export stuff from

00:06:54,729 --> 00:06:58,660
objects inside of your middle where the

00:06:56,620 --> 00:07:00,940
end of would really complex AP is and I

00:06:58,660 --> 00:07:01,960
think that's people hated so mostly

00:07:00,940 --> 00:07:03,460
because that's what ended up happening

00:07:01,960 --> 00:07:06,340
not necessarily because of the

00:07:03,460 --> 00:07:10,900
technology and so we all laugh because

00:07:06,340 --> 00:07:12,880
we hate really complex API contracts but

00:07:10,900 --> 00:07:15,610
what we asked our Linux distros to do

00:07:12,880 --> 00:07:18,370
for the last 15 years or so through

00:07:15,610 --> 00:07:20,740
package management which package

00:07:18,370 --> 00:07:23,319
management is what caused the popularity

00:07:20,740 --> 00:07:25,090
of Linux to explode I would argue he is

00:07:23,319 --> 00:07:27,190
at your fingertips you able to access

00:07:25,090 --> 00:07:29,680
you know tens of thousands of pieces of

00:07:27,190 --> 00:07:33,280
software really easily but in the server

00:07:29,680 --> 00:07:36,430
world it complicates things because say

00:07:33,280 --> 00:07:40,509
i'm running debian 6 i'm asking that

00:07:36,430 --> 00:07:42,490
that distro to do vast majority of my

00:07:40,509 --> 00:07:44,560
infrastructure I'm telling them to put a

00:07:42,490 --> 00:07:46,509
single release number on it so I'm

00:07:44,560 --> 00:07:48,880
saying I would like you to maintain a

00:07:46,509 --> 00:07:50,830
set of API is for me for a long period

00:07:48,880 --> 00:07:52,750
of time I want you to abstract my

00:07:50,830 --> 00:07:55,180
hardware away with the colonel I want

00:07:52,750 --> 00:07:57,490
you to provide me to a basic anit system

00:07:55,180 --> 00:07:59,409
I also want you to do ssh make sure that

00:07:57,490 --> 00:08:01,419
remains secure didn't have my language

00:07:59,409 --> 00:08:03,460
runtime my database all this stuff

00:08:01,419 --> 00:08:06,460
packaged into essentially one version of

00:08:03,460 --> 00:08:08,889
a thing as the distro it's a really

00:08:06,460 --> 00:08:10,569
difficult API contract to maintain I

00:08:08,889 --> 00:08:12,520
know this because i worked at souza i

00:08:10,569 --> 00:08:14,740
was passing kernels that were released

00:08:12,520 --> 00:08:17,319
before i even graduate or even started

00:08:14,740 --> 00:08:20,130
college and these maintaining these

00:08:17,319 --> 00:08:22,870
contracts over long periods of time and

00:08:20,130 --> 00:08:25,300
it's just a difficult proposition and

00:08:22,870 --> 00:08:28,120
one that would argue isn't really true

00:08:25,300 --> 00:08:29,590
we promised users as just your maintain

00:08:28,120 --> 00:08:31,659
errs hey we're not going to break stuff

00:08:29,590 --> 00:08:34,750
but i can guarantee everyone has a

00:08:31,659 --> 00:08:37,959
horror story of o is just one simple lib

00:08:34,750 --> 00:08:40,419
c patch and then their entire thing goes

00:08:37,959 --> 00:08:43,270
down right so what we want to do is

00:08:40,419 --> 00:08:45,550
redesign the linux distro so that it's

00:08:43,270 --> 00:08:48,459
actually providing more or fewer api

00:08:45,550 --> 00:08:50,770
contracts so what we essentially do with

00:08:48,459 --> 00:08:52,600
core OS Linux is say our API contracts

00:08:50,770 --> 00:08:55,180
stops at the container runtime

00:08:52,600 --> 00:08:58,509
at that point you have the colonel you

00:08:55,180 --> 00:09:00,100
have what the colonel gives you and then

00:08:58,509 --> 00:09:01,449
we're stopping we'll make sure that

00:09:00,100 --> 00:09:03,160
colonel is up to date or make sure your

00:09:01,449 --> 00:09:05,410
ssh is secure we'll make sure that the

00:09:03,160 --> 00:09:08,949
runtimes stay up to date but we'll stop

00:09:05,410 --> 00:09:12,069
there and so this kind of moves the

00:09:08,949 --> 00:09:14,319
problem around a little bit because now

00:09:12,069 --> 00:09:16,779
suddenly we've given you a wonderful

00:09:14,319 --> 00:09:18,120
gift we've given you the gift of

00:09:16,779 --> 00:09:22,990
maintaining your own language runtime

00:09:18,120 --> 00:09:25,449
your own openssl etc and so this is good

00:09:22,990 --> 00:09:27,699
this is good because we've now separated

00:09:25,449 --> 00:09:29,769
the concerns of application packaging

00:09:27,699 --> 00:09:31,899
from the operating system this

00:09:29,769 --> 00:09:34,060
introduces new challenges and challenges

00:09:31,899 --> 00:09:36,360
that I think containers have yet to have

00:09:34,060 --> 00:09:38,920
a good story around I think will improve

00:09:36,360 --> 00:09:41,139
companies like Google have good stories

00:09:38,920 --> 00:09:44,110
around actually how to maintain this

00:09:41,139 --> 00:09:47,350
complexity over time but you end up with

00:09:44,110 --> 00:09:49,029
odd and sometimes difficult to resolve

00:09:47,350 --> 00:09:51,069
situations like having multiple copies

00:09:49,029 --> 00:09:53,980
of openness to sell in production I

00:09:51,069 --> 00:09:56,709
would start to question why we have

00:09:53,980 --> 00:09:58,500
openssl in most of our applications to

00:09:56,709 --> 00:10:02,410
begin with but that's another discussion

00:09:58,500 --> 00:10:04,019
and so we we've introduced this great

00:10:02,410 --> 00:10:06,220
mechanism of separation of concerns

00:10:04,019 --> 00:10:08,889
we've introduced a few new problems

00:10:06,220 --> 00:10:11,439
things that I think are surmountable

00:10:08,889 --> 00:10:13,779
things that we can we can tackle but

00:10:11,439 --> 00:10:15,699
this is kind of what the opportunity was

00:10:13,779 --> 00:10:20,620
with containers is the separation of

00:10:15,699 --> 00:10:23,410
operating system API concerns so to just

00:10:20,620 --> 00:10:25,209
draw it really loud and clear core OS is

00:10:23,410 --> 00:10:28,930
the little thing on the left and then

00:10:25,209 --> 00:10:32,079
containers on the right and so this

00:10:28,930 --> 00:10:35,079
leads out to the concept of OS

00:10:32,079 --> 00:10:38,079
operations this kind of new thing that

00:10:35,079 --> 00:10:41,589
we can separate out from DevOps or

00:10:38,079 --> 00:10:44,439
infrastructure operations etc the focus

00:10:41,589 --> 00:10:46,000
of just being able to worry and concern

00:10:44,439 --> 00:10:49,509
yourself with just operating system

00:10:46,000 --> 00:10:53,009
itself and so who remembers the battle

00:10:49,509 --> 00:10:55,269
days of internet explorer manual updates

00:10:53,009 --> 00:11:00,449
there's sad times some people are just

00:10:55,269 --> 00:11:03,459
probably in PTSD and not remembering but

00:11:00,449 --> 00:11:04,899
there's a time where the the front end

00:11:03,459 --> 00:11:06,560
of the internet was actually pretty

00:11:04,899 --> 00:11:08,150
terribly insecure ever

00:11:06,560 --> 00:11:11,120
a couple of weeks we were rolling out

00:11:08,150 --> 00:11:14,060
manual updates of Internet Explorer and

00:11:11,120 --> 00:11:15,650
Firefox was a more secure browser but he

00:11:14,060 --> 00:11:18,529
also had this similar like click click

00:11:15,650 --> 00:11:21,200
yes next yes my admin password next yes

00:11:18,529 --> 00:11:22,970
yes alright installed process that I had

00:11:21,200 --> 00:11:26,750
to do every few weeks in order to keep

00:11:22,970 --> 00:11:29,810
my browser secure I mean chrome came

00:11:26,750 --> 00:11:31,390
along and said the API contract of a

00:11:29,810 --> 00:11:33,620
browser is fairly well understood

00:11:31,390 --> 00:11:35,390
they're not perfect at it but it's

00:11:33,620 --> 00:11:38,330
mostly a white box where you type Google

00:11:35,390 --> 00:11:40,550
things into and so they said we can

00:11:38,330 --> 00:11:44,390
probably keep this application better

00:11:40,550 --> 00:11:46,220
secured than the individual user can so

00:11:44,390 --> 00:11:47,810
we'll do that automatically sure enough

00:11:46,220 --> 00:11:50,779
internet explorer and firefox kind of

00:11:47,810 --> 00:11:54,589
followed along and the way we do this

00:11:50,779 --> 00:11:56,510
with core OS is we have we have an A&B

00:11:54,589 --> 00:11:59,210
partition so that we're able to do a

00:11:56,510 --> 00:12:00,950
fully atomic update so we have two

00:11:59,210 --> 00:12:03,260
copies of core OS on every chorus

00:12:00,950 --> 00:12:05,779
machine the active version and then the

00:12:03,260 --> 00:12:07,400
passive version while the active version

00:12:05,779 --> 00:12:09,640
is running in the background our update

00:12:07,400 --> 00:12:15,530
service or update engine sits there

00:12:09,640 --> 00:12:17,089
updates it and then notifies via a few

00:12:15,530 --> 00:12:18,800
different policies either cluster level

00:12:17,089 --> 00:12:21,380
notification or individual host

00:12:18,800 --> 00:12:24,080
notification hey I'd like to reboot in

00:12:21,380 --> 00:12:25,970
order to apply the update the updates

00:12:24,080 --> 00:12:29,390
already been applied but we need to like

00:12:25,970 --> 00:12:32,690
move over to the new kernel and so at

00:12:29,390 --> 00:12:36,320
that point we through bootloader magic

00:12:32,690 --> 00:12:37,970
and grub that we've patched and is going

00:12:36,320 --> 00:12:42,530
upstream but it's hard to work with

00:12:37,970 --> 00:12:44,000
canoe it's going upstream is will

00:12:42,530 --> 00:12:45,530
actually atomically move you over and

00:12:44,000 --> 00:12:47,240
then if the colonel doesn't actually

00:12:45,530 --> 00:12:51,470
come up if we don't actually reach user

00:12:47,240 --> 00:12:54,260
space we do some bootloader tricks on

00:12:51,470 --> 00:12:57,589
the MBR and GPT partition to track

00:12:54,260 --> 00:12:59,510
whether the update was successful and so

00:12:57,589 --> 00:13:01,220
if the machine actually wedges itself

00:12:59,510 --> 00:13:03,140
after that kernel update you can reset

00:13:01,220 --> 00:13:07,040
it and it'll go back to the old version

00:13:03,140 --> 00:13:11,150
and hopefully we'll have a new update

00:13:07,040 --> 00:13:15,709
spun out by then so chorus updates are

00:13:11,150 --> 00:13:18,890
atomic updates with rollback which is

00:13:15,709 --> 00:13:20,150
essentially the whole design mechanism

00:13:18,890 --> 00:13:23,390
of all this stuff is

00:13:20,150 --> 00:13:26,510
you want to design it for atomic meaning

00:13:23,390 --> 00:13:30,590
that we have we have updates that are

00:13:26,510 --> 00:13:32,720
happening without interfering with each

00:13:30,590 --> 00:13:34,490
other we do this at the ellis level you

00:13:32,720 --> 00:13:36,590
do this at the level of the application

00:13:34,490 --> 00:13:38,150
etc and you want to make it really easy

00:13:36,590 --> 00:13:42,050
to roll back if you make a mistake

00:13:38,150 --> 00:13:46,100
because mistakes are inevitable so the

00:13:42,050 --> 00:13:49,610
next piece of OS operations is is actual

00:13:46,100 --> 00:13:51,500
machine configuration so it's not just

00:13:49,610 --> 00:13:54,350
about running processes but figuring out

00:13:51,500 --> 00:14:00,380
how to get the machine to get those

00:13:54,350 --> 00:14:04,520
processes down on to it yeah mostly to

00:14:00,380 --> 00:14:07,760
get the machine into the cluster so this

00:14:04,520 --> 00:14:12,710
font always renders poorly but bear with

00:14:07,760 --> 00:14:15,230
me the basic idea is that when you boot

00:14:12,710 --> 00:14:16,570
up a chorus machine you want to have you

00:14:15,230 --> 00:14:21,620
want to have it joins some type of

00:14:16,570 --> 00:14:23,090
control cluster and so there's a variety

00:14:21,620 --> 00:14:25,280
of different schedulers and things

00:14:23,090 --> 00:14:28,490
people use core OS Linux with mesas they

00:14:25,280 --> 00:14:30,140
use we wrote a scheduler called fleet in

00:14:28,490 --> 00:14:31,820
this example I'll be using the key bulet

00:14:30,140 --> 00:14:34,250
but really they're all the same there's

00:14:31,820 --> 00:14:36,830
a few basic pieces of information you

00:14:34,250 --> 00:14:40,220
need the IP address or dns name of the

00:14:36,830 --> 00:14:43,190
control cluster you need some identity

00:14:40,220 --> 00:14:45,920
information so the in this case the TLS

00:14:43,190 --> 00:14:48,980
certs of the control cluster and the

00:14:45,920 --> 00:14:52,160
private key file of the machine so that

00:14:48,980 --> 00:14:55,370
it can identify itself and identify the

00:14:52,160 --> 00:14:56,450
control cluster it's connecting to and

00:14:55,370 --> 00:14:59,090
then also some miscellaneous

00:14:56,450 --> 00:15:02,060
configuration in this case where the

00:14:59,090 --> 00:15:07,840
clusters DNS resolver is and where the

00:15:02,060 --> 00:15:10,730
the top-level domain for the cluster is

00:15:07,840 --> 00:15:12,740
so at this point what we've gotten up to

00:15:10,730 --> 00:15:14,300
is we've gotten up to an individual

00:15:12,740 --> 00:15:17,030
machine knows how to connect to some

00:15:14,300 --> 00:15:20,870
type of control plane and we stop so

00:15:17,030 --> 00:15:24,620
we've designed the the data center

00:15:20,870 --> 00:15:27,140
equivalent of a botnet which is great it

00:15:24,620 --> 00:15:30,470
makes for nice central control of these

00:15:27,140 --> 00:15:32,420
machines the next piece is cluster

00:15:30,470 --> 00:15:34,010
operations and this is about

00:15:32,420 --> 00:15:35,870
distributing configuration

00:15:34,010 --> 00:15:39,020
across all those worker machines that

00:15:35,870 --> 00:15:40,850
you've just configured and so one

00:15:39,020 --> 00:15:44,690
challenge with cluster operations is

00:15:40,850 --> 00:15:47,360
that we want to design for individual

00:15:44,690 --> 00:15:49,580
machine failure to be okay and in order

00:15:47,360 --> 00:15:52,010
to do this we need some sort of central

00:15:49,580 --> 00:15:56,870
fault tolerant configuration store and

00:15:52,010 --> 00:16:01,460
so we built this thing called etsy d SE

00:15:56,870 --> 00:16:03,140
d stands for / etsy distributed we have

00:16:01,460 --> 00:16:06,260
this strategy where we try to name

00:16:03,140 --> 00:16:10,040
things incomprehensible names so at CD

00:16:06,260 --> 00:16:13,000
quay it's just part of our branding make

00:16:10,040 --> 00:16:16,570
sure that nobody can find anything and

00:16:13,000 --> 00:16:20,090
so / etsy distributed so it acts

00:16:16,570 --> 00:16:21,830
sensually as a cluster level / etsy so

00:16:20,090 --> 00:16:24,200
we're where we store configuration data

00:16:21,830 --> 00:16:26,000
only it's designed quite a bit

00:16:24,200 --> 00:16:27,950
differently it's not a POSIX file system

00:16:26,000 --> 00:16:30,380
although people have written a fuse file

00:16:27,950 --> 00:16:32,780
system where you can mount at CD locally

00:16:30,380 --> 00:16:35,870
which is super adorable I wouldn't

00:16:32,780 --> 00:16:37,490
recommend it for any use case but it it

00:16:35,870 --> 00:16:42,470
is kind of fun to debug and play around

00:16:37,490 --> 00:16:44,030
with so etsy d is designed as a key

00:16:42,470 --> 00:16:46,400
value store so you're able to set and

00:16:44,030 --> 00:16:49,070
put keys but it has this special

00:16:46,400 --> 00:16:52,210
property that it's able to do leader

00:16:49,070 --> 00:16:54,650
elections on its own so a regular NCD

00:16:52,210 --> 00:16:56,990
cluster looks like this you have five

00:16:54,650 --> 00:17:00,290
members generally we say five members

00:16:56,990 --> 00:17:02,840
because you want to be able to have one

00:17:00,290 --> 00:17:05,060
an unexpected outage so Jimmy the intern

00:17:02,840 --> 00:17:07,220
trips over the cable and then one

00:17:05,060 --> 00:17:09,380
planned outage which is like I need to

00:17:07,220 --> 00:17:11,540
upgrade this host or pull a disk or

00:17:09,380 --> 00:17:14,959
whatever so this this gives you some

00:17:11,540 --> 00:17:17,110
tolerance and so Jimmy Jimmy tripped and

00:17:14,959 --> 00:17:21,410
then I took the machine out of rotation

00:17:17,110 --> 00:17:26,510
by LCD remains available and you can

00:17:21,410 --> 00:17:28,190
continue to write into a TD now at CD

00:17:26,510 --> 00:17:29,960
becomes unavailable after a third

00:17:28,190 --> 00:17:33,380
machine and this five machine cluster

00:17:29,960 --> 00:17:35,420
goes down and this is because SED is

00:17:33,380 --> 00:17:36,670
able to do auto leader election the only

00:17:35,420 --> 00:17:40,370
way to make this safe without

00:17:36,670 --> 00:17:42,980
split-brain conditions is for fifty

00:17:40,370 --> 00:17:45,070
percent plus one of the cluster to be

00:17:42,980 --> 00:17:47,550
available so you always have a majority

00:17:45,070 --> 00:17:50,490
and you never split brain

00:17:47,550 --> 00:17:52,830
now that's that's not super interesting

00:17:50,490 --> 00:17:55,200
because most databases have a strong

00:17:52,830 --> 00:17:57,750
leader like postgres and the idea of

00:17:55,200 --> 00:18:00,480
having a a week read-only follower and

00:17:57,750 --> 00:18:03,500
so that wouldn't be super useful at all

00:18:00,480 --> 00:18:06,690
except for etsy d actually designs for

00:18:03,500 --> 00:18:09,720
handling leader failures so if the self

00:18:06,690 --> 00:18:14,040
elected leader of the cluster fails for

00:18:09,720 --> 00:18:18,660
whatever reason at CD within five to ten

00:18:14,040 --> 00:18:22,020
times our TT RTT is generally around

00:18:18,660 --> 00:18:23,190
your network latency generally this is

00:18:22,020 --> 00:18:26,430
going to be around a second or two

00:18:23,190 --> 00:18:28,380
seconds etsy d will figure out that the

00:18:26,430 --> 00:18:30,050
leader has been lost talking about

00:18:28,380 --> 00:18:33,030
themselves and elect a new leader

00:18:30,050 --> 00:18:36,390
meanwhile any rights that you've put in

00:18:33,030 --> 00:18:37,710
tet CD will essentially pause until the

00:18:36,390 --> 00:18:41,130
leader election happens and then those

00:18:37,710 --> 00:18:44,310
rights will go through so the whole

00:18:41,130 --> 00:18:46,320
system is designed essentially to be a

00:18:44,310 --> 00:18:48,810
database for central configuration

00:18:46,320 --> 00:18:51,530
that's tolerant to machine failure and

00:18:48,810 --> 00:18:53,340
this is why a lot of systems that are

00:18:51,530 --> 00:18:54,960
focused on this google like

00:18:53,340 --> 00:18:59,730
infrastructure are starting to build on

00:18:54,960 --> 00:19:01,320
top of things like etsy d oh right and

00:18:59,730 --> 00:19:03,180
then of course if you lose the third

00:19:01,320 --> 00:19:08,600
machine still after leader election at

00:19:03,180 --> 00:19:10,680
cds unavailable so close to operations

00:19:08,600 --> 00:19:12,990
you know they need this configuration

00:19:10,680 --> 00:19:15,720
store but really what it's about is what

00:19:12,990 --> 00:19:17,010
should be running in your cluster so

00:19:15,720 --> 00:19:20,130
there's a lot of different schedulers

00:19:17,010 --> 00:19:22,110
out there kuber Nettie's has a scheduler

00:19:20,130 --> 00:19:25,230
Mace's has a scheduler we wrote a

00:19:22,110 --> 00:19:28,320
scheduler called fleet hachey corp came

00:19:25,230 --> 00:19:30,360
out with one called nomad etc etc

00:19:28,320 --> 00:19:32,160
schedulers are like a fun thing to write

00:19:30,360 --> 00:19:36,060
they're awesome we wrote one it's a lot

00:19:32,160 --> 00:19:37,680
of fun and they they do get to the point

00:19:36,060 --> 00:19:42,930
where they provide a minimum amount of

00:19:37,680 --> 00:19:44,610
usefulness great but you know schedulers

00:19:42,930 --> 00:19:47,190
are just about getting the work to the

00:19:44,610 --> 00:19:49,110
schedule to the individual machines in

00:19:47,190 --> 00:19:52,740
the cluster and that's that's a pretty

00:19:49,110 --> 00:19:55,530
easy job actually we've all done this

00:19:52,740 --> 00:19:57,990
ourselves right like we've all been

00:19:55,530 --> 00:20:00,150
schedulers who's used an actual schedule

00:19:57,990 --> 00:20:00,630
based system like Borgore may Zeus or

00:20:00,150 --> 00:20:03,990
something

00:20:00,630 --> 00:20:05,670
that at work okay very few people so

00:20:03,990 --> 00:20:07,560
we've all we've all written schedulers

00:20:05,670 --> 00:20:09,120
right like this is the scheduler that

00:20:07,560 --> 00:20:12,660
most of us use when we start our careers

00:20:09,120 --> 00:20:16,590
the write some code s copy it to the

00:20:12,660 --> 00:20:20,040
server and then launch it okay who's

00:20:16,590 --> 00:20:23,700
used systemd run who's used screen and

00:20:20,040 --> 00:20:26,580
production okay so systemd run is like

00:20:23,700 --> 00:20:29,760
screen done correctly what it does is it

00:20:26,580 --> 00:20:31,110
creates a service on the command line so

00:20:29,760 --> 00:20:32,880
you do systemd run and then whatever

00:20:31,110 --> 00:20:36,330
command you want and it returns you a

00:20:32,880 --> 00:20:38,370
proper system the service file and then

00:20:36,330 --> 00:20:41,370
you can like check back in on that

00:20:38,370 --> 00:20:43,440
service file you get cgroups information

00:20:41,370 --> 00:20:46,020
you get log information you can actually

00:20:43,440 --> 00:20:47,730
tell the logs of it it's not running as

00:20:46,020 --> 00:20:50,160
your user it doesn't have access to your

00:20:47,730 --> 00:20:52,320
environment variables or SSH agent all

00:20:50,160 --> 00:20:54,780
things that you know the thing you ran

00:20:52,320 --> 00:20:57,780
in screen has access to please stop

00:20:54,780 --> 00:20:59,550
running things in screen but this is the

00:20:57,780 --> 00:21:01,970
scheduler that a lot of us start with is

00:20:59,550 --> 00:21:06,030
the S copy it to a host and run it thing

00:21:01,970 --> 00:21:07,560
whoo we schedule it to the box and then

00:21:06,030 --> 00:21:09,960
maybe we'll get really fancy will use

00:21:07,560 --> 00:21:12,660
like fabric or Capistrano and we'll have

00:21:09,960 --> 00:21:15,690
a set of hosts maybe like eight hosts on

00:21:12,660 --> 00:21:17,190
AWS and then we'll deploy the app so

00:21:15,690 --> 00:21:19,890
we'll run this fabric file and it'll

00:21:17,190 --> 00:21:22,440
just go through one by one teacher the

00:21:19,890 --> 00:21:25,020
host and deploy the app so that that's

00:21:22,440 --> 00:21:26,880
that's something that a lot of us do is

00:21:25,020 --> 00:21:28,410
the next step is I got to automate my

00:21:26,880 --> 00:21:31,170
deploys s copies know all you're

00:21:28,410 --> 00:21:33,330
handling it or maybe we'll get more

00:21:31,170 --> 00:21:35,730
sophisticated and we'll go to dell and

00:21:33,330 --> 00:21:37,590
be like i need some beefy boxes you know

00:21:35,730 --> 00:21:39,780
a couple of beefy boxes in the rack

00:21:37,590 --> 00:21:41,580
because i'm going to be doing some log

00:21:39,780 --> 00:21:43,950
aggregation on these boxes or i'm going

00:21:41,580 --> 00:21:46,770
to be doing a heavy database workload so

00:21:43,950 --> 00:21:48,630
we choose a couple of beefy boxes these

00:21:46,770 --> 00:21:51,300
yellow boxes that are the special boxes

00:21:48,630 --> 00:21:53,040
and we kind of partition or cluster and

00:21:51,300 --> 00:22:00,870
a couple of different things so we

00:21:53,040 --> 00:22:02,640
deploy that app onto those or or maybe

00:22:00,870 --> 00:22:04,260
we'll get really sophisticated will say

00:22:02,640 --> 00:22:06,450
you know I'm going to have a resource of

00:22:04,260 --> 00:22:08,400
where scheduler so I can kind of pick a

00:22:06,450 --> 00:22:10,140
box in my cluster that doesn't have a

00:22:08,400 --> 00:22:12,240
lot of work happening so maybe we'll

00:22:10,140 --> 00:22:14,130
choose will write like a fabric script

00:22:12,240 --> 00:22:16,200
that finds the host with a

00:22:14,130 --> 00:22:18,660
lowest low average so SSH into them and

00:22:16,200 --> 00:22:20,430
run load average and it'll say all right

00:22:18,660 --> 00:22:23,130
host one has the lowest load average and

00:22:20,430 --> 00:22:26,430
so you'll deploy your job to that to

00:22:23,130 --> 00:22:28,440
that host so this is kind of giving us a

00:22:26,430 --> 00:22:31,980
sense of what schedulers actually do

00:22:28,440 --> 00:22:37,010
which is they do things like look for

00:22:31,980 --> 00:22:41,610
hosts inside that we have that that have

00:22:37,010 --> 00:22:44,340
low resource utilization and they free

00:22:41,610 --> 00:22:46,650
us up from this kind of mental thinking

00:22:44,340 --> 00:22:49,620
of well I need some special hosts that

00:22:46,650 --> 00:22:52,110
handle this workload and I need to think

00:22:49,620 --> 00:22:53,940
about which ones are not doing a lot of

00:22:52,110 --> 00:22:55,980
work today so that I can put my jobs

00:22:53,940 --> 00:22:59,190
over there kind of frees us up from

00:22:55,980 --> 00:23:02,220
thinking about these problems and so the

00:22:59,190 --> 00:23:04,880
the basic workflow of a scheduler based

00:23:02,220 --> 00:23:08,760
system like uber Nettie's is that you

00:23:04,880 --> 00:23:11,610
write down on some api's or in some JSON

00:23:08,760 --> 00:23:13,380
or yeah no files that you want some X

00:23:11,610 --> 00:23:15,810
number of copies of your app running you

00:23:13,380 --> 00:23:17,820
talk to the scheduler API the scheduler

00:23:15,810 --> 00:23:20,340
is this little active piece of software

00:23:17,820 --> 00:23:22,440
that whose only job is to look at what

00:23:20,340 --> 00:23:23,490
the human has asked for look at the

00:23:22,440 --> 00:23:25,530
machines and what they're currently

00:23:23,490 --> 00:23:28,260
doing and then schedule that work to

00:23:25,530 --> 00:23:29,910
individual machines and so the reason I

00:23:28,260 --> 00:23:31,890
say schedulers are fun to write is

00:23:29,910 --> 00:23:33,750
because the algorithms really really

00:23:31,890 --> 00:23:36,780
simple but it looks really powerful when

00:23:33,750 --> 00:23:39,420
you do it so it's really just a couple

00:23:36,780 --> 00:23:41,250
of data structures you have the desired

00:23:39,420 --> 00:23:42,720
state which is what the human beings

00:23:41,250 --> 00:23:45,270
have put into the system as what they

00:23:42,720 --> 00:23:46,590
want the computers to be doing and then

00:23:45,270 --> 00:23:49,560
you have the current state which is

00:23:46,590 --> 00:23:50,970
generally it's just the up-to-date

00:23:49,560 --> 00:23:53,490
version of what the computers think

00:23:50,970 --> 00:23:57,810
they're doing currently so you know

00:23:53,490 --> 00:23:59,850
what's running on what hosts you do diff

00:23:57,810 --> 00:24:01,890
of these things and then all the magic

00:23:59,850 --> 00:24:03,870
all the white papers etc are written in

00:24:01,890 --> 00:24:07,350
this other function called schedule

00:24:03,870 --> 00:24:09,870
which is well how do how do we take our

00:24:07,350 --> 00:24:12,840
current state and desired state and then

00:24:09,870 --> 00:24:17,220
figure out an efficient packing that

00:24:12,840 --> 00:24:19,050
will end up being the best fit for the

00:24:17,220 --> 00:24:21,150
work that needs to be done and so that

00:24:19,050 --> 00:24:24,970
schedule routine takes these two things

00:24:21,150 --> 00:24:28,450
and then maps it down onto a host

00:24:24,970 --> 00:24:32,409
so how this works in practice is that we

00:24:28,450 --> 00:24:36,940
say something like khoob CTL run we give

00:24:32,409 --> 00:24:40,179
the intention this idea of a replication

00:24:36,940 --> 00:24:41,740
controller a name so host info we say

00:24:40,179 --> 00:24:44,140
the image it wants to run from and how

00:24:41,740 --> 00:24:46,059
many we need few seconds later the

00:24:44,140 --> 00:24:48,720
system figures it out and actually

00:24:46,059 --> 00:24:51,520
launches a copy of the application and

00:24:48,720 --> 00:24:52,750
then similarly later we can say all

00:24:51,520 --> 00:24:55,510
right cool the application is running

00:24:52,750 --> 00:24:58,630
but now I need two copies of it scale it

00:24:55,510 --> 00:25:00,490
up for me and this is we'll go into

00:24:58,630 --> 00:25:02,110
details of how this works but this is

00:25:00,490 --> 00:25:05,530
why it's important to kind of give our

00:25:02,110 --> 00:25:07,179
thing a name like host info this idea of

00:25:05,530 --> 00:25:08,980
a replication control a name so that we

00:25:07,179 --> 00:25:10,690
can update it later and say you know

00:25:08,980 --> 00:25:12,190
that thing I told you to do where I want

00:25:10,690 --> 00:25:14,289
you to run one of these things will not

00:25:12,190 --> 00:25:16,090
need you to run two of these things you

00:25:14,289 --> 00:25:20,830
need something that's tracking state of

00:25:16,090 --> 00:25:22,270
what your intent was over time and so we

00:25:20,830 --> 00:25:24,850
can think about why this is important

00:25:22,270 --> 00:25:26,530
right now with this example so i have

00:25:24,850 --> 00:25:28,150
this replication controller we can think

00:25:26,530 --> 00:25:31,000
of a replication controller is your

00:25:28,150 --> 00:25:32,320
intent similar to a thermostat so when

00:25:31,000 --> 00:25:34,419
we walk into a room we turn the

00:25:32,320 --> 00:25:36,610
thermostat our intent is for the

00:25:34,419 --> 00:25:38,409
thermostat talk to the heater to make

00:25:36,610 --> 00:25:39,909
whatever is currently the ambient

00:25:38,409 --> 00:25:42,309
temperature the new temperature by

00:25:39,909 --> 00:25:43,809
turning the heater on and off similarly

00:25:42,309 --> 00:25:45,640
we describe our work through these I

00:25:43,809 --> 00:25:49,000
this concept of replication controller

00:25:45,640 --> 00:25:52,900
this replication controller will say is

00:25:49,000 --> 00:25:55,000
web prod it's in control of applications

00:25:52,900 --> 00:25:56,980
that match environment equals prod and

00:25:55,000 --> 00:26:00,820
app equals web and we've told our

00:25:56,980 --> 00:26:02,320
replication controller that's weird we

00:26:00,820 --> 00:26:04,390
told our replication controller that we

00:26:02,320 --> 00:26:07,179
want one of these applications running

00:26:04,390 --> 00:26:09,370
now that the current site on the other

00:26:07,179 --> 00:26:10,840
side shows that there's three versions

00:26:09,370 --> 00:26:12,970
of the application running three copies

00:26:10,840 --> 00:26:18,730
can anyone imagine what's going to

00:26:12,970 --> 00:26:22,419
happen next all right after lunch crowd

00:26:18,730 --> 00:26:24,669
got it got it what's going to happen is

00:26:22,419 --> 00:26:26,590
that the computers are going to change

00:26:24,669 --> 00:26:28,450
the state of the world from three copies

00:26:26,590 --> 00:26:31,059
of application running to one copy

00:26:28,450 --> 00:26:32,980
because the human told the system look

00:26:31,059 --> 00:26:35,440
everything labeled environment equals

00:26:32,980 --> 00:26:37,870
prod a nap equals web I need you

00:26:35,440 --> 00:26:38,320
computer to go talk to all the machines

00:26:37,870 --> 00:26:41,350
and make

00:26:38,320 --> 00:26:43,809
only one of those is running so over

00:26:41,350 --> 00:26:45,880
time sitting there the thermostats

00:26:43,809 --> 00:26:49,480
working working working and we get to

00:26:45,880 --> 00:26:52,330
our desired state all right we're going

00:26:49,480 --> 00:26:54,490
to try a second time let's say that I

00:26:52,330 --> 00:26:56,799
tell the system that i want count equals

00:26:54,490 --> 00:27:00,429
five and would imagine how many copies

00:26:56,799 --> 00:27:02,380
are going to come out of this so the the

00:27:00,429 --> 00:27:04,299
host is the the scheduler system is

00:27:02,380 --> 00:27:06,159
actually going to move talk to the

00:27:04,299 --> 00:27:08,230
machines talk to the scheduler and move

00:27:06,159 --> 00:27:10,450
up to five copies of the app pretty

00:27:08,230 --> 00:27:12,129
straightforward so this is the idea of a

00:27:10,450 --> 00:27:15,190
replication controller sounds

00:27:12,129 --> 00:27:18,070
complicated it's a lot it's a mouthful I

00:27:15,190 --> 00:27:19,539
really hate the name but that's the

00:27:18,070 --> 00:27:21,909
object that they've decided to call it

00:27:19,539 --> 00:27:24,039
replication controller and that's what

00:27:21,909 --> 00:27:29,590
it does it's a thermostat for our

00:27:24,039 --> 00:27:31,539
deployments now I say schedulers are fun

00:27:29,590 --> 00:27:33,279
to write because getting work on to the

00:27:31,539 --> 00:27:36,850
servers is really easy right anyone can

00:27:33,279 --> 00:27:40,570
do that you write for I in sequence 110

00:27:36,850 --> 00:27:43,240
SSH like scheduler system the hard part

00:27:40,570 --> 00:27:45,460
is the service discovery so if I if I

00:27:43,240 --> 00:27:46,870
told you like you know I I just wrote

00:27:45,460 --> 00:27:48,250
this great scheduling system it put all

00:27:46,870 --> 00:27:51,070
the work on the machines I'm heading

00:27:48,250 --> 00:27:52,990
home you'd be pretty upset at me because

00:27:51,070 --> 00:27:54,850
now you have all this work happening on

00:27:52,990 --> 00:27:56,440
your host you have no idea how to load

00:27:54,850 --> 00:27:59,830
balance it you have no idea what version

00:27:56,440 --> 00:28:01,779
is running where and so the second side

00:27:59,830 --> 00:28:04,029
of this is you actually need a pis and

00:28:01,779 --> 00:28:07,509
ways of figuring out where is the work

00:28:04,029 --> 00:28:09,850
that was just scheduled running and then

00:28:07,509 --> 00:28:15,399
you want to expose it in and useful and

00:28:09,850 --> 00:28:18,519
interesting ways dns is it's not the

00:28:15,399 --> 00:28:23,019
best thing that's ever been invented but

00:28:18,519 --> 00:28:26,980
it is is ubiquitous and so q bearnaise

00:28:23,019 --> 00:28:29,019
exposes services as dns entries it also

00:28:26,980 --> 00:28:31,840
will generate load balancers for you so

00:28:29,019 --> 00:28:35,169
l3 load balancers in the next version it

00:28:31,840 --> 00:28:36,820
will generate l7 load balancers and then

00:28:35,169 --> 00:28:38,679
also you can talk to the API directly

00:28:36,820 --> 00:28:41,440
and start to query the API and figure

00:28:38,679 --> 00:28:44,379
out where services have landed that have

00:28:41,440 --> 00:28:47,159
been scheduled in Coober Nettie's has

00:28:44,379 --> 00:28:50,980
this very interesting and powerful

00:28:47,159 --> 00:28:53,230
concept called labels and

00:28:50,980 --> 00:28:56,950
and the labels allow you to essentially

00:28:53,230 --> 00:28:58,660
have queries that that find objects

00:28:56,950 --> 00:29:00,160
inside of the system very very much

00:28:58,660 --> 00:29:02,290
similar to have we have sequel queries

00:29:00,160 --> 00:29:05,350
to find things inside of our structured

00:29:02,290 --> 00:29:07,480
data labels allow us to find things

00:29:05,350 --> 00:29:10,809
objects running services and

00:29:07,480 --> 00:29:12,309
applications inside of our cluster so

00:29:10,809 --> 00:29:16,960
there's a few patterns of how you can

00:29:12,309 --> 00:29:19,030
use labels this a pod essentially is a

00:29:16,960 --> 00:29:21,700
running application running container

00:29:19,030 --> 00:29:23,770
image it's called a pod because you can

00:29:21,700 --> 00:29:27,040
have multiple things inside of the

00:29:23,770 --> 00:29:29,049
running application so imagine that you

00:29:27,040 --> 00:29:31,929
have engine X engine X is serving up

00:29:29,049 --> 00:29:34,720
your static assets for your website and

00:29:31,929 --> 00:29:36,850
you don't want to kill this this engine

00:29:34,720 --> 00:29:38,830
X instance every time you have to update

00:29:36,850 --> 00:29:41,980
one of the images in your static website

00:29:38,830 --> 00:29:43,900
right so a pod that for your static

00:29:41,980 --> 00:29:46,360
website could be engine X and then it

00:29:43,900 --> 00:29:48,160
could be this little Damon that sits in

00:29:46,360 --> 00:29:50,470
a while loop pulling from a git

00:29:48,160 --> 00:29:52,600
repository or are sinking from a

00:29:50,470 --> 00:29:54,850
repository and so these two things are

00:29:52,600 --> 00:29:56,320
like logically coupled and they're like

00:29:54,850 --> 00:29:57,640
a single application but there's two

00:29:56,320 --> 00:30:00,250
pieces of code that are operating

00:29:57,640 --> 00:30:02,080
together those ninja next thing serving

00:30:00,250 --> 00:30:04,660
up a chibi traffic and the sinker thing

00:30:02,080 --> 00:30:06,730
so this is the concept of a pod and the

00:30:04,660 --> 00:30:09,250
pod is the ubiquitous like addressable

00:30:06,730 --> 00:30:13,360
running application inside of Cooper

00:30:09,250 --> 00:30:15,580
Nettie's now pods you can label so when

00:30:13,360 --> 00:30:17,650
I say I want to run an application I can

00:30:15,580 --> 00:30:20,890
label it say environment calls dev at

00:30:17,650 --> 00:30:23,500
the equals web environment equals test a

00:30:20,890 --> 00:30:25,480
pickles web vitam equals prod app equals

00:30:23,500 --> 00:30:26,950
web and this is a very common pattern

00:30:25,480 --> 00:30:28,929
that a lot of us used is having these

00:30:26,950 --> 00:30:31,690
these different deployments of our

00:30:28,929 --> 00:30:33,840
application up and running at a time now

00:30:31,690 --> 00:30:36,309
this actually starts to get fun when we

00:30:33,840 --> 00:30:39,929
think about adding load balancers to the

00:30:36,309 --> 00:30:42,490
situation so we can have a service that

00:30:39,929 --> 00:30:45,040
selects for environment equals dev and

00:30:42,490 --> 00:30:48,820
puts it behind test example calm for our

00:30:45,040 --> 00:30:51,730
application or we can have beta example

00:30:48,820 --> 00:30:54,309
calm that selects for in vehicles tests

00:30:51,730 --> 00:30:55,960
and in vehicles prod and so you can

00:30:54,309 --> 00:30:59,080
start to test well how's this rolling

00:30:55,960 --> 00:31:01,240
update going to look when I have to have

00:30:59,080 --> 00:31:02,799
you know our testing version in our prod

00:31:01,240 --> 00:31:04,760
bersin behind the same load balancer

00:31:02,799 --> 00:31:06,860
which is inevitable

00:31:04,760 --> 00:31:08,690
how is that going to look for users are

00:31:06,860 --> 00:31:10,940
we going to have quirky UX behavior etc

00:31:08,690 --> 00:31:12,410
and then of course you can have your

00:31:10,940 --> 00:31:14,960
production load balancer that has

00:31:12,410 --> 00:31:17,810
selects only for environment equals prod

00:31:14,960 --> 00:31:20,390
and so by using these label queries we

00:31:17,810 --> 00:31:23,150
actually start to be able to design for

00:31:20,390 --> 00:31:25,340
it and map the way our cluster operates

00:31:23,150 --> 00:31:28,160
to the way that we think about things

00:31:25,340 --> 00:31:31,760
the way we organize systems for humans

00:31:28,160 --> 00:31:34,670
to consume and naturally of course these

00:31:31,760 --> 00:31:36,590
these selections you label selections

00:31:34,670 --> 00:31:38,960
work across all instances that are

00:31:36,590 --> 00:31:44,030
labeled this way so whether that's one

00:31:38,960 --> 00:31:45,530
or five or a thousand copies you can

00:31:44,030 --> 00:31:48,200
also start to do interesting things with

00:31:45,530 --> 00:31:51,460
versioning so I can say you know app

00:31:48,200 --> 00:31:54,170
equals foo so I have a service that

00:31:51,460 --> 00:31:57,140
finds all versions of food no matter if

00:31:54,170 --> 00:31:58,760
it's version one or two or three and so

00:31:57,140 --> 00:32:00,830
you can imagine like moving a load

00:31:58,760 --> 00:32:04,100
balancer doing Bluegreen deploys that

00:32:00,830 --> 00:32:06,410
sort of thing and the way this is done

00:32:04,100 --> 00:32:09,110
from the command line and through the

00:32:06,410 --> 00:32:11,000
API of Cooper Nettie's is you you have

00:32:09,110 --> 00:32:13,700
your replication controller which maps

00:32:11,000 --> 00:32:15,950
to a number of pods that have been

00:32:13,700 --> 00:32:18,130
launched those pods have labels of some

00:32:15,950 --> 00:32:20,450
sort that you've set up and you can say

00:32:18,130 --> 00:32:23,780
given the replication controller and

00:32:20,450 --> 00:32:26,810
it's labels I want you to expose a port

00:32:23,780 --> 00:32:29,600
for me that will be accessible to

00:32:26,810 --> 00:32:31,670
cluster and what this actually does is

00:32:29,600 --> 00:32:34,730
it generates a random port number and

00:32:31,670 --> 00:32:37,250
maps it to an external IP address on

00:32:34,730 --> 00:32:40,040
your worker machines will get into why

00:32:37,250 --> 00:32:42,470
this is necessary but the basic idea is

00:32:40,040 --> 00:32:46,760
that because most of our hosts today

00:32:42,470 --> 00:32:48,680
only have a single IP so when we think

00:32:46,760 --> 00:32:50,420
about hosts we don't think about hosts

00:32:48,680 --> 00:32:52,490
running lots and lots of applications we

00:32:50,420 --> 00:32:54,560
think about a single host and that host

00:32:52,490 --> 00:32:56,590
has a number of ports and those

00:32:54,560 --> 00:32:58,910
applications are through those ports

00:32:56,590 --> 00:33:01,130
inside of Cooper Nettie's every single

00:32:58,910 --> 00:33:02,750
pod gets its own IP address and that IP

00:33:01,130 --> 00:33:05,030
address needs to be routable between

00:33:02,750 --> 00:33:06,710
pods between hosts so it starts to look

00:33:05,030 --> 00:33:08,720
a lot more like a network that we would

00:33:06,710 --> 00:33:10,730
use inside of a infrastructure as a

00:33:08,720 --> 00:33:12,350
service where you have you know in

00:33:10,730 --> 00:33:14,330
number of virtual machines and those

00:33:12,350 --> 00:33:16,280
virtual machines you may be able to

00:33:14,330 --> 00:33:17,320
address other virtual machines on other

00:33:16,280 --> 00:33:18,880
hosts

00:33:17,320 --> 00:33:21,730
and a similar weight kuber Nettie's has

00:33:18,880 --> 00:33:23,230
every Podesta's an IP address this

00:33:21,730 --> 00:33:26,139
doesn't map very well to a lot of

00:33:23,230 --> 00:33:27,700
network fabrics that we have inside of

00:33:26,139 --> 00:33:30,309
our data centers inside of our clouds

00:33:27,700 --> 00:33:32,200
etc so this nerd port allows us to

00:33:30,309 --> 00:33:34,509
expose on worker machines this

00:33:32,200 --> 00:33:36,639
consistent IP address the kind of maps

00:33:34,509 --> 00:33:38,620
very logically to the world that we have

00:33:36,639 --> 00:33:40,299
today where a lot of us have this text

00:33:38,620 --> 00:33:42,100
file internally that's version

00:33:40,299 --> 00:33:46,139
controlled and everyone hates but it's

00:33:42,100 --> 00:33:50,230
like you know port 30 2158 is

00:33:46,139 --> 00:33:52,600
application foo and port 30 2159 as

00:33:50,230 --> 00:33:54,460
application bar and this is kind of how

00:33:52,600 --> 00:33:58,389
we do load balancing etc in our data

00:33:54,460 --> 00:34:00,370
centers so how does this look in

00:33:58,389 --> 00:34:02,679
practice these are a lot of moving

00:34:00,370 --> 00:34:04,659
pieces there's there's a lot to

00:34:02,679 --> 00:34:08,470
understand and digest here but houses

00:34:04,659 --> 00:34:11,859
look in practice we have a single

00:34:08,470 --> 00:34:13,839
scheduler API we have at CD which is

00:34:11,859 --> 00:34:17,649
acting as a database and then we have a

00:34:13,839 --> 00:34:19,450
lot of machines so one thing that people

00:34:17,649 --> 00:34:22,510
get super afraid of is there like whoa I

00:34:19,450 --> 00:34:24,089
have nothing like Google like needs any

00:34:22,510 --> 00:34:26,589
like five machines to run my application

00:34:24,089 --> 00:34:29,919
well that's fine this architecture

00:34:26,589 --> 00:34:32,409
scales down just fine so right now on my

00:34:29,919 --> 00:34:34,599
host i have four machines that CD

00:34:32,409 --> 00:34:37,149
machine schedule api and a worker

00:34:34,599 --> 00:34:39,369
machine and then if you're getting

00:34:37,149 --> 00:34:41,530
really resource-constrained it actually

00:34:39,369 --> 00:34:42,520
scales down just fine to one node where

00:34:41,530 --> 00:34:45,369
you have the worker machine in the

00:34:42,520 --> 00:34:47,290
scheduler and everything on one host and

00:34:45,369 --> 00:34:48,879
that's that's an option that you have to

00:34:47,290 --> 00:34:50,770
if you want to run it on vagrant run it

00:34:48,879 --> 00:34:52,990
on a single AWS host or something like

00:34:50,770 --> 00:34:57,220
that that's what I use for my IRC

00:34:52,990 --> 00:34:59,319
bouncer so it kind of just scales down

00:34:57,220 --> 00:35:01,480
to it's just about having an API and

00:34:59,319 --> 00:35:06,220
then having that API launch processes on

00:35:01,480 --> 00:35:08,470
your host alright so I'm going to dive

00:35:06,220 --> 00:35:10,380
into a few minutes of demos here again

00:35:08,470 --> 00:35:13,599
if you want to try this out at home

00:35:10,380 --> 00:35:20,650
corros com docs q brunette ease you can

00:35:13,599 --> 00:35:22,960
find this all there sooo so I have a

00:35:20,650 --> 00:35:25,390
couple of things one is that I have a

00:35:22,960 --> 00:35:27,359
cluster that's running rocket so it's

00:35:25,390 --> 00:35:30,670
cooper Nettie's plus the rocket run time

00:35:27,359 --> 00:35:32,980
we built rocket as a alternate

00:35:30,670 --> 00:35:34,420
native to your doctor because we had a

00:35:32,980 --> 00:35:37,270
number of things that we wanted to see

00:35:34,420 --> 00:35:38,920
dr. doing differently we can get into

00:35:37,270 --> 00:35:41,020
those details we also have a document

00:35:38,920 --> 00:35:44,290
that explains in detail but it's mostly

00:35:41,020 --> 00:35:46,180
about process model rocket follows a

00:35:44,290 --> 00:35:48,369
very traditional unix process model way

00:35:46,180 --> 00:35:51,579
when i say rocket run it actually runs

00:35:48,369 --> 00:35:54,250
as a child or as the process that

00:35:51,579 --> 00:35:56,260
executed rocket run docker has this

00:35:54,250 --> 00:35:58,059
whole RPC mechanism where you type dr.

00:35:56,260 --> 00:35:59,319
run and you get logs out to your

00:35:58,059 --> 00:36:01,270
standard out but it's actually talking

00:35:59,319 --> 00:36:04,059
to a daemon and that Damon fork to use X

00:36:01,270 --> 00:36:06,369
things it causes problems overall with

00:36:04,059 --> 00:36:07,809
the architecture so we built rocket to

00:36:06,369 --> 00:36:09,430
fix that and a bunch of other things

00:36:07,809 --> 00:36:11,470
that we used like didn't like

00:36:09,430 --> 00:36:18,069
architectural II about the rocket the

00:36:11,470 --> 00:36:24,510
dr. Damon so i have i have this example

00:36:18,069 --> 00:36:28,420
cluster here use condom you see oh

00:36:24,510 --> 00:36:30,520
goodness fib cdale convict so one nice

00:36:28,420 --> 00:36:32,950
thing about crew benetti's is it has

00:36:30,520 --> 00:36:34,630
this really nice command-line tool

00:36:32,950 --> 00:36:38,520
called coop CTL can everyone see the

00:36:34,630 --> 00:36:41,410
text command line tool called coop CTL

00:36:38,520 --> 00:36:44,109
and coops echale allows you to address

00:36:41,410 --> 00:36:46,900
lots of different clusters similar to

00:36:44,109 --> 00:36:48,069
the AWS command line tools so I'm saying

00:36:46,900 --> 00:36:51,280
that I want to use my vagrant

00:36:48,069 --> 00:36:53,559
single-member cluster here and so we're

00:36:51,280 --> 00:36:55,720
able to check that API endpoint works

00:36:53,559 --> 00:36:58,569
correctly by you know saying get pause

00:36:55,720 --> 00:37:02,410
or get nodes nodes or the actual worker

00:36:58,569 --> 00:37:04,299
machines in the cluster so that all

00:37:02,410 --> 00:37:07,510
works great so what I'm going to be

00:37:04,299 --> 00:37:10,359
doing here is I've selected the right

00:37:07,510 --> 00:37:12,400
cluster so I'm going to do is launch a

00:37:10,359 --> 00:37:14,200
service so I have this really simple

00:37:12,400 --> 00:37:17,109
service that I wrote called host info

00:37:14,200 --> 00:37:19,990
just dumped some IP address and and

00:37:17,109 --> 00:37:22,990
other metadata about the inside of the

00:37:19,990 --> 00:37:25,630
container so we'll run cube CTL run this

00:37:22,990 --> 00:37:27,220
and so what will happen is that this

00:37:25,630 --> 00:37:29,380
will create a replication controller

00:37:27,220 --> 00:37:33,010
that uses that name and just launched

00:37:29,380 --> 00:37:35,500
one copy of it so Cooper Nettie's has

00:37:33,010 --> 00:37:38,589
this really nice Vincent interface so

00:37:35,500 --> 00:37:41,440
I'm able to kind of get a tail the log

00:37:38,589 --> 00:37:43,420
of what's happening and so what we'll

00:37:41,440 --> 00:37:44,410
see here is that it's really gross and

00:37:43,420 --> 00:37:47,380
ugly sorry

00:37:44,410 --> 00:37:49,960
big fonts but it says at the very bottom

00:37:47,380 --> 00:37:51,970
that the container has been started so

00:37:49,960 --> 00:37:55,210
now I should be able to do get pods and

00:37:51,970 --> 00:37:58,060
see that there's one one copy of host

00:37:55,210 --> 00:37:59,680
info running now the next thing is like

00:37:58,060 --> 00:38:01,120
that's sort of interesting but now I

00:37:59,680 --> 00:38:02,680
just know that there's a process running

00:38:01,120 --> 00:38:05,170
I actually need to be able to hit it

00:38:02,680 --> 00:38:08,410
behind a load balancer so this next step

00:38:05,170 --> 00:38:12,430
is to say I want to expose through a

00:38:08,410 --> 00:38:15,220
node port 548 3 which is the port that

00:38:12,430 --> 00:38:17,860
this application listens on expose this

00:38:15,220 --> 00:38:20,680
as a service so now we should be able to

00:38:17,860 --> 00:38:23,800
do Coop's et al get service host info

00:38:20,680 --> 00:38:26,830
and we'll see that there's a load

00:38:23,800 --> 00:38:28,660
balancer running called host info and we

00:38:26,830 --> 00:38:30,910
if we describe it we should see that it

00:38:28,660 --> 00:38:34,840
is signed it a high port number for us

00:38:30,910 --> 00:38:37,930
to hit so the next thing to do here is

00:38:34,840 --> 00:38:42,070
that we'll go ahead and hit it on our

00:38:37,930 --> 00:38:43,600
web browser so the IP address of the

00:38:42,070 --> 00:38:46,930
virtual machine that this is running on

00:38:43,600 --> 00:38:49,330
is 172 17 499 so then we plug in the

00:38:46,930 --> 00:38:53,620
pork and then woohoo it worked live

00:38:49,330 --> 00:38:55,390
demos and so we we essentially set up a

00:38:53,620 --> 00:38:57,280
load balancer set up a basic stateless

00:38:55,390 --> 00:38:58,840
web application and that stateless web

00:38:57,280 --> 00:39:01,600
application dumps out some metadata

00:38:58,840 --> 00:39:04,720
about the host it's running on so that

00:39:01,600 --> 00:39:07,020
that's great and then what's neat here

00:39:04,720 --> 00:39:09,940
is that we can start to really trivially

00:39:07,020 --> 00:39:12,400
scale it up so we can say all right I

00:39:09,940 --> 00:39:15,880
have one copy but I want multiple copies

00:39:12,400 --> 00:39:17,740
so if we go through here and get pods

00:39:15,880 --> 00:39:19,390
again we should see that there's one

00:39:17,740 --> 00:39:21,640
copy running and one copying pending

00:39:19,390 --> 00:39:23,140
okay that will get added to the load

00:39:21,640 --> 00:39:26,590
balancer and actually eventually get

00:39:23,140 --> 00:39:31,600
running general air perfect live demos

00:39:26,590 --> 00:39:35,200
okay so it's probably pulling it out

00:39:31,600 --> 00:39:42,310
there we go so at this point we should

00:39:35,200 --> 00:39:49,000
all do things like watch oops watch and

00:39:42,310 --> 00:39:50,590
fetch the actual application and be

00:39:49,000 --> 00:39:52,030
hitting it behind load balancer and

00:39:50,590 --> 00:39:53,980
you'll see it's flopping between number

00:39:52,030 --> 00:39:55,270
of visitors of five and forty seven

00:39:53,980 --> 00:39:56,590
because I'm hitting two things behind

00:39:55,270 --> 00:40:02,170
below balancer

00:39:56,590 --> 00:40:05,830
okay I'm so cool now I have a little bit

00:40:02,170 --> 00:40:09,340
of time for more demo but these are 40

00:40:05,830 --> 00:40:10,540
minute sessions right yeah yeah okay so

00:40:09,340 --> 00:40:13,630
what I'm going to do here is pause the

00:40:10,540 --> 00:40:16,570
demo I'd love to go further into this

00:40:13,630 --> 00:40:18,250
stuff if you want to grab me in the

00:40:16,570 --> 00:40:19,930
hallway I'd love to go through more of

00:40:18,250 --> 00:40:25,260
it I leave a little bit of time for

00:40:19,930 --> 00:40:25,260
questions and call it that so thank you

00:40:31,730 --> 00:40:47,430
hey yes yep right so core OS we use

00:40:43,980 --> 00:40:49,050
upstream kernel.org kernels these live

00:40:47,430 --> 00:40:50,790
patching things there's a lot of these

00:40:49,050 --> 00:40:53,040
live patching things Oracle as well and

00:40:50,790 --> 00:40:56,250
suz as one I think Red Hat has one now

00:40:53,040 --> 00:40:59,250
these are great for I'm running like air

00:40:56,250 --> 00:41:01,349
traffic control and we found that life

00:40:59,250 --> 00:41:03,000
there's an mm leak and I really can't

00:41:01,349 --> 00:41:06,810
reboot this air traffic control right

00:41:03,000 --> 00:41:09,359
now but it's not going to be like the we

00:41:06,810 --> 00:41:11,550
introduced a new Phi and our network

00:41:09,359 --> 00:41:15,540
controller or like any of these more

00:41:11,550 --> 00:41:17,280
complex patches and so all of those live

00:41:15,540 --> 00:41:19,470
patches require human intervention and

00:41:17,280 --> 00:41:21,780
suddenly your kernel is completely

00:41:19,470 --> 00:41:23,400
inconsistent because the ONS disk state

00:41:21,780 --> 00:41:26,040
of your kernel and the live state of

00:41:23,400 --> 00:41:27,960
your kernel are divergent and so what

00:41:26,040 --> 00:41:30,270
we're driving for is fault tolerance of

00:41:27,960 --> 00:41:32,099
individual hosts and extreme consistency

00:41:30,270 --> 00:41:34,290
and live patching is just not something

00:41:32,099 --> 00:41:35,670
that we're interested in doing there are

00:41:34,290 --> 00:41:37,800
some interesting things you can do with

00:41:35,670 --> 00:41:39,359
checkpoint restore and stuff that's more

00:41:37,800 --> 00:41:48,319
research eat like a few years from now

00:41:39,359 --> 00:41:48,319
live updating in the colonel yes yeah

00:41:52,160 --> 00:41:56,930
yeah so that was the next part of the

00:41:54,530 --> 00:41:59,480
demo there's so the neat thing about

00:41:56,930 --> 00:42:01,849
putting up sorry so the question was

00:41:59,480 --> 00:42:04,130
with microservices is there any view

00:42:01,849 --> 00:42:05,930
where you can see like this app talks to

00:42:04,130 --> 00:42:08,809
that app and etc etc kind of

00:42:05,930 --> 00:42:10,849
decomposition of the application and so

00:42:08,809 --> 00:42:13,010
the nice thing about this idea of

00:42:10,849 --> 00:42:14,750
services and labels and replication

00:42:13,010 --> 00:42:17,450
controllers is that you can really

00:42:14,750 --> 00:42:19,549
trivially start to build those

00:42:17,450 --> 00:42:21,799
architecture diagrams because you can

00:42:19,549 --> 00:42:24,049
say this service depends on this other

00:42:21,799 --> 00:42:26,750
service and then you can get a view of

00:42:24,049 --> 00:42:28,520
well that other service has 18 copies

00:42:26,750 --> 00:42:31,250
running on these hosts and then you can

00:42:28,520 --> 00:42:33,109
actually like drill down from logical

00:42:31,250 --> 00:42:35,720
service the dependent services the

00:42:33,109 --> 00:42:37,069
individual hosts and all the metadata is

00:42:35,720 --> 00:42:39,500
there there hasn't been a great

00:42:37,069 --> 00:42:41,150
visualization of it yet it's a conic

00:42:39,500 --> 00:42:43,130
dashboard that we have allows you to

00:42:41,150 --> 00:42:45,500
drill down for individual like services

00:42:43,130 --> 00:42:47,420
to pods to host but I think over time

00:42:45,500 --> 00:42:49,490
you'll see more of those sorts of things

00:42:47,420 --> 00:42:51,260
I think it gets more interesting once

00:42:49,490 --> 00:42:53,839
you think about like network flow

00:42:51,260 --> 00:42:56,359
analysis and that sort of thing or even

00:42:53,839 --> 00:42:58,369
adding in RPC awareness so you can say

00:42:56,359 --> 00:43:00,740
this customer request was tagged with

00:42:58,369 --> 00:43:02,900
fubar XYZ requests and you actually

00:43:00,740 --> 00:43:05,210
track it through the system between

00:43:02,900 --> 00:43:07,309
hosts that's all future work and we have

00:43:05,210 --> 00:43:10,609
to like lay the foundation and we're

00:43:07,309 --> 00:43:12,440
laying foundation right now all right

00:43:10,609 --> 00:43:14,599
I'm at a time if you want some more

00:43:12,440 --> 00:43:16,720
stickers feel free and otherwise thank

00:43:14,599 --> 00:43:16,720

YouTube URL: https://www.youtube.com/watch?v=F8qBqcF94Zg


