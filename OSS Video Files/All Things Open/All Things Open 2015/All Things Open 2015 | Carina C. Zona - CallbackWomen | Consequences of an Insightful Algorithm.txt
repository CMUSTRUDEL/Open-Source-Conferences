Title: All Things Open 2015 | Carina C. Zona - CallbackWomen | Consequences of an Insightful Algorithm
Publication date: 2015-11-19
Playlist: All Things Open 2015
Description: 
	All Things Open 2014 All Things Open 2015, October 19th and 20th, Raleigh NC. All Things Open 2015, October 19th and 20th, Raleigh NC.
Captions: 
	00:00:04,700 --> 00:00:10,800
so my name is criticized ona I am a

00:00:08,700 --> 00:00:12,360
developer developer evangelist and an

00:00:10,800 --> 00:00:14,309
advocate I'm a founder of call back

00:00:12,360 --> 00:00:15,859
women among other things and I really

00:00:14,309 --> 00:00:17,880
like to talk about the unintended

00:00:15,859 --> 00:00:19,410
consequences we have of ordinary

00:00:17,880 --> 00:00:20,609
decisions will make its programmer so

00:00:19,410 --> 00:00:22,890
that's what this talk is really going to

00:00:20,609 --> 00:00:25,170
be dealing with as well in particular

00:00:22,890 --> 00:00:26,849
this talk is a toolkit for empathetic

00:00:25,170 --> 00:00:28,769
coding we're going to be delving into

00:00:26,849 --> 00:00:30,720
some specific examples of a critical

00:00:28,769 --> 00:00:32,550
programming and the painful results from

00:00:30,720 --> 00:00:35,059
doing things in ways that are initially

00:00:32,550 --> 00:00:36,989
really just been only intended and

00:00:35,059 --> 00:00:38,399
because I want to deal with empathy i

00:00:36,989 --> 00:00:40,050
want to start here with a content

00:00:38,399 --> 00:00:41,730
warning for all of you I'll be delving

00:00:40,050 --> 00:00:43,680
into some examples that deal with pretty

00:00:41,730 --> 00:00:45,600
sensitive topics including grief PTSD

00:00:43,680 --> 00:00:48,000
depression miscarriage and fertility

00:00:45,600 --> 00:00:49,920
sexual history consent surveillance

00:00:48,000 --> 00:00:51,719
racial profiling and the Holocaust and

00:00:49,920 --> 00:00:54,420
while none of these are the main topic

00:00:51,719 --> 00:00:55,949
they will come up so that's starting

00:00:54,420 --> 00:00:57,360
about 5-10 minutes you got a little time

00:00:55,949 --> 00:00:59,579
to decide whether that's of interest to

00:00:57,360 --> 00:01:03,239
you or not I hope you will stay I think

00:00:59,579 --> 00:01:06,180
you'll find it really interesting so we

00:01:03,239 --> 00:01:08,700
are able to extract remarkably precise

00:01:06,180 --> 00:01:10,890
insights about an individual these days

00:01:08,700 --> 00:01:12,990
the question is do we have a right to

00:01:10,890 --> 00:01:14,970
know what they didn't consent to share

00:01:12,990 --> 00:01:16,830
with us even when they've shared

00:01:14,970 --> 00:01:19,320
something willingly that led us there

00:01:16,830 --> 00:01:21,119
and then we have to ask ourselves how do

00:01:19,320 --> 00:01:24,960
we mitigate against unintended

00:01:21,119 --> 00:01:26,250
consequences from doing that it helps to

00:01:24,960 --> 00:01:27,990
start we just asking me a general

00:01:26,250 --> 00:01:29,880
question of what is an algorithm to find

00:01:27,990 --> 00:01:32,009
just very generically it's a

00:01:29,880 --> 00:01:34,020
step-by-step set of operations for

00:01:32,009 --> 00:01:35,820
predictably arriving at an outcome and

00:01:34,020 --> 00:01:37,110
predictably is going to end up being

00:01:35,820 --> 00:01:39,930
something that's really key for us today

00:01:37,110 --> 00:01:43,290
so step by step set of operations for

00:01:39,930 --> 00:01:44,579
predictably arriving at an outcome so of

00:01:43,290 --> 00:01:46,320
course typically when we're talking to

00:01:44,579 --> 00:01:47,939
algorithms we need something that's

00:01:46,320 --> 00:01:50,280
patterns of instructions that are being

00:01:47,939 --> 00:01:52,649
articulated in code or perhaps in

00:01:50,280 --> 00:01:54,750
mathematical formulas but you can also

00:01:52,649 --> 00:01:56,939
think of them as algorithms are

00:01:54,750 --> 00:01:58,290
throughout everyday life they are just

00:01:56,939 --> 00:01:58,930
patterns of instructions that can be

00:01:58,290 --> 00:02:01,210
articulated

00:01:58,930 --> 00:02:06,030
various ways such as for instance a

00:02:01,210 --> 00:02:08,440
recipe or instructions maps directions

00:02:06,030 --> 00:02:10,570
even a crochet pattern is an algorithm

00:02:08,440 --> 00:02:13,960
as well and I would argue a lot more

00:02:10,570 --> 00:02:16,540
complicated than your average code so

00:02:13,960 --> 00:02:19,630
deep learning is the new hotness right

00:02:16,540 --> 00:02:21,700
now for mining data using artificial

00:02:19,630 --> 00:02:23,290
intelligence specifically the branch of

00:02:21,700 --> 00:02:25,570
artificial intelligence that's machine

00:02:23,290 --> 00:02:28,600
learning so essentially deep learning is

00:02:25,570 --> 00:02:30,910
algorithms for fast trainable artificial

00:02:28,600 --> 00:02:32,050
neural networks it's a branch of machine

00:02:30,910 --> 00:02:34,570
learning that's been around for a few

00:02:32,050 --> 00:02:36,760
decades at least since the 1980s but

00:02:34,570 --> 00:02:38,950
mostly in theoretical scale and mostly

00:02:36,760 --> 00:02:40,840
really trapped in academia but in recent

00:02:38,950 --> 00:02:44,260
advances just in past couple years since

00:02:40,840 --> 00:02:46,530
around 2012 13 deep learning has now

00:02:44,260 --> 00:02:50,830
become realistically able to extract

00:02:46,530 --> 00:02:52,930
insights out of the vastness of deep big

00:02:50,830 --> 00:02:57,790
data and we're talking about in

00:02:52,930 --> 00:03:00,700
production live so it's a particular

00:02:57,790 --> 00:03:02,519
approach to building a training of ANS

00:03:00,700 --> 00:03:04,810
and what you can do is think of this as

00:03:02,519 --> 00:03:07,410
decision-making black boxes that's

00:03:04,810 --> 00:03:10,150
really as heart all that the guy DL is

00:03:07,410 --> 00:03:11,709
it's driving major advances right now in

00:03:10,150 --> 00:03:15,700
a number of areas including data

00:03:11,709 --> 00:03:17,830
analysis and visualization NLP sentiment

00:03:15,700 --> 00:03:22,450
analysis computer vision including even

00:03:17,830 --> 00:03:24,280
things like self-driving cars and it's

00:03:22,450 --> 00:03:26,380
just basically you can think of it as

00:03:24,280 --> 00:03:29,049
you throwing in some inputs just in a

00:03:26,380 --> 00:03:30,760
ray of input that are representing us

00:03:29,049 --> 00:03:34,950
something that could be you know for

00:03:30,760 --> 00:03:38,080
instance language words documents images

00:03:34,950 --> 00:03:41,350
just doing whatever operations on that

00:03:38,080 --> 00:03:44,290
and what you're looking for is that the

00:03:41,350 --> 00:03:47,500
deep learning actually is coming up with

00:03:44,290 --> 00:03:50,079
a prediction from the training data set

00:03:47,500 --> 00:03:51,760
that you give it about what kind of

00:03:50,079 --> 00:03:54,910
properties in the future could be used

00:03:51,760 --> 00:03:56,140
to draw intuitions about data you get it

00:03:54,910 --> 00:03:58,480
that similar ish

00:03:56,140 --> 00:03:59,590
training set as you can see that would

00:03:58,480 --> 00:04:01,000
leave to some interesting little

00:03:59,590 --> 00:04:02,620
dependencies there you have to add that

00:04:01,000 --> 00:04:04,540
tight coupling between the training data

00:04:02,620 --> 00:04:07,420
set and the one that you use in the

00:04:04,540 --> 00:04:08,680
future of much larger scale today we'll

00:04:07,420 --> 00:04:10,090
be looking at some use cases that

00:04:08,680 --> 00:04:12,550
include things like behavioral

00:04:10,090 --> 00:04:15,940
prediction image classification face

00:04:12,550 --> 00:04:17,440
recognition and sentiment analysis so

00:04:15,940 --> 00:04:19,450
let's take a step back for just a second

00:04:17,440 --> 00:04:22,419
and go back to that summary deep

00:04:19,450 --> 00:04:24,910
learning relies on an and ends automated

00:04:22,419 --> 00:04:27,010
discovery of patterns within the

00:04:24,910 --> 00:04:28,960
training data set it's not looking at

00:04:27,010 --> 00:04:31,150
labeled data it's not looking at

00:04:28,960 --> 00:04:33,760
classified data it's simply looking at

00:04:31,150 --> 00:04:35,950
jumble of data and then turning patterns

00:04:33,760 --> 00:04:38,110
that may exist within it and trying to

00:04:35,950 --> 00:04:40,090
assign something to that it applies

00:04:38,110 --> 00:04:42,550
those discoveries to draw intuitions

00:04:40,090 --> 00:04:44,530
about future inputs and that's kind of

00:04:42,550 --> 00:04:46,560
nice abstract to think about so I wanted

00:04:44,530 --> 00:04:50,050
to show you a neat little example here

00:04:46,560 --> 00:04:52,600
this is Mario it's an aann that teaches

00:04:50,050 --> 00:04:54,070
itself how to play super mario world it

00:04:52,600 --> 00:04:55,990
starts with absolutely no clue

00:04:54,070 --> 00:04:57,490
whatsoever it is not given instructions

00:04:55,990 --> 00:04:59,440
it's not given rules it's not given the

00:04:57,490 --> 00:05:02,080
concept of gameplay at all it knows

00:04:59,440 --> 00:05:04,210
nothing about its world all it does is

00:05:02,080 --> 00:05:07,120
manipulate numbers and observe what

00:05:04,210 --> 00:05:10,870
happens and it observes that sometimes

00:05:07,120 --> 00:05:12,580
things change and over a period of

00:05:10,870 --> 00:05:15,940
iterations of doing this and

00:05:12,580 --> 00:05:18,130
increasingly granular layers it starts

00:05:15,940 --> 00:05:19,990
to ferret out patterns and by the end of

00:05:18,130 --> 00:05:23,020
24 hours of doing this just

00:05:19,990 --> 00:05:26,970
investigative series of iterations it's

00:05:23,020 --> 00:05:29,590
learned how to play the game and win

00:05:26,970 --> 00:05:35,320
this is what we're dealing with right

00:05:29,590 --> 00:05:37,060
now is fascinating new breakthroughs so

00:05:35,320 --> 00:05:39,190
what does it tell us about the ability

00:05:37,060 --> 00:05:42,640
to predict some insights for other kinds

00:05:39,190 --> 00:05:45,789
of data well let's try playing a game

00:05:42,640 --> 00:05:47,680
and we'll find out it looks a little bit

00:05:45,789 --> 00:05:50,010
like bingo and it's called data mining

00:05:47,680 --> 00:05:50,010
fail

00:05:51,419 --> 00:05:56,080
insightful algorithms are full of

00:05:54,039 --> 00:05:58,629
pitfalls and by looking at case studies

00:05:56,080 --> 00:06:00,129
it's easier for us to explore some of

00:05:58,629 --> 00:06:04,300
the pitfalls that you can see here on

00:06:00,129 --> 00:06:09,039
the board and let's just try taking a

00:06:04,300 --> 00:06:10,810
peek of you first one is target so in

00:06:09,039 --> 00:06:13,830
the retail sector the second trimester

00:06:10,810 --> 00:06:18,669
of pregnancy is known as the Holy Grail

00:06:13,830 --> 00:06:22,509
why would that be in the second

00:06:18,669 --> 00:06:24,970
trimester of pregnancy a person's buying

00:06:22,509 --> 00:06:27,099
habits are for one of the very few times

00:06:24,970 --> 00:06:29,110
in their life completely up for grabs

00:06:27,099 --> 00:06:31,270
they will change their product loyalty

00:06:29,110 --> 00:06:34,780
their brand loyalty their store loyalty

00:06:31,270 --> 00:06:37,389
and start afresh which means that if a

00:06:34,780 --> 00:06:38,770
retailer can get that person to start

00:06:37,389 --> 00:06:41,469
buying at their store during that

00:06:38,770 --> 00:06:43,569
pivotal time then they are locking in a

00:06:41,469 --> 00:06:46,000
consumer for potentially the rest of

00:06:43,569 --> 00:06:50,229
their life as well as that families this

00:06:46,000 --> 00:06:51,940
is a really pivotal powerful moment so

00:06:50,229 --> 00:06:54,460
of course they're very interested in

00:06:51,940 --> 00:06:56,409
discovering who is in that second

00:06:54,460 --> 00:06:58,389
trimester of pregnancy it's a lot easier

00:06:56,409 --> 00:07:00,819
for merchandisers to identify that

00:06:58,389 --> 00:07:03,159
around the third trimester but second

00:07:00,819 --> 00:07:04,719
was hard target one day had some

00:07:03,159 --> 00:07:07,240
marketers go to one of the programmers

00:07:04,719 --> 00:07:08,680
and disaster you know hey do we have

00:07:07,240 --> 00:07:11,370
something to do we think is there some

00:07:08,680 --> 00:07:14,440
way we can use the data figure this out

00:07:11,370 --> 00:07:17,919
guy came up with an algorithm they

00:07:14,440 --> 00:07:20,650
started sending out mailers something

00:07:17,919 --> 00:07:22,300
like this full of things that you need

00:07:20,650 --> 00:07:23,949
when you are pregnant when you are

00:07:22,300 --> 00:07:28,090
expecting a baby you and you will soon

00:07:23,949 --> 00:07:29,740
have an infant in your home that seemed

00:07:28,090 --> 00:07:35,349
to go pretty well right up until the guy

00:07:29,740 --> 00:07:37,060
came into a store raged how dare you how

00:07:35,349 --> 00:07:40,349
can you send this to my teenage daughter

00:07:37,060 --> 00:07:42,030
are you trying to tell her to have sex

00:07:40,349 --> 00:07:44,910
store manager

00:07:42,030 --> 00:07:47,100
of course not like he created this

00:07:44,910 --> 00:07:49,590
probably had absolutely no idea why he

00:07:47,100 --> 00:07:52,950
was being yelled at but understandably

00:07:49,590 --> 00:07:56,040
apologized and then went away he came

00:07:52,950 --> 00:07:57,570
back the next day and he said I owe you

00:07:56,040 --> 00:07:59,580
an apology because I've had

00:07:57,570 --> 00:08:01,020
conversations my daughter and it turns

00:07:59,580 --> 00:08:05,070
out there's some things I didn't know

00:08:01,020 --> 00:08:11,730
she is in fact pregnant so the algorithm

00:08:05,070 --> 00:08:15,030
work right success was that girl ready

00:08:11,730 --> 00:08:19,680
to have a conversation that day was that

00:08:15,030 --> 00:08:21,360
her moment was it right that an

00:08:19,680 --> 00:08:24,030
algorithm forced her to have a

00:08:21,360 --> 00:08:27,330
conversation with an angry parent of all

00:08:24,030 --> 00:08:30,180
things this is an algorithm stealing

00:08:27,330 --> 00:08:31,980
something from people their right to say

00:08:30,180 --> 00:08:36,990
something that's the most important news

00:08:31,980 --> 00:08:38,670
of their life so target did learn a few

00:08:36,990 --> 00:08:40,740
things from this what they learned is to

00:08:38,670 --> 00:08:46,770
change the way they send out those ads

00:08:40,740 --> 00:08:48,570
instead it's something like this they

00:08:46,770 --> 00:08:51,150
buried them they send out a male are

00:08:48,570 --> 00:08:52,950
full of other random things as far

00:08:51,150 --> 00:08:54,990
removed from seem to be related as

00:08:52,950 --> 00:08:57,570
possible so you know for instance now

00:08:54,990 --> 00:08:59,670
you might get coupons for diapers and

00:08:57,570 --> 00:09:00,960
for men's cologne no chance that that's

00:08:59,670 --> 00:09:04,020
targeting you because you're pregnant

00:09:00,960 --> 00:09:06,660
right they found that they really liked

00:09:04,020 --> 00:09:08,880
this idea because as long as a pregnant

00:09:06,660 --> 00:09:14,270
woman thinks she hasn't been spied on as

00:09:08,880 --> 00:09:14,270
long as we don't spook her it works

00:09:17,860 --> 00:09:23,120
shutterfly also was very interested in

00:09:20,839 --> 00:09:27,259
this moment when people have babies

00:09:23,120 --> 00:09:29,329
right and spending power so they

00:09:27,259 --> 00:09:31,490
identified some people and send out some

00:09:29,329 --> 00:09:34,100
emails hey congratulations on your

00:09:31,490 --> 00:09:38,420
lovely new bundle of joy time to buy

00:09:34,100 --> 00:09:41,089
some cards from us the thing is they're

00:09:38,420 --> 00:09:43,519
targeting was a bit off they had some

00:09:41,089 --> 00:09:45,949
false positives in there some of them

00:09:43,519 --> 00:09:50,750
worthed people saying you know I'm a

00:09:45,949 --> 00:09:53,990
dude so haha some people had a little

00:09:50,750 --> 00:09:56,060
bit different feedback Thank You

00:09:53,990 --> 00:09:58,630
shutterfly for the congratulations on my

00:09:56,060 --> 00:10:04,399
new bundle of joy I'm horribly infertile

00:09:58,630 --> 00:10:06,290
but hey I'm adopting a kidney so I lost

00:10:04,399 --> 00:10:09,199
a baby into a member who would have been

00:10:06,290 --> 00:10:14,300
due this week it was like hitting a wall

00:10:09,199 --> 00:10:18,440
all over again there were many people

00:10:14,300 --> 00:10:21,019
would feedback like this shutterfly

00:10:18,440 --> 00:10:22,790
responded the intent of the email was to

00:10:21,019 --> 00:10:28,610
target customers who had recently had a

00:10:22,790 --> 00:10:31,550
baby yes we know that's not an apology

00:10:28,610 --> 00:10:35,240
that's not even really an explanation it

00:10:31,550 --> 00:10:38,690
is not learning something the point is

00:10:35,240 --> 00:10:44,040
that you're targeting failed and what is

00:10:38,690 --> 00:10:46,949
the cost of a false positive here

00:10:44,040 --> 00:10:48,540
a few months ago mark zuckerberg

00:10:46,949 --> 00:10:51,240
excitedly announced that he's going to

00:10:48,540 --> 00:10:53,759
be a father he also used that blog post

00:10:51,240 --> 00:10:55,709
to announce that he and his wife

00:10:53,759 --> 00:10:57,720
Priscilla had also had a series of

00:10:55,709 --> 00:11:00,029
miscarriages that preceded this and that

00:10:57,720 --> 00:11:02,519
they'd have to deal with as a couple he

00:11:00,029 --> 00:11:05,850
wrote you feel so hopeful when you learn

00:11:02,519 --> 00:11:08,130
you're going to have a child you start

00:11:05,850 --> 00:11:10,800
imagining who they'll become and

00:11:08,130 --> 00:11:14,089
dreaming of hopes for their future you

00:11:10,800 --> 00:11:17,750
start making plans and then they're gone

00:11:14,089 --> 00:11:17,750
it's a lonely experience

00:11:21,190 --> 00:11:25,870
Facebook year in review that's been

00:11:23,230 --> 00:11:27,970
around for quite a while every year they

00:11:25,870 --> 00:11:30,220
kind of tweak how they handle it the

00:11:27,970 --> 00:11:31,870
basic principle being at the end of the

00:11:30,220 --> 00:11:33,370
year wouldn't you like to revisit some

00:11:31,870 --> 00:11:36,790
of your posts from the past year that

00:11:33,370 --> 00:11:38,410
have been particularly special in some

00:11:36,790 --> 00:11:39,970
way maybe they had a bunch of legs maybe

00:11:38,410 --> 00:11:42,820
a lot more commenting than usual

00:11:39,970 --> 00:11:46,060
whatever it was here they are again for

00:11:42,820 --> 00:11:48,040
you to relive the past year last year

00:11:46,060 --> 00:11:51,400
they got particularly algorithmic about

00:11:48,040 --> 00:11:54,460
doing this selecting them for you what

00:11:51,400 --> 00:11:58,720
they fail to take into account is that

00:11:54,460 --> 00:12:01,000
lives change even if something start off

00:11:58,720 --> 00:12:02,970
being a joyous moment by the time the

00:12:01,000 --> 00:12:06,730
end of the year comes around our

00:12:02,970 --> 00:12:09,400
relationships change our jobs change how

00:12:06,730 --> 00:12:12,070
we feel about past things has changed

00:12:09,400 --> 00:12:14,680
you can't just say that based on some

00:12:12,070 --> 00:12:17,350
metrics of what happened to a post back

00:12:14,680 --> 00:12:19,650
when even if back one was yesterday that

00:12:17,350 --> 00:12:23,530
you know how people feel about it today

00:12:19,650 --> 00:12:25,090
so what happens when you're wrong Eric

00:12:23,530 --> 00:12:27,190
Meyer coined the term inadvertent

00:12:25,090 --> 00:12:29,290
algorithmic cruelty which he defines as

00:12:27,190 --> 00:12:30,970
result of code that works in the

00:12:29,290 --> 00:12:33,130
overwhelming majority of cases but

00:12:30,970 --> 00:12:38,560
doesn't take into account other use

00:12:33,130 --> 00:12:39,970
cases why does he get to name it because

00:12:38,560 --> 00:12:44,680
he's one of the people that had happened

00:12:39,970 --> 00:12:49,090
to this is a picture of my daughter who

00:12:44,680 --> 00:12:51,040
is dead who died this year the year in

00:12:49,090 --> 00:12:52,900
view add keeps coming up in my feed

00:12:51,040 --> 00:12:55,330
rotating through different fun and

00:12:52,900 --> 00:12:58,720
fabulous backgrounds as if celebrating

00:12:55,330 --> 00:13:01,590
her death and there is no obvious way to

00:12:58,720 --> 00:13:01,590
stop it

00:13:06,150 --> 00:13:11,400
Eric asks us to increase awareness of

00:13:09,240 --> 00:13:14,790
and consideration of the failure modes

00:13:11,400 --> 00:13:17,460
the edge cases the worst-case scenarios

00:13:14,790 --> 00:13:19,260
and that's obviously what I'm trying to

00:13:17,460 --> 00:13:21,990
do here today and I hope that you will

00:13:19,260 --> 00:13:23,790
bring forward when you leave here to

00:13:21,990 --> 00:13:25,440
others that's really the best message

00:13:23,790 --> 00:13:27,690
that we can take is to be thinking about

00:13:25,440 --> 00:13:29,790
these much harder with that in mind

00:13:27,690 --> 00:13:34,680
here's my first recommendation for us

00:13:29,790 --> 00:13:39,950
all be humble we cannot into it

00:13:34,680 --> 00:13:46,710
interstate emotions private subjectivity

00:13:39,950 --> 00:13:48,570
not yet anyway Fitbit you probably know

00:13:46,710 --> 00:13:50,280
well as a little device to help you do

00:13:48,570 --> 00:13:52,560
things like track how many steps you

00:13:50,280 --> 00:13:55,170
take how many miles you've traveled how

00:13:52,560 --> 00:13:58,050
many what evers you've done physically

00:13:55,170 --> 00:13:59,970
and when it first emerged it had among

00:13:58,050 --> 00:14:04,350
those things you could track your sex

00:13:59,970 --> 00:14:08,630
life there was a little glitch and as

00:14:04,350 --> 00:14:08,630
much as it was default public

00:14:13,120 --> 00:14:20,690
this is what happens when we treat data

00:14:16,580 --> 00:14:23,180
as essentially all neutral one algorithm

00:14:20,690 --> 00:14:26,750
applied to all data as if all data is

00:14:23,180 --> 00:14:29,240
the same all we're doing as a device is

00:14:26,750 --> 00:14:31,370
collecting data and helping people game

00:14:29,240 --> 00:14:34,310
afya and make it socially fun and

00:14:31,370 --> 00:14:37,310
competitive right didn't think through

00:14:34,310 --> 00:14:38,990
how data differs that sometimes we

00:14:37,310 --> 00:14:45,350
collect data for reasons other than

00:14:38,990 --> 00:14:47,630
public consumption so how did give users

00:14:45,350 --> 00:14:51,140
find out they found out from the

00:14:47,630 --> 00:14:59,510
internet that their information was as

00:14:51,140 --> 00:15:02,270
public as that so they fixed it they

00:14:59,510 --> 00:15:03,710
didn't address the sort of core problem

00:15:02,270 --> 00:15:05,930
there what they did instead is they no

00:15:03,710 --> 00:15:07,730
longer have sex tracker which is a shame

00:15:05,930 --> 00:15:09,860
because I'm also a certified sex

00:15:07,730 --> 00:15:12,880
educator and this is legitimately useful

00:15:09,860 --> 00:15:16,280
information for a variety of reasons so

00:15:12,880 --> 00:15:18,680
really just being unable or unwilling to

00:15:16,280 --> 00:15:20,150
deal with differences in beta is

00:15:18,680 --> 00:15:21,980
cheating users out of something that

00:15:20,150 --> 00:15:24,560
started off there and could have been

00:15:21,980 --> 00:15:28,360
valuable and no one's really willing to

00:15:24,560 --> 00:15:28,360
touch that third rail and that's a shame

00:15:28,810 --> 00:15:34,760
uber so of course most of us need some

00:15:33,050 --> 00:15:36,770
form of internal ops tools this is a

00:15:34,760 --> 00:15:38,800
given right it may be for monitoring

00:15:36,770 --> 00:15:42,050
performance tuning business metrics

00:15:38,800 --> 00:15:45,200
whatever it is uber us is known as God

00:15:42,050 --> 00:15:48,440
view and in early iterations it look

00:15:45,200 --> 00:15:50,870
like this just tracking the vehicles

00:15:48,440 --> 00:15:52,670
passengers people waiting having an

00:15:50,870 --> 00:15:55,220
overall picture of how things are out

00:15:52,670 --> 00:15:56,270
there on the roads but over didn't limit

00:15:55,220 --> 00:15:58,730
access

00:15:56,270 --> 00:16:01,340
admins or restricted to operational uses

00:15:58,730 --> 00:16:03,980
employees could really identify any

00:16:01,340 --> 00:16:07,190
passenger and monitor that person's

00:16:03,980 --> 00:16:09,560
movement in real time drivers used to

00:16:07,190 --> 00:16:12,530
have access to those God view records to

00:16:09,560 --> 00:16:14,450
to see things other than the transaction

00:16:12,530 --> 00:16:16,520
that they were involved in to see

00:16:14,450 --> 00:16:19,850
passengers other than those that they

00:16:16,520 --> 00:16:22,190
had personally taken somewhere even a

00:16:19,850 --> 00:16:25,510
job applicant was welcome to access

00:16:22,190 --> 00:16:27,830
these incredibly private records

00:16:25,510 --> 00:16:29,750
meanwhile even managers felt free to

00:16:27,830 --> 00:16:32,240
have used God view for non operational

00:16:29,750 --> 00:16:35,240
purposes such as stocking celebrities

00:16:32,240 --> 00:16:37,490
rides in real time and showing it off as

00:16:35,240 --> 00:16:43,850
party entertainment this is from their

00:16:37,490 --> 00:16:48,080
own Facebook posts abuse of algorithms

00:16:43,850 --> 00:16:50,630
is also imposing consequences we can

00:16:48,080 --> 00:16:53,240
have good tools written for good reasons

00:16:50,630 --> 00:16:57,530
coming up with good analysis and still

00:16:53,240 --> 00:16:59,780
being used in ways that are harmful the

00:16:57,530 --> 00:17:01,700
research group at dating site OkCupid

00:16:59,780 --> 00:17:02,990
used to have a popular blog about things

00:17:01,700 --> 00:17:05,510
that they were learning from aggregate

00:17:02,990 --> 00:17:07,190
data trends the blog would focus on

00:17:05,510 --> 00:17:09,650
sharing insights into just simple ways

00:17:07,190 --> 00:17:12,380
that as an OkCupid user you could use

00:17:09,650 --> 00:17:14,570
the dating site to date better overall

00:17:12,380 --> 00:17:17,890
so used to blog about their data some

00:17:14,570 --> 00:17:20,810
differences though crucial difference

00:17:17,890 --> 00:17:23,500
boobers approach is not about improving

00:17:20,810 --> 00:17:26,720
customers experience of their service

00:17:23,500 --> 00:17:30,140
look at this quote uber can and does

00:17:26,720 --> 00:17:32,600
track your one-night stands it's

00:17:30,140 --> 00:17:36,410
tracking whether you've been here areas

00:17:32,600 --> 00:17:38,780
populated by prostitutes this is

00:17:36,410 --> 00:17:41,930
invading people's privacy not for any

00:17:38,780 --> 00:17:46,370
operational purpose purely for the sake

00:17:41,930 --> 00:17:48,910
of judging and shaming them and think

00:17:46,370 --> 00:17:51,920
back to all that access now and

00:17:48,910 --> 00:17:53,660
recontextualize that all the people have

00:17:51,920 --> 00:17:58,520
access to that kind of personal

00:17:53,660 --> 00:17:59,960
formation google adwords there was an

00:17:58,520 --> 00:18:02,720
interesting study done by harvard a

00:17:59,960 --> 00:18:04,460
couple of years ago they took two names

00:18:02,720 --> 00:18:06,230
of first dates two sets of first names

00:18:04,460 --> 00:18:08,150
one highly correlated with

00:18:06,230 --> 00:18:10,970
african-americans and one highly

00:18:08,150 --> 00:18:13,190
correlated with white people and so an

00:18:10,970 --> 00:18:16,790
example name might say the latonya

00:18:13,190 --> 00:18:19,130
versus jump and what they did is they

00:18:16,790 --> 00:18:22,760
ran searches on some sites that had

00:18:19,130 --> 00:18:25,130
AdWords ads running and they matched

00:18:22,760 --> 00:18:28,340
those first names against actual people

00:18:25,130 --> 00:18:30,800
who are genuine professors who had that

00:18:28,340 --> 00:18:32,180
first name and some last name so that

00:18:30,800 --> 00:18:34,340
they were searching for real people and

00:18:32,180 --> 00:18:37,520
in searching for those real people were

00:18:34,340 --> 00:18:40,520
those highly racialized names they had

00:18:37,520 --> 00:18:42,500
some interesting findings a black

00:18:40,520 --> 00:18:44,450
identifying name was twenty five percent

00:18:42,500 --> 00:18:47,480
more likely to result in an ad that

00:18:44,450 --> 00:18:52,190
applied that real person had an arrest

00:18:47,480 --> 00:18:54,140
record examples like this it's important

00:18:52,190 --> 00:18:56,180
to remember what AdWords algorithm is

00:18:54,140 --> 00:18:58,550
for because it focuses purely on

00:18:56,180 --> 00:19:00,380
predicting what will click on that's it

00:18:58,550 --> 00:19:02,420
it doesn't care about the real world

00:19:00,380 --> 00:19:04,010
this isn't saying anything about the

00:19:02,420 --> 00:19:06,470
reality of whether anyone's being

00:19:04,010 --> 00:19:08,300
arrested at all although certainly as an

00:19:06,470 --> 00:19:11,390
end user you probably aren't thinking

00:19:08,300 --> 00:19:14,300
that its job purely is to make us click

00:19:11,390 --> 00:19:16,250
takes blank ad templates rose them out

00:19:14,300 --> 00:19:19,010
there randomly and then watches which

00:19:16,250 --> 00:19:20,990
ones people click and gets reinforcement

00:19:19,010 --> 00:19:24,140
that it uses to keep on sending those

00:19:20,990 --> 00:19:27,470
ones back out so what we see here is our

00:19:24,140 --> 00:19:31,490
collective bias being both reflected to

00:19:27,470 --> 00:19:34,940
us and reinforced by us data is

00:19:31,490 --> 00:19:38,360
generated by people it's never objective

00:19:34,940 --> 00:19:41,330
is constrained by our tunnel vision it's

00:19:38,360 --> 00:19:44,170
replicating our flaws it's echoing our

00:19:41,330 --> 00:19:44,170
preconceptions

00:19:44,910 --> 00:19:50,470
Google photos and Flickr this is where

00:19:49,150 --> 00:19:55,720
we had to find the image recognition

00:19:50,470 --> 00:19:57,520
stuff like this one we've been seeing

00:19:55,720 --> 00:19:59,020
facial recognition technology and

00:19:57,520 --> 00:20:00,460
consumer technology for a few years now

00:19:59,020 --> 00:20:03,040
and we've seen some pretty funny

00:20:00,460 --> 00:20:05,200
mistakes happen along the way such as

00:20:03,040 --> 00:20:07,410
early versions of iPhoto helpfully

00:20:05,200 --> 00:20:10,540
detecting you know facing your cookie

00:20:07,410 --> 00:20:11,950
why not so harmless mistake it's

00:20:10,540 --> 00:20:13,900
actually pretty funny it's a false

00:20:11,950 --> 00:20:14,920
positive but you know unlike some of the

00:20:13,900 --> 00:20:16,480
other ones we've seen it's a harmless

00:20:14,920 --> 00:20:19,720
false positive so it's really easy to

00:20:16,480 --> 00:20:22,960
chuckle at some mistakes are not funny

00:20:19,720 --> 00:20:25,720
though such as in this next photo flickr

00:20:22,960 --> 00:20:30,250
classified this just a few months ago as

00:20:25,720 --> 00:20:34,240
children's playground equipment this is

00:20:30,250 --> 00:20:36,760
the Dachau concentration camp the white

00:20:34,240 --> 00:20:39,690
tags you see at the right are flickers

00:20:36,760 --> 00:20:42,730
the gray ones are the photographer's own

00:20:39,690 --> 00:20:46,510
this is an algorithm that's not only

00:20:42,730 --> 00:20:48,250
wrong and totally fails at meaning but

00:20:46,510 --> 00:20:50,670
this is an algorithm treating human

00:20:48,250 --> 00:20:54,310
knowledge of meaning as your relevant

00:20:50,670 --> 00:20:56,800
it's substituting machine intuition as

00:20:54,310 --> 00:21:00,840
more important and valuable and treating

00:20:56,800 --> 00:21:00,840
data is if it's inherently neutral

00:21:01,410 --> 00:21:08,710
flickr tagged him not only as an animal

00:21:05,710 --> 00:21:12,700
but originally it also had a tag here as

00:21:08,710 --> 00:21:14,820
ape this i think as we all know in the

00:21:12,700 --> 00:21:19,260
u.s. is a particularly problematic

00:21:14,820 --> 00:21:22,000
comparison with a really ugly history

00:21:19,260 --> 00:21:25,930
here a month later is google photos

00:21:22,000 --> 00:21:27,430
making virtually the same mistake so how

00:21:25,930 --> 00:21:30,520
does this happen these are enormous

00:21:27,430 --> 00:21:33,700
companies that are dedicated to this

00:21:30,520 --> 00:21:35,620
kind of work right like this isn't an

00:21:33,700 --> 00:21:36,530
easy mistake and it's really unlikely

00:21:35,620 --> 00:21:38,690
that

00:21:36,530 --> 00:21:41,300
scale it there's one rogue employee

00:21:38,690 --> 00:21:43,490
who's inserting racism in the algorithm

00:21:41,300 --> 00:21:45,440
sure it's possible but what are some

00:21:43,490 --> 00:21:47,930
other reasons why this might happen well

00:21:45,440 --> 00:21:51,320
one answer you have to go all the way

00:21:47,930 --> 00:21:55,930
back to the 1950s to understand the

00:21:51,320 --> 00:21:58,550
1950s Kodak was creating color film and

00:21:55,930 --> 00:22:00,860
as part of that you needed to be able to

00:21:58,550 --> 00:22:01,850
have reliable color results and that

00:22:00,860 --> 00:22:04,760
means you have to make choices about

00:22:01,850 --> 00:22:07,280
what colors you want to render and in

00:22:04,760 --> 00:22:09,890
what level detail and Kodak favored

00:22:07,280 --> 00:22:14,810
rendering as much detail as possible in

00:22:09,890 --> 00:22:16,940
white skin specifically and so black

00:22:14,810 --> 00:22:18,350
technicians every day would start their

00:22:16,940 --> 00:22:20,810
day with what was called the Shirley

00:22:18,350 --> 00:22:23,120
card a photo of a woman with nice pale

00:22:20,810 --> 00:22:24,920
white skin and the idea was it every day

00:22:23,120 --> 00:22:27,320
you ran through sample photos and made

00:22:24,920 --> 00:22:29,690
sure that Shirley's skin features were

00:22:27,320 --> 00:22:33,560
all perfectly replicated and all the

00:22:29,690 --> 00:22:35,930
colors were well balanced making sure

00:22:33,560 --> 00:22:38,300
that black skin was well exposed was not

00:22:35,930 --> 00:22:41,360
on the agenda and in fact for decades

00:22:38,300 --> 00:22:43,970
black skin photos were always fairly

00:22:41,360 --> 00:22:46,460
poorly rendered it wasn't getting good

00:22:43,970 --> 00:22:49,040
detail it wasn't gathering the level of

00:22:46,460 --> 00:22:52,430
detail and data that we have photos of

00:22:49,040 --> 00:22:54,680
white skin people for generations now so

00:22:52,430 --> 00:22:56,390
what does that mean it means that in

00:22:54,680 --> 00:22:58,610
these training data sets remember the

00:22:56,390 --> 00:23:00,410
training data set has to be something

00:22:58,610 --> 00:23:03,800
like the live production when you're

00:23:00,410 --> 00:23:06,110
going to use we have terabytes of data

00:23:03,800 --> 00:23:09,380
being used for training data sets but

00:23:06,110 --> 00:23:11,840
it's made of data phone junk data that

00:23:09,380 --> 00:23:13,790
looks to us as though it has equal

00:23:11,840 --> 00:23:16,730
quality of photographs of the white and

00:23:13,790 --> 00:23:18,710
black skin and it does not even today

00:23:16,730 --> 00:23:20,480
with digital sensors it's not as though

00:23:18,710 --> 00:23:23,720
photography companies could have just

00:23:20,480 --> 00:23:25,580
reinvented sensors and had own photos

00:23:23,720 --> 00:23:27,950
suddenly reproduced images in completely

00:23:25,580 --> 00:23:30,350
diff ashen we would all been screaming

00:23:27,950 --> 00:23:32,090
that digital photography sucks look at

00:23:30,350 --> 00:23:35,810
how awful these photos are completely

00:23:32,090 --> 00:23:38,660
different than my really nice SLR we are

00:23:35,810 --> 00:23:42,390
living with the legacy of a decision an

00:23:38,660 --> 00:23:44,640
algorithm that is decades old generator

00:23:42,390 --> 00:23:47,550
Shinzo and it will continue to be a

00:23:44,640 --> 00:23:54,180
problem because we still have data that

00:23:47,550 --> 00:23:57,270
is junk a firm is a highly specialized

00:23:54,180 --> 00:23:59,970
consumer lending company it is using big

00:23:57,270 --> 00:24:03,930
data to determine creditworthiness for

00:23:59,970 --> 00:24:05,520
buying certain consumer goods it's basic

00:24:03,930 --> 00:24:07,710
principle here is that they gather just

00:24:05,520 --> 00:24:10,260
a few data points and from that are able

00:24:07,710 --> 00:24:13,110
to make a number of decisions so just

00:24:10,260 --> 00:24:15,600
basically your email address your mobile

00:24:13,110 --> 00:24:17,790
number of birthday social security

00:24:15,600 --> 00:24:21,860
number this is enough for them to then

00:24:17,790 --> 00:24:27,710
go deep diving into big data all over

00:24:21,860 --> 00:24:31,050
and when it does so it looks at a very

00:24:27,710 --> 00:24:33,420
basic 70,000 data points is what their

00:24:31,050 --> 00:24:35,820
algorithm looks like they're nice black

00:24:33,420 --> 00:24:37,020
box algorithm and if they find that

00:24:35,820 --> 00:24:38,340
that's not enough they'll ask you to

00:24:37,020 --> 00:24:40,140
voluntarily provide more information

00:24:38,340 --> 00:24:42,660
from sources for instance like github

00:24:40,140 --> 00:24:44,340
okay well here's a problem only two

00:24:42,660 --> 00:24:46,410
percent of open-source contributors are

00:24:44,340 --> 00:24:48,570
women so as soon as you start using a

00:24:46,410 --> 00:24:50,820
criteria like github participation

00:24:48,570 --> 00:24:52,950
already this algorithm is going to be

00:24:50,820 --> 00:24:55,170
creating bias that is totally

00:24:52,950 --> 00:24:57,450
unaccounted for here and how many of

00:24:55,170 --> 00:25:00,170
those other metrics likewise have biases

00:24:57,450 --> 00:25:00,170
unnoticed

00:25:01,490 --> 00:25:07,010
it's also it's people here ok so it's

00:25:05,390 --> 00:25:08,900
also looking behavioral factors such as

00:25:07,010 --> 00:25:10,640
how long someone takes to remember that

00:25:08,900 --> 00:25:13,309
kind of basic information or to read the

00:25:10,640 --> 00:25:14,570
Terms of Service and that seems like I

00:25:13,309 --> 00:25:16,160
might be pretty good you know are they

00:25:14,570 --> 00:25:19,210
like trying to reach forward or make up

00:25:16,160 --> 00:25:21,470
a date right this might be something

00:25:19,210 --> 00:25:23,480
unless say you're someone who has some

00:25:21,470 --> 00:25:25,010
sort of cognitive disorder Stephen

00:25:23,480 --> 00:25:28,940
Hawking clearly is a terrible credit

00:25:25,010 --> 00:25:34,179
risk so are many other people for

00:25:28,940 --> 00:25:38,679
instance parents this is algorithms

00:25:34,179 --> 00:25:42,350
replicating privilege finding it and

00:25:38,679 --> 00:25:44,150
replicating even more congratulations

00:25:42,350 --> 00:25:46,450
you're getting fall you're getting true

00:25:44,150 --> 00:25:49,910
positives but the rate of false

00:25:46,450 --> 00:25:51,800
negatives is so high and all those

00:25:49,910 --> 00:25:54,080
people being unnecessarily excluded who

00:25:51,800 --> 00:25:56,330
could be participants and potentially

00:25:54,080 --> 00:26:01,700
are really suffering from being excluded

00:25:56,330 --> 00:26:03,530
like this where is comprehension of

00:26:01,700 --> 00:26:05,660
meaning and context in this stuff

00:26:03,530 --> 00:26:08,660
without it bias is always going to run

00:26:05,660 --> 00:26:10,130
rampant a firm analyzes things like

00:26:08,660 --> 00:26:11,690
those social media accounts but they're

00:26:10,130 --> 00:26:14,059
not the only ones there's a number of

00:26:11,690 --> 00:26:16,220
others in 2012 Germany's biggest

00:26:14,059 --> 00:26:18,770
predator agent read left sorry I credit

00:26:16,220 --> 00:26:21,220
rated rating agency considered

00:26:18,770 --> 00:26:24,110
applicants Facebook relationships and

00:26:21,220 --> 00:26:26,240
Facebook itself has recently defended a

00:26:24,110 --> 00:26:28,670
patent that pushes this even further

00:26:26,240 --> 00:26:31,040
into making credit decisions about a

00:26:28,670 --> 00:26:35,720
person based on the unrelated credit

00:26:31,040 --> 00:26:37,940
history of your facebook friends this is

00:26:35,720 --> 00:26:39,500
nuts it's like even Facebook doesn't

00:26:37,940 --> 00:26:41,540
understand that facebook friends and

00:26:39,500 --> 00:26:44,950
real life friends are not necessarily

00:26:41,540 --> 00:26:51,510
the same friends but bigger than that

00:26:44,950 --> 00:26:54,320
you're being assigned credit or blame

00:26:51,510 --> 00:26:56,850
something that you have no control over

00:26:54,320 --> 00:26:59,400
here's an algorithm with potential to

00:26:56,850 --> 00:27:01,620
deeply intrude on an altar relationships

00:26:59,400 --> 00:27:03,900
just so people can make sure that their

00:27:01,620 --> 00:27:06,060
financial choices are isolated from

00:27:03,900 --> 00:27:09,800
friends they care about who don't have

00:27:06,060 --> 00:27:09,800
the same ability to make good choices

00:27:11,840 --> 00:27:16,350
this is the CEO of firm he says it's

00:27:14,880 --> 00:27:19,110
important to maintain the discipline of

00:27:16,350 --> 00:27:21,030
not trying to explain too much because

00:27:19,110 --> 00:27:29,490
adding some assumptions could introduce

00:27:21,030 --> 00:27:32,880
bias into the data analysis what it is

00:27:29,490 --> 00:27:36,180
not objective it always has bias its

00:27:32,880 --> 00:27:38,040
inherent at minimum from how the data is

00:27:36,180 --> 00:27:41,430
collected and interpreted in the first

00:27:38,040 --> 00:27:43,500
place every flaw every assumption in a

00:27:41,430 --> 00:27:45,980
data training set and the original

00:27:43,500 --> 00:27:48,480
functions they're all of course having

00:27:45,980 --> 00:27:51,510
unrecognized influence on algorithms and

00:27:48,480 --> 00:27:54,690
the outcomes they generate a firm says

00:27:51,510 --> 00:27:57,060
that algorithm assesses 70,000 personal

00:27:54,690 --> 00:27:59,070
qualities how many of those have

00:27:57,060 --> 00:28:02,910
potential for discriminatory outcomes

00:27:59,070 --> 00:28:05,150
how would anyone know it's not like they

00:28:02,910 --> 00:28:07,680
know they're proud that they don't know

00:28:05,150 --> 00:28:09,720
it's not something where you'll ever be

00:28:07,680 --> 00:28:12,270
able to say what was the criteria under

00:28:09,720 --> 00:28:13,830
which I was denied alone rationale for

00:28:12,270 --> 00:28:17,880
the algorithm can only be seen from

00:28:13,830 --> 00:28:22,160
inside the black box so I took a picture

00:28:17,880 --> 00:28:22,160
of the inside of the black box

00:28:22,950 --> 00:28:28,180
making lending decisions inside of a

00:28:25,240 --> 00:28:31,570
black box is not a radical new business

00:28:28,180 --> 00:28:36,970
model it's a regression it's disrupting

00:28:31,570 --> 00:28:41,410
fairness and oversight we're in an arms

00:28:36,970 --> 00:28:42,760
race right now Google Facebook Apple so

00:28:41,410 --> 00:28:44,950
many other companies they are all

00:28:42,760 --> 00:28:48,070
committing to big bets on deep learning

00:28:44,950 --> 00:28:50,560
and its opaque intuitions and for the

00:28:48,070 --> 00:28:52,090
moment quality obviously varies but

00:28:50,560 --> 00:28:54,040
remember the whole point is that deep

00:28:52,090 --> 00:28:57,640
learning is all about iteratively

00:28:54,040 --> 00:29:00,370
drawing intuitions at extremely fine

00:28:57,640 --> 00:29:02,170
grain levels and learning which means

00:29:00,370 --> 00:29:04,780
that it's getting more precise every day

00:29:02,170 --> 00:29:07,270
and its correctness and more damaging

00:29:04,780 --> 00:29:08,800
and its wrongness that's a dilemma for

00:29:07,270 --> 00:29:11,950
all of us it takes seriously as

00:29:08,800 --> 00:29:13,630
developers algorithms always have

00:29:11,950 --> 00:29:15,370
underlying assumptions about meaning

00:29:13,630 --> 00:29:17,350
about accuracy about the world in which

00:29:15,370 --> 00:29:19,150
they're generated about how code should

00:29:17,350 --> 00:29:21,070
assign meaning to them which means that

00:29:19,150 --> 00:29:23,560
underlying assumptions influence

00:29:21,070 --> 00:29:27,430
outcomes and consequences being

00:29:23,560 --> 00:29:28,930
generated we do care about getting the

00:29:27,430 --> 00:29:32,020
stuff right we want to be empathetic

00:29:28,930 --> 00:29:33,730
coders there's no question so the

00:29:32,020 --> 00:29:36,400
question becomes then how do we flip the

00:29:33,730 --> 00:29:38,140
paradigm well we can do some things like

00:29:36,400 --> 00:29:40,510
looking at what the professional

00:29:38,140 --> 00:29:42,430
ethicists say here are a few guidelines

00:29:40,510 --> 00:29:44,530
that I have adapted from the Association

00:29:42,430 --> 00:29:47,800
for Computing Machinery and from some

00:29:44,530 --> 00:29:49,510
various other sources first we have to

00:29:47,800 --> 00:29:51,010
consider decisions for temporal impact

00:29:49,510 --> 00:29:52,690
on others such as how might a false

00:29:51,010 --> 00:29:55,390
positive affects someone how might a

00:29:52,690 --> 00:29:57,370
false negative effects of one how might

00:29:55,390 --> 00:29:59,830
an algorithms intuition be seemingly

00:29:57,370 --> 00:30:05,020
correct and yet deeply wrong about human

00:29:59,830 --> 00:30:07,770
context project the likelihood of

00:30:05,020 --> 00:30:10,570
consequences while we're planning

00:30:07,770 --> 00:30:13,390
minimize negative consequences to others

00:30:10,570 --> 00:30:16,330
in early stages be thinking about these

00:30:13,390 --> 00:30:18,460
things have to be honest and be

00:30:16,330 --> 00:30:20,530
trustworthy and we do this of course

00:30:18,460 --> 00:30:23,130
because we simply want to be those kind

00:30:20,530 --> 00:30:26,170
human beings but we have to be overtly

00:30:23,130 --> 00:30:28,720
honest and trustworthy so the users know

00:30:26,170 --> 00:30:30,490
and have built belief in us that when we

00:30:28,720 --> 00:30:32,680
do eventually screw up which we will

00:30:30,490 --> 00:30:34,750
that they can trust this Wednesday it

00:30:32,680 --> 00:30:36,700
was an honest mistake we are sorry we

00:30:34,750 --> 00:30:39,160
are fixing it we will make sure that

00:30:36,700 --> 00:30:41,770
this does not happen again because if we

00:30:39,160 --> 00:30:46,390
have not done that work in advance we

00:30:41,770 --> 00:30:48,670
will have much bigger collapse we have

00:30:46,390 --> 00:30:50,950
to build in recourse that's the ability

00:30:48,670 --> 00:30:53,170
for people to be able to say you've made

00:30:50,950 --> 00:30:55,900
a mistake here's what you do to fix it

00:30:53,170 --> 00:30:58,480
and almost all these examples users have

00:30:55,900 --> 00:31:00,460
nothing they can do it's important to

00:30:58,480 --> 00:31:02,440
always build in recourse for someone to

00:31:00,460 --> 00:31:05,260
easily correct a conclusion that was

00:31:02,440 --> 00:31:07,090
wrong we had to provide others with full

00:31:05,260 --> 00:31:09,340
disclosure or limitations and call

00:31:07,090 --> 00:31:10,930
attention to signs of risk of harm to

00:31:09,340 --> 00:31:12,460
others and you'll notice they keep

00:31:10,930 --> 00:31:13,720
saying to others to other centers

00:31:12,460 --> 00:31:16,000
because we're good at taking care of

00:31:13,720 --> 00:31:17,500
ourselves and our company's part of this

00:31:16,000 --> 00:31:21,310
whole flipping paradigm has to be

00:31:17,500 --> 00:31:22,870
thinking about outside of that we have

00:31:21,310 --> 00:31:25,660
to be visionary about creating more ways

00:31:22,870 --> 00:31:29,260
to counteract bias bias and data

00:31:25,660 --> 00:31:31,930
analyses impacts across the board and

00:31:29,260 --> 00:31:34,900
finally we need to anticipate diverse

00:31:31,930 --> 00:31:36,910
ways to screw up because as long as

00:31:34,900 --> 00:31:39,370
teams are charged with defining data

00:31:36,910 --> 00:31:41,740
collection and use are anything less

00:31:39,370 --> 00:31:46,030
diverse than the intended user base we

00:31:41,740 --> 00:31:48,190
will keep failing them we have to have

00:31:46,030 --> 00:31:53,440
decision-making authority in the hands

00:31:48,190 --> 00:31:56,170
of highly diverse teams culture fit is

00:31:53,440 --> 00:31:58,950
the antithesis of diversity superficial

00:31:56,170 --> 00:32:01,960
variations are allowed to exist they're

00:31:58,950 --> 00:32:04,390
tolerated but their unique perspective

00:32:01,960 --> 00:32:06,700
is suppressed because the whole point of

00:32:04,390 --> 00:32:08,320
groupthink is to avoid disrupt our

00:32:06,700 --> 00:32:11,530
culture food is to disrupt avoid

00:32:08,320 --> 00:32:15,100
disrupting group thing yoona dimensional

00:32:11,530 --> 00:32:18,280
variety is also not diversity this is

00:32:15,100 --> 00:32:20,590
diversity it's wildly varying on as many

00:32:18,280 --> 00:32:21,080
dimensions as possible different origins

00:32:20,590 --> 00:32:23,900
different

00:32:21,080 --> 00:32:25,670
ages assumptions experiences where

00:32:23,900 --> 00:32:28,460
there's no longer an identifiable

00:32:25,670 --> 00:32:32,570
majority that's when we have diversity

00:32:28,460 --> 00:32:34,940
we are such a long way away from that we

00:32:32,570 --> 00:32:37,130
have to cultivate informed consent ask

00:32:34,940 --> 00:32:39,940
permission with the default being no

00:32:37,130 --> 00:32:42,490
focus on the many people who are eagerly

00:32:39,940 --> 00:32:44,360
wanting to share themselves are

00:32:42,490 --> 00:32:46,250
enthusiastic about giving consent

00:32:44,360 --> 00:32:47,420
between no more and serve better there

00:32:46,250 --> 00:32:51,530
are plenty of people who would have

00:32:47,420 --> 00:32:55,130
loved coupons like those asked why are

00:32:51,530 --> 00:32:57,050
we so reflexively going to let's invade

00:32:55,130 --> 00:33:01,280
privacy instead of asking people whether

00:32:57,050 --> 00:33:02,900
they would love to have it we also have

00:33:01,280 --> 00:33:04,850
the audit outcomes constantly and what

00:33:02,900 --> 00:33:06,710
that means is this is stuff that's

00:33:04,850 --> 00:33:09,530
widely used to look for for instance

00:33:06,710 --> 00:33:10,880
housing discrimination and employment

00:33:09,530 --> 00:33:12,920
discrimination essentially put in two

00:33:10,880 --> 00:33:15,770
sets of inputs that are identical on all

00:33:12,920 --> 00:33:18,350
but one axis so for example two sets of

00:33:15,770 --> 00:33:21,620
resumes exactly identical except for the

00:33:18,350 --> 00:33:23,900
name and life at harvard study put them

00:33:21,620 --> 00:33:26,060
in and you should have identical results

00:33:23,900 --> 00:33:29,030
if the system is not biased on the basis

00:33:26,060 --> 00:33:30,830
of say your name then those two should

00:33:29,030 --> 00:33:33,200
have exactly identical the result result

00:33:30,830 --> 00:33:35,990
if they don't then the algorithm is

00:33:33,200 --> 00:33:39,140
failing the algorithm has to immediately

00:33:35,990 --> 00:33:41,920
be treated as suspect and corrected that

00:33:39,140 --> 00:33:45,050
kind of auditing what's happen regularly

00:33:41,920 --> 00:33:47,750
because again all we've got it's a

00:33:45,050 --> 00:33:50,260
photograph of an inside of a big black

00:33:47,750 --> 00:33:50,260
box

00:33:51,080 --> 00:33:54,740
and that's why we have to also be

00:33:52,909 --> 00:33:59,179
willing to commit to data transparency

00:33:54,740 --> 00:34:00,710
and algorithmic transparency both and I

00:33:59,179 --> 00:34:03,230
know this is the hardest part of the

00:34:00,710 --> 00:34:04,669
conversation to have internally too many

00:34:03,230 --> 00:34:07,250
companies are obsessed with the idea

00:34:04,669 --> 00:34:08,960
that proprietary data at proprietary

00:34:07,250 --> 00:34:11,570
algorithms are the real special sauce

00:34:08,960 --> 00:34:14,419
with our companies right that's how we

00:34:11,570 --> 00:34:16,220
win it wasn't that long ago that we had

00:34:14,419 --> 00:34:18,560
to have similar fights about open source

00:34:16,220 --> 00:34:20,810
software we need these in our toolkit

00:34:18,560 --> 00:34:24,560
the proprietary tools are not in fact

00:34:20,810 --> 00:34:28,909
better for what we do would push back we

00:34:24,560 --> 00:34:31,220
were right we won we're professionals we

00:34:28,909 --> 00:34:33,200
know the transparency is crucial for

00:34:31,220 --> 00:34:36,740
drawing insights that are genuine and

00:34:33,200 --> 00:34:39,589
useful so it's worthwhile to start the

00:34:36,740 --> 00:34:40,790
conversation and be insistent we can

00:34:39,589 --> 00:34:43,429
argue for increasing transparency

00:34:40,790 --> 00:34:46,820
because it's for the sake of better

00:34:43,429 --> 00:34:50,659
product for cleaner features for fewer

00:34:46,820 --> 00:34:54,109
bugs stronger products overall happier

00:34:50,659 --> 00:34:56,480
loyal or users anyway says if your

00:34:54,109 --> 00:34:58,790
product has to do with something that

00:34:56,480 --> 00:35:03,589
deeply affects people either care or

00:34:58,790 --> 00:35:05,810
quit that's harsh words but when you

00:35:03,589 --> 00:35:09,650
look at it even something like uber as

00:35:05,810 --> 00:35:14,359
it turns out deeply affect people we are

00:35:09,650 --> 00:35:16,339
working on stuff it does it is so easy

00:35:14,359 --> 00:35:18,500
to unthinkingly build and out full of

00:35:16,339 --> 00:35:22,070
data mining fail building differently

00:35:18,500 --> 00:35:23,780
requires that awareness that critical

00:35:22,070 --> 00:35:26,180
thinking and most of all deciding

00:35:23,780 --> 00:35:29,540
together as a team to take a stance to

00:35:26,180 --> 00:35:30,940
say hey listen here's the deal we do not

00:35:29,540 --> 00:35:33,320
build here without understanding

00:35:30,940 --> 00:35:35,780
consequences to others this is just our

00:35:33,320 --> 00:35:39,950
process this is our way it's the right

00:35:35,780 --> 00:35:43,010
one we're hired for more than just a

00:35:39,950 --> 00:35:46,190
code we're not code monkeys were hired

00:35:43,010 --> 00:35:48,140
as professionals for our wisdom we are

00:35:46,190 --> 00:35:50,540
professionals who apply judgment and

00:35:48,140 --> 00:35:53,180
expertise about how to solve problems

00:35:50,540 --> 00:35:55,609
that is really what we do we are problem

00:35:53,180 --> 00:35:58,160
solvers who put problem-solving into

00:35:55,609 --> 00:36:00,710
code our role is to be opinionated about

00:35:58,160 --> 00:36:04,790
how to make code serve a problem space

00:36:00,710 --> 00:36:06,109
well this is our territory when we're

00:36:04,790 --> 00:36:08,540
asked to write code that presumes to

00:36:06,109 --> 00:36:10,190
intuit people's internal life and act on

00:36:08,540 --> 00:36:13,460
those assumptions as professionals we

00:36:10,190 --> 00:36:16,160
have to be people's proxies we have to

00:36:13,460 --> 00:36:18,230
be their advocates we have to say no on

00:36:16,160 --> 00:36:20,690
there we have to using their data in

00:36:18,230 --> 00:36:23,300
ways that they have not enthusiastically

00:36:20,690 --> 00:36:26,270
and knowing we can set you too we have

00:36:23,300 --> 00:36:28,460
to be the ones to say no uncritically

00:36:26,270 --> 00:36:32,900
reproducing systems that will buy it to

00:36:28,460 --> 00:36:35,960
begin with say no to writing code that

00:36:32,900 --> 00:36:41,810
imposes unauthorized consequences into

00:36:35,960 --> 00:36:46,300
their lives sure we have to refuse to

00:36:41,810 --> 00:36:46,300

YouTube URL: https://www.youtube.com/watch?v=1H9NrgyUw3Q


