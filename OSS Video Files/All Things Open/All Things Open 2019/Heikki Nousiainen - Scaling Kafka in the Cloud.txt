Title: Heikki Nousiainen - Scaling Kafka in the Cloud
Publication date: 2019-11-04
Playlist: All Things Open 2019
Description: 
	Apache Kafka is the backbone of modern data architecture, a streaming platform that enables real-time event driven data processing and can act as a solid integration point for microservices.

In this presentation, we share our experiences and approaches for operating, monitoring and maintaining hundreds of managed Apache Kafka clusters on behalf of Aiven customers. We touch on our architecture, tooling and automation used to maintain high uptimes, actively manage partition placement, perform live scaling and migrations as well as perform roll-forward updates for software patches and major version upgrades.
Captions: 
	00:00:03,430 --> 00:00:10,769
[Music]

00:00:07,010 --> 00:00:14,750
hello everyone and welcome and thanks

00:00:10,769 --> 00:00:16,139
for having me last talk on this track so

00:00:14,750 --> 00:00:19,080
appreciate it

00:00:16,139 --> 00:00:23,340
euro here today I'm gonna talk a little

00:00:19,080 --> 00:00:28,529
bit about how we run a posh kafka in in

00:00:23,340 --> 00:00:30,179
cloud and what are some of our kind of

00:00:28,529 --> 00:00:31,380
technical difficulties technical

00:00:30,179 --> 00:00:34,610
constraints that we have to overcome

00:00:31,380 --> 00:00:38,300
than technical solutions that we've used

00:00:34,610 --> 00:00:40,710
well that's a start

00:00:38,300 --> 00:00:42,930
quickly on myself my name is hey keynote

00:00:40,710 --> 00:00:48,149
Shannon I'm CTO and one of the founder

00:00:42,930 --> 00:00:50,070
of Ivan and I'm also do you run a local

00:00:48,149 --> 00:00:54,570
Kafka meetup group in Helsinki Finland

00:00:50,070 --> 00:00:56,730
my hometown I've been or the first

00:00:54,570 --> 00:00:59,699
introduction to a posh Kafka came in

00:00:56,730 --> 00:01:01,859
2014 where we started to build a data

00:00:59,699 --> 00:01:06,479
analytics pipeline to kind of capture

00:01:01,859 --> 00:01:07,950
all or create a pipeline to intake all

00:01:06,479 --> 00:01:09,479
information that we could gather from

00:01:07,950 --> 00:01:12,390
client performance and then only later

00:01:09,479 --> 00:01:15,990
figure out how we could utilize to to a

00:01:12,390 --> 00:01:17,790
greater greater customer experience and

00:01:15,990 --> 00:01:19,020
I really fell in love with the Kafka

00:01:17,790 --> 00:01:21,570
model at that point that we don't

00:01:19,020 --> 00:01:23,189
necessarily need to define first hand on

00:01:21,570 --> 00:01:26,549
how we're gonna utilize today that what

00:01:23,189 --> 00:01:28,920
how how its processed but after that is

00:01:26,549 --> 00:01:30,689
really left left to the clients I think

00:01:28,920 --> 00:01:33,840
that's that's something that a Bosch

00:01:30,689 --> 00:01:36,900
Kafka really gets right quickly about my

00:01:33,840 --> 00:01:39,180
company we are a managed service

00:01:36,900 --> 00:01:41,040
provider for open source database and

00:01:39,180 --> 00:01:43,110
data management technologies we run

00:01:41,040 --> 00:01:46,350
seven engines all together a peshkov

00:01:43,110 --> 00:01:49,530
copying one Postgres MySQL elasticsearch

00:01:46,350 --> 00:01:51,960
Cassandra Redis influx DB and we operate

00:01:49,530 --> 00:01:54,750
on six different public clouds over 80

00:01:51,960 --> 00:02:03,420
regions overall we've been offering a

00:01:54,750 --> 00:02:05,700
posh Kafka service since 2016 and kind

00:02:03,420 --> 00:02:08,399
of our overall scale at this point is

00:02:05,700 --> 00:02:11,430
that we we have 500 Kafka clusters under

00:02:08,399 --> 00:02:13,370
our management from ranging from brokers

00:02:11,430 --> 00:02:15,739
or three broker

00:02:13,370 --> 00:02:18,099
small clusters - then all the way to 30

00:02:15,739 --> 00:02:21,170
broker clusters under our management

00:02:18,099 --> 00:02:22,760
overall Ivan has some 10,000 services

00:02:21,170 --> 00:02:25,640
under our management so that's a fairly

00:02:22,760 --> 00:02:26,209
fairly sizable chunk and that's the kind

00:02:25,640 --> 00:02:28,099
of DevOps

00:02:26,209 --> 00:02:30,349
or a reason why we're doing things with

00:02:28,099 --> 00:02:34,459
a DevOps way and automation we'll go

00:02:30,349 --> 00:02:36,769
through in a bit our selves we rely

00:02:34,459 --> 00:02:39,769
Kafka's a critical component for all of

00:02:36,769 --> 00:02:41,329
our operations so it's a central message

00:02:39,769 --> 00:02:42,799
bus between all of these virtual

00:02:41,329 --> 00:02:45,920
machines and all of our infrastructure

00:02:42,799 --> 00:02:48,859
to deliver configuration and state

00:02:45,920 --> 00:02:53,299
updates between that fleet we use Kafka

00:02:48,859 --> 00:02:55,879
to centrally collect logs and statistics

00:02:53,299 --> 00:02:58,010
telemetry so that we can we can then

00:02:55,879 --> 00:03:00,940
have access to them no matter what

00:02:58,010 --> 00:03:03,379
happens the actual underlying VMs

00:03:00,940 --> 00:03:05,930
similarly starts then we can create

00:03:03,379 --> 00:03:07,879
automation alerts or in central place

00:03:05,930 --> 00:03:11,569
regardless of the locations of the

00:03:07,879 --> 00:03:13,489
instances themselves for us the benefits

00:03:11,569 --> 00:03:16,250
of using Kafka is this decoupled

00:03:13,489 --> 00:03:18,319
architecture so with that fleet and with

00:03:16,250 --> 00:03:20,660
that the range of regions that we run in

00:03:18,319 --> 00:03:23,090
networks are always broken somewhere and

00:03:20,660 --> 00:03:25,940
we have some network splits and whatnot

00:03:23,090 --> 00:03:27,980
and using Kafka here allows us to

00:03:25,940 --> 00:03:30,620
decouple these services so we don't have

00:03:27,980 --> 00:03:32,239
any interdependence between the virtual

00:03:30,620 --> 00:03:33,310
machines that implement the services or

00:03:32,239 --> 00:03:35,720
our management play

00:03:33,310 --> 00:03:38,150
similarly we we usually can avoid

00:03:35,720 --> 00:03:40,310
locking so we can push data logs and

00:03:38,150 --> 00:03:41,989
Static sticks and telemetry to Kafka

00:03:40,310 --> 00:03:45,220
even if we have something downstream

00:03:41,989 --> 00:03:48,139
that doesn't kind of keep up with a load

00:03:45,220 --> 00:03:51,260
Kafka has proven really good at scaling

00:03:48,139 --> 00:03:53,750
just as it promises so we are seeing

00:03:51,260 --> 00:03:56,269
that it scales nearly near to the

00:03:53,750 --> 00:03:57,949
tradition of brokers so arkad our

00:03:56,269 --> 00:04:00,799
architecture is essentially the same as

00:03:57,949 --> 00:04:02,900
we started in 2016 just the growth we

00:04:00,799 --> 00:04:06,019
are increasing our sizes of our Kafka

00:04:02,900 --> 00:04:08,780
clusters and similarly we can adjust a

00:04:06,019 --> 00:04:10,910
number of consumer side processors and

00:04:08,780 --> 00:04:13,310
that scale is quite nicely so so

00:04:10,910 --> 00:04:17,870
definitely we've witnessed the promise

00:04:13,310 --> 00:04:20,150
of Kafka to be to be true additional

00:04:17,870 --> 00:04:21,229
benefits for us is that now when we

00:04:20,150 --> 00:04:24,020
started to use kavkaza

00:04:21,229 --> 00:04:24,860
interface we realized that we can use

00:04:24,020 --> 00:04:26,719
the same

00:04:24,860 --> 00:04:29,569
interface for many different kinds of

00:04:26,719 --> 00:04:31,849
signaling so having just one set up way

00:04:29,569 --> 00:04:34,520
of communicating across our fleet has

00:04:31,849 --> 00:04:36,259
really increased the different use cases

00:04:34,520 --> 00:04:39,710
that we can implement on top of it

00:04:36,259 --> 00:04:42,889
and vice versa by submitting some data

00:04:39,710 --> 00:04:45,590
we can then reuse the same data from the

00:04:42,889 --> 00:04:48,229
pipeline in multiple different use cases

00:04:45,590 --> 00:04:50,360
some alerting some analytics may be

00:04:48,229 --> 00:04:53,060
doing log storage may be doing log

00:04:50,360 --> 00:04:56,780
analytics or capturing audit events out

00:04:53,060 --> 00:05:00,580
of the logs or the interesting detection

00:04:56,780 --> 00:05:03,650
systems that kind of habits cough costs

00:05:00,580 --> 00:05:07,099
security model support for ACLs allows

00:05:03,650 --> 00:05:09,379
us then to even if we have these big

00:05:07,099 --> 00:05:11,810
clusters we can cut logically separate

00:05:09,379 --> 00:05:15,469
the rights of consumers and producers so

00:05:11,810 --> 00:05:19,699
that we can control access quite tightly

00:05:15,469 --> 00:05:23,680
on and kind of say right audit events

00:05:19,699 --> 00:05:23,680
but never be allowed to read the same

00:05:24,279 --> 00:05:28,370
just gonna give a brief introduction to

00:05:26,750 --> 00:05:31,449
our architecture on our principles

00:05:28,370 --> 00:05:35,870
before moving the actual technical bits

00:05:31,449 --> 00:05:38,360
we look at all of our operations as code

00:05:35,870 --> 00:05:40,430
so no matter whether it's it's the code

00:05:38,360 --> 00:05:43,190
itself or configuration or deployment

00:05:40,430 --> 00:05:44,779
tooling or operator tooling all of that

00:05:43,190 --> 00:05:47,900
is part of what we handle we are

00:05:44,779 --> 00:05:51,139
software development pros processes so

00:05:47,900 --> 00:05:54,680
that's code reviews that's just testing

00:05:51,139 --> 00:05:56,270
everything that goes with it with our

00:05:54,680 --> 00:05:59,659
fleet we wanted from the beginning

00:05:56,270 --> 00:06:01,400
automate everything so full lifecycle of

00:05:59,659 --> 00:06:03,050
running base kafka clusters and for

00:06:01,400 --> 00:06:05,629
every service that ivan provides

00:06:03,050 --> 00:06:08,810
essentially doing deployment doing

00:06:05,629 --> 00:06:10,639
operations doing responses default by

00:06:08,810 --> 00:06:12,229
automated means so that we would not

00:06:10,639 --> 00:06:15,050
have to touch any of any of those

00:06:12,229 --> 00:06:19,879
clusters enough you know kind of during

00:06:15,050 --> 00:06:21,620
their lifetime in normal conditions one

00:06:19,879 --> 00:06:23,960
principle for us is this converging

00:06:21,620 --> 00:06:26,509
operation we realize that there are lots

00:06:23,960 --> 00:06:29,689
of faults and a lot of messages might

00:06:26,509 --> 00:06:31,610
get lost so our setup is created in such

00:06:29,689 --> 00:06:34,849
a way that we have a desired declared

00:06:31,610 --> 00:06:36,709
state and then any deviations the system

00:06:34,849 --> 00:06:38,520
will try to average or correct and

00:06:36,709 --> 00:06:43,770
eventually we'll reach

00:06:38,520 --> 00:06:45,629
desert desired state one principle for

00:06:43,770 --> 00:06:47,159
us was also immutable infrastructure

00:06:45,629 --> 00:06:49,110
meaning that we didn't want to do any

00:06:47,159 --> 00:06:50,879
in-place upgrades rather we wanted to

00:06:49,110 --> 00:06:53,370
always know exact state of each

00:06:50,879 --> 00:06:55,919
component in the system so if we do any

00:06:53,370 --> 00:06:57,780
software updates we rather do a full

00:06:55,919 --> 00:07:01,699
replacement of a virtual machine rather

00:06:57,780 --> 00:07:01,699
than update the virtual machine in place

00:07:01,819 --> 00:07:08,940
um just brief over you so we deploy

00:07:05,849 --> 00:07:11,639
kafka clusters in a dedicated virtual

00:07:08,940 --> 00:07:13,620
machines and then we have a shared

00:07:11,639 --> 00:07:18,180
management plane that takes care of some

00:07:13,620 --> 00:07:20,789
actions the virtual machines themselves

00:07:18,180 --> 00:07:23,220
the kafka brokers they have an agent

00:07:20,789 --> 00:07:26,039
that realizes to conquer service meaning

00:07:23,220 --> 00:07:29,580
they do all configuration of Kafka or

00:07:26,039 --> 00:07:34,919
zookeeper as well also respond to local

00:07:29,580 --> 00:07:37,590
events there we see in a bit what kind

00:07:34,919 --> 00:07:40,740
of actions are taken and in the

00:07:37,590 --> 00:07:42,690
management plane kind of is responsible

00:07:40,740 --> 00:07:45,330
provisioning resources in the cluster

00:07:42,690 --> 00:07:47,669
joining those resources into the cluster

00:07:45,330 --> 00:07:50,039
detect if there are health problems if

00:07:47,669 --> 00:07:52,889
if one of those virtual machines or

00:07:50,039 --> 00:07:55,319
brokers goes away or if they are invalid

00:07:52,889 --> 00:07:57,060
spec so we can handle the similarly

00:07:55,319 --> 00:07:59,639
software updates mandatory software

00:07:57,060 --> 00:08:02,819
updates or if we want to do scale ups in

00:07:59,639 --> 00:08:05,550
the resources of the single broker or

00:08:02,819 --> 00:08:07,889
adjust number of brokers in the cluster

00:08:05,550 --> 00:08:10,740
all this then will be acted on by the

00:08:07,889 --> 00:08:13,289
management plane and all of our

00:08:10,740 --> 00:08:15,210
operations use or distribute those

00:08:13,289 --> 00:08:19,319
virtual machines across availability

00:08:15,210 --> 00:08:25,440
zones for maximum availability and kind

00:08:19,319 --> 00:08:28,319
of thoughtful Torre's so that sets up

00:08:25,440 --> 00:08:30,419
with us up with some unique challenges

00:08:28,319 --> 00:08:32,669
and constraints and the rest of the

00:08:30,419 --> 00:08:35,010
presentation somewhat solutions that we

00:08:32,669 --> 00:08:36,810
set up so taking the context in this one

00:08:35,010 --> 00:08:38,700
they might not be the best practices for

00:08:36,810 --> 00:08:41,099
running Kafka in general but these are

00:08:38,700 --> 00:08:43,529
the problems that we've faced and we we

00:08:41,099 --> 00:08:46,829
think there are interesting to share so

00:08:43,529 --> 00:08:49,440
to start off we run across a variety of

00:08:46,829 --> 00:08:52,600
different cloud providers and their

00:08:49,440 --> 00:08:55,389
support for different kind of services

00:08:52,600 --> 00:08:57,550
vary quite a bit some have volumes some

00:08:55,389 --> 00:09:01,089
dots some have very different networking

00:08:57,550 --> 00:09:02,860
capabilities than others we rely on that

00:09:01,089 --> 00:09:06,790
immutable infrastructure to be able to

00:09:02,860 --> 00:09:09,730
maintain always or know the exact state

00:09:06,790 --> 00:09:12,509
of everything in our inventory and

00:09:09,730 --> 00:09:15,430
that's why we do those rollover upgrades

00:09:12,509 --> 00:09:19,000
for everything every piece of software

00:09:15,430 --> 00:09:21,430
update that we do and that comes with

00:09:19,000 --> 00:09:23,110
and every security software patch or

00:09:21,430 --> 00:09:24,730
anything means that we need to roll

00:09:23,110 --> 00:09:28,060
forward kafka clusters kind of on a

00:09:24,730 --> 00:09:30,519
continuous base we want to offer our

00:09:28,060 --> 00:09:34,589
customers the ability to change its spec

00:09:30,519 --> 00:09:36,970
of the clusters so in that means either

00:09:34,589 --> 00:09:38,380
sizing of the instance is sizing of the

00:09:36,970 --> 00:09:41,800
brokers themselves or number of the

00:09:38,380 --> 00:09:43,120
brokers or even so migrate between

00:09:41,800 --> 00:09:44,529
different clouds or different or

00:09:43,120 --> 00:09:47,110
different regions or different clouds

00:09:44,529 --> 00:09:49,720
altogether just to be able to have that

00:09:47,110 --> 00:09:54,069
flexibility and kind of being able to

00:09:49,720 --> 00:09:56,050
adjust as a load grows or or changes and

00:09:54,069 --> 00:09:58,959
at the same time we definitely need

00:09:56,050 --> 00:10:00,670
maintain high availability and we have

00:09:58,959 --> 00:10:03,240
isolated guarantees that we offer to our

00:10:00,670 --> 00:10:05,740
customers so this in all gives us a

00:10:03,240 --> 00:10:09,939
environment where we academ are on a

00:10:05,740 --> 00:10:11,350
constant flux our cluster are not stable

00:10:09,939 --> 00:10:13,720
but rather they keep rolling and

00:10:11,350 --> 00:10:16,389
modifying at all time means that all the

00:10:13,720 --> 00:10:18,220
IP address is changed and with with our

00:10:16,389 --> 00:10:20,529
cloud mix we cannot really rely on

00:10:18,220 --> 00:10:21,939
persistent volumes either so we use an

00:10:20,529 --> 00:10:24,630
ephemeral disks for all of our

00:10:21,939 --> 00:10:27,519
operations so that puts us in a you know

00:10:24,630 --> 00:10:29,759
maybe a little bit unique position so

00:10:27,519 --> 00:10:32,649
but problems are nevertheless

00:10:29,759 --> 00:10:36,279
interesting and hopefully our solutions

00:10:32,649 --> 00:10:39,310
to them are interesting as well so with

00:10:36,279 --> 00:10:43,120
that I'll start with networking and with

00:10:39,310 --> 00:10:45,850
this different clouds Excel for example

00:10:43,120 --> 00:10:48,569
the private and public networks they

00:10:45,850 --> 00:10:48,569

YouTube URL: https://www.youtube.com/watch?v=M6Ry7nrEuQI


