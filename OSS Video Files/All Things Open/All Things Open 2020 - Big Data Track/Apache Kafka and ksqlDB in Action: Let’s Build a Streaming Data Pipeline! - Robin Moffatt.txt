Title: Apache Kafka and ksqlDB in Action: Let’s Build a Streaming Data Pipeline! - Robin Moffatt
Publication date: 2020-12-11
Playlist: All Things Open 2020 - Big Data Track
Description: 
	Presented by: Robin Moffatt, Confluent
Presented at All Things Open 2020 - Big Data Track

Abstract: Have you ever thought that you needed to be a programmer to do stream processing and build streaming data pipelines? Think again!

Apache Kafka is a distributed, scalable, and fault-tolerant streaming platform, providing low-latency pub-sub messaging coupled with native storage and stream processing capabilities. Integrating Kafka with RDBMS, NoSQL, and object stores is simple with Kafka Connect, which is part of Apache Kafka. ksqlDB is the source-available SQL streaming engine for Apache Kafka, and makes it possible to build stream processing applications at scale, written using a familiar SQL interface.

In this talk, we’ll explain the architectural reasoning for Apache Kafka and the benefits of real-time integration, and we’ll build a streaming data pipeline using nothing but our bare hands, Kafka Connect, and ksqlDB.

Gasp as we filter events in real-time! Be amazed at how we can enrich streams of data with data from RDBMS! Be astonished at the power of streaming aggregates for anomaly detection!
Captions: 
	00:00:05,200 --> 00:00:07,839
good afternoon good morning good evening

00:00:06,560 --> 00:00:09,840
everyone wherever you are

00:00:07,839 --> 00:00:11,120
uh i'm over here in the uk so it's the

00:00:09,840 --> 00:00:13,440
early evening here

00:00:11,120 --> 00:00:14,639
um and my name is robin moffett i work

00:00:13,440 --> 00:00:17,039
at confluence

00:00:14,639 --> 00:00:18,000
as a developer advocate uh confluent if

00:00:17,039 --> 00:00:19,359
you've not heard of us

00:00:18,000 --> 00:00:21,760
we're one of the companies that

00:00:19,359 --> 00:00:22,800
contributes the open source apache kafka

00:00:21,760 --> 00:00:25,199
project

00:00:22,800 --> 00:00:27,119
and i'd like to spend the next 30 or 40

00:00:25,199 --> 00:00:29,519
minutes talking about building

00:00:27,119 --> 00:00:30,320
streaming data pipelines with apache

00:00:29,519 --> 00:00:33,760
kafka

00:00:30,320 --> 00:00:35,360
and its surrounding ecosystem so

00:00:33,760 --> 00:00:37,360
when we talk about streaming data

00:00:35,360 --> 00:00:39,920
pipelines there's different

00:00:37,360 --> 00:00:41,200
things entailed in that sometimes it's

00:00:39,920 --> 00:00:43,840
simply getting data

00:00:41,200 --> 00:00:44,559
from one place and streaming it over to

00:00:43,840 --> 00:00:46,320
another

00:00:44,559 --> 00:00:48,320
sometimes it's a bit more complex

00:00:46,320 --> 00:00:49,520
sometimes as the data passes through

00:00:48,320 --> 00:00:51,680
that pipeline

00:00:49,520 --> 00:00:53,520
we want to modify it and transform it to

00:00:51,680 --> 00:00:55,600
enrich it along the way

00:00:53,520 --> 00:00:57,120
so there are no slides you'll be glad or

00:00:55,600 --> 00:00:58,559
disappointed to know

00:00:57,120 --> 00:01:00,160
i'll put some online afterwards you can

00:00:58,559 --> 00:01:01,760
kind of go back to them and then talk

00:01:00,160 --> 00:01:02,320
about some of the components that i've

00:01:01,760 --> 00:01:04,159
shown

00:01:02,320 --> 00:01:05,360
but they this afternoon this evening

00:01:04,159 --> 00:01:07,360
wherever you are

00:01:05,360 --> 00:01:08,960
is all just hands-on showing you this

00:01:07,360 --> 00:01:11,040
thing and talking about it

00:01:08,960 --> 00:01:12,560
um i'll pause periodically to answer

00:01:11,040 --> 00:01:14,159
questions if we've got them

00:01:12,560 --> 00:01:16,240
and if not we'll have a good uh chat at

00:01:14,159 --> 00:01:17,840
the end an opportunity to

00:01:16,240 --> 00:01:20,000
if you do the twitters you can follow me

00:01:17,840 --> 00:01:20,880
i'm at i'm off you can always contact me

00:01:20,000 --> 00:01:24,000
on twitter there

00:01:20,880 --> 00:01:25,200
send me a message and i'll help you out

00:01:24,000 --> 00:01:28,080
more than happily

00:01:25,200 --> 00:01:28,720
so let's get started the demo i'm going

00:01:28,080 --> 00:01:31,200
to show you

00:01:28,720 --> 00:01:32,640
um it's available for you to go and try

00:01:31,200 --> 00:01:34,640
you can go along to github to this

00:01:32,640 --> 00:01:36,479
repository the demo scene repository

00:01:34,640 --> 00:01:37,840
and i'll share this link wherever i can

00:01:36,479 --> 00:01:39,439
share these links with you so

00:01:37,840 --> 00:01:41,200
on twitter and if there's somewhere on

00:01:39,439 --> 00:01:42,960
the the conference website also

00:01:41,200 --> 00:01:44,880
um but you basically go and clone the

00:01:42,960 --> 00:01:46,479
repository and you can actually get to

00:01:44,880 --> 00:01:48,000
the demo script itself that i'm

00:01:46,479 --> 00:01:49,360
going through now so you'll probably see

00:01:48,000 --> 00:01:49,920
my head popping up and down because i've

00:01:49,360 --> 00:01:51,280
got like

00:01:49,920 --> 00:01:52,960
different screens going on i've got the

00:01:51,280 --> 00:01:54,000
scripts on one of them but the great

00:01:52,960 --> 00:01:55,680
thing about this is that

00:01:54,000 --> 00:01:57,520
it means that i've practiced it and the

00:01:55,680 --> 00:01:58,159
scripts i know works but it means that

00:01:57,520 --> 00:01:59,680
you can

00:01:58,159 --> 00:02:01,600
take that script and you can go and try

00:01:59,680 --> 00:02:03,200
it out for yourself so

00:02:01,600 --> 00:02:05,040
what we're going to do is we're going to

00:02:03,200 --> 00:02:06,960
take a stream of data

00:02:05,040 --> 00:02:09,039
coming in from somewhere in our

00:02:06,960 --> 00:02:12,400
fictitious example we've got data

00:02:09,039 --> 00:02:13,840
streaming in from a website application

00:02:12,400 --> 00:02:16,160
where people can leave reviews on their

00:02:13,840 --> 00:02:17,040
website so the website's got a producer

00:02:16,160 --> 00:02:19,599
api and it's

00:02:17,040 --> 00:02:20,720
streaming messages into a kafka topic

00:02:19,599 --> 00:02:21,599
and the first thing we're going to do is

00:02:20,720 --> 00:02:22,560
we're simply going to have a look at

00:02:21,599 --> 00:02:25,040
those messages

00:02:22,560 --> 00:02:26,000
and then stream them straight on into

00:02:25,040 --> 00:02:27,599
elasticsearch

00:02:26,000 --> 00:02:29,840
so we can use a tool called kafka

00:02:27,599 --> 00:02:30,319
connect for this and kafka connects part

00:02:29,840 --> 00:02:33,360
of

00:02:30,319 --> 00:02:36,560
apache kafka whenever you see kafka

00:02:33,360 --> 00:02:37,280
and systems hooking into it then kafka

00:02:36,560 --> 00:02:39,519
connects

00:02:37,280 --> 00:02:40,319
is usually the tool you should be using

00:02:39,519 --> 00:02:41,920
for that

00:02:40,319 --> 00:02:44,080
so whether we're streaming data from

00:02:41,920 --> 00:02:46,480
kafka down to elasticsearch

00:02:44,080 --> 00:02:48,000
or indeed from a database into kafka

00:02:46,480 --> 00:02:50,480
which is what we'll do later on

00:02:48,000 --> 00:02:52,319
kafkac is that piece of the puzzle so

00:02:50,480 --> 00:02:53,280
apache kafka itself provides you the

00:02:52,319 --> 00:02:54,959
broker for

00:02:53,280 --> 00:02:57,040
storing and streaming and transforming

00:02:54,959 --> 00:03:00,400
that data kafka connect

00:02:57,040 --> 00:03:03,120
provides you that integration api

00:03:00,400 --> 00:03:04,319
so let's go and do that i'm using k sql

00:03:03,120 --> 00:03:06,159
db which is

00:03:04,319 --> 00:03:07,440
part of a confluent platform it's

00:03:06,159 --> 00:03:09,360
community licensed

00:03:07,440 --> 00:03:10,480
and you can do a bunch of things with it

00:03:09,360 --> 00:03:12,720
which i'm going to show you

00:03:10,480 --> 00:03:14,640
during the talk but to start with we can

00:03:12,720 --> 00:03:16,720
simply use it as an interface

00:03:14,640 --> 00:03:18,400
for our kafka broker and for kafka

00:03:16,720 --> 00:03:20,080
connect so i'm going to do this i'm

00:03:18,400 --> 00:03:22,879
going to say show me what topics

00:03:20,080 --> 00:03:24,239
i've got on my kafka brokers and it says

00:03:22,879 --> 00:03:26,720
you've got a couple of topics

00:03:24,239 --> 00:03:28,159
one of them is called ratings so this is

00:03:26,720 --> 00:03:30,879
the ratings topic to which

00:03:28,159 --> 00:03:32,879
ratings are being written continually so

00:03:30,879 --> 00:03:35,920
we can just run a consumer against that

00:03:32,879 --> 00:03:37,120
we can say print ratings

00:03:35,920 --> 00:03:39,120
and it's going to say well here are

00:03:37,120 --> 00:03:41,040
those messages as they arrive

00:03:39,120 --> 00:03:42,720
in that topic so if i pause the screen

00:03:41,040 --> 00:03:44,720
there and page up you can see

00:03:42,720 --> 00:03:46,080
casey called db has said well i reckon

00:03:44,720 --> 00:03:48,000
the value

00:03:46,080 --> 00:03:49,280
format of the messages the 30 being used

00:03:48,000 --> 00:03:51,440
for the value part of the message

00:03:49,280 --> 00:03:52,400
i reckon it's avro so i'll deserialize

00:03:51,440 --> 00:03:54,000
them as that's

00:03:52,400 --> 00:03:55,280
the key format it could be one of these

00:03:54,000 --> 00:03:56,239
different ones we reckon it's probably

00:03:55,280 --> 00:03:58,239
an integer

00:03:56,239 --> 00:04:00,480
we can see the key and the value for the

00:03:58,239 --> 00:04:02,159
messages you can see the timestamp okay

00:04:00,480 --> 00:04:03,760
so that's uttc so it's

00:04:02,159 --> 00:04:06,319
just gone half past seven at night over

00:04:03,760 --> 00:04:08,959
here in the uk but that's the timestamp

00:04:06,319 --> 00:04:10,400
of the message as they arrive so we can

00:04:08,959 --> 00:04:11,599
inspect those messages and we can see

00:04:10,400 --> 00:04:14,480
we've got a kafka topic

00:04:11,599 --> 00:04:15,680
with data arriving continually so the

00:04:14,480 --> 00:04:15,920
first thing we're actually going to do

00:04:15,680 --> 00:04:17,600
is

00:04:15,920 --> 00:04:21,040
we're going to take that data and we're

00:04:17,600 --> 00:04:22,960
going to stream it into elasticsearch

00:04:21,040 --> 00:04:25,360
so we're going to use kafka connect

00:04:22,960 --> 00:04:28,080
kafka connect has got a rest api

00:04:25,360 --> 00:04:29,919
so you can use the rest api directly you

00:04:28,080 --> 00:04:31,680
can use a web-based interface you can

00:04:29,919 --> 00:04:33,840
use confirm control center

00:04:31,680 --> 00:04:35,040
and you can also use k-sql db so

00:04:33,840 --> 00:04:37,440
k-sequel db

00:04:35,040 --> 00:04:38,080
wraps around kafka connect and we can

00:04:37,440 --> 00:04:40,240
say

00:04:38,080 --> 00:04:41,680
create a sync connector and give it a

00:04:40,240 --> 00:04:43,280
name and then we say which

00:04:41,680 --> 00:04:45,120
connector we want to use well we're

00:04:43,280 --> 00:04:46,479
writing data to elasticsearch so

00:04:45,120 --> 00:04:48,720
we'll use the elasticsearch sync

00:04:46,479 --> 00:04:50,080
connector where's our data it's in a

00:04:48,720 --> 00:04:52,000
topic called ratings

00:04:50,080 --> 00:04:54,400
where's our elasticsearch it's on this

00:04:52,000 --> 00:04:56,160
host called elasticsearch at this port

00:04:54,400 --> 00:04:57,759
and we say go and create that it says

00:04:56,160 --> 00:05:00,240
well i've created it we can say show

00:04:57,759 --> 00:05:01,520
connectors and say well is it working

00:05:00,240 --> 00:05:03,440
then it says we've got this one here the

00:05:01,520 --> 00:05:06,000
elasticsearch sync connector

00:05:03,440 --> 00:05:06,800
and it says it's running which means if

00:05:06,000 --> 00:05:09,600
we head over

00:05:06,800 --> 00:05:11,520
to elasticsearch we should see our data

00:05:09,600 --> 00:05:14,880
as it's arriving in our kafka topic

00:05:11,520 --> 00:05:18,080
arriving into our elasticsearch index

00:05:14,880 --> 00:05:19,199
so over here i've got kibana so kibana

00:05:18,080 --> 00:05:21,039
sits on top of

00:05:19,199 --> 00:05:22,479
elasticsearch and gives us a nice visual

00:05:21,039 --> 00:05:24,960
front-end for it

00:05:22,479 --> 00:05:26,479
so if i hit uh reload indices we can see

00:05:24,960 --> 00:05:28,080
here we've got one called ratings

00:05:26,479 --> 00:05:29,520
and we can see we've got some documents

00:05:28,080 --> 00:05:31,199
in it let's make that a bit bigger so

00:05:29,520 --> 00:05:35,360
you can see what's going on

00:05:31,199 --> 00:05:37,520
so 3257 documents 3262

00:05:35,360 --> 00:05:39,039
that number's going up because new data

00:05:37,520 --> 00:05:40,880
is arriving in the kafka topic

00:05:39,039 --> 00:05:43,440
so it's arriving in the elastic search

00:05:40,880 --> 00:05:44,800
index and if we just pause for a moment

00:05:43,440 --> 00:05:47,360
and think about what we've done

00:05:44,800 --> 00:05:47,840
we've run one piece of configuration to

00:05:47,360 --> 00:05:50,320
say

00:05:47,840 --> 00:05:50,960
take data from this topic and go and

00:05:50,320 --> 00:05:53,360
stream it

00:05:50,960 --> 00:05:54,800
over there and we can say well let's go

00:05:53,360 --> 00:05:57,600
and create an index pattern

00:05:54,800 --> 00:05:58,800
on top of that data so an index pattern

00:05:57,600 --> 00:06:00,160
that's kibana

00:05:58,800 --> 00:06:02,479
understand more about the fields that

00:06:00,160 --> 00:06:04,479
are available there and we tell it which

00:06:02,479 --> 00:06:07,520
is the time stamp field

00:06:04,479 --> 00:06:08,720
and once we've done that we can then go

00:06:07,520 --> 00:06:11,199
and have a look at the actual

00:06:08,720 --> 00:06:13,280
data as it flows in so we're going to

00:06:11,199 --> 00:06:15,440
click on this here

00:06:13,280 --> 00:06:17,600
and we can see that date is flowing in

00:06:15,440 --> 00:06:20,479
basically in real time if i set this to

00:06:17,600 --> 00:06:22,639
refresh every second

00:06:20,479 --> 00:06:24,400
and hit start on that you can see so

00:06:22,639 --> 00:06:27,600
it's 1936 which is the

00:06:24,400 --> 00:06:29,840
the time over here in uh uh bst

00:06:27,600 --> 00:06:30,639
utc plus one and that data's arriving

00:06:29,840 --> 00:06:32,960
continually

00:06:30,639 --> 00:06:34,479
so arriving in the kafka topic kafka

00:06:32,960 --> 00:06:37,520
connects streaming it over

00:06:34,479 --> 00:06:38,240
into elasticsearch so you can use kafka

00:06:37,520 --> 00:06:39,919
connect

00:06:38,240 --> 00:06:41,680
with pretty much any system you can

00:06:39,919 --> 00:06:44,319
think of you can use it with

00:06:41,680 --> 00:06:44,720
elasticsearch with snowflake with s3

00:06:44,319 --> 00:06:48,240
with

00:06:44,720 --> 00:06:49,440
any database with jdbc support you can

00:06:48,240 --> 00:06:50,880
write down to

00:06:49,440 --> 00:06:52,000
flat files if you really want to

00:06:50,880 --> 00:06:52,800
although it's probably not always a

00:06:52,000 --> 00:06:54,639
great idea

00:06:52,800 --> 00:06:56,400
and you can ingest data from loads of

00:06:54,639 --> 00:06:58,479
different places you can ingest data

00:06:56,400 --> 00:07:00,639
from databases which i'll show you later

00:06:58,479 --> 00:07:02,800
from message queues from syslog

00:07:00,639 --> 00:07:04,720
endpoints from rest endpoints

00:07:02,800 --> 00:07:06,720
wherever you've got data that you want

00:07:04,720 --> 00:07:09,599
to stream into kafka

00:07:06,720 --> 00:07:11,520
or from kafka out to other systems kafka

00:07:09,599 --> 00:07:14,160
connects is that integration api

00:07:11,520 --> 00:07:15,680
as part of apache kafka that does it for

00:07:14,160 --> 00:07:19,599
you and it's just configuration

00:07:15,680 --> 00:07:21,039
to use but what if we want to do a bit

00:07:19,599 --> 00:07:23,440
more with this data

00:07:21,039 --> 00:07:24,880
than simply dump it across into a target

00:07:23,440 --> 00:07:26,639
system let's have a little look at the

00:07:24,880 --> 00:07:29,039
data that we've got so we'll understand

00:07:26,639 --> 00:07:30,960
quite why that might be so if we just

00:07:29,039 --> 00:07:32,560
add some of these uh things into it so

00:07:30,960 --> 00:07:35,840
we've got a channel and we've got a

00:07:32,560 --> 00:07:37,919
message we've got the rating id

00:07:35,840 --> 00:07:40,000
so as this theater is coming in we've

00:07:37,919 --> 00:07:41,440
got various different fields within it

00:07:40,000 --> 00:07:43,039
so we've got one here called the channel

00:07:41,440 --> 00:07:43,680
this is the device that someone left the

00:07:43,039 --> 00:07:45,599
rating

00:07:43,680 --> 00:07:47,120
on now we can use this nice thing in

00:07:45,599 --> 00:07:49,039
cabana here to look at the

00:07:47,120 --> 00:07:50,400
information about the data within it and

00:07:49,039 --> 00:07:52,479
it breaks it down

00:07:50,400 --> 00:07:53,520
you can see we've got test information

00:07:52,479 --> 00:07:55,520
but this is first

00:07:53,520 --> 00:07:57,039
a production system we've also got kind

00:07:55,520 --> 00:07:57,680
of like production stuff called ios and

00:07:57,039 --> 00:08:00,400
so off

00:07:57,680 --> 00:08:02,240
but you've also got ios dash test so

00:08:00,400 --> 00:08:02,879
we've got data arriving in this source

00:08:02,240 --> 00:08:05,520
topic

00:08:02,879 --> 00:08:06,160
that we don't want in the target system

00:08:05,520 --> 00:08:08,000
now in the

00:08:06,160 --> 00:08:10,240
target system i can click on that little

00:08:08,000 --> 00:08:11,840
magnifying glass there and exclude it

00:08:10,240 --> 00:08:13,360
from what's shown but we're still

00:08:11,840 --> 00:08:14,720
shunting that data around we're still

00:08:13,360 --> 00:08:15,759
storing that type that data in the

00:08:14,720 --> 00:08:17,599
target system

00:08:15,759 --> 00:08:19,759
we're still inadvertently processing

00:08:17,599 --> 00:08:20,960
that data if we forget to exclude it

00:08:19,759 --> 00:08:22,879
from our queries

00:08:20,960 --> 00:08:25,280
we probably want to get rid of that

00:08:22,879 --> 00:08:27,360
we've also got this field here user id

00:08:25,280 --> 00:08:29,039
we've got a user id field so that's who

00:08:27,360 --> 00:08:30,960
left that particular rating

00:08:29,039 --> 00:08:32,479
but we don't know who that user is we've

00:08:30,960 --> 00:08:34,719
got that information about a user

00:08:32,479 --> 00:08:36,159
up in a database somewhere but if we

00:08:34,719 --> 00:08:38,159
think about bringing that into the

00:08:36,159 --> 00:08:40,320
target system and our target system has

00:08:38,159 --> 00:08:42,080
to join between two different sets of

00:08:40,320 --> 00:08:44,240
data and denormalize it

00:08:42,080 --> 00:08:46,160
such that we can show it in a particular

00:08:44,240 --> 00:08:48,320
uh analysis

00:08:46,160 --> 00:08:50,000
it's less easy to do when you hit the

00:08:48,320 --> 00:08:51,920
final system or you end up kind of like

00:08:50,000 --> 00:08:53,200
taking a stream of data and processing

00:08:51,920 --> 00:08:55,040
it as a batch

00:08:53,200 --> 00:08:56,560
we can actually say as that data's

00:08:55,040 --> 00:08:59,440
arriving in kafka

00:08:56,560 --> 00:09:01,120
we can enrich it as it goes through so

00:08:59,440 --> 00:09:02,959
let's do that i'm going to disable this

00:09:01,120 --> 00:09:05,680
refresh here because it's going to

00:09:02,959 --> 00:09:06,080
upset my zoom session otherwise so

00:09:05,680 --> 00:09:08,000
that's

00:09:06,080 --> 00:09:10,800
uh pause and now we're gonna head back

00:09:08,000 --> 00:09:12,560
over here into kc equal db

00:09:10,800 --> 00:09:13,920
so what we're going to do is let's

00:09:12,560 --> 00:09:15,120
remind ourselves we've got a topic

00:09:13,920 --> 00:09:17,200
called ratings

00:09:15,120 --> 00:09:18,720
we're going to take that ratings topic

00:09:17,200 --> 00:09:20,800
and instead of just filing on

00:09:18,720 --> 00:09:22,080
downstream somewhere else we're going to

00:09:20,800 --> 00:09:24,320
enrich and process

00:09:22,080 --> 00:09:26,320
it write it back onto another kafka

00:09:24,320 --> 00:09:27,839
topic and from that kafka topic then we

00:09:26,320 --> 00:09:30,640
could push that kafka topic

00:09:27,839 --> 00:09:31,120
down to our target system so let's do

00:09:30,640 --> 00:09:33,680
this

00:09:31,120 --> 00:09:34,399
we're going to create ourselves a stream

00:09:33,680 --> 00:09:36,880
against

00:09:34,399 --> 00:09:38,480
this topic so a casey called db it's an

00:09:36,880 --> 00:09:41,440
event streaming database

00:09:38,480 --> 00:09:42,399
it lets you do streaming etl stream

00:09:41,440 --> 00:09:45,120
processing

00:09:42,399 --> 00:09:47,040
against your data in kafka it also lets

00:09:45,120 --> 00:09:48,959
you build materialized views

00:09:47,040 --> 00:09:50,640
against your data in kafka that you can

00:09:48,959 --> 00:09:51,920
then query directly

00:09:50,640 --> 00:09:54,399
so we're going to mostly talk about

00:09:51,920 --> 00:09:56,560
streaming on etl here streaming

00:09:54,399 --> 00:09:57,920
processing applied to data that's in a

00:09:56,560 --> 00:09:58,800
kafka topic we'll talk a little bit

00:09:57,920 --> 00:10:01,760
about the

00:09:58,800 --> 00:10:03,279
materialized views later on but within

00:10:01,760 --> 00:10:06,160
keystick or db

00:10:03,279 --> 00:10:07,519
we have two primary types of objects we

00:10:06,160 --> 00:10:09,839
have a stream

00:10:07,519 --> 00:10:11,760
and we have a table so let's create a

00:10:09,839 --> 00:10:14,240
stream to start with so a stream

00:10:11,760 --> 00:10:15,920
is basically a kafka topic an unbounded

00:10:14,240 --> 00:10:18,000
series of events that continue

00:10:15,920 --> 00:10:19,279
indefinitely because it's unbounded but

00:10:18,000 --> 00:10:21,519
with a schema

00:10:19,279 --> 00:10:22,959
because the data in apache kafka is just

00:10:21,519 --> 00:10:25,040
bytes kafka

00:10:22,959 --> 00:10:26,160
it just takes bites whereas we're going

00:10:25,040 --> 00:10:28,160
to say well we're going to start

00:10:26,160 --> 00:10:29,440
projecting fields and applying predators

00:10:28,160 --> 00:10:31,440
and doing the kind of stuff

00:10:29,440 --> 00:10:32,880
that needs a schema so we've actually

00:10:31,440 --> 00:10:34,240
got a schema in the data

00:10:32,880 --> 00:10:36,560
because the good people who wrote that

00:10:34,240 --> 00:10:38,240
data into the topic they took the wise

00:10:36,560 --> 00:10:40,480
decision to serialize it

00:10:38,240 --> 00:10:41,760
in a way that it persisted that schema

00:10:40,480 --> 00:10:43,360
they didn't say oh we'll just chuck some

00:10:41,760 --> 00:10:44,959
csv data into the topic

00:10:43,360 --> 00:10:46,320
and now that poor soul who reads the

00:10:44,959 --> 00:10:47,440
data from the topic has to figure out

00:10:46,320 --> 00:10:49,200
what's going on

00:10:47,440 --> 00:10:51,200
instead they said well here's the schema

00:10:49,200 --> 00:10:53,440
for the data we'll store that

00:10:51,200 --> 00:10:55,600
as it gets serialized onto the topic and

00:10:53,440 --> 00:10:56,480
that schema gets stored in the schema

00:10:55,600 --> 00:10:58,560
registry

00:10:56,480 --> 00:11:01,040
which means that as a user i come along

00:10:58,560 --> 00:11:03,839
and say well i've got data in a topic

00:11:01,040 --> 00:11:04,959
called ratings it's in other formats so

00:11:03,839 --> 00:11:06,000
we're going to say creator streaming

00:11:04,959 --> 00:11:07,760
ratings

00:11:06,000 --> 00:11:09,600
with this information he said okay i've

00:11:07,760 --> 00:11:11,279
created the stream

00:11:09,600 --> 00:11:12,800
so we can say well tell me about that

00:11:11,279 --> 00:11:13,600
stream i don't quite believe that we've

00:11:12,800 --> 00:11:16,160
got a schema

00:11:13,600 --> 00:11:16,880
but we have because key sql db is the

00:11:16,160 --> 00:11:18,399
consumer

00:11:16,880 --> 00:11:20,640
it's pulled down that schema from the

00:11:18,399 --> 00:11:22,480
schema registry and it can use that in

00:11:20,640 --> 00:11:24,240
creating that stream object

00:11:22,480 --> 00:11:25,920
and because we've got a schema it means

00:11:24,240 --> 00:11:27,839
that we can project fields

00:11:25,920 --> 00:11:29,839
it means we can do this we can say show

00:11:27,839 --> 00:11:33,279
me just certain fields

00:11:29,839 --> 00:11:35,680
from that stream of data as it arrives

00:11:33,279 --> 00:11:37,279
as messages arrive on that kafka topic

00:11:35,680 --> 00:11:38,079
pick out just those particular fields

00:11:37,279 --> 00:11:40,000
and in this case

00:11:38,079 --> 00:11:41,279
just write it to the screen so again we

00:11:40,000 --> 00:11:44,720
can see those that data

00:11:41,279 --> 00:11:46,880
is ticking by we can also say well

00:11:44,720 --> 00:11:48,320
remember that channel field here we've

00:11:46,880 --> 00:11:49,440
got data flowing in and you've got kind

00:11:48,320 --> 00:11:51,760
of like ios

00:11:49,440 --> 00:11:53,040
tests and we've got this test data that

00:11:51,760 --> 00:11:55,519
we don't want

00:11:53,040 --> 00:11:57,279
in that target system or that target

00:11:55,519 --> 00:11:59,680
topic for others to read from

00:11:57,279 --> 00:12:01,040
so instead we can do this we can say

00:11:59,680 --> 00:12:03,680
let's select

00:12:01,040 --> 00:12:05,279
just this particular set of information

00:12:03,680 --> 00:12:07,279
from our source stream

00:12:05,279 --> 00:12:10,639
and apply predicate where the channel

00:12:07,279 --> 00:12:12,480
doesn't have tests in it

00:12:10,639 --> 00:12:14,160
so now pulling all this information in

00:12:12,480 --> 00:12:16,079
from that topic as it arrives

00:12:14,160 --> 00:12:17,279
and you'll notice in that channel there

00:12:16,079 --> 00:12:19,680
is no test

00:12:17,279 --> 00:12:21,040
data anymore because we've excluded it

00:12:19,680 --> 00:12:22,560
in that predicate

00:12:21,040 --> 00:12:25,519
now all we're doing at the moment is

00:12:22,560 --> 00:12:27,360
writing that select out onto the screen

00:12:25,519 --> 00:12:30,000
but the great thing with k sequel db is

00:12:27,360 --> 00:12:31,920
we can say well actually take that

00:12:30,000 --> 00:12:33,760
and write it to a new topic so we're

00:12:31,920 --> 00:12:37,279
going to say create stream

00:12:33,760 --> 00:12:40,000
and we'll call it ratings test as

00:12:37,279 --> 00:12:41,519
just select those particular fields

00:12:40,000 --> 00:12:42,000
sorry not ratings test that's the wrong

00:12:41,519 --> 00:12:43,920
one we've

00:12:42,000 --> 00:12:45,120
given it the wrong name there so i said

00:12:43,920 --> 00:12:46,800
i'd written the test script and it

00:12:45,120 --> 00:12:48,560
definitely worked

00:12:46,800 --> 00:12:50,720
so now here we go create stream ratings

00:12:48,560 --> 00:12:51,680
test two and that's still the wrong one

00:12:50,720 --> 00:12:54,160
because it could should be called

00:12:51,680 --> 00:12:56,160
ratings live

00:12:54,160 --> 00:12:57,600
or get there eventually third time lucky

00:12:56,160 --> 00:13:00,399
so we've created a stream called

00:12:57,600 --> 00:13:01,440
ratings live which has just got the data

00:13:00,399 --> 00:13:04,320
that doesn't include

00:13:01,440 --> 00:13:06,320
test in the particular field if i say

00:13:04,320 --> 00:13:07,680
show topics we can see we've got

00:13:06,320 --> 00:13:09,519
the two ones which are created by

00:13:07,680 --> 00:13:12,240
accidents we've got a topic called

00:13:09,519 --> 00:13:12,880
ratings live first say print ratings

00:13:12,240 --> 00:13:14,240
live

00:13:12,880 --> 00:13:15,920
you can see you've got the data in that

00:13:14,240 --> 00:13:17,440
we've got the fields that we asked for

00:13:15,920 --> 00:13:21,279
in that select statement

00:13:17,440 --> 00:13:24,320
with the channel and no test data

00:13:21,279 --> 00:13:26,240
so we say create stream as select

00:13:24,320 --> 00:13:28,000
the as select says take the output of

00:13:26,240 --> 00:13:30,720
that continuous select statement

00:13:28,000 --> 00:13:31,519
write it into this stream but k sql db

00:13:30,720 --> 00:13:34,720
streams

00:13:31,519 --> 00:13:35,760
are backed by kafka topics so the first

00:13:34,720 --> 00:13:38,320
create stream we did

00:13:35,760 --> 00:13:39,920
basically registered the existing topic

00:13:38,320 --> 00:13:42,240
the second one createstream

00:13:39,920 --> 00:13:43,199
as select created a brand new kafka

00:13:42,240 --> 00:13:45,680
topic or

00:13:43,199 --> 00:13:48,560
populated an existing kafka topic with

00:13:45,680 --> 00:13:50,079
the results of that select statement

00:13:48,560 --> 00:13:52,800
i can do the same thing we can also

00:13:50,079 --> 00:13:54,560
create a test one

00:13:52,800 --> 00:13:56,720
which is probably not going to work

00:13:54,560 --> 00:13:58,240
because it might do let's say

00:13:56,720 --> 00:13:59,680
create stream ratings test and it is

00:13:58,240 --> 00:14:00,880
actually test data this time so saying

00:13:59,680 --> 00:14:03,199
where the channel is

00:14:00,880 --> 00:14:04,079
like test he says well that exists

00:14:03,199 --> 00:14:05,600
already

00:14:04,079 --> 00:14:07,440
so let's go about our piece let's say

00:14:05,600 --> 00:14:08,320
okay we're going to drop this particular

00:14:07,440 --> 00:14:11,600
stream

00:14:08,320 --> 00:14:13,120
so drop that stream there

00:14:11,600 --> 00:14:14,320
and it says well you can't really do

00:14:13,120 --> 00:14:15,360
that because at the moment you've got a

00:14:14,320 --> 00:14:17,199
query that's running

00:14:15,360 --> 00:14:18,639
let's say show queries we've got these

00:14:17,199 --> 00:14:20,800
different queries that are running

00:14:18,639 --> 00:14:22,320
writing into that object i'm trying to

00:14:20,800 --> 00:14:23,120
drop which would kind of like not make

00:14:22,320 --> 00:14:25,360
sense if i did

00:14:23,120 --> 00:14:27,680
if i did drop it so instead we're going

00:14:25,360 --> 00:14:30,639
to say terminate that query first

00:14:27,680 --> 00:14:32,639
and then we can drop that stream which

00:14:30,639 --> 00:14:35,839
means that we can then create it

00:14:32,639 --> 00:14:38,480
like this and now

00:14:35,839 --> 00:14:38,959
we've got new streams we've got ratings

00:14:38,480 --> 00:14:40,959
live

00:14:38,959 --> 00:14:42,720
and ratings test ignore the test two

00:14:40,959 --> 00:14:45,279
that was me getting fat fingered

00:14:42,720 --> 00:14:45,839
so ratings live is just our source

00:14:45,279 --> 00:14:48,560
stream

00:14:45,839 --> 00:14:50,240
with just those live events the test one

00:14:48,560 --> 00:14:53,199
is the source stream with just

00:14:50,240 --> 00:14:53,920
the test events so we can take that and

00:14:53,199 --> 00:14:55,040
we can

00:14:53,920 --> 00:14:56,959
go and look at it further we can

00:14:55,040 --> 00:14:59,199
describe it so we can say

00:14:56,959 --> 00:15:00,160
describe ratings live obviously you've

00:14:59,199 --> 00:15:02,399
got the schema

00:15:00,160 --> 00:15:03,920
just of those fields that we asked for

00:15:02,399 --> 00:15:05,199
i'm going to turn my

00:15:03,920 --> 00:15:07,680
track pad off because that's getting

00:15:05,199 --> 00:15:10,079
kind of upset don't know why

00:15:07,680 --> 00:15:10,959
i think my mac knows that i'm doing a

00:15:10,079 --> 00:15:13,440
presentation

00:15:10,959 --> 00:15:15,120
and it's being unhelpful so we'll

00:15:13,440 --> 00:15:17,839
disable that

00:15:15,120 --> 00:15:19,279
okay so we've got this stream of data

00:15:17,839 --> 00:15:22,320
it's arriving in this particular stream

00:15:19,279 --> 00:15:25,440
i could say select star from ratings

00:15:22,320 --> 00:15:26,560
live and it says we need that bit as

00:15:25,440 --> 00:15:29,040
well

00:15:26,560 --> 00:15:30,079
and so now we've got that data arriving

00:15:29,040 --> 00:15:31,759
on the source topic

00:15:30,079 --> 00:15:33,519
and it's now showing on this new one

00:15:31,759 --> 00:15:34,240
this new stream here and we could take

00:15:33,519 --> 00:15:35,600
that stream

00:15:34,240 --> 00:15:37,759
and we could write it to elasticsearch

00:15:35,600 --> 00:15:38,320
if we wanted we're simply taking a

00:15:37,759 --> 00:15:40,320
source

00:15:38,320 --> 00:15:42,320
transform it write it out to a new topic

00:15:40,320 --> 00:15:44,399
and then write that over to the target

00:15:42,320 --> 00:15:45,839
but we're not finished yet we're going

00:15:44,399 --> 00:15:47,680
to take that source data

00:15:45,839 --> 00:15:49,199
and we're going to add in information

00:15:47,680 --> 00:15:50,800
about our particular users

00:15:49,199 --> 00:15:53,040
so if we remind ourselves what the

00:15:50,800 --> 00:15:54,079
fields are here we've got a field called

00:15:53,040 --> 00:15:55,440
user id

00:15:54,079 --> 00:15:57,360
and we want to be able to say well if

00:15:55,440 --> 00:15:58,959
it's user id 42

00:15:57,360 --> 00:16:00,560
we want to know from a database well

00:15:58,959 --> 00:16:02,560
that's so-and-so person

00:16:00,560 --> 00:16:04,639
and this is their email address this is

00:16:02,560 --> 00:16:05,360
their club status are they a vip

00:16:04,639 --> 00:16:07,680
customer

00:16:05,360 --> 00:16:09,120
are they a brand new customer if they're

00:16:07,680 --> 00:16:10,639
a really important customer

00:16:09,120 --> 00:16:11,680
and they're leaving a bad rating we

00:16:10,639 --> 00:16:12,880
probably want to know about that so

00:16:11,680 --> 00:16:16,399
we'll maybe process their

00:16:12,880 --> 00:16:16,800
their data differently so let's do that

00:16:16,399 --> 00:16:19,360
now

00:16:16,800 --> 00:16:20,959
let's head over to our database so

00:16:19,360 --> 00:16:22,320
terminate that query

00:16:20,959 --> 00:16:24,560
and we're going to do this we're going

00:16:22,320 --> 00:16:27,199
to split the screen i'm going to launch

00:16:24,560 --> 00:16:28,800
into a database so here i'm using my sql

00:16:27,199 --> 00:16:31,519
but what i'm going to show you

00:16:28,800 --> 00:16:32,880
applies to any relational database so

00:16:31,519 --> 00:16:34,160
we're going to take data

00:16:32,880 --> 00:16:36,399
from a relational database we're going

00:16:34,160 --> 00:16:37,360
to take a snapshot of it and stream it

00:16:36,399 --> 00:16:38,720
into our topic

00:16:37,360 --> 00:16:41,360
but we're then going to capture every

00:16:38,720 --> 00:16:44,959
single subsequent change to that data

00:16:41,360 --> 00:16:45,839
into our topic so within our database

00:16:44,959 --> 00:16:47,600
we've got a table

00:16:45,839 --> 00:16:50,000
called customers you can probably guess

00:16:47,600 --> 00:16:52,160
what kind of data sits in that table

00:16:50,000 --> 00:16:53,839
it's information about our customers if

00:16:52,160 --> 00:16:55,519
we show just the first five rows we can

00:16:53,839 --> 00:16:56,880
see they've got an id they've got first

00:16:55,519 --> 00:16:59,759
name a last name

00:16:56,880 --> 00:17:01,440
an email address and their club status

00:16:59,759 --> 00:17:04,160
so we're gonna do things later on

00:17:01,440 --> 00:17:05,839
and say if it was a platinum user who

00:17:04,160 --> 00:17:08,400
left a rating that was less than

00:17:05,839 --> 00:17:09,520
three stars out of five we'll call that

00:17:08,400 --> 00:17:11,199
like a negative review

00:17:09,520 --> 00:17:12,880
for a really important customer or

00:17:11,199 --> 00:17:15,120
actually route that to a separate topic

00:17:12,880 --> 00:17:17,439
we just want to process it differently

00:17:15,120 --> 00:17:18,959
but let's get this data into our kafka

00:17:17,439 --> 00:17:20,319
topic

00:17:18,959 --> 00:17:21,679
again we're going to use kafka connect

00:17:20,319 --> 00:17:22,720
for this so we're going to take this

00:17:21,679 --> 00:17:24,959
data

00:17:22,720 --> 00:17:26,640
and at the top of the screen we're back

00:17:24,959 --> 00:17:28,400
in k sql db

00:17:26,640 --> 00:17:29,840
again we're using k sql db as the

00:17:28,400 --> 00:17:32,080
wrapper around

00:17:29,840 --> 00:17:33,200
kafka connect and if we go up to the top

00:17:32,080 --> 00:17:35,200
here we can see we've got

00:17:33,200 --> 00:17:36,880
we're creating a source connector we're

00:17:35,200 --> 00:17:37,440
using a connector from the debesium

00:17:36,880 --> 00:17:39,520
project

00:17:37,440 --> 00:17:40,799
so debesium is an open source project

00:17:39,520 --> 00:17:42,320
they've got a ton of really good

00:17:40,799 --> 00:17:45,200
characters for kafka connect

00:17:42,320 --> 00:17:46,080
for doing cdc out of source systems so

00:17:45,200 --> 00:17:48,720
there's my sequel

00:17:46,080 --> 00:17:49,200
there's postgres there's uh

00:17:48,720 --> 00:17:51,200
there's

00:17:49,200 --> 00:17:52,320
an oracle one they're working on this

00:17:51,200 --> 00:17:54,400
sql server

00:17:52,320 --> 00:17:55,440
so you've got data in relational systems

00:17:54,400 --> 00:17:57,919
you want to get into

00:17:55,440 --> 00:17:58,720
a kafka topic most of them or all of

00:17:57,919 --> 00:18:01,360
them i think

00:17:58,720 --> 00:18:02,799
use what's called log based change data

00:18:01,360 --> 00:18:04,720
capture so actually going down to the

00:18:02,799 --> 00:18:08,160
transaction log of that database

00:18:04,720 --> 00:18:09,440
to get the data in so we hit enter on

00:18:08,160 --> 00:18:11,679
that it's created

00:18:09,440 --> 00:18:13,200
that source connector show connectors

00:18:11,679 --> 00:18:15,679
make sure it's working

00:18:13,200 --> 00:18:17,280
and it says it is working so here's our

00:18:15,679 --> 00:18:19,280
source connector for my sql

00:18:17,280 --> 00:18:22,240
and it says it's running there which

00:18:19,280 --> 00:18:24,160
means if we go to show topics

00:18:22,240 --> 00:18:26,160
we can see we've got a new topic in the

00:18:24,160 --> 00:18:28,480
system so we've got our existing

00:18:26,160 --> 00:18:30,080
original source one vehicle ratings

00:18:28,480 --> 00:18:32,400
we've got that little accident the

00:18:30,080 --> 00:18:34,320
ratings test two we've got our two ones

00:18:32,400 --> 00:18:36,320
that we created here ratings live

00:18:34,320 --> 00:18:38,240
a ratings test where we split that

00:18:36,320 --> 00:18:40,160
source stream into different streams

00:18:38,240 --> 00:18:41,120
based on the channel value

00:18:40,160 --> 00:18:43,280
and now we've got this one called

00:18:41,120 --> 00:18:45,440
customers so i can say print

00:18:43,280 --> 00:18:47,600
customers i'm going to say from

00:18:45,440 --> 00:18:49,760
beginning

00:18:47,600 --> 00:18:51,600
so from beginning is important i'm going

00:18:49,760 --> 00:18:54,799
to talk about offsets in a moment

00:18:51,600 --> 00:18:56,480
because with the data in a kafka topic

00:18:54,799 --> 00:18:58,240
at the moment we've simply been saying

00:18:56,480 --> 00:18:59,600
as that new data arrives

00:18:58,240 --> 00:19:02,000
write it out to the screen or write it

00:18:59,600 --> 00:19:04,480
out to a new topic but because kafka

00:19:02,000 --> 00:19:05,679
stores data for as long as we tell it to

00:19:04,480 --> 00:19:08,320
it means that we can actually go back

00:19:05,679 --> 00:19:10,000
and reprocess that set of data

00:19:08,320 --> 00:19:11,840
and it also means if we say print this

00:19:10,000 --> 00:19:13,520
topic by default k sequel db you'll sit

00:19:11,840 --> 00:19:15,600
there and wait for things to print

00:19:13,520 --> 00:19:17,200
we can say print data from the beginning

00:19:15,600 --> 00:19:18,559
of the topic and it shows us all the

00:19:17,200 --> 00:19:19,760
data that's already

00:19:18,559 --> 00:19:21,840
in the topic and then it says press

00:19:19,760 --> 00:19:22,880
control c to interrupt because we might

00:19:21,840 --> 00:19:24,640
get new data

00:19:22,880 --> 00:19:26,080
arriving and you can see the kind of

00:19:24,640 --> 00:19:27,120
data that we get through we've got the

00:19:26,080 --> 00:19:28,559
user id

00:19:27,120 --> 00:19:30,720
we've got their first name their last

00:19:28,559 --> 00:19:33,039
name and so on

00:19:30,720 --> 00:19:34,160
so we've now got a snapshot of what's in

00:19:33,039 --> 00:19:36,400
the database

00:19:34,160 --> 00:19:37,360
in a kafka topic let's show you what

00:19:36,400 --> 00:19:40,240
happens what

00:19:37,360 --> 00:19:41,440
if you create or if you change data in

00:19:40,240 --> 00:19:43,840
that database

00:19:41,440 --> 00:19:45,440
it's going to create a row in my sequel

00:19:43,840 --> 00:19:47,840
so i'm going to do an insert into

00:19:45,440 --> 00:19:50,240
i'm using insert into but obviously the

00:19:47,840 --> 00:19:52,880
databases that were hooking up to kafka

00:19:50,240 --> 00:19:54,320
are the databases which underpin most of

00:19:52,880 --> 00:19:55,600
the existing applications that

00:19:54,320 --> 00:19:58,240
businesses tend to use

00:19:55,600 --> 00:19:58,720
our crms our inventory platforms all of

00:19:58,240 --> 00:20:01,039
those

00:19:58,720 --> 00:20:01,919
used databases and the cool thing here

00:20:01,039 --> 00:20:04,000
is we can

00:20:01,919 --> 00:20:06,159
take that existing set of data into our

00:20:04,000 --> 00:20:08,240
existing systems in our businesses

00:20:06,159 --> 00:20:09,840
and stream that data and changes to that

00:20:08,240 --> 00:20:11,679
data into kafka

00:20:09,840 --> 00:20:13,280
and once the data is in kafka the world

00:20:11,679 --> 00:20:15,840
is our oyster because most

00:20:13,280 --> 00:20:16,480
systems and applications integrate with

00:20:15,840 --> 00:20:18,400
kafka

00:20:16,480 --> 00:20:19,520
so we can now get that data from what we

00:20:18,400 --> 00:20:20,320
thought was kind of like a closed

00:20:19,520 --> 00:20:22,400
database

00:20:20,320 --> 00:20:24,000
stream it into kafka and now we can

00:20:22,400 --> 00:20:25,440
write analytics against it write

00:20:24,000 --> 00:20:28,320
applications against it

00:20:25,440 --> 00:20:30,400
and do loads of fun stuff so let's

00:20:28,320 --> 00:20:32,159
insert a row into our database we do an

00:20:30,400 --> 00:20:34,080
insert into customers

00:20:32,159 --> 00:20:35,919
over here in my sql we've inserted a new

00:20:34,080 --> 00:20:38,080
row and as i did it

00:20:35,919 --> 00:20:39,120
it showed up in our kafka topic here

00:20:38,080 --> 00:20:41,360
because that's

00:20:39,120 --> 00:20:42,159
a consumer that k sql db is running at

00:20:41,360 --> 00:20:43,840
the top there

00:20:42,159 --> 00:20:45,280
it just sits there waiting for new

00:20:43,840 --> 00:20:46,720
messages to arrive

00:20:45,280 --> 00:20:47,760
if i didn't make another change we say

00:20:46,720 --> 00:20:48,880
right we're going to update the

00:20:47,760 --> 00:20:50,480
customers

00:20:48,880 --> 00:20:51,919
so at the bottom of the screen i'm going

00:20:50,480 --> 00:20:53,600
to enter the command but watch

00:20:51,919 --> 00:20:54,960
what happens at the top of the screen i

00:20:53,600 --> 00:20:56,960
do an update and

00:20:54,960 --> 00:20:59,360
the updates arrive straight away on that

00:20:56,960 --> 00:21:02,080
kafka topic

00:20:59,360 --> 00:21:04,640
so we've got a kafka topic with

00:21:02,080 --> 00:21:06,960
information about our customers

00:21:04,640 --> 00:21:08,559
the customers come from a table with a

00:21:06,960 --> 00:21:11,600
primary key the primary key

00:21:08,559 --> 00:21:13,200
is that user that customer id and we can

00:21:11,600 --> 00:21:15,760
see that here the key is

00:21:13,200 --> 00:21:17,360
picked up as the the id from the

00:21:15,760 --> 00:21:20,240
customer information

00:21:17,360 --> 00:21:21,520
and in key sql db let's just move this

00:21:20,240 --> 00:21:22,640
and move this a little bit so we can see

00:21:21,520 --> 00:21:24,720
what's going on

00:21:22,640 --> 00:21:27,120
in key sql db we're going to create a

00:21:24,720 --> 00:21:27,919
new object to sit on top of that kafka

00:21:27,120 --> 00:21:29,840
topic

00:21:27,919 --> 00:21:30,960
because to work with data from kafka in

00:21:29,840 --> 00:21:34,320
k sql db

00:21:30,960 --> 00:21:36,240
it has to be a stream or a table

00:21:34,320 --> 00:21:37,840
so we've built a stream already to sit

00:21:36,240 --> 00:21:39,760
on top of that ratings topic where it's

00:21:37,840 --> 00:21:40,880
a topic with a schema when we saw that

00:21:39,760 --> 00:21:43,120
schema

00:21:40,880 --> 00:21:44,720
now we're going to create a table which

00:21:43,120 --> 00:21:47,919
is the other type of object

00:21:44,720 --> 00:21:51,200
in k sql db so the table looks

00:21:47,919 --> 00:21:51,919
like this if i copy it from my little

00:21:51,200 --> 00:21:55,440
cheat sheet

00:21:51,919 --> 00:21:58,320
down here and it should

00:21:55,440 --> 00:21:59,679
look like that create table customers so

00:21:58,320 --> 00:22:02,159
saying create a table

00:21:59,679 --> 00:22:02,799
called customers against this existing

00:22:02,159 --> 00:22:04,640
topic

00:22:02,799 --> 00:22:06,559
it's also in afro and we could use

00:22:04,640 --> 00:22:09,200
protobuf we could use json schema

00:22:06,559 --> 00:22:10,799
anything that persists that schema and

00:22:09,200 --> 00:22:13,120
we're saying the primary key

00:22:10,799 --> 00:22:14,880
for our customer information is this

00:22:13,120 --> 00:22:18,080
customer id field

00:22:14,880 --> 00:22:21,360
so a table in k sql db

00:22:18,080 --> 00:22:22,400
is a value for a given key now that

00:22:21,360 --> 00:22:23,760
value might change

00:22:22,400 --> 00:22:25,600
in the same way in the database world

00:22:23,760 --> 00:22:26,000
when we go and insert something and then

00:22:25,600 --> 00:22:28,799
update

00:22:26,000 --> 00:22:29,280
it that value changes it's the same

00:22:28,799 --> 00:22:32,240
thing

00:22:29,280 --> 00:22:34,080
in the case equal db world a stream is

00:22:32,240 --> 00:22:35,360
just an unbounded series of events

00:22:34,080 --> 00:22:37,679
this happened and then this happened and

00:22:35,360 --> 00:22:41,520
then this happened a table

00:22:37,679 --> 00:22:44,640
is the state table is for this given key

00:22:41,520 --> 00:22:47,200
what's the value but both

00:22:44,640 --> 00:22:48,720
are built on top of a kafka topic so

00:22:47,200 --> 00:22:50,720
this is the really really cool thing

00:22:48,720 --> 00:22:52,720
we're taking a series of events a stream

00:22:50,720 --> 00:22:54,880
of events and that's all you need

00:22:52,720 --> 00:22:56,080
to be able to either work with streams

00:22:54,880 --> 00:22:57,760
or tables

00:22:56,080 --> 00:23:00,799
because from a stream of events you can

00:22:57,760 --> 00:23:02,799
build a table of state

00:23:00,799 --> 00:23:05,360
so let's take that and what we can do

00:23:02,799 --> 00:23:07,039
now is we can query that table

00:23:05,360 --> 00:23:08,559
and i'm going to run this command here i

00:23:07,039 --> 00:23:09,600
mentioned talked earlier briefly about

00:23:08,559 --> 00:23:11,120
offsets

00:23:09,600 --> 00:23:13,679
and i said print this topic from

00:23:11,120 --> 00:23:14,640
beginning well when we're running k sql

00:23:13,679 --> 00:23:16,080
db commands

00:23:14,640 --> 00:23:17,760
we say we can set this particular

00:23:16,080 --> 00:23:19,120
session property called

00:23:17,760 --> 00:23:21,039
auto offset reset we're going to set it

00:23:19,120 --> 00:23:22,720
back to the earliest because we're going

00:23:21,039 --> 00:23:24,559
to query our table

00:23:22,720 --> 00:23:25,760
so we'll say select these values from

00:23:24,559 --> 00:23:28,400
here submit

00:23:25,760 --> 00:23:31,120
five just show me the first five rows

00:23:28,400 --> 00:23:34,000
from my k sql db table

00:23:31,120 --> 00:23:34,480
and if i go down to my sql and say

00:23:34,000 --> 00:23:36,480
select

00:23:34,480 --> 00:23:38,320
the first five rows from the table it

00:23:36,480 --> 00:23:40,159
says well here we go customer id one is

00:23:38,320 --> 00:23:42,080
this customer id2 is that

00:23:40,159 --> 00:23:43,600
and we've got in the database down here

00:23:42,080 --> 00:23:46,640
at the bottom of the screen

00:23:43,600 --> 00:23:49,200
matches what we've got in our k sql db

00:23:46,640 --> 00:23:50,080
table at the top of the screen but our

00:23:49,200 --> 00:23:53,520
k-sequel db

00:23:50,080 --> 00:23:54,720
table is built on top of a kafka topic

00:23:53,520 --> 00:23:56,559
let me show you what happens if we

00:23:54,720 --> 00:23:58,159
change data so

00:23:56,559 --> 00:24:01,120
at the top of the screen let's just

00:23:58,159 --> 00:24:04,480
query that information

00:24:01,120 --> 00:24:07,919
for from customers where customer

00:24:04,480 --> 00:24:11,120
id equals 42. and it's not happy with

00:24:07,919 --> 00:24:11,120
that's because it's a string

00:24:11,520 --> 00:24:15,279
and it says here is customer id 42

00:24:14,000 --> 00:24:17,200
customer id 42

00:24:15,279 --> 00:24:18,880
is mr rick astley and he's got an email

00:24:17,200 --> 00:24:20,720
address but no club status

00:24:18,880 --> 00:24:22,559
and if i go down to my database i'm

00:24:20,720 --> 00:24:24,080
going to my sql adder on the same query

00:24:22,559 --> 00:24:26,559
from customers

00:24:24,080 --> 00:24:27,760
id equals 42. it says okay here's mr

00:24:26,559 --> 00:24:29,520
rickassey you've got an email address

00:24:27,760 --> 00:24:31,600
but no club status

00:24:29,520 --> 00:24:32,640
so my source and what we're showing in

00:24:31,600 --> 00:24:34,640
kc equal db

00:24:32,640 --> 00:24:35,840
they agree with each other so now let's

00:24:34,640 --> 00:24:37,840
give them a club status

00:24:35,840 --> 00:24:39,039
so we make an update to the database

00:24:37,840 --> 00:24:40,720
you'll notice at the top of the screen

00:24:39,039 --> 00:24:42,240
it says press ctrl c to interrupt it's a

00:24:40,720 --> 00:24:44,480
continuous query

00:24:42,240 --> 00:24:45,679
as the state changes from our table

00:24:44,480 --> 00:24:46,799
we're going to see that update come

00:24:45,679 --> 00:24:48,240
through

00:24:46,799 --> 00:24:50,000
so the database at the bottom of the

00:24:48,240 --> 00:24:50,960
screen we do an update we give them a

00:24:50,000 --> 00:24:52,080
club status

00:24:50,960 --> 00:24:54,159
and at the top of the screen it says

00:24:52,080 --> 00:24:56,080
okay that states has changed so i'll

00:24:54,159 --> 00:24:57,440
show you the new row i would say oh we

00:24:56,080 --> 00:24:58,080
made a mistake obviously it can't be

00:24:57,440 --> 00:24:59,919
bronze

00:24:58,080 --> 00:25:01,840
definitely got to be platinum status so

00:24:59,919 --> 00:25:04,559
about that change in the database

00:25:01,840 --> 00:25:05,919
and the case equal db table reflects

00:25:04,559 --> 00:25:08,320
that change

00:25:05,919 --> 00:25:09,600
but here's the difference if i say

00:25:08,320 --> 00:25:12,960
select the current

00:25:09,600 --> 00:25:15,120
state of that key from a k sql db table

00:25:12,960 --> 00:25:16,480
so where you customer id equals 42

00:25:15,120 --> 00:25:18,720
what's the current

00:25:16,480 --> 00:25:19,520
state kc equal db says this is the

00:25:18,720 --> 00:25:22,159
current

00:25:19,520 --> 00:25:22,960
state the current state is 42 uh email

00:25:22,159 --> 00:25:25,440
address this

00:25:22,960 --> 00:25:26,840
club status platinum and it's building

00:25:25,440 --> 00:25:29,840
that from a kafka

00:25:26,840 --> 00:25:29,840
topic

00:25:32,159 --> 00:25:35,279
okay i'm seeing messages saying that you

00:25:33,679 --> 00:25:39,679
can't hear me

00:25:35,279 --> 00:25:39,679
can someone confirm or deny

00:25:41,679 --> 00:25:45,520
just send a message on the webinar chat

00:25:43,360 --> 00:25:47,120
saying if you can hear phone

00:25:45,520 --> 00:25:48,480
okay you can all hear fine good stuff so

00:25:47,120 --> 00:25:49,520
it's been recorded so if your audio

00:25:48,480 --> 00:25:50,720
dropped out then you can watch the

00:25:49,520 --> 00:25:51,919
recording later

00:25:50,720 --> 00:25:54,400
and people are actually awake and this

00:25:51,919 --> 00:25:55,600
is great okay so we've got a table

00:25:54,400 --> 00:25:57,919
showing at the top it's showing the

00:25:55,600 --> 00:25:59,600
current state and it's built on top of

00:25:57,919 --> 00:26:01,840
the kafka topic

00:25:59,600 --> 00:26:03,120
but what about if we want to know well

00:26:01,840 --> 00:26:05,360
what happened

00:26:03,120 --> 00:26:07,520
to this object when it was created when

00:26:05,360 --> 00:26:09,120
was it last updated how many times has

00:26:07,520 --> 00:26:11,760
that club status changed

00:26:09,120 --> 00:26:12,640
well this is where a stream comes out so

00:26:11,760 --> 00:26:15,039
we can do this

00:26:12,640 --> 00:26:17,840
we can say we find the correct commands

00:26:15,039 --> 00:26:19,360
we're going to say create stream

00:26:17,840 --> 00:26:21,200
so in case you call ebay we're going to

00:26:19,360 --> 00:26:23,360
say create a stream

00:26:21,200 --> 00:26:24,799
for customers with the same syntax but

00:26:23,360 --> 00:26:26,400
we're just saying create a stream

00:26:24,799 --> 00:26:28,799
this time as opposed to create a table

00:26:26,400 --> 00:26:31,919
it's on top of the same topic

00:26:28,799 --> 00:26:33,679
create the stream so now if i say

00:26:31,919 --> 00:26:35,200
select these values from the customer

00:26:33,679 --> 00:26:37,919
table we say

00:26:35,200 --> 00:26:39,520
here is the current state based on that

00:26:37,919 --> 00:26:41,039
kafka topic of replay in those events to

00:26:39,520 --> 00:26:44,000
work out the current state it says

00:26:41,039 --> 00:26:45,600
here is the current state if i run that

00:26:44,000 --> 00:26:49,600
same query

00:26:45,600 --> 00:26:52,320
i say from customers stream

00:26:49,600 --> 00:26:53,039
it says here are the series of events in

00:26:52,320 --> 00:26:54,480
that topic

00:26:53,039 --> 00:26:56,720
where it matches that predicate that

00:26:54,480 --> 00:26:58,400
we've given it but both the stream and

00:26:56,720 --> 00:27:00,400
the table come from the same

00:26:58,400 --> 00:27:01,600
topic and whether you use a stream or a

00:27:00,400 --> 00:27:03,120
table depends on

00:27:01,600 --> 00:27:05,440
the semantics of the query that you're

00:27:03,120 --> 00:27:08,000
asking are you asking what's the state

00:27:05,440 --> 00:27:09,279
for this key are you asking what are the

00:27:08,000 --> 00:27:11,279
series of events

00:27:09,279 --> 00:27:13,200
if i'm writing an application which

00:27:11,279 --> 00:27:14,559
needs to know when someone changes their

00:27:13,200 --> 00:27:15,919
email address because i need to go and

00:27:14,559 --> 00:27:17,600
do something about it or

00:27:15,919 --> 00:27:19,360
they get their club status changes to

00:27:17,600 --> 00:27:20,640
platinum so i need to trigger something

00:27:19,360 --> 00:27:21,279
which sends them a nice little welcome

00:27:20,640 --> 00:27:22,880
pack

00:27:21,279 --> 00:27:24,720
i want to subscribe to that stream of

00:27:22,880 --> 00:27:28,399
events i want to be driven by

00:27:24,720 --> 00:27:31,200
events if i want to join to that

00:27:28,399 --> 00:27:32,880
uh kafka topic to enrich a series of

00:27:31,200 --> 00:27:35,200
events and basically do a look up

00:27:32,880 --> 00:27:37,120
against that data then i want the table

00:27:35,200 --> 00:27:39,120
i would say what's the current state

00:27:37,120 --> 00:27:41,600
for that key and we're going to do that

00:27:39,120 --> 00:27:44,159
now we're going to take this information

00:27:41,600 --> 00:27:45,679
from our ratings as they've been flowing

00:27:44,159 --> 00:27:46,720
in so if you remember those we saw those

00:27:45,679 --> 00:27:49,919
first time rounds

00:27:46,720 --> 00:27:52,000
let's just move that down there we're

00:27:49,919 --> 00:27:54,240
gonna say select the rating information

00:27:52,000 --> 00:27:55,039
and our customer information from

00:27:54,240 --> 00:27:56,960
ratings

00:27:55,039 --> 00:27:58,240
and do a left join to our customer

00:27:56,960 --> 00:28:01,279
information on

00:27:58,240 --> 00:28:02,320
user id equals user id oops don't want

00:28:01,279 --> 00:28:04,960
that bit in there because that'll

00:28:02,320 --> 00:28:04,960
confuse things

00:28:05,679 --> 00:28:08,720
so now as the ratings arrive it's going

00:28:07,360 --> 00:28:09,600
to get a full screen of information at

00:28:08,720 --> 00:28:11,039
the moment

00:28:09,600 --> 00:28:12,960
because if you remember we said offset

00:28:11,039 --> 00:28:14,720
to earliest so this is now going back

00:28:12,960 --> 00:28:15,360
through that whole ratings topic from

00:28:14,720 --> 00:28:16,399
the beginning

00:28:15,360 --> 00:28:18,799
of time when it started getting

00:28:16,399 --> 00:28:20,320
populated and it's taking every single

00:28:18,799 --> 00:28:22,240
rating let me just pause it there

00:28:20,320 --> 00:28:24,159
every single rating as it arrives it's

00:28:22,240 --> 00:28:24,799
saying well who is that particular user

00:28:24,159 --> 00:28:27,840
id to

00:28:24,799 --> 00:28:30,080
join out to that customers table

00:28:27,840 --> 00:28:31,200
and have reached the information add in

00:28:30,080 --> 00:28:34,320
the username

00:28:31,200 --> 00:28:35,679
add in the club status and instead of

00:28:34,320 --> 00:28:37,600
writing it to the screen which kind of

00:28:35,679 --> 00:28:40,080
gets boring watching it scroll by

00:28:37,600 --> 00:28:41,600
we're going to say take that information

00:28:40,080 --> 00:28:45,039
and go and write it

00:28:41,600 --> 00:28:46,559
to a new key sql db stream and it's a

00:28:45,039 --> 00:28:47,840
stream because it's a series of events

00:28:46,559 --> 00:28:49,760
those ratings events

00:28:47,840 --> 00:28:51,840
to which we're simply denormalizing and

00:28:49,760 --> 00:28:53,440
adding in additional information

00:28:51,840 --> 00:28:55,760
so here's our kafka topic so we can

00:28:53,440 --> 00:28:56,720
actually specify the name of the topic

00:28:55,760 --> 00:28:59,440
this time

00:28:56,720 --> 00:29:01,360
by default when you create a stream the

00:28:59,440 --> 00:29:03,039
topic is the same as the stream name

00:29:01,360 --> 00:29:04,559
we're actually just saying uh give it a

00:29:03,039 --> 00:29:06,000
different topic name

00:29:04,559 --> 00:29:08,159
we're going to select those values from

00:29:06,000 --> 00:29:09,600
here and then it changes

00:29:08,159 --> 00:29:12,960
and now it just says okay i've created

00:29:09,600 --> 00:29:14,480
that query we say show streams

00:29:12,960 --> 00:29:17,200
we've created a new stream ratings and

00:29:14,480 --> 00:29:18,720
customer data this time the kafka topic

00:29:17,200 --> 00:29:21,039
is called ratings enriched instead of

00:29:18,720 --> 00:29:23,760
just inheriting the stream name

00:29:21,039 --> 00:29:24,559
and i can do this i can say describe

00:29:23,760 --> 00:29:26,559
that stream

00:29:24,559 --> 00:29:28,640
it says here is the schema for that

00:29:26,559 --> 00:29:31,200
stream we also say describe

00:29:28,640 --> 00:29:32,880
extended and now it says well here's

00:29:31,200 --> 00:29:34,960
more information about it

00:29:32,880 --> 00:29:36,159
we can see what's writing into that

00:29:34,960 --> 00:29:38,159
particular topic

00:29:36,159 --> 00:29:40,080
if i say do that again we can see how

00:29:38,159 --> 00:29:41,600
many messages have we processed

00:29:40,080 --> 00:29:43,760
so here's our total messages that we've

00:29:41,600 --> 00:29:45,440
processed 5610

00:29:43,760 --> 00:29:47,039
when was the last one that we processed

00:29:45,440 --> 00:29:49,679
it was 1859

00:29:47,039 --> 00:29:51,840
utc which is going out now and if i

00:29:49,679 --> 00:29:53,960
rerun that query

00:29:51,840 --> 00:29:55,600
you can see that value has gone up 5

00:29:53,960 --> 00:29:58,080
645.

00:29:55,600 --> 00:29:59,600
so this is our streaming etl this is our

00:29:58,080 --> 00:30:02,799
series of events a series of

00:29:59,600 --> 00:30:04,080
facts arriving being enriched with our

00:30:02,799 --> 00:30:06,320
reference information

00:30:04,080 --> 00:30:07,520
written out to a new catholic topic and

00:30:06,320 --> 00:30:08,080
in the moment we can take that kafka

00:30:07,520 --> 00:30:09,200
topic

00:30:08,080 --> 00:30:11,200
and go and put it into elasticsearch

00:30:09,200 --> 00:30:12,799
again so now

00:30:11,200 --> 00:30:15,200
the other thing we're going to do is

00:30:12,799 --> 00:30:17,279
we're going to create a second stream

00:30:15,200 --> 00:30:18,399
which is going to daisy chain off this

00:30:17,279 --> 00:30:19,600
existing one

00:30:18,399 --> 00:30:22,080
so now we're going to say create a

00:30:19,600 --> 00:30:24,000
stream of unhappy platinum customers

00:30:22,080 --> 00:30:24,880
so we know about who our customers are

00:30:24,000 --> 00:30:26,799
because we've got that customer

00:30:24,880 --> 00:30:29,919
information brought in from the database

00:30:26,799 --> 00:30:31,919
into a kafka topic we know the rating

00:30:29,919 --> 00:30:33,440
star rating was it one to five based on

00:30:31,919 --> 00:30:35,360
those rating events

00:30:33,440 --> 00:30:36,960
so you can say well do a join between

00:30:35,360 --> 00:30:38,080
our stream of events and our reference

00:30:36,960 --> 00:30:40,320
information

00:30:38,080 --> 00:30:42,080
if the person who left a bad rating the

00:30:40,320 --> 00:30:43,919
star rating was less than three

00:30:42,080 --> 00:30:45,760
and they're a platinum customer then

00:30:43,919 --> 00:30:47,039
write it out to a new stream which is a

00:30:45,760 --> 00:30:48,960
new kafka topic

00:30:47,039 --> 00:30:51,120
so here's our logic we say here's our

00:30:48,960 --> 00:30:54,320
predicate where stars is lesson three

00:30:51,120 --> 00:30:55,679
and club status is platinum and we've

00:30:54,320 --> 00:30:57,200
gone and created another one

00:30:55,679 --> 00:30:59,600
show streams we've got another stream

00:30:57,200 --> 00:31:02,840
called unhappy platinum customers

00:30:59,600 --> 00:31:05,919
and now we go and put it all into

00:31:02,840 --> 00:31:07,279
elasticsearch so create a sync connector

00:31:05,919 --> 00:31:09,279
same as before but this time we're

00:31:07,279 --> 00:31:10,000
saying putting the data from these two

00:31:09,279 --> 00:31:11,919
topics here

00:31:10,000 --> 00:31:13,760
ratings enriched and unhappy platinum

00:31:11,919 --> 00:31:14,559
customers so kafka connect you can

00:31:13,760 --> 00:31:17,200
specify

00:31:14,559 --> 00:31:18,399
multiple topics and that's created the

00:31:17,200 --> 00:31:21,200
connector

00:31:18,399 --> 00:31:22,720
so show connectors says it's running

00:31:21,200 --> 00:31:26,320
which means if we head over

00:31:22,720 --> 00:31:28,320
into elasticsearch

00:31:26,320 --> 00:31:29,360
that's going to show us the data as it's

00:31:28,320 --> 00:31:33,840
arriving from

00:31:29,360 --> 00:31:33,840
that source topic

00:31:34,960 --> 00:31:38,080
so you can see here we've got the um how

00:31:37,200 --> 00:31:39,600
many and uh

00:31:38,080 --> 00:31:41,279
unhappy platinum customers in the last

00:31:39,600 --> 00:31:42,080
15 minutes so obviously not doing a very

00:31:41,279 --> 00:31:43,600
good job

00:31:42,080 --> 00:31:45,679
that's a lot of unhappy customers in a

00:31:43,600 --> 00:31:46,720
15 minute period we're going to set it

00:31:45,679 --> 00:31:49,120
to refresh

00:31:46,720 --> 00:31:50,720
so these values are changing continually

00:31:49,120 --> 00:31:52,640
because new data is arriving

00:31:50,720 --> 00:31:54,000
continually and a one second refresh is

00:31:52,640 --> 00:31:54,799
probably not such a great idea for my

00:31:54,000 --> 00:31:56,799
cpu

00:31:54,799 --> 00:31:58,799
but it gives you this idea that you can

00:31:56,799 --> 00:31:59,600
actually do a real-time view of your

00:31:58,799 --> 00:32:02,799
data

00:31:59,600 --> 00:32:04,720
your enriched data as it passes through

00:32:02,799 --> 00:32:06,399
kafka you can break it down as you want

00:32:04,720 --> 00:32:07,760
to and see the different messages

00:32:06,399 --> 00:32:10,640
and all that kind of fun stuff that you

00:32:07,760 --> 00:32:10,640
do with kibana

00:32:10,960 --> 00:32:14,320
i want to show you one more thing and

00:32:13,840 --> 00:32:17,519
that's

00:32:14,320 --> 00:32:19,679
this concept of materialized views

00:32:17,519 --> 00:32:21,200
so far we've taken data from a catholic

00:32:19,679 --> 00:32:21,919
topic we've said well that data's got a

00:32:21,200 --> 00:32:24,640
schema

00:32:21,919 --> 00:32:25,440
we're gonna enrich that data we're gonna

00:32:24,640 --> 00:32:27,519
join it out

00:32:25,440 --> 00:32:28,880
to a separate set of information some

00:32:27,519 --> 00:32:29,919
reference information

00:32:28,880 --> 00:32:32,159
we're not going to go and put it into a

00:32:29,919 --> 00:32:33,679
target system and do lookups out to our

00:32:32,159 --> 00:32:34,799
source database to actually pull that

00:32:33,679 --> 00:32:36,399
source database

00:32:34,799 --> 00:32:38,240
into our kafka topic and keep that

00:32:36,399 --> 00:32:39,919
updated in real time

00:32:38,240 --> 00:32:42,080
we can enrich that data push it down to

00:32:39,919 --> 00:32:43,279
elasticsearch streaming etf

00:32:42,080 --> 00:32:45,120
here's our streaming pipeline that's

00:32:43,279 --> 00:32:47,760
what it says on the cam

00:32:45,120 --> 00:32:49,519
but we're also going to see how k sql db

00:32:47,760 --> 00:32:52,399
can build these materialized views

00:32:49,519 --> 00:32:53,519
that we can query so let's close that

00:32:52,399 --> 00:32:54,799
there

00:32:53,519 --> 00:32:56,559
and give ourselves a clear screen to

00:32:54,799 --> 00:32:58,000
work from and what we're going to do now

00:32:56,559 --> 00:33:00,880
is we're going to build ourselves

00:32:58,000 --> 00:33:01,840
an aggregate so if you think about an

00:33:00,880 --> 00:33:04,320
aggregate

00:33:01,840 --> 00:33:05,679
and aggregate is well here's a good

00:33:04,320 --> 00:33:06,080
question a little exercise for you at

00:33:05,679 --> 00:33:09,440
home

00:33:06,080 --> 00:33:10,720
is it a stream or is it a table and i'll

00:33:09,440 --> 00:33:13,440
start typing it and you'll see

00:33:10,720 --> 00:33:14,559
straight away what it is it's a table

00:33:13,440 --> 00:33:17,760
because

00:33:14,559 --> 00:33:19,600
it's the value for a given key

00:33:17,760 --> 00:33:20,960
in this case at a point in time because

00:33:19,600 --> 00:33:24,159
we're doing a windowed

00:33:20,960 --> 00:33:26,960
aggregate as new information arrives

00:33:24,159 --> 00:33:27,440
that aggregate updates so for that given

00:33:26,960 --> 00:33:29,440
key

00:33:27,440 --> 00:33:31,039
the value is changing we're doing a

00:33:29,440 --> 00:33:32,000
count now we're doing a collect list if

00:33:31,039 --> 00:33:33,679
we were doing a sum

00:33:32,000 --> 00:33:35,039
the value would be going up or down

00:33:33,679 --> 00:33:37,039
depending on the

00:33:35,039 --> 00:33:39,120
inbound data but it's not just a

00:33:37,039 --> 00:33:41,360
continual series of events

00:33:39,120 --> 00:33:43,120
so it's a table so we create ourselves a

00:33:41,360 --> 00:33:45,919
table it's going to be ratings

00:33:43,120 --> 00:33:47,440
per customer per 15-minute window so

00:33:45,919 --> 00:33:49,519
select the customer name

00:33:47,440 --> 00:33:51,440
how many ratings have they left do a

00:33:49,519 --> 00:33:53,600
collect list against the star rating so

00:33:51,440 --> 00:33:56,880
every single star rating gets appended

00:33:53,600 --> 00:33:57,279
to this list pull it in from our source

00:33:56,880 --> 00:33:58,640
stream

00:33:57,279 --> 00:34:00,720
that we created before so you see this

00:33:58,640 --> 00:34:02,240
idea of daisy chaining them so you build

00:34:00,720 --> 00:34:03,279
a source a main one that's gonna be

00:34:02,240 --> 00:34:04,240
useful for lots of different

00:34:03,279 --> 00:34:06,000
applications

00:34:04,240 --> 00:34:07,919
and then you build on top of it for

00:34:06,000 --> 00:34:10,159
additional functionality

00:34:07,919 --> 00:34:12,320
we're doing a tumbling window so every

00:34:10,159 --> 00:34:14,000
15 minutes we get a new window

00:34:12,320 --> 00:34:15,599
you don't have to have a tumbling window

00:34:14,000 --> 00:34:17,040
you could just do an aggregation over

00:34:15,599 --> 00:34:18,720
all the data you've got

00:34:17,040 --> 00:34:20,800
but if we start thinking about unbounded

00:34:18,720 --> 00:34:22,879
data and the concept of aggregating

00:34:20,800 --> 00:34:24,079
all of it it starts to make less sense

00:34:22,879 --> 00:34:25,440
most of the time we're asking the kind

00:34:24,079 --> 00:34:27,520
of question of our data

00:34:25,440 --> 00:34:29,040
how many things have happened in the

00:34:27,520 --> 00:34:31,919
last minute

00:34:29,040 --> 00:34:32,960
hour day week something but it involves

00:34:31,919 --> 00:34:34,240
time

00:34:32,960 --> 00:34:36,480
but you don't have to but we are doing

00:34:34,240 --> 00:34:38,159
here so it's a tumbling window you also

00:34:36,480 --> 00:34:38,960
have hopping windows and session windows

00:34:38,159 --> 00:34:40,800
if you want

00:34:38,960 --> 00:34:42,079
and we're grouping it by the customer

00:34:40,800 --> 00:34:44,480
name

00:34:42,079 --> 00:34:45,119
so we go and create the table and if i

00:34:44,480 --> 00:34:47,440
say show

00:34:45,119 --> 00:34:49,359
tables you'll see we've got two tables

00:34:47,440 --> 00:34:51,040
we've got our existing one our customers

00:34:49,359 --> 00:34:52,240
table which sits on top of that kafka

00:34:51,040 --> 00:34:54,639
topic there

00:34:52,240 --> 00:34:56,079
we've also got a table here which is

00:34:54,639 --> 00:34:58,480
also writing to

00:34:56,079 --> 00:35:00,160
a kafka topic so if we want we can

00:34:58,480 --> 00:35:02,320
simply use this to build

00:35:00,160 --> 00:35:04,400
pre-calculated aggregations which are

00:35:02,320 --> 00:35:06,560
then streamed down to a target system

00:35:04,400 --> 00:35:08,240
with any kind of traditional analytics

00:35:06,560 --> 00:35:10,000
here's our aggregation instead of

00:35:08,240 --> 00:35:11,760
landing a bunch of data and doing batch

00:35:10,000 --> 00:35:13,280
jobs to build those aggregates

00:35:11,760 --> 00:35:15,440
you can build those aggregates on the

00:35:13,280 --> 00:35:17,040
fly as the data's arriving

00:35:15,440 --> 00:35:18,640
and push that down to a target system so

00:35:17,040 --> 00:35:19,680
it's continually updated

00:35:18,640 --> 00:35:21,280
but we're not going to do that here

00:35:19,680 --> 00:35:23,440
we're actually going to take that data

00:35:21,280 --> 00:35:25,920
and we're going to query it

00:35:23,440 --> 00:35:27,200
so the same query that i've been showing

00:35:25,920 --> 00:35:29,760
you all along

00:35:27,200 --> 00:35:30,880
is what's known as a push query and i've

00:35:29,760 --> 00:35:32,240
pointed at the screen and i've said

00:35:30,880 --> 00:35:35,040
notice how it says control c

00:35:32,240 --> 00:35:36,240
to cancel because it's a continuous

00:35:35,040 --> 00:35:39,280
unbounded query

00:35:36,240 --> 00:35:40,480
data arriving in kafka is unbounded so

00:35:39,280 --> 00:35:41,920
the queries are

00:35:40,480 --> 00:35:43,440
unbounded we don't know if we've

00:35:41,920 --> 00:35:44,560
finished reading all the data so we just

00:35:43,440 --> 00:35:46,000
keep on running

00:35:44,560 --> 00:35:47,599
and maybe the data will be arriving

00:35:46,000 --> 00:35:48,400
continually maybe there won't be any

00:35:47,599 --> 00:35:50,720
data for

00:35:48,400 --> 00:35:52,240
a minute an hour but it's unbounded we

00:35:50,720 --> 00:35:54,079
don't know

00:35:52,240 --> 00:35:55,359
so this is what we get if we query the

00:35:54,079 --> 00:35:58,320
database table

00:35:55,359 --> 00:35:59,359
it says here are the time windows and

00:35:58,320 --> 00:36:02,320
press control g

00:35:59,359 --> 00:36:03,920
control c to interrupt it's a push query

00:36:02,320 --> 00:36:05,440
as the new data arrives

00:36:03,920 --> 00:36:07,280
then we will see that echoed out to the

00:36:05,440 --> 00:36:09,040
screen or the aggregate will update

00:36:07,280 --> 00:36:11,280
and refresh itself on the screen so we

00:36:09,040 --> 00:36:12,079
saw that here's our time window from

00:36:11,280 --> 00:36:15,839
00:36:12,079 --> 00:36:18,320
uh up to 19 14 59

00:36:15,839 --> 00:36:19,520
and as these new events are arriving the

00:36:18,320 --> 00:36:21,200
count is going up

00:36:19,520 --> 00:36:23,280
so we've got a new event arriving it's

00:36:21,200 --> 00:36:24,160
getting upended to this collectivist

00:36:23,280 --> 00:36:26,320
here

00:36:24,160 --> 00:36:28,320
but the state is changing and so it's

00:36:26,320 --> 00:36:29,599
getting emitted out of the screen here

00:36:28,320 --> 00:36:31,280
we can subscribe to that from our

00:36:29,599 --> 00:36:31,760
applications if we want to and subscribe

00:36:31,280 --> 00:36:33,359
to

00:36:31,760 --> 00:36:35,680
that series of events and we can push

00:36:33,359 --> 00:36:38,560
that down to a target system and update

00:36:35,680 --> 00:36:41,200
aggregates in place on a target system

00:36:38,560 --> 00:36:41,839
but what about if we want to know here

00:36:41,200 --> 00:36:44,000
and now

00:36:41,839 --> 00:36:45,680
what's the value we just want to do a

00:36:44,000 --> 00:36:47,040
look up we just want to query it

00:36:45,680 --> 00:36:49,599
we want to write an application which

00:36:47,040 --> 00:36:51,359
says rika's saying or someone's saying

00:36:49,599 --> 00:36:52,800
this customer here rico blaisdell how

00:36:51,359 --> 00:36:53,359
many reviews has she left i don't want

00:36:52,800 --> 00:36:54,880
to know like

00:36:53,359 --> 00:36:56,880
a continual update i just want to know

00:36:54,880 --> 00:36:59,359
here and now how many is she left

00:36:56,880 --> 00:37:00,960
in the last 15 minutes well in that case

00:36:59,359 --> 00:37:03,599
we use what's called a pull

00:37:00,960 --> 00:37:04,160
query so a poll query is very very

00:37:03,599 --> 00:37:07,200
similar

00:37:04,160 --> 00:37:09,839
to a push query except we don't say

00:37:07,200 --> 00:37:11,200
emit changes we just write a query that

00:37:09,839 --> 00:37:12,079
looks like we're talking to a relational

00:37:11,200 --> 00:37:14,800
database

00:37:12,079 --> 00:37:15,760
select these values from this place with

00:37:14,800 --> 00:37:18,160
this predicate

00:37:15,760 --> 00:37:19,440
my full name's rica blaisdell and times

00:37:18,160 --> 00:37:20,880
sum let's see if i can get this right

00:37:19,440 --> 00:37:23,440
with my time zones

00:37:20,880 --> 00:37:25,359
it's the 20th of october and it was

00:37:23,440 --> 00:37:28,640
sometime after

00:37:25,359 --> 00:37:31,200
1900 so let's go for this one

00:37:28,640 --> 00:37:33,040
and it says here is the current value

00:37:31,200 --> 00:37:34,400
for that time window and we could dial

00:37:33,040 --> 00:37:35,119
the time back a little bit so just show

00:37:34,400 --> 00:37:37,119
me the

00:37:35,119 --> 00:37:38,960
previous two time windows it says there

00:37:37,119 --> 00:37:39,520
are two time windows but this is the

00:37:38,960 --> 00:37:42,960
current

00:37:39,520 --> 00:37:44,400
value for that aggregation so this is

00:37:42,960 --> 00:37:46,720
our materialized view

00:37:44,400 --> 00:37:48,079
concept so key sequel db i've not talked

00:37:46,720 --> 00:37:49,200
about the guts of it there are separate

00:37:48,079 --> 00:37:50,400
talks which i'll

00:37:49,200 --> 00:37:52,079
tweet and if there's somewhere else to

00:37:50,400 --> 00:37:54,000
share them i will also share them there

00:37:52,079 --> 00:37:54,480
there's like introduction to k-sql db

00:37:54,000 --> 00:37:56,880
and

00:37:54,480 --> 00:37:58,480
talk about how key sql db is built on

00:37:56,880 --> 00:38:00,160
top of kafka streams which is

00:37:58,480 --> 00:38:02,720
stream processing library is part of

00:38:00,160 --> 00:38:04,320
apache kafka and because all of this

00:38:02,720 --> 00:38:05,680
it's scalable it's distributed it's

00:38:04,320 --> 00:38:06,560
fault tolerant it does all of these

00:38:05,680 --> 00:38:08,480
things

00:38:06,560 --> 00:38:09,920
and it means that we can take data in a

00:38:08,480 --> 00:38:12,480
kafka topic

00:38:09,920 --> 00:38:14,400
and build a stateful aggregation that's

00:38:12,480 --> 00:38:16,079
maintained by k sql db

00:38:14,400 --> 00:38:18,400
that's backed by a kafka topic that we

00:38:16,079 --> 00:38:21,119
can push down to a target system

00:38:18,400 --> 00:38:22,800
but that we can also query directly so

00:38:21,119 --> 00:38:25,200
we can query it directly here and say

00:38:22,800 --> 00:38:26,800
well has that value changed we can see

00:38:25,200 --> 00:38:27,599
that value has changed if we query it

00:38:26,800 --> 00:38:28,960
again

00:38:27,599 --> 00:38:31,200
at some point it will probably change

00:38:28,960 --> 00:38:33,520
again if i'm sat here doing this

00:38:31,200 --> 00:38:34,240
because it went to 61 ratings then i

00:38:33,520 --> 00:38:35,839
should be using

00:38:34,240 --> 00:38:37,440
a push query because i'm saying well

00:38:35,839 --> 00:38:38,720
obviously i want to know if that's

00:38:37,440 --> 00:38:40,960
changed so then i want

00:38:38,720 --> 00:38:41,760
k sequel db to push to me when it

00:38:40,960 --> 00:38:42,880
changes

00:38:41,760 --> 00:38:44,560
but actually i just want to know what's

00:38:42,880 --> 00:38:45,599
the current value and that's fine that's

00:38:44,560 --> 00:38:47,680
answered my question

00:38:45,599 --> 00:38:49,040
to drive my application or whatever i'm

00:38:47,680 --> 00:38:52,720
building

00:38:49,040 --> 00:38:54,400
and with kc called db it has a rest api

00:38:52,720 --> 00:38:56,320
so on top of that rest api there's a

00:38:54,400 --> 00:38:58,800
java client there's community

00:38:56,320 --> 00:39:00,000
i think python and go clients as well

00:38:58,800 --> 00:39:02,320
which means that you can write your

00:39:00,000 --> 00:39:03,359
applications querying this state

00:39:02,320 --> 00:39:05,119
directly

00:39:03,359 --> 00:39:06,960
so let me show you this before i finish

00:39:05,119 --> 00:39:09,119
up and show you a couple of links

00:39:06,960 --> 00:39:10,800
we're going to go into kc called db uh

00:39:09,119 --> 00:39:11,920
into it's right it's already in a docker

00:39:10,800 --> 00:39:13,839
container

00:39:11,920 --> 00:39:15,599
and we're going to say let's set this

00:39:13,839 --> 00:39:18,320
environment value here

00:39:15,599 --> 00:39:20,240
and then we're going to run a rest call

00:39:18,320 --> 00:39:22,400
against it

00:39:20,240 --> 00:39:25,280
so here we're going against the select

00:39:22,400 --> 00:39:27,280
the query endpoint 4k sql db

00:39:25,280 --> 00:39:29,280
we're running that same select query as

00:39:27,280 --> 00:39:31,200
before select these values

00:39:29,280 --> 00:39:32,400
for this particular user and the windows

00:39:31,200 --> 00:39:34,960
start date we've set so

00:39:32,400 --> 00:39:36,079
five minutes or 15 minutes ago and it

00:39:34,960 --> 00:39:38,079
says here you go

00:39:36,079 --> 00:39:40,000
here's the user it's ruka blaisdell

00:39:38,079 --> 00:39:41,520
they've been 75 different reviews

00:39:40,000 --> 00:39:44,240
this is what all those different reviews

00:39:41,520 --> 00:39:45,359
were this is not so well illustrated on

00:39:44,240 --> 00:39:47,680
a bash command line

00:39:45,359 --> 00:39:49,680
other than i've run curl to go and look

00:39:47,680 --> 00:39:53,359
up a value for a given key

00:39:49,680 --> 00:39:56,320
against this materialized view store

00:39:53,359 --> 00:39:57,040
excuse me so that's all i'd like to show

00:39:56,320 --> 00:40:00,720
you today

00:39:57,040 --> 00:40:04,000
i have a final useful slide to show you

00:40:00,720 --> 00:40:05,359
so developer.confluence.io is where you

00:40:04,000 --> 00:40:08,640
can go and get tons of

00:40:05,359 --> 00:40:10,000
tutorials and podcasts and video links

00:40:08,640 --> 00:40:12,560
and all sorts of stuff

00:40:10,000 --> 00:40:14,079
to go and learn more about kafka so from

00:40:12,560 --> 00:40:15,280
there you can get like discount codes

00:40:14,079 --> 00:40:17,359
for confluent clouds

00:40:15,280 --> 00:40:19,520
you can go find these demos head over to

00:40:17,359 --> 00:40:21,040
twitter at i'm off follow me on there

00:40:19,520 --> 00:40:22,880
and follow me for the links i'll tweet

00:40:21,040 --> 00:40:24,079
links out to this talk out to the demo

00:40:22,880 --> 00:40:25,680
and all the rest of it

00:40:24,079 --> 00:40:26,960
and with that thank you very much for

00:40:25,680 --> 00:40:27,520
everyone's time this evening this

00:40:26,960 --> 00:40:35,359
morning

00:40:27,520 --> 00:40:35,359

YouTube URL: https://www.youtube.com/watch?v=j6_JnPr_m_s


