Title: Open Source for Data Quality - Jackie Goldschmidt
Publication date: 2020-12-11
Playlist: All Things Open 2020 - Big Data Track
Description: 
	Presented by: Jackie Goldschmidt, Tech co.
Presented at All Things Open 2020 - Big Data Track

Abstract: You've probably heard the expression, "Garbage in, garbage out." The truth is that engineers and data scientists often work with imperfect data to create business value. During this talk we'll explore open source projects and methodologies you can use to build valuable data pipelines with real world data. We'll cover data anonymization, pipeline testing and automated validation. We will focus on action items you can bring to your future or current projects.
Captions: 
	00:00:05,440 --> 00:00:08,639
okay

00:00:06,399 --> 00:00:09,519
all right i'll go ahead and get started

00:00:08,639 --> 00:00:13,280
i know that

00:00:09,519 --> 00:00:16,640
everyone's time is precious so

00:00:13,280 --> 00:00:19,039
um here we go open source data quality

00:00:16,640 --> 00:00:20,320
um so this is going to be a brief

00:00:19,039 --> 00:00:23,519
15-minute talk

00:00:20,320 --> 00:00:24,800
about open source methodologies and

00:00:23,519 --> 00:00:27,680
tools that you can use

00:00:24,800 --> 00:00:29,199
for data quality um we'll be

00:00:27,680 --> 00:00:31,679
specifically focusing

00:00:29,199 --> 00:00:33,920
on python tools and we'll be talking

00:00:31,679 --> 00:00:36,000
about how you can build data pipelines

00:00:33,920 --> 00:00:37,120
even when you're working with real world

00:00:36,000 --> 00:00:39,200
data that

00:00:37,120 --> 00:00:41,760
may be human generated and may be

00:00:39,200 --> 00:00:44,399
potentially messy

00:00:41,760 --> 00:00:45,920
so as i like to do when i start my talks

00:00:44,399 --> 00:00:49,280
i'm going to start with

00:00:45,920 --> 00:00:52,239
a cartoon and in this cartoon we have

00:00:49,280 --> 00:00:53,360
dilbert and his manager discussing uh

00:00:52,239 --> 00:00:55,840
data quality

00:00:53,360 --> 00:00:57,600
dilbert's manager says use the crs

00:00:55,840 --> 00:00:59,920
database to size the market

00:00:57,600 --> 00:01:01,039
and dilbert responds but that data is

00:00:59,920 --> 00:01:03,280
wrong

00:01:01,039 --> 00:01:04,320
his manager says then use another

00:01:03,280 --> 00:01:07,600
database

00:01:04,320 --> 00:01:10,560
and dilbert says that data is also wrong

00:01:07,600 --> 00:01:11,040
uh dilbert's manager has the great idea

00:01:10,560 --> 00:01:13,920
can you

00:01:11,040 --> 00:01:14,960
average the two databases and dilbert

00:01:13,920 --> 00:01:18,000
says sure

00:01:14,960 --> 00:01:20,080
i can multiply them too

00:01:18,000 --> 00:01:21,680
so what we're going to be talking about

00:01:20,080 --> 00:01:24,479
is how do we avoid

00:01:21,680 --> 00:01:26,000
getting into a situation where we are

00:01:24,479 --> 00:01:29,040
averaging or multiplying

00:01:26,000 --> 00:01:31,920
data to come up with an outcome um

00:01:29,040 --> 00:01:33,119
and particularly when we work with messy

00:01:31,920 --> 00:01:34,880
data

00:01:33,119 --> 00:01:36,479
how do we build data pipelines that we

00:01:34,880 --> 00:01:38,720
can trust

00:01:36,479 --> 00:01:40,560
so the kind of things that i'd like you

00:01:38,720 --> 00:01:42,479
to take away from this talk if there are

00:01:40,560 --> 00:01:45,119
only a few things you take away

00:01:42,479 --> 00:01:48,159
are that when you work with data

00:01:45,119 --> 00:01:50,240
pipelines using real world data

00:01:48,159 --> 00:01:51,680
first thing that you need to do is just

00:01:50,240 --> 00:01:54,479
get to know your data

00:01:51,680 --> 00:01:54,799
ask qualitative questions to understand

00:01:54,479 --> 00:01:57,200
and

00:01:54,799 --> 00:01:58,719
use tools like data profiling tools to

00:01:57,200 --> 00:02:01,759
drill into what your data set

00:01:58,719 --> 00:02:03,759
actually looks like second once you have

00:02:01,759 --> 00:02:05,759
a strong understanding of your data

00:02:03,759 --> 00:02:07,680
make sure that you are validating that

00:02:05,759 --> 00:02:10,560
data each step of the way

00:02:07,680 --> 00:02:11,920
in your data pipelines or your system

00:02:10,560 --> 00:02:14,800
approaches that we can take

00:02:11,920 --> 00:02:16,400
for validating our data are we can use

00:02:14,800 --> 00:02:18,720
api contracts

00:02:16,400 --> 00:02:19,760
validation tools and we can use data

00:02:18,720 --> 00:02:22,560
anonymization

00:02:19,760 --> 00:02:25,280
and fake data generation to create

00:02:22,560 --> 00:02:28,560
reliable tasks

00:02:25,280 --> 00:02:31,760
so first up getting to know your data

00:02:28,560 --> 00:02:34,080
before you even open up your ide before

00:02:31,760 --> 00:02:36,160
you start your jupyter notebook server

00:02:34,080 --> 00:02:37,440
ask yourself a couple of questions and

00:02:36,160 --> 00:02:40,400
get to know that data set

00:02:37,440 --> 00:02:42,080
qualitatively first question is just how

00:02:40,400 --> 00:02:45,200
was the data collected

00:02:42,080 --> 00:02:48,400
um is the data maybe a sample or

00:02:45,200 --> 00:02:50,720
was it collected by um people that were

00:02:48,400 --> 00:02:52,239
looking at documents and then typing out

00:02:50,720 --> 00:02:53,120
what was in that document into a

00:02:52,239 --> 00:02:55,280
computer

00:02:53,120 --> 00:02:58,640
was it maybe scanned and there could

00:02:55,280 --> 00:03:00,400
have been typos that were scanned in

00:02:58,640 --> 00:03:02,720
what what potential issues could come

00:03:00,400 --> 00:03:04,640
from that data collection

00:03:02,720 --> 00:03:06,239
also ask yourself it may be that you're

00:03:04,640 --> 00:03:08,800
working with a sample

00:03:06,239 --> 00:03:10,959
of data which is perfectly fine but in a

00:03:08,800 --> 00:03:11,599
lot of cases we don't have the luxury of

00:03:10,959 --> 00:03:14,480
working with

00:03:11,599 --> 00:03:16,239
a perfectly random sample that is

00:03:14,480 --> 00:03:17,280
representative necessarily of the

00:03:16,239 --> 00:03:19,440
population

00:03:17,280 --> 00:03:20,319
so if you are working with sample data

00:03:19,440 --> 00:03:23,200
is there bias

00:03:20,319 --> 00:03:23,920
inherent in that sample um also ask

00:03:23,200 --> 00:03:26,640
yourself

00:03:23,920 --> 00:03:27,360
was the data set transformed in any way

00:03:26,640 --> 00:03:29,840
between

00:03:27,360 --> 00:03:31,519
sort of when it was initially ingested

00:03:29,840 --> 00:03:33,280
and when it got its way to you

00:03:31,519 --> 00:03:35,280
so is it possible that there are nulls

00:03:33,280 --> 00:03:36,159
in this data set where there should not

00:03:35,280 --> 00:03:38,080
be

00:03:36,159 --> 00:03:40,239
is it possible that some values have

00:03:38,080 --> 00:03:44,159
been coalesced or coerced

00:03:40,239 --> 00:03:45,519
um or malformed in some way

00:03:44,159 --> 00:03:47,519
once you've gone through and you've done

00:03:45,519 --> 00:03:49,440
your data quality detective work

00:03:47,519 --> 00:03:51,440
the next step is kind of what i consider

00:03:49,440 --> 00:03:52,560
to be the more fun steps we get to

00:03:51,440 --> 00:03:54,400
actually start working with the

00:03:52,560 --> 00:03:55,760
technology and the tools

00:03:54,400 --> 00:03:57,599
the first set of tools that we're going

00:03:55,760 --> 00:04:00,000
to talk about are open source

00:03:57,599 --> 00:04:00,959
data profiling tools and a data

00:04:00,000 --> 00:04:02,799
profiling tool

00:04:00,959 --> 00:04:04,159
is just a tool that allows you to

00:04:02,799 --> 00:04:06,640
examine a data set

00:04:04,159 --> 00:04:07,599
and get some statistics and informative

00:04:06,640 --> 00:04:10,640
summaries

00:04:07,599 --> 00:04:12,560
about that data so

00:04:10,640 --> 00:04:14,239
my first recommendation for an open

00:04:12,560 --> 00:04:16,799
source tool that you can use

00:04:14,239 --> 00:04:18,639
for data quality is you can use jupyter

00:04:16,799 --> 00:04:19,919
notebooks which many of you are probably

00:04:18,639 --> 00:04:21,680
familiar with

00:04:19,919 --> 00:04:24,000
and you can use a tool called pandas

00:04:21,680 --> 00:04:26,960
profiling panos profiling

00:04:24,000 --> 00:04:28,720
is sort of an extension of the pandas

00:04:26,960 --> 00:04:31,199
tool which we many of us

00:04:28,720 --> 00:04:32,160
know and love it's a common python tool

00:04:31,199 --> 00:04:34,639
for working with

00:04:32,160 --> 00:04:35,280
data sets using something called a

00:04:34,639 --> 00:04:38,160
pandas

00:04:35,280 --> 00:04:38,720
data frame and with something as simple

00:04:38,160 --> 00:04:41,199
as just

00:04:38,720 --> 00:04:42,800
pip install pandas pip install pan is

00:04:41,199 --> 00:04:44,560
profiling and then

00:04:42,800 --> 00:04:46,479
literally these four lines that we have

00:04:44,560 --> 00:04:48,400
here importing

00:04:46,479 --> 00:04:50,160
reading in your data frame as a as a

00:04:48,400 --> 00:04:52,800
pandas data frame and then running

00:04:50,160 --> 00:04:53,680
data frame dot profile report you can

00:04:52,800 --> 00:04:55,919
get this

00:04:53,680 --> 00:04:57,600
pretty quick and easy overview into your

00:04:55,919 --> 00:04:59,600
data set

00:04:57,600 --> 00:05:01,280
i'll go ahead and just quickly show you

00:04:59,600 --> 00:05:01,759
what that looks like with some sample

00:05:01,280 --> 00:05:04,080
data

00:05:01,759 --> 00:05:05,120
from the census so what you can see is

00:05:04,080 --> 00:05:07,360
that just reading in

00:05:05,120 --> 00:05:09,440
that data frame if we look at what

00:05:07,360 --> 00:05:11,120
pandas profiling shows us

00:05:09,440 --> 00:05:13,440
it's going to show us things like what

00:05:11,120 --> 00:05:14,639
is our distribution of age in our census

00:05:13,440 --> 00:05:16,800
data set

00:05:14,639 --> 00:05:18,560
what is our distribution of work class

00:05:16,800 --> 00:05:20,240
and education

00:05:18,560 --> 00:05:22,160
and it will also show us other things

00:05:20,240 --> 00:05:23,440
like the interactions between our

00:05:22,160 --> 00:05:26,240
different variables

00:05:23,440 --> 00:05:26,800
the correlations that we see any missing

00:05:26,240 --> 00:05:29,759
data

00:05:26,800 --> 00:05:31,600
in our data set and a sample of the

00:05:29,759 --> 00:05:35,600
first rows and the last rows

00:05:31,600 --> 00:05:38,240
in the data set so going back

00:05:35,600 --> 00:05:40,240
to panas profiling downsides of panas

00:05:38,240 --> 00:05:42,400
profiling are that you should have some

00:05:40,240 --> 00:05:44,000
knowledge of python and pandas

00:05:42,400 --> 00:05:46,720
but if you don't have that yet i would

00:05:44,000 --> 00:05:48,960
highly recommend investing in

00:05:46,720 --> 00:05:51,440
learning about python and pandas as

00:05:48,960 --> 00:05:53,759
tools for working with data

00:05:51,440 --> 00:05:55,280
the next open source data quality the

00:05:53,759 --> 00:05:57,680
tool that we'll discuss today is called

00:05:55,280 --> 00:05:58,960
apache superset it's also a profiling

00:05:57,680 --> 00:06:00,880
tool

00:05:58,960 --> 00:06:02,479
and i have to say that doing some

00:06:00,880 --> 00:06:04,880
research i was very impressed with

00:06:02,479 --> 00:06:07,199
apache superset as a tool

00:06:04,880 --> 00:06:08,880
first their documentation is excellent

00:06:07,199 --> 00:06:10,639
and second for people that

00:06:08,880 --> 00:06:12,800
may not be very far along in their

00:06:10,639 --> 00:06:15,199
programming careers or people that

00:06:12,800 --> 00:06:16,960
you know they just kind of want to be

00:06:15,199 --> 00:06:17,840
able to do things easily without writing

00:06:16,960 --> 00:06:20,000
a lot of code

00:06:17,840 --> 00:06:22,000
it's considered a low code tool where

00:06:20,000 --> 00:06:23,919
it's very similar to like a tableau

00:06:22,000 --> 00:06:25,600
or a spot fire except that it's open

00:06:23,919 --> 00:06:27,199
source and can run locally on your

00:06:25,600 --> 00:06:28,560
computer

00:06:27,199 --> 00:06:30,080
and it gives you a huge variety of

00:06:28,560 --> 00:06:31,759
visualizations that you can make where

00:06:30,080 --> 00:06:32,560
you can really drill in and explore your

00:06:31,759 --> 00:06:34,400
data

00:06:32,560 --> 00:06:36,240
so here's a screenshot of what for

00:06:34,400 --> 00:06:39,440
example you could make as an apache

00:06:36,240 --> 00:06:42,160
superset dashboard on a certain data set

00:06:39,440 --> 00:06:43,680
downsides to apache superset are that if

00:06:42,160 --> 00:06:45,360
you are a windows user you'll have to

00:06:43,680 --> 00:06:48,000
use virtualbox because it's not

00:06:45,360 --> 00:06:50,080
officially supported for windows users

00:06:48,000 --> 00:06:51,440
um and you all will need to set up

00:06:50,080 --> 00:06:54,160
docker on your computer

00:06:51,440 --> 00:06:55,840
and um get this uh it's basically a

00:06:54,160 --> 00:06:58,880
flask application that will run inside

00:06:55,840 --> 00:06:58,880
of a docker container

00:06:59,039 --> 00:07:02,400
all right so next now that we've taken

00:07:01,360 --> 00:07:04,319
care of

00:07:02,400 --> 00:07:06,160
doing our detective work we've taken

00:07:04,319 --> 00:07:08,800
care of data profiling

00:07:06,160 --> 00:07:10,479
let's talk about data validation and we

00:07:08,800 --> 00:07:12,639
have another dilbert cartoon

00:07:10,479 --> 00:07:14,160
where dilbert's manager says are you

00:07:12,639 --> 00:07:15,039
sure that the data you gave me is

00:07:14,160 --> 00:07:18,080
correct

00:07:15,039 --> 00:07:20,240
and deliberate responds to him i've been

00:07:18,080 --> 00:07:22,400
giving you incorrect data for years

00:07:20,240 --> 00:07:23,280
this is the first time you've asked his

00:07:22,400 --> 00:07:26,560
manager says

00:07:23,280 --> 00:07:28,840
what and dilbert says i said the data is

00:07:26,560 --> 00:07:30,560
totally

00:07:28,840 --> 00:07:32,319
accurate

00:07:30,560 --> 00:07:34,080
so for the first thing that we're going

00:07:32,319 --> 00:07:37,120
to talk about in terms of

00:07:34,080 --> 00:07:39,840
validation before you can reasonably do

00:07:37,120 --> 00:07:41,280
a validation on the data in your data

00:07:39,840 --> 00:07:42,000
pipelines or the system that you're

00:07:41,280 --> 00:07:43,840
building

00:07:42,000 --> 00:07:45,199
you need to take some time and actually

00:07:43,840 --> 00:07:47,840
define what

00:07:45,199 --> 00:07:49,120
valid data is for those pipelines or for

00:07:47,840 --> 00:07:51,280
those systems

00:07:49,120 --> 00:07:53,520
um a concept that can be very helpful

00:07:51,280 --> 00:07:56,160
for this is called design by contract

00:07:53,520 --> 00:07:57,120
which is basically a theory that says

00:07:56,160 --> 00:08:00,000
when we build

00:07:57,120 --> 00:08:02,560
out a software element a crucial part of

00:08:00,000 --> 00:08:04,560
that is figuring out how that software

00:08:02,560 --> 00:08:06,160
interacts with the world around it what

00:08:04,560 --> 00:08:09,440
are the inputs that it accepts

00:08:06,160 --> 00:08:11,440
what are the outputs that it will return

00:08:09,440 --> 00:08:13,520
and in terms of open source

00:08:11,440 --> 00:08:15,199
methodologies and tools that we can use

00:08:13,520 --> 00:08:17,360
for design by contract

00:08:15,199 --> 00:08:19,039
when you're thinking about what valid

00:08:17,360 --> 00:08:20,080
data looks like as it flows through your

00:08:19,039 --> 00:08:21,680
data pipeline

00:08:20,080 --> 00:08:23,440
you can use something called the open

00:08:21,680 --> 00:08:25,919
api specification

00:08:23,440 --> 00:08:28,000
in order to define what your service

00:08:25,919 --> 00:08:30,160
expects as valid data inputting

00:08:28,000 --> 00:08:33,440
into each step of the pipeline and what

00:08:30,160 --> 00:08:33,440
your service will return

00:08:33,599 --> 00:08:37,839
open api specification tool that you can

00:08:35,839 --> 00:08:41,120
use is called swagger

00:08:37,839 --> 00:08:43,279
swagger is excellent for rest apis

00:08:41,120 --> 00:08:45,600
it defines all kinds of things from

00:08:43,279 --> 00:08:48,399
authentication to endpoints

00:08:45,600 --> 00:08:50,480
to like the collections in your rest api

00:08:48,399 --> 00:08:52,640
however it is rest specific

00:08:50,480 --> 00:08:54,640
and it requires maintenance so when

00:08:52,640 --> 00:08:56,320
you're working with a rest

00:08:54,640 --> 00:08:58,399
api you need to make sure that you keep

00:08:56,320 --> 00:08:58,800
that contract up to date if you do end

00:08:58,399 --> 00:09:00,959
up

00:08:58,800 --> 00:09:02,800
changing what you consider to be valid

00:09:00,959 --> 00:09:04,720
data or what you're going to return in

00:09:02,800 --> 00:09:07,680
your data pipeline

00:09:04,720 --> 00:09:08,399
um next i'm going to talk about async

00:09:07,680 --> 00:09:10,320
api

00:09:08,399 --> 00:09:12,399
which is also an open source tool that

00:09:10,320 --> 00:09:14,720
you can use it's not actually for

00:09:12,399 --> 00:09:17,440
defining rest apis it's for

00:09:14,720 --> 00:09:19,200
defining asynchronous apis so for

00:09:17,440 --> 00:09:21,920
example if you have a

00:09:19,200 --> 00:09:23,360
kafka service that is accepting a

00:09:21,920 --> 00:09:25,279
certain input of data

00:09:23,360 --> 00:09:26,800
and returning or like writing to another

00:09:25,279 --> 00:09:30,240
topic certain output

00:09:26,800 --> 00:09:31,120
you can um describe that inside of async

00:09:30,240 --> 00:09:32,880
api

00:09:31,120 --> 00:09:34,160
it's a pretty new project a lot more new

00:09:32,880 --> 00:09:35,920
than swagger

00:09:34,160 --> 00:09:38,080
but i think it's mostly useful as a

00:09:35,920 --> 00:09:40,720
pattern for how you can think through

00:09:38,080 --> 00:09:42,320
when i'm building out my data pipelines

00:09:40,720 --> 00:09:44,480
what exactly do i expect

00:09:42,320 --> 00:09:45,600
and really start to just explore what

00:09:44,480 --> 00:09:47,760
the assumptions are that

00:09:45,600 --> 00:09:49,120
you're making about the data because the

00:09:47,760 --> 00:09:50,720
less assumptions you make

00:09:49,120 --> 00:09:52,000
the tighter your data pipelines and the

00:09:50,720 --> 00:09:54,880
higher the quality of the data that

00:09:52,000 --> 00:09:56,880
you're going to have moving through them

00:09:54,880 --> 00:09:57,920
validation tools are what we will

00:09:56,880 --> 00:10:00,880
discuss now

00:09:57,920 --> 00:10:01,839
and if your contract is defining what is

00:10:00,880 --> 00:10:03,519
valid data

00:10:01,839 --> 00:10:05,120
your validation tools are really what's

00:10:03,519 --> 00:10:08,399
going to make sure

00:10:05,120 --> 00:10:10,160
that in code you are asserting

00:10:08,399 --> 00:10:11,839
that you're getting valid data every

00:10:10,160 --> 00:10:13,600
step of the way

00:10:11,839 --> 00:10:15,040
so the first validation tool that we'll

00:10:13,600 --> 00:10:18,160
discuss is called

00:10:15,040 --> 00:10:20,720
cerberus in greek mythology cerberus

00:10:18,160 --> 00:10:21,519
is the dog that guards the gates of

00:10:20,720 --> 00:10:25,120
hades

00:10:21,519 --> 00:10:26,640
um in terms of data pipelines with messy

00:10:25,120 --> 00:10:28,560
real-world data

00:10:26,640 --> 00:10:30,720
you can think of cerberus as the tool

00:10:28,560 --> 00:10:32,320
that is guarding your data pipeline it's

00:10:30,720 --> 00:10:32,959
going to make sure that when you receive

00:10:32,320 --> 00:10:35,360
data

00:10:32,959 --> 00:10:36,320
that data is actually valid data before

00:10:35,360 --> 00:10:38,480
the data moves

00:10:36,320 --> 00:10:40,399
further into your system or your

00:10:38,480 --> 00:10:42,399
pipelines

00:10:40,399 --> 00:10:44,959
sort of pros for cerberus is that the

00:10:42,399 --> 00:10:48,160
the documentation is excellent on it

00:10:44,959 --> 00:10:50,880
it also has a lot of variety for how it

00:10:48,160 --> 00:10:52,640
supports nested fields and dependencies

00:10:50,880 --> 00:10:54,079
um and it's sort of like a swiss army

00:10:52,640 --> 00:10:57,360
knife that you can use

00:10:54,079 --> 00:10:57,920
um to to define this is exactly what i

00:10:57,360 --> 00:11:01,440
expect

00:10:57,920 --> 00:11:03,120
as my schema for my data um

00:11:01,440 --> 00:11:05,120
another tool that you can use that's

00:11:03,120 --> 00:11:07,440
also open source all of these are

00:11:05,120 --> 00:11:09,680
is you can use json schema um json

00:11:07,440 --> 00:11:11,360
schema is a little bit more popular than

00:11:09,680 --> 00:11:14,160
cerberus and from what i can see looking

00:11:11,360 --> 00:11:16,480
at their github is more active on github

00:11:14,160 --> 00:11:18,320
although they're both pretty active um

00:11:16,480 --> 00:11:21,279
the huge benefit to json schema

00:11:18,320 --> 00:11:21,839
is that rather than defining your like

00:11:21,279 --> 00:11:24,399
data

00:11:21,839 --> 00:11:26,640
schemas in code you're really defining

00:11:24,399 --> 00:11:27,760
those data schemas inside of this like

00:11:26,640 --> 00:11:30,640
highly readable

00:11:27,760 --> 00:11:32,880
json so here we can see that from json

00:11:30,640 --> 00:11:35,360
schema we're importing validate

00:11:32,880 --> 00:11:36,240
we define the schema using a python

00:11:35,360 --> 00:11:40,320
dictionary

00:11:36,240 --> 00:11:42,160
which is adjacent to a json

00:11:40,320 --> 00:11:43,760
and then we just run this validate call

00:11:42,160 --> 00:11:45,680
and if something is not

00:11:43,760 --> 00:11:47,680
valid as per the schema that we've

00:11:45,680 --> 00:11:48,959
defined we're going to get a helpful

00:11:47,680 --> 00:11:50,720
error message that says

00:11:48,959 --> 00:11:54,320
hey this piece of data that you've sent

00:11:50,720 --> 00:11:54,320
in this isn't right

00:11:54,399 --> 00:11:57,680
um the next thing that we're going to

00:11:55,680 --> 00:11:58,639
talk about in terms of tools that you

00:11:57,680 --> 00:12:01,120
can use

00:11:58,639 --> 00:12:02,240
for building real world real world

00:12:01,120 --> 00:12:05,920
pipelines

00:12:02,240 --> 00:12:08,720
um with data is data anonymization

00:12:05,920 --> 00:12:10,320
and generating fake data tools so

00:12:08,720 --> 00:12:12,079
initially when i started working on this

00:12:10,320 --> 00:12:14,720
talk i looked into

00:12:12,079 --> 00:12:16,399
open source data anonymization tools

00:12:14,720 --> 00:12:18,720
what i found is that there really

00:12:16,399 --> 00:12:19,600
isn't an outstanding tool on the market

00:12:18,720 --> 00:12:21,839
in my opinion

00:12:19,600 --> 00:12:22,959
for data anonymization that is open

00:12:21,839 --> 00:12:25,040
source

00:12:22,959 --> 00:12:26,160
and my hypothesis for that is that

00:12:25,040 --> 00:12:27,519
people who have

00:12:26,160 --> 00:12:29,600
sensitive data that they want to

00:12:27,519 --> 00:12:31,680
anonymize may be

00:12:29,600 --> 00:12:33,440
reluctant to hand that data over to an

00:12:31,680 --> 00:12:34,639
open source tool

00:12:33,440 --> 00:12:36,800
however if you're an open source

00:12:34,639 --> 00:12:39,040
developer this could be an opportunity

00:12:36,800 --> 00:12:40,959
if you can assure others of security

00:12:39,040 --> 00:12:42,800
um what i ended up looking into instead

00:12:40,959 --> 00:12:43,519
of data anonymization tools that were

00:12:42,800 --> 00:12:45,440
open source

00:12:43,519 --> 00:12:47,600
i ended up looking into fake data

00:12:45,440 --> 00:12:48,959
generators because the real purpose of

00:12:47,600 --> 00:12:51,519
what we're trying to do here

00:12:48,959 --> 00:12:52,399
is now that we've defined valid data and

00:12:51,519 --> 00:12:54,160
we've

00:12:52,399 --> 00:12:55,920
assured ourselves that we have ways to

00:12:54,160 --> 00:12:57,839
validate that data as it moves through

00:12:55,920 --> 00:12:59,120
our data pipelines

00:12:57,839 --> 00:13:01,360
the next step is that we need to write

00:12:59,120 --> 00:13:02,160
our tests and we can use fake data

00:13:01,360 --> 00:13:03,920
generators

00:13:02,160 --> 00:13:06,240
to write tests that make sure that our

00:13:03,920 --> 00:13:07,600
pipelines are performant meaning that

00:13:06,240 --> 00:13:09,760
they won't tip over

00:13:07,600 --> 00:13:11,360
when they're exposed to stress and that

00:13:09,760 --> 00:13:14,639
they're fast even when they

00:13:11,360 --> 00:13:18,079
have a lot of load on them we can also

00:13:14,639 --> 00:13:19,680
use uh fake data generators to

00:13:18,079 --> 00:13:21,279
write tests that show that we handle

00:13:19,680 --> 00:13:21,920
duplicate data the way that we would

00:13:21,279 --> 00:13:24,240
expect

00:13:21,920 --> 00:13:25,600
so if a person fills out a form 10

00:13:24,240 --> 00:13:26,720
minutes later they submit what looks

00:13:25,600 --> 00:13:28,000
like the same form

00:13:26,720 --> 00:13:30,079
how do we want to handle that in our

00:13:28,000 --> 00:13:31,839
system do we want to create two records

00:13:30,079 --> 00:13:33,440
or one record

00:13:31,839 --> 00:13:36,000
we can use it to make sure that we're

00:13:33,440 --> 00:13:38,079
handling edge cases like for example we

00:13:36,000 --> 00:13:40,560
have a name that's written in non-ascii

00:13:38,079 --> 00:13:42,800
how do we want to handle that and that

00:13:40,560 --> 00:13:44,639
we're doing error handling if we do

00:13:42,800 --> 00:13:46,480
want something to throw an error in our

00:13:44,639 --> 00:13:48,320
data pipeline are we logging it the way

00:13:46,480 --> 00:13:50,160
that we expect are we alerting on it if

00:13:48,320 --> 00:13:51,519
necessary

00:13:50,160 --> 00:13:54,000
those are all the things that generating

00:13:51,519 --> 00:13:56,160
fake data can help you test

00:13:54,000 --> 00:13:57,440
so the first fake data generator that

00:13:56,160 --> 00:13:59,040
we'll discuss

00:13:57,440 --> 00:14:00,800
also open source like all of these is

00:13:59,040 --> 00:14:02,800
called faker

00:14:00,800 --> 00:14:04,880
pretty good name pretty straightforward

00:14:02,800 --> 00:14:05,760
it's really easy and it has a wide

00:14:04,880 --> 00:14:09,199
variety of

00:14:05,760 --> 00:14:10,560
fake data providers a variety of

00:14:09,199 --> 00:14:13,440
providers includes

00:14:10,560 --> 00:14:15,519
license plate numbers and phone numbers

00:14:13,440 --> 00:14:18,560
and i think social security numbers

00:14:15,519 --> 00:14:20,079
names etc it's multilingual so it

00:14:18,560 --> 00:14:22,639
supports many different languages

00:14:20,079 --> 00:14:23,440
other than english and it can guarantee

00:14:22,639 --> 00:14:25,279
fake value or

00:14:23,440 --> 00:14:27,279
it can guarantee unique values you can

00:14:25,279 --> 00:14:28,880
say hey i need a million unique names

00:14:27,279 --> 00:14:31,120
and it will give you that

00:14:28,880 --> 00:14:32,720
you can also customize providers you can

00:14:31,120 --> 00:14:35,920
inherit the existing providers

00:14:32,720 --> 00:14:38,480
and build on top of them and there is a

00:14:35,920 --> 00:14:40,800
repository now that's a pi test plugin

00:14:38,480 --> 00:14:44,560
for faker so you can easily integrate

00:14:40,800 --> 00:14:47,839
these fake data sets into your pi test

00:14:44,560 --> 00:14:49,279
test cases if you have those the next

00:14:47,839 --> 00:14:52,880
tool that we'll talk about

00:14:49,279 --> 00:14:55,680
for fake data generators is mimesis

00:14:52,880 --> 00:14:57,279
and mimesis is about 10 times faster

00:14:55,680 --> 00:14:59,680
than faker which is its major

00:14:57,279 --> 00:15:01,040
claim to fame they're both pretty fast

00:14:59,680 --> 00:15:02,720
but if performance

00:15:01,040 --> 00:15:04,560
um and like the speed of your test is

00:15:02,720 --> 00:15:05,680
very very important mimesis might be a

00:15:04,560 --> 00:15:08,160
better choice

00:15:05,680 --> 00:15:09,120
it also allows schema based generators

00:15:08,160 --> 00:15:11,199
so you can

00:15:09,120 --> 00:15:13,199
for example rather than sort of defining

00:15:11,199 --> 00:15:13,920
your generator in code you can define

00:15:13,199 --> 00:15:16,560
your generator

00:15:13,920 --> 00:15:18,000
through a mapping like this and it will

00:15:16,560 --> 00:15:20,959
generate

00:15:18,000 --> 00:15:23,279
data that that conforms to that schema

00:15:20,959 --> 00:15:25,199
there's also a cli mma system to

00:15:23,279 --> 00:15:28,720
populate a database

00:15:25,199 --> 00:15:29,920
using flask downsides to mimesis are

00:15:28,720 --> 00:15:31,839
that it only supports

00:15:29,920 --> 00:15:33,279
python greater than or equal to version

00:15:31,839 --> 00:15:35,279
36 and

00:15:33,279 --> 00:15:36,720
it's just not as popular or as well

00:15:35,279 --> 00:15:40,240
documented as faker but

00:15:36,720 --> 00:15:42,880
that could change in the future so

00:15:40,240 --> 00:15:43,360
in conclusion uh thank you for your time

00:15:42,880 --> 00:15:45,600
um

00:15:43,360 --> 00:15:47,920
when you're working with potentially

00:15:45,600 --> 00:15:49,120
messy real world data sets

00:15:47,920 --> 00:15:51,199
and you want to make sure that your

00:15:49,120 --> 00:15:53,360
quality of your data is high

00:15:51,199 --> 00:15:55,360
start by asking those qualitative

00:15:53,360 --> 00:15:57,440
questions and digging into your data

00:15:55,360 --> 00:16:00,240
using data profiling tools like pandas

00:15:57,440 --> 00:16:03,199
profiling or apache superset

00:16:00,240 --> 00:16:04,160
from there define what valid data is in

00:16:03,199 --> 00:16:08,480
your system

00:16:04,160 --> 00:16:10,880
using tools like swagger or async api

00:16:08,480 --> 00:16:11,920
write tests that ensure that you are

00:16:10,880 --> 00:16:13,519
handling

00:16:11,920 --> 00:16:15,199
invalid data the way that you would

00:16:13,519 --> 00:16:17,519
expect to um

00:16:15,199 --> 00:16:18,240
you can use validation tools like

00:16:17,519 --> 00:16:21,199
cerberus

00:16:18,240 --> 00:16:22,000
or json schema and you can use fake data

00:16:21,199 --> 00:16:25,600
generators like

00:16:22,000 --> 00:16:33,279
faker and the mesis and that's it

00:16:25,600 --> 00:16:33,279

YouTube URL: https://www.youtube.com/watch?v=coJwPuWJ3Ok


