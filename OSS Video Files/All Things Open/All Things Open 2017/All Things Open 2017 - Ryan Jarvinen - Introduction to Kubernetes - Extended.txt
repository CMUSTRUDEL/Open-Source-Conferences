Title: All Things Open 2017 - Ryan Jarvinen - Introduction to Kubernetes - Extended
Publication date: 2017-11-28
Playlist: All Things Open 2017
Description: 
	All Things Open 2017, Raleigh NC, Raleigh Convention Center. www.Allthingsopen.org
Captions: 
	00:00:00,030 --> 00:00:10,860
I'm Ryan Jarvan --an Ryan J here's my my

00:00:06,270 --> 00:00:14,480
avatar photo here I work at Red Hat you

00:00:10,860 --> 00:00:17,880
know my my hat is black today I am a

00:00:14,480 --> 00:00:19,590
developer advocate at Red Hat's today

00:00:17,880 --> 00:00:22,560
we're going to be covering a

00:00:19,590 --> 00:00:26,070
introduction to kubernetes so come on in

00:00:22,560 --> 00:00:29,460
grab your laptops it looks like it's

00:00:26,070 --> 00:00:31,529
right about 1:30 in ballroom a so let's

00:00:29,460 --> 00:00:35,340
get started if you would like to follow

00:00:31,529 --> 00:00:38,040
along on your laptop I have some slides

00:00:35,340 --> 00:00:41,930
that you can follow along with there's a

00:00:38,040 --> 00:00:43,140
bit the bitly URL right here bitly slash

00:00:41,930 --> 00:00:47,360
ato

00:00:43,140 --> 00:00:49,829
- Kas if you go to that URL

00:00:47,360 --> 00:00:53,579
hopefully Wi-Fi permitting you'll be

00:00:49,829 --> 00:00:56,660
able to follow along on your laptop so

00:00:53,579 --> 00:00:59,489
the way I've organized this content is

00:00:56,660 --> 00:01:02,670
really intended to be a hands-on

00:00:59,489 --> 00:01:06,090
interactive course where you are

00:01:02,670 --> 00:01:09,450
learning by doing and if you don't learn

00:01:06,090 --> 00:01:11,670
by doing in this particular session the

00:01:09,450 --> 00:01:15,439
slides have all the information for you

00:01:11,670 --> 00:01:19,439
to learn by doing later after the fact

00:01:15,439 --> 00:01:22,530
and hopefully enough information for you

00:01:19,439 --> 00:01:25,229
to relay this information and train

00:01:22,530 --> 00:01:28,619
someone else on how to do some basic web

00:01:25,229 --> 00:01:30,960
development using kubernetes so that's

00:01:28,619 --> 00:01:33,840
that's one of my top level goals for

00:01:30,960 --> 00:01:35,310
everyone in this room and hopefully I

00:01:33,840 --> 00:01:37,380
cover a lot of different kubernetes

00:01:35,310 --> 00:01:39,090
material hopefully something in it for

00:01:37,380 --> 00:01:45,000
everybody

00:01:39,090 --> 00:01:49,979
so again Ryan J welcome I was brought to

00:01:45,000 --> 00:01:50,670
you by Red Hat today but otherwise let's

00:01:49,979 --> 00:01:54,030
get started

00:01:50,670 --> 00:01:56,700
so workshop agenda I'll cover some notes

00:01:54,030 --> 00:01:59,040
from the workshop setup if you didn't

00:01:56,700 --> 00:02:01,500
complete the workshop setup in advance

00:01:59,040 --> 00:02:04,409
I've got it all covered in the slides

00:02:01,500 --> 00:02:07,170
feel free to skip it skip ahead if you

00:02:04,409 --> 00:02:09,959
haven't completed that section with this

00:02:07,170 --> 00:02:10,960
many people in the room I'm expecting

00:02:09,959 --> 00:02:14,650
some why

00:02:10,960 --> 00:02:16,210
by issues so get started quick before

00:02:14,650 --> 00:02:19,810
your neighbor does hopefully we'll all

00:02:16,210 --> 00:02:22,120
be able to participate in it workshop

00:02:19,810 --> 00:02:27,370
set up notes feel free to just click on

00:02:22,120 --> 00:02:32,530
that or follow the link on the course

00:02:27,370 --> 00:02:36,180
page on the web kubernetes basics will

00:02:32,530 --> 00:02:39,600
be the next topic we cover I'll try to

00:02:36,180 --> 00:02:42,670
introduce you to five kubernetes

00:02:39,600 --> 00:02:45,520
primitives these are kind of building

00:02:42,670 --> 00:02:48,760
blocks that you'll use to architect

00:02:45,520 --> 00:02:50,770
container based solutions so we'll

00:02:48,760 --> 00:02:53,830
figure out what kubernetes provides as

00:02:50,770 --> 00:02:56,410
far as these primitive objects or

00:02:53,830 --> 00:02:59,980
kubernetes resources as they're often

00:02:56,410 --> 00:03:03,790
referred to after that we'll take a look

00:02:59,980 --> 00:03:05,740
at kubernetes architecture a little

00:03:03,790 --> 00:03:09,160
shorter session on that since we'll be

00:03:05,740 --> 00:03:14,010
using mini Kubb and lastly i'll talk

00:03:09,160 --> 00:03:18,310
about local development with mini Kubb

00:03:14,010 --> 00:03:21,160
so I have an intro survey this is a lot

00:03:18,310 --> 00:03:24,040
of people so but I like to try to get a

00:03:21,160 --> 00:03:27,880
sense of the room and see which sections

00:03:24,040 --> 00:03:29,710
I should prioritize with my time so just

00:03:27,880 --> 00:03:32,100
by a show of hands how many people are

00:03:29,710 --> 00:03:34,540
doing something with containers today

00:03:32,100 --> 00:03:36,430
that looks like a good number of people

00:03:34,540 --> 00:03:38,860
the lights are bright but I would say

00:03:36,430 --> 00:03:40,510
that's the the Farb the majority of the

00:03:38,860 --> 00:03:42,310
room so these are meant to be kind of

00:03:40,510 --> 00:03:46,810
decreasing I'm expecting to see less

00:03:42,310 --> 00:03:49,840
hands on this that was probably 80% or

00:03:46,810 --> 00:03:52,060
more how many are using kubernetes or

00:03:49,840 --> 00:03:54,940
have experience using kubernetes there's

00:03:52,060 --> 00:03:57,520
a much lower this is less than 50% I'd

00:03:54,940 --> 00:04:01,210
say maybe 30 20 percent something like

00:03:57,520 --> 00:04:04,480
that so good this will be a lot of new

00:04:01,210 --> 00:04:06,760
content for you and since not very many

00:04:04,480 --> 00:04:09,010
of you are familiar with kubernetes less

00:04:06,760 --> 00:04:10,780
folks who might be proficient consider

00:04:09,010 --> 00:04:13,330
themselves to be proficient with coop

00:04:10,780 --> 00:04:17,100
CTL probably fewer people but thanks

00:04:13,330 --> 00:04:19,450
thanks for following along cool and

00:04:17,100 --> 00:04:21,850
these last two don't worry about

00:04:19,450 --> 00:04:23,740
hopefully by the end of the course you

00:04:21,850 --> 00:04:26,800
all feel confident raising

00:04:23,740 --> 00:04:29,650
hands-on these last two questions

00:04:26,800 --> 00:04:31,720
can you name five kubernetes primitives

00:04:29,650 --> 00:04:33,910
and tell me about five pieces of

00:04:31,720 --> 00:04:36,580
kubernetes architecture you may not have

00:04:33,910 --> 00:04:38,919
it now but hopefully you'll be able to

00:04:36,580 --> 00:04:41,169
raise your hand for all of these plus an

00:04:38,919 --> 00:04:44,259
additional bonus question of are you

00:04:41,169 --> 00:04:46,990
ready to onboard a web dev that's that's

00:04:44,259 --> 00:04:49,330
my ideal like if everyone's hands go up

00:04:46,990 --> 00:04:51,430
I'll be very happy at the end otherwise

00:04:49,330 --> 00:04:53,860
I got more work to do on my slides which

00:04:51,430 --> 00:04:57,490
is definitely true as well

00:04:53,860 --> 00:05:00,099
so introduction to kubernetes workshop

00:04:57,490 --> 00:05:03,819
prep these this is everything that you

00:05:00,099 --> 00:05:06,370
will need for your laptop environment in

00:05:03,819 --> 00:05:09,400
order to follow along the first item is

00:05:06,370 --> 00:05:12,669
Kubb CTL I've tried to make it easy for

00:05:09,400 --> 00:05:17,050
most people I may I gave the Windows

00:05:12,669 --> 00:05:21,069
users a little extra challenge I only

00:05:17,050 --> 00:05:23,699
had a Bosch one-liner for OSX and Linux

00:05:21,069 --> 00:05:28,229
so those are the ones you get a easy

00:05:23,699 --> 00:05:32,919
one-line copy and paste I could do this

00:05:28,229 --> 00:05:36,280
to just demonstrate but this has a cost

00:05:32,919 --> 00:05:39,580
this will download a very large binary

00:05:36,280 --> 00:05:42,310
over the Wi-Fi so I'm in a skip on this

00:05:39,580 --> 00:05:43,210
particular one if you already have it

00:05:42,310 --> 00:05:46,570
great

00:05:43,210 --> 00:05:48,759
skip this step as well if you don't have

00:05:46,570 --> 00:05:52,300
it this one-liner should get you the

00:05:48,759 --> 00:05:57,610
coupe's ETL command-line tool for linux

00:05:52,300 --> 00:05:59,710
and this one for Apple and if you are

00:05:57,610 --> 00:06:02,740
not in either of those two camps click

00:05:59,710 --> 00:06:04,659
on the official coop CTL setup notes and

00:06:02,740 --> 00:06:06,669
they've got really solid documentation

00:06:04,659 --> 00:06:10,389
for Windows I just didn't have a

00:06:06,669 --> 00:06:12,930
one-liner for Windows but let me know if

00:06:10,389 --> 00:06:17,530
you run into any trouble with that one

00:06:12,930 --> 00:06:21,159
next up is mini Kubb similar one-line

00:06:17,530 --> 00:06:24,190
install for Windows and Apple or sorry

00:06:21,159 --> 00:06:26,500
Linux and Apple Windows folks click

00:06:24,190 --> 00:06:29,590
through to the official Docs there's a

00:06:26,500 --> 00:06:32,139
Exe file for you to run really easy to

00:06:29,590 --> 00:06:35,560
get started and if you have a laptop

00:06:32,139 --> 00:06:35,880
with plenty of extra memory you can set

00:06:35,560 --> 00:06:38,100
some

00:06:35,880 --> 00:06:40,920
to config variables to take advantage of

00:06:38,100 --> 00:06:43,530
that this will set some configs to allow

00:06:40,920 --> 00:06:47,430
4 gigs of ram to be used by the vm

00:06:43,530 --> 00:06:50,160
whoops and to cpu cores to be available

00:06:47,430 --> 00:06:51,570
so depending on your environment this is

00:06:50,160 --> 00:06:55,050
all going to be running kubernetes

00:06:51,570 --> 00:06:57,360
within a single VM for today's purposes

00:06:55,050 --> 00:06:59,400
but you can of course deploy a

00:06:57,360 --> 00:07:01,350
full-scale kubernetes environment this

00:06:59,400 --> 00:07:04,710
is just so you can get your hands dirty

00:07:01,350 --> 00:07:07,260
learn something and have environment

00:07:04,710 --> 00:07:09,270
that costs you zero cents to stand up

00:07:07,260 --> 00:07:13,050
and one that you can take home with you

00:07:09,270 --> 00:07:16,200
at the end of the day mini coop basics

00:07:13,050 --> 00:07:19,290
once you have coop CTL and mini coop

00:07:16,200 --> 00:07:24,200
available you should be able to run mini

00:07:19,290 --> 00:07:28,530
coop start in order to boot up a VM

00:07:24,200 --> 00:07:34,050
I'll run mini coop start error looks

00:07:28,530 --> 00:07:36,660
like ooh some kind of bug I might

00:07:34,050 --> 00:07:47,190
already have one running let's see if I

00:07:36,660 --> 00:07:51,570
could do a coop CTL status oh nice well

00:07:47,190 --> 00:07:53,280
we'll get back to that I can you can

00:07:51,570 --> 00:07:55,260
always if you run into problems with

00:07:53,280 --> 00:07:56,460
your mini coop environment which I may

00:07:55,260 --> 00:07:58,680
or may not be running into problems

00:07:56,460 --> 00:08:03,210
right now and you can always mini coop

00:07:58,680 --> 00:08:06,480
stop to halt your VM mini coop delete to

00:08:03,210 --> 00:08:11,400
delete the VM and then you can run many

00:08:06,480 --> 00:08:14,040
coop start to init a new blank in empty

00:08:11,400 --> 00:08:17,610
mini coop environment each time you do

00:08:14,040 --> 00:08:20,700
this you may be redownload anew assets

00:08:17,610 --> 00:08:23,150
over the Wi-Fi so do this sparingly if

00:08:20,700 --> 00:08:31,530
you don't need to reset your environment

00:08:23,150 --> 00:08:34,979
and let me see coop CTL version was the

00:08:31,530 --> 00:08:38,310
command I wanted to run and it looks

00:08:34,979 --> 00:08:41,490
like I do have a mini coop up and

00:08:38,310 --> 00:08:45,540
running so if you run into trouble while

00:08:41,490 --> 00:08:47,420
trying to start many Kubb there's one

00:08:45,540 --> 00:08:50,510
common issue that

00:08:47,420 --> 00:08:52,820
people run into and it's usually to do

00:08:50,510 --> 00:08:55,820
with the virtualization support that

00:08:52,820 --> 00:08:57,649
many cube tries to use here's some great

00:08:55,820 --> 00:09:00,070
three reminders for you to keep in mind

00:08:57,649 --> 00:09:03,649
many cube does require some type of

00:09:00,070 --> 00:09:05,000
virtualization back-end most OS has

00:09:03,649 --> 00:09:07,490
already includes support for

00:09:05,000 --> 00:09:10,639
virtualization if you want to opt into

00:09:07,490 --> 00:09:15,709
any specific provider you could use the

00:09:10,639 --> 00:09:20,600
- - VM driver flag and VirtualBox is a

00:09:15,709 --> 00:09:22,639
nice saying default so here hopefully if

00:09:20,600 --> 00:09:26,120
you already have VirtualBox installed

00:09:22,639 --> 00:09:28,160
running this line might fix things if

00:09:26,120 --> 00:09:30,829
you're running into bugs because it may

00:09:28,160 --> 00:09:37,070
be getting confused on which VM provider

00:09:30,829 --> 00:09:38,810
to use right docker is one thing that

00:09:37,070 --> 00:09:40,820
you will need if you plan on following

00:09:38,810 --> 00:09:42,740
along with the later stages with the

00:09:40,820 --> 00:09:44,630
local development stages if you don't

00:09:42,740 --> 00:09:47,680
want to follow along with the local

00:09:44,630 --> 00:09:51,500
development you can skip this slide I

00:09:47,680 --> 00:09:53,510
Justin used a on Apple you could do brew

00:09:51,500 --> 00:09:55,070
install if you have a package manager

00:09:53,510 --> 00:09:57,980
like DNF or

00:09:55,070 --> 00:10:01,160
apt-get or yum you could use those to

00:09:57,980 --> 00:10:03,980
install docker or you can always get it

00:10:01,160 --> 00:10:06,589
from the docker store download Dockers

00:10:03,980 --> 00:10:09,980
binaries directly from them any of those

00:10:06,589 --> 00:10:11,720
should work for today's workshop last

00:10:09,980 --> 00:10:17,089
step you'll need for setting up docker

00:10:11,720 --> 00:10:20,300
is this eval statement and you could

00:10:17,089 --> 00:10:22,220
paste this into your command line what

00:10:20,300 --> 00:10:25,250
this will end up doing I'll take the

00:10:22,220 --> 00:10:29,630
eval off here so we could see this if

00:10:25,250 --> 00:10:31,810
you have docker installed will tell set

00:10:29,630 --> 00:10:37,279
some environment variables that should

00:10:31,810 --> 00:10:40,220
instruct mini Kubb how to use or

00:10:37,279 --> 00:10:42,620
instruct your docker CLI so that when

00:10:40,220 --> 00:10:45,170
you do your docker build the result of

00:10:42,620 --> 00:10:48,079
that build goes inside mini cube this

00:10:45,170 --> 00:10:50,269
will save you from having to docker push

00:10:48,079 --> 00:10:52,550
out to the outside Internet and then

00:10:50,269 --> 00:10:54,500
pull back down onto our Wi-Fi network

00:10:52,550 --> 00:10:56,779
you could just do a docker build and

00:10:54,500 --> 00:10:59,600
it'll end up right inside the kubernetes

00:10:56,779 --> 00:11:01,250
environment so make sure to follow along

00:10:59,600 --> 00:11:05,370
with this

00:11:01,250 --> 00:11:07,770
eval mini coop docker in statement if

00:11:05,370 --> 00:11:11,430
you are going to build your own

00:11:07,770 --> 00:11:13,470
container later last step on

00:11:11,430 --> 00:11:16,290
installation and hopefully everyone has

00:11:13,470 --> 00:11:18,750
had a good chance to catch up by now if

00:11:16,290 --> 00:11:20,520
you're following along you want to

00:11:18,750 --> 00:11:23,070
install get this is commonly available

00:11:20,520 --> 00:11:26,910
through a package manager on your

00:11:23,070 --> 00:11:29,160
operating system consult the official

00:11:26,910 --> 00:11:31,080
get Docs or your OS Docs if you need

00:11:29,160 --> 00:11:33,150
help installing get but I'm assuming

00:11:31,080 --> 00:11:36,240
with this audience I probably didn't

00:11:33,150 --> 00:11:40,430
even need this slide so that's the last

00:11:36,240 --> 00:11:42,540
step for setup how many folks are ready

00:11:40,430 --> 00:11:47,000
how many how many folks plan on

00:11:42,540 --> 00:11:51,270
following along and are not ready okay

00:11:47,000 --> 00:11:55,200
all right ru same folks are you waiting

00:11:51,270 --> 00:11:57,740
on downloads okay good luck hope it

00:11:55,200 --> 00:12:00,210
works out for you

00:11:57,740 --> 00:12:02,750
you get these slides will be up long

00:12:00,210 --> 00:12:06,570
term you can always follow along later

00:12:02,750 --> 00:12:09,330
and if your neighbor finishes before you

00:12:06,570 --> 00:12:10,980
feel free to introduce yourself ask if

00:12:09,330 --> 00:12:14,640
you could peek over their shoulder I

00:12:10,980 --> 00:12:17,430
know that's not best practices with with

00:12:14,640 --> 00:12:20,010
laptops but feel free to share your

00:12:17,430 --> 00:12:23,040
environment if you have someone who got

00:12:20,010 --> 00:12:24,750
these requirements done early I'd like

00:12:23,040 --> 00:12:26,640
everyone to be able to visually follow

00:12:24,750 --> 00:12:28,680
along so definitely reach out to your

00:12:26,640 --> 00:12:31,080
neighbor and check in with them see how

00:12:28,680 --> 00:12:34,290
they're doing in regards to downloads

00:12:31,080 --> 00:12:37,140
and and setup and I'll try to check back

00:12:34,290 --> 00:12:38,730
in on this topic later and attempt to

00:12:37,140 --> 00:12:42,300
stall for the folks that are still

00:12:38,730 --> 00:12:45,960
waiting on the download so to validate

00:12:42,300 --> 00:12:48,420
your environment when you are ready this

00:12:45,960 --> 00:12:51,180
is the command that you would run and

00:12:48,420 --> 00:12:56,670
I'll get a clean screen here I'm going

00:12:51,180 --> 00:13:00,300
to run Kubb CTL version this like most

00:12:56,670 --> 00:13:03,570
things from the kubernetes api is going

00:13:00,300 --> 00:13:06,960
to give me two responses the kubernetes

00:13:03,570 --> 00:13:09,510
api generally well we'll get into this

00:13:06,960 --> 00:13:11,640
next in the next next topic but I got

00:13:09,510 --> 00:13:13,430
two responses here the first response is

00:13:11,640 --> 00:13:17,089
from the client this is

00:13:13,430 --> 00:13:19,700
the coupe CTL client version and I'm

00:13:17,089 --> 00:13:22,160
running the latest one I'm not sure if

00:13:19,700 --> 00:13:25,190
its latest now but the latest major

00:13:22,160 --> 00:13:31,520
release 1.8 and it looks like my

00:13:25,190 --> 00:13:34,370
kubernetes server version is 1.78 if you

00:13:31,520 --> 00:13:38,300
are feeling adventurous and want to opt

00:13:34,370 --> 00:13:44,209
in to the latest kubernetes 1.8 code you

00:13:38,300 --> 00:13:47,060
can always run this right here this

00:13:44,209 --> 00:13:51,230
advanced challenge option to boot into

00:13:47,060 --> 00:13:53,330
kubernetes 1.8 it's still experimental

00:13:51,230 --> 00:13:55,149
mini coop doesn't have full support for

00:13:53,330 --> 00:13:58,880
it and I've heard there's some bugs with

00:13:55,149 --> 00:14:03,560
volume access but for the adventurous

00:13:58,880 --> 00:14:05,570
folks there's the 1.8 in it line I'm

00:14:03,560 --> 00:14:09,880
gonna stick with my current environment

00:14:05,570 --> 00:14:13,070
just to save the Wi-Fi strain okay so

00:14:09,880 --> 00:14:15,500
hopefully some portion of you are ready

00:14:13,070 --> 00:14:17,950
good luck with the Wi-Fi download for

00:14:15,500 --> 00:14:21,080
the rest of you and I'll try to

00:14:17,950 --> 00:14:25,450
distribute more warnings in advance to

00:14:21,080 --> 00:14:27,920
try to get you set up beforehand

00:14:25,450 --> 00:14:30,230
kubernetes command-line basics with coop

00:14:27,920 --> 00:14:33,100
CTL i'll start this section hopefully

00:14:30,230 --> 00:14:35,690
you get a chance to catch up this whole

00:14:33,100 --> 00:14:38,870
section is available as a standalone

00:14:35,690 --> 00:14:40,279
training module starting right here so

00:14:38,870 --> 00:14:43,510
if you want to do just this section

00:14:40,279 --> 00:14:47,150
later command-line basics I have as a

00:14:43,510 --> 00:14:51,350
individual workshop for you to recycle

00:14:47,150 --> 00:14:54,800
and share at meetups potentially so why

00:14:51,350 --> 00:14:56,230
kubernetes here's my broad overview now

00:14:54,800 --> 00:14:59,050
that we have set up out-of-the-way

00:14:56,230 --> 00:15:02,209
kubernetes is an open source platform

00:14:59,050 --> 00:15:05,839
for running container based distributed

00:15:02,209 --> 00:15:08,060
solutions featuring fully modular high

00:15:05,839 --> 00:15:10,010
available systems architect this kind of

00:15:08,060 --> 00:15:12,560
a run-on sentence but I was trying to do

00:15:10,010 --> 00:15:16,940
my best to sum up what is kubernetes in

00:15:12,560 --> 00:15:19,790
a single single quote I think there's a

00:15:16,940 --> 00:15:22,130
lot going on in there there's a lot

00:15:19,790 --> 00:15:24,050
that's kind of hard to describe but I

00:15:22,130 --> 00:15:26,470
think fundamentally you're architecting

00:15:24,050 --> 00:15:32,200
systems using containers

00:15:26,470 --> 00:15:34,900
and you're building with a high

00:15:32,200 --> 00:15:40,000
availability as a default assumption of

00:15:34,900 --> 00:15:43,240
what your needs are the best kubernetes

00:15:40,000 --> 00:15:46,090
can all is also known to be the best way

00:15:43,240 --> 00:15:50,050
to manage distributed solutions at scale

00:15:46,090 --> 00:15:53,740
based on not my experience well let's

00:15:50,050 --> 00:15:57,400
see well based on what Google might tell

00:15:53,740 --> 00:15:59,560
you and they're in there they're not

00:15:57,400 --> 00:16:01,690
using it internally but they use a lot

00:15:59,560 --> 00:16:03,820
of these same concepts in how they

00:16:01,690 --> 00:16:06,430
manage all of their workloads internally

00:16:03,820 --> 00:16:08,680
and they've been using containers for

00:16:06,430 --> 00:16:11,800
over 10 years to run everything from

00:16:08,680 --> 00:16:14,230
Google search to Gmail to Google Docs

00:16:11,800 --> 00:16:16,270
and just with a number of browser tabs I

00:16:14,230 --> 00:16:18,880
have open right now they're probably

00:16:16,270 --> 00:16:21,400
running half a dozen containers just for

00:16:18,880 --> 00:16:23,460
my individual user I might assume I

00:16:21,400 --> 00:16:26,380
don't know that I'm connecting with

00:16:23,460 --> 00:16:29,350
currently so I think Google has a lot of

00:16:26,380 --> 00:16:32,260
experience using containers at scale and

00:16:29,350 --> 00:16:35,980
kubernetes really tries to distill the

00:16:32,260 --> 00:16:39,310
best of those learnings and offer it to

00:16:35,980 --> 00:16:42,040
the open source community so that's one

00:16:39,310 --> 00:16:45,280
reason I'm really excited about it it's

00:16:42,040 --> 00:16:47,260
an extendable modeling language with a

00:16:45,280 --> 00:16:51,850
huge community following that's another

00:16:47,260 --> 00:16:53,980
plus one of my favorite attributes of

00:16:51,850 --> 00:16:56,650
what is kubernetes it's a multi vendor

00:16:53,980 --> 00:16:59,140
effort with a lot of different companies

00:16:56,650 --> 00:17:02,230
all collaborating it's not just Red Hat

00:16:59,140 --> 00:17:05,740
or just Google or just Microsoft or just

00:17:02,230 --> 00:17:08,439
any other specific vendor it's really a

00:17:05,740 --> 00:17:12,310
best of worlds of a lot of different

00:17:08,439 --> 00:17:13,750
vendors all pitching in which is maybe

00:17:12,310 --> 00:17:18,510
different than what you'll see from

00:17:13,750 --> 00:17:21,490
other other folks in the container space

00:17:18,510 --> 00:17:24,579
kubernetes what it does fundamentally

00:17:21,490 --> 00:17:26,530
how how it acts acts on all these things

00:17:24,579 --> 00:17:28,569
is it provides an API

00:17:26,530 --> 00:17:32,950
every time you contact the kubernetes

00:17:28,569 --> 00:17:35,500
api you'll see some familiar things in

00:17:32,950 --> 00:17:38,230
your response the response is always

00:17:35,500 --> 00:17:40,320
going to be usually it's going to be

00:17:38,230 --> 00:17:42,539
formatted as a JSON or

00:17:40,320 --> 00:17:45,059
amel object and it's going to have

00:17:42,539 --> 00:17:48,630
attributes generally that look a lot

00:17:45,059 --> 00:17:51,299
like this it'll have a a kind of a type

00:17:48,630 --> 00:17:56,759
of attribute or a kind of attribute or

00:17:51,299 --> 00:18:01,679
data packet it'll eat data resource is

00:17:56,759 --> 00:18:07,139
versioned so if I create instantiate a

00:18:01,679 --> 00:18:11,100
container pod with API v1 I know I'll

00:18:07,139 --> 00:18:14,700
get a certain action and if I use the

00:18:11,100 --> 00:18:16,769
exact same data with API 1.1 I may get a

00:18:14,700 --> 00:18:18,509
slightly different side effects but I

00:18:16,769 --> 00:18:20,639
know I have both of these api's

00:18:18,509 --> 00:18:22,980
available concurrently and I can opt

00:18:20,639 --> 00:18:26,490
into the behavior of either one and

00:18:22,980 --> 00:18:30,149
reduce the difficulty around migrating

00:18:26,490 --> 00:18:32,580
across API versions so the data itself

00:18:30,149 --> 00:18:35,429
is versioned there's always a almost

00:18:32,580 --> 00:18:40,320
always a mated data metadata field and

00:18:35,429 --> 00:18:42,240
then a spec and a status and I have an

00:18:40,320 --> 00:18:45,690
asterisks here because this is mostly

00:18:42,240 --> 00:18:48,509
true almost every type of data you get

00:18:45,690 --> 00:18:50,309
in and out of kubernetes usually has all

00:18:48,509 --> 00:18:52,710
these attributes and we'll see a few

00:18:50,309 --> 00:18:56,990
exceptions to this as we go through the

00:18:52,710 --> 00:18:59,909
content so these are my big five

00:18:56,990 --> 00:19:03,120
kubernetes building blocks that I want

00:18:59,909 --> 00:19:05,879
you to learn throughout this this course

00:19:03,120 --> 00:19:08,549
this is my first achievement the first

00:19:05,879 --> 00:19:12,149
module that we'll go through will start

00:19:08,549 --> 00:19:17,100
with what is a node in kubernetes terms

00:19:12,149 --> 00:19:19,259
it's not no js' what is a pod what is a

00:19:17,100 --> 00:19:21,450
service what's a deployment and what's a

00:19:19,259 --> 00:19:25,409
replica set these will be our building

00:19:21,450 --> 00:19:28,409
blocks for today so a node in kubernetes

00:19:25,409 --> 00:19:32,149
terms a node is a host machine this can

00:19:28,409 --> 00:19:35,850
be a physical machine a virtual machine

00:19:32,149 --> 00:19:36,909
bare-metal VMs doesn't really matter as

00:19:35,850 --> 00:19:40,690
long as

00:19:36,909 --> 00:19:42,669
it has resources and an IP address and

00:19:40,690 --> 00:19:46,440
and can be contacted over the network

00:19:42,669 --> 00:19:51,519
but this is where your process is run

00:19:46,440 --> 00:19:54,909
the activity on each node is managed via

00:19:51,519 --> 00:19:57,539
one or more master instances so there's

00:19:54,909 --> 00:20:01,269
two types machine types in kubernetes

00:19:57,539 --> 00:20:03,149
generally there's people who come up

00:20:01,269 --> 00:20:05,799
with more classifications but two main

00:20:03,149 --> 00:20:08,559
usually you have your masters which run

00:20:05,799 --> 00:20:13,570
the API services and then nodes where

00:20:08,559 --> 00:20:15,190
workloads are executed this is our first

00:20:13,570 --> 00:20:16,960
command line example so if you're

00:20:15,190 --> 00:20:20,320
following along from the command line

00:20:16,960 --> 00:20:23,109
get ready to copy and paste I'm going to

00:20:20,320 --> 00:20:25,090
have you run this command and hopefully

00:20:23,109 --> 00:20:28,599
you'll learn something about kubernetes

00:20:25,090 --> 00:20:31,679
as the results so I'll run get nodes

00:20:28,599 --> 00:20:38,320
here not sure why my laptop is so

00:20:31,679 --> 00:20:40,269
painfully slow today but hopefully it

00:20:38,320 --> 00:20:42,759
keeps up to speed all right so it looks

00:20:40,269 --> 00:20:44,229
like I have one node in a writ ready

00:20:42,759 --> 00:20:47,590
status that's because I'm running in

00:20:44,229 --> 00:20:49,509
mini coop and so I'm in a single VM if

00:20:47,590 --> 00:20:51,609
you had a full kubernetes environment

00:20:49,509 --> 00:20:55,149
you might depending on your access level

00:20:51,609 --> 00:20:58,769
see a lot of nodes that are available I

00:20:55,149 --> 00:21:02,679
can request the same information but ask

00:20:58,769 --> 00:21:05,820
that the results be structured as Yam

00:21:02,679 --> 00:21:09,159
all let's give that a try

00:21:05,820 --> 00:21:13,869
here's same get nodes command and I'm

00:21:09,159 --> 00:21:16,929
gonna add a - oh yeah Mille so you could

00:21:13,869 --> 00:21:19,659
see from the output we have much more

00:21:16,929 --> 00:21:23,139
machine readable output here

00:21:19,659 --> 00:21:24,609
it's structured let me get to the top

00:21:23,139 --> 00:21:26,710
well it's a lot of stuff just for one

00:21:24,609 --> 00:21:28,509
node so there's attributes that I

00:21:26,710 --> 00:21:30,789
mentioned you'll see that'll be familiar

00:21:28,509 --> 00:21:31,450
API version was one of them this is

00:21:30,789 --> 00:21:35,379
versioned

00:21:31,450 --> 00:21:38,049
data this is the type of data and this

00:21:35,379 --> 00:21:40,029
one is actually a list type it's saying

00:21:38,049 --> 00:21:43,059
we have a list of items so this is a

00:21:40,029 --> 00:21:45,999
data type that doesn't have a kind or a

00:21:43,059 --> 00:21:47,589
maded metadata or a spec or a status

00:21:45,999 --> 00:21:49,570
this is one of the special cases that I

00:21:47,589 --> 00:21:50,650
mentioned but you could see inside this

00:21:49,570 --> 00:21:54,340
list

00:21:50,650 --> 00:21:55,000
the option there's another embedded

00:21:54,340 --> 00:21:59,080
object

00:21:55,000 --> 00:22:02,470
so this is our embedded node object that

00:21:59,080 --> 00:22:04,929
came back in the response you could see

00:22:02,470 --> 00:22:07,840
there's a API version on this resource

00:22:04,929 --> 00:22:11,020
there is a type of resource or a kind of

00:22:07,840 --> 00:22:13,360
resource metadata is definitely in there

00:22:11,020 --> 00:22:16,929
and there should be here's the spec and

00:22:13,360 --> 00:22:21,029
then a status right just like I said all

00:22:16,929 --> 00:22:28,830
output from the API is going to have

00:22:21,029 --> 00:22:32,710
those those attributes from this slide

00:22:28,830 --> 00:22:36,820
API version metadata spec status should

00:22:32,710 --> 00:22:44,049
all be there so let's try the same thing

00:22:36,820 --> 00:22:48,399
since we know the name of our node we

00:22:44,049 --> 00:22:52,720
can do coop CTL get node with the node

00:22:48,399 --> 00:22:55,840
name it looks like oh I'm talking to gke

00:22:52,720 --> 00:23:03,399
that's what's going on okay

00:22:55,840 --> 00:23:05,980
many cube stop another nice thing about

00:23:03,399 --> 00:23:08,649
kubernetes is you could set the context

00:23:05,980 --> 00:23:11,590
that your coop CTL command line tool was

00:23:08,649 --> 00:23:18,250
talking to I'm talking to

00:23:11,590 --> 00:23:22,899
I was talking to make you broad disk who

00:23:18,250 --> 00:23:27,909
okay I'm going to potentially ruin my

00:23:22,899 --> 00:23:31,090
presentation maybe I should stay I

00:23:27,909 --> 00:23:33,270
should stay connected all right all

00:23:31,090 --> 00:23:36,190
right high risk move I'm gonna try this

00:23:33,270 --> 00:23:44,470
I'm gonna try a mini cubed elite and

00:23:36,190 --> 00:23:45,820
what's that say that again yeah yeah I

00:23:44,470 --> 00:23:52,029
know that's what I'm worried about

00:23:45,820 --> 00:23:53,409
yeah yeah 30 minutes okay so don't try

00:23:52,029 --> 00:24:01,140
this is what you're saying all right

00:23:53,409 --> 00:24:01,140
okay well let me see if I can find an

00:24:01,590 --> 00:24:09,910
all right brief moment well while I try

00:24:05,680 --> 00:24:14,080
to fix my Kubb CTL context that's why I

00:24:09,910 --> 00:24:16,510
was getting earlier when I did coop CTL

00:24:14,080 --> 00:24:19,120
version sorry you don't need to hear all

00:24:16,510 --> 00:24:21,730
of my debugging when I saw the the

00:24:19,120 --> 00:24:25,690
server version was one seven eight

00:24:21,730 --> 00:24:29,500
I was actually contacting Google's cloud

00:24:25,690 --> 00:24:31,480
here so I have an extra coop CTL let me

00:24:29,500 --> 00:24:34,330
see if I can find it if you want to

00:24:31,480 --> 00:24:35,920
figure out where your credentials are

00:24:34,330 --> 00:24:40,210
you should be able to look in this

00:24:35,920 --> 00:24:52,320
folder and I have a config gke I'm going

00:24:40,210 --> 00:24:56,680
to RM Kubb config and then mini cube

00:24:52,320 --> 00:25:01,990
start this probably isn't gonna do it

00:24:56,680 --> 00:25:05,160
permission denied stop stop the machine

00:25:01,990 --> 00:25:05,160
and start the machine again

00:25:07,740 --> 00:25:17,530
I've got some issue with my VM provider

00:25:13,170 --> 00:25:20,050
well I'm gonna rip the band-aid off fast

00:25:17,530 --> 00:25:27,030
and hope this works and if not I'll

00:25:20,050 --> 00:25:27,030
switch laptops I brought a backup okay

00:25:27,820 --> 00:25:36,520
it looks like I may have the ISO cached

00:25:31,980 --> 00:25:38,080
fingers crossed so if you ever need to

00:25:36,520 --> 00:25:40,540
reset your mini coop environment it's

00:25:38,080 --> 00:25:42,610
very easy to commands mini cubed elite

00:25:40,540 --> 00:25:44,950
mini coop start should get you back

00:25:42,610 --> 00:25:49,660
running especially if you have your ISO

00:25:44,950 --> 00:25:53,800
cached and we'll see if I do shortly all

00:25:49,660 --> 00:25:56,230
right if I wanted more information human

00:25:53,800 --> 00:25:58,810
readable API output I could do coop CTL

00:25:56,230 --> 00:26:01,240
described with the same resource

00:25:58,810 --> 00:26:04,780
hopefully you're noticing a pattern here

00:26:01,240 --> 00:26:08,320
these get requests are going to an API

00:26:04,780 --> 00:26:12,520
endpoint slash nodes here we're doing a

00:26:08,320 --> 00:26:15,760
get request on node slash node ID here

00:26:12,520 --> 00:26:20,290
we're doing a described on node slash

00:26:15,760 --> 00:26:22,090
node ID I can also instead of a slash we

00:26:20,290 --> 00:26:24,100
could do a space but this is basically

00:26:22,090 --> 00:26:27,130
the resource type and the resource ID

00:26:24,100 --> 00:26:30,340
and this is just a convenience for doing

00:26:27,130 --> 00:26:35,410
basically a API get request with either

00:26:30,340 --> 00:26:38,920
JSON or yam all data looks like I am

00:26:35,410 --> 00:26:41,820
downloading a binary so we'll have a

00:26:38,920 --> 00:26:44,560
couple minutes while this downloads

00:26:41,820 --> 00:26:48,910
hopefully we all end up with excellent

00:26:44,560 --> 00:26:52,360
Wi-Fi participation okay yeah

00:26:48,910 --> 00:26:54,250
next one observations oh so observations

00:26:52,360 --> 00:26:56,080
from this round now hopefully you know

00:26:54,250 --> 00:27:00,090
what a node is that was the goal for

00:26:56,080 --> 00:27:02,740
this vertical deck of slides here

00:27:00,090 --> 00:27:04,330
kubernetes designed to exist on multiple

00:27:02,740 --> 00:27:06,040
machines if you were building a

00:27:04,330 --> 00:27:09,910
distributed system

00:27:06,040 --> 00:27:12,820
the basic platform itself has high

00:27:09,910 --> 00:27:15,220
availability of node instances and high

00:27:12,820 --> 00:27:17,920
availability of master instances for

00:27:15,220 --> 00:27:20,290
running the API this allows you to scale

00:27:17,920 --> 00:27:22,930
out the platform since you can scale out

00:27:20,290 --> 00:27:25,180
the platform you can also scale out

00:27:22,930 --> 00:27:28,420
within the platform the number of

00:27:25,180 --> 00:27:31,240
containers you're running within the

00:27:28,420 --> 00:27:34,870
bounds of how much resources you have

00:27:31,240 --> 00:27:37,540
oh also the API ambidextrious lis

00:27:34,870 --> 00:27:40,630
supports both JSON and yam also if you

00:27:37,540 --> 00:27:41,440
have a shop that prefers JSON or prefer

00:27:40,630 --> 00:27:42,639
GMO

00:27:41,440 --> 00:27:44,649
you could really standardize on either

00:27:42,639 --> 00:27:47,700
one and kubernetes doesn't really care

00:27:44,649 --> 00:27:47,700
which is really nice

00:27:48,539 --> 00:27:57,429
okay next resource type we'll learn

00:27:51,370 --> 00:28:01,870
about a pod pod is an official

00:27:57,429 --> 00:28:06,039
scientific term for a group of anybody

00:28:01,870 --> 00:28:08,429
know what a pod group of whales it's a

00:28:06,039 --> 00:28:12,789
group of whales like the docker whale

00:28:08,429 --> 00:28:14,830
hahaha it's it's a pod so anytime in

00:28:12,789 --> 00:28:17,830
kubernetes where you have multiple

00:28:14,830 --> 00:28:21,909
containers and you expect them to run

00:28:17,830 --> 00:28:24,970
together this pod analogy is the right

00:28:21,909 --> 00:28:28,629
abstraction for you to use one of my

00:28:24,970 --> 00:28:32,049
co-workers at Red Hat the the steve-o he

00:28:28,629 --> 00:28:34,990
has this quote that I really appreciate

00:28:32,049 --> 00:28:38,529
he says pods scaled together and they

00:28:34,990 --> 00:28:40,480
fail together and this statement is one

00:28:38,529 --> 00:28:43,179
metric I use for trying to figure out

00:28:40,480 --> 00:28:45,759
how to architect my systems on

00:28:43,179 --> 00:28:49,929
kubernetes and so when I think of this

00:28:45,759 --> 00:28:51,820
statement I think of if I have if I have

00:28:49,929 --> 00:28:53,409
something with multiple containers and I

00:28:51,820 --> 00:28:55,659
want to run it together usually here's a

00:28:53,409 --> 00:29:01,029
here's a trick I play on the audience

00:28:55,659 --> 00:29:03,669
can anyone suggest a an instance where I

00:29:01,029 --> 00:29:05,679
would have a web front-end and some type

00:29:03,669 --> 00:29:08,440
of back-end that I want to run together

00:29:05,679 --> 00:29:08,889
any volunteers now that I said it's a

00:29:08,440 --> 00:29:13,570
trap

00:29:08,889 --> 00:29:15,789
no no WordPress is a common example that

00:29:13,570 --> 00:29:19,629
people throw out so WordPress what I

00:29:15,789 --> 00:29:26,250
want my web front-end and my DB back-end

00:29:19,629 --> 00:29:28,840
too co-located in the same pod group

00:29:26,250 --> 00:29:31,210
initially people might think yes because

00:29:28,840 --> 00:29:33,220
that would have them physically located

00:29:31,210 --> 00:29:36,070
on the same machine they'd have low

00:29:33,220 --> 00:29:38,950
latency connections but the real answer

00:29:36,070 --> 00:29:41,559
is absolutely not if you want to be able

00:29:38,950 --> 00:29:44,080
to scale these things independently or

00:29:41,559 --> 00:29:46,509
if you want the web front-end to be able

00:29:44,080 --> 00:29:49,090
to fail and restart without restarting

00:29:46,509 --> 00:29:52,090
your DB then you definitely want them as

00:29:49,090 --> 00:29:54,260
separate abstractions where pods really

00:29:52,090 --> 00:29:56,840
come really shine is

00:29:54,260 --> 00:29:59,530
if you want to attach a what's known as

00:29:56,840 --> 00:30:04,040
a sidecar to a process something like

00:29:59,530 --> 00:30:07,250
additional metrics monitoring or maybe

00:30:04,040 --> 00:30:10,820
some lightweight build process or some

00:30:07,250 --> 00:30:13,160
other thing that can run on the side one

00:30:10,820 --> 00:30:17,690
example that I like to use is if I'm

00:30:13,160 --> 00:30:20,090
doing some light some ops work and I

00:30:17,690 --> 00:30:21,410
have some some logs that are in the

00:30:20,090 --> 00:30:23,990
wrong format and I want to do some

00:30:21,410 --> 00:30:26,450
translation on the logs I could have a

00:30:23,990 --> 00:30:28,580
sidecar container that processes all the

00:30:26,450 --> 00:30:30,140
logs standardizes it in a different

00:30:28,580 --> 00:30:31,970
format and writes it out I should

00:30:30,140 --> 00:30:34,430
probably have that code in the web code

00:30:31,970 --> 00:30:37,310
but if I needed to always attach a

00:30:34,430 --> 00:30:39,140
logging agency agent or metrics agent

00:30:37,310 --> 00:30:41,840
and make sure that any time the web

00:30:39,140 --> 00:30:43,880
front-end dies I always restart the

00:30:41,840 --> 00:30:46,370
metrics agent as well and they're always

00:30:43,880 --> 00:30:49,370
running as a pair if I want them to

00:30:46,370 --> 00:30:52,160
scale together and/or get restarted

00:30:49,370 --> 00:30:55,490
together as a pair that's when I need to

00:30:52,160 --> 00:30:58,310
use this pod analogy so for the purposes

00:30:55,490 --> 00:31:00,050
of this workshop we will only have one

00:30:58,310 --> 00:31:02,690
container per pod so you can almost

00:31:00,050 --> 00:31:03,740
think of pods as containers I just need

00:31:02,690 --> 00:31:06,650
to point out that there's a difference

00:31:03,740 --> 00:31:09,800
you can have multiple containers in a

00:31:06,650 --> 00:31:13,880
pod so let's take a look at some

00:31:09,800 --> 00:31:18,740
examples and let's see Oh half way

00:31:13,880 --> 00:31:22,120
almost encouraging not where I hope to

00:31:18,740 --> 00:31:25,670
be but encouraging okay not a total loss

00:31:22,120 --> 00:31:27,800
the rest of you can do try something

00:31:25,670 --> 00:31:31,720
like this and in fact I might be able to

00:31:27,800 --> 00:31:32,930
try this since I'm still connected to

00:31:31,720 --> 00:31:36,560
gke

00:31:32,930 --> 00:31:38,210
no it rewrote my config well I'm pointed

00:31:36,560 --> 00:31:43,010
my config pointed to the right instance

00:31:38,210 --> 00:31:45,920
now at least at least that's fixed so I

00:31:43,010 --> 00:31:47,560
can restore my gke one now I'm not going

00:31:45,920 --> 00:31:50,960
to mess with it all right

00:31:47,560 --> 00:31:53,510
Coop's et al get pods should allow us to

00:31:50,960 --> 00:31:55,580
list a different type of resource the

00:31:53,510 --> 00:31:57,260
pods resource we should initially see

00:31:55,580 --> 00:31:59,360
when we run this that we get an empty

00:31:57,260 --> 00:32:01,520
response hopefully if you're running

00:31:59,360 --> 00:32:03,980
that on your laptop and you're getting a

00:32:01,520 --> 00:32:05,810
response it's probably an empty response

00:32:03,980 --> 00:32:07,820
so if you're not sure if it's working or

00:32:05,810 --> 00:32:10,490
not it's working you just

00:32:07,820 --> 00:32:12,830
have any pods yet if you would like to

00:32:10,490 --> 00:32:14,960
launch a pod I have an example it'll

00:32:12,830 --> 00:32:19,610
trigger a small download on your laptop

00:32:14,960 --> 00:32:23,090
but that's the scenario we're in okay so

00:32:19,610 --> 00:32:30,410
you can run this curl command and this

00:32:23,090 --> 00:32:32,930
should download a pod file and so here's

00:32:30,410 --> 00:32:35,030
what this spec would look like and we'll

00:32:32,930 --> 00:32:40,130
learn about how to generate this type of

00:32:35,030 --> 00:32:41,860
spec in the next section so you can see

00:32:40,130 --> 00:32:45,080
the type of data is a pod

00:32:41,860 --> 00:32:47,090
it's versioned data like I like I

00:32:45,080 --> 00:32:51,530
promised we would have there's a

00:32:47,090 --> 00:32:53,300
metadata field as expected you could see

00:32:51,530 --> 00:32:55,340
the name of this particular pot or the

00:32:53,300 --> 00:32:57,800
desired name that we would like to give

00:32:55,340 --> 00:33:00,830
the pod and you could see containers is

00:32:57,800 --> 00:33:04,700
a list like I said we only have one in

00:33:00,830 --> 00:33:08,990
here but it's a organized as a list as

00:33:04,700 --> 00:33:11,270
per this bracket here you could see the

00:33:08,990 --> 00:33:15,310
name of the image that we're deploying

00:33:11,270 --> 00:33:18,440
or it's it's given a short name and

00:33:15,310 --> 00:33:22,450
here's the path that we're going to be

00:33:18,440 --> 00:33:26,840
pulling the image from so I'll deploy

00:33:22,450 --> 00:33:28,370
I'll do a coop CTL create on that after

00:33:26,840 --> 00:33:32,270
my environment comes up that should

00:33:28,370 --> 00:33:36,230
deploy the single pod so if we wanted to

00:33:32,270 --> 00:33:39,800
coop CTL can create resources based on a

00:33:36,230 --> 00:33:43,220
specification so this is the spec the

00:33:39,800 --> 00:33:46,360
pod spec that we'll be using to create a

00:33:43,220 --> 00:33:49,940
pod and we could do coop CTL create

00:33:46,360 --> 00:33:53,330
against that local file that pod JSON or

00:33:49,940 --> 00:33:57,410
pod yamo or we could also do coop CTL

00:33:53,330 --> 00:34:00,470
create the dash F stands for file input

00:33:57,410 --> 00:34:02,750
so you could do - F it could be a remote

00:34:00,470 --> 00:34:05,450
file it could be a local file it could

00:34:02,750 --> 00:34:08,300
even be a directory name with multiple

00:34:05,450 --> 00:34:11,170
JSON and or yamo files so there's a

00:34:08,300 --> 00:34:15,350
variety of ways to say create this

00:34:11,170 --> 00:34:17,000
series of assets and I'm using an

00:34:15,350 --> 00:34:17,540
example where we're pulling it over the

00:34:17,000 --> 00:34:19,220
network

00:34:17,540 --> 00:34:23,750
but the ass

00:34:19,220 --> 00:34:25,429
Manifest anyway spec once we're done

00:34:23,750 --> 00:34:27,349
with that we should be able to list pods

00:34:25,429 --> 00:34:29,659
let's see how my progress is going

00:34:27,349 --> 00:34:30,409
making more progress but I can't list

00:34:29,659 --> 00:34:33,159
pods yet

00:34:30,409 --> 00:34:37,250
we'll run quickly back through these

00:34:33,159 --> 00:34:41,060
once my environment comes up if I wanted

00:34:37,250 --> 00:34:43,700
to see resource by type oh if I wanted

00:34:41,060 --> 00:34:46,580
to see the same thing all the details

00:34:43,700 --> 00:34:50,270
for that pod converted into yeah mole I

00:34:46,580 --> 00:34:52,669
can run this and and if you've run it

00:34:50,270 --> 00:34:54,619
you'll notice some major changes I'll

00:34:52,669 --> 00:34:59,270
get back to this as soon as my

00:34:54,619 --> 00:35:01,490
environment boots so observations from

00:34:59,270 --> 00:35:06,560
this section pods are scheduled to be

00:35:01,490 --> 00:35:09,070
run on nodes we should see that in the

00:35:06,560 --> 00:35:11,510
output I'm gonna come back to this one

00:35:09,070 --> 00:35:14,630
yeah we'll see some of that stuff in the

00:35:11,510 --> 00:35:16,460
output the next big abstraction we're

00:35:14,630 --> 00:35:20,090
going to be building with is a service

00:35:16,460 --> 00:35:22,430
so so far we have nodes pods services is

00:35:20,090 --> 00:35:25,910
our third abstraction all right a

00:35:22,430 --> 00:35:28,460
service I think is a difficult thing for

00:35:25,910 --> 00:35:31,010
me to describe because in kubernetes

00:35:28,460 --> 00:35:33,740
there's sometimes a name collision right

00:35:31,010 --> 00:35:34,700
we we talked about node not being node J

00:35:33,740 --> 00:35:38,330
s right

00:35:34,700 --> 00:35:42,140
it's a machine the concept of node as a

00:35:38,330 --> 00:35:45,740
machine service is really seems like it

00:35:42,140 --> 00:35:47,510
was named by a networking engineer right

00:35:45,740 --> 00:35:50,180
where they're talking about here's all

00:35:47,510 --> 00:35:53,540
the endpoints for this whole group of

00:35:50,180 --> 00:35:56,810
micro services hundreds of them right I

00:35:53,540 --> 00:35:58,760
think service the better name for it and

00:35:56,810 --> 00:36:02,020
in my opinion would maybe be load

00:35:58,760 --> 00:36:06,710
balancer because it's meant to be a

00:36:02,020 --> 00:36:09,440
unique network URL that points to a

00:36:06,710 --> 00:36:11,839
scaled set of pods and it'll address

00:36:09,440 --> 00:36:15,200
spread the traffic across all of these

00:36:11,839 --> 00:36:17,960
pods so it really acts software wise as

00:36:15,200 --> 00:36:21,260
a load balancer the interesting thing

00:36:17,960 --> 00:36:23,150
about services is if you start a service

00:36:21,260 --> 00:36:25,880
on mini cube you'll get one type of

00:36:23,150 --> 00:36:29,540
software load balancer locally if you

00:36:25,880 --> 00:36:31,760
start a service on Google's cloud on gke

00:36:29,540 --> 00:36:34,670
they'll actually hook up

00:36:31,760 --> 00:36:39,920
some nice google load balancers instead

00:36:34,670 --> 00:36:42,470
of this defaults service type most folks

00:36:39,920 --> 00:36:44,750
if they have this running on kubernetes

00:36:42,470 --> 00:36:47,480
running on amazon and you request a

00:36:44,750 --> 00:36:53,240
service it'll probably go provision a

00:36:47,480 --> 00:36:56,420
amazon ELB so the implementation of how

00:36:53,240 --> 00:36:58,700
these load balancers are actually

00:36:56,420 --> 00:37:00,650
provisioned from an Operations

00:36:58,700 --> 00:37:03,230
standpoint is completely separated from

00:37:00,650 --> 00:37:05,870
how you would make the request as a user

00:37:03,230 --> 00:37:08,960
you just say I need I need to model

00:37:05,870 --> 00:37:10,730
things so that this group of resources

00:37:08,960 --> 00:37:13,820
can talk to this other large group of

00:37:10,730 --> 00:37:16,640
resources and communication can flow

00:37:13,820 --> 00:37:20,630
both ways but you don't need to say

00:37:16,640 --> 00:37:22,940
specifically what hardware is used in

00:37:20,630 --> 00:37:25,190
order to route that traffic right you're

00:37:22,940 --> 00:37:28,400
just doing distributed systems modeling

00:37:25,190 --> 00:37:31,190
and you're using building blocks to

00:37:28,400 --> 00:37:34,820
convey the right concepts so that this

00:37:31,190 --> 00:37:37,490
solution will be portable to Google's

00:37:34,820 --> 00:37:40,130
cloud to Amazon's cloud or any other

00:37:37,490 --> 00:37:42,890
cloud and that's really one of the real

00:37:40,130 --> 00:37:47,060
compelling aspects of kubernetes is the

00:37:42,890 --> 00:37:48,830
portability let me check in status check

00:37:47,060 --> 00:37:51,290
uh-huh

00:37:48,830 --> 00:37:53,000
I have got to a hundred percent if you

00:37:51,290 --> 00:37:55,130
cancelled your download to help me get

00:37:53,000 --> 00:37:58,220
there thank you very much

00:37:55,130 --> 00:38:01,220
feel free to resume and and hopefully

00:37:58,220 --> 00:38:04,250
you all catch up I'm going to try to

00:38:01,220 --> 00:38:08,450
skip back really quickly to this last

00:38:04,250 --> 00:38:11,570
page on pods and quickly provision a pod

00:38:08,450 --> 00:38:16,130
into my new environment let's see how

00:38:11,570 --> 00:38:21,860
this goes pod created all right I'm back

00:38:16,130 --> 00:38:24,320
up to speed now I can run get nodes much

00:38:21,860 --> 00:38:26,030
faster all right that's why my command

00:38:24,320 --> 00:38:28,310
line was so slow to respond it was

00:38:26,030 --> 00:38:30,290
fighting with the network traffic so I

00:38:28,310 --> 00:38:32,540
could see I have one container the

00:38:30,290 --> 00:38:34,070
status is container creating because

00:38:32,540 --> 00:38:37,550
it's trying to download that container

00:38:34,070 --> 00:38:39,470
over the network so now as soon as I got

00:38:37,550 --> 00:38:41,180
my stuff running of course I flooded the

00:38:39,470 --> 00:38:42,820
network with more more traffic sorry

00:38:41,180 --> 00:38:45,610
about that everyone

00:38:42,820 --> 00:38:48,990
if I want to see specifics on how much

00:38:45,610 --> 00:38:53,320
progress it's gotten as part of this

00:38:48,990 --> 00:38:57,490
container creating progress I can look

00:38:53,320 --> 00:39:00,250
at some of the detailed API output so we

00:38:57,490 --> 00:39:03,340
could see here I did the get pod

00:39:00,250 --> 00:39:04,390
- oh yeah Mille so I have llamo

00:39:03,340 --> 00:39:07,360
formatted output

00:39:04,390 --> 00:39:09,340
it's versioned data the type of data or

00:39:07,360 --> 00:39:12,190
the kind of data is a pod there's

00:39:09,340 --> 00:39:14,530
metadata with a lot more information

00:39:12,190 --> 00:39:16,900
than what we started with remember I

00:39:14,530 --> 00:39:19,780
don't know if you remember but I'll go

00:39:16,900 --> 00:39:24,580
back and take a look at the original

00:39:19,780 --> 00:39:26,860
spec and if we compare these the

00:39:24,580 --> 00:39:28,570
original spec is really short this is it

00:39:26,860 --> 00:39:31,060
we said we want a pod

00:39:28,570 --> 00:39:33,220
there's no creation timestamp that's

00:39:31,060 --> 00:39:35,110
null and there's a spec with our

00:39:33,220 --> 00:39:38,380
containers that's the whole thing that

00:39:35,110 --> 00:39:41,590
we shipped to the API but yet when we

00:39:38,380 --> 00:39:44,740
retrieved it from the API we got all

00:39:41,590 --> 00:39:47,560
kinds of data back right that's because

00:39:44,740 --> 00:39:50,770
the API is giving us two different

00:39:47,560 --> 00:39:53,020
responses like I said we have version

00:39:50,770 --> 00:39:55,930
data we have metadata we have a spec

00:39:53,020 --> 00:39:57,970
this is what we initially asked for now

00:39:55,930 --> 00:40:01,180
some additional fields have been filled

00:39:57,970 --> 00:40:03,870
out in here like there was a creation

00:40:01,180 --> 00:40:07,290
timestamp is gone and got rid of that

00:40:03,870 --> 00:40:10,120
the ports have been filled in

00:40:07,290 --> 00:40:13,810
termination message all this stuff has

00:40:10,120 --> 00:40:16,920
been filled in dns policy oh these are

00:40:13,810 --> 00:40:20,710
all stuff for a and then and then status

00:40:16,920 --> 00:40:22,480
so the spec is what we requested plus a

00:40:20,710 --> 00:40:23,860
lot of extra stuff that's been filled in

00:40:22,480 --> 00:40:26,290
by the server as part of the

00:40:23,860 --> 00:40:29,860
provisioning process like I said if we

00:40:26,290 --> 00:40:33,040
were deploying something on a service to

00:40:29,860 --> 00:40:36,040
Amazon it might go create a TLB so the

00:40:33,040 --> 00:40:38,410
the platform is filling in a lot of

00:40:36,040 --> 00:40:40,810
details for us so we keep our interface

00:40:38,410 --> 00:40:43,780
simple as a developer and say here's the

00:40:40,810 --> 00:40:45,730
minimum of what I need go please fulfill

00:40:43,780 --> 00:40:46,540
this request and then the server fills

00:40:45,730 --> 00:40:48,490
in the

00:40:46,540 --> 00:40:51,730
the details on how it's going to get the

00:40:48,490 --> 00:40:54,730
job done so the way you interact with

00:40:51,730 --> 00:40:57,790
the API is very declarative you stay

00:40:54,730 --> 00:40:59,560
explicitly what you need and then it

00:40:57,790 --> 00:41:03,970
goes and tries to follow through on that

00:40:59,560 --> 00:41:07,480
request so hopefully that's clear based

00:41:03,970 --> 00:41:09,940
on this file that a lot more information

00:41:07,480 --> 00:41:12,880
has been filled in and this status field

00:41:09,940 --> 00:41:14,740
has now been added with a lot of output

00:41:12,880 --> 00:41:18,040
there on how much progress has been made

00:41:14,740 --> 00:41:20,050
so big difference from our before and

00:41:18,040 --> 00:41:22,120
after that's supposed to be the changes

00:41:20,050 --> 00:41:24,880
that you notice and so based on those

00:41:22,120 --> 00:41:27,580
changes you should have learned a couple

00:41:24,880 --> 00:41:31,020
observations one pods are scheduled to

00:41:27,580 --> 00:41:35,490
be run on nodes you can actually see

00:41:31,020 --> 00:41:39,280
there's a specific field in the spec

00:41:35,490 --> 00:41:41,770
called node name and you could see this

00:41:39,280 --> 00:41:44,920
was scheduled to be run on a node called

00:41:41,770 --> 00:41:47,100
mini cube which was my only node so glad

00:41:44,920 --> 00:41:50,710
it landed there

00:41:47,100 --> 00:41:53,020
asynchronous fulfillment of requests so

00:41:50,710 --> 00:41:55,690
I made a request for the pod to be

00:41:53,020 --> 00:41:57,610
scheduled and it happened later whenever

00:41:55,690 --> 00:42:01,120
I'm not sure when but it happened at

00:41:57,610 --> 00:42:03,250
some point given network conditions we

00:42:01,120 --> 00:42:05,020
have declarative specifications we say

00:42:03,250 --> 00:42:07,090
what we need and hopefully the platform

00:42:05,020 --> 00:42:09,100
goes and gets the job done

00:42:07,090 --> 00:42:11,320
automatic health checks and lifecycle

00:42:09,100 --> 00:42:13,810
management for containers are another

00:42:11,320 --> 00:42:16,210
thing that are added into this pod spec

00:42:13,810 --> 00:42:18,910
we're I don't have any health checks

00:42:16,210 --> 00:42:22,500
listed but you can definitely add checks

00:42:18,910 --> 00:42:26,230
and things like volumes in to your

00:42:22,500 --> 00:42:29,470
specification okay that should be your

00:42:26,230 --> 00:42:30,100
observations for pods services are up

00:42:29,470 --> 00:42:33,580
next

00:42:30,100 --> 00:42:37,060
so services we would these are our load

00:42:33,580 --> 00:42:40,690
balancers basically if you wanted to

00:42:37,060 --> 00:42:45,340
contact the pod that we just started you

00:42:40,690 --> 00:42:49,870
can run this command here Coop's ETL

00:42:45,340 --> 00:42:52,800
expose will generate a service after

00:42:49,870 --> 00:42:54,910
that you can run Coop's ETL service and

00:42:52,800 --> 00:42:57,550
the name of the service that you've

00:42:54,910 --> 00:42:58,200
created that should open up a new

00:42:57,550 --> 00:43:03,150
browser

00:42:58,200 --> 00:43:06,829
tab see I probably have full screen on

00:43:03,150 --> 00:43:09,839
that's yeah that should open up a new

00:43:06,829 --> 00:43:10,920
browser tab and if you want to see the

00:43:09,839 --> 00:43:16,050
specific URL

00:43:10,920 --> 00:43:18,390
oh no URL that's because I left off node

00:43:16,050 --> 00:43:21,210
port if you copy the whole example that

00:43:18,390 --> 00:43:25,079
I have in the slides it'll it'll it'll

00:43:21,210 --> 00:43:28,710
cooperate you need this type node port

00:43:25,079 --> 00:43:31,619
added when you're done go ahead and

00:43:28,710 --> 00:43:35,550
clean that up delete that example and we

00:43:31,619 --> 00:43:37,740
will garbage collect that particular pod

00:43:35,550 --> 00:43:40,320
and you'll learn how to do the delete

00:43:37,740 --> 00:43:45,119
operation the API has standard kind of

00:43:40,320 --> 00:43:46,500
Get Set delete list operations so here

00:43:45,119 --> 00:43:49,589
now that you've created a resource

00:43:46,500 --> 00:43:52,109
exposed it you should be able to check

00:43:49,589 --> 00:43:56,010
again many cube service metrics k eights

00:43:52,109 --> 00:43:57,900
and try to reload that pod and it'll

00:43:56,010 --> 00:44:00,630
give you an error that's because the

00:43:57,900 --> 00:44:02,819
load balancer doesn't have a pod to pass

00:44:00,630 --> 00:44:04,500
the traffic through to right so i'm

00:44:02,819 --> 00:44:07,260
trying to force you to hit an error with

00:44:04,500 --> 00:44:11,490
this mini coop service metrics k eights

00:44:07,260 --> 00:44:13,710
it should not route through and when

00:44:11,490 --> 00:44:17,730
you're done you can clean up the service

00:44:13,710 --> 00:44:20,460
as well so that should be basics of just

00:44:17,730 --> 00:44:23,400
creating a single container and routing

00:44:20,460 --> 00:44:28,619
traffic into that container I'm gonna

00:44:23,400 --> 00:44:30,900
try yeah we'll see more examples of how

00:44:28,619 --> 00:44:32,849
to do this much more sophisticated

00:44:30,900 --> 00:44:35,579
examples I would actually never

00:44:32,849 --> 00:44:37,950
recommend creating pods against the API

00:44:35,579 --> 00:44:40,050
directly this is just so you understand

00:44:37,950 --> 00:44:42,390
the fundamentals and so the next section

00:44:40,050 --> 00:44:45,329
makes more sense observations from this

00:44:42,390 --> 00:44:48,930
section service basically means load

00:44:45,329 --> 00:44:50,880
balancer pods and services exist

00:44:48,930 --> 00:44:54,390
independently and have disjoint life

00:44:50,880 --> 00:44:55,829
cycles so I can delete a service which

00:44:54,390 --> 00:44:58,770
is just the network routing information

00:44:55,829 --> 00:45:02,310
or I can delete the pods which are where

00:44:58,770 --> 00:45:05,850
the actual workloads are running and or

00:45:02,310 --> 00:45:08,460
create multiple services and

00:45:05,850 --> 00:45:11,190
they don't need to exist at the same

00:45:08,460 --> 00:45:12,690
time you may have errors but deleting

00:45:11,190 --> 00:45:19,670
one doesn't automatically delete the

00:45:12,690 --> 00:45:23,400
other deployments are the next topic so

00:45:19,670 --> 00:45:26,460
this is the better way to schedule pods

00:45:23,400 --> 00:45:29,250
to actual scheduled containers on your

00:45:26,460 --> 00:45:33,330
system you want to use the coupe's ETL

00:45:29,250 --> 00:45:37,140
run command I'm going to add a couple

00:45:33,330 --> 00:45:40,740
extra flags just to make it fancier will

00:45:37,140 --> 00:45:44,450
do coops ETL run I'll make a deployment

00:45:40,740 --> 00:45:49,530
called metrics k-8 I'll use this image

00:45:44,450 --> 00:45:51,840
from Quay or ki depending on how you

00:45:49,530 --> 00:45:56,010
pronounce it I'm gonna add this expose

00:45:51,840 --> 00:45:58,590
flag because this will automatically

00:45:56,010 --> 00:46:00,570
create a service as well

00:45:58,590 --> 00:46:03,330
we'll use the node port type service

00:46:00,570 --> 00:46:05,700
which works well on many Kubb and I'm

00:46:03,330 --> 00:46:08,610
also gonna add this a couple extra flags

00:46:05,700 --> 00:46:11,700
just to complicate it even more - -

00:46:08,610 --> 00:46:14,190
dry-run says don't actually pass this to

00:46:11,700 --> 00:46:18,510
the API at all just generate the

00:46:14,190 --> 00:46:20,850
specification and then output it as yeah

00:46:18,510 --> 00:46:23,670
Mille so we're basically gonna generate

00:46:20,850 --> 00:46:26,040
a spec that would be shipped to the API

00:46:23,670 --> 00:46:27,570
and then instead we're just gonna save

00:46:26,040 --> 00:46:30,170
it to disk and we're not going to touch

00:46:27,570 --> 00:46:33,720
the API at all this is how you would

00:46:30,170 --> 00:46:36,450
generate new specifications to onboard a

00:46:33,720 --> 00:46:38,580
new web developer perhaps right which is

00:46:36,450 --> 00:46:41,670
something all of you will need to convey

00:46:38,580 --> 00:46:44,400
if you want to adopt kubernetes at your

00:46:41,670 --> 00:46:49,020
shop so i'm gonna copy and paste this

00:46:44,400 --> 00:46:51,740
whole big long string whoops that's not

00:46:49,020 --> 00:46:51,740
pasting

00:46:55,860 --> 00:47:05,310
copy this guy okay

00:47:01,860 --> 00:47:08,370
I paste this string in and save this as

00:47:05,310 --> 00:47:10,650
a file called deployment yeah Mille we

00:47:08,370 --> 00:47:15,780
could take a look at that see what we've

00:47:10,650 --> 00:47:19,530
generated there is a service with a API

00:47:15,780 --> 00:47:23,010
version type metadata spec and status

00:47:19,530 --> 00:47:25,050
and then we also see this is new if you

00:47:23,010 --> 00:47:28,560
have an old version of coop CTL they

00:47:25,050 --> 00:47:31,410
they there was a bug that this separator

00:47:28,560 --> 00:47:33,510
was not added this triple dash is a

00:47:31,410 --> 00:47:35,400
separator between resource 1 and

00:47:33,510 --> 00:47:37,800
resource two so here's our second

00:47:35,400 --> 00:47:40,740
resource we've generated a service and a

00:47:37,800 --> 00:47:42,870
deployment and if you have the latest

00:47:40,740 --> 00:47:44,850
command-line tools they fixed they've

00:47:42,870 --> 00:47:47,910
added in the separator so you can output

00:47:44,850 --> 00:47:50,990
both but there used to be a bug and my

00:47:47,910 --> 00:47:54,120
slides might hint that depending on

00:47:50,990 --> 00:47:56,730
depending on which version of coop CTL

00:47:54,120 --> 00:48:00,270
you're using you may have to manually

00:47:56,730 --> 00:48:05,060
add in that triple dash in between the

00:48:00,270 --> 00:48:05,060
deployment and the spec and the service

00:48:05,180 --> 00:48:11,610
question for the audience can anyone

00:48:07,530 --> 00:48:14,400
think of any other way using a maybe

00:48:11,610 --> 00:48:16,830
perhaps a non-standard resource type

00:48:14,400 --> 00:48:21,870
maybe a resource type that doesn't have

00:48:16,830 --> 00:48:23,610
a kind or a status or there was one I

00:48:21,870 --> 00:48:26,310
mentioned earlier there's a list

00:48:23,610 --> 00:48:28,710
operator in the API so that's another

00:48:26,310 --> 00:48:30,930
way to fix this instead of adding the

00:48:28,710 --> 00:48:34,800
triple - between the two resources in a

00:48:30,930 --> 00:48:36,990
yamo file you can use the brackets and

00:48:34,800 --> 00:48:41,340
JavaScript and make it a list of

00:48:36,990 --> 00:48:45,660
resources but I'm glad the API clients

00:48:41,340 --> 00:48:49,050
fixed so let's launch a copy of this we

00:48:45,660 --> 00:48:52,530
have our local resource file I'm going

00:48:49,050 --> 00:48:55,830
to get one last look at it this is going

00:48:52,530 --> 00:49:02,040
to be deploying this container on that

00:48:55,830 --> 00:49:04,670
port let's go ahead and initialize that

00:49:02,040 --> 00:49:04,670
deployment

00:49:05,150 --> 00:49:12,000
that should have created two resource

00:49:07,680 --> 00:49:15,120
types and we could see if we do coop CTL

00:49:12,000 --> 00:49:16,940
get resource type 1 comma resource type

00:49:15,120 --> 00:49:21,150
2 this is the trick you could actually

00:49:16,940 --> 00:49:25,770
say give me both resource types and I've

00:49:21,150 --> 00:49:28,080
got 2 again 2 API responses this is how

00:49:25,770 --> 00:49:35,910
many pods I'm running and this is how

00:49:28,080 --> 00:49:38,550
many services I'm running and then to

00:49:35,910 --> 00:49:43,100
actually give this a test let's view it

00:49:38,550 --> 00:49:47,070
and our web browser here we go

00:49:43,100 --> 00:49:49,920
to delete the service yet the command to

00:49:47,070 --> 00:49:54,300
delete a service you can run coupe CTL

00:49:49,920 --> 00:49:59,400
delete service and then the name of the

00:49:54,300 --> 00:50:02,760
service slash service name and if you're

00:49:59,400 --> 00:50:07,500
unsure what the name of your service is

00:50:02,760 --> 00:50:10,650
you can do coupe CTL get service or if

00:50:07,500 --> 00:50:17,670
you're a lazy typer SVC that's shorthand

00:50:10,650 --> 00:50:20,940
for service so here's my this is some

00:50:17,670 --> 00:50:25,620
contributor metrics from kubernetes that

00:50:20,940 --> 00:50:31,830
that were displaying here any questions

00:50:25,620 --> 00:50:34,620
about that section No all right

00:50:31,830 --> 00:50:36,990
replication scaling up the metrics

00:50:34,620 --> 00:50:39,780
deployment to 3 replicas now that we

00:50:36,990 --> 00:50:41,610
have an easy way to initialize new

00:50:39,780 --> 00:50:44,010
container based environments let's try

00:50:41,610 --> 00:50:47,700
scaling them I'm gonna do coops ETL

00:50:44,010 --> 00:50:50,670
scale we're going to use the scale

00:50:47,700 --> 00:50:54,210
command on this type of resource on a

00:50:50,670 --> 00:50:58,140
deployment this deploy is shorthand for

00:50:54,210 --> 00:51:01,800
deployment and the actual resource ID a

00:50:58,140 --> 00:51:05,910
resource name is metrics Kate's and I'm

00:51:01,800 --> 00:51:12,080
going to set the replicas - - replicas

00:51:05,910 --> 00:51:12,080
equal three need to make this

00:51:14,780 --> 00:51:17,530
okay

00:51:20,480 --> 00:51:24,799
can everyone read I'll bump it up one

00:51:24,380 --> 00:51:26,990
more

00:51:24,799 --> 00:51:32,049
that's probably readable yeah anyone

00:51:26,990 --> 00:51:32,049
have problems reading that no okay

00:51:34,530 --> 00:51:41,130
so now I should be able to check how

00:51:38,190 --> 00:51:46,410
many pods I have get Pio is short for

00:51:41,130 --> 00:51:48,540
pods same output whether I do that or P

00:51:46,410 --> 00:51:55,560
ODS but you could see I've scaled to

00:51:48,540 --> 00:51:58,950
three if I want to here's another nice

00:51:55,560 --> 00:52:02,400
coop CTL command line trick not

00:51:58,950 --> 00:52:05,280
recommended but if you're hacking around

00:52:02,400 --> 00:52:08,070
you can run we learned about the get

00:52:05,280 --> 00:52:10,890
operation we learned about how to do a

00:52:08,070 --> 00:52:12,990
list operation by just leaving leaving

00:52:10,890 --> 00:52:14,640
off the resource name or we just say the

00:52:12,990 --> 00:52:18,780
resource type and no name and that's

00:52:14,640 --> 00:52:22,170
basically a list ya get with no resource

00:52:18,780 --> 00:52:27,840
name is list get with a specific

00:52:22,170 --> 00:52:33,420
resource name is an actual API get this

00:52:27,840 --> 00:52:35,940
edit is a lot like a API put almost so

00:52:33,420 --> 00:52:39,120
let's try this this should open up the

00:52:35,940 --> 00:52:43,320
resource the remote resource in your

00:52:39,120 --> 00:52:44,850
editor so for me this is VI I can go

00:52:43,320 --> 00:52:53,340
down here and I could set the number of

00:52:44,850 --> 00:52:55,590
replicas to two and then type in WQ down

00:52:53,340 --> 00:52:59,060
here or save and exit depending on your

00:52:55,590 --> 00:53:02,400
editor and look what it did it actually

00:52:59,060 --> 00:53:04,380
says that the file was edited let me

00:53:02,400 --> 00:53:06,630
open it again and go ahead and check

00:53:04,380 --> 00:53:10,650
yeah it looks like now it says replicas

00:53:06,630 --> 00:53:12,360
r2 in the spec and if I try to do the

00:53:10,650 --> 00:53:14,670
same thing it says edit cancelled no

00:53:12,360 --> 00:53:19,050
changes are made and noticed that I left

00:53:14,670 --> 00:53:20,490
it at two right if we do get pods looks

00:53:19,050 --> 00:53:24,270
like it's shut in one down once

00:53:20,490 --> 00:53:27,090
terminating we have two available if I

00:53:24,270 --> 00:53:28,920
do edit again we could see there's

00:53:27,090 --> 00:53:31,830
another there's another field there's

00:53:28,920 --> 00:53:35,310
our spec which says we want two replicas

00:53:31,830 --> 00:53:38,040
there's also further down our status

00:53:35,310 --> 00:53:39,780
available replicas this one I could set

00:53:38,040 --> 00:53:45,090
this to whatever I want it's not going

00:53:39,780 --> 00:53:46,970
to matter yeah doesn't increase my

00:53:45,090 --> 00:53:50,360
number of available req replicas

00:53:46,970 --> 00:53:53,990
but I can always go in and adjust the

00:53:50,360 --> 00:53:57,020
spec in a very declarative way say this

00:53:53,990 --> 00:53:59,510
is what I need don't worry about I don't

00:53:57,020 --> 00:54:04,310
need to worry about how it happens and

00:53:59,510 --> 00:54:06,950
hopefully I end up getting what I need I

00:54:04,310 --> 00:54:11,270
just update the spec the platform

00:54:06,950 --> 00:54:12,980
updates the status field right I can try

00:54:11,270 --> 00:54:14,930
to make changes to it but that's updated

00:54:12,980 --> 00:54:19,070
by the platform does that make sense

00:54:14,930 --> 00:54:22,490
spec fields you set its declarative the

00:54:19,070 --> 00:54:24,740
status field is the reality of what's

00:54:22,490 --> 00:54:27,200
actually happening and so what

00:54:24,740 --> 00:54:31,070
kubernetes continually tries to do is

00:54:27,200 --> 00:54:33,680
compare the spec and the status look for

00:54:31,070 --> 00:54:37,520
mismatches and if you've said my

00:54:33,680 --> 00:54:39,740
replicas equals two and you only have

00:54:37,520 --> 00:54:42,440
one in the status it's going to continue

00:54:39,740 --> 00:54:46,070
trying to spin up more containers until

00:54:42,440 --> 00:54:50,240
your spec is fulfilled right this makes

00:54:46,070 --> 00:54:51,800
it very easy to do a lot of complex

00:54:50,240 --> 00:54:53,240
stuff you just say what you need and

00:54:51,800 --> 00:54:57,410
hopefully the system goes and tries to

00:54:53,240 --> 00:54:59,360
achieve that result save and quit we

00:54:57,410 --> 00:55:03,380
learned what happens there auto recovery

00:54:59,360 --> 00:55:05,780
ooh this is a killer killer example and

00:55:03,380 --> 00:55:09,370
I'm gonna speed through it because we're

00:55:05,780 --> 00:55:11,870
running shorter on time auto recovery

00:55:09,370 --> 00:55:17,150
here's one way we can watch this in

00:55:11,870 --> 00:55:22,280
action this get pods - - watch or a

00:55:17,150 --> 00:55:26,180
single - W we'll do the list operation

00:55:22,280 --> 00:55:28,940
from the API but it will leave the API

00:55:26,180 --> 00:55:32,030
request the network socket open and it

00:55:28,940 --> 00:55:35,240
will continue listening and watching for

00:55:32,030 --> 00:55:37,970
new pods so if I do get pods with this

00:55:35,240 --> 00:55:40,760
watch flag you could see it didn't

00:55:37,970 --> 00:55:44,110
return the bash prompt to me then I

00:55:40,760 --> 00:55:48,130
could do in another window I have a

00:55:44,110 --> 00:55:53,750
squirrely little bash ism here that will

00:55:48,130 --> 00:55:56,960
do coop CTL get pods it will cut out the

00:55:53,750 --> 00:55:59,540
name of the top three of them

00:55:56,960 --> 00:56:02,540
and then normalize it into a list I'll

00:55:59,540 --> 00:56:04,640
do this in I'll do this command in two

00:56:02,540 --> 00:56:07,089
sections just so it's super clear what

00:56:04,640 --> 00:56:07,089
I'm doing

00:56:07,430 --> 00:56:20,630
let me grab another terminal okay

00:56:13,609 --> 00:56:23,660
so first I'll run the nested command the

00:56:20,630 --> 00:56:25,849
nested Coop's ETL get command and you

00:56:23,660 --> 00:56:30,380
could see what it normalizes everything

00:56:25,849 --> 00:56:36,050
to is just a list of three pod names

00:56:30,380 --> 00:56:39,020
from coops ETL get pods so it's it's

00:56:36,050 --> 00:56:41,320
three random pods out of that list so if

00:56:39,020 --> 00:56:45,950
I run this command this should delete

00:56:41,320 --> 00:56:49,220
three random pods okay so I'm gonna move

00:56:45,950 --> 00:56:51,830
this down here and we should be able to

00:56:49,220 --> 00:56:54,530
watch in the upper window how many pods

00:56:51,830 --> 00:56:57,170
are in action as we're running this

00:56:54,530 --> 00:56:59,359
command and you can see some of it gets

00:56:57,170 --> 00:57:02,240
listed twice but we could just

00:56:59,359 --> 00:57:06,220
continually take like shotgun blasts of

00:57:02,240 --> 00:57:10,280
damage across our dashboard and delete

00:57:06,220 --> 00:57:13,130
pods groups at a time and the API will

00:57:10,280 --> 00:57:16,310
continually look what's the spec what's

00:57:13,130 --> 00:57:18,830
the status is there a mismatch and do I

00:57:16,310 --> 00:57:21,500
need to create more resources right so

00:57:18,830 --> 00:57:23,480
this simulation I'm doing here of

00:57:21,500 --> 00:57:25,790
deleting pods this is actually using the

00:57:23,480 --> 00:57:28,040
API but if it was a whole node that fell

00:57:25,790 --> 00:57:31,790
off line with a hundred pods running on

00:57:28,040 --> 00:57:33,910
it the API would notice this nodes

00:57:31,790 --> 00:57:37,369
unavailable these pods are unavailable

00:57:33,910 --> 00:57:39,470
let's spin up these pods on other

00:57:37,369 --> 00:57:42,230
available nodes and get us back to where

00:57:39,470 --> 00:57:44,240
we need to be from a spec perspective

00:57:42,230 --> 00:57:47,630
and let's make sure spec and status are

00:57:44,240 --> 00:57:50,000
in agreement right any questions about

00:57:47,630 --> 00:57:51,380
that now

00:57:50,000 --> 00:57:52,730
feel free to follow up later for

00:57:51,380 --> 00:57:57,140
questions I know it's hard to do in a

00:57:52,730 --> 00:58:00,250
big room observations from this section

00:57:57,140 --> 00:58:03,440
you can use the dry run flag to simulate

00:58:00,250 --> 00:58:07,369
interactions with the API a deployment

00:58:03,440 --> 00:58:08,380
spec contains a pod spec hopefully you

00:58:07,369 --> 00:58:11,740
notice that in there

00:58:08,380 --> 00:58:14,339
this final section on learning API

00:58:11,740 --> 00:58:16,569
object primitives is about replica sets

00:58:14,339 --> 00:58:18,609
replica sets are kind of hidden from

00:58:16,569 --> 00:58:21,039
your view but this is actually doing all

00:58:18,609 --> 00:58:23,920
the dirty work of a deployment so here's

00:58:21,039 --> 00:58:28,299
another let's do instead of pods watch

00:58:23,920 --> 00:58:30,789
I'm going to do deployments watch this

00:58:28,299 --> 00:58:35,680
basically is going to give us a kind of

00:58:30,789 --> 00:58:38,829
a dashboard for how many containers we

00:58:35,680 --> 00:58:42,000
have running you can see it's updating

00:58:38,829 --> 00:58:45,490
when I did that pod delete statement

00:58:42,000 --> 00:58:51,250
here is our dashboard that's up and

00:58:45,490 --> 00:58:54,069
running if I wanted to do a deployment I

00:58:51,250 --> 00:58:55,960
could run a command like this a couple

00:58:54,069 --> 00:59:00,970
ways I could do it but I'll run a

00:58:55,960 --> 00:59:03,730
command like this set image coop CTL set

00:59:00,970 --> 00:59:06,250
image I'm going to set the image on the

00:59:03,730 --> 00:59:10,539
deployment resource the resource name is

00:59:06,250 --> 00:59:18,240
metrics Kate's and I'm going to set to

00:59:10,539 --> 00:59:18,240
this particular is there an extra

00:59:20,380 --> 00:59:28,119
I think this supposed to be image equals

00:59:22,210 --> 00:59:31,839
I think let's see let's see if my copy

00:59:28,119 --> 00:59:34,480
and paste works first okay it's whatever

00:59:31,839 --> 00:59:36,220
is in my example don't don't don't

00:59:34,480 --> 00:59:39,430
follow what I typed okay but we could

00:59:36,220 --> 00:59:42,039
see here that there's four desired

00:59:39,430 --> 00:59:43,599
currently there's five two of them are

00:59:42,039 --> 00:59:48,099
up to date we're doing the rolling

00:59:43,599 --> 00:59:49,839
deployment across the dashboard that we

00:59:48,099 --> 00:59:52,769
have so right here it says contributor

00:59:49,839 --> 00:59:56,049
metrics at the very top and if i refresh

00:59:52,769 --> 00:59:59,009
still getting the old containers but at

00:59:56,049 --> 01:00:04,240
some point i think it switches to

00:59:59,009 --> 01:00:06,549
kubernetes metrics or some guess it's

01:00:04,240 --> 01:00:09,910
taken a while to spin up the additional

01:00:06,549 --> 01:00:13,690
containers but this should do a rolling

01:00:09,910 --> 01:00:15,579
update with zero downtime so that's a

01:00:13,690 --> 01:00:18,309
nice feature that usually people have

01:00:15,579 --> 01:00:20,829
kind of packaged up outside of their

01:00:18,309 --> 01:00:23,789
platform or maintained on their own or

01:00:20,829 --> 01:00:25,900
use some off-the-shelf deployment tool

01:00:23,789 --> 01:00:28,480
or there's a variety of different ways

01:00:25,900 --> 01:00:30,819
to stitch all that together but if you

01:00:28,480 --> 01:00:33,519
really want deployments to be part of

01:00:30,819 --> 01:00:36,460
and code delivery to be something that's

01:00:33,519 --> 01:00:38,740
testable now it is with kubernetes you

01:00:36,460 --> 01:00:42,150
can have you can run your delivery

01:00:38,740 --> 01:00:45,309
simulation locally on your laptop and

01:00:42,150 --> 01:00:48,579
since i'm able to run many containers i

01:00:45,309 --> 01:00:52,539
can actually do a full CI pass locally

01:00:48,579 --> 01:00:54,910
before I promote my changes and I have

01:00:52,539 --> 01:00:58,839
some examples on how to do that in our

01:00:54,910 --> 01:01:00,670
final section let me do one more reload

01:00:58,839 --> 01:01:03,700
uh-huh there the title changed to come

01:01:00,670 --> 01:01:07,569
kubernetes contributor metrics because

01:01:03,700 --> 01:01:11,769
our rolling deployment has finally got

01:01:07,569 --> 01:01:14,289
us all across two fully healthy for for

01:01:11,769 --> 01:01:18,549
requested for desired for up to date and

01:01:14,289 --> 01:01:22,660
for available so that was a full rolling

01:01:18,549 --> 01:01:27,789
update across four containers the way

01:01:22,660 --> 01:01:30,819
that worked is it used a replica set for

01:01:27,789 --> 01:01:32,730
deployment number one and a replica set

01:01:30,819 --> 01:01:36,240
for deployment number two

01:01:32,730 --> 01:01:40,320
we initially had four replicas in the

01:01:36,240 --> 01:01:42,540
initial replica set one and we started

01:01:40,320 --> 01:01:45,089
scaling down replicas set one at the

01:01:42,540 --> 01:01:48,329
same rate we start scaling up replica

01:01:45,089 --> 01:01:50,880
set two and so that's one of the ways

01:01:48,329 --> 01:01:55,280
this rolling deployment is carried out

01:01:50,880 --> 01:02:00,500
we can check a history of these rollouts

01:01:55,280 --> 01:02:03,480
by running a coop CTL roll out history

01:02:00,500 --> 01:02:06,900
for that particular deployment and we

01:02:03,480 --> 01:02:07,619
can even do a rollback with a command

01:02:06,900 --> 01:02:12,000
alike

01:02:07,619 --> 01:02:14,720
roll out undo so here's that going into

01:02:12,000 --> 01:02:17,730
effect rolling back all of our changes

01:02:14,720 --> 01:02:19,920
with the command line kind of acting

01:02:17,730 --> 01:02:21,780
like a dashboard for how how much

01:02:19,920 --> 01:02:25,560
progress our deploy our deployment is

01:02:21,780 --> 01:02:29,040
getting should be all rolled back so now

01:02:25,560 --> 01:02:31,530
I should be able to go here and if i

01:02:29,040 --> 01:02:34,140
refresh says kubernetes contributor

01:02:31,530 --> 01:02:39,890
metrics it should be back to contributor

01:02:34,140 --> 01:02:42,390
metrics right any questions on that one

01:02:39,890 --> 01:02:45,510
hopefully not because we don't have time

01:02:42,390 --> 01:02:48,780
for questions sorry folks all right all

01:02:45,510 --> 01:02:51,300
right clean up to clean up all of this

01:02:48,780 --> 01:02:54,660
that you've done so far you could run a

01:02:51,300 --> 01:02:58,230
coop CTL delete of both the service and

01:02:54,660 --> 01:03:01,740
the deployments list them both together

01:02:58,230 --> 01:03:04,200
on the command line delete service comma

01:03:01,740 --> 01:03:06,450
deployment and then the resource name

01:03:04,200 --> 01:03:10,430
and since both resources share the same

01:03:06,450 --> 01:03:13,440
name you can use a comma separated

01:03:10,430 --> 01:03:16,589
resource identifier to delete both

01:03:13,440 --> 01:03:19,410
there's our service that should delete

01:03:16,589 --> 01:03:22,730
our deployment and our deeding deleting

01:03:19,410 --> 01:03:26,670
our deployment will then delete our pods

01:03:22,730 --> 01:03:29,579
right here's all of our pods getting

01:03:26,670 --> 01:03:32,690
shut down and going away getting cleaned

01:03:29,579 --> 01:03:32,690
up by the deployment

01:03:34,460 --> 01:03:39,450
observations from this section the API

01:03:36,809 --> 01:03:42,019
allows you to watch operations in

01:03:39,450 --> 01:03:44,309
addition to doing get set and list

01:03:42,019 --> 01:03:47,150
replica sets provide lifecycle

01:03:44,309 --> 01:03:49,950
management for these pod resources and

01:03:47,150 --> 01:03:54,089
deployments will then create a replica

01:03:49,950 --> 01:03:57,539
set to manage replication during during

01:03:54,089 --> 01:04:01,009
that deployment window and and switch

01:03:57,539 --> 01:04:01,009
over from one deployment to another

01:04:06,820 --> 01:04:11,490
we're running short on time so this

01:04:09,040 --> 01:04:14,440
would normally be where I'd have a pause

01:04:11,490 --> 01:04:17,680
the content that I've done so far we've

01:04:14,440 --> 01:04:20,440
gone through two chapters that I have

01:04:17,680 --> 01:04:23,260
said separated into modules I have a

01:04:20,440 --> 01:04:26,230
module on how to set up your laptop with

01:04:23,260 --> 01:04:28,990
many Kubb and then a module on learning

01:04:26,230 --> 01:04:33,040
coupe CTL tricks hopefully at this point

01:04:28,990 --> 01:04:35,970
you know some basics of coop CTL and can

01:04:33,040 --> 01:04:40,619
maybe name five kubernetes primitives

01:04:35,970 --> 01:04:43,420
the next section I have I think I might

01:04:40,619 --> 01:04:45,490
I'll do a pull of hands and then we'll

01:04:43,420 --> 01:04:48,430
see what people want to see more of

01:04:45,490 --> 01:04:50,890
because I have two more sections and

01:04:48,430 --> 01:04:53,349
only enough time for one of the two so

01:04:50,890 --> 01:04:56,890
I'll ask you all how many folks are

01:04:53,349 --> 01:05:00,700
interested in architecture raise your

01:04:56,890 --> 01:05:02,680
hand for architecture oh yeah yeah but

01:05:00,700 --> 01:05:04,900
the other option before yeah everyone's

01:05:02,680 --> 01:05:06,760
like no architecture who cares what the

01:05:04,900 --> 01:05:10,750
other option is all right the other

01:05:06,760 --> 01:05:13,540
option is local development tricks with

01:05:10,750 --> 01:05:18,819
many cube local development I show you

01:05:13,540 --> 01:05:21,760
how to use a volume from your laptop and

01:05:18,819 --> 01:05:25,030
hook up a local source source code

01:05:21,760 --> 01:05:27,369
folder into the mini coop VM so you can

01:05:25,030 --> 01:05:30,730
do live development I think live

01:05:27,369 --> 01:05:33,700
development is really what you need in

01:05:30,730 --> 01:05:36,220
order to make this stuff realistic and

01:05:33,700 --> 01:05:38,349
to be able to hand off any of this

01:05:36,220 --> 01:05:42,190
information to your web team if you

01:05:38,349 --> 01:05:44,260
can't convey to them how to do how to

01:05:42,190 --> 01:05:48,040
get quick feedback while developing with

01:05:44,260 --> 01:05:49,230
containers then this isn't faster you

01:05:48,040 --> 01:05:51,010
know they're gonna have a slower

01:05:49,230 --> 01:05:52,750
development lifecycle

01:05:51,010 --> 01:05:56,530
if they're waiting on a docker build

01:05:52,750 --> 01:05:57,790
between each change of CSS values or

01:05:56,530 --> 01:05:59,859
something like you know for web

01:05:57,790 --> 01:06:02,980
developers they really need to see live

01:05:59,859 --> 01:06:05,290
development I think so this this is my

01:06:02,980 --> 01:06:09,250
latest chapter that I just finished

01:06:05,290 --> 01:06:11,589
cooking this morning so hopefully it

01:06:09,250 --> 01:06:13,810
works other than that we have

01:06:11,589 --> 01:06:16,420
architecture and that really goes into

01:06:13,810 --> 01:06:19,060
how these requests get fulfilled and

01:06:16,420 --> 01:06:21,010
what happens when you type in coop CTL

01:06:19,060 --> 01:06:23,710
where the request goes where the data

01:06:21,010 --> 01:06:26,290
gets stored and how the operations

01:06:23,710 --> 01:06:30,070
behind the scenes actually gets plumbed

01:06:26,290 --> 01:06:32,770
together how many people our ops and

01:06:30,070 --> 01:06:34,930
infrastructure folks versus web dev ops

01:06:32,770 --> 01:06:38,130
and infrastructure I'm in the ops room

01:06:34,930 --> 01:06:41,830
huh all right and then how many web dev

01:06:38,130 --> 01:06:44,440
I think it's less on web dev but I have

01:06:41,830 --> 01:06:46,300
it available as a standalone module and

01:06:44,440 --> 01:06:49,210
if I have time I'll try to speed through

01:06:46,300 --> 01:06:51,520
that section as well okay it should be

01:06:49,210 --> 01:06:54,060
they're both easy to follow along on

01:06:51,520 --> 01:06:58,690
your own but this one's a little bit

01:06:54,060 --> 01:07:01,480
nicer to talk over I think so for our

01:06:58,690 --> 01:07:03,570
last well we'll see how much time we

01:07:01,480 --> 01:07:06,040
have architecture let's get at it

01:07:03,570 --> 01:07:08,800
kubernetes we introduced kubernetes

01:07:06,040 --> 01:07:11,790
before you're managing distributed

01:07:08,800 --> 01:07:16,440
systems highly available control plane

01:07:11,790 --> 01:07:24,820
uses workloads run via pod

01:07:16,440 --> 01:07:27,910
specifications yeah you can you can

01:07:24,820 --> 01:07:30,490
actually do a lot of damage to the

01:07:27,910 --> 01:07:32,950
control plane and have the whole system

01:07:30,490 --> 01:07:35,410
keep running and I'll try to show a

01:07:32,950 --> 01:07:40,600
couple examples of that in this in this

01:07:35,410 --> 01:07:43,480
section to present to persist all of the

01:07:40,600 --> 01:07:45,540
platform state in kubernetes there's a

01:07:43,480 --> 01:07:48,430
database that's used called Etsy D

01:07:45,540 --> 01:07:50,820
that's basically where all the

01:07:48,430 --> 01:07:54,910
statefulness of the whole platform

01:07:50,820 --> 01:07:56,920
resides so the first piece of kubernetes

01:07:54,910 --> 01:08:01,000
architecture I'd like to introduce is

01:07:56,920 --> 01:08:04,240
@cd it's a distributed key-value store

01:08:01,000 --> 01:08:08,580
it implements the raft consensus

01:08:04,240 --> 01:08:10,780
protocol if you like reading

01:08:08,580 --> 01:08:13,890
specifications on how things are

01:08:10,780 --> 01:08:18,540
implemented you could look up raft

01:08:13,890 --> 01:08:21,880
there's a there's also a theorem about

01:08:18,540 --> 01:08:25,180
highly available distributed databases

01:08:21,880 --> 01:08:28,810
called the cap theorem and this theory

01:08:25,180 --> 01:08:30,069
states that you can you have to pick two

01:08:28,810 --> 01:08:31,830
out of these three

01:08:30,069 --> 01:08:34,770
you can't optimize on all three

01:08:31,830 --> 01:08:36,710
because if you optimize on consistency

01:08:34,770 --> 01:08:39,210
and availability you might not have

01:08:36,710 --> 01:08:41,430
partition tolerance or if you optimize

01:08:39,210 --> 01:08:44,430
on availability you might lose

01:08:41,430 --> 01:08:46,740
consistency or you know this is a kind

01:08:44,430 --> 01:08:49,380
of a trade-off you you have to pick the

01:08:46,740 --> 01:08:52,440
right tool for the job and for this

01:08:49,380 --> 01:08:56,640
particular job what we want is a

01:08:52,440 --> 01:09:00,120
database that can survive failures we

01:08:56,640 --> 01:09:02,370
want a control plane that can be shot

01:09:00,120 --> 01:09:06,089
full of holes and still keep running and

01:09:02,370 --> 01:09:12,600
so @cd is one of the tools that helps

01:09:06,089 --> 01:09:14,790
achieve that this is a chart a sizing

01:09:12,600 --> 01:09:17,850
chart for how many instances you should

01:09:14,790 --> 01:09:22,020
have in your at CD cluster and this is

01:09:17,850 --> 01:09:24,540
based on your tolerance for loss of

01:09:22,020 --> 01:09:26,040
machines right so if you're running in

01:09:24,540 --> 01:09:28,500
an environment where your machines

01:09:26,040 --> 01:09:32,280
frequently just go missing for no reason

01:09:28,500 --> 01:09:35,550
then you might want to have a larger

01:09:32,280 --> 01:09:39,150
cluster size with a higher level of

01:09:35,550 --> 01:09:42,510
fault tolerance if you have a net CD

01:09:39,150 --> 01:09:45,750
cluster with nine nodes then in order to

01:09:42,510 --> 01:09:49,740
achieve consensus you need the majority

01:09:45,750 --> 01:09:52,410
of these nodes to agree let CD tries to

01:09:49,740 --> 01:09:56,400
always have consistent agreement between

01:09:52,410 --> 01:09:58,710
the the master control plane and so in

01:09:56,400 --> 01:10:02,730
order for your right to go into the

01:09:58,710 --> 01:10:05,760
database it won't it won't acknowledge

01:10:02,730 --> 01:10:09,030
that the right has succeeded until the

01:10:05,760 --> 01:10:11,790
right has been replicated to a majority

01:10:09,030 --> 01:10:13,470
of the nodes in the database cluster so

01:10:11,790 --> 01:10:16,860
you're not just writing to the master

01:10:13,470 --> 01:10:18,180
and then hoping DB replication gets the

01:10:16,860 --> 01:10:21,030
Jew the rest of the job done

01:10:18,180 --> 01:10:23,400
you're actually writing to one of many

01:10:21,030 --> 01:10:26,160
masters that then replicates it and then

01:10:23,400 --> 01:10:28,910
sends back an acknowledgement that yes

01:10:26,160 --> 01:10:32,400
your data is stored in a way that will

01:10:28,910 --> 01:10:35,580
even if we lose nodes will still not

01:10:32,400 --> 01:10:39,470
lose your data so that's part of this a

01:10:35,580 --> 01:10:41,909
sizing chart I think most folks use

01:10:39,470 --> 01:10:45,360
three at CD nodes

01:10:41,909 --> 01:10:48,000
and often will run @cd on their master

01:10:45,360 --> 01:10:51,449
instances along with the api I've seen

01:10:48,000 --> 01:10:53,909
Red Hat frequently set up three for the

01:10:51,449 --> 01:10:55,530
API server and then three separate that

01:10:53,909 --> 01:10:57,659
they call infrastructure nodes that are

01:10:55,530 --> 01:11:01,380
just running at CD on their own

01:10:57,659 --> 01:11:03,449
so if SC D gets busy or if the API gets

01:11:01,380 --> 01:11:05,370
busy they're not competing for the same

01:11:03,449 --> 01:11:09,060
network connection or the same CPU

01:11:05,370 --> 01:11:10,980
resources so cluster size of three is

01:11:09,060 --> 01:11:13,080
pretty common for a full deployment with

01:11:10,980 --> 01:11:15,570
mini cube of course we only have one so

01:11:13,080 --> 01:11:18,690
we are not able to show the high

01:11:15,570 --> 01:11:20,070
availability aspects quite as well if

01:11:18,690 --> 01:11:22,770
you want to play around with that CD

01:11:20,070 --> 01:11:26,070
there's a nice simulator at played at CD

01:11:22,770 --> 01:11:29,370
i/o where you can crash different nodes

01:11:26,070 --> 01:11:32,429
and watch a leader election happen and a

01:11:29,370 --> 01:11:34,409
lot of other nice things and it's set up

01:11:32,429 --> 01:11:36,630
in a way where you can interactively

01:11:34,409 --> 01:11:40,770
demo with your audience so if you do

01:11:36,630 --> 01:11:42,060
talks it's it's a nice thing to run

01:11:40,770 --> 01:11:44,580
through with an audience when you have

01:11:42,060 --> 01:11:46,980
time for it the kubernetes api is

01:11:44,580 --> 01:11:50,550
another big piece of infrastructure soft

01:11:46,980 --> 01:11:53,909
infrastructure that the is in kubernetes

01:11:50,550 --> 01:11:57,090
this is the gatekeeper for at CD so any

01:11:53,909 --> 01:12:00,179
time you want to access at CD at CD is

01:11:57,090 --> 01:12:02,520
generally unsecured so if someone had

01:12:00,179 --> 01:12:05,070
access to at CD they would probably be

01:12:02,520 --> 01:12:07,790
able to read all the database secrets or

01:12:05,070 --> 01:12:09,960
all the passwords or all the

01:12:07,790 --> 01:12:12,090
configuration for all the nodes and all

01:12:09,960 --> 01:12:14,520
the way way too much information on how

01:12:12,090 --> 01:12:18,330
to compromise the platform essentially

01:12:14,520 --> 01:12:20,159
so the gatekeeper for at CD is the

01:12:18,330 --> 01:12:23,400
kubernetes api and there's very

01:12:20,159 --> 01:12:26,040
fine-grained access controls i think

01:12:23,400 --> 01:12:29,639
there's role based access control that

01:12:26,040 --> 01:12:35,190
was recently up streamed the variety of

01:12:29,639 --> 01:12:36,719
folks collaborated on this is the

01:12:35,190 --> 01:12:41,520
kubernetes api

01:12:36,719 --> 01:12:45,540
is not required to be up for your pods

01:12:41,520 --> 01:12:49,830
to be up it's just for receiving new

01:12:45,540 --> 01:12:53,419
specs and shuffling information in and

01:12:49,830 --> 01:12:56,599
out of the DB but anytime that you

01:12:53,419 --> 01:12:59,269
send in a request for a pod if the API

01:12:56,599 --> 01:13:00,340
is up and your request makes it into the

01:12:59,269 --> 01:13:04,939
database

01:13:00,340 --> 01:13:06,769
notification then gets sent through what

01:13:04,939 --> 01:13:10,099
I clicked clicked on something

01:13:06,769 --> 01:13:13,880
accidentally architecture here we go

01:13:10,099 --> 01:13:16,510
alright that's C D we'll see how that

01:13:13,880 --> 01:13:20,630
information gets routed in a second

01:13:16,510 --> 01:13:24,169
there's a nice set of outage simulations

01:13:20,630 --> 01:13:30,110
that you can run through in this odds

01:13:24,169 --> 01:13:33,079
con talk from a year ago I really like

01:13:30,110 --> 01:13:35,719
this list because it doesn't work quite

01:13:33,079 --> 01:13:38,570
as well with many Kubb but it shows you

01:13:35,719 --> 01:13:40,070
how to do things like destroy at CD

01:13:38,570 --> 01:13:44,090
while the clusters running and then

01:13:40,070 --> 01:13:46,999
restore it from backup just kill the API

01:13:44,090 --> 01:13:50,979
server entirely and then make sure that

01:13:46,999 --> 01:13:54,249
you still have uptime on all your pods

01:13:50,979 --> 01:13:57,079
and they have like uptime graphs

01:13:54,249 --> 01:13:58,579
simulate a failure of the API scheduler

01:13:57,079 --> 01:14:02,239
this one I can actually do from the

01:13:58,579 --> 01:14:06,439
command line and we have some slides on

01:14:02,239 --> 01:14:08,239
how to do it so we might well get back

01:14:06,439 --> 01:14:12,079
to the slides and we could see how that

01:14:08,239 --> 01:14:15,800
one would shape up so if we wanted to

01:14:12,079 --> 01:14:17,570
simulate a outage of the scheduler I

01:14:15,800 --> 01:14:20,360
think that's what this one is oh this is

01:14:17,570 --> 01:14:24,219
the API server I could do a coop CTL run

01:14:20,360 --> 01:14:28,189
and launch this deployment contact it

01:14:24,219 --> 01:14:31,880
via the web browser let's see would look

01:14:28,189 --> 01:14:35,239
kind of like that then we can SSH into

01:14:31,880 --> 01:14:36,019
the cluster we could look for a process

01:14:35,239 --> 01:14:39,320
called

01:14:36,019 --> 01:14:41,869
local cube within mini Kubb there's a

01:14:39,320 --> 01:14:45,199
binary called local cube that's actually

01:14:41,869 --> 01:14:48,559
running the API server and the scheduler

01:14:45,199 --> 01:14:50,869
and I think one other part and usually I

01:14:48,559 --> 01:14:53,689
try to like break each of these pieces

01:14:50,869 --> 01:14:55,880
individually and show how the rest of

01:14:53,689 --> 01:14:58,939
the system keeps running with any one of

01:14:55,880 --> 01:15:00,919
them broken but since they're all tied

01:14:58,939 --> 01:15:03,590
together in mini coop into a single

01:15:00,919 --> 01:15:05,659
binary called local cube I can't

01:15:03,590 --> 01:15:06,829
actually kill these individually it just

01:15:05,659 --> 01:15:09,869
kills the whole staff

01:15:06,829 --> 01:15:12,809
but I could do a kill all local Kubb and

01:15:09,869 --> 01:15:14,760
then if you quickly log out and list all

01:15:12,809 --> 01:15:17,999
pods you should see an error that says

01:15:14,760 --> 01:15:20,519
connection to the server is refused

01:15:17,999 --> 01:15:24,269
you know it'll definitely error back if

01:15:20,519 --> 01:15:26,429
you walk through these examples and kill

01:15:24,269 --> 01:15:30,599
local Kubb and then get pods right away

01:15:26,429 --> 01:15:33,239
if you wait more than a second before

01:15:30,599 --> 01:15:35,249
running that get pods command local coop

01:15:33,239 --> 01:15:37,530
will probably be back up it

01:15:35,249 --> 01:15:39,449
automatically bounces back up and so

01:15:37,530 --> 01:15:41,070
this is one of those things like you can

01:15:39,449 --> 01:15:43,800
go through and watch you could kill the

01:15:41,070 --> 01:15:46,130
API server pods will stay up and API

01:15:43,800 --> 01:15:48,719
server will come right back up and be

01:15:46,130 --> 01:15:53,369
available within a second or two

01:15:48,719 --> 01:15:55,800
so usually very responsive recovery and

01:15:53,369 --> 01:15:58,919
the service will keep running connection

01:15:55,800 --> 01:16:01,139
to the pods will keep running there's

01:15:58,919 --> 01:16:04,199
another major infrastructure piece

01:16:01,139 --> 01:16:07,650
called a couplet a couplet is a process

01:16:04,199 --> 01:16:10,469
that runs on every node and listens to

01:16:07,650 --> 01:16:13,349
the API server using one of those - -

01:16:10,469 --> 01:16:15,419
watch flags essentially it's going to

01:16:13,349 --> 01:16:18,269
leave a long-running connection and it's

01:16:15,419 --> 01:16:22,260
going to look for any resource that has

01:16:18,269 --> 01:16:27,059
a spec and within the spec we need a

01:16:22,260 --> 01:16:29,400
very specific thing we want a spec it's

01:16:27,059 --> 01:16:32,939
not in this scroll back it's in my other

01:16:29,400 --> 01:16:39,300
we want a spec that says that the node

01:16:32,939 --> 01:16:39,959
name has been assigned spec where do we

01:16:39,300 --> 01:16:43,650
got it

01:16:39,959 --> 01:16:47,939
status here we go a node name it's gonna

01:16:43,650 --> 01:16:50,969
listen to the API for pods for services

01:16:47,939 --> 01:16:53,820
for deployments for any object type that

01:16:50,969 --> 01:16:56,309
has a spec that says node name equals

01:16:53,820 --> 01:16:57,780
whatever node that couplet is running on

01:16:56,309 --> 01:16:59,939
right

01:16:57,780 --> 01:17:02,610
each Kubla is going to be running on a

01:16:59,939 --> 01:17:05,909
different node and so they all listen to

01:17:02,610 --> 01:17:09,510
the API for any new objects with the

01:17:05,909 --> 01:17:10,860
label node name equals hey it's my name

01:17:09,510 --> 01:17:13,679
this is the node I'm running on right

01:17:10,860 --> 01:17:16,499
and then the coab couplet will catch

01:17:13,679 --> 01:17:20,219
that event and provision new containers

01:17:16,499 --> 01:17:22,489
right that's how actual workloads get to

01:17:20,219 --> 01:17:24,900
is Kubla is listening to the api

01:17:22,489 --> 01:17:27,360
watching for things that should land on

01:17:24,900 --> 01:17:31,860
its node and then launching them as

01:17:27,360 --> 01:17:34,229
appropriate the kubernetes scheduler

01:17:31,860 --> 01:17:35,429
will just basically apply that node name

01:17:34,229 --> 01:17:37,469
that's all it does

01:17:35,429 --> 01:17:41,249
so if you want to simulate an outage of

01:17:37,469 --> 01:17:43,800
the scheduler you can look at the

01:17:41,249 --> 01:17:47,939
difference between these two create

01:17:43,800 --> 01:17:49,229
statements and the actual file one file

01:17:47,939 --> 01:17:50,760
in here the only difference between

01:17:49,229 --> 01:17:54,599
these two they're they're two different

01:17:50,760 --> 01:17:57,689
pod specs here's the first one and you

01:17:54,599 --> 01:18:01,050
could see spec containers resources and

01:17:57,689 --> 01:18:02,909
the second one is basically the same

01:18:01,050 --> 01:18:10,019
thing but it's already been scheduled

01:18:02,909 --> 01:18:12,439
and the only difference the only

01:18:10,019 --> 01:18:12,439
difference

01:18:15,300 --> 01:18:21,240
is the node name has been added here we

01:18:18,870 --> 01:18:22,680
could see that node named mini cube so

01:18:21,240 --> 01:18:25,380
that's all the scheduler is going to do

01:18:22,680 --> 01:18:27,660
if you wanted to essentially bypass the

01:18:25,380 --> 01:18:31,740
scheduler there's an event in the logs

01:18:27,660 --> 01:18:34,380
you could do coups CTL gets events and

01:18:31,740 --> 01:18:36,990
you could see when you do a normal

01:18:34,380 --> 01:18:39,210
deployment you'll see this there'll be

01:18:36,990 --> 01:18:40,830
an event in here from the scheduler well

01:18:39,210 --> 01:18:45,330
that's where the scheduler basically

01:18:40,830 --> 01:18:49,580
applies that node name I've done a whole

01:18:45,330 --> 01:18:52,880
lot of started container created killing

01:18:49,580 --> 01:18:55,680
it should be somewhere in here

01:18:52,880 --> 01:18:58,250
successfully pulled pulled there's

01:18:55,680 --> 01:18:58,250
usually

01:19:02,169 --> 01:19:07,300
yeah here's a bunch of events where the

01:19:04,149 --> 01:19:09,729
scheduler is jumping in and if you from

01:19:07,300 --> 01:19:12,789
the command line provision both of these

01:19:09,729 --> 01:19:14,800
pods one of them will add an event in

01:19:12,789 --> 01:19:16,719
the event log that says hey scheduler

01:19:14,800 --> 01:19:18,340
jumped in and took an action and the

01:19:16,719 --> 01:19:20,289
other one is just gonna skip that step

01:19:18,340 --> 01:19:22,899
entirely because we've bypassed the

01:19:20,289 --> 01:19:25,719
scheduler we've automatically scheduled

01:19:22,899 --> 01:19:29,800
this to the mini coop node by including

01:19:25,719 --> 01:19:33,300
that in our specification so that's one

01:19:29,800 --> 01:19:36,969
way to bypass the scheduler Kubb dns is

01:19:33,300 --> 01:19:42,159
basically a way from any individual node

01:19:36,969 --> 01:19:46,090
or in individual pod you can do a curl

01:19:42,159 --> 01:19:51,519
request to the name of the service so if

01:19:46,090 --> 01:19:55,030
we name our service metrics k8s and then

01:19:51,519 --> 01:19:56,110
we do curl metrics k8s that curl request

01:19:55,030 --> 01:20:00,159
will actually go to the load balancer

01:19:56,110 --> 01:20:04,840
and then back down to our array of pods

01:20:00,159 --> 01:20:08,070
so Kubb dns is a internal DNS service

01:20:04,840 --> 01:20:10,179
that provides discoverability for your

01:20:08,070 --> 01:20:13,090
containers and your services

01:20:10,179 --> 01:20:15,939
specifically Kubb proxy is how a lot of

01:20:13,090 --> 01:20:16,599
usually traffic gets on to the overlay

01:20:15,939 --> 01:20:17,979
Network

01:20:16,599 --> 01:20:20,590
I'm just kind of listing the major

01:20:17,979 --> 01:20:23,260
architectural components because it's

01:20:20,590 --> 01:20:26,380
harder to go through and step through

01:20:23,260 --> 01:20:28,510
each of these the last major piece that

01:20:26,380 --> 01:20:31,419
I'd like to point out is kubernetes

01:20:28,510 --> 01:20:34,179
controllers this allows you to run kind

01:20:31,419 --> 01:20:37,059
of a reconciliation loop like the

01:20:34,179 --> 01:20:39,519
situation I described before the Kubla

01:20:37,059 --> 01:20:41,709
is going to use that watch events listen

01:20:39,519 --> 01:20:44,829
for the API and look for things that are

01:20:41,709 --> 01:20:48,429
named node name with its particular node

01:20:44,829 --> 01:20:50,769
and then take an action right that type

01:20:48,429 --> 01:20:53,260
of reconciliation loop where it's

01:20:50,769 --> 01:20:55,630
constantly listening for actions to take

01:20:53,260 --> 01:20:57,729
and then taking an action when needed

01:20:55,630 --> 01:21:00,639
that's how kubernetes controllers work

01:20:57,729 --> 01:21:03,099
and usually there's a controller for

01:21:00,639 --> 01:21:06,519
deployment resources there's a

01:21:03,099 --> 01:21:09,729
controller for service resources there's

01:21:06,519 --> 01:21:14,670
a controller for a lot of different API

01:21:09,729 --> 01:21:17,800
resources so as you're going through and

01:21:14,670 --> 01:21:20,170
requesting you know submitting your pod

01:21:17,800 --> 01:21:23,050
spec or your deployment spec to the API

01:21:20,170 --> 01:21:25,120
the controller is noticing that a new

01:21:23,050 --> 01:21:27,430
spec has arrived filling in the

01:21:25,120 --> 01:21:31,480
additional data that's required to

01:21:27,430 --> 01:21:33,460
actually deploy the asset and updating

01:21:31,480 --> 01:21:37,870
the status actually Kubla it would

01:21:33,460 --> 01:21:39,880
update the status as per the results if

01:21:37,870 --> 01:21:42,160
all of that sounds like a whole lot of

01:21:39,880 --> 01:21:44,470
talking and not enough diagrams here

01:21:42,160 --> 01:21:48,850
hopefully is the piece that links the

01:21:44,470 --> 01:21:52,510
picture together so coming in from the

01:21:48,850 --> 01:21:55,960
command line over here coups ETL or from

01:21:52,510 --> 01:22:00,040
any kind of API interaction we'll be

01:21:55,960 --> 01:22:02,800
making a call into the API server so if

01:22:00,040 --> 01:22:05,140
it was our coups ETL run example where

01:22:02,800 --> 01:22:07,450
we're provisioning a new container that

01:22:05,140 --> 01:22:10,720
would come into the API server the API

01:22:07,450 --> 01:22:13,810
server would check our credentials check

01:22:10,720 --> 01:22:18,220
to see it basically would store that

01:22:13,810 --> 01:22:21,010
spec into at CD the scheduler would then

01:22:18,220 --> 01:22:24,730
get an alert from the API server that

01:22:21,010 --> 01:22:28,450
new data has arrived in at CD that is

01:22:24,730 --> 01:22:31,660
lacking a node name the scheduler will

01:22:28,450 --> 01:22:33,580
then apply a node name and save that

01:22:31,660 --> 01:22:36,610
resource back to the kubernetes api

01:22:33,580 --> 01:22:41,920
which will then put the object back in

01:22:36,610 --> 01:22:45,550
at CD once that happens the couplet will

01:22:41,920 --> 01:22:49,290
get a notification that a resource has

01:22:45,550 --> 01:22:53,610
been scheduled onto this particular node

01:22:49,290 --> 01:22:57,400
it will then basically pull that

01:22:53,610 --> 01:23:00,670
manifest from the API server and go to

01:22:57,400 --> 01:23:04,000
provision that resource on the node once

01:23:00,670 --> 01:23:07,090
that resource spins up it can use things

01:23:04,000 --> 01:23:09,400
like the service proxy to contact load

01:23:07,090 --> 01:23:15,280
balancers and the rest of the kubernetes

01:23:09,400 --> 01:23:16,300
network any questions on this no feel

01:23:15,280 --> 01:23:20,220
free to hit me up with questions

01:23:16,300 --> 01:23:20,220
afterwards one one question yet

01:23:22,620 --> 01:23:28,590
the question was is Etsy D the only

01:23:25,410 --> 01:23:30,570
database that's used with kubernetes

01:23:28,590 --> 01:23:33,720
today the answer is yes there's

01:23:30,570 --> 01:23:38,130
basically convergence on a single

01:23:33,720 --> 01:23:40,770
database today I don't know of any other

01:23:38,130 --> 01:23:46,560
major players that are really trying to

01:23:40,770 --> 01:23:48,660
do a raft style consensus datastore I

01:23:46,560 --> 01:23:50,760
think there are other options for

01:23:48,660 --> 01:23:53,250
certain types of data you can use things

01:23:50,760 --> 01:23:55,560
like vault from a Shi Corp for some

01:23:53,250 --> 01:23:58,320
secrets and another credentials so I've

01:23:55,560 --> 01:24:02,510
seen people start to split out some of

01:23:58,320 --> 01:24:04,830
what's stored from Etsy D and put more

01:24:02,510 --> 01:24:06,630
application credentials and applications

01:24:04,830 --> 01:24:08,790
state into a different data store and

01:24:06,630 --> 01:24:12,030
have that CD be more of the platform

01:24:08,790 --> 01:24:13,410
state so it it really depends on where

01:24:12,030 --> 01:24:15,390
you draw the line between what's

01:24:13,410 --> 01:24:17,670
platform State and what's app State I

01:24:15,390 --> 01:24:20,940
think sometimes and how secure you need

01:24:17,670 --> 01:24:23,820
to be on those but there's a lot of

01:24:20,940 --> 01:24:28,050
really nice protections around what's

01:24:23,820 --> 01:24:30,990
visible in @cd you can restrict a

01:24:28,050 --> 01:24:34,320
couplet so that it can only make

01:24:30,990 --> 01:24:36,930
requests pertinent to things that belong

01:24:34,320 --> 01:24:38,880
on its node or you know like you could

01:24:36,930 --> 01:24:43,890
try to scope down the level of access to

01:24:38,880 --> 01:24:45,200
@ d if if that's your angle yeah good

01:24:43,890 --> 01:24:48,720
question

01:24:45,200 --> 01:24:51,120
last diagram I have is this is from a

01:24:48,720 --> 01:24:52,640
docker blog so I don't know they may

01:24:51,120 --> 01:24:56,970
have been trying to point out the

01:24:52,640 --> 01:24:59,040
complexity of how complex and hard to

01:24:56,970 --> 01:25:01,770
understand kubernetes is versus docker

01:24:59,040 --> 01:25:04,620
and maybe maybe they achieved it via

01:25:01,770 --> 01:25:08,400
this talk I don't know but this is a

01:25:04,620 --> 01:25:12,620
workflow diagram of how how coops ETL

01:25:08,400 --> 01:25:15,930
might step through this interaction and

01:25:12,620 --> 01:25:18,540
it's hard to see with this monitor

01:25:15,930 --> 01:25:20,580
resolution that we have here but if you

01:25:18,540 --> 01:25:22,830
have this loaded on your laptop it's a

01:25:20,580 --> 01:25:24,630
lot easier to read or you could click

01:25:22,830 --> 01:25:26,040
through to the docker blog I'm just

01:25:24,630 --> 01:25:28,950
having trouble with my screen resolution

01:25:26,040 --> 01:25:32,250
but this is tracing the request from the

01:25:28,950 --> 01:25:32,610
API the client to the API server to Etsy

01:25:32,250 --> 01:25:35,100
D

01:25:32,610 --> 01:25:37,949
and then back to give the client the

01:25:35,100 --> 01:25:40,590
response that yes we've received your

01:25:37,949 --> 01:25:42,840
request from there the API server is

01:25:40,590 --> 01:25:45,659
going to then make some more requests

01:25:42,840 --> 01:25:48,270
into sed and to the controller manager

01:25:45,659 --> 01:25:51,150
it's going to go to the scheduler and

01:25:48,270 --> 01:25:54,300
get its node name it'll then get passed

01:25:51,150 --> 01:25:58,110
onto a couplet and then talk to docker

01:25:54,300 --> 01:26:01,050
to provision the pods so that's the full

01:25:58,110 --> 01:26:06,230
kind of workflow diagram according to

01:26:01,050 --> 01:26:11,369
Dockers blog that is all I have for the

01:26:06,230 --> 01:26:12,920
architecture section so I have about 15

01:26:11,369 --> 01:26:15,449
minutes left

01:26:12,920 --> 01:26:17,550
thank you all for sticking around I'm

01:26:15,449 --> 01:26:18,989
gonna go through I'm try to speed

01:26:17,550 --> 01:26:22,830
through unless there's any more

01:26:18,989 --> 01:26:25,530
questions about architecture here No

01:26:22,830 --> 01:26:27,780
okay I'm gonna try to I noticed a lot of

01:26:25,530 --> 01:26:31,139
information and if I was smart I would

01:26:27,780 --> 01:26:33,389
just end it right here because this is a

01:26:31,139 --> 01:26:36,719
lot of information to absorb and I know

01:26:33,389 --> 01:26:38,429
that a lot of these talks like you one

01:26:36,719 --> 01:26:40,699
thing I've heard from speakers is like

01:26:38,429 --> 01:26:43,260
try to boil it down to three takeaways

01:26:40,699 --> 01:26:46,110
they they're only gonna remember three

01:26:43,260 --> 01:26:49,340
things at the end of this talk so what I

01:26:46,110 --> 01:26:52,440
have at the end of this talk is links to

01:26:49,340 --> 01:26:54,330
three separate chapters of what I've

01:26:52,440 --> 01:26:58,020
covered I don't know if that's helpful

01:26:54,330 --> 01:27:00,389
but hopefully it gives you a way to if

01:26:58,020 --> 01:27:02,820
you like this content you can fork any

01:27:00,389 --> 01:27:04,619
of this these slides and recycle them

01:27:02,820 --> 01:27:07,739
very easily I have a lightning talk

01:27:04,619 --> 01:27:09,780
tomorrow on how I do all my slides and

01:27:07,739 --> 01:27:11,760
my workshop decks so that's interesting

01:27:09,780 --> 01:27:13,230
stop by the lightning talks I'll be

01:27:11,760 --> 01:27:15,449
talking a little bit about how I built

01:27:13,230 --> 01:27:18,600
these slides today and how I make these

01:27:15,449 --> 01:27:20,400
workshop modules easy to reuse you're

01:27:18,600 --> 01:27:22,860
getting a whole barrage of a whole bunch

01:27:20,400 --> 01:27:25,770
of workshop modules today which is maybe

01:27:22,860 --> 01:27:27,929
more content than I should be dumping on

01:27:25,770 --> 01:27:30,719
you but this is an extended session so

01:27:27,929 --> 01:27:33,270
I'm gonna try to work through one more

01:27:30,719 --> 01:27:35,880
section really quickly and then get to

01:27:33,270 --> 01:27:37,739
the wrap-up so what I would cover in

01:27:35,880 --> 01:27:41,909
this section I'm just going to summarize

01:27:37,739 --> 01:27:43,679
is local development here's what

01:27:41,909 --> 01:27:44,580
kubernetes provides we've already said

01:27:43,679 --> 01:27:46,740
this standard

01:27:44,580 --> 01:27:48,930
Packaging load balancing scaling

01:27:46,740 --> 01:27:51,570
automation how many people need any of

01:27:48,930 --> 01:27:53,850
this stuff for local development nobody

01:27:51,570 --> 01:27:56,430
nobody needs this stuff for local

01:27:53,850 --> 01:27:58,350
development so a lot of what kubernetes

01:27:56,430 --> 01:28:00,870
is why it's hard to sell kubernetes to

01:27:58,350 --> 01:28:02,970
web developers you're trying to sell

01:28:00,870 --> 01:28:05,820
them on concepts that just they don't

01:28:02,970 --> 01:28:06,900
have any use case for at least during

01:28:05,820 --> 01:28:10,080
local development

01:28:06,900 --> 01:28:13,020
so hopefully this chapter is what you

01:28:10,080 --> 01:28:15,810
need if they're far front-end web dev

01:28:13,020 --> 01:28:17,550
just doing HTML and CSS and a little bit

01:28:15,810 --> 01:28:18,600
of JavaScript this ought to be

01:28:17,550 --> 01:28:22,740
everything you need

01:28:18,600 --> 01:28:24,450
I use a nodejs example in here so here's

01:28:22,740 --> 01:28:27,210
some selling points why you might want

01:28:24,450 --> 01:28:30,510
to run kubernetes locally like we have

01:28:27,210 --> 01:28:33,000
been in this workshop one is the ability

01:28:30,510 --> 01:28:35,850
to offer reproducible development

01:28:33,000 --> 01:28:38,190
environments and and that's not just

01:28:35,850 --> 01:28:40,710
many Kubb but also that deployment

01:28:38,190 --> 01:28:42,990
manifest that we generated right we

01:28:40,710 --> 01:28:45,870
generated a spec that you could then

01:28:42,990 --> 01:28:47,610
hand off to a web developer and say hey

01:28:45,870 --> 01:28:50,490
get ready to on board yourself it's just

01:28:47,610 --> 01:28:52,080
coops ETL create on this file that'll

01:28:50,490 --> 01:28:56,010
get your local dev environment up and

01:28:52,080 --> 01:28:58,290
running right so that's a big win

01:28:56,010 --> 01:29:01,590
potentially standardizing the onboarding

01:28:58,290 --> 01:29:04,290
process minimizing deltas between Devon

01:29:01,590 --> 01:29:08,340
production right I might not care about

01:29:04,290 --> 01:29:10,890
load balancers in local development but

01:29:08,340 --> 01:29:13,470
if I can model all of those abstractions

01:29:10,890 --> 01:29:16,020
locally and make sure that what I'm

01:29:13,470 --> 01:29:19,260
running locally is going to pass the CI

01:29:16,020 --> 01:29:21,900
test before I do a get push I can save

01:29:19,260 --> 01:29:24,270
everyone a lot of time and I can free up

01:29:21,900 --> 01:29:26,400
the Jenkins or whatever build pipeline

01:29:24,270 --> 01:29:28,830
from getting full of a bunch of mess

01:29:26,400 --> 01:29:30,330
that it's never gonna like we ought to

01:29:28,830 --> 01:29:32,940
be able to figure out these things

01:29:30,330 --> 01:29:34,980
locally now that we can run kubernetes

01:29:32,940 --> 01:29:37,620
locally we could do a full functional

01:29:34,980 --> 01:29:39,990
evaluation before we promote our code

01:29:37,620 --> 01:29:44,250
changes and that's a that's a revolution

01:29:39,990 --> 01:29:47,160
I think getting that feedback earlier on

01:29:44,250 --> 01:29:49,740
in the developer lifecycle leads to

01:29:47,160 --> 01:29:52,050
faster code velocity than we would

01:29:49,740 --> 01:29:54,150
otherwise have and this is why I'm

01:29:52,050 --> 01:29:56,010
interested in using kubernetes for local

01:29:54,150 --> 01:29:57,110
development if I have to stop and do a

01:29:56,010 --> 01:29:59,750
docker build

01:29:57,110 --> 01:30:00,290
to see my results and it takes two

01:29:59,750 --> 01:30:02,659
minutes

01:30:00,290 --> 01:30:07,300
I'm not iterating fast enough and and

01:30:02,659 --> 01:30:09,739
this these tricks is what I use I guess

01:30:07,300 --> 01:30:11,810
de-centralized your release pipeline and

01:30:09,739 --> 01:30:14,030
allow your CI suite to be run locally

01:30:11,810 --> 01:30:16,850
potentially if you're up for that that's

01:30:14,030 --> 01:30:18,860
maybe some extra work but it'll allow

01:30:16,850 --> 01:30:21,280
you to have functional systems

01:30:18,860 --> 01:30:24,230
integration feedback for those devs and

01:30:21,280 --> 01:30:27,889
the ultimate is being able to achieve

01:30:24,230 --> 01:30:30,889
fully offline distributed systems

01:30:27,889 --> 01:30:33,469
development all on your laptop all using

01:30:30,889 --> 01:30:35,719
containers not no one's doing that these

01:30:33,469 --> 01:30:37,280
days but with many coop I think I think

01:30:35,719 --> 01:30:40,429
you could basically do it with some of

01:30:37,280 --> 01:30:42,409
these tips not fully offline all the

01:30:40,429 --> 01:30:44,030
time you know you got a download mini

01:30:42,409 --> 01:30:45,619
coup but you got to get your laptop set

01:30:44,030 --> 01:30:47,900
up but once you have some docker

01:30:45,619 --> 01:30:51,219
binaries you can really do a lot of work

01:30:47,900 --> 01:30:54,619
offline on the plane or whatever so

01:30:51,219 --> 01:30:56,570
local development checklist this is what

01:30:54,619 --> 01:30:59,179
I would expect as a local developer

01:30:56,570 --> 01:31:01,760
someone needs to onboard me I need to be

01:30:59,179 --> 01:31:04,310
able to preview my changes or don't tell

01:31:01,760 --> 01:31:06,260
me to do kubernetes I need to be able to

01:31:04,310 --> 01:31:09,710
test my changes and promote my changes

01:31:06,260 --> 01:31:12,860
let me speed through these four sections

01:31:09,710 --> 01:31:13,940
or okay onboarding here's what I would

01:31:12,860 --> 01:31:17,090
do yesterday

01:31:13,940 --> 01:31:18,710
git clone change directories into that

01:31:17,090 --> 01:31:21,650
and then I'd probably run in p.m.

01:31:18,710 --> 01:31:23,300
install and NPM start for nodejs

01:31:21,650 --> 01:31:26,330
this is just if I'm developing you know

01:31:23,300 --> 01:31:29,659
one single micro service today I'd do

01:31:26,330 --> 01:31:31,880
something like this instead I do i if

01:31:29,659 --> 01:31:34,280
i'm onboarding let's say I'm training

01:31:31,880 --> 01:31:37,000
someone else I'm gonna do coop CTL run

01:31:34,280 --> 01:31:39,949
and I'm gonna pipe all of this into a

01:31:37,000 --> 01:31:42,139
metrics review yeah Mille and this is

01:31:39,949 --> 01:31:45,429
how I'm gonna do local code reviews I

01:31:42,139 --> 01:31:50,600
would then create this and test it out

01:31:45,429 --> 01:31:51,880
all right and in fact I might just see

01:31:50,600 --> 01:31:56,440
if I have time

01:31:51,880 --> 01:31:56,440
yeah where am I at

01:31:56,500 --> 01:32:01,900
yeah I'm gonna I'm gonna copy and paste

01:31:58,820 --> 01:32:01,900
it and see what happens

01:32:05,619 --> 01:32:13,300
okay so there goes that step see if I

01:32:10,719 --> 01:32:17,369
could just copy and paste and do a speed

01:32:13,300 --> 01:32:17,369
run through these create the resource

01:32:17,639 --> 01:32:25,360
contact the service preview my changes

01:32:22,630 --> 01:32:29,349
okay so now I've got I've got a trick

01:32:25,360 --> 01:32:33,730
right locally I have a clone and I think

01:32:29,349 --> 01:32:35,560
I forgot to include this in the notes so

01:32:33,730 --> 01:32:37,780
this is one step that's oh no what we

01:32:35,560 --> 01:32:41,440
had it in our in our last it was in

01:32:37,780 --> 01:32:44,980
yesterday's jam yesterday's jam you

01:32:41,440 --> 01:32:46,389
would need to get clone here's a line

01:32:44,980 --> 01:32:48,310
you might want to copy and paste if

01:32:46,389 --> 01:32:50,710
you're following along get clone this

01:32:48,310 --> 01:32:53,590
repo and we'll make some local changes

01:32:50,710 --> 01:32:56,260
preview it before we do our docker build

01:32:53,590 --> 01:33:00,520
okay so I already have a copy of this

01:32:56,260 --> 01:33:02,650
and I'm already in the directory so I'm

01:33:00,520 --> 01:33:04,690
gonna go back to this mini coop mount

01:33:02,650 --> 01:33:06,429
and I know I'm in the right directory

01:33:04,690 --> 01:33:08,980
because I just explained that part right

01:33:06,429 --> 01:33:14,820
but I'm gonna mount my current working

01:33:08,980 --> 01:33:20,080
directory inside the mini coop VM and

01:33:14,820 --> 01:33:22,989
then I'm going to copy my metrics review

01:33:20,080 --> 01:33:27,790
file and I'm gonna change the name to

01:33:22,989 --> 01:33:30,099
metrics dev this is gonna give me a it's

01:33:27,790 --> 01:33:32,849
going on oh that's that mount process

01:33:30,099 --> 01:33:37,230
okay I'm gonna just leave that running

01:33:32,849 --> 01:33:37,230
and do some work over here

01:33:38,050 --> 01:33:52,570
okay so I have a metrics dev file

01:33:47,640 --> 01:34:04,710
metrics dev I'm going to edit this and

01:33:52,570 --> 01:34:08,410
I'm gonna replace metrics review with

01:34:04,710 --> 01:34:12,510
metrics dev I'm gonna globally replace

01:34:08,410 --> 01:34:15,190
that just so I basically can spin up I

01:34:12,510 --> 01:34:16,690
could do coupe CT I'll create on each

01:34:15,190 --> 01:34:19,060
and they won't conflict they'll come up

01:34:16,690 --> 01:34:20,620
with different resource names right so

01:34:19,060 --> 01:34:24,100
I've got two separate files to work with

01:34:20,620 --> 01:34:29,380
I'm going to edit the dev version and

01:34:24,100 --> 01:34:34,590
I'm going to add in a volume with this

01:34:29,380 --> 01:34:37,180
host port type so sorry I've got some

01:34:34,590 --> 01:34:43,450
graphics in the way there but let's try

01:34:37,180 --> 01:34:45,820
to paste this in oops VI metrics dev I'm

01:34:43,450 --> 01:34:54,720
gonna go right down here near the end of

01:34:45,820 --> 01:34:54,720
the file and do a little bit of cleanup

01:34:55,080 --> 01:34:58,410
this is

01:35:00,620 --> 01:35:05,540
this is how the file should look and if

01:35:03,890 --> 01:35:08,300
you didn't follow along because I did

01:35:05,540 --> 01:35:09,770
all of that way too quickly I have the

01:35:08,300 --> 01:35:11,660
finished result right here you could

01:35:09,770 --> 01:35:15,220
download click on that link that'll be

01:35:11,660 --> 01:35:18,470
the finished result with the host path

01:35:15,220 --> 01:35:21,230
declaration added in once you have those

01:35:18,470 --> 01:35:28,670
ready to go you should be able to mini

01:35:21,230 --> 01:35:31,340
Kubb create dash F metrics dev what

01:35:28,670 --> 01:35:39,770
happened many up coupes et al create

01:35:31,340 --> 01:35:48,200
sorry coupe CTL create that should spin

01:35:39,770 --> 01:35:53,960
up my new local dev environment should

01:35:48,200 --> 01:35:56,660
be able to preview it over here let's

01:35:53,960 --> 01:36:00,440
see is this it or not nope that's not it

01:35:56,660 --> 01:36:09,350
I find out where it ended up metric

01:36:00,440 --> 01:36:17,000
stove URL please know URL hey Scoob CTL

01:36:09,350 --> 01:36:20,150
get services any Kubiak's well I could

01:36:17,000 --> 01:36:25,970
expose yeah no bummer well I did

01:36:20,150 --> 01:36:29,380
something wrong but basically basically

01:36:25,970 --> 01:36:32,630
the result is you what you should have

01:36:29,380 --> 01:36:34,820
with many cube service and I'll double

01:36:32,630 --> 01:36:36,770
check these notes and figure out what

01:36:34,820 --> 01:36:39,200
exactly why is not showing it I think

01:36:36,770 --> 01:36:44,360
it's I didn't use the node port flag I

01:36:39,200 --> 01:36:46,970
think in in the service type I can I can

01:36:44,360 --> 01:36:50,300
fix that but I shouldn't because I'm

01:36:46,970 --> 01:36:52,460
running out of time so anyway this would

01:36:50,300 --> 01:36:57,710
what this does is allow you to basically

01:36:52,460 --> 01:36:59,720
open up your index.html and then go and

01:36:57,710 --> 01:37:02,750
make as many changes as you want locally

01:36:59,720 --> 01:37:06,020
and since your local folder is mounted

01:37:02,750 --> 01:37:08,750
into the VM and then from the VM into

01:37:06,020 --> 01:37:10,970
the pod then as soon as I make changes

01:37:08,750 --> 01:37:13,010
locally I'll be able to make changes and

01:37:10,970 --> 01:37:13,719
not need to do any type of build so as

01:37:13,010 --> 01:37:16,719
long as I'm change

01:37:13,719 --> 01:37:20,469
front end code I don't need a no js' web

01:37:16,719 --> 01:37:22,360
server I don't need a lot of stuff

01:37:20,469 --> 01:37:24,010
installed all my no js' dependencies

01:37:22,360 --> 01:37:27,519
I've extracted all of that into the

01:37:24,010 --> 01:37:30,429
docker image so I can really distribute

01:37:27,519 --> 01:37:32,320
all of my onboarding requirements

01:37:30,429 --> 01:37:34,239
including the whole build out of the dev

01:37:32,320 --> 01:37:36,639
environment I can distribute as

01:37:34,239 --> 01:37:38,320
containers and then just have them mount

01:37:36,639 --> 01:37:41,349
their local source repo into the

01:37:38,320 --> 01:37:43,269
container make as many HTML client-side

01:37:41,349 --> 01:37:47,199
changes as they want this isn't going to

01:37:43,269 --> 01:37:49,690
work for things like PHP Ruby you know

01:37:47,199 --> 01:37:52,479
other processes would need a little bit

01:37:49,690 --> 01:37:54,369
more work but this is just my you know

01:37:52,479 --> 01:37:57,150
try to get the ball rolling for you

01:37:54,369 --> 01:38:00,940
folks and show how you would maybe use

01:37:57,150 --> 01:38:03,249
volumes to mount a local directory into

01:38:00,940 --> 01:38:06,429
many Kubb and use that as part of your

01:38:03,249 --> 01:38:09,820
local development workflow I will fix

01:38:06,429 --> 01:38:12,280
the example on on the page I had here I

01:38:09,820 --> 01:38:15,760
think it just needs the service type

01:38:12,280 --> 01:38:20,170
equals node port but I'll definitely

01:38:15,760 --> 01:38:23,590
troll me online if you have any feedback

01:38:20,170 --> 01:38:25,090
about that sorry and eventually you're

01:38:23,590 --> 01:38:26,979
going to want to promote your changes

01:38:25,090 --> 01:38:29,979
this also has some notes on how you

01:38:26,979 --> 01:38:32,590
would run docker build and then apply

01:38:29,979 --> 01:38:34,679
those changes locally so you can

01:38:32,590 --> 01:38:37,809
simulate the rollout to your next stage

01:38:34,679 --> 01:38:41,619
your actual code promotion might be a

01:38:37,809 --> 01:38:44,050
get push might be a docker push might be

01:38:41,619 --> 01:38:45,639
handled by your CI suite I really don't

01:38:44,050 --> 01:38:47,289
know it's kind of implementation

01:38:45,639 --> 01:38:49,119
specific and I'll have to let you fill

01:38:47,289 --> 01:38:52,389
in some of the details on promoting

01:38:49,119 --> 01:38:54,460
changes if you want support for

01:38:52,389 --> 01:38:56,260
kubernetes and maybe an upgraded

01:38:54,460 --> 01:38:57,030
developer workflow take a look at

01:38:56,260 --> 01:38:59,800
OpenShift

01:38:57,030 --> 01:39:02,889
we do a lot of work around kubernetes at

01:38:59,800 --> 01:39:05,499
Red Hat and would be happy to help you

01:39:02,889 --> 01:39:09,489
get started if you like these workshops

01:39:05,499 --> 01:39:12,820
take a look at bitly k8s workshops for a

01:39:09,489 --> 01:39:15,130
series of each of these sections into

01:39:12,820 --> 01:39:17,320
kind of bite-sized chunks so local

01:39:15,130 --> 01:39:19,570
environments with mini Kubb how to get

01:39:17,320 --> 01:39:22,659
your environment connected to gke and

01:39:19,570 --> 01:39:24,849
g-cloud kubernetes command-line basics

01:39:22,659 --> 01:39:25,680
kubernetes architecture and then our

01:39:24,849 --> 01:39:27,600
bonus

01:39:25,680 --> 01:39:30,240
action on local development I'll have

01:39:27,600 --> 01:39:33,810
added to this menu of choices very

01:39:30,240 --> 01:39:36,150
shortly kubernetes documentation straits

01:39:33,810 --> 01:39:39,480
you can get free hosting on kubernetes

01:39:36,150 --> 01:39:41,340
with open shift starter and i've got 4

01:39:39,480 --> 01:39:45,450
ebooks that you could download for free

01:39:41,340 --> 01:39:47,520
for free O'Reilly ebooks from from Red

01:39:45,450 --> 01:39:49,020
Hat courtesy of Red Hat so feel free to

01:39:47,520 --> 01:39:53,070
click on any of these links if you'd

01:39:49,020 --> 01:39:55,260
like a book download and so I know

01:39:53,070 --> 01:39:57,540
there's very few people left compared to

01:39:55,260 --> 01:39:59,340
the intro but so the biased

01:39:57,540 --> 01:40:01,230
I'm only sampling the folks that stuck

01:39:59,340 --> 01:40:03,600
around but out of the folks that are

01:40:01,230 --> 01:40:07,740
still here have you ever used containers

01:40:03,600 --> 01:40:11,460
yeah most most of you majority great ok

01:40:07,740 --> 01:40:13,140
have you used kubernetes yeah I'll get

01:40:11,460 --> 01:40:15,480
to the important ones how many people

01:40:13,140 --> 01:40:18,450
feel like they have a minimal basic

01:40:15,480 --> 01:40:21,930
proficiency with the coop CTL command

01:40:18,450 --> 01:40:25,170
line tool good good all right all right

01:40:21,930 --> 01:40:27,290
good I did the people who stayed got

01:40:25,170 --> 01:40:30,150
some info all right ok

01:40:27,290 --> 01:40:33,500
can how many feel people feel like they

01:40:30,150 --> 01:40:38,270
can name five basic kubernetes resources

01:40:33,500 --> 01:40:42,870
pretty good ok pod services nodes

01:40:38,270 --> 01:40:46,830
deployments replication controllers that

01:40:42,870 --> 01:40:48,750
was the tricky one architecture and and

01:40:46,830 --> 01:40:51,990
hopefully the last one are you prepared

01:40:48,750 --> 01:40:54,810
to on board a new web dev I made me more

01:40:51,990 --> 01:40:57,030
work on my last section but there's more

01:40:54,810 --> 01:40:59,070
work for the whole kubernetes community

01:40:57,030 --> 01:41:02,490
to do around web development

01:40:59,070 --> 01:41:04,620
it's a tricky spot openshift adds a lot

01:41:02,490 --> 01:41:06,180
of nice stuff there so definitely give

01:41:04,620 --> 01:41:06,690
that a look thank you for sticking

01:41:06,180 --> 01:41:09,690
around

01:41:06,690 --> 01:41:12,980
I'm Ryan J and this has been

01:41:09,690 --> 01:41:12,980

YouTube URL: https://www.youtube.com/watch?v=MmQtZYoiM0I


