Title: All Things Open 2018 - Lauren Maffeo - GetApp - Lightning Talk
Publication date: 2018-12-05
Playlist: All Things Open 2018 Playlist
Description: 
	All Things Open 2018 - Lauren Maffeo - GetApp - Lightning Talk
Captions: 
	00:00:03,170 --> 00:00:07,440
hi everyone thanks for being here and

00:00:05,879 --> 00:00:09,870
spending your lunch with us today as

00:00:07,440 --> 00:00:12,360
Jason mentioned my name is Lauren Maffeo

00:00:09,870 --> 00:00:14,280
I'm a senior content analyst at get app

00:00:12,360 --> 00:00:16,440
which is a Gartner company where I cover

00:00:14,280 --> 00:00:18,750
trends in cloud project management

00:00:16,440 --> 00:00:20,880
software and emerging technology trends

00:00:18,750 --> 00:00:22,529
for small and midsize businesses as

00:00:20,880 --> 00:00:24,449
Jason mentioned I also just wrote a

00:00:22,529 --> 00:00:25,949
piece for open source comm and the

00:00:24,449 --> 00:00:28,289
subject I'm going to talk about with you

00:00:25,949 --> 00:00:30,539
today which is how to erase unconscious

00:00:28,289 --> 00:00:34,800
bias from the data sets used to train a

00:00:30,539 --> 00:00:36,989
I powered products and to do that I want

00:00:34,800 --> 00:00:39,719
to start out by taking us back to 2015

00:00:36,989 --> 00:00:41,430
there was a tool called how old met it

00:00:39,719 --> 00:00:44,910
was a website where you could upload

00:00:41,430 --> 00:00:46,670
photos of yourself and the software

00:00:44,910 --> 00:00:49,020
would spit out recommendations or

00:00:46,670 --> 00:00:50,820
guesses for how old it thought you were

00:00:49,020 --> 00:00:52,739
so I had some fun with this in August

00:00:50,820 --> 00:00:54,600
and put in a photo of myself that had

00:00:52,739 --> 00:00:56,399
been taken that month and I was

00:00:54,600 --> 00:00:58,980
flattered when the site thought that I

00:00:56,399 --> 00:01:01,890
was 24 years old which is younger than I

00:00:58,980 --> 00:01:03,989
actually am i was less flattered when i

00:01:01,890 --> 00:01:06,360
put in a photo of myself taken in May

00:01:03,989 --> 00:01:09,270
and it thought that I was 38 years old

00:01:06,360 --> 00:01:10,500
so this was an it used as an example in

00:01:09,270 --> 00:01:13,920
the press of how facial recognition

00:01:10,500 --> 00:01:15,840
software means well but it still has a

00:01:13,920 --> 00:01:19,680
long way to go before it has a hundred

00:01:15,840 --> 00:01:21,720
percent accuracy the problem is that

00:01:19,680 --> 00:01:23,880
machine learning gaffes aren't always

00:01:21,720 --> 00:01:25,650
funny like this they can actually have

00:01:23,880 --> 00:01:27,869
pretty serious consequences for end

00:01:25,650 --> 00:01:29,640
users when the datasets that are used to

00:01:27,869 --> 00:01:31,650
train these machine learning algorithms

00:01:29,640 --> 00:01:35,189
aren't diverse enough in terms of the

00:01:31,650 --> 00:01:37,380
data they take in so one now-infamous

00:01:35,189 --> 00:01:39,990
example of how machine bias can hurt

00:01:37,380 --> 00:01:41,970
users is a product called compass it's a

00:01:39,990 --> 00:01:43,860
machine learning algorithm that predicts

00:01:41,970 --> 00:01:46,500
defendant's likelihoods to recommit

00:01:43,860 --> 00:01:48,509
crimes and research from ProPublica

00:01:46,500 --> 00:01:50,250
which is a nonprofit journalism outlet

00:01:48,509 --> 00:01:52,470
found that it has made biased

00:01:50,250 --> 00:01:55,170
predictions about recidivism based on

00:01:52,470 --> 00:01:56,969
race their research found that compass

00:01:55,170 --> 00:01:59,159
has been 2 times more likely to

00:01:56,969 --> 00:02:01,229
incorrectly cite black defendants as

00:01:59,159 --> 00:02:03,960
being high risk for recommitting crimes

00:02:01,229 --> 00:02:06,240
it has also been 2 times more likely to

00:02:03,960 --> 00:02:08,929
cite white defendants incorrectly as

00:02:06,240 --> 00:02:11,459
being low risk for recommitting crimes

00:02:08,929 --> 00:02:13,150
encompass isn't a hypothetical product

00:02:11,459 --> 00:02:15,670
it's used by judges and over 2

00:02:13,150 --> 00:02:17,290
all of us states and it's flawed results

00:02:15,670 --> 00:02:19,329
have impacted everything from whether

00:02:17,290 --> 00:02:21,909
defendants were let out on bail are held

00:02:19,329 --> 00:02:24,129
before trial it also impacted the

00:02:21,909 --> 00:02:27,010
lengths of their sentences in many cases

00:02:24,129 --> 00:02:29,260
and it's really become an infamous case

00:02:27,010 --> 00:02:31,870
study and how machine learning can use

00:02:29,260 --> 00:02:35,799
inference to draw biased connections

00:02:31,870 --> 00:02:39,489
between data points in a system but that

00:02:35,799 --> 00:02:41,799
brings up an important question is was

00:02:39,489 --> 00:02:44,200
this done on purpose it probably wasn't

00:02:41,799 --> 00:02:46,659
in fact race wasn't one of the variables

00:02:44,200 --> 00:02:48,400
that the algorithm accounted for it's

00:02:46,659 --> 00:02:51,099
more likely that this was an unconscious

00:02:48,400 --> 00:02:52,989
mistake the compas algorithm probably

00:02:51,099 --> 00:02:55,569
didn't get enough training to recognize

00:02:52,989 --> 00:02:58,450
diverse kinds of facial variations

00:02:55,569 --> 00:03:00,639
including skin tone it's also possible

00:02:58,450 --> 00:03:02,799
that the algorithm drew from historical

00:03:00,639 --> 00:03:04,720
data about rates of arrests between

00:03:02,799 --> 00:03:07,450
people who are black versus people who

00:03:04,720 --> 00:03:10,810
are white and incorrectly correlated

00:03:07,450 --> 00:03:12,459
skin tone with rates of recidivism part

00:03:10,810 --> 00:03:14,859
of the problem is that we actually don't

00:03:12,459 --> 00:03:17,440
know that for sure because the creators

00:03:14,859 --> 00:03:19,629
of compass refused to disclose how the

00:03:17,440 --> 00:03:22,480
algorithm works and how it comes to its

00:03:19,629 --> 00:03:23,799
decisions citing proprietary reasons and

00:03:22,480 --> 00:03:25,810
that means that it's a black box

00:03:23,799 --> 00:03:27,160
algorithm and this is one of the big

00:03:25,810 --> 00:03:29,799
issues we currently have with

00:03:27,160 --> 00:03:31,870
transparency and AI oftentimes the

00:03:29,799 --> 00:03:34,449
creators of these algorithms can't

00:03:31,870 --> 00:03:36,549
explain how they work but we can infer

00:03:34,449 --> 00:03:38,500
based on the flawed results of products

00:03:36,549 --> 00:03:41,109
like compass that machines have their

00:03:38,500 --> 00:03:42,790
own biases just like people and that's

00:03:41,109 --> 00:03:44,620
really what machine bias is it's

00:03:42,790 --> 00:03:47,889
programming that assumes the prejudice

00:03:44,620 --> 00:03:52,720
of its creators or data even if that

00:03:47,889 --> 00:03:55,000
prejudice is conscious or not so I

00:03:52,720 --> 00:03:57,430
really wanted this talk to be a call to

00:03:55,000 --> 00:03:58,989
action for why you should add bias

00:03:57,430 --> 00:04:01,120
testing to product development life

00:03:58,989 --> 00:04:03,489
cycles if you're working on ML

00:04:01,120 --> 00:04:07,120
algorithms that are used to train AI

00:04:03,489 --> 00:04:09,099
products as how old net shows AI is

00:04:07,120 --> 00:04:11,319
still in its earliest days with a lot of

00:04:09,099 --> 00:04:13,120
room for improvement and it's a unique

00:04:11,319 --> 00:04:15,430
technology because it's constantly

00:04:13,120 --> 00:04:16,840
receiving and learning from new data the

00:04:15,430 --> 00:04:19,389
risk you run there is that it could

00:04:16,840 --> 00:04:22,389
either reinforce early biases within the

00:04:19,389 --> 00:04:24,820
system or it could learn new biases in

00:04:22,389 --> 00:04:25,990
production and then refine its results

00:04:24,820 --> 00:04:29,420
based on that day

00:04:25,990 --> 00:04:30,590
whew we have relatively few products

00:04:29,420 --> 00:04:33,380
that are powered by this type of

00:04:30,590 --> 00:04:35,960
technology today and even fewer product

00:04:33,380 --> 00:04:37,400
teams use biased testing today but if

00:04:35,960 --> 00:04:39,800
you appoint someone like a data

00:04:37,400 --> 00:04:42,320
scientist or a product manager on your

00:04:39,800 --> 00:04:44,360
product dev team who can own the health

00:04:42,320 --> 00:04:45,980
of your data sets we can go a long way

00:04:44,360 --> 00:04:48,800
towards solving this problem and

00:04:45,980 --> 00:04:51,770
ultimately building AI powered products

00:04:48,800 --> 00:04:55,750
that are more equitable and benefit all

00:04:51,770 --> 00:05:04,249
users not just a select few thank you

00:04:55,750 --> 00:05:04,249

YouTube URL: https://www.youtube.com/watch?v=_X7OV4bpJug


