Title: Managing Data Consistency among Microservices with Debezium - Justin Chao
Publication date: 2020-12-17
Playlist: All Things Open 2020 - Back-End Dev Track
Description: 
	Presented by: Justin Chao, Optum
Presented at All Things Open 2020 - Back-End Dev Track

Abstract: Microservices are quickly becoming the trend in cloud-native application development, providing services that are more agile, scalable, and resilient. However, how do you ensure data consistency across your microservices, while maintaining a healthy amount of independency across service boundaries?

Debezium is an open source distributed platform that captures row-level changes in a database, using Apache Kafka Connect compatible connectors. By subscribing to these change-event streams, data changes can be propagated to other services for further processing.

In this session, the audience will learn about Debezium, and how it supports scalable microservice architectures and the propagation of data and data changes amongst services.
Captions: 
	00:00:04,960 --> 00:00:08,000
quick introduction about ourselves

00:00:06,560 --> 00:00:09,519
um if you're here to learn about how to

00:00:08,000 --> 00:00:11,679
manage data consistency among

00:00:09,519 --> 00:00:12,559
microservices with the bzm you are in

00:00:11,679 --> 00:00:14,320
the right

00:00:12,559 --> 00:00:16,480
place with the right link to the

00:00:14,320 --> 00:00:19,680
zoomtalk my name is justin chow

00:00:16,480 --> 00:00:21,680
i'm here with my co-host hi i'm tyler

00:00:19,680 --> 00:00:23,840
yam

00:00:21,680 --> 00:00:25,519
next night please yeah so real quick

00:00:23,840 --> 00:00:26,800
about ourselves i'm a senior software

00:00:25,519 --> 00:00:29,039
engineer at optim

00:00:26,800 --> 00:00:30,720
this is my third time attending all

00:00:29,039 --> 00:00:31,679
things open first time virtually

00:00:30,720 --> 00:00:34,719
obviously

00:00:31,679 --> 00:00:36,880
uh tyler yeah and i'm a software

00:00:34,719 --> 00:00:38,320
engineer at optum and this is my second

00:00:36,880 --> 00:00:40,719
time attending and you know excited to

00:00:38,320 --> 00:00:43,040
talk to you guys about the bdm

00:00:40,719 --> 00:00:44,079
yeah so the agenda for today is roughly

00:00:43,040 --> 00:00:45,840
going to go like this we're going to

00:00:44,079 --> 00:00:46,239
probably spend the first half of our 15

00:00:45,840 --> 00:00:48,800
minute

00:00:46,239 --> 00:00:50,160
session giving a quick overview of what

00:00:48,800 --> 00:00:52,160
the bzm is what it does

00:00:50,160 --> 00:00:54,160
why it matters and then the second half

00:00:52,160 --> 00:00:57,280
hopefully we'll have a successful demo

00:00:54,160 --> 00:00:58,000
of how to use the bzm um so before we

00:00:57,280 --> 00:01:01,199
really talk about

00:00:58,000 --> 00:01:03,920
division i want to briefly

00:01:01,199 --> 00:01:05,760
set the context here right uh and so i'm

00:01:03,920 --> 00:01:07,600
sure most if not all of us are familiar

00:01:05,760 --> 00:01:09,040
with what migrant services are

00:01:07,600 --> 00:01:10,640
it's probably been mentioned at least

00:01:09,040 --> 00:01:12,479
hundreds of thousands of times

00:01:10,640 --> 00:01:13,680
in the course of the past 48 hours in

00:01:12,479 --> 00:01:16,159
this conference

00:01:13,680 --> 00:01:17,680
um and so most if not all of us are well

00:01:16,159 --> 00:01:19,680
versed in all the advantages that

00:01:17,680 --> 00:01:22,320
microservices architecture provide us

00:01:19,680 --> 00:01:24,640
with development time decoupling being

00:01:22,320 --> 00:01:28,240
able to autonomously

00:01:24,640 --> 00:01:31,360
with very quick agility develop services

00:01:28,240 --> 00:01:33,200
um across different teams uh

00:01:31,360 --> 00:01:34,880
and be able to own that kind of bounded

00:01:33,200 --> 00:01:36,159
business domain right for whatever

00:01:34,880 --> 00:01:39,119
features or capabilities

00:01:36,159 --> 00:01:41,200
that you're trying to develop for um but

00:01:39,119 --> 00:01:43,200
largely for the most part i think

00:01:41,200 --> 00:01:45,920
many of us still fall into this

00:01:43,200 --> 00:01:49,680
bottleneck in our development processes

00:01:45,920 --> 00:01:51,600
where we become constrained by the data

00:01:49,680 --> 00:01:52,720
because most of the time while we may be

00:01:51,600 --> 00:01:54,880
trying to

00:01:52,720 --> 00:01:55,920
you know utilize microservices

00:01:54,880 --> 00:01:58,799
architectures

00:01:55,920 --> 00:02:00,479
we're still trying to use a shared

00:01:58,799 --> 00:02:02,079
database across all of our services

00:02:00,479 --> 00:02:03,680
right and so i'm looking at all the data

00:02:02,079 --> 00:02:05,920
lakes

00:02:03,680 --> 00:02:07,200
that large enterprise organizations

00:02:05,920 --> 00:02:09,280
pride themselves on

00:02:07,200 --> 00:02:10,399
or as i like to call them sometimes data

00:02:09,280 --> 00:02:12,319
swaps

00:02:10,399 --> 00:02:14,160
right and there's many times where we're

00:02:12,319 --> 00:02:17,040
developing our services quickly

00:02:14,160 --> 00:02:19,280
but then we're bottlenecked by how fast

00:02:17,040 --> 00:02:21,280
we can access or manage that data

00:02:19,280 --> 00:02:22,800
in shared database there's development

00:02:21,280 --> 00:02:24,720
type coupling if you need to change a

00:02:22,800 --> 00:02:25,440
schema in your data for one service then

00:02:24,720 --> 00:02:28,160
you have to

00:02:25,440 --> 00:02:29,840
make sure that the other teams

00:02:28,160 --> 00:02:31,519
developing their services are okay with

00:02:29,840 --> 00:02:33,519
you changing the schema and you have to

00:02:31,519 --> 00:02:35,440
manage those dependencies as well

00:02:33,519 --> 00:02:36,640
there's runtime coupling dependencies

00:02:35,440 --> 00:02:39,120
potentially as well

00:02:36,640 --> 00:02:41,040
if you wanted to use you know like a

00:02:39,120 --> 00:02:41,760
graph database for example because graph

00:02:41,040 --> 00:02:44,080
databases

00:02:41,760 --> 00:02:45,360
suit your particular service better than

00:02:44,080 --> 00:02:47,440
a nosql database

00:02:45,360 --> 00:02:49,680
then you know there's those stabilities

00:02:47,440 --> 00:02:52,560
and technology lock-ins as well

00:02:49,680 --> 00:02:53,040
and so there's an emerging pattern where

00:02:52,560 --> 00:02:55,360
we

00:02:53,040 --> 00:02:56,360
can not only potentially think about the

00:02:55,360 --> 00:03:01,040
way we manage

00:02:56,360 --> 00:03:04,159
microservices as domain data ownership

00:03:01,040 --> 00:03:06,000
domain driven design but also

00:03:04,159 --> 00:03:08,000
can we think about the way we manage

00:03:06,000 --> 00:03:09,920
databases in a similar fashion so quote

00:03:08,000 --> 00:03:10,640
unquote micro databases where each

00:03:09,920 --> 00:03:13,599
database

00:03:10,640 --> 00:03:15,440
is bounded by its own business domain

00:03:13,599 --> 00:03:16,000
like the service that is associated with

00:03:15,440 --> 00:03:19,120
it

00:03:16,000 --> 00:03:21,040
then each team is able to autonomously

00:03:19,120 --> 00:03:22,159
develop their services against their own

00:03:21,040 --> 00:03:24,799
databases where they

00:03:22,159 --> 00:03:25,680
manage and own their own data as well

00:03:24,799 --> 00:03:28,879
and so there's

00:03:25,680 --> 00:03:30,799
that nice clean separation but

00:03:28,879 --> 00:03:32,640
most if not all the time our services

00:03:30,799 --> 00:03:33,760
aren't so isolated from each other that

00:03:32,640 --> 00:03:36,400
we can simply

00:03:33,760 --> 00:03:37,519
you know develop with this pattern

00:03:36,400 --> 00:03:39,760
eventually

00:03:37,519 --> 00:03:41,200
service b is going to need data from

00:03:39,760 --> 00:03:42,720
another service because

00:03:41,200 --> 00:03:44,879
otherwise your application is not going

00:03:42,720 --> 00:03:48,080
to work right and and so

00:03:44,879 --> 00:03:49,280
there's an emerging um there's an

00:03:48,080 --> 00:03:51,599
emerging

00:03:49,280 --> 00:03:53,760
pattern where we can potentially use a

00:03:51,599 --> 00:03:55,599
streaming platform like kafka

00:03:53,760 --> 00:03:58,000
for example as a source of truth where

00:03:55,599 --> 00:04:00,959
you can capture it and send events

00:03:58,000 --> 00:04:02,560
to the streaming platform to kafka and

00:04:00,959 --> 00:04:04,000
then be able to read those events and

00:04:02,560 --> 00:04:06,239
consume off of them in a

00:04:04,000 --> 00:04:08,000
very distributed very scalable really

00:04:06,239 --> 00:04:10,080
resist uh resilient

00:04:08,000 --> 00:04:11,920
type of pattern uh but then this

00:04:10,080 --> 00:04:13,439
introduces its own set of challenges too

00:04:11,920 --> 00:04:14,319
right so what we've got depicted here is

00:04:13,439 --> 00:04:16,639
this prediction

00:04:14,319 --> 00:04:18,000
uh penetral issue where you got a dual

00:04:16,639 --> 00:04:20,079
write pattern

00:04:18,000 --> 00:04:21,280
where you have to write to your micro

00:04:20,079 --> 00:04:22,320
database first and then you have a

00:04:21,280 --> 00:04:25,440
second right to

00:04:22,320 --> 00:04:27,040
this uh kafka streaming platform and if

00:04:25,440 --> 00:04:27,600
anybody's familiar with the dual right

00:04:27,040 --> 00:04:29,919
pattern

00:04:27,600 --> 00:04:31,840
you know that it's very frowned upon if

00:04:29,919 --> 00:04:32,720
your first transaction succeeds but your

00:04:31,840 --> 00:04:35,440
second transaction

00:04:32,720 --> 00:04:36,720
fails well now you've run into the same

00:04:35,440 --> 00:04:38,000
issue that we've been trying to solve in

00:04:36,720 --> 00:04:40,560
the first place and that is

00:04:38,000 --> 00:04:42,080
how to manage data consistency right and

00:04:40,560 --> 00:04:43,840
so this is where the beezum comes in and

00:04:42,080 --> 00:04:45,040
i'll go ahead and turn it over to tire

00:04:43,840 --> 00:04:48,720
to go over

00:04:45,040 --> 00:04:51,680
what the bezium is yeah so the bzm

00:04:48,720 --> 00:04:54,160
is uh deploys or uses a set of

00:04:51,680 --> 00:04:56,560
connectors that are deployed via the

00:04:54,160 --> 00:04:57,919
kafka connect api and so it's able to

00:04:56,560 --> 00:05:00,400
get access to

00:04:57,919 --> 00:05:01,440
a databases right ahead logs and capture

00:05:00,400 --> 00:05:04,479
all the data changes

00:05:01,440 --> 00:05:06,160
and then send it to kafka or distributed

00:05:04,479 --> 00:05:09,440
logs as

00:05:06,160 --> 00:05:11,280
change events and so the great thing

00:05:09,440 --> 00:05:12,160
about this is and then on the other side

00:05:11,280 --> 00:05:14,080
you know

00:05:12,160 --> 00:05:15,600
kafka has a lot of different connectors

00:05:14,080 --> 00:05:16,479
that also can consume it and so you're

00:05:15,600 --> 00:05:20,400
able to

00:05:16,479 --> 00:05:22,240
add these consuming microservices and

00:05:20,400 --> 00:05:25,520
as suggestion pointed out we're able to

00:05:22,240 --> 00:05:27,600
avoid the dual rate issue because

00:05:25,520 --> 00:05:29,199
now a domain or a microservice only has

00:05:27,600 --> 00:05:31,919
to worry about publishing to their own

00:05:29,199 --> 00:05:32,800
database because the bzm connectors are

00:05:31,919 --> 00:05:35,759
able to

00:05:32,800 --> 00:05:36,160
capture these data changes at a pretty

00:05:35,759 --> 00:05:39,680
low

00:05:36,160 --> 00:05:40,560
delay and the you know different sets of

00:05:39,680 --> 00:05:42,479
connectors that

00:05:40,560 --> 00:05:45,199
supports most of the uh open source

00:05:42,479 --> 00:05:47,120
databases out there

00:05:45,199 --> 00:05:48,479
so kind of digging a little deeper into

00:05:47,120 --> 00:05:51,919
the bzm architecture

00:05:48,479 --> 00:05:53,120
as i kind of mentioned these uh the bdm

00:05:51,919 --> 00:05:56,000
source connectors

00:05:53,120 --> 00:05:57,360
are able to access you know databases

00:05:56,000 --> 00:06:01,680
right it has logs so in

00:05:57,360 --> 00:06:04,479
my sql databa uh case it's the bin logs

00:06:01,680 --> 00:06:06,240
and then they're able by default each of

00:06:04,479 --> 00:06:08,880
these different kafka topics

00:06:06,240 --> 00:06:10,960
are set to a different database table i

00:06:08,880 --> 00:06:12,319
just have a box of dibezia

00:06:10,960 --> 00:06:14,240
and so it's able to capture each of

00:06:12,319 --> 00:06:15,680
these row events uh

00:06:14,240 --> 00:06:17,440
and then on the other end they have

00:06:15,680 --> 00:06:19,199
these sync connectors that are then

00:06:17,440 --> 00:06:21,120
you know able to kind of scale out and

00:06:19,199 --> 00:06:24,080
add different uh

00:06:21,120 --> 00:06:26,479
consuming micro services and kind of for

00:06:24,080 --> 00:06:28,960
the idea of the disaster recovery

00:06:26,479 --> 00:06:30,080
uh these both of these connectors are

00:06:28,960 --> 00:06:33,280
able to have

00:06:30,080 --> 00:06:36,319
periodically record their position and

00:06:33,280 --> 00:06:38,560
offset so that they know what the last

00:06:36,319 --> 00:06:39,840
change event they had read so if for

00:06:38,560 --> 00:06:42,400
some reason why it

00:06:39,840 --> 00:06:43,520
fails then it's able to restart back up

00:06:42,400 --> 00:06:47,199
and then note where it has

00:06:43,520 --> 00:06:48,960
last left off and so that's where

00:06:47,199 --> 00:06:51,039
you know kind of the frequency that you

00:06:48,960 --> 00:06:53,039
set your connectors to kind of record it

00:06:51,039 --> 00:06:55,599
will depend on how

00:06:53,039 --> 00:06:56,400
uh well it's able to you know if it

00:06:55,599 --> 00:06:58,160
re-reads

00:06:56,400 --> 00:07:00,080
something that it has already read when

00:06:58,160 --> 00:07:01,680
it has failed uh so that's where

00:07:00,080 --> 00:07:03,599
instead of exactly like an ideal

00:07:01,680 --> 00:07:04,560
situation would be exactly once delivery

00:07:03,599 --> 00:07:07,759
instead it would be

00:07:04,560 --> 00:07:07,759
at least once delivery

00:07:08,240 --> 00:07:11,759
and uh one of the great things too about

00:07:10,240 --> 00:07:14,240
this is because of the offsets and the

00:07:11,759 --> 00:07:16,800
positions you're able to replay

00:07:14,240 --> 00:07:18,160
like if you completely lose a connector

00:07:16,800 --> 00:07:19,440
you want to add on a new microservice

00:07:18,160 --> 00:07:20,560
you're able to kind of scale up and

00:07:19,440 --> 00:07:22,080
replay

00:07:20,560 --> 00:07:24,400
these connectors at different points and

00:07:22,080 --> 00:07:27,759
positions

00:07:24,400 --> 00:07:29,759
and kind of just uh brief so the bzm

00:07:27,759 --> 00:07:30,720
requirements you know as i mentioned

00:07:29,759 --> 00:07:32,240
you kind of need to have this

00:07:30,720 --> 00:07:33,680
configuration for the database to have

00:07:32,240 --> 00:07:36,639
these right ahead logs and

00:07:33,680 --> 00:07:38,319
uh you also need to give r back

00:07:36,639 --> 00:07:42,000
permission for division

00:07:38,319 --> 00:07:45,039
i'm going to stop sharing and let

00:07:42,000 --> 00:07:47,440
justin go with the demo

00:07:45,039 --> 00:07:50,560
all right let's see if i can make sure i

00:07:47,440 --> 00:07:50,560
share the right screen here

00:07:53,759 --> 00:07:58,879
okay you should see my terminal window

00:07:56,840 --> 00:08:00,080
now

00:07:58,879 --> 00:08:02,319
they're right everybody can see that

00:08:00,080 --> 00:08:04,000
yeah okay yeah all right so

00:08:02,319 --> 00:08:05,440
um i've got my terminal window open up

00:08:04,000 --> 00:08:08,160
here there's four panes

00:08:05,440 --> 00:08:09,520
labeled zero one two and three uh in the

00:08:08,160 --> 00:08:11,120
first pane which is in the upper right

00:08:09,520 --> 00:08:12,639
hand corner

00:08:11,120 --> 00:08:14,240
you can see a list of pods that i'm

00:08:12,639 --> 00:08:15,199
running in the kubernetes cluster right

00:08:14,240 --> 00:08:16,560
now so

00:08:15,199 --> 00:08:18,720
this is just the kubernetes cluster

00:08:16,560 --> 00:08:21,120
deployed out on azure

00:08:18,720 --> 00:08:21,759
using aks and what i'm going to go ahead

00:08:21,120 --> 00:08:23,440
and demo

00:08:21,759 --> 00:08:25,360
is the deployment of the debesium

00:08:23,440 --> 00:08:27,360
connector is tired

00:08:25,360 --> 00:08:28,720
just briefly very briefly talked about

00:08:27,360 --> 00:08:30,639
and show you how

00:08:28,720 --> 00:08:32,560
how it works how it reads from the mysql

00:08:30,639 --> 00:08:34,800
database and then the uh

00:08:32,560 --> 00:08:36,560
putting use a console consumer from

00:08:34,800 --> 00:08:38,640
kafka to kind of show you what that

00:08:36,560 --> 00:08:41,519
change event looks like

00:08:38,640 --> 00:08:42,800
all right so i've already got uh a kafka

00:08:41,519 --> 00:08:45,920
single node

00:08:42,800 --> 00:08:48,880
broker deployed along with its zookeeper

00:08:45,920 --> 00:08:50,080
uh as you can see i'm using a stream z

00:08:48,880 --> 00:08:51,279
operator framework if you're not

00:08:50,080 --> 00:08:53,760
familiar with strings

00:08:51,279 --> 00:08:54,720
great project uh and i've also got my

00:08:53,760 --> 00:08:57,920
sql database

00:08:54,720 --> 00:08:59,839
and so if i just do a show databases

00:08:57,920 --> 00:09:02,160
here in this window you can see it's

00:08:59,839 --> 00:09:03,920
a bit vanilla installation of my sql

00:09:02,160 --> 00:09:05,760
nothing fancy with it no databases have

00:09:03,920 --> 00:09:08,240
been created at all yet

00:09:05,760 --> 00:09:09,519
uh and down here if i just do a list

00:09:08,240 --> 00:09:11,680
topics if my

00:09:09,519 --> 00:09:12,720
kafka broker there's there's no topics

00:09:11,680 --> 00:09:13,839
that will be created

00:09:12,720 --> 00:09:16,000
all right and so what i'm going to go

00:09:13,839 --> 00:09:17,600
ahead and do now is apply

00:09:16,000 --> 00:09:20,080
and go ahead and create my the bsm

00:09:17,600 --> 00:09:20,080
connector

00:09:21,040 --> 00:09:26,000
in my namespace and once it does that

00:09:24,320 --> 00:09:27,839
you should see that the connector pod

00:09:26,000 --> 00:09:29,680
comes up in the upper right hand corner

00:09:27,839 --> 00:09:33,279
of my terminal window

00:09:29,680 --> 00:09:35,680
it is now running perfect now

00:09:33,279 --> 00:09:37,120
now if i go ahead and list the topics

00:09:35,680 --> 00:09:39,680
you should now see

00:09:37,120 --> 00:09:41,440
uh some topics have been created by the

00:09:39,680 --> 00:09:44,720
bzm connector to be able to store those

00:09:41,440 --> 00:09:46,240
offsets as tony was mentioning before

00:09:44,720 --> 00:09:48,080
and so now what we're going to go ahead

00:09:46,240 --> 00:09:51,680
and do is

00:09:48,080 --> 00:09:55,360
bootstrap our connector with the right

00:09:51,680 --> 00:09:56,640
um connector to the my sql database

00:09:55,360 --> 00:09:58,560
i know there's a lot of connectors being

00:09:56,640 --> 00:10:00,399
thrown around here but um

00:09:58,560 --> 00:10:02,079
hopefully it'll make sense as you see it

00:10:00,399 --> 00:10:04,880
being done here so

00:10:02,079 --> 00:10:07,440
i'm going to go ahead and extract into

00:10:04,880 --> 00:10:10,079
the container so i can just execute a

00:10:07,440 --> 00:10:12,320
post request um the way the bsm works is

00:10:10,079 --> 00:10:13,120
is it exposes a set of apis that will

00:10:12,320 --> 00:10:15,760
allow you to

00:10:13,120 --> 00:10:17,360
interface with it via you know current

00:10:15,760 --> 00:10:20,640
commands or post

00:10:17,360 --> 00:10:24,079
gate requests restful services basically

00:10:20,640 --> 00:10:26,560
um and so if i do now a curl

00:10:24,079 --> 00:10:27,200
uh you can see here in this empty array

00:10:26,560 --> 00:10:29,120
that no

00:10:27,200 --> 00:10:31,040
connectors have been created right and

00:10:29,120 --> 00:10:35,200
so what i'm going to go ahead and do now

00:10:31,040 --> 00:10:39,440
is actually

00:10:35,200 --> 00:10:39,440
let me exit out of this real quick okay

00:10:42,480 --> 00:10:48,480
and copy my curl command

00:10:45,680 --> 00:10:51,120
now i have to exit back into it and run

00:10:48,480 --> 00:10:54,160
my curl command now to post against

00:10:51,120 --> 00:10:55,760
the bezier we should see that a

00:10:54,160 --> 00:10:58,560
connector has been created

00:10:55,760 --> 00:11:01,839
and now if i go ahead and curl the list

00:10:58,560 --> 00:11:01,839
of connectors again

00:11:05,040 --> 00:11:09,600
there we go so one connector called the

00:11:07,680 --> 00:11:11,680
inventory connector has now been created

00:11:09,600 --> 00:11:13,600
and what that's going to do is listen

00:11:11,680 --> 00:11:16,240
against the mysql database that i've

00:11:13,600 --> 00:11:17,360
specified in the payload of my post

00:11:16,240 --> 00:11:18,880
request

00:11:17,360 --> 00:11:21,040
the payload of my post request contains

00:11:18,880 --> 00:11:22,480
all the parameters for connecting to

00:11:21,040 --> 00:11:24,880
the mysql database as well as the

00:11:22,480 --> 00:11:26,640
capital broker

00:11:24,880 --> 00:11:28,320
now what i'm going to go ahead and do is

00:11:26,640 --> 00:11:32,000
run a listener

00:11:28,320 --> 00:11:32,000
on my kafka consumer

00:11:34,399 --> 00:11:37,440
actually before i do that

00:11:37,519 --> 00:11:41,680
before i do that let me actually create

00:11:39,440 --> 00:11:43,839
a database in my sql

00:11:41,680 --> 00:11:43,839
so

00:11:47,279 --> 00:11:51,360
what i'm doing here is i'm just going to

00:11:49,279 --> 00:11:53,040
go ahead and run a whole bunch of mysql

00:11:51,360 --> 00:11:53,600
commands to create a database create

00:11:53,040 --> 00:11:55,200
tables

00:11:53,600 --> 00:11:57,839
and create some test data with it right

00:11:55,200 --> 00:11:59,519
so now if i show databases you can see

00:11:57,839 --> 00:12:04,240
i've got an inventory database

00:11:59,519 --> 00:12:05,920
i go into that database and show tables

00:12:04,240 --> 00:12:07,440
those are my tables and now if i do a

00:12:05,920 --> 00:12:11,440
select all

00:12:07,440 --> 00:12:15,600
from customers those are my customers

00:12:11,440 --> 00:12:17,920
okay now let's run the console consumer

00:12:15,600 --> 00:12:19,360
to listen against the inventory

00:12:17,920 --> 00:12:22,480
customers table

00:12:19,360 --> 00:12:23,200
again um actually what i can show you is

00:12:22,480 --> 00:12:25,839
if i list

00:12:23,200 --> 00:12:25,839
the topics

00:12:27,120 --> 00:12:31,440
if i list the topics in kafka now you

00:12:30,160 --> 00:12:35,279
should see that there has

00:12:31,440 --> 00:12:35,279
now been a list of

00:12:35,600 --> 00:12:38,800
topics that have been created in kafka

00:12:38,240 --> 00:12:41,760
that map

00:12:38,800 --> 00:12:43,519
back to the uh database tables right so

00:12:41,760 --> 00:12:47,040
address disable customer disable

00:12:43,519 --> 00:12:50,480
orders table things of that nature okay

00:12:47,040 --> 00:12:54,720
so now if we go ahead and

00:12:50,480 --> 00:12:56,560
run the console consumer against the

00:12:54,720 --> 00:12:57,120
customers table what happens if we

00:12:56,560 --> 00:12:58,800
insert

00:12:57,120 --> 00:13:00,959
a new customer so i'm going to go ahead

00:12:58,800 --> 00:13:02,800
and insert kenneth anderson

00:13:00,959 --> 00:13:04,240
and right off the bat immediately you

00:13:02,800 --> 00:13:07,519
can see that the

00:13:04,240 --> 00:13:09,760
kafka console consumer spot out

00:13:07,519 --> 00:13:11,600
spat out uh the change of it that

00:13:09,760 --> 00:13:13,839
happened right and so

00:13:11,600 --> 00:13:15,040
this change event is in json format it's

00:13:13,839 --> 00:13:18,160
a little bit difficult to read

00:13:15,040 --> 00:13:18,880
but fortunately we have in our slide

00:13:18,160 --> 00:13:22,959
deck

00:13:18,880 --> 00:13:22,959
a nicely formatted

00:13:24,880 --> 00:13:28,639
change of it for you right and so this

00:13:27,120 --> 00:13:30,000
is an example of what that change

00:13:28,639 --> 00:13:31,839
might look like we don't have time to

00:13:30,000 --> 00:13:33,920
really dig into the details

00:13:31,839 --> 00:13:35,360
of what makes up this change event but

00:13:33,920 --> 00:13:39,600
it's enough to

00:13:35,360 --> 00:13:42,160
hopefully give you a bit of a

00:13:39,600 --> 00:13:44,639
high level understanding of what you

00:13:42,160 --> 00:13:47,199
could do with the bezia right

00:13:44,639 --> 00:13:49,279
and so in conclusion we talked about the

00:13:47,199 --> 00:13:51,839
use of microservices right and then

00:13:49,279 --> 00:13:52,800
eventually how we become potentially

00:13:51,839 --> 00:13:54,399
constrained

00:13:52,800 --> 00:13:56,079
by the way we're able to manage and

00:13:54,399 --> 00:13:57,680
access our data right and so then

00:13:56,079 --> 00:13:59,680
that introduced the concept of can we

00:13:57,680 --> 00:14:02,560
apply domain driven data ownership

00:13:59,680 --> 00:14:04,240
to data the same way that we do with our

00:14:02,560 --> 00:14:07,279
services

00:14:04,240 --> 00:14:10,399
and we do so by leveraging kafka as

00:14:07,279 --> 00:14:12,560
a centralized log for that golden

00:14:10,399 --> 00:14:14,320
source of truth for our data as a

00:14:12,560 --> 00:14:16,160
streaming platform that is scalable and

00:14:14,320 --> 00:14:18,000
retentive and replayable

00:14:16,160 --> 00:14:19,279
however we can develop our services

00:14:18,000 --> 00:14:20,880
autonomously

00:14:19,279 --> 00:14:22,560
uh leveraging the business for change

00:14:20,880 --> 00:14:24,959
data capture

00:14:22,560 --> 00:14:26,639
all right um and so i think that

00:14:24,959 --> 00:14:29,839
concludes our

00:14:26,639 --> 00:14:31,440
talk uh how are we doing on time exactly

00:14:29,839 --> 00:14:33,680
at 3 45 perfect

00:14:31,440 --> 00:14:35,440
so we'll go ahead and end the talk there

00:14:33,680 --> 00:14:37,839
uh if you haven't visited

00:14:35,440 --> 00:14:39,360
optim's virtual booth yet be sure to

00:14:37,839 --> 00:14:40,880
check us out if you want to learn more

00:14:39,360 --> 00:14:43,760
about how we're leveraging open source

00:14:40,880 --> 00:14:45,199
technologies to solve healthcare

00:14:43,760 --> 00:14:53,120
or if you just want to learn more about

00:14:45,199 --> 00:14:53,120

YouTube URL: https://www.youtube.com/watch?v=StQKKJ3Darw


