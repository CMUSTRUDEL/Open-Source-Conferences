Title: How to Build a Kubernetes Networking Solution from Scratch - Antonin Bas & Jianjun Shen
Publication date: 2020-12-10
Playlist: All Things Open 2020 - Linux Infrastructure Track
Description: 
	Presented by: Antonin Bas & Jianjun Shen, VMware
Presented at All Things Open 2020 - Linux/Infrastructure Track

Abstract: For the non-initiated, Kubernetes (K8s) networking can be a bit like dark magic. Many clusters have requirements beyond what the default network plugin, kubenet, can provide and require the use of a third-party Container Network Interface (CNI) plugin. But what exactly is the role of these plugins, how do they differ from each other and how does the choice of one affect your cluster?

In this talk, Antonin and Jianjun will describe how a group of developers was able to build a CNI plugin - an open source project called Antrea - from scratch and bring it to production in a matter of months. This velocity was achieved by leveraging existing open-source technologies extensively: Open vSwitch, a well-established programmable virtual switch for the data plane, and the K8s libraries for the control plane. Antonin and Jianjun will explain the responsibilities of a CNI plugin in the context of K8s and will walk the audience through the steps required to create one. They will show how Antrea integrates with the rest of the cloud-native ecosystem (e.g. dashboards such as Octant and Prometheus) to provide insight into the network and ensure that K8s networking is not just dark magic anymore.
Captions: 
	00:00:14,719 --> 00:00:17,840
maintainer

00:00:15,519 --> 00:00:19,439
for a vmware open source project called

00:00:17,840 --> 00:00:22,160
uh project entrio

00:00:19,439 --> 00:00:24,000
and i'm also a staff engineer at vmware

00:00:22,160 --> 00:00:28,240
and today i'm presenting with

00:00:24,000 --> 00:00:31,279
jen jun hello everyone my name is

00:00:28,240 --> 00:00:33,760
senior staff engineer at wema i'm also a

00:00:31,279 --> 00:00:35,680
maintenance projector

00:00:33,760 --> 00:00:37,520
so i decided to be here talk about cobas

00:00:35,680 --> 00:00:40,480
networking and how to build a quest

00:00:37,520 --> 00:00:40,480
network solution

00:00:41,680 --> 00:00:46,879
so um we'll start through some basics of

00:00:44,879 --> 00:00:49,120
container and quest networking

00:00:46,879 --> 00:00:50,399
we'll talk about what a questionnaire

00:00:49,120 --> 00:00:53,920
plugin provides

00:00:50,399 --> 00:00:56,160
how it works and works through um

00:00:53,920 --> 00:00:57,600
how to implement the questionnaire

00:00:56,160 --> 00:00:59,359
plugin with opengl switch

00:00:57,600 --> 00:01:02,000
that is a well established virtual

00:00:59,359 --> 00:01:04,400
switch we also introduce project

00:01:02,000 --> 00:01:05,439
tria which is an open source quest

00:01:04,400 --> 00:01:07,840
network plugin

00:01:05,439 --> 00:01:09,280
with some open switch we have demos

00:01:07,840 --> 00:01:11,040
about kubernetes networking uh

00:01:09,280 --> 00:01:14,560
networking with openweight switch and

00:01:11,040 --> 00:01:16,880
project onto you network visibility is

00:01:14,560 --> 00:01:18,720
one focus area presidential

00:01:16,880 --> 00:01:20,880
we will look into how entria provides

00:01:18,720 --> 00:01:24,560
visibility into kubernetes networks

00:01:20,880 --> 00:01:24,560
with the support from opengl switch

00:01:25,600 --> 00:01:29,119
so let's first look at container and

00:01:28,000 --> 00:01:31,360
docker

00:01:29,119 --> 00:01:33,119
containers provide isolated environments

00:01:31,360 --> 00:01:35,360
to run applications

00:01:33,119 --> 00:01:36,240
in the lightworking front the isolation

00:01:35,360 --> 00:01:38,960
is achieved by

00:01:36,240 --> 00:01:40,079
lightweight namespaces so each container

00:01:38,960 --> 00:01:42,720
can have its own

00:01:40,079 --> 00:01:44,399
network namespace and its own network

00:01:42,720 --> 00:01:46,640
interface and ip address

00:01:44,399 --> 00:01:47,520
inside the namespace the container

00:01:46,640 --> 00:01:49,680
network

00:01:47,520 --> 00:01:51,680
name space is isolated from the host

00:01:49,680 --> 00:01:53,759
network and other containers

00:01:51,680 --> 00:01:54,799
a container cannot see the network

00:01:53,759 --> 00:01:57,040
interface

00:01:54,799 --> 00:01:59,360
and the ip addresses of the host and

00:01:57,040 --> 00:02:01,600
other containers

00:01:59,360 --> 00:02:02,799
so there are quite a few ways to connect

00:02:01,600 --> 00:02:05,680
a container to

00:02:02,799 --> 00:02:07,680
to a network one popular solution is to

00:02:05,680 --> 00:02:10,399
use virtual ethernet devices

00:02:07,680 --> 00:02:11,760
and the nina's bridge so as shown in

00:02:10,399 --> 00:02:14,000
this diagram

00:02:11,760 --> 00:02:15,440
you can create a pair of virtual inside

00:02:14,000 --> 00:02:18,239
devices

00:02:15,440 --> 00:02:18,800
with one um inside the container network

00:02:18,239 --> 00:02:20,959
space

00:02:18,800 --> 00:02:22,160
to be the containers network interface

00:02:20,959 --> 00:02:24,720
is a zero

00:02:22,160 --> 00:02:25,760
and that other connected to the linux

00:02:24,720 --> 00:02:27,599
bridge

00:02:25,760 --> 00:02:29,360
so it's like a virtual link between the

00:02:27,599 --> 00:02:31,360
container bridge

00:02:29,360 --> 00:02:33,040
then the traffic from the container will

00:02:31,360 --> 00:02:35,200
enter the bridge through the

00:02:33,040 --> 00:02:36,800
virtual devices and the bridger for the

00:02:35,200 --> 00:02:39,920
traffic between containers based on

00:02:36,800 --> 00:02:41,920
their mic addresses

00:02:39,920 --> 00:02:44,959
the nearest bridge network is also one

00:02:41,920 --> 00:02:46,879
basic network mode supported by docker

00:02:44,959 --> 00:02:49,360
docker demon implements ip address

00:02:46,879 --> 00:02:50,959
management and allocates one ip address

00:02:49,360 --> 00:02:54,400
for each container

00:02:50,959 --> 00:02:56,319
from a local subnet but since it's a

00:02:54,400 --> 00:02:59,040
container sublight is local to a docker

00:02:56,319 --> 00:03:01,120
host the communication of course host

00:02:59,040 --> 00:03:03,120
might require translating the container

00:03:01,120 --> 00:03:05,840
ip address to the ip address of the

00:03:03,120 --> 00:03:05,840
hosts

00:03:07,440 --> 00:03:13,040
then let's look at kubernetes kubernetes

00:03:10,560 --> 00:03:16,800
is a container generation platform

00:03:13,040 --> 00:03:18,239
it also treats deployment scaling

00:03:16,800 --> 00:03:22,480
and operations of our becoming

00:03:18,239 --> 00:03:22,480
containers across the class of the house

00:03:25,040 --> 00:03:30,159
cubase cluster consists consists of

00:03:28,000 --> 00:03:32,400
masters and worker nodes

00:03:30,159 --> 00:03:34,799
the masters around the core control plan

00:03:32,400 --> 00:03:37,440
services of coblatius

00:03:34,799 --> 00:03:40,480
including the api server scheduler

00:03:37,440 --> 00:03:43,599
controller manager and the etcd store

00:03:40,480 --> 00:03:45,280
the api server exposes sap api to both

00:03:43,599 --> 00:03:47,040
applications and to kubernetes

00:03:45,280 --> 00:03:49,360
components

00:03:47,040 --> 00:03:51,200
other components including the scheduler

00:03:49,360 --> 00:03:52,560
controller manager and the components on

00:03:51,200 --> 00:03:55,680
the worker nodes

00:03:52,560 --> 00:03:57,599
will communicate with each other and

00:03:55,680 --> 00:04:00,480
they get and change the state of the

00:03:57,599 --> 00:04:02,319
cluster through api server

00:04:00,480 --> 00:04:04,400
the scheduler schedules containers to

00:04:02,319 --> 00:04:06,640
run and work nodes

00:04:04,400 --> 00:04:08,239
and the controller manager includes the

00:04:06,640 --> 00:04:10,239
core set of controllers

00:04:08,239 --> 00:04:12,319
which implement the core functions of

00:04:10,239 --> 00:04:15,200
kubernetes

00:04:12,319 --> 00:04:15,840
etc is a distributed keyword store it

00:04:15,200 --> 00:04:19,199
persists

00:04:15,840 --> 00:04:19,199
all the state of the cluster

00:04:20,000 --> 00:04:24,479
and worker nodes are the costs to run

00:04:22,479 --> 00:04:27,840
arbitrary containers

00:04:24,479 --> 00:04:29,759
one major class demand and work node is

00:04:27,840 --> 00:04:31,680
it interacts with the container wrong

00:04:29,759 --> 00:04:32,560
time to manage the containers on the

00:04:31,680 --> 00:04:34,479
node

00:04:32,560 --> 00:04:36,080
following the requests from the api

00:04:34,479 --> 00:04:39,360
server

00:04:36,080 --> 00:04:42,240
kobe process is another demon run on

00:04:39,360 --> 00:04:44,639
orca nodes it implements load balancing

00:04:42,240 --> 00:04:46,639
for combined services

00:04:44,639 --> 00:04:47,680
in later slides we will talk more about

00:04:46,639 --> 00:04:50,720
grant services

00:04:47,680 --> 00:04:50,720
and could be proxy

00:04:53,600 --> 00:04:56,880
ports are the smallest unit to deploy

00:04:56,000 --> 00:05:00,320
applications

00:04:56,880 --> 00:05:02,080
incubators a port can include one or

00:05:00,320 --> 00:05:03,840
more application containers

00:05:02,080 --> 00:05:05,440
for the world is continuously here a

00:05:03,840 --> 00:05:08,880
single ip address

00:05:05,440 --> 00:05:11,919
and a single network namespace so

00:05:08,880 --> 00:05:13,919
from networking perspective a port is a

00:05:11,919 --> 00:05:15,440
single network entity

00:05:13,919 --> 00:05:16,960
and the communication the internet

00:05:15,440 --> 00:05:20,000
communicating between ports

00:05:16,960 --> 00:05:22,840
between containers in a single port

00:05:20,000 --> 00:05:24,160
don't don't really go through the polar

00:05:22,840 --> 00:05:27,280
network

00:05:24,160 --> 00:05:29,440
so in a sense they are very like the ipc

00:05:27,280 --> 00:05:32,240
uh written processes in a single

00:05:29,440 --> 00:05:32,240
operating system

00:05:34,720 --> 00:05:38,320
so name space is another key concept in

00:05:36,880 --> 00:05:40,479
kubernetes

00:05:38,320 --> 00:05:42,400
namespace provide a way to divide the

00:05:40,479 --> 00:05:45,039
class resources between

00:05:42,400 --> 00:05:46,160
net between users uh so note that this

00:05:45,039 --> 00:05:48,160
namespace is a

00:05:46,160 --> 00:05:51,280
concept in class api not a network

00:05:48,160 --> 00:05:53,840
namespace we just talked about

00:05:51,280 --> 00:05:57,039
so many types of request resources must

00:05:53,840 --> 00:05:58,880
be created under namespace

00:05:57,039 --> 00:06:00,560
we think the namespace names of the

00:05:58,880 --> 00:06:02,400
resource must be unique

00:06:00,560 --> 00:06:03,919
but they need not to be unique course

00:06:02,400 --> 00:06:07,360
namespaces

00:06:03,919 --> 00:06:10,960
for example in this slide we

00:06:07,360 --> 00:06:13,520
shield two name spaces four and the bar

00:06:10,960 --> 00:06:16,479
labels have a port named radius master

00:06:13,520 --> 00:06:16,479
and the throw is named

00:06:18,840 --> 00:06:23,199
radius

00:06:20,240 --> 00:06:23,520
a kubaz service is poses an application

00:06:23,199 --> 00:06:27,280
by

00:06:23,520 --> 00:06:28,080
by a set of ports for example in this

00:06:27,280 --> 00:06:30,400
slide

00:06:28,080 --> 00:06:31,360
which your service exposes the redis

00:06:30,400 --> 00:06:35,520
database

00:06:31,360 --> 00:06:38,560
which is bad by two radius server ports

00:06:35,520 --> 00:06:41,520
so in kubernetes most of the workers

00:06:38,560 --> 00:06:44,000
actually will be deployed with services

00:06:41,520 --> 00:06:45,120
within the cluster a service can be

00:06:44,000 --> 00:06:46,960
accessed through it

00:06:45,120 --> 00:06:48,160
through a virtual ip allocated by

00:06:46,960 --> 00:06:51,520
kubernetes

00:06:48,160 --> 00:06:53,599
called class ip so this is like a load

00:06:51,520 --> 00:06:55,680
balancer virtual ip

00:06:53,599 --> 00:06:57,360
service request to the class ip will be

00:06:55,680 --> 00:07:00,240
distributed to the backend

00:06:57,360 --> 00:07:01,680
ports of the service we'll introduce the

00:07:00,240 --> 00:07:02,560
implementation of this solid-state

00:07:01,680 --> 00:07:05,680
balancing

00:07:02,560 --> 00:07:05,680
in our later slides

00:07:05,840 --> 00:07:10,960
also inside the cluster a service has a

00:07:09,039 --> 00:07:13,759
dns name

00:07:10,960 --> 00:07:17,440
post can look up a service class ip from

00:07:13,759 --> 00:07:19,199
building a dns server in the cluster

00:07:17,440 --> 00:07:20,800
so this is a very convenient service

00:07:19,199 --> 00:07:22,240
discovery mechanism for ports to

00:07:20,800 --> 00:07:25,759
discover service

00:07:22,240 --> 00:07:28,639
and look up the class ip of the service

00:07:25,759 --> 00:07:29,599
so besides access from the inside

00:07:28,639 --> 00:07:31,280
cluster

00:07:29,599 --> 00:07:33,199
you can also create a service of load

00:07:31,280 --> 00:07:34,720
balancer type to expose a service to

00:07:33,199 --> 00:07:36,319
installer clients

00:07:34,720 --> 00:07:38,560
but in this case you will need a

00:07:36,319 --> 00:07:44,720
terminal balancer to implement the load

00:07:38,560 --> 00:07:47,919
balancing for the service

00:07:44,720 --> 00:07:50,240
light policies uh define how ports

00:07:47,919 --> 00:07:51,039
are allowed to communicate with each

00:07:50,240 --> 00:07:54,080
other and

00:07:51,039 --> 00:07:55,280
other network conditions a ladder policy

00:07:54,080 --> 00:07:57,199
selects posts by

00:07:55,280 --> 00:07:59,840
matching the user defined labels on the

00:07:57,199 --> 00:08:01,520
ports or name spaces

00:07:59,840 --> 00:08:03,599
this includes the pause to apply the

00:08:01,520 --> 00:08:05,919
policy or

00:08:03,599 --> 00:08:08,000
source what does this impose for the

00:08:05,919 --> 00:08:11,199
traffic

00:08:08,000 --> 00:08:14,400
for example these styles use a simple

00:08:11,199 --> 00:08:15,120
net policy that allows white front end

00:08:14,400 --> 00:08:17,840
ports

00:08:15,120 --> 00:08:18,960
to access ready service provided by the

00:08:17,840 --> 00:08:21,919
radius ports

00:08:18,960 --> 00:08:24,479
on tcp port 6379 i think it's very

00:08:21,919 --> 00:08:25,680
straightforward to understand

00:08:24,479 --> 00:08:27,360
we'll further describe the

00:08:25,680 --> 00:08:30,560
implementation light policies with

00:08:27,360 --> 00:08:30,560
openweight switch later

00:08:33,839 --> 00:08:36,880
at a high level there are three

00:08:35,680 --> 00:08:38,399
communicating paths

00:08:36,880 --> 00:08:40,159
must be supported in kubernetes

00:08:38,399 --> 00:08:42,560
networking the first is

00:08:40,159 --> 00:08:43,839
product support communication each port

00:08:42,560 --> 00:08:47,200
should get its own

00:08:43,839 --> 00:08:49,360
ip address and all ports can communicate

00:08:47,200 --> 00:08:52,000
with other ports on all the nodes

00:08:49,360 --> 00:08:54,240
using their ip address without net the

00:08:52,000 --> 00:08:56,560
second is port to service communication

00:08:54,240 --> 00:08:58,240
as described earlier a port should be

00:08:56,560 --> 00:08:59,680
able to reach a service during the class

00:08:58,240 --> 00:09:01,360
ip of the service

00:08:59,680 --> 00:09:03,519
the last one is turner to service

00:09:01,360 --> 00:09:05,519
traffic we talk about it too

00:09:03,519 --> 00:09:07,600
a service can be exposed to external

00:09:05,519 --> 00:09:10,160
clients through external balancer or

00:09:07,600 --> 00:09:10,160
load port

00:09:11,200 --> 00:09:15,040
then come back to the main subject of

00:09:12,880 --> 00:09:15,600
today's talk compressing my network

00:09:15,040 --> 00:09:17,279
plugging

00:09:15,600 --> 00:09:19,279
so what a kubernetes network plugin is

00:09:17,279 --> 00:09:21,040
responsible for

00:09:19,279 --> 00:09:23,279
a lot of plugin imprints the portal

00:09:21,040 --> 00:09:24,080
network which should support the world's

00:09:23,279 --> 00:09:26,480
three connect

00:09:24,080 --> 00:09:27,760
connectivity scenarios we just shielded

00:09:26,480 --> 00:09:29,680
in last night

00:09:27,760 --> 00:09:31,120
pro to port port to service and internal

00:09:29,680 --> 00:09:33,279
to service

00:09:31,120 --> 00:09:36,080
it configures the network interface and

00:09:33,279 --> 00:09:38,880
allocates ip addresses for the ports

00:09:36,080 --> 00:09:39,519
it should support service traffic for

00:09:38,880 --> 00:09:41,040
the most

00:09:39,519 --> 00:09:42,959
like the plugins don't really implement

00:09:41,040 --> 00:09:45,920
the load balancing for service but just

00:09:42,959 --> 00:09:47,839
work with qb proxy for that

00:09:45,920 --> 00:09:50,720
a lot of plugins can also implement the

00:09:47,839 --> 00:09:52,640
light policy enforcement but don't

00:09:50,720 --> 00:09:54,320
forget not to order plugins support that

00:09:52,640 --> 00:09:56,080
policies

00:09:54,320 --> 00:09:58,160
last lateral plugin can implement

00:09:56,080 --> 00:10:00,640
traffic shipping which is the

00:09:58,160 --> 00:10:02,399
is an experimental feature of graphics

00:10:00,640 --> 00:10:05,839
to restrict the

00:10:02,399 --> 00:10:05,839
metal boundaries the ports

00:10:08,320 --> 00:10:13,600
so let's first look at kubilet

00:10:11,519 --> 00:10:14,959
which is a lot of plug-in cons with

00:10:13,600 --> 00:10:17,360
kubernetes

00:10:14,959 --> 00:10:19,040
keylight is very simple just like the

00:10:17,360 --> 00:10:21,839
darker bridge network it

00:10:19,040 --> 00:10:23,680
connects ports to another bridge using

00:10:21,839 --> 00:10:25,440
the virtual installation devices

00:10:23,680 --> 00:10:28,640
and the wall ports on the node will get

00:10:25,440 --> 00:10:30,720
the ip address from a single subnet

00:10:28,640 --> 00:10:32,320
so for the communicating of course notes

00:10:30,720 --> 00:10:35,040
kubernetes itself doesn't really do

00:10:32,320 --> 00:10:35,920
anything if i just relies on the

00:10:35,040 --> 00:10:37,920
underlaying

00:10:35,920 --> 00:10:39,440
cloud network to route the port traffic

00:10:37,920 --> 00:10:41,600
between nodes

00:10:39,440 --> 00:10:43,120
this will require support from the cloud

00:10:41,600 --> 00:10:46,880
network and works on

00:10:43,120 --> 00:10:48,959
some clouds like aws or tcp

00:10:46,880 --> 00:10:51,120
it's typically implemented by the

00:10:48,959 --> 00:10:53,200
kubrick's cloud provider

00:10:51,120 --> 00:10:55,360
which adds a round entry to the cloud

00:10:53,200 --> 00:10:57,440
network router for each node

00:10:55,360 --> 00:10:58,720
that tells the router uh the port

00:10:57,440 --> 00:11:01,839
standard can be reached

00:10:58,720 --> 00:11:01,839
through the node ip address

00:11:01,920 --> 00:11:06,079
could have done support later policies

00:11:08,800 --> 00:11:13,760
we mentioned it could be proxy earlier

00:11:11,440 --> 00:11:15,120
it's a demon runs on every node that

00:11:13,760 --> 00:11:17,440
implements load balancing for

00:11:15,120 --> 00:11:18,880
service of class ip and the load port

00:11:17,440 --> 00:11:20,720
tab

00:11:18,880 --> 00:11:22,959
for request from a local client to

00:11:20,720 --> 00:11:23,760
access a service series class ipo note

00:11:22,959 --> 00:11:26,160
port

00:11:23,760 --> 00:11:27,120
could be possible intercepted packet

00:11:26,160 --> 00:11:28,959
select one

00:11:27,120 --> 00:11:31,120
back-end port of the service to server

00:11:28,959 --> 00:11:32,160
request and the folder package to the

00:11:31,120 --> 00:11:34,800
port

00:11:32,160 --> 00:11:36,800
after changing after after changing the

00:11:34,800 --> 00:11:38,079
destination ip address to the portless

00:11:36,800 --> 00:11:40,560
ip address

00:11:38,079 --> 00:11:42,240
so anita's notes pre-process supports

00:11:40,560 --> 00:11:44,640
using ib tables

00:11:42,240 --> 00:11:45,360
ipos will use a space proxy mode to

00:11:44,640 --> 00:11:48,640
intercept

00:11:45,360 --> 00:11:49,360
and process the service traffic it gets

00:11:48,640 --> 00:11:52,160
the service and

00:11:49,360 --> 00:11:53,920
importance information from the api

00:11:52,160 --> 00:11:55,519
server

00:11:53,920 --> 00:11:57,519
for example with live tables could be

00:11:55,519 --> 00:11:59,680
processed creates iptables delight

00:11:57,519 --> 00:12:01,120
and group rules for each service to

00:11:59,680 --> 00:12:03,839
implement a load balancing of the

00:12:01,120 --> 00:12:03,839
service traffic

00:12:04,240 --> 00:12:08,399
so far i went through the quest

00:12:06,959 --> 00:12:10,320
networking concepts

00:12:08,399 --> 00:12:12,720
introduced the building network plug-in

00:12:10,320 --> 00:12:15,120
kubernetes and the kobe proxy

00:12:12,720 --> 00:12:17,200
last i will hand over to antonin who

00:12:15,120 --> 00:12:19,600
will do a deep depth of

00:12:17,200 --> 00:12:20,320
singing later plugging and how to build

00:12:19,600 --> 00:12:25,839
a

00:12:20,320 --> 00:12:25,839
quest network plug-in with open research

00:12:26,800 --> 00:12:30,639
uh thanks janjun so hopefully you can

00:12:28,959 --> 00:12:32,560
see my screen

00:12:30,639 --> 00:12:34,480
so the genjen has mentioned the cni

00:12:32,560 --> 00:12:36,639
interface a couple times

00:12:34,480 --> 00:12:38,079
the container network interface but how

00:12:36,639 --> 00:12:40,160
does it fit into the lifecycle

00:12:38,079 --> 00:12:42,160
of a pod in in a communities cluster uh

00:12:40,160 --> 00:12:44,240
we can look at it step by step

00:12:42,160 --> 00:12:46,560
at first the user creates a pod

00:12:44,240 --> 00:12:48,240
specification using the communities api

00:12:46,560 --> 00:12:50,240
for example this can be done by applying

00:12:48,240 --> 00:12:52,959
a yaml manifest using the kubectl

00:12:50,240 --> 00:12:54,959
command line tool then the community's

00:12:52,959 --> 00:12:56,720
control plane is going to select a node

00:12:54,959 --> 00:12:57,680
in the cluster and schedule the pattern

00:12:56,720 --> 00:12:59,680
to that node

00:12:57,680 --> 00:13:01,279
the cubelet agent running on the node is

00:12:59,680 --> 00:13:04,240
going to be notified and is now in

00:13:01,279 --> 00:13:06,639
charge of creating and running the pod

00:13:04,240 --> 00:13:08,399
in order to do so uh cubelet is going to

00:13:06,639 --> 00:13:11,519
invoke an interface called the container

00:13:08,399 --> 00:13:13,440
runtime interface or cri

00:13:11,519 --> 00:13:15,200
based on the operating system and the

00:13:13,440 --> 00:13:16,639
container runtime this is going to

00:13:15,200 --> 00:13:18,880
involve different things

00:13:16,639 --> 00:13:20,480
on linux as genji mentioned a network

00:13:18,880 --> 00:13:22,320
namespace is going to be created for the

00:13:20,480 --> 00:13:25,360
pod

00:13:22,320 --> 00:13:27,360
the container runtime also needs uh to

00:13:25,360 --> 00:13:29,120
invoke the container network interface

00:13:27,360 --> 00:13:30,480
the cni interface

00:13:29,120 --> 00:13:32,240
with the necessary information to

00:13:30,480 --> 00:13:34,079
configure networking for the pod

00:13:32,240 --> 00:13:36,959
and ensure that the pod becomes a part

00:13:34,079 --> 00:13:38,480
of the cluster network

00:13:36,959 --> 00:13:40,720
this involves creating the primary

00:13:38,480 --> 00:13:42,560
network interface for the pod is zero

00:13:40,720 --> 00:13:44,720
assigning an ip address to it or

00:13:42,560 --> 00:13:45,440
multiple ip addresses if you have v4 and

00:13:44,720 --> 00:13:48,720
v6

00:13:45,440 --> 00:13:50,399
potentially configuring routes and so on

00:13:48,720 --> 00:13:51,839
the cni interface is very simple to

00:13:50,399 --> 00:13:52,560
understand it has three different

00:13:51,839 --> 00:13:54,560
commands

00:13:52,560 --> 00:13:56,880
cni add which is invoked when the pod is

00:13:54,560 --> 00:13:57,600
created cni delete which is called when

00:13:56,880 --> 00:13:59,680
the pod is

00:13:57,600 --> 00:14:01,199
teared down and cni check which can be

00:13:59,680 --> 00:14:02,880
called periodically to validate the

00:14:01,199 --> 00:14:03,680
current network configuration for the

00:14:02,880 --> 00:14:05,040
pod

00:14:03,680 --> 00:14:07,040
on the right hand side you can see the

00:14:05,040 --> 00:14:09,040
api interactions between cubelet

00:14:07,040 --> 00:14:11,440
the container runtime and the network

00:14:09,040 --> 00:14:11,440
plugin

00:14:12,959 --> 00:14:16,079
so we're going to dive deeper and look

00:14:14,399 --> 00:14:16,959
at the different steps performed by the

00:14:16,079 --> 00:14:18,639
cni plugin

00:14:16,959 --> 00:14:20,639
but first we need to introduce open v

00:14:18,639 --> 00:14:22,399
switch also known as ovs

00:14:20,639 --> 00:14:24,639
as this is a data plane technology we're

00:14:22,399 --> 00:14:27,120
going to use as a running example

00:14:24,639 --> 00:14:28,639
ovs is a programmable virtual switch

00:14:27,120 --> 00:14:30,160
it's used to connect vms virtual

00:14:28,639 --> 00:14:31,920
machines or containers

00:14:30,160 --> 00:14:33,360
it has been a linux foundation project

00:14:31,920 --> 00:14:35,680
since 2016

00:14:33,360 --> 00:14:36,720
is widely used in production for example

00:14:35,680 --> 00:14:38,720
in openstack

00:14:36,720 --> 00:14:40,480
as a very active developers community

00:14:38,720 --> 00:14:42,639
and a lot of support for hardware from

00:14:40,480 --> 00:14:45,760
hardware vendors

00:14:42,639 --> 00:14:47,680
ovs is well integrated into the linux

00:14:45,760 --> 00:14:49,760
the ovs kernel module provides high

00:14:47,680 --> 00:14:51,680
performance forwarding and is available

00:14:49,760 --> 00:14:53,120
in a mainstream linux since kernel

00:14:51,680 --> 00:14:54,800
version 3.3

00:14:53,120 --> 00:14:57,040
that means that ovs is going to work out

00:14:54,800 --> 00:14:59,279
of the box in practically all the linux

00:14:57,040 --> 00:15:01,199
distributions

00:14:59,279 --> 00:15:02,800
ovs is also supported on windows which

00:15:01,199 --> 00:15:04,480
makes it a great choice if you want to

00:15:02,800 --> 00:15:05,040
use it to build the community's network

00:15:04,480 --> 00:15:06,720
plugin

00:15:05,040 --> 00:15:08,480
because then you don't have to duplicate

00:15:06,720 --> 00:15:10,000
software development efforts between

00:15:08,480 --> 00:15:13,680
linux and windows you can

00:15:10,000 --> 00:15:15,360
pretty much use the same data paths

00:15:13,680 --> 00:15:16,880
and i said before that obs was a

00:15:15,360 --> 00:15:18,320
programmable switch but what does

00:15:16,880 --> 00:15:20,320
programmability mean here

00:15:18,320 --> 00:15:22,160
well obvious can of course be used as a

00:15:20,320 --> 00:15:23,279
drop-in replacement for the linux bridge

00:15:22,160 --> 00:15:24,959
and just do

00:15:23,279 --> 00:15:26,399
regular switching between containers but

00:15:24,959 --> 00:15:28,480
that's not very interesting

00:15:26,399 --> 00:15:30,000
what obs lets you do is it lets you

00:15:28,480 --> 00:15:31,680
define your own pipeline

00:15:30,000 --> 00:15:33,759
and choose which packet other fields you

00:15:31,680 --> 00:15:34,399
want to match on and which actions you

00:15:33,759 --> 00:15:37,279
want to take

00:15:34,399 --> 00:15:38,560
as a result of those matches so thanks

00:15:37,279 --> 00:15:40,480
to programmability and we're going to

00:15:38,560 --> 00:15:42,720
see it in an instant

00:15:40,480 --> 00:15:43,920
all the communities network requirements

00:15:42,720 --> 00:15:46,079
that

00:15:43,920 --> 00:15:47,759
can be implemented in obs all of the all

00:15:46,079 --> 00:15:49,120
of those that janjan mentioned that's

00:15:47,759 --> 00:15:51,040
spot connectivity

00:15:49,120 --> 00:15:53,600
network policy enforcement service load

00:15:51,040 --> 00:15:55,680
balancing and so on

00:15:53,600 --> 00:15:57,440
and unlike the linux bridge ovs supports

00:15:55,680 --> 00:15:58,079
high performance packet io when you need

00:15:57,440 --> 00:16:01,120
it

00:15:58,079 --> 00:16:03,120
using technologies like dpdk af xdp

00:16:01,120 --> 00:16:06,000
and possibly hardware offload on

00:16:03,120 --> 00:16:06,000
supported next

00:16:07,600 --> 00:16:11,600
so here you have an example of the input

00:16:09,759 --> 00:16:13,279
parameters that are made available to a

00:16:11,600 --> 00:16:15,440
cni plugin

00:16:13,279 --> 00:16:16,800
the arguments specific to the current to

00:16:15,440 --> 00:16:18,320
the current container

00:16:16,800 --> 00:16:20,320
to the current pod being created are

00:16:18,320 --> 00:16:21,519
being passed as environment variables to

00:16:20,320 --> 00:16:23,839
the cni plugin

00:16:21,519 --> 00:16:26,000
and you can see those at the top here

00:16:23,839 --> 00:16:27,680
for kubernetes spot being created

00:16:26,000 --> 00:16:29,920
and the network configuration that you

00:16:27,680 --> 00:16:31,680
see at the bottom is uh streamed on the

00:16:29,920 --> 00:16:34,880
standard input when invoking the

00:16:31,680 --> 00:16:37,040
is the executable so let's see how we

00:16:34,880 --> 00:16:37,920
can use ovs to configure networking for

00:16:37,040 --> 00:16:39,600
this pod

00:16:37,920 --> 00:16:41,360
and build a network where all parts can

00:16:39,600 --> 00:16:44,800
communicate and that satisfies the

00:16:41,360 --> 00:16:44,800
community's network requirements

00:16:45,519 --> 00:16:48,560
so this is how things look initially on

00:16:47,600 --> 00:16:50,240
your node

00:16:48,560 --> 00:16:52,000
we assume that we have already created

00:16:50,240 --> 00:16:53,120
an obvious bridge on each kubernetes

00:16:52,000 --> 00:16:54,560
node in the cluster

00:16:53,120 --> 00:16:56,560
and we can do that using the command

00:16:54,560 --> 00:16:57,279
that you can see on the right hand side

00:16:56,560 --> 00:16:58,639
here

00:16:57,279 --> 00:17:00,720
and we're going to call that bridge

00:16:58,639 --> 00:17:01,279
brint which stands for integration

00:17:00,720 --> 00:17:02,720
bridge

00:17:01,279 --> 00:17:05,520
and it's a pretty common name for that

00:17:02,720 --> 00:17:06,720
kind of virtual network scenario

00:17:05,520 --> 00:17:09,120
we're also going to assume that all

00:17:06,720 --> 00:17:10,559
those nodes are running linux in the

00:17:09,120 --> 00:17:12,640
cluster

00:17:10,559 --> 00:17:14,480
so a network namespace has been created

00:17:12,640 --> 00:17:15,280
for the pod already by the container

00:17:14,480 --> 00:17:17,120
runtime

00:17:15,280 --> 00:17:18,959
and this network namespace that you can

00:17:17,120 --> 00:17:20,880
see on the picture has been passed

00:17:18,959 --> 00:17:22,720
as an argument to our network plugin

00:17:20,880 --> 00:17:25,439
using an environment variable as we saw

00:17:22,720 --> 00:17:27,120
in the previous slide

00:17:25,439 --> 00:17:28,400
so the first thing we're going to do and

00:17:27,120 --> 00:17:28,960
you can see those commands on the right

00:17:28,400 --> 00:17:30,160
hand side

00:17:28,960 --> 00:17:32,000
is we're going to enter the pod's

00:17:30,160 --> 00:17:33,840
network namespace and from now on

00:17:32,000 --> 00:17:35,120
every command we will run will be in

00:17:33,840 --> 00:17:37,280
that network namespace

00:17:35,120 --> 00:17:39,039
and we're going to create a vsphere as

00:17:37,280 --> 00:17:40,640
mentioned earlier a vsphere is a

00:17:39,039 --> 00:17:42,480
standard way of connecting a network

00:17:40,640 --> 00:17:45,039
namespace to a virtual bridge

00:17:42,480 --> 00:17:46,160
on linux this is basically a virtual

00:17:45,039 --> 00:17:47,840
ethernet cable

00:17:46,160 --> 00:17:49,360
and both ends of the cable both

00:17:47,840 --> 00:17:50,720
interfaces

00:17:49,360 --> 00:17:53,280
can can be in their own network

00:17:50,720 --> 00:17:53,280
namespace

00:17:53,520 --> 00:17:57,440
so speaking of which we will now move

00:17:55,520 --> 00:17:58,720
one of the two interfaces from the pods

00:17:57,440 --> 00:18:01,280
network namespace

00:17:58,720 --> 00:18:02,559
where we created the vsphere to the root

00:18:01,280 --> 00:18:04,480
network namespace

00:18:02,559 --> 00:18:06,480
of the host and we can see the command

00:18:04,480 --> 00:18:09,520
for that on the right-hand side

00:18:06,480 --> 00:18:10,640
so communities asked us to use e0 as a

00:18:09,520 --> 00:18:12,400
network interface

00:18:10,640 --> 00:18:14,320
a name for the pod so we need to make

00:18:12,400 --> 00:18:17,200
sure that we keep e0 in the pod's

00:18:14,320 --> 00:18:17,200
network namespace

00:18:18,240 --> 00:18:22,559
moving on we will configure the ip

00:18:20,480 --> 00:18:24,320
address on the e0 interface

00:18:22,559 --> 00:18:26,640
we don't have enough time today to dive

00:18:24,320 --> 00:18:29,679
into ip address management and how ip

00:18:26,640 --> 00:18:31,960
addresses or assigned to pods

00:18:29,679 --> 00:18:33,280
but the address you can see here

00:18:31,960 --> 00:18:35,039
10.10.1.2

00:18:33,280 --> 00:18:36,960
is taken from the subnet which was

00:18:35,039 --> 00:18:39,360
included in the network configuration

00:18:36,960 --> 00:18:41,679
that we passed to the cni executable

00:18:39,360 --> 00:18:42,640
on the standard input and typically in

00:18:41,679 --> 00:18:44,799
communities

00:18:42,640 --> 00:18:46,640
each node is going to receive one subnet

00:18:44,799 --> 00:18:48,720
allocated from a larger slider

00:18:46,640 --> 00:18:50,320
and the cni plugin running on each node

00:18:48,720 --> 00:18:51,679
will assign ip addresses from that

00:18:50,320 --> 00:18:53,440
subnet to pods

00:18:51,679 --> 00:18:54,799
but there are many different ipad

00:18:53,440 --> 00:18:56,960
mechanisms that

00:18:54,799 --> 00:19:00,080
cni plugins can use to to accommodate

00:18:56,960 --> 00:19:00,080
for different use cases

00:19:00,960 --> 00:19:04,720
so the last step is we're going to

00:19:02,480 --> 00:19:06,240
attach the vs interface that we move to

00:19:04,720 --> 00:19:10,000
the root network namespace

00:19:06,240 --> 00:19:10,880
to the obvs bridge and after that we can

00:19:10,000 --> 00:19:14,000
use here

00:19:10,880 --> 00:19:15,919
the ovs via ctl command line utility uh

00:19:14,000 --> 00:19:17,360
to check the bridge configuration and

00:19:15,919 --> 00:19:20,799
and we see that

00:19:17,360 --> 00:19:22,640
the v1 port uh which connects our

00:19:20,799 --> 00:19:25,840
pod to the bridge has been added to the

00:19:22,640 --> 00:19:25,840
to the port list

00:19:27,120 --> 00:19:31,280
so it's good for the scope of a single

00:19:29,679 --> 00:19:32,960
pad here but let's assume that we have

00:19:31,280 --> 00:19:34,640
just created multiple pods like the

00:19:32,960 --> 00:19:35,919
previous one on one node

00:19:34,640 --> 00:19:38,559
and we want them to be able to

00:19:35,919 --> 00:19:40,559
communicate well by default

00:19:38,559 --> 00:19:43,039
ovs is going to behave like a regular l2

00:19:40,559 --> 00:19:44,720
linux bridge and in our case

00:19:43,039 --> 00:19:46,960
all of those pods here as we saw

00:19:44,720 --> 00:19:48,400
previously are on the same subnet

00:19:46,960 --> 00:19:50,640
so they should be able to talk to each

00:19:48,400 --> 00:19:52,880
other just fine

00:19:50,640 --> 00:19:54,480
however because obs is a programmable

00:19:52,880 --> 00:19:54,960
switch we can do things a little bit

00:19:54,480 --> 00:19:57,440
better

00:19:54,960 --> 00:19:59,039
uh for example here we can add some

00:19:57,440 --> 00:20:02,000
flows to the obvious bridge

00:19:59,039 --> 00:20:03,679
uh to prevent iep or arb spoofing uh

00:20:02,000 --> 00:20:04,320
flows are how you define forwarding in

00:20:03,679 --> 00:20:06,240
obs

00:20:04,320 --> 00:20:07,679
so you in each flow you match on some

00:20:06,240 --> 00:20:10,000
parts of the packet headers

00:20:07,679 --> 00:20:11,440
and you take actions for example modify

00:20:10,000 --> 00:20:13,919
the value of some error field

00:20:11,440 --> 00:20:15,760
decrement the ttl output the packet on a

00:20:13,919 --> 00:20:17,440
specific port

00:20:15,760 --> 00:20:18,960
and if you look at the first command in

00:20:17,440 --> 00:20:20,640
the in the blue box

00:20:18,960 --> 00:20:22,720
you can see that we add a flow that will

00:20:20,640 --> 00:20:23,280
match all our packets coming from our

00:20:22,720 --> 00:20:26,240
pod

00:20:23,280 --> 00:20:27,360
and if the ip address or the mac address

00:20:26,240 --> 00:20:28,960
advertised by the

00:20:27,360 --> 00:20:30,720
and the mac address advertised by the r

00:20:28,960 --> 00:20:32,000
packet match what we have configured for

00:20:30,720 --> 00:20:33,280
the pod previously

00:20:32,000 --> 00:20:35,120
then we will forward the traffic

00:20:33,280 --> 00:20:37,919
normally if they don't match we will

00:20:35,120 --> 00:20:37,919
drop the traffic

00:20:38,799 --> 00:20:42,880
so that's great for intranode part to

00:20:40,960 --> 00:20:44,480
put traffic but on the same node

00:20:42,880 --> 00:20:45,600
but what about pods which are scheduled

00:20:44,480 --> 00:20:46,960
on different nodes how do they

00:20:45,600 --> 00:20:49,120
communicate

00:20:46,960 --> 00:20:50,799
we saw earlier that cubenet the default

00:20:49,120 --> 00:20:52,720
communities network solution

00:20:50,799 --> 00:20:54,880
relies on the cloud provider to program

00:20:52,720 --> 00:20:56,640
routes into the cloud network fabric

00:20:54,880 --> 00:20:58,480
but in our case we can take an alternate

00:20:56,640 --> 00:21:02,000
approach and we can build an overlay

00:20:58,480 --> 00:21:03,440
network with openview switch

00:21:02,000 --> 00:21:05,760
and it's actually pretty straightforward

00:21:03,440 --> 00:21:06,320
to do we can configure tunnels between

00:21:05,760 --> 00:21:08,640
nodes

00:21:06,320 --> 00:21:11,280
and we instruct obs to do forward pod

00:21:08,640 --> 00:21:14,640
traffic on on the appropriate tunnel

00:21:11,280 --> 00:21:16,480
uh based on the destination ip address

00:21:14,640 --> 00:21:18,240
uh it's very easy to add a node to

00:21:16,480 --> 00:21:19,919
another network

00:21:18,240 --> 00:21:22,080
you can see the command on in the blue

00:21:19,919 --> 00:21:25,120
box here and uh

00:21:22,080 --> 00:21:26,640
as you create a tunnel port to join the

00:21:25,120 --> 00:21:28,400
overall network you can choose which

00:21:26,640 --> 00:21:29,440
encapsulation protocol you want to use

00:21:28,400 --> 00:21:31,520
for the overlay

00:21:29,440 --> 00:21:33,840
in the example here we choose to use

00:21:31,520 --> 00:21:35,440
geneve

00:21:33,840 --> 00:21:37,200
we take an approach called flow based

00:21:35,440 --> 00:21:39,440
tunneling so instead of creating one

00:21:37,200 --> 00:21:41,200
terminal port for each node

00:21:39,440 --> 00:21:43,600
on each node for each remote node in the

00:21:41,200 --> 00:21:45,440
cluster we create a single tunnel port

00:21:43,600 --> 00:21:47,120
and then we're gonna

00:21:45,440 --> 00:21:48,799
add different flows to configure

00:21:47,120 --> 00:21:51,600
tunneling for the different

00:21:48,799 --> 00:21:51,600
remote nodes

00:21:52,320 --> 00:21:55,520
uh we can look at a specific scenario

00:21:54,240 --> 00:21:57,280
here on the picture

00:21:55,520 --> 00:21:59,440
uh and which flows we would need to add

00:21:57,280 --> 00:22:01,600
to provide inter node part connectivity

00:21:59,440 --> 00:22:03,679
uh so if you look at node one here

00:22:01,600 --> 00:22:05,200
you can see we are running two parts but

00:22:03,679 --> 00:22:08,400
one and pod one b

00:22:05,200 --> 00:22:09,120
and uh the flows that we need to add uh

00:22:08,400 --> 00:22:11,440
to

00:22:09,120 --> 00:22:12,640
to communicate with the pods on on note

00:22:11,440 --> 00:22:15,840
2 or shown in the

00:22:12,640 --> 00:22:17,760
in the blue box um

00:22:15,840 --> 00:22:19,039
an interesting observation is that with

00:22:17,760 --> 00:22:21,280
our design

00:22:19,039 --> 00:22:22,159
and our overall network we do not send

00:22:21,280 --> 00:22:23,919
we didn't

00:22:22,159 --> 00:22:26,080
send any broadcast traffic for example

00:22:23,919 --> 00:22:27,760
our traffic across nodes

00:22:26,080 --> 00:22:30,400
and this is going to help reduce the

00:22:27,760 --> 00:22:31,919
amount of traffic overall in the cluster

00:22:30,400 --> 00:22:33,520
and it means that we do not have to

00:22:31,919 --> 00:22:34,320
worry about loops for the overall

00:22:33,520 --> 00:22:36,400
network

00:22:34,320 --> 00:22:41,120
and we don't have to enable a protocol

00:22:36,400 --> 00:22:43,120
like a spanning tree

00:22:41,120 --> 00:22:44,559
so as a recap let's take a step back and

00:22:43,120 --> 00:22:46,400
look at the different packet paths for

00:22:44,559 --> 00:22:47,919
the different categories of traffics or

00:22:46,400 --> 00:22:50,559
that our ci plugin can

00:22:47,919 --> 00:22:52,080
handle so intra node part-to-pot traffic

00:22:50,559 --> 00:22:53,200
is going to be switched locally by the

00:22:52,080 --> 00:22:55,520
obs bridge

00:22:53,200 --> 00:22:58,080
for inter node traffic we have built an

00:22:55,520 --> 00:23:00,480
overall network and we use encapsulation

00:22:58,080 --> 00:23:01,440
finally we didn't look at the details of

00:23:00,480 --> 00:23:03,679
this but

00:23:01,440 --> 00:23:05,120
for pod traffic that needs to go outside

00:23:03,679 --> 00:23:07,360
of the pod network

00:23:05,120 --> 00:23:09,600
we can configure ip tables for example

00:23:07,360 --> 00:23:12,799
and use snap

00:23:09,600 --> 00:23:14,720
to masquerade outgoing traffic

00:23:12,799 --> 00:23:17,039
the important takeaway here is that the

00:23:14,720 --> 00:23:18,720
programmability of ovs enables us to

00:23:17,039 --> 00:23:19,440
implement the entire community's network

00:23:18,720 --> 00:23:21,039
model

00:23:19,440 --> 00:23:22,480
up until now we have been looking at a

00:23:21,039 --> 00:23:24,159
toy example

00:23:22,480 --> 00:23:25,919
looking at how we can use the ovs

00:23:24,159 --> 00:23:26,960
command line tools to configure the pod

00:23:25,919 --> 00:23:28,720
network

00:23:26,960 --> 00:23:31,600
but genjin is now going to introduce you

00:23:28,720 --> 00:23:36,799
to a real life currency network plugin

00:23:31,600 --> 00:23:39,919
which was built with obs

00:23:36,799 --> 00:23:41,520
so while i'm toning just described

00:23:39,919 --> 00:23:43,440
how to implant quest networking with

00:23:41,520 --> 00:23:45,279
openweight switch there are actually

00:23:43,440 --> 00:23:48,000
many other implementations of cli

00:23:45,279 --> 00:23:50,000
plugins using different technologies

00:23:48,000 --> 00:23:51,760
on a single project page there are

00:23:50,000 --> 00:23:54,480
totally 26

00:23:51,760 --> 00:23:57,840
third-party plugins listed besides a few

00:23:54,480 --> 00:24:00,400
plugins maintained by the cni project

00:23:57,840 --> 00:24:01,120
so inside i would give a high-level

00:24:00,400 --> 00:24:02,880
overview of

00:24:01,120 --> 00:24:04,159
several later plugins filled with

00:24:02,880 --> 00:24:06,880
different technologies

00:24:04,159 --> 00:24:08,799
including andrea karnikov celia and the

00:24:06,880 --> 00:24:11,520
flannel

00:24:08,799 --> 00:24:13,039
so andrea used openly switch as a

00:24:11,520 --> 00:24:15,120
network data plan

00:24:13,039 --> 00:24:17,360
almost all its features are implemented

00:24:15,120 --> 00:24:19,039
using openweight switch

00:24:17,360 --> 00:24:20,400
used bgp to implement the routing of

00:24:19,039 --> 00:24:22,240
port traffic

00:24:20,400 --> 00:24:24,480
and the language ip tables for later

00:24:22,240 --> 00:24:27,520
policy enforcement

00:24:24,480 --> 00:24:30,559
since the recent version 316 it also

00:24:27,520 --> 00:24:32,080
supports ebpf for that policies

00:24:30,559 --> 00:24:34,159
seniors data plan implementation is

00:24:32,080 --> 00:24:36,320
mainly based on ebpf

00:24:34,159 --> 00:24:37,600
phenol is another popular network plugin

00:24:36,320 --> 00:24:40,080
um

00:24:37,600 --> 00:24:42,000
it's very simple and then leverage the

00:24:40,080 --> 00:24:44,480
linux bridge for connecting ports to the

00:24:42,000 --> 00:24:44,480
network

00:24:44,960 --> 00:24:51,440
so for network led for port

00:24:48,080 --> 00:24:53,440
network uh all the four plugins support

00:24:51,440 --> 00:24:56,880
overlay but with different tunnel

00:24:53,440 --> 00:24:57,919
particles and they uh also for the low

00:24:56,880 --> 00:24:59,600
encapsulation mode

00:24:57,919 --> 00:25:01,120
that leverage the cloud network to

00:24:59,600 --> 00:25:04,159
resolve traffic

00:25:01,120 --> 00:25:06,080
between nodes calico additionally

00:25:04,159 --> 00:25:07,760
supports pdp routing

00:25:06,080 --> 00:25:10,000
and can change results with other

00:25:07,760 --> 00:25:11,600
electrical routers using pgp

00:25:10,000 --> 00:25:13,600
when that is supported by the underlying

00:25:11,600 --> 00:25:15,840
network

00:25:13,600 --> 00:25:17,600
england 3 our land policy are enforced

00:25:15,840 --> 00:25:19,520
by open research 2.

00:25:17,600 --> 00:25:21,440
different from other implementations

00:25:19,520 --> 00:25:22,960
andrea does centralized land policy

00:25:21,440 --> 00:25:24,799
computation

00:25:22,960 --> 00:25:26,720
in which a single controller computes

00:25:24,799 --> 00:25:27,600
select policy and disseminates them to

00:25:26,720 --> 00:25:30,559
the relevant

00:25:27,600 --> 00:25:33,200
nodes instead of having every node watch

00:25:30,559 --> 00:25:34,720
policies and the computer locally

00:25:33,200 --> 00:25:38,240
we have more details about the light

00:25:34,720 --> 00:25:41,039
voicing implementation entries later

00:25:38,240 --> 00:25:44,400
called ecology leveraged ib tables for

00:25:41,039 --> 00:25:47,440
or ebpf to enforce net policies

00:25:44,400 --> 00:25:48,880
syrian use uh eppf only

00:25:47,440 --> 00:25:51,039
and the flannel dynasty for data

00:25:48,880 --> 00:25:53,840
policies

00:25:51,039 --> 00:25:55,440
except the cnn there are other three uh

00:25:53,840 --> 00:25:58,720
the other three plugins

00:25:55,440 --> 00:26:00,480
support class and windows 2.

00:25:58,720 --> 00:26:02,559
entry are still libraries open with

00:26:00,480 --> 00:26:05,039
switch on windows notes

00:26:02,559 --> 00:26:06,559
calico changed to use windows building

00:26:05,039 --> 00:26:08,720
bgp implementation

00:26:06,559 --> 00:26:11,600
and they use this windows virtual filter

00:26:08,720 --> 00:26:13,919
platform for that policies

00:26:11,600 --> 00:26:17,600
flanno also supports windows with the

00:26:13,919 --> 00:26:17,600
windows bridge and overlay drivers

00:26:17,760 --> 00:26:22,480
there are also a field letter plugin

00:26:20,000 --> 00:26:25,840
built for specific cloud platforms

00:26:22,480 --> 00:26:28,080
like aws azure gcp or nst

00:26:25,840 --> 00:26:30,240
these plugins implement port network

00:26:28,080 --> 00:26:32,159
connectivity with cloud native network

00:26:30,240 --> 00:26:34,080
for example they might just call cloud

00:26:32,159 --> 00:26:35,919
api to create networks and allocate ip

00:26:34,080 --> 00:26:39,840
address for the ports

00:26:35,919 --> 00:26:39,840
from the network

00:26:40,640 --> 00:26:45,360
so as stated earlier project has an open

00:26:43,279 --> 00:26:47,840
source network plugin for queries

00:26:45,360 --> 00:26:49,360
that uses open with switch on the

00:26:47,840 --> 00:26:51,679
network data plan

00:26:49,360 --> 00:26:52,799
so last we will go a little deeper with

00:26:51,679 --> 00:26:54,720
project here

00:26:52,799 --> 00:26:56,240
and use it to further demonstrate

00:26:54,720 --> 00:26:58,559
kubernetes networking with openweight

00:26:56,240 --> 00:26:58,559
switch

00:27:00,640 --> 00:27:07,840
and we are still a young project that

00:27:04,840 --> 00:27:07,840
has

00:27:09,760 --> 00:27:16,880
it has picked up some momentum already

00:27:14,400 --> 00:27:18,159
one month ago the product was to provide

00:27:16,880 --> 00:27:20,399
a good user experience

00:27:18,159 --> 00:27:22,000
it's very simple to deploy and manage

00:27:20,399 --> 00:27:24,480
and provides tooling for

00:27:22,000 --> 00:27:25,919
network diagnostics and the fun green

00:27:24,480 --> 00:27:28,000
visibility

00:27:25,919 --> 00:27:29,279
we will see this express from our demos

00:27:28,000 --> 00:27:32,880
later

00:27:29,279 --> 00:27:36,559
andrea can run any welcome restaurants

00:27:32,880 --> 00:27:39,200
private clouds particles eyes

00:27:36,559 --> 00:27:40,880
liners and windows notes stands to the

00:27:39,200 --> 00:27:43,120
good portability openmic switch

00:27:40,880 --> 00:27:44,159
andrea supports all these platforms and

00:27:43,120 --> 00:27:47,440
operating systems

00:27:44,159 --> 00:27:48,000
with the unified implementation besides

00:27:47,440 --> 00:27:51,039
questionnaire

00:27:48,000 --> 00:27:51,600
policy entry also provides a native

00:27:51,039 --> 00:27:54,000
secret

00:27:51,600 --> 00:28:02,000
security policies and builds a

00:27:54,000 --> 00:28:04,240
comprehensive policy model

00:28:02,000 --> 00:28:08,080
so this style shows the anterior

00:28:04,240 --> 00:28:08,080
components of heart and how they fit the

00:28:12,320 --> 00:28:16,240
uncharging runs on every node using a

00:28:14,720 --> 00:28:18,799
kubrick's debug set

00:28:16,240 --> 00:28:20,880
it manages the os bridge under node

00:28:18,799 --> 00:28:22,320
takes care of a port network interface

00:28:20,880 --> 00:28:24,880
calculation

00:28:22,320 --> 00:28:27,120
it handles a ci cores from kubrick and

00:28:24,880 --> 00:28:27,919
creates virtual design devices for new

00:28:27,120 --> 00:28:31,039
ports

00:28:27,919 --> 00:28:32,960
and connects them to the os bridge it

00:28:31,039 --> 00:28:35,440
creates overlay tunnels

00:28:32,960 --> 00:28:36,799
invents traffic forwarding nato policy

00:28:35,440 --> 00:28:38,960
enforcement

00:28:36,799 --> 00:28:38,960
and

00:28:42,080 --> 00:28:45,760
functions can be implemented with openly

00:28:44,080 --> 00:28:47,840
switch and open flow

00:28:45,760 --> 00:28:50,080
so in a sense android agent just

00:28:47,840 --> 00:28:51,919
automates the os configuration and

00:28:50,080 --> 00:28:53,919
open flow programming based on the

00:28:51,919 --> 00:28:55,200
information from kubernetes and android

00:28:53,919 --> 00:28:58,480
controller

00:28:55,200 --> 00:29:01,200
for example it watches node service

00:28:58,480 --> 00:29:03,200
and the points from chroma's api server

00:29:01,200 --> 00:29:04,880
gets the port subnet to know the ip

00:29:03,200 --> 00:29:05,840
mapping to create tunnels to remote

00:29:04,880 --> 00:29:07,600
nodes

00:29:05,840 --> 00:29:10,399
and the gas and the points of the source

00:29:07,600 --> 00:29:14,000
create as slow as load balancing flows

00:29:10,399 --> 00:29:14,000
in open wave switch with openflow

00:29:14,080 --> 00:29:17,360
the other major component is anterior

00:29:15,840 --> 00:29:19,840
controller

00:29:17,360 --> 00:29:22,240
which runs as a compressed department

00:29:19,840 --> 00:29:25,120
until a controller watches correct

00:29:22,240 --> 00:29:26,320
network policies from the api server

00:29:25,120 --> 00:29:28,399
transforms them and

00:29:26,320 --> 00:29:30,080
disseminates them to the relevant entry

00:29:28,399 --> 00:29:32,080
agents

00:29:30,080 --> 00:29:34,880
we have a slide later to talk more about

00:29:32,080 --> 00:29:36,720
the library information country

00:29:34,880 --> 00:29:38,480
uh what's to mention and share

00:29:36,720 --> 00:29:40,159
controller leverage graphics api server

00:29:38,480 --> 00:29:43,200
library to build a high performance

00:29:40,159 --> 00:29:45,200
communicating channel to agents and also

00:29:43,200 --> 00:29:47,919
to use post quest style api

00:29:45,200 --> 00:29:49,840
the channel can scale pretty well

00:29:47,919 --> 00:29:51,200
actually we try to leverage class

00:29:49,840 --> 00:29:54,960
technologies

00:29:51,200 --> 00:29:57,200
as much as possible when we build andrea

00:29:54,960 --> 00:29:59,120
the strategy together with the openly

00:29:57,200 --> 00:30:00,720
switch reduced the implementing

00:29:59,120 --> 00:30:03,760
complexity a lot

00:30:00,720 --> 00:30:07,520
and enabled us to add more features for

00:30:03,760 --> 00:30:09,520
crest networking at a very fast pace

00:30:07,520 --> 00:30:11,200
all the entire components including the

00:30:09,520 --> 00:30:13,200
controller agent

00:30:11,200 --> 00:30:14,880
and the even os daemons are

00:30:13,200 --> 00:30:17,120
containerized

00:30:14,880 --> 00:30:20,840
and they can be deployed by applying a

00:30:17,120 --> 00:30:22,000
quest manifest with a single line of

00:30:20,840 --> 00:30:24,399
command

00:30:22,000 --> 00:30:25,039
you jenjen uh so now that uh generally

00:30:24,399 --> 00:30:27,679
introduced

00:30:25,039 --> 00:30:28,320
everyone to project entry i want to show

00:30:27,679 --> 00:30:31,600
a short

00:30:28,320 --> 00:30:34,559
uh demo video of how to deploy

00:30:31,600 --> 00:30:36,480
entry on a cluster so hopefully everyone

00:30:34,559 --> 00:30:39,279
can see the video now

00:30:36,480 --> 00:30:40,159
so what i'm doing is i'm creating a

00:30:39,279 --> 00:30:42,559
cluster

00:30:40,159 --> 00:30:43,919
right now using a tool called k-ops on

00:30:42,559 --> 00:30:45,919
aws ec2

00:30:43,919 --> 00:30:47,120
as there are many different tools both

00:30:45,919 --> 00:30:49,760
open source and

00:30:47,120 --> 00:30:51,760
commercial to create clusters like this

00:30:49,760 --> 00:30:53,120
so it only takes a few minutes to create

00:30:51,760 --> 00:30:56,320
a multi-node cluster

00:30:53,120 --> 00:30:57,519
and uh i i skipped followed here so that

00:30:56,320 --> 00:30:59,919
we don't have to wait

00:30:57,519 --> 00:31:01,600
but once it's done i can use cubectl to

00:30:59,919 --> 00:31:02,480
access my cluster and here i'm looking

00:31:01,600 --> 00:31:04,640
at the

00:31:02,480 --> 00:31:06,320
different nodes in the cluster you can

00:31:04,640 --> 00:31:07,600
see that they are in the not ready state

00:31:06,320 --> 00:31:09,760
and that's because i haven't

00:31:07,600 --> 00:31:10,720
deployed a network plugin to my cluster

00:31:09,760 --> 00:31:13,840
yet

00:31:10,720 --> 00:31:14,640
so i'm going to download entria um right

00:31:13,840 --> 00:31:16,880
now i'm just

00:31:14,640 --> 00:31:18,000
looking at the entry ammo manifest as

00:31:16,880 --> 00:31:19,840
janjun said everything

00:31:18,000 --> 00:31:21,039
can be done by applying a single

00:31:19,840 --> 00:31:23,519
manifest and

00:31:21,039 --> 00:31:25,600
i'm quickly editing some configuration

00:31:23,519 --> 00:31:26,559
parameters to enable some alpha level

00:31:25,600 --> 00:31:30,880
features which are

00:31:26,559 --> 00:31:32,720
disabled by default once this is done

00:31:30,880 --> 00:31:34,320
i just need to apply the yaml which is

00:31:32,720 --> 00:31:36,320
going to create all the communities

00:31:34,320 --> 00:31:38,399
resources

00:31:36,320 --> 00:31:39,440
needed for entria and after that we can

00:31:38,399 --> 00:31:41,200
watch the pods

00:31:39,440 --> 00:31:43,360
and we can see we have three entry agent

00:31:41,200 --> 00:31:44,799
pods one per node and one entry a

00:31:43,360 --> 00:31:45,760
controller pod which is like the

00:31:44,799 --> 00:31:48,000
centralized

00:31:45,760 --> 00:31:48,880
network policy computation engine uh it

00:31:48,000 --> 00:31:50,720
takes

00:31:48,880 --> 00:31:52,480
about a minute for everything to become

00:31:50,720 --> 00:31:53,840
ready here because a docker image has to

00:31:52,480 --> 00:31:57,039
be pulled from

00:31:53,840 --> 00:31:58,480
a docker hub and and so on but after

00:31:57,039 --> 00:32:00,880
that

00:31:58,480 --> 00:32:02,320
we can see that all the dns services

00:32:00,880 --> 00:32:03,600
already and if we look at the nodes

00:32:02,320 --> 00:32:05,519
again for the cluster

00:32:03,600 --> 00:32:06,799
we can see that those nodes are now

00:32:05,519 --> 00:32:09,120
ready

00:32:06,799 --> 00:32:11,840
because my network plugin is has been

00:32:09,120 --> 00:32:14,240
installed and is running

00:32:11,840 --> 00:32:15,440
so what i'm doing here is i want to

00:32:14,240 --> 00:32:17,440
deploy some pods

00:32:15,440 --> 00:32:18,480
uh some web server pods three of them as

00:32:17,440 --> 00:32:20,720
a deployment

00:32:18,480 --> 00:32:22,799
uh to my cluster just as an example to

00:32:20,720 --> 00:32:24,159
see how networking is configured for

00:32:22,799 --> 00:32:25,039
them so i'm looking at those parts

00:32:24,159 --> 00:32:28,159
they're running

00:32:25,039 --> 00:32:29,360
i can see on which nodes are running

00:32:28,159 --> 00:32:31,360
and what i'm going to do is i'm going to

00:32:29,360 --> 00:32:33,200
select one of those uh

00:32:31,360 --> 00:32:34,559
nodes here i think the one with the ip

00:32:33,200 --> 00:32:37,919
address that ends in

00:32:34,559 --> 00:32:39,600
201 and i want to find out which agent

00:32:37,919 --> 00:32:41,440
pod is running on that node so that

00:32:39,600 --> 00:32:43,440
would be that one and now i can

00:32:41,440 --> 00:32:44,960
exact into that pod more precisely i'm

00:32:43,440 --> 00:32:47,039
gonna exec into the

00:32:44,960 --> 00:32:49,679
entry ovs container for that pod which

00:32:47,039 --> 00:32:51,760
runs the ovs daemons

00:32:49,679 --> 00:32:53,760
and i'm gonna inspect a few things so we

00:32:51,760 --> 00:32:54,640
can use obvious vsctl to look at the

00:32:53,760 --> 00:32:57,440
bridge

00:32:54,640 --> 00:32:59,279
and we can see that we have one port for

00:32:57,440 --> 00:33:00,159
each one of our web server pods running

00:32:59,279 --> 00:33:02,000
on that node

00:33:00,159 --> 00:33:03,519
we're like a gateway port we didn't get

00:33:02,000 --> 00:33:04,799
into the details of that and we have the

00:33:03,519 --> 00:33:06,720
tunnel port

00:33:04,799 --> 00:33:08,320
which makes sure that the node becomes

00:33:06,720 --> 00:33:10,799
part of the overlay network

00:33:08,320 --> 00:33:11,600
we can also dump all the ovs flows for

00:33:10,799 --> 00:33:13,279
that bridge

00:33:11,600 --> 00:33:14,640
and we can see that even with a small

00:33:13,279 --> 00:33:16,880
number of pod

00:33:14,640 --> 00:33:19,279
that's that can be quite a long list uh

00:33:16,880 --> 00:33:21,200
which is why for entria we've built some

00:33:19,279 --> 00:33:22,480
command line utility which is called

00:33:21,200 --> 00:33:25,440
ncarl

00:33:22,480 --> 00:33:27,360
to help with uh troubleshooting and

00:33:25,440 --> 00:33:29,200
inspecting the network configuration

00:33:27,360 --> 00:33:30,480
on the node so here i'm looking at some

00:33:29,200 --> 00:33:34,080
available command and

00:33:30,480 --> 00:33:37,440
one of them is getpod interface to print

00:33:34,080 --> 00:33:38,000
information about uh the vc interface

00:33:37,440 --> 00:33:40,480
for local

00:33:38,000 --> 00:33:42,799
pods and we can see once again our two

00:33:40,480 --> 00:33:44,159
web server pods with their ip address

00:33:42,799 --> 00:33:47,440
and mac address and

00:33:44,159 --> 00:33:48,799
of port id we can also look at the

00:33:47,440 --> 00:33:52,480
different flows

00:33:48,799 --> 00:33:55,120
uh in in a way that's a bit more

00:33:52,480 --> 00:33:56,559
structured than with uh obvious command

00:33:55,120 --> 00:33:58,399
line tools directly

00:33:56,559 --> 00:34:00,000
so here i'm gonna dump the flows for a

00:33:58,399 --> 00:34:01,760
specific web server pod

00:34:00,000 --> 00:34:04,799
and we can see i only get a list of four

00:34:01,760 --> 00:34:07,840
flows uh those flows are dedicated to

00:34:04,799 --> 00:34:10,800
uh to the to that pod

00:34:07,840 --> 00:34:11,679
let's stop the video here um and let's

00:34:10,800 --> 00:34:15,280
go back

00:34:11,679 --> 00:34:17,200
to the slides um now moving on to

00:34:15,280 --> 00:34:18,560
network policy implementation and janjan

00:34:17,200 --> 00:34:20,639
already talked a lot about this and i

00:34:18,560 --> 00:34:23,599
don't want to get into too many details

00:34:20,639 --> 00:34:25,359
uh for the sake of time but i think the

00:34:23,599 --> 00:34:27,280
the key takeaway here is that

00:34:25,359 --> 00:34:28,399
we have a centralized approach in in

00:34:27,280 --> 00:34:30,240
entry uh

00:34:28,399 --> 00:34:31,839
because we see multiple advantages to

00:34:30,240 --> 00:34:33,359
that uh

00:34:31,839 --> 00:34:35,440
one of them is that we reduce the amount

00:34:33,359 --> 00:34:36,800
of computation that each agent on each

00:34:35,440 --> 00:34:38,079
node has to perform

00:34:36,800 --> 00:34:40,079
and we reduce the load on the

00:34:38,079 --> 00:34:42,639
communities api server by

00:34:40,079 --> 00:34:46,159
having a single entity the controller

00:34:42,639 --> 00:34:46,159
connect to the kubernetes api

00:34:46,839 --> 00:34:51,280
um and so

00:34:49,200 --> 00:34:52,560
uh yeah as janjan mentioned to

00:34:51,280 --> 00:34:55,919
distribute computed

00:34:52,560 --> 00:34:57,760
internal policy objects uh we use

00:34:55,919 --> 00:34:59,440
the communities api server library so

00:34:57,760 --> 00:34:59,920
the same library which is used to run

00:34:59,440 --> 00:35:03,040
the

00:34:59,920 --> 00:35:05,359
main communities api and

00:35:03,040 --> 00:35:07,280
using this enabled us to achieve a very

00:35:05,359 --> 00:35:11,680
performant communication channel

00:35:07,280 --> 00:35:11,680
with little engineering effort

00:35:11,920 --> 00:35:15,520
so i'm going quickly because i want to

00:35:13,440 --> 00:35:16,000
be able to show the all the demo videos

00:35:15,520 --> 00:35:18,720
so if

00:35:16,000 --> 00:35:21,200
i move on to the second one which is

00:35:18,720 --> 00:35:23,839
also pretty short

00:35:21,200 --> 00:35:25,040
it kind of like dives deeper into

00:35:23,839 --> 00:35:26,880
network policies so

00:35:25,040 --> 00:35:28,720
once again we have our web server

00:35:26,880 --> 00:35:31,520
deployment

00:35:28,720 --> 00:35:33,359
we have also a couple web clients and a

00:35:31,520 --> 00:35:35,359
couple of other clients which are

00:35:33,359 --> 00:35:37,520
named and labeled other clients here and

00:35:35,359 --> 00:35:39,200
you'll understand why in a second

00:35:37,520 --> 00:35:40,880
so i'm going to apply those two

00:35:39,200 --> 00:35:43,440
deployments to create the web clients

00:35:40,880 --> 00:35:46,079
and the other clients

00:35:43,440 --> 00:35:47,680
and let's look at the pods so we can see

00:35:46,079 --> 00:35:48,960
that we have all those parts running

00:35:47,680 --> 00:35:51,760
servers clients

00:35:48,960 --> 00:35:52,960
other clients so i'm going to exec into

00:35:51,760 --> 00:35:55,520
one of the web clients that i'm

00:35:52,960 --> 00:35:57,599
selecting randomly here

00:35:55,520 --> 00:35:58,560
and once i'm exact into it i'm going to

00:35:57,599 --> 00:36:01,280
try to do

00:35:58,560 --> 00:36:02,079
an http request to the web server

00:36:01,280 --> 00:36:04,079
service

00:36:02,079 --> 00:36:05,680
which can i can use i can do using your

00:36:04,079 --> 00:36:07,839
dns name in communities

00:36:05,680 --> 00:36:09,599
and you can see okay i can i can i can

00:36:07,839 --> 00:36:12,640
see the engine's

00:36:09,599 --> 00:36:16,160
welcome message so

00:36:12,640 --> 00:36:17,680
everything is working fine let's try

00:36:16,160 --> 00:36:19,280
with one of the other clients

00:36:17,680 --> 00:36:21,440
i'm going to do the same thing the same

00:36:19,280 --> 00:36:22,880
exact call request

00:36:21,440 --> 00:36:24,800
which is again going to succeed which

00:36:22,880 --> 00:36:26,640
makes sense because i haven't added any

00:36:24,800 --> 00:36:29,440
network policies to my cluster

00:36:26,640 --> 00:36:31,040
so all communications between all pods

00:36:29,440 --> 00:36:32,480
are allowed

00:36:31,040 --> 00:36:34,960
but now i'm going to imply apply this

00:36:32,480 --> 00:36:37,280
network policy to all my web server pods

00:36:34,960 --> 00:36:38,240
and it's an ingress network policy and

00:36:37,280 --> 00:36:41,040
what it says

00:36:38,240 --> 00:36:42,000
is okay the web servers can only receive

00:36:41,040 --> 00:36:44,079
traffic

00:36:42,000 --> 00:36:48,400
from the web client not from the other

00:36:44,079 --> 00:36:49,440
clients and only on tcp port 80.

00:36:48,400 --> 00:36:51,040
so what we're going to do is we're going

00:36:49,440 --> 00:36:52,720
to apply that network policy and then

00:36:51,040 --> 00:36:53,839
we're going to do exactly what we just

00:36:52,720 --> 00:36:56,000
did

00:36:53,839 --> 00:36:57,200
with the cube ctl exact command and the

00:36:56,000 --> 00:36:59,280
call request

00:36:57,200 --> 00:37:00,400
and see how things have changed so from

00:36:59,280 --> 00:37:02,079
the web client

00:37:00,400 --> 00:37:04,320
nothing has changed we can still access

00:37:02,079 --> 00:37:06,880
the web service without any issue we

00:37:04,320 --> 00:37:09,280
still see the engix welcome message

00:37:06,880 --> 00:37:11,760
but if we exec into the other client and

00:37:09,280 --> 00:37:13,680
we try to do the same call request again

00:37:11,760 --> 00:37:14,960
we can see that now it hangs i'm not

00:37:13,680 --> 00:37:17,760
getting the

00:37:14,960 --> 00:37:22,320
the web page and that's because all the

00:37:17,760 --> 00:37:25,359
traffic is being dropped by obs

00:37:22,320 --> 00:37:26,800
um what i wanted to show as well is the

00:37:25,359 --> 00:37:28,800
octant ui which is a

00:37:26,800 --> 00:37:30,320
web dashboard for communities so by

00:37:28,800 --> 00:37:32,320
default it shows

00:37:30,320 --> 00:37:34,160
information about all the pods for

00:37:32,320 --> 00:37:36,400
example all the services

00:37:34,160 --> 00:37:37,680
here i'm looking at one of the entry

00:37:36,400 --> 00:37:39,359
agent pod and i'm

00:37:37,680 --> 00:37:41,440
looking at the logs it's nice to be able

00:37:39,359 --> 00:37:44,720
to look at the logs in a web ui

00:37:41,440 --> 00:37:47,280
in a centralized place

00:37:44,720 --> 00:37:49,440
however for entria we built a nokton

00:37:47,280 --> 00:37:50,079
plugin which is used to display some

00:37:49,440 --> 00:37:52,320
specific

00:37:50,079 --> 00:37:54,400
information about entria so here what

00:37:52,320 --> 00:37:55,040
you see is some runtime information

00:37:54,400 --> 00:37:57,359
about

00:37:55,040 --> 00:37:59,599
the entry agent and the entry controller

00:37:57,359 --> 00:38:02,640
what's the bridge what's the node subnet

00:37:59,599 --> 00:38:04,800
how many pods are running on each node

00:38:02,640 --> 00:38:06,400
and what is the status of each agent and

00:38:04,800 --> 00:38:08,240
controller

00:38:06,400 --> 00:38:10,000
but more interestingly we have built a

00:38:08,240 --> 00:38:12,480
feature called trace flow

00:38:10,000 --> 00:38:13,359
which lets you send traced packets

00:38:12,480 --> 00:38:15,200
through the

00:38:13,359 --> 00:38:17,440
communities network that we've created

00:38:15,200 --> 00:38:18,320
with obs and see what happens to those

00:38:17,440 --> 00:38:20,960
packets

00:38:18,320 --> 00:38:22,079
so here i'm generating one trace one

00:38:20,960 --> 00:38:25,040
trace request

00:38:22,079 --> 00:38:25,599
from a web client to the web server

00:38:25,040 --> 00:38:28,240
service

00:38:25,599 --> 00:38:28,960
on port 80. i'm going to hit submit and

00:38:28,240 --> 00:38:30,800
we're going to see

00:38:28,960 --> 00:38:32,560
that it takes a bit of time to run but

00:38:30,800 --> 00:38:35,280
here i'm going to get a graph

00:38:32,560 --> 00:38:36,240
eventually of the path taken by the

00:38:35,280 --> 00:38:39,680
packet

00:38:36,240 --> 00:38:41,520
through the cluster and you can see that

00:38:39,680 --> 00:38:42,720
the packet goes from one source node to

00:38:41,520 --> 00:38:44,480
a destination node

00:38:42,720 --> 00:38:46,160
and you can see the different components

00:38:44,480 --> 00:38:49,520
of the pipeline

00:38:46,160 --> 00:38:49,520
traversed by the packet

00:38:50,320 --> 00:38:53,839
so that's that's all good because we

00:38:51,920 --> 00:38:56,400
know that web clients can talk to web

00:38:53,839 --> 00:38:57,760
servers here but let's start a new trace

00:38:56,400 --> 00:39:00,079
where instead of

00:38:57,760 --> 00:39:03,040
using a web client as a source we use

00:39:00,079 --> 00:39:05,280
one of the other clients as a source

00:39:03,040 --> 00:39:08,000
the rest of the trace request is exactly

00:39:05,280 --> 00:39:11,359
the same same destination same port

00:39:08,000 --> 00:39:13,680
let's run the the new request again

00:39:11,359 --> 00:39:15,040
and oh what do we see okay the graph is

00:39:13,680 --> 00:39:16,320
like different right because now we're

00:39:15,040 --> 00:39:19,359
dropping the packet

00:39:16,320 --> 00:39:20,640
on the destination node uh because of an

00:39:19,359 --> 00:39:22,480
ingress network policies

00:39:20,640 --> 00:39:24,800
ingress network policy roles that we've

00:39:22,480 --> 00:39:27,440
defined using gamma

00:39:24,800 --> 00:39:29,440
so cool feature we have in entria is the

00:39:27,440 --> 00:39:32,560
concept of network policy stats

00:39:29,440 --> 00:39:34,240
where we can dump stats about uh

00:39:32,560 --> 00:39:35,920
the number of sessions and the number of

00:39:34,240 --> 00:39:37,520
packets which have been allowed by a

00:39:35,920 --> 00:39:38,960
specific network policy

00:39:37,520 --> 00:39:42,160
and that's what i'm showing here i'm

00:39:38,960 --> 00:39:44,160
showing the stats for our policy

00:39:42,160 --> 00:39:45,760
web np and we see the number of bytes

00:39:44,160 --> 00:39:46,480
packets and sessions allowed by this

00:39:45,760 --> 00:39:48,960
policy

00:39:46,480 --> 00:39:50,320
now let's exact into the client do a new

00:39:48,960 --> 00:39:52,720
request

00:39:50,320 --> 00:39:53,760
and let's look at those stats again and

00:39:52,720 --> 00:39:55,520
here

00:39:53,760 --> 00:39:57,119
we see that the stats haven't changed so

00:39:55,520 --> 00:39:59,760
i'm going to use watch and i'm going to

00:39:57,119 --> 00:40:01,280
periodically look at those stats like

00:39:59,760 --> 00:40:03,119
we're at six sessions

00:40:01,280 --> 00:40:05,520
in a moment to change to seven and

00:40:03,119 --> 00:40:06,240
that's because to avoid overlapping the

00:40:05,520 --> 00:40:09,119
cluster

00:40:06,240 --> 00:40:10,640
we only report stats asynchronously like

00:40:09,119 --> 00:40:12,400
every minute or so

00:40:10,640 --> 00:40:14,640
to avoid generating too much control

00:40:12,400 --> 00:40:16,800
traffic

00:40:14,640 --> 00:40:19,040
all right that was all for this demo

00:40:16,800 --> 00:40:22,240
video

00:40:19,040 --> 00:40:23,760
i need to go fast i think basically i

00:40:22,240 --> 00:40:25,680
wanted to show an overview of the

00:40:23,760 --> 00:40:27,920
obvious pipeline used by andrea

00:40:25,680 --> 00:40:30,000
uh so you can understand that it's kind

00:40:27,920 --> 00:40:31,760
of like one one big step away from the

00:40:30,000 --> 00:40:33,359
toy example that we were looking at

00:40:31,760 --> 00:40:35,280
previously in the presentation

00:40:33,359 --> 00:40:37,040
so some of the forwarding stages the

00:40:35,280 --> 00:40:39,119
blue boxes the blue rectangles that you

00:40:37,040 --> 00:40:40,880
see here

00:40:39,119 --> 00:40:42,960
should be familiar from our running

00:40:40,880 --> 00:40:44,560
example previously you have the spoof

00:40:42,960 --> 00:40:47,599
guard table

00:40:44,560 --> 00:40:50,720
to prevent arp and ip spoofing you have

00:40:47,599 --> 00:40:53,359
the l3 forwarding table which is how

00:40:50,720 --> 00:40:56,240
we send traffic on the correct tunnels

00:40:53,359 --> 00:40:58,319
as part of overlay routing

00:40:56,240 --> 00:41:00,240
and the reason why i'm showing this is

00:40:58,319 --> 00:41:02,079
to emphasize that the obvious pipeline

00:41:00,240 --> 00:41:03,920
can become quite complex

00:41:02,079 --> 00:41:05,440
which is why with entria we've been

00:41:03,920 --> 00:41:06,720
looking into different tools and

00:41:05,440 --> 00:41:09,599
integrations

00:41:06,720 --> 00:41:11,200
to provide visibility into the network

00:41:09,599 --> 00:41:13,119
we just talked about the middle one here

00:41:11,200 --> 00:41:15,280
the octent one

00:41:13,119 --> 00:41:16,319
but i want to highlight uh two other

00:41:15,280 --> 00:41:19,440
ones which is

00:41:16,319 --> 00:41:21,280
promisius the fact that in entria we

00:41:19,440 --> 00:41:22,160
have the different components agents and

00:41:21,280 --> 00:41:24,240
controller

00:41:22,160 --> 00:41:26,720
export prometus metrics which can then

00:41:24,240 --> 00:41:30,319
be collected and visualized

00:41:26,720 --> 00:41:32,400
and our elastic stack integration

00:41:30,319 --> 00:41:33,359
so every entry agent is going to export

00:41:32,400 --> 00:41:35,680
information

00:41:33,359 --> 00:41:37,680
about the flows in the cluster network

00:41:35,680 --> 00:41:40,319
using a standard export protocol called

00:41:37,680 --> 00:41:41,200
ipfix and this information can then be

00:41:40,319 --> 00:41:43,520
collected

00:41:41,200 --> 00:41:44,800
analyzed and visualized using the lk

00:41:43,520 --> 00:41:46,880
stack

00:41:44,800 --> 00:41:48,319
and i'm going to show one more quick

00:41:46,880 --> 00:41:51,680
video

00:41:48,319 --> 00:41:53,359
of how this works in practice

00:41:51,680 --> 00:41:55,359
so if we go back to our clusters is the

00:41:53,359 --> 00:41:58,240
same cluster as before

00:41:55,359 --> 00:42:00,160
we're going to apply a new yaml manifest

00:41:58,240 --> 00:42:02,000
and create some workload pods

00:42:00,160 --> 00:42:03,359
web clients and web servers but this

00:42:02,000 --> 00:42:05,119
time we're going to generate

00:42:03,359 --> 00:42:07,040
a lot of connections between them with a

00:42:05,119 --> 00:42:08,640
lot of traffic being

00:42:07,040 --> 00:42:10,880
sent and received so we have a lot of

00:42:08,640 --> 00:42:12,800
ongoing traffic in that cluster

00:42:10,880 --> 00:42:14,560
and if i look at the monitoring

00:42:12,800 --> 00:42:16,160
namespace for my cluster i can see that

00:42:14,560 --> 00:42:18,240
i have a bunch of stuff

00:42:16,160 --> 00:42:20,560
related to prometheus because i've

00:42:18,240 --> 00:42:22,319
already deployed this in my cluster

00:42:20,560 --> 00:42:24,160
and that's not shown in the video but

00:42:22,319 --> 00:42:25,760
what we can notice is that

00:42:24,160 --> 00:42:28,160
what's highlighted here is that we have

00:42:25,760 --> 00:42:29,920
a promiscuous service running

00:42:28,160 --> 00:42:31,839
as an output server which means we can

00:42:29,920 --> 00:42:35,359
access it outside of the cluster

00:42:31,839 --> 00:42:37,440
on port 30 000. so here i'm jumping into

00:42:35,359 --> 00:42:38,800
graphina which is a ui to visualize

00:42:37,440 --> 00:42:41,280
promiscuous metrics

00:42:38,800 --> 00:42:42,640
and i'm going to add as a data source my

00:42:41,280 --> 00:42:45,839
promisius service

00:42:42,640 --> 00:42:47,680
from my cluster on port 3000

00:42:45,839 --> 00:42:49,280
this is all very straightforward i'm

00:42:47,680 --> 00:42:51,280
going to save the data source

00:42:49,280 --> 00:42:52,800
and now i can go ahead and i'm going to

00:42:51,280 --> 00:42:54,720
create a dashboard

00:42:52,800 --> 00:42:57,200
to visualize a specific metric in my

00:42:54,720 --> 00:42:57,200
cluster

00:42:57,520 --> 00:43:01,200
changing a few parameters for the demo

00:42:59,359 --> 00:43:04,319
here and the metric i'm going to choose

00:43:01,200 --> 00:43:05,440
is atria agent contract entry a

00:43:04,319 --> 00:43:06,800
connection count

00:43:05,440 --> 00:43:09,359
which shows the total number of

00:43:06,800 --> 00:43:11,920
connections committed to contract

00:43:09,359 --> 00:43:13,359
by the entry agent on a specific node

00:43:11,920 --> 00:43:14,480
that's something that's interesting to

00:43:13,359 --> 00:43:16,720
see because

00:43:14,480 --> 00:43:18,480
uh it it's a very common issue in

00:43:16,720 --> 00:43:20,720
communities clusters that you can

00:43:18,480 --> 00:43:21,760
reach the maximum occupancy for a

00:43:20,720 --> 00:43:23,920
contract

00:43:21,760 --> 00:43:25,599
and then new connections are going to be

00:43:23,920 --> 00:43:28,319
dropped which is going to affect your

00:43:25,599 --> 00:43:31,200
connectivity

00:43:28,319 --> 00:43:32,319
and let me move forward here we also

00:43:31,200 --> 00:43:34,880
have that feature for

00:43:32,319 --> 00:43:36,160
flow export information where we send

00:43:34,880 --> 00:43:40,000
connection information

00:43:36,160 --> 00:43:42,800
to the elk stack and a specificity

00:43:40,000 --> 00:43:45,520
of entry is we add kubernetes related

00:43:42,800 --> 00:43:48,319
information to those ipfix records about

00:43:45,520 --> 00:43:50,079
source pod name destination pod name

00:43:48,319 --> 00:43:52,640
source pod namespace

00:43:50,079 --> 00:43:54,160
destination service and thanks to that

00:43:52,640 --> 00:43:57,119
extra information

00:43:54,160 --> 00:43:57,599
we can visualize some flow maps not in

00:43:57,119 --> 00:44:00,800
terms

00:43:57,599 --> 00:44:03,359
of destination ip source ip

00:44:00,800 --> 00:44:05,359
but in terms of kubernetes objects which

00:44:03,359 --> 00:44:07,520
pods are talking to each other pods

00:44:05,359 --> 00:44:09,839
what the bandwidths for each service and

00:44:07,520 --> 00:44:09,839
so on

00:44:10,720 --> 00:44:17,280
uh sorry so yeah here we go

00:44:13,920 --> 00:44:20,400
uh i i'm gonna skip the conclusion but

00:44:17,280 --> 00:44:22,319
the idea here is that we're able to

00:44:20,400 --> 00:44:24,160
build the communities network blocking

00:44:22,319 --> 00:44:26,720
by leveraging ovs

00:44:24,160 --> 00:44:27,680
uh in actually less than a year thanks

00:44:26,720 --> 00:44:29,359
to

00:44:27,680 --> 00:44:32,319
the programmability of obs its

00:44:29,359 --> 00:44:33,680
portability uh thanks to the communities

00:44:32,319 --> 00:44:35,359
libraries that we've been able to

00:44:33,680 --> 00:44:36,000
leverage to build an efficient control

00:44:35,359 --> 00:44:38,319
plane

00:44:36,000 --> 00:44:39,520
and we have a very ambitious roadmap

00:44:38,319 --> 00:44:42,640
we're happy to

00:44:39,520 --> 00:44:44,480
uh get new contributors and guide them

00:44:42,640 --> 00:44:46,160
and i have some community information

00:44:44,480 --> 00:44:47,599
here feel free to reach out if you want

00:44:46,160 --> 00:44:51,839
to contribute or if you

00:44:47,599 --> 00:44:51,839
try entry and you run into any issue

00:44:52,720 --> 00:44:58,400
thank you antonin and thank you jen jen

00:44:56,000 --> 00:44:59,599
we will see you guys later thank you for

00:44:58,400 --> 00:45:02,720
attending

00:44:59,599 --> 00:45:04,240
this all things open 2020 really

00:45:02,720 --> 00:45:07,359
appreciate your time

00:45:04,240 --> 00:45:15,440
and your attendance and i guess we'll

00:45:07,359 --> 00:45:15,440

YouTube URL: https://www.youtube.com/watch?v=l4Qdltsl01U


