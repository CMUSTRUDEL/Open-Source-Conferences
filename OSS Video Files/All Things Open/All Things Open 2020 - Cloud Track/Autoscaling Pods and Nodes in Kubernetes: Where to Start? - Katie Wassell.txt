Title: Autoscaling Pods and Nodes in Kubernetes: Where to Start? - Katie Wassell
Publication date: 2020-12-11
Playlist: All Things Open 2020 - Cloud Track
Description: 
	Presented by: Katie Wassell, IBM
Presented at All Things Open 2020 - Cloud Track

Abstract: Do you want to stop manually scaling Kubernetes pods and manually requesting new hardware every time your service usage spikes?  

Do you wonder what types of autoscaler you should be using, or how to tune the parameters for your particular service workload?

Join Katie Wassell, of the IBM Watson Performance Engineering Team, for an overview of the different autoscaling options available in Kubernetes 1.16, including Cluster Autoscaling, Horizontal Pod Autoscaling on CPU and on Custom Metrics, and Vertical Pod Autoscaling.  Katie will go over the most common use cases sheâ€™s seen for each of the autoscalers, as well as strategies that her team uses to visualize and tune autoscaling artifacts based on production workloads.
Captions: 
	00:00:04,960 --> 00:00:06,720
so

00:00:05,279 --> 00:00:08,800
auto scaling pods and nodes and

00:00:06,720 --> 00:00:11,519
kubernetes where to start

00:00:08,800 --> 00:00:11,519
screen is up

00:00:12,000 --> 00:00:15,759
all right so who am i and why am i

00:00:14,400 --> 00:00:17,840
talking to you today

00:00:15,759 --> 00:00:19,520
i come from the ibm watson performance

00:00:17,840 --> 00:00:22,160
engineering team

00:00:19,520 --> 00:00:24,000
and our job is basically we look after

00:00:22,160 --> 00:00:26,160
the ibm watson clusters so

00:00:24,000 --> 00:00:27,599
22 different kubernetes clusters across

00:00:26,160 --> 00:00:30,080
six regions

00:00:27,599 --> 00:00:31,199
they're anywhere from 100 pods to 8500

00:00:30,080 --> 00:00:33,200
pods

00:00:31,199 --> 00:00:34,239
we're looking at stability performance

00:00:33,200 --> 00:00:36,960
and cost savings

00:00:34,239 --> 00:00:38,160
so are they staying up are they staying

00:00:36,960 --> 00:00:38,719
up when they're being hit with heavy

00:00:38,160 --> 00:00:40,640
loads

00:00:38,719 --> 00:00:42,079
and then when they're not being hit with

00:00:40,640 --> 00:00:42,719
heavy loads can we kind of roll them

00:00:42,079 --> 00:00:45,120
back

00:00:42,719 --> 00:00:48,719
and save on some of the money that comes

00:00:45,120 --> 00:00:51,039
from having all that hardware

00:00:48,719 --> 00:00:53,039
our team's been creating auto scaling

00:00:51,039 --> 00:00:54,160
resources and guidelines for the

00:00:53,039 --> 00:00:55,840
different watson service and

00:00:54,160 --> 00:00:57,360
infrastructure teams since kubernetes

00:00:55,840 --> 00:00:59,120
00:00:57,360 --> 00:01:00,719
and we have run into our fair share of

00:00:59,120 --> 00:01:02,160
walls so

00:01:00,719 --> 00:01:03,680
listening to people in the chat today it

00:01:02,160 --> 00:01:06,159
sounds like we have some familiarity

00:01:03,680 --> 00:01:08,400
with auto scaling and some not at all

00:01:06,159 --> 00:01:09,920
and i'm here to show you what we have

00:01:08,400 --> 00:01:12,880
learned so that you don't have to learn

00:01:09,920 --> 00:01:16,159
it the hard way

00:01:12,880 --> 00:01:17,280
our agenda so there will be four main

00:01:16,159 --> 00:01:20,080
categories

00:01:17,280 --> 00:01:22,000
one is a fundamentals refresher course

00:01:20,080 --> 00:01:23,439
followed by the types of kubernetes auto

00:01:22,000 --> 00:01:25,119
scaling

00:01:23,439 --> 00:01:27,600
and then we'll go into a deep dive in a

00:01:25,119 --> 00:01:29,520
case study so my hope is for those of

00:01:27,600 --> 00:01:31,759
you who walked in off the street said i

00:01:29,520 --> 00:01:34,400
know nothing about auto scaling but

00:01:31,759 --> 00:01:36,079
devops sounds fun these first two

00:01:34,400 --> 00:01:38,400
sections you should be able to follow

00:01:36,079 --> 00:01:40,240
you should pick something up

00:01:38,400 --> 00:01:41,920
for those of you who come from i've

00:01:40,240 --> 00:01:44,880
played with auto scalers before

00:01:41,920 --> 00:01:47,200
we can get into this deep dive for hpa

00:01:44,880 --> 00:01:49,040
sounds cool how do we set it up

00:01:47,200 --> 00:01:50,640
and then for those of you who have

00:01:49,040 --> 00:01:52,799
already installed it because it sounds

00:01:50,640 --> 00:01:54,560
cool and now it's doing weird things

00:01:52,799 --> 00:01:57,040
that's where the case study for tuning

00:01:54,560 --> 00:01:57,040
will come in

00:01:58,240 --> 00:02:01,439
so when we hit these slide breaks i'm

00:02:00,719 --> 00:02:03,439
going to take

00:02:01,439 --> 00:02:05,520
questions just so we don't get a backlog

00:02:03,439 --> 00:02:08,000
going and then there should be some time

00:02:05,520 --> 00:02:09,679
at the end as well

00:02:08,000 --> 00:02:12,480
all right so let's jump into it

00:02:09,679 --> 00:02:14,800
fundamentals of refresher course

00:02:12,480 --> 00:02:16,319
refresher why do you care about auto

00:02:14,800 --> 00:02:18,480
scaling

00:02:16,319 --> 00:02:20,319
so one of the main perks is it handles

00:02:18,480 --> 00:02:20,879
load spikes without a human having to be

00:02:20,319 --> 00:02:23,280
present

00:02:20,879 --> 00:02:24,319
so you've left your service you're on

00:02:23,280 --> 00:02:26,720
your lunch break

00:02:24,319 --> 00:02:28,239
or you're sleeping in the dead of night

00:02:26,720 --> 00:02:29,680
or it's your weekend and you'd rather

00:02:28,239 --> 00:02:31,599
not be bothered

00:02:29,680 --> 00:02:33,599
and somebody says you know what we

00:02:31,599 --> 00:02:36,000
should do we should do a hackathon using

00:02:33,599 --> 00:02:37,760
your service and we'll hit it with

00:02:36,000 --> 00:02:40,000
you know a couple thousand people more

00:02:37,760 --> 00:02:42,959
than you've ever tested it with before

00:02:40,000 --> 00:02:43,920
probably it'll be fine so auto scaling

00:02:42,959 --> 00:02:46,000
kicks in

00:02:43,920 --> 00:02:47,200
it says okay i need more pods i need

00:02:46,000 --> 00:02:50,080
more nodes

00:02:47,200 --> 00:02:51,360
i will handle this spike in traffic and

00:02:50,080 --> 00:02:54,720
you're not going to get pinged with

00:02:51,360 --> 00:02:57,920
somebody saying hey go fix the thing

00:02:54,720 --> 00:02:59,760
the other main appeal of auto scaling is

00:02:57,920 --> 00:03:02,400
you know on the flip side of having

00:02:59,760 --> 00:03:04,560
really high loads you could also have

00:03:02,400 --> 00:03:05,760
really low load times and if that's the

00:03:04,560 --> 00:03:08,800
case if

00:03:05,760 --> 00:03:10,080
say you work with businesses and it's 4

00:03:08,800 --> 00:03:11,599
am on a saturday

00:03:10,080 --> 00:03:13,920
you can probably scale down your

00:03:11,599 --> 00:03:16,239
hardware and save some money

00:03:13,920 --> 00:03:18,000
the other advantages are it's automatic

00:03:16,239 --> 00:03:18,640
scaling so you're not relying on a

00:03:18,000 --> 00:03:20,400
person

00:03:18,640 --> 00:03:22,640
so there's nobody manually typing in

00:03:20,400 --> 00:03:24,319
commands during a crisis

00:03:22,640 --> 00:03:26,560
trying to scale out your deployments

00:03:24,319 --> 00:03:29,040
accurately request your hardware

00:03:26,560 --> 00:03:31,040
accurately so you're not having that

00:03:29,040 --> 00:03:33,360
you know the mishaps that can happen

00:03:31,040 --> 00:03:34,640
when humans realize oops we've got to

00:03:33,360 --> 00:03:37,680
fix the thing we've got to do

00:03:34,640 --> 00:03:37,680
now under pressure

00:03:38,319 --> 00:03:43,519
so second main concept that you should

00:03:40,560 --> 00:03:45,200
know here performance bottlenecks

00:03:43,519 --> 00:03:46,640
several people have a vague idea let's

00:03:45,200 --> 00:03:48,640
just get this out in the open so

00:03:46,640 --> 00:03:50,560
everybody's on the same page

00:03:48,640 --> 00:03:53,200
every service has something that will

00:03:50,560 --> 00:03:55,599
prevent it from scaling to handle load

00:03:53,200 --> 00:03:57,120
so if you keep increasing how much stuff

00:03:55,599 --> 00:03:59,200
is hitting your service

00:03:57,120 --> 00:04:02,080
at some point you're gonna go from a

00:03:59,200 --> 00:04:04,480
nice flat gradually increasing line to

00:04:02,080 --> 00:04:07,280
the hockey stick of doom where your

00:04:04,480 --> 00:04:10,480
response times they've jumped

00:04:07,280 --> 00:04:13,040
and your error rates have increased

00:04:10,480 --> 00:04:14,000
so terminology if we say a service is

00:04:13,040 --> 00:04:15,599
cpu bound

00:04:14,000 --> 00:04:18,720
performance is going to be limited by

00:04:15,599 --> 00:04:20,720
the cpu resources it has available

00:04:18,720 --> 00:04:22,240
once you run out of cpu that's it you

00:04:20,720 --> 00:04:24,160
can have a bunch of memory you can have

00:04:22,240 --> 00:04:25,680
a bunch of bandwidth left over but

00:04:24,160 --> 00:04:27,840
if you run out of cpu and that's your

00:04:25,680 --> 00:04:30,400
bottleneck your service is not going to

00:04:27,840 --> 00:04:30,400
be happy

00:04:30,639 --> 00:04:34,000
then refresher on kubernetes for those

00:04:32,720 --> 00:04:34,960
of you who said you picked this at

00:04:34,000 --> 00:04:37,440
random

00:04:34,960 --> 00:04:38,479
kubernetes it's an open source system it

00:04:37,440 --> 00:04:40,840
automates deployment

00:04:38,479 --> 00:04:43,120
scaling and management of containerized

00:04:40,840 --> 00:04:45,600
applications obviously today we'll be

00:04:43,120 --> 00:04:49,040
talking about scaling

00:04:45,600 --> 00:04:50,400
so if we look at our terminology a node

00:04:49,040 --> 00:04:52,400
is a worker machine or a piece of

00:04:50,400 --> 00:04:56,000
hardware and it contains

00:04:52,400 --> 00:04:57,199
everything necessary to run pods a pod

00:04:56,000 --> 00:04:59,840
is going to be a collection of

00:04:57,199 --> 00:05:01,520
containers

00:04:59,840 --> 00:05:03,440
a container is going to be a single

00:05:01,520 --> 00:05:06,720
microservice so a pod is basically

00:05:03,440 --> 00:05:09,360
microservices that live well together

00:05:06,720 --> 00:05:10,320
a deployment is going to be the thing

00:05:09,360 --> 00:05:12,479
that rolls out

00:05:10,320 --> 00:05:14,320
duplicates of your pods it will be

00:05:12,479 --> 00:05:15,919
backed by a replica set so if i talk

00:05:14,320 --> 00:05:16,880
about replicas that's what i'm talking

00:05:15,919 --> 00:05:18,400
about

00:05:16,880 --> 00:05:20,639
and then a service is kind of a human

00:05:18,400 --> 00:05:24,720
readable port that gives you a framework

00:05:20,639 --> 00:05:27,199
to access these pods

00:05:24,720 --> 00:05:29,199
and then we get into the scary chart so

00:05:27,199 --> 00:05:32,320
this is a refresher on requests

00:05:29,199 --> 00:05:34,800
limits and quality of service classes

00:05:32,320 --> 00:05:36,160
so kubernetes has two main resources

00:05:34,800 --> 00:05:39,440
that will actually be

00:05:36,160 --> 00:05:41,440
specified in your spec so you'll say i

00:05:39,440 --> 00:05:43,280
need this amount of cpu and i need this

00:05:41,440 --> 00:05:44,880
amount of memory

00:05:43,280 --> 00:05:46,479
the main difference between cpu and

00:05:44,880 --> 00:05:47,360
memory is whether they're compressible

00:05:46,479 --> 00:05:49,440
or not

00:05:47,360 --> 00:05:51,039
cpu is compressible so you can grab a

00:05:49,440 --> 00:05:52,400
bunch of it and if you find that you

00:05:51,039 --> 00:05:54,400
have to give some back

00:05:52,400 --> 00:05:56,000
that's totally fine you can just give it

00:05:54,400 --> 00:05:58,479
back and move on

00:05:56,000 --> 00:06:00,160
memory is non-compressible so if you

00:05:58,479 --> 00:06:01,600
find that you need more memory

00:06:00,160 --> 00:06:03,680
and then somebody else needs it more

00:06:01,600 --> 00:06:04,400
than you there's no easy way to get it

00:06:03,680 --> 00:06:07,039
back from you

00:06:04,400 --> 00:06:08,960
short of killing your pod so that's a

00:06:07,039 --> 00:06:11,280
distinction that most people don't know

00:06:08,960 --> 00:06:13,199
but it can get you into trouble

00:06:11,280 --> 00:06:15,120
so for each of these resources you give

00:06:13,199 --> 00:06:18,160
two values you say

00:06:15,120 --> 00:06:20,080
i want to request this much and you can

00:06:18,160 --> 00:06:22,880
also say i want to give a limit

00:06:20,080 --> 00:06:24,960
of this much requests are something you

00:06:22,880 --> 00:06:26,720
will always need you talk to kubernetes

00:06:24,960 --> 00:06:28,720
and say i need this to schedule onto a

00:06:26,720 --> 00:06:31,120
node if i can't get this just leave me

00:06:28,720 --> 00:06:34,639
in pending in a queue somewhere because

00:06:31,120 --> 00:06:37,680
there's no point in me even being here

00:06:34,639 --> 00:06:40,479
limits you have options so you can

00:06:37,680 --> 00:06:41,039
kind of take your risk reward ratio you

00:06:40,479 --> 00:06:42,960
can say

00:06:41,039 --> 00:06:44,960
i want to be a guaranteed pod on the

00:06:42,960 --> 00:06:46,160
highest possible priority so set my

00:06:44,960 --> 00:06:49,520
requests and my limits

00:06:46,160 --> 00:06:51,599
equal so i will have what i said i need

00:06:49,520 --> 00:06:53,919
to work i will never have any more

00:06:51,599 --> 00:06:56,160
but you will not kill me until the very

00:06:53,919 --> 00:06:57,680
very end

00:06:56,160 --> 00:07:00,080
so if you're feeling a little more

00:06:57,680 --> 00:07:01,919
daring you can say

00:07:00,080 --> 00:07:03,680
actually i think i want to be burstable

00:07:01,919 --> 00:07:05,440
i'm going to tell you i want this level

00:07:03,680 --> 00:07:06,000
of requests and also this level of

00:07:05,440 --> 00:07:07,360
limits

00:07:06,000 --> 00:07:10,240
my limits will be higher than my

00:07:07,360 --> 00:07:11,199
requests and if i can cheat up and get

00:07:10,240 --> 00:07:13,120
that resource

00:07:11,199 --> 00:07:16,160
then that's great for me i can run

00:07:13,120 --> 00:07:17,680
faster or i can do better calculations

00:07:16,160 --> 00:07:19,759
there's something that makes that risk

00:07:17,680 --> 00:07:21,680
worth it

00:07:19,759 --> 00:07:24,240
but if i try and take more than the

00:07:21,680 --> 00:07:26,319
limits you're going to stop me

00:07:24,240 --> 00:07:27,919
with cpu if i try and take more than my

00:07:26,319 --> 00:07:29,039
limits then you're going to throttle me

00:07:27,919 --> 00:07:31,039
and you'll actually use

00:07:29,039 --> 00:07:33,599
the linux scheduler to make sure that i

00:07:31,039 --> 00:07:35,599
can't get more cpu

00:07:33,599 --> 00:07:37,360
if i try and use more than my limits in

00:07:35,599 --> 00:07:39,280
memory

00:07:37,360 --> 00:07:40,400
there's no great way to get it back i've

00:07:39,280 --> 00:07:43,120
gone over

00:07:40,400 --> 00:07:44,879
my limit i'm non-compressible so kill my

00:07:43,120 --> 00:07:46,800
pod take it back and we'll start over i

00:07:44,879 --> 00:07:48,479
guess

00:07:46,800 --> 00:07:50,960
and then there's also best effort pods

00:07:48,479 --> 00:07:52,879
which say sometimes you have a bunch of

00:07:50,960 --> 00:07:54,400
resources left over on your cluster

00:07:52,879 --> 00:07:56,240
and if you're feeling particularly

00:07:54,400 --> 00:07:58,560
daring you can just

00:07:56,240 --> 00:08:00,240
pick up all those idle resources try and

00:07:58,560 --> 00:08:02,160
run really fast and just

00:08:00,240 --> 00:08:03,680
finish whatever you need to do before

00:08:02,160 --> 00:08:05,919
you get killed because somebody else

00:08:03,680 --> 00:08:07,919
needs them

00:08:05,919 --> 00:08:09,039
all right so that should have been some

00:08:07,919 --> 00:08:10,960
level setting

00:08:09,039 --> 00:08:13,840
do we have any questions from that first

00:08:10,960 --> 00:08:13,840
part

00:08:18,560 --> 00:08:24,160
um patrick brooks asks in chat what is

00:08:24,840 --> 00:08:30,400
cfa

00:08:26,240 --> 00:08:32,560
cfa i might need

00:08:30,400 --> 00:08:35,200
context on that i don't remember where

00:08:32,560 --> 00:08:35,200
that came from

00:08:36,399 --> 00:08:40,560
uh well patrick gets that um krishna

00:08:39,680 --> 00:08:42,959
lottie

00:08:40,560 --> 00:08:45,680
what ai capabilities are available for

00:08:42,959 --> 00:08:45,680
autoscaling

00:08:46,880 --> 00:08:51,680
what ai capabilities are available for

00:08:49,279 --> 00:08:53,760
auto scaling

00:08:51,680 --> 00:08:55,519
so several of the auto scalers are going

00:08:53,760 --> 00:08:56,800
to have algorithms that take into

00:08:55,519 --> 00:09:00,000
account the memory

00:08:56,800 --> 00:09:01,760
or the cpu or some kind of metric

00:09:00,000 --> 00:09:04,320
and based on that they're going to run a

00:09:01,760 --> 00:09:07,519
calculation of how many pods you need

00:09:04,320 --> 00:09:10,160
or how big your pods need to be

00:09:07,519 --> 00:09:12,080
so i wouldn't exactly call it ai but it

00:09:10,160 --> 00:09:15,200
is very data driven and we'll get into

00:09:12,080 --> 00:09:17,839
that in the next section

00:09:15,200 --> 00:09:20,560
and patrick brooke replies it's on your

00:09:17,839 --> 00:09:20,560
previous slide

00:09:21,040 --> 00:09:28,640
cfs uh the completely fair scheduler

00:09:24,880 --> 00:09:31,519
so that's a linux process that basically

00:09:28,640 --> 00:09:33,200
checks who's running in cpu and if it

00:09:31,519 --> 00:09:35,839
sees that somebody is cheating up

00:09:33,200 --> 00:09:37,440
on how many cpu cycles they're taking it

00:09:35,839 --> 00:09:39,760
will prevent them until the next time

00:09:37,440 --> 00:09:39,760
slot

00:09:41,680 --> 00:09:47,200
cool cool uh that seems to be all

00:09:44,800 --> 00:09:49,120
questions in chat for now

00:09:47,200 --> 00:09:51,680
all right so we'll get in the types of

00:09:49,120 --> 00:09:53,120
autoscalers

00:09:51,680 --> 00:09:55,440
there are three major flavors of

00:09:53,120 --> 00:09:57,440
autoscalers as you can see they're

00:09:55,440 --> 00:09:58,800
pineapples strawberry and blue raspberry

00:09:57,440 --> 00:10:01,200
no

00:09:58,800 --> 00:10:04,240
so we have the horizontal pot autoscaler

00:10:01,200 --> 00:10:06,320
which commonly referred to as hpa

00:10:04,240 --> 00:10:08,079
and there are two different flavors of

00:10:06,320 --> 00:10:10,399
this i guess two types

00:10:08,079 --> 00:10:11,839
one is cpu based which has been here

00:10:10,399 --> 00:10:15,040
basically since the beginning

00:10:11,839 --> 00:10:17,279
and then one is the new hot freshness

00:10:15,040 --> 00:10:19,600
custom metrics so scaling on something

00:10:17,279 --> 00:10:21,600
other than cpu

00:10:19,600 --> 00:10:23,839
the next major flavor will be cluster

00:10:21,600 --> 00:10:26,720
auto scaling so this is scaling out your

00:10:23,839 --> 00:10:29,040
actual kubernetes cluster your hardware

00:10:26,720 --> 00:10:31,040
and then the third flavor is vertical

00:10:29,040 --> 00:10:32,560
pod auto scaling

00:10:31,040 --> 00:10:34,240
sometimes you'll see something called an

00:10:32,560 --> 00:10:36,959
add-on resizer

00:10:34,240 --> 00:10:37,440
this is like a vertical pod auto scaler

00:10:36,959 --> 00:10:39,360
light

00:10:37,440 --> 00:10:42,079
but for deployments and that's actually

00:10:39,360 --> 00:10:44,959
used inside the kubernetes metrics

00:10:42,079 --> 00:10:46,000
services themselves a good way to

00:10:44,959 --> 00:10:48,240
remember this is

00:10:46,000 --> 00:10:49,519
horizontal pod autoscaler you have a

00:10:48,240 --> 00:10:52,640
bunch of little pods

00:10:49,519 --> 00:10:54,480
that lead off into the horizon uh

00:10:52,640 --> 00:10:56,880
cluster autoscaler it scales your

00:10:54,480 --> 00:10:58,720
cluster and then vertical pod auto

00:10:56,880 --> 00:11:00,560
scaling it's like if you took one pod

00:10:58,720 --> 00:11:03,360
and you stretched it vertically and you

00:11:00,560 --> 00:11:03,360
made it really big

00:11:03,839 --> 00:11:07,760
so let's go into what use cases look

00:11:06,000 --> 00:11:11,200
like for these

00:11:07,760 --> 00:11:13,279
so horizontal pod auto scaling on cpu

00:11:11,200 --> 00:11:15,040
you've got a service it's cpu bound so

00:11:13,279 --> 00:11:18,000
there's your bottleneck

00:11:15,040 --> 00:11:19,600
it's comfortably sized to handle x

00:11:18,000 --> 00:11:22,399
amount of requests so you're going to

00:11:19,600 --> 00:11:24,399
have x amount of cpu usage

00:11:22,399 --> 00:11:26,800
suddenly somebody says we're going to

00:11:24,399 --> 00:11:28,880
have a hackathon with your thing

00:11:26,800 --> 00:11:31,519
and now you're getting 5x requests so

00:11:28,880 --> 00:11:34,240
you need 5x the cpu

00:11:31,519 --> 00:11:35,680
so you're in your before state oh no

00:11:34,240 --> 00:11:37,440
catastrophe

00:11:35,680 --> 00:11:39,279
you can assume that the spike is going

00:11:37,440 --> 00:11:41,360
to pass so it's not like you should just

00:11:39,279 --> 00:11:42,480
permanently allocate five times as much

00:11:41,360 --> 00:11:44,320
stuff

00:11:42,480 --> 00:11:47,760
and you can assume that at some point

00:11:44,320 --> 00:11:51,440
you'll resume your typical usage levels

00:11:47,760 --> 00:11:54,800
so the auto scaler looks at this

00:11:51,440 --> 00:11:57,440
and says okay we need five of these it

00:11:54,800 --> 00:12:00,959
sends a node out to kubernetes schedules

00:11:57,440 --> 00:12:00,959
five pods now you're good to go

00:12:01,279 --> 00:12:04,959
so the other option here is horizontal

00:12:03,680 --> 00:12:07,519
pod auto scaling on

00:12:04,959 --> 00:12:08,480
something other than cpu you've got a

00:12:07,519 --> 00:12:10,079
service

00:12:08,480 --> 00:12:12,240
the bottleneck here is different it's

00:12:10,079 --> 00:12:16,160
not cpu it's some kind of other metric

00:12:12,240 --> 00:12:19,440
so memory the number of items in a queue

00:12:16,160 --> 00:12:21,680
number of requests number of people

00:12:19,440 --> 00:12:24,000
talking to you

00:12:21,680 --> 00:12:26,000
so you've got this metric this pog can

00:12:24,000 --> 00:12:28,240
handle two documents in your queue

00:12:26,000 --> 00:12:30,720
and let's say you're running a

00:12:28,240 --> 00:12:32,480
translation service and somebody goes

00:12:30,720 --> 00:12:34,959
oh no i really need to translate this

00:12:32,480 --> 00:12:37,279
gigantic hundred page document

00:12:34,959 --> 00:12:39,279
let me just throw it at this service so

00:12:37,279 --> 00:12:40,800
now all of a sudden you have three times

00:12:39,279 --> 00:12:42,399
as many things in your queue you've got

00:12:40,800 --> 00:12:44,079
this massive backlog

00:12:42,399 --> 00:12:47,600
and you're realizing if you have one pod

00:12:44,079 --> 00:12:49,839
handling it it's gonna take you forever

00:12:47,600 --> 00:12:51,440
so autoscaler looks at this says there's

00:12:49,839 --> 00:12:52,000
a bunch of stuff in your queue i think

00:12:51,440 --> 00:12:56,079
you need more

00:12:52,000 --> 00:12:56,079
pods scales up more pods

00:12:57,440 --> 00:13:01,680
so we move on to cluster auto scaling

00:12:59,440 --> 00:13:03,680
this one talking about hardware

00:13:01,680 --> 00:13:06,160
so one of the previous auto scalers has

00:13:03,680 --> 00:13:07,200
scaled up a whole bunch of pods and then

00:13:06,160 --> 00:13:09,760
kubernetes says

00:13:07,200 --> 00:13:12,959
hey wait a sec looking at these requests

00:13:09,760 --> 00:13:14,880
these don't actually fit on my hardware

00:13:12,959 --> 00:13:16,160
so there's more cpus or memories

00:13:14,880 --> 00:13:18,160
requested than i have

00:13:16,160 --> 00:13:21,040
so some pod here is just going to be

00:13:18,160 --> 00:13:22,720
stuck in a pending state

00:13:21,040 --> 00:13:24,720
we're still assuming that it's either

00:13:22,720 --> 00:13:26,560
the hackathon or the person translating

00:13:24,720 --> 00:13:28,399
their 100 page paper

00:13:26,560 --> 00:13:30,320
and then at some point this is going to

00:13:28,399 --> 00:13:32,160
end so it doesn't make sense to

00:13:30,320 --> 00:13:34,399
permanently put more hardware just

00:13:32,160 --> 00:13:36,639
because you've seen this once

00:13:34,399 --> 00:13:38,320
so cluster autoscaler looks at this it

00:13:36,639 --> 00:13:40,000
says i see pending pods

00:13:38,320 --> 00:13:41,600
i know what i need to do here is i can

00:13:40,000 --> 00:13:42,800
grab another node that looks like my

00:13:41,600 --> 00:13:45,440
first node

00:13:42,800 --> 00:13:47,120
and we'll grab that and then we'll plunk

00:13:45,440 --> 00:13:51,279
this new pod onto it

00:13:47,120 --> 00:13:53,600
everybody's good to go so here we go

00:13:51,279 --> 00:13:55,360
and then the final part that we'll be

00:13:53,600 --> 00:13:56,639
going over today vertical pod auto

00:13:55,360 --> 00:13:58,240
scaling

00:13:56,639 --> 00:14:00,560
this is for the people who have a

00:13:58,240 --> 00:14:01,279
service and then game i know i need to

00:14:00,560 --> 00:14:03,600
size this

00:14:01,279 --> 00:14:05,760
and i'm just going to pick some size and

00:14:03,600 --> 00:14:08,160
i'm going to throw it in production

00:14:05,760 --> 00:14:08,959
and probably it'll be fine so if you

00:14:08,160 --> 00:14:10,880
think about this

00:14:08,959 --> 00:14:13,040
thought pattern normally it means you

00:14:10,880 --> 00:14:15,839
really oversize your pod just so nothing

00:14:13,040 --> 00:14:17,600
terrible happens to it in production

00:14:15,839 --> 00:14:19,360
so you used your best guess and you

00:14:17,600 --> 00:14:22,000
picked something

00:14:19,360 --> 00:14:23,120
and it would be lovely if somebody would

00:14:22,000 --> 00:14:25,600
go behind you

00:14:23,120 --> 00:14:28,160
and look at the actual usage and figure

00:14:25,600 --> 00:14:30,160
out how to resize your container

00:14:28,160 --> 00:14:31,680
and it'd be even more lovely if somebody

00:14:30,160 --> 00:14:33,600
would just take care of that for the

00:14:31,680 --> 00:14:34,959
rest of the pod's known life

00:14:33,600 --> 00:14:38,000
have an auto scaler that just

00:14:34,959 --> 00:14:40,079
permanently right sizes things for you

00:14:38,000 --> 00:14:41,760
so that's more of the vertical pod auto

00:14:40,079 --> 00:14:44,079
scaling use case

00:14:41,760 --> 00:14:45,440
i mentioned the add-on resizer which is

00:14:44,079 --> 00:14:47,120
like vertical auto-scaling for

00:14:45,440 --> 00:14:48,959
deployments

00:14:47,120 --> 00:14:50,639
the way that this gets used is in

00:14:48,959 --> 00:14:52,399
kubernetes

00:14:50,639 --> 00:14:55,120
it'll look at the actual size of your

00:14:52,399 --> 00:14:57,199
cluster and the metric server and the

00:14:55,120 --> 00:14:59,360
metric server nanny will talk

00:14:57,199 --> 00:15:00,399
and they'll say based on how many things

00:14:59,360 --> 00:15:02,399
are in your cluster

00:15:00,399 --> 00:15:03,839
this is how big you need to be to handle

00:15:02,399 --> 00:15:06,560
all the metrics that are going to be

00:15:03,839 --> 00:15:06,560
coming at you

00:15:08,160 --> 00:15:12,320
and all of these are great are they all

00:15:10,320 --> 00:15:15,760
at the same level of maturity

00:15:12,320 --> 00:15:17,920
no no they are not so the most mature

00:15:15,760 --> 00:15:18,720
one of these auto scalers is cpu-based

00:15:17,920 --> 00:15:20,720
hpa

00:15:18,720 --> 00:15:23,600
it's been built into the guts of

00:15:20,720 --> 00:15:25,920
kubernetes it's been there since 1.3

00:15:23,600 --> 00:15:27,199
the algorithms are continuing to improve

00:15:25,920 --> 00:15:30,079
taking away things like

00:15:27,199 --> 00:15:30,399
long cooldown timers or points where it

00:15:30,079 --> 00:15:34,079
just

00:15:30,399 --> 00:15:36,079
oscillated right over a threshold so

00:15:34,079 --> 00:15:38,560
that's at the point where we've got

00:15:36,079 --> 00:15:39,199
about 30 percent of our production

00:15:38,560 --> 00:15:41,440
deployments

00:15:39,199 --> 00:15:42,639
going ahead and using it we trust it we

00:15:41,440 --> 00:15:45,839
know it's been around

00:15:42,639 --> 00:15:49,040
it's stable enough to actually use in

00:15:45,839 --> 00:15:51,759
critical applications so

00:15:49,040 --> 00:15:54,079
you look at custom metrics hpa it's v2

00:15:51,759 --> 00:15:56,480
beta 2 code so it's coming

00:15:54,079 --> 00:15:58,880
it's more stable than it has been but

00:15:56,480 --> 00:16:01,440
it's still not ga yet

00:15:58,880 --> 00:16:03,279
and the tricky part about this one is

00:16:01,440 --> 00:16:04,800
depending on your metrics engine you're

00:16:03,279 --> 00:16:06,079
going to need some kind of third-party

00:16:04,800 --> 00:16:08,079
adapter code

00:16:06,079 --> 00:16:10,959
this adapter code will not be maintained

00:16:08,079 --> 00:16:13,240
by kubernetes so you're relying on

00:16:10,959 --> 00:16:14,880
prometheus datadog systig

00:16:13,240 --> 00:16:16,720
googlestackdriver

00:16:14,880 --> 00:16:18,320
whoever you use for metrics has to

00:16:16,720 --> 00:16:19,199
actually go ahead and maintain that

00:16:18,320 --> 00:16:22,079
adapter code

00:16:19,199 --> 00:16:24,320
and assumably maintain it moving forward

00:16:22,079 --> 00:16:26,000
as they move from the v2 format to the

00:16:24,320 --> 00:16:28,240
ga format

00:16:26,000 --> 00:16:29,839
so depending on who you're using you

00:16:28,240 --> 00:16:31,440
want to keep an eye on that before you

00:16:29,839 --> 00:16:32,959
trust it

00:16:31,440 --> 00:16:35,759
right now we use this in our dev and

00:16:32,959 --> 00:16:38,240
staging environments and we're still

00:16:35,759 --> 00:16:42,000
monitoring because we don't fully trust

00:16:38,240 --> 00:16:43,759
that adapter code in production yet

00:16:42,000 --> 00:16:45,680
cluster auto scaling it's been around

00:16:43,759 --> 00:16:47,920
since cube 1.8

00:16:45,680 --> 00:16:49,839
most cloud providers generally have ga

00:16:47,920 --> 00:16:51,680
support everybody's kind of done their

00:16:49,839 --> 00:16:53,600
little flavor on it

00:16:51,680 --> 00:16:55,360
so it will work differently moving from

00:16:53,600 --> 00:16:58,000
cloud to cloud so

00:16:55,360 --> 00:17:00,880
read the fine print and then vertical

00:16:58,000 --> 00:17:02,720
pod auto scaling it's currently in beta

00:17:00,880 --> 00:17:05,600
several cloud providers are talking

00:17:02,720 --> 00:17:08,400
about this as a tech preview

00:17:05,600 --> 00:17:10,319
so it's interesting i don't necessarily

00:17:08,400 --> 00:17:13,919
trust it in production yet but

00:17:10,319 --> 00:17:16,839
i'll be curious to see where it goes

00:17:13,919 --> 00:17:19,839
all right uh stopping for questions

00:17:16,839 --> 00:17:19,839
again

00:17:23,679 --> 00:17:27,520
apologize my display is being buggy it

00:17:25,839 --> 00:17:30,640
turned itself off

00:17:27,520 --> 00:17:32,559
um so krishna gelati

00:17:30,640 --> 00:17:34,400
um asked are you able to share this

00:17:32,559 --> 00:17:36,160
presentation

00:17:34,400 --> 00:17:37,520
uh i believe that all of these

00:17:36,160 --> 00:17:41,039
presentations will be

00:17:37,520 --> 00:17:44,799
shared both the presentations and the

00:17:41,039 --> 00:17:44,799
slides at least that's my understanding

00:17:44,840 --> 00:17:50,960
okay um and then

00:17:47,360 --> 00:17:53,280
uh vikrant verma um asks

00:17:50,960 --> 00:17:54,400
how is kubernetes auto scaler related to

00:17:53,280 --> 00:17:55,760
its load balancer

00:17:54,400 --> 00:17:57,600
is it possible that it's still sending

00:17:55,760 --> 00:17:59,679
requests to a few pods only

00:17:57,600 --> 00:18:01,360
instead of evenly distributing after got

00:17:59,679 --> 00:18:04,080
scaled up

00:18:01,360 --> 00:18:06,559
yes that is possible they do not

00:18:04,080 --> 00:18:08,559
necessarily work together

00:18:06,559 --> 00:18:10,960
if you have something like pod affinity

00:18:08,559 --> 00:18:12,799
or anti-affinity or something with nodes

00:18:10,960 --> 00:18:14,799
where it's directing traffic

00:18:12,799 --> 00:18:17,120
yes that can still cause issues even if

00:18:14,799 --> 00:18:18,720
you have an auto scaler active

00:18:17,120 --> 00:18:20,640
um that's actually one of the things

00:18:18,720 --> 00:18:22,480
i'll say to look for because we've seen

00:18:20,640 --> 00:18:24,960
deployments where that's had problems in

00:18:22,480 --> 00:18:24,960
the past

00:18:26,320 --> 00:18:31,919
okay and hannah lehmann asks are each of

00:18:29,679 --> 00:18:34,559
these types of autoscalers just as easy

00:18:31,919 --> 00:18:34,559
to implement

00:18:36,880 --> 00:18:41,120
i'm going to go with no mainly because

00:18:39,840 --> 00:18:42,799
whenever you have the

00:18:41,120 --> 00:18:44,960
party adapter code at that point you're

00:18:42,799 --> 00:18:47,919
having to do additional hookups

00:18:44,960 --> 00:18:48,640
uh hpa on cpu which is what we're about

00:18:47,919 --> 00:18:51,520
to go into

00:18:48,640 --> 00:18:53,200
is super easy to set up it's a single

00:18:51,520 --> 00:18:54,960
kubernetes object

00:18:53,200 --> 00:18:56,880
once you start getting into cluster auto

00:18:54,960 --> 00:18:58,480
scaling you have a bunch of config maps

00:18:56,880 --> 00:19:01,280
you need to toggle some switches

00:18:58,480 --> 00:19:03,120
install some packages it's all well

00:19:01,280 --> 00:19:04,320
documented it's just not as

00:19:03,120 --> 00:19:06,080
straightforward

00:19:04,320 --> 00:19:07,600
and then with the third-party adapter

00:19:06,080 --> 00:19:09,280
code

00:19:07,600 --> 00:19:11,840
your mileage may vary depending on who

00:19:09,280 --> 00:19:11,840
you're working with

00:19:13,440 --> 00:19:20,640
all right um hannah says

00:19:17,120 --> 00:19:23,440
cool thank you all right

00:19:20,640 --> 00:19:25,280
that's it for questions for now so

00:19:23,440 --> 00:19:26,799
apologies to everybody who wandered in

00:19:25,280 --> 00:19:28,160
off the street this is when we start

00:19:26,799 --> 00:19:29,760
going into the weeds

00:19:28,160 --> 00:19:31,840
uh you're welcome to stick around and

00:19:29,760 --> 00:19:33,760
try and keep up

00:19:31,840 --> 00:19:36,400
all right so we're gonna go into a deep

00:19:33,760 --> 00:19:39,520
dive on creating a cpu-based hpa

00:19:36,400 --> 00:19:40,799
object i will not show actual code but i

00:19:39,520 --> 00:19:42,880
will give you

00:19:40,799 --> 00:19:44,320
concepts that are very useful to have

00:19:42,880 --> 00:19:47,520
when you're trying to do one of these

00:19:44,320 --> 00:19:47,520
and size it correctly

00:19:47,919 --> 00:19:52,000
right so there are two major disclaimers

00:19:50,400 --> 00:19:53,600
that i feel the need to mention

00:19:52,000 --> 00:19:55,440
before you start preparing to make one

00:19:53,600 --> 00:19:59,120
of these the first one

00:19:55,440 --> 00:20:01,760
is several people have moved from a vm

00:19:59,120 --> 00:20:02,159
environment where you have this gigantic

00:20:01,760 --> 00:20:05,520
you know

00:20:02,159 --> 00:20:08,480
4 cpu 8 cpu 12 cpu machine

00:20:05,520 --> 00:20:10,240
and you've been asked to squish it into

00:20:08,480 --> 00:20:12,559
a containerized environment

00:20:10,240 --> 00:20:14,960
and some people just grab that vm they

00:20:12,559 --> 00:20:17,919
package it up in a very tight jacket

00:20:14,960 --> 00:20:18,880
and they shove it into kubernetes so you

00:20:17,919 --> 00:20:22,799
now have

00:20:18,880 --> 00:20:25,679
a 12 cpu pod that takes

00:20:22,799 --> 00:20:27,440
5 to 10 minutes to boot that's not going

00:20:25,679 --> 00:20:29,120
to work well with auto scaling

00:20:27,440 --> 00:20:31,440
you want to try and make your pods as

00:20:29,120 --> 00:20:33,919
small and quick to boot as possible

00:20:31,440 --> 00:20:35,840
because if you're auto scaling that

00:20:33,919 --> 00:20:37,280
means you have an issue with load that

00:20:35,840 --> 00:20:39,520
needs to be addressed

00:20:37,280 --> 00:20:41,919
you would like to get a pod that can

00:20:39,520 --> 00:20:44,880
address the load in something

00:20:41,919 --> 00:20:45,520
fairly fast so if you have macro

00:20:44,880 --> 00:20:47,440
services

00:20:45,520 --> 00:20:49,039
a way that you can compensate for them

00:20:47,440 --> 00:20:51,600
is you can set the cpu

00:20:49,039 --> 00:20:53,600
threshold lower and allow more time to

00:20:51,600 --> 00:20:55,120
scale and hopefully they can make it

00:20:53,600 --> 00:20:57,120
but long term you really want to

00:20:55,120 --> 00:21:00,559
re-architect to have the micro services

00:20:57,120 --> 00:21:02,720
that kubernetes was meant to run with

00:21:00,559 --> 00:21:04,240
the other major disclaimer is another

00:21:02,720 --> 00:21:08,480
anti-pattern i've seen

00:21:04,240 --> 00:21:10,000
is people have clusters and they go

00:21:08,480 --> 00:21:11,440
you know i'm not sure how much space i

00:21:10,000 --> 00:21:11,919
need so i'm just going to get a lot of

00:21:11,440 --> 00:21:14,080
space

00:21:11,919 --> 00:21:15,520
i'm going to use a lot of versatile pods

00:21:14,080 --> 00:21:17,840
and

00:21:15,520 --> 00:21:20,559
we'll just use a bunch of limits because

00:21:17,840 --> 00:21:22,159
we have all this space

00:21:20,559 --> 00:21:23,679
once you move to a point where you have

00:21:22,159 --> 00:21:25,200
cluster auto scaling and you're doing

00:21:23,679 --> 00:21:27,120
just in time hardware

00:21:25,200 --> 00:21:28,400
all of a sudden all of that extra space

00:21:27,120 --> 00:21:31,120
you were using for limits

00:21:28,400 --> 00:21:32,240
is no longer available at this point

00:21:31,120 --> 00:21:33,600
it's good to go through

00:21:32,240 --> 00:21:35,440
check all your pods and make sure that

00:21:33,600 --> 00:21:37,919
they can actually run at their request

00:21:35,440 --> 00:21:37,919
values

00:21:40,640 --> 00:21:43,760
okay so the question about how do i make

00:21:43,360 --> 00:21:46,480
it

00:21:43,760 --> 00:21:49,240
hpa object um the actual code you can

00:21:46,480 --> 00:21:52,080
find online but it's basically

00:21:49,240 --> 00:21:54,080
cubecontrolcreatehpa given information

00:21:52,080 --> 00:21:55,200
and then an object like this is going to

00:21:54,080 --> 00:21:57,280
pop out

00:21:55,200 --> 00:21:58,640
so you've got the name of the hpa object

00:21:57,280 --> 00:22:01,280
you've got the deployment that it's

00:21:58,640 --> 00:22:02,960
referencing to scale up and scale down

00:22:01,280 --> 00:22:04,720
you've got the age it was created and

00:22:02,960 --> 00:22:06,720
then you've got these interesting fields

00:22:04,720 --> 00:22:10,159
here

00:22:06,720 --> 00:22:11,760
so the first thing is how many replicas

00:22:10,159 --> 00:22:12,960
can you have what's the minimum number

00:22:11,760 --> 00:22:14,480
of things you can have what's the

00:22:12,960 --> 00:22:17,600
maximum

00:22:14,480 --> 00:22:18,080
so these are numbers you'd specify and

00:22:17,600 --> 00:22:20,559
then

00:22:18,080 --> 00:22:21,679
the other important number here is what

00:22:20,559 --> 00:22:24,000
is your target

00:22:21,679 --> 00:22:24,960
scaling threshold so this will be a

00:22:24,000 --> 00:22:27,919
percentage value

00:22:24,960 --> 00:22:29,679
it will be a percentage of your request

00:22:27,919 --> 00:22:31,679
so most clusters have something called a

00:22:29,679 --> 00:22:34,240
request to limit ratio

00:22:31,679 --> 00:22:35,919
and this basically says you know if i

00:22:34,240 --> 00:22:38,240
can have this amount of requests i can

00:22:35,919 --> 00:22:40,400
have five times that many limits

00:22:38,240 --> 00:22:41,840
so if that's the case then theoretically

00:22:40,400 --> 00:22:44,960
you can make this value from

00:22:41,840 --> 00:22:46,960
1 to 500 and that would be fine anything

00:22:44,960 --> 00:22:48,640
more than 500 would become meaningless

00:22:46,960 --> 00:22:51,120
because it's outside of that request

00:22:48,640 --> 00:22:51,120
ratio

00:22:51,440 --> 00:22:55,679
and when you want to calculate cpu

00:22:53,200 --> 00:22:57,600
utilization it's actually

00:22:55,679 --> 00:22:59,039
multiplying these two values together of

00:22:57,600 --> 00:23:01,600
the first of the target values

00:22:59,039 --> 00:23:02,400
and the replicas so say that we went

00:23:01,600 --> 00:23:05,280
from this

00:23:02,400 --> 00:23:06,080
and we had a load spike so we realized

00:23:05,280 --> 00:23:09,120
we had

00:23:06,080 --> 00:23:11,520
60 percent utilization sixty percent

00:23:09,120 --> 00:23:12,960
is higher than our scaling threshold so

00:23:11,520 --> 00:23:14,960
at this point it tries to split it

00:23:12,960 --> 00:23:15,679
across different pods so we go from 60

00:23:14,960 --> 00:23:18,880
over 50

00:23:15,679 --> 00:23:20,080
and one replica to 30 over 50 and two

00:23:18,880 --> 00:23:21,919
replicas

00:23:20,080 --> 00:23:22,960
because it's multiplying the numbers

00:23:21,919 --> 00:23:23,840
it's still the same amount of

00:23:22,960 --> 00:23:27,360
utilization

00:23:23,840 --> 00:23:27,360
it's just distributed better

00:23:28,320 --> 00:23:31,360
so calculating utilization with multiple

00:23:30,559 --> 00:23:33,679
containers

00:23:31,360 --> 00:23:35,440
this one comes up a lot because it's not

00:23:33,679 --> 00:23:37,840
intuitive

00:23:35,440 --> 00:23:38,880
so if you have an unbalanced pod and

00:23:37,840 --> 00:23:41,440
that's where

00:23:38,880 --> 00:23:42,960
you've set some kind of load against it

00:23:41,440 --> 00:23:43,440
and there are multiple containers in

00:23:42,960 --> 00:23:44,960
there

00:23:43,440 --> 00:23:46,799
and they all have different levels of

00:23:44,960 --> 00:23:49,520
cpu utilization

00:23:46,799 --> 00:23:50,080
you can wind up in a funny spot where

00:23:49,520 --> 00:23:53,600
you don't

00:23:50,080 --> 00:23:55,840
entirely know what the utilization is

00:23:53,600 --> 00:23:57,840
so you may have an intuitive idea okay

00:23:55,840 --> 00:24:00,480
we got two fifties and one hundred so

00:23:57,840 --> 00:24:03,600
that's probably sixty-six percent

00:24:00,480 --> 00:24:05,840
no the way that the formula works is it

00:24:03,600 --> 00:24:07,919
actually goes over the cpu used

00:24:05,840 --> 00:24:09,840
and the cpu requested and it sums them

00:24:07,919 --> 00:24:12,559
across all the containers

00:24:09,840 --> 00:24:14,000
so it's not necessarily intuitive and it

00:24:12,559 --> 00:24:16,159
can get you into problems

00:24:14,000 --> 00:24:18,000
if there's a case where you've got one

00:24:16,159 --> 00:24:19,600
container that is completely maxed

00:24:18,000 --> 00:24:21,039
out it's hit its bottleneck it's

00:24:19,600 --> 00:24:22,720
throwing errors

00:24:21,039 --> 00:24:24,080
but there are larger containers on the

00:24:22,720 --> 00:24:25,919
pod that look fine

00:24:24,080 --> 00:24:27,360
the autoscaler just looks at this one

00:24:25,919 --> 00:24:29,279
overall number and says

00:24:27,360 --> 00:24:31,279
oh you're good there's no need to worry

00:24:29,279 --> 00:24:33,600
about it

00:24:31,279 --> 00:24:36,559
so be aware of that that's gotten us

00:24:33,600 --> 00:24:36,559
into trouble before

00:24:36,960 --> 00:24:42,640
so for sizing guidance

00:24:40,080 --> 00:24:44,320
we found that the different quality of

00:24:42,640 --> 00:24:45,760
service classes you need to handle them

00:24:44,320 --> 00:24:47,760
slightly different

00:24:45,760 --> 00:24:49,679
the most common type of pod that we have

00:24:47,760 --> 00:24:52,960
is burstable so it has a request

00:24:49,679 --> 00:24:54,000
it has a higher limit so we found that

00:24:52,960 --> 00:24:56,640
for these pods

00:24:54,000 --> 00:24:58,640
size them somewhere between 70 and 100

00:24:56,640 --> 00:25:00,640
utilization

00:24:58,640 --> 00:25:03,679
some of our pods they do nothing and

00:25:00,640 --> 00:25:06,480
then they get a massive spike of traffic

00:25:03,679 --> 00:25:06,880
so we go ahead and we say okay when you

00:25:06,480 --> 00:25:09,440
see

00:25:06,880 --> 00:25:09,919
something closer to 100 that's fine

00:25:09,440 --> 00:25:11,760
because

00:25:09,919 --> 00:25:14,640
honestly it's going to hit all of those

00:25:11,760 --> 00:25:16,400
marks pretty quickly

00:25:14,640 --> 00:25:18,080
when we have those macro services like

00:25:16,400 --> 00:25:21,039
we talked about the vm

00:25:18,080 --> 00:25:23,200
kind of in a very tight jacket we go

00:25:21,039 --> 00:25:26,240
ahead and size those on the lower end

00:25:23,200 --> 00:25:28,640
we'll say okay size at 70 and hopefully

00:25:26,240 --> 00:25:30,080
that rest of that request and your limit

00:25:28,640 --> 00:25:32,000
will help you survive long enough to

00:25:30,080 --> 00:25:35,039
reach ready state

00:25:32,000 --> 00:25:37,760
we actually said if you can't do this

00:25:35,039 --> 00:25:39,520
sizing on a threshold of 70 or higher

00:25:37,760 --> 00:25:40,880
you really just should not be auto

00:25:39,520 --> 00:25:44,640
scaling we found that

00:25:40,880 --> 00:25:46,559
it's just not cost effective

00:25:44,640 --> 00:25:48,400
so there are different rules for a

00:25:46,559 --> 00:25:50,000
guaranteed quality of service class

00:25:48,400 --> 00:25:51,919
because these are the ones where request

00:25:50,000 --> 00:25:54,400
and limits are equal

00:25:51,919 --> 00:25:55,840
so when you start running into a problem

00:25:54,400 --> 00:25:56,480
with how much you have left of your

00:25:55,840 --> 00:25:59,440
request

00:25:56,480 --> 00:26:00,480
you have no limits to save you afterward

00:25:59,440 --> 00:26:02,240
so for these we've said

00:26:00,480 --> 00:26:03,600
size lower than an eighty percent

00:26:02,240 --> 00:26:05,200
utilization because

00:26:03,600 --> 00:26:06,640
if you're cheating up into the ninety

00:26:05,200 --> 00:26:08,080
percent range

00:26:06,640 --> 00:26:10,559
you're just going to get yourself in

00:26:08,080 --> 00:26:12,240
trouble we actually found there's a

00:26:10,559 --> 00:26:13,840
counter-intuitive strategy for these

00:26:12,240 --> 00:26:15,039
where you could set a really low

00:26:13,840 --> 00:26:17,120
threshold

00:26:15,039 --> 00:26:19,039
and a low number of max replicas and

00:26:17,120 --> 00:26:22,720
have it scale out from its min

00:26:19,039 --> 00:26:24,480
hit its max and then it will operate

00:26:22,720 --> 00:26:27,200
inside its cpu requests

00:26:24,480 --> 00:26:28,799
afterward and that way you get an

00:26:27,200 --> 00:26:30,799
immediate response when you need the

00:26:28,799 --> 00:26:33,200
pods and then they can also kind of

00:26:30,799 --> 00:26:34,960
gradually fill up

00:26:33,200 --> 00:26:36,640
one thing to caution you about

00:26:34,960 --> 00:26:40,799
thresholds above 100

00:26:36,640 --> 00:26:42,240
are risky uh requests

00:26:40,799 --> 00:26:44,159
are the only thing that you are ever

00:26:42,240 --> 00:26:46,480
guaranteed on a kubernetes cluster

00:26:44,159 --> 00:26:48,799
limits are lovely when they exist but

00:26:46,480 --> 00:26:51,520
you will only ever have requests

00:26:48,799 --> 00:26:51,840
so it's a bad idea to rely on limits

00:26:51,520 --> 00:26:54,240
being

00:26:51,840 --> 00:26:55,039
available to you especially if you're

00:26:54,240 --> 00:26:57,279
going to

00:26:55,039 --> 00:26:58,320
gate your auto scaling behind having a

00:26:57,279 --> 00:27:00,000
limit

00:26:58,320 --> 00:27:03,600
because you can hit a point where you're

00:27:00,000 --> 00:27:06,480
in resource stress in your cluster

00:27:03,600 --> 00:27:09,120
and you can never get above 100 to point

00:27:06,480 --> 00:27:11,440
out hey i should have some more pods

00:27:09,120 --> 00:27:14,159
so we've seen people get into bad states

00:27:11,440 --> 00:27:14,159
because of that

00:27:15,039 --> 00:27:18,880
so if you're sizing your max and your

00:27:17,039 --> 00:27:20,480
min replicas

00:27:18,880 --> 00:27:22,320
first thing to know set your threshold

00:27:20,480 --> 00:27:24,080
first depending on your threshold the

00:27:22,320 --> 00:27:26,559
max replica value is going to ping-pong

00:27:24,080 --> 00:27:28,480
back and forth

00:27:26,559 --> 00:27:30,240
so the way that we like to size these

00:27:28,480 --> 00:27:32,480
whenever we have the opportunity is we

00:27:30,240 --> 00:27:34,159
look at the current cpu behavior

00:27:32,480 --> 00:27:36,000
what kind of usage have you had for the

00:27:34,159 --> 00:27:39,120
last 90 days

00:27:36,000 --> 00:27:40,159
so this slightly hard to read yellow

00:27:39,120 --> 00:27:43,760
chart

00:27:40,159 --> 00:27:45,279
is a graph of the cpu utilization that a

00:27:43,760 --> 00:27:47,919
particular service has seen

00:27:45,279 --> 00:27:49,840
so you can see steady state and then

00:27:47,919 --> 00:27:53,279
they get a massive spike of traffic and

00:27:49,840 --> 00:27:53,279
it goes right back down again

00:27:53,360 --> 00:27:56,559
so i've drawn the lines that we've taken

00:27:55,520 --> 00:27:59,840
to say okay

00:27:56,559 --> 00:28:01,279
this is our median cpu usage so

00:27:59,840 --> 00:28:02,960
at least half the time we're going to be

00:28:01,279 --> 00:28:05,760
below that and as you can see this is

00:28:02,960 --> 00:28:08,080
actually a very conservative line there

00:28:05,760 --> 00:28:10,320
and then i've also drawn a line for this

00:28:08,080 --> 00:28:11,360
is above the maximum cpu usage we've

00:28:10,320 --> 00:28:12,720
ever seen

00:28:11,360 --> 00:28:15,039
and you'll have to do a little bit of

00:28:12,720 --> 00:28:15,600
math here but you want to sum the cpu

00:28:15,039 --> 00:28:18,720
requests

00:28:15,600 --> 00:28:20,640
of the minimum number of pods you have

00:28:18,720 --> 00:28:22,000
make sure that's above your median cpu

00:28:20,640 --> 00:28:23,520
usage

00:28:22,000 --> 00:28:26,240
if you're not above your median you can

00:28:23,520 --> 00:28:28,799
wind up kind of toggling back and forth

00:28:26,240 --> 00:28:31,120
extremely quickly and that can actually

00:28:28,799 --> 00:28:32,480
cause some of the other kubernetes pods

00:28:31,120 --> 00:28:34,000
to get stressed

00:28:32,480 --> 00:28:35,600
because they keep having to process the

00:28:34,000 --> 00:28:36,000
events of you scaling up and scaling

00:28:35,600 --> 00:28:40,159
down

00:28:36,000 --> 00:28:42,799
around one particular value

00:28:40,159 --> 00:28:45,120
for your max replicas a piece of advice

00:28:42,799 --> 00:28:46,880
set your max higher than your min

00:28:45,120 --> 00:28:48,880
uh you wouldn't think you have to say it

00:28:46,880 --> 00:28:51,279
out loud but we have seen

00:28:48,880 --> 00:28:51,919
multiple occasions where the min and max

00:28:51,279 --> 00:28:54,720
were equal

00:28:51,919 --> 00:28:56,000
the auto scaler said i should scale and

00:28:54,720 --> 00:28:58,559
then it realized that it couldn't

00:28:56,000 --> 00:29:03,440
because it was already at its maximum

00:28:58,559 --> 00:29:06,000
so check that if you're having issues

00:29:03,440 --> 00:29:07,919
um similar calculation to the min make

00:29:06,000 --> 00:29:09,919
sure that your summed cpu requests are

00:29:07,919 --> 00:29:10,320
higher than whatever cpu usage you've

00:29:09,919 --> 00:29:13,600
seen

00:29:10,320 --> 00:29:15,600
in this observation period and then

00:29:13,600 --> 00:29:17,919
one other value to be aware of some

00:29:15,600 --> 00:29:20,799
namespaces come with a cpu quota

00:29:17,919 --> 00:29:22,000
this is a kubernetes concept that

00:29:20,799 --> 00:29:24,000
basically says

00:29:22,000 --> 00:29:25,600
i'm putting you in namespaces so that

00:29:24,000 --> 00:29:27,039
you never have to talk you're not going

00:29:25,600 --> 00:29:29,279
to intersect

00:29:27,039 --> 00:29:30,960
but i've realized some namespaces can

00:29:29,279 --> 00:29:31,520
run away with all the resources in a

00:29:30,960 --> 00:29:35,120
cluster

00:29:31,520 --> 00:29:35,520
if i don't cap them somehow so a quota

00:29:35,120 --> 00:29:37,360
says

00:29:35,520 --> 00:29:39,760
in your namespace you can only use this

00:29:37,360 --> 00:29:42,240
much cpu

00:29:39,760 --> 00:29:43,840
if your quota has limited your cpu and

00:29:42,240 --> 00:29:46,000
your auto scaler wants to

00:29:43,840 --> 00:29:47,360
create more pods that would go over that

00:29:46,000 --> 00:29:49,120
the quota will win

00:29:47,360 --> 00:29:50,399
so you need to be aware of that value

00:29:49,120 --> 00:29:54,720
and if it's going to keep you from

00:29:50,399 --> 00:29:57,760
scaling when you think you could

00:29:54,720 --> 00:29:59,120
so a basic process to create modify and

00:29:57,760 --> 00:30:01,360
verify in staging

00:29:59,120 --> 00:30:02,880
or whatever non-production environment

00:30:01,360 --> 00:30:05,760
you like

00:30:02,880 --> 00:30:06,880
uh first off create an object uh low

00:30:05,760 --> 00:30:11,279
threshold low min

00:30:06,880 --> 00:30:14,640
high max stick it in your environment

00:30:11,279 --> 00:30:16,480
make sure it turns on so drive some kind

00:30:14,640 --> 00:30:17,279
of load against it it doesn't have to be

00:30:16,480 --> 00:30:19,200
much

00:30:17,279 --> 00:30:21,520
what you're mainly looking for here is

00:30:19,200 --> 00:30:23,360
do you see a utilization percentage

00:30:21,520 --> 00:30:24,880
if it shows unknown that means

00:30:23,360 --> 00:30:27,760
something's hooked up incorrectly and

00:30:24,880 --> 00:30:30,480
you need to go back and fix it

00:30:27,760 --> 00:30:32,159
if you see your percent utilization go

00:30:30,480 --> 00:30:33,760
over your threshold you should see new

00:30:32,159 --> 00:30:36,240
replicas scale up

00:30:33,760 --> 00:30:37,760
so do this and kind of sanity check make

00:30:36,240 --> 00:30:39,279
sure that your objects are set up

00:30:37,760 --> 00:30:40,960
correctly

00:30:39,279 --> 00:30:42,720
then after that you can modify to your

00:30:40,960 --> 00:30:43,919
preferred threshold and your scale down

00:30:42,720 --> 00:30:46,880
min and max value

00:30:43,919 --> 00:30:48,320
and then you can drive more load so

00:30:46,880 --> 00:30:51,360
you'd expect that it would continue

00:30:48,320 --> 00:30:53,840
scaling up replicas as expected

00:30:51,360 --> 00:30:55,440
one way to make sure that you're doing

00:30:53,840 --> 00:30:57,519
what you think you're doing is go

00:30:55,440 --> 00:30:59,360
through and check your cpu metrics

00:30:57,519 --> 00:31:01,120
you can use cubecontrol top to do it

00:30:59,360 --> 00:31:03,360
entirely in the system or you can use

00:31:01,120 --> 00:31:05,760
your third-party metrics tool

00:31:03,360 --> 00:31:08,159
and you want to look at you know does

00:31:05,760 --> 00:31:10,559
this match with having a cpu bottleneck

00:31:08,159 --> 00:31:14,080
and does this match with having scaling

00:31:10,559 --> 00:31:15,760
on the cpu utilization threshold

00:31:14,080 --> 00:31:18,240
after that you want to review your load

00:31:15,760 --> 00:31:20,159
driver logs you may have scaled up

00:31:18,240 --> 00:31:21,519
but if you don't get there in the right

00:31:20,159 --> 00:31:22,880
amount of time you're still going to see

00:31:21,519 --> 00:31:24,960
those error rates you're still going to

00:31:22,880 --> 00:31:26,799
see the long running requests

00:31:24,960 --> 00:31:28,720
so just make sure that yes you're

00:31:26,799 --> 00:31:30,559
scaling but you're also doing what you

00:31:28,720 --> 00:31:34,159
need for your service

00:31:30,559 --> 00:31:35,440
and then test modify verify

00:31:34,159 --> 00:31:37,760
and keep going till you like your

00:31:35,440 --> 00:31:39,519
results after that

00:31:37,760 --> 00:31:41,600
you deploy into production and you sail

00:31:39,519 --> 00:31:43,120
on into the sunset

00:31:41,600 --> 00:31:45,120
this is the part where several of my

00:31:43,120 --> 00:31:46,080
service teams got really nervous because

00:31:45,120 --> 00:31:49,360
they didn't trust

00:31:46,080 --> 00:31:49,840
hpa objects enough yet if you're worried

00:31:49,360 --> 00:31:51,279
about

00:31:49,840 --> 00:31:52,960
having one of these objects running in

00:31:51,279 --> 00:31:54,640
your production environment and you

00:31:52,960 --> 00:31:57,679
really just want a sanity check

00:31:54,640 --> 00:31:59,760
one easy way to do a sanity check is

00:31:57,679 --> 00:32:01,200
make an object set your minimum to

00:31:59,760 --> 00:32:02,960
whatever the current number of things

00:32:01,200 --> 00:32:04,960
you have in production is

00:32:02,960 --> 00:32:07,440
that way you cannot make it any worse

00:32:04,960 --> 00:32:08,720
even if hpa just falls dead on the floor

00:32:07,440 --> 00:32:11,360
you still have the same number of

00:32:08,720 --> 00:32:13,760
replicas and then you set your max

00:32:11,360 --> 00:32:15,279
replicas to some count higher than that

00:32:13,760 --> 00:32:18,880
and now if you do have that surprise

00:32:15,279 --> 00:32:18,880
load surge it can handle it

00:32:19,039 --> 00:32:22,240
and then the other important thing about

00:32:20,720 --> 00:32:24,559
deploying to production

00:32:22,240 --> 00:32:26,000
your workloads will change over time the

00:32:24,559 --> 00:32:28,559
thing you size today

00:32:26,000 --> 00:32:29,279
may not work next year it's always good

00:32:28,559 --> 00:32:32,000
to monitor

00:32:29,279 --> 00:32:33,440
and iterate if you see large changes in

00:32:32,000 --> 00:32:35,760
behaviors

00:32:33,440 --> 00:32:38,720
so it's helpful to have alerts that say

00:32:35,760 --> 00:32:42,080
i have scaled up to my maximum number of

00:32:38,720 --> 00:32:44,000
replicas so if you see that alert if you

00:32:42,080 --> 00:32:45,840
see it more than once you go okay

00:32:44,000 --> 00:32:47,039
something is wrong here we should check

00:32:45,840 --> 00:32:48,799
and make sure

00:32:47,039 --> 00:32:51,679
that we are actually handling everything

00:32:48,799 --> 00:32:54,799
and we don't in fact need more replicas

00:32:51,679 --> 00:32:57,600
um you can also add an alert for

00:32:54,799 --> 00:33:00,960
i've literally never scaled above my men

00:32:57,600 --> 00:33:00,960
do i need this much hardware

00:33:01,840 --> 00:33:14,080
okay break for questions again

00:33:10,399 --> 00:33:16,480
um all right so we have a question from

00:33:14,080 --> 00:33:17,279
krishna gelati uh could you share some

00:33:16,480 --> 00:33:20,720
information

00:33:17,279 --> 00:33:20,720
about autoscale down

00:33:21,279 --> 00:33:26,799
so scale down so

00:33:24,399 --> 00:33:29,440
coming particularly from an hpa cpu

00:33:26,799 --> 00:33:29,440
perspective

00:33:29,519 --> 00:33:34,640
we've talked about scaling up it's based

00:33:31,679 --> 00:33:36,480
on that utilization threshold

00:33:34,640 --> 00:33:38,799
by the same token it will look at that

00:33:36,480 --> 00:33:41,200
utilization threshold and if it sees

00:33:38,799 --> 00:33:42,799
that it's dropped to a point where it

00:33:41,200 --> 00:33:45,600
could do that math in reverse

00:33:42,799 --> 00:33:46,880
and it could say yes we can fit less

00:33:45,600 --> 00:33:48,880
pods

00:33:46,880 --> 00:33:51,279
if you stay in a point where the math

00:33:48,880 --> 00:33:53,600
works for

00:33:51,279 --> 00:33:54,880
i believe that it was two minutes in the

00:33:53,600 --> 00:33:56,720
last release i looked at

00:33:54,880 --> 00:33:58,159
there will be a certain amount of scale

00:33:56,720 --> 00:34:00,320
down time

00:33:58,159 --> 00:34:02,000
i believe it's configurable if you stay

00:34:00,320 --> 00:34:03,919
there long enough kubernetes will say

00:34:02,000 --> 00:34:07,440
okay the math works

00:34:03,919 --> 00:34:11,679
i can start scaling down now

00:34:07,440 --> 00:34:13,760
so it actually runs the same calculation

00:34:11,679 --> 00:34:15,440
it runs a calculation and it spits out

00:34:13,760 --> 00:34:16,480
this is the number of replicas i need

00:34:15,440 --> 00:34:19,440
right now

00:34:16,480 --> 00:34:20,960
and if that is different from the actual

00:34:19,440 --> 00:34:21,359
amount of replicas you have for long

00:34:20,960 --> 00:34:22,960
enough

00:34:21,359 --> 00:34:25,200
that's what actually triggers that scale

00:34:22,960 --> 00:34:25,200
down

00:34:25,520 --> 00:34:28,480
hopefully that's helpful

00:34:32,839 --> 00:34:36,720
cool doesn't look like we have any other

00:34:35,520 --> 00:34:39,359
questions for now oh

00:34:36,720 --> 00:34:40,159
actually we got another one uh patrick

00:34:39,359 --> 00:34:42,000
brooks

00:34:40,159 --> 00:34:43,440
trigger alarm when max replicas are

00:34:42,000 --> 00:34:44,240
reached and trigger alarm have never

00:34:43,440 --> 00:34:45,919
scaled down

00:34:44,240 --> 00:34:47,919
above min replicas have you ever seen

00:34:45,919 --> 00:34:50,399
these alarms created via prometheus or

00:34:47,919 --> 00:34:53,119
alert manager

00:34:50,399 --> 00:34:53,839
i haven't seen it in prometheus um i

00:34:53,119 --> 00:34:56,960
have seen

00:34:53,839 --> 00:35:00,560
teams use it in systig i believe

00:34:56,960 --> 00:35:00,560
prometheus should work the same way

00:35:01,119 --> 00:35:05,040
let's see most third-party metrics

00:35:03,040 --> 00:35:05,839
providers have some level of alerting

00:35:05,040 --> 00:35:08,880
for

00:35:05,839 --> 00:35:10,320
you've hit this i would assume that

00:35:08,880 --> 00:35:12,480
would be a standard feature

00:35:10,320 --> 00:35:14,320
so i can't tell you exactly how to do it

00:35:12,480 --> 00:35:17,839
but i would expect that to be

00:35:14,320 --> 00:35:17,839
an option that you can do

00:35:20,480 --> 00:35:24,880
all right patrick brooks says cool

00:35:22,560 --> 00:35:24,880
thanks

00:35:26,079 --> 00:35:29,599
and all right that looks about

00:35:27,760 --> 00:35:31,440
everything we got for now

00:35:29,599 --> 00:35:32,720
and we got about 10 minutes left i know

00:35:31,440 --> 00:35:34,400
it's a little bit early

00:35:32,720 --> 00:35:37,359
to call that up but letting you know

00:35:34,400 --> 00:35:40,079
okay that's super helpful thank you

00:35:37,359 --> 00:35:41,040
all right so case study i know we had a

00:35:40,079 --> 00:35:43,440
request for

00:35:41,040 --> 00:35:44,880
how do i tune my hbas for real service

00:35:43,440 --> 00:35:46,160
if they're acting kind of funky in

00:35:44,880 --> 00:35:47,920
production

00:35:46,160 --> 00:35:49,920
so i'm going to go through the

00:35:47,920 --> 00:35:53,280
methodology that our team uses

00:35:49,920 --> 00:35:55,200
and also walk you through a service that

00:35:53,280 --> 00:35:58,320
was having problems and what we did to

00:35:55,200 --> 00:36:00,640
get them back on track

00:35:58,320 --> 00:36:02,000
so when people come to us and say our

00:36:00,640 --> 00:36:04,480
hpa is broken

00:36:02,000 --> 00:36:06,000
we don't know why i make them run

00:36:04,480 --> 00:36:08,560
through a prereqs checklist

00:36:06,000 --> 00:36:10,160
just to rule out a lot of the funny

00:36:08,560 --> 00:36:11,200
situations that we've found along the

00:36:10,160 --> 00:36:13,440
way

00:36:11,200 --> 00:36:14,640
so first off is have you run for long

00:36:13,440 --> 00:36:16,720
enough to gather data

00:36:14,640 --> 00:36:19,680
again i really like that three month

00:36:16,720 --> 00:36:21,680
look back when we can get it

00:36:19,680 --> 00:36:23,040
next is have you seen evidence of load

00:36:21,680 --> 00:36:24,480
spikes higher than your day-to-day

00:36:23,040 --> 00:36:26,720
utilization

00:36:24,480 --> 00:36:28,240
if there's no load spikes auto scaling

00:36:26,720 --> 00:36:29,920
might not be your right choice it may

00:36:28,240 --> 00:36:32,560
just be you don't have enough machines

00:36:29,920 --> 00:36:32,560
to begin with

00:36:32,880 --> 00:36:37,520
we need to check that the bottleneck is

00:36:34,720 --> 00:36:40,000
cpu if you're using a cpu based hpa

00:36:37,520 --> 00:36:42,960
because if it's a different metric it's

00:36:40,000 --> 00:36:45,280
not going to scale the way you think

00:36:42,960 --> 00:36:46,960
so here's where we get into a question

00:36:45,280 --> 00:36:48,480
we had earlier

00:36:46,960 --> 00:36:50,560
need to make sure that the cpu

00:36:48,480 --> 00:36:52,240
utilization is spread evenly or near

00:36:50,560 --> 00:36:53,920
evenly across all the pods

00:36:52,240 --> 00:36:55,359
because you can get into a point where

00:36:53,920 --> 00:36:58,720
you're scaling things up but only

00:36:55,359 --> 00:37:00,240
directing traffic to one place

00:36:58,720 --> 00:37:02,079
you need to check that your load spike

00:37:00,240 --> 00:37:03,440
duration is longer than your startup

00:37:02,079 --> 00:37:05,280
time for new pod

00:37:03,440 --> 00:37:06,480
if your pod takes 10 minutes to start up

00:37:05,280 --> 00:37:08,640
and the spike is

00:37:06,480 --> 00:37:11,760
already done by then it's not really

00:37:08,640 --> 00:37:12,800
helpful to start a new pod

00:37:11,760 --> 00:37:15,040
you should check that all your

00:37:12,800 --> 00:37:16,880
containers have explicit cpu requests

00:37:15,040 --> 00:37:18,480
and limits defined

00:37:16,880 --> 00:37:20,560
why is this because if they're not

00:37:18,480 --> 00:37:23,280
defined kubernetes uses the default

00:37:20,560 --> 00:37:26,800
values and they are tiny tiny values

00:37:23,280 --> 00:37:28,160
we're talking a quarter of a cpu here

00:37:26,800 --> 00:37:30,320
you should check that your containers

00:37:28,160 --> 00:37:32,240
are normalized so this is what we talked

00:37:30,320 --> 00:37:32,720
about with balanced or unbalanced if

00:37:32,240 --> 00:37:35,359
you've got

00:37:32,720 --> 00:37:36,240
multiple containers they should all

00:37:35,359 --> 00:37:38,079
reach

00:37:36,240 --> 00:37:39,280
the same percentage of their cpu request

00:37:38,079 --> 00:37:41,520
at the same time

00:37:39,280 --> 00:37:42,480
and also cpu should be the bottleneck

00:37:41,520 --> 00:37:44,000
for all of them

00:37:42,480 --> 00:37:46,240
you shouldn't hit some other problem

00:37:44,000 --> 00:37:47,920
first

00:37:46,240 --> 00:37:50,079
it's also helpful if the team knows

00:37:47,920 --> 00:37:51,520
their typical production workload and if

00:37:50,079 --> 00:37:53,440
they have some kind of automation that

00:37:51,520 --> 00:37:54,000
they can use to drive workload and test

00:37:53,440 --> 00:37:55,440
it

00:37:54,000 --> 00:37:57,920
you know before they go live and it

00:37:55,440 --> 00:37:57,920
blows up

00:37:58,079 --> 00:38:03,440
so let's talk about foo service

00:38:01,280 --> 00:38:05,450
food service they are one of our

00:38:03,440 --> 00:38:06,720
services they were running in production

00:38:05,450 --> 00:38:09,040
[Music]

00:38:06,720 --> 00:38:11,040
and we kept getting incidents from the

00:38:09,040 --> 00:38:13,280
sre team going

00:38:11,040 --> 00:38:14,720
you know it's odd but occasionally we'll

00:38:13,280 --> 00:38:17,760
see the service they get

00:38:14,720 --> 00:38:19,440
a big spike of traffic and then it works

00:38:17,760 --> 00:38:21,440
great for part of the time

00:38:19,440 --> 00:38:22,560
and then we also have random bursts of

00:38:21,440 --> 00:38:24,560
500 errors

00:38:22,560 --> 00:38:27,119
and they slow way down and we're not

00:38:24,560 --> 00:38:27,119
sure why

00:38:27,359 --> 00:38:30,560
so we looked at this they meet most of

00:38:29,280 --> 00:38:32,160
the prereqs

00:38:30,560 --> 00:38:33,839
um everything else we could kind of

00:38:32,160 --> 00:38:36,320
smush in so it worked

00:38:33,839 --> 00:38:38,480
their cpu bound their usage was across

00:38:36,320 --> 00:38:40,560
all the pods evenly

00:38:38,480 --> 00:38:43,440
the spikes were an hour plus and their

00:38:40,560 --> 00:38:44,880
pods started faster than that thankfully

00:38:43,440 --> 00:38:46,400
they've got explicit requests they've

00:38:44,880 --> 00:38:48,000
got explicit limits there's one

00:38:46,400 --> 00:38:49,280
container per pod so there's no

00:38:48,000 --> 00:38:51,520
normalizing weirdness

00:38:49,280 --> 00:38:54,560
and they had automation so we said great

00:38:51,520 --> 00:38:54,560
we can take them on

00:38:55,359 --> 00:38:58,880
so first step is gather and visualize

00:38:57,359 --> 00:39:00,800
your data

00:38:58,880 --> 00:39:01,920
auto scalers are interesting to try and

00:39:00,800 --> 00:39:04,400
visualize because

00:39:01,920 --> 00:39:06,720
it's changing constantly based on what

00:39:04,400 --> 00:39:10,000
happened in the time stamps before

00:39:06,720 --> 00:39:11,040
whatever time you are now so you can

00:39:10,000 --> 00:39:13,119
either

00:39:11,040 --> 00:39:14,800
try and figure this out by running the

00:39:13,119 --> 00:39:16,000
exact right amount of load

00:39:14,800 --> 00:39:18,480
or you can try and figure it out

00:39:16,000 --> 00:39:19,440
beforehand but whenever you change one

00:39:18,480 --> 00:39:22,400
of the variables

00:39:19,440 --> 00:39:23,839
your min your max your utilization

00:39:22,400 --> 00:39:25,839
you're going to see the entire

00:39:23,839 --> 00:39:27,760
calculation change

00:39:25,839 --> 00:39:30,000
so first time we tried to write it out

00:39:27,760 --> 00:39:31,760
by hand we said okay minute zero minute

00:39:30,000 --> 00:39:34,320
one minute two

00:39:31,760 --> 00:39:36,240
what kind of usage do we see what's the

00:39:34,320 --> 00:39:38,560
utilization that we see

00:39:36,240 --> 00:39:40,640
how many pods do we need when do they

00:39:38,560 --> 00:39:42,720
actually start up

00:39:40,640 --> 00:39:44,640
and we realized this is a nightmare to

00:39:42,720 --> 00:39:45,839
write out it took us about half an hour

00:39:44,640 --> 00:39:46,400
and then we said you know what we need

00:39:45,839 --> 00:39:48,000
to do

00:39:46,400 --> 00:39:50,480
we need to change the utilization

00:39:48,000 --> 00:39:52,000
threshold that changes literally every

00:39:50,480 --> 00:39:55,839
value on this chart

00:39:52,000 --> 00:39:57,680
cool so we went ahead and we actually

00:39:55,839 --> 00:40:00,079
wrote a python notebook

00:39:57,680 --> 00:40:02,000
created an emulator that mocks what will

00:40:00,079 --> 00:40:03,440
happen

00:40:02,000 --> 00:40:06,000
if you're going to do this kind of

00:40:03,440 --> 00:40:07,280
calculation mocking i would highly

00:40:06,000 --> 00:40:09,280
recommend

00:40:07,280 --> 00:40:10,319
look at the number of pods that you need

00:40:09,280 --> 00:40:13,440
per timestamp

00:40:10,319 --> 00:40:15,119
and look at the utilization separately

00:40:13,440 --> 00:40:16,560
because sometimes you have issues with

00:40:15,119 --> 00:40:18,880
the number of pods in play

00:40:16,560 --> 00:40:20,640
and sometimes you have issues with the

00:40:18,880 --> 00:40:24,319
amount of utilization that you think you

00:40:20,640 --> 00:40:25,839
can get but you actually can't

00:40:24,319 --> 00:40:27,599
uh katie you have about five minutes

00:40:25,839 --> 00:40:30,480
left sorry for interrupting

00:40:27,599 --> 00:40:31,839
all ready so data gathering we looked at

00:40:30,480 --> 00:40:33,520
our foo service

00:40:31,839 --> 00:40:35,520
we reached into cube control to get

00:40:33,520 --> 00:40:36,640
these values so you've got your cpu

00:40:35,520 --> 00:40:39,760
values

00:40:36,640 --> 00:40:41,839
you've got your hpa values

00:40:39,760 --> 00:40:43,440
and we also wrote another jupyter

00:40:41,839 --> 00:40:45,280
notebook to go through and look at how

00:40:43,440 --> 00:40:47,760
long does our pod take to boot to get

00:40:45,280 --> 00:40:51,599
scheduled initialized and ready

00:40:47,760 --> 00:40:54,640
so based on that we pulled these out

00:40:51,599 --> 00:40:56,400
um we looked at the scaling situation so

00:40:54,640 --> 00:40:59,119
we went into our metrics provider

00:40:56,400 --> 00:41:00,880
looked at we've got seven cpus to start

00:40:59,119 --> 00:41:02,960
94 to finish

00:41:00,880 --> 00:41:04,240
how long was that scale up how long did

00:41:02,960 --> 00:41:05,839
we hold it there

00:41:04,240 --> 00:41:08,079
what did scale down look like does it

00:41:05,839 --> 00:41:11,280
look like that was the problem

00:41:08,079 --> 00:41:12,000
and we plugged everything in so when we

00:41:11,280 --> 00:41:15,680
plugged it in

00:41:12,000 --> 00:41:17,280
we found that from a pod replica

00:41:15,680 --> 00:41:18,800
standpoint we're totally fine

00:41:17,280 --> 00:41:21,359
we go from the minimum to something

00:41:18,800 --> 00:41:22,880
below the maximum should be totally fine

00:41:21,359 --> 00:41:26,000
so the team looked at this and they said

00:41:22,880 --> 00:41:28,160
oh hba's not having any problems

00:41:26,000 --> 00:41:30,079
now the issue is on the other side and

00:41:28,160 --> 00:41:31,760
utilization

00:41:30,079 --> 00:41:34,240
so i've mentioned this several times

00:41:31,760 --> 00:41:35,680
before anything over 100 is not

00:41:34,240 --> 00:41:37,599
guaranteed

00:41:35,680 --> 00:41:39,920
so what we had was they were running for

00:41:37,599 --> 00:41:41,200
70 minutes in the danger zone of relying

00:41:39,920 --> 00:41:44,079
on limits to be there

00:41:41,200 --> 00:41:45,680
and periodically in those 70 minutes

00:41:44,079 --> 00:41:47,119
limits were not available

00:41:45,680 --> 00:41:48,960
so they actually started dropping

00:41:47,119 --> 00:41:51,119
requests

00:41:48,960 --> 00:41:52,319
so to get this team back to a good spot

00:41:51,119 --> 00:41:53,760
we said

00:41:52,319 --> 00:41:55,760
change your threshold to something

00:41:53,760 --> 00:41:57,839
that's a hundred percent or less so that

00:41:55,760 --> 00:41:59,599
you're running in requests

00:41:57,839 --> 00:42:01,359
which you could do that you could say

00:41:59,599 --> 00:42:03,040
yeah we're done except

00:42:01,359 --> 00:42:05,040
that's not true when you change your

00:42:03,040 --> 00:42:07,839
threshold you change your max

00:42:05,040 --> 00:42:09,280
so moving from 200 to 100 they would

00:42:07,839 --> 00:42:12,720
then have to change from

00:42:09,280 --> 00:42:14,640
the 12 pods they need to needing 24 pods

00:42:12,720 --> 00:42:17,119
which is above their hpa max so

00:42:14,640 --> 00:42:19,680
something else starts bottlenecking them

00:42:17,119 --> 00:42:20,560
so for this we said change to 100 or

00:42:19,680 --> 00:42:23,359
less

00:42:20,560 --> 00:42:25,839
and 24 or higher depending on what the

00:42:23,359 --> 00:42:27,599
math would actually say

00:42:25,839 --> 00:42:30,319
and then for the sake of completion on

00:42:27,599 --> 00:42:33,119
tuning we looked at it and said okay in

00:42:30,319 --> 00:42:34,880
your normal load you need two replicas

00:42:33,119 --> 00:42:37,440
now if we tried to scale you from those

00:42:34,880 --> 00:42:38,160
two to your max of 24 during a load

00:42:37,440 --> 00:42:40,000
spike

00:42:38,160 --> 00:42:41,440
we hit issues because not only do you

00:42:40,000 --> 00:42:43,520
need limits you actually need

00:42:41,440 --> 00:42:46,240
more than what's available in limits

00:42:43,520 --> 00:42:48,319
because your pods take so long to start

00:42:46,240 --> 00:42:51,359
so in the end we wound up going okay

00:42:48,319 --> 00:42:54,000
keep your minimum of 10 pods

00:42:51,359 --> 00:42:55,760
and use that scale that up to 24 and

00:42:54,000 --> 00:42:58,400
then you'll go into limits a little bit

00:42:55,760 --> 00:42:59,040
the team deemed this unacceptable risk

00:42:58,400 --> 00:43:01,760
and then

00:42:59,040 --> 00:43:03,680
you know in the future if you find that

00:43:01,760 --> 00:43:05,280
you can take on a little more risk

00:43:03,680 --> 00:43:06,720
to save a little more money you could

00:43:05,280 --> 00:43:07,920
actually scale your men back a little

00:43:06,720 --> 00:43:09,839
bit

00:43:07,920 --> 00:43:12,560
so recommendations for this team

00:43:09,839 --> 00:43:15,359
required cpu belongs in your request

00:43:12,560 --> 00:43:16,960
target utilization below 100 because

00:43:15,359 --> 00:43:19,200
otherwise you get incidents when you're

00:43:16,960 --> 00:43:21,040
resource constrained

00:43:19,200 --> 00:43:22,640
once you figure out utilization raise

00:43:21,040 --> 00:43:24,640
your max accordingly

00:43:22,640 --> 00:43:26,960
and then consider setting men a little

00:43:24,640 --> 00:43:29,920
bit lower to reduce costs but still

00:43:26,960 --> 00:43:32,240
handle load spikes

00:43:29,920 --> 00:43:34,160
so to recap we went through fundamentals

00:43:32,240 --> 00:43:37,280
we went through types of auto scaling

00:43:34,160 --> 00:43:41,839
had a deep dive and a case study uh

00:43:37,280 --> 00:43:41,839
any final questions

00:43:53,359 --> 00:43:58,160
i was on mute i'm so sorry um okay

00:43:56,160 --> 00:44:01,200
hannah lehmann asks is your code for the

00:43:58,160 --> 00:44:04,240
hpa emulator shared anywhere

00:44:01,200 --> 00:44:06,000
it is not it's also somewhat outdated

00:44:04,240 --> 00:44:08,640
because it was built for 116

00:44:06,000 --> 00:44:09,839
and the algorithms have changed again um

00:44:08,640 --> 00:44:11,599
it's something that i would be

00:44:09,839 --> 00:44:14,160
interested in open sourcing i have not

00:44:11,599 --> 00:44:14,160
done that yet

00:44:14,640 --> 00:44:23,839
all right

00:44:28,000 --> 00:44:32,400
um we have quite a few thank yous

00:44:33,119 --> 00:44:37,040
over here going on uh krishna gelati

00:44:35,680 --> 00:44:37,920
says thank you katie excellent

00:44:37,040 --> 00:44:40,160
presentation

00:44:37,920 --> 00:44:42,000
learned a lot out of it uh patrick

00:44:40,160 --> 00:44:44,640
brooks says spectacular presentation my

00:44:42,000 --> 00:44:46,240
co-workers need to see this today

00:44:44,640 --> 00:44:48,240
and hannah lehmann says great

00:44:46,240 --> 00:44:51,280
presentation thank you

00:44:48,240 --> 00:44:54,800
vikrant vermin says well explained

00:44:51,280 --> 00:44:54,800
presentation thank you katie

00:44:58,079 --> 00:45:02,560
all right well thank you everybody i

00:45:00,880 --> 00:45:03,359
hope this helps you on your road to auto

00:45:02,560 --> 00:45:06,079
scaling

00:45:03,359 --> 00:45:08,400
and when you do get the presentation

00:45:06,079 --> 00:45:10,400
slides i've included resources for

00:45:08,400 --> 00:45:12,880
the special interest group and auto

00:45:10,400 --> 00:45:14,880
scaling design proposals

00:45:12,880 --> 00:45:16,800
the different source code locations

00:45:14,880 --> 00:45:18,960
documentation highlights

00:45:16,800 --> 00:45:20,720
and also a disclaimer if you use a

00:45:18,960 --> 00:45:21,280
third-party metrics provider or cloud

00:45:20,720 --> 00:45:22,720
provider

00:45:21,280 --> 00:45:25,599
you're going to want to check their docs

00:45:22,720 --> 00:45:25,599
because they may differ

00:45:25,680 --> 00:45:33,680

YouTube URL: https://www.youtube.com/watch?v=K99NkFMmhaE


