Title: Finding the Golden Signals with Prometheus - Jack Neely
Publication date: 2020-12-11
Playlist: All Things Open 2020 - Cloud Track
Description: 
	Presented by: Jack Neely, 42 Lines, Inc.
Presented at All Things Open 2020 - Cloud Track

Abstract: Prometheus is an awesome tool to consume raw data from your application stacks and extract real data about your users' experience and the state of your fleet. But many teams end up throwing more and more data at Prometheus, or other metrics vendors, and just assume that the tool can make sense of the chaos. Well, garbage in makes for garbage out.  We will look at the common methodologies for instrumenting resources and HTTP calls and how these can be applied to produce consistency across the environment.  Especially in a micro-services style environment, consistency in instrumentation is key for creating debuggable systems.

On this journey we discover that Prometheus has problematic support for percentiles, but percentiles are key for understanding our applications.  As application stacks grow, metric cardinality and high error rates in percentiles can blind a whole organization.  How can we use Prometheus to build consistent 30 day service level objectives across our applications?  We will tackle accuracy in percentile calculations, tactics for handling cardinality, and methods to handle long service level object periods.
Captions: 
	00:00:05,279 --> 00:00:08,639
greetings everyone

00:00:06,560 --> 00:00:10,800
my name is jack neely i'm one of the

00:00:08,639 --> 00:00:13,599
engineers at 42 lines

00:00:10,800 --> 00:00:14,639
uh with me today is jared watkins as

00:00:13,599 --> 00:00:16,720
well

00:00:14,639 --> 00:00:17,760
i asked him to tag along and sort of

00:00:16,720 --> 00:00:21,359
help out with uh

00:00:17,760 --> 00:00:23,600
questions and managing q a and stuff

00:00:21,359 --> 00:00:24,880
so if you have questions uh please feel

00:00:23,600 --> 00:00:28,000
free to stick them in q

00:00:24,880 --> 00:00:28,560
a or chat and if they're on topic and

00:00:28,000 --> 00:00:30,880
pertinent

00:00:28,560 --> 00:00:31,920
uh will interrupt me with the on the

00:00:30,880 --> 00:00:33,760
presentation

00:00:31,920 --> 00:00:35,120
and we'll try to address those otherwise

00:00:33,760 --> 00:00:38,559
we'll probably take most of them

00:00:35,120 --> 00:00:38,559
after the presentation

00:00:39,200 --> 00:00:43,040
so again i'm with 42 lines jared is with

00:00:41,680 --> 00:00:45,120
42 lines

00:00:43,040 --> 00:00:47,200
uh we've spent no small amount of time

00:00:45,120 --> 00:00:47,840
with a small company that you might have

00:00:47,200 --> 00:00:51,039
heard about

00:00:47,840 --> 00:00:53,199
uh called fitbit um where we've done a

00:00:51,039 --> 00:00:54,239
lot of visibility and observability work

00:00:53,199 --> 00:00:57,360
for that company

00:00:54,239 --> 00:01:00,640
and we've worked with them uh since

00:00:57,360 --> 00:01:02,879
they were a 50-person startup to ipo

00:01:00,640 --> 00:01:06,080
and now as they look for to completing

00:01:02,879 --> 00:01:06,080
their acquisition with google

00:01:07,840 --> 00:01:11,119
um so if you're interested about what 42

00:01:10,320 --> 00:01:13,119
lines can

00:01:11,119 --> 00:01:14,400
bring to your company what we can bring

00:01:13,119 --> 00:01:17,360
to the table

00:01:14,400 --> 00:01:19,040
please feel free to send me an email or

00:01:17,360 --> 00:01:21,040
stop by our booth

00:01:19,040 --> 00:01:23,280
in the exhibit hall and feel free to ask

00:01:21,040 --> 00:01:27,840
questions questions are totally free and

00:01:23,280 --> 00:01:27,840
sewer answers

00:01:29,119 --> 00:01:32,720
okay so i'm going to talk about

00:01:30,880 --> 00:01:34,560
prometheus

00:01:32,720 --> 00:01:35,840
finding the golden signals with

00:01:34,560 --> 00:01:39,119
prometheus some

00:01:35,840 --> 00:01:40,960
stories about scaling prometheus

00:01:39,119 --> 00:01:42,960
and things to think about as your

00:01:40,960 --> 00:01:46,399
company's grows

00:01:42,960 --> 00:01:49,680
on your prometheus monitoring platform

00:01:46,399 --> 00:01:52,240
so prometheus needs no introduction

00:01:49,680 --> 00:01:54,960
it's an incredibly popular time series

00:01:52,240 --> 00:01:58,320
database for monitoring and alerting

00:01:54,960 --> 00:02:00,960
often or most often paired with grafana

00:01:58,320 --> 00:02:01,360
in fact i find it really hard to think

00:02:00,960 --> 00:02:03,600
about

00:02:01,360 --> 00:02:04,560
some of the competitors in this space

00:02:03,600 --> 00:02:07,600
that's that's

00:02:04,560 --> 00:02:09,440
not a sas vendor because prometheus has

00:02:07,600 --> 00:02:12,080
become really the de facto

00:02:09,440 --> 00:02:13,040
solution um especially as we do cloud

00:02:12,080 --> 00:02:15,440
native work

00:02:13,040 --> 00:02:16,560
uh working more moving workloads toward

00:02:15,440 --> 00:02:19,280
kubernetes

00:02:16,560 --> 00:02:20,000
and adopting that that next generation

00:02:19,280 --> 00:02:23,760
of

00:02:20,000 --> 00:02:26,879
of working in the cloud

00:02:23,760 --> 00:02:27,760
there are plenty of of competitors out

00:02:26,879 --> 00:02:29,440
there

00:02:27,760 --> 00:02:31,040
i want to point out victoria metrics

00:02:29,440 --> 00:02:34,480
because they probably

00:02:31,040 --> 00:02:36,959
the only other open source alternative

00:02:34,480 --> 00:02:38,879
that has some of the same simplicity

00:02:36,959 --> 00:02:41,519
that prometheus has

00:02:38,879 --> 00:02:42,640
being you can stand up prometheus

00:02:41,519 --> 00:02:44,800
literally in minutes

00:02:42,640 --> 00:02:46,640
start gathering intelligence and start

00:02:44,800 --> 00:02:49,360
making analytical decisions

00:02:46,640 --> 00:02:51,840
incredibly quickly which really makes it

00:02:49,360 --> 00:02:51,840
very powerful

00:02:52,239 --> 00:02:58,640
back in august google released a paper

00:02:55,360 --> 00:03:01,200
on their internal uh planet-wide

00:02:58,640 --> 00:03:04,080
time series monitoring tool which is

00:03:01,200 --> 00:03:06,400
called monarch

00:03:04,080 --> 00:03:07,519
that paper detailed their transition

00:03:06,400 --> 00:03:09,519
from borgmon

00:03:07,519 --> 00:03:10,800
to monarch and borgmon was the

00:03:09,519 --> 00:03:14,080
inspiration

00:03:10,800 --> 00:03:14,879
uh to prometheus so google of course is

00:03:14,080 --> 00:03:18,640
moved on

00:03:14,879 --> 00:03:20,560
there are there are bigger and better

00:03:18,640 --> 00:03:23,920
things out there as well

00:03:20,560 --> 00:03:27,440
and they outlined in this paper um

00:03:23,920 --> 00:03:29,040
four main points about borgmon that

00:03:27,440 --> 00:03:31,680
really applied to prometheus

00:03:29,040 --> 00:03:33,440
and i'm reading this paper and thinking

00:03:31,680 --> 00:03:33,760
this is exactly what i'm trying to get

00:03:33,440 --> 00:03:37,200
at

00:03:33,760 --> 00:03:39,280
in my presentation for today the first

00:03:37,200 --> 00:03:41,680
big point in the monarch paper was

00:03:39,280 --> 00:03:42,799
prometheuses or borgmon's distributed

00:03:41,680 --> 00:03:44,319
nature

00:03:42,799 --> 00:03:47,280
the fact that you end up with a

00:03:44,319 --> 00:03:50,480
prometheus per application team

00:03:47,280 --> 00:03:53,040
so you end up with people on each team

00:03:50,480 --> 00:03:53,519
have to be a prometheus and monitoring

00:03:53,040 --> 00:03:56,400
and

00:03:53,519 --> 00:03:57,760
statistics expert to get the best out of

00:03:56,400 --> 00:04:00,879
it

00:03:57,760 --> 00:04:04,480
secondly borgmon and prometheus

00:04:00,879 --> 00:04:06,879
don't have any sort of of hard schema

00:04:04,480 --> 00:04:08,239
so now that you have each team their own

00:04:06,879 --> 00:04:10,959
prometheus

00:04:08,239 --> 00:04:13,200
each set of labels each team's

00:04:10,959 --> 00:04:16,479
implementation of metric names

00:04:13,200 --> 00:04:17,919
varies sometimes in subtle ways and when

00:04:16,479 --> 00:04:20,400
you get to a point where

00:04:17,919 --> 00:04:22,400
one team needs to ask a analytical

00:04:20,400 --> 00:04:24,479
question from another team

00:04:22,400 --> 00:04:25,600
their prometheus doesn't really speak

00:04:24,479 --> 00:04:27,360
the same language

00:04:25,600 --> 00:04:29,680
it's difficult for teams to work

00:04:27,360 --> 00:04:32,479
together

00:04:29,680 --> 00:04:34,479
the third big point in the monarch paper

00:04:32,479 --> 00:04:36,960
was that borgmon had a

00:04:34,479 --> 00:04:38,400
lack of support for generating

00:04:36,960 --> 00:04:41,440
statistical models

00:04:38,400 --> 00:04:44,400
dealing with timers and

00:04:41,440 --> 00:04:45,199
size objects um we solve this with

00:04:44,400 --> 00:04:47,680
prometheus

00:04:45,199 --> 00:04:49,600
with the histogram data type but

00:04:47,680 --> 00:04:51,680
prometheus's histograms

00:04:49,600 --> 00:04:53,680
leave much to be desired so i'm going to

00:04:51,680 --> 00:04:55,040
go over some a cheater methods how to

00:04:53,680 --> 00:04:56,880
work around some of the shortcomings

00:04:55,040 --> 00:04:59,360
there

00:04:56,880 --> 00:05:02,000
finally the fourth jack jared are your

00:04:59,360 --> 00:05:05,039
slides moving

00:05:02,000 --> 00:05:05,039
not yet okay

00:05:05,120 --> 00:05:08,720
um the fourth point in the monarch paper

00:05:07,360 --> 00:05:11,680
is about sharding

00:05:08,720 --> 00:05:12,080
which happens to prometheus as well that

00:05:11,680 --> 00:05:14,320
is

00:05:12,080 --> 00:05:17,280
truly another presentation so we'll save

00:05:14,320 --> 00:05:17,280
that for a later day

00:05:18,000 --> 00:05:22,639
but changing slides now what we're going

00:05:21,280 --> 00:05:25,280
to talk about today

00:05:22,639 --> 00:05:27,520
is how to build some methods of common

00:05:25,280 --> 00:05:30,800
instrumentation how do we handle

00:05:27,520 --> 00:05:31,360
the lack of schema we're going to talk

00:05:30,800 --> 00:05:33,840
about

00:05:31,360 --> 00:05:35,919
uh doing service level objectives

00:05:33,840 --> 00:05:37,199
setting those goals using prometheus to

00:05:35,919 --> 00:05:40,000
monitor them

00:05:37,199 --> 00:05:41,919
and how to get some more accurate data

00:05:40,000 --> 00:05:44,880
and what to look at

00:05:41,919 --> 00:05:45,759
when using prometheus for those sre

00:05:44,880 --> 00:05:49,039
kinds of

00:05:45,759 --> 00:05:50,800
of goal setting and measurement

00:05:49,039 --> 00:05:52,080
and finally on our journey we're going

00:05:50,800 --> 00:05:54,479
to talk about how do we do

00:05:52,080 --> 00:05:56,400
effective alerting in sort of that site

00:05:54,479 --> 00:05:58,080
reliability mentality

00:05:56,400 --> 00:06:00,639
building on top of our service level

00:05:58,080 --> 00:06:00,639
objectives

00:06:03,440 --> 00:06:06,720
so that's kind of my introduction about

00:06:05,360 --> 00:06:10,160
where i am and where i've come

00:06:06,720 --> 00:06:11,600
from um i've wanted to talk about this

00:06:10,160 --> 00:06:14,479
for a long time

00:06:11,600 --> 00:06:16,479
there are the initial problems with

00:06:14,479 --> 00:06:18,840
working with prometheus with scaling

00:06:16,479 --> 00:06:20,880
prometheus is really kind of a social

00:06:18,840 --> 00:06:23,039
problem it's

00:06:20,880 --> 00:06:25,199
the fact that we need to build some sort

00:06:23,039 --> 00:06:27,440
of common understanding that we can use

00:06:25,199 --> 00:06:30,160
in place of a schema

00:06:27,440 --> 00:06:30,720
so the best way i found to go about this

00:06:30,160 --> 00:06:32,319
is

00:06:30,720 --> 00:06:34,240
bring your teams together bring your

00:06:32,319 --> 00:06:35,520
developers your visibility teams your

00:06:34,240 --> 00:06:38,800
sre teams

00:06:35,520 --> 00:06:39,680
maybe just one or two teams but have a

00:06:38,800 --> 00:06:42,720
plan

00:06:39,680 --> 00:06:45,199
for how you're going to use prometheus

00:06:42,720 --> 00:06:47,039
better yet and especially if you're

00:06:45,199 --> 00:06:49,919
working with microservices

00:06:47,039 --> 00:06:52,160
build some sort of a kit that you can

00:06:49,919 --> 00:06:54,000
use as a starting point for each new

00:06:52,160 --> 00:06:57,199
microservice or each time you write

00:06:54,000 --> 00:06:57,599
a new script or some new service and

00:06:57,199 --> 00:07:01,039
that

00:06:57,599 --> 00:07:03,199
kit has prometheus the the prometheus

00:07:01,039 --> 00:07:05,599
client library already integrated

00:07:03,199 --> 00:07:06,800
some key standard metrics that you know

00:07:05,599 --> 00:07:11,120
you're going to use

00:07:06,800 --> 00:07:13,680
to instrument http api calls for example

00:07:11,120 --> 00:07:17,840
and some good examples already coded in

00:07:13,680 --> 00:07:17,840
so you're starting on the ground running

00:07:18,080 --> 00:07:24,479
create and encourage a consistent use

00:07:21,520 --> 00:07:25,440
of metric names as well you can

00:07:24,479 --> 00:07:27,120
definitely start

00:07:25,440 --> 00:07:29,120
with the prometheus naming best

00:07:27,120 --> 00:07:30,800
practices as a guide here

00:07:29,120 --> 00:07:32,479
there's tons of information there about

00:07:30,800 --> 00:07:34,479
how to form uh

00:07:32,479 --> 00:07:36,800
best practices around naming metrics

00:07:34,479 --> 00:07:38,080
naming labels

00:07:36,800 --> 00:07:40,240
and one thing you definitely want to

00:07:38,080 --> 00:07:43,599
look at is how do you

00:07:40,240 --> 00:07:44,720
label metrics consistently so you have

00:07:43,599 --> 00:07:46,240
that schema of

00:07:44,720 --> 00:07:48,720
you know which team generated this

00:07:46,240 --> 00:07:53,840
metric you know where it came from

00:07:48,720 --> 00:07:56,720
which kubernetes cluster which pod etc

00:07:53,840 --> 00:07:58,800
ideally you want to be able to run prom

00:07:56,720 --> 00:08:01,599
ql queries like we have

00:07:58,800 --> 00:08:02,160
on the screen if most of what you have

00:08:01,599 --> 00:08:05,360
if

00:08:02,160 --> 00:08:09,280
what you're working with is http

00:08:05,360 --> 00:08:12,319
requests http apis then there should be

00:08:09,280 --> 00:08:15,520
one simple query that can generate the

00:08:12,319 --> 00:08:16,479
traffic rate of all services in your

00:08:15,520 --> 00:08:18,960
fleet

00:08:16,479 --> 00:08:20,560
and you can use that as you know a top

00:08:18,960 --> 00:08:22,800
level grafana dashboard

00:08:20,560 --> 00:08:24,400
that you can drill down from and since

00:08:22,800 --> 00:08:27,840
we're summing my job

00:08:24,400 --> 00:08:30,639
we can see traffic rate per service

00:08:27,840 --> 00:08:32,479
together on the same graph and that's a

00:08:30,639 --> 00:08:34,640
really powerful concept

00:08:32,479 --> 00:08:37,680
that because of prometheus's distributed

00:08:34,640 --> 00:08:40,240
nature sometimes you miss out on

00:08:37,680 --> 00:08:42,080
don't think of this as you know a social

00:08:40,240 --> 00:08:44,480
burden to be fixed

00:08:42,080 --> 00:08:46,959
this is one of the opportunities we have

00:08:44,480 --> 00:08:49,279
as site reliability engineers

00:08:46,959 --> 00:08:51,519
to help encourage a culture of well

00:08:49,279 --> 00:08:54,000
devops frankly

00:08:51,519 --> 00:08:55,839
having a kit having this schema breaks

00:08:54,000 --> 00:08:57,360
down silos that naturally build up

00:08:55,839 --> 00:09:00,640
between teams

00:08:57,360 --> 00:09:02,080
and enables us to change teams to look

00:09:00,640 --> 00:09:05,279
at each team

00:09:02,080 --> 00:09:06,080
for managers and c-level folks to look

00:09:05,279 --> 00:09:09,120
at each team

00:09:06,080 --> 00:09:11,760
understand what does health mean

00:09:09,120 --> 00:09:13,600
and so that really helps build that

00:09:11,760 --> 00:09:16,240
devops culture that we're all sort of

00:09:13,600 --> 00:09:16,240
striving for

00:09:16,560 --> 00:09:21,200
when you're building your kit and and

00:09:18,800 --> 00:09:23,360
making some of these decisions

00:09:21,200 --> 00:09:25,519
use one of these standard methods to to

00:09:23,360 --> 00:09:28,480
inform some of your decisions

00:09:25,519 --> 00:09:31,279
uh brendan gregg started this concept in

00:09:28,480 --> 00:09:32,880
2012 with the use method

00:09:31,279 --> 00:09:35,519
that focuses more toward hardware and

00:09:32,880 --> 00:09:38,480
resources i've done a bunch more work

00:09:35,519 --> 00:09:39,120
in the application space so the red

00:09:38,480 --> 00:09:42,160
method

00:09:39,120 --> 00:09:44,000
makes more sense to me the red method

00:09:42,160 --> 00:09:45,600
tom wilkie started blogging about this

00:09:44,000 --> 00:09:48,800
in about 2015

00:09:45,600 --> 00:09:52,240
so not too many years ago aimed at

00:09:48,800 --> 00:09:53,839
services apis like a microservice style

00:09:52,240 --> 00:09:57,200
architecture

00:09:53,839 --> 00:10:00,560
red stands for the traffic rate

00:09:57,200 --> 00:10:04,560
the error rate and the duration

00:10:00,560 --> 00:10:06,160
of those api calls

00:10:04,560 --> 00:10:08,079
clearly when google was building up

00:10:06,160 --> 00:10:10,480
their internal sre teams

00:10:08,079 --> 00:10:11,680
they were thinking along the same lines

00:10:10,480 --> 00:10:13,839
um

00:10:11,680 --> 00:10:15,040
waldo did an earlier talk yesterday

00:10:13,839 --> 00:10:17,200
about

00:10:15,040 --> 00:10:19,839
some great tenants of sre and he talked

00:10:17,200 --> 00:10:21,600
about this as well

00:10:19,839 --> 00:10:24,079
the google site reliability engineering

00:10:21,600 --> 00:10:25,839
book came out in 2016

00:10:24,079 --> 00:10:28,240
and that introduced us to some of the

00:10:25,839 --> 00:10:30,320
concepts that google is using internally

00:10:28,240 --> 00:10:32,160
and one of these concepts is called the

00:10:30,320 --> 00:10:36,240
four golden signals

00:10:32,160 --> 00:10:39,680
which stands for traffic latency

00:10:36,240 --> 00:10:42,560
errors and saturation so four

00:10:39,680 --> 00:10:44,959
common signals that you want to measure

00:10:42,560 --> 00:10:46,880
from each and every application

00:10:44,959 --> 00:10:50,640
and basically it's the red method plus

00:10:46,880 --> 00:10:50,640
saturation from the use method

00:10:54,000 --> 00:11:01,279
okay so once you've identified a method

00:10:56,880 --> 00:11:03,920
that's part of your kit to use

00:11:01,279 --> 00:11:06,320
some templating and automation start to

00:11:03,920 --> 00:11:08,480
naturally fall out of this

00:11:06,320 --> 00:11:09,680
the idea is that all of your

00:11:08,480 --> 00:11:12,480
applications

00:11:09,680 --> 00:11:14,320
export some common prometheus metrics

00:11:12,480 --> 00:11:14,959
that are named the same that follow the

00:11:14,320 --> 00:11:18,000
same

00:11:14,959 --> 00:11:20,800
naming scheme or schema that

00:11:18,000 --> 00:11:21,440
outs that underscore the latency the

00:11:20,800 --> 00:11:24,240
traffic

00:11:21,440 --> 00:11:25,920
the errors and the saturation if you're

00:11:24,240 --> 00:11:27,200
working with a third-party application

00:11:25,920 --> 00:11:29,600
where you have less control

00:11:27,200 --> 00:11:31,360
over the instrumentation maybe this is a

00:11:29,600 --> 00:11:35,120
layer of recording rules

00:11:31,360 --> 00:11:35,120
on top of the existing instrumentation

00:11:35,279 --> 00:11:42,560
but a traffic or a latency signal

00:11:39,440 --> 00:11:46,399
immediately can become a performance

00:11:42,560 --> 00:11:48,480
slo using traffic and errors

00:11:46,399 --> 00:11:49,440
helps us engineers identify you know

00:11:48,480 --> 00:11:52,399
where problems

00:11:49,440 --> 00:11:54,639
lie when they arise but you can combine

00:11:52,399 --> 00:11:58,079
those signals into a ratio

00:11:54,639 --> 00:12:02,079
which gives you availability or up time

00:11:58,079 --> 00:12:04,240
which you can also measure as an slo

00:12:02,079 --> 00:12:05,279
saturation is always the hardest signal

00:12:04,240 --> 00:12:07,279
to deal with

00:12:05,279 --> 00:12:08,639
in fact a lot of people use the red

00:12:07,279 --> 00:12:10,160
method

00:12:08,639 --> 00:12:11,839
because saturation can be more

00:12:10,160 --> 00:12:14,800
challenging to deal with

00:12:11,839 --> 00:12:15,600
saturation indicates how full your

00:12:14,800 --> 00:12:18,560
service is

00:12:15,600 --> 00:12:19,440
at present and obviously has uh

00:12:18,560 --> 00:12:22,959
indications for

00:12:19,440 --> 00:12:25,519
capacity planning as we you know look at

00:12:22,959 --> 00:12:26,800
the upcoming holidays how how many

00:12:25,519 --> 00:12:29,040
devices have we sold

00:12:26,800 --> 00:12:31,200
how much do we think we're going to grow

00:12:29,040 --> 00:12:33,440
what resources do we need to spin up to

00:12:31,200 --> 00:12:39,519
increase our fleet to be able to handle

00:12:33,440 --> 00:12:42,399
the incoming demand

00:12:39,519 --> 00:12:43,200
so that leads me to kind of a preferred

00:12:42,399 --> 00:12:46,399
workflow

00:12:43,200 --> 00:12:49,680
for working with prometheus and grafana

00:12:46,399 --> 00:12:52,399
and the goal here is to reduce the

00:12:49,680 --> 00:12:52,800
cognitive burden of each team having to

00:12:52,399 --> 00:12:56,800
be an

00:12:52,800 --> 00:12:59,600
expert in in telemetry and statistics

00:12:56,800 --> 00:13:00,320
to run their own monitoring and what i

00:12:59,600 --> 00:13:03,680
found

00:13:00,320 --> 00:13:04,959
works best is working through an sre

00:13:03,680 --> 00:13:07,600
abstraction layer

00:13:04,959 --> 00:13:09,040
where we're thinking in terms of our

00:13:07,600 --> 00:13:13,040
service level indicators

00:13:09,040 --> 00:13:16,720
those are the signals in our method

00:13:13,040 --> 00:13:20,399
setting goals for them to build slos

00:13:16,720 --> 00:13:21,519
and using slack ops git ops or some sort

00:13:20,399 --> 00:13:23,760
of cli tool

00:13:21,519 --> 00:13:24,720
that perhaps uploads some some yammel

00:13:23,760 --> 00:13:27,680
somewhere

00:13:24,720 --> 00:13:28,480
that expresses you know i want traffic

00:13:27,680 --> 00:13:32,639
to be

00:13:28,480 --> 00:13:34,480
two seconds or less for 95 of the time

00:13:32,639 --> 00:13:37,120
and that little bit of information goes

00:13:34,480 --> 00:13:38,000
into a a code generation or a templating

00:13:37,120 --> 00:13:40,320
engine

00:13:38,000 --> 00:13:41,279
that uses your recording rule libraries

00:13:40,320 --> 00:13:44,880
that you've built

00:13:41,279 --> 00:13:48,240
and renders out prometheus and grafana

00:13:44,880 --> 00:13:50,320
dashboards alerts

00:13:48,240 --> 00:13:52,160
and other things you use to monitor your

00:13:50,320 --> 00:13:52,880
application even custom dashboards as

00:13:52,160 --> 00:13:55,440
well

00:13:52,880 --> 00:13:57,279
you stick those in some sort of of api

00:13:55,440 --> 00:14:00,560
accessible place like

00:13:57,279 --> 00:14:03,760
s3 maybe gcs if you're

00:14:00,560 --> 00:14:05,279
using the google cloud and

00:14:03,760 --> 00:14:07,680
the storage objects generate a

00:14:05,279 --> 00:14:09,120
notification prometheus receives the

00:14:07,680 --> 00:14:10,399
notification pulls down its new

00:14:09,120 --> 00:14:12,320
configuration

00:14:10,399 --> 00:14:14,480
and within a couple seconds you are

00:14:12,320 --> 00:14:17,440
working with a brand new configuration

00:14:14,480 --> 00:14:18,959
based on the changes in your goals that

00:14:17,440 --> 00:14:20,639
that you just submitted a few minutes

00:14:18,959 --> 00:14:22,639
ago

00:14:20,639 --> 00:14:24,320
so to me this abstraction layer works

00:14:22,639 --> 00:14:26,480
really well to offload some of that

00:14:24,320 --> 00:14:28,839
cognitive burden

00:14:26,480 --> 00:14:30,240
to encourage us to use schema and name

00:14:28,839 --> 00:14:33,760
spaces

00:14:30,240 --> 00:14:35,760
and recording rule libraries as well

00:14:33,760 --> 00:14:37,839
sort of in the kubernetes sense the

00:14:35,760 --> 00:14:40,480
templating layer you think of this is

00:14:37,839 --> 00:14:41,839
as a kubernetes control plane tool

00:14:40,480 --> 00:14:44,079
object

00:14:41,839 --> 00:14:46,000
that reacts to state changes that it

00:14:44,079 --> 00:14:48,720
monitors externally

00:14:46,000 --> 00:14:51,440
it sees a state change it pushes updated

00:14:48,720 --> 00:14:53,760
information to your prometheus operator

00:14:51,440 --> 00:14:55,360
which makes configuration changes to

00:14:53,760 --> 00:14:59,839
your prometheus

00:14:55,360 --> 00:14:59,839
pods running in production

00:15:03,199 --> 00:15:07,680
so let's look a little more closely at

00:15:05,440 --> 00:15:13,040
working with service level objectives

00:15:07,680 --> 00:15:13,519
and prometheus so if you're familiar

00:15:13,040 --> 00:15:16,320
with

00:15:13,519 --> 00:15:18,079
uh site reliability engineering and

00:15:16,320 --> 00:15:20,000
doing service level objectives this

00:15:18,079 --> 00:15:22,160
comes as new surprise

00:15:20,000 --> 00:15:23,839
a service level objective is about four

00:15:22,160 --> 00:15:26,320
things you have your

00:15:23,839 --> 00:15:28,560
service level indicator usually you know

00:15:26,320 --> 00:15:32,079
a signal from your method

00:15:28,560 --> 00:15:35,279
you have a threshold or you know what

00:15:32,079 --> 00:15:36,880
what target are you aiming for um this

00:15:35,279 --> 00:15:39,120
can be one of two classes

00:15:36,880 --> 00:15:40,079
if you're dealing with latency or sizes

00:15:39,120 --> 00:15:42,320
you have

00:15:40,079 --> 00:15:44,639
a static value like you want latencies

00:15:42,320 --> 00:15:46,959
to be 2 seconds or under

00:15:44,639 --> 00:15:48,880
in some cases you can use a boolean

00:15:46,959 --> 00:15:51,199
value here which simplifies some of the

00:15:48,880 --> 00:15:51,199
math

00:15:51,519 --> 00:15:54,880
when folks start with service cell

00:15:53,519 --> 00:15:57,680
service level objectives

00:15:54,880 --> 00:15:59,920
i tell them to you start with a 95

00:15:57,680 --> 00:16:02,079
percent availability goal

00:15:59,920 --> 00:16:04,240
that probably doesn't represent where

00:16:02,079 --> 00:16:06,959
you want to be long-term

00:16:04,240 --> 00:16:07,360
but it's a great starting place it lets

00:16:06,959 --> 00:16:10,639
you

00:16:07,360 --> 00:16:11,199
model out the slos and really see where

00:16:10,639 --> 00:16:15,360
you are

00:16:11,199 --> 00:16:18,079
today uh compared to you know a

00:16:15,360 --> 00:16:19,199
first iteration slo and of course we can

00:16:18,079 --> 00:16:23,199
iterate on this

00:16:19,199 --> 00:16:25,759
you know as we uh tune our inputs

00:16:23,199 --> 00:16:27,440
and finally the time window that's

00:16:25,759 --> 00:16:28,079
associated with a service level

00:16:27,440 --> 00:16:30,240
objective

00:16:28,079 --> 00:16:32,959
is really critically important and

00:16:30,240 --> 00:16:35,279
sometimes overlooked

00:16:32,959 --> 00:16:36,880
more formal slo seem to be in the 30 day

00:16:35,279 --> 00:16:39,199
range i've definitely worked with

00:16:36,880 --> 00:16:42,320
shorter slos as well

00:16:39,199 --> 00:16:44,959
but that time definition

00:16:42,320 --> 00:16:46,880
means that another person on your team

00:16:44,959 --> 00:16:48,959
can reproduce your slo math

00:16:46,880 --> 00:16:50,320
or perhaps your customers can reproduce

00:16:48,959 --> 00:16:53,440
your slo math

00:16:50,320 --> 00:16:56,079
and we can have peer verification

00:16:53,440 --> 00:16:57,839
that you know we are in fact meeting our

00:16:56,079 --> 00:17:00,560
service level objective goals

00:16:57,839 --> 00:17:01,360
so that's an important part you'll see

00:17:00,560 --> 00:17:03,279
slos

00:17:01,360 --> 00:17:04,640
phrased sort of like some of these

00:17:03,279 --> 00:17:08,400
examples

00:17:04,640 --> 00:17:09,520
like a service will return an http api

00:17:08,400 --> 00:17:12,079
result

00:17:09,520 --> 00:17:13,280
within two seconds or less for 95

00:17:12,079 --> 00:17:15,839
percent of requests

00:17:13,280 --> 00:17:19,120
over the trailing 30 days or perhaps

00:17:15,839 --> 00:17:21,600
over the calendar month

00:17:19,120 --> 00:17:23,039
and when you can articulate slos in that

00:17:21,600 --> 00:17:25,439
fashion

00:17:23,039 --> 00:17:28,079
they fit really nicely into our

00:17:25,439 --> 00:17:30,559
telemetry systems like prometheus

00:17:28,079 --> 00:17:34,160
and they work through our templating

00:17:30,559 --> 00:17:34,160
engine to help make some of this easier

00:17:34,559 --> 00:17:40,559
looking at some example here

00:17:37,600 --> 00:17:41,520
this is an availability ratio so this is

00:17:40,559 --> 00:17:44,640
a boolean

00:17:41,520 --> 00:17:46,080
state either the http requests responded

00:17:44,640 --> 00:17:48,480
in a healthy manner

00:17:46,080 --> 00:17:50,480
or we responded in an error and the

00:17:48,480 --> 00:17:53,679
customer's angry

00:17:50,480 --> 00:17:56,480
so since this is a boolean we can

00:17:53,679 --> 00:18:00,960
simplify this to a ratio

00:17:56,480 --> 00:18:04,960
of requests that were that were healthy

00:18:00,960 --> 00:18:07,600
over the total number of requests

00:18:04,960 --> 00:18:09,520
and you can see here we are normalizing

00:18:07,600 --> 00:18:12,160
uh per second but we're looking back

00:18:09,520 --> 00:18:13,520
30 days so we build that 30 day slo

00:18:12,160 --> 00:18:16,320
window

00:18:13,520 --> 00:18:18,160
we're summing by job which means that

00:18:16,320 --> 00:18:20,640
any service that

00:18:18,160 --> 00:18:21,919
exports this metric and conforms to our

00:18:20,640 --> 00:18:25,039
naming scheme

00:18:21,919 --> 00:18:28,320
this rule will work with and we build

00:18:25,039 --> 00:18:28,880
a recording rule that stores the

00:18:28,320 --> 00:18:31,919
available

00:18:28,880 --> 00:18:34,240
availability ratio for the rolling 30

00:18:31,919 --> 00:18:34,240
days

00:18:34,720 --> 00:18:38,799
we can take the results of that

00:18:36,160 --> 00:18:39,679
recording rule stick that into an alert

00:18:38,799 --> 00:18:43,200
rule

00:18:39,679 --> 00:18:45,840
and simply ask the question is that

00:18:43,200 --> 00:18:47,840
availability ratio for the past 30 days

00:18:45,840 --> 00:18:52,960
ever less than 0.95

00:18:47,840 --> 00:18:52,960
or 95 that means we've broken our slo

00:18:55,520 --> 00:19:01,200
the next example uh is latency

00:18:59,039 --> 00:19:02,160
so we have that value of we want to be

00:19:01,200 --> 00:19:05,840
two seconds

00:19:02,160 --> 00:19:09,039
or under for 95 percent of

00:19:05,840 --> 00:19:10,240
of our requests so this is pretty

00:19:09,039 --> 00:19:12,400
similar we'll

00:19:10,240 --> 00:19:13,679
we are building a recording rule that

00:19:12,400 --> 00:19:16,799
stores the

00:19:13,679 --> 00:19:19,919
95th percentile of all requests

00:19:16,799 --> 00:19:21,200
over the last 30 days so that is the

00:19:19,919 --> 00:19:25,120
duration that

00:19:21,200 --> 00:19:28,400
95 of requests are either equal to

00:19:25,120 --> 00:19:31,360
or less than um so

00:19:28,400 --> 00:19:32,160
if this was two seconds we know that 95

00:19:31,360 --> 00:19:34,640
percent

00:19:32,160 --> 00:19:36,960
of our requests have been served in two

00:19:34,640 --> 00:19:39,280
seconds or less

00:19:36,960 --> 00:19:40,080
but again since we're of dealing with

00:19:39,280 --> 00:19:42,559
latencies

00:19:40,080 --> 00:19:44,400
we store this information in a histogram

00:19:42,559 --> 00:19:45,840
so we use prometheus's histogram

00:19:44,400 --> 00:19:49,120
quantile function to do the

00:19:45,840 --> 00:19:52,240
the percentile estimation again

00:19:49,120 --> 00:19:54,799
we're summing by job so importantly we

00:19:52,240 --> 00:19:55,120
can reuse this rule for any application

00:19:54,799 --> 00:19:59,600
that

00:19:55,120 --> 00:19:59,600
exports signals in these common formats

00:19:59,679 --> 00:20:05,360
once we've got our recording rule set up

00:20:02,320 --> 00:20:10,080
we can put the output into an alert rule

00:20:05,360 --> 00:20:13,039
and we simply ask the question is our

00:20:10,080 --> 00:20:13,760
95th percentile duration if the last 30

00:20:13,039 --> 00:20:16,960
days

00:20:13,760 --> 00:20:17,520
ever greater than 2 or 2 seconds if it

00:20:16,960 --> 00:20:21,840
is

00:20:17,520 --> 00:20:21,840
we know we have a problem with our slo

00:20:22,000 --> 00:20:26,880
but wait histograms prometheus

00:20:25,039 --> 00:20:29,679
i keep indicating that that they're

00:20:26,880 --> 00:20:31,120
somehow problematic

00:20:29,679 --> 00:20:35,120
so let's look a little closer at

00:20:31,120 --> 00:20:36,880
prometheus's histograms

00:20:35,120 --> 00:20:38,400
when i was trying to understand this

00:20:36,880 --> 00:20:40,640
myself

00:20:38,400 --> 00:20:41,840
i took a data set from my old graphite

00:20:40,640 --> 00:20:44,960
cluster

00:20:41,840 --> 00:20:48,480
in just a data set of of query

00:20:44,960 --> 00:20:51,520
latencies for 10 000 queries

00:20:48,480 --> 00:20:53,360
i threw this into r and modeled some

00:20:51,520 --> 00:20:56,559
histograms around it

00:20:53,360 --> 00:21:00,400
and this these are the outputs um so

00:20:56,559 --> 00:21:03,200
in this case most of my uh

00:21:00,400 --> 00:21:04,960
graphite responses happen in really

00:21:03,200 --> 00:21:07,520
close to zero seconds

00:21:04,960 --> 00:21:09,120
i've got buckets every 0.2 seconds so

00:21:07,520 --> 00:21:13,039
five buckets

00:21:09,120 --> 00:21:15,440
uh per second and clearly from the graph

00:21:13,039 --> 00:21:17,360
a large majority of them happen you know

00:21:15,440 --> 00:21:19,039
really quite quickly so this looks good

00:21:17,360 --> 00:21:22,480
right

00:21:19,039 --> 00:21:25,760
well my tail is really long in fact my

00:21:22,480 --> 00:21:27,760
tail extends out to 29 seconds

00:21:25,760 --> 00:21:30,159
and i've actually cut off the graph so

00:21:27,760 --> 00:21:33,280
it fits on the screen

00:21:30,159 --> 00:21:35,760
i've measured for the averages

00:21:33,280 --> 00:21:36,640
at 0.29 i've measured the standard

00:21:35,760 --> 00:21:40,960
deviation

00:21:36,640 --> 00:21:42,960
at 0.733 so i know already

00:21:40,960 --> 00:21:44,880
that since my data doesn't lie within

00:21:42,960 --> 00:21:45,760
three or four standard deviations of the

00:21:44,880 --> 00:21:49,120
mean

00:21:45,760 --> 00:21:51,440
that this isn't a normal distribution

00:21:49,120 --> 00:21:53,919
and this is exactly why we don't use

00:21:51,440 --> 00:21:56,159
averages to look at latency information

00:21:53,919 --> 00:21:58,000
because averages are meant to work with

00:21:56,159 --> 00:22:01,440
normal distributions

00:21:58,000 --> 00:22:02,000
and latencies download sizes rarely

00:22:01,440 --> 00:22:03,360
rarely

00:22:02,000 --> 00:22:06,080
are in fact they're usually gamma

00:22:03,360 --> 00:22:07,919
distributions

00:22:06,080 --> 00:22:09,600
i've used r to sort of plot out where

00:22:07,919 --> 00:22:13,840
the 95th percentile

00:22:09,600 --> 00:22:13,840
is as sort of my reference value

00:22:14,880 --> 00:22:18,640
so if prometheus were to have a

00:22:16,640 --> 00:22:22,080
histogram like this

00:22:18,640 --> 00:22:23,840
and what does it look like and so you're

00:22:22,080 --> 00:22:26,720
thinking okay i can make buckets every

00:22:23,840 --> 00:22:29,280
0.2 seconds and

00:22:26,720 --> 00:22:29,760
feed that information into prometheus

00:22:29,280 --> 00:22:31,840
and

00:22:29,760 --> 00:22:33,440
ask prometheus to generate the 95th

00:22:31,840 --> 00:22:35,360
percentile

00:22:33,440 --> 00:22:37,440
they're 10 000 samples so we're going to

00:22:35,360 --> 00:22:40,720
use the value of the 9500

00:22:37,440 --> 00:22:42,799
first we know provably that that sample

00:22:40,720 --> 00:22:44,480
value is in the eighth bucket

00:22:42,799 --> 00:22:46,640
and prometheus uses some linear

00:22:44,480 --> 00:22:50,320
approximation across that bucket

00:22:46,640 --> 00:22:52,559
to fine-tune where that where exactly

00:22:50,320 --> 00:22:55,039
uh that sample would be in the bucket to

00:22:52,559 --> 00:22:59,440
give us our estimate

00:22:55,039 --> 00:23:02,320
but again this histogram extends out to

00:22:59,440 --> 00:23:04,720
29 seconds if i were smart enough to

00:23:02,320 --> 00:23:08,000
know to stop at 29 seconds

00:23:04,720 --> 00:23:11,039
that means there's 144 buckets here

00:23:08,000 --> 00:23:12,480
and this is an r model i've built on top

00:23:11,039 --> 00:23:14,320
of the data i had

00:23:12,480 --> 00:23:15,760
when we're working with prometheus we

00:23:14,320 --> 00:23:16,960
build our instrumentation and our

00:23:15,760 --> 00:23:23,280
histograms

00:23:16,960 --> 00:23:26,880
before we've seen the data

00:23:23,280 --> 00:23:28,080
so that leads us to histograms inside

00:23:26,880 --> 00:23:30,480
prometheus

00:23:28,080 --> 00:23:31,520
that kind of look like this when we

00:23:30,480 --> 00:23:34,480
build histogram

00:23:31,520 --> 00:23:35,440
implement instrumentation in our code we

00:23:34,480 --> 00:23:38,960
specify

00:23:35,440 --> 00:23:39,919
a bucketing scheme to use and we might

00:23:38,960 --> 00:23:43,120
use something like

00:23:39,919 --> 00:23:46,400
0.05 0.1 0.5

00:23:43,120 --> 00:23:48,960
1 5 some sort of of numerical pattern

00:23:46,400 --> 00:23:52,240
there those are super common

00:23:48,960 --> 00:23:55,679
what this ends up doing especially as us

00:23:52,240 --> 00:23:57,919
humans you know code this in by hand is

00:23:55,679 --> 00:24:00,000
we have more buckets where we think a

00:23:57,919 --> 00:24:02,559
majority of the data will be

00:24:00,000 --> 00:24:03,760
and fewer wider buckets where the tails

00:24:02,559 --> 00:24:06,640
will be

00:24:03,760 --> 00:24:08,640
but it's the tails that is where the

00:24:06,640 --> 00:24:10,320
important data is that's

00:24:08,640 --> 00:24:12,240
that's the behavior we want to analyze

00:24:10,320 --> 00:24:15,279
and understand

00:24:12,240 --> 00:24:17,760
so understanding the tail is really

00:24:15,279 --> 00:24:20,720
quite frankly more important than where

00:24:17,760 --> 00:24:24,159
the majority of our data is

00:24:20,720 --> 00:24:27,200
so it's not uncommon to find

00:24:24,159 --> 00:24:28,400
errors in prometheus's percentile

00:24:27,200 --> 00:24:32,159
estimation

00:24:28,400 --> 00:24:36,640
of 200 or 300 percent and

00:24:32,159 --> 00:24:36,640
it's not cool that's not cool at all

00:24:38,159 --> 00:24:41,440
so what would i like to see i'd really

00:24:40,720 --> 00:24:44,480
like to see

00:24:41,440 --> 00:24:47,279
something like a logarithmic bucketing

00:24:44,480 --> 00:24:48,240
algorithm in prometheus this doesn't

00:24:47,279 --> 00:24:50,880
exist

00:24:48,240 --> 00:24:52,960
but if we had an algorithm that

00:24:50,880 --> 00:24:54,720
generated our buckets for us

00:24:52,960 --> 00:24:57,200
our buckets would be finer therefore

00:24:54,720 --> 00:24:58,720
increasing our accuracy

00:24:57,200 --> 00:25:00,480
all of our histograms would have the

00:24:58,720 --> 00:25:01,919
same bucketing scheme

00:25:00,480 --> 00:25:04,159
and all of our histograms would be

00:25:01,919 --> 00:25:05,520
aggregatable because if the bucketing

00:25:04,159 --> 00:25:09,279
varies they're not very

00:25:05,520 --> 00:25:11,919
aggregatable and just looking at that

00:25:09,279 --> 00:25:14,320
same data through a logarithmic approach

00:25:11,919 --> 00:25:16,880
and there's so much more story there you

00:25:14,320 --> 00:25:18,240
can tell the data is multimodal

00:25:16,880 --> 00:25:20,159
there's obviously three different

00:25:18,240 --> 00:25:20,799
classes of queries that are happening

00:25:20,159 --> 00:25:22,880
here

00:25:20,799 --> 00:25:25,039
in fact the 95th percentile outlines

00:25:22,880 --> 00:25:26,720
this entire query class

00:25:25,039 --> 00:25:29,520
that i probably want to dig into and

00:25:26,720 --> 00:25:31,200
figure out what's going on there

00:25:29,520 --> 00:25:34,960
this is the power of histograms that

00:25:31,200 --> 00:25:34,960
prometheus kind of misses the boat on

00:25:35,840 --> 00:25:40,559
so how do we work around this well i

00:25:38,640 --> 00:25:41,760
suggest we cheat

00:25:40,559 --> 00:25:44,080
if you're not cheating you're not trying

00:25:41,760 --> 00:25:45,840
hard enough right if you read the

00:25:44,080 --> 00:25:49,120
prometheus best practices

00:25:45,840 --> 00:25:53,039
they suggest you use your slo threshold

00:25:49,120 --> 00:25:55,279
as a bucket boundary in your histogram

00:25:53,039 --> 00:25:56,480
so your application already knows that

00:25:55,279 --> 00:25:58,880
so i suggest

00:25:56,480 --> 00:26:00,559
go ahead and build a metric around that

00:25:58,880 --> 00:26:02,640
so you know what it is and you can track

00:26:00,559 --> 00:26:05,679
it when it changes

00:26:02,640 --> 00:26:07,600
instead of a histogram use these three

00:26:05,679 --> 00:26:10,720
counters

00:26:07,600 --> 00:26:11,919
count the total number of requests count

00:26:10,720 --> 00:26:12,559
the total number of requests that

00:26:11,919 --> 00:26:15,360
resulted in

00:26:12,559 --> 00:26:16,080
an error count the total number of

00:26:15,360 --> 00:26:18,240
requests

00:26:16,080 --> 00:26:19,760
that took longer than your slo goal to

00:26:18,240 --> 00:26:21,919
process

00:26:19,760 --> 00:26:23,679
you can add some additional labels here

00:26:21,919 --> 00:26:25,200
so you can

00:26:23,679 --> 00:26:26,960
play with a little bit more cardinality

00:26:25,200 --> 00:26:31,360
than you could safely do

00:26:26,960 --> 00:26:34,559
with histograms this ends up

00:26:31,360 --> 00:26:36,960
being much less data being lower

00:26:34,559 --> 00:26:40,159
cardinality for prometheus to deal with

00:26:36,960 --> 00:26:42,720
and we can build a ratio like we have

00:26:40,159 --> 00:26:44,320
on the screen so we've we've taken this

00:26:42,720 --> 00:26:46,159
latency problem

00:26:44,320 --> 00:26:47,760
and we've turned it into a simple

00:26:46,159 --> 00:26:50,159
boolean problem

00:26:47,760 --> 00:26:52,000
where we can build a ratio look back

00:26:50,159 --> 00:26:55,520
over the past 30 days

00:26:52,000 --> 00:26:59,279
see if that's less than 0.95 or point or

00:26:55,520 --> 00:27:03,120
95 percent and the best part of this

00:26:59,279 --> 00:27:03,120
is it's actually accurate

00:27:03,840 --> 00:27:09,600
so that's a powerful trick for working

00:27:06,720 --> 00:27:13,039
with latencies in prometheus

00:27:09,600 --> 00:27:16,480
so we've looked at slos but alerting on

00:27:13,039 --> 00:27:18,399
slos is problematic

00:27:16,480 --> 00:27:20,960
a lot of people look at slos in the

00:27:18,399 --> 00:27:21,840
holistic sense they look at the entire

00:27:20,960 --> 00:27:24,320
30 days

00:27:21,840 --> 00:27:26,159
and they build alerts on did we violate

00:27:24,320 --> 00:27:30,080
our slo

00:27:26,159 --> 00:27:32,159
and this has really poor reset time

00:27:30,080 --> 00:27:34,080
when a problem happens you page an

00:27:32,159 --> 00:27:37,279
engineer the engineer comes on

00:27:34,080 --> 00:27:39,520
site fixes the problem the engineer the

00:27:37,279 --> 00:27:41,440
the business want to see verification

00:27:39,520 --> 00:27:44,080
that the problem has been fixed

00:27:41,440 --> 00:27:45,200
and see that very quickly with a

00:27:44,080 --> 00:27:47,679
holistic slo

00:27:45,200 --> 00:27:50,399
approach to alerting the alert doesn't

00:27:47,679 --> 00:27:51,919
resolve until that 30-day window elapses

00:27:50,399 --> 00:27:53,440
and that doesn't give us the feedback we

00:27:51,919 --> 00:27:56,640
know we need to

00:27:53,440 --> 00:27:59,440
know that we fix the problem detection

00:27:56,640 --> 00:28:02,320
time can also be really problematic

00:27:59,440 --> 00:28:04,799
the slo-based alert doesn't fire until

00:28:02,320 --> 00:28:08,080
we've burned our entire error budget

00:28:04,799 --> 00:28:10,000
so if this is a more subtle problem

00:28:08,080 --> 00:28:12,480
this is a problem that may have existed

00:28:10,000 --> 00:28:17,840
for hours or days

00:28:12,480 --> 00:28:19,600
that's gone unnoticed

00:28:17,840 --> 00:28:22,480
so i talked i mentioned error budgets

00:28:19,600 --> 00:28:22,799
briefly when we measure our slo we have

00:28:22,480 --> 00:28:27,200
a

00:28:22,799 --> 00:28:29,520
95 goal what is that other five percent

00:28:27,200 --> 00:28:31,840
that other five percent is our error

00:28:29,520 --> 00:28:35,200
budget it's our allowable margin for

00:28:31,840 --> 00:28:36,799
error and we can measure how much of

00:28:35,200 --> 00:28:38,640
that we're using

00:28:36,799 --> 00:28:40,399
and use that to figure out knew how

00:28:38,640 --> 00:28:41,360
risky can we be can we focus on new

00:28:40,399 --> 00:28:44,000
features

00:28:41,360 --> 00:28:47,200
or do we need to make sure our our ci cd

00:28:44,000 --> 00:28:47,200
pipeline is super stable

00:28:51,679 --> 00:28:55,840
the google site reliability engineering

00:28:53,520 --> 00:28:57,600
book talks about using burn rates as

00:28:55,840 --> 00:29:00,080
your alerting strategy

00:28:57,600 --> 00:29:01,039
so what we're doing is we're looking at

00:29:00,080 --> 00:29:03,120
the rate

00:29:01,039 --> 00:29:04,240
at which we're consuming our error

00:29:03,120 --> 00:29:07,120
budget

00:29:04,240 --> 00:29:09,600
and rather than looking back 30 days to

00:29:07,120 --> 00:29:11,679
figure out what our burn rate is

00:29:09,600 --> 00:29:13,360
you can that's perfectly valid but you

00:29:11,679 --> 00:29:13,679
can also look back at smaller intervals

00:29:13,360 --> 00:29:17,039
like

00:29:13,679 --> 00:29:18,559
30 minutes or an hour or three hours

00:29:17,039 --> 00:29:20,720
and you can calculate you know what the

00:29:18,559 --> 00:29:23,200
error ratio has been in the last hour

00:29:20,720 --> 00:29:26,880
divided by the error budget to get the

00:29:23,200 --> 00:29:30,240
error rate for the last hour

00:29:26,880 --> 00:29:32,960
if we compensate for our

00:29:30,240 --> 00:29:34,000
last hour compared to the total slo time

00:29:32,960 --> 00:29:36,559
window

00:29:34,000 --> 00:29:38,880
multiply that by our burn rate we get

00:29:36,559 --> 00:29:40,720
the ratio of our error budget that we've

00:29:38,880 --> 00:29:42,480
consumed

00:29:40,720 --> 00:29:44,000
this allows us to ask questions like

00:29:42,480 --> 00:29:47,120
this

00:29:44,000 --> 00:29:48,960
we can build an alert that fires

00:29:47,120 --> 00:29:51,039
if we've consumed two percent of our

00:29:48,960 --> 00:29:52,559
error budget to two percent of that five

00:29:51,039 --> 00:29:56,240
percent error budget

00:29:52,559 --> 00:29:57,440
within the last hour all we need to do

00:29:56,240 --> 00:30:00,240
in this case

00:29:57,440 --> 00:30:01,039
is figure out what our burn rate is we

00:30:00,240 --> 00:30:03,120
know what our

00:30:01,039 --> 00:30:04,399
budget consume ratio is going to be

00:30:03,120 --> 00:30:07,760
we're looking back an hour

00:30:04,399 --> 00:30:12,320
our total time window is 30 days so we

00:30:07,760 --> 00:30:12,320
solve that equation and we get 14.4

00:30:13,039 --> 00:30:17,440
so let's look at some real examples

00:30:15,440 --> 00:30:19,679
again i'm using my cheater method to

00:30:17,440 --> 00:30:22,480
measure latency slos here

00:30:19,679 --> 00:30:24,159
so i'm looking at the the total number

00:30:22,480 --> 00:30:25,520
of requests that took longer than our

00:30:24,159 --> 00:30:29,360
slo to process

00:30:25,520 --> 00:30:31,679
over the last hour over the total number

00:30:29,360 --> 00:30:34,080
of requests in that last hour

00:30:31,679 --> 00:30:36,159
again we sum by job so anything that

00:30:34,080 --> 00:30:39,360
exports metrics in our standard format

00:30:36,159 --> 00:30:40,799
this rule can handle so this builds a

00:30:39,360 --> 00:30:44,159
recording rule

00:30:40,799 --> 00:30:47,200
that is a ratio of the requests that are

00:30:44,159 --> 00:30:50,159
over our slo goal

00:30:47,200 --> 00:30:51,520
we stick that into an alert rule and we

00:30:50,159 --> 00:30:54,799
ask the question

00:30:51,520 --> 00:30:56,080
is that ratio greater than the burn rate

00:30:54,799 --> 00:30:59,679
we've calculated

00:30:56,080 --> 00:31:03,279
times our error budget so 14.4

00:30:59,679 --> 00:31:06,159
times our error budget of five percent

00:31:03,279 --> 00:31:08,000
this alert fires if we've burnt two

00:31:06,159 --> 00:31:08,799
percent of our error budget in the last

00:31:08,000 --> 00:31:11,440
hour

00:31:08,799 --> 00:31:13,360
so this indicates we probably have an

00:31:11,440 --> 00:31:16,799
emergent problem happening

00:31:13,360 --> 00:31:19,679
we need to page an engineer and

00:31:16,799 --> 00:31:21,200
address the problem at hand this alert

00:31:19,679 --> 00:31:24,480
clears very quickly

00:31:21,200 --> 00:31:27,600
when that problem is solved

00:31:24,480 --> 00:31:29,679
and this alert fires before we've

00:31:27,600 --> 00:31:31,600
consumed all of our error budget before

00:31:29,679 --> 00:31:34,480
we've broken our slo

00:31:31,600 --> 00:31:37,519
so we can react to changes that happen

00:31:34,480 --> 00:31:40,000
in an operational sense on the ground

00:31:37,519 --> 00:31:41,039
without putting at risk our slo goals

00:31:40,000 --> 00:31:44,000
for the month

00:31:41,039 --> 00:31:45,519
and that's really powerful the google

00:31:44,000 --> 00:31:48,159
sre book

00:31:45,519 --> 00:31:50,399
covers this in much more detail in fact

00:31:48,159 --> 00:31:52,240
it covers a multiple burn rate method

00:31:50,399 --> 00:31:54,960
that you can use to identify

00:31:52,240 --> 00:31:56,640
highly critical problems that you page

00:31:54,960 --> 00:31:59,440
somebody right away for

00:31:56,640 --> 00:32:00,799
versus more subtle issues that you know

00:31:59,440 --> 00:32:04,000
let's create a ticket

00:32:00,799 --> 00:32:04,000
and solve that monday morning

00:32:07,120 --> 00:32:12,640
so some takeaways from this

00:32:10,480 --> 00:32:14,480
it's important to think about how you're

00:32:12,640 --> 00:32:17,679
going to use prometheus

00:32:14,480 --> 00:32:19,760
build a schema and and break down some

00:32:17,679 --> 00:32:20,480
of those silo barriers between team and

00:32:19,760 --> 00:32:23,200
keep those

00:32:20,480 --> 00:32:25,440
those barriers broken down by working

00:32:23,200 --> 00:32:29,840
with teams to build a common way to use

00:32:25,440 --> 00:32:29,840
prometheus across your fleet

00:32:30,159 --> 00:32:35,039
think about making rule libraries think

00:32:32,720 --> 00:32:37,200
about being able to automate a way

00:32:35,039 --> 00:32:39,600
creating dashboards and prometheus

00:32:37,200 --> 00:32:39,600
alerting

00:32:39,919 --> 00:32:44,799
slo-based measuring has a lot of really

00:32:42,320 --> 00:32:47,039
powerful aspects

00:32:44,799 --> 00:32:48,480
but they're based on percentiles and

00:32:47,039 --> 00:32:50,880
prometheus is not

00:32:48,480 --> 00:32:53,039
great with working with percentiles so

00:32:50,880 --> 00:32:55,679
once we understand the caveats there

00:32:53,039 --> 00:32:56,080
we know how we can work around them it's

00:32:55,679 --> 00:32:59,279
important

00:32:56,080 --> 00:33:00,640
to get your math right here and having

00:32:59,279 --> 00:33:02,399
someone that

00:33:00,640 --> 00:33:04,000
that knows the ins and outs here to make

00:33:02,399 --> 00:33:05,120
sure that you can make good business

00:33:04,000 --> 00:33:08,240
decisions

00:33:05,120 --> 00:33:11,279
uh based on the data that you have is

00:33:08,240 --> 00:33:14,880
really super important

00:33:11,279 --> 00:33:15,840
i think that slo's category that slo is

00:33:14,880 --> 00:33:18,640
categorized

00:33:15,840 --> 00:33:19,279
more into a report which is something

00:33:18,640 --> 00:33:22,799
that's

00:33:19,279 --> 00:33:24,640
more aimed at informing the business

00:33:22,799 --> 00:33:26,640
and it's something that we refer to

00:33:24,640 --> 00:33:28,640
weekly daily to figure out you know how

00:33:26,640 --> 00:33:31,600
well we're doing

00:33:28,640 --> 00:33:33,519
however as far as operational alerting

00:33:31,600 --> 00:33:34,240
goes and what's been happening in the

00:33:33,519 --> 00:33:36,399
fleet

00:33:34,240 --> 00:33:37,760
over the last hour over the last three

00:33:36,399 --> 00:33:40,480
hours

00:33:37,760 --> 00:33:41,919
using a burn rate style alerting pattern

00:33:40,480 --> 00:33:45,600
can give us much better

00:33:41,919 --> 00:33:49,200
much better visibility into our fleet

00:33:45,600 --> 00:33:52,399
and help us meet our slo goals

00:33:49,200 --> 00:33:54,640
going forward and

00:33:52,399 --> 00:33:57,120
keep moving iteration in in a good

00:33:54,640 --> 00:33:57,120
direction

00:33:58,960 --> 00:34:02,159
so i've handed over my presentation so

00:34:01,440 --> 00:34:05,440
that should be

00:34:02,159 --> 00:34:07,440
available to folks as well the google

00:34:05,440 --> 00:34:08,000
site reliability engineering books are

00:34:07,440 --> 00:34:11,040
available

00:34:08,000 --> 00:34:14,240
online for free there are great reads

00:34:11,040 --> 00:34:16,560
the examples for burn rates

00:34:14,240 --> 00:34:18,639
are included in the site reliability

00:34:16,560 --> 00:34:21,599
engineering workbook

00:34:18,639 --> 00:34:22,720
which came out in 2018 i've also linked

00:34:21,599 --> 00:34:25,200
to the monarch paper i

00:34:22,720 --> 00:34:27,440
referenced it doesn't describe how

00:34:25,200 --> 00:34:29,440
monarch works internally to google

00:34:27,440 --> 00:34:30,879
but it tells us so much about the

00:34:29,440 --> 00:34:32,720
history there

00:34:30,879 --> 00:34:35,119
and so much about how google thought

00:34:32,720 --> 00:34:36,000
about these very same problems and how

00:34:35,119 --> 00:34:38,720
they solve them

00:34:36,000 --> 00:34:40,720
and that really kind of informs you as

00:34:38,720 --> 00:34:43,359
we scale up

00:34:40,720 --> 00:34:45,919
what to look for as far as as challenges

00:34:43,359 --> 00:34:45,919
and problems

00:34:46,720 --> 00:34:51,280
again my name is jack neely i'm with

00:34:49,800 --> 00:34:52,800
42lines.net

00:34:51,280 --> 00:34:54,399
if you have any questions about this

00:34:52,800 --> 00:34:57,599
presentation um

00:34:54,399 --> 00:34:58,400
hit me up in chat you can email me at 42

00:34:57,599 --> 00:35:00,480
lines

00:34:58,400 --> 00:35:02,079
you're welcome to ask us questions

00:35:00,480 --> 00:35:04,960
questions and answers are totally

00:35:02,079 --> 00:35:06,400
free if you'd like this kind of content

00:35:04,960 --> 00:35:08,720
i'm also the co-host

00:35:06,400 --> 00:35:10,320
along with jared of the practical

00:35:08,720 --> 00:35:13,599
operations podcast

00:35:10,320 --> 00:35:16,880
find us at operations.fm or wherever you

00:35:13,599 --> 00:35:19,520
listen to your podcasts

00:35:16,880 --> 00:35:20,160
thanks a bunch and jared if there are

00:35:19,520 --> 00:35:23,119
questions

00:35:20,160 --> 00:35:23,119
i'll let you go at it

00:35:25,440 --> 00:35:28,079
none just yet

00:35:29,680 --> 00:35:33,200
you all have about 10 minutes in a rush

00:35:37,599 --> 00:35:45,839
and of course someone moves their lawn

00:35:39,359 --> 00:35:45,839
as my presentation begins

00:36:11,200 --> 00:36:16,400
yeah the mowers come out in force when

00:36:12,720 --> 00:36:16,400
we do a conference especially virtual

00:36:20,800 --> 00:36:25,359
there's a question from alejandro i'm

00:36:23,359 --> 00:36:26,000
new to prometheus is it comparable to

00:36:25,359 --> 00:36:28,640
datadog

00:36:26,000 --> 00:36:28,640
and new relic

00:36:32,000 --> 00:36:38,560
it definitely has some of these features

00:36:35,280 --> 00:36:40,480
prometheus is really targeted at

00:36:38,560 --> 00:36:43,040
the metric side of the equation and less

00:36:40,480 --> 00:36:46,240
so at uh events and logs

00:36:43,040 --> 00:36:48,000
which both of those uh sas vendors

00:36:46,240 --> 00:36:50,079
support both

00:36:48,000 --> 00:36:52,079
both of those sas vendors have different

00:36:50,079 --> 00:36:54,320
strengths and weaknesses

00:36:52,079 --> 00:36:56,720
and this is what jared and i have done

00:36:54,320 --> 00:36:58,640
with a couple companies at this point

00:36:56,720 --> 00:36:59,839
is there comes a point where throwing

00:36:58,640 --> 00:37:04,000
everything at datadog

00:36:59,839 --> 00:37:05,599
is just expensive and being able to

00:37:04,000 --> 00:37:07,440
analyze where you are and what your

00:37:05,599 --> 00:37:09,119
goals are moving forward

00:37:07,440 --> 00:37:11,520
we can sort of put together a better

00:37:09,119 --> 00:37:12,400
strategy about new how we might use

00:37:11,520 --> 00:37:14,640
prometheus

00:37:12,400 --> 00:37:16,720
and elasticsearch locally for high

00:37:14,640 --> 00:37:20,320
volume high cardinality

00:37:16,720 --> 00:37:23,839
and pair that with an external vendor

00:37:20,320 --> 00:37:24,960
to archive and handle the the business

00:37:23,839 --> 00:37:27,280
metrics of things that are

00:37:24,960 --> 00:37:28,240
long-term important for a business and

00:37:27,280 --> 00:37:31,040
that can often

00:37:28,240 --> 00:37:37,839
find some significant savings in in your

00:37:31,040 --> 00:37:37,839
visibility stack

00:37:39,200 --> 00:37:42,720
jared i'm going to let you take the next

00:37:40,400 --> 00:37:42,720
question

00:37:42,960 --> 00:37:47,599
okay um so uh another question is

00:37:46,160 --> 00:37:50,240
uh where is a good place to begin with

00:37:47,599 --> 00:37:53,119
prometheus recommended sites to learn it

00:37:50,240 --> 00:37:53,680
actually the prometheus dot io website

00:37:53,119 --> 00:37:55,119
their

00:37:53,680 --> 00:37:57,119
main website has some great

00:37:55,119 --> 00:37:58,480
documentation uh i

00:37:57,119 --> 00:38:00,720
go to that every time i need to

00:37:58,480 --> 00:38:02,000
reference for configuration changes and

00:38:00,720 --> 00:38:05,040
that kind of thing

00:38:02,000 --> 00:38:08,000
also uh brian brazil one of the uh

00:38:05,040 --> 00:38:10,040
core committers to prometheus he runs a

00:38:08,000 --> 00:38:12,560
blog on

00:38:10,040 --> 00:38:13,440
robustperception.io i'll drop a link in

00:38:12,560 --> 00:38:16,320
the chat

00:38:13,440 --> 00:38:17,920
um he blogs uh about common problems

00:38:16,320 --> 00:38:18,480
that you run into with prometheus as

00:38:17,920 --> 00:38:20,800
well as

00:38:18,480 --> 00:38:22,560
uh recently or i guess for a while now

00:38:20,800 --> 00:38:23,359
whenever there's a new version he talks

00:38:22,560 --> 00:38:25,200
about

00:38:23,359 --> 00:38:27,599
uh new features and how you might use

00:38:25,200 --> 00:38:29,760
them that actually apply

00:38:27,599 --> 00:38:32,320
um so those are some two great resources

00:38:29,760 --> 00:38:33,839
but honestly prometheus is so simple

00:38:32,320 --> 00:38:36,400
it's great just to download and it will

00:38:33,839 --> 00:38:39,040
run on you know mac linux windows

00:38:36,400 --> 00:38:40,960
just download it and just start

00:38:39,040 --> 00:38:43,359
ingesting data even from prometheus

00:38:40,960 --> 00:38:45,599
itself and just play with it it's a

00:38:43,359 --> 00:38:46,720
very approachable tool and it's just

00:38:45,599 --> 00:38:50,880
something that takes

00:38:46,720 --> 00:38:50,880
a very little time to master

00:38:57,040 --> 00:39:00,560
jack you want to take this one does

00:38:58,720 --> 00:39:01,760
using local storage versus a remote

00:39:00,560 --> 00:39:05,680
storage integration

00:39:01,760 --> 00:39:05,680
have a large impact on queries

00:39:06,000 --> 00:39:10,640
it can depending on how large

00:39:09,119 --> 00:39:13,119
how much data you're querying back and

00:39:10,640 --> 00:39:16,000
forth the local storage

00:39:13,119 --> 00:39:17,760
is local on disk and memory mapped and

00:39:16,000 --> 00:39:19,359
really super efficient

00:39:17,760 --> 00:39:21,440
any sort of remote storage is going to

00:39:19,359 --> 00:39:23,920
be a network call away

00:39:21,440 --> 00:39:25,599
and you know if you're referencing um s3

00:39:23,920 --> 00:39:28,560
buckets or gcs buckets

00:39:25,599 --> 00:39:29,280
in in the cloud that can be pretty fast

00:39:28,560 --> 00:39:32,240
um

00:39:29,280 --> 00:39:33,359
but there's definitely network calls

00:39:32,240 --> 00:39:36,079
involved there so that's definitely

00:39:33,359 --> 00:39:36,079
something to think about

00:39:40,320 --> 00:39:45,280
also um if you use the slo error count

00:39:43,359 --> 00:39:47,119
total counter ratio approach

00:39:45,280 --> 00:39:51,200
how do you deal with labels when an

00:39:47,119 --> 00:39:51,200
error event hasn't been observed yet

00:39:51,680 --> 00:39:55,520
you know that's a great question that

00:39:53,680 --> 00:39:58,800
i've seen a lot that many

00:39:55,520 --> 00:40:02,480
people have asked and if you dig down

00:39:58,800 --> 00:40:03,680
deep enough into the prometheus best

00:40:02,480 --> 00:40:06,960
practices

00:40:03,680 --> 00:40:09,520
what they advise you do is

00:40:06,960 --> 00:40:11,760
is try to pre-initialize all of your

00:40:09,520 --> 00:40:13,599
metrics and label combinations when your

00:40:11,760 --> 00:40:16,800
application starts

00:40:13,599 --> 00:40:17,760
so your application starts your it

00:40:16,800 --> 00:40:21,359
already exports

00:40:17,760 --> 00:40:22,160
a error metric with a classification

00:40:21,359 --> 00:40:23,839
label

00:40:22,160 --> 00:40:25,520
you know four or four errors and that's

00:40:23,839 --> 00:40:26,960
already set to zero

00:40:25,520 --> 00:40:29,359
uh that can be a really important

00:40:26,960 --> 00:40:30,400
technique um it doesn't always work in

00:40:29,359 --> 00:40:33,200
every case

00:40:30,400 --> 00:40:34,319
um but that's what i would suggest for

00:40:33,200 --> 00:40:37,119
first

00:40:34,319 --> 00:40:38,160
there's some problem ql isms that we can

00:40:37,119 --> 00:40:41,440
use to

00:40:38,160 --> 00:40:41,440
to help mitigate that as well

00:40:44,000 --> 00:40:47,119
prom ql is pretty powerful as a language

00:40:46,800 --> 00:40:49,599
um

00:40:47,119 --> 00:40:53,839
you can play the game of life in it not

00:40:49,599 --> 00:40:53,839
for the faint of heart

00:40:55,280 --> 00:40:58,800
just a reminder there are five minutes

00:41:04,839 --> 00:41:07,839
left

00:41:14,160 --> 00:41:21,839
i see a question about um

00:41:17,520 --> 00:41:21,839
or on vector

00:41:25,040 --> 00:41:29,040
i could but this is a really limiting

00:41:26,480 --> 00:41:32,640
way to deal with that question

00:41:29,040 --> 00:41:35,359
um email me j-j-n-e-e-l-y

00:41:32,640 --> 00:41:38,240
at 42 42lines.net if it hasn't gotten

00:41:35,359 --> 00:41:38,240
into the chat yet

00:41:41,920 --> 00:41:45,440
i can type it doesn't appear

00:41:47,200 --> 00:41:51,280
on top of what jared said there's a

00:41:48,960 --> 00:41:52,319
really healthy reddit for prometheus

00:41:51,280 --> 00:41:55,359
prometheus

00:41:52,319 --> 00:41:57,440
monitoring um and that's a great place

00:41:55,359 --> 00:41:59,040
as well to post questions

00:41:57,440 --> 00:42:03,839
just like that and get some more

00:41:59,040 --> 00:42:03,839
in-depth information

00:42:16,880 --> 00:42:21,119
oh yeah this is a related missing

00:42:18,240 --> 00:42:24,319
account of initialization

00:42:21,119 --> 00:42:27,200
yeah email me and i can probably

00:42:24,319 --> 00:42:30,160
um help you more thoroughly than than in

00:42:27,200 --> 00:42:45,839
the chat

00:42:30,160 --> 00:42:45,839
awesome thank you

00:42:46,000 --> 00:42:49,839
all right if that's it i guess we're out

00:42:47,599 --> 00:42:50,720
of here um big thanks to jared for

00:42:49,839 --> 00:42:53,680
helping me with

00:42:50,720 --> 00:42:54,880
uh questions and moderations and ben i

00:42:53,680 --> 00:42:57,040
really appreciate the

00:42:54,880 --> 00:42:59,839
the moderators here at eto you guys have

00:42:57,040 --> 00:42:59,839

YouTube URL: https://www.youtube.com/watch?v=wiQa1mmwI6Y


