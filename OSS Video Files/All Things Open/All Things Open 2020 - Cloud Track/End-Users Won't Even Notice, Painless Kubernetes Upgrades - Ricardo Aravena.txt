Title: End-Users Won't Even Notice, Painless Kubernetes Upgrades - Ricardo Aravena
Publication date: 2020-12-11
Playlist: All Things Open 2020 - Cloud Track
Description: 
	Presented by: Ricardo Aravena, Rakuten
Presented at All Things Open 2020 - Cloud Track

Abstract: Since its first release, Kubernetes has had continuing innovation and regular 3-month releases. This has made it challenging for some organizations to keep pace with new features and to keep their Kubernetes clusters up to date. Furthermore, the constant change of the Kubernetes API and CRDs may make it difficult to maintain the required uptime.

This talk will demonstrate using different tools with automation to update the major version of your cluster using a blue/green approach. What kind of open source tools can we use? Can we take advantage of a GitOps approach using open source tools like wksctl? How do we automatically change manifests to support the new API? How can we verify that our workloads are compatible with a new version? Can we take advantage of some chaos engineering tools to improve resiliency before and after the upgrades? Why is it essential to have the same versions in the Kubernetes control and data planes?

The audience will come away with an understanding of how to use different open-source tools and processes to make Kubernetes upgrades seamless with minimal to no downtime in production.
Captions: 
	00:00:05,200 --> 00:00:09,519
thank you everyone for

00:00:07,279 --> 00:00:10,880
attending this session uh very excited

00:00:09,519 --> 00:00:14,000
to be here at

00:00:10,880 --> 00:00:14,960
all things open um and today i'm going

00:00:14,000 --> 00:00:17,840
to tell you

00:00:14,960 --> 00:00:18,880
a little bit about kubernetes cluster

00:00:17,840 --> 00:00:22,880
upgrades

00:00:18,880 --> 00:00:24,720
and an approach to doing this cluster

00:00:22,880 --> 00:00:30,800
upgrades using

00:00:24,720 --> 00:00:33,520
blue and green clusters

00:00:30,800 --> 00:00:35,040
a little bit about myself i work as an

00:00:33,520 --> 00:00:38,960
sre manager

00:00:35,040 --> 00:00:41,120
at rakuten and we run

00:00:38,960 --> 00:00:42,320
kubernetes clusters in production and we

00:00:41,120 --> 00:00:45,600
have several

00:00:42,320 --> 00:00:48,640
environments and i'm also a

00:00:45,600 --> 00:00:52,320
co-chair for the cncf sig runtime

00:00:48,640 --> 00:00:54,239
and i've been a

00:00:52,320 --> 00:00:55,680
contributor to the kata containers

00:00:54,239 --> 00:00:59,840
project for

00:00:55,680 --> 00:00:59,840
about the last two years

00:01:00,399 --> 00:01:03,760
yeah so some of what i'll be talking

00:01:02,320 --> 00:01:06,799
about today uh

00:01:03,760 --> 00:01:09,439
first i'll briefly touch on the main

00:01:06,799 --> 00:01:10,640
kubernetes components you have the

00:01:09,439 --> 00:01:13,360
control plane

00:01:10,640 --> 00:01:15,280
in the data plane then i'll talk about

00:01:13,360 --> 00:01:18,400
some

00:01:15,280 --> 00:01:20,799
upgrade problems that you may run into

00:01:18,400 --> 00:01:21,520
for kubernetes clusters then i'll talk

00:01:20,799 --> 00:01:25,360
about

00:01:21,520 --> 00:01:27,119
some open source solutions and tools

00:01:25,360 --> 00:01:28,720
then i'll talk about what it may look

00:01:27,119 --> 00:01:29,360
like in when you have everything

00:01:28,720 --> 00:01:32,159
together

00:01:29,360 --> 00:01:32,479
in production then i'll touch on what it

00:01:32,159 --> 00:01:34,799
may

00:01:32,479 --> 00:01:35,920
look like in in the future for some of

00:01:34,799 --> 00:01:39,119
these tools and

00:01:35,920 --> 00:01:41,600
and when you do kubernetes upgrades

00:01:39,119 --> 00:01:44,240
and finally i'll provide you some

00:01:41,600 --> 00:01:44,240
takeaways

00:01:46,079 --> 00:01:49,200
so yeah so the kubernetes control plane

00:01:48,159 --> 00:01:52,320
so typically

00:01:49,200 --> 00:01:55,759
uh you either have a

00:01:52,320 --> 00:02:00,640
three node or or five node or nine

00:01:55,759 --> 00:02:02,960
node uh uh control plane setup with uh

00:02:00,640 --> 00:02:04,159
uh either number of main nodes i change

00:02:02,960 --> 00:02:06,719
the terminology here

00:02:04,159 --> 00:02:08,319
uh from master domain because they're

00:02:06,719 --> 00:02:11,039
changing that terminology

00:02:08,319 --> 00:02:12,560
in a lot of places but in essence you

00:02:11,039 --> 00:02:15,040
have in every node

00:02:12,560 --> 00:02:16,800
uh unique component of all these

00:02:15,040 --> 00:02:17,440
different parts of kubernetes like the

00:02:16,800 --> 00:02:20,480
api

00:02:17,440 --> 00:02:23,280
server the cube controller manager the

00:02:20,480 --> 00:02:24,560
cloud controller and the scheduler and

00:02:23,280 --> 00:02:27,920
when you have either the

00:02:24,560 --> 00:02:30,480
three five or nine um control plane

00:02:27,920 --> 00:02:32,000
set up you actually have a leader and

00:02:30,480 --> 00:02:33,599
that makes the decisions

00:02:32,000 --> 00:02:37,040
and that's for each one of these

00:02:33,599 --> 00:02:39,200
components too

00:02:37,040 --> 00:02:40,720
so when you have a specific kubernetes

00:02:39,200 --> 00:02:43,360
version

00:02:40,720 --> 00:02:43,920
you have that specific kubernetes

00:02:43,360 --> 00:02:46,800
version

00:02:43,920 --> 00:02:47,120
for each one of these components too so

00:02:46,800 --> 00:02:50,160
the

00:02:47,120 --> 00:02:54,000
controller manager the api server

00:02:50,160 --> 00:02:56,400
cloud controller all of them

00:02:54,000 --> 00:02:58,400
uh ha in this example have the 118

00:02:56,400 --> 00:03:00,159
version right so they're all matching

00:02:58,400 --> 00:03:01,760
that's a recommended practice some

00:03:00,159 --> 00:03:03,200
people don't run it this way but then

00:03:01,760 --> 00:03:05,840
generally that's the recommended

00:03:03,200 --> 00:03:05,840
practice

00:03:06,720 --> 00:03:13,200
similarly with the data plane you have

00:03:09,760 --> 00:03:14,879
uh your kubernetes nodes where you run

00:03:13,200 --> 00:03:18,319
your workloads

00:03:14,879 --> 00:03:21,440
and the most typical components there

00:03:18,319 --> 00:03:25,120
are the cubelet and the cube proxy

00:03:21,440 --> 00:03:27,040
so for those if you have kubernetes 118

00:03:25,120 --> 00:03:28,640
you also have that same version for

00:03:27,040 --> 00:03:33,440
those specific components

00:03:28,640 --> 00:03:36,159
so both of them match

00:03:33,440 --> 00:03:36,879
and that's the the control plane in the

00:03:36,159 --> 00:03:39,280
data point

00:03:36,879 --> 00:03:40,640
but what are some of the upgrade

00:03:39,280 --> 00:03:44,959
problems that you may run

00:03:40,640 --> 00:03:48,319
into for for these components

00:03:44,959 --> 00:03:51,760
so in this tweet from last year stefan

00:03:48,319 --> 00:03:53,519
uh said that he upgraded from kubernetes

00:03:51,760 --> 00:03:56,959
00:03:53,519 --> 00:04:00,239
uh and then or to kubernetes 113

00:03:56,959 --> 00:04:00,640
and using gke and he did this feature

00:04:00,239 --> 00:04:03,120
called

00:04:00,640 --> 00:04:03,760
in place upgraded in the cloud provider

00:04:03,120 --> 00:04:06,720
and

00:04:03,760 --> 00:04:07,920
he had istio running and then istio went

00:04:06,720 --> 00:04:12,239
from 1.0

00:04:07,920 --> 00:04:14,159
to 1.1 and then a lot of the

00:04:12,239 --> 00:04:15,680
pods started crashing so i was having

00:04:14,159 --> 00:04:18,239
having problems

00:04:15,680 --> 00:04:20,560
so basically he had to restart all the

00:04:18,239 --> 00:04:21,840
different components uh one by one in a

00:04:20,560 --> 00:04:24,960
specific order

00:04:21,840 --> 00:04:25,600
and it got fixed right so but then

00:04:24,960 --> 00:04:27,280
that's when

00:04:25,600 --> 00:04:28,800
what happened when he did the the

00:04:27,280 --> 00:04:31,199
cluster upgrade the in-place

00:04:28,800 --> 00:04:31,199
upgrade

00:04:31,919 --> 00:04:35,919
so one of the problems in kubernetes uh

00:04:34,800 --> 00:04:39,280
api

00:04:35,919 --> 00:04:42,639
or ap the api version or versions

00:04:39,280 --> 00:04:45,600
is that they keep changing them so

00:04:42,639 --> 00:04:46,720
uh initially when an object in

00:04:45,600 --> 00:04:49,040
kubernetes

00:04:46,720 --> 00:04:51,360
gets released it gets released under an

00:04:49,040 --> 00:04:52,000
api group and typically that api group

00:04:51,360 --> 00:04:55,360
was

00:04:52,000 --> 00:04:55,680
defined as an api group uh alpha alpha

00:04:55,360 --> 00:04:58,560
one

00:04:55,680 --> 00:05:00,000
one two three and four so forth and then

00:04:58,560 --> 00:05:01,759
it goes into beta when it's

00:05:00,000 --> 00:05:04,080
a little bit more stable and then

00:05:01,759 --> 00:05:07,199
finally when it becomes ga

00:05:04,080 --> 00:05:09,520
it it gets released into a stable

00:05:07,199 --> 00:05:11,440
api group and that's usually like a v1

00:05:09,520 --> 00:05:13,199
or v2

00:05:11,440 --> 00:05:14,560
so they keep constantly changing between

00:05:13,199 --> 00:05:18,000
different versions

00:05:14,560 --> 00:05:18,000
or kubernetes versions

00:05:18,320 --> 00:05:22,000
so this is how it goes typically you

00:05:20,320 --> 00:05:25,280
have a

00:05:22,000 --> 00:05:26,800
v1 alpha 1 and then there

00:05:25,280 --> 00:05:29,360
are some iterations behind that

00:05:26,800 --> 00:05:32,400
kubernetes api group and object

00:05:29,360 --> 00:05:34,560
and then uh it goes to v1 alpha two

00:05:32,400 --> 00:05:36,560
three four and then into beta one and

00:05:34,560 --> 00:05:39,280
then beta one two three

00:05:36,560 --> 00:05:40,800
and then it goes to v1 and then it

00:05:39,280 --> 00:05:42,080
starts to cycle again when it's

00:05:40,800 --> 00:05:45,600
something new like v2

00:05:42,080 --> 00:05:48,240
alpha 1 and then b2 beta1 and then v2

00:05:45,600 --> 00:05:48,240
and so forth

00:05:48,880 --> 00:05:57,840
so here another example where uh

00:05:52,880 --> 00:06:00,400
you see changes in the kubernetes uh api

00:05:57,840 --> 00:06:00,960
with different releases right so here

00:06:00,400 --> 00:06:05,600
with uh

00:06:00,960 --> 00:06:08,400
our back in kubernetes 120

00:06:05,600 --> 00:06:11,520
you see that it's actually uh removing

00:06:08,400 --> 00:06:14,400
the support for v1 alpha 1 and b1

00:06:11,520 --> 00:06:15,280
beta1 so if you have any r back

00:06:14,400 --> 00:06:18,240
definitions

00:06:15,280 --> 00:06:18,639
uh using b1 beta1 or maybe some rolls

00:06:18,240 --> 00:06:20,960
some

00:06:18,639 --> 00:06:21,680
cluster roles and then if you don't

00:06:20,960 --> 00:06:24,880
change them

00:06:21,680 --> 00:06:26,720
before 120 they will no longer be there

00:06:24,880 --> 00:06:29,199
so you may run into problems there

00:06:26,720 --> 00:06:30,479
so so you you may not have access to

00:06:29,199 --> 00:06:33,120
your cluster

00:06:30,479 --> 00:06:34,319
so some things are not going to work and

00:06:33,120 --> 00:06:38,319
then another example

00:06:34,319 --> 00:06:40,160
is like plugins so kubernetes uses a lot

00:06:38,319 --> 00:06:42,479
of different plugins in this case it's

00:06:40,160 --> 00:06:44,720
aws evs

00:06:42,479 --> 00:06:45,759
and then on the release notes it says

00:06:44,720 --> 00:06:49,199
that it's going to be

00:06:45,759 --> 00:06:50,240
removed in 121 in favor of this new

00:06:49,199 --> 00:06:53,520
feature for

00:06:50,240 --> 00:06:56,240
csi or csi support for aws

00:06:53,520 --> 00:06:58,240
so that means if you have a volume on an

00:06:56,240 --> 00:06:59,759
eva elastic block storage volume where

00:06:58,240 --> 00:07:03,840
you have your data

00:06:59,759 --> 00:07:07,280
and you upgrade your cluster to 121

00:07:03,840 --> 00:07:09,039
then you're using this and and uh

00:07:07,280 --> 00:07:10,479
maybe your volume will not be available

00:07:09,039 --> 00:07:11,120
your your data is not going to be

00:07:10,479 --> 00:07:14,840
available

00:07:11,120 --> 00:07:17,840
so uh so these are some of the problems

00:07:14,840 --> 00:07:17,840
right

00:07:19,280 --> 00:07:22,800
so in this example we have the our back

00:07:22,240 --> 00:07:25,919
roll

00:07:22,800 --> 00:07:28,400
in b1 beta 1 in 117

00:07:25,919 --> 00:07:29,680
and if if you upgrade it to your cluster

00:07:28,400 --> 00:07:31,599
to 120

00:07:29,680 --> 00:07:33,759
and then you keep that same definition

00:07:31,599 --> 00:07:36,800
for b1 beta 1

00:07:33,759 --> 00:07:38,880
uh you'll see that it's not

00:07:36,800 --> 00:07:40,720
necessarily going to work or it's not

00:07:38,880 --> 00:07:43,280
going to going to work and

00:07:40,720 --> 00:07:44,720
your your parts may not be available and

00:07:43,280 --> 00:07:46,879
here you're accessing your

00:07:44,720 --> 00:07:50,160
parts and your logs or maybe your locks

00:07:46,879 --> 00:07:50,160
will not be accessible

00:07:52,080 --> 00:07:58,560
now if you do this ahead of time

00:07:55,599 --> 00:07:58,800
maybe in any place upgrade you have that

00:07:58,560 --> 00:08:02,400
b

00:07:58,800 --> 00:08:03,280
beta 1 on 117 but then before you do the

00:08:02,400 --> 00:08:06,240
00:08:03,280 --> 00:08:08,720
upgrade you change the api definition to

00:08:06,240 --> 00:08:08,720
v1

00:08:09,199 --> 00:08:14,400
so in this case everything works

00:08:12,560 --> 00:08:16,160
and you're happy it's because you did it

00:08:14,400 --> 00:08:19,599
ahead of time right so

00:08:16,160 --> 00:08:21,759
um but you need to plan ahead of time

00:08:19,599 --> 00:08:23,120
and and some and this is just an example

00:08:21,759 --> 00:08:24,960
of with roles

00:08:23,120 --> 00:08:26,400
so there may be many other examples

00:08:24,960 --> 00:08:28,080
where you

00:08:26,400 --> 00:08:30,000
might have to care about all these

00:08:28,080 --> 00:08:31,680
changes with kubernetes and one of the

00:08:30,000 --> 00:08:34,959
things with kubernetes is that

00:08:31,680 --> 00:08:36,719
there it's a very fast moving project

00:08:34,959 --> 00:08:40,399
and there are a lot of new features

00:08:36,719 --> 00:08:42,399
being released

00:08:40,399 --> 00:08:43,440
so there are other problems with uh

00:08:42,399 --> 00:08:46,399
kubernetes

00:08:43,440 --> 00:08:47,519
uh versions for the different components

00:08:46,399 --> 00:08:50,000
they do support

00:08:47,519 --> 00:08:50,640
version skews although the recommended

00:08:50,000 --> 00:08:53,360
practice

00:08:50,640 --> 00:08:54,000
is uh to have all these different

00:08:53,360 --> 00:08:55,920
components

00:08:54,000 --> 00:08:57,040
with the same version some people don't

00:08:55,920 --> 00:08:59,519
right so

00:08:57,040 --> 00:09:00,640
uh if you if you don't then you need to

00:08:59,519 --> 00:09:03,440
be really careful

00:09:00,640 --> 00:09:04,720
about you know what supports what for

00:09:03,440 --> 00:09:08,640
example here

00:09:04,720 --> 00:09:12,320
if you have the cube api server 117

00:09:08,640 --> 00:09:14,320
and if you have a cupola 116 and 117

00:09:12,320 --> 00:09:17,200
it will work fine but if you have

00:09:14,320 --> 00:09:19,920
something like cube like 115 or 114

00:09:17,200 --> 00:09:21,600
that means your kubernetes nodes it

00:09:19,920 --> 00:09:22,600
means that

00:09:21,600 --> 00:09:24,880
you're going to have to

00:09:22,600 --> 00:09:28,560
incompatibilities and then basically

00:09:24,880 --> 00:09:28,560
your clusters may not actually run

00:09:30,000 --> 00:09:33,200
so what are some of the solutions that

00:09:32,080 --> 00:09:35,519
you can use

00:09:33,200 --> 00:09:38,320
and that are available for these types

00:09:35,519 --> 00:09:38,320
of problems

00:09:39,279 --> 00:09:45,200
so initially what i shared was a gke

00:09:42,800 --> 00:09:46,160
cluster upgrade that was an in-place

00:09:45,200 --> 00:09:48,240
upgrade

00:09:46,160 --> 00:09:49,600
so in this case you have all your

00:09:48,240 --> 00:09:54,640
control plane here with

00:09:49,600 --> 00:09:54,640
118 in this example you have three nodes

00:09:55,600 --> 00:10:01,920
and then what it does is actually

00:09:58,640 --> 00:10:04,640
when you click that button the

00:10:01,920 --> 00:10:06,480
main nodes uh that are not the leader

00:10:04,640 --> 00:10:09,200
typically get upgraded first

00:10:06,480 --> 00:10:10,399
because they're not making any decisions

00:10:09,200 --> 00:10:12,880
right so

00:10:10,399 --> 00:10:16,399
uh they're easy to you know shut down

00:10:12,880 --> 00:10:16,399
and then bring up with a new version

00:10:16,800 --> 00:10:23,920
and now when you upgrade that later

00:10:20,720 --> 00:10:26,160
there's going to be some

00:10:23,920 --> 00:10:28,000
downtime there because the api server

00:10:26,160 --> 00:10:30,480
all these components need to restart

00:10:28,000 --> 00:10:32,000
so what happens is because kubernetes

00:10:30,480 --> 00:10:33,920
has a

00:10:32,000 --> 00:10:35,360
consensus algorithm on all these

00:10:33,920 --> 00:10:37,200
different components and kubernetes have

00:10:35,360 --> 00:10:39,040
consensus algorithms

00:10:37,200 --> 00:10:40,880
they choose a new leader right so that

00:10:39,040 --> 00:10:43,120
leader is not there anymore so

00:10:40,880 --> 00:10:44,320
it might be the the one on the right

00:10:43,120 --> 00:10:47,839
right here

00:10:44,320 --> 00:10:47,839
so you have a brand new leader

00:10:48,480 --> 00:10:52,240
so what happens is that when this uh the

00:10:51,519 --> 00:10:54,720
new leader

00:10:52,240 --> 00:10:56,399
gets elected and like i mentioned before

00:10:54,720 --> 00:10:57,600
so these some of these apis are not

00:10:56,399 --> 00:10:59,200
supported

00:10:57,600 --> 00:11:01,120
you may run into this right so you may

00:10:59,200 --> 00:11:02,640
run into crash loop back offs

00:11:01,120 --> 00:11:04,079
uh some of your pots are not going to

00:11:02,640 --> 00:11:05,200
restart maybe you won't be able to

00:11:04,079 --> 00:11:07,519
access some of your

00:11:05,200 --> 00:11:09,760
our back resources or your or your

00:11:07,519 --> 00:11:15,839
resources managed by our back

00:11:09,760 --> 00:11:15,839
so a lot of unpredictable results

00:11:16,160 --> 00:11:20,800
now another approach is just to have

00:11:18,000 --> 00:11:23,760
these blue and green clusters

00:11:20,800 --> 00:11:25,040
so you may want to just have more than

00:11:23,760 --> 00:11:28,720
just one production

00:11:25,040 --> 00:11:30,240
cluster so in this case you can have

00:11:28,720 --> 00:11:31,120
production clusters with different

00:11:30,240 --> 00:11:34,160
versions

00:11:31,120 --> 00:11:37,360
when you do that upgrade

00:11:34,160 --> 00:11:39,760
so what does that look like so in this

00:11:37,360 --> 00:11:42,079
example you have a blue cluster with 118

00:11:39,760 --> 00:11:42,880
118 and all the different components api

00:11:42,079 --> 00:11:45,920
server

00:11:42,880 --> 00:11:48,480
server cube controller manager the cloud

00:11:45,920 --> 00:11:48,480
controller

00:11:48,640 --> 00:11:52,560
and then you have a brand new cluster

00:11:50,959 --> 00:11:53,920
with 119

00:11:52,560 --> 00:11:57,839
again all these different components

00:11:53,920 --> 00:12:00,000
within 119 version

00:11:57,839 --> 00:12:01,760
so here you have the clusters side by

00:12:00,000 --> 00:12:03,920
side um

00:12:01,760 --> 00:12:05,120
those separate your workloads are

00:12:03,920 --> 00:12:08,240
separate uh

00:12:05,120 --> 00:12:11,680
what you do is typically

00:12:08,240 --> 00:12:14,720
uh you just create the

00:12:11,680 --> 00:12:15,600
the same workloads on in this example

00:12:14,720 --> 00:12:18,720
00:12:15,600 --> 00:12:21,839
you create those same workloads on 119

00:12:18,720 --> 00:12:23,920
so you have them ready there uh

00:12:21,839 --> 00:12:25,120
for something to be switched over right

00:12:23,920 --> 00:12:28,720
so uh

00:12:25,120 --> 00:12:31,839
and then also that gives you opportunity

00:12:28,720 --> 00:12:34,000
to test what's in 119 make sure that

00:12:31,839 --> 00:12:35,839
your pots are not crashing

00:12:34,000 --> 00:12:37,600
make sure that your traffic is flowing

00:12:35,839 --> 00:12:40,720
seamlessly make sure you

00:12:37,600 --> 00:12:43,920
you're running those qa automation tests

00:12:40,720 --> 00:12:47,200
so you have that ahead of time and then

00:12:43,920 --> 00:12:49,120
what once you do all the verification

00:12:47,200 --> 00:12:50,720
it makes you make sure everything is

00:12:49,120 --> 00:12:53,040
running the way you want to

00:12:50,720 --> 00:12:54,560
you can just simply get rid of the old

00:12:53,040 --> 00:12:56,399
cluster

00:12:54,560 --> 00:12:59,360
after you do whatever switch over you

00:12:56,399 --> 00:13:02,560
need to do a traffic switchover or data

00:12:59,360 --> 00:13:05,360
or data migration depending on what

00:13:02,560 --> 00:13:05,360
you're trying to do

00:13:05,760 --> 00:13:08,880
in the same way if you have the data

00:13:08,320 --> 00:13:11,519
plane

00:13:08,880 --> 00:13:12,240
you have all these kubernetes nodes in

00:13:11,519 --> 00:13:14,639
00:13:12,240 --> 00:13:16,959
and also you create all these brand new

00:13:14,639 --> 00:13:20,720
nodes in 119 that are talking

00:13:16,959 --> 00:13:23,120
to that 119 control plane

00:13:20,720 --> 00:13:24,399
and then the same way you start the the

00:13:23,120 --> 00:13:27,519
workloads on those

00:13:24,399 --> 00:13:29,200
nodes on 119 exactly as 118

00:13:27,519 --> 00:13:30,800
and then once you're done you're

00:13:29,200 --> 00:13:34,000
switching over that traffic you

00:13:30,800 --> 00:13:36,000
you can just get rid of those 118 notes

00:13:34,000 --> 00:13:37,839
this is all possible because there's a

00:13:36,000 --> 00:13:38,639
lot of elasticity on all these cloud

00:13:37,839 --> 00:13:40,079
providers

00:13:38,639 --> 00:13:41,680
you know where you can just bring up a

00:13:40,079 --> 00:13:42,480
lot of infrastructure you can tear it

00:13:41,680 --> 00:13:46,000
down

00:13:42,480 --> 00:13:46,000
and have all this automation

00:13:47,040 --> 00:13:51,600
so other ways or other tools that you

00:13:48,800 --> 00:13:54,720
can use for uh blue and green cluster

00:13:51,600 --> 00:13:56,959
deployments you can use um

00:13:54,720 --> 00:13:58,240
a feature from the kubernetes community

00:13:56,959 --> 00:14:00,000
called federation

00:13:58,240 --> 00:14:02,320
that allows you to have kubernetes

00:14:00,000 --> 00:14:05,440
clusters in multiple regions

00:14:02,320 --> 00:14:06,160
right now it's in alpha so you can use

00:14:05,440 --> 00:14:08,639
it in a way

00:14:06,160 --> 00:14:10,240
that you know you have clusters maybe in

00:14:08,639 --> 00:14:13,920
the same data center but

00:14:10,240 --> 00:14:13,920
but you have them side by side

00:14:14,079 --> 00:14:17,680
and then linker d also provide has this

00:14:17,199 --> 00:14:20,720
new

00:14:17,680 --> 00:14:21,680
uh feature called cluster mirroring that

00:14:20,720 --> 00:14:25,120
allows you to

00:14:21,680 --> 00:14:27,040
um think uh or allows kubernetes or

00:14:25,120 --> 00:14:31,839
kubernetes cluster to think

00:14:27,040 --> 00:14:31,839
that it has a service um

00:14:32,240 --> 00:14:39,440
sorry about that

00:14:33,420 --> 00:14:45,839
[Music]

00:14:39,440 --> 00:14:45,839
apologies for this

00:14:52,480 --> 00:15:00,079
okay so that and so you can use this um

00:14:56,839 --> 00:15:04,079
um way that kubernetes to think

00:15:00,079 --> 00:15:06,000
uh that the service is local but um

00:15:04,079 --> 00:15:08,000
in reality that service is in some other

00:15:06,000 --> 00:15:10,720
cluster right so this a cluster

00:15:08,000 --> 00:15:13,600
mirroring allows you to do that

00:15:10,720 --> 00:15:14,959
then you have a cluster api which is a

00:15:13,600 --> 00:15:17,760
open source project

00:15:14,959 --> 00:15:19,920
um from the kubernetes community too and

00:15:17,760 --> 00:15:21,920
it's using kubernetes itself

00:15:19,920 --> 00:15:23,120
to manage all these different clusters

00:15:21,920 --> 00:15:24,800
right so you can have your free the

00:15:23,120 --> 00:15:26,800
clusters you can create your blue

00:15:24,800 --> 00:15:28,160
blue cluster and then your green cluster

00:15:26,800 --> 00:15:31,839
and manage this

00:15:28,160 --> 00:15:31,839
through something like cluster api

00:15:32,320 --> 00:15:36,079
now let's talk about some tools open

00:15:34,480 --> 00:15:38,839
source tools that you can use for these

00:15:36,079 --> 00:15:40,880
kubernetes cluster

00:15:38,839 --> 00:15:43,600
upgrades

00:15:40,880 --> 00:15:44,560
so whenever it comes to automation uh

00:15:43,600 --> 00:15:48,079
you can use

00:15:44,560 --> 00:15:48,560
any language so you can use bash python

00:15:48,079 --> 00:15:50,880
go

00:15:48,560 --> 00:15:53,600
whatever but typically you want to

00:15:50,880 --> 00:15:56,880
orchestrate what's in your

00:15:53,600 --> 00:15:58,800
kubernetes cluster add-ons right so

00:15:56,880 --> 00:16:00,000
there are many things besides just the

00:15:58,800 --> 00:16:01,519
control plane

00:16:00,000 --> 00:16:05,279
the in the data plane that you need to

00:16:01,519 --> 00:16:05,279
run in a cluster so you have your

00:16:06,000 --> 00:16:10,079
mechanism for authenticating you you

00:16:08,079 --> 00:16:11,120
have your nginx ingress controller you

00:16:10,079 --> 00:16:13,920
have your

00:16:11,120 --> 00:16:14,959
monitoring tools you have your log

00:16:13,920 --> 00:16:16,639
aggregator

00:16:14,959 --> 00:16:19,519
and your cluster auto scaler for

00:16:16,639 --> 00:16:21,199
automatically scaling kubernetes nodes

00:16:19,519 --> 00:16:23,120
all those different components you can

00:16:21,199 --> 00:16:26,320
actually automate these

00:16:23,120 --> 00:16:26,959
with just simple uh programming right so

00:16:26,320 --> 00:16:31,120
you have

00:16:26,959 --> 00:16:33,440
like like any language of your choice so

00:16:31,120 --> 00:16:34,720
that's typically the case that you for

00:16:33,440 --> 00:16:37,920
for you to

00:16:34,720 --> 00:16:39,839
automate some of these components

00:16:37,920 --> 00:16:42,000
there are other tools though so uh you

00:16:39,839 --> 00:16:45,600
have the terraform

00:16:42,000 --> 00:16:47,519
that helps you create clusters across

00:16:45,600 --> 00:16:48,880
multiple cloud providers or maybe your

00:16:47,519 --> 00:16:52,320
own cloud provider

00:16:48,880 --> 00:16:56,000
and maybe even dismantled offerings from

00:16:52,320 --> 00:16:58,720
the cloud providers like aws eks

00:16:56,000 --> 00:16:59,519
uh also there's another tool from

00:16:58,720 --> 00:17:02,560
webworks

00:16:59,519 --> 00:17:05,199
called eksctl specifically meant for

00:17:02,560 --> 00:17:06,799
eks but you can use it to define your

00:17:05,199 --> 00:17:09,919
clusters in a

00:17:06,799 --> 00:17:11,280
easy to read yaml format easy to read

00:17:09,919 --> 00:17:15,360
some people think it's easy

00:17:11,280 --> 00:17:15,360
but it depends on on your preference

00:17:17,120 --> 00:17:20,480
another tool is flux and this is from

00:17:19,360 --> 00:17:23,280
webworks and

00:17:20,480 --> 00:17:24,000
it also allows you to manage these uh

00:17:23,280 --> 00:17:27,199
add-ons

00:17:24,000 --> 00:17:29,760
using a way to have

00:17:27,199 --> 00:17:30,320
some sort of like a get-ups approach to

00:17:29,760 --> 00:17:33,679
manage

00:17:30,320 --> 00:17:35,360
these add-ons so you could have uh

00:17:33,679 --> 00:17:37,760
these components that i talked about

00:17:35,360 --> 00:17:39,039
before like core dns or clos the cluster

00:17:37,760 --> 00:17:42,640
auto scaler

00:17:39,039 --> 00:17:43,520
uh cube verb from running the ends with

00:17:42,640 --> 00:17:44,880
kubernetes

00:17:43,520 --> 00:17:46,559
so all these different components you

00:17:44,880 --> 00:17:50,160
can manage with something like flux

00:17:46,559 --> 00:17:53,600
where they're running with container

00:17:50,160 --> 00:17:55,679
images or container versions of images

00:17:53,600 --> 00:17:57,760
and you can make it so that it watches

00:17:55,679 --> 00:17:59,919
the versions of these container images

00:17:57,760 --> 00:18:00,720
and then it automatically upgrades these

00:17:59,919 --> 00:18:02,720
components

00:18:00,720 --> 00:18:03,840
when a new container version gets

00:18:02,720 --> 00:18:07,520
released

00:18:03,840 --> 00:18:09,600
so you can have the full automation

00:18:07,520 --> 00:18:12,160
so here you can you see that typically

00:18:09,600 --> 00:18:13,840
with uh a flux workflow you have this

00:18:12,160 --> 00:18:15,919
git ops approach would

00:18:13,840 --> 00:18:17,360
use for developers but you don't

00:18:15,919 --> 00:18:20,640
necessarily need to do

00:18:17,360 --> 00:18:23,760
this pushing uh you can have it maybe

00:18:20,640 --> 00:18:25,120
watch some container registry that

00:18:23,760 --> 00:18:26,640
actually

00:18:25,120 --> 00:18:29,840
publishes a new version of one of the

00:18:26,640 --> 00:18:29,840
kubernetes components

00:18:33,760 --> 00:18:39,520
another tool from webworks it's

00:18:36,880 --> 00:18:40,240
kind of early but it's in the works and

00:18:39,520 --> 00:18:42,640
basically

00:18:40,240 --> 00:18:44,799
it allows you to do just plain get offs

00:18:42,640 --> 00:18:46,880
for kubernetes clusters

00:18:44,799 --> 00:18:48,160
so you can keep that state of that

00:18:46,880 --> 00:18:51,360
kubernetes cluster

00:18:48,160 --> 00:18:54,720
in a yaml file and and currently

00:18:51,360 --> 00:18:57,840
it doesn't support a lot of different

00:18:54,720 --> 00:18:59,120
projects or or different uh kubernetes

00:18:57,840 --> 00:19:02,080
distributions or

00:18:59,120 --> 00:19:04,080
kubernetes cloud offerings so there's no

00:19:02,080 --> 00:19:07,760
support for aws

00:19:04,080 --> 00:19:11,039
there is support for gc google um

00:19:07,760 --> 00:19:12,960
google compute engine uh in in support

00:19:11,039 --> 00:19:15,679
for other things like fire cube

00:19:12,960 --> 00:19:16,400
that allows you to run firecracker vms

00:19:15,679 --> 00:19:19,200
uh

00:19:16,400 --> 00:19:21,360
in kubernetes within vagrant that allows

00:19:19,200 --> 00:19:24,720
you to run this uh vms um

00:19:21,360 --> 00:19:27,280
locally but you know it's it's still

00:19:24,720 --> 00:19:27,760
in the works but it's something that you

00:19:27,280 --> 00:19:31,280
can

00:19:27,760 --> 00:19:33,919
keep a lookout uh for if you want to

00:19:31,280 --> 00:19:34,559
maybe manage your clusters in in in a

00:19:33,919 --> 00:19:37,679
yamo

00:19:34,559 --> 00:19:38,960
format get offs yaml type of way just

00:19:37,679 --> 00:19:43,120
like something like uh

00:19:38,960 --> 00:19:43,120
flux does for applications

00:19:45,039 --> 00:19:53,200
another tool is kco and this tool is for

00:19:50,000 --> 00:19:56,320
also managing your kubernetes nodes uh

00:19:53,200 --> 00:19:59,600
it only supports aws

00:19:56,320 --> 00:20:00,080
um the only um yeah the only thing is

00:19:59,600 --> 00:20:03,200
that

00:20:00,080 --> 00:20:05,280
they they're very specific to aws but um

00:20:03,200 --> 00:20:07,520
and it's because it's from intuit and

00:20:05,280 --> 00:20:09,679
they're heavily used in aws but it has

00:20:07,520 --> 00:20:12,159
some nice features that allows you to do

00:20:09,679 --> 00:20:12,880
uh forensic dumps for security it helps

00:20:12,159 --> 00:20:16,320
you manage

00:20:12,880 --> 00:20:19,120
the cost of your cluster uh it

00:20:16,320 --> 00:20:20,000
helps you see the compliance of your

00:20:19,120 --> 00:20:23,039
cluster with no

00:20:20,000 --> 00:20:27,520
failures so it's a good tool especially

00:20:23,039 --> 00:20:30,960
if you're running in aws infrastructure

00:20:27,520 --> 00:20:32,720
so uh this is caico at a glance so

00:20:30,960 --> 00:20:35,200
you have all these different components

00:20:32,720 --> 00:20:39,200
you have your instance manager to

00:20:35,200 --> 00:20:41,039
to manage your your nodes um

00:20:39,200 --> 00:20:42,320
a way to manage your cost with the

00:20:41,039 --> 00:20:44,400
minimum manager

00:20:42,320 --> 00:20:46,960
a way to see the reliability of your

00:20:44,400 --> 00:20:49,039
nodes with the governor

00:20:46,960 --> 00:20:50,159
you can do forensics with cube forensics

00:20:49,039 --> 00:20:52,159
and security

00:20:50,159 --> 00:20:56,000
and you can monitor constantly what the

00:20:52,159 --> 00:20:56,000
behavior is for for these clusters

00:20:57,520 --> 00:21:02,000
so other tools for deploying clusters

00:20:59,520 --> 00:21:05,039
and help you do

00:21:02,000 --> 00:21:06,720
this blue and green approach so this

00:21:05,039 --> 00:21:09,200
they're very popular tools in the

00:21:06,720 --> 00:21:11,520
community like cops

00:21:09,200 --> 00:21:13,200
it's basically deploying kubernetes

00:21:11,520 --> 00:21:16,080
clusters uh

00:21:13,200 --> 00:21:18,159
in different cloud providers it's

00:21:16,080 --> 00:21:22,559
written in golang so it supports

00:21:18,159 --> 00:21:26,240
google cloud it supports uh aws

00:21:22,559 --> 00:21:28,320
i believe it also supports azure so

00:21:26,240 --> 00:21:30,080
it's a very good tool to manage clusters

00:21:28,320 --> 00:21:32,799
across multiple clouds

00:21:30,080 --> 00:21:33,280
and you can create multiple uh blue and

00:21:32,799 --> 00:21:35,200
green

00:21:33,280 --> 00:21:36,880
clusters when you want to do the the

00:21:35,200 --> 00:21:38,960
cluster upgrade

00:21:36,880 --> 00:21:40,640
cube sprayed is another similar tool the

00:21:38,960 --> 00:21:42,480
only difference or the only main

00:21:40,640 --> 00:21:44,720
difference between that and cops

00:21:42,480 --> 00:21:46,159
is that it's written in ansible so he

00:21:44,720 --> 00:21:48,799
prefers something enanciable

00:21:46,159 --> 00:21:49,840
you can use something like you spray and

00:21:48,799 --> 00:21:52,000
then link rd

00:21:49,840 --> 00:21:52,960
i mentioned this before briefly but then

00:21:52,000 --> 00:21:55,440
it allows you to

00:21:52,960 --> 00:21:56,400
uh switch over uh the traffic more

00:21:55,440 --> 00:21:58,559
seamlessly

00:21:56,400 --> 00:22:00,559
by making kubernetes think the the

00:21:58,559 --> 00:22:04,640
service sits on the local cluster but in

00:22:00,559 --> 00:22:04,640
reality it sits on on a remote cluster

00:22:06,000 --> 00:22:12,480
another interesting tool is pluto

00:22:09,200 --> 00:22:16,159
uh in this tool

00:22:12,480 --> 00:22:20,080
allows you to see all the different

00:22:16,159 --> 00:22:24,320
uh or the main kubernetes components

00:22:20,080 --> 00:22:26,559
with their api groups and versions

00:22:24,320 --> 00:22:27,440
and when they're going to be deprecated

00:22:26,559 --> 00:22:29,520
and meaning

00:22:27,440 --> 00:22:31,200
what kubernetes version main kubernetes

00:22:29,520 --> 00:22:31,760
version they're going to be deprecated

00:22:31,200 --> 00:22:35,520
in

00:22:31,760 --> 00:22:38,400
and when they're going to be removed so

00:22:35,520 --> 00:22:40,880
for example here you have deployment

00:22:38,400 --> 00:22:44,080
extensions b1 beta1

00:22:40,880 --> 00:22:44,640
uh deprecated in 190 and then completely

00:22:44,080 --> 00:22:48,080
remove

00:22:44,640 --> 00:22:49,919
in 116 and the replace replacement for

00:22:48,080 --> 00:22:52,880
that is apps b1

00:22:49,919 --> 00:22:54,400
meaning if you have a deployment uh with

00:22:52,880 --> 00:22:57,360
the definition

00:22:54,400 --> 00:22:58,720
of extensions b1 beta 1 and then you're

00:22:57,360 --> 00:23:01,679
running

00:22:58,720 --> 00:23:02,880
115 and then the moment that you upgrade

00:23:01,679 --> 00:23:07,120
to 1 uh

00:23:02,880 --> 00:23:08,720
16 that definition is not there anymore

00:23:07,120 --> 00:23:11,120
which means that when the deployment

00:23:08,720 --> 00:23:12,799
wants to start or once wants to change

00:23:11,120 --> 00:23:14,720
uh something that wants to restart its

00:23:12,799 --> 00:23:15,360
spot then it won't be able to do it

00:23:14,720 --> 00:23:17,679
right because

00:23:15,360 --> 00:23:18,559
yeah it's not going to be supported so

00:23:17,679 --> 00:23:20,640
this tool

00:23:18,559 --> 00:23:22,480
actually allows you to see this ahead of

00:23:20,640 --> 00:23:23,600
time and and i touched on this in the

00:23:22,480 --> 00:23:25,360
beginning of the talk

00:23:23,600 --> 00:23:27,039
but here you can see all of these

00:23:25,360 --> 00:23:29,200
different main

00:23:27,039 --> 00:23:30,960
kubernetes components like stateful sets

00:23:29,200 --> 00:23:33,440
ingress controllers demonstrate

00:23:30,960 --> 00:23:33,440
etc

00:23:34,720 --> 00:23:38,320
so i have a question here but i'll get

00:23:36,480 --> 00:23:40,640
to it at the end of the talk

00:23:38,320 --> 00:23:42,240
so what does it look like in in

00:23:40,640 --> 00:23:43,279
production or when you have all these

00:23:42,240 --> 00:23:47,120
different components

00:23:43,279 --> 00:23:47,760
in production so yeah so you want to

00:23:47,120 --> 00:23:50,080
have

00:23:47,760 --> 00:23:51,840
most of the components automated it's

00:23:50,080 --> 00:23:54,559
this is the ideal world

00:23:51,840 --> 00:23:57,039
i think a lot of the people don't uh

00:23:54,559 --> 00:24:00,080
unless you're a main cloud provider

00:23:57,039 --> 00:24:02,640
but you want to get to that point

00:24:00,080 --> 00:24:04,640
so you want to fully um automate that

00:24:02,640 --> 00:24:05,440
control plane the data planes to bring

00:24:04,640 --> 00:24:08,720
up the whole

00:24:05,440 --> 00:24:11,200
new uh set of kubernetes main main nodes

00:24:08,720 --> 00:24:13,679
and kubernetes uh worker nodes

00:24:11,200 --> 00:24:14,720
uh also the add-ons uh cluster

00:24:13,679 --> 00:24:17,039
autoscaler

00:24:14,720 --> 00:24:18,880
uh core dns everything in full

00:24:17,039 --> 00:24:21,760
automation

00:24:18,880 --> 00:24:23,840
and then maybe you want to uh have

00:24:21,760 --> 00:24:26,080
manage your stateless applications

00:24:23,840 --> 00:24:27,279
in an automated way so you have your ci

00:24:26,080 --> 00:24:30,960
cd system

00:24:27,279 --> 00:24:32,960
that deploys the full application

00:24:30,960 --> 00:24:34,240
uh on the brand new clusters you have

00:24:32,960 --> 00:24:36,080
you just like you do

00:24:34,240 --> 00:24:37,360
blue and green deploy blue and green

00:24:36,080 --> 00:24:39,919
deployments of your

00:24:37,360 --> 00:24:40,799
applications you can do a blue

00:24:39,919 --> 00:24:42,720
deployment

00:24:40,799 --> 00:24:44,640
of your application on the blue cluster

00:24:42,720 --> 00:24:46,480
and it can do a green deployment

00:24:44,640 --> 00:24:48,320
of that specific application on the

00:24:46,480 --> 00:24:50,480
green cluster and then

00:24:48,320 --> 00:24:52,159
you can do the switch over afterwards

00:24:50,480 --> 00:24:54,080
when after you do all the

00:24:52,159 --> 00:24:56,640
verification you've run all your qa

00:24:54,080 --> 00:24:59,440
tasks also in a fully automated way

00:24:56,640 --> 00:25:00,000
so ideally you want to have everything

00:24:59,440 --> 00:25:01,679
automated

00:25:00,000 --> 00:25:03,679
especially in production you know if

00:25:01,679 --> 00:25:06,720
you're handling lots and lots of

00:25:03,679 --> 00:25:06,720
kubernetes clusters

00:25:08,480 --> 00:25:15,679
so what about staple applications uh

00:25:11,840 --> 00:25:19,200
so these are looked a little bit

00:25:15,679 --> 00:25:21,919
more tricky or trickier to handle

00:25:19,200 --> 00:25:23,760
because uh you you're actually uh

00:25:21,919 --> 00:25:26,360
dealing with the data so

00:25:23,760 --> 00:25:29,440
typically typically you want to schedule

00:25:26,360 --> 00:25:31,919
a maintenance window

00:25:29,440 --> 00:25:34,000
uh and so make sure everything is okay

00:25:31,919 --> 00:25:35,600
but i think one important aspect is that

00:25:34,000 --> 00:25:38,640
you also need want to back up

00:25:35,600 --> 00:25:39,440
all your data that's where maybe you're

00:25:38,640 --> 00:25:41,919
keeping the cost

00:25:39,440 --> 00:25:42,640
the the data for your customers right so

00:25:41,919 --> 00:25:45,600
um

00:25:42,640 --> 00:25:47,679
maybe you're a major website like we are

00:25:45,600 --> 00:25:50,480
then we have to keep

00:25:47,679 --> 00:25:53,360
uh the data there secure and then when

00:25:50,480 --> 00:25:56,000
we do the migration we have to um

00:25:53,360 --> 00:25:57,520
um make sure we don't lose it so it's

00:25:56,000 --> 00:26:00,720
you know back up that data

00:25:57,520 --> 00:26:01,440
constantly uh so if you're using a cloud

00:26:00,720 --> 00:26:05,039
provider

00:26:01,440 --> 00:26:07,360
or uh an advanced storage uh mechanism

00:26:05,039 --> 00:26:09,440
you can you make use of these snapshots

00:26:07,360 --> 00:26:12,480
for volumes

00:26:09,440 --> 00:26:14,960
and instead of just like um

00:26:12,480 --> 00:26:15,600
moving the data doing raw copy of your

00:26:14,960 --> 00:26:17,679
data

00:26:15,600 --> 00:26:19,840
you can just snapshot these volumes and

00:26:17,679 --> 00:26:21,200
then create a whole instance of that new

00:26:19,840 --> 00:26:23,600
volume somewhere else

00:26:21,200 --> 00:26:24,240
and start with the with the new uh

00:26:23,600 --> 00:26:27,600
kubernetes

00:26:24,240 --> 00:26:29,360
cluster then create multiple replicas if

00:26:27,600 --> 00:26:33,120
you're using something like uh

00:26:29,360 --> 00:26:35,960
mysql so mysql allows you to have

00:26:33,120 --> 00:26:37,440
multiple read replicas uh and

00:26:35,960 --> 00:26:39,200
[Music]

00:26:37,440 --> 00:26:41,200
when you do that cluster switchover

00:26:39,200 --> 00:26:44,640
maybe you can

00:26:41,200 --> 00:26:46,720
create a a read replica and then when

00:26:44,640 --> 00:26:48,880
at the moment of doing that switch over

00:26:46,720 --> 00:26:51,919
you change that master

00:26:48,880 --> 00:26:53,520
from the old cluster to the new cluster

00:26:51,919 --> 00:26:55,200
and

00:26:53,520 --> 00:26:58,320
if you have multiple masters which is

00:26:55,200 --> 00:27:01,760
not that typical you can also do this by

00:26:58,320 --> 00:27:04,640
by um moving these one by one

00:27:01,760 --> 00:27:06,000
uh for e for um for each cluster so it's

00:27:04,640 --> 00:27:09,120
so if for the old cluster

00:27:06,000 --> 00:27:11,360
and the new cluster so this is very

00:27:09,120 --> 00:27:15,200
critical because of your data and it's

00:27:11,360 --> 00:27:15,919
it's a lot uh more tricky than than just

00:27:15,200 --> 00:27:18,840
the

00:27:15,919 --> 00:27:21,039
the regular stake state less

00:27:18,840 --> 00:27:24,000
applications

00:27:21,039 --> 00:27:26,000
so what does it look like for the uh

00:27:24,000 --> 00:27:26,799
these applications these tools this

00:27:26,000 --> 00:27:30,399
kubernetes

00:27:26,799 --> 00:27:30,399
cluster upgrades in the future

00:27:31,039 --> 00:27:35,120
so we'll see more more of these tools to

00:27:33,360 --> 00:27:38,480
fill some of the gaps

00:27:35,120 --> 00:27:40,399
open source tools and even some vendors

00:27:38,480 --> 00:27:41,919
that help you do that automatics uh

00:27:40,399 --> 00:27:44,000
traffic switch over so

00:27:41,919 --> 00:27:45,840
say you're handling uh millions of

00:27:44,000 --> 00:27:47,440
millions of requests or

00:27:45,840 --> 00:27:49,840
billions of requests and then you want

00:27:47,440 --> 00:27:51,440
to do that switch over maybe at the dns

00:27:49,840 --> 00:27:53,679
provider when you when you do that

00:27:51,440 --> 00:27:55,120
cluster upgrade

00:27:53,679 --> 00:27:57,679
so maybe we'll have these tools that

00:27:55,120 --> 00:28:02,080
automatically change that dns

00:27:57,679 --> 00:28:05,520
entry or um talk to that api uh

00:28:02,080 --> 00:28:08,000
service like uh for example route 53 553

00:28:05,520 --> 00:28:10,480
and in aws and change that that that

00:28:08,000 --> 00:28:13,679
today to the new endpoint that has the

00:28:10,480 --> 00:28:16,080
that is running in the new cluster and

00:28:13,679 --> 00:28:18,080
maybe we'll see more tools that help you

00:28:16,080 --> 00:28:21,520
have that data consistency

00:28:18,080 --> 00:28:23,840
across multiple stateful uh

00:28:21,520 --> 00:28:25,279
applications or within the same stateful

00:28:23,840 --> 00:28:25,840
application but when you're migrating

00:28:25,279 --> 00:28:28,399
that

00:28:25,840 --> 00:28:30,320
kubernetes uh cluster in moving that

00:28:28,399 --> 00:28:31,600
application to that new kubernetes

00:28:30,320 --> 00:28:33,520
cluster so

00:28:31,600 --> 00:28:35,120
make sure that you have the backup of

00:28:33,520 --> 00:28:37,600
your data and make sure that

00:28:35,120 --> 00:28:40,240
that you it does the master switch over

00:28:37,600 --> 00:28:42,960
if you're using something like a mysql

00:28:40,240 --> 00:28:44,000
so we'll see more of that uh so we'll

00:28:42,960 --> 00:28:47,039
see more more more

00:28:44,000 --> 00:28:50,080
tools maybe that help you monitor when

00:28:47,039 --> 00:28:53,120
these upgrades uh happen for the add-ons

00:28:50,080 --> 00:28:55,200
uh so so did we actually upgrade the

00:28:53,120 --> 00:28:57,520
cluster autoscaler does it have the

00:28:55,200 --> 00:28:58,880
right version is it not crashing because

00:28:57,520 --> 00:29:02,159
uh it's it's talking to

00:28:58,880 --> 00:29:03,200
an older api uh version so all these

00:29:02,159 --> 00:29:06,320
different things are the

00:29:03,200 --> 00:29:10,480
in terms of monitoring the another

00:29:06,320 --> 00:29:13,120
important aspect here is security so um

00:29:10,480 --> 00:29:15,279
there are a lot of uh vulnerabilities or

00:29:13,120 --> 00:29:18,159
cves that actually get published for

00:29:15,279 --> 00:29:20,000
say kubernetes so so you want to check

00:29:18,159 --> 00:29:21,840
these

00:29:20,000 --> 00:29:23,039
before you do your upgrade and after you

00:29:21,840 --> 00:29:25,279
upgrade uh

00:29:23,039 --> 00:29:26,240
you also want to check your container

00:29:25,279 --> 00:29:29,840
images

00:29:26,240 --> 00:29:29,840
that are not having any

00:29:29,919 --> 00:29:33,600
code that could be maybe exposed to

00:29:32,240 --> 00:29:35,360
vulnerabilities

00:29:33,600 --> 00:29:37,039
so if you're using a runtime of a

00:29:35,360 --> 00:29:38,159
programming language that has some

00:29:37,039 --> 00:29:40,000
vulnerabilities

00:29:38,159 --> 00:29:41,840
you can use image scanners for example

00:29:40,000 --> 00:29:44,080
but we'll see more of these tools that

00:29:41,840 --> 00:29:47,520
help you integrate a lot of these uh

00:29:44,080 --> 00:29:50,000
security mechanisms uh so

00:29:47,520 --> 00:29:52,000
another aspect maybe where we'll see

00:29:50,000 --> 00:29:53,679
more uh innovation and changes is the

00:29:52,000 --> 00:29:55,279
service meshing

00:29:53,679 --> 00:29:56,960
in multi-cluster we talked about

00:29:55,279 --> 00:30:00,720
federation but what about

00:29:56,960 --> 00:30:02,799
uh having a microservice in in different

00:30:00,720 --> 00:30:04,720
spread across different clusters and

00:30:02,799 --> 00:30:05,440
having this seamlessly switch over

00:30:04,720 --> 00:30:08,320
between

00:30:05,440 --> 00:30:09,440
the different clusters and and do the

00:30:08,320 --> 00:30:11,440
rebalancing

00:30:09,440 --> 00:30:12,559
if you don't want to have uh all these

00:30:11,440 --> 00:30:15,600
services uh

00:30:12,559 --> 00:30:17,679
um uh you know in just a single cluster

00:30:15,600 --> 00:30:20,720
or you want to get rid of them in

00:30:17,679 --> 00:30:24,399
in in the old cluster

00:30:20,720 --> 00:30:27,039
and then maybe operator uh uh

00:30:24,399 --> 00:30:29,520
there's a lot of uh operators in the

00:30:27,039 --> 00:30:32,640
ecosystem now kubernetes operators

00:30:29,520 --> 00:30:34,240
and then some of these may be aware of

00:30:32,640 --> 00:30:36,080
when you do the cluster upgrade right

00:30:34,240 --> 00:30:38,240
now i believe most of them

00:30:36,080 --> 00:30:39,840
are just focused on just managing maybe

00:30:38,240 --> 00:30:41,600
like a stateful application within

00:30:39,840 --> 00:30:44,799
kubernetes like uh like a

00:30:41,600 --> 00:30:47,600
like an apache kafka or like um

00:30:44,799 --> 00:30:48,880
like a database like a a cassandra

00:30:47,600 --> 00:30:51,600
cluster or something

00:30:48,880 --> 00:30:53,360
like staple but then maybe some of these

00:30:51,600 --> 00:30:57,120
will become more aware of cluster

00:30:53,360 --> 00:30:57,600
upgrades we'll see more innovations with

00:30:57,120 --> 00:31:00,080
the

00:30:57,600 --> 00:31:01,679
in the cloud providers too so we'll see

00:31:00,080 --> 00:31:03,919
for example in eks

00:31:01,679 --> 00:31:04,720
they're talking more about doing that

00:31:03,919 --> 00:31:06,320
full

00:31:04,720 --> 00:31:07,919
one click cluster upgrade with all the

00:31:06,320 --> 00:31:11,200
different verifications that

00:31:07,919 --> 00:31:14,480
need to happen uh notifications uh

00:31:11,200 --> 00:31:17,760
when there's a patch so and then

00:31:14,480 --> 00:31:19,039
uh also more ways to manage these

00:31:17,760 --> 00:31:21,679
kubernetes nodes

00:31:19,039 --> 00:31:22,320
uh so make sure that you actually patch

00:31:21,679 --> 00:31:23,919
up your

00:31:22,320 --> 00:31:25,679
operating system and your kubernetes

00:31:23,919 --> 00:31:28,399
node or patch up the

00:31:25,679 --> 00:31:29,360
the kubernetes miner version right so

00:31:28,399 --> 00:31:30,960
the cube light

00:31:29,360 --> 00:31:33,440
has this minor version that gets

00:31:30,960 --> 00:31:35,279
released very often so you may want to

00:31:33,440 --> 00:31:37,360
the cloud providers will facilitate that

00:31:35,279 --> 00:31:40,399
on at the kubernetes node level or

00:31:37,360 --> 00:31:40,399
kubernetes worker

00:31:40,960 --> 00:31:45,120
so what can you get out of all of this

00:31:42,799 --> 00:31:45,120
um

00:31:45,760 --> 00:31:48,880
so when you're doing the the cluster

00:31:48,240 --> 00:31:52,240
upgrades

00:31:48,880 --> 00:31:52,559
uh always kind of start small begin with

00:31:52,240 --> 00:31:55,360
a

00:31:52,559 --> 00:31:58,080
like a small set of your applications

00:31:55,360 --> 00:32:00,720
start with a small cluster

00:31:58,080 --> 00:32:01,200
and you always want to test ahead of

00:32:00,720 --> 00:32:03,600
time

00:32:01,200 --> 00:32:04,720
uh so you always want to have a qa type

00:32:03,600 --> 00:32:07,360
of environment

00:32:04,720 --> 00:32:08,960
never actually you know do this upgrades

00:32:07,360 --> 00:32:10,720
in production especially if you're doing

00:32:08,960 --> 00:32:14,240
in-place upgrades

00:32:10,720 --> 00:32:16,640
um you also want to let continue

00:32:14,240 --> 00:32:18,640
leveraging all these open source tools

00:32:16,640 --> 00:32:20,720
lots of them i mentioned

00:32:18,640 --> 00:32:22,240
a lot of them there actually there are

00:32:20,720 --> 00:32:23,679
quite a few other tools that you can use

00:32:22,240 --> 00:32:27,279
that i didn't mention

00:32:23,679 --> 00:32:29,279
uh in this example you know terraform

00:32:27,279 --> 00:32:31,200
with kco to manage your kubernetes node

00:32:29,279 --> 00:32:33,679
and terraform to manage your

00:32:31,200 --> 00:32:35,279
your kubernetes uh creation cluster

00:32:33,679 --> 00:32:36,960
creation

00:32:35,279 --> 00:32:38,880
or you can use something like flux or

00:32:36,960 --> 00:32:42,000
terraform

00:32:38,880 --> 00:32:44,000
then uh when you create

00:32:42,000 --> 00:32:45,760
that that green cluster with uh we

00:32:44,000 --> 00:32:47,919
talked about the standard practice of

00:32:45,760 --> 00:32:50,320
having the same kubernetes version

00:32:47,919 --> 00:32:51,360
for all the different components so make

00:32:50,320 --> 00:32:53,919
sure you never

00:32:51,360 --> 00:32:55,120
mix these versions because that actually

00:32:53,919 --> 00:32:56,960
complicates things

00:32:55,120 --> 00:32:58,880
you always want to keep your cube api

00:32:56,960 --> 00:33:00,480
server your cube controller manager

00:32:58,880 --> 00:33:03,440
your cubelet all these different

00:33:00,480 --> 00:33:06,320
components in in the kubernetes uh

00:33:03,440 --> 00:33:08,480
that are part of the kubernetes uh

00:33:06,320 --> 00:33:11,519
cluster

00:33:08,480 --> 00:33:13,440
the same version you can use also this

00:33:11,519 --> 00:33:16,320
tool called cube ctl convert

00:33:13,440 --> 00:33:18,320
that it helps you convert uh the api

00:33:16,320 --> 00:33:21,919
manifests or the

00:33:18,320 --> 00:33:24,320
kubernetes manifests from one version to

00:33:21,919 --> 00:33:25,919
another so you can migrate

00:33:24,320 --> 00:33:27,679
your old definitions to your new

00:33:25,919 --> 00:33:30,159
definitions and hopefully

00:33:27,679 --> 00:33:31,360
when you create that green new cluster

00:33:30,159 --> 00:33:33,519
you don't have any

00:33:31,360 --> 00:33:34,880
any problems with with talking to the

00:33:33,519 --> 00:33:38,960
api server

00:33:34,880 --> 00:33:41,200
the cube api server always want to test

00:33:38,960 --> 00:33:43,519
test run your qa automation ahead of

00:33:41,200 --> 00:33:46,320
time before you do that switch over

00:33:43,519 --> 00:33:48,320
and always back up your data especially

00:33:46,320 --> 00:33:50,480
if you're running stateful applications

00:33:48,320 --> 00:33:52,000
uh i mentioned that this is very

00:33:50,480 --> 00:33:54,720
important because this is where you have

00:33:52,000 --> 00:33:56,080
your customer data and this is where uh

00:33:54,720 --> 00:33:59,120
maybe

00:33:56,080 --> 00:34:00,480
if you're running a business you uh

00:33:59,120 --> 00:34:03,039
maybe one of the most critical

00:34:00,480 --> 00:34:03,039
components

00:34:04,559 --> 00:34:08,399
yeah with that i i have some references

00:34:07,120 --> 00:34:11,919
here to some of the

00:34:08,399 --> 00:34:14,399
the projects that i mentioned and

00:34:11,919 --> 00:34:15,359
yeah and thank you i think that's that's

00:34:14,399 --> 00:34:17,599
all i have

00:34:15,359 --> 00:34:18,560
for now um i love to talk about this

00:34:17,599 --> 00:34:19,919
topic and

00:34:18,560 --> 00:34:21,919
you know we're constantly using

00:34:19,919 --> 00:34:23,839
kubernetes and

00:34:21,919 --> 00:34:25,679
uh doing upgrades so if you have any

00:34:23,839 --> 00:34:28,639
problems with with them and if

00:34:25,679 --> 00:34:29,040
a lot to chat about what solutions you

00:34:28,639 --> 00:34:32,320
you

00:34:29,040 --> 00:34:34,639
actually found and

00:34:32,320 --> 00:34:35,359
and and what challenges you've actually

00:34:34,639 --> 00:34:38,480
come across

00:34:35,359 --> 00:34:40,079
uh and that that may need solutions so

00:34:38,480 --> 00:34:43,119
thank you so with that i'll

00:34:40,079 --> 00:34:43,119
i'll take some questions

00:34:43,280 --> 00:34:52,480
so i think i have

00:34:49,040 --> 00:34:55,839
a question here so um

00:34:52,480 --> 00:34:58,640
are there any work any any workloads

00:34:55,839 --> 00:35:00,640
not suited for blue green deployment

00:34:58,640 --> 00:35:05,599
thinking maybe clusters with large

00:35:00,640 --> 00:35:07,520
stable apps yeah so

00:35:05,599 --> 00:35:09,760
i think this question came before i

00:35:07,520 --> 00:35:10,720
mentioned okay so he mentioned that i

00:35:09,760 --> 00:35:13,839
covered it pretty well

00:35:10,720 --> 00:35:16,560
yeah stateful applications yeah they're

00:35:13,839 --> 00:35:17,280
more of a challenge uh it can still be

00:35:16,560 --> 00:35:20,320
done

00:35:17,280 --> 00:35:21,359
uh but then you you have to be maybe

00:35:20,320 --> 00:35:25,280
schedule some

00:35:21,359 --> 00:35:27,520
uh downtime or maintenance window

00:35:25,280 --> 00:35:28,720
and and you have to make use of all

00:35:27,520 --> 00:35:32,880
these different tools

00:35:28,720 --> 00:35:35,599
for the different uh staple type of uh

00:35:32,880 --> 00:35:35,920
application that you're running so for

00:35:35,599 --> 00:35:38,320
say

00:35:35,920 --> 00:35:39,440
like mysql or postgres you have to

00:35:38,320 --> 00:35:43,599
create multiple

00:35:39,440 --> 00:35:45,680
replicas uh if you are storing on disk

00:35:43,599 --> 00:35:47,599
maybe use something like a volume like

00:35:45,680 --> 00:35:50,640
an elastic volume and then you

00:35:47,599 --> 00:35:53,599
make use of snapshots with

00:35:50,640 --> 00:35:56,160
something like cassandra database maybe

00:35:53,599 --> 00:35:58,720
you want to have

00:35:56,160 --> 00:36:00,079
expand the cluster and have a tool to

00:35:58,720 --> 00:36:01,760
rebalance your cluster

00:36:00,079 --> 00:36:03,520
of course if you have a lot of data this

00:36:01,760 --> 00:36:06,079
becomes really challenging

00:36:03,520 --> 00:36:08,160
because uh to rebalance your cluster it

00:36:06,079 --> 00:36:10,079
may take a lot of time

00:36:08,160 --> 00:36:11,200
but then yeah you have to keep thinking

00:36:10,079 --> 00:36:12,480
about these and

00:36:11,200 --> 00:36:14,320
and there are some other things that you

00:36:12,480 --> 00:36:15,520
have you can think about maybe like

00:36:14,320 --> 00:36:17,760
sharding your data

00:36:15,520 --> 00:36:18,640
so that that is not relying on a single

00:36:17,760 --> 00:36:20,560
cluster

00:36:18,640 --> 00:36:22,240
so you do it bit by bit so those are

00:36:20,560 --> 00:36:24,839
some of the architectural decisions that

00:36:22,240 --> 00:36:27,839
you have to weigh but it's a lot more

00:36:24,839 --> 00:36:27,839
challenging

00:36:29,040 --> 00:36:35,599
okay so okay i don't

00:36:32,400 --> 00:36:38,720
think i have any more questions

00:36:35,599 --> 00:36:42,720
okay okay there's another question

00:36:38,720 --> 00:36:46,000
would you be hesitant to do blue green

00:36:42,720 --> 00:36:47,680
if you don't own the data another team

00:36:46,000 --> 00:36:51,119
is using a cluster

00:36:47,680 --> 00:36:51,119
you manage

00:36:51,359 --> 00:36:57,280
yeah i mean that i will be a little bit

00:36:55,280 --> 00:36:58,960
more concerned about that if i don't

00:36:57,280 --> 00:37:03,440
have all the

00:36:58,960 --> 00:37:08,000
the automation in place but um

00:37:03,440 --> 00:37:10,240
you also have to uh watch what your

00:37:08,000 --> 00:37:11,119
service level agreements are and maybe

00:37:10,240 --> 00:37:13,280
if you are

00:37:11,119 --> 00:37:15,520
internally within an organization if you

00:37:13,280 --> 00:37:16,640
have some sort of slos or service level

00:37:15,520 --> 00:37:20,800
objectives

00:37:16,640 --> 00:37:23,119
uh and then you you can see what is

00:37:20,800 --> 00:37:24,480
risky or less risky to do right

00:37:23,119 --> 00:37:27,680
depending on what you have

00:37:24,480 --> 00:37:29,280
there uh in in and depending on what

00:37:27,680 --> 00:37:32,400
kinds of tasks you act

00:37:29,280 --> 00:37:35,599
you will actually you have actually run

00:37:32,400 --> 00:37:39,200
prior to doing this uh upgrades

00:37:35,599 --> 00:37:40,160
so it is it it when you always when you

00:37:39,200 --> 00:37:43,200
talk about

00:37:40,160 --> 00:37:46,640
stateful applications it's always uh

00:37:43,200 --> 00:37:50,240
uh riskier there's always more risky

00:37:46,640 --> 00:37:52,720
risk in there so yeah

00:37:50,240 --> 00:37:54,400
does that answer your question yeah okay

00:37:52,720 --> 00:37:57,119
yeah

00:37:54,400 --> 00:37:57,119
yeah you're welcome

00:37:58,000 --> 00:38:01,359
okay any other questions

00:38:02,320 --> 00:38:05,599
i think that we have one question from

00:38:04,079 --> 00:38:08,240
ramesh here um

00:38:05,599 --> 00:38:09,920
it looks like he has his hand rose um i

00:38:08,240 --> 00:38:10,320
will allow him to talk here and see if

00:38:09,920 --> 00:38:12,800
he

00:38:10,320 --> 00:38:14,400
can ask this question okay i think i

00:38:12,800 --> 00:38:18,320
have it on the chat

00:38:14,400 --> 00:38:20,079
yeah oh so i have a question regarding

00:38:18,320 --> 00:38:22,480
so if you maintain uh two different

00:38:20,079 --> 00:38:25,280
clusters like uh green and blue

00:38:22,480 --> 00:38:26,160
so keeping the project cost in mind so

00:38:25,280 --> 00:38:28,000
that may

00:38:26,160 --> 00:38:30,560
increase the total cost of the project

00:38:28,000 --> 00:38:33,599
correct so how to solve that problem

00:38:30,560 --> 00:38:36,880
in that case so

00:38:33,599 --> 00:38:37,520
it it will increase the cost of your

00:38:36,880 --> 00:38:41,839
project

00:38:37,520 --> 00:38:44,720
regardless but you can mitigate it by

00:38:41,839 --> 00:38:47,119
by using some of these cloud providers i

00:38:44,720 --> 00:38:48,640
mean if you run your own

00:38:47,119 --> 00:38:50,960
infrastructure it becomes more

00:38:48,640 --> 00:38:53,599
challenging but if you have your own

00:38:50,960 --> 00:38:55,119
infrastructure maybe you already have

00:38:53,599 --> 00:38:58,240
the money to pay for that

00:38:55,119 --> 00:38:59,520
uh so that that may not be too much of a

00:38:58,240 --> 00:39:02,800
challenge there but like

00:38:59,520 --> 00:39:04,880
like if you were maybe a place that

00:39:02,800 --> 00:39:06,160
that is you know constrained

00:39:04,880 --> 00:39:08,079
economically you

00:39:06,160 --> 00:39:09,599
you could um you know just take

00:39:08,079 --> 00:39:09,839
advantage of some of the cloud providers

00:39:09,599 --> 00:39:13,040
and

00:39:09,839 --> 00:39:16,320
their elasticity right where you um

00:39:13,040 --> 00:39:17,119
you basically uh just bring up the brand

00:39:16,320 --> 00:39:19,040
new node

00:39:17,119 --> 00:39:20,320
or the brand new kubernetes cluster and

00:39:19,040 --> 00:39:23,040
and maybe

00:39:20,320 --> 00:39:25,440
it doesn't have to be very if you have a

00:39:23,040 --> 00:39:27,359
lot of workloads on the old cluster

00:39:25,440 --> 00:39:28,720
and then you bring up the new node don't

00:39:27,359 --> 00:39:30,400
bring up that or bring

00:39:28,720 --> 00:39:31,839
bring up the new cluster don't bring up

00:39:30,400 --> 00:39:35,040
that new cluster

00:39:31,839 --> 00:39:37,520
uh with um with all the

00:39:35,040 --> 00:39:38,480
kubernetes nodes uh it's just bring them

00:39:37,520 --> 00:39:40,240
up gradually

00:39:38,480 --> 00:39:42,480
as you move the workloads and then you

00:39:40,240 --> 00:39:44,160
can continue to shut down the old ones

00:39:42,480 --> 00:39:45,599
actually gradually so that you kind of

00:39:44,160 --> 00:39:47,520
minimize your

00:39:45,599 --> 00:39:49,359
the the the cost right so you're always

00:39:47,520 --> 00:39:52,000
going to have a little bit more cost

00:39:49,359 --> 00:39:52,800
uh and there are some other things that

00:39:52,000 --> 00:39:55,359
you can use to

00:39:52,800 --> 00:39:57,680
uh to use to optimize costs for example

00:39:55,359 --> 00:39:59,680
um the

00:39:57,680 --> 00:40:01,760
use of something like spot instances or

00:39:59,680 --> 00:40:03,839
preemptable instances

00:40:01,760 --> 00:40:05,119
uh so if you if you don't have workloads

00:40:03,839 --> 00:40:06,480
that are very critical i know

00:40:05,119 --> 00:40:08,720
we talked about stateful type of

00:40:06,480 --> 00:40:11,920
workloads but so those may not be

00:40:08,720 --> 00:40:14,400
the best um use case

00:40:11,920 --> 00:40:15,359
but if you have uh some stateless

00:40:14,400 --> 00:40:17,280
applications

00:40:15,359 --> 00:40:20,079
maybe they're okay if you're running

00:40:17,280 --> 00:40:21,040
those in implementable notes or spot

00:40:20,079 --> 00:40:23,520
nodes

00:40:21,040 --> 00:40:24,400
uh and there are a lot of other tools

00:40:23,520 --> 00:40:27,839
for example

00:40:24,400 --> 00:40:31,359
like aws i know has this

00:40:27,839 --> 00:40:34,960
way to to run spot fleets and

00:40:31,359 --> 00:40:35,920
in a certain number of different spot

00:40:34,960 --> 00:40:39,599
instances

00:40:35,920 --> 00:40:40,240
where you can say uh give me like 10

00:40:39,599 --> 00:40:43,119
different

00:40:40,240 --> 00:40:44,960
uh instance types that i can run so that

00:40:43,119 --> 00:40:47,680
means like if one of them

00:40:44,960 --> 00:40:49,520
uh actually becomes super expensive uh

00:40:47,680 --> 00:40:51,440
you will still have capacity enough

00:40:49,520 --> 00:40:54,560
there to run your instances

00:40:51,440 --> 00:40:58,000
so you you could actually uh you know

00:40:54,560 --> 00:40:59,359
uh optimize your cost but the bottom

00:40:58,000 --> 00:40:59,920
line is that you're always going to pay

00:40:59,359 --> 00:41:03,119
just

00:40:59,920 --> 00:41:05,599
a little bit more and i think it it it

00:41:03,119 --> 00:41:07,839
is more reliable because you

00:41:05,599 --> 00:41:09,440
you're actually having a whole new uh

00:41:07,839 --> 00:41:11,599
cluster with your new workloads

00:41:09,440 --> 00:41:12,560
and it gives you time to test ahead of

00:41:11,599 --> 00:41:14,240
time

00:41:12,560 --> 00:41:16,160
but again if you want a little bit more

00:41:14,240 --> 00:41:18,640
reliability there's going to be a little

00:41:16,160 --> 00:41:21,760
bit more cost

00:41:18,640 --> 00:41:25,119
does that make sense yeah

00:41:21,760 --> 00:41:25,119
thank you yeah

00:41:28,800 --> 00:41:35,839
okay any other questions okay oh there's

00:41:32,079 --> 00:41:35,839
another one here

00:41:39,040 --> 00:41:41,280
okay

00:41:46,800 --> 00:41:50,480
all right well if there's no other

00:41:48,480 --> 00:41:51,920
questions um thank you very much ricardo

00:41:50,480 --> 00:41:53,680
for the talk that was great

00:41:51,920 --> 00:41:55,119
um thank you for everyone that attended

00:41:53,680 --> 00:41:56,880
for attending this talk

00:41:55,119 --> 00:41:58,160
and this is the last one for the cloud

00:41:56,880 --> 00:42:02,319
track of the day

00:41:58,160 --> 00:42:04,560
um you are encouraged uh

00:42:02,319 --> 00:42:06,079
sorry ben i see i see that you got a

00:42:04,560 --> 00:42:09,200
question coming through so

00:42:06,079 --> 00:42:17,839
uh we'll answer ben's question quick

00:42:09,200 --> 00:42:17,839
go ahead and type it or request a talk

00:42:21,920 --> 00:42:26,480
okay there it is at the top of the q a

00:42:24,720 --> 00:42:30,000
okay yeah

00:42:26,480 --> 00:42:31,040
all right um a question for autoscaling

00:42:30,000 --> 00:42:32,720
aws

00:42:31,040 --> 00:42:34,240
i've had a hard time helping node

00:42:32,720 --> 00:42:37,440
scaling quicker

00:42:34,240 --> 00:42:39,920
more sensitive based on

00:42:37,440 --> 00:42:40,560
the parameters like scan interval scale

00:42:39,920 --> 00:42:44,000
down

00:42:40,560 --> 00:42:45,920
unneeded scale down utilization

00:42:44,000 --> 00:42:48,480
so you have a good reference for me to

00:42:45,920 --> 00:42:50,800
understand how these parameters

00:42:48,480 --> 00:42:50,800
work

00:42:53,440 --> 00:42:57,359
scan interval scale down unneeded scaled

00:42:56,960 --> 00:43:01,440
down

00:42:57,359 --> 00:43:05,680
utilization um

00:43:01,440 --> 00:43:05,680
oh this is for aws yeah so

00:43:08,480 --> 00:43:15,520
yeah something it's it's these are

00:43:12,000 --> 00:43:15,520
kind of uh

00:43:15,760 --> 00:43:19,040
it's yeah this question is kind of broad

00:43:18,160 --> 00:43:22,319
um

00:43:19,040 --> 00:43:24,000
so you i mean the typical scenario is

00:43:22,319 --> 00:43:26,319
where

00:43:24,000 --> 00:43:29,280
you have cpu or memory constraints right

00:43:26,319 --> 00:43:32,079
so where where you uh

00:43:29,280 --> 00:43:32,800
when you have like uh maybe if you if

00:43:32,079 --> 00:43:34,800
you

00:43:32,800 --> 00:43:35,839
have the average of all your kubernetes

00:43:34,800 --> 00:43:39,280
nodes at

00:43:35,839 --> 00:43:40,400
60 and aws you may have like auto

00:43:39,280 --> 00:43:43,520
scaling group but

00:43:40,400 --> 00:43:46,079
and then say you have it at 60 percent

00:43:43,520 --> 00:43:47,119
and start adding new uh kubernetes node

00:43:46,079 --> 00:43:49,760
and and that's just

00:43:47,119 --> 00:43:51,440
standard auto scaling in kubernetes it

00:43:49,760 --> 00:43:53,839
could be also memory right so if you

00:43:51,440 --> 00:43:57,040
have memory

00:43:53,839 --> 00:43:58,560
let's say 70 or 80 across all your notes

00:43:57,040 --> 00:44:00,640
you start scaling up

00:43:58,560 --> 00:44:01,760
now if that average if it starts to go

00:44:00,640 --> 00:44:04,800
down

00:44:01,760 --> 00:44:05,440
then it starts to uh come down but now

00:44:04,800 --> 00:44:08,000
like uh

00:44:05,440 --> 00:44:09,760
it it's easier for stateless type of

00:44:08,000 --> 00:44:11,680
workloads because those are

00:44:09,760 --> 00:44:13,359
you know you don't have any data there

00:44:11,680 --> 00:44:15,599
and then the data can be

00:44:13,359 --> 00:44:18,400
picked up from like a database that is

00:44:15,599 --> 00:44:21,599
not running a new kubernetes cluster

00:44:18,400 --> 00:44:23,599
uh so he's

00:44:21,599 --> 00:44:24,720
so you're asking whether there's a good

00:44:23,599 --> 00:44:27,839
reference so

00:44:24,720 --> 00:44:30,160
i think uh you know aws was pretty good

00:44:27,839 --> 00:44:32,720
i think he has pretty good documentation

00:44:30,160 --> 00:44:34,160
on auto scaling groups but then also

00:44:32,720 --> 00:44:36,960
there are parts that

00:44:34,160 --> 00:44:37,839
kubernetes that are not part of an auto

00:44:36,960 --> 00:44:40,319
scaling group

00:44:37,839 --> 00:44:42,319
and that is the cluster auto scaler uh

00:44:40,319 --> 00:44:45,520
so that's an open source project

00:44:42,319 --> 00:44:48,640
so you may want to go to just

00:44:45,520 --> 00:44:49,839
google uh cluster autoscaler in on

00:44:48,640 --> 00:44:53,040
github

00:44:49,839 --> 00:44:54,079
then then you'll find that the page of

00:44:53,040 --> 00:44:55,440
cluster autoscaler with its

00:44:54,079 --> 00:44:57,760
documentation

00:44:55,440 --> 00:44:58,560
and how to use it but the cluster of the

00:44:57,760 --> 00:45:02,000
scalar

00:44:58,560 --> 00:45:05,119
scales based on cluster capacity

00:45:02,000 --> 00:45:06,480
so if a pod cannot be scheduled anymore

00:45:05,119 --> 00:45:08,640
on a kubernetes node

00:45:06,480 --> 00:45:10,319
what it does is actually brings up a new

00:45:08,640 --> 00:45:12,960
uh kubernetes node

00:45:10,319 --> 00:45:14,160
uh in in the cloud provider in in for

00:45:12,960 --> 00:45:18,400
say aws

00:45:14,160 --> 00:45:20,640
and so i i don't know uh

00:45:18,400 --> 00:45:22,240
if that answers part of the question

00:45:20,640 --> 00:45:25,599
maybe it does a little bit

00:45:22,240 --> 00:45:29,520
but i would be happy to chat uh more if

00:45:25,599 --> 00:45:31,440
uh if you have uh based on some of the

00:45:29,520 --> 00:45:32,800
some resources might be helpful for you

00:45:31,440 --> 00:45:35,839
then i'll be happy to

00:45:32,800 --> 00:45:35,839
to provide some more

00:45:38,000 --> 00:45:42,160
and then feel free to feel free to speak

00:45:40,079 --> 00:45:47,839
up to you if you'd like to

00:45:42,160 --> 00:45:47,839
to discuss anything

00:45:48,880 --> 00:45:52,160
okay that's it thank you yeah yeah i

00:45:51,760 --> 00:45:54,160
just

00:45:52,160 --> 00:45:55,760
uh i'm available on twitter so uh you

00:45:54,160 --> 00:45:58,240
can pick me on twitter anytime

00:45:55,760 --> 00:45:59,520
so if you have any questions then uh

00:45:58,240 --> 00:46:03,200
happy to job

00:45:59,520 --> 00:46:03,200
oh you can chat outside the channel yeah

00:46:09,720 --> 00:46:12,720

YouTube URL: https://www.youtube.com/watch?v=Ec3JtiTamiQ


