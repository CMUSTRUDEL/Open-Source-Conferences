Title: Challenges in Building Multi-Cloud-Provider Platform With Managed Kubernetes - Joerg Schad, PhD
Publication date: 2020-12-11
Playlist: All Things Open 2020 - Cloud Track
Description: 
	Presented by: Joerg Schad, PhD, ArangoDB
Presented at All Things Open 2020 - Cloud Track

Abstract: Building a cloud-agnostic platform used to be a challenging task as one had to deal with a large number of different cloud APIs and service offerings. Today, as most Cloud providers are offering a managed Kubernetes solution (e.g., GKE, AKS, or EKS), it seems like developers could simply build a platform based on Kubernetes and be cloud-agnostic. While this assumption is mostly correct, there are still a number of differences and pitfalls when deploying across those managed Kubernetes solutions.

This talk discusses the experiences made while building the ArangoDB Managed Service offering across and GKE, AKS, or EKS.

While the (managed) Kubernetes API being a great abstraction from the actual cloud provider, a number of challenges remain including for example networking, autoscaler, cluster provisioning, or node sizing. This talk provides an overview of those challenges and also discuss how they were solved as part of the ArangoDB managed Service.
Captions: 
	00:00:04,880 --> 00:00:07,919
uh

00:00:05,359 --> 00:00:09,840
yes we're ready actually to begin right

00:00:07,919 --> 00:00:10,480
now maybe let's just stall a bit for

00:00:09,840 --> 00:00:13,200
like this

00:00:10,480 --> 00:00:13,920
one minute for people to still join but

00:00:13,200 --> 00:00:15,759
what you just

00:00:13,920 --> 00:00:17,760
mentioned the distributed databases in

00:00:15,759 --> 00:00:19,760
kubernetes this is basically what this

00:00:17,760 --> 00:00:22,720
talk is about as well

00:00:19,760 --> 00:00:23,199
so the title it might actually sound a

00:00:22,720 --> 00:00:26,320
bit

00:00:23,199 --> 00:00:29,199
uh long or repelling so

00:00:26,320 --> 00:00:31,279
in short if we had to shorten the title

00:00:29,199 --> 00:00:34,320
we could have actually also named it

00:00:31,279 --> 00:00:36,079
kubernetes not equals kubernetes so what

00:00:34,320 --> 00:00:38,879
we'll talk a bit about

00:00:36,079 --> 00:00:40,079
is as how to build actually a managed

00:00:38,879 --> 00:00:43,440
database service

00:00:40,079 --> 00:00:46,000
across first of all on top of kubernetes

00:00:43,440 --> 00:00:47,039
but in particular across multiple cloud

00:00:46,000 --> 00:00:50,640
vendors

00:00:47,039 --> 00:00:52,719
so um we built actually our

00:00:50,640 --> 00:00:53,920
orango db oasis which is the managed

00:00:52,719 --> 00:00:57,440
cloud service for

00:00:53,920 --> 00:01:01,359
a rangodb across multiple

00:00:57,440 --> 00:01:04,799
managed kubernetes solutions such as eks

00:01:01,359 --> 00:01:06,560
google kubernetes engine or azure and

00:01:04,799 --> 00:01:08,320
this is what this talk will be focused

00:01:06,560 --> 00:01:10,400
about so it's actually for you if you're

00:01:08,320 --> 00:01:11,200
interested in building any persistent

00:01:10,400 --> 00:01:14,240
service

00:01:11,200 --> 00:01:16,720
on top of kubernetes and

00:01:14,240 --> 00:01:17,680
probably even more if you are looking

00:01:16,720 --> 00:01:19,680
into

00:01:17,680 --> 00:01:20,960
running across multiple different cloud

00:01:19,680 --> 00:01:24,000
vendors seeing

00:01:20,960 --> 00:01:27,200
what the challenges were we faced and

00:01:24,000 --> 00:01:30,159
uh what pitfalls you can avoid

00:01:27,200 --> 00:01:30,560
with me today is actually chris here so

00:01:30,159 --> 00:01:33,439
chris

00:01:30,560 --> 00:01:34,400
is also one of our engineers but we'll

00:01:33,439 --> 00:01:37,680
introduce him just

00:01:34,400 --> 00:01:40,320
shortly first myself

00:01:37,680 --> 00:01:42,479
so yorkshire i'm the head of engineering

00:01:40,320 --> 00:01:45,280
at machine learning at the wrangler db

00:01:42,479 --> 00:01:46,960
and uh kind of my past has always been

00:01:45,280 --> 00:01:47,439
switching back and forth between

00:01:46,960 --> 00:01:50,640
building

00:01:47,439 --> 00:01:52,479
large-scale infrastructure so also back

00:01:50,640 --> 00:01:53,600
at mesosphere i had the pleasure of

00:01:52,479 --> 00:01:56,560
being one of the

00:01:53,600 --> 00:01:59,119
early contributors to kubernetes so we

00:01:56,560 --> 00:02:02,159
actually worked on kubernetes 1.0 which

00:01:59,119 --> 00:02:04,320
was probably more rebasing than coding

00:02:02,159 --> 00:02:05,680
but those were pretty interesting and

00:02:04,320 --> 00:02:08,080
fun times

00:02:05,680 --> 00:02:10,560
and prior to that i've already been in

00:02:08,080 --> 00:02:13,120
the database field with both my phd

00:02:10,560 --> 00:02:15,440
and then as one of the architects of uh

00:02:13,120 --> 00:02:17,599
over at sap was hana

00:02:15,440 --> 00:02:19,680
and so right now i'm pretty happy that i

00:02:17,599 --> 00:02:21,680
can combine both those passions over

00:02:19,680 --> 00:02:24,160
here at the rangodp

00:02:21,680 --> 00:02:25,040
working on yeah large-scale distributed

00:02:24,160 --> 00:02:28,400
systems

00:02:25,040 --> 00:02:30,160
on top of kubernetes as mentioned with

00:02:28,400 --> 00:02:31,840
me today is chris he's one of our

00:02:30,160 --> 00:02:34,080
developer relations

00:02:31,840 --> 00:02:37,840
engineers and chris i believe you can

00:02:34,080 --> 00:02:37,840
actually best introduce yourself

00:02:38,480 --> 00:02:42,800
all righty thank you yes as you said my

00:02:40,560 --> 00:02:43,360
name is chris woodward and uh thank you

00:02:42,800 --> 00:02:46,080
again

00:02:43,360 --> 00:02:47,840
ben and everyone for having us uh i am

00:02:46,080 --> 00:02:49,200
the developer relations engineer at

00:02:47,840 --> 00:02:51,920
orangutb

00:02:49,200 --> 00:02:52,959
i came to orangutib with about 10 years

00:02:51,920 --> 00:02:54,959
of experience and

00:02:52,959 --> 00:02:56,640
a couple different areas of technology

00:02:54,959 --> 00:02:59,519
and support administration

00:02:56,640 --> 00:03:01,360
and development at orangutube i get to

00:02:59,519 --> 00:03:03,360
work with our engineers on various

00:03:01,360 --> 00:03:06,319
development projects and do community

00:03:03,360 --> 00:03:08,480
outreach as well as a big focus on

00:03:06,319 --> 00:03:09,519
just improving the learning and training

00:03:08,480 --> 00:03:11,360
experience uh

00:03:09,519 --> 00:03:13,680
when getting started with the wrong db

00:03:11,360 --> 00:03:15,200
so if you do have any questions or

00:03:13,680 --> 00:03:16,159
suggestions there anything you would

00:03:15,200 --> 00:03:19,280
like to see

00:03:16,159 --> 00:03:19,760
or maybe improved uh adoramadv feel free

00:03:19,280 --> 00:03:23,040
to

00:03:19,760 --> 00:03:24,680
at me at on twitter or uh message me

00:03:23,040 --> 00:03:27,120
on our community slack with

00:03:24,680 --> 00:03:33,840
chris.orangutv

00:03:27,120 --> 00:03:33,840
um the next slide here if i can

00:03:34,319 --> 00:03:37,920
uh yeah this is our oasis team i've

00:03:36,720 --> 00:03:39,440
actually also been able to work with

00:03:37,920 --> 00:03:41,200
this team on some projects as well but

00:03:39,440 --> 00:03:41,519
they're they're kind of the rock stars

00:03:41,200 --> 00:03:43,440
of

00:03:41,519 --> 00:03:44,560
oasis they they had to face a lot of the

00:03:43,440 --> 00:03:47,680
challenges that

00:03:44,560 --> 00:03:49,519
we'll be talking about today um and just

00:03:47,680 --> 00:03:50,480
a great group and yeah this is our our

00:03:49,519 --> 00:03:53,680
main oasis

00:03:50,480 --> 00:03:56,159
definitely worth a call out there and

00:03:53,680 --> 00:03:58,480
next slide please

00:03:56,159 --> 00:04:00,879
and uh if you're not already familiar

00:03:58,480 --> 00:04:03,519
with orangute aromadb is actually a

00:04:00,879 --> 00:04:04,000
scalable native multi-model database

00:04:03,519 --> 00:04:06,959
that

00:04:04,000 --> 00:04:08,480
takes you from graphs and beyond now

00:04:06,959 --> 00:04:10,239
what that means is that you can model

00:04:08,480 --> 00:04:12,799
your data as a document

00:04:10,239 --> 00:04:13,439
a key value store and as a graph

00:04:12,799 --> 00:04:15,360
database

00:04:13,439 --> 00:04:16,799
all at the same time now the kind of

00:04:15,360 --> 00:04:18,400
benefit that brings to you

00:04:16,799 --> 00:04:20,799
working with a multi-model database

00:04:18,400 --> 00:04:22,160
means that your application can have all

00:04:20,799 --> 00:04:24,320
the features it needs

00:04:22,160 --> 00:04:25,520
while only needing to use a single

00:04:24,320 --> 00:04:27,759
database

00:04:25,520 --> 00:04:28,800
this can dramatically improve

00:04:27,759 --> 00:04:33,360
development time

00:04:28,800 --> 00:04:36,560
and operations ease of use there

00:04:33,360 --> 00:04:36,560
and next slide please

00:04:36,800 --> 00:04:40,639
so knowing that what we are how long do

00:04:39,919 --> 00:04:42,880
we

00:04:40,639 --> 00:04:44,560
why why do we care so much about some of

00:04:42,880 --> 00:04:45,520
these cloud technologies that york was

00:04:44,560 --> 00:04:48,639
mentioning

00:04:45,520 --> 00:04:50,320
um this is you know for us it was big

00:04:48,639 --> 00:04:51,440
because we actually needed to put a

00:04:50,320 --> 00:04:53,759
wrong db

00:04:51,440 --> 00:04:54,639
on kubernetes we needed this for our

00:04:53,759 --> 00:04:56,400
managed service

00:04:54,639 --> 00:04:58,000
oasis which we'll be talking about as

00:04:56,400 --> 00:05:00,960
well

00:04:58,000 --> 00:05:02,960
this was a big challenge for us because

00:05:00,960 --> 00:05:04,800
we were going to production just like

00:05:02,960 --> 00:05:06,000
a lot of developers do day to day this

00:05:04,800 --> 00:05:09,199
is something that we can

00:05:06,000 --> 00:05:11,919
now say we confidently can relate to and

00:05:09,199 --> 00:05:13,600
we you know we can we can understand

00:05:11,919 --> 00:05:16,479
some of the challenges there because

00:05:13,600 --> 00:05:17,840
oasis literally runs on these cloud

00:05:16,479 --> 00:05:21,039
technologies

00:05:17,840 --> 00:05:22,880
um okay and next slide and

00:05:21,039 --> 00:05:25,759
uh oasis if you aren't familiar with it

00:05:22,880 --> 00:05:28,960
as well is our cloud managed service

00:05:25,759 --> 00:05:32,080
it is available on all the major

00:05:28,960 --> 00:05:33,440
cloud providers including aws google and

00:05:32,080 --> 00:05:35,840
azure

00:05:33,440 --> 00:05:36,479
it is a fully managed service and it

00:05:35,840 --> 00:05:39,039
includes

00:05:36,479 --> 00:05:40,160
it runs our enterprise version which has

00:05:39,039 --> 00:05:42,000
some

00:05:40,160 --> 00:05:43,440
extra features on top of the community

00:05:42,000 --> 00:05:46,080
version including

00:05:43,440 --> 00:05:47,520
ironclad security that you don't get in

00:05:46,080 --> 00:05:50,080
the community version which is perfect

00:05:47,520 --> 00:05:53,199
for enterprise use cases as well

00:05:50,080 --> 00:05:54,960
and of course it is highly scalable and

00:05:53,199 --> 00:05:56,560
it is easily adjustable if we take a

00:05:54,960 --> 00:05:57,840
look at our deployment configuration

00:05:56,560 --> 00:06:00,319
screen here

00:05:57,840 --> 00:06:02,319
you can configure initially and come

00:06:00,319 --> 00:06:03,199
back and change deployment types and

00:06:02,319 --> 00:06:05,039
modes

00:06:03,199 --> 00:06:06,639
as well as the different hardware

00:06:05,039 --> 00:06:08,319
configurations that you have available

00:06:06,639 --> 00:06:10,560
and once you get this set up

00:06:08,319 --> 00:06:12,160
we also offer a lot of examples that you

00:06:10,560 --> 00:06:14,960
can get familiar with just

00:06:12,160 --> 00:06:16,960
orangutabe in general as well as get

00:06:14,960 --> 00:06:18,240
familiar with oasis and these examples

00:06:16,960 --> 00:06:20,160
include

00:06:18,240 --> 00:06:22,400
everything from a fraud detection use

00:06:20,160 --> 00:06:23,199
case uh getting started with our

00:06:22,400 --> 00:06:26,080
built-in

00:06:23,199 --> 00:06:28,400
search engine or ongo search as well as

00:06:26,080 --> 00:06:30,560
a really neat e-commerce use case which

00:06:28,400 --> 00:06:32,720
shows data analytics with the instacart

00:06:30,560 --> 00:06:36,560
data set

00:06:32,720 --> 00:06:39,520
next car uh next slide please

00:06:36,560 --> 00:06:41,440
and so oasis is sort of why orangutybee

00:06:39,520 --> 00:06:43,440
cares about these technologies because

00:06:41,440 --> 00:06:45,199
it's it's you know it's a part of our

00:06:43,440 --> 00:06:48,160
product but why should you care

00:06:45,199 --> 00:06:50,240
and you should care about orangutbee as

00:06:48,160 --> 00:06:53,520
well as these cloud technologies

00:06:50,240 --> 00:06:55,280
um because around db is there for

00:06:53,520 --> 00:06:57,199
whenever the inevitable happens with

00:06:55,280 --> 00:06:58,479
your application where you have new

00:06:57,199 --> 00:07:01,120
requirements or

00:06:58,479 --> 00:07:02,720
the requirements change or new features

00:07:01,120 --> 00:07:04,720
that's where using a multi-model

00:07:02,720 --> 00:07:06,319
database comes in handy because now

00:07:04,720 --> 00:07:08,639
rather than having to learn a whole new

00:07:06,319 --> 00:07:10,639
database technology or or

00:07:08,639 --> 00:07:12,720
different technologies out there you're

00:07:10,639 --> 00:07:14,000
just adding or updating your existing

00:07:12,720 --> 00:07:15,919
queries

00:07:14,000 --> 00:07:18,479
and that's the the power of the

00:07:15,919 --> 00:07:20,720
wrongdoing and multi model whereas

00:07:18,479 --> 00:07:22,560
on the deployment side things are

00:07:20,720 --> 00:07:23,919
evolving in the same way as well

00:07:22,560 --> 00:07:26,000
uh and we've seen this with our

00:07:23,919 --> 00:07:28,560
customers as well a trend

00:07:26,000 --> 00:07:30,400
towards containerization and using some

00:07:28,560 --> 00:07:32,080
of these cloud technologies

00:07:30,400 --> 00:07:33,840
and that's why we make it as easy as

00:07:32,080 --> 00:07:35,199
possible to get started as well as to go

00:07:33,840 --> 00:07:36,800
to production

00:07:35,199 --> 00:07:38,479
by having simple things like being able

00:07:36,800 --> 00:07:40,720
to do a docker pool and

00:07:38,479 --> 00:07:43,039
running a rongodb with a kubernetes

00:07:40,720 --> 00:07:46,479
operator

00:07:43,039 --> 00:07:48,639
and uh next slide please and actually

00:07:46,479 --> 00:07:50,800
that's kind of uh ending it for me i

00:07:48,639 --> 00:07:52,800
will actually pass it back over to york

00:07:50,800 --> 00:07:54,879
uh to take it from here to dive a little

00:07:52,800 --> 00:07:55,360
bit deeper into some of the challenges

00:07:54,879 --> 00:07:56,960
that

00:07:55,360 --> 00:07:58,800
we did face when using some of these

00:07:56,960 --> 00:08:02,800
technologies and

00:07:58,800 --> 00:08:04,240
from there thank you

00:08:02,800 --> 00:08:06,479
thanks so much chris for the

00:08:04,240 --> 00:08:08,879
introduction um

00:08:06,479 --> 00:08:11,599
before we actually start into the kind

00:08:08,879 --> 00:08:14,560
of like more devops topic of how to

00:08:11,599 --> 00:08:16,160
leverage and how to build an application

00:08:14,560 --> 00:08:18,240
or how to deploy and manage an

00:08:16,160 --> 00:08:21,759
application on top of kubernetes

00:08:18,240 --> 00:08:24,160
even different kubernetes distributions

00:08:21,759 --> 00:08:26,160
uh just maybe one shout out to more the

00:08:24,160 --> 00:08:29,280
core backend programmers here

00:08:26,160 --> 00:08:31,599
so being able to deploy something on

00:08:29,280 --> 00:08:32,719
top of kubernetes or any container

00:08:31,599 --> 00:08:36,479
solution

00:08:32,719 --> 00:08:39,120
might also be nomad swarm

00:08:36,479 --> 00:08:40,320
mesos or whatever you're thinking about

00:08:39,120 --> 00:08:42,240
is actually that you

00:08:40,320 --> 00:08:43,760
it also changes the way in which you

00:08:42,240 --> 00:08:46,080
have to write a database or any

00:08:43,760 --> 00:08:48,399
persistent application in particular

00:08:46,080 --> 00:08:50,080
so the one change we have been seeing in

00:08:48,399 --> 00:08:50,959
terms of like deployment towards

00:08:50,080 --> 00:08:53,920
kubernetes

00:08:50,959 --> 00:08:55,839
it actually also needs to reflect a lot

00:08:53,920 --> 00:08:58,160
of changes in our back

00:08:55,839 --> 00:09:00,880
end because it's a more dynamic

00:08:58,160 --> 00:09:03,440
infrastructure like 10 years ago

00:09:00,880 --> 00:09:04,320
databases you had those five servers

00:09:03,440 --> 00:09:05,839
somewhere

00:09:04,320 --> 00:09:07,519
somewhere down in your basement and

00:09:05,839 --> 00:09:08,000
those five servers they would run your

00:09:07,519 --> 00:09:10,640
database

00:09:08,000 --> 00:09:12,640
for the next years probably nowadays

00:09:10,640 --> 00:09:15,440
with the cloud and containers

00:09:12,640 --> 00:09:17,519
uh out there it's a way more volatile

00:09:15,440 --> 00:09:18,399
way more dynamic environment where you

00:09:17,519 --> 00:09:21,040
might get

00:09:18,399 --> 00:09:22,240
uh rescheduled because a pot is being

00:09:21,040 --> 00:09:23,920
restarted or

00:09:22,240 --> 00:09:25,440
amazon is telling you that they actually

00:09:23,920 --> 00:09:27,839
need to restart that note for

00:09:25,440 --> 00:09:28,880
maintenance uh google is upgrading your

00:09:27,839 --> 00:09:31,440
kubernetes

00:09:28,880 --> 00:09:32,000
distribution or other things so in

00:09:31,440 --> 00:09:34,720
general

00:09:32,000 --> 00:09:36,080
as a database the infrastructure you run

00:09:34,720 --> 00:09:38,160
on has become

00:09:36,080 --> 00:09:39,680
more fluid and you actually have to

00:09:38,160 --> 00:09:43,519
reflect that

00:09:39,680 --> 00:09:45,200
also in your database design but as

00:09:43,519 --> 00:09:48,240
chris already mentioned we actually

00:09:45,200 --> 00:09:50,080
tried to encapsulate as much as possible

00:09:48,240 --> 00:09:52,640
of this management for you and this is

00:09:50,080 --> 00:09:55,279
done by our managed service oasis

00:09:52,640 --> 00:09:56,959
uh most of this is actually just to give

00:09:55,279 --> 00:09:58,720
you the background for this talk to

00:09:56,959 --> 00:10:00,160
understand what we're talking about with

00:09:58,720 --> 00:10:02,640
like specific terms

00:10:00,160 --> 00:10:04,880
but the high level architecture of oasis

00:10:02,640 --> 00:10:06,560
actually split into two components

00:10:04,880 --> 00:10:08,000
if you're familiar with kind of the core

00:10:06,560 --> 00:10:10,720
concept of kubernetes

00:10:08,000 --> 00:10:12,640
it's actually a pretty similar approach

00:10:10,720 --> 00:10:15,760
so we have the control plane

00:10:12,640 --> 00:10:16,800
managing all our different clusters out

00:10:15,760 --> 00:10:18,880
there

00:10:16,800 --> 00:10:19,839
and so here we collect the central

00:10:18,880 --> 00:10:22,320
metrics

00:10:19,839 --> 00:10:22,959
uh we have our data cluster operator for

00:10:22,320 --> 00:10:25,440
spinning up

00:10:22,959 --> 00:10:26,160
new data clusters we have our deployment

00:10:25,440 --> 00:10:29,279
handler

00:10:26,160 --> 00:10:30,480
we have all our logging our alerting and

00:10:29,279 --> 00:10:33,680
of course and also the

00:10:30,480 --> 00:10:34,959
metrics we actually need for charging

00:10:33,680 --> 00:10:38,240
later on

00:10:34,959 --> 00:10:39,839
so the data clusters themselves there is

00:10:38,240 --> 00:10:42,640
a kind of

00:10:39,839 --> 00:10:43,200
actually in this case kubernetes

00:10:42,640 --> 00:10:46,800
clusters

00:10:43,200 --> 00:10:48,720
running our arrange deployment so each

00:10:46,800 --> 00:10:51,839
deployment is corresponding to

00:10:48,720 --> 00:10:54,000
at least a three node cluster and so the

00:10:51,839 --> 00:10:54,720
data clusters are basically easy back

00:10:54,000 --> 00:10:56,640
ends running

00:10:54,720 --> 00:10:58,480
the actual workload so if you want to

00:10:56,640 --> 00:10:59,200
kind of compare that to the kubernetes

00:10:58,480 --> 00:11:02,320
node

00:10:59,200 --> 00:11:05,360
each part in kubernetes would be like

00:11:02,320 --> 00:11:06,160
one of the parts for ourselves what

00:11:05,360 --> 00:11:09,120
would be one

00:11:06,160 --> 00:11:11,040
rango db cluster in our terms and each

00:11:09,120 --> 00:11:14,800
data cluster would kind of correspond to

00:11:11,040 --> 00:11:14,800
a worker node in kubernetes

00:11:14,880 --> 00:11:20,240
and so for us we actually have a number

00:11:18,240 --> 00:11:21,680
of data clusters so first of all they

00:11:20,240 --> 00:11:24,480
are spread out across

00:11:21,680 --> 00:11:26,079
different cloud providers and also

00:11:24,480 --> 00:11:27,920
across different regions for cloud

00:11:26,079 --> 00:11:28,959
providers so roughly you can think about

00:11:27,920 --> 00:11:31,360
we got

00:11:28,959 --> 00:11:32,560
one data cluster per cloud provider per

00:11:31,360 --> 00:11:35,120
region

00:11:32,560 --> 00:11:36,399
sometimes even more for like privacy

00:11:35,120 --> 00:11:39,519
concerns

00:11:36,399 --> 00:11:41,360
testing clusters and other things the

00:11:39,519 --> 00:11:42,000
control plane as mentioned it's kind of

00:11:41,360 --> 00:11:43,519
the brain

00:11:42,000 --> 00:11:45,279
it's also running in a managed

00:11:43,519 --> 00:11:47,600
kubernetes in this case

00:11:45,279 --> 00:11:49,760
on google infrastructure and it's

00:11:47,600 --> 00:11:52,480
responsible for all the dashboards

00:11:49,760 --> 00:11:54,079
uh the actual creation of new data

00:11:52,480 --> 00:11:54,880
clusters and the management of data

00:11:54,079 --> 00:11:56,560
clusters

00:11:54,880 --> 00:11:58,639
and furthermore it's the only

00:11:56,560 --> 00:11:58,959
communication hub for data clusters for

00:11:58,639 --> 00:12:00,639
the

00:11:58,959 --> 00:12:02,000
rare cases where they actually have to

00:12:00,639 --> 00:12:04,320
communicate

00:12:02,000 --> 00:12:05,600
this is not often but all communication

00:12:04,320 --> 00:12:07,760
is basically going through

00:12:05,600 --> 00:12:10,560
the control plane and data planes aren't

00:12:07,760 --> 00:12:13,360
talking to each other directly

00:12:10,560 --> 00:12:14,639
the data clusters there is the running

00:12:13,360 --> 00:12:17,120
the actual cloud

00:12:14,639 --> 00:12:18,959
customer deployments and as mentioned we

00:12:17,120 --> 00:12:22,480
got a number of data clusters

00:12:18,959 --> 00:12:25,040
per cloud provider per zone and the goal

00:12:22,480 --> 00:12:27,360
of a data cluster is to of why we

00:12:25,040 --> 00:12:29,600
introduced the concept of a data cluster

00:12:27,360 --> 00:12:30,880
is to actually abstract away the cluster

00:12:29,600 --> 00:12:33,920
details

00:12:30,880 --> 00:12:35,839
and in cluster details also abstract the

00:12:33,920 --> 00:12:37,600
way the cloud provider details

00:12:35,839 --> 00:12:39,519
that we don't have to care from the

00:12:37,600 --> 00:12:40,480
control plane whether this is actually

00:12:39,519 --> 00:12:42,639
running on

00:12:40,480 --> 00:12:43,519
google or whether this is actually

00:12:42,639 --> 00:12:47,519
running on

00:12:43,519 --> 00:12:49,200
amazon infrastructure this mostly works

00:12:47,519 --> 00:12:50,880
this is what the remainder of this talk

00:12:49,200 --> 00:12:52,160
is going to be about where it doesn't

00:12:50,880 --> 00:12:56,639
work

00:12:52,160 --> 00:12:56,639
but we'll see how that's going

00:12:57,920 --> 00:13:04,079
so why should you care again and

00:13:01,600 --> 00:13:05,600
this again comes down to kubernetes

00:13:04,079 --> 00:13:06,720
actually provides a pretty good

00:13:05,600 --> 00:13:09,760
abstraction

00:13:06,720 --> 00:13:11,839
if we had to build all of this across

00:13:09,760 --> 00:13:13,200
a random infrastructure it would have

00:13:11,839 --> 00:13:15,040
been way more work

00:13:13,200 --> 00:13:16,959
and this has already been the promise

00:13:15,040 --> 00:13:18,320
when containers were introduced so the

00:13:16,959 --> 00:13:19,760
container concept

00:13:18,320 --> 00:13:21,600
probably all of you are familiar with

00:13:19,760 --> 00:13:22,959
that but the idea is i can just simply

00:13:21,600 --> 00:13:25,120
run that container

00:13:22,959 --> 00:13:26,160
the same way on anything supporting that

00:13:25,120 --> 00:13:27,600
infrastructure

00:13:26,160 --> 00:13:29,120
and this has been also kind of the

00:13:27,600 --> 00:13:30,880
motivation when containers were

00:13:29,120 --> 00:13:32,880
introduced in the real world

00:13:30,880 --> 00:13:34,079
where we can deploy a container onto a

00:13:32,880 --> 00:13:37,680
train ship

00:13:34,079 --> 00:13:42,079
a truck because it's a standardized

00:13:37,680 --> 00:13:44,000
interface the next level to actually get

00:13:42,079 --> 00:13:45,760
something meaningful out of containers

00:13:44,000 --> 00:13:48,480
is container orchestration

00:13:45,760 --> 00:13:49,680
by itself if a container is failing

00:13:48,480 --> 00:13:53,199
there's no one there

00:13:49,680 --> 00:13:55,519
who will restart it if we need to scale

00:13:53,199 --> 00:13:56,560
or actually assign resources to it we

00:13:55,519 --> 00:13:58,560
need a layer

00:13:56,560 --> 00:14:00,959
called usually referred to as container

00:13:58,560 --> 00:14:03,360
orchestration nowadays this is often

00:14:00,959 --> 00:14:05,680
used synonymous with kubernetes because

00:14:03,360 --> 00:14:07,760
kubernetes has become the default

00:14:05,680 --> 00:14:10,639
container orchestration layer but there

00:14:07,760 --> 00:14:12,800
are actually others out there as well

00:14:10,639 --> 00:14:14,880
what are the different responsibilities

00:14:12,800 --> 00:14:16,560
for the container orchestration layer

00:14:14,880 --> 00:14:18,880
first of all it's managing the

00:14:16,560 --> 00:14:21,760
containers this can go from

00:14:18,880 --> 00:14:23,680
scaling up if i need more of these same

00:14:21,760 --> 00:14:26,720
instances of a container

00:14:23,680 --> 00:14:29,920
uh or also like restarting a failed

00:14:26,720 --> 00:14:32,240
container or a failed pot in kubernetes

00:14:29,920 --> 00:14:33,440
if we have hit some quota or some error

00:14:32,240 --> 00:14:35,360
here

00:14:33,440 --> 00:14:36,800
second big responsibility is the

00:14:35,360 --> 00:14:38,720
resource management

00:14:36,800 --> 00:14:40,079
making sure that we don't overload

00:14:38,720 --> 00:14:43,680
certain resources by

00:14:40,079 --> 00:14:46,560
too many or too large containers and

00:14:43,680 --> 00:14:47,360
then actually take down that particular

00:14:46,560 --> 00:14:50,560
server

00:14:47,360 --> 00:14:53,040
or overload that particular server the

00:14:50,560 --> 00:14:53,839
last and probably most important but

00:14:53,040 --> 00:14:55,600
also most

00:14:53,839 --> 00:14:58,399
challenging aspect for container

00:14:55,600 --> 00:15:00,160
orchestrations is service management

00:14:58,399 --> 00:15:02,000
where we need to go in and actually

00:15:00,160 --> 00:15:03,279
connect multiple containers into a

00:15:02,000 --> 00:15:05,920
meaningful service

00:15:03,279 --> 00:15:06,880
and this includes uh topics like service

00:15:05,920 --> 00:15:12,079
discovery

00:15:06,880 --> 00:15:15,279
uh networking also network segregation

00:15:12,079 --> 00:15:17,600
so um this is so

00:15:15,279 --> 00:15:20,000
with this second abstraction layer aka

00:15:17,600 --> 00:15:21,760
container orchestration or kubernetes

00:15:20,000 --> 00:15:23,600
we can actually run our containers

00:15:21,760 --> 00:15:25,760
across all different infrastructures

00:15:23,600 --> 00:15:26,320
without having to worry how to deploy it

00:15:25,760 --> 00:15:28,880
we can

00:15:26,320 --> 00:15:31,120
have these same scripts for scaling them

00:15:28,880 --> 00:15:34,160
and all seems well

00:15:31,120 --> 00:15:36,160
but if we actually take a deeper look

00:15:34,160 --> 00:15:38,079
into kubernetes itself

00:15:36,160 --> 00:15:40,320
there are many moving components in

00:15:38,079 --> 00:15:42,320
there so just from like a high-level

00:15:40,320 --> 00:15:45,120
perspective there's the master nodes

00:15:42,320 --> 00:15:46,639
which correspond to the control plane

00:15:45,120 --> 00:15:49,360
there's the worker nodes

00:15:46,639 --> 00:15:51,360
corresponding to the data plane and uh

00:15:49,360 --> 00:15:52,240
those workers there's ones doing the

00:15:51,360 --> 00:15:54,880
actual work

00:15:52,240 --> 00:15:56,399
and then on top we got the kind of state

00:15:54,880 --> 00:16:00,560
story at cd

00:15:56,399 --> 00:16:03,120
managing z state challenge comes in

00:16:00,560 --> 00:16:03,600
uh that we not only have to deploy and

00:16:03,120 --> 00:16:05,920
manage

00:16:03,600 --> 00:16:07,680
one of those kubernetes clusters but a

00:16:05,920 --> 00:16:09,839
former colleague of mine used to say

00:16:07,680 --> 00:16:12,560
kubernetes clusters are like pringles

00:16:09,839 --> 00:16:13,920
you can't just have one so typically you

00:16:12,560 --> 00:16:15,360
end up with a number of different

00:16:13,920 --> 00:16:17,600
kubernetes clusters

00:16:15,360 --> 00:16:19,839
as mentioned for us we got one

00:16:17,600 --> 00:16:21,440
kubernetes cluster per cloud provider

00:16:19,839 --> 00:16:22,000
per region and that's just for the

00:16:21,440 --> 00:16:24,800
production

00:16:22,000 --> 00:16:27,199
system uh you will also have different

00:16:24,800 --> 00:16:29,440
clusters for your test

00:16:27,199 --> 00:16:31,120
and for your staging system maybe even

00:16:29,440 --> 00:16:33,360
for your development system

00:16:31,120 --> 00:16:34,399
so in a typical setup you'll probably

00:16:33,360 --> 00:16:36,160
end up

00:16:34,399 --> 00:16:37,519
with somewhere beyond five different

00:16:36,160 --> 00:16:40,320
kubernetes clusters

00:16:37,519 --> 00:16:41,360
and if you want to go the multi cloud

00:16:40,320 --> 00:16:43,519
provider

00:16:41,360 --> 00:16:45,519
road which we are going here you'll

00:16:43,519 --> 00:16:48,399
probably even end up with more

00:16:45,519 --> 00:16:48,959
uh so it's somewhere an open-ended

00:16:48,399 --> 00:16:50,959
challenge

00:16:48,959 --> 00:16:53,600
or an open-ended number of kubernetes

00:16:50,959 --> 00:16:56,639
clusters to mention

00:16:53,600 --> 00:16:59,279
and so for that we want to

00:16:56,639 --> 00:17:01,519
deploy and manage all those different

00:16:59,279 --> 00:17:04,640
instances we've seen in the architecture

00:17:01,519 --> 00:17:06,959
ourselves luckily there's

00:17:04,640 --> 00:17:08,480
yet another abstraction layer or an

00:17:06,959 --> 00:17:11,280
abstraction help for us

00:17:08,480 --> 00:17:11,919
and this is the managed kubernetes

00:17:11,280 --> 00:17:14,160
services

00:17:11,919 --> 00:17:15,520
offered by the different cloud providers

00:17:14,160 --> 00:17:17,199
meaning we don't have to set up

00:17:15,520 --> 00:17:20,160
kubernetes ourselves

00:17:17,199 --> 00:17:22,959
it's a few clicks or just again a short

00:17:20,160 --> 00:17:25,360
yaml a short terraform deploy

00:17:22,959 --> 00:17:26,640
and this is going to set us up with an

00:17:25,360 --> 00:17:29,919
entire new

00:17:26,640 --> 00:17:31,360
uh kubernetes cluster sounds great so

00:17:29,919 --> 00:17:34,960
far

00:17:31,360 --> 00:17:36,880
but naturally the next question comes

00:17:34,960 --> 00:17:37,760
why don't we just pick like for example

00:17:36,880 --> 00:17:40,000
eks

00:17:37,760 --> 00:17:41,120
google or azure as kind of our only

00:17:40,000 --> 00:17:42,799
target so

00:17:41,120 --> 00:17:46,960
why does someone actually need to do

00:17:42,799 --> 00:17:49,600
that across multiple cloud providers and

00:17:46,960 --> 00:17:51,120
this is actually the for us the most

00:17:49,600 --> 00:17:53,679
important reason assesses

00:17:51,120 --> 00:17:54,799
demand by our customers and they have

00:17:53,679 --> 00:17:58,000
different reasons

00:17:54,799 --> 00:18:00,400
for demanding this or requesting this

00:17:58,000 --> 00:18:01,600
and often this is company policy so this

00:18:00,400 --> 00:18:04,480
can either be in or

00:18:01,600 --> 00:18:05,600
exclusive so for example as amazon might

00:18:04,480 --> 00:18:07,280
be a competitor

00:18:05,600 --> 00:18:10,000
if you're yourself somewhere in the

00:18:07,280 --> 00:18:12,160
retail space you might choose not to run

00:18:10,000 --> 00:18:13,440
your workload on top of aws

00:18:12,160 --> 00:18:15,600
infrastructure

00:18:13,440 --> 00:18:16,960
other reasons might include privacy

00:18:15,600 --> 00:18:20,000
requirements uh

00:18:16,960 --> 00:18:22,320
especially with like gdpr or ccpa

00:18:20,000 --> 00:18:23,600
on the rise where you might have certain

00:18:22,320 --> 00:18:26,160
rules that your cluster

00:18:23,600 --> 00:18:28,080
might or your data is not allowed to end

00:18:26,160 --> 00:18:30,880
up somewhere in u.s

00:18:28,080 --> 00:18:32,960
or somewhere in asia depending on your

00:18:30,880 --> 00:18:36,160
company rules

00:18:32,960 --> 00:18:38,240
for other users the reason is to avoid a

00:18:36,160 --> 00:18:41,440
single vendor dependency

00:18:38,240 --> 00:18:43,760
or to achieve more flexibility the last

00:18:41,440 --> 00:18:44,799
reason unfortunately is also very common

00:18:43,760 --> 00:18:48,400
one and

00:18:44,799 --> 00:18:49,360
i would just refer to that as uh cto has

00:18:48,400 --> 00:18:51,600
read and some

00:18:49,360 --> 00:18:54,080
nice magazine that multi-cloud provider

00:18:51,600 --> 00:18:56,160
strategy is a new way to go

00:18:54,080 --> 00:18:57,919
and so this is often also just kind of

00:18:56,160 --> 00:18:59,520
like a buzzword which needs to be

00:18:57,919 --> 00:19:01,520
checked off from some list

00:18:59,520 --> 00:19:03,360
where i would actually urge everyone to

00:19:01,520 --> 00:19:05,360
really consider that because it's a

00:19:03,360 --> 00:19:08,240
pretty big investment in terms of

00:19:05,360 --> 00:19:09,039
uh resources knowledge and also

00:19:08,240 --> 00:19:12,400
maintenance

00:19:09,039 --> 00:19:15,440
over time so

00:19:12,400 --> 00:19:17,600
what were the challenges we faced by

00:19:15,440 --> 00:19:19,280
deploying our services across different

00:19:17,600 --> 00:19:21,600
cloud providers

00:19:19,280 --> 00:19:22,559
and so kind of the overview is resource

00:19:21,600 --> 00:19:25,280
handling

00:19:22,559 --> 00:19:27,200
different kubernetes versions or the

00:19:25,280 --> 00:19:28,080
version handling by the different cloud

00:19:27,200 --> 00:19:30,080
providers

00:19:28,080 --> 00:19:31,600
security with authentication

00:19:30,080 --> 00:19:34,240
authorization

00:19:31,600 --> 00:19:36,080
logging networking storage and then

00:19:34,240 --> 00:19:38,720
different container runtimes

00:19:36,080 --> 00:19:39,600
these are only the i would say kind of

00:19:38,720 --> 00:19:42,400
critical one

00:19:39,600 --> 00:19:44,400
so most important ones if you actually

00:19:42,400 --> 00:19:45,840
yourself have experienced other ones or

00:19:44,400 --> 00:19:48,640
you have any questions of

00:19:45,840 --> 00:19:49,360
what else we saw or faced out there

00:19:48,640 --> 00:19:52,240
please

00:19:49,360 --> 00:19:53,679
ping either chris or myself we are happy

00:19:52,240 --> 00:19:55,679
to either answer or forward your

00:19:53,679 --> 00:19:58,240
questions

00:19:55,679 --> 00:20:00,720
so first blowing up some steam so if we

00:19:58,240 --> 00:20:03,520
had to write an email to each of the

00:20:00,720 --> 00:20:06,480
manage cloud provider solutions this

00:20:03,520 --> 00:20:10,400
would probably be some of the content

00:20:06,480 --> 00:20:12,559
just before writing that email

00:20:10,400 --> 00:20:14,320
trust us we are pretty happy with all of

00:20:12,559 --> 00:20:15,120
them and it's been great working with

00:20:14,320 --> 00:20:17,919
all of them

00:20:15,120 --> 00:20:20,240
because they take up a lot of work of

00:20:17,919 --> 00:20:22,320
managing kubernetes from our plates

00:20:20,240 --> 00:20:24,320
we're actually pretty happy about them

00:20:22,320 --> 00:20:26,640
despite they all kind of have their own

00:20:24,320 --> 00:20:30,159
perks and their own challenges

00:20:26,640 --> 00:20:32,159
so if we are starting with amazon eks so

00:20:30,159 --> 00:20:34,720
one the probably biggest challenge we

00:20:32,159 --> 00:20:36,960
face is here the resource handling where

00:20:34,720 --> 00:20:39,280
aws is creating a number of resources on

00:20:36,960 --> 00:20:40,320
the fly load balancers security groups

00:20:39,280 --> 00:20:44,240
etc

00:20:40,320 --> 00:20:46,559
and uh these this dependency chain

00:20:44,240 --> 00:20:49,360
uh is sometimes hard to manage because

00:20:46,559 --> 00:20:51,760
removal has to occur in a certain order

00:20:49,360 --> 00:20:52,799
and also not all resources are tagged by

00:20:51,760 --> 00:20:55,679
default

00:20:52,799 --> 00:20:56,960
so we kind of had to trial an error

00:20:55,679 --> 00:21:00,159
until we had like a

00:20:56,960 --> 00:21:02,799
decent and fail safe scripts to actually

00:21:00,159 --> 00:21:06,240
remove clusters later on

00:21:02,799 --> 00:21:07,760
the second aspect is that eks being one

00:21:06,240 --> 00:21:10,960
of the

00:21:07,760 --> 00:21:11,440
mature solutions out there it actually

00:21:10,960 --> 00:21:13,919
also

00:21:11,440 --> 00:21:14,799
shows in some of ch especially with the

00:21:13,919 --> 00:21:17,039
error handling

00:21:14,799 --> 00:21:19,120
which let's just say it's not as

00:21:17,039 --> 00:21:21,360
structured as it could be

00:21:19,120 --> 00:21:23,760
so our error handling here actually

00:21:21,360 --> 00:21:27,600
involves a lot of string parsing

00:21:23,760 --> 00:21:27,600
to determine the actual error cost

00:21:28,080 --> 00:21:32,640
for google probably the biggest pain

00:21:30,400 --> 00:21:34,559
point we face is a rather aggressive

00:21:32,640 --> 00:21:35,520
update policy for the kubernetes

00:21:34,559 --> 00:21:37,760
clusters

00:21:35,520 --> 00:21:40,159
so google is very much on the forefront

00:21:37,760 --> 00:21:43,360
if it comes to new kubernetes solution

00:21:40,159 --> 00:21:43,679
kubernetes versions which is great for

00:21:43,360 --> 00:21:45,520
us

00:21:43,679 --> 00:21:47,679
because we can always test the new

00:21:45,520 --> 00:21:49,200
versions but then on the other hand

00:21:47,679 --> 00:21:52,960
they're also

00:21:49,200 --> 00:21:56,000
rather aggressive in deprecating all the

00:21:52,960 --> 00:21:58,159
versions which act uh that

00:21:56,000 --> 00:22:00,880
they will force an upgrade at a certain

00:21:58,159 --> 00:22:03,600
point and if you're running

00:22:00,880 --> 00:22:05,520
any persistent service even though we

00:22:03,600 --> 00:22:09,039
can't handle the kubernetes upgrade

00:22:05,520 --> 00:22:12,640
underneath seemingly

00:22:09,039 --> 00:22:14,320
it's only to any of the upper end stuff

00:22:12,640 --> 00:22:16,480
this is still something we want out

00:22:14,320 --> 00:22:18,480
under our control so we don't want the

00:22:16,480 --> 00:22:19,840
kubernetes upgrade to overlap with a

00:22:18,480 --> 00:22:21,840
database upgrade

00:22:19,840 --> 00:22:24,080
we want to keep that in certain

00:22:21,840 --> 00:22:27,280
maintenance windows where we actually

00:22:24,080 --> 00:22:30,000
can monitor that that happen like uh

00:22:27,280 --> 00:22:31,600
late uh middle of the night when all of

00:22:30,000 --> 00:22:34,400
our engineers are

00:22:31,600 --> 00:22:36,480
sleeping and uh this would only wake the

00:22:34,400 --> 00:22:39,919
on call if something goes wrong

00:22:36,480 --> 00:22:42,799
so uh this is where we have to cut and

00:22:39,919 --> 00:22:46,000
make sure that we are always on top of

00:22:42,799 --> 00:22:49,440
uh the latest version here uh

00:22:46,000 --> 00:22:50,400
for azure it is actually it feels in

00:22:49,440 --> 00:22:52,880
some aspect

00:22:50,400 --> 00:22:53,679
the least mature of these three big

00:22:52,880 --> 00:22:56,720
clusters

00:22:53,679 --> 00:23:00,720
missing some simple features and

00:22:56,720 --> 00:23:04,240
issues around the uh persistent

00:23:00,720 --> 00:23:06,799
uh volumes and also about cbm

00:23:04,240 --> 00:23:08,559
sets so this is actually improved quite

00:23:06,799 --> 00:23:10,960
drastically over the past

00:23:08,559 --> 00:23:11,679
months but there are always those small

00:23:10,960 --> 00:23:13,440
quirks

00:23:11,679 --> 00:23:15,280
where we kind of really have to give a

00:23:13,440 --> 00:23:18,000
shout out to the team is that

00:23:15,280 --> 00:23:19,919
the azure team is super responsive and

00:23:18,000 --> 00:23:22,799
super helpful and i think that

00:23:19,919 --> 00:23:24,240
so it's we're really curious and we're

00:23:22,799 --> 00:23:26,799
really happy in the way this is

00:23:24,240 --> 00:23:26,799
developing

00:23:27,039 --> 00:23:32,480
so the first topic um is resource

00:23:30,480 --> 00:23:35,440
creation and resource handling

00:23:32,480 --> 00:23:37,039
and we already talked about uh what be

00:23:35,440 --> 00:23:38,559
the challenges we are having here on the

00:23:37,039 --> 00:23:41,120
amazon eks side

00:23:38,559 --> 00:23:42,799
and maybe just to showcase how many

00:23:41,120 --> 00:23:44,720
different resources are created

00:23:42,799 --> 00:23:46,559
so for each kubernetes cluster we

00:23:44,720 --> 00:23:48,320
actually have a large number of

00:23:46,559 --> 00:23:50,880
different resources here so we have got

00:23:48,320 --> 00:23:52,080
to be pc we got an internet gateway we

00:23:50,880 --> 00:23:54,159
got a net gateway

00:23:52,080 --> 00:23:55,760
we got subnets we got routing tables

00:23:54,159 --> 00:23:58,960
security groups

00:23:55,760 --> 00:24:01,600
auto scaling groups uh own amis

00:23:58,960 --> 00:24:02,080
and then the actual eks cluster and

00:24:01,600 --> 00:24:03,919
again

00:24:02,080 --> 00:24:05,520
they're like certain dependencies in

00:24:03,919 --> 00:24:09,200
which they can be removed

00:24:05,520 --> 00:24:11,039
and in which uh we have to identify them

00:24:09,200 --> 00:24:12,960
because they are not necessarily all

00:24:11,039 --> 00:24:16,080
tagged if they are created as a

00:24:12,960 --> 00:24:18,080
dependency of something else with google

00:24:16,080 --> 00:24:19,360
those are actually less resources so we

00:24:18,080 --> 00:24:22,720
got a bpc

00:24:19,360 --> 00:24:24,320
the actual uh kubernetes cluster

00:24:22,720 --> 00:24:26,400
and then the different node pools used

00:24:24,320 --> 00:24:28,400
for scaling and this is actually pretty

00:24:26,400 --> 00:24:32,240
similar on the azure side

00:24:28,400 --> 00:24:34,559
where we got resource groups uh the

00:24:32,240 --> 00:24:36,000
vm scale sets and then the storage

00:24:34,559 --> 00:24:38,400
account so

00:24:36,000 --> 00:24:39,440
again this uh is pretty easily

00:24:38,400 --> 00:24:41,279
manageable

00:24:39,440 --> 00:24:42,880
the only challenge we faced here on the

00:24:41,279 --> 00:24:45,039
azure site is that some of those

00:24:42,880 --> 00:24:48,320
resources

00:24:45,039 --> 00:24:51,360
cannot be scaled to either zero which

00:24:48,320 --> 00:24:51,919
we sometimes use to actually pass a

00:24:51,360 --> 00:24:54,640
certain

00:24:51,919 --> 00:24:56,799
cluster and also the number of vm skill

00:24:54,640 --> 00:24:58,000
sets used to be limited to eight so this

00:24:56,799 --> 00:25:00,559
has been lifted by now

00:24:58,000 --> 00:25:01,440
but some of the limitations is actually

00:25:00,559 --> 00:25:04,000
rather

00:25:01,440 --> 00:25:04,640
short or rather low on a per account

00:25:04,000 --> 00:25:07,440
level

00:25:04,640 --> 00:25:09,440
whereas uh which actually limits us if

00:25:07,440 --> 00:25:10,320
we are creating many clusters across

00:25:09,440 --> 00:25:14,240
different

00:25:10,320 --> 00:25:16,480
regions kubernetes versions

00:25:14,240 --> 00:25:17,760
as also mentioned this is actually

00:25:16,480 --> 00:25:19,679
mostly an issue

00:25:17,760 --> 00:25:21,120
or a challenge i wouldn't really call it

00:25:19,679 --> 00:25:23,919
an issue actually

00:25:21,120 --> 00:25:25,919
we have with google uh kubernetes engine

00:25:23,919 --> 00:25:28,640
because they're moving really quickly

00:25:25,919 --> 00:25:31,200
both major and minor upgrades but this

00:25:28,640 --> 00:25:33,919
is just also leading to forced upgrade

00:25:31,200 --> 00:25:36,080
so it both has a good and a bad side for

00:25:33,919 --> 00:25:36,080
us

00:25:38,480 --> 00:25:45,200
kubernetes clusters in general

00:25:41,679 --> 00:25:48,080
so the cool thing is actually

00:25:45,200 --> 00:25:48,720
having a managed solutions it's great

00:25:48,080 --> 00:25:51,840
because

00:25:48,720 --> 00:25:54,799
it will actually move a lot of

00:25:51,840 --> 00:25:56,960
management burden from our shoulders but

00:25:54,799 --> 00:25:59,360
it also comes kind of at

00:25:56,960 --> 00:26:00,559
a cost so the cost is like less less

00:25:59,360 --> 00:26:03,520
flexibility

00:26:00,559 --> 00:26:04,799
so for example especially access to

00:26:03,520 --> 00:26:06,640
kubernetes api

00:26:04,799 --> 00:26:08,480
server options when starting them up so

00:26:06,640 --> 00:26:10,720
there are certain command line options

00:26:08,480 --> 00:26:13,200
we sometimes would like to have we have

00:26:10,720 --> 00:26:15,440
when we bring up a cluster ourselves

00:26:13,200 --> 00:26:16,720
but as the managed kubernetes solution

00:26:15,440 --> 00:26:18,880
doesn't expose them

00:26:16,720 --> 00:26:20,159
we simply don't have access to them and

00:26:18,880 --> 00:26:21,679
so for example one

00:26:20,159 --> 00:26:24,080
which will also come up when we talk

00:26:21,679 --> 00:26:27,279
about rc is like the authorization

00:26:24,080 --> 00:26:30,080
vapor config parameter

00:26:27,279 --> 00:26:31,760
we actually understand that ourselves

00:26:30,080 --> 00:26:34,880
simply because

00:26:31,760 --> 00:26:37,440
we are also building our own managed

00:26:34,880 --> 00:26:38,000
database service on top and we also

00:26:37,440 --> 00:26:41,039
choose to

00:26:38,000 --> 00:26:44,000
not export all little tiny detail

00:26:41,039 --> 00:26:46,320
parameter but only the parameters we

00:26:44,000 --> 00:26:50,240
feel are most critical to customers

00:26:46,320 --> 00:26:53,120
and also the kind of like sensitive ones

00:26:50,240 --> 00:26:54,159
uh we try to kind of hide because we

00:26:53,120 --> 00:26:56,799
also need to

00:26:54,159 --> 00:26:57,760
like limit ourselves to what we can test

00:26:56,799 --> 00:27:00,640
in the end

00:26:57,760 --> 00:27:02,720
and safe safely support in the end so we

00:27:00,640 --> 00:27:05,440
kind of understand the limitations

00:27:02,720 --> 00:27:06,000
here from so we're doing similar things

00:27:05,440 --> 00:27:08,799
but on the

00:27:06,000 --> 00:27:09,360
other hand it also sometimes is very

00:27:08,799 --> 00:27:11,279
annoying

00:27:09,360 --> 00:27:13,679
not to have access to all the different

00:27:11,279 --> 00:27:13,679
options

00:27:14,400 --> 00:27:18,399
authentication and authorization here

00:27:16,559 --> 00:27:19,279
each cloud provider kind of has its own

00:27:18,399 --> 00:27:21,760
solution

00:27:19,279 --> 00:27:24,000
and there also exists a number of open

00:27:21,760 --> 00:27:26,480
source and proprietary solutions

00:27:24,000 --> 00:27:28,480
we were a bit shocked when we researched

00:27:26,480 --> 00:27:31,520
different open source solutions how

00:27:28,480 --> 00:27:34,080
insecure they are underneath so

00:27:31,520 --> 00:27:36,000
if you actually look at something you we

00:27:34,080 --> 00:27:38,240
would just give you the recommendation

00:27:36,000 --> 00:27:40,559
to actually look at the code

00:27:38,240 --> 00:27:42,240
and uh some of the other proprietary

00:27:40,559 --> 00:27:43,039
solutions where we couldn't even do the

00:27:42,240 --> 00:27:44,880
code check

00:27:43,039 --> 00:27:46,640
also didn't meet our requirements

00:27:44,880 --> 00:27:49,360
because they were kind of like

00:27:46,640 --> 00:27:50,720
targeted to like a very either just one

00:27:49,360 --> 00:27:52,320
single organization and not a

00:27:50,720 --> 00:27:55,520
multi-tenant use case

00:27:52,320 --> 00:27:57,440
uh or other reasons and then uh again

00:27:55,520 --> 00:27:58,720
the issue that we don't have access to

00:27:57,440 --> 00:28:02,399
all the

00:27:58,720 --> 00:28:03,919
api server options so some of these

00:28:02,399 --> 00:28:06,480
also simply don't work in such

00:28:03,919 --> 00:28:09,120
environments so our solution right now

00:28:06,480 --> 00:28:10,960
is kubernetes service accounts for

00:28:09,120 --> 00:28:11,600
basically anything on the kubernetes

00:28:10,960 --> 00:28:13,840
side

00:28:11,600 --> 00:28:16,480
plus with this very simple layer of

00:28:13,840 --> 00:28:20,000
oasis authentication on top leveraging

00:28:16,480 --> 00:28:20,000
the service accounts underneath

00:28:20,320 --> 00:28:23,840
next challenge is logging in audit logs

00:28:23,279 --> 00:28:25,679
again

00:28:23,840 --> 00:28:28,799
here each cloud provider kind of comes

00:28:25,679 --> 00:28:31,520
up with a proprietary solution

00:28:28,799 --> 00:28:32,880
we've looked at a number of open source

00:28:31,520 --> 00:28:34,960
contributions here and

00:28:32,880 --> 00:28:36,080
what we really feel right now and we're

00:28:34,960 --> 00:28:38,320
also pretty happy

00:28:36,080 --> 00:28:39,840
having that run for like all now almost

00:28:38,320 --> 00:28:43,360
one year in production is

00:28:39,840 --> 00:28:45,360
grafana loki um which

00:28:43,360 --> 00:28:46,399
really serves as well as kind of like a

00:28:45,360 --> 00:28:49,120
central

00:28:46,399 --> 00:28:51,600
uh place to grab all our logs in in the

00:28:49,120 --> 00:28:54,720
same fashion with the same api

00:28:51,600 --> 00:28:57,760
audit blogging still is a specific topic

00:28:54,720 --> 00:29:00,000
so we offer audit logging or we support

00:28:57,760 --> 00:29:02,399
audit blogging on kind of two layers

00:29:00,000 --> 00:29:03,919
first of all the kind of kubernetes or

00:29:02,399 --> 00:29:06,720
oasis layer

00:29:03,919 --> 00:29:08,880
uh down to the actual infrastructure and

00:29:06,720 --> 00:29:10,720
then also the audit log from a wrangler

00:29:08,880 --> 00:29:14,399
db itself

00:29:10,720 --> 00:29:18,480
where each database access can be

00:29:14,399 --> 00:29:20,000
locked and retraced who triggered that

00:29:18,480 --> 00:29:21,840
so the challenge here is that we

00:29:20,000 --> 00:29:23,039
actually need some stable storage

00:29:21,840 --> 00:29:25,760
outside

00:29:23,039 --> 00:29:26,640
uh at best we would like immutable

00:29:25,760 --> 00:29:29,279
storage

00:29:26,640 --> 00:29:31,039
and there again cloud providers have

00:29:29,279 --> 00:29:32,720
very different options which we'll see

00:29:31,039 --> 00:29:34,720
on the next slide when we talk about

00:29:32,720 --> 00:29:39,360
storage

00:29:34,720 --> 00:29:40,960
so um here first of all the

00:29:39,360 --> 00:29:42,720
first challenge was to kind of see

00:29:40,960 --> 00:29:45,440
database storage or the database

00:29:42,720 --> 00:29:47,600
volumes running the actual databases is

00:29:45,440 --> 00:29:50,080
that different cloud providers have

00:29:47,600 --> 00:29:51,520
rather different performance concepts so

00:29:50,080 --> 00:29:53,919
for example

00:29:51,520 --> 00:29:55,840
even within one cloud provider like aws

00:29:53,919 --> 00:29:58,240
you've got some volume types supporting

00:29:55,840 --> 00:30:00,080
configurable iops

00:29:58,240 --> 00:30:01,919
and in general the performance

00:30:00,080 --> 00:30:03,679
characteristics so we ran pretty

00:30:01,919 --> 00:30:06,559
extensive benchmarks on that

00:30:03,679 --> 00:30:08,559
we're pretty pretty uh different let's

00:30:06,559 --> 00:30:10,880
just put it this way and so we needed

00:30:08,559 --> 00:30:14,320
some experimentation to actually offer

00:30:10,880 --> 00:30:16,320
similar performance uh ranges across

00:30:14,320 --> 00:30:19,679
different cloud providers

00:30:16,320 --> 00:30:21,760
so uh one issue we hit on

00:30:19,679 --> 00:30:23,600
azure and i just put that here not to

00:30:21,760 --> 00:30:25,440
kind of like blame azure because i said

00:30:23,600 --> 00:30:27,760
we're actually pretty happy with them

00:30:25,440 --> 00:30:28,559
but just to show you like some of the uh

00:30:27,760 --> 00:30:31,440
little

00:30:28,559 --> 00:30:32,240
uh quirks we and encountered where we

00:30:31,440 --> 00:30:34,480
see that

00:30:32,240 --> 00:30:36,480
the maturity right now is rising and so

00:30:34,480 --> 00:30:38,320
for example that was like that the pvc

00:30:36,480 --> 00:30:39,279
resizing the persistent volume claim

00:30:38,320 --> 00:30:41,919
resizing was

00:30:39,279 --> 00:30:44,000
broken uh they changed the metadata when

00:30:41,919 --> 00:30:45,840
you resized one and the pricing

00:30:44,000 --> 00:30:47,440
also reflected that but there were no

00:30:45,840 --> 00:30:49,520
changes in the file system

00:30:47,440 --> 00:30:50,960
the team was super supportive and they

00:30:49,520 --> 00:30:53,200
actually supported with a manual

00:30:50,960 --> 00:30:56,799
workaround

00:30:53,200 --> 00:30:59,679
and so now uh we are actually there

00:30:56,799 --> 00:31:00,799
if you're on azure itself that also is

00:30:59,679 --> 00:31:04,000
not an issue for you

00:31:00,799 --> 00:31:06,000
but if you're an aks i think by

00:31:04,000 --> 00:31:08,720
now it's also been fixed and we could

00:31:06,000 --> 00:31:10,880
move away from the workaround

00:31:08,720 --> 00:31:12,480
so thanks to the azure team for their

00:31:10,880 --> 00:31:14,240
support

00:31:12,480 --> 00:31:16,559
the second part is actually cluster

00:31:14,240 --> 00:31:18,000
external storage so we not only need

00:31:16,559 --> 00:31:20,960
volumes for the database

00:31:18,000 --> 00:31:23,200
itself but also for storing backups for

00:31:20,960 --> 00:31:24,720
storing audit logs so basically anything

00:31:23,200 --> 00:31:27,120
outside the database

00:31:24,720 --> 00:31:28,640
and also external to the clusterer

00:31:27,120 --> 00:31:30,480
external to the cluster meaning

00:31:28,640 --> 00:31:31,360
independent of the life cycle of the

00:31:30,480 --> 00:31:33,279
cluster

00:31:31,360 --> 00:31:34,960
and again here each cloud provider has

00:31:33,279 --> 00:31:37,360
proprietary solution

00:31:34,960 --> 00:31:38,720
and uh currently this unfortunately

00:31:37,360 --> 00:31:41,760
forces us to have multiple

00:31:38,720 --> 00:31:44,480
implementations because the s3 interface

00:31:41,760 --> 00:31:47,840
for example differs greatly from the

00:31:44,480 --> 00:31:47,840
other interfaces

00:31:48,080 --> 00:31:51,919
the next big topic you should actually

00:31:50,880 --> 00:31:55,120
look at

00:31:51,919 --> 00:31:56,080
is wherever you deploy a kubernetes

00:31:55,120 --> 00:32:00,000
cluster

00:31:56,080 --> 00:32:03,519
is networking so especially for us

00:32:00,000 --> 00:32:05,919
as we are running a multi-tenant

00:32:03,519 --> 00:32:06,960
setup we really want strict separation

00:32:05,919 --> 00:32:09,279
from deployments from

00:32:06,960 --> 00:32:10,640
each other but also of course separate

00:32:09,279 --> 00:32:12,720
the deployments

00:32:10,640 --> 00:32:14,559
so the customers at tenants from like

00:32:12,720 --> 00:32:17,440
our own platform

00:32:14,559 --> 00:32:19,279
and so the challenge here is that uh

00:32:17,440 --> 00:32:21,519
each data cluster actually

00:32:19,279 --> 00:32:24,000
it's being one kubernetes cluster needs

00:32:21,519 --> 00:32:26,720
to be placed inside one vpc

00:32:24,000 --> 00:32:27,360
and so we makes it a bit harder to

00:32:26,720 --> 00:32:30,159
leverage

00:32:27,360 --> 00:32:30,880
vpcs for a network segregation but

00:32:30,159 --> 00:32:33,360
luckily

00:32:30,880 --> 00:32:34,880
there are a number of different great

00:32:33,360 --> 00:32:37,120
networking solutions now

00:32:34,880 --> 00:32:39,760
available for kubernetes and the

00:32:37,120 --> 00:32:42,240
solution we're here for is psyllium

00:32:39,760 --> 00:32:43,919
there the others are also great just

00:32:42,240 --> 00:32:45,760
from our experimentation

00:32:43,919 --> 00:32:48,080
which happened around one and a half

00:32:45,760 --> 00:32:50,320
years ago uh this was actually the one

00:32:48,080 --> 00:32:52,480
which met our requirements best

00:32:50,320 --> 00:32:54,880
uh we've been following the other ones

00:32:52,480 --> 00:32:56,240
and uh must say like by now probably all

00:32:54,880 --> 00:32:59,600
of them are pretty

00:32:56,240 --> 00:33:01,760
in a pretty good shape so

00:32:59,600 --> 00:33:03,120
kind of the promise is and this is what

00:33:01,760 --> 00:33:05,360
we are looking for is you can also

00:33:03,120 --> 00:33:07,840
replace psyllium with your favorite tool

00:33:05,360 --> 00:33:09,200
is to actually enable network policy

00:33:07,840 --> 00:33:12,799
rules so for example

00:33:09,200 --> 00:33:15,519
parts or in our case like parts running

00:33:12,799 --> 00:33:18,559
database servers in namespace a

00:33:15,519 --> 00:33:19,440
can talk to parts in namespace b but not

00:33:18,559 --> 00:33:22,840
vice versa

00:33:19,440 --> 00:33:27,039
so we can either allow a forbid traffic

00:33:22,840 --> 00:33:30,080
explicitly and those rules are

00:33:27,039 --> 00:33:31,679
implemented using bpf or ebpf and so we

00:33:30,080 --> 00:33:33,120
actually also have the performance we

00:33:31,679 --> 00:33:35,600
need for that

00:33:33,120 --> 00:33:37,360
in theory this is cloud provider

00:33:35,600 --> 00:33:38,960
independent so this works the same way

00:33:37,360 --> 00:33:41,360
we can have the same rules across

00:33:38,960 --> 00:33:43,600
all cloud providers the only thing which

00:33:41,360 --> 00:33:46,320
significantly differs between

00:33:43,600 --> 00:33:46,720
cloud providers is the setup actually so

00:33:46,320 --> 00:33:49,360
here

00:33:46,720 --> 00:33:50,640
to set up psyllium we actually have

00:33:49,360 --> 00:33:52,799
different scripts

00:33:50,640 --> 00:33:53,760
for the different data clusters uh

00:33:52,799 --> 00:33:56,000
depending on

00:33:53,760 --> 00:33:56,799
which cloud provider that is but again

00:33:56,000 --> 00:33:58,640
this is

00:33:56,799 --> 00:33:59,919
the reason why we actually have set

00:33:58,640 --> 00:34:03,440
abstraction layer

00:33:59,919 --> 00:34:07,360
uh called data cluster

00:34:03,440 --> 00:34:10,639
so the reality if it works it's great

00:34:07,360 --> 00:34:12,320
but the cloud provider independence it's

00:34:10,639 --> 00:34:14,399
mostly there and it's getting better

00:34:12,320 --> 00:34:16,720
with each version so for example

00:34:14,399 --> 00:34:19,520
precision 1.7

00:34:16,720 --> 00:34:20,560
there was the issue that pod siders were

00:34:19,520 --> 00:34:22,639
said differently

00:34:20,560 --> 00:34:24,399
across different cloud providers which

00:34:22,639 --> 00:34:24,720
could actually mean that we ended up

00:34:24,399 --> 00:34:29,200
with

00:34:24,720 --> 00:34:32,800
colliding pot ciders uh on

00:34:29,200 --> 00:34:35,280
on aws and so our kind of

00:34:32,800 --> 00:34:36,159
impression was at the beginning that

00:34:35,280 --> 00:34:40,159
they used

00:34:36,159 --> 00:34:42,879
mostly google for testing but then

00:34:40,159 --> 00:34:43,520
included like just minor testing for aws

00:34:42,879 --> 00:34:45,919
this is just

00:34:43,520 --> 00:34:46,560
our theory and again this is not to kind

00:34:45,919 --> 00:34:48,960
of like

00:34:46,560 --> 00:34:50,800
blame or shame any of these solutions

00:34:48,960 --> 00:34:52,480
it's just one of those challenges you

00:34:50,800 --> 00:34:55,440
should be aware about

00:34:52,480 --> 00:34:56,720
just because you test one tool on one

00:34:55,440 --> 00:34:58,720
cloud provider

00:34:56,720 --> 00:35:00,800
and it should just work across all of

00:34:58,720 --> 00:35:02,000
them you should actually test all the

00:35:00,800 --> 00:35:05,119
tools across all

00:35:02,000 --> 00:35:07,040
different cloud providers so it's still

00:35:05,119 --> 00:35:09,200
great

00:35:07,040 --> 00:35:10,560
so after seeing all of this you might

00:35:09,200 --> 00:35:12,720
actually wonder

00:35:10,560 --> 00:35:13,839
i don't want to run with any of those

00:35:12,720 --> 00:35:16,640
cloud providers

00:35:13,839 --> 00:35:17,040
well i still have the same problem here

00:35:16,640 --> 00:35:20,240
so

00:35:17,040 --> 00:35:23,440
if we are looking at the on prem kind of

00:35:20,240 --> 00:35:25,119
distributions for kubernetes so i

00:35:23,440 --> 00:35:27,520
checked i think this was like

00:35:25,119 --> 00:35:28,160
three four days ago there were 68

00:35:27,520 --> 00:35:30,640
different

00:35:28,160 --> 00:35:32,000
certified kubernetes distribution by

00:35:30,640 --> 00:35:35,359
cncf

00:35:32,000 --> 00:35:36,880
and they also all kind of go for

00:35:35,359 --> 00:35:40,079
different

00:35:36,880 --> 00:35:40,720
tools being included it's probably not

00:35:40,079 --> 00:35:43,359
going to

00:35:40,720 --> 00:35:44,000
be such a big problem as if you have to

00:35:43,359 --> 00:35:45,760
compare

00:35:44,000 --> 00:35:48,000
between different cloud providers

00:35:45,760 --> 00:35:50,079
because most likely you'll choose

00:35:48,000 --> 00:35:51,520
one of those distributions and stick to

00:35:50,079 --> 00:35:53,760
that

00:35:51,520 --> 00:35:55,599
but still you should be aware that

00:35:53,760 --> 00:35:59,200
moving between those different

00:35:55,599 --> 00:36:00,480
uh kubernetes distributions despite them

00:35:59,200 --> 00:36:03,119
being certified

00:36:00,480 --> 00:36:03,760
might actually yield similar problems uh

00:36:03,119 --> 00:36:05,839
than

00:36:03,760 --> 00:36:07,520
uh we faced when moving between

00:36:05,839 --> 00:36:10,839
different cloud providers

00:36:07,520 --> 00:36:12,000
um on uh between different cloud

00:36:10,839 --> 00:36:15,040
providers

00:36:12,000 --> 00:36:16,800
so will that get better in the future so

00:36:15,040 --> 00:36:18,400
the progress we are seeing so for

00:36:16,800 --> 00:36:19,920
example with the tools

00:36:18,400 --> 00:36:21,839
with the different cloud providers

00:36:19,920 --> 00:36:24,480
getting more matures we believe yes

00:36:21,839 --> 00:36:26,079
it will get better but there will also

00:36:24,480 --> 00:36:28,560
be new challenges so

00:36:26,079 --> 00:36:30,079
for example a field we are currently

00:36:28,560 --> 00:36:33,440
tracking quite a bit

00:36:30,079 --> 00:36:36,240
is i would call it continuum runtimes uh

00:36:33,440 --> 00:36:36,960
i'll split that up a bit more in a

00:36:36,240 --> 00:36:39,760
second

00:36:36,960 --> 00:36:41,680
so just to structure that a bit more so

00:36:39,760 --> 00:36:44,000
actual container runtimes

00:36:41,680 --> 00:36:44,960
and then the second field which i would

00:36:44,000 --> 00:36:48,240
kind of

00:36:44,960 --> 00:36:49,520
label secure container containerization

00:36:48,240 --> 00:36:53,280
tools

00:36:49,520 --> 00:36:55,599
where we are following this area because

00:36:53,280 --> 00:36:56,320
a we want to be more efficient in terms

00:36:55,599 --> 00:36:58,640
of

00:36:56,320 --> 00:37:01,119
what we run so that's why we look at

00:36:58,640 --> 00:37:04,800
different container runtimes

00:37:01,119 --> 00:37:06,480
but also these secure containers

00:37:04,800 --> 00:37:08,400
options like firecracker cutter

00:37:06,480 --> 00:37:10,720
container g-visor nabla

00:37:08,400 --> 00:37:12,960
they're being introduced to kind of

00:37:10,720 --> 00:37:16,079
bridge this gap between

00:37:12,960 --> 00:37:17,040
the flexibility and low resource

00:37:16,079 --> 00:37:20,720
footprint of

00:37:17,040 --> 00:37:21,599
containers and then the true isolation

00:37:20,720 --> 00:37:24,720
properties

00:37:21,599 --> 00:37:27,200
of virtual machines and as we

00:37:24,720 --> 00:37:28,240
running multiple tenants with actually

00:37:27,200 --> 00:37:30,800
sensitive data

00:37:28,240 --> 00:37:31,599
on on a platform we are really

00:37:30,800 --> 00:37:34,240
interested

00:37:31,599 --> 00:37:34,960
in efficient isolation between different

00:37:34,240 --> 00:37:36,560
tenants

00:37:34,960 --> 00:37:38,880
and this is of course why we are also

00:37:36,560 --> 00:37:41,280
following this and so

00:37:38,880 --> 00:37:42,720
what we are kind of seeing here is that

00:37:41,280 --> 00:37:44,880
different cloud providers

00:37:42,720 --> 00:37:46,599
google kind of pushing for g visor

00:37:44,880 --> 00:37:48,480
amazon pushing more for on the

00:37:46,599 --> 00:37:53,119
firecracker side

00:37:48,480 --> 00:37:55,359
uh ibm uh highly involved in qatar

00:37:53,119 --> 00:37:58,000
and so different providers are actually

00:37:55,359 --> 00:37:59,920
pushing for different options here

00:37:58,000 --> 00:38:01,280
and so i believe this is still going to

00:37:59,920 --> 00:38:04,480
be interesting

00:38:01,280 --> 00:38:05,440
where we will end up in 2021 with those

00:38:04,480 --> 00:38:07,839
different

00:38:05,440 --> 00:38:11,200
container runtimes and also secure

00:38:07,839 --> 00:38:14,480
containerization options

00:38:11,200 --> 00:38:17,359
with this this actually brings us to the

00:38:14,480 --> 00:38:18,480
end of our talk and so kind of if we had

00:38:17,359 --> 00:38:21,520
to summarize

00:38:18,480 --> 00:38:22,160
our our experiences and again this is

00:38:21,520 --> 00:38:25,359
kind of like

00:38:22,160 --> 00:38:27,920
a gut feeling so uh

00:38:25,359 --> 00:38:28,720
take it with a grain of salt aws is

00:38:27,920 --> 00:38:33,040
clearly the

00:38:28,720 --> 00:38:36,320
older uh older cloud compared to gcp

00:38:33,040 --> 00:38:38,720
or azure and especially the api is

00:38:36,320 --> 00:38:39,839
showing some age on the other hand this

00:38:38,720 --> 00:38:43,280
maturity

00:38:39,839 --> 00:38:45,680
also results in stability so for example

00:38:43,280 --> 00:38:47,359
if we compare network outages

00:38:45,680 --> 00:38:49,119
across the different ones or just

00:38:47,359 --> 00:38:52,240
overall stability amazon is actually

00:38:49,119 --> 00:38:52,240
doing pretty great

00:38:52,560 --> 00:38:56,000
if we look at managed kubernetes

00:38:54,960 --> 00:38:58,480
solution

00:38:56,000 --> 00:39:00,560
google actually has a pretty strong

00:38:58,480 --> 00:39:04,560
offering just like very nice

00:39:00,560 --> 00:39:05,520
shiny apis but we are missing some of

00:39:04,560 --> 00:39:09,280
the features

00:39:05,520 --> 00:39:11,760
uh which amazon is offering being the

00:39:09,280 --> 00:39:12,839
let's call it more mature cloud out

00:39:11,760 --> 00:39:16,400
there

00:39:12,839 --> 00:39:18,560
um what we are seeing with azure it's

00:39:16,400 --> 00:39:19,760
less mature but it's actually catching

00:39:18,560 --> 00:39:22,400
up rather quickly

00:39:19,760 --> 00:39:22,880
so we are really curious in uh following

00:39:22,400 --> 00:39:24,880
this

00:39:22,880 --> 00:39:28,640
and seeing where they end up end up

00:39:24,880 --> 00:39:31,760
pretty happy with the progress here

00:39:28,640 --> 00:39:33,760
so uh with that thanks for listening i

00:39:31,760 --> 00:39:37,359
would leave a few minutes

00:39:33,760 --> 00:39:39,200
now for questions but also feel free to

00:39:37,359 --> 00:39:40,160
reach out with any feedback or questions

00:39:39,200 --> 00:39:43,680
to either chris

00:39:40,160 --> 00:39:45,839
myself a wrangler db if you just want to

00:39:43,680 --> 00:39:46,720
try out a rangodb being an open source

00:39:45,839 --> 00:39:48,640
database

00:39:46,720 --> 00:39:51,440
you can either contribute or just start

00:39:48,640 --> 00:39:53,440
by doing a simple docker puller rangodb

00:39:51,440 --> 00:39:55,839
and start using multimodel as your

00:39:53,440 --> 00:39:58,240
database

00:39:55,839 --> 00:39:59,920
any questions in the chat or anywhere

00:39:58,240 --> 00:40:03,760
else chris feel also free

00:39:59,920 --> 00:40:03,760
to shout in any questions

00:40:05,599 --> 00:40:09,040
i haven't seen any questions oh looks

00:40:07,839 --> 00:40:12,560
like helen

00:40:09,040 --> 00:40:12,560
had a question i saw a hand go up

00:40:15,920 --> 00:40:21,440
um we've got one from

00:40:19,119 --> 00:40:25,839
krishna as well how do you manage

00:40:21,440 --> 00:40:25,839
multi-cloud providers

00:40:28,880 --> 00:40:32,960
can you specify the question a bit more

00:40:36,960 --> 00:40:40,160
so which aspect of managing do you mean

00:40:39,680 --> 00:40:44,079
so

00:40:40,160 --> 00:40:46,720
uh as as i think the takeaway kind of

00:40:44,079 --> 00:40:48,079
uh of this talk is like kubernetes is

00:40:46,720 --> 00:40:51,839
really helping

00:40:48,079 --> 00:40:52,640
us to abstract away a lot of the details

00:40:51,839 --> 00:40:54,800
and then

00:40:52,640 --> 00:40:57,359
again managed kubernetes helps us to

00:40:54,800 --> 00:41:00,800
again abstract away details from

00:40:57,359 --> 00:41:01,920
managing kubernetes but on the other

00:41:00,800 --> 00:41:05,520
hand different

00:41:01,920 --> 00:41:08,240
choices and different tool support

00:41:05,520 --> 00:41:09,839
external tool support might lead to some

00:41:08,240 --> 00:41:13,359
of the challenges that it's not

00:41:09,839 --> 00:41:13,359
all fully abstracted away

00:41:18,319 --> 00:41:21,680
and we have another question uh how do

00:41:20,880 --> 00:41:25,200
you manage

00:41:21,680 --> 00:41:25,200
kubernetes configuration

00:41:25,680 --> 00:41:33,280
oh that's uh that's a good question so

00:41:29,520 --> 00:41:36,319
um we actually have that uh version as

00:41:33,280 --> 00:41:37,520
well and we only change that between

00:41:36,319 --> 00:41:39,920
different upgrades

00:41:37,520 --> 00:41:40,720
and this is currently managed we

00:41:39,920 --> 00:41:42,480
actually have

00:41:40,720 --> 00:41:45,040
written an own operator i think there

00:41:42,480 --> 00:41:47,359
was already today a talk about operators

00:41:45,040 --> 00:41:48,560
uh for basically deploying that and

00:41:47,359 --> 00:41:51,920
setting that up

00:41:48,560 --> 00:41:54,000
we looked a lot at different kind of

00:41:51,920 --> 00:41:55,200
solutions from terraform over other

00:41:54,000 --> 00:41:57,599
things

00:41:55,200 --> 00:41:59,359
but the operator basically reading those

00:41:57,599 --> 00:42:02,400
config files

00:41:59,359 --> 00:42:05,920
ended up being the most flexible and

00:42:02,400 --> 00:42:05,920
most stable solution for us here

00:42:06,319 --> 00:42:11,280
excellent and next one did you ever

00:42:08,079 --> 00:42:14,000
consider cops instead of eks to run k8

00:42:11,280 --> 00:42:14,000
in aws

00:42:14,400 --> 00:42:21,040
yes we looked at that and

00:42:17,440 --> 00:42:24,560
again there the uh argument right now is

00:42:21,040 --> 00:42:27,119
uh is uh that a we

00:42:24,560 --> 00:42:28,560
we have the intro quite a good working

00:42:27,119 --> 00:42:30,720
infrastructure in place

00:42:28,560 --> 00:42:32,160
and we started when cops were still kind

00:42:30,720 --> 00:42:35,200
of coming up

00:42:32,160 --> 00:42:39,280
but it's it's still a topic which we

00:42:35,200 --> 00:42:39,280
might change uh at some point

00:42:39,520 --> 00:42:45,760
it's just currently not our biggest pain

00:42:41,760 --> 00:42:47,200
point here

00:42:45,760 --> 00:42:49,920
excellent all right pretty good

00:42:47,200 --> 00:42:49,920
questions there

00:42:50,560 --> 00:42:54,640
pardon my interruption there are three

00:42:51,839 --> 00:42:59,839
minutes left three minutes left

00:42:54,640 --> 00:42:59,839
any final questions we have

00:43:01,119 --> 00:43:05,359
would you two like to uh shout out your

00:43:03,760 --> 00:43:07,760
social media or contact information one

00:43:05,359 --> 00:43:07,760
more time

00:43:08,319 --> 00:43:11,440
uh sure yeah reach me on our community

00:43:10,960 --> 00:43:13,760
slack

00:43:11,440 --> 00:43:15,280
chris.orango tv or you can find me at

00:43:13,760 --> 00:43:18,240
twitter with

00:43:15,280 --> 00:43:21,040
c woodward with zeros in place of the

00:43:18,240 --> 00:43:21,040
o's and the a's

00:43:22,079 --> 00:43:28,079
and yeah yorkshire on twitter

00:43:26,079 --> 00:43:30,560
and also in the community slack this is

00:43:28,079 --> 00:43:33,119
one advantage of my name that

00:43:30,560 --> 00:43:34,880
it's pretty easy to find so if you're on

00:43:33,119 --> 00:43:36,640
our community slack

00:43:34,880 --> 00:43:38,880
i just look for jerk and you should be

00:43:36,640 --> 00:43:41,839
able to find me

00:43:38,880 --> 00:43:41,839
there's one more question

00:43:42,000 --> 00:43:45,400
uh well yeah one last one i know you

00:43:43,760 --> 00:43:48,319
talked about logging

00:43:45,400 --> 00:43:49,599
aggregation with grafana what about

00:43:48,319 --> 00:43:51,359
metric monitoring are you using

00:43:49,599 --> 00:43:53,119
prometheus cloudwatch or something

00:43:51,359 --> 00:43:56,079
similar

00:43:53,119 --> 00:43:58,079
so uh we used prometheus currently we're

00:43:56,079 --> 00:44:01,280
actually evaluating

00:43:58,079 --> 00:44:02,960
uh how to combine so currently our setup

00:44:01,280 --> 00:44:04,560
is actually that we have one prometeur

00:44:02,960 --> 00:44:07,920
set up for each cluster

00:44:04,560 --> 00:44:09,359
and currently we are actually evaluating

00:44:07,920 --> 00:44:12,640
how we can pull all of this

00:44:09,359 --> 00:44:14,640
uh together into one big uh instance

00:44:12,640 --> 00:44:16,720
or one central instance and this is

00:44:14,640 --> 00:44:18,960
actually a really interesting

00:44:16,720 --> 00:44:20,800
development so there are

00:44:18,960 --> 00:44:22,800
two main projects we are looking in and

00:44:20,800 --> 00:44:24,160
i believe if i looked correctly at the

00:44:22,800 --> 00:44:27,520
schedule there's actually

00:44:24,160 --> 00:44:30,480
also an own talk about that

00:44:27,520 --> 00:44:31,920
and uh we'll probably give an update

00:44:30,480 --> 00:44:34,839
because that's the

00:44:31,920 --> 00:44:37,760
decision hopefully is gonna come soon uh

00:44:34,839 --> 00:44:41,359
whether how we're going to standardize

00:44:37,760 --> 00:44:42,240
on or centralize our metric server so

00:44:41,359 --> 00:44:45,280
it's still

00:44:42,240 --> 00:44:51,839
it's basically just uh which centralized

00:44:45,280 --> 00:44:53,920
solution are we going to go for

00:44:51,839 --> 00:44:53,920

YouTube URL: https://www.youtube.com/watch?v=08nz7Ot_qh4


