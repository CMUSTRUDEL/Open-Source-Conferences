Title: Karl Weinmeister - Deploy your ML Workloads on Kubernetes with Kubeflow
Publication date: 2020-05-20
Playlist: Open Source 101 at Home 2020
Description: 
	Deploy your ML Workloads on Kubernetes with Kubeflow

Presented at: Open Source 101 at Home 2020
Presented by: Karl Weinmeister, Google

Abstract: Kubeflow is an open-source project dedicated to making deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable. In this lightning talk, youâ€™ll learn about the problems Kubeflow solves and how it works.

In this talk, we will cover:
- What are typical problems that Kubeflow solves
- What are the main components of Kubeflow
- How does Kubeflow interact with Kubernetes

For more info: https://opensource101.com
Captions: 
	00:00:05,400 --> 00:00:11,840
[Music]

00:00:09,260 --> 00:00:14,130
well hi everyone my name is Jason pivots

00:00:11,840 --> 00:00:16,379
thanks for attending and supporting open

00:00:14,130 --> 00:00:17,760
source 101 I'd like to invite you to

00:00:16,379 --> 00:00:20,369
visit the Red Hat booth after the

00:00:17,760 --> 00:00:22,320
session and learn more about some of our

00:00:20,369 --> 00:00:23,699
communities today I've got two quick

00:00:22,320 --> 00:00:25,830
blog posts so I'm going to drop in the

00:00:23,699 --> 00:00:28,650
chat for you to share first we've got

00:00:25,830 --> 00:00:30,240
one for work cube flow on open shift

00:00:28,650 --> 00:00:33,000
hopefully encourage you to check that

00:00:30,240 --> 00:00:36,170
out and then I've got a fun one for you

00:00:33,000 --> 00:00:38,640
the definitive pronunciation guide for

00:00:36,170 --> 00:00:40,230
well I'll I won't read it to you but

00:00:38,640 --> 00:00:42,270
you'll you'll see how it's spelled here

00:00:40,230 --> 00:00:44,460
drop those in chat and now I'd like to

00:00:42,270 --> 00:00:45,600
introduce you to today's speaker Karl

00:00:44,460 --> 00:00:47,640
who's going to talk to you about

00:00:45,600 --> 00:00:49,950
deploying your machine learning

00:00:47,640 --> 00:00:54,120
workflows on kubernetes with cube flow

00:00:49,950 --> 00:00:58,199
Karl take it away everyone happy to be

00:00:54,120 --> 00:00:59,430
here from home here in Austin so I was

00:00:58,199 --> 00:01:01,760
going to be there in person you know

00:00:59,430 --> 00:01:05,790
would be a local event but here we are

00:01:01,760 --> 00:01:08,670
so I'm with Google's developer advocacy

00:01:05,790 --> 00:01:10,340
team I focus on data science and data

00:01:08,670 --> 00:01:14,100
engineering I've been doing a lot of

00:01:10,340 --> 00:01:16,290
DevOps and female works or that

00:01:14,100 --> 00:01:20,400
intersection that they now call ml Ops

00:01:16,290 --> 00:01:24,150
for several years now so I'm gonna start

00:01:20,400 --> 00:01:28,560
with our presentation and I'm going to

00:01:24,150 --> 00:01:35,970
go ahead and get started with that in

00:01:28,560 --> 00:01:40,619
just a moment here we go and I hope

00:01:35,970 --> 00:01:43,979
everybody can see my screen okay so

00:01:40,619 --> 00:01:45,390
let's go through the agenda here I'll

00:01:43,979 --> 00:01:47,549
talk a little bit about the challenges

00:01:45,390 --> 00:01:50,729
of running a machine learning project

00:01:47,549 --> 00:01:53,670
and we'll talk about how cube flow helps

00:01:50,729 --> 00:01:55,229
with that and we'll get into a demo

00:01:53,670 --> 00:01:56,759
which is always the fun part so that's

00:01:55,229 --> 00:02:00,030
just kind of a very quick version of the

00:01:56,759 --> 00:02:03,540
agenda today so let's just set the stage

00:02:00,030 --> 00:02:05,610
with why are we even here so especially

00:02:03,540 --> 00:02:09,030
a machine learning there's been a lot of

00:02:05,610 --> 00:02:13,080
emphasis on prototyping and research

00:02:09,030 --> 00:02:16,230
getting a model out the door

00:02:13,080 --> 00:02:18,870
it looks great it's got 99% accuracy

00:02:16,230 --> 00:02:21,060
let's put it in production well we all

00:02:18,870 --> 00:02:23,520
know that from working in the software

00:02:21,060 --> 00:02:26,310
world for many years that you know we

00:02:23,520 --> 00:02:28,410
write our unit tests we set up C ICD

00:02:26,310 --> 00:02:30,000
there's just a lot of guardrails that we

00:02:28,410 --> 00:02:33,600
need to set up to make sure we're

00:02:30,000 --> 00:02:37,200
successful after we've got an initial

00:02:33,600 --> 00:02:38,480
promise of a working application so

00:02:37,200 --> 00:02:41,430
let's talk about some of the challenges

00:02:38,480 --> 00:02:44,610
the first is continuous monitoring and

00:02:41,430 --> 00:02:47,880
that's this idea that you know over time

00:02:44,610 --> 00:02:50,209
your model accuracy may drop and we'll

00:02:47,880 --> 00:02:52,890
talk about a few reasons for that and

00:02:50,209 --> 00:02:55,260
you'll need some process in place for

00:02:52,890 --> 00:02:56,760
continuously monitoring to see how is

00:02:55,260 --> 00:02:59,940
that model doing so you don't end up

00:02:56,760 --> 00:03:02,250
with fire drills as it starts to degrade

00:02:59,940 --> 00:03:05,610
over time and so you have a process in

00:03:02,250 --> 00:03:08,310
place for that another issue is called

00:03:05,610 --> 00:03:10,260
training serving skew so when you build

00:03:08,310 --> 00:03:12,120
a machine learning model you're learning

00:03:10,260 --> 00:03:14,010
from a lot of examples you're looking at

00:03:12,120 --> 00:03:17,430
raw data and trying to figure out what

00:03:14,010 --> 00:03:19,470
the rules are from that raw data now if

00:03:17,430 --> 00:03:22,920
there's a difference between the data

00:03:19,470 --> 00:03:25,190
you trained on and the data that people

00:03:22,920 --> 00:03:27,630
predict on after it's in production

00:03:25,190 --> 00:03:29,190
you're also going to start seeing some

00:03:27,630 --> 00:03:32,880
issues and this is a very common thing

00:03:29,190 --> 00:03:34,950
that can happen accidentally if maybe

00:03:32,880 --> 00:03:38,910
there's a bug in your code the code that

00:03:34,950 --> 00:03:41,190
you use to transform your data in the

00:03:38,910 --> 00:03:44,190
training set is actually different from

00:03:41,190 --> 00:03:47,310
the code for prediction maybe there's

00:03:44,190 --> 00:03:49,410
some you know extra feature or some

00:03:47,310 --> 00:03:52,290
difference in how you transform the data

00:03:49,410 --> 00:03:54,900
that can happen or just you start to see

00:03:52,290 --> 00:03:57,750
that the model the the real world used

00:03:54,900 --> 00:03:59,700
usage of your model different data is

00:03:57,750 --> 00:04:02,730
being sent to that model so that's

00:03:59,700 --> 00:04:03,900
something you want to look for finally

00:04:02,730 --> 00:04:06,959
another challenge I want to throw out

00:04:03,900 --> 00:04:09,180
there is this idea of models having

00:04:06,959 --> 00:04:11,760
different freshness requirements so

00:04:09,180 --> 00:04:14,190
let's look at some examples of this so

00:04:11,760 --> 00:04:16,169
if it's say you may be looking at news

00:04:14,190 --> 00:04:19,709
clearly that that's changing so quickly

00:04:16,169 --> 00:04:21,720
that a model may get stale quicker than

00:04:19,709 --> 00:04:24,720
something like on the other extreme a

00:04:21,720 --> 00:04:26,590
voice-recognition model that's going to

00:04:24,720 --> 00:04:28,330
say stay stable for

00:04:26,590 --> 00:04:31,240
time--and and everywhere in between

00:04:28,330 --> 00:04:34,660
maybe in e-commerce your product catalog

00:04:31,240 --> 00:04:38,139
hmm starts to change and if you're doing

00:04:34,660 --> 00:04:40,990
recommendations or the movies change or

00:04:38,139 --> 00:04:43,419
what have you then you'll just need to

00:04:40,990 --> 00:04:46,300
think about a strategy for retraining

00:04:43,419 --> 00:04:48,610
your models with new data so that you're

00:04:46,300 --> 00:04:53,830
not just looking at data that's no

00:04:48,610 --> 00:04:56,080
longer relevant to the data for today so

00:04:53,830 --> 00:04:58,210
when you look at data science a lot of

00:04:56,080 --> 00:05:00,669
times we think about building the model

00:04:58,210 --> 00:05:03,970
as everything there is this is the

00:05:00,669 --> 00:05:06,580
center of data science right

00:05:03,970 --> 00:05:08,650
but there's a lot more and especially if

00:05:06,580 --> 00:05:12,070
we worked in other disciplines of

00:05:08,650 --> 00:05:13,660
software engineering data engineering to

00:05:12,070 --> 00:05:16,390
get to the point where you can even

00:05:13,660 --> 00:05:18,690
build a model there's understanding your

00:05:16,390 --> 00:05:20,730
data sources transforming the data

00:05:18,690 --> 00:05:24,640
validating it you know making sure that

00:05:20,730 --> 00:05:27,610
it's all you have the right amount of

00:05:24,640 --> 00:05:29,530
data the right types of data you'll need

00:05:27,610 --> 00:05:31,690
to train your model you'll need to look

00:05:29,530 --> 00:05:33,580
at the outputs of your model and we'll

00:05:31,690 --> 00:05:36,070
look at that a little bit in the demo

00:05:33,580 --> 00:05:38,320
later today we'll need to put into

00:05:36,070 --> 00:05:42,100
production serve that put a REST API

00:05:38,320 --> 00:05:43,900
around it you monitor it log it so a

00:05:42,100 --> 00:05:46,350
whole bunch of other things that you

00:05:43,900 --> 00:05:48,460
can't just take for granted and that's

00:05:46,350 --> 00:05:50,020
we're going to see how the cube flow can

00:05:48,460 --> 00:05:52,630
help it

00:05:50,020 --> 00:05:54,100
so another angle here and the difficulty

00:05:52,630 --> 00:05:57,460
of setting up a machine learning

00:05:54,100 --> 00:05:59,710
environment is not just the process but

00:05:57,460 --> 00:06:02,370
the layers of the stack there are a lot

00:05:59,710 --> 00:06:06,160
of them there's the model tooling

00:06:02,370 --> 00:06:09,450
drivers etc you know there's specialized

00:06:06,160 --> 00:06:11,890
chipsets that may require a different

00:06:09,450 --> 00:06:15,550
different code to run on different

00:06:11,890 --> 00:06:17,260
parameters and so it's just critical to

00:06:15,550 --> 00:06:20,710
get all these pieces of the stack

00:06:17,260 --> 00:06:22,780
working well together to be able to run

00:06:20,710 --> 00:06:25,090
those processes in a consistent way so

00:06:22,780 --> 00:06:27,100
say you have that set up in your dev

00:06:25,090 --> 00:06:29,080
environment there's the issue of having

00:06:27,100 --> 00:06:31,530
consistent environments making sure that

00:06:29,080 --> 00:06:34,030
everybody's using this you know simple

00:06:31,530 --> 00:06:36,520
simple issue is just using the same

00:06:34,030 --> 00:06:38,710
version of tensorflow or different

00:06:36,520 --> 00:06:40,030
Python packages but there's a whole

00:06:38,710 --> 00:06:41,890
bunch of other things you want to make

00:06:40,030 --> 00:06:43,990
sure that are consistent across these

00:06:41,890 --> 00:06:47,500
different stacks for each of your

00:06:43,990 --> 00:06:49,690
environments so here comes Cube flow so

00:06:47,500 --> 00:06:51,820
let's look at what the mission statement

00:06:49,690 --> 00:06:54,580
is for cube flow and I like this mission

00:06:51,820 --> 00:06:57,280
statement it's stayed constant for the

00:06:54,580 --> 00:06:59,230
last three years of the project been the

00:06:57,280 --> 00:07:01,240
North Star for the project and I think

00:06:59,230 --> 00:07:03,580
it does a good job of explaining all the

00:07:01,240 --> 00:07:06,190
things at Det so first making it easy

00:07:03,580 --> 00:07:09,240
for everyone so the project tries to

00:07:06,190 --> 00:07:11,740
provide defaults out-of-the-box

00:07:09,240 --> 00:07:14,860
configurations and installation so that

00:07:11,740 --> 00:07:16,600
you don't have to be a kubernetes expert

00:07:14,860 --> 00:07:19,060
but you

00:07:16,600 --> 00:07:21,490
and customize it quite a bit as neaten

00:07:19,060 --> 00:07:23,920
as a beauty of open source that it's

00:07:21,490 --> 00:07:25,750
open you can override things you can

00:07:23,920 --> 00:07:29,410
take the docker files that come with it

00:07:25,750 --> 00:07:31,930
and extend those the next part of the

00:07:29,410 --> 00:07:33,880
mission statement is what you can do it

00:07:31,930 --> 00:07:36,510
that develop deploy and manage so the

00:07:33,880 --> 00:07:40,180
whole lifecycle of machine learning and

00:07:36,510 --> 00:07:43,690
finally it's portable and distributed so

00:07:40,180 --> 00:07:47,440
you can easily take your workloads and

00:07:43,690 --> 00:07:49,600
port them across on pram cloud different

00:07:47,440 --> 00:07:52,030
types of environments a nice thing too

00:07:49,600 --> 00:07:54,610
is if you're using kubernetes for your

00:07:52,030 --> 00:07:56,800
traditional software development you can

00:07:54,610 --> 00:07:58,630
use it for machine learning as well and

00:07:56,800 --> 00:08:00,610
you don't have two different stacks to

00:07:58,630 --> 00:08:02,520
worry about you know your DevOps folks

00:08:00,610 --> 00:08:04,630
don't have to manage two different

00:08:02,520 --> 00:08:06,280
environments so learning curves around

00:08:04,630 --> 00:08:08,350
those and then finally there's a

00:08:06,280 --> 00:08:09,880
distributed nature of kubernetes which

00:08:08,350 --> 00:08:13,110
is perfect for machine learning where

00:08:09,880 --> 00:08:16,030
you need to scale your workload across

00:08:13,110 --> 00:08:20,920
your whole cluster as you're doing

00:08:16,030 --> 00:08:23,700
training and massive scale so let's look

00:08:20,920 --> 00:08:26,890
at the core capabilities of queue flow

00:08:23,700 --> 00:08:29,500
starting and clockwise fashion from the

00:08:26,890 --> 00:08:31,000
top so you have your development

00:08:29,500 --> 00:08:33,280
environment that's Jupiter notebooks

00:08:31,000 --> 00:08:35,560
I'll show you what that looks like later

00:08:33,280 --> 00:08:38,560
that's sort of the IDE for your data

00:08:35,560 --> 00:08:41,800
scientists you've got operators which

00:08:38,560 --> 00:08:45,040
are essentially kubernetes api is for

00:08:41,800 --> 00:08:47,310
specific framework options like

00:08:45,040 --> 00:08:49,480
tensorflow pi torch XP base etcetera

00:08:47,310 --> 00:08:52,210
you've got the ability to build

00:08:49,480 --> 00:08:54,250
workflows and pipelines and and the

00:08:52,210 --> 00:08:55,900
pipeline's in particular an area that I

00:08:54,250 --> 00:08:58,480
want to focus on in this presentation

00:08:55,900 --> 00:09:01,960
and show in the demo there's data

00:08:58,480 --> 00:09:04,570
management and this is key in that the

00:09:01,960 --> 00:09:09,550
have reproducible results you need to

00:09:04,570 --> 00:09:11,860
not just know what your model parameters

00:09:09,550 --> 00:09:14,320
are but you need to know where the data

00:09:11,860 --> 00:09:16,840
came from and have that lineage be able

00:09:14,320 --> 00:09:20,010
to track the the data coming into your

00:09:16,840 --> 00:09:22,660
model you also need some tools around

00:09:20,010 --> 00:09:25,420
debugging your models something called

00:09:22,660 --> 00:09:28,180
hyper parameter tuning where you're

00:09:25,420 --> 00:09:29,660
trying to find say the optimal number of

00:09:28,180 --> 00:09:32,720
layers in a knurled

00:09:29,660 --> 00:09:35,360
or units and you can do a search across

00:09:32,720 --> 00:09:37,280
a broad space of possibilities to

00:09:35,360 --> 00:09:39,200
optimize what those parameters will be

00:09:37,280 --> 00:09:41,810
to get the best result again another

00:09:39,200 --> 00:09:43,700
great use case for kubernetes to

00:09:41,810 --> 00:09:46,790
distribute that hyper parameter tuning

00:09:43,700 --> 00:09:49,220
across your cluster there's metadata so

00:09:46,790 --> 00:09:52,070
tracking information about each model

00:09:49,220 --> 00:09:55,880
run you know who built it

00:09:52,070 --> 00:09:59,060
when did it run what was its success

00:09:55,880 --> 00:10:00,620
we're not a custom metadata that you

00:09:59,060 --> 00:10:04,130
might want to add on it to categorize

00:10:00,620 --> 00:10:06,380
etc etc and finally serving which often

00:10:04,130 --> 00:10:09,050
is overlooked there's a lot of work to

00:10:06,380 --> 00:10:10,340
build the model and then if it's a

00:10:09,050 --> 00:10:13,220
successful model hopefully you're

00:10:10,340 --> 00:10:16,300
getting a lot of API hits against it so

00:10:13,220 --> 00:10:19,040
the KF serving or queue flow serving

00:10:16,300 --> 00:10:21,560
approach that we'll discuss a little

00:10:19,040 --> 00:10:25,160
more detail later gives you an API

00:10:21,560 --> 00:10:27,470
around your model and helps serve it in

00:10:25,160 --> 00:10:29,300
a very scalable way in your kubernetes

00:10:27,470 --> 00:10:33,020
cluster so in a nutshell that's what

00:10:29,300 --> 00:10:34,940
cube flow provides and let's look at

00:10:33,020 --> 00:10:38,710
some of the characteristics of cube flow

00:10:34,940 --> 00:10:41,270
we mentioned portability scalability

00:10:38,710 --> 00:10:43,310
composability so that's what we'll focus

00:10:41,270 --> 00:10:45,680
on the pipeline section you have one

00:10:43,310 --> 00:10:47,900
tool that can bring together multiple

00:10:45,680 --> 00:10:51,440
disparate parts of your machine learning

00:10:47,900 --> 00:10:53,600
lifecycle and finally it supports

00:10:51,440 --> 00:10:57,530
specialized hardware so it will allow

00:10:53,600 --> 00:11:01,220
you to unlock chipsets that allow you to

00:10:57,530 --> 00:11:04,790
run neural networks much quicker so

00:11:01,220 --> 00:11:08,930
that's always great from a design

00:11:04,790 --> 00:11:11,720
perspective what has guided cube flow

00:11:08,930 --> 00:11:14,780
number one making a kubernetes native so

00:11:11,720 --> 00:11:18,050
using the constructs that might be you

00:11:14,780 --> 00:11:21,410
know things like CR DS or using the a

00:11:18,050 --> 00:11:25,310
command line that there's a special COI

00:11:21,410 --> 00:11:28,370
called KF CTL just like cube C tail that

00:11:25,310 --> 00:11:33,020
allows you to run different commands and

00:11:28,370 --> 00:11:36,920
it's a framework agnostic so that's it's

00:11:33,020 --> 00:11:39,950
always a good thing here is a rough view

00:11:36,920 --> 00:11:43,179
of the components of cube flow so you

00:11:39,950 --> 00:11:46,959
have ingress you know coming into your

00:11:43,179 --> 00:11:51,279
kubernetes cluster you basically traffic

00:11:46,959 --> 00:11:53,289
gets routed to the appropriate API on

00:11:51,279 --> 00:11:56,379
the left side there is a central

00:11:53,289 --> 00:11:58,359
dashboard that provides a UI to launch a

00:11:56,379 --> 00:12:02,199
lot of the different tools there are

00:11:58,359 --> 00:12:04,509
operators which are basically custom API

00:12:02,199 --> 00:12:07,329
s to run different machine learning jobs

00:12:04,509 --> 00:12:12,639
and finally there is the serving

00:12:07,329 --> 00:12:16,179
capability to serve your model so how

00:12:12,639 --> 00:12:19,119
would you install queue flow you there

00:12:16,179 --> 00:12:20,529
not only is there a UI on certain cloud

00:12:19,119 --> 00:12:22,269
providers there's there's some

00:12:20,529 --> 00:12:26,249
additional capability but the the

00:12:22,269 --> 00:12:30,809
universal way to install it is using the

00:12:26,249 --> 00:12:34,179
KF kuddle command here and you just

00:12:30,809 --> 00:12:36,069
apply the configuration files and you

00:12:34,179 --> 00:12:40,079
can see there's this is just a handful

00:12:36,069 --> 00:12:43,329
of the many yamo files available in the

00:12:40,079 --> 00:12:48,009
github repo for cube flow that you know

00:12:43,329 --> 00:12:50,049
basically describes the cluster details

00:12:48,009 --> 00:12:53,229
and all the different services that are

00:12:50,049 --> 00:12:55,869
installed and running this will install

00:12:53,229 --> 00:12:58,329
the all the cube flow services that you

00:12:55,869 --> 00:13:03,039
need right there and more informations

00:12:58,329 --> 00:13:05,709
in the docs for installation so just

00:13:03,039 --> 00:13:08,529
another view on what it does we'll kind

00:13:05,709 --> 00:13:10,239
of again work clockwise here through the

00:13:08,529 --> 00:13:13,619
different things starting with

00:13:10,239 --> 00:13:16,629
development how do you train a model

00:13:13,619 --> 00:13:18,639
orchestration and serving so from a

00:13:16,629 --> 00:13:21,189
development perspective this is what it

00:13:18,639 --> 00:13:24,489
looks like in the at the top when you

00:13:21,189 --> 00:13:28,059
view your notebook servers so this gives

00:13:24,489 --> 00:13:31,229
you a set of container images that have

00:13:28,059 --> 00:13:34,599
a pre installed configuration of

00:13:31,229 --> 00:13:36,639
tensorflow or whatever other frameworks

00:13:34,599 --> 00:13:38,199
you want to use you can customize these

00:13:36,639 --> 00:13:42,789
images or use the ones out of the box

00:13:38,199 --> 00:13:44,499
and you specify the size of the notebook

00:13:42,789 --> 00:13:45,699
server you want to use you connect to it

00:13:44,499 --> 00:13:47,889
and then you see at the bottom it

00:13:45,699 --> 00:13:50,559
launches that instance and that allows

00:13:47,889 --> 00:13:52,720
you to create a notebook file which is

00:13:50,559 --> 00:13:55,059
essentially a scratch pad for your

00:13:52,720 --> 00:13:56,899
research that allows you to run

00:13:55,059 --> 00:14:00,290
different cells of Python code and

00:13:56,899 --> 00:14:03,050
your model so that's the development

00:14:00,290 --> 00:14:06,709
piece of cue flow let's look at how

00:14:03,050 --> 00:14:11,119
you'd run training this is just a simple

00:14:06,709 --> 00:14:12,559
example to get the concept across of you

00:14:11,119 --> 00:14:16,009
know how it works under the covers here

00:14:12,559 --> 00:14:18,290
so on the left side we have a llamo file

00:14:16,009 --> 00:14:20,569
which defines one of the jobs and

00:14:18,290 --> 00:14:25,100
there's a lot of these llamo files

00:14:20,569 --> 00:14:28,040
available in the cue flow repo so you

00:14:25,100 --> 00:14:29,949
see here for example with PI torch or if

00:14:28,040 --> 00:14:33,860
you want to run a PI torch job it might

00:14:29,949 --> 00:14:36,350
provide specifications around the

00:14:33,860 --> 00:14:38,480
resources that that job needs the

00:14:36,350 --> 00:14:40,160
container image which contains the

00:14:38,480 --> 00:14:43,009
training code for that image

00:14:40,160 --> 00:14:46,550
and that's what the C Amalur descriptor

00:14:43,009 --> 00:14:49,670
file provides to launch the training job

00:14:46,550 --> 00:14:52,999
you might then run a standard kubernetes

00:14:49,670 --> 00:14:55,399
command to launch the job and then also

00:14:52,999 --> 00:14:56,720
if say you want to monitor the job you

00:14:55,399 --> 00:14:58,879
know we wish these jobs of Brent

00:14:56,720 --> 00:15:01,069
immediately but you know they can often

00:14:58,879 --> 00:15:04,670
take some time and you can view the logs

00:15:01,069 --> 00:15:06,499
and you can see it doing all the passes

00:15:04,670 --> 00:15:07,970
through the data and seeing the status

00:15:06,499 --> 00:15:13,759
of it with your standard kubernetes

00:15:07,970 --> 00:15:16,519
commands so coober KF serving provides

00:15:13,759 --> 00:15:18,470
you an interface once you've created

00:15:16,519 --> 00:15:20,689
your model to host the model in the

00:15:18,470 --> 00:15:23,059
cluster and there are two different

00:15:20,689 --> 00:15:26,300
operations you see her to predict and to

00:15:23,059 --> 00:15:29,600
explain it well it will just mostly

00:15:26,300 --> 00:15:32,929
focus on predict as explained gives you

00:15:29,600 --> 00:15:34,759
extra information to tell you what were

00:15:32,929 --> 00:15:37,699
the important features of your model you

00:15:34,759 --> 00:15:40,069
know your model may be a function of 50

00:15:37,699 --> 00:15:42,559
different pieces of data some are more

00:15:40,069 --> 00:15:45,470
relevant than others explaining the

00:15:42,559 --> 00:15:48,079
model can provide some insight into what

00:15:45,470 --> 00:15:50,839
move the needle the most in your model

00:15:48,079 --> 00:15:54,589
so as far as prediction is concerned

00:15:50,839 --> 00:15:56,959
this is where you pass in some given

00:15:54,589 --> 00:15:59,720
variables and you want to get an answer

00:15:56,959 --> 00:16:01,459
back you know was this a cat or a dog or

00:15:59,720 --> 00:16:04,399
you know I always have to use that in in

00:16:01,459 --> 00:16:05,689
all presentations whatever it is that

00:16:04,399 --> 00:16:07,910
you're defining in your model that's

00:16:05,689 --> 00:16:10,910
what predict will provide and it allows

00:16:07,910 --> 00:16:13,150
for your standard deployments in this

00:16:10,910 --> 00:16:16,460
of a canary deployment where you might

00:16:13,150 --> 00:16:18,590
take a experimental model that you're

00:16:16,460 --> 00:16:20,600
not quite totally sure about it but you

00:16:18,590 --> 00:16:23,410
can direct a little bit of traffic to it

00:16:20,600 --> 00:16:25,880
see how it performs and then you know

00:16:23,410 --> 00:16:29,510
roll more and more of that traffic over

00:16:25,880 --> 00:16:32,350
to that new end point and so this

00:16:29,510 --> 00:16:34,790
component again has explained predict

00:16:32,350 --> 00:16:37,430
capabilities it also has a transformer

00:16:34,790 --> 00:16:40,340
because sometimes the raw inputs that

00:16:37,430 --> 00:16:42,950
you get I'll make this up you pass in a

00:16:40,340 --> 00:16:45,710
date object but your model expects a

00:16:42,950 --> 00:16:47,990
month and a day the transformer might

00:16:45,710 --> 00:16:51,410
break that up into the different

00:16:47,990 --> 00:16:53,420
components for instance there's a lot of

00:16:51,410 --> 00:16:55,490
cases for how you might need to tweak

00:16:53,420 --> 00:16:56,990
the data to put it into the format the

00:16:55,490 --> 00:16:58,790
model dates that's what the transformer

00:16:56,990 --> 00:17:02,180
could do all right so that's an overview

00:16:58,790 --> 00:17:04,460
of serving in pipelines is the area that

00:17:02,180 --> 00:17:07,010
I want to focus on the most today and

00:17:04,460 --> 00:17:09,050
pipelines give you that end-to-end ml

00:17:07,010 --> 00:17:12,950
workflow and this is what really gives

00:17:09,050 --> 00:17:15,530
you machine learning CI CD type of

00:17:12,950 --> 00:17:19,190
capability and we'll walk through how to

00:17:15,530 --> 00:17:21,500
actually build your own pipeline so you

00:17:19,190 --> 00:17:24,860
can define your pipeline in Python an

00:17:21,500 --> 00:17:28,250
idea here is that data scientists can

00:17:24,860 --> 00:17:33,310
build this pipeline and under the hood

00:17:28,250 --> 00:17:37,580
the the various docker containers and

00:17:33,310 --> 00:17:39,680
infrastructure is built for them the the

00:17:37,580 --> 00:17:41,690
data engineers the DevOps folks the ml

00:17:39,680 --> 00:17:43,250
engineers can definitely go in and work

00:17:41,690 --> 00:17:45,200
at that level and work with the

00:17:43,250 --> 00:17:48,820
containers and the Python files if they

00:17:45,200 --> 00:17:51,940
want but the data scientists can do this

00:17:48,820 --> 00:17:54,860
programmatically with a with with a

00:17:51,940 --> 00:17:56,390
simplified language so that they don't

00:17:54,860 --> 00:17:58,430
have to worry about that infrastructure

00:17:56,390 --> 00:18:00,590
so let's just walk through a simple

00:17:58,430 --> 00:18:02,930
example here that you can see it's from

00:18:00,590 --> 00:18:06,560
the cue flow examples repo I'm just

00:18:02,930 --> 00:18:08,030
saying a couple of code snippets so

00:18:06,560 --> 00:18:09,770
you're not seeing the full picture but

00:18:08,030 --> 00:18:11,660
let's walk through the concepts so the

00:18:09,770 --> 00:18:13,460
first part is defining the pipe one you

00:18:11,660 --> 00:18:16,220
see that there is a DSL or

00:18:13,460 --> 00:18:18,560
domain-specific language where you say

00:18:16,220 --> 00:18:21,290
hey here's my pipeline I'm going to

00:18:18,560 --> 00:18:23,570
start defining it and I might provide

00:18:21,290 --> 00:18:24,140
some metadata for it the name and

00:18:23,570 --> 00:18:27,140
description

00:18:24,140 --> 00:18:30,890
and some other things then you might

00:18:27,140 --> 00:18:33,740
define the method for that pipeline that

00:18:30,890 --> 00:18:35,690
has some default values you know how

00:18:33,740 --> 00:18:37,940
many training steps where's the data

00:18:35,690 --> 00:18:41,060
going to come from and these things can

00:18:37,940 --> 00:18:43,640
be overwritten but it's often nice to

00:18:41,060 --> 00:18:45,710
provide a standard pass you don't have

00:18:43,640 --> 00:18:47,960
to populate it with all the different

00:18:45,710 --> 00:18:49,910
values okay so that's the pipeline

00:18:47,960 --> 00:18:51,950
itself now what about the components

00:18:49,910 --> 00:18:54,380
within that pipeline so this is what

00:18:51,950 --> 00:18:57,050
that code would look like here first you

00:18:54,380 --> 00:19:00,380
might imports here you're seeing the cue

00:18:57,050 --> 00:19:03,350
flow pipelines SDK you're importing the

00:19:00,380 --> 00:19:06,170
components a package and then what you

00:19:03,350 --> 00:19:08,060
might do is import the component from a

00:19:06,170 --> 00:19:11,540
URL so here you see that there is a

00:19:08,060 --> 00:19:14,900
Hamel descriptor of that component here

00:19:11,540 --> 00:19:17,540
on the web that we're going to import

00:19:14,900 --> 00:19:20,030
that component from the next part might

00:19:17,540 --> 00:19:21,320
be instantiating that component so that

00:19:20,030 --> 00:19:23,960
component is going to have different

00:19:21,320 --> 00:19:26,630
arguments here in it in a simplified

00:19:23,960 --> 00:19:28,520
example we have two things it's a copied

00:19:26,630 --> 00:19:30,470
component it has a source directory in a

00:19:28,520 --> 00:19:34,310
target directory so you pass those in

00:19:30,470 --> 00:19:35,930
and you've got your component next let's

00:19:34,310 --> 00:19:37,850
talk about how do you wire these

00:19:35,930 --> 00:19:40,940
different components together so let's

00:19:37,850 --> 00:19:43,070
say that we have a log component and we

00:19:40,940 --> 00:19:46,100
define it however we define it

00:19:43,070 --> 00:19:49,460
you have commands such as before and

00:19:46,100 --> 00:19:52,520
after etc that allow you to specify the

00:19:49,460 --> 00:19:54,590
flow of what comes after the next in

00:19:52,520 --> 00:19:56,330
that pipeline and the neat thing here

00:19:54,590 --> 00:19:59,030
too is that you don't always have to

00:19:56,330 --> 00:20:02,600
explicitly do that say if log data

00:19:59,030 --> 00:20:05,750
required as an input copy data it would

00:20:02,600 --> 00:20:08,420
be smart enough to figure out and infer

00:20:05,750 --> 00:20:10,970
that what the right flow is that the log

00:20:08,420 --> 00:20:13,130
data always is after copy data but if

00:20:10,970 --> 00:20:14,750
that's if you're not including outputs

00:20:13,130 --> 00:20:16,250
and inputs within other components

00:20:14,750 --> 00:20:18,680
you'll need to manually do something

00:20:16,250 --> 00:20:21,410
like this so now you're building a

00:20:18,680 --> 00:20:25,460
pipeline okay so how do we deploy that

00:20:21,410 --> 00:20:27,410
pipeline so we would use the cue flow

00:20:25,460 --> 00:20:29,870
pipelines compiler package we would

00:20:27,410 --> 00:20:32,800
compile it it would end up as a tart

00:20:29,870 --> 00:20:36,080
gzipped file and then you see in the

00:20:32,800 --> 00:20:37,760
dashboard below you can upload that

00:20:36,080 --> 00:20:40,730
pipeline

00:20:37,760 --> 00:20:42,890
from a URL use it in the dashboard and

00:20:40,730 --> 00:20:44,480
this is the we're in the demo will

00:20:42,890 --> 00:20:47,590
actually walk through this in more

00:20:44,480 --> 00:20:52,220
detail and now you've got a pipeline

00:20:47,590 --> 00:20:54,350
alright so we talked about writing your

00:20:52,220 --> 00:20:57,220
own custom component and I know that

00:20:54,350 --> 00:20:59,420
looks a little challenging at first and

00:20:57,220 --> 00:21:01,760
often you don't even need to do that

00:20:59,420 --> 00:21:03,590
there are a whole set of library of

00:21:01,760 --> 00:21:06,050
components for common machine learning

00:21:03,590 --> 00:21:09,740
tasks that you can wire together without

00:21:06,050 --> 00:21:11,530
writing your own so if you look at some

00:21:09,740 --> 00:21:14,720
of these examples querying data

00:21:11,530 --> 00:21:17,570
transforming spark jobs so on and so

00:21:14,720 --> 00:21:20,120
forth they're all available in the

00:21:17,570 --> 00:21:23,570
github repo under pipelines components

00:21:20,120 --> 00:21:26,360
and one area I want to dive into a

00:21:23,570 --> 00:21:28,940
little bit more is tensorflow extended

00:21:26,360 --> 00:21:32,180
or tf-x where there's a library of

00:21:28,940 --> 00:21:35,690
various components that have been honed

00:21:32,180 --> 00:21:37,580
and tested by google and released as

00:21:35,690 --> 00:21:40,580
part of this tensorflow extended library

00:21:37,580 --> 00:21:41,840
that we can take advantage of here so

00:21:40,580 --> 00:21:44,030
let's look at what some of these

00:21:41,840 --> 00:21:45,680
components are because we're going to

00:21:44,030 --> 00:21:47,840
use them in the demo we're not gonna go

00:21:45,680 --> 00:21:51,110
into a lot of depth here but just show

00:21:47,840 --> 00:21:54,620
an example of a workflow using them so

00:21:51,110 --> 00:21:57,950
you see here if this workflow takes

00:21:54,620 --> 00:22:00,320
various examples or piece of raw data it

00:21:57,950 --> 00:22:02,990
generates statistics on them so you

00:22:00,320 --> 00:22:05,630
understand the date a bit better then

00:22:02,990 --> 00:22:07,700
we're basically I'll skip some other

00:22:05,630 --> 00:22:09,320
things we transform the data we train it

00:22:07,700 --> 00:22:11,240
we evaluate it and then we push it into

00:22:09,320 --> 00:22:14,270
production that's what these various

00:22:11,240 --> 00:22:16,250
standard components do and we'll show

00:22:14,270 --> 00:22:20,720
how that they can be run in queue flow

00:22:16,250 --> 00:22:23,120
pipelines so from a architecture

00:22:20,720 --> 00:22:26,380
perspective one thing I wanted to point

00:22:23,120 --> 00:22:29,270
out here is that if you look at the top

00:22:26,380 --> 00:22:34,160
what we've talked about so far is the

00:22:29,270 --> 00:22:36,290
cube flow pipelines SDK as the way to

00:22:34,160 --> 00:22:38,570
define a pipeline and you see sort of in

00:22:36,290 --> 00:22:40,160
the middle list custom pipelines with

00:22:38,570 --> 00:22:44,740
custom and pre-built components we

00:22:40,160 --> 00:22:49,250
talked about now as we're bringing the

00:22:44,740 --> 00:22:51,680
cube flow pipelines and tf-x projects in

00:22:49,250 --> 00:22:56,450
closer harmony together

00:22:51,680 --> 00:23:00,950
you're seeing a tf-x SDK which allows

00:22:56,450 --> 00:23:03,760
you to define it in the that other SDK

00:23:00,950 --> 00:23:06,800
that focuses more on tensor flow

00:23:03,760 --> 00:23:08,890
specific terminology and things like

00:23:06,800 --> 00:23:11,330
that so they all work together nicely

00:23:08,890 --> 00:23:14,390
for our demo we're going to look more at

00:23:11,330 --> 00:23:20,180
the kfp SDK but just to show that these

00:23:14,390 --> 00:23:23,600
all work together well alright so let's

00:23:20,180 --> 00:23:26,900
look at a custom component and you know

00:23:23,600 --> 00:23:29,090
how you'd actually build that there's a

00:23:26,900 --> 00:23:30,950
lot here but I want to just get across

00:23:29,090 --> 00:23:32,900
some of the main concepts so say you are

00:23:30,950 --> 00:23:35,240
gonna build your own component you would

00:23:32,900 --> 00:23:38,360
start with a descriptor this is the ML

00:23:35,240 --> 00:23:39,680
file and what are the key things you see

00:23:38,360 --> 00:23:43,730
here you might see a name a description

00:23:39,680 --> 00:23:46,970
of your component maybe some labels that

00:23:43,730 --> 00:23:49,310
are used around that component and some

00:23:46,970 --> 00:23:51,350
arguments you see her on the inputs on

00:23:49,310 --> 00:23:52,940
the left side what are the types and

00:23:51,350 --> 00:23:56,090
what are the names of those arguments

00:23:52,940 --> 00:23:57,890
for your component on the right side the

00:23:56,090 --> 00:24:00,290
key thing here is the implementation

00:23:57,890 --> 00:24:02,060
section but regardless or with the

00:24:00,290 --> 00:24:04,190
outputs right what the components of

00:24:02,060 --> 00:24:07,690
course in the implementation section you

00:24:04,190 --> 00:24:11,960
see a link to the image now this is the

00:24:07,690 --> 00:24:13,580
docker a container image here on it

00:24:11,960 --> 00:24:16,730
happens to be in the Google container

00:24:13,580 --> 00:24:20,090
registry and the various arguments for

00:24:16,730 --> 00:24:22,190
that so this really is what if you want

00:24:20,090 --> 00:24:25,250
to import a component you point to this

00:24:22,190 --> 00:24:27,260
file and that's the first step in this

00:24:25,250 --> 00:24:29,360
process I'm going to show you three

00:24:27,260 --> 00:24:32,750
different pieces of code to build that

00:24:29,360 --> 00:24:35,150
component so the next is the docker file

00:24:32,750 --> 00:24:37,880
so you start with the base image and

00:24:35,150 --> 00:24:40,490
then we can add some additional packages

00:24:37,880 --> 00:24:42,950
to it and the key thing you see at the

00:24:40,490 --> 00:24:46,310
bottom here is the entry point into that

00:24:42,950 --> 00:24:51,470
image this is a code that's going to do

00:24:46,310 --> 00:24:54,140
the work within that docker file so what

00:24:51,470 --> 00:24:56,510
might the code look like within the

00:24:54,140 --> 00:24:59,660
docker file or rather within the

00:24:56,510 --> 00:25:03,260
container you might see on the left side

00:24:59,660 --> 00:25:05,480
here like your definition of the you

00:25:03,260 --> 00:25:07,549
know the main method here you see

00:25:05,480 --> 00:25:09,860
parsing the different arguments coming

00:25:07,549 --> 00:25:12,080
in from the component you know what

00:25:09,860 --> 00:25:15,799
director is the model in where's the

00:25:12,080 --> 00:25:18,860
data etc and then on the right side you

00:25:15,799 --> 00:25:21,470
can see the action happening where we're

00:25:18,860 --> 00:25:24,559
taking some of that information and

00:25:21,470 --> 00:25:27,679
we're actually doing the things running

00:25:24,559 --> 00:25:30,410
training or logging etc and so this is

00:25:27,679 --> 00:25:33,530
how it kind of all gets brought together

00:25:30,410 --> 00:25:38,030
a component definition the docker file

00:25:33,530 --> 00:25:41,179
and within a docker image the model

00:25:38,030 --> 00:25:42,919
itself okay so now that you build a

00:25:41,179 --> 00:25:45,679
pipeline let's what would it look like

00:25:42,919 --> 00:25:48,169
in the user interface so you're actually

00:25:45,679 --> 00:25:50,660
able to see experiment run results I

00:25:48,169 --> 00:25:53,809
look at this as like you're the

00:25:50,660 --> 00:25:55,730
equivalent of your build system for

00:25:53,809 --> 00:26:00,620
traditional software development where

00:25:55,730 --> 00:26:04,160
you see the the run what time it started

00:26:00,620 --> 00:26:06,380
did it pass or fail links to logs and

00:26:04,160 --> 00:26:08,630
then accuracy right so maybe instead of

00:26:06,380 --> 00:26:10,429
thinking about oh how many unit tests

00:26:08,630 --> 00:26:12,890
passed or something like that how

00:26:10,429 --> 00:26:15,980
successful was this how accurate was

00:26:12,890 --> 00:26:17,660
this model and and what we'll talk about

00:26:15,980 --> 00:26:21,169
how you could even do things like maybe

00:26:17,660 --> 00:26:24,260
have tests that prevent your model from

00:26:21,169 --> 00:26:26,240
being pushed to production unless only

00:26:24,260 --> 00:26:30,380
only if it hits above a certain accuracy

00:26:26,240 --> 00:26:32,540
level it allows you to track artifacts

00:26:30,380 --> 00:26:36,200
and I know that this a little bit blurry

00:26:32,540 --> 00:26:39,230
but this screenshot just shows that as

00:26:36,200 --> 00:26:42,440
you're doing things with your pipelines

00:26:39,230 --> 00:26:46,100
you can then track and store artifacts

00:26:42,440 --> 00:26:47,929
so that your team can share them so in

00:26:46,100 --> 00:26:50,720
summary we've talked about cube flow

00:26:47,929 --> 00:26:53,179
which is a clouded cloud native multi

00:26:50,720 --> 00:26:55,940
cloud solution for ml it provides a

00:26:53,179 --> 00:26:57,440
platform for composable pipelines and if

00:26:55,940 --> 00:27:01,850
you have kubernetes you can run cube

00:26:57,440 --> 00:27:04,940
flow and just to talk a little bit about

00:27:01,850 --> 00:27:08,410
our communities an open community love

00:27:04,940 --> 00:27:12,220
to get more folks participating involved

00:27:08,410 --> 00:27:15,470
and here are a few ways that you can

00:27:12,220 --> 00:27:19,380
connect with the cube flow community so

00:27:15,470 --> 00:27:23,460
with that now I want to switch to a

00:27:19,380 --> 00:27:27,510
demo and so what I'm going to show now

00:27:23,460 --> 00:27:29,100
is the the pipelines effort so this

00:27:27,510 --> 00:27:31,350
happens beyond Google cloud platform

00:27:29,100 --> 00:27:34,530
again this can run anywhere you have a

00:27:31,350 --> 00:27:37,230
kubernetes cluster what I'm showing you

00:27:34,530 --> 00:27:39,870
here is the standalone the sort of

00:27:37,230 --> 00:27:43,340
lightweight version with cube flow

00:27:39,870 --> 00:27:46,410
pipelines alone right so there's a pre

00:27:43,340 --> 00:27:48,240
we've got a installation already created

00:27:46,410 --> 00:27:50,580
here so we don't have to go through all

00:27:48,240 --> 00:27:52,470
of the creation of the cluster but it

00:27:50,580 --> 00:27:55,290
does have a nice way that if you want to

00:27:52,470 --> 00:27:59,090
use the user interface to create a

00:27:55,290 --> 00:28:03,900
pipeline you can simply configure it and

00:27:59,090 --> 00:28:05,850
specify the cluster or create you know

00:28:03,900 --> 00:28:07,830
and it will create it for you where you

00:28:05,850 --> 00:28:09,750
can select an existing cluster and so

00:28:07,830 --> 00:28:13,800
and so forth but I've already done that

00:28:09,750 --> 00:28:16,290
piece okay so let's open up the

00:28:13,800 --> 00:28:18,660
pipeline's dashboard and take a look and

00:28:16,290 --> 00:28:21,980
what we see here so let's look at the

00:28:18,660 --> 00:28:24,600
pipeline's that we have I am going to

00:28:21,980 --> 00:28:26,430
look at one of the out-of-the-box of

00:28:24,600 --> 00:28:29,730
demos that's included with cube flow

00:28:26,430 --> 00:28:33,210
pipelines and this shows you a very

00:28:29,730 --> 00:28:35,280
robust end and model with all the

00:28:33,210 --> 00:28:38,490
different components wired together so

00:28:35,280 --> 00:28:41,030
this is a taxi tip prediction model what

00:28:38,490 --> 00:28:44,940
this is doing is it's looking at some

00:28:41,030 --> 00:28:46,680
data from the city of Chicago they've

00:28:44,940 --> 00:28:49,590
you know a lot of government data

00:28:46,680 --> 00:28:52,350
publicly available and it looks at all

00:28:49,590 --> 00:28:54,870
these different taxi trips and what

00:28:52,350 --> 00:28:58,410
we're trying to do is predict just it's

00:28:54,870 --> 00:28:59,820
a binary classification model is do we

00:28:58,410 --> 00:29:02,460
predict that the tip is going to be

00:28:59,820 --> 00:29:03,960
greater than twenty percent or not okay

00:29:02,460 --> 00:29:06,360
so that's all that this model is going

00:29:03,960 --> 00:29:07,830
to do and when you install queue flow

00:29:06,360 --> 00:29:09,660
you know pipelines you're going to see

00:29:07,830 --> 00:29:11,490
this you can click there to see the

00:29:09,660 --> 00:29:14,600
source code so if you're interested in

00:29:11,490 --> 00:29:18,300
offline looking more at this model

00:29:14,600 --> 00:29:21,000
here's where you can you know do that

00:29:18,300 --> 00:29:24,000
you see you see the pipe the notebook

00:29:21,000 --> 00:29:25,470
and and all that right there so let's

00:29:24,000 --> 00:29:29,910
keep moving

00:29:25,470 --> 00:29:31,710
so let's look at this pipeline so this

00:29:29,910 --> 00:29:32,770
is the pipeline think of it sort of this

00:29:31,710 --> 00:29:36,040
is the class

00:29:32,770 --> 00:29:38,080
not the instance this is the you know

00:29:36,040 --> 00:29:41,050
the the structure of the pipeline but

00:29:38,080 --> 00:29:43,980
it's not an actual run if I want to run

00:29:41,050 --> 00:29:46,720
the pipeline I click create run and

00:29:43,980 --> 00:29:51,040
here's where I might specify different

00:29:46,720 --> 00:29:55,330
parameters to say run it on its data set

00:29:51,040 --> 00:29:56,980
X or Y or as we said before maybe how

00:29:55,330 --> 00:29:59,680
much how many passes through the data

00:29:56,980 --> 00:30:02,290
different ml parameters you might want

00:29:59,680 --> 00:30:05,170
to pass your burn-up so that's how you

00:30:02,290 --> 00:30:08,110
start to run doesn't hurt I could

00:30:05,170 --> 00:30:09,550
actually kick one off now so but let's

00:30:08,110 --> 00:30:12,850
look at one that I've already done here

00:30:09,550 --> 00:30:16,060
so I'll click on this run and now you

00:30:12,850 --> 00:30:17,590
see the green check boxes so yeah every

00:30:16,060 --> 00:30:20,800
step worked successfully that's always

00:30:17,590 --> 00:30:22,870
good for a demo and I know there's a lot

00:30:20,800 --> 00:30:24,670
here but I'll try to just kind of

00:30:22,870 --> 00:30:27,760
briefly talk through the key points that

00:30:24,670 --> 00:30:30,070
I think shows a real world production ml

00:30:27,760 --> 00:30:32,470
workflow and why this is pretty cool so

00:30:30,070 --> 00:30:34,660
first one is looking at

00:30:32,470 --> 00:30:36,910
it's called example gen and all that's

00:30:34,660 --> 00:30:38,770
doing is looking at your data and doing

00:30:36,910 --> 00:30:41,500
the training test flight it's parsing

00:30:38,770 --> 00:30:45,670
the CSV it's holding out some data to

00:30:41,500 --> 00:30:47,530
test on here you see on every every step

00:30:45,670 --> 00:30:50,830
you know within that container looking

00:30:47,530 --> 00:30:53,050
at the logs within it so if you needed

00:30:50,830 --> 00:30:55,900
for some reason to troubleshoot

00:30:53,050 --> 00:30:57,640
something it's all there the next thing

00:30:55,900 --> 00:31:00,970
we're going to look at is statistics gen

00:30:57,640 --> 00:31:04,300
so now that we've pulled in this data

00:31:00,970 --> 00:31:06,460
let's look at what's in the data set and

00:31:04,300 --> 00:31:09,430
that's what statistics gen can do for

00:31:06,460 --> 00:31:11,950
you again these are the tf-x components

00:31:09,430 --> 00:31:14,920
that you can pull into your workflow and

00:31:11,950 --> 00:31:17,110
you don't have to write this these

00:31:14,920 --> 00:31:19,360
components yourself so let's look at

00:31:17,110 --> 00:31:22,060
statistics gen I'm gonna just expand

00:31:19,360 --> 00:31:24,970
this window a little bit so here you see

00:31:22,060 --> 00:31:27,880
it provides you a nice view of what's in

00:31:24,970 --> 00:31:30,550
your data so remember this is a taxi tip

00:31:27,880 --> 00:31:32,410
prediction model and you can see things

00:31:30,550 --> 00:31:35,950
like for each you know that we've got

00:31:32,410 --> 00:31:39,820
about 5,000 ish I guess I think it's

00:31:35,950 --> 00:31:42,970
5,000 91 rows in our data set we always

00:31:39,820 --> 00:31:46,210
have a fare which that's key the mean

00:31:42,970 --> 00:31:46,720
fare is about twelve dollars tells you

00:31:46,210 --> 00:31:50,230
the

00:31:46,720 --> 00:31:52,030
standard deviation median max all that

00:31:50,230 --> 00:31:54,039
kind of good stuff and then here are

00:31:52,030 --> 00:31:56,980
some of the other data this is what we

00:31:54,039 --> 00:31:59,740
used to predict you know the very allow

00:31:56,980 --> 00:32:02,520
tatooed longitude you know so on and so

00:31:59,740 --> 00:32:05,919
forth and then we have some tip data

00:32:02,520 --> 00:32:07,840
start time you know the company the

00:32:05,919 --> 00:32:10,840
payment type all kinds of stuff like

00:32:07,840 --> 00:32:12,669
that okay so so this is a great way to

00:32:10,840 --> 00:32:13,500
understand your data after it's been

00:32:12,669 --> 00:32:17,169
imported

00:32:13,500 --> 00:32:20,440
okay the next thing it does is called

00:32:17,169 --> 00:32:24,450
schema gem so this is where it can look

00:32:20,440 --> 00:32:26,740
at your data and try to infer the types

00:32:24,450 --> 00:32:29,620
automatically so it's looking at this

00:32:26,740 --> 00:32:33,580
different data and saying okay there's a

00:32:29,620 --> 00:32:34,900
company which is a string and you know

00:32:33,580 --> 00:32:37,120
all these different things you know as a

00:32:34,900 --> 00:32:40,450
required or optional based on how often

00:32:37,120 --> 00:32:43,179
it showed up in the data set then you

00:32:40,450 --> 00:32:44,980
might see for what's called categorical

00:32:43,179 --> 00:32:47,860
values where you have think of it as a

00:32:44,980 --> 00:32:50,080
drop-down where it's a within a set of

00:32:47,860 --> 00:32:51,789
values what are those values that it's

00:32:50,080 --> 00:32:53,919
seeing in the data set so it's seeing

00:32:51,789 --> 00:32:56,320
all these different companies and then

00:32:53,919 --> 00:32:59,289
payment types it sees about six

00:32:56,320 --> 00:33:03,070
different payment types make sense cash

00:32:59,289 --> 00:33:05,440
credit card etc okay the next step is

00:33:03,070 --> 00:33:07,990
going to do is an example validator so

00:33:05,440 --> 00:33:09,970
it's gonna pass the test data through

00:33:07,990 --> 00:33:13,409
that scheme and just make sure did that

00:33:09,970 --> 00:33:16,390
line up with the schema that we have and

00:33:13,409 --> 00:33:18,070
generally it did but here it's pointing

00:33:16,390 --> 00:33:20,919
out some of the anomalies and some of

00:33:18,070 --> 00:33:22,390
these make sense like the companies we

00:33:20,919 --> 00:33:24,460
didn't capture all of those in our

00:33:22,390 --> 00:33:26,650
training data so that looks like they're

00:33:24,460 --> 00:33:28,390
in the the holdout records there were a

00:33:26,650 --> 00:33:30,549
few other company so probably we don't

00:33:28,390 --> 00:33:32,320
want to harden this endorse schema and

00:33:30,549 --> 00:33:35,230
say that these are the only available

00:33:32,320 --> 00:33:35,710
companies because that seems to change

00:33:35,230 --> 00:33:37,750
quite a bit

00:33:35,710 --> 00:33:40,809
it looks like for payment type we may

00:33:37,750 --> 00:33:42,940
have missed something called PR card so

00:33:40,809 --> 00:33:46,030
anyway it's gonna flag for you if

00:33:42,940 --> 00:33:47,470
there's any anomalies in n differences

00:33:46,030 --> 00:33:50,320
between the schema that you've generated

00:33:47,470 --> 00:33:53,280
and an additional data flowing through

00:33:50,320 --> 00:33:55,960
the system all right so so now we've

00:33:53,280 --> 00:33:57,929
looked at the data we understand it

00:33:55,960 --> 00:34:00,280
let's we might go through a transfer

00:33:57,929 --> 00:34:03,070
transform step the

00:34:00,280 --> 00:34:05,380
see here I'll kind of keep moving on and

00:34:03,070 --> 00:34:07,270
then now let's look at sort of this is

00:34:05,380 --> 00:34:09,669
where things really come into production

00:34:07,270 --> 00:34:13,030
so there's a trainer step where you can

00:34:09,669 --> 00:34:16,390
train your model and you're gonna see

00:34:13,030 --> 00:34:19,810
things like tensor board which shows you

00:34:16,390 --> 00:34:22,780
a visualization of the loss of the model

00:34:19,810 --> 00:34:25,780
you want to see the error you know being

00:34:22,780 --> 00:34:27,669
more and more minimized over time and

00:34:25,780 --> 00:34:29,290
you know you're gonna see logs during

00:34:27,669 --> 00:34:31,480
the training this is where the actual a

00:34:29,290 --> 00:34:34,690
lot of the tensorflow code is being run

00:34:31,480 --> 00:34:38,169
where it's you know you see the loss for

00:34:34,690 --> 00:34:41,650
the model your stuff like that is all in

00:34:38,169 --> 00:34:44,380
the training step alright and now what

00:34:41,650 --> 00:34:47,290
we might do is here's the evaluator step

00:34:44,380 --> 00:34:50,440
and I think this is a neat one because

00:34:47,290 --> 00:34:52,960
what this can do it's using something

00:34:50,440 --> 00:34:56,080
called tensor flow model analysis and

00:34:52,960 --> 00:34:58,150
what that's doing for you is it's

00:34:56,080 --> 00:35:00,250
allowing you to look at the accuracy by

00:34:58,150 --> 00:35:04,510
different slices right so it's one thing

00:35:00,250 --> 00:35:07,090
to say hey my model is 80% accurate

00:35:04,510 --> 00:35:09,400
right but you might want to you know

00:35:07,090 --> 00:35:11,680
peel back that onion a little bit and

00:35:09,400 --> 00:35:13,480
understand that accuracy through

00:35:11,680 --> 00:35:17,290
different slices and that's what this

00:35:13,480 --> 00:35:20,590
can do is it can say well let's see how

00:35:17,290 --> 00:35:24,970
accurate the model is you know when the

00:35:20,590 --> 00:35:27,160
trips start at 1 a.m. versus say 9 a.m.

00:35:24,970 --> 00:35:29,830
or whatever it so you see all these

00:35:27,160 --> 00:35:32,230
different slices here and we could we

00:35:29,830 --> 00:35:34,240
could pick any number of slices but this

00:35:32,230 --> 00:35:36,820
is a good way to see like is the

00:35:34,240 --> 00:35:39,730
experience for all of your users are we

00:35:36,820 --> 00:35:41,650
getting a similar experience and if not

00:35:39,730 --> 00:35:44,770
how can we improve that right so you can

00:35:41,650 --> 00:35:46,810
see you know accuracy broken down by

00:35:44,770 --> 00:35:52,290
these different slices which is very

00:35:46,810 --> 00:35:54,730
cool to see so now let's talk about the

00:35:52,290 --> 00:35:57,010
continuous deployment aspect so what we

00:35:54,730 --> 00:35:59,140
can do here in the model validator is

00:35:57,010 --> 00:36:02,950
where you can define rules where you can

00:35:59,140 --> 00:36:05,860
say if this model is better than my

00:36:02,950 --> 00:36:08,710
previous one we can create what's called

00:36:05,860 --> 00:36:12,520
a blest model so you can see that here

00:36:08,710 --> 00:36:13,770
and if if the model is blessed then what

00:36:12,520 --> 00:36:16,230
happens is

00:36:13,770 --> 00:36:19,590
in the final stage of the pusher will

00:36:16,230 --> 00:36:22,500
then push that to production push

00:36:19,590 --> 00:36:26,250
whatever the blessed model is there and

00:36:22,500 --> 00:36:28,320
then again you'll see the logs and you

00:36:26,250 --> 00:36:31,260
see you know where the model got pushed

00:36:28,320 --> 00:36:33,780
to and you're good to go and you know

00:36:31,260 --> 00:36:36,840
there are extensions for say different

00:36:33,780 --> 00:36:39,180
cloud platforms or you could host it in

00:36:36,840 --> 00:36:42,540
in the cloud or there's all kinds of

00:36:39,180 --> 00:36:44,820
different configurations the as far as

00:36:42,540 --> 00:36:47,790
the pusher so this is an end-to-end

00:36:44,820 --> 00:36:49,830
example of a real world ml workflow as

00:36:47,790 --> 00:36:51,930
you can see there's a lot involved in

00:36:49,830 --> 00:36:55,770
and it's great to see this all being

00:36:51,930 --> 00:36:58,350
tracked so let's so now we've seen that

00:36:55,770 --> 00:37:01,290
run of an experiment let's look now at

00:36:58,350 --> 00:37:04,320
the artifacts so we talked about every

00:37:01,290 --> 00:37:09,590
run contract different things so if we

00:37:04,320 --> 00:37:12,390
go to the the you know different

00:37:09,590 --> 00:37:14,820
artifacts generated by this pipeline you

00:37:12,390 --> 00:37:16,260
can see you can link directly to some of

00:37:14,820 --> 00:37:19,050
those statistics and things like that

00:37:16,260 --> 00:37:21,450
let's dive into the model now so if I

00:37:19,050 --> 00:37:23,250
click on the model so I can see some of

00:37:21,450 --> 00:37:26,100
those custom properties we talked about

00:37:23,250 --> 00:37:28,020
you can add more if you'd like but I

00:37:26,100 --> 00:37:31,170
what I think that's pretty cool is this

00:37:28,020 --> 00:37:35,040
lineage Explorer where this is where you

00:37:31,170 --> 00:37:39,180
can see for your model what were the

00:37:35,040 --> 00:37:43,230
different other steps and how did they

00:37:39,180 --> 00:37:44,910
how were they combined together to get

00:37:43,230 --> 00:37:47,520
to the next step of the model and you

00:37:44,910 --> 00:37:49,760
can click on any of these individual

00:37:47,520 --> 00:37:52,190
artifacts and kind of see see that

00:37:49,760 --> 00:37:57,390
relationship between them here in the

00:37:52,190 --> 00:38:01,320
lineage Explorer and then finally you

00:37:57,390 --> 00:38:03,330
know with executions you can see you

00:38:01,320 --> 00:38:05,340
know just more detail on on each of

00:38:03,330 --> 00:38:08,910
these different models and their

00:38:05,340 --> 00:38:12,570
artifacts so that's cute flow pipelines

00:38:08,910 --> 00:38:15,869
in a nutshell and I'm going to go back

00:38:12,570 --> 00:38:20,340
to the presentation and that's pretty

00:38:15,869 --> 00:38:22,710
much it i i'll stop sharing now and i'll

00:38:20,340 --> 00:38:28,020
see if there's any questions in the chat

00:38:22,710 --> 00:38:31,330
window and before you wrap up

00:38:28,020 --> 00:38:34,510
and I like that yeah so I am not sure

00:38:31,330 --> 00:38:36,780
how to pronounce cube cuddle or cube CTL

00:38:34,510 --> 00:38:40,090
maybe we need a poll about that

00:38:36,780 --> 00:38:41,170
so all right I'll just wait around for

00:38:40,090 --> 00:38:42,820
another minute see if there's any

00:38:41,170 --> 00:38:44,530
questions I appreciate everybody

00:38:42,820 --> 00:38:47,380
attending hopefully you got out a lot

00:38:44,530 --> 00:38:49,720
out of this running you know machine

00:38:47,380 --> 00:38:51,190
learning in production is difficult it's

00:38:49,720 --> 00:38:57,580
great we have tools like cube flow to

00:38:51,190 --> 00:39:00,810
help so I'll just see if there's any

00:38:57,580 --> 00:39:00,810
other questions

00:39:06,930 --> 00:39:12,869
right okay thank you everybody

00:39:14,160 --> 00:39:22,400
and all right I think we're ready to

00:39:20,009 --> 00:39:22,400

YouTube URL: https://www.youtube.com/watch?v=nlm0CEp6Abc


