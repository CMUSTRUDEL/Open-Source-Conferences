Title: AI – Addressing the Challenges of Today’s Data Scientists -Jeff Reser & Alessandro Festa
Publication date: 2020-10-26
Playlist: All Things Open 2020
Description: 
	Presented by: Jeff Reser & Alessandro Festa, SUSE
Presented at All Things Open 2020 - Machine Learning/AI Track

Abstract: Recent events have taught us that processing and interpreting volumes of data is important for business continuity – whether on premises, in the cloud or at the edge.  Enterprises are turning to AI, machine learning and analytics to make the right inferences from that data.  However, they are challenged to get their AI project into production while satisfying all the requirements for being deployed across multiple environments with security and manageability. SUSE has a better way. Learn how to reduce the complexity of the AI infrastructure through a holistic approach spanning services, infrastructure and support. Join Jeff Reser (SUSE Solutions) and Alessandro Festa (SUSE Product Manager, AI) as they discuss the opportunities for AI and demonstrate the tools needed.
Captions: 
	00:00:05,120 --> 00:00:09,840
so uh welcome everyone

00:00:06,879 --> 00:00:10,480
and thanks a lot for joining in so i'm

00:00:09,840 --> 00:00:12,080
jeff

00:00:10,480 --> 00:00:14,000
i'm located in north carolina if you

00:00:12,080 --> 00:00:14,880
didn't hear some of the conversation we

00:00:14,000 --> 00:00:16,400
just had

00:00:14,880 --> 00:00:19,119
and i'm joined by my colleague

00:00:16,400 --> 00:00:22,480
alessandro from torino italy

00:00:19,119 --> 00:00:24,880
and that's right and it's it's pretty

00:00:22,480 --> 00:00:26,640
late for the in the day for him so

00:00:24,880 --> 00:00:30,400
uh he should have a glass of wine with

00:00:26,640 --> 00:00:33,840
him i don't know why he doesn't

00:00:30,400 --> 00:00:33,840
i was not sharing the screen

00:00:34,880 --> 00:00:39,280
so i'm confident that what we have for

00:00:37,200 --> 00:00:43,120
you today is a great discussion

00:00:39,280 --> 00:00:45,120
on a really exciting topic in recent

00:00:43,120 --> 00:00:46,559
events around the world as you know have

00:00:45,120 --> 00:00:48,559
taught us that processing and

00:00:46,559 --> 00:00:50,320
interpreting volumes of data

00:00:48,559 --> 00:00:51,680
is really important for both business

00:00:50,320 --> 00:00:55,280
continuity

00:00:51,680 --> 00:00:57,680
and enhancing end user experiences

00:00:55,280 --> 00:00:59,120
enterprises are turning to ai to machine

00:00:57,680 --> 00:01:01,039
learning to analytics

00:00:59,120 --> 00:01:02,320
to make the right inferences from all of

00:01:01,039 --> 00:01:04,239
that data

00:01:02,320 --> 00:01:06,000
so here i'm going to talk about how

00:01:04,239 --> 00:01:08,720
enterprises are challenged

00:01:06,000 --> 00:01:09,920
to get that important ai project into

00:01:08,720 --> 00:01:11,920
production

00:01:09,920 --> 00:01:13,760
while satisfying all the requirements

00:01:11,920 --> 00:01:14,880
for being deployed across multiple

00:01:13,760 --> 00:01:18,000
environments

00:01:14,880 --> 00:01:20,240
with security and with manageability so

00:01:18,000 --> 00:01:23,200
we'll go on to the next slide

00:01:20,240 --> 00:01:24,000
yep there you go and we're going to

00:01:23,200 --> 00:01:26,000
start with the

00:01:24,000 --> 00:01:27,200
challenges and a little bit about

00:01:26,000 --> 00:01:30,479
strategy

00:01:27,200 --> 00:01:33,439
and go on to the next one

00:01:30,479 --> 00:01:34,880
so organizations around the world apply

00:01:33,439 --> 00:01:37,360
ai technologies

00:01:34,880 --> 00:01:38,320
to transform business in ways that will

00:01:37,360 --> 00:01:42,000
reinvent how

00:01:38,320 --> 00:01:44,399
firms win serve and retain customers

00:01:42,000 --> 00:01:45,520
businesses are under pressure to evolve

00:01:44,399 --> 00:01:48,479
quickly

00:01:45,520 --> 00:01:49,360
and optimize processes maximize revenue

00:01:48,479 --> 00:01:51,840
streams

00:01:49,360 --> 00:01:54,399
track trends and develop new business

00:01:51,840 --> 00:01:57,439
models using digital technologies

00:01:54,399 --> 00:01:59,600
ai has emerged as a core capability

00:01:57,439 --> 00:02:02,240
which every organization must leverage

00:01:59,600 --> 00:02:03,680
in order to be an industry player it

00:02:02,240 --> 00:02:06,079
takes leadership

00:02:03,680 --> 00:02:07,360
a knowledgeable workforce and a mix of

00:02:06,079 --> 00:02:10,000
the right tools

00:02:07,360 --> 00:02:12,480
to see success but an ai infrastructure

00:02:10,000 --> 00:02:14,640
is also required to expand

00:02:12,480 --> 00:02:16,480
and infuse that intelligence into

00:02:14,640 --> 00:02:19,120
practical use cases

00:02:16,480 --> 00:02:20,800
so i wanted to start out with a baseline

00:02:19,120 --> 00:02:23,200
set of assertions

00:02:20,800 --> 00:02:23,920
and the first one is that the upsurge of

00:02:23,200 --> 00:02:26,160
data

00:02:23,920 --> 00:02:27,440
does not automatically translate into

00:02:26,160 --> 00:02:29,440
business improvements

00:02:27,440 --> 00:02:32,640
without leveraging ai or machine

00:02:29,440 --> 00:02:35,280
learning to turn that data into insights

00:02:32,640 --> 00:02:37,200
that may be obvious but getting there

00:02:35,280 --> 00:02:39,360
can be a rocky road

00:02:37,200 --> 00:02:41,120
and the second is that businesses will

00:02:39,360 --> 00:02:42,400
increasingly leverage machine learning

00:02:41,120 --> 00:02:44,800
as more data

00:02:42,400 --> 00:02:48,080
is collected through iot sensors through

00:02:44,800 --> 00:02:49,840
edge devices through cloud sources

00:02:48,080 --> 00:02:52,000
machine learning is how we discover

00:02:49,840 --> 00:02:53,760
patterns and massive amounts of data

00:02:52,000 --> 00:02:56,080
and make inferences based on those

00:02:53,760 --> 00:02:58,480
patterns the third one

00:02:56,080 --> 00:02:59,200
introduces what i call practical ai

00:02:58,480 --> 00:03:01,040
where

00:02:59,200 --> 00:03:03,200
ai machine learning is used to gain

00:03:01,040 --> 00:03:04,000
insights and in first something that

00:03:03,200 --> 00:03:06,959
becomes more

00:03:04,000 --> 00:03:08,959
useful in everyday life where the

00:03:06,959 --> 00:03:11,280
unexpected is inferred

00:03:08,959 --> 00:03:14,000
it results in positive outcomes and i

00:03:11,280 --> 00:03:15,840
refer to this as practical ai

00:03:14,000 --> 00:03:18,400
and that leads into the fourth assertion

00:03:15,840 --> 00:03:20,400
i have here that practical ai

00:03:18,400 --> 00:03:21,599
strengthens the trust people start to

00:03:20,400 --> 00:03:24,640
have in it

00:03:21,599 --> 00:03:26,239
and this shines a light on dark data and

00:03:24,640 --> 00:03:28,319
leads to decisions

00:03:26,239 --> 00:03:29,280
that can positively affect not only our

00:03:28,319 --> 00:03:32,799
businesses

00:03:29,280 --> 00:03:34,319
but our everyday lives we go on to the

00:03:32,799 --> 00:03:37,360
next slide

00:03:34,319 --> 00:03:38,560
so ai has emerged as rapidly becoming

00:03:37,360 --> 00:03:40,319
instrumental

00:03:38,560 --> 00:03:43,519
in driving the insights that empower

00:03:40,319 --> 00:03:46,000
businesses in today's connected world

00:03:43,519 --> 00:03:48,640
organizations today need to find complex

00:03:46,000 --> 00:03:51,120
relationships in large amounts of data

00:03:48,640 --> 00:03:53,120
and predict things that either cut costs

00:03:51,120 --> 00:03:55,680
or create value

00:03:53,120 --> 00:03:57,680
ai can help minimize mistakes reduce the

00:03:55,680 --> 00:04:01,040
probability of human error

00:03:57,680 --> 00:04:03,680
and improve overall business efficiency

00:04:01,040 --> 00:04:05,599
ai helps customize and automate large

00:04:03,680 --> 00:04:07,519
volumes of data and content

00:04:05,599 --> 00:04:08,640
which has the capacity to improve

00:04:07,519 --> 00:04:12,080
productivity

00:04:08,640 --> 00:04:13,840
and creativity so before we can start

00:04:12,080 --> 00:04:15,360
reaping the benefits of ai there are

00:04:13,840 --> 00:04:18,000
some obvious factors that

00:04:15,360 --> 00:04:18,479
organizations should consider along the

00:04:18,000 --> 00:04:21,199
journey

00:04:18,479 --> 00:04:23,040
first is strategy in order for people

00:04:21,199 --> 00:04:24,320
process and technology to function

00:04:23,040 --> 00:04:27,520
seamlessly

00:04:24,320 --> 00:04:30,800
an ai strategy must be clear concise

00:04:27,520 --> 00:04:32,880
and adaptable second is people

00:04:30,800 --> 00:04:33,840
the right people with the right digital

00:04:32,880 --> 00:04:36,080
skills

00:04:33,840 --> 00:04:38,880
are critical to successfully leveraging

00:04:36,080 --> 00:04:40,720
ai technology to transform the business

00:04:38,880 --> 00:04:42,960
talent can make or break digital

00:04:40,720 --> 00:04:45,520
transformation objectives

00:04:42,960 --> 00:04:46,560
third one is process it's no surprise

00:04:45,520 --> 00:04:49,040
that

00:04:46,560 --> 00:04:50,800
firms of all sizes are facing a lack of

00:04:49,040 --> 00:04:53,680
both well-curated data

00:04:50,800 --> 00:04:55,280
to train ai systems and formalized

00:04:53,680 --> 00:04:58,639
processes to assess

00:04:55,280 --> 00:04:59,919
key business challenges and last but not

00:04:58,639 --> 00:05:02,479
least is technology

00:04:59,919 --> 00:05:03,919
on one hand ai needs the utilization of

00:05:02,479 --> 00:05:06,720
other technology tools

00:05:03,919 --> 00:05:07,280
to execute its tasks efficiently on the

00:05:06,720 --> 00:05:09,440
other hand

00:05:07,280 --> 00:05:10,639
it requires employees to trust the

00:05:09,440 --> 00:05:13,039
technology

00:05:10,639 --> 00:05:14,000
ai can drive the insights that empower

00:05:13,039 --> 00:05:15,520
businesses

00:05:14,000 --> 00:05:18,560
as long as we can overcome this

00:05:15,520 --> 00:05:18,560
technology barrier

00:05:21,919 --> 00:05:26,960
so here let's start with strategy a

00:05:24,240 --> 00:05:29,759
little bit so ai success requires

00:05:26,960 --> 00:05:31,919
a formalized investment strategy the

00:05:29,759 --> 00:05:34,560
first step on the left is to understand

00:05:31,919 --> 00:05:35,520
how and where data is being captured and

00:05:34,560 --> 00:05:38,080
stored

00:05:35,520 --> 00:05:39,280
for data analytics we need to understand

00:05:38,080 --> 00:05:42,080
where the

00:05:39,280 --> 00:05:44,320
analysis will occur and how much data

00:05:42,080 --> 00:05:46,320
movement will be involved

00:05:44,320 --> 00:05:47,440
are these high performance environments

00:05:46,320 --> 00:05:50,639
is it edge

00:05:47,440 --> 00:05:52,320
cloud hybrid on premise how will the

00:05:50,639 --> 00:05:54,479
data be used

00:05:52,320 --> 00:05:55,840
to get to an end game where we have a

00:05:54,479 --> 00:05:58,720
production grade

00:05:55,840 --> 00:05:59,600
ai project that generates iterative

00:05:58,720 --> 00:06:02,000
insights

00:05:59,600 --> 00:06:03,680
that feed into business decisions it all

00:06:02,000 --> 00:06:06,479
starts with data science

00:06:03,680 --> 00:06:08,720
and with modeling predictive modeling

00:06:06,479 --> 00:06:09,680
that relies on machine learning or even

00:06:08,720 --> 00:06:11,840
deep learning

00:06:09,680 --> 00:06:12,720
will eventually lead to unexpected

00:06:11,840 --> 00:06:14,720
inferences

00:06:12,720 --> 00:06:15,840
that could have a major impact on the

00:06:14,720 --> 00:06:18,800
business

00:06:15,840 --> 00:06:20,080
and the services provided so this is

00:06:18,800 --> 00:06:22,800
again practical ai

00:06:20,080 --> 00:06:23,360
that can be trusted and can positively

00:06:22,800 --> 00:06:26,000
affect

00:06:23,360 --> 00:06:28,560
practical or everyday lives as well as

00:06:26,000 --> 00:06:28,560
businesses

00:06:30,720 --> 00:06:34,080
for people we also need to ensure the

00:06:32,479 --> 00:06:36,560
right skills are available

00:06:34,080 --> 00:06:38,960
that can leverage ai technology and

00:06:36,560 --> 00:06:41,039
eventually transform the business

00:06:38,960 --> 00:06:42,319
whether you need to develop these skills

00:06:41,039 --> 00:06:44,639
in-house

00:06:42,319 --> 00:06:46,160
hiring some data scientists or get

00:06:44,639 --> 00:06:47,280
advice from outside consultants and

00:06:46,160 --> 00:06:49,520
suppliers

00:06:47,280 --> 00:06:51,919
it's pretty likely that your workforce

00:06:49,520 --> 00:06:53,759
will need to adapt to handle this

00:06:51,919 --> 00:06:55,759
the chart here shows a really good list

00:06:53,759 --> 00:06:56,720
of skill sets that would be useful for

00:06:55,759 --> 00:06:58,880
ai

00:06:56,720 --> 00:07:00,639
from the data analysts to the engineers

00:06:58,880 --> 00:07:02,479
to the data scientists

00:07:00,639 --> 00:07:04,000
it also includes everything from

00:07:02,479 --> 00:07:06,560
statistical programming

00:07:04,000 --> 00:07:08,560
to machine learning techniques to data

00:07:06,560 --> 00:07:11,440
visualization

00:07:08,560 --> 00:07:11,440
pretty good table there

00:07:12,639 --> 00:07:18,720
next slide so

00:07:16,479 --> 00:07:20,720
as far as process goes so the process to

00:07:18,720 --> 00:07:22,560
achieve a production model

00:07:20,720 --> 00:07:24,319
is key to eventually improving

00:07:22,560 --> 00:07:27,520
efficiency efficiencies

00:07:24,319 --> 00:07:30,400
in business and in it operations

00:07:27,520 --> 00:07:32,400
ai is kind of like an iceberg where

00:07:30,400 --> 00:07:33,039
everyone focuses on the visible top

00:07:32,400 --> 00:07:35,680
layer

00:07:33,039 --> 00:07:36,800
the shiny application layer but forgets

00:07:35,680 --> 00:07:39,599
what is under the water

00:07:36,800 --> 00:07:42,560
the big part and that's seen as simply

00:07:39,599 --> 00:07:44,400
an infrastructure problem in some ways

00:07:42,560 --> 00:07:45,599
how to get the data how to clean and

00:07:44,400 --> 00:07:47,440
prepare the data

00:07:45,599 --> 00:07:49,120
how to build models into

00:07:47,440 --> 00:07:52,160
production-ready artifacts

00:07:49,120 --> 00:07:55,599
how to manage workloads and scalability

00:07:52,160 --> 00:07:58,479
that's all under the water the goal here

00:07:55,599 --> 00:07:59,680
is to eliminate the complexity of the ai

00:07:58,479 --> 00:08:01,919
infrastructure

00:07:59,680 --> 00:08:03,440
through a more holistic approach that

00:08:01,919 --> 00:08:06,560
spans from services

00:08:03,440 --> 00:08:07,120
to infrastructure to support the

00:08:06,560 --> 00:08:09,759
challenge

00:08:07,120 --> 00:08:11,599
is to get the ai project into production

00:08:09,759 --> 00:08:12,800
while satisfying all the requirements

00:08:11,599 --> 00:08:15,120
for being deployed

00:08:12,800 --> 00:08:15,919
across multiple environments with

00:08:15,120 --> 00:08:19,599
security

00:08:15,919 --> 00:08:21,360
and with manageability we know data

00:08:19,599 --> 00:08:23,599
scientists are not

00:08:21,360 --> 00:08:24,720
infrastructure experts and devops

00:08:23,599 --> 00:08:27,919
operators are not

00:08:24,720 --> 00:08:30,560
ai experts but data scientists need

00:08:27,919 --> 00:08:32,399
workloads that can be changed as the

00:08:30,560 --> 00:08:34,640
underlying hardware changes

00:08:32,399 --> 00:08:35,680
and data scientists and ai operators

00:08:34,640 --> 00:08:38,320
both need

00:08:35,680 --> 00:08:39,360
software building blocks and guidance on

00:08:38,320 --> 00:08:42,159
deployment

00:08:39,360 --> 00:08:43,839
as well as how to manage remotely and a

00:08:42,159 --> 00:08:47,279
lot of the work that we've been doing

00:08:43,839 --> 00:08:48,560
um inside sousa is trying to address

00:08:47,279 --> 00:08:50,640
those challenges

00:08:48,560 --> 00:08:53,040
and that's something that that

00:08:50,640 --> 00:08:55,680
alessandro in a few minutes will

00:08:53,040 --> 00:08:58,160
will demo for us as well you go to the

00:08:55,680 --> 00:08:58,160
next line

00:08:59,600 --> 00:09:04,560
ultimately it comes down to technology

00:09:02,480 --> 00:09:04,959
and the tools and packages that will be

00:09:04,560 --> 00:09:06,839
used

00:09:04,959 --> 00:09:08,240
to unlock the insights from the data

00:09:06,839 --> 00:09:11,120
analysis

00:09:08,240 --> 00:09:12,959
an ai stack such as what is shown here

00:09:11,120 --> 00:09:15,040
provides a guide post

00:09:12,959 --> 00:09:16,399
that businesses can use in developing

00:09:15,040 --> 00:09:19,839
the right environment

00:09:16,399 --> 00:09:20,240
and models for ai projects there are

00:09:19,839 --> 00:09:22,080
many

00:09:20,240 --> 00:09:23,519
technology decisions to be made along

00:09:22,080 --> 00:09:24,560
the way starting at the bottom of the

00:09:23,519 --> 00:09:26,720
diagram

00:09:24,560 --> 00:09:28,640
what hardware platforms and accelerators

00:09:26,720 --> 00:09:30,720
are available to you

00:09:28,640 --> 00:09:31,920
where will your analytics applications

00:09:30,720 --> 00:09:34,320
be deployed

00:09:31,920 --> 00:09:36,959
monitored and managed where will the

00:09:34,320 --> 00:09:38,560
data be collected and then analyzed

00:09:36,959 --> 00:09:40,320
what additional infrastructure or

00:09:38,560 --> 00:09:42,640
container orchestration

00:09:40,320 --> 00:09:44,399
and management might be needed for

00:09:42,640 --> 00:09:46,320
application orchestration how will your

00:09:44,399 --> 00:09:47,680
pipelines and workflows be developed and

00:09:46,320 --> 00:09:49,519
validated

00:09:47,680 --> 00:09:51,920
and also what additional machine

00:09:49,519 --> 00:09:53,600
learning or neural network libraries and

00:09:51,920 --> 00:09:56,880
tools will be needed

00:09:53,600 --> 00:09:59,120
in developing ai containers

00:09:56,880 --> 00:10:00,800
once you have these building blocks for

00:09:59,120 --> 00:10:03,279
this reference architecture

00:10:00,800 --> 00:10:05,680
this end-to-end technology guidance you

00:10:03,279 --> 00:10:08,480
can achieve an optimized infrastructure

00:10:05,680 --> 00:10:11,839
for your ai project that fits within

00:10:08,480 --> 00:10:14,399
your architectural environments

00:10:11,839 --> 00:10:15,839
okay so with that i'll turn it over to

00:10:14,399 --> 00:10:17,360
alessandro now

00:10:15,839 --> 00:10:19,040
and like i said he's going to go through

00:10:17,360 --> 00:10:21,839
a demo um

00:10:19,040 --> 00:10:22,959
hopefully it works live and alessandro

00:10:21,839 --> 00:10:25,360
over to you

00:10:22,959 --> 00:10:27,279
thank you jeff and actually well i will

00:10:25,360 --> 00:10:27,839
try to do even something more risky that

00:10:27,279 --> 00:10:30,320
is like

00:10:27,839 --> 00:10:32,160
two demos in one so literally two

00:10:30,320 --> 00:10:33,360
products in one single demo so

00:10:32,160 --> 00:10:36,079
that definitely definitely i'm

00:10:33,360 --> 00:10:38,720
challenging the gods of demos tonight

00:10:36,079 --> 00:10:40,240
so let's see how it goes so let's start

00:10:38,720 --> 00:10:42,560
from addressing the challenge actually

00:10:40,240 --> 00:10:44,880
jeff did an amazing job in describing

00:10:42,560 --> 00:10:48,800
the what we call the artificial iceberg

00:10:44,880 --> 00:10:51,120
um that explain you know comparison

00:10:48,800 --> 00:10:52,240
how ai could be more complex than what

00:10:51,120 --> 00:10:54,880
it seems

00:10:52,240 --> 00:10:56,720
so in order to explain it more let me

00:10:54,880 --> 00:11:00,000
start from

00:10:56,720 --> 00:11:01,600
what will serve being a day-to-day of

00:11:00,000 --> 00:11:03,760
a data scientist or maybe other

00:11:01,600 --> 00:11:06,800
companies that actually want to run ai

00:11:03,760 --> 00:11:09,120
production so let me start with this

00:11:06,800 --> 00:11:11,600
story we got two personas here

00:11:09,120 --> 00:11:12,160
the lady here is our data scientist we

00:11:11,600 --> 00:11:16,640
caller

00:11:12,160 --> 00:11:19,920
jim while the guy is our devops

00:11:16,640 --> 00:11:22,720
and we call her sasha they say that gene

00:11:19,920 --> 00:11:24,399
has been tasked to solve a specific use

00:11:22,720 --> 00:11:25,120
case could be could be basically any

00:11:24,399 --> 00:11:28,880
there's about

00:11:25,120 --> 00:11:32,160
maybe i don't know identify

00:11:28,880 --> 00:11:33,600
health tissue in a healthcare scenario

00:11:32,160 --> 00:11:36,480
so she has to build a model

00:11:33,600 --> 00:11:37,279
that looking on the mri scan can

00:11:36,480 --> 00:11:41,440
identify

00:11:37,279 --> 00:11:44,240
properly any issue and then the

00:11:41,440 --> 00:11:46,560
the idea is that at the very end we will

00:11:44,240 --> 00:11:48,399
basically pass this modis training model

00:11:46,560 --> 00:11:50,320
to the inference server where we can

00:11:48,399 --> 00:11:50,800
apply it to the application so we will

00:11:50,320 --> 00:11:52,480
have a

00:11:50,800 --> 00:11:54,320
let's call it between brackets and smart

00:11:52,480 --> 00:11:56,320
applications that help doctors

00:11:54,320 --> 00:11:58,000
to identify any health tissue before

00:11:56,320 --> 00:12:00,079
time now

00:11:58,000 --> 00:12:01,279
what typically gene does as a data

00:12:00,079 --> 00:12:04,160
scientist

00:12:01,279 --> 00:12:06,160
is she writes probably locally on an

00:12:04,160 --> 00:12:06,880
interactive environment like a jupiter

00:12:06,160 --> 00:12:10,079
notebook

00:12:06,880 --> 00:12:12,320
she writes a model using a chunk of the

00:12:10,079 --> 00:12:13,120
original data set she explored different

00:12:12,320 --> 00:12:16,000
models

00:12:13,120 --> 00:12:16,959
uh to analyze and get uh a good results

00:12:16,000 --> 00:12:19,360
at least what she

00:12:16,959 --> 00:12:21,519
she observed as a good result with

00:12:19,360 --> 00:12:24,480
enough precision with enough

00:12:21,519 --> 00:12:26,079
performance and so on but then she has

00:12:24,480 --> 00:12:28,160
to embed this model

00:12:26,079 --> 00:12:29,600
in a in what we call an ai pipeline and

00:12:28,160 --> 00:12:32,160
what is an ipad mine

00:12:29,600 --> 00:12:33,600
an ai pipeline is the full step

00:12:32,160 --> 00:12:35,839
end-to-end

00:12:33,600 --> 00:12:36,720
of the entire scenario the data that

00:12:35,839 --> 00:12:38,720
comes in

00:12:36,720 --> 00:12:39,839
in the form of a batch or a real-time

00:12:38,720 --> 00:12:42,800
streaming

00:12:39,839 --> 00:12:44,720
the way we manipulate this data how we

00:12:42,800 --> 00:12:45,839
apply the model to the data we train the

00:12:44,720 --> 00:12:48,079
model

00:12:45,839 --> 00:12:50,560
eventually analyzing different models

00:12:48,079 --> 00:12:53,519
and deciding which one applies better

00:12:50,560 --> 00:12:54,959
and then finally how we output the model

00:12:53,519 --> 00:12:55,600
and pass the model directory to the

00:12:54,959 --> 00:12:57,680
inference

00:12:55,600 --> 00:12:59,120
the service models to serve the model to

00:12:57,680 --> 00:13:02,480
the inference

00:12:59,120 --> 00:13:05,279
now in order to create such a pipeline

00:13:02,480 --> 00:13:07,200
typically we have to use a platform and

00:13:05,279 --> 00:13:10,880
a multiple platform out there

00:13:07,200 --> 00:13:13,680
to name a few q flow argo apache airflow

00:13:10,880 --> 00:13:15,600
and so on whether they are running as a

00:13:13,680 --> 00:13:17,120
within a cloud provider offering or

00:13:15,600 --> 00:13:19,600
on-prem offering

00:13:17,120 --> 00:13:20,160
and maybe you either even have multiple

00:13:19,600 --> 00:13:22,240
environments

00:13:20,160 --> 00:13:24,240
around you where you have that test and

00:13:22,240 --> 00:13:27,120
production environment

00:13:24,240 --> 00:13:28,399
now the problem is that jesus is not an

00:13:27,120 --> 00:13:30,079
inflationary expert

00:13:28,399 --> 00:13:31,920
she knows everything about mathematical

00:13:30,079 --> 00:13:33,279
models but she doesn't almost anything

00:13:31,920 --> 00:13:35,680
about

00:13:33,279 --> 00:13:37,279
infraction about containers about how to

00:13:35,680 --> 00:13:37,920
embed the model within the platform

00:13:37,279 --> 00:13:40,639
because

00:13:37,920 --> 00:13:42,480
in order to do so she should have to use

00:13:40,639 --> 00:13:45,040
a specific metal language an

00:13:42,480 --> 00:13:46,959
sdk let's take for the sake of this

00:13:45,040 --> 00:13:49,440
presentation q plot

00:13:46,959 --> 00:13:51,279
in order to run and create a pipeline on

00:13:49,440 --> 00:13:54,880
queue flow you probably have to use

00:13:51,279 --> 00:13:56,720
the sdk from cube or kfp and the sl

00:13:54,880 --> 00:13:58,959
and you have to write it in a way that

00:13:56,720 --> 00:14:00,160
actually that allowed ask you many many

00:13:58,959 --> 00:14:02,880
information

00:14:00,160 --> 00:14:04,720
sasha on the other hand he knows this

00:14:02,880 --> 00:14:06,320
information he knows what the container

00:14:04,720 --> 00:14:08,000
is he knows how to optimize the

00:14:06,320 --> 00:14:10,480
container he probably knows even how to

00:14:08,000 --> 00:14:12,320
optimize the infrastructure

00:14:10,480 --> 00:14:14,000
but again sasha doesn't know anything

00:14:12,320 --> 00:14:14,880
about machine work he doesn't know

00:14:14,000 --> 00:14:17,040
anything about

00:14:14,880 --> 00:14:18,160
data science and it doesn't anything

00:14:17,040 --> 00:14:20,720
know about

00:14:18,160 --> 00:14:22,160
the pipeline the gene has in mind so

00:14:20,720 --> 00:14:24,320
they are stuck there

00:14:22,160 --> 00:14:25,760
they are basically trying to collaborate

00:14:24,320 --> 00:14:29,440
but actually there are many many

00:14:25,760 --> 00:14:30,639
issues so to visualize better what i'm

00:14:29,440 --> 00:14:34,079
just saying

00:14:30,639 --> 00:14:36,480
this is a simple example of of the

00:14:34,079 --> 00:14:37,839
of the flow on the left that's the

00:14:36,480 --> 00:14:39,279
jupiter notebook

00:14:37,839 --> 00:14:41,600
that's the part that's being written by

00:14:39,279 --> 00:14:43,519
gene and probably it's been written in

00:14:41,600 --> 00:14:44,079
jupiter noble so it's an entire vitamin

00:14:43,519 --> 00:14:46,800
environment

00:14:44,079 --> 00:14:48,160
so you give our results on the right

00:14:46,800 --> 00:14:50,399
that is a the graph

00:14:48,160 --> 00:14:51,519
of a queue for pipeline and each box

00:14:50,399 --> 00:14:54,079
there

00:14:51,519 --> 00:14:56,480
is a specific step and each step is a

00:14:54,079 --> 00:14:57,440
specific container with his own specific

00:14:56,480 --> 00:14:59,680
arguments and

00:14:57,440 --> 00:15:01,040
in its capability like the outputs of a

00:14:59,680 --> 00:15:04,240
file if you need it

00:15:01,040 --> 00:15:06,480
and so on the part in the center that is

00:15:04,240 --> 00:15:09,519
the meta language i was mentioning there

00:15:06,480 --> 00:15:11,120
so one of the two sash or jean has to

00:15:09,519 --> 00:15:14,880
learn how to write that

00:15:11,120 --> 00:15:16,399
and ask to learn how to execute that

00:15:14,880 --> 00:15:18,240
and as you can see the part in the

00:15:16,399 --> 00:15:20,560
center is not really data science

00:15:18,240 --> 00:15:22,160
and it's not really pure infrastructure

00:15:20,560 --> 00:15:26,240
pure devops kind of

00:15:22,160 --> 00:15:28,480
it's more a mix of that so that end up

00:15:26,240 --> 00:15:30,240
in a scenario where you can have

00:15:28,480 --> 00:15:32,560
multiple data scientists

00:15:30,240 --> 00:15:34,639
few devops that can learn or knows how

00:15:32,560 --> 00:15:36,800
to run the pipelines

00:15:34,639 --> 00:15:37,759
and they really struggle with

00:15:36,800 --> 00:15:40,320
collaborating

00:15:37,759 --> 00:15:42,399
so we thought that because we are an

00:15:40,320 --> 00:15:44,560
infrastructure company because we

00:15:42,399 --> 00:15:46,720
our dna is to solve those infrastructure

00:15:44,560 --> 00:15:48,880
challenges we can help out there

00:15:46,720 --> 00:15:50,000
and so we start with a project called ai

00:15:48,880 --> 00:15:53,040
orchestrator

00:15:50,000 --> 00:15:54,720
and the orchestrator basically try to

00:15:53,040 --> 00:15:56,880
solve this this struggling

00:15:54,720 --> 00:15:59,199
or this pain if you want so let me

00:15:56,880 --> 00:16:01,360
rewrite a little bit outflow

00:15:59,199 --> 00:16:02,320
generates a model as before nothing

00:16:01,360 --> 00:16:04,079
changes there

00:16:02,320 --> 00:16:06,079
the only thing that it changes that we

00:16:04,079 --> 00:16:08,160
ask jean to

00:16:06,079 --> 00:16:09,519
save everything she write in a git

00:16:08,160 --> 00:16:12,639
repository

00:16:09,519 --> 00:16:16,160
github github bamboo as long as it's git

00:16:12,639 --> 00:16:17,120
private public doesn't matter the second

00:16:16,160 --> 00:16:19,920
things we

00:16:17,120 --> 00:16:21,360
we do this because first we can leverage

00:16:19,920 --> 00:16:23,360
versioning from git

00:16:21,360 --> 00:16:25,199
from the git repos and the second is

00:16:23,360 --> 00:16:26,079
that we can automatically be triggered

00:16:25,199 --> 00:16:28,399
by this

00:16:26,079 --> 00:16:30,480
git rep so if things changes we can

00:16:28,399 --> 00:16:32,800
automatically trigger a new pipeline

00:16:30,480 --> 00:16:33,920
and i'm using pipeline the term timeline

00:16:32,800 --> 00:16:36,480
proposed here

00:16:33,920 --> 00:16:37,279
so generates a modem she has no clue

00:16:36,480 --> 00:16:38,880
about

00:16:37,279 --> 00:16:40,720
metal languages she has no clue about

00:16:38,880 --> 00:16:42,160
the pipeline person she has a design in

00:16:40,720 --> 00:16:44,480
mind probably

00:16:42,160 --> 00:16:47,360
maybe she would be happy if she can she

00:16:44,480 --> 00:16:50,560
if she can design this graphically but

00:16:47,360 --> 00:16:53,199
that's it that's all she knows now

00:16:50,560 --> 00:16:55,360
once she's ready with the models she

00:16:53,199 --> 00:16:57,120
basically upload her artifacts to the

00:16:55,360 --> 00:16:59,440
suzeraire orchestrator

00:16:57,120 --> 00:17:01,120
and basically she will just input few

00:16:59,440 --> 00:17:02,880
information like the name of

00:17:01,120 --> 00:17:04,640
the pipeline the description of a

00:17:02,880 --> 00:17:06,640
pipeline to help al sasha later to

00:17:04,640 --> 00:17:09,600
identify what she's going to do

00:17:06,640 --> 00:17:10,160
there the steps that she want to execute

00:17:09,600 --> 00:17:12,959
um

00:17:10,160 --> 00:17:13,839
with the artifacts she save it if there

00:17:12,959 --> 00:17:16,079
was our script

00:17:13,839 --> 00:17:16,880
the way we can run these scripts is it

00:17:16,079 --> 00:17:19,760
python

00:17:16,880 --> 00:17:20,160
is it julia is it r we just need to have

00:17:19,760 --> 00:17:23,360
a

00:17:20,160 --> 00:17:25,439
definition of the steps and that's

00:17:23,360 --> 00:17:27,600
it that's all she has to do sasha will

00:17:25,439 --> 00:17:29,200
receive a notification that says hey

00:17:27,600 --> 00:17:30,799
one of your data scientists just upload

00:17:29,200 --> 00:17:32,480
a new pipeline

00:17:30,799 --> 00:17:34,720
what happened actually in the meantime

00:17:32,480 --> 00:17:37,360
is that we transformed those artifacts

00:17:34,720 --> 00:17:38,720
in a compliant pipeline with single or

00:17:37,360 --> 00:17:40,640
multiple steps

00:17:38,720 --> 00:17:42,080
apply a template that can be the default

00:17:40,640 --> 00:17:44,720
template or

00:17:42,080 --> 00:17:46,080
a customized version of the template so

00:17:44,720 --> 00:17:48,320
that sasha can

00:17:46,080 --> 00:17:49,280
actually change the parameters the

00:17:48,320 --> 00:17:51,280
containers

00:17:49,280 --> 00:17:52,400
the arguments the things that actually

00:17:51,280 --> 00:17:55,919
he knows

00:17:52,400 --> 00:17:56,880
uh mostly or off and at the same time we

00:17:55,919 --> 00:17:59,360
can run this

00:17:56,880 --> 00:18:00,240
new pipeline template against any of the

00:17:59,360 --> 00:18:03,200
target that you see

00:18:00,240 --> 00:18:03,600
on this slide so literally we transform

00:18:03,200 --> 00:18:05,520
a

00:18:03,600 --> 00:18:07,679
universe in a universal metal language

00:18:05,520 --> 00:18:09,440
that allow you to run the pipeline

00:18:07,679 --> 00:18:11,520
against this platform without having to

00:18:09,440 --> 00:18:13,440
rewrite your code and even more

00:18:11,520 --> 00:18:15,039
such at that point can schedule the

00:18:13,440 --> 00:18:16,880
pipeline if you have

00:18:15,039 --> 00:18:19,919
the multiple environment scenario like i

00:18:16,880 --> 00:18:23,200
described it before that test production

00:18:19,919 --> 00:18:26,720
then actually sasha can literally

00:18:23,200 --> 00:18:29,200
use that and decided that you run on dev

00:18:26,720 --> 00:18:30,400
then you will trigger if it's successful

00:18:29,200 --> 00:18:32,880
to run it on

00:18:30,400 --> 00:18:34,559
on on test and then if triggered if

00:18:32,880 --> 00:18:35,840
you're running successfully on prod

00:18:34,559 --> 00:18:38,000
direct directly

00:18:35,840 --> 00:18:39,520
and maybe in the meantime just reapply

00:18:38,000 --> 00:18:42,320
different templates

00:18:39,520 --> 00:18:44,000
now again because visual is better than

00:18:42,320 --> 00:18:45,760
than just describing things

00:18:44,000 --> 00:18:48,240
i'm going to to go directly for the

00:18:45,760 --> 00:18:51,600
lifetime but as i said

00:18:48,240 --> 00:18:54,559
i want to run a really special demo uh

00:18:51,600 --> 00:18:56,400
it's two demos in one so let me let me

00:18:54,559 --> 00:18:57,919
explain this let me first of all let's

00:18:56,400 --> 00:19:00,160
see if i can actually

00:18:57,919 --> 00:19:01,520
move this guy here so you see all you

00:19:00,160 --> 00:19:05,600
you can see better

00:19:01,520 --> 00:19:10,799
and let me just do a little bit more

00:19:05,600 --> 00:19:14,799
zoom here so the first step was

00:19:10,799 --> 00:19:17,120
i want to write a pipeline and i want to

00:19:14,799 --> 00:19:18,880
simulate what gina has to do every day

00:19:17,120 --> 00:19:21,120
and then i want to have a simple radio

00:19:18,880 --> 00:19:24,320
orchestrator

00:19:21,120 --> 00:19:26,400
to run this pipeline but

00:19:24,320 --> 00:19:28,160
i want to do a little bit more and this

00:19:26,400 --> 00:19:31,280
little bit more is about

00:19:28,160 --> 00:19:32,720
i'd like to have an environment on my

00:19:31,280 --> 00:19:35,120
own laptop

00:19:32,720 --> 00:19:36,640
that is really similar to what i will

00:19:35,120 --> 00:19:38,160
find in the data center

00:19:36,640 --> 00:19:40,320
let's say that i want to run the qfl

00:19:38,160 --> 00:19:43,280
pipe on my own laptop

00:19:40,320 --> 00:19:45,200
and possibly i want to also have

00:19:43,280 --> 00:19:48,000
eventually orchestrator as well

00:19:45,200 --> 00:19:48,400
but i want to install every kind of ai

00:19:48,000 --> 00:19:51,679
tool

00:19:48,400 --> 00:19:54,480
or platform i want to test out

00:19:51,679 --> 00:19:55,600
so that basically i can literally design

00:19:54,480 --> 00:19:58,160
and create

00:19:55,600 --> 00:19:59,679
my own end-to-end scenario easily and

00:19:58,160 --> 00:20:02,240
then just at that point

00:19:59,679 --> 00:20:03,760
move everything to the data center

00:20:02,240 --> 00:20:05,840
seamlessly

00:20:03,760 --> 00:20:07,919
in order to do so i'm going to use

00:20:05,840 --> 00:20:09,679
another open source project we released

00:20:07,919 --> 00:20:11,440
that i will describe a minute after the

00:20:09,679 --> 00:20:15,120
demo this is called k3i

00:20:11,440 --> 00:20:18,320
ktri is a simple way to install

00:20:15,120 --> 00:20:20,640
every ai tools every i platform you like

00:20:18,320 --> 00:20:22,720
including supporting gpus including

00:20:20,640 --> 00:20:25,360
supporting wsl

00:20:22,720 --> 00:20:26,480
directly in your laptop or any edge of

00:20:25,360 --> 00:20:29,840
edge devices

00:20:26,480 --> 00:20:32,159
and then start experiment from there so

00:20:29,840 --> 00:20:32,960
my goal is i want to have q flow

00:20:32,159 --> 00:20:34,880
pipeline

00:20:32,960 --> 00:20:36,799
and i want to start from a notebook so i

00:20:34,880 --> 00:20:39,280
need to have a jupiter number

00:20:36,799 --> 00:20:40,880
so all i have to do is just go to the

00:20:39,280 --> 00:20:43,280
k2i website

00:20:40,880 --> 00:20:44,320
decided i want to have the jupyter

00:20:43,280 --> 00:20:46,480
notebooks

00:20:44,320 --> 00:20:48,000
and i have the whole stack of jupyter

00:20:46,480 --> 00:20:51,280
notebook i will take the minimal

00:20:48,000 --> 00:20:52,720
one the minimum notebook click there and

00:20:51,280 --> 00:20:54,640
it won't install everything for me

00:20:52,720 --> 00:20:56,000
and to demonstrate that i have my

00:20:54,640 --> 00:20:57,840
environment ready

00:20:56,000 --> 00:20:59,120
so i already pre-installed the pipeline

00:20:57,840 --> 00:21:02,000
because for the sake

00:20:59,120 --> 00:21:02,640
for the sake of time but as you can see

00:21:02,000 --> 00:21:04,880
here

00:21:02,640 --> 00:21:08,080
there's no jupiter there so all i have

00:21:04,880 --> 00:21:11,120
to do is just

00:21:08,080 --> 00:21:13,039
copy and paste that the steps adding a

00:21:11,120 --> 00:21:14,159
flag that says well because you already

00:21:13,039 --> 00:21:16,480
installed

00:21:14,159 --> 00:21:17,840
k3i you can skip the cluster

00:21:16,480 --> 00:21:21,200
installation

00:21:17,840 --> 00:21:23,520
so let me oops it seems like i have a

00:21:21,200 --> 00:21:23,520
okay

00:21:26,320 --> 00:21:31,440
okay s and then run it

00:21:29,600 --> 00:21:33,360
you will see that basically one single

00:21:31,440 --> 00:21:36,080
line command i'm not doing anything

00:21:33,360 --> 00:21:38,080
you see jupiter appearing here is

00:21:36,080 --> 00:21:39,200
defining the ingress so i can basically

00:21:38,080 --> 00:21:41,760
just

00:21:39,200 --> 00:21:43,280
take the url of this jupyter and do it

00:21:41,760 --> 00:21:44,400
and you will see that i will do even a

00:21:43,280 --> 00:21:46,240
bunch of stuff

00:21:44,400 --> 00:21:48,080
i'm going to use this jupiter with q for

00:21:46,240 --> 00:21:51,039
pipeline so i suppose to have

00:21:48,080 --> 00:21:51,760
the meta language installed kfp and in

00:21:51,039 --> 00:21:53,520
fact

00:21:51,760 --> 00:21:55,280
the automation installation is doing

00:21:53,520 --> 00:21:57,200
everything for me i don't have to know

00:21:55,280 --> 00:21:59,440
anything about kubernetes i don't know

00:21:57,200 --> 00:22:02,400
to do anything on my side

00:21:59,440 --> 00:22:03,200
all i have to do is just sit the ear

00:22:02,400 --> 00:22:06,400
waiting

00:22:03,200 --> 00:22:08,000
and get a url at the end and using it so

00:22:06,400 --> 00:22:11,200
i could be a data scientist

00:22:08,000 --> 00:22:14,559
and still i'm starting to make

00:22:11,200 --> 00:22:15,360
a usage ovi without effort now let's

00:22:14,559 --> 00:22:17,280
wait for

00:22:15,360 --> 00:22:18,880
a couple of seconds everything's done

00:22:17,280 --> 00:22:21,679
you can see the url here

00:22:18,880 --> 00:22:23,600
and just copy and paste the url come

00:22:21,679 --> 00:22:26,640
back here

00:22:23,600 --> 00:22:29,919
paste my url and i got the full jupiter

00:22:26,640 --> 00:22:31,600
workbook there now in order to

00:22:29,919 --> 00:22:32,159
demonstrate that actually i'm doing

00:22:31,600 --> 00:22:35,360
something

00:22:32,159 --> 00:22:36,559
real i'm taking a public example from

00:22:35,360 --> 00:22:39,280
qflo

00:22:36,559 --> 00:22:40,799
so first step now i got the q flow

00:22:39,280 --> 00:22:42,320
pipeline and the jupiter notebook

00:22:40,799 --> 00:22:44,480
up and running in my laptop so i can

00:22:42,320 --> 00:22:47,520
experiment i want to learn

00:22:44,480 --> 00:22:48,480
the sdk just to demonstrate how complex

00:22:47,520 --> 00:22:50,480
is

00:22:48,480 --> 00:22:51,840
building pipelines nothing wrong with

00:22:50,480 --> 00:22:53,440
that but

00:22:51,840 --> 00:22:54,960
there's a friction there i'm a data

00:22:53,440 --> 00:22:56,960
scientist i'm not an

00:22:54,960 --> 00:22:58,640
expert so i'm taking this notebook this

00:22:56,960 --> 00:23:02,000
notebook comes from the

00:22:58,640 --> 00:23:05,039
public example of qflo so i'm going to

00:23:02,000 --> 00:23:05,039
save the notebook

00:23:05,300 --> 00:23:10,080
[Music]

00:23:07,440 --> 00:23:10,080
as it is

00:23:11,039 --> 00:23:18,480
and i'm going to upload my new up

00:23:14,400 --> 00:23:21,200
notebook into my environment

00:23:18,480 --> 00:23:22,080
as you can see this is no coding is just

00:23:21,200 --> 00:23:25,120
simply click

00:23:22,080 --> 00:23:25,919
and down open the notebook is exactly

00:23:25,120 --> 00:23:29,440
the same

00:23:25,919 --> 00:23:32,480
as before all i have to do is just

00:23:29,440 --> 00:23:34,080
take in notes of my url

00:23:32,480 --> 00:23:35,679
and then start to execute in the

00:23:34,080 --> 00:23:37,360
notebook i'm not going to execute the

00:23:35,679 --> 00:23:38,640
first cell because kfb already been

00:23:37,360 --> 00:23:40,640
installed for me

00:23:38,640 --> 00:23:42,480
so i'm going to execute the first cell

00:23:40,640 --> 00:23:44,000
i'm going to execute the second cell you

00:23:42,480 --> 00:23:45,840
see i have no error because everything

00:23:44,000 --> 00:23:47,919
is already preset for me

00:23:45,840 --> 00:23:49,039
the third cell this is pretty stupid

00:23:47,919 --> 00:23:52,320
pipeline just

00:23:49,039 --> 00:23:55,440
do a calculation and it will be there so

00:23:52,320 --> 00:23:58,320
almost at the end i'm ready to compile

00:23:55,440 --> 00:23:59,600
i'm adding one single things that the

00:23:58,320 --> 00:24:02,880
url of my

00:23:59,600 --> 00:24:03,520
q flow local pipeline that was the url i

00:24:02,880 --> 00:24:06,080
took from

00:24:03,520 --> 00:24:07,919
from here and then executed and if

00:24:06,080 --> 00:24:09,279
everything goes right when i click here

00:24:07,919 --> 00:24:12,240
it will open up

00:24:09,279 --> 00:24:14,240
qr pipeline for me with the pipeline

00:24:12,240 --> 00:24:17,520
running an experiment running for me

00:24:14,240 --> 00:24:20,320
i did not have to do anything it was so

00:24:17,520 --> 00:24:22,000
easy that literally in three minutes i'm

00:24:20,320 --> 00:24:24,559
literally running a pipeline

00:24:22,000 --> 00:24:25,279
so i can spend my time learning the

00:24:24,559 --> 00:24:28,240
stuff that i

00:24:25,279 --> 00:24:29,520
i care of i can spend my time learning

00:24:28,240 --> 00:24:32,240
kfp

00:24:29,520 --> 00:24:34,640
but now let's do a step back so first

00:24:32,240 --> 00:24:38,880
half of the demo okay what i've done

00:24:34,640 --> 00:24:42,799
good let's do a step back this notebook

00:24:38,880 --> 00:24:46,720
just did all these steps in the very end

00:24:42,799 --> 00:24:49,520
just to do a sum this simple calculation

00:24:46,720 --> 00:24:51,279
you take two numbers and you sum them

00:24:49,520 --> 00:24:54,240
that's it

00:24:51,279 --> 00:24:55,440
so in terms of script i simplify a

00:24:54,240 --> 00:24:58,480
little bit the script

00:24:55,440 --> 00:25:00,640
is just something like that you pass me

00:24:58,480 --> 00:25:01,360
a couple of arguments you print out the

00:25:00,640 --> 00:25:04,480
arguments

00:25:01,360 --> 00:25:05,360
that's it so in order to run this in a

00:25:04,480 --> 00:25:08,799
pipeline

00:25:05,360 --> 00:25:12,320
i had basically to build something like

00:25:08,799 --> 00:25:15,679
this again nothing wrong with that

00:25:12,320 --> 00:25:17,840
but the degree of complexity

00:25:15,679 --> 00:25:19,200
is higher and higher and higher of

00:25:17,840 --> 00:25:20,320
course there's also a degree of

00:25:19,200 --> 00:25:22,559
flexibility

00:25:20,320 --> 00:25:23,679
it allows me to do almost everything i

00:25:22,559 --> 00:25:26,960
like

00:25:23,679 --> 00:25:28,080
but it adds a lot of complexity as usual

00:25:26,960 --> 00:25:31,279
so can i

00:25:28,080 --> 00:25:32,480
basically avoid that so

00:25:31,279 --> 00:25:35,200
let's try it with the suzerai

00:25:32,480 --> 00:25:36,799
orchestrator just to demonstrate what i

00:25:35,200 --> 00:25:39,520
have here

00:25:36,799 --> 00:25:41,279
i have a full environment with my q flow

00:25:39,520 --> 00:25:44,559
pipeline locally

00:25:41,279 --> 00:25:44,880
q flow pipeline on aws i potentially can

00:25:44,559 --> 00:25:47,760
have

00:25:44,880 --> 00:25:48,400
also argo that is another platform so

00:25:47,760 --> 00:25:51,919
let's

00:25:48,400 --> 00:25:54,960
redo the same stuff but this time

00:25:51,919 --> 00:25:56,960
that from the orchestrator i'm gene now

00:25:54,960 --> 00:25:58,880
you will see all the option all together

00:25:56,960 --> 00:26:00,000
i'm not going to log out log in just for

00:25:58,880 --> 00:26:04,480
the sake of the demo

00:26:00,000 --> 00:26:04,480
i click on the pipeline give a name

00:26:05,760 --> 00:26:08,799
let's say demo pipeline i'm going just

00:26:08,400 --> 00:26:11,679
to

00:26:08,799 --> 00:26:13,200
say in this case because we are going to

00:26:11,679 --> 00:26:14,880
release the project so in my demo

00:26:13,200 --> 00:26:15,600
environment i expose the docker image

00:26:14,880 --> 00:26:17,919
for

00:26:15,600 --> 00:26:19,760
for me to control better demo otherwise

00:26:17,919 --> 00:26:20,799
in the final version this part will be

00:26:19,760 --> 00:26:22,960
no scene

00:26:20,799 --> 00:26:24,240
i'm going to put a custom image because

00:26:22,960 --> 00:26:26,559
i want to really

00:26:24,240 --> 00:26:28,320
do a be as much as flexible and i'm

00:26:26,559 --> 00:26:31,520
going to use a specific

00:26:28,320 --> 00:26:33,840
uh container like tensorflow again

00:26:31,520 --> 00:26:35,679
this part will be exposed to the devops

00:26:33,840 --> 00:26:37,600
not to the scientists i'm just using

00:26:35,679 --> 00:26:41,919
these for the demo purpose

00:26:37,600 --> 00:26:44,000
and then i can ask for basically the

00:26:41,919 --> 00:26:45,600
the demo requirement let's say that i'm

00:26:44,000 --> 00:26:48,880
doing something like that

00:26:45,600 --> 00:26:50,640
so what is this this is exactly what i

00:26:48,880 --> 00:26:54,240
wrote here

00:26:50,640 --> 00:26:57,039
take two arguments in input and sum them

00:26:54,240 --> 00:26:57,039
that's all i do

00:26:58,880 --> 00:27:04,720
so i come here i just pass my git repo

00:27:02,480 --> 00:27:06,000
eventually i can pass my branch if i

00:27:04,720 --> 00:27:09,360
have multiple range you remember

00:27:06,000 --> 00:27:11,760
the versioning things and just submit it

00:27:09,360 --> 00:27:13,679
that's all i have to do my job is done

00:27:11,760 --> 00:27:16,240
as a data scientist

00:27:13,679 --> 00:27:18,080
now the devops receiver notification

00:27:16,240 --> 00:27:18,960
maybe i as a data scientist have the

00:27:18,080 --> 00:27:21,360
same

00:27:18,960 --> 00:27:23,200
role so i can go straight away to the

00:27:21,360 --> 00:27:24,559
workloads i go to the workload i see the

00:27:23,200 --> 00:27:27,600
demo there's a request

00:27:24,559 --> 00:27:31,120
for a new demo there i can shadow around

00:27:27,600 --> 00:27:33,520
and i see all the all the platform

00:27:31,120 --> 00:27:34,480
connected to me so literally i can

00:27:33,520 --> 00:27:37,120
decided to

00:27:34,480 --> 00:27:38,000
want to run on both of them at the same

00:27:37,120 --> 00:27:41,120
time

00:27:38,000 --> 00:27:42,720
and run it and so what happened is that

00:27:41,120 --> 00:27:45,440
when i come here

00:27:42,720 --> 00:27:47,279
and i open up here's a demo running

00:27:45,440 --> 00:27:49,200
there is a pipeline

00:27:47,279 --> 00:27:50,399
and if i open up here here's another

00:27:49,200 --> 00:27:54,640
demo running

00:27:50,399 --> 00:27:56,320
what is happening i just passed a script

00:27:54,640 --> 00:27:58,080
we are building an entire piper we

00:27:56,320 --> 00:27:59,440
create a disk for you

00:27:58,080 --> 00:28:03,039
based on the configuration of the

00:27:59,440 --> 00:28:06,399
orchestrator we are collecting the data

00:28:03,039 --> 00:28:09,760
as expected once the data is collected

00:28:06,399 --> 00:28:12,480
we will basically do the calculation

00:28:09,760 --> 00:28:14,240
for you and once everything will be run

00:28:12,480 --> 00:28:15,919
we will expose the outcome to you we

00:28:14,240 --> 00:28:17,440
will send you the outcome so you will

00:28:15,919 --> 00:28:20,480
get the results there

00:28:17,440 --> 00:28:22,640
so later we create a pipeline and it's

00:28:20,480 --> 00:28:26,480
exactly the same pipeline

00:28:22,640 --> 00:28:30,640
as before in this exactly the same

00:28:26,480 --> 00:28:32,880
kind of flavor

00:28:30,640 --> 00:28:34,559
independently from the platform you're

00:28:32,880 --> 00:28:36,880
using you have to not to

00:28:34,559 --> 00:28:37,679
learn kfp you have to learn anything

00:28:36,880 --> 00:28:40,000
about the

00:28:37,679 --> 00:28:40,799
the platform you simply get the results

00:28:40,000 --> 00:28:43,120
out

00:28:40,799 --> 00:28:45,279
evaluate them and eventually move out

00:28:43,120 --> 00:28:47,360
but this is not enough

00:28:45,279 --> 00:28:49,600
because that's okay if it is a fast

00:28:47,360 --> 00:28:52,799
iteration kind of workload

00:28:49,600 --> 00:28:56,080
maybe i want to do something more um

00:28:52,799 --> 00:29:00,000
complex so i said before well indeed

00:28:56,080 --> 00:29:03,200
the data science knows how to build the

00:29:00,000 --> 00:29:05,520
um the pipeline so maybe

00:29:03,200 --> 00:29:07,600
you want to design the pipeline up front

00:29:05,520 --> 00:29:09,679
and that's why we are introducing the

00:29:07,600 --> 00:29:11,600
composer the composer is away and is

00:29:09,679 --> 00:29:12,559
still in development we will release the

00:29:11,600 --> 00:29:15,440
orchestrator

00:29:12,559 --> 00:29:16,799
more or less at the end of november as

00:29:15,440 --> 00:29:20,720
open source project

00:29:16,799 --> 00:29:23,760
is a way to build your pipeline

00:29:20,720 --> 00:29:26,799
dynamically for each steps you can say

00:29:23,760 --> 00:29:27,600
what you want to do let's say something

00:29:26,799 --> 00:29:30,159
like that

00:29:27,600 --> 00:29:31,840
and then the step is there and then you

00:29:30,159 --> 00:29:33,360
can literally create the pipeline and

00:29:31,840 --> 00:29:34,960
save it as a template so it's an

00:29:33,360 --> 00:29:38,159
agnostic pipeline

00:29:34,960 --> 00:29:40,000
you simply design your flow you pass the

00:29:38,159 --> 00:29:42,240
information that you have at that time

00:29:40,000 --> 00:29:43,520
you save it as a template and the

00:29:42,240 --> 00:29:45,840
template is here

00:29:43,520 --> 00:29:46,960
and what is a template is a template is

00:29:45,840 --> 00:29:50,080
a way

00:29:46,960 --> 00:29:53,760
to write the pipeline in a universal way

00:29:50,080 --> 00:29:58,480
so let me for example uh give an example

00:29:53,760 --> 00:29:58,480
i i'm going to create a new template

00:29:59,919 --> 00:30:03,919
for queue flow since we are using queue

00:30:01,679 --> 00:30:06,640
flow leveraging

00:30:03,919 --> 00:30:08,000
one of my template then i will download

00:30:06,640 --> 00:30:10,799
the templates just to explain

00:30:08,000 --> 00:30:10,799
you how it works

00:30:17,360 --> 00:30:23,760
submit it now i'm coming back

00:30:20,640 --> 00:30:24,720
to our original workload and rerun the

00:30:23,760 --> 00:30:26,640
same workload

00:30:24,720 --> 00:30:28,399
and this time only on local and i'm

00:30:26,640 --> 00:30:30,640
going to use the demo template

00:30:28,399 --> 00:30:33,360
this time and the pipeline will be

00:30:30,640 --> 00:30:35,600
executed exactly as before

00:30:33,360 --> 00:30:36,640
but this time it will be a completely

00:30:35,600 --> 00:30:39,039
different time

00:30:36,640 --> 00:30:39,760
pipeline in order to visualize better

00:30:39,039 --> 00:30:42,799
this

00:30:39,760 --> 00:30:45,200
i changed the names of the steps

00:30:42,799 --> 00:30:46,799
and i of course adding some some other

00:30:45,200 --> 00:30:47,919
steps just to explain them so the

00:30:46,799 --> 00:30:50,240
collecting data

00:30:47,919 --> 00:30:52,159
as before is collecting everything and

00:30:50,240 --> 00:30:54,080
then it's executing by this time instead

00:30:52,159 --> 00:30:56,080
of executing one step it will execute on

00:30:54,080 --> 00:30:57,519
multiple step at the same time

00:30:56,080 --> 00:30:59,279
now of course because of demo

00:30:57,519 --> 00:30:59,919
proposition we cannot wait everything

00:30:59,279 --> 00:31:02,799
for that but

00:30:59,919 --> 00:31:04,480
actually you can see that i'm executing

00:31:02,799 --> 00:31:06,320
evaluating other modules and depending

00:31:04,480 --> 00:31:08,720
on the results i'm basically

00:31:06,320 --> 00:31:09,360
moving from one to another and go you

00:31:08,720 --> 00:31:11,919
see actually

00:31:09,360 --> 00:31:12,559
the pipeline is completely different now

00:31:11,919 --> 00:31:14,880
you can

00:31:12,559 --> 00:31:17,279
probably point out yes but how hard it

00:31:14,880 --> 00:31:20,799
is to build that

00:31:17,279 --> 00:31:22,799
the composer showed me the graphical

00:31:20,799 --> 00:31:23,919
capability to design the pipe i can

00:31:22,799 --> 00:31:27,200
visualize my

00:31:23,919 --> 00:31:29,279
my pipeline up front the templates

00:31:27,200 --> 00:31:30,559
allow me to go to the extreme this is

00:31:29,279 --> 00:31:33,120
for the levels

00:31:30,559 --> 00:31:33,600
so let me download my template and open

00:31:33,120 --> 00:31:36,000
it up

00:31:33,600 --> 00:31:36,000
for you

00:31:38,240 --> 00:31:43,519
there you go and then just do something

00:31:41,760 --> 00:31:46,159
like that

00:31:43,519 --> 00:31:46,640
it's mean and another zoom in just to

00:31:46,159 --> 00:31:49,600
give you

00:31:46,640 --> 00:31:51,039
a little bit more redness capabilities

00:31:49,600 --> 00:31:53,840
and i'm going to take this

00:31:51,039 --> 00:31:56,159
so you will see that it's pretty similar

00:31:53,840 --> 00:31:59,120
to kfp for those that knows kfb

00:31:56,159 --> 00:32:01,360
but also to m charts for example we're

00:31:59,120 --> 00:32:02,159
using variables within that so you don't

00:32:01,360 --> 00:32:05,279
have to write them

00:32:02,159 --> 00:32:07,200
the volume size the storage class even

00:32:05,279 --> 00:32:09,519
the container name

00:32:07,200 --> 00:32:11,279
the container name image are all

00:32:09,519 --> 00:32:12,320
variables you pass directly from the

00:32:11,279 --> 00:32:14,080
orchestrator

00:32:12,320 --> 00:32:15,600
we will by inheritance we will take them

00:32:14,080 --> 00:32:16,640
so you don't have to rewrite them all

00:32:15,600 --> 00:32:19,360
you have to care

00:32:16,640 --> 00:32:21,039
is just passing the right command and

00:32:19,360 --> 00:32:23,760
the command is split

00:32:21,039 --> 00:32:25,200
pretty quickly here so you can basically

00:32:23,760 --> 00:32:27,679
just

00:32:25,200 --> 00:32:28,799
do the command you just give a name to

00:32:27,679 --> 00:32:30,799
the step

00:32:28,799 --> 00:32:32,080
and they decide when the stepping runs

00:32:30,799 --> 00:32:34,880
after what

00:32:32,080 --> 00:32:37,440
that's it you learn once you reuse

00:32:34,880 --> 00:32:38,559
multiples on any kind of platform

00:32:37,440 --> 00:32:40,799
and that's the beauty of the

00:32:38,559 --> 00:32:42,960
orchestrator once it's done

00:32:40,799 --> 00:32:43,840
you basically just upload it here as

00:32:42,960 --> 00:32:46,720
before

00:32:43,840 --> 00:32:48,880
and then you immediately apply to your

00:32:46,720 --> 00:32:51,760
pipeline

00:32:48,880 --> 00:32:52,640
and with that basically i'm finishing my

00:32:51,760 --> 00:32:54,880
two demos

00:32:52,640 --> 00:32:56,240
the goals of them was definitely where

00:32:54,880 --> 00:32:59,120
uh really

00:32:56,240 --> 00:33:00,080
nice with me to today so let me come

00:32:59,120 --> 00:33:03,200
back to them

00:33:00,080 --> 00:33:04,159
so back to katrina so orchestrator is

00:33:03,200 --> 00:33:05,840
mint

00:33:04,159 --> 00:33:07,840
and we release the end of november as

00:33:05,840 --> 00:33:09,840
open source project is meant

00:33:07,840 --> 00:33:11,360
to help data scientists and devops to

00:33:09,840 --> 00:33:13,360
collaborate

00:33:11,360 --> 00:33:15,039
and to orchestrate the pipeline of

00:33:13,360 --> 00:33:16,000
multiple environment no matter where

00:33:15,039 --> 00:33:18,399
those environments

00:33:16,000 --> 00:33:19,039
are and no matter which kind of

00:33:18,399 --> 00:33:20,880
environment

00:33:19,039 --> 00:33:22,880
are this way you can optimize the

00:33:20,880 --> 00:33:24,559
workloads you can optimize

00:33:22,880 --> 00:33:26,159
the way you run the pipe and you're sure

00:33:24,559 --> 00:33:27,919
that whenever you run it it runs

00:33:26,159 --> 00:33:29,360
successfully because it's pre-built for

00:33:27,919 --> 00:33:30,880
you

00:33:29,360 --> 00:33:32,559
of course you can create your custom

00:33:30,880 --> 00:33:34,799
templates so you have a degree of

00:33:32,559 --> 00:33:38,480
customization and flexibility there

00:33:34,799 --> 00:33:40,720
k3i is edge infrastructure for ai

00:33:38,480 --> 00:33:42,640
the goal of k3i that is already being

00:33:40,720 --> 00:33:45,760
documented on the queue flow

00:33:42,640 --> 00:33:47,200
community documentation for example is

00:33:45,760 --> 00:33:51,200
to make the life

00:33:47,200 --> 00:33:53,519
of ai citizens oii professional

00:33:51,200 --> 00:33:54,880
easier when it comes to experiment and

00:33:53,519 --> 00:33:56,880
start with ai tools

00:33:54,880 --> 00:34:00,640
so as you can see we already have a good

00:33:56,880 --> 00:34:03,679
list of things we are working there

00:34:00,640 --> 00:34:05,200
those k3i dot in allow you to start

00:34:03,679 --> 00:34:07,919
immediately you can install queue flow

00:34:05,200 --> 00:34:10,399
pipelines with with cpus with gpus

00:34:07,919 --> 00:34:11,679
argo or floss is already um there's a

00:34:10,399 --> 00:34:13,040
type which is not anymore working

00:34:11,679 --> 00:34:14,879
progress argo warfare is already

00:34:13,040 --> 00:34:15,919
supported tensorflow serving is already

00:34:14,879 --> 00:34:18,639
supported

00:34:15,919 --> 00:34:20,480
we got wsl support so for those that are

00:34:18,639 --> 00:34:22,240
on windows they can literally use

00:34:20,480 --> 00:34:23,280
windows assistant for linux to install

00:34:22,240 --> 00:34:25,760
everything

00:34:23,280 --> 00:34:27,119
all the jupiter stacks are there and we

00:34:25,760 --> 00:34:30,240
are looking for contributors

00:34:27,119 --> 00:34:32,720
is a open source project you can go

00:34:30,240 --> 00:34:34,480
there consume it or if you like you can

00:34:32,720 --> 00:34:35,919
contribute there we are going to add

00:34:34,480 --> 00:34:38,240
python pytorch

00:34:35,919 --> 00:34:39,839
serving as well in terms of serving um

00:34:38,240 --> 00:34:42,879
as well as said

00:34:39,839 --> 00:34:45,200
so k2i means you can install it on

00:34:42,879 --> 00:34:46,560
everything literally everything we even

00:34:45,200 --> 00:34:48,560
support arm

00:34:46,560 --> 00:34:50,639
because it's based on well-known ranger

00:34:48,560 --> 00:34:52,639
caterers behind the scenes

00:34:50,639 --> 00:34:54,480
and in fact we are going to release also

00:34:52,639 --> 00:34:57,440
are the forearm

00:34:54,480 --> 00:34:59,599
on that and literally it's at the

00:34:57,440 --> 00:35:01,040
distance of a one single command the one

00:34:59,599 --> 00:35:02,800
that is involved here

00:35:01,040 --> 00:35:04,160
you can literally see that in one single

00:35:02,800 --> 00:35:05,839
command you don't have to do

00:35:04,160 --> 00:35:07,359
anything you don't have to care about

00:35:05,839 --> 00:35:09,040
anything we will take care of all

00:35:07,359 --> 00:35:11,440
complexity you just need to

00:35:09,040 --> 00:35:13,359
start and play with the tools and

00:35:11,440 --> 00:35:14,800
combining those two things

00:35:13,359 --> 00:35:16,800
it comes to the idea that jeff was

00:35:14,800 --> 00:35:19,920
describing before the eye stack

00:35:16,800 --> 00:35:22,000
and the eye stack is exactly that is the

00:35:19,920 --> 00:35:23,680
let's say the enterprise version of this

00:35:22,000 --> 00:35:26,720
is about

00:35:23,680 --> 00:35:29,599
making complex things easier

00:35:26,720 --> 00:35:32,800
faster and ready to be run at your

00:35:29,599 --> 00:35:35,520
disposal at the distance of a click

00:35:32,800 --> 00:35:36,640
and in fact the istek is made of an

00:35:35,520 --> 00:35:38,240
operative system

00:35:36,640 --> 00:35:39,760
optimizing for the accelerator

00:35:38,240 --> 00:35:43,119
architecture underneath

00:35:39,760 --> 00:35:46,320
for the platforms underneath x86 arm

00:35:43,119 --> 00:35:48,079
with storages that that are fit for

00:35:46,320 --> 00:35:49,760
those accelerators that

00:35:48,079 --> 00:35:52,160
not to be integrated with this operating

00:35:49,760 --> 00:35:54,800
system like suse enterprise storage

00:35:52,160 --> 00:35:55,280
with orchestrator you probably all know

00:35:54,800 --> 00:35:58,480
about

00:35:55,280 --> 00:36:00,160
susan rancher and we cannot actually

00:35:58,480 --> 00:36:00,480
claim anything because it's not complete

00:36:00,160 --> 00:36:01,920
but

00:36:00,480 --> 00:36:03,599
to give you an idea even though the

00:36:01,920 --> 00:36:05,359
flexibility

00:36:03,599 --> 00:36:08,320
everything you see here can come from

00:36:05,359 --> 00:36:10,560
souza but can even come from third party

00:36:08,320 --> 00:36:11,839
you can have the flexibility there's no

00:36:10,560 --> 00:36:13,839
vendor looking you don't like the

00:36:11,839 --> 00:36:14,720
storage from suse you can add any other

00:36:13,839 --> 00:36:16,240
storage

00:36:14,720 --> 00:36:18,400
we will take care just of the

00:36:16,240 --> 00:36:19,040
combination between us and partners we

00:36:18,400 --> 00:36:20,720
have

00:36:19,040 --> 00:36:23,040
to have a stack that's still working the

00:36:20,720 --> 00:36:24,800
orchestrator fit perfectly on the

00:36:23,040 --> 00:36:26,800
infrastructure orchestrator

00:36:24,800 --> 00:36:28,480
in a matter that actually allow you to

00:36:26,800 --> 00:36:31,680
do move to the next

00:36:28,480 --> 00:36:34,240
layer that is the platforms q flow r4

00:36:31,680 --> 00:36:34,960
argo and so on k2 i eventually could be

00:36:34,240 --> 00:36:36,880
part of it

00:36:34,960 --> 00:36:38,079
and on top of that you can use any

00:36:36,880 --> 00:36:41,040
container

00:36:38,079 --> 00:36:41,920
that run your own pipeline so basically

00:36:41,040 --> 00:36:44,800
you have a

00:36:41,920 --> 00:36:45,599
full seamless infracture in one single

00:36:44,800 --> 00:36:48,480
stack

00:36:45,599 --> 00:36:48,880
fully supported end-to-end by susan and

00:36:48,480 --> 00:36:50,400
still

00:36:48,880 --> 00:36:52,320
completely compliant with the open

00:36:50,400 --> 00:36:56,720
source um

00:36:52,320 --> 00:36:59,839
dna and policies so

00:36:56,720 --> 00:37:02,000
the procedure ml development means that

00:36:59,839 --> 00:37:03,280
and we just demonstrated that it should

00:37:02,000 --> 00:37:06,240
be fast enough for me

00:37:03,280 --> 00:37:07,680
to experiment to use templates optimized

00:37:06,240 --> 00:37:10,240
for specific environments

00:37:07,680 --> 00:37:10,960
like gpu accelerators and customize it

00:37:10,240 --> 00:37:12,640
for my need

00:37:10,960 --> 00:37:14,800
even when it comes to data center and

00:37:12,640 --> 00:37:17,359
when you come in hybrid scenario between

00:37:14,800 --> 00:37:19,119
on-prem and cloud it should be able to

00:37:17,359 --> 00:37:20,640
be configured for multiple environments

00:37:19,119 --> 00:37:22,400
for the same pipelines as we

00:37:20,640 --> 00:37:25,200
demonstrated before

00:37:22,400 --> 00:37:26,720
maybe or better i should have if i want

00:37:25,200 --> 00:37:29,280
to run an eye in production

00:37:26,720 --> 00:37:30,800
that test production but i don't want to

00:37:29,280 --> 00:37:32,560
rewrite every time in the pipe i don't

00:37:30,800 --> 00:37:33,280
want to re-optimize the pipeline every

00:37:32,560 --> 00:37:35,839
time

00:37:33,280 --> 00:37:37,599
completely writing again every single

00:37:35,839 --> 00:37:39,599
step i want to just reuse i don't

00:37:37,599 --> 00:37:41,280
i want to avoid all the manual steps

00:37:39,599 --> 00:37:44,079
there that takes

00:37:41,280 --> 00:37:45,119
a lot of time with longer time at least

00:37:44,079 --> 00:37:48,079
and

00:37:45,119 --> 00:37:50,240
they expose me to human errors it should

00:37:48,079 --> 00:37:52,160
be pre bb automation workflows

00:37:50,240 --> 00:37:54,560
because it can it removes the complexity

00:37:52,160 --> 00:37:56,480
but also it allows me to even

00:37:54,560 --> 00:37:58,400
have a degree of automation that

00:37:56,480 --> 00:38:03,119
otherwise i cannot reach out

00:37:58,400 --> 00:38:03,119
i said we are leveraging the

00:38:03,280 --> 00:38:07,760
the capability to use kit repos but it's

00:38:05,920 --> 00:38:11,200
not just that

00:38:07,760 --> 00:38:11,839
when you use orchestrator i show you the

00:38:11,200 --> 00:38:15,280
ui

00:38:11,839 --> 00:38:18,960
this amazing ui made by the our ux team

00:38:15,280 --> 00:38:21,359
and i hope you perceive the simplicity

00:38:18,960 --> 00:38:23,359
of this usage it was so simple that you

00:38:21,359 --> 00:38:26,640
don't have to learn everything was

00:38:23,359 --> 00:38:29,839
clear and intuitive but it does

00:38:26,640 --> 00:38:32,079
an amazing job by our ux team to do that

00:38:29,839 --> 00:38:33,200
but for each functionality you saw

00:38:32,079 --> 00:38:38,000
there's an equivalent

00:38:33,200 --> 00:38:41,200
api we design everything in ux api first

00:38:38,000 --> 00:38:42,800
one ui functionality one api one to one

00:38:41,200 --> 00:38:44,320
that means that you can literally embed

00:38:42,800 --> 00:38:47,599
the orchestrator within

00:38:44,320 --> 00:38:50,000
any platform any application you have

00:38:47,599 --> 00:38:51,200
any infrastructure you have and make it

00:38:50,000 --> 00:38:52,880
completely transparent

00:38:51,200 --> 00:38:54,640
transparent let's say you're using i

00:38:52,880 --> 00:38:57,839
don't know azure devops

00:38:54,640 --> 00:38:59,520
or you're using jira or github actions

00:38:57,839 --> 00:39:01,040
you can literally call the api from the

00:38:59,520 --> 00:39:03,040
orchestrator and orchestrate the

00:39:01,040 --> 00:39:05,920
pipeline diet from there

00:39:03,040 --> 00:39:06,400
the uh this is the output pilot guidance

00:39:05,920 --> 00:39:08,079
meaning

00:39:06,400 --> 00:39:09,680
to drive the operator to the entire

00:39:08,079 --> 00:39:11,520
deployment experience

00:39:09,680 --> 00:39:13,359
and overall we try to reduce the

00:39:11,520 --> 00:39:17,440
frictional separator project

00:39:13,359 --> 00:39:20,000
we test the maintenance solution so

00:39:17,440 --> 00:39:21,920
that was all i got more or less from my

00:39:20,000 --> 00:39:23,119
side and i ask you jeff if you want to

00:39:21,920 --> 00:39:24,720
chime in for that

00:39:23,119 --> 00:39:26,960
if you have any question feel free to

00:39:24,720 --> 00:39:30,320
reach me out i hope you perceive

00:39:26,960 --> 00:39:32,400
there there's more and more about ai um

00:39:30,320 --> 00:39:33,599
for example we as soon as we release and

00:39:32,400 --> 00:39:36,880
maintain a

00:39:33,599 --> 00:39:38,720
a good list of packages um

00:39:36,880 --> 00:39:41,040
for the machine learning frameworks

00:39:38,720 --> 00:39:43,760
means if you're running on

00:39:41,040 --> 00:39:44,800
is suse and you use to install packages

00:39:43,760 --> 00:39:47,839
a command line called

00:39:44,800 --> 00:39:48,800
zipper zipper installed something so we

00:39:47,839 --> 00:39:50,160
made it easier

00:39:48,800 --> 00:39:52,000
to install also the machine learning

00:39:50,160 --> 00:39:52,960
framework zipper in tensorflow deeper in

00:39:52,000 --> 00:39:55,920
pytorch

00:39:52,960 --> 00:39:56,800
so you don't have to care about thinking

00:39:55,920 --> 00:39:59,359
how to do so

00:39:56,800 --> 00:40:00,560
even on susan opensuse if you want to

00:39:59,359 --> 00:40:04,400
try out

00:40:00,560 --> 00:40:05,520
um our high performance computing

00:40:04,400 --> 00:40:07,680
solution

00:40:05,520 --> 00:40:10,000
we have we can use those packages within

00:40:07,680 --> 00:40:13,040
that and there's even more and more

00:40:10,000 --> 00:40:15,760
gpu support and so on and of course we

00:40:13,040 --> 00:40:18,640
also contributed upstream to queue flow

00:40:15,760 --> 00:40:21,280
so for example we release a manifesto

00:40:18,640 --> 00:40:23,680
queue flow to support non-docker runtime

00:40:21,280 --> 00:40:25,599
like the pns runtimes and we added more

00:40:23,680 --> 00:40:27,760
and more catria is another addition to

00:40:25,599 --> 00:40:31,119
the q flow documentation to help out

00:40:27,760 --> 00:40:33,040
ai faults to to start with and with that

00:40:31,119 --> 00:40:34,240
really and alessandro just want to

00:40:33,040 --> 00:40:36,560
interject so

00:40:34,240 --> 00:40:38,079
everything you showed and everything we

00:40:36,560 --> 00:40:41,280
we both talked about

00:40:38,079 --> 00:40:44,079
it's really to accelerate the ai

00:40:41,280 --> 00:40:44,640
project making it easier to get that

00:40:44,079 --> 00:40:47,280
project

00:40:44,640 --> 00:40:48,480
into production where we can really

00:40:47,280 --> 00:40:51,359
start to get

00:40:48,480 --> 00:40:53,280
the insights and the feedback and things

00:40:51,359 --> 00:40:54,160
that will affect the businesses so

00:40:53,280 --> 00:40:55,680
that's

00:40:54,160 --> 00:40:57,920
that's what this is really all about

00:40:55,680 --> 00:41:00,319
orchestrating it automating it

00:40:57,920 --> 00:41:00,960
and and getting to production quicker

00:41:00,319 --> 00:41:02,880
yeah and

00:41:00,960 --> 00:41:04,640
you're absolutely right jeff because the

00:41:02,880 --> 00:41:07,839
the meaning is exciting that is

00:41:04,640 --> 00:41:09,920
making life simpler up to the point

00:41:07,839 --> 00:41:11,760
where you don't perceive the complexity

00:41:09,920 --> 00:41:13,200
and i hope that for example in my demo

00:41:11,760 --> 00:41:16,240
that was perceived

00:41:13,200 --> 00:41:18,800
i basically literally drive the demo

00:41:16,240 --> 00:41:19,520
click from one point another get results

00:41:18,800 --> 00:41:22,000
without

00:41:19,520 --> 00:41:22,800
any any any complexity any friction

00:41:22,000 --> 00:41:25,359
there

00:41:22,800 --> 00:41:27,520
and so i hope that folks that actually

00:41:25,359 --> 00:41:29,599
are looking into the presentation have

00:41:27,520 --> 00:41:30,800
questions and or eventually even

00:41:29,599 --> 00:41:32,560
feedback because we are

00:41:30,800 --> 00:41:34,720
definitely open for that that that's

00:41:32,560 --> 00:41:36,720
basically the souza way

00:41:34,720 --> 00:41:38,319
to be open and because that's what we

00:41:36,720 --> 00:41:46,400
claim we are the open open

00:41:38,319 --> 00:41:46,400

YouTube URL: https://www.youtube.com/watch?v=1isURnUkYGY


