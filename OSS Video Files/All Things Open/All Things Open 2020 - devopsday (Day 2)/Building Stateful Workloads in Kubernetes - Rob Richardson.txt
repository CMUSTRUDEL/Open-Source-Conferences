Title: Building Stateful Workloads in Kubernetes - Rob Richardson
Publication date: 2020-12-18
Playlist: All Things Open 2020 - devopsday (Day 2)
Description: 
	Presented by: Rob Richardson
Presented at All Things Open 2020 - devopsday (Day 2)

Abstract: It's day 2. Kubernetes is running. You have your deployments and services set. Now how do you migrate the data store? Let's journey together on this code-focused tour through ConfigMaps, Secrets, Persistent Volumes, Persistent Volume Claims, and StatefulSets. We'll craft and launch a strategy to care for your users' data in this new container world. You can power your business on Kubernetes: stateless or stateful.
Captions: 
	00:00:05,520 --> 00:00:08,320
building stateful workloads and

00:00:07,040 --> 00:00:10,320
kubernetes

00:00:08,320 --> 00:00:11,599
this is going to be so much fun here's

00:00:10,320 --> 00:00:12,960
the part where i tell you i'm definitely

00:00:11,599 --> 00:00:13,759
going to post the slides on my site

00:00:12,960 --> 00:00:15,839
tonight

00:00:13,759 --> 00:00:17,840
and you're going to hit up chat and

00:00:15,839 --> 00:00:18,960
you're gonna look at my site and you're

00:00:17,840 --> 00:00:22,080
gonna

00:00:18,960 --> 00:00:22,800
refresh tomorrow and next week and next

00:00:22,080 --> 00:00:25,119
month

00:00:22,800 --> 00:00:26,640
and in about three months you're gonna

00:00:25,119 --> 00:00:28,320
get tired of pushing refresh and you'll

00:00:26,640 --> 00:00:29,679
email me and i'll promptly reply in six

00:00:28,320 --> 00:00:31,679
months and say no i'm definitely gonna

00:00:29,679 --> 00:00:33,440
post them on my site tonight

00:00:31,679 --> 00:00:35,440
and i'll never post them at all i've

00:00:33,440 --> 00:00:36,480
been that guy i've chased the speaker

00:00:35,440 --> 00:00:38,559
for six months

00:00:36,480 --> 00:00:40,960
and that's no fun which is why you can

00:00:38,559 --> 00:00:43,760
go to robrich.org right now

00:00:40,960 --> 00:00:44,640
and you can click on presentations and

00:00:43,760 --> 00:00:46,239
you'll get to hear

00:00:44,640 --> 00:00:48,079
here's building stateful workloads and

00:00:46,239 --> 00:00:49,039
kubernetes you can get to the slides

00:00:48,079 --> 00:00:51,840
there online

00:00:49,039 --> 00:00:54,399
right now on robrich.org achievement

00:00:51,840 --> 00:00:56,079
unlocked

00:00:54,399 --> 00:00:57,680
while you're here on robrich.org let's

00:00:56,079 --> 00:00:58,000
click on about me and talk about some of

00:00:57,680 --> 00:01:00,239
the things

00:00:58,000 --> 00:01:02,800
that i've done recently i'm a microsoft

00:01:00,239 --> 00:01:04,960
mvp i'm a friend of redgate

00:01:02,800 --> 00:01:06,400
azgive camp is really cool az gift camp

00:01:04,960 --> 00:01:07,920
brings volunteer developers

00:01:06,400 --> 00:01:09,520
together with charities who otherwise

00:01:07,920 --> 00:01:10,960
couldn't afford software services

00:01:09,520 --> 00:01:13,040
we start building software for them

00:01:10,960 --> 00:01:14,720
friday after work sunday afternoon

00:01:13,040 --> 00:01:16,240
we deliver that completed software back

00:01:14,720 --> 00:01:17,759
to the charities sleep is optional

00:01:16,240 --> 00:01:19,200
caffeine provided

00:01:17,759 --> 00:01:20,960
if you're in phoenix come join us for

00:01:19,200 --> 00:01:22,240
the next a-z gift camp or if you'd like

00:01:20,960 --> 00:01:24,400
a gift camp in your area

00:01:22,240 --> 00:01:25,759
hit me up by twitter or email and let's

00:01:24,400 --> 00:01:27,360
get philanthropy installed in your

00:01:25,759 --> 00:01:28,799
neighborhood as well

00:01:27,360 --> 00:01:31,439
some of the other things that i've done

00:01:28,799 --> 00:01:32,960
um uh sql source control basics minus

00:01:31,439 --> 00:01:33,520
chapter eight that was really fun to

00:01:32,960 --> 00:01:35,520
build

00:01:33,520 --> 00:01:36,799
i worked on the gulp team in version two

00:01:35,520 --> 00:01:38,159
and version three

00:01:36,799 --> 00:01:39,759
and one of the things i'm particularly

00:01:38,159 --> 00:01:41,280
proud of is i replied to a dot-net

00:01:39,759 --> 00:01:42,960
rock's podcast episode

00:01:41,280 --> 00:01:45,280
they read my comment on the air and they

00:01:42,960 --> 00:01:47,280
sent me a mug

00:01:45,280 --> 00:01:50,399
so there's my claim to fame my

00:01:47,280 --> 00:01:51,520
coveted.net rocks mug

00:01:50,399 --> 00:01:52,840
okay so we're going to talk about

00:01:51,520 --> 00:01:54,399
building stateful workloads in

00:01:52,840 --> 00:01:57,920
kubernetes

00:01:54,399 --> 00:02:00,880
this is cool we talked about this guy

00:01:57,920 --> 00:02:01,280
stateless kubernetes so this is the easy

00:02:00,880 --> 00:02:02,799
part

00:02:01,280 --> 00:02:05,119
this is the things that we've already

00:02:02,799 --> 00:02:06,960
got stateless kubernetes

00:02:05,119 --> 00:02:08,800
users come in they hit our ingress

00:02:06,960 --> 00:02:09,759
controller they go from there to a

00:02:08,800 --> 00:02:11,440
service

00:02:09,759 --> 00:02:13,840
a deployment will spin up a bunch of

00:02:11,440 --> 00:02:17,120
pods and each pod has a container

00:02:13,840 --> 00:02:18,640
we got this well maybe if we don't then

00:02:17,120 --> 00:02:20,560
definitely hit me up by twitter and

00:02:18,640 --> 00:02:22,640
let's dig in some more because

00:02:20,560 --> 00:02:24,640
yeah this is kind of the stateless

00:02:22,640 --> 00:02:27,840
workload that gives us

00:02:24,640 --> 00:02:31,440
really easy mechanisms into containers

00:02:27,840 --> 00:02:34,879
but um our site kind of looks like this

00:02:31,440 --> 00:02:37,360
it has more pieces going on so we

00:02:34,879 --> 00:02:38,720
we have the same user to ingress to

00:02:37,360 --> 00:02:41,280
service

00:02:38,720 --> 00:02:42,800
but now the pods need to reach out to

00:02:41,280 --> 00:02:44,319
other things maybe they need to reach

00:02:42,800 --> 00:02:46,879
out to file systems

00:02:44,319 --> 00:02:48,720
to store files or maybe they need to

00:02:46,879 --> 00:02:50,400
reach out to a database cluster

00:02:48,720 --> 00:02:52,400
and we need multiple machines in that

00:02:50,400 --> 00:02:55,280
database cluster to be able to

00:02:52,400 --> 00:02:56,080
keep high availability in that system we

00:02:55,280 --> 00:02:59,120
may have

00:02:56,080 --> 00:03:02,159
in our ingress https certificates

00:02:59,120 --> 00:03:02,959
we've got more pieces involved in this

00:03:02,159 --> 00:03:05,840
system

00:03:02,959 --> 00:03:07,040
than just ingress service pods and

00:03:05,840 --> 00:03:10,000
containers

00:03:07,040 --> 00:03:11,440
so how do we add state how do we get the

00:03:10,000 --> 00:03:15,200
rest of our application

00:03:11,440 --> 00:03:19,200
into kubernetes well first

00:03:15,200 --> 00:03:22,400
let's talk about state what is state

00:03:19,200 --> 00:03:23,840
well here's state

00:03:22,400 --> 00:03:25,440
yeah we're not going to talk about those

00:03:23,840 --> 00:03:27,040
states we're going to talk about this

00:03:25,440 --> 00:03:30,080
state

00:03:27,040 --> 00:03:31,280
state is persisting things beyond the

00:03:30,080 --> 00:03:32,879
current lifetime

00:03:31,280 --> 00:03:35,120
now what is the current lifetime well it

00:03:32,879 --> 00:03:36,480
kind of depends is the current lifetime

00:03:35,120 --> 00:03:39,680
a function call

00:03:36,480 --> 00:03:41,200
if we have stateful resources outside of

00:03:39,680 --> 00:03:43,360
a function call we'll probably put

00:03:41,200 --> 00:03:45,920
properties in our classes

00:03:43,360 --> 00:03:46,560
well how do we do stateful resources

00:03:45,920 --> 00:03:48,640
across

00:03:46,560 --> 00:03:50,879
a request and response we'll probably

00:03:48,640 --> 00:03:53,360
want to store something on disk

00:03:50,879 --> 00:03:54,959
well what about across a server restart

00:03:53,360 --> 00:03:57,599
or across a load balancer

00:03:54,959 --> 00:03:59,439
if we have many machines well then maybe

00:03:57,599 --> 00:04:02,400
that machine isn't durable maybe we need

00:03:59,439 --> 00:04:05,840
to store it somewhere off the machine

00:04:02,400 --> 00:04:07,680
state that piece that we want to persist

00:04:05,840 --> 00:04:09,040
beyond the current lifetime

00:04:07,680 --> 00:04:10,720
that's what we need to get into

00:04:09,040 --> 00:04:14,000
kubernetes how do we get that

00:04:10,720 --> 00:04:17,120
into a largely stateless

00:04:14,000 --> 00:04:19,359
environment let's dig in

00:04:17,120 --> 00:04:20,720
so we'll look at various types of state

00:04:19,359 --> 00:04:22,560
as we get going here

00:04:20,720 --> 00:04:24,080
we're going to look at configuration

00:04:22,560 --> 00:04:27,280
secrets data stores

00:04:24,080 --> 00:04:28,400
file stores singleton services singleton

00:04:27,280 --> 00:04:30,320
services

00:04:28,400 --> 00:04:31,440
what do we mean by that well a singleton

00:04:30,320 --> 00:04:35,440
service is

00:04:31,440 --> 00:04:37,840
well that thing that needs

00:04:35,440 --> 00:04:39,360
to have all of the system to make

00:04:37,840 --> 00:04:41,199
calculations

00:04:39,360 --> 00:04:42,880
a database is a great example when we're

00:04:41,199 --> 00:04:45,199
incrementing an auto

00:04:42,880 --> 00:04:46,960
incrementing primary key there needs to

00:04:45,199 --> 00:04:49,360
be exactly one thing

00:04:46,960 --> 00:04:52,000
building that auto increment if there's

00:04:49,360 --> 00:04:54,000
two or three or five places that

00:04:52,000 --> 00:04:56,160
are auto incrementing then we'll get

00:04:54,000 --> 00:04:58,720
collisions or we'll get corruption or

00:04:56,160 --> 00:05:00,720
we'll get duplicate entries

00:04:58,720 --> 00:05:02,160
we could also think of it maybe as like

00:05:00,720 --> 00:05:03,919
the month end report

00:05:02,160 --> 00:05:06,720
the month end report needs to be able to

00:05:03,919 --> 00:05:08,560
see the entire system to calculate

00:05:06,720 --> 00:05:10,080
quantities purchased in that month for

00:05:08,560 --> 00:05:12,080
each customer

00:05:10,080 --> 00:05:13,919
now maybe we can start to shard that

00:05:12,080 --> 00:05:16,400
workload up a little bit but

00:05:13,919 --> 00:05:17,360
for the most part it's those pieces

00:05:16,400 --> 00:05:19,919
where it needs to

00:05:17,360 --> 00:05:20,880
look holistically at the system to be

00:05:19,919 --> 00:05:24,160
able to make

00:05:20,880 --> 00:05:26,160
decisions these singleton services

00:05:24,160 --> 00:05:27,680
we need to place in context with our

00:05:26,160 --> 00:05:31,199
other stateful resources

00:05:27,680 --> 00:05:33,120
inside of kubernetes configurations

00:05:31,199 --> 00:05:34,479
secrets data stores file stores

00:05:33,120 --> 00:05:36,800
let's take a look at each one and

00:05:34,479 --> 00:05:39,840
understand how we can do this

00:05:36,800 --> 00:05:39,840
inside kubernetes

00:05:40,880 --> 00:05:45,039
first step cube ctl get all cube ctl get

00:05:44,720 --> 00:05:48,400
all

00:05:45,039 --> 00:05:51,520
is supposed to return everything but

00:05:48,400 --> 00:05:52,240
it kind of doesn't it returns all of the

00:05:51,520 --> 00:05:54,800
stateless

00:05:52,240 --> 00:05:57,280
things and this is an intentional design

00:05:54,800 --> 00:05:59,039
that it only returns stateless resources

00:05:57,280 --> 00:06:00,800
because they don't want to accidentally

00:05:59,039 --> 00:06:02,800
expose you to something that you could

00:06:00,800 --> 00:06:03,919
delete and not be able to recreate

00:06:02,800 --> 00:06:05,919
easily

00:06:03,919 --> 00:06:07,199
so it doesn't show you stateful

00:06:05,919 --> 00:06:08,800
resources

00:06:07,199 --> 00:06:10,800
now here's a couple of issues as you

00:06:08,800 --> 00:06:12,560
grab these slides from robrich.org click

00:06:10,800 --> 00:06:13,919
through to each of these issues the blue

00:06:12,560 --> 00:06:15,919
links and let's take a look

00:06:13,919 --> 00:06:17,120
at the different ways that they've

00:06:15,919 --> 00:06:20,240
chosen to

00:06:17,120 --> 00:06:22,560
do this and with each mechanism saying

00:06:20,240 --> 00:06:25,120
cube ctl get all doesn't return all

00:06:22,560 --> 00:06:27,520
they're like yeah sorry and they're

00:06:25,120 --> 00:06:29,520
working towards deprecating cube ctl git

00:06:27,520 --> 00:06:30,000
all for exactly that reason because it's

00:06:29,520 --> 00:06:33,600
not

00:06:30,000 --> 00:06:36,160
all it's all the stateless things

00:06:33,600 --> 00:06:36,800
so how do we work around this well cube

00:06:36,160 --> 00:06:40,240
ctl

00:06:36,800 --> 00:06:41,680
api resources will return all of the

00:06:40,240 --> 00:06:43,759
resources

00:06:41,680 --> 00:06:45,600
and it also includes custom resource

00:06:43,759 --> 00:06:46,800
definitions that you may have created

00:06:45,600 --> 00:06:50,479
inside your cluster

00:06:46,800 --> 00:06:53,199
it is indeed truly everything

00:06:50,479 --> 00:06:53,599
so we can do this command and again grab

00:06:53,199 --> 00:06:55,440
the

00:06:53,599 --> 00:06:57,039
slides from robrich.org click through to

00:06:55,440 --> 00:06:59,360
this comment where you see

00:06:57,039 --> 00:07:02,240
this script as it builds it where it

00:06:59,360 --> 00:07:05,280
goes and grabs cube ctl api resources

00:07:02,240 --> 00:07:08,720
uses awk to pull out only the name

00:07:05,280 --> 00:07:12,000
piece and then pipes that

00:07:08,720 --> 00:07:12,639
into a comma separated list and uses

00:07:12,000 --> 00:07:16,080
that

00:07:12,639 --> 00:07:18,479
for cube ctl ghetto and so you may

00:07:16,080 --> 00:07:20,319
choose to alias this in bash or in your

00:07:18,479 --> 00:07:21,199
shell of choice so that you can do a

00:07:20,319 --> 00:07:24,400
cube ctl

00:07:21,199 --> 00:07:26,240
get all for real or

00:07:24,400 --> 00:07:27,919
cube all or whatever you want to call

00:07:26,240 --> 00:07:31,199
your alias that will go get

00:07:27,919 --> 00:07:33,280
all the resources and all the things

00:07:31,199 --> 00:07:35,199
you can also specify each one so cube

00:07:33,280 --> 00:07:37,680
ctl get

00:07:35,199 --> 00:07:39,840
pod comma deployment and that will

00:07:37,680 --> 00:07:42,000
definitely return all of the

00:07:39,840 --> 00:07:45,280
things in that category so as we're

00:07:42,000 --> 00:07:49,039
looking into we can cube ctl get

00:07:45,280 --> 00:07:49,680
secrets okay so many of the things we

00:07:49,039 --> 00:07:51,680
see today

00:07:49,680 --> 00:07:54,720
aren't in cube ctl get all and

00:07:51,680 --> 00:07:57,840
unfortunately that is by design

00:07:54,720 --> 00:07:59,039
first step volumes volumes is a great

00:07:57,840 --> 00:08:02,400
place to store

00:07:59,039 --> 00:08:04,160
things on disk so

00:08:02,400 --> 00:08:06,879
it's great for storing things beyond a

00:08:04,160 --> 00:08:09,840
pod restart for sharing files between

00:08:06,879 --> 00:08:12,479
pods if they both share the same volume

00:08:09,840 --> 00:08:16,080
they can both get at the same resources

00:08:12,479 --> 00:08:19,199
which is really helpful those two pods

00:08:16,080 --> 00:08:21,280
now share one drive that

00:08:19,199 --> 00:08:22,479
does interesting things well the way

00:08:21,280 --> 00:08:23,840
it's implemented

00:08:22,479 --> 00:08:27,280
in spite of the fact that they may

00:08:23,840 --> 00:08:30,400
appear to be a local folder to each pod

00:08:27,280 --> 00:08:33,919
they are a network resource

00:08:30,400 --> 00:08:36,800
and so file locks don't work

00:08:33,919 --> 00:08:38,959
well one pod may consider that file

00:08:36,800 --> 00:08:40,880
locked and so it marks that locked in

00:08:38,959 --> 00:08:42,880
all of its places but the other pod has

00:08:40,880 --> 00:08:44,560
no idea that that file lock exists it's

00:08:42,880 --> 00:08:47,839
across a network share

00:08:44,560 --> 00:08:50,720
so it is possible for us accidentally

00:08:47,839 --> 00:08:53,279
to write to two different from two

00:08:50,720 --> 00:08:55,519
different pods to the same file

00:08:53,279 --> 00:08:57,519
and so create a naming convention create

00:08:55,519 --> 00:08:58,880
a mechanism so that you don't end up

00:08:57,519 --> 00:09:02,080
clobbering your data

00:08:58,880 --> 00:09:04,399
across two different pods

00:09:02,080 --> 00:09:06,240
okay so here's a pod definition and we

00:09:04,399 --> 00:09:07,600
may choose to put this pod definition

00:09:06,240 --> 00:09:09,519
inside of a deployment

00:09:07,600 --> 00:09:11,600
or we may just create a pod definition

00:09:09,519 --> 00:09:13,440
straight off now i've snipped out some

00:09:11,600 --> 00:09:16,080
of the interesting things where we might

00:09:13,440 --> 00:09:16,800
specify resource limits and health

00:09:16,080 --> 00:09:19,279
checks

00:09:16,800 --> 00:09:21,600
so once i've specified the major details

00:09:19,279 --> 00:09:24,720
i can specify volume mounts

00:09:21,600 --> 00:09:27,440
now similar to the way we mount docker

00:09:24,720 --> 00:09:28,000
volumes we'll specify the name of that

00:09:27,440 --> 00:09:30,480
volume

00:09:28,000 --> 00:09:32,640
and the path inside the container to

00:09:30,480 --> 00:09:34,720
that particular directory

00:09:32,640 --> 00:09:36,000
now that's the path in the container and

00:09:34,720 --> 00:09:37,839
the container then can just

00:09:36,000 --> 00:09:39,680
read and write files as if it was a

00:09:37,839 --> 00:09:41,680
regular folder

00:09:39,680 --> 00:09:42,720
but here's that name down here in the

00:09:41,680 --> 00:09:44,959
volume section

00:09:42,720 --> 00:09:46,000
that specifies where it actually exists

00:09:44,959 --> 00:09:47,839
on disk

00:09:46,000 --> 00:09:49,120
now in this case i've chosen hostpath

00:09:47,839 --> 00:09:51,440
which is a path

00:09:49,120 --> 00:09:52,959
on the host volume this isn't a great

00:09:51,440 --> 00:09:55,920
durable choice for

00:09:52,959 --> 00:09:58,000
a multi-node cluster because now it's on

00:09:55,920 --> 00:09:58,560
a particular node we'll probably want to

00:09:58,000 --> 00:10:01,760
use

00:09:58,560 --> 00:10:05,519
a samba nfs file share

00:10:01,760 --> 00:10:06,399
or blob storage like amazon s3 or azure

00:10:05,519 --> 00:10:10,160
blob storage

00:10:06,399 --> 00:10:13,360
and get it off of even the node hosting

00:10:10,160 --> 00:10:15,120
the pod but for testing

00:10:13,360 --> 00:10:17,200
this is great and so you can copy this

00:10:15,120 --> 00:10:18,160
you can set it in place and now you have

00:10:17,200 --> 00:10:20,959
a volume

00:10:18,160 --> 00:10:21,839
for this particular pod this pod can now

00:10:20,959 --> 00:10:25,839
save

00:10:21,839 --> 00:10:28,720
files more durably that's cool

00:10:25,839 --> 00:10:29,600
so file storage as we look at this

00:10:28,720 --> 00:10:33,120
definition

00:10:29,600 --> 00:10:35,760
we might ask ourselves well who owns

00:10:33,120 --> 00:10:38,399
this folder this folder is now outside

00:10:35,760 --> 00:10:39,680
the pod it's durable

00:10:38,399 --> 00:10:41,360
should the developer own it the

00:10:39,680 --> 00:10:43,200
developer created all the rest of the

00:10:41,360 --> 00:10:45,600
details around this

00:10:43,200 --> 00:10:46,959
pod definition they specified the image

00:10:45,600 --> 00:10:48,560
version they might have specified

00:10:46,959 --> 00:10:49,600
mechanisms for building it they're

00:10:48,560 --> 00:10:52,959
incrementing the

00:10:49,600 --> 00:10:56,399
uh version based on build definition um

00:10:52,959 --> 00:10:58,240
it might have labels like the git sha so

00:10:56,399 --> 00:11:00,079
if they're specifying everything else

00:10:58,240 --> 00:11:02,399
should they specify this

00:11:00,079 --> 00:11:04,640
or should operations specify this they

00:11:02,399 --> 00:11:07,519
probably have mechanisms for backing up

00:11:04,640 --> 00:11:07,839
folders in particular places they have

00:11:07,519 --> 00:11:11,120
the

00:11:07,839 --> 00:11:11,839
specific mounts that they need to be

00:11:11,120 --> 00:11:14,720
able to

00:11:11,839 --> 00:11:15,040
ensure that these are used consistently

00:11:14,720 --> 00:11:18,480
so

00:11:15,040 --> 00:11:22,720
maybe we should let operations own

00:11:18,480 --> 00:11:26,240
this pod definition well

00:11:22,720 --> 00:11:27,839
now things get murky because

00:11:26,240 --> 00:11:29,440
i'm going to update this part but i just

00:11:27,839 --> 00:11:31,360
overwrite that part and

00:11:29,440 --> 00:11:32,959
so kubernetes has actually thought of

00:11:31,360 --> 00:11:35,200
this and created a really nice

00:11:32,959 --> 00:11:37,760
abstraction that separates the concerns

00:11:35,200 --> 00:11:39,120
of dev and ops which is really really

00:11:37,760 --> 00:11:41,839
elegant

00:11:39,120 --> 00:11:42,959
so we have storage here's our drive our

00:11:41,839 --> 00:11:44,560
physical medium

00:11:42,959 --> 00:11:46,320
and let's carve it up into little

00:11:44,560 --> 00:11:47,839
volumes we'll call these persistent

00:11:46,320 --> 00:11:48,880
volumes

00:11:47,839 --> 00:11:50,639
i'm going to take one of these

00:11:48,880 --> 00:11:51,200
persistent volumes and i'm going to

00:11:50,639 --> 00:11:52,639
claim it

00:11:51,200 --> 00:11:55,279
i'm going to claim this persistent

00:11:52,639 --> 00:11:57,440
volume in a persistent volume claim

00:11:55,279 --> 00:11:59,279
and then i will map that as a puzzle

00:11:57,440 --> 00:12:02,639
piece into the container

00:11:59,279 --> 00:12:04,399
in the normal way so the container just

00:12:02,639 --> 00:12:06,240
believes it has a file system it knows

00:12:04,399 --> 00:12:08,560
nothing of this volume

00:12:06,240 --> 00:12:10,000
but now i have this clean abstraction

00:12:08,560 --> 00:12:11,760
the physical hard drive

00:12:10,000 --> 00:12:13,200
and the persistent volume can be owned

00:12:11,760 --> 00:12:15,200
by operations

00:12:13,200 --> 00:12:16,560
the persistent volume claim and the pod

00:12:15,200 --> 00:12:17,680
definition can be owned by the

00:12:16,560 --> 00:12:19,920
developers

00:12:17,680 --> 00:12:22,079
and because we have this separation now

00:12:19,920 --> 00:12:23,040
ops can store the files in the ways that

00:12:22,079 --> 00:12:25,120
they need to

00:12:23,040 --> 00:12:26,160
and ensure that their backup backed up

00:12:25,120 --> 00:12:27,760
correctly and

00:12:26,160 --> 00:12:30,160
arranged in the namespace that they've

00:12:27,760 --> 00:12:32,399
decided and developers can create the

00:12:30,160 --> 00:12:34,079
pod definition and map it exactly into

00:12:32,399 --> 00:12:37,279
the right place in the container

00:12:34,079 --> 00:12:41,120
there's no ambiguity about where

00:12:37,279 --> 00:12:43,279
the the ownership lies operations owns

00:12:41,120 --> 00:12:45,120
the storage and the persistent volume

00:12:43,279 --> 00:12:47,200
development owns the persistent volume

00:12:45,120 --> 00:12:49,839
claim and the pod definition

00:12:47,200 --> 00:12:50,560
this is perfect we'll see this type of

00:12:49,839 --> 00:12:52,560
abstraction

00:12:50,560 --> 00:12:53,600
in lots of things where we can do the

00:12:52,560 --> 00:12:55,440
naive approach

00:12:53,600 --> 00:12:56,880
and include them all in one piece that

00:12:55,440 --> 00:12:59,760
makes it a lot simpler

00:12:56,880 --> 00:13:00,720
or we can separate them out here into

00:12:59,760 --> 00:13:02,880
different

00:13:00,720 --> 00:13:06,800
pieces so that operations can own one

00:13:02,880 --> 00:13:10,560
side and developers can own another side

00:13:06,800 --> 00:13:10,560
any questions on file storage

00:13:18,399 --> 00:13:22,079
let's dig into each one and take a look

00:13:20,880 --> 00:13:25,200
at how to build them

00:13:22,079 --> 00:13:28,000
here is the yaml for a persistent volume

00:13:25,200 --> 00:13:29,760
this is that abstraction over the top of

00:13:28,000 --> 00:13:33,120
storage this is that piece

00:13:29,760 --> 00:13:35,279
of the hard drive so here we'll specify

00:13:33,120 --> 00:13:38,880
the type of driver and where it actually

00:13:35,279 --> 00:13:40,959
lives host path is great for testing

00:13:38,880 --> 00:13:42,240
access modifiers this one is read write

00:13:40,959 --> 00:13:44,880
once so only one

00:13:42,240 --> 00:13:45,360
pod can attach to this we can have read

00:13:44,880 --> 00:13:47,120
only

00:13:45,360 --> 00:13:48,480
we can have read write once and we can

00:13:47,120 --> 00:13:50,320
have read write many

00:13:48,480 --> 00:13:51,519
where many pods can read and write to

00:13:50,320 --> 00:13:53,440
the drive

00:13:51,519 --> 00:13:55,440
the capacity in this case will give it

00:13:53,440 --> 00:13:56,959
10 gigs of storage

00:13:55,440 --> 00:13:59,680
and we'll give it a name in this case

00:13:56,959 --> 00:14:02,560
we'll call it pv volume

00:13:59,680 --> 00:14:04,560
okay so we've got this persistent volume

00:14:02,560 --> 00:14:05,199
operations has carved up storage into

00:14:04,560 --> 00:14:07,519
the piece

00:14:05,199 --> 00:14:09,440
and let's create a persistent volume

00:14:07,519 --> 00:14:12,160
claim to claim that

00:14:09,440 --> 00:14:12,800
here's the persistent volume claim so we

00:14:12,160 --> 00:14:15,920
have a name

00:14:12,800 --> 00:14:16,800
pv claim and that just makes it unique

00:14:15,920 --> 00:14:19,040
storage class

00:14:16,800 --> 00:14:21,040
access modes read write once and the

00:14:19,040 --> 00:14:22,880
storage three gigs

00:14:21,040 --> 00:14:25,199
now the interesting thing is that this

00:14:22,880 --> 00:14:27,120
says i want at least three gigs

00:14:25,199 --> 00:14:28,800
that is read write once of this storage

00:14:27,120 --> 00:14:30,639
class

00:14:28,800 --> 00:14:32,800
here in the persistent volume we say

00:14:30,639 --> 00:14:33,760
okay i have the storage class i have

00:14:32,800 --> 00:14:36,880
read write once

00:14:33,760 --> 00:14:38,560
and i have 10 gigs okay that matches my

00:14:36,880 --> 00:14:41,040
minimum of three gigs and so that's the

00:14:38,560 --> 00:14:43,680
persistent volume that i'll grab

00:14:41,040 --> 00:14:44,880
now let's map it into the pod so we have

00:14:43,680 --> 00:14:47,920
this persistent volume

00:14:44,880 --> 00:14:50,560
claim that specifies pv claim

00:14:47,920 --> 00:14:50,959
is the claim that i will use to create

00:14:50,560 --> 00:14:53,839
this

00:14:50,959 --> 00:14:55,440
volume mount the mount path into the

00:14:53,839 --> 00:14:57,120
container is identical the

00:14:55,440 --> 00:14:58,480
container knows nothing about this

00:14:57,120 --> 00:15:00,240
volume it just

00:14:58,480 --> 00:15:02,399
starts to read and write to path in the

00:15:00,240 --> 00:15:04,000
container dirt and that goes through the

00:15:02,399 --> 00:15:06,000
persistent volume claim

00:15:04,000 --> 00:15:08,000
through the persistent volume and into

00:15:06,000 --> 00:15:10,240
the physical storage

00:15:08,000 --> 00:15:12,079
that's perfect so we were able to

00:15:10,240 --> 00:15:14,399
separate these concerns between

00:15:12,079 --> 00:15:16,240
developers and operations to ensure that

00:15:14,399 --> 00:15:18,480
developers can ensure the proper

00:15:16,240 --> 00:15:21,120
configuration right here into

00:15:18,480 --> 00:15:22,160
their container and operations can

00:15:21,120 --> 00:15:25,040
ensure that

00:15:22,160 --> 00:15:27,839
the data is backed up and archived

00:15:25,040 --> 00:15:27,839
properly

00:15:28,880 --> 00:15:34,079
storage carved up into persistent

00:15:30,959 --> 00:15:36,399
volumes both owned by operations

00:15:34,079 --> 00:15:39,199
persistent volume claims and containers

00:15:36,399 --> 00:15:43,120
owned by developers

00:15:39,199 --> 00:15:46,000
that's perfect so we saw how we can do

00:15:43,120 --> 00:15:46,399
file storage this is a great way to get

00:15:46,000 --> 00:15:48,959
that

00:15:46,399 --> 00:15:50,320
next piece of stateful information into

00:15:48,959 --> 00:15:52,399
kubernetes

00:15:50,320 --> 00:15:54,320
next let's talk about configuration now

00:15:52,399 --> 00:15:54,959
configuration can be lots of things to

00:15:54,320 --> 00:15:57,600
lots and

00:15:54,959 --> 00:15:58,079
lots of people but we'll look at config

00:15:57,600 --> 00:16:00,720
map

00:15:58,079 --> 00:16:02,320
that talks about that configuration

00:16:00,720 --> 00:16:03,920
configuration the read-only

00:16:02,320 --> 00:16:05,839
configuration details

00:16:03,920 --> 00:16:07,199
now we can choose to mount these either

00:16:05,839 --> 00:16:09,839
as environment variables

00:16:07,199 --> 00:16:10,320
or as files in a drive and there are

00:16:09,839 --> 00:16:13,279
great

00:16:10,320 --> 00:16:13,279
reasons for each

00:16:13,440 --> 00:16:19,279
first up configuration

00:16:16,880 --> 00:16:19,920
so down here we've specified other

00:16:19,279 --> 00:16:22,560
details

00:16:19,920 --> 00:16:24,880
now we may choose volumes we may have

00:16:22,560 --> 00:16:26,959
resource limits again i've snipped this

00:16:24,880 --> 00:16:28,959
but here's that the environment

00:16:26,959 --> 00:16:31,279
variables that my application needs

00:16:28,959 --> 00:16:32,320
it has one that's called the message one

00:16:31,279 --> 00:16:35,680
that's called foo

00:16:32,320 --> 00:16:38,560
and i can specify the values for this

00:16:35,680 --> 00:16:39,680
now these may be environment specific or

00:16:38,560 --> 00:16:42,959
again we may say

00:16:39,680 --> 00:16:44,800
well operations needs to uh own the

00:16:42,959 --> 00:16:47,199
actual values of these

00:16:44,800 --> 00:16:48,160
the developers should leave those alone

00:16:47,199 --> 00:16:50,720
well

00:16:48,160 --> 00:16:52,800
now do developers on the top half of the

00:16:50,720 --> 00:16:54,399
file and operation zones the bottom half

00:16:52,800 --> 00:16:56,240
of the file

00:16:54,399 --> 00:16:57,759
we have mechanisms where we can separate

00:16:56,240 --> 00:16:59,680
them out

00:16:57,759 --> 00:17:01,120
so developers can own the pieces

00:16:59,680 --> 00:17:03,040
specific to the container

00:17:01,120 --> 00:17:04,720
and operations can own that

00:17:03,040 --> 00:17:06,959
configuration detail and keep it

00:17:04,720 --> 00:17:09,679
specific for each environment

00:17:06,959 --> 00:17:10,799
here's a config map and this config map

00:17:09,679 --> 00:17:13,679
very specifically

00:17:10,799 --> 00:17:14,799
says here's the data associated with

00:17:13,679 --> 00:17:17,199
this configuration

00:17:14,799 --> 00:17:18,000
so i have a name for this entire config

00:17:17,199 --> 00:17:20,000
map

00:17:18,000 --> 00:17:22,000
and then i have keys and values inside

00:17:20,000 --> 00:17:23,839
that so in this case my database is

00:17:22,000 --> 00:17:27,600
memsql i've turned on logging

00:17:23,839 --> 00:17:29,440
and my api url is example.com api

00:17:27,600 --> 00:17:30,720
i now have all of the pieces of

00:17:29,440 --> 00:17:33,520
configuration

00:17:30,720 --> 00:17:35,200
specified in a config map and now i can

00:17:33,520 --> 00:17:38,720
map that config map

00:17:35,200 --> 00:17:40,960
into place in my pod now i could choose

00:17:38,720 --> 00:17:43,280
to enumerate all of the keys and values

00:17:40,960 --> 00:17:44,880
but i'm just going to say end from

00:17:43,280 --> 00:17:47,360
config map rep

00:17:44,880 --> 00:17:48,000
and named that config map so now i'm

00:17:47,360 --> 00:17:49,600
going to get

00:17:48,000 --> 00:17:51,840
all of the configuration from that

00:17:49,600 --> 00:17:54,880
config map mapped as specific

00:17:51,840 --> 00:17:58,080
environment variables in my

00:17:54,880 --> 00:18:01,280
pod and this pod definition could

00:17:58,080 --> 00:18:02,000
indeed be part of a template inside of a

00:18:01,280 --> 00:18:05,120
deployment

00:18:02,000 --> 00:18:08,080
or it can be a standalone pod like this

00:18:05,120 --> 00:18:10,000
so now developers can own all of the

00:18:08,080 --> 00:18:11,600
pieces of mapping that to the correct

00:18:10,000 --> 00:18:14,000
place inside the container

00:18:11,600 --> 00:18:15,280
and operations can own the actual values

00:18:14,000 --> 00:18:17,679
we don't need to expose

00:18:15,280 --> 00:18:20,960
the production configuration values to

00:18:17,679 --> 00:18:20,960
developers if we don't need to

00:18:21,520 --> 00:18:25,679
configmap has files so if we'd rather

00:18:24,400 --> 00:18:27,919
instead of having them as

00:18:25,679 --> 00:18:30,400
environment variables make them as files

00:18:27,919 --> 00:18:32,960
we can do this as well the config map

00:18:30,400 --> 00:18:35,600
config map yml doesn't change at all

00:18:32,960 --> 00:18:38,400
it's just how we map it inside the pod

00:18:35,600 --> 00:18:39,360
so here we've got volumes and i'm saying

00:18:38,400 --> 00:18:41,440
config map

00:18:39,360 --> 00:18:43,039
and here's the config map name that's

00:18:41,440 --> 00:18:43,679
the name of that config map yaml

00:18:43,039 --> 00:18:46,640
resource

00:18:43,679 --> 00:18:48,320
and now we have volume mounts the volume

00:18:46,640 --> 00:18:50,080
so these two names match so i know that

00:18:48,320 --> 00:18:52,559
this is the volume pulling from here

00:18:50,080 --> 00:18:53,919
and here's the path in the container to

00:18:52,559 --> 00:18:56,080
that directory

00:18:53,919 --> 00:18:56,960
so inside that directory i will get a

00:18:56,080 --> 00:19:00,640
file

00:18:56,960 --> 00:19:02,720
per configure config map key and so my

00:19:00,640 --> 00:19:05,200
application can choose to open each file

00:19:02,720 --> 00:19:06,880
read the contents and get the value of

00:19:05,200 --> 00:19:08,160
that configuration

00:19:06,880 --> 00:19:10,480
now i may choose to do this

00:19:08,160 --> 00:19:12,799
configuration as files to make it

00:19:10,480 --> 00:19:13,679
a little less obvious to people who

00:19:12,799 --> 00:19:15,760
might

00:19:13,679 --> 00:19:18,480
have malicious intent with my machine or

00:19:15,760 --> 00:19:20,160
maybe i don't need that value very often

00:19:18,480 --> 00:19:21,520
or i may choose to make it as

00:19:20,160 --> 00:19:23,200
environment variable so it's really

00:19:21,520 --> 00:19:25,760
convenient for my application

00:19:23,200 --> 00:19:27,120
whichever makes the most sense for your

00:19:25,760 --> 00:19:29,120
application

00:19:27,120 --> 00:19:31,440
there really isn't a difference inside

00:19:29,120 --> 00:19:31,919
kubernetes the config map is mapped into

00:19:31,440 --> 00:19:34,160
place

00:19:31,919 --> 00:19:36,840
and i can read that data and use it in

00:19:34,160 --> 00:19:38,080
my application in the way that makes

00:19:36,840 --> 00:19:41,280
sense

00:19:38,080 --> 00:19:44,320
so some configuration is

00:19:41,280 --> 00:19:48,320
secret and i may choose to

00:19:44,320 --> 00:19:51,760
not have that exposed

00:19:48,320 --> 00:19:55,120
now the interesting thing is for secrets

00:19:51,760 --> 00:19:58,240
before 1.13 config

00:19:55,120 --> 00:19:59,039
secrets were stored as base64 encoded

00:19:58,240 --> 00:20:01,520
data

00:19:59,039 --> 00:20:03,679
starting with 1.13 you could choose to

00:20:01,520 --> 00:20:04,880
enable a configuration that stores them

00:20:03,679 --> 00:20:08,240
encrypted as rest

00:20:04,880 --> 00:20:11,760
and i would invite you to do so because

00:20:08,240 --> 00:20:12,799
yeah secret stored as base64 means that

00:20:11,760 --> 00:20:15,520
if you lost

00:20:12,799 --> 00:20:17,679
any node in your cluster you lost all

00:20:15,520 --> 00:20:18,720
the secrets for all the applications in

00:20:17,679 --> 00:20:20,480
the cluster

00:20:18,720 --> 00:20:22,720
remember that any pod that needs to

00:20:20,480 --> 00:20:23,440
start up needs to have access to those

00:20:22,720 --> 00:20:25,679
secrets

00:20:23,440 --> 00:20:27,679
so secrets are distributed to every node

00:20:25,679 --> 00:20:31,679
in the cluster

00:20:27,679 --> 00:20:33,679
that's maybe not great turn on secret

00:20:31,679 --> 00:20:35,280
storage encryption and at that point

00:20:33,679 --> 00:20:39,039
then your secrets are stored

00:20:35,280 --> 00:20:40,640
safely alternatively you may choose to

00:20:39,039 --> 00:20:42,480
have an external store and have your

00:20:40,640 --> 00:20:43,520
application reach out to that store for

00:20:42,480 --> 00:20:45,679
the secrets

00:20:43,520 --> 00:20:47,120
that is interesting but that adds more

00:20:45,679 --> 00:20:49,280
complexity

00:20:47,120 --> 00:20:50,720
so let's look at secrets stored inside

00:20:49,280 --> 00:20:53,039
kubernetes

00:20:50,720 --> 00:20:54,000
so i can create a secret here using a

00:20:53,039 --> 00:20:56,640
yaml file

00:20:54,000 --> 00:20:59,200
now in this case i'm saying string data

00:20:56,640 --> 00:21:00,960
so that i don't need to base64 encode my

00:20:59,200 --> 00:21:04,559
value first but i could also choose

00:21:00,960 --> 00:21:05,919
data and pre base64 encode my values if

00:21:04,559 --> 00:21:07,919
i wanted to

00:21:05,919 --> 00:21:09,520
now this is really interesting this gets

00:21:07,919 --> 00:21:11,600
the

00:21:09,520 --> 00:21:13,200
kubernetes or gets the secret into

00:21:11,600 --> 00:21:15,600
kubernetes really easily

00:21:13,200 --> 00:21:16,960
but it's really easy to accidentally

00:21:15,600 --> 00:21:19,039
commit this secret

00:21:16,960 --> 00:21:21,520
into source control and so i would

00:21:19,039 --> 00:21:23,919
recommend instead of creating it in yaml

00:21:21,520 --> 00:21:26,080
create it on the command line instead

00:21:23,919 --> 00:21:29,039
here's the cubectl create command

00:21:26,080 --> 00:21:30,159
that will create this secret now similar

00:21:29,039 --> 00:21:32,240
to config maps

00:21:30,159 --> 00:21:34,159
we've named our secret db connection and

00:21:32,240 --> 00:21:34,880
we have two keys and values in this

00:21:34,159 --> 00:21:37,120
secret

00:21:34,880 --> 00:21:39,360
so here's the username and the password

00:21:37,120 --> 00:21:41,679
to my database

00:21:39,360 --> 00:21:43,360
so in my application i go read this

00:21:41,679 --> 00:21:44,080
secret i get out all of the

00:21:43,360 --> 00:21:46,080
configuration

00:21:44,080 --> 00:21:49,360
keys and i can get at those values to

00:21:46,080 --> 00:21:49,360
use them in my application

00:21:49,440 --> 00:21:54,960
so how do i mop map them to my pod

00:21:52,640 --> 00:21:55,679
now the pod could be part of a

00:21:54,960 --> 00:21:57,360
deployment

00:21:55,679 --> 00:21:59,200
template or it could be just a

00:21:57,360 --> 00:22:01,520
straightaway pod as

00:21:59,200 --> 00:22:02,720
we've done here i'm specifying now the

00:22:01,520 --> 00:22:06,080
volumes

00:22:02,720 --> 00:22:08,559
and instead of in from or

00:22:06,080 --> 00:22:10,159
a host path i'm saying this is coming

00:22:08,559 --> 00:22:13,520
from a secret

00:22:10,159 --> 00:22:16,320
here's my secret name dbconnection

00:22:13,520 --> 00:22:17,440
so volume mounts in this path in

00:22:16,320 --> 00:22:20,159
container to dur

00:22:17,440 --> 00:22:21,679
i will have files for each of the keys

00:22:20,159 --> 00:22:24,000
in my secret

00:22:21,679 --> 00:22:26,320
my application can open up each file and

00:22:24,000 --> 00:22:30,640
get at the values that it needs to

00:22:26,320 --> 00:22:31,360
that's perfect i can also choose to map

00:22:30,640 --> 00:22:33,919
these as

00:22:31,360 --> 00:22:35,520
environment variables now i could just

00:22:33,919 --> 00:22:38,000
map them all as

00:22:35,520 --> 00:22:39,520
and from that would grab all of the

00:22:38,000 --> 00:22:40,080
environment variables but i can also

00:22:39,520 --> 00:22:43,039
specify

00:22:40,080 --> 00:22:44,960
each one so here i specified value from

00:22:43,039 --> 00:22:46,240
secret key ref here's the name and the

00:22:44,960 --> 00:22:47,760
key of that secret

00:22:46,240 --> 00:22:49,280
here's the name and the key of that

00:22:47,760 --> 00:22:53,120
secret so now i've got

00:22:49,280 --> 00:22:55,200
all of the secrets that i use enumerated

00:22:53,120 --> 00:22:57,120
and my application can just read those

00:22:55,200 --> 00:22:59,280
environment variables

00:22:57,120 --> 00:23:00,400
now does it make more sense to store

00:22:59,280 --> 00:23:02,159
them as files

00:23:00,400 --> 00:23:03,840
or as environment variables it

00:23:02,159 --> 00:23:06,159
definitely depends on the needs of your

00:23:03,840 --> 00:23:10,240
application but built into kubernetes

00:23:06,159 --> 00:23:10,240
you have both that's perfect

00:23:10,720 --> 00:23:14,559
so we've got config maps we've got

00:23:13,760 --> 00:23:17,120
secrets

00:23:14,559 --> 00:23:19,440
those store configuration details we've

00:23:17,120 --> 00:23:20,880
got volumes that can store more durable

00:23:19,440 --> 00:23:23,679
storage

00:23:20,880 --> 00:23:24,480
but how do we have a cluster of machines

00:23:23,679 --> 00:23:26,640
maybe we have an

00:23:24,480 --> 00:23:27,679
elk stack that needs three or five

00:23:26,640 --> 00:23:29,520
machines to achieve

00:23:27,679 --> 00:23:30,799
quorum or maybe we have a database

00:23:29,520 --> 00:23:35,039
server that needs

00:23:30,799 --> 00:23:37,280
three or five machines to achieve quorum

00:23:35,039 --> 00:23:39,200
this stateful set allows us to do

00:23:37,280 --> 00:23:42,240
exactly that a stateful set

00:23:39,200 --> 00:23:42,960
will create a mechanism where the

00:23:42,240 --> 00:23:45,120
machines

00:23:42,960 --> 00:23:47,200
well in this case the containers can

00:23:45,120 --> 00:23:51,760
start to interact with each other

00:23:47,200 --> 00:23:54,320
in very predictable ways stateful set

00:23:51,760 --> 00:23:55,120
it creates pods with predictable names

00:23:54,320 --> 00:23:56,880
and

00:23:55,120 --> 00:23:58,480
we can communicate between those

00:23:56,880 --> 00:24:00,960
containers and

00:23:58,480 --> 00:24:03,120
list the uh and so we don't need to

00:24:00,960 --> 00:24:04,720
specify a list of machines

00:24:03,120 --> 00:24:06,559
like for example in our connection

00:24:04,720 --> 00:24:09,919
string we can just connect

00:24:06,559 --> 00:24:13,039
to this cluster and the cluster can

00:24:09,919 --> 00:24:13,440
keep track of itself we can think of

00:24:13,039 --> 00:24:16,559
this

00:24:13,440 --> 00:24:18,720
if we squint real hard as a deployment

00:24:16,559 --> 00:24:20,720
but instead of auto-generated names we

00:24:18,720 --> 00:24:23,520
have very predictable names

00:24:20,720 --> 00:24:25,600
and we'll see that come through it

00:24:23,520 --> 00:24:26,400
really is a deployment with predictable

00:24:25,600 --> 00:24:28,880
names

00:24:26,400 --> 00:24:30,000
so i might use it for a kafka cluster or

00:24:28,880 --> 00:24:34,240
an elk stack

00:24:30,000 --> 00:24:34,240
or maybe a database cluster like memsql

00:24:35,039 --> 00:24:41,279
so here's a stateful set each of the

00:24:38,400 --> 00:24:42,000
pods hosts the container but instead of

00:24:41,279 --> 00:24:45,120
the pod

00:24:42,000 --> 00:24:48,640
being randomly named this one will be

00:24:45,120 --> 00:24:50,320
zero pod one and pod two

00:24:48,640 --> 00:24:52,080
and now i can reach in through this

00:24:50,320 --> 00:24:52,640
headless service to communicate with

00:24:52,080 --> 00:24:54,400
each other

00:24:52,640 --> 00:24:56,880
so pod two can say i would like to

00:24:54,400 --> 00:24:59,039
communicate specifically with pod zero

00:24:56,880 --> 00:25:00,159
or pod zero can specifically reach out

00:24:59,039 --> 00:25:01,919
into pod one

00:25:00,159 --> 00:25:03,600
now because each of the containers can

00:25:01,919 --> 00:25:04,480
talk to each other each of the

00:25:03,600 --> 00:25:07,120
containers

00:25:04,480 --> 00:25:08,240
can understand the health of the cluster

00:25:07,120 --> 00:25:11,120
hold an election

00:25:08,240 --> 00:25:11,840
if one of the pods becomes unhealthy and

00:25:11,120 --> 00:25:13,440
they can keep

00:25:11,840 --> 00:25:15,039
track of that cluster and ensure that

00:25:13,440 --> 00:25:18,320
that cluster of

00:25:15,039 --> 00:25:20,080
containers is healthy the stateful

00:25:18,320 --> 00:25:22,159
the headless service allows that

00:25:20,080 --> 00:25:24,720
communication

00:25:22,159 --> 00:25:26,400
so here's a stateful set and it pretty

00:25:24,720 --> 00:25:29,279
much looks like a deployment

00:25:26,400 --> 00:25:30,559
we have match labels we have replicas we

00:25:29,279 --> 00:25:32,640
have a template

00:25:30,559 --> 00:25:34,159
and here's our template that defines our

00:25:32,640 --> 00:25:36,799
pod um

00:25:34,159 --> 00:25:37,279
here the only thing that is interesting

00:25:36,799 --> 00:25:39,600
is

00:25:37,279 --> 00:25:41,279
the headless service where is that

00:25:39,600 --> 00:25:44,400
headless service

00:25:41,279 --> 00:25:46,880
there we go service name is dbservice so

00:25:44,400 --> 00:25:49,279
this db service this headless service

00:25:46,880 --> 00:25:51,440
is that mechanism where the stateful set

00:25:49,279 --> 00:25:53,440
can communicate with each other

00:25:51,440 --> 00:25:55,360
it's going to reach through dbservice to

00:25:53,440 --> 00:25:58,159
get to the other pods

00:25:55,360 --> 00:26:00,799
in this stateful set now in this case i

00:25:58,159 --> 00:26:04,880
have three so i'm going to get

00:26:00,799 --> 00:26:07,440
the db or in this case my app

00:26:04,880 --> 00:26:11,039
underscore 0 my app underscore 1 and my

00:26:07,440 --> 00:26:13,360
app underscore 2.

00:26:11,039 --> 00:26:15,039
here's that headless service now it's

00:26:13,360 --> 00:26:15,679
not that different from any other

00:26:15,039 --> 00:26:18,080
service

00:26:15,679 --> 00:26:19,120
except for we're not specifying a

00:26:18,080 --> 00:26:21,200
cluster ip

00:26:19,120 --> 00:26:23,760
or a type of node port in this case

00:26:21,200 --> 00:26:26,159
we're saying cluster ip of none

00:26:23,760 --> 00:26:28,799
that's what makes it headless it has no

00:26:26,159 --> 00:26:30,240
cluster ip

00:26:28,799 --> 00:26:32,080
everything else in this service is

00:26:30,240 --> 00:26:34,480
pretty standard and so we can grab

00:26:32,080 --> 00:26:35,840
a service and a deployment for a

00:26:34,480 --> 00:26:38,400
stateless resource

00:26:35,840 --> 00:26:40,080
and turn it into a stateful set just by

00:26:38,400 --> 00:26:42,720
mangling this part

00:26:40,080 --> 00:26:44,000
or giving it the service name and

00:26:42,720 --> 00:26:47,360
setting the kind

00:26:44,000 --> 00:26:48,400
to stateful set now we've got a stateful

00:26:47,360 --> 00:26:51,360
set

00:26:48,400 --> 00:26:53,039
so if i were to curl the db service it

00:26:51,360 --> 00:26:54,559
would act like any other service and it

00:26:53,039 --> 00:26:57,600
would round robin across

00:26:54,559 --> 00:27:00,400
all of the pods in that

00:26:57,600 --> 00:27:01,679
stateful set but if i wanted to browse

00:27:00,400 --> 00:27:04,880
between the machines

00:27:01,679 --> 00:27:09,120
i can hit db service but i will hit

00:27:04,880 --> 00:27:12,320
the db1 dot so if i had named my pods

00:27:09,120 --> 00:27:13,840
the db here's the db1 and now i'm

00:27:12,320 --> 00:27:17,200
getting at the first machine

00:27:13,840 --> 00:27:17,200
inside that stateful set

00:27:18,080 --> 00:27:24,559
so in many cases i would want to create

00:27:21,840 --> 00:27:26,720
a headless service to allow

00:27:24,559 --> 00:27:29,279
communication between the nodes

00:27:26,720 --> 00:27:29,760
in my cluster but i'll probably also

00:27:29,279 --> 00:27:31,760
want

00:27:29,760 --> 00:27:32,880
a different service that allows

00:27:31,760 --> 00:27:36,320
communication

00:27:32,880 --> 00:27:38,320
from for externally from other

00:27:36,320 --> 00:27:39,360
pods in the cluster or from outside the

00:27:38,320 --> 00:27:41,120
cluster

00:27:39,360 --> 00:27:42,799
the headless service will facilitate

00:27:41,120 --> 00:27:46,159
communication between

00:27:42,799 --> 00:27:47,200
containers and the node port or cluster

00:27:46,159 --> 00:27:49,200
ip service

00:27:47,200 --> 00:27:50,880
or maybe load balancer service will

00:27:49,200 --> 00:27:54,159
facilitate communication

00:27:50,880 --> 00:27:55,440
from other pieces now it's possible that

00:27:54,159 --> 00:27:58,399
i could have others

00:27:55,440 --> 00:28:00,159
reach out into this headless service but

00:27:58,399 --> 00:28:02,159
that's really not good form

00:28:00,159 --> 00:28:04,000
this headless service is really about

00:28:02,159 --> 00:28:06,320
communication between the nodes

00:28:04,000 --> 00:28:07,120
and probably i don't want to expose all

00:28:06,320 --> 00:28:09,200
the nodes

00:28:07,120 --> 00:28:11,919
i just want to expose the aggregator

00:28:09,200 --> 00:28:14,320
nodes the internal nodes that store the

00:28:11,919 --> 00:28:16,720
data i'll probably want to not have them

00:28:14,320 --> 00:28:18,240
as part of that public exposed service

00:28:16,720 --> 00:28:19,520
but they will need to be part of the

00:28:18,240 --> 00:28:21,279
headless service so that we can

00:28:19,520 --> 00:28:23,120
communicate between the nodes in our

00:28:21,279 --> 00:28:27,360
cluster

00:28:23,120 --> 00:28:29,360
perfect so we looked at all of the

00:28:27,360 --> 00:28:31,919
different pieces

00:28:29,360 --> 00:28:34,000
we looked at ingress and service in

00:28:31,919 --> 00:28:36,320
containers and pods and all of those are

00:28:34,000 --> 00:28:39,360
the stateless resources

00:28:36,320 --> 00:28:41,760
but now we added to it all the extra

00:28:39,360 --> 00:28:42,720
pieces that are the stateful resources

00:28:41,760 --> 00:28:44,799
in our cluster

00:28:42,720 --> 00:28:46,320
we may have file storage we did that

00:28:44,799 --> 00:28:48,399
with volumes

00:28:46,320 --> 00:28:50,159
we may have a persistent volume and a

00:28:48,399 --> 00:28:52,799
persistent volume claim

00:28:50,159 --> 00:28:54,480
to be able to map those into containers

00:28:52,799 --> 00:28:57,520
we might have authorization

00:28:54,480 --> 00:28:58,000
and path details those might be config

00:28:57,520 --> 00:29:00,880
maps

00:28:58,000 --> 00:29:02,480
or secrets the url and the username and

00:29:00,880 --> 00:29:03,600
password secrets getting into the

00:29:02,480 --> 00:29:05,360
database cluster

00:29:03,600 --> 00:29:06,799
and that database cluster is probably

00:29:05,360 --> 00:29:11,039
managed by a

00:29:06,799 --> 00:29:13,679
stateful set and a headless service

00:29:11,039 --> 00:29:14,240
the containers and pods the service the

00:29:13,679 --> 00:29:16,399
ingress

00:29:14,240 --> 00:29:18,399
those are all what we expect but the

00:29:16,399 --> 00:29:19,679
ingress it might have a certificate

00:29:18,399 --> 00:29:23,200
there's another secret

00:29:19,679 --> 00:29:25,760
inside of our cluster so we can see that

00:29:23,200 --> 00:29:26,240
moving all of the rest of the resources

00:29:25,760 --> 00:29:29,600
here

00:29:26,240 --> 00:29:32,720
into kubernetes allows us to build a

00:29:29,600 --> 00:29:34,799
stateful mechanism and all this is baked

00:29:32,720 --> 00:29:36,559
into kubernetes we're not inventing any

00:29:34,799 --> 00:29:38,640
crds to make this happen

00:29:36,559 --> 00:29:40,320
but if we did want to swap them out we

00:29:38,640 --> 00:29:41,840
might choose to swap out our file

00:29:40,320 --> 00:29:43,679
storage with a different

00:29:41,840 --> 00:29:46,720
provider maybe we're going to store this

00:29:43,679 --> 00:29:50,399
on azure s3 or

00:29:46,720 --> 00:29:52,720
amazon amazon s3 or azure file storage

00:29:50,399 --> 00:29:53,679
to be able to get that system to be more

00:29:52,720 --> 00:29:56,080
durable

00:29:53,679 --> 00:29:56,799
maybe these configurations might be

00:29:56,080 --> 00:29:58,799
backed by

00:29:56,799 --> 00:30:00,559
a key vault and so at that point then it

00:29:58,799 --> 00:30:02,080
reaches into the key vault to go grab

00:30:00,559 --> 00:30:05,360
the secrets that's

00:30:02,080 --> 00:30:07,360
baking into the pod perhaps our database

00:30:05,360 --> 00:30:09,039
cluster is not just a stateful set but a

00:30:07,360 --> 00:30:10,320
series of stateful sets so that we can

00:30:09,039 --> 00:30:12,880
have those public

00:30:10,320 --> 00:30:15,039
exposed machines in one stateful set and

00:30:12,880 --> 00:30:17,440
the private machines that store the data

00:30:15,039 --> 00:30:19,440
in another stateful set maybe the

00:30:17,440 --> 00:30:21,279
database cluster also needs some file

00:30:19,440 --> 00:30:23,039
storage to make this happen so they may

00:30:21,279 --> 00:30:24,720
have some persistent volumes and some

00:30:23,039 --> 00:30:27,760
persistent volume claims

00:30:24,720 --> 00:30:30,720
along with their stateful sets

00:30:27,760 --> 00:30:31,039
all of these pieces allow us to build up

00:30:30,720 --> 00:30:34,240
a

00:30:31,039 --> 00:30:36,640
stateful experience inside kubernetes

00:30:34,240 --> 00:30:37,760
using just normal kubernetes resources

00:30:36,640 --> 00:30:40,240
we got

00:30:37,760 --> 00:30:43,039
the rest of our application into

00:30:40,240 --> 00:30:43,039
kubernetes

00:30:43,279 --> 00:30:48,399
that was really fun it's really cool to

00:30:45,840 --> 00:30:50,960
see all of the stateful pieces come into

00:30:48,399 --> 00:30:52,559
place inside kubernetes

00:30:50,960 --> 00:30:54,240
if you're watching this online you can

00:30:52,559 --> 00:30:54,880
reach me at rob underscore rich on

00:30:54,240 --> 00:30:57,200
twitter

00:30:54,880 --> 00:30:59,120
and you can grab these slides right now

00:30:57,200 --> 00:31:00,880
at robrich.org

00:30:59,120 --> 00:31:05,200
for those who are watching live what are

00:31:00,880 --> 00:31:06,480
our questions

00:31:05,200 --> 00:31:08,559
all right rob that was a great

00:31:06,480 --> 00:31:10,559
presentation thanks for that uh we do

00:31:08,559 --> 00:31:12,799
have one question so far and maybe we'll

00:31:10,559 --> 00:31:14,399
get another couple come in as we discuss

00:31:12,799 --> 00:31:15,840
uh we do have a few minutes for

00:31:14,399 --> 00:31:17,679
questions here so uh

00:31:15,840 --> 00:31:18,960
to everybody who's stuck around we

00:31:17,679 --> 00:31:20,799
appreciate it we got a pretty good group

00:31:18,960 --> 00:31:22,480
here we do encourage you to post your

00:31:20,799 --> 00:31:25,840
questions in the chat

00:31:22,480 --> 00:31:28,640
uh rob justin in i'm sorry uh

00:31:25,840 --> 00:31:30,399
catherine in the chat uh says is round

00:31:28,640 --> 00:31:32,240
robin the only algorithm for load

00:31:30,399 --> 00:31:36,799
balancing between the headless service

00:31:32,240 --> 00:31:38,960
and the pods is round robin the only

00:31:36,799 --> 00:31:41,360
mechanism for the headless service good

00:31:38,960 --> 00:31:43,840
call for normal services round robin

00:31:41,360 --> 00:31:44,880
is the default and i may choose to take

00:31:43,840 --> 00:31:48,080
over that with a

00:31:44,880 --> 00:31:48,799
crd but by default round robin is that

00:31:48,080 --> 00:31:50,320
mechanism

00:31:48,799 --> 00:31:52,000
what's cool about a headless service

00:31:50,320 --> 00:31:53,519
though is i'm probably not going to use

00:31:52,000 --> 00:31:55,360
it as a round robin

00:31:53,519 --> 00:31:57,279
load balancer over the top of each of

00:31:55,360 --> 00:31:58,880
them the headless service is there to

00:31:57,279 --> 00:32:01,200
facilitate communication

00:31:58,880 --> 00:32:02,000
and so i'm misusing it not as a load

00:32:01,200 --> 00:32:05,200
balancer

00:32:02,000 --> 00:32:05,919
but i'm using it as dns entries so i can

00:32:05,200 --> 00:32:08,480
get dns

00:32:05,919 --> 00:32:10,240
entries for all the rest of the pods in

00:32:08,480 --> 00:32:13,440
my cluster

00:32:10,240 --> 00:32:15,600
so is round robin the only

00:32:13,440 --> 00:32:16,480
mechanism well it's the default but it

00:32:15,600 --> 00:32:18,799
isn't the only

00:32:16,480 --> 00:32:20,960
but i'm probably not using it that way

00:32:18,799 --> 00:32:24,799
as i'm using it as a headless service

00:32:20,960 --> 00:32:26,480
uh as we looked in here at this diagram

00:32:24,799 --> 00:32:28,000
the headless service will sit in front

00:32:26,480 --> 00:32:30,000
of the database cluster

00:32:28,000 --> 00:32:32,080
this service here is probably that one

00:32:30,000 --> 00:32:35,279
where we're using

00:32:32,080 --> 00:32:38,399
round robin and so is it the only one

00:32:35,279 --> 00:32:39,919
it's not but it works pretty well as we

00:32:38,399 --> 00:32:42,159
get into service meshes

00:32:39,919 --> 00:32:43,120
we'll definitely replace that with a

00:32:42,159 --> 00:32:45,679
service mesh

00:32:43,120 --> 00:32:48,159
based piece that will round robin much

00:32:45,679 --> 00:32:48,159
differently

00:32:49,200 --> 00:32:52,799
cool thanks for that answer rob uh and

00:32:51,760 --> 00:32:55,760
so then our next

00:32:52,799 --> 00:32:56,159
remark in the chat uh not a question but

00:32:55,760 --> 00:32:57,919
uh

00:32:56,159 --> 00:32:59,679
do i think it would be nice to share

00:32:57,919 --> 00:33:02,320
justin a chance says uh

00:32:59,679 --> 00:33:03,200
really enjoyed rob's presentation so

00:33:02,320 --> 00:33:05,760
kudos

00:33:03,200 --> 00:33:06,559
justin and then we do have another

00:33:05,760 --> 00:33:09,120
question from

00:33:06,559 --> 00:33:10,000
craig uh craig in the chat says how do

00:33:09,120 --> 00:33:13,360
namespaces

00:33:10,000 --> 00:33:14,799
fit into all of this good call how do

00:33:13,360 --> 00:33:16,880
namespaces fit

00:33:14,799 --> 00:33:20,480
all of the things that we've looked at

00:33:16,880 --> 00:33:23,519
today stateful sets config maps secrets

00:33:20,480 --> 00:33:24,799
uh volumes all of them are namespace

00:33:23,519 --> 00:33:27,039
specific

00:33:24,799 --> 00:33:28,320
and so we can still use namespaces as an

00:33:27,039 --> 00:33:30,000
organizational boundary

00:33:28,320 --> 00:33:32,159
when i talk about service meshes i love

00:33:30,000 --> 00:33:33,200
to talk about how namespaces are not a

00:33:32,159 --> 00:33:35,519
security boundary

00:33:33,200 --> 00:33:36,880
only an organizational boundary and so

00:33:35,519 --> 00:33:39,760
if you have a namespace

00:33:36,880 --> 00:33:41,200
you can definitely separate those pieces

00:33:39,760 --> 00:33:43,039
but make sure that

00:33:41,200 --> 00:33:45,360
it's still okay that they if they

00:33:43,039 --> 00:33:48,159
accidentally call each other that

00:33:45,360 --> 00:33:50,240
bad things won't happen so i may choose

00:33:48,159 --> 00:33:53,919
to create all of these resources

00:33:50,240 --> 00:33:55,840
in one namespace maybe that's my

00:33:53,919 --> 00:33:57,360
name space for this particular team or

00:33:55,840 --> 00:33:58,640
this particular product

00:33:57,360 --> 00:34:00,240
and then i might have a completely

00:33:58,640 --> 00:34:03,360
different name space with a completely

00:34:00,240 --> 00:34:03,360
different application in it

00:34:05,039 --> 00:34:10,399
cool all right appreciate that um

00:34:08,159 --> 00:34:11,440
we have just a few minutes left uh

00:34:10,399 --> 00:34:14,560
before we

00:34:11,440 --> 00:34:15,679
proceed on to our next uh part of this

00:34:14,560 --> 00:34:17,599
session so

00:34:15,679 --> 00:34:19,119
if anybody has any final feedback or

00:34:17,599 --> 00:34:21,040
questions for rob we'll

00:34:19,119 --> 00:34:22,159
leave it open for just a few moments

00:34:21,040 --> 00:34:25,119
before we close

00:34:22,159 --> 00:34:26,560
um and if we don't see any further q a

00:34:25,119 --> 00:34:29,599
we'll go ahead and close

00:34:26,560 --> 00:34:31,280
rob any parting remarks

00:34:29,599 --> 00:34:32,879
this was so much fun getting to share

00:34:31,280 --> 00:34:35,359
stateful kubernetes

00:34:32,879 --> 00:34:36,240
i love getting applications into

00:34:35,359 --> 00:34:38,159
containers

00:34:36,240 --> 00:34:40,399
and i love the question that actually

00:34:38,159 --> 00:34:42,159
prompted this talk how do i get the rest

00:34:40,399 --> 00:34:44,240
of my app into kubernetes

00:34:42,159 --> 00:34:46,000
and so to the person that asked me that

00:34:44,240 --> 00:34:49,760
question here's the answer

00:34:46,000 --> 00:34:52,639
it's totally possible

00:34:49,760 --> 00:34:53,359
awesome well uh that was again a really

00:34:52,639 --> 00:34:56,399
great talk

00:34:53,359 --> 00:34:58,640
rob i enjoyed it uh myself um oh oh here

00:34:56,399 --> 00:35:01,119
we have one more late-breaking question

00:34:58,640 --> 00:35:01,680
craig in the chat asks reasons to have

00:35:01,119 --> 00:35:04,320
secrets

00:35:01,680 --> 00:35:05,440
in cube versus keeping them outside of

00:35:04,320 --> 00:35:08,480
cube like vault

00:35:05,440 --> 00:35:10,800
any comment there rob good question

00:35:08,480 --> 00:35:11,680
i may choose to put secrets inside of

00:35:10,800 --> 00:35:13,920
kubernetes

00:35:11,680 --> 00:35:16,079
because it's really easy i map them in

00:35:13,920 --> 00:35:18,640
the same way that i map a config map

00:35:16,079 --> 00:35:19,839
and so the container doesn't need to be

00:35:18,640 --> 00:35:24,480
any wiser about

00:35:19,839 --> 00:35:27,680
are they secrets or are they config maps

00:35:24,480 --> 00:35:28,800
and so i generally choose um secrets

00:35:27,680 --> 00:35:31,359
inside kubernetes

00:35:28,800 --> 00:35:33,119
for ease of use but i also acknowledge

00:35:31,359 --> 00:35:35,200
that that isn't the safest

00:35:33,119 --> 00:35:36,480
place to put them and in particular with

00:35:35,200 --> 00:35:38,240
secrets

00:35:36,480 --> 00:35:39,680
it would be really nice to be able to

00:35:38,240 --> 00:35:42,640
put them in a

00:35:39,680 --> 00:35:45,359
secret vault in a key vault and so

00:35:42,640 --> 00:35:47,359
whenever possible i do like to move my

00:35:45,359 --> 00:35:48,720
secrets outside of kubernetes into a

00:35:47,359 --> 00:35:51,599
config vault

00:35:48,720 --> 00:35:52,560
but now i need to teach my application

00:35:51,599 --> 00:35:55,119
how to do it

00:35:52,560 --> 00:35:56,240
and there isn't a real great way you

00:35:55,119 --> 00:35:58,240
know i could

00:35:56,240 --> 00:35:59,920
hook up a provider that knew how to

00:35:58,240 --> 00:36:01,599
reach into the vault but now it's

00:35:59,920 --> 00:36:04,400
you know spreading my secrets around

00:36:01,599 --> 00:36:05,599
anyway so at that point i add a whole

00:36:04,400 --> 00:36:08,320
bunch of complexity

00:36:05,599 --> 00:36:11,599
inside each application to be able to

00:36:08,320 --> 00:36:13,520
get at those configuration details

00:36:11,599 --> 00:36:15,200
at that point it's this balancing act do

00:36:13,520 --> 00:36:16,000
i want that additional complexity and

00:36:15,200 --> 00:36:18,480
safety

00:36:16,000 --> 00:36:20,000
or i do i want the simplicity if i'm

00:36:18,480 --> 00:36:22,480
reaching for simplicity

00:36:20,000 --> 00:36:24,079
my secrets will stay inside kubernetes

00:36:22,480 --> 00:36:26,800
if i'm reaching for ultimate

00:36:24,079 --> 00:36:29,040
safety then those secrets will live in a

00:36:26,800 --> 00:36:31,119
key vault outside of kubernetes and i'll

00:36:29,040 --> 00:36:33,839
add the piece into my application

00:36:31,119 --> 00:36:35,920
to go reach out to get them at each

00:36:33,839 --> 00:36:37,040
moment where they're needed

00:36:35,920 --> 00:36:41,839
that was a really thoughtful and

00:36:37,040 --> 00:36:41,839
insightful question thanks craig

00:36:41,920 --> 00:36:45,920
excellent uh well thanks everybody for

00:36:43,920 --> 00:36:54,720
all the questions we will go ahead

00:36:45,920 --> 00:36:54,720

YouTube URL: https://www.youtube.com/watch?v=FdSxM-kfV7U


