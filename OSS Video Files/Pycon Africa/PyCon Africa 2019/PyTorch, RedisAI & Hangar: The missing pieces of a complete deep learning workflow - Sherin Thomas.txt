Title: PyTorch, RedisAI & Hangar: The missing pieces of a complete deep learning workflow - Sherin Thomas
Publication date: 2019-11-23
Playlist: PyCon Africa 2019
Description: 
	Managing DL workflow is always a nightmare. Problems include handling the scale, efficient resource utilization, version controlling the data. With the highly optimized RedisAI, super flexible PyTorch and heavily organized Hangar, all the sleepless nights are stories of the past.
Captions: 
	00:00:02,600 --> 00:00:07,649
other folks

00:00:04,740 --> 00:00:10,530
good morning thank you so much for being

00:00:07,649 --> 00:00:13,259
here I'm like super excited and honored

00:00:10,530 --> 00:00:15,599
to be part of first-ever bicone Africa

00:00:13,259 --> 00:00:19,470
and kudos to all of you and the

00:00:15,599 --> 00:00:23,340
organizers for doing this am i audible

00:00:19,470 --> 00:00:30,449
everywhere like it's Mike working no it

00:00:23,340 --> 00:00:32,790
is working yeah yeah so I'm sure in

00:00:30,449 --> 00:00:34,860
Thomas and I'm gonna talk about hanger

00:00:32,790 --> 00:00:37,350
fighters and ready say I don't worry

00:00:34,860 --> 00:00:40,020
worry about this new term terminologies

00:00:37,350 --> 00:00:41,940
that you are listening today because I'm

00:00:40,020 --> 00:00:46,200
gonna get take you through each of this

00:00:41,940 --> 00:00:48,629
and trying to make you understand why or

00:00:46,200 --> 00:00:52,800
what a deepening workflow is and why is

00:00:48,629 --> 00:00:56,190
that important before jumping in I want

00:00:52,800 --> 00:00:59,489
to understand the audience how many of

00:00:56,190 --> 00:01:03,480
you guys are not Python developers never

00:00:59,489 --> 00:01:08,369
use Python before nobody very minimal

00:01:03,480 --> 00:01:10,170
Python knowledge like very basic no 1 2

00:01:08,369 --> 00:01:14,490
3 don't be shy that's okay

00:01:10,170 --> 00:01:16,110
4 or 5 oh wow ok it's like 20% of the

00:01:14,490 --> 00:01:18,810
audience cool

00:01:16,110 --> 00:01:20,820
how many of you guys out of nard or like

00:01:18,810 --> 00:01:25,400
never done any deep learning stuff so

00:01:20,820 --> 00:01:32,759
far anything in deep learning never done

00:01:25,400 --> 00:01:35,460
ok so for the people who have used or

00:01:32,759 --> 00:01:39,360
did something in deep learning have you

00:01:35,460 --> 00:01:41,640
guys done traditional ml or tensile for

00:01:39,360 --> 00:01:46,920
do or PI torch deal or some other

00:01:41,640 --> 00:01:49,340
frameworks hands for those who have done

00:01:46,920 --> 00:01:52,909
traditionally mother only traditional ml

00:01:49,340 --> 00:01:57,110
ok tensorflow

00:01:52,909 --> 00:02:03,410
pi torch 3

00:01:57,110 --> 00:02:05,870
okay cool so you have listened to one

00:02:03,410 --> 00:02:07,970
talk previously which was or almost all

00:02:05,870 --> 00:02:09,950
the talk that you cannot listen to in

00:02:07,970 --> 00:02:12,520
the real world about deep learning is

00:02:09,950 --> 00:02:16,310
how you build a deep learning model and

00:02:12,520 --> 00:02:17,810
how you can train it how we can you know

00:02:16,310 --> 00:02:20,270
run through a loop and train your

00:02:17,810 --> 00:02:24,380
network and build it build layers of

00:02:20,270 --> 00:02:27,170
your network but almost always you don't

00:02:24,380 --> 00:02:29,210
get to listen to the whole deep learning

00:02:27,170 --> 00:02:31,910
workflow which is what you actually need

00:02:29,210 --> 00:02:34,010
to implement something from scratch if

00:02:31,910 --> 00:02:36,560
you have an idea how you can take that

00:02:34,010 --> 00:02:37,310
to a process where you can deploy that

00:02:36,560 --> 00:02:40,330
to production

00:02:37,310 --> 00:02:42,650
so the talk I'm gonna cover is

00:02:40,330 --> 00:02:44,720
through-hole those process but not

00:02:42,650 --> 00:02:47,060
probably covering every aspect of it

00:02:44,720 --> 00:02:49,550
because of the time limitation and the

00:02:47,060 --> 00:02:51,830
core is the the whole slide is sort of

00:02:49,550 --> 00:02:54,290
code engine see so if you are low

00:02:51,830 --> 00:02:56,420
somewhere I'm happy to host 30 minutes

00:02:54,290 --> 00:02:58,550
session after the lunch or something if

00:02:56,420 --> 00:03:00,590
there's a group of people who want to

00:02:58,550 --> 00:03:01,880
you know actually run the code or want

00:03:00,590 --> 00:03:08,680
me to explain the code and run you

00:03:01,880 --> 00:03:08,680
through it it's not working

00:03:16,030 --> 00:03:21,010
sorry okay

00:03:18,140 --> 00:03:21,010
I'll probably use the

00:03:25,790 --> 00:03:31,189
okay no it's working cool so yeah this

00:03:29,239 --> 00:03:34,010
mean I'm a two-second

00:03:31,189 --> 00:03:38,420
H H is easy or nd everywhere t to our

00:03:34,010 --> 00:03:42,010
github linkedin so if so as part of the

00:03:38,420 --> 00:03:44,510
community service we are we are doing a

00:03:42,010 --> 00:03:47,299
college training of deep learning /

00:03:44,510 --> 00:03:50,299
python and part of that i'm we are

00:03:47,299 --> 00:03:53,659
planning to start by next month a deep

00:03:50,299 --> 00:03:55,220
learning weekly tips which will be sent

00:03:53,659 --> 00:03:58,069
through my Twitter account and through

00:03:55,220 --> 00:03:59,840
tor network door day I slash tips so if

00:03:58,069 --> 00:04:02,569
you are new to deep learning and if you

00:03:59,840 --> 00:04:05,359
want to learn tips like that that could

00:04:02,569 --> 00:04:07,579
take you further you can watch my tutor

00:04:05,359 --> 00:04:11,810
handle so we're gonna be planning to

00:04:07,579 --> 00:04:13,760
start by starting off September so I

00:04:11,810 --> 00:04:18,079
work a tensor work which is a super

00:04:13,760 --> 00:04:20,750
early stage startup me and my CEO is

00:04:18,079 --> 00:04:22,940
from Italy my CTO is from New York so we

00:04:20,750 --> 00:04:25,970
just three people working on couple of

00:04:22,940 --> 00:04:28,100
toolkits that we think are relevant to

00:04:25,970 --> 00:04:30,020
the community so it all of this I all

00:04:28,100 --> 00:04:32,510
this ideas came from the frustration

00:04:30,020 --> 00:04:35,960
that we had when we were doing deep

00:04:32,510 --> 00:04:37,910
learning for work so I create a Turing

00:04:35,960 --> 00:04:41,449
network toward AI again that's an

00:04:37,910 --> 00:04:43,940
exploration - it's not for public use

00:04:41,449 --> 00:04:45,530
yet we are trying to I'm trying to find

00:04:43,940 --> 00:04:48,500
resources who could help me out with you

00:04:45,530 --> 00:04:50,539
know pushing it further I co-organized

00:04:48,500 --> 00:04:54,020
deep learning Bangalore which is one of

00:04:50,539 --> 00:04:56,110
the most effective and popular made deep

00:04:54,020 --> 00:05:00,110
learning meetup in Bangalore

00:04:56,110 --> 00:05:02,300
yeah so that's about me before you know

00:05:00,110 --> 00:05:04,070
going to the whole idea of people owning

00:05:02,300 --> 00:05:07,880
workflow I want to introduce you to

00:05:04,070 --> 00:05:10,130
software 2.0 has anybody heard the storm

00:05:07,880 --> 00:05:12,800
software 2.0 No

00:05:10,130 --> 00:05:16,940
okay awesome so anyone know and record

00:05:12,800 --> 00:05:19,370
for the director of AI Tesla no has

00:05:16,940 --> 00:05:22,070
anyone done cs2 31 n from Stanford the

00:05:19,370 --> 00:05:24,680
course it's available in YouTube okay so

00:05:22,070 --> 00:05:27,919
before jumping in if you if you are a

00:05:24,680 --> 00:05:29,990
deep learning if you want to be a deep

00:05:27,919 --> 00:05:33,380
learning engineer it's a good start to

00:05:29,990 --> 00:05:35,750
go to CS 231 n you can YouTube it you

00:05:33,380 --> 00:05:38,180
get like you know a bunch of video 121

00:05:35,750 --> 00:05:39,470
videos it's actually started the video

00:05:38,180 --> 00:05:41,390
streaming was started by and

00:05:39,470 --> 00:05:44,120
record worthy he's the director of ai+

00:05:41,390 --> 00:05:45,980
lab so he defined the term software 2.0

00:05:44,120 --> 00:05:46,880
I'm not sure whether there were any

00:05:45,980 --> 00:05:50,810
other atoms

00:05:46,880 --> 00:05:54,200
to do that before so software 2.0 as per

00:05:50,810 --> 00:05:56,630
definition is basically what we had is

00:05:54,200 --> 00:05:58,820
software 1.0 the components of software

00:05:56,630 --> 00:06:01,070
software 1.0 is here right you have

00:05:58,820 --> 00:06:05,450
source code compiler executables virtual

00:06:01,070 --> 00:06:08,900
virtual machine may be the test cases so

00:06:05,450 --> 00:06:11,450
after 2.0 says this is going to be

00:06:08,900 --> 00:06:14,950
replaced by couple of other things the

00:06:11,450 --> 00:06:17,780
alternatives and which is probably or

00:06:14,950 --> 00:06:19,490
fundamentally driven by data right so

00:06:17,780 --> 00:06:21,680
the source code is going to be replaced

00:06:19,490 --> 00:06:23,660
by data because earlier used to you used

00:06:21,680 --> 00:06:26,090
to write you know conditions for loops

00:06:23,660 --> 00:06:28,190
extra etc but now it's gonna be changed

00:06:26,090 --> 00:06:30,919
by decision that's been made by the data

00:06:28,190 --> 00:06:33,380
through training process right so that's

00:06:30,919 --> 00:06:35,510
idea so all of this components has an

00:06:33,380 --> 00:06:37,100
alternative it's like a very long blog

00:06:35,510 --> 00:06:38,570
post made by him you should check it out

00:06:37,100 --> 00:06:39,560
off to a two-pointer google it you

00:06:38,570 --> 00:06:44,660
should find you should be able to find

00:06:39,560 --> 00:06:48,590
it so software 2.0 which is what we call

00:06:44,660 --> 00:06:51,410
us the AI first world just not have

00:06:48,590 --> 00:06:56,000
enough tools or the tools required you

00:06:51,410 --> 00:06:59,450
to build software 2.0 right so could you

00:06:56,000 --> 00:07:02,810
guys imagine if you did not have github

00:06:59,450 --> 00:07:04,400
and you programming right now it's it's

00:07:02,810 --> 00:07:07,120
not possible for us to imagine because

00:07:04,400 --> 00:07:10,010
that's so part of our daily workflow

00:07:07,120 --> 00:07:11,419
when you do software 1.0 so similar to

00:07:10,010 --> 00:07:13,430
that there's a bunch of toolkits like

00:07:11,419 --> 00:07:16,520
Jenkins ansible like a bunch bunch of

00:07:13,430 --> 00:07:19,760
toolkits that is helping you to build

00:07:16,520 --> 00:07:22,640
software in 1.0 world but do you have

00:07:19,760 --> 00:07:24,740
anything to do that in software 2.0 so

00:07:22,640 --> 00:07:27,440
I'm referring to the talk mustafa has

00:07:24,740 --> 00:07:31,070
said Esther given yesterday you have

00:07:27,440 --> 00:07:33,740
certain tool kits available use those we

00:07:31,070 --> 00:07:36,260
do have certain tool kits available but

00:07:33,740 --> 00:07:38,660
we don't have all the tool kits required

00:07:36,260 --> 00:07:41,690
for doing software 2.0 when you you know

00:07:38,660 --> 00:07:45,650
move from software 1.0 to 2.0 so tends

00:07:41,690 --> 00:07:48,410
to work exists - like I said through the

00:07:45,650 --> 00:07:50,990
frustrations that we had while we are

00:07:48,410 --> 00:07:52,850
developing software 2.0 with the tools

00:07:50,990 --> 00:07:54,500
that we had for software 1.0

00:07:52,850 --> 00:07:57,100
and one typical example that I could

00:07:54,500 --> 00:08:00,410
give is where do you save your data

00:07:57,100 --> 00:08:03,920
right so all this deep learning machine

00:08:00,410 --> 00:08:05,930
learning stuff starts by where it starts

00:08:03,920 --> 00:08:08,030
where you collect the data you have a

00:08:05,930 --> 00:08:10,340
lot of data then you clean that data

00:08:08,030 --> 00:08:11,690
then you use that data to train a

00:08:10,340 --> 00:08:15,800
network that you have built

00:08:11,690 --> 00:08:18,770
correct everyone agrees ok so where do

00:08:15,800 --> 00:08:21,440
you keep this data where do we keep

00:08:18,770 --> 00:08:23,600
source code some Washington drawing

00:08:21,440 --> 00:08:26,870
right so that we know we're not missing

00:08:23,600 --> 00:08:27,470
it so if you if you don't understand

00:08:26,870 --> 00:08:29,420
what I'm saying

00:08:27,470 --> 00:08:33,320
raise your hands I'm super happy to

00:08:29,420 --> 00:08:36,140
explain it again so what we did was we

00:08:33,320 --> 00:08:39,940
put this data into git LFS that stores

00:08:36,140 --> 00:08:43,480
you know bigger files like blobs so

00:08:39,940 --> 00:08:46,490
putting image to get is not really ideal

00:08:43,480 --> 00:08:48,410
so we try to use the existing toolkits

00:08:46,490 --> 00:08:50,930
which is developed for software 1.0 but

00:08:48,410 --> 00:08:53,030
that never worked out so similar to that

00:08:50,930 --> 00:08:56,120
there were other frustration points that

00:08:53,030 --> 00:08:57,260
led us to build the tensor work and the

00:08:56,120 --> 00:09:01,160
tool kits that we are building is

00:08:57,260 --> 00:09:03,530
basically for handling such issues this

00:09:01,160 --> 00:09:05,360
is the few that we are developing hangar

00:09:03,530 --> 00:09:08,150
is the version controlling that I'm

00:09:05,360 --> 00:09:11,270
talking about in this oppression which

00:09:08,150 --> 00:09:12,590
is a iot thing to then substrat a CIC

00:09:11,270 --> 00:09:16,190
the automation so if you have used

00:09:12,590 --> 00:09:17,840
Jenkins or ansible you probably have to

00:09:16,190 --> 00:09:20,480
replace that with something like

00:09:17,840 --> 00:09:23,780
subscribe then ready say I is for

00:09:20,480 --> 00:09:25,190
deploying your Model T production so I'm

00:09:23,780 --> 00:09:28,700
not gonna talk about Prussian and

00:09:25,190 --> 00:09:31,520
substrat because it doesn't exist it is

00:09:28,700 --> 00:09:33,650
just basically theoretically in our mind

00:09:31,520 --> 00:09:35,300
that we know that we will build this in

00:09:33,650 --> 00:09:38,270
some point in time but we have built

00:09:35,300 --> 00:09:40,460
hangar and ready say I already and I'm

00:09:38,270 --> 00:09:43,400
gonna run you through how to store data

00:09:40,460 --> 00:09:45,350
in hangar how you can take that to a

00:09:43,400 --> 00:09:47,570
world of you know deep learning model

00:09:45,350 --> 00:09:49,970
training through pi torch then from

00:09:47,570 --> 00:09:52,100
there how you can once you have a

00:09:49,970 --> 00:09:56,180
trained model how we can take that from

00:09:52,100 --> 00:09:59,690
you know so ok so give give me some

00:09:56,180 --> 00:10:01,850
examples have you so if you have a deep

00:09:59,690 --> 00:10:03,200
learning model that you trained right

00:10:01,850 --> 00:10:05,480
you have trained a deep learning model

00:10:03,200 --> 00:10:08,330
and you have like 99.9% accuracy

00:10:05,480 --> 00:10:10,580
what do you do next like you have a

00:10:08,330 --> 00:10:12,920
model you have trained it what do you do

00:10:10,580 --> 00:10:15,020
next you have a model let's say you made

00:10:12,920 --> 00:10:17,390
to help farmers right you have a bunch

00:10:15,020 --> 00:10:17,750
of farmers to identify the disease or

00:10:17,390 --> 00:10:20,360
something

00:10:17,750 --> 00:10:21,080
you made a model have you what what do

00:10:20,360 --> 00:10:25,490
you do next

00:10:21,080 --> 00:10:29,960
where will you take it next right wrap

00:10:25,490 --> 00:10:34,670
it with an API and Daniele's build a

00:10:29,960 --> 00:10:36,740
mobile app tensile force or ring cool so

00:10:34,670 --> 00:10:38,450
we have like two options now wrap it

00:10:36,740 --> 00:10:40,280
with an API probably flask or Django or

00:10:38,450 --> 00:10:44,060
something then tons of loss or being

00:10:40,280 --> 00:10:45,650
cool we have two choice okay good let's

00:10:44,060 --> 00:10:48,080
say you are building up a company and

00:10:45,650 --> 00:10:50,270
you're scaling now you're scaling to ten

00:10:48,080 --> 00:10:52,430
thousand users your request for a second

00:10:50,270 --> 00:10:54,170
do you think you can scale the observing

00:10:52,430 --> 00:10:56,360
to that level without introducing a new

00:10:54,170 --> 00:10:57,890
tool kit no you can't you have to

00:10:56,360 --> 00:11:00,500
introduce kubernetes or something

00:10:57,890 --> 00:11:02,540
similar do you think you can scale API

00:11:00,500 --> 00:11:03,890
which is written in flask or Django no

00:11:02,540 --> 00:11:06,080
you can't you have to introduce some new

00:11:03,890 --> 00:11:08,960
toolkits now you now you are in need of

00:11:06,080 --> 00:11:09,170
hiring a new person who could help you

00:11:08,960 --> 00:11:11,990
out

00:11:09,170 --> 00:11:13,220
with scaling it up right with some other

00:11:11,990 --> 00:11:15,800
tool case which you probably don't know

00:11:13,220 --> 00:11:20,090
already or you have to put your ass

00:11:15,800 --> 00:11:22,460
online and learn it then you know take

00:11:20,090 --> 00:11:25,730
it so these are the fundamental problems

00:11:22,460 --> 00:11:27,740
that you're gonna have so ready say II's

00:11:25,730 --> 00:11:29,660
we both ready say I along with you know

00:11:27,740 --> 00:11:31,490
in collaboration with rooibos labs okay

00:11:29,660 --> 00:11:35,750
before going there how many of you don't

00:11:31,490 --> 00:11:37,520
know red is what ready says okay again

00:11:35,750 --> 00:11:40,580
like 10% off the crowd so ready since

00:11:37,520 --> 00:11:43,700
that database in-memory database that

00:11:40,580 --> 00:11:45,380
can store data in the memory and okay I

00:11:43,700 --> 00:11:48,620
have a slide about that will go there

00:11:45,380 --> 00:11:52,160
many okay so this is a typical deep

00:11:48,620 --> 00:11:53,690
learning workflow that I I have defined

00:11:52,160 --> 00:11:57,230
I don't know whether this is what

00:11:53,690 --> 00:11:58,730
everyone use so this the the whole the

00:11:57,230 --> 00:12:00,710
whole process start from the ideation

00:11:58,730 --> 00:12:02,930
and planning phase this is probably

00:12:00,710 --> 00:12:06,320
similar to agile if you have worked with

00:12:02,930 --> 00:12:07,850
agile but not exactly a joy so you have

00:12:06,320 --> 00:12:10,760
an ideation and planning phase that's

00:12:07,850 --> 00:12:12,620
like you have got an idea to build

00:12:10,760 --> 00:12:15,230
something you you have an idea in your

00:12:12,620 --> 00:12:16,790
mind and you are trying to ID it or

00:12:15,230 --> 00:12:19,130
brainstorm with your friends what you

00:12:16,790 --> 00:12:22,610
could do next when you move to design

00:12:19,130 --> 00:12:25,250
experimentation phase you you collect a

00:12:22,610 --> 00:12:26,990
bit of data you try to build a small

00:12:25,250 --> 00:12:29,300
model you try to use an existing model

00:12:26,990 --> 00:12:31,040
available already you train it you see

00:12:29,300 --> 00:12:32,900
whether you are getting some good result

00:12:31,040 --> 00:12:34,940
then you go to the model implementation

00:12:32,900 --> 00:12:37,010
phase where you probably create a new

00:12:34,940 --> 00:12:39,230
model a new architecture itself and then

00:12:37,010 --> 00:12:42,830
you try to use the same small data set

00:12:39,230 --> 00:12:45,590
to you know understand where where your

00:12:42,830 --> 00:12:47,510
accuracies are then you move to training

00:12:45,590 --> 00:12:49,070
and where else you have you have decided

00:12:47,510 --> 00:12:52,760
what your model should be you have

00:12:49,070 --> 00:12:55,490
enough data you implement the model then

00:12:52,760 --> 00:12:57,650
you train and you know train it and then

00:12:55,490 --> 00:12:59,710
validate it now you have a trained super

00:12:57,650 --> 00:13:02,180
accurate model you take it to production

00:12:59,710 --> 00:13:03,950
along with all these phases there's a

00:13:02,180 --> 00:13:06,560
parallel phase running which has couple

00:13:03,950 --> 00:13:08,510
of things to do one is data collection

00:13:06,560 --> 00:13:12,200
data storage another you see unit

00:13:08,510 --> 00:13:17,480
testing and etc and exit drum right ok

00:13:12,200 --> 00:13:21,770
so in this in this graph in this

00:13:17,480 --> 00:13:23,480
flowchart what you already know it ends

00:13:21,770 --> 00:13:25,490
up with tencel floor PI tour just like

00:13:23,480 --> 00:13:28,550
it learn or spark or whatever is

00:13:25,490 --> 00:13:30,650
probably going to be in that design and

00:13:28,550 --> 00:13:32,600
experimentation modern elimination and

00:13:30,650 --> 00:13:34,670
training and validation phase so right

00:13:32,600 --> 00:13:37,220
these three phases are going to be

00:13:34,670 --> 00:13:38,810
covered by what you already know or what

00:13:37,220 --> 00:13:40,820
you are aspiring to learn so we knew

00:13:38,810 --> 00:13:43,220
some when somebody say I want to be a

00:13:40,820 --> 00:13:45,440
deep learning engineer this is what you

00:13:43,220 --> 00:13:47,870
are probably you know in your mind this

00:13:45,440 --> 00:13:50,000
is what you probably want to do covering

00:13:47,870 --> 00:13:52,340
these three phases but what's next and

00:13:50,000 --> 00:13:55,310
what's before right you need that to do

00:13:52,340 --> 00:13:57,110
the just go through this prefaces you

00:13:55,310 --> 00:13:59,330
need once you have the model train you

00:13:57,110 --> 00:14:01,490
have to take it further right what what

00:13:59,330 --> 00:14:05,020
do you do there so we starting with

00:14:01,490 --> 00:14:07,940
hanger hanger is good for data basically

00:14:05,020 --> 00:14:10,430
it has all the features get provided a

00:14:07,940 --> 00:14:13,490
couple of years back to build software

00:14:10,430 --> 00:14:16,400
1.0 you can branch you can create new

00:14:13,490 --> 00:14:18,290
branches you can merge branches you can

00:14:16,400 --> 00:14:20,330
time time travel through the history you

00:14:18,290 --> 00:14:23,810
can go back to an older commit and see

00:14:20,330 --> 00:14:26,330
what was the data clone fetch and push

00:14:23,810 --> 00:14:28,670
you can clone the data we can fetch push

00:14:26,330 --> 00:14:31,760
so basically whatever you could do with

00:14:28,670 --> 00:14:32,640
the gate you can do here right what is

00:14:31,760 --> 00:14:35,730
the necessity

00:14:32,640 --> 00:14:38,250
do you need that so almost always what

00:14:35,730 --> 00:14:40,770
are you gonna do it especially when you

00:14:38,250 --> 00:14:43,680
learn right like in the previous speaker

00:14:40,770 --> 00:14:46,080
said you probably go to Cal go download

00:14:43,680 --> 00:14:48,210
the data then pull them at net morale

00:14:46,080 --> 00:14:50,250
right that's not how real would work you

00:14:48,210 --> 00:14:52,800
are going to get the data from the real

00:14:50,250 --> 00:14:55,710
world probably every week every day

00:14:52,800 --> 00:14:58,080
every second I don't know so you have

00:14:55,710 --> 00:15:00,870
some weight to store this data and some

00:14:58,080 --> 00:15:03,360
way to look at this data and say the

00:15:00,870 --> 00:15:05,520
data that came this week has reduced the

00:15:03,360 --> 00:15:07,800
performance of my model the data has

00:15:05,520 --> 00:15:09,660
came last week has increased the

00:15:07,800 --> 00:15:11,100
performance of my model right so you

00:15:09,660 --> 00:15:13,860
have some way to do that you need

00:15:11,100 --> 00:15:16,530
somebody to do that so with the existing

00:15:13,860 --> 00:15:19,140
tools that you have it's extremely

00:15:16,530 --> 00:15:21,300
difficult and it's the difficulty

00:15:19,140 --> 00:15:23,220
escalates if you have more than one

00:15:21,300 --> 00:15:25,860
source where you get the data from let's

00:15:23,220 --> 00:15:28,020
say you have five people working in five

00:15:25,860 --> 00:15:30,510
universities to collect the data correct

00:15:28,020 --> 00:15:32,960
five people are uploading the data to

00:15:30,510 --> 00:15:35,700
some common storage that you have and

00:15:32,960 --> 00:15:37,770
one finder you realized your model is

00:15:35,700 --> 00:15:40,350
not performing well right and that's

00:15:37,770 --> 00:15:42,120
probably not because of the data from

00:15:40,350 --> 00:15:44,250
five sources probably because of one

00:15:42,120 --> 00:15:46,050
source do you have any way to know that

00:15:44,250 --> 00:15:47,880
no you just push the data to a folder

00:15:46,050 --> 00:15:51,630
and you don't know who pushed that you

00:15:47,880 --> 00:15:54,440
unless you keep a track of it so okay

00:15:51,630 --> 00:15:56,370
another problem that you might be facing

00:15:54,440 --> 00:16:00,780
especially when downloading data from

00:15:56,370 --> 00:16:02,460
Google so I was training a semantic

00:16:00,780 --> 00:16:05,940
segmentation model where you can segment

00:16:02,460 --> 00:16:10,860
the objects in inner frame and the data

00:16:05,940 --> 00:16:13,110
set was 3 TB right so for and I was

00:16:10,860 --> 00:16:14,760
trying it out early stage of my career

00:16:13,110 --> 00:16:17,400
or I was trying it arrived I was trying

00:16:14,760 --> 00:16:19,980
to learn deep learning so for just a

00:16:17,400 --> 00:16:22,020
lawn I have to download 3 TB of data

00:16:19,980 --> 00:16:24,360
which I'm not sure is going to help me

00:16:22,020 --> 00:16:25,530
that's crazy right I probably don't need

00:16:24,360 --> 00:16:28,500
3 TB of data

00:16:25,530 --> 00:16:30,030
so with hangar what we the way we try to

00:16:28,500 --> 00:16:33,030
avoid that problem is you know you can't

00:16:30,030 --> 00:16:36,120
partial clone we can say hangar dude I

00:16:33,030 --> 00:16:38,190
need like doing 200 MB of data

00:16:36,120 --> 00:16:40,380
so hangar take the first 200 MB of data

00:16:38,190 --> 00:16:43,640
and give it to you so that you don't

00:16:40,380 --> 00:16:43,640
have to sorry

00:16:45,940 --> 00:16:53,570
you can tell hanger to do that a random

00:16:50,170 --> 00:16:58,520
download or like Faust whatever yeah we

00:16:53,570 --> 00:17:01,010
can you can do that okay

00:16:58,520 --> 00:17:02,930
so data interaction you have like a

00:17:01,010 --> 00:17:06,740
powerful CI so if you have like a folder

00:17:02,930 --> 00:17:08,780
full of data you can do hangout import

00:17:06,740 --> 00:17:10,819
that folder full of data that's gonna

00:17:08,780 --> 00:17:13,579
get you the data and it has a powerful

00:17:10,819 --> 00:17:17,329
API because the data in hangar are

00:17:13,579 --> 00:17:19,370
number arrays so you can directly so if

00:17:17,329 --> 00:17:21,110
the data is an image what you have to do

00:17:19,370 --> 00:17:23,600
is go through a pre-processing step

00:17:21,110 --> 00:17:25,939
where you read the image and make it as

00:17:23,600 --> 00:17:27,230
a tensor then run through the model but

00:17:25,939 --> 00:17:29,090
with hang or you don't have to do that

00:17:27,230 --> 00:17:30,980
because the data is already tensor when

00:17:29,090 --> 00:17:33,170
you do hang or import you get the tensor

00:17:30,980 --> 00:17:35,030
version of that image or whatever then

00:17:33,170 --> 00:17:36,530
you then use the Python API to Lourdes

00:17:35,030 --> 00:17:40,130
oh can we take the Christians because I

00:17:36,530 --> 00:17:47,360
you know okay were you trying to ask you

00:17:40,130 --> 00:17:49,970
soon right right right so you can pull

00:17:47,360 --> 00:17:52,220
the data from hangar while you run the

00:17:49,970 --> 00:17:54,110
model without doing pre-processing which

00:17:52,220 --> 00:17:55,250
is cool so these are the command-line

00:17:54,110 --> 00:17:58,070
options that you have

00:17:55,250 --> 00:18:01,340
you have branch clone all the options to

00:17:58,070 --> 00:18:03,260
particular I want you to really careful

00:18:01,340 --> 00:18:05,900
about our like really in excited about

00:18:03,260 --> 00:18:07,970
these export and import so you can

00:18:05,900 --> 00:18:09,860
export like I said you can hang our

00:18:07,970 --> 00:18:11,600
import folder full of images hang or

00:18:09,860 --> 00:18:14,210
make sure that is being converted to

00:18:11,600 --> 00:18:17,630
tensor hang our export one image that

00:18:14,210 --> 00:18:19,460
make sure you are exporting one image so

00:18:17,630 --> 00:18:21,290
this is a Python API you import hangar

00:18:19,460 --> 00:18:23,210
import requests tree then you have like

00:18:21,290 --> 00:18:25,910
all operation that you have in you know

00:18:23,210 --> 00:18:27,860
github the port or checkout week and if

00:18:25,910 --> 00:18:32,300
you can see the difference between two

00:18:27,860 --> 00:18:34,010
branches you can so make torch data set

00:18:32,300 --> 00:18:36,290
will create a torch data set with your

00:18:34,010 --> 00:18:38,570
data then you can use that for you know

00:18:36,290 --> 00:18:40,310
training your network hangar is zero

00:18:38,570 --> 00:18:41,990
point one point one now we are pushing

00:18:40,310 --> 00:18:43,970
sudo point to release today or tomorrow

00:18:41,990 --> 00:18:45,170
which like with like law office so most

00:18:43,970 --> 00:18:48,980
of the good features that I've taught

00:18:45,170 --> 00:18:50,240
today is going to be there tomorrow so I

00:18:48,980 --> 00:18:52,490
was trying to push it before I came here

00:18:50,240 --> 00:18:54,620
but you know like I was planning to and

00:18:52,490 --> 00:18:56,570
work on flight but never happens

00:18:54,620 --> 00:18:58,700
sorry sorry about that so you could get

00:18:56,570 --> 00:19:01,640
that tomorrow what we really need from

00:18:58,700 --> 00:19:03,890
you is a star there if you if you are

00:19:01,640 --> 00:19:05,750
interested in hangar if you think this

00:19:03,890 --> 00:19:07,460
is gonna help you in somewhere and if

00:19:05,750 --> 00:19:10,190
you like the project please go there he

00:19:07,460 --> 00:19:11,390
was a star next to spy tours I don't

00:19:10,190 --> 00:19:16,310
have a lot of time like five minutes

00:19:11,390 --> 00:19:17,240
more that's sad okay so you so the only

00:19:16,310 --> 00:19:20,660
three things that I'm gonna introduce

00:19:17,240 --> 00:19:22,670
you to a store store data loader get a

00:19:20,660 --> 00:19:25,120
tour data loader store store Denon the

00:19:22,670 --> 00:19:27,560
whole package of crippled layers for you

00:19:25,120 --> 00:19:29,900
then tour stored Optima that has to

00:19:27,560 --> 00:19:32,600
optimizers so this is a symbol script

00:19:29,900 --> 00:19:34,460
that I have you know make towards data

00:19:32,600 --> 00:19:40,640
set the same function that we have seen

00:19:34,460 --> 00:19:43,840
in the previous slide here so make dots

00:19:40,640 --> 00:19:47,270
that I said you import that from hangar

00:19:43,840 --> 00:19:50,120
then you can ask you can pass the hangar

00:19:47,270 --> 00:19:52,190
data set to create a tour data set then

00:19:50,120 --> 00:19:53,900
you can this is a model that I have like

00:19:52,190 --> 00:19:55,670
I have a sequential model so making a

00:19:53,900 --> 00:19:57,650
model in PI torch is extremely easy

00:19:55,670 --> 00:19:59,720
because almost all the models that you

00:19:57,650 --> 00:20:01,730
need is already available sequential

00:19:59,720 --> 00:20:03,590
linear rally so all those you are

00:20:01,730 --> 00:20:05,540
stacked those together you have the

00:20:03,590 --> 00:20:08,450
model ready you have the lost ready

00:20:05,540 --> 00:20:10,490
you have the optimizer ready then you

00:20:08,450 --> 00:20:12,350
build a dynamic graph this this is where

00:20:10,490 --> 00:20:14,660
things get different from tensorflow to

00:20:12,350 --> 00:20:16,880
PI torch so if you like if you are a new

00:20:14,660 --> 00:20:18,740
person to deep learning world I would

00:20:16,880 --> 00:20:20,390
encourage you to go and check out by

00:20:18,740 --> 00:20:21,070
torch instead of starting with

00:20:20,390 --> 00:20:23,960
tensorflow

00:20:21,070 --> 00:20:24,890
because PI Taj is extremely easy for AC

00:20:23,960 --> 00:20:26,840
and beginner-friendly

00:20:24,890 --> 00:20:28,370
and once you know the basics of why

00:20:26,840 --> 00:20:30,170
tours you probably can move to ten to

00:20:28,370 --> 00:20:34,520
the floor for production deployments and

00:20:30,170 --> 00:20:35,960
stuff but start with by George okay so

00:20:34,520 --> 00:20:37,790
once you have the PI tours model you

00:20:35,960 --> 00:20:40,460
have trained it this is the training

00:20:37,790 --> 00:20:43,400
loop you trained it then you convert

00:20:40,460 --> 00:20:46,040
that to an optimized version by doing

00:20:43,400 --> 00:20:49,280
tours dot G dot race so there is a new

00:20:46,040 --> 00:20:51,560
version of tours released yesterday that

00:20:49,280 --> 00:20:53,390
has a much better version of a min much

00:20:51,560 --> 00:20:55,970
yeah much much better

00:20:53,390 --> 00:20:57,980
API to do this you know conversion it's

00:20:55,970 --> 00:21:00,470
like you have a model which is trained

00:20:57,980 --> 00:21:03,200
which is in the Python scope now we have

00:21:00,470 --> 00:21:05,000
to see lies that disk and then load that

00:21:03,200 --> 00:21:07,280
into a production run time so you can

00:21:05,000 --> 00:21:08,070
use this so traced you create a trace

00:21:07,280 --> 00:21:11,340
graph or

00:21:08,070 --> 00:21:14,400
scripted graph then graph dot save you

00:21:11,340 --> 00:21:16,020
can save it to you know the disk ready

00:21:14,400 --> 00:21:17,850
to say I okay so this is where like

00:21:16,020 --> 00:21:21,750
you've seen sorry this is where like

00:21:17,850 --> 00:21:23,310
your sin comes oh okay

00:21:21,750 --> 00:21:25,020
what are the production strategies the

00:21:23,310 --> 00:21:27,390
the question I have asked different

00:21:25,020 --> 00:21:29,520
approaches but none of this are standard

00:21:27,390 --> 00:21:31,680
and of this are efficient enough to

00:21:29,520 --> 00:21:34,800
handle all the requirements that you

00:21:31,680 --> 00:21:36,690
have okay so for answering your question

00:21:34,800 --> 00:21:38,430
about Redis or yeah for those who

00:21:36,690 --> 00:21:42,150
doesn't know what ready sees where this

00:21:38,430 --> 00:21:45,090
was top on the developers favorite

00:21:42,150 --> 00:21:48,180
database in last three consecutive use

00:21:45,090 --> 00:21:52,820
by Stack Overflow survey so that is the

00:21:48,180 --> 00:21:55,350
you know the the range of or the the

00:21:52,820 --> 00:21:57,510
size of the community that uses read is

00:21:55,350 --> 00:22:00,000
so ready store popular because of

00:21:57,510 --> 00:22:01,980
several reasons it's extremely fast it

00:22:00,000 --> 00:22:04,020
can scale to a million billion requests

00:22:01,980 --> 00:22:05,760
per second without you doing a lot of

00:22:04,020 --> 00:22:07,710
work you just have to plug different

00:22:05,760 --> 00:22:08,760
nodes to this and say this is my cluster

00:22:07,710 --> 00:22:10,080
do whatever you want

00:22:08,760 --> 00:22:14,040
everybody's automatically scales and

00:22:10,080 --> 00:22:16,740
make sure your so one doesn't go down so

00:22:14,040 --> 00:22:18,510
we used Redis the reason why we use

00:22:16,740 --> 00:22:21,600
Redis is because of all these cool

00:22:18,510 --> 00:22:25,260
features where this has so what we do

00:22:21,600 --> 00:22:28,620
with Redis is we took tensor flows back

00:22:25,260 --> 00:22:30,270
end by toss back end and or in our next

00:22:28,620 --> 00:22:32,130
one time which is made by Microsoft

00:22:30,270 --> 00:22:35,280
just these three back ends and plugged

00:22:32,130 --> 00:22:37,830
that to Redis so now Redis is not just a

00:22:35,280 --> 00:22:39,990
database with ready say I writes can't

00:22:37,830 --> 00:22:41,850
run your deep learning model inside the

00:22:39,990 --> 00:22:43,530
database right since reduces the

00:22:41,850 --> 00:22:47,160
database you can store the data in Redis

00:22:43,530 --> 00:22:49,320
and you can run your model inside where

00:22:47,160 --> 00:22:52,050
your data lives which will save you you

00:22:49,320 --> 00:22:53,700
know millions of millions in cost if you

00:22:52,050 --> 00:22:57,060
have like lot of requests that's

00:22:53,700 --> 00:22:59,100
happening this is already say I github

00:22:57,060 --> 00:23:01,050
so both of the products are and by

00:22:59,100 --> 00:23:02,610
towards all three are open source feel

00:23:01,050 --> 00:23:05,250
free to contribute feel free to raise

00:23:02,610 --> 00:23:07,320
issues if you have any again we i we

00:23:05,250 --> 00:23:09,650
really appreciate if you can give us a

00:23:07,320 --> 00:23:09,650
star

00:23:18,029 --> 00:23:22,570
so ready say is not another Redis you

00:23:21,009 --> 00:23:24,309
plug that and do it extra sting gratis

00:23:22,570 --> 00:23:28,629
yeah it's a module it so much so I have

00:23:24,309 --> 00:23:32,320
I have an example you can do that right

00:23:28,629 --> 00:23:33,779
right exactly exactly so lettuce has 10

00:23:32,320 --> 00:23:36,459
to 4 pie doors in our necks and

00:23:33,779 --> 00:23:38,919
basically all this you know likes I

00:23:36,459 --> 00:23:40,179
could learn spark all of this the models

00:23:38,919 --> 00:23:42,700
built in those framework can be

00:23:40,179 --> 00:23:46,059
converted to or in our necks so we have

00:23:42,700 --> 00:23:47,799
made a package called ml to RT m l2 r T

00:23:46,059 --> 00:23:49,599
is a package that can help you convert

00:23:47,799 --> 00:23:54,219
your model to ion X and then deploy

00:23:49,599 --> 00:23:55,419
through this ai so this is steps so if

00:23:54,219 --> 00:23:57,579
you have set up cuban at ease you know

00:23:55,419 --> 00:23:59,919
how difficult it could be to understand

00:23:57,579 --> 00:24:02,320
the internals but this is the only thing

00:23:59,919 --> 00:24:04,479
that you have to know like I don't know

00:24:02,320 --> 00:24:07,779
20 lines or like 15 lines of shell

00:24:04,479 --> 00:24:09,729
commands so first set 6-7 line is how to

00:24:07,779 --> 00:24:12,159
install Redis so once you have read is

00:24:09,729 --> 00:24:14,139
you install ready say I that's like

00:24:12,159 --> 00:24:16,059
another six seven lines then this is the

00:24:14,139 --> 00:24:17,409
last line so readies - server is the

00:24:16,059 --> 00:24:19,269
command that you give to bring up a

00:24:17,409 --> 00:24:21,549
ready server but along with that if you

00:24:19,269 --> 00:24:24,339
pass lot more you already say already

00:24:21,549 --> 00:24:27,039
say I so it's a share yeah it's a shared

00:24:24,339 --> 00:24:29,019
object so once you do that you already

00:24:27,039 --> 00:24:32,049
say I service up and running without you

00:24:29,019 --> 00:24:34,299
know you do much of a hassle so this is

00:24:32,049 --> 00:24:37,359
architecture of Redis it can run on CPU

00:24:34,299 --> 00:24:40,029
can run or GPU it can run on GPU we can

00:24:37,359 --> 00:24:43,559
we are trying to integrate couple more -

00:24:40,029 --> 00:24:46,539
MO - couple more backends so ready say I

00:24:43,559 --> 00:24:48,879
read is never suppose people who know

00:24:46,539 --> 00:24:51,519
read is already ready just never had a

00:24:48,879 --> 00:24:53,950
data type tensor with ready say I we had

00:24:51,519 --> 00:24:55,989
introduced Reddy's tensor acid era type

00:24:53,950 --> 00:24:58,029
and then you know those backends for

00:24:55,989 --> 00:25:00,489
running the model so this is a typical

00:24:58,029 --> 00:25:02,859
architecture you have something called

00:25:00,489 --> 00:25:05,109
script which I'm not covering today you

00:25:02,859 --> 00:25:07,570
deploy the models there you can run the

00:25:05,109 --> 00:25:11,289
whole process and the process not look

00:25:07,570 --> 00:25:12,909
like this so this is very eco system

00:25:11,289 --> 00:25:15,309
where you know like one run off the

00:25:12,909 --> 00:25:19,659
particular a good feature of Redis so

00:25:15,309 --> 00:25:22,479
this is your typical setup we have four

00:25:19,659 --> 00:25:23,530
machines with one master and three

00:25:22,479 --> 00:25:25,330
slaves

00:25:23,530 --> 00:25:27,480
and let's say your master went down

00:25:25,330 --> 00:25:29,680
right your master has had going down

00:25:27,480 --> 00:25:32,590
everyone can everyone listen to me

00:25:29,680 --> 00:25:35,220
I mean am i audible oh cool so your must

00:25:32,590 --> 00:25:37,660
have gone down in a typical scenario

00:25:35,220 --> 00:25:39,940
unless you have kubernetes you probably

00:25:37,660 --> 00:25:42,340
the DevOps engineer in the mid night

00:25:39,940 --> 00:25:44,290
take taking a call saying the mastery so

00:25:42,340 --> 00:25:45,340
things are not working he goes down and

00:25:44,290 --> 00:25:47,290
bring it up right

00:25:45,340 --> 00:25:49,690
but what readies could do is right is

00:25:47,290 --> 00:25:51,100
there something already Sentinel what

00:25:49,690 --> 00:25:52,990
ready Sentinel could do is it

00:25:51,100 --> 00:25:55,420
automatically elects something else one

00:25:52,990 --> 00:25:57,280
other notice master and you know bring

00:25:55,420 --> 00:25:59,560
up the all infra without you worrying

00:25:57,280 --> 00:26:01,420
about anything stick no users would

00:25:59,560 --> 00:26:03,130
never know anything going down and you

00:26:01,420 --> 00:26:05,290
would probably get an alert saying dude

00:26:03,130 --> 00:26:08,560
this your mission had gone down bring up

00:26:05,290 --> 00:26:10,750
that machine which is down advantages

00:26:08,560 --> 00:26:12,490
not going there this is a Python example

00:26:10,750 --> 00:26:13,870
there's a package so that similar ml -

00:26:12,490 --> 00:26:16,300
Artie the converter that I was talking

00:26:13,870 --> 00:26:18,700
about then ready say I then you know

00:26:16,300 --> 00:26:20,590
this R so I've written a blog post at

00:26:18,700 --> 00:26:22,900
explaining ready say I we can go through

00:26:20,590 --> 00:26:30,340
it and try to understand how does it

00:26:22,900 --> 00:26:32,620
work yep so I'm done I'm super hopeful

00:26:30,340 --> 00:26:34,510
about your feedbacks and if you have

00:26:32,620 --> 00:26:36,880
gone if your ideas that you want to

00:26:34,510 --> 00:26:38,710
contribute go to github create an issue

00:26:36,880 --> 00:26:40,210
somebody would definitely reply to you

00:26:38,710 --> 00:26:41,860
so I guess it'd be three people are

00:26:40,210 --> 00:26:44,200
working from three time zones so you'll

00:26:41,860 --> 00:26:45,490
get support 24/7 some of us will answer

00:26:44,200 --> 00:26:48,070
for sure

00:26:45,490 --> 00:26:50,520
github stars of course yeah thank you so

00:26:48,070 --> 00:26:59,789
much for having me it's a great pleasure

00:26:50,520 --> 00:26:59,789

YouTube URL: https://www.youtube.com/watch?v=PLrqZwTJt5g


