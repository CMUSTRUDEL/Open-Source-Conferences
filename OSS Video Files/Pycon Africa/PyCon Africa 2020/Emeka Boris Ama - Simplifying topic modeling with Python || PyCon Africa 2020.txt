Title: Emeka Boris Ama - Simplifying topic modeling with Python || PyCon Africa 2020
Publication date: 2020-08-29
Playlist: PyCon Africa 2020
Description: 
	It is said that over 2.5 quadrillions of data are being generated every second and obviously over time, it will become difficult to access the data. We apparently need tools to help us organise, search and digest quality information.


Topic modelling aids us to comprehend and extract useful insight from Large textual data, also it helps to find a group of the word(topic) from a document.

In this session, we will take through understanding topic modelling techniques.
Captions: 
	00:00:15,440 --> 00:00:20,640
okay hi everyone

00:00:17,039 --> 00:00:22,800
uh welcome to the section today

00:00:20,640 --> 00:00:24,400
i guess everyone can hear me so if you

00:00:22,800 --> 00:00:25,039
can hear me you can just go ahead and

00:00:24,400 --> 00:00:28,080
type

00:00:25,039 --> 00:00:31,359
um yeah okay

00:00:28,080 --> 00:00:32,640
great okay so uh basically we are going

00:00:31,359 --> 00:00:35,600
to be talking about

00:00:32,640 --> 00:00:35,600
topic modeling

00:00:37,920 --> 00:00:42,559
and in this section i'm going to talk

00:00:40,879 --> 00:00:42,879
about myself first of all before i move

00:00:42,559 --> 00:00:44,879
into

00:00:42,879 --> 00:00:47,360
topic modeling so i'm a gamer i'm a

00:00:44,879 --> 00:00:50,480
youtuber and a data scientist working

00:00:47,360 --> 00:00:52,640
at data ability lab and basically what i

00:00:50,480 --> 00:00:55,039
do at data delete to lab is uh

00:00:52,640 --> 00:00:55,920
basically top modeling on parliamentary

00:00:55,039 --> 00:00:58,480
views

00:00:55,920 --> 00:01:00,239
and we're working on digitalizing

00:00:58,480 --> 00:01:01,680
legislative number of men so we're

00:01:00,239 --> 00:01:02,719
basically looking at three african

00:01:01,680 --> 00:01:05,760
countries

00:01:02,719 --> 00:01:09,040
and so much more i can't say

00:01:05,760 --> 00:01:09,760
so um i'm the co-organizer of efs level

00:01:09,040 --> 00:01:12,320
nigeria

00:01:09,760 --> 00:01:14,000
and also protector scrub yeah so yeah

00:01:12,320 --> 00:01:16,560
effective in nigeria is one of the

00:01:14,000 --> 00:01:17,360
amazing events that happened in nigeria

00:01:16,560 --> 00:01:20,799
and only

00:01:17,360 --> 00:01:23,759
the biblically busy gap between

00:01:20,799 --> 00:01:25,119
the researchers the ai researchers and

00:01:23,759 --> 00:01:27,840
industry experts

00:01:25,119 --> 00:01:28,560
also patakos group of ai uh which will

00:01:27,840 --> 00:01:30,640
help

00:01:28,560 --> 00:01:31,920
to educate people about data science

00:01:30,640 --> 00:01:33,119
artificial intelligence and machine

00:01:31,920 --> 00:01:34,799
learning for free

00:01:33,119 --> 00:01:36,880
uh yeah we're actually going global this

00:01:34,799 --> 00:01:38,240
time away from potaka city which is one

00:01:36,880 --> 00:01:41,280
of the most amazing city in

00:01:38,240 --> 00:01:43,360
nigeria then i'm also an ibm champion

00:01:41,280 --> 00:01:44,640
on analytics so you can also follow me

00:01:43,360 --> 00:01:46,560
on youtube at

00:01:44,640 --> 00:01:47,840
dinner night and on twitter at

00:01:46,560 --> 00:01:50,880
mkhabarris

00:01:47,840 --> 00:01:53,280
yeah right there uh so basically a few

00:01:50,880 --> 00:01:54,159
a few days ago i came across this

00:01:53,280 --> 00:01:57,680
particular

00:01:54,159 --> 00:01:59,360
i'm sure everybody can hear me okay good

00:01:57,680 --> 00:02:00,799
a few days ago i came across this by the

00:01:59,360 --> 00:02:03,680
cloud suite and

00:02:00,799 --> 00:02:04,079
somebody was like oh twitter algorithm

00:02:03,680 --> 00:02:07,840
has

00:02:04,079 --> 00:02:10,879
come again you know basically

00:02:07,840 --> 00:02:13,840
showing you two tweets that

00:02:10,879 --> 00:02:16,239
are basically the same unlike the same

00:02:13,840 --> 00:02:20,080
body context actually the similarity

00:02:16,239 --> 00:02:20,800
in the context okay so this particular

00:02:20,080 --> 00:02:24,000
tweet

00:02:20,800 --> 00:02:26,720
is uh from jen f

00:02:24,000 --> 00:02:28,560
harry kristen and sierra so sierra

00:02:26,720 --> 00:02:30,400
screenshotted this i was like ah

00:02:28,560 --> 00:02:32,480
twitter has shared stuff you know and

00:02:30,400 --> 00:02:33,360
stuff like that so basically what you

00:02:32,480 --> 00:02:35,760
can see here

00:02:33,360 --> 00:02:38,959
is a very good application of topic

00:02:35,760 --> 00:02:41,120
modeling in social media

00:02:38,959 --> 00:02:42,480
in social media industry basically um

00:02:41,120 --> 00:02:44,959
chat and stuff like that

00:02:42,480 --> 00:02:45,519
so if you notice there's a similarity

00:02:44,959 --> 00:02:48,319
between

00:02:45,519 --> 00:02:49,040
this particular truth so the context in

00:02:48,319 --> 00:02:51,440
this tweet

00:02:49,040 --> 00:02:52,239
and then similarity in the topics

00:02:51,440 --> 00:02:55,200
exactly

00:02:52,239 --> 00:02:56,319
so now let's move over to the topic

00:02:55,200 --> 00:02:58,800
modeling part of it

00:02:56,319 --> 00:02:59,920
so what is topic modeling i'm sure you

00:02:58,800 --> 00:03:01,680
must have been hearing about

00:02:59,920 --> 00:03:03,120
it for a long time topic modeling and

00:03:01,680 --> 00:03:07,120
other stuff like that so

00:03:03,120 --> 00:03:07,120
basically subject modeling is

00:03:08,159 --> 00:03:12,080
which everyone can hear me basically

00:03:10,080 --> 00:03:15,360
topic modeling is

00:03:12,080 --> 00:03:17,040
a techniques okay basically an

00:03:15,360 --> 00:03:18,959
unsupervised learning techniques that

00:03:17,040 --> 00:03:21,519
analyze a large document

00:03:18,959 --> 00:03:22,400
before i move into that um we all know

00:03:21,519 --> 00:03:24,239
that over

00:03:22,400 --> 00:03:25,680
2.5 quadrillion of data has been

00:03:24,239 --> 00:03:29,120
generated every day

00:03:25,680 --> 00:03:30,959
and over time you get to see a large

00:03:29,120 --> 00:03:32,959
amount a large volume of these have been

00:03:30,959 --> 00:03:36,080
generated from the internet

00:03:32,959 --> 00:03:38,560
and also from what you do every day

00:03:36,080 --> 00:03:39,360
so it's important that we have the

00:03:38,560 --> 00:03:42,319
techniques

00:03:39,360 --> 00:03:43,840
or algorithms that help us analyze this

00:03:42,319 --> 00:03:45,200
large document

00:03:43,840 --> 00:03:47,440
that's where topic modeling

00:03:45,200 --> 00:03:48,000
contemplation so basically what topic

00:03:47,440 --> 00:03:51,040
modelling

00:03:48,000 --> 00:03:55,120
is is that it analyze large document and

00:03:51,040 --> 00:03:58,560
also find topics from a group of words

00:03:55,120 --> 00:04:00,879
for instance if we have word like hello

00:03:58,560 --> 00:04:02,000
how are you i have another word like

00:04:00,879 --> 00:04:05,840
this is the boy

00:04:02,000 --> 00:04:08,000
we have another word like i am good

00:04:05,840 --> 00:04:09,760
so there's a similarity in this

00:04:08,000 --> 00:04:10,640
particular text but even if there's no

00:04:09,760 --> 00:04:13,360
similarity

00:04:10,640 --> 00:04:14,159
this particular text have a this

00:04:13,360 --> 00:04:17,519
particular

00:04:14,159 --> 00:04:19,600
topics okay now it can be it can be a

00:04:17,519 --> 00:04:23,759
business related topic it can be

00:04:19,600 --> 00:04:25,919
a uh it can be an adventure related

00:04:23,759 --> 00:04:29,199
topic it can be a movie related topic

00:04:25,919 --> 00:04:29,840
and stuff like that exactly so in topic

00:04:29,199 --> 00:04:32,639
modeling

00:04:29,840 --> 00:04:34,880
you actually deal with topics word and

00:04:32,639 --> 00:04:37,600
probability distribution

00:04:34,880 --> 00:04:38,960
just to be sure i'm sure everybody's

00:04:37,600 --> 00:04:41,680
hearing me okay

00:04:38,960 --> 00:04:44,000
great so you did with topic probability

00:04:41,680 --> 00:04:46,720
distribution and words so basically

00:04:44,000 --> 00:04:48,080
what is a topic is a group of words and

00:04:46,720 --> 00:04:48,479
like we're on the word it's kind of a

00:04:48,080 --> 00:04:50,560
text

00:04:48,479 --> 00:04:52,639
inside a document and probability

00:04:50,560 --> 00:04:55,680
distribution is one of the way that

00:04:52,639 --> 00:04:58,639
you uh you get to see

00:04:55,680 --> 00:04:59,919
what particular word belongs to one

00:04:58,639 --> 00:05:01,919
particular topic

00:04:59,919 --> 00:05:03,360
but the redefinition of probability

00:05:01,919 --> 00:05:05,360
distribution is the function that

00:05:03,360 --> 00:05:07,520
actually describe the likelihood

00:05:05,360 --> 00:05:08,960
of obtaining a possible value of random

00:05:07,520 --> 00:05:11,520
variable that can imagine

00:05:08,960 --> 00:05:12,240
for instance you get to see something

00:05:11,520 --> 00:05:16,639
like

00:05:12,240 --> 00:05:18,479
zero points um 0.4

00:05:16,639 --> 00:05:20,960
probability or you get to something like

00:05:18,479 --> 00:05:24,160
20 percent 10 percent probability 40

00:05:20,960 --> 00:05:26,080
probability uh 60 probability you know

00:05:24,160 --> 00:05:29,039
that's just equals to having

00:05:26,080 --> 00:05:30,320
zero point nine uh probability zero

00:05:29,039 --> 00:05:33,840
point you know and

00:05:30,320 --> 00:05:36,320
just in between j1 and one exactly so um

00:05:33,840 --> 00:05:38,320
one thing about topic modeling is that

00:05:36,320 --> 00:05:41,120
each document consists about

00:05:38,320 --> 00:05:42,160
a mixture of topics and each topic

00:05:41,120 --> 00:05:44,880
consists of a

00:05:42,160 --> 00:05:45,840
collection of words like i mentioned

00:05:44,880 --> 00:05:49,360
earlier

00:05:45,840 --> 00:05:52,240
topic modeling is basically extracting

00:05:49,360 --> 00:05:53,680
or analyzing that document to find

00:05:52,240 --> 00:05:57,280
meaningful value out of it and

00:05:53,680 --> 00:05:59,520
also finding topic from a group of words

00:05:57,280 --> 00:06:00,319
exactly so basically a topic is a group

00:05:59,520 --> 00:06:03,759
of words

00:06:00,319 --> 00:06:05,199
that are similar in its context okay

00:06:03,759 --> 00:06:06,319
like i mentioned earlier where you have

00:06:05,199 --> 00:06:07,360
something like okay let's say for

00:06:06,319 --> 00:06:10,240
instance we have

00:06:07,360 --> 00:06:10,960
an article and in that article you have

00:06:10,240 --> 00:06:13,120
things

00:06:10,960 --> 00:06:14,960
related to business related to data

00:06:13,120 --> 00:06:16,960
science related to machine learning

00:06:14,960 --> 00:06:18,400
where you see things like classification

00:06:16,960 --> 00:06:20,800
regression

00:06:18,400 --> 00:06:21,520
um supervised learning also provides

00:06:20,800 --> 00:06:23,360
learning

00:06:21,520 --> 00:06:25,280
and stuff like that so it basically

00:06:23,360 --> 00:06:26,720
analyzes the article and tells you that

00:06:25,280 --> 00:06:29,919
this article has

00:06:26,720 --> 00:06:30,560
um this article is basically classified

00:06:29,919 --> 00:06:32,880
into this

00:06:30,560 --> 00:06:34,240
uh into these topics so if the business

00:06:32,880 --> 00:06:36,240
related article just

00:06:34,240 --> 00:06:38,960
had the context of data science and

00:06:36,240 --> 00:06:42,000
machine learning also had the context of

00:06:38,960 --> 00:06:44,800
entrepreneurship exactly so

00:06:42,000 --> 00:06:46,400
um now let's move over to how topic

00:06:44,800 --> 00:06:48,720
modeling

00:06:46,400 --> 00:06:50,639
works it's important for you to

00:06:48,720 --> 00:06:52,479
understand how topic modeling works

00:06:50,639 --> 00:06:55,199
because that is the beginning of

00:06:52,479 --> 00:06:58,880
understanding how to do topic modelling

00:06:55,199 --> 00:06:59,440
or basically how to um analyze a large

00:06:58,880 --> 00:07:03,120
document

00:06:59,440 --> 00:07:05,680
and extract topics from it

00:07:03,120 --> 00:07:06,880
so for instance if we have document 1

00:07:05,680 --> 00:07:11,039
document 2

00:07:06,880 --> 00:07:11,039
documentary and document 4.

00:07:13,520 --> 00:07:16,720
and the first document is data is

00:07:15,039 --> 00:07:19,680
important access to business

00:07:16,720 --> 00:07:21,520
second documents speak in section two of

00:07:19,680 --> 00:07:23,919
the constitution

00:07:21,520 --> 00:07:25,680
no one is above the law in documentary

00:07:23,919 --> 00:07:27,919
it is important for business to create

00:07:25,680 --> 00:07:29,199
marketing strategy documents for the

00:07:27,919 --> 00:07:32,560
nigeria government to

00:07:29,199 --> 00:07:35,039
1.3 million from imf now

00:07:32,560 --> 00:07:36,720
if we actually um before moving to the

00:07:35,039 --> 00:07:38,080
topic modeling part of it so

00:07:36,720 --> 00:07:41,599
it's important for you to understand

00:07:38,080 --> 00:07:44,639
that each topic is actually

00:07:41,599 --> 00:07:46,800
um each topic actually have a meaning so

00:07:44,639 --> 00:07:48,720
let's move out the topic one on topic

00:07:46,800 --> 00:07:51,120
two it might be a business related topic

00:07:48,720 --> 00:07:52,479
maybe a machine learning related topic

00:07:51,120 --> 00:07:54,560
and so on and so forth so

00:07:52,479 --> 00:07:55,840
the first topic is 40 business 10

00:07:54,560 --> 00:07:58,400
percent dealer 10

00:07:55,840 --> 00:08:00,319
assets 10 marketing and we can see this

00:07:58,400 --> 00:08:02,160
particular topic for under business so

00:08:00,319 --> 00:08:03,759
let's say topic one is across the

00:08:02,160 --> 00:08:06,000
business something like that

00:08:03,759 --> 00:08:08,080
exactly and we do topic two is

00:08:06,000 --> 00:08:09,919
government ten percent constitution ten

00:08:08,080 --> 00:08:13,599
percent nigeria you can say this topic

00:08:09,919 --> 00:08:16,720
from the government and topic three

00:08:13,599 --> 00:08:18,720
uh for a pet or possibly

00:08:16,720 --> 00:08:19,919
veterinarians or something like that you

00:08:18,720 --> 00:08:23,599
know exactly

00:08:19,919 --> 00:08:25,280
so um topic one is uh

00:08:23,599 --> 00:08:26,960
hundred percent so document one and

00:08:25,280 --> 00:08:27,360
document three is hundred percent topic

00:08:26,960 --> 00:08:30,560
one

00:08:27,360 --> 00:08:32,640
because it's um related to uh business

00:08:30,560 --> 00:08:33,360
related to data science it has marketing

00:08:32,640 --> 00:08:34,479
in it

00:08:33,360 --> 00:08:36,000
and stuff like that so if you take a

00:08:34,479 --> 00:08:37,599
look at it you notice the important

00:08:36,000 --> 00:08:41,839
keyword there which is

00:08:37,599 --> 00:08:44,399
marketing strategy business and data

00:08:41,839 --> 00:08:46,320
and business so you notice that um there

00:08:44,399 --> 00:08:49,200
will be a high probability

00:08:46,320 --> 00:08:50,320
of business in topic one yeah where you

00:08:49,200 --> 00:08:54,399
can see something like

00:08:50,320 --> 00:08:57,600
destiny zero point uh i say 0.80

00:08:54,399 --> 00:08:59,760
yeah 0.80 because you get the uh

00:08:57,600 --> 00:09:00,880
data sorry business and marketing

00:08:59,760 --> 00:09:02,800
strategy

00:09:00,880 --> 00:09:03,920
so if you use a semantic approach

00:09:02,800 --> 00:09:05,279
understanding

00:09:03,920 --> 00:09:09,040
this word you notice that that

00:09:05,279 --> 00:09:09,040
particular topic is going

00:09:09,920 --> 00:09:16,720
so topic two and topic sorry document

00:09:13,279 --> 00:09:19,360
two and document four for under

00:09:16,720 --> 00:09:20,480
topic two now document two is basically

00:09:19,360 --> 00:09:22,000
talking about government

00:09:20,480 --> 00:09:23,839
and document four is talking about how

00:09:22,000 --> 00:09:26,480
the nigerian government will draw

00:09:23,839 --> 00:09:26,880
to someone from imf and imf is actually

00:09:26,480 --> 00:09:28,320
in

00:09:26,880 --> 00:09:29,920
government institution i'm sure that's

00:09:28,320 --> 00:09:32,560
not a private institution

00:09:29,920 --> 00:09:33,200
exactly and basically it's going to be

00:09:32,560 --> 00:09:35,600
talking the

00:09:33,200 --> 00:09:37,760
topic four topic two and third document

00:09:35,600 --> 00:09:39,040
two and topic on document four therefore

00:09:37,760 --> 00:09:40,640
under topic two

00:09:39,040 --> 00:09:42,240
we'll be talking about government

00:09:40,640 --> 00:09:44,000
talking about nigeria and talking about

00:09:42,240 --> 00:09:47,279
money

00:09:44,000 --> 00:09:50,080
okay because that particular document um

00:09:47,279 --> 00:09:51,120
has a lot of context within government

00:09:50,080 --> 00:09:54,399
constitution

00:09:51,120 --> 00:09:56,800
and money and yeah exactly

00:09:54,399 --> 00:09:58,399
so the next one is document five for

00:09:56,800 --> 00:10:02,320
under topic three

00:09:58,399 --> 00:10:02,320
yeah document five uh

00:10:02,399 --> 00:10:05,440
so basically document five you're

00:10:03,680 --> 00:10:06,800
talking about pets yeah

00:10:05,440 --> 00:10:08,959
exactly it's a document five we're

00:10:06,800 --> 00:10:12,079
talking about pet it's following the

00:10:08,959 --> 00:10:15,680
uh topic three which is car

00:10:12,079 --> 00:10:18,800
dog and so on and so forth okay so

00:10:15,680 --> 00:10:20,000
this is a simple approach on what

00:10:18,800 --> 00:10:22,079
happened

00:10:20,000 --> 00:10:24,000
when you actually use the topic modern

00:10:22,079 --> 00:10:26,000
algorithm so a lot of topic method

00:10:24,000 --> 00:10:28,640
algorithm have various mathematies

00:10:26,000 --> 00:10:29,839
uh equation around it and how it works

00:10:28,640 --> 00:10:31,920
and how

00:10:29,839 --> 00:10:33,040
and the symmetric the thematic approach

00:10:31,920 --> 00:10:35,600
behind it

00:10:33,040 --> 00:10:36,959
exactly so uh if you take a look at the

00:10:35,600 --> 00:10:40,399
document which is

00:10:36,959 --> 00:10:43,120
uh the bread avocado candies and

00:10:40,399 --> 00:10:44,399
documentary so you can take a look at

00:10:43,120 --> 00:10:47,600
the topic

00:10:44,399 --> 00:10:51,440
the topic zero um has 3.99

00:10:47,600 --> 00:10:55,120
avocado topic to 0.99 bread

00:10:51,440 --> 00:10:58,640
uh topic 1 0.999 um

00:10:55,120 --> 00:10:59,120
candies now moving over to the part

00:10:58,640 --> 00:11:02,760
where

00:10:59,120 --> 00:11:05,680
you get to work you see document 0 has

00:11:02,760 --> 00:11:11,760
0.30 uh topic 0

00:11:05,680 --> 00:11:14,000
0.30 topic 1 0.92939

00:11:11,760 --> 00:11:15,200
for document 0. so you notice that

00:11:14,000 --> 00:11:18,399
document deal for

00:11:15,200 --> 00:11:18,399
under topic two

00:11:20,160 --> 00:11:26,640
i'm sure everybody's hearing me great

00:11:23,279 --> 00:11:28,959
so um documentary one for on the

00:11:26,640 --> 00:11:30,800
topic zero because it has a high

00:11:28,959 --> 00:11:34,880
probability

00:11:30,800 --> 00:11:39,279
ratio of topic zero

00:11:34,880 --> 00:11:41,839
document two under topic one documentary

00:11:39,279 --> 00:11:43,600
is neutral so it's important for you to

00:11:41,839 --> 00:11:46,800
understand that

00:11:43,600 --> 00:11:50,320
a text or sorry a word might actually

00:11:46,800 --> 00:11:53,040
occur in various topics so for instance

00:11:50,320 --> 00:11:53,839
uh this particular context you might get

00:11:53,040 --> 00:11:57,680
to see

00:11:53,839 --> 00:12:00,000
a word like let's imagine if we had

00:11:57,680 --> 00:12:01,600
something like uh data and important

00:12:00,000 --> 00:12:05,360
access to business

00:12:01,600 --> 00:12:08,720
as rely on an important access

00:12:05,360 --> 00:12:11,920
to zones so you notice that

00:12:08,720 --> 00:12:13,920
the pet actually is in document one and

00:12:11,920 --> 00:12:16,399
it's okay for you to see

00:12:13,920 --> 00:12:17,120
a particular word in various topics but

00:12:16,399 --> 00:12:19,839
it's important

00:12:17,120 --> 00:12:21,600
that you notice the probability ratio of

00:12:19,839 --> 00:12:23,040
that particular word so the probability

00:12:21,600 --> 00:12:26,240
ratio determines

00:12:23,040 --> 00:12:30,320
um what particular

00:12:26,240 --> 00:12:32,720
republican that means the topic exactly

00:12:30,320 --> 00:12:33,600
so let's look at topic modeling

00:12:32,720 --> 00:12:36,800
algorithms

00:12:33,600 --> 00:12:38,240
so these algorithms are a lot of

00:12:36,800 --> 00:12:39,760
algorithms out there

00:12:38,240 --> 00:12:41,200
these algorithms are the most popular

00:12:39,760 --> 00:12:42,000
and the useful ones that i've come

00:12:41,200 --> 00:12:44,720
across

00:12:42,000 --> 00:12:45,519
exactly in my own opinion exactly now

00:12:44,720 --> 00:12:48,720
the first one

00:12:45,519 --> 00:12:51,279
is latin sematic analysis so

00:12:48,720 --> 00:12:52,399
latin semantic analysis with hyptopic

00:12:51,279 --> 00:12:58,160
modeling

00:12:52,399 --> 00:12:58,160
algorithms i'm sure everyone can hear me

00:13:04,839 --> 00:13:08,959
right

00:13:06,880 --> 00:13:11,440
okay so if you can if you stay with me

00:13:08,959 --> 00:13:13,519
you can say hello

00:13:11,440 --> 00:13:21,839
or you can just put in your country flag

00:13:13,519 --> 00:13:21,839
or something you know

00:13:22,160 --> 00:13:27,440
yeah okay great so latin semantic output

00:13:25,440 --> 00:13:29,600
is an effective way of analyzing text

00:13:27,440 --> 00:13:31,360
and finding hidden topics to understand

00:13:29,600 --> 00:13:34,560
the context of the text

00:13:31,360 --> 00:13:37,519
so latin and latin semantic analysis use

00:13:34,560 --> 00:13:38,639
the concept approach so instead of

00:13:37,519 --> 00:13:42,240
looking for

00:13:38,639 --> 00:13:44,000
topics you go ahead to um check out or

00:13:42,240 --> 00:13:48,079
to actually

00:13:44,000 --> 00:13:50,079
understand the concept in the document

00:13:48,079 --> 00:13:51,199
exactly so let's move into the next one

00:13:50,079 --> 00:13:53,360
latch and uh

00:13:51,199 --> 00:13:55,600
new challenge and location so this is

00:13:53,360 --> 00:13:58,880
one of the most popular

00:13:55,600 --> 00:14:02,639
um topic modeling approach exactly

00:13:58,880 --> 00:14:04,560
so latin this device check to select

00:14:02,639 --> 00:14:06,399
a location actually discover topic in

00:14:04,560 --> 00:14:07,600
the collection of document and assign it

00:14:06,399 --> 00:14:10,560
to key

00:14:07,600 --> 00:14:11,360
and then tag each document with a topic

00:14:10,560 --> 00:14:13,519
exactly

00:14:11,360 --> 00:14:15,680
so the next one is dynamic topic

00:14:13,519 --> 00:14:16,000
modeling so dynamic topic modeling is

00:14:15,680 --> 00:14:19,680
one

00:14:16,000 --> 00:14:21,839
of the most advanced approach of lda

00:14:19,680 --> 00:14:23,360
so you can also call it dlda which is

00:14:21,839 --> 00:14:27,279
dynamic lda

00:14:23,360 --> 00:14:29,360
it actually gives you the ability to

00:14:27,279 --> 00:14:30,639
to see the timestamp or to actually add

00:14:29,360 --> 00:14:33,760
a timestamp to each

00:14:30,639 --> 00:14:34,240
topic so you get to see the evolution of

00:14:33,760 --> 00:14:38,079
topic

00:14:34,240 --> 00:14:40,560
over time so for instance if you have um

00:14:38,079 --> 00:14:43,440
a document or if you have a customer

00:14:40,560 --> 00:14:46,000
review from 1999 to 2000

00:14:43,440 --> 00:14:46,560
so you want to see customer review on

00:14:46,000 --> 00:14:48,959
how

00:14:46,560 --> 00:14:49,600
um the evolution of customer review over

00:14:48,959 --> 00:14:53,120
time

00:14:49,600 --> 00:14:55,839
to see okay what the

00:14:53,120 --> 00:14:57,680
uh what particular topic your customers

00:14:55,839 --> 00:14:58,639
always complain about really complaining

00:14:57,680 --> 00:15:03,279
about

00:14:58,639 --> 00:15:05,279
um how your product actually

00:15:03,279 --> 00:15:06,639
possibly application gets you trash

00:15:05,279 --> 00:15:09,040
every minute of the day

00:15:06,639 --> 00:15:10,240
or something exactly so you get to see

00:15:09,040 --> 00:15:13,440
the evolution

00:15:10,240 --> 00:15:16,160
of customer review over time

00:15:13,440 --> 00:15:18,160
so the next one is embedding topic model

00:15:16,160 --> 00:15:21,920
so this has to do with embedding

00:15:18,160 --> 00:15:23,920
uh basically um integrating embedding

00:15:21,920 --> 00:15:25,839
which is um the representation of the

00:15:23,920 --> 00:15:28,320
new representation of checks

00:15:25,839 --> 00:15:28,320
exactly

00:15:39,600 --> 00:15:42,639
which is the low representation of text

00:15:41,839 --> 00:15:44,880
basically

00:15:42,639 --> 00:15:46,320
and the next one is dynamic embedding

00:15:44,880 --> 00:15:48,880
topic netherlands so

00:15:46,320 --> 00:15:50,000
dynamic embedding topic modeling is one

00:15:48,880 --> 00:15:53,120
of the

00:15:50,000 --> 00:15:54,240
amazing approach for topic modeling

00:15:53,120 --> 00:15:56,880
basically

00:15:54,240 --> 00:15:57,680
um it's basically embedding topic

00:15:56,880 --> 00:16:00,880
modeling

00:15:57,680 --> 00:16:02,399
first and the lda

00:16:00,880 --> 00:16:04,320
so you're actually working with

00:16:02,399 --> 00:16:06,160
embedding topic medallion in

00:16:04,320 --> 00:16:08,160
combination of embedding topic meddling

00:16:06,160 --> 00:16:11,759
and dld where you have to do it

00:16:08,160 --> 00:16:14,160
timestamp you know

00:16:11,759 --> 00:16:14,160
exactly

00:16:15,519 --> 00:16:20,000
and actually a lot of them apart from

00:16:18,160 --> 00:16:22,480
these ones i've listed

00:16:20,000 --> 00:16:23,839
and you can again see uh i think that

00:16:22,480 --> 00:16:26,240
there's really a lot of

00:16:23,839 --> 00:16:27,839
topics within the algorithm but this is

00:16:26,240 --> 00:16:31,759
quite the popular one

00:16:27,839 --> 00:16:34,639
exactly so uh let's move into the

00:16:31,759 --> 00:16:38,240
application of topic medallion

00:16:34,639 --> 00:16:41,199
topic modeling is useful in various

00:16:38,240 --> 00:16:42,480
industries starting from the chatbot

00:16:41,199 --> 00:16:44,720
industry which is

00:16:42,480 --> 00:16:45,519
customer getting to understand their

00:16:44,720 --> 00:16:49,199
customer

00:16:45,519 --> 00:16:51,920
and helping your customer out exactly

00:16:49,199 --> 00:16:53,040
so chatbot is actually one of the

00:16:51,920 --> 00:16:55,519
amazing

00:16:53,040 --> 00:16:56,720
uh use of artificial intelligence in

00:16:55,519 --> 00:16:58,959
customer

00:16:56,720 --> 00:16:59,839
service exactly so you can use topic

00:16:58,959 --> 00:17:01,600
modeling to

00:16:59,839 --> 00:17:04,079
understand the context of what your

00:17:01,600 --> 00:17:06,799
customer is asking for and how you can

00:17:04,079 --> 00:17:07,919
help them so um hr actually use topic

00:17:06,799 --> 00:17:12,480
modeling for

00:17:07,919 --> 00:17:15,120
a lot of things so basically um finding

00:17:12,480 --> 00:17:17,280
important context in your curriculum or

00:17:15,120 --> 00:17:20,880
your cv to see if it actually match

00:17:17,280 --> 00:17:24,160
what they they really want

00:17:20,880 --> 00:17:25,760
so the next one is

00:17:24,160 --> 00:17:27,679
customer satisfaction analysis to

00:17:25,760 --> 00:17:28,880
basically getting to see the customer

00:17:27,679 --> 00:17:31,600
satisfaction

00:17:28,880 --> 00:17:33,280
exactly so topic tracking search engine

00:17:31,600 --> 00:17:34,160
to google actually make use of topic

00:17:33,280 --> 00:17:35,679
modeling

00:17:34,160 --> 00:17:37,039
uh yeah google actually use topic

00:17:35,679 --> 00:17:38,799
modeling which is one of the most

00:17:37,039 --> 00:17:41,520
amazing application of public modelling

00:17:38,799 --> 00:17:44,880
so social media like i mentioned earlier

00:17:41,520 --> 00:17:46,480
on twitter where you see two texts

00:17:44,880 --> 00:17:49,200
uh i don't know if you actually get

00:17:46,480 --> 00:17:52,559
wonder why you see

00:17:49,200 --> 00:17:53,600
a check a treat sorry that from somebody

00:17:52,559 --> 00:17:55,600
you do not follow

00:17:53,600 --> 00:17:58,080
but that person actually tweets

00:17:55,600 --> 00:18:01,280
something related to a topic that

00:17:58,080 --> 00:18:02,160
you actually love exactly so that's the

00:18:01,280 --> 00:18:04,720
most important

00:18:02,160 --> 00:18:06,240
um oh that's one of the amazing approach

00:18:04,720 --> 00:18:08,000
of topic modeling

00:18:06,240 --> 00:18:10,480
then the next one is recommendation

00:18:08,000 --> 00:18:11,120
engine and spam filter so there really a

00:18:10,480 --> 00:18:15,280
lot more

00:18:11,120 --> 00:18:18,559
application of topic modeling

00:18:15,280 --> 00:18:20,480
and before i move into the last part i

00:18:18,559 --> 00:18:23,280
would like to

00:18:20,480 --> 00:18:24,880
possibly show you an overview of a basic

00:18:23,280 --> 00:18:30,640
approach

00:18:24,880 --> 00:18:30,640
on how to perform an lg topic modeling

00:18:30,960 --> 00:18:35,440
so i think i need to share my screen for

00:18:33,200 --> 00:18:35,440
this

00:18:37,360 --> 00:18:43,840
okay yeah let me share my screen for

00:18:40,840 --> 00:18:43,840
this

00:18:48,160 --> 00:18:52,880
okay great so this is if i this is the

00:18:51,360 --> 00:18:54,720
platform i was working in although i'm

00:18:52,880 --> 00:18:57,280
not really pretty done yet so

00:18:54,720 --> 00:19:00,000
but this is basically going to show you

00:18:57,280 --> 00:19:01,600
how to perform a topic modeling

00:19:00,000 --> 00:19:03,120
without this thing writing a single line

00:19:01,600 --> 00:19:04,880
of code but then it's not just

00:19:03,120 --> 00:19:06,160
giving you an automated approach it's

00:19:04,880 --> 00:19:07,840
also allowing you to do

00:19:06,160 --> 00:19:09,200
the work you need to do so imagine if

00:19:07,840 --> 00:19:10,880
you're writing the code

00:19:09,200 --> 00:19:12,080
and then you have to do everything you

00:19:10,880 --> 00:19:13,440
need to do by writing the code so

00:19:12,080 --> 00:19:17,760
basically

00:19:13,440 --> 00:19:17,760
for instance if i have a document let's

00:19:22,840 --> 00:19:25,840
see

00:19:26,720 --> 00:19:30,960
let's see the nigerian government

00:19:33,520 --> 00:19:39,840
yeah okay let me just

00:19:50,840 --> 00:19:53,840
it's

00:19:57,600 --> 00:20:02,640
okay so let's use this particular text

00:20:04,840 --> 00:20:09,919
okay

00:20:07,039 --> 00:20:10,480
let's say we have a persuasion here and

00:20:09,919 --> 00:20:13,760
let's see

00:20:10,480 --> 00:20:16,720
i kind of make this

00:20:13,760 --> 00:20:21,840
profit punctuation uh because most of

00:20:16,720 --> 00:20:21,840
the texts are not gonna be coming clean

00:20:30,080 --> 00:20:33,440
so let's say we have a text like this

00:20:32,320 --> 00:20:36,080
together

00:20:33,440 --> 00:20:38,080
the true chamber make up the body in

00:20:36,080 --> 00:20:39,679
nigeria for the national assemblage we

00:20:38,080 --> 00:20:41,200
started checking it i'm a government and

00:20:39,679 --> 00:20:43,039
so on and so forth so

00:20:41,200 --> 00:20:44,400
you get to clean up your data which is

00:20:43,039 --> 00:20:47,200
one of the most important things you

00:20:44,400 --> 00:20:50,000
need to do so

00:20:47,200 --> 00:20:51,440
done um proof of that oh sorry um it

00:20:50,000 --> 00:20:53,440
quotes that saves

00:20:51,440 --> 00:20:55,200
that you spend about 80 percent of your

00:20:53,440 --> 00:20:57,440
time cleaned it yeah a lot of time you

00:20:55,200 --> 00:20:59,840
spend that but the truth about it is

00:20:57,440 --> 00:21:01,039
most data actually don't come the same

00:20:59,840 --> 00:21:03,200
way so

00:21:01,039 --> 00:21:04,880
that's one thing the data's a dynamic

00:21:03,200 --> 00:21:06,000
exactly you never know how the data is

00:21:04,880 --> 00:21:09,919
going to come

00:21:06,000 --> 00:21:12,880
and your claiming approach depends on

00:21:09,919 --> 00:21:13,360
your data set so when i see a lot of

00:21:12,880 --> 00:21:16,480
people

00:21:13,360 --> 00:21:17,200
set out the standard for cleaning i'm

00:21:16,480 --> 00:21:19,440
like

00:21:17,200 --> 00:21:20,960
okay you get to see data that is

00:21:19,440 --> 00:21:22,720
different so what do you do

00:21:20,960 --> 00:21:24,000
i mean it's basically not written in the

00:21:22,720 --> 00:21:26,240
context that

00:21:24,000 --> 00:21:27,039
you're going to see data like this your

00:21:26,240 --> 00:21:28,960
data set

00:21:27,039 --> 00:21:31,520
determine your cleaning apple channel

00:21:28,960 --> 00:21:35,120
cleaning up would depend on

00:21:31,520 --> 00:21:36,640
your data set so the cleaning approach

00:21:35,120 --> 00:21:40,559
on this one is basically

00:21:36,640 --> 00:21:43,280
removing punctuation removing

00:21:40,559 --> 00:21:44,400
numbers yeah exactly i don't think we

00:21:43,280 --> 00:21:46,559
need the numbers

00:21:44,400 --> 00:21:48,320
moving punctuation and moving numbers so

00:21:46,559 --> 00:21:50,559
in the embedding aspect

00:21:48,320 --> 00:21:52,240
um you actually will not need the

00:21:50,559 --> 00:21:53,919
punctuation actually you don't need to

00:21:52,240 --> 00:21:56,880
clean the punctuation because

00:21:53,919 --> 00:22:00,400
there is naturally um standard way of

00:21:56,880 --> 00:22:00,400
cleaning data for embedding

00:22:00,799 --> 00:22:06,640
so embedding actually allow you to use

00:22:04,240 --> 00:22:08,960
your punctuation to do topic modeling

00:22:06,640 --> 00:22:09,840
embedding like embedding topic modeling

00:22:08,960 --> 00:22:12,400
and

00:22:09,840 --> 00:22:13,520
dynamic embedding topics so for this we

00:22:12,400 --> 00:22:15,360
are going to be using the

00:22:13,520 --> 00:22:17,200
uh one of the most popular one which is

00:22:15,360 --> 00:22:18,080
out here so you're going to clean your

00:22:17,200 --> 00:22:19,840
data set

00:22:18,080 --> 00:22:21,679
and then you can view your data set

00:22:19,840 --> 00:22:22,320
after cleaning so this is our data set

00:22:21,679 --> 00:22:25,760
attacking

00:22:22,320 --> 00:22:28,400
you don't see anything like

00:22:25,760 --> 00:22:30,159
punctuation numbers and stuff like that

00:22:28,400 --> 00:22:30,960
so the data set is screened just with

00:22:30,159 --> 00:22:33,120
text

00:22:30,960 --> 00:22:34,400
you know lower case there is no practice

00:22:33,120 --> 00:22:36,400
there's no um

00:22:34,400 --> 00:22:38,400
not sensitive you know exactly so you

00:22:36,400 --> 00:22:40,880
can get a view columns to

00:22:38,400 --> 00:22:42,320
uh or height columns which is some other

00:22:40,880 --> 00:22:43,679
thing to do while writing the code so

00:22:42,320 --> 00:22:44,400
let's imagine that we are writing this

00:22:43,679 --> 00:22:46,400
code and get

00:22:44,400 --> 00:22:48,000
used on pandas to clean our data set and

00:22:46,400 --> 00:22:51,360
move our punctuation

00:22:48,000 --> 00:22:52,400
move out uh yeah move that punctuation

00:22:51,360 --> 00:22:55,120
without numbers

00:22:52,400 --> 00:22:55,919
they have http or something exactly

00:22:55,120 --> 00:22:58,400
coming without

00:22:55,919 --> 00:22:59,679
so you can also get to see the shape of

00:22:58,400 --> 00:23:02,720
your data set

00:22:59,679 --> 00:23:05,840
and yeah which is one word one column

00:23:02,720 --> 00:23:09,039
and you now drop unwanted columns

00:23:05,840 --> 00:23:12,880
so this unwanted columns are maybe

00:23:09,039 --> 00:23:16,320
you have a column id or you have

00:23:12,880 --> 00:23:17,919
um you have another

00:23:16,320 --> 00:23:19,520
context under that column name that's

00:23:17,919 --> 00:23:20,640
not really important for you so the most

00:23:19,520 --> 00:23:23,520
important thing here

00:23:20,640 --> 00:23:24,480
is the text okay so you can jump on that

00:23:23,520 --> 00:23:26,480
columns

00:23:24,480 --> 00:23:28,080
and then you get to see a data after you

00:23:26,480 --> 00:23:31,039
jump onto the column which is just

00:23:28,080 --> 00:23:31,679
meaning the syntax the index order the

00:23:31,039 --> 00:23:34,559
index

00:23:31,679 --> 00:23:35,760
and your text so we now move over to

00:23:34,559 --> 00:23:39,039
tokenization

00:23:35,760 --> 00:23:41,600
so tokenization is basically

00:23:39,039 --> 00:23:43,200
grouping 10 times into what so basically

00:23:41,600 --> 00:23:43,840
finding this thematic opportunity

00:23:43,200 --> 00:23:46,559
sentence

00:23:43,840 --> 00:23:47,919
so let's say uh let's let's do this week

00:23:46,559 --> 00:23:51,520
so

00:23:47,919 --> 00:23:54,960
let's see tokenization is

00:23:51,520 --> 00:23:54,960
there's a simple approach to it

00:23:55,919 --> 00:23:59,600
yeah so basically splitting your data

00:23:57,600 --> 00:24:01,200
into tokens exactly

00:23:59,600 --> 00:24:03,279
and then that's what we call our

00:24:01,200 --> 00:24:06,159
vectorization so that's basically

00:24:03,279 --> 00:24:07,120
uh specifically into vectors or 10

00:24:06,159 --> 00:24:10,640
metrics

00:24:07,120 --> 00:24:13,679
exactly so you can use convectorizer or

00:24:10,640 --> 00:24:15,840
tdi vectorizer for your tokenization

00:24:13,679 --> 00:24:16,960
so when you tokenize the data split it

00:24:15,840 --> 00:24:20,400
into token

00:24:16,960 --> 00:24:23,120
and documenting time matrix which is uh

00:24:20,400 --> 00:24:24,880
yeah basically so your account

00:24:23,120 --> 00:24:26,880
vectorizer or uc

00:24:24,880 --> 00:24:28,000
have idea of conventionalizer or use

00:24:26,880 --> 00:24:30,080
that vector

00:24:28,000 --> 00:24:31,520
best tokenizer so bad technology is one

00:24:30,080 --> 00:24:34,000
of the recent

00:24:31,520 --> 00:24:35,679
tokenization techniques i think is

00:24:34,000 --> 00:24:37,799
amazing although it's not implemented

00:24:35,679 --> 00:24:41,200
there so the one we are going to use is

00:24:37,799 --> 00:24:43,919
convectorizer so this is just a

00:24:41,200 --> 00:24:45,200
an approach exactly so let's imagine if

00:24:43,919 --> 00:24:46,400
you're writing the code

00:24:45,200 --> 00:24:48,240
and this is everything you'll be doing

00:24:46,400 --> 00:24:49,120
while writing the code to build your

00:24:48,240 --> 00:24:52,400
first

00:24:49,120 --> 00:24:53,760
lda topic module so let's just count

00:24:52,400 --> 00:24:57,279
vectorizer

00:24:53,760 --> 00:25:00,480
i'm going to see our text uh in tokens

00:24:57,279 --> 00:25:02,320
exactly so you can now train your lda

00:25:00,480 --> 00:25:03,600
with uh you cannot change your model

00:25:02,320 --> 00:25:05,360
with lda so

00:25:03,600 --> 00:25:07,039
before you get to training your model

00:25:05,360 --> 00:25:08,400
with lg there's a lot of things you need

00:25:07,039 --> 00:25:11,600
to do which is

00:25:08,400 --> 00:25:15,600
um basically converting

00:25:11,600 --> 00:25:18,720
um your text into bag of words

00:25:15,600 --> 00:25:20,320
using id to word dictionary and a lot of

00:25:18,720 --> 00:25:23,360
it so but this is just a simple

00:25:20,320 --> 00:25:24,400
approach okay so you can go ahead to you

00:25:23,360 --> 00:25:26,799
know

00:25:24,400 --> 00:25:28,559
do something quite complex exactly if

00:25:26,799 --> 00:25:30,320
you want to get an important

00:25:28,559 --> 00:25:32,159
uh oh actually you want to make your

00:25:30,320 --> 00:25:34,400
model

00:25:32,159 --> 00:25:35,279
great exactly so this is just a simple

00:25:34,400 --> 00:25:37,440
approach

00:25:35,279 --> 00:25:38,559
exactly so you can see the number of

00:25:37,440 --> 00:25:43,760
topics

00:25:38,559 --> 00:25:46,880
and you can choose the number of topics

00:25:43,760 --> 00:25:46,880
oh can you hear me now

00:25:48,000 --> 00:25:57,760
yeah if you can hear me you can just say

00:25:49,919 --> 00:26:01,279
hi or with your country flag

00:25:57,760 --> 00:26:03,120
oh great okay because i saw um

00:26:01,279 --> 00:26:05,520
someone talks you can't hear me okay

00:26:03,120 --> 00:26:07,440
yeah so the number of topics

00:26:05,520 --> 00:26:09,200
you can pick the number of topics so

00:26:07,440 --> 00:26:10,720
what how many um

00:26:09,200 --> 00:26:12,559
what number of topics do you want to

00:26:10,720 --> 00:26:14,559
extract from that document so let's say

00:26:12,559 --> 00:26:16,559
we want to extract two topics

00:26:14,559 --> 00:26:18,720
and we want to know if the first topic

00:26:16,559 --> 00:26:20,960
is going to be government related topics

00:26:18,720 --> 00:26:20,960
or

00:26:21,039 --> 00:26:25,279
institution or school or education or

00:26:23,520 --> 00:26:27,760
corruption or terrorism

00:26:25,279 --> 00:26:29,279
and stuff like that so what the number

00:26:27,760 --> 00:26:32,080
of words you want to assign

00:26:29,279 --> 00:26:33,440
to a topic don't forget that a topic is

00:26:32,080 --> 00:26:36,640
a group of word

00:26:33,440 --> 00:26:38,320
so you can actually

00:26:36,640 --> 00:26:41,039
basically refresh the topic name which

00:26:38,320 --> 00:26:43,120
is topic one to a particular context

00:26:41,039 --> 00:26:45,120
based on the probability of that

00:26:43,120 --> 00:26:48,799
particular word in the topic

00:26:45,120 --> 00:26:52,720
so let's say we have a tree from a word

00:26:48,799 --> 00:26:55,919
and we have three topics

00:26:52,720 --> 00:26:57,600
and now showing the topic model and once

00:26:55,919 --> 00:27:01,679
you're training you get to see

00:26:57,600 --> 00:27:05,200
the topics so uh if you notice topic one

00:27:01,679 --> 00:27:08,400
topic zero topic one and topic two

00:27:05,200 --> 00:27:12,480
second so you get to see a lot of them

00:27:08,400 --> 00:27:15,520
so if we have more text document

00:27:12,480 --> 00:27:16,640
or more um word in our document then we

00:27:15,520 --> 00:27:19,279
get to see

00:27:16,640 --> 00:27:21,039
how dynamic or how amazing the mode is

00:27:19,279 --> 00:27:22,320
actually gonna perform so it's important

00:27:21,039 --> 00:27:24,480
that you have

00:27:22,320 --> 00:27:25,600
a good amount of text before you can do

00:27:24,480 --> 00:27:28,640
topic modeling

00:27:25,600 --> 00:27:31,440
so you can actually get add up um more

00:27:28,640 --> 00:27:31,440
text to this

00:27:31,679 --> 00:27:35,840
can you find an article and

00:27:42,240 --> 00:27:46,080
yeah so i can add an attribute to that

00:27:44,240 --> 00:27:50,720
and again it's the

00:27:46,080 --> 00:27:50,720
topic so let me just find an article and

00:28:00,840 --> 00:28:17,840
then

00:28:03,440 --> 00:28:17,840
what's up

00:28:20,159 --> 00:28:24,159
okay so we clean our data set

00:28:26,080 --> 00:28:31,840
you see the numbers here

00:28:56,840 --> 00:28:59,840
okay

00:29:16,000 --> 00:29:22,799
okay you know data set

00:29:19,440 --> 00:29:22,799
uh video data set

00:29:23,760 --> 00:29:27,360
click on that column we have the

00:29:25,279 --> 00:29:31,520
characterizers

00:29:27,360 --> 00:29:31,520
we use five number of topics

00:29:34,880 --> 00:29:40,960
you can attach okay so

00:29:38,240 --> 00:29:42,640
um you can see that mother's actually

00:29:40,960 --> 00:29:44,720
going to perform better when you

00:29:42,640 --> 00:29:46,880
actually work on the processing part of

00:29:44,720 --> 00:29:49,760
it which has to do with tokenization

00:29:46,880 --> 00:29:51,679
converting to bag of wood i'm using id

00:29:49,760 --> 00:29:53,279
dictionary idt dictionaries

00:29:51,679 --> 00:29:56,159
and stuff like that i need to work

00:29:53,279 --> 00:29:58,000
dictionaries and a lot of

00:29:56,159 --> 00:29:59,360
pre-processing stuff so this is just a

00:29:58,000 --> 00:30:01,840
simple topic model

00:29:59,360 --> 00:30:03,520
and we get to see two topics uh let's

00:30:01,840 --> 00:30:07,120
leave the motherboard and topics

00:30:03,520 --> 00:30:09,840
and turn the model back again exactly

00:30:07,120 --> 00:30:10,720
so you get to see the topics there and

00:30:09,840 --> 00:30:14,399
you can now pick

00:30:10,720 --> 00:30:16,399
what uh particular um context

00:30:14,399 --> 00:30:17,440
or topics does that particular group of

00:30:16,399 --> 00:30:19,760
words belongs to

00:30:17,440 --> 00:30:21,039
so you can do a lot of them and this is

00:30:19,760 --> 00:30:23,600
just a sample output

00:30:21,039 --> 00:30:25,679
so you can get to do an advanced

00:30:23,600 --> 00:30:28,000
approach which has to do with uh

00:30:25,679 --> 00:30:28,000
you know

00:30:28,960 --> 00:30:33,760
which has to do with um the processing

00:30:30,880 --> 00:30:37,840
step um you know converting bag of wood

00:30:33,760 --> 00:30:37,840
and a lot of tests so

00:30:48,080 --> 00:30:52,559
okay let me go back to my slide okay

00:30:50,320 --> 00:30:52,559
great

00:30:55,120 --> 00:30:58,320
okay so um after the application of

00:30:57,279 --> 00:31:00,559
topic medallion

00:30:58,320 --> 00:31:01,519
i think this is the end of my section

00:31:00,559 --> 00:31:04,000
and i have an

00:31:01,519 --> 00:31:05,120
article talking about various topic

00:31:04,000 --> 00:31:07,200
modeling apps

00:31:05,120 --> 00:31:08,399
so you can actually go to the articles

00:31:07,200 --> 00:31:11,600
on hash node

00:31:08,399 --> 00:31:13,039
you can actually go to makeupbois.com to

00:31:11,600 --> 00:31:15,760
see

00:31:13,039 --> 00:31:17,919
various topic modeling approach and i

00:31:15,760 --> 00:31:20,240
discussed the mathematics behind this

00:31:17,919 --> 00:31:23,440
particular

00:31:20,240 --> 00:31:26,880
topic modeling especially on etm and

00:31:23,440 --> 00:31:28,080
detail okay so where and for those who

00:31:26,880 --> 00:31:30,159
don't know me i'm

00:31:28,080 --> 00:31:32,320
a youtuber data scientist editability

00:31:30,159 --> 00:31:34,640
lab and coordinator of air festival

00:31:32,320 --> 00:31:36,799
nigeria school of ai and also

00:31:34,640 --> 00:31:38,640
an ibm champion you can also subscribe

00:31:36,799 --> 00:31:39,840
to my channel where i give payment

00:31:38,640 --> 00:31:41,440
content about

00:31:39,840 --> 00:31:43,120
data science machine learning and

00:31:41,440 --> 00:31:44,240
artificial intelligence and also follow

00:31:43,120 --> 00:31:47,840
me on twitter

00:31:44,240 --> 00:31:47,840

YouTube URL: https://www.youtube.com/watch?v=jvEiCbnJWWA


