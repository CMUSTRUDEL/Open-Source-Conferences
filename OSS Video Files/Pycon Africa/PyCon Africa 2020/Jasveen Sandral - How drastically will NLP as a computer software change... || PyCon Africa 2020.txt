Title: Jasveen Sandral - How drastically will NLP as a computer software change... || PyCon Africa 2020
Publication date: 2020-08-29
Playlist: PyCon Africa 2020
Description: 
	In today's Information Age large volumes of information are available in English â€“ whether it be information for competitive exams or general reading. However, many people whose primary language is Hindi or any other Indian language (or, any African language, too) are unable to access information in English. Anusaaraka (we) aims to bridge this language barrier by allowing a user to enter an English text into Anusaaraka and read it in an Indian language (or, any African language) of their choice.


I'll be talking about computer software (Anusaaraka) which renders text from one Indian language into another, and this can be applied for the African language, as well. We'll be looking briefly into the task of building an MT (machine translation) system. The core Anusaaraka output is in a language close to the target language and can be understood by the human reader after some training.


URL of the materials and source code used in the past on this topic:
https://code.google.com/archive/p/anusaaraka/
https://sourceforge.net/projects/anusaaraka/
http://cdn.iiit.ac.in/cdn/ltrc.iiit.ac.in/downloads/nlpbook/nlp-panini.pdf
Captions: 
	00:00:15,120 --> 00:00:18,800
um

00:00:15,599 --> 00:00:20,800
hello everybody um you know um

00:00:18,800 --> 00:00:22,800
i i don't know if you know from which uh

00:00:20,800 --> 00:00:25,920
times on your you know

00:00:22,800 --> 00:00:27,359
um seeing me from but then yes um

00:00:25,920 --> 00:00:28,800
i'm assuming that you're having a really

00:00:27,359 --> 00:00:30,400
good day and in case you're not i hope

00:00:28,800 --> 00:00:31,279
you would have a good day and you know a

00:00:30,400 --> 00:00:34,320
very

00:00:31,279 --> 00:00:35,440
cool week ahead so um just a quick

00:00:34,320 --> 00:00:38,239
introduction about me

00:00:35,440 --> 00:00:39,920
um my name is jasvin and i'm basically

00:00:38,239 --> 00:00:41,840
from the southern state of india

00:00:39,920 --> 00:00:44,000
so india which is basically in southeast

00:00:41,840 --> 00:00:46,800
asia or i would say south asia

00:00:44,000 --> 00:00:48,719
um there's this another state in the

00:00:46,800 --> 00:00:50,000
southern part of india and i'm right in

00:00:48,719 --> 00:00:50,480
the middle of the seventh state it's

00:00:50,000 --> 00:00:51,920
called

00:00:50,480 --> 00:00:54,079
the state is called and i'm from

00:00:51,920 --> 00:00:55,680
hyderabad so um

00:00:54,079 --> 00:00:57,360
greetings from hyderabad so you know

00:00:55,680 --> 00:01:00,800
i'll be um

00:00:57,360 --> 00:01:02,960
talking about my um i mean my

00:01:00,800 --> 00:01:03,840
speech or my session is about how

00:01:02,960 --> 00:01:06,080
drastically

00:01:03,840 --> 00:01:07,760
nlp as a computer software change their

00:01:06,080 --> 00:01:11,520
types in the near future

00:01:07,760 --> 00:01:14,640
so um what we're seeing these days is

00:01:11,520 --> 00:01:16,080
nlp um as a computer software is

00:01:14,640 --> 00:01:18,560
progressing great i mean

00:01:16,080 --> 00:01:21,280
just just couple of days back we had the

00:01:18,560 --> 00:01:23,040
um gpt3 model by open ai and we we

00:01:21,280 --> 00:01:24,479
already had the existing previous models

00:01:23,040 --> 00:01:26,159
by open a and we have a lot of open

00:01:24,479 --> 00:01:29,680
source work going on

00:01:26,159 --> 00:01:30,880
but what we eventually um are not

00:01:29,680 --> 00:01:32,720
seeing or i would say what we're

00:01:30,880 --> 00:01:35,200
ignoring is that um

00:01:32,720 --> 00:01:37,119
nlp has a great potential but it's not

00:01:35,200 --> 00:01:40,320
being unlocked at the same pace you know

00:01:37,119 --> 00:01:42,079
um probably blockchain or or ai or any

00:01:40,320 --> 00:01:44,479
of those like terms are being done

00:01:42,079 --> 00:01:45,920
so this is small introduction or i would

00:01:44,479 --> 00:01:48,240
say small talk on how

00:01:45,920 --> 00:01:49,119
i feel nlp can be used for the near

00:01:48,240 --> 00:01:51,439
future and

00:01:49,119 --> 00:01:52,880
what sort of work i have done um

00:01:51,439 --> 00:01:55,439
integration with this talk

00:01:52,880 --> 00:01:56,560
or in relation to this session and i

00:01:55,439 --> 00:01:59,360
hope there would be good

00:01:56,560 --> 00:02:00,320
takeaways for the home um so on that

00:01:59,360 --> 00:02:04,719
note let's start

00:02:00,320 --> 00:02:05,520
so um i am basically um an undergraduate

00:02:04,719 --> 00:02:08,399
research attendant

00:02:05,520 --> 00:02:09,360
at a lab called anusorka which is the

00:02:08,399 --> 00:02:11,840
largest

00:02:09,360 --> 00:02:13,120
um speech and language transmission lab

00:02:11,840 --> 00:02:15,760
in south asia

00:02:13,120 --> 00:02:17,840
um the lab's name is ltrc and i'll work

00:02:15,760 --> 00:02:20,319
in one of the labs called anuserkas so

00:02:17,840 --> 00:02:21,120
anusarga is basically computer software

00:02:20,319 --> 00:02:24,480
which

00:02:21,120 --> 00:02:26,800
i would say it it works at rendering um

00:02:24,480 --> 00:02:28,800
text from one indian language to another

00:02:26,800 --> 00:02:32,080
um but but the catch here is

00:02:28,800 --> 00:02:32,560
um you do not specifically have to think

00:02:32,080 --> 00:02:34,640
of it

00:02:32,560 --> 00:02:36,560
as a machine translation system because

00:02:34,640 --> 00:02:38,239
this is something much more

00:02:36,560 --> 00:02:40,239
beyond the lines of machine translations

00:02:38,239 --> 00:02:42,080
machine translator system because

00:02:40,239 --> 00:02:43,840
um when we have a machine translate

00:02:42,080 --> 00:02:46,000
system you just input a

00:02:43,840 --> 00:02:48,160
um property sentence like that system

00:02:46,000 --> 00:02:49,599
you want to translate it from english to

00:02:48,160 --> 00:02:51,519
german or english to french you just

00:02:49,599 --> 00:02:52,480
trans trans i mean you just put in your

00:02:51,519 --> 00:02:54,080
english word

00:02:52,480 --> 00:02:56,080
and you say you know just convert it to

00:02:54,080 --> 00:02:57,440
friends and google translator bing it

00:02:56,080 --> 00:03:00,000
brings you the result but

00:02:57,440 --> 00:03:01,360
this this blues out of um transparity

00:03:00,000 --> 00:03:04,000
there's no sort of

00:03:01,360 --> 00:03:05,440
um i would say you know the flow of

00:03:04,000 --> 00:03:06,959
instructions it does not really tell us

00:03:05,440 --> 00:03:07,519
how it caught that but it just caught

00:03:06,959 --> 00:03:10,000
that

00:03:07,519 --> 00:03:12,080
and i feel that it it does sort of an

00:03:10,000 --> 00:03:14,000
injustice to the

00:03:12,080 --> 00:03:16,159
um future we're looking forward to nlp

00:03:14,000 --> 00:03:18,879
considering what gpt3 has already

00:03:16,159 --> 00:03:19,440
done um let me explain it more briefly

00:03:18,879 --> 00:03:22,560
so

00:03:19,440 --> 00:03:25,680
um is basically computer

00:03:22,560 --> 00:03:26,879
software and what we do is um it

00:03:25,680 --> 00:03:28,879
produces an output

00:03:26,879 --> 00:03:31,280
which is obviously comprehensible to the

00:03:28,879 --> 00:03:34,560
reader but it might not be grammatical

00:03:31,280 --> 00:03:36,560
so what that means is um let's assume um

00:03:34,560 --> 00:03:38,159
i have a machine translation system in

00:03:36,560 --> 00:03:39,920
underserved learning from one indian

00:03:38,159 --> 00:03:42,000
language to another indian language

00:03:39,920 --> 00:03:43,360
and my output is supposed to be in hindi

00:03:42,000 --> 00:03:45,599
so um

00:03:43,360 --> 00:03:46,400
my output in hindi can be understood by

00:03:45,599 --> 00:03:49,599
hindi reader

00:03:46,400 --> 00:03:52,720
but it may not be fully grammatical so

00:03:49,599 --> 00:03:53,599
what that means is um the reader will

00:03:52,720 --> 00:03:56,640
require

00:03:53,599 --> 00:03:59,680
some sort of um background training

00:03:56,640 --> 00:04:00,959
so that um he will require sufficient

00:03:59,680 --> 00:04:02,080
amount of training so that he can

00:04:00,959 --> 00:04:04,239
understand the output because it

00:04:02,080 --> 00:04:07,200
grammatically might not make sense

00:04:04,239 --> 00:04:08,080
but its efficiency in terms of uh the

00:04:07,200 --> 00:04:10,400
translations

00:04:08,080 --> 00:04:12,080
would be very very good and pretty high

00:04:10,400 --> 00:04:13,920
so this is this is the sort of

00:04:12,080 --> 00:04:15,439
um overview of the circle that i have to

00:04:13,920 --> 00:04:17,359
give because um

00:04:15,439 --> 00:04:19,519
the work of nlp the research that i've

00:04:17,359 --> 00:04:21,120
done is closely related to underserved

00:04:19,519 --> 00:04:23,520
so that is what i thought

00:04:21,120 --> 00:04:24,639
it would make more sense bring this in

00:04:23,520 --> 00:04:26,960
um

00:04:24,639 --> 00:04:28,720
next um so you know you have heard a lot

00:04:26,960 --> 00:04:30,800
of people talking about nlp you have you

00:04:28,720 --> 00:04:32,639
have read a lot of tutorials on this

00:04:30,800 --> 00:04:35,120
course there are courses coming up

00:04:32,639 --> 00:04:36,800
but one of your most basic doubt is what

00:04:35,120 --> 00:04:38,800
is nlp what is natural language

00:04:36,800 --> 00:04:41,759
processing um there's a lot of hype

00:04:38,800 --> 00:04:42,800
and i would say that energy at the end

00:04:41,759 --> 00:04:46,479
of the day

00:04:42,800 --> 00:04:49,680
um condenses down to these six things um

00:04:46,479 --> 00:04:51,120
the number one is text processing second

00:04:49,680 --> 00:04:54,000
one is syntactical

00:04:51,120 --> 00:04:55,520
parsing um third is entity parsing i'll

00:04:54,000 --> 00:04:57,280
be talking deeply about them

00:04:55,520 --> 00:04:58,720
so just giving you a brief of you know

00:04:57,280 --> 00:05:00,400
once entity personally happens there's

00:04:58,720 --> 00:05:02,000
statistical features very involved a lot

00:05:00,400 --> 00:05:03,120
of stats probability and maths and then

00:05:02,000 --> 00:05:04,240
there's word embeddings and there's

00:05:03,120 --> 00:05:06,560
fixed matching

00:05:04,240 --> 00:05:07,840
so um i'll be taking one on one and

00:05:06,560 --> 00:05:10,639
let's see what we have

00:05:07,840 --> 00:05:11,680
for this so um what you see right now in

00:05:10,639 --> 00:05:14,960
front of screens

00:05:11,680 --> 00:05:18,240
um is text processing so

00:05:14,960 --> 00:05:19,600
um usually text processing is something

00:05:18,240 --> 00:05:22,960
that is done on

00:05:19,600 --> 00:05:24,320
data which is not really clean or i

00:05:22,960 --> 00:05:26,320
would say um

00:05:24,320 --> 00:05:27,759
in in a sort um there's a lot of noise

00:05:26,320 --> 00:05:29,520
present so what you have to do is you

00:05:27,759 --> 00:05:30,639
have to clean the data first you have to

00:05:29,520 --> 00:05:32,240
make it noise free

00:05:30,639 --> 00:05:34,160
and you also have to make sure that it's

00:05:32,240 --> 00:05:36,560
pretty um feasible

00:05:34,160 --> 00:05:38,080
for your analysis so what you do is it

00:05:36,560 --> 00:05:39,199
consists of the three steps that is

00:05:38,080 --> 00:05:41,280
you know that's right there on the

00:05:39,199 --> 00:05:43,120
screen um noise removal

00:05:41,280 --> 00:05:44,400
lexical normalization and object

00:05:43,120 --> 00:05:47,440
standardization

00:05:44,400 --> 00:05:49,840
so um nice removal so

00:05:47,440 --> 00:05:50,800
what happens is um any piece of

00:05:49,840 --> 00:05:53,840
information

00:05:50,800 --> 00:05:56,160
which is not relevant to the context and

00:05:53,840 --> 00:05:56,960
is present um that's that's usually

00:05:56,160 --> 00:05:59,919
called noise

00:05:56,960 --> 00:06:01,120
so let's assume um you know usually a

00:05:59,919 --> 00:06:03,919
lot of

00:06:01,120 --> 00:06:05,520
data has etc or it has a lot of dots and

00:06:03,919 --> 00:06:06,000
you might not specifically wish to

00:06:05,520 --> 00:06:08,080
consider

00:06:06,000 --> 00:06:09,360
that so that is when your noise removal

00:06:08,080 --> 00:06:11,199
comes in um

00:06:09,360 --> 00:06:12,800
to be more specific you're just cleaning

00:06:11,199 --> 00:06:13,280
out of reading order data so that you

00:06:12,800 --> 00:06:15,199
get the

00:06:13,280 --> 00:06:16,400
good feasible amount of data for your

00:06:15,199 --> 00:06:18,960
work um

00:06:16,400 --> 00:06:19,520
next we have the lexical normalization

00:06:18,960 --> 00:06:21,199
um

00:06:19,520 --> 00:06:22,560
let's assume we have i give you four

00:06:21,199 --> 00:06:25,919
words um

00:06:22,560 --> 00:06:28,240
play player player and probably player

00:06:25,919 --> 00:06:29,680
so what i see is all these four words

00:06:28,240 --> 00:06:31,199
have their root of play

00:06:29,680 --> 00:06:33,120
so that's where the lexical

00:06:31,199 --> 00:06:36,479
normalization comes in um

00:06:33,120 --> 00:06:38,639
what what you're basically doing is um

00:06:36,479 --> 00:06:40,240
there are the the they are different

00:06:38,639 --> 00:06:42,800
variations of the word play

00:06:40,240 --> 00:06:43,440
so they are contexted or similar so you

00:06:42,800 --> 00:06:46,160
can

00:06:43,440 --> 00:06:49,039
um in a sense group them together to

00:06:46,160 --> 00:06:52,080
make them sort of lexicalized normalized

00:06:49,039 --> 00:06:52,319
if that makes sense um after that you

00:06:52,080 --> 00:06:54,160
know

00:06:52,319 --> 00:06:56,639
probably we have object standardization

00:06:54,160 --> 00:07:00,319
um i i would say you know we

00:06:56,639 --> 00:07:00,800
probably have um a habit of um slang we

00:07:00,319 --> 00:07:03,919
use

00:07:00,800 --> 00:07:04,880
dm for direct message and we use brb for

00:07:03,919 --> 00:07:07,599
be right back

00:07:04,880 --> 00:07:09,280
so all of the short term messages are

00:07:07,599 --> 00:07:11,599
all of the short term contextual

00:07:09,280 --> 00:07:13,440
context which you might not necessarily

00:07:11,599 --> 00:07:16,880
want to show in your data or

00:07:13,440 --> 00:07:18,479
or in your parsing but um and you you

00:07:16,880 --> 00:07:19,759
have to get rid of them that's when the

00:07:18,479 --> 00:07:21,680
object standardization comes

00:07:19,759 --> 00:07:23,759
so once the noise is removed once you

00:07:21,680 --> 00:07:25,599
have you know lexicalized normalized

00:07:23,759 --> 00:07:27,680
data where you have all of this remove

00:07:25,599 --> 00:07:28,880
dot filter dot read out even the object

00:07:27,680 --> 00:07:30,319
standardization is done

00:07:28,880 --> 00:07:32,000
that's when a pre-processing part is

00:07:30,319 --> 00:07:33,759
done that's cool um

00:07:32,000 --> 00:07:35,759
i'll probably do one thing i'll be

00:07:33,759 --> 00:07:38,240
sharing my notebook with you

00:07:35,759 --> 00:07:45,840
um so that can make things easier for us

00:07:38,240 --> 00:07:45,840
to work on

00:07:47,360 --> 00:07:52,160
yeah i hope my one note is visible now

00:07:56,240 --> 00:07:59,840
um is it visible

00:08:02,800 --> 00:08:07,360
so let's let's talk about this in some

00:08:05,680 --> 00:08:08,800
of the most fundamental terms whatever

00:08:07,360 --> 00:08:12,240
what i wanted to do

00:08:08,800 --> 00:08:14,000
so let's assume we are beginning with

00:08:12,240 --> 00:08:17,280
the raw data first

00:08:14,000 --> 00:08:19,360
now once i have my raw data um

00:08:17,280 --> 00:08:20,560
i would i would probably like to do some

00:08:19,360 --> 00:08:23,919
sort of um

00:08:20,560 --> 00:08:27,199
cleaning in it so um i

00:08:23,919 --> 00:08:30,879
i would say um um

00:08:27,199 --> 00:08:34,399
probably the urls or vbcs or

00:08:30,879 --> 00:08:36,800
any sort of short words that might not

00:08:34,399 --> 00:08:38,320
that might necessarily confuse our

00:08:36,800 --> 00:08:41,360
machine translation system

00:08:38,320 --> 00:08:44,399
is what we're going to put in this um

00:08:41,360 --> 00:08:55,360
once that is done um we

00:08:44,399 --> 00:08:58,560
can probably put in the terms of um

00:08:55,360 --> 00:08:59,360
yeah so once that is done um we can

00:08:58,560 --> 00:09:03,200
probably

00:08:59,360 --> 00:09:05,600
lemmatize our data or we can perform

00:09:03,200 --> 00:09:07,440
um part of speech tagging or any any

00:09:05,600 --> 00:09:10,240
sort of tagging that can be done

00:09:07,440 --> 00:09:10,880
with our data which we think can be done

00:09:10,240 --> 00:09:12,480
and

00:09:10,880 --> 00:09:14,399
once once you know it has been cleaned

00:09:12,480 --> 00:09:15,279
up once we have some basic operations

00:09:14,399 --> 00:09:17,680
performed

00:09:15,279 --> 00:09:19,120
you might want to do a sort of um i

00:09:17,680 --> 00:09:21,360
would say a dictionary

00:09:19,120 --> 00:09:22,720
checkup or you know look up in the table

00:09:21,360 --> 00:09:24,560
so i would rather say look up in the

00:09:22,720 --> 00:09:28,320
table

00:09:24,560 --> 00:09:31,279
or you know dictionary click so that

00:09:28,320 --> 00:09:32,880
system um gets through a couple of words

00:09:31,279 --> 00:09:35,440
which are not there in the

00:09:32,880 --> 00:09:37,120
dictionary already um it can add them in

00:09:35,440 --> 00:09:40,160
so that might make sense

00:09:37,120 --> 00:09:43,440
and once that is done um

00:09:40,160 --> 00:09:44,880
you get your clean filter or data

00:09:43,440 --> 00:09:46,880
so i would i would rather say this is a

00:09:44,880 --> 00:09:49,200
clean data so this is a rough skit

00:09:46,880 --> 00:09:50,480
of how exactly this should be done you

00:09:49,200 --> 00:09:52,959
have a raw data

00:09:50,480 --> 00:09:54,800
um you you have this top words or url or

00:09:52,959 --> 00:09:56,959
punctuation of mentions anything

00:09:54,800 --> 00:09:57,839
um which you usually refer to this as

00:09:56,959 --> 00:09:59,920
the

00:09:57,839 --> 00:10:01,440
noise entity removal we would want to

00:09:59,920 --> 00:10:03,279
remove all of this

00:10:01,440 --> 00:10:04,560
and then we have the word normalization

00:10:03,279 --> 00:10:06,560
which you can see you know you have

00:10:04,560 --> 00:10:08,320
limitization or you have eos tagging or

00:10:06,560 --> 00:10:10,640
stemming or tokenization

00:10:08,320 --> 00:10:11,440
once that is done our last step would be

00:10:10,640 --> 00:10:13,279
to do the

00:10:11,440 --> 00:10:15,360
standardization of word where you want

00:10:13,279 --> 00:10:17,680
to do the lookup lookup tables or

00:10:15,360 --> 00:10:18,560
maybe regular expressions or maybe the

00:10:17,680 --> 00:10:20,959
dictionary checkup

00:10:18,560 --> 00:10:22,880
once all of this has been passed on um

00:10:20,959 --> 00:10:23,360
you you would get a clean data so this

00:10:22,880 --> 00:10:26,640
is how

00:10:23,360 --> 00:10:28,560
the rough sketch uh should be um i'll be

00:10:26,640 --> 00:10:33,839
getting back to my

00:10:28,560 --> 00:10:33,839
ppt now

00:10:36,240 --> 00:10:40,000
right so um i wanted to give you an exa

00:10:38,560 --> 00:10:42,240
i mean i just wanted to give you an

00:10:40,000 --> 00:10:43,279
insight on how exactly the clean data is

00:10:42,240 --> 00:10:46,480
processed so

00:10:43,279 --> 00:10:49,200
once you have that um our next step is

00:10:46,480 --> 00:10:51,360
the syntactical parsing so this is one

00:10:49,200 --> 00:10:53,680
of the most interesting parts and

00:10:51,360 --> 00:10:54,560
one of the parts which every beginner

00:10:53,680 --> 00:10:56,399
should do so

00:10:54,560 --> 00:10:58,079
you have something of dependency grammar

00:10:56,399 --> 00:11:00,399
so um

00:10:58,079 --> 00:11:02,320
what you need to do for this is um once

00:11:00,399 --> 00:11:02,800
you have a particular sentence um let's

00:11:02,320 --> 00:11:03,920
let's

00:11:02,800 --> 00:11:05,600
assume that you have some sort of

00:11:03,920 --> 00:11:06,240
sentence so you're supposed to build a

00:11:05,600 --> 00:11:07,839
tree

00:11:06,240 --> 00:11:09,120
um for that you know you can probably

00:11:07,839 --> 00:11:12,160
have from the previous videos i've

00:11:09,120 --> 00:11:12,160
created a couple of videos

00:11:12,640 --> 00:11:16,000
you know you could join the name so what

00:11:14,959 --> 00:11:19,360
you're basically doing

00:11:16,000 --> 00:11:20,240
is um you know um usually sentences are

00:11:19,360 --> 00:11:22,160
composed of

00:11:20,240 --> 00:11:23,360
words which are together so what that

00:11:22,160 --> 00:11:25,839
means is you

00:11:23,360 --> 00:11:26,880
need to know basic dependency grammar so

00:11:25,839 --> 00:11:29,519
that you can format

00:11:26,880 --> 00:11:31,200
a sort of dependency tree so let's

00:11:29,519 --> 00:11:32,320
assume you know you you have some sort

00:11:31,200 --> 00:11:34,959
of particular sentence

00:11:32,320 --> 00:11:36,880
you start with you know the root word

00:11:34,959 --> 00:11:39,600
once you get the root you create

00:11:36,880 --> 00:11:41,680
again some sort of two sub trees and you

00:11:39,600 --> 00:11:43,120
know a sort of subtree pattern comes in

00:11:41,680 --> 00:11:44,880
because you know it's tree sub tree

00:11:43,120 --> 00:11:46,880
subtree and you know you get the overall

00:11:44,880 --> 00:11:48,959
look of it that's the dependency grammar

00:11:46,880 --> 00:11:50,000
for you and then we have the part of

00:11:48,959 --> 00:11:52,560
speech tagging

00:11:50,000 --> 00:11:53,600
um it's it's called pos tagging in in in

00:11:52,560 --> 00:11:56,720
the short term

00:11:53,600 --> 00:11:59,519
so um we know pio's tags are

00:11:56,720 --> 00:12:00,880
noun verbs adjective adverbs etc so what

00:11:59,519 --> 00:12:04,240
we're doing is we are

00:12:00,880 --> 00:12:05,120
basically tagging each of the word of

00:12:04,240 --> 00:12:08,000
our sentence

00:12:05,120 --> 00:12:10,000
with a pos tag so let's assume um i

00:12:08,000 --> 00:12:11,920
given a system i mean i give

00:12:10,000 --> 00:12:13,920
i given a sent statement to my machine

00:12:11,920 --> 00:12:14,320
transformation system um i am speaking

00:12:13,920 --> 00:12:17,600
here

00:12:14,320 --> 00:12:20,399
icon j uh by pycon africa so

00:12:17,600 --> 00:12:21,600
um my system would take in the i as prp

00:12:20,399 --> 00:12:24,560
which is the pronoun phase

00:12:21,600 --> 00:12:24,880
and m as the vbp which is the verb phase

00:12:24,560 --> 00:12:27,519
and

00:12:24,880 --> 00:12:29,120
speaking as the vbg which is verb

00:12:27,519 --> 00:12:32,800
happening in the continuous form

00:12:29,120 --> 00:12:34,079
so so this is um the basic output of the

00:12:32,800 --> 00:12:36,000
part of speech tagging

00:12:34,079 --> 00:12:37,760
um you know where um everything that

00:12:36,000 --> 00:12:40,800
you're um

00:12:37,760 --> 00:12:42,800
sort of doing um is getting tagged up

00:12:40,800 --> 00:12:44,399
so that makes up your syntactical

00:12:42,800 --> 00:12:46,959
parsing then we have the

00:12:44,399 --> 00:12:47,920
entity parsing so this this gets sort of

00:12:46,959 --> 00:12:50,880
interesting so

00:12:47,920 --> 00:12:52,160
we have the phrase detection the near

00:12:50,880 --> 00:12:55,680
the named entity direct

00:12:52,160 --> 00:12:57,760
um recognition topic modeling and grams

00:12:55,680 --> 00:12:59,279
so um i would like to start with you

00:12:57,760 --> 00:13:02,240
know probably named

00:12:59,279 --> 00:13:03,760
recognition so let's assume um i have a

00:13:02,240 --> 00:13:07,680
sentence called you know

00:13:03,760 --> 00:13:10,480
um probably something like um x is the

00:13:07,680 --> 00:13:11,440
ceo of google so here my ex is the name

00:13:10,480 --> 00:13:12,880
of the person

00:13:11,440 --> 00:13:14,959
and google is the name of the

00:13:12,880 --> 00:13:18,320
organization obviously and

00:13:14,959 --> 00:13:19,920
um google i say he is the ceo of google

00:13:18,320 --> 00:13:21,519
at new york so new york is location so

00:13:19,920 --> 00:13:25,120
my name is entity recognition

00:13:21,519 --> 00:13:27,760
will in short put in um the

00:13:25,120 --> 00:13:28,560
name location organization all of it

00:13:27,760 --> 00:13:31,040
into the view

00:13:28,560 --> 00:13:32,320
of the machine transfer system so that

00:13:31,040 --> 00:13:33,920
our system becomes

00:13:32,320 --> 00:13:35,360
smart enough to detect that you know

00:13:33,920 --> 00:13:36,240
voice there's a name once it's a

00:13:35,360 --> 00:13:38,560
location

00:13:36,240 --> 00:13:39,839
it it'll it'll automatically you know

00:13:38,560 --> 00:13:42,160
sort of recognize

00:13:39,839 --> 00:13:42,880
or detect that all right so there's a

00:13:42,160 --> 00:13:44,639
location

00:13:42,880 --> 00:13:46,240
so probably he's talking about somebody

00:13:44,639 --> 00:13:47,519
or he's talking about an organization so

00:13:46,240 --> 00:13:48,720
maybe i need to link a tree

00:13:47,519 --> 00:13:51,040
so that's how the basic machine

00:13:48,720 --> 00:13:51,519
translation model works out and then you

00:13:51,040 --> 00:13:55,040
have

00:13:51,519 --> 00:13:57,760
um the topic modeling um you

00:13:55,040 --> 00:13:58,720
um topic modeling is simple um you know

00:13:57,760 --> 00:14:01,519
probably

00:13:58,720 --> 00:14:02,880
um if i talk about let's say prevailing

00:14:01,519 --> 00:14:06,399
in the covert situation

00:14:02,880 --> 00:14:09,120
um i talk about health doctors treatment

00:14:06,399 --> 00:14:10,959
um anything on those terms it closely

00:14:09,120 --> 00:14:14,160
matches with the term of health

00:14:10,959 --> 00:14:15,040
or at least you know probably talk of

00:14:14,160 --> 00:14:18,560
doctors

00:14:15,040 --> 00:14:19,199
so it all of this is being referred to a

00:14:18,560 --> 00:14:21,120
topic

00:14:19,199 --> 00:14:22,959
and these topics gathered within

00:14:21,120 --> 00:14:24,800
themselves and that is how the topic

00:14:22,959 --> 00:14:26,480
modeling comes in so when you have the

00:14:24,800 --> 00:14:28,639
name entity recognition theory

00:14:26,480 --> 00:14:30,240
along with the topic modeling when you

00:14:28,639 --> 00:14:30,720
club these two you have an amazing

00:14:30,240 --> 00:14:33,360
system

00:14:30,720 --> 00:14:34,639
of recognition or an entity parsing

00:14:33,360 --> 00:14:36,160
where the system will be very smart

00:14:34,639 --> 00:14:37,760
enough to detect you know um

00:14:36,160 --> 00:14:39,600
all right so this is the situation of

00:14:37,760 --> 00:14:40,959
covert being discussed um i have the

00:14:39,600 --> 00:14:44,079
location let's say you know

00:14:40,959 --> 00:14:45,519
um i mean i can say that the cases are

00:14:44,079 --> 00:14:47,920
pretty bad in india i mean

00:14:45,519 --> 00:14:49,360
the the the situation is very bad in

00:14:47,920 --> 00:14:50,880
india assertion our system would be

00:14:49,360 --> 00:14:52,639
smart enough to detect that all right

00:14:50,880 --> 00:14:54,880
cases are really bad in india

00:14:52,639 --> 00:14:57,120
so this is the user is probably talking

00:14:54,880 --> 00:14:59,519
something negative so i might have to

00:14:57,120 --> 00:15:01,760
um probably you know put it in the same

00:14:59,519 --> 00:15:04,800
terms so that's how you make your system

00:15:01,760 --> 00:15:05,519
smarter and engrams um it's it's

00:15:04,800 --> 00:15:10,000
basically

00:15:05,519 --> 00:15:13,199
um you you put in you know sort of

00:15:10,000 --> 00:15:15,279
i would say let's assume n grams

00:15:13,199 --> 00:15:17,120
is probably compared to words so there's

00:15:15,279 --> 00:15:18,560
a criteria um let's assume your end

00:15:17,120 --> 00:15:20,880
grams is more than one

00:15:18,560 --> 00:15:21,680
it's called unigrams and when you're

00:15:20,880 --> 00:15:24,079
when you have

00:15:21,680 --> 00:15:24,880
an equal to which is a five grams what

00:15:24,079 --> 00:15:27,519
you do is

00:15:24,880 --> 00:15:28,160
you basically club the two words

00:15:27,519 --> 00:15:30,639
together

00:15:28,160 --> 00:15:31,920
which have the same word i mean let's

00:15:30,639 --> 00:15:34,959
assume i say

00:15:31,920 --> 00:15:37,680
this is a simple sample text so this end

00:15:34,959 --> 00:15:38,240
is match up along with is and a along

00:15:37,680 --> 00:15:40,079
with a

00:15:38,240 --> 00:15:41,680
and sample and along with sample index

00:15:40,079 --> 00:15:44,560
so what happens is um

00:15:41,680 --> 00:15:46,240
we are creating an end graph so that it

00:15:44,560 --> 00:15:47,839
can help our other

00:15:46,240 --> 00:15:50,720
i mean it can help our machine transfer

00:15:47,839 --> 00:15:53,680
system um to efficiently detect

00:15:50,720 --> 00:15:55,360
if you know there are a lot of chains

00:15:53,680 --> 00:15:56,720
happening up or there's a chain reaction

00:15:55,360 --> 00:15:58,880
happening in a sort of

00:15:56,720 --> 00:16:00,240
sort of manner so that's your n-gram so

00:15:58,880 --> 00:16:02,959
this completes an entity part

00:16:00,240 --> 00:16:03,600
um parsing then we have the statistical

00:16:02,959 --> 00:16:06,160
features

00:16:03,600 --> 00:16:08,079
this is really interesting um i have

00:16:06,160 --> 00:16:10,079
written about tf idf

00:16:08,079 --> 00:16:11,440
and frequency density features and

00:16:10,079 --> 00:16:14,959
readability features

00:16:11,440 --> 00:16:16,959
so pf idf um stands for term frequency

00:16:14,959 --> 00:16:20,000
inverse document frequency

00:16:16,959 --> 00:16:21,040
so in its most simplest term it is used

00:16:20,000 --> 00:16:23,759
for information

00:16:21,040 --> 00:16:24,800
retrieval problems and the tf that you

00:16:23,759 --> 00:16:27,279
see in the beginning

00:16:24,800 --> 00:16:28,720
it's called term frequency um so what

00:16:27,279 --> 00:16:29,600
that means is let's assume you have a

00:16:28,720 --> 00:16:32,720
document d

00:16:29,600 --> 00:16:33,600
you look out for a term t um this is a

00:16:32,720 --> 00:16:36,560
term t

00:16:33,600 --> 00:16:38,639
so the term t is defined as the count of

00:16:36,560 --> 00:16:41,120
the same t within the document

00:16:38,639 --> 00:16:42,800
and idf is basically a logarithmic of

00:16:41,120 --> 00:16:43,279
the ratios of the total documents coming

00:16:42,800 --> 00:16:45,600
in

00:16:43,279 --> 00:16:47,120
so this your corpus called this is

00:16:45,600 --> 00:16:48,560
basically a list of documents so your

00:16:47,120 --> 00:16:51,040
corpus will define

00:16:48,560 --> 00:16:52,880
um you know probably that how many how

00:16:51,040 --> 00:16:55,839
many number of documents are coming in

00:16:52,880 --> 00:16:56,800
and how many how many number of the

00:16:55,839 --> 00:16:58,560
counts of the same

00:16:56,800 --> 00:17:00,560
text on the counts of the terms coming

00:16:58,560 --> 00:17:02,480
in um there's another

00:17:00,560 --> 00:17:04,000
formula for that i would not be going

00:17:02,480 --> 00:17:06,240
into the mathematical aspect

00:17:04,000 --> 00:17:07,839
of things right now and then you have

00:17:06,240 --> 00:17:09,839
the frequency

00:17:07,839 --> 00:17:11,120
or you know probably the other features

00:17:09,839 --> 00:17:13,839
of credibility

00:17:11,120 --> 00:17:14,799
so here you focus on the word count

00:17:13,839 --> 00:17:17,439
sentence count

00:17:14,799 --> 00:17:19,120
punctuation counts um probably the

00:17:17,439 --> 00:17:21,199
industry specific word counts

00:17:19,120 --> 00:17:22,799
so that whenever you know um let's

00:17:21,199 --> 00:17:23,199
assume i talked about like in africa

00:17:22,799 --> 00:17:25,520
some

00:17:23,199 --> 00:17:27,520
pike in africa is a specific word crown

00:17:25,520 --> 00:17:29,440
so as soon as i see pike on africa

00:17:27,520 --> 00:17:30,880
it's going to increment the counter and

00:17:29,440 --> 00:17:32,720
at the end of my

00:17:30,880 --> 00:17:34,160
um translation my machine translation

00:17:32,720 --> 00:17:34,720
would tell me that you know there were a

00:17:34,160 --> 00:17:36,400
number of

00:17:34,720 --> 00:17:38,000
python africans occurring in a

00:17:36,400 --> 00:17:38,880
particular sentence or in a particular

00:17:38,000 --> 00:17:40,640
document

00:17:38,880 --> 00:17:42,960
and you know probably a machine

00:17:40,640 --> 00:17:46,640
translation system should work more on

00:17:42,960 --> 00:17:46,640
probably efficiency of the machine

00:17:46,720 --> 00:17:50,720
occurring i mean once there's a python

00:17:48,799 --> 00:17:51,840
the next word could be africa so that's

00:17:50,720 --> 00:17:53,039
how you train a model to be more

00:17:51,840 --> 00:17:54,799
efficient

00:17:53,039 --> 00:17:57,120
um that is what the statistical features

00:17:54,799 --> 00:17:59,440
involves a lot of maths heavy maths so

00:17:57,120 --> 00:18:00,960
you better be good at math for that um

00:17:59,440 --> 00:18:01,679
then we have the concept of word

00:18:00,960 --> 00:18:04,880
embeddings

00:18:01,679 --> 00:18:08,559
word embeddings um this is a topic of

00:18:04,880 --> 00:18:09,039
vectors so vectors are very similar to

00:18:08,559 --> 00:18:12,400
what

00:18:09,039 --> 00:18:15,520
we know in mathematics you know um

00:18:12,400 --> 00:18:15,919
probably we know that you know um the

00:18:15,520 --> 00:18:19,039
high

00:18:15,919 --> 00:18:20,480
dimensional word features are mapped

00:18:19,039 --> 00:18:21,200
with the lower dimensional feature

00:18:20,480 --> 00:18:22,559
vectors

00:18:21,200 --> 00:18:24,320
and you know what you do is you

00:18:22,559 --> 00:18:27,520
basically create a sort of array

00:18:24,320 --> 00:18:29,360
um you sort you create a neural network

00:18:27,520 --> 00:18:30,559
um you create a shallow neural network

00:18:29,360 --> 00:18:32,960
called skipgram

00:18:30,559 --> 00:18:35,039
and what to do is um whenever a sort of

00:18:32,960 --> 00:18:37,520
sentence has been entered into a model

00:18:35,039 --> 00:18:38,320
um you try to train it saying that you

00:18:37,520 --> 00:18:39,919
know um

00:18:38,320 --> 00:18:41,520
if there are specific number of word

00:18:39,919 --> 00:18:42,640
embeddings or if this specific number of

00:18:41,520 --> 00:18:45,679
vocabularies

00:18:42,640 --> 00:18:49,039
you try to um i would say train

00:18:45,679 --> 00:18:52,160
or the terms

00:18:49,039 --> 00:18:53,440
um this might um seem part of the blues

00:18:52,160 --> 00:18:55,200
right now but then

00:18:53,440 --> 00:18:56,720
word embeddings are helpful when you're

00:18:55,200 --> 00:18:57,919
building a really sophisticated machine

00:18:56,720 --> 00:18:59,679
transmission system

00:18:57,919 --> 00:19:01,679
um specifically when you're dealing with

00:18:59,679 --> 00:19:04,960
a large data set

00:19:01,679 --> 00:19:06,960
and last we have the text matching so

00:19:04,960 --> 00:19:08,400
we have the the advantage distance

00:19:06,960 --> 00:19:10,320
fioratic matching and the

00:19:08,400 --> 00:19:11,840
flexible string matching so i will not

00:19:10,320 --> 00:19:13,600
be going to details of this because this

00:19:11,840 --> 00:19:17,120
is involves heavy maths but

00:19:13,600 --> 00:19:17,600
um on on a lighter side um a lot of

00:19:17,120 --> 00:19:20,080
these

00:19:17,600 --> 00:19:21,520
have a correlation with the specific

00:19:20,080 --> 00:19:24,880
distance between two terms

00:19:21,520 --> 00:19:27,280
or the two words that you see and

00:19:24,880 --> 00:19:28,400
what what basically it um it does this

00:19:27,280 --> 00:19:30,640
um it just finds

00:19:28,400 --> 00:19:31,520
the distance and if it sees if it makes

00:19:30,640 --> 00:19:33,520
sense to

00:19:31,520 --> 00:19:35,679
sort of put in the next word i mean just

00:19:33,520 --> 00:19:38,880
like our keyboard autocorrect works

00:19:35,679 --> 00:19:41,919
so it works on the same work of

00:19:38,880 --> 00:19:43,679
um you know text matching it um i think

00:19:41,919 --> 00:19:47,280
this was too much for

00:19:43,679 --> 00:19:48,799
what and it might be um talking about

00:19:47,280 --> 00:19:50,799
some standard components of a machine

00:19:48,799 --> 00:19:53,120
transmission system um

00:19:50,799 --> 00:19:54,400
these five are roughly the components

00:19:53,120 --> 00:19:56,799
that you might need

00:19:54,400 --> 00:19:58,799
so you know you have first word analyzer

00:19:56,799 --> 00:20:00,880
word analyzer is very similar to what we

00:19:58,799 --> 00:20:04,559
have discussed in a previous slide on

00:20:00,880 --> 00:20:07,840
the word grouping or word matching so we

00:20:04,559 --> 00:20:08,240
um analyzer probably works on the lines

00:20:07,840 --> 00:20:11,679
of

00:20:08,240 --> 00:20:14,159
you know um morphological analysis

00:20:11,679 --> 00:20:15,200
so we have two terms here you should be

00:20:14,159 --> 00:20:17,919
well aware of them

00:20:15,200 --> 00:20:18,880
so one of them is called gmp another is

00:20:17,919 --> 00:20:22,159
called tm

00:20:18,880 --> 00:20:22,640
so cnp is called um general number

00:20:22,159 --> 00:20:26,080
person

00:20:22,640 --> 00:20:29,120
and tam tam is called tense

00:20:26,080 --> 00:20:30,159
aspect modality so what it does is

00:20:29,120 --> 00:20:33,440
basically whenever

00:20:30,159 --> 00:20:36,320
a word of a sentence has been entered in

00:20:33,440 --> 00:20:36,720
it it tries to put in it it tries to you

00:20:36,320 --> 00:20:40,240
know

00:20:36,720 --> 00:20:40,799
um give specific wordings to a specific

00:20:40,240 --> 00:20:43,200
word

00:20:40,799 --> 00:20:44,080
it sees if you know this is you might

00:20:43,200 --> 00:20:45,840
given a noun

00:20:44,080 --> 00:20:48,159
um or let's assume i give somebody's

00:20:45,840 --> 00:20:50,320
name it says that you know the gnp

00:20:48,159 --> 00:20:51,600
or the gender non-pronoun um i mean

00:20:50,320 --> 00:20:53,760
gender none person

00:20:51,600 --> 00:20:55,679
um it defines if you know if it is

00:20:53,760 --> 00:20:57,600
talking about a specific gender

00:20:55,679 --> 00:20:59,840
if there's any number of people involved

00:20:57,600 --> 00:21:01,120
there or is it is there somebody being

00:20:59,840 --> 00:21:03,360
discussed in that aspect

00:21:01,120 --> 00:21:04,159
and tense aspect mobility it talks about

00:21:03,360 --> 00:21:06,720
you know

00:21:04,159 --> 00:21:08,640
um what sort of case is being discussed

00:21:06,720 --> 00:21:10,720
is it is it being about past or present

00:21:08,640 --> 00:21:11,600
and this is very specific to the indian

00:21:10,720 --> 00:21:14,320
languages so

00:21:11,600 --> 00:21:15,280
it might not actually make sense to the

00:21:14,320 --> 00:21:17,200
um

00:21:15,280 --> 00:21:18,400
um languages which are derived from

00:21:17,200 --> 00:21:21,440
other continents so

00:21:18,400 --> 00:21:23,120
you know just uh brief up and then we

00:21:21,440 --> 00:21:25,360
have the local world grouper

00:21:23,120 --> 00:21:26,559
so this works similar on the terms of

00:21:25,360 --> 00:21:28,559
what analyzer but

00:21:26,559 --> 00:21:30,159
um as the name is suggesting you this

00:21:28,559 --> 00:21:32,240
literally groups words

00:21:30,159 --> 00:21:34,240
um you know whenever we have sequences

00:21:32,240 --> 00:21:36,640
of words that have a meaning

00:21:34,240 --> 00:21:38,240
which cannot be composed out of the

00:21:36,640 --> 00:21:40,799
meanings of individual words

00:21:38,240 --> 00:21:41,360
um they must be grouped together and the

00:21:40,799 --> 00:21:43,919
group

00:21:41,360 --> 00:21:44,480
as a whole will have a meaning so this

00:21:43,919 --> 00:21:46,000
meaning

00:21:44,480 --> 00:21:47,760
will then be stored in a table that was

00:21:46,000 --> 00:21:49,679
discussed in the previous slide

00:21:47,760 --> 00:21:51,120
so what i'm trying to say here is the

00:21:49,679 --> 00:21:53,840
local words which

00:21:51,120 --> 00:21:55,200
might not specifically make sense in a

00:21:53,840 --> 00:21:57,039
in a given

00:21:55,200 --> 00:21:59,280
frame of time or in a given specific

00:21:57,039 --> 00:22:00,559
document you try to group them together

00:21:59,280 --> 00:22:02,799
and you try to add them into your

00:22:00,559 --> 00:22:03,840
dictionary and you keep it for a future

00:22:02,799 --> 00:22:05,200
reference answer

00:22:03,840 --> 00:22:07,280
as well as for a present reference so

00:22:05,200 --> 00:22:09,200
that it helps you in

00:22:07,280 --> 00:22:11,039
a good mapping of your property machine

00:22:09,200 --> 00:22:13,280
transmission system

00:22:11,039 --> 00:22:14,640
after that we have mapping using the

00:22:13,280 --> 00:22:17,200
bilingual diaries

00:22:14,640 --> 00:22:19,039
so um you know usually when you're

00:22:17,200 --> 00:22:20,240
translating from one language to another

00:22:19,039 --> 00:22:20,960
obviously there'll be a lot of

00:22:20,240 --> 00:22:23,840
dictionaries

00:22:20,960 --> 00:22:25,520
involved so what you basically do is

00:22:23,840 --> 00:22:28,880
doing this is um

00:22:25,520 --> 00:22:30,640
once you have a specific word of one

00:22:28,880 --> 00:22:32,159
um language and you want to convert into

00:22:30,640 --> 00:22:33,760
the second one um

00:22:32,159 --> 00:22:35,200
you try to look up the meaning for the

00:22:33,760 --> 00:22:37,840
first one in a dictionary

00:22:35,200 --> 00:22:38,640
and you you know you try to look at for

00:22:37,840 --> 00:22:42,000
its pronoun

00:22:38,640 --> 00:22:45,280
or its noun in the target language um

00:22:42,000 --> 00:22:47,120
but you also take care about the gnp's

00:22:45,280 --> 00:22:49,360
and tam that was discussed above

00:22:47,120 --> 00:22:51,039
you you need to make sure that the same

00:22:49,360 --> 00:22:52,159
number and the same person is being

00:22:51,039 --> 00:22:54,320
discussed

00:22:52,159 --> 00:22:55,679
um i actually cannot show a code

00:22:54,320 --> 00:22:57,440
tutorial right now because

00:22:55,679 --> 00:22:59,039
i think we're an ignorant type so i

00:22:57,440 --> 00:23:00,159
would i would just discuss it on air

00:22:59,039 --> 00:23:02,240
that you know

00:23:00,159 --> 00:23:03,760
mapper using bilingual dictionaries is

00:23:02,240 --> 00:23:06,880
one of the most simplest

00:23:03,760 --> 00:23:09,520
manner of um mapping

00:23:06,880 --> 00:23:11,520
things directly from the root word and

00:23:09,520 --> 00:23:13,440
then we have the word synthesizer

00:23:11,520 --> 00:23:15,440
so it is basically the reverse of word

00:23:13,440 --> 00:23:17,679
analyzer it takes a root

00:23:15,440 --> 00:23:18,720
and it takes its lexical category in

00:23:17,679 --> 00:23:21,120
grammatical features

00:23:18,720 --> 00:23:22,880
and then it generates a word so you know

00:23:21,120 --> 00:23:24,720
um again this is very specific to the

00:23:22,880 --> 00:23:26,559
indian languages but it is a much

00:23:24,720 --> 00:23:28,400
simpler task compared to the word and

00:23:26,559 --> 00:23:32,000
analyzers this can be

00:23:28,400 --> 00:23:34,000
um done directly you do not really have

00:23:32,000 --> 00:23:35,360
to go by other alternatives of proposing

00:23:34,000 --> 00:23:37,520
or testing and so on

00:23:35,360 --> 00:23:38,880
and um the last one says you know

00:23:37,520 --> 00:23:41,679
putting the components together

00:23:38,880 --> 00:23:42,320
so once everything is done you're all

00:23:41,679 --> 00:23:45,120
set

00:23:42,320 --> 00:23:46,240
um you know you you put in all of this

00:23:45,120 --> 00:23:48,000
when you get a machine

00:23:46,240 --> 00:23:50,080
system it's that easy i mean it sounds

00:23:48,000 --> 00:23:52,960
easy but definitely is not

00:23:50,080 --> 00:23:53,440
so first the input text in a source

00:23:52,960 --> 00:23:55,200
language

00:23:53,440 --> 00:23:56,559
is passed to the word analyzer which

00:23:55,200 --> 00:23:59,120
analyzes each word

00:23:56,559 --> 00:23:59,919
and it produces its root and grammatical

00:23:59,120 --> 00:24:01,840
features

00:23:59,919 --> 00:24:04,159
and then these are fit into the local

00:24:01,840 --> 00:24:07,279
word broker which combines the words

00:24:04,159 --> 00:24:09,520
and produces local word groups and

00:24:07,279 --> 00:24:10,559
then you know the mapper takes the

00:24:09,520 --> 00:24:12,400
output and

00:24:10,559 --> 00:24:13,760
replaces the elements of the source

00:24:12,400 --> 00:24:17,360
language with the elements of

00:24:13,760 --> 00:24:20,000
the root language and you know um

00:24:17,360 --> 00:24:20,799
so and so you you work on with your

00:24:20,000 --> 00:24:22,880
features of

00:24:20,799 --> 00:24:25,120
the word synthesizer in case it does not

00:24:22,880 --> 00:24:27,600
match up and then you put together

00:24:25,120 --> 00:24:28,559
and this is your machine transmission

00:24:27,600 --> 00:24:29,919
system

00:24:28,559 --> 00:24:33,840
so these are the components of machine

00:24:29,919 --> 00:24:33,840
translation system

00:24:36,830 --> 00:24:41,039
[Music]

00:24:38,480 --> 00:24:42,640
the features that it has to give out so

00:24:41,039 --> 00:24:45,840
number one we have the

00:24:42,640 --> 00:24:47,760
faithful representation of text and

00:24:45,840 --> 00:24:50,799
source language so what this means is

00:24:47,760 --> 00:24:52,480
um as i told you earlier um

00:24:50,799 --> 00:24:54,080
you know when you give out a text in

00:24:52,480 --> 00:24:56,320
english and suppose

00:24:54,080 --> 00:24:57,679
assume that you have to convert it to

00:24:56,320 --> 00:24:59,919
french and there's not

00:24:57,679 --> 00:25:01,840
good um you know that when you feed it

00:24:59,919 --> 00:25:02,720
into google translator it directly gives

00:25:01,840 --> 00:25:04,320
you the word

00:25:02,720 --> 00:25:07,279
but it does not really show you how it

00:25:04,320 --> 00:25:09,279
happens it does not show you how many

00:25:07,279 --> 00:25:10,559
so the circle research work that's been

00:25:09,279 --> 00:25:12,720
done it helps us

00:25:10,559 --> 00:25:14,000
in the faithful representation and also

00:25:12,720 --> 00:25:16,159
the reversibility

00:25:14,000 --> 00:25:17,440
um let me explain so faithful

00:25:16,159 --> 00:25:20,720
representation is

00:25:17,440 --> 00:25:21,919
um we or the software tries to make sure

00:25:20,720 --> 00:25:23,360
that the user

00:25:21,919 --> 00:25:24,960
is understanding the information which

00:25:23,360 --> 00:25:26,960
is contained in the english sentence

00:25:24,960 --> 00:25:28,159
because these days a lot of information

00:25:26,960 --> 00:25:30,960
is available in english

00:25:28,159 --> 00:25:32,400
and not in the language that might be a

00:25:30,960 --> 00:25:32,799
mother tongue or might be a language

00:25:32,400 --> 00:25:34,960
that

00:25:32,799 --> 00:25:36,000
that is necessarily not english and

00:25:34,960 --> 00:25:39,520
something that you might

00:25:36,000 --> 00:25:41,039
find um something easy preference too so

00:25:39,520 --> 00:25:43,440
it would actually be pointless to have a

00:25:41,039 --> 00:25:44,960
translation that reads well but does not

00:25:43,440 --> 00:25:45,840
truly capture the information of the

00:25:44,960 --> 00:25:47,919
source text

00:25:45,840 --> 00:25:49,279
so what we do is we do not really focus

00:25:47,919 --> 00:25:52,000
on the translation we focus

00:25:49,279 --> 00:25:54,080
i mean we do focus on the translation um

00:25:52,000 --> 00:25:55,760
but we do not focus on reading we focus

00:25:54,080 --> 00:25:57,919
on capturing the information of the

00:25:55,760 --> 00:25:59,039
source code and this is very unique to

00:25:57,919 --> 00:26:01,760
our software

00:25:59,039 --> 00:26:02,480
and we we also see that you know um

00:26:01,760 --> 00:26:04,720
probably

00:26:02,480 --> 00:26:05,520
at every step when their information is

00:26:04,720 --> 00:26:08,880
given in

00:26:05,520 --> 00:26:11,840
it it is given back with

00:26:08,880 --> 00:26:12,559
the damage of information um that can be

00:26:11,840 --> 00:26:14,480
avoided

00:26:12,559 --> 00:26:16,000
is done gradually you you obviously

00:26:14,480 --> 00:26:19,039
cannot uh you know

00:26:16,000 --> 00:26:21,039
reduce the damage at once and

00:26:19,039 --> 00:26:22,240
obviously it would not be as perfect as

00:26:21,039 --> 00:26:25,279
human translation

00:26:22,240 --> 00:26:27,200
but you know with some efforts and some

00:26:25,279 --> 00:26:28,320
some consistent effort from the user

00:26:27,200 --> 00:26:31,679
some sort of training

00:26:28,320 --> 00:26:34,480
can probably help you in some sort of um

00:26:31,679 --> 00:26:36,000
easy transformation and reversibility is

00:26:34,480 --> 00:26:38,720
one of the biggest features in life

00:26:36,000 --> 00:26:39,039
in our system so what happens is you

00:26:38,720 --> 00:26:42,799
know

00:26:39,039 --> 00:26:44,559
um usually when you're given a word um

00:26:42,799 --> 00:26:46,559
there's not much transparency the

00:26:44,559 --> 00:26:48,080
transparency is reduced because we do

00:26:46,559 --> 00:26:49,039
not really know how many layers have

00:26:48,080 --> 00:26:50,960
been taken in

00:26:49,039 --> 00:26:52,320
what is the confidence to the end user

00:26:50,960 --> 00:26:53,919
because um

00:26:52,320 --> 00:26:55,919
you give a sentence to me and if i'm a

00:26:53,919 --> 00:26:56,559
machine translator system if i'm an

00:26:55,919 --> 00:26:58,400
empty

00:26:56,559 --> 00:27:00,720
um i can translate and give it back to

00:26:58,400 --> 00:27:02,240
you but you must also have confidence in

00:27:00,720 --> 00:27:03,919
me that you know there's some sort of

00:27:02,240 --> 00:27:06,960
transparency so

00:27:03,919 --> 00:27:09,360
um regardless of the

00:27:06,960 --> 00:27:11,440
text that is coming in we try to give

00:27:09,360 --> 00:27:13,200
analysis of all the output layers and we

00:27:11,440 --> 00:27:16,400
also try to give reference

00:27:13,200 --> 00:27:18,320
to the context um these are about the

00:27:16,400 --> 00:27:21,600
features

00:27:18,320 --> 00:27:23,360
and anybody who's beginning new um

00:27:21,600 --> 00:27:25,919
although the session was smart and you

00:27:23,360 --> 00:27:29,120
know probably intermediate i would say

00:27:25,919 --> 00:27:30,640
um nlp is an easy going thing and

00:27:29,120 --> 00:27:32,320
these are some important libraries for

00:27:30,640 --> 00:27:34,399
nlp sky kit learn

00:27:32,320 --> 00:27:35,760
um you use it mostly to do machine

00:27:34,399 --> 00:27:38,880
learning in python

00:27:35,760 --> 00:27:40,159
but then it it really helps you in the

00:27:38,880 --> 00:27:41,679
overall analysis of

00:27:40,159 --> 00:27:43,360
learning both machine learning and

00:27:41,679 --> 00:27:45,120
energy at once so

00:27:43,360 --> 00:27:46,799
um in a way it would be a great thing if

00:27:45,120 --> 00:27:49,120
you go through skykit learn

00:27:46,799 --> 00:27:50,080
and then we have the nrdk or the natural

00:27:49,120 --> 00:27:53,679
language toolkit

00:27:50,080 --> 00:27:56,399
this is one of the most amazing tool of

00:27:53,679 --> 00:27:57,840
by of uh one of the most amazing

00:27:56,399 --> 00:28:00,720
libraries for nfp

00:27:57,840 --> 00:28:02,240
because this literally has the

00:28:00,720 --> 00:28:03,600
capability to generate a machine

00:28:02,240 --> 00:28:05,200
transmission system for you

00:28:03,600 --> 00:28:06,799
and it has almost everything that you

00:28:05,200 --> 00:28:10,080
would need for an nmp

00:28:06,799 --> 00:28:11,919
um mmt that is the machine translator

00:28:10,080 --> 00:28:13,600
and then there's also the pattern

00:28:11,919 --> 00:28:15,120
library which i have not used much but

00:28:13,600 --> 00:28:16,720
i've heard that it's a really good

00:28:15,120 --> 00:28:19,039
mining module so

00:28:16,720 --> 00:28:20,799
um it has tools for both energy and

00:28:19,039 --> 00:28:22,320
machine learning but i would prefer

00:28:20,799 --> 00:28:23,919
skykit nerd because that's also being

00:28:22,320 --> 00:28:24,640
used in some sort of data analysis of

00:28:23,919 --> 00:28:28,399
data science

00:28:24,640 --> 00:28:29,760
and you know um some sort of

00:28:28,399 --> 00:28:31,360
the analysis that you might want to do

00:28:29,760 --> 00:28:32,880
on your documents which might not be

00:28:31,360 --> 00:28:35,279
available

00:28:32,880 --> 00:28:37,679
system so i feel that these three are

00:28:35,279 --> 00:28:40,000
the most important libraries for

00:28:37,679 --> 00:28:42,960
nrp specifically also for emoji

00:28:40,000 --> 00:28:42,960
translation system

00:28:43,039 --> 00:28:46,240
and this is one of the code which i like

00:28:45,120 --> 00:28:48,080
the most you know if

00:28:46,240 --> 00:28:49,600
if you can talk to a man in language he

00:28:48,080 --> 00:28:51,120
understands you you talk with him in

00:28:49,600 --> 00:28:54,159
whatever language you want

00:28:51,120 --> 00:28:58,000
but then if you talk i mean that

00:28:54,159 --> 00:29:00,559
goes to his head but if you want to talk

00:28:58,000 --> 00:29:01,279
um if you want to go to his heart then

00:29:00,559 --> 00:29:03,200
probably

00:29:01,279 --> 00:29:04,799
talking to him in his own language is

00:29:03,200 --> 00:29:07,679
the question that's said by

00:29:04,799 --> 00:29:09,440
the former south african president and

00:29:07,679 --> 00:29:11,840
one of the most amazing things is

00:29:09,440 --> 00:29:12,799
um we are forgetting what our own

00:29:11,840 --> 00:29:15,840
language is

00:29:12,799 --> 00:29:17,120
we are all complexicated by the english

00:29:15,840 --> 00:29:18,320
language or we are all

00:29:17,120 --> 00:29:20,159
into english language that we're

00:29:18,320 --> 00:29:23,520
actually forgetting our fruits

00:29:20,159 --> 00:29:24,880
so what that means is um in case you

00:29:23,520 --> 00:29:25,520
have a mother tongue in case you have a

00:29:24,880 --> 00:29:27,520
language

00:29:25,520 --> 00:29:29,279
and if there's not much machine

00:29:27,520 --> 00:29:31,120
translate won't have been done all day

00:29:29,279 --> 00:29:32,799
if there's not much energy work on it

00:29:31,120 --> 00:29:34,080
you could probably be the first one to

00:29:32,799 --> 00:29:35,200
define a dictionary and you know

00:29:34,080 --> 00:29:37,520
probably work on it

00:29:35,200 --> 00:29:38,960
um you do not really need to care about

00:29:37,520 --> 00:29:40,559
probably google translator being

00:29:38,960 --> 00:29:41,520
translated they do not really have good

00:29:40,559 --> 00:29:43,840
efficiency

00:29:41,520 --> 00:29:45,200
i mean when comparing to what you can

00:29:43,840 --> 00:29:46,880
build because i feel there's a lot of

00:29:45,200 --> 00:29:47,919
scope this there's a lot of work that

00:29:46,880 --> 00:29:49,919
can be done

00:29:47,919 --> 00:29:51,600
so if you have your own language i i

00:29:49,919 --> 00:29:54,159
assume everybody has their

00:29:51,600 --> 00:29:55,679
own mother tongue and you know you can

00:29:54,159 --> 00:29:56,320
probably work really well with your own

00:29:55,679 --> 00:29:59,279
language

00:29:56,320 --> 00:30:00,240
i would suggest you to get begin with

00:29:59,279 --> 00:30:02,000
the nlp

00:30:00,240 --> 00:30:04,480
machine translation system building and

00:30:02,000 --> 00:30:07,919
yes that should help you

00:30:04,480 --> 00:30:08,799
and lastly that is all about it um i'm

00:30:07,919 --> 00:30:10,559
open for doubts

00:30:08,799 --> 00:30:12,720
and you can contact me at this email id

00:30:10,559 --> 00:30:13,279
for any sort of you know projects coming

00:30:12,720 --> 00:30:15,440
in

00:30:13,279 --> 00:30:17,039
or any sort of collaborations and any

00:30:15,440 --> 00:30:17,600
links to my linkedin or github are in

00:30:17,039 --> 00:30:23,840
the

00:30:17,600 --> 00:30:23,840
um africa webpage

00:30:28,000 --> 00:30:30,080

YouTube URL: https://www.youtube.com/watch?v=TjQvPz4IKNQ


