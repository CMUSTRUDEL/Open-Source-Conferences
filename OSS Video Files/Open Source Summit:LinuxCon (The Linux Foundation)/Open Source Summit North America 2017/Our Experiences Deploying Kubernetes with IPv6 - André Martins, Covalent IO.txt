Title: Our Experiences Deploying Kubernetes with IPv6 - André Martins, Covalent IO
Publication date: 2017-09-14
Playlist: Open Source Summit North America 2017
Description: 
	Our Experiences Deploying Kubernetes with IPv6 - André Martins, Covalent IO

IPv6 will turn 20 years old in 2018, IPv4 addresses are nearly extinct, it is time to give IPv6 a real chance. This talk will cover the process of deploying Kubernetes with IPv6 step by step. We will discuss the current state of IPv6 in Kubernetes and all related components and list what is left to be done. We will walk through the deployment step by step in a easy to follow demo where questions can be asked. This talk will give a chance to learn more about Kubernetes networking and how IPv6 will enable to scale public addressing inside your Kubernetes cluster.

About André Martins
André Martins started his open source career through a Linux Foundation Internship 3 years ago where he focused on projects that ease the deployment of networking infrastructures by contributing to the OpenDaylight project. André then switched focused to containers and orchestration and started contributing to Kubernetes and Docker. He is also a co-creator of the Cilium Project which brings BPF to the container networking and security world to provide faster networking, better security, tracing and visibility for application developers and infrastructure operators.
Captions: 
	00:00:00,030 --> 00:00:08,400
hello good afternoon so I'm here to

00:00:02,639 --> 00:00:12,300
about with ipv6 I work at covalent i/o

00:00:08,400 --> 00:00:14,370
we are developing a software called

00:00:12,300 --> 00:00:23,689
cilium it's open source that allows you

00:00:14,370 --> 00:00:23,689
to have security at layers and

00:00:24,900 --> 00:00:31,900
it works well and it also provides happy

00:00:29,289 --> 00:00:34,750
with six connectivity in your

00:00:31,900 --> 00:00:37,510
infrastructure as well so what should

00:00:34,750 --> 00:00:39,250
you expect from this talk I'm going to

00:00:37,510 --> 00:00:43,720
give a really quick history is just

00:00:39,250 --> 00:00:46,690
about ipv6 and then I'll talk about ipv6

00:00:43,720 --> 00:00:50,229
in kubernetes itself if it can throw it

00:00:46,690 --> 00:00:52,120
can run it can run it or not and while

00:00:50,229 --> 00:00:54,550
I'm doing that I'll also do a step by

00:00:52,120 --> 00:00:57,219
step tutorial to talk about each

00:00:54,550 --> 00:00:58,570
component in kubernetes and what you

00:00:57,219 --> 00:01:03,609
need to change or not

00:00:58,570 --> 00:01:05,860
in each options so let's see about IP

00:01:03,609 --> 00:01:10,780
physics is true so everything started in

00:01:05,860 --> 00:01:13,479
around 91 when ITF might wondering if we

00:01:10,780 --> 00:01:14,350
had enough ipv4 addresses available for

00:01:13,479 --> 00:01:18,009
in the future

00:01:14,350 --> 00:01:20,020
so and as they realized that we might

00:01:18,009 --> 00:01:26,020
not have a problem in the future they

00:01:20,020 --> 00:01:28,179
created a group called road and in 95

00:01:26,020 --> 00:01:30,790
they developed the first personal foul

00:01:28,179 --> 00:01:34,720
let's call IP next generation which was

00:01:30,790 --> 00:01:37,509
the draft for ipv6 and after a couple

00:01:34,720 --> 00:01:41,979
years like three years later it came up

00:01:37,509 --> 00:01:46,600
the RFC 2460 which was the ipv6 draft

00:01:41,979 --> 00:01:51,520
for RFC and in 2005 there was a stable

00:01:46,600 --> 00:01:53,380
version of ipv6 in linux and in 2008 we

00:01:51,520 --> 00:01:56,710
start seeing some news regarding the

00:01:53,380 --> 00:01:58,719
ipv4 exhaustion and we should do

00:01:56,710 --> 00:02:00,939
something about it some ice P start

00:01:58,719 --> 00:02:04,840
looking around and they thought the best

00:02:00,939 --> 00:02:08,170
way to to prevent a ipv4 exhaustion was

00:02:04,840 --> 00:02:11,500
to do not over net over net and instead

00:02:08,170 --> 00:02:14,530
of upgrading directly to ipv6 and in

00:02:11,500 --> 00:02:17,620
around 2000 1214 container starting to

00:02:14,530 --> 00:02:20,799
pop up which were they were designed to

00:02:17,620 --> 00:02:25,870
have ipv4 connectivity and not going

00:02:20,799 --> 00:02:28,500
directly to ipv6 and July this year ipv6

00:02:25,870 --> 00:02:33,250
was considered the internet standard

00:02:28,500 --> 00:02:36,860
number 86 and hopefully let's see if

00:02:33,250 --> 00:02:41,660
next year in 2018 will be the

00:02:36,860 --> 00:02:44,810
ipv6 so what about government is running

00:02:41,660 --> 00:02:46,820
with ipv6 the first question that I came

00:02:44,810 --> 00:02:49,400
up with it was is it really worth it to

00:02:46,820 --> 00:02:51,800
make this talk and what about

00:02:49,400 --> 00:02:53,240
infrastructure if I have enough may I

00:02:51,800 --> 00:02:55,820
have the infrastructure to run

00:02:53,240 --> 00:02:57,290
kubernetes what happen is 6 and what

00:02:55,820 --> 00:03:01,010
about the coordinate is itself does it

00:02:57,290 --> 00:03:04,460
run does the pods and the services at

00:03:01,010 --> 00:03:06,110
the ingress communities concepts do I

00:03:04,460 --> 00:03:08,660
need to make any changes at all and

00:03:06,110 --> 00:03:11,510
after that if I have a coordinated

00:03:08,660 --> 00:03:14,810
infrastructure what about my apps will

00:03:11,510 --> 00:03:17,570
they run it or not so let's find out so

00:03:14,810 --> 00:03:20,840
when I start googling the first result

00:03:17,570 --> 00:03:24,800
that I got was if there is any benefit

00:03:20,840 --> 00:03:26,990
of using ipv6 in my home and I'm not

00:03:24,800 --> 00:03:30,170
sure if you can see it the first answer

00:03:26,990 --> 00:03:32,810
here was a funny answer was but with

00:03:30,170 --> 00:03:36,070
ipv4 you cannot have billions of IP

00:03:32,810 --> 00:03:38,930
addresses for your own biases and we did

00:03:36,070 --> 00:03:45,350
jingyang will not be able to have his

00:03:38,930 --> 00:03:48,380
smart fridge in this home so but the

00:03:45,350 --> 00:03:50,480
answer that was chosen was this one no

00:03:48,380 --> 00:03:53,090
there is not any benefit of using ipv6

00:03:50,480 --> 00:03:53,510
at home which I don't completely agree

00:03:53,090 --> 00:03:55,459
with

00:03:53,510 --> 00:03:57,739
I agree more with the next answer

00:03:55,459 --> 00:04:00,230
available which was yes there is a

00:03:57,739 --> 00:04:02,989
benefit if you are using for education

00:04:00,230 --> 00:04:04,760
if you want to try it out ipv6 I think

00:04:02,989 --> 00:04:07,310
the safest place to use it is in your

00:04:04,760 --> 00:04:09,400
home but it can bright things and at

00:04:07,310 --> 00:04:13,730
least to find out if things work or not

00:04:09,400 --> 00:04:16,040
and later on you can at least deploy it

00:04:13,730 --> 00:04:19,549
in your infrastructure in your company

00:04:16,040 --> 00:04:22,220
infrastructure of course your house is

00:04:19,549 --> 00:04:24,169
not the data centers so unless it's a

00:04:22,220 --> 00:04:26,360
start-up of course but if it is a start

00:04:24,169 --> 00:04:29,930
up you start to plug things up and

00:04:26,360 --> 00:04:34,050
things might burn out it is

00:04:29,930 --> 00:04:37,259
so this concept petrol scuttle is not

00:04:34,050 --> 00:04:41,400
you with something that it has popping

00:04:37,259 --> 00:04:43,979
up around the containers world and in

00:04:41,400 --> 00:04:45,659
your house you usually have pets so what

00:04:43,979 --> 00:04:49,379
does this mean does this this means you

00:04:45,659 --> 00:04:51,870
have an IP out of / 24 for example for

00:04:49,379 --> 00:04:56,189
each type of pet and you know which pet

00:04:51,870 --> 00:04:58,289
has which IP address but in your data

00:04:56,189 --> 00:05:02,550
center you have more pets so you have

00:04:58,289 --> 00:05:06,000
cattle and you start doing / sacred / 8

00:05:02,550 --> 00:05:08,189
for that each type of animal so we have

00:05:06,000 --> 00:05:10,650
cows and you have ships and you start

00:05:08,189 --> 00:05:15,560
spitting them by colors for example have

00:05:10,650 --> 00:05:18,150
black cars on / 16 and so on so but

00:05:15,560 --> 00:05:20,610
hopefully your data center doesn't have

00:05:18,150 --> 00:05:22,979
actual cattle it has containers so you

00:05:20,610 --> 00:05:26,069
start doing that this approach with

00:05:22,979 --> 00:05:29,789
containers start splitting them up by

00:05:26,069 --> 00:05:31,949
color types for example and as your

00:05:29,789 --> 00:05:34,020
container starts to growing up as the

00:05:31,949 --> 00:05:36,029
numeral developer starting growing up

00:05:34,020 --> 00:05:38,430
the number of containers start to grow

00:05:36,029 --> 00:05:40,940
up as well and your users as well so you

00:05:38,430 --> 00:05:43,319
end up with millions of containers and

00:05:40,940 --> 00:05:47,009
dressing to manage that will be

00:05:43,319 --> 00:05:49,199
impossible to control that so what is

00:05:47,009 --> 00:05:52,469
the best solution for this will be ipv6

00:05:49,199 --> 00:05:54,150
or no let's do net for example let's do

00:05:52,469 --> 00:05:56,069
a snap that let's always solves a

00:05:54,150 --> 00:05:58,669
problem with this so we have a

00:05:56,069 --> 00:06:01,889
multi-level net and you start having

00:05:58,669 --> 00:06:04,319
containers on top of PM's and then you

00:06:01,889 --> 00:06:07,620
have your club provider or do I speed

00:06:04,319 --> 00:06:09,779
they also use Nets oh yeah you are

00:06:07,620 --> 00:06:11,219
wasting resources and your users will be

00:06:09,779 --> 00:06:14,069
happy about it because they'll have a

00:06:11,219 --> 00:06:16,759
slow connection that that's also always

00:06:14,069 --> 00:06:19,979
a good idea no buts yours let's go to

00:06:16,759 --> 00:06:21,870
assemble an ipv6 cluster so you usually

00:06:19,979 --> 00:06:24,659
end up with two options in the beginning

00:06:21,870 --> 00:06:28,500
you either deployed on premises or do

00:06:24,659 --> 00:06:30,180
you deploy it on on on the cloud and the

00:06:28,500 --> 00:06:32,789
first question that you might wondering

00:06:30,180 --> 00:06:36,210
is that is my operating system supports

00:06:32,789 --> 00:06:37,550
ipv6 or not well as I've shown in the

00:06:36,210 --> 00:06:39,620
first life

00:06:37,550 --> 00:06:43,069
at least Linux has a stable version of

00:06:39,620 --> 00:06:45,349
ipv6 since 2005 and if you are not

00:06:43,069 --> 00:06:47,870
upgrading your Linux since 2005 I think

00:06:45,349 --> 00:06:52,669
you have worse problems than to worry

00:06:47,870 --> 00:06:55,520
about ipv6 and the next one will be does

00:06:52,669 --> 00:06:57,949
my servers support ipv6 or does the

00:06:55,520 --> 00:07:01,569
cloud provider support ipv6 you for

00:06:57,949 --> 00:07:04,069
example here I have look at AWS and JC

00:07:01,569 --> 00:07:07,490
edible yes can at least provide you a

00:07:04,069 --> 00:07:09,470
public IP v6 to the VM itself GC can

00:07:07,490 --> 00:07:11,870
provide you at least as far as I know

00:07:09,470 --> 00:07:15,580
can provide at least until the load

00:07:11,870 --> 00:07:18,259
balancer so inside your your own a

00:07:15,580 --> 00:07:23,419
cluster you have ipv4 until the load

00:07:18,259 --> 00:07:26,180
bounds that you can have ipv6 and what

00:07:23,419 --> 00:07:29,030
about your users will they use ipv6 well

00:07:26,180 --> 00:07:32,539
I took this screenshot at the end of

00:07:29,030 --> 00:07:35,300
August this one here and we can find out

00:07:32,539 --> 00:07:38,300
it one in five users are using ipv6

00:07:35,300 --> 00:07:41,389
worldwide so I think it's it's a good

00:07:38,300 --> 00:07:44,210
idea to start looking at ipv6 and this

00:07:41,389 --> 00:07:46,430
is what this is worldwide and this is

00:07:44,210 --> 00:07:50,719
from Akamai the top countries that are

00:07:46,430 --> 00:07:53,569
using ipv6 so we can see in Belgium at

00:07:50,719 --> 00:07:55,789
least half of the half of the users are

00:07:53,569 --> 00:07:59,180
using already ipv6 and United States

00:07:55,789 --> 00:08:02,150
alone it's two out of five so at least

00:07:59,180 --> 00:08:05,479
for the for the Belgium side if you have

00:08:02,150 --> 00:08:08,870
Belgium customers we should starting to

00:08:05,479 --> 00:08:10,789
get worried about this so let's go to

00:08:08,870 --> 00:08:15,349
the purpose of this talk let's go a

00:08:10,789 --> 00:08:18,639
little bit tip types so this is a

00:08:15,349 --> 00:08:21,650
kubernetes normal cluster like you have

00:08:18,639 --> 00:08:23,810
master and you can have multiple masters

00:08:21,650 --> 00:08:26,060
and you usually have multiple workers on

00:08:23,810 --> 00:08:28,310
the master side you have controller

00:08:26,060 --> 00:08:30,440
manager the API server and the scheduler

00:08:28,310 --> 00:08:32,870
and you also have the it CD which stores

00:08:30,440 --> 00:08:34,669
all the data and on the worker side you

00:08:32,870 --> 00:08:37,430
usually have a container of runtime and

00:08:34,669 --> 00:08:40,610
you have couplet cube proxy and the CNF

00:08:37,430 --> 00:08:43,029
plugin to manage all the network in the

00:08:40,610 --> 00:08:46,010
containers running in a worker

00:08:43,029 --> 00:08:49,279
so let's start with the ETD in the

00:08:46,010 --> 00:08:50,130
master side so it's it is 53 silly

00:08:49,279 --> 00:08:52,290
options

00:08:50,130 --> 00:08:53,850
and I'm talking about Italy not it's the

00:08:52,290 --> 00:08:57,420
cattle which is the client to connect

00:08:53,850 --> 00:09:00,540
with city and out of which only five of

00:08:57,420 --> 00:09:03,350
them are relevant for activities and we

00:09:00,540 --> 00:09:05,579
can see that this is mostly like

00:09:03,350 --> 00:09:07,769
addressing it so it's just an address

00:09:05,579 --> 00:09:12,269
and we can simply replace the address

00:09:07,769 --> 00:09:15,149
with the with a local house of ipv6 it's

00:09:12,269 --> 00:09:17,190
a simple yes if it works it will work if

00:09:15,149 --> 00:09:19,170
it's not working you'll eventually find

00:09:17,190 --> 00:09:21,860
out and you can report it on a github

00:09:19,170 --> 00:09:24,899
issue says this is all open source and

00:09:21,860 --> 00:09:28,170
if you're asking me what about HTTPS

00:09:24,899 --> 00:09:31,649
well HTTPS should not matter if it's run

00:09:28,170 --> 00:09:33,660
on ipv4 or ipv6 but if you are talking

00:09:31,649 --> 00:09:36,600
about certificates then yes the

00:09:33,660 --> 00:09:39,240
configurations are aware of ipv6 so you

00:09:36,600 --> 00:09:42,720
can have a configuration for ipv6 in the

00:09:39,240 --> 00:09:46,259
certificate itself I will use HTTPS and

00:09:42,720 --> 00:09:50,339
HCD so I'll have certificates with with

00:09:46,259 --> 00:09:53,190
ipv6 on so I'll start by deploying it CD

00:09:50,339 --> 00:09:55,730
on the on the master node and I will

00:09:53,190 --> 00:09:59,069
have only one instance of it CD running

00:09:55,730 --> 00:10:01,250
can you guys see the the fonts on the

00:09:59,069 --> 00:10:01,250
back

00:10:07,180 --> 00:10:13,960
so this will be my service of service

00:10:11,530 --> 00:10:16,960
file for each CDI you can see that I

00:10:13,960 --> 00:10:18,610
have lots of options here but the

00:10:16,960 --> 00:10:21,970
important ones are the ones that have

00:10:18,610 --> 00:10:25,180
ipv6 on it and I also have the

00:10:21,970 --> 00:10:28,420
certificates they have they are aware of

00:10:25,180 --> 00:10:32,140
the ipv6 address in there and this is

00:10:28,420 --> 00:10:38,580
basically it on the ETD side so let's

00:10:32,140 --> 00:10:38,580
start let's start a CD

00:10:46,780 --> 00:10:56,500
I always messed up the operation and we

00:10:53,590 --> 00:11:00,460
can see they are up in it's up and

00:10:56,500 --> 00:11:05,410
running since 40 seconds ago so at least

00:11:00,460 --> 00:11:08,320
it's working so far so let's move on for

00:11:05,410 --> 00:11:10,990
the next component which is kubernetes

00:11:08,320 --> 00:11:13,900
now we are going to coop scheduler coop

00:11:10,990 --> 00:11:18,040
scheduler is a simple component it only

00:11:13,900 --> 00:11:21,130
has around 30 options and only kind of

00:11:18,040 --> 00:11:27,280
three of them are relevant for ipv6 in

00:11:21,130 --> 00:11:29,080
our puzzle this will we can also apply

00:11:27,280 --> 00:11:32,500
the same solution I have applied in each

00:11:29,080 --> 00:11:34,990
CD and we can see if it works or not

00:11:32,500 --> 00:11:37,210
like the same way we have implemented CD

00:11:34,990 --> 00:11:42,250
this is a simple component will get

00:11:37,210 --> 00:11:45,520
regarding the ipv6 side of our cluster

00:11:42,250 --> 00:11:50,830
because it only manages which node will

00:11:45,520 --> 00:11:52,540
receive which pod so I will not start

00:11:50,830 --> 00:11:55,690
tube scheduler now because we do need

00:11:52,540 --> 00:11:58,720
cube API server to run first and later

00:11:55,690 --> 00:12:01,510
on I'll run both at the same time so

00:11:58,720 --> 00:12:03,790
cube API server is a really important

00:12:01,510 --> 00:12:07,990
piece of the of the puzzle in kubernetes

00:12:03,790 --> 00:12:11,460
so it has 120 CLI options and our only

00:12:07,990 --> 00:12:14,770
five of them already relevant for ipv6

00:12:11,460 --> 00:12:16,480
so we have a new option which is also

00:12:14,770 --> 00:12:20,170
important for the kubernetes cluster

00:12:16,480 --> 00:12:24,420
which is the surface plus IP range and

00:12:20,170 --> 00:12:26,710
I've selected fd0 3 column column 112 so

00:12:24,420 --> 00:12:30,520
what does this mean this means I can

00:12:26,710 --> 00:12:33,640
have 65 file around 65,000 different

00:12:30,520 --> 00:12:37,150
services running on kubernetes and a

00:12:33,640 --> 00:12:40,030
service allows you to have multiple pods

00:12:37,150 --> 00:12:41,950
serving the same service but the service

00:12:40,030 --> 00:12:45,880
will be an abstraction for those pods

00:12:41,950 --> 00:12:50,620
running on back-end so a few controller

00:12:45,880 --> 00:12:54,160
manager will assign automatically an IP

00:12:50,620 --> 00:12:56,290
out of those range 112 and it will

00:12:54,160 --> 00:12:58,120
automatically assign for the service and

00:12:56,290 --> 00:13:00,010
this way

00:12:58,120 --> 00:13:02,170
all the other pods will be able to

00:13:00,010 --> 00:13:04,750
communicate with the service without

00:13:02,170 --> 00:13:07,990
knowing the destination IP because parts

00:13:04,750 --> 00:13:11,850
come and go and you cannot make make

00:13:07,990 --> 00:13:17,770
sure which products which idea at this

00:13:11,850 --> 00:13:19,600
central point in time a warning here

00:13:17,770 --> 00:13:22,450
please do not try this at home with

00:13:19,600 --> 00:13:25,750
kubernetes less than 1.8 because this

00:13:22,450 --> 00:13:29,470
was a pull request recently marked and

00:13:25,750 --> 00:13:33,130
it will be available and 1.8 that will

00:13:29,470 --> 00:13:35,529
be released in the like later this month

00:13:33,130 --> 00:13:37,720
this option service cluster IP is

00:13:35,529 --> 00:13:41,410
available for a TV for but the pull

00:13:37,720 --> 00:13:45,190
request was designed for the ipv6 plus

00:13:41,410 --> 00:13:48,670
to have the ipv6 option enable so let's

00:13:45,190 --> 00:13:57,610
start cube iti server first and then

00:13:48,670 --> 00:14:01,000
let's start cube schedule so this is the

00:13:57,610 --> 00:14:05,980
service file we can see here I have the

00:14:01,000 --> 00:14:08,440
Protoss address which is the node IP the

00:14:05,980 --> 00:14:12,540
ETD servers which is deity deserved that

00:14:08,440 --> 00:14:15,730
I've started up three minutes ago and

00:14:12,540 --> 00:14:18,880
also the the certificates to to connect

00:14:15,730 --> 00:14:22,620
to HDD and also the service plus the IP

00:14:18,880 --> 00:14:22,620
range and I've talked about previously

00:14:25,470 --> 00:14:38,440
so let's start a cube API server and

00:14:31,360 --> 00:14:40,740
let's say let's shake the status that's

00:14:38,440 --> 00:14:45,010
up and running so so far we have to

00:14:40,740 --> 00:14:58,080
running it CDL cube API server and now I

00:14:45,010 --> 00:14:58,080
have to start and cube scheduler mm-hm

00:15:00,270 --> 00:15:12,280
yeah yeah it's about to be 6 as well so

00:15:05,860 --> 00:15:15,510
on this this cube scheduler only has 30

00:15:12,280 --> 00:15:17,770
options I will use cube config to have

00:15:15,510 --> 00:15:20,680
everything to connect to the master

00:15:17,770 --> 00:15:22,660
inside so cube config is a configuration

00:15:20,680 --> 00:15:24,820
file that you can create each couplet

00:15:22,660 --> 00:15:27,490
and it allows you to create a

00:15:24,820 --> 00:15:31,780
configuration file to have a fine to

00:15:27,490 --> 00:15:33,580
know how to connect to the API server so

00:15:31,780 --> 00:15:37,240
what the screen is too small to see all

00:15:33,580 --> 00:15:39,880
the file but it has the certificate

00:15:37,240 --> 00:15:42,940
embedded and it also has the server

00:15:39,880 --> 00:15:44,920
address here as you can see and the user

00:15:42,940 --> 00:15:51,270
will be the cube scheduler that I've set

00:15:44,920 --> 00:15:51,270
up so let's start you scheduler

00:15:55,010 --> 00:15:58,790
and check the status

00:16:02,100 --> 00:16:10,870
okay so been running so far things are

00:16:05,610 --> 00:16:12,760
going well so the next one will be an

00:16:10,870 --> 00:16:14,830
important component also an important

00:16:12,760 --> 00:16:18,070
component of cornetist which is the cube

00:16:14,830 --> 00:16:19,960
controller so cue controller manage the

00:16:18,070 --> 00:16:23,380
whole cluster is the brain of the of the

00:16:19,960 --> 00:16:26,740
whole cluster and it has five relevant

00:16:23,380 --> 00:16:30,730
options similar with cool API server but

00:16:26,740 --> 00:16:32,860
we are having three important ones

00:16:30,730 --> 00:16:35,320
because they have specifically the IP

00:16:32,860 --> 00:16:37,270
addresses on it so let's take a look at

00:16:35,320 --> 00:16:40,120
them so we have the service cluster

00:16:37,270 --> 00:16:42,040
silent that we service plus our IP range

00:16:40,120 --> 00:16:44,650
it's the exact same one that we are

00:16:42,040 --> 00:16:47,710
specified in cube ipi server previously

00:16:44,650 --> 00:16:49,630
and the services will be across cluster

00:16:47,710 --> 00:16:52,990
so it will be the abstraction for the

00:16:49,630 --> 00:16:55,390
pods running and now we have the cluster

00:16:52,990 --> 00:16:58,030
cider so the cluster cider will be

00:16:55,390 --> 00:17:00,910
decided for your pods running in

00:16:58,030 --> 00:17:02,680
kubernetes do not confuse them don't do

00:17:00,910 --> 00:17:04,540
not confuse the cluster cider with the

00:17:02,680 --> 00:17:07,600
with the physical cluster addressing

00:17:04,540 --> 00:17:09,970
that you will set up for your VMs so all

00:17:07,600 --> 00:17:11,709
the pods running in my password will

00:17:09,970 --> 00:17:14,890
have F D is your troop column column

00:17:11,709 --> 00:17:19,120
slash effective and now we have the node

00:17:14,890 --> 00:17:21,189
cider mass size which would be 96 what

00:17:19,120 --> 00:17:23,079
does this mean so we have the cluster

00:17:21,189 --> 00:17:25,720
cider which will be that one I'm not

00:17:23,079 --> 00:17:29,590
sure if you can see the balls which are

00:17:25,720 --> 00:17:32,050
the the mass for the for the IP and now

00:17:29,590 --> 00:17:34,780
we have the node cider of 96 this means

00:17:32,050 --> 00:17:37,630
the first node that will register itself

00:17:34,780 --> 00:17:42,190
to the cure to the cube controller will

00:17:37,630 --> 00:17:46,810
have a cider of a subnet of / 96 out of

00:17:42,190 --> 00:17:51,700
the / 80 the second one will have so the

00:17:46,810 --> 00:17:54,100
second one will have ft 0 - 0 0 0 0 1 so

00:17:51,700 --> 00:17:56,830
which will be submit out of the / ID

00:17:54,100 --> 00:17:59,500
this is the net out the kubernetes deals

00:17:56,830 --> 00:18:03,700
with the networking so it adds a subnet

00:17:59,500 --> 00:18:05,620
for each node out of the whole network

00:18:03,700 --> 00:18:09,390
that will be assigned for the pods

00:18:05,620 --> 00:18:12,850
containers and the last now we'll have

00:18:09,390 --> 00:18:16,720
ffff count columns you comment

00:18:12,850 --> 00:18:19,630
/ 96 as well so we can have 65,000 nodes

00:18:16,720 --> 00:18:21,580
and on each node we can have four point

00:18:19,630 --> 00:18:25,110
three billion containers if you want to

00:18:21,580 --> 00:18:28,420
run it for pictorial in containers so

00:18:25,110 --> 00:18:30,370
this is lots of IP addresses but you

00:18:28,420 --> 00:18:32,170
don't you should not care about IP

00:18:30,370 --> 00:18:34,180
addresses at this point because you have

00:18:32,170 --> 00:18:37,060
a controller manager that deals with it

00:18:34,180 --> 00:18:42,670
and you should forget about IP addresses

00:18:37,060 --> 00:18:44,860
so you should really only to know where

00:18:42,670 --> 00:18:47,080
the pubs will be how the pods will be

00:18:44,860 --> 00:18:49,840
deployed you should not care about a is

00:18:47,080 --> 00:18:51,760
running on worker 1 if puppies were

00:18:49,840 --> 00:18:54,580
running on worker - you should have

00:18:51,760 --> 00:18:57,190
other options if you want security you

00:18:54,580 --> 00:19:03,000
should look at different levels than l3

00:18:57,190 --> 00:19:06,930
l2 and l3 at this point so let's go

00:19:03,000 --> 00:19:06,930
let's start queue controller

00:19:20,060 --> 00:19:22,930
this one

00:19:29,890 --> 00:19:33,670
you'll control a minute

00:19:36,660 --> 00:19:44,730
so I am the cluster cider here which is

00:19:39,570 --> 00:19:47,400
ft0 - and an outsider mask oh I also

00:19:44,730 --> 00:19:51,060
have the allocated cider so this is what

00:19:47,400 --> 00:19:53,970
a cube controller will know that it

00:19:51,060 --> 00:19:57,600
should assign to each node a particular

00:19:53,970 --> 00:20:03,450
cider out of the / 80 and I also have

00:19:57,600 --> 00:20:11,310
the service cluster IP range here so

00:20:03,450 --> 00:20:14,330
let's start a cube controller and let's

00:20:11,310 --> 00:20:14,330
check the status

00:20:16,800 --> 00:20:21,850
up and running so so far we are good in

00:20:20,080 --> 00:20:26,230
the workers side in the master side

00:20:21,850 --> 00:20:28,660
let's move on to the worker and in the

00:20:26,230 --> 00:20:31,030
workers side you you have a controller

00:20:28,660 --> 00:20:35,290
runtime on my case I choose docker

00:20:31,030 --> 00:20:37,990
and so the talk the network plumbing

00:20:35,290 --> 00:20:41,260
will be made by in kubernetes will be

00:20:37,990 --> 00:20:44,170
made by CNI and you can find the reasons

00:20:41,260 --> 00:20:46,720
in the link that is in on the slide or

00:20:44,170 --> 00:20:48,190
if you google why kubernetes doesn't use

00:20:46,720 --> 00:20:50,350
live Network

00:20:48,190 --> 00:20:52,900
so what is live Network live network

00:20:50,350 --> 00:20:54,790
clip network is the plugin for docker

00:20:52,900 --> 00:20:58,780
only if you start a container with

00:20:54,790 --> 00:21:01,930
docker ran - - net and you choose a

00:20:58,780 --> 00:21:04,900
plugin it will be the live network

00:21:01,930 --> 00:21:07,810
plugin that will run in docker in docker

00:21:04,900 --> 00:21:09,850
itself and kubernetes allows you to have

00:21:07,810 --> 00:21:13,720
a CNI plugin which is different which is

00:21:09,850 --> 00:21:16,320
a different type of target it doesn't it

00:21:13,720 --> 00:21:18,910
does not belong to docket itself and

00:21:16,320 --> 00:21:24,510
it's a different choice that they have

00:21:18,910 --> 00:21:24,510
made for their for their for kubernetes

00:21:24,540 --> 00:21:32,620
so the Siena plugin that i will choose

00:21:27,520 --> 00:21:35,290
will be psyllium so as I told in the

00:21:32,620 --> 00:21:38,920
beginning ceiling provides the l7

00:21:35,290 --> 00:21:39,640
security as well ipv6 as a first-class

00:21:38,920 --> 00:21:41,380
citizen

00:21:39,640 --> 00:21:43,660
whenever you are creating psyllium in

00:21:41,380 --> 00:21:46,150
the beginning we thought okay so we need

00:21:43,660 --> 00:21:48,610
to have something for containers the

00:21:46,150 --> 00:21:51,820
first thing we thought about it was we

00:21:48,610 --> 00:21:54,850
need high scalability so and we were not

00:21:51,820 --> 00:21:58,270
going to choose ipv4 we later on find

00:21:54,850 --> 00:22:01,660
out like we have to have happy before

00:21:58,270 --> 00:22:05,800
but for other environments then don't

00:22:01,660 --> 00:22:08,290
use ipv6 yet so psyllium will use the

00:22:05,800 --> 00:22:10,630
will be aware of the options that I've

00:22:08,290 --> 00:22:12,910
chosen in control and manager which is

00:22:10,630 --> 00:22:15,730
the locating Outsiders the cluster sided

00:22:12,910 --> 00:22:18,850
an outsider and it will know which side

00:22:15,730 --> 00:22:21,460
it will be running on each node and it

00:22:18,850 --> 00:22:24,610
will route all the traffic across nodes

00:22:21,460 --> 00:22:26,350
and so that's why I told before that you

00:22:24,610 --> 00:22:29,179
should not care about IPS because the

00:22:26,350 --> 00:22:33,559
CNF plugin will be taking care of

00:22:29,179 --> 00:22:36,049
so you should not really be aware of

00:22:33,559 --> 00:22:38,330
that this anyone and you also have the

00:22:36,049 --> 00:22:40,969
service routing so the services that

00:22:38,330 --> 00:22:42,979
I've explained before as well will be

00:22:40,969 --> 00:22:45,379
the routing of those services will be

00:22:42,979 --> 00:22:48,200
made by the CNI plug-in in this case

00:22:45,379 --> 00:22:51,139
you'll be my PI silly if you are using a

00:22:48,200 --> 00:22:53,269
different channel plug-in it might not

00:22:51,139 --> 00:22:58,219
be the same a plug-in itself that will

00:22:53,269 --> 00:23:01,759
deal with this it will be Q proxy so I

00:22:58,219 --> 00:23:04,219
will not run tube proxy because scene I

00:23:01,759 --> 00:23:07,519
assume does already the surface routing

00:23:04,219 --> 00:23:11,210
but some plugins will use it so as far

00:23:07,519 --> 00:23:13,249
as I know there are not an option that

00:23:11,210 --> 00:23:15,710
you need to change into procedure and

00:23:13,249 --> 00:23:21,320
with ipv6 if you are using a different a

00:23:15,710 --> 00:23:26,139
different plugin then Silvia and we have

00:23:21,320 --> 00:23:31,129
to put which has the 160 CLI options but

00:23:26,139 --> 00:23:35,479
fortunately we only need 3 and so we

00:23:31,129 --> 00:23:37,700
need the master DNS which is a DNS that

00:23:35,479 --> 00:23:41,210
will run on the on the kubernetes

00:23:37,700 --> 00:23:43,599
cluster and this IP needs to to be

00:23:41,210 --> 00:23:46,639
written beforehand that's why I had

00:23:43,599 --> 00:23:49,460
cubed NS running with this IP it's just

00:23:46,639 --> 00:23:52,729
about be out of the range of the service

00:23:49,460 --> 00:23:55,849
cluster IP range and I also specified

00:23:52,729 --> 00:23:59,629
the node IP of the node itself in on

00:23:55,849 --> 00:24:03,919
each worker and the node IP is still

00:23:59,629 --> 00:24:06,799
pull request that number over there it's

00:24:03,919 --> 00:24:09,919
not we have been merged yet but all of

00:24:06,799 --> 00:24:14,599
the this demo data I'm doing right now

00:24:09,919 --> 00:24:22,700
it has dispatch compiled with version

00:24:14,599 --> 00:24:27,619
1.8 better running on so I'll run

00:24:22,700 --> 00:24:29,929
cubelet and I'll sucess my machine is

00:24:27,619 --> 00:24:32,599
not that powerful I will have qubit on

00:24:29,929 --> 00:24:34,789
the same VM they type the worker on so

00:24:32,599 --> 00:24:37,759
I'll have a dedicated worker and I'll

00:24:34,789 --> 00:24:41,710
have a VM running master and the cubelet

00:24:37,759 --> 00:24:41,710
or it will be the master slash worker

00:24:49,850 --> 00:24:55,820
okay so you can see the options here the

00:24:53,630 --> 00:24:59,750
node IP the network plugin which should

00:24:55,820 --> 00:25:08,179
be CNI and also the cluster DNS that we

00:24:59,750 --> 00:25:14,120
you need to specify it before let's

00:25:08,179 --> 00:25:16,570
start qubit and let's say let's check

00:25:14,120 --> 00:25:16,570
the status

00:25:18,960 --> 00:25:27,620
it's running from 8 seconds ago and I'll

00:25:22,380 --> 00:25:27,620
also start cilium as the cni played

00:25:28,280 --> 00:25:36,720
let's make sure the swing is also

00:25:31,050 --> 00:25:38,070
working it is and on so on the lower

00:25:36,720 --> 00:25:41,400
side of the screen

00:25:38,070 --> 00:25:44,940
it's the the work it's the dedicated

00:25:41,400 --> 00:25:57,780
work that I'll only start cubelet as

00:25:44,940 --> 00:25:59,970
well and Silvia sorry King now yeah are

00:25:57,780 --> 00:26:08,970
you will not starting good proxy I don't

00:25:59,970 --> 00:26:11,250
need it so I will not use it ok so we

00:26:08,970 --> 00:26:14,850
have a cover dentist plaster up in

00:26:11,250 --> 00:26:18,930
running with ipv6 and so far things have

00:26:14,850 --> 00:26:22,560
been working but now we need cubed anus

00:26:18,930 --> 00:26:24,840
so cubed anus will be the dns for all

00:26:22,560 --> 00:26:27,690
the copper dentists cluster it serves

00:26:24,840 --> 00:26:30,680
all the dinosaurs DNS requests for all

00:26:27,690 --> 00:26:33,990
the pods running in a cluster and its

00:26:30,680 --> 00:26:37,140
deployment coordinated spec file that is

00:26:33,990 --> 00:26:39,870
available in cornetist github and i only

00:26:37,140 --> 00:26:41,940
had to make one one small change in the

00:26:39,870 --> 00:26:44,460
in the deployment file which i had to

00:26:41,940 --> 00:26:47,790
change the probe from fork what a

00:26:44,460 --> 00:26:50,310
instead of a single eye what does what

00:26:47,790 --> 00:26:53,100
does the probe does so the probe was

00:26:50,310 --> 00:26:56,700
checking if kubernetes was running or if

00:26:53,100 --> 00:26:59,340
cubed NS was up and running and since

00:26:56,700 --> 00:27:01,860
the cube master so it was checking for

00:26:59,340 --> 00:27:04,710
this name here kubernetes not default of

00:27:01,860 --> 00:27:07,470
service it wasn't so this name here will

00:27:04,710 --> 00:27:09,870
be the default one for cube master says

00:27:07,470 --> 00:27:12,690
cube master was running with ipv6 and

00:27:09,870 --> 00:27:14,910
what i be before the the probe itself

00:27:12,690 --> 00:27:17,730
thought ok cube dennis is not working

00:27:14,910 --> 00:27:19,530
because i'm making a query for a single

00:27:17,730 --> 00:27:21,990
life for that time and the query camp

00:27:19,530 --> 00:27:24,210
comes up empty so something's going on

00:27:21,990 --> 00:27:27,420
here I had to change it to check for

00:27:24,210 --> 00:27:30,780
water a and the reply was the proper

00:27:27,420 --> 00:27:35,130
ipv6 that was serving the cube API

00:27:30,780 --> 00:27:38,790
Master Q by PI server sorry so I will

00:27:35,130 --> 00:27:46,130
start cubed anus which is in this

00:27:38,790 --> 00:27:46,130
directory hopefully no it's not

00:27:49,330 --> 00:27:55,559
that's here I'll just start it and I

00:27:52,809 --> 00:27:55,559
will explain it

00:28:00,630 --> 00:28:05,490
so this file is available with

00:28:02,880 --> 00:28:07,350
kubernetes github and this was the

00:28:05,490 --> 00:28:14,550
change that I had to my so I changed it

00:28:07,350 --> 00:28:19,920
to help to connect to a local host on

00:28:14,550 --> 00:28:25,920
this port and also the to request for

00:28:19,920 --> 00:28:29,130
twelve eggs of stimuli I'll also deploy

00:28:25,920 --> 00:28:32,820
the application itself and ingress I

00:28:29,130 --> 00:28:35,970
will explain which one what they might

00:28:32,820 --> 00:28:38,040
under pressure I they just need a little

00:28:35,970 --> 00:28:41,130
bit type to set it to but up in the

00:28:38,040 --> 00:28:43,740
cluster so keep their nests will Brad I

00:28:41,130 --> 00:28:46,590
don't know which worker maybe work your

00:28:43,740 --> 00:28:50,280
two or a worker node I don't want to

00:28:46,590 --> 00:28:52,980
care about it and the next step will be

00:28:50,280 --> 00:28:55,680
words ingress so ingress allows you to

00:28:52,980 --> 00:28:58,170
expose your services your pods that are

00:28:55,680 --> 00:29:00,420
running outside inside the cluster to

00:28:58,170 --> 00:29:03,540
expose them to outside to the outside

00:29:00,420 --> 00:29:05,100
world so what's the point of having a

00:29:03,540 --> 00:29:07,800
cluster that is not connected to

00:29:05,100 --> 00:29:10,710
internet so ingress allows you to have

00:29:07,800 --> 00:29:14,520
those that kind of exposure to the

00:29:10,710 --> 00:29:17,820
outside world so it's also a kubernetes

00:29:14,520 --> 00:29:21,180
spec file and I didn't need to change

00:29:17,820 --> 00:29:26,340
anything on the under available specs

00:29:21,180 --> 00:29:27,840
files on github as well so and the nginx

00:29:26,340 --> 00:29:30,990
controller ingress controller will be

00:29:27,840 --> 00:29:33,320
run on worker two or word one I don't

00:29:30,990 --> 00:29:35,700
care

00:29:33,320 --> 00:29:38,400
so this will be by the

00:29:35,700 --> 00:29:42,570
my demo data I've already deployed it

00:29:38,400 --> 00:29:44,250
and I'll have a I'll be the user on the

00:29:42,570 --> 00:29:48,210
left side of the screen that will

00:29:44,250 --> 00:29:50,730
connect to the nginx controller and how

00:29:48,210 --> 00:29:53,130
do I found out which address it will be

00:29:50,730 --> 00:29:59,850
available so I'll type

00:29:53,130 --> 00:30:02,040
Kolkata get ingress and I can see that

00:29:59,850 --> 00:30:05,750
this was a file that I've deployed

00:30:02,040 --> 00:30:07,970
previously it will serve for the host

00:30:05,750 --> 00:30:12,420
food bar.com

00:30:07,970 --> 00:30:14,480
it's available on these two addresses

00:30:12,420 --> 00:30:17,179
here so f DS

00:30:14,480 --> 00:30:23,000
zero comma column B and F T zero zero

00:30:17,179 --> 00:30:25,820
column column C so in theory if I go to

00:30:23,000 --> 00:30:29,770
these links to this address here

00:30:25,820 --> 00:30:33,309
I should see something in my browser and

00:30:29,770 --> 00:30:33,309
let's find out

00:30:42,150 --> 00:30:47,880
it's little big but

00:30:45,600 --> 00:30:52,470
okay I just type one of the addresses

00:30:47,880 --> 00:30:53,010
and I see a 502 bad gateway this is good

00:30:52,470 --> 00:30:55,170
news

00:30:53,010 --> 00:30:58,230
so so far we are able to connect where

00:30:55,170 --> 00:31:01,860
gene X which is running on one of the

00:30:58,230 --> 00:31:04,490
nodes and I'm able to ping this side so

00:31:01,860 --> 00:31:08,580
why am I getting immediate ways so I

00:31:04,490 --> 00:31:14,160
have this host here so it will be food

00:31:08,580 --> 00:31:17,910
bar.com if I change the host header to

00:31:14,160 --> 00:31:25,500
fubar comm I should be able to little

00:31:17,910 --> 00:31:27,960
bit but I'll change it on my side so

00:31:25,500 --> 00:31:29,730
this is a chrome plugin that allows you

00:31:27,960 --> 00:31:32,490
to modify the host header of your

00:31:29,730 --> 00:31:36,690
request and that's why I'm changing the

00:31:32,490 --> 00:31:39,330
host header to food bar calm let's see

00:31:36,690 --> 00:31:43,110
if there's an accept button here or not

00:31:39,330 --> 00:31:46,680
okay there's not and if i refresh the

00:31:43,110 --> 00:31:48,210
page I should be able to see my service

00:31:46,680 --> 00:31:52,140
that is running inside the cluster

00:31:48,210 --> 00:31:53,160
inside my cluster via nginx so what is

00:31:52,140 --> 00:31:57,390
happening here

00:31:53,160 --> 00:32:00,210
so far I'm hitting an Janette's that is

00:31:57,390 --> 00:32:04,980
running on one of the one of the workers

00:32:00,210 --> 00:32:07,170
and the request goes to guestbook so

00:32:04,980 --> 00:32:13,460
nginx knows there is a service called

00:32:07,170 --> 00:32:13,460
guestbook and that service the guestbook

00:32:15,500 --> 00:32:23,340
service is this one here it will have

00:32:21,090 --> 00:32:25,320
this cluster IP this IPO was

00:32:23,340 --> 00:32:33,510
automatically assigned by the controller

00:32:25,320 --> 00:32:36,300
manager and it also knows there are some

00:32:33,510 --> 00:32:40,260
endpoint serving this service here the

00:32:36,300 --> 00:32:42,840
guestbook which will be this address

00:32:40,260 --> 00:32:45,480
here so this address is actually a

00:32:42,840 --> 00:32:48,150
container pod running inside my cluster

00:32:45,480 --> 00:32:50,820
and the translation between cluster IP

00:32:48,150 --> 00:32:54,060
and the port itself will be made by

00:32:50,820 --> 00:32:56,160
psyllium so the request goes from nginx

00:32:54,060 --> 00:32:59,070
if he tries to connect the guestbook

00:32:56,160 --> 00:33:01,620
service ceiling we'll know

00:32:59,070 --> 00:33:05,010
proper address to redirect the traffic

00:33:01,620 --> 00:33:07,740
and it will go to strew psyllium and it

00:33:05,010 --> 00:33:10,260
will go directly to the pot itself so

00:33:07,740 --> 00:33:12,720
guestbook later tries to contact qns

00:33:10,260 --> 00:33:15,780
because it needs to know the address of

00:33:12,720 --> 00:33:19,830
the Redis master it will receive the

00:33:15,780 --> 00:33:23,880
reply and then guestbook will write the

00:33:19,830 --> 00:33:27,510
name in the Redis master and Ready's

00:33:23,880 --> 00:33:30,590
life will be will be duplicating the

00:33:27,510 --> 00:33:33,480
entries on reddy's master on its own and

00:33:30,590 --> 00:33:35,610
read guestbook will read the data from

00:33:33,480 --> 00:33:38,160
Redis life so the data will be written

00:33:35,610 --> 00:33:40,710
on Redis master Redis light will

00:33:38,160 --> 00:33:45,020
replicate all the data and guestbook

00:33:40,710 --> 00:33:45,020
will read all the data from Redis life

00:33:46,250 --> 00:33:53,520
so if I type here John for example and I

00:33:50,490 --> 00:33:57,780
click Submit so the data was written on

00:33:53,520 --> 00:34:02,490
the Redis master on this side if I type

00:33:57,780 --> 00:34:05,220
refresh this happened really quick sorry

00:34:02,490 --> 00:34:07,500
but what happened was guest poop was

00:34:05,220 --> 00:34:09,450
craving ready slide for the data and it

00:34:07,500 --> 00:34:16,190
was the same data that I've written on

00:34:09,450 --> 00:34:19,169
ready semester so final thoughts on this

00:34:16,190 --> 00:34:22,260
kubernetes has lots of silly options I

00:34:19,169 --> 00:34:24,210
know that but you should at least read

00:34:22,260 --> 00:34:27,320
all of them to have some knowledge in

00:34:24,210 --> 00:34:31,260
kubernetes and to try things out and

00:34:27,320 --> 00:34:33,630
ipv6 is coming so you will start having

00:34:31,260 --> 00:34:35,580
users running ipv6 and you should be

00:34:33,630 --> 00:34:37,410
aware of that you should try this at

00:34:35,580 --> 00:34:41,310
least this tutorial will try it at your

00:34:37,410 --> 00:34:44,580
home to have some ipv6 knowledge where

00:34:41,310 --> 00:34:46,770
for example I have no developers that

00:34:44,580 --> 00:34:49,560
don't even know how to type an ipv6

00:34:46,770 --> 00:34:52,290
address with the part itself like in the

00:34:49,560 --> 00:34:55,410
browser so this this concepts of

00:34:52,290 --> 00:34:58,380
brackets with the port if I want to type

00:34:55,410 --> 00:35:02,600
this it's something a couple of

00:34:58,380 --> 00:35:02,600
developers don't are not aware of and

00:35:02,810 --> 00:35:08,210
kubernetes is getting ready so there are

00:35:06,000 --> 00:35:11,040
a couple of tools

00:35:08,210 --> 00:35:14,490
dual stack will be nice to have to have

00:35:11,040 --> 00:35:18,510
ipv4 and ipv6 on the same part right now

00:35:14,490 --> 00:35:22,470
I only have ipv6 running and these two

00:35:18,510 --> 00:35:25,050
pull requests were integrated in my demo

00:35:22,470 --> 00:35:27,660
so I have compiled other components with

00:35:25,050 --> 00:35:30,150
this pull request which is the couplets

00:35:27,660 --> 00:35:33,990
node IP option two I have ipv6 and wife

00:35:30,150 --> 00:35:37,290
ipv6 perfect size limit for cluster side

00:35:33,990 --> 00:35:39,090
and also cube admin so for the ones that

00:35:37,290 --> 00:35:41,520
don't know what cube adrene is it's a

00:35:39,090 --> 00:35:43,710
real really good tool that allows you to

00:35:41,520 --> 00:35:46,710
set up a plasterer with single line

00:35:43,710 --> 00:35:48,990
basically and I've set up all of these

00:35:46,710 --> 00:35:52,620
services and Q by PI server cube

00:35:48,990 --> 00:35:55,020
scheduler etc etc this was only for

00:35:52,620 --> 00:35:56,880
educational purposes if you want to try

00:35:55,020 --> 00:35:59,070
it out you can also try it out but if

00:35:56,880 --> 00:36:00,750
you want to try cube admin you should

00:35:59,070 --> 00:36:03,900
try it because if you basically just

00:36:00,750 --> 00:36:06,330
type to admin in it and you start a

00:36:03,900 --> 00:36:09,780
master and cube admin join and you start

00:36:06,330 --> 00:36:11,970
a worker that's the tip and unless you

00:36:09,780 --> 00:36:14,910
try to never find out if you are ready

00:36:11,970 --> 00:36:21,210
for ipv6 or if your infrastructure in

00:36:14,910 --> 00:36:24,300
red is ready for ipv6 and we have we had

00:36:21,210 --> 00:36:26,880
a vote on type of one I think the the

00:36:24,300 --> 00:36:29,400
Blue Fairy is already closed and if you

00:36:26,880 --> 00:36:33,180
want to ask me some questions on Twitter

00:36:29,400 --> 00:36:36,120
there's my handle and coming next after

00:36:33,180 --> 00:36:38,850
this talk in 2:50 p.m. there will be a

00:36:36,120 --> 00:36:41,760
talk about shielding itself if you want

00:36:38,850 --> 00:36:45,270
to know more if you have some questions

00:36:41,760 --> 00:36:47,720
I think I have three minutes left to

00:36:45,270 --> 00:36:47,720
answer you

00:36:51,250 --> 00:37:22,549
yeah so Sigma has an option so the

00:37:19,640 --> 00:37:25,880
question was how psyllium knows how to

00:37:22,549 --> 00:37:29,000
route the traffic to each node correct

00:37:25,880 --> 00:37:32,630
from one node to each node so soon as an

00:37:29,000 --> 00:37:34,759
option called Auto v6 route I don't

00:37:32,630 --> 00:37:38,710
remember the exact name of the option

00:37:34,759 --> 00:37:45,039
but it inserts the not this one

00:37:38,710 --> 00:37:45,039
it inserts that the the route

00:37:47,079 --> 00:37:53,690
yes fix is the first so it is searched

00:37:51,440 --> 00:37:56,299
the routes for each node silly knows

00:37:53,690 --> 00:37:59,450
which pot cider belongs to which node

00:37:56,299 --> 00:38:02,440
and it also knows which not IP belongs

00:37:59,450 --> 00:38:06,950
to which node serum automatically adds

00:38:02,440 --> 00:38:10,039
that which subnet should be go to which

00:38:06,950 --> 00:38:12,259
network or to each IP so for example the

00:38:10,039 --> 00:38:16,009
other node I believe it's it's this one

00:38:12,259 --> 00:38:19,430
and it should go via via FD is 0 0 in

00:38:16,009 --> 00:38:22,339
column C and F t0 compound seats on

00:38:19,430 --> 00:38:29,410
device this name here so that's that's

00:38:22,339 --> 00:38:29,410
wait it does it sorry yeah yeah

00:38:42,480 --> 00:38:48,700
so yeah Suleiman connects to the API

00:38:45,579 --> 00:38:50,770
server and kubernetes sense it'll look

00:38:48,700 --> 00:38:54,220
the cube controller also locates the

00:38:50,770 --> 00:38:57,940
ciders for each node that information is

00:38:54,220 --> 00:38:59,920
available and you can see four odds for

00:38:57,940 --> 00:39:02,410
the node one you'll have the port side

00:38:59,920 --> 00:39:04,839
of that one and cilium also is aware of

00:39:02,410 --> 00:39:07,839
that of that cider so that's the way

00:39:04,839 --> 00:39:10,839
that you can check it it check it if I

00:39:07,839 --> 00:39:27,700
do probably to be a little bigger for

00:39:10,839 --> 00:39:29,799
the screen but exactly correct yeah all

00:39:27,700 --> 00:39:32,130
right thank you so much oh yeah from one

00:39:29,799 --> 00:39:32,130
question

00:39:37,030 --> 00:39:46,160
yeah dude no no no

00:39:42,640 --> 00:39:49,299
so because nginx only is aware of them

00:39:46,160 --> 00:39:53,329
of the service of the coop cattle gets

00:39:49,299 --> 00:39:55,940
it's only aware of this IP here and this

00:39:53,329 --> 00:39:58,579
IP is not it should not be on the wire

00:39:55,940 --> 00:40:18,650
so sealing does this translation to the

00:39:58,579 --> 00:40:22,099
to the IP to the right IP itself correct

00:40:18,650 --> 00:40:24,230
exactly that all right I need to stop

00:40:22,099 --> 00:40:26,050
like the lady told me to stop so thank

00:40:24,230 --> 00:40:29,979
you very much

00:40:26,050 --> 00:40:29,979

YouTube URL: https://www.youtube.com/watch?v=hv_QKzlUm2A


