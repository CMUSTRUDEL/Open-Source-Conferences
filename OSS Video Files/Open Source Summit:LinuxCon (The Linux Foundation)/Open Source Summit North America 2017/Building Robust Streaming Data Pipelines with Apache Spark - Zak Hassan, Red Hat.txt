Title: Building Robust Streaming Data Pipelines with Apache Spark - Zak Hassan, Red Hat
Publication date: 2017-09-12
Playlist: Open Source Summit North America 2017
Description: 
	Building Robust Streaming Data Pipelines with Apache Spark - Zak Hassan, Red Hat

There are challenges to architecting a solution that will allow for developers to stream data into Kafka and be able to manage dirty data which is always an issue in ETL pipelines. I'd like to share lessons learned and demonstrate how we can put Apache Kafka, Apache Spark and Apache Camel together to provide developers with a continuous data pipeline for the Spark applications. Without data it is very difficult to take advantage of its full capabilities of Spark. Companies sometimes have their data stored in many different systems and Apache Camel allows developers to Extract, Transform and Load their data to many systems Apache Kafka is one example. Apache Kafka is great for aggregating data in a centralized location and Apache Spark already comes with a built in connector to connect to Kafka. I'll also be explaining lessons learned from running these technologies inside docker.

About Zak Hassan
Zak is a Software Engineer on the Data Analytics Platform Team working on Data Science and Machine Learning on OpenShift. Zak Previously worked as a Software Consultant in financial services and insurance industry building end to end software solutions for enterprise customer. Zak spends his spare time working on OpenSource software and trying to innovate . Zak contributes to open source projects and also posts personal projects on https://github.com/zmhassan .
Captions: 
	00:00:00,030 --> 00:00:04,529
so my name is Zack Hassan I work at Red

00:00:01,979 --> 00:00:10,800
Hat I'm a data I'm an engineer on the

00:00:04,529 --> 00:00:19,439
data analytics platform so a little bit

00:00:10,800 --> 00:00:23,609
about me so the project that I work on

00:00:19,439 --> 00:00:26,099
is called rad analytics IO we we use

00:00:23,609 --> 00:00:29,910
apache spark to power our analytics

00:00:26,099 --> 00:00:32,790
engine and today i'm gonna be talking

00:00:29,910 --> 00:00:43,500
about how you can use apache spark to

00:00:32,790 --> 00:00:45,480
build data pipelines all right so the

00:00:43,500 --> 00:00:47,730
agenda for today is i'm going to talk a

00:00:45,480 --> 00:00:49,860
little bit about data preparation which

00:00:47,730 --> 00:00:51,300
is pretty important before you you build

00:00:49,860 --> 00:00:54,989
a data pipeline you need to make sure

00:00:51,300 --> 00:00:58,890
the data is in a format that structured

00:00:54,989 --> 00:01:01,680
a format that is usable queryable and

00:00:58,890 --> 00:01:04,979
we'll talk more about the criteria what

00:01:01,680 --> 00:01:07,080
a good data format is we'll talk about

00:01:04,979 --> 00:01:09,060
different form different technology that

00:01:07,080 --> 00:01:12,180
you can use to extract data from

00:01:09,060 --> 00:01:16,170
different systems and then we'll talk a

00:01:12,180 --> 00:01:19,549
little bit about stream processing and

00:01:16,170 --> 00:01:22,200
how you can use apache spark running in

00:01:19,549 --> 00:01:24,619
containers on openshift

00:01:22,200 --> 00:01:25,799
so if you don't know about OpenShift

00:01:24,619 --> 00:01:28,829
openshift

00:01:25,799 --> 00:01:31,439
has kubernetes and it's our platform as

00:01:28,829 --> 00:01:33,630
a service and I'll be everything I'm

00:01:31,439 --> 00:01:37,350
using here is all open source things you

00:01:33,630 --> 00:01:39,119
can download and use so let's get

00:01:37,350 --> 00:01:43,110
started by talking about data

00:01:39,119 --> 00:01:46,470
preparation so when you're preparing

00:01:43,110 --> 00:01:50,399
your data you're getting data from so

00:01:46,470 --> 00:01:53,729
many different systems so for example

00:01:50,399 --> 00:01:55,950
one big problem is since you're dealing

00:01:53,729 --> 00:01:57,930
with so many different data types

00:01:55,950 --> 00:02:00,719
different systems that you have to

00:01:57,930 --> 00:02:03,390
integrate with you know and then you

00:02:00,719 --> 00:02:05,820
have sensor devices and then you have

00:02:03,390 --> 00:02:09,929
different protocols for example there's

00:02:05,820 --> 00:02:12,270
the MQTT protocol there's a mqp there's

00:02:09,929 --> 00:02:13,590
so many different protocols that these

00:02:12,270 --> 00:02:15,510
different devices my

00:02:13,590 --> 00:02:17,370
use that you want to consume from and

00:02:15,510 --> 00:02:19,440
then do analytics and two other

00:02:17,370 --> 00:02:23,400
different things around that then you

00:02:19,440 --> 00:02:25,349
might want to have your data you know

00:02:23,400 --> 00:02:27,180
you might get a lot of social media data

00:02:25,349 --> 00:02:32,730
and you want to do sentiment analysis on

00:02:27,180 --> 00:02:38,580
that which is something that people use

00:02:32,730 --> 00:02:41,310
spark for and you know anything from

00:02:38,580 --> 00:02:46,019
from you know tracking inventory to you

00:02:41,310 --> 00:02:48,989
know just basically you know trying to

00:02:46,019 --> 00:02:52,500
give even more value with the data and

00:02:48,989 --> 00:02:54,299
get analytics so let's talk about one

00:02:52,500 --> 00:02:57,630
technology in particular that that I

00:02:54,299 --> 00:02:59,940
find very useful for data preparation so

00:02:57,630 --> 00:03:03,360
when you're getting data from different

00:02:59,940 --> 00:03:06,060
systems right often what happens is you

00:03:03,360 --> 00:03:09,569
write data to you right you know

00:03:06,060 --> 00:03:12,030
integration code to connect to MongoDB

00:03:09,569 --> 00:03:14,190
or you write code to connect to some

00:03:12,030 --> 00:03:19,200
JDBC or you might write some code to

00:03:14,190 --> 00:03:21,120
connect to MQTT protocol or AMQP and the

00:03:19,200 --> 00:03:23,730
nice thing about camel is it has

00:03:21,120 --> 00:03:25,560
components that you can utilize to

00:03:23,730 --> 00:03:27,959
connect to those components you can

00:03:25,560 --> 00:03:29,519
connect to those systems by using these

00:03:27,959 --> 00:03:31,880
components and these are pre-built and

00:03:29,519 --> 00:03:35,040
the interesting thing about camel is

00:03:31,880 --> 00:03:38,010
it's open source it has over 200

00:03:35,040 --> 00:03:39,150
component or connectors that you can use

00:03:38,010 --> 00:03:43,889
to connect to all these different

00:03:39,150 --> 00:03:45,810
systems so the criteria is you're gonna

00:03:43,889 --> 00:03:50,370
try and connect you're gonna try and get

00:03:45,810 --> 00:03:53,489
data from a source to a sync so what is

00:03:50,370 --> 00:03:56,069
code like that look like so camel you

00:03:53,489 --> 00:03:59,370
can write code in Java you can get data

00:03:56,069 --> 00:04:01,349
from some location so here we're using a

00:03:59,370 --> 00:04:04,799
file component but that can be easily

00:04:01,349 --> 00:04:08,519
replaced with for example the Kafka

00:04:04,799 --> 00:04:11,970
component or you know MongoDB you just

00:04:08,519 --> 00:04:14,400
replace file with that component and as

00:04:11,970 --> 00:04:17,789
you can see here you can see I'm

00:04:14,400 --> 00:04:20,220
pointing to the word extract the from

00:04:17,789 --> 00:04:21,690
statement is it's very declarative you

00:04:20,220 --> 00:04:24,090
know you're you're writing this code you

00:04:21,690 --> 00:04:27,190
don't have to worry about making sure

00:04:24,090 --> 00:04:28,960
that this this code connects to

00:04:27,190 --> 00:04:31,680
MongoDB or connects to all these other

00:04:28,960 --> 00:04:35,920
differences this is already done for you

00:04:31,680 --> 00:04:38,920
and then when you get your data it goes

00:04:35,920 --> 00:04:42,340
through the next space which is you

00:04:38,920 --> 00:04:43,750
chaining the beam so you're you're

00:04:42,340 --> 00:04:46,000
passing your data through a beam and

00:04:43,750 --> 00:04:48,820
then in that in that beam you can write

00:04:46,000 --> 00:04:51,250
custom code to do some custom validation

00:04:48,820 --> 00:04:53,530
custom things that you want to you know

00:04:51,250 --> 00:04:55,960
double-check on and then you want to

00:04:53,530 --> 00:04:56,830
send it to a sync which is that to

00:04:55,960 --> 00:04:59,230
statement over there

00:04:56,830 --> 00:05:01,780
so you're loading that data somewhere so

00:04:59,230 --> 00:05:04,870
what you could do is actually send that

00:05:01,780 --> 00:05:08,410
data to Kafka and then once the data is

00:05:04,870 --> 00:05:10,660
in Kafka spark structure streaming can

00:05:08,410 --> 00:05:12,940
pick up that data and do some more stuff

00:05:10,660 --> 00:05:16,860
and we'll talk about this and we'll have

00:05:12,940 --> 00:05:16,860
a demo at the end of this talk

00:05:17,610 --> 00:05:24,280
so let's first try and understand just a

00:05:21,610 --> 00:05:27,280
very basic concept I'm not gonna go to

00:05:24,280 --> 00:05:29,710
too long with this but basically you

00:05:27,280 --> 00:05:34,180
have a sender and you have a receiver

00:05:29,710 --> 00:05:36,880
and you have a message so when you send

00:05:34,180 --> 00:05:38,140
a message right you're sending it

00:05:36,880 --> 00:05:41,710
somewhere and then somebody else is

00:05:38,140 --> 00:05:47,020
receiving it we whip in messaging world

00:05:41,710 --> 00:05:49,270
we call this producer and consumer so

00:05:47,020 --> 00:05:51,820
things can get a little bit more complex

00:05:49,270 --> 00:05:54,040
the more you know you add more systems

00:05:51,820 --> 00:05:58,570
to this the more the rules change

00:05:54,040 --> 00:06:00,220
different different different criteria

00:05:58,570 --> 00:06:01,450
that you have the different business

00:06:00,220 --> 00:06:03,669
requirements so you're gonna have to do

00:06:01,450 --> 00:06:05,440
different things so in this diagram here

00:06:03,669 --> 00:06:07,870
I'm showing that there's a new order

00:06:05,440 --> 00:06:09,460
coming in and then I'm using a wiretap

00:06:07,870 --> 00:06:11,500
so as soon as a message comes in I'm

00:06:09,460 --> 00:06:13,390
making a copy and I'm sending it

00:06:11,500 --> 00:06:15,700
somewhere else and then the message goes

00:06:13,390 --> 00:06:17,620
through then I split and then I send it

00:06:15,700 --> 00:06:20,380
through a recipient list and I also

00:06:17,620 --> 00:06:24,250
filter here so it's not very visible

00:06:20,380 --> 00:06:30,220
here on this screen let me see if I can

00:06:24,250 --> 00:06:32,500
make this a little bit bigger it's not

00:06:30,220 --> 00:06:35,320
very visible but basically this is XML

00:06:32,500 --> 00:06:38,080
there will pretty much have a filter

00:06:35,320 --> 00:06:40,860
rule if this criteria is correct it'll

00:06:38,080 --> 00:06:40,860
log that message

00:06:41,710 --> 00:06:47,930
so let's talk a little bit about Kafka

00:06:44,650 --> 00:06:50,780
so now that we understand the data

00:06:47,930 --> 00:06:53,150
preparation portion right and now that

00:06:50,780 --> 00:06:54,920
we you know we validated we cleaned our

00:06:53,150 --> 00:06:57,920
data we did everything that we needed to

00:06:54,920 --> 00:07:01,310
do to prepare that data now we send it

00:06:57,920 --> 00:07:04,520
to a messaging queue or a messaging

00:07:01,310 --> 00:07:06,890
topic pretty in particular so Apache

00:07:04,520 --> 00:07:10,640
Kafka is an open source project it's a

00:07:06,890 --> 00:07:15,170
distributed fault tolerant replicated

00:07:10,640 --> 00:07:19,040
commit log it uses zookeeper for high

00:07:15,170 --> 00:07:24,770
availability and messages arrive in

00:07:19,040 --> 00:07:27,710
what's called a topic and once the

00:07:24,770 --> 00:07:31,400
message arrives at a topic so as you can

00:07:27,710 --> 00:07:34,010
see over here the mess the producer

00:07:31,400 --> 00:07:36,530
sends the message it arrives in Kafka

00:07:34,010 --> 00:07:38,180
when the message gets received the

00:07:36,530 --> 00:07:41,840
consumer picks up that message and does

00:07:38,180 --> 00:07:44,330
other processing so you know

00:07:41,840 --> 00:07:46,910
traditionally people use HTTP you know

00:07:44,330 --> 00:07:48,140
they write web services they send a you

00:07:46,910 --> 00:07:51,830
know our call to another web service

00:07:48,140 --> 00:07:53,780
this is more of a fire-and-forget so you

00:07:51,830 --> 00:07:55,430
fire that message and you let the

00:07:53,780 --> 00:07:57,170
consumer pick it up and you let the

00:07:55,430 --> 00:08:01,430
consumer do what it needs to do to

00:07:57,170 --> 00:08:04,400
complete that task so that's what we use

00:08:01,430 --> 00:08:06,440
Kafka for the nice thing about Kafka is

00:08:04,400 --> 00:08:09,140
there's so many different you know a

00:08:06,440 --> 00:08:11,810
really rich ecosystem where they have

00:08:09,140 --> 00:08:14,210
connectors to Apache spark you got

00:08:11,810 --> 00:08:18,320
connectors to you know there's there's

00:08:14,210 --> 00:08:22,310
also spark connectors for the apache

00:08:18,320 --> 00:08:26,120
camel comes with to send messages to

00:08:22,310 --> 00:08:27,770
Kafka as well and then there's also CAF

00:08:26,120 --> 00:08:32,840
Connect which is another thing as well

00:08:27,770 --> 00:08:37,820
that you can look into as well so let's

00:08:32,840 --> 00:08:41,660
talk a little bit about data formats so

00:08:37,820 --> 00:08:42,440
you have schemas and everything has a

00:08:41,660 --> 00:08:45,580
schema either

00:08:42,440 --> 00:08:48,260
it's schema on right or schema on read

00:08:45,580 --> 00:08:49,880
the important thing here is when you

00:08:48,260 --> 00:08:51,920
have schemas you want to have it

00:08:49,880 --> 00:08:52,650
versioned you want to have something

00:08:51,920 --> 00:08:55,530
where

00:08:52,650 --> 00:08:57,510
it's documented because what often

00:08:55,530 --> 00:08:59,880
happens sometimes is some companies is

00:08:57,510 --> 00:09:02,670
you know some engineer is hired to do a

00:08:59,880 --> 00:09:06,300
particular job they you know create the

00:09:02,670 --> 00:09:08,370
schema and then they're gone and there's

00:09:06,300 --> 00:09:11,100
no documentation for what what what does

00:09:08,370 --> 00:09:13,440
this field mean and then it's it takes a

00:09:11,100 --> 00:09:15,990
long time to track down what these

00:09:13,440 --> 00:09:19,140
fields actually mean so it's very very

00:09:15,990 --> 00:09:22,980
important to have schemas and have it

00:09:19,140 --> 00:09:25,080
version and have it documented as well

00:09:22,980 --> 00:09:30,780
and we'll talk about different types of

00:09:25,080 --> 00:09:33,000
schemas so let's list let's just take a

00:09:30,780 --> 00:09:35,730
look and see what type of different data

00:09:33,000 --> 00:09:39,870
formats there are out there I'll just

00:09:35,730 --> 00:09:42,810
mention about seven or about six of them

00:09:39,870 --> 00:09:45,450
that are there there are that are pretty

00:09:42,810 --> 00:09:47,280
common out there so there's text you can

00:09:45,450 --> 00:09:49,200
put all your data in text you can choose

00:09:47,280 --> 00:09:52,020
to do it in JSON which is pretty popular

00:09:49,200 --> 00:09:55,950
then there's Avro Park a thrift and

00:09:52,020 --> 00:09:57,870
protobuf all these ones have their own

00:09:55,950 --> 00:10:00,240
different advantages and disadvantages

00:09:57,870 --> 00:10:03,840
and I've highlighted with a check mark

00:10:00,240 --> 00:10:09,450
and an x mark which ones have these

00:10:03,840 --> 00:10:11,340
features so you can write you can store

00:10:09,450 --> 00:10:14,970
all your data in text and you can store

00:10:11,340 --> 00:10:16,950
it in Hadoop and you're okay but you

00:10:14,970 --> 00:10:18,450
you're missing the part of compression

00:10:16,950 --> 00:10:20,820
because compression you want to improve

00:10:18,450 --> 00:10:24,090
performance as well and you want to have

00:10:20,820 --> 00:10:25,920
a schema where you know which what data

00:10:24,090 --> 00:10:31,650
is associated with to which particular

00:10:25,920 --> 00:10:34,530
field JSON you have a JSON you have an

00:10:31,650 --> 00:10:37,740
array of objects within that JSON right

00:10:34,530 --> 00:10:39,480
it's not splittable you can't really do

00:10:37,740 --> 00:10:41,520
compression on it it doesn't have like a

00:10:39,480 --> 00:10:44,720
schema it's semi structures so you can

00:10:41,520 --> 00:10:44,720
add more fields to it if you want

00:10:44,810 --> 00:10:52,260
Avro is particularly interesting data

00:10:49,830 --> 00:10:54,600
format and it's it's a good data format

00:10:52,260 --> 00:10:57,840
to use and I'll and I'll show a demo of

00:10:54,600 --> 00:10:59,850
working with Avro and also I'll talk

00:10:57,840 --> 00:11:02,550
about working with park' those are the

00:10:59,850 --> 00:11:04,290
two top ones they check off all the the

00:11:02,550 --> 00:11:06,570
checkmarks in the list so the

00:11:04,290 --> 00:11:10,800
compression is good schema

00:11:06,570 --> 00:11:14,910
it has one and it's splittable and then

00:11:10,800 --> 00:11:17,850
we have thrift and protobuf which they

00:11:14,910 --> 00:11:19,500
have schemas but they're not so great

00:11:17,850 --> 00:11:21,930
with with the other categories as well

00:11:19,500 --> 00:11:25,110
so the two that check off all our list

00:11:21,930 --> 00:11:29,880
is outro and park' and I'm going to talk

00:11:25,110 --> 00:11:34,790
a little bit more about Avro park' so a

00:11:29,880 --> 00:11:37,470
bro is a data serialization format so

00:11:34,790 --> 00:11:42,330
you're storing your data in a particular

00:11:37,470 --> 00:11:45,330
format and one of the particular things

00:11:42,330 --> 00:11:47,880
with Avro is it has an external schema

00:11:45,330 --> 00:11:51,720
file and I'll show you how that looks

00:11:47,880 --> 00:11:54,360
like and that file is like a JSON format

00:11:51,720 --> 00:11:57,000
and you store and you can version that

00:11:54,360 --> 00:12:00,810
that file and that is your schema and

00:11:57,000 --> 00:12:04,050
then use that schema to read the you

00:12:00,810 --> 00:12:05,790
know and you know generate more you just

00:12:04,050 --> 00:12:08,390
just to work with the file basically and

00:12:05,790 --> 00:12:11,610
I'll show you all that stuff as well and

00:12:08,390 --> 00:12:14,010
then there's one particular Java library

00:12:11,610 --> 00:12:16,500
that I happen to really like which is

00:12:14,010 --> 00:12:19,400
the Jackson data format bionaire if you

00:12:16,500 --> 00:12:23,010
use Java that's a good library for you

00:12:19,400 --> 00:12:26,310
and then there is a a bro tooling which

00:12:23,010 --> 00:12:27,990
is really cool because if you I'll show

00:12:26,310 --> 00:12:31,920
you what what what it looks like when

00:12:27,990 --> 00:12:33,990
you print or you try to you know output

00:12:31,920 --> 00:12:36,480
all the data into the terminal you want

00:12:33,990 --> 00:12:39,240
to see what's in this Avro file so

00:12:36,480 --> 00:12:40,710
Averell tools gives you a way to see

00:12:39,240 --> 00:12:45,270
what's inside that those files and

00:12:40,710 --> 00:12:48,810
deserialize are on the terminal so this

00:12:45,270 --> 00:12:52,610
is what a schema looks like in Avro so

00:12:48,810 --> 00:12:55,830
as you can see over here you have the

00:12:52,610 --> 00:12:57,540
type which is record you give it a name

00:12:55,830 --> 00:13:00,210
you give it a name space and you can

00:12:57,540 --> 00:13:02,460
write some documentation in there which

00:13:00,210 --> 00:13:03,840
is really helpful so anybody who wants

00:13:02,460 --> 00:13:08,540
to look at this schema they'll know that

00:13:03,840 --> 00:13:10,680
this schema has this particular

00:13:08,540 --> 00:13:14,840
documentation associated with it there's

00:13:10,680 --> 00:13:18,060
fields and over here under name name is

00:13:14,840 --> 00:13:19,920
optional where I put whenever you put

00:13:18,060 --> 00:13:22,470
null string or

00:13:19,920 --> 00:13:25,110
report null int or you put know whatever

00:13:22,470 --> 00:13:28,700
it is that means that this field is

00:13:25,110 --> 00:13:33,450
optional so you can choose to you know

00:13:28,700 --> 00:13:35,459
provide that field or not and I'll show

00:13:33,450 --> 00:13:42,089
you the the difference in schemas when

00:13:35,459 --> 00:13:44,490
you look at parquet as well so before we

00:13:42,089 --> 00:13:46,050
go further you know feel free to stop me

00:13:44,490 --> 00:13:49,019
at any time and and you know ask

00:13:46,050 --> 00:13:53,010
questions I will just wanted to let you

00:13:49,019 --> 00:13:56,930
guys know as well so let's look at a

00:13:53,010 --> 00:13:56,930
demo of all this stuff

00:14:15,040 --> 00:14:23,410
okay so we have a file called customer

00:14:18,399 --> 00:14:25,930
Avril okay so customer dot Avro is a

00:14:23,410 --> 00:14:27,910
serialized file i've serialized it and

00:14:25,930 --> 00:14:31,389
I've gone in some data and I've stored

00:14:27,910 --> 00:14:34,779
it in this file so you know first

00:14:31,389 --> 00:14:36,160
reaction for develop would be like okay

00:14:34,779 --> 00:14:38,529
I'm gonna just cut out this file let's

00:14:36,160 --> 00:14:43,750
see let's see the contents of this file

00:14:38,529 --> 00:14:45,639
and when I do that I get junk as you can

00:14:43,750 --> 00:14:49,480
see I have a bunch of junk we get we get

00:14:45,639 --> 00:14:54,130
some data some schema stuff but where's

00:14:49,480 --> 00:14:57,759
the data I want to see the data okay so

00:14:54,130 --> 00:15:01,089
if I decide to instead of doing that if

00:14:57,759 --> 00:15:06,639
I decided to use Avro tooling it's as

00:15:01,089 --> 00:15:10,569
simple as Avro tools to JSON or you can

00:15:06,639 --> 00:15:13,060
put it in in the but to JSON is how you

00:15:10,569 --> 00:15:18,310
can do it as well and then you have the

00:15:13,060 --> 00:15:21,490
file and there you go you got your your

00:15:18,310 --> 00:15:26,550
fields so you have name steve ID is one

00:15:21,490 --> 00:15:33,790
jeff ID is two and justin ID is three

00:15:26,550 --> 00:15:37,089
okay so let's look and and see the park

00:15:33,790 --> 00:15:38,860
a file let's see if we can you know chat

00:15:37,089 --> 00:15:41,110
it out as well in the terminal let's see

00:15:38,860 --> 00:15:45,279
if we're gonna have the same problem so

00:15:41,110 --> 00:15:47,709
when we see the park a file we just cut

00:15:45,279 --> 00:15:49,600
out that file we get a lot of junk as

00:15:47,709 --> 00:15:51,130
you can see we have all this junk here

00:15:49,600 --> 00:15:53,399
but we don't have the data that we're

00:15:51,130 --> 00:15:53,399
looking for

00:15:58,010 --> 00:16:01,010
yeah

00:16:09,570 --> 00:16:17,260
okay so um so in this particular example

00:16:14,650 --> 00:16:20,410
I'm adding like a couple of fields and

00:16:17,260 --> 00:16:22,300
stuff like that but if you're if you're

00:16:20,410 --> 00:16:24,610
choosing between these different formats

00:16:22,300 --> 00:16:26,620
and you're looking for performance then

00:16:24,610 --> 00:16:28,630
if you're looking for role oriented

00:16:26,620 --> 00:16:31,270
you're doing queries with rows

00:16:28,630 --> 00:16:34,240
then I'll go with Avro if you're doing

00:16:31,270 --> 00:16:37,270
column-oriented query then I'll choose

00:16:34,240 --> 00:16:40,990
parquet because parquet uses column our

00:16:37,270 --> 00:16:45,040
technology and columns are stored

00:16:40,990 --> 00:16:47,290
together in HDFS block so I would choose

00:16:45,040 --> 00:16:52,930
parquet if you if you look into create

00:16:47,290 --> 00:16:54,910
columns I hope that answers it so as you

00:16:52,930 --> 00:16:59,590
can see I got a lot of junk when I did

00:16:54,910 --> 00:17:02,530
the cat command and now when let me

00:16:59,590 --> 00:17:05,050
clear out this terminal there's another

00:17:02,530 --> 00:17:07,270
tool that you can download as well where

00:17:05,050 --> 00:17:09,850
you can actually query parquet it's

00:17:07,270 --> 00:17:14,460
called parkade tools similar to the Avro

00:17:09,850 --> 00:17:14,460
tools but it has different commands and

00:17:14,820 --> 00:17:24,130
before before we let's take a look at

00:17:18,220 --> 00:17:28,120
that okay so we basically do cat rock

00:17:24,130 --> 00:17:35,010
and roll dot parquet and as you can see

00:17:28,120 --> 00:17:38,350
we can we got our output you know

00:17:35,010 --> 00:17:42,580
product type Product ID price quantity

00:17:38,350 --> 00:17:45,370
and whatnot so so park is a little

00:17:42,580 --> 00:17:49,000
different because the the schema is

00:17:45,370 --> 00:17:52,420
actually in embedded in the file in the

00:17:49,000 --> 00:17:56,550
footer so if we want to see the schema

00:17:52,420 --> 00:18:01,920
we can just do this Parkay tools and

00:17:56,550 --> 00:18:01,920
then we just do get schema

00:18:14,240 --> 00:18:21,890
schema so as you can see we can see that

00:18:17,809 --> 00:18:28,370
our schema has an ID product category

00:18:21,890 --> 00:18:31,130
name price okay and now let's take a

00:18:28,370 --> 00:18:32,179
look at getting the schema for that Avro

00:18:31,130 --> 00:18:34,040
file that we were looking at the

00:18:32,179 --> 00:18:37,210
customer ever file if we wanted to use

00:18:34,040 --> 00:18:46,250
the terminal tools so if we go Averell

00:18:37,210 --> 00:18:49,010
tools okay get schema and customer there

00:18:46,250 --> 00:18:51,380
we go we have our schema we have this is

00:18:49,010 --> 00:18:53,770
this is the name of the file there's a

00:18:51,380 --> 00:18:58,250
namespace this is the documentation and

00:18:53,770 --> 00:19:02,030
this is the the ID and this is optional

00:18:58,250 --> 00:19:04,309
too as well okay so now that we've

00:19:02,030 --> 00:19:07,730
covered the tooling around Avro and

00:19:04,309 --> 00:19:11,380
parquet we're gonna talk more about a

00:19:07,730 --> 00:19:20,480
little bit more about parquet as well

00:19:11,380 --> 00:19:23,450
one moment so a little bit more about

00:19:20,480 --> 00:19:26,120
party so as I mentioned it is data

00:19:23,450 --> 00:19:28,309
serialization format kind of like a bro

00:19:26,120 --> 00:19:31,790
but more oriented for column our

00:19:28,309 --> 00:19:33,320
technology but I mean I'm sure you heard

00:19:31,790 --> 00:19:36,140
of column our technology before you

00:19:33,320 --> 00:19:40,429
probably heard of orc so you know often

00:19:36,140 --> 00:19:42,950
people ask okay you know if why don't I

00:19:40,429 --> 00:19:45,440
just use orc right you could use orc but

00:19:42,950 --> 00:19:48,860
then again if you want to operate

00:19:45,440 --> 00:19:51,320
outside of hive then it's better to work

00:19:48,860 --> 00:19:52,880
with parquet because you can use it in

00:19:51,320 --> 00:19:55,220
in hive and you can use it outside as

00:19:52,880 --> 00:19:56,840
well and you can query with spark sequel

00:19:55,220 --> 00:20:00,590
as well and we'll we'll look at the demo

00:19:56,840 --> 00:20:05,150
for that and it integrates great with

00:20:00,590 --> 00:20:07,910
spark so you know if we look back at

00:20:05,150 --> 00:20:10,760
what life could be like with hive and

00:20:07,910 --> 00:20:13,940
then we'll look at working with spark

00:20:10,760 --> 00:20:16,970
okay so hive is you know more of sequel

00:20:13,940 --> 00:20:19,640
you're writing sequel to create tables

00:20:16,970 --> 00:20:21,770
and then you you know you you do select

00:20:19,640 --> 00:20:23,600
statements and whatnot so this is what

00:20:21,770 --> 00:20:25,700
hive looks like if you if you were to

00:20:23,600 --> 00:20:27,810
take that party file and then put it in

00:20:25,700 --> 00:20:31,080
a table and then query that table

00:20:27,810 --> 00:20:34,470
that's what it looks like what if you

00:20:31,080 --> 00:20:37,320
wanted to use spark sequel instead this

00:20:34,470 --> 00:20:40,290
is how sparks equal looks so basically

00:20:37,320 --> 00:20:42,450
what sparks equal you go ahead and you

00:20:40,290 --> 00:20:45,650
set up the that this is the HDFS

00:20:42,450 --> 00:20:50,310
location you you want to query use spark

00:20:45,650 --> 00:20:52,980
to create a session and then you do this

00:20:50,310 --> 00:20:55,980
sequel but the interesting thing is you

00:20:52,980 --> 00:20:59,820
have a declarative you know API that you

00:20:55,980 --> 00:21:01,290
can you can use you know and and it's

00:20:59,820 --> 00:21:02,940
nice to have a programming language that

00:21:01,290 --> 00:21:04,800
you can just do group bys and do

00:21:02,940 --> 00:21:06,360
different you know sequel type of

00:21:04,800 --> 00:21:08,460
queries but then you're just using an

00:21:06,360 --> 00:21:16,440
API for it and there's interesting

00:21:08,460 --> 00:21:20,120
optimization as well with with spark so

00:21:16,440 --> 00:21:22,350
let's talk a little bit about spark I

00:21:20,120 --> 00:21:25,290
know I mentioned it a couple of times

00:21:22,350 --> 00:21:28,800
during this talk I want to dive a little

00:21:25,290 --> 00:21:33,060
deeper into spark so spark is a

00:21:28,800 --> 00:21:34,610
distributed you know parallel framework

00:21:33,060 --> 00:21:37,490
where you're doing distributed computing

00:21:34,610 --> 00:21:42,420
and you're distributing across a cluster

00:21:37,490 --> 00:21:48,180
so our team packaged spark in docker

00:21:42,420 --> 00:21:51,560
containers and we run it on you know

00:21:48,180 --> 00:21:56,660
kubernetes ads within OpenShift

00:21:51,560 --> 00:21:56,660
so and you're gonna see that in a demo

00:21:57,440 --> 00:22:04,080
so let's just look at high-level you

00:22:01,350 --> 00:22:06,540
know sparked what's the pieces the the

00:22:04,080 --> 00:22:09,680
building blocks of spark here so we have

00:22:06,540 --> 00:22:13,530
spark sequel where you can do query

00:22:09,680 --> 00:22:17,820
query data whether it's you know data in

00:22:13,530 --> 00:22:20,760
HDFS or data in s3 you have spark ml lib

00:22:17,820 --> 00:22:22,830
where you can do machine learning you

00:22:20,760 --> 00:22:25,470
have spark graph where you can do graph

00:22:22,830 --> 00:22:28,880
processing and then you have spark

00:22:25,470 --> 00:22:31,620
streaming where you can stream data in

00:22:28,880 --> 00:22:33,570
now there's there's obviously there's

00:22:31,620 --> 00:22:35,580
the the old way to do streaming which is

00:22:33,570 --> 00:22:37,410
just spark streaming and then there's a

00:22:35,580 --> 00:22:38,880
new way which is Strutt spark structure

00:22:37,410 --> 00:22:41,379
streaming and that's what i'm going to

00:22:38,880 --> 00:22:44,029
be demoing today

00:22:41,379 --> 00:22:48,080
so the interesting thing is you can run

00:22:44,029 --> 00:22:51,139
spark standalone on yarn or on maces but

00:22:48,080 --> 00:22:55,009
the way we run it within you know Cooper

00:22:51,139 --> 00:22:57,200
knee pods is we have we have it running

00:22:55,009 --> 00:22:59,690
as standalone and you can have more than

00:22:57,200 --> 00:23:03,529
one spark cluster if you want you know

00:22:59,690 --> 00:23:05,359
and you can have a job run and then that

00:23:03,529 --> 00:23:06,799
has his own spark cluster associated

00:23:05,359 --> 00:23:09,109
with it and when the job is finished

00:23:06,799 --> 00:23:11,509
then it tears down your spark cluster

00:23:09,109 --> 00:23:13,820
and then you have another job with a

00:23:11,509 --> 00:23:15,649
separate cluster so you never interrupt

00:23:13,820 --> 00:23:16,779
anybody else while they're doing their

00:23:15,649 --> 00:23:20,209
stuff as well

00:23:16,779 --> 00:23:22,070
you get multi-tenancy by default because

00:23:20,209 --> 00:23:25,459
you're running with an open shift and

00:23:22,070 --> 00:23:27,649
open shift has namespaces so different

00:23:25,459 --> 00:23:30,739
you know you can have different projects

00:23:27,649 --> 00:23:33,200
and then you'll never have to you know

00:23:30,739 --> 00:23:35,119
have conflicts with each other it's not

00:23:33,200 --> 00:23:37,039
one shared cluster that everybody shares

00:23:35,119 --> 00:23:41,809
with each other you can have your own

00:23:37,039 --> 00:23:44,539
independent cluster and then as I

00:23:41,809 --> 00:23:46,999
mentioned before data formats are very

00:23:44,539 --> 00:23:50,239
important the interesting thing is spark

00:23:46,999 --> 00:23:54,639
has API is where you can write to park a

00:23:50,239 --> 00:23:59,389
you can write to Avro or JSON or CSV and

00:23:54,639 --> 00:24:00,950
also a lot of data access as well so

00:23:59,389 --> 00:24:04,070
let's look high level what's the

00:24:00,950 --> 00:24:06,950
architecture look like okay so we have a

00:24:04,070 --> 00:24:10,969
master and then we have one or more

00:24:06,950 --> 00:24:12,799
workers and these workers have a JVM

00:24:10,969 --> 00:24:17,179
process called executors and these

00:24:12,799 --> 00:24:20,690
executors execute the work and what we

00:24:17,179 --> 00:24:23,209
have is a Java or Python or a Scalla or

00:24:20,690 --> 00:24:25,579
an our program that we submit to this

00:24:23,209 --> 00:24:28,129
cluster so when we submit that that

00:24:25,579 --> 00:24:30,070
program to this cluster we call that

00:24:28,129 --> 00:24:32,779
driver okay

00:24:30,070 --> 00:24:35,899
it gets submitted to the cluster and it

00:24:32,779 --> 00:24:39,889
gets scheduled to run on one of these

00:24:35,899 --> 00:24:46,219
workers the executors and then once the

00:24:39,889 --> 00:24:48,829
job is done it's done parallel and for

00:24:46,219 --> 00:24:51,669
right analytics we're running these in

00:24:48,829 --> 00:24:51,669
in separate parts

00:24:51,809 --> 00:24:56,239
so let's talk a little bit about the

00:24:53,729 --> 00:24:59,159
demo that I'm gonna be demoing today

00:24:56,239 --> 00:25:03,139
it's it's a it's an order processing

00:24:59,159 --> 00:25:06,299
application so you have a Hadoop cluster

00:25:03,139 --> 00:25:09,899
you are basically you have a web service

00:25:06,299 --> 00:25:13,289
that accepts orders when you get an

00:25:09,899 --> 00:25:15,889
order coming in we we do two things

00:25:13,289 --> 00:25:19,259
we store it in Cassandra database and

00:25:15,889 --> 00:25:21,659
then we send a message to Kafka so we're

00:25:19,259 --> 00:25:23,580
doing two operations there and then once

00:25:21,659 --> 00:25:25,320
we send a message to Kafka as soon as

00:25:23,580 --> 00:25:29,909
the message arrives like with within

00:25:25,320 --> 00:25:32,009
Kafka then what we do is we spark

00:25:29,909 --> 00:25:34,229
structure streaming picks up that

00:25:32,009 --> 00:25:37,109
message and does some processing and

00:25:34,229 --> 00:25:39,749
then turns it into a data frame and then

00:25:37,109 --> 00:25:42,960
turns it into a park a file and stores

00:25:39,749 --> 00:25:46,710
it in a in a directory and as data is

00:25:42,960 --> 00:25:49,399
streaming in it's continuously updating

00:25:46,710 --> 00:25:52,559
so this is more of a more of a visual of

00:25:49,399 --> 00:25:55,080
what's going on here so as I mentioned

00:25:52,559 --> 00:25:57,629
an order event happen it goes to Kafka

00:25:55,080 --> 00:26:01,049
Kafka sends an event a spark spark picks

00:25:57,629 --> 00:26:05,580
it up spark streaming goes and stores it

00:26:01,049 --> 00:26:07,339
in in park' in HDFS and as this this

00:26:05,580 --> 00:26:10,559
data pipeline is streaming data

00:26:07,339 --> 00:26:13,379
continuously I can go ahead and in

00:26:10,559 --> 00:26:15,509
Jupiter notebook I can query the data I

00:26:13,379 --> 00:26:19,649
can find out how many orders do I have

00:26:15,509 --> 00:26:21,389
for item number one how many orders do I

00:26:19,649 --> 00:26:24,859
have for item room two how many orders

00:26:21,389 --> 00:26:27,330
do I have from a particular geography

00:26:24,859 --> 00:26:29,849
you know I can do in you know

00:26:27,330 --> 00:26:31,799
intelligent queries while the data

00:26:29,849 --> 00:26:33,989
streaming so I don't have to wait for

00:26:31,799 --> 00:26:37,440
some job to to run I don't have to run

00:26:33,989 --> 00:26:44,869
no cron job I can just query it right

00:26:37,440 --> 00:26:44,869
away so I guess it's demo time

00:26:47,920 --> 00:26:56,120
one moment but before we go to them one

00:26:54,260 --> 00:27:01,070
let's look at the code a little bit okay

00:26:56,120 --> 00:27:03,140
so actually we'll go to demo and then

00:27:01,070 --> 00:27:06,110
we'll discuss the code after well

00:27:03,140 --> 00:27:14,830
basically this is the camel web service

00:27:06,110 --> 00:27:14,830
that I have and one moment

00:27:39,500 --> 00:27:43,460
okay so this is the camel web service so

00:27:41,690 --> 00:27:46,070
the interesting thing with camel is you

00:27:43,460 --> 00:27:49,100
can actually set up a web service using

00:27:46,070 --> 00:27:51,440
camels API saying that as whenever

00:27:49,100 --> 00:27:54,110
there's a message or a JSON that gets

00:27:51,440 --> 00:27:58,850
posted here just just send it through

00:27:54,110 --> 00:28:01,250
and just send it make sure that you know

00:27:58,850 --> 00:28:03,549
cast it through this data type you know

00:28:01,250 --> 00:28:07,370
and then just send it to this beam here

00:28:03,549 --> 00:28:10,880
order service okay and then that order

00:28:07,370 --> 00:28:12,980
service is gonna go and let's go take a

00:28:10,880 --> 00:28:16,750
look at order service actually it's

00:28:12,980 --> 00:28:16,750
gonna go and do two things let's sleep

00:28:20,799 --> 00:28:27,309
it's gonna go ahead and and and create

00:28:24,320 --> 00:28:31,610
an event and then just send it to Kafka

00:28:27,309 --> 00:28:35,750
to a topic and then it's gonna it'll

00:28:31,610 --> 00:28:40,490
also save into Cassandra as well I'd

00:28:35,750 --> 00:28:45,250
order okay so now let's let's see what

00:28:40,490 --> 00:28:45,250
that looks like give me a moment here

00:29:00,760 --> 00:29:03,390
yeah

00:29:10,530 --> 00:29:17,910
okay so we're gonna we're gonna start

00:29:14,040 --> 00:29:20,340
off over here okay

00:29:17,910 --> 00:29:22,500
so currently I've already deployed the

00:29:20,340 --> 00:29:24,750
camel web service that I showed you it's

00:29:22,500 --> 00:29:27,150
already deployed it's running in a

00:29:24,750 --> 00:29:30,030
container within a pod

00:29:27,150 --> 00:29:32,100
a kubernetes part within OpenShift okay

00:29:30,030 --> 00:29:34,080
and this is the the free openshift

00:29:32,100 --> 00:29:36,660
that's on open ship

00:29:34,080 --> 00:29:44,040
origin right so I'm running this I have

00:29:36,660 --> 00:29:47,460
that pod running okay so basically I

00:29:44,040 --> 00:29:50,790
wrote a Python program that's a spark

00:29:47,460 --> 00:29:53,670
Python program and what I want openshift

00:29:50,790 --> 00:29:55,500
to do is get my source code I don't care

00:29:53,670 --> 00:29:57,330
about creating a docker container I

00:29:55,500 --> 00:29:59,550
don't care about all these other things

00:29:57,330 --> 00:30:02,280
all I care about is that

00:29:59,550 --> 00:30:04,680
openshift goes and gets the Python

00:30:02,280 --> 00:30:08,700
source code converts that into a docker

00:30:04,680 --> 00:30:10,680
image or a container and then stores it

00:30:08,700 --> 00:30:14,000
in a registry a container registry

00:30:10,680 --> 00:30:18,000
within OpenShift and once that container

00:30:14,000 --> 00:30:21,270
registry gets that image then and it

00:30:18,000 --> 00:30:24,690
does the build then I wanted to go ahead

00:30:21,270 --> 00:30:28,170
and you know submit that to a spark

00:30:24,690 --> 00:30:30,480
cluster once the job is ready and also

00:30:28,170 --> 00:30:32,940
it's gonna also create a spark cluster

00:30:30,480 --> 00:30:35,100
as well well we'll see this video so

00:30:32,940 --> 00:30:37,040
this is the the build happening right

00:30:35,100 --> 00:30:39,930
now so it's building the source code and

00:30:37,040 --> 00:30:45,780
then it's now creating a container for

00:30:39,930 --> 00:30:51,360
it so this is pretty fast it's gonna

00:30:45,780 --> 00:30:54,450
quickly build and we're gonna take a

00:30:51,360 --> 00:30:55,770
look at the logs here so it's Co the

00:30:54,450 --> 00:30:57,360
things with openshift is telling you

00:30:55,770 --> 00:30:59,220
exactly what's going on that it's

00:30:57,360 --> 00:31:01,920
building the layers and then they push

00:30:59,220 --> 00:31:07,200
the push it to the registry and now you

00:31:01,920 --> 00:31:12,000
have a cluster here so this is these two

00:31:07,200 --> 00:31:14,340
top ones over here are clusters this is

00:31:12,000 --> 00:31:16,350
a cluster so this is a one node cluster

00:31:14,340 --> 00:31:21,210
so you have one mass in one worker here

00:31:16,350 --> 00:31:22,500
and what you have there is you have your

00:31:21,210 --> 00:31:24,900
streaming app

00:31:22,500 --> 00:31:30,090
and then you have your built you know

00:31:24,900 --> 00:31:32,100
everything getting built so now we're

00:31:30,090 --> 00:31:33,720
looking at the logs in which in the

00:31:32,100 --> 00:31:35,190
worker it's saying where is the master

00:31:33,720 --> 00:31:39,540
it's trying to connect to the master

00:31:35,190 --> 00:31:42,600
once it connects then now the the job is

00:31:39,540 --> 00:31:45,180
being submitted so as you can see this

00:31:42,600 --> 00:31:47,940
job it's actually going and connecting

00:31:45,180 --> 00:31:50,070
to a particular Hadoop cluster

00:31:47,940 --> 00:31:51,420
so I specify in the configuration the

00:31:50,070 --> 00:31:54,210
Hadoop cluster that I'm connecting to

00:31:51,420 --> 00:31:59,480
and also the packages that are being

00:31:54,210 --> 00:32:02,610
used and it's going ahead and and and

00:31:59,480 --> 00:32:04,350
submitting this job to the cluster so

00:32:02,610 --> 00:32:05,850
this Python application that I wrote I

00:32:04,350 --> 00:32:10,550
didn't write any docker

00:32:05,850 --> 00:32:12,840
I just submitted it to this cluster and

00:32:10,550 --> 00:32:17,700
let let OpenShift do the heavy lifting

00:32:12,840 --> 00:32:22,580
for me so this process is pretty quick

00:32:17,700 --> 00:32:26,940
as you can see so now it's connected to

00:32:22,580 --> 00:32:29,520
HDFS so this is HDFS here one

00:32:26,940 --> 00:32:33,990
interesting or important point that I

00:32:29,520 --> 00:32:36,180
want to mention right now is if you have

00:32:33,990 --> 00:32:38,400
a stream that's running you want to make

00:32:36,180 --> 00:32:40,650
sure that its fault tolerant one

00:32:38,400 --> 00:32:42,960
interesting technique there is water

00:32:40,650 --> 00:32:44,400
marking so with spark structure

00:32:42,960 --> 00:32:47,370
streaming unless you do watermarking

00:32:44,400 --> 00:32:49,980
because when you get a message there is

00:32:47,370 --> 00:32:51,840
an offset in Kafka and you want to make

00:32:49,980 --> 00:32:54,480
sure you don't go back in time and

00:32:51,840 --> 00:32:57,710
process the same message again you know

00:32:54,480 --> 00:33:01,080
it's like customer orders pair of shoes

00:32:57,710 --> 00:33:06,900
you know and you charge them twice it's

00:33:01,080 --> 00:33:10,200
not good for business so here once I set

00:33:06,900 --> 00:33:11,940
this up it creates the files and this

00:33:10,200 --> 00:33:13,620
also has check pointing and I'll show

00:33:11,940 --> 00:33:15,930
you the work the code of how to do a

00:33:13,620 --> 00:33:19,500
check pointing in it in a few minutes

00:33:15,930 --> 00:33:21,720
after this so this is a Jupiter notebook

00:33:19,500 --> 00:33:25,350
here so I'm just taking a little

00:33:21,720 --> 00:33:28,140
snippets which parts that I'll need and

00:33:25,350 --> 00:33:30,620
I'll start off with with a new notebook

00:33:28,140 --> 00:33:33,750
where I'm gonna do ad hoc queries as

00:33:30,620 --> 00:33:35,720
data streaming in so the data is

00:33:33,750 --> 00:33:39,440
streaming in continuously

00:33:35,720 --> 00:33:42,110
and at any point I will I can query and

00:33:39,440 --> 00:33:44,780
say how many orders do I have which

00:33:42,110 --> 00:33:47,990
orders do I have give me the ID number

00:33:44,780 --> 00:33:50,240
for those orders at will I don't have to

00:33:47,990 --> 00:33:56,000
wait for a cron job to do it it's

00:33:50,240 --> 00:33:59,720
streaming so I'm setting up the notebook

00:33:56,000 --> 00:34:02,080
and this snippet here what that snippet

00:33:59,720 --> 00:34:07,130
is doing it's going and connecting to

00:34:02,080 --> 00:34:08,869
the HDFS node with the particular

00:34:07,130 --> 00:34:13,899
directory where all the park' file is

00:34:08,869 --> 00:34:13,899
being stored and then it's doing a query

00:34:24,150 --> 00:34:29,400
okay so once I'm connected here I I get

00:34:28,680 --> 00:34:32,310
a panda

00:34:29,400 --> 00:34:34,470
so panda has their own data frames it's

00:34:32,310 --> 00:34:36,000
different than the other one that we're

00:34:34,470 --> 00:34:38,670
talking about with it with spark and

00:34:36,000 --> 00:34:41,550
then it's it's nice to use it pandas

00:34:38,670 --> 00:34:43,650
with with notebooks because you can get

00:34:41,550 --> 00:34:45,690
like you know you know you can get

00:34:43,650 --> 00:34:52,530
printouts and counts and you can do

00:34:45,690 --> 00:34:55,200
interesting things within Python so so

00:34:52,530 --> 00:34:57,810
over here I have this you know this

00:34:55,200 --> 00:35:03,570
Python client that I'm using to simulate

00:34:57,810 --> 00:35:05,130
a customer order so this Python script

00:35:03,570 --> 00:35:08,130
is basically just gonna pick a random

00:35:05,130 --> 00:35:10,830
customer it'll pick a random product and

00:35:08,130 --> 00:35:13,470
in a little issue in order for that

00:35:10,830 --> 00:35:17,010
product and we're gonna see in the

00:35:13,470 --> 00:35:19,680
screen the screen back there that I just

00:35:17,010 --> 00:35:21,510
opened up right now that screen is is

00:35:19,680 --> 00:35:24,300
listening in for all messages that are

00:35:21,510 --> 00:35:28,410
coming into Kafka so it's going to print

00:35:24,300 --> 00:35:31,680
out the JSON within that screen so over

00:35:28,410 --> 00:35:34,200
here in this demo actually the IP

00:35:31,680 --> 00:35:38,880
address needs to be fixed so we'll skip

00:35:34,200 --> 00:35:42,260
a little bit of it and as you can see I

00:35:38,880 --> 00:35:42,260
correct the IP address there

00:35:51,640 --> 00:35:57,700
okay so as you can see in the background

00:35:54,390 --> 00:36:02,350
the order just came through so now there

00:35:57,700 --> 00:36:06,010
should be one order as you can see we

00:36:02,350 --> 00:36:13,630
have one order and it's grouping by the

00:36:06,010 --> 00:36:16,330
product ID okay so one product and then

00:36:13,630 --> 00:36:20,140
I run I run it again I run that or that

00:36:16,330 --> 00:36:26,790
client again to run and send another

00:36:20,140 --> 00:36:29,260
order and then I'm gonna run a watch

00:36:26,790 --> 00:36:31,810
where that watch is gonna continuously

00:36:29,260 --> 00:36:35,230
add a particular interval keep on

00:36:31,810 --> 00:36:37,510
sending orders you know and I'm gonna

00:36:35,230 --> 00:36:39,910
keep on running Jupiter notebook and as

00:36:37,510 --> 00:36:45,270
you can see this table is getting

00:36:39,910 --> 00:36:45,270
updated now we have five orders

00:36:54,250 --> 00:36:57,670
six orders

00:37:02,060 --> 00:37:08,330
ten orders and this is streaming I'm not

00:37:06,980 --> 00:37:10,370
running any other program other than

00:37:08,330 --> 00:37:14,090
this spark structure streaming program

00:37:10,370 --> 00:37:17,060
and this is that that is the the files

00:37:14,090 --> 00:37:19,600
those are the files actually let's take

00:37:17,060 --> 00:37:19,600
a look here

00:37:24,050 --> 00:37:28,960
so this is where the files are getting

00:37:26,120 --> 00:37:33,080
stored so they're getting stored and

00:37:28,960 --> 00:37:34,700
compressed here so you can download one

00:37:33,080 --> 00:37:37,190
of these files and you can use park'

00:37:34,700 --> 00:37:39,670
tools to print out the contents of any

00:37:37,190 --> 00:37:41,990
of these files if you if you will but

00:37:39,670 --> 00:37:44,120
since spark is connecting to a folder

00:37:41,990 --> 00:37:45,170
there its it'll be able to query all

00:37:44,120 --> 00:37:49,130
those files that are in there it's not

00:37:45,170 --> 00:37:55,220
doing individual files because of sparks

00:37:49,130 --> 00:37:58,240
you know technology so we're gonna take

00:37:55,220 --> 00:38:01,760
a quick look at one more thing which is

00:37:58,240 --> 00:38:04,570
how do you do checkpointing which is

00:38:01,760 --> 00:38:04,570
pretty important

00:38:11,030 --> 00:38:17,870
or you know what we'll just we'll look

00:38:13,130 --> 00:38:20,470
at it it'll github it's a lot easier to

00:38:17,870 --> 00:38:20,470
read on the screen

00:38:27,920 --> 00:38:38,930
okay and this is the code so basically I

00:38:35,420 --> 00:38:44,480
create a spark session I connect to

00:38:38,930 --> 00:38:47,210
Kafka the topic that I want and then I'm

00:38:44,480 --> 00:38:49,369
getting a JSON I'm getting a JSON object

00:38:47,210 --> 00:38:52,490
and I'm turning that JSON object into a

00:38:49,369 --> 00:38:55,549
data frame and then I'm basically

00:38:52,490 --> 00:38:58,880
writing this write this stream to this

00:38:55,549 --> 00:39:03,290
HDFS folder and make a check point at

00:38:58,880 --> 00:39:05,210
this location okay and this check point

00:39:03,290 --> 00:39:06,710
location will have all the metadata of

00:39:05,210 --> 00:39:07,640
like the offices and all these other

00:39:06,710 --> 00:39:11,000
different things which are very

00:39:07,640 --> 00:39:16,569
important so I just got two minutes

00:39:11,000 --> 00:39:18,710
three minutes left so I'm gonna quickly

00:39:16,569 --> 00:39:20,770
take questions if anybody has any

00:39:18,710 --> 00:39:20,770
questions

00:39:28,620 --> 00:39:33,870
so in Python Jupiter notebooks you can

00:39:31,860 --> 00:39:36,630
you can use pandas to do you know

00:39:33,870 --> 00:39:38,520
different things for example some folks

00:39:36,630 --> 00:39:40,500
use pandas and then they use matplotlib

00:39:38,520 --> 00:39:42,600
and then they create graphs and charts

00:39:40,500 --> 00:39:45,600
and different different things there

00:39:42,600 --> 00:39:50,820
within jupiter notebook let me get back

00:39:45,600 --> 00:39:52,890
to the slides I just want to recap so

00:39:50,820 --> 00:39:56,310
just recap of what we what we've learned

00:39:52,890 --> 00:39:59,850
so we've learned about ETL and using

00:39:56,310 --> 00:40:02,280
apache camel for processing data we've

00:39:59,850 --> 00:40:04,260
learned about data formats like Avro

00:40:02,280 --> 00:40:07,620
park' and comparing it with other

00:40:04,260 --> 00:40:10,100
formats we've learned about integrating

00:40:07,620 --> 00:40:12,690
Kafka with spark structure streaming

00:40:10,100 --> 00:40:15,780
we've learned that we can do ad-hoc

00:40:12,690 --> 00:40:21,300
queries on data that's getting streamed

00:40:15,780 --> 00:40:24,210
live and we've also learned about my

00:40:21,300 --> 00:40:26,810
project write analytics IO which is an

00:40:24,210 --> 00:40:29,250
an open source community project I

00:40:26,810 --> 00:40:31,170
highly encourage anybody to take a look

00:40:29,250 --> 00:40:34,530
we have tutorials on there if you want

00:40:31,170 --> 00:40:39,420
to try it out running spark on on on

00:40:34,530 --> 00:40:42,480
OpenShift and we've learned how to

00:40:39,420 --> 00:40:45,990
deploy a spark application on OpenShift

00:40:42,480 --> 00:40:55,500
and how did the how to run a streaming

00:40:45,990 --> 00:40:57,180
app and that's all I have if anybody has

00:40:55,500 --> 00:40:59,520
any more questions I'm glad to take them

00:40:57,180 --> 00:41:01,680
and also I'll have office hours at the

00:40:59,520 --> 00:41:03,600
booth right away right after this talk

00:41:01,680 --> 00:41:05,220
I'm gonna go to the booth so if anybody

00:41:03,600 --> 00:41:07,410
wants to stop by and take a look at this

00:41:05,220 --> 00:41:09,750
I'm more than happy to go through that

00:41:07,410 --> 00:41:11,790
demo again and also talk about the code

00:41:09,750 --> 00:41:14,010
as well if you want to go dive a little

00:41:11,790 --> 00:41:15,780
deeper and yeah

00:41:14,010 --> 00:41:18,320
that's all I have thank you thank you

00:41:15,780 --> 00:41:18,320
all for your time

00:41:20,630 --> 00:41:27,919
oh and this is the link to the slides if

00:41:25,309 --> 00:41:34,069
you if you like it should be good

00:41:27,919 --> 00:41:37,339
I see no actually you know it's no I'm

00:41:34,069 --> 00:41:40,219
gonna give you a it's not this link it's

00:41:37,339 --> 00:41:42,319
on the link on the conference page just

00:41:40,219 --> 00:41:43,819
click that the slides will be there this

00:41:42,319 --> 00:41:49,459
is this link isn't it it's an old

00:41:43,819 --> 00:41:51,049
different slide yeah actually we we do

00:41:49,459 --> 00:41:53,150
have Jupiter notebook running in darker

00:41:51,049 --> 00:41:55,160
and I think I can give you links to that

00:41:53,150 --> 00:41:57,199
the darker image and that docker image

00:41:55,160 --> 00:41:58,400
is pretty pretty good like you can we

00:41:57,199 --> 00:42:01,759
can we even have it running with

00:41:58,400 --> 00:42:03,709
tensorflow and and spark and everything

00:42:01,759 --> 00:42:04,910
so all the heavy lifting is already done

00:42:03,709 --> 00:42:07,130
for you guys you just need to just use

00:42:04,910 --> 00:42:09,169
that darker image so I'll I'll provide

00:42:07,130 --> 00:42:10,999
you link if you want to talk to me after

00:42:09,169 --> 00:42:12,789
the talk I'm gonna be hitting downstairs

00:42:10,999 --> 00:42:19,089
to the booth or we can talk here or

00:42:12,789 --> 00:42:19,089

YouTube URL: https://www.youtube.com/watch?v=YKvmbfdjJ1k


