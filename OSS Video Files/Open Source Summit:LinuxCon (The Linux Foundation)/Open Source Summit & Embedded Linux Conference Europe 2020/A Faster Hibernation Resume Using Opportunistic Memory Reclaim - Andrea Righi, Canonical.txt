Title: A Faster Hibernation Resume Using Opportunistic Memory Reclaim - Andrea Righi, Canonical
Publication date: 2020-11-13
Playlist: Open Source Summit & Embedded Linux Conference Europe 2020
Description: 
	A Faster Hibernation/Resume Using Opportunistic Memory Reclaim - Andrea Righi, Canonical
Captions: 
	00:00:06,160 --> 00:00:09,599
all right

00:00:06,879 --> 00:00:11,519
welcome everybody my name is andrea i

00:00:09,599 --> 00:00:13,840
work in the canonical kernel team

00:00:11,519 --> 00:00:16,560
mostly focusing on cloud kernels and

00:00:13,840 --> 00:00:18,240
kernel space in general in this talk i

00:00:16,560 --> 00:00:19,600
want to show you something i'm working

00:00:18,240 --> 00:00:22,480
on recently

00:00:19,600 --> 00:00:24,000
called opportunistic memory reclaim and

00:00:22,480 --> 00:00:24,880
i'm going to show you how we can use

00:00:24,000 --> 00:00:28,240
this feature

00:00:24,880 --> 00:00:30,960
to improve hibernation performance

00:00:28,240 --> 00:00:32,000
first of all a quick introduction about

00:00:30,960 --> 00:00:35,280
ibernation

00:00:32,000 --> 00:00:36,160
so what is hibernation hibernation is a

00:00:35,280 --> 00:00:38,079
feature

00:00:36,160 --> 00:00:39,280
that allows you to power down your

00:00:38,079 --> 00:00:41,520
system and

00:00:39,280 --> 00:00:44,320
restore it to its previous state when

00:00:41,520 --> 00:00:47,280
the system is powered back on

00:00:44,320 --> 00:00:49,760
this is actually a quite old feature

00:00:47,280 --> 00:00:51,920
provided by the linux kernel

00:00:49,760 --> 00:00:53,520
in fact it was originally designed as a

00:00:51,920 --> 00:00:56,239
feature for laptops

00:00:53,520 --> 00:00:58,719
and it used to be a really hot topic

00:00:56,239 --> 00:01:00,719
back in around 2004

00:00:58,719 --> 00:01:03,920
when we didn't have the modern suspended

00:01:00,719 --> 00:01:06,400
ram technologies that we have nowadays

00:01:03,920 --> 00:01:07,200
and that time the idea was to implement

00:01:06,400 --> 00:01:10,000
a mechanism

00:01:07,200 --> 00:01:12,000
in the kernel that would allow to dump

00:01:10,000 --> 00:01:13,840
the entire content of your memory to

00:01:12,000 --> 00:01:16,560
have existing storage

00:01:13,840 --> 00:01:17,840
so that you could power off your system

00:01:16,560 --> 00:01:20,720
save your battery

00:01:17,840 --> 00:01:21,439
and when the system was powered back on

00:01:20,720 --> 00:01:24,240
the memory

00:01:21,439 --> 00:01:26,080
could be restored reading the data from

00:01:24,240 --> 00:01:28,799
the persistent storage

00:01:26,080 --> 00:01:30,400
and in this way you could presume all

00:01:28,799 --> 00:01:34,159
your running applications

00:01:30,400 --> 00:01:37,040
all your active sessions and so on

00:01:34,159 --> 00:01:37,759
now like i was saying this is quite an

00:01:37,040 --> 00:01:40,560
old feature

00:01:37,759 --> 00:01:42,799
but is it still relevant to talk about

00:01:40,560 --> 00:01:45,040
hibernation nowadays

00:01:42,799 --> 00:01:46,079
well interestingly enough there are

00:01:45,040 --> 00:01:49,119
still some

00:01:46,079 --> 00:01:53,040
modern use cases about hibernation

00:01:49,119 --> 00:01:56,799
and one of them is cloud computing

00:01:53,040 --> 00:01:59,920
in fact in late 2018

00:01:56,799 --> 00:02:04,159
amazon announced the support for

00:01:59,920 --> 00:02:08,160
hibernating ec2 instances in their cloud

00:02:04,159 --> 00:02:10,800
question is why why a cloud provider

00:02:08,160 --> 00:02:11,920
would want to invest development time

00:02:10,800 --> 00:02:15,120
and efforts

00:02:11,920 --> 00:02:17,760
to provide such feature

00:02:15,120 --> 00:02:19,040
and it's interesting to notice that

00:02:17,760 --> 00:02:22,160
there are

00:02:19,040 --> 00:02:25,200
some useful use cases

00:02:22,160 --> 00:02:27,120
for this for this particular feature one

00:02:25,200 --> 00:02:30,160
of them is to give the ability

00:02:27,120 --> 00:02:33,840
to pose your workload nicely when your

00:02:30,160 --> 00:02:36,560
budget is over for example

00:02:33,840 --> 00:02:37,840
in the amazon infrastructure the amazon

00:02:36,560 --> 00:02:40,160
cloud

00:02:37,840 --> 00:02:41,519
you can create instances that are called

00:02:40,160 --> 00:02:44,959
spot instances

00:02:41,519 --> 00:02:46,239
and you can see them as low priority

00:02:44,959 --> 00:02:50,720
instances

00:02:46,239 --> 00:02:53,760
that run on temporarily unused resources

00:02:50,720 --> 00:02:55,280
they are cheap but if a high priority

00:02:53,760 --> 00:02:59,519
incident comes in

00:02:55,280 --> 00:03:02,800
and reclaim the resources they paid for

00:02:59,519 --> 00:03:03,680
we need to find a way to migrate the low

00:03:02,800 --> 00:03:07,280
priority

00:03:03,680 --> 00:03:10,720
instances somewhere else however

00:03:07,280 --> 00:03:11,519
if uh can't find enough resources to

00:03:10,720 --> 00:03:13,920
migrate

00:03:11,519 --> 00:03:15,360
the only alternative is to stop these

00:03:13,920 --> 00:03:19,280
instances

00:03:15,360 --> 00:03:21,360
and in this case hibernation can provide

00:03:19,280 --> 00:03:22,400
a pretty nice way to stop these

00:03:21,360 --> 00:03:24,640
instances

00:03:22,400 --> 00:03:26,480
way better than simply shutting them

00:03:24,640 --> 00:03:29,040
down

00:03:26,480 --> 00:03:31,519
another use case is to give the ability

00:03:29,040 --> 00:03:34,879
to deploy worm instances

00:03:31,519 --> 00:03:37,920
so basically you can deploy an instance

00:03:34,879 --> 00:03:40,879
install all the required packages

00:03:37,920 --> 00:03:41,920
start all the services that you want and

00:03:40,879 --> 00:03:44,959
at that time

00:03:41,920 --> 00:03:47,440
instead of adding the instances

00:03:44,959 --> 00:03:49,360
immediately to production you can

00:03:47,440 --> 00:03:52,480
hibernate the instance and

00:03:49,360 --> 00:03:54,239
keep it in a frozen state and when it's

00:03:52,480 --> 00:03:55,200
really needed we can just resume from

00:03:54,239 --> 00:03:58,239
hibernation and

00:03:55,200 --> 00:03:58,799
immediately add the production and this

00:03:58,239 --> 00:04:00,480
works

00:03:58,799 --> 00:04:03,120
because usually resuming from

00:04:00,480 --> 00:04:04,640
hibernation can be faster than a cold

00:04:03,120 --> 00:04:07,920
boot

00:04:04,640 --> 00:04:10,239
now in all these examples hibernation

00:04:07,920 --> 00:04:13,760
can be a winning solution

00:04:10,239 --> 00:04:15,920
but performance is important

00:04:13,760 --> 00:04:17,919
in fact hibernation is a winning

00:04:15,920 --> 00:04:20,239
solution only if

00:04:17,919 --> 00:04:21,280
hibernation is faster in the previous

00:04:20,239 --> 00:04:26,160
case

00:04:21,280 --> 00:04:28,960
and if resume is fast in the second case

00:04:26,160 --> 00:04:29,600
especially in the first case i would

00:04:28,960 --> 00:04:33,840
like to

00:04:29,600 --> 00:04:36,800
i like the fact that uh it's actually a

00:04:33,840 --> 00:04:38,960
regular scheduling problem where we have

00:04:36,800 --> 00:04:42,000
like low priority tasks that are

00:04:38,960 --> 00:04:45,040
interrupted and by higher priority

00:04:42,000 --> 00:04:48,240
tasks that's the same problem so

00:04:45,040 --> 00:04:50,960
in this scenario hibernation

00:04:48,240 --> 00:04:52,080
is really the way to implement a content

00:04:50,960 --> 00:04:54,479
switch

00:04:52,080 --> 00:04:56,880
between a low priority instance and a

00:04:54,479 --> 00:04:59,919
high priority instance

00:04:56,880 --> 00:05:01,280
and if this context switch is not fast

00:04:59,919 --> 00:05:05,840
enough

00:05:01,280 --> 00:05:09,360
clearly this is not a winning solution

00:05:05,840 --> 00:05:12,240
so how can we make hibernation

00:05:09,360 --> 00:05:12,800
faster well first of all let's try to

00:05:12,240 --> 00:05:16,479
understand

00:05:12,800 --> 00:05:19,520
how hibernation works and i've tried to

00:05:16,479 --> 00:05:22,320
summarize everything in this diagram

00:05:19,520 --> 00:05:23,199
so on the left we can see the main

00:05:22,320 --> 00:05:26,240
memory

00:05:23,199 --> 00:05:26,720
and all the light blue boxes represent

00:05:26,240 --> 00:05:29,520
the

00:05:26,720 --> 00:05:30,720
chunks of allocated memory when an

00:05:29,520 --> 00:05:33,919
ivernation event

00:05:30,720 --> 00:05:36,880
happens the kernel needs to pack

00:05:33,919 --> 00:05:38,240
all these light blue boxes together and

00:05:36,880 --> 00:05:41,280
create what is called

00:05:38,240 --> 00:05:42,000
an hibernation image then the ivernation

00:05:41,280 --> 00:05:45,440
image is

00:05:42,000 --> 00:05:46,960
written to the swap device and at this

00:05:45,440 --> 00:05:50,800
point the system can

00:05:46,960 --> 00:05:53,280
be simply powered off on resume

00:05:50,800 --> 00:05:54,560
we are booting an instance of another

00:05:53,280 --> 00:05:57,199
kernel

00:05:54,560 --> 00:05:59,360
that is going to check if there is a

00:05:57,199 --> 00:06:02,319
valid hibernation signature

00:05:59,360 --> 00:06:04,080
in the swap device if a valid ibernation

00:06:02,319 --> 00:06:06,800
signature is found

00:06:04,080 --> 00:06:07,600
the kernel will copy all the hibernation

00:06:06,800 --> 00:06:10,720
image

00:06:07,600 --> 00:06:12,880
from the swap device back to memory and

00:06:10,720 --> 00:06:14,479
all the blocks will be presumed at the

00:06:12,880 --> 00:06:17,759
right locations

00:06:14,479 --> 00:06:20,560
at that point the kernel will

00:06:17,759 --> 00:06:21,440
perform a special jump operation that

00:06:20,560 --> 00:06:24,560
will jump

00:06:21,440 --> 00:06:28,160
into the previous instance of the kernel

00:06:24,560 --> 00:06:30,639
and the system will be resumed now

00:06:28,160 --> 00:06:32,080
i want to highlight a couple of things

00:06:30,639 --> 00:06:35,360
in this

00:06:32,080 --> 00:06:37,280
in this diagram one thing is that

00:06:35,360 --> 00:06:39,280
in order to generate the hibernation

00:06:37,280 --> 00:06:43,440
image the kernel needs to allocate

00:06:39,280 --> 00:06:46,720
memory and if there's not enough memory

00:06:43,440 --> 00:06:50,080
we can either abort hibernation

00:06:46,720 --> 00:06:53,120
and resume the normal execution or

00:06:50,080 --> 00:06:54,639
we can try to free up some allocated

00:06:53,120 --> 00:06:58,160
memory

00:06:54,639 --> 00:07:00,880
obviously it's really easy to free

00:06:58,160 --> 00:07:02,800
memory that already has a copy in the

00:07:00,880 --> 00:07:06,720
corresponding backing store

00:07:02,800 --> 00:07:09,680
for instance clean page cast pages

00:07:06,720 --> 00:07:10,960
this is memory that can be proclaimed

00:07:09,680 --> 00:07:14,000
immediately without

00:07:10,960 --> 00:07:17,280
triggering any additional i o

00:07:14,000 --> 00:07:21,039
or we can decide for example to swap off

00:07:17,280 --> 00:07:25,360
some allocated anonymous memory

00:07:21,039 --> 00:07:29,120
or to flash some dirty page cache pages

00:07:25,360 --> 00:07:32,240
but in the last two cases

00:07:29,120 --> 00:07:32,800
in order to free up some memory we need

00:07:32,240 --> 00:07:36,240
to do

00:07:32,800 --> 00:07:39,759
io so yes we can

00:07:36,240 --> 00:07:42,400
definitely drop memory uh but this

00:07:39,759 --> 00:07:44,879
will cause more io another thing that i

00:07:42,400 --> 00:07:48,720
want to highlight is the fact that

00:07:44,879 --> 00:07:50,720
on the right side we don't have all the

00:07:48,720 --> 00:07:52,080
light blue boxes that we have on the

00:07:50,720 --> 00:07:55,280
left side

00:07:52,080 --> 00:07:58,000
and this is to i like the fact that

00:07:55,280 --> 00:07:58,319
not all the memory needs to be saved

00:07:58,000 --> 00:08:01,680
into

00:07:58,319 --> 00:08:05,520
the hibernation image so on resume

00:08:01,680 --> 00:08:08,240
probably some caches will not be present

00:08:05,520 --> 00:08:08,800
or some memory won't be present because

00:08:08,240 --> 00:08:13,440
it's been

00:08:08,800 --> 00:08:16,319
swapped out now that we understand

00:08:13,440 --> 00:08:18,879
more how hibernation works we can see

00:08:16,319 --> 00:08:21,440
what we can do to speed up performance

00:08:18,879 --> 00:08:22,560
and first of all let's try to identify

00:08:21,440 --> 00:08:26,080
the main bottleneck

00:08:22,560 --> 00:08:28,800
vibranation usually the main buckle neck

00:08:26,080 --> 00:08:31,120
is represented by the io that is

00:08:28,800 --> 00:08:31,759
required to write the hibernation image

00:08:31,120 --> 00:08:36,159
to

00:08:31,759 --> 00:08:39,360
to the swap device and on resume

00:08:36,159 --> 00:08:40,159
the bottleneck would be to the again the

00:08:39,360 --> 00:08:42,399
i o

00:08:40,159 --> 00:08:45,839
required to load the hibernation image

00:08:42,399 --> 00:08:49,120
from swap device back to memory

00:08:45,839 --> 00:08:51,760
so if we are able to reduce this i o

00:08:49,120 --> 00:08:55,120
we can probably achieve a faster

00:08:51,760 --> 00:08:56,080
hibernation resume now how can we reduce

00:08:55,120 --> 00:09:00,480
the io

00:08:56,080 --> 00:09:02,880
well one way is using compression so

00:09:00,480 --> 00:09:04,080
the hibernation image can be reduced by

00:09:02,880 --> 00:09:07,200
compressing it

00:09:04,080 --> 00:09:09,920
in this way we will probably do less io

00:09:07,200 --> 00:09:13,279
so hibernation will be faster

00:09:09,920 --> 00:09:16,640
another way like i was mentioning is to

00:09:13,279 --> 00:09:19,680
drop some memory again to in order to

00:09:16,640 --> 00:09:23,200
reduce the size of the hibernation image

00:09:19,680 --> 00:09:25,200
and we can drop some lean page cache

00:09:23,200 --> 00:09:28,480
pages for example we can just

00:09:25,200 --> 00:09:29,839
drop some caches uh that doesn't require

00:09:28,480 --> 00:09:32,560
additional io

00:09:29,839 --> 00:09:33,440
and this would reduce the hibernation

00:09:32,560 --> 00:09:36,959
image and

00:09:33,440 --> 00:09:37,920
would speed up ionization time but if we

00:09:36,959 --> 00:09:41,200
need to drop

00:09:37,920 --> 00:09:44,480
some anonymous memory for example or

00:09:41,200 --> 00:09:47,200
to flash some dirty page cache pages

00:09:44,480 --> 00:09:48,240
in this case we are still generating io

00:09:47,200 --> 00:09:50,080
so this is not

00:09:48,240 --> 00:09:51,519
actually improving hibernation

00:09:50,080 --> 00:09:54,080
performance because

00:09:51,519 --> 00:09:54,959
yes the hibernation image will be

00:09:54,080 --> 00:09:57,360
smaller

00:09:54,959 --> 00:10:00,399
but in order to have a smaller

00:09:57,360 --> 00:10:03,760
hibernation image we still need to do iu

00:10:00,399 --> 00:10:05,760
so here comes the idea of opportunistic

00:10:03,760 --> 00:10:09,519
memory reclaim

00:10:05,760 --> 00:10:12,720
the generic idea is to provide

00:10:09,519 --> 00:10:16,480
to the user space an interface that

00:10:12,720 --> 00:10:18,720
allows to trigger a memory claim

00:10:16,480 --> 00:10:20,320
in the kernel so we can use this

00:10:18,720 --> 00:10:22,480
interface to trigger

00:10:20,320 --> 00:10:24,800
memory reclaim in advance and prepare

00:10:22,480 --> 00:10:26,079
the system to be more responsive when

00:10:24,800 --> 00:10:30,480
needed

00:10:26,079 --> 00:10:34,320
in the particular hibernation scenario

00:10:30,480 --> 00:10:37,200
we can use opportunistic memory reclaim

00:10:34,320 --> 00:10:39,040
to drop or reclaim memory in advance for

00:10:37,200 --> 00:10:42,560
example using some

00:10:39,040 --> 00:10:43,839
idle cycles in the system like if the

00:10:42,560 --> 00:10:47,440
system is idle

00:10:43,839 --> 00:10:49,279
or mostly idle we can opportunistically

00:10:47,440 --> 00:10:52,560
trigger a memory reclaim

00:10:49,279 --> 00:10:53,519
so we can prepare the system to be

00:10:52,560 --> 00:10:56,959
hibernated

00:10:53,519 --> 00:10:58,880
and if an ibernation event happens

00:10:56,959 --> 00:11:00,000
most of the memory has been already

00:10:58,880 --> 00:11:03,040
swapped out or

00:11:00,000 --> 00:11:06,240
flushed or reclaimed so hibernation

00:11:03,040 --> 00:11:09,360
will probably be faster

00:11:06,240 --> 00:11:11,839
and that is the the idea

00:11:09,360 --> 00:11:13,120
the particular usage of opportunistic

00:11:11,839 --> 00:11:16,880
memory reclaim that i

00:11:13,120 --> 00:11:20,240
have experimented and how does it work

00:11:16,880 --> 00:11:23,279
so this is how i implemented it there's

00:11:20,240 --> 00:11:26,160
a small python script

00:11:23,279 --> 00:11:27,440
that is periodically checking the idle

00:11:26,160 --> 00:11:30,880
percentage of the

00:11:27,440 --> 00:11:31,680
cpus in the system if the idle

00:11:30,880 --> 00:11:34,480
percentage

00:11:31,680 --> 00:11:36,720
is greater than a certain threshold for

00:11:34,480 --> 00:11:39,440
a certain amount of time

00:11:36,720 --> 00:11:40,320
it will trigger the opportunistic memory

00:11:39,440 --> 00:11:43,839
reclaim

00:11:40,320 --> 00:11:46,800
via this memory swapper claim interface

00:11:43,839 --> 00:11:47,600
that is obviously a specific kernel

00:11:46,800 --> 00:11:50,800
patch

00:11:47,600 --> 00:11:54,959
and um yeah kernel will start to

00:11:50,800 --> 00:11:58,160
reclaim memory during using isil

00:11:54,959 --> 00:11:59,920
cycles in the system so

00:11:58,160 --> 00:12:02,079
that's the mechanism detect when the

00:11:59,920 --> 00:12:03,360
system is mostly idle for a certain

00:12:02,079 --> 00:12:06,399
amount of time

00:12:03,360 --> 00:12:06,880
trigger artificial memory pressure so

00:12:06,399 --> 00:12:10,079
that

00:12:06,880 --> 00:12:13,440
the system will start to reclaim memory

00:12:10,079 --> 00:12:15,600
and at that point if hibernation happens

00:12:13,440 --> 00:12:17,600
most of the memory is already saved to

00:12:15,600 --> 00:12:21,440
swap and we can

00:12:17,600 --> 00:12:23,680
hibernate faster

00:12:21,440 --> 00:12:26,240
the interface that i've been using is

00:12:23,680 --> 00:12:28,800
the c group memory controller

00:12:26,240 --> 00:12:31,360
actually the first patch that i posted

00:12:28,800 --> 00:12:34,399
to the kernel mailing list

00:12:31,360 --> 00:12:39,600
in the first page i was using

00:12:34,399 --> 00:12:42,959
a file under csfs and sees power

00:12:39,600 --> 00:12:43,839
because yeah that was a very hibernation

00:12:42,959 --> 00:12:48,079
specific

00:12:43,839 --> 00:12:50,320
uh interface and that's because i

00:12:48,079 --> 00:12:51,519
typically use the the opportunistic

00:12:50,320 --> 00:12:54,160
memory claim for

00:12:51,519 --> 00:12:55,680
hibernation but then i realized this

00:12:54,160 --> 00:12:59,360
feature can be more generic

00:12:55,680 --> 00:13:02,560
and there can be benefits for other

00:12:59,360 --> 00:13:05,760
scenarios so this is why uh in the

00:13:02,560 --> 00:13:07,760
next versions i i moved to an

00:13:05,760 --> 00:13:09,760
interface implemented in the c group

00:13:07,760 --> 00:13:12,959
memory controller

00:13:09,760 --> 00:13:16,639
and so the main reason

00:13:12,959 --> 00:13:19,920
to use c group memory controller is to

00:13:16,639 --> 00:13:21,440
be able to apply a fine-grained memory

00:13:19,920 --> 00:13:25,360
reclaimed policy

00:13:21,440 --> 00:13:27,920
for example we can create multiple c

00:13:25,360 --> 00:13:28,880
groups this is just an example where i

00:13:27,920 --> 00:13:31,519
create

00:13:28,880 --> 00:13:33,600
two c groups one is called foreground

00:13:31,519 --> 00:13:36,079
the other is called background

00:13:33,600 --> 00:13:38,560
and the idea is to move the place

00:13:36,079 --> 00:13:41,120
latency sensitive applications

00:13:38,560 --> 00:13:43,519
into the foreground c group and the

00:13:41,120 --> 00:13:45,760
latency insensitive applications

00:13:43,519 --> 00:13:46,720
into the background c group and then

00:13:45,760 --> 00:13:49,680
when i

00:13:46,720 --> 00:13:52,079
want to trigger memory reclaim i can

00:13:49,680 --> 00:13:54,240
reclaim memory only from the

00:13:52,079 --> 00:13:56,240
background c groups so in this way i

00:13:54,240 --> 00:13:59,760
won't affect performance

00:13:56,240 --> 00:14:01,760
of the foreground c group

00:13:59,760 --> 00:14:03,199
this can be useful for example in a

00:14:01,760 --> 00:14:05,440
context

00:14:03,199 --> 00:14:07,279
in a mobile device for example let's say

00:14:05,440 --> 00:14:08,240
you're playing a video game in a

00:14:07,279 --> 00:14:10,639
smartphone

00:14:08,240 --> 00:14:13,120
you may want to move the task of the

00:14:10,639 --> 00:14:16,320
video game in the foreground c group

00:14:13,120 --> 00:14:17,760
and keep all the other tasks in the

00:14:16,320 --> 00:14:20,000
background c groups like

00:14:17,760 --> 00:14:20,880
casting is periodically checking for my

00:14:20,000 --> 00:14:23,600
emails

00:14:20,880 --> 00:14:25,040
or the task that is periodically

00:14:23,600 --> 00:14:27,600
checking if i have

00:14:25,040 --> 00:14:28,720
new messages on facebook or something

00:14:27,600 --> 00:14:31,040
like that

00:14:28,720 --> 00:14:33,920
and if you want to use opportunistic

00:14:31,040 --> 00:14:36,800
memory reclaim in this context

00:14:33,920 --> 00:14:38,639
you may start the task that is

00:14:36,800 --> 00:14:40,880
reclaiming memory

00:14:38,639 --> 00:14:42,560
only from the background c groups so

00:14:40,880 --> 00:14:44,480
your video game

00:14:42,560 --> 00:14:46,560
won't be affected the performance of

00:14:44,480 --> 00:14:46,880
your video game won't be affected and

00:14:46,560 --> 00:14:51,519
you

00:14:46,880 --> 00:14:51,519
won't notice any additional latency

00:14:53,839 --> 00:14:57,440
this is the test case that i that i've

00:14:56,240 --> 00:14:59,279
been using

00:14:57,440 --> 00:15:01,839
to show the benefits of the

00:14:59,279 --> 00:15:05,040
opportunistic memory reclaim

00:15:01,839 --> 00:15:07,600
so i created a vm with

00:15:05,040 --> 00:15:08,639
eight gigabytes of ram and eight

00:15:07,600 --> 00:15:12,079
gigabytes of swap

00:15:08,639 --> 00:15:16,000
file and i explicitly

00:15:12,079 --> 00:15:16,320
set at this speed maximum i o bandwidth

00:15:16,000 --> 00:15:18,959
of

00:15:16,320 --> 00:15:20,639
100 megabytes per second because i

00:15:18,959 --> 00:15:23,120
wanted to show

00:15:20,639 --> 00:15:24,320
the benefits of this solution if we

00:15:23,120 --> 00:15:28,000
don't have a super

00:15:24,320 --> 00:15:30,639
fast storage in fact this is actually

00:15:28,000 --> 00:15:31,600
simulating pretty well what we usually

00:15:30,639 --> 00:15:34,800
have in a

00:15:31,600 --> 00:15:36,079
cloud environment in fact multiple vms

00:15:34,800 --> 00:15:37,920
are running in a

00:15:36,079 --> 00:15:39,519
usually in a shared environment on the

00:15:37,920 --> 00:15:43,920
same hypervisor

00:15:39,519 --> 00:15:46,720
and they share also the i bandwidth or

00:15:43,920 --> 00:15:48,399
in some cases there are explicit i o

00:15:46,720 --> 00:15:51,600
limits

00:15:48,399 --> 00:15:55,199
so this is to

00:15:51,600 --> 00:15:56,720
simulate this scenario and the test that

00:15:55,199 --> 00:16:00,320
i've been using

00:15:56,720 --> 00:16:01,040
is this one i i allocate 85 percent of

00:16:00,320 --> 00:16:04,399
memory

00:16:01,040 --> 00:16:07,279
i wait for 60 seconds almost in idle

00:16:04,399 --> 00:16:08,399
and then i trigger an hibernation and i

00:16:07,279 --> 00:16:12,800
resume

00:16:08,399 --> 00:16:16,720
measuring the time so this test case

00:16:12,800 --> 00:16:19,839
is probably simulating pretty well what

00:16:16,720 --> 00:16:23,360
usually happens on a one of those spot

00:16:19,839 --> 00:16:26,240
instances that i mentioned earlier

00:16:23,360 --> 00:16:27,440
because spot instances usually are

00:16:26,240 --> 00:16:29,839
deployed

00:16:27,440 --> 00:16:30,639
they start a bunch of services that

00:16:29,839 --> 00:16:34,160
could be

00:16:30,639 --> 00:16:37,680
like a large jvm application

00:16:34,160 --> 00:16:38,079
or a web server or any other services

00:16:37,680 --> 00:16:41,600
that

00:16:38,079 --> 00:16:44,480
are allocating a bunch of memory

00:16:41,600 --> 00:16:45,839
then they serve a bunch of requests but

00:16:44,480 --> 00:16:49,759
after that they usually

00:16:45,839 --> 00:16:53,040
sit in an idle condition and

00:16:49,759 --> 00:16:56,240
we can use this idle state to

00:16:53,040 --> 00:16:58,240
reclaim some memory so when we need to

00:16:56,240 --> 00:17:01,680
hibernate these instances

00:16:58,240 --> 00:17:04,640
hibernation can be faster and

00:17:01,680 --> 00:17:07,439
so is it really working let's see here's

00:17:04,640 --> 00:17:07,439
some results

00:17:08,240 --> 00:17:11,280
in this column we can see the results

00:17:10,720 --> 00:17:15,280
using

00:17:11,280 --> 00:17:17,679
a 5 9 mainline kernel

00:17:15,280 --> 00:17:18,720
and and i repeated the test so this is

00:17:17,679 --> 00:17:22,559
an average

00:17:18,720 --> 00:17:26,160
uh an average time over ten months

00:17:22,559 --> 00:17:28,559
um this is the five nine

00:17:26,160 --> 00:17:30,799
mainline kernel and this is the finite

00:17:28,559 --> 00:17:31,280
mainline kernel with the opportunistic

00:17:30,799 --> 00:17:35,360
memory

00:17:31,280 --> 00:17:37,280
claim patch and the uh

00:17:35,360 --> 00:17:39,760
the small user space script that is

00:17:37,280 --> 00:17:41,919
running the one that is monitoring for

00:17:39,760 --> 00:17:44,160
the cpu idle percentage and the

00:17:41,919 --> 00:17:47,440
distributing memory reclaim

00:17:44,160 --> 00:17:48,640
and i did the test with the using image

00:17:47,440 --> 00:17:52,240
size default or

00:17:48,640 --> 00:17:54,799
image size 0. and as we can see

00:17:52,240 --> 00:17:55,919
the results are really promising like in

00:17:54,799 --> 00:17:59,440
the

00:17:55,919 --> 00:18:02,480
image size default case the mainline

00:17:59,440 --> 00:18:03,760
kernel took almost 50 seconds to

00:18:02,480 --> 00:18:05,760
hibernate

00:18:03,760 --> 00:18:07,039
and with the opportunistic memory

00:18:05,760 --> 00:18:10,799
reclaiming place

00:18:07,039 --> 00:18:14,240
it took only 3.4 seconds so

00:18:10,799 --> 00:18:17,440
that uh hibernation in this case is

00:18:14,240 --> 00:18:20,080
more than 10 times faster and also the

00:18:17,440 --> 00:18:21,679
resume case is faster is like more than

00:18:20,080 --> 00:18:23,760
two times faster

00:18:21,679 --> 00:18:26,000
and that's because in the opportunistic

00:18:23,760 --> 00:18:29,520
memory claim case

00:18:26,000 --> 00:18:32,160
the hibernation image is way smaller

00:18:29,520 --> 00:18:32,960
and that's because the memory has been

00:18:32,160 --> 00:18:36,000
already

00:18:32,960 --> 00:18:38,480
reclaimed or swapped off or flushed in

00:18:36,000 --> 00:18:41,600
advance

00:18:38,480 --> 00:18:42,720
i repeated the tests also with image

00:18:41,600 --> 00:18:46,720
side zero

00:18:42,720 --> 00:18:49,840
and basically image size is a

00:18:46,720 --> 00:18:53,919
scissorfest tunable in sees

00:18:49,840 --> 00:18:56,160
power that where you can specify how

00:18:53,919 --> 00:18:59,360
aggressive the kernel should be

00:18:56,160 --> 00:19:03,280
at reclaiming memory when hibernation

00:18:59,360 --> 00:19:04,080
needs to be done and using a smaller

00:19:03,280 --> 00:19:06,400
value

00:19:04,080 --> 00:19:07,360
means that the we're asking the kernel

00:19:06,400 --> 00:19:09,679
to make the

00:19:07,360 --> 00:19:10,799
to try to make the hibernation image

00:19:09,679 --> 00:19:14,240
smaller

00:19:10,799 --> 00:19:16,480
so zero means we

00:19:14,240 --> 00:19:17,679
make the hibernation image as small as

00:19:16,480 --> 00:19:20,080
possible

00:19:17,679 --> 00:19:21,440
and in this case we can see that we are

00:19:20,080 --> 00:19:24,559
paying the price

00:19:21,440 --> 00:19:27,600
of trying to reduce the size of the

00:19:24,559 --> 00:19:30,799
ignition image during hibernation

00:19:27,600 --> 00:19:32,799
but then ultimately the kernel can

00:19:30,799 --> 00:19:34,880
really make a smaller hibernation image

00:19:32,799 --> 00:19:37,600
and we can see the advantage on

00:19:34,880 --> 00:19:38,080
resume because the resume time with

00:19:37,600 --> 00:19:41,440
image

00:19:38,080 --> 00:19:44,160
size 0 is reduced

00:19:41,440 --> 00:19:46,160
respect to the image size default in the

00:19:44,160 --> 00:19:49,840
mainline kernel

00:19:46,160 --> 00:19:53,280
and hibernation's ignition time however

00:19:49,840 --> 00:19:55,600
is is bigger because it with the image

00:19:53,280 --> 00:19:58,640
size default

00:19:55,600 --> 00:20:05,120
we needed 50 seconds to hibernate

00:19:58,640 --> 00:20:05,120
with image size 0 now we need 71 seconds

00:20:05,200 --> 00:20:09,120
however in the opportunistic memory

00:20:07,600 --> 00:20:12,480
reclaimed cases

00:20:09,120 --> 00:20:15,600
all performance are pretty much the same

00:20:12,480 --> 00:20:18,640
in both cases and that's because we we

00:20:15,600 --> 00:20:20,720
have already optimized the size of the

00:20:18,640 --> 00:20:24,240
hibernation image using

00:20:20,720 --> 00:20:27,520
the spare idle cycles of the system

00:20:24,240 --> 00:20:31,200
before the isolation event happens so

00:20:27,520 --> 00:20:34,720
i've also prepared a live demo

00:20:31,200 --> 00:20:37,039
to show you better how opportunistic

00:20:34,720 --> 00:20:40,240
memory cleaning works

00:20:37,039 --> 00:20:40,880
right so let's switch to a consult

00:20:40,240 --> 00:20:44,400
session

00:20:40,880 --> 00:20:44,880
here right here we can see a virtual

00:20:44,400 --> 00:20:48,000
machine

00:20:44,880 --> 00:20:52,400
that is running it's running

00:20:48,000 --> 00:20:56,720
a ubuntu groovy distribution

00:20:52,400 --> 00:20:59,200
kernel that i'm using is a 5 9 kernel

00:20:56,720 --> 00:21:01,679
with the opportunistic memory reclaim

00:20:59,200 --> 00:21:04,720
patch applied

00:21:01,679 --> 00:21:07,280
now in the first example i'm not going

00:21:04,720 --> 00:21:08,400
to use the opportunistic memory claim

00:21:07,280 --> 00:21:10,880
because we want to

00:21:08,400 --> 00:21:12,799
measure like the baseline of the

00:21:10,880 --> 00:21:15,760
hibernation performance

00:21:12,799 --> 00:21:16,080
so first thing that i'm going to do is

00:21:15,760 --> 00:21:19,919
to

00:21:16,080 --> 00:21:22,960
activate swap using a swap file

00:21:19,919 --> 00:21:26,720
and then i'm going to start a

00:21:22,960 --> 00:21:29,280
memory stress test allocator we can see

00:21:26,720 --> 00:21:30,440
that initially i'm not using very much

00:21:29,280 --> 00:21:33,520
memory only

00:21:30,440 --> 00:21:36,480
150 megabytes are located

00:21:33,520 --> 00:21:37,280
and the swap is not used at all of

00:21:36,480 --> 00:21:39,760
course

00:21:37,280 --> 00:21:41,520
there are eight gigabytes available and

00:21:39,760 --> 00:21:44,960
no swap is used

00:21:41,520 --> 00:21:48,960
now i'm going to start a

00:21:44,960 --> 00:21:51,760
memory allocator that is allocating 85

00:21:48,960 --> 00:21:52,640
of the available memory and we can see

00:21:51,760 --> 00:21:56,080
up here

00:21:52,640 --> 00:21:59,840
that the memory is being allocated

00:21:56,080 --> 00:22:02,880
when we reach 85 percent of the memory

00:21:59,840 --> 00:22:06,400
uh in my usually my test case i

00:22:02,880 --> 00:22:07,919
have still within 60 seconds in idle to

00:22:06,400 --> 00:22:11,840
simulate the workload

00:22:07,919 --> 00:22:14,400
of a usual of a typical

00:22:11,840 --> 00:22:16,880
spot instance but in this time to speed

00:22:14,400 --> 00:22:20,000
up the demo a little bit we can

00:22:16,880 --> 00:22:21,840
we can just hibernate because nothing is

00:22:20,000 --> 00:22:23,360
going to happen in the system at this

00:22:21,840 --> 00:22:25,440
point

00:22:23,360 --> 00:22:27,360
no one is triggering the opportunistic

00:22:25,440 --> 00:22:31,280
memory reclaim

00:22:27,360 --> 00:22:35,120
so if i hibernate now

00:22:31,280 --> 00:22:38,720
system needs to swap off almost

00:22:35,120 --> 00:22:42,320
6.5 gigabytes of memory and

00:22:38,720 --> 00:22:45,679
after that the hibernation will complete

00:22:42,320 --> 00:22:50,080
so now i'm triggering hibernation and

00:22:45,679 --> 00:22:53,520
as we can see this task is stopped

00:22:50,080 --> 00:22:56,799
meaning that the system is currently

00:22:53,520 --> 00:23:00,320
a unusable and we need to wait that

00:22:56,799 --> 00:23:03,039
all this memory is flashed

00:23:00,320 --> 00:23:05,120
to the swap device then the system can

00:23:03,039 --> 00:23:07,840
be stopped

00:23:05,120 --> 00:23:08,480
while we're waiting i am going to

00:23:07,840 --> 00:23:11,200
describe

00:23:08,480 --> 00:23:12,080
what i'm going to do next the next step

00:23:11,200 --> 00:23:15,520
will be to

00:23:12,080 --> 00:23:18,320
activate the omr cpud

00:23:15,520 --> 00:23:19,760
that is the user space component that is

00:23:18,320 --> 00:23:24,240
monitoring for the

00:23:19,760 --> 00:23:26,480
idle percentage in the cpus

00:23:24,240 --> 00:23:27,679
and that is triggering the opportunistic

00:23:26,480 --> 00:23:30,880
memory

00:23:27,679 --> 00:23:31,919
all right so we can see here on the

00:23:30,880 --> 00:23:35,200
right side

00:23:31,919 --> 00:23:37,360
that hibernation almost completed and

00:23:35,200 --> 00:23:39,840
so when we see the prompt here it means

00:23:37,360 --> 00:23:42,720
that the system is fully either leaving

00:23:39,840 --> 00:23:44,080
hibernate and i can start the system

00:23:42,720 --> 00:23:46,559
again

00:23:44,080 --> 00:23:48,000
and hopefully my sessions will be

00:23:46,559 --> 00:23:50,400
resumed

00:23:48,000 --> 00:23:51,520
we can see that now that's the presumed

00:23:50,400 --> 00:23:54,320
time that is

00:23:51,520 --> 00:23:55,919
taking a little bit of time because

00:23:54,320 --> 00:23:58,480
again it's loading

00:23:55,919 --> 00:24:00,320
the hibernation image from the swap

00:23:58,480 --> 00:24:03,440
device to memory

00:24:00,320 --> 00:24:06,559
and soon we should be

00:24:03,440 --> 00:24:09,760
able to see the spinner going again

00:24:06,559 --> 00:24:14,320
meaning that the system is being fully

00:24:09,760 --> 00:24:18,159
resumed and we should also see the

00:24:14,320 --> 00:24:19,919
memory appear being updated

00:24:18,159 --> 00:24:22,720
let's see all right the spinner is

00:24:19,919 --> 00:24:26,400
spinning again the memory is updated

00:24:22,720 --> 00:24:29,760
so the system has been fully resumed

00:24:26,400 --> 00:24:33,360
now i'm gonna repeat the same test

00:24:29,760 --> 00:24:36,000
so just to make sure i'm starting

00:24:33,360 --> 00:24:37,600
with the same from the same conditions

00:24:36,000 --> 00:24:40,880
uh what i'm gonna do i'm gonna

00:24:37,600 --> 00:24:43,440
actually uh reboot the system

00:24:40,880 --> 00:24:44,159
all right well back into the system and

00:24:43,440 --> 00:24:47,120
up here

00:24:44,159 --> 00:24:49,600
i'm gonna start the my watch that is

00:24:47,120 --> 00:24:53,039
checking for the available memory

00:24:49,600 --> 00:24:55,679
down here i'm going to prepare this time

00:24:53,039 --> 00:24:57,840
omr cpud that is the user space

00:24:55,679 --> 00:25:01,039
component that is monitoring

00:24:57,840 --> 00:25:02,000
the idle state of the cpus and that is

00:25:01,039 --> 00:25:05,760
triggering

00:25:02,000 --> 00:25:08,880
the manager reclaiming if the system is

00:25:05,760 --> 00:25:12,240
mostly in idle and

00:25:08,880 --> 00:25:15,039
then i'm going to prepare my memory

00:25:12,240 --> 00:25:17,679
stress test

00:25:15,039 --> 00:25:19,279
and down here i'm gonna prepare my

00:25:17,679 --> 00:25:22,400
interactive session to

00:25:19,279 --> 00:25:26,840
trigger idonation now

00:25:22,400 --> 00:25:29,600
first thing let's start the omrc beauty

00:25:26,840 --> 00:25:30,880
um and then i'm gonna start the memory

00:25:29,600 --> 00:25:33,600
allocator

00:25:30,880 --> 00:25:34,799
as we can see well initially the system

00:25:33,600 --> 00:25:37,679
was fully idle

00:25:34,799 --> 00:25:38,240
at 100 percent now we can see that one

00:25:37,679 --> 00:25:41,039
core

00:25:38,240 --> 00:25:41,600
as an idle percentage of zero percent

00:25:41,039 --> 00:25:43,919
that

00:25:41,600 --> 00:25:45,679
is the memory stress test that is

00:25:43,919 --> 00:25:49,200
allocating on memory

00:25:45,679 --> 00:25:50,720
all the memory up to 85 and as we can

00:25:49,200 --> 00:25:51,919
see up here the memory is being

00:25:50,720 --> 00:25:55,279
allocated

00:25:51,919 --> 00:25:57,279
the swap is currently not used

00:25:55,279 --> 00:25:58,720
because the memory reclaiming is not

00:25:57,279 --> 00:26:00,880
triggered

00:25:58,720 --> 00:26:02,240
now that the memory allocator is done

00:26:00,880 --> 00:26:08,159
and the system is

00:26:02,240 --> 00:26:08,159
mostly idle for a certain amount of time

00:26:08,320 --> 00:26:12,159
the memory you're claiming has been

00:26:10,080 --> 00:26:15,600
triggered and

00:26:12,159 --> 00:26:16,559
the so we are using the idle state of

00:26:15,600 --> 00:26:19,760
the system

00:26:16,559 --> 00:26:23,520
to swap to pre-swap

00:26:19,760 --> 00:26:27,360
of some memory so now

00:26:23,520 --> 00:26:30,640
if uh if an hibernation event happens

00:26:27,360 --> 00:26:31,919
i don't have to swap all of all the

00:26:30,640 --> 00:26:33,760
memory because

00:26:31,919 --> 00:26:35,279
most of the memory has been already

00:26:33,760 --> 00:26:38,640
swapped off

00:26:35,279 --> 00:26:40,480
and we can see that basically yes this

00:26:38,640 --> 00:26:43,600
task is running but there's

00:26:40,480 --> 00:26:46,640
just a spin there that uh so

00:26:43,600 --> 00:26:49,039
it's just doing print and it's not very

00:26:46,640 --> 00:26:52,400
cpu intensive it's actually using

00:26:49,039 --> 00:26:54,880
a small amount of cpu

00:26:52,400 --> 00:26:55,919
something else is coming into the system

00:26:54,880 --> 00:26:58,320
and it

00:26:55,919 --> 00:27:00,400
either it's either refaulting pages or

00:26:58,320 --> 00:27:03,120
it's starting to use cpus

00:27:00,400 --> 00:27:04,960
the memo the triggered memory reclaim

00:27:03,120 --> 00:27:08,400
would just stop

00:27:04,960 --> 00:27:11,440
so now we are approaching to reclaiming

00:27:08,400 --> 00:27:15,679
all the memory basically so if an

00:27:11,440 --> 00:27:15,679
ibernation event happens now

00:27:16,000 --> 00:27:20,240
uh yeah remember before we had to wait a

00:27:18,559 --> 00:27:22,000
long time to hibernate

00:27:20,240 --> 00:27:23,279
and now we have already prepared the

00:27:22,000 --> 00:27:26,799
system in advance

00:27:23,279 --> 00:27:27,679
using the idle cycles of the system

00:27:26,799 --> 00:27:30,480
itself

00:27:27,679 --> 00:27:30,960
so if i hibernate now we can probably

00:27:30,480 --> 00:27:33,679
count

00:27:30,960 --> 00:27:34,880
three and hibernation will be down let's

00:27:33,679 --> 00:27:38,080
see

00:27:34,880 --> 00:27:41,760
so one two

00:27:38,080 --> 00:27:43,440
three done you can see immediately the

00:27:41,760 --> 00:27:44,399
prompt right here we have already

00:27:43,440 --> 00:27:48,480
underneath it

00:27:44,399 --> 00:27:49,440
and i can resume and also the resume

00:27:48,480 --> 00:27:51,919
should be

00:27:49,440 --> 00:27:53,679
faster than before as you can see we

00:27:51,919 --> 00:27:57,279
have already uploaded

00:27:53,679 --> 00:28:00,720
the hibernation image right here

00:27:57,279 --> 00:28:04,320
and soon we should see the spinner

00:28:00,720 --> 00:28:04,320
spinning again exactly

00:28:08,240 --> 00:28:12,080
so another test that i would like to

00:28:11,440 --> 00:28:15,200
show you

00:28:12,080 --> 00:28:16,080
is to something to highlight the

00:28:15,200 --> 00:28:19,919
advantage

00:28:16,080 --> 00:28:19,919
of using a

00:28:20,000 --> 00:28:23,360
memory c group interface so what i'm

00:28:22,880 --> 00:28:26,399
gonna do

00:28:23,360 --> 00:28:30,000
is to yeah let's

00:28:26,399 --> 00:28:33,200
reset the swap to do that i usually

00:28:30,000 --> 00:28:36,240
run a swap file off

00:28:33,200 --> 00:28:37,200
followed by another swap pawn and this

00:28:36,240 --> 00:28:39,520
way i'm basically

00:28:37,200 --> 00:28:40,559
cleaning up the swap reinitializes as

00:28:39,520 --> 00:28:44,320
well

00:28:40,559 --> 00:28:45,679
and this time uh so

00:28:44,320 --> 00:28:47,600
like i was saying this time i want to

00:28:45,679 --> 00:28:51,039
show you the advantage of using

00:28:47,600 --> 00:28:55,840
a memory c group interface so

00:28:51,039 --> 00:28:58,880
uh still running my memory stress test

00:28:55,840 --> 00:29:02,240
this time i'm also running a

00:28:58,880 --> 00:29:04,640
latency sensitive application

00:29:02,240 --> 00:29:05,840
it's this time delta is a simple

00:29:04,640 --> 00:29:10,159
application it's a

00:29:05,840 --> 00:29:14,159
again a simple python script that is

00:29:10,159 --> 00:29:17,520
showing a little spinner and each time

00:29:14,159 --> 00:29:20,960
that it's doing a print it's

00:29:17,520 --> 00:29:24,159
printing also the delta time between the

00:29:20,960 --> 00:29:26,320
with between one print and another and

00:29:24,159 --> 00:29:27,679
this since the system is not doing

00:29:26,320 --> 00:29:30,159
anything special at

00:29:27,679 --> 00:29:31,919
the moment uh we can see that the

00:29:30,159 --> 00:29:34,000
latency is always

00:29:31,919 --> 00:29:35,840
just perfect it's always 1000

00:29:34,000 --> 00:29:40,000
milliseconds

00:29:35,840 --> 00:29:43,440
now uh i'm going to start the

00:29:40,000 --> 00:29:46,960
uh omar cpu demon

00:29:43,440 --> 00:29:48,720
that is using the file in the

00:29:46,960 --> 00:29:50,720
c group in the memory group root that

00:29:48,720 --> 00:29:53,440
best so that means that

00:29:50,720 --> 00:29:54,960
memory reclaim is still happening

00:29:53,440 --> 00:29:57,600
system-wide

00:29:54,960 --> 00:29:58,240
so when memory reclaim is triggered i'm

00:29:57,600 --> 00:30:00,480
going to

00:29:58,240 --> 00:30:01,840
reclaim memory from everyone in the

00:30:00,480 --> 00:30:04,640
system

00:30:01,840 --> 00:30:05,679
the latency sensitive task included and

00:30:04,640 --> 00:30:07,840
as we can see

00:30:05,679 --> 00:30:09,600
every time that we trigger memory

00:30:07,840 --> 00:30:11,840
reclaim we see a little

00:30:09,600 --> 00:30:12,720
spike in the latency like three

00:30:11,840 --> 00:30:16,399
milliseconds

00:30:12,720 --> 00:30:18,159
here let's see if another another memory

00:30:16,399 --> 00:30:20,480
reclaim happens so another three

00:30:18,159 --> 00:30:23,840
milliseconds and so on

00:30:20,480 --> 00:30:27,200
now i can also start the

00:30:23,840 --> 00:30:29,200
stress test allocator again when this

00:30:27,200 --> 00:30:32,399
guy is running because it's also

00:30:29,200 --> 00:30:35,039
cpu intensive uh the memory reclaim

00:30:32,399 --> 00:30:35,440
won't be triggered but as soon as we eat

00:30:35,039 --> 00:30:39,279
the

00:30:35,440 --> 00:30:43,039
target percentage that is 85 percent

00:30:39,279 --> 00:30:43,919
uh we will start to kick off some memory

00:30:43,039 --> 00:30:47,200
reclaiming

00:30:43,919 --> 00:30:50,880
and we will probably see other

00:30:47,200 --> 00:30:51,679
spikes in the uh in the latency

00:30:50,880 --> 00:30:56,559
sensitive

00:30:51,679 --> 00:30:58,799
task probably not during the first

00:30:56,559 --> 00:31:00,880
memory reclaim because there is a lot of

00:30:58,799 --> 00:31:03,679
memory allocated by

00:31:00,880 --> 00:31:04,000
the member intensive application well

00:31:03,679 --> 00:31:07,440
actually

00:31:04,000 --> 00:31:10,480
we have seen a four millisecond spike

00:31:07,440 --> 00:31:14,000
this could be also done by the io

00:31:10,480 --> 00:31:17,679
that is required to swap off the

00:31:14,000 --> 00:31:21,120
the memory so now let's

00:31:17,679 --> 00:31:22,799
wait uh as before for the whole memory

00:31:21,120 --> 00:31:26,240
to be swapped off

00:31:22,799 --> 00:31:29,519
so that we can see uh if we are

00:31:26,240 --> 00:31:32,080
also eating other spikes down here

00:31:29,519 --> 00:31:34,000
and while we're waiting i'm gonna tell

00:31:32,080 --> 00:31:37,279
you what i want to do next

00:31:34,000 --> 00:31:40,720
uh the next test would be

00:31:37,279 --> 00:31:43,760
to create two separate c groups and

00:31:40,720 --> 00:31:45,919
i'm gonna move the latency um

00:31:43,760 --> 00:31:47,039
sorry i'm gonna move the memory

00:31:45,919 --> 00:31:50,000
allocator

00:31:47,039 --> 00:31:50,480
stress test into a c group and the last

00:31:50,000 --> 00:31:53,760
and c

00:31:50,480 --> 00:31:56,480
sensitive application into another c

00:31:53,760 --> 00:31:57,200
group and i'm going to call this one a

00:31:56,480 --> 00:32:00,159
background c

00:31:57,200 --> 00:32:02,159
group and this one a foreground c group

00:32:00,159 --> 00:32:06,399
oh let's see now we have

00:32:02,159 --> 00:32:07,679
it a pretty big latency spike like 17

00:32:06,399 --> 00:32:11,240
milliseconds

00:32:07,679 --> 00:32:15,120
oh here's like yeah

00:32:11,240 --> 00:32:18,240
271 milliseconds so we are hitting

00:32:15,120 --> 00:32:21,039
significant latency spikes

00:32:18,240 --> 00:32:22,159
down here doing memory reclaim and

00:32:21,039 --> 00:32:25,039
that's because we are doing a

00:32:22,159 --> 00:32:28,880
system-wide memory reclaim

00:32:25,039 --> 00:32:32,000
now let's try again

00:32:28,880 --> 00:32:35,440
i'm gonna repeat the test but

00:32:32,000 --> 00:32:38,320
this time like i was saying we are going

00:32:35,440 --> 00:32:42,480
to create the

00:32:38,320 --> 00:32:46,399
two c groups

00:32:42,480 --> 00:32:51,200
one called fg and the other is called pg

00:32:46,399 --> 00:32:51,200
and i'm going to move this guy here

00:32:51,760 --> 00:32:59,279
in the background c group

00:32:56,399 --> 00:32:59,679
because this one is the session is the

00:32:59,279 --> 00:33:02,320
shell

00:32:59,679 --> 00:33:02,880
session where the memory stress test

00:33:02,320 --> 00:33:06,559
will run

00:33:02,880 --> 00:33:09,760
the memory allocator will run and

00:33:06,559 --> 00:33:13,039
this guy down here that is going to be

00:33:09,760 --> 00:33:16,240
the foreground task or the

00:33:13,039 --> 00:33:19,600
latency sensitive task that

00:33:16,240 --> 00:33:19,600
will be moved in

00:33:20,399 --> 00:33:26,799
in the foreground c group

00:33:24,480 --> 00:33:28,640
perfect just to make sure i'm running

00:33:26,799 --> 00:33:31,519
everything in the

00:33:28,640 --> 00:33:34,559
correct c groups right this one is

00:33:31,519 --> 00:33:34,559
running in vg

00:33:35,039 --> 00:33:41,919
and this one is running in fg

00:33:38,720 --> 00:33:47,039
fine now i'm going to restart two

00:33:41,919 --> 00:33:50,559
different benchmarks memory stress test

00:33:47,039 --> 00:33:54,799
the latency sensitive

00:33:50,559 --> 00:33:58,080
test and i'm gonna start omr cpud again

00:33:54,799 --> 00:34:00,480
but this time i'm going to plain memory

00:33:58,080 --> 00:34:04,399
only from the background c group

00:34:00,480 --> 00:34:07,039
so uh what i should see

00:34:04,399 --> 00:34:09,280
uh is that memory will still be

00:34:07,039 --> 00:34:12,480
reclaimed

00:34:09,280 --> 00:34:14,560
but this time i shouldn't see

00:34:12,480 --> 00:34:16,079
theoretically i shouldn't see any extra

00:34:14,560 --> 00:34:19,520
latency here

00:34:16,079 --> 00:34:23,200
so i won't affect performance at all

00:34:19,520 --> 00:34:25,760
from the latency sensitive task

00:34:23,200 --> 00:34:27,359
and because i'm all right memory reclaim

00:34:25,760 --> 00:34:30,960
has been triggered because this

00:34:27,359 --> 00:34:33,839
this time i will only reclaim memory

00:34:30,960 --> 00:34:34,399
from the background c group so i won't

00:34:33,839 --> 00:34:36,639
affect

00:34:34,399 --> 00:34:38,000
performance of this guy that's running

00:34:36,639 --> 00:34:41,359
here

00:34:38,000 --> 00:34:44,480
all right memory is reclaimed

00:34:41,359 --> 00:34:47,760
so far so good i don't see any extra

00:34:44,480 --> 00:34:51,040
latency down here and if you remember in

00:34:47,760 --> 00:34:54,399
the previous case we had we already

00:34:51,040 --> 00:34:58,320
uh we had already seen some some

00:34:54,399 --> 00:35:01,760
spikes down here let's wait

00:34:58,320 --> 00:35:03,920
till the end since uh when all the

00:35:01,760 --> 00:35:06,720
memory will be reclaimed

00:35:03,920 --> 00:35:07,520
uh now we are i think we are approaching

00:35:06,720 --> 00:35:09,920
to

00:35:07,520 --> 00:35:10,640
almost all the memory being reclaimed

00:35:09,920 --> 00:35:14,320
but but

00:35:10,640 --> 00:35:17,520
as we can see this task doesn't notice

00:35:14,320 --> 00:35:21,040
any impact on performance

00:35:17,520 --> 00:35:23,839
so if you recall the uh

00:35:21,040 --> 00:35:27,359
example that i mentioned and almost at

00:35:23,839 --> 00:35:27,359
the beginning of the presentation

00:35:29,280 --> 00:35:34,400
yeah the fact that we have a like a

00:35:32,480 --> 00:35:37,119
latency sensitive application that could

00:35:34,400 --> 00:35:38,000
be like a video game played on a mobile

00:35:37,119 --> 00:35:40,480
phone

00:35:38,000 --> 00:35:43,040
we can see that performance are not

00:35:40,480 --> 00:35:45,839
affected at all

00:35:43,040 --> 00:35:45,839
uh

00:35:46,720 --> 00:35:50,160
yeah all right we have proclaimed all

00:35:48,560 --> 00:35:54,640
the memory we are

00:35:50,160 --> 00:36:00,400
getting more memories reclaiming but

00:35:54,640 --> 00:36:00,400
uh yeah performance are not affected

00:36:05,280 --> 00:36:08,240
all right so

00:36:09,280 --> 00:36:16,400
let's let's go back to the slides

00:36:12,800 --> 00:36:17,520
conclusion the opportunistic memory

00:36:16,400 --> 00:36:19,920
reclaim can

00:36:17,520 --> 00:36:21,040
definitely help to speed up hibernation

00:36:19,920 --> 00:36:24,240
resume time

00:36:21,040 --> 00:36:27,680
as we have seen but

00:36:24,240 --> 00:36:30,960
hibernation is not the only scenario

00:36:27,680 --> 00:36:34,079
where this feature can be helpful uh

00:36:30,960 --> 00:36:34,880
being able to trigger memory claim in

00:36:34,079 --> 00:36:38,320
advance

00:36:34,880 --> 00:36:40,960
from user space can provide benefits to

00:36:38,320 --> 00:36:42,720
the scenario like that we have seen in

00:36:40,960 --> 00:36:45,280
the last example

00:36:42,720 --> 00:36:46,640
like if we want to improve system

00:36:45,280 --> 00:36:48,960
responsive

00:36:46,640 --> 00:36:50,880
during larger location bars for example

00:36:48,960 --> 00:36:51,920
if you want to prepare the system to be

00:36:50,880 --> 00:36:54,720
able to handle

00:36:51,920 --> 00:36:55,839
large allocations or if we want to

00:36:54,720 --> 00:36:59,119
prioritize

00:36:55,839 --> 00:37:00,000
responsiveness of certain latency

00:36:59,119 --> 00:37:03,200
sensitive

00:37:00,000 --> 00:37:05,200
applications versus latency insensitive

00:37:03,200 --> 00:37:07,839
applications

00:37:05,200 --> 00:37:09,040
or even if we want to reduce the overall

00:37:07,839 --> 00:37:11,520
memory footprint

00:37:09,040 --> 00:37:14,560
in a system like there are cases where

00:37:11,520 --> 00:37:18,079
memory can be really expensive and

00:37:14,560 --> 00:37:19,280
being able to trigger memory reclaiming

00:37:18,079 --> 00:37:23,359
from user space

00:37:19,280 --> 00:37:26,800
can help to reduce the memory footprint

00:37:23,359 --> 00:37:26,800
when if it's needed

00:37:27,119 --> 00:37:33,440
now future work so the overall idea

00:37:30,320 --> 00:37:35,760
is still work in progress

00:37:33,440 --> 00:37:38,720
in particular we still need to figure

00:37:35,760 --> 00:37:39,839
out the ideal api that the kernel should

00:37:38,720 --> 00:37:43,200
provide

00:37:39,839 --> 00:37:47,040
to use the space to make this feature as

00:37:43,200 --> 00:37:48,240
generic as possible and so that it could

00:37:47,040 --> 00:37:51,119
benefit

00:37:48,240 --> 00:37:53,200
many different contexts and and not only

00:37:51,119 --> 00:37:55,839
hibernation of course

00:37:53,200 --> 00:37:56,960
i know that google is working on a

00:37:55,839 --> 00:38:00,160
similar solution

00:37:56,960 --> 00:38:01,760
they are experimenting similar proactive

00:38:00,160 --> 00:38:07,119
memory reclaiming

00:38:01,760 --> 00:38:10,480
uh uh still based on memory c groups

00:38:07,119 --> 00:38:13,040
and there's also uh

00:38:10,480 --> 00:38:14,000
one important thing to mention and is

00:38:13,040 --> 00:38:16,320
the fact that

00:38:14,000 --> 00:38:17,200
even with the mainline kernel as it is

00:38:16,320 --> 00:38:21,280
right now

00:38:17,200 --> 00:38:25,040
it would be possible to trigger

00:38:21,280 --> 00:38:28,079
an opportunistic memory reclaiming

00:38:25,040 --> 00:38:28,960
specifically there's a file in the

00:38:28,079 --> 00:38:32,160
memory c group

00:38:28,960 --> 00:38:35,599
fs in c group v2

00:38:32,160 --> 00:38:36,480
that is called memory.hi and basically

00:38:35,599 --> 00:38:39,440
what you can do

00:38:36,480 --> 00:38:40,240
you can you can set a limit into this

00:38:39,440 --> 00:38:43,040
file

00:38:40,240 --> 00:38:45,040
that will represent the maximum

00:38:43,040 --> 00:38:46,640
threshold of allocated memory for a

00:38:45,040 --> 00:38:50,160
specific c group

00:38:46,640 --> 00:38:53,280
so theoretically setting a very small

00:38:50,160 --> 00:38:56,400
value into this file would force the

00:38:53,280 --> 00:38:58,960
kernel to proclaim memory

00:38:56,400 --> 00:38:59,520
however the downside of this approach is

00:38:58,960 --> 00:39:02,880
that

00:38:59,520 --> 00:39:04,160
we need to react faster and increase the

00:39:02,880 --> 00:39:07,520
limit

00:39:04,160 --> 00:39:10,400
soon enough or otherwise we may risk to

00:39:07,520 --> 00:39:11,920
charge the entire c group or even worse

00:39:10,400 --> 00:39:14,640
the entire system

00:39:11,920 --> 00:39:16,800
if we don't respond fast at

00:39:14,640 --> 00:39:18,560
re-increasing the limit once the memory

00:39:16,800 --> 00:39:22,079
has been reclaimed

00:39:18,560 --> 00:39:25,440
so the advantage in my opinion

00:39:22,079 --> 00:39:28,640
or the advantage of the single

00:39:25,440 --> 00:39:32,160
shot memory reclaiming is that

00:39:28,640 --> 00:39:35,440
there's really no way to

00:39:32,160 --> 00:39:38,640
badly affect or matter to affect

00:39:35,440 --> 00:39:42,480
performance in a two bed

00:39:38,640 --> 00:39:46,240
like if the system is starting to

00:39:42,480 --> 00:39:47,200
re-fold and and request memory that have

00:39:46,240 --> 00:39:50,000
been

00:39:47,200 --> 00:39:52,000
memory that has been reclaimed basically

00:39:50,000 --> 00:39:54,320
it's just a one-shot reclaim so

00:39:52,000 --> 00:39:55,680
system can refault and reload memory

00:39:54,320 --> 00:39:58,320
from swap

00:39:55,680 --> 00:40:00,160
or or in the page cache from the

00:39:58,320 --> 00:40:02,640
corresponding files

00:40:00,160 --> 00:40:03,920
and we can decide from user space if we

00:40:02,640 --> 00:40:06,160
want to

00:40:03,920 --> 00:40:08,000
retry again to reclaim memory or if we

00:40:06,160 --> 00:40:11,040
just need to give up

00:40:08,000 --> 00:40:13,520
and based on certain

00:40:11,040 --> 00:40:15,359
system statistics like the idle time

00:40:13,520 --> 00:40:19,599
that i was using or we can you

00:40:15,359 --> 00:40:19,599
either even use other statistics

00:40:20,160 --> 00:40:27,920
so i also uh added a

00:40:24,079 --> 00:40:31,359
few references here if you want to

00:40:27,920 --> 00:40:33,040
learn more about this topic i guess the

00:40:31,359 --> 00:40:36,079
slides will be

00:40:33,040 --> 00:40:36,960
available somewhere uh if not you can

00:40:36,079 --> 00:40:42,079
reach out to me

00:40:36,960 --> 00:40:42,079
and ask for any information or questions

00:40:42,160 --> 00:40:45,599
i think that's it so thank you for

00:40:44,480 --> 00:40:49,839
listening

00:40:45,599 --> 00:40:49,839
if you have any question feel free to

00:40:50,839 --> 00:40:53,839
ask

00:40:58,720 --> 00:41:00,800

YouTube URL: https://www.youtube.com/watch?v=ariyHxpYw8E


