Title: Being Fluentd with Logs - Phil Wilkins, Capgemini UK
Publication date: 2020-10-30
Playlist: Open Source Summit & Embedded Linux Conference Europe 2020
Description: 
	Being Fluentd with Logs - Phil Wilkins, Capgemini UK
Captions: 
	00:00:06,160 --> 00:00:08,880
hi

00:00:06,480 --> 00:00:10,480
my name is i'm a tech evangelist an

00:00:08,880 --> 00:00:13,599
oracle ace director

00:00:10,480 --> 00:00:16,960
uh i'm going to be talking about fluentd

00:00:13,599 --> 00:00:18,960
today and it's uh work with logs on how

00:00:16,960 --> 00:00:20,320
we can use it to make our lives a lot

00:00:18,960 --> 00:00:21,920
easier uh

00:00:20,320 --> 00:00:23,680
in the operations and development

00:00:21,920 --> 00:00:26,320
environments

00:00:23,680 --> 00:00:27,680
so let's introduce myself properly first

00:00:26,320 --> 00:00:31,199
um

00:00:27,680 --> 00:00:34,559
in five quick bullets i'm a

00:00:31,199 --> 00:00:38,880
father i'm a husband

00:00:34,559 --> 00:00:38,880
uh a blogger and an author

00:00:39,280 --> 00:00:42,719
i also happen to help run a developer

00:00:42,079 --> 00:00:46,239
meet up

00:00:42,719 --> 00:00:48,800
here in london and

00:00:46,239 --> 00:00:51,120
currently working on a book called

00:00:48,800 --> 00:00:53,600
unified login with fluentd

00:00:51,120 --> 00:00:54,640
and it's in the early access program at

00:00:53,600 --> 00:00:57,680
the moment so about

00:00:54,640 --> 00:00:58,160
halfway through and you can read a lot

00:00:57,680 --> 00:01:02,079
about

00:00:58,160 --> 00:01:04,159
uh the use and configuration of fluentd

00:01:02,079 --> 00:01:06,320
and why we might want to use fluentd in

00:01:04,159 --> 00:01:08,560
different scenarios

00:01:06,320 --> 00:01:10,240
uh i work as i say i work for capgemini

00:01:08,560 --> 00:01:14,080
and i'm fortunate enough to work

00:01:10,240 --> 00:01:16,320
with a very successful uk team

00:01:14,080 --> 00:01:18,479
and we've won a number of rewards over

00:01:16,320 --> 00:01:22,000
the last couple of years

00:01:18,479 --> 00:01:22,000
so that's us

00:01:22,640 --> 00:01:26,400
but before i get into into the nuts and

00:01:25,360 --> 00:01:29,840
bolts of it

00:01:26,400 --> 00:01:30,720
it's worth looking at uh looking at

00:01:29,840 --> 00:01:34,799
monitoring

00:01:30,720 --> 00:01:36,560
and putting it into context uh so

00:01:34,799 --> 00:01:38,560
the idea of monitoring is actually if

00:01:36,560 --> 00:01:41,040
you use the that go back to the english

00:01:38,560 --> 00:01:42,960
language definitions is to observe

00:01:41,040 --> 00:01:45,439
and see what's happening over a period

00:01:42,960 --> 00:01:48,720
at a time

00:01:45,439 --> 00:01:51,159
and and see what's happening and uh

00:01:48,720 --> 00:01:52,479
that's led to some ideas such as

00:01:51,159 --> 00:01:55,600
observability

00:01:52,479 --> 00:01:58,719
and the three pillars of observability

00:01:55,600 --> 00:02:01,759
which is quite a useful vehicle

00:01:58,719 --> 00:02:03,360
it's important because we all work from

00:02:01,759 --> 00:02:05,759
different perspectives

00:02:03,360 --> 00:02:07,759
and depending on your viewpoint and your

00:02:05,759 --> 00:02:10,080
role in an organization

00:02:07,759 --> 00:02:11,520
you may well i think logging or

00:02:10,080 --> 00:02:14,640
monitoring it

00:02:11,520 --> 00:02:16,720
is very different if you're a

00:02:14,640 --> 00:02:18,560
an infrastructure expert to when you're

00:02:16,720 --> 00:02:20,840
an apps or

00:02:18,560 --> 00:02:22,879
if you're working from a local

00:02:20,840 --> 00:02:26,080
perspective or

00:02:22,879 --> 00:02:28,720
into business solutions and

00:02:26,080 --> 00:02:29,440
bpm and things like that so let's have a

00:02:28,720 --> 00:02:32,319
quick look

00:02:29,440 --> 00:02:33,760
at the stack and the three layers of

00:02:32,319 --> 00:02:36,080
observability

00:02:33,760 --> 00:02:38,239
so one way of observing things is to

00:02:36,080 --> 00:02:38,879
look at it from a numerical perspective

00:02:38,239 --> 00:02:41,440
this is

00:02:38,879 --> 00:02:42,720
capturing your cpu usage this is kept in

00:02:41,440 --> 00:02:45,920
your memory

00:02:42,720 --> 00:02:50,319
uh resources and

00:02:45,920 --> 00:02:53,360
very numeric statistical perspective

00:02:50,319 --> 00:02:56,239
the next way we can look at um

00:02:53,360 --> 00:02:58,319
this monitoring capabilities uh is

00:02:56,239 --> 00:03:00,640
around the idea of logs

00:02:58,319 --> 00:03:02,400
and this is where fluent is really at

00:03:00,640 --> 00:03:04,879
its strength but we'll come back to that

00:03:02,400 --> 00:03:07,280
in a little while

00:03:04,879 --> 00:03:09,599
but you know logs are equally important

00:03:07,280 --> 00:03:11,840
as those statistical facts

00:03:09,599 --> 00:03:14,159
uh when you have a problem with your

00:03:11,840 --> 00:03:15,280
server it's likely to put in information

00:03:14,159 --> 00:03:18,879
into an snmp

00:03:15,280 --> 00:03:22,000
trap or using another mechanism

00:03:18,879 --> 00:03:23,760
to report the nature of the issues that

00:03:22,000 --> 00:03:25,920
you're experiencing

00:03:23,760 --> 00:03:28,080
uh you know hardware fault and things

00:03:25,920 --> 00:03:30,799
like that

00:03:28,080 --> 00:03:31,120
and of course in applications most of us

00:03:30,799 --> 00:03:34,159
that

00:03:31,120 --> 00:03:36,959
work as developers actually use logs

00:03:34,159 --> 00:03:38,959
as a bread and butter and we use it for

00:03:36,959 --> 00:03:41,040
a number of different reasons from

00:03:38,959 --> 00:03:42,319
helping us understand and making sure

00:03:41,040 --> 00:03:45,280
the software develop

00:03:42,319 --> 00:03:46,640
we're developing is running and behaving

00:03:45,280 --> 00:03:51,280
as expected

00:03:46,640 --> 00:03:53,760
through to creating audit trail events

00:03:51,280 --> 00:03:56,080
and information that meets security

00:03:53,760 --> 00:03:59,680
requirements

00:03:56,080 --> 00:04:03,200
and tracking user activities

00:03:59,680 --> 00:04:06,319
to help address siem requirements

00:04:03,200 --> 00:04:09,280
and then you've got the the latest

00:04:06,319 --> 00:04:11,519
evolution which is around the

00:04:09,280 --> 00:04:14,239
application of tracing

00:04:11,519 --> 00:04:14,799
where you're wanting to see how events

00:04:14,239 --> 00:04:18,479
start

00:04:14,799 --> 00:04:20,160
on one node in a distributed environment

00:04:18,479 --> 00:04:21,840
and they will track through different

00:04:20,160 --> 00:04:24,639
applications and and

00:04:21,840 --> 00:04:27,040
components such as various microservices

00:04:24,639 --> 00:04:31,280
that you might be running in kubernetes

00:04:27,040 --> 00:04:34,800
through a kafka stream perhaps

00:04:31,280 --> 00:04:37,199
through your api gateways in and out

00:04:34,800 --> 00:04:39,120
and seeing where the delays are where

00:04:37,199 --> 00:04:42,240
the performance issues are

00:04:39,120 --> 00:04:46,240
and details like that and that's

00:04:42,240 --> 00:04:48,320
tracing for you and then the best way to

00:04:46,240 --> 00:04:49,520
examine that is as i mentioned some of

00:04:48,320 --> 00:04:52,160
the examples

00:04:49,520 --> 00:04:53,840
but um let's look you know hosting in

00:04:52,160 --> 00:04:56,560
the infrastructure level monitoring

00:04:53,840 --> 00:04:59,199
activities tend to be uh

00:04:56,560 --> 00:05:01,600
heavily focused on the metrics side of

00:04:59,199 --> 00:05:05,360
things with a little bit of

00:05:01,600 --> 00:05:07,520
the the log characteristics and

00:05:05,360 --> 00:05:09,120
practically no real tracing on the

00:05:07,520 --> 00:05:11,440
infrastructure layer

00:05:09,120 --> 00:05:13,680
and then when we get to virtualization

00:05:11,440 --> 00:05:16,240
uh the vms or you can turn them

00:05:13,680 --> 00:05:19,759
frameworks such as kubernetes

00:05:16,240 --> 00:05:21,919
um it's again still fairly strong

00:05:19,759 --> 00:05:24,080
on the numerical side of things but logs

00:05:21,919 --> 00:05:26,000
become a lot more prevalent

00:05:24,080 --> 00:05:27,520
and we start to see a little bit of

00:05:26,000 --> 00:05:29,680
tracing coming in

00:05:27,520 --> 00:05:30,639
and we move up stack a bit further and

00:05:29,680 --> 00:05:34,240
more into the

00:05:30,639 --> 00:05:37,120
application space uh your tracing is is

00:05:34,240 --> 00:05:38,000
probably just as important now as your

00:05:37,120 --> 00:05:40,639
logs

00:05:38,000 --> 00:05:41,440
and actually the stats drop off a little

00:05:40,639 --> 00:05:43,199
bit

00:05:41,440 --> 00:05:45,199
and they change in nature because you're

00:05:43,199 --> 00:05:47,919
more likely to be interested in

00:05:45,199 --> 00:05:50,080
the stats from your virtual machine uh

00:05:47,919 --> 00:05:50,560
and making sure that your application is

00:05:50,080 --> 00:05:53,919
tuned

00:05:50,560 --> 00:05:57,600
correctly rather than perhaps raw

00:05:53,919 --> 00:05:57,600
pure cpu issues

00:05:58,319 --> 00:06:03,360
we can then look at uh above that in

00:06:00,880 --> 00:06:06,080
into the the business uh

00:06:03,360 --> 00:06:07,840
application monitoring so that's looking

00:06:06,080 --> 00:06:10,000
at how many transactions

00:06:07,840 --> 00:06:11,600
you've completed from a business

00:06:10,000 --> 00:06:14,639
perspective so that's you

00:06:11,600 --> 00:06:15,919
from uh purchase to to completion to

00:06:14,639 --> 00:06:18,720
fulfillment

00:06:15,919 --> 00:06:19,360
uh and processes like that and of course

00:06:18,720 --> 00:06:22,960
then

00:06:19,360 --> 00:06:25,520
on top of that we have security

00:06:22,960 --> 00:06:26,160
uh considerations and securities looking

00:06:25,520 --> 00:06:28,000
at uh

00:06:26,160 --> 00:06:29,520
a lot of log things that you know what's

00:06:28,000 --> 00:06:31,280
a user done when

00:06:29,520 --> 00:06:34,080
when did they sign in when did they sign

00:06:31,280 --> 00:06:37,280
out how often are they interacting

00:06:34,080 --> 00:06:39,600
uh how often do they fail to

00:06:37,280 --> 00:06:40,639
get the credentials right and things

00:06:39,600 --> 00:06:41,919
like that

00:06:40,639 --> 00:06:43,840
and then on the other side of the

00:06:41,919 --> 00:06:46,720
spectrum you're trying to capture the

00:06:43,840 --> 00:06:49,280
capacity in sites

00:06:46,720 --> 00:06:50,560
so that you can scale and forecast

00:06:49,280 --> 00:06:53,680
demand

00:06:50,560 --> 00:06:56,720
do uh charging potentially if you're

00:06:53,680 --> 00:06:57,840
sharing the costs of running a back end

00:06:56,720 --> 00:07:03,599
amongst different

00:06:57,840 --> 00:07:06,319
teams and things like that

00:07:03,599 --> 00:07:07,680
so let's have a look at application logs

00:07:06,319 --> 00:07:10,000
more specifically

00:07:07,680 --> 00:07:10,800
uh why do we use them well we're looking

00:07:10,000 --> 00:07:14,479
for uh

00:07:10,800 --> 00:07:16,400
unexpected errors as mentioned

00:07:14,479 --> 00:07:18,720
when when something doesn't behave as we

00:07:16,400 --> 00:07:20,639
expected one of the things that

00:07:18,720 --> 00:07:23,039
will tell us what's going on or why it's

00:07:20,639 --> 00:07:24,240
happened is obviously our application

00:07:23,039 --> 00:07:27,520
log

00:07:24,240 --> 00:07:31,360
um we want to look at things like

00:07:27,520 --> 00:07:33,759
uh performance issues uh

00:07:31,360 --> 00:07:36,080
classical uh measures for that things

00:07:33,759 --> 00:07:38,720
like uh looking at your database or

00:07:36,080 --> 00:07:41,120
uh your storage mechanism and looking at

00:07:38,720 --> 00:07:43,759
things like slow query logs and

00:07:41,120 --> 00:07:44,479
uh looking at the query performance and

00:07:43,759 --> 00:07:47,759
pulling that

00:07:44,479 --> 00:07:50,000
out and

00:07:47,759 --> 00:07:51,280
then of course as i mentioned that we've

00:07:50,000 --> 00:07:54,400
got

00:07:51,280 --> 00:07:55,360
gdpr compliance and all sorts of other

00:07:54,400 --> 00:07:57,039
legislative

00:07:55,360 --> 00:07:59,120
requirements these days that mean that

00:07:57,039 --> 00:08:02,720
we have to create audit trails

00:07:59,120 --> 00:08:04,960
and record what users are doing so that

00:08:02,720 --> 00:08:07,520
in the event of an investigation or

00:08:04,960 --> 00:08:09,120
something suspect happening

00:08:07,520 --> 00:08:11,840
or you're questioning their users

00:08:09,120 --> 00:08:13,680
actions for some reason

00:08:11,840 --> 00:08:15,680
then you can actually go back through

00:08:13,680 --> 00:08:18,400
and understand what they've done

00:08:15,680 --> 00:08:20,879
when they did it even potentially why

00:08:18,400 --> 00:08:20,879
they did it

00:08:21,039 --> 00:08:25,039
but the bottom line is in all of this

00:08:23,520 --> 00:08:29,680
it's always about feeding

00:08:25,039 --> 00:08:31,759
the business need so that

00:08:29,680 --> 00:08:33,919
we can understand the value that we're

00:08:31,759 --> 00:08:37,039
bringing to the business and succeeding

00:08:33,919 --> 00:08:38,320
in resolving uh business problems or

00:08:37,039 --> 00:08:41,279
confirming the business

00:08:38,320 --> 00:08:43,360
that is running uh smoothly and

00:08:41,279 --> 00:08:45,200
everything's behaving as expected

00:08:43,360 --> 00:08:47,040
and we're not losing data and things

00:08:45,200 --> 00:08:47,920
like that so if anybody raises a

00:08:47,040 --> 00:08:51,440
question

00:08:47,920 --> 00:08:53,760
you got the evidence to support it

00:08:51,440 --> 00:08:56,080
but it's all about the impact if if

00:08:53,760 --> 00:08:57,760
there's no clear business connection to

00:08:56,080 --> 00:08:59,839
to what we're monitoring

00:08:57,760 --> 00:09:01,920
then uh sooner or later we're going to

00:08:59,839 --> 00:09:03,120
get the activity to develop and

00:09:01,920 --> 00:09:06,240
monitoring

00:09:03,120 --> 00:09:08,000
um shut down so you know that could be

00:09:06,240 --> 00:09:11,120
simply monitoring to

00:09:08,000 --> 00:09:11,680
optimize performance and make sure that

00:09:11,120 --> 00:09:15,200
we're

00:09:11,680 --> 00:09:16,720
running our environment efficiently or

00:09:15,200 --> 00:09:18,720
at the other end of the spectrum

00:09:16,720 --> 00:09:22,000
monitoring to make sure that nobody's

00:09:18,720 --> 00:09:25,040
abusing the system

00:09:22,000 --> 00:09:25,920
um one of the things that we've seen

00:09:25,040 --> 00:09:29,040
over the years

00:09:25,920 --> 00:09:31,279
is changing complexity and that's

00:09:29,040 --> 00:09:33,440
impacted our ability to monitor whether

00:09:31,279 --> 00:09:35,640
it's through logs or through metrics

00:09:33,440 --> 00:09:37,200
but it's worth just understanding how

00:09:35,640 --> 00:09:40,560
significant the

00:09:37,200 --> 00:09:44,880
complexities evolved so when

00:09:40,560 --> 00:09:46,959
we uh started the it industry if you

00:09:44,880 --> 00:09:48,720
like back in the 50s and 60s

00:09:46,959 --> 00:09:51,839
we didn't have to deal with concurrency

00:09:48,720 --> 00:09:55,120
we didn't have issues of distribution

00:09:51,839 --> 00:09:57,440
um scaling was

00:09:55,120 --> 00:09:59,279
whatever the computer could do if you

00:09:57,440 --> 00:10:01,680
could buy a bigger computer then

00:09:59,279 --> 00:10:04,320
maybe you could scale and do a bit more

00:10:01,680 --> 00:10:07,200
but that was the limitations

00:10:04,320 --> 00:10:08,959
and then as if times moved on we've got

00:10:07,200 --> 00:10:12,800
into multi-threaded in

00:10:08,959 --> 00:10:15,040
a single cpu or within a single server

00:10:12,800 --> 00:10:18,640
multiple cpus on a single server

00:10:15,040 --> 00:10:21,839
as you'd see in a

00:10:18,640 --> 00:10:25,360
large enterprise uh infrastructure

00:10:21,839 --> 00:10:28,480
and mainframe computing and

00:10:25,360 --> 00:10:33,600
with that we've seen uh things like

00:10:28,480 --> 00:10:37,279
tomcat and servant-based applications

00:10:33,600 --> 00:10:39,040
uh some sewer platforms such as uh the

00:10:37,279 --> 00:10:42,000
oracle service bus

00:10:39,040 --> 00:10:43,120
amongst others coming in but it's

00:10:42,000 --> 00:10:46,880
largely

00:10:43,120 --> 00:10:49,360
uh focused on a single cpu

00:10:46,880 --> 00:10:51,440
with threads and then over time we've

00:10:49,360 --> 00:10:53,600
actually scaled that out

00:10:51,440 --> 00:10:55,040
uh and started to distribute the

00:10:53,600 --> 00:10:56,640
workload but typically when you

00:10:55,040 --> 00:10:58,959
distribute the workload

00:10:56,640 --> 00:11:01,120
an end-to-end process still remains on

00:10:58,959 --> 00:11:02,800
one server

00:11:01,120 --> 00:11:04,640
you don't start jumping around the

00:11:02,800 --> 00:11:08,079
servers too much

00:11:04,640 --> 00:11:11,360
uh unless you're calling a discrete

00:11:08,079 --> 00:11:13,760
component like a database in which case

00:11:11,360 --> 00:11:16,959
that might be hosted on a different a

00:11:13,760 --> 00:11:19,600
piece of infrastructure but you're still

00:11:16,959 --> 00:11:21,120
fairly easy to track what's going on

00:11:19,600 --> 00:11:25,279
it's still happening within

00:11:21,120 --> 00:11:28,320
one environment normally then as we've

00:11:25,279 --> 00:11:30,240
evolved again we've driven up the level

00:11:28,320 --> 00:11:33,839
of asynchronous behavior

00:11:30,240 --> 00:11:35,600
and we've got a look at node.js is a

00:11:33,839 --> 00:11:37,600
great example of that

00:11:35,600 --> 00:11:39,360
where we've moved away from a threading

00:11:37,600 --> 00:11:42,800
model um

00:11:39,360 --> 00:11:44,640
to more of a

00:11:42,800 --> 00:11:47,519
single threaded but we're picking up and

00:11:44,640 --> 00:11:50,720
putting down workloads based on the io

00:11:47,519 --> 00:11:54,639
requirements and things like that and

00:11:50,720 --> 00:11:56,880
asynchronous behavior kafka is another

00:11:54,639 --> 00:11:59,440
example of that kind of thing

00:11:56,880 --> 00:12:01,440
and then of course we're into all our

00:11:59,440 --> 00:12:04,720
distributed

00:12:01,440 --> 00:12:06,800
mechanisms as well where we've developed

00:12:04,720 --> 00:12:08,800
scale out

00:12:06,800 --> 00:12:10,639
and we can spin up particularly in the

00:12:08,800 --> 00:12:14,639
advent of

00:12:10,639 --> 00:12:18,639
of cloud and greater elasticity in our

00:12:14,639 --> 00:12:20,800
operations we can spin up new servers

00:12:18,639 --> 00:12:23,279
at the top of the hat to scale out part

00:12:20,800 --> 00:12:26,000
of our application space

00:12:23,279 --> 00:12:26,959
and as a result combine that with

00:12:26,000 --> 00:12:29,279
asynchronous

00:12:26,959 --> 00:12:30,639
and jobs can suddenly bounce around

00:12:29,279 --> 00:12:33,360
different servers

00:12:30,639 --> 00:12:35,680
as they process through their life

00:12:33,360 --> 00:12:39,200
within our application

00:12:35,680 --> 00:12:41,440
so the complexity just escalated

00:12:39,200 --> 00:12:43,360
phenomenally and we've got to respond to

00:12:41,440 --> 00:12:44,560
that with our ability to measure and

00:12:43,360 --> 00:12:46,880
monitor

00:12:44,560 --> 00:12:48,320
um and and then there are a number of

00:12:46,880 --> 00:12:50,720
techniques that we've put

00:12:48,320 --> 00:12:52,399
have been brought to bear you know 20

00:12:50,720 --> 00:12:54,639
years ago we might have just looked at

00:12:52,399 --> 00:12:57,120
one log file now we have many

00:12:54,639 --> 00:12:59,920
many sources of logs as well and it's

00:12:57,120 --> 00:12:59,920
distributed

00:13:00,639 --> 00:13:08,560
so fluently is a a tool that can help us

00:13:05,200 --> 00:13:11,360
address that so let's introduce fluentd

00:13:08,560 --> 00:13:12,800
um it's not the most well known of

00:13:11,360 --> 00:13:15,680
products out there but

00:13:12,800 --> 00:13:18,079
uh it is certainly amongst those that

00:13:15,680 --> 00:13:22,399
are interested and involved in

00:13:18,079 --> 00:13:25,519
in devops and uh operational activities

00:13:22,399 --> 00:13:28,480
um something that people are becoming

00:13:25,519 --> 00:13:29,360
increasingly aware of its legacy we

00:13:28,480 --> 00:13:30,880
actually was a

00:13:29,360 --> 00:13:33,360
in big data with a company called

00:13:30,880 --> 00:13:37,120
treasure data who open source debt

00:13:33,360 --> 00:13:39,040
and then over time it's become

00:13:37,120 --> 00:13:40,160
an open source project and under the

00:13:39,040 --> 00:13:42,560
governance of the

00:13:40,160 --> 00:13:44,880
cloud native computing foundation or

00:13:42,560 --> 00:13:47,920
cncf

00:13:44,880 --> 00:13:49,519
and that's really when it started to get

00:13:47,920 --> 00:13:52,560
a lot of traction

00:13:49,519 --> 00:13:54,320
uh in many respects because it's got

00:13:52,560 --> 00:13:56,720
that open governance

00:13:54,320 --> 00:13:57,920
uh it's become part of the the

00:13:56,720 --> 00:14:01,360
kubernetes

00:13:57,920 --> 00:14:04,560
ecosystem and it's

00:14:01,360 --> 00:14:06,880
vendor neutral uh but it's

00:14:04,560 --> 00:14:08,880
highly pluggable there are other

00:14:06,880 --> 00:14:12,320
technologies in this space that uh

00:14:08,880 --> 00:14:14,720
people know we've heard of the elk stack

00:14:12,320 --> 00:14:17,920
which we'll come up into in a moment

00:14:14,720 --> 00:14:20,880
uh but fluentd is basically

00:14:17,920 --> 00:14:22,320
a a a very lightweight framework that

00:14:20,880 --> 00:14:25,600
actually just

00:14:22,320 --> 00:14:27,040
uses a lot of plugins so you can build

00:14:25,600 --> 00:14:29,839
your own plugins

00:14:27,040 --> 00:14:31,920
a lot of vendors have made their

00:14:29,839 --> 00:14:35,199
solutions compatible with

00:14:31,920 --> 00:14:38,639
fluency by building plugins so

00:14:35,199 --> 00:14:43,040
you can incorporate or feed

00:14:38,639 --> 00:14:46,240
your fluent the monitored environments

00:14:43,040 --> 00:14:49,839
to their tools so

00:14:46,240 --> 00:14:52,000
we can see from many directions

00:14:49,839 --> 00:14:54,800
capturing the data and feeding it to

00:14:52,000 --> 00:14:57,680
splunk as an industry leading product

00:14:54,800 --> 00:15:01,440
in the log aggregation analytics

00:14:57,680 --> 00:15:03,680
particularly for security use cases

00:15:01,440 --> 00:15:06,000
through to integrating with the cloud

00:15:03,680 --> 00:15:10,160
native solutions that

00:15:06,000 --> 00:15:12,720
amazon and google provide and oracle now

00:15:10,160 --> 00:15:14,959
as well with their latest offering in in

00:15:12,720 --> 00:15:16,959
this space

00:15:14,959 --> 00:15:18,160
and of course if you're building a

00:15:16,959 --> 00:15:21,440
bespoke

00:15:18,160 --> 00:15:23,199
solution you might want to build your

00:15:21,440 --> 00:15:27,199
own custom plugin

00:15:23,199 --> 00:15:29,360
either to inject log data into

00:15:27,199 --> 00:15:31,839
fluentd or to actually pull it out if

00:15:29,360 --> 00:15:34,000
you're trying to do some analysis

00:15:31,839 --> 00:15:36,880
but this highly plugable nature has made

00:15:34,000 --> 00:15:38,240
it very very powerful very very flexible

00:15:36,880 --> 00:15:41,040
and there's something in the order of

00:15:38,240 --> 00:15:44,079
500 plus plugins not all of them

00:15:41,040 --> 00:15:48,000
are out of the box fluently

00:15:44,079 --> 00:15:50,560
uh there is a very lively and vibrant uh

00:15:48,000 --> 00:15:53,519
open source community contributing

00:15:50,560 --> 00:15:54,800
uh quite a few of these in addition to

00:15:53,519 --> 00:15:58,160
the core

00:15:54,800 --> 00:16:01,120
uh governed uh uh part of

00:15:58,160 --> 00:16:01,120
uh nd

00:16:01,199 --> 00:16:04,480
so it means that the plugins allow us to

00:16:04,160 --> 00:16:08,000
uh

00:16:04,480 --> 00:16:10,560
do things like formatting filtering

00:16:08,000 --> 00:16:12,240
deal with different storage technologies

00:16:10,560 --> 00:16:16,000
from elasticsearch

00:16:12,240 --> 00:16:18,000
through to s3 buckets and many other

00:16:16,000 --> 00:16:20,560
things

00:16:18,000 --> 00:16:21,519
it allows us to build caching and using

00:16:20,560 --> 00:16:24,160
caching

00:16:21,519 --> 00:16:25,680
products as well as parsing the payload

00:16:24,160 --> 00:16:28,240
so we can start to extract

00:16:25,680 --> 00:16:29,680
meaning out of the log events that we

00:16:28,240 --> 00:16:32,240
capture

00:16:29,680 --> 00:16:34,399
and start to then apply rules about you

00:16:32,240 --> 00:16:34,959
know do we need to pass it on who needs

00:16:34,399 --> 00:16:37,920
it

00:16:34,959 --> 00:16:39,759
because some organizations as well see

00:16:37,920 --> 00:16:41,839
shortly

00:16:39,759 --> 00:16:43,360
we'll say actually the security team

00:16:41,839 --> 00:16:45,600
want to use splunk but

00:16:43,360 --> 00:16:47,360
uh perhaps the ops team want to use

00:16:45,600 --> 00:16:49,279
nagios

00:16:47,360 --> 00:16:51,199
and there will be an overlap of

00:16:49,279 --> 00:16:53,920
information so we need to route it to

00:16:51,199 --> 00:16:53,920
the right place

00:16:55,600 --> 00:17:00,320
we can actually look at the life cycle

00:16:57,600 --> 00:17:04,160
of logs

00:17:00,320 --> 00:17:05,760
and therefore what we expect

00:17:04,160 --> 00:17:08,079
and what we need from an overall

00:17:05,760 --> 00:17:11,839
monitoring solution

00:17:08,079 --> 00:17:13,039
and a log management and log unification

00:17:11,839 --> 00:17:16,400
mechanism

00:17:13,039 --> 00:17:19,280
um as a life cycle so

00:17:16,400 --> 00:17:21,600
we see one end of the spectrum the log

00:17:19,280 --> 00:17:23,600
events have been generated by

00:17:21,600 --> 00:17:26,640
vast numbers of different components

00:17:23,600 --> 00:17:29,760
from from kubernetes even

00:17:26,640 --> 00:17:32,240
from the server infrastructure from your

00:17:29,760 --> 00:17:35,840
applications from the business process

00:17:32,240 --> 00:17:37,520
layers and uh

00:17:35,840 --> 00:17:40,240
they all need to be captured and

00:17:37,520 --> 00:17:44,880
processed or

00:17:40,240 --> 00:17:48,080
determined on their relevance and need

00:17:44,880 --> 00:17:50,000
in terms of what to do with them so we

00:17:48,080 --> 00:17:53,520
need to take them in

00:17:50,000 --> 00:17:57,120
and ingest them dynamically

00:17:53,520 --> 00:17:59,679
and then uh evaluate what they are and

00:17:57,120 --> 00:18:01,760
uh root them potentially manipulating

00:17:59,679 --> 00:18:04,160
the structure of the the log events so

00:18:01,760 --> 00:18:05,280
that they can be consumed by the target

00:18:04,160 --> 00:18:08,799
product

00:18:05,280 --> 00:18:11,440
uh some systems just want the whole blob

00:18:08,799 --> 00:18:12,000
as text others want to structure json

00:18:11,440 --> 00:18:15,200
file

00:18:12,000 --> 00:18:18,320
a payload so they can process it

00:18:15,200 --> 00:18:18,320
in a particular way

00:18:19,039 --> 00:18:25,679
and the in that process as you uh

00:18:22,640 --> 00:18:26,799
feed that into a system you might want

00:18:25,679 --> 00:18:29,600
to

00:18:26,799 --> 00:18:31,679
either dynamically grab it and push an

00:18:29,600 --> 00:18:33,840
event out because it's a critical area

00:18:31,679 --> 00:18:35,520
that's been spotted within the stream

00:18:33,840 --> 00:18:37,200
of vlog events that you're you're

00:18:35,520 --> 00:18:39,280
capturing or

00:18:37,200 --> 00:18:40,559
actually you want to aggregate and look

00:18:39,280 --> 00:18:44,160
for patterns

00:18:40,559 --> 00:18:47,520
in activity and this is how

00:18:44,160 --> 00:18:49,600
a lot of the sa siem tools work

00:18:47,520 --> 00:18:50,640
uh where they're looking at user

00:18:49,600 --> 00:18:52,960
behavior

00:18:50,640 --> 00:18:53,919
over time to see if there's a number any

00:18:52,960 --> 00:18:57,600
anomalies in

00:18:53,919 --> 00:18:58,960
in the ways people are using systems

00:18:57,600 --> 00:19:00,799
and then of course we might want to

00:18:58,960 --> 00:19:03,520
visualize the data you know what's the

00:19:00,799 --> 00:19:07,440
demand profile look like visually

00:19:03,520 --> 00:19:10,559
what's our consumption when do we see

00:19:07,440 --> 00:19:13,200
problems occurring and performance

00:19:10,559 --> 00:19:13,760
and correlate that and visually show how

00:19:13,200 --> 00:19:16,320
that might

00:19:13,760 --> 00:19:18,000
relate to how other components in our

00:19:16,320 --> 00:19:20,240
infrastructure are working

00:19:18,000 --> 00:19:21,600
yeah does our application suddenly throw

00:19:20,240 --> 00:19:23,919
an error

00:19:21,600 --> 00:19:25,520
when the database server shows it's

00:19:23,919 --> 00:19:27,520
under some load

00:19:25,520 --> 00:19:28,559
uh well if you've got that visually

00:19:27,520 --> 00:19:31,360
represented it's

00:19:28,559 --> 00:19:32,880
easy to spot those correlations or you

00:19:31,360 --> 00:19:35,440
could incorporate ai

00:19:32,880 --> 00:19:37,360
and and some clever tricks to actually

00:19:35,440 --> 00:19:39,200
find those patterns for you

00:19:37,360 --> 00:19:40,400
and of course there's no point in doing

00:19:39,200 --> 00:19:42,480
all that analysis if

00:19:40,400 --> 00:19:44,640
if you're not going to then act upon it

00:19:42,480 --> 00:19:47,919
so we need to be able to notify people

00:19:44,640 --> 00:19:51,360
alert people to these things

00:19:47,919 --> 00:19:54,640
particularly time-sensitive

00:19:51,360 --> 00:19:56,559
notifications uh when it's

00:19:54,640 --> 00:19:58,080
operationally critical you know servers

00:19:56,559 --> 00:19:59,919
collapsed

00:19:58,080 --> 00:20:01,360
your database is suddenly starting to

00:19:59,919 --> 00:20:03,919
grind because it's

00:20:01,360 --> 00:20:06,480
running out of storage for some

00:20:03,919 --> 00:20:09,679
unexpected reason

00:20:06,480 --> 00:20:12,960
along with perhaps even generating uh

00:20:09,679 --> 00:20:15,440
juror tickets to say look you know

00:20:12,960 --> 00:20:17,280
this is a bug or this is an error that

00:20:15,440 --> 00:20:19,679
has cropped up uh

00:20:17,280 --> 00:20:21,440
so many times today it needs to be

00:20:19,679 --> 00:20:25,120
investigated because it's

00:20:21,440 --> 00:20:25,120
uh you know too frequent

00:20:28,720 --> 00:20:35,840
so i mentioned the alk stack or elastic

00:20:32,720 --> 00:20:37,520
log stash and uh kibana um

00:20:35,840 --> 00:20:40,320
it's probably the best known stack for

00:20:37,520 --> 00:20:42,240
monitoring um

00:20:40,320 --> 00:20:45,120
we have logstash at the bottom of the

00:20:42,240 --> 00:20:47,440
stack and it has a a baby

00:20:45,120 --> 00:20:48,720
uh brother if you like called beats

00:20:47,440 --> 00:20:52,799
which

00:20:48,720 --> 00:20:55,520
is designed to be ultralight footprint

00:20:52,799 --> 00:20:56,559
so that you can deploy in iot style

00:20:55,520 --> 00:21:00,799
solutions

00:20:56,559 --> 00:21:03,039
and highly distributed solutions

00:21:00,799 --> 00:21:04,799
and it has a limited capability compared

00:21:03,039 --> 00:21:06,880
to the full log stash

00:21:04,799 --> 00:21:08,240
then of course you've got the aggregated

00:21:06,880 --> 00:21:10,880
layer where uh

00:21:08,240 --> 00:21:12,159
you've taken all your of your log events

00:21:10,880 --> 00:21:14,640
uh and putting them into

00:21:12,159 --> 00:21:15,360
elasticsearch you so you can start doing

00:21:14,640 --> 00:21:18,799
analysis

00:21:15,360 --> 00:21:21,679
trend uh occurrences what's happened

00:21:18,799 --> 00:21:23,520
over time look at your overall history

00:21:21,679 --> 00:21:26,080
so it means that local servers do not

00:21:23,520 --> 00:21:28,240
have to retain logs for very long

00:21:26,080 --> 00:21:30,080
uh it gives you a central repository

00:21:28,240 --> 00:21:32,799
that people can then

00:21:30,080 --> 00:21:33,280
interrogate what's going on and of

00:21:32,799 --> 00:21:35,120
course

00:21:33,280 --> 00:21:36,400
as i say we need to visualize all of

00:21:35,120 --> 00:21:39,919
that and there's

00:21:36,400 --> 00:21:43,200
a kibana all of these come from elastic

00:21:39,919 --> 00:21:47,120
as a vendor

00:21:43,200 --> 00:21:50,640
and um there's a lot of commonality

00:21:47,120 --> 00:21:53,919
with fluentd in in this

00:21:50,640 --> 00:21:57,360
so let me bring in the efk stack

00:21:53,919 --> 00:21:58,799
efk is coming in because fluentd is

00:21:57,360 --> 00:22:02,880
starting to be seen as an

00:21:58,799 --> 00:22:06,480
alternative and potentially displacing

00:22:02,880 --> 00:22:09,120
uh the log stash

00:22:06,480 --> 00:22:10,080
um and that's been driven from a couple

00:22:09,120 --> 00:22:12,720
of dimensions

00:22:10,080 --> 00:22:13,280
first of all as i say fluentd has got a

00:22:12,720 --> 00:22:15,919
lot more

00:22:13,280 --> 00:22:16,559
richness in terms of plug-in uh that

00:22:15,919 --> 00:22:19,600
gives you a

00:22:16,559 --> 00:22:21,600
more facility in terms of

00:22:19,600 --> 00:22:24,400
things that you can capture natively

00:22:21,600 --> 00:22:28,240
without doing any work

00:22:24,400 --> 00:22:31,520
it is also a natural part of um

00:22:28,240 --> 00:22:32,400
kubernetes which means that uh it's

00:22:31,520 --> 00:22:34,480
becoming

00:22:32,400 --> 00:22:37,360
uh you know a first-class citizen in the

00:22:34,480 --> 00:22:37,360
infrastructure

00:22:37,600 --> 00:22:45,120
and it's a very powerful uh uh utility

00:22:41,120 --> 00:22:47,919
that has some flexibility around caching

00:22:45,120 --> 00:22:49,760
in ways that log stash is uh not quite

00:22:47,919 --> 00:22:52,880
so freeing

00:22:49,760 --> 00:22:54,720
but so people are looking at uh phone d

00:22:52,880 --> 00:22:56,640
rather than logsdash but keeping the

00:22:54,720 --> 00:22:57,760
elastic search and its analytical

00:22:56,640 --> 00:23:00,400
capabilities

00:22:57,760 --> 00:23:02,880
and the visualization of cabana and as a

00:23:00,400 --> 00:23:05,360
result we have it efk

00:23:02,880 --> 00:23:06,480
and fluentd as well can behave like

00:23:05,360 --> 00:23:10,080
logstash

00:23:06,480 --> 00:23:13,600
to the rest of the stack as one of its

00:23:10,080 --> 00:23:16,640
uh plugins is a log stash style behavior

00:23:13,600 --> 00:23:16,640
to elasticsearch

00:23:16,720 --> 00:23:19,919
so i'm going to do a little demo in a

00:23:18,480 --> 00:23:22,720
moment this is uh

00:23:19,919 --> 00:23:24,720
um gonna show you just a little bit a

00:23:22,720 --> 00:23:27,919
hint of the art of the possible

00:23:24,720 --> 00:23:31,840
uh and a little bit of fun so

00:23:27,919 --> 00:23:35,200
what's that demo do so i'm gonna have a

00:23:31,840 --> 00:23:37,760
a server running which is gonna take two

00:23:35,200 --> 00:23:39,120
log files and it's going to do a little

00:23:37,760 --> 00:23:42,240
a tunnel transform

00:23:39,120 --> 00:23:44,159
on one of the log files and then

00:23:42,240 --> 00:23:45,919
pass that log file onto what's known as

00:23:44,159 --> 00:23:47,760
a folder

00:23:45,919 --> 00:23:49,200
which we'll see in a minute and

00:23:47,760 --> 00:23:52,559
understand its role

00:23:49,200 --> 00:23:53,679
uh and also where it just pushes uh logs

00:23:52,559 --> 00:23:56,000
back out

00:23:53,679 --> 00:23:57,919
i could put filters into that pipeline

00:23:56,000 --> 00:23:58,720
just to say okay these are the events of

00:23:57,919 --> 00:24:01,279
interest

00:23:58,720 --> 00:24:03,600
and only write those to the log file uh

00:24:01,279 --> 00:24:06,400
but for simplicity we've we've provided

00:24:03,600 --> 00:24:08,799
all of that at this stage

00:24:06,400 --> 00:24:09,520
uh and that could occur on one node or

00:24:08,799 --> 00:24:12,640
it could have

00:24:09,520 --> 00:24:14,799
occurred on many nodes and in fact uh

00:24:12,640 --> 00:24:16,559
whilst the demo will only run it on one

00:24:14,799 --> 00:24:17,760
node there is nothing stopping us

00:24:16,559 --> 00:24:21,200
actually

00:24:17,760 --> 00:24:24,799
running this up on lots of servers and

00:24:21,200 --> 00:24:28,080
simulating the source through

00:24:24,799 --> 00:24:29,440
of course um in a highly distributed

00:24:28,080 --> 00:24:32,960
environment we want to sort of

00:24:29,440 --> 00:24:35,279
centralize and aggregate that together

00:24:32,960 --> 00:24:36,480
and what we've done is therefore built a

00:24:35,279 --> 00:24:40,240
second node

00:24:36,480 --> 00:24:42,880
uh with another fluency configuration

00:24:40,240 --> 00:24:44,320
and this is like a a an aggregation

00:24:42,880 --> 00:24:48,720
point which is going to take

00:24:44,320 --> 00:24:50,320
the folder and do a common process

00:24:48,720 --> 00:24:51,840
and that common process is simply

00:24:50,320 --> 00:24:55,440
examining uh

00:24:51,840 --> 00:24:56,000
the log filter to so that we can have a

00:24:55,440 --> 00:24:57,919
quick

00:24:56,000 --> 00:25:00,960
standard out peak of what's going on

00:24:57,919 --> 00:25:04,000
just let it stream through

00:25:00,960 --> 00:25:04,559
and then specific events specific log

00:25:04,000 --> 00:25:05,919
entries

00:25:04,559 --> 00:25:08,640
we're going to tease out and i'm going

00:25:05,919 --> 00:25:12,159
to send to slack

00:25:08,640 --> 00:25:15,440
so if i was actually configured to

00:25:12,159 --> 00:25:17,840
to look at or look for specific errors

00:25:15,440 --> 00:25:19,760
i could grab those and send it to slack

00:25:17,840 --> 00:25:20,480
i could even get very clever and say

00:25:19,760 --> 00:25:24,720
right

00:25:20,480 --> 00:25:26,080
okay that uh exception is in this part

00:25:24,720 --> 00:25:27,600
of the application

00:25:26,080 --> 00:25:29,520
therefore i'm going to send a slack

00:25:27,600 --> 00:25:31,679
message to

00:25:29,520 --> 00:25:32,960
joe the developer because that's his uh

00:25:31,679 --> 00:25:34,640
responsibility

00:25:32,960 --> 00:25:37,440
he should know that there's a production

00:25:34,640 --> 00:25:40,880
error occurred

00:25:37,440 --> 00:25:42,960
right so i'm not going to uh

00:25:40,880 --> 00:25:43,919
use the screwing gaps i'm actually going

00:25:42,960 --> 00:25:46,960
to

00:25:43,919 --> 00:25:46,960
jump in to

00:25:47,360 --> 00:25:55,120
some real-life development environment

00:25:51,440 --> 00:25:57,600
and what we've got here is

00:25:55,120 --> 00:25:59,760
two fluently configurations you can see

00:25:57,600 --> 00:26:02,880
node one and node two

00:25:59,760 --> 00:26:06,320
and uh let's just quickly walk through

00:26:02,880 --> 00:26:10,240
the uh node one so i can tell

00:26:06,320 --> 00:26:12,640
uh the system fluent d how to log

00:26:10,240 --> 00:26:14,400
itself and i'm just saying okay so

00:26:12,640 --> 00:26:15,440
indeed i just want you to report your

00:26:14,400 --> 00:26:19,279
info

00:26:15,440 --> 00:26:20,799
uh and then um in a fairly decorative

00:26:19,279 --> 00:26:24,080
manner i define

00:26:20,799 --> 00:26:26,159
one or many sources in this case i'm

00:26:24,080 --> 00:26:29,279
defining two sources

00:26:26,159 --> 00:26:31,200
uh one called basic file

00:26:29,279 --> 00:26:33,039
and then i'm getting a second so it's

00:26:31,200 --> 00:26:35,520
called basic file two

00:26:33,039 --> 00:26:36,559
uh in the real world uh you would

00:26:35,520 --> 00:26:39,600
potentially

00:26:36,559 --> 00:26:41,279
uh if you're running in a legacy app

00:26:39,600 --> 00:26:46,080
server perhaps

00:26:41,279 --> 00:26:47,120
be using uh you know a weblogic or web

00:26:46,080 --> 00:26:49,360
sphere

00:26:47,120 --> 00:26:49,360
or

00:26:50,240 --> 00:26:56,960
a red hat fuse container

00:26:53,360 --> 00:26:59,039
um and uh it will be generated in lots

00:26:56,960 --> 00:27:00,480
of different log files for different

00:26:59,039 --> 00:27:02,559
applications

00:27:00,480 --> 00:27:03,840
you'll potentially separate out log

00:27:02,559 --> 00:27:07,120
files from

00:27:03,840 --> 00:27:10,880
your application or your uh war

00:27:07,120 --> 00:27:13,360
or air file perhaps uh compared to the

00:27:10,880 --> 00:27:14,320
actual logs being generated by the core

00:27:13,360 --> 00:27:17,760
engine

00:27:14,320 --> 00:27:22,159
of that j2e framework or

00:27:17,760 --> 00:27:25,840
micro profile container

00:27:22,159 --> 00:27:26,320
but however it's set up you can collect

00:27:25,840 --> 00:27:28,720
these

00:27:26,320 --> 00:27:30,080
and you could i could have easily have

00:27:28,720 --> 00:27:32,240
infrastructure

00:27:30,080 --> 00:27:34,320
collections going on here as well

00:27:32,240 --> 00:27:37,840
collecting aspects like

00:27:34,320 --> 00:27:41,039
an smp trap on my machine

00:27:37,840 --> 00:27:41,760
and then i declare those and then i've

00:27:41,039 --> 00:27:45,039
defined

00:27:41,760 --> 00:27:49,840
a very very simple pipeline which

00:27:45,039 --> 00:27:49,840
is going to do a simple transform

00:27:51,200 --> 00:27:56,240
and manipulate the structure of the the

00:27:54,240 --> 00:27:58,559
log event so that the log event

00:27:56,240 --> 00:28:00,000
uh when it's passed to the central node

00:27:58,559 --> 00:28:03,120
will appear in the same

00:28:00,000 --> 00:28:06,960
structure irrespective of

00:28:03,120 --> 00:28:07,919
the origin um and then you could i can

00:28:06,960 --> 00:28:10,559
show you the how

00:28:07,919 --> 00:28:11,520
that differs i have uh two configuration

00:28:10,559 --> 00:28:14,880
files here

00:28:11,520 --> 00:28:17,840
that i use in a an open source tool

00:28:14,880 --> 00:28:17,840
that we've built

00:28:18,159 --> 00:28:22,240
and available through github that allows

00:28:20,960 --> 00:28:25,840
us to

00:28:22,240 --> 00:28:29,679
take data and create log events

00:28:25,840 --> 00:28:31,840
or replay an application log file again

00:28:29,679 --> 00:28:35,279
and re-stamp it

00:28:31,840 --> 00:28:37,760
and simulate it well play it back in in

00:28:35,279 --> 00:28:41,440
real time so the time intervals are

00:28:37,760 --> 00:28:41,440
controlled and things like that

00:28:41,520 --> 00:28:45,840
if you're trying to use it to test you

00:28:43,919 --> 00:28:48,480
your monitoring configuration

00:28:45,840 --> 00:28:49,919
that you might want to reiterate or loop

00:28:48,480 --> 00:28:52,080
through your logs

00:28:49,919 --> 00:28:53,200
uh multiple times so we can do things

00:28:52,080 --> 00:28:56,159
like that lots of

00:28:53,200 --> 00:28:56,720
flexibility but the bottom line is is in

00:28:56,159 --> 00:28:58,799
here

00:28:56,720 --> 00:29:00,240
i've defined how i'm going to structure

00:28:58,799 --> 00:29:04,640
the payload

00:29:00,240 --> 00:29:06,399
uh and how i'm reading the source

00:29:04,640 --> 00:29:09,039
and then as you can see here again

00:29:06,399 --> 00:29:09,440
sources the the basic file and event and

00:29:09,039 --> 00:29:12,880
then

00:29:09,440 --> 00:29:15,840
take the event message and add

00:29:12,880 --> 00:29:17,360
a counter to how many times i've

00:29:15,840 --> 00:29:21,760
iterated through the

00:29:17,360 --> 00:29:23,919
test log data set that i've got

00:29:21,760 --> 00:29:24,880
if i go to my second stream you can see

00:29:23,919 --> 00:29:28,080
it's different

00:29:24,880 --> 00:29:31,200
the message and i've got stream and then

00:29:28,080 --> 00:29:34,080
i've got it same value but

00:29:31,200 --> 00:29:35,840
i've just given it a different name and

00:29:34,080 --> 00:29:37,440
you can see i've given it attributes as

00:29:35,840 --> 00:29:41,840
well that i can simulate

00:29:37,440 --> 00:29:41,840
class paths and things like that

00:29:42,000 --> 00:29:47,039
so that's gonna go and get picked up by

00:29:45,679 --> 00:29:50,720
node one

00:29:47,039 --> 00:29:52,480
and uh it will get processed

00:29:50,720 --> 00:29:54,240
and the important thing is it comes

00:29:52,480 --> 00:29:57,440
through uh

00:29:54,240 --> 00:29:59,360
here now once i've done the transform

00:29:57,440 --> 00:30:00,799
uh and i'm gonna send it to two places

00:29:59,360 --> 00:30:03,840
i'm gonna send it to

00:30:00,799 --> 00:30:05,600
my uh file that i mentioned

00:30:03,840 --> 00:30:08,480
and then i'm gonna fold it on to the

00:30:05,600 --> 00:30:11,760
other node and that's declared with this

00:30:08,480 --> 00:30:14,640
declaration here okay oh sorry

00:30:11,760 --> 00:30:18,799
this bit then says okay right i'm going

00:30:14,640 --> 00:30:18,799
to pass it on i want another pipeline

00:30:19,279 --> 00:30:22,720
that we call refer to as a label which

00:30:21,600 --> 00:30:26,080
is a series of

00:30:22,720 --> 00:30:28,399
activities and

00:30:26,080 --> 00:30:30,640
it will then match all the events that

00:30:28,399 --> 00:30:31,360
come in and every five seconds i will

00:30:30,640 --> 00:30:34,399
push

00:30:31,360 --> 00:30:37,039
log events over to the consolidated

00:30:34,399 --> 00:30:38,559
second node and then if we look at the

00:30:37,039 --> 00:30:40,640
second node

00:30:38,559 --> 00:30:43,039
which is here you can see my source

00:30:40,640 --> 00:30:43,440
coming in which is now my folder and i'm

00:30:43,039 --> 00:30:46,880
saying

00:30:43,440 --> 00:30:50,480
on port 28 80 capture

00:30:46,880 --> 00:30:53,840
accept events in and then

00:30:50,480 --> 00:30:56,240
i can look for uh

00:30:53,840 --> 00:30:57,440
log events that got referenced to

00:30:56,240 --> 00:31:00,399
computer

00:30:57,440 --> 00:31:02,320
uh so i can have computer with a capital

00:31:00,399 --> 00:31:05,039
c or lowercase c of course i could be

00:31:02,320 --> 00:31:07,279
clever with my regex

00:31:05,039 --> 00:31:09,679
and say well it's going to be a capital

00:31:07,279 --> 00:31:12,480
c or a lower case c

00:31:09,679 --> 00:31:14,159
or you know ignore capitalization in the

00:31:12,480 --> 00:31:17,440
expression

00:31:14,159 --> 00:31:18,000
um once i've filtered those outs so not

00:31:17,440 --> 00:31:19,840
all

00:31:18,000 --> 00:31:22,640
of my simulated logs are going to have

00:31:19,840 --> 00:31:24,399
talk about computer in any way

00:31:22,640 --> 00:31:26,320
but those that do will then that come

00:31:24,399 --> 00:31:30,240
through the standard out

00:31:26,320 --> 00:31:33,519
i mentioned and then go on to slack

00:31:30,240 --> 00:31:36,080
and yes by the time uh

00:31:33,519 --> 00:31:37,440
that we finish this presentation i'll be

00:31:36,080 --> 00:31:41,279
changing my

00:31:37,440 --> 00:31:43,760
uh token into slack

00:31:41,279 --> 00:31:44,720
uh but you can then say i've configured

00:31:43,760 --> 00:31:46,960
it and it

00:31:44,720 --> 00:31:48,960
describes the message which aspects of

00:31:46,960 --> 00:31:52,080
the log message could be used

00:31:48,960 --> 00:31:54,080
the moment i'm just using all of it uh

00:31:52,080 --> 00:31:56,000
but it'd be very easy to say i just want

00:31:54,080 --> 00:31:57,039
these attributes and pull specific

00:31:56,000 --> 00:32:00,320
attributes

00:31:57,039 --> 00:32:01,919
out of the message and traverse it uh

00:32:00,320 --> 00:32:04,320
the the the log

00:32:01,919 --> 00:32:07,279
event because i i've applied some

00:32:04,320 --> 00:32:14,240
structure and meaning to it

00:32:07,279 --> 00:32:17,360
so let's fire things up

00:32:14,240 --> 00:32:20,960
so here we've got uh a few uh

00:32:17,360 --> 00:32:24,320
shell environments on my machine

00:32:20,960 --> 00:32:28,000
and we're just going to fire up

00:32:24,320 --> 00:32:30,000
the fluent d nodes so there's one

00:32:28,000 --> 00:32:31,840
and there's the other and we can see

00:32:30,000 --> 00:32:34,960
it's processing the

00:32:31,840 --> 00:32:38,000
configuration uh it's just telling us

00:32:34,960 --> 00:32:41,360
about some buffer configurations

00:32:38,000 --> 00:32:44,480
uh and then i'm going to fire up

00:32:41,360 --> 00:32:45,279
the log generation process and you'll

00:32:44,480 --> 00:32:47,360
see it's

00:32:45,279 --> 00:32:49,200
saying i'm talking about the the logs

00:32:47,360 --> 00:32:52,240
that it's building

00:32:49,200 --> 00:32:56,000
and what we'll see uh is it's

00:32:52,240 --> 00:32:59,039
now processing these if i now uh

00:32:56,000 --> 00:32:59,039
bring up my

00:32:59,360 --> 00:33:02,480
slack

00:33:00,790 --> 00:33:06,320
[Music]

00:33:02,480 --> 00:33:11,840
environment then what we'll see

00:33:06,320 --> 00:33:11,840
in a moment is

00:33:17,919 --> 00:33:21,600
as you can see it's starting to nudge me

00:33:19,840 --> 00:33:22,640
telling me that i've got events coming

00:33:21,600 --> 00:33:26,880
through

00:33:22,640 --> 00:33:26,880
and if i uh

00:33:27,120 --> 00:33:33,600
bring this over you can see

00:33:30,399 --> 00:33:34,640
it's now to fluent the is getting

00:33:33,600 --> 00:33:37,440
messages

00:33:34,640 --> 00:33:38,159
and you can see it's built the message

00:33:37,440 --> 00:33:42,320
up

00:33:38,159 --> 00:33:44,720
uh based on uh telling me about which

00:33:42,320 --> 00:33:46,240
node it is but i've hardwired that into

00:33:44,720 --> 00:33:47,760
the configuration if we look at the

00:33:46,240 --> 00:33:51,039
configuration

00:33:47,760 --> 00:33:53,440
and then it's uh displaying

00:33:51,039 --> 00:33:55,679
the messages and you can see it's

00:33:53,440 --> 00:33:59,279
changing the headers

00:33:55,679 --> 00:34:02,159
based on the uh uh origin of the uh

00:33:59,279 --> 00:34:04,240
the payload and that's entirely my

00:34:02,159 --> 00:34:06,559
configuration

00:34:04,240 --> 00:34:09,119
so if i whiz down it will keep churning

00:34:06,559 --> 00:34:12,320
away sending more messages to me

00:34:09,119 --> 00:34:15,440
um as we go

00:34:12,320 --> 00:34:18,879
uh and that's really a very simple

00:34:15,440 --> 00:34:22,000
uh example of uh fluency in action

00:34:18,879 --> 00:34:27,040
yes it's very mickey mouse but

00:34:22,000 --> 00:34:29,760
through a a few dozen lines of code

00:34:27,040 --> 00:34:30,320
or configuration should i say if i come

00:34:29,760 --> 00:34:34,000
back

00:34:30,320 --> 00:34:36,879
here we'll see

00:34:34,000 --> 00:34:36,879
that uh

00:34:37,119 --> 00:34:40,720
i'm consuming and rooting the messages

00:34:39,839 --> 00:34:45,040
very easily

00:34:40,720 --> 00:34:48,800
and i'm filtering out messages over here

00:34:45,040 --> 00:34:52,960
and there we go so let me uh stop

00:34:48,800 --> 00:34:52,960
slack because it's pinging away

00:34:56,879 --> 00:35:04,079
and actually what we'll do is we'll

00:35:00,880 --> 00:35:04,079
kill the processes

00:35:14,839 --> 00:35:17,839
okay

00:35:18,720 --> 00:35:23,599
so in the real world you know

00:35:22,160 --> 00:35:27,119
we've talked about that the need for

00:35:23,599 --> 00:35:29,280
highly distributed uh models so let me

00:35:27,119 --> 00:35:30,880
show you some options and ways you can

00:35:29,280 --> 00:35:34,160
deploy fluently

00:35:30,880 --> 00:35:37,440
um we can think about

00:35:34,160 --> 00:35:39,520
how we're reactive and and to support

00:35:37,440 --> 00:35:40,800
the the tracing considerations

00:35:39,520 --> 00:35:43,280
potentially as well

00:35:40,800 --> 00:35:44,079
although you might want to use something

00:35:43,280 --> 00:35:46,079
like

00:35:44,079 --> 00:35:49,200
jaeger if you're in a microservices

00:35:46,079 --> 00:35:49,200
world to do tracing

00:35:50,240 --> 00:35:53,760
we've talked about that the challenges

00:35:52,000 --> 00:35:54,560
have been had to distribute to different

00:35:53,760 --> 00:35:57,359
tools

00:35:54,560 --> 00:35:59,520
for different people teams to use for

00:35:57,359 --> 00:36:01,839
different purposes

00:35:59,520 --> 00:36:03,119
but whilst i've talked a lot about

00:36:01,839 --> 00:36:06,160
kubernetes here

00:36:03,119 --> 00:36:08,160
there is also a lot this will do

00:36:06,160 --> 00:36:09,839
and is equally valid in a legacy

00:36:08,160 --> 00:36:13,119
environment you don't have to be

00:36:09,839 --> 00:36:13,440
a microservice in a container this can

00:36:13,119 --> 00:36:16,240
be

00:36:13,440 --> 00:36:17,599
on a real physical machine because at

00:36:16,240 --> 00:36:20,880
the end of the day you're just

00:36:17,599 --> 00:36:23,040
harvesting up log files or log events

00:36:20,880 --> 00:36:27,359
because you can actually wire

00:36:23,040 --> 00:36:30,720
your applications directly to fluentd

00:36:27,359 --> 00:36:34,240
if you're using in java for example

00:36:30,720 --> 00:36:37,359
log back or sofa j

00:36:34,240 --> 00:36:37,359
then there are

00:36:38,000 --> 00:36:41,200
configurations that you can apply that

00:36:40,240 --> 00:36:43,920
mean that they talk

00:36:41,200 --> 00:36:44,720
directly to front d cutting out the file

00:36:43,920 --> 00:36:46,800
which gives you

00:36:44,720 --> 00:36:48,640
an efficiency improvement and a

00:36:46,800 --> 00:36:52,160
performance gain because

00:36:48,640 --> 00:36:54,480
fluentd hasn't got to pass the

00:36:52,160 --> 00:36:55,280
lines out of the file to go oh okay

00:36:54,480 --> 00:36:58,160
right

00:36:55,280 --> 00:36:59,119
uh i've got this record now i'm going to

00:36:58,160 --> 00:37:01,359
apply some

00:36:59,119 --> 00:37:03,040
meaning to it but i'm not having to

00:37:01,359 --> 00:37:05,760
understand the structure of your

00:37:03,040 --> 00:37:05,760
your log file

00:37:06,720 --> 00:37:11,520
but let's look at some possibilities in

00:37:09,359 --> 00:37:13,839
terms of scaling the deployment

00:37:11,520 --> 00:37:15,040
uh this obviously is a little bit like

00:37:13,839 --> 00:37:17,520
the the

00:37:15,040 --> 00:37:18,960
scenario mocked up scenario that i've

00:37:17,520 --> 00:37:22,640
shown you

00:37:18,960 --> 00:37:26,079
we have three servers uh showing you

00:37:22,640 --> 00:37:28,880
uh one application and they're

00:37:26,079 --> 00:37:31,760
aggregating to a central server

00:37:28,880 --> 00:37:33,680
and they could have a secondary or dr

00:37:31,760 --> 00:37:36,400
node

00:37:33,680 --> 00:37:38,240
ready and available and you could then

00:37:36,400 --> 00:37:40,800
have some other servers

00:37:38,240 --> 00:37:42,000
using uh with fluency instances

00:37:40,800 --> 00:37:45,280
aggregating to

00:37:42,000 --> 00:37:47,440
another central one and the

00:37:45,280 --> 00:37:49,440
central servers are perhaps filtering

00:37:47,440 --> 00:37:53,599
out the most critical messages

00:37:49,440 --> 00:37:55,359
uh and sending it to a an alerting tool

00:37:53,599 --> 00:37:57,920
i've used slack but that could just as

00:37:55,359 --> 00:38:00,720
easily be paid you do to your teams or

00:37:57,920 --> 00:38:03,040
or many other tools in that space and

00:38:00,720 --> 00:38:06,480
then pushed on the logs for

00:38:03,040 --> 00:38:08,480
more substantial long-term analysis uh

00:38:06,480 --> 00:38:11,040
by putting them into a persistence layer

00:38:08,480 --> 00:38:14,240
such as elasticsearch or

00:38:11,040 --> 00:38:17,440
just dropping it in s3 buckets to be

00:38:14,240 --> 00:38:22,160
pulled into a tool of choice

00:38:17,440 --> 00:38:25,200
and so on but

00:38:22,160 --> 00:38:28,560
that could equally be a

00:38:25,200 --> 00:38:31,599
docker set up

00:38:28,560 --> 00:38:33,359
being managed by kubernetes

00:38:31,599 --> 00:38:35,119
well on the left hand side here now i

00:38:33,359 --> 00:38:38,720
have a worker node where

00:38:35,119 --> 00:38:41,760
uh it's just assuming that uh

00:38:38,720 --> 00:38:44,160
all the uh

00:38:41,760 --> 00:38:45,119
microservices are just spitting too

00:38:44,160 --> 00:38:47,040
standard out

00:38:45,119 --> 00:38:48,160
which means that kubernetes will pick

00:38:47,040 --> 00:38:52,480
them up on the

00:38:48,160 --> 00:38:55,520
uh working that off the worker nodes

00:38:52,480 --> 00:38:57,920
and uh be able to process them

00:38:55,520 --> 00:39:00,079
obviously that's less efficient because

00:38:57,920 --> 00:39:02,640
you've got to take a standard out feed

00:39:00,079 --> 00:39:04,640
and then reapply the meaning you know

00:39:02,640 --> 00:39:07,040
slice the the

00:39:04,640 --> 00:39:08,320
the liner text up to extract the time

00:39:07,040 --> 00:39:11,040
and date and

00:39:08,320 --> 00:39:12,560
the node that generated that the event

00:39:11,040 --> 00:39:15,359
and then the messages

00:39:12,560 --> 00:39:16,720
which can contain structured data as

00:39:15,359 --> 00:39:19,599
well

00:39:16,720 --> 00:39:20,240
um you know so if you can avoid that

00:39:19,599 --> 00:39:22,240
then

00:39:20,240 --> 00:39:23,440
you perhaps might want to use the the

00:39:22,240 --> 00:39:27,280
model on the

00:39:23,440 --> 00:39:31,359
right hand side where i've gone and put

00:39:27,280 --> 00:39:35,280
a fluent d smaller brethren fluent bit

00:39:31,359 --> 00:39:39,040
which is able to do a subset of fluent d

00:39:35,280 --> 00:39:42,560
uh but has a very very small footprint

00:39:39,040 --> 00:39:46,160
and it can root the log events through

00:39:42,560 --> 00:39:49,280
uh so we don't have to re-parse

00:39:46,160 --> 00:39:50,880
things to get that meaning uh the fluent

00:39:49,280 --> 00:39:52,720
bit has done that

00:39:50,880 --> 00:39:54,800
or actually the interaction with fluent

00:39:52,720 --> 00:39:56,160
bits meant that we've not lost it from

00:39:54,800 --> 00:39:57,839
the outset

00:39:56,160 --> 00:40:00,240
and we aggregate and we send to a

00:39:57,839 --> 00:40:02,720
central node

00:40:00,240 --> 00:40:04,640
and we can do things like failover and

00:40:02,720 --> 00:40:07,760
and stuff like that or

00:40:04,640 --> 00:40:11,599
as flint d would talk about uh

00:40:07,760 --> 00:40:14,240
do node discovery so it can go and find

00:40:11,599 --> 00:40:15,119
a node and we can put the concentrators

00:40:14,240 --> 00:40:20,240
into

00:40:15,119 --> 00:40:23,359
pods if we like and so on

00:40:20,240 --> 00:40:24,000
we could also address it by using the

00:40:23,359 --> 00:40:27,200
sidecar

00:40:24,000 --> 00:40:28,000
pattern uh so you have a docker and

00:40:27,200 --> 00:40:31,680
those

00:40:28,000 --> 00:40:33,359
pre-built docker images with phone d

00:40:31,680 --> 00:40:34,800
and you can then just inject the

00:40:33,359 --> 00:40:38,160
configuration file for

00:40:34,800 --> 00:40:41,119
it to use and off you go

00:40:38,160 --> 00:40:41,839
so again you know another option is it's

00:40:41,119 --> 00:40:43,839
just in

00:40:41,839 --> 00:40:45,280
the the options and the possibilities

00:40:43,839 --> 00:40:47,440
are really down to

00:40:45,280 --> 00:40:49,760
how you want to work and the the pros

00:40:47,440 --> 00:40:53,839
and cons and the benefits

00:40:49,760 --> 00:40:53,839
of different configuration approaches

00:40:53,920 --> 00:40:58,720
to wrap up i just wanted to share this

00:40:56,319 --> 00:41:02,000
this is

00:40:58,720 --> 00:41:03,359
the basis of a real world use case that

00:41:02,000 --> 00:41:05,760
we've built

00:41:03,359 --> 00:41:08,319
as you can see across the bottom i've

00:41:05,760 --> 00:41:13,119
shown the life cycle

00:41:08,319 --> 00:41:16,720
of the log events

00:41:13,119 --> 00:41:20,319
and fluent decents in the middle is his

00:41:16,720 --> 00:41:22,240
real strength is the structure and route

00:41:20,319 --> 00:41:24,079
and we've got the information sources on

00:41:22,240 --> 00:41:26,960
the left varying from

00:41:24,079 --> 00:41:28,960
virtual machines to kubernetes and

00:41:26,960 --> 00:41:32,839
docker images

00:41:28,960 --> 00:41:34,640
log4j on our java application sdos

00:41:32,839 --> 00:41:37,440
involved

00:41:34,640 --> 00:41:38,400
and of course we could even harvest if

00:41:37,440 --> 00:41:42,400
we want to

00:41:38,400 --> 00:41:45,599
deal with multi-cloud potentially the

00:41:42,400 --> 00:41:46,960
log events being captured by a cloud

00:41:45,599 --> 00:41:49,520
provider

00:41:46,960 --> 00:41:50,560
that could be you know an aws or google

00:41:49,520 --> 00:41:54,720
or oracle

00:41:50,560 --> 00:41:57,760
or azure um and so on

00:41:54,720 --> 00:41:59,440
uh and then what phone do you did for in

00:41:57,760 --> 00:42:02,400
this case was uh

00:41:59,440 --> 00:42:03,920
uh fed those events through to a number

00:42:02,400 --> 00:42:06,960
of different tools

00:42:03,920 --> 00:42:09,599
so we sent the security any security

00:42:06,960 --> 00:42:11,680
related activities to splunk

00:42:09,599 --> 00:42:12,880
but didn't send everything to splunk

00:42:11,680 --> 00:42:15,520
because it was deployed

00:42:12,880 --> 00:42:17,839
in a different location and we didn't

00:42:15,520 --> 00:42:18,560
want to generate vast amounts of data

00:42:17,839 --> 00:42:22,640
transfer

00:42:18,560 --> 00:42:25,760
across from cloud to cloud because you

00:42:22,640 --> 00:42:27,920
uh start incurring costs there so we

00:42:25,760 --> 00:42:31,200
were pulling out the the relevant uh

00:42:27,920 --> 00:42:33,200
log events and pushing those out we were

00:42:31,200 --> 00:42:34,400
actually building uh some very simple

00:42:33,200 --> 00:42:37,680
metrics

00:42:34,400 --> 00:42:39,680
uh within fluentd uh which were then

00:42:37,680 --> 00:42:42,319
being shared with prometheus

00:42:39,680 --> 00:42:43,280
and rooting on certain log events that

00:42:42,319 --> 00:42:47,040
were just showing

00:42:43,280 --> 00:42:50,079
uh stats that were coming out of the jvm

00:42:47,040 --> 00:42:52,960
um and of course a subset that

00:42:50,079 --> 00:42:55,760
uh was being sent to elasticsearch for

00:42:52,960 --> 00:42:58,160
uh visualization through kibana

00:42:55,760 --> 00:42:59,359
and we had some rules on elasticsearch

00:42:58,160 --> 00:43:02,560
that if it found

00:42:59,359 --> 00:43:07,119
a certain number of errors uh

00:43:02,560 --> 00:43:10,000
of a particular common characteristic

00:43:07,119 --> 00:43:12,240
then we would raise a due a ticket and

00:43:10,000 --> 00:43:16,319
of course we were nudging people through

00:43:12,240 --> 00:43:16,800
uh email and slack on specific events of

00:43:16,319 --> 00:43:19,920
interest

00:43:16,800 --> 00:43:24,640
for them and um in line with

00:43:19,920 --> 00:43:26,720
uh you know right tool for the right job

00:43:24,640 --> 00:43:28,640
despite having fluently there for the

00:43:26,720 --> 00:43:31,680
tracing activities we still

00:43:28,640 --> 00:43:35,040
use the jaeger to collect

00:43:31,680 --> 00:43:37,280
the trace data and

00:43:35,040 --> 00:43:38,880
provide the tracing analysis and

00:43:37,280 --> 00:43:42,079
visualization

00:43:38,880 --> 00:43:42,880
yeah because it's not a case of one tool

00:43:42,079 --> 00:43:46,000
fit

00:43:42,880 --> 00:43:49,040
fits all fluent d works well with

00:43:46,000 --> 00:43:52,640
semi-structured uh data

00:43:49,040 --> 00:43:55,839
uh and structured data

00:43:52,640 --> 00:43:58,240
but uh pure metrics uh are

00:43:55,839 --> 00:44:00,880
quite often best handled by dedicated

00:43:58,240 --> 00:44:03,440
tools particularly for things like trace

00:44:00,880 --> 00:44:04,079
um you could do it if you were really

00:44:03,440 --> 00:44:07,680
perverse

00:44:04,079 --> 00:44:09,440
but uh it'd be a lot of work um

00:44:07,680 --> 00:44:11,280
when actually there's a tool out there

00:44:09,440 --> 00:44:14,319
does a better job and it's designed

00:44:11,280 --> 00:44:16,960
for that very purpose

00:44:14,319 --> 00:44:18,000
so thank you for your time i hope you

00:44:16,960 --> 00:44:21,200
found

00:44:18,000 --> 00:44:23,119
it useful a lot of this information is

00:44:21,200 --> 00:44:25,200
available online as i mentioned at the

00:44:23,119 --> 00:44:28,240
start

00:44:25,200 --> 00:44:37,839
and we'll take any questions

00:44:28,240 --> 00:44:37,839
thank you very much

00:44:40,319 --> 00:44:42,400

YouTube URL: https://www.youtube.com/watch?v=42PcptOzesc


