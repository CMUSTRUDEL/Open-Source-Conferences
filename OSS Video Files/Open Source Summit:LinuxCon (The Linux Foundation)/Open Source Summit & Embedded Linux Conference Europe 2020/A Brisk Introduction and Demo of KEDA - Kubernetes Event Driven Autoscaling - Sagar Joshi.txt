Title: A Brisk Introduction and Demo of KEDA - Kubernetes Event Driven Autoscaling - Sagar Joshi
Publication date: 2020-10-29
Playlist: Open Source Summit & Embedded Linux Conference Europe 2020
Description: 
	Lightning Talk: A Brisk Introduction and Demo of KEDA - Kubernetes Event Driven Autoscaling - Sagar Joshi, Microsoft
Captions: 
	00:00:00,799 --> 00:00:05,520
hello everyone my name is sagar yoshi

00:00:03,840 --> 00:00:07,520
and i work with microsoft as a partner

00:00:05,520 --> 00:00:09,200
technology strategist

00:00:07,520 --> 00:00:11,679
you can get in touch with me using the

00:00:09,200 --> 00:00:13,519
twitter and linkedin information you see

00:00:11,679 --> 00:00:16,080
on the screen

00:00:13,519 --> 00:00:18,960
today's topic is keda an acronym for

00:00:16,080 --> 00:00:20,320
kubernetes even driven auto skating

00:00:18,960 --> 00:00:21,760
we'll first start by looking at some of

00:00:20,320 --> 00:00:22,800
the challenges which developers

00:00:21,760 --> 00:00:25,599
experience

00:00:22,800 --> 00:00:26,720
how kda solves it we will look at the

00:00:25,599 --> 00:00:30,160
kdr community

00:00:26,720 --> 00:00:33,280
and architecture and end with a demo

00:00:30,160 --> 00:00:34,640
all right let's get started

00:00:33,280 --> 00:00:37,200
if you're a developer working with

00:00:34,640 --> 00:00:39,200
kubernetes to host your applications

00:00:37,200 --> 00:00:40,719
you might be aware of a concept called

00:00:39,200 --> 00:00:44,000
as hpa

00:00:40,719 --> 00:00:45,840
hpa means horizontal pod autoscaler

00:00:44,000 --> 00:00:47,200
it's a component of kubernetes which

00:00:45,840 --> 00:00:49,760
helps you to scale

00:00:47,200 --> 00:00:51,199
your parts which is the cpu or memory

00:00:49,760 --> 00:00:53,440
matrix

00:00:51,199 --> 00:00:54,960
but as we move into the modern world of

00:00:53,440 --> 00:00:56,000
microservices and event driven

00:00:54,960 --> 00:00:59,039
architectures

00:00:56,000 --> 00:01:00,480
our scaling needs are also changing

00:00:59,039 --> 00:01:03,120
you might need to scale your

00:01:00,480 --> 00:01:05,920
applications missing some custom events

00:01:03,120 --> 00:01:07,760
or messages uh which are there in your

00:01:05,920 --> 00:01:09,680
application

00:01:07,760 --> 00:01:11,119
well you can solve that problem possibly

00:01:09,680 --> 00:01:14,320
writing your own frameworks

00:01:11,119 --> 00:01:15,200
operators or custom resources that could

00:01:14,320 --> 00:01:17,360
sometimes be

00:01:15,200 --> 00:01:19,200
time consuming or difficult to maintain

00:01:17,360 --> 00:01:21,600
in the long run

00:01:19,200 --> 00:01:23,119
hence in order to drive developer

00:01:21,600 --> 00:01:25,920
productivity and agility

00:01:23,119 --> 00:01:28,320
in designing robust systems keda solves

00:01:25,920 --> 00:01:30,240
this problem for you

00:01:28,320 --> 00:01:31,360
cada helps you to scale your

00:01:30,240 --> 00:01:34,000
applications

00:01:31,360 --> 00:01:36,560
basis the events or messages in your

00:01:34,000 --> 00:01:36,560
application

00:01:37,040 --> 00:01:41,520
let's look at some of the features of

00:01:38,840 --> 00:01:43,759
data it is very relevant in an event

00:01:41,520 --> 00:01:46,320
driven architecture

00:01:43,759 --> 00:01:47,759
cada helps you to implement auto scaling

00:01:46,320 --> 00:01:50,720
easily

00:01:47,759 --> 00:01:52,799
we have lot of scalars for example from

00:01:50,720 --> 00:01:55,040
telemetry systems database systems

00:01:52,799 --> 00:01:56,719
messaging systems which are used across

00:01:55,040 --> 00:01:59,040
the industry

00:01:56,719 --> 00:02:00,880
you can scale jobs as well as parts

00:01:59,040 --> 00:02:03,040
using data

00:02:00,880 --> 00:02:04,960
cada is also vendor agnostic whether you

00:02:03,040 --> 00:02:05,680
are running your kubernetes cluster on

00:02:04,960 --> 00:02:08,399
premises

00:02:05,680 --> 00:02:10,560
on your laptop or on the cloud kdi will

00:02:08,399 --> 00:02:12,319
work just fine

00:02:10,560 --> 00:02:14,400
if you are working with microsoft as

00:02:12,319 --> 00:02:17,680
your functions which run on the cloud

00:02:14,400 --> 00:02:18,879
on the edge or on on premises you can

00:02:17,680 --> 00:02:23,840
use scada

00:02:18,879 --> 00:02:23,840
to scale as your functions as well

00:02:24,879 --> 00:02:30,879
again the value proposition of data is

00:02:28,400 --> 00:02:34,400
helping you easily implement auto

00:02:30,879 --> 00:02:34,400
scaling for your applications

00:02:34,720 --> 00:02:38,560
let's look at some of the community work

00:02:36,800 --> 00:02:41,440
around kedar

00:02:38,560 --> 00:02:43,920
kira is now cncf sandbox project which

00:02:41,440 --> 00:02:45,519
is cloud native computing foundation

00:02:43,920 --> 00:02:47,599
it was originally developed and now

00:02:45,519 --> 00:02:48,560
maintained by microsoft red hat and many

00:02:47,599 --> 00:02:50,239
other companies

00:02:48,560 --> 00:02:51,680
who are contributing to the open source

00:02:50,239 --> 00:02:54,080
ecosystem

00:02:51,680 --> 00:02:55,120
if you want to get involved just log

00:02:54,080 --> 00:02:58,560
into keda dot

00:02:55,120 --> 00:02:58,560
sh slash community

00:02:58,800 --> 00:03:02,879
now let's quickly look at the creda

00:03:00,159 --> 00:03:05,200
architecture cada sits between

00:03:02,879 --> 00:03:06,000
the horizontal pod auto scaler and the

00:03:05,200 --> 00:03:08,080
event

00:03:06,000 --> 00:03:10,239
trigger source now this could be a

00:03:08,080 --> 00:03:12,879
messaging system like kafka

00:03:10,239 --> 00:03:14,400
rabbit mq or as your service bus which

00:03:12,879 --> 00:03:16,959
generates messages

00:03:14,400 --> 00:03:18,959
i did is it then feeds that information

00:03:16,959 --> 00:03:21,120
to the horizontal pod auto scaler

00:03:18,959 --> 00:03:22,720
which then initiates scale action for

00:03:21,120 --> 00:03:24,480
your parts

00:03:22,720 --> 00:03:26,799
this is just a simple diagonal diagram

00:03:24,480 --> 00:03:32,000
explaining where does cada sits inside

00:03:26,799 --> 00:03:33,440
your kubernetes cluster

00:03:32,000 --> 00:03:35,280
just to take an example of some of the

00:03:33,440 --> 00:03:36,879
scalars which are available

00:03:35,280 --> 00:03:39,120
and very prominent softwares in the

00:03:36,879 --> 00:03:40,959
industry which are used for messaging or

00:03:39,120 --> 00:03:43,200
eventing architectures

00:03:40,959 --> 00:03:45,599
some of the examples are apache kafka as

00:03:43,200 --> 00:03:49,280
your service bus as your event hubs

00:03:45,599 --> 00:03:50,799
radius mysql databases and many others

00:03:49,280 --> 00:03:53,280
from the other cloud providers and

00:03:50,799 --> 00:03:53,280
industry

00:03:54,879 --> 00:03:58,640
what are some of the use cases that you

00:03:56,480 --> 00:04:01,519
can use creda in

00:03:58,640 --> 00:04:02,400
number one pod auto scaling or notes so

00:04:01,519 --> 00:04:04,879
basically

00:04:02,400 --> 00:04:06,799
you can scale your application pods

00:04:04,879 --> 00:04:08,720
basis the number of events or messages

00:04:06,799 --> 00:04:10,720
that are there in the system

00:04:08,720 --> 00:04:12,480
just carrying by cpu or memory might not

00:04:10,720 --> 00:04:14,400
be sufficient

00:04:12,480 --> 00:04:17,519
keda can also be used to implement

00:04:14,400 --> 00:04:19,280
serverless workloads in kubernetes

00:04:17,519 --> 00:04:21,359
i highly recommend you read about

00:04:19,280 --> 00:04:23,600
concept called as virtual cube like if

00:04:21,359 --> 00:04:25,680
you're not done so already

00:04:23,600 --> 00:04:27,840
an example of virtual qubit you can find

00:04:25,680 --> 00:04:30,880
on for example in one of the cloud

00:04:27,840 --> 00:04:33,280
environments is is uh virtual node

00:04:30,880 --> 00:04:35,199
in azure kubernetes service so you can

00:04:33,280 --> 00:04:38,880
scale from zero to end pods

00:04:35,199 --> 00:04:40,720
or shrink in from end to zero ports

00:04:38,880 --> 00:04:42,800
on a virtual cube that which basically

00:04:40,720 --> 00:04:45,120
is truly serverless

00:04:42,800 --> 00:04:46,560
and this in combination with the cloud

00:04:45,120 --> 00:04:47,440
environment can really help you to

00:04:46,560 --> 00:04:49,840
implement

00:04:47,440 --> 00:04:52,479
truly serverless workloads using

00:04:49,840 --> 00:04:52,479
kubernetes

00:04:53,040 --> 00:04:56,880
all right let's jump right into the demo

00:04:55,759 --> 00:05:00,240
we're going to use

00:04:56,880 --> 00:05:02,000
a azure service bus as the event source

00:05:00,240 --> 00:05:03,600
and then we're going to push some

00:05:02,000 --> 00:05:06,160
messages into the queue

00:05:03,600 --> 00:05:07,680
and scale the number of parts bases the

00:05:06,160 --> 00:05:08,400
number of events which are there in the

00:05:07,680 --> 00:05:10,560
queue

00:05:08,400 --> 00:05:12,479
and if it crosses certain threshold

00:05:10,560 --> 00:05:16,160
we're going to keep adding pods

00:05:12,479 --> 00:05:16,160
right so let's jump right in

00:05:18,560 --> 00:05:23,919
here you see is my console i have a

00:05:21,840 --> 00:05:25,759
kubernetes cluster running on the cloud

00:05:23,919 --> 00:05:26,400
and i can show you the number of nodes

00:05:25,759 --> 00:05:27,919
which i have

00:05:26,400 --> 00:05:30,080
so i can just say cube city and get

00:05:27,919 --> 00:05:31,280
nodes i have two nodes currently which

00:05:30,080 --> 00:05:33,840
are running

00:05:31,280 --> 00:05:36,080
now let's see uh if i have any parts

00:05:33,840 --> 00:05:39,120
running so i'll say q3 here

00:05:36,080 --> 00:05:42,160
get parts and i basically have a

00:05:39,120 --> 00:05:42,160
namespace for this

00:05:42,560 --> 00:05:45,680
i only have one part which is a web part

00:05:45,120 --> 00:05:51,919
and

00:05:45,680 --> 00:05:53,600
if i want to see the deployments of pods

00:05:51,919 --> 00:05:56,319
the order processor part

00:05:53,600 --> 00:05:56,960
so basically the sample uh processes

00:05:56,319 --> 00:05:59,039
orders

00:05:56,960 --> 00:06:00,960
by picking up messages from the queue

00:05:59,039 --> 00:06:02,960
and currently there are no orders

00:06:00,960 --> 00:06:05,840
hence none of the parts running which is

00:06:02,960 --> 00:06:08,720
zero and this is a speciality of data

00:06:05,840 --> 00:06:09,919
from zero to n or zero to one and back

00:06:08,720 --> 00:06:11,680
to zero

00:06:09,919 --> 00:06:13,280
which cannot be implemented by using a

00:06:11,680 --> 00:06:14,880
simple hpa

00:06:13,280 --> 00:06:16,880
now let's try to see and push some

00:06:14,880 --> 00:06:19,039
messages and

00:06:16,880 --> 00:06:20,960
we will be able to experience that the

00:06:19,039 --> 00:06:22,319
parts or the number of parts are getting

00:06:20,960 --> 00:06:24,160
added to the deployment

00:06:22,319 --> 00:06:25,600
as the number of messages in the queue

00:06:24,160 --> 00:06:27,840
scale up right

00:06:25,600 --> 00:06:29,600
so let me add a watch on the number of

00:06:27,840 --> 00:06:32,720
parts here

00:06:29,600 --> 00:06:35,360
all right and let me sort of zoom

00:06:32,720 --> 00:06:36,639
in a little bit so that we are able to

00:06:35,360 --> 00:06:49,840
precisely see

00:06:36,639 --> 00:06:49,840
what's exactly happening

00:06:50,240 --> 00:06:53,360
awesome now i'm going to start pumping

00:06:52,319 --> 00:06:55,919
messages to the queue

00:06:53,360 --> 00:06:56,960
and we'll see the number of parts go up

00:06:55,919 --> 00:07:02,000
all right

00:06:56,960 --> 00:07:04,479
so let me start my vs code

00:07:02,000 --> 00:07:05,440
i'm going to just start pushing messages

00:07:04,479 --> 00:07:07,599
to the queue

00:07:05,440 --> 00:07:09,280
but before that let me show you a small

00:07:07,599 --> 00:07:10,319
visualization which can help you to

00:07:09,280 --> 00:07:14,240
understand

00:07:10,319 --> 00:07:17,039
how many parts are there all right

00:07:14,240 --> 00:07:19,440
so let me start my application which is

00:07:17,039 --> 00:07:21,440
actually responsible for pushing

00:07:19,440 --> 00:07:23,759
messages to the queue i'm just going to

00:07:21,440 --> 00:07:27,280
say dotnet run it's a dot net

00:07:23,759 --> 00:07:29,440
code application to ask me how many

00:07:27,280 --> 00:07:33,120
messages do i want to really push

00:07:29,440 --> 00:07:35,280
200 fine

00:07:33,120 --> 00:07:37,599
and as you see the number of messages

00:07:35,280 --> 00:07:39,199
are going up right now

00:07:37,599 --> 00:07:40,960
and on the left hand side you'll be able

00:07:39,199 --> 00:07:43,680
to see that the parts

00:07:40,960 --> 00:07:45,520
will also be starting to create and then

00:07:43,680 --> 00:07:49,039
header will keep adding parts

00:07:45,520 --> 00:07:52,240
until the orders uh

00:07:49,039 --> 00:07:54,240
is below the threshold number now if you

00:07:52,240 --> 00:07:56,240
see parts are getting created

00:07:54,240 --> 00:07:57,440
right the order processing parts which

00:07:56,240 --> 00:07:59,360
are zero

00:07:57,440 --> 00:08:01,039
now we are adding more parts into the

00:07:59,360 --> 00:08:02,800
deployments and if you see

00:08:01,039 --> 00:08:05,680
different parts are getting created

00:08:02,800 --> 00:08:05,680
within the cluster

00:08:07,440 --> 00:08:14,240
right and then if we just pause it and

00:08:10,840 --> 00:08:17,039
see instead of the watch

00:08:14,240 --> 00:08:18,319
now we have almost one two three four

00:08:17,039 --> 00:08:20,319
parts running and if you just

00:08:18,319 --> 00:08:21,680
keep doing this you'll probably see more

00:08:20,319 --> 00:08:23,840
parts as well

00:08:21,680 --> 00:08:24,960
right so you are able to scale in the

00:08:23,840 --> 00:08:27,280
matter of seconds

00:08:24,960 --> 00:08:28,560
until the queue length goes drops below

00:08:27,280 --> 00:08:30,960
the threshold

00:08:28,560 --> 00:08:32,320
parts will also come down to zero if

00:08:30,960 --> 00:08:33,039
there are no more messages left in the

00:08:32,320 --> 00:08:34,640
queue

00:08:33,039 --> 00:08:36,479
and this is how you can implement auto

00:08:34,640 --> 00:08:38,399
scaling in response to

00:08:36,479 --> 00:08:39,599
the number of messages arriving in the

00:08:38,399 --> 00:08:41,839
queue

00:08:39,599 --> 00:08:43,919
now just look let's look at a sample

00:08:41,839 --> 00:08:48,480
yaml file which i used to

00:08:43,919 --> 00:08:50,320
deploy the uh cada scaling

00:08:48,480 --> 00:08:51,519
this says i'm using the azure service

00:08:50,320 --> 00:08:54,480
bus as

00:08:51,519 --> 00:08:56,880
a trigger source for my messages i'm

00:08:54,480 --> 00:09:00,000
monitoring the queue name called orders

00:08:56,880 --> 00:09:01,600
and the threshold is five which

00:09:00,000 --> 00:09:03,600
basically means if there are

00:09:01,600 --> 00:09:06,480
more than five messages in the queue

00:09:03,600 --> 00:09:08,720
start adding parts to the deployment

00:09:06,480 --> 00:09:10,160
and then if you see i'm configuring what

00:09:08,720 --> 00:09:11,680
is the maximum number of replicas of the

00:09:10,160 --> 00:09:15,120
parts that i can add

00:09:11,680 --> 00:09:18,000
by default the medium replicas is zero

00:09:15,120 --> 00:09:19,839
analysis i'll just use cube ctl apply

00:09:18,000 --> 00:09:23,600
for applying this configuration

00:09:19,839 --> 00:09:25,920
to my pods or to my deployments

00:09:23,600 --> 00:09:27,680
now as you see the order queue has now

00:09:25,920 --> 00:09:28,480
come down to zero because the increase

00:09:27,680 --> 00:09:31,440
in parts

00:09:28,480 --> 00:09:32,560
has led to higher processing throughput

00:09:31,440 --> 00:09:34,800
now if you see here

00:09:32,560 --> 00:09:35,760
we can just say again how many parts are

00:09:34,800 --> 00:09:37,120
there

00:09:35,760 --> 00:09:40,080
and we see almost we have reached the

00:09:37,120 --> 00:09:43,760
capacity we can also go and check

00:09:40,080 --> 00:09:47,279
the deployments or you know what is

00:09:43,760 --> 00:09:49,279
the number of ports in the deployment we

00:09:47,279 --> 00:09:51,279
have reached the maximum capacity of 10.

00:09:49,279 --> 00:09:52,560
now these 10 parts are actually

00:09:51,279 --> 00:09:54,720
processing

00:09:52,560 --> 00:09:55,680
the workload and then we'll be able to

00:09:54,720 --> 00:09:58,640
see that

00:09:55,680 --> 00:09:59,680
it also goes down to zero after the

00:09:58,640 --> 00:10:02,160
application

00:09:59,680 --> 00:10:03,200
has processed all the messages so if we

00:10:02,160 --> 00:10:05,040
wait for some more time

00:10:03,200 --> 00:10:06,640
we will see that the number of parts

00:10:05,040 --> 00:10:09,600
will drop down to zero again

00:10:06,640 --> 00:10:10,079
as data starts removing those from the

00:10:09,600 --> 00:10:13,200
uh

00:10:10,079 --> 00:10:15,440
deployments all right

00:10:13,200 --> 00:10:17,279
so that was the demo and i highly

00:10:15,440 --> 00:10:18,959
encourage you to go and explore all the

00:10:17,279 --> 00:10:23,360
samples of cada

00:10:18,959 --> 00:10:25,040
on github.com code samples

00:10:23,360 --> 00:10:26,959
please feel free to reach out to me in

00:10:25,040 --> 00:10:29,839
case you have any questions on concerns

00:10:26,959 --> 00:10:32,079
and i'll be happy to answer them

00:10:29,839 --> 00:10:34,160
thank you for being a great audience and

00:10:32,079 --> 00:10:39,279
i hope you have a great conference ahead

00:10:34,160 --> 00:10:39,279

YouTube URL: https://www.youtube.com/watch?v=qrEwtc7x74c


