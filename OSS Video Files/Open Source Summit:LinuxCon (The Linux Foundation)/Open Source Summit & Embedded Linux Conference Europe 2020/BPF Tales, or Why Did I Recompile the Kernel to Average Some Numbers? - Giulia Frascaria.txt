Title: BPF Tales, or Why Did I Recompile the Kernel to Average Some Numbers? - Giulia Frascaria
Publication date: 2020-11-13
Playlist: Open Source Summit & Embedded Linux Conference Europe 2020
Description: 
	BPF Tales, or Why Did I Recompile the Kernel to Average Some Numbers? - Giulia Frascaria, Vrije Universiteit Amsterdam
Captions: 
	00:00:04,720 --> 00:00:08,400
okay

00:00:05,759 --> 00:00:09,280
oh welcome everyone thank you for being

00:00:08,400 --> 00:00:12,400
here

00:00:09,280 --> 00:00:15,040
uh welcome to the talk the bpf details

00:00:12,400 --> 00:00:17,600
or why did i recompile the kernel just

00:00:15,040 --> 00:00:19,840
to average some numbers

00:00:17,600 --> 00:00:20,800
as you can see from the bpf logo we're

00:00:19,840 --> 00:00:23,039
gonna

00:00:20,800 --> 00:00:25,199
be dealing with some etf and as you can

00:00:23,039 --> 00:00:28,400
see from the subtitle

00:00:25,199 --> 00:00:29,119
we are also gonna have some some talk

00:00:28,400 --> 00:00:32,559
about the

00:00:29,119 --> 00:00:35,600
linux kernel internals but before

00:00:32,559 --> 00:00:37,440
i begin to explain why

00:00:35,600 --> 00:00:38,719
uh we're gonna have this talk i think i

00:00:37,440 --> 00:00:42,480
need to give you some

00:00:38,719 --> 00:00:46,640
context um first of all

00:00:42,480 --> 00:00:50,000
um i think that we all know the

00:00:46,640 --> 00:00:50,640
big data trend or as i like to call it

00:00:50,000 --> 00:00:53,760
the

00:00:50,640 --> 00:00:56,640
data the lose i'm pretty sure that

00:00:53,760 --> 00:00:58,000
most of you in the audience actually

00:00:56,640 --> 00:01:00,879
work in industries

00:00:58,000 --> 00:01:02,000
um that deal with big data because

00:01:00,879 --> 00:01:04,400
that's just

00:01:02,000 --> 00:01:05,280
too common and so widespread in the

00:01:04,400 --> 00:01:08,400
industry

00:01:05,280 --> 00:01:10,560
i'm just reporting some some data

00:01:08,400 --> 00:01:12,000
and some figures here if you want to see

00:01:10,560 --> 00:01:15,280
more you have a link

00:01:12,000 --> 00:01:19,200
at the at the bottom of the slide but

00:01:15,280 --> 00:01:22,479
yeah we live in a in a big data era as

00:01:19,200 --> 00:01:24,080
we all know and for example by 2020 by

00:01:22,479 --> 00:01:27,119
now we have 44

00:01:24,080 --> 00:01:27,920
zettabytes of stored data and companies

00:01:27,119 --> 00:01:30,799
00:01:27,920 --> 00:01:32,159
of the companies use that to perform big

00:01:30,799 --> 00:01:34,400
data analytics

00:01:32,159 --> 00:01:36,079
because it is estimated that it

00:01:34,400 --> 00:01:39,759
increases the revenue value by

00:01:36,079 --> 00:01:42,960
almost 10 every if you can implement

00:01:39,759 --> 00:01:44,000
big data analytics to tailor to your

00:01:42,960 --> 00:01:46,960
business

00:01:44,000 --> 00:01:48,960
and this is a very very big industry and

00:01:46,960 --> 00:01:53,280
by 2023 it's gonna reach

00:01:48,960 --> 00:01:56,079
a market cap of up to 77 billion dollars

00:01:53,280 --> 00:01:56,799
so clearly there's a lot of interest and

00:01:56,079 --> 00:01:59,920
a lot of

00:01:56,799 --> 00:02:01,840
attention on this topic and i'm sure all

00:01:59,920 --> 00:02:06,560
of you or most of you at least

00:02:01,840 --> 00:02:09,200
used or know about the main big data

00:02:06,560 --> 00:02:09,759
software and interfaces that are used

00:02:09,200 --> 00:02:13,360
like

00:02:09,759 --> 00:02:14,959
hadoop mapreduce or spark apache spark

00:02:13,360 --> 00:02:17,520
or for machine learning we're talking

00:02:14,959 --> 00:02:20,000
about tensorflow for example

00:02:17,520 --> 00:02:21,440
because they are widespread and they

00:02:20,000 --> 00:02:24,400
they almost became

00:02:21,440 --> 00:02:25,120
common sense and a synonym of the i.t

00:02:24,400 --> 00:02:28,319
industry

00:02:25,120 --> 00:02:31,040
as a whole but

00:02:28,319 --> 00:02:31,680
the the problem is that this is the

00:02:31,040 --> 00:02:36,080
surface

00:02:31,680 --> 00:02:38,720
so it's a highly valuable market

00:02:36,080 --> 00:02:40,319
and a lot of compass in companies are on

00:02:38,720 --> 00:02:43,760
the business

00:02:40,319 --> 00:02:46,000
but what actually happens underneath

00:02:43,760 --> 00:02:48,160
uh the hood uh we're gonna have a look

00:02:46,000 --> 00:02:51,680
at that today

00:02:48,160 --> 00:02:52,400
so clearly the data is too much to be

00:02:51,680 --> 00:02:54,800
stored in any

00:02:52,400 --> 00:02:55,920
commodity infrastructure so what usually

00:02:54,800 --> 00:02:59,280
happens is that

00:02:55,920 --> 00:03:02,800
to perform the data analytics we go to

00:02:59,280 --> 00:03:04,640
data centers data centers

00:03:02,800 --> 00:03:06,720
are a bit more complex than what this

00:03:04,640 --> 00:03:09,120
picture actually represents but this

00:03:06,720 --> 00:03:12,239
gives a fairly good schematic and

00:03:09,120 --> 00:03:13,840
summary of what i want to convey in this

00:03:12,239 --> 00:03:16,400
talk

00:03:13,840 --> 00:03:18,080
what is important to understand is that

00:03:16,400 --> 00:03:19,040
usually the architecture of the data

00:03:18,080 --> 00:03:22,080
centers

00:03:19,040 --> 00:03:23,760
is not really aggregate is rather

00:03:22,080 --> 00:03:25,840
separate between

00:03:23,760 --> 00:03:28,000
compute nodes where you know the data

00:03:25,840 --> 00:03:30,480
crunching happens and

00:03:28,000 --> 00:03:32,000
all the computation and data analytics

00:03:30,480 --> 00:03:33,760
usually take place on

00:03:32,000 --> 00:03:35,040
bulky servers with you know high

00:03:33,760 --> 00:03:36,720
resources

00:03:35,040 --> 00:03:38,640
and then connected through the network

00:03:36,720 --> 00:03:41,360
we have storage nodes which are the ones

00:03:38,640 --> 00:03:44,159
that sort of petabytes terabytes of data

00:03:41,360 --> 00:03:45,280
and then ship them to the compute nodes

00:03:44,159 --> 00:03:48,080
according to the

00:03:45,280 --> 00:03:49,920
elasticity and scalability requirements

00:03:48,080 --> 00:03:52,159
in that moment

00:03:49,920 --> 00:03:53,200
as you can see however from the storage

00:03:52,159 --> 00:03:56,799
node

00:03:53,200 --> 00:03:59,840
i didn't use the good old hard drive

00:03:56,799 --> 00:04:00,080
icon the cylinder and instead i use it i

00:03:59,840 --> 00:04:03,439
use

00:04:00,080 --> 00:04:06,319
flash and that's very important because

00:04:03,439 --> 00:04:07,439
that's where all the problems of the

00:04:06,319 --> 00:04:10,080
stock come from

00:04:07,439 --> 00:04:12,400
flash is a high performance storage

00:04:10,080 --> 00:04:15,120
technology that introduces a

00:04:12,400 --> 00:04:16,400
a new problem in data centers this

00:04:15,120 --> 00:04:18,400
problem

00:04:16,400 --> 00:04:20,400
is born from the fact that it is true

00:04:18,400 --> 00:04:23,759
that networks

00:04:20,400 --> 00:04:24,800
are very high performance so there's no

00:04:23,759 --> 00:04:28,400
doubt about that we

00:04:24,800 --> 00:04:31,360
usually there's 60 lane 16 lane pcie

00:04:28,400 --> 00:04:32,560
connections that can accommodate 16

00:04:31,360 --> 00:04:36,320
gigabytes per second of

00:04:32,560 --> 00:04:39,360
transfer between you know nodes and

00:04:36,320 --> 00:04:42,639
switches but that

00:04:39,360 --> 00:04:45,919
looks a lot but if we look at what a

00:04:42,639 --> 00:04:50,479
normal storage node looks like

00:04:45,919 --> 00:04:52,560
where for example we have 64 ssds

00:04:50,479 --> 00:04:55,600
we can see that actually the ssds with

00:04:52,560 --> 00:04:59,120
the their very good performance they can

00:04:55,600 --> 00:05:01,280
have a throughput of 128 gigabytes per

00:04:59,120 --> 00:05:02,720
second which is

00:05:01,280 --> 00:05:05,120
more than what the network can

00:05:02,720 --> 00:05:06,639
accommodate so when we are performing

00:05:05,120 --> 00:05:08,880
the transfer we can see that

00:05:06,639 --> 00:05:11,120
there's going to be a throughput gap the

00:05:08,880 --> 00:05:14,400
storage just produces the data

00:05:11,120 --> 00:05:17,600
faster than the network can accommodate

00:05:14,400 --> 00:05:18,479
and this even gets worse if we consider

00:05:17,600 --> 00:05:21,840
the

00:05:18,479 --> 00:05:24,080
internal latency and throughput of ssds

00:05:21,840 --> 00:05:25,039
which is right now not even exploited

00:05:24,080 --> 00:05:26,479
that much

00:05:25,039 --> 00:05:28,240
that's even worse because it could

00:05:26,479 --> 00:05:30,880
potentially be up to

00:05:28,240 --> 00:05:32,560
one terabyte per second actually and get

00:05:30,880 --> 00:05:35,440
to a throughput gap of

00:05:32,560 --> 00:05:37,360
66 times the one that the network can

00:05:35,440 --> 00:05:39,919
accommodate

00:05:37,360 --> 00:05:40,880
a bit more in detail for for this you

00:05:39,919 --> 00:05:43,360
can actually see

00:05:40,880 --> 00:05:44,560
in the in the article that i reported

00:05:43,360 --> 00:05:45,759
but

00:05:44,560 --> 00:05:47,840
this is the problem that i want to

00:05:45,759 --> 00:05:51,360
address it's a problem that is

00:05:47,840 --> 00:05:53,280
known as the data movement wall um and

00:05:51,360 --> 00:05:54,720
yeah it stems from the fact that

00:05:53,280 --> 00:05:57,039
actually moving data

00:05:54,720 --> 00:05:58,639
from storage to compute nodes across the

00:05:57,039 --> 00:06:02,080
network

00:05:58,639 --> 00:06:03,440
um is a bottleneck in the current

00:06:02,080 --> 00:06:08,000
industry

00:06:03,440 --> 00:06:10,560
and data center architecture so

00:06:08,000 --> 00:06:11,360
we are at a point where we see that you

00:06:10,560 --> 00:06:14,560
know data

00:06:11,360 --> 00:06:15,600
is just increasing we gather more and

00:06:14,560 --> 00:06:18,400
more that and

00:06:15,600 --> 00:06:19,759
in order to make sense out of it we need

00:06:18,400 --> 00:06:22,880
to compute

00:06:19,759 --> 00:06:23,919
information and move it to the compute

00:06:22,880 --> 00:06:26,400
nodes

00:06:23,919 --> 00:06:27,919
however at the same time we have the

00:06:26,400 --> 00:06:31,280
storage that is

00:06:27,919 --> 00:06:32,000
becoming too fast to move it efficiently

00:06:31,280 --> 00:06:34,400
and

00:06:32,000 --> 00:06:35,199
now data movement is becoming the bottom

00:06:34,400 --> 00:06:38,240
line

00:06:35,199 --> 00:06:41,360
as i said this is commonly referred to

00:06:38,240 --> 00:06:43,759
as a data movement world

00:06:41,360 --> 00:06:45,120
this is the the problem that we're going

00:06:43,759 --> 00:06:47,919
to talk about in this

00:06:45,120 --> 00:06:50,479
in this talk today and try to solve and

00:06:47,919 --> 00:06:50,479
mitigate

00:06:51,039 --> 00:06:57,840
um at this point i think that

00:06:54,400 --> 00:07:00,080
it's time that i give a hint

00:06:57,840 --> 00:07:02,160
about the solution because it took some

00:07:00,080 --> 00:07:05,759
time to develop it almost

00:07:02,160 --> 00:07:09,039
took a year of work for me um

00:07:05,759 --> 00:07:12,800
but the idea that we are gonna address

00:07:09,039 --> 00:07:13,360
now is that in order to reduce the data

00:07:12,800 --> 00:07:16,800
movement

00:07:13,360 --> 00:07:18,000
we want to possibly move a bit of the

00:07:16,800 --> 00:07:21,840
computation

00:07:18,000 --> 00:07:24,960
from the compute nodes to the storage

00:07:21,840 --> 00:07:26,080
and in this way we aim at optimizing the

00:07:24,960 --> 00:07:28,639
data transfer

00:07:26,080 --> 00:07:30,560
so instead hopefully of transferring

00:07:28,639 --> 00:07:32,720
terabytes of data we maybe

00:07:30,560 --> 00:07:34,080
are gonna have to just do a bit less

00:07:32,720 --> 00:07:36,160
than that and

00:07:34,080 --> 00:07:38,800
just have an overall optimization and

00:07:36,160 --> 00:07:42,319
mitigate this data movement world

00:07:38,800 --> 00:07:44,000
we're gonna do this and

00:07:42,319 --> 00:07:45,440
this means that we're going to use some

00:07:44,000 --> 00:07:48,000
programmability which is a

00:07:45,440 --> 00:07:48,960
modern trend that is just happening all

00:07:48,000 --> 00:07:52,960
across the board

00:07:48,960 --> 00:07:54,319
um networks already have a very strong

00:07:52,960 --> 00:07:56,560
programmability aspect

00:07:54,319 --> 00:07:58,960
for example in the in the switches it's

00:07:56,560 --> 00:08:01,680
something that is really happening

00:07:58,960 --> 00:08:02,960
uh not so much in storage and that's

00:08:01,680 --> 00:08:07,360
what we're gonna try to do

00:08:02,960 --> 00:08:10,479
instead in particular with ebpf

00:08:07,360 --> 00:08:13,360
uh but before we start with the

00:08:10,479 --> 00:08:14,720
real details of the of the talk i guess

00:08:13,360 --> 00:08:18,400
that i need to

00:08:14,720 --> 00:08:21,039
introduce myself i am julia

00:08:18,400 --> 00:08:22,720
um i come from italy but the the work

00:08:21,039 --> 00:08:23,039
that i'm about to present was produced

00:08:22,720 --> 00:08:26,160
in

00:08:23,039 --> 00:08:29,520
in the high university of amsterdam

00:08:26,160 --> 00:08:31,440
where i am a soon to be phd

00:08:29,520 --> 00:08:32,640
candidate for the at large research

00:08:31,440 --> 00:08:35,839
group which is the

00:08:32,640 --> 00:08:39,120
group for which i also did this

00:08:35,839 --> 00:08:41,760
work in our research group

00:08:39,120 --> 00:08:43,039
we really have a strong focus on data

00:08:41,760 --> 00:08:46,240
center performance

00:08:43,039 --> 00:08:48,000
and big data workloads analysis

00:08:46,240 --> 00:08:50,240
and we try to optimize all sorts of

00:08:48,000 --> 00:08:53,440
things from scheduling decisions

00:08:50,240 --> 00:08:55,279
in data centers to network performance

00:08:53,440 --> 00:08:56,800
variability to make it as stable as

00:08:55,279 --> 00:08:58,880
possible and now

00:08:56,800 --> 00:09:00,560
my work will for example focus on the

00:08:58,880 --> 00:09:04,080
storage side so this is

00:09:00,560 --> 00:09:08,240
why um i'm having this angle on the

00:09:04,080 --> 00:09:10,720
on the top and on the topic um

00:09:08,240 --> 00:09:12,640
now that we know what's the problem so

00:09:10,720 --> 00:09:15,600
the data movement work

00:09:12,640 --> 00:09:16,880
and what is actually who is trying to

00:09:15,600 --> 00:09:20,399
solve it that is me

00:09:16,880 --> 00:09:24,160
i think that we can begin um

00:09:20,399 --> 00:09:27,440
and talk about the details uh so

00:09:24,160 --> 00:09:29,040
we know that as i mentioned these are

00:09:27,440 --> 00:09:31,040
the data centers

00:09:29,040 --> 00:09:32,800
and when we are talking about big data

00:09:31,040 --> 00:09:33,920
analytics for example we might want to

00:09:32,800 --> 00:09:37,040
do something like compute

00:09:33,920 --> 00:09:41,200
the average of numbers

00:09:37,040 --> 00:09:41,200
what happens when we need to do this

00:09:41,600 --> 00:09:48,959
well the compute node or rather the user

00:09:46,320 --> 00:09:50,080
makes a request so ask the storage node

00:09:48,959 --> 00:09:53,040
for some numbers

00:09:50,080 --> 00:09:54,560
that they need to compute the average on

00:09:53,040 --> 00:09:57,440
this travels through the network

00:09:54,560 --> 00:09:58,320
the storage node on his behalf produces

00:09:57,440 --> 00:10:02,320
the data

00:09:58,320 --> 00:10:04,880
and that that is then transferred

00:10:02,320 --> 00:10:07,519
across the network slowly but surely and

00:10:04,880 --> 00:10:10,800
it's gonna eventually reach the

00:10:07,519 --> 00:10:13,200
compute node which starts to compute the

00:10:10,800 --> 00:10:14,640
results and comes up with a result that

00:10:13,200 --> 00:10:16,839
looks something like the average is

00:10:14,640 --> 00:10:20,959
actually 42.

00:10:16,839 --> 00:10:23,600
um this is good but as we saw

00:10:20,959 --> 00:10:24,160
the if we're talking about gigabytes of

00:10:23,600 --> 00:10:26,800
data

00:10:24,160 --> 00:10:27,760
terabytes of data moving that across the

00:10:26,800 --> 00:10:31,200
network

00:10:27,760 --> 00:10:32,079
is not that convenient and this does not

00:10:31,200 --> 00:10:35,360
really make sense

00:10:32,079 --> 00:10:36,399
if we think about it we performed a huge

00:10:35,360 --> 00:10:39,920
data transfer

00:10:36,399 --> 00:10:42,640
which has introduced latency because

00:10:39,920 --> 00:10:43,680
the storage node potentially was ready

00:10:42,640 --> 00:10:45,519
with that data

00:10:43,680 --> 00:10:48,480
way earlier than the network could then

00:10:45,519 --> 00:10:50,320
consume it and transfer it

00:10:48,480 --> 00:10:51,760
this introduced unnecessary network

00:10:50,320 --> 00:10:55,440
congestion potentially

00:10:51,760 --> 00:10:58,560
and it had a bottleneck on storage

00:10:55,440 --> 00:11:00,320
and all for something that looks like a

00:10:58,560 --> 00:11:00,959
number we computed an average which is a

00:11:00,320 --> 00:11:04,160
single

00:11:00,959 --> 00:11:05,920
digit potentially number

00:11:04,160 --> 00:11:08,399
that came from gigabytes and gigabytes

00:11:05,920 --> 00:11:11,200
of data transfer

00:11:08,399 --> 00:11:12,160
well the the idea of this talk is to

00:11:11,200 --> 00:11:14,560
optimize this

00:11:12,160 --> 00:11:15,279
with programmability on storage so what

00:11:14,560 --> 00:11:16,880
we're going to do

00:11:15,279 --> 00:11:19,279
is instead we're going to move the

00:11:16,880 --> 00:11:21,519
computation on the storage

00:11:19,279 --> 00:11:23,519
somehow this is going to look a bit like

00:11:21,519 --> 00:11:27,440
this at this point

00:11:23,519 --> 00:11:29,920
the compute node will instruct

00:11:27,440 --> 00:11:31,440
the storage with whatever function that

00:11:29,920 --> 00:11:34,320
they need to compute in this moment is

00:11:31,440 --> 00:11:37,279
going to be again the average

00:11:34,320 --> 00:11:37,519
and the storage on his behalf learns how

00:11:37,279 --> 00:11:40,880
to

00:11:37,519 --> 00:11:42,160
perform this operation somehow and then

00:11:40,880 --> 00:11:45,120
retreats the data and

00:11:42,160 --> 00:11:45,440
immediately starts to compute the result

00:11:45,120 --> 00:11:47,279
and

00:11:45,440 --> 00:11:49,600
comes up with the result the average is

00:11:47,279 --> 00:11:50,399
again 0.2 and only that is then

00:11:49,600 --> 00:11:53,360
transferred

00:11:50,399 --> 00:11:53,360
to the compute node

00:11:53,680 --> 00:11:58,000
now we we can see that this is a bit

00:11:55,839 --> 00:12:01,120
different than what we did before and

00:11:58,000 --> 00:12:02,800
it makes more sense because we reduce

00:12:01,120 --> 00:12:04,160
the data transfer we only transfer the

00:12:02,800 --> 00:12:06,560
results

00:12:04,160 --> 00:12:07,760
which by the way also save some money if

00:12:06,560 --> 00:12:09,839
we are paying for

00:12:07,760 --> 00:12:12,959
only a byte of network transfer instead

00:12:09,839 --> 00:12:14,959
of year by 10 gigabytes of data

00:12:12,959 --> 00:12:16,720
it reduced the network congestion which

00:12:14,959 --> 00:12:19,839
is always a

00:12:16,720 --> 00:12:23,279
desirable thing to do in a data center

00:12:19,839 --> 00:12:24,639
and we could actually you know leverage

00:12:23,279 --> 00:12:26,800
the high throughput of the storage

00:12:24,639 --> 00:12:29,360
because it was not you know

00:12:26,800 --> 00:12:30,800
waiting for the network to you know go

00:12:29,360 --> 00:12:32,959
through all the transfer

00:12:30,800 --> 00:12:36,079
before it would actually start

00:12:32,959 --> 00:12:36,079
performing the computation

00:12:36,839 --> 00:12:43,519
now we saw that we want to

00:12:40,639 --> 00:12:46,320
move this to the storage how can we

00:12:43,519 --> 00:12:49,760
actually do this though

00:12:46,320 --> 00:12:51,760
um well we still

00:12:49,760 --> 00:12:52,959
want to keep all the data center

00:12:51,760 --> 00:12:54,959
requirements in mind

00:12:52,959 --> 00:12:56,399
we are not giving up anything from the

00:12:54,959 --> 00:12:58,800
point of view of multi-tenancy

00:12:56,399 --> 00:13:00,639
multiple users need to be able to

00:12:58,800 --> 00:13:02,560
perform their own you know computation

00:13:00,639 --> 00:13:05,680
on the data

00:13:02,560 --> 00:13:06,959
and so of course all the users still

00:13:05,680 --> 00:13:10,000
need to be isolated

00:13:06,959 --> 00:13:11,920
between themselves of course we

00:13:10,000 --> 00:13:13,279
want to have as high of a performance as

00:13:11,920 --> 00:13:16,160
possible

00:13:13,279 --> 00:13:17,839
and both the users the tenants and the

00:13:16,160 --> 00:13:21,839
data center operators

00:13:17,839 --> 00:13:25,839
would rather have this implemented with

00:13:21,839 --> 00:13:29,120
as low of a deployment cost as possible

00:13:25,839 --> 00:13:30,079
in addition to this though the solution

00:13:29,120 --> 00:13:33,600
that i'm looking for

00:13:30,079 --> 00:13:35,680
also aims at reducing the data movement

00:13:33,600 --> 00:13:38,320
so we have this additional requirements

00:13:35,680 --> 00:13:38,320
to keep in mind

00:13:38,560 --> 00:13:43,440
with this list of requirements we can go

00:13:41,920 --> 00:13:47,199
on and explore

00:13:43,440 --> 00:13:51,760
the space and look for a solution

00:13:47,199 --> 00:13:51,760
what are the options on to do this

00:13:53,440 --> 00:13:59,519
so we all know that data centers

00:13:56,639 --> 00:14:00,480
are made of computers so most likely

00:13:59,519 --> 00:14:03,279
they will have

00:14:00,480 --> 00:14:04,880
operating systems installed this means

00:14:03,279 --> 00:14:07,839
that the

00:14:04,880 --> 00:14:08,800
computation that the users can run their

00:14:07,839 --> 00:14:11,680
own applications

00:14:08,800 --> 00:14:13,040
so for example we could think about

00:14:11,680 --> 00:14:15,360
implementing the average

00:14:13,040 --> 00:14:16,240
in any kind of programming language that

00:14:15,360 --> 00:14:19,279
we can think of

00:14:16,240 --> 00:14:20,480
so we could do this programmability

00:14:19,279 --> 00:14:23,760
aspect on

00:14:20,480 --> 00:14:26,240
user space and let the users just define

00:14:23,760 --> 00:14:30,320
their own functions and do that

00:14:26,240 --> 00:14:32,160
or another solution that

00:14:30,320 --> 00:14:34,000
is usually for example something that

00:14:32,160 --> 00:14:37,680
the networking aspect goes

00:14:34,000 --> 00:14:40,240
um goes to is the hardware part

00:14:37,680 --> 00:14:43,199
so optimizing small operations with

00:14:40,240 --> 00:14:45,920
optimized hardware like asics fpgas or

00:14:43,199 --> 00:14:46,959
gpus and that's a very common trend

00:14:45,920 --> 00:14:49,600
nowadays

00:14:46,959 --> 00:14:50,880
but we see that there is a gap in

00:14:49,600 --> 00:14:52,720
between

00:14:50,880 --> 00:14:53,920
what the users could decide to implement

00:14:52,720 --> 00:14:56,079
in user space

00:14:53,920 --> 00:14:58,240
and the hardware which is then more

00:14:56,079 --> 00:15:01,279
expensive to deploy which goes against

00:14:58,240 --> 00:15:03,760
our requirement but there is the kernel

00:15:01,279 --> 00:15:06,880
space in between

00:15:03,760 --> 00:15:09,680
and actually turns out that the

00:15:06,880 --> 00:15:11,519
data centers usually as you may know run

00:15:09,680 --> 00:15:14,959
linux

00:15:11,519 --> 00:15:17,680
and actually linux since a few years has

00:15:14,959 --> 00:15:20,720
been shipping with ebpf

00:15:17,680 --> 00:15:23,279
which is which means uh extended

00:15:20,720 --> 00:15:26,800
berkeley pocket builder

00:15:23,279 --> 00:15:30,079
what is a bpf exactly um

00:15:26,800 --> 00:15:34,399
some people refer to it as a javascript

00:15:30,079 --> 00:15:36,240
for the kernel so it is a

00:15:34,399 --> 00:15:38,880
programming language basically it's an

00:15:36,240 --> 00:15:42,079
instruction set so if you want to

00:15:38,880 --> 00:15:44,880
go hardcore you could write ebpf

00:15:42,079 --> 00:15:46,160
in an assembly like language but it also

00:15:44,880 --> 00:15:48,240
has a c interface

00:15:46,160 --> 00:15:49,680
and which makes it a bit more

00:15:48,240 --> 00:15:51,920
approachable

00:15:49,680 --> 00:15:53,199
which allows every user to write their

00:15:51,920 --> 00:15:55,680
own

00:15:53,199 --> 00:15:57,199
code and then executed in the kernel

00:15:55,680 --> 00:16:00,000
which

00:15:57,199 --> 00:16:01,519
happens without actually reinstalling

00:16:00,000 --> 00:16:03,279
the kernel

00:16:01,519 --> 00:16:05,839
it's a bit like a current module you can

00:16:03,279 --> 00:16:07,600
load it and unload it as needed

00:16:05,839 --> 00:16:09,279
but it has the added benefit from the

00:16:07,600 --> 00:16:13,440
kernel module that

00:16:09,279 --> 00:16:15,040
abpf is actually formally verified

00:16:13,440 --> 00:16:16,959
it will be a very bad idea to allow

00:16:15,040 --> 00:16:19,759
users in their centers to run their own

00:16:16,959 --> 00:16:22,000
code like in kernel space we we know and

00:16:19,759 --> 00:16:24,560
we don't want to do that and

00:16:22,000 --> 00:16:26,160
but if it provides a safety net for that

00:16:24,560 --> 00:16:28,240
because the verification

00:16:26,160 --> 00:16:29,360
guarantees that the code is going to

00:16:28,240 --> 00:16:31,680
terminate

00:16:29,360 --> 00:16:33,360
it's not going to crush the kernel it's

00:16:31,680 --> 00:16:35,279
going to terminate in a fairly limited

00:16:33,360 --> 00:16:37,759
amount of time because you cannot run

00:16:35,279 --> 00:16:40,000
indefinitely it has a maximum

00:16:37,759 --> 00:16:43,120
instruction number

00:16:40,000 --> 00:16:44,639
so it is gonna terminate pretty quickly

00:16:43,120 --> 00:16:46,160
actually

00:16:44,639 --> 00:16:47,839
and it's not gonna corrupt kernel

00:16:46,160 --> 00:16:50,560
structures so

00:16:47,839 --> 00:16:52,160
the safety guarantees are pretty solid

00:16:50,560 --> 00:16:55,839
and

00:16:52,160 --> 00:16:57,759
that's why ebpf is actually being used

00:16:55,839 --> 00:16:59,839
more and more in kernel because it

00:16:57,759 --> 00:17:01,759
allows to change function behaviors

00:16:59,839 --> 00:17:03,519
without having to update the kernel

00:17:01,759 --> 00:17:04,720
and reinstall from scratch which is not

00:17:03,519 --> 00:17:07,600
something that you want to do in a data

00:17:04,720 --> 00:17:07,600
center constantly

00:17:07,919 --> 00:17:12,799
in particular as i said mpf stands for

00:17:10,880 --> 00:17:14,640
berkeley pocket filter

00:17:12,799 --> 00:17:16,400
extend it now because it has some added

00:17:14,640 --> 00:17:20,160
functionalities since

00:17:16,400 --> 00:17:23,360
what it could do at the beginning

00:17:20,160 --> 00:17:24,480
uh but as i said berkeley packet filter

00:17:23,360 --> 00:17:26,559
makes you

00:17:24,480 --> 00:17:28,799
understand that it's mostly for

00:17:26,559 --> 00:17:31,280
networking initially

00:17:28,799 --> 00:17:32,160
and in fact for the network on the

00:17:31,280 --> 00:17:36,000
network stack

00:17:32,160 --> 00:17:39,280
it can do some very powerful things like

00:17:36,000 --> 00:17:40,960
in-flight packet inspection

00:17:39,280 --> 00:17:43,280
then you can actually even modify the

00:17:40,960 --> 00:17:47,200
packets so for example changing the

00:17:43,280 --> 00:17:49,440
address from ipv4 to ipv6 or

00:17:47,200 --> 00:17:51,919
changing just the ip and implementing

00:17:49,440 --> 00:17:54,559
some sort of load balancing

00:17:51,919 --> 00:17:56,080
or even dropping packets if you if you

00:17:54,559 --> 00:17:58,880
want to implement some sort of

00:17:56,080 --> 00:18:00,080
you know denial of service prevention

00:17:58,880 --> 00:18:01,679
and just drop packets

00:18:00,080 --> 00:18:03,440
before they even enter the kernel

00:18:01,679 --> 00:18:06,799
because you can

00:18:03,440 --> 00:18:09,520
place ebtf code in the network interface

00:18:06,799 --> 00:18:12,720
card basically

00:18:09,520 --> 00:18:14,720
all these things look very powerful

00:18:12,720 --> 00:18:16,799
tools that you can have and actually the

00:18:14,720 --> 00:18:20,000
way i look at them

00:18:16,799 --> 00:18:23,520
is that they look like just

00:18:20,000 --> 00:18:26,160
primitives a read write and drop on

00:18:23,520 --> 00:18:29,360
buffers which are packets in the

00:18:26,160 --> 00:18:32,320
networking case and in the i o

00:18:29,360 --> 00:18:33,200
case from file they will likely be just

00:18:32,320 --> 00:18:37,200
file data

00:18:33,200 --> 00:18:40,640
coming in going in the kernel

00:18:37,200 --> 00:18:41,520
so the the idea right now that i had for

00:18:40,640 --> 00:18:44,000
these thesis

00:18:41,520 --> 00:18:45,200
is that this looks like something that i

00:18:44,000 --> 00:18:48,160
could do on storage and

00:18:45,200 --> 00:18:51,039
implement just what i had in mind packet

00:18:48,160 --> 00:18:51,039
filters namely

00:18:51,120 --> 00:18:54,799
um in order to do that though we need to

00:18:54,080 --> 00:18:57,840
understand that

00:18:54,799 --> 00:18:59,679
while ebpf was born for networking as

00:18:57,840 --> 00:19:01,280
and is really well established on that

00:18:59,679 --> 00:19:04,080
side

00:19:01,280 --> 00:19:05,919
it's not through at the same level for

00:19:04,080 --> 00:19:09,200
the storage stack

00:19:05,919 --> 00:19:10,559
so for example in the networking stack

00:19:09,200 --> 00:19:13,600
you can just

00:19:10,559 --> 00:19:16,799
attach to a socket and

00:19:13,600 --> 00:19:17,760
go from there in the i o step it might

00:19:16,799 --> 00:19:21,440
be a bit different

00:19:17,760 --> 00:19:22,720
and we actually don't really have those

00:19:21,440 --> 00:19:24,240
tools so first of all i need to

00:19:22,720 --> 00:19:25,039
understand how the code looks like to

00:19:24,240 --> 00:19:27,840
understand

00:19:25,039 --> 00:19:30,000
what i can actually do in this in this

00:19:27,840 --> 00:19:32,640
use case

00:19:30,000 --> 00:19:33,280
so don't worry it's not gonna last too

00:19:32,640 --> 00:19:34,960
long but

00:19:33,280 --> 00:19:37,039
we're gonna go through a little bit of

00:19:34,960 --> 00:19:40,480
kernel code before

00:19:37,039 --> 00:19:40,480
proposing the actual solution

00:19:42,559 --> 00:19:47,440
um as i said first of all

00:19:45,600 --> 00:19:49,760
it is important to understand what's the

00:19:47,440 --> 00:19:53,200
difference between networking

00:19:49,760 --> 00:19:56,400
and io staff tab in

00:19:53,200 --> 00:19:59,760
my use case ideally what i want to

00:19:56,400 --> 00:20:04,000
intercept is the read tab so when a user

00:19:59,760 --> 00:20:06,159
asks for a read and read a file

00:20:04,000 --> 00:20:07,919
somewhere in the code i'm gonna have to

00:20:06,159 --> 00:20:10,880
insert my bpf

00:20:07,919 --> 00:20:13,039
extension that is gonna perform the

00:20:10,880 --> 00:20:14,720
computation that i need

00:20:13,039 --> 00:20:16,240
so first of all i need to understand how

00:20:14,720 --> 00:20:19,760
the code looks like and i

00:20:16,240 --> 00:20:21,440
collected the trace of the

00:20:19,760 --> 00:20:23,200
of the call trace in the kernel for a

00:20:21,440 --> 00:20:26,640
read system called

00:20:23,200 --> 00:20:28,480
and it's a bit complex but i

00:20:26,640 --> 00:20:30,640
i knew the requirements and how to

00:20:28,480 --> 00:20:34,720
navigate that in order to find the right

00:20:30,640 --> 00:20:37,600
touch point and while analyzing that

00:20:34,720 --> 00:20:39,120
somewhere i knew that i had to look for

00:20:37,600 --> 00:20:41,360
a way to

00:20:39,120 --> 00:20:43,440
identify the user and the file that were

00:20:41,360 --> 00:20:44,320
being targeted so that in a data center

00:20:43,440 --> 00:20:46,080
scenario we're going to

00:20:44,320 --> 00:20:47,600
be able to distinguish between multiple

00:20:46,080 --> 00:20:50,000
users

00:20:47,600 --> 00:20:51,520
of course it needs to be a function that

00:20:50,000 --> 00:20:55,039
allows me to inspect

00:20:51,520 --> 00:20:58,799
the data that is flying in

00:20:55,039 --> 00:21:01,280
in the buffers and this

00:20:58,799 --> 00:21:03,039
can happen because ebpf allows to access

00:21:01,280 --> 00:21:03,679
the function arguments when they are

00:21:03,039 --> 00:21:06,000
called

00:21:03,679 --> 00:21:07,440
so i'm looking for a function that gives

00:21:06,000 --> 00:21:10,320
me observability

00:21:07,440 --> 00:21:11,919
on the data that is being transferred

00:21:10,320 --> 00:21:15,440
and from that point on

00:21:11,919 --> 00:21:17,039
i'm gonna enter my btf code and execute

00:21:15,440 --> 00:21:19,600
whatever the user

00:21:17,039 --> 00:21:22,000
told me to execute for that specific

00:21:19,600 --> 00:21:22,000
file

00:21:22,640 --> 00:21:28,480
what i found out is that somewhere in

00:21:25,440 --> 00:21:30,320
that call trace that i

00:21:28,480 --> 00:21:31,760
summarized earlier on that was just an

00:21:30,320 --> 00:21:33,440
excerpt

00:21:31,760 --> 00:21:35,360
there is a function that is called copy

00:21:33,440 --> 00:21:38,320
out and

00:21:35,360 --> 00:21:38,640
it copies from somewhere to somewhere

00:21:38,320 --> 00:21:41,760
else

00:21:38,640 --> 00:21:43,440
for a certain size and this looks like

00:21:41,760 --> 00:21:44,000
something that i could actually use

00:21:43,440 --> 00:21:46,480
because i

00:21:44,000 --> 00:21:48,240
have close to all the information that i

00:21:46,480 --> 00:21:51,200
need

00:21:48,240 --> 00:21:53,039
all the calls before were not really

00:21:51,200 --> 00:21:54,559
suitable because they didn't have access

00:21:53,039 --> 00:21:56,240
to the raw data

00:21:54,559 --> 00:21:58,320
there was some metadata in between and

00:21:56,240 --> 00:22:00,400
we we thought that it was better to

00:21:58,320 --> 00:22:03,280
avoid the user having

00:22:00,400 --> 00:22:05,200
uh capability to access kernel metadata

00:22:03,280 --> 00:22:07,120
that doesn't look like a good idea

00:22:05,200 --> 00:22:08,720
so we had to exclude those ones even

00:22:07,120 --> 00:22:11,360
though they had some

00:22:08,720 --> 00:22:12,880
good benefits of for example reporting

00:22:11,360 --> 00:22:14,640
the file name

00:22:12,880 --> 00:22:17,360
which was a good way to identify the

00:22:14,640 --> 00:22:19,360
user by the way

00:22:17,360 --> 00:22:21,679
anything below that becomes architecture

00:22:19,360 --> 00:22:24,480
specific it's assembly code for example

00:22:21,679 --> 00:22:26,080
different from intel to arm to md

00:22:24,480 --> 00:22:28,480
and we didn't want to really go there

00:22:26,080 --> 00:22:30,559
because then whatever we wrote

00:22:28,480 --> 00:22:32,080
would be specific to only one

00:22:30,559 --> 00:22:35,039
architecture which is not

00:22:32,080 --> 00:22:36,159
not nice to have as a requirement and as

00:22:35,039 --> 00:22:39,840
a

00:22:36,159 --> 00:22:40,559
feature not really so we stood with copy

00:22:39,840 --> 00:22:42,960
out

00:22:40,559 --> 00:22:44,559
and we decided to identify the user with

00:22:42,960 --> 00:22:48,640
the the tuple

00:22:44,559 --> 00:22:50,240
process id and buffer address

00:22:48,640 --> 00:22:52,080
because we can communicate that from the

00:22:50,240 --> 00:22:54,640
user space to the

00:22:52,080 --> 00:22:56,400
to the copy out function and that works

00:22:54,640 --> 00:22:57,120
well enough assuming that the address

00:22:56,400 --> 00:23:00,159
space

00:22:57,120 --> 00:23:02,400
is large enough which it is nowadays

00:23:00,159 --> 00:23:04,159
so we figured that it gave a pretty

00:23:02,400 --> 00:23:08,159
unique identifier for the user and the

00:23:04,159 --> 00:23:10,640
file that was being accessed

00:23:08,159 --> 00:23:12,640
so this is where we insert the k pro

00:23:10,640 --> 00:23:15,120
which is the

00:23:12,640 --> 00:23:16,320
program type that bpf can use to

00:23:15,120 --> 00:23:18,799
intercept this

00:23:16,320 --> 00:23:23,840
and then performs a random computation

00:23:18,799 --> 00:23:23,840
not not random but user defined rather

00:23:24,799 --> 00:23:28,080
now that we know all of this it doesn't

00:23:27,360 --> 00:23:32,400
look like

00:23:28,080 --> 00:23:34,840
that difficult so actually

00:23:32,400 --> 00:23:36,240
everything is going according to plan

00:23:34,840 --> 00:23:38,320
right

00:23:36,240 --> 00:23:40,080
in a way yeah at this point we were able

00:23:38,320 --> 00:23:42,559
to implement the prototype

00:23:40,080 --> 00:23:43,600
and the structure that we decided to use

00:23:42,559 --> 00:23:46,880
was to offer

00:23:43,600 --> 00:23:48,159
some restrictions to the user that would

00:23:46,880 --> 00:23:51,600
aid

00:23:48,159 --> 00:23:54,240
the deployment of some

00:23:51,600 --> 00:23:55,200
and offloading of some functions but

00:23:54,240 --> 00:23:56,880
without giving

00:23:55,200 --> 00:23:59,200
too much freedom because then it would

00:23:56,880 --> 00:24:01,520
be hard to navigate the code because

00:23:59,200 --> 00:24:03,120
the kernel code is still a bit complex

00:24:01,520 --> 00:24:06,000
so we try to give the user a

00:24:03,120 --> 00:24:07,279
friendly interface where they can

00:24:06,000 --> 00:24:11,039
implement

00:24:07,279 --> 00:24:12,640
a two-step filter that we call filter

00:24:11,039 --> 00:24:15,760
reduce

00:24:12,640 --> 00:24:16,720
and ideally it takes the input that is

00:24:15,760 --> 00:24:20,080
the whole buffer

00:24:16,720 --> 00:24:22,400
and file it filters so it reduces the

00:24:20,080 --> 00:24:24,480
size a little bit and then the reduce

00:24:22,400 --> 00:24:26,720
outputs a single number which is for

00:24:24,480 --> 00:24:29,679
example the average

00:24:26,720 --> 00:24:31,520
so examples of operations is that you

00:24:29,679 --> 00:24:33,360
filter and only keep the numbers that

00:24:31,520 --> 00:24:34,080
are more than 5 and then compute the

00:24:33,360 --> 00:24:35,919
average

00:24:34,080 --> 00:24:38,000
or you can do something different only

00:24:35,919 --> 00:24:41,200
keep the numbers that are equal to 18

00:24:38,000 --> 00:24:43,120
and then count them

00:24:41,200 --> 00:24:44,720
and of course you can mix and match as

00:24:43,120 --> 00:24:47,840
you prefer

00:24:44,720 --> 00:24:51,440
you can do mean max count exists

00:24:47,840 --> 00:24:54,559
um is equal to you name it

00:24:51,440 --> 00:24:58,159
this functions that result in a single

00:24:54,559 --> 00:24:58,159
numerical output

00:24:58,480 --> 00:25:01,760
why did we choose this design as i

00:25:00,799 --> 00:25:05,120
mentioned we

00:25:01,760 --> 00:25:07,039
wanted to keep a standard in the

00:25:05,120 --> 00:25:09,120
interface

00:25:07,039 --> 00:25:11,440
but still give the user some flexibility

00:25:09,120 --> 00:25:13,200
because we don't actually know what

00:25:11,440 --> 00:25:14,480
computation needs to happen for every

00:25:13,200 --> 00:25:17,840
use case so

00:25:14,480 --> 00:25:19,679
in this way we figure that this looks

00:25:17,840 --> 00:25:22,480
very much like a map reduce you can

00:25:19,679 --> 00:25:25,520
implement a mapreduce like

00:25:22,480 --> 00:25:28,559
function with a filter reduce and it's

00:25:25,520 --> 00:25:31,679
very used in relational data processing

00:25:28,559 --> 00:25:32,559
so for example we looked at the tpch

00:25:31,679 --> 00:25:34,799
benchmarking

00:25:32,559 --> 00:25:36,799
and we saw that all the benchmarking

00:25:34,799 --> 00:25:38,559
queries actually perform

00:25:36,799 --> 00:25:40,960
more than one filter and one reduce

00:25:38,559 --> 00:25:44,400
operation

00:25:40,960 --> 00:25:47,919
so this was looked like a

00:25:44,400 --> 00:25:50,880
a good um in between

00:25:47,919 --> 00:25:52,640
and compromise between the full freedom

00:25:50,880 --> 00:25:53,760
of implementing any kind of filter that

00:25:52,640 --> 00:25:56,640
you want

00:25:53,760 --> 00:25:57,760
and the ease of access of only having to

00:25:56,640 --> 00:26:00,880
implement

00:25:57,760 --> 00:26:03,679
num uh two functions in a very specific

00:26:00,880 --> 00:26:05,120
api and this leads to the maximum data

00:26:03,679 --> 00:26:07,039
reduction because it goes from a full

00:26:05,120 --> 00:26:08,880
buffer to a single number

00:26:07,039 --> 00:26:10,080
so that's we fear that this is pretty

00:26:08,880 --> 00:26:11,919
convenient to solve the

00:26:10,080 --> 00:26:15,440
and mitigate at least the issue of data

00:26:11,919 --> 00:26:15,440
movement that we were looking at before

00:26:15,919 --> 00:26:20,000
so actually for one last time we can

00:26:18,400 --> 00:26:24,960
actually have a look at the data center

00:26:20,000 --> 00:26:27,440
and what is happening now

00:26:24,960 --> 00:26:29,120
the once again the cpu and the user will

00:26:27,440 --> 00:26:29,600
request and instruct the source to

00:26:29,120 --> 00:26:33,760
perform

00:26:29,600 --> 00:26:34,799
a computation the ebpf extension will

00:26:33,760 --> 00:26:37,440
actually

00:26:34,799 --> 00:26:39,200
intercept the transfer while it is

00:26:37,440 --> 00:26:42,000
happening

00:26:39,200 --> 00:26:43,520
and then it's only gonna actually return

00:26:42,000 --> 00:26:47,360
to the user land

00:26:43,520 --> 00:26:50,240
the result and that's pretty

00:26:47,360 --> 00:26:51,120
convenient because in this way what we

00:26:50,240 --> 00:26:53,679
achieved

00:26:51,120 --> 00:26:54,960
was all the all the above that we saw

00:26:53,679 --> 00:26:56,720
about programmability

00:26:54,960 --> 00:26:58,559
so we managed to reduce network

00:26:56,720 --> 00:27:02,320
congestion

00:26:58,559 --> 00:27:04,720
uh but in addition to that we have this

00:27:02,320 --> 00:27:06,080
additional gain of avoiding the copy of

00:27:04,720 --> 00:27:09,679
the data from kernel

00:27:06,080 --> 00:27:11,039
to user space and the user does not have

00:27:09,679 --> 00:27:14,480
access to the data

00:27:11,039 --> 00:27:18,130
in user space so it kind of acts as a

00:27:14,480 --> 00:27:19,440
small sandbox inside the kernel

00:27:18,130 --> 00:27:23,279
[Music]

00:27:19,440 --> 00:27:27,039
this looks very good but once again

00:27:23,279 --> 00:27:30,159
that did it go according to plan um

00:27:27,039 --> 00:27:32,320
well you read the title so the fact that

00:27:30,159 --> 00:27:34,640
i had to recompile the kernel to perform

00:27:32,320 --> 00:27:36,080
the average of numbers so something

00:27:34,640 --> 00:27:40,080
didn't quite go

00:27:36,080 --> 00:27:43,760
as expected and let me explain why

00:27:40,080 --> 00:27:46,159
well first of all the first problem as i

00:27:43,760 --> 00:27:47,039
mentioned was that i needed to know

00:27:46,159 --> 00:27:49,279
where to

00:27:47,039 --> 00:27:52,080
intercept the kernel in the read path

00:27:49,279 --> 00:27:52,080
and to understand

00:27:52,240 --> 00:27:55,600
the appropriate place turns out that

00:27:54,320 --> 00:27:58,320
copy out was

00:27:55,600 --> 00:28:01,200
just what i was looking for but it is

00:27:58,320 --> 00:28:02,720
not an exported symbol

00:28:01,200 --> 00:28:05,039
this is not that big of a deal it just

00:28:02,720 --> 00:28:08,559
means that a bpf in the standard

00:28:05,039 --> 00:28:10,240
kernel that ships cannot see the symbol

00:28:08,559 --> 00:28:12,320
of the function the function name

00:28:10,240 --> 00:28:14,320
so it cannot actually stop its execution

00:28:12,320 --> 00:28:16,159
and start right before

00:28:14,320 --> 00:28:17,679
so i had to export the symbol first

00:28:16,159 --> 00:28:18,480
recompile the kernel and then it was

00:28:17,679 --> 00:28:21,200
good to go

00:28:18,480 --> 00:28:21,840
it's not ideal because ideally we would

00:28:21,200 --> 00:28:23,600
want to

00:28:21,840 --> 00:28:25,840
just go natively without having to

00:28:23,600 --> 00:28:28,000
recompile the kernel but

00:28:25,840 --> 00:28:30,080
for the sake of the prototype we may do

00:28:28,000 --> 00:28:33,600
with this

00:28:30,080 --> 00:28:37,200
second second problem is that uh

00:28:33,600 --> 00:28:39,360
as i told you ebpf allows to have access

00:28:37,200 --> 00:28:41,520
to the function arguments which in my

00:28:39,360 --> 00:28:43,440
case was actually the raw data

00:28:41,520 --> 00:28:45,520
that was being passed from the kernel to

00:28:43,440 --> 00:28:48,000
the user space

00:28:45,520 --> 00:28:50,320
but while the networking stock is very

00:28:48,000 --> 00:28:53,440
optimized for those operations

00:28:50,320 --> 00:28:55,919
it is not true for the

00:28:53,440 --> 00:28:57,279
i o stack because i i couldn't use

00:28:55,919 --> 00:28:59,120
native

00:28:57,279 --> 00:29:00,720
access for that i cannot reference a

00:28:59,120 --> 00:29:03,679
pointer yet

00:29:00,720 --> 00:29:05,360
the verifier is going to complain and so

00:29:03,679 --> 00:29:07,360
i need to use a helper function which

00:29:05,360 --> 00:29:09,840
performs the access for me

00:29:07,360 --> 00:29:11,200
and even if it's read only so

00:29:09,840 --> 00:29:15,120
technically

00:29:11,200 --> 00:29:16,640
i'm not going to really damage the data

00:29:15,120 --> 00:29:19,120
and we're not going to access kernel

00:29:16,640 --> 00:29:20,799
metadata as i mentioned

00:29:19,120 --> 00:29:22,320
using the helper function adds a bit of

00:29:20,799 --> 00:29:26,080
an overhead which is

00:29:22,320 --> 00:29:29,279
not ideal but it is

00:29:26,080 --> 00:29:30,799
um it works but having direct access to

00:29:29,279 --> 00:29:34,320
the pointer read only would

00:29:30,799 --> 00:29:34,320
of course be more efficient

00:29:35,279 --> 00:29:38,640
it is not done yet so we have another

00:29:37,760 --> 00:29:41,840
problem

00:29:38,640 --> 00:29:42,960
that is of course ebpf being formally

00:29:41,840 --> 00:29:46,799
verified and very

00:29:42,960 --> 00:29:49,840
sandboxed and is very

00:29:46,799 --> 00:29:52,080
memory constrained in a way so

00:29:49,840 --> 00:29:54,480
there's no dynamic memory allocation so

00:29:52,080 --> 00:29:57,200
a variable buffer size

00:29:54,480 --> 00:29:59,200
it's not gonna serve you well in a bpf

00:29:57,200 --> 00:30:01,760
but you cannot really do that

00:29:59,200 --> 00:30:03,440
so likely you're gonna have to isolate

00:30:01,760 --> 00:30:06,159
in batches

00:30:03,440 --> 00:30:07,200
if you wanna do that on the stack so and

00:30:06,159 --> 00:30:10,720
since you can only

00:30:07,200 --> 00:30:13,919
use that kind of memory so static memory

00:30:10,720 --> 00:30:14,480
you're limited to 512 bytes which is not

00:30:13,919 --> 00:30:17,120
a lot

00:30:14,480 --> 00:30:18,000
for big data but you can use maps which

00:30:17,120 --> 00:30:22,159
i

00:30:18,000 --> 00:30:25,360
potentially can have way more

00:30:22,159 --> 00:30:26,799
storage available for you yeah but still

00:30:25,360 --> 00:30:29,120
if you need to iterate in batches you

00:30:26,799 --> 00:30:32,399
need to keep in mind that ebps has a

00:30:29,120 --> 00:30:33,440
limited instruction number um that you

00:30:32,399 --> 00:30:35,840
can execute

00:30:33,440 --> 00:30:37,120
in a single run which is a million

00:30:35,840 --> 00:30:39,760
instructions

00:30:37,120 --> 00:30:40,559
but still you might hit that if you

00:30:39,760 --> 00:30:42,159
perform too

00:30:40,559 --> 00:30:44,080
complex of an operation which is also

00:30:42,159 --> 00:30:47,440
why we wanted to keep filter

00:30:44,080 --> 00:30:49,520
reviews as a guideline to avoid users

00:30:47,440 --> 00:30:51,039
performing all sorts of operations on

00:30:49,520 --> 00:30:54,159
the

00:30:51,039 --> 00:30:55,200
on the data which could probably lead to

00:30:54,159 --> 00:30:58,240
overrunning the

00:30:55,200 --> 00:30:58,240
instruction number limit

00:30:59,039 --> 00:31:04,000
uh last thing which is the most

00:31:01,600 --> 00:31:06,000
embarrassing is that

00:31:04,000 --> 00:31:08,720
even computing the average is not that

00:31:06,000 --> 00:31:10,720
easy because

00:31:08,720 --> 00:31:12,080
you need first of all a helper function

00:31:10,720 --> 00:31:13,840
to convert from

00:31:12,080 --> 00:31:15,760
the charts that you read in the buffer

00:31:13,840 --> 00:31:17,440
to integers that you can then operate

00:31:15,760 --> 00:31:20,240
with

00:31:17,440 --> 00:31:22,399
then integer operations work fine so you

00:31:20,240 --> 00:31:25,519
can perform the count max mean

00:31:22,399 --> 00:31:27,760
exists um without a problem but

00:31:25,519 --> 00:31:29,279
average needs a division and floating

00:31:27,760 --> 00:31:30,880
point division is not supporting in the

00:31:29,279 --> 00:31:34,240
kernel

00:31:30,880 --> 00:31:35,840
so yeah it leads it yields to not very

00:31:34,240 --> 00:31:39,919
accurate results

00:31:35,840 --> 00:31:43,440
as of now but we might implement

00:31:39,919 --> 00:31:43,440
we might kind of work around for that

00:31:44,000 --> 00:31:48,320
uh but yeah uh the prototype that i

00:31:47,200 --> 00:31:52,240
presented

00:31:48,320 --> 00:31:55,600
to although this there were quite some

00:31:52,240 --> 00:31:59,360
problems and hiccups on the way

00:31:55,600 --> 00:32:01,440
um does not really perform anything

00:31:59,360 --> 00:32:02,480
different from the networking stack so

00:32:01,440 --> 00:32:06,240
it is

00:32:02,480 --> 00:32:10,080
it does operations that are just reduxes

00:32:06,240 --> 00:32:12,320
drop packets on the on the i o second

00:32:10,080 --> 00:32:13,840
set of networking so conceptually

00:32:12,320 --> 00:32:16,080
it's something that bppf already has the

00:32:13,840 --> 00:32:18,559
power to do it achieves

00:32:16,080 --> 00:32:20,799
a data movement reduction because as we

00:32:18,559 --> 00:32:24,159
saw we go from a full buffer of data to

00:32:20,799 --> 00:32:27,039
just one number and it is

00:32:24,159 --> 00:32:28,159
executed in a safe isolated environment

00:32:27,039 --> 00:32:30,000
in a vpf

00:32:28,159 --> 00:32:31,360
which is already available in data

00:32:30,000 --> 00:32:34,399
centers

00:32:31,360 --> 00:32:37,519
right now but indeed

00:32:34,399 --> 00:32:40,840
in order to be production ready

00:32:37,519 --> 00:32:42,080
like many networking products are free

00:32:40,840 --> 00:32:45,200
bpf

00:32:42,080 --> 00:32:47,519
it needs more support for the i o staff

00:32:45,200 --> 00:32:48,960
so there is some work to do but we we do

00:32:47,519 --> 00:32:52,559
see the potential in this

00:32:48,960 --> 00:32:56,240
and actually we think that

00:32:52,559 --> 00:32:58,880
programmability is where ebtf is headed

00:32:56,240 --> 00:33:00,000
there has been so many development in

00:32:58,880 --> 00:33:02,880
the bpf

00:33:00,000 --> 00:33:03,519
lately and it's used all across the

00:33:02,880 --> 00:33:06,640
board for

00:33:03,519 --> 00:33:08,480
tracing and there are products that are

00:33:06,640 --> 00:33:12,080
very solid and well developed

00:33:08,480 --> 00:33:14,480
well developed like bcc bpf3s

00:33:12,080 --> 00:33:15,120
then in networking it's used for their

00:33:14,480 --> 00:33:18,080
stylium

00:33:15,120 --> 00:33:19,039
and google actually right now is

00:33:18,080 --> 00:33:20,720
starting to use

00:33:19,039 --> 00:33:22,640
ebpf for their data plane

00:33:20,720 --> 00:33:26,320
programmability operations

00:33:22,640 --> 00:33:28,320
for security you can instrument ebtf

00:33:26,320 --> 00:33:29,919
uh and there's the kernel runtime

00:33:28,320 --> 00:33:30,640
security instrumentation from google

00:33:29,919 --> 00:33:34,960
again

00:33:30,640 --> 00:33:36,960
or falco so it it is expanding it's not

00:33:34,960 --> 00:33:40,080
networking only anymore

00:33:36,960 --> 00:33:44,159
and programmability might actually

00:33:40,080 --> 00:33:47,279
be the next step and in fact

00:33:44,159 --> 00:33:49,760
we we believe that that's the case and

00:33:47,279 --> 00:33:53,279
turns out that this year these are all

00:33:49,760 --> 00:33:56,159
articles and headlines for 2020

00:33:53,279 --> 00:33:57,760
the ebtf is being looked at as a way to

00:33:56,159 --> 00:34:00,320
change

00:33:57,760 --> 00:34:02,000
the way in which linux kernel

00:34:00,320 --> 00:34:06,080
programming happens

00:34:02,000 --> 00:34:09,200
and a way potentially to turn the

00:34:06,080 --> 00:34:09,760
the kernel into a programmable linux

00:34:09,200 --> 00:34:13,839
kernel

00:34:09,760 --> 00:34:14,399
and we think that this is the direction

00:34:13,839 --> 00:34:17,839
that

00:34:14,399 --> 00:34:18,879
things are going to we are going to work

00:34:17,839 --> 00:34:21,599
on that and

00:34:18,879 --> 00:34:22,560
we hope that you are going to join us as

00:34:21,599 --> 00:34:26,000
well

00:34:22,560 --> 00:34:28,800
so thank you for listening and

00:34:26,000 --> 00:34:29,359
i will be very happy to to hear your

00:34:28,800 --> 00:34:35,839
question

00:34:29,359 --> 00:34:35,839
now thanks a lot

00:34:37,280 --> 00:34:39,359

YouTube URL: https://www.youtube.com/watch?v=jtkfjPUrfZQ


