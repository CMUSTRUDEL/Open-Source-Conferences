Title: Keynote: Virtualization at Scale @ Alibaba Cloud - Hong Tang, Chief Architect, Alibaba Cloud
Publication date: 2017-06-26
Playlist: LinuxCon Beijing 2017
Description: 
	Keynote: Virtualization at Scale @ Alibaba Cloud [E] - Hong Tang, Chief Architect, Alibaba Cloud

This talk will give attendees a glance into how Alibaba built China's top 1 virtualization cluster, and what their perspective is about the future of cloud computing. 

About Hong Tang
Dr. Hong serves as Chief Architect at Alibaba Cloud, the cloud computing arm of Alibaba Group. He joined Alibaba Cloud in 2010 and has been instrumental in the development of Apsara, Alibaba Cloud’s large-scale computational engine. 

Prior to Alibaba Cloud, he was a Director of Search System Infrastructure at the fourth largest search engine company Ask.com. Later in 2008, he joined Yahoo’s cloud computing team as a senior principal engineer, driving the research and development of Hadoop.

Dr. Hong has nineteen years’ research and industrial experience in high- performance computing and parallel systems, cluster-based distributed computing and storage systems, large-scale Internet services and cloud computing. Dr. Hong holds B.S. degree in Computer Science from Zhejiang University and the Doctoral degree from University of California, Santa Barbara. He was elected as an Innovative Talent in the national Recruitment Program of Global Experts in 2014 (aka National Thousand Talent Plan).
Captions: 
	00:00:00,030 --> 00:00:06,150
hi everyone it's my great honor to be

00:00:04,170 --> 00:00:08,250
here to tell you a little bit about

00:00:06,150 --> 00:00:11,460
Alibaba cloud and some of the

00:00:08,250 --> 00:00:14,250
technologies behind it so let's get

00:00:11,460 --> 00:00:17,250
started first I want to share with you a

00:00:14,250 --> 00:00:20,910
set of numbers to give you a sense of

00:00:17,250 --> 00:00:24,300
Alibaba cloud Alibaba cloud was founded

00:00:20,910 --> 00:00:26,820
in on September 10th 2009 which is

00:00:24,300 --> 00:00:31,140
exactly 10th anniversary of Alibaba

00:00:26,820 --> 00:00:34,950
group's founding and after two years we

00:00:31,140 --> 00:00:36,480
rolled out our first product on on July

00:00:34,950 --> 00:00:38,340
28 2011

00:00:36,480 --> 00:00:41,820
that's the elastic computing service

00:00:38,340 --> 00:00:43,950
which is virtual machine since then

00:00:41,820 --> 00:00:48,360
we've seen tremendous growth of Alibaba

00:00:43,950 --> 00:00:50,910
cloud in the last eight quarters we have

00:00:48,360 --> 00:00:52,190
seen triple digit growth quarter

00:00:50,910 --> 00:00:56,370
year-over-year

00:00:52,190 --> 00:00:59,879
convectively that puts us the number one

00:00:56,370 --> 00:01:04,019
in terms of cloud companies within China

00:00:59,879 --> 00:01:08,580
and in my you know last quarter

00:01:04,019 --> 00:01:12,060
quarterly report we reported 0.87

00:01:08,580 --> 00:01:15,630
million paying customers and the total

00:01:12,060 --> 00:01:17,640
revenue for fiscal year 2017 was six

00:01:15,630 --> 00:01:23,490
point six billing almost 1 billion US

00:01:17,640 --> 00:01:26,490
dollars so the technology powering

00:01:23,490 --> 00:01:28,020
Alibaba cloud is called Apsara in

00:01:26,490 --> 00:01:31,200
Chinese seyton

00:01:28,020 --> 00:01:35,369
so let me just go through briefly about

00:01:31,200 --> 00:01:39,240
the journey of apps are the development

00:01:35,369 --> 00:01:41,610
of apps ara started in February 2009 if

00:01:39,240 --> 00:01:43,110
you recall in my previous slides you

00:01:41,610 --> 00:01:45,479
wouldn't notice that actually

00:01:43,110 --> 00:01:48,659
development of Apsara precedes the

00:01:45,479 --> 00:01:51,360
founding of Alibaba cloud the reason

00:01:48,659 --> 00:01:55,229
being that when we started doing

00:01:51,360 --> 00:01:57,420
building Apsara what we wanted to do was

00:01:55,229 --> 00:01:59,130
to build a unified a foundation

00:01:57,420 --> 00:02:01,890
technology foundation for all the

00:01:59,130 --> 00:02:03,750
Alibaba group's business units because

00:02:01,890 --> 00:02:05,700
at that time the business was growing

00:02:03,750 --> 00:02:08,580
very fast in different business groups

00:02:05,700 --> 00:02:11,550
and also the technology wise we have

00:02:08,580 --> 00:02:13,370
commercial software open sources inside

00:02:11,550 --> 00:02:15,049
of the fashions so we wanted

00:02:13,370 --> 00:02:17,000
really unify everything together have a

00:02:15,049 --> 00:02:22,480
single foundation for all the Alibaba

00:02:17,000 --> 00:02:27,849
groups business groups so after about

00:02:22,480 --> 00:02:31,930
one year's development on in August 2011

00:02:27,849 --> 00:02:33,709
ten I'm sorry I'm sorry I became the

00:02:31,930 --> 00:02:36,739
technology foundation the cloud

00:02:33,709 --> 00:02:39,379
infrastructure within Alibaba Group with

00:02:36,739 --> 00:02:42,140
that we support four major applications

00:02:39,379 --> 00:02:44,239
including web search web mail in the

00:02:42,140 --> 00:02:46,459
image storage in the micro loan payment

00:02:44,239 --> 00:02:49,790
service which now is part of provide and

00:02:46,459 --> 00:02:55,040
financial the furthermost I'll talk

00:02:49,790 --> 00:02:58,120
about is in August 2013 this is what we

00:02:55,040 --> 00:03:04,519
have play what we internally called a 5k

00:02:58,120 --> 00:03:08,090
project so on that date August 15 2013

00:03:04,519 --> 00:03:09,700
the first 5,000 no the flower class went

00:03:08,090 --> 00:03:13,280
online went to production

00:03:09,700 --> 00:03:15,799
the reason 5,000 is the 5k so important

00:03:13,280 --> 00:03:18,560
is because when we set out to do a Sarah

00:03:15,799 --> 00:03:20,829
we wanted to build a large-scale general

00:03:18,560 --> 00:03:24,980
computing platform so what do we buy

00:03:20,829 --> 00:03:27,019
large-scale in 2009 what we know in the

00:03:24,980 --> 00:03:29,299
world the largest class was about five

00:03:27,019 --> 00:03:31,910
thousand nodes probably in Google so

00:03:29,299 --> 00:03:34,790
that's what we think that to really put

00:03:31,910 --> 00:03:37,310
a concrete definition of a large-scale

00:03:34,790 --> 00:03:41,209
we said that we have to really make it

00:03:37,310 --> 00:03:43,030
at least the 5,000 node cluster we

00:03:41,209 --> 00:03:46,160
thought we could do it by the end of

00:03:43,030 --> 00:03:46,819
2010 but turns out it's much harder than

00:03:46,160 --> 00:03:49,160
we thought

00:03:46,819 --> 00:03:51,650
so it took us three more years so in

00:03:49,160 --> 00:03:54,650
2013 we actually did it I'm going to

00:03:51,650 --> 00:03:59,959
talk in a bit more or later in the next

00:03:54,650 --> 00:04:01,549
milestone we we not only proved that we

00:03:59,959 --> 00:04:05,780
can build the skill also want to prove

00:04:01,549 --> 00:04:09,260
that the system can outperform other

00:04:05,780 --> 00:04:14,230
systems so in 2015 we participated into

00:04:09,260 --> 00:04:18,500
this sort contest and our flower system

00:04:14,230 --> 00:04:20,930
won't broke the world record in a sort

00:04:18,500 --> 00:04:25,440
of competition by sorting 100 terabytes

00:04:20,930 --> 00:04:28,620
of data in 377 seconds

00:04:25,440 --> 00:04:32,880
next so it's really not a milestone

00:04:28,620 --> 00:04:35,760
because starting from 2011 we hold and

00:04:32,880 --> 00:04:36,840
you develop a conferences so in 2016

00:04:35,760 --> 00:04:38,820
obviously that's a sixth of the

00:04:36,840 --> 00:04:41,630
conference's and you can see some

00:04:38,820 --> 00:04:44,330
numbers there that we have 40,000

00:04:41,630 --> 00:04:47,520
developers attending the conference and

00:04:44,330 --> 00:04:50,100
over seven million people actually watch

00:04:47,520 --> 00:04:56,070
the wrong line we did it live broadcast

00:04:50,100 --> 00:04:59,400
so what exactly does after I do

00:04:56,070 --> 00:05:01,920
Apsara in a nutshell it manages a plan

00:04:59,400 --> 00:05:05,010
is skill infrastructure so what does it

00:05:01,920 --> 00:05:06,780
look like first of all our data centers

00:05:05,010 --> 00:05:09,720
are organized into regions and between

00:05:06,780 --> 00:05:12,480
these regions we have vast networks

00:05:09,720 --> 00:05:16,800
which is called a transfer transfer

00:05:12,480 --> 00:05:18,990
Network and also we have our backbone

00:05:16,800 --> 00:05:22,580
network that connects our data centers

00:05:18,990 --> 00:05:25,500
with the telecom carriers that's how we

00:05:22,580 --> 00:05:28,340
connect the developers users from

00:05:25,500 --> 00:05:31,170
outside networks to our data centers and

00:05:28,340 --> 00:05:33,720
finally we have this edge network which

00:05:31,170 --> 00:05:36,840
is essentially point out presence and

00:05:33,720 --> 00:05:39,210
sites across the globe so that that

00:05:36,840 --> 00:05:41,750
accelerates the access of the services

00:05:39,210 --> 00:05:46,740
being deployed in our data centers so

00:05:41,750 --> 00:05:49,260
today we have four data centers regions

00:05:46,740 --> 00:05:54,750
across the globe in the mainland we have

00:05:49,260 --> 00:05:57,300
six regions three in China North - in

00:05:54,750 --> 00:05:59,610
China in China East and the two in China

00:05:57,300 --> 00:06:02,430
so sorry one in China south and two in

00:05:59,610 --> 00:06:03,930
China East and also we have eleven

00:06:02,430 --> 00:06:06,630
overseas regions I'm not going to

00:06:03,930 --> 00:06:09,000
enumerate them here and finally about

00:06:06,630 --> 00:06:12,180
the edge mode network we have over six

00:06:09,000 --> 00:06:15,530
hundred point of presence nodes and over

00:06:12,180 --> 00:06:18,180
20 terabytes all for bandwidth capacity

00:06:15,530 --> 00:06:18,930
this slide shows the architecture of

00:06:18,180 --> 00:06:22,500
Apsara

00:06:18,930 --> 00:06:25,470
so Adam bottom is our data centers our

00:06:22,500 --> 00:06:29,970
variable zones and the regions and on

00:06:25,470 --> 00:06:31,950
top of that we have these four red

00:06:29,970 --> 00:06:33,120
squares are the common building blocks

00:06:31,950 --> 00:06:36,020
of digital systems

00:06:33,120 --> 00:06:38,300
these include additional coordination

00:06:36,020 --> 00:06:40,819
security management

00:06:38,300 --> 00:06:44,840
la collection and this is the monitoring

00:06:40,819 --> 00:06:47,539
Diagnostics etc and on top of that there

00:06:44,840 --> 00:06:50,060
are two big pieces why is Inc what why

00:06:47,539 --> 00:06:52,250
is what we call Pangu which is unified a

00:06:50,060 --> 00:06:53,750
storage management the other is called a

00:06:52,250 --> 00:06:57,860
Fuji which is a dis reduce resource

00:06:53,750 --> 00:07:00,710
management so we actually have have

00:06:57,860 --> 00:07:03,199
these two pieces to pull off all the

00:07:00,710 --> 00:07:05,990
resources in our data centers as unify

00:07:03,199 --> 00:07:09,139
the resources poor end and unify the

00:07:05,990 --> 00:07:10,699
storage on the side we have a component

00:07:09,139 --> 00:07:12,560
called a king G which does the

00:07:10,699 --> 00:07:15,050
infrastructure management and the

00:07:12,560 --> 00:07:17,180
service management look at the this is

00:07:15,050 --> 00:07:19,190
only component that's that's aligned

00:07:17,180 --> 00:07:21,800
vertically that's because it's actually

00:07:19,190 --> 00:07:23,780
connecting the applications running on

00:07:21,800 --> 00:07:25,940
top of those the red we call it The

00:07:23,780 --> 00:07:29,840
Observer call and also the bottom line

00:07:25,940 --> 00:07:31,729
infrastructure so this so basically what

00:07:29,840 --> 00:07:36,650
it does is that it has the service

00:07:31,729 --> 00:07:38,569
deployment expansion and also a server

00:07:36,650 --> 00:07:41,360
healing insert in the sense that when

00:07:38,569 --> 00:07:43,460
some some service service instance that

00:07:41,360 --> 00:07:45,949
has problems it will automatically shut

00:07:43,460 --> 00:07:50,029
it down isolated it and start a new

00:07:45,949 --> 00:07:52,250
instances then on top of that we have

00:07:50,029 --> 00:07:54,259
accounting authentication or

00:07:52,250 --> 00:07:56,389
authorization metering the billing these

00:07:54,259 --> 00:08:01,419
are the service is very common to

00:07:56,389 --> 00:08:05,150
support a public service and in those

00:08:01,419 --> 00:08:08,150
blue squares these are the public cloud

00:08:05,150 --> 00:08:10,669
services will provide they include some

00:08:08,150 --> 00:08:14,389
basic service like a compute storage

00:08:10,669 --> 00:08:16,400
databases networking and also we have a

00:08:14,389 --> 00:08:19,190
collection of services to help

00:08:16,400 --> 00:08:21,319
developers build a application that

00:08:19,190 --> 00:08:22,940
includes the middleware service

00:08:21,319 --> 00:08:25,729
orchestration and the service

00:08:22,940 --> 00:08:28,129
Computing's then on top of that we have

00:08:25,729 --> 00:08:32,510
some advanced services including data

00:08:28,129 --> 00:08:35,209
intelligence such as bi such as AI or

00:08:32,510 --> 00:08:39,229
machine learning and also some security

00:08:35,209 --> 00:08:41,089
products and when the orange pieces are

00:08:39,229 --> 00:08:44,990
those ones that dealing with the with

00:08:41,089 --> 00:08:47,350
the connectivity such as the data

00:08:44,990 --> 00:08:49,610
transfer database synchronization

00:08:47,350 --> 00:08:51,980
content delivery network and also

00:08:49,610 --> 00:08:54,200
Express connect which is as the

00:08:51,980 --> 00:08:56,600
hybrid a cloud connecting between the

00:08:54,200 --> 00:08:59,630
on-premise infrastructure and our public

00:08:56,600 --> 00:09:01,220
cloud and finally we have a what we

00:08:59,630 --> 00:09:03,470
could you can consider the App Store for

00:09:01,220 --> 00:09:06,440
cloud which is which we call call the

00:09:03,470 --> 00:09:09,080
marketplace we would like to think of

00:09:06,440 --> 00:09:12,590
Sarah as a hyper scale cloud operating

00:09:09,080 --> 00:09:15,710
system the reason we quit offices every

00:09:12,590 --> 00:09:19,220
system is because if the active tools

00:09:15,710 --> 00:09:21,740
are very clear analogy between a Sarah

00:09:19,220 --> 00:09:23,570
in the PC operating system so if you

00:09:21,740 --> 00:09:26,120
look at all those layers at the bottom

00:09:23,570 --> 00:09:29,000
layer it corresponds to a PC hardware

00:09:26,120 --> 00:09:31,430
then on top of that the red pieces

00:09:29,000 --> 00:09:32,990
really correspond to the kernel then I

00:09:31,430 --> 00:09:34,910
even on top of that you know all the

00:09:32,990 --> 00:09:37,220
modern operating systems are multi-user

00:09:34,910 --> 00:09:39,950
enabled so that's why every open set has

00:09:37,220 --> 00:09:42,980
to have account management all those

00:09:39,950 --> 00:09:46,000
blue squares corresponds to system call

00:09:42,980 --> 00:09:48,470
system services and a bundled apps and

00:09:46,000 --> 00:09:50,240
the orange pieces corresponds to the

00:09:48,470 --> 00:09:51,890
input and output subsystem of an

00:09:50,240 --> 00:09:54,140
operating system and then finally

00:09:51,890 --> 00:09:58,940
obviously most of the modern operating

00:09:54,140 --> 00:10:03,920
system they have an app store so I want

00:09:58,940 --> 00:10:05,360
to talk briefly about some of the design

00:10:03,920 --> 00:10:08,570
highlights of a Sarah

00:10:05,360 --> 00:10:10,790
first of all we talked about we want to

00:10:08,570 --> 00:10:15,100
build a general-purpose computing a

00:10:10,790 --> 00:10:18,380
platform for for our internal

00:10:15,100 --> 00:10:20,450
applications that's why we think that

00:10:18,380 --> 00:10:23,840
it's very the reason we want to do that

00:10:20,450 --> 00:10:25,670
we think it's very important to run a

00:10:23,840 --> 00:10:28,270
mixture of latency sensitive

00:10:25,670 --> 00:10:30,740
applications and batch applications so

00:10:28,270 --> 00:10:32,630
the first key highlighting is that apps

00:10:30,740 --> 00:10:34,910
are really is a uniform the computing

00:10:32,630 --> 00:10:36,770
platform for running latency sensitive

00:10:34,910 --> 00:10:39,710
applications in a batch application on

00:10:36,770 --> 00:10:41,840
one platform the second line is that I

00:10:39,710 --> 00:10:44,570
thought we were talking about in 2013 we

00:10:41,840 --> 00:10:47,750
achieve a scale over 5k but we didn't

00:10:44,570 --> 00:10:51,320
stop there so currently our largest our

00:10:47,750 --> 00:10:53,840
classes is exceeded ten thousand servers

00:10:51,320 --> 00:10:56,570
with hundreds of petabytes of raw

00:10:53,840 --> 00:11:00,770
storage and the hundreds of thousands of

00:10:56,570 --> 00:11:01,790
our for CPU cores and thirdly in the

00:11:00,770 --> 00:11:05,550
design of a Sarah

00:11:01,790 --> 00:11:08,129
given such a large scale that design

00:11:05,550 --> 00:11:09,749
throughout the absurd design it has zero

00:11:08,129 --> 00:11:12,379
single point of failure and achieves

00:11:09,749 --> 00:11:16,529
three nine to five solve availability

00:11:12,379 --> 00:11:19,709
enforce all the data stored in Apsara

00:11:16,529 --> 00:11:22,879
are ripple replicated by default and

00:11:19,709 --> 00:11:27,209
achieve 1090 our data availability and

00:11:22,879 --> 00:11:29,720
in fifth to really match to such a

00:11:27,209 --> 00:11:33,360
large-scale infrastructure or the

00:11:29,720 --> 00:11:36,059
monitoring or the Diagnostics or the

00:11:33,360 --> 00:11:38,429
deployment our fully distributed and

00:11:36,059 --> 00:11:40,980
lastly you must have noticed that in my

00:11:38,429 --> 00:11:43,740
architecture diagram security is

00:11:40,980 --> 00:11:46,189
embedded in the deep of the core of a

00:11:43,740 --> 00:11:50,399
para and we employed what we call

00:11:46,189 --> 00:11:56,209
minimum PCB trust the computing base to

00:11:50,399 --> 00:12:00,589
ensure the security of this whole system

00:11:56,209 --> 00:12:03,569
so like I said of previously I want to

00:12:00,589 --> 00:12:05,519
develop a little bit more thorough 5k

00:12:03,569 --> 00:12:09,649
the reason it is important is because

00:12:05,519 --> 00:12:12,959
you know not many companies can actually

00:12:09,649 --> 00:12:15,360
well in this world out many they are not

00:12:12,959 --> 00:12:19,290
many commercial or open-source offerings

00:12:15,360 --> 00:12:23,449
that can do 5k we became one of that and

00:12:19,290 --> 00:12:27,240
also why it's so important is because

00:12:23,449 --> 00:12:29,939
with 5k it was the first time that the

00:12:27,240 --> 00:12:32,730
skill of a a star cluster exceeds that

00:12:29,939 --> 00:12:35,610
of a Hadoop cluster and so with that we

00:12:32,730 --> 00:12:37,860
actually decided to have we should

00:12:35,610 --> 00:12:40,619
unified all the data processing workload

00:12:37,860 --> 00:12:42,990
on one platform so that's what we

00:12:40,619 --> 00:12:45,269
internally it's our internal project

00:12:42,990 --> 00:12:49,649
Apollo internal we call it a ten-year so

00:12:45,269 --> 00:12:51,509
now every call up

00:12:49,649 --> 00:12:54,059
Ariba back for businesses the data

00:12:51,509 --> 00:12:55,589
processing workloads up front are being

00:12:54,059 --> 00:13:01,799
processed on asara

00:12:55,589 --> 00:13:04,139
and also on July 1st 2014 we made a Mac

00:13:01,799 --> 00:13:06,929
computer generally available so that's

00:13:04,139 --> 00:13:10,679
the distributed data processing platform

00:13:06,929 --> 00:13:14,610
built on top of affero and that was the

00:13:10,679 --> 00:13:17,850
first time that in a one company that

00:13:14,610 --> 00:13:19,260
offers such large-scale data processing

00:13:17,850 --> 00:13:24,120
capability commercially

00:13:19,260 --> 00:13:26,250
to the world and it the the impact all

00:13:24,120 --> 00:13:28,500
for the availability of such kind of

00:13:26,250 --> 00:13:30,900
service that actually exceeds even our

00:13:28,500 --> 00:13:35,130
imagination so I want to give you one

00:13:30,900 --> 00:13:36,870
example so prior to the releases to

00:13:35,130 --> 00:13:38,850
reduce to the public of Mac's computer

00:13:36,870 --> 00:13:40,950
we conduct an experiment the ten

00:13:38,850 --> 00:13:43,470
challenge which is essentially similar

00:13:40,950 --> 00:13:46,340
to Netflix challenge what we did is that

00:13:43,470 --> 00:13:50,010
we give away slice over alibaba's

00:13:46,340 --> 00:13:52,890
internal data so anonymize that remove

00:13:50,010 --> 00:13:55,340
all the sensitive field and let people

00:13:52,890 --> 00:13:58,650
to do prediction algorithm essentially

00:13:55,340 --> 00:14:02,100
in this case what a user would a clicker

00:13:58,650 --> 00:14:05,100
for a certain add-on as a matter of fact

00:14:02,100 --> 00:14:08,030
over 7,000 the teams all over the clover

00:14:05,100 --> 00:14:13,110
attended to register for competition and

00:14:08,030 --> 00:14:17,280
even 351 groups were from outside China

00:14:13,110 --> 00:14:21,000
that is the more than 10x of similar

00:14:17,280 --> 00:14:24,840
contacts held in some kunia conference

00:14:21,000 --> 00:14:28,110
like a kdd and even what amazingly we

00:14:24,840 --> 00:14:31,380
led to the top 10 team to participate in

00:14:28,110 --> 00:14:33,990
our singles day event in 2014 what we

00:14:31,380 --> 00:14:36,990
did is that we slice the traffic online

00:14:33,990 --> 00:14:39,840
traffic into 11 pieces each one of them

00:14:36,990 --> 00:14:41,850
each one of those 11 111 Spears

00:14:39,840 --> 00:14:44,250
trafficker goes to one of those

00:14:41,850 --> 00:14:46,770
participants and we relive one piece

00:14:44,250 --> 00:14:48,660
going to our in-house algorithms we set

00:14:46,770 --> 00:14:51,570
out a milling run challenge in the sense

00:14:48,660 --> 00:14:53,670
that if the click-through rate of any of

00:14:51,570 --> 00:14:56,550
those participants exceeds our in-house

00:14:53,670 --> 00:14:58,830
algorithm by 15% we give away win

00:14:56,550 --> 00:15:01,110
thousand won so it turns out that one

00:14:58,830 --> 00:15:04,440
could actually won so there are person

00:15:01,110 --> 00:15:06,720
bid our in-house algorithm by 16% so as

00:15:04,440 --> 00:15:08,430
you can see that the availability of

00:15:06,720 --> 00:15:10,440
such kind of data processing classroom

00:15:08,430 --> 00:15:13,320
letting people to very easily iterate

00:15:10,440 --> 00:15:14,940
their algorithms makes all the data

00:15:13,320 --> 00:15:20,610
scientists the from the global to

00:15:14,940 --> 00:15:22,080
realize their ideas much faster so I'm

00:15:20,610 --> 00:15:25,530
not going to go through that slide I'm

00:15:22,080 --> 00:15:27,830
just this is just use the comprehensive

00:15:25,530 --> 00:15:31,620
portfolio of our cloud products

00:15:27,830 --> 00:15:33,750
so um I'm almost running out of time but

00:15:31,620 --> 00:15:35,130
I want to just maybe give me one minute

00:15:33,750 --> 00:15:37,890
I'll borrow time to talk a little bit

00:15:35,130 --> 00:15:39,870
about the virtualization in Alibaba

00:15:37,890 --> 00:15:42,270
cloud bio virtualization are really

00:15:39,870 --> 00:15:44,790
being virtualization in general so that

00:15:42,270 --> 00:15:46,350
means because you know young cloud it's

00:15:44,790 --> 00:15:51,450
always multi-talented

00:15:46,350 --> 00:15:53,760
the isolation the the runtime so I want

00:15:51,450 --> 00:15:56,340
to talk about virtualization in general

00:15:53,760 --> 00:15:58,650
in three aspects why is the resource

00:15:56,340 --> 00:16:00,150
isolation the second is what we

00:15:58,650 --> 00:16:01,980
traditionally know as virtualizing which

00:16:00,150 --> 00:16:06,090
you I call the server virtualization and

00:16:01,980 --> 00:16:08,700
so there is a container technology so um

00:16:06,090 --> 00:16:11,910
I dive a little bit talk about the Linux

00:16:08,700 --> 00:16:14,010
at Alibaba so currently or our physical

00:16:11,910 --> 00:16:16,740
servers run a variant of thickness

00:16:14,010 --> 00:16:19,890
distribution including Federer CentOS

00:16:16,740 --> 00:16:22,830
and also we release our customized the

00:16:19,890 --> 00:16:28,920
kernel in 2011 which we call the re

00:16:22,830 --> 00:16:31,770
kernel and also since 2010 we have have

00:16:28,920 --> 00:16:34,980
almost 300 patches being accepted by

00:16:31,770 --> 00:16:38,460
Linux and that makes us a number one

00:16:34,980 --> 00:16:43,080
cloud company Chinese car company in

00:16:38,460 --> 00:16:45,780
terms of contribution why mixing

00:16:43,080 --> 00:16:48,120
latencies of late insensitive workload

00:16:45,780 --> 00:16:50,130
in the patch worker is so important this

00:16:48,120 --> 00:16:52,650
is borrowed from data center as a

00:16:50,130 --> 00:16:56,100
computing box from Google so you can see

00:16:52,650 --> 00:16:57,510
that typically the clock running ladies

00:16:56,100 --> 00:16:59,760
and since offensive workload that has

00:16:57,510 --> 00:17:02,730
much lower CPU utilization than those

00:16:59,760 --> 00:17:04,290
running batch workload so the basic idea

00:17:02,730 --> 00:17:05,790
is why can we mix them together to

00:17:04,290 --> 00:17:08,220
improve the overall utilization

00:17:05,790 --> 00:17:12,480
well the key challenge is want to make

00:17:08,220 --> 00:17:14,459
sure on the the even though the usuals

00:17:12,480 --> 00:17:16,140
utilizing increased we don't want to

00:17:14,459 --> 00:17:19,680
sacrifice the performance of relating to

00:17:16,140 --> 00:17:21,180
sensitive workload so we've done a bunch

00:17:19,680 --> 00:17:23,040
of things I'm running out of time I'm

00:17:21,180 --> 00:17:25,079
not gonna go through the details of all

00:17:23,040 --> 00:17:27,510
those technologies essentially what we

00:17:25,079 --> 00:17:29,040
did is that we actually did a resource

00:17:27,510 --> 00:17:32,610
isolation in all the dimensions

00:17:29,040 --> 00:17:35,610
including CPU including the networking

00:17:32,610 --> 00:17:38,100
and including the i/o so let me show

00:17:35,610 --> 00:17:40,260
some of those results so this is router

00:17:38,100 --> 00:17:41,580
this slide shows the effectiveness of a

00:17:40,260 --> 00:17:44,820
CPU isolation

00:17:41,580 --> 00:17:47,580
so this vertical line essentially is the

00:17:44,820 --> 00:17:51,120
triggering event when a patchwork loader

00:17:47,580 --> 00:17:53,070
is being launched in latency our cloud

00:17:51,120 --> 00:17:55,830
the previous only running ladies and

00:17:53,070 --> 00:17:58,500
sensitive workload as we can see here

00:17:55,830 --> 00:18:00,540
that before and after graph the CPU

00:17:58,500 --> 00:18:03,180
utilization increased from 35 percent to

00:18:00,540 --> 00:18:05,610
over 65 percent that's over 30 percent

00:18:03,180 --> 00:18:07,830
increase and also in terms of

00:18:05,610 --> 00:18:09,990
degradation wise both the latency and

00:18:07,830 --> 00:18:12,780
the throughput for the late insensitive

00:18:09,990 --> 00:18:14,550
workload is less than 5% so that

00:18:12,780 --> 00:18:18,960
definitely shows the effect in the

00:18:14,550 --> 00:18:22,110
resolve our CPU isolation next we look

00:18:18,960 --> 00:18:25,050
at the networking isolation so we have

00:18:22,110 --> 00:18:28,350
four three about here the green ones

00:18:25,050 --> 00:18:30,390
issues the latency of related latency of

00:18:28,350 --> 00:18:33,210
those ladies and sensitive workloads

00:18:30,390 --> 00:18:36,600
with requests and the blue bar shows

00:18:33,210 --> 00:18:38,520
request latency when we just simply mix

00:18:36,600 --> 00:18:41,610
the workload without doing any kind of

00:18:38,520 --> 00:18:44,790
networking isolation and the yellow ones

00:18:41,610 --> 00:18:48,030
to choose the effect that when we have

00:18:44,790 --> 00:18:51,300
the network isolation enabled as we can

00:18:48,030 --> 00:18:54,630
see without the network isolation the

00:18:51,300 --> 00:18:57,210
average latency requests the processing

00:18:54,630 --> 00:18:59,520
latency increases by a two-fold and even

00:18:57,210 --> 00:19:03,210
was the tail latency increase by 20 fold

00:18:59,520 --> 00:19:06,030
while with the network isolation enabled

00:19:03,210 --> 00:19:08,150
the degradation goes down to 10 to 20

00:19:06,030 --> 00:19:12,150
percent so that's effective software

00:19:08,150 --> 00:19:15,720
network isolation and now we look at the

00:19:12,150 --> 00:19:19,200
i/o throttling so we what we did is that

00:19:15,720 --> 00:19:21,270
we allow applications to show also their

00:19:19,200 --> 00:19:24,390
iOS based on tourism it's a file

00:19:21,270 --> 00:19:27,150
directories here we can see that we have

00:19:24,390 --> 00:19:29,400
set up a single process some data being

00:19:27,150 --> 00:19:31,380
on swaddles and data some come some

00:19:29,400 --> 00:19:34,830
files are being throttled we can clear

00:19:31,380 --> 00:19:36,900
see the throttle threshold is 250 K IRS

00:19:34,830 --> 00:19:40,860
and we can clear see that we can pound

00:19:36,900 --> 00:19:42,600
the dials for those throttle files next

00:19:40,860 --> 00:19:43,920
I want to talk a little bit about the

00:19:42,600 --> 00:19:46,980
server virtualization

00:19:43,920 --> 00:19:48,750
so I'm server virtualization virtual

00:19:46,980 --> 00:19:50,700
ization essentially is a bread butter

00:19:48,750 --> 00:19:52,710
avocado computing as a matter of fact

00:19:50,700 --> 00:19:54,240
our first service that's the computing

00:19:52,710 --> 00:19:55,530
services essentially is the server

00:19:54,240 --> 00:19:59,520
virtualization

00:19:55,530 --> 00:20:01,470
and underneath the hood we we conduct we

00:19:59,520 --> 00:20:05,310
did a transition from zen based the

00:20:01,470 --> 00:20:10,410
hypervisor to KVM based the hypervisor -

00:20:05,310 --> 00:20:12,960
in 2014 and we get engaged to the cow

00:20:10,410 --> 00:20:14,640
for a Linux Foundation in 2017 by

00:20:12,960 --> 00:20:18,510
upgrading our membership to the gold

00:20:14,640 --> 00:20:22,410
membership one of the key things that we

00:20:18,510 --> 00:20:25,970
did with the KVM was to support a hot

00:20:22,410 --> 00:20:29,460
upgrade so the key problem is that

00:20:25,970 --> 00:20:32,160
typically a hypervisor upgrader would

00:20:29,460 --> 00:20:36,050
impact the service availability a casein

00:20:32,160 --> 00:20:39,660
pork point is in 2014 one of the

00:20:36,050 --> 00:20:41,490
vulnerability data would leader - if

00:20:39,660 --> 00:20:43,710
that's one beautiful then by the way

00:20:41,490 --> 00:20:46,290
that would lead to a vulnerable a

00:20:43,710 --> 00:20:48,120
malicious user to gain access to the

00:20:46,290 --> 00:20:49,620
physical machine or even impact the some

00:20:48,120 --> 00:20:53,430
the case that always sits on the same

00:20:49,620 --> 00:20:55,980
server and I know the Dino the that's a

00:20:53,430 --> 00:20:58,110
company has another have to restart

00:20:55,980 --> 00:21:01,020
order or the other server instances to

00:20:58,110 --> 00:21:04,860
patch the kernel so what we did is that

00:21:01,020 --> 00:21:07,560
we did that very pencil taking engineer

00:21:04,860 --> 00:21:10,920
effort we made all those components

00:21:07,560 --> 00:21:13,170
related to hypervisor hot upgradeable

00:21:10,920 --> 00:21:16,200
including k mode and Hakeem you I'm not

00:21:13,170 --> 00:21:18,390
going to the details here and today

00:21:16,200 --> 00:21:20,430
every VM will go through some kind of

00:21:18,390 --> 00:21:23,670
upgrading event once every one to two

00:21:20,430 --> 00:21:25,140
months and during that upgrade the event

00:21:23,670 --> 00:21:27,030
typically there's only tens of

00:21:25,140 --> 00:21:29,460
milliseconds the service interruption so

00:21:27,030 --> 00:21:31,650
user doesn't see a thing is just the

00:21:29,460 --> 00:21:35,280
service Gator paused maybe for 20 30

00:21:31,650 --> 00:21:37,320
milliseconds finally I want to talk

00:21:35,280 --> 00:21:39,650
about the containers Alibaba cloud that

00:21:37,320 --> 00:21:41,340
embraced the cloud container technology

00:21:39,650 --> 00:21:44,130
and very dearly

00:21:41,340 --> 00:21:48,150
so we perform we formed a strategic

00:21:44,130 --> 00:21:51,930
partnership with darker in 2016 and just

00:21:48,150 --> 00:21:55,260
just this month we actually announcer

00:21:51,930 --> 00:21:59,420
the darker hobby in China we also joined

00:21:55,260 --> 00:22:03,660
the CNCs foundation as code member in

00:21:59,420 --> 00:22:06,660
two months ago and we are the only as

00:22:03,660 --> 00:22:08,669
far as our only cloud provider in China

00:22:06,660 --> 00:22:11,940
that supports both darker swamp

00:22:08,669 --> 00:22:14,669
and kubernetes so some of the bits of

00:22:11,940 --> 00:22:16,799
what we did for the container front

00:22:14,669 --> 00:22:18,509
first of all we did the native

00:22:16,799 --> 00:22:21,509
integration all the darkest one was

00:22:18,509 --> 00:22:22,549
called the infrastructure so that means

00:22:21,509 --> 00:22:25,769
that we have comprehensive

00:22:22,549 --> 00:22:28,529
infrastructure part for storage

00:22:25,769 --> 00:22:31,440
networking in the logging and we are

00:22:28,529 --> 00:22:34,200
actually working on to make such kind of

00:22:31,440 --> 00:22:36,210
integration available for kubernetes we

00:22:34,200 --> 00:22:40,080
also had a lot of scalability

00:22:36,210 --> 00:22:43,369
enhancements currently a single docker

00:22:40,080 --> 00:22:46,590
container classic could exceed over

00:22:43,369 --> 00:22:48,480
30,000 the VM nodes and finally we

00:22:46,590 --> 00:22:50,369
believe we are hosting the world's

00:22:48,480 --> 00:22:51,840
largest the container based application

00:22:50,369 --> 00:22:57,119
which is valued by about the e-commerce

00:22:51,840 --> 00:23:01,529
platform so in in the single day of 2016

00:22:57,119 --> 00:23:04,710
over 300 so for target containers are

00:23:01,529 --> 00:23:07,710
being deployed over multiple regions and

00:23:04,710 --> 00:23:10,980
the peak through 40 is 170,000 requests

00:23:07,710 --> 00:23:13,889
per second finally I want to conclude my

00:23:10,980 --> 00:23:17,190
talk so to talk about some ideas our

00:23:13,889 --> 00:23:19,289
further future directions first of all

00:23:17,190 --> 00:23:22,409
is lightweight virtualization for

00:23:19,289 --> 00:23:24,090
containers obviously given the more and

00:23:22,409 --> 00:23:27,239
more weight we put on the container

00:23:24,090 --> 00:23:28,919
technologies this is critical for us the

00:23:27,239 --> 00:23:32,249
second thing is that the hardware it

00:23:28,919 --> 00:23:35,639
becomes much faster such as nvme based

00:23:32,249 --> 00:23:37,830
storage or 25g network so the file

00:23:35,639 --> 00:23:39,779
system and the networking optimizations

00:23:37,830 --> 00:23:41,999
for such kind of ultra-fast Hardware I

00:23:39,779 --> 00:23:44,039
think is going to be a very very

00:23:41,999 --> 00:23:47,340
interesting direction to destroy and

00:23:44,039 --> 00:23:49,710
thirdly the security security

00:23:47,340 --> 00:23:53,730
enhancements for heterogeneous hardware

00:23:49,710 --> 00:23:59,760
we talked about FPGA GPU and some other

00:23:53,730 --> 00:24:03,010
customized races with that thank you

00:23:59,760 --> 00:24:03,010

YouTube URL: https://www.youtube.com/watch?v=5qXd4yYjhs4


