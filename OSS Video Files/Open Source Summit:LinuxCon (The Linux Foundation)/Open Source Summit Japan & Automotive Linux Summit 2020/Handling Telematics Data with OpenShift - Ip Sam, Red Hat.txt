Title: Handling Telematics Data with OpenShift - Ip Sam, Red Hat
Publication date: 2020-12-03
Playlist: Open Source Summit Japan & Automotive Linux Summit 2020
Description: 
	Handling Telematics Data with OpenShift - Ip Sam, Red Hat
Captions: 
	00:00:06,160 --> 00:00:09,760
hi

00:00:06,640 --> 00:00:12,320
everybody my name is ip sam i'm a senior

00:00:09,760 --> 00:00:15,679
consultant at red hat

00:00:12,320 --> 00:00:20,320
today welcome to the open source submit

00:00:15,679 --> 00:00:22,640
in japan 2020 presentation is about how

00:00:20,320 --> 00:00:25,519
to handle telemetric data

00:00:22,640 --> 00:00:25,519
with openshift

00:00:27,279 --> 00:00:34,040
what is telematic kinematic

00:00:30,560 --> 00:00:36,320
simple concept that combines

00:00:34,040 --> 00:00:38,239
telecommunication and data processing

00:00:36,320 --> 00:00:41,360
together

00:00:38,239 --> 00:00:44,640
telematics relies on using

00:00:41,360 --> 00:00:46,079
wireless device they were embedded into

00:00:44,640 --> 00:00:49,280
your vehicle

00:00:46,079 --> 00:00:52,000
and transmit the data in a real time to

00:00:49,280 --> 00:00:56,000
an organization

00:00:52,000 --> 00:00:58,640
humble of ethic include

00:00:56,000 --> 00:01:01,280
virtual vehicle with collecting

00:00:58,640 --> 00:01:01,280
information

00:01:02,960 --> 00:01:10,320
so what is first of all kinematic

00:01:06,880 --> 00:01:12,720
well we use telematic to increase the

00:01:10,320 --> 00:01:15,680
operational efficiency

00:01:12,720 --> 00:01:17,520
so that when for example when the truck

00:01:15,680 --> 00:01:21,680
is going to a different direction

00:01:17,520 --> 00:01:24,159
different wall we identify that

00:01:21,680 --> 00:01:25,200
and so that we could improve dictionary

00:01:24,159 --> 00:01:28,720
efficiency

00:01:25,200 --> 00:01:29,520
and identify the optimal path we want to

00:01:28,720 --> 00:01:32,640
improve the

00:01:29,520 --> 00:01:33,360
customer phase so for example when the

00:01:32,640 --> 00:01:36,640
customer

00:01:33,360 --> 00:01:38,400
do not receive the deliverable on time

00:01:36,640 --> 00:01:40,000
we want to tell the customer where

00:01:38,400 --> 00:01:42,799
exactly the truck is

00:01:40,000 --> 00:01:44,479
and when do they expect to receive the

00:01:42,799 --> 00:01:46,399
deliverable

00:01:44,479 --> 00:01:47,680
we want to protect the driver for

00:01:46,399 --> 00:01:50,479
example

00:01:47,680 --> 00:01:52,399
when there is an accident on the freeway

00:01:50,479 --> 00:01:55,280
we want to alert the driver

00:01:52,399 --> 00:01:56,479
and try to avoid the accident by taking

00:01:55,280 --> 00:01:59,680
a different route or

00:01:56,479 --> 00:02:01,920
taking a different time shift

00:01:59,680 --> 00:02:04,159
we also want to come with safety

00:02:01,920 --> 00:02:08,399
compliances

00:02:04,159 --> 00:02:11,520
for example when certain

00:02:08,399 --> 00:02:14,560
direction and certain vow is not

00:02:11,520 --> 00:02:17,920
is not compliance to specific delay of

00:02:14,560 --> 00:02:21,040
the deliverable objective

00:02:17,920 --> 00:02:23,840
we would use that to do the safety

00:02:21,040 --> 00:02:23,840
measure

00:02:24,800 --> 00:02:29,840
telemetric data includes the wireless

00:02:27,920 --> 00:02:32,800
communication

00:02:29,840 --> 00:02:34,400
location usually based on the gps

00:02:32,800 --> 00:02:38,640
location data

00:02:34,400 --> 00:02:41,200
and some in vehicle electronics

00:02:38,640 --> 00:02:42,800
it is also used to integrate to the

00:02:41,200 --> 00:02:46,800
cloud

00:02:42,800 --> 00:02:49,360
from the automobile and it is also used

00:02:46,800 --> 00:02:52,239
for data analysis

00:02:49,360 --> 00:02:53,360
for example when we could use machine

00:02:52,239 --> 00:02:56,959
learning

00:02:53,360 --> 00:02:59,920
to do different type of predictability

00:02:56,959 --> 00:03:02,800
to calculate the sales event and plan

00:02:59,920 --> 00:03:06,080
ahead on you know how much data how much

00:03:02,800 --> 00:03:07,440
block load how much db of deliverable we

00:03:06,080 --> 00:03:11,360
need to deliver from

00:03:07,440 --> 00:03:11,360
destination one to destination two

00:03:12,560 --> 00:03:21,040
an example of telematic data lifecycle

00:03:17,280 --> 00:03:25,040
it contains the first step where

00:03:21,040 --> 00:03:28,640
the data is generated from the vehicle

00:03:25,040 --> 00:03:30,840
from the telematic device the data will

00:03:28,640 --> 00:03:33,680
get transmitted

00:03:30,840 --> 00:03:36,080
to to the cloud

00:03:33,680 --> 00:03:37,760
the cloud will do the data collection

00:03:36,080 --> 00:03:40,480
and aggregation

00:03:37,760 --> 00:03:41,360
meaning that it will convert the

00:03:40,480 --> 00:03:44,319
external

00:03:41,360 --> 00:03:45,680
data object into internal object and you

00:03:44,319 --> 00:03:47,760
could aggregate the data

00:03:45,680 --> 00:03:50,239
together based on clustering based on

00:03:47,760 --> 00:03:54,000
data similarity

00:03:50,239 --> 00:03:56,480
and after that the data will be analyzed

00:03:54,000 --> 00:03:57,840
and sometimes it could be used by

00:03:56,480 --> 00:04:00,640
machine learning

00:03:57,840 --> 00:04:04,000
or it could be done by manual analysis

00:04:00,640 --> 00:04:06,080
to analyze the behavior of the data

00:04:04,000 --> 00:04:07,680
and at the end we would go into some

00:04:06,080 --> 00:04:11,200
sort of evaluation

00:04:07,680 --> 00:04:13,840
for example the driver's performance

00:04:11,200 --> 00:04:16,959
the truck's performance are we

00:04:13,840 --> 00:04:18,639
delivering as an organization

00:04:16,959 --> 00:04:21,440
and then you can see the cycle get

00:04:18,639 --> 00:04:24,880
repeated again and it will go back

00:04:21,440 --> 00:04:24,880
into our second iteration

00:04:26,320 --> 00:04:31,840
commercial telematic contains on-board

00:04:29,919 --> 00:04:33,919
tracking systems

00:04:31,840 --> 00:04:35,040
that send gps location wireless

00:04:33,919 --> 00:04:38,560
communication

00:04:35,040 --> 00:04:41,600
truck data and also it contains

00:04:38,560 --> 00:04:44,320
jurisdiction and tax management

00:04:41,600 --> 00:04:45,440
the driver would have some interfaces

00:04:44,320 --> 00:04:48,080
the interface

00:04:45,440 --> 00:04:50,240
could support the voice communication to

00:04:48,080 --> 00:04:53,120
talk to a dispatcher

00:04:50,240 --> 00:04:54,960
and it also support the text message

00:04:53,120 --> 00:04:58,479
communication

00:04:54,960 --> 00:04:59,040
the on-board sensor is embedded into the

00:04:58,479 --> 00:05:02,639
truck

00:04:59,040 --> 00:05:05,280
monitoring the trunk load the capacity

00:05:02,639 --> 00:05:06,080
the temperature of the container and the

00:05:05,280 --> 00:05:09,199
tire

00:05:06,080 --> 00:05:10,960
and the weight of the vehicle all these

00:05:09,199 --> 00:05:13,840
different real-time information will

00:05:10,960 --> 00:05:13,840
send to the cloud

00:05:14,400 --> 00:05:22,880
this is an example of a telemetric uh

00:05:18,240 --> 00:05:25,280
system integrated into a container truck

00:05:22,880 --> 00:05:26,479
at the end of the truck you see that

00:05:25,280 --> 00:05:29,039
there is a

00:05:26,479 --> 00:05:29,919
door sensor that will keep track of how

00:05:29,039 --> 00:05:31,840
many times

00:05:29,919 --> 00:05:33,039
the door of the container has been

00:05:31,840 --> 00:05:36,000
opened

00:05:33,039 --> 00:05:36,639
how long it takes for the operator to

00:05:36,000 --> 00:05:40,080
load

00:05:36,639 --> 00:05:41,440
the the goods on the truck and out of

00:05:40,080 --> 00:05:44,479
the truck

00:05:41,440 --> 00:05:47,039
the door locks is also a monitor

00:05:44,479 --> 00:05:49,039
so that we could track how many times

00:05:47,039 --> 00:05:51,440
the lock has been engaged

00:05:49,039 --> 00:05:53,360
is the lock secure is it protecting the

00:05:51,440 --> 00:05:54,960
item

00:05:53,360 --> 00:05:57,759
at the bottom of the trunk you see there

00:05:54,960 --> 00:06:02,160
were a monitor for the tire

00:05:57,759 --> 00:06:05,600
including the tire pressure the um

00:06:02,160 --> 00:06:07,759
the temperature of the tire the

00:06:05,600 --> 00:06:08,800
the the rotation of the tire how many

00:06:07,759 --> 00:06:12,800
times has it's been

00:06:08,800 --> 00:06:15,520
uh rotating by the rpm and all that

00:06:12,800 --> 00:06:17,680
the truck id is also uh monitor

00:06:15,520 --> 00:06:20,800
integrated as part of the truck

00:06:17,680 --> 00:06:21,280
uh other temperature sensor and also the

00:06:20,800 --> 00:06:23,919
weight

00:06:21,280 --> 00:06:26,080
sensor were also integrated so this is a

00:06:23,919 --> 00:06:28,000
complex system you see there was a lot

00:06:26,080 --> 00:06:32,000
of data we could use

00:06:28,000 --> 00:06:32,000
on all these different type of sensors

00:06:33,039 --> 00:06:38,960
similarly in the passenger vehicle

00:06:36,080 --> 00:06:39,680
you can see that when we are driving we

00:06:38,960 --> 00:06:42,479
have the

00:06:39,680 --> 00:06:45,280
emergency call the call could keep track

00:06:42,479 --> 00:06:48,400
of how many times we are calling for

00:06:45,280 --> 00:06:51,520
emergency for roadside assistance

00:06:48,400 --> 00:06:54,319
navigation gps system tracking our

00:06:51,520 --> 00:06:55,919
driving behaviors device to device

00:06:54,319 --> 00:06:58,960
communication

00:06:55,919 --> 00:07:00,319
so for instance when there are accidents

00:06:58,960 --> 00:07:02,160
happening ahead

00:07:00,319 --> 00:07:04,880
the device could communicate to other

00:07:02,160 --> 00:07:07,759
devices embedded into the vehicle

00:07:04,880 --> 00:07:08,400
to send out alert and notification

00:07:07,759 --> 00:07:11,360
remote

00:07:08,400 --> 00:07:11,680
vehicle access though it will allow you

00:07:11,360 --> 00:07:14,080
or

00:07:11,680 --> 00:07:15,520
other people who are allowed to access

00:07:14,080 --> 00:07:18,800
the vehicle

00:07:15,520 --> 00:07:20,319
access to the vehicle vehicle tracking

00:07:18,800 --> 00:07:22,960
and information

00:07:20,319 --> 00:07:24,240
tracking the owner right the rig of the

00:07:22,960 --> 00:07:26,880
vehicle

00:07:24,240 --> 00:07:27,840
the wireless phone integration tracking

00:07:26,880 --> 00:07:30,160
your

00:07:27,840 --> 00:07:31,120
phone activity checking your phone

00:07:30,160 --> 00:07:34,800
number

00:07:31,120 --> 00:07:37,360
right radio system and other user

00:07:34,800 --> 00:07:37,360
preferences

00:07:38,960 --> 00:07:43,440
this is a picture of a telematic control

00:07:42,080 --> 00:07:46,400
unit

00:07:43,440 --> 00:07:49,280
the control unit contain different

00:07:46,400 --> 00:07:53,680
integration point with different ports

00:07:49,280 --> 00:07:56,879
each port will get some information from

00:07:53,680 --> 00:07:59,840
other integrated system

00:07:56,879 --> 00:08:01,199
so you can see the micro controller in

00:07:59,840 --> 00:08:04,160
the middle

00:08:01,199 --> 00:08:05,039
can take the information about the

00:08:04,160 --> 00:08:08,400
battery

00:08:05,039 --> 00:08:12,319
about the memory about the

00:08:08,400 --> 00:08:14,400
gps about the you know car

00:08:12,319 --> 00:08:16,560
internal system and all that information

00:08:14,400 --> 00:08:20,560
will get consolidated

00:08:16,560 --> 00:08:20,560
through the telemetry controller unit

00:08:20,800 --> 00:08:25,680
and then the purpose of this unit as you

00:08:22,560 --> 00:08:29,199
can see is to create and collect

00:08:25,680 --> 00:08:30,639
vehicle data from the ports it manages

00:08:29,199 --> 00:08:33,279
the information

00:08:30,639 --> 00:08:34,000
from the communication interface it

00:08:33,279 --> 00:08:36,719
manages

00:08:34,000 --> 00:08:38,479
memory and battery it manage the

00:08:36,719 --> 00:08:41,039
communication to the cloud

00:08:38,479 --> 00:08:43,279
and also manage the application to the

00:08:41,039 --> 00:08:46,080
dashboard in the device

00:08:43,279 --> 00:08:47,360
this is usually embedded into your edge

00:08:46,080 --> 00:08:49,200
devices

00:08:47,360 --> 00:08:52,880
we will talk about the meaning of edge

00:08:49,200 --> 00:08:52,880
devices in the following slides

00:08:53,360 --> 00:08:57,040
an example of telemetric data category

00:08:56,240 --> 00:09:00,240
you have

00:08:57,040 --> 00:09:01,360
the vehicle system category so this data

00:09:00,240 --> 00:09:05,200
will focus on

00:09:01,360 --> 00:09:09,040
the engine the transmission the

00:09:05,200 --> 00:09:10,080
obd the the weight right the lights and

00:09:09,040 --> 00:09:13,200
the tire of the

00:09:10,080 --> 00:09:14,160
vehicle and now on the right-hand side

00:09:13,200 --> 00:09:16,640
you have the

00:09:14,160 --> 00:09:19,279
category for the driver that usually

00:09:16,640 --> 00:09:22,399
keep track of the driver's identity

00:09:19,279 --> 00:09:25,440
the driver's driving behaviors the

00:09:22,399 --> 00:09:27,440
hour of services right the speed the

00:09:25,440 --> 00:09:29,120
acceleration the idling time of the

00:09:27,440 --> 00:09:31,040
driver

00:09:29,120 --> 00:09:32,160
and then in the bottom you have the

00:09:31,040 --> 00:09:35,600
operation

00:09:32,160 --> 00:09:39,440
categories this is usually used by the

00:09:35,600 --> 00:09:42,640
organization to plan and

00:09:39,440 --> 00:09:44,880
and optimize the operation to usually

00:09:42,640 --> 00:09:47,920
reduce operational cost

00:09:44,880 --> 00:09:51,279
and increase performance and efficiency

00:09:47,920 --> 00:09:52,640
this includes tracking the location and

00:09:51,279 --> 00:09:56,480
navigation

00:09:52,640 --> 00:09:58,720
the dispatcher schedule

00:09:56,480 --> 00:10:01,760
the roadside assistant and then the

00:09:58,720 --> 00:10:01,760
delivery time

00:10:03,600 --> 00:10:08,480
now we have a basic understanding on the

00:10:06,720 --> 00:10:11,279
telematic data

00:10:08,480 --> 00:10:11,839
so let's look at how openshift could

00:10:11,279 --> 00:10:15,279
help

00:10:11,839 --> 00:10:18,079
with the telemetric data openshift is a

00:10:15,279 --> 00:10:19,040
distribution of kubernetes it's

00:10:18,079 --> 00:10:21,519
optimized for

00:10:19,040 --> 00:10:23,839
continuous application development

00:10:21,519 --> 00:10:26,560
multi-tenant development

00:10:23,839 --> 00:10:28,640
openshift contains developers and

00:10:26,560 --> 00:10:30,720
operator-centric tools on top of

00:10:28,640 --> 00:10:32,880
kubernetes

00:10:30,720 --> 00:10:35,120
and it enables rapid application and

00:10:32,880 --> 00:10:39,440
development easy deployment

00:10:35,120 --> 00:10:39,440
scaling and life cycle management

00:10:40,640 --> 00:10:44,160
an architecture of openshift will

00:10:42,480 --> 00:10:47,680
contain full stack

00:10:44,160 --> 00:10:48,320
end to end automation so on the top you

00:10:47,680 --> 00:10:52,000
can see

00:10:48,320 --> 00:10:55,200
it manage the git repository

00:10:52,000 --> 00:10:59,680
to infrastructure to databases

00:10:55,200 --> 00:11:01,760
to release cycle and to deployment

00:10:59,680 --> 00:11:03,680
so all these were managed by openshift

00:11:01,760 --> 00:11:07,120
using the

00:11:03,680 --> 00:11:07,120
openshift web console

00:11:07,440 --> 00:11:11,120
in the bottom you can you can see that

00:11:09,360 --> 00:11:14,240
openshift support

00:11:11,120 --> 00:11:16,640
the multi cloud platform

00:11:14,240 --> 00:11:18,160
the hybrid cloud platform and edge

00:11:16,640 --> 00:11:20,240
computing

00:11:18,160 --> 00:11:24,240
we will talk about edge computing in

00:11:20,240 --> 00:11:24,240
more detail in the following slides

00:11:25,760 --> 00:11:30,880
first before we move to openshift we

00:11:28,880 --> 00:11:33,519
need to

00:11:30,880 --> 00:11:37,360
make changes to the telematics system to

00:11:33,519 --> 00:11:39,360
make it as a cloud native application

00:11:37,360 --> 00:11:40,880
usually cloud native application comes

00:11:39,360 --> 00:11:44,480
from the concept of

00:11:40,880 --> 00:11:49,120
devops continuous delivery

00:11:44,480 --> 00:11:49,120
micro services and container

00:11:52,160 --> 00:11:59,360
regarding the openshift cloud native app

00:11:55,279 --> 00:12:02,720
the benefit is that the app is scalable

00:11:59,360 --> 00:12:04,639
it's self-heal based on health check and

00:12:02,720 --> 00:12:07,519
monitoring

00:12:04,639 --> 00:12:09,040
the port depth of approach is continuous

00:12:07,519 --> 00:12:13,680
delivery

00:12:09,040 --> 00:12:15,200
and it decomposes microservices with api

00:12:13,680 --> 00:12:18,720
and this is also leveraging the

00:12:15,200 --> 00:12:18,720
container architecture

00:12:20,560 --> 00:12:27,120
therefore process usually involves

00:12:24,079 --> 00:12:28,240
continuous integration continuous

00:12:27,120 --> 00:12:30,639
testing

00:12:28,240 --> 00:12:32,160
continuous delivery and continuous

00:12:30,639 --> 00:12:35,040
monitoring

00:12:32,160 --> 00:12:37,680
as soon as you change your code into git

00:12:35,040 --> 00:12:40,399
the code will get deployed to

00:12:37,680 --> 00:12:42,240
the cloud and using the health check and

00:12:40,399 --> 00:12:44,320
monitoring we can

00:12:42,240 --> 00:12:45,839
continuously looking at the container

00:12:44,320 --> 00:12:49,040
and making sure that

00:12:45,839 --> 00:12:52,320
the container is up and running

00:12:49,040 --> 00:12:54,480
the as soon as the code is in

00:12:52,320 --> 00:12:56,160
as long as all the integration tests and

00:12:54,480 --> 00:12:58,720
unit tests are passing

00:12:56,160 --> 00:13:02,000
it will integrate it into production and

00:12:58,720 --> 00:13:02,000
that becomes the next release

00:13:03,040 --> 00:13:09,200
the jenkins cicd pipeline will get the

00:13:06,240 --> 00:13:12,079
latest code from the git repository

00:13:09,200 --> 00:13:14,959
dockerize the micro services with the

00:13:12,079 --> 00:13:17,680
docker registry and images

00:13:14,959 --> 00:13:19,279
hook up the unit test integration test

00:13:17,680 --> 00:13:22,399
and automation test

00:13:19,279 --> 00:13:25,360
to the jenkins cicd pipeline with the

00:13:22,399 --> 00:13:26,240
jenkins gated check-in you can recheck

00:13:25,360 --> 00:13:29,120
for example

00:13:26,240 --> 00:13:30,639
any code change that failed the test or

00:13:29,120 --> 00:13:33,200
that did not meet a specific code

00:13:30,639 --> 00:13:36,399
coverage number

00:13:33,200 --> 00:13:41,040
the build artifact will get

00:13:36,399 --> 00:13:43,120
pushed to the artifactory such as jfob

00:13:41,040 --> 00:13:44,480
the jenkins pipeline also supports

00:13:43,120 --> 00:13:47,120
statical analysis

00:13:44,480 --> 00:13:48,000
security cad scanning so that it can

00:13:47,120 --> 00:13:50,880
catch for any

00:13:48,000 --> 00:13:53,440
security vulnerability as part of the

00:13:50,880 --> 00:13:53,440
code change

00:13:54,720 --> 00:13:59,199
chang can also support deploying to

00:13:56,720 --> 00:14:00,800
different openshift environments

00:13:59,199 --> 00:14:02,639
and then after the deployment you will

00:14:00,800 --> 00:14:07,440
do the health check and smoke test

00:14:02,639 --> 00:14:07,440
to validate deployment is successful

00:14:09,360 --> 00:14:14,399
the migration to microservices is the

00:14:12,399 --> 00:14:18,320
idea to break down telemetric

00:14:14,399 --> 00:14:20,880
services into microservices by function

00:14:18,320 --> 00:14:23,519
the microservices needs to contain

00:14:20,880 --> 00:14:25,760
logical blocks of function

00:14:23,519 --> 00:14:26,800
each surface is self-contained

00:14:25,760 --> 00:14:29,519
self-deploy

00:14:26,800 --> 00:14:31,519
and they work independently so each

00:14:29,519 --> 00:14:32,880
surface you can think about it as a quad

00:14:31,519 --> 00:14:35,680
operation

00:14:32,880 --> 00:14:37,680
and for example we may have a location

00:14:35,680 --> 00:14:40,560
surface that deal with

00:14:37,680 --> 00:14:41,440
the location data endpoint right so it

00:14:40,560 --> 00:14:44,480
is a self

00:14:41,440 --> 00:14:45,600
standalone small micro surface when

00:14:44,480 --> 00:14:49,040
there's a high peak

00:14:45,600 --> 00:14:52,320
going into this specific surface it will

00:14:49,040 --> 00:14:54,240
automatically scale up by open shift

00:14:52,320 --> 00:14:57,120
and it would also scale your databases

00:14:54,240 --> 00:14:57,120
based on usage

00:14:57,680 --> 00:15:02,240
other benefit coming with openshift is

00:15:00,000 --> 00:15:04,800
the operational readiness

00:15:02,240 --> 00:15:06,480
we talk about health check health check

00:15:04,800 --> 00:15:09,360
have the liveness probe and

00:15:06,480 --> 00:15:09,920
readiness probe health check it would be

00:15:09,360 --> 00:15:12,880
able to

00:15:09,920 --> 00:15:16,560
paint the surface end point and verify

00:15:12,880 --> 00:15:19,040
the dependent services are running

00:15:16,560 --> 00:15:21,040
other interns such as the vector ops

00:15:19,040 --> 00:15:24,639
integration allow

00:15:21,040 --> 00:15:26,639
error and alert notification to send out

00:15:24,639 --> 00:15:28,399
to engineers when there's a production

00:15:26,639 --> 00:15:30,480
issue

00:15:28,399 --> 00:15:31,759
it also provides integration with slack

00:15:30,480 --> 00:15:33,920
and email

00:15:31,759 --> 00:15:36,560
so that the engineer will get notified

00:15:33,920 --> 00:15:40,240
when there's a production issue

00:15:36,560 --> 00:15:43,040
boomifies is also integrated with shift

00:15:40,240 --> 00:15:44,880
it allows us to track the api calls the

00:15:43,040 --> 00:15:47,680
request the response time

00:15:44,880 --> 00:15:49,360
the error rate so that when there is a

00:15:47,680 --> 00:15:53,199
performance issue we can

00:15:49,360 --> 00:15:55,199
use prometheus to identify the problem

00:15:53,199 --> 00:15:57,519
and for example if a specific error

00:15:55,199 --> 00:16:00,320
messages is happening too many times

00:15:57,519 --> 00:16:03,759
omifius will be able to flag the alert

00:16:00,320 --> 00:16:03,759
and send out notification

00:16:04,160 --> 00:16:07,279
openshift also has sprung in her

00:16:05,920 --> 00:16:09,360
integration

00:16:07,279 --> 00:16:10,639
all the warning and error logs will get

00:16:09,360 --> 00:16:13,040
sent to spun

00:16:10,639 --> 00:16:15,920
so that you can query and look at this

00:16:13,040 --> 00:16:15,920
log in details

00:16:16,079 --> 00:16:19,120
and jaeger tracing for example is

00:16:18,480 --> 00:16:22,160
another

00:16:19,120 --> 00:16:22,959
integration with openshift it allows us

00:16:22,160 --> 00:16:25,440
to group

00:16:22,959 --> 00:16:26,800
services together using a common request

00:16:25,440 --> 00:16:29,920
id

00:16:26,800 --> 00:16:31,360
so that we can track the surface

00:16:29,920 --> 00:16:33,680
orchestration

00:16:31,360 --> 00:16:37,839
the surface core workflow and use that

00:16:33,680 --> 00:16:37,839
for debugging and troubleshooting

00:16:38,320 --> 00:16:41,519
so now you understand health check and

00:16:40,240 --> 00:16:45,120
monitoring

00:16:41,519 --> 00:16:47,920
are all tied to our container and

00:16:45,120 --> 00:16:48,800
we could also use metrics locking and

00:16:47,920 --> 00:16:51,839
tracing

00:16:48,800 --> 00:16:51,839
to get it from the container

00:16:54,160 --> 00:16:58,800
so now let's introduce our idea called

00:16:56,320 --> 00:17:01,839
ifta ifta

00:16:58,800 --> 00:17:05,600
gifter stands for international field

00:17:01,839 --> 00:17:09,439
tax agreement so each day

00:17:05,600 --> 00:17:11,600
has their own ifta and then basically

00:17:09,439 --> 00:17:13,039
the idea is that when you are driving

00:17:11,600 --> 00:17:14,799
your truck

00:17:13,039 --> 00:17:16,160
going through different stay you need to

00:17:14,799 --> 00:17:18,640
pay tax

00:17:16,160 --> 00:17:20,559
right and then the tax is a lot of time

00:17:18,640 --> 00:17:24,000
based on the mileage

00:17:20,559 --> 00:17:24,640
or based on how much time you have spent

00:17:24,000 --> 00:17:28,319
in each

00:17:24,640 --> 00:17:29,440
stay and this calculation is super

00:17:28,319 --> 00:17:35,039
complicated because

00:17:29,440 --> 00:17:35,039
because each day has its own easter law

00:17:35,760 --> 00:17:39,440
so this is as example micro surface

00:17:38,640 --> 00:17:41,600
architecture

00:17:39,440 --> 00:17:43,679
where you can see we are using the

00:17:41,600 --> 00:17:45,919
telemetric surface

00:17:43,679 --> 00:17:48,160
generate the telematic events and feed

00:17:45,919 --> 00:17:50,799
the data back to ifta

00:17:48,160 --> 00:17:52,320
so that iftar can use that information

00:17:50,799 --> 00:17:55,120
to calculate the tax

00:17:52,320 --> 00:17:56,960
for for for the payment for the state

00:17:55,120 --> 00:17:58,880
and then at the same time you have you

00:17:56,960 --> 00:18:02,000
know vehicle service that keep track of

00:17:58,880 --> 00:18:03,840
other vehicles and they all fit into

00:18:02,000 --> 00:18:07,280
different

00:18:03,840 --> 00:18:10,400
databases in the in the bottom and then

00:18:07,280 --> 00:18:13,120
on top of the database there was a layer

00:18:10,400 --> 00:18:14,000
a surface layer that get the data from

00:18:13,120 --> 00:18:16,960
the database

00:18:14,000 --> 00:18:19,679
and feed that information back to the ui

00:18:16,960 --> 00:18:19,679
and the gateway

00:18:22,400 --> 00:18:29,280
so the iftar calculation is complicated

00:18:25,919 --> 00:18:32,880
for example if you are driving from

00:18:29,280 --> 00:18:36,160
trinidad colorado to a

00:18:32,880 --> 00:18:39,039
tradford in texas right

00:18:36,160 --> 00:18:40,720
you could easily go through four

00:18:39,039 --> 00:18:44,720
different states

00:18:40,720 --> 00:18:48,240
based on your gps calculation but

00:18:44,720 --> 00:18:49,200
this calculation is changing at real

00:18:48,240 --> 00:18:52,320
time

00:18:49,200 --> 00:18:54,240
so for example if there's an accident

00:18:52,320 --> 00:18:56,320
on the freeway or if there's a

00:18:54,240 --> 00:19:00,320
construction in the freeway

00:18:56,320 --> 00:19:03,360
the wow from the gps could change

00:19:00,320 --> 00:19:06,559
so any change will result in

00:19:03,360 --> 00:19:08,960
different iftar calculation you may end

00:19:06,559 --> 00:19:11,919
up not going to a specific state

00:19:08,960 --> 00:19:12,400
or you can end up you know going into

00:19:11,919 --> 00:19:16,160
more

00:19:12,400 --> 00:19:19,039
different state so this calculation is

00:19:16,160 --> 00:19:24,160
dynamic and it rely heavily on the

00:19:19,039 --> 00:19:26,320
real-time telemetric data

00:19:24,160 --> 00:19:28,000
so for iftar calculation we need the

00:19:26,320 --> 00:19:28,880
real-time data from the telemetric

00:19:28,000 --> 00:19:31,760
surface

00:19:28,880 --> 00:19:32,480
to be performed the actual calculation

00:19:31,760 --> 00:19:35,440
right

00:19:32,480 --> 00:19:37,360
any change in the gps calculation could

00:19:35,440 --> 00:19:41,039
result in different mileage and time

00:19:37,360 --> 00:19:41,039
span in different jurisdiction

00:19:41,200 --> 00:19:44,559
a lot of time operation

00:19:44,720 --> 00:19:49,280
would be focused on how to optimize the

00:19:47,600 --> 00:19:50,880
operational cost

00:19:49,280 --> 00:19:52,799
by going through the state with the

00:19:50,880 --> 00:19:56,160
lowest tax

00:19:52,799 --> 00:19:59,679
and lowers fuel tax in this situation

00:19:56,160 --> 00:19:59,679
the edge computing could help

00:20:00,480 --> 00:20:04,960
so now we understand the big picture of

00:20:03,760 --> 00:20:07,039
open shift

00:20:04,960 --> 00:20:09,600
so how does edge computing come into

00:20:07,039 --> 00:20:09,600
open shift

00:20:09,679 --> 00:20:13,600
edge computing is the idea to place the

00:20:12,320 --> 00:20:15,760
workload

00:20:13,600 --> 00:20:17,440
as close to the device where the data

00:20:15,760 --> 00:20:20,880
were created

00:20:17,440 --> 00:20:22,880
and the action was taken as possible

00:20:20,880 --> 00:20:25,039
so the idea is if you put your

00:20:22,880 --> 00:20:26,240
calculation and computational power and

00:20:25,039 --> 00:20:28,799
resources

00:20:26,240 --> 00:20:29,360
closest to your device you would get a

00:20:28,799 --> 00:20:32,559
much

00:20:29,360 --> 00:20:34,000
faster response from the calculation so

00:20:32,559 --> 00:20:37,440
that you can take action

00:20:34,000 --> 00:20:39,760
a little bit faster the telematic data

00:20:37,440 --> 00:20:42,080
gathered from the device will make will

00:20:39,760 --> 00:20:45,440
be made available to the cloud

00:20:42,080 --> 00:20:48,159
so how does that work we leverage the 5g

00:20:45,440 --> 00:20:52,640
network to open up the compute

00:20:48,159 --> 00:20:52,640
computing capacity into the devices

00:20:54,480 --> 00:20:59,679
this is a picture of edge computing

00:20:57,440 --> 00:21:02,159
using 5g network

00:20:59,679 --> 00:21:04,320
and in the bottom of the page you see

00:21:02,159 --> 00:21:06,480
there are different edge devices

00:21:04,320 --> 00:21:09,280
that were embedded into the truck and

00:21:06,480 --> 00:21:11,360
different trucks were driving

00:21:09,280 --> 00:21:12,720
as soon as there is a 5g network

00:21:11,360 --> 00:21:15,200
available

00:21:12,720 --> 00:21:16,640
the data will get transmitted to the

00:21:15,200 --> 00:21:20,000
edge node

00:21:16,640 --> 00:21:23,360
edge node is a layer it's a sub layer

00:21:20,000 --> 00:21:25,440
on the cloud that's specifically

00:21:23,360 --> 00:21:26,559
configured to interface with the edge

00:21:25,440 --> 00:21:30,320
devices

00:21:26,559 --> 00:21:33,200
so that it would do to the computation

00:21:30,320 --> 00:21:34,720
um at the lowest level and as fast as

00:21:33,200 --> 00:21:37,039
possible

00:21:34,720 --> 00:21:38,799
right and then the edge node also

00:21:37,039 --> 00:21:40,320
integrate with the other the rest of the

00:21:38,799 --> 00:21:44,960
cloud system

00:21:40,320 --> 00:21:47,840
by submitting the data to the cloud

00:21:44,960 --> 00:21:49,840
the cloud will do the data backup and do

00:21:47,840 --> 00:21:52,480
further data analysis

00:21:49,840 --> 00:21:53,120
and and generate the you know user

00:21:52,480 --> 00:21:56,080
dashboard

00:21:53,120 --> 00:21:58,559
and interfaces for the operator at the

00:21:56,080 --> 00:22:01,520
organization to look at the data

00:21:58,559 --> 00:22:02,080
so this is a high level architecture

00:22:01,520 --> 00:22:04,960
right

00:22:02,080 --> 00:22:05,280
sometimes you have more than one tiers

00:22:04,960 --> 00:22:08,080
of

00:22:05,280 --> 00:22:09,360
edge nodes and we will talk about the

00:22:08,080 --> 00:22:13,840
benefit of doing that

00:22:09,360 --> 00:22:13,840
in the following slides

00:22:14,000 --> 00:22:18,640
in the edge node this is just a piece of

00:22:17,440 --> 00:22:22,080
it equipment

00:22:18,640 --> 00:22:24,720
built for itu workload computation

00:22:22,080 --> 00:22:26,080
it could be a full blade server right

00:22:24,720 --> 00:22:28,159
usually it means to be

00:22:26,080 --> 00:22:29,120
lightweight right you have a small

00:22:28,159 --> 00:22:31,679
server

00:22:29,120 --> 00:22:32,720
that's with you know limited or some

00:22:31,679 --> 00:22:34,960
small

00:22:32,720 --> 00:22:36,640
computational power and we want to do

00:22:34,960 --> 00:22:40,480
the calculation fast

00:22:36,640 --> 00:22:42,080
and lightweight in terms of the edge

00:22:40,480 --> 00:22:45,200
devices

00:22:42,080 --> 00:22:47,120
you would have a piece of id equipment

00:22:45,200 --> 00:22:49,280
built for gathering different telemetric

00:22:47,120 --> 00:22:52,960
data in your vehicle

00:22:49,280 --> 00:22:56,960
including the sensors the gps navigation

00:22:52,960 --> 00:23:01,840
the radio camera internal car system

00:22:56,960 --> 00:23:05,200
and its cpu so on average we have about

00:23:01,840 --> 00:23:06,880
20 to 30 different cpu in it so you see

00:23:05,200 --> 00:23:09,600
there was a lot a lot of

00:23:06,880 --> 00:23:10,640
you computational resources we could

00:23:09,600 --> 00:23:13,840
take advantage

00:23:10,640 --> 00:23:13,840
from within the vehicle

00:23:15,280 --> 00:23:19,679
and for the edge devices the

00:23:18,000 --> 00:23:22,080
computational capacity

00:23:19,679 --> 00:23:23,520
has increased significantly in the last

00:23:22,080 --> 00:23:26,880
decade

00:23:23,520 --> 00:23:28,720
many of them were running in linux

00:23:26,880 --> 00:23:30,159
that means we can we could deploy

00:23:28,720 --> 00:23:32,720
containerize

00:23:30,159 --> 00:23:34,840
workload into the edge devices as long

00:23:32,720 --> 00:23:37,840
as the edge device support

00:23:34,840 --> 00:23:37,840
linux

00:23:38,960 --> 00:23:43,840
so now the next question would be how do

00:23:41,200 --> 00:23:45,679
we manage these different environments

00:23:43,840 --> 00:23:47,279
and ensure that the wide workload

00:23:45,679 --> 00:23:50,080
deployed to the

00:23:47,279 --> 00:23:52,960
edge devices and the edge node at

00:23:50,080 --> 00:23:52,960
specific time

00:23:54,720 --> 00:23:58,880
the answer would be using openshift

00:23:57,360 --> 00:24:01,600
openshift

00:23:58,880 --> 00:24:04,720
build the workloads as containers

00:24:01,600 --> 00:24:07,840
containers are deployed for scaling

00:24:04,720 --> 00:24:07,840
and availability

00:24:08,240 --> 00:24:12,720
it enabled the kubernetes to run on the

00:24:11,039 --> 00:24:14,480
cloud

00:24:12,720 --> 00:24:17,200
and then we basically use the same

00:24:14,480 --> 00:24:21,760
concept to manage and deploy

00:24:17,200 --> 00:24:21,760
workloads into the edge devices

00:24:23,440 --> 00:24:29,600
some technical problem that we have seen

00:24:26,640 --> 00:24:32,240
the number of edge devices are very high

00:24:29,600 --> 00:24:33,120
it's estimated all together 50 to 100

00:24:32,240 --> 00:24:35,919
billions of

00:24:33,120 --> 00:24:37,760
devices in the fields right these

00:24:35,919 --> 00:24:40,400
devices are coming into many different

00:24:37,760 --> 00:24:43,440
forms and configuration

00:24:40,400 --> 00:24:46,400
security is also a concern because the

00:24:43,440 --> 00:24:49,679
edge devices are outside of the boundary

00:24:46,400 --> 00:24:51,679
of your data center

00:24:49,679 --> 00:24:55,279
therefore the data associated with the

00:24:51,679 --> 00:24:58,640
edge devices need to be protected

00:24:55,279 --> 00:25:01,039
and it is not possible to do any manual

00:24:58,640 --> 00:25:03,039
deployment or configuration

00:25:01,039 --> 00:25:05,679
on the edge devices because there's just

00:25:03,039 --> 00:25:07,600
so many

00:25:05,679 --> 00:25:10,000
so we need to a way to distribute the

00:25:07,600 --> 00:25:12,559
workload to billions of edge devices in

00:25:10,000 --> 00:25:12,559
a massive

00:25:12,840 --> 00:25:15,760
scale

00:25:14,080 --> 00:25:18,400
how do we use azure deployment with

00:25:15,760 --> 00:25:18,400
openshift

00:25:18,480 --> 00:25:26,000
openshift ensure the uniformity

00:25:22,400 --> 00:25:31,120
consistency security and reliability

00:25:26,000 --> 00:25:31,120
force the astronaut and edge devices

00:25:31,200 --> 00:25:37,360
openshift define and provision

00:25:34,080 --> 00:25:39,120
standardization in snow and ash devices

00:25:37,360 --> 00:25:41,200
we could deploy the appropriate

00:25:39,120 --> 00:25:42,960
application resources based on their

00:25:41,200 --> 00:25:45,919
purpose

00:25:42,960 --> 00:25:46,720
so for example the gps has a specific

00:25:45,919 --> 00:25:50,240
purpose of

00:25:46,720 --> 00:25:52,720
gathering and tracking the location

00:25:50,240 --> 00:25:53,919
so we would only deploy workloads that

00:25:52,720 --> 00:26:00,000
were related to

00:25:53,919 --> 00:26:00,000
locations to the gpu to the gps device

00:26:00,240 --> 00:26:03,600
when we deploy new edge location

00:26:02,640 --> 00:26:05,600
openshift will

00:26:03,600 --> 00:26:07,039
ensure that the new appointment are

00:26:05,600 --> 00:26:10,640
standardized

00:26:07,039 --> 00:26:14,559
compliant and secure so that it would

00:26:10,640 --> 00:26:15,440
keep the cluster to to to be uniform

00:26:14,559 --> 00:26:18,400
across all

00:26:15,440 --> 00:26:19,279
business footprint so uniformity is

00:26:18,400 --> 00:26:21,279
really the key

00:26:19,279 --> 00:26:22,799
right you want to make sure that the

00:26:21,279 --> 00:26:24,240
same

00:26:22,799 --> 00:26:27,840
container continualized application

00:26:24,240 --> 00:26:30,159
could be deployed to all this different

00:26:27,840 --> 00:26:32,880
platform using a container-wise

00:26:30,159 --> 00:26:32,880
architecture

00:26:34,720 --> 00:26:41,440
so earlier with the diagram about the

00:26:37,760 --> 00:26:42,480
edge deployment so so there are actually

00:26:41,440 --> 00:26:44,640
more than two

00:26:42,480 --> 00:26:47,520
layers right sometimes in this situation

00:26:44,640 --> 00:26:51,520
you have five or six different layers of

00:26:47,520 --> 00:26:54,000
edge tiers for deployments so

00:26:51,520 --> 00:26:54,720
on the far left you have the device

00:26:54,000 --> 00:26:58,159
layer

00:26:54,720 --> 00:27:02,320
right this is where you deploy the

00:26:58,159 --> 00:27:04,720
um container allocation to devices

00:27:02,320 --> 00:27:06,000
it used to you know think about this

00:27:04,720 --> 00:27:09,600
device layer is

00:27:06,000 --> 00:27:12,880
really limited

00:27:09,600 --> 00:27:13,919
computational power you know with

00:27:12,880 --> 00:27:16,400
sometimes you don't have any

00:27:13,919 --> 00:27:18,799
availability uh ha

00:27:16,400 --> 00:27:21,360
setup but you have one instant that will

00:27:18,799 --> 00:27:23,039
deploy to this edge device

00:27:21,360 --> 00:27:26,320
so on top of the edge device you have

00:27:23,039 --> 00:27:30,240
the snow layer where you could have some

00:27:26,320 --> 00:27:34,640
simple infrastructure configuration

00:27:30,240 --> 00:27:38,799
you could set up high availability

00:27:34,640 --> 00:27:41,039
scalability geo redundancy on that layer

00:27:38,799 --> 00:27:43,679
at the provider layer this is the layer

00:27:41,039 --> 00:27:47,760
where you would

00:27:43,679 --> 00:27:51,200
you would communicate between the edge

00:27:47,760 --> 00:27:54,000
no edge device and your data center

00:27:51,200 --> 00:27:54,880
right it have different provider layer

00:27:54,000 --> 00:27:56,640
setup

00:27:54,880 --> 00:27:58,880
for example the provider layer for the

00:27:56,640 --> 00:28:02,000
far edge

00:27:58,880 --> 00:28:04,000
the provider layer for access edge so

00:28:02,000 --> 00:28:05,760
for example you have different

00:28:04,000 --> 00:28:07,360
type different tiers of the data you

00:28:05,760 --> 00:28:08,720
want them to have different

00:28:07,360 --> 00:28:11,360
accessibility

00:28:08,720 --> 00:28:13,679
so this would be the way the the the

00:28:11,360 --> 00:28:16,159
level the layer that you set up the

00:28:13,679 --> 00:28:18,000
access edge and at the end you have the

00:28:16,159 --> 00:28:20,159
provider aggregation edge

00:28:18,000 --> 00:28:21,679
where you aggregate these different

00:28:20,159 --> 00:28:24,000
different data together

00:28:21,679 --> 00:28:24,880
into an aggregation layer so that you

00:28:24,000 --> 00:28:28,559
could do

00:28:24,880 --> 00:28:30,640
a layer surface orchestration

00:28:28,559 --> 00:28:33,120
and then in the core circle in the

00:28:30,640 --> 00:28:36,320
middle you have the

00:28:33,120 --> 00:28:37,679
provider enterprise the data center

00:28:36,320 --> 00:28:39,360
right your data center could be a

00:28:37,679 --> 00:28:40,480
regional data center or it could be on

00:28:39,360 --> 00:28:43,600
the cloud

00:28:40,480 --> 00:28:47,520
right so this layer is also

00:28:43,600 --> 00:28:47,520
deploy using openshift

00:28:49,039 --> 00:28:53,360
so so taking looking at this picture you

00:28:52,159 --> 00:28:56,080
can see that

00:28:53,360 --> 00:28:57,520
as we are moving from the edge cluster

00:28:56,080 --> 00:29:01,520
layer

00:28:57,520 --> 00:29:03,360
to the right to the device layer

00:29:01,520 --> 00:29:04,720
this configuration will change

00:29:03,360 --> 00:29:07,840
significantly

00:29:04,720 --> 00:29:10,399
at the edge layer if it comes with if

00:29:07,840 --> 00:29:12,559
you deploy with openshift 4.5

00:29:10,399 --> 00:29:15,919
it automatically go with three

00:29:12,559 --> 00:29:19,120
masternode and three worker nodes

00:29:15,919 --> 00:29:21,919
these are set up for high availability

00:29:19,120 --> 00:29:23,919
and if one of the worker is down you the

00:29:21,919 --> 00:29:27,200
other worker will step up and

00:29:23,919 --> 00:29:29,200
continue to support the workload

00:29:27,200 --> 00:29:31,039
as you move to the right you see the

00:29:29,200 --> 00:29:33,600
remote worker node

00:29:31,039 --> 00:29:35,760
uh will config will be configured a

00:29:33,600 --> 00:29:38,799
little bit differently you have

00:29:35,760 --> 00:29:40,880
one worker with three master

00:29:38,799 --> 00:29:42,559
and then as you go further to the y you

00:29:40,880 --> 00:29:44,720
have a single node

00:29:42,559 --> 00:29:47,520
single node single edge node server is

00:29:44,720 --> 00:29:49,679
basically one worker and one

00:29:47,520 --> 00:29:50,880
and then on the device you you basically

00:29:49,679 --> 00:29:53,440
don't have any uh

00:29:50,880 --> 00:29:55,200
anymore replication you only have one

00:29:53,440 --> 00:29:57,120
single container deployed into the

00:29:55,200 --> 00:30:00,799
device because of the limitation

00:29:57,120 --> 00:30:03,120
of the of the computational resources

00:30:00,799 --> 00:30:04,240
and as you go from the left to the right

00:30:03,120 --> 00:30:06,960
you can see

00:30:04,240 --> 00:30:08,159
this is also depend on the network

00:30:06,960 --> 00:30:11,760
connection

00:30:08,159 --> 00:30:15,440
the edge cluster with three nodes

00:30:11,760 --> 00:30:18,320
have more reliable network right

00:30:15,440 --> 00:30:19,440
and then as you go all the way to the to

00:30:18,320 --> 00:30:23,200
the right

00:30:19,440 --> 00:30:25,600
the edge device which basically depends

00:30:23,200 --> 00:30:28,880
on the 5g network has really weak

00:30:25,600 --> 00:30:32,480
connection though the idea is that if

00:30:28,880 --> 00:30:35,440
the connection is not available it

00:30:32,480 --> 00:30:37,679
the the workload won't be won't be able

00:30:35,440 --> 00:30:40,080
to pass on to the left

00:30:37,679 --> 00:30:41,120
so so some retry logic needs to be

00:30:40,080 --> 00:30:43,600
happened

00:30:41,120 --> 00:30:45,200
so that you can cache the data locally

00:30:43,600 --> 00:30:47,039
on the edge device

00:30:45,200 --> 00:30:49,440
and as soon as the 5g network is

00:30:47,039 --> 00:30:52,159
available it will push the data up to

00:30:49,440 --> 00:30:52,159
the next level

00:30:54,320 --> 00:31:00,640
so we have an example object where

00:30:58,559 --> 00:31:02,000
it take the the highway the role

00:31:00,640 --> 00:31:03,840
condition

00:31:02,000 --> 00:31:06,000
and send our alert based on telemetric

00:31:03,840 --> 00:31:09,200
data

00:31:06,000 --> 00:31:11,519
the truck camera will keep track and

00:31:09,200 --> 00:31:13,760
capture the world condition as an image

00:31:11,519 --> 00:31:16,720
stream as a video stream

00:31:13,760 --> 00:31:18,240
video stream is a collection of images

00:31:16,720 --> 00:31:21,519
each image is tagged

00:31:18,240 --> 00:31:23,840
with the location led long and the time

00:31:21,519 --> 00:31:23,840
frame

00:31:24,000 --> 00:31:29,840
when the image was saved and gets

00:31:26,559 --> 00:31:32,960
submitted to the edge node

00:31:29,840 --> 00:31:33,440
we have some machine learning program to

00:31:32,960 --> 00:31:37,200
run

00:31:33,440 --> 00:31:37,840
on flag the images with with hot servers

00:31:37,200 --> 00:31:41,279
condition

00:31:37,840 --> 00:31:42,559
on the edge node so for the images that

00:31:41,279 --> 00:31:44,840
were flagged

00:31:42,559 --> 00:31:46,159
we will trigger the alert and

00:31:44,840 --> 00:31:48,000
notification

00:31:46,159 --> 00:31:50,240
based on the image location and based on

00:31:48,000 --> 00:31:53,440
the image timestamp

00:31:50,240 --> 00:31:57,279
and send the alert back to the

00:31:53,440 --> 00:32:02,080
edge devices which are within the

00:31:57,279 --> 00:32:04,080
the location and the time stamp

00:32:02,080 --> 00:32:05,440
so once these different edge devices get

00:32:04,080 --> 00:32:09,200
a notification

00:32:05,440 --> 00:32:10,240
it will trigger the gps to recalculate

00:32:09,200 --> 00:32:12,080
at the world

00:32:10,240 --> 00:32:15,039
so that it can take a different while to

00:32:12,080 --> 00:32:16,720
avoid the accident

00:32:15,039 --> 00:32:18,799
and then similarly the machine learning

00:32:16,720 --> 00:32:19,919
layer will also communicate to the cloud

00:32:18,799 --> 00:32:22,960
layer

00:32:19,919 --> 00:32:26,799
and talk to openshift

00:32:22,960 --> 00:32:29,360
when openshift got notified about

00:32:26,799 --> 00:32:30,480
about the the images that reflect for

00:32:29,360 --> 00:32:34,720
accident

00:32:30,480 --> 00:32:36,799
it will update and notify the operator

00:32:34,720 --> 00:32:37,919
the telematics dashboard will also get

00:32:36,799 --> 00:32:39,919
updated

00:32:37,919 --> 00:32:42,080
so that the operator can go in and

00:32:39,919 --> 00:32:45,919
manually configure

00:32:42,080 --> 00:32:49,519
the driver to take a different schedule

00:32:45,919 --> 00:32:53,360
or or go to a different destination

00:32:49,519 --> 00:32:55,279
right basically the

00:32:53,360 --> 00:32:56,880
alert will allow the operator to

00:32:55,279 --> 00:33:00,559
optimize

00:32:56,880 --> 00:33:01,760
the activities and minimize the

00:33:00,559 --> 00:33:04,320
operational cost

00:33:01,760 --> 00:33:04,320
of the date

00:33:06,960 --> 00:33:10,480
the telematic image with openshift is

00:33:09,600 --> 00:33:14,080
based on the

00:33:10,480 --> 00:33:16,799
telematic image from the edge devices

00:33:14,080 --> 00:33:19,279
and as you see the world condition will

00:33:16,799 --> 00:33:23,039
get analyzed at the ethno and alert will

00:33:19,279 --> 00:33:26,240
get triggered to notify other devices

00:33:23,039 --> 00:33:30,000
so um the edge computing solved the

00:33:26,240 --> 00:33:32,480
computational resources and latency

00:33:30,000 --> 00:33:34,640
the image were able to transmit through

00:33:32,480 --> 00:33:38,000
5g network

00:33:34,640 --> 00:33:41,200
when the network is not available we try

00:33:38,000 --> 00:33:43,120
will happen if after a couple week tries

00:33:41,200 --> 00:33:47,039
and is still not available

00:33:43,120 --> 00:33:49,679
the edge device will cache the image

00:33:47,039 --> 00:33:52,399
and wait for the next availability of

00:33:49,679 --> 00:33:55,600
the 5g network

00:33:52,399 --> 00:33:58,960
the telematic image will be backup

00:33:55,600 --> 00:34:01,919
in openshift when it receives the data

00:33:58,960 --> 00:34:01,919
from the edge node

00:34:04,399 --> 00:34:08,399
so from the dashboard you can see this

00:34:06,480 --> 00:34:12,960
is an example where

00:34:08,399 --> 00:34:16,240
it would update the accident

00:34:12,960 --> 00:34:17,200
indicator based on the image based on

00:34:16,240 --> 00:34:19,280
the

00:34:17,200 --> 00:34:21,520
location that long and then the

00:34:19,280 --> 00:34:24,480
timestamp

00:34:21,520 --> 00:34:25,520
so from the operator point of view the

00:34:24,480 --> 00:34:27,440
operator can come

00:34:25,520 --> 00:34:29,760
in and look at all these different

00:34:27,440 --> 00:34:35,839
incidents from the dashboard

00:34:29,760 --> 00:34:35,839
and then try to plan ahead for the day

00:34:36,480 --> 00:34:40,879
so um as you see the our application is

00:34:39,839 --> 00:34:44,560
using

00:34:40,879 --> 00:34:46,879
a container-wise application

00:34:44,560 --> 00:34:49,280
openshift support gpu graphical

00:34:46,879 --> 00:34:52,960
processing unit

00:34:49,280 --> 00:34:55,040
so the latency of the image data

00:34:52,960 --> 00:34:56,159
going from the edge device to the edge

00:34:55,040 --> 00:34:59,440
node is

00:34:56,159 --> 00:35:01,520
less than 200 milliseconds

00:34:59,440 --> 00:35:02,800
the latency for the machine learning for

00:35:01,520 --> 00:35:06,160
scanning each image

00:35:02,800 --> 00:35:07,280
is less than 300 milliseconds and we

00:35:06,160 --> 00:35:10,560
have a bandwidth of

00:35:07,280 --> 00:35:11,359
of about 5 megabyte per second for

00:35:10,560 --> 00:35:17,599
upload

00:35:11,359 --> 00:35:20,480
of the telemetric image

00:35:17,599 --> 00:35:23,359
this is a layer of machine learning

00:35:20,480 --> 00:35:25,040
configuration that we use

00:35:23,359 --> 00:35:27,440
this is basically an artificial neural

00:35:25,040 --> 00:35:31,040
network each image coming in

00:35:27,440 --> 00:35:32,960
we will do some image feature extraction

00:35:31,040 --> 00:35:34,560
and based on the feature we can predict

00:35:32,960 --> 00:35:38,720
the true or false

00:35:34,560 --> 00:35:38,720
whether the image is an accident or not

00:35:39,200 --> 00:35:42,400
and these are some of the factors that

00:35:40,640 --> 00:35:44,079
you could consider when you are doing

00:35:42,400 --> 00:35:46,160
the image extraction

00:35:44,079 --> 00:35:48,000
or you could look for the accident call

00:35:46,160 --> 00:35:51,599
you can look for specific

00:35:48,000 --> 00:35:56,079
accident sign right look for police car

00:35:51,599 --> 00:35:56,079
ambulance right and so on

00:35:56,400 --> 00:36:00,160
so each image has a score between one

00:35:58,560 --> 00:36:03,599
and zero

00:36:00,160 --> 00:36:06,079
one means the image uh signify a

00:36:03,599 --> 00:36:08,320
prediction of a potential accident

00:36:06,079 --> 00:36:10,480
the alert will get trigger based on the

00:36:08,320 --> 00:36:13,359
image location

00:36:10,480 --> 00:36:16,000
a score of 0 would signify the image

00:36:13,359 --> 00:36:19,599
does not associate with an accident

00:36:16,000 --> 00:36:21,839
the cutoff value we set off is 70

00:36:19,599 --> 00:36:24,480
right if it's lower than 70 percent is

00:36:21,839 --> 00:36:26,720
not an accident

00:36:24,480 --> 00:36:29,119
and based on the model we can see that

00:36:26,720 --> 00:36:31,760
the prediction could reach 97

00:36:29,119 --> 00:36:31,760
accuracy

00:36:33,200 --> 00:36:37,839
so the last piece we want to cover is

00:36:35,839 --> 00:36:40,240
the real time telematic

00:36:37,839 --> 00:36:41,680
we want to execute jobs within minute or

00:36:40,240 --> 00:36:44,880
within a mile

00:36:41,680 --> 00:36:47,280
when the real time data come in into

00:36:44,880 --> 00:36:49,520
the telematics system so how do we do

00:36:47,280 --> 00:36:49,520
that

00:36:50,480 --> 00:36:54,560
the idea is that we could use change

00:36:53,040 --> 00:36:57,920
data capture

00:36:54,560 --> 00:37:00,000
on the applications this is a design

00:36:57,920 --> 00:37:02,640
pattern for event driven cloud native

00:37:00,000 --> 00:37:05,119
application with openshift

00:37:02,640 --> 00:37:08,560
change data capture continuously

00:37:05,119 --> 00:37:11,680
identify the incremental data change

00:37:08,560 --> 00:37:14,160
the real-time data replication across

00:37:11,680 --> 00:37:17,280
databases and replica were done through

00:37:14,160 --> 00:37:17,280
the transaction logs

00:37:17,520 --> 00:37:21,760
we triggered events based on data change

00:37:20,400 --> 00:37:24,720
we captured

00:37:21,760 --> 00:37:28,880
and propagate the data to and the events

00:37:24,720 --> 00:37:28,880
to the micro surface within the system

00:37:30,000 --> 00:37:35,200
based on the exchange data capture is

00:37:33,440 --> 00:37:38,320
based on the upstream

00:37:35,200 --> 00:37:40,240
the pcm project and it will integrate

00:37:38,320 --> 00:37:42,960
really well with spring boot

00:37:40,240 --> 00:37:42,960
and open shift

00:37:44,079 --> 00:37:47,599
so the purpose of using cdc is to

00:37:46,800 --> 00:37:50,240
increase

00:37:47,599 --> 00:37:52,160
as we increase the data collection where

00:37:50,240 --> 00:37:52,880
we and the entire database become really

00:37:52,160 --> 00:37:56,079
expensive

00:37:52,880 --> 00:37:58,480
as your data size is growing

00:37:56,079 --> 00:38:00,079
and you know backing up database and you

00:37:58,480 --> 00:38:01,920
know through a full scan on a database

00:38:00,079 --> 00:38:04,480
is also expensive

00:38:01,920 --> 00:38:07,359
cdc would make the real-time data

00:38:04,480 --> 00:38:09,119
replication possible

00:38:07,359 --> 00:38:11,359
the idea is that we have the kinematic

00:38:09,119 --> 00:38:13,839
database on the left

00:38:11,359 --> 00:38:15,200
we have different query coming in to do

00:38:13,839 --> 00:38:19,440
the insert

00:38:15,200 --> 00:38:22,000
update and delete and when this query

00:38:19,440 --> 00:38:25,280
finish we have the transaction logs

00:38:22,000 --> 00:38:25,680
these logs will be uh then then waiting

00:38:25,280 --> 00:38:28,720
at the

00:38:25,680 --> 00:38:32,800
event bus we push it to the

00:38:28,720 --> 00:38:36,400
cdc relay which is the dbcm and then

00:38:32,800 --> 00:38:38,960
the bcm will notify the elasticsearch

00:38:36,400 --> 00:38:41,839
engine on the right hand side which

00:38:38,960 --> 00:38:44,400
contain the telemetric index

00:38:41,839 --> 00:38:44,960
so once this data is updated it will

00:38:44,400 --> 00:38:48,400
trigger

00:38:44,960 --> 00:38:51,200
the notification to do the

00:38:48,400 --> 00:38:52,000
uh dependent services and it will update

00:38:51,200 --> 00:38:55,040
the

00:38:52,000 --> 00:38:58,720
automatic dashboard the um

00:38:55,040 --> 00:39:00,560
operator dashboard right and email

00:38:58,720 --> 00:39:02,720
notification and all that

00:39:00,560 --> 00:39:04,400
so you can see um this is a really

00:39:02,720 --> 00:39:07,440
effective um

00:39:04,400 --> 00:39:07,839
architecture because now everything when

00:39:07,440 --> 00:39:10,000
that

00:39:07,839 --> 00:39:12,640
every time there's a new change coming

00:39:10,000 --> 00:39:13,359
in and if the changes are insert update

00:39:12,640 --> 00:39:15,359
and delete

00:39:13,359 --> 00:39:17,119
then we will do some action based on the

00:39:15,359 --> 00:39:19,839
text based on the

00:39:17,119 --> 00:39:19,839
query type

00:39:20,160 --> 00:39:28,560
to integrate the dbcm into springboot

00:39:24,640 --> 00:39:32,599
all you need is to open the dependency

00:39:28,560 --> 00:39:37,599
tom.demo file and update the

00:39:32,599 --> 00:39:37,599
fdbcm dependency

00:39:38,079 --> 00:39:42,720
the cdc listener is basically a java

00:39:41,200 --> 00:39:47,040
program that

00:39:42,720 --> 00:39:49,280
is set up to to take the constructor

00:39:47,040 --> 00:39:51,839
to load the configuration and set

00:39:49,280 --> 00:39:54,640
callback on the handle event method

00:39:51,839 --> 00:39:57,920
the handle event method is then get

00:39:54,640 --> 00:40:01,040
invoked when a transaction is performed

00:39:57,920 --> 00:40:02,960
the embedded engine is a weapon of the

00:40:01,040 --> 00:40:06,880
connector that

00:40:02,960 --> 00:40:06,880
manage the connector life cycle

00:40:08,160 --> 00:40:12,480
so this is an example of a cdc listener

00:40:11,839 --> 00:40:14,720
that will

00:40:12,480 --> 00:40:16,240
perform on the dynamic connector and the

00:40:14,720 --> 00:40:18,400
kinematic surface

00:40:16,240 --> 00:40:24,400
and it will notify the handle events

00:40:18,400 --> 00:40:26,880
when the transaction happens so the

00:40:24,400 --> 00:40:29,200
start method is called when the ecm

00:40:26,880 --> 00:40:31,920
engine is initialized and it started

00:40:29,200 --> 00:40:33,760
asynchronously using the executor

00:40:31,920 --> 00:40:36,079
the end method will be called when the

00:40:33,760 --> 00:40:38,720
container is destroyed

00:40:36,079 --> 00:40:40,560
this will stop the dpcm and merging the

00:40:38,720 --> 00:40:42,480
executor

00:40:40,560 --> 00:40:43,839
handle event will be called when the

00:40:42,480 --> 00:40:45,760
transaction is performed on the

00:40:43,839 --> 00:40:47,680
kinematics table

00:40:45,760 --> 00:40:50,000
and handle event method will identify

00:40:47,680 --> 00:40:52,000
what operation takes place

00:40:50,000 --> 00:40:54,400
and call the corresponding telematics

00:40:52,000 --> 00:40:55,440
service to perform the create update and

00:40:54,400 --> 00:40:58,720
delete

00:40:55,440 --> 00:40:58,720
on the elastic search

00:40:59,680 --> 00:41:03,040
and this is an example of the start and

00:41:01,680 --> 00:41:06,880
stop method

00:41:03,040 --> 00:41:09,520
on the cdc listener

00:41:06,880 --> 00:41:12,640
and the kinematic connector will listen

00:41:09,520 --> 00:41:14,800
to the change from the telemetric table

00:41:12,640 --> 00:41:16,960
using the postgres connector class from

00:41:14,800 --> 00:41:18,960
the dbcm

00:41:16,960 --> 00:41:20,240
it will have the offset storage to keep

00:41:18,960 --> 00:41:22,880
track of how much data has been

00:41:20,240 --> 00:41:24,880
processed from the transaction mark

00:41:22,880 --> 00:41:26,800
it can resume from the failure point

00:41:24,880 --> 00:41:29,200
when an error has occurred

00:41:26,800 --> 00:41:31,040
usually this type of error means there's

00:41:29,200 --> 00:41:33,440
a connection error

00:41:31,040 --> 00:41:34,720
the database you know went offline when

00:41:33,440 --> 00:41:38,079
it go online again

00:41:34,720 --> 00:41:41,119
it will be able to resume um

00:41:38,079 --> 00:41:43,280
the file offset back install to

00:41:41,119 --> 00:41:44,319
it is used to store the offset in the

00:41:43,280 --> 00:41:46,240
local file

00:41:44,319 --> 00:41:48,319
so that you can have a cache copy of

00:41:46,240 --> 00:41:50,560
that offset

00:41:48,319 --> 00:41:52,720
the connector we call the offset in the

00:41:50,560 --> 00:41:54,720
file or for the new change

00:41:52,720 --> 00:41:58,880
and then the dbcm engine flush the

00:41:54,720 --> 00:41:58,880
offset based on the flushing interval

00:41:59,359 --> 00:42:02,880
and this is uh the bcm connector

00:42:01,119 --> 00:42:05,920
configuration

00:42:02,880 --> 00:42:07,440
based on the name of the connector the

00:42:05,920 --> 00:42:10,400
flush interface

00:42:07,440 --> 00:42:12,880
database name port number username and

00:42:10,400 --> 00:42:12,880
password

00:42:13,520 --> 00:42:19,280
and the magnetic surface maintains

00:42:17,119 --> 00:42:20,880
the read model method and though the

00:42:19,280 --> 00:42:22,880
update insert and delete on the

00:42:20,880 --> 00:42:24,800
telematic data

00:42:22,880 --> 00:42:26,640
the telematics repository is an

00:42:24,800 --> 00:42:28,000
interface to perform the perform the

00:42:26,640 --> 00:42:31,599
quad operation

00:42:28,000 --> 00:42:32,960
on the telematic database

00:42:31,599 --> 00:42:35,520
all right so this is our telematics

00:42:32,960 --> 00:42:37,200
surface that and as you see

00:42:35,520 --> 00:42:39,200
you know if the operation is deleted

00:42:37,200 --> 00:42:42,560
then you will call the thematic

00:42:39,200 --> 00:42:45,920
repository and delete the transaction

00:42:42,560 --> 00:42:47,440
the telematic based on the kinematic id

00:42:45,920 --> 00:42:50,800
and if it's not delete then you will

00:42:47,440 --> 00:42:50,800
save the dynamic object

00:42:52,400 --> 00:42:55,760
so at the end today we have gone through

00:42:54,560 --> 00:42:59,119
a lot in this

00:42:55,760 --> 00:43:01,040
presentation you learn about open shift

00:42:59,119 --> 00:43:01,440
you learn about edge computing you learn

00:43:01,040 --> 00:43:04,720
about

00:43:01,440 --> 00:43:06,720
telemetric data you learn about cdc

00:43:04,720 --> 00:43:10,800
and you can see that the telemetric data

00:43:06,720 --> 00:43:13,119
integration with edge computing on cdc

00:43:10,800 --> 00:43:13,920
work really well with openshift and it

00:43:13,119 --> 00:43:17,280
enables

00:43:13,920 --> 00:43:20,079
easy highly distributed event different

00:43:17,280 --> 00:43:22,960
transactional dock driven stateful micro

00:43:20,079 --> 00:43:25,359
micro services development

00:43:22,960 --> 00:43:28,000
the edge computing with 5g network offer

00:43:25,359 --> 00:43:31,040
fast and reliable solution

00:43:28,000 --> 00:43:32,800
the event stream created on is based on

00:43:31,040 --> 00:43:34,240
the lock change

00:43:32,800 --> 00:43:36,160
the application could listen to the

00:43:34,240 --> 00:43:36,800
event and perform action based on data

00:43:36,160 --> 00:43:38,880
change

00:43:36,800 --> 00:43:41,599
or did that make it super useful need to

00:43:38,880 --> 00:43:44,720
deal with real-time data

00:43:41,599 --> 00:43:48,480
it also performs consistent data

00:43:44,720 --> 00:43:51,760
manipulation using edge computing

00:43:48,480 --> 00:43:52,560
the data will be processed at the edge

00:43:51,760 --> 00:43:55,200
layer

00:43:52,560 --> 00:43:56,720
before it will go to the open shift

00:43:55,200 --> 00:43:59,920
layer

00:43:56,720 --> 00:44:03,040
you can see the distributed connector

00:43:59,920 --> 00:44:06,800
cluster also provide high availability

00:44:03,040 --> 00:44:09,359
scalability and improve the performance

00:44:06,800 --> 00:44:12,640
of the overall system

00:44:09,359 --> 00:44:15,760
with edge computing and openshift and

00:44:12,640 --> 00:44:19,680
cdc the architecture is

00:44:15,760 --> 00:44:21,200
open source is supported by the open

00:44:19,680 --> 00:44:24,079
source community

00:44:21,200 --> 00:44:26,160
and we support any programming languages

00:44:24,079 --> 00:44:29,520
and development framework

00:44:26,160 --> 00:44:32,800
this is really useful so

00:44:29,520 --> 00:44:36,319
for the next slide thank you again for

00:44:32,800 --> 00:44:37,680
attending this conference we enjoy

00:44:36,319 --> 00:44:39,520
talking with you

00:44:37,680 --> 00:44:41,440
if you have any other further question

00:44:39,520 --> 00:44:44,880
please reach out to

00:44:41,440 --> 00:44:45,599
red hat consulting we would be more than

00:44:44,880 --> 00:44:49,119
happy to

00:44:45,599 --> 00:44:52,720
assist you in any architecture

00:44:49,119 --> 00:44:54,640
related to openshift and edge computing

00:44:52,720 --> 00:45:03,680
thank you again i hope you have a great

00:44:54,640 --> 00:45:03,680

YouTube URL: https://www.youtube.com/watch?v=2yLw5GUOCsM


