Title: Lessons Learned from the Collaboration of Big Data & AIML Technologies for Giant Hogweed Eradication
Publication date: 2020-12-03
Playlist: Open Source Summit Japan & Automotive Linux Summit 2020
Description: 
	Lessons Learned from the Collaboration of Big Data and AI/ML Technologies for Giant Hogweed Eradication - Naoto Umemori & Masaru Dobashi, NTT DATA
Captions: 
	00:00:06,160 --> 00:00:10,000
hello

00:00:07,040 --> 00:00:10,880
welcome to our session today in this

00:00:10,000 --> 00:00:13,679
session

00:00:10,880 --> 00:00:15,759
we will introduce our lessons around and

00:00:13,679 --> 00:00:16,160
insights of the collaboration of big

00:00:15,759 --> 00:00:18,800
data

00:00:16,160 --> 00:00:19,199
and ai machine learning technologies

00:00:18,800 --> 00:00:21,760
through

00:00:19,199 --> 00:00:25,279
an actual project to detect a kind of

00:00:21,760 --> 00:00:27,920
poison weight in nordic 2018.

00:00:25,279 --> 00:00:31,119
we have utilized mainly two kinds of

00:00:27,920 --> 00:00:33,600
open technologies in this project

00:00:31,119 --> 00:00:35,200
the one is distributed processing

00:00:33,600 --> 00:00:38,719
platform technologies

00:00:35,200 --> 00:00:41,280
which are apache hadoop and spark

00:00:38,719 --> 00:00:43,520
another is a deep learning framework

00:00:41,280 --> 00:00:46,640
tensorflow

00:00:43,520 --> 00:00:49,200
would also introduce some design tips

00:00:46,640 --> 00:00:50,079
of combination two heterogeneous

00:00:49,200 --> 00:00:53,199
technologies

00:00:50,079 --> 00:00:57,120
in this session it would be great if you

00:00:53,199 --> 00:00:57,120
get something which is better for you

00:00:57,199 --> 00:01:01,359
my name is naruto memory from ntt data

00:01:00,160 --> 00:01:03,840
japan

00:01:01,359 --> 00:01:05,600
my background skills are mainly id

00:01:03,840 --> 00:01:08,000
infrastructure

00:01:05,600 --> 00:01:09,439
distributed computing based on open

00:01:08,000 --> 00:01:11,600
source technologies

00:01:09,439 --> 00:01:13,200
and machine learning infrastructure and

00:01:11,600 --> 00:01:15,840
so on

00:01:13,200 --> 00:01:19,360
the core speaker is masare dobasi who

00:01:15,840 --> 00:01:22,080
are also belong to ntt data japan

00:01:19,360 --> 00:01:24,479
and he has a bunch of experiences of

00:01:22,080 --> 00:01:25,280
design and implement commercial system

00:01:24,479 --> 00:01:27,600
by using

00:01:25,280 --> 00:01:28,560
open source softwares regarding apache

00:01:27,600 --> 00:01:33,040
hadoop

00:01:28,560 --> 00:01:33,040
spark kafuga and so on

00:01:33,360 --> 00:01:38,079
this dock has two sections the first

00:01:36,640 --> 00:01:41,360
sections will be took

00:01:38,079 --> 00:01:44,000
by myself in the first section

00:01:41,360 --> 00:01:44,880
i introduce our journey how we have

00:01:44,000 --> 00:01:47,360
utilized

00:01:44,880 --> 00:01:48,960
deep learning for a use case of image

00:01:47,360 --> 00:01:50,960
detection

00:01:48,960 --> 00:01:53,200
i would introduce some project

00:01:50,960 --> 00:01:55,280
background motivation

00:01:53,200 --> 00:01:58,320
challenges and architecture and dead

00:01:55,280 --> 00:02:00,479
pipeline will be implemented

00:01:58,320 --> 00:02:02,320
the latest section would be taught by

00:02:00,479 --> 00:02:05,119
call speaker masalu

00:02:02,320 --> 00:02:06,719
he will share to you our lessons learned

00:02:05,119 --> 00:02:10,239
from the consideration of

00:02:06,719 --> 00:02:10,959
architecture design so later section be

00:02:10,239 --> 00:02:14,400
a little bit

00:02:10,959 --> 00:02:19,040
high-level topics however we'll be happy

00:02:14,400 --> 00:02:19,040
if it will be something useful for you

00:02:19,680 --> 00:02:25,280
okay let's move on the first section

00:02:22,720 --> 00:02:26,800
this section is the talk regarding to

00:02:25,280 --> 00:02:29,680
our actual project

00:02:26,800 --> 00:02:29,680
experiences

00:02:30,480 --> 00:02:37,760
now a time comes when i would explain an

00:02:33,920 --> 00:02:40,480
impactful proper noun giant whole wheat

00:02:37,760 --> 00:02:41,519
i strongly believe most of japanese and

00:02:40,480 --> 00:02:45,519
asian have no

00:02:41,519 --> 00:02:47,200
idea of giant whole wheat giant hogweed

00:02:45,519 --> 00:02:50,000
is a highly toxic plant

00:02:47,200 --> 00:02:50,560
originating in the western caucasus of

00:02:50,000 --> 00:02:54,160
region

00:02:50,560 --> 00:02:57,200
of the ulesi it has spread across

00:02:54,160 --> 00:02:59,599
central and western europe and there are

00:02:57,200 --> 00:03:00,959
sightings of giant hogwarts reported

00:02:59,599 --> 00:03:04,560
from north america

00:03:00,959 --> 00:03:07,120
too the sap of giant hollywood is

00:03:04,560 --> 00:03:09,200
phototoxic and causes the fight for

00:03:07,120 --> 00:03:12,800
dermatitis in humans

00:03:09,200 --> 00:03:15,599
resulting in blasters and scars

00:03:12,800 --> 00:03:16,480
land owners in europe are obliged to

00:03:15,599 --> 00:03:19,599
eradicate

00:03:16,480 --> 00:03:22,720
giant hogweed from their land due to its

00:03:19,599 --> 00:03:25,760
toxicity and invasive nature

00:03:22,720 --> 00:03:28,480
however it's a very cumbersome process

00:03:25,760 --> 00:03:31,840
for the land owners to find and remove

00:03:28,480 --> 00:03:34,239
giant hogweed from large land because

00:03:31,840 --> 00:03:37,360
the land owners are doing by

00:03:34,239 --> 00:03:37,360
manual process

00:03:38,560 --> 00:03:42,799
the objective of our project is to

00:03:41,120 --> 00:03:45,680
realize a system to

00:03:42,799 --> 00:03:46,000
automate the detection of giant hogweed

00:03:45,680 --> 00:03:48,720
by

00:03:46,000 --> 00:03:51,280
using strong machine learning

00:03:48,720 --> 00:03:53,840
distributed computing and those kind of

00:03:51,280 --> 00:03:56,480
visualization dashboard

00:03:53,840 --> 00:03:58,159
in particular the system has three

00:03:56,480 --> 00:04:01,360
phases

00:03:58,159 --> 00:04:02,080
the first is to correct aerial images

00:04:01,360 --> 00:04:05,760
taken by

00:04:02,080 --> 00:04:07,599
the drone this process is instead of a

00:04:05,760 --> 00:04:11,519
conversion manual process

00:04:07,599 --> 00:04:14,400
which is the hog wheel seeking by human

00:04:11,519 --> 00:04:14,720
the second is to store and pre-process

00:04:14,400 --> 00:04:18,000
the

00:04:14,720 --> 00:04:18,799
taken images into data lake to train the

00:04:18,000 --> 00:04:21,759
model by

00:04:18,799 --> 00:04:23,280
python script with tensorflow and to

00:04:21,759 --> 00:04:27,840
invert the hotweed by

00:04:23,280 --> 00:04:31,199
spark application which uses tensorflow

00:04:27,840 --> 00:04:34,240
last is to utilize the resort data

00:04:31,199 --> 00:04:36,800
and especially visually plot the game

00:04:34,240 --> 00:04:38,160
called nature coordinate where the giant

00:04:36,800 --> 00:04:41,919
hogway would be

00:04:38,160 --> 00:04:43,040
into the map this is the overview of our

00:04:41,919 --> 00:04:46,800
project and to

00:04:43,040 --> 00:04:46,800
end to end data flow

00:04:47,680 --> 00:04:52,639
we had to tackle to solve some

00:04:50,080 --> 00:04:55,759
challenges in the project

00:04:52,639 --> 00:04:57,360
among them these were three symbolic

00:04:55,759 --> 00:05:00,080
challenges in applying

00:04:57,360 --> 00:05:03,840
deep learning to big data platform to

00:05:00,080 --> 00:05:03,840
process a bunch of images

00:05:04,960 --> 00:05:11,440
the first challenge is the data volumes

00:05:08,639 --> 00:05:13,520
our customers in this project are local

00:05:11,440 --> 00:05:16,400
government in denmark

00:05:13,520 --> 00:05:18,560
each local government manages wide area

00:05:16,400 --> 00:05:20,800
land and they must find the giant

00:05:18,560 --> 00:05:26,240
hogweed in the land

00:05:20,800 --> 00:05:28,800
the long idea of theirs is around 3217

00:05:26,240 --> 00:05:31,680
square kilometers

00:05:28,800 --> 00:05:32,759
and the agricultural rate of denmark is

00:05:31,680 --> 00:05:35,360
about

00:05:32,759 --> 00:05:38,000
62.01 percent

00:05:35,360 --> 00:05:41,840
so the estimated landed here is two

00:05:38,000 --> 00:05:44,960
thousand square kilometers approximately

00:05:41,840 --> 00:05:47,360
it means draw has to fly on

00:05:44,960 --> 00:05:49,360
two thousand square kilometers with

00:05:47,360 --> 00:05:52,240
taking pictures

00:05:49,360 --> 00:05:52,960
then in our estimation the total dead

00:05:52,240 --> 00:05:56,400
volume is

00:05:52,960 --> 00:05:59,360
over 200 terabytes

00:05:56,400 --> 00:06:01,680
on the other hand against most of ai

00:05:59,360 --> 00:06:03,440
machine learning projects should be land

00:06:01,680 --> 00:06:06,479
on single node environment

00:06:03,440 --> 00:06:09,360
as like single laptop for quick data

00:06:06,479 --> 00:06:10,960
analytics will actually trial and error

00:06:09,360 --> 00:06:13,360
to pleasure models

00:06:10,960 --> 00:06:15,199
by samsung chill as like notebook

00:06:13,360 --> 00:06:18,319
feature

00:06:15,199 --> 00:06:20,479
however machine learning toolsets or

00:06:18,319 --> 00:06:24,479
libraries may not be good for

00:06:20,479 --> 00:06:27,039
processing big data how the data science

00:06:24,479 --> 00:06:28,160
scientists should tackle such kind of

00:06:27,039 --> 00:06:31,840
challenges

00:06:28,160 --> 00:06:31,840
this is the first challenge

00:06:32,560 --> 00:06:36,800
the second challenge is to prepare

00:06:34,800 --> 00:06:39,759
supervised data

00:06:36,800 --> 00:06:42,000
as you know lack of supervised data for

00:06:39,759 --> 00:06:44,479
the training is a common story in

00:06:42,000 --> 00:06:47,199
machine learning good world

00:06:44,479 --> 00:06:48,880
it's usually only possible for experts

00:06:47,199 --> 00:06:52,639
to prepare supervised

00:06:48,880 --> 00:06:54,160
data and this is a very time consuming

00:06:52,639 --> 00:06:56,720
process

00:06:54,160 --> 00:06:59,039
so the challenge is how do we make this

00:06:56,720 --> 00:07:02,400
process efficient

00:06:59,039 --> 00:07:04,000
by the way in our case i didn't find

00:07:02,400 --> 00:07:07,759
giant hot wheels from

00:07:04,000 --> 00:07:11,360
an aerial image taken by drone requires

00:07:07,759 --> 00:07:13,919
specialized knowledge by biologists

00:07:11,360 --> 00:07:16,160
these are simple images which are giant

00:07:13,919 --> 00:07:18,800
hogwarts and not

00:07:16,160 --> 00:07:21,759
could you tell which is the hogweed from

00:07:18,800 --> 00:07:25,840
the images of positive class

00:07:21,759 --> 00:07:29,280
yes we can't possibly find it

00:07:25,840 --> 00:07:31,919
unless you are an expert you can't

00:07:29,280 --> 00:07:33,520
find the whole bead even among images

00:07:31,919 --> 00:07:37,440
classified in the positive

00:07:33,520 --> 00:07:37,840
class and it would be impossible to find

00:07:37,440 --> 00:07:41,360
this

00:07:37,840 --> 00:07:41,360
man or the images

00:07:42,319 --> 00:07:49,199
the last challenge is various speciality

00:07:46,080 --> 00:07:50,319
meaning it's to integrate heterogeneous

00:07:49,199 --> 00:07:52,319
technologies and

00:07:50,319 --> 00:07:54,000
cooperate with different background

00:07:52,319 --> 00:07:56,479
persons

00:07:54,000 --> 00:07:57,840
this is well-known data science band

00:07:56,479 --> 00:08:00,960
diagram

00:07:57,840 --> 00:08:03,759
in case of our project the domain

00:08:00,960 --> 00:08:06,000
experts were as the biologist to

00:08:03,759 --> 00:08:07,680
determine the hollywood from aerial

00:08:06,000 --> 00:08:09,759
images

00:08:07,680 --> 00:08:12,080
the person of the mathematics versus

00:08:09,759 --> 00:08:14,240
modern developer who are familiar with

00:08:12,080 --> 00:08:17,599
sensing machine learning flamework

00:08:14,240 --> 00:08:20,720
such as dancer pro and persons of

00:08:17,599 --> 00:08:22,639
computer science were model developer

00:08:20,720 --> 00:08:24,639
and data engineer

00:08:22,639 --> 00:08:26,720
who are especially familiar with

00:08:24,639 --> 00:08:30,800
distributed computing such as

00:08:26,720 --> 00:08:33,839
apes hadoop and spark in our case

00:08:30,800 --> 00:08:36,880
so the challenge was how we should

00:08:33,839 --> 00:08:39,680
choose proper technologies and tool sets

00:08:36,880 --> 00:08:42,800
which are good for each

00:08:39,680 --> 00:08:45,920
this challenge may sound simple in words

00:08:42,800 --> 00:08:48,959
but we found it very tough to work

00:08:45,920 --> 00:08:52,000
on this project because

00:08:48,959 --> 00:08:52,399
if we have different interests we care

00:08:52,000 --> 00:08:56,000
about

00:08:52,399 --> 00:08:59,120
different things sometimes it's not easy

00:08:56,000 --> 00:09:02,399
to even have a well conversation

00:08:59,120 --> 00:09:04,560
honestly speaking communication between

00:09:02,399 --> 00:09:09,839
the modular developers and the model

00:09:04,560 --> 00:09:09,839
operators was not good in our case

00:09:10,720 --> 00:09:15,920
these are three challenges we faced them

00:09:14,320 --> 00:09:19,040
what would you think about our

00:09:15,920 --> 00:09:20,959
challenges would you identify with any

00:09:19,040 --> 00:09:23,440
of these challenges

00:09:20,959 --> 00:09:25,600
if you have any other challenges that

00:09:23,440 --> 00:09:28,080
you can share with us

00:09:25,600 --> 00:09:29,040
i'd be grateful for your comment in the

00:09:28,080 --> 00:09:32,720
quest

00:09:29,040 --> 00:09:35,519
chat now considering these

00:09:32,720 --> 00:09:37,600
challenges i'm going to show you the

00:09:35,519 --> 00:09:40,000
system architecture that we have

00:09:37,600 --> 00:09:42,560
designed and implemented in the next

00:09:40,000 --> 00:09:42,560
slide

00:09:43,040 --> 00:09:48,080
this diagram shows an overview of our

00:09:45,920 --> 00:09:50,000
system architecture

00:09:48,080 --> 00:09:51,519
these are two points in this

00:09:50,000 --> 00:09:54,240
architecture

00:09:51,519 --> 00:09:55,200
first the ai machine learning

00:09:54,240 --> 00:09:57,519
infrastructure

00:09:55,200 --> 00:09:59,839
integrates a mechanism for manual

00:09:57,519 --> 00:10:03,279
operation to prepare supervised

00:09:59,839 --> 00:10:06,160
data the second is that

00:10:03,279 --> 00:10:07,760
also some enterprise products are

00:10:06,160 --> 00:10:10,320
included that

00:10:07,760 --> 00:10:13,279
the system is basically consisted of

00:10:10,320 --> 00:10:16,959
open source software

00:10:13,279 --> 00:10:21,040
now let me explain how to look at

00:10:16,959 --> 00:10:24,560
diagram these are three major flames

00:10:21,040 --> 00:10:25,760
from the left upstream in other words

00:10:24,560 --> 00:10:29,040
data source

00:10:25,760 --> 00:10:32,000
data processing and downstream

00:10:29,040 --> 00:10:34,240
now the part of the data processing in

00:10:32,000 --> 00:10:34,480
the middle of the architecture consists

00:10:34,240 --> 00:10:37,760
of

00:10:34,480 --> 00:10:40,560
four major elements each element

00:10:37,760 --> 00:10:42,800
largely conforms to the life cycle of

00:10:40,560 --> 00:10:46,160
machine learning

00:10:42,800 --> 00:10:50,480
that's it preparation training

00:10:46,160 --> 00:10:51,279
inference and analysis analysis in this

00:10:50,480 --> 00:10:55,760
context

00:10:51,279 --> 00:10:58,800
means analyzing the inference results

00:10:55,760 --> 00:11:02,160
the elemental technologies used in each

00:10:58,800 --> 00:11:05,279
element to look like this

00:11:02,160 --> 00:11:07,920
if we focus on open source software part

00:11:05,279 --> 00:11:08,560
we use tensorflow as a library for

00:11:07,920 --> 00:11:12,399
training

00:11:08,560 --> 00:11:15,600
and influencing data this project had

00:11:12,399 --> 00:11:18,800
gotten started at the 2018

00:11:15,600 --> 00:11:20,800
and the fact that diploma was really

00:11:18,800 --> 00:11:23,839
exciting at that time

00:11:20,800 --> 00:11:24,800
and there are numerous examples of it in

00:11:23,839 --> 00:11:28,399
the world

00:11:24,800 --> 00:11:31,519
encouraged us to use tensorflow

00:11:28,399 --> 00:11:33,760
apache adobe hlfs and apache spark

00:11:31,519 --> 00:11:34,560
are responsible for the datastore and

00:11:33,760 --> 00:11:37,440
processing

00:11:34,560 --> 00:11:39,680
engine that manages large amounts of

00:11:37,440 --> 00:11:42,240
data

00:11:39,680 --> 00:11:43,600
and for the inference part we call

00:11:42,240 --> 00:11:46,480
tensorflow from

00:11:43,600 --> 00:11:49,839
our spark application to perform simple

00:11:46,480 --> 00:11:52,480
distributed inference processing

00:11:49,839 --> 00:11:53,360
we have considered we considered using

00:11:52,480 --> 00:11:56,399
distributed

00:11:53,360 --> 00:11:58,959
tensorflow however finally had

00:11:56,399 --> 00:12:01,440
decided to use effect spark to get

00:11:58,959 --> 00:12:03,839
scalability easily because

00:12:01,440 --> 00:12:05,680
we are more familiar with spark rather

00:12:03,839 --> 00:12:10,800
than distributed tensorflow

00:12:05,680 --> 00:12:13,760
at the time for the data analysis part

00:12:10,800 --> 00:12:14,639
we are using openstreetmap and i'll

00:12:13,760 --> 00:12:19,839
explain it

00:12:14,639 --> 00:12:19,839
later why we use it and how we use it

00:12:20,959 --> 00:12:25,040
this is the architecture we've designed

00:12:23,040 --> 00:12:28,160
and implemented

00:12:25,040 --> 00:12:28,880
we've adopted amazon s3 as data lake to

00:12:28,160 --> 00:12:32,240
accumulate

00:12:28,880 --> 00:12:34,560
massive data use apache hadoop and spark

00:12:32,240 --> 00:12:37,279
to process the data

00:12:34,560 --> 00:12:38,000
regarding machine learning part use

00:12:37,279 --> 00:12:40,800
tensorflow

00:12:38,000 --> 00:12:41,839
for training and inference and

00:12:40,800 --> 00:12:44,800
especially

00:12:41,839 --> 00:12:46,720
we have also used apache spark framework

00:12:44,800 --> 00:12:50,000
for inference processing to get

00:12:46,720 --> 00:12:51,120
benefit scalability easily this

00:12:50,000 --> 00:12:54,399
architecture has

00:12:51,120 --> 00:12:56,639
four data pipelines which are supervised

00:12:54,399 --> 00:12:59,920
data preparation pipeline

00:12:56,639 --> 00:13:03,040
training pipeline inference pipeline and

00:12:59,920 --> 00:13:05,200
data analysis pipeline

00:13:03,040 --> 00:13:06,160
from the next slide i'll show you the

00:13:05,200 --> 00:13:11,360
data as

00:13:06,160 --> 00:13:11,360
detail of each data pipeline one by one

00:13:12,399 --> 00:13:16,160
the first pipeline is the data

00:13:14,399 --> 00:13:19,440
preparation

00:13:16,160 --> 00:13:22,800
this pipeline is used to prepare super

00:13:19,440 --> 00:13:26,800
data labeled by biologists

00:13:22,800 --> 00:13:30,560
as initial status there are a bunch of

00:13:26,800 --> 00:13:34,480
raw images stored in amazon s3

00:13:30,560 --> 00:13:38,079
and the first step an application loads

00:13:34,480 --> 00:13:41,279
raw images from amazon s3 splits

00:13:38,079 --> 00:13:45,199
lower images to 2x2 size and stores

00:13:41,279 --> 00:13:48,880
spread images back into the amazon s3

00:13:45,199 --> 00:13:51,120
and second step an application pushes

00:13:48,880 --> 00:13:51,839
the file parts of the spread images

00:13:51,120 --> 00:13:56,560
stored in

00:13:51,839 --> 00:14:00,079
amazon s3 touring sap hana's data mart

00:13:56,560 --> 00:14:02,720
at south step the biologist

00:14:00,079 --> 00:14:05,120
uses manual labeling application which

00:14:02,720 --> 00:14:06,000
is running on sap hana platform to

00:14:05,120 --> 00:14:10,720
prepare

00:14:06,000 --> 00:14:15,199
available data as final step

00:14:10,720 --> 00:14:17,440
the label data are stored into amazon s3

00:14:15,199 --> 00:14:19,680
this is the data flow regarding data

00:14:17,440 --> 00:14:22,959
preparation pipeline

00:14:19,680 --> 00:14:26,320
the point of this part data pipeline is

00:14:22,959 --> 00:14:29,440
especially the dot red line part

00:14:26,320 --> 00:14:32,639
i show you the detail of the application

00:14:29,440 --> 00:14:32,639
on the next slide

00:14:33,600 --> 00:14:37,360
this labeling application has a

00:14:35,720 --> 00:14:39,360
user-friendly ui

00:14:37,360 --> 00:14:41,199
the new engineers can operate

00:14:39,360 --> 00:14:43,760
intuitively

00:14:41,199 --> 00:14:45,600
when the biologist judges that this is

00:14:43,760 --> 00:14:48,959
the giant hallway

00:14:45,600 --> 00:14:52,079
the biologist selects a cell on the

00:14:48,959 --> 00:14:54,720
image for instance this

00:14:52,079 --> 00:14:58,160
yellow cell means the picture which is

00:14:54,720 --> 00:14:59,199
the giant hogwarts in the holder the

00:14:58,160 --> 00:15:02,079
part is

00:14:59,199 --> 00:15:04,959
so this application does not require us

00:15:02,079 --> 00:15:07,120
any id specialized knowledge

00:15:04,959 --> 00:15:08,320
another point is to introduce

00:15:07,120 --> 00:15:12,399
specialized knowledge

00:15:08,320 --> 00:15:15,040
or biobiologists into the system easily

00:15:12,399 --> 00:15:15,920
by providing such a data preparation

00:15:15,040 --> 00:15:19,519
mechanism in

00:15:15,920 --> 00:15:22,959
advance we are able to prepare enough

00:15:19,519 --> 00:15:25,519
data for analysis efficiently

00:15:22,959 --> 00:15:26,000
why experts may often prepare super

00:15:25,519 --> 00:15:29,600
parts

00:15:26,000 --> 00:15:32,639
data in outside of the system but

00:15:29,600 --> 00:15:36,480
it should be ideal for model developers

00:15:32,639 --> 00:15:37,279
model operators and domain experts also

00:15:36,480 --> 00:15:41,279
to use

00:15:37,279 --> 00:15:44,959
one integrated system this ui tool

00:15:41,279 --> 00:15:47,920
is not just ui tool for data preparation

00:15:44,959 --> 00:15:50,160
this tool is a law of gateway for domain

00:15:47,920 --> 00:15:54,800
expert to integrate their specialized

00:15:50,160 --> 00:15:57,920
knowledge to the platform

00:15:54,800 --> 00:16:00,959
so next is about training pipeline

00:15:57,920 --> 00:16:01,440
a training application reads that label

00:16:00,959 --> 00:16:04,880
data

00:16:01,440 --> 00:16:08,560
and splits images from amazon s3

00:16:04,880 --> 00:16:10,959
called tensorflow library to do training

00:16:08,560 --> 00:16:12,160
and the stores the training results

00:16:10,959 --> 00:16:17,600
which are border and

00:16:12,160 --> 00:16:17,600
parameter in to hdfs

00:16:18,800 --> 00:16:24,480
inference pipeline we have integrated

00:16:21,759 --> 00:16:28,000
tensorflow and apache spark to realize

00:16:24,480 --> 00:16:31,120
distributed inference processing easily

00:16:28,000 --> 00:16:33,680
in our approach the spark executors

00:16:31,120 --> 00:16:35,040
natively called tensorflow library to

00:16:33,680 --> 00:16:38,160
infer the wizard

00:16:35,040 --> 00:16:41,199
this is the giant hog video or not

00:16:38,160 --> 00:16:42,560
then the inference results stored into

00:16:41,199 --> 00:16:46,880
the sap hana

00:16:42,560 --> 00:16:46,880
data mart for data analytics use

00:16:48,160 --> 00:16:51,519
let me give you a little more

00:16:49,759 --> 00:16:54,720
information about the processing

00:16:51,519 --> 00:16:57,680
overview of the inference application

00:16:54,720 --> 00:16:59,680
a diagram representation of overview of

00:16:57,680 --> 00:17:01,920
the inference processing method is

00:16:59,680 --> 00:17:04,000
showing like this value

00:17:01,920 --> 00:17:05,199
the inference application calls the

00:17:04,000 --> 00:17:07,679
spark driver

00:17:05,199 --> 00:17:09,760
and each spark executor calls the

00:17:07,679 --> 00:17:12,160
tensorflow library to perform the

00:17:09,760 --> 00:17:14,400
inference process

00:17:12,160 --> 00:17:15,360
any synchronization process doesn't

00:17:14,400 --> 00:17:18,559
happen

00:17:15,360 --> 00:17:20,400
and the task is distributed in a very

00:17:18,559 --> 00:17:23,839
simple manner

00:17:20,400 --> 00:17:24,880
distributed if inference may seem a bit

00:17:23,839 --> 00:17:28,640
complicated

00:17:24,880 --> 00:17:30,559
but spark absorbs the conversion parts

00:17:28,640 --> 00:17:32,720
of the distributed process

00:17:30,559 --> 00:17:36,160
and give us a way to implement

00:17:32,720 --> 00:17:36,160
application simplify

00:17:37,919 --> 00:17:43,840
next data analysis pipeline

00:17:41,600 --> 00:17:46,080
the inference results have the

00:17:43,840 --> 00:17:50,080
confidence and the geo coordinate

00:17:46,080 --> 00:17:53,360
where the giant hogwarts would be like

00:17:50,080 --> 00:17:55,440
this one we brought this information on

00:17:53,360 --> 00:17:58,799
the open straight map

00:17:55,440 --> 00:18:02,720
the red icon or almost certainly means

00:17:58,799 --> 00:18:04,640
that it's a giant hog wheat specifically

00:18:02,720 --> 00:18:06,240
it's an influenced dessert with a

00:18:04,640 --> 00:18:10,000
confidence barrier of

00:18:06,240 --> 00:18:10,720
90 percent or more the orange icon

00:18:10,000 --> 00:18:14,080
probably

00:18:10,720 --> 00:18:15,919
means it's a giant hogweed specifically

00:18:14,080 --> 00:18:18,559
it's an influenced result with a

00:18:15,919 --> 00:18:19,760
confidence body between 80 percent or

00:18:18,559 --> 00:18:23,280
more

00:18:19,760 --> 00:18:26,400
or and less than 90 percent

00:18:23,280 --> 00:18:28,320
and finally the blur icon means that

00:18:26,400 --> 00:18:31,280
maybe it's a giant hogue

00:18:28,320 --> 00:18:33,840
the confidence score is less than 80

00:18:31,280 --> 00:18:33,840
percent

00:18:34,960 --> 00:18:39,600
so i've explained system architecture

00:18:37,360 --> 00:18:41,679
and the data pipelines in the first

00:18:39,600 --> 00:18:44,799
section of this talk

00:18:41,679 --> 00:18:46,960
what do you think you may able to see

00:18:44,799 --> 00:18:49,280
that the technologies used by each

00:18:46,960 --> 00:18:52,480
players were different

00:18:49,280 --> 00:18:54,960
again the biology says the domain expert

00:18:52,480 --> 00:18:56,880
had leveraged the platform to prepare

00:18:54,960 --> 00:19:00,240
supervised data with a

00:18:56,880 --> 00:19:03,840
web based ui which doesn't

00:19:00,240 --> 00:19:07,200
require ideal skills

00:19:03,840 --> 00:19:09,440
the model developers had used tensorflow

00:19:07,200 --> 00:19:12,080
to develop the model

00:19:09,440 --> 00:19:14,559
the motor operator had built a

00:19:12,080 --> 00:19:17,520
distributed processing infrastructure

00:19:14,559 --> 00:19:19,200
using apache hadoop in the spark to get

00:19:17,520 --> 00:19:21,280
system scalability

00:19:19,200 --> 00:19:24,000
and then had performed inference

00:19:21,280 --> 00:19:27,360
processing using the models prepared by

00:19:24,000 --> 00:19:29,520
the model developer by combining

00:19:27,360 --> 00:19:30,720
the inference result with location

00:19:29,520 --> 00:19:34,080
information

00:19:30,720 --> 00:19:36,640
and visualizing them we were able to

00:19:34,080 --> 00:19:37,600
achieve a use case for visualizing the

00:19:36,640 --> 00:19:41,280
habitant of

00:19:37,600 --> 00:19:44,400
exotic plant species in danger

00:19:41,280 --> 00:19:48,000
this is an overview of the project

00:19:44,400 --> 00:19:50,480
so what did we learn from this project

00:19:48,000 --> 00:19:52,000
in the next section we would like to

00:19:50,480 --> 00:19:54,480
share our findings

00:19:52,000 --> 00:19:54,480
with you

00:19:58,160 --> 00:20:03,760
hello everyone i'm master dobashi of

00:20:01,360 --> 00:20:07,120
nadt data

00:20:03,760 --> 00:20:08,000
the later part i will talk about lesson

00:20:07,120 --> 00:20:10,000
learned

00:20:08,000 --> 00:20:13,760
from consideration of architecture

00:20:10,000 --> 00:20:13,760
design in this project

00:20:16,159 --> 00:20:23,520
so this is very famous picture

00:20:20,640 --> 00:20:25,679
and a common view in machine learning

00:20:23,520 --> 00:20:29,039
systems

00:20:25,679 --> 00:20:32,400
as you know machine learning is just

00:20:29,039 --> 00:20:35,600
one piece of machine learning systems

00:20:32,400 --> 00:20:37,200
so how to wrap the machine learning

00:20:35,600 --> 00:20:40,240
algorithms

00:20:37,200 --> 00:20:41,679
and how to establish them as a stable

00:20:40,240 --> 00:20:44,640
system

00:20:41,679 --> 00:20:44,640
as essential

00:20:46,240 --> 00:20:52,559
so basically machine learning

00:20:49,440 --> 00:20:55,760
is not specific to big data

00:20:52,559 --> 00:21:00,320
but we often use massive data

00:20:55,760 --> 00:21:00,320
to obtain more appropriate models

00:21:01,760 --> 00:21:06,000
consider if machine learning system

00:21:04,159 --> 00:21:08,880
meets big data

00:21:06,000 --> 00:21:11,679
to what would machine learning systems

00:21:08,880 --> 00:21:11,679
be changed

00:21:15,120 --> 00:21:21,360
so this is consideration

00:21:18,400 --> 00:21:24,320
about what happens if we scale machine

00:21:21,360 --> 00:21:24,320
learning systems

00:21:28,559 --> 00:21:35,200
the left side is the original words

00:21:32,480 --> 00:21:38,080
which is font which were found in the

00:21:35,200 --> 00:21:38,080
original paper

00:21:38,720 --> 00:21:42,960
on the other hand the right side is our

00:21:41,600 --> 00:21:45,760
consideration

00:21:42,960 --> 00:21:46,080
about what machine learning systems will

00:21:45,760 --> 00:21:48,720
be

00:21:46,080 --> 00:21:48,720
changed to

00:21:49,440 --> 00:21:56,000
in most cases we need consider consider

00:21:52,960 --> 00:21:58,640
we need to consider about usability and

00:21:56,000 --> 00:21:59,440
scalability of the file machine learning

00:21:58,640 --> 00:22:04,720
system

00:21:59,440 --> 00:22:04,720
carefully when we scale out systems

00:22:10,400 --> 00:22:16,400
so in this presentation our focus is on

00:22:14,400 --> 00:22:19,120
i will focus on machine learning

00:22:16,400 --> 00:22:22,559
application with scalability

00:22:19,120 --> 00:22:25,120
the first topic is workflow

00:22:22,559 --> 00:22:27,840
and the second topic is the system

00:22:25,120 --> 00:22:27,840
architecture

00:22:30,640 --> 00:22:36,080
okay so first i'll talk about

00:22:38,840 --> 00:22:41,840
workflow

00:22:42,000 --> 00:22:46,720
this is an architecture image of a

00:22:44,960 --> 00:22:49,600
machine learning application

00:22:46,720 --> 00:22:54,640
with a scalability on our architecture

00:22:49,600 --> 00:22:57,520
on our systems

00:22:54,640 --> 00:22:59,120
training process chance should be an ad

00:22:57,520 --> 00:23:02,799
hoc processes

00:22:59,120 --> 00:23:07,520
or interactive processes for analytics

00:23:02,799 --> 00:23:10,880
it will prior usability better than

00:23:07,520 --> 00:23:14,080
system performance on the other hand

00:23:10,880 --> 00:23:17,120
inference process often handles

00:23:14,080 --> 00:23:19,760
high workloads and may be imposed a

00:23:17,120 --> 00:23:22,799
severe service level

00:23:19,760 --> 00:23:26,000
so it needs stability sustainability

00:23:22,799 --> 00:23:26,000
and scalability

00:23:29,840 --> 00:23:37,520
and also there are two phases

00:23:33,520 --> 00:23:41,200
in machine learning life cycles

00:23:37,520 --> 00:23:44,159
first is model development phase

00:23:41,200 --> 00:23:46,080
and the second is the model operating

00:23:44,159 --> 00:23:49,440
phase

00:23:46,080 --> 00:23:51,679
so first i will talk about

00:23:49,440 --> 00:23:54,159
two aspects of machine learning life

00:23:51,679 --> 00:23:54,159
cycles

00:23:58,320 --> 00:24:06,720
this is the abstract of two

00:24:01,520 --> 00:24:11,600
phases in machine learning life cycle

00:24:06,720 --> 00:24:14,720
model development versus model operating

00:24:11,600 --> 00:24:17,360
the executors and stakeholders may

00:24:14,720 --> 00:24:18,000
change a model development phase and

00:24:17,360 --> 00:24:20,880
also

00:24:18,000 --> 00:24:20,880
operation phase

00:24:23,039 --> 00:24:26,559
in some cases they may be different at

00:24:25,520 --> 00:24:30,240
the company level

00:24:26,559 --> 00:24:33,360
for example you may

00:24:30,240 --> 00:24:35,520
you may order to

00:24:33,360 --> 00:24:36,640
specify specialized machine learning

00:24:35,520 --> 00:24:39,200
developer the

00:24:36,640 --> 00:24:41,679
vendors to develop machine learning

00:24:39,200 --> 00:24:41,679
models

00:24:45,520 --> 00:24:53,600
the left side has a model development

00:24:49,440 --> 00:24:56,240
phase in this phase business kpi

00:24:53,600 --> 00:24:57,840
and defining defining business kpi is

00:24:56,240 --> 00:25:00,960
important

00:24:57,840 --> 00:25:05,840
and we often try often execute trial

00:25:00,960 --> 00:25:08,159
and error or prototyping of applications

00:25:05,840 --> 00:25:10,880
and mathematics and the business logics

00:25:08,159 --> 00:25:15,840
is important

00:25:10,880 --> 00:25:15,840
on the other hand in operation phase

00:25:15,919 --> 00:25:22,159
the stable development of model

00:25:18,960 --> 00:25:25,679
of models to systems is important

00:25:22,159 --> 00:25:28,159
and also we often evaluate or monitor

00:25:25,679 --> 00:25:30,480
the inference results and kpi

00:25:28,159 --> 00:25:32,960
achievement

00:25:30,480 --> 00:25:34,080
in this phase business logics and

00:25:32,960 --> 00:25:43,840
computer science

00:25:34,080 --> 00:25:43,840
is important

00:25:44,799 --> 00:25:48,640
there are tools with high affinity there

00:25:47,679 --> 00:25:53,039
are tools

00:25:48,640 --> 00:25:53,440
for each phases tools with high affinity

00:25:53,039 --> 00:25:55,679
for

00:25:53,440 --> 00:25:58,240
scaled machine learning systems vary

00:25:55,679 --> 00:26:01,200
from face to face

00:25:58,240 --> 00:26:02,720
this is just an example and the

00:26:01,200 --> 00:26:05,679
impression of tools

00:26:02,720 --> 00:26:06,480
depends on each individuals in each

00:26:05,679 --> 00:26:09,600
individual

00:26:06,480 --> 00:26:14,480
or organization's cultures so

00:26:09,600 --> 00:26:14,480
how we choose a tool set for each

00:26:17,360 --> 00:26:25,120
okay this is a consideration about

00:26:21,120 --> 00:26:28,960
design patterns of software choices

00:26:25,120 --> 00:26:30,640
or three patterns of the workflow

00:26:28,960 --> 00:26:32,000
regarding combinational model

00:26:30,640 --> 00:26:35,600
development phase

00:26:32,000 --> 00:26:35,600
and the model operation phase

00:26:37,440 --> 00:26:41,279
as you can see in this figure we can

00:26:39,600 --> 00:26:43,600
define three patterns

00:26:41,279 --> 00:26:46,000
and i will briefly introduce each

00:26:43,600 --> 00:26:46,000
pattern

00:26:48,400 --> 00:26:56,799
okay this is the pattern number one

00:26:53,600 --> 00:26:59,760
in this pattern we

00:26:56,799 --> 00:27:01,279
selected a tool set familiar to model

00:26:59,760 --> 00:27:04,159
developers

00:27:01,279 --> 00:27:07,600
for both module development and

00:27:04,159 --> 00:27:10,559
operation phase

00:27:07,600 --> 00:27:11,440
the benefits of this model of this

00:27:10,559 --> 00:27:13,840
pattern

00:27:11,440 --> 00:27:16,159
is that model developers have a high

00:27:13,840 --> 00:27:19,679
degree of freedom in development

00:27:16,159 --> 00:27:23,200
of models this enabled us

00:27:19,679 --> 00:27:26,880
quick and small starts to development

00:27:23,200 --> 00:27:26,880
to develop applications

00:27:27,039 --> 00:27:31,360
and also interactivity accelerates

00:27:30,000 --> 00:27:36,159
application development

00:27:31,360 --> 00:27:39,039
and debug on the other hand

00:27:36,159 --> 00:27:40,240
this apparatus weakness is that it's

00:27:39,039 --> 00:27:42,799
often difficult

00:27:40,240 --> 00:27:46,320
to achieve quality to meet system

00:27:42,799 --> 00:27:48,880
infrastructure requirements

00:27:46,320 --> 00:27:49,520
and also experimental calls tends to be

00:27:48,880 --> 00:27:53,840
remained

00:27:49,520 --> 00:27:56,880
in applications it's hard to construct

00:27:53,840 --> 00:28:03,840
development teams who are well trained

00:27:56,880 --> 00:28:03,840
about enterprise architecting

00:28:04,960 --> 00:28:08,480
this is an example of open source

00:28:07,279 --> 00:28:11,440
software

00:28:08,480 --> 00:28:14,000
suitable for this pattern meta flow by

00:28:11,440 --> 00:28:14,000
netflix

00:28:15,279 --> 00:28:18,640
metaflow is a human-friendly python

00:28:17,600 --> 00:28:21,520
framework

00:28:18,640 --> 00:28:22,159
that helps scientists and engineers to

00:28:21,520 --> 00:28:24,559
build

00:28:22,159 --> 00:28:26,880
and manage real-life data science

00:28:24,559 --> 00:28:30,320
projects

00:28:26,880 --> 00:28:31,760
metaflow provides a unified api to in to

00:28:30,320 --> 00:28:34,080
the infrastructure stack

00:28:31,760 --> 00:28:35,039
that is required to execute data science

00:28:34,080 --> 00:28:39,200
projects

00:28:35,039 --> 00:28:39,200
from per time to production

00:28:42,960 --> 00:28:49,279
as you can see this library

00:28:46,320 --> 00:28:50,559
provides intuitively understandable api

00:28:49,279 --> 00:28:53,919
for python users

00:28:50,559 --> 00:28:55,679
including data scientists

00:28:53,919 --> 00:28:57,120
based on data scientist friendly

00:28:55,679 --> 00:29:00,159
framework

00:28:57,120 --> 00:29:00,960
we can specify steps and environmental

00:29:00,159 --> 00:29:03,600
parameters

00:29:00,960 --> 00:29:05,679
using the declaration style programming

00:29:03,600 --> 00:29:09,360
paradigms

00:29:05,679 --> 00:29:10,640
so if your team holds infra engineers

00:29:09,360 --> 00:29:13,360
who are specialized

00:29:10,640 --> 00:29:14,159
for developing python based enterprise

00:29:13,360 --> 00:29:18,640
systems

00:29:14,159 --> 00:29:18,640
you can level its metaphors for benefits

00:29:22,640 --> 00:29:27,840
then pattern this is a pattern number

00:29:25,679 --> 00:29:27,840
two

00:29:28,000 --> 00:29:32,159
in this pattern we use developer

00:29:31,200 --> 00:29:35,200
friendly

00:29:32,159 --> 00:29:38,399
tools in development phase

00:29:35,200 --> 00:29:41,840
and also we used operational friendly

00:29:38,399 --> 00:29:41,840
tools in operational phase

00:29:42,320 --> 00:29:49,760
this pattern's benefit is that

00:29:46,399 --> 00:29:52,880
easy to use technology for model

00:29:49,760 --> 00:29:56,240
developers and operators

00:29:52,880 --> 00:29:57,360
and also we can achieve clear

00:29:56,240 --> 00:30:00,799
responsibility

00:29:57,360 --> 00:30:04,240
to each other direct there is a chance

00:30:00,799 --> 00:30:08,080
to reverb experimental cause during

00:30:04,240 --> 00:30:10,640
refactoring of applications

00:30:08,080 --> 00:30:11,600
also battery of the scalability as

00:30:10,640 --> 00:30:16,159
included

00:30:11,600 --> 00:30:19,600
in the operation phase but

00:30:16,159 --> 00:30:24,240
the this pattern's weakness is

00:30:19,600 --> 00:30:27,279
the conversion cost for example

00:30:24,240 --> 00:30:30,320
difficulty of refactoring limitations

00:30:27,279 --> 00:30:32,799
of exporting and importing of models and

00:30:30,320 --> 00:30:32,799
so on

00:30:34,399 --> 00:30:37,520
and also the degree of freedom is a

00:30:36,960 --> 00:30:42,159
little

00:30:37,520 --> 00:30:43,120
low it's somewhat difficult to manage

00:30:42,159 --> 00:30:45,520
the models and

00:30:43,120 --> 00:30:47,840
pass the learning results to the

00:30:45,520 --> 00:30:50,880
inference pipeline

00:30:47,840 --> 00:30:51,840
this is because the systems may be

00:30:50,880 --> 00:30:54,399
separated

00:30:51,840 --> 00:30:54,399
each other

00:30:59,760 --> 00:31:05,679
and if this is an example of use cases

00:31:03,919 --> 00:31:08,720
we were implemented machine

00:31:05,679 --> 00:31:09,840
ml ops on spark we were using the

00:31:08,720 --> 00:31:12,240
machine learning

00:31:09,840 --> 00:31:14,399
and deep learning technologies for

00:31:12,240 --> 00:31:17,360
various services

00:31:14,399 --> 00:31:18,080
they used spark as a platform to

00:31:17,360 --> 00:31:20,960
construct

00:31:18,080 --> 00:31:22,399
the pipeline for example preprocessing

00:31:20,960 --> 00:31:26,880
etl

00:31:22,399 --> 00:31:26,880
training predicting and so on

00:31:27,840 --> 00:31:34,640
why did they choose park they said

00:31:31,279 --> 00:31:37,519
spark are suitable for etl and

00:31:34,640 --> 00:31:39,200
we consider that spark will support

00:31:37,519 --> 00:31:40,960
traditional machine learning mode

00:31:39,200 --> 00:31:44,399
algorithms

00:31:40,960 --> 00:31:46,000
they also used horrible to train machine

00:31:44,399 --> 00:31:50,480
learning models

00:31:46,000 --> 00:31:53,679
and and search process can be changed

00:31:50,480 --> 00:31:54,640
with other processes they use better

00:31:53,679 --> 00:31:58,080
store

00:31:54,640 --> 00:32:00,240
which converts data sets from

00:31:58,080 --> 00:32:03,200
to the format readable for the machine

00:32:00,240 --> 00:32:03,200
learning libraries

00:32:05,919 --> 00:32:13,039
so this is the pattern number three

00:32:09,760 --> 00:32:15,600
this pattern in this pattern we selected

00:32:13,039 --> 00:32:16,799
are two sets familiar to model dev model

00:32:15,600 --> 00:32:20,240
operator

00:32:16,799 --> 00:32:22,399
for both model development and operation

00:32:20,240 --> 00:32:26,559
phase

00:32:22,399 --> 00:32:30,960
this pattern's benefit is

00:32:26,559 --> 00:32:33,600
easy to operate systems and scale out

00:32:30,960 --> 00:32:34,960
this is because battery of scalability

00:32:33,600 --> 00:32:38,000
should be included

00:32:34,960 --> 00:32:40,320
in the infrastructure

00:32:38,000 --> 00:32:41,760
on the other hand the weakness of this

00:32:40,320 --> 00:32:44,080
pattern is

00:32:41,760 --> 00:32:46,399
that model developers seem to be a

00:32:44,080 --> 00:32:49,679
little painful

00:32:46,399 --> 00:32:51,360
because api and framework may not be

00:32:49,679 --> 00:32:54,640
well known for them

00:32:51,360 --> 00:32:55,519
and it's somewhat sometimes hard to use

00:32:54,640 --> 00:32:59,039
state of art

00:32:55,519 --> 00:32:59,039
proposed approaches

00:33:00,000 --> 00:33:06,720
this may be because this may be because

00:33:03,200 --> 00:33:08,320
research projects tend to use a certain

00:33:06,720 --> 00:33:11,840
framework or library

00:33:08,320 --> 00:33:11,840
to prototype algorithms

00:33:14,880 --> 00:33:20,960
this is an example of use cases twitter

00:33:18,640 --> 00:33:22,159
leverages scholar for feature

00:33:20,960 --> 00:33:24,720
engineering

00:33:22,159 --> 00:33:25,760
twitter has a features has a feature

00:33:24,720 --> 00:33:29,440
store

00:33:25,760 --> 00:33:31,279
and they use scouting a cascading

00:33:29,440 --> 00:33:35,039
framework

00:33:31,279 --> 00:33:37,840
to consume the offline feature data

00:33:35,039 --> 00:33:39,760
they used scholar-based libraries to

00:33:37,840 --> 00:33:42,320
abstract feature catalog

00:33:39,760 --> 00:33:45,360
and also features are stored in apache

00:33:42,320 --> 00:33:45,360
hadoop hdfs

00:33:46,559 --> 00:33:50,799
they chose color as a basic language for

00:33:49,120 --> 00:33:53,679
the feature engineering

00:33:50,799 --> 00:33:54,159
and we consider that scala is used to

00:33:53,679 --> 00:33:56,960
use

00:33:54,159 --> 00:33:58,559
in several software and systems for

00:33:56,960 --> 00:34:01,120
distributed computing

00:33:58,559 --> 00:34:01,840
and well suited to the platform

00:34:01,120 --> 00:34:05,760
engineers

00:34:01,840 --> 00:34:08,320
who has knowledge about gvm

00:34:05,760 --> 00:34:10,240
this approach is powerful when your team

00:34:08,320 --> 00:34:13,040
holds machine learning engineers

00:34:10,240 --> 00:34:20,399
who are well trained about jvm based

00:34:13,040 --> 00:34:22,960
system instruction infrastructures

00:34:20,399 --> 00:34:24,960
this is an example of software open

00:34:22,960 --> 00:34:28,560
source software

00:34:24,960 --> 00:34:31,119
bigdl by intel bigdl

00:34:28,560 --> 00:34:33,359
is a distributed deep learning library

00:34:31,119 --> 00:34:36,639
for apache spark

00:34:33,359 --> 00:34:39,520
we used bigdl to train

00:34:36,639 --> 00:34:40,720
simple neural network model on our spark

00:34:39,520 --> 00:34:43,599
based cluster in

00:34:40,720 --> 00:34:43,599
another project

00:34:46,240 --> 00:34:52,320
as you can see big deal provided

00:34:49,839 --> 00:34:53,919
provides simple api to define neural

00:34:52,320 --> 00:34:57,839
network model

00:34:53,919 --> 00:35:01,119
and it's easy to leverage sparks feature

00:34:57,839 --> 00:35:06,000
scalability stability and operational

00:35:01,119 --> 00:35:06,000
knowledge of the gvm based architecture

00:35:08,800 --> 00:35:13,440
this is a reprint of patterns of

00:35:10,960 --> 00:35:16,640
software choices

00:35:13,440 --> 00:35:20,079
i'll talk about main three

00:35:16,640 --> 00:35:25,440
top main three patterns

00:35:20,079 --> 00:35:25,440
of the application choice two choices

00:35:26,079 --> 00:35:32,240
through this consideration we learned we

00:35:29,280 --> 00:35:33,440
we have some kinds of lesson learned

00:35:32,240 --> 00:35:36,320
it's difficult to

00:35:33,440 --> 00:35:37,520
create an ideal architecture that makes

00:35:36,320 --> 00:35:41,520
it easy

00:35:37,520 --> 00:35:44,320
for each stakeholders to move around

00:35:41,520 --> 00:35:45,680
while keeping in mind the division of

00:35:44,320 --> 00:35:47,920
responsibility

00:35:45,680 --> 00:35:50,560
between model development and model

00:35:47,920 --> 00:35:50,560
operation

00:35:51,200 --> 00:35:57,520
and second since appropriate form

00:35:54,960 --> 00:35:58,240
appropriate form varies from company to

00:35:57,520 --> 00:36:01,119
company

00:35:58,240 --> 00:36:03,119
and organization to organization

00:36:01,119 --> 00:36:03,520
individual architectural studies are

00:36:03,119 --> 00:36:07,119
need

00:36:03,520 --> 00:36:07,119
are necessary for your team

00:36:09,680 --> 00:36:17,280
okay then next i'll talk about

00:36:13,119 --> 00:36:17,280
system architecture of this project

00:36:18,560 --> 00:36:26,079
this is a reprint of architecture and

00:36:21,280 --> 00:36:29,680
data pipeline of this project

00:36:26,079 --> 00:36:33,040
defined four data pipelines

00:36:29,680 --> 00:36:35,839
preparation training inference

00:36:33,040 --> 00:36:35,839
and analysis

00:36:36,160 --> 00:36:40,000
this data pipeline is not so bad but

00:36:38,880 --> 00:36:43,599
this is just a version

00:36:40,000 --> 00:36:46,320
one and we had some pain points in this

00:36:43,599 --> 00:36:46,320
architecture

00:36:47,520 --> 00:36:52,079
this is why we redesigned architecture

00:36:50,480 --> 00:36:55,680
and data pipeline

00:36:52,079 --> 00:36:55,680
and this is a version 2.

00:36:56,400 --> 00:37:02,560
we tried to realize both subscalability

00:36:59,440 --> 00:37:06,079
and flexibility of model representation

00:37:02,560 --> 00:37:09,839
using spark as a framework to deploy

00:37:06,079 --> 00:37:09,839
a kinds of models

00:37:11,680 --> 00:37:15,520
here are some key points i will briefly

00:37:14,400 --> 00:37:18,400
introduce you about

00:37:15,520 --> 00:37:18,400
such key points

00:37:21,359 --> 00:37:24,160
so first

00:37:24,560 --> 00:37:28,640
we config we configured eval evaluation

00:37:27,359 --> 00:37:32,240
process

00:37:28,640 --> 00:37:35,839
for it for each id stack

00:37:32,240 --> 00:37:37,040
and second we monitored the we monitored

00:37:35,839 --> 00:37:40,320
the deterioration

00:37:37,040 --> 00:37:40,320
of the inference results

00:37:41,920 --> 00:37:47,839
and managing and using property model is

00:37:45,359 --> 00:37:47,839
important

00:37:48,160 --> 00:37:53,920
and also traceability of the module

00:37:50,240 --> 00:37:53,920
lifecycle is also important

00:37:55,119 --> 00:38:01,280
and we we often use

00:37:58,480 --> 00:38:02,000
the wide variety of toolsets to deploy

00:38:01,280 --> 00:38:05,680
models

00:38:02,000 --> 00:38:08,720
so it's so that is why it's to be

00:38:05,680 --> 00:38:11,920
uh it will be cured

00:38:08,720 --> 00:38:19,839
and finally scalability should be

00:38:11,920 --> 00:38:19,839
cared in everywhere in the system

00:38:20,560 --> 00:38:28,160
so in this talk we focuses on infra

00:38:25,040 --> 00:38:28,160
inference processes

00:38:30,839 --> 00:38:37,200
so this is internal this is an

00:38:34,320 --> 00:38:39,839
important internals of the inference

00:38:37,200 --> 00:38:39,839
pipeline

00:38:41,119 --> 00:38:47,680
it's not enough just to enter the data

00:38:44,240 --> 00:38:49,680
to be inferred and get the result

00:38:47,680 --> 00:38:51,200
for example the system needs to be

00:38:49,680 --> 00:38:56,079
designed to include

00:38:51,200 --> 00:38:56,079
a flow to evaluate the results

00:38:58,880 --> 00:39:05,920
okay for example in this model

00:39:02,480 --> 00:39:05,920
we use two kinds of data

00:39:06,079 --> 00:39:15,839
images itself for the inference and also

00:39:09,520 --> 00:39:15,839
we used images for sanity checking

00:39:16,320 --> 00:39:20,560
and we distributed inference models

00:39:19,200 --> 00:39:22,880
using spark feature

00:39:20,560 --> 00:39:23,680
and did not depend on the model serving

00:39:22,880 --> 00:39:26,400
systems

00:39:23,680 --> 00:39:26,400
in this model

00:39:27,119 --> 00:39:30,800
this is inference processes and after

00:39:30,240 --> 00:39:33,040
obtaining

00:39:30,800 --> 00:39:34,640
outputs we stored several kinds of

00:39:33,040 --> 00:39:39,760
results in the data lag

00:39:34,640 --> 00:39:43,599
on hdfs and aggregated aggregate them

00:39:39,760 --> 00:39:44,800
and exported data to the visualization

00:39:43,599 --> 00:39:48,480
systems

00:39:44,800 --> 00:39:50,640
and finally data scientists can

00:39:48,480 --> 00:39:57,200
obtain information to detect

00:39:50,640 --> 00:40:00,640
deterioration of models

00:39:57,200 --> 00:40:04,640
so i will talk about

00:40:00,640 --> 00:40:04,640
more detail about inference process

00:40:06,480 --> 00:40:10,560
the abstraction of functions used in

00:40:09,680 --> 00:40:13,599
applications

00:40:10,560 --> 00:40:13,599
is very important

00:40:13,920 --> 00:40:18,400
we called inferencing functions defined

00:40:16,640 --> 00:40:21,680
in the model driver

00:40:18,400 --> 00:40:23,920
which is defined in the separated class

00:40:21,680 --> 00:40:26,640
of the spark application and the

00:40:23,920 --> 00:40:26,640
training model

00:40:26,880 --> 00:40:30,640
the signature of the inferencing

00:40:28,880 --> 00:40:32,880
function is generalized

00:40:30,640 --> 00:40:38,480
to be independent of any machine

00:40:32,880 --> 00:40:43,119
learning methods or algorithms

00:40:38,480 --> 00:40:43,119
this is an example of spark application

00:40:43,920 --> 00:40:47,200
in this case this frame holds metadata

00:40:46,720 --> 00:40:52,000
as

00:40:47,200 --> 00:40:55,200
as well as images to filter or to select

00:40:52,000 --> 00:40:55,200
the required data

00:40:55,359 --> 00:40:59,040
and as you can see in this figure we

00:40:58,400 --> 00:41:01,680
defined

00:40:59,040 --> 00:41:03,200
pre-processing in-frame and

00:41:01,680 --> 00:41:05,839
post-processing methods

00:41:03,200 --> 00:41:06,960
externally from the application

00:41:05,839 --> 00:41:11,200
externally from

00:41:06,960 --> 00:41:11,200
the driver pro program of spark

00:41:12,319 --> 00:41:18,640
this is a brief summary of sequence of

00:41:15,520 --> 00:41:22,000
spark application

00:41:18,640 --> 00:41:22,960
from infra app or spark application

00:41:22,000 --> 00:41:25,680
driver

00:41:22,960 --> 00:41:27,359
spark driver application we called the

00:41:25,680 --> 00:41:30,480
infrared method

00:41:27,359 --> 00:41:34,160
and info image method which is

00:41:30,480 --> 00:41:37,839
defined in moto driver

00:41:34,160 --> 00:41:42,400
the motor driver has defined

00:41:37,839 --> 00:41:42,400
an external of spark driver application

00:41:44,640 --> 00:41:52,560
this abstract this this is abstraction

00:41:48,079 --> 00:41:52,560
to represent various inference systems

00:41:52,640 --> 00:41:58,079
this is this this makes

00:41:56,079 --> 00:42:00,240
some kinds of benefits for us for

00:41:58,079 --> 00:42:02,640
example you can use

00:42:00,240 --> 00:42:04,319
motor serving systems instead of

00:42:02,640 --> 00:42:07,839
directly load

00:42:04,319 --> 00:42:11,839
models in your application we can easily

00:42:07,839 --> 00:42:11,839
extend such kind of features

00:42:14,240 --> 00:42:22,319
then next i will talk about

00:42:18,400 --> 00:42:25,359
the internal of inference pipeline

00:42:22,319 --> 00:42:25,359
and output data

00:42:27,760 --> 00:42:33,200
the detecting and storing deterioration

00:42:31,040 --> 00:42:35,440
of confidence is important for machine

00:42:33,200 --> 00:42:39,359
learning life cycle

00:42:35,440 --> 00:42:41,520
so we stored results with low confidence

00:42:39,359 --> 00:42:43,359
on each device separately from other

00:42:41,520 --> 00:42:45,280
results

00:42:43,359 --> 00:42:47,280
this made the data eccentric

00:42:45,280 --> 00:42:50,079
architecture simple

00:42:47,280 --> 00:42:50,560
and allowed us to extract candidates for

00:42:50,079 --> 00:42:55,200
data

00:42:50,560 --> 00:42:57,920
to be investigated and retrained

00:42:55,200 --> 00:42:58,560
as a result we were able to see at a

00:42:57,920 --> 00:43:00,880
glance

00:42:58,560 --> 00:43:03,520
the results that we need to pay

00:43:00,880 --> 00:43:03,520
attention

00:43:05,920 --> 00:43:11,599
okay i summarize a full this

00:43:09,440 --> 00:43:14,480
presentation

00:43:11,599 --> 00:43:15,359
the former part of this presentation we

00:43:14,480 --> 00:43:18,319
talked about

00:43:15,359 --> 00:43:21,440
our journey to apply deep learning for

00:43:18,319 --> 00:43:24,079
giant hogweed eradication

00:43:21,440 --> 00:43:26,079
in the later part i talked about

00:43:24,079 --> 00:43:29,200
consideration of architecture design

00:43:26,079 --> 00:43:32,800
to adapt deep learning technologies

00:43:29,200 --> 00:43:36,800
for big data infrastructures for example

00:43:32,800 --> 00:43:36,800
workflow and system architecture

00:43:37,440 --> 00:43:41,839

YouTube URL: https://www.youtube.com/watch?v=MRg7pgE6AVk


