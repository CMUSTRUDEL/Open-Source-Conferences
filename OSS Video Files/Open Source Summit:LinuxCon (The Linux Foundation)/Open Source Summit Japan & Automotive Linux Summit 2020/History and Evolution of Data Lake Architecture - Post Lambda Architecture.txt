Title: History and Evolution of Data Lake Architecture - Post Lambda Architecture
Publication date: 2020-12-03
Playlist: Open Source Summit Japan & Automotive Linux Summit 2020
Description: 
	History and Evolution of Data Lake Architecture - Post Lambda Architecture - Takuya Fukuhisa & Masaru Dobashi, NTT DATA
Captions: 
	00:00:06,720 --> 00:00:12,639
hello everyone

00:00:08,080 --> 00:00:17,840
thank you for watching today i'm taking

00:00:12,639 --> 00:00:20,880
an i.t architect at ntt data corporation

00:00:17,840 --> 00:00:24,160
in this session masala dobasi

00:00:20,880 --> 00:00:29,199
and i would like to tell you history

00:00:24,160 --> 00:00:29,199
and evolution of data lake architecture

00:00:29,359 --> 00:00:37,840
quas is a free self-introduction

00:00:34,000 --> 00:00:40,800
i am an id architect with expertise

00:00:37,840 --> 00:00:43,040
in distributed computing and mission

00:00:40,800 --> 00:00:46,879
critical systems

00:00:43,040 --> 00:00:50,160
energy data corporation develops system

00:00:46,879 --> 00:00:53,760
for multiple sectors such as public

00:00:50,160 --> 00:00:58,000
financial and corporate

00:00:53,760 --> 00:01:01,280
masaru dobasi is an anti-specialist

00:00:58,000 --> 00:01:04,559
is expertise in oss

00:01:01,280 --> 00:01:08,479
about distributed computer and

00:01:04,559 --> 00:01:11,360
machine learning infra etc

00:01:08,479 --> 00:01:12,080
he wrote technical books about apache

00:01:11,360 --> 00:01:15,520
kafka

00:01:12,080 --> 00:01:18,799
and apache spark in japan

00:01:15,520 --> 00:01:22,720
we are widely involved in project

00:01:18,799 --> 00:01:27,840
and rnd that use oss

00:01:22,720 --> 00:01:27,840
focus on data utilization in structure

00:01:29,119 --> 00:01:35,759
in this session we talk about two topic

00:01:32,720 --> 00:01:37,759
in the first half masala will talk about

00:01:35,759 --> 00:01:41,680
the history of oss that

00:01:37,759 --> 00:01:44,560
handles big data in second half

00:01:41,680 --> 00:01:47,520
i will talk about related recent

00:01:44,560 --> 00:01:47,520
technologies

00:01:49,360 --> 00:01:55,119
before we talk about its topic

00:01:52,399 --> 00:01:56,240
let me explain the target that we will

00:01:55,119 --> 00:01:59,439
mainly talk about

00:01:56,240 --> 00:02:02,719
in this session the

00:01:59,439 --> 00:02:07,759
data utilization platform is

00:02:02,719 --> 00:02:11,680
composed various technic technologies

00:02:07,759 --> 00:02:16,080
in this session we will focus on storing

00:02:11,680 --> 00:02:16,080
and processing big data

00:02:17,200 --> 00:02:20,879
as a technology stack we focus on the

00:02:20,239 --> 00:02:24,080
layer

00:02:20,879 --> 00:02:28,160
between the access interface

00:02:24,080 --> 00:02:31,519
and distributed stretch

00:02:28,160 --> 00:02:33,200
we will refer to this area as stretch

00:02:31,519 --> 00:02:37,120
and stretch layer

00:02:33,200 --> 00:02:39,920
for this session now i will change

00:02:37,120 --> 00:02:42,800
the speaker to masala to talk about the

00:02:39,920 --> 00:02:42,800
first topic

00:02:45,040 --> 00:02:50,959
hello everyone i'm master tobacci of

00:02:48,640 --> 00:02:53,440
nadt data

00:02:50,959 --> 00:02:55,440
i first talked about history of open

00:02:53,440 --> 00:02:59,120
source software for processing

00:02:55,440 --> 00:02:59,120
and utilizing big data

00:03:03,040 --> 00:03:09,360
this is a free summary of history of

00:03:06,080 --> 00:03:12,080
lfx system here

00:03:09,360 --> 00:03:14,080
as you've seen there are many kinds of

00:03:12,080 --> 00:03:18,720
ecosystem

00:03:14,080 --> 00:03:21,360
around the apache hadoop

00:03:18,720 --> 00:03:24,159
we can categorize popular hadoop

00:03:21,360 --> 00:03:24,159
ecosystems

00:03:25,120 --> 00:03:33,840
i will briefly introduce each category

00:03:28,239 --> 00:03:33,840
for you in this session

00:03:34,319 --> 00:03:40,799
so first of all

00:03:37,680 --> 00:03:47,840
i talk about appreciative and

00:03:40,799 --> 00:03:47,840
some kind of x-system category one

00:03:48,319 --> 00:03:53,519
first part is about dynamic hadoop in a

00:03:51,440 --> 00:03:56,400
map reduce based

00:03:53,519 --> 00:03:56,400
query engine

00:03:56,799 --> 00:04:03,200
hideo enabled enterprises to store

00:04:00,400 --> 00:04:07,040
and process huge data with distributed

00:04:03,200 --> 00:04:07,040
computing using commodity hardware

00:04:07,599 --> 00:04:10,879
before hadoop was born

00:04:12,400 --> 00:04:16,239
when hadoop was getting to be polar the

00:04:14,720 --> 00:04:19,759
mainstream

00:04:16,239 --> 00:04:23,840
of ies servers only had a cheap spec

00:04:19,759 --> 00:04:29,360
such as two gigabyte memory two turbine

00:04:23,840 --> 00:04:32,479
hard disk drive a few cores of cpu

00:04:29,360 --> 00:04:35,919
it was difficult to store huge data and

00:04:32,479 --> 00:04:36,479
processes in realistic time for example

00:04:35,919 --> 00:04:39,919
using

00:04:36,479 --> 00:04:39,919
the single lwms

00:04:40,960 --> 00:04:48,960
after hadoop was born we can use

00:04:44,720 --> 00:04:52,479
a lot of machines transparently as if

00:04:48,960 --> 00:04:54,840
it was huge one computing resource

00:04:52,479 --> 00:04:56,160
and had made distributed computing

00:04:54,840 --> 00:04:59,120
familiar

00:04:56,160 --> 00:05:00,720
to enterprises we could obtain more

00:04:59,120 --> 00:05:04,720
computing resources

00:05:00,720 --> 00:05:11,840
as we added commodity computing machines

00:05:04,720 --> 00:05:11,840
like these figures

00:05:12,000 --> 00:05:25,840
so so we met or found

00:05:16,080 --> 00:05:25,840
impressive use cases like these

00:05:26,000 --> 00:05:31,840
okay the left side example is a

00:05:28,400 --> 00:05:34,000
financial area use case

00:05:31,840 --> 00:05:34,960
in a certain financial use case he did

00:05:34,000 --> 00:05:37,600
reduce

00:05:34,960 --> 00:05:38,080
the produce processing time from 8 hours

00:05:37,600 --> 00:05:41,440
to the

00:05:38,080 --> 00:05:45,360
5 and half hours

00:05:41,440 --> 00:05:48,160
the right side is another use case

00:05:45,360 --> 00:05:50,160
in this use case we executed the name

00:05:48,160 --> 00:05:53,520
identification process

00:05:50,160 --> 00:05:56,560
the processing time got to be 1 got to

00:05:53,520 --> 00:05:56,560
be 100

00:05:59,280 --> 00:06:04,960
hadoop became a de facto standard

00:06:02,639 --> 00:06:07,840
of the open source software to level its

00:06:04,960 --> 00:06:07,840
masking data

00:06:10,800 --> 00:06:17,120
however there was troublesome to write

00:06:13,520 --> 00:06:20,160
my previous applications directly

00:06:17,120 --> 00:06:24,400
so lots of technologies to increase

00:06:20,160 --> 00:06:24,400
productivity of application worldwide

00:06:25,039 --> 00:06:30,479
they abstracted map-reduce-based

00:06:27,199 --> 00:06:30,479
distributed computing

00:06:33,440 --> 00:06:37,759
in this approach hadoop map reduce

00:06:35,919 --> 00:06:40,240
transparently provide us

00:06:37,759 --> 00:06:41,680
provided us with difficult features to

00:06:40,240 --> 00:06:44,720
realize

00:06:41,680 --> 00:06:48,080
such as scalability for trains and

00:06:44,720 --> 00:06:48,960
commodity hardware utilization this is

00:06:48,080 --> 00:06:51,440
why

00:06:48,960 --> 00:06:54,319
various software that depends on the

00:06:51,440 --> 00:06:54,319
hadoop was born

00:07:00,479 --> 00:07:06,240
for example using apache hive

00:07:03,840 --> 00:07:08,000
users can develop applications using

00:07:06,240 --> 00:07:11,680
sql-like language

00:07:08,000 --> 00:07:14,000
in other words iql hive was first

00:07:11,680 --> 00:07:17,840
developed in facebook and joined a patch

00:07:14,000 --> 00:07:17,840
project after

00:07:20,240 --> 00:07:24,479
using a patch pick users can develop

00:07:23,199 --> 00:07:27,840
applications using

00:07:24,479 --> 00:07:31,840
peak latin a domain specific language

00:07:27,840 --> 00:07:31,840
your first developed pig

00:07:33,599 --> 00:07:40,880
and also apache my house

00:07:37,039 --> 00:07:46,479
was born as a machine learning library

00:07:40,880 --> 00:07:49,919
and giraffe is for the graph processing

00:07:46,479 --> 00:07:53,360
and cascading is for framework or

00:07:49,919 --> 00:07:59,840
library to build the

00:07:53,360 --> 00:07:59,840
to build the custom dsl

00:08:02,400 --> 00:08:04,639
but

00:08:05,440 --> 00:08:12,639
hadoop also has a weak point for example

00:08:08,800 --> 00:08:16,240
a div hdfs combined with map reduce

00:08:12,639 --> 00:08:18,960
is not a good at handle good at handling

00:08:16,240 --> 00:08:18,960
small files

00:08:19,759 --> 00:08:22,240
of course

00:08:23,360 --> 00:08:27,840
like this figure

00:08:28,560 --> 00:08:33,919
as hadoop is good at handling large

00:08:31,440 --> 00:08:36,560
files sequentially

00:08:33,919 --> 00:08:36,959
however it's difficult to handle lots of

00:08:36,560 --> 00:08:40,880
small

00:08:36,959 --> 00:08:46,000
files or lots of small data

00:08:40,880 --> 00:08:46,000
update records in large data set

00:08:46,320 --> 00:08:55,839
and possibly delete records in large

00:08:48,800 --> 00:08:55,839
data files

00:08:57,839 --> 00:09:02,720
the more you store small files the more

00:09:00,480 --> 00:09:05,120
massive metadata grow

00:09:02,720 --> 00:09:06,959
this results in the high load to the

00:09:05,120 --> 00:09:09,600
master server

00:09:06,959 --> 00:09:12,000
which manage the metadata of the file

00:09:09,600 --> 00:09:12,000
system

00:09:14,240 --> 00:09:16,880
for example

00:09:17,360 --> 00:09:20,160
in left side

00:09:20,800 --> 00:09:24,720
there are many there are many and one

00:09:23,200 --> 00:09:27,839
gigabyte data

00:09:24,720 --> 00:09:28,800
the number of files less than one

00:09:27,839 --> 00:09:32,160
million

00:09:28,800 --> 00:09:35,600
on the other hand the right side

00:09:32,160 --> 00:09:38,720
there are many one kilobyte files

00:09:35,600 --> 00:09:41,600
but the number of files is one trillion

00:09:38,720 --> 00:09:42,800
the total num the total size of data is

00:09:41,600 --> 00:09:45,440
the same

00:09:42,800 --> 00:09:46,160
but the muscle shop constrains more

00:09:45,440 --> 00:09:49,360
memory

00:09:46,160 --> 00:09:51,839
a body metadata in right side

00:09:49,360 --> 00:09:54,240
it might hit the performance of the file

00:09:51,839 --> 00:09:54,240
system

00:09:57,200 --> 00:10:05,839
and secondly he-div is not good at

00:10:01,279 --> 00:10:05,839
data processing with the low latency

00:10:07,680 --> 00:10:13,519
adobe is good at batch processing using

00:10:10,880 --> 00:10:14,560
the large input file and also the output

00:10:13,519 --> 00:10:17,200
file

00:10:14,560 --> 00:10:18,720
on the other hand it's default tool it's

00:10:17,200 --> 00:10:22,480
difficult for hadith

00:10:18,720 --> 00:10:25,600
to handle small input output data

00:10:22,480 --> 00:10:28,000
and respond to the request in the online

00:10:25,600 --> 00:10:28,000
manner

00:10:28,880 --> 00:10:35,279
okay then secondly i'll talk about hbase

00:10:36,480 --> 00:10:40,800
hdbase add a feature to hollow the small

00:10:39,519 --> 00:10:44,399
size of data

00:10:40,800 --> 00:10:44,399
into hadoop x-system

00:10:44,560 --> 00:10:48,640
you can insert a record in a low latency

00:10:47,600 --> 00:10:52,079
for example

00:10:48,640 --> 00:10:56,000
in millisecond order and you can use

00:10:52,079 --> 00:10:59,279
simple api to read data such as get

00:10:56,000 --> 00:11:02,720
and scan you can read only a part of

00:10:59,279 --> 00:11:02,720
data efficiently

00:11:10,320 --> 00:11:14,640
using hbase we can use the api simple

00:11:13,920 --> 00:11:18,079
api

00:11:14,640 --> 00:11:19,839
get put scan delete and so on

00:11:18,079 --> 00:11:21,440
the one of the key strengths is the

00:11:19,839 --> 00:11:24,640
performance throughput

00:11:21,440 --> 00:11:24,640
of a put api

00:11:24,720 --> 00:11:28,640
each base is based on a hadoop so that

00:11:27,279 --> 00:11:30,800
it also levellates

00:11:28,640 --> 00:11:33,600
the feature of a hadoop such as

00:11:30,800 --> 00:11:33,600
scalability

00:11:33,839 --> 00:11:37,440
for example you can expand the space of

00:11:36,800 --> 00:11:40,720
straits

00:11:37,440 --> 00:11:42,160
by adding nodes and also you can improve

00:11:40,720 --> 00:11:46,240
the triplet of read

00:11:42,160 --> 00:11:46,240
and write by adding notes

00:11:46,959 --> 00:11:50,480
a famous use case when hbase first

00:11:49,600 --> 00:11:53,360
appeared

00:11:50,480 --> 00:11:57,839
as a background debbie database of

00:11:53,360 --> 00:11:57,839
facebook messenger

00:11:58,560 --> 00:12:04,880
third one is the distributed stream

00:12:02,240 --> 00:12:04,880
processing

00:12:07,200 --> 00:12:12,399
the patch stone in the apache kafka

00:12:10,240 --> 00:12:15,040
provided scalability to the stream

00:12:12,399 --> 00:12:15,040
processing

00:12:15,600 --> 00:12:20,399
apache storm has been born to process

00:12:18,079 --> 00:12:24,079
the massive stream data

00:12:20,399 --> 00:12:25,600
in near real time the batch kafka is an

00:12:24,079 --> 00:12:28,480
open source software

00:12:25,600 --> 00:12:30,079
for the messaging of massive stream data

00:12:28,480 --> 00:12:34,079
which was originally born in

00:12:30,079 --> 00:12:36,839
linkedin using a patch cover

00:12:34,079 --> 00:12:38,399
we can receive and transfer massive data

00:12:36,839 --> 00:12:41,839
freely

00:12:38,399 --> 00:12:42,160
and also using stone we can receive data

00:12:41,839 --> 00:12:46,320
and

00:12:42,160 --> 00:12:46,320
process them in near real time

00:12:47,440 --> 00:12:51,360
one example of this kind of use case is

00:12:50,399 --> 00:12:54,800
analytics

00:12:51,360 --> 00:12:57,519
about tweets to clarify what content is

00:12:54,800 --> 00:12:57,519
in buggy

00:12:59,680 --> 00:13:03,920
the first topic is rise of error of

00:13:02,480 --> 00:13:07,839
sikkil on hadiv

00:13:03,920 --> 00:13:07,839
and the columnar format

00:13:08,480 --> 00:13:12,079
after the distributed computing became

00:13:10,720 --> 00:13:14,880
popular

00:13:12,079 --> 00:13:17,360
various sequel on hadith solutions for

00:13:14,880 --> 00:13:21,839
the ad-hoc query to improve usability

00:13:17,360 --> 00:13:21,839
for the end users was born developed

00:13:22,320 --> 00:13:26,560
since about 23d kinds of sequel on

00:13:25,839 --> 00:13:30,639
hadith

00:13:26,560 --> 00:13:33,760
for the low latency query were born

00:13:30,639 --> 00:13:34,160
the difference between original hive and

00:13:33,760 --> 00:13:36,880
such

00:13:34,160 --> 00:13:38,000
product is the dependency on adobe

00:13:36,880 --> 00:13:41,360
produce

00:13:38,000 --> 00:13:42,160
secure onhatif in this context does not

00:13:41,360 --> 00:13:44,959
depend on

00:13:42,160 --> 00:13:44,959
developer use

00:13:47,519 --> 00:13:51,600
sql and hadoop technologies sometimes

00:13:49,920 --> 00:13:54,560
perform overwhelmingly

00:13:51,600 --> 00:13:55,040
better than head map use combined with

00:13:54,560 --> 00:13:57,360
hive

00:13:55,040 --> 00:14:00,079
approach in specific work laws which

00:13:57,360 --> 00:14:02,160
they are good at

00:14:00,079 --> 00:14:03,440
this is because some of them will

00:14:02,160 --> 00:14:07,600
leverage memories

00:14:03,440 --> 00:14:07,600
and networks as well as disks

00:14:10,639 --> 00:14:18,320
this is an image of hardware trends

00:14:14,079 --> 00:14:18,320
changed since hadith was born

00:14:20,240 --> 00:14:27,680
when hadoop and

00:14:23,760 --> 00:14:27,680
high was born high were born

00:14:27,920 --> 00:14:34,639
the mainstream machine had only a

00:14:31,600 --> 00:14:38,240
only hard disk drives and

00:14:34,639 --> 00:14:45,839
uh memory the size of memory

00:14:38,240 --> 00:14:45,839
was a few gigabytes per machine

00:14:46,160 --> 00:14:49,680
on the other hand this is a difficult

00:14:48,800 --> 00:14:51,920
how to aspect

00:14:49,680 --> 00:14:55,040
when fresh stone and in polyurethane

00:14:51,920 --> 00:14:55,040
environment were born

00:14:55,920 --> 00:15:01,040
with often used ssd instead of hard disk

00:14:59,440 --> 00:15:04,399
drive

00:15:01,040 --> 00:15:09,519
the size of memory increased

00:15:04,399 --> 00:15:09,519
to tens or hundreds of gigabytes per

00:15:10,839 --> 00:15:13,839
machine

00:15:17,120 --> 00:15:21,440
and also the columnar oriented format

00:15:19,839 --> 00:15:26,320
was getting to be known

00:15:21,440 --> 00:15:28,800
as a technology to assist adhocries this

00:15:26,320 --> 00:15:29,759
warehouse system as well as hadoop

00:15:28,800 --> 00:15:33,040
ecosystem

00:15:29,759 --> 00:15:34,480
uses these kinds of formats compared to

00:15:33,040 --> 00:15:36,639
role-oriented formats

00:15:34,480 --> 00:15:40,320
the column-oriented format achieves the

00:15:36,639 --> 00:15:43,199
following improvements

00:15:40,320 --> 00:15:45,839
first is efficiency to encode and

00:15:43,199 --> 00:15:48,240
compress data

00:15:45,839 --> 00:15:49,440
the second is better optimization for

00:15:48,240 --> 00:15:55,839
read operation

00:15:49,440 --> 00:15:55,839
for analytics queries

00:15:56,639 --> 00:16:00,720
so first i introduce about the

00:15:59,920 --> 00:16:04,000
improvement

00:16:00,720 --> 00:16:04,000
about column printing

00:16:04,240 --> 00:16:08,000
data analysts often only use posture

00:16:07,279 --> 00:16:12,000
kinds of data

00:16:08,000 --> 00:16:14,959
for specific analytics application can

00:16:12,000 --> 00:16:17,920
read only targeted columns using column

00:16:14,959 --> 00:16:17,920
oriented format

00:16:20,880 --> 00:16:27,120
the second size is the example of

00:16:24,079 --> 00:16:29,759
row oriented cases

00:16:27,120 --> 00:16:32,160
applications need to read every column

00:16:29,759 --> 00:16:36,079
not required for analysis

00:16:32,160 --> 00:16:38,480
on the other hand column oriented case

00:16:36,079 --> 00:16:39,199
applications can read only targeted

00:16:38,480 --> 00:16:42,240
columns

00:16:39,199 --> 00:16:42,800
for analytics and skip not required to

00:16:42,240 --> 00:16:47,040
run

00:16:42,800 --> 00:16:51,839
not required ones this omission

00:16:47,040 --> 00:16:51,839
reduces io for analytics

00:16:56,399 --> 00:17:01,920
so next i talk about improvement about

00:16:59,680 --> 00:17:05,360
the predicate push down

00:17:01,920 --> 00:17:06,480
data analysis often only use a partial

00:17:05,360 --> 00:17:10,559
range of data

00:17:06,480 --> 00:17:13,520
for specific analysis application can

00:17:10,559 --> 00:17:16,319
use metadata to narrow range of actual

00:17:13,520 --> 00:17:16,319
read operation

00:17:17,679 --> 00:17:24,640
this is an example in this example

00:17:20,959 --> 00:17:29,760
data are status as sorted by

00:17:24,640 --> 00:17:29,760
date in this case

00:17:29,919 --> 00:17:38,080
if you want if we want the searching

00:17:33,600 --> 00:17:38,080
data created and a certain date

00:17:38,400 --> 00:17:44,240
according to the metadata we can skip

00:17:41,440 --> 00:17:45,039
data 1 and the data 3 the left side data

00:17:44,240 --> 00:17:48,080
or

00:17:45,039 --> 00:17:51,360
right side data and we can

00:17:48,080 --> 00:17:53,679
read the only targeted data the second

00:17:51,360 --> 00:17:53,679
data

00:17:57,200 --> 00:18:01,600
but column oriented format is not a

00:17:59,600 --> 00:18:04,080
silver bullet

00:18:01,600 --> 00:18:04,960
transformation to the column oriented

00:18:04,080 --> 00:18:07,039
format

00:18:04,960 --> 00:18:10,880
and absolute operation needs high

00:18:07,039 --> 00:18:10,880
computation and resource uses

00:18:11,840 --> 00:18:19,200
this is an image of transformation

00:18:15,200 --> 00:18:22,480
first we create

00:18:19,200 --> 00:18:26,160
the role oriented data in memory

00:18:22,480 --> 00:18:26,160
and with a statistical data

00:18:26,960 --> 00:18:32,240
after that we can convert such kind of

00:18:29,919 --> 00:18:34,559
row oriented data to the column oriented

00:18:32,240 --> 00:18:34,559
data

00:18:37,760 --> 00:18:46,240
this conversion needs some computation

00:18:42,160 --> 00:18:46,240
rearrangement of data

00:18:49,360 --> 00:18:53,440
this is why the data which is written

00:18:51,440 --> 00:18:55,039
once and repeatedly read

00:18:53,440 --> 00:18:56,480
is faithful for these kinds of

00:18:55,039 --> 00:18:59,039
technologies

00:18:56,480 --> 00:19:02,799
however i don't recommend to use it for

00:18:59,039 --> 00:19:05,520
intermediate data of computation

00:19:02,799 --> 00:19:06,080
this is because done this is because

00:19:05,520 --> 00:19:08,720
this

00:19:06,080 --> 00:19:11,840
transformation is not cost effective for

00:19:08,720 --> 00:19:11,840
the intermediate

00:19:18,880 --> 00:19:24,480
and it's not easy if we want to fully

00:19:21,760 --> 00:19:28,160
leverage the optimization for example

00:19:24,480 --> 00:19:31,840
the predicate pushdown i mentioned

00:19:28,160 --> 00:19:33,280
as useful only when the data is stored

00:19:31,840 --> 00:19:37,919
sorted

00:19:33,280 --> 00:19:39,919
enough by the targeted search column

00:19:37,919 --> 00:19:43,440
it's essential to understand

00:19:39,919 --> 00:19:47,039
the characters of the data

00:19:43,440 --> 00:19:47,039
and access patterns to them

00:19:51,440 --> 00:19:55,520
this is an example where the data is not

00:19:54,080 --> 00:19:59,679
sorted by the targeted

00:19:55,520 --> 00:20:05,280
search column in this data set

00:19:59,679 --> 00:20:05,280
the data is sorted by the alphabet

00:20:05,440 --> 00:20:13,120
on the other hand if you want search

00:20:08,559 --> 00:20:20,559
the data created in a certain date

00:20:13,120 --> 00:20:24,000
you cannot you cannot use the metadata

00:20:20,559 --> 00:20:26,640
so you should read the fall data set

00:20:24,000 --> 00:20:26,640
in this data

00:20:29,200 --> 00:20:35,840
the next topic is object straight

00:20:32,400 --> 00:20:35,840
as a data lake

00:20:37,440 --> 00:20:41,360
the objects to trace is the one of the

00:20:39,760 --> 00:20:44,000
architecture to store

00:20:41,360 --> 00:20:46,159
and manage data on a granularity of

00:20:44,000 --> 00:20:48,880
object

00:20:46,159 --> 00:20:50,880
it is recently also popular as an

00:20:48,880 --> 00:20:52,960
architecture for datelay

00:20:50,880 --> 00:20:55,200
a persistent data store for the long

00:20:52,960 --> 00:20:55,200
term

00:20:57,760 --> 00:21:04,080
the object to store has a house of

00:21:00,880 --> 00:21:07,200
has features common between hadoop hdfs

00:21:04,080 --> 00:21:10,880
for example scalability and it's also

00:21:07,200 --> 00:21:10,880
able to store petabytes data

00:21:12,080 --> 00:21:16,960
but there are differences between head

00:21:14,320 --> 00:21:19,919
div for example it's often

00:21:16,960 --> 00:21:20,960
good at storing lots of small files on

00:21:19,919 --> 00:21:24,240
the other hand

00:21:20,960 --> 00:21:28,480
there are characters unfold

00:21:24,240 --> 00:21:28,480
unfavorable for the data processing

00:21:29,600 --> 00:21:32,799
the movement of data within the stretch

00:21:31,600 --> 00:21:35,600
is not so

00:21:32,799 --> 00:21:36,400
not lightweight and also consistency

00:21:35,600 --> 00:21:44,240
level

00:21:36,400 --> 00:21:48,640
of some operations is often not high

00:21:44,240 --> 00:21:49,280
as of lately aws s3 since 276 is one of

00:21:48,640 --> 00:21:52,720
the popular

00:21:49,280 --> 00:21:54,960
solution and it has a long history

00:21:52,720 --> 00:21:56,880
deep hdfs and other solutions support

00:21:54,960 --> 00:21:58,559
history protocol

00:21:56,880 --> 00:22:01,280
in the hadith community started to

00:21:58,559 --> 00:22:04,400
develop the object storage feature

00:22:01,280 --> 00:22:07,200
as ecosystem of hadoop

00:22:04,400 --> 00:22:12,720
since hadiv3 apache hadith ozone became

00:22:07,200 --> 00:22:15,919
some feature of hadeep

00:22:12,720 --> 00:22:18,240
then next i'll talk about

00:22:15,919 --> 00:22:21,760
rise in the fall of open source of

00:22:18,240 --> 00:22:21,760
theory about the big data

00:22:24,559 --> 00:22:29,440
so some open source software slowed down

00:22:27,440 --> 00:22:30,640
the maintenance compared to the first

00:22:29,440 --> 00:22:34,880
release error

00:22:30,640 --> 00:22:34,880
and some change the main features

00:22:35,200 --> 00:22:40,480
the patch hive many

00:22:38,400 --> 00:22:42,000
about apache hive many competitors

00:22:40,480 --> 00:22:44,480
competitors appeared

00:22:42,000 --> 00:22:45,679
but the development is still active on

00:22:44,480 --> 00:22:48,799
the other hand

00:22:45,679 --> 00:22:54,240
for example apache peaks

00:22:48,799 --> 00:22:57,840
the final release was in 2017.

00:22:54,240 --> 00:23:00,320
and also about apache mahout

00:22:57,840 --> 00:23:00,880
the main feature is now the distributed

00:23:00,320 --> 00:23:03,760
linear

00:23:00,880 --> 00:23:05,200
algebra framework and a mathematically

00:23:03,760 --> 00:23:09,840
expressive

00:23:05,200 --> 00:23:09,840
scalar dsl

00:23:12,080 --> 00:23:14,559
glove

00:23:15,120 --> 00:23:19,120
deluxe release cycle slowed down

00:23:20,840 --> 00:23:24,960
so the products which depend on a

00:23:23,919 --> 00:23:27,600
particular product

00:23:24,960 --> 00:23:28,720
tends to be affected by the

00:23:27,600 --> 00:23:33,200
deterioration

00:23:28,720 --> 00:23:36,080
of dependent products for example

00:23:33,200 --> 00:23:38,480
the above products seem to be affected

00:23:36,080 --> 00:23:43,279
by the rise of apache spark

00:23:38,480 --> 00:23:43,279
and deterioration of hadoop map reduce

00:23:44,159 --> 00:23:49,840
however sikkim-like language

00:23:47,200 --> 00:23:51,360
got to be the de facto started in the

00:23:49,840 --> 00:23:54,240
data processing engine

00:23:51,360 --> 00:23:57,039
combined with data lake as well as data

00:23:54,240 --> 00:23:57,039
frame api

00:23:57,440 --> 00:24:04,240
on the other hand many users did not use

00:24:01,039 --> 00:24:07,039
extremely characteristic languages

00:24:04,240 --> 00:24:07,039
in the long term

00:24:09,279 --> 00:24:13,600
and also low layer technologies such as

00:24:12,000 --> 00:24:17,120
hadoop hdfs

00:24:13,600 --> 00:24:19,360
and hbase are still actively developed

00:24:17,120 --> 00:24:20,159
the architecture is evolving and the

00:24:19,360 --> 00:24:24,960
developers

00:24:20,159 --> 00:24:24,960
are adding the new ambitious features

00:24:25,679 --> 00:24:30,320
finally i summarize the part one

00:24:30,400 --> 00:24:33,679
this is overview of typical

00:24:32,159 --> 00:24:37,679
architectures

00:24:33,679 --> 00:24:37,679
combined with some kinds of technologies

00:24:41,200 --> 00:24:46,960
i'll talk about features which the

00:24:43,679 --> 00:24:50,320
typical architecture wants to achieve

00:24:46,960 --> 00:24:54,159
first we often tend to achieve

00:24:50,320 --> 00:24:57,760
the batch processing of massive data and

00:24:54,159 --> 00:24:57,760
storing domestic data

00:24:58,320 --> 00:25:04,640
and second the writing reading records

00:25:01,600 --> 00:25:07,919
in high circuit and low latency

00:25:04,640 --> 00:25:10,559
and also we like to accept a lot of

00:25:07,919 --> 00:25:10,559
small data

00:25:10,640 --> 00:25:16,000
and also near real timely process lots

00:25:13,840 --> 00:25:20,880
of small data

00:25:16,000 --> 00:25:26,720
and finally we tend to want to

00:25:20,880 --> 00:25:26,720
adhoc cream for analytical

00:25:26,840 --> 00:25:29,840
process

00:25:31,919 --> 00:25:36,320
we tend to desire the ideal slaves with

00:25:35,039 --> 00:25:40,000
scalability

00:25:36,320 --> 00:25:43,279
acid support load latency heist rapid

00:25:40,000 --> 00:25:44,000
efficiency child access efficient random

00:25:43,279 --> 00:25:47,039
access

00:25:44,000 --> 00:25:48,080
chip and so on however it's not easy to

00:25:47,039 --> 00:25:51,279
realize it

00:25:48,080 --> 00:25:53,520
using one technology

00:25:51,279 --> 00:25:56,480
so we often integrate several kinds of

00:25:53,520 --> 00:26:00,720
products to balance trade-offs

00:25:56,480 --> 00:26:04,400
now this is a example of trade-offs

00:26:00,720 --> 00:26:04,400
we often we often met

00:26:05,919 --> 00:26:12,400
but these approaches often forces

00:26:09,360 --> 00:26:15,279
force users to accept complexity

00:26:12,400 --> 00:26:18,159
and as sometimes too tricky to use

00:26:15,279 --> 00:26:18,159
appropriately

00:26:18,400 --> 00:26:25,039
so there are new demands for the data

00:26:21,760 --> 00:26:27,600
right recently

00:26:25,039 --> 00:26:30,159
we can categorize recent demands for the

00:26:27,600 --> 00:26:30,159
data lake

00:26:30,400 --> 00:26:34,320
we can define two categories of recent

00:26:32,799 --> 00:26:38,159
demands for data lake

00:26:34,320 --> 00:26:41,039
and the data platform

00:26:38,159 --> 00:26:42,159
one is left side the flexibility of

00:26:41,039 --> 00:26:44,480
handling

00:26:42,159 --> 00:26:44,480
data

00:26:45,279 --> 00:26:53,840
and second is right side to provide

00:26:48,320 --> 00:26:53,840
features for the analytics

00:26:54,559 --> 00:27:02,480
so the rest of part

00:26:59,200 --> 00:27:06,159
is presentation by

00:27:02,480 --> 00:27:10,640
takia an introduction

00:27:06,159 --> 00:27:10,640
of related recent technologies

00:27:10,880 --> 00:27:14,640
in the first half we mainly talk about

00:27:13,679 --> 00:27:18,159
the history

00:27:14,640 --> 00:27:20,720
of oss that handle big data

00:27:18,159 --> 00:27:24,880
in the second half i'd like to talk

00:27:20,720 --> 00:27:27,919
about related recent technologies

00:27:24,880 --> 00:27:31,279
firstly i'll talk about requirements

00:27:27,919 --> 00:27:34,159
for the recent trade rail

00:27:31,279 --> 00:27:36,480
secondly i'll explain the use case of

00:27:34,159 --> 00:27:39,840
these requirements

00:27:36,480 --> 00:27:42,559
suddenly i'll talk about the challenges

00:27:39,840 --> 00:27:44,880
of the architecture that realizes

00:27:42,559 --> 00:27:48,159
requirements

00:27:44,880 --> 00:27:48,960
furthermore i'll introduce three oss

00:27:48,159 --> 00:27:52,559
products

00:27:48,960 --> 00:27:55,039
to solve the challenges finally

00:27:52,559 --> 00:27:58,000
i mentioned considerations and

00:27:55,039 --> 00:27:58,000
conclusions

00:27:59,200 --> 00:28:02,240
in this slide i'll talk about the

00:28:01,440 --> 00:28:05,200
traditional

00:28:02,240 --> 00:28:05,919
requirements for straight layer which

00:28:05,200 --> 00:28:09,200
process

00:28:05,919 --> 00:28:12,960
big data there are four

00:28:09,200 --> 00:28:18,320
main requirements scalability

00:28:12,960 --> 00:28:21,440
usability affordability durability

00:28:18,320 --> 00:28:22,880
as for scalability users want to be able

00:28:21,440 --> 00:28:27,200
to add resource

00:28:22,880 --> 00:28:30,159
easily as for usability

00:28:27,200 --> 00:28:32,559
users want to be able to handle big data

00:28:30,159 --> 00:28:35,840
easily

00:28:32,559 --> 00:28:36,240
as affordability users want to be able

00:28:35,840 --> 00:28:38,600
to

00:28:36,240 --> 00:28:42,000
process big data by arranging

00:28:38,600 --> 00:28:45,919
inexpensive resources

00:28:42,000 --> 00:28:49,600
as durability users want to be able to

00:28:45,919 --> 00:28:53,200
continue the service without losing data

00:28:49,600 --> 00:28:56,080
even if one unit breaks down

00:28:53,200 --> 00:28:56,880
we believe that user dealing with big

00:28:56,080 --> 00:29:00,159
data

00:28:56,880 --> 00:29:02,799
will continue to need these demands

00:29:00,159 --> 00:29:02,799
in the future

00:29:03,600 --> 00:29:10,880
now i will mention recent requirements

00:29:07,200 --> 00:29:14,720
of straight rail processing big data

00:29:10,880 --> 00:29:18,399
there are two main requirements ones

00:29:14,720 --> 00:29:21,039
swim processing and analyze various

00:29:18,399 --> 00:29:24,000
analytics

00:29:21,039 --> 00:29:24,960
as for stream processing the strategies

00:29:24,000 --> 00:29:28,399
need to keep

00:29:24,960 --> 00:29:32,080
receiving large amounts of small data

00:29:28,399 --> 00:29:36,399
that are constantly coming

00:29:32,080 --> 00:29:39,760
for example it is iot data

00:29:36,399 --> 00:29:43,679
as various analytics i

00:29:39,760 --> 00:29:48,000
exemplify four requirements

00:29:43,679 --> 00:29:51,120
first use of real-time data

00:29:48,000 --> 00:29:55,760
users may want to analyze

00:29:51,120 --> 00:30:00,000
real-time data that has just arrived

00:29:55,760 --> 00:30:03,200
second consider historical data

00:30:00,000 --> 00:30:07,279
users may want to analyze data with a

00:30:03,200 --> 00:30:10,960
large amount of historical data

00:30:07,279 --> 00:30:15,120
third low latency users

00:30:10,960 --> 00:30:18,720
may want to get a result quickly

00:30:15,120 --> 00:30:21,440
finally trial and error

00:30:18,720 --> 00:30:22,399
users may want to try and wear some

00:30:21,440 --> 00:30:27,279
query to a

00:30:22,399 --> 00:30:30,880
data set for example machine learning

00:30:27,279 --> 00:30:34,840
upcoming slide we consider

00:30:30,880 --> 00:30:37,840
real-time analytics based on these

00:30:34,840 --> 00:30:37,840
requirements

00:30:37,919 --> 00:30:42,320
next let's consider an example of a use

00:30:41,760 --> 00:30:47,039
case

00:30:42,320 --> 00:30:47,039
that requires real-time analytics

00:30:47,760 --> 00:30:53,840
here as a generalized use case

00:30:51,039 --> 00:30:56,080
we will introduce an example of linking

00:30:53,840 --> 00:31:00,159
information to store and users

00:30:56,080 --> 00:31:04,399
in real time let's take step by step

00:31:00,159 --> 00:31:07,760
look at the date flow

00:31:04,399 --> 00:31:08,480
first we accommodate the behavioral

00:31:07,760 --> 00:31:12,000
history

00:31:08,480 --> 00:31:18,880
and operation history by batch

00:31:12,000 --> 00:31:22,159
and stream processing in advance next

00:31:18,880 --> 00:31:22,960
we execute machine learning and create a

00:31:22,159 --> 00:31:27,840
model

00:31:22,960 --> 00:31:30,840
that is easy to handle in processing

00:31:27,840 --> 00:31:33,840
they are the content to be finished in

00:31:30,840 --> 00:31:33,840
advance

00:31:33,919 --> 00:31:38,720
from now on it will be real-time

00:31:36,399 --> 00:31:41,600
analytics

00:31:38,720 --> 00:31:42,399
when a user with a smartphone approaches

00:31:41,600 --> 00:31:46,960
store

00:31:42,399 --> 00:31:49,600
a system cut this information by stream

00:31:46,960 --> 00:31:51,200
then the system analyzes user's

00:31:49,600 --> 00:31:55,440
historical data

00:31:51,200 --> 00:31:59,840
and ranks the customer service policy

00:31:55,440 --> 00:32:02,880
and recommendation to the store

00:31:59,840 --> 00:32:05,360
result users can receive a smooth

00:32:02,880 --> 00:32:05,360
service

00:32:06,159 --> 00:32:13,679
besides the system analyze inquiry from

00:32:10,240 --> 00:32:18,559
users and pushes

00:32:13,679 --> 00:32:18,559
useful information to user in real time

00:32:19,919 --> 00:32:26,000
by repeating this process we will be

00:32:23,600 --> 00:32:26,880
able to continue to provide useful

00:32:26,000 --> 00:32:30,000
information

00:32:26,880 --> 00:32:32,559
based on flash data to start and

00:32:30,000 --> 00:32:32,559
users

00:32:33,120 --> 00:32:39,440
next let's consider the challenges

00:32:36,159 --> 00:32:43,840
of architecture for realizing such

00:32:39,440 --> 00:32:43,840
real-time analytics

00:32:43,919 --> 00:32:52,720
i present two typical architectures

00:32:48,240 --> 00:32:56,159
the first is batch focused architecture

00:32:52,720 --> 00:32:59,760
in batch architecture we use a batch

00:32:56,159 --> 00:33:03,679
etl pipeline to collect and

00:32:59,760 --> 00:33:06,480
process data however

00:33:03,679 --> 00:33:08,640
there is a problem that we can't analyze

00:33:06,480 --> 00:33:11,440
real-time data

00:33:08,640 --> 00:33:12,880
because it take a long time between

00:33:11,440 --> 00:33:16,240
collecting data

00:33:12,880 --> 00:33:16,240
and using data

00:33:16,559 --> 00:33:22,320
the second is stream focused

00:33:19,919 --> 00:33:26,159
architecture

00:33:22,320 --> 00:33:29,440
in stream architecture we use data tab

00:33:26,159 --> 00:33:32,880
to collect data and we use

00:33:29,440 --> 00:33:36,320
stream pipeline visualize data

00:33:32,880 --> 00:33:39,279
notify information however

00:33:36,320 --> 00:33:42,480
there is a problem that we can't handle

00:33:39,279 --> 00:33:45,279
large amount of historical data

00:33:42,480 --> 00:33:45,919
because stream architecture does not

00:33:45,279 --> 00:33:49,039
expect

00:33:45,919 --> 00:33:53,840
the query to as much data as

00:33:49,039 --> 00:33:57,120
batches and it is difficult to execute

00:33:53,840 --> 00:33:58,480
an ad hoc analytic query to stream

00:33:57,120 --> 00:34:02,159
pipelines

00:33:58,480 --> 00:34:07,679
because the query to the stream pipeline

00:34:02,159 --> 00:34:13,280
is predetermined

00:34:07,679 --> 00:34:13,280
now i introduce ramuda architecture

00:34:13,359 --> 00:34:17,919
to meet these challenges ramuda

00:34:16,399 --> 00:34:21,839
architecture integrates

00:34:17,919 --> 00:34:24,960
bachelor stream pipelines

00:34:21,839 --> 00:34:26,919
round architecture consists of three

00:34:24,960 --> 00:34:30,320
layers

00:34:26,919 --> 00:34:33,440
departure analyze large amount of data

00:34:30,320 --> 00:34:36,079
in batch processing

00:34:33,440 --> 00:34:39,440
the service layer provides a bad

00:34:36,079 --> 00:34:39,440
processing result

00:34:39,760 --> 00:34:43,679
the speed layer analyzes what is

00:34:42,399 --> 00:34:48,000
happening in real

00:34:43,679 --> 00:34:51,119
time however why the round architecture

00:34:48,000 --> 00:34:53,760
looks perfect it has the following

00:34:51,119 --> 00:34:53,760
concerns

00:34:54,000 --> 00:34:58,640
first to analyze bad strength processing

00:34:57,599 --> 00:35:01,760
together

00:34:58,640 --> 00:35:05,520
it is necessary to integrate

00:35:01,760 --> 00:35:08,880
the result of two systems

00:35:05,520 --> 00:35:12,480
but it is too difficult to make them

00:35:08,880 --> 00:35:15,599
consistent second

00:35:12,480 --> 00:35:19,040
pipeline complexity

00:35:15,599 --> 00:35:22,400
caused increased system and

00:35:19,040 --> 00:35:22,400
operating costs

00:35:24,400 --> 00:35:31,280
here is a summary of the essence that

00:35:27,440 --> 00:35:34,079
have comes out so far we think about

00:35:31,280 --> 00:35:35,839
their four essence for the strange layer

00:35:34,079 --> 00:35:39,839
to realize real time

00:35:35,839 --> 00:35:43,040
analytics first

00:35:39,839 --> 00:35:48,480
the straight layer must be able to input

00:35:43,040 --> 00:35:48,480
both batch and stream processing

00:35:48,560 --> 00:35:53,920
second the storage layer must be able to

00:35:52,160 --> 00:35:57,680
accommodate

00:35:53,920 --> 00:36:01,599
utilize both real-time data

00:35:57,680 --> 00:36:01,599
and historical data

00:36:01,760 --> 00:36:08,160
third the strategy must be able to

00:36:05,040 --> 00:36:09,839
execute various analytics query for

00:36:08,160 --> 00:36:13,680
example machine learning

00:36:09,839 --> 00:36:17,119
ad-hoc queries and so on

00:36:13,680 --> 00:36:19,359
finally the strategy must be able to

00:36:17,119 --> 00:36:22,800
realize a simple pipeline

00:36:19,359 --> 00:36:22,800
to optimize cost

00:36:22,880 --> 00:36:28,000
next i introduce the recent trade layer

00:36:26,320 --> 00:36:31,920
open source software

00:36:28,000 --> 00:36:31,920
for real-time analytics

00:36:32,079 --> 00:36:38,560
as explained in the first half part

00:36:35,200 --> 00:36:42,000
the world is full of old and new

00:36:38,560 --> 00:36:45,359
products that can process big

00:36:42,000 --> 00:36:49,599
data of course there are

00:36:45,359 --> 00:36:53,119
many products that are not listed here

00:36:49,599 --> 00:36:56,560
even in these situations we need to sex

00:36:53,119 --> 00:36:56,560
the proper products

00:36:57,520 --> 00:37:04,320
this time as a recent stress layer

00:37:00,560 --> 00:37:07,920
oss products for real-time analytics

00:37:04,320 --> 00:37:09,440
i'd like to briefly introduce a patch

00:37:07,920 --> 00:37:14,240
iceberg

00:37:09,440 --> 00:37:14,240
patch hoodie and dirt rake

00:37:16,240 --> 00:37:20,800
first i'd like to introduce apache

00:37:19,040 --> 00:37:24,240
iceberg

00:37:20,800 --> 00:37:27,599
i think the main feature of iceberg

00:37:24,240 --> 00:37:31,520
are transactional capability and

00:37:27,599 --> 00:37:32,480
multiformat support the transactional

00:37:31,520 --> 00:37:36,000
feature

00:37:32,480 --> 00:37:39,599
allows users to analyze a consistent

00:37:36,000 --> 00:37:43,280
state besides

00:37:39,599 --> 00:37:48,400
user can easily reproduce the analysis

00:37:43,280 --> 00:37:52,000
and execute analysis using past data set

00:37:48,400 --> 00:37:55,040
on the other hand multi-format support

00:37:52,000 --> 00:37:56,079
allow users to choose the formats that

00:37:55,040 --> 00:37:59,200
suit

00:37:56,079 --> 00:37:59,200
their requirements

00:38:01,359 --> 00:38:08,880
next i'd like to introduce apache hoodie

00:38:05,599 --> 00:38:12,880
i think the main feature of foodie is

00:38:08,880 --> 00:38:17,359
merge views according to purpose

00:38:12,880 --> 00:38:22,560
users can use read optimized view

00:38:17,359 --> 00:38:27,200
when the user want to reduce bid latency

00:38:22,560 --> 00:38:30,240
users can use real-time view

00:38:27,200 --> 00:38:30,960
when the user wants to analyze the most

00:38:30,240 --> 00:38:34,240
recent

00:38:30,960 --> 00:38:37,839
data user

00:38:34,240 --> 00:38:39,040
can use incremental view when the user

00:38:37,839 --> 00:38:42,320
wants to use

00:38:39,040 --> 00:38:42,320
update difference

00:38:43,680 --> 00:38:50,400
finally i'd like to introduce delta lake

00:38:47,680 --> 00:38:52,320
i think the main feature of deltrek are

00:38:50,400 --> 00:38:56,160
transactional capability

00:38:52,320 --> 00:38:57,200
and rich gamer commands as same as

00:38:56,160 --> 00:39:00,240
iceberg

00:38:57,200 --> 00:39:03,520
transaction function gave users

00:39:00,240 --> 00:39:07,119
consistency and reproductivity of

00:39:03,520 --> 00:39:10,720
analytics in addition

00:39:07,119 --> 00:39:14,880
users can complete gdpr by using

00:39:10,720 --> 00:39:18,640
enough dml commands for example delete

00:39:14,880 --> 00:39:18,640
update and merge

00:39:20,400 --> 00:39:27,200
then i'd like to show you the comparison

00:39:24,160 --> 00:39:29,839
of entire approach of these three

00:39:27,200 --> 00:39:29,839
products

00:39:31,920 --> 00:39:43,119
first i'd like to focus to the concept

00:39:36,160 --> 00:39:46,240
of data store layer and process

00:39:43,119 --> 00:39:47,200
all three products have the same layer

00:39:46,240 --> 00:39:50,720
structure

00:39:47,200 --> 00:39:54,160
and process concepts

00:39:50,720 --> 00:39:57,599
iceberg hoodie deltrek are

00:39:54,160 --> 00:39:58,800
all located in layer between the

00:39:57,599 --> 00:40:02,960
application

00:39:58,800 --> 00:40:05,920
and datastore we tentatively

00:40:02,960 --> 00:40:08,560
call this software straight layer

00:40:05,920 --> 00:40:08,560
software

00:40:08,839 --> 00:40:15,920
generally date store provides

00:40:11,920 --> 00:40:18,960
simple function to application

00:40:15,920 --> 00:40:21,920
stress layer software provides a

00:40:18,960 --> 00:40:23,359
logical data set and table to the

00:40:21,920 --> 00:40:26,480
application

00:40:23,359 --> 00:40:29,839
as usual future

00:40:26,480 --> 00:40:32,880
this feature allows applications

00:40:29,839 --> 00:40:34,319
to transparently read and write to the

00:40:32,880 --> 00:40:37,680
data store using

00:40:34,319 --> 00:40:40,720
convenient functions

00:40:37,680 --> 00:40:41,200
when application requests to read and

00:40:40,720 --> 00:40:45,119
write

00:40:41,200 --> 00:40:48,160
data to straight layer software

00:40:45,119 --> 00:40:51,440
thread layer software raise and write

00:40:48,160 --> 00:40:52,480
not only actual debt but also the

00:40:51,440 --> 00:40:56,079
management

00:40:52,480 --> 00:40:59,520
information in this way

00:40:56,079 --> 00:41:03,680
the application use data store

00:40:59,520 --> 00:41:07,119
with ease in other hands

00:41:03,680 --> 00:41:10,480
trade layer software has no data store

00:41:07,119 --> 00:41:12,640
and no diamond process

00:41:10,480 --> 00:41:13,760
swell layer software delegates

00:41:12,640 --> 00:41:18,079
scalability

00:41:13,760 --> 00:41:21,599
to date store such as hdfs

00:41:18,079 --> 00:41:24,480
objects threads in class

00:41:21,599 --> 00:41:27,760
in this way these three products and

00:41:24,480 --> 00:41:27,760
your scalability

00:41:30,319 --> 00:41:36,960
next i'd like to focus the format

00:41:33,520 --> 00:41:36,960
of each product

00:41:39,880 --> 00:41:46,000
apache and deltrek mainly

00:41:43,119 --> 00:41:46,720
use the kind of format for efficient

00:41:46,000 --> 00:41:49,839
reading

00:41:46,720 --> 00:41:53,920
during analysis

00:41:49,839 --> 00:41:57,839
patch icebergs sports mulch format

00:41:53,920 --> 00:42:01,760
parque washi and arrow

00:41:57,839 --> 00:42:04,960
so that the user can choose a format

00:42:01,760 --> 00:42:05,680
each product keeps read efficiency by

00:42:04,960 --> 00:42:10,079
using

00:42:05,680 --> 00:42:10,079
effective format for analytics

00:42:11,520 --> 00:42:18,400
finally i'd like to forecast to the

00:42:14,880 --> 00:42:21,680
file structure and read write mechanism

00:42:18,400 --> 00:42:21,680
of each product

00:42:23,599 --> 00:42:28,800
the file structure and read write

00:42:26,160 --> 00:42:32,160
mechanism are different between hoodie

00:42:28,800 --> 00:42:34,079
and other products as well dirt rake

00:42:32,160 --> 00:42:36,240
i talk about the difference between

00:42:34,079 --> 00:42:39,680
deltrek and iceberg in the next

00:42:36,240 --> 00:42:44,400
slide first i'd like to

00:42:39,680 --> 00:42:48,640
explain what is margin right

00:42:44,400 --> 00:42:52,160
hoodie holds two main type of files

00:42:48,640 --> 00:42:55,280
these are the files called base file

00:42:52,160 --> 00:42:59,440
and append log

00:42:55,280 --> 00:43:02,960
when writing the writer as a data tools

00:42:59,440 --> 00:43:05,920
role oriented append log

00:43:02,960 --> 00:43:07,359
this file is converted into column

00:43:05,920 --> 00:43:11,200
oriented base file

00:43:07,359 --> 00:43:11,200
at regular intervals

00:43:11,359 --> 00:43:18,640
the radar read only the base file

00:43:15,359 --> 00:43:22,960
when user want to read with low latency

00:43:18,640 --> 00:43:26,160
as a read optimized view

00:43:22,960 --> 00:43:29,119
the radar read both base files

00:43:26,160 --> 00:43:30,640
and append work when users want to raise

00:43:29,119 --> 00:43:34,960
real time data

00:43:30,640 --> 00:43:38,079
as real-time view in this way

00:43:34,960 --> 00:43:41,920
hoodie realize various analytics

00:43:38,079 --> 00:43:45,119
in combination with multiple view

00:43:41,920 --> 00:43:48,720
next i'd like to explain iceberg

00:43:45,119 --> 00:43:51,839
and deathtrack both of

00:43:48,720 --> 00:43:52,560
these creates the management information

00:43:51,839 --> 00:43:55,760
file

00:43:52,560 --> 00:43:59,119
and actual date file at the same time

00:43:55,760 --> 00:43:59,119
when the writer writes

00:43:59,280 --> 00:44:06,800
the version etc is described

00:44:02,640 --> 00:44:06,800
in the management information

00:44:07,040 --> 00:44:11,280
the radar identifies a necessary actual

00:44:09,920 --> 00:44:14,240
date file from

00:44:11,280 --> 00:44:16,240
rank and management information and read

00:44:14,240 --> 00:44:18,880
them

00:44:16,240 --> 00:44:23,359
let's take a closer look at each product

00:44:18,880 --> 00:44:25,440
in next slide

00:44:23,359 --> 00:44:27,119
i shall explain the difference in

00:44:25,440 --> 00:44:30,640
mechanism between

00:44:27,119 --> 00:44:30,640
iceberg and deltric

00:44:30,880 --> 00:44:37,440
iceberg manages that file using

00:44:34,079 --> 00:44:40,720
the file called manifest list

00:44:37,440 --> 00:44:44,640
and manifest the radar traces

00:44:40,720 --> 00:44:48,640
manifest from manifest risk of snapshot

00:44:44,640 --> 00:44:48,640
to identify the required files

00:44:48,800 --> 00:44:54,640
del deltrac manages that files using the

00:44:51,839 --> 00:44:57,839
files called delta logs

00:44:54,640 --> 00:45:01,920
the radar identifies the required data

00:44:57,839 --> 00:45:05,119
file by the delta log

00:45:01,920 --> 00:45:07,920
delt log aggregates all delta logs

00:45:05,119 --> 00:45:10,640
into checkpoint to prevent the growth of

00:45:07,920 --> 00:45:10,640
delta logs

00:45:11,040 --> 00:45:19,839
both products have the common to

00:45:14,319 --> 00:45:19,839
aggregate small data files

00:45:20,800 --> 00:45:24,240
finally i'd like to talk about

00:45:23,200 --> 00:45:27,359
considerations

00:45:24,240 --> 00:45:27,359
and conclusions

00:45:28,000 --> 00:45:33,119
from now on i'd like to carry out

00:45:30,640 --> 00:45:35,119
consideration based on the internal

00:45:33,119 --> 00:45:38,000
approaches

00:45:35,119 --> 00:45:38,640
in the first half we mentioned that

00:45:38,000 --> 00:45:41,680
products

00:45:38,640 --> 00:45:42,319
processing backlash are strongly aware

00:45:41,680 --> 00:45:46,240
of one

00:45:42,319 --> 00:45:49,040
in many trade-offs

00:45:46,240 --> 00:45:49,760
the real sense stress layer software

00:45:49,040 --> 00:45:52,800
products

00:45:49,760 --> 00:45:55,920
take an approach that strike a

00:45:52,800 --> 00:45:56,960
balance between these tradeoffs while

00:45:55,920 --> 00:46:00,319
resolving

00:45:56,960 --> 00:46:04,000
bus there are three

00:46:00,319 --> 00:46:08,079
points the first is

00:46:04,000 --> 00:46:12,800
right quickly the second

00:46:08,079 --> 00:46:12,800
is to analyze efficiently

00:46:12,960 --> 00:46:19,440
the third is to fill the gap between

00:46:16,560 --> 00:46:19,440
those two

00:46:19,760 --> 00:46:26,240
first in order to write quickly

00:46:23,119 --> 00:46:28,400
hoodie first write to a law oriented

00:46:26,240 --> 00:46:32,160
date format

00:46:28,400 --> 00:46:32,960
iceberg and their track write button

00:46:32,160 --> 00:46:37,280
information

00:46:32,960 --> 00:46:37,280
at the same time as the data

00:46:37,680 --> 00:46:44,800
second in order to read efficiently

00:46:41,520 --> 00:46:47,280
each product adopts an efficient

00:46:44,800 --> 00:46:47,280
format

00:46:48,400 --> 00:46:53,200
third in order to fill the gap between

00:46:51,440 --> 00:46:56,560
the traders

00:46:53,200 --> 00:47:00,560
its products comes in a variety of

00:46:56,560 --> 00:47:06,160
formats and views

00:47:00,560 --> 00:47:06,160
and also has background file aggregation

00:47:08,640 --> 00:47:15,920
now i'd like to talk a little more about

00:47:12,480 --> 00:47:15,920
the future forecast

00:47:16,480 --> 00:47:23,280
we have realized that utilization

00:47:19,680 --> 00:47:26,640
pipeline by combining specialized

00:47:23,280 --> 00:47:30,559
software recent

00:47:26,640 --> 00:47:33,599
oss products about big data processing

00:47:30,559 --> 00:47:38,240
evolved interaction of balanced

00:47:33,599 --> 00:47:40,559
trade-offs that existing in the past

00:47:38,240 --> 00:47:43,760
they have created a new body such as

00:47:40,559 --> 00:47:47,839
real-time analytics

00:47:43,760 --> 00:47:52,400
as this evolution continues we expect

00:47:47,839 --> 00:47:55,599
that jew's interface became simpler

00:47:52,400 --> 00:47:59,680
while the internal mechanism became

00:47:55,599 --> 00:48:03,200
more complex after that

00:47:59,680 --> 00:48:06,800
we think that software for the

00:48:03,200 --> 00:48:09,760
above hardware will rebound to solve the

00:48:06,800 --> 00:48:09,760
complexity

00:48:13,280 --> 00:48:18,480
finally i'll give a summary of this

00:48:16,400 --> 00:48:22,400
session

00:48:18,480 --> 00:48:25,520
oss and architecture are able

00:48:22,400 --> 00:48:28,720
to meet the need of the times

00:48:25,520 --> 00:48:33,839
and the range of data utilization is

00:48:28,720 --> 00:48:36,240
expanding recent big data processing oss

00:48:33,839 --> 00:48:39,760
evolved to balance the trials

00:48:36,240 --> 00:48:42,800
that existed in the past using various

00:48:39,760 --> 00:48:46,240
approaches to meet various requirements

00:48:42,800 --> 00:48:48,800
such as real-time analytics

00:48:46,240 --> 00:48:49,440
therefore the internal structure of the

00:48:48,800 --> 00:48:52,640
products

00:48:49,440 --> 00:48:55,200
has become complicated and

00:48:52,640 --> 00:48:56,640
it has become difficult to select the

00:48:55,200 --> 00:49:00,480
optimum architecture

00:48:56,640 --> 00:49:03,440
and optimum product as architect

00:49:00,480 --> 00:49:03,680
we should continue to develop our sense

00:49:03,440 --> 00:49:06,160
and

00:49:03,680 --> 00:49:09,280
power to realize the architects that

00:49:06,160 --> 00:49:11,920
meet new requirements

00:49:09,280 --> 00:49:13,520
if we have any problems in this

00:49:11,920 --> 00:49:17,839
technical field

00:49:13,520 --> 00:49:17,839
would you discuss them

00:49:18,800 --> 00:49:30,319

YouTube URL: https://www.youtube.com/watch?v=0EpHAysQBoE


