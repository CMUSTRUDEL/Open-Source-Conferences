Title: Deep Learning in OpenCV - Wu Zhiwen, Intel
Publication date: 2018-10-25
Playlist: Open Source Summit Europe & ELC + OpenIoT Summit Europe 2018
Description: 
	Deep Learning in OpenCV - Wu Zhiwen, Intel

Deep Learning is the most popular and the fastest growing area in Computer Vision nowadays. OpenCV is the famous Computer Vision library. Since OpenCV 3.1 there is a deep learning module (called OpenCV DNN) introduced in the library that implements forward pass (inferencing) with deep networks, which are pre-trained using some popular deep learning frameworks, such as Caffe. tensorflow and torch. Now it support most popular networks.

This presentation will introduce OpenCV deep learning architecture, It will introduce how to setup and run a deep learning network with OpenCV DNN. It will also talk about important techniques for deep network acceleration, including convolution performance auto-tuning, layer fusion and FP16 (half-float) support. We are key contributors for this work. All these optimization are important to help user achieve best performance.

About Zhiwen Wu
Zhiwen works at Open Source Technology Center at Intel Software group. He has been working on various Linux project for over ten years, including Moblin, Meego and OpenCV. He has rich experience on Open Source project development, especially the Linux media project. He is also a contributor of OpenCV deep learning module.
Captions: 
	00:00:00,060 --> 00:00:07,259
my name is Alex whoo I worked for Intel

00:00:03,480 --> 00:00:09,469
I mostly do the work about media

00:00:07,259 --> 00:00:13,200
processing and deep learning

00:00:09,469 --> 00:00:17,420
acceleration for India hard well I and

00:00:13,200 --> 00:00:20,250
my colleague implemented OpenCL

00:00:17,420 --> 00:00:23,010
acceleration for the opencv DN module

00:00:20,250 --> 00:00:30,539
today and I'm going to talk about the

00:00:23,010 --> 00:00:34,620
deep learning in open CV firstly we are

00:00:30,539 --> 00:00:37,370
going to cover something about the

00:00:34,620 --> 00:00:40,680
opencv project and the deep learning and

00:00:37,370 --> 00:00:44,640
then we are going to talk about the

00:00:40,680 --> 00:00:49,230
details of the opencv DN module as well

00:00:44,640 --> 00:00:51,840
as it's open CR acceleration I will also

00:00:49,230 --> 00:00:55,530
introduced my current walk or walk and

00:00:51,840 --> 00:01:02,570
back end at last I was shown a sample

00:00:55,530 --> 00:01:05,580
using the air module so what is open CV

00:01:02,570 --> 00:01:10,549
open CV stands for open source computer

00:01:05,580 --> 00:01:15,450
vision library is used it's used for

00:01:10,549 --> 00:01:19,110
image processing analysis and image

00:01:15,450 --> 00:01:22,680
manipulation the library has more than

00:01:19,110 --> 00:01:26,189
2000 and the 500 optimized algorithms

00:01:22,680 --> 00:01:29,640
used for computer vision and machine

00:01:26,189 --> 00:01:35,720
learning you can use the library in C

00:01:29,640 --> 00:01:39,659
C++ Python and various operating system

00:01:35,720 --> 00:01:43,890
the library has more than 20,000 folks

00:01:39,659 --> 00:01:46,640
in github many many would agree that the

00:01:43,890 --> 00:01:52,049
open sea obesity is the most well known

00:01:46,640 --> 00:01:57,170
computer vision library nowadays seven

00:01:52,049 --> 00:02:00,149
weeks ago open CV 4.0 was released and

00:01:57,170 --> 00:02:04,040
the code version is planned to release

00:02:00,149 --> 00:02:04,040
in the end of this month

00:02:06,320 --> 00:02:14,720
OpenCV 4.0 has many new features it's

00:02:10,009 --> 00:02:17,410
switched to sleep last class 11 and has

00:02:14,720 --> 00:02:21,340
no longer binary compatible

00:02:17,410 --> 00:02:25,430
compatibility to previous version and

00:02:21,340 --> 00:02:29,690
better performance on CPU with avx2

00:02:25,430 --> 00:02:36,860
inch in six compact footprint and bigger

00:02:29,690 --> 00:02:38,990
vision of the air module for those who

00:02:36,860 --> 00:02:40,910
are not familiar with deep learning I

00:02:38,990 --> 00:02:45,310
would like to talk a little bit about

00:02:40,910 --> 00:02:49,990
the key concepts of deep neural networks

00:02:45,310 --> 00:02:49,990
before going deep dive into our topic

00:02:50,830 --> 00:02:59,000
the primitive unit of a neural network

00:02:54,320 --> 00:03:02,560
is known or neuron of reception this

00:02:59,000 --> 00:03:09,860
reverse represented the same thing

00:03:02,560 --> 00:03:13,970
another is just a place to to compute

00:03:09,860 --> 00:03:18,860
the is just a place well the computer

00:03:13,970 --> 00:03:23,090
happens a node contains ampuls with a

00:03:18,860 --> 00:03:26,570
set of weights this input weights

00:03:23,090 --> 00:03:31,360
products sound and the Sun is pass

00:03:26,570 --> 00:03:31,360
through a so-called activation function

00:03:32,860 --> 00:03:39,380
you get the final result of an old

00:03:36,980 --> 00:03:48,140
computation after the activation

00:03:39,380 --> 00:03:51,340
function nos make up of layers each

00:03:48,140 --> 00:03:56,480
layers output is simultaneously the

00:03:51,340 --> 00:04:00,880
subsequent layers input the first layer

00:03:56,480 --> 00:04:04,910
of the neural network is input layer and

00:04:00,880 --> 00:04:10,760
the last layer is the output layer layer

00:04:04,910 --> 00:04:13,370
between them are the hidden layer deep

00:04:10,760 --> 00:04:15,639
neural networks are distinguished from

00:04:13,370 --> 00:04:18,919
the more commonplace a single layer

00:04:15,639 --> 00:04:20,209
single hidden layer neural networks by

00:04:18,919 --> 00:04:22,819
their depth

00:04:20,209 --> 00:04:26,289
that is there are more than one hidden

00:04:22,819 --> 00:04:26,289
layer in the network architecture

00:04:27,970 --> 00:04:36,729
chaining Chaney is a process to make

00:04:32,449 --> 00:04:39,919
your network capable of inference tasks

00:04:36,729 --> 00:04:44,830
the type of coaching process has four

00:04:39,919 --> 00:04:48,970
steps step one initialized ways step two

00:04:44,830 --> 00:04:52,240
set input data for example an image and

00:04:48,970 --> 00:04:55,940
compute the network output

00:04:52,240 --> 00:05:00,280
step three compelled output and ground

00:04:55,940 --> 00:05:02,960
choose and calculate the arrow step four

00:05:00,280 --> 00:05:07,400
modify the weights and go to step two

00:05:02,960 --> 00:05:09,650
and here the arrow is small enough this

00:05:07,400 --> 00:05:12,800
process is a little bit complicated

00:05:09,650 --> 00:05:14,960
right but don't worry

00:05:12,800 --> 00:05:23,300
deep learning for walks will do that for

00:05:14,960 --> 00:05:26,270
you inference you have a chain model

00:05:23,300 --> 00:05:29,750
that is a set of ways in other

00:05:26,270 --> 00:05:32,330
parameters set input data and compute

00:05:29,750 --> 00:05:36,710
the network output using deep learning

00:05:32,330 --> 00:05:44,900
library down inference is pretty simple

00:05:36,710 --> 00:05:48,469
compared to the chaining here are some

00:05:44,900 --> 00:05:51,250
scenarios that apprise the deep neural

00:05:48,469 --> 00:05:55,360
network to the field of computer vision

00:05:51,250 --> 00:05:59,780
you can do face recognition

00:05:55,360 --> 00:06:06,380
click so segmentation object detection

00:05:59,780 --> 00:06:10,190
and a lot of other interesting things ok

00:06:06,380 --> 00:06:14,750
let's start our topic today open CVD a

00:06:10,190 --> 00:06:17,900
module in recent years and a many area

00:06:14,750 --> 00:06:22,270
the deep learning has shown result far

00:06:17,900 --> 00:06:25,070
exceeding rows of classical of reasons

00:06:22,270 --> 00:06:28,279
these also apply to the field of

00:06:25,070 --> 00:06:33,010
computer vision well the mass of program

00:06:28,279 --> 00:06:37,940
is solved using neural networks since

00:06:33,010 --> 00:06:40,450
Open Series three point three DN mojo is

00:06:37,940 --> 00:06:44,810
included in the men open CV wrap

00:06:40,450 --> 00:06:48,710
repository it implements interest only

00:06:44,810 --> 00:06:51,890
and is compatible to many popular people

00:06:48,710 --> 00:06:56,750
in different walks like Tessa floor Cafe

00:06:51,890 --> 00:07:02,420
Taj darknet that means you can use the

00:06:56,750 --> 00:07:07,960
net model pretend from this free walk

00:07:02,420 --> 00:07:07,960
directly without any moto transformation

00:07:08,470 --> 00:07:17,270
so why we need a new way of DNA in open

00:07:12,830 --> 00:07:20,720
CV since we have already so many the

00:07:17,270 --> 00:07:25,940
planning library I think there are many

00:07:20,720 --> 00:07:28,160
reasons here firstly the line is it's

00:07:25,940 --> 00:07:32,180
possible to achieve the lightness of the

00:07:28,160 --> 00:07:36,700
solution leaving the only ability to

00:07:32,180 --> 00:07:36,700
perform a forward pass over the network

00:07:37,870 --> 00:07:46,000
these cans simplify the code speed up

00:07:42,080 --> 00:07:50,090
the installation and compilation process

00:07:46,000 --> 00:07:53,950
secondly the convenience da mojo is a

00:07:50,090 --> 00:07:58,910
safe content it's possible to reduce the

00:07:53,950 --> 00:08:01,100
external dependency to a minimum this

00:07:58,910 --> 00:08:05,919
will simplify the distribution of

00:08:01,100 --> 00:08:10,669
applications and if the project

00:08:05,919 --> 00:08:13,370
previously used OpenCV library is not

00:08:10,669 --> 00:08:18,880
difficult to add a support for the

00:08:13,370 --> 00:08:24,110
neural network to such a project and

00:08:18,880 --> 00:08:27,500
universality DN module provides a

00:08:24,110 --> 00:08:29,510
unified interface to manipulate net

00:08:27,500 --> 00:08:33,950
models from different different

00:08:29,510 --> 00:08:38,719
infraworks it supports multiple targeted

00:08:33,950 --> 00:08:45,269
device like CPU GPU and vpu it can run

00:08:38,719 --> 00:08:48,860
on Linux Windows Android and Mac OS

00:08:45,269 --> 00:08:53,600
it's possible to make your application

00:08:48,860 --> 00:08:53,600
light wise and run everywhere

00:08:55,970 --> 00:09:03,029
dear mojo support all the basic layers

00:08:59,929 --> 00:09:09,720
from the basic convolution fully

00:09:03,029 --> 00:09:11,899
connected to more specialized wines in

00:09:09,720 --> 00:09:15,149
addition to support individual layers

00:09:11,899 --> 00:09:17,869
support for specific neural network

00:09:15,149 --> 00:09:22,110
architecture is also important

00:09:17,869 --> 00:09:25,889
Alex net Google net rest rest net and

00:09:22,110 --> 00:09:28,980
squeeze net FCN enid and SSD

00:09:25,889 --> 00:09:32,490
architecture are well tested in the air

00:09:28,980 --> 00:09:37,980
module and more support will be added on

00:09:32,490 --> 00:09:41,459
the leader let's go through the

00:09:37,980 --> 00:09:45,179
technical details of the DNA job this is

00:09:41,459 --> 00:09:48,949
the architecture from the top to bottom

00:09:45,179 --> 00:09:52,920
the first layer is language bindings

00:09:48,949 --> 00:09:55,259
Python and Java are supported other

00:09:52,920 --> 00:10:01,559
components in this layer are accuracy

00:09:55,259 --> 00:10:07,519
test performance test and samples the

00:10:01,559 --> 00:10:11,360
next layer is C++ API this provides the

00:10:07,519 --> 00:10:14,999
high level interface to the end module

00:10:11,360 --> 00:10:18,420
you can use this interface to load that

00:10:14,999 --> 00:10:22,709
model long it and the receive network

00:10:18,420 --> 00:10:27,990
outputs the next layer is implementation

00:10:22,709 --> 00:10:31,170
layer it includes model importers which

00:10:27,990 --> 00:10:35,689
convert the preach and net model from

00:10:31,170 --> 00:10:40,139
different deplaning for a walk into the

00:10:35,689 --> 00:10:42,540
internal representation this layer also

00:10:40,139 --> 00:10:45,779
includes the engine and layer

00:10:42,540 --> 00:10:49,490
implementations which implement the

00:10:45,779 --> 00:10:54,679
general logic of the neural network at

00:10:49,490 --> 00:10:54,679
the bottom is the acceleration layer

00:10:56,750 --> 00:11:04,649
dear mojo implemented CPU acceleration

00:11:01,829 --> 00:11:07,980
OpenCL acceleration and highlight

00:11:04,649 --> 00:11:11,310
acceleration by itself for the cpu

00:11:07,980 --> 00:11:15,259
acceleration SS Yi and a VX inches inch

00:11:11,310 --> 00:11:20,850
in six and multi-thread are heavily used

00:11:15,259 --> 00:11:23,009
for open CL acceleration some highly

00:11:20,850 --> 00:11:26,069
optimized the kernels are implemented

00:11:23,009 --> 00:11:29,480
and for the highlight acceleration

00:11:26,069 --> 00:11:36,680
highlight language is used to implement

00:11:29,480 --> 00:11:40,350
layout computations besides is all

00:11:36,680 --> 00:11:42,509
implementation TN module also use Intel

00:11:40,350 --> 00:11:48,990
interest engine library to do the

00:11:42,509 --> 00:11:51,959
acceleration angel inference engine is a

00:11:48,990 --> 00:11:56,899
part of entire open window toolkit which

00:11:51,959 --> 00:12:03,630
is a set of tools and libraries for

00:11:56,899 --> 00:12:06,630
computer vision application da mojo

00:12:03,630 --> 00:12:15,529
defies back-end and target to manage

00:12:06,630 --> 00:12:19,980
this different acceleration method it

00:12:15,529 --> 00:12:23,130
provides users with the flexibility to

00:12:19,980 --> 00:12:26,850
choose the proper acceleration method

00:12:23,130 --> 00:12:33,600
according to their software and hardware

00:12:26,850 --> 00:12:36,930
environment OpenCV back-end is the

00:12:33,600 --> 00:12:41,660
default packet in the support CPU

00:12:36,930 --> 00:12:41,660
acceleration and overseer acceleration

00:12:42,110 --> 00:12:49,949
these acceleration are built in

00:12:46,050 --> 00:12:53,009
implementations without net without

00:12:49,949 --> 00:12:59,569
external dependency so you can use them

00:12:53,009 --> 00:13:03,569
out of box highlight highlight back-end

00:12:59,569 --> 00:13:06,449
also support GPU acceleration and OpenCL

00:13:03,569 --> 00:13:10,130
acceleration test depends on the

00:13:06,449 --> 00:13:10,130
highlight compiler and run

00:13:11,170 --> 00:13:19,040
if you have a if you have inhale open

00:13:14,840 --> 00:13:20,750
vino SDK installed you can choose you

00:13:19,040 --> 00:13:24,980
can choose angel inference engine

00:13:20,750 --> 00:13:28,580
back-end it's a support CPU opens they

00:13:24,980 --> 00:13:33,410
are and the Madrid acceleration by using

00:13:28,580 --> 00:13:42,110
the MK r DN C LT n and more videos VP

00:13:33,410 --> 00:13:43,940
you respectively underneath use the set

00:13:42,110 --> 00:13:47,660
preferable back-end and the set

00:13:43,940 --> 00:13:52,100
profitable target to choose what the

00:13:47,660 --> 00:13:55,160
acceleration method you want for example

00:13:52,100 --> 00:13:59,210
if you won't use my videos VP you to

00:13:55,160 --> 00:14:01,820
acceleration set the back end to the

00:13:59,210 --> 00:14:06,350
inference engine and the target to

00:14:01,820 --> 00:14:10,190
Madrid tier module will fall back to the

00:14:06,350 --> 00:14:14,860
default back end if it failed to detect

00:14:10,190 --> 00:14:14,860
the back end and target you said to it

00:14:18,190 --> 00:14:25,460
network optimizations dear module has

00:14:21,800 --> 00:14:26,740
some general optimizations on network

00:14:25,460 --> 00:14:30,980
level

00:14:26,740 --> 00:14:33,770
thanks to is internal implementation of

00:14:30,980 --> 00:14:36,830
different network this optimization are

00:14:33,770 --> 00:14:40,630
not held to any specific the planning

00:14:36,830 --> 00:14:43,220
frameworks that means discipline

00:14:40,630 --> 00:14:46,190
optimizations are benefit all the net

00:14:43,220 --> 00:14:50,090
models no matter what the original

00:14:46,190 --> 00:14:57,410
framework is I will introduce these

00:14:50,090 --> 00:15:00,260
optimizations layer fusion when TN

00:14:57,410 --> 00:15:03,550
module setup is internal representation

00:15:00,260 --> 00:15:07,490
of network it analysis the network act

00:15:03,550 --> 00:15:10,640
structure and if possible merge some

00:15:07,490 --> 00:15:13,940
layers into other layer this can reduce

00:15:10,640 --> 00:15:17,420
network complexity and compute

00:15:13,940 --> 00:15:21,949
computation workload there are three

00:15:17,420 --> 00:15:25,249
fusion types currently in the module

00:15:21,949 --> 00:15:28,040
in this case a convolution layer fills

00:15:25,249 --> 00:15:31,569
the subsequent batch normal layer scale

00:15:28,040 --> 00:15:37,480
layer and lelou layer you can find this

00:15:31,569 --> 00:15:41,059
structure in ResNet 50 architecture and

00:15:37,480 --> 00:15:43,699
in this case a convolution layer fills

00:15:41,059 --> 00:15:46,639
the subsequent admin element-wise layer

00:15:43,699 --> 00:15:52,189
and relu layer and take another

00:15:46,639 --> 00:15:55,699
convolution layer as its input and in

00:15:52,189 --> 00:15:59,209
this case the concat layer is eliminated

00:15:55,699 --> 00:16:05,660
this is a type of case in SSD

00:15:59,209 --> 00:16:09,109
architecture another optimization is

00:16:05,660 --> 00:16:12,709
memory reuse this is the normal case

00:16:09,109 --> 00:16:15,949
without any memory reuse the red box

00:16:12,709 --> 00:16:19,040
represents the allocated memory and the

00:16:15,949 --> 00:16:23,059
green box represent the reference memory

00:16:19,040 --> 00:16:25,519
in this case each layer arrow case is

00:16:23,059 --> 00:16:28,069
hung out to the memory and the

00:16:25,519 --> 00:16:33,040
subsequent layers reference these output

00:16:28,069 --> 00:16:36,619
memory as is input memory da module

00:16:33,040 --> 00:16:39,379
analysis the memory lifecycle and reuse

00:16:36,619 --> 00:16:46,220
the previously allocated memory if

00:16:39,379 --> 00:16:50,829
possible the first case is to reduce the

00:16:46,220 --> 00:16:54,739
input memory for example if the layer to

00:16:50,829 --> 00:16:57,230
support in place computation it has no

00:16:54,739 --> 00:17:02,389
need to allocate the output memory

00:16:57,230 --> 00:17:08,169
instead it reuse is input memory like

00:17:02,389 --> 00:17:10,760
this the second case is more general

00:17:08,169 --> 00:17:12,019
considering that the inference process

00:17:10,760 --> 00:17:15,500
is sequential

00:17:12,019 --> 00:17:19,309
from bottom to top only one layer works

00:17:15,500 --> 00:17:22,309
at a time that means the higher layer

00:17:19,309 --> 00:17:25,909
has chance to reuse the memory allocated

00:17:22,309 --> 00:17:30,019
at the lower layer in this case the

00:17:25,909 --> 00:17:34,250
layer 3 don't allocated its own output

00:17:30,019 --> 00:17:35,510
layer instead it reused output the

00:17:34,250 --> 00:17:42,350
memory of the layer

00:17:35,510 --> 00:17:45,110
one with this memory optimization the

00:17:42,350 --> 00:17:49,420
runtime memory footprint of an app model

00:17:45,110 --> 00:17:49,420
can be significantly reduced

00:17:51,790 --> 00:17:57,130
OpenCL acceleration the open sea

00:17:55,460 --> 00:17:59,630
acceleration is the built-in

00:17:57,130 --> 00:18:03,590
implementation it has no external

00:17:59,630 --> 00:18:04,070
dependency except for the open sea at

00:18:03,590 --> 00:18:07,610
runtime

00:18:04,070 --> 00:18:11,960
it supports fraud point 232 and fraud

00:18:07,610 --> 00:18:16,520
pointers sixteen date format if you want

00:18:11,960 --> 00:18:19,040
in a naval obviously acceleration just

00:18:16,520 --> 00:18:23,420
to set back-end to open CV and the

00:18:19,040 --> 00:18:26,390
target to open CL or open CL FP 16 if

00:18:23,420 --> 00:18:32,870
you want use the Froude pointer 16 date

00:18:26,390 --> 00:18:35,410
format open CL acceleration has some

00:18:32,870 --> 00:18:39,650
highly optimized the convolution kernels

00:18:35,410 --> 00:18:42,530
we took an auto tuning approach to find

00:18:39,650 --> 00:18:47,630
the best kernel convert configurations

00:18:42,530 --> 00:18:50,720
for specific GPU there are a set of pre

00:18:47,630 --> 00:18:54,350
tuned kernel configurations built in the

00:18:50,720 --> 00:18:58,790
library DM module we are used then by

00:18:54,350 --> 00:19:02,180
default but if you want to get the best

00:18:58,790 --> 00:19:07,300
performance for your GPU try to run out

00:19:02,180 --> 00:19:07,300
tuning instead of using the default one

00:19:08,500 --> 00:19:18,290
it's easy to enable auto tuning just set

00:19:12,680 --> 00:19:22,610
the environment variable OpenCV ocl for

00:19:18,290 --> 00:19:26,840
the end config path to the directory to

00:19:22,610 --> 00:19:29,300
store your config files if you enable

00:19:26,840 --> 00:19:32,680
auto tuning the first time learning and

00:19:29,300 --> 00:19:35,660
that model will be a little bit long

00:19:32,680 --> 00:19:38,840
next time the end module will use the

00:19:35,660 --> 00:19:42,190
cached configurations directly and no

00:19:38,840 --> 00:19:42,190
need to tuning again

00:19:43,960 --> 00:19:50,890
for bad performance of Angel GPU use Neo

00:19:47,230 --> 00:19:55,570
Java new is the open source open CL Java

00:19:50,890 --> 00:19:59,200
for Intel GPU it's support gen8 graphics

00:19:55,570 --> 00:20:02,590
and beyond the best practice is to use

00:19:59,200 --> 00:20:05,950
the version as new as possible according

00:20:02,590 --> 00:20:08,520
to our experience new version always has

00:20:05,950 --> 00:20:08,520
better performance

00:20:11,070 --> 00:20:17,470
these are inference time in milliseconds

00:20:14,380 --> 00:20:20,890
for scipio acceleration and an open C

00:20:17,470 --> 00:20:25,890
acceleration the test machine has an

00:20:20,890 --> 00:20:31,630
Intel Core i7 CPU with 8 cores and Inter

00:20:25,890 --> 00:20:34,780
iris pro GPU with 72 exclude execution

00:20:31,630 --> 00:20:37,420
units in this case the open CL

00:20:34,780 --> 00:20:42,850
performance is far exceeding CPU

00:20:37,420 --> 00:20:50,890
performance you can find more

00:20:42,850 --> 00:20:51,900
performance data in this page walk'n

00:20:50,890 --> 00:20:54,970
backend

00:20:51,900 --> 00:20:58,480
walk'n is the next-generation graphics

00:20:54,970 --> 00:21:00,540
and computer api from Kronos the same

00:20:58,480 --> 00:21:06,130
cross industry group that maintains

00:21:00,540 --> 00:21:09,960
opengl Vulcan back-end can extend the

00:21:06,130 --> 00:21:13,960
usage of GPU acceleration for DM module

00:21:09,960 --> 00:21:20,230
for example the and roy has no support

00:21:13,960 --> 00:21:22,630
for OpenGL but it'll support woken so

00:21:20,230 --> 00:21:27,640
you can leverage the GPU acceleration

00:21:22,630 --> 00:21:30,100
using Vulcan back-end on Android and the

00:21:27,640 --> 00:21:31,720
working back-end use computer shader to

00:21:30,100 --> 00:21:38,080
implement layer compute

00:21:31,720 --> 00:21:40,870
computation I have been working on the

00:21:38,080 --> 00:21:43,240
working back-end for some time this is

00:21:40,870 --> 00:21:46,090
the PR in review if you have any

00:21:43,240 --> 00:21:48,810
interests don't hesitate to give your

00:21:46,090 --> 00:21:48,810
comments there

00:21:49,950 --> 00:21:56,760
ok the sample this is Tizen program to

00:21:55,960 --> 00:21:59,850
do the real

00:21:56,760 --> 00:22:05,850
objection did have objection detection

00:21:59,850 --> 00:22:09,049
with mobile net SSD the first thing is

00:22:05,850 --> 00:22:12,950
to import the OpenCV Pisan module and

00:22:09,049 --> 00:22:16,760
then defy the pass to net model files a

00:22:12,950 --> 00:22:20,610
meter size this model can accept of

00:22:16,760 --> 00:22:24,210
confidence threshold mean value and the

00:22:20,610 --> 00:22:26,880
class name list of course you can pass

00:22:24,210 --> 00:22:35,370
in this information use command line

00:22:26,880 --> 00:22:41,970
arguments on the ligh 16 open a camera

00:22:35,370 --> 00:22:44,400
device and actually you just need a few

00:22:41,970 --> 00:22:48,780
lines of code to introduce the a

00:22:44,400 --> 00:22:53,720
functionality on line 19 load the net

00:22:48,780 --> 00:22:59,309
model and on line 20 23 and 24

00:22:53,720 --> 00:23:03,419
pre-processed the captured image only 26

00:22:59,309 --> 00:23:07,320
set image as the network input in only

00:23:03,419 --> 00:23:14,280
27 forward the network and and it will

00:23:07,320 --> 00:23:16,830
return you the detection result the rest

00:23:14,280 --> 00:23:20,309
of the code are for visualizing the

00:23:16,830 --> 00:23:22,710
detection result they feel they are

00:23:20,309 --> 00:23:26,250
filtering the candidates according to

00:23:22,710 --> 00:23:29,900
the confidence threshold join bounding

00:23:26,250 --> 00:23:36,330
box class name and confidence and

00:23:29,900 --> 00:23:42,059
displayed image you can find more simple

00:23:36,330 --> 00:23:48,200
codes in this page okay I will sure I

00:23:42,059 --> 00:23:48,200
will run a program and see what happened

00:23:53,320 --> 00:24:01,560
okay cannot show on screen sorry

00:24:21,920 --> 00:24:31,399
okay

00:24:24,330 --> 00:24:36,299
I 20 classes mobile net bottle so you

00:24:31,399 --> 00:24:39,600
can use another model for example 90

00:24:36,299 --> 00:24:45,210
classes model to detect more object

00:24:39,600 --> 00:24:55,340
objects on the Left top is the class

00:24:45,210 --> 00:24:55,340
class name and the confidence okay

00:24:57,320 --> 00:25:03,400
okay that's all anybody has any question

00:25:10,230 --> 00:25:14,780
okay thanks Oh what this

00:25:24,410 --> 00:25:29,640
it's okay

00:25:26,040 --> 00:25:34,380
the the native the native API is the

00:25:29,640 --> 00:25:34,890
simplest class it's okay but but for the

00:25:34,380 --> 00:25:41,720
devil

00:25:34,890 --> 00:25:41,720
I use the Pisan is easy for proper type

00:25:49,029 --> 00:26:02,159
from race okay in my machine the friend

00:25:54,549 --> 00:26:06,489
race can reach more than 60 FPS 30 FPS

00:26:02,159 --> 00:26:14,549
that depends on your machine and depends

00:26:06,489 --> 00:26:14,549
on what back-end you choose okay

00:26:16,340 --> 00:26:22,259
okay thank you very much

00:26:18,740 --> 00:26:22,259

YouTube URL: https://www.youtube.com/watch?v=VTX5WmWSEEk


