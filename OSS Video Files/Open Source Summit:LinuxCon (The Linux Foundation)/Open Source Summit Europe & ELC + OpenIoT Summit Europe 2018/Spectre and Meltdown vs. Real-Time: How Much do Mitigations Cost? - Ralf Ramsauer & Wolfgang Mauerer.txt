Title: Spectre and Meltdown vs. Real-Time: How Much do Mitigations Cost? - Ralf Ramsauer & Wolfgang Mauerer
Publication date: 2018-10-25
Playlist: Open Source Summit Europe & ELC + OpenIoT Summit Europe 2018
Description: 
	Spectre and Meltdown vs. Real-Time: How Much do Mitigations Cost? - Ralf Ramsauer & Wolfgang Mauerer, OTH Regensburg; Jan Kiszka, Siemens AG

2018 had a bumpy landing with the disclosure of hardware vulnerabilities in microprocessors. With Spectre and Meltdown leading the way, they enabled attackers to read private data from kernel and user space. Since then, several other hardware bugs were revealed.

Lower level system software such as operating systems or hypervisors needs to implement countermeasures to the vulnerabilities. The fixes come at a cost, particularly in execution time. Embedded real-time environments have a strong focus on low latency and determinism and may be affected by those fixes.

Whatâ€™s the mitigation cost and impact on typical real-time applications? To answer these question, we use benchmarks as well as a set of microbenchmarks to quantify the consequences of the mitigations. Our targets are real-time operating systems, such as the Preempt-RT patch for Linux as well as embedded real-time hypervisors. 

About Jan Kiszka
Jan Kiszka is working as consultant, open source evangelist and senior software engineer in the Competence Center for Embedded Linux at Siemens Corporate Technology. He is supporting Siemens division and subsidiaries with adapting and enhancing open source as platform for their products. For customer projects and whenever his spare time permits, he is contributing to open source projects, specifically in the area of real-time and virtualization. 

About Ralf Ramsauer
Ralf Ramsauer is a PhD student at the University of Applied Sciences Regensburg where he works in a joint project together with Siemens Corporate Competence Center Embedded Linux. His academic research interests focus on finding successful long term maintenance strategies for Open Source Software in embedded industrial context. This covers the full software stack of embedded systems, from hardware-related low-level virtualisation technologies via kernel-space through to userland.
Captions: 
	00:00:00,030 --> 00:00:05,279
yeah so hello and welcome to our talk

00:00:02,310 --> 00:00:08,360
spectra and Motown versus real time how

00:00:05,279 --> 00:00:12,120
much do mitigations cost so why are we

00:00:08,360 --> 00:00:14,969
doing all this in the beginning of this

00:00:12,120 --> 00:00:18,930
year people started to speculate on

00:00:14,969 --> 00:00:21,960
speculative execution of CPUs and that

00:00:18,930 --> 00:00:23,670
literally hits the core of our systems

00:00:21,960 --> 00:00:26,760
or the course of our systems

00:00:23,670 --> 00:00:31,590
there were many CPU bugs that were

00:00:26,760 --> 00:00:34,280
disclosed since then and there are

00:00:31,590 --> 00:00:36,719
already many benchmarks out there that

00:00:34,280 --> 00:00:40,980
investigate on the effect of those

00:00:36,719 --> 00:00:42,510
mitigations for desktop systems for data

00:00:40,980 --> 00:00:44,760
center appliances there are many

00:00:42,510 --> 00:00:48,090
benchmarks that focus on web service

00:00:44,760 --> 00:00:50,670
like nginx apache on database servers on

00:00:48,090 --> 00:00:53,190
compilation copulation is a popular

00:00:50,670 --> 00:00:55,469
benchmark video and decoding apparently

00:00:53,190 --> 00:00:57,510
it appears that git is also used as a

00:00:55,469 --> 00:00:59,520
benchmark it does not only produce CPU

00:00:57,510 --> 00:01:02,280
load also produces this Gayo load and

00:00:59,520 --> 00:01:05,970
many other benchmarks but the best of

00:01:02,280 --> 00:01:08,850
our knowledge none of them focused on

00:01:05,970 --> 00:01:11,939
the effect of those mitigations on

00:01:08,850 --> 00:01:17,070
real-time systems where determinism and

00:01:11,939 --> 00:01:20,640
low latency Smither yes so what are we

00:01:17,070 --> 00:01:23,549
focusing on in this talk we will only

00:01:20,640 --> 00:01:26,640
focus on Orion one two four there are

00:01:23,549 --> 00:01:29,310
many other variety will not be part of

00:01:26,640 --> 00:01:32,130
this talk and the level one terminal for

00:01:29,310 --> 00:01:36,720
Dell 1tf which was disclosed in I think

00:01:32,130 --> 00:01:39,930
it was August this year yeah so the

00:01:36,720 --> 00:01:43,530
commonality of those bugs is that they

00:01:39,930 --> 00:01:45,780
somehow in a certain way they exploit

00:01:43,530 --> 00:01:49,229
speculative execution they exploit

00:01:45,780 --> 00:01:53,280
branch prediction units together with

00:01:49,229 --> 00:01:56,939
the behavior of caches and time

00:01:53,280 --> 00:01:59,909
concentrated side-channel attacks with

00:01:56,939 --> 00:02:03,409
this attackers are able to leak

00:01:59,909 --> 00:02:06,270
sensitive information and this breaks

00:02:03,409 --> 00:02:11,190
guarantees that work that are given by

00:02:06,270 --> 00:02:12,590
memory management unit units such as the

00:02:11,190 --> 00:02:16,930
memory protection so

00:02:12,590 --> 00:02:21,940
this is violated by those bugs and

00:02:16,930 --> 00:02:24,650
attackers may be able to read not only

00:02:21,940 --> 00:02:26,989
memory of other processes but also

00:02:24,650 --> 00:02:28,640
memory from kernel space and under

00:02:26,989 --> 00:02:31,970
certain conditions even arbitrary

00:02:28,640 --> 00:02:34,160
physical memory nevertheless we need to

00:02:31,970 --> 00:02:37,280
keep in mind that these are local

00:02:34,160 --> 00:02:39,319
vectors only so an attacker needs to

00:02:37,280 --> 00:02:41,629
have arbitrary or needs to be able to

00:02:39,319 --> 00:02:48,049
execute arbitrary code on an effective

00:02:41,629 --> 00:02:50,450
system if an effective system runs a

00:02:48,049 --> 00:02:52,459
browser which executes JavaScript this

00:02:50,450 --> 00:02:54,620
is already the case but we need to keep

00:02:52,459 --> 00:02:57,110
in mind that this is a local vector only

00:02:54,620 --> 00:03:00,530
there are some other effects like the

00:02:57,110 --> 00:03:03,620
net spectra that also forms a remote

00:03:00,530 --> 00:03:06,280
vector so a surface for a remote attack

00:03:03,620 --> 00:03:10,160
but even under laboratory conditions

00:03:06,280 --> 00:03:11,900
they were only able to leak bytes per

00:03:10,160 --> 00:03:14,030
hour so this will not be focused on this

00:03:11,900 --> 00:03:15,860
talk and we have to keep in mind that

00:03:14,030 --> 00:03:17,359
these are that these all of these

00:03:15,860 --> 00:03:21,799
attacks are have a rather high

00:03:17,359 --> 00:03:24,230
complexity so on the mitigations we I

00:03:21,799 --> 00:03:27,650
will especially focus or a particularly

00:03:24,230 --> 00:03:29,470
focus on x86 and on arm 64 platforms

00:03:27,650 --> 00:03:34,280
those are the platforms that we measured

00:03:29,470 --> 00:03:36,680
during our analysis I do not want to go

00:03:34,280 --> 00:03:38,420
in greater detail on how those attacks

00:03:36,680 --> 00:03:42,260
work because they are very complex I

00:03:38,420 --> 00:03:44,389
want to focus on the mitigations only

00:03:42,260 --> 00:03:46,160
and for spectral around 1 the bounds

00:03:44,389 --> 00:03:49,760
check bypass we have the so-called user

00:03:46,160 --> 00:03:52,190
pointer sanitization where when we enter

00:03:49,760 --> 00:03:54,769
kernel space we need to protect some

00:03:52,190 --> 00:03:57,650
input that might be forwarded by the

00:03:54,769 --> 00:04:01,609
user such as for example it's a system

00:03:57,650 --> 00:04:04,010
call number the system call number is a

00:04:01,609 --> 00:04:05,810
value that is used for an indirect array

00:04:04,010 --> 00:04:08,150
access in the kernel space and we need

00:04:05,810 --> 00:04:09,680
to protect that speculation is done on

00:04:08,150 --> 00:04:13,970
this value because the user has control

00:04:09,680 --> 00:04:15,709
over over this value therefore on both

00:04:13,970 --> 00:04:18,859
platforms we use the so-called no

00:04:15,709 --> 00:04:23,090
speculative accessors that are used to

00:04:18,859 --> 00:04:26,330
protect against this attack luckily the

00:04:23,090 --> 00:04:29,379
cost per mitigation call on those

00:04:26,330 --> 00:04:32,090
is rather low so it comes at a low cost

00:04:29,379 --> 00:04:36,439
for specter of Orion - it looks a bit

00:04:32,090 --> 00:04:39,139
different on x86 as well as on arm 64 we

00:04:36,439 --> 00:04:41,919
have the config red line option on Linux

00:04:39,139 --> 00:04:46,099
the return trampoline which converts

00:04:41,919 --> 00:04:49,280
indirect calls indirect jumps to returns

00:04:46,099 --> 00:04:51,650
and in this way it reads again it

00:04:49,280 --> 00:04:56,050
prevents accusation in addition to that

00:04:51,650 --> 00:05:00,469
on x86 as well as on arm 64

00:04:56,050 --> 00:05:03,229
we now need the possibility to control a

00:05:00,469 --> 00:05:05,840
speculative execution and on x86 this is

00:05:03,229 --> 00:05:08,090
done by the speculative control

00:05:05,840 --> 00:05:11,930
extensions with which come with micro

00:05:08,090 --> 00:05:17,930
code updates for for Intel and AMD CPUs

00:05:11,930 --> 00:05:21,469
and on arm 64 we apply those mitigations

00:05:17,930 --> 00:05:24,590
in the film there in the monitor to

00:05:21,469 --> 00:05:27,229
mitigate against Brian - we need at

00:05:24,590 --> 00:05:30,279
least arm trusted firmware version 1.1

00:05:27,229 --> 00:05:33,349
point 6 which implements the system

00:05:30,279 --> 00:05:36,169
management calling system management

00:05:33,349 --> 00:05:38,479
calling convention version 1.1 and we

00:05:36,169 --> 00:05:42,229
also need to fill returns like buffers

00:05:38,479 --> 00:05:45,259
on context switch and in sum this leads

00:05:42,229 --> 00:05:47,509
to a very high cost per a mitigation

00:05:45,259 --> 00:05:50,210
call on those systems especially on arm

00:05:47,509 --> 00:05:57,860
64 where we need to switch the privilege

00:05:50,210 --> 00:05:59,990
level to apply the medication the two

00:05:57,860 --> 00:06:02,270
other variants where I am three and four

00:05:59,990 --> 00:06:05,870
meltdown and speculative store bypass

00:06:02,270 --> 00:06:08,000
for meltdown we now have the so-called

00:06:05,870 --> 00:06:09,770
page table isolation in the kernel

00:06:08,000 --> 00:06:13,339
initially the page table isolation was

00:06:09,770 --> 00:06:14,270
implemented - to have further protection

00:06:13,339 --> 00:06:17,000
for the address space layout

00:06:14,270 --> 00:06:19,400
randomization now this protection is

00:06:17,000 --> 00:06:22,550
also used to mitigate the meltdown

00:06:19,400 --> 00:06:24,349
attack on x86 it's simply called page

00:06:22,550 --> 00:06:25,879
table isolation on arm 64 it's called

00:06:24,349 --> 00:06:29,599
the kernel page table isolation or

00:06:25,879 --> 00:06:31,909
Kaiser as it was initially named by the

00:06:29,599 --> 00:06:33,409
authors they introduced different page

00:06:31,909 --> 00:06:35,779
directories for

00:06:33,409 --> 00:06:39,529
kernel space and for user space so

00:06:35,779 --> 00:06:41,899
before those before page table isolation

00:06:39,529 --> 00:06:45,229
space and user space shared the same

00:06:41,899 --> 00:06:48,949
pages now we have two unmet the

00:06:45,229 --> 00:06:51,529
kernel-space pages from user space which

00:06:48,949 --> 00:06:54,499
requires us on each context switch on or

00:06:51,529 --> 00:06:56,629
on each switch to the kernel space to

00:06:54,499 --> 00:06:59,659
apply the mitigation to switch the page

00:06:56,629 --> 00:07:01,609
table entries for vara and for we again

00:06:59,659 --> 00:07:03,949
we need some micro code updates on arm

00:07:01,609 --> 00:07:06,859
64 we again we need an update for the

00:07:03,949 --> 00:07:09,819
aam trusted firmware and authors say

00:07:06,859 --> 00:07:13,249
that Brian 4 comes with a notable

00:07:09,819 --> 00:07:16,369
performance impact so mitigations are

00:07:13,249 --> 00:07:19,819
applied on a per processor basis using

00:07:16,369 --> 00:07:21,889
PR control and second the cost per

00:07:19,819 --> 00:07:25,249
mitigation again is of course very high

00:07:21,889 --> 00:07:27,439
Brian for will not be a topic of this

00:07:25,249 --> 00:07:29,809
talk because we didn't activate it in

00:07:27,439 --> 00:07:31,909
our measurements then there is a

00:07:29,809 --> 00:07:36,009
slightly different attack the level 1

00:07:31,909 --> 00:07:40,669
terminals fault also called foreshadow

00:07:36,009 --> 00:07:43,219
unaffected systems we either want to

00:07:40,669 --> 00:07:46,459
disable symmetric multi threading or use

00:07:43,219 --> 00:07:48,289
other medications to deal with this this

00:07:46,459 --> 00:07:50,809
issue this is not a problem for

00:07:48,289 --> 00:07:53,509
real-time systems in particular because

00:07:50,809 --> 00:07:55,999
we deactivate symmetric multi multi

00:07:53,509 --> 00:07:59,989
threading ever since because running

00:07:55,999 --> 00:08:02,539
real-time payloads across on SMT

00:07:59,989 --> 00:08:05,869
siblings causes undesired side effects

00:08:02,539 --> 00:08:07,219
due to contention or any other effects

00:08:05,869 --> 00:08:08,719
that come with a symmetric

00:08:07,219 --> 00:08:12,079
multi-threading so this is nothing new

00:08:08,719 --> 00:08:15,339
for for the real-time folks we disable

00:08:12,079 --> 00:08:18,289
SMT ever since the second mitigation for

00:08:15,339 --> 00:08:22,459
the page table is the page table entry

00:08:18,289 --> 00:08:25,069
inversion where we simply flip the upper

00:08:22,459 --> 00:08:27,409
bits of page table entries this comes as

00:08:25,069 --> 00:08:30,429
a neglected cost because we only apply a

00:08:27,409 --> 00:08:33,829
simple bit mask to the affected entries

00:08:30,429 --> 00:08:36,649
so the cost per our mitigation call for

00:08:33,829 --> 00:08:39,169
native workloads for real-time ver

00:08:36,649 --> 00:08:42,439
clothes for usual workloads is rather

00:08:39,169 --> 00:08:44,929
low whereas for datacenter appliances or

00:08:42,439 --> 00:08:50,259
for virtualized environments level one

00:08:44,929 --> 00:08:53,300
terminal fault is very very high okay so

00:08:50,259 --> 00:08:56,360
that's on the mitigations

00:08:53,300 --> 00:09:00,530
which CPUs are actually affected

00:08:56,360 --> 00:09:05,060
basically all CPUs are affected to a

00:09:00,530 --> 00:09:08,750
certain degree leading in tow they are

00:09:05,060 --> 00:09:11,420
vulnerable to spectral around 1 2 3 4

00:09:08,750 --> 00:09:14,600
and level 1 terminal for various AMD is

00:09:11,420 --> 00:09:17,180
not affected to version 3 version 3

00:09:14,600 --> 00:09:18,440
meltdown and it is not affected to the

00:09:17,180 --> 00:09:20,930
level 1 terminal 4

00:09:18,440 --> 00:09:24,200
there are also some non-speculative

00:09:20,930 --> 00:09:26,600
interval of orion's like older atom

00:09:24,200 --> 00:09:32,180
processors or core processors those are

00:09:26,600 --> 00:09:34,160
also not affected by those attacks on

00:09:32,180 --> 00:09:36,500
the arm side of life it looks a bit

00:09:34,160 --> 00:09:38,300
different there are some CPUs that are

00:09:36,500 --> 00:09:40,070
affected to version 1 there are some

00:09:38,300 --> 00:09:42,410
that are affected to version 2 there are

00:09:40,070 --> 00:09:44,810
some that are affected on version 4 no

00:09:42,410 --> 00:09:47,060
arm CPUs or affected for the level 1

00:09:44,810 --> 00:09:51,680
terminal fault and only the cortex a 75

00:09:47,060 --> 00:09:53,750
is is vulnerable to meltdown please look

00:09:51,680 --> 00:09:57,410
it up on those pages that are provided

00:09:53,750 --> 00:10:00,170
by the vendors here you will get exact

00:09:57,410 --> 00:10:04,490
detail which CPUs are affected to what

00:10:00,170 --> 00:10:07,880
degree ok mitigations versus real-time

00:10:04,490 --> 00:10:09,860
what do now those medications cost and

00:10:07,880 --> 00:10:13,550
before I want to explain the

00:10:09,860 --> 00:10:15,620
measurements that we did I would like to

00:10:13,550 --> 00:10:19,040
refresh in the knowledge what real-time

00:10:15,620 --> 00:10:21,470
actually means on real times systems in

00:10:19,040 --> 00:10:25,030
contrast to desktop or server

00:10:21,470 --> 00:10:27,380
applications the determinism matters

00:10:25,030 --> 00:10:29,510
deterministic responds to certain

00:10:27,380 --> 00:10:31,870
stimuli to may be external stimuli

00:10:29,510 --> 00:10:35,110
matters we might have time-sensitive

00:10:31,870 --> 00:10:38,120
reoccurring repetitive cyclic execution

00:10:35,110 --> 00:10:40,610
where we must not answer a very much

00:10:38,120 --> 00:10:43,850
given response not too early and not too

00:10:40,610 --> 00:10:45,590
and not too late so we might have found

00:10:43,850 --> 00:10:48,860
that latency is the time window where we

00:10:45,590 --> 00:10:51,740
must give an answer we must be sure that

00:10:48,860 --> 00:10:53,450
we on every event we are able to give

00:10:51,740 --> 00:10:56,900
this answer within a certain time window

00:10:53,450 --> 00:10:59,150
and in contrast to desktop systems data

00:10:56,900 --> 00:11:02,690
center systems we optimize for the worst

00:10:59,150 --> 00:11:05,370
case and not for the average case so for

00:11:02,690 --> 00:11:07,890
those targets

00:11:05,370 --> 00:11:11,130
Herman isn't matters more than the

00:11:07,890 --> 00:11:12,660
actual throughput and then there are the

00:11:11,130 --> 00:11:14,460
mitigations there are the cheap ones

00:11:12,660 --> 00:11:16,080
that I explains the page table entry I

00:11:14,460 --> 00:11:17,700
inversion the cheap one that use the

00:11:16,080 --> 00:11:19,410
pointer sanitization also a cheap one

00:11:17,700 --> 00:11:21,990
but then there are the expensive ones

00:11:19,410 --> 00:11:24,839
then at some points they might want to

00:11:21,990 --> 00:11:27,570
flush the caches the translation

00:11:24,839 --> 00:11:29,580
lookaside buffer we have the page table

00:11:27,570 --> 00:11:32,070
isolation the return stack buffer fills

00:11:29,580 --> 00:11:34,170
the return trampolines film records and

00:11:32,070 --> 00:11:36,210
context switches to replace the

00:11:34,170 --> 00:11:39,360
microcode to introduce new instructions

00:11:36,210 --> 00:11:41,310
to control speculative executions so we

00:11:39,360 --> 00:11:44,670
have a lot of potential sources that

00:11:41,310 --> 00:11:47,070
introduce additional overhead to our

00:11:44,670 --> 00:11:49,110
systems and those mitigations might of

00:11:47,070 --> 00:11:50,880
course increase the latency and the

00:11:49,110 --> 00:11:54,930
behavior of the systems and this is what

00:11:50,880 --> 00:11:58,890
we want to measure with our with our

00:11:54,930 --> 00:12:03,180
tests so the basic idea of our test is

00:11:58,890 --> 00:12:06,510
to run real-time payloads on multiple

00:12:03,180 --> 00:12:09,060
cores at once we do this because we also

00:12:06,510 --> 00:12:11,850
want to measure side effects across

00:12:09,060 --> 00:12:14,459
cores like shared caches like in the

00:12:11,850 --> 00:12:16,260
process communication cause so we run

00:12:14,459 --> 00:12:18,270
our real-time payload across multiple

00:12:16,260 --> 00:12:20,580
cores then we want to measure the

00:12:18,270 --> 00:12:22,709
responsivity of the systems and we

00:12:20,580 --> 00:12:25,470
repeat those measurements with and

00:12:22,709 --> 00:12:28,350
without the mitigations enabled and with

00:12:25,470 --> 00:12:32,570
and without additional workload payload

00:12:28,350 --> 00:12:35,660
on top of those CPUs to also measure

00:12:32,570 --> 00:12:38,040
additional side effects that might occur

00:12:35,660 --> 00:12:40,200
yet this is a standard problem of

00:12:38,040 --> 00:12:43,080
software product lines we now have the

00:12:40,200 --> 00:12:47,420
combinatorial explosion of variants that

00:12:43,080 --> 00:12:50,010
we might want to measure so we only

00:12:47,420 --> 00:12:53,250
confine on some certain measurements

00:12:50,010 --> 00:12:56,490
like one important measurement is to

00:12:53,250 --> 00:12:58,589
measure our system without any

00:12:56,490 --> 00:13:02,400
mitigation applied so they like the

00:12:58,589 --> 00:13:04,970
state at the end of 2017 and then we

00:13:02,400 --> 00:13:08,610
activate those mitigations that are

00:13:04,970 --> 00:13:10,709
necessary to protect the vulnerabilities

00:13:08,610 --> 00:13:13,470
of those systems for instance on an AMD

00:13:10,709 --> 00:13:15,750
system we will only we will apply the

00:13:13,470 --> 00:13:16,190
rubber iron to mitigations very very

00:13:15,750 --> 00:13:19,430
very

00:13:16,190 --> 00:13:22,819
on an intellect 86 system we will also

00:13:19,430 --> 00:13:26,680
apply the page table isolation and then

00:13:22,819 --> 00:13:29,449
for a bit fine grained analysis we only

00:13:26,680 --> 00:13:31,250
analyze the page table isolation to get

00:13:29,449 --> 00:13:33,620
the idea impact of the page table

00:13:31,250 --> 00:13:37,699
isolation only as well as the variant 2

00:13:33,620 --> 00:13:44,300
only and then we apply proper statistic

00:13:37,699 --> 00:13:49,129
analysis to to strengthen the yeah the

00:13:44,300 --> 00:13:51,259
outcome D the truth of our results the

00:13:49,129 --> 00:13:54,290
tools that we therefore use our standard

00:13:51,259 --> 00:13:55,790
real-time is a standard real-time test

00:13:54,290 --> 00:14:01,120
so our real-time payload is the

00:13:55,790 --> 00:14:05,360
well-known cyclic test cyclic test fires

00:14:01,120 --> 00:14:08,269
cyclically races and timer interrupts

00:14:05,360 --> 00:14:11,509
and it measures then the jitter of the

00:14:08,269 --> 00:14:13,370
interrupt in the user space if we have

00:14:11,509 --> 00:14:16,370
additional non real-time payload as a

00:14:13,370 --> 00:14:17,629
stress horror on those CPUs we use the

00:14:16,370 --> 00:14:21,350
stress and cheat tool to produce

00:14:17,629 --> 00:14:24,860
additional load we repeat this

00:14:21,350 --> 00:14:27,199
measurement in the hypervisor that we

00:14:24,860 --> 00:14:30,230
develop in our virtualized environment

00:14:27,199 --> 00:14:34,959
jail house because we want to know how

00:14:30,230 --> 00:14:38,120
those mitigations impact the overall

00:14:34,959 --> 00:14:42,560
behavior of the systems when we when we

00:14:38,120 --> 00:14:45,769
have a virtualized layer underneath our

00:14:42,560 --> 00:14:49,880
operating system please keep in mind

00:14:45,769 --> 00:14:51,949
that complex RT payloads may vary from

00:14:49,880 --> 00:14:53,779
the tests that we made because we have

00:14:51,949 --> 00:14:56,149
no inter process communication within

00:14:53,779 --> 00:14:58,880
the real-time context and these

00:14:56,149 --> 00:15:01,339
measurements shall just give you an

00:14:58,880 --> 00:15:03,589
overview of how those mythic or a

00:15:01,339 --> 00:15:06,319
tendency of how those mitigations might

00:15:03,589 --> 00:15:11,600
affect your system so what thus our

00:15:06,319 --> 00:15:14,899
measurement do we have in user space the

00:15:11,600 --> 00:15:16,430
cyclic test in the beginning runs and it

00:15:14,899 --> 00:15:19,279
wants to simply sleep for one

00:15:16,430 --> 00:15:21,559
millisecond it calls the kernel to set

00:15:19,279 --> 00:15:24,259
up the timer the kernel says ok I'll do

00:15:21,559 --> 00:15:27,050
that and go to sleep because it has

00:15:24,259 --> 00:15:29,780
nothing else to do later after a

00:15:27,050 --> 00:15:32,990
millisecond or so it

00:15:29,780 --> 00:15:35,060
gets an interrupt from the timer and it

00:15:32,990 --> 00:15:36,800
dispatches the timer it sees that cyclic

00:15:35,060 --> 00:15:39,010
tests request to the timer and it will

00:15:36,800 --> 00:15:41,750
forward it to the user space again and

00:15:39,010 --> 00:15:44,540
the user space would then measure that

00:15:41,750 --> 00:15:47,630
jitter the this is the time or the time

00:15:44,540 --> 00:15:50,060
delta between the timer when it actually

00:15:47,630 --> 00:15:53,210
occurred and it was and when it was

00:15:50,060 --> 00:15:56,000
expected to occur so the Chittor is the

00:15:53,210 --> 00:15:59,720
time window where the interrupt occurred

00:15:56,000 --> 00:16:05,570
and when we measured the actual time

00:15:59,720 --> 00:16:08,750
when it when it arrived in user space so

00:16:05,570 --> 00:16:12,110
how do those medications affect our

00:16:08,750 --> 00:16:15,140
system we have certain points where the

00:16:12,110 --> 00:16:18,110
mitigations now take place for info x86

00:16:15,140 --> 00:16:21,410
systems or for that cortex a 55 57 for

00:16:18,110 --> 00:16:23,120
instance we have to apply the page table

00:16:21,410 --> 00:16:24,770
as relation mitigation when we switch

00:16:23,120 --> 00:16:27,740
the context from user space to kernel

00:16:24,770 --> 00:16:29,390
space as well as when we leave the

00:16:27,740 --> 00:16:36,950
kernel space that can be again we have

00:16:29,390 --> 00:16:38,810
to apply the meltdown mitigation for the

00:16:36,950 --> 00:16:41,990
system call for instance we have to

00:16:38,810 --> 00:16:44,060
apply the mitigation for Brian one Dino

00:16:41,990 --> 00:16:46,010
speculative array accessors because the

00:16:44,060 --> 00:16:49,250
system call is dispatched in an indirect

00:16:46,010 --> 00:16:51,460
array access all right when it comes to

00:16:49,250 --> 00:16:53,810
the point where we have an indirect jump

00:16:51,460 --> 00:16:56,360
like the function pointer or something

00:16:53,810 --> 00:16:59,180
like this we have to apply the return

00:16:56,360 --> 00:17:02,290
trampoline mitigations and at some other

00:16:59,180 --> 00:17:05,030
points in time we also might want to use

00:17:02,290 --> 00:17:07,780
the microcode extensions the new

00:17:05,030 --> 00:17:09,680
instructions of our systems for

00:17:07,780 --> 00:17:12,640
speculative

00:17:09,680 --> 00:17:15,500
yeah protection diversion to protections

00:17:12,640 --> 00:17:20,150
so we have certain points that might

00:17:15,500 --> 00:17:22,040
affect the behavior of the latencies the

00:17:20,150 --> 00:17:24,199
second test where we put additional load

00:17:22,040 --> 00:17:26,000
on those systems we have a second user

00:17:24,199 --> 00:17:29,270
space process running with a lower

00:17:26,000 --> 00:17:32,590
priority than cyclic test again cyclic

00:17:29,270 --> 00:17:35,450
test wants to sleep for one millisecond

00:17:32,590 --> 00:17:37,010
cause the kernel to go to sleep the

00:17:35,450 --> 00:17:39,650
kernel sees oh okay I have another

00:17:37,010 --> 00:17:41,570
process that could now that I could now

00:17:39,650 --> 00:17:43,250
activate it schedules to the stress and

00:17:41,570 --> 00:17:45,050
Chi stress and Shera

00:17:43,250 --> 00:17:47,210
we do not know what tres and she

00:17:45,050 --> 00:17:49,790
actually wants to do on our system so

00:17:47,210 --> 00:17:51,800
there might be further system calls in

00:17:49,790 --> 00:17:53,750
between we do not exactly know what it

00:17:51,800 --> 00:17:56,840
does but when the interrupt occurs

00:17:53,750 --> 00:18:01,520
stress and Chi will be preempted again

00:17:56,840 --> 00:18:04,310
and the other kernel will schedule back

00:18:01,520 --> 00:18:08,710
to our cyclic test so now we have more

00:18:04,310 --> 00:18:11,390
mitigation points especially on every

00:18:08,710 --> 00:18:13,100
switch of stress and Chi down to the

00:18:11,390 --> 00:18:15,680
kernel space we have an additional page

00:18:13,100 --> 00:18:18,290
table isolation call and let's say we

00:18:15,680 --> 00:18:20,510
are in the middle of the page table

00:18:18,290 --> 00:18:22,160
isolation when an interrupt occurs when

00:18:20,510 --> 00:18:24,230
the interrupt occurs then we have to

00:18:22,160 --> 00:18:27,980
finish the page table isolation before

00:18:24,230 --> 00:18:30,500
we can we can handle the interrupt so

00:18:27,980 --> 00:18:32,300
this adduce --is this this introduces

00:18:30,500 --> 00:18:35,840
additional overhead and it's relevant

00:18:32,300 --> 00:18:37,610
for the titter of the system so this is

00:18:35,840 --> 00:18:40,400
take the explanation of the measurements

00:18:37,610 --> 00:18:47,920
that that we were doing and our target

00:18:40,400 --> 00:18:47,920
systems are a intro x86 64 system

00:18:48,340 --> 00:18:56,300
service system accion a5 processor

00:18:50,720 --> 00:19:00,020
fourth generation an AMD x86 system

00:18:56,300 --> 00:19:04,730
middle end system rise in 2700 x and

00:19:00,020 --> 00:19:07,550
forearm 64 we did did those measurements

00:19:04,730 --> 00:19:11,990
on an Nvidia Chetan TX 1 which is

00:19:07,550 --> 00:19:17,570
equipped with 4 cortex a 50s 5 a 57

00:19:11,990 --> 00:19:20,810
course so first of all we somehow had to

00:19:17,570 --> 00:19:23,180
get our systems under control and this

00:19:20,810 --> 00:19:25,130
took more time than expected because we

00:19:23,180 --> 00:19:28,370
had undesired highlight and C's on our

00:19:25,130 --> 00:19:32,600
systems there were many many tweaks and

00:19:28,370 --> 00:19:36,170
Tunes that yeah we had to we had to do

00:19:32,600 --> 00:19:38,540
through to run to have a system where we

00:19:36,170 --> 00:19:41,660
can actually run our mitigations on so

00:19:38,540 --> 00:19:44,540
this was a lot of work but it was

00:19:41,660 --> 00:19:47,360
nothing nothing special you can look at

00:19:44,540 --> 00:19:48,800
this wiki page you want to know on if

00:19:47,360 --> 00:19:52,420
you if you want to know how you can

00:19:48,800 --> 00:19:56,530
strengthen your system to become more

00:19:52,420 --> 00:19:59,470
yeah or to become real-time capable

00:19:56,530 --> 00:20:06,310
in addition to that we also wanted to

00:19:59,470 --> 00:20:08,110
reduce the noise of of the system so we

00:20:06,310 --> 00:20:09,700
want to get rid of interrupts on the

00:20:08,110 --> 00:20:11,260
course that we are measuring because we

00:20:09,700 --> 00:20:13,210
want to measure the effects of the

00:20:11,260 --> 00:20:16,150
mitigation and we do not want to measure

00:20:13,210 --> 00:20:19,450
the system as such so we isolated the

00:20:16,150 --> 00:20:23,170
CPUs where we run our measurements on we

00:20:19,450 --> 00:20:25,870
rebound the SMP iraq' affinity of some i

00:20:23,170 --> 00:20:27,670
recuse to the non real-time CPUs we

00:20:25,870 --> 00:20:30,010
rebound net device queue affinities to

00:20:27,670 --> 00:20:34,950
non real-time CPUs we disabled machine

00:20:30,010 --> 00:20:38,800
check pause in some other tweaks we've

00:20:34,950 --> 00:20:41,260
realized that if we are running on on

00:20:38,800 --> 00:20:43,510
separate Numa cores that we had that we

00:20:41,260 --> 00:20:46,030
also have some undesired high latency so

00:20:43,510 --> 00:20:48,490
we decided not to leave Numa course if

00:20:46,030 --> 00:20:52,450
we have what more than one socket on our

00:20:48,490 --> 00:20:54,490
system turns out that the task set

00:20:52,450 --> 00:20:56,470
parsing in cyclic test currently is

00:20:54,490 --> 00:20:59,080
broken this is already reported upstream

00:20:56,470 --> 00:21:03,010
so the first few days we measured on

00:20:59,080 --> 00:21:04,420
random CPUs on in contrast to the CPUs

00:21:03,010 --> 00:21:07,060
that we actually wanted to run our

00:21:04,420 --> 00:21:09,220
measurement on so we had some box tear

00:21:07,060 --> 00:21:12,250
then on stress and she we had some

00:21:09,220 --> 00:21:14,880
problem on on the behavior of stress and

00:21:12,250 --> 00:21:17,830
she together with the FIFO scheduler

00:21:14,880 --> 00:21:20,980
alarms and real-time this is somehow

00:21:17,830 --> 00:21:24,670
broken because alarm timers will never

00:21:20,980 --> 00:21:26,650
be propagated to the user space if the

00:21:24,670 --> 00:21:28,450
CPU is 100 percent on the load because

00:21:26,650 --> 00:21:30,550
the run on the software acute context

00:21:28,450 --> 00:21:34,440
these are much details but we are

00:21:30,550 --> 00:21:40,300
currently working on on these issues and

00:21:34,440 --> 00:21:46,180
we have workarounds to run our

00:21:40,300 --> 00:21:49,780
measurements though yeah and the fun

00:21:46,180 --> 00:21:52,930
didn't end so we had more issues to deal

00:21:49,780 --> 00:21:56,620
with for the different medications we

00:21:52,930 --> 00:21:58,420
had to exchange the microcode on x86 so

00:21:56,620 --> 00:22:01,090
if we want to run the no protection

00:21:58,420 --> 00:22:03,520
variant then we had to use old micro

00:22:01,090 --> 00:22:05,230
code whereas if you want to apply the

00:22:03,520 --> 00:22:06,429
Varenne to mitigations then we had to

00:22:05,230 --> 00:22:09,010
switch the macro code to newer versions

00:22:06,429 --> 00:22:12,390
so a lot of reboots a lot of

00:22:09,010 --> 00:22:16,870
research which micro coats of orion's

00:22:12,390 --> 00:22:19,630
actually do what thing under Chetan tx1

00:22:16,870 --> 00:22:23,620
on the aim on the arm 64 platform we

00:22:19,630 --> 00:22:26,680
were missing official a support of the

00:22:23,620 --> 00:22:30,130
arm trusted film there which implements

00:22:26,680 --> 00:22:33,850
the the SMC CC version 1.1 interface so

00:22:30,130 --> 00:22:35,620
we had to build it on our own compiling

00:22:33,850 --> 00:22:38,110
was easy but getting it running on the

00:22:35,620 --> 00:22:39,790
target was not that easy we have

00:22:38,110 --> 00:22:41,290
different kernel of orion's for the

00:22:39,790 --> 00:22:42,790
different measurements for instance if

00:22:41,290 --> 00:22:46,750
you have no protection and we want to

00:22:42,790 --> 00:22:49,690
deactivate the return trample line and

00:22:46,750 --> 00:22:51,250
so on we also had some issues with our

00:22:49,690 --> 00:22:53,190
virtualized environment with the

00:22:51,250 --> 00:22:58,000
trailers hypervisor we had numerous

00:22:53,190 --> 00:23:02,200
traps that we didn't see before that

00:22:58,000 --> 00:23:05,740
were because of high highly parallelized

00:23:02,200 --> 00:23:09,190
loads on many CPUs on arm 64 we were

00:23:05,740 --> 00:23:11,980
actually missing a proper spectrum Orien

00:23:09,190 --> 00:23:13,750
to mitigations on our hypervisor we have

00:23:11,980 --> 00:23:16,780
a proof of concepts implementation

00:23:13,750 --> 00:23:20,920
already running proper pictures to be

00:23:16,780 --> 00:23:22,720
main light or in work currently so we

00:23:20,920 --> 00:23:24,940
had obstacles in the whole system stack

00:23:22,720 --> 00:23:28,120
from box in the user space to

00:23:24,940 --> 00:23:29,710
misbehavior in kernel space issues in

00:23:28,120 --> 00:23:32,530
the hypervisor and also in the film

00:23:29,710 --> 00:23:38,790
where obstacles in the whole system

00:23:32,530 --> 00:23:43,510
stick to the results of our measurements

00:23:38,790 --> 00:23:47,560
let's look at the xeon e5 system so on

00:23:43,510 --> 00:23:50,290
the left side of this plot you see the

00:23:47,560 --> 00:23:52,510
behavior of the system when it is not

00:23:50,290 --> 00:23:56,880
under load so the no stress of Orion on

00:23:52,510 --> 00:23:59,970
the right side you can see the result of

00:23:56,880 --> 00:24:04,210
a cyclic test when the system is under

00:23:59,970 --> 00:24:05,950
additional load on the x-axis you can

00:24:04,210 --> 00:24:08,470
see the latency in microsecond that

00:24:05,950 --> 00:24:10,540
occurred and on the y-axis you can see

00:24:08,470 --> 00:24:12,900
the number of occurrences where this

00:24:10,540 --> 00:24:16,840
specific or where this particular

00:24:12,900 --> 00:24:20,410
latency actually occurred the vertical

00:24:16,840 --> 00:24:21,749
line denotes the maximum latency that

00:24:20,410 --> 00:24:26,759
occurred

00:24:21,749 --> 00:24:28,739
in this measurement so without load we

00:24:26,759 --> 00:24:30,929
are somewhere under five microseconds

00:24:28,739 --> 00:24:33,149
and if the system is under stress then

00:24:30,929 --> 00:24:35,609
we are somewhere under 30 microseconds

00:24:33,149 --> 00:24:38,929
which is pretty okay for such a systems

00:24:35,609 --> 00:24:43,139
so now what happens if we turn on all

00:24:38,929 --> 00:24:45,389
mitigations for this system our

00:24:43,139 --> 00:24:46,919
mitigations for all medications that are

00:24:45,389 --> 00:24:48,719
required for the system means we

00:24:46,919 --> 00:24:50,700
especially activate spectra of orion

00:24:48,719 --> 00:24:53,669
mitigation again spectra of Orion - and

00:24:50,700 --> 00:24:58,709
against meltdown and then this graph

00:24:53,669 --> 00:25:00,929
looks like this so it's almost or it

00:24:58,709 --> 00:25:02,909
looks like this would be a constant

00:25:00,929 --> 00:25:05,579
shift to the right we will later see

00:25:02,909 --> 00:25:08,879
that this is not the whole truth but

00:25:05,579 --> 00:25:11,820
yeah it looks consistent so we do have

00:25:08,879 --> 00:25:14,099
additional overhead doesn't matter if

00:25:11,820 --> 00:25:16,169
you have no protector if you have the no

00:25:14,099 --> 00:25:18,839
stress case or distress case we

00:25:16,169 --> 00:25:21,479
introduce additional latency on the

00:25:18,839 --> 00:25:24,269
system so now what does it look like if

00:25:21,479 --> 00:25:26,879
we only activate the page table

00:25:24,269 --> 00:25:31,079
isolation medication without spectra of

00:25:26,879 --> 00:25:33,179
Orion - the results are pretty

00:25:31,079 --> 00:25:35,039
consistent we are somewhere between no

00:25:33,179 --> 00:25:39,329
protection and default protection so if

00:25:35,039 --> 00:25:42,440
we activate PTI only we are somewhere in

00:25:39,329 --> 00:25:47,009
between turns out that at least for our

00:25:42,440 --> 00:25:49,109
workload that were Ryan - only the

00:25:47,009 --> 00:25:53,339
variety - mitigation of spectra costs

00:25:49,109 --> 00:25:56,489
more than the page stabilization yeah

00:25:53,339 --> 00:25:59,099
but can we be sure can we really trust

00:25:56,489 --> 00:26:03,119
these results and this is where we need

00:25:59,099 --> 00:26:05,940
proper statistic analysis and where I

00:26:03,119 --> 00:26:09,179
would like to hand over to morph Khan

00:26:05,940 --> 00:26:10,799
yeah that's true so we were wondering I

00:26:09,179 --> 00:26:13,169
mean I can ascertain you that arrive

00:26:10,799 --> 00:26:14,759
after doing all these measurements after

00:26:13,169 --> 00:26:18,749
doing all these complicated setups

00:26:14,759 --> 00:26:21,149
really started looking like Doc Brown is

00:26:18,749 --> 00:26:23,429
doc brown right in the third picture so

00:26:21,149 --> 00:26:24,899
we decided to take some of the load of

00:26:23,429 --> 00:26:27,059
his shoulders that's where the two of us

00:26:24,899 --> 00:26:29,429
come in and yeah the question is so what

00:26:27,059 --> 00:26:31,889
do the data actually tell us can we

00:26:29,429 --> 00:26:33,860
learn more about the system behavior

00:26:31,889 --> 00:26:35,870
than just the

00:26:33,860 --> 00:26:38,630
maximum latency in the distribution and

00:26:35,870 --> 00:26:40,730
the second question of course is can you

00:26:38,630 --> 00:26:43,280
trust our measurements can we trust our

00:26:40,730 --> 00:26:45,559
measurements that's where the statistics

00:26:43,280 --> 00:26:47,510
come in but despair knots I'm only going

00:26:45,559 --> 00:26:49,760
to take a few minutes to convince you

00:26:47,510 --> 00:26:54,380
that this is right without going into

00:26:49,760 --> 00:26:56,059
the details too far so in a traditional

00:26:54,380 --> 00:26:57,500
real time system in a traditional hard

00:26:56,059 --> 00:26:59,540
real-time system you'd be mostly

00:26:57,500 --> 00:27:03,650
concerned about the maximum latencies

00:26:59,540 --> 00:27:05,299
because if you exceed these maximum

00:27:03,650 --> 00:27:08,240
latencies people made the I if you're in

00:27:05,299 --> 00:27:10,179
a safety critical system for preempt RT

00:27:08,240 --> 00:27:14,120
the kernel that we've been looking at

00:27:10,179 --> 00:27:15,890
the situation is usually not that bad so

00:27:14,120 --> 00:27:17,780
people use it for audio processing for

00:27:15,890 --> 00:27:20,210
video processing and you may be annoyed

00:27:17,780 --> 00:27:21,799
if you maybe skip a frame or if you hear

00:27:20,210 --> 00:27:24,830
a glitch in your audio recording but

00:27:21,799 --> 00:27:26,480
you're most likely not going to die so

00:27:24,830 --> 00:27:28,540
it's not just the maximum of the extreme

00:27:26,480 --> 00:27:32,090
values that are important but also the

00:27:28,540 --> 00:27:35,150
behavior of the distribution and by

00:27:32,090 --> 00:27:38,120
precisely checking the behavior of the

00:27:35,150 --> 00:27:40,070
distributions we can actually also

00:27:38,120 --> 00:27:41,929
handle the second question namely can

00:27:40,070 --> 00:27:44,179
you trust our measurements and to do

00:27:41,929 --> 00:27:46,010
that that's the the answer to if you can

00:27:44,179 --> 00:27:48,740
trust us or not and is in these graphs

00:27:46,010 --> 00:27:51,440
what we did is use the so called shift

00:27:48,740 --> 00:27:54,770
function method we basically would like

00:27:51,440 --> 00:27:57,679
to learn to the distributions differ by

00:27:54,770 --> 00:27:59,809
a constant shift is the spread

00:27:57,679 --> 00:28:02,660
difference do they differ in in more

00:27:59,809 --> 00:28:04,940
properties and to learn about that we

00:28:02,660 --> 00:28:07,460
split up the values into different

00:28:04,940 --> 00:28:09,410
portions Dessel so we take the first 10%

00:28:07,460 --> 00:28:11,450
of the values and compare these values

00:28:09,410 --> 00:28:13,130
with each other that's one point in

00:28:11,450 --> 00:28:14,750
these diagrams we take the next 10

00:28:13,130 --> 00:28:17,630
percent the next 10 percent and so on

00:28:14,750 --> 00:28:20,840
and compare these portions from A to C

00:28:17,630 --> 00:28:22,760
we have an increasing amount of measured

00:28:20,840 --> 00:28:25,040
data points what you see from these

00:28:22,760 --> 00:28:27,320
graphs is that actually the did the

00:28:25,040 --> 00:28:30,020
difference is between the case with all

00:28:27,320 --> 00:28:32,120
mitigations on in no mitigations is not

00:28:30,020 --> 00:28:33,980
just shifted by a constant amount by a

00:28:32,120 --> 00:28:36,080
constant increase in latency but the

00:28:33,980 --> 00:28:40,010
latency increase since we have an upward

00:28:36,080 --> 00:28:41,450
slope gets worse and worse the larger

00:28:40,010 --> 00:28:43,760
the latencies become so if you're

00:28:41,450 --> 00:28:46,300
already in trouble then you're even more

00:28:43,760 --> 00:28:47,480
in trouble with the premium with the

00:28:46,300 --> 00:28:51,050
respect

00:28:47,480 --> 00:28:53,210
mitigations now since we compute the

00:28:51,050 --> 00:28:55,970
comparison between the portions of the

00:28:53,210 --> 00:28:58,070
distributions we can use proper

00:28:55,970 --> 00:29:00,980
statistics to do a so called bootstrap

00:28:58,070 --> 00:29:02,870
resampling to estimate confidence

00:29:00,980 --> 00:29:05,890
intervals I'm not going to discuss in

00:29:02,870 --> 00:29:09,560
detail what a confidence interval says

00:29:05,890 --> 00:29:12,500
but that's indicated by the bars by the

00:29:09,560 --> 00:29:14,960
vertical bars and very roughly speaking

00:29:12,500 --> 00:29:17,600
the longer the length of the vertical

00:29:14,960 --> 00:29:20,180
bars bar tells you something about the

00:29:17,600 --> 00:29:22,070
Equality of the estimation procedures or

00:29:20,180 --> 00:29:24,350
how well we learned the true difference

00:29:22,070 --> 00:29:26,900
between the distributions you see the

00:29:24,350 --> 00:29:28,970
quality is not so good in case a when we

00:29:26,900 --> 00:29:30,530
don't measure sufficient data points but

00:29:28,970 --> 00:29:33,350
curves becomes better and better and

00:29:30,530 --> 00:29:35,630
better until V if we looked at the final

00:29:33,350 --> 00:29:38,150
data then these bars would disappear

00:29:35,630 --> 00:29:40,880
because we have very precise results

00:29:38,150 --> 00:29:42,970
that you really can rely upon that's

00:29:40,880 --> 00:29:45,500
about you can statistically rely upon

00:29:42,970 --> 00:29:47,630
I'm not going to bore you with further

00:29:45,500 --> 00:29:51,020
details so we can do that for the other

00:29:47,630 --> 00:29:53,270
measurements as well same result and if

00:29:51,020 --> 00:29:55,340
you want the key message to take away

00:29:53,270 --> 00:29:57,050
with you is a bayesian people would

00:29:55,340 --> 00:29:58,940
maybe talk about credibility but I'm

00:29:57,050 --> 00:30:00,830
repeating Alive's words what we're

00:29:58,940 --> 00:30:03,290
telling you here is the truth the truth

00:30:00,830 --> 00:30:09,740
and nothing but the truth and statistics

00:30:03,290 --> 00:30:11,660
shows we right thank you for coming ok

00:30:09,740 --> 00:30:17,930
let's continue with the AMD system with

00:30:11,660 --> 00:30:20,390
horizon 2700 X is the distribution of

00:30:17,930 --> 00:30:24,140
the latency is pretty similar to the to

00:30:20,390 --> 00:30:27,140
the Intel processor in this graph you

00:30:24,140 --> 00:30:29,810
again see the distribution of the

00:30:27,140 --> 00:30:31,820
latency of the latencies with no

00:30:29,810 --> 00:30:34,730
protection activated no protection

00:30:31,820 --> 00:30:36,800
between means there are no protection in

00:30:34,730 --> 00:30:42,170
default protection in the AMD case means

00:30:36,800 --> 00:30:44,210
we have a new microcode version 4 for

00:30:42,170 --> 00:30:46,400
the AMD processor and the specter of

00:30:44,210 --> 00:30:48,590
Orion to mitigation activated we do not

00:30:46,400 --> 00:30:51,110
need to protect against meltdown because

00:30:48,590 --> 00:30:53,780
the system is not affected by data so if

00:30:51,110 --> 00:30:56,030
we again activate the default mitigation

00:30:53,780 --> 00:31:00,380
we had we have the same consistent

00:30:56,030 --> 00:31:02,090
picture as for the Intel processor but

00:31:00,380 --> 00:31:04,430
it doesn't

00:31:02,090 --> 00:31:06,680
the overall behavior of the system or of

00:31:04,430 --> 00:31:10,490
the workload that we are measuring that

00:31:06,680 --> 00:31:14,270
much okay let's have a look at the

00:31:10,490 --> 00:31:17,480
Nvidia Chetan tx1 the core-tex a57

00:31:14,270 --> 00:31:21,980
system again these are the results

00:31:17,480 --> 00:31:23,840
without any protection enabled let's

00:31:21,980 --> 00:31:25,700
activate the default protection the

00:31:23,840 --> 00:31:28,570
fraud protection means we have an update

00:31:25,700 --> 00:31:32,480
of the untrusted film where together

00:31:28,570 --> 00:31:35,780
with yeah update off the untrusted film

00:31:32,480 --> 00:31:37,720
will be together with the medication

00:31:35,780 --> 00:31:43,250
enabled in kernel space so that it uses

00:31:37,720 --> 00:31:46,280
those those mitigation pause and this is

00:31:43,250 --> 00:31:49,310
what it looks like on the arm 64 we have

00:31:46,280 --> 00:31:52,040
a drop of light and sees somewhere here

00:31:49,310 --> 00:31:55,640
we do not exactly know what happens or

00:31:52,040 --> 00:31:59,810
what causes this this decrease of the

00:31:55,640 --> 00:32:04,490
number of occurrences in this window but

00:31:59,810 --> 00:32:08,300
we assume that in this window we we take

00:32:04,490 --> 00:32:13,240
the path to the system monitor which

00:32:08,300 --> 00:32:16,400
eliminates a certain range of latencies

00:32:13,240 --> 00:32:19,160
okay these were all the measurements

00:32:16,400 --> 00:32:22,730
without virtualized environments and now

00:32:19,160 --> 00:32:27,640
we will also have a look on how those

00:32:22,730 --> 00:32:27,640
systems behave when the trolley station

00:32:30,370 --> 00:32:35,480
yeah thank you

00:32:32,290 --> 00:32:37,190
so virtualization is considered for for

00:32:35,480 --> 00:32:38,930
two reasons first of all one as I've

00:32:37,190 --> 00:32:40,490
already mentioned if we are running in a

00:32:38,930 --> 00:32:43,070
virtual environment what does the impact

00:32:40,490 --> 00:32:45,650
what does virtualization does with it a

00:32:43,070 --> 00:32:47,780
second question we may wonder if we can

00:32:45,650 --> 00:32:51,560
use virtualization also as a mitigation

00:32:47,780 --> 00:32:54,200
possibly so first of all we picked one

00:32:51,560 --> 00:32:56,630
hypervisor which we knew best for this

00:32:54,200 --> 00:32:59,500
case which is not representing the whole

00:32:56,630 --> 00:33:02,300
ecosystem of course the hypervisor is

00:32:59,500 --> 00:33:04,070
specifically tiny and small one the

00:33:02,300 --> 00:33:06,440
separate talks about is just to remember

00:33:04,070 --> 00:33:08,660
this one is targeting static

00:33:06,440 --> 00:33:11,090
partitioning only it has no scheduling

00:33:08,660 --> 00:33:12,710
it just was one one resource assignment

00:33:11,090 --> 00:33:14,600
and it's specifically designed for hard

00:33:12,710 --> 00:33:15,200
real-time which is a good candidate in

00:33:14,600 --> 00:33:18,559
this case

00:33:15,200 --> 00:33:20,929
well it has some specific properties

00:33:18,559 --> 00:33:23,630
here of in general static partitioning

00:33:20,929 --> 00:33:26,120
allows to reduce a certain set of

00:33:23,630 --> 00:33:27,980
attacks or avoid them simply by by

00:33:26,120 --> 00:33:30,710
separating them via the virtualization

00:33:27,980 --> 00:33:33,860
via the partitioning approach that is a

00:33:30,710 --> 00:33:36,529
positive aspect furthermore jailhouse

00:33:33,860 --> 00:33:39,830
has some non-standard mitigation

00:33:36,529 --> 00:33:43,070
built-in against the known issues simply

00:33:39,830 --> 00:33:44,960
by avoiding what these issues what what

00:33:43,070 --> 00:33:47,899
the attacker usually exploits exploits

00:33:44,960 --> 00:33:50,840
the confused deputy scenario so that a

00:33:47,899 --> 00:33:53,179
kernel or a higher privileged part of

00:33:50,840 --> 00:33:55,820
the system is tricked to leak

00:33:53,179 --> 00:33:59,720
information about things that shouldn't

00:33:55,820 --> 00:34:02,480
be leaked that has been avoided in

00:33:59,720 --> 00:34:05,510
jailhouse by now it's a similar approach

00:34:02,480 --> 00:34:07,760
just simpler like hyper-v is doing as

00:34:05,510 --> 00:34:11,899
well so you can only what you cannot see

00:34:07,760 --> 00:34:13,579
simply talk and this of course also

00:34:11,899 --> 00:34:15,889
raises the question and can be used

00:34:13,579 --> 00:34:19,040
static partitioning instead of using

00:34:15,889 --> 00:34:23,389
these well not cheap mitigation measures

00:34:19,040 --> 00:34:26,569
also a question so first of all collect

00:34:23,389 --> 00:34:28,099
ground truth um so you see it above the

00:34:26,569 --> 00:34:30,319
bar you already seen it is basically the

00:34:28,099 --> 00:34:31,909
number on the on accion system and the

00:34:30,319 --> 00:34:34,819
multi-core system running our benchmark

00:34:31,909 --> 00:34:37,159
and below you see well virtualization

00:34:34,819 --> 00:34:39,050
surprise surprise isn't for free so you

00:34:37,159 --> 00:34:41,450
see an increase of the maximum latency

00:34:39,050 --> 00:34:43,760
of the same yeah

00:34:41,450 --> 00:34:47,179
real-time workload just running in a

00:34:43,760 --> 00:34:49,909
virtualized environment so you primarily

00:34:47,179 --> 00:34:52,609
see what you one aspect you see is the

00:34:49,909 --> 00:34:56,000
hardware induced virtualization overhead

00:34:52,609 --> 00:34:58,339
but this is just tiny it can be a few

00:34:56,000 --> 00:35:00,170
nanosecond Rodino seconds up to a few

00:34:58,339 --> 00:35:01,930
microseconds but the bigger part you see

00:35:00,170 --> 00:35:04,280
here basically is the unavoidable

00:35:01,930 --> 00:35:06,680
interception of virtualization of the

00:35:04,280 --> 00:35:08,420
multi-core workload so numbers may look

00:35:06,680 --> 00:35:10,400
better if you are not on the multi-core

00:35:08,420 --> 00:35:12,740
Rotom worked up as a single core

00:35:10,400 --> 00:35:17,089
workload but anyway this is what we got

00:35:12,740 --> 00:35:18,859
on this eight CPU setup now let's add

00:35:17,089 --> 00:35:21,280
the mitigation so the buff power you

00:35:18,859 --> 00:35:24,859
know already so there's an overhead and

00:35:21,280 --> 00:35:27,740
below you see now under the virtualized

00:35:24,859 --> 00:35:29,060
environment the mitigations on intel

00:35:27,740 --> 00:35:32,020
specifically both

00:35:29,060 --> 00:35:34,340
they scale so it become worse actually

00:35:32,020 --> 00:35:37,490
so the overhead virtualization is

00:35:34,340 --> 00:35:40,520
introducing is yeah magnifying so to say

00:35:37,490 --> 00:35:43,760
to slow down data mitigations yeah cause

00:35:40,520 --> 00:35:46,250
here in our system so just to confirm

00:35:43,760 --> 00:35:48,290
this we also looked at the arm system

00:35:46,250 --> 00:35:52,220
how it look like here well first of all

00:35:48,290 --> 00:35:54,680
even worse the primary reason here for

00:35:52,220 --> 00:35:56,780
this ground truth comparison here while

00:35:54,680 --> 00:36:00,740
it's so much higher under virtualized

00:35:56,780 --> 00:36:02,270
environment well on an intel on x86 info

00:36:00,740 --> 00:36:04,340
we can exploit direct interrupt

00:36:02,270 --> 00:36:06,440
injection into the guest system with

00:36:04,340 --> 00:36:08,420
this static partitioning approach we

00:36:06,440 --> 00:36:10,940
can't do this on arm system on this arm

00:36:08,420 --> 00:36:13,490
system specifically so the overhead of

00:36:10,940 --> 00:36:18,500
any kind of interrupts happening here is

00:36:13,490 --> 00:36:20,810
even higher on arm than it is on x86 so

00:36:18,500 --> 00:36:22,820
we have a significant increase without

00:36:20,810 --> 00:36:28,070
any of the mitigations applied between

00:36:22,820 --> 00:36:30,260
bare metal and virtualized this virtual

00:36:28,070 --> 00:36:32,600
is which mitigations enabled um yeah

00:36:30,260 --> 00:36:33,950
well we have an further increase of

00:36:32,600 --> 00:36:36,560
course so now the numbers are getting

00:36:33,950 --> 00:36:41,150
worse and you see here just like we saw

00:36:36,560 --> 00:36:44,650
before on x86 actually now there is a

00:36:41,150 --> 00:36:46,790
small or ASIS noticeable increase of the

00:36:44,650 --> 00:36:49,520
overhead the virtualization brings to

00:36:46,790 --> 00:36:51,440
the mitigations and the overall overhead

00:36:49,520 --> 00:36:54,320
we see here also of virtualization

00:36:51,440 --> 00:36:57,410
compared to a full mitigated bare metal

00:36:54,320 --> 00:37:00,260
system well this is worse then well so

00:36:57,410 --> 00:37:02,140
it's not really the solution in this

00:37:00,260 --> 00:37:06,610
case is not really jaqen virtualization

00:37:02,140 --> 00:37:11,630
simply it comes with a cost

00:37:06,610 --> 00:37:13,940
still virtualization partitioning also

00:37:11,630 --> 00:37:15,800
offers some kind of mitigation that we

00:37:13,940 --> 00:37:18,800
don't get from individual mitigation

00:37:15,800 --> 00:37:21,410
because there's a potential that by

00:37:18,800 --> 00:37:22,970
splitting the workload statically you

00:37:21,410 --> 00:37:24,590
can mitigate what we do not even know

00:37:22,970 --> 00:37:27,470
today so it may be a little bit of

00:37:24,590 --> 00:37:29,780
future proof news as well so this is a

00:37:27,470 --> 00:37:31,900
would be a benefit to consider and also

00:37:29,780 --> 00:37:34,910
for a real-time system furthermore

00:37:31,900 --> 00:37:36,710
real-time systems are also often systems

00:37:34,910 --> 00:37:40,150
which you do not like to touch in the

00:37:36,710 --> 00:37:42,800
field anymore and if you can use

00:37:40,150 --> 00:37:45,080
partitioning as a mechanism to split

00:37:42,800 --> 00:37:47,390
the real-time part from the potentially

00:37:45,080 --> 00:37:50,180
more affected non real-time part from

00:37:47,390 --> 00:37:51,410
the exposed real-time part you there's a

00:37:50,180 --> 00:37:54,350
potential that you can keep your

00:37:51,410 --> 00:37:56,870
real-time part unpatched unmodified and

00:37:54,350 --> 00:37:59,210
if you recall the the back parts of all

00:37:56,870 --> 00:38:01,670
these mitigations to older kernel they

00:37:59,210 --> 00:38:04,040
burned without some glitches and some

00:38:01,670 --> 00:38:05,870
efforts as well and you may not want to

00:38:04,040 --> 00:38:07,760
apply them on your running systems so

00:38:05,870 --> 00:38:10,730
this is a potential also an alternative

00:38:07,760 --> 00:38:12,470
pass this way what you have to keep in

00:38:10,730 --> 00:38:14,450
mind regarding virtualization as I said

00:38:12,470 --> 00:38:16,910
virtual jade house is a very lightweight

00:38:14,450 --> 00:38:20,210
approach here it's possibly the best

00:38:16,910 --> 00:38:23,330
case you get architectural II regarding

00:38:20,210 --> 00:38:25,280
the impact of the mitigations to be fair

00:38:23,330 --> 00:38:27,650
we have measured other hypervisors this

00:38:25,280 --> 00:38:30,140
is just arrived from design aspects but

00:38:27,650 --> 00:38:34,310
you can easily imagine there is no LT 1

00:38:30,140 --> 00:38:35,930
L 1 TF dynamic mitigation applied in

00:38:34,310 --> 00:38:37,820
jail hours by architectures not needed

00:38:35,930 --> 00:38:39,470
at the point where you have a hypervisor

00:38:37,820 --> 00:38:41,390
design which needs it because it does

00:38:39,470 --> 00:38:47,870
for example scheduling the impact will

00:38:41,390 --> 00:38:49,790
be much higher yeah thank you yan so

00:38:47,870 --> 00:38:52,580
let's come to the conclusion of this

00:38:49,790 --> 00:38:55,520
talk I want to I would like to repeat

00:38:52,580 --> 00:38:58,160
that the histograms that we showed you

00:38:55,520 --> 00:39:00,290
only investigated one specific workload

00:38:58,160 --> 00:39:03,350
and must not be generalized to any

00:39:00,290 --> 00:39:06,860
real-time workload but they should give

00:39:03,350 --> 00:39:08,450
you a tendencies a tendency of how your

00:39:06,860 --> 00:39:10,940
system might behave when those

00:39:08,450 --> 00:39:12,650
medications are applied so go out and

00:39:10,940 --> 00:39:14,680
measure your own workload your own

00:39:12,650 --> 00:39:17,570
real-time workload when you want to

00:39:14,680 --> 00:39:20,210
apply those mythic apply those

00:39:17,570 --> 00:39:22,400
mitigations and as I said at the

00:39:20,210 --> 00:39:25,280
beginning of this talk there's all those

00:39:22,400 --> 00:39:28,730
mitigations only have a local attack

00:39:25,280 --> 00:39:31,700
surface so usually you do not execute

00:39:28,730 --> 00:39:33,230
arbitrary code from untrusted parties on

00:39:31,700 --> 00:39:36,980
a real-time system on a hard real-time

00:39:33,230 --> 00:39:38,660
system so you have to you have to raise

00:39:36,980 --> 00:39:43,250
the question if you are actually

00:39:38,660 --> 00:39:46,250
affected by by these local vectors so

00:39:43,250 --> 00:39:49,460
unless no one is able to execute real

00:39:46,250 --> 00:39:51,740
work the arbitrary code on your system

00:39:49,460 --> 00:39:53,870
then you can also turn off those

00:39:51,740 --> 00:39:55,500
medications especially for instance if

00:39:53,870 --> 00:39:58,530
you have no network connection

00:39:55,500 --> 00:40:02,640
on your real-time system we still have

00:39:58,530 --> 00:40:05,220
some open issues as I said we have

00:40:02,640 --> 00:40:07,620
problems with the preempt RT alarm

00:40:05,220 --> 00:40:09,840
timers we have problems with cyclic test

00:40:07,620 --> 00:40:11,670
broken task sets and we are still

00:40:09,840 --> 00:40:18,120
working on proper Spector version 2 of

00:40:11,670 --> 00:40:20,670
around 2 mitigations for arm 64 so thank

00:40:18,120 --> 00:40:22,860
you very much for your attention and I

00:40:20,670 --> 00:40:28,830
think we still have a couple of minutes

00:40:22,860 --> 00:40:37,639
left to answer some questions thank you

00:40:28,830 --> 00:40:37,639
[Applause]

00:40:44,240 --> 00:40:50,640
did you measure the impact of the macro

00:40:47,190 --> 00:40:53,010
code only when you have the old macro

00:40:50,640 --> 00:40:57,030
code and the new macro code so the

00:40:53,010 --> 00:41:00,380
question is if I did measure the impact

00:40:57,030 --> 00:41:03,090
of the microcode only without applying

00:41:00,380 --> 00:41:06,210
the mitigation so the microcode bring

00:41:03,090 --> 00:41:09,000
new instructions we did not measure the

00:41:06,210 --> 00:41:11,010
microcode only but I would expect and

00:41:09,000 --> 00:41:12,720
now it's the question what does the

00:41:11,010 --> 00:41:14,340
microcode - that would be that would be

00:41:12,720 --> 00:41:16,710
an interesting measurement so actually I

00:41:14,340 --> 00:41:19,410
would say that it only makes some

00:41:16,710 --> 00:41:21,840
difference if you really execute those

00:41:19,410 --> 00:41:24,330
instructions but you would have to

00:41:21,840 --> 00:41:27,390
measure it to exclude that it doesn't do

00:41:24,330 --> 00:41:29,430
any other voodoo magic of course no we

00:41:27,390 --> 00:41:31,140
didn't do this measurement but with our

00:41:29,430 --> 00:41:34,580
measurement setup we could if you are

00:41:31,140 --> 00:41:34,580
interested in this we can repeat that

00:41:42,790 --> 00:41:46,300
further questions

00:41:57,950 --> 00:42:02,349
okay so again thank you very much

00:42:03,140 --> 00:42:08,460

YouTube URL: https://www.youtube.com/watch?v=nqU4j2M_Ul4


