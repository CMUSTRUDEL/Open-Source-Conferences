Title: Preempt-RT Latency Benchmarking of the Cortex-A53 Processor - Paul Thomas, AMSC
Publication date: 2018-10-25
Playlist: Open Source Summit Europe & ELC + OpenIoT Summit Europe 2018
Description: 
	Preempt-RT Latency Benchmarking of the Cortex-A53 Processor - Paul Thomas, AMSC

ARMv8 processors are becoming more common in the industrial market. It is useful to understand how well these processors perform with Preempt-RT. AMSC looked at the CortexA53 utilized in the ZYNQ UlataScale+ processors form Xilinx. The Cortex-A53 is an ARMv8 processor with an 8-stage in-order pipeline. The performance was compared with a Cortex-A9, a 10+ stage out-of-order pipeline. Even with the difference in pipeline design in these two processors, it is an interesting comparison because it represents the evolution of Xilinxâ€™s ZYNQ product line.

Three different latency tests were performed: standard cyclictest latency, external hardware interrupt latency (as utilized by the IIO subsystem) and Ethernet UDP latency. Analyzing the performance of a hardware interrupt can be tricky, the method of utilizing a timer with a capture function is detailed in the presentation. 

About Paul Thomas
Paul Thomas is the Lead Embedded Software Engineer at AMSC. Paul has over 15 years of experience working with embedded Linux. He has designed complex systems built on embedded Linux such as: the tracking system for large solar dishes and cellular based IoT connectivity solutions. At AMSC he leverages real-time embedded Linux to create distributed real-time control systems.
Captions: 
	00:00:00,030 --> 00:00:08,340
can you guys hear me okay great thanks

00:00:05,060 --> 00:00:11,849
well thank you all for coming today my

00:00:08,340 --> 00:00:14,880
name is Paul Thomas we'll be talking

00:00:11,849 --> 00:00:25,050
today about some latency benchmarking of

00:00:14,880 --> 00:00:30,320
the cortex a53 processor from arm so I'm

00:00:25,050 --> 00:00:33,059
from a MSC we're had founded in 1987

00:00:30,320 --> 00:00:36,809
headquartered near Boston Massachusetts

00:00:33,059 --> 00:00:38,879
in in err Massachusetts and we

00:00:36,809 --> 00:00:42,210
specialize in the design and manufacture

00:00:38,879 --> 00:00:45,270
of power systems and superconducting

00:00:42,210 --> 00:00:48,390
wire I don't get too involved in the

00:00:45,270 --> 00:00:56,730
superconducting wire part but it's it is

00:00:48,390 --> 00:00:58,140
very fascinating as we go along if you

00:00:56,730 --> 00:01:00,510
guys have questions I know it's hard

00:00:58,140 --> 00:01:03,780
with the microphones but feel free to

00:01:00,510 --> 00:01:06,990
raise your hand and interrupt and I'll

00:01:03,780 --> 00:01:07,860
try to repeat the question or get you a

00:01:06,990 --> 00:01:09,960
microphone quick

00:01:07,860 --> 00:01:11,610
I would rather if you have a question

00:01:09,960 --> 00:01:14,189
I'm sure other people have questions and

00:01:11,610 --> 00:01:16,920
so just just raise your hand interrupt

00:01:14,189 --> 00:01:22,159
and we'll talk about something for for

00:01:16,920 --> 00:01:22,159
longer so feel free to do that

00:01:29,210 --> 00:01:32,210
okay

00:01:33,850 --> 00:01:39,000
so today well I'll go through the

00:01:35,950 --> 00:01:43,060
software and hardware setup for these

00:01:39,000 --> 00:01:46,210
benchmarks then we'll go through some of

00:01:43,060 --> 00:01:50,380
the basic latency tests using the cyclic

00:01:46,210 --> 00:01:53,860
test tool it's a common a common tool to

00:01:50,380 --> 00:01:57,580
just test the standard latency but it

00:01:53,860 --> 00:02:00,150
doesn't have doesn't have any external

00:01:57,580 --> 00:02:04,360
interrupts and then we'll look at some

00:02:00,150 --> 00:02:08,230
ping pong latency tests between two

00:02:04,360 --> 00:02:10,800
separate boards going back and forth and

00:02:08,230 --> 00:02:14,470
then finally we'll look at a real world

00:02:10,800 --> 00:02:17,370
ADC interrupt latency actually a it's an

00:02:14,470 --> 00:02:20,560
80a analog to digital converter but

00:02:17,370 --> 00:02:22,960
ultimately the interrupt is a DMA

00:02:20,560 --> 00:02:37,240
complete interrupt so we'll take a bit a

00:02:22,960 --> 00:02:42,520
look at that later on okay so why

00:02:37,240 --> 00:02:44,590
real-time Linux stable and supported

00:02:42,520 --> 00:02:46,590
code base we we all know that that's why

00:02:44,590 --> 00:02:51,120
that's why we're here

00:02:46,590 --> 00:02:54,040
deep api's so oftentimes you're in the

00:02:51,120 --> 00:02:57,880
real-time context you're kind of trading

00:02:54,040 --> 00:03:02,020
off between the breadth and depth of the

00:02:57,880 --> 00:03:06,130
api's and the performance of the real

00:03:02,020 --> 00:03:08,290
time and with preempt RT you you still

00:03:06,130 --> 00:03:10,870
have good latency performance but you

00:03:08,290 --> 00:03:14,050
have all the the linux api's that we

00:03:10,870 --> 00:03:18,459
know and love just within one one system

00:03:14,050 --> 00:03:21,880
the I think some of you will be at the

00:03:18,459 --> 00:03:24,310
the real time summit on Thursday we'll

00:03:21,880 --> 00:03:26,920
get an update on the real-time Linux

00:03:24,310 --> 00:03:32,830
collaboration project which ultimately

00:03:26,920 --> 00:03:36,580
aims to mainline the preempt RT I'm not

00:03:32,830 --> 00:03:40,080
involved in that but looking at the

00:03:36,580 --> 00:03:40,080
performance of that

00:03:46,230 --> 00:03:55,140
okay so the cortex a53

00:03:50,130 --> 00:03:59,140
why why is that an important part we're

00:03:55,140 --> 00:04:03,700
interested in it because the zinc ultra

00:03:59,140 --> 00:04:08,590
scale plus parts use that arm core it's

00:04:03,700 --> 00:04:10,959
a 64-bit ARM core arm v8 and then

00:04:08,590 --> 00:04:13,320
there's lots of other parts that are

00:04:10,959 --> 00:04:18,010
based on this including the Raspberry Pi

00:04:13,320 --> 00:04:21,160
the model 3 the I just listed a couple

00:04:18,010 --> 00:04:24,990
on here the I guess it's in XP now used

00:04:21,160 --> 00:04:31,090
to be free scale has the I M X 8 and

00:04:24,990 --> 00:04:34,470
Odroid c2 it's a very common arm v8 core

00:04:31,090 --> 00:04:38,520
in embedded embedded marketplace you

00:04:34,470 --> 00:04:42,820
probably not gonna see too many cortex

00:04:38,520 --> 00:04:44,440
a53 s and earphones these days but maybe

00:04:42,820 --> 00:04:56,130
a few of the older phones would would

00:04:44,440 --> 00:04:59,590
happen on there so the hardware setup so

00:04:56,130 --> 00:05:05,110
this is comparing kind of two different

00:04:59,590 --> 00:05:06,760
boards one based on the arm cortex a9

00:05:05,110 --> 00:05:12,700
and the other based on the arm cortex

00:05:06,760 --> 00:05:16,440
a53 the actual module we're looking at

00:05:12,700 --> 00:05:23,460
is the encrust in cluster a-- mercury

00:05:16,440 --> 00:05:27,130
xu5 module it's based on that Xilinx SOC

00:05:23,460 --> 00:05:31,090
it's an eight stage pipeline 1.3

00:05:27,130 --> 00:05:35,010
gigahertz clock speed contrasted with

00:05:31,090 --> 00:05:39,540
the cortex a9 it's several years older

00:05:35,010 --> 00:05:47,229
the board for that test is the Zed board

00:05:39,540 --> 00:05:51,669
from from Abnett and others the and

00:05:47,229 --> 00:05:55,240
that's the original that's the core used

00:05:51,669 --> 00:05:57,760
in the original Zink parts from Xilinx

00:05:55,240 --> 00:05:59,260
so this is kind of the evolution of the

00:05:57,760 --> 00:06:01,840
Xilinx parts and so it's kind of

00:05:59,260 --> 00:06:04,240
interesting to look at the performance

00:06:01,840 --> 00:06:08,110
there one thing to note when we get to

00:06:04,240 --> 00:06:12,099
the UDP ping-pong test the Zed board

00:06:08,110 --> 00:06:16,479
does not have a second Ethernet port and

00:06:12,099 --> 00:06:19,479
so those tests were the the second

00:06:16,479 --> 00:06:21,340
Ethernet port could not be because it

00:06:19,479 --> 00:06:24,280
wasn't there it was just shared with the

00:06:21,340 --> 00:06:26,919
same Ethernet port that was used for SSH

00:06:24,280 --> 00:06:30,280
and the rest of the system and so I had

00:06:26,919 --> 00:06:36,460
other traffic on it besides the the UDP

00:06:30,280 --> 00:06:41,470
UDP traffic and the it's so as a tense

00:06:36,460 --> 00:06:44,410
10 plus stage pipeline and 666 megahertz

00:06:41,470 --> 00:06:48,400
so you you can see just from the specs

00:06:44,410 --> 00:06:59,650
that the the cortex a53 should be a much

00:06:48,400 --> 00:07:02,770
higher performance part okay the kernel

00:06:59,650 --> 00:07:09,460
so the starting point for both of these

00:07:02,770 --> 00:07:11,740
was 4.18 so it had the preempt the

00:07:09,460 --> 00:07:13,020
preempt RT patch came out shortly after

00:07:11,740 --> 00:07:17,620
that and so it's the very first one that

00:07:13,020 --> 00:07:22,210
our t1 Pat reply you can see the URL

00:07:17,620 --> 00:07:26,020
there and then for the zinc the zinc MP

00:07:22,210 --> 00:07:30,580
that also had this firmware and Hawke

00:07:26,020 --> 00:07:33,340
driver patch applied this is those eye

00:07:30,580 --> 00:07:36,370
links is actively trying to get all the

00:07:33,340 --> 00:07:38,409
new features up streamed and this and

00:07:36,370 --> 00:07:41,380
some of those features you know are just

00:07:38,409 --> 00:07:45,599
needed to run and so we had to pull I

00:07:41,380 --> 00:07:49,240
had to pull off a that patch and then

00:07:45,599 --> 00:07:51,039
beyond that there were there were a few

00:07:49,240 --> 00:07:53,620
more hurdles to get through to get the

00:07:51,039 --> 00:07:59,229
the latest kernel to run and then the

00:07:53,620 --> 00:08:01,690
zinc MP the you had to have the the

00:07:59,229 --> 00:08:04,039
lower-level firmware so the first stage

00:08:01,690 --> 00:08:07,639
bootloader

00:08:04,039 --> 00:08:10,279
the PMU firmware and the arm trusted

00:08:07,639 --> 00:08:12,589
firmware all had to be up to date and so

00:08:10,279 --> 00:08:14,119
there was a was a little bit of work

00:08:12,589 --> 00:08:18,740
getting that all up to date and then a

00:08:14,119 --> 00:08:21,559
few more things that the Xilinx guys

00:08:18,740 --> 00:08:27,469
helped with a little bit but that should

00:08:21,559 --> 00:08:32,050
all be as the patches are mainlined then

00:08:27,469 --> 00:08:35,000
future versions should be and as the

00:08:32,050 --> 00:08:41,229
standard the regular software releases

00:08:35,000 --> 00:08:47,810
from Xilinx come in the that should be a

00:08:41,229 --> 00:08:49,610
seamless process in the future okay so

00:08:47,810 --> 00:08:52,399
here's the first the first result so

00:08:49,610 --> 00:08:55,970
this is the basic cyclic test results so

00:08:52,399 --> 00:09:01,850
this you can see the cortex a9 had a

00:08:55,970 --> 00:09:06,430
maximum latency of 54 microseconds and

00:09:01,850 --> 00:09:09,500
the cortex a53 had a maximum latency of

00:09:06,430 --> 00:09:11,779
17 microseconds normally in the real

00:09:09,500 --> 00:09:14,540
time latency testing we're just

00:09:11,779 --> 00:09:16,880
concerned with the maximum I did put the

00:09:14,540 --> 00:09:20,649
the mote on here just as a kind of a

00:09:16,880 --> 00:09:23,420
point of reference for the the peak on

00:09:20,649 --> 00:09:24,800
yeah so you can see I put the mode on

00:09:23,420 --> 00:09:27,139
there just so you can see kind of where

00:09:24,800 --> 00:09:30,740
those Peaks are so those Peaks are

00:09:27,139 --> 00:09:38,029
coming in at 19 and 7 microseconds

00:09:30,740 --> 00:09:40,310
respectively this this is similar this

00:09:38,029 --> 00:09:45,709
54 microseconds is very similar to a

00:09:40,310 --> 00:09:47,899
kernel I ran several years ago for the

00:09:45,709 --> 00:09:53,389
the core cortex a9 but these are both

00:09:47,899 --> 00:09:56,630
with the four 4.18 kernel um and you can

00:09:53,389 --> 00:09:59,389
see there it's you know about three

00:09:56,630 --> 00:10:08,670
times the the cortex a53 is about three

00:09:59,389 --> 00:10:11,880
times three times faster or yeah so

00:10:08,670 --> 00:10:13,640
see so let's talk about the set up a

00:10:11,880 --> 00:10:20,010
little bit

00:10:13,640 --> 00:10:23,460
so for the set up CPU sets we're used so

00:10:20,010 --> 00:10:27,030
that's a way to shield specific cores

00:10:23,460 --> 00:10:30,830
and so the kernel portion of that is CPU

00:10:27,030 --> 00:10:35,120
sets the user space management is by a

00:10:30,830 --> 00:10:38,520
CPU CPU set and that's a Python

00:10:35,120 --> 00:10:40,680
application so it's an effective way to

00:10:38,520 --> 00:10:43,380
shield one or more cores from scheduling

00:10:40,680 --> 00:10:45,420
ordinary tasks so this is so if you have

00:10:43,380 --> 00:10:47,460
a real-time system you know the system

00:10:45,420 --> 00:10:50,190
has to do all of its normal things you

00:10:47,460 --> 00:10:53,760
know it has to have its ssh server it

00:10:50,190 --> 00:10:56,130
has to you know do all of its

00:10:53,760 --> 00:10:58,920
housekeeping stuff for the system but

00:10:56,130 --> 00:11:02,400
then if you want to one real-time tasks

00:10:58,920 --> 00:11:05,490
as well you may want to dedicate kind of

00:11:02,400 --> 00:11:08,010
one or more cores for that and to ensure

00:11:05,490 --> 00:11:10,200
that the scheduler isn't you know

00:11:08,010 --> 00:11:14,720
scheduling other things on those cores

00:11:10,200 --> 00:11:14,720
you can use these CPU sets

00:11:26,000 --> 00:11:33,470
so here's the test configurations so the

00:11:29,680 --> 00:11:37,910
we basically set up in a shielded way so

00:11:33,470 --> 00:11:43,040
CPU three was the user set and then the

00:11:37,910 --> 00:11:45,230
CPU 0 1 & 2 were the system set the for

00:11:43,040 --> 00:11:48,430
the cortex a9 it's only a dual-core so

00:11:45,230 --> 00:11:52,759
the system set was was just one core so

00:11:48,430 --> 00:11:58,420
you know just one less there and then so

00:11:52,759 --> 00:12:00,920
the actual loading so the system set had

00:11:58,420 --> 00:12:06,649
cyclic test with a priority of 98

00:12:00,920 --> 00:12:09,680
running and cyclic test with a priority

00:12:06,649 --> 00:12:15,009
of 99 and then a stress program and the

00:12:09,680 --> 00:12:18,620
stress it had eight CPU hogs and eight

00:12:15,009 --> 00:12:20,810
virtual memory threads and so those are

00:12:18,620 --> 00:12:24,500
trying to kind of be memory intensive

00:12:20,810 --> 00:12:28,040
and CPU intensive stress the the user

00:12:24,500 --> 00:12:30,920
set you could set this up different ways

00:12:28,040 --> 00:12:33,740
depending on what the real-world load

00:12:30,920 --> 00:12:36,170
was in this case I only gave it a

00:12:33,740 --> 00:12:40,759
loading of a lower priority cyclic test

00:12:36,170 --> 00:12:43,220
so so it had a cyclic test priority of

00:12:40,759 --> 00:12:46,870
98 and then and then the actual test

00:12:43,220 --> 00:12:46,870
itself which was you know the

00:12:56,670 --> 00:13:01,050
so the results the results from the

00:12:59,130 --> 00:13:07,370
graph are the cyclic test with a

00:13:01,050 --> 00:13:07,370
priority of 99 on the shielded CPU core

00:13:15,180 --> 00:13:29,959
and yeah you can see the actual stress

00:13:19,650 --> 00:13:34,980
command there okay

00:13:29,959 --> 00:13:39,029
then the UDP ping-pong test so this

00:13:34,980 --> 00:13:40,410
setup was so here you can see so this

00:13:39,029 --> 00:13:45,060
was run two different ways this was one

00:13:40,410 --> 00:13:47,040
run from the cortex a53 to the to

00:13:45,060 --> 00:13:48,420
another cortex a53 and that's the setup

00:13:47,040 --> 00:13:50,310
you see here and you can see this

00:13:48,420 --> 00:13:53,100
Ethernet cable here that connects them

00:13:50,310 --> 00:13:55,910
that was actually a separate Ethernet

00:13:53,100 --> 00:13:59,279
port from where all the ssh traffic was

00:13:55,910 --> 00:14:06,600
and so it only had the the UDP traffic

00:13:59,279 --> 00:14:07,889
of this ping-pong test on it and so yeah

00:14:06,600 --> 00:14:11,579
this is from the same board from the

00:14:07,889 --> 00:14:13,350
cortex a53 for the the Z board I I don't

00:14:11,579 --> 00:14:17,250
show that set up here but it's just with

00:14:13,350 --> 00:14:20,880
a single Ethernet cable I ended up not

00:14:17,250 --> 00:14:22,829
using the CPU sets because it was

00:14:20,880 --> 00:14:25,940
affecting the performance a little bit

00:14:22,829 --> 00:14:30,839
that that's something that's worth

00:14:25,940 --> 00:14:32,819
investigating I wonder if I had certain

00:14:30,839 --> 00:14:37,170
threat I wonder if there were K threads

00:14:32,819 --> 00:14:40,139
from the networking that were affecting

00:14:37,170 --> 00:14:42,720
how the isolation was working so I'm not

00:14:40,139 --> 00:14:48,810
I'm not exactly sure but I pulled off

00:14:42,720 --> 00:14:50,519
the the CPU set stuff but the but the

00:14:48,810 --> 00:14:52,170
loading the stress loading and the

00:14:50,519 --> 00:14:55,980
cyclic test loading was all still the

00:14:52,170 --> 00:14:58,980
same so it all had a it was the

00:14:55,980 --> 00:15:01,970
processors were loaded and there was a

00:14:58,980 --> 00:15:05,459
cyclic test running with a priority of

00:15:01,970 --> 00:15:08,730
997 in this case because the irq so the

00:15:05,459 --> 00:15:13,740
ethernet irq was given a priority of 99

00:15:08,730 --> 00:15:16,709
the the thread or the processors were

00:15:13,740 --> 00:15:18,889
given a priority of 98 and then the

00:15:16,709 --> 00:15:24,680
cyclic test loading just for background

00:15:18,889 --> 00:15:24,680
loading was given a priority of 97

00:15:28,600 --> 00:15:38,540
so you can see the performance here the

00:15:33,220 --> 00:15:41,540
the cortex a53 is still reasonably

00:15:38,540 --> 00:15:43,820
faster the 168 if you want to think of

00:15:41,540 --> 00:15:48,740
the one-way path you probably think of

00:15:43,820 --> 00:15:52,100
half about 80 84 I didn't put it in I

00:15:48,740 --> 00:15:55,459
put it in roundtrip just because when

00:15:52,100 --> 00:15:59,360
you're considering the the blue one here

00:15:55,459 --> 00:16:02,630
that has its 1/2 of the cortex a53 and

00:15:59,360 --> 00:16:05,600
1/2 of the cortex a9 so it's not it's

00:16:02,630 --> 00:16:07,010
not really half of that it's you could

00:16:05,600 --> 00:16:09,589
you could break it up like that but I

00:16:07,010 --> 00:16:14,209
just left it us the raw full round-trip

00:16:09,589 --> 00:16:17,260
for both tests and so this this is quite

00:16:14,209 --> 00:16:20,990
a bit longer than the cyclic test so it

00:16:17,260 --> 00:16:23,540
for the maximum of you know kind of 84

00:16:20,990 --> 00:16:27,410
there but it's still very very

00:16:23,540 --> 00:16:30,380
respectable the the cortex a9 you can

00:16:27,410 --> 00:16:33,800
see it it kind of has these stragglers

00:16:30,380 --> 00:16:36,709
and I didn't I didn't really have time

00:16:33,800 --> 00:16:40,370
to investigate what the what the cause

00:16:36,709 --> 00:16:43,040
of that was so that these go out above

00:16:40,370 --> 00:16:44,899
800 microseconds eight hundred

00:16:43,040 --> 00:16:48,200
microseconds was just the largest bin I

00:16:44,899 --> 00:16:54,290
I did for the for the latency capture

00:16:48,200 --> 00:16:56,540
and so I wonder if there isn't I wonder

00:16:54,290 --> 00:17:00,860
if there isn't something going wrong to

00:16:56,540 --> 00:17:02,839
have to have that continually go out

00:17:00,860 --> 00:17:05,900
really high so that that could be

00:17:02,839 --> 00:17:08,630
investigated because certainly the the

00:17:05,900 --> 00:17:11,660
cyclic tests performance by themselves

00:17:08,630 --> 00:17:13,490
even though it was higher than the a 53

00:17:11,660 --> 00:17:16,819
it was still bound you could kind of

00:17:13,490 --> 00:17:22,730
still see the the clear maximum at the

00:17:16,819 --> 00:17:31,630
top there yeah eight hundred

00:17:22,730 --> 00:17:31,630
microseconds was the largest bin okay

00:17:31,980 --> 00:17:39,030
so a real-world test so this is almost a

00:17:36,000 --> 00:17:40,680
real-world test so I'm show you the

00:17:39,030 --> 00:17:44,790
setup in a minute here but basically an

00:17:40,680 --> 00:17:46,800
analog-to-digital driver using the

00:17:44,790 --> 00:17:50,820
industrial i/o a subsystem of the kernel

00:17:46,800 --> 00:17:54,030
I think there's a talk later today from

00:17:50,820 --> 00:17:55,590
the maintainer of that so check that out

00:17:54,030 --> 00:18:00,090
if you're interested in the industrial

00:17:55,590 --> 00:18:03,810
IO subsystem this is the the test here

00:18:00,090 --> 00:18:06,930
is dm8 engine based and then the

00:18:03,810 --> 00:18:08,970
performance is captured using a hardware

00:18:06,930 --> 00:18:11,550
timer so I'll kind of go into how we

00:18:08,970 --> 00:18:19,080
actually capture the latency performance

00:18:11,550 --> 00:18:22,670
in a minute it's a little bit tricky so

00:18:19,080 --> 00:18:26,190
just a quick blurb on industrial i/o I

00:18:22,670 --> 00:18:30,630
how many people have used industrial IO

00:18:26,190 --> 00:18:34,290
and the in the kernel okay so maybe a

00:18:30,630 --> 00:18:38,600
third or so a quarter it's a extremely

00:18:34,290 --> 00:18:41,580
useful subsystem you can you can get

00:18:38,600 --> 00:18:45,570
there's many pre-existing drivers for

00:18:41,580 --> 00:18:48,360
analog to digital converters or digital

00:18:45,570 --> 00:18:52,160
to analog converters accelerometers a

00:18:48,360 --> 00:18:55,200
whole a whole slew of different devices

00:18:52,160 --> 00:18:58,710
it can provide text formatted

00:18:55,200 --> 00:19:00,720
measurements you know just cat a sis

00:18:58,710 --> 00:19:03,210
file and you can see the measurement or

00:19:00,720 --> 00:19:05,940
you can kind of do there's a ring

00:19:03,210 --> 00:19:08,880
buffered version where you can actually

00:19:05,940 --> 00:19:11,250
read in the raw data you know from a

00:19:08,880 --> 00:19:12,540
from a C program you don't you know if

00:19:11,250 --> 00:19:14,310
you if you're dealing with a lot of data

00:19:12,540 --> 00:19:19,470
you don't really want to go you know to

00:19:14,310 --> 00:19:23,120
a text string and then back to a back to

00:19:19,470 --> 00:19:23,120
a an integer or float

00:19:29,170 --> 00:19:38,420
okay so here's this test setup so in the

00:19:33,250 --> 00:19:40,670
the the zinc MP parts or the MP SOC

00:19:38,420 --> 00:19:42,740
parts they have both the programmable

00:19:40,670 --> 00:19:46,490
logic so they have an FPGA portion and

00:19:42,740 --> 00:19:49,120
they have the processing system so you

00:19:46,490 --> 00:19:52,550
have you have both of these pieces and

00:19:49,120 --> 00:19:55,160
the programmable logic you can do all

00:19:52,550 --> 00:19:56,510
sorts of things with it but one thing

00:19:55,160 --> 00:20:02,420
you can do with it is you can kind of

00:19:56,510 --> 00:20:05,720
decouple kind of very fast low-level

00:20:02,420 --> 00:20:08,240
housekeeping of in this case an SPI bus

00:20:05,720 --> 00:20:11,600
are actually a whole whole bunch of SPI

00:20:08,240 --> 00:20:14,660
buses and so you have a in the in this

00:20:11,600 --> 00:20:16,760
setup I didn't have external hardware so

00:20:14,660 --> 00:20:18,620
I was just using kind of a simulated

00:20:16,760 --> 00:20:22,010
Hardware in the programmable logic so

00:20:18,620 --> 00:20:25,130
you have a you have a simulated a-to-d

00:20:22,010 --> 00:20:28,520
converter and then that's through spi

00:20:25,130 --> 00:20:32,270
connected to this controller the A to D

00:20:28,520 --> 00:20:35,560
controller the a e to D controller has a

00:20:32,270 --> 00:20:39,680
SPI it kind of spits out an ACCI stream

00:20:35,560 --> 00:20:44,570
and that AXI stream connects straight to

00:20:39,680 --> 00:20:48,260
the the DMA block the DMA block so this

00:20:44,570 --> 00:20:51,560
DMA block has a corresponding dma engine

00:20:48,260 --> 00:20:55,820
driver in the Linux kernel so Xilinx has

00:20:51,560 --> 00:20:58,490
a DMA engine driver that can can

00:20:55,820 --> 00:21:04,880
directly connect to this DMA block

00:20:58,490 --> 00:21:05,840
that's in the programmable logic yeah

00:21:04,880 --> 00:21:08,030
that's it

00:21:05,840 --> 00:21:14,000
this path isn't really important but

00:21:08,030 --> 00:21:19,370
it's oh no that is the main data path it

00:21:14,000 --> 00:21:21,050
actually is also has a configuration it

00:21:19,370 --> 00:21:23,030
has a configuration section that's not

00:21:21,050 --> 00:21:26,270
not shown here and so when that

00:21:23,030 --> 00:21:28,460
interrupts so when the DMA is complete

00:21:26,270 --> 00:21:31,640
there's an interrupt that goes to the

00:21:28,460 --> 00:21:35,300
process the processing system and so I

00:21:31,640 --> 00:21:40,610
just parallel that signal into a timer

00:21:35,300 --> 00:21:42,730
capture block so the and so this this

00:21:40,610 --> 00:21:47,289
timer capture block

00:21:42,730 --> 00:21:51,230
it has a freerunning timer and then when

00:21:47,289 --> 00:21:53,750
when it sees that interrupts rising edge

00:21:51,230 --> 00:21:56,090
it just kind of saves off that

00:21:53,750 --> 00:21:58,700
freerunning timer for future use and so

00:21:56,090 --> 00:22:00,679
that's I'll go into that more in the

00:21:58,700 --> 00:22:04,909
next few slides but that's kind of how

00:22:00,679 --> 00:22:08,720
you can use the the hardware block to

00:22:04,909 --> 00:22:10,220
capture the latency of a external even

00:22:08,720 --> 00:22:12,740
though this is all within the chip this

00:22:10,220 --> 00:22:14,350
is kind of this is all external to Linux

00:22:12,740 --> 00:22:16,940
Linux doesn't really know anything about

00:22:14,350 --> 00:22:23,210
programmable logic so that's all kind of

00:22:16,940 --> 00:22:25,940
external system ok so then what's the

00:22:23,210 --> 00:22:33,200
latency there so the maximum latency was

00:22:25,940 --> 00:22:40,600
30 microseconds so not quite as good as

00:22:33,200 --> 00:22:43,669
the cyclic test one of was it 717

00:22:40,600 --> 00:22:54,590
microseconds are down in that range but

00:22:43,669 --> 00:22:55,850
still still still very good let me yeah

00:22:54,590 --> 00:23:01,850
so let me go into the some more detail

00:22:55,850 --> 00:23:04,309
on that timer capture function yeah so

00:23:01,850 --> 00:23:06,740
Hardware timers with a capture function

00:23:04,309 --> 00:23:10,700
are common in SOC s and microcontrollers

00:23:06,740 --> 00:23:12,679
I'm sure many of you have used you know

00:23:10,700 --> 00:23:15,980
often times timers will be used for like

00:23:12,679 --> 00:23:18,649
a PWM output but many times the same

00:23:15,980 --> 00:23:21,049
timer block and a microcontroller also

00:23:18,649 --> 00:23:24,159
can be used for this capture function so

00:23:21,049 --> 00:23:28,399
to very accurately essentially

00:23:24,159 --> 00:23:31,309
time-stamping an external event so upon

00:23:28,399 --> 00:23:33,080
upon the trigger that the present value

00:23:31,309 --> 00:23:35,029
of pre running timer is stored in the

00:23:33,080 --> 00:23:37,039
load register so all in all the

00:23:35,029 --> 00:23:39,470
different microcontroller vendors they

00:23:37,039 --> 00:23:41,740
all you know all the peripherals are

00:23:39,470 --> 00:23:44,210
implemented a tiny bit different and so

00:23:41,740 --> 00:23:47,450
you know just you have to look at the

00:23:44,210 --> 00:23:51,289
how how your specific timer is

00:23:47,450 --> 00:23:54,020
implemented and then you can kind of use

00:23:51,289 --> 00:23:58,970
it to do similar functions like this

00:23:54,020 --> 00:24:02,570
so in the so then in the kernel is our

00:23:58,970 --> 00:24:04,610
so the the dma engine has a callback so

00:24:02,570 --> 00:24:09,650
in the i io driver the industrial i/o

00:24:04,610 --> 00:24:12,410
driver when you set up the DMA you

00:24:09,650 --> 00:24:14,540
register the callback so the latency

00:24:12,410 --> 00:24:17,360
that we're measuring is from when the

00:24:14,540 --> 00:24:23,179
hardware interrupts happened to win were

00:24:17,360 --> 00:24:26,870
actually in the DMA callback within the

00:24:23,179 --> 00:24:28,400
IO driver so all the DMA stuff before

00:24:26,870 --> 00:24:30,650
that callback has already been taken

00:24:28,400 --> 00:24:33,050
care of so you could you could push it

00:24:30,650 --> 00:24:36,050
upstream a little bit and try to get the

00:24:33,050 --> 00:24:40,100
latency kind of when it first enters the

00:24:36,050 --> 00:24:43,300
kernel but this this test was for when

00:24:40,100 --> 00:24:49,190
you're in the the callback in the driver

00:24:43,300 --> 00:25:00,070
so there any questions on how that how

00:24:49,190 --> 00:25:00,070
that hardware capture works okay

00:25:01,929 --> 00:25:12,470
conclusions so the cortex a53 is a

00:25:07,100 --> 00:25:14,780
variable latency low latency core using

00:25:12,470 --> 00:25:19,280
the programmable logic to decouple the

00:25:14,780 --> 00:25:21,770
spi bus is very effective so oftentimes

00:25:19,280 --> 00:25:24,350
when you have an A to D converter like

00:25:21,770 --> 00:25:25,580
this you would have several different

00:25:24,350 --> 00:25:28,570
things going on

00:25:25,580 --> 00:25:32,809
normally you would you can still do a

00:25:28,570 --> 00:25:35,900
DMA transfer and you can the work that

00:25:32,809 --> 00:25:38,059
the kernel is doing is still relatively

00:25:35,900 --> 00:25:41,179
minimal but oftentimes what you have is

00:25:38,059 --> 00:25:43,600
if you have an external analog to

00:25:41,179 --> 00:25:45,920
digital converter chip it'll have a

00:25:43,600 --> 00:25:49,100
signal that says the conversions done

00:25:45,920 --> 00:25:51,920
and so you you could easily just use

00:25:49,100 --> 00:25:54,920
that conversion done signal you know you

00:25:51,920 --> 00:25:57,860
get that interrupts within you know say

00:25:54,920 --> 00:26:00,500
you're using the iio driver you get that

00:25:57,860 --> 00:26:02,600
interrupts within the IO driver and then

00:26:00,500 --> 00:26:05,929
you say okay the conversions done go

00:26:02,600 --> 00:26:11,240
grab go grab the transfer and

00:26:05,929 --> 00:26:14,720
then you you issue the DMA command I you

00:26:11,240 --> 00:26:17,659
mean for the SPI bus or some other buses

00:26:14,720 --> 00:26:20,740
you can still use the DMA and then when

00:26:17,659 --> 00:26:23,629
that DMA is ultimately finished you can

00:26:20,740 --> 00:26:26,299
get that interrupt within the driver and

00:26:23,629 --> 00:26:28,700
actually you know push it push the data

00:26:26,299 --> 00:26:30,860
to the ring buffers but you're dealing

00:26:28,700 --> 00:26:35,330
with at least two interrupts there when

00:26:30,860 --> 00:26:37,730
what what you really want is you you

00:26:35,330 --> 00:26:39,710
just want the single interrupt when

00:26:37,730 --> 00:26:43,159
everything's all transferred and so you

00:26:39,710 --> 00:26:45,679
can you can parallel a whole bunch of

00:26:43,159 --> 00:26:48,740
different converters and you can have

00:26:45,679 --> 00:26:51,139
the programmable logic deal with all the

00:26:48,740 --> 00:26:54,679
low-level stuff and then you can just

00:26:51,139 --> 00:26:56,149
let Linux you kind of notify Linux when

00:26:54,679 --> 00:26:58,999
everything's done and transferred to

00:26:56,149 --> 00:27:07,490
memory and it's very effective to to

00:26:58,999 --> 00:27:12,619
have minimal latencies doing that future

00:27:07,490 --> 00:27:17,200
work so investigate the UDP path latency

00:27:12,619 --> 00:27:21,619
so those those seem very high and i i

00:27:17,200 --> 00:27:27,470
guessing that is a lot to do with how

00:27:21,619 --> 00:27:31,100
much processing the the tcp/ip stack

00:27:27,470 --> 00:27:35,080
within the kernel is doing and so there

00:27:31,100 --> 00:27:38,950
may be there's ways that we can either

00:27:35,080 --> 00:27:42,200
bypass that stack or speed up that stack

00:27:38,950 --> 00:27:47,450
i think we heard jonathan talked about

00:27:42,200 --> 00:27:49,480
the AF x DP on monday so that's one

00:27:47,450 --> 00:27:49,480
option

00:27:50,649 --> 00:27:55,220
yeah and investigate the difference

00:27:53,119 --> 00:27:56,869
between the cyclic test and a DC driver

00:27:55,220 --> 00:28:00,740
results so that we did see a little bit

00:27:56,869 --> 00:28:02,690
of a maximum so those are you know

00:28:00,740 --> 00:28:05,539
slightly different interrupt paths but

00:28:02,690 --> 00:28:07,159
they had a different maximum time so it

00:28:05,539 --> 00:28:08,779
might be interesting to kind of dig a

00:28:07,159 --> 00:28:13,450
little deeper there and see what's

00:28:08,779 --> 00:28:16,820
what's causing those differences track

00:28:13,450 --> 00:28:20,120
preempt RT across kernel versions so

00:28:16,820 --> 00:28:22,700
this is only one snapshot and

00:28:20,120 --> 00:28:25,430
and so I think it would be interesting

00:28:22,700 --> 00:28:28,700
for you know specific set up to kind of

00:28:25,430 --> 00:28:38,320
track that as the kernel versions move

00:28:28,700 --> 00:28:41,780
along let's see so just quick thanks

00:28:38,320 --> 00:28:45,190
Raja and Bahji from Xilinx he helped us

00:28:41,780 --> 00:28:48,830
get up and running with the 418 kernel

00:28:45,190 --> 00:28:51,940
the board's from in Costra and then I

00:28:48,830 --> 00:29:04,340
want to thank my wife and family so

00:28:51,940 --> 00:29:06,730
that's that's all I have so are there

00:29:04,340 --> 00:29:06,730
questions

00:29:18,520 --> 00:29:23,540
I'm so been working with a mainline

00:29:21,380 --> 00:29:26,090
kernel for your test

00:29:23,540 --> 00:29:29,150
so can you recommend now using mainline

00:29:26,090 --> 00:29:30,920
for Cinco tres k plus MPC SOC because

00:29:29,150 --> 00:29:33,170
I'm always struggling once in a while I

00:29:30,920 --> 00:29:36,410
try the mainline but then I go back to

00:29:33,170 --> 00:29:41,140
the Exile inks tree which I really don't

00:29:36,410 --> 00:29:44,420
like yeah you seem to be successful yeah

00:29:41,140 --> 00:29:48,170
so yeah I was successful getting the 418

00:29:44,420 --> 00:29:51,650
kernel to work was was a little bit

00:29:48,170 --> 00:29:56,090
painful at times and so I think without

00:29:51,650 --> 00:29:59,180
without that extra help I'm not sure I

00:29:56,090 --> 00:30:01,310
would have gotten there I they are they

00:29:59,180 --> 00:30:03,560
are actively incorporating those patches

00:30:01,310 --> 00:30:06,320
so the biggest one is that and

00:30:03,560 --> 00:30:09,890
firmware one so I haven't checked in the

00:30:06,320 --> 00:30:11,870
last week or two and so once that gets

00:30:09,890 --> 00:30:13,910
incorporated into the mainline that's

00:30:11,870 --> 00:30:19,490
the biggest one and then the other the

00:30:13,910 --> 00:30:22,060
other big thing is that although low

00:30:19,490 --> 00:30:27,100
level firmware pieces

00:30:22,060 --> 00:30:29,560
get up-to-date so all I think humans is

00:30:27,100 --> 00:30:34,330
world like 2018 - everything has to be

00:30:29,560 --> 00:30:38,860
up to 2018 - to Ryan I would be really

00:30:34,330 --> 00:30:42,490
interested to build it like you I can

00:30:38,860 --> 00:30:45,610
provide if you email me I can provide

00:30:42,490 --> 00:30:48,010
you the kernel I'm using its besides

00:30:45,610 --> 00:30:50,770
those two patches the modifications are

00:30:48,010 --> 00:30:53,140
very subtle there's not much going on so

00:30:50,770 --> 00:30:54,760
but but but if if there's still the gap

00:30:53,140 --> 00:30:57,400
in time if there's you know the next

00:30:54,760 --> 00:30:59,710
three or six months when it's helpful to

00:30:57,400 --> 00:31:02,320
have that specific kernel out there I

00:30:59,710 --> 00:31:04,150
can I can provide it to you or push it

00:31:02,320 --> 00:31:16,570
to github or something okay that's cool

00:31:04,150 --> 00:31:20,590
Thanks yeah you did you at any point run

00:31:16,570 --> 00:31:23,260
your ping pong test on the a53 using a

00:31:20,590 --> 00:31:25,060
single Ethernet port because I'm

00:31:23,260 --> 00:31:27,430
wondering if the long tail of packets

00:31:25,060 --> 00:31:29,470
could just be that you'd get caught

00:31:27,430 --> 00:31:30,930
behind a few 1,500 byte frames and

00:31:29,470 --> 00:31:36,550
that'll add hundreds of microseconds

00:31:30,930 --> 00:31:39,700
straight away yeah no I did not run it

00:31:36,550 --> 00:31:41,440
with a dedicated Ethernet port so but I

00:31:39,700 --> 00:31:43,480
don't think I wasn't running anything

00:31:41,440 --> 00:31:46,750
that would have large packets on the I

00:31:43,480 --> 00:31:48,970
mean it was just ssh traffic and so it

00:31:46,750 --> 00:31:52,030
was just terminal traffic and it wasn't

00:31:48,970 --> 00:31:54,480
even it was like a few bytes at a time

00:31:52,030 --> 00:31:58,060
and so there wouldn't have been any

00:31:54,480 --> 00:32:00,490
large packets that were causing issues

00:31:58,060 --> 00:32:02,350
but it definitely could be congestion or

00:32:00,490 --> 00:32:04,480
not not congestion because it was tiny

00:32:02,350 --> 00:32:07,420
bit of the bandwidth but there could be

00:32:04,480 --> 00:32:09,610
some dependencies there for the it might

00:32:07,420 --> 00:32:11,350
be an interesting test to run the a53 on

00:32:09,610 --> 00:32:16,930
one port and see if you get that same

00:32:11,350 --> 00:32:22,870
spread on the tail yeah yeah that I did

00:32:16,930 --> 00:32:25,870
do that just a little bit and it the

00:32:22,870 --> 00:32:31,350
performance the performance wasn't as

00:32:25,870 --> 00:32:34,230
good but it didn't have those unbounded

00:32:31,350 --> 00:32:41,100
tails yeah

00:32:34,230 --> 00:32:43,470
yeah so for the tests that you did with

00:32:41,100 --> 00:32:45,390
the Zen board with the single Ethernet

00:32:43,470 --> 00:32:50,070
port when you showed the pictures with

00:32:45,390 --> 00:32:52,290
the a53 you were using a sink able

00:32:50,070 --> 00:32:56,640
between the two secondary Ethernet ports

00:32:52,290 --> 00:32:59,700
yes for the test with the a nine did you

00:32:56,640 --> 00:33:01,830
run that to a just directly to another

00:32:59,700 --> 00:33:03,210
device and SSH from that device in

00:33:01,830 --> 00:33:05,220
addition to running the test from that

00:33:03,210 --> 00:33:10,590
device or were you running it through

00:33:05,220 --> 00:33:12,600
some sort of hover switch so I mean

00:33:10,590 --> 00:33:15,030
bring up what was on the network that

00:33:12,600 --> 00:33:17,190
had the a53 just that and the device it

00:33:15,030 --> 00:33:21,120
was round tripping with or also some

00:33:17,190 --> 00:33:24,060
like laptop urs was aging from make a

00:33:21,120 --> 00:33:29,130
difference when it was the a 53 to the

00:33:24,060 --> 00:33:32,910
a9 right everything all three ports were

00:33:29,130 --> 00:33:36,180
plugged into a local gigabit switch that

00:33:32,910 --> 00:33:39,300
also had other other so it would be

00:33:36,180 --> 00:33:43,440
interesting to try it directly from the

00:33:39,300 --> 00:33:45,540
a nine to the a 53 without any switch

00:33:43,440 --> 00:33:48,120
involved and then you could always SSH

00:33:45,540 --> 00:33:50,550
from the a 53 to the a9 in order to do

00:33:48,120 --> 00:33:52,680
the test or just set up an automated yes

00:33:50,550 --> 00:33:56,360
that doesn't require your activity yeah

00:33:52,680 --> 00:33:59,850
there's several other ways to do that

00:33:56,360 --> 00:34:02,100
another one and I have USB Ethernet

00:33:59,850 --> 00:34:03,540
ports and you know but introduce more

00:34:02,100 --> 00:34:05,580
confounding factors I was wondering

00:34:03,540 --> 00:34:07,320
about if you could have less confounding

00:34:05,580 --> 00:34:09,180
factor right I just having the one right

00:34:07,320 --> 00:34:10,770
Ethernet for the so yeah and the main

00:34:09,180 --> 00:34:13,440
reason I didn't do that is because the

00:34:10,770 --> 00:34:15,780
boot on all these was tied to a TFTP

00:34:13,440 --> 00:34:20,130
server I see and so the TFTP server

00:34:15,780 --> 00:34:22,530
wants to be there so it can grab that so

00:34:20,130 --> 00:34:24,710
that that's why the the setup was like

00:34:22,530 --> 00:34:24,710
that

00:34:27,440 --> 00:34:32,250
for the oil cyclic test measurements you

00:34:30,000 --> 00:34:34,589
said you have you used two instances of

00:34:32,250 --> 00:34:35,879
the tool with different priorities can

00:34:34,589 --> 00:34:38,720
you please explain what's the reason for

00:34:35,879 --> 00:34:42,569
that or if it's a usual way to do it I

00:34:38,720 --> 00:34:44,579
just wanted a lot of internet traffic so

00:34:42,569 --> 00:34:46,470
the interrupt traffic at a lower

00:34:44,579 --> 00:34:49,609
priority seemed like a good way to kind

00:34:46,470 --> 00:34:52,859
of mimic a lot of stuff going on

00:34:49,609 --> 00:34:57,869
89 priority is just some kind of stress

00:34:52,859 --> 00:34:59,940
tool in that case yeah yeah but this

00:34:57,869 --> 00:35:01,559
this one is another stress basically

00:34:59,940 --> 00:35:03,569
it's another it's another stressor for

00:35:01,559 --> 00:35:05,339
the for the processor and another

00:35:03,569 --> 00:35:06,960
question ed we performed one long

00:35:05,339 --> 00:35:09,990
measurement with a lot of measurement

00:35:06,960 --> 00:35:12,180
points have you done short measurements

00:35:09,990 --> 00:35:14,040
has restarted multiple measurements and

00:35:12,180 --> 00:35:16,260
accumulated the results this was just

00:35:14,040 --> 00:35:17,790
one long measurement and if you repeat

00:35:16,260 --> 00:35:19,559
that measurement does it depend on the

00:35:17,790 --> 00:35:21,510
starting condition of the measurement or

00:35:19,559 --> 00:35:23,670
have you tried just one long measurement

00:35:21,510 --> 00:35:25,980
and I write it several times it doesn't

00:35:23,670 --> 00:35:27,990
it doesn't very much okay it takes I

00:35:25,980 --> 00:35:34,109
mean you can it's fun to watch cuz you

00:35:27,990 --> 00:35:43,680
can you can let me get that chart up

00:35:34,109 --> 00:35:47,609
here so you know you can most of the

00:35:43,680 --> 00:35:49,890
time oh so most of the time you don't

00:35:47,609 --> 00:35:51,900
get the highest point right away and so

00:35:49,890 --> 00:35:54,180
you can you can see it fill in and then

00:35:51,900 --> 00:35:56,250
it'll like bump up one to the next

00:35:54,180 --> 00:35:58,980
microsecond in and then you run for

00:35:56,250 --> 00:36:01,710
another half hour and then bumps up to

00:35:58,980 --> 00:36:04,920
the to the next pin and so then but by

00:36:01,710 --> 00:36:07,020
you know the end you it's not moving

00:36:04,920 --> 00:36:11,089
much it's not that the maximum isn't

00:36:07,020 --> 00:36:11,089

YouTube URL: https://www.youtube.com/watch?v=dcupw4Z99Ls


