Title: Keynote: The Future of AI is Data...In More Ways than You Think - Eric Berlow
Publication date: 2018-10-25
Playlist: Open Source Summit Europe & ELC + OpenIoT Summit Europe 2018
Description: 
	Keynote: The Future of AI is Data...In More Ways than You Think - Eric Berlow, Co-Founder, Chief Science Officer, Vibrant Data Inc.

About Eric Berlow
Eric Berlow is an ecologist and network scientist who co-founded Vibrant Data, a data science and data visualization company that was acquired by Rakuten Inc. Berlow is internationally recognized for his research on ecological complexity – with articles in Nature, Science, and the Proceedings of the National Academy of Sciences that have received Thomson Reuter’s Most Highly Cited Papers awards. His two talks on TED.com on finding hidden patterns in complex data have received over 2 million views. Prior to Vibrant Data, Berlow was the director of the University of California’s first science institute inside Yosemite National Park, which focused on facilitating data-driven decisions for protecting natural ecosystems. Berlow has received a TED Fellowship, a TED Senior Fellowship, an Alexander von Humboldt Fellowship, and a National Science Foundation Post-Doctoral Fellowship. He was named one of the Top 100 Creatives by Origin Magazine. Berlow holds a B.A. in Biology from Brown University and a Ph.D. in Ecology from Oregon State University.
Captions: 
	00:00:00,000 --> 00:00:06,960
all right so there have been amazing

00:00:03,929 --> 00:00:09,240
advances in the past few years and what

00:00:06,960 --> 00:00:11,210
machines can do as you read about in the

00:00:09,240 --> 00:00:13,500
news headlines like this machines can

00:00:11,210 --> 00:00:15,269
recognize people and images it can

00:00:13,500 --> 00:00:18,750
transcribe speech and translate

00:00:15,269 --> 00:00:20,340
languages they can drive a car diagnose

00:00:18,750 --> 00:00:22,199
diseases the this just came out last

00:00:20,340 --> 00:00:24,570
week they can even tell you you're

00:00:22,199 --> 00:00:26,670
depressed before you know it based on

00:00:24,570 --> 00:00:30,420
what you type and scroll how you type in

00:00:26,670 --> 00:00:31,380
scroll but why now like a eyes been

00:00:30,420 --> 00:00:33,690
around for a while

00:00:31,380 --> 00:00:35,190
machine learning why suddenly all these

00:00:33,690 --> 00:00:38,129
big advances and I want to argue that

00:00:35,190 --> 00:00:40,920
it's it's not about the algorithms it's

00:00:38,129 --> 00:00:43,890
actually all about the data and think if

00:00:40,920 --> 00:00:45,719
you think about it the top 20

00:00:43,890 --> 00:00:48,780
contributors to open-source projects on

00:00:45,719 --> 00:00:52,649
github among those are Google Facebook

00:00:48,780 --> 00:00:54,539
Amazon Microsoft IBM uber Alibaba most

00:00:52,649 --> 00:00:56,820
of the biggest players in the AI space

00:00:54,539 --> 00:00:59,359
are open sourcing their aai pipelines

00:00:56,820 --> 00:01:01,530
but what are they not open sourcing

00:00:59,359 --> 00:01:04,049
they're not open sourcing their data

00:01:01,530 --> 00:01:10,049
because it's their number-one asset the

00:01:04,049 --> 00:01:11,580
data so while in 2017 a lot of the

00:01:10,049 --> 00:01:14,460
headlines look like this there's a

00:01:11,580 --> 00:01:15,900
really big year for AI now we start to

00:01:14,460 --> 00:01:23,040
see a lot more headlines that look like

00:01:15,900 --> 00:01:25,200
this right so bias Ti is leading to

00:01:23,040 --> 00:01:27,210
racial discrimination and policing and

00:01:25,200 --> 00:01:29,729
criminal convictions gender

00:01:27,210 --> 00:01:31,439
discrimination and job ads headlines

00:01:29,729 --> 00:01:34,619
about racial discrimination and image

00:01:31,439 --> 00:01:37,759
recognition and film recommendations and

00:01:34,619 --> 00:01:39,860
these are all data problems they're not

00:01:37,759 --> 00:01:41,729
algorithm problems

00:01:39,860 --> 00:01:43,140
Kathy O'Neill has made a very good

00:01:41,729 --> 00:01:46,140
argument that we need more transparency

00:01:43,140 --> 00:01:48,720
in unpacking the algorithms especially

00:01:46,140 --> 00:01:50,009
deep neural nets but again I want to

00:01:48,720 --> 00:01:52,140
argue today that many of the problems

00:01:50,009 --> 00:01:55,680
we're seeing and the solutions to them

00:01:52,140 --> 00:01:57,360
are really about data and not algorithms

00:01:55,680 --> 00:02:00,750
and not just data like training data for

00:01:57,360 --> 00:02:04,619
models but the entire data ecosystem so

00:02:00,750 --> 00:02:07,350
why do I care about this a healthy AI

00:02:04,619 --> 00:02:09,179
depends on a healthy ecosystem and I'm

00:02:07,350 --> 00:02:10,979
an ecologist that's my background so I

00:02:09,179 --> 00:02:12,630
like to think about ecosystems I've been

00:02:10,979 --> 00:02:14,940
studying complex

00:02:12,630 --> 00:02:16,470
in nature for about 25 years I used to

00:02:14,940 --> 00:02:20,010
run an Institute in Yosemite National

00:02:16,470 --> 00:02:22,770
Park that where we did work on Alpine

00:02:20,010 --> 00:02:24,900
ecosystems and it was very data heavy we

00:02:22,770 --> 00:02:27,930
we compiled 30 years of satellite

00:02:24,900 --> 00:02:29,490
imagery data blended that with a lot of

00:02:27,930 --> 00:02:32,520
hard-earned data on the ground to

00:02:29,490 --> 00:02:34,560
validate models and and to build and

00:02:32,520 --> 00:02:36,750
train and test models to help predict

00:02:34,560 --> 00:02:39,990
where endangered species are living in

00:02:36,750 --> 00:02:42,060
the park and how to best prioritize

00:02:39,990 --> 00:02:44,220
decisions for how to save them no these

00:02:42,060 --> 00:02:47,760
are things where at the time we're very

00:02:44,220 --> 00:02:49,320
aware of concern about potential

00:02:47,760 --> 00:02:50,400
unintended consequences because if

00:02:49,320 --> 00:02:52,230
something's going to disappear off the

00:02:50,400 --> 00:02:55,710
face of the earth we don't want that to

00:02:52,230 --> 00:02:58,680
happen now I work at Rakuten

00:02:55,710 --> 00:03:01,140
intelligence parícutin and I work more

00:02:58,680 --> 00:03:02,520
on economic ecosystems where I work on

00:03:01,140 --> 00:03:04,800
e-commerce data and stuff and I'm

00:03:02,520 --> 00:03:06,690
becoming much more aware also of the

00:03:04,800 --> 00:03:09,260
potential unintended consequences of how

00:03:06,690 --> 00:03:13,830
our personal data is used by algorithms

00:03:09,260 --> 00:03:16,410
so if we want to understand this let's

00:03:13,830 --> 00:03:18,950
use an example for an unhealthy data

00:03:16,410 --> 00:03:22,980
ecosystem to understand what goes wrong

00:03:18,950 --> 00:03:24,300
this is the Russia Today news channel on

00:03:22,980 --> 00:03:25,590
YouTube I don't know if you guys have

00:03:24,300 --> 00:03:28,380
seen it it turns out it's the

00:03:25,590 --> 00:03:30,930
most-watched news channel on YouTube in

00:03:28,380 --> 00:03:33,480
the United States so how is it that a

00:03:30,930 --> 00:03:35,640
Russian news channel is the most-watched

00:03:33,480 --> 00:03:41,190
news channel on YouTube in the United

00:03:35,640 --> 00:03:43,440
States similarly if i type in syria news

00:03:41,190 --> 00:03:46,590
into my browser and youtube

00:03:43,440 --> 00:03:48,330
three of the four top results are russia

00:03:46,590 --> 00:03:49,709
today are russian news channels one of

00:03:48,330 --> 00:03:51,750
them actually in russian even though

00:03:49,709 --> 00:03:53,490
Google knows that I'm searching from a

00:03:51,750 --> 00:03:56,220
computer in San Francisco California

00:03:53,490 --> 00:03:57,500
in English how does that happen what's

00:03:56,220 --> 00:04:01,380
happening

00:03:57,500 --> 00:04:02,430
well the algorithms are doing just fine

00:04:01,380 --> 00:04:03,720
they're doing exactly what they're

00:04:02,430 --> 00:04:05,220
supposed to do the problem is the data

00:04:03,720 --> 00:04:07,830
are not the algorithms and so I want to

00:04:05,220 --> 00:04:10,080
unpack this problem about about the data

00:04:07,830 --> 00:04:12,150
behind it so let's just think about a

00:04:10,080 --> 00:04:15,480
traditional simple machine learning

00:04:12,150 --> 00:04:17,850
pipeline where first we need training

00:04:15,480 --> 00:04:19,830
data to fit a model and predict

00:04:17,850 --> 00:04:21,960
something that's how it starts and then

00:04:19,830 --> 00:04:24,600
we need additional validation data to

00:04:21,960 --> 00:04:25,070
may be tuned the model or pick amongst

00:04:24,600 --> 00:04:27,650
many

00:04:25,070 --> 00:04:29,660
which is the best one with the goal of

00:04:27,650 --> 00:04:33,110
minimizing the error in maximizing

00:04:29,660 --> 00:04:34,700
prediction success and then we have

00:04:33,110 --> 00:04:36,710
independent test data to independently

00:04:34,700 --> 00:04:38,840
test the model and evaluate its accuracy

00:04:36,710 --> 00:04:40,460
and then once we've done that and have

00:04:38,840 --> 00:04:43,310
our tested model we put it out in the

00:04:40,460 --> 00:04:45,080
wild and it goes wild so what happened

00:04:43,310 --> 00:04:47,390
with our tnews well if you go back into

00:04:45,080 --> 00:04:50,660
the history of their their YouTube

00:04:47,390 --> 00:04:53,690
channel they actively seeded the

00:04:50,660 --> 00:04:56,510
training testing and two validation

00:04:53,690 --> 00:04:58,940
testing data with clickbait viral videos

00:04:56,510 --> 00:05:01,610
if you zoom in to those they're really

00:04:58,940 --> 00:05:03,890
this is a news channel and so these are

00:05:01,610 --> 00:05:05,870
videos like horror footage shocking

00:05:03,890 --> 00:05:08,630
video of a plane crash kid dies from

00:05:05,870 --> 00:05:11,000
fallout falling from Ferris wheel whale

00:05:08,630 --> 00:05:13,070
hits and smashes yacht surfer fights off

00:05:11,000 --> 00:05:16,250
shark attack live on TV so these are

00:05:13,070 --> 00:05:17,960
just like salacious clickbait videos to

00:05:16,250 --> 00:05:21,350
drive high engagement in the site

00:05:17,960 --> 00:05:22,520
because they know that the models are

00:05:21,350 --> 00:05:24,860
trying to predict engagement and they're

00:05:22,520 --> 00:05:26,240
optimizing for that so then the

00:05:24,860 --> 00:05:28,880
algorithm is flagged every news channel

00:05:26,240 --> 00:05:31,730
as a high engagement channel and then lo

00:05:28,880 --> 00:05:33,140
and behold they're new they puts their

00:05:31,730 --> 00:05:34,700
news in the search results and all the

00:05:33,140 --> 00:05:37,340
recommendations which then leads to

00:05:34,700 --> 00:05:40,490
higher engagement than any other news

00:05:37,340 --> 00:05:43,160
channel on YouTube this is a classic

00:05:40,490 --> 00:05:46,010
case of biased data leading to biased

00:05:43,160 --> 00:05:47,810
outcomes so a classic data problem where

00:05:46,010 --> 00:05:52,610
artino is actively created a bias

00:05:47,810 --> 00:05:54,410
training data set to game the models and

00:05:52,610 --> 00:05:56,720
it's a virtuous cycle for RT news

00:05:54,410 --> 00:05:58,130
because there is the outcome results are

00:05:56,720 --> 00:06:01,990
then used for more training and testing

00:05:58,130 --> 00:06:04,100
and and if we go in the virtuous cycle

00:06:01,990 --> 00:06:05,840
just because it's not big data doesn't

00:06:04,100 --> 00:06:06,470
mean it's not biased it's a it's a big

00:06:05,840 --> 00:06:08,840
problem

00:06:06,470 --> 00:06:10,460
and again the algorithms are doing a

00:06:08,840 --> 00:06:12,290
great job they are doing a great job of

00:06:10,460 --> 00:06:17,150
protecting engagement it's just not the

00:06:12,290 --> 00:06:19,880
right thing so there's a second problem

00:06:17,150 --> 00:06:21,530
a data problem in this ecosystem that

00:06:19,880 --> 00:06:23,240
I'd like to look at too let's look at

00:06:21,530 --> 00:06:25,900
not the input data but the outcome so

00:06:23,240 --> 00:06:27,920
they're trying to predict engagement and

00:06:25,900 --> 00:06:29,450
that that's sort of what the models

00:06:27,920 --> 00:06:32,660
focus on trying to minimize the error on

00:06:29,450 --> 00:06:34,490
and when the model is deployed if people

00:06:32,660 --> 00:06:35,780
are recommending viral videos of course

00:06:34,490 --> 00:06:36,370
they click them and watch through the

00:06:35,780 --> 00:06:38,440
whole thing

00:06:36,370 --> 00:06:40,060
to comment on them etc so it looks like

00:06:38,440 --> 00:06:41,320
it's doing great the ad revenue is

00:06:40,060 --> 00:06:42,790
rolling in so nobody pays attention

00:06:41,320 --> 00:06:46,330
because it seems like it must be working

00:06:42,790 --> 00:06:49,060
but the goal was actually to in theory

00:06:46,330 --> 00:06:50,639
recommend good quality content relevant

00:06:49,060 --> 00:06:53,320
content that's good quality and

00:06:50,639 --> 00:06:55,020
engagement is just a proxy for that that

00:06:53,320 --> 00:06:58,030
we can measure easily and optimize for

00:06:55,020 --> 00:07:00,960
so since there's a mismatch between our

00:06:58,030 --> 00:07:04,510
proxy for success and our actual goals

00:07:00,960 --> 00:07:06,910
then we end up recommending propaganda

00:07:04,510 --> 00:07:08,290
instead of high quality content and and

00:07:06,910 --> 00:07:10,660
again there's nothing wrong with the

00:07:08,290 --> 00:07:12,699
algorithms they're just they're working

00:07:10,660 --> 00:07:14,470
great for engagement in ad revenue but

00:07:12,699 --> 00:07:19,120
not for predicting high quality good

00:07:14,470 --> 00:07:21,310
content this is this problem on the

00:07:19,120 --> 00:07:24,490
other side is a mismatch between our

00:07:21,310 --> 00:07:26,080
outcome metric and the goal so meanwhile

00:07:24,490 --> 00:07:28,000
the algorithm is doing a great job at

00:07:26,080 --> 00:07:30,190
actual really predicting the wrong thing

00:07:28,000 --> 00:07:32,710
and the key problem here is a data

00:07:30,190 --> 00:07:34,360
problem that we're often missing outcome

00:07:32,710 --> 00:07:36,430
data data on real outcomes that's

00:07:34,360 --> 00:07:37,600
independent from the engagement from

00:07:36,430 --> 00:07:41,530
that from that one metric like

00:07:37,600 --> 00:07:43,720
engagement metric so so for example we

00:07:41,530 --> 00:07:45,639
rarely have good data on like who got

00:07:43,720 --> 00:07:48,820
recommended what just big picture data

00:07:45,639 --> 00:07:49,960
and did it is it what we intended we

00:07:48,820 --> 00:07:52,000
don't often get to see that big picture

00:07:49,960 --> 00:07:53,110
and without alternative outcome data

00:07:52,000 --> 00:07:55,720
we'll never know if we actually

00:07:53,110 --> 00:07:56,849
optimized for the right thing until it's

00:07:55,720 --> 00:07:59,590
too late

00:07:56,849 --> 00:08:01,150
again this is not an algorithm problem

00:07:59,590 --> 00:08:04,060
it's a data problem and it really boils

00:08:01,150 --> 00:08:07,840
down to two very simple data challenges

00:08:04,060 --> 00:08:11,260
one is biased a tin is biased outcome

00:08:07,840 --> 00:08:14,949
out the other is wrong success metric

00:08:11,260 --> 00:08:16,539
wrong outcome they're pre trivial in

00:08:14,949 --> 00:08:18,430
retrospect but we miss them a lot

00:08:16,539 --> 00:08:21,280
so the question is how do we better

00:08:18,430 --> 00:08:23,710
detect and correct bias how do we get

00:08:21,280 --> 00:08:26,289
real outcome data so that we can check

00:08:23,710 --> 00:08:28,780
our results and gut check things and how

00:08:26,289 --> 00:08:31,030
do we optimize for the right thing or

00:08:28,780 --> 00:08:33,940
prevent accurately predicting the wrong

00:08:31,030 --> 00:08:36,070
thing so there's two core editions to

00:08:33,940 --> 00:08:37,510
this data ecosystem that I want to that

00:08:36,070 --> 00:08:38,830
I think can help a lot and that are

00:08:37,510 --> 00:08:41,050
emerging right now and it's really great

00:08:38,830 --> 00:08:42,849
to know about one our data market

00:08:41,050 --> 00:08:44,020
place's and the other is data interfaces

00:08:42,849 --> 00:08:45,160
and I'm going to go through each of

00:08:44,020 --> 00:08:47,170
these so let's start with data

00:08:45,160 --> 00:08:48,600
marketplaces there's a lot of really

00:08:47,170 --> 00:08:50,519
interesting

00:08:48,600 --> 00:08:52,490
movement happening in this space to

00:08:50,519 --> 00:08:54,959
improve the health of our data ecosystem

00:08:52,490 --> 00:08:56,670
just a couple weeks ago Tim berners-lee

00:08:54,959 --> 00:09:01,670
the the creator of the world wide web

00:08:56,670 --> 00:09:03,990
were launched this project which is a

00:09:01,670 --> 00:09:05,490
decentralized personal data marketplace

00:09:03,990 --> 00:09:08,339
where you can have your own pods

00:09:05,490 --> 00:09:10,079
personal online data stores and where

00:09:08,339 --> 00:09:12,060
you can own and control all your

00:09:10,079 --> 00:09:14,069
personal data in a personal data

00:09:12,060 --> 00:09:18,690
marketplace this really interesting

00:09:14,069 --> 00:09:20,339
project the Linux Foundation here last

00:09:18,690 --> 00:09:22,709
year just launched the community data

00:09:20,339 --> 00:09:24,930
licensing agreement to facilitate

00:09:22,709 --> 00:09:26,970
sharing of open data not just open code

00:09:24,930 --> 00:09:28,920
because remember it's not the algorithms

00:09:26,970 --> 00:09:30,269
it's the data that's really important

00:09:28,920 --> 00:09:33,509
here

00:09:30,269 --> 00:09:35,610
Oasis labs in Berkeley UC Berkeley just

00:09:33,509 --> 00:09:37,889
raised 45 million including from

00:09:35,610 --> 00:09:39,569
Anderson Horowitz to make it easier for

00:09:37,889 --> 00:09:41,639
us to actually share that data without

00:09:39,569 --> 00:09:43,440
losing control of it to prevent another

00:09:41,639 --> 00:09:45,589
Cambridge analytical type of scandal

00:09:43,440 --> 00:09:49,620
from happening again

00:09:45,589 --> 00:09:51,870
Oshin protocol just raised also about 45

00:09:49,620 --> 00:09:53,310
million to kick-start a decentralized

00:09:51,870 --> 00:09:55,769
data marketplace that's focused

00:09:53,310 --> 00:09:59,430
specifically on data for AI and machine

00:09:55,769 --> 00:10:02,339
learning and here's an example nebula

00:09:59,430 --> 00:10:05,250
genomics just recently was funded to

00:10:02,339 --> 00:10:07,620
take on 23andme to build a decentralized

00:10:05,250 --> 00:10:10,019
personal genomics data marketplace which

00:10:07,620 --> 00:10:12,509
lets you own your genomic data control

00:10:10,019 --> 00:10:14,660
who gets to use it and for what and even

00:10:12,509 --> 00:10:17,899
get directly rewarded for sharing it

00:10:14,660 --> 00:10:20,430
alright so last summer martin gillies

00:10:17,899 --> 00:10:22,769
said that made a case that we need to

00:10:20,430 --> 00:10:25,050
rein in the data barons to restrain

00:10:22,769 --> 00:10:26,639
their market power but I think that it's

00:10:25,050 --> 00:10:28,529
possible with all these emerging open

00:10:26,639 --> 00:10:32,029
data marketplaces we'll just make them

00:10:28,529 --> 00:10:32,029
irrelevant that's sort of the goal

00:10:32,269 --> 00:10:36,689
so decentralized data mark places are

00:10:35,310 --> 00:10:38,370
coming and it's really great news

00:10:36,689 --> 00:10:40,860
they're trying to solve some really

00:10:38,370 --> 00:10:42,839
important challenges around ownership

00:10:40,860 --> 00:10:44,910
you know and how do you how do you set

00:10:42,839 --> 00:10:47,220
licensing how do you control who uses

00:10:44,910 --> 00:10:49,050
your data and for what purpose how do

00:10:47,220 --> 00:10:51,649
you have secure systems for sharing data

00:10:49,050 --> 00:10:53,579
but without losing control of it

00:10:51,649 --> 00:10:55,560
transparency to see who's using your

00:10:53,579 --> 00:10:57,809
data for what and how to have aligned

00:10:55,560 --> 00:10:59,639
incentives between the data provider and

00:10:57,809 --> 00:11:01,510
the data user it's all really exciting

00:10:59,639 --> 00:11:04,180
but even if we have

00:11:01,510 --> 00:11:07,389
better and more diverse data we also

00:11:04,180 --> 00:11:09,699
need better tools for thinking about how

00:11:07,389 --> 00:11:10,839
to gut check what we're doing and this

00:11:09,699 --> 00:11:13,329
is the human side of the date of

00:11:10,839 --> 00:11:15,940
ecosystem and it's one that I'm really

00:11:13,329 --> 00:11:17,709
interested in let me give you some

00:11:15,940 --> 00:11:20,829
examples in response to all the recent

00:11:17,709 --> 00:11:23,579
press about AI bias there's been some

00:11:20,829 --> 00:11:25,630
interesting activity Facebook Google IBM

00:11:23,579 --> 00:11:28,000
Microsoft all in the last few months

00:11:25,630 --> 00:11:31,300
announced that they're launching tools

00:11:28,000 --> 00:11:33,040
to detect bias and algorithms and what's

00:11:31,300 --> 00:11:35,260
really interesting to me about some of

00:11:33,040 --> 00:11:37,449
these that have seen like here's the the

00:11:35,260 --> 00:11:39,130
one the Google what-if tool it's called

00:11:37,449 --> 00:11:40,720
code free probing of machine learning

00:11:39,130 --> 00:11:41,380
models there's two things worth noting

00:11:40,720 --> 00:11:43,300
about it

00:11:41,380 --> 00:11:46,630
one is it's not a data visualization

00:11:43,300 --> 00:11:48,610
tool even though it's visual it's a

00:11:46,630 --> 00:11:51,250
visual interface for probing and

00:11:48,610 --> 00:11:54,339
discovering it's an interface it's not

00:11:51,250 --> 00:11:56,560
just a data vis it's also the second

00:11:54,339 --> 00:11:58,720
thing is it's code free which means it

00:11:56,560 --> 00:12:00,160
facilitates just spontaneous one-off

00:11:58,720 --> 00:12:04,060
exploration rather than intentional

00:12:00,160 --> 00:12:06,970
scripted visualization these are really

00:12:04,060 --> 00:12:08,980
important for discovery interfaces my

00:12:06,970 --> 00:12:12,310
company that I co-founded vibrant data

00:12:08,980 --> 00:12:13,630
created this one and we were we were

00:12:12,310 --> 00:12:15,430
recently acquired by Rakuten and I'm

00:12:13,630 --> 00:12:18,399
really excited to announce that we just

00:12:15,430 --> 00:12:21,040
open sourced it open map or dot org and

00:12:18,399 --> 00:12:23,050
it's focuses on exploration of complex

00:12:21,040 --> 00:12:24,699
networks and I want to give you a quick

00:12:23,050 --> 00:12:27,519
example of how some researchers at

00:12:24,699 --> 00:12:29,860
National Geographic used it recently for

00:12:27,519 --> 00:12:34,149
some of their machine learning processes

00:12:29,860 --> 00:12:36,069
so this is an image of one of my study

00:12:34,149 --> 00:12:37,449
sites in the Sierra Nevada which is

00:12:36,069 --> 00:12:38,949
pretty nice place to be and I'm really

00:12:37,449 --> 00:12:41,440
thankful that I get to go to places like

00:12:38,949 --> 00:12:43,089
this and it turns out that some of the

00:12:41,440 --> 00:12:45,250
researchers at National Geographic are

00:12:43,089 --> 00:12:47,980
learning that even just imagery of

00:12:45,250 --> 00:12:49,839
nature can have calming effects just

00:12:47,980 --> 00:12:51,370
like not maybe quite as good as being

00:12:49,839 --> 00:12:53,139
there but they do have real effects on

00:12:51,370 --> 00:12:55,600
the brain and they're wondering whether

00:12:53,139 --> 00:12:57,930
they could try to understand if it'd be

00:12:55,600 --> 00:13:00,399
useful for places like this where

00:12:57,930 --> 00:13:02,440
environments where people have no access

00:13:00,399 --> 00:13:04,810
to nature and no hope for it you know

00:13:02,440 --> 00:13:07,089
maybe could images help and if so which

00:13:04,810 --> 00:13:09,670
ones are best so they launched a

00:13:07,089 --> 00:13:12,130
research project to try to find out

00:13:09,670 --> 00:13:14,440
they're analyzing thousands of data

00:13:12,130 --> 00:13:18,459
of images from the National Geographic

00:13:14,440 --> 00:13:19,810
Instagram feed and they they gather in

00:13:18,459 --> 00:13:21,250
this preliminary round they gathered a

00:13:19,810 --> 00:13:23,319
few months of data and measured

00:13:21,250 --> 00:13:26,079
attributes of every image and tagged

00:13:23,319 --> 00:13:27,639
them according to whether there's like a

00:13:26,079 --> 00:13:29,230
person in the image or not whether it's

00:13:27,639 --> 00:13:30,880
urban or nature whether there's a

00:13:29,230 --> 00:13:33,130
mountain and the picture it's a distant

00:13:30,880 --> 00:13:36,009
view is it from above or below is it a

00:13:33,130 --> 00:13:37,329
close-up far away etc and then they're

00:13:36,009 --> 00:13:39,339
trying to use that to build models and

00:13:37,329 --> 00:13:44,050
predict which in which types of images

00:13:39,339 --> 00:13:46,899
have the best responses so to explore

00:13:44,050 --> 00:13:48,220
that preliminary data set they just

00:13:46,899 --> 00:13:52,029
upload it into mapper it's a cloud-based

00:13:48,220 --> 00:13:54,579
tool right now and they just upload it

00:13:52,029 --> 00:13:56,350
as a CSV and then once you upload it

00:13:54,579 --> 00:13:58,029
open mapper can generate a similarity

00:13:56,350 --> 00:14:01,120
network to find structure in the data

00:13:58,029 --> 00:14:02,440
immediately so here you know I taught

00:14:01,120 --> 00:14:03,420
Ematic Lee detects high dimensional

00:14:02,440 --> 00:14:06,490
patterns in the data

00:14:03,420 --> 00:14:08,230
every node here is an image and then

00:14:06,490 --> 00:14:09,730
they're linked to one another if they're

00:14:08,230 --> 00:14:12,279
similar if they share similar image

00:14:09,730 --> 00:14:14,589
attributes and then similar images

00:14:12,279 --> 00:14:16,420
automatically cluster together into

00:14:14,589 --> 00:14:18,069
groups that are Auto interpreted Auto

00:14:16,420 --> 00:14:20,230
labeled based on the features that are

00:14:18,069 --> 00:14:22,600
the most common in the group so this is

00:14:20,230 --> 00:14:24,490
a group of images that are distant they

00:14:22,600 --> 00:14:26,380
tend to be distant unobstructed views of

00:14:24,490 --> 00:14:28,120
snow-capped mountains and then if you

00:14:26,380 --> 00:14:29,500
look on the upper left there it turns

00:14:28,120 --> 00:14:31,990
out that's the biggest group of images

00:14:29,500 --> 00:14:33,699
so right away we can see in this high

00:14:31,990 --> 00:14:37,089
dimensional space that there's a there's

00:14:33,699 --> 00:14:39,310
already a bias towards these kinds of

00:14:37,089 --> 00:14:40,420
nature images in the data set not that

00:14:39,310 --> 00:14:45,279
it's bad but just so that we know that

00:14:40,420 --> 00:14:47,380
it's there so here's another cluster of

00:14:45,279 --> 00:14:51,490
images the next biggest one is focused

00:14:47,380 --> 00:14:54,550
on oceans and pictures of water and

00:14:51,490 --> 00:14:56,800
animals and oceans kind of and they all

00:14:54,550 --> 00:14:57,990
have I think like low consistent but at

00:14:56,800 --> 00:15:01,480
low hue in them

00:14:57,990 --> 00:15:05,860
here is another sort of cluster of

00:15:01,480 --> 00:15:08,079
images that are close-ups of animal

00:15:05,860 --> 00:15:11,259
faces that have high edge density and

00:15:08,079 --> 00:15:12,670
high fractal dimension so so the cool

00:15:11,259 --> 00:15:14,680
thing is that just by having even the

00:15:12,670 --> 00:15:17,319
images render in the nodes you have

00:15:14,680 --> 00:15:18,790
instant like gut checking on what are

00:15:17,319 --> 00:15:20,740
these images what do they look like you

00:15:18,790 --> 00:15:22,990
can zoom out and then zoom into

00:15:20,740 --> 00:15:24,020
individual individual data points really

00:15:22,990 --> 00:15:26,510
quickly

00:15:24,020 --> 00:15:28,070
so we can also zoom out and we sort

00:15:26,510 --> 00:15:30,709
those things and the scatterplot let's

00:15:28,070 --> 00:15:32,750
say and just see which images are high

00:15:30,709 --> 00:15:35,000
engagement here they're sorted

00:15:32,750 --> 00:15:38,120
vertically by the emergent themes and

00:15:35,000 --> 00:15:39,709
then horizontally by engagement and off

00:15:38,120 --> 00:15:41,089
to the left we have distributions of all

00:15:39,709 --> 00:15:43,310
the attributes so we can see here just

00:15:41,089 --> 00:15:45,380
as an example on this urban natural

00:15:43,310 --> 00:15:46,820
gradient that we can see there's a

00:15:45,380 --> 00:15:50,000
really heavy bias in this dataset

00:15:46,820 --> 00:15:52,339
towards nature images which is not

00:15:50,000 --> 00:15:53,690
surprising but at least it sparked an

00:15:52,339 --> 00:15:55,459
idea to say oh there's all these other

00:15:53,690 --> 00:15:57,170
ones with humans and urban scenes in

00:15:55,459 --> 00:15:58,610
them what does look like in are their

00:15:57,170 --> 00:16:00,529
high engagement ones in there as well so

00:15:58,610 --> 00:16:03,399
you can just quickly take the filters

00:16:00,529 --> 00:16:05,649
and have a look see who they are

00:16:03,399 --> 00:16:09,020
urban images that have relatively high

00:16:05,649 --> 00:16:11,330
engagement and then subset those and

00:16:09,020 --> 00:16:13,580
dive in and look at them and see what

00:16:11,330 --> 00:16:15,230
they look like so there of course we

00:16:13,580 --> 00:16:17,750
have some really nice unobstructed

00:16:15,230 --> 00:16:20,600
distant views that are sunsets a lot has

00:16:17,750 --> 00:16:22,310
sunset images the people liked with with

00:16:20,600 --> 00:16:24,290
cities in the foreground but also some

00:16:22,310 --> 00:16:26,330
really interesting cultural landscapes

00:16:24,290 --> 00:16:28,880
as well and then of course we have

00:16:26,330 --> 00:16:31,820
people seem to like panda and on lab

00:16:28,880 --> 00:16:34,130
bench and then this last one struck my

00:16:31,820 --> 00:16:35,990
attention because you got a tiger in a

00:16:34,130 --> 00:16:39,140
cage it's kind of scary and I thought

00:16:35,990 --> 00:16:41,420
you know it's a high engagement image

00:16:39,140 --> 00:16:43,490
but probably not great to put that in a

00:16:41,420 --> 00:16:45,320
prison right if we're if the algorithm

00:16:43,490 --> 00:16:47,060
automatically recommended it and and of

00:16:45,320 --> 00:16:49,490
course it's obvious no but if that was

00:16:47,060 --> 00:16:51,320
just the image ID some random ID number

00:16:49,490 --> 00:16:53,720
I just wouldn't have thought to you know

00:16:51,320 --> 00:16:55,430
wouldn't have known that so now we step

00:16:53,720 --> 00:16:58,190
back and it just encourages to go back

00:16:55,430 --> 00:17:02,450
and look at the data now and just go

00:16:58,190 --> 00:17:04,069
find other metrics of responses so they

00:17:02,450 --> 00:17:06,559
also measure instead of engagement they

00:17:04,069 --> 00:17:08,630
measured valence which is a sense of the

00:17:06,559 --> 00:17:10,400
mood like what's you have a positive or

00:17:08,630 --> 00:17:12,800
negative emotional response to the image

00:17:10,400 --> 00:17:15,380
and also arousal which is the intensity

00:17:12,800 --> 00:17:16,579
of your emotional response and so now

00:17:15,380 --> 00:17:17,990
with this we could look and say well

00:17:16,579 --> 00:17:20,059
let's look at all those negative

00:17:17,990 --> 00:17:22,760
emotional response images and ones that

00:17:20,059 --> 00:17:24,890
also had a high intensity of a response

00:17:22,760 --> 00:17:26,390
dive into those see where they lay on

00:17:24,890 --> 00:17:28,700
there's high engagement ones there as

00:17:26,390 --> 00:17:32,150
well and look at that and see where they

00:17:28,700 --> 00:17:34,580
are and and well that we suddenly found

00:17:32,150 --> 00:17:37,460
all the predators and all the shark

00:17:34,580 --> 00:17:41,990
attacks even a

00:17:37,460 --> 00:17:43,370
killer baby polar bear and and these are

00:17:41,990 --> 00:17:45,860
there's plenty these in the data set and

00:17:43,370 --> 00:17:49,370
so we could go back and look look at the

00:17:45,860 --> 00:17:50,779
other examples of high mood and high

00:17:49,370 --> 00:17:52,309
arousal and see what they look like

00:17:50,779 --> 00:17:54,860
these might be a little bit better to

00:17:52,309 --> 00:17:56,929
put into prison and sure enough these

00:17:54,860 --> 00:18:00,559
are what we are the warm and fuzzy

00:17:56,929 --> 00:18:02,210
creatures that you kind of expect right

00:18:00,559 --> 00:18:04,640
and what was interesting here too is

00:18:02,210 --> 00:18:06,860
that there's also some interesting human

00:18:04,640 --> 00:18:08,990
scenes that were part of that group as

00:18:06,860 --> 00:18:11,270
well so it led us to think that maybe we

00:18:08,990 --> 00:18:13,279
should try to include more human and

00:18:11,270 --> 00:18:15,640
urban things in the data set that might

00:18:13,279 --> 00:18:18,919
also elicit positive emotional responses

00:18:15,640 --> 00:18:23,179
so that's just a you know review of the

00:18:18,919 --> 00:18:25,070
data the this view for them really

00:18:23,179 --> 00:18:26,809
helped them just explore in the data it

00:18:25,070 --> 00:18:28,190
helped them find errors it helped them

00:18:26,809 --> 00:18:30,260
identify biases in the data

00:18:28,190 --> 00:18:33,020
unintentional towards certain types of

00:18:30,260 --> 00:18:35,419
nature imagery helped explore the

00:18:33,020 --> 00:18:36,559
unexpected value of maybe some urban and

00:18:35,419 --> 00:18:38,899
human scenes they hadn't thought about

00:18:36,559 --> 00:18:40,789
before the help them think creatively

00:18:38,899 --> 00:18:43,610
about alternative outcome metrics that

00:18:40,789 --> 00:18:45,770
might better match the goals and it and

00:18:43,610 --> 00:18:48,350
also help them spot and fix errors in

00:18:45,770 --> 00:18:50,539
the data they couldn't have done that

00:18:48,350 --> 00:18:51,799
just by staring at the data like this I

00:18:50,539 --> 00:18:54,140
don't think it's possible

00:18:51,799 --> 00:18:56,740
okay so data and data interfaces are not

00:18:54,140 --> 00:18:59,779
data visualizations they're not data

00:18:56,740 --> 00:19:02,029
dashboards they are tools to facilitate

00:18:59,779 --> 00:19:03,559
critical thinking that allow you to

00:19:02,029 --> 00:19:06,919
easily go back and forth between a

00:19:03,559 --> 00:19:09,830
big-picture view and and dive into you

00:19:06,919 --> 00:19:13,159
know point level details there because

00:19:09,830 --> 00:19:14,929
they're code-free they allow us to

00:19:13,159 --> 00:19:17,990
follow our curiosity with one-off

00:19:14,929 --> 00:19:19,640
spontaneous questions so while a lot of

00:19:17,990 --> 00:19:21,529
artificial intelligence is focused on

00:19:19,640 --> 00:19:24,380
training machines to be smarter to spit

00:19:21,529 --> 00:19:27,020
out predictions data interfaces use

00:19:24,380 --> 00:19:30,070
machines to help make us smarter so we

00:19:27,020 --> 00:19:32,480
can increase our understanding the data

00:19:30,070 --> 00:19:34,640
okay so I'm just going to wrap up here

00:19:32,480 --> 00:19:37,309
and summarize how this all fits together

00:19:34,640 --> 00:19:41,899
into our data ecosystem so in our

00:19:37,309 --> 00:19:44,960
current system real outcomes are

00:19:41,899 --> 00:19:46,370
generally a black box it's rarely is our

00:19:44,960 --> 00:19:47,580
data to see the big picture of who got

00:19:46,370 --> 00:19:49,490
served what add or who

00:19:47,580 --> 00:19:52,170
recommended what video or what product

00:19:49,490 --> 00:19:53,460
because of that it means that it's

00:19:52,170 --> 00:19:56,640
really hard to track if we're actually

00:19:53,460 --> 00:20:00,240
achieving our goal or if we actually are

00:19:56,640 --> 00:20:02,130
optimizing on the right thing meanwhile

00:20:00,240 --> 00:20:04,410
a lot of the training tuning and test

00:20:02,130 --> 00:20:08,670
data are biased they're hidden behind

00:20:04,410 --> 00:20:13,130
closed doors and this situation makes it

00:20:08,670 --> 00:20:16,230
really ripe to have a model that is very

00:20:13,130 --> 00:20:18,180
accurate but without knowing whether

00:20:16,230 --> 00:20:22,950
it's actually achieving the goal that we

00:20:18,180 --> 00:20:25,290
wanted the emergence of these distribute

00:20:22,950 --> 00:20:27,720
data marketplaces are critical for

00:20:25,290 --> 00:20:28,830
providing better data and increasing the

00:20:27,720 --> 00:20:32,370
accessibility of that data and the

00:20:28,830 --> 00:20:33,900
transparency of it they're also going to

00:20:32,370 --> 00:20:35,940
be critical for providing independent

00:20:33,900 --> 00:20:38,610
data that brings transparency to the

00:20:35,940 --> 00:20:42,360
outcomes of machine learning models from

00:20:38,610 --> 00:20:44,160
other companies the data interfaces are

00:20:42,360 --> 00:20:45,900
critical for helping us gut check

00:20:44,160 --> 00:20:49,890
whether or not the outcomes match our

00:20:45,900 --> 00:20:53,250
goals and they can help us be more

00:20:49,890 --> 00:20:54,960
creative about evaluating this the

00:20:53,250 --> 00:20:57,690
success metrics that we use to evaluate

00:20:54,960 --> 00:21:01,710
the models so they're more likely to

00:20:57,690 --> 00:21:04,830
match our goals and finally those data

00:21:01,710 --> 00:21:07,230
interfaces can help detect bias early

00:21:04,830 --> 00:21:08,610
and help us reward people particularly

00:21:07,230 --> 00:21:10,440
for providing data that's going to

00:21:08,610 --> 00:21:13,200
eliminate that bias or fill the gaps

00:21:10,440 --> 00:21:16,440
this to me is a healthy data ecosystem

00:21:13,200 --> 00:21:18,150
and that's what it should look like and

00:21:16,440 --> 00:21:19,230
we need we really need to invest in

00:21:18,150 --> 00:21:22,950
these and we need help I did it because

00:21:19,230 --> 00:21:26,580
of ecosystems if we want future AI

00:21:22,950 --> 00:21:29,160
headlines to look more like this and not

00:21:26,580 --> 00:21:36,650
like this thank you very much

00:21:29,160 --> 00:21:36,650

YouTube URL: https://www.youtube.com/watch?v=0cZBbkn9M2M


