Title: Keynote: Digital Echoes: Understanding Mass Violence with Data and Statistics - Patrick Ball
Publication date: 2018-10-25
Playlist: Open Source Summit Europe & ELC + OpenIoT Summit Europe 2018
Description: 
	Keynote: Digital Echoes: Understanding Mass Violence with Data and Statistics - Patrick Ball, Director of Research, Human Rights Data Analysis Group

Data about mass violence can seem to offer insights into patterns: is violence getting better, or worse, over time? Is violence directed more against men or women? But in human rights data collection, we (usually) don’t know what we don’t know – and worse, what we don’t know is likely to be systematically different from what we do know.

This talk will explore the assumption that nearly every project using data must make: that the data are representative of reality in the world. We will explore how, contrary to the standard assumption, statistical patterns in raw data tend to be quite different than patterns in the world. Statistical patterns in data tend to reflect how the data were collected rather than changes in the real-world phenomena data purport to represent.

Using analysis of mortality in Chadian prisons in the 1980s, killings in Iraq 2005-2010, homicides committed by police in the US 2005-2011, killings in the conflict in Syria, and analysis of genocide in Guatemala in 1982-1983, this talk will contrast patterns in raw data with estimates of total patterns of violence – where the estimates correct for heterogeneous underreporting. The talk will show how biases in raw data can -- sometimes -- be addressed through estimation. The examples will be grounded in their use in public debates and in expert testimony in criminal trials for genocide and war crimes. 

About Patrick Ball
Patrick Ball has spent more than twenty years conducting quantitative analysis for truth commissions, non-governmental organizations, international criminal tribunals, and United Nations missions in El Salvador, Ethiopia, Guatemala, Haiti, South Africa, Chad, Sri Lanka, East Timor, Sierra Leone, South Africa, Kosovo, Liberia, Perú, Colombia, the Democratic Republic of Congo, and Syria. Patrick began working in the human rights field in El Salvador in 1991. From 1993 to 2003, he worked in several capacities in the Science and Human Rights Program at the American Association for the Advancement of Science, where he began recruiting colleagues to build HRDAG. From 2003 to 2013 he continued to develop HRDAG from within Benetech, a nonprofit technology company in Silicon Valley. From 2013 through 2015, Patrick was Executive Director of HRDAG; on December 1, 2015, he became the Director of Research. Patrick provided testimony in two cases at the International Criminal Tribunal for the former Yugoslavia, the first in the trial of Slobodan Milošević, the former President of Serbia. He provided technical advice to the Special Court in Sierra Leone and the International Criminal Court. In 2013 he provided expert testimony in Guatemala’s Supreme Court in the trial of General José Efraín Ríos Montt, the de-facto president of Guatemala in 1982-1983. Gen. Ríos was found guilty of genocide and crimes against humanity; it was the first time ever that a former head of state was found guilty of genocide in his own country.
Captions: 
	00:00:00,000 --> 00:00:03,899
thank you I'm really psyched to be here

00:00:01,949 --> 00:00:05,640
and I'll tell you as we go through the

00:00:03,899 --> 00:00:08,580
talk line particularly happy to be

00:00:05,640 --> 00:00:10,349
talking to you today as was mentioned in

00:00:08,580 --> 00:00:12,450
the intro I've been at this for actually

00:00:10,349 --> 00:00:15,389
nearly coming up on 30 years I started

00:00:12,450 --> 00:00:17,369
1991 in El Salvador building databases

00:00:15,389 --> 00:00:20,730
and I think like a lot of data people my

00:00:17,369 --> 00:00:22,350
career has gone from sequel to machine

00:00:20,730 --> 00:00:24,539
learning I mean that's kind of this sort

00:00:22,350 --> 00:00:26,250
of the arc that a lot of us have and in

00:00:24,539 --> 00:00:28,320
the course of working through with these

00:00:26,250 --> 00:00:30,179
various partners whose logos I'm putting

00:00:28,320 --> 00:00:32,279
up behind me their truth Commission's

00:00:30,179 --> 00:00:34,860
from places from El Salvador to South

00:00:32,279 --> 00:00:36,480
Africa to Sierra Leone to East Timor to

00:00:34,860 --> 00:00:38,640
international criminal tribunals and

00:00:36,480 --> 00:00:40,739
I'll talk about a few of them to local

00:00:38,640 --> 00:00:43,020
groups groups looking for their lost

00:00:40,739 --> 00:00:45,059
family members who've been murdered and

00:00:43,020 --> 00:00:47,250
their bodies hidden one of the things

00:00:45,059 --> 00:00:48,960
that we work with every single time is

00:00:47,250 --> 00:00:51,660
to try to figure out what the truth

00:00:48,960 --> 00:00:53,610
means what is it that happened and how

00:00:51,660 --> 00:00:56,010
can we use that to try to bring an end

00:00:53,610 --> 00:00:58,289
to this kind of violence but to do that

00:00:56,010 --> 00:01:00,629
we get to use all these really cool

00:00:58,289 --> 00:01:02,489
tools and I've never shown these slides

00:01:00,629 --> 00:01:04,199
before I give talks like this a lot but

00:01:02,489 --> 00:01:06,510
I haven't had a chance to thank people

00:01:04,199 --> 00:01:08,310
who write the backend stack all the

00:01:06,510 --> 00:01:08,520
stuff that we use every day thanks so

00:01:08,310 --> 00:01:10,439
much

00:01:08,520 --> 00:01:12,180
and the front-end stack the stuff that

00:01:10,439 --> 00:01:14,430
we use to do the analysis I really

00:01:12,180 --> 00:01:16,470
appreciate it and so if any of you have

00:01:14,430 --> 00:01:18,600
committed to any of these projects thank

00:01:16,470 --> 00:01:22,740
you very much I really really appreciate

00:01:18,600 --> 00:01:24,360
it and my crew does to my team so when

00:01:22,740 --> 00:01:27,630
we do this kind of work we're always

00:01:24,360 --> 00:01:31,130
facing people who apologize for mass

00:01:27,630 --> 00:01:35,280
violence they tell us grotesque lies

00:01:31,130 --> 00:01:37,290
about that they use to attempt to excuse

00:01:35,280 --> 00:01:39,930
this violence they deny that it happened

00:01:37,290 --> 00:01:41,579
they blame the victims this is common

00:01:39,930 --> 00:01:44,280
and this is common of course in our

00:01:41,579 --> 00:01:47,070
world today and what human rights

00:01:44,280 --> 00:01:49,500
campaigns are able to do the reason that

00:01:47,070 --> 00:01:51,060
we're sometimes successful is that we

00:01:49,500 --> 00:01:52,979
speak with the moral voice of the

00:01:51,060 --> 00:01:56,719
victims that moral voice is founded on

00:01:52,979 --> 00:01:59,700
our claim that what we're saying is true

00:01:56,719 --> 00:02:01,890
we have to be right and in particular

00:01:59,700 --> 00:02:03,570
statistics have to be right and by

00:02:01,890 --> 00:02:05,219
statistics I want to include machine

00:02:03,570 --> 00:02:07,350
learning which I'll go along with some

00:02:05,219 --> 00:02:09,629
of the Stanford folks call statistical

00:02:07,350 --> 00:02:12,090
learning but we have to get it right and

00:02:09,629 --> 00:02:13,470
I want to explain a tiny bit about what

00:02:12,090 --> 00:02:15,180
I mean by right and

00:02:13,470 --> 00:02:17,100
what happens when it isn't right how

00:02:15,180 --> 00:02:18,780
that can go horribly wrong so I'm gonna

00:02:17,100 --> 00:02:21,660
give you three examples and let me start

00:02:18,780 --> 00:02:24,270
with the trial of a really really

00:02:21,660 --> 00:02:27,210
terrible war criminal I mean the one on

00:02:24,270 --> 00:02:29,190
the left in white not the other guy I

00:02:27,210 --> 00:02:30,810
didn't get to try him but I did

00:02:29,190 --> 00:02:32,610
participate in the trial of former

00:02:30,810 --> 00:02:35,010
president of Chad Hutchins habré's an

00:02:32,610 --> 00:02:38,550
expert witness and what we had in that

00:02:35,010 --> 00:02:40,530
trial were thousands of documents that

00:02:38,550 --> 00:02:42,030
were originally found by a researcher

00:02:40,530 --> 00:02:44,310
from Human Rights Watch Reed Brody in

00:02:42,030 --> 00:02:46,170
the top photo as a pile of trash

00:02:44,310 --> 00:02:48,180
discovered in an abandoned prison and

00:02:46,170 --> 00:02:50,010
when the documents were reviewed they

00:02:48,180 --> 00:02:51,290
turned out to be the operational records

00:02:50,010 --> 00:02:54,060
of the secret police

00:02:51,290 --> 00:02:57,900
so those documents were cleaned up they

00:02:54,060 --> 00:03:00,150
were organized and we took about 12,000

00:02:57,900 --> 00:03:02,580
high-resolution photographs of those

00:03:00,150 --> 00:03:04,709
documents and what we did with the

00:03:02,580 --> 00:03:07,110
documents in particular is focus on a

00:03:04,709 --> 00:03:10,020
particular kind of document that daily

00:03:07,110 --> 00:03:11,580
situation report in the daily situation

00:03:10,020 --> 00:03:13,380
report we had the number of prisoners

00:03:11,580 --> 00:03:15,060
that were held at the beginning of the

00:03:13,380 --> 00:03:16,709
day the number that were held at the end

00:03:15,060 --> 00:03:18,570
of the day and the differential the

00:03:16,709 --> 00:03:20,580
Delta between those two values is

00:03:18,570 --> 00:03:22,470
accounted for by the number who were

00:03:20,580 --> 00:03:23,730
released new prisoners who were received

00:03:22,470 --> 00:03:26,220
prisoners who were transferred to other

00:03:23,730 --> 00:03:28,260
places and prisoners who had died

00:03:26,220 --> 00:03:29,670
through the course of the day if you

00:03:28,260 --> 00:03:30,750
divide the number of people who died

00:03:29,670 --> 00:03:32,970
through the course of the day by the

00:03:30,750 --> 00:03:35,640
number live in the morning you get the

00:03:32,970 --> 00:03:37,709
crude mortality rate and this turns out

00:03:35,640 --> 00:03:40,230
to be a crucial piece of this analysis

00:03:37,709 --> 00:03:41,730
so first you can just plot over time the

00:03:40,230 --> 00:03:43,560
number of prisoners held by day and you

00:03:41,730 --> 00:03:45,750
can see that at one point there's an

00:03:43,560 --> 00:03:47,489
enormous influx of new prisoners most of

00:03:45,750 --> 00:03:50,220
these prisoners are prisoners of war in

00:03:47,489 --> 00:03:55,380
the war that Chad was waging against

00:03:50,220 --> 00:03:58,080
Libya but that in the desert and so this

00:03:55,380 --> 00:03:59,970
status of prisoners of war turns out to

00:03:58,080 --> 00:04:01,980
be critical for the legal regime that we

00:03:59,970 --> 00:04:04,860
can bring to bear in the trial of former

00:04:01,980 --> 00:04:07,170
President ha Bray because the crude

00:04:04,860 --> 00:04:09,239
mortality rate that I mentioned earlier

00:04:07,170 --> 00:04:12,540
was extraordinarily high in fact it

00:04:09,239 --> 00:04:15,900
peaked at about 0.62 prisoners per

00:04:12,540 --> 00:04:17,549
hundred per day that's an extraordinary

00:04:15,900 --> 00:04:19,290
mortality rate for those of you who are

00:04:17,549 --> 00:04:21,660
not mathematical demographers let me

00:04:19,290 --> 00:04:25,710
give you some context on that that's 90

00:04:21,660 --> 00:04:27,060
to 540 times greater than normal adult

00:04:25,710 --> 00:04:30,060
mortality

00:04:27,060 --> 00:04:33,060
in chat for men it's worse than other

00:04:30,060 --> 00:04:34,470
prisoner contexts for example it's one

00:04:33,060 --> 00:04:36,990
point three to four point one times

00:04:34,470 --> 00:04:38,669
greater than the mortality rate of us

00:04:36,990 --> 00:04:39,419
prisoners in Japanese custody during

00:04:38,669 --> 00:04:42,389
World War two

00:04:39,419 --> 00:04:45,510
that's relevant because at the Tokyo

00:04:42,389 --> 00:04:47,580
Tribunal in 1948 after World War two the

00:04:45,510 --> 00:04:50,130
treatment of u.s. prisoners by Japanese

00:04:47,580 --> 00:04:53,340
authorities was judged a war crime

00:04:50,130 --> 00:04:55,710
it was also approximately the same rate

00:04:53,340 --> 00:04:58,020
greater than the mortality of German

00:04:55,710 --> 00:04:59,910
prisoners in Soviet custody during World

00:04:58,020 --> 00:05:03,810
War two so this is very very high

00:04:59,910 --> 00:05:07,710
mortality and that image is a Senegalese

00:05:03,810 --> 00:05:10,320
newspaper drawing of me I love that

00:05:07,710 --> 00:05:13,800
photo that's my avatar all over the net

00:05:10,320 --> 00:05:15,750
now the verdict in the case of the

00:05:13,800 --> 00:05:20,780
extraordinary African chambers cited

00:05:15,750 --> 00:05:24,090
this evidence in particular as a way of

00:05:20,780 --> 00:05:25,979
rebutting the defense claimed that

00:05:24,090 --> 00:05:28,710
mortality in the prison was somehow

00:05:25,979 --> 00:05:29,970
normal it was somehow reasonable and I

00:05:28,710 --> 00:05:32,340
want to tie this back to my original

00:05:29,970 --> 00:05:34,440
concern what were what we're doing in

00:05:32,340 --> 00:05:37,200
human rights data analysis is trying to

00:05:34,440 --> 00:05:40,050
push back on grotesque lies we're trying

00:05:37,200 --> 00:05:42,419
to push back on apologies for mass

00:05:40,050 --> 00:05:45,090
violence and in fact the judges in the

00:05:42,419 --> 00:05:48,450
case saw precisely that usage and cited

00:05:45,090 --> 00:05:50,100
our evidence for that purpose to reject

00:05:48,450 --> 00:05:51,630
president habré's defense that

00:05:50,100 --> 00:05:56,669
conditions in the prison were nothing

00:05:51,630 --> 00:05:58,139
extraordinary so that's a win and I want

00:05:56,669 --> 00:05:59,910
to mark that as a win because human

00:05:58,139 --> 00:06:02,280
rights doesn't get very many human

00:05:59,910 --> 00:06:04,560
rights is a tiny community of activists

00:06:02,280 --> 00:06:06,630
and community members and the families

00:06:04,560 --> 00:06:08,970
of the victims and the survivors of mass

00:06:06,630 --> 00:06:11,700
atrocity pushing back on the full weight

00:06:08,970 --> 00:06:14,039
of governments so we don't get many wins

00:06:11,700 --> 00:06:16,080
and when we do get one we celebrate it

00:06:14,039 --> 00:06:17,160
it's important this is a former head of

00:06:16,080 --> 00:06:22,410
state who will spend the rest of his

00:06:17,160 --> 00:06:24,240
life in prison so yeah now let me give

00:06:22,410 --> 00:06:25,979
you another example and here I want to

00:06:24,240 --> 00:06:28,139
look this is a project that's very much

00:06:25,979 --> 00:06:30,660
in progress here we're looking for

00:06:28,139 --> 00:06:33,510
Phocis pandas Dena's hidden graves in

00:06:30,660 --> 00:06:35,520
Mexico these hidden graves often contain

00:06:33,510 --> 00:06:37,020
the bodies of people who can disappeared

00:06:35,520 --> 00:06:39,240
people who've been kidnapped from their

00:06:37,020 --> 00:06:40,289
families murdered and their bodies have

00:06:39,240 --> 00:06:40,920
been hidden and their families would

00:06:40,289 --> 00:06:42,960
very much

00:06:40,920 --> 00:06:44,730
like to know what happened to them so

00:06:42,960 --> 00:06:47,040
let's model it as a machine learning

00:06:44,730 --> 00:06:49,140
problem where are those graves and let's

00:06:47,040 --> 00:06:51,480
figure out how can we use some kind of

00:06:49,140 --> 00:06:53,730
element ml model to predict where we are

00:06:51,480 --> 00:06:56,880
likely to find those graves in order to

00:06:53,730 --> 00:06:59,160
guide search resources right to

00:06:56,880 --> 00:07:00,420
prioritize where it is that we're going

00:06:59,160 --> 00:07:03,120
to look for bodies

00:07:00,420 --> 00:07:07,350
so there's 2455 municipio sort counties

00:07:03,120 --> 00:07:09,420
in Mexico and we know each year about 75

00:07:07,350 --> 00:07:11,460
counties in which graves are found now

00:07:09,420 --> 00:07:13,890
there are lots more graves than that but

00:07:11,460 --> 00:07:16,050
you can imagine that if people in your

00:07:13,890 --> 00:07:19,410
community are regularly being abducted

00:07:16,050 --> 00:07:22,200
and murdered and never turn up you might

00:07:19,410 --> 00:07:24,090
be reluctant to make a complaint to the

00:07:22,200 --> 00:07:26,040
local police who you might suspect of

00:07:24,090 --> 00:07:28,320
being involved in those disappearances

00:07:26,040 --> 00:07:29,960
so we have a lot of disappearances we

00:07:28,320 --> 00:07:32,790
have a lot of graves that go unreported

00:07:29,960 --> 00:07:36,180
we have another 75 or so counties in

00:07:32,790 --> 00:07:37,740
which we can be sure because of good

00:07:36,180 --> 00:07:38,940
contacts in the local communities that

00:07:37,740 --> 00:07:40,920
there are not grapes these are peaceful

00:07:38,940 --> 00:07:42,750
places Mexico's violence is highly

00:07:40,920 --> 00:07:45,300
heterogeneous so it's distributed

00:07:42,750 --> 00:07:48,150
unevenly so now we have positive cases

00:07:45,300 --> 00:07:50,970
with negative cases and we have tons of

00:07:48,150 --> 00:07:52,650
independent information in an ml context

00:07:50,970 --> 00:07:54,090
these are features to a statistician

00:07:52,650 --> 00:07:56,850
these are covariance with tons of

00:07:54,090 --> 00:07:59,940
information about counties so let's just

00:07:56,850 --> 00:08:02,820
model it let's see so will 50/50 split

00:07:59,940 --> 00:08:04,860
random randomly split the cases we know

00:08:02,820 --> 00:08:06,990
about into test and training sets we'll

00:08:04,860 --> 00:08:09,330
train a train up a model will predict

00:08:06,990 --> 00:08:13,110
the test data and then we'll iterate

00:08:09,330 --> 00:08:16,170
that split test train excuse me split

00:08:13,110 --> 00:08:18,060
train test process a thousand times and

00:08:16,170 --> 00:08:19,890
what we'll find is that over the course

00:08:18,060 --> 00:08:21,480
of four years that we've been looking at

00:08:19,890 --> 00:08:23,880
more than a third of the time we can

00:08:21,480 --> 00:08:27,150
perfectly predict the counties that have

00:08:23,880 --> 00:08:29,670
graves now in recent years we have two

00:08:27,150 --> 00:08:32,580
ways of assessing the finding of grapes

00:08:29,670 --> 00:08:34,890
one is that graves are reported by

00:08:32,580 --> 00:08:38,160
family members or by the media that's

00:08:34,890 --> 00:08:40,170
one way of identifying graves a second

00:08:38,160 --> 00:08:42,870
way is that some prosecutors offices

00:08:40,170 --> 00:08:44,670
some local prosecutors are looking for

00:08:42,870 --> 00:08:46,260
and finding graves and we're hearing

00:08:44,670 --> 00:08:48,180
about it so we have two different models

00:08:46,260 --> 00:08:49,920
of that and I want us to I want to

00:08:48,180 --> 00:08:51,210
present I'm gonna excuse me emphasize

00:08:49,920 --> 00:08:53,400
that we are talking about two different

00:08:51,210 --> 00:08:54,450
models because now we can look at the

00:08:53,400 --> 00:08:56,370
interaction of those

00:08:54,450 --> 00:08:58,110
two different models and find out that

00:08:56,370 --> 00:09:00,630
there's different kinds of patterns of

00:08:58,110 --> 00:09:03,420
graves now this is a pretty complicated

00:09:00,630 --> 00:09:05,280
slide and this is a fairly short talk so

00:09:03,420 --> 00:09:07,950
what I want to emphasize is that just

00:09:05,280 --> 00:09:10,950
the stuff on the bottom the white dots

00:09:07,950 --> 00:09:13,740
are the counties in the test data for

00:09:10,950 --> 00:09:15,870
which we have strong weak we strongly

00:09:13,740 --> 00:09:17,820
believe do not have graves and indeed

00:09:15,870 --> 00:09:20,160
both models are clear that the

00:09:17,820 --> 00:09:23,700
probability of finding a grave in those

00:09:20,160 --> 00:09:27,510
counties is low the light blue circles

00:09:23,700 --> 00:09:30,990
are held out data from the prosecutors

00:09:27,510 --> 00:09:33,450
offices and the red dots are held out

00:09:30,990 --> 00:09:36,240
data from the media and the families and

00:09:33,450 --> 00:09:40,380
in both cases the model does a pretty

00:09:36,240 --> 00:09:42,030
good job of predicting the the model

00:09:40,380 --> 00:09:43,530
excuse me that is trained on that kind

00:09:42,030 --> 00:09:45,000
of data does a pretty good job of

00:09:43,530 --> 00:09:46,980
predicting that kind of grave but you

00:09:45,000 --> 00:09:48,870
can see there's actually different kinds

00:09:46,980 --> 00:09:50,460
of graves going on here and I want to

00:09:48,870 --> 00:09:51,810
kind of highlight this that machine

00:09:50,460 --> 00:09:53,520
learning models are really good at

00:09:51,810 --> 00:09:55,200
predicting things that are like the

00:09:53,520 --> 00:09:56,820
things they were trained on right

00:09:55,200 --> 00:09:59,160
machine learning is really good at

00:09:56,820 --> 00:10:00,990
assuming that the future is like the

00:09:59,160 --> 00:10:03,690
past conditional of course on all the

00:10:00,990 --> 00:10:05,370
features and the covariates but you know

00:10:03,690 --> 00:10:07,080
when stuff gets really different there

00:10:05,370 --> 00:10:08,430
are cases in which for example the

00:10:07,080 --> 00:10:10,860
prosecutor's office finds a grave but

00:10:08,430 --> 00:10:12,060
the families and media kinds of graves

00:10:10,860 --> 00:10:14,550
the kinds of graves that appeared in the

00:10:12,060 --> 00:10:16,890
families and and media reports not so

00:10:14,550 --> 00:10:19,050
good at reporting it now for the

00:10:16,890 --> 00:10:20,970
purposes of the application that we

00:10:19,050 --> 00:10:25,860
built these models for the most

00:10:20,970 --> 00:10:28,110
interesting cases on this graph are the

00:10:25,860 --> 00:10:30,900
green dots up in the upper right and the

00:10:28,110 --> 00:10:33,390
green dots represent counties in which

00:10:30,900 --> 00:10:36,780
neither reporting mechanism reported a

00:10:33,390 --> 00:10:38,820
grave but both models predict a high

00:10:36,780 --> 00:10:40,800
probability of finding graves those are

00:10:38,820 --> 00:10:42,810
places we should go search and when we

00:10:40,800 --> 00:10:44,700
showed those counties to people who know

00:10:42,810 --> 00:10:47,490
a lot about violence in Mexico they were

00:10:44,700 --> 00:10:50,610
like oh yeah oh yeah there's graves

00:10:47,490 --> 00:10:52,680
there for sure so not surprising why

00:10:50,610 --> 00:10:53,970
bother doing it and the reason that it's

00:10:52,680 --> 00:10:57,600
worth doing even though it's not

00:10:53,970 --> 00:10:59,880
surprising is that this is an incredibly

00:10:57,600 --> 00:11:02,100
powerful advocacy tool this is a way of

00:10:59,880 --> 00:11:03,870
demanding that state authorities in fact

00:11:02,100 --> 00:11:05,730
go and look for graves because the state

00:11:03,870 --> 00:11:06,750
authorities are often reluctant to do

00:11:05,730 --> 00:11:08,100
that and they're reluctant to do that

00:11:06,750 --> 00:11:09,720
for a variety of reasons but

00:11:08,100 --> 00:11:11,519
the most important one is the people who

00:11:09,720 --> 00:11:13,050
go look made themselves be at risk and

00:11:11,519 --> 00:11:15,029
if they can use a machine learning model

00:11:13,050 --> 00:11:17,279
out front they can say hey look I'm just

00:11:15,029 --> 00:11:20,970
going with the model it's a computer I'm

00:11:17,279 --> 00:11:22,860
just and that's a very powerful tool and

00:11:20,970 --> 00:11:24,660
we can come up with visualizations that

00:11:22,860 --> 00:11:26,940
distribute the probability of finding

00:11:24,660 --> 00:11:29,009
mass graves by county across Mexico and

00:11:26,940 --> 00:11:31,560
this can generate press attention and

00:11:29,009 --> 00:11:34,139
then help us in this advocacy campaign

00:11:31,560 --> 00:11:36,899
to bring state authorities to the search

00:11:34,139 --> 00:11:38,220
process so that's helpful that's useful

00:11:36,899 --> 00:11:39,930
that's valuable

00:11:38,220 --> 00:11:42,060
that's machine learning contributing

00:11:39,930 --> 00:11:43,560
positively to society does that mean

00:11:42,060 --> 00:11:44,670
that machine learning is necessarily

00:11:43,560 --> 00:11:48,509
positive for society

00:11:44,670 --> 00:11:49,949
yeah no no and in fact many many many

00:11:48,509 --> 00:11:52,319
machine learning applications are

00:11:49,949 --> 00:11:54,060
terribly detrimental to human rights in

00:11:52,319 --> 00:11:56,220
society and that's going to be my final

00:11:54,060 --> 00:11:58,170
example I want to talk about predictive

00:11:56,220 --> 00:12:03,569
policing predictive policing is

00:11:58,170 --> 00:12:06,300
catastrophic predictive policing is the

00:12:03,569 --> 00:12:08,430
use of machine learning to predict where

00:12:06,300 --> 00:12:11,850
crime is going to happen in the next

00:12:08,430 --> 00:12:15,120
iteration of the model and it uses

00:12:11,850 --> 00:12:18,420
police records to learn patterns about

00:12:15,120 --> 00:12:21,779
crime no of course not it learns

00:12:18,420 --> 00:12:23,160
patterns about police records now

00:12:21,779 --> 00:12:24,269
there's a big difference between police

00:12:23,160 --> 00:12:25,560
records and crime and we're going to

00:12:24,269 --> 00:12:28,350
talk about that through the course of

00:12:25,560 --> 00:12:29,730
this example using these patterns the

00:12:28,350 --> 00:12:32,490
computer will then make a prediction of

00:12:29,730 --> 00:12:35,699
course the model predict the most likely

00:12:32,490 --> 00:12:37,199
locations of future crime know the model

00:12:35,699 --> 00:12:41,399
is going to predict where crime will be

00:12:37,199 --> 00:12:42,959
detected in the future so then in the

00:12:41,399 --> 00:12:44,610
application in the use of these models

00:12:42,959 --> 00:12:47,069
additional police resources are

00:12:44,610 --> 00:12:50,120
dispatched the patience that the models

00:12:47,069 --> 00:12:54,509
predicted in order to prevent crime or

00:12:50,120 --> 00:12:56,670
to do what this is a big problem and let

00:12:54,509 --> 00:12:58,079
me explain what happens so let's look at

00:12:56,670 --> 00:13:01,259
drug crimes in Oakland I'm from San

00:12:58,079 --> 00:13:04,470
Francisco and so we looked at the bay

00:13:01,259 --> 00:13:08,250
area first on the right this blue

00:13:04,470 --> 00:13:13,589
pattern shows you the heat map of the

00:13:08,250 --> 00:13:15,029
density of drug use in Oakland based on

00:13:13,589 --> 00:13:16,769
Public Health Survey so this is

00:13:15,029 --> 00:13:20,370
completely outside the criminal justice

00:13:16,769 --> 00:13:21,510
process and as you can see the highest

00:13:20,370 --> 00:13:24,360
use of draw

00:13:21,510 --> 00:13:25,200
is in the corner in the far north which

00:13:24,360 --> 00:13:26,820
close to University of California

00:13:25,200 --> 00:13:28,680
perhaps not all that surprising

00:13:26,820 --> 00:13:30,630
University students tend to use a lot of

00:13:28,680 --> 00:13:32,730
drugs but for the most part drugs are

00:13:30,630 --> 00:13:36,450
ubiquitous in Oakland I'm from the Bay

00:13:32,730 --> 00:13:38,520
Area not surprising to me drug arrests

00:13:36,450 --> 00:13:40,380
are not distributed evenly across

00:13:38,520 --> 00:13:44,220
Oakland in fact they're concentrated in

00:13:40,380 --> 00:13:45,450
this corridor along this kind of western

00:13:44,220 --> 00:13:46,590
edge and for those of you know the bay

00:13:45,450 --> 00:13:48,720
area a little bit those are the flats

00:13:46,590 --> 00:13:51,870
that's international Avenue that's a

00:13:48,720 --> 00:13:54,840
primarily minority part of the city so

00:13:51,870 --> 00:13:57,600
drug crimes are everywhere but drug

00:13:54,840 --> 00:14:01,200
arrests are not what do we think the

00:13:57,600 --> 00:14:03,000
models are gonna do so what we did and

00:14:01,200 --> 00:14:05,820
by we here I mean my colleagues

00:14:03,000 --> 00:14:07,350
Christian lumen William Isaac ways is

00:14:05,820 --> 00:14:09,390
that we reimplemented one of the most

00:14:07,350 --> 00:14:13,470
popular of the predictive policing

00:14:09,390 --> 00:14:16,650
algorithms okay and we implemented it

00:14:13,470 --> 00:14:20,280
and then we started using it to predict

00:14:16,650 --> 00:14:21,300
crimes based on this data and what we're

00:14:20,280 --> 00:14:24,000
gonna what I'm going to do is show you

00:14:21,300 --> 00:14:24,960
that model running in a little animation

00:14:24,000 --> 00:14:27,210
in the next slide

00:14:24,960 --> 00:14:31,230
so here we go we're running the

00:14:27,210 --> 00:14:33,630
animation and the little dots in the

00:14:31,230 --> 00:14:35,010
grid are drug arrests and the data is

00:14:33,630 --> 00:14:36,990
being trained the models being trained

00:14:35,010 --> 00:14:38,790
and right here bang we turn it on and we

00:14:36,990 --> 00:14:40,470
make predictions and hello the

00:14:38,790 --> 00:14:42,690
predictions are precisely in the same

00:14:40,470 --> 00:14:44,270
locations where the data was observed

00:14:42,690 --> 00:14:48,150
when to where the arrests were observed

00:14:44,270 --> 00:14:49,440
huh who woulda guessed well anyone who

00:14:48,150 --> 00:14:51,840
knows anything about mo would have

00:14:49,440 --> 00:14:55,050
guessed that right so what happens if

00:14:51,840 --> 00:14:57,150
the underlying data is biased well what

00:14:55,050 --> 00:14:59,850
we do of course is that we recycle that

00:14:57,150 --> 00:15:02,160
bias now biased data leads to biased

00:14:59,850 --> 00:15:03,900
predictions and by bias I do not mean

00:15:02,160 --> 00:15:06,660
necessarily biased in a racial sense

00:15:03,900 --> 00:15:08,400
that's in fact what happens here but

00:15:06,660 --> 00:15:11,100
biased in a specifically technical sense

00:15:08,400 --> 00:15:13,320
okay because bias in a technical sense

00:15:11,100 --> 00:15:14,460
means that we are over particular one

00:15:13,320 --> 00:15:16,200
thing and we are under predicting

00:15:14,460 --> 00:15:19,020
something else in fact what we're under

00:15:16,200 --> 00:15:20,820
predicting here is white crime we're

00:15:19,020 --> 00:15:23,970
under predicting why crime we're

00:15:20,820 --> 00:15:25,980
teaching the police dispatchers through

00:15:23,970 --> 00:15:29,160
this ml that they should go to the

00:15:25,980 --> 00:15:30,930
places that they went before which again

00:15:29,160 --> 00:15:32,760
what does ml do it assumes that the

00:15:30,930 --> 00:15:35,040
futures like the past conditional on the

00:15:32,760 --> 00:15:38,639
covariance so I'm going to skip the neck

00:15:35,040 --> 00:15:40,470
slide because time is short go and say

00:15:38,639 --> 00:15:42,750
well okay that's great that's what

00:15:40,470 --> 00:15:44,970
happens when we just deploy the model

00:15:42,750 --> 00:15:48,269
and we just look at it but when police

00:15:44,970 --> 00:15:51,269
look at the model they react to it they

00:15:48,269 --> 00:15:53,339
don't just you know look at it and say

00:15:51,269 --> 00:15:55,440
that's cool they actually deploy more

00:15:53,339 --> 00:15:57,569
resources based on that model so what

00:15:55,440 --> 00:16:00,360
happens if more police go to the places

00:15:57,569 --> 00:16:02,279
that the model tells them to go well now

00:16:00,360 --> 00:16:04,649
we've got a feedback loop and what

00:16:02,279 --> 00:16:06,870
happens is that we'll do the same thing

00:16:04,649 --> 00:16:09,149
we're training again the model the

00:16:06,870 --> 00:16:10,920
little green dots are again training the

00:16:09,149 --> 00:16:13,800
model and then what happens is that when

00:16:10,920 --> 00:16:16,040
we turn the model on bang same

00:16:13,800 --> 00:16:20,600
predictions but the consequence is that

00:16:16,040 --> 00:16:22,980
targeted policing overwhelmingly hits

00:16:20,600 --> 00:16:24,000
the minority neighborhoods the

00:16:22,980 --> 00:16:26,480
neighborhoods that were already over

00:16:24,000 --> 00:16:28,860
policed to become more over police so

00:16:26,480 --> 00:16:31,649
machine learning in this context does

00:16:28,860 --> 00:16:34,920
not simply recycle racial disparities in

00:16:31,649 --> 00:16:36,660
policing MML amplifies the racial

00:16:34,920 --> 00:16:39,240
disparities in policing and this is

00:16:36,660 --> 00:16:40,620
catastrophic policing it already facing

00:16:39,240 --> 00:16:43,649
a crisis of legitimacy in the United

00:16:40,620 --> 00:16:45,480
States as a consequence of decades or

00:16:43,649 --> 00:16:49,920
some might argue centuries of unfair

00:16:45,480 --> 00:16:53,430
policing MML makes it worse this is bad

00:16:49,920 --> 00:16:55,380
this is bad so what I'd like to talk

00:16:53,430 --> 00:16:57,000
about is what's the difference between

00:16:55,380 --> 00:16:58,760
these two applications a human-rights

00:16:57,000 --> 00:17:01,139
difference between these two

00:16:58,760 --> 00:17:03,480
applications finding hidden graves and

00:17:01,139 --> 00:17:06,240
policing in grave predictions a false

00:17:03,480 --> 00:17:09,299
positive means that we waste some search

00:17:06,240 --> 00:17:10,559
resources we go look someplace where you

00:17:09,299 --> 00:17:12,179
know there probably aren't graves and a

00:17:10,559 --> 00:17:13,740
false negative means that we fail to

00:17:12,179 --> 00:17:15,360
search someplace we should have gone

00:17:13,740 --> 00:17:17,549
neither of these is actually worse than

00:17:15,360 --> 00:17:19,470
the status quo we're already not

00:17:17,549 --> 00:17:22,429
searching for very many graves so this

00:17:19,470 --> 00:17:24,990
is a cost but it's not a terrible cost

00:17:22,429 --> 00:17:26,520
for in predictive policing a false

00:17:24,990 --> 00:17:27,919
positive means that a neighborhood can

00:17:26,520 --> 00:17:30,540
be systematically over policed

00:17:27,919 --> 00:17:31,710
contributing to the perception of the

00:17:30,540 --> 00:17:33,659
citizens in that neighborhood that

00:17:31,710 --> 00:17:34,830
they're being harassed and that erodes

00:17:33,659 --> 00:17:37,260
trust between the police in the

00:17:34,830 --> 00:17:39,480
community furthermore a false negative

00:17:37,260 --> 00:17:43,230
means that police may fail to respond

00:17:39,480 --> 00:17:44,179
quickly to real crime and as I mentioned

00:17:43,230 --> 00:17:46,320
earlier if predictive policing

00:17:44,179 --> 00:17:48,690
reproduces prior patterns of police

00:17:46,320 --> 00:17:48,840
deployment police may perceive the model

00:17:48,690 --> 00:17:50,700
as

00:17:48,840 --> 00:17:53,610
accurate so we create a confirmation

00:17:50,700 --> 00:17:56,070
bias problem feedback then deepens

00:17:53,610 --> 00:17:58,559
existing biases and police deployment so

00:17:56,070 --> 00:18:01,049
we're not when we call it predictive

00:17:58,559 --> 00:18:02,669
policing in my team we generally say

00:18:01,049 --> 00:18:05,070
we're not-- we actually were predicting

00:18:02,669 --> 00:18:07,010
policing what we're really doing is just

00:18:05,070 --> 00:18:11,250
predicting where the police are gonna go

00:18:07,010 --> 00:18:12,450
so look why are ml models wrong all ml

00:18:11,250 --> 00:18:13,860
models are a little bit wrong right I

00:18:12,450 --> 00:18:15,960
mean if we don't get a little tiny bit

00:18:13,860 --> 00:18:17,909
of error we've over fit the model we've

00:18:15,960 --> 00:18:20,159
done a bad job that's just variance

00:18:17,909 --> 00:18:22,110
that's not a huge problem we can deal

00:18:20,159 --> 00:18:24,809
with a little bit of random error but

00:18:22,110 --> 00:18:26,909
bias is a bigger problem if we have data

00:18:24,809 --> 00:18:28,710
that is unrepresentative of the

00:18:26,909 --> 00:18:32,250
population to which we intend to apply

00:18:28,710 --> 00:18:34,520
the model the model is unlikely to be

00:18:32,250 --> 00:18:37,380
correct it is likely to reproduce

00:18:34,520 --> 00:18:40,529
whatever that bias is in the input side

00:18:37,380 --> 00:18:42,480
and in real world social data there's

00:18:40,529 --> 00:18:46,289
almost always a relationship between the

00:18:42,480 --> 00:18:48,149
observability of a phenomenon and the

00:18:46,289 --> 00:18:50,580
question we're trying to address right

00:18:48,149 --> 00:18:51,840
so can we observe crime we want to know

00:18:50,580 --> 00:18:53,549
where it is but the problem is we don't

00:18:51,840 --> 00:18:55,500
observe it all but our pattern of

00:18:53,549 --> 00:18:57,899
observation is systematically distorted

00:18:55,500 --> 00:19:00,299
it's not that we simply under observe

00:18:57,899 --> 00:19:02,399
the crime but we under observe some

00:19:00,299 --> 00:19:04,740
crime at a much greater rate than other

00:19:02,399 --> 00:19:06,149
crime and in particular in the United

00:19:04,740 --> 00:19:10,169
States that tends to be distributed by

00:19:06,149 --> 00:19:12,149
race and biased models then result and I

00:19:10,169 --> 00:19:14,220
want to make sure that none of us have

00:19:12,149 --> 00:19:16,770
the illusion that data visualizations or

00:19:14,220 --> 00:19:18,059
maps are anything other than statistical

00:19:16,770 --> 00:19:20,100
conclusions they are entirely

00:19:18,059 --> 00:19:22,110
statistical conclusions subject to all

00:19:20,100 --> 00:19:27,120
the concerns and biases that I'm raising

00:19:22,110 --> 00:19:29,039
here so what's the cost of being wrong

00:19:27,120 --> 00:19:30,779
and if there's anything you take away

00:19:29,039 --> 00:19:34,260
from my talk I really hope it's this

00:19:30,779 --> 00:19:37,260
it's the reasoning about what happens if

00:19:34,260 --> 00:19:39,990
we build an m/l model and we're wrong do

00:19:37,260 --> 00:19:41,130
we just waste some resources or do we

00:19:39,990 --> 00:19:43,919
affect people's lives do we destroy

00:19:41,130 --> 00:19:46,710
people's lives that's critical okay and

00:19:43,919 --> 00:19:49,860
more to the point who bears the cost of

00:19:46,710 --> 00:19:52,169
being wrong look if we serve a customer

00:19:49,860 --> 00:19:55,320
an ad for sneakers when they were

00:19:52,169 --> 00:19:57,630
looking for boots not such a big deal

00:19:55,320 --> 00:19:59,580
right first of all customer doesn't care

00:19:57,630 --> 00:20:01,710
she just ignores it and second of all

00:19:59,580 --> 00:20:02,580
the person who or the organization that

00:20:01,710 --> 00:20:06,390
bears the cost of

00:20:02,580 --> 00:20:09,029
tiny error is probably the client right

00:20:06,390 --> 00:20:11,549
so we haven't foisted the cost of being

00:20:09,029 --> 00:20:15,080
wrong onto the customer but if we make

00:20:11,549 --> 00:20:16,950
systematic errors in police deployment

00:20:15,080 --> 00:20:18,870
it's the neighborhoods and the

00:20:16,950 --> 00:20:20,730
communities that bear that cost not the

00:20:18,870 --> 00:20:21,559
machine learning vendor and probably not

00:20:20,730 --> 00:20:24,690
the police

00:20:21,559 --> 00:20:26,640
so what we've done is we have we have a

00:20:24,690 --> 00:20:29,100
real problem in how the costs are

00:20:26,640 --> 00:20:30,570
distributed it's very difficult to

00:20:29,100 --> 00:20:32,820
assess whether or not training data is

00:20:30,570 --> 00:20:34,919
correct and so this this problem can be

00:20:32,820 --> 00:20:38,340
very hard to detect I want to close with

00:20:34,919 --> 00:20:41,039
a story about how valuable it can be to

00:20:38,340 --> 00:20:45,240
get this right this guy in the upper

00:20:41,039 --> 00:20:47,700
left his name's Edgar Fernando Garcia he

00:20:45,240 --> 00:20:49,950
was a Guatemalan student and labor

00:20:47,700 --> 00:20:52,860
activist who one day in February of 1984

00:20:49,950 --> 00:20:55,440
left his home I just used me he left his

00:20:52,860 --> 00:20:57,480
office and he didn't turn up at home now

00:20:55,440 --> 00:20:59,399
people who were activists in Guatemala

00:20:57,480 --> 00:21:01,169
in the early 1980s had a pretty good

00:20:59,399 --> 00:21:03,409
idea of what might that might have meant

00:21:01,169 --> 00:21:05,880
when that happened so his wife Nanette

00:21:03,409 --> 00:21:07,559
immediately immediately that night

00:21:05,880 --> 00:21:08,760
started going to police stations around

00:21:07,559 --> 00:21:10,080
the city and she said have you arrested

00:21:08,760 --> 00:21:12,750
my husband do you have my husband she

00:21:10,080 --> 00:21:14,130
went to army bases have you arrested my

00:21:12,750 --> 00:21:15,960
husband do you have him she went to

00:21:14,130 --> 00:21:17,250
embassies she talked to international

00:21:15,960 --> 00:21:19,470
human rights groups there were

00:21:17,250 --> 00:21:21,539
international campaigns pressuring the

00:21:19,470 --> 00:21:22,889
Guatemalan government to give them

00:21:21,539 --> 00:21:25,679
information about what had happened to

00:21:22,889 --> 00:21:28,799
mr. Garcia the Guatemala girl was like

00:21:25,679 --> 00:21:30,240
no no I don't worry is I never seen him

00:21:28,799 --> 00:21:31,279
don't I don't know what me maybe the

00:21:30,240 --> 00:21:36,929
leftists killed him

00:21:31,279 --> 00:21:39,120
as I said grotesque lies well in 2006 we

00:21:36,929 --> 00:21:41,909
discovered the archives of the National

00:21:39,120 --> 00:21:45,240
Police 70 million pages of paper for

00:21:41,909 --> 00:21:49,950
warehouses full of paper covered in rat

00:21:45,240 --> 00:21:51,870
feces mold dead insects and we cleaned

00:21:49,950 --> 00:21:54,120
that all away my colleagues and I

00:21:51,870 --> 00:21:56,340
conducted a rolling random sample

00:21:54,120 --> 00:21:58,080
topologically sampling from this whole

00:21:56,340 --> 00:21:59,940
huge stack of paper in order to

00:21:58,080 --> 00:22:02,399
statistically characterize the documents

00:21:59,940 --> 00:22:04,080
the historians reading this stuff found

00:22:02,399 --> 00:22:07,019
some of those documents and they found a

00:22:04,080 --> 00:22:08,519
police sweep in exactly the area of the

00:22:07,019 --> 00:22:10,620
city where mr. Garcia was disappeared on

00:22:08,519 --> 00:22:12,659
the day he was disappeared and they

00:22:10,620 --> 00:22:14,610
identified the officers who arrested him

00:22:12,659 --> 00:22:16,559
those two officers in the upper right

00:22:14,610 --> 00:22:18,179
here were brought to court where

00:22:16,559 --> 00:22:21,600
said I'm sorry judge we were just

00:22:18,179 --> 00:22:23,340
following orders and the judge said

00:22:21,600 --> 00:22:25,799
thank you very much that is not a

00:22:23,340 --> 00:22:26,940
defense the way we usually say that the

00:22:25,799 --> 00:22:28,409
United States is they're just doing

00:22:26,940 --> 00:22:30,539
their job but it's exactly the same

00:22:28,409 --> 00:22:34,379
phrase you're guilty you go to jail for

00:22:30,539 --> 00:22:36,600
40 years but by the way please

00:22:34,379 --> 00:22:37,860
madam prosecutor if they were just

00:22:36,600 --> 00:22:39,830
following orders could you go arrest

00:22:37,860 --> 00:22:44,220
their boss

00:22:39,830 --> 00:22:47,039
that's this guy down here okay Hector

00:22:44,220 --> 00:22:48,389
Bolden la Cruz who was the director of

00:22:47,039 --> 00:22:50,460
the National Police at the time mr.

00:22:48,389 --> 00:22:52,889
Garcia was disappeared I was an expert

00:22:50,460 --> 00:22:53,850
witness in the trial against him and in

00:22:52,889 --> 00:22:55,740
my evidence

00:22:53,850 --> 00:22:56,909
I presented statistical analysis showing

00:22:55,740 --> 00:22:59,460
that the documents used by the

00:22:56,909 --> 00:23:01,320
historians were statistically very

00:22:59,460 --> 00:23:03,029
similar to the remain to other documents

00:23:01,320 --> 00:23:04,679
in the archive these were not documents

00:23:03,029 --> 00:23:05,940
that have been cherry-picked rather they

00:23:04,679 --> 00:23:07,799
were completely consistent with the

00:23:05,940 --> 00:23:10,769
normal bureaucratic functioning of the

00:23:07,799 --> 00:23:12,659
police like every bureaucracy orders are

00:23:10,769 --> 00:23:14,940
generated through strategies which

00:23:12,659 --> 00:23:16,590
become plans which become orders passing

00:23:14,940 --> 00:23:17,999
down the chain people who are at the

00:23:16,590 --> 00:23:18,869
operational level receive the orders

00:23:17,999 --> 00:23:21,419
they do what they're told

00:23:18,869 --> 00:23:22,919
they go out they come home they write

00:23:21,419 --> 00:23:24,869
reports the reports go back up the chain

00:23:22,919 --> 00:23:27,480
the documents were precisely of that

00:23:24,869 --> 00:23:29,639
kind and Colonel bull de la Cruz was

00:23:27,480 --> 00:23:34,799
convicted and sentenced to 40 years in

00:23:29,639 --> 00:23:37,409
prison after that trial the infant girl

00:23:34,799 --> 00:23:39,450
in her mother's arms there she's now a

00:23:37,409 --> 00:23:41,159
grown human rights lawyer in Guatemala

00:23:39,450 --> 00:23:43,940
and here she is embracing her

00:23:41,159 --> 00:23:46,700
grandmother mr. Garcia's mother

00:23:43,940 --> 00:23:50,269
you can feel looking at that photograph

00:23:46,700 --> 00:23:53,309
the relief that a family member feels

00:23:50,269 --> 00:23:56,659
when finally they learned to speak of

00:23:53,309 --> 00:23:59,490
their loved ones in the past tense

00:23:56,659 --> 00:24:01,019
that's what it means to do good human

00:23:59,490 --> 00:24:02,990
rights work and that's why it's so

00:24:01,019 --> 00:24:05,940
critical that we get it right and so

00:24:02,990 --> 00:24:08,960
critical that we avoid applications of

00:24:05,940 --> 00:24:11,320
our technology that make things worse

00:24:08,960 --> 00:24:15,310
thank you very much for your time

00:24:11,320 --> 00:24:15,310
[Applause]

00:24:15,930 --> 00:24:19,820

YouTube URL: https://www.youtube.com/watch?v=wGF6zAlE1hs


