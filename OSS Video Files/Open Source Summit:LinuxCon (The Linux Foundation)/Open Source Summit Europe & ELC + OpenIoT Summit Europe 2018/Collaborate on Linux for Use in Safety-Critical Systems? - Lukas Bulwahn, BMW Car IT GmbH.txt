Title: Collaborate on Linux for Use in Safety-Critical Systems? - Lukas Bulwahn, BMW Car IT GmbH
Publication date: 2018-10-25
Playlist: Open Source Summit Europe & ELC + OpenIoT Summit Europe 2018
Description: 
	Collaborate on Linux for Use in Safety-Critical Systems? - Lukas Bulwahn, BMW Car IT GmbH

There is a current industry trend to build fully autonomous systems. To reach this goal, industry must manage complex software systems with high performance, safety and security requirements.
The operating system is non-differentiating in these systems and it is intended to be used multiple times over the whole product portfolio for a long time span. These conditions make it appealing to use Linux as robust open-source operating system.

OSADL's SIL2LinuxMP project has done initial investigations to use Linux in safety-critical systems, and provides first results to the public. Its further success however depends on enlarging the development partnership to provide an attractive solution to the overall industry. The talk shall sketch goals of a collaboration, development and business models of a collaboration and call interested companies to participate in this new collaborative project. 

About Lukas Bulwahn
Lukas Bulwahn has received a diploma in computer science and a PhD in formal methods from Technische Universität München. Since 2012, he is working at BMW Car IT GmbH on research and development of an open-source software platform for autonomous driving systems. One part of this research has been the development of Adaptive AUTOSAR, a standard to develop future software in C++ on top of POSIX operating systems. As another part of this research, he considers if Linux is sufficient for use as operating system for autonomous driving, which ultimately led to his participation in the OSADL SIL2LinuxMP project, where this question is answered in an industrial collaboration. He has presented his work at various industrial and open-source conferences, including FOSDEM, ELCE 2017, Open-Source Summit Japan 2017, safe.tech 2018, Verification Futures 2018 and many more.
Captions: 
	00:00:00,030 --> 00:00:06,540
so welcome to my talk on collaboration

00:00:04,650 --> 00:00:11,880
on Linux for use in safety critical

00:00:06,540 --> 00:00:13,440
systems and this talk I'm going to

00:00:11,880 --> 00:00:16,590
sketch how we want to develop a

00:00:13,440 --> 00:00:19,410
collaboration collaboration project for

00:00:16,590 --> 00:00:22,820
using safety critical systems but before

00:00:19,410 --> 00:00:25,380
I start just a few things about myself

00:00:22,820 --> 00:00:29,250
I'm a functional safety software expert

00:00:25,380 --> 00:00:31,650
at BMW and I've been working on a number

00:00:29,250 --> 00:00:34,980
of collaboration collaboration projects

00:00:31,650 --> 00:00:38,340
since 2012 so in the very beginning

00:00:34,980 --> 00:00:41,340
we've looked into the idea of doing

00:00:38,340 --> 00:00:44,100
research on an open source software

00:00:41,340 --> 00:00:48,050
platform Fortnum is driving using open

00:00:44,100 --> 00:00:51,350
source components this led them to a

00:00:48,050 --> 00:00:54,600
development in an industrial

00:00:51,350 --> 00:00:58,739
collaboration forum adaptive autos are

00:00:54,600 --> 00:01:01,850
in the outers a consortium a well known

00:00:58,739 --> 00:01:06,510
player in the automotive industry and

00:01:01,850 --> 00:01:09,200
afterwards I said this has taken off I

00:01:06,510 --> 00:01:12,390
looked into the next challenge namely

00:01:09,200 --> 00:01:14,670
building up a safety argumentation for

00:01:12,390 --> 00:01:17,580
Linux and again doing that

00:01:14,670 --> 00:01:22,110
collaboratively in the social linux MP

00:01:17,580 --> 00:01:25,740
project at asado so I'm presenting here

00:01:22,110 --> 00:01:30,030
actually in three roles I was being I've

00:01:25,740 --> 00:01:32,960
been active in the Soto Linux MP project

00:01:30,030 --> 00:01:36,810
I'm also active in a new development

00:01:32,960 --> 00:01:39,990
that we are having in the Linux

00:01:36,810 --> 00:01:44,490
Foundation headed by Kate Stewart where

00:01:39,990 --> 00:01:47,640
we're discussing a new formation and I'm

00:01:44,490 --> 00:01:51,000
of course also an employee at BMW where

00:01:47,640 --> 00:01:53,430
I'm responsible for the operating system

00:01:51,000 --> 00:01:56,040
safety argumentation in autonomous

00:01:53,430 --> 00:01:59,880
driving and what I'm presenting here is

00:01:56,040 --> 00:02:02,700
really just the combination of the ideas

00:01:59,880 --> 00:02:07,979
and thoughts that we have in these three

00:02:02,700 --> 00:02:10,530
different projects so before I start

00:02:07,979 --> 00:02:13,500
actually with the it's a engineering

00:02:10,530 --> 00:02:15,600
challenge I want to give you a show

00:02:13,500 --> 00:02:17,040
introduction of why are we even

00:02:15,600 --> 00:02:21,060
considering that from a business

00:02:17,040 --> 00:02:25,920
perspective and what you're seeing in

00:02:21,060 --> 00:02:29,100
the in the recent years is that software

00:02:25,920 --> 00:02:31,470
innovations are taking over very

00:02:29,100 --> 00:02:33,890
established industries and the

00:02:31,470 --> 00:02:36,570
well-known Wall Street Journal article

00:02:33,890 --> 00:02:38,460
why software is eating the world from

00:02:36,570 --> 00:02:40,140
2011 really puts it on the point right

00:02:38,460 --> 00:02:42,660
these software innovations are

00:02:40,140 --> 00:02:45,570
disrupting all the industries even the

00:02:42,660 --> 00:02:47,340
very established ones and now companies

00:02:45,570 --> 00:02:50,070
in the mechatronic industry are really

00:02:47,340 --> 00:02:53,459
struggling with this change they have to

00:02:50,070 --> 00:02:55,650
consider will they give their profits to

00:02:53,459 --> 00:02:58,410
software vendors for the new

00:02:55,650 --> 00:03:00,410
technologies that will emerge or do they

00:02:58,410 --> 00:03:02,850
invest themselves to explore

00:03:00,410 --> 00:03:05,550
alternatives scenarios where they

00:03:02,850 --> 00:03:10,080
actually are in control of their

00:03:05,550 --> 00:03:13,590
software SEC and to understand at which

00:03:10,080 --> 00:03:15,840
point we are actually in the situation

00:03:13,590 --> 00:03:18,630
with safety-critical operating systems

00:03:15,840 --> 00:03:21,510
at the current date we have to

00:03:18,630 --> 00:03:25,380
understand we should kind of look at the

00:03:21,510 --> 00:03:28,680
history of operating systems in the in

00:03:25,380 --> 00:03:32,100
the in the general IT industry and

00:03:28,680 --> 00:03:34,830
that's really the history of UNIX so in

00:03:32,100 --> 00:03:36,750
the 1980s there were various UNIX

00:03:34,830 --> 00:03:39,330
operating systems in the market and

00:03:36,750 --> 00:03:41,760
those vendors largely controlled their

00:03:39,330 --> 00:03:45,180
users with a vendor lock-in and

00:03:41,760 --> 00:03:46,620
compatibility mess of course the

00:03:45,180 --> 00:03:50,160
industry reacted to that and they

00:03:46,620 --> 00:03:53,519
defined there was extenders but the the

00:03:50,160 --> 00:03:56,970
real disruption was really done by the

00:03:53,519 --> 00:03:59,130
Linux operating system in the 1990s

00:03:56,970 --> 00:04:02,910
Linux was born it was really formed as

00:03:59,130 --> 00:04:06,390
the coalition of tired users and the

00:04:02,910 --> 00:04:07,920
losers of this UNIX war and kind of by

00:04:06,390 --> 00:04:11,040
working together in an open source

00:04:07,920 --> 00:04:12,660
collaboration model it built a strong

00:04:11,040 --> 00:04:16,669
ecosystem of users and software

00:04:12,660 --> 00:04:20,070
companies so nowadays is clear that

00:04:16,669 --> 00:04:23,910
Linux has really led to world domination

00:04:20,070 --> 00:04:26,270
in the general IT industry the numbers

00:04:23,910 --> 00:04:29,030
speak for themselves but

00:04:26,270 --> 00:04:32,720
clearly its dominating in in public work

00:04:29,030 --> 00:04:35,000
klaus is dominating in the embedded

00:04:32,720 --> 00:04:37,940
market share on supercomputers on smart

00:04:35,000 --> 00:04:39,610
frozen and so on and the observation is

00:04:37,940 --> 00:04:43,039
that there are a number of companies and

00:04:39,610 --> 00:04:45,349
you'll know them they're active in the

00:04:43,039 --> 00:04:48,470
linux kernel development for their own

00:04:45,349 --> 00:04:50,240
linux based operating systems so they're

00:04:48,470 --> 00:04:52,280
controlling the software chain and stack

00:04:50,240 --> 00:04:54,139
even though they don't really build a

00:04:52,280 --> 00:04:56,330
software product but they might sell

00:04:54,139 --> 00:04:57,710
hardware they might sell services to

00:04:56,330 --> 00:05:00,560
someone but they don't actually have a

00:04:57,710 --> 00:05:04,580
software product but you engage in

00:05:00,560 --> 00:05:07,520
software development and as it happens

00:05:04,580 --> 00:05:09,590
this history could repeat itself as well

00:05:07,520 --> 00:05:11,870
so the mechatronic industry is now at

00:05:09,590 --> 00:05:14,960
the same crossroads for safety critical

00:05:11,870 --> 00:05:17,840
operating systems to run kind of complex

00:05:14,960 --> 00:05:19,849
algorithms and software and again they

00:05:17,840 --> 00:05:23,210
have the question which way they should

00:05:19,849 --> 00:05:25,550
go right how do they create a healthy

00:05:23,210 --> 00:05:28,520
ecosystem of safety critical operating

00:05:25,550 --> 00:05:30,199
systems and then focus on the innovative

00:05:28,520 --> 00:05:36,139
software functions that they want to

00:05:30,199 --> 00:05:39,889
develop so here I'm providing a proposal

00:05:36,139 --> 00:05:41,750
how this future could look like and it's

00:05:39,889 --> 00:05:44,050
up to the industry in the end to see

00:05:41,750 --> 00:05:46,880
what will really evolve from that and

00:05:44,050 --> 00:05:50,000
the proposal is of course that we use

00:05:46,880 --> 00:05:54,380
Linux as a safety critical operating

00:05:50,000 --> 00:05:56,240
system and the Linux kernel has a number

00:05:54,380 --> 00:05:58,219
of strengths and weaknesses the

00:05:56,240 --> 00:06:00,620
strengths are large development

00:05:58,219 --> 00:06:03,560
ecosystem I think this conference shows

00:06:00,620 --> 00:06:05,779
how large this ecosystem really is we

00:06:03,560 --> 00:06:08,960
have security capabilities we have

00:06:05,779 --> 00:06:10,690
multi-core support we have unmatched

00:06:08,960 --> 00:06:13,789
hardware support so there's no other

00:06:10,690 --> 00:06:17,479
operating system out there that provides

00:06:13,789 --> 00:06:19,580
provide support for so many drivers and

00:06:17,479 --> 00:06:23,990
hardware out there and we have many

00:06:19,580 --> 00:06:25,610
Linux experts at all levels but it's

00:06:23,990 --> 00:06:29,840
also missing a few things to actually

00:06:25,610 --> 00:06:32,539
use it in a safety critical system the

00:06:29,840 --> 00:06:34,610
one thing is that we need that many use

00:06:32,539 --> 00:06:38,570
cases are thinking about hard real-time

00:06:34,610 --> 00:06:39,400
capabilities that is more or less

00:06:38,570 --> 00:06:42,310
address

00:06:39,400 --> 00:06:44,680
the preempt are key patches getting the

00:06:42,310 --> 00:06:47,199
mainline is let's say a challenge but

00:06:44,680 --> 00:06:49,900
we're on the way in that direction the

00:06:47,199 --> 00:06:52,150
real-time denotes project is moving

00:06:49,900 --> 00:06:55,150
forward and the second and that's the

00:06:52,150 --> 00:06:57,250
part of my talk here is the question how

00:06:55,150 --> 00:07:01,139
can we prove that the development

00:06:57,250 --> 00:07:05,380
process is actually compliant to the

00:07:01,139 --> 00:07:07,270
objectives of the safety standard and to

00:07:05,380 --> 00:07:11,199
do that to address that second question

00:07:07,270 --> 00:07:14,800
we actually formed a collaboration

00:07:11,199 --> 00:07:18,039
project a couple of years ago called the

00:07:14,800 --> 00:07:20,050
seal to Linux MP project and its mission

00:07:18,039 --> 00:07:23,139
is really to provide the procedures and

00:07:20,050 --> 00:07:25,960
methods to qualify Linux on a multi-core

00:07:23,139 --> 00:07:31,810
embedded platform at safety integrity

00:07:25,960 --> 00:07:33,910
level 2 according to IEC 61508 so if you

00:07:31,810 --> 00:07:37,210
don't know what safety integrity level 2

00:07:33,910 --> 00:07:38,590
is in IEC 61508 is don't have to worry

00:07:37,210 --> 00:07:42,070
about that I'm going to explain a little

00:07:38,590 --> 00:07:43,570
bit about that later on but of course we

00:07:42,070 --> 00:07:45,700
don't only want to provide those

00:07:43,570 --> 00:07:47,860
procedures and methods we really want to

00:07:45,700 --> 00:07:49,900
show that this works right so we're

00:07:47,860 --> 00:07:52,990
showing that we can apply these these

00:07:49,900 --> 00:07:55,090
methods in a real-world system and then

00:07:52,990 --> 00:07:57,490
we show that we can actually have a

00:07:55,090 --> 00:08:00,060
potential for collaborating on these

00:07:57,490 --> 00:08:02,710
artifacts that we're creating because

00:08:00,060 --> 00:08:04,690
again it doesn't help if we show it only

00:08:02,710 --> 00:08:08,620
once and you have to redo it and there's

00:08:04,690 --> 00:08:11,229
no further use of the things that you

00:08:08,620 --> 00:08:13,300
did before it's a collaboration project

00:08:11,229 --> 00:08:15,789
with a number of companies so there are

00:08:13,300 --> 00:08:18,699
16 companies in total we have support

00:08:15,789 --> 00:08:21,520
from academia so from Alex a coalition

00:08:18,699 --> 00:08:24,520
for them from Miss Pross and from junior

00:08:21,520 --> 00:08:27,070
Laval from in Rio we have experts from

00:08:24,520 --> 00:08:29,979
certification bodies and we have of

00:08:27,070 --> 00:08:32,469
course the silver linux core working

00:08:29,979 --> 00:08:34,510
team so there were four people Nicolas

00:08:32,469 --> 00:08:37,419
McGuire who presented yesterday

00:08:34,510 --> 00:08:41,020
Andrea's plutchik Lucas don't boom and

00:08:37,419 --> 00:08:46,600
MCOs keidel and they were doing the main

00:08:41,020 --> 00:08:48,490
work in that project so before I can

00:08:46,600 --> 00:08:50,320
really explain what we were doing in

00:08:48,490 --> 00:08:51,880
this project I'm going to give a short

00:08:50,320 --> 00:08:54,670
introduction into functional

00:08:51,880 --> 00:08:57,790
because as I've heard in the keynotes

00:08:54,670 --> 00:09:00,310
and a number number of other conferences

00:08:57,790 --> 00:09:01,959
there's a would say a slight confusion

00:09:00,310 --> 00:09:06,399
what really the goal of functional

00:09:01,959 --> 00:09:09,060
safety is so functional safety is the

00:09:06,399 --> 00:09:12,160
part of the overall safety of a system

00:09:09,060 --> 00:09:14,470
that depends on the system operating

00:09:12,160 --> 00:09:16,959
correctly in response to its input

00:09:14,470 --> 00:09:19,420
including the safe management of likely

00:09:16,959 --> 00:09:22,720
operator errors hardware failures and

00:09:19,420 --> 00:09:25,060
environmental changes and the objective

00:09:22,720 --> 00:09:27,970
of functional safety is freedom from

00:09:25,060 --> 00:09:29,949
unacceptable risk a physical injury or

00:09:27,970 --> 00:09:33,399
of damage to the health of people either

00:09:29,949 --> 00:09:34,839
directly or indirectly so that's that's

00:09:33,399 --> 00:09:38,079
the definition that you can read from

00:09:34,839 --> 00:09:41,079
Wikipedia but let's break this down

00:09:38,079 --> 00:09:42,970
right so when we looking at functional

00:09:41,079 --> 00:09:45,550
safety we want to make sure that the

00:09:42,970 --> 00:09:48,730
system operates correctly but it should

00:09:45,550 --> 00:09:51,220
not only operate correctly as it was

00:09:48,730 --> 00:09:55,180
intended to be used but it also should

00:09:51,220 --> 00:09:57,880
operate correctly if the operator does

00:09:55,180 --> 00:10:00,279
something wrong if the hardware fails or

00:09:57,880 --> 00:10:02,949
if the environment changes beyond that

00:10:00,279 --> 00:10:06,370
what we initially thought that system to

00:10:02,949 --> 00:10:08,730
be built for and it's clear that when

00:10:06,370 --> 00:10:11,620
you have this kind of large space of

00:10:08,730 --> 00:10:13,839
possible changes that can affect your

00:10:11,620 --> 00:10:18,069
system you're not going to build a

00:10:13,839 --> 00:10:20,759
system that is a hundred percent perfect

00:10:18,069 --> 00:10:24,040
in that scenario so you are actually

00:10:20,759 --> 00:10:27,069
aware that these kind of changes can

00:10:24,040 --> 00:10:29,110
injure or harm a person in that

00:10:27,069 --> 00:10:32,589
surrounding but you want to make sure

00:10:29,110 --> 00:10:36,040
that you build a system where this risk

00:10:32,589 --> 00:10:38,589
of hurting someone is acceptable right

00:10:36,040 --> 00:10:41,199
so it's the freedom of the freedom of

00:10:38,589 --> 00:10:43,269
unacceptable risk means we have limited

00:10:41,199 --> 00:10:46,360
the possible harm to people to an

00:10:43,269 --> 00:10:51,250
acceptable level so how do we do that

00:10:46,360 --> 00:10:55,089
and the main course to do that is really

00:10:51,250 --> 00:10:58,569
an risk management activity and risk

00:10:55,089 --> 00:11:01,959
management is judging your system

00:10:58,569 --> 00:11:04,389
analyzing your system and to focus the

00:11:01,959 --> 00:11:04,840
Quality Assurance on the right aspects

00:11:04,389 --> 00:11:08,880
and there

00:11:04,840 --> 00:11:12,130
parts and it's not just that you write

00:11:08,880 --> 00:11:13,750
hundreds of documents to prove something

00:11:12,130 --> 00:11:16,570
to someone but it's really the thought

00:11:13,750 --> 00:11:18,460
process that you do to create these

00:11:16,570 --> 00:11:24,790
documents to document what you've

00:11:18,460 --> 00:11:27,310
thought of so but how do we decide if

00:11:24,790 --> 00:11:30,880
the risk is now acceptable or not but

00:11:27,310 --> 00:11:33,010
this is a highly subjective matter one

00:11:30,880 --> 00:11:34,990
person in the audience might say well

00:11:33,010 --> 00:11:37,420
I'm I'm crossing the road without

00:11:34,990 --> 00:11:39,190
looking left and right and I'm just

00:11:37,420 --> 00:11:40,960
passing it and he's he's of course

00:11:39,190 --> 00:11:43,150
taking a lot much larger risk in his

00:11:40,960 --> 00:11:45,190
life than someone who would say well I'm

00:11:43,150 --> 00:11:46,660
not going to leave the conference if it

00:11:45,190 --> 00:11:50,440
starts raining because I could slip

00:11:46,660 --> 00:11:51,970
right but again we have to if we're

00:11:50,440 --> 00:11:53,830
going to sell this product to to

00:11:51,970 --> 00:11:56,380
millions of people we have to come to a

00:11:53,830 --> 00:11:58,750
common agreement what that means that it

00:11:56,380 --> 00:12:01,450
is acceptable and this agreement has

00:11:58,750 --> 00:12:04,960
been laid out in global safety standards

00:12:01,450 --> 00:12:07,140
so there's IEC 61508 and it's a

00:12:04,960 --> 00:12:11,020
functional safety standard for

00:12:07,140 --> 00:12:13,780
electrical safety related systems and it

00:12:11,020 --> 00:12:15,040
provides a basis for all kinds of

00:12:13,780 --> 00:12:19,180
industries and you find further

00:12:15,040 --> 00:12:21,330
adaptions for different domains and try

00:12:19,180 --> 00:12:24,940
to tailor it for for their specific

00:12:21,330 --> 00:12:29,350
scenarios that they have and you can now

00:12:24,940 --> 00:12:31,780
take this safety standard and you read

00:12:29,350 --> 00:12:34,030
it as a goodnight lecture it's about

00:12:31,780 --> 00:12:36,460
10,000 pages so it's gonna take you a

00:12:34,030 --> 00:12:38,560
while and you try to summarize that on

00:12:36,460 --> 00:12:42,010
half the slide right so then you would

00:12:38,560 --> 00:12:43,810
say well if you want to design a safe

00:12:42,010 --> 00:12:46,480
system there are two things that you

00:12:43,810 --> 00:12:49,209
should consider first of all you do a

00:12:46,480 --> 00:12:51,730
system design and the system analysis so

00:12:49,209 --> 00:12:54,820
you analyze your system you know which

00:12:51,730 --> 00:12:58,110
parts have to be of high quality so that

00:12:54,820 --> 00:13:01,360
the system does not harm a person and

00:12:58,110 --> 00:13:04,360
then you assign safety integrity levels

00:13:01,360 --> 00:13:06,550
to those parts and to those properties

00:13:04,360 --> 00:13:10,930
of the system that are relevant for you

00:13:06,550 --> 00:13:14,320
and the safety standard gives you kind

00:13:10,930 --> 00:13:17,530
of a guideline of considering four

00:13:14,320 --> 00:13:18,310
levels so one two civil for wear so one

00:13:17,530 --> 00:13:20,860
means it's kind

00:13:18,310 --> 00:13:23,010
the low safety impact where sulphur

00:13:20,860 --> 00:13:26,020
means it has a very high safety impact

00:13:23,010 --> 00:13:28,900
if that system does not work so when I'm

00:13:26,020 --> 00:13:30,610
talking about a SIL two level we're

00:13:28,900 --> 00:13:34,060
considering that it has a medium impact

00:13:30,610 --> 00:13:36,370
on the overall safety of that system and

00:13:34,060 --> 00:13:40,600
then the question is of course so how do

00:13:36,370 --> 00:13:43,420
I achieve a high quality high quality

00:13:40,600 --> 00:13:47,440
system element and the answer that the

00:13:43,420 --> 00:13:49,860
safety standards provide is well it's

00:13:47,440 --> 00:13:52,570
really difficult to say that there's one

00:13:49,860 --> 00:13:54,760
possible way of doing it but they say

00:13:52,570 --> 00:13:56,560
it's generally the development process

00:13:54,760 --> 00:13:59,589
that you go through and that has to be

00:13:56,560 --> 00:14:02,380
rigorous so you develop those parts with

00:13:59,589 --> 00:14:04,839
this high safety integrity level with a

00:14:02,380 --> 00:14:07,270
sufficient rigor and the safety

00:14:04,839 --> 00:14:09,610
standards tells you which objectives you

00:14:07,270 --> 00:14:11,550
want to meet at each development phase

00:14:09,610 --> 00:14:15,040
so you make sure that the overall

00:14:11,550 --> 00:14:20,410
product that you develop has a certain

00:14:15,040 --> 00:14:21,940
quality okay so let's let's look at move

00:14:20,410 --> 00:14:24,760
away from that kind of abstract

00:14:21,940 --> 00:14:27,730
description to a concrete example um and

00:14:24,760 --> 00:14:30,700
we're going to look at a robot um Robert

00:14:27,730 --> 00:14:32,680
that can move around and he can he has a

00:14:30,700 --> 00:14:34,780
possibility to harm people but he's

00:14:32,680 --> 00:14:37,720
really only it can only really harm a

00:14:34,780 --> 00:14:40,600
person if you kind of lose leaves of the

00:14:37,720 --> 00:14:43,089
blue area so here we have an example

00:14:40,600 --> 00:14:45,580
where you see that Robert is in the in

00:14:43,089 --> 00:14:49,810
the blue area and around that blue area

00:14:45,580 --> 00:14:52,750
is a red fence right and now the

00:14:49,810 --> 00:14:55,089
question is with this red fence is this

00:14:52,750 --> 00:14:57,100
system safe or not so maybe that's a

00:14:55,089 --> 00:15:01,560
question for the audience who believes

00:14:57,100 --> 00:15:01,560
that this system is safe with the fence

00:15:01,890 --> 00:15:06,660
okay so who believes that it's not safe

00:15:07,410 --> 00:15:13,420
okay

00:15:08,589 --> 00:15:16,209
and who believes that I didn't tell you

00:15:13,420 --> 00:15:19,120
enough about the system to even judge if

00:15:16,209 --> 00:15:21,430
it's safe or not safe okay so there's a

00:15:19,120 --> 00:15:23,620
large majority that understands that

00:15:21,430 --> 00:15:27,190
when I just give you a little bit of

00:15:23,620 --> 00:15:31,290
this knowledge you don't know if this

00:15:27,190 --> 00:15:33,510
can go wrong or not and of course so

00:15:31,290 --> 00:15:35,850
the questions that come up is well is

00:15:33,510 --> 00:15:37,530
this fence actually strong enough to

00:15:35,850 --> 00:15:39,900
hold back the robot from moving out of

00:15:37,530 --> 00:15:42,450
the area so this is something that we

00:15:39,900 --> 00:15:45,900
should consider further we have to know

00:15:42,450 --> 00:15:47,700
is it actually set up properly when when

00:15:45,900 --> 00:15:50,040
you start operating with it and does

00:15:47,700 --> 00:15:53,370
someone keep maintaining it right if the

00:15:50,040 --> 00:15:57,030
fence breaks down and you don't notice

00:15:53,370 --> 00:16:00,840
the robot could just leave that area and

00:15:57,030 --> 00:16:03,000
so let's consider a second example right

00:16:00,840 --> 00:16:06,270
we have again the robot and it's it's

00:16:03,000 --> 00:16:08,430
connected to the it's connected to a

00:16:06,270 --> 00:16:11,010
power supply it doesn't have its own

00:16:08,430 --> 00:16:13,770
battery so it can only work with that

00:16:11,010 --> 00:16:17,400
power supply and that power supplies in

00:16:13,770 --> 00:16:20,550
the middle of that blue circle and we

00:16:17,400 --> 00:16:22,770
have a line that connects this Robert to

00:16:20,550 --> 00:16:26,610
that power supply and now the question

00:16:22,770 --> 00:16:29,460
is again is this system safe and again

00:16:26,610 --> 00:16:32,460
we look at it in a result bond probably

00:16:29,460 --> 00:16:36,810
if we know a few more things about the

00:16:32,460 --> 00:16:39,530
system we know that the cord is shorter

00:16:36,810 --> 00:16:41,940
than the radius of that blue circle then

00:16:39,530 --> 00:16:44,250
what could possibly go wrong

00:16:41,940 --> 00:16:48,240
well the robber tries to move out of

00:16:44,250 --> 00:16:51,630
that blue area and either the rope or

00:16:48,240 --> 00:16:54,680
the the cable is really stopping that

00:16:51,630 --> 00:17:00,920
robot because it cannot move with its

00:16:54,680 --> 00:17:03,450
with its motor beyond and that cable or

00:17:00,920 --> 00:17:05,730
well it can actually do it the Hatter

00:17:03,450 --> 00:17:08,610
has a motor that's strong enough but

00:17:05,730 --> 00:17:10,530
then the cable will just unplug or it

00:17:08,610 --> 00:17:12,570
would break and then the robot will have

00:17:10,530 --> 00:17:16,860
to stay in that area because it doesn't

00:17:12,570 --> 00:17:19,230
have any further so it's also if we know

00:17:16,860 --> 00:17:23,160
the length of that that cable that

00:17:19,230 --> 00:17:25,290
system would be safe and now we have the

00:17:23,160 --> 00:17:28,140
third example we say well there's

00:17:25,290 --> 00:17:31,470
actually nothing physically around that

00:17:28,140 --> 00:17:33,840
robot and that's stopping it and the

00:17:31,470 --> 00:17:36,750
question now is that system safe and now

00:17:33,840 --> 00:17:39,390
we actually have to look much deeper

00:17:36,750 --> 00:17:42,270
into what is actually steering the robot

00:17:39,390 --> 00:17:44,510
right what is actually controlling that

00:17:42,270 --> 00:17:47,090
robot to stay within that the way

00:17:44,510 --> 00:17:49,760
and then if in cases a software system

00:17:47,090 --> 00:17:51,710
we have to understand software system

00:17:49,760 --> 00:17:54,710
the hardware and its interaction with

00:17:51,710 --> 00:17:57,290
the fit with all the devices on that on

00:17:54,710 --> 00:18:00,650
that robot to know if it will only be

00:17:57,290 --> 00:18:03,920
speared within that system and what we

00:18:00,650 --> 00:18:05,120
get out of these examples is that you

00:18:03,920 --> 00:18:07,910
really want if you really want to

00:18:05,120 --> 00:18:09,920
understand if your system is safe you

00:18:07,910 --> 00:18:12,230
need to understand your system

00:18:09,920 --> 00:18:15,140
sufficiently but you have to understand

00:18:12,230 --> 00:18:16,790
what are the important conditions for

00:18:15,140 --> 00:18:21,020
your system to make that proper

00:18:16,790 --> 00:18:23,720
assessment and also what was mentioned

00:18:21,020 --> 00:18:25,160
in I have to contradict to one of the

00:18:23,720 --> 00:18:27,950
things that was mentioned in the keynote

00:18:25,160 --> 00:18:29,950
yesterday morning there was the the the

00:18:27,950 --> 00:18:34,520
claim statement for the the statement

00:18:29,950 --> 00:18:37,010
that you would build a safe system from

00:18:34,520 --> 00:18:39,890
a bottom-up approach as we see in these

00:18:37,010 --> 00:18:42,250
examples it's clearly that I don't have

00:18:39,890 --> 00:18:44,540
to understand all its internals

00:18:42,250 --> 00:18:46,970
considering the fence considering the

00:18:44,540 --> 00:18:47,900
the cable I don't have to understand all

00:18:46,970 --> 00:18:50,059
its internals

00:18:47,900 --> 00:18:53,390
I can already judge on a very high level

00:18:50,059 --> 00:18:55,760
that the system can be safe because of

00:18:53,390 --> 00:18:57,980
that so system safety is something that

00:18:55,760 --> 00:19:01,700
you develop top-down not something that

00:18:57,980 --> 00:19:03,890
you develop bottom-up but let's look at

00:19:01,700 --> 00:19:06,110
let's play this game a little bit

00:19:03,890 --> 00:19:07,760
further and let's say it's there's a

00:19:06,110 --> 00:19:10,400
steering application that steers this

00:19:07,760 --> 00:19:12,050
robot and now the robot can only harm

00:19:10,400 --> 00:19:14,090
someone if the steering application

00:19:12,050 --> 00:19:17,570
steers the robot to lose that leave that

00:19:14,090 --> 00:19:19,340
blue area and now we look at kind of the

00:19:17,570 --> 00:19:20,870
architecture how this looks like there's

00:19:19,340 --> 00:19:23,090
a steering application there's gillip

00:19:20,870 --> 00:19:25,850
see it's there's a kernel there's

00:19:23,090 --> 00:19:28,640
hardware and now the question is is this

00:19:25,850 --> 00:19:30,530
software safe and here it's not of

00:19:28,640 --> 00:19:33,050
course much more difficult to to answer

00:19:30,530 --> 00:19:35,360
but it's clear that you have to consider

00:19:33,050 --> 00:19:37,040
the influences of gilepsy on the

00:19:35,360 --> 00:19:39,050
steering application the possible

00:19:37,040 --> 00:19:41,390
influences of the kernel on the steering

00:19:39,050 --> 00:19:42,380
application and the possible influences

00:19:41,390 --> 00:19:44,179
of the hardware on the steering

00:19:42,380 --> 00:19:47,390
application and then you can start

00:19:44,179 --> 00:19:48,890
judging if that system is safe or not so

00:19:47,390 --> 00:19:51,230
an alternative system some are

00:19:48,890 --> 00:19:52,970
alternatives we could come up with yet

00:19:51,230 --> 00:19:54,830
another architecture one could say well

00:19:52,970 --> 00:19:56,300
I have a steering application I have

00:19:54,830 --> 00:19:58,110
gilepsy of the kernel I have the

00:19:56,300 --> 00:19:59,640
hypervisor of the hardware

00:19:58,110 --> 00:20:02,940
and again the question is the system

00:19:59,640 --> 00:20:05,040
safe and all the questions that I

00:20:02,940 --> 00:20:07,440
mentions they are just repeating again

00:20:05,040 --> 00:20:08,910
right I'm asking what's the influence of

00:20:07,440 --> 00:20:10,679
G let's see what's the influence of the

00:20:08,910 --> 00:20:12,840
kernel what's the influence of the

00:20:10,679 --> 00:20:14,940
hypervisor what's the influence of the

00:20:12,840 --> 00:20:17,490
hardware so just by adding further

00:20:14,940 --> 00:20:19,890
complexity into my system I just have to

00:20:17,490 --> 00:20:22,110
answer more and more questions about my

00:20:19,890 --> 00:20:24,630
system and I have to do more work to

00:20:22,110 --> 00:20:27,570
understand if that system is safe or not

00:20:24,630 --> 00:20:29,280
so again if you really want to know if

00:20:27,570 --> 00:20:31,049
your system is safe you have to

00:20:29,280 --> 00:20:35,160
understand the systems system

00:20:31,049 --> 00:20:36,660
sufficiently so let's think about how we

00:20:35,160 --> 00:20:39,960
would use Linux and safety-critical

00:20:36,660 --> 00:20:43,710
applications now that we know these kind

00:20:39,960 --> 00:20:46,799
of basics of functional safety and if we

00:20:43,710 --> 00:20:48,990
take this general message to assess

00:20:46,799 --> 00:20:51,059
whether your system is safe you need to

00:20:48,990 --> 00:20:53,700
understand your system sufficiently and

00:20:51,059 --> 00:20:57,240
apply that to the Linux kernel it means

00:20:53,700 --> 00:20:59,400
well if your system safety depends on

00:20:57,240 --> 00:21:01,980
Linux you need to understand Linux

00:20:59,400 --> 00:21:05,730
sufficiently for your system context and

00:21:01,980 --> 00:21:08,549
use and understanding Linux includes two

00:21:05,730 --> 00:21:10,860
things first of all it means you

00:21:08,549 --> 00:21:13,950
actually have to know how a Linux works

00:21:10,860 --> 00:21:15,809
that's the obvious answer right you have

00:21:13,950 --> 00:21:17,760
to understand how the Linux works for

00:21:15,809 --> 00:21:19,500
your system context and the way you want

00:21:17,760 --> 00:21:21,690
to use it and if it actually does that

00:21:19,500 --> 00:21:24,570
what you ain't what you are intending to

00:21:21,690 --> 00:21:27,169
use it for and the second thing is of

00:21:24,570 --> 00:21:30,059
course you have to understand Linux as

00:21:27,169 --> 00:21:32,700
how was it developed what was the

00:21:30,059 --> 00:21:37,890
development process that was used that

00:21:32,700 --> 00:21:40,970
made this system exist right so you have

00:21:37,890 --> 00:21:43,110
to understand who's working on it and

00:21:40,970 --> 00:21:45,690
with which rigor are they're working

00:21:43,110 --> 00:21:50,700
who's checking that who's testing it and

00:21:45,690 --> 00:21:53,850
so on and this is this is something that

00:21:50,700 --> 00:21:59,250
Nicolas McGuire explained yesterday this

00:21:53,850 --> 00:22:01,590
is the assessment of the safety by a

00:21:59,250 --> 00:22:04,049
process argumentation so we want to show

00:22:01,590 --> 00:22:08,280
that there's a compliance to the

00:22:04,049 --> 00:22:11,040
objectives of a safety standard by a

00:22:08,280 --> 00:22:11,270
development process assessment and kind

00:22:11,040 --> 00:22:13,950
of

00:22:11,270 --> 00:22:17,630
putting that together on one slide it

00:22:13,950 --> 00:22:20,550
really is roughly the following

00:22:17,630 --> 00:22:22,350
argumentation we said Linux has been

00:22:20,550 --> 00:22:26,340
kind of continuously developed for the

00:22:22,350 --> 00:22:30,090
last 25 years and there are various

00:22:26,340 --> 00:22:32,340
people out there that state that the

00:22:30,090 --> 00:22:35,880
process how it has been developed has

00:22:32,340 --> 00:22:37,680
been continuously improved and so

00:22:35,880 --> 00:22:41,550
whenever there's something technical or

00:22:37,680 --> 00:22:44,280
procedural in the kernel development

00:22:41,550 --> 00:22:47,160
that is pressing to them the community

00:22:44,280 --> 00:22:50,130
addresses that we have seen that for

00:22:47,160 --> 00:22:52,050
technical incidents fixing bugs or

00:22:50,130 --> 00:22:54,300
redesigning certain parts of the system

00:22:52,050 --> 00:22:56,790
but also from a procedural point of view

00:22:54,300 --> 00:23:01,050
right or even from a social point of

00:22:56,790 --> 00:23:03,180
view now what you can actually do with

00:23:01,050 --> 00:23:06,180
open source software is that you don't

00:23:03,180 --> 00:23:08,280
have to just trust these statements of a

00:23:06,180 --> 00:23:10,980
few of those kernel developers that are

00:23:08,280 --> 00:23:12,780
saying that this continuous process

00:23:10,980 --> 00:23:16,410
improvement is in place but you can

00:23:12,780 --> 00:23:18,540
actually provide evidences that this

00:23:16,410 --> 00:23:22,050
process quality and process improvement

00:23:18,540 --> 00:23:24,210
really exists and now you take these

00:23:22,050 --> 00:23:26,910
evidences and these evidences can

00:23:24,210 --> 00:23:29,370
indicate that all the objectives of a

00:23:26,910 --> 00:23:31,110
safety integrity level - for some

00:23:29,370 --> 00:23:33,570
selected parts and for selected

00:23:31,110 --> 00:23:36,150
properties are actually met but that's

00:23:33,570 --> 00:23:39,420
the that's the the plan that you would

00:23:36,150 --> 00:23:41,550
go through to show that Linux has been

00:23:39,420 --> 00:23:44,160
developed or has been developed with a

00:23:41,550 --> 00:23:45,660
with a sufficient rigor for the selected

00:23:44,160 --> 00:23:49,920
parts and properties that you're

00:23:45,660 --> 00:23:52,140
interested in so but if you think about

00:23:49,920 --> 00:23:55,530
this argument a little bit you'll find

00:23:52,140 --> 00:23:59,040
out that the real difference now between

00:23:55,530 --> 00:24:01,920
a safety-critical Linux and the mainline

00:23:59,040 --> 00:24:03,810
Linux is really not the source code that

00:24:01,920 --> 00:24:07,230
you're looking at it's really the way

00:24:03,810 --> 00:24:09,930
that you use it right you understand

00:24:07,230 --> 00:24:12,150
your system and you understand Linux and

00:24:09,930 --> 00:24:14,490
you understand and you make sure that

00:24:12,150 --> 00:24:16,530
your system uses Linux based on those

00:24:14,490 --> 00:24:18,060
selected properties that you

00:24:16,530 --> 00:24:20,810
investigated and that you can assure

00:24:18,060 --> 00:24:24,140
that are working according to the

00:24:20,810 --> 00:24:24,140
expectations that you have

00:24:24,240 --> 00:24:32,100
and when I'm talking about well you have

00:24:28,800 --> 00:24:34,200
to understand a certain property of the

00:24:32,100 --> 00:24:36,510
last kernel when we were doing a system

00:24:34,200 --> 00:24:39,330
development we're not talking about a

00:24:36,510 --> 00:24:40,980
single person building a system we're

00:24:39,330 --> 00:24:46,680
usually talking about a larger

00:24:40,980 --> 00:24:50,370
organization a team or even a company

00:24:46,680 --> 00:24:53,000
building such a system and the

00:24:50,370 --> 00:24:56,130
organization's knowledge is really

00:24:53,000 --> 00:24:58,470
encoded not only in the individuals but

00:24:56,130 --> 00:25:01,590
it's also encoded in the processes you

00:24:58,470 --> 00:25:03,900
use and the methods you apply when you

00:25:01,590 --> 00:25:05,760
build your system so really you have to

00:25:03,900 --> 00:25:08,720
have these processes and methods

00:25:05,760 --> 00:25:11,310
established in your company then you can

00:25:08,720 --> 00:25:13,200
find out what are the qualities of your

00:25:11,310 --> 00:25:16,650
complex software system and what are the

00:25:13,200 --> 00:25:18,150
qualities of the Linux kernel and then

00:25:16,650 --> 00:25:22,620
you could actually make sure that when

00:25:18,150 --> 00:25:26,520
you build a system it's going to work or

00:25:22,620 --> 00:25:29,040
it's going to have the properties to

00:25:26,520 --> 00:25:32,760
claim it to be functionally safe under

00:25:29,040 --> 00:25:35,910
the under the constraints that you you

00:25:32,760 --> 00:25:38,040
were considering and of course to

00:25:35,910 --> 00:25:40,320
establish processes and methods the

00:25:38,040 --> 00:25:42,690
first step is education on these topics

00:25:40,320 --> 00:25:44,910
and that's really the key to your safety

00:25:42,690 --> 00:25:47,790
product development if you start using

00:25:44,910 --> 00:25:50,640
Linux in a safety critical system so you

00:25:47,790 --> 00:25:52,320
get more information on this the

00:25:50,640 --> 00:25:55,760
different activities that that are

00:25:52,320 --> 00:25:58,950
required in that in that respect at a

00:25:55,760 --> 00:26:01,350
summit tomorrow and we have the Linux

00:25:58,950 --> 00:26:04,200
and safety systems summit tomorrow at

00:26:01,350 --> 00:26:07,170
the Sheraton and if you're interested in

00:26:04,200 --> 00:26:10,710
that topic in greater detail just come

00:26:07,170 --> 00:26:12,720
by it starts tomorrow after day after

00:26:10,710 --> 00:26:15,680
the keynotes at 11 o'clock and goes

00:26:12,720 --> 00:26:19,320
until until 5:00 and it's also free of

00:26:15,680 --> 00:26:24,780
free of charge so just come by and join

00:26:19,320 --> 00:26:27,690
us yes so I'm now kind of stepping back

00:26:24,780 --> 00:26:31,380
what we did the last three years and we

00:26:27,690 --> 00:26:33,240
kind of in retrospective considered how

00:26:31,380 --> 00:26:35,700
the silver Linux and P project was

00:26:33,240 --> 00:26:38,820
running and we found out that we

00:26:35,700 --> 00:26:41,700
actually had a very successful exchange

00:26:38,820 --> 00:26:46,020
of ideas and education of the challenges

00:26:41,700 --> 00:26:48,030
that we were facing this result to a

00:26:46,020 --> 00:26:50,850
this resulted in a defined plan and

00:26:48,030 --> 00:26:53,490
compliance wrote that was reviewed by

00:26:50,850 --> 00:26:55,650
the project party participants and the

00:26:53,490 --> 00:26:59,070
Safety Authority which kind of sketches

00:26:55,650 --> 00:27:00,720
in which way you can do to your project

00:26:59,070 --> 00:27:02,880
development and we had some first

00:27:00,720 --> 00:27:04,620
technical insights in that area looking

00:27:02,880 --> 00:27:07,860
into system engineering methods for

00:27:04,620 --> 00:27:09,690
complex software looking into methods

00:27:07,860 --> 00:27:12,179
and tools for kernel investigations and

00:27:09,690 --> 00:27:15,540
understanding the existing Linux kernel

00:27:12,179 --> 00:27:17,970
verification tools and we understood

00:27:15,540 --> 00:27:20,760
also that it was really important that

00:27:17,970 --> 00:27:23,520
we keep on educating and exchanging

00:27:20,760 --> 00:27:25,470
ideas which we've been in the past in a

00:27:23,520 --> 00:27:30,150
number of three day workshops on

00:27:25,470 --> 00:27:32,040
different topics so but what we also

00:27:30,150 --> 00:27:36,720
learned is that there were a couple of

00:27:32,040 --> 00:27:39,090
things that we just did not expect when

00:27:36,720 --> 00:27:40,679
we started the project first of all we

00:27:39,090 --> 00:27:43,650
really organized it as a research

00:27:40,679 --> 00:27:46,950
project we didn't think about it how we

00:27:43,650 --> 00:27:49,500
would actually properly collaborate with

00:27:46,950 --> 00:27:51,570
each other and we also underestimated

00:27:49,500 --> 00:27:54,750
the difficulty of collaboration around

00:27:51,570 --> 00:27:56,630
functional safety we always thought okay

00:27:54,750 --> 00:27:59,940
this kind of this open-source model

00:27:56,630 --> 00:28:02,370
everyone understands and it should just

00:27:59,940 --> 00:28:04,440
apply right it applies in in software

00:28:02,370 --> 00:28:06,630
development so easy so why shouldn't it

00:28:04,440 --> 00:28:08,669
work on the functional safety but

00:28:06,630 --> 00:28:10,679
functional safety is a difficult and

00:28:08,669 --> 00:28:12,919
kind of mind-bending field it's very

00:28:10,679 --> 00:28:17,220
different from software engineering and

00:28:12,919 --> 00:28:19,260
also whereas in open-source software a

00:28:17,220 --> 00:28:21,570
collaboration model was already

00:28:19,260 --> 00:28:24,090
established it wasn't established in

00:28:21,570 --> 00:28:25,559
functional safety at that time and there

00:28:24,090 --> 00:28:28,080
was also a misunderstanding of that

00:28:25,559 --> 00:28:30,179
educational goal so we told the people

00:28:28,080 --> 00:28:32,850
well in the next three years you're

00:28:30,179 --> 00:28:34,740
going to learn how you can do this well

00:28:32,850 --> 00:28:37,710
and they left off and came back three

00:28:34,740 --> 00:28:40,260
and years later and asked okay so now I

00:28:37,710 --> 00:28:42,299
should know how to do it right but

00:28:40,260 --> 00:28:44,400
that's of course not the case what we

00:28:42,299 --> 00:28:46,919
meant was well if you follow us the next

00:28:44,400 --> 00:28:48,690
three years and you learn the next three

00:28:46,919 --> 00:28:51,539
years you're gonna be at that point it's

00:28:48,690 --> 00:28:55,080
not that the time bound was bound by

00:28:51,539 --> 00:28:57,059
I the the fact that time just goes

00:28:55,080 --> 00:28:59,190
forward it was the time that they took

00:28:57,059 --> 00:29:02,129
to learn and to understand and establish

00:28:59,190 --> 00:29:04,710
that in their organization we didn't

00:29:02,129 --> 00:29:07,109
have access to suitable hardware that

00:29:04,710 --> 00:29:09,149
was actually in the documentation for

00:29:07,109 --> 00:29:10,919
that for the collaboration and of course

00:29:09,149 --> 00:29:13,049
members that didn't participate at

00:29:10,919 --> 00:29:16,109
really difficult time to use make use of

00:29:13,049 --> 00:29:18,929
the results so now we're thinking about

00:29:16,109 --> 00:29:23,700
how do we move from from research to

00:29:18,929 --> 00:29:25,679
collaboration and for that we have to

00:29:23,700 --> 00:29:28,289
first kind of set the goals of that

00:29:25,679 --> 00:29:29,879
collaboration project and now our

00:29:28,289 --> 00:29:31,799
understanding the the goal of that

00:29:29,879 --> 00:29:34,169
collaboration project is that we have a

00:29:31,799 --> 00:29:36,659
shared development and working effort on

00:29:34,169 --> 00:29:39,029
a number of topics right we want to

00:29:36,659 --> 00:29:41,789
understand how we do safety engineering

00:29:39,029 --> 00:29:44,700
of a complex system we want to create

00:29:41,789 --> 00:29:47,849
the risk assessments of the kernel

00:29:44,700 --> 00:29:49,559
subsistence and and features we want to

00:29:47,849 --> 00:29:52,229
gather the evidences of kernel

00:29:49,559 --> 00:29:54,210
development process compliance develop

00:29:52,229 --> 00:29:56,399
supporting tools together and create

00:29:54,210 --> 00:29:59,039
material to train and educate engineers

00:29:56,399 --> 00:30:01,499
together so that's more or less the

00:29:59,039 --> 00:30:04,529
scope of activities that we're thinking

00:30:01,499 --> 00:30:06,299
about for our collaboration then of

00:30:04,529 --> 00:30:09,330
course if we want to do this we have to

00:30:06,299 --> 00:30:12,989
consider a number of conditions that we

00:30:09,330 --> 00:30:15,179
should follow we do need to establish a

00:30:12,989 --> 00:30:16,999
well-defined governance and project

00:30:15,179 --> 00:30:20,039
steering and in a neutral organization

00:30:16,999 --> 00:30:22,619
right you want you don't want to have a

00:30:20,039 --> 00:30:26,749
situation where someone thinks he's he's

00:30:22,619 --> 00:30:30,720
left out because of because of because

00:30:26,749 --> 00:30:33,179
of the common set up you have to

00:30:30,720 --> 00:30:36,899
maintain a good community health you

00:30:33,179 --> 00:30:40,759
want to make sure that that people

00:30:36,899 --> 00:30:43,919
continue to work nicely together that

00:30:40,759 --> 00:30:47,669
things are not dropped and that the

00:30:43,919 --> 00:30:49,769
community really grows you have to keep

00:30:47,669 --> 00:30:52,470
educating on functional safety and

00:30:49,769 --> 00:30:54,149
process assessment we didn't only

00:30:52,470 --> 00:30:57,389
consider that those was a good point but

00:30:54,149 --> 00:31:00,989
it's also kind of a prerequisite to get

00:30:57,389 --> 00:31:03,419
new members into the group we have to

00:31:00,989 --> 00:31:04,480
share a common system to focus on common

00:31:03,419 --> 00:31:07,750
activities

00:31:04,480 --> 00:31:10,179
again think of the robot example that I

00:31:07,750 --> 00:31:12,790
showed the activities that you would do

00:31:10,179 --> 00:31:15,370
are very different if you have the fence

00:31:12,790 --> 00:31:18,070
around it or you have the cable or you

00:31:15,370 --> 00:31:20,140
have yet another set up so if you really

00:31:18,070 --> 00:31:22,540
want to say what's important to do

00:31:20,140 --> 00:31:24,580
what's the important activities that you

00:31:22,540 --> 00:31:28,210
want to look into you have to have a

00:31:24,580 --> 00:31:30,340
common system otherwise everyone's going

00:31:28,210 --> 00:31:34,150
to do any something else and you don't

00:31:30,340 --> 00:31:36,700
come to a proper collaboration and we

00:31:34,150 --> 00:31:38,410
can't of course do that alone right we

00:31:36,700 --> 00:31:40,540
have to reach out to the different

00:31:38,410 --> 00:31:42,400
communities that are supporting that

00:31:40,540 --> 00:31:44,290
activity that's the Linux community the

00:31:42,400 --> 00:31:46,360
safety communities and the hardware

00:31:44,290 --> 00:31:49,570
vendors that have to support that

00:31:46,360 --> 00:31:51,520
otherwise we don't create a full system

00:31:49,570 --> 00:31:56,260
understand how the full system would

00:31:51,520 --> 00:31:58,360
work so here I'm really just sketching

00:31:56,260 --> 00:32:01,750
what a successful outcome of that

00:31:58,360 --> 00:32:04,780
collaboration would be and this is

00:32:01,750 --> 00:32:07,600
really an ambitious and challenging goal

00:32:04,780 --> 00:32:09,790
but let's go through it right we are

00:32:07,600 --> 00:32:12,250
thinking about creating the assets for a

00:32:09,790 --> 00:32:15,780
safe safety certification of a Linux

00:32:12,250 --> 00:32:18,070
based system and that assets they

00:32:15,780 --> 00:32:21,790
consist of a complete process

00:32:18,070 --> 00:32:25,360
description selected kernel features

00:32:21,790 --> 00:32:28,030
that we considered the tools that we

00:32:25,360 --> 00:32:31,000
considered and previous process

00:32:28,030 --> 00:32:33,280
assessments to give you confidence that

00:32:31,000 --> 00:32:35,440
you can move forward we have to show

00:32:33,280 --> 00:32:37,720
that this is again feasible with the

00:32:35,440 --> 00:32:40,929
restaurant system so it's not just

00:32:37,720 --> 00:32:43,120
paperwork that you don't know if you

00:32:40,929 --> 00:32:47,020
could trust or not and it has to of

00:32:43,120 --> 00:32:50,190
course be usable for someone else to to

00:32:47,020 --> 00:32:52,690
integrate in their own organization and

00:32:50,190 --> 00:32:56,710
that can be only used by properly

00:32:52,690 --> 00:32:58,059
educated system integrators it has it

00:32:56,710 --> 00:32:59,700
should be maintained over an

00:32:58,059 --> 00:33:03,309
industrial-grade

00:32:59,700 --> 00:33:06,270
product lifetime right so doesn't help

00:33:03,309 --> 00:33:09,370
you if you know that this is a one-time

00:33:06,270 --> 00:33:11,710
activity if you know that your product

00:33:09,370 --> 00:33:15,250
is going to be out with its customers

00:33:11,710 --> 00:33:16,900
for much longer time it has to be well

00:33:15,250 --> 00:33:18,040
known and accepted by the safety

00:33:16,900 --> 00:33:19,630
community

00:33:18,040 --> 00:33:22,000
the certification authorities and

00:33:19,630 --> 00:33:25,060
standardization bodies in multiple

00:33:22,000 --> 00:33:27,280
industries and it should be positively

00:33:25,060 --> 00:33:30,790
recognized and impacting the Linux

00:33:27,280 --> 00:33:33,580
kernel community and come with hardware

00:33:30,790 --> 00:33:37,150
collateral from multiple supporting

00:33:33,580 --> 00:33:40,330
vendors so these are the for me these

00:33:37,150 --> 00:33:42,790
are the important factors with which you

00:33:40,330 --> 00:33:46,330
could in the end judge if a

00:33:42,790 --> 00:33:49,120
collaboration project is working towards

00:33:46,330 --> 00:33:51,490
a successful outcome or not if we can

00:33:49,120 --> 00:33:55,200
reach all these goals then we have a

00:33:51,490 --> 00:33:58,240
really successful project in that domain

00:33:55,200 --> 00:34:00,760
so we were thinking about that and

00:33:58,240 --> 00:34:05,890
thinking about how you would set up such

00:34:00,760 --> 00:34:09,159
a project structure for for this kind of

00:34:05,890 --> 00:34:12,639
collaboration and we identified that

00:34:09,159 --> 00:34:16,540
there is a core steering team that

00:34:12,639 --> 00:34:18,370
should try try to bring the different

00:34:16,540 --> 00:34:20,440
project working groups that we came up

00:34:18,370 --> 00:34:22,840
with together and it comes with four

00:34:20,440 --> 00:34:25,929
roles but comes with the project manager

00:34:22,840 --> 00:34:28,960
he makes sure that things are moving

00:34:25,929 --> 00:34:30,280
forward in time and and fit to each

00:34:28,960 --> 00:34:32,889
other you have a Community Health

00:34:30,280 --> 00:34:36,040
Manager that makes sure everyone works

00:34:32,889 --> 00:34:38,129
nicely together and that you can onboard

00:34:36,040 --> 00:34:41,470
new people you have a functional safety

00:34:38,129 --> 00:34:44,770
architect that understands the system

00:34:41,470 --> 00:34:46,810
and make sure that the artifacts that

00:34:44,770 --> 00:34:49,060
the different groups create fit together

00:34:46,810 --> 00:34:51,550
and you have a functional safety manager

00:34:49,060 --> 00:34:54,129
that makes sure that the artifacts that

00:34:51,550 --> 00:34:56,560
are created are complete right you're

00:34:54,129 --> 00:34:58,780
not missing a piece so these are kind of

00:34:56,560 --> 00:35:01,890
the roles that you usually have in such

00:34:58,780 --> 00:35:06,940
an organization and that interact to get

00:35:01,890 --> 00:35:08,230
your your overall results done and then

00:35:06,940 --> 00:35:10,630
we have a number of project working

00:35:08,230 --> 00:35:13,510
groups so compliance and certification

00:35:10,630 --> 00:35:15,280
component Quality Assurance tooling

00:35:13,510 --> 00:35:17,950
development reference use case and

00:35:15,280 --> 00:35:19,510
incident and hazard monitoring so that

00:35:17,950 --> 00:35:20,980
was kind of the first sketch of the

00:35:19,510 --> 00:35:24,000
different topics that you would have to

00:35:20,980 --> 00:35:28,980
address in such a collaboration project

00:35:24,000 --> 00:35:31,720
of course we cannot just only talk about

00:35:28,980 --> 00:35:35,320
what this project will do

00:35:31,720 --> 00:35:37,810
but we also have to make to make clear

00:35:35,320 --> 00:35:42,609
but this project cannot do everything

00:35:37,810 --> 00:35:45,640
and very specifically I'm going to point

00:35:42,609 --> 00:35:50,109
out four points here right this project

00:35:45,640 --> 00:35:53,920
cannot really engineer your system to be

00:35:50,109 --> 00:35:56,230
safe if you're using that system you're

00:35:53,920 --> 00:35:58,720
using the collaterals the assets that

00:35:56,230 --> 00:36:01,270
will provide to you we still do not know

00:35:58,720 --> 00:36:03,849
how your system works right we can

00:36:01,270 --> 00:36:05,740
provide certain artifacts to you but you

00:36:03,849 --> 00:36:07,420
have to do the engineering on your side

00:36:05,740 --> 00:36:11,080
to make sure that it really fits

00:36:07,420 --> 00:36:14,080
together we also cannot ensure that you

00:36:11,080 --> 00:36:16,960
know how to apply this described process

00:36:14,080 --> 00:36:19,840
and methods we can educate you in that

00:36:16,960 --> 00:36:22,869
respect and we can provide you the

00:36:19,840 --> 00:36:25,150
guidance but in the end it's your or you

00:36:22,869 --> 00:36:29,800
and your organization that has to decide

00:36:25,150 --> 00:36:34,390
and has to make sure how to incorporate

00:36:29,800 --> 00:36:37,300
that into your processes we also cannot

00:36:34,390 --> 00:36:39,960
create an out of three Linux kernel for

00:36:37,300 --> 00:36:42,460
safety applications recall the

00:36:39,960 --> 00:36:44,980
argumentation that I sketched for the

00:36:42,460 --> 00:36:47,109
process compliance argument we said

00:36:44,980 --> 00:36:49,060
there's a continuous process improvement

00:36:47,109 --> 00:36:51,880
in place and if we now just start and

00:36:49,060 --> 00:36:54,339
create an out of three Linux kernel we

00:36:51,880 --> 00:36:58,140
immediately invalidate that

00:36:54,339 --> 00:37:01,210
argumentation because we're not

00:36:58,140 --> 00:37:04,630
expecting a further process improvement

00:37:01,210 --> 00:37:06,099
and we cannot relieve you from your

00:37:04,630 --> 00:37:09,220
responsibilities your legal obligations

00:37:06,099 --> 00:37:12,780
and liabilities you're putting the

00:37:09,220 --> 00:37:17,170
system out into the real world and

00:37:12,780 --> 00:37:21,970
that's you have to live with the

00:37:17,170 --> 00:37:23,349
consequences of that behavior so we were

00:37:21,970 --> 00:37:28,150
thinking about the different modes of

00:37:23,349 --> 00:37:30,550
collaboration and that requires an

00:37:28,150 --> 00:37:33,460
informal exchanges of experts common

00:37:30,550 --> 00:37:35,470
training shared development shared

00:37:33,460 --> 00:37:37,359
maintenance of evidences and a

00:37:35,470 --> 00:37:39,580
collaboration or in this case and we're

00:37:37,359 --> 00:37:42,400
going to see which of those modes of

00:37:39,580 --> 00:37:43,330
collaboration will be established in in

00:37:42,400 --> 00:37:46,060
this

00:37:43,330 --> 00:37:48,850
Russian project so of course they also

00:37:46,060 --> 00:37:51,190
as I said is its ambitious and a

00:37:48,850 --> 00:37:53,470
challenging task that were trying to

00:37:51,190 --> 00:37:58,000
address and there are a number of risks

00:37:53,470 --> 00:37:59,590
and opportunities when we do this rather

00:37:58,000 --> 00:38:02,740
than going now through all of them I

00:37:59,590 --> 00:38:05,500
just want to point out one thing that we

00:38:02,740 --> 00:38:06,790
discussed over the last last years that

00:38:05,500 --> 00:38:10,930
there are different conceptual

00:38:06,790 --> 00:38:13,600
approaches to come to a safety

00:38:10,930 --> 00:38:17,590
argumentation and what I've seen in the

00:38:13,600 --> 00:38:19,360
last year was that people were starting

00:38:17,590 --> 00:38:20,890
to thinking in camps they were saying

00:38:19,360 --> 00:38:22,390
well if you do it you can do it this way

00:38:20,890 --> 00:38:24,760
or you can do it that way

00:38:22,390 --> 00:38:27,190
and I'm only going to engage in this one

00:38:24,760 --> 00:38:29,230
way of doing it but really having

00:38:27,190 --> 00:38:31,690
different conceptual approaches can

00:38:29,230 --> 00:38:34,840
really result in an overall more robust

00:38:31,690 --> 00:38:38,230
argumentation right you know that it's

00:38:34,840 --> 00:38:40,090
not single single Larry singular

00:38:38,230 --> 00:38:41,890
thoughts that have led to that

00:38:40,090 --> 00:38:44,710
conclusion but they're really multiple

00:38:41,890 --> 00:38:46,270
ones and that's really the thing that we

00:38:44,710 --> 00:38:48,730
should try to address in that project

00:38:46,270 --> 00:38:52,090
that we actually come to objective

00:38:48,730 --> 00:38:57,790
assessments without thinking and in kind

00:38:52,090 --> 00:39:00,130
of different camps yes so this is more

00:38:57,790 --> 00:39:03,340
or less the things that we have

00:39:00,130 --> 00:39:06,340
discussed in various people in there in

00:39:03,340 --> 00:39:11,500
the industry and how we could

00:39:06,340 --> 00:39:14,410
collaborate on the use of of Linux in in

00:39:11,500 --> 00:39:16,360
safety critical systems so let's let's

00:39:14,410 --> 00:39:19,080
come to a conclusion I'm just going to

00:39:16,360 --> 00:39:22,630
summarize what I've told you in the last

00:39:19,080 --> 00:39:25,930
40 minutes so we want to we see that

00:39:22,630 --> 00:39:27,700
there's a need in industry to have an

00:39:25,930 --> 00:39:30,280
operating system for these complex

00:39:27,700 --> 00:39:33,700
algorithms and software suitable for

00:39:30,280 --> 00:39:35,470
safety critical systems and should be

00:39:33,700 --> 00:39:37,390
aware that functional safety is really

00:39:35,470 --> 00:39:40,750
about managing risk in your product

00:39:37,390 --> 00:39:43,210
development and you can only understand

00:39:40,750 --> 00:39:46,000
this risk with Linux based systems if

00:39:43,210 --> 00:39:48,790
you understand the system and you

00:39:46,000 --> 00:39:51,280
understand the kernel and this basis for

00:39:48,790 --> 00:39:53,950
understanding Linux in safety critical

00:39:51,280 --> 00:39:55,290
systems is available this is what we

00:39:53,950 --> 00:39:56,380
developed in this ultra-lux

00:39:55,290 --> 00:39:59,079
project

00:39:56,380 --> 00:40:01,450
now if you want to really extend this

00:39:59,079 --> 00:40:04,119
basis that we created it requires a

00:40:01,450 --> 00:40:06,700
larger industry collaboration I've

00:40:04,119 --> 00:40:09,779
sketched the technical challenges and

00:40:06,700 --> 00:40:11,739
the organizational proposal that I made

00:40:09,779 --> 00:40:13,960
to kind of move forward in that

00:40:11,739 --> 00:40:16,269
direction so really at the current point

00:40:13,960 --> 00:40:20,769
it's at the current stage that we are

00:40:16,269 --> 00:40:22,930
it's really now the question to all of

00:40:20,769 --> 00:40:25,180
Industry how is really going to continue

00:40:22,930 --> 00:40:27,009
right so the the future of really

00:40:25,180 --> 00:40:30,220
enabling Linux and safety applications

00:40:27,009 --> 00:40:32,680
is really up to all of us if we want to

00:40:30,220 --> 00:40:34,479
go in this crossroads one way or the

00:40:32,680 --> 00:40:38,880
other if you want to interested in that

00:40:34,479 --> 00:40:42,130
topic you can follow us tomorrow and the

00:40:38,880 --> 00:40:43,539
Linux and safety systems summit and

00:40:42,130 --> 00:40:46,979
you'll find the information on there on

00:40:43,539 --> 00:40:50,529
the webpage of the co-located events and

00:40:46,979 --> 00:41:13,690
that thanks for your attention and happy

00:40:50,529 --> 00:41:14,920
to answer any questions so the the

00:41:13,690 --> 00:41:20,559
question was is this knowledgebase

00:41:14,920 --> 00:41:22,839
already available somewhere so I think

00:41:20,559 --> 00:41:26,079
the last few weeks so actually the last

00:41:22,839 --> 00:41:29,739
few months actually Nicholas my choir

00:41:26,079 --> 00:41:33,249
and and and all the project participants

00:41:29,739 --> 00:41:35,710
were discussing to make that information

00:41:33,249 --> 00:41:38,589
publicly available we wanted to have it

00:41:35,710 --> 00:41:40,359
in place for this conference but I think

00:41:38,589 --> 00:41:42,460
there were a few things that technically

00:41:40,359 --> 00:41:44,369
didn't work out but I expect that it

00:41:42,460 --> 00:41:50,079
will be within the next two weeks

00:41:44,369 --> 00:41:52,329
available yeah yeah it's probably best

00:41:50,079 --> 00:41:55,719
if you just sent me your email address

00:41:52,329 --> 00:41:58,119
in them we can provide it to you it will

00:41:55,719 --> 00:42:01,119
be probably announced on the asado web

00:41:58,119 --> 00:42:02,289
page and there you can get the

00:42:01,119 --> 00:42:04,420
information but if you sent me your

00:42:02,289 --> 00:42:10,380
email I can inform you as well

00:42:04,420 --> 00:42:10,380
yeah any further questions

00:42:25,300 --> 00:42:30,890
so the question was does this continuous

00:42:28,340 --> 00:42:34,340
process improvement and breakdown as

00:42:30,890 --> 00:42:38,330
soon as you ship the product in your

00:42:34,340 --> 00:42:42,410
safety critical device car or robot or

00:42:38,330 --> 00:42:47,180
whatever so it doesn't really break down

00:42:42,410 --> 00:42:51,760
right what you did is you ensure that at

00:42:47,180 --> 00:42:54,860
the time where you ship the product you

00:42:51,760 --> 00:42:57,350
evaluate it if it's all the activities

00:42:54,860 --> 00:43:00,430
were up to a certain level of that

00:42:57,350 --> 00:43:02,990
state-of-the-art at that point and

00:43:00,430 --> 00:43:05,600
that's what that's what you do when you

00:43:02,990 --> 00:43:07,850
you ship your product now of course if

00:43:05,600 --> 00:43:10,420
you then say I'm not going to update my

00:43:07,850 --> 00:43:13,460
product after that you're probably

00:43:10,420 --> 00:43:15,920
probably really not a benefiting from

00:43:13,460 --> 00:43:18,170
any for the bug fixing or any kind of

00:43:15,920 --> 00:43:20,990
improvements that they make afterwards

00:43:18,170 --> 00:43:23,810
but you really have to then kind of grow

00:43:20,990 --> 00:43:26,660
with that product especially as state of

00:43:23,810 --> 00:43:29,990
the art moves forward you have to kind

00:43:26,660 --> 00:43:31,190
of consider that but you're not by the

00:43:29,990 --> 00:43:34,790
point where you're shipping it you're

00:43:31,190 --> 00:43:38,030
not putting your your customers or your

00:43:34,790 --> 00:43:40,640
users at a level of harm that you

00:43:38,030 --> 00:43:46,400
wouldn't accept right so it's not that

00:43:40,640 --> 00:43:48,290
you're not that that continues process

00:43:46,400 --> 00:43:50,420
improvement breaks down because of that

00:43:48,290 --> 00:43:52,310
you just have to adjust in your

00:43:50,420 --> 00:44:00,100
development afterwards in a different

00:43:52,310 --> 00:44:00,100
way yeah

00:44:02,840 --> 00:44:06,630
well or you have a stable Colonel

00:44:05,160 --> 00:44:09,120
because that's exactly the process

00:44:06,630 --> 00:44:11,760
that's defined in the Linux kernel

00:44:09,120 --> 00:44:13,620
development that you have a stable

00:44:11,760 --> 00:44:17,450
maintenance kernel for a certain time

00:44:13,620 --> 00:44:23,040
and then a new version is released and

00:44:17,450 --> 00:44:41,340
again that is improved and maintained

00:44:23,040 --> 00:44:46,610
over time but if you don't know enough

00:44:41,340 --> 00:44:50,700
about your system I wouldn't say it's

00:44:46,610 --> 00:44:53,190
unsafe by default but for sure you

00:44:50,700 --> 00:44:56,070
should not know enough about your system

00:44:53,190 --> 00:44:59,040
to kind of make this argumentation that

00:44:56,070 --> 00:45:02,490
I made in the examples then there's a

00:44:59,040 --> 00:45:04,470
very very high risk as soon as you have

00:45:02,490 --> 00:45:07,470
a rather complex system that you

00:45:04,470 --> 00:45:11,130
overlooked something and then that would

00:45:07,470 --> 00:45:13,550
hit you in that in that case at some

00:45:11,130 --> 00:45:13,550
point

00:45:20,180 --> 00:45:25,510
yes so yes the question was is there a

00:45:23,720 --> 00:45:28,789
difference between not safe and unsafe

00:45:25,510 --> 00:45:32,779
and I think you can consider when we say

00:45:28,789 --> 00:45:36,619
something is safe we have the definition

00:45:32,779 --> 00:45:39,829
of we did certain activities to make

00:45:36,619 --> 00:45:43,730
sure that we understand the system that

00:45:39,829 --> 00:45:47,930
we did a proper risk assessment that we

00:45:43,730 --> 00:45:50,289
can say that we have limited this risk

00:45:47,930 --> 00:45:52,880
of harming someone to some level of

00:45:50,289 --> 00:45:55,549
course if you did not do all these

00:45:52,880 --> 00:45:58,940
activities it doesn't mean that the

00:45:55,549 --> 00:46:01,730
system will immediately kill someone but

00:45:58,940 --> 00:46:04,039
you just don't know right and and I

00:46:01,730 --> 00:46:06,490
think that's the point right and you

00:46:04,039 --> 00:46:08,569
usually look at functional safety

00:46:06,490 --> 00:46:12,010
because you know that there is a

00:46:08,569 --> 00:46:16,220
potential for it right if I develop a

00:46:12,010 --> 00:46:19,670
robot that can move around and has some

00:46:16,220 --> 00:46:24,400
kind of possibility of harming someone

00:46:19,670 --> 00:46:27,230
and I program it to do whatever it wants

00:46:24,400 --> 00:46:30,079
do random movements it's highly likely

00:46:27,230 --> 00:46:32,380
that he's going to move somewhere where

00:46:30,079 --> 00:46:41,240
someone stands and then I'm just gonna

00:46:32,380 --> 00:46:52,070
yeah hurt that person okay

00:46:41,240 --> 00:46:55,570
Oh yep maybe in the back here so the

00:46:52,070 --> 00:46:55,570
question was what is there any

00:47:02,190 --> 00:47:05,910
yes so I think the question was is there

00:47:04,799 --> 00:47:09,690
something like a safety critical

00:47:05,910 --> 00:47:12,380
hypervisor um and I think the answer is

00:47:09,690 --> 00:47:14,549
as I've shown in the example right

00:47:12,380 --> 00:47:18,780
what do you would what's the definition

00:47:14,549 --> 00:47:21,150
of a safety critical hypervisor you have

00:47:18,780 --> 00:47:23,400
to look at the whole system and then the

00:47:21,150 --> 00:47:28,079
what you probably want to argue is that

00:47:23,400 --> 00:47:31,980
this hypervisor has no impact on the

00:47:28,079 --> 00:47:35,940
safety application that can from it from

00:47:31,980 --> 00:47:38,970
the hypervisor itself can make the the

00:47:35,940 --> 00:47:40,410
safety application go wrong but that

00:47:38,970 --> 00:47:43,559
doesn't really answer your question

00:47:40,410 --> 00:47:46,079
because you still have the question does

00:47:43,559 --> 00:47:49,319
the hardware make the safety application

00:47:46,079 --> 00:47:51,150
go wrong does the operating system that

00:47:49,319 --> 00:47:54,660
you use make the safety application go

00:47:51,150 --> 00:47:56,940
wrong does the G Lipsy make the safety

00:47:54,660 --> 00:47:59,640
application go wrong does the safety

00:47:56,940 --> 00:48:04,260
application in itself do something that

00:47:59,640 --> 00:48:06,180
the system would harm someone and the

00:48:04,260 --> 00:48:08,460
real question is really what's your

00:48:06,180 --> 00:48:11,339
intent of providing using that

00:48:08,460 --> 00:48:13,770
hypervisor what are the what's your

00:48:11,339 --> 00:48:16,440
expectation that lowers the impact of

00:48:13,770 --> 00:48:18,569
any of those elements and so far nobody

00:48:16,440 --> 00:48:21,779
has really answered that question

00:48:18,569 --> 00:48:27,150
what it really reduces the impact of

00:48:21,779 --> 00:48:28,740
certain elements but yeah that's that's

00:48:27,150 --> 00:48:30,119
really a system engineering activity

00:48:28,740 --> 00:48:31,589
that you have to go through but it

00:48:30,119 --> 00:48:35,720
doesn't you can kind of have this kind

00:48:31,589 --> 00:48:35,720
of default answer to that yep

00:48:50,100 --> 00:48:56,350
that does conflict with that for sure

00:48:54,490 --> 00:48:59,500
that statement certainly conflicts with

00:48:56,350 --> 00:49:01,300
what is presented but it also conflicts

00:48:59,500 --> 00:49:03,790
with hopefully your personal

00:49:01,300 --> 00:49:06,820
understanding of functional safety now

00:49:03,790 --> 00:49:08,830
that you've seen those examples right do

00:49:06,820 --> 00:49:11,650
you have to understand every line of

00:49:08,830 --> 00:49:13,420
that robot where the fence was around

00:49:11,650 --> 00:49:16,150
that blue area you have to understand

00:49:13,420 --> 00:49:19,120
every line of that or do you really want

00:49:16,150 --> 00:49:22,120
to spend the effort on building a proper

00:49:19,120 --> 00:49:24,370
fence do you want to spend the effort on

00:49:22,120 --> 00:49:26,950
making sure that that fence is installed

00:49:24,370 --> 00:49:28,720
properly but you want to make sure that

00:49:26,950 --> 00:49:31,300
you check every morning that that fence

00:49:28,720 --> 00:49:35,850
is still there you don't have to check

00:49:31,300 --> 00:49:35,850
every line for for building that system

00:49:42,060 --> 00:49:47,350
well you have to know a certain property

00:49:44,830 --> 00:49:49,600
of that fence the property of that fence

00:49:47,350 --> 00:49:53,380
that you have to know is its strong

00:49:49,600 --> 00:49:55,240
enough to stop the robot right you don't

00:49:53,380 --> 00:49:57,670
have to know other than that and what

00:49:55,240 --> 00:49:59,710
are the properties that are important

00:49:57,670 --> 00:50:02,170
for or what are this let's say sub

00:49:59,710 --> 00:50:05,200
properties for that property to hold

00:50:02,170 --> 00:50:06,730
that is that is the relevant things you

00:50:05,200 --> 00:50:08,920
don't have to you don't have to care

00:50:06,730 --> 00:50:11,860
about its color right the color is

00:50:08,920 --> 00:50:15,190
really not important for that for that

00:50:11,860 --> 00:50:17,110
fence if it holds back the robot the

00:50:15,190 --> 00:50:19,630
color is irrelevant and the same holds

00:50:17,110 --> 00:50:21,850
for code right if there's parts of code

00:50:19,630 --> 00:50:24,510
that determined the color of that

00:50:21,850 --> 00:50:27,490
operating system it's really irrelevant

00:50:24,510 --> 00:50:30,070
because you want to know what other

00:50:27,490 --> 00:50:36,490
really the properties that you want to

00:50:30,070 --> 00:50:39,760
guarantee but of course there are

00:50:36,490 --> 00:50:41,410
different approaches to it if you know

00:50:39,760 --> 00:50:42,580
every line you're going to spend a lot

00:50:41,410 --> 00:50:45,660
of effort but I don't know if it's

00:50:42,580 --> 00:50:45,660
really making the system safer

00:50:50,190 --> 00:50:55,450
yes so that's exactly the question how

00:50:53,110 --> 00:50:57,040
many what are the selected properties

00:50:55,450 --> 00:51:00,130
that are relevant for you and that you

00:50:57,040 --> 00:51:02,530
can only drive with a with a concrete

00:51:00,130 --> 00:51:05,230
system that would tell you okay what are

00:51:02,530 --> 00:51:09,270
the relevant parts and that's why we

00:51:05,230 --> 00:51:09,270
have a common system to discuss that

00:51:09,480 --> 00:51:15,460
okay yeah thanks for your time

00:51:13,300 --> 00:51:17,290
I already 10 minutes over so if you have

00:51:15,460 --> 00:51:19,660
further questions we should discuss that

00:51:17,290 --> 00:51:22,510
afterwards but yeah thanks for your time

00:51:19,660 --> 00:51:24,880
and if you want to know more just join

00:51:22,510 --> 00:51:28,420
tomorrow and we're gonna show more about

00:51:24,880 --> 00:51:30,360
the methods and process approaches that

00:51:28,420 --> 00:51:33,959
we want to take

00:51:30,360 --> 00:51:33,959

YouTube URL: https://www.youtube.com/watch?v=Use04MfL2ms


