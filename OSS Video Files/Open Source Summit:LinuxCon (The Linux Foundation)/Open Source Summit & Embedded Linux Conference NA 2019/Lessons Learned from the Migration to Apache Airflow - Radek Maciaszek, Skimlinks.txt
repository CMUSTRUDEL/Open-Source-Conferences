Title: Lessons Learned from the Migration to Apache Airflow - Radek Maciaszek, Skimlinks
Publication date: 2019-09-16
Playlist: Open Source Summit & Embedded Linux Conference NA 2019
Description: 
	Lessons Learned from the Migration to Apache Airflow - Radek Maciaszek, Skimlinks

Apache Airflow is an open-source tool for orchestrating complex workflows and data processing pipelines.In this talk, Radek Maciaszek will present his learnings from the migration of machine learning and big data processing pipelines to Apache Airflow.Radek will discuss examples of how are they using Airflow to power their company big data infrastructure where they analyze hundreds of terabytes of data. Examples will cover the building of the ETL pipeline and use of Airflow to manage the machine learning Spark pipeline workflow.This talk will cover the basic Airflow concepts and show real-life examples of how to define your own workflows in the Python code. The talk will finish with more advanced topics related to Apache Airflow, such as adding custom task operators, sensors and plugins as well as best practices and both the pros and cons of this tool.
Captions: 
	00:00:00,110 --> 00:00:06,720
so can you hear me a bit better right

00:00:04,920 --> 00:00:09,059
welcome everyone to the talk about

00:00:06,720 --> 00:00:10,920
lessons learned from the migration to

00:00:09,059 --> 00:00:14,250
Apache airflow

00:00:10,920 --> 00:00:19,010
my name is radicand I work as a chief

00:00:14,250 --> 00:00:21,840
architect Skimlinks so skimmings we do

00:00:19,010 --> 00:00:24,840
commercial content monetization I will

00:00:21,840 --> 00:00:26,670
explain a bit more about that also in my

00:00:24,840 --> 00:00:29,400
free time worked as a trainer at

00:00:26,670 --> 00:00:33,870
framework training where we do some big

00:00:29,400 --> 00:00:36,270
data related trainings I used to Orca

00:00:33,870 --> 00:00:38,879
CTO at a couple of companies including a

00:00:36,270 --> 00:00:42,180
4G data my lab and I was involved in

00:00:38,879 --> 00:00:45,450
many big data projects in the past with

00:00:42,180 --> 00:00:47,750
such companies as Orange FCA can

00:00:45,450 --> 00:00:52,199
Terrapin eggs and many many more

00:00:47,750 --> 00:00:54,960
so agenda for this talk as we start from

00:00:52,199 --> 00:00:56,730
telling you a bit more about what is

00:00:54,960 --> 00:00:58,890
that we do at skimmings

00:00:56,730 --> 00:01:01,170
like how what kind of data we process

00:00:58,890 --> 00:01:04,320
how do we do it with the air flow and

00:01:01,170 --> 00:01:07,830
why did we choose air flow to process

00:01:04,320 --> 00:01:10,290
the data the third one will be focusing

00:01:07,830 --> 00:01:12,390
on the air flow basic concept so if you

00:01:10,290 --> 00:01:13,350
don't know anything about air flow yet

00:01:12,390 --> 00:01:16,470
don't worry

00:01:13,350 --> 00:01:18,000
like I will give you some some basics so

00:01:16,470 --> 00:01:20,009
we talked about the components features

00:01:18,000 --> 00:01:22,710
and I will show you some sample code and

00:01:20,009 --> 00:01:24,330
then in the part two I will focus on

00:01:22,710 --> 00:01:26,400
some of the best practices that we

00:01:24,330 --> 00:01:29,400
developed over the last year

00:01:26,400 --> 00:01:31,710
Becker schemings and we mentioned few

00:01:29,400 --> 00:01:34,320
things about the deployment and what I

00:01:31,710 --> 00:01:36,500
like to call it the good the bad and the

00:01:34,320 --> 00:01:36,500
ugly

00:01:37,189 --> 00:01:45,299
so schemings

00:01:39,329 --> 00:01:47,250
data panel pipeline so the longer

00:01:45,299 --> 00:01:49,500
version of what we do at Skimlinks is we

00:01:47,250 --> 00:01:51,990
monetize the product links in the

00:01:49,500 --> 00:01:55,259
commerce related content to earn

00:01:51,990 --> 00:01:57,240
publishers a share of sales so I like to

00:01:55,259 --> 00:01:59,219
talk I like to call it a better version

00:01:57,240 --> 00:02:01,500
of advertising because we don't actually

00:01:59,219 --> 00:02:04,229
show and display advertising from the

00:02:01,500 --> 00:02:06,689
websites what we do is we give the

00:02:04,229 --> 00:02:10,920
publishers like a snippet of JavaScript

00:02:06,689 --> 00:02:13,349
code they drop it be included in their

00:02:10,920 --> 00:02:16,590
website and that JavaScript called scan

00:02:13,349 --> 00:02:19,049
all the links to the external merchants

00:02:16,590 --> 00:02:22,650
such as Amazon eBay and and so on and

00:02:19,049 --> 00:02:24,959
turns them into Athleta things so if the

00:02:22,650 --> 00:02:28,620
end user clicks on any of those links

00:02:24,959 --> 00:02:30,689
and then buy the product we give the

00:02:28,620 --> 00:02:33,750
percentage share of that purchase to the

00:02:30,689 --> 00:02:35,760
publisher essentially it's something

00:02:33,750 --> 00:02:39,840
which is competing invisible from the

00:02:35,760 --> 00:02:41,879
user point of view from I will give you

00:02:39,840 --> 00:02:45,500
some numbers so we work with around

00:02:41,879 --> 00:02:50,989
60,000 publishers around the world

00:02:45,500 --> 00:02:55,500
including over 50% of top 100 websites

00:02:50,989 --> 00:02:59,099
publishers in us in UK we also work with

00:02:55,500 --> 00:03:02,040
almost 50,000 merchants around the world

00:02:59,099 --> 00:03:04,409
and we work with them through networks

00:03:02,040 --> 00:03:07,439
so we don't do the tracking of the

00:03:04,409 --> 00:03:09,120
athlete's wings ourselves we are just

00:03:07,439 --> 00:03:13,409
like this extra layer on top of the

00:03:09,120 --> 00:03:16,230
networks last year we process over 80

00:03:13,409 --> 00:03:20,310
billion paid impressions and almost half

00:03:16,230 --> 00:03:22,349
a billion of clicks and we drew a value

00:03:20,310 --> 00:03:24,389
of around 800 million of e-commerce

00:03:22,349 --> 00:03:27,769
transactions to all the publishers

00:03:24,389 --> 00:03:30,299
around the world which translates into

00:03:27,769 --> 00:03:32,639
hundreds of terabytes of that it has a

00:03:30,299 --> 00:03:35,040
pretty pretty massive Big Data project

00:03:32,639 --> 00:03:37,650
from that point of view what we do with

00:03:35,040 --> 00:03:40,979
this data is we process it react gated

00:03:37,650 --> 00:03:44,069
and represented as customer reports to

00:03:40,979 --> 00:03:46,709
the publishers we also do some data

00:03:44,069 --> 00:03:49,290
exports so we aggregate the data and we

00:03:46,709 --> 00:03:52,040
send it to publishers and the customers

00:03:49,290 --> 00:03:55,560
into the wee drop we can drop it into

00:03:52,040 --> 00:03:58,489
Amazon s3 bucket or Google Cloud storage

00:03:55,560 --> 00:04:05,159
etc and we also do some machine learning

00:03:58,489 --> 00:04:08,639
predictions so why airflow around a year

00:04:05,159 --> 00:04:10,979
ago we decided to do some big changes in

00:04:08,639 --> 00:04:15,739
scheme links we used to use Hadoop

00:04:10,979 --> 00:04:19,650
cluster and we decided to basically

00:04:15,739 --> 00:04:24,840
replace the Hadoop with bigquery which

00:04:19,650 --> 00:04:26,700
is managed SQL database and we used to

00:04:24,840 --> 00:04:30,680
for the scheduling queries to you

00:04:26,700 --> 00:04:33,810
using apache using and we settled

00:04:30,680 --> 00:04:37,950
instead for the airflow why air flow

00:04:33,810 --> 00:04:40,830
well air flow is written entirely in

00:04:37,950 --> 00:04:43,020
Python so what that means is Python

00:04:40,830 --> 00:04:45,270
coincidentally is language of choice of

00:04:43,020 --> 00:04:46,830
a lot of data scientist so if you are a

00:04:45,270 --> 00:04:48,870
data scientist and you want to implement

00:04:46,830 --> 00:04:50,460
some of the machine learning you know

00:04:48,870 --> 00:04:53,460
you can do it directly inside the

00:04:50,460 --> 00:04:56,370
airflow you can think about the air flow

00:04:53,460 --> 00:05:00,360
as a crown on steroids so it will help

00:04:56,370 --> 00:05:01,950
you to schedule your things in a cube

00:05:00,360 --> 00:05:03,870
batch processing so let's say that we

00:05:01,950 --> 00:05:06,810
want to do some computations every one

00:05:03,870 --> 00:05:10,470
hour or every day you can really easily

00:05:06,810 --> 00:05:12,950
do it with the airflow it's been a great

00:05:10,470 --> 00:05:15,810
productivity enhancer for us so we

00:05:12,950 --> 00:05:19,380
calculated that during that last year

00:05:15,810 --> 00:05:21,390
when we migrated from Hadoop to bigquery

00:05:19,380 --> 00:05:24,930
and the air flow we managed to release

00:05:21,390 --> 00:05:26,880
roughly twice as many features thanks to

00:05:24,930 --> 00:05:28,380
air flow as what we did before with the

00:05:26,880 --> 00:05:31,710
Uzis

00:05:28,380 --> 00:05:33,240
this is in large degree thanks to a lot

00:05:31,710 --> 00:05:36,750
of of the features that air flow gives

00:05:33,240 --> 00:05:38,040
us namely modern user interface which is

00:05:36,750 --> 00:05:41,250
something that you would never consider

00:05:38,040 --> 00:05:42,780
as productivity and answer but it helps

00:05:41,250 --> 00:05:45,570
you a lot because it basically it's

00:05:42,780 --> 00:05:48,630
canceled the Python code in your jobs

00:05:45,570 --> 00:05:51,630
and it visualizes all the jobs that you

00:05:48,630 --> 00:05:53,430
implemented in that user interface that

00:05:51,630 --> 00:05:56,100
you can see on the right side it shows

00:05:53,430 --> 00:05:59,130
you also which jobs are currently

00:05:56,100 --> 00:06:01,260
running which jobs were finished you can

00:05:59,130 --> 00:06:03,690
you have access to real-time logs of

00:06:01,260 --> 00:06:07,350
every job every task in your job so you

00:06:03,690 --> 00:06:09,510
can see if anything went wrong you have

00:06:07,350 --> 00:06:12,420
it's not just only static view but you

00:06:09,510 --> 00:06:14,550
can all actually drill in into an a job

00:06:12,420 --> 00:06:16,950
or a task and if something went wrong

00:06:14,550 --> 00:06:20,700
with two clicks you can retry and a task

00:06:16,950 --> 00:06:22,410
very easily and then on top of that we

00:06:20,700 --> 00:06:26,280
have such features like data profiling

00:06:22,410 --> 00:06:28,650
so let's say that you notice that your

00:06:26,280 --> 00:06:31,140
jobs are getting slower and slower and

00:06:28,650 --> 00:06:33,390
you don't really know why you can just

00:06:31,140 --> 00:06:35,250
go into on the section of the UI called

00:06:33,390 --> 00:06:38,610
data profiling and the data profiling is

00:06:35,250 --> 00:06:40,110
going to show you which part of the jobs

00:06:38,610 --> 00:06:43,080
are the slowest

00:06:40,110 --> 00:06:45,150
you can immediately see where you need

00:06:43,080 --> 00:06:47,759
to spend the effort to speed things up

00:06:45,150 --> 00:06:50,550
on top of that we have the comment line

00:06:47,759 --> 00:06:52,349
so if you if you are the guy who doesn't

00:06:50,550 --> 00:06:54,150
really like the user interface you can

00:06:52,349 --> 00:06:57,180
just run a lot of things directly from

00:06:54,150 --> 00:07:00,629
the command line and then last but not

00:06:57,180 --> 00:07:04,169
the least this entire air flow stack is

00:07:00,629 --> 00:07:08,849
horizontally scalable so and schemings

00:07:04,169 --> 00:07:11,430
who run it with the kubernetes using

00:07:08,849 --> 00:07:13,039
so-called salary executor I will come

00:07:11,430 --> 00:07:15,240
back a little bit to the subject later

00:07:13,039 --> 00:07:17,039
what that means is that we can have

00:07:15,240 --> 00:07:19,460
multiple workers you can have pretty

00:07:17,039 --> 00:07:23,629
much as many workers as you want to

00:07:19,460 --> 00:07:27,960
which will in parallel run all your

00:07:23,629 --> 00:07:30,599
computations and so I already said it's

00:07:27,960 --> 00:07:33,150
written in Python is also called first

00:07:30,599 --> 00:07:36,270
scheduler what that means is that you

00:07:33,150 --> 00:07:39,000
define everything inside your Python

00:07:36,270 --> 00:07:40,800
code so things like where that job

00:07:39,000 --> 00:07:43,169
should start what is the start date what

00:07:40,800 --> 00:07:45,900
is you know how frequently that job

00:07:43,169 --> 00:07:48,479
should run however one hour every one

00:07:45,900 --> 00:07:51,810
hour or every one day all of that goes

00:07:48,479 --> 00:07:54,120
into into the code and last but not the

00:07:51,810 --> 00:07:57,150
least it has a great open source

00:07:54,120 --> 00:07:59,099
community so chances are that whatever

00:07:57,150 --> 00:08:00,889
you are trying to do you will probably

00:07:59,099 --> 00:08:05,370
find already some existing

00:08:00,889 --> 00:08:09,389
implementation online inside the earth

00:08:05,370 --> 00:08:13,169
or repository so here is what we arrived

00:08:09,389 --> 00:08:15,479
at after one year of migration from the

00:08:13,169 --> 00:08:18,960
hadoop to bigquery so this is roughly

00:08:15,479 --> 00:08:23,069
the architecture so on the left side in

00:08:18,960 --> 00:08:24,960
here we have the input data so that the

00:08:23,069 --> 00:08:29,699
page impressions is the most heavy part

00:08:24,960 --> 00:08:33,120
where we have the terabytes of data so

00:08:29,699 --> 00:08:35,190
we basically push all the page

00:08:33,120 --> 00:08:38,279
impressions to the Google pops up and

00:08:35,190 --> 00:08:41,699
then right into the Google Cloud storage

00:08:38,279 --> 00:08:44,219
and use data flow about the open source

00:08:41,699 --> 00:08:45,959
version of that is Apache beam so we use

00:08:44,219 --> 00:08:47,699
the Apache beam to read the data from

00:08:45,959 --> 00:08:50,610
the Google Cloud storage and import it

00:08:47,699 --> 00:08:53,210
into the bigquery and then the

00:08:50,610 --> 00:08:56,300
colleagues are imported in our

00:08:53,210 --> 00:08:58,850
is in real-time so we have a system that

00:08:56,300 --> 00:09:00,470
reads them in small batches of few

00:08:58,850 --> 00:09:03,440
hundred clicks from the pops up and

00:09:00,470 --> 00:09:06,980
inserts them in real-time into into the

00:09:03,440 --> 00:09:09,800
bigquery one important thing to note in

00:09:06,980 --> 00:09:11,570
here is that airflow is going to help

00:09:09,800 --> 00:09:13,720
you with the batch processing but not

00:09:11,570 --> 00:09:16,850
will do the real-time processing

00:09:13,720 --> 00:09:18,980
airflow by definition is the scheduler

00:09:16,850 --> 00:09:22,190
that will schedule some job for you to

00:09:18,980 --> 00:09:23,839
run in predefined period of time while

00:09:22,190 --> 00:09:25,580
with the real-time processing you need a

00:09:23,839 --> 00:09:28,370
process that just keeps running keep the

00:09:25,580 --> 00:09:30,920
background all the time so a part of

00:09:28,370 --> 00:09:33,470
this real-time processing beat

00:09:30,920 --> 00:09:36,050
everything else in here is orchestrated

00:09:33,470 --> 00:09:41,029
by airflow so we also import some data

00:09:36,050 --> 00:09:44,000
from the MySQL so customers metadata

00:09:41,029 --> 00:09:46,970
Commission's data things like that all

00:09:44,000 --> 00:09:49,330
of that ends up in in the big query but

00:09:46,970 --> 00:09:52,580
you can very easily do it in any other

00:09:49,330 --> 00:09:56,740
SQL database we just altered for

00:09:52,580 --> 00:09:59,270
bigquery because of the size of our data

00:09:56,740 --> 00:10:01,220
very important thing is also the

00:09:59,270 --> 00:10:04,610
monitoring so you always want to make

00:10:01,220 --> 00:10:07,820
sure that you have a like very good

00:10:04,610 --> 00:10:09,800
checks on the quality of the data so

00:10:07,820 --> 00:10:12,050
whenever something goes wrong we'll get

00:10:09,800 --> 00:10:15,140
the notifications on this like we track

00:10:12,050 --> 00:10:17,390
all the like important time series and

00:10:15,140 --> 00:10:19,430
the improves DB and visualize them with

00:10:17,390 --> 00:10:23,709
the graph on now and then on the right

00:10:19,430 --> 00:10:27,950
side we export the data into three

00:10:23,709 --> 00:10:30,320
things so we have the internal data

00:10:27,950 --> 00:10:33,589
warehouse which is hosted in the MySQL

00:10:30,320 --> 00:10:35,630
we also have the reporting for the end

00:10:33,589 --> 00:10:37,490
customer which is actually done in the

00:10:35,630 --> 00:10:40,820
elasticsearch it works surprisingly

00:10:37,490 --> 00:10:43,100
works pretty well for us and then we as

00:10:40,820 --> 00:10:50,360
I mentioned earlier we send some data to

00:10:43,100 --> 00:10:53,570
Google Cloud Storage and Amazon s3 so a

00:10:50,360 --> 00:10:56,149
part of this pipeline that I just talked

00:10:53,570 --> 00:10:58,520
about we also use the Apaches Park for

00:10:56,149 --> 00:11:02,810
some heavy machine learning processing

00:10:58,520 --> 00:11:05,990
and in here airflow gives you really

00:11:02,810 --> 00:11:09,230
great flexibility one when it comes to

00:11:05,990 --> 00:11:10,970
running but really all your processing

00:11:09,230 --> 00:11:13,550
there are always multiple ways to do the

00:11:10,970 --> 00:11:15,529
same thing the same is true for a party

00:11:13,550 --> 00:11:17,540
spark so we have three ways in which we

00:11:15,529 --> 00:11:19,279
can run their party spark

00:11:17,540 --> 00:11:23,600
probably the best way would be to do it

00:11:19,279 --> 00:11:26,209
using the time spark module together

00:11:23,600 --> 00:11:28,430
with the air flows height and operator

00:11:26,209 --> 00:11:31,010
so inside the air for Python operator

00:11:28,430 --> 00:11:34,070
you can run any Python code you want to

00:11:31,010 --> 00:11:36,800
and then you have the price park module

00:11:34,070 --> 00:11:38,480
where you implement your own like spark

00:11:36,800 --> 00:11:41,810
computations that you want to do and

00:11:38,480 --> 00:11:44,449
this way every will orchestrate the the

00:11:41,810 --> 00:11:47,329
spark from within the Python code

00:11:44,449 --> 00:11:49,610
another two alternatives is to use the

00:11:47,329 --> 00:11:52,130
spark sub mean so basically you keep all

00:11:49,610 --> 00:11:54,949
your code inside the apache spark and

00:11:52,130 --> 00:11:57,140
then you just submit the spark jobs to

00:11:54,949 --> 00:12:00,230
spark from air flow using this park

00:11:57,140 --> 00:12:02,390
submit which we can do very easy using

00:12:00,230 --> 00:12:04,550
the well there is a spark step

00:12:02,390 --> 00:12:05,990
interpreter for that or if you prefer

00:12:04,550 --> 00:12:08,779
you can actually implement it yourself

00:12:05,990 --> 00:12:10,339
using the bash operator class called the

00:12:08,779 --> 00:12:14,560
spark submit from from the batch

00:12:10,339 --> 00:12:17,870
operator the reality though is that

00:12:14,560 --> 00:12:21,850
majority of the machine learning tasks

00:12:17,870 --> 00:12:26,510
are usually spent on data engineering so

00:12:21,850 --> 00:12:28,600
things like cleaning input date ETL

00:12:26,510 --> 00:12:31,490
extraction transformation and loading

00:12:28,600 --> 00:12:33,850
preparing key features running the

00:12:31,490 --> 00:12:35,930
series of jobs and then eventually

00:12:33,850 --> 00:12:38,600
production izing the entire data

00:12:35,930 --> 00:12:41,660
pipeline that's something that air flow

00:12:38,600 --> 00:12:43,130
is really going to help you with so it's

00:12:41,660 --> 00:12:44,930
not really going to help you much with

00:12:43,130 --> 00:12:47,300
the machine learning which you implement

00:12:44,930 --> 00:12:52,070
yourself with python but production

00:12:47,300 --> 00:12:54,320
izing all the jobs will help a lot with

00:12:52,070 --> 00:12:55,730
all right so before we go into the

00:12:54,320 --> 00:12:58,310
details on how the air flow actually

00:12:55,730 --> 00:13:01,610
works I wanted to ask you but anyone

00:12:58,310 --> 00:13:03,850
here uses it already air flow ends up

00:13:01,610 --> 00:13:06,170
all right

00:13:03,850 --> 00:13:10,220
only one person so it's good that we are

00:13:06,170 --> 00:13:14,000
covering some basics so when you will be

00:13:10,220 --> 00:13:17,589
working with air flow you will hear the

00:13:14,000 --> 00:13:20,170
times of dogs all the time

00:13:17,589 --> 00:13:24,670
so Dyke stands for the directed acyclic

00:13:20,170 --> 00:13:27,640
graph and you will create so your dark

00:13:24,670 --> 00:13:31,779
is essentially your your job your

00:13:27,640 --> 00:13:33,850
workflow the arc is built from the tasks

00:13:31,779 --> 00:13:37,930
and the dependencies between the tasks

00:13:33,850 --> 00:13:41,310
so the task is the bit where you process

00:13:37,930 --> 00:13:45,370
some data or you for example around the

00:13:41,310 --> 00:13:47,380
apache spark computation and then you

00:13:45,370 --> 00:13:50,890
also specify the dependencies between

00:13:47,380 --> 00:13:53,020
the tasks which tell air flow what

00:13:50,890 --> 00:13:55,570
should be what should happen next

00:13:53,020 --> 00:13:57,279
once that task will finish so for

00:13:55,570 --> 00:14:00,550
example in here on the right side we

00:13:57,279 --> 00:14:05,440
have sample pipeline from Skimlinks

00:14:00,550 --> 00:14:07,660
where we first we process the clicks you

00:14:05,440 --> 00:14:10,089
can notice that coms pipeline and pages

00:14:07,660 --> 00:14:11,950
pipeline which are all green they don't

00:14:10,089 --> 00:14:15,700
have any dependencies so they can all

00:14:11,950 --> 00:14:17,740
run in parallel for example the link

00:14:15,700 --> 00:14:19,120
activity pipeline has a dependence you

00:14:17,740 --> 00:14:20,710
know the clicks pipeline which means

00:14:19,120 --> 00:14:22,900
that it can only start once the clicks

00:14:20,710 --> 00:14:25,720
pipeline will finish link of a new

00:14:22,900 --> 00:14:27,220
pipeline on the left top bottom left I

00:14:25,720 --> 00:14:28,900
mean this name doesn't really matter

00:14:27,220 --> 00:14:32,140
what you know it's just the name of the

00:14:28,900 --> 00:14:34,480
task this task can only start once the

00:14:32,140 --> 00:14:38,050
combo comes pipeline and link activity

00:14:34,480 --> 00:14:41,980
pipeline will finish you can you can

00:14:38,050 --> 00:14:44,050
define much more complex dependencies in

00:14:41,980 --> 00:14:46,870
the task so you can have some branching

00:14:44,050 --> 00:14:49,690
co-operators you can have joins so you

00:14:46,870 --> 00:14:51,459
know like once both tasks finished and

00:14:49,690 --> 00:14:54,850
the next one should finish it should

00:14:51,459 --> 00:14:56,320
start branching where you can skip some

00:14:54,850 --> 00:14:59,560
tasks so for example you can have

00:14:56,320 --> 00:15:01,810
certain tasks that run over weekdays but

00:14:59,560 --> 00:15:05,589
you want to skip them during the weekend

00:15:01,810 --> 00:15:09,400
and so on so it's it's a very simple but

00:15:05,589 --> 00:15:12,640
very powerful concept examples of the

00:15:09,400 --> 00:15:15,400
dikes and jobs you can for example

00:15:12,640 --> 00:15:18,220
create the report running by running

00:15:15,400 --> 00:15:20,920
some SQL query and then storing the

00:15:18,220 --> 00:15:23,740
results in the output file or the output

00:15:20,920 --> 00:15:26,170
table you can extract the features for

00:15:23,740 --> 00:15:27,940
the machine learning pipeline or you can

00:15:26,170 --> 00:15:28,470
trigger the party spark

00:15:27,940 --> 00:15:31,650
job

00:15:28,470 --> 00:15:36,330
then it is very exciting about this is

00:15:31,650 --> 00:15:38,580
that airflow is going to parse your

00:15:36,330 --> 00:15:40,560
tasks and the dependencies between them

00:15:38,580 --> 00:15:41,990
which you implement it in Python and

00:15:40,560 --> 00:15:44,160
we'll do this visualization

00:15:41,990 --> 00:15:47,280
automatically for you and this

00:15:44,160 --> 00:15:49,410
visualization is interactive so you can

00:15:47,280 --> 00:15:51,870
click on any of this tasks you can drink

00:15:49,410 --> 00:15:54,750
to them and you can see the logs for the

00:15:51,870 --> 00:15:57,930
specific tasks or you can rerun this

00:15:54,750 --> 00:15:59,550
task together with any dependencies so

00:15:57,930 --> 00:16:02,760
let's say that if for whatever reason

00:15:59,550 --> 00:16:05,160
the page pipe pipeline failed which is

00:16:02,760 --> 00:16:07,590
on the right side and we want to rerun

00:16:05,160 --> 00:16:09,240
it if we select that we want to also

00:16:07,590 --> 00:16:11,610
rerun the dependencies we are not

00:16:09,240 --> 00:16:13,080
airflow is going to automatically run

00:16:11,610 --> 00:16:14,670
for us the pages pipeline the

00:16:13,080 --> 00:16:18,570
productivity pipeline and all the

00:16:14,670 --> 00:16:20,040
dependencies below that as well it's

00:16:18,570 --> 00:16:22,410
very useful because you know the

00:16:20,040 --> 00:16:24,090
problems in the I mean there are always

00:16:22,410 --> 00:16:25,740
problems that do happen from time to

00:16:24,090 --> 00:16:29,070
time in the production system and you

00:16:25,740 --> 00:16:34,320
might want to fix such things so

00:16:29,070 --> 00:16:38,490
operator is you can think about the

00:16:34,320 --> 00:16:42,810
operator as a class that defines what is

00:16:38,490 --> 00:16:45,240
going to be done inside your task so we

00:16:42,810 --> 00:16:47,130
have a choice of operators that there's

00:16:45,240 --> 00:16:49,140
some building operators that are country

00:16:47,130 --> 00:16:52,740
pop raters or you can write your own

00:16:49,140 --> 00:16:54,870
custom operators example of the operator

00:16:52,740 --> 00:16:58,590
would be for example Postgres operator

00:16:54,870 --> 00:17:00,600
which allows you to execute any sequel

00:16:58,590 --> 00:17:02,760
query of your choice against the

00:17:00,600 --> 00:17:05,730
Postgres database or a bus operator

00:17:02,760 --> 00:17:12,900
where you can run any custom bash

00:17:05,730 --> 00:17:15,900
command inside the bus then the task is

00:17:12,900 --> 00:17:20,360
an instance of the operator so you can

00:17:15,900 --> 00:17:20,360
think or it's an instance of operator or

00:17:21,050 --> 00:17:27,510
and when I would come back to it or a

00:17:24,300 --> 00:17:29,400
sensor so from that point of view you

00:17:27,510 --> 00:17:33,600
can think about the task is an instance

00:17:29,400 --> 00:17:37,320
of of the class or an object a task is

00:17:33,600 --> 00:17:39,990
not in your dock and then so when you

00:17:37,320 --> 00:17:42,120
instant when you create the instance of

00:17:39,990 --> 00:17:44,610
the operator you also have to provide

00:17:42,120 --> 00:17:48,360
parameters for this operator so for

00:17:44,610 --> 00:17:50,850
example with the Postgres operate or

00:17:48,360 --> 00:17:51,990
your parameter will be SQL coordinate

00:17:50,850 --> 00:17:53,520
that you want to run against the

00:17:51,990 --> 00:17:56,820
Postgres plus the connection details

00:17:53,520 --> 00:18:00,750
that you want to execute against the

00:17:56,820 --> 00:18:03,180
database we can also use very powerful

00:18:00,750 --> 00:18:07,500
ginger templating system inside the

00:18:03,180 --> 00:18:10,080
airflow so it allows you for example

00:18:07,500 --> 00:18:13,680
with the bash operator you can it can

00:18:10,080 --> 00:18:16,020
put some parameters inside the ginger

00:18:13,680 --> 00:18:19,110
template and then just parameterize them

00:18:16,020 --> 00:18:21,770
by passing variables inside inside your

00:18:19,110 --> 00:18:25,080
tasks on the right side we have the

00:18:21,770 --> 00:18:30,000
example of the bigquery operator so we

00:18:25,080 --> 00:18:32,160
create merge merge task which is the

00:18:30,000 --> 00:18:35,520
instance of the bigquery operator we

00:18:32,160 --> 00:18:37,590
specify the name of this task the SQL

00:18:35,520 --> 00:18:43,440
query that we want to run against

00:18:37,590 --> 00:18:44,970
bigquery data base the connection ID so

00:18:43,440 --> 00:18:46,830
like you know you might have a

00:18:44,970 --> 00:18:50,040
production that a very staging database

00:18:46,830 --> 00:18:52,230
and and so on as well as some additional

00:18:50,040 --> 00:18:54,000
parameters the interesting parameter in

00:18:52,230 --> 00:18:56,670
here I wanted to point out is the number

00:18:54,000 --> 00:19:00,780
of retries that you want air flow to

00:18:56,670 --> 00:19:06,480
perform before succeed in the task so

00:19:00,780 --> 00:19:08,250
for example if air flow will be running

00:19:06,480 --> 00:19:10,470
this task for the first time and the

00:19:08,250 --> 00:19:12,179
task will fail for whatever reason maybe

00:19:10,470 --> 00:19:14,790
because there was some problem with the

00:19:12,179 --> 00:19:17,190
connecting to the database then air flow

00:19:14,790 --> 00:19:19,830
is automatically going to retry that

00:19:17,190 --> 00:19:22,890
task again you can you can define how

00:19:19,830 --> 00:19:25,440
many tries you want this is a very

00:19:22,890 --> 00:19:28,980
powerful feature because it allows air

00:19:25,440 --> 00:19:31,710
flow to seal itself so like very often

00:19:28,980 --> 00:19:33,270
what we find out is that we go to the

00:19:31,710 --> 00:19:35,880
office on the morning see some emails

00:19:33,270 --> 00:19:37,770
that there was some problem with one of

00:19:35,880 --> 00:19:39,480
the pipeline but actually everything

00:19:37,770 --> 00:19:42,540
went fine in the end

00:19:39,480 --> 00:19:44,450
because of the retries so there are

00:19:42,540 --> 00:19:49,679
quite a lot of problem that just happen

00:19:44,450 --> 00:19:51,900
randomly from time to time some advanced

00:19:49,679 --> 00:19:54,690
features that I'm not going to be

00:19:51,900 --> 00:19:56,180
covering in details in here but you can

00:19:54,690 --> 00:19:59,730
you can use with the air

00:19:56,180 --> 00:20:03,120
hooks gives you the interface to

00:19:59,730 --> 00:20:05,580
external databases and platforms so for

00:20:03,120 --> 00:20:08,190
example there will be a MySQL who post

00:20:05,580 --> 00:20:11,250
Chris who bigquery hook and and so on

00:20:08,190 --> 00:20:14,040
then the connections are stored in

00:20:11,250 --> 00:20:17,040
airflow inside the airflow meta base and

00:20:14,040 --> 00:20:21,060
you can modify them using the airflow

00:20:17,040 --> 00:20:23,160
user interface so that's a very useful

00:20:21,060 --> 00:20:25,680
feature because if you have two

00:20:23,160 --> 00:20:28,200
environments let's say production and

00:20:25,680 --> 00:20:30,360
the staging environment you don't have

00:20:28,200 --> 00:20:32,760
to bake your connection inside your code

00:20:30,360 --> 00:20:35,340
you can you can just store it inside the

00:20:32,760 --> 00:20:38,370
airflow UI which will be done stored

00:20:35,340 --> 00:20:42,600
inside there for database then we also

00:20:38,370 --> 00:20:45,090
have variables so you can also define

00:20:42,600 --> 00:20:46,890
variables inside the airflow meta base

00:20:45,090 --> 00:20:50,160
which you can control through the

00:20:46,890 --> 00:20:53,040
airflow user interface same story in

00:20:50,160 --> 00:20:54,840
here you don't have to bake and hard

00:20:53,040 --> 00:20:58,770
cuts on some of the variables inside

00:20:54,840 --> 00:21:01,290
your code such as let's say you might

00:20:58,770 --> 00:21:03,450
want to use a different Amazon s3 path

00:21:01,290 --> 00:21:06,240
in production and different Amazon s3

00:21:03,450 --> 00:21:07,950
paths in your staging environment you

00:21:06,240 --> 00:21:10,380
don't have to put it into your code you

00:21:07,950 --> 00:21:14,430
can just modify it directly from the

00:21:10,380 --> 00:21:17,460
airflow user interface X comms allows

00:21:14,430 --> 00:21:19,890
you to pass some parameters from one

00:21:17,460 --> 00:21:22,410
task to another task so it's a mean of

00:21:19,890 --> 00:21:26,970
communication between the tasks so let's

00:21:22,410 --> 00:21:29,520
say that one task 1 task can push some

00:21:26,970 --> 00:21:33,060
some information to the next one such as

00:21:29,520 --> 00:21:35,970
I process the data for the last three

00:21:33,060 --> 00:21:38,670
days and the next task needs to be aware

00:21:35,970 --> 00:21:44,640
of how many days it needs to process the

00:21:38,670 --> 00:21:47,780
data four sensors allow you to schedule

00:21:44,640 --> 00:21:52,230
the tasks when certain criteria are true

00:21:47,780 --> 00:21:54,990
so as I mentioned earlier you can also

00:21:52,230 --> 00:21:57,030
allows you to schedule your batches so

00:21:54,990 --> 00:21:59,250
you can write you can tell air flow to

00:21:57,030 --> 00:22:02,010
run some kind of processing once a day

00:21:59,250 --> 00:22:04,770
every day at 9 a.m. but you might add

00:22:02,010 --> 00:22:07,230
some extra criteria for it such as ok

00:22:04,770 --> 00:22:09,059
run this at 9:00 a.m. but only if that

00:22:07,230 --> 00:22:12,799
specific file

00:22:09,059 --> 00:22:16,139
already exist in in this specific folder

00:22:12,799 --> 00:22:18,629
so for example in HDFS you might you

00:22:16,139 --> 00:22:21,360
might want to wait for some external

00:22:18,629 --> 00:22:23,190
dependency to finish that airflow

00:22:21,360 --> 00:22:24,840
doesn't have any control over with the

00:22:23,190 --> 00:22:26,789
sensor you can you can basically check

00:22:24,840 --> 00:22:29,580
whether this is this criteria is already

00:22:26,789 --> 00:22:32,129
true or not and then you know start

00:22:29,580 --> 00:22:36,149
execution of your task once once it will

00:22:32,129 --> 00:22:41,399
be the case and then there is a lot more

00:22:36,149 --> 00:22:43,950
in the plugins so hooks connections web

00:22:41,399 --> 00:22:46,320
views template macros and and so on they

00:22:43,950 --> 00:22:48,809
could be packaged into the plugin and

00:22:46,320 --> 00:22:51,869
anything that should not be a part of

00:22:48,809 --> 00:22:54,499
the airflow kora usually goes into into

00:22:51,869 --> 00:22:57,299
the plugins there's a great repository

00:22:54,499 --> 00:23:01,190
in the github airflow plugins where you

00:22:57,299 --> 00:23:03,059
can find some very interesting

00:23:01,190 --> 00:23:04,710
implementations of such things like

00:23:03,059 --> 00:23:06,749
Google Analytics for example it's

00:23:04,710 --> 00:23:08,759
something that you know you didn't want

00:23:06,749 --> 00:23:10,529
to have in the airflow core but a lot of

00:23:08,759 --> 00:23:11,639
people use it and they might want to

00:23:10,529 --> 00:23:14,190
download the data from the Google

00:23:11,639 --> 00:23:16,470
Analytics or some integrations with the

00:23:14,190 --> 00:23:22,610
payment platforms such as stripe and and

00:23:16,470 --> 00:23:25,919
so on so here is the sample code that

00:23:22,610 --> 00:23:29,340
shows how you would create a doc inside

00:23:25,919 --> 00:23:31,559
your airflow so on the right side we

00:23:29,340 --> 00:23:34,320
basically create in your dock where we

00:23:31,559 --> 00:23:36,749
give it our name called tutorial and we

00:23:34,320 --> 00:23:40,110
create three tasks so we create the

00:23:36,749 --> 00:23:43,559
tasks t1 t2 and t3 all of them are

00:23:40,110 --> 00:23:46,860
instances of the bash operator and then

00:23:43,559 --> 00:23:49,259
in this case we just give it some IDs

00:23:46,860 --> 00:23:51,269
and then provide the commands that we

00:23:49,259 --> 00:23:54,029
want to run inside each of those bash

00:23:51,269 --> 00:23:56,100
operators so the first task we just run

00:23:54,029 --> 00:23:58,710
the command date the second one will

00:23:56,100 --> 00:24:00,929
sleep for 5 seconds and the third one

00:23:58,710 --> 00:24:03,119
will use the ginger template that I

00:24:00,929 --> 00:24:05,759
mentioned earlier that is parameterize

00:24:03,119 --> 00:24:09,210
with the with our param Singh here so in

00:24:05,759 --> 00:24:09,990
the params we can we pass a parameter

00:24:09,210 --> 00:24:12,899
called

00:24:09,990 --> 00:24:15,869
its dictionary with the key called my

00:24:12,899 --> 00:24:19,259
param and that my para MS going to be

00:24:15,869 --> 00:24:20,679
injected in here so you can access it

00:24:19,259 --> 00:24:24,129
from your ginger template that

00:24:20,679 --> 00:24:25,509
Parham stilt and the name of the

00:24:24,129 --> 00:24:28,230
parameter which is defined in that

00:24:25,509 --> 00:24:31,059
dictionary there are some additional

00:24:28,230 --> 00:24:34,240
macros that are available out of the box

00:24:31,059 --> 00:24:36,309
inside the airflow so for example in

00:24:34,240 --> 00:24:40,590
here we have the macro called DS art

00:24:36,309 --> 00:24:42,879
where we had seven day to execution date

00:24:40,590 --> 00:24:45,369
you can also create your own macros

00:24:42,879 --> 00:24:48,039
which you cannot refer to inside the

00:24:45,369 --> 00:24:51,850
ginger template on the left side in here

00:24:48,039 --> 00:24:55,240
we have some parameters such as start

00:24:51,850 --> 00:24:59,350
date which defines from which date this

00:24:55,240 --> 00:25:02,649
air flow dark should start running and

00:24:59,350 --> 00:25:05,559
we also define the scheduled interval so

00:25:02,649 --> 00:25:07,090
how frequently that should run so in

00:25:05,559 --> 00:25:10,600
this case we have a scheduled interval

00:25:07,090 --> 00:25:12,519
of one day okay so as you can see

00:25:10,600 --> 00:25:15,700
everything is inside everything is

00:25:12,519 --> 00:25:19,539
implemented inside Python and then air

00:25:15,700 --> 00:25:22,210
flow is intelligent enough to basically

00:25:19,539 --> 00:25:25,629
parse this code extract all the tasks

00:25:22,210 --> 00:25:27,639
from it and do the visualization showing

00:25:25,629 --> 00:25:30,100
what kind of tasks we have inside and

00:25:27,639 --> 00:25:33,220
also showing the dependencies between

00:25:30,100 --> 00:25:38,470
these tasks so in this case we have task

00:25:33,220 --> 00:25:41,710
T 2 and T 3 that are dependent on T 1 so

00:25:38,470 --> 00:25:44,590
T 1 is upstream from T 2 and T 3 which

00:25:41,710 --> 00:25:48,009
means that only once the T 1 task will

00:25:44,590 --> 00:25:55,629
finish then the both T 2 and T 3 start

00:25:48,009 --> 00:25:56,980
and they can run in parallel ok so let's

00:25:55,629 --> 00:25:59,850
talk about some of the air flow best

00:25:56,980 --> 00:26:03,429
practices that we use in in schemings

00:25:59,850 --> 00:26:08,249
so a very very important one that saved

00:26:03,429 --> 00:26:13,090
our life few times I DuPont and dogs so

00:26:08,249 --> 00:26:15,070
it gives you so if you implement your

00:26:13,090 --> 00:26:18,749
tasks in such a way that there are Ida

00:26:15,070 --> 00:26:22,950
pond and you gain the possibility to

00:26:18,749 --> 00:26:25,720
rerun those tasks in the repeatable way

00:26:22,950 --> 00:26:28,529
to do it you just need to make sure that

00:26:25,720 --> 00:26:31,840
the task is always cleaning after itself

00:26:28,529 --> 00:26:34,179
doesn't leave any side-effects and you

00:26:31,840 --> 00:26:34,480
can basically rerun it safely twice and

00:26:34,179 --> 00:26:38,740
have

00:26:34,480 --> 00:26:40,960
the same result so if you don't do that

00:26:38,740 --> 00:26:43,990
then you might have a problem because

00:26:40,960 --> 00:26:46,059
for example you know the life happens

00:26:43,990 --> 00:26:48,970
and won't be some issues in the

00:26:46,059 --> 00:26:52,270
production system from time to time and

00:26:48,970 --> 00:26:54,760
then if you would try to rerun a task in

00:26:52,270 --> 00:26:58,480
a production system suddenly you will

00:26:54,760 --> 00:27:00,190
find that there were some side effects

00:26:58,480 --> 00:27:03,309
that you didn't you didn't account for

00:27:00,190 --> 00:27:05,140
so for example you might have twice as

00:27:03,309 --> 00:27:07,419
much data or maybe you have three times

00:27:05,140 --> 00:27:09,429
more data because you run the same task

00:27:07,419 --> 00:27:13,390
three times and the task didn't clean

00:27:09,429 --> 00:27:16,840
after itself so it's very very important

00:27:13,390 --> 00:27:21,970
because it allows you to rerun the tasks

00:27:16,840 --> 00:27:25,750
in the repeatable way test so we did

00:27:21,970 --> 00:27:29,799
struggle a little bit with creating good

00:27:25,750 --> 00:27:32,650
tests for our dogs and the tasks so the

00:27:29,799 --> 00:27:35,590
good news is that air flow gives you

00:27:32,650 --> 00:27:38,260
like a quite easy way to test your tasks

00:27:35,590 --> 00:27:40,240
there is a command line called air flow

00:27:38,260 --> 00:27:43,090
test where you specify the name of your

00:27:40,240 --> 00:27:45,460
dog the ID of your task and the date and

00:27:43,090 --> 00:27:50,080
air flow is going to execute that

00:27:45,460 --> 00:27:51,790
specific task however that wasn't enough

00:27:50,080 --> 00:27:55,030
really for us because we have some

00:27:51,790 --> 00:27:57,730
complex dikes with you know many tasks

00:27:55,030 --> 00:28:00,040
inside them and then running manually

00:27:57,730 --> 00:28:02,790
and testing manually every single task

00:28:00,040 --> 00:28:04,929
wasn't just wasn't cutting it because

00:28:02,790 --> 00:28:06,549
there's a lot of dependencies between

00:28:04,929 --> 00:28:09,070
those tasks you want to make sure that

00:28:06,549 --> 00:28:11,799
you around them all in a correct order

00:28:09,070 --> 00:28:15,460
so what we ended up doing is basically

00:28:11,799 --> 00:28:20,290
creating a test that for every single

00:28:15,460 --> 00:28:24,220
dog that runs exactly the same code as

00:28:20,290 --> 00:28:27,700
the dark itself and then runs it on the

00:28:24,220 --> 00:28:29,440
input data creates the output data and

00:28:27,700 --> 00:28:31,630
compares that output against the

00:28:29,440 --> 00:28:33,730
expected output so pretty much a

00:28:31,630 --> 00:28:36,900
standard integral to you would expect

00:28:33,730 --> 00:28:39,490
from the standard integration test we

00:28:36,900 --> 00:28:42,250
execute the integration test during each

00:28:39,490 --> 00:28:46,120
build so basically if anything breaks

00:28:42,250 --> 00:28:47,059
you you will be notified by Jenkins or

00:28:46,120 --> 00:28:50,659
whichever building

00:28:47,059 --> 00:28:53,120
that you're using also it's a very good

00:28:50,659 --> 00:28:58,669
idea to use separate environments for

00:28:53,120 --> 00:29:02,600
production staging and the test or local

00:28:58,669 --> 00:29:05,960
environment so in our case for the local

00:29:02,600 --> 00:29:09,830
environment we have a fully decreased

00:29:05,960 --> 00:29:14,210
local solution it's very easy to do with

00:29:09,830 --> 00:29:16,190
the air thaw and it once you do it you

00:29:14,210 --> 00:29:17,899
essentially can around the local dock

00:29:16,190 --> 00:29:20,629
your eyes airflow with a single line of

00:29:17,899 --> 00:29:22,639
code so let's say that we have a new

00:29:20,629 --> 00:29:24,499
developer who wants to start and join

00:29:22,639 --> 00:29:26,149
your team they can do it in three

00:29:24,499 --> 00:29:28,279
minutes because all they have to do is

00:29:26,149 --> 00:29:30,440
just download the git repo and then

00:29:28,279 --> 00:29:34,100
start the local version of the air flow

00:29:30,440 --> 00:29:37,399
with the docker compose up in production

00:29:34,100 --> 00:29:41,720
we are reusing the same docker images

00:29:37,399 --> 00:29:46,549
and inside the kubernetes cluster so we

00:29:41,720 --> 00:29:49,639
have horizontally scalable workers with

00:29:46,549 --> 00:29:51,559
the salary executors very important

00:29:49,639 --> 00:29:54,080
thing is to make sure that you store

00:29:51,559 --> 00:29:57,619
your logs from your production cluster

00:29:54,080 --> 00:30:00,759
somewhere in the persistent storage so

00:29:57,619 --> 00:30:03,919
Amazon s3 HDFS or Google Cloud Storage

00:30:00,759 --> 00:30:07,100
because you workers will die from time

00:30:03,919 --> 00:30:08,960
to time and when the worker dies and all

00:30:07,100 --> 00:30:11,119
the logs were on the worker you are not

00:30:08,960 --> 00:30:16,129
going to know what happened like what

00:30:11,119 --> 00:30:20,299
caused that problem ideally if you can

00:30:16,129 --> 00:30:22,129
run a fully managed cluster that that's

00:30:20,299 --> 00:30:23,629
the best solution because then you don't

00:30:22,129 --> 00:30:26,450
have to worry about maintaining the

00:30:23,629 --> 00:30:28,999
cluster upgrading the airflow and and so

00:30:26,450 --> 00:30:32,480
on there are some commercial solutions

00:30:28,999 --> 00:30:35,629
that allow you to do it when it comes to

00:30:32,480 --> 00:30:40,009
deployment you have huge strategies so

00:30:35,629 --> 00:30:41,659
there is a pool method so you can run

00:30:40,009 --> 00:30:44,149
the get sync and every single of your

00:30:41,659 --> 00:30:46,369
workers UI and the scheduler and then

00:30:44,149 --> 00:30:48,470
pull the latest code from the master

00:30:46,369 --> 00:30:52,330
branch or production or the developer

00:30:48,470 --> 00:30:55,129
branch if that's a staging cluster or

00:30:52,330 --> 00:30:57,259
you have a push model where we can push

00:30:55,129 --> 00:30:57,760
the code using something like our thing

00:30:57,259 --> 00:31:00,970
to have

00:30:57,760 --> 00:31:04,000
single few workers alternatively you can

00:31:00,970 --> 00:31:06,490
use the persistent volume so you can

00:31:04,000 --> 00:31:09,070
have all the workers in the read-only

00:31:06,490 --> 00:31:12,610
mode that read the data read the latest

00:31:09,070 --> 00:31:15,970
Python code from from a specific volume

00:31:12,610 --> 00:31:18,580
and then you have a writable volume

00:31:15,970 --> 00:31:20,950
where you push all the latest code or

00:31:18,580 --> 00:31:23,500
you can just bake in your code inside

00:31:20,950 --> 00:31:26,200
the docker containers the does a

00:31:23,500 --> 00:31:28,030
disadvantage in here is that whenever

00:31:26,200 --> 00:31:30,970
you are creating then you build you will

00:31:28,030 --> 00:31:32,830
have to recreate an entire cluster so

00:31:30,970 --> 00:31:35,680
you have to kill all the workers it

00:31:32,830 --> 00:31:37,660
cannot really just push easily the new

00:31:35,680 --> 00:31:41,170
code if that code is baked into the

00:31:37,660 --> 00:31:43,720
docker containers whichever way

00:31:41,170 --> 00:31:46,030
whichever a solution you choose air flow

00:31:43,720 --> 00:31:49,390
gives you very good support for scanning

00:31:46,030 --> 00:31:52,090
the changes in the latest files so there

00:31:49,390 --> 00:31:55,300
is a setting in the air flow that will

00:31:52,090 --> 00:31:57,520
scan that allows that defines basically

00:31:55,300 --> 00:31:59,500
how frequently air flow should scan all

00:31:57,520 --> 00:32:02,590
the files for the latest changes I

00:31:59,500 --> 00:32:04,890
believe by default it's every few

00:32:02,590 --> 00:32:07,930
seconds but you can you can increase it

00:32:04,890 --> 00:32:10,590
every few minute and then air flow will

00:32:07,930 --> 00:32:13,090
automatically pick up latest changes and

00:32:10,590 --> 00:32:17,980
will around the latest version of your

00:32:13,090 --> 00:32:20,800
air flow docks so to summarize some of

00:32:17,980 --> 00:32:25,740
the good the bad and the ugly that we

00:32:20,800 --> 00:32:29,020
found in air flow during the last year

00:32:25,740 --> 00:32:30,490
there are some issues as well with the

00:32:29,020 --> 00:32:34,540
air flow I mean it's a relatively new

00:32:30,490 --> 00:32:37,990
open source project so you know it's

00:32:34,540 --> 00:32:40,150
expected one problem that we've seen is

00:32:37,990 --> 00:32:40,830
displaying the net dynamically generated

00:32:40,150 --> 00:32:44,080
tags

00:32:40,830 --> 00:32:46,390
so because you create a dark in the

00:32:44,080 --> 00:32:48,250
Python code you can you know you can

00:32:46,390 --> 00:32:51,250
dynamically create the tasks so you can

00:32:48,250 --> 00:32:55,150
for example create ten tasks today but

00:32:51,250 --> 00:32:57,430
five tasks tomorrow if you number of

00:32:55,150 --> 00:32:59,740
tasks dynamically changes that could be

00:32:57,430 --> 00:33:02,070
a bit tricky for the user interface to

00:32:59,740 --> 00:33:06,040
visualize so just keep that in mind

00:33:02,070 --> 00:33:08,620
dodj dependencies in here you have a

00:33:06,040 --> 00:33:10,180
choice to either put all your logic

00:33:08,620 --> 00:33:13,330
inside some big

00:33:10,180 --> 00:33:17,410
complex dikes or speed them into some

00:33:13,330 --> 00:33:20,890
smaller dogs usually smaller dogs is

00:33:17,410 --> 00:33:24,700
better but then you have to somehow make

00:33:20,890 --> 00:33:27,220
sure that you connect you connect them

00:33:24,700 --> 00:33:29,140
within the airflow so you need to let

00:33:27,220 --> 00:33:33,300
airflow know that once this dark finish

00:33:29,140 --> 00:33:36,070
is the second I can start you do it with

00:33:33,300 --> 00:33:37,570
with the sensors the problem in the same

00:33:36,070 --> 00:33:39,640
service is that they're going to take

00:33:37,570 --> 00:33:41,740
your resources because the sensor is

00:33:39,640 --> 00:33:44,290
essentially a process that is waiting

00:33:41,740 --> 00:33:47,110
and checking some condition all the time

00:33:44,290 --> 00:33:49,090
so like it can wait and check every few

00:33:47,110 --> 00:33:51,820
seconds whether this file already exists

00:33:49,090 --> 00:33:55,330
in the HDFS or not so you know it's

00:33:51,820 --> 00:33:57,220
extra resource one problem that we are

00:33:55,330 --> 00:33:58,960
seeing in our current version of the air

00:33:57,220 --> 00:34:01,800
flow is that the scheduler just stopped

00:33:58,960 --> 00:34:04,810
scheduling things after a few days so

00:34:01,800 --> 00:34:07,600
essentially we there is a setting in the

00:34:04,810 --> 00:34:10,530
air flow that allows you to restart the

00:34:07,600 --> 00:34:13,030
scheduler we restarted every few hours

00:34:10,530 --> 00:34:15,310
it's not the case apparently in the

00:34:13,030 --> 00:34:18,640
latest version of the air flow so if you

00:34:15,310 --> 00:34:21,940
use the latest one point ten point three

00:34:18,640 --> 00:34:24,010
that should not be the case anymore and

00:34:21,940 --> 00:34:26,380
then we also see some zombies problems

00:34:24,010 --> 00:34:30,820
from time to time so basically the

00:34:26,380 --> 00:34:32,740
processes just die there is no heartbeat

00:34:30,820 --> 00:34:35,200
coming out of them and then air flow

00:34:32,740 --> 00:34:37,300
marks the process it's a zombie even

00:34:35,200 --> 00:34:40,899
though the process continues and works

00:34:37,300 --> 00:34:43,300
just fine fortunately it's not an issue

00:34:40,899 --> 00:34:45,929
because we talked earlier about the

00:34:43,300 --> 00:34:49,179
retries so ever we automatically try to

00:34:45,929 --> 00:34:51,760
retry that process and usually if you

00:34:49,179 --> 00:34:53,860
implemented it in the IDE upon tenth way

00:34:51,760 --> 00:34:56,320
meaning you know there is no side

00:34:53,860 --> 00:35:00,430
effects of rerunning it it's going to

00:34:56,320 --> 00:35:02,620
work fine then the last thing is the

00:35:00,430 --> 00:35:06,070
some tags so there are some developers

00:35:02,620 --> 00:35:07,000
that embrace them we like to use them

00:35:06,070 --> 00:35:09,240
quite a lot but there are some

00:35:07,000 --> 00:35:11,980
developers that really try to avoid them

00:35:09,240 --> 00:35:15,190
we like them because they allow us to

00:35:11,980 --> 00:35:17,740
encapsulate the complex code so if you

00:35:15,190 --> 00:35:19,510
have a huge dog with lots of tasks you

00:35:17,740 --> 00:35:21,730
can group a lot of of the tasks into a

00:35:19,510 --> 00:35:22,690
separate doc and you can include the

00:35:21,730 --> 00:35:23,609
separate ha

00:35:22,690 --> 00:35:26,400
doc as a

00:35:23,609 --> 00:35:32,730
subtask in your existing in your

00:35:26,400 --> 00:35:35,670
existing workflow so to summarize we

00:35:32,730 --> 00:35:39,509
managed to finish the migration from

00:35:35,670 --> 00:35:41,819
Hadoop to bigquery everything works

00:35:39,509 --> 00:35:44,819
great air flow has been a huge

00:35:41,819 --> 00:35:47,640
productivity enhancer for us like we

00:35:44,819 --> 00:35:50,069
believe that we are at least twice as

00:35:47,640 --> 00:35:53,420
productive using bigquery and air flow

00:35:50,069 --> 00:35:57,119
than we were with the Hadoop and an uzi

00:35:53,420 --> 00:36:00,710
so yeah we are very happy as you can see

00:35:57,119 --> 00:36:07,739
at scheme links and how you recommended

00:36:00,710 --> 00:36:11,849
thanks very much so I don't know if we

00:36:07,739 --> 00:36:14,249
have any time for questions but if if

00:36:11,849 --> 00:36:18,529
anyone has any question just let me know

00:36:14,249 --> 00:36:18,529
I hand go on

00:36:22,680 --> 00:36:28,859
but the subtext so some tag

00:36:26,579 --> 00:36:30,930
so essentially you remember when we

00:36:28,859 --> 00:36:32,400
talked about the operators different

00:36:30,930 --> 00:36:36,359
type of operators you have a basically

00:36:32,400 --> 00:36:41,790
dark operator where you use another dark

00:36:36,359 --> 00:36:45,480
as your task so so basically it just

00:36:41,790 --> 00:36:48,060
becomes similar to one of your tasks you

00:36:45,480 --> 00:36:50,369
can drill in into such a such a task and

00:36:48,060 --> 00:36:54,710
you can see like expansion of all the

00:36:50,369 --> 00:36:54,710
tasks that are within that other dug

00:36:55,730 --> 00:37:01,380
well we were reading a law that their

00:36:58,819 --> 00:37:03,750
scheduler is struggling with some of

00:37:01,380 --> 00:37:05,520
them because basically you know there

00:37:03,750 --> 00:37:08,780
will be scheduled on the same level

00:37:05,520 --> 00:37:11,160
there will be executed as a single task

00:37:08,780 --> 00:37:14,099
we didn't really see any problems with

00:37:11,160 --> 00:37:16,430
that but I was reading a lot about some

00:37:14,099 --> 00:37:16,430
issues

00:37:22,780 --> 00:37:28,700
well it depends how you schedule it how

00:37:25,970 --> 00:37:31,760
you set it up but this shouldn't be in a

00:37:28,700 --> 00:37:33,830
single point of failure I think probably

00:37:31,760 --> 00:37:36,140
using that user interface might be a

00:37:33,830 --> 00:37:40,100
single point of failure but a part of

00:37:36,140 --> 00:37:42,170
this you you can very easily do like

00:37:40,100 --> 00:37:46,730
multiple threads for the scheduler you

00:37:42,170 --> 00:37:52,960
have multiple workers so to show me the

00:37:46,730 --> 00:37:57,190
case I'm sorry for taking extra time

00:37:52,960 --> 00:37:57,190

YouTube URL: https://www.youtube.com/watch?v=gmYtb7ImnEI


