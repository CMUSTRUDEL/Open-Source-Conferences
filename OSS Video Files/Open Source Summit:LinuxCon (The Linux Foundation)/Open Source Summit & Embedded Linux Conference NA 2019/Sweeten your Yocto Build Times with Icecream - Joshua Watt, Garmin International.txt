Title: Sweeten your Yocto Build Times with Icecream - Joshua Watt, Garmin International
Publication date: 2019-09-16
Playlist: Open Source Summit & Embedded Linux Conference NA 2019
Description: 
	Sweeten your Yocto Build Times with Icecream - Joshua Watt, Garmin International

When building complex embedded systems with Yocto, a considerable amount of time can be spent waiting for builds to complete. One method of accelerating these builds is to use a distributed compiler such as icecream. Joshua will discuss the current state of icecream support in Yocto, pros and cons of using icecream, typical configurations, as well as tips, tricks, and quirks he has learned from using icecream for the past 2 years.
Captions: 
	00:00:00,030 --> 00:00:05,370
all right we probably better get started

00:00:02,990 --> 00:00:06,930
so my name is Joshua Watt and I'm here

00:00:05,370 --> 00:00:08,960
to talk to you today about sweetening

00:00:06,930 --> 00:00:11,910
your octo build times using ice cream a

00:00:08,960 --> 00:00:13,259
little bit about about myself I've been

00:00:11,910 --> 00:00:15,540
working for Garmin for the past ten

00:00:13,259 --> 00:00:18,270
years down in Kansas City and we've been

00:00:15,540 --> 00:00:20,850
using Yocto to do embedded Linux for

00:00:18,270 --> 00:00:22,170
about the past four years and I took an

00:00:20,850 --> 00:00:24,240
early interest in making our builds

00:00:22,170 --> 00:00:26,550
faster so we've been using ice cream for

00:00:24,240 --> 00:00:29,929
most of that time there's my email

00:00:26,550 --> 00:00:31,830
addresses if you'd like to contact me

00:00:29,929 --> 00:00:33,149
there's a brief outline of what I'm

00:00:31,830 --> 00:00:34,800
gonna go over I'm gonna cover what ice

00:00:33,149 --> 00:00:37,140
cream is why you should use ice cream

00:00:34,800 --> 00:00:38,430
how you how to use ice cream how to

00:00:37,140 --> 00:00:40,559
maximize the performance when you're

00:00:38,430 --> 00:00:42,649
using ice cream and then things that we

00:00:40,559 --> 00:00:47,579
can do in the future with ice cream

00:00:42,649 --> 00:00:49,770
first of all what is ice cream ice cream

00:00:47,579 --> 00:00:52,440
is a distributed compiler very similar

00:00:49,770 --> 00:00:54,449
to this you see however unlike DCC it

00:00:52,440 --> 00:00:59,460
uses a single scheduler to dispatch the

00:00:54,449 --> 00:01:01,079
jobs between the various nodes the

00:00:59,460 --> 00:01:02,910
advantage of a centralized scheduler is

00:01:01,079 --> 00:01:04,739
that it allows it to quickly make

00:01:02,910 --> 00:01:08,040
selections about which node should

00:01:04,739 --> 00:01:09,960
compile given jobs it's also able to

00:01:08,040 --> 00:01:11,880
easily distribute the jobs across the

00:01:09,960 --> 00:01:15,150
entire cluster to prevent nodes from

00:01:11,880 --> 00:01:16,890
getting too overloaded additionally it's

00:01:15,150 --> 00:01:19,140
able to factor in information about all

00:01:16,890 --> 00:01:21,090
of the nodes such as the CPU usage and

00:01:19,140 --> 00:01:23,210
memory usage and things like that when

00:01:21,090 --> 00:01:25,409
making scheduling determinations

00:01:23,210 --> 00:01:27,090
additionally cluster administration is

00:01:25,409 --> 00:01:28,710
easier because there's a centralized

00:01:27,090 --> 00:01:30,090
scheduler that you can talk to instead

00:01:28,710 --> 00:01:33,960
of having to go out and find all the

00:01:30,090 --> 00:01:35,549
individual nodes yourself I'm going to

00:01:33,960 --> 00:01:39,329
give you a brief overview about how ice

00:01:35,549 --> 00:01:45,810
cream works in this example we have two

00:01:39,329 --> 00:01:47,970
nodes node a and node B ice-cream itself

00:01:45,810 --> 00:01:50,520
has two components there is the

00:01:47,970 --> 00:01:53,520
ice-cream client compiler shim shown

00:01:50,520 --> 00:01:55,020
here that is invoked in place of GCC

00:01:53,520 --> 00:01:58,110
when you want to use ice cream to

00:01:55,020 --> 00:02:01,500
compile your source code there is also

00:01:58,110 --> 00:02:03,689
the ice cream daemon icy CD that runs

00:02:01,500 --> 00:02:06,149
here and talks to the scheduler and

00:02:03,689 --> 00:02:11,700
compiled source code on behalf of other

00:02:06,149 --> 00:02:13,560
clients the ice cream client shim will

00:02:11,700 --> 00:02:16,050
directly in a number of

00:02:13,560 --> 00:02:17,520
cases the first is when it determines

00:02:16,050 --> 00:02:19,290
that a compiler should happen locally

00:02:17,520 --> 00:02:21,360
and will simply pass the compile off the

00:02:19,290 --> 00:02:24,510
GCC wholesale let it compile and wait

00:02:21,360 --> 00:02:27,390
for the result the second case in which

00:02:24,510 --> 00:02:29,130
it uses GCC is when it wants to

00:02:27,390 --> 00:02:31,140
pre-process the source code for remote

00:02:29,130 --> 00:02:33,360
compiling it does this that there's a

00:02:31,140 --> 00:02:36,980
single self-contained source file that

00:02:33,360 --> 00:02:36,980
it can send over to be compiled remotely

00:02:37,550 --> 00:02:41,220
when the ice-cream client shim

00:02:39,540 --> 00:02:43,410
determines that a compile should happen

00:02:41,220 --> 00:02:45,780
remotely it will first talk to the

00:02:43,410 --> 00:02:48,209
daemon running on the localhost and ask

00:02:45,780 --> 00:02:49,410
it to select a node for compiling the

00:02:48,209 --> 00:02:51,810
daemon will in turn talk to the

00:02:49,410 --> 00:02:54,330
scheduler the scheduler will look at all

00:02:51,810 --> 00:02:59,190
the nodes on the system and select one

00:02:54,330 --> 00:03:01,799
in our example node B and then report

00:02:59,190 --> 00:03:03,290
that back to the daemon the daemon will

00:03:01,799 --> 00:03:06,030
in turn report that back to the shim

00:03:03,290 --> 00:03:09,870
which will then connect directly to the

00:03:06,030 --> 00:03:12,000
node B daemon if the node B daemon does

00:03:09,870 --> 00:03:13,920
not already have one the client will

00:03:12,000 --> 00:03:15,750
send over a tool chain to be used when

00:03:13,920 --> 00:03:19,230
compiling source code on behalf of node

00:03:15,750 --> 00:03:21,239
a finally it will send over the source

00:03:19,230 --> 00:03:24,530
code to be compiled using that tool

00:03:21,239 --> 00:03:24,530
chain and wait for the results come back

00:03:26,900 --> 00:03:31,200
one of the advantages of this setup is

00:03:29,489 --> 00:03:33,450
that the client shim only has to know

00:03:31,200 --> 00:03:35,730
how to talk to the Damons it's actually

00:03:33,450 --> 00:03:38,329
not really aware of the scheduler it

00:03:35,730 --> 00:03:41,519
only speaks a protocol with the Damons

00:03:38,329 --> 00:03:43,200
it's also important to note that your

00:03:41,519 --> 00:03:44,970
compiles do not flow through your local

00:03:43,200 --> 00:03:46,350
daemon or through the scheduler they go

00:03:44,970 --> 00:03:51,090
straight from the client shim to the

00:03:46,350 --> 00:03:55,859
remote daemon so why should you use ice

00:03:51,090 --> 00:03:57,600
cream to try and answer this question

00:03:55,859 --> 00:03:59,810
I've done some performance analysis

00:03:57,600 --> 00:04:02,489
using the bitbake commands shown here i

00:03:59,810 --> 00:04:05,340
analyze the results using the build set

00:04:02,489 --> 00:04:07,440
tools from a weak or for example build

00:04:05,340 --> 00:04:09,660
stat diff and then a couple of custom

00:04:07,440 --> 00:04:13,590
scripts that I wrote to do statistical

00:04:09,660 --> 00:04:17,489
analysis the reason for splitting up the

00:04:13,590 --> 00:04:19,139
command like this is that the do n fetch

00:04:17,489 --> 00:04:20,760
and do PAC commands can be highly

00:04:19,139 --> 00:04:22,950
variable t2 cashing in your internet

00:04:20,760 --> 00:04:24,990
connection speed and so pre running them

00:04:22,950 --> 00:04:25,690
like this and only analyzing the results

00:04:24,990 --> 00:04:27,280
from the second

00:04:25,690 --> 00:04:30,760
location of bitbake helps remove them as

00:04:27,280 --> 00:04:32,740
noise in the output if you would like to

00:04:30,760 --> 00:04:34,480
try these tests on your own cluster you

00:04:32,740 --> 00:04:39,460
can download the scripts at that github

00:04:34,480 --> 00:04:41,080
link here's the environment I used for

00:04:39,460 --> 00:04:42,940
testing it's the cluster that we use

00:04:41,080 --> 00:04:45,280
every day at Garmin we have about 21

00:04:42,940 --> 00:04:47,470
compiled nodes and about 184 total job

00:04:45,280 --> 00:04:52,450
capacity and there's my test machine

00:04:47,470 --> 00:04:54,490
it's not particularly spectacular this

00:04:52,450 --> 00:04:56,410
first chart is the build stat diff

00:04:54,490 --> 00:04:59,400
output showing the CPU time from the

00:04:56,410 --> 00:05:02,290
various tasks in a single example run

00:04:59,400 --> 00:05:04,450
the CPU time is the amount of time the

00:05:02,290 --> 00:05:06,670
task actually executed instructions on

00:05:04,450 --> 00:05:09,190
the CPU meaning it does not factor in

00:05:06,670 --> 00:05:11,560
time that the task was waiting on i/o or

00:05:09,190 --> 00:05:17,410
when it was suspended while other tasks

00:05:11,560 --> 00:05:19,090
ran the CPU time to column on the far

00:05:17,410 --> 00:05:20,920
right here shows the amount of time that

00:05:19,090 --> 00:05:23,830
that task took with ice cream enabled

00:05:20,920 --> 00:05:25,540
and the CPU one time column next to it

00:05:23,830 --> 00:05:28,360
shows the amount of time that that task

00:05:25,540 --> 00:05:30,130
took with ice cream disabled the

00:05:28,360 --> 00:05:32,290
absolute and relative DIF show the

00:05:30,130 --> 00:05:33,880
difference between those with negative

00:05:32,290 --> 00:05:38,770
numbers meaning that ice cream was

00:05:33,880 --> 00:05:40,930
faster when enabled as one might expect

00:05:38,770 --> 00:05:42,550
shipping off most of your compiles to be

00:05:40,930 --> 00:05:44,100
done remotely drastically reduces the

00:05:42,550 --> 00:05:46,600
amount of time that you run on the CPU

00:05:44,100 --> 00:05:48,790
with you can see some of these tasks

00:05:46,600 --> 00:05:52,750
have up to 90 percent reductions in CPU

00:05:48,790 --> 00:05:55,120
time the tasks that seem to benefit the

00:05:52,750 --> 00:05:57,370
most are those that have a high ratio of

00:05:55,120 --> 00:06:01,030
compiling to pre processing time for

00:05:57,370 --> 00:06:03,520
example the kernel qemu and most c++

00:06:01,030 --> 00:06:05,680
time spends farm or c++ code

00:06:03,520 --> 00:06:08,430
spends far more time compiling than it

00:06:05,680 --> 00:06:08,430
does pre-processing

00:06:08,580 --> 00:06:12,160
not everything gets a net benefit though

00:06:10,780 --> 00:06:13,720
as you can see down here at the bottom

00:06:12,160 --> 00:06:19,090
there are some tasks that got longer

00:06:13,720 --> 00:06:22,150
with ice cream enabled some of these do

00:06:19,090 --> 00:06:23,919
configure tasks are the result of do

00:06:22,150 --> 00:06:25,990
configure executing a lot of small test

00:06:23,919 --> 00:06:28,240
programs and small test programs don't

00:06:25,990 --> 00:06:30,100
play to ice-creams strength due to not

00:06:28,240 --> 00:06:32,950
having a very high ratio of compiling to

00:06:30,100 --> 00:06:34,900
pre-processing time additionally

00:06:32,950 --> 00:06:36,700
sometimes source code can't be compiled

00:06:34,900 --> 00:06:38,620
remotely but ice cream doesn't realize

00:06:36,700 --> 00:06:39,639
this until after it's tried and so it

00:06:38,620 --> 00:06:41,439
has to give up and redo

00:06:39,639 --> 00:06:45,009
locally and that can also inflate those

00:06:41,439 --> 00:06:46,810
numbers overall though you can see down

00:06:45,009 --> 00:06:50,080
here at the bottom there is a 40%

00:06:46,810 --> 00:06:51,849
reduction in overall CPU time down from

00:06:50,080 --> 00:06:56,949
about eight and a half hours to about

00:06:51,849 --> 00:06:58,810
five note that thus this time is not the

00:06:56,949 --> 00:07:03,129
same as the total elapsed time due to

00:06:58,810 --> 00:07:07,330
tasks executing in parallel for a more

00:07:03,129 --> 00:07:09,909
rigorous analysis I the total CPU time

00:07:07,330 --> 00:07:11,189
over 15 builds and divided it up per

00:07:09,909 --> 00:07:15,039
task

00:07:11,189 --> 00:07:17,469
ice cream directly affects four tasks do

00:07:15,039 --> 00:07:21,639
compile do compiled kernel modules do

00:07:17,469 --> 00:07:24,039
configure and do install all other tasks

00:07:21,639 --> 00:07:26,620
that were executed are in the other

00:07:24,039 --> 00:07:28,629
column and this overall column shows the

00:07:26,620 --> 00:07:30,490
total CPU time for the build which is

00:07:28,629 --> 00:07:39,039
basically the sum of the other five

00:07:30,490 --> 00:07:41,259
columns the difference between having

00:07:39,039 --> 00:07:43,330
ice cream or having ice cream disabled

00:07:41,259 --> 00:07:45,460
and having ice cream enabled was shown

00:07:43,330 --> 00:07:49,509
to be statistically significant for all

00:07:45,460 --> 00:07:51,159
changes meaning that we can be certain

00:07:49,509 --> 00:07:56,770
that these changes were actually caused

00:07:51,159 --> 00:07:57,189
by ice cream to try to understand this

00:07:56,770 --> 00:07:59,199
better

00:07:57,189 --> 00:08:01,419
I've normalized the results from the

00:07:59,199 --> 00:08:06,909
previous graph showing the percent

00:08:01,419 --> 00:08:10,710
change for each of the tasks this is

00:08:06,909 --> 00:08:10,710
just like the build stat diff relative

00:08:11,099 --> 00:08:18,399
difficulty was faster as you can see all

00:08:15,879 --> 00:08:23,050
the tasks accelerated by ice cream had

00:08:18,399 --> 00:08:25,960
at least some improvement all other

00:08:23,050 --> 00:08:29,050
tasks in the system had a very minor 1%

00:08:25,960 --> 00:08:30,610
increase in total CPU time which is

00:08:29,050 --> 00:08:32,529
actually good because it matches the

00:08:30,610 --> 00:08:34,479
expectation that tasks not directly

00:08:32,529 --> 00:08:36,159
accelerated by ice cream should have no

00:08:34,479 --> 00:08:37,839
change since ice cream isn't helping

00:08:36,159 --> 00:08:39,640
them it's actually a good indication

00:08:37,839 --> 00:08:43,419
that we have correctly partitioned our

00:08:39,640 --> 00:08:48,730
results overall there was a 44%

00:08:43,419 --> 00:08:50,680
reduction in CPU time we can do the same

00:08:48,730 --> 00:08:52,390
thing for the wall time the wall time

00:08:50,680 --> 00:08:53,020
measures the total amount of time that

00:08:52,390 --> 00:08:55,720
the tasks

00:08:53,020 --> 00:08:57,640
executed which means it includes the

00:08:55,720 --> 00:09:00,940
time that the tasks spent waiting on IO

00:08:57,640 --> 00:09:04,840
and when it was suspended while other

00:09:00,940 --> 00:09:07,270
tasks ran due to non determinism in the

00:09:04,840 --> 00:09:08,980
builds from parallelism and IO delays

00:09:07,270 --> 00:09:12,190
there's a lot more variance in these

00:09:08,980 --> 00:09:14,140
numbers from run to run generally

00:09:12,190 --> 00:09:17,460
however tasks that have a reduction in

00:09:14,140 --> 00:09:19,690
CPU time have a reduction in wall time

00:09:17,460 --> 00:09:23,620
there are a few downsides down here at

00:09:19,690 --> 00:09:25,750
the bottom again some of these are due

00:09:23,620 --> 00:09:29,530
to the increase in CPU time we saw from

00:09:25,750 --> 00:09:31,120
the CPU time slide others are due to a

00:09:29,530 --> 00:09:33,010
bug that I have not quite tracked down

00:09:31,120 --> 00:09:34,780
in ice cream where sometimes jobs will

00:09:33,010 --> 00:09:36,280
get sent off to be compiled remotely and

00:09:34,780 --> 00:09:37,750
get lost somewhere and then a really

00:09:36,280 --> 00:09:41,130
long time out has to elapse before it

00:09:37,750 --> 00:09:44,350
figures it out and recompiles it locally

00:09:41,130 --> 00:09:49,300
overall this build had a 30% reduction

00:09:44,350 --> 00:09:51,430
in wall time this is less than CPU time

00:09:49,300 --> 00:09:55,330
which is to be somewhat expected since

00:09:51,430 --> 00:09:57,220
ice cream does have some IO overhead it

00:09:55,330 --> 00:09:59,380
is interesting to see however that there

00:09:57,220 --> 00:10:02,080
are some tasks here not directly

00:09:59,380 --> 00:10:05,920
accelerated by ice cream that are faster

00:10:02,080 --> 00:10:08,980
and substantially so the question is can

00:10:05,920 --> 00:10:11,410
we attribute this to ice cream does ice

00:10:08,980 --> 00:10:14,020
cream make the CPU bound tasks that is

00:10:11,410 --> 00:10:15,610
that it accelerates IO bound enough that

00:10:14,020 --> 00:10:17,980
other tasks can run while they're

00:10:15,610 --> 00:10:20,140
waiting on their IO if that were the

00:10:17,980 --> 00:10:22,840
case we would expect to see a reduction

00:10:20,140 --> 00:10:25,480
in wall time for the other tasks while

00:10:22,840 --> 00:10:27,700
the CPU time remained close to zero and

00:10:25,480 --> 00:10:29,680
if you recall the CPU time was very

00:10:27,700 --> 00:10:33,790
close to zero so did the wall time

00:10:29,680 --> 00:10:35,560
change try and answer that question we

00:10:33,790 --> 00:10:40,120
can again look at the average total wall

00:10:35,560 --> 00:10:41,950
time over those 15 builds again here you

00:10:40,120 --> 00:10:44,380
can see with the do compile do compile

00:10:41,950 --> 00:10:45,460
kernel modules do configure and overall

00:10:44,380 --> 00:10:48,730
differences are statistically

00:10:45,460 --> 00:10:50,560
significant however there's too much of

00:10:48,730 --> 00:10:52,510
variance and do install and the other

00:10:50,560 --> 00:10:54,160
tasks which means we can't say if any

00:10:52,510 --> 00:10:58,570
difference in them was the result of ice

00:10:54,160 --> 00:11:01,120
cream I've again normalized the percent

00:10:58,570 --> 00:11:02,830
change first thing to note is that the

00:11:01,120 --> 00:11:04,300
error bars are much larger because

00:11:02,830 --> 00:11:06,460
there's a lot more variance in the wall

00:11:04,300 --> 00:11:09,230
time

00:11:06,460 --> 00:11:11,660
there's generally less improvement in

00:11:09,230 --> 00:11:13,580
wall time than CPU time because ice

00:11:11,660 --> 00:11:16,100
cream does have some I overhead IO

00:11:13,580 --> 00:11:21,680
overhead but there still does appear to

00:11:16,100 --> 00:11:23,300
be in that benefit because of the lack

00:11:21,680 --> 00:11:24,920
of significance we can't really say if

00:11:23,300 --> 00:11:27,170
this number is the result of ice cream

00:11:24,920 --> 00:11:33,470
we may need more tests and then we could

00:11:27,170 --> 00:11:35,120
say so decisively we can also look at

00:11:33,470 --> 00:11:38,290
the average elapsed build time and

00:11:35,120 --> 00:11:40,850
average CPU usage over those 15 builds

00:11:38,290 --> 00:11:42,920
for the average elapsed build time there

00:11:40,850 --> 00:11:49,430
is a reduction of 1,100 seconds

00:11:42,920 --> 00:11:53,630
approximately a 20 percent change the

00:11:49,430 --> 00:11:55,790
CPU usage had a 22% reduction it's

00:11:53,630 --> 00:11:57,050
important to note that this is actually

00:11:55,790 --> 00:11:58,520
the straight difference between these

00:11:57,050 --> 00:12:00,020
two columns and not the percent change

00:11:58,520 --> 00:12:01,160
because I think it's weird to take the

00:12:00,020 --> 00:12:06,260
percent change of something that's

00:12:01,160 --> 00:12:09,920
already a percentage it's also important

00:12:06,260 --> 00:12:11,710
to note that the CPU usage is actually

00:12:09,920 --> 00:12:15,920
measured over the elapsed time

00:12:11,710 --> 00:12:17,810
so the 22% reduction in CPU usage is

00:12:15,920 --> 00:12:21,590
actually a much larger reduction in

00:12:17,810 --> 00:12:23,380
total CPU cycles due to the it being

00:12:21,590 --> 00:12:26,870
measured over a shorter period of time

00:12:23,380 --> 00:12:28,790
if you do some rough calculations you

00:12:26,870 --> 00:12:31,220
can estimate with these numbers that

00:12:28,790 --> 00:12:33,410
there is about a 49% reduction in CPU

00:12:31,220 --> 00:12:35,990
cycles which fits fairly closely with

00:12:33,410 --> 00:12:44,260
the 44% reduction in CPU time we saw

00:12:35,990 --> 00:12:47,510
earlier so why should you use ice cream

00:12:44,260 --> 00:12:49,820
in our example we saw a 20% reduction in

00:12:47,510 --> 00:12:51,770
build time that is an earth-shattering

00:12:49,820 --> 00:12:56,480
by any means but as one of my co-worker

00:12:51,770 --> 00:12:58,580
says 20% is 20% more importantly however

00:12:56,480 --> 00:13:00,560
I find that it's a lot more pleasant to

00:12:58,580 --> 00:13:03,560
do builds when ice cream is enabled due

00:13:00,560 --> 00:13:05,420
to the reduction in overall CPU usage my

00:13:03,560 --> 00:13:06,920
computer doesn't just lock up for hours

00:13:05,420 --> 00:13:11,260
on end when I'm doing a build with ice

00:13:06,920 --> 00:13:14,360
cream enabled additionally many recipes

00:13:11,260 --> 00:13:16,130
do much better than the average such as

00:13:14,360 --> 00:13:17,750
the kernel and rebuilds of those

00:13:16,130 --> 00:13:19,200
individual recipes if you're doing

00:13:17,750 --> 00:13:23,250
iterative development can be

00:13:19,200 --> 00:13:24,510
much faster finally it's free and open

00:13:23,250 --> 00:13:30,180
embedded does most of the hard work for

00:13:24,510 --> 00:13:31,590
you in setting up a tool chain so how do

00:13:30,180 --> 00:13:36,570
you use ice cream to accelerate your

00:13:31,590 --> 00:13:39,470
builds to enable ice cream you had the

00:13:36,570 --> 00:13:43,500
following two lines to your local comm

00:13:39,470 --> 00:13:45,330
the ICC class replaces GCC with the ice

00:13:43,500 --> 00:13:47,280
cream client shim for the du configure

00:13:45,330 --> 00:13:50,490
du compile du compile kernel modules and

00:13:47,280 --> 00:13:52,530
do install tasks as we saw earlier ICC

00:13:50,490 --> 00:13:54,620
parallel make controls how parallel

00:13:52,530 --> 00:13:57,330
builds are when ice cream is enabled I

00:13:54,620 --> 00:13:59,340
generally find that it's best to set it

00:13:57,330 --> 00:14:02,100
to 3 to 4 times the number of CPU cores

00:13:59,340 --> 00:14:03,720
you have this variable is analogous to

00:14:02,100 --> 00:14:06,720
parallel make when you're not using ice

00:14:03,720 --> 00:14:08,700
cream it's obviously not quite this easy

00:14:06,720 --> 00:14:10,290
you do have to have ice cream installed

00:14:08,700 --> 00:14:12,450
on your host and you do have to be part

00:14:10,290 --> 00:14:14,160
of a properly configured cluster there's

00:14:12,450 --> 00:14:20,160
also a couple of other caveats that I

00:14:14,160 --> 00:14:22,020
will address as a side note enabling ice

00:14:20,160 --> 00:14:25,530
cream for your builds also enables it

00:14:22,020 --> 00:14:28,020
for your traditional SDK when you

00:14:25,530 --> 00:14:31,170
install a traditional SDK that was built

00:14:28,020 --> 00:14:32,910
with ice cream enabled it will try to

00:14:31,170 --> 00:14:36,750
detect ice cream on the host when it is

00:14:32,910 --> 00:14:39,390
installed if it finds it it will

00:14:36,750 --> 00:14:42,330
configure it to use ice cream in place

00:14:39,390 --> 00:14:46,520
of GCC in the SDK and then automatically

00:14:42,330 --> 00:14:46,520
create a tool chain for you to be used

00:14:48,290 --> 00:14:52,020
there are a few things you should be

00:14:50,040 --> 00:14:54,900
aware of if you try to combine ice cream

00:14:52,020 --> 00:14:58,020
and S State the first is that you should

00:14:54,900 --> 00:14:59,490
always get s state working first S State

00:14:58,020 --> 00:15:00,930
will give you much better performance

00:14:59,490 --> 00:15:03,780
improvement than ice cream ever will

00:15:00,930 --> 00:15:06,180
because s state allows you to skip

00:15:03,780 --> 00:15:09,840
entire recipes and tasks instead of just

00:15:06,180 --> 00:15:12,150
accelerating a few tasks the second

00:15:09,840 --> 00:15:16,530
thing to be aware of is that you can

00:15:12,150 --> 00:15:19,080
combine ice cream in S state but ICC BB

00:15:16,530 --> 00:15:23,100
class changes the task caches for the

00:15:19,080 --> 00:15:24,900
tasks that it accelerates as such you

00:15:23,100 --> 00:15:27,210
can't share s state between a host that

00:15:24,900 --> 00:15:30,540
has icy BB class enabled and one that

00:15:27,210 --> 00:15:33,250
does not the solution I found to this

00:15:30,540 --> 00:15:35,170
problem is to always inherit the ICCB

00:15:33,250 --> 00:15:38,050
and either your distro calm for your

00:15:35,170 --> 00:15:39,819
local comm and then use the ICC disabled

00:15:38,050 --> 00:15:42,610
variable to control if ice cream is

00:15:39,819 --> 00:15:44,199
actually used the ICC disabled variable

00:15:42,610 --> 00:15:46,360
does not change the task caches and

00:15:44,199 --> 00:15:51,670
therefore allows you to share estate

00:15:46,360 --> 00:15:55,139
between all the hosts so how do you

00:15:51,670 --> 00:15:58,149
maximize the performance of your cluster

00:15:55,139 --> 00:15:59,860
as you've seen some recipes don't build

00:15:58,149 --> 00:16:01,629
well with ice cream and some recipes

00:15:59,860 --> 00:16:02,430
don't actually build it all with ice

00:16:01,629 --> 00:16:05,050
cream

00:16:02,430 --> 00:16:08,620
you can blacklist these recipes using

00:16:05,050 --> 00:16:10,779
the ICC user package blacklist there is

00:16:08,620 --> 00:16:12,879
a shared system blacklist in ICC BB

00:16:10,779 --> 00:16:15,040
class but it doesn't have very much in

00:16:12,879 --> 00:16:17,079
it and it needs to be managed better so

00:16:15,040 --> 00:16:19,389
for now you just need to add this in

00:16:17,079 --> 00:16:21,389
your local comm to get everything to

00:16:19,389 --> 00:16:23,860
build

00:16:21,389 --> 00:16:25,990
additionally in my example tests I

00:16:23,860 --> 00:16:28,360
didn't blacklist any recipes that

00:16:25,990 --> 00:16:30,100
weren't performing well so one of the

00:16:28,360 --> 00:16:31,449
better ways to improve performance would

00:16:30,100 --> 00:16:34,889
be to blacklist some of those recipes

00:16:31,449 --> 00:16:34,889
that didn't build well

00:16:43,110 --> 00:16:46,270
network performance is crucial to

00:16:45,400 --> 00:16:48,430
getting good performance out of

00:16:46,270 --> 00:16:52,029
ice-cream you need a fast and low

00:16:48,430 --> 00:16:53,980
latency network in the cluster that I'd

00:16:52,029 --> 00:16:55,540
ran our tests on all of the nodes have a

00:16:53,980 --> 00:17:00,279
gigabit link between each other and

00:16:55,540 --> 00:17:01,779
they're all on a single subnet I don't

00:17:00,279 --> 00:17:03,610
recommend using anything less than 100

00:17:01,779 --> 00:17:06,130
megabits and even that's pretty slow

00:17:03,610 --> 00:17:08,170
when transferring the tool chains I also

00:17:06,130 --> 00:17:14,650
don't recommend Wi-Fi as latency and

00:17:08,170 --> 00:17:15,939
drops cause a lot of problems it's also

00:17:14,650 --> 00:17:17,730
important to try and keep up to date

00:17:15,939 --> 00:17:21,130
with the upstream version of ice-cream

00:17:17,730 --> 00:17:23,250
newer versions of GCC generally tend to

00:17:21,130 --> 00:17:25,900
require newer versions of ice cream and

00:17:23,250 --> 00:17:29,410
openembedded adopts new versions of GCC

00:17:25,900 --> 00:17:30,850
fairly quickly one of the problems with

00:17:29,410 --> 00:17:32,830
this is that ice cream generally only

00:17:30,850 --> 00:17:34,450
updates about once a year and it takes

00:17:32,830 --> 00:17:35,950
additional time for those to get into

00:17:34,450 --> 00:17:37,240
the latest version of your distro if

00:17:35,950 --> 00:17:42,100
you're even updating to the latest

00:17:37,240 --> 00:17:46,150
version of your distro fortunately

00:17:42,100 --> 00:17:47,980
however the ice cream client shim uses a

00:17:46,150 --> 00:17:49,410
stable and backwards compatible API to

00:17:47,980 --> 00:17:51,940
talk to the Damons

00:17:49,410 --> 00:17:55,710
this means it can pretty much be updated

00:17:51,940 --> 00:17:58,090
independently of the Damons and it works

00:17:55,710 --> 00:18:00,190
at garmin we have a fairly ingenious

00:17:58,090 --> 00:18:01,810
method for doing this we're doing all of

00:18:00,190 --> 00:18:04,300
our builds in a docker container anyway

00:18:01,810 --> 00:18:06,460
so we've included a patched client shim

00:18:04,300 --> 00:18:08,470
in that docker container and it talks to

00:18:06,460 --> 00:18:10,570
the host daemon running outside of the

00:18:08,470 --> 00:18:17,260
container this has worked very well for

00:18:10,570 --> 00:18:18,850
us on a wide variety of distros I also

00:18:17,260 --> 00:18:22,179
highly recommended using a dedicated

00:18:18,850 --> 00:18:25,150
scheduler you can set up your cluster

00:18:22,179 --> 00:18:27,460
such that all the nodes run the

00:18:25,150 --> 00:18:29,020
scheduler daemon and they'll

00:18:27,460 --> 00:18:31,240
automatically elect one from among

00:18:29,020 --> 00:18:32,830
themselves but this means your scheduler

00:18:31,240 --> 00:18:35,080
can change or disappear without much

00:18:32,830 --> 00:18:38,950
notice or can end up on someone's laptop

00:18:35,080 --> 00:18:41,740
running Wi-Fi with bad results it's

00:18:38,950 --> 00:18:43,300
fairly easy to install the scheduler on

00:18:41,740 --> 00:18:46,090
a box and throw it in the corner and

00:18:43,300 --> 00:18:48,910
forget about it we don't even have the

00:18:46,090 --> 00:18:50,440
scheduler participate as a compile node

00:18:48,910 --> 00:18:55,980
on our cluster because we don't need the

00:18:50,440 --> 00:18:58,240
extra capacity the scheduler also uses

00:18:55,980 --> 00:19:00,460
backwards-compatible protocol to talk to

00:18:58,240 --> 00:19:02,050
the Damon's so it's fairly easy to keep

00:19:00,460 --> 00:19:07,900
a dedicated scheduler up to date with

00:19:02,050 --> 00:19:09,250
the latest version of ice cream you also

00:19:07,900 --> 00:19:12,160
need to be careful about who's on your

00:19:09,250 --> 00:19:14,320
cluster virtual machines in particular

00:19:12,160 --> 00:19:16,090
tend to be bad cluster citizens they're

00:19:14,320 --> 00:19:18,370
constantly overestimating how good they

00:19:16,090 --> 00:19:20,110
are at compiling probably because they

00:19:18,370 --> 00:19:22,360
can't see the host CPU usage or memory

00:19:20,110 --> 00:19:24,190
usage and they often look completely

00:19:22,360 --> 00:19:27,430
completely idle which means the

00:19:24,190 --> 00:19:28,660
scheduler really likes to pick them but

00:19:27,430 --> 00:19:31,960
it turns out they actually compile

00:19:28,660 --> 00:19:33,550
really slowly a Garmin we either

00:19:31,960 --> 00:19:35,980
blacklist or if I'm feeling generous

00:19:33,550 --> 00:19:37,510
that day mark the virtual machines as no

00:19:35,980 --> 00:19:43,960
remote which means they can send out

00:19:37,510 --> 00:19:45,820
compiled jobs but not receive them ice

00:19:43,960 --> 00:19:47,530
cream has the ability to remotely pre

00:19:45,820 --> 00:19:50,410
process source code for even more

00:19:47,530 --> 00:19:53,200
performance this uses a feature of GCC

00:19:50,410 --> 00:19:55,330
called F directives only this directive

00:19:53,200 --> 00:19:56,770
only pre processes the file enough to

00:19:55,330 --> 00:19:58,450
get a single source file meaning it

00:19:56,770 --> 00:20:00,850
expands pound includes and a few other

00:19:58,450 --> 00:20:04,810
things but doesn't expand all the macros

00:20:00,850 --> 00:20:06,430
that's done remotely it does have some

00:20:04,810 --> 00:20:08,320
issues though and it doesn't work for

00:20:06,430 --> 00:20:11,500
all recipes so we haven't been able to

00:20:08,320 --> 00:20:12,670
enable it in the general case in OE if

00:20:11,500 --> 00:20:18,100
you're interested in looking at some of

00:20:12,670 --> 00:20:19,900
these issues I have a link there so

00:20:18,100 --> 00:20:21,100
what's next with ice cream because a

00:20:19,900 --> 00:20:25,420
couple things we could do to make the

00:20:21,100 --> 00:20:26,920
ice cream experience better one would be

00:20:25,420 --> 00:20:29,350
to try and build the ice cream client

00:20:26,920 --> 00:20:31,710
shim in OE core itself using an ICC

00:20:29,350 --> 00:20:34,420
native recipe or something like that

00:20:31,710 --> 00:20:37,780
this would have a few issues the primary

00:20:34,420 --> 00:20:39,280
one being that ICC native would have

00:20:37,780 --> 00:20:41,590
quite a few dependencies and those

00:20:39,280 --> 00:20:42,880
dependency dependencies themselves

00:20:41,590 --> 00:20:46,930
wouldn't be able to be built with ice

00:20:42,880 --> 00:20:51,640
cream ice cream can also support clang

00:20:46,930 --> 00:20:54,100
but I didn't add support for it it also

00:20:51,640 --> 00:20:55,960
works with C cache but again I didn't

00:20:54,100 --> 00:20:58,300
add support for that the trick there is

00:20:55,960 --> 00:21:00,340
to make sure that C cache runs first so

00:20:58,300 --> 00:21:03,180
that it can check its local cache before

00:21:00,340 --> 00:21:05,380
passing off the compiled ice cream I

00:21:03,180 --> 00:21:07,210
think it would be really interesting to

00:21:05,380 --> 00:21:07,760
gather more data from other clusters and

00:21:07,210 --> 00:21:10,280
compare

00:21:07,760 --> 00:21:12,170
for example some of the improvements I

00:21:10,280 --> 00:21:14,780
saw to do install were fairly marginal

00:21:12,170 --> 00:21:16,970
and other people on other clusters may

00:21:14,780 --> 00:21:19,070
not see any improvement or it may

00:21:16,970 --> 00:21:20,540
actually make to install tasks worse in

00:21:19,070 --> 00:21:25,130
which case maybe we shouldn't run

00:21:20,540 --> 00:21:26,270
ice-cream for to install if you're

00:21:25,130 --> 00:21:28,130
really interested you might be able to

00:21:26,270 --> 00:21:30,580
fix up some of the problems with GCC Zef

00:21:28,130 --> 00:21:35,540
directives only support to enable remote

00:21:30,580 --> 00:21:36,950
pre-processing and finally as I said ice

00:21:35,540 --> 00:21:39,350
cream is supported in the traditional

00:21:36,950 --> 00:21:41,480
SDK but as far as I know it doesn't work

00:21:39,350 --> 00:21:46,550
at the extensive velocity K so you could

00:21:41,480 --> 00:21:50,240
add that if you're interested in

00:21:46,550 --> 00:21:51,880
conclusion I've shown that ice cream is

00:21:50,240 --> 00:21:54,080
a distributed compiler and how it works

00:21:51,880 --> 00:21:55,430
I've shown that you can use ice cream to

00:21:54,080 --> 00:21:57,680
accelerate your builds what kind of

00:21:55,430 --> 00:22:01,040
acceleration you might expect and how to

00:21:57,680 --> 00:22:02,600
measure any improvement I've also shown

00:22:01,040 --> 00:22:03,860
you ways to contribute to the ongoing

00:22:02,600 --> 00:22:07,010
development of ice cream and

00:22:03,860 --> 00:22:10,520
openembedded additionally I encourage

00:22:07,010 --> 00:22:12,920
you to try it for yourself I think it

00:22:10,520 --> 00:22:14,870
would be really interesting if people

00:22:12,920 --> 00:22:16,790
would go try this on their own clusters

00:22:14,870 --> 00:22:23,570
and see what kind of performance

00:22:16,790 --> 00:22:25,310
improvements you can get there's a list

00:22:23,570 --> 00:22:27,290
of useful links that first one there is

00:22:25,310 --> 00:22:28,790
the open embedded wiki page that has

00:22:27,290 --> 00:22:30,520
most of the setup instructions on how to

00:22:28,790 --> 00:22:33,770
enable ice cream that you've seen here

00:22:30,520 --> 00:22:35,860
there's also some monitor programs there

00:22:33,770 --> 00:22:38,090
in the middle if you're interested in

00:22:35,860 --> 00:22:42,080
monitoring the status of your ice cream

00:22:38,090 --> 00:22:48,680
cluster and finally there's the github

00:22:42,080 --> 00:22:50,660
link for my tests special thanks to

00:22:48,680 --> 00:22:52,580
Garmin for allowing me to run an ice

00:22:50,660 --> 00:22:55,220
cream cluster at work and to give this

00:22:52,580 --> 00:22:57,140
talk the upstream ice cream developers I

00:22:55,220 --> 00:22:58,760
think ice cream is pretty awesome all

00:22:57,140 --> 00:23:00,920
the open embedded people who have helped

00:22:58,760 --> 00:23:04,570
me on ice cream and my wife who helped

00:23:00,920 --> 00:23:04,570
me significantly with the statistics

00:23:05,900 --> 00:23:23,430
any questions yes okay how many nodes

00:23:18,480 --> 00:23:25,500
does it take to see a win yeah I I think

00:23:23,430 --> 00:23:28,650
probably if I were gonna guess I'd say

00:23:25,500 --> 00:23:29,820
as few as three or four it I think it

00:23:28,650 --> 00:23:33,480
really depends on how many builds you

00:23:29,820 --> 00:23:38,130
want to do simultaneously so I've

00:23:33,480 --> 00:23:40,080
definitely seen just from my builds you

00:23:38,130 --> 00:23:43,020
know three or four extra nodes would

00:23:40,080 --> 00:23:45,060
allow me to do builds if you have two or

00:23:43,020 --> 00:23:47,100
three simultaneous builds you wouldn't

00:23:45,060 --> 00:23:49,200
need more nodes but I think a lot of

00:23:47,100 --> 00:23:53,340
that also has to do with how parallel

00:23:49,200 --> 00:23:56,060
you know you set your builds to be any

00:23:53,340 --> 00:23:56,060
other questions

00:24:18,380 --> 00:24:21,770
no I didn't look okay so the question

00:24:20,540 --> 00:24:23,390
was that I look at it for my cost

00:24:21,770 --> 00:24:25,520
directions I repeat it for my phone I'll

00:24:23,390 --> 00:24:29,360
look it up for a cost perspective of the

00:24:25,520 --> 00:24:30,830
cost of using ice cream versus getting a

00:24:29,360 --> 00:24:31,430
more powerful machine I think is what

00:24:30,830 --> 00:24:36,530
you're asking

00:24:31,430 --> 00:24:38,470
no I didn't yeah no the answer is the

00:24:36,530 --> 00:24:40,910
short answer is no I mean

00:24:38,470 --> 00:24:43,250
one of the advantages of ice cream I

00:24:40,910 --> 00:24:45,290
think is that you can just install it on

00:24:43,250 --> 00:24:46,190
the machines so what we do is we

00:24:45,290 --> 00:24:47,900
actually just have a script that

00:24:46,190 --> 00:24:49,460
everyone runs when they get their Linux

00:24:47,900 --> 00:24:51,200
machine and just sets up their machine

00:24:49,460 --> 00:24:53,030
and puts it on the cluster and so when

00:24:51,200 --> 00:24:55,820
they're not using their machine it

00:24:53,030 --> 00:24:59,240
compiles you know other people's stuff

00:24:55,820 --> 00:25:00,710
and so I mean other than the cluster

00:24:59,240 --> 00:25:02,360
maintenance there's not really a lot of

00:25:00,710 --> 00:25:04,460
cost associated with it

00:25:02,360 --> 00:25:07,100
at least from my perspective I don't I

00:25:04,460 --> 00:25:12,460
definitely don't spend much time

00:25:07,100 --> 00:25:12,460
administering the cluster so yeah

00:25:20,880 --> 00:25:27,580
right so the question was to use it for

00:25:24,370 --> 00:25:30,130
our release software just day-to-day it

00:25:27,580 --> 00:25:32,950
depends on where that release build is

00:25:30,130 --> 00:25:34,900
done so if the release we don't we don't

00:25:32,950 --> 00:25:37,150
enable ice cream for our CI servers for

00:25:34,900 --> 00:25:39,760
example because it doesn't really make

00:25:37,150 --> 00:25:41,170
sense we have kind of a internal

00:25:39,760 --> 00:25:42,550
cloud-based service and so we're just

00:25:41,170 --> 00:25:44,290
spinning up machines and tearing them

00:25:42,550 --> 00:25:45,700
down to do our builds and in that case

00:25:44,290 --> 00:25:47,950
ice cream doesn't really make sense

00:25:45,700 --> 00:25:50,890
because why would we spin up a bunch of

00:25:47,950 --> 00:25:52,360
machines you know just to run ice cream

00:25:50,890 --> 00:25:53,620
you know the machines are either fully

00:25:52,360 --> 00:25:55,630
utilize doing the bill they're not

00:25:53,620 --> 00:25:58,990
running right and so we don't we don't

00:25:55,630 --> 00:26:02,080
do it for our CI but sometimes we do

00:25:58,990 --> 00:26:05,890
releases on our you know desktop PCs and

00:26:02,080 --> 00:26:08,830
then usually that does use ice cream so

00:26:05,890 --> 00:26:53,440
it's a good question anyone else have

00:26:08,830 --> 00:26:55,300
questions yes yeah I I don't know if

00:26:53,440 --> 00:26:56,920
it's just because we have a reasonably

00:26:55,300 --> 00:26:59,410
sized cluster but I've never noticed

00:26:56,920 --> 00:27:01,720
that really being a problem I guess it's

00:26:59,410 --> 00:27:04,140
yeah the nodes do keep track of how

00:27:01,720 --> 00:27:06,340
utilized they are and the scheduler

00:27:04,140 --> 00:27:10,930
won't pick them if they're being heavily

00:27:06,340 --> 00:27:13,270
utilized at least as far as I've seen so

00:27:10,930 --> 00:27:15,100
it might just be because we have so many

00:27:13,270 --> 00:27:17,020
nodes in the cluster versus people

00:27:15,100 --> 00:27:18,730
actually building at any one time

00:27:17,020 --> 00:27:20,710
also the machines are a lot more idle

00:27:18,730 --> 00:27:24,340
than you might think they are you know

00:27:20,710 --> 00:27:28,500
your desktop yeah so it's idle for more

00:27:24,340 --> 00:27:28,500
time than you might expect so

00:27:30,230 --> 00:27:41,330
any other questions yes does it by

00:27:38,840 --> 00:27:43,520
default run with lower i/o and CPU

00:27:41,330 --> 00:27:54,040
priority I don't know the answer to that

00:27:43,520 --> 00:27:56,630
I I'm not actually sure I don't know

00:27:54,040 --> 00:27:58,130
like I said I've never noticed it being

00:27:56,630 --> 00:28:03,470
slow because people were using my

00:27:58,130 --> 00:28:07,190
machine to compile but I don't it might

00:28:03,470 --> 00:28:10,100
do that it might not it's kind of hard

00:28:07,190 --> 00:28:21,230
to say you can find out probably pretty

00:28:10,100 --> 00:28:23,120
easily yes sorry what yes that node a

00:28:21,230 --> 00:28:24,710
tool chain is cached and the scheduler

00:28:23,120 --> 00:28:27,980
will actually prefer nodes that already

00:28:24,710 --> 00:28:29,300
have your tool chain yes so that

00:28:27,980 --> 00:28:32,300
actually brings up an interesting point

00:28:29,300 --> 00:28:33,950
by default the cache is quite small it's

00:28:32,300 --> 00:28:37,850
like 50 megabytes which will hold about

00:28:33,950 --> 00:28:39,890
3 openembedded tool chains and so if you

00:28:37,850 --> 00:28:44,540
happen to run this on your own you might

00:28:39,890 --> 00:28:45,770
notice this behavior where your your

00:28:44,540 --> 00:28:47,840
compiles if you have a large enough

00:28:45,770 --> 00:28:49,430
cluster your compiles will always be on

00:28:47,840 --> 00:28:50,750
those same three machines and someone

00:28:49,430 --> 00:28:51,920
else's compiles will be happening on the

00:28:50,750 --> 00:28:53,690
other three machines and you'll just be

00:28:51,920 --> 00:28:55,310
using those three machines and that's

00:28:53,690 --> 00:28:56,840
mostly just because those machines are

00:28:55,310 --> 00:28:59,060
idle and already have your tool chain

00:28:56,840 --> 00:29:00,410
and so it makes more sense to reuse it

00:28:59,060 --> 00:29:02,510
than to try to transfer the tool chain

00:29:00,410 --> 00:29:05,480
you can't increase the size of the cache

00:29:02,510 --> 00:29:08,330
I think it's 50 megabytes by default but

00:29:05,480 --> 00:29:10,070
that's unfortunately a per node

00:29:08,330 --> 00:29:15,940
configuration item and you can't just

00:29:10,070 --> 00:29:15,940
change it across the whole cluster yes

00:29:19,820 --> 00:29:29,519
does it notice if people know so the

00:29:26,940 --> 00:29:31,619
tool chains are just given an arbitrary

00:29:29,519 --> 00:29:35,909
name and that's how they're identified

00:29:31,619 --> 00:29:38,279
and so what we do and openembedded is we

00:29:35,909 --> 00:29:40,710
generate a fairly unique name for the

00:29:38,279 --> 00:29:42,090
tool chain that's actually very long and

00:29:40,710 --> 00:29:45,080
it includes things like your GCC version

00:29:42,090 --> 00:29:50,759
and your host name and things like that

00:29:45,080 --> 00:29:53,269
and so there might be there probably

00:29:50,759 --> 00:29:55,499
said we could be some advantage there to

00:29:53,269 --> 00:29:56,850
trying to detect the same tool chain

00:29:55,499 --> 00:29:59,220
from multiple hosts because you do

00:29:56,850 --> 00:30:00,480
generally have that but it's not there

00:29:59,220 --> 00:30:05,700
yet that's that would be a good thing to

00:30:00,480 --> 00:30:22,080
improve if you could do that any other

00:30:05,700 --> 00:30:31,230
questions yes yeah yeah so what's the

00:30:22,080 --> 00:30:33,539
question yeah yeah it should it should

00:30:31,230 --> 00:30:35,340
be doable I know you can do it like

00:30:33,539 --> 00:30:37,200
there's actually an explicit whole thing

00:30:35,340 --> 00:30:39,299
on the ice cream

00:30:37,200 --> 00:30:43,679
documentation that says how to use C

00:30:39,299 --> 00:30:46,619
cash with ice cream but we don't really

00:30:43,679 --> 00:30:49,409
use C cash a lot so and I think if you

00:30:46,619 --> 00:30:52,259
look in ICC BB class it actually

00:30:49,409 --> 00:30:53,730
explicitly says C cash disabled equals

00:30:52,259 --> 00:30:55,679
one so it actually just explicitly turns

00:30:53,730 --> 00:30:57,869
it off because it's tricky to get it in

00:30:55,679 --> 00:30:59,129
the right order is the problem because

00:30:57,869 --> 00:31:03,840
they're both basically trying to do the

00:30:59,129 --> 00:31:06,240
same thing where they masquerade is GCC

00:31:03,840 --> 00:31:07,379
and then try to find GCC further on in

00:31:06,240 --> 00:31:08,999
the path and you can get into these

00:31:07,379 --> 00:31:11,399
weird cases where there each finding the

00:31:08,999 --> 00:31:12,720
other one in the path and you just get

00:31:11,399 --> 00:31:14,009
like they just keep passing it off to

00:31:12,720 --> 00:31:16,110
the other one and being like hey compile

00:31:14,009 --> 00:31:17,549
this for me and you know like they

00:31:16,110 --> 00:31:21,029
detect it and break the cycle but that

00:31:17,549 --> 00:31:23,039
means your compile fails and so but it's

00:31:21,029 --> 00:31:25,950
it's entirely doable I'm sure to get C

00:31:23,039 --> 00:31:28,220
cash working with it it just isn't right

00:31:25,950 --> 00:31:28,220
now

00:31:30,840 --> 00:31:34,710
are there any other questions

00:31:41,570 --> 00:31:44,200
all right

00:31:46,350 --> 00:31:52,850

YouTube URL: https://www.youtube.com/watch?v=VpK27pI64jQ


