Title: Keynote: Trusted AI: Bringing Trust Back into AI through Open Source - Todd Moore
Publication date: 2019-08-28
Playlist: Open Source Summit & Embedded Linux Conference NA 2019
Description: 
	Keynote: Trusted AI: Bringing Trust Back into AI through Open Source - Todd Moore, Vice President, Open Technology, Cognitive 

Applications, IBM As businesses move beyond experimentation to full-blown AI projects across the enterprise, they are recognizing that thereâ€™s more to successful implementations than simply having the right datasets, AI models and scalability. Increasingly, dimensions of trust, including fairness, robustness and explainability, are important metrics that help evaluate AI model behavior. Here, IBM is going to discuss how we are leveraging the power of Open Source to bring trust back in AI.
Captions: 
	00:00:00,030 --> 00:00:04,560
oh good morning everybody first just a

00:00:02,639 --> 00:00:07,290
quick nod to the open power team I was

00:00:04,560 --> 00:00:09,809
really excited to see what they had done

00:00:07,290 --> 00:00:11,370
there and the work that's going on I'm

00:00:09,809 --> 00:00:13,710
gonna talk a little bit about some AI

00:00:11,370 --> 00:00:15,809
things today trusted in particular but

00:00:13,710 --> 00:00:18,300
you know training models moving data

00:00:15,809 --> 00:00:20,010
around being able to attach accelerators

00:00:18,300 --> 00:00:23,340
all those things are really important

00:00:20,010 --> 00:00:25,439
and you know the open capi open mi stuff

00:00:23,340 --> 00:00:28,380
in particular is great and you know

00:00:25,439 --> 00:00:30,300
actually honestly I did work on the

00:00:28,380 --> 00:00:32,430
power instruction set there a long time

00:00:30,300 --> 00:00:35,700
ago so I'm really happy to see that that

00:00:32,430 --> 00:00:37,380
moved forward so so the people know me

00:00:35,700 --> 00:00:40,140
usually think of me either is that open

00:00:37,380 --> 00:00:43,559
source guy or the you know the guy who

00:00:40,140 --> 00:00:46,469
does things with open j/s or you know

00:00:43,559 --> 00:00:48,210
someone who is you know cloud oriented

00:00:46,469 --> 00:00:51,390
but actually I have some other

00:00:48,210 --> 00:00:53,910
responsibilities at IBM to the Center

00:00:51,390 --> 00:00:56,850
for open data and AI technology is one

00:00:53,910 --> 00:00:58,739
of my responsibilities and for those of

00:00:56,850 --> 00:01:00,390
you don't know what IBM joined this

00:00:58,739 --> 00:01:02,370
group a few years ago we actually helped

00:01:00,390 --> 00:01:05,339
found it it's the partnership on AI

00:01:02,370 --> 00:01:09,060
and and it's there to go create you know

00:01:05,339 --> 00:01:11,880
the best practices for using AI to

00:01:09,060 --> 00:01:14,520
actually champion ethical use of AI as

00:01:11,880 --> 00:01:16,680
well too so I was one of the IBM leaders

00:01:14,520 --> 00:01:18,540
who helped to go and Shepherd that

00:01:16,680 --> 00:01:22,320
forward and bring the group together and

00:01:18,540 --> 00:01:23,549
make that work so this topic however is

00:01:22,320 --> 00:01:26,189
something that I think we're all

00:01:23,549 --> 00:01:29,520
starting to see in the press quite a bit

00:01:26,189 --> 00:01:32,820
right trusting AI very difficult thing

00:01:29,520 --> 00:01:34,979
these days and and why is that right

00:01:32,820 --> 00:01:36,810
well you know we've seen some of the

00:01:34,979 --> 00:01:38,909
examples with people are tampering with

00:01:36,810 --> 00:01:40,560
models right when you tamper with it or

00:01:38,909 --> 00:01:43,740
tamper with the data to color something

00:01:40,560 --> 00:01:48,360
it can create a problem right is it fair

00:01:43,740 --> 00:01:50,939
does it have biases tremendous issues

00:01:48,360 --> 00:01:52,979
around race and and gender and other

00:01:50,939 --> 00:01:55,560
things that can potentially impact how

00:01:52,979 --> 00:01:59,310
society views the use of these models

00:01:55,560 --> 00:02:01,710
right can anybody understand how it made

00:01:59,310 --> 00:02:04,110
its decision you know you saw some

00:02:01,710 --> 00:02:06,450
examples earlier of deep learning and

00:02:04,110 --> 00:02:07,799
the path that a model can possibly take

00:02:06,450 --> 00:02:10,099
through its progression and how it got

00:02:07,799 --> 00:02:12,750
to where it is and it's going beyond

00:02:10,099 --> 00:02:13,740
necessarily the level of which we humans

00:02:12,750 --> 00:02:15,960
can

00:02:13,740 --> 00:02:17,850
Drock what's going on there right okay

00:02:15,960 --> 00:02:20,160
and then of course is it accountable

00:02:17,850 --> 00:02:21,390
right throughout its lifecycle can you

00:02:20,160 --> 00:02:23,730
figure out the decisions they've made

00:02:21,390 --> 00:02:25,680
and then what to do about those this is

00:02:23,730 --> 00:02:27,480
a hard one there's not really much out

00:02:25,680 --> 00:02:29,010
there yet and I'm not really going to

00:02:27,480 --> 00:02:31,740
talk much about that one but those are

00:02:29,010 --> 00:02:34,140
sort of the four pillars as as I look at

00:02:31,740 --> 00:02:37,160
decisions that are made by a computer

00:02:34,140 --> 00:02:40,740
for me they're going to impact my life

00:02:37,160 --> 00:02:43,260
so in the news I think you know just to

00:02:40,740 --> 00:02:46,080
point this one back out you see an image

00:02:43,260 --> 00:02:49,470
there right and that image is is what we

00:02:46,080 --> 00:02:50,670
all would see however you know the

00:02:49,470 --> 00:02:52,170
computer sees something a little

00:02:50,670 --> 00:02:53,640
different right it's all math it's ones

00:02:52,170 --> 00:02:55,770
and zeroes put together looking at the

00:02:53,640 --> 00:02:57,900
bit you know the little bit map around

00:02:55,770 --> 00:03:00,000
everything and trying to figure out you

00:02:57,900 --> 00:03:01,680
know what that is okay great

00:03:00,000 --> 00:03:04,260
we saw a little of that earlier right

00:03:01,680 --> 00:03:06,660
but then if you go and attack that

00:03:04,260 --> 00:03:07,350
target you can actually have that same

00:03:06,660 --> 00:03:09,210
image

00:03:07,350 --> 00:03:11,280
we'll see it we'll look at it the human

00:03:09,210 --> 00:03:13,530
eye won't see anything wrong with it but

00:03:11,280 --> 00:03:14,640
it's now been attacked and in the AI

00:03:13,530 --> 00:03:17,880
sees nothing

00:03:14,640 --> 00:03:20,250
now this is adversarial machine learning

00:03:17,880 --> 00:03:23,910
right and people can actually go and use

00:03:20,250 --> 00:03:26,090
this to attack models and come out with

00:03:23,910 --> 00:03:28,770
outcomes that you don't want to have

00:03:26,090 --> 00:03:29,970
there's and I'm not going to really go

00:03:28,770 --> 00:03:32,570
into this when there's an adversarial

00:03:29,970 --> 00:03:34,860
robustness toolbox that IBM has done

00:03:32,570 --> 00:03:38,730
there's a if you want to go to the URL

00:03:34,860 --> 00:03:41,340
it's art - demo dot my bluemix.net and

00:03:38,730 --> 00:03:43,520
you can see this at work and you can see

00:03:41,340 --> 00:03:46,740
how to mitigate those attacks as well -

00:03:43,520 --> 00:03:48,780
my favorite example on that is there's a

00:03:46,740 --> 00:03:50,910
cat it's a Siamese cat you can look at

00:03:48,780 --> 00:03:53,760
it you can attack it and when you attack

00:03:50,910 --> 00:03:56,220
it the AI decides it's an ambulance

00:03:53,760 --> 00:03:58,440
right so you know but to you that

00:03:56,220 --> 00:03:59,820
picture just looks like a cat so this is

00:03:58,440 --> 00:04:01,620
something that's it's really a very

00:03:59,820 --> 00:04:03,260
serious problem that we have to make

00:04:01,620 --> 00:04:06,690
sure is we want to trust these models

00:04:03,260 --> 00:04:08,280
okay bias a pretty famous thing out in

00:04:06,690 --> 00:04:10,650
the press as well - you can google this

00:04:08,280 --> 00:04:13,230
you could look it up north points

00:04:10,650 --> 00:04:15,480
compass system was engaged out there

00:04:13,230 --> 00:04:17,520
they thought they were helping with the

00:04:15,480 --> 00:04:19,820
process of figuring out you know what

00:04:17,520 --> 00:04:22,049
kind of sentencing should happen and

00:04:19,820 --> 00:04:24,720
looking at folks and building a model

00:04:22,049 --> 00:04:27,130
and hey great but it you know it flagged

00:04:24,720 --> 00:04:30,400
black defendant defendants

00:04:27,130 --> 00:04:32,680
almost 2x as much as white defendants

00:04:30,400 --> 00:04:37,450
who already had had a previous criminal

00:04:32,680 --> 00:04:39,730
defense or offense I should say so this

00:04:37,450 --> 00:04:41,680
is things that you know the press

00:04:39,730 --> 00:04:44,590
discovered by going in and doing

00:04:41,680 --> 00:04:46,120
investigative reporting right and people

00:04:44,590 --> 00:04:47,860
more and more and more are gonna go

00:04:46,120 --> 00:04:50,530
start looking at these things as AI

00:04:47,860 --> 00:04:52,870
technologies are employed to basically

00:04:50,530 --> 00:04:55,600
say is this being done well is it being

00:04:52,870 --> 00:04:57,520
done fairly right bias is clearly a

00:04:55,600 --> 00:04:59,170
problem and it can creep into the data

00:04:57,520 --> 00:05:01,060
it can creep into the model we there's

00:04:59,170 --> 00:05:05,710
there's lots of things that can cause

00:05:01,060 --> 00:05:07,660
this to happen so IBM how would we solve

00:05:05,710 --> 00:05:09,070
this let's do some open source stuff

00:05:07,660 --> 00:05:10,600
right that's what I do

00:05:09,070 --> 00:05:14,170
we've joined a lot of organizations we

00:05:10,600 --> 00:05:15,970
have this long history so there's a

00:05:14,170 --> 00:05:18,070
trusted AI lifecycle that we have to go

00:05:15,970 --> 00:05:19,600
and engage so we've created some open

00:05:18,070 --> 00:05:21,820
source projects we've put them out there

00:05:19,600 --> 00:05:23,380
there's the abbess Oreo robustus toolbox

00:05:21,820 --> 00:05:25,510
I just talked about that there's AI

00:05:23,380 --> 00:05:26,800
fairness and it's a I explained ability

00:05:25,510 --> 00:05:28,720
and soon there'll be something coming

00:05:26,800 --> 00:05:31,840
for lineage as well too so let's just

00:05:28,720 --> 00:05:34,450
talk about fairness and understand

00:05:31,840 --> 00:05:36,760
ability explain ability so you know in

00:05:34,450 --> 00:05:38,740
in the fairness world we want to make

00:05:36,760 --> 00:05:41,440
sure that we get the biases out of the

00:05:38,740 --> 00:05:45,610
model so if you go out and take a look

00:05:41,440 --> 00:05:47,830
there's AI F 360 my bluemix.net again

00:05:45,610 --> 00:05:51,700
you can get quite a bit more data on

00:05:47,830 --> 00:05:54,120
this that same compass example go click

00:05:51,700 --> 00:05:56,740
on it try it see what happens just

00:05:54,120 --> 00:05:58,870
simply re waiting that model just a

00:05:56,740 --> 00:06:00,940
little bit could take almost all of the

00:05:58,870 --> 00:06:04,630
bias out of that model of course there's

00:06:00,940 --> 00:06:06,910
a whole series of different algorithms

00:06:04,630 --> 00:06:09,130
you can use metrics you can look at is

00:06:06,910 --> 00:06:10,810
at least ten I think it's more like

00:06:09,130 --> 00:06:13,660
twelve mitigation algorithms right now

00:06:10,810 --> 00:06:15,720
in over seventy fairness metrics that

00:06:13,660 --> 00:06:21,580
are in play here it's a Python package

00:06:15,720 --> 00:06:24,760
explained ability same thing okay think

00:06:21,580 --> 00:06:26,590
about being out there and you're going

00:06:24,760 --> 00:06:29,500
to get a loan right something that we

00:06:26,590 --> 00:06:31,510
all worry about right in our lives we

00:06:29,500 --> 00:06:33,640
want to finance that new boat or the new

00:06:31,510 --> 00:06:35,110
house or whatever and there's going to

00:06:33,640 --> 00:06:37,390
loan officer who's going to interact

00:06:35,110 --> 00:06:39,849
with that and the financial institution

00:06:37,390 --> 00:06:41,259
behind them they've got models right

00:06:39,849 --> 00:06:43,449
they look at a lot of data they look at

00:06:41,259 --> 00:06:44,919
you your ones and zeros and numbers and

00:06:43,449 --> 00:06:47,020
things and they crunch that and they

00:06:44,919 --> 00:06:49,569
have a machine learning model that gives

00:06:47,020 --> 00:06:51,159
you a score and basically decides

00:06:49,569 --> 00:06:52,869
whether you get that credit or not and

00:06:51,159 --> 00:06:55,089
there's many factors that might go into

00:06:52,869 --> 00:06:58,179
that how many times have you gone and

00:06:55,089 --> 00:06:59,679
applied for credit recently have you

00:06:58,179 --> 00:07:02,289
ever been late on payments within

00:06:59,679 --> 00:07:03,009
somebody days etc they look at a lot of

00:07:02,289 --> 00:07:05,709
different

00:07:03,009 --> 00:07:08,679
waiting's and things to characteristics

00:07:05,709 --> 00:07:10,389
to come up with this so what we've seen

00:07:08,679 --> 00:07:12,279
and this is a project to be put out

00:07:10,389 --> 00:07:15,969
within the last couple of weeks it's a

00:07:12,279 --> 00:07:21,159
aia x360 or explained ability model is

00:07:15,969 --> 00:07:24,039
that you can actually go in and use

00:07:21,159 --> 00:07:25,240
tools to help people explain how the AI

00:07:24,039 --> 00:07:27,909
made its decision

00:07:25,240 --> 00:07:29,379
looking at the tags explaining how this

00:07:27,909 --> 00:07:31,990
goes and and think about it there's

00:07:29,379 --> 00:07:33,639
multiple constituents in this case that

00:07:31,990 --> 00:07:35,679
I just talked about the loan case right

00:07:33,639 --> 00:07:38,319
there's certainly the end user you who

00:07:35,679 --> 00:07:40,119
wants to understand how the AI made its

00:07:38,319 --> 00:07:41,919
decision and it might be some specific

00:07:40,119 --> 00:07:43,899
things that you should get feedback on

00:07:41,919 --> 00:07:47,259
within that but then there's a loan

00:07:43,899 --> 00:07:49,479
officer who wants to see how you compare

00:07:47,259 --> 00:07:51,249
to other people and making his decision

00:07:49,479 --> 00:07:53,259
as well as the scoring's that you had

00:07:51,249 --> 00:07:54,879
across various aspects of that model and

00:07:53,259 --> 00:07:56,649
then there's of course the data

00:07:54,879 --> 00:07:58,389
scientists who's trying to keep that

00:07:56,649 --> 00:08:00,909
thing up over time and man it's a life

00:07:58,389 --> 00:08:02,860
cycle of it and understand how it's

00:08:00,909 --> 00:08:05,619
making its decisions over time how it's

00:08:02,860 --> 00:08:07,029
being improved changed etc so this is a

00:08:05,619 --> 00:08:15,039
toolbox that will help you go and do

00:08:07,029 --> 00:08:17,169
that all right so we're not just putting

00:08:15,039 --> 00:08:20,019
these things out an open source these

00:08:17,169 --> 00:08:21,189
are projects that we're using internally

00:08:20,019 --> 00:08:24,189
as well

00:08:21,189 --> 00:08:26,469
if you come to IBM and take a look at

00:08:24,189 --> 00:08:28,509
what's an open scale you'll see that

00:08:26,469 --> 00:08:31,749
these capabilities are things that we've

00:08:28,509 --> 00:08:34,479
gone and built in to the dashboard the

00:08:31,749 --> 00:08:37,329
insights dashboard that helps us track

00:08:34,479 --> 00:08:40,209
and measure outcomes in AI and then of

00:08:37,329 --> 00:08:42,339
course what's become much more important

00:08:40,209 --> 00:08:45,670
these days is managing regulatory

00:08:42,339 --> 00:08:47,889
compliance so you know I got a chance

00:08:45,670 --> 00:08:49,779
the other day to meet with ibm's officer

00:08:47,889 --> 00:08:52,630
who's in charge of these things who's

00:08:49,779 --> 00:08:53,660
using the tools that we've created and

00:08:52,630 --> 00:08:55,970
working with the very

00:08:53,660 --> 00:08:58,399
teams who have these models to make sure

00:08:55,970 --> 00:09:01,819
that they pass the test that ISO tests

00:08:58,399 --> 00:09:05,199
and other things that we have to course

00:09:01,819 --> 00:09:07,879
maintain regulatory compliance right so

00:09:05,199 --> 00:09:10,490
decisions get made they get made but a

00:09:07,879 --> 00:09:12,709
lot more trust we can track our models

00:09:10,490 --> 00:09:14,629
you too can take advantage of these

00:09:12,709 --> 00:09:17,779
things they're out in the open you can

00:09:14,629 --> 00:09:22,790
come you can participate it's it's part

00:09:17,779 --> 00:09:25,279
of product as well down to so being the

00:09:22,790 --> 00:09:26,899
open source guy how else can we go get

00:09:25,279 --> 00:09:29,629
more involved in this what can I be I'm

00:09:26,899 --> 00:09:32,629
going to well we've announced that we're

00:09:29,629 --> 00:09:35,329
joining the Linux Foundation AI group L

00:09:32,629 --> 00:09:38,689
FAI right they were formed a year or so

00:09:35,329 --> 00:09:40,819
ago I guess we've been informally part

00:09:38,689 --> 00:09:44,689
of it for a while working out in some of

00:09:40,819 --> 00:09:47,449
the work groups like the ML workflow but

00:09:44,689 --> 00:09:49,579
we want to take this trust topic into

00:09:47,449 --> 00:09:51,110
the discussion that's going on there so

00:09:49,579 --> 00:09:53,449
you're gonna see us very active in that

00:09:51,110 --> 00:09:54,470
I invite you all as well to come out and

00:09:53,449 --> 00:09:56,389
join the group we think it's a great

00:09:54,470 --> 00:09:58,759
group they've got some good projects

00:09:56,389 --> 00:10:01,069
that are going on within it and I really

00:09:58,759 --> 00:10:02,899
thank the Linux Foundation for for

00:10:01,069 --> 00:10:05,029
taking on this topic with us as well too

00:10:02,899 --> 00:10:08,600
so trust and transparency you'll see us

00:10:05,029 --> 00:10:10,339
doing a lot more with it there okay the

00:10:08,600 --> 00:10:11,990
other place to come and join in you want

00:10:10,339 --> 00:10:14,480
to participate in the code understand

00:10:11,990 --> 00:10:17,149
the projects be part of it come to that

00:10:14,480 --> 00:10:18,589
code aid org site that's an easy way of

00:10:17,149 --> 00:10:21,319
finding it or you can find the code on

00:10:18,589 --> 00:10:23,240
github just go out do a search and pick

00:10:21,319 --> 00:10:26,209
it up and we're happy to go and

00:10:23,240 --> 00:10:28,519
participate with you so that was it I

00:10:26,209 --> 00:10:30,649
just want to say thank you and hope you

00:10:28,519 --> 00:10:32,550
all have a an excellent conference here

00:10:30,649 --> 00:10:34,790
this week

00:10:32,550 --> 00:10:34,790

YouTube URL: https://www.youtube.com/watch?v=CBEaQukG0ZQ


