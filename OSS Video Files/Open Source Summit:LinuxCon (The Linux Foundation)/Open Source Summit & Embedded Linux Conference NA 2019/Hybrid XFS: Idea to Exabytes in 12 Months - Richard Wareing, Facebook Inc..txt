Title: Hybrid XFS: Idea to Exabytes in 12 Months - Richard Wareing, Facebook Inc.
Publication date: 2019-09-16
Playlist: Open Source Summit & Embedded Linux Conference NA 2019
Description: 
	Hybrid XFS: Idea to Exabytes in 12 Months - Richard Wareing, Facebook Inc.

This talk will be broken down into three parts:Part 1 will describe some of the challenges using many open-source file systems (e.g. ext4, XFS) with clustered storage system such as GlusterFS, HDFS or even closed source exa-byte systems.Part 2 will describe what Hybrid XFS actually is: a form of XFS where we leverage two different media types (a flash drive & traditional HDDs) along with a long forgotten feature of XFS to create a single high performing file system suitable for the demands of exa-byte scale clustered storage.And finally in part 3, Richard will delve into how we went from a discussion & patch sets on the XFS mailing list to rolling out to an entire fleet of storage servers at Facebook in only 12 months. Moving fast, and *not* breaking things.
Captions: 
	00:00:00,030 --> 00:00:06,629
all right let's get rolling we have

00:00:03,470 --> 00:00:08,820
quite a bit to get through so hey my

00:00:06,629 --> 00:00:11,099
name is Richard wearing I'm a production

00:00:08,820 --> 00:00:15,780
junior at Facebook I'm on the warehouse

00:00:11,099 --> 00:00:18,230
foundation team and I work on storage

00:00:15,780 --> 00:00:20,810
related problems in the data warehouse

00:00:18,230 --> 00:00:25,350
specifically on a storage system we use

00:00:20,810 --> 00:00:29,689
called warm storage so I'm gonna kind of

00:00:25,350 --> 00:00:33,480
talk about an idea we came up with

00:00:29,689 --> 00:00:35,579
called hybrid XFS and kind of the story

00:00:33,480 --> 00:00:37,140
and background on how he came from the

00:00:35,579 --> 00:00:39,899
from an idea all the way to got a

00:00:37,140 --> 00:00:42,390
rolling us out to an exabyte storage

00:00:39,899 --> 00:00:46,200
system really multi exabyte storage

00:00:42,390 --> 00:00:47,309
system in about 12 months so first I

00:00:46,200 --> 00:00:48,510
think before you even get into the

00:00:47,309 --> 00:00:50,940
problem I think it's important to kind

00:00:48,510 --> 00:00:54,239
of understand like you know as a store

00:00:50,940 --> 00:00:55,680
engineer engineer like you know at

00:00:54,239 --> 00:00:57,690
Facebook at least like what is our job

00:00:55,680 --> 00:00:59,820
what is our what is our goal and it's

00:00:57,690 --> 00:01:03,180
really to keep our system storage bound

00:00:59,820 --> 00:01:04,979
as long as possible I always tell new

00:01:03,180 --> 00:01:06,570
engineers that come onto our teams I was

00:01:04,979 --> 00:01:09,570
like your job is to take that dotted

00:01:06,570 --> 00:01:12,659
line and keep kicking it as far to the

00:01:09,570 --> 00:01:15,540
right as you possibly can where storage

00:01:12,659 --> 00:01:17,729
engineers not IO engineers we want to

00:01:15,540 --> 00:01:24,210
keep these systems being used for

00:01:17,729 --> 00:01:25,830
storage so what kind of what are some of

00:01:24,210 --> 00:01:28,650
the factors that kind of make this job

00:01:25,830 --> 00:01:31,619
pretty tough so the first one is if we

00:01:28,650 --> 00:01:35,520
look at the vendor roadmaps you'll kind

00:01:31,619 --> 00:01:37,979
of see something like this we have kind

00:01:35,520 --> 00:01:41,579
of traditional hard drive technology

00:01:37,979 --> 00:01:44,220
also called PM rpm or Plus that we use

00:01:41,579 --> 00:01:48,990
today and these work all the way up into

00:01:44,220 --> 00:01:51,299
around 14 TB even maybe 16 TV and but

00:01:48,990 --> 00:01:52,860
coming online in like 2020 2022 and

00:01:51,299 --> 00:01:55,079
onward is going to be the hammer and

00:01:52,860 --> 00:01:56,310
Namur drives and the densities as you

00:01:55,079 --> 00:01:58,799
can see are going to be getting pretty

00:01:56,310 --> 00:02:02,610
insane like there's talk of things like

00:01:58,799 --> 00:02:05,159
40 terabyte terabyte drives Oh to say

00:02:02,610 --> 00:02:07,170
the mid mid point of the next decade so

00:02:05,159 --> 00:02:10,560
these are these are pretty kind of

00:02:07,170 --> 00:02:11,520
daunting sizes of drives and that's

00:02:10,560 --> 00:02:14,370
gonna end

00:02:11,520 --> 00:02:16,140
a complicating factor here is if we look

00:02:14,370 --> 00:02:18,600
at the amount of i/o that these drives

00:02:16,140 --> 00:02:20,040
are giving us it has not kept up I think

00:02:18,600 --> 00:02:21,870
this is kind of a fact everyone kind of

00:02:20,040 --> 00:02:25,400
knows about its if you look at it

00:02:21,870 --> 00:02:27,300
through the lens of AI ops per terabyte

00:02:25,400 --> 00:02:28,410
you end up with kind of a chart

00:02:27,300 --> 00:02:31,710
something like this so you can kind of

00:02:28,410 --> 00:02:33,210
see it's pretty precipitous decline so

00:02:31,710 --> 00:02:35,940
so some of that is just kind of a

00:02:33,210 --> 00:02:37,800
strategic concern kind of the landscape

00:02:35,940 --> 00:02:40,590
is changing very quickly and we need to

00:02:37,800 --> 00:02:44,160
kind of react to this the second thing

00:02:40,590 --> 00:02:45,740
is POSIX filesystem behavior we've built

00:02:44,160 --> 00:02:49,560
a lot of our storage systems on

00:02:45,740 --> 00:02:52,050
basically x FS and xf s is a great

00:02:49,560 --> 00:02:54,150
general-purpose file system but it was

00:02:52,050 --> 00:02:56,610
not designed with kind of distributed

00:02:54,150 --> 00:02:59,220
storage in mind and in doing so it has

00:02:56,610 --> 00:03:02,430
certain kind of behaviors which make

00:02:59,220 --> 00:03:04,920
total sense from the point of having a

00:03:02,430 --> 00:03:06,990
file system that runs on a UNIX machine

00:03:04,920 --> 00:03:09,390
or a Linux machines and you know if you

00:03:06,990 --> 00:03:10,920
look at XFS when you do a writes it's

00:03:09,390 --> 00:03:11,910
going to journal the metadata it's going

00:03:10,920 --> 00:03:13,190
to write the data block and then

00:03:11,910 --> 00:03:17,400
eventually it's going to flush that

00:03:13,190 --> 00:03:19,950
flush there journal and on the read side

00:03:17,400 --> 00:03:22,560
potentially you'll have a hit on the

00:03:19,950 --> 00:03:25,020
drive for reading the metadata if it's

00:03:22,560 --> 00:03:26,430
not in page cache and finally it will

00:03:25,020 --> 00:03:29,070
read the data so you can kind of see

00:03:26,430 --> 00:03:30,420
like for small iOS instead of doing one

00:03:29,070 --> 00:03:34,470
you're potentially doing three or were

00:03:30,420 --> 00:03:36,180
potentially doing - we have to kind of

00:03:34,470 --> 00:03:37,560
when we started kind of digging into

00:03:36,180 --> 00:03:40,440
this a little bit more we we use the

00:03:37,560 --> 00:03:42,540
tool called block trace and you can kind

00:03:40,440 --> 00:03:45,270
of see when we did it on our systems you

00:03:42,540 --> 00:03:47,820
kind of see things like this 24 percent

00:03:45,270 --> 00:03:49,290
of all the iOS so going into the file

00:03:47,820 --> 00:03:51,750
system are actually meditative rights

00:03:49,290 --> 00:03:53,880
you start adding up this this amount of

00:03:51,750 --> 00:03:55,830
i/o it's actually like non-trivial you

00:03:53,880 --> 00:03:58,650
start adding it up on an exabyte storage

00:03:55,830 --> 00:04:00,000
system it's a lot of money so

00:03:58,650 --> 00:04:02,400
traditionally how do people deal with

00:04:00,000 --> 00:04:03,780
this kind of the standard Linux answer

00:04:02,400 --> 00:04:07,230
is like hey page caps that's what it's

00:04:03,780 --> 00:04:09,570
for you just like put a bunch of DRAM in

00:04:07,230 --> 00:04:11,760
your machines it just works and indeed

00:04:09,570 --> 00:04:14,370
it's super simple works for almost any

00:04:11,760 --> 00:04:17,340
storage system the cons are its DRAM is

00:04:14,370 --> 00:04:19,350
not free at scale becomes really

00:04:17,340 --> 00:04:21,030
expensive there's also little control

00:04:19,350 --> 00:04:22,950
our software engineers kind of want

00:04:21,030 --> 00:04:24,910
pretty nuanced control of like what is

00:04:22,950 --> 00:04:27,280
going to be cached and what's

00:04:24,910 --> 00:04:29,170
we tend to have more information about

00:04:27,280 --> 00:04:31,060
the data than the operating system so we

00:04:29,170 --> 00:04:33,010
can kind of make those decisions a

00:04:31,060 --> 00:04:36,400
little bit better so as soon as we go

00:04:33,010 --> 00:04:38,080
into that area page cache becomes less

00:04:36,400 --> 00:04:40,210
useful the other thing we actually found

00:04:38,080 --> 00:04:41,290
just maybe not on non-obvious our

00:04:40,210 --> 00:04:44,470
systems are pretty lean when we're

00:04:41,290 --> 00:04:46,600
making these systems we on the storage

00:04:44,470 --> 00:04:48,670
side we don't put piles of memory and

00:04:46,600 --> 00:04:50,800
CPU into them we try and keep them as

00:04:48,670 --> 00:04:52,390
lean as possible so things like memory

00:04:50,800 --> 00:04:55,390
bandwidth actually becomes a concern so

00:04:52,390 --> 00:04:58,690
the less kind of i/o we can do on the on

00:04:55,390 --> 00:05:01,210
the memory bus the better the other

00:04:58,690 --> 00:05:03,490
classic solution to this would be like a

00:05:01,210 --> 00:05:05,740
dedicated meta store on a lot of big

00:05:03,490 --> 00:05:08,680
systems you'll see its companies like

00:05:05,740 --> 00:05:11,590
Facebook this could be a dedicated meta

00:05:08,680 --> 00:05:14,110
store these are pretty good and indeed

00:05:11,590 --> 00:05:18,520
our our store system actually does have

00:05:14,110 --> 00:05:21,040
a dedicated metadata store for our level

00:05:18,520 --> 00:05:23,500
of objects that we store but this does

00:05:21,040 --> 00:05:26,650
not go down to like the XFS file system

00:05:23,500 --> 00:05:28,050
level and these work pretty well they

00:05:26,650 --> 00:05:30,580
handle great heavy workloads pretty well

00:05:28,050 --> 00:05:32,410
the cons are there they're complex

00:05:30,580 --> 00:05:34,360
they're actually hard to get rights

00:05:32,410 --> 00:05:35,530
there's a lot of kind of landmines for

00:05:34,360 --> 00:05:37,870
your design these things to make sure

00:05:35,530 --> 00:05:39,730
that they scale correctly we've gone

00:05:37,870 --> 00:05:41,560
through probably a couple generations of

00:05:39,730 --> 00:05:44,080
our metadata layer to try and get it

00:05:41,560 --> 00:05:46,330
right and of course there are

00:05:44,080 --> 00:05:47,890
proprietary so you kind of don't benefit

00:05:46,330 --> 00:05:50,290
from the open source community is kind

00:05:47,890 --> 00:05:52,900
of collective wisdom and knowledge and

00:05:50,290 --> 00:05:55,530
all that kind of decades of storage

00:05:52,900 --> 00:06:01,510
engineering knowledge can be can be lost

00:05:55,530 --> 00:06:02,980
so with fiber FS it's really XFS with

00:06:01,510 --> 00:06:05,800
real-time sub volumes this is actually

00:06:02,980 --> 00:06:08,170
kind of a feature that I don't think is

00:06:05,800 --> 00:06:10,840
either well understood or even most

00:06:08,170 --> 00:06:14,220
people even know it exists

00:06:10,840 --> 00:06:17,860
and it's basically taking X FS and

00:06:14,220 --> 00:06:19,150
adding in a second block device so the

00:06:17,860 --> 00:06:21,060
standard block device everyone kind of

00:06:19,150 --> 00:06:23,320
knows and loves you've got a metadata

00:06:21,060 --> 00:06:24,520
portion of the filesystem you've got

00:06:23,320 --> 00:06:27,250
your journal and then you've got your

00:06:24,520 --> 00:06:29,050
data what real time mode does is it

00:06:27,250 --> 00:06:30,730
actually adds in this kind of second

00:06:29,050 --> 00:06:34,370
block device called the real time block

00:06:30,730 --> 00:06:37,100
device where you can also store data and

00:06:34,370 --> 00:06:38,840
how you can if you store data there as

00:06:37,100 --> 00:06:41,120
controlled by a real-time flag that you

00:06:38,840 --> 00:06:45,889
can either place on the file system the

00:06:41,120 --> 00:06:48,020
directory or an individual file the

00:06:45,889 --> 00:06:51,560
other thing to understand about kind of

00:06:48,020 --> 00:06:53,240
the real-time mode is it's actually got

00:06:51,560 --> 00:06:55,669
a separate allocator for that real-time

00:06:53,240 --> 00:07:02,000
device and I'll get into that in a

00:06:55,669 --> 00:07:04,430
second so how do we apply this we

00:07:02,000 --> 00:07:08,000
basically swap it out the standard block

00:07:04,430 --> 00:07:09,800
device we put an SSD there and for the

00:07:08,000 --> 00:07:11,419
real-time device we use a hard disk it's

00:07:09,800 --> 00:07:13,669
really that simple

00:07:11,419 --> 00:07:15,080
and then we layer on a few other things

00:07:13,669 --> 00:07:16,790
that we'll get into in a second that

00:07:15,080 --> 00:07:21,139
kind of creates the whole hybrid xs/s

00:07:16,790 --> 00:07:23,060
system so the real-time allocator I

00:07:21,139 --> 00:07:24,680
think it's worth kind of pausing for a

00:07:23,060 --> 00:07:28,070
moment and kind of kind of diving into

00:07:24,680 --> 00:07:30,789
how this allocator works a lot of

00:07:28,070 --> 00:07:33,440
systems tend to solve this problem using

00:07:30,789 --> 00:07:34,610
raw disk like just writing to the raw

00:07:33,440 --> 00:07:37,280
disk and then managing that data

00:07:34,610 --> 00:07:38,120
themselves and frankly I think a lot of

00:07:37,280 --> 00:07:39,650
them end up with something that actually

00:07:38,120 --> 00:07:41,210
looks quite a bit like this they take a

00:07:39,650 --> 00:07:44,720
drive they chop it up into a bunch of

00:07:41,210 --> 00:07:48,500
pieces which kind of look like extents

00:07:44,720 --> 00:07:50,599
from an XSS standpoint and they're of a

00:07:48,500 --> 00:07:52,580
fixed size and indeed that's really what

00:07:50,599 --> 00:07:54,289
the real-time bitmap block allocator

00:07:52,580 --> 00:07:55,550
actually does takes the drive you can

00:07:54,289 --> 00:07:57,889
pick whatever size you want to chop it

00:07:55,550 --> 00:07:59,750
up as that becomes your extents and when

00:07:57,889 --> 00:08:01,520
you write data it's basically going to

00:07:59,750 --> 00:08:05,120
slam that data into one of them were

00:08:01,520 --> 00:08:06,440
these extents and if you're writing

00:08:05,120 --> 00:08:08,330
something over the extend sides that's

00:08:06,440 --> 00:08:11,780
going to try just like a normal block

00:08:08,330 --> 00:08:13,820
allocator would it's find you a biggest

00:08:11,780 --> 00:08:16,430
piece that can that's contiguous so that

00:08:13,820 --> 00:08:18,880
kind of behavior is still there but in

00:08:16,430 --> 00:08:21,620
contrast to say the Ag alligator on XFS

00:08:18,880 --> 00:08:23,330
most of the AG alligator kind of brains

00:08:21,620 --> 00:08:26,000
that you might be used to or heuristics

00:08:23,330 --> 00:08:28,610
the AG alligator has do not exist in

00:08:26,000 --> 00:08:32,120
this this alligator so for example

00:08:28,610 --> 00:08:36,110
things like AG alligator has a certain

00:08:32,120 --> 00:08:38,659
ability to hold open a large extent of

00:08:36,110 --> 00:08:40,190
the file if you're streaming rights to

00:08:38,659 --> 00:08:42,919
it and then it will kind of close it if

00:08:40,190 --> 00:08:44,120
it sees that those rights cease and in

00:08:42,919 --> 00:08:46,819
doing so we can create like kind of

00:08:44,120 --> 00:08:49,249
large continuous

00:08:46,819 --> 00:08:50,749
files on the file system another

00:08:49,249 --> 00:08:52,370
behavior this thing would not have for

00:08:50,749 --> 00:08:55,279
example that exit the eg a locator does

00:08:52,370 --> 00:08:57,019
is also this notion that it can be glq

00:08:55,279 --> 00:08:58,759
kind of dynamically allocate you extents

00:08:57,019 --> 00:09:00,829
based on how much space is on the on the

00:08:58,759 --> 00:09:04,610
drive the real-time allocator you don't

00:09:00,829 --> 00:09:07,420
get any of that so in order to make this

00:09:04,610 --> 00:09:12,860
kind of all work the kind of real-time

00:09:07,420 --> 00:09:14,569
mode of X of s was not perfect things

00:09:12,860 --> 00:09:17,269
that we wanted to change included kind

00:09:14,569 --> 00:09:19,519
of these three aspects we wanted to

00:09:17,269 --> 00:09:20,959
change house TANF s worked when we

00:09:19,519 --> 00:09:23,180
started using this it would actually

00:09:20,959 --> 00:09:25,730
return the most space on the SSD not the

00:09:23,180 --> 00:09:27,290
hard-disk regardless of what Flags you

00:09:25,730 --> 00:09:28,879
had on the filesystem so if you're

00:09:27,290 --> 00:09:31,069
actually like defaulting the data to the

00:09:28,879 --> 00:09:33,379
hard disk it would still show you this

00:09:31,069 --> 00:09:34,100
data fest information for the SSD kind

00:09:33,379 --> 00:09:37,339
of non-intuitive

00:09:34,100 --> 00:09:38,720
we want to change that we also wanted to

00:09:37,339 --> 00:09:39,850
change that so like a lot of our tooling

00:09:38,720 --> 00:09:42,170
wouldn't break as well

00:09:39,850 --> 00:09:44,779
and the other thing we want to change is

00:09:42,170 --> 00:09:46,879
actually we kind of had our eye on small

00:09:44,779 --> 00:09:49,370
files from the beginning we wanted to

00:09:46,879 --> 00:09:51,620
make sure that small files could get

00:09:49,370 --> 00:09:53,329
routed to the SSD and doing that we kind

00:09:51,620 --> 00:09:55,579
of we created a couple patches to

00:09:53,329 --> 00:10:00,379
facilitate this the RT Alex size and RT

00:09:55,579 --> 00:10:01,490
fall back percentage patch and the idea

00:10:00,379 --> 00:10:02,809
behind these is it we just kind of

00:10:01,490 --> 00:10:04,939
automatically read small files to the

00:10:02,809 --> 00:10:08,629
SSD and fall and fall back to the hard

00:10:04,939 --> 00:10:11,629
disk if that failed now in collaborating

00:10:08,629 --> 00:10:13,129
with the upstream and and the accessor

00:10:11,629 --> 00:10:13,639
maintainers you don't always get what

00:10:13,129 --> 00:10:16,160
you want

00:10:13,639 --> 00:10:17,480
indeed that's was the case for us they

00:10:16,160 --> 00:10:19,009
kind of gave this we settled on a

00:10:17,480 --> 00:10:21,800
modified version of this patch which

00:10:19,009 --> 00:10:24,110
basically based on the presence of the

00:10:21,800 --> 00:10:25,279
RT inherit flag on the exodus file

00:10:24,110 --> 00:10:27,740
system it would actually give us the

00:10:25,279 --> 00:10:29,000
behavior we wanted so no flag it

00:10:27,740 --> 00:10:32,350
actually just kind of magically works

00:10:29,000 --> 00:10:35,149
the way you and you would expect it to

00:10:32,350 --> 00:10:37,160
so how do we actually deploy this at

00:10:35,149 --> 00:10:39,649
Facebook you'll actually see something

00:10:37,160 --> 00:10:43,279
like this we take an SSD we chop it up

00:10:39,649 --> 00:10:45,529
into however many pieces we need one for

00:10:43,279 --> 00:10:47,990
one piece or one partition for each

00:10:45,529 --> 00:10:49,779
drive we have on the system we then map

00:10:47,990 --> 00:10:52,519
those partitions over to our hard drives

00:10:49,779 --> 00:10:53,780
and on the SSD partitions we have our

00:10:52,519 --> 00:10:56,150
metadata

00:10:53,780 --> 00:10:59,390
our metadata our intent log and on our

00:10:56,150 --> 00:11:00,230
hard disks we have our data blocks what

00:10:59,390 --> 00:11:02,210
you'll notice here is we don't actually

00:11:00,230 --> 00:11:04,700
put data in our kind of Gen one version

00:11:02,210 --> 00:11:08,450
of this on the SSD yet I'll get into

00:11:04,700 --> 00:11:10,190
that in a second so how does this

00:11:08,450 --> 00:11:12,260
actually look like when you actually log

00:11:10,190 --> 00:11:14,210
into a system and and view a hybrid XML

00:11:12,260 --> 00:11:15,560
system when you look at the Mount you

00:11:14,210 --> 00:11:18,260
ought to see its mounting the SSD

00:11:15,560 --> 00:11:20,210
partition and you'll only see the hard

00:11:18,260 --> 00:11:22,550
drive show up in the mount parameters

00:11:20,210 --> 00:11:25,070
you'll see this like RT dev thing that's

00:11:22,550 --> 00:11:26,510
your hard disk and when you actually on

00:11:25,070 --> 00:11:27,770
the file system it actually looks and

00:11:26,510 --> 00:11:29,360
feels just like a normal access

00:11:27,770 --> 00:11:30,980
filesystem except when you write data to

00:11:29,360 --> 00:11:36,350
it it goes to the hard drive not the SSD

00:11:30,980 --> 00:11:37,760
and the metadata goes to the SSD and we

00:11:36,350 --> 00:11:38,960
can kind of after we did this we of

00:11:37,760 --> 00:11:40,850
course want to like go back to block

00:11:38,960 --> 00:11:43,490
trace verify we kind of get the expected

00:11:40,850 --> 00:11:45,620
behavior we want and indeed we do on the

00:11:43,490 --> 00:11:47,150
hard drive there's zero metadata

00:11:45,620 --> 00:11:48,800
operations it's all just purely

00:11:47,150 --> 00:11:54,020
beautiful data writes going to that

00:11:48,800 --> 00:11:56,000
device so some of you might also be kind

00:11:54,020 --> 00:11:57,410
of thinking right now like hey you have

00:11:56,000 --> 00:11:59,240
like all these drives hooked up to the

00:11:57,410 --> 00:12:01,550
SSD this sounds like a horrible terrible

00:11:59,240 --> 00:12:04,670
scary idea what happens left that's that

00:12:01,550 --> 00:12:10,339
SSD fails you lose all your drives right

00:12:04,670 --> 00:12:12,800
so we're thinking this too and we we

00:12:10,339 --> 00:12:13,880
wanted a contingency plan should this

00:12:12,800 --> 00:12:15,940
happen

00:12:13,880 --> 00:12:18,170
for example maybe we get a big batch of

00:12:15,940 --> 00:12:19,580
SSD is from a vendor and they have a

00:12:18,170 --> 00:12:20,810
firmware bug and we start seeing these

00:12:19,580 --> 00:12:23,750
things drop like flies we obviously

00:12:20,810 --> 00:12:25,400
don't want to lose all their data so we

00:12:23,750 --> 00:12:27,770
thought of that and introduced a

00:12:25,400 --> 00:12:29,060
metadata rescue partition on every

00:12:27,770 --> 00:12:31,310
single one of our drives so if you look

00:12:29,060 --> 00:12:33,460
at our hard disks we actually have this

00:12:31,310 --> 00:12:37,010
extra section which is size

00:12:33,460 --> 00:12:38,480
proportionate to the SSD partition in

00:12:37,010 --> 00:12:39,710
the event we see something like that or

00:12:38,480 --> 00:12:41,180
we just want to do maintenance we can

00:12:39,710 --> 00:12:43,280
simply drain Elda meditate over to the

00:12:41,180 --> 00:12:45,380
hard disk remount the filesystem and

00:12:43,280 --> 00:12:47,750
we're back to kind of a hard drive only

00:12:45,380 --> 00:12:49,970
mode we've lost some of our i/o but our

00:12:47,750 --> 00:12:52,670
data is safe and we can then swap out

00:12:49,970 --> 00:12:58,480
the SSD or just kind of operate into

00:12:52,670 --> 00:13:01,100
degraded mode if necessary all right so

00:12:58,480 --> 00:13:04,970
that's basically the nuts and bolts of

00:13:01,100 --> 00:13:06,630
hybrid XFS so we didn't actually jump

00:13:04,970 --> 00:13:09,000
into the

00:13:06,630 --> 00:13:12,660
all then we want to actually do a

00:13:09,000 --> 00:13:15,360
proof-of-concept first part of doing

00:13:12,660 --> 00:13:16,650
engineering at Facebook is you basically

00:13:15,360 --> 00:13:17,970
have to convince your colleagues that

00:13:16,650 --> 00:13:20,700
hey this idea you want to do is actually

00:13:17,970 --> 00:13:24,930
a good idea in order to do that we did a

00:13:20,700 --> 00:13:28,080
proof of concept so what metrics did we

00:13:24,930 --> 00:13:29,850
look at so again top of mind was like

00:13:28,080 --> 00:13:32,220
hey we understood the risk from the

00:13:29,850 --> 00:13:34,470
outset of this that hooking up a bunch

00:13:32,220 --> 00:13:36,750
of drives into a single device could be

00:13:34,470 --> 00:13:38,130
risky but we did the math and we figured

00:13:36,750 --> 00:13:41,040
that if the fr stayed within the

00:13:38,130 --> 00:13:43,800
manufacturer specs we should be alright

00:13:41,040 --> 00:13:45,450
this basically the probability of an SSD

00:13:43,800 --> 00:13:47,760
failing along with all your hard drives

00:13:45,450 --> 00:13:49,470
feeling you can basically model that as

00:13:47,760 --> 00:13:50,700
simply adding that probability onto all

00:13:49,470 --> 00:13:52,170
the failure rates of your hard drives so

00:13:50,700 --> 00:13:54,330
if your hard drive has a failure rate of

00:13:52,170 --> 00:13:57,840
say 2% your SSD has a failure rate of

00:13:54,330 --> 00:13:59,250
0.4% your new failure rate is 2.4

00:13:57,840 --> 00:14:01,880
percent and you can kind of model that

00:13:59,250 --> 00:14:04,530
and figure out if you're going to be ok

00:14:01,880 --> 00:14:06,360
so the second thing we wanted to do is

00:14:04,530 --> 00:14:08,460
make sure that our the endurance of

00:14:06,360 --> 00:14:11,070
these drives would actually outlast the

00:14:08,460 --> 00:14:13,320
hardware cycle typically we try and keep

00:14:11,070 --> 00:14:15,390
our hardware around for around 4 or 5

00:14:13,320 --> 00:14:18,000
years something along that along those

00:14:15,390 --> 00:14:20,250
lines for storage and the drive rights

00:14:18,000 --> 00:14:21,600
per day needs to actually be below a

00:14:20,250 --> 00:14:25,440
certain threshold in order to actually

00:14:21,600 --> 00:14:27,450
maintain that second thing are the third

00:14:25,440 --> 00:14:29,300
thing is disk utilization kind of a

00:14:27,450 --> 00:14:31,950
whole point of this is actually save IO

00:14:29,300 --> 00:14:34,760
so if you don't actually see much of a

00:14:31,950 --> 00:14:37,260
savings this isn't really worth doing

00:14:34,760 --> 00:14:38,840
and for this project for a project this

00:14:37,260 --> 00:14:41,400
caliber where we're kind of like

00:14:38,840 --> 00:14:44,040
potentially changing Hardware if it's

00:14:41,400 --> 00:14:45,870
not 10% or better we're probably not

00:14:44,040 --> 00:14:48,720
gonna do it and even at 10% we'd think

00:14:45,870 --> 00:14:50,730
long and hard before we did it the

00:14:48,720 --> 00:14:52,140
reasoning there is like we're gonna get

00:14:50,730 --> 00:14:55,050
things wrong and generally things may

00:14:52,140 --> 00:14:57,210
seem rosier in a POC than they actually

00:14:55,050 --> 00:14:58,590
are in full-blown production so we want

00:14:57,210 --> 00:15:02,220
a bit of a kind of engineering margin of

00:14:58,590 --> 00:15:04,470
error an application latency we don't

00:15:02,220 --> 00:15:06,240
want regressions there our application

00:15:04,470 --> 00:15:07,490
should still be able to operate just as

00:15:06,240 --> 00:15:10,080
I did before

00:15:07,490 --> 00:15:12,930
so in our PFC this this is the actual

00:15:10,080 --> 00:15:14,260
data we saw in this case we were

00:15:12,930 --> 00:15:15,370
actually

00:15:14,260 --> 00:15:16,750
we've got a little lucky here we

00:15:15,370 --> 00:15:18,520
actually had a we did this with a boot

00:15:16,750 --> 00:15:23,200
class drive that happened to have a

00:15:18,520 --> 00:15:26,230
portion of SLC on this disc and that

00:15:23,200 --> 00:15:27,760
actually reduced this a little bit than

00:15:26,230 --> 00:15:29,230
it otherwise would be on just a standard

00:15:27,760 --> 00:15:30,940
boot class drive now in our full-blown

00:15:29,230 --> 00:15:33,700
production we have to use enterprise

00:15:30,940 --> 00:15:38,610
drives which have a much higher

00:15:33,700 --> 00:15:42,580
endurance so our concerns there weren't

00:15:38,610 --> 00:15:44,410
weren't quite the same as on our PLC so

00:15:42,580 --> 00:15:47,230
the i/o rates this is a bit hard to read

00:15:44,410 --> 00:15:49,390
we use a tool called FB outrace which is

00:15:47,230 --> 00:15:52,690
basically able to kind of look at the

00:15:49,390 --> 00:15:55,990
iOS going into our hard drives and look

00:15:52,690 --> 00:15:57,010
for sequentiality as well as ran - what

00:15:55,990 --> 00:15:59,230
you're seeing here is basically just a

00:15:57,010 --> 00:16:01,120
layered chart of coalesced reads

00:15:59,230 --> 00:16:03,610
coalesced writes as well as random reads

00:16:01,120 --> 00:16:05,380
and random writes the random writes is

00:16:03,610 --> 00:16:07,750
really what to keep your eye on that

00:16:05,380 --> 00:16:09,370
those are the metadata rights which are

00:16:07,750 --> 00:16:11,740
dominating in that graph so what we

00:16:09,370 --> 00:16:14,860
would have expected to see is those

00:16:11,740 --> 00:16:18,900
random writes to actually be reduced and

00:16:14,860 --> 00:16:21,520
indeed in our in our POC we did see that

00:16:18,900 --> 00:16:23,290
also a thing to note here is you'll see

00:16:21,520 --> 00:16:27,100
kind of us talking about kind of a

00:16:23,290 --> 00:16:29,980
control a control batch of machines

00:16:27,100 --> 00:16:31,570
we're never doing a change like this we

00:16:29,980 --> 00:16:33,580
always kind of do it kind of in a kind

00:16:31,570 --> 00:16:34,960
of a scientific way we have our test

00:16:33,580 --> 00:16:36,430
group we have a control group where we

00:16:34,960 --> 00:16:39,310
do nothing - and we really want to see

00:16:36,430 --> 00:16:40,660
kind of air between the two lines we're

00:16:39,310 --> 00:16:42,820
not actually just looking from like a

00:16:40,660 --> 00:16:46,270
day to day change we're looking we want

00:16:42,820 --> 00:16:47,950
to see like change and you know the same

00:16:46,270 --> 00:16:52,780
moment in time between two groups of

00:16:47,950 --> 00:16:54,880
systems so here we've got our hybrid XFS

00:16:52,780 --> 00:16:57,040
and our control now we're looking at

00:16:54,880 --> 00:17:00,310
application latency this is probably not

00:16:57,040 --> 00:17:01,210
quite surprising it's nice of a

00:17:00,310 --> 00:17:02,080
regression but it's kind of not

00:17:01,210 --> 00:17:04,690
surprising you'd actually see a

00:17:02,080 --> 00:17:06,490
performance improvement here in this

00:17:04,690 --> 00:17:09,130
case we're seeing about a something like

00:17:06,490 --> 00:17:11,230
a 15 to 25 percent reduction because

00:17:09,130 --> 00:17:12,670
those metadata IOT's the applications no

00:17:11,230 --> 00:17:18,490
longer have any kind of hang around and

00:17:12,670 --> 00:17:19,780
wait for them all right so now onto like

00:17:18,490 --> 00:17:20,740
kind of a full rollout so now we have

00:17:19,780 --> 00:17:23,560
this problem we're kind of pretty

00:17:20,740 --> 00:17:25,180
convinced this is going to work but

00:17:23,560 --> 00:17:27,090
we've got like thousands and thousands

00:17:25,180 --> 00:17:32,550
of machines that we have to do

00:17:27,090 --> 00:17:34,200
convert to hybrid EXO fest so this is

00:17:32,550 --> 00:17:36,990
really where we kind of put our

00:17:34,200 --> 00:17:38,930
production engineering hats on and we

00:17:36,990 --> 00:17:41,070
figure out how we're gonna like automate

00:17:38,930 --> 00:17:42,540
automate this change so we have these

00:17:41,070 --> 00:17:44,130
stored systems are in place they're

00:17:42,540 --> 00:17:47,130
actually in production we cannot take

00:17:44,130 --> 00:17:50,610
them out of production and we've got to

00:17:47,130 --> 00:17:52,230
drain them we've got to drain them

00:17:50,610 --> 00:17:54,390
recreate the file system in this kind of

00:17:52,230 --> 00:17:55,830
hyper hydaburg primary brew to XFS mode

00:17:54,390 --> 00:17:57,480
and then undrained them to put the data

00:17:55,830 --> 00:18:00,840
back on and load that machine back into

00:17:57,480 --> 00:18:03,870
our production cluster so we have two

00:18:00,840 --> 00:18:06,480
levels of automation that we can use the

00:18:03,870 --> 00:18:07,860
first one is a system called F bar you

00:18:06,480 --> 00:18:10,380
can think of this as just a really

00:18:07,860 --> 00:18:13,080
simple codified alarm remediation alarm

00:18:10,380 --> 00:18:15,600
is raised in our infrastructure we then

00:18:13,080 --> 00:18:20,220
cut some Python code to go remediate

00:18:15,600 --> 00:18:22,950
that alarm really simple really quick to

00:18:20,220 --> 00:18:24,980
make the second piece that we have is

00:18:22,950 --> 00:18:27,090
something called an F bar job engine

00:18:24,980 --> 00:18:29,070
this actually allows you could create

00:18:27,090 --> 00:18:31,260
kind of these state machine based

00:18:29,070 --> 00:18:34,230
automation flows a little bit more

00:18:31,260 --> 00:18:37,800
complex to do our conversion we actually

00:18:34,230 --> 00:18:40,110
use both and the way we did it is we

00:18:37,800 --> 00:18:41,670
used to have an f bar flow first we're

00:18:40,110 --> 00:18:43,830
an alarm is raised which basically that

00:18:41,670 --> 00:18:46,170
machine is requesting conversion we're

00:18:43,830 --> 00:18:47,640
able to rate limit that we actually have

00:18:46,170 --> 00:18:49,710
beyond rate limiting we actually had

00:18:47,640 --> 00:18:52,680
basically a pool of hosts that we

00:18:49,710 --> 00:18:54,090
defined to which an alarm could only be

00:18:52,680 --> 00:18:55,440
raised if you were in this pool and then

00:18:54,090 --> 00:18:59,120
it was rate limited on top of that just

00:18:55,440 --> 00:19:01,860
to be super safe once a host was deemed

00:18:59,120 --> 00:19:06,150
safe to convert we then send it to our

00:19:01,860 --> 00:19:08,060
FBG flow where the actual conversion

00:19:06,150 --> 00:19:12,210
happens in this kind of state machine

00:19:08,060 --> 00:19:13,590
system we don't have a whole bunch of

00:19:12,210 --> 00:19:15,960
time today so I'm only going to go

00:19:13,590 --> 00:19:17,070
through the f bar flow to kind of give

00:19:15,960 --> 00:19:21,450
you a taste of kind of how this

00:19:17,070 --> 00:19:25,380
automation works so first we raise our

00:19:21,450 --> 00:19:27,030
alarm we double check to make sure it's

00:19:25,380 --> 00:19:28,140
actually staged for conversion here

00:19:27,030 --> 00:19:30,450
we're actually kind of looking for bugs

00:19:28,140 --> 00:19:32,070
we want to make sure that this alarms

00:19:30,450 --> 00:19:34,590
somehow didn't get raised

00:19:32,070 --> 00:19:37,470
even though machine is not targeted for

00:19:34,590 --> 00:19:38,790
conversion if it is we escalate this to

00:19:37,470 --> 00:19:39,810
a human and say hey something's gone

00:19:38,790 --> 00:19:43,200
wrong here

00:19:39,810 --> 00:19:45,000
someone might need to check on this if

00:19:43,200 --> 00:19:48,630
it is actually in this pool of machines

00:19:45,000 --> 00:19:49,860
slated to be converted we then check to

00:19:48,630 --> 00:19:51,630
see hey maybe this thing has already

00:19:49,860 --> 00:19:57,150
been converted if so we're moving from

00:19:51,630 --> 00:19:58,950
the staging tier and lastly we will run

00:19:57,150 --> 00:20:00,120
a bunch of safety checks we want to

00:19:58,950 --> 00:20:02,970
check for things like hey is there

00:20:00,120 --> 00:20:06,240
actually data on the SSD is there an old

00:20:02,970 --> 00:20:07,740
hybrid XFS set up on this SSD we expect

00:20:06,240 --> 00:20:11,880
before a conversion these things to be

00:20:07,740 --> 00:20:13,140
like perfectly clean if we see any data

00:20:11,880 --> 00:20:16,770
on these things we're gonna stop the

00:20:13,140 --> 00:20:19,320
process and kick it out assuming the SSD

00:20:16,770 --> 00:20:22,890
is clean the other safety check

00:20:19,320 --> 00:20:24,150
correction you check for is actually the

00:20:22,890 --> 00:20:26,450
drain state of the machine as well we

00:20:24,150 --> 00:20:28,980
want to make sure that it's been drained

00:20:26,450 --> 00:20:31,440
if that all checks out we actually send

00:20:28,980 --> 00:20:33,780
it off to our conversion flow we then

00:20:31,440 --> 00:20:35,970
kind of wait an hour and go back to this

00:20:33,780 --> 00:20:37,740
loop in that case it's actually gonna

00:20:35,970 --> 00:20:39,180
kick out at the kind of the second

00:20:37,740 --> 00:20:40,620
triangle there as it's going to show up

00:20:39,180 --> 00:20:47,130
it's already completed we then clear the

00:20:40,620 --> 00:20:50,370
alarm we're good to go all right so how

00:20:47,130 --> 00:20:51,870
did this roll out actually look this is

00:20:50,370 --> 00:20:54,270
basically looks this is basically our

00:20:51,870 --> 00:20:55,440
percent complete we actually probably

00:20:54,270 --> 00:20:56,790
would have got this conversion done a

00:20:55,440 --> 00:20:59,760
little bit quicker you can see at the

00:20:56,790 --> 00:21:01,680
beginning we had a very quick ramp to

00:20:59,760 --> 00:21:04,110
get probably close to 50% machines

00:21:01,680 --> 00:21:05,010
converted and maybe only two months we

00:21:04,110 --> 00:21:07,950
actually ran to a bunch of drain

00:21:05,010 --> 00:21:09,150
problems unrelated to hybrid XFS so we

00:21:07,950 --> 00:21:10,650
have to work through those and that kind

00:21:09,150 --> 00:21:12,210
of slowed down our conversion process so

00:21:10,650 --> 00:21:15,960
it kind of shows you the power of this

00:21:12,210 --> 00:21:17,160
automation we would probably think that

00:21:15,960 --> 00:21:20,820
we could have done this in six months

00:21:17,160 --> 00:21:24,630
probably if the those drain issues

00:21:20,820 --> 00:21:27,900
weren't there all right so not

00:21:24,630 --> 00:21:29,790
everything goes perfect in life we we

00:21:27,900 --> 00:21:32,610
missed things we learn things from this

00:21:29,790 --> 00:21:34,350
the real and some of them like

00:21:32,610 --> 00:21:35,490
fragmentation we kind of suspected but

00:21:34,350 --> 00:21:37,350
it's fragmentation is one of those

00:21:35,490 --> 00:21:38,490
things that you have to cut up wait do

00:21:37,350 --> 00:21:41,190
you have to wait quite a long time to

00:21:38,490 --> 00:21:43,920
actually see if it's gonna be a problem

00:21:41,190 --> 00:21:46,410
or not aging filesystems is kind of

00:21:43,920 --> 00:21:48,240
notoriously difficult but based on our

00:21:46,410 --> 00:21:49,770
kind of theoretical knowledge of the way

00:21:48,240 --> 00:21:52,230
the real-time allocator works this was a

00:21:49,770 --> 00:21:55,080
possibility

00:21:52,230 --> 00:21:56,490
and we did indeed see some fragmentation

00:21:55,080 --> 00:21:58,409
starting to take hold it was still

00:21:56,490 --> 00:22:01,470
better than what we were doing without

00:21:58,409 --> 00:22:04,169
hybrid XFS but you know our performance

00:22:01,470 --> 00:22:05,639
wasn't as great as when we started so

00:22:04,169 --> 00:22:07,500
this actually pushed us to move to

00:22:05,639 --> 00:22:10,139
actually a larger extent size we

00:22:07,500 --> 00:22:12,269
originally started with 256 K we then

00:22:10,139 --> 00:22:14,429
went to 1 Meg now 1 Meg has its own

00:22:12,269 --> 00:22:17,309
problems which is store 4k file you're

00:22:14,429 --> 00:22:18,750
going to burn one Mik so this now you're

00:22:17,309 --> 00:22:20,340
moving up you're moving your trading

00:22:18,750 --> 00:22:22,169
this fragmentation problem or i/o

00:22:20,340 --> 00:22:25,080
efficiency now for storage and

00:22:22,169 --> 00:22:26,850
efficiency indeed this is actually ends

00:22:25,080 --> 00:22:28,470
up being actually an OK trade even if

00:22:26,850 --> 00:22:30,059
you couldn't fix this because what we

00:22:28,470 --> 00:22:32,580
observe is something like a 30 percent

00:22:30,059 --> 00:22:34,260
uplift and i/o efficiency for Navy say a

00:22:32,580 --> 00:22:36,600
10 percent trade-off in storage

00:22:34,260 --> 00:22:39,480
efficiency so you're still kind of

00:22:36,600 --> 00:22:41,789
netting out 20 percent there but being

00:22:39,480 --> 00:22:43,289
engineers we like kind of perfection we

00:22:41,789 --> 00:22:49,289
want to see if how good we can make this

00:22:43,289 --> 00:22:50,760
thing oh I guess yeah I think yeah so we

00:22:49,289 --> 00:22:51,720
went so now we're kind of our future

00:22:50,760 --> 00:22:53,600
direction is actually moving into a

00:22:51,720 --> 00:22:56,039
system that looks a little bit like this

00:22:53,600 --> 00:23:01,320
we actually are going to start writing

00:22:56,039 --> 00:23:04,110
to data onto the SSDs I have actually a

00:23:01,320 --> 00:23:06,510
really smart intern working on this and

00:23:04,110 --> 00:23:08,850
we're gonna be getting testing this in

00:23:06,510 --> 00:23:11,549
the next month and the idea here is

00:23:08,850 --> 00:23:15,389
watch they take small files and redirect

00:23:11,549 --> 00:23:17,279
them to the SSD effectively before we

00:23:15,389 --> 00:23:19,049
write them we're gonna set a flag based

00:23:17,279 --> 00:23:21,720
on the size redirect up to the SSD

00:23:19,049 --> 00:23:24,990
leaving any files say larger than 64 ok

00:23:21,720 --> 00:23:26,340
on the hard disk 64 K is kind of a guess

00:23:24,990 --> 00:23:28,230
right now we actually do have to do a

00:23:26,340 --> 00:23:29,370
bit of an analysis to see to bounce out

00:23:28,230 --> 00:23:31,889
how much space we're gonna use on the

00:23:29,370 --> 00:23:34,470
SSD what impact that will have on the

00:23:31,889 --> 00:23:36,389
endurance versus kind of a space savings

00:23:34,470 --> 00:23:39,990
I think if we can get something to like

00:23:36,389 --> 00:23:45,409
5% of storage overhead we'll be in a

00:23:39,990 --> 00:23:49,070
pretty good place all right

00:23:45,409 --> 00:23:52,070
and a little bit early so any questions

00:23:49,070 --> 00:23:52,070
yes

00:23:53,620 --> 00:24:00,020
that's a good question we actually got

00:23:55,520 --> 00:24:02,810
lucky and our in our POC we had SSDs in

00:24:00,020 --> 00:24:04,460
our Sweta the store stack that we were

00:24:02,810 --> 00:24:07,040
using for the data warehouse those

00:24:04,460 --> 00:24:09,770
actually had SSDs in them for us totally

00:24:07,040 --> 00:24:10,430
different use case and there we got a

00:24:09,770 --> 00:24:13,280
little bit lucky

00:24:10,430 --> 00:24:14,930
so that bad side is as these SSDs were

00:24:13,280 --> 00:24:17,000
actually boot class there was a little

00:24:14,930 --> 00:24:18,770
kind of a lot of nervousness around like

00:24:17,000 --> 00:24:23,090
hey could you actually use boot class

00:24:18,770 --> 00:24:24,560
drives for a purpose like this we

00:24:23,090 --> 00:24:26,450
eventually moved away from those boot

00:24:24,560 --> 00:24:27,860
class drives as our hardware skew

00:24:26,450 --> 00:24:29,840
changed we kind of had the opportunity

00:24:27,860 --> 00:24:32,210
to make the change SSDs or actually

00:24:29,840 --> 00:24:33,500
flash was actually dropping in price so

00:24:32,210 --> 00:24:34,910
we actually and wanted that extra

00:24:33,500 --> 00:24:36,110
insurance but we've actually found with

00:24:34,910 --> 00:24:38,150
those weak laughs drives we've actually

00:24:36,110 --> 00:24:40,660
been just we've been okay so it's not

00:24:38,150 --> 00:24:40,660
gonna problem

00:24:45,330 --> 00:24:49,950
so the exact code we run our production

00:24:47,999 --> 00:24:52,289
is upstream so that stata fest behavior

00:24:49,950 --> 00:24:54,960
that patch that is upstream so if you

00:24:52,289 --> 00:24:57,119
were to like format your file system use

00:24:54,960 --> 00:24:58,679
the RT inherit during the format you'll

00:24:57,119 --> 00:25:01,559
get that behavior that we get which is

00:24:58,679 --> 00:25:13,320
the Stata Festival return the space free

00:25:01,559 --> 00:25:15,509
on the hard disk not the SSD so we don't

00:25:13,320 --> 00:25:18,359
have plans of using single media in this

00:25:15,509 --> 00:25:20,669
solution and I think right now they're

00:25:18,359 --> 00:25:23,039
not really something on a radar we just

00:25:20,669 --> 00:25:25,559
have too much like random writes and

00:25:23,039 --> 00:25:28,710
going on in our systems to kind of make

00:25:25,559 --> 00:25:30,149
that workable so depending you know we

00:25:28,710 --> 00:25:31,409
may be forced our hand may be forced

00:25:30,149 --> 00:25:34,289
eventually and I'll if we start getting

00:25:31,409 --> 00:25:37,169
like 40 T drives that may be something

00:25:34,289 --> 00:25:39,269
we have to look at but so far I kind of

00:25:37,169 --> 00:25:42,119
we try and look out say like two years

00:25:39,269 --> 00:25:44,549
maybe three years from an actual like

00:25:42,119 --> 00:25:46,169
stuff will working on today will be for

00:25:44,549 --> 00:25:47,639
that time horizon and then we're kind of

00:25:46,169 --> 00:25:50,039
keeping our eye on maybe five or six

00:25:47,639 --> 00:25:51,809
years out but the vendor has kind of

00:25:50,039 --> 00:25:54,179
changed our roadmaps so frequently it's

00:25:51,809 --> 00:25:56,669
really hard to actually work on

00:25:54,179 --> 00:25:59,720
something and then have the vendors kind

00:25:56,669 --> 00:25:59,720
of change their minds

00:26:06,720 --> 00:26:12,960
yeah so actually we are there's actually

00:26:09,540 --> 00:26:14,550
a bunch of Engineers working I pretty

00:26:12,960 --> 00:26:16,080
sure it's open source called a system

00:26:14,550 --> 00:26:18,660
called cache Lib it's basically a

00:26:16,080 --> 00:26:20,520
library as that's designed for caching

00:26:18,660 --> 00:26:22,170
and that's actually being integrated

00:26:20,520 --> 00:26:23,520
into our storage system and that was

00:26:22,170 --> 00:26:25,110
that's the other purpose we're gonna use

00:26:23,520 --> 00:26:27,630
those SSDs for as well now that we've

00:26:25,110 --> 00:26:29,010
got them we actually look at them in

00:26:27,630 --> 00:26:30,510
kind of three different modes as we

00:26:29,010 --> 00:26:31,920
could use them for metadata we could use

00:26:30,510 --> 00:26:35,400
them to store small files or we can use

00:26:31,920 --> 00:26:38,150
them to cache data and the answer is we

00:26:35,400 --> 00:26:41,150
actually wanna use them for all three

00:26:38,150 --> 00:26:41,150
yes

00:26:49,480 --> 00:26:55,150
we use one SSD for all of the hard disks

00:26:56,230 --> 00:27:01,070
so our systems have 36 hard disks on our

00:26:59,540 --> 00:27:03,080
what we call our Bryce Canyon machines

00:27:01,070 --> 00:27:05,570
and then we'll put a one terabyte SSD

00:27:03,080 --> 00:27:07,130
attach to that so it's actually a very

00:27:05,570 --> 00:27:10,490
small ratio and a lot of systems will

00:27:07,130 --> 00:27:19,850
have like - even as high as 5% flash for

00:27:10,490 --> 00:27:21,980
backing their storage yeah um they are I

00:27:19,850 --> 00:27:25,360
actually have kind of like a side bet

00:27:21,980 --> 00:27:28,280
with my colleagues that like you know my

00:27:25,360 --> 00:27:30,200
my hunch is that like eventually the

00:27:28,280 --> 00:27:31,760
hard drives actually maybe get more

00:27:30,200 --> 00:27:33,590
expensive but what you may end up paying

00:27:31,760 --> 00:27:35,900
more for is actually unlimited endurance

00:27:33,590 --> 00:27:38,000
which is something that the flash will

00:27:35,900 --> 00:27:39,580
still struggle with so I don't know

00:27:38,000 --> 00:27:41,840
legate's it's gonna be interesting to

00:27:39,580 --> 00:27:47,390
see the next 10 years to see how things

00:27:41,840 --> 00:27:48,710
play out yeah even with SOC I think we'd

00:27:47,390 --> 00:27:50,660
probably still have endurance problems

00:27:48,710 --> 00:27:52,580
so the flash industry are still have to

00:27:50,660 --> 00:27:55,000
kind of solve that we write a lot to our

00:27:52,580 --> 00:27:55,000
systems

00:28:08,170 --> 00:28:11,580
sorry can't repeat that one more time

00:28:23,790 --> 00:28:27,450
yeah so yeah this is the question like

00:28:25,890 --> 00:28:35,580
are the SSDs kind of bottlenecking the

00:28:27,450 --> 00:28:37,920
hard disk throughput so I'd say the the

00:28:35,580 --> 00:28:39,750
ratio is largely driven by flash costs

00:28:37,920 --> 00:28:42,840
we do look to where we are looking for

00:28:39,750 --> 00:28:45,300
things like bottlenecking generally the

00:28:42,840 --> 00:28:47,340
the I operate on that flash is actually

00:28:45,300 --> 00:28:48,420
still pretty low such that that's not an

00:28:47,340 --> 00:28:50,250
issue

00:28:48,420 --> 00:28:52,500
endurance is probably the thing that

00:28:50,250 --> 00:28:54,060
would be kind of the thing that will hit

00:28:52,500 --> 00:28:55,310
first so we probably keep a closer eye

00:28:54,060 --> 00:28:57,360
on the endurance than anything else

00:28:55,310 --> 00:29:00,720
especially when we start using things

00:28:57,360 --> 00:29:03,330
like using it for small file caching or

00:29:00,720 --> 00:29:04,470
cache like in those cases if we don't

00:29:03,330 --> 00:29:07,050
watch it really closely we could burn

00:29:04,470 --> 00:29:11,130
out we could burn out the SSD pretty

00:29:07,050 --> 00:29:12,540
easily on the hard drive rate actually

00:29:11,130 --> 00:29:14,220
the thing there and we find is actually

00:29:12,540 --> 00:29:18,120
the more the thing to really make sure

00:29:14,220 --> 00:29:22,590
besides well it's really CPU the NIC and

00:29:18,120 --> 00:29:29,790
the in the having enough PCI bandwidth

00:29:22,590 --> 00:29:32,370
is really let me look further cool all

00:29:29,790 --> 00:29:33,390
right thanks a lot if you have any

00:29:32,370 --> 00:29:35,100
questions you can come track me down

00:29:33,390 --> 00:29:37,100
later and all you have to do to answer

00:29:35,100 --> 00:29:40,250
them thanks a lot

00:29:37,100 --> 00:29:40,250

YouTube URL: https://www.youtube.com/watch?v=41G_kFCC_KA


