Title: Lightning Talk: RDMA Programming Design and Case Studies - For Better Performance Distributed
Publication date: 2018-08-29
Playlist: Open Source Summit NA 2018 - Vancouver
Description: 
	Lightning Talk: RDMA Programming Design and Case Studies - For Better Performance Distributed Applications - Yoshiro Yamabe, NTT

Remote Direct Memory Access (RDMA) is a network protocol that offers low latency, high bandwidth, and low CPU utilization. So recently, RDMA has been expected to improve performance of applications which need frequent network communications, and there are several attempts applying RDMA to OSS. For instance, Tensorflow (deep learning framework) and Apache Spark (data analytics platform) have RDMA implementations.

However, RDMA programming offers many decisions for programmers which may affect performance (e.g. RDMA operation, application's data layout on memory region). Depending on them, extra overheads such as memcpy() will occur and reduce RDMA's benefits. To make matters more complex, they rely on target application's features.

This presentation shows overviews of RDMA, choices of design, and example of applying RDMA to MXNet, an OSS deep learning framework, by using Infiniband and its native API, ibverbs.

About Yoshiro Yamabe
Yoshiro Yamabe is a software engineer at NTT Laboratories. He has been investigating RDMA and tries to apply RDMA to OSS (MXNet).
Captions: 
	00:00:00,000 --> 00:00:05,490
hi I'm mr. ooyama babe I'm working at

00:00:03,149 --> 00:00:08,519
entity software Innovation Center and

00:00:05,490 --> 00:00:10,650
now I'm investigating about remote

00:00:08,519 --> 00:00:13,200
direct memory access our DMA

00:00:10,650 --> 00:00:16,170
technologies today I will introduce

00:00:13,200 --> 00:00:18,720
about our demands overview and case

00:00:16,170 --> 00:00:24,990
studies and several implementation

00:00:18,720 --> 00:00:27,869
techniques and my conclusion sorry at

00:00:24,990 --> 00:00:31,710
was i'll introduce about argument

00:00:27,869 --> 00:00:34,440
features our d-mail is low ratings and

00:00:31,710 --> 00:00:38,129
recipie overhead but hard to implement

00:00:34,440 --> 00:00:40,530
technologies for example in simple

00:00:38,129 --> 00:00:43,649
pimple micro benchmark arguments

00:00:40,530 --> 00:00:47,460
radiancy is about 1/5 of IPO virus

00:00:43,649 --> 00:00:50,100
radiation however argument program

00:00:47,460 --> 00:00:53,579
requires programmer to implement and

00:00:50,100 --> 00:00:56,760
understand several rubric mechanisms so

00:00:53,579 --> 00:01:01,160
in previous benchmark elements program

00:00:56,760 --> 00:01:04,820
is about 1600 ryan's a 600 rains but

00:01:01,160 --> 00:01:09,960
tcp/ip program is only about 300 rains

00:01:04,820 --> 00:01:13,439
about half but arguments minute is very

00:01:09,960 --> 00:01:16,650
up 30 other octaves so I try to make use

00:01:13,439 --> 00:01:19,170
of Alan potential I apply our limit a

00:01:16,650 --> 00:01:22,680
Mac sent an open source distributed deep

00:01:19,170 --> 00:01:25,229
learning framework a magnet adopts

00:01:22,680 --> 00:01:27,810
parameters of architecture and primate

00:01:25,229 --> 00:01:30,960
this figure shows parameter acts server

00:01:27,810 --> 00:01:33,409
architectures processing rule a trust

00:01:30,960 --> 00:01:36,240
each worker calculate parameter on

00:01:33,409 --> 00:01:38,880
seconds push parameter to parameters of

00:01:36,240 --> 00:01:41,880
a node and parameters are aggregated zip

00:01:38,880 --> 00:01:45,060
unrest each work output parameter from

00:01:41,880 --> 00:01:47,280
parameters over there are two times of

00:01:45,060 --> 00:01:51,229
the communication for each batch so I

00:01:47,280 --> 00:01:55,140
think army is effective for this model

00:01:51,229 --> 00:01:59,340
by applying Rd male the data row is

00:01:55,140 --> 00:02:01,950
changes as this figure in existing

00:01:59,340 --> 00:02:05,850
implementation there are four memory

00:02:01,950 --> 00:02:08,250
copies for each push or pull user to use

00:02:05,850 --> 00:02:11,000
a memory to user to use a memory copy on

00:02:08,250 --> 00:02:12,330
the two times of Conner to use a copy

00:02:11,000 --> 00:02:15,300
but

00:02:12,330 --> 00:02:18,000
in argument implementation the user to

00:02:15,300 --> 00:02:20,220
connemara Copic could be reduced so

00:02:18,000 --> 00:02:23,130
there are only two memory copies for

00:02:20,220 --> 00:02:24,380
each push or pull two memory copies are

00:02:23,130 --> 00:02:28,320
reduced

00:02:24,380 --> 00:02:31,350
but in EDR Artman implementation the

00:02:28,320 --> 00:02:35,100
user to use a copy should be reduced our

00:02:31,350 --> 00:02:37,800
limit can be promoted directly but it is

00:02:35,100 --> 00:02:41,570
very very high cost so in this work it

00:02:37,800 --> 00:02:44,550
is out of scope it's my future work and

00:02:41,570 --> 00:02:46,920
in addition to the RMS memory copy

00:02:44,550 --> 00:02:49,320
reduction ultimate implementation

00:02:46,920 --> 00:02:52,440
techniques is very important for

00:02:49,320 --> 00:02:54,870
achieving high performance this table

00:02:52,440 --> 00:02:56,940
shows folks representative

00:02:54,870 --> 00:03:00,840
implementation techniques in this work

00:02:56,940 --> 00:03:03,540
and the implementation status of each

00:03:00,840 --> 00:03:05,400
step step zero is my first

00:03:03,540 --> 00:03:08,790
implementation under step 2

00:03:05,400 --> 00:03:11,520
it's my greatest implementation a first

00:03:08,790 --> 00:03:14,370
technique are my operation means which

00:03:11,520 --> 00:03:18,810
are the operation is to use rdn rights

00:03:14,370 --> 00:03:21,450
were originally in this work we I used a

00:03:18,810 --> 00:03:24,450
diamond light only so all step using

00:03:21,450 --> 00:03:27,600
argument right and ii o technique is

00:03:24,450 --> 00:03:30,150
detecting compression the way to detect

00:03:27,600 --> 00:03:33,900
compression in the two ways pouring or

00:03:30,150 --> 00:03:37,830
interrupt pouring is high CPU overhead

00:03:33,900 --> 00:03:41,820
Patrol agency and interrupt is high

00:03:37,830 --> 00:03:44,640
latency but Rho CP overhead and shadow

00:03:41,820 --> 00:03:47,480
technique is ring buffer it is the same

00:03:44,640 --> 00:03:50,580
as a socket buffer in kernel real and

00:03:47,480 --> 00:03:56,120
rust technique is separating detector

00:03:50,580 --> 00:03:56,120
stress completion thread it is same as

00:03:56,270 --> 00:04:00,900
separating

00:03:57,590 --> 00:04:03,180
receiving process on the data processing

00:04:00,900 --> 00:04:04,530
data process into user to color in

00:04:03,180 --> 00:04:08,010
tcp/ip

00:04:04,530 --> 00:04:10,950
I want to emphasize the Assad and the

00:04:08,010 --> 00:04:12,150
false technique is important our image

00:04:10,950 --> 00:04:15,660
can bypass

00:04:12,150 --> 00:04:18,060
Canarias so edema can reduce canary to

00:04:15,660 --> 00:04:22,320
use a memory copy but our may also

00:04:18,060 --> 00:04:25,040
bypassing Canaria tunings for tcp/ip so

00:04:22,320 --> 00:04:28,640
programmers should be implement

00:04:25,040 --> 00:04:31,880
Canaria tunings into user programs it is

00:04:28,640 --> 00:04:35,840
very important I think and results

00:04:31,880 --> 00:04:38,150
step two is of one-and-a-half Fosters

00:04:35,840 --> 00:04:42,230
and step I'm limitation I am very happy

00:04:38,150 --> 00:04:45,770
but existing implementation is also fast

00:04:42,230 --> 00:04:49,040
as same as step two but there are still

00:04:45,770 --> 00:04:51,110
other chilling so RMS can be first Asus

00:04:49,040 --> 00:04:53,690
Nexus step three implementation it

00:04:51,110 --> 00:04:57,530
fosters an existing the information I

00:04:53,690 --> 00:05:00,860
hope and conclusion army is efficient

00:04:57,530 --> 00:05:04,610
but several techniques is needed to

00:05:00,860 --> 00:05:06,230
achieve high performance and as a lot of

00:05:04,610 --> 00:05:08,930
course it is needed to choose

00:05:06,230 --> 00:05:11,900
appropriate applications for our DMM in

00:05:08,930 --> 00:05:15,020
this work roles and environment network

00:05:11,900 --> 00:05:16,910
road may be not adequate test reportes

00:05:15,020 --> 00:05:20,420
hundred is very fast on the self attains

00:05:16,910 --> 00:05:23,030
right with my conclusion to enjoy

00:05:20,420 --> 00:05:24,830
ultimate effect easy libraries providing

00:05:23,030 --> 00:05:27,380
ultimate design patterns are needed and

00:05:24,830 --> 00:05:30,560
the truth zero copy our implementation

00:05:27,380 --> 00:05:34,040
is needed for higher performance these

00:05:30,560 --> 00:05:36,830
are two is my future works thank you for

00:05:34,040 --> 00:05:41,690
listening my presentation thank you

00:05:36,830 --> 00:05:41,690

YouTube URL: https://www.youtube.com/watch?v=td8HZZl2QRc


