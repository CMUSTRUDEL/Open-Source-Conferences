Title: Lightning Talk: How to Erase Unconscious Bias From Your AI Datasets - Lauren Maffeo, GetApp
Publication date: 2018-08-29
Playlist: Open Source Summit NA 2018 - Vancouver
Description: 
	Lightning Talk: How to Erase Unconscious Bias From Your AI Datasets - Lauren Maffeo, GetApp

About Lauren Maffeo
Lauren Maffeo has reported on and worked within the global technology sector. She started her career as a freelance journalist covering tech trends for The Guardian and The Next Web from London. Today, she works as a senior content analyst at GetApp (a Gartner company), where she covers the impact of emerging tech like AI and blockchain on small and midsize business owners. Lauren has been cited by sources including Forbes, Fox Business, DevOps Digest, The Atlantic, and Inc.com. She has spoken at events including Gartner's Symposium in Florida, The Global Talent Summit at ETH Zurich in Switzerland, Drupal GovCon in Washington, DC, and Mashable's Social Media Day. In 2017, Lauren was named to The Drum's 50 Under 30 list of women worth watching in digital. That same year, she helped organize Women Startup Challenge Europe, which was the continent's largest venture capital competition for women-led startups. She holds an MSc from The London School of Economics and a certificate in Artificial Intelligence: Implications for Business Strategy from MIT's Sloan School of Management.
Captions: 
	00:00:00,000 --> 00:00:04,770
hi everyone thanks for being here with

00:00:02,070 --> 00:00:06,960
me today my name is Lauren Maffeo and as

00:00:04,770 --> 00:00:09,690
mentioned I'm a senior content analyst

00:00:06,960 --> 00:00:11,730
at get app which is a Gartner company I

00:00:09,690 --> 00:00:15,030
cover the impact of emerging

00:00:11,730 --> 00:00:17,220
technologies on AI and blockchain or the

00:00:15,030 --> 00:00:19,770
impact of AI and blockchain on small and

00:00:17,220 --> 00:00:21,420
mid-sized business owners and I want to

00:00:19,770 --> 00:00:25,590
start this presentation by taking us

00:00:21,420 --> 00:00:28,529
back to 2015 there was a tool called how

00:00:25,590 --> 00:00:30,269
old dotnet where you could upload photos

00:00:28,529 --> 00:00:33,329
of yourself and have the facial

00:00:30,269 --> 00:00:34,980
recognition software predict your age so

00:00:33,329 --> 00:00:36,870
I had some fun with this earlier this

00:00:34,980 --> 00:00:39,719
week and uploaded a photo of myself

00:00:36,870 --> 00:00:41,760
taken earlier this month and I was

00:00:39,719 --> 00:00:44,040
flattered to see that how old dotnet

00:00:41,760 --> 00:00:47,250
thought I was 24 years old which is

00:00:44,040 --> 00:00:49,860
younger than I actually am i was less

00:00:47,250 --> 00:00:52,289
flattered when i put in a photo that i

00:00:49,860 --> 00:00:54,780
would have that i had taken three months

00:00:52,289 --> 00:00:57,030
ago and the same tool predicted minutes

00:00:54,780 --> 00:00:59,100
later that i'm actually 38 years old

00:00:57,030 --> 00:01:02,640
which is a little far north of what i am

00:00:59,100 --> 00:01:06,810
at the moment so what how old that made

00:01:02,640 --> 00:01:08,790
news in 2015 as an example of how facial

00:01:06,810 --> 00:01:11,400
recognition software is making strides

00:01:08,790 --> 00:01:15,390
but missing the mark in some key areas

00:01:11,400 --> 00:01:18,150
the challenge here is that machine

00:01:15,390 --> 00:01:20,240
learning gaffes aren't always funny they

00:01:18,150 --> 00:01:22,950
can actually have serious unintended

00:01:20,240 --> 00:01:27,060
consequences for end users as a result

00:01:22,950 --> 00:01:29,759
of datasets that are too monolithic one

00:01:27,060 --> 00:01:31,950
example is a product called compass

00:01:29,759 --> 00:01:34,320
compass is a machine learning algorithm

00:01:31,950 --> 00:01:37,439
that predicts defendant's likelihoods to

00:01:34,320 --> 00:01:40,170
recommit crimes it has however been

00:01:37,439 --> 00:01:43,250
shown to make biased predictions about

00:01:40,170 --> 00:01:46,500
who is more likely to recommit crimes

00:01:43,250 --> 00:01:48,450
research from ProPublica found that the

00:01:46,500 --> 00:01:50,579
tool was 2 times more likely to

00:01:48,450 --> 00:01:52,950
incorrectly cite black defendants as

00:01:50,579 --> 00:01:55,380
being high risk for recommitting crimes

00:01:52,950 --> 00:01:57,030
it was also 2 times more likely to

00:01:55,380 --> 00:01:58,469
incorrectly predict that white

00:01:57,030 --> 00:02:01,020
defendants were low risk for

00:01:58,469 --> 00:02:02,850
recommitting crimes and this isn't a

00:02:01,020 --> 00:02:05,520
hypothetical scenario compass is

00:02:02,850 --> 00:02:08,310
currently used by judges in over 12 US

00:02:05,520 --> 00:02:10,039
states and it's used as a tool to do

00:02:08,310 --> 00:02:13,010
everything from figuring out whether

00:02:10,039 --> 00:02:15,140
people in jail should be let out on bail

00:02:13,010 --> 00:02:18,200
before they go to trial it has an impact

00:02:15,140 --> 00:02:20,209
on the length of people's sentences and

00:02:18,200 --> 00:02:23,269
so the consequences of this algorithm

00:02:20,209 --> 00:02:27,620
are very real for the people affected by

00:02:23,269 --> 00:02:29,930
it however we can't say that this is

00:02:27,620 --> 00:02:32,720
intentional it's not likely that the

00:02:29,930 --> 00:02:35,390
engineers who built compass put bias

00:02:32,720 --> 00:02:37,849
into the system rather it's more likely

00:02:35,390 --> 00:02:40,069
that compass was trained on a data set

00:02:37,849 --> 00:02:43,189
that hadn't been exposed to different

00:02:40,069 --> 00:02:45,349
face phases including skin tones and

00:02:43,189 --> 00:02:47,810
that's an example of what Nicol

00:02:45,349 --> 00:02:49,849
shadowing calls machine bias its

00:02:47,810 --> 00:02:52,159
programming that assumes the prejudices

00:02:49,849 --> 00:02:54,590
of its creators or data and a lot of

00:02:52,159 --> 00:02:56,359
this is unconscious so that makes it

00:02:54,590 --> 00:02:58,519
almost more dangerous because you're not

00:02:56,359 --> 00:03:01,220
necessarily sure when you're building

00:02:58,519 --> 00:03:03,290
the algorithms what whether they're

00:03:01,220 --> 00:03:05,239
going to make the correct outcomes and a

00:03:03,290 --> 00:03:07,099
lot of algorithms are also black box

00:03:05,239 --> 00:03:08,540
which means that if you're not directly

00:03:07,099 --> 00:03:13,420
working on them you can't necessarily

00:03:08,540 --> 00:03:15,799
assess whether they're unbiased or not

00:03:13,420 --> 00:03:17,599
so in the limited time that we have

00:03:15,799 --> 00:03:19,459
today I wanted to use this lightning

00:03:17,599 --> 00:03:21,560
talk to make a case for why you should

00:03:19,459 --> 00:03:23,900
add bias testing to your product

00:03:21,560 --> 00:03:26,329
development cycles we don't have time

00:03:23,900 --> 00:03:28,910
today to get into the nuances of bias

00:03:26,329 --> 00:03:30,680
testing but I do want to make the case

00:03:28,910 --> 00:03:32,389
that this is the right time to start

00:03:30,680 --> 00:03:35,569
doing it from both the product and

00:03:32,389 --> 00:03:38,510
development perspectives ai is still in

00:03:35,569 --> 00:03:40,699
its early days and it's a unique tool

00:03:38,510 --> 00:03:42,650
because it always keeps receiving and

00:03:40,699 --> 00:03:44,720
learning from the data that is inputted

00:03:42,650 --> 00:03:46,790
into the system this means that in

00:03:44,720 --> 00:03:49,579
theory it can retrain itself to give

00:03:46,790 --> 00:03:51,889
more unbiased results but it also runs

00:03:49,579 --> 00:03:54,190
the risk of reinforcing biases that were

00:03:51,889 --> 00:03:58,159
put into the system in the first place

00:03:54,190 --> 00:04:00,500
few AI products use bias testing today

00:03:58,159 --> 00:04:02,870
but if you add it at the start of the

00:04:00,500 --> 00:04:05,329
product lifecycle and then can add

00:04:02,870 --> 00:04:07,099
continuous testing throughout it can

00:04:05,329 --> 00:04:09,919
alleviate some of the issues that we've

00:04:07,099 --> 00:04:12,409
seen with products that are built on AI

00:04:09,919 --> 00:04:13,639
architecture and so that's something

00:04:12,409 --> 00:04:17,329
that I really want to hit home because

00:04:13,639 --> 00:04:19,699
the AI has the potential to transform

00:04:17,329 --> 00:04:22,400
the way we all live and work in ways

00:04:19,699 --> 00:04:24,469
that we haven't seen on a mass scale but

00:04:22,400 --> 00:04:25,170
that heightened opportunity brings

00:04:24,469 --> 00:04:27,660
heightened

00:04:25,170 --> 00:04:30,120
squit it so the best way to make sure

00:04:27,660 --> 00:04:32,640
that your algorithms are unbiased and

00:04:30,120 --> 00:04:35,160
that your impacting consumers in a

00:04:32,640 --> 00:04:37,980
positive way is to design the algorithms

00:04:35,160 --> 00:04:40,560
with diversity in mind from the outset

00:04:37,980 --> 00:04:43,080
and have someone in a diet at a

00:04:40,560 --> 00:04:45,630
scientist role who can constantly comb

00:04:43,080 --> 00:04:47,400
your algorithms to make sure that

00:04:45,630 --> 00:04:49,800
they're being exposed to a wide enough

00:04:47,400 --> 00:04:51,630
range of data that positively has an

00:04:49,800 --> 00:04:53,800
impact on the people who will use your

00:04:51,630 --> 00:04:57,889
products thank you

00:04:53,800 --> 00:04:57,889

YouTube URL: https://www.youtube.com/watch?v=JtQzdTDv-P4


