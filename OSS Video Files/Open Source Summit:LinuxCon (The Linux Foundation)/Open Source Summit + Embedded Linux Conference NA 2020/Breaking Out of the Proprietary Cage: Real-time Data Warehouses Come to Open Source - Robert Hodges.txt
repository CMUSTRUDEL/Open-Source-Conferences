Title: Breaking Out of the Proprietary Cage: Real-time Data Warehouses Come to Open Source - Robert Hodges
Publication date: 2020-09-08
Playlist: Open Source Summit + Embedded Linux Conference NA 2020
Description: 
	Breaking Out of the Proprietary Cage: Real-time Data Warehouses Come to Open Source - Robert Hodges, Altinity LTD
Captions: 
	00:00:03,760 --> 00:00:06,720
hi everybody

00:00:04,560 --> 00:00:07,600
this is robert hodges and let's get this

00:00:06,720 --> 00:00:09,280
party started

00:00:07,600 --> 00:00:11,040
i'll be talking about breaking out of

00:00:09,280 --> 00:00:13,040
the proprietary cage

00:00:11,040 --> 00:00:14,559
real-time data warehouses come to open

00:00:13,040 --> 00:00:17,119
source

00:00:14,559 --> 00:00:19,359
so before i go into the slides i'd like

00:00:17,119 --> 00:00:19,840
to give a big shout out to the people

00:00:19,359 --> 00:00:21,520
who

00:00:19,840 --> 00:00:22,880
at the open source summit that made this

00:00:21,520 --> 00:00:24,480
happen um

00:00:22,880 --> 00:00:27,199
i've done a lot of conferences and i

00:00:24,480 --> 00:00:27,920
think you've all of course attended many

00:00:27,199 --> 00:00:29,840
of them

00:00:27,920 --> 00:00:30,960
it's just a huge effort to get something

00:00:29,840 --> 00:00:33,360
like this to work

00:00:30,960 --> 00:00:34,640
in the in the circumstances that we're

00:00:33,360 --> 00:00:36,640
all facing right now

00:00:34,640 --> 00:00:38,000
so thank you so much it's just a real

00:00:36,640 --> 00:00:39,920
pleasure to be here and i appreciate

00:00:38,000 --> 00:00:42,960
everything you've done

00:00:39,920 --> 00:00:43,760
uh with that let's dive in um here's my

00:00:42,960 --> 00:00:45,200
title slide

00:00:43,760 --> 00:00:47,200
same thing so i'm going to be talking

00:00:45,200 --> 00:00:49,520
about data warehouses and

00:00:47,200 --> 00:00:51,680
how a new kind of data warehouse that i

00:00:49,520 --> 00:00:54,960
call a real-time data warehouse is now

00:00:51,680 --> 00:00:56,840
emerging in open source

00:00:54,960 --> 00:00:58,640
let me just give you a little bit of

00:00:56,840 --> 00:01:00,399
background

00:00:58,640 --> 00:01:02,079
first of all about me this is a picture

00:01:00,399 --> 00:01:02,879
of what i look like when things are

00:01:02,079 --> 00:01:07,439
going well

00:01:02,879 --> 00:01:10,400
on uh on on on my sql examples

00:01:07,439 --> 00:01:12,159
i'm ceo of altenety we are a software

00:01:10,400 --> 00:01:13,840
and services provider for click house

00:01:12,159 --> 00:01:16,080
which is the data main data warehouse

00:01:13,840 --> 00:01:17,840
i'll be talking about in this talk

00:01:16,080 --> 00:01:19,600
as a company we're major committers to

00:01:17,840 --> 00:01:20,400
the project we're also big community

00:01:19,600 --> 00:01:22,640
sponsors

00:01:20,400 --> 00:01:24,960
in the us and western europe in

00:01:22,640 --> 00:01:26,479
particular

00:01:24,960 --> 00:01:29,520
as far as my own background i've been

00:01:26,479 --> 00:01:32,159
working on databases since 1983.

00:01:29,520 --> 00:01:33,520
um plus i took a few breaks i was

00:01:32,159 --> 00:01:36,159
working at vmware for a while on

00:01:33,520 --> 00:01:38,079
virtualization and security

00:01:36,159 --> 00:01:40,079
when i go back and count i think click

00:01:38,079 --> 00:01:43,439
house is number 20 but after a while you

00:01:40,079 --> 00:01:45,840
tend to you tend to lose count so

00:01:43,439 --> 00:01:46,799
it's basically been the main focus of my

00:01:45,840 --> 00:01:49,040
career

00:01:46,799 --> 00:01:51,360
and it's a really really great subject

00:01:49,040 --> 00:01:54,240
and i hope that in this talk i can

00:01:51,360 --> 00:01:55,759
i transmit some of the enthusiasm i feel

00:01:54,240 --> 00:01:59,600
about working with data and

00:01:55,759 --> 00:02:02,000
particularly with data warehouses okay

00:01:59,600 --> 00:02:03,439
oh by the way i see questions popping up

00:02:02,000 --> 00:02:04,960
and i just want to give you a heads up

00:02:03,439 --> 00:02:06,640
that i'm going to make sure that i have

00:02:04,960 --> 00:02:09,840
time to answer them

00:02:06,640 --> 00:02:11,360
um in this at the end of the talk but

00:02:09,840 --> 00:02:13,200
because of the way this platform works

00:02:11,360 --> 00:02:14,800
it's really difficult for me to see them

00:02:13,200 --> 00:02:15,680
coming up and i don't want to screw up

00:02:14,800 --> 00:02:18,800
the talk

00:02:15,680 --> 00:02:20,640
by trying to manipulate the questions so

00:02:18,800 --> 00:02:22,160
they'll definitely be time to answer

00:02:20,640 --> 00:02:24,160
them please cue them up and i'll be

00:02:22,160 --> 00:02:24,879
delighted to answer as many as i can on

00:02:24,160 --> 00:02:26,480
the talk

00:02:24,879 --> 00:02:28,319
and then pick up anything else after

00:02:26,480 --> 00:02:30,800
that in slack

00:02:28,319 --> 00:02:33,200
all right so what i'd like to do is is

00:02:30,800 --> 00:02:36,640
dive in and kind of frame this problem

00:02:33,200 --> 00:02:38,000
about what makes analytic applications

00:02:36,640 --> 00:02:40,480
special because

00:02:38,000 --> 00:02:42,160
in this track we've talked about uh

00:02:40,480 --> 00:02:43,200
different kinds of databases in fact if

00:02:42,160 --> 00:02:45,440
you saw

00:02:43,200 --> 00:02:46,959
amanda's talk a few minutes ago she was

00:02:45,440 --> 00:02:49,519
just scanning the whole

00:02:46,959 --> 00:02:50,239
uh environment of you know sort of the

00:02:49,519 --> 00:02:53,120
panoply

00:02:50,239 --> 00:02:54,160
of different kinds of data databases

00:02:53,120 --> 00:02:55,599
that exist

00:02:54,160 --> 00:02:58,959
and what we're going to be focusing on

00:02:55,599 --> 00:02:59,840
today is databases that are specifically

00:02:58,959 --> 00:03:03,760
designed

00:02:59,840 --> 00:03:06,800
to do analytics and these are

00:03:03,760 --> 00:03:08,319
basically answering um in the

00:03:06,800 --> 00:03:10,720
when it first emerged they were

00:03:08,319 --> 00:03:13,040
answering rather general questions

00:03:10,720 --> 00:03:15,040
about uh business problems and i'm

00:03:13,040 --> 00:03:16,239
giving a simple example here of just

00:03:15,040 --> 00:03:17,920
sales data

00:03:16,239 --> 00:03:20,000
and imagine that it's organized in a

00:03:17,920 --> 00:03:21,840
table because we're dealing with

00:03:20,000 --> 00:03:23,840
relational databases they're concerned

00:03:21,840 --> 00:03:24,640
they're they they like to have things in

00:03:23,840 --> 00:03:27,680
tables

00:03:24,640 --> 00:03:30,720
and you can ask general questions about

00:03:27,680 --> 00:03:34,319
sales data that can then help you drive

00:03:30,720 --> 00:03:36,080
company strategy and even make in some

00:03:34,319 --> 00:03:37,519
cases real-time decisions

00:03:36,080 --> 00:03:39,519
about how you should react to things

00:03:37,519 --> 00:03:41,599
going on in the market so let's just

00:03:39,519 --> 00:03:43,760
take one of these questions

00:03:41,599 --> 00:03:46,560
which kinds of companies are most likely

00:03:43,760 --> 00:03:47,440
to buy sku 556 so that's some product

00:03:46,560 --> 00:03:50,640
number

00:03:47,440 --> 00:03:52,480
and what we're trying to ask is you know

00:03:50,640 --> 00:03:54,959
why is it that why is it that somebody

00:03:52,480 --> 00:03:56,879
buys this when do they buy it can we can

00:03:54,959 --> 00:03:59,040
we understand their buying patterns

00:03:56,879 --> 00:04:01,360
because that would allow us to for

00:03:59,040 --> 00:04:03,680
example pick companies to market to

00:04:01,360 --> 00:04:04,400
maybe give them special offers maybe

00:04:03,680 --> 00:04:07,200
have a

00:04:04,400 --> 00:04:08,000
inventory positioned in particular

00:04:07,200 --> 00:04:10,080
places

00:04:08,000 --> 00:04:11,599
these are all questions that these are

00:04:10,080 --> 00:04:13,280
all things that come out of having an

00:04:11,599 --> 00:04:15,040
accurate answer to that

00:04:13,280 --> 00:04:16,639
and when you go back to the table that

00:04:15,040 --> 00:04:19,919
contains the data

00:04:16,639 --> 00:04:22,320
it quickly becomes apparent that this

00:04:19,919 --> 00:04:23,040
answering this kind of question is very

00:04:22,320 --> 00:04:25,199
different

00:04:23,040 --> 00:04:26,800
from just querying a table in a database

00:04:25,199 --> 00:04:27,840
like mysql

00:04:26,800 --> 00:04:30,639
and i'm going to give you three

00:04:27,840 --> 00:04:31,680
differences that that that make this

00:04:30,639 --> 00:04:33,280
problem

00:04:31,680 --> 00:04:35,199
qualitatively different and therefore

00:04:33,280 --> 00:04:37,120
require a different technical approach

00:04:35,199 --> 00:04:38,639
so the first thing is that when you look

00:04:37,120 --> 00:04:41,199
at an individual sale

00:04:38,639 --> 00:04:42,479
there's just an enormous amount of data

00:04:41,199 --> 00:04:44,240
related to that

00:04:42,479 --> 00:04:45,840
sale that you might want to know we have

00:04:44,240 --> 00:04:47,040
the part number but what's the name of

00:04:45,840 --> 00:04:50,080
the product

00:04:47,040 --> 00:04:50,800
that corresponds to 556. we have the

00:04:50,080 --> 00:04:53,360
date but

00:04:50,800 --> 00:04:55,199
we could be thinking in terms of weeks

00:04:53,360 --> 00:04:56,479
or months or years things like that so

00:04:55,199 --> 00:04:59,600
we want to have different

00:04:56,479 --> 00:05:01,680
different levels of time we have the

00:04:59,600 --> 00:05:02,800
city but what about geographic regions

00:05:01,680 --> 00:05:04,880
we have the customer

00:05:02,800 --> 00:05:06,880
what about the industry that they belong

00:05:04,880 --> 00:05:08,000
to the country where their headquarters

00:05:06,880 --> 00:05:10,720
is located

00:05:08,000 --> 00:05:12,720
things like that so you can see that

00:05:10,720 --> 00:05:13,440
there's an enormous amount of data that

00:05:12,720 --> 00:05:15,520
you need

00:05:13,440 --> 00:05:17,759
to effectively to have at your

00:05:15,520 --> 00:05:18,479
fingertips that allow you to adorn this

00:05:17,759 --> 00:05:21,759
simple

00:05:18,479 --> 00:05:24,400
sale and then have more context about

00:05:21,759 --> 00:05:25,840
what was going on where and how when

00:05:24,400 --> 00:05:29,039
this happened

00:05:25,840 --> 00:05:31,120
the next thing is looking down the table

00:05:29,039 --> 00:05:33,360
is the data values could be the number

00:05:31,120 --> 00:05:35,759
of data values could be enormous

00:05:33,360 --> 00:05:37,759
in sales data it actually tends to be

00:05:35,759 --> 00:05:38,720
kind of small because it's generated by

00:05:37,759 --> 00:05:41,039
humans

00:05:38,720 --> 00:05:42,880
but in a lot of analytic applications

00:05:41,039 --> 00:05:45,120
the data is actually things like

00:05:42,880 --> 00:05:46,560
people's locations off their cell phones

00:05:45,120 --> 00:05:48,639
or where they were clicking on

00:05:46,560 --> 00:05:50,880
on on a web screen we're talking

00:05:48,639 --> 00:05:52,720
trillions or even quadrillions of rows

00:05:50,880 --> 00:05:54,639
of data

00:05:52,720 --> 00:05:56,000
and then the final thing is to answer

00:05:54,639 --> 00:05:58,400
these questions

00:05:56,000 --> 00:05:59,199
we need to be able to take this very

00:05:58,400 --> 00:06:00,479
long

00:05:59,199 --> 00:06:03,280
you know sort of very long list of

00:06:00,479 --> 00:06:04,479
records this very wide range of data

00:06:03,280 --> 00:06:06,479
associated with each

00:06:04,479 --> 00:06:07,520
record and we basically need to be able

00:06:06,479 --> 00:06:10,240
to combine it

00:06:07,520 --> 00:06:11,919
in any pattern imaginable we call this

00:06:10,240 --> 00:06:15,120
slicing and dicing

00:06:11,919 --> 00:06:17,039
so so there's no particular we can't

00:06:15,120 --> 00:06:18,800
make an assumption about what the access

00:06:17,039 --> 00:06:20,560
pattern is on the data

00:06:18,800 --> 00:06:22,000
the person who's asking these questions

00:06:20,560 --> 00:06:23,680
and trying to get solutions

00:06:22,000 --> 00:06:25,039
could look at the data any way that

00:06:23,680 --> 00:06:28,960
pleases them

00:06:25,039 --> 00:06:29,360
so these three things became apparent to

00:06:28,960 --> 00:06:33,280
people

00:06:29,360 --> 00:06:36,240
starting in the in the in the 1980s

00:06:33,280 --> 00:06:37,440
and they led to some really interesting

00:06:36,240 --> 00:06:39,520
innovation

00:06:37,440 --> 00:06:41,120
to create what we now know of as data

00:06:39,520 --> 00:06:43,039
warehouses

00:06:41,120 --> 00:06:44,960
so there were basically three looking

00:06:43,039 --> 00:06:48,639
back in the history there were basically

00:06:44,960 --> 00:06:51,599
three um sort of rounds i think

00:06:48,639 --> 00:06:52,960
if uh if you will of technical uh

00:06:51,599 --> 00:06:55,039
technical advances

00:06:52,960 --> 00:06:56,960
so the first one was sort of set up in

00:06:55,039 --> 00:06:58,960
the 1980s but really became apparent in

00:06:56,960 --> 00:07:02,319
the early 90s with a couple of

00:06:58,960 --> 00:07:03,039
products cybase aq and teradata so the

00:07:02,319 --> 00:07:04,639
first thing

00:07:03,039 --> 00:07:06,479
that the people did was they said hey

00:07:04,639 --> 00:07:07,840
we've got a lot of data what we'd like

00:07:06,479 --> 00:07:09,520
to be able to do is process it on

00:07:07,840 --> 00:07:10,080
multiple hosts and this is something

00:07:09,520 --> 00:07:12,880
that

00:07:10,080 --> 00:07:14,000
teradata did spread the data out be able

00:07:12,880 --> 00:07:16,560
to

00:07:14,000 --> 00:07:18,000
multi you know sort of issue a query

00:07:16,560 --> 00:07:19,840
that that breaks

00:07:18,000 --> 00:07:21,440
that attacks the data on each machine

00:07:19,840 --> 00:07:23,599
and takes advantage of more computing

00:07:21,440 --> 00:07:24,960
resources so that's mpp enabling

00:07:23,599 --> 00:07:26,880
there were things like column storage

00:07:24,960 --> 00:07:28,720
and bitmap indexes

00:07:26,880 --> 00:07:30,160
i was actually working at scibase when

00:07:28,720 --> 00:07:32,000
we acquired iq

00:07:30,160 --> 00:07:33,360
and i remember hearing a talk from a guy

00:07:32,000 --> 00:07:35,440
called clark french

00:07:33,360 --> 00:07:37,280
about hey this this is a new kind of

00:07:35,440 --> 00:07:39,280
business problem here's how we have to

00:07:37,280 --> 00:07:40,000
cluster the data basically loaded in

00:07:39,280 --> 00:07:42,800
columns

00:07:40,000 --> 00:07:44,560
use bitmap indexes because this is a

00:07:42,800 --> 00:07:46,720
different kind of problem and he

00:07:44,560 --> 00:07:48,000
you know we we sort of dimly realized at

00:07:46,720 --> 00:07:49,520
the time hey this is a

00:07:48,000 --> 00:07:51,680
this work this really does require

00:07:49,520 --> 00:07:54,240
different technology

00:07:51,680 --> 00:07:55,280
the next line of advance a couple of

00:07:54,240 --> 00:07:58,160
advances came in the

00:07:55,280 --> 00:07:59,599
between 2000 and 2010 and that was with

00:07:58,160 --> 00:08:02,000
products like vectorwise

00:07:59,599 --> 00:08:05,160
and vertica so things like being able to

00:08:02,000 --> 00:08:06,639
vectorize the execution using simd

00:08:05,160 --> 00:08:08,879
instructions

00:08:06,639 --> 00:08:10,879
being able to reorganize have different

00:08:08,879 --> 00:08:12,560
organizations of the same data that was

00:08:10,879 --> 00:08:14,080
an innovation from vertica

00:08:12,560 --> 00:08:16,160
and then of course things like uh

00:08:14,080 --> 00:08:19,599
compression and then the final

00:08:16,160 --> 00:08:22,000
the final um uh

00:08:19,599 --> 00:08:23,599
sort of set of of technical innovations

00:08:22,000 --> 00:08:25,440
have come over the last decade

00:08:23,599 --> 00:08:27,360
and with the advent of the cloud and

00:08:25,440 --> 00:08:30,080
just to to name one

00:08:27,360 --> 00:08:31,599
amazon redshift was a groundbreaking

00:08:30,080 --> 00:08:33,039
product because what it did is it took

00:08:31,599 --> 00:08:35,599
these data warehouses

00:08:33,039 --> 00:08:37,279
which are complex software you often had

00:08:35,599 --> 00:08:38,640
to wait months

00:08:37,279 --> 00:08:40,719
just to get the hardware for them let

00:08:38,640 --> 00:08:43,760
alone get the software installed on it

00:08:40,719 --> 00:08:45,600
and basically the the redshift team

00:08:43,760 --> 00:08:47,360
made this happen within a few seconds

00:08:45,600 --> 00:08:50,480
you go to a

00:08:47,360 --> 00:08:51,920
a screen enter a few clicks you know

00:08:50,480 --> 00:08:53,600
tell you know what side of the database

00:08:51,920 --> 00:08:55,040
you want and within a few minutes you

00:08:53,600 --> 00:08:55,519
have a data warehouse spun up that you

00:08:55,040 --> 00:08:58,320
can

00:08:55,519 --> 00:08:59,200
begin playing around with that so these

00:08:58,320 --> 00:09:02,560
are all

00:08:59,200 --> 00:09:05,519
um sort of profound innovations and and

00:09:02,560 --> 00:09:07,680
sort of in each in each area

00:09:05,519 --> 00:09:09,200
demonstrated enormous creativity to try

00:09:07,680 --> 00:09:10,720
and answer these questions more

00:09:09,200 --> 00:09:15,279
effectively

00:09:10,720 --> 00:09:17,600
however the solutions all had a single

00:09:15,279 --> 00:09:19,519
unifying or the the people working on

00:09:17,600 --> 00:09:20,800
them all had sort of a unifying

00:09:19,519 --> 00:09:22,640
characteristic

00:09:20,800 --> 00:09:24,800
and you know if you're an open source

00:09:22,640 --> 00:09:25,760
person or or a database person you look

00:09:24,800 --> 00:09:27,360
at this and you may

00:09:25,760 --> 00:09:29,440
it may just jump out at you these are

00:09:27,360 --> 00:09:32,480
all proprietary products

00:09:29,440 --> 00:09:34,560
so this was this innovation

00:09:32,480 --> 00:09:36,080
particularly early on but extending up

00:09:34,560 --> 00:09:38,399
into even to today

00:09:36,080 --> 00:09:42,000
has been driven in large part by

00:09:38,399 --> 00:09:44,880
innovations on proprietary products

00:09:42,000 --> 00:09:45,279
and in fact when we look at the open

00:09:44,880 --> 00:09:47,600
source

00:09:45,279 --> 00:09:48,640
competitors specifically for data

00:09:47,600 --> 00:09:51,040
warehouses again

00:09:48,640 --> 00:09:52,560
sort of these are relational databases

00:09:51,040 --> 00:09:55,519
with relational model

00:09:52,560 --> 00:09:57,279
um sort of tabular data designed to move

00:09:55,519 --> 00:09:59,440
you know answer quite general

00:09:57,279 --> 00:10:01,279
open-ended questions very very quickly

00:09:59,440 --> 00:10:04,000
over large amounts of data

00:10:01,279 --> 00:10:04,480
there's actually not that much out in

00:10:04,000 --> 00:10:07,040
the

00:10:04,480 --> 00:10:08,720
um in the open source that's a direct

00:10:07,040 --> 00:10:10,560
competitor

00:10:08,720 --> 00:10:12,240
i'm going to give three examples here

00:10:10,560 --> 00:10:13,360
that that illustrate the kind of

00:10:12,240 --> 00:10:15,040
technology there are

00:10:13,360 --> 00:10:17,760
others of course but i think these give

00:10:15,040 --> 00:10:21,120
you a sense of where people were going

00:10:17,760 --> 00:10:23,120
so for example there's presta which was

00:10:21,120 --> 00:10:26,000
originally developed at facebook

00:10:23,120 --> 00:10:26,480
it is designed to do query over data

00:10:26,000 --> 00:10:28,560
lakes

00:10:26,480 --> 00:10:30,839
so large collections of data for example

00:10:28,560 --> 00:10:32,399
living in object storage or living on

00:10:30,839 --> 00:10:34,959
hdfs

00:10:32,399 --> 00:10:37,519
that's one solution that's out there not

00:10:34,959 --> 00:10:40,079
really a direct competitor to the

00:10:37,519 --> 00:10:40,959
to the data warehouses but but sql based

00:10:40,079 --> 00:10:42,480
and

00:10:40,959 --> 00:10:44,000
and focused on the problem of large

00:10:42,480 --> 00:10:47,519
amounts of data

00:10:44,000 --> 00:10:50,800
at the other end is druid which is a

00:10:47,519 --> 00:10:52,079
popular open source uh system that's

00:10:50,800 --> 00:10:55,200
designed to handle

00:10:52,079 --> 00:10:56,640
queries on large event streams so things

00:10:55,200 --> 00:10:59,279
running into

00:10:56,640 --> 00:11:00,320
trillions or quadrillions of records and

00:10:59,279 --> 00:11:03,279
it was innovated

00:11:00,320 --> 00:11:03,760
uh particularly innovative because it it

00:11:03,279 --> 00:11:05,519
was

00:11:03,760 --> 00:11:07,519
able to throw a lot of hardware at the

00:11:05,519 --> 00:11:08,240
problem and guarantee a certain level of

00:11:07,519 --> 00:11:10,880
latency

00:11:08,240 --> 00:11:12,399
so this is this is a definitely an

00:11:10,880 --> 00:11:14,640
interesting technology

00:11:12,399 --> 00:11:16,079
but it was not actually originally sql

00:11:14,640 --> 00:11:17,760
and then in the middle

00:11:16,079 --> 00:11:19,440
what i'm going to focus on for the rest

00:11:17,760 --> 00:11:21,760
of this talk is a database

00:11:19,440 --> 00:11:23,839
called click house and this is a ground

00:11:21,760 --> 00:11:26,720
up sql implementation

00:11:23,839 --> 00:11:28,880
to get to get quick answers on

00:11:26,720 --> 00:11:30,640
structured data so it started out as a

00:11:28,880 --> 00:11:32,720
relational database

00:11:30,640 --> 00:11:34,720
it was originally developed at yandex

00:11:32,720 --> 00:11:38,240
the the first prototype of it

00:11:34,720 --> 00:11:40,480
was uh was done in 2008 and it was open

00:11:38,240 --> 00:11:44,560
sourced under the apache tool license

00:11:40,480 --> 00:11:46,000
in um in 2016.

00:11:44,560 --> 00:11:47,519
so what i'm now going to do is shift

00:11:46,000 --> 00:11:48,560
away from history and let's look

00:11:47,519 --> 00:11:52,880
specifically

00:11:48,560 --> 00:11:52,880
at what makes these databases

00:11:53,440 --> 00:12:00,320
you know sort of a particularly powerful

00:11:56,880 --> 00:12:01,839
so let's um let's just look at the

00:12:00,320 --> 00:12:03,519
at the key features of qlik house and

00:12:01,839 --> 00:12:05,200
the way that i like to explain click

00:12:03,519 --> 00:12:06,000
house for people that aren't familiar

00:12:05,200 --> 00:12:09,760
with it

00:12:06,000 --> 00:12:12,160
is hey imagine that my

00:12:09,760 --> 00:12:13,600
which is a very very popular open source

00:12:12,160 --> 00:12:16,160
database

00:12:13,600 --> 00:12:18,320
and vertica which was one of the the

00:12:16,160 --> 00:12:19,519
databases i mentioned the proprietary

00:12:18,320 --> 00:12:21,760
databases i mentioned

00:12:19,519 --> 00:12:23,760
in the data warehouse field imagine they

00:12:21,760 --> 00:12:26,880
got married and had a baby

00:12:23,760 --> 00:12:30,000
well that baby would be click house and

00:12:26,880 --> 00:12:30,560
so from the my sequel side you get a

00:12:30,000 --> 00:12:32,639
simple

00:12:30,560 --> 00:12:33,920
uh simple operational model so click

00:12:32,639 --> 00:12:36,800
house is just a single c

00:12:33,920 --> 00:12:38,959
plus binary you can basically install it

00:12:36,800 --> 00:12:41,040
and get it running in about 60 seconds

00:12:38,959 --> 00:12:42,079
it's about the same speed as installing

00:12:41,040 --> 00:12:44,639
and bringing up

00:12:42,079 --> 00:12:47,360
my sequel it has sql language of course

00:12:44,639 --> 00:12:50,240
we get that from both parents

00:12:47,360 --> 00:12:52,160
it is open source and it's relatively

00:12:50,240 --> 00:12:54,639
simple to run

00:12:52,160 --> 00:12:55,200
from the from the vertica side of the

00:12:54,639 --> 00:12:56,639
house

00:12:55,200 --> 00:12:58,079
we get things like shared nothing

00:12:56,639 --> 00:13:00,079
architecture so we have a bunch of

00:12:58,079 --> 00:13:02,959
computers each with their own storage

00:13:00,079 --> 00:13:03,680
uh but don't have shared file systems uh

00:13:02,959 --> 00:13:06,399
don't have

00:13:03,680 --> 00:13:08,320
uh you know sort of complicated

00:13:06,399 --> 00:13:10,480
networking architectures

00:13:08,320 --> 00:13:12,000
it it's a relatively simple and easy to

00:13:10,480 --> 00:13:13,760
understand architecture

00:13:12,000 --> 00:13:15,760
we have column storage with high

00:13:13,760 --> 00:13:16,399
compression and codecs we'll talk about

00:13:15,760 --> 00:13:18,800
that

00:13:16,399 --> 00:13:19,600
in more detail we have vectorized query

00:13:18,800 --> 00:13:21,600
execution

00:13:19,600 --> 00:13:24,160
and then we have mpp enablement

00:13:21,600 --> 00:13:26,480
basically being able to split the data

00:13:24,160 --> 00:13:27,600
up into shards and replicants so that's

00:13:26,480 --> 00:13:30,720
coming from the

00:13:27,600 --> 00:13:32,480
conceptually from from vertica

00:13:30,720 --> 00:13:34,320
and then the whole thing is really fast

00:13:32,480 --> 00:13:36,959
so everything in

00:13:34,320 --> 00:13:38,560
when we when we think about click house

00:13:36,959 --> 00:13:40,240
we're using column storage we think of

00:13:38,560 --> 00:13:41,279
everything as either a sequential read

00:13:40,240 --> 00:13:42,399
or sequential write

00:13:41,279 --> 00:13:44,880
and there's a huge number of

00:13:42,399 --> 00:13:45,920
optimizations that are built into the

00:13:44,880 --> 00:13:47,920
product as well as

00:13:45,920 --> 00:13:50,480
along with product features that allow

00:13:47,920 --> 00:13:51,920
us to get answers very very quickly

00:13:50,480 --> 00:13:55,600
and that's what i'm going to jump into

00:13:51,920 --> 00:13:58,800
next so that's your basic overview

00:13:55,600 --> 00:14:01,760
and then now what we can do is dig down

00:13:58,800 --> 00:14:02,000
into click house itself and understand

00:14:01,760 --> 00:14:04,880
how

00:14:02,000 --> 00:14:06,480
it works and what makes it fast before i

00:14:04,880 --> 00:14:08,240
do that though i want to talk a little

00:14:06,480 --> 00:14:11,440
bit about what it's not

00:14:08,240 --> 00:14:13,519
because every time you solve a problem

00:14:11,440 --> 00:14:15,120
you're making choices about problems

00:14:13,519 --> 00:14:17,519
you're not going to solve

00:14:15,120 --> 00:14:19,839
so what doesn't click us do well it's

00:14:17,519 --> 00:14:22,880
not an asset compliant database

00:14:19,839 --> 00:14:25,199
like mysql or postgres click house has a

00:14:22,880 --> 00:14:27,839
transaction model

00:14:25,199 --> 00:14:28,800
it deals with large basically the

00:14:27,839 --> 00:14:30,959
transactional

00:14:28,800 --> 00:14:32,639
unit if you will is is a large chunk of

00:14:30,959 --> 00:14:35,040
data called a part

00:14:32,639 --> 00:14:37,199
but it's not focused on things

00:14:35,040 --> 00:14:40,079
necessarily like isolation

00:14:37,199 --> 00:14:42,800
like although it does support it to a

00:14:40,079 --> 00:14:42,800
certain extent

00:14:42,880 --> 00:14:49,760
so it's not um not completely acid

00:14:46,320 --> 00:14:51,040
acid compliant and it also doesn't um

00:14:49,760 --> 00:14:52,480
deal with things like updates

00:14:51,040 --> 00:14:54,480
particularly well because it makes the

00:14:52,480 --> 00:14:55,760
basic assumption data is immutable

00:14:54,480 --> 00:14:57,440
what else is it not well it's not a

00:14:55,760 --> 00:14:58,000
distributed key value store so if you

00:14:57,440 --> 00:15:01,839
have a large

00:14:58,000 --> 00:15:03,600
number of of you know sensor values and

00:15:01,839 --> 00:15:05,199
you want to go to each one individually

00:15:03,600 --> 00:15:05,680
and and visit them and see what they're

00:15:05,199 --> 00:15:07,360
doing

00:15:05,680 --> 00:15:08,880
something like cassandra is probably

00:15:07,360 --> 00:15:10,720
better

00:15:08,880 --> 00:15:12,560
it's not a highly concurrent cache

00:15:10,720 --> 00:15:15,680
server like redis so if you're storing

00:15:12,560 --> 00:15:17,760
session data for um for users

00:15:15,680 --> 00:15:19,360
click us is probably not the product and

00:15:17,760 --> 00:15:20,480
finally and this is really important

00:15:19,360 --> 00:15:22,320
when we look at

00:15:20,480 --> 00:15:23,519
direct competitors in the in the data

00:15:22,320 --> 00:15:27,360
warehouse space

00:15:23,519 --> 00:15:28,399
it is not it the full sql complete sql

00:15:27,360 --> 00:15:30,720
compliance

00:15:28,399 --> 00:15:31,519
is not the main design point of the

00:15:30,720 --> 00:15:34,959
system

00:15:31,519 --> 00:15:37,199
it is speed followed by having enough

00:15:34,959 --> 00:15:39,120
sql that you can get the job done and

00:15:37,199 --> 00:15:41,519
also feel comfortable working with it so

00:15:39,120 --> 00:15:43,120
for example things like window functions

00:15:41,519 --> 00:15:44,880
which you may be familiar with if you

00:15:43,120 --> 00:15:47,519
work with analytic databases

00:15:44,880 --> 00:15:48,160
they don't ex they don't exist yet in

00:15:47,519 --> 00:15:50,959
click house

00:15:48,160 --> 00:15:52,160
although we're working on them so what

00:15:50,959 --> 00:15:55,680
we do have though

00:15:52,160 --> 00:15:57,360
is speed so um

00:15:55,680 --> 00:15:59,600
let me just talk about the code this is

00:15:57,360 --> 00:16:01,600
sort of a sort of an eye test i want to

00:15:59,600 --> 00:16:04,880
just focus on a couple things

00:16:01,600 --> 00:16:07,440
basically click house is some of the

00:16:04,880 --> 00:16:09,360
best c plus plus i've ever seen

00:16:07,440 --> 00:16:11,759
it's actually readable if you if you've

00:16:09,360 --> 00:16:13,759
used i'm not a c plus programmer so

00:16:11,759 --> 00:16:17,199
don't anybody hammer me i've mostly

00:16:13,759 --> 00:16:20,079
worked in java go and python

00:16:17,199 --> 00:16:21,600
but the code is really readable it is

00:16:20,079 --> 00:16:24,800
very well written code

00:16:21,600 --> 00:16:26,560
and it's ex there's a huge emphasis on

00:16:24,800 --> 00:16:28,240
optimizations for speed i'll just give

00:16:26,560 --> 00:16:30,399
you a simple example

00:16:28,240 --> 00:16:31,680
when we talk about group by which is the

00:16:30,399 --> 00:16:34,880
sql construct

00:16:31,680 --> 00:16:36,000
to do aggregation click house has 14

00:16:34,880 --> 00:16:38,880
algorithms for doing

00:16:36,000 --> 00:16:40,320
group by that are specific to data type

00:16:38,880 --> 00:16:42,959
so we're always trying to

00:16:40,320 --> 00:16:44,079
choose algorithms that are best suited

00:16:42,959 --> 00:16:45,920
for the data

00:16:44,079 --> 00:16:47,519
the type of data that we're dealing and

00:16:45,920 --> 00:16:48,880
its distribution

00:16:47,519 --> 00:16:50,240
and you'll see that constantly

00:16:48,880 --> 00:16:52,079
throughout the code in click house

00:16:50,240 --> 00:16:53,199
there's no one way for example to do a

00:16:52,079 --> 00:16:55,279
hash table

00:16:53,199 --> 00:16:56,240
there's a bunch of different algorithms

00:16:55,279 --> 00:16:58,480
for hash tables

00:16:56,240 --> 00:17:01,040
and we'll pick one according to what

00:16:58,480 --> 00:17:04,319
what particular use case we're solving

00:17:01,040 --> 00:17:06,640
another really important feature of qlik

00:17:04,319 --> 00:17:09,280
house is vectorized query execution

00:17:06,640 --> 00:17:11,280
so basically as you'll you'll see this

00:17:09,280 --> 00:17:13,039
more but we basically focus on breaking

00:17:11,280 --> 00:17:15,280
up data into pieces

00:17:13,039 --> 00:17:17,839
farming out the process to every core

00:17:15,280 --> 00:17:19,280
and every hyper thread on those cores

00:17:17,839 --> 00:17:20,000
and doing that as efficiently as

00:17:19,280 --> 00:17:22,720
possible

00:17:20,000 --> 00:17:23,919
and where applicable applying s semdi

00:17:22,720 --> 00:17:26,480
instructions

00:17:23,919 --> 00:17:27,039
so that we single instruction multiple

00:17:26,480 --> 00:17:28,960
data

00:17:27,039 --> 00:17:30,320
so that we can basically get multiple

00:17:28,960 --> 00:17:32,160
operations done

00:17:30,320 --> 00:17:33,679
in a small in a smaller number of

00:17:32,160 --> 00:17:35,520
machine cycles

00:17:33,679 --> 00:17:37,120
so i'm not going to go deeper into this

00:17:35,520 --> 00:17:38,799
but if you look at the code it's it's

00:17:37,120 --> 00:17:40,559
filled with interesting optimizations

00:17:38,799 --> 00:17:41,280
and there's some great talks on how this

00:17:40,559 --> 00:17:44,240
works

00:17:41,280 --> 00:17:44,240
that you can check out

00:17:44,480 --> 00:17:48,000
now turning to stuff that's visible to a

00:17:47,120 --> 00:17:51,200
user

00:17:48,000 --> 00:17:53,039
we have a table type called merge tree

00:17:51,200 --> 00:17:55,039
and one of the interesting things about

00:17:53,039 --> 00:17:56,720
click house at least to me because i

00:17:55,039 --> 00:17:59,200
worked with my sql for a long time

00:17:56,720 --> 00:18:00,000
is click house uses table engines so if

00:17:59,200 --> 00:18:02,880
you've used

00:18:00,000 --> 00:18:05,360
my sql you you probably remember these

00:18:02,880 --> 00:18:06,720
there there was nodb there was my i sam

00:18:05,360 --> 00:18:07,919
there's falcon

00:18:06,720 --> 00:18:09,840
you know a bunch of different table

00:18:07,919 --> 00:18:13,360
types um

00:18:09,840 --> 00:18:15,760
uh my sequel follow excuse me

00:18:13,360 --> 00:18:17,360
click house followed that uh that design

00:18:15,760 --> 00:18:18,640
pattern partly because the

00:18:17,360 --> 00:18:20,880
the folks that wrote it were very

00:18:18,640 --> 00:18:22,720
familiar with uh

00:18:20,880 --> 00:18:24,320
with my sequel but it's used much more

00:18:22,720 --> 00:18:26,320
broadly so there's about

00:18:24,320 --> 00:18:28,080
40 different table engines and they all

00:18:26,320 --> 00:18:30,240
do something useful they're basically

00:18:28,080 --> 00:18:32,320
tuned to particular work cases the

00:18:30,240 --> 00:18:33,200
number one table engine is called merge

00:18:32,320 --> 00:18:36,160
tree

00:18:33,200 --> 00:18:36,559
and it's really the uh kind of a family

00:18:36,160 --> 00:18:39,200
of

00:18:36,559 --> 00:18:39,919
actually turns out to be a family of

00:18:39,200 --> 00:18:42,320
tables

00:18:39,919 --> 00:18:42,320
of which

00:18:45,440 --> 00:18:49,360
so it's a create table if you've used

00:18:47,440 --> 00:18:50,559
sql before this looks fairly familiar

00:18:49,360 --> 00:18:52,320
our data types are a little bit

00:18:50,559 --> 00:18:55,120
different from ansi sql

00:18:52,320 --> 00:18:56,480
but then you have these uh you have

00:18:55,120 --> 00:18:58,000
these

00:18:56,480 --> 00:19:00,400
extra things at the end of the table

00:18:58,000 --> 00:19:03,919
first of all defining the engine

00:19:00,400 --> 00:19:05,840
so it's merge tree giving us a way to

00:19:03,919 --> 00:19:08,000
partition the data because this is

00:19:05,840 --> 00:19:09,520
a table type that is designed for very

00:19:08,000 --> 00:19:11,520
large amounts of data

00:19:09,520 --> 00:19:12,640
we want to have a way to break it up

00:19:11,520 --> 00:19:14,960
into parts

00:19:12,640 --> 00:19:16,880
and what this sql is saying is take the

00:19:14,960 --> 00:19:18,080
date this is flight data as it turns out

00:19:16,880 --> 00:19:19,840
flight on time data

00:19:18,080 --> 00:19:22,160
and break it up by month and then

00:19:19,840 --> 00:19:23,679
finally within those parts how to order

00:19:22,160 --> 00:19:25,200
it and we're going to order by the

00:19:23,679 --> 00:19:26,400
carrier and the date of the actual

00:19:25,200 --> 00:19:28,240
flight

00:19:26,400 --> 00:19:30,000
so so this is something that you see

00:19:28,240 --> 00:19:33,280
visibly and then what happens

00:19:30,000 --> 00:19:36,160
when the data is actually implemented

00:19:33,280 --> 00:19:38,640
what we see is that you'll actually go

00:19:36,160 --> 00:19:42,559
to the file system

00:19:38,640 --> 00:19:44,880
and you can see parts which consist of

00:19:42,559 --> 00:19:46,799
what we call a primary key index it's a

00:19:44,880 --> 00:19:48,000
sparse index i'll show you the structure

00:19:46,799 --> 00:19:49,919
of it in a minute

00:19:48,000 --> 00:19:51,600
and then all the data is present in

00:19:49,919 --> 00:19:54,240
columns so we store the data

00:19:51,600 --> 00:19:56,400
as highly compressed arrays and those

00:19:54,240 --> 00:19:57,280
arrays are sorted on the order by

00:19:56,400 --> 00:19:59,760
columns

00:19:57,280 --> 00:20:01,679
and the index gives us the ability to

00:19:59,760 --> 00:20:03,840
find particular rows

00:20:01,679 --> 00:20:05,360
and group them together so that we can

00:20:03,840 --> 00:20:07,360
take the

00:20:05,360 --> 00:20:08,960
you know if we have to if we're

00:20:07,360 --> 00:20:11,200
referring to like three columns

00:20:08,960 --> 00:20:12,559
we can we have a way of finding out

00:20:11,200 --> 00:20:13,600
where they're located in each of their

00:20:12,559 --> 00:20:15,280
individual arrays

00:20:13,600 --> 00:20:16,799
and getting the data get the data

00:20:15,280 --> 00:20:18,799
consistently

00:20:16,799 --> 00:20:21,280
so that's the basic high level layout

00:20:18,799 --> 00:20:23,120
and you can see this on the file system

00:20:21,280 --> 00:20:24,960
when you actually go to one of the

00:20:23,120 --> 00:20:26,640
directories that contains this

00:20:24,960 --> 00:20:28,400
and one of the cool things about click

00:20:26,640 --> 00:20:29,840
house that i really like

00:20:28,400 --> 00:20:32,320
is that this is all visible it's kind of

00:20:29,840 --> 00:20:35,039
like my sql

00:20:32,320 --> 00:20:36,400
where you can just go and and not with

00:20:35,039 --> 00:20:39,200
the nodb table of course

00:20:36,400 --> 00:20:40,320
or in odb table type but with my isam

00:20:39,200 --> 00:20:41,679
you can just go and you can see all the

00:20:40,320 --> 00:20:44,799
data lying around on

00:20:41,679 --> 00:20:46,960
on disk uh click house is very much

00:20:44,799 --> 00:20:48,720
uh follows that and you can see you can

00:20:46,960 --> 00:20:50,640
basically see all the structures

00:20:48,720 --> 00:20:52,880
and what you see when you come in is

00:20:50,640 --> 00:20:55,760
this primary.idx file

00:20:52,880 --> 00:20:56,480
and that's as i said a sparse index it's

00:20:55,760 --> 00:20:58,000
not

00:20:56,480 --> 00:20:59,840
it's not used to maintain data

00:20:58,000 --> 00:21:01,679
consistency in this in the way that a

00:20:59,840 --> 00:21:04,080
primary key would be in postgres

00:21:01,679 --> 00:21:04,960
or or click house it's used to find

00:21:04,080 --> 00:21:06,799
things

00:21:04,960 --> 00:21:08,480
and it's sparse in the sense that we

00:21:06,799 --> 00:21:12,320
only have entries

00:21:08,480 --> 00:21:14,559
for every say 8192 entries

00:21:12,320 --> 00:21:15,600
so what that means is we read data in

00:21:14,559 --> 00:21:17,919
chunks

00:21:15,600 --> 00:21:19,039
and the lowest resolution that we're

00:21:17,919 --> 00:21:20,559
going to get in aquarius we're going to

00:21:19,039 --> 00:21:22,799
read about 8 000 rows

00:21:20,559 --> 00:21:23,919
you can change this if you want that's

00:21:22,799 --> 00:21:26,159
called a granule

00:21:23,919 --> 00:21:27,760
and then to actually find where the data

00:21:26,159 --> 00:21:32,320
for the columns is located

00:21:27,760 --> 00:21:34,960
in each in each of these

00:21:32,320 --> 00:21:35,919
columns we have what are called dot mrk

00:21:34,960 --> 00:21:39,039
files

00:21:35,919 --> 00:21:40,559
and these are just an array that matches

00:21:39,039 --> 00:21:43,520
that primary.idx

00:21:40,559 --> 00:21:44,640
and they contain offsets into the actual

00:21:43,520 --> 00:21:47,039
column data

00:21:44,640 --> 00:21:48,480
and each of those offsets is a chunk of

00:21:47,039 --> 00:21:51,840
compressed data

00:21:48,480 --> 00:21:53,440
so compressed block that's that's also

00:21:51,840 --> 00:21:55,360
may have some additional transformations

00:21:53,440 --> 00:21:58,240
on it so this is your basic

00:21:55,360 --> 00:22:00,320
structure those blocks are called marks

00:21:58,240 --> 00:22:01,679
and as i say they contain a compressed

00:22:00,320 --> 00:22:03,440
block of data and you can just

00:22:01,679 --> 00:22:04,720
bounce in and start reading at that

00:22:03,440 --> 00:22:06,640
point and

00:22:04,720 --> 00:22:08,559
and then bring the data into memory and

00:22:06,640 --> 00:22:11,280
begin processing it

00:22:08,559 --> 00:22:13,120
so this structure is is really important

00:22:11,280 --> 00:22:14,720
it's super efficient because it

00:22:13,120 --> 00:22:16,320
minimizes the amount of data we store in

00:22:14,720 --> 00:22:16,880
storage i'll show you some examples of

00:22:16,320 --> 00:22:18,640
that

00:22:16,880 --> 00:22:20,400
and the other thing is it means that hey

00:22:18,640 --> 00:22:22,240
if we're only talking if we're only

00:22:20,400 --> 00:22:25,679
looking at two out of 100 columns

00:22:22,240 --> 00:22:27,440
we actually only read those columns and

00:22:25,679 --> 00:22:30,960
then only the parts of those columns

00:22:27,440 --> 00:22:30,960
that we think are relevant for the query

00:22:31,440 --> 00:22:38,400
so there's a um

00:22:35,120 --> 00:22:42,400
oh looks like my uh got a little bit

00:22:38,400 --> 00:22:42,400
trunk so click house is very focused

00:22:43,120 --> 00:22:46,480
definitely queryable there's no you

00:22:45,200 --> 00:22:48,159
don't have to stick stuff in and then

00:22:46,480 --> 00:22:50,159
wait a while for it to be

00:22:48,159 --> 00:22:52,080
to be available what click house does is

00:22:50,159 --> 00:22:54,080
when you insert data

00:22:52,080 --> 00:22:56,960
it you typically do it in blocks it's

00:22:54,080 --> 00:22:58,640
common to do you know say ten thousand

00:22:56,960 --> 00:23:00,640
fifty thousand a hundred thousand rows

00:22:58,640 --> 00:23:02,159
in one block and what clickcast will do

00:23:00,640 --> 00:23:05,280
is it will insert it

00:23:02,159 --> 00:23:06,400
and create a part and so when your

00:23:05,280 --> 00:23:08,400
insert comes back

00:23:06,400 --> 00:23:09,840
your part has been added to the table

00:23:08,400 --> 00:23:11,919
it's now queryable

00:23:09,840 --> 00:23:13,919
and this is optimized to be pretty quick

00:23:11,919 --> 00:23:14,799
you can the the ingest rates are very

00:23:13,919 --> 00:23:17,039
very high

00:23:14,799 --> 00:23:18,799
because what we're doing is creating

00:23:17,039 --> 00:23:20,960
this part and you can describe this as

00:23:18,799 --> 00:23:22,880
kind of a fast but half-hearted

00:23:20,960 --> 00:23:24,240
organization because the part might not

00:23:22,880 --> 00:23:26,720
be very big

00:23:24,240 --> 00:23:28,080
um you know and and so when we're

00:23:26,720 --> 00:23:29,760
actually doing the queries we might have

00:23:28,080 --> 00:23:31,919
to read a lot of parts

00:23:29,760 --> 00:23:32,880
if we just took the data as it was

00:23:31,919 --> 00:23:34,880
inserted

00:23:32,880 --> 00:23:36,159
well click house takes care of that by

00:23:34,880 --> 00:23:38,880
doing what are called background

00:23:36,159 --> 00:23:40,320
merges and that's where the name of this

00:23:38,880 --> 00:23:42,320
table type comes

00:23:40,320 --> 00:23:44,080
is that what it'll do is look at the

00:23:42,320 --> 00:23:46,480
parts and over time

00:23:44,080 --> 00:23:47,919
small parts will be coalesced atomically

00:23:46,480 --> 00:23:49,919
into larger parts

00:23:47,919 --> 00:23:51,760
so that what will happen is your your

00:23:49,919 --> 00:23:53,279
queries will run much faster

00:23:51,760 --> 00:23:55,279
and the difference of you know sort of

00:23:53,279 --> 00:23:57,039
when you aggregate these parts together

00:23:55,279 --> 00:23:59,120
it can make order and like an order of

00:23:57,039 --> 00:24:00,559
magnitude difference in your performance

00:23:59,120 --> 00:24:03,039
but it's it's something that happens

00:24:00,559 --> 00:24:05,200
fairly quickly and so your initial data

00:24:03,039 --> 00:24:06,640
is not organized as optimally as it can

00:24:05,200 --> 00:24:09,360
be but then it quickly

00:24:06,640 --> 00:24:09,679
is merged over time you know merged into

00:24:09,360 --> 00:24:12,400
this

00:24:09,679 --> 00:24:14,000
this more efficient structure so that's

00:24:12,400 --> 00:24:15,679
once again where the merge tree comes

00:24:14,000 --> 00:24:18,960
from and this is really fundamental for

00:24:15,679 --> 00:24:21,440
getting high performance

00:24:18,960 --> 00:24:22,400
so the um and speaking of performance

00:24:21,440 --> 00:24:24,400
the first thing you can

00:24:22,400 --> 00:24:25,520
do when you're trying to make things

00:24:24,400 --> 00:24:28,640
fast with click house

00:24:25,520 --> 00:24:30,640
is just add cpus it's very good about

00:24:28,640 --> 00:24:33,360
everything is by definition parallel

00:24:30,640 --> 00:24:35,360
by default it'll use uh it'll it'll grab

00:24:33,360 --> 00:24:39,200
half the course that it can see

00:24:35,360 --> 00:24:41,679
so anything that's in uh proc cpu

00:24:39,200 --> 00:24:42,559
the but there's also a setting called

00:24:41,679 --> 00:24:44,880
max threads

00:24:42,559 --> 00:24:45,600
and this is just a simple example of a

00:24:44,880 --> 00:24:47,760
query

00:24:45,600 --> 00:24:49,120
on this flight data where i've set the

00:24:47,760 --> 00:24:50,799
max threads

00:24:49,120 --> 00:24:52,480
and these are actually hardware threads

00:24:50,799 --> 00:24:54,559
as it turns out to be

00:24:52,480 --> 00:24:56,159
two four six and eight and then just

00:24:54,559 --> 00:24:57,520
check the query response and what you

00:24:56,159 --> 00:24:59,520
can see is with two

00:24:57,520 --> 00:25:02,000
when you go from two to four threads it

00:24:59,520 --> 00:25:03,919
pretty much cuts your response in half

00:25:02,000 --> 00:25:05,760
adding more threads doesn't really help

00:25:03,919 --> 00:25:07,200
because in this particular case

00:25:05,760 --> 00:25:09,360
you get you're seeing some amdahl

00:25:07,200 --> 00:25:10,080
effects that we're not just scanning the

00:25:09,360 --> 00:25:12,240
data

00:25:10,080 --> 00:25:13,760
but we're also doing some aggregation

00:25:12,240 --> 00:25:14,000
that has to be done at the end where we

00:25:13,760 --> 00:25:17,440
have

00:25:14,000 --> 00:25:18,960
to do that in a in a single thread

00:25:17,440 --> 00:25:20,720
but you can basically control the

00:25:18,960 --> 00:25:22,240
performance very well using

00:25:20,720 --> 00:25:23,440
this and this is the first thing i do

00:25:22,240 --> 00:25:24,320
when when i got something i want to make

00:25:23,440 --> 00:25:26,960
it run faster

00:25:24,320 --> 00:25:29,360
just add more cpus and click house will

00:25:26,960 --> 00:25:30,880
use them efficiently

00:25:29,360 --> 00:25:33,360
the other thing though that you can do

00:25:30,880 --> 00:25:36,159
and this is where you really get the big

00:25:33,360 --> 00:25:37,279
the you know huge impact is to minimize

00:25:36,159 --> 00:25:39,679
i o

00:25:37,279 --> 00:25:41,679
when you know when you're trying to make

00:25:39,679 --> 00:25:43,039
databases fast the less they have to go

00:25:41,679 --> 00:25:44,880
to storage

00:25:43,039 --> 00:25:46,880
the faster they go and click house is

00:25:44,880 --> 00:25:48,159
actually quite efficient about this so

00:25:46,880 --> 00:25:50,240
for example

00:25:48,159 --> 00:25:51,840
this is an example of a query on the

00:25:50,240 --> 00:25:53,679
left again on flight data

00:25:51,840 --> 00:25:55,120
where i'm looking at canceled and delays

00:25:53,679 --> 00:25:56,559
delayed flights

00:25:55,120 --> 00:25:58,720
and what i've done oh this is

00:25:56,559 --> 00:26:00,720
interesting um

00:25:58,720 --> 00:26:03,440
so part of the a little bit of the data

00:26:00,720 --> 00:26:05,760
is missing in this picture

00:26:03,440 --> 00:26:06,559
what we can see in the query responses

00:26:05,760 --> 00:26:09,120
is that if i put

00:26:06,559 --> 00:26:10,080
no filter on this query and have to read

00:26:09,120 --> 00:26:12,480
everything

00:26:10,080 --> 00:26:13,200
then i end up basically reading all the

00:26:12,480 --> 00:26:15,760
parts

00:26:13,200 --> 00:26:17,440
and all the chunks of data the marks we

00:26:15,760 --> 00:26:19,120
call them

00:26:17,440 --> 00:26:20,880
in the table so i get a certain query

00:26:19,120 --> 00:26:22,000
response and that's the the no filter

00:26:20,880 --> 00:26:26,320
part of the graph

00:26:22,000 --> 00:26:27,360
if i um you know if i only restrict it

00:26:26,320 --> 00:26:28,960
to one year

00:26:27,360 --> 00:26:30,640
i'm going to read less data and if i

00:26:28,960 --> 00:26:32,400
restrict it to 40 days

00:26:30,640 --> 00:26:34,000
i'm going to read even less data and

00:26:32,400 --> 00:26:36,400
what this graph is showing

00:26:34,000 --> 00:26:37,039
is that the query performance and the

00:26:36,400 --> 00:26:39,279
number of

00:26:37,039 --> 00:26:41,200
marks that click house thinks it has to

00:26:39,279 --> 00:26:43,039
read are pretty much

00:26:41,200 --> 00:26:44,320
pretty much tracked linearly in this

00:26:43,039 --> 00:26:45,760
case and

00:26:44,320 --> 00:26:48,240
what's missing from this is there's a

00:26:45,760 --> 00:26:49,760
nice uh picture here that actually

00:26:48,240 --> 00:26:50,159
showed that which you may not be able to

00:26:49,760 --> 00:26:52,480
see

00:26:50,159 --> 00:26:54,080
it showed click house demonstrating that

00:26:52,480 --> 00:26:55,279
it knew how many marks it was reading

00:26:54,080 --> 00:26:58,080
and that's actually how it collected

00:26:55,279 --> 00:27:00,400
this data so when we're looking at

00:26:58,080 --> 00:27:01,360
it at optimizing click house performance

00:27:00,400 --> 00:27:03,520
we're going to do

00:27:01,360 --> 00:27:05,120
things to reduce we're always going to

00:27:03,520 --> 00:27:06,799
be focused on reducing the amount of

00:27:05,120 --> 00:27:09,360
data we read

00:27:06,799 --> 00:27:10,960
how do we do that well a good way is to

00:27:09,360 --> 00:27:14,320
improve compression

00:27:10,960 --> 00:27:16,320
so okay

00:27:14,320 --> 00:27:19,279
once again we're missing some data here

00:27:16,320 --> 00:27:20,480
i apologize for this it looks like

00:27:19,279 --> 00:27:22,399
it looks like the way that this is

00:27:20,480 --> 00:27:23,919
showing up let me explain what you

00:27:22,399 --> 00:27:27,039
should have seen here

00:27:23,919 --> 00:27:29,760
is um we're adding codecs

00:27:27,039 --> 00:27:31,360
so click house says lz4 and zsdd

00:27:29,760 --> 00:27:32,640
compression you can choose them

00:27:31,360 --> 00:27:34,240
generally

00:27:32,640 --> 00:27:36,640
these compression algorithms have no

00:27:34,240 --> 00:27:37,520
knowledge of the data so lz4 which is

00:27:36,640 --> 00:27:39,039
the default

00:27:37,520 --> 00:27:41,039
if click house can see data it's going

00:27:39,039 --> 00:27:42,799
to try and compress it with lz4

00:27:41,039 --> 00:27:44,799
if if you don't tell it anything what

00:27:42,799 --> 00:27:45,120
you can do though is you can add codecs

00:27:44,799 --> 00:27:48,399
which

00:27:45,120 --> 00:27:48,880
are transformations on the data that

00:27:48,399 --> 00:27:52,159
would

00:27:48,880 --> 00:27:53,279
um you know that actually you know

00:27:52,159 --> 00:27:56,640
change the

00:27:53,279 --> 00:27:58,720
change the values so and they are type

00:27:56,640 --> 00:28:00,799
specific so for example

00:27:58,720 --> 00:28:02,480
one of the things that i can do and that

00:28:00,799 --> 00:28:04,399
was shown in this a

00:28:02,480 --> 00:28:06,159
under bar lc is i can use dictionary

00:28:04,399 --> 00:28:07,919
encodings so if i have a bunch of

00:28:06,159 --> 00:28:10,240
strings like airport names

00:28:07,919 --> 00:28:11,440
i can actually apply this this

00:28:10,240 --> 00:28:13,360
transformation

00:28:11,440 --> 00:28:14,960
and instead of storing the airport name

00:28:13,360 --> 00:28:17,200
it will just store an integer

00:28:14,960 --> 00:28:18,960
which vastly reduces the amount of disk

00:28:17,200 --> 00:28:21,039
that's that's inside

00:28:18,960 --> 00:28:22,960
let's go ahead and look at the graph so

00:28:21,039 --> 00:28:24,960
what you see is this

00:28:22,960 --> 00:28:27,120
actually a dramatic effect on storage

00:28:24,960 --> 00:28:29,120
size so

00:28:27,120 --> 00:28:31,039
the low cardinality encoding that's what

00:28:29,120 --> 00:28:34,640
i was describing there

00:28:31,039 --> 00:28:37,919
will reduce the data significantly

00:28:34,640 --> 00:28:40,000
so and basically the the benefit here

00:28:37,919 --> 00:28:42,240
is that it it compress it reduces the

00:28:40,000 --> 00:28:42,720
data size before we even try to compress

00:28:42,240 --> 00:28:45,200
it

00:28:42,720 --> 00:28:46,480
so we end up once it's fully compressed

00:28:45,200 --> 00:28:48,640
with 89

00:28:46,480 --> 00:28:50,080
compression rate actually this was using

00:28:48,640 --> 00:28:52,559
lz4 if you go with

00:28:50,080 --> 00:28:54,720
zsd compression you can get it down even

00:28:52,559 --> 00:28:55,919
lower you can get like 93

00:28:54,720 --> 00:28:57,840
and similarly we have a bunch of

00:28:55,919 --> 00:28:58,559
different numeric encodings that you can

00:28:57,840 --> 00:29:00,880
apply like

00:28:58,559 --> 00:29:02,640
delta encoding what's the difference you

00:29:00,880 --> 00:29:04,159
know where basically all we do is store

00:29:02,640 --> 00:29:05,520
the difference between

00:29:04,159 --> 00:29:07,440
numbers works great if they're

00:29:05,520 --> 00:29:09,600
increasing

00:29:07,440 --> 00:29:11,200
or we can do double delta we store the

00:29:09,600 --> 00:29:13,120
difference between the numbers

00:29:11,200 --> 00:29:14,480
actually the difference in the change in

00:29:13,120 --> 00:29:16,080
the numbers so if they're slowly

00:29:14,480 --> 00:29:18,720
increasing this is optimal

00:29:16,080 --> 00:29:19,600
and the actual results you get in terms

00:29:18,720 --> 00:29:22,640
of storage

00:29:19,600 --> 00:29:26,000
it makes a huge difference

00:29:22,640 --> 00:29:28,000
so um so so basically

00:29:26,000 --> 00:29:29,200
this is one of the key tools another

00:29:28,000 --> 00:29:32,559
tool that we use

00:29:29,200 --> 00:29:34,480
is to create materialized views and once

00:29:32,559 --> 00:29:36,799
again apologies we have some

00:29:34,480 --> 00:29:38,720
stuff missing from this the slides will

00:29:36,799 --> 00:29:40,159
will show it but the basic idea with a

00:29:38,720 --> 00:29:42,159
materialized view

00:29:40,159 --> 00:29:43,760
is we're going to just reduce the amount

00:29:42,159 --> 00:29:45,840
of data that we read

00:29:43,760 --> 00:29:48,000
by applying a transformation on the

00:29:45,840 --> 00:29:50,640
source and putting the produced

00:29:48,000 --> 00:29:51,279
data in another location this is an

00:29:50,640 --> 00:29:54,399
example

00:29:51,279 --> 00:29:58,080
of a materialized view that the

00:29:54,399 --> 00:30:01,279
um where the cpu where we're basically

00:29:58,080 --> 00:30:04,480
asking for the last value of of

00:30:01,279 --> 00:30:05,440
cpu usage on a bunch of measurements on

00:30:04,480 --> 00:30:07,840
cpus

00:30:05,440 --> 00:30:08,960
so it basically allows us to take a

00:30:07,840 --> 00:30:10,399
table instead of

00:30:08,960 --> 00:30:12,799
scanning the whole table to find out

00:30:10,399 --> 00:30:15,279
what the last value was for each cpu

00:30:12,799 --> 00:30:16,399
we can collect that information in the

00:30:15,279 --> 00:30:20,000
in the

00:30:16,399 --> 00:30:22,720
in the materialized view and basically

00:30:20,000 --> 00:30:24,799
we get an effective compression ratio

00:30:22,720 --> 00:30:26,880
that's that's enormous we end up with

00:30:24,799 --> 00:30:28,240
far less than one percent of the data

00:30:26,880 --> 00:30:30,320
and as a result

00:30:28,240 --> 00:30:32,559
the queries are orders of magnitudes

00:30:30,320 --> 00:30:35,440
faster

00:30:32,559 --> 00:30:37,840
a very common pattern here is to use

00:30:35,440 --> 00:30:40,960
this for

00:30:37,840 --> 00:30:42,399
you know to to aggregate data so for

00:30:40,960 --> 00:30:43,600
example you have a website you're doing

00:30:42,399 --> 00:30:45,760
web analytics

00:30:43,600 --> 00:30:47,200
you want to track hourly unique visits

00:30:45,760 --> 00:30:48,880
hourly sessions

00:30:47,200 --> 00:30:50,960
you keep those long term and then

00:30:48,880 --> 00:30:54,480
there's a

00:30:50,960 --> 00:30:56,480
a a feature in the you know in the table

00:30:54,480 --> 00:30:58,080
definition where you can add a ttl

00:30:56,480 --> 00:31:00,480
click house supports this it's

00:30:58,080 --> 00:31:02,799
unfortunately not showing on this slide

00:31:00,480 --> 00:31:04,480
but basically it deletes the data after

00:31:02,799 --> 00:31:06,159
seven days in this example

00:31:04,480 --> 00:31:07,600
so you keep the source data for seven

00:31:06,159 --> 00:31:09,120
days that's your detail but then you

00:31:07,600 --> 00:31:11,760
keep the aggregates forever

00:31:09,120 --> 00:31:13,039
so this is really efficient way of using

00:31:11,760 --> 00:31:16,080
storage

00:31:13,039 --> 00:31:17,600
and then what we can also do is um

00:31:16,080 --> 00:31:20,159
click house has a feature called tiered

00:31:17,600 --> 00:31:23,519
storage and what this allows us to do

00:31:20,159 --> 00:31:26,159
is within that source table we can also

00:31:23,519 --> 00:31:28,240
have different qualities of different

00:31:26,159 --> 00:31:31,679
types of storage so for example

00:31:28,240 --> 00:31:34,320
we can use nvme ssd for the for the data

00:31:31,679 --> 00:31:34,960
that is has just arrived we can put a

00:31:34,320 --> 00:31:37,200
ttl

00:31:34,960 --> 00:31:39,440
on the table that will then say hey move

00:31:37,200 --> 00:31:41,919
it from my hot storage because nvme is

00:31:39,440 --> 00:31:43,440
really fast suitable for the you know

00:31:41,919 --> 00:31:45,360
the small percentage of queries that

00:31:43,440 --> 00:31:47,039
you're looking at most commonly

00:31:45,360 --> 00:31:48,720
after a certain period of time like two

00:31:47,039 --> 00:31:52,000
days move it off

00:31:48,720 --> 00:31:54,880
to um to a hard disk

00:31:52,000 --> 00:31:55,440
and and we can also group those uh you

00:31:54,880 --> 00:31:56,880
can

00:31:55,440 --> 00:31:58,399
you know you can rate them yourselves

00:31:56,880 --> 00:31:58,960
click house also understands raid

00:31:58,399 --> 00:32:02,080
patterns

00:31:58,960 --> 00:32:03,600
and can do it itself so this is a really

00:32:02,080 --> 00:32:05,360
an increasingly common pattern and

00:32:03,600 --> 00:32:07,519
actually something that we worked on

00:32:05,360 --> 00:32:10,320
uh quite a bit over the last year or so

00:32:07,519 --> 00:32:14,159
to implement

00:32:10,320 --> 00:32:17,200
beyond single click house servers we can

00:32:14,159 --> 00:32:19,120
of course cluster them and

00:32:17,200 --> 00:32:21,519
this gives you the horizontal scaling

00:32:19,120 --> 00:32:24,720
basically sharding is built in

00:32:21,519 --> 00:32:27,679
so you and a shard is a portion

00:32:24,720 --> 00:32:29,600
of data that um like you can think of

00:32:27,679 --> 00:32:30,480
the data being divided up into disjoint

00:32:29,600 --> 00:32:32,720
sets like

00:32:30,480 --> 00:32:34,799
you know groups of tenants for example

00:32:32,720 --> 00:32:36,399
um you can also replicate it

00:32:34,799 --> 00:32:38,399
that's that's also built in there's

00:32:36,399 --> 00:32:41,200
multi-master replication

00:32:38,399 --> 00:32:42,720
and what those what that does is allows

00:32:41,200 --> 00:32:44,399
you to get more concurrency because

00:32:42,720 --> 00:32:45,600
there's more copies of data that you can

00:32:44,399 --> 00:32:50,480
use

00:32:45,600 --> 00:32:50,480
so as we look at this we see the um

00:32:51,440 --> 00:32:55,519
you know and how this is actually

00:32:53,840 --> 00:32:56,720
implemented what click house uses more

00:32:55,519 --> 00:32:58,320
table engines

00:32:56,720 --> 00:33:00,720
so for example we have a distributed

00:32:58,320 --> 00:33:04,000
table engine that understands

00:33:00,720 --> 00:33:06,000
how to uh take the parts so the t

00:33:04,000 --> 00:33:07,679
or excuse me that a bunch of replicas

00:33:06,000 --> 00:33:09,200
divided up into shards

00:33:07,679 --> 00:33:11,120
and it knows that when it receives a

00:33:09,200 --> 00:33:12,080
query it should pick one replica from

00:33:11,120 --> 00:33:14,399
each shard

00:33:12,080 --> 00:33:16,640
and then break the bring the data back

00:33:14,399 --> 00:33:18,559
together

00:33:16,640 --> 00:33:19,760
when we do when we set these clusters up

00:33:18,559 --> 00:33:21,840
one thing that clicks

00:33:19,760 --> 00:33:23,200
we have to add to the architecture is

00:33:21,840 --> 00:33:24,640
zookeeper

00:33:23,200 --> 00:33:26,799
that's something that we are looking at

00:33:24,640 --> 00:33:27,039
how that can be can be eased but that's

00:33:26,799 --> 00:33:30,159
an

00:33:27,039 --> 00:33:32,399
extra piece that is added to the system

00:33:30,159 --> 00:33:35,039
because you need to have consensus

00:33:32,399 --> 00:33:37,360
about which parts have which replicas

00:33:35,039 --> 00:33:40,000
and how they move back and forth

00:33:37,360 --> 00:33:40,720
so what this what the distributed table

00:33:40,000 --> 00:33:43,519
does

00:33:40,720 --> 00:33:44,960
is it then allows you what you typically

00:33:43,519 --> 00:33:46,320
do the common patterns you have the

00:33:44,960 --> 00:33:49,200
distributed table

00:33:46,320 --> 00:33:51,600
on every uh on every node an application

00:33:49,200 --> 00:33:53,679
can connect to any node

00:33:51,600 --> 00:33:54,960
run the query against the table and then

00:33:53,679 --> 00:33:57,760
the distributed table

00:33:54,960 --> 00:33:58,480
automatically distributes the queries

00:33:57,760 --> 00:34:00,960
down to

00:33:58,480 --> 00:34:02,880
uh down to the local copies so and

00:34:00,960 --> 00:34:04,320
there's various options that you can use

00:34:02,880 --> 00:34:07,360
to make this more efficient

00:34:04,320 --> 00:34:09,040
but the basic idea is to do a push down

00:34:07,360 --> 00:34:11,040
where you get as much of the query as

00:34:09,040 --> 00:34:14,320
possible to run on the local select

00:34:11,040 --> 00:34:16,639
or the local local data and then the

00:34:14,320 --> 00:34:18,240
and compute aggregates locally and then

00:34:16,639 --> 00:34:20,000
they come back to the initiator which

00:34:18,240 --> 00:34:21,760
merges them together and hands them back

00:34:20,000 --> 00:34:24,720
to the application

00:34:21,760 --> 00:34:25,599
so this is is a really powerful feature

00:34:24,720 --> 00:34:28,639
and

00:34:25,599 --> 00:34:31,839
for good queries so well-behaved queries

00:34:28,639 --> 00:34:33,760
and and when you set things up properly

00:34:31,839 --> 00:34:35,200
you can actually get linear improvements

00:34:33,760 --> 00:34:37,599
by adding

00:34:35,200 --> 00:34:38,560
additional nodes and this graph shows a

00:34:37,599 --> 00:34:41,440
couple of runs

00:34:38,560 --> 00:34:42,159
on on this airline data in a case where

00:34:41,440 --> 00:34:45,440
we're just

00:34:42,159 --> 00:34:46,960
doing a pretty expensive query the cold

00:34:45,440 --> 00:34:49,119
data and the hot data

00:34:46,960 --> 00:34:50,399
are blue and red respectively so that's

00:34:49,119 --> 00:34:52,560
you know with caches and

00:34:50,399 --> 00:34:54,159
caching enabled and without but in both

00:34:52,560 --> 00:34:55,200
cases you get essentially linear

00:34:54,159 --> 00:34:57,280
performance with

00:34:55,200 --> 00:34:58,400
by adding nodes so it's a really

00:34:57,280 --> 00:35:00,160
powerful feature

00:34:58,400 --> 00:35:02,480
and what this means is that for

00:35:00,160 --> 00:35:06,240
extremely large data sets we can

00:35:02,480 --> 00:35:09,520
for example split them up over 50 nodes

00:35:06,240 --> 00:35:10,800
for example and and thereby get vastly

00:35:09,520 --> 00:35:13,920
better performance than we would if

00:35:10,800 --> 00:35:17,280
we're running a single node

00:35:13,920 --> 00:35:18,560
so that's click house internals i'm

00:35:17,280 --> 00:35:20,240
going to just talk about a couple of

00:35:18,560 --> 00:35:23,359
patterns of use

00:35:20,240 --> 00:35:23,760
to kind of close things off and then we

00:35:23,359 --> 00:35:27,440
can

00:35:23,760 --> 00:35:29,599
then we can take some questions so

00:35:27,440 --> 00:35:31,920
a really common way that click house is

00:35:29,599 --> 00:35:34,400
used is together with kafka

00:35:31,920 --> 00:35:36,720
so click house can ingest data very fast

00:35:34,400 --> 00:35:37,920
if you go look at published

00:35:36,720 --> 00:35:40,240
published articles for example

00:35:37,920 --> 00:35:41,440
cloudflare wrote a really widely read

00:35:40,240 --> 00:35:43,760
blog

00:35:41,440 --> 00:35:45,680
a post about about using a clickhouse

00:35:43,760 --> 00:35:47,440
and they talked about how they get

00:35:45,680 --> 00:35:49,200
what what is now about 10 million events

00:35:47,440 --> 00:35:51,040
per second coming into their cluster for

00:35:49,200 --> 00:35:54,400
all their web analytics dns

00:35:51,040 --> 00:35:54,960
and service logs um they they use kafka

00:35:54,400 --> 00:35:58,160
to

00:35:54,960 --> 00:35:58,880
drive this because kafka is able to like

00:35:58,160 --> 00:36:01,839
click house

00:35:58,880 --> 00:36:03,040
can scale horizontally it allows you to

00:36:01,839 --> 00:36:05,440
have very high

00:36:03,040 --> 00:36:06,079
to basically share enormous amounts of

00:36:05,440 --> 00:36:09,280
data

00:36:06,079 --> 00:36:11,359
and ingest them quickly and to collect

00:36:09,280 --> 00:36:14,560
them and ingest them quickly

00:36:11,359 --> 00:36:16,480
so and a pretty common pattern is

00:36:14,560 --> 00:36:18,000
to just write your own uh consumer so

00:36:16,480 --> 00:36:20,560
you write a you know write some little

00:36:18,000 --> 00:36:22,800
go programs that read out of the

00:36:20,560 --> 00:36:24,320
you know read off the topics turn around

00:36:22,800 --> 00:36:26,640
and hand it to click house

00:36:24,320 --> 00:36:28,079
but click house also has a cool feature

00:36:26,640 --> 00:36:30,640
which is

00:36:28,079 --> 00:36:32,640
there's a table engine that makes kafka

00:36:30,640 --> 00:36:36,240
topics look like a table

00:36:32,640 --> 00:36:38,960
and and this this is actually kind of a

00:36:36,240 --> 00:36:40,400
cool example of the creativity that

00:36:38,960 --> 00:36:43,119
buying both table engines

00:36:40,400 --> 00:36:43,760
and materialized views so the table

00:36:43,119 --> 00:36:45,760
engine

00:36:43,760 --> 00:36:47,440
encapsulates the topic and it looks like

00:36:45,760 --> 00:36:49,040
a table

00:36:47,440 --> 00:36:51,280
and you can select off it and what that

00:36:49,040 --> 00:36:52,800
will do is remove whatever

00:36:51,280 --> 00:36:54,320
you know it'll basically read it from

00:36:52,800 --> 00:36:55,760
the topic

00:36:54,320 --> 00:36:57,680
well you don't want to do that manually

00:36:55,760 --> 00:36:59,680
so what you can do is actually have a

00:36:57,680 --> 00:37:01,839
materialized view

00:36:59,680 --> 00:37:03,599
which is constructed on the table it

00:37:01,839 --> 00:37:07,359
automatically selects

00:37:03,599 --> 00:37:09,520
and then puts the it puts the

00:37:07,359 --> 00:37:11,280
values in a different location in this

00:37:09,520 --> 00:37:13,280
case in this example

00:37:11,280 --> 00:37:14,720
into a merged tree table and so

00:37:13,280 --> 00:37:16,880
basically what that does

00:37:14,720 --> 00:37:18,160
is gives you an automated transfer out

00:37:16,880 --> 00:37:20,560
of the topic

00:37:18,160 --> 00:37:21,440
and into the merge tree table so this is

00:37:20,560 --> 00:37:24,160
is

00:37:21,440 --> 00:37:25,280
is also another way of of integrating

00:37:24,160 --> 00:37:27,280
with kafka

00:37:25,280 --> 00:37:29,280
and i would say actually overall

00:37:27,280 --> 00:37:30,079
probably fifty percent of all people

00:37:29,280 --> 00:37:33,440
that use

00:37:30,079 --> 00:37:35,839
uh click house are are integrated

00:37:33,440 --> 00:37:38,640
they are also using kafka as a way of

00:37:35,839 --> 00:37:38,640
ingesting data

00:37:39,280 --> 00:37:45,119
um another pattern is visualization

00:37:42,320 --> 00:37:46,400
using grafana so the click house

00:37:45,119 --> 00:37:49,280
integration with with the

00:37:46,400 --> 00:37:50,400
front is quite good there's a grafana a

00:37:49,280 --> 00:37:53,520
plug-in that

00:37:50,400 --> 00:37:54,960
actually we maintain and i play around

00:37:53,520 --> 00:37:56,240
with constantly

00:37:54,960 --> 00:37:58,800
what we're looking at here is actually

00:37:56,240 --> 00:38:00,720
some of our

00:37:58,800 --> 00:38:02,640
data from our amazon cost billing which

00:38:00,720 --> 00:38:04,400
we of course stick it to click house

00:38:02,640 --> 00:38:07,119
but we serve it up in grafana and so

00:38:04,400 --> 00:38:10,400
grafana is written in typescript

00:38:07,119 --> 00:38:12,400
there's a clickhouse plugin which uses

00:38:10,400 --> 00:38:14,960
one of two major interfaces and click

00:38:12,400 --> 00:38:17,760
house it's an http interface it can just

00:38:14,960 --> 00:38:19,520
do gets or posts to run the queries

00:38:17,760 --> 00:38:22,880
there's also a wide number

00:38:19,520 --> 00:38:26,960
of other client types uh

00:38:22,880 --> 00:38:29,280
python go lag c plus plus

00:38:26,960 --> 00:38:30,320
and then of course javascript java curl

00:38:29,280 --> 00:38:32,560
things like that

00:38:30,320 --> 00:38:34,000
so and there's also um i i should

00:38:32,560 --> 00:38:35,920
mention there's a really great

00:38:34,000 --> 00:38:37,440
uh command line client it's called click

00:38:35,920 --> 00:38:38,560
house client it's super good it's like

00:38:37,440 --> 00:38:41,599
my sql or

00:38:38,560 --> 00:38:43,119
or p sql if you're uh familiar with uh

00:38:41,599 --> 00:38:45,200
my sql postgres

00:38:43,119 --> 00:38:47,280
really handy to use and easy to load

00:38:45,200 --> 00:38:49,920
data

00:38:47,280 --> 00:38:50,720
so there's one final use pattern that i

00:38:49,920 --> 00:38:52,720
think is

00:38:50,720 --> 00:38:54,800
interesting to talk about and that's

00:38:52,720 --> 00:38:58,560
running on kubernetes

00:38:54,800 --> 00:39:00,000
so click house is not cloud native

00:38:58,560 --> 00:39:02,160
exactly but

00:39:00,000 --> 00:39:03,920
it's fair to call it cloud friendly and

00:39:02,160 --> 00:39:07,200
the reason it's cloud friendly

00:39:03,920 --> 00:39:10,160
is it's just a single it

00:39:07,200 --> 00:39:11,040
it is a single process so it's easy to

00:39:10,160 --> 00:39:13,599
put it runs

00:39:11,040 --> 00:39:14,320
really well inside docker or you know

00:39:13,599 --> 00:39:15,839
whatever

00:39:14,320 --> 00:39:18,079
you know whatever container technology

00:39:15,839 --> 00:39:19,920
you're using and it has a relatively

00:39:18,079 --> 00:39:23,280
simple relationship with storage

00:39:19,920 --> 00:39:24,560
so uh what what one of the things that

00:39:23,280 --> 00:39:26,800
we've worked on for

00:39:24,560 --> 00:39:28,400
actually now about a year and a half is

00:39:26,800 --> 00:39:31,040
building an operator

00:39:28,400 --> 00:39:33,119
for uh for kubernetes that allows you to

00:39:31,040 --> 00:39:36,079
set up clusters on kubernetes

00:39:33,119 --> 00:39:37,839
and this this is a very common pattern

00:39:36,079 --> 00:39:40,240
just about every other major database

00:39:37,839 --> 00:39:42,240
particularly the open source world has a

00:39:40,240 --> 00:39:44,640
has an operator for kubernetes

00:39:42,240 --> 00:39:46,240
the click house operator allows you to

00:39:44,640 --> 00:39:48,000
define your cluster

00:39:46,240 --> 00:39:49,440
in a single file it's called a custom

00:39:48,000 --> 00:39:51,520
resource definition

00:39:49,440 --> 00:39:52,560
you feed it to the operator and then

00:39:51,520 --> 00:39:54,480
what it will do

00:39:52,560 --> 00:39:57,760
is go look at that and say hey you need

00:39:54,480 --> 00:39:59,280
a cluster that contains in this case

00:39:57,760 --> 00:40:02,320
you know like one shard and three

00:39:59,280 --> 00:40:07,200
replicas it'll go allocate

00:40:02,320 --> 00:40:09,280
the storage and start containers

00:40:07,200 --> 00:40:11,280
to access it and then put a nice load

00:40:09,280 --> 00:40:13,359
balancing service in front of it

00:40:11,280 --> 00:40:14,640
so this is an increasingly common

00:40:13,359 --> 00:40:15,920
pattern we

00:40:14,640 --> 00:40:18,640
we have a number of customers and

00:40:15,920 --> 00:40:20,640
there's many people outside our customer

00:40:18,640 --> 00:40:22,480
base that are using this

00:40:20,640 --> 00:40:24,000
and the cool thing about it is it

00:40:22,480 --> 00:40:27,599
basically allows you to have a

00:40:24,000 --> 00:40:29,680
lot of data warehouses because you could

00:40:27,599 --> 00:40:30,560
spin them up and blow them away pretty

00:40:29,680 --> 00:40:32,960
quickly on

00:40:30,560 --> 00:40:34,880
on kubernetes so it means each service

00:40:32,960 --> 00:40:36,880
can actually now have its own

00:40:34,880 --> 00:40:38,319
data warehouse so that's a really

00:40:36,880 --> 00:40:40,160
exciting development and i think

00:40:38,319 --> 00:40:43,440
something very different from

00:40:40,160 --> 00:40:44,480
uh from what we see in the you know

00:40:43,440 --> 00:40:48,000
across the

00:40:44,480 --> 00:40:51,119
you know in the proprietary databases

00:40:48,000 --> 00:40:54,160
so just as a wrap up um

00:40:51,119 --> 00:40:54,640
click house is you know sort of first

00:40:54,160 --> 00:40:57,200
off it

00:40:54,640 --> 00:40:58,400
it's really the first data warehouse sql

00:40:57,200 --> 00:41:01,280
data warehouse that

00:40:58,400 --> 00:41:02,880
meets or beats the proprietary offerings

00:41:01,280 --> 00:41:04,240
and head-to-head comparisons and the

00:41:02,880 --> 00:41:06,000
places where it shines

00:41:04,240 --> 00:41:08,000
as i said if you're looking for a

00:41:06,000 --> 00:41:10,319
complete sql implementation

00:41:08,000 --> 00:41:11,040
probably not the first stop it's better

00:41:10,319 --> 00:41:13,040
to go

00:41:11,040 --> 00:41:15,040
pay vertica the money but if you're

00:41:13,040 --> 00:41:18,000
looking for speed and cost efficiency

00:41:15,040 --> 00:41:18,960
it definitely it definitely fits the

00:41:18,000 --> 00:41:20,560
bill and that's why

00:41:18,960 --> 00:41:22,079
most people are using it for one of

00:41:20,560 --> 00:41:24,079
those two reasons

00:41:22,079 --> 00:41:25,440
the second is the feature coverage is

00:41:24,079 --> 00:41:27,119
expanding rapidly

00:41:25,440 --> 00:41:28,720
so we're working on the sql features

00:41:27,119 --> 00:41:30,880
like a big thing that's going in is

00:41:28,720 --> 00:41:32,400
role-based access control complete

00:41:30,880 --> 00:41:35,119
implementation matches

00:41:32,400 --> 00:41:36,079
what my sql does nice things like object

00:41:35,119 --> 00:41:38,400
storage

00:41:36,079 --> 00:41:39,200
stuff like that and then i think the

00:41:38,400 --> 00:41:42,640
final thing

00:41:39,200 --> 00:41:44,560
is because of the scaling click house

00:41:42,640 --> 00:41:46,720
has this interesting property that it

00:41:44,560 --> 00:41:47,760
can deliver reliable real-time

00:41:46,720 --> 00:41:49,520
performance

00:41:47,760 --> 00:41:50,960
and there's a couple of metrics that

00:41:49,520 --> 00:41:54,480
that we see commonly

00:41:50,960 --> 00:41:58,319
one is to shoot for ad-hoc consistent

00:41:54,480 --> 00:42:00,560
at one second ad-hoc query performance

00:41:58,319 --> 00:42:02,160
and you know for those analyst queries

00:42:00,560 --> 00:42:04,160
and then for things where you're driving

00:42:02,160 --> 00:42:06,640
like you know online applications like

00:42:04,160 --> 00:42:08,240
martech applications 10 millisecond

00:42:06,640 --> 00:42:09,680
pre-defined query response

00:42:08,240 --> 00:42:12,240
there's a bunch of people using click

00:42:09,680 --> 00:42:15,760
house to get these response levels

00:42:12,240 --> 00:42:17,760
and it does this very effectively

00:42:15,760 --> 00:42:18,880
so here's some resources i'm going to

00:42:17,760 --> 00:42:21,200
pass over them

00:42:18,880 --> 00:42:22,480
great documentation a lot of good talks

00:42:21,200 --> 00:42:25,680
we do a blog that

00:42:22,480 --> 00:42:27,440
covers some of these general issues and

00:42:25,680 --> 00:42:29,200
i just want to say thank you and again

00:42:27,440 --> 00:42:31,760
once again big shout out to the

00:42:29,200 --> 00:42:32,880
linux foundation you all have done

00:42:31,760 --> 00:42:34,480
really wonderful i'm

00:42:32,880 --> 00:42:36,480
really really happy to be able to

00:42:34,480 --> 00:42:38,880
present um

00:42:36,480 --> 00:42:40,319
it wouldn't be a real startup talk if i

00:42:38,880 --> 00:42:43,599
didn't say we're hiring

00:42:40,319 --> 00:42:45,200
and and here's uh you know sort of ways

00:42:43,599 --> 00:42:47,440
you can contact us and in the middle

00:42:45,200 --> 00:42:48,960
is click house don't believe me if you

00:42:47,440 --> 00:42:50,319
haven't used it

00:42:48,960 --> 00:42:52,160
don't believe anything in this talk just

00:42:50,319 --> 00:42:52,400
go get it try it out see if it works for

00:42:52,160 --> 00:42:54,640
you

00:42:52,400 --> 00:42:56,079
i mean this is open source and and if

00:42:54,640 --> 00:42:58,160
you see something that

00:42:56,079 --> 00:42:59,440
that you don't like hey give us a pr or

00:42:58,160 --> 00:43:02,640
submit an issue

00:42:59,440 --> 00:43:04,000
uh we're really the the community is

00:43:02,640 --> 00:43:06,000
already enormous

00:43:04,000 --> 00:43:07,920
and international we love to have new

00:43:06,000 --> 00:43:10,960
people

00:43:07,920 --> 00:43:14,319
so with that i'm going to go ahead

00:43:10,960 --> 00:43:17,359
and look at some of the questions

00:43:14,319 --> 00:43:19,440
so um okay i

00:43:17,359 --> 00:43:20,480
had a question here and i'm just going

00:43:19,440 --> 00:43:24,240
to take them in

00:43:20,480 --> 00:43:25,839
and um let's see we've got

00:43:24,240 --> 00:43:27,839
oh i got a good okay i'm just gonna run

00:43:25,839 --> 00:43:30,079
these questions until i run out of time

00:43:27,839 --> 00:43:32,400
so uh data the question here are those

00:43:30,079 --> 00:43:35,440
data warehouses i showed in the uh

00:43:32,400 --> 00:43:37,680
in the initial um

00:43:35,440 --> 00:43:38,640
uh picture are they similar to sap hana

00:43:37,680 --> 00:43:40,880
uh yes and no

00:43:38,640 --> 00:43:41,920
one of the things about hana that's that

00:43:40,880 --> 00:43:44,640
is distinctive is

00:43:41,920 --> 00:43:46,560
hana actually stores stuff as rows and

00:43:44,640 --> 00:43:49,680
then goes to columns later on

00:43:46,560 --> 00:43:52,960
it's also used it makes heavy use of

00:43:49,680 --> 00:43:55,200
of in-memory databases so it is a proper

00:43:52,960 --> 00:43:57,599
data warehouse many people use

00:43:55,200 --> 00:43:58,640
use it for i'm not that familiar with

00:43:57,599 --> 00:44:01,359
the architecture

00:43:58,640 --> 00:44:02,560
but it uses the the innovations i

00:44:01,359 --> 00:44:03,760
described at the beginning of the talk

00:44:02,560 --> 00:44:06,480
they are well known

00:44:03,760 --> 00:44:08,319
in many cases are decades old so the

00:44:06,480 --> 00:44:10,079
stuff that i showed there you there

00:44:08,319 --> 00:44:11,520
you can be sure that most of it is is

00:44:10,079 --> 00:44:14,079
being used on some level like

00:44:11,520 --> 00:44:16,480
compression for example or codecs

00:44:14,079 --> 00:44:18,640
so what are your thoughts on delta like

00:44:16,480 --> 00:44:22,000
which sits on top of spark

00:44:18,640 --> 00:44:25,599
um it's complicated that's my thought

00:44:22,000 --> 00:44:28,000
this is the this is where um

00:44:25,599 --> 00:44:29,040
what and one of the one of the big

00:44:28,000 --> 00:44:32,160
reasons that people

00:44:29,040 --> 00:44:35,680
use data warehouses

00:44:32,160 --> 00:44:37,359
is that they are simpler than uh

00:44:35,680 --> 00:44:39,440
than building stuff through pipelines i

00:44:37,359 --> 00:44:41,599
think they that spark is

00:44:39,440 --> 00:44:42,480
is really really powerful and has

00:44:41,599 --> 00:44:45,119
excellent ml

00:44:42,480 --> 00:44:45,520
integration but the thing that's nice

00:44:45,119 --> 00:44:48,000
about

00:44:45,520 --> 00:44:49,119
click house is that you can connect it

00:44:48,000 --> 00:44:50,960
with kafka

00:44:49,119 --> 00:44:52,960
ingest your data and just instantly

00:44:50,960 --> 00:44:54,800
answer questions about it along

00:44:52,960 --> 00:44:56,240
with all the previous history that you

00:44:54,800 --> 00:44:58,319
had so

00:44:56,240 --> 00:45:00,800
when you're trying to get you know when

00:44:58,319 --> 00:45:02,960
you're trying to get quick answers to

00:45:00,800 --> 00:45:05,040
the big problems on top of structured

00:45:02,960 --> 00:45:06,160
data data warehouses are definitely the

00:45:05,040 --> 00:45:09,359
way to go

00:45:06,160 --> 00:45:11,040
and in fact if you want guaranteed

00:45:09,359 --> 00:45:13,680
latency i don't think there's

00:45:11,040 --> 00:45:16,000
any substitute for actually having your

00:45:13,680 --> 00:45:17,440
own storage format that organizes the

00:45:16,000 --> 00:45:19,440
data in a

00:45:17,440 --> 00:45:20,880
consistent way so that's that's kind of

00:45:19,440 --> 00:45:23,440
my basic thought

00:45:20,880 --> 00:45:24,720
um the these are alternative

00:45:23,440 --> 00:45:26,880
technologies

00:45:24,720 --> 00:45:28,720
if if you have enormous amounts of data

00:45:26,880 --> 00:45:29,359
you don't care how fast you get the

00:45:28,720 --> 00:45:31,920
answers

00:45:29,359 --> 00:45:32,720
yeah something like spark works if you

00:45:31,920 --> 00:45:35,119
have

00:45:32,720 --> 00:45:37,760
smaller amounts of data and i you know

00:45:35,119 --> 00:45:40,160
say 20 petabytes as opposed to exabytes

00:45:37,760 --> 00:45:42,800
then click houses and data warehouses in

00:45:40,160 --> 00:45:45,880
general much better approach

00:45:42,800 --> 00:45:48,979
um let's see uh

00:45:45,880 --> 00:45:48,979
[Applause]

00:45:49,839 --> 00:45:52,480
let's see there's a question given query

00:45:51,280 --> 00:45:54,400
performance the fact you immediately

00:45:52,480 --> 00:45:56,560
query that you insert is a good fit for

00:45:54,400 --> 00:45:58,720
the query portion of the cqrs

00:45:56,560 --> 00:45:59,760
pattern uh i don't know what that

00:45:58,720 --> 00:46:02,160
pattern is for

00:45:59,760 --> 00:46:03,920
interactive user experiences read from

00:46:02,160 --> 00:46:04,640
click house and transact with the system

00:46:03,920 --> 00:46:06,560
of record

00:46:04,640 --> 00:46:08,079
actually i wouldn't recommend doing that

00:46:06,560 --> 00:46:12,000
but there's kind of an interesting

00:46:08,079 --> 00:46:14,480
um i i think that what you want to do is

00:46:12,000 --> 00:46:16,079
think in terms of databases that serve a

00:46:14,480 --> 00:46:18,880
single purpose

00:46:16,079 --> 00:46:20,880
so if um you know if you're doing

00:46:18,880 --> 00:46:23,440
transaction processing

00:46:20,880 --> 00:46:24,400
uh in mysql like doing sales just have

00:46:23,440 --> 00:46:26,960
them be there

00:46:24,400 --> 00:46:28,240
but i wouldn't you wouldn't typically

00:46:26,960 --> 00:46:29,760
want to have an application

00:46:28,240 --> 00:46:31,599
do something in my sequel and then read

00:46:29,760 --> 00:46:33,280
from click house there's just too many

00:46:31,599 --> 00:46:34,640
ways it could get screwed up

00:46:33,280 --> 00:46:37,040
where click house is really going to

00:46:34,640 --> 00:46:38,160
help you as if is if you have a data

00:46:37,040 --> 00:46:41,119
source that's

00:46:38,160 --> 00:46:42,319
like you know behavioral data from

00:46:41,119 --> 00:46:43,920
marketing you know like what they've

00:46:42,319 --> 00:46:45,440
done on different websites

00:46:43,920 --> 00:46:47,200
you just stream that stuff straight into

00:46:45,440 --> 00:46:49,200
click house and it bypasses my sql

00:46:47,200 --> 00:46:52,079
completely

00:46:49,200 --> 00:46:53,280
um let's see what enterprises use click

00:46:52,079 --> 00:46:55,520
house cloudflare

00:46:53,280 --> 00:46:56,800
uh cisco uh i'm just trying to think of

00:46:55,520 --> 00:46:59,119
the people i can name

00:46:56,800 --> 00:47:00,319
comcast uh yandex of course they

00:46:59,119 --> 00:47:03,040
invented it

00:47:00,319 --> 00:47:04,160
um message bird i don't know if they're

00:47:03,040 --> 00:47:07,839
still using it but they were

00:47:04,160 --> 00:47:09,680
early adopters uh spotify so a bunch of

00:47:07,839 --> 00:47:11,119
people and i'm just naming the ones that

00:47:09,680 --> 00:47:12,960
have talked publicly about it there are

00:47:11,119 --> 00:47:14,640
many many more

00:47:12,960 --> 00:47:17,200
including a lot of people in financial

00:47:14,640 --> 00:47:17,200
services

00:47:17,760 --> 00:47:21,760
can the kafka table engine enable us to

00:47:19,680 --> 00:47:22,559
serve real-time online use cases as

00:47:21,760 --> 00:47:26,160
opposed to just

00:47:22,559 --> 00:47:27,440
analytics use cases yes so typically you

00:47:26,160 --> 00:47:30,880
can achieve ingest

00:47:27,440 --> 00:47:33,119
you know sort of uh time to query uh of

00:47:30,880 --> 00:47:34,960
500 milliseconds it really depends on

00:47:33,119 --> 00:47:36,800
how you have to make sure that kafka is

00:47:34,960 --> 00:47:39,359
well tuned but that's why kafka one of

00:47:36,800 --> 00:47:41,760
the big reasons why we like kafka

00:47:39,359 --> 00:47:44,079
is it's not just the scalability uh

00:47:41,760 --> 00:47:45,599
which of course kafka is wonderful

00:47:44,079 --> 00:47:47,599
but it's also the fact you can get stuff

00:47:45,599 --> 00:47:50,400
really fast so

00:47:47,599 --> 00:47:51,040
that is in fact a an important use case

00:47:50,400 --> 00:47:54,400
and

00:47:51,040 --> 00:47:56,640
what where i think this you know if i

00:47:54,400 --> 00:47:58,400
if i could you know contrast this for

00:47:56,640 --> 00:47:59,920
example what conflict is doing

00:47:58,400 --> 00:48:01,680
if you talk to confluent what they would

00:47:59,920 --> 00:48:05,280
say is hey we should we should run

00:48:01,680 --> 00:48:08,640
um you know uh

00:48:05,280 --> 00:48:09,200
ksql on on the query stream as it's

00:48:08,640 --> 00:48:11,520
coming through

00:48:09,200 --> 00:48:12,480
yes you can do that but in a way it's

00:48:11,520 --> 00:48:14,000
just easier to

00:48:12,480 --> 00:48:15,440
dump it as fast as possible into the

00:48:14,000 --> 00:48:16,400
data warehouse and just query the whole

00:48:15,440 --> 00:48:17,359
thing there because you're not just

00:48:16,400 --> 00:48:19,839
looking at the stream

00:48:17,359 --> 00:48:20,880
you have all your data and moreover you

00:48:19,839 --> 00:48:23,040
can partition it

00:48:20,880 --> 00:48:24,800
in ways and store it in ways that mean

00:48:23,040 --> 00:48:27,760
that the most recent stuff is hot and

00:48:24,800 --> 00:48:30,160
you can get at it really fast

00:48:27,760 --> 00:48:33,359
um how does click off speed compare to

00:48:30,160 --> 00:48:36,640
hana i don't have numbers on that

00:48:33,359 --> 00:48:38,079
so it is much much it's definitely much

00:48:36,640 --> 00:48:40,000
faster than redshift

00:48:38,079 --> 00:48:42,079
fast and elastic for use cases that are

00:48:40,000 --> 00:48:43,359
well uh suited but i don't have numbers

00:48:42,079 --> 00:48:45,440
on hana

00:48:43,359 --> 00:48:46,960
um when would it be a good idea to

00:48:45,440 --> 00:48:47,520
choose click house over dbs like

00:48:46,960 --> 00:48:49,359
postgres

00:48:47,520 --> 00:48:51,440
great question when you have a lot of

00:48:49,359 --> 00:48:55,839
data and in fact what happens

00:48:51,440 --> 00:48:57,920
is people mux.com

00:48:55,839 --> 00:49:00,400
is an example of a company that started

00:48:57,920 --> 00:49:02,000
with postgres they started with one

00:49:00,400 --> 00:49:04,160
and then they made it bigger and then

00:49:02,000 --> 00:49:05,839
they and they brought in situs

00:49:04,160 --> 00:49:08,079
and then they had you know like a big

00:49:05,839 --> 00:49:08,720
situs uh distribution and finally they

00:49:08,079 --> 00:49:10,800
gave up

00:49:08,720 --> 00:49:12,319
and just put it all inside click house

00:49:10,800 --> 00:49:14,240
and the reason is

00:49:12,319 --> 00:49:16,640
that with click house they could just

00:49:14,240 --> 00:49:19,920
throw the data in and

00:49:16,640 --> 00:49:21,599
basically be able to to do their queries

00:49:19,920 --> 00:49:24,960
without a lot of complex pipelines

00:49:21,599 --> 00:49:28,079
aggregation things like that

00:49:24,960 --> 00:49:31,280
okay um i'm just looking back i want to

00:49:28,079 --> 00:49:31,280
make sure that

00:49:34,000 --> 00:49:39,599
i think i got all the questions if uh

00:49:37,839 --> 00:49:42,160
let's see i'm just checking i didn't

00:49:39,599 --> 00:49:42,160
miss any

00:49:43,440 --> 00:49:50,960
great so um if uh

00:49:48,000 --> 00:49:52,480
if there are no further questions we'll

00:49:50,960 --> 00:49:54,640
go ahead and close this up

00:49:52,480 --> 00:49:56,480
i am available on slack if you want to

00:49:54,640 --> 00:49:58,000
uh post additional questions and you're

00:49:56,480 --> 00:50:00,160
interested in this i'd love to answer

00:49:58,000 --> 00:50:02,240
them i do hope you'll try out click

00:50:00,160 --> 00:50:04,800
house it's open source

00:50:02,240 --> 00:50:06,559
um just one last pitch i think it's

00:50:04,800 --> 00:50:07,280
other than sybase which was probably my

00:50:06,559 --> 00:50:10,079
favorite

00:50:07,280 --> 00:50:10,720
favorite relational database click house

00:50:10,079 --> 00:50:12,160
is

00:50:10,720 --> 00:50:14,400
the most interesting database i've ever

00:50:12,160 --> 00:50:16,079
worked with so

00:50:14,400 --> 00:50:18,000
definitely try it out it's really

00:50:16,079 --> 00:50:18,880
accessible and is a very very welcoming

00:50:18,000 --> 00:50:21,200
community

00:50:18,880 --> 00:50:22,800
and once again thanks for the third time

00:50:21,200 --> 00:50:24,000
the linux foundation for making this

00:50:22,800 --> 00:50:26,319
talk possible

00:50:24,000 --> 00:50:36,800
i hope to see you all in person soon

00:50:26,319 --> 00:50:36,800

YouTube URL: https://www.youtube.com/watch?v=D__W-e2d6GY


