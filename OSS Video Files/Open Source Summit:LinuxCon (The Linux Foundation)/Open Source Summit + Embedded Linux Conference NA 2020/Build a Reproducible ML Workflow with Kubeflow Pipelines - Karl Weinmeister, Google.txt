Title: Build a Reproducible ML Workflow with Kubeflow Pipelines - Karl Weinmeister, Google
Publication date: 2020-09-03
Playlist: Open Source Summit + Embedded Linux Conference NA 2020
Description: 
	Build a Reproducible ML Workflow with Kubeflow Pipelines - Karl Weinmeister, Google
Captions: 
	00:00:03,600 --> 00:00:07,279
hello everyone

00:00:04,640 --> 00:00:08,960
uh welcome to my session my name is carl

00:00:07,279 --> 00:00:12,080
weinmeister i'm going to talk to you

00:00:08,960 --> 00:00:14,639
about uh building a reproducible ml

00:00:12,080 --> 00:00:17,840
workflow with kubeflow pipelines

00:00:14,639 --> 00:00:20,800
and so um i've spent some time

00:00:17,840 --> 00:00:22,640
uh in the software engineering world as

00:00:20,800 --> 00:00:24,000
well as the machine learning world and i

00:00:22,640 --> 00:00:28,080
know all the challenges

00:00:24,000 --> 00:00:30,320
of trying to uh not just build a

00:00:28,080 --> 00:00:31,599
machine learning project and get to that

00:00:30,320 --> 00:00:34,640
initial

00:00:31,599 --> 00:00:36,480
models a high level of accuracy but

00:00:34,640 --> 00:00:37,920
how do you put that into production how

00:00:36,480 --> 00:00:39,440
do you make sure that you get the same

00:00:37,920 --> 00:00:41,520
results every time

00:00:39,440 --> 00:00:43,360
and we're going to talk about an open

00:00:41,520 --> 00:00:44,800
source platform that allows you to do

00:00:43,360 --> 00:00:46,640
that

00:00:44,800 --> 00:00:48,960
so let's start with looking at the

00:00:46,640 --> 00:00:52,239
agenda here

00:00:48,960 --> 00:00:54,640
the um we're going to start with

00:00:52,239 --> 00:00:56,800
why you know some of the challenges in

00:00:54,640 --> 00:00:58,399
building machine learning projects

00:00:56,800 --> 00:01:01,600
we're going to talk about the kubeflow

00:00:58,399 --> 00:01:02,320
platform and specifically the pipeline

00:01:01,600 --> 00:01:04,960
component

00:01:02,320 --> 00:01:06,240
of kubeflow what allows you to build an

00:01:04,960 --> 00:01:09,040
end to end

00:01:06,240 --> 00:01:10,560
reproducible machine learning pipeline

00:01:09,040 --> 00:01:13,600
we'll get into a how-to

00:01:10,560 --> 00:01:15,280
so you'll see some code we'll look at a

00:01:13,600 --> 00:01:17,360
docker file

00:01:15,280 --> 00:01:18,799
we'll get a flavor for what it takes to

00:01:17,360 --> 00:01:22,320
build a pipeline

00:01:18,799 --> 00:01:24,960
and then finally we'll do a demo

00:01:22,320 --> 00:01:25,680
it is using screenshots but it's going

00:01:24,960 --> 00:01:28,640
to walk

00:01:25,680 --> 00:01:30,000
through an actual pipeline and what that

00:01:28,640 --> 00:01:33,759
looks like in a

00:01:30,000 --> 00:01:35,119
real deployment so so again this is not

00:01:33,759 --> 00:01:37,280
necessarily about

00:01:35,119 --> 00:01:38,560
building a model it's not an ai

00:01:37,280 --> 00:01:41,119
introduction

00:01:38,560 --> 00:01:41,840
but really it's about all the important

00:01:41,119 --> 00:01:44,000
things

00:01:41,840 --> 00:01:46,720
around building the model so how do you

00:01:44,000 --> 00:01:48,479
manage it in a production setting

00:01:46,720 --> 00:01:50,399
and showing how kubeflow can help with

00:01:48,479 --> 00:01:53,360
that

00:01:50,399 --> 00:01:54,000
so first let's start with uh just

00:01:53,360 --> 00:01:57,280
overall this

00:01:54,000 --> 00:01:57,920
idea of um operating in machine learning

00:01:57,280 --> 00:01:59,360
can be

00:01:57,920 --> 00:02:01,600
very difficult we'll talk through some

00:01:59,360 --> 00:02:04,240
specific issues for machine learning

00:02:01,600 --> 00:02:06,320
uh you know i wouldn't necessarily say

00:02:04,240 --> 00:02:07,920
launching is super easy often you know

00:02:06,320 --> 00:02:09,440
machine learning uh you know you

00:02:07,920 --> 00:02:12,720
experience challenges with

00:02:09,440 --> 00:02:14,560
uh getting the right data um

00:02:12,720 --> 00:02:16,000
having your model get to the right level

00:02:14,560 --> 00:02:17,840
of accuracy where

00:02:16,000 --> 00:02:20,400
you're solving the problem well without

00:02:17,840 --> 00:02:22,959
too many false positives or negatives

00:02:20,400 --> 00:02:23,520
so that in itself is a challenge but

00:02:22,959 --> 00:02:26,319
it's

00:02:23,520 --> 00:02:27,040
the next level of challenge around how

00:02:26,319 --> 00:02:28,959
do you

00:02:27,040 --> 00:02:30,640
track all these different models that

00:02:28,959 --> 00:02:32,640
you're experimenting with

00:02:30,640 --> 00:02:35,120
how do you make sure that the data

00:02:32,640 --> 00:02:38,640
that's being used for those models from

00:02:35,120 --> 00:02:40,319
all these different upstream systems

00:02:38,640 --> 00:02:43,519
is correct that you know you don't have

00:02:40,319 --> 00:02:45,360
a garbage in garbage out kind of issue

00:02:43,519 --> 00:02:47,519
how are you tracking the accuracy of the

00:02:45,360 --> 00:02:49,280
models that you have

00:02:47,519 --> 00:02:50,879
how are you ensuring that your models

00:02:49,280 --> 00:02:52,000
are performing well for all of your

00:02:50,879 --> 00:02:54,400
users

00:02:52,000 --> 00:02:55,760
there's a lot of things that you need to

00:02:54,400 --> 00:02:58,480
think about

00:02:55,760 --> 00:02:59,200
once you get past that experiment stage

00:02:58,480 --> 00:03:01,200
and

00:02:59,200 --> 00:03:02,480
you're running this constant stream of

00:03:01,200 --> 00:03:04,480
new models

00:03:02,480 --> 00:03:06,560
putting it into production ensuring that

00:03:04,480 --> 00:03:08,239
you don't have

00:03:06,560 --> 00:03:10,159
bugs in your model if you will or

00:03:08,239 --> 00:03:13,040
regressions where

00:03:10,159 --> 00:03:14,800
you start making uh very critical

00:03:13,040 --> 00:03:16,480
mistakes in the model how do you

00:03:14,800 --> 00:03:17,360
put some guard rails against that so

00:03:16,480 --> 00:03:19,680
that's what we're going to talk about

00:03:17,360 --> 00:03:22,480
today

00:03:19,680 --> 00:03:24,640
so one of those challenges is a lack of

00:03:22,480 --> 00:03:27,280
continuous monitoring

00:03:24,640 --> 00:03:28,799
so say that you build a model it's

00:03:27,280 --> 00:03:30,879
working fine

00:03:28,799 --> 00:03:33,760
but slowly you start to see this drift

00:03:30,879 --> 00:03:35,519
where the model accuracy drops

00:03:33,760 --> 00:03:37,280
and you might run into this if you don't

00:03:35,519 --> 00:03:38,400
have a practice for continuous

00:03:37,280 --> 00:03:41,040
monitoring

00:03:38,400 --> 00:03:41,920
in other words once you've deployed a

00:03:41,040 --> 00:03:45,360
model

00:03:41,920 --> 00:03:48,400
watching the accuracy of that and

00:03:45,360 --> 00:03:50,799
uh being able to take steps if

00:03:48,400 --> 00:03:51,920
that accuracy drips drops below a

00:03:50,799 --> 00:03:54,480
certain threshold

00:03:51,920 --> 00:03:56,720
so we don't want a crisis where uh you

00:03:54,480 --> 00:03:59,680
know everything's okay and then one day

00:03:56,720 --> 00:04:00,000
um it just is so obvious that this model

00:03:59,680 --> 00:04:02,799
isn't

00:04:00,000 --> 00:04:04,640
working well that you've got to you know

00:04:02,799 --> 00:04:05,280
solve it in emergency type of setting

00:04:04,640 --> 00:04:08,799
without

00:04:05,280 --> 00:04:10,959
having monitoring in place

00:04:08,799 --> 00:04:13,760
another kind of issue you could run into

00:04:10,959 --> 00:04:16,880
is what's called training serving sku

00:04:13,760 --> 00:04:19,519
where you may have code

00:04:16,880 --> 00:04:21,359
that's used for training say to do

00:04:19,519 --> 00:04:24,320
feature extraction where you

00:04:21,359 --> 00:04:25,759
are pulling machine learning features

00:04:24,320 --> 00:04:27,759
out of your data set

00:04:25,759 --> 00:04:29,520
your training on those features and

00:04:27,759 --> 00:04:32,240
building a model

00:04:29,520 --> 00:04:34,320
however your serving code or the code

00:04:32,240 --> 00:04:36,880
that's used by your applications

00:04:34,320 --> 00:04:37,759
to do a prediction with that trained

00:04:36,880 --> 00:04:39,280
model

00:04:37,759 --> 00:04:41,840
might be a little bit different maybe

00:04:39,280 --> 00:04:42,880
there's a code that's not being shared

00:04:41,840 --> 00:04:45,360
between

00:04:42,880 --> 00:04:46,320
uh those different uh parts of the

00:04:45,360 --> 00:04:49,520
process

00:04:46,320 --> 00:04:51,840
and so a bug might be introduced where

00:04:49,520 --> 00:04:53,919
the data is transformed slightly

00:04:51,840 --> 00:04:55,680
different coming in from one step to the

00:04:53,919 --> 00:04:58,400
other maybe there's

00:04:55,680 --> 00:04:59,120
missing data nulls this kind of thing

00:04:58,400 --> 00:05:01,199
and so

00:04:59,120 --> 00:05:02,320
you have to be very careful on ensuring

00:05:01,199 --> 00:05:05,600
that

00:05:02,320 --> 00:05:07,120
both the one-time training process where

00:05:05,600 --> 00:05:08,800
you're looking at all of your data and

00:05:07,120 --> 00:05:11,759
coming up with the model

00:05:08,800 --> 00:05:12,560
and the runtime aspect where you have a

00:05:11,759 --> 00:05:14,880
model

00:05:12,560 --> 00:05:16,479
and you're serving users or serving your

00:05:14,880 --> 00:05:19,120
application with that

00:05:16,479 --> 00:05:22,479
that code is in sync and that can be

00:05:19,120 --> 00:05:22,479
another source of issues

00:05:22,800 --> 00:05:26,479
another consideration is around

00:05:25,039 --> 00:05:29,440
freshness uh and

00:05:26,479 --> 00:05:31,199
what we mean by that is really thinking

00:05:29,440 --> 00:05:31,680
about the business requirements of your

00:05:31,199 --> 00:05:34,320
model

00:05:31,680 --> 00:05:34,960
how often does that data change and here

00:05:34,320 --> 00:05:37,759
are a few

00:05:34,960 --> 00:05:40,000
examples just to think about so on one

00:05:37,759 --> 00:05:42,479
extreme you might have

00:05:40,000 --> 00:05:44,000
a model that's based on news that's

00:05:42,479 --> 00:05:45,919
constantly changing

00:05:44,000 --> 00:05:47,759
on another extreme you might have a

00:05:45,919 --> 00:05:50,880
voice recognition model

00:05:47,759 --> 00:05:52,639
that's fairly stable and doesn't need to

00:05:50,880 --> 00:05:55,600
be retrained very often and

00:05:52,639 --> 00:05:56,800
everywhere in between maybe you have a

00:05:55,600 --> 00:05:59,520
model for

00:05:56,800 --> 00:06:00,720
an e-commerce site that recommends new

00:05:59,520 --> 00:06:02,240
products

00:06:00,720 --> 00:06:03,840
if you think about it that catalog of

00:06:02,240 --> 00:06:07,440
products may change over time

00:06:03,840 --> 00:06:08,160
so it's useful to occasionally retrain

00:06:07,440 --> 00:06:11,919
that model

00:06:08,160 --> 00:06:14,479
to say okay some time has passed i want

00:06:11,919 --> 00:06:16,240
to add new data to my model that's more

00:06:14,479 --> 00:06:18,400
relevant and current

00:06:16,240 --> 00:06:21,199
and so you want to think about a process

00:06:18,400 --> 00:06:24,400
for retraining redeploying

00:06:21,199 --> 00:06:26,080
um versioning that model because

00:06:24,400 --> 00:06:27,680
you know what we're seeing here is that

00:06:26,080 --> 00:06:30,400
ai is not a

00:06:27,680 --> 00:06:32,479
one-time thing it's a process just a

00:06:30,400 --> 00:06:33,360
continuous improvement process just like

00:06:32,479 --> 00:06:35,840
you might

00:06:33,360 --> 00:06:37,199
improve your code in a software

00:06:35,840 --> 00:06:39,600
development application

00:06:37,199 --> 00:06:41,039
version it again we need to think the

00:06:39,600 --> 00:06:43,680
same way when it comes to machine

00:06:41,039 --> 00:06:43,680
learning as well

00:06:43,840 --> 00:06:48,479
and so we often think about data science

00:06:47,840 --> 00:06:50,720
as

00:06:48,479 --> 00:06:51,840
building a model you know the fun part

00:06:50,720 --> 00:06:54,720
of

00:06:51,840 --> 00:06:55,280
you have data and you're looking for an

00:06:54,720 --> 00:06:58,400
insight

00:06:55,280 --> 00:07:00,880
and you're hoping to find that you know

00:06:58,400 --> 00:07:02,400
that it classifies well or it predicts

00:07:00,880 --> 00:07:04,319
the future well and

00:07:02,400 --> 00:07:06,160
it's super exciting to kind of go

00:07:04,319 --> 00:07:08,000
through that process and get to that

00:07:06,160 --> 00:07:10,800
point

00:07:08,000 --> 00:07:13,280
however that's just a piece of the

00:07:10,800 --> 00:07:15,680
puzzle you know once you've done that

00:07:13,280 --> 00:07:17,360
these are all the things that you might

00:07:15,680 --> 00:07:18,560
need to think about in a production

00:07:17,360 --> 00:07:21,440
system

00:07:18,560 --> 00:07:22,080
and we've talked about some of these so

00:07:21,440 --> 00:07:24,960
far

00:07:22,080 --> 00:07:26,880
uh you know ingesting the data

00:07:24,960 --> 00:07:28,840
transforming the data often the data

00:07:26,880 --> 00:07:32,479
coming in from these other systems

00:07:28,840 --> 00:07:32,960
isn't in its raw state how you need it

00:07:32,479 --> 00:07:36,000
to

00:07:32,960 --> 00:07:38,319
be formed so that it can be

00:07:36,000 --> 00:07:40,560
used in a machine learning model you

00:07:38,319 --> 00:07:42,479
might need validation on both sides of

00:07:40,560 --> 00:07:45,680
the model the data coming in

00:07:42,479 --> 00:07:48,000
the model outputs coming out you need to

00:07:45,680 --> 00:07:51,680
do things like maybe monitoring

00:07:48,000 --> 00:07:55,120
and logging of the model

00:07:51,680 --> 00:07:58,400
so all kinds of different considerations

00:07:55,120 --> 00:07:59,840
as you are doing your machine learning

00:07:58,400 --> 00:08:01,840
process here

00:07:59,840 --> 00:08:03,440
okay so we've talked about the process

00:08:01,840 --> 00:08:05,280
and the considerations now let's look at

00:08:03,440 --> 00:08:08,240
it from a different angle even

00:08:05,280 --> 00:08:09,440
which is the stack right the

00:08:08,240 --> 00:08:11,919
architecture

00:08:09,440 --> 00:08:13,840
and ensuring consistency so you could

00:08:11,919 --> 00:08:17,039
think about all these different layers

00:08:13,840 --> 00:08:20,080
um you know in terms of your

00:08:17,039 --> 00:08:23,280
your team members and your dev

00:08:20,080 --> 00:08:25,840
environment your production environment

00:08:23,280 --> 00:08:27,280
they have to have consistency across the

00:08:25,840 --> 00:08:29,840
board everything from the model

00:08:27,280 --> 00:08:32,080
to the tooling the version of the

00:08:29,840 --> 00:08:35,200
framework that you're using

00:08:32,080 --> 00:08:37,360
uh gpus etc um

00:08:35,200 --> 00:08:40,000
for consistency you you need to make

00:08:37,360 --> 00:08:43,599
sure that these things are all aligned

00:08:40,000 --> 00:08:46,320
so if you think about it that the

00:08:43,599 --> 00:08:47,279
um that process that we just talked

00:08:46,320 --> 00:08:50,000
about

00:08:47,279 --> 00:08:50,399
needs to operate the same way on the

00:08:50,000 --> 00:08:53,120
same

00:08:50,399 --> 00:08:54,560
on these various different stacks so

00:08:53,120 --> 00:08:56,160
just thinking about it like we would in

00:08:54,560 --> 00:08:58,880
a software development

00:08:56,160 --> 00:08:59,440
uh scenario dev test prod need to be

00:08:58,880 --> 00:09:02,240
aligned

00:08:59,440 --> 00:09:03,680
all team members using the same versions

00:09:02,240 --> 00:09:05,519
if you're using python

00:09:03,680 --> 00:09:07,200
uh you know when you're doing pip

00:09:05,519 --> 00:09:08,800
install are you working from a common

00:09:07,200 --> 00:09:10,959
set of requirements

00:09:08,800 --> 00:09:11,959
uh you know all these things are

00:09:10,959 --> 00:09:14,160
important to ensure

00:09:11,959 --> 00:09:15,360
reproducibility of your machine learning

00:09:14,160 --> 00:09:18,160
system

00:09:15,360 --> 00:09:20,000
okay so this is where kubeflow comes

00:09:18,160 --> 00:09:23,360
into the picture

00:09:20,000 --> 00:09:24,320
so kubeflow's mission statement uh is to

00:09:23,360 --> 00:09:26,880
let's break it down

00:09:24,320 --> 00:09:27,680
into pieces here um making it easy for

00:09:26,880 --> 00:09:31,760
everyone

00:09:27,680 --> 00:09:35,360
so it was designed for everyone from

00:09:31,760 --> 00:09:38,480
uh beginner users and folks that are

00:09:35,360 --> 00:09:40,880
either data scientists who don't

00:09:38,480 --> 00:09:43,120
necessarily need to get into all the

00:09:40,880 --> 00:09:45,360
infrastructure details of kubernetes and

00:09:43,120 --> 00:09:47,680
docker that they can work in python

00:09:45,360 --> 00:09:49,440
there's reasonable defaults but since

00:09:47,680 --> 00:09:52,000
it's open source

00:09:49,440 --> 00:09:53,440
there is the ability to customize a lot

00:09:52,000 --> 00:09:56,720
of things in the

00:09:53,440 --> 00:09:57,200
in the stack the other things you see

00:09:56,720 --> 00:09:59,440
here

00:09:57,200 --> 00:10:01,600
it's about the whole life cycle develop

00:09:59,440 --> 00:10:02,560
deploy and manage so building the model

00:10:01,600 --> 00:10:05,200
deploying it

00:10:02,560 --> 00:10:06,959
and then what you do after it's live

00:10:05,200 --> 00:10:07,839
finally i think the other key points

00:10:06,959 --> 00:10:11,040
here is

00:10:07,839 --> 00:10:14,000
why is it why kubernetes right so

00:10:11,040 --> 00:10:14,720
portability so we talked about having a

00:10:14,000 --> 00:10:18,160
uniform

00:10:14,720 --> 00:10:19,519
infrastructure so if you abstract away

00:10:18,160 --> 00:10:22,560
the underlying

00:10:19,519 --> 00:10:23,360
uh architecture you can ensure that you

00:10:22,560 --> 00:10:27,760
can port

00:10:23,360 --> 00:10:30,079
your processes between the cloud on-prem

00:10:27,760 --> 00:10:32,240
uh etc all these different environments

00:10:30,079 --> 00:10:34,160
and ensure it works the same

00:10:32,240 --> 00:10:35,440
and then another great thing about using

00:10:34,160 --> 00:10:38,399
kubernetes is

00:10:35,440 --> 00:10:39,040
the distributed nature of it so running

00:10:38,399 --> 00:10:42,000
jobs

00:10:39,040 --> 00:10:42,959
on uh you know clusters and work in the

00:10:42,000 --> 00:10:45,839
pods

00:10:42,959 --> 00:10:47,839
uh it really helps to you know help

00:10:45,839 --> 00:10:49,200
scale out some work like in a training

00:10:47,839 --> 00:10:52,640
job where you

00:10:49,200 --> 00:10:53,519
have a one-time uh large training set

00:10:52,640 --> 00:10:55,920
that might take

00:10:53,519 --> 00:10:57,760
all night to run if you can run on a

00:10:55,920 --> 00:11:00,560
kubernetes cluster

00:10:57,760 --> 00:11:02,079
maybe you can speed that up dramatically

00:11:00,560 --> 00:11:02,399
especially if it's in the cloud and you

00:11:02,079 --> 00:11:05,279
have

00:11:02,399 --> 00:11:07,360
auto scaling auto provisioning uh you

00:11:05,279 --> 00:11:08,079
know horizontally scale that finish your

00:11:07,360 --> 00:11:11,120
job

00:11:08,079 --> 00:11:13,279
and then shrink the number of um you

00:11:11,120 --> 00:11:15,600
know pods allocated for that so

00:11:13,279 --> 00:11:16,720
fits very nicely to the model of

00:11:15,600 --> 00:11:18,959
kubernetes

00:11:16,720 --> 00:11:20,079
and one thing that i have found is also

00:11:18,959 --> 00:11:22,079
useful is

00:11:20,079 --> 00:11:24,240
if you're trying to standardize your

00:11:22,079 --> 00:11:28,399
devops processes across

00:11:24,240 --> 00:11:30,480
your company it's nice having one stack

00:11:28,399 --> 00:11:32,240
and one set of best practices and

00:11:30,480 --> 00:11:34,800
learning curve around

00:11:32,240 --> 00:11:35,920
how do you manage your software

00:11:34,800 --> 00:11:38,079
applications

00:11:35,920 --> 00:11:39,360
and your machine learning and putting

00:11:38,079 --> 00:11:42,880
those all on one

00:11:39,360 --> 00:11:45,360
common infrastructure or kubernetes

00:11:42,880 --> 00:11:47,600
so that's kubeflow in a nutshell let's

00:11:45,360 --> 00:11:49,279
talk about some of the key capabilities

00:11:47,600 --> 00:11:51,279
from it i'm going to work my way

00:11:49,279 --> 00:11:54,079
clockwise through this pres

00:11:51,279 --> 00:11:54,800
through this slide so uh jupiter

00:11:54,079 --> 00:11:58,079
notebooks

00:11:54,800 --> 00:12:00,320
uh is the ide if you will the

00:11:58,079 --> 00:12:02,160
development environment for writing

00:12:00,320 --> 00:12:03,360
data science code we'll see a little bit

00:12:02,160 --> 00:12:06,639
of that

00:12:03,360 --> 00:12:09,680
there are training operators these

00:12:06,639 --> 00:12:12,079
an operator is a kubernetes construct

00:12:09,680 --> 00:12:14,240
that allows you to do something

00:12:12,079 --> 00:12:15,279
basically it's a custom api on

00:12:14,240 --> 00:12:17,120
kubernetes

00:12:15,279 --> 00:12:18,720
and there are training operators for a

00:12:17,120 --> 00:12:21,200
variety of different machine learning

00:12:18,720 --> 00:12:24,720
frameworks allow you to

00:12:21,200 --> 00:12:28,000
run those training jobs and have a model

00:12:24,720 --> 00:12:30,000
out as a result there are workflow

00:12:28,000 --> 00:12:31,360
building and pipeline capabilities that

00:12:30,000 --> 00:12:33,279
we're going to talk more about that i

00:12:31,360 --> 00:12:36,160
think are really

00:12:33,279 --> 00:12:37,760
key to ensuring you get the same result

00:12:36,160 --> 00:12:41,120
every time

00:12:37,760 --> 00:12:42,399
data management capabilities to version

00:12:41,120 --> 00:12:43,760
the data

00:12:42,399 --> 00:12:48,240
ensure that you're working with that

00:12:43,760 --> 00:12:50,320
same reproducible data set every time

00:12:48,240 --> 00:12:52,639
there are a variety of other tools like

00:12:50,320 --> 00:12:54,240
if you've done

00:12:52,639 --> 00:12:55,440
some data science in the past you might

00:12:54,240 --> 00:12:57,279
be familiar with the concept called

00:12:55,440 --> 00:13:00,639
hyper parameter tuning

00:12:57,279 --> 00:13:02,639
where you might search across all these

00:13:00,639 --> 00:13:05,680
different possibilities of

00:13:02,639 --> 00:13:08,079
um you know what kind of

00:13:05,680 --> 00:13:09,200
you know learning rate or batch size

00:13:08,079 --> 00:13:12,560
should i use

00:13:09,200 --> 00:13:15,200
and uh this is another capability that

00:13:12,560 --> 00:13:17,440
fits well with the model of distributing

00:13:15,200 --> 00:13:20,320
the load over kubernetes where you can

00:13:17,440 --> 00:13:22,639
do that search uh over your cluster to

00:13:20,320 --> 00:13:25,040
find those optimal parameters

00:13:22,639 --> 00:13:26,959
finally or you have metadata management

00:13:25,040 --> 00:13:28,160
so maybe you want to track data against

00:13:26,959 --> 00:13:30,560
your model

00:13:28,160 --> 00:13:32,720
what type of framework was used what

00:13:30,560 --> 00:13:35,040
date was it created

00:13:32,720 --> 00:13:37,440
so on and so forth so you can attach

00:13:35,040 --> 00:13:40,720
metadata to the different objects

00:13:37,440 --> 00:13:42,639
and finally serving so hosting

00:13:40,720 --> 00:13:44,240
the model once it's built in your

00:13:42,639 --> 00:13:47,199
cluster so providing a

00:13:44,240 --> 00:13:48,880
rest endpoint for you to use the model

00:13:47,199 --> 00:13:50,880
so all these different things

00:13:48,880 --> 00:13:53,360
from end to end can be accomplished with

00:13:50,880 --> 00:13:53,360
kubeflow

00:13:53,760 --> 00:13:57,680
so to sum it up you know we talked about

00:13:56,800 --> 00:14:00,399
portability

00:13:57,680 --> 00:14:02,079
scalability we're going to get more into

00:14:00,399 --> 00:14:05,120
composability how do you build

00:14:02,079 --> 00:14:06,880
a reproducible pipeline

00:14:05,120 --> 00:14:08,480
you know it cuts across all the

00:14:06,880 --> 00:14:09,839
different parts of machine learning in

00:14:08,480 --> 00:14:12,639
the life cycle

00:14:09,839 --> 00:14:15,040
and finally another key point is it's

00:14:12,639 --> 00:14:16,880
designed for machine learning so it has

00:14:15,040 --> 00:14:18,800
in this idea in mind of using

00:14:16,880 --> 00:14:22,160
specialized chipsets

00:14:18,800 --> 00:14:23,519
like gpus and tpus to improve

00:14:22,160 --> 00:14:26,560
performance

00:14:23,519 --> 00:14:29,600
so you can mark

00:14:26,560 --> 00:14:30,959
various uh pods and set labels so that

00:14:29,600 --> 00:14:34,079
you're

00:14:30,959 --> 00:14:34,480
say on uh you know part of your cluster

00:14:34,079 --> 00:14:37,600
that's

00:14:34,480 --> 00:14:41,199
uh that has gpus on specific

00:14:37,600 --> 00:14:44,000
vms that have the gpus enabled you can

00:14:41,199 --> 00:14:45,839
send a workload to those gpus so a lot

00:14:44,000 --> 00:14:46,800
of capability like that is built into

00:14:45,839 --> 00:14:51,040
the

00:14:46,800 --> 00:14:53,920
platform from a design perspective

00:14:51,040 --> 00:14:55,600
uh this was designed to fit well with

00:14:53,920 --> 00:14:58,639
kubernetes

00:14:55,600 --> 00:15:00,639
constructs so it's not uh some sort of

00:14:58,639 --> 00:15:04,160
proprietary design pattern

00:15:00,639 --> 00:15:06,160
it uses the kubernetes

00:15:04,160 --> 00:15:08,160
you know apis if you will and we'll look

00:15:06,160 --> 00:15:09,279
in this you know a little more detail on

00:15:08,160 --> 00:15:10,399
that later

00:15:09,279 --> 00:15:13,120
um you know really using those

00:15:10,399 --> 00:15:15,519
kubernetes design patterns

00:15:13,120 --> 00:15:16,160
and it supports a variety of frameworks

00:15:15,519 --> 00:15:19,760
uh

00:15:16,160 --> 00:15:23,760
you know tensorflow pytorch scikit-learn

00:15:19,760 --> 00:15:27,120
boosts and many more so it's very open

00:15:23,760 --> 00:15:30,560
in that way let's look at the components

00:15:27,120 --> 00:15:32,240
briefly so any

00:15:30,560 --> 00:15:34,480
workload coming in you know there's an

00:15:32,240 --> 00:15:38,720
ingress part of kubernetes that

00:15:34,480 --> 00:15:42,000
work gets distributed to the right

00:15:38,720 --> 00:15:42,800
place in the cluster uh and you really

00:15:42,000 --> 00:15:45,360
have about

00:15:42,800 --> 00:15:47,600
three main areas here the dashboard a

00:15:45,360 --> 00:15:51,440
user interface where you have

00:15:47,600 --> 00:15:54,880
uh the ability to run your data science

00:15:51,440 --> 00:15:57,440
uh notebooks to to write code to

00:15:54,880 --> 00:15:59,680
view your pipelines etc give your

00:15:57,440 --> 00:16:02,079
operators where you can

00:15:59,680 --> 00:16:03,360
actually do the heavy lifting of running

00:16:02,079 --> 00:16:06,399
the jobs

00:16:03,360 --> 00:16:08,320
and the training jobs and then serving

00:16:06,399 --> 00:16:10,399
that we talked about before where you

00:16:08,320 --> 00:16:12,800
host the model in your kubernetes

00:16:10,399 --> 00:16:12,800
environment

00:16:13,839 --> 00:16:20,560
all right so how do we get started um

00:16:17,360 --> 00:16:23,440
so here we see that there's

00:16:20,560 --> 00:16:25,759
some yaml files these are all in github

00:16:23,440 --> 00:16:28,079
by the way here are some examples

00:16:25,759 --> 00:16:30,480
and just a few of the many examples that

00:16:28,079 --> 00:16:34,000
are available in github

00:16:30,480 --> 00:16:35,279
where you run a cli command

00:16:34,000 --> 00:16:37,680
and it's similar to if you've used

00:16:35,279 --> 00:16:40,720
kubernetes a cube ctl

00:16:37,680 --> 00:16:42,240
there's kf or kubeflow ctl

00:16:40,720 --> 00:16:44,800
and what you're doing here is you're

00:16:42,240 --> 00:16:47,360
applying this yaml file

00:16:44,800 --> 00:16:49,040
and that yaml file has the instructions

00:16:47,360 --> 00:16:51,440
to

00:16:49,040 --> 00:16:52,880
set up the cluster for you and the

00:16:51,440 --> 00:16:54,480
services

00:16:52,880 --> 00:16:56,720
and configure everything you know

00:16:54,480 --> 00:16:59,560
permissions firewall etc

00:16:56,720 --> 00:17:00,959
um and and you see there there's a few

00:16:59,560 --> 00:17:03,600
configurations

00:17:00,959 --> 00:17:06,240
uh for various different platforms and

00:17:03,600 --> 00:17:10,720
that's all it takes to get started

00:17:06,240 --> 00:17:12,559
with uh with kubeflow so

00:17:10,720 --> 00:17:15,199
we're going to walk through several

00:17:12,559 --> 00:17:16,720
different screenshots and how to

00:17:15,199 --> 00:17:19,360
implement these capabilities from

00:17:16,720 --> 00:17:22,559
development to training your model

00:17:19,360 --> 00:17:26,559
serving it and orchestration

00:17:22,559 --> 00:17:29,440
okay so from a development perspective

00:17:26,559 --> 00:17:29,760
there's a concept of a notebook server

00:17:29,440 --> 00:17:33,200
so

00:17:29,760 --> 00:17:36,640
you would define a various

00:17:33,200 --> 00:17:38,880
servers using a docker file so maybe

00:17:36,640 --> 00:17:39,919
you use the out of the box docker file

00:17:38,880 --> 00:17:42,400
that has a

00:17:39,919 --> 00:17:44,960
say tensorflow pre-installed it's

00:17:42,400 --> 00:17:47,360
configured for gpus etc

00:17:44,960 --> 00:17:48,160
or you could say at you know our

00:17:47,360 --> 00:17:50,160
enterprise

00:17:48,160 --> 00:17:51,440
we have a different version of

00:17:50,160 --> 00:17:54,000
tensorflow or some

00:17:51,440 --> 00:17:55,840
you know different dependencies that we

00:17:54,000 --> 00:17:58,080
want to bake into our

00:17:55,840 --> 00:18:01,120
standard image you can do that and add

00:17:58,080 --> 00:18:02,880
that to that list of notebook servers

00:18:01,120 --> 00:18:04,640
and then each of those notebook servers

00:18:02,880 --> 00:18:07,760
could then be

00:18:04,640 --> 00:18:09,760
pre-configured with

00:18:07,760 --> 00:18:11,760
you know different settings of how much

00:18:09,760 --> 00:18:14,320
ram and cpu etc to

00:18:11,760 --> 00:18:15,840
allocate to it and then you're able to

00:18:14,320 --> 00:18:18,320
have your data scientists work in that

00:18:15,840 --> 00:18:21,120
environment

00:18:18,320 --> 00:18:22,400
okay now let's look at how training

00:18:21,120 --> 00:18:24,880
works

00:18:22,400 --> 00:18:26,160
so i'm going to get into probably more

00:18:24,880 --> 00:18:29,200
detail than

00:18:26,160 --> 00:18:30,640
typically most folks would need to but i

00:18:29,200 --> 00:18:31,520
think this helps illustrate what's

00:18:30,640 --> 00:18:34,960
happening

00:18:31,520 --> 00:18:38,320
under the hood so just like we saw

00:18:34,960 --> 00:18:41,280
the installation file to

00:18:38,320 --> 00:18:42,080
the yaml to set up the cluster here's an

00:18:41,280 --> 00:18:45,200
example

00:18:42,080 --> 00:18:45,600
of how a training would work so uh this

00:18:45,200 --> 00:18:47,840
is

00:18:45,600 --> 00:18:48,960
a code snippet from the pi torch

00:18:47,840 --> 00:18:52,480
training ammo

00:18:48,960 --> 00:18:54,799
again this is in the github dot com

00:18:52,480 --> 00:18:56,880
kubeflow repo if you want to take a

00:18:54,799 --> 00:19:00,559
deeper look at some point

00:18:56,880 --> 00:19:05,360
and so it really just defines uh details

00:19:00,559 --> 00:19:07,840
around the you know configuration like

00:19:05,360 --> 00:19:08,960
which docker image will it use for

00:19:07,840 --> 00:19:12,400
training

00:19:08,960 --> 00:19:14,000
which you know file is it going to run

00:19:12,400 --> 00:19:16,400
and then you can set things like you see

00:19:14,000 --> 00:19:19,679
at the bottom a limit uh you know that

00:19:16,400 --> 00:19:20,559
maybe it can use up to one gpu so this

00:19:19,679 --> 00:19:22,799
is where

00:19:20,559 --> 00:19:24,000
that training job uh this is the

00:19:22,799 --> 00:19:26,400
descriptor for

00:19:24,000 --> 00:19:27,280
how the training job works and so you'd

00:19:26,400 --> 00:19:29,440
use a standard

00:19:27,280 --> 00:19:31,600
kubernetes command to launch that

00:19:29,440 --> 00:19:34,880
training jobs you see on the right side

00:19:31,600 --> 00:19:37,520
uh cubectl and then it uh

00:19:34,880 --> 00:19:38,480
you know launches that job and then from

00:19:37,520 --> 00:19:42,080
there you can

00:19:38,480 --> 00:19:45,120
again use standard kubernetes commands

00:19:42,080 --> 00:19:48,960
to monitor that job so here we might

00:19:45,120 --> 00:19:50,880
look at that particular job

00:19:48,960 --> 00:19:52,080
and you're going to see the log as it's

00:19:50,880 --> 00:19:55,120
you know printing

00:19:52,080 --> 00:19:57,520
out the loss as it you know how many

00:19:55,120 --> 00:19:59,039
uh you know epochs has it gone through

00:19:57,520 --> 00:20:01,039
how many passes through the data

00:19:59,039 --> 00:20:02,480
has it performed uh you're going to see

00:20:01,039 --> 00:20:03,679
all that it's sort of in a standard way

00:20:02,480 --> 00:20:07,760
of looking at the

00:20:03,679 --> 00:20:11,440
container logs so that's training

00:20:07,760 --> 00:20:13,440
let's talk a little bit about serving so

00:20:11,440 --> 00:20:15,440
with serving there there is an

00:20:13,440 --> 00:20:18,559
abstraction layer where

00:20:15,440 --> 00:20:20,400
there are two different

00:20:18,559 --> 00:20:21,760
operations that you can perform a

00:20:20,400 --> 00:20:24,000
serving and

00:20:21,760 --> 00:20:25,840
uh various different frameworks will you

00:20:24,000 --> 00:20:28,159
know implement that slightly different

00:20:25,840 --> 00:20:29,919
but it's the same concept here

00:20:28,159 --> 00:20:31,360
so there's a predict and explain

00:20:29,919 --> 00:20:35,360
operation so

00:20:31,360 --> 00:20:38,159
um predict will uh predict or infer

00:20:35,360 --> 00:20:38,960
given a given something new that you

00:20:38,159 --> 00:20:40,799
give it

00:20:38,960 --> 00:20:42,240
you give to the model what's the result

00:20:40,799 --> 00:20:43,840
you're going to get back

00:20:42,240 --> 00:20:45,600
and you have two different endpoints you

00:20:43,840 --> 00:20:48,080
can define your default

00:20:45,600 --> 00:20:50,000
and then it does support this concept of

00:20:48,080 --> 00:20:52,400
the canary deployment which is a

00:20:50,000 --> 00:20:55,679
technique where you might say

00:20:52,400 --> 00:20:57,919
well i'm i'm testing out or it's in

00:20:55,679 --> 00:21:00,159
alpha and i don't necessarily want to

00:20:57,919 --> 00:21:02,000
roll this out to everyone yet i want to

00:21:00,159 --> 00:21:05,039
kind of see how it performs so

00:21:02,000 --> 00:21:07,120
i'm going to create a split of 99 of my

00:21:05,039 --> 00:21:09,039
traffic goes the default and one percent

00:21:07,120 --> 00:21:11,840
goes to the canary endpoint

00:21:09,039 --> 00:21:14,000
and so we'll see how that goes with that

00:21:11,840 --> 00:21:15,919
new model and then over time

00:21:14,000 --> 00:21:17,440
you know shift that and if things are

00:21:15,919 --> 00:21:19,200
going well the canary

00:21:17,440 --> 00:21:20,720
um you know the model becomes the

00:21:19,200 --> 00:21:22,799
default model

00:21:20,720 --> 00:21:25,760
so so it's a nice uh capability there

00:21:22,799 --> 00:21:28,480
for uh piloting new capabilities

00:21:25,760 --> 00:21:29,919
um and so once you're uh through that

00:21:28,480 --> 00:21:32,480
endpoint

00:21:29,919 --> 00:21:33,840
you have a concept of a transformer and

00:21:32,480 --> 00:21:36,880
the transformer

00:21:33,840 --> 00:21:39,520
works by taking the raw data

00:21:36,880 --> 00:21:39,919
that is passed in for a prediction and

00:21:39,520 --> 00:21:41,520
maybe

00:21:39,919 --> 00:21:42,960
you have to tweak it a little bit to go

00:21:41,520 --> 00:21:45,200
into your model so what i mean by that

00:21:42,960 --> 00:21:48,320
is this so um

00:21:45,200 --> 00:21:49,679
say your model is built in such a way

00:21:48,320 --> 00:21:52,720
that it expects

00:21:49,679 --> 00:21:55,039
a a month integer

00:21:52,720 --> 00:21:56,880
and a day integer and a year integer

00:21:55,039 --> 00:21:58,400
let's just say that's the way that

00:21:56,880 --> 00:22:00,320
those are the variables that are passed

00:21:58,400 --> 00:22:03,200
the model and the output is

00:22:00,320 --> 00:22:04,000
you know some prediction um but maybe

00:22:03,200 --> 00:22:06,799
your

00:22:04,000 --> 00:22:08,400
that's extra work for the apis and you

00:22:06,799 --> 00:22:10,720
simply just want

00:22:08,400 --> 00:22:12,320
folks to be able to pass in a date

00:22:10,720 --> 00:22:14,159
object and not have to

00:22:12,320 --> 00:22:16,159
you know kind of go through some of that

00:22:14,159 --> 00:22:18,240
um so you that's where the transformer

00:22:16,159 --> 00:22:19,919
might come in where it's a layer that

00:22:18,240 --> 00:22:21,280
does some pre-processing that might

00:22:19,919 --> 00:22:23,760
split that date

00:22:21,280 --> 00:22:24,559
into the components that your model

00:22:23,760 --> 00:22:27,600
expects

00:22:24,559 --> 00:22:30,799
and then you know basically you can

00:22:27,600 --> 00:22:32,400
use it from there and so

00:22:30,799 --> 00:22:34,240
most of the time you might do

00:22:32,400 --> 00:22:35,039
predictions with that and then you get

00:22:34,240 --> 00:22:37,440
the result

00:22:35,039 --> 00:22:38,400
and then in other cases you might do

00:22:37,440 --> 00:22:41,679
explanation

00:22:38,400 --> 00:22:44,080
an explanation is a capability to say

00:22:41,679 --> 00:22:45,679
why did the model do what it did what

00:22:44,080 --> 00:22:49,120
are the important

00:22:45,679 --> 00:22:51,039
what were the important factors in uh

00:22:49,120 --> 00:22:53,200
making the decision that i did so that's

00:22:51,039 --> 00:22:54,159
an optional capability that support as

00:22:53,200 --> 00:22:56,960
well

00:22:54,159 --> 00:22:58,159
so that's the serving component that's

00:22:56,960 --> 00:23:01,280
in the

00:22:58,159 --> 00:23:02,320
framework so now let's move on to uh

00:23:01,280 --> 00:23:04,880
pipelines and this

00:23:02,320 --> 00:23:05,760
is really going to be the focus of this

00:23:04,880 --> 00:23:08,240
talk from

00:23:05,760 --> 00:23:10,080
for the remainder of the time so

00:23:08,240 --> 00:23:12,640
pipelines allow you to build

00:23:10,080 --> 00:23:14,559
end-to-end workflows using those

00:23:12,640 --> 00:23:18,240
different components that we've

00:23:14,559 --> 00:23:21,919
been talking about so let's talk about

00:23:18,240 --> 00:23:24,240
how we might implement a pipeline so

00:23:21,919 --> 00:23:25,120
i'm going to go over a few code snippets

00:23:24,240 --> 00:23:28,400
these again

00:23:25,120 --> 00:23:31,840
come from the repo of examples

00:23:28,400 --> 00:23:34,480
so this particular snippet

00:23:31,840 --> 00:23:36,559
is for github issue summarization which

00:23:34,480 --> 00:23:39,760
i think is a really cool demo

00:23:36,559 --> 00:23:43,039
what it does is it

00:23:39,760 --> 00:23:44,080
looks at a github issue and it looks at

00:23:43,039 --> 00:23:46,159
that text

00:23:44,080 --> 00:23:48,640
and it gives you a summarization like

00:23:46,159 --> 00:23:51,360
maybe a one liner that tells you

00:23:48,640 --> 00:23:52,000
what was this issue about so it's you

00:23:51,360 --> 00:23:54,480
know this

00:23:52,000 --> 00:23:56,400
idea of text summarization which is a

00:23:54,480 --> 00:23:58,159
natural language capability machine

00:23:56,400 --> 00:24:01,279
learning

00:23:58,159 --> 00:24:02,960
and so how you create a pipeline with

00:24:01,279 --> 00:24:05,520
this there's a

00:24:02,960 --> 00:24:06,400
domain specific language basically a

00:24:05,520 --> 00:24:08,799
markup

00:24:06,400 --> 00:24:09,919
that your data scientist can add to

00:24:08,799 --> 00:24:11,760
their code

00:24:09,919 --> 00:24:13,840
to define the pipeline that's you're

00:24:11,760 --> 00:24:16,960
seeing here now a name

00:24:13,840 --> 00:24:18,640
a description and then and then you

00:24:16,960 --> 00:24:21,200
start to see the pipeline

00:24:18,640 --> 00:24:22,159
itself where you have parameters coming

00:24:21,200 --> 00:24:25,120
into it

00:24:22,159 --> 00:24:27,039
how many training steps do i want what

00:24:25,120 --> 00:24:29,039
is the directory for the data that i'm

00:24:27,039 --> 00:24:31,919
going to use and so on and so forth

00:24:29,039 --> 00:24:32,640
and this becomes the input to your

00:24:31,919 --> 00:24:35,919
pipeline

00:24:32,640 --> 00:24:37,360
that you define here okay so let's keep

00:24:35,919 --> 00:24:38,960
working through this so we defined a

00:24:37,360 --> 00:24:41,279
pipeline

00:24:38,960 --> 00:24:43,279
now let's look at the steps of that

00:24:41,279 --> 00:24:46,960
pipeline or the components

00:24:43,279 --> 00:24:49,039
so what we see here the first piece is

00:24:46,960 --> 00:24:50,080
you're importing the package for the

00:24:49,039 --> 00:24:52,640
components

00:24:50,080 --> 00:24:53,120
and then you're pointing to a descriptor

00:24:52,640 --> 00:24:55,760
so

00:24:53,120 --> 00:24:57,120
there's a yaml file that's out there in

00:24:55,760 --> 00:25:00,880
this case on the web on

00:24:57,120 --> 00:25:04,080
on github that defines uh

00:25:00,880 --> 00:25:06,320
the interface for this component and in

00:25:04,080 --> 00:25:09,120
this case the component is

00:25:06,320 --> 00:25:11,440
um simply it's an example a copy

00:25:09,120 --> 00:25:12,799
component it takes data from a source to

00:25:11,440 --> 00:25:15,120
a target

00:25:12,799 --> 00:25:16,880
and so the next step is you instantiate

00:25:15,120 --> 00:25:18,799
that component you have the

00:25:16,880 --> 00:25:21,120
description of it and now you're

00:25:18,799 --> 00:25:23,840
actually creating an instance of it

00:25:21,120 --> 00:25:25,440
the next thing i want to show is how do

00:25:23,840 --> 00:25:28,159
you create dependencies between

00:25:25,440 --> 00:25:29,200
components how does kubeflow pipelines

00:25:28,159 --> 00:25:32,240
know

00:25:29,200 --> 00:25:34,880
which step comes after which step so

00:25:32,240 --> 00:25:36,240
say we defined a log data component in a

00:25:34,880 --> 00:25:39,120
similar way

00:25:36,240 --> 00:25:41,120
we could say that the log data comes

00:25:39,120 --> 00:25:42,480
after the copy data components you can

00:25:41,120 --> 00:25:45,440
explicitly

00:25:42,480 --> 00:25:45,760
specify the order what you can also do

00:25:45,440 --> 00:25:48,880
though

00:25:45,760 --> 00:25:52,480
is it's smart and it can infer the order

00:25:48,880 --> 00:25:54,960
say if log data had as one of its inputs

00:25:52,480 --> 00:25:57,279
that output from copy data it would be

00:25:54,960 --> 00:26:00,159
able to wire those components together

00:25:57,279 --> 00:26:02,000
into that order so that you'd end up

00:26:00,159 --> 00:26:02,400
ultimately with something like as we saw

00:26:02,000 --> 00:26:04,480
in the

00:26:02,400 --> 00:26:05,440
this last screenshot that's how it

00:26:04,480 --> 00:26:08,640
figures out

00:26:05,440 --> 00:26:11,440
the order of operations either

00:26:08,640 --> 00:26:14,480
implicitly or explicitly in how you

00:26:11,440 --> 00:26:14,480
define the pipeline

00:26:14,880 --> 00:26:19,200
okay so now you've created your pipeline

00:26:17,679 --> 00:26:22,799
how do you deploy it

00:26:19,200 --> 00:26:24,720
so there is a package to compile your

00:26:22,799 --> 00:26:29,600
pipeline and what that does

00:26:24,720 --> 00:26:32,720
is it takes that python code

00:26:29,600 --> 00:26:35,760
and it turns it into a yaml file

00:26:32,720 --> 00:26:36,159
and then zips up that yaml file from

00:26:35,760 --> 00:26:38,400
there

00:26:36,159 --> 00:26:40,159
you can upload the pipeline

00:26:38,400 --> 00:26:42,400
programmatically or via the

00:26:40,159 --> 00:26:44,960
ui that we're seeing here and then now

00:26:42,400 --> 00:26:48,240
it's accessible in the dashboard so

00:26:44,960 --> 00:26:50,320
your team can now use that pipeline and

00:26:48,240 --> 00:26:51,520
um you know it's parameterized remember

00:26:50,320 --> 00:26:53,120
we depending on

00:26:51,520 --> 00:26:54,559
you can use that same pipeline in

00:26:53,120 --> 00:26:55,760
different contexts with different

00:26:54,559 --> 00:26:57,200
parameters now

00:26:55,760 --> 00:26:59,679
and this is what we're going to see in

00:26:57,200 --> 00:26:59,679
the demo

00:27:00,159 --> 00:27:05,279
okay so the component so we talked about

00:27:04,159 --> 00:27:08,320
the pipeline

00:27:05,279 --> 00:27:10,240
now how do you use components do you

00:27:08,320 --> 00:27:13,520
have to write them all yourself no

00:27:10,240 --> 00:27:16,559
there are lots of reusable components

00:27:13,520 --> 00:27:19,200
uh you know written you know

00:27:16,559 --> 00:27:21,679
for various cloud providers as well as

00:27:19,200 --> 00:27:23,520
just sort of standard components

00:27:21,679 --> 00:27:26,960
you see here one example on the right of

00:27:23,520 --> 00:27:29,440
a bigquery component if i needed to

00:27:26,960 --> 00:27:30,240
extract some data to use in my training

00:27:29,440 --> 00:27:32,320
job but

00:27:30,240 --> 00:27:33,600
i just put a list of on the left of all

00:27:32,320 --> 00:27:35,279
the different

00:27:33,600 --> 00:27:37,600
types of components that are already

00:27:35,279 --> 00:27:39,360
available and you can pass in

00:27:37,600 --> 00:27:40,799
you know the inputs and you can see what

00:27:39,360 --> 00:27:44,159
the outputs are they're

00:27:40,799 --> 00:27:46,880
you know well documented so it's a great

00:27:44,159 --> 00:27:46,880
place to start

00:27:47,120 --> 00:27:50,799
and i wanted to also add if you happen

00:27:49,760 --> 00:27:53,840
to be using

00:27:50,799 --> 00:27:56,960
uh tensorflow for uh development

00:27:53,840 --> 00:27:57,200
tensorflow extended is a great library

00:27:56,960 --> 00:27:59,760
of

00:27:57,200 --> 00:28:00,720
out of the box components for doing

00:27:59,760 --> 00:28:04,399
machine learning

00:28:00,720 --> 00:28:06,000
ops so the all of these components can

00:28:04,399 --> 00:28:07,600
be used in your workflow

00:28:06,000 --> 00:28:10,480
and we're going to actually walk through

00:28:07,600 --> 00:28:12,080
these in the demo section at the end

00:28:10,480 --> 00:28:13,679
so i won't go into too much detail right

00:28:12,080 --> 00:28:17,039
now

00:28:13,679 --> 00:28:17,440
and tfx can either be used through its

00:28:17,039 --> 00:28:19,919
own

00:28:17,440 --> 00:28:22,240
sdk or through that standard kubeflow

00:28:19,919 --> 00:28:25,440
sdk that i just

00:28:22,240 --> 00:28:28,799
showed how to use okay

00:28:25,440 --> 00:28:30,320
all right so let's briefly talk about

00:28:28,799 --> 00:28:31,679
how you would build your own custom

00:28:30,320 --> 00:28:33,039
component i know there's a lot of

00:28:31,679 --> 00:28:34,720
information here but i just want to

00:28:33,039 --> 00:28:37,679
really illustrate there's

00:28:34,720 --> 00:28:39,600
three steps required and hopefully you

00:28:37,679 --> 00:28:40,320
can see how it all fits together first

00:28:39,600 --> 00:28:42,399
is

00:28:40,320 --> 00:28:43,440
writing your descriptor that's what we

00:28:42,399 --> 00:28:45,200
have here

00:28:43,440 --> 00:28:47,919
what's the name of the component it's

00:28:45,200 --> 00:28:49,760
description it's inputs outputs

00:28:47,919 --> 00:28:51,120
and then a key part here to link it

00:28:49,760 --> 00:28:54,720
together is

00:28:51,120 --> 00:28:57,679
the link to the container image so

00:28:54,720 --> 00:28:58,720
where can i actually execute this code

00:28:57,679 --> 00:29:00,240
and it points here

00:28:58,720 --> 00:29:02,960
for example to the google container

00:29:00,240 --> 00:29:03,840
register so this is the description of

00:29:02,960 --> 00:29:08,080
your component

00:29:03,840 --> 00:29:12,240
by yaml the next part is a docker file

00:29:08,080 --> 00:29:15,440
so that that this describes

00:29:12,240 --> 00:29:16,559
what's in your image so you might start

00:29:15,440 --> 00:29:19,360
from a base

00:29:16,559 --> 00:29:20,559
uh say a tensorflow with a certain

00:29:19,360 --> 00:29:22,960
version a gpu

00:29:20,559 --> 00:29:25,360
you might update that image a little bit

00:29:22,960 --> 00:29:28,320
you might install a couple more packages

00:29:25,360 --> 00:29:30,399
and then you specify the entry point

00:29:28,320 --> 00:29:32,000
here is the code within the image

00:29:30,399 --> 00:29:33,760
that i'm actually going to run my

00:29:32,000 --> 00:29:37,520
component with

00:29:33,760 --> 00:29:40,880
okay and then finally the code itself

00:29:37,520 --> 00:29:41,279
so that this is that entry point into

00:29:40,880 --> 00:29:44,000
the

00:29:41,279 --> 00:29:45,520
image and here's where our code might do

00:29:44,000 --> 00:29:48,480
something like

00:29:45,520 --> 00:29:50,159
parse the different arguments you know

00:29:48,480 --> 00:29:52,880
like you saw in our

00:29:50,159 --> 00:29:54,399
python script before the data directory

00:29:52,880 --> 00:29:57,120
the number of steps

00:29:54,399 --> 00:29:58,320
so on and so forth you might parse those

00:29:57,120 --> 00:29:59,760
and then

00:29:58,320 --> 00:30:01,360
here's where you're actually running the

00:29:59,760 --> 00:30:05,039
code

00:30:01,360 --> 00:30:08,240
you're going to run some sort of action

00:30:05,039 --> 00:30:10,799
and then you're going to set the output

00:30:08,240 --> 00:30:12,799
and so this is what's really happening

00:30:10,799 --> 00:30:15,760
under the hood

00:30:12,799 --> 00:30:16,480
okay so to summarize what we've seen so

00:30:15,760 --> 00:30:19,520
far

00:30:16,480 --> 00:30:20,240
kubeflow is a cloud native a multi-cloud

00:30:19,520 --> 00:30:23,520
solution for

00:30:20,240 --> 00:30:24,960
ml it allows you to build pipelines

00:30:23,520 --> 00:30:26,799
and we're going to see a demo of that

00:30:24,960 --> 00:30:28,320
shortly and

00:30:26,799 --> 00:30:30,799
if you have kubernetes you can run

00:30:28,320 --> 00:30:34,480
kubeflow it's a platform that runs

00:30:30,799 --> 00:30:38,000
nicely on top of kubernetes

00:30:34,480 --> 00:30:41,279
okay and kubeflow is an open community

00:30:38,000 --> 00:30:42,640
uh with all kinds of ideas from many

00:30:41,279 --> 00:30:46,000
different places

00:30:42,640 --> 00:30:49,360
um and uh if you want to find out more

00:30:46,000 --> 00:30:51,200
here are some resources uh kubeflow.org

00:30:49,360 --> 00:30:53,440
is probably the best place to start it

00:30:51,200 --> 00:30:57,679
has links to all these other

00:30:53,440 --> 00:30:58,320
resources all right so let's spend a few

00:30:57,679 --> 00:31:01,039
minutes

00:30:58,320 --> 00:31:02,320
going over a demo this is using

00:31:01,039 --> 00:31:05,279
screenshots

00:31:02,320 --> 00:31:05,919
but just imagine that uh i have my web

00:31:05,279 --> 00:31:08,080
browser

00:31:05,919 --> 00:31:09,120
uh running while i'm going through this

00:31:08,080 --> 00:31:12,000
um

00:31:09,120 --> 00:31:12,720
and i apologize if it's a little uh hard

00:31:12,000 --> 00:31:14,559
to read

00:31:12,720 --> 00:31:16,000
uh you know so you might want to

00:31:14,559 --> 00:31:19,919
maximize your window if you are

00:31:16,000 --> 00:31:20,960
already so here is a user interface to

00:31:19,919 --> 00:31:23,519
see

00:31:20,960 --> 00:31:24,960
your pipeline instances in the cloud so

00:31:23,519 --> 00:31:28,080
you can actually see

00:31:24,960 --> 00:31:32,000
uh this happens to be on google cloud

00:31:28,080 --> 00:31:34,320
but you see that there is a a list of

00:31:32,000 --> 00:31:35,760
different uh instances you know pointing

00:31:34,320 --> 00:31:38,159
to a cluster

00:31:35,760 --> 00:31:40,559
and a version of kubeflow pipelines and

00:31:38,159 --> 00:31:41,760
a link to directly open that pipeline's

00:31:40,559 --> 00:31:45,519
dashboard

00:31:41,760 --> 00:31:47,840
uh from the cloud so say we clicked on

00:31:45,519 --> 00:31:49,279
that link and we opened the dashboard

00:31:47,840 --> 00:31:51,519
so here's what you would see you would

00:31:49,279 --> 00:31:52,960
see this getting started page and a set

00:31:51,519 --> 00:31:55,919
of different

00:31:52,960 --> 00:31:57,519
options here on the left side and let's

00:31:55,919 --> 00:32:01,039
walk through those

00:31:57,519 --> 00:32:02,159
okay so here is the list of pipelines

00:32:01,039 --> 00:32:04,880
that you would see

00:32:02,159 --> 00:32:05,919
um if you recall i showed how to build a

00:32:04,880 --> 00:32:07,519
pipeline

00:32:05,919 --> 00:32:09,279
and here's where you would see if you

00:32:07,519 --> 00:32:11,120
uploaded a pipeline and you can see in

00:32:09,279 --> 00:32:12,720
the top right corner there's a link to

00:32:11,120 --> 00:32:14,000
upload a pipeline here's where you'd do

00:32:12,720 --> 00:32:14,799
that you'd say okay i've built my

00:32:14,000 --> 00:32:16,720
pipeline

00:32:14,799 --> 00:32:19,440
let me now make it available to the rest

00:32:16,720 --> 00:32:21,760
of my team here

00:32:19,440 --> 00:32:23,440
okay and so now what we're going to do

00:32:21,760 --> 00:32:26,880
is walk through

00:32:23,440 --> 00:32:29,679
an example of the chicago

00:32:26,880 --> 00:32:31,360
taxicab data set this is a well-known

00:32:29,679 --> 00:32:33,360
data set

00:32:31,360 --> 00:32:34,399
where we're it's a what's called a

00:32:33,360 --> 00:32:37,760
classification

00:32:34,399 --> 00:32:39,519
problem we're looking at the tips

00:32:37,760 --> 00:32:42,559
that have been provided the taxi cab

00:32:39,519 --> 00:32:43,519
drivers and trying to predict based on

00:32:42,559 --> 00:32:45,679
you know

00:32:43,519 --> 00:32:47,039
miles driven and all these different

00:32:45,679 --> 00:32:49,919
factors in there

00:32:47,039 --> 00:32:51,120
is the tip going to be greater or less

00:32:49,919 --> 00:32:53,360
than 20 percent

00:32:51,120 --> 00:32:54,559
uh so that's you can find out more about

00:32:53,360 --> 00:32:56,799
this model uh

00:32:54,559 --> 00:32:59,360
you know in this example here and i'll

00:32:56,799 --> 00:33:02,720
give you a link at the very end

00:32:59,360 --> 00:33:04,720
okay so here's the graph um that you'd

00:33:02,720 --> 00:33:07,279
see if you wanted to look at this

00:33:04,720 --> 00:33:09,440
demo and by the way this is already

00:33:07,279 --> 00:33:11,440
built into kubeflow pipelines it's one

00:33:09,440 --> 00:33:12,399
of the examples already provided so if

00:33:11,440 --> 00:33:14,399
you just

00:33:12,399 --> 00:33:16,720
fire up a pipelines instance you would

00:33:14,399 --> 00:33:19,440
see this as one of the examples

00:33:16,720 --> 00:33:21,440
so this is the graph it's doesn't have

00:33:19,440 --> 00:33:23,200
any status because it's just simply the

00:33:21,440 --> 00:33:25,840
abstraction it's not an actual

00:33:23,200 --> 00:33:26,559
run of the graph it's a description of

00:33:25,840 --> 00:33:29,919
the steps

00:33:26,559 --> 00:33:32,080
so we haven't actually run it yet okay

00:33:29,919 --> 00:33:33,200
so now let's look at if we were to start

00:33:32,080 --> 00:33:35,360
a run

00:33:33,200 --> 00:33:36,559
you would fill out some information here

00:33:35,360 --> 00:33:39,679
you can see

00:33:36,559 --> 00:33:42,080
what's the name of it um a description

00:33:39,679 --> 00:33:43,600
of my run

00:33:42,080 --> 00:33:45,440
and then you can even specify some

00:33:43,600 --> 00:33:47,440
things like is this a one-off

00:33:45,440 --> 00:33:50,840
or is this a recurring run where i want

00:33:47,440 --> 00:33:53,760
to run it monthly or something like that

00:33:50,840 --> 00:33:57,440
um so uh

00:33:53,760 --> 00:34:00,960
let's go uh to the next slide here

00:33:57,440 --> 00:34:04,240
um and so here's where we see a um

00:34:00,960 --> 00:34:06,240
a list of your runs and so you'd see um

00:34:04,240 --> 00:34:08,399
an existing run and then a run that's in

00:34:06,240 --> 00:34:11,280
progress how many minutes it it

00:34:08,399 --> 00:34:12,960
takes so if you're if you use like ci cd

00:34:11,280 --> 00:34:14,079
tools in the past you know kind of looks

00:34:12,960 --> 00:34:17,679
a little similar you know

00:34:14,079 --> 00:34:20,079
how did my uh you know jenkins job uh go

00:34:17,679 --> 00:34:21,760
you know was it successful or not and

00:34:20,079 --> 00:34:23,200
seeing a status and length and things

00:34:21,760 --> 00:34:26,000
like that

00:34:23,200 --> 00:34:26,960
um so here's in progress execution so

00:34:26,000 --> 00:34:29,200
you see

00:34:26,960 --> 00:34:30,960
um you know here are the steps that have

00:34:29,200 --> 00:34:33,200
been performed so far

00:34:30,960 --> 00:34:35,359
and then what's still to be done in my

00:34:33,200 --> 00:34:38,079
graph

00:34:35,359 --> 00:34:39,200
so if i click on an individual step on

00:34:38,079 --> 00:34:42,399
the graph

00:34:39,200 --> 00:34:43,520
then i see on the right side a variety

00:34:42,399 --> 00:34:44,560
of different things like what were the

00:34:43,520 --> 00:34:47,119
artifacts the

00:34:44,560 --> 00:34:48,879
inputs the outputs things of that nature

00:34:47,119 --> 00:34:51,440
there so it's a way to really drill down

00:34:48,879 --> 00:34:54,560
into what's happening

00:34:51,440 --> 00:34:55,599
so i my first step was it's called

00:34:54,560 --> 00:34:58,560
example gen

00:34:55,599 --> 00:35:00,400
i'm extracting from a csv file into

00:34:58,560 --> 00:35:04,079
examples or records

00:35:00,400 --> 00:35:06,960
this next step here is the um

00:35:04,079 --> 00:35:08,960
statistics gen so what this does these

00:35:06,960 --> 00:35:11,599
by the way these are the tfx's

00:35:08,960 --> 00:35:14,480
tensorflow extended built-in components

00:35:11,599 --> 00:35:16,400
so i can then get a picture of the data

00:35:14,480 --> 00:35:18,000
coming into my model

00:35:16,400 --> 00:35:21,119
and i think on the next screenshot we're

00:35:18,000 --> 00:35:22,880
going to see if i zoom in a little bit

00:35:21,119 --> 00:35:25,520
what are the features in my model so you

00:35:22,880 --> 00:35:28,800
see a nice like summary statistics

00:35:25,520 --> 00:35:32,000
uh how many rows were null uh

00:35:28,800 --> 00:35:34,560
you know of a particular column um

00:35:32,000 --> 00:35:36,079
what were the mean the min the max

00:35:34,560 --> 00:35:38,160
things of that nature what's the

00:35:36,079 --> 00:35:39,920
data distribution so allows you at a

00:35:38,160 --> 00:35:43,839
glance to see

00:35:39,920 --> 00:35:45,760
uh the nate with the data in your job

00:35:43,839 --> 00:35:46,960
the next thing is a component called

00:35:45,760 --> 00:35:49,200
schema gen

00:35:46,960 --> 00:35:50,800
and here's where you can look at the

00:35:49,200 --> 00:35:53,200
data coming in your model and think of

00:35:50,800 --> 00:35:56,400
this too as the guard rails of ensuring

00:35:53,200 --> 00:35:59,040
as a way to see is the data

00:35:56,400 --> 00:36:00,560
um you know whether it's nullable or not

00:35:59,040 --> 00:36:02,079
or what the columns are ensuring that's

00:36:00,560 --> 00:36:04,960
the same every time

00:36:02,079 --> 00:36:08,160
so this can infer what the schema is

00:36:04,960 --> 00:36:08,160
from your training set

00:36:08,240 --> 00:36:12,240
now the next step this is pretty cool

00:36:10,240 --> 00:36:12,640
this is the data validator and it can

00:36:12,240 --> 00:36:15,839
use

00:36:12,640 --> 00:36:17,920
the inferred schema as new records are

00:36:15,839 --> 00:36:19,680
coming in in this case the test set

00:36:17,920 --> 00:36:21,599
the holdout set that we're trying to see

00:36:19,680 --> 00:36:24,160
how useful the model is

00:36:21,599 --> 00:36:24,720
we're seeing a couple anomalies so like

00:36:24,160 --> 00:36:26,880
you know

00:36:24,720 --> 00:36:27,920
for one for one example we see the

00:36:26,880 --> 00:36:29,680
payment type

00:36:27,920 --> 00:36:31,359
you know if you you know again it's a

00:36:29,680 --> 00:36:32,880
little hard to read but on the last

00:36:31,359 --> 00:36:34,720
screen we saw oh there's like six

00:36:32,880 --> 00:36:36,960
different payment types credit card

00:36:34,720 --> 00:36:38,640
check things of that nature so here

00:36:36,960 --> 00:36:40,640
there was a new payment type that was

00:36:38,640 --> 00:36:42,640
never seen before and so now as we're

00:36:40,640 --> 00:36:44,720
evaluating this we can say

00:36:42,640 --> 00:36:46,640
okay well maybe our schema needs to be

00:36:44,720 --> 00:36:49,760
broadened a little bit this is valid

00:36:46,640 --> 00:36:51,920
or okay well this is outside the bounds

00:36:49,760 --> 00:36:52,880
and the you know these records should be

00:36:51,920 --> 00:36:54,880
uh rejected

00:36:52,880 --> 00:36:56,720
uh and there's some issue that we need

00:36:54,880 --> 00:36:58,960
to resolve that we shouldn't be

00:36:56,720 --> 00:37:00,320
seeing that payment type so it's a great

00:36:58,960 --> 00:37:03,520
way to kind of validate

00:37:00,320 --> 00:37:05,920
the data coming into your model okay

00:37:03,520 --> 00:37:07,280
um there's a transformer step and i'm

00:37:05,920 --> 00:37:08,880
going to move a little bit quick to make

00:37:07,280 --> 00:37:09,760
sure we have enough time for questions

00:37:08,880 --> 00:37:12,000
here

00:37:09,760 --> 00:37:13,920
a transformer step as we talked about if

00:37:12,000 --> 00:37:17,119
we need to change the data

00:37:13,920 --> 00:37:19,200
we can do that you can see now here's

00:37:17,119 --> 00:37:20,880
the actual training so as your model is

00:37:19,200 --> 00:37:22,560
being built if you want to see like the

00:37:20,880 --> 00:37:25,200
tensorflow logs

00:37:22,560 --> 00:37:26,880
the loss going down over time and you

00:37:25,200 --> 00:37:28,320
can click on that any of these steps you

00:37:26,880 --> 00:37:30,720
see what's inside

00:37:28,320 --> 00:37:33,119
of the container with the with the

00:37:30,720 --> 00:37:33,119
logging

00:37:33,359 --> 00:37:37,760
then there's a model analysis step where

00:37:35,760 --> 00:37:38,880
you can see a graph of how well your

00:37:37,760 --> 00:37:41,200
model is performing

00:37:38,880 --> 00:37:42,880
seeing statistics and seeing it sliced

00:37:41,200 --> 00:37:46,480
in different ways ensure it's

00:37:42,880 --> 00:37:46,480
working well for all of your users

00:37:46,800 --> 00:37:50,240
and near the end here is called the

00:37:49,119 --> 00:37:53,599
pusher step

00:37:50,240 --> 00:37:54,320
so you can have logic to say if this

00:37:53,599 --> 00:37:56,400
model is

00:37:54,320 --> 00:37:58,000
better than my previous model if it has

00:37:56,400 --> 00:38:00,960
a higher accuracy

00:37:58,000 --> 00:38:01,599
it can be so-called blessed a blessed

00:38:00,960 --> 00:38:04,320
model

00:38:01,599 --> 00:38:05,119
and then it can be pushed to production

00:38:04,320 --> 00:38:07,359
and so

00:38:05,119 --> 00:38:08,640
this is a way to kind of have some rules

00:38:07,359 --> 00:38:10,800
around

00:38:08,640 --> 00:38:14,079
you know pushing only what you want to

00:38:10,800 --> 00:38:14,079
into your production system

00:38:14,560 --> 00:38:18,560
so and then finally to sort of wrap it

00:38:16,400 --> 00:38:21,599
up everything you've done here

00:38:18,560 --> 00:38:22,160
you see an artifact list so the data the

00:38:21,599 --> 00:38:24,000
model

00:38:22,160 --> 00:38:27,440
it'll point to a storage bucket where

00:38:24,000 --> 00:38:28,960
you can take a deeper look at that

00:38:27,440 --> 00:38:30,960
you can see an example your model here's

00:38:28,960 --> 00:38:32,000
where metadata management comes in where

00:38:30,960 --> 00:38:34,320
you see

00:38:32,000 --> 00:38:35,839
you know the state of the model and

00:38:34,320 --> 00:38:37,359
other you know you can

00:38:35,839 --> 00:38:39,839
standard metadata as well as custom

00:38:37,359 --> 00:38:41,599
metadata that you can put in here

00:38:39,839 --> 00:38:44,000
there is a lineage explorer where you

00:38:41,599 --> 00:38:47,280
can see all the different steps and data

00:38:44,000 --> 00:38:49,359
how they're linked together

00:38:47,280 --> 00:38:51,000
if you want to try out this tutorial

00:38:49,359 --> 00:38:53,680
here's a link to it

00:38:51,000 --> 00:38:55,359
tensorflow.org and then you go into tfx

00:38:53,680 --> 00:38:59,200
or tensorflow extended

00:38:55,359 --> 00:39:02,560
there's a nice tutorial that runs in 45

00:38:59,200 --> 00:39:05,520
to 60 minutes it uses the free tier

00:39:02,560 --> 00:39:05,920
so you know no charge to try that it's a

00:39:05,520 --> 00:39:08,000
great

00:39:05,920 --> 00:39:09,680
i think end-to-end example to get

00:39:08,000 --> 00:39:13,200
started

00:39:09,680 --> 00:39:14,160
okay so that was it i have about a

00:39:13,200 --> 00:39:17,280
minute to answer

00:39:14,160 --> 00:39:19,280
questions sorry that it was uh there's

00:39:17,280 --> 00:39:21,920
not a ton of time

00:39:19,280 --> 00:39:23,920
but what i will do is i will go to the

00:39:21,920 --> 00:39:25,920
slack channel

00:39:23,920 --> 00:39:28,640
after this talk and then we can get into

00:39:25,920 --> 00:39:31,839
more detail

00:39:28,640 --> 00:39:31,839
so um

00:39:33,839 --> 00:39:38,320
so let's start with uh what like a quick

00:39:36,400 --> 00:39:39,599
deployment questions i think it answers

00:39:38,320 --> 00:39:41,359
a couple of the questions

00:39:39,599 --> 00:39:43,040
that was asked what are the best ways to

00:39:41,359 --> 00:39:44,320
deploy kubeflow in a self-managed

00:39:43,040 --> 00:39:46,400
kubernetes cluster

00:39:44,320 --> 00:39:48,720
so in the installation there are there

00:39:46,400 --> 00:39:51,440
are basically two steps one is

00:39:48,720 --> 00:39:53,680
to create the cluster uh for that cloud

00:39:51,440 --> 00:39:55,920
environment the second is to

00:39:53,680 --> 00:39:57,119
lay down the kubeflow services so you

00:39:55,920 --> 00:40:00,560
can decide

00:39:57,119 --> 00:40:03,760
um hey i've got an existing cluster just

00:40:00,560 --> 00:40:04,319
uh use the yaml that installs the uh

00:40:03,760 --> 00:40:06,160
kubrick

00:40:04,319 --> 00:40:07,920
kubels services itself so that allows

00:40:06,160 --> 00:40:10,720
you in a on-prem

00:40:07,920 --> 00:40:12,400
or a cloud environment in different ways

00:40:10,720 --> 00:40:17,200
to use this so i think i'm

00:40:12,400 --> 00:40:19,440
out of time i believe um

00:40:17,200 --> 00:40:20,720
unless the moderators say here i have

00:40:19,440 --> 00:40:24,560
some more time i

00:40:20,720 --> 00:40:27,520
i think i'll wrap up

00:40:24,560 --> 00:40:29,680
um i really appreciate everyone's time

00:40:27,520 --> 00:40:31,200
thank you so much uh and again i'm gonna

00:40:29,680 --> 00:40:33,200
move to the slack channel

00:40:31,200 --> 00:40:35,119
to answer all the questions uh

00:40:33,200 --> 00:40:37,359
appreciate all the questions glad that

00:40:35,119 --> 00:40:39,200
uh it was a useful session for you all

00:40:37,359 --> 00:40:50,800
and have a great rest your day

00:40:39,200 --> 00:40:50,800

YouTube URL: https://www.youtube.com/watch?v=ARXrMvhT2os


