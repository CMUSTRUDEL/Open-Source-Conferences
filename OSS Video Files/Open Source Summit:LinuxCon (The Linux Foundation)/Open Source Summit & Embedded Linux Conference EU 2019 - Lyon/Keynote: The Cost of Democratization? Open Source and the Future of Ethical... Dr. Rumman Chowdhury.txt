Title: Keynote: The Cost of Democratization? Open Source and the Future of Ethical... Dr. Rumman Chowdhury
Publication date: 2019-10-28
Playlist: Open Source Summit & Embedded Linux Conference EU 2019 - Lyon
Description: 
	Keynote: The Cost of Democratization? Open Source and the Future of Ethical Use - Dr. Rumman Chowdhury, Global Lead for Responsible AI, Accenture Applied Intelligence
Captions: 
	00:00:00,030 --> 00:00:04,770
thank you and good morning thanks for

00:00:01,949 --> 00:00:07,620
coming out in this rainy weather so as a

00:00:04,770 --> 00:00:11,099
bit of a possibly heart pivot from the

00:00:07,620 --> 00:00:12,570
the previous few talks as mentioned my

00:00:11,099 --> 00:00:14,549
name is dr. Ahmad Chaudhry and I'm the

00:00:12,570 --> 00:00:17,520
global lead for responsible AI at

00:00:14,549 --> 00:00:19,320
Accenture Applied intelligence in my job

00:00:17,520 --> 00:00:21,029
I work with clients I actually work with

00:00:19,320 --> 00:00:22,890
academics and I work with folks like

00:00:21,029 --> 00:00:25,140
yourself I'm thinking through the

00:00:22,890 --> 00:00:27,390
ethical implications of the technology

00:00:25,140 --> 00:00:29,099
we launched so I specifically at the

00:00:27,390 --> 00:00:30,390
moment focus on artificial intelligence

00:00:29,099 --> 00:00:33,570
but there are questions being raised

00:00:30,390 --> 00:00:35,730
about blockchain quantum and a lot of

00:00:33,570 --> 00:00:38,850
the emerging technologies that are going

00:00:35,730 --> 00:00:40,469
to significantly shape Society so I'm

00:00:38,850 --> 00:00:41,790
here to talk a bit about open source in

00:00:40,469 --> 00:00:43,620
the future of ethical use and the

00:00:41,790 --> 00:00:45,270
question that a lot of us in the

00:00:43,620 --> 00:00:49,950
industry are grappling with at the

00:00:45,270 --> 00:00:53,610
moment so this innocuous seeming blog

00:00:49,950 --> 00:00:57,390
post is by open AI if you're unfamiliar

00:00:53,610 --> 00:01:00,840
open AI is a previously nonprofit now

00:00:57,390 --> 00:01:04,070
capped profit startup organization in

00:01:00,840 --> 00:01:06,900
Silicon Valley ultimately dedicated to

00:01:04,070 --> 00:01:09,570
solving the problem of finding

00:01:06,900 --> 00:01:11,460
artificial general intelligence so this

00:01:09,570 --> 00:01:14,909
blog post is about something they

00:01:11,460 --> 00:01:17,700
created called GPT - this was posted in

00:01:14,909 --> 00:01:20,729
February and when they did it was met

00:01:17,700 --> 00:01:24,000
with a lot of controversy so why well

00:01:20,729 --> 00:01:26,310
when they created GPT - it was a natural

00:01:24,000 --> 00:01:28,710
language processing text generating

00:01:26,310 --> 00:01:31,320
model that could based on only a couple

00:01:28,710 --> 00:01:33,479
of sentences create a very realistic

00:01:31,320 --> 00:01:36,390
narrative now this could be used to

00:01:33,479 --> 00:01:39,299
create fake news fake media fake blog

00:01:36,390 --> 00:01:40,890
posts fake articles and all you had to

00:01:39,299 --> 00:01:43,920
do was actually feed it one or two

00:01:40,890 --> 00:01:45,270
sentences and this is one of the

00:01:43,920 --> 00:01:47,100
questions we come up with an artificial

00:01:45,270 --> 00:01:50,220
intelligence all the time what we build

00:01:47,100 --> 00:01:52,590
is a multi-purpose tool so something

00:01:50,220 --> 00:01:56,100
like GPT - can be used for fun for

00:01:52,590 --> 00:01:58,979
example The Economist had a contest and

00:01:56,100 --> 00:02:02,399
somebody replied with a GPT - created

00:01:58,979 --> 00:02:03,840
answer they didn't win but it was

00:02:02,399 --> 00:02:04,320
interesting to see how realistic it

00:02:03,840 --> 00:02:06,090
looked

00:02:04,320 --> 00:02:07,829
we have journalists playing around with

00:02:06,090 --> 00:02:10,319
it as well for similar reason so it

00:02:07,829 --> 00:02:11,940
creates these coherent paragraphs but at

00:02:10,319 --> 00:02:12,569
the same time we can see how this could

00:02:11,940 --> 00:02:14,280
be

00:02:12,569 --> 00:02:16,790
used and for example in the upcoming

00:02:14,280 --> 00:02:20,549
election the United States to create

00:02:16,790 --> 00:02:23,700
fake media fake news so what they

00:02:20,549 --> 00:02:25,980
decided to do was hold back rollout they

00:02:23,700 --> 00:02:27,629
did not open-source this product the way

00:02:25,980 --> 00:02:28,980
they do with everything else they make

00:02:27,629 --> 00:02:31,319
and they said well you know what we

00:02:28,980 --> 00:02:33,060
created this thing but oh my gosh it's

00:02:31,319 --> 00:02:34,530
so scary we're just gonna tell you that

00:02:33,060 --> 00:02:37,469
it exists and we're not we're not

00:02:34,530 --> 00:02:39,419
releasing it to the world now this was

00:02:37,469 --> 00:02:42,090
not met with a lot of positivity in the

00:02:39,419 --> 00:02:43,680
community as you all know obviously our

00:02:42,090 --> 00:02:46,680
community is built on democratization

00:02:43,680 --> 00:02:48,540
it's built on sharing of materials and

00:02:46,680 --> 00:02:52,439
in a few articles we really saw that

00:02:48,540 --> 00:02:54,060
pushback so in in you know in the upper

00:02:52,439 --> 00:02:57,480
corner we're seeing some quotes by

00:02:54,060 --> 00:02:58,980
frustrate UC Berkeley who said that you

00:02:57,480 --> 00:03:00,329
know they've got a lot of money they've

00:02:58,980 --> 00:03:02,849
got a lot of parlor tricks

00:03:00,329 --> 00:03:04,230
calling what they did hyperbolic that it

00:03:02,849 --> 00:03:06,719
was very extreme that it was meant to

00:03:04,230 --> 00:03:08,790
drive media and actually the bottom post

00:03:06,719 --> 00:03:11,280
is by dr. Brit Paris of Rutgers

00:03:08,790 --> 00:03:13,889
University who writes who actually

00:03:11,280 --> 00:03:16,139
entitled her blog post from panic to

00:03:13,889 --> 00:03:18,930
profit and basically said this was

00:03:16,139 --> 00:03:20,970
actually a big move for them to get more

00:03:18,930 --> 00:03:22,319
hype around what they do because that is

00:03:20,970 --> 00:03:23,909
a big fear in this industry there's so

00:03:22,319 --> 00:03:26,489
much hype like what is real what isn't

00:03:23,909 --> 00:03:28,620
real and what Britt was saying which is

00:03:26,489 --> 00:03:30,569
you know which is a valid critique is

00:03:28,620 --> 00:03:32,370
that well you're gonna tell us it's bad

00:03:30,569 --> 00:03:35,220
you're not gonna tell us what it is and

00:03:32,370 --> 00:03:37,229
you're gonna say you get to control who

00:03:35,220 --> 00:03:40,169
use this technology and how they use it

00:03:37,229 --> 00:03:41,759
and by the way she pointed out this is

00:03:40,169 --> 00:03:44,729
also how some of the biggest companies

00:03:41,759 --> 00:03:46,739
in the Cold War got their massive

00:03:44,729 --> 00:03:48,900
lucrative media contracts by creating

00:03:46,739 --> 00:03:50,970
this mystique this hype and getting the

00:03:48,900 --> 00:03:53,009
government to fund them getting DARPA

00:03:50,970 --> 00:03:55,290
etc so what she's calling out is what

00:03:53,009 --> 00:03:57,299
could be a brilliant marketing move but

00:03:55,290 --> 00:03:59,819
at the same time what this is really

00:03:57,299 --> 00:04:02,159
adjusting is this problem that we have

00:03:59,819 --> 00:04:06,150
an industry well we actually don't know

00:04:02,159 --> 00:04:08,189
how to address these issues GPT two

00:04:06,150 --> 00:04:10,680
aside what we saw earlier this year is

00:04:08,189 --> 00:04:14,250
an open source technology of deep fakes

00:04:10,680 --> 00:04:16,500
being used to create deep nudes now if

00:04:14,250 --> 00:04:18,630
you didn't hear about this this was an

00:04:16,500 --> 00:04:21,269
entrepreneurial individual who decided

00:04:18,630 --> 00:04:23,940
to charge $50 for the ability to take

00:04:21,269 --> 00:04:25,760
any photograph of any woman

00:04:23,940 --> 00:04:28,070
interestingly not men

00:04:25,760 --> 00:04:30,710
only women as he claimed there were just

00:04:28,070 --> 00:04:33,410
more pictures of naked women online and

00:04:30,710 --> 00:04:34,790
pretty much undress her in the photo the

00:04:33,410 --> 00:04:37,820
original article which is broken by

00:04:34,790 --> 00:04:40,250
Samko latvia she points out that you

00:04:37,820 --> 00:04:42,680
know they downloaded it you can run it

00:04:40,250 --> 00:04:44,570
like a Windows application it's super

00:04:42,680 --> 00:04:46,250
easy you don't you don't have to have

00:04:44,570 --> 00:04:48,020
advanced programming knowledge you don't

00:04:46,250 --> 00:04:50,090
have to actually do a massive install

00:04:48,020 --> 00:04:51,440
you just paid this dude 50 bucks got

00:04:50,090 --> 00:04:53,570
this thing on your computer put in

00:04:51,440 --> 00:04:56,090
whatever picture you wanted of any woman

00:04:53,570 --> 00:04:59,540
you knew and came out with a fairly

00:04:56,090 --> 00:05:01,730
realistic nude of her now Sam's work

00:04:59,540 --> 00:05:04,820
she's been doing this since about 2017

00:05:01,730 --> 00:05:05,900
she's been chasing deep fakes and sadly

00:05:04,820 --> 00:05:07,970
but presciently

00:05:05,900 --> 00:05:10,820
which he had always said was deep fakes

00:05:07,970 --> 00:05:12,890
is not actually used as much for fake

00:05:10,820 --> 00:05:15,410
media what is you it is used to do is

00:05:12,890 --> 00:05:17,240
create fake revenge porn so the number

00:05:15,410 --> 00:05:19,820
one use of deep fakes is actually to do

00:05:17,240 --> 00:05:21,980
exactly this so people to go in and

00:05:19,820 --> 00:05:24,020
create images of their ex-girlfriends

00:05:21,980 --> 00:05:27,710
girls they don't women they don't like

00:05:24,020 --> 00:05:29,780
their bosses and shame them publicly and

00:05:27,710 --> 00:05:32,060
embarrass them of the most fundamental

00:05:29,780 --> 00:05:34,370
way possible and this app just made it

00:05:32,060 --> 00:05:36,620
freely available and this individual was

00:05:34,370 --> 00:05:38,930
able to make it using something that was

00:05:36,620 --> 00:05:41,390
open-source so clearly we have this

00:05:38,930 --> 00:05:43,760
issue so you know critique aside when

00:05:41,390 --> 00:05:45,560
open AI was trying to do is answer a

00:05:43,760 --> 00:05:47,510
problem that we have an industry today

00:05:45,560 --> 00:05:49,940
we create these multi-purpose tools

00:05:47,510 --> 00:05:52,160
these tools have very specific very

00:05:49,940 --> 00:05:54,830
significant societal impact that we

00:05:52,160 --> 00:05:56,600
actually cannot ignore and even the

00:05:54,830 --> 00:05:59,870
individual who created deep nudes he got

00:05:56,600 --> 00:06:02,240
he didn't he didn't really have overtly

00:05:59,870 --> 00:06:05,300
malicious intent he didn't create it in

00:06:02,240 --> 00:06:08,540
in with the intent of doing harm

00:06:05,300 --> 00:06:09,520
I suppose it seemed like he was I don't

00:06:08,540 --> 00:06:12,500
know

00:06:09,520 --> 00:06:14,060
naive about it about what it might do

00:06:12,500 --> 00:06:17,030
and the impact it might have I don't

00:06:14,060 --> 00:06:19,280
know but then how can we reconcile this

00:06:17,030 --> 00:06:21,650
right so as I said the reason why open a

00:06:19,280 --> 00:06:23,900
eyes response to GPT 2 was met with so

00:06:21,650 --> 00:06:25,310
much criticism is that the culture of

00:06:23,900 --> 00:06:27,830
open source of revolutionize how we

00:06:25,310 --> 00:06:30,530
adopt and utilize technology I myself I

00:06:27,830 --> 00:06:32,240
used to work at a boot camp teaching

00:06:30,530 --> 00:06:34,340
data science before I joined Accenture

00:06:32,240 --> 00:06:37,430
and every single one of my students had

00:06:34,340 --> 00:06:38,850
gone through Coursera classes or some

00:06:37,430 --> 00:06:40,710
sort of free

00:06:38,850 --> 00:06:42,720
you know some sort of free online class

00:06:40,710 --> 00:06:46,230
which are only possible because of

00:06:42,720 --> 00:06:47,670
accessibility to two repos on github

00:06:46,230 --> 00:06:48,930
that make things freely and widely

00:06:47,670 --> 00:06:51,690
available

00:06:48,930 --> 00:06:54,030
careers are builds off of Kaggle right

00:06:51,690 --> 00:06:56,580
where individuals use the tools they've

00:06:54,030 --> 00:06:58,350
got on the laptops they have to

00:06:56,580 --> 00:07:00,570
establish themselves as a presence it's

00:06:58,350 --> 00:07:02,910
been revolutionary because we live in a

00:07:00,570 --> 00:07:05,610
society often where privilege money and

00:07:02,910 --> 00:07:07,260
power buy you access the ability to pay

00:07:05,610 --> 00:07:09,300
for the best schools or to have the

00:07:07,260 --> 00:07:11,550
right internship and the democratization

00:07:09,300 --> 00:07:13,020
of these technologies actually breaks

00:07:11,550 --> 00:07:14,720
down a lot of those barriers my don't

00:07:13,020 --> 00:07:17,580
have told people in this room that right

00:07:14,720 --> 00:07:19,170
the power of democratized tools helps

00:07:17,580 --> 00:07:21,180
deconstruct the centralization of power

00:07:19,170 --> 00:07:23,190
and that is a beautiful thing

00:07:21,180 --> 00:07:25,590
and the rapid democratization is what

00:07:23,190 --> 00:07:26,870
can lead to a level playing field but

00:07:25,590 --> 00:07:30,540
then these things are not without

00:07:26,870 --> 00:07:32,550
consequences so how do we stop malicious

00:07:30,540 --> 00:07:34,560
actors or can we and is it our

00:07:32,550 --> 00:07:36,150
responsibility so I'm not gonna launch

00:07:34,560 --> 00:07:37,830
into the is it our responsibility to

00:07:36,150 --> 00:07:39,990
talk that's actually another whole talk

00:07:37,830 --> 00:07:42,810
that I do that if actually if you are at

00:07:39,990 --> 00:07:44,580
the the Linux summit in Half Moon Bay I

00:07:42,810 --> 00:07:47,310
gave that talk then it's on something I

00:07:44,580 --> 00:07:50,280
call moral outsourcing and with the use

00:07:47,310 --> 00:07:52,890
of technology and our response to our

00:07:50,280 --> 00:07:55,440
individual responsibilities given that

00:07:52,890 --> 00:07:58,260
we've pushed off a lot of the work on to

00:07:55,440 --> 00:08:00,210
automation but let's just say we've all

00:07:58,260 --> 00:08:02,070
agreed that it is our responsibility we

00:08:00,210 --> 00:08:03,750
built we build these products we put

00:08:02,070 --> 00:08:07,050
them out there in the world how do we

00:08:03,750 --> 00:08:09,930
stop these malicious actors can we so

00:08:07,050 --> 00:08:11,640
there is some worlds so in in Brits blog

00:08:09,930 --> 00:08:13,590
post I mentioned earlier she does touch

00:08:11,640 --> 00:08:15,540
on a few communities that we can draw

00:08:13,590 --> 00:08:17,580
from this is not a new and novel problem

00:08:15,540 --> 00:08:19,020
we have in the past built technologies

00:08:17,580 --> 00:08:21,000
that can be used for harm that can be

00:08:19,020 --> 00:08:24,120
used for good we worry about it falling

00:08:21,000 --> 00:08:26,460
into the wrong so people talk about the

00:08:24,120 --> 00:08:30,620
biomedical industry right so you think

00:08:26,460 --> 00:08:33,330
about cloning genetic genetic research

00:08:30,620 --> 00:08:35,219
nuclear is the obvious one nuclear

00:08:33,330 --> 00:08:37,590
research creating the atom bomb etcetera

00:08:35,219 --> 00:08:39,960
and there's some talk about the way

00:08:37,590 --> 00:08:42,030
people do work in the InfoSec community

00:08:39,960 --> 00:08:44,790
in particular was this idea of

00:08:42,030 --> 00:08:48,330
responsible disclosure so identifying

00:08:44,790 --> 00:08:51,240
harms and flaws in your in what you've

00:08:48,330 --> 00:08:52,360
built and making it public what these

00:08:51,240 --> 00:08:54,070
flaws are and there

00:08:52,360 --> 00:08:56,410
a lot of models of for example bug

00:08:54,070 --> 00:08:58,120
bounties etc that are actually current

00:08:56,410 --> 00:09:01,329
be currently being understood and

00:08:58,120 --> 00:09:04,240
explored in the AI security community

00:09:01,329 --> 00:09:05,950
today but what are the shortcoming so

00:09:04,240 --> 00:09:07,990
let's do them one by one so in the

00:09:05,950 --> 00:09:10,000
biomedical industry and actually a

00:09:07,990 --> 00:09:12,310
nuclear what you have is a massive

00:09:10,000 --> 00:09:15,160
barrier to entry you can't just do

00:09:12,310 --> 00:09:18,640
genetic testing in your backyard you

00:09:15,160 --> 00:09:20,800
can't you know create nuclear weapons in

00:09:18,640 --> 00:09:23,079
your garage you have to actually have

00:09:20,800 --> 00:09:24,399
access and actually what they do in and

00:09:23,079 --> 00:09:26,170
even if you did have a lot of money

00:09:24,399 --> 00:09:28,420
didn't have access what they do is they

00:09:26,170 --> 00:09:31,420
they trace and track for example like in

00:09:28,420 --> 00:09:33,610
in understanding how to control nuclear

00:09:31,420 --> 00:09:35,620
proliferation they trace and track the

00:09:33,610 --> 00:09:37,810
creators of the rare materials that are

00:09:35,620 --> 00:09:40,060
used to create these to create these

00:09:37,810 --> 00:09:41,740
weapons not really possible with what

00:09:40,060 --> 00:09:44,260
we're talking about it is our our

00:09:41,740 --> 00:09:46,029
industry our world is built on making

00:09:44,260 --> 00:09:48,519
things cheaply freely readily available

00:09:46,029 --> 00:09:50,980
right so we kind of can't adopt the same

00:09:48,519 --> 00:09:52,660
way and ultimately regulation leads to

00:09:50,980 --> 00:09:54,279
resent realization if we're gonna say oh

00:09:52,660 --> 00:09:55,839
well we need to regulate set it up well

00:09:54,279 --> 00:09:57,220
then guess what we're doing we're kind

00:09:55,839 --> 00:10:00,940
of fighting the culture of what we have

00:09:57,220 --> 00:10:03,250
built and we're resent iliza power which

00:10:00,940 --> 00:10:04,990
is kind of what what we're trying to not

00:10:03,250 --> 00:10:07,600
do because our industry culture is built

00:10:04,990 --> 00:10:11,130
on easy to adopt easy to share easy to

00:10:07,600 --> 00:10:13,420
learn and distribute tools and software

00:10:11,130 --> 00:10:15,550
so there's this notion going around this

00:10:13,420 --> 00:10:16,810
idea of responsible the least so the

00:10:15,550 --> 00:10:18,730
difference between responsible ISA

00:10:16,810 --> 00:10:21,910
responsible disclosure responsible

00:10:18,730 --> 00:10:24,339
disclosure assumes that you can find all

00:10:21,910 --> 00:10:27,370
the bugs because in a straightforward to

00:10:24,339 --> 00:10:29,529
use software it has n outputs and then

00:10:27,370 --> 00:10:31,630
you can calculate for n outputs how

00:10:29,529 --> 00:10:32,800
things might go wrong not so the case

00:10:31,630 --> 00:10:35,140
when we're thinking about artificial

00:10:32,800 --> 00:10:36,970
intelligence multi-purpose tools and

00:10:35,140 --> 00:10:39,100
also a technology that technically quote

00:10:36,970 --> 00:10:41,050
learns so what is responsible release

00:10:39,100 --> 00:10:42,699
it's the idea of can kind of what open

00:10:41,050 --> 00:10:44,890
AI was trying to do how do I create

00:10:42,699 --> 00:10:46,510
something hey recognize it a convicted

00:10:44,890 --> 00:10:48,640
be potentially harmful and release it

00:10:46,510 --> 00:10:50,410
responsibly that's actually a big

00:10:48,640 --> 00:10:53,890
discussion our community right now so

00:10:50,410 --> 00:10:57,070
what is it we have no clue we have zero

00:10:53,890 --> 00:11:00,190
idea and we are trying to sort this out

00:10:57,070 --> 00:11:02,110
at the moment so some thoughts on how to

00:11:00,190 --> 00:11:04,089
start there's this really great paper

00:11:02,110 --> 00:11:04,899
called law and adversarial machine

00:11:04,089 --> 00:11:06,970
learning

00:11:04,899 --> 00:11:10,180
specifically about adversarial machine

00:11:06,970 --> 00:11:12,309
learning attacks so AML essentially what

00:11:10,180 --> 00:11:14,589
you do is you have an AI that tries to

00:11:12,309 --> 00:11:17,319
fight another a I that's why it's called

00:11:14,589 --> 00:11:19,180
adversarial and as one improves on let's

00:11:17,319 --> 00:11:21,249
say tricking the other one the other one

00:11:19,180 --> 00:11:23,290
improves on catching virulence you can

00:11:21,249 --> 00:11:25,990
imagine it like the example I used to

00:11:23,290 --> 00:11:27,429
give when I taught data science is a cop

00:11:25,990 --> 00:11:29,499
and a counterfeiter right so

00:11:27,429 --> 00:11:31,389
counterfeiters making fake money the

00:11:29,499 --> 00:11:33,189
detective gets better at figuring out

00:11:31,389 --> 00:11:34,300
what fake money is and then the

00:11:33,189 --> 00:11:35,829
counterfeiter gets better and that

00:11:34,300 --> 00:11:38,139
aren't making fake money so ultimately

00:11:35,829 --> 00:11:40,059
you know it's it's a constant push-pull

00:11:38,139 --> 00:11:43,749
think about AML is it's a really great

00:11:40,059 --> 00:11:46,300
way to hack AI systems so in this you

00:11:43,749 --> 00:11:47,949
know it in this paper it's it's actually

00:11:46,300 --> 00:11:49,869
really wonderful paper it's addressing

00:11:47,949 --> 00:11:52,209
not just the social impact but the role

00:11:49,869 --> 00:11:54,519
of law in this place and they have three

00:11:52,209 --> 00:11:57,850
recommendations on how you might want to

00:11:54,519 --> 00:12:00,040
do this so number one is benchmarking

00:11:57,850 --> 00:12:03,429
attacks and defenses so this is

00:12:00,040 --> 00:12:05,589
something akin to responsible disclosure

00:12:03,429 --> 00:12:07,439
so thinking through all the different

00:12:05,589 --> 00:12:10,749
ways in which things might go wrong

00:12:07,439 --> 00:12:13,240
writing this down somewhere and possibly

00:12:10,749 --> 00:12:16,059
you know giving ways to defend against

00:12:13,240 --> 00:12:19,059
it that is a task to be done one cannot

00:12:16,059 --> 00:12:21,490
just release and say oops sorry second

00:12:19,059 --> 00:12:22,959
is to architect for forensics in other

00:12:21,490 --> 00:12:25,809
words I'm gonna talk about some in about

00:12:22,959 --> 00:12:27,879
some initiatives going on creating the

00:12:25,809 --> 00:12:29,829
right kind of paperwork so you can trace

00:12:27,879 --> 00:12:32,050
back if something does go wrong that was

00:12:29,829 --> 00:12:33,699
unexpected how it went wrong again if

00:12:32,050 --> 00:12:34,749
anyone anyone in this room dabbles in

00:12:33,699 --> 00:12:37,149
data science say you know that we

00:12:34,749 --> 00:12:38,410
actually don't really have standards you

00:12:37,149 --> 00:12:40,569
know we have standards on how our code

00:12:38,410 --> 00:12:42,850
should look but if you create a project

00:12:40,569 --> 00:12:45,610
and to end pipeline we actually don't

00:12:42,850 --> 00:12:49,929
have standards on data provenance data

00:12:45,610 --> 00:12:52,480
storage data usage potential impact we

00:12:49,929 --> 00:12:54,129
don't track any of this how we tune our

00:12:52,480 --> 00:12:56,259
parameters how we choose or how we

00:12:54,129 --> 00:12:58,779
choose our variables we just kind of

00:12:56,259 --> 00:13:00,009
wing it and then the last version thing

00:12:58,779 --> 00:13:01,779
is very interesting is to take into

00:13:00,009 --> 00:13:03,699
account civil liberties so when I say

00:13:01,779 --> 00:13:05,799
something is a multi-purpose tool it

00:13:03,699 --> 00:13:07,600
means this is not just that like some

00:13:05,799 --> 00:13:09,759
random actor can take it it means a

00:13:07,600 --> 00:13:11,139
government can take it there are a few

00:13:09,759 --> 00:13:14,199
examples I'm going to talk about when I

00:13:11,139 --> 00:13:16,240
get a little bit deeper into this so

00:13:14,199 --> 00:13:17,439
benchmarking benchmarking which what

00:13:16,240 --> 00:13:18,209
does one thing is great is we can draw

00:13:17,439 --> 00:13:19,529
from the InfoSec

00:13:18,209 --> 00:13:22,350
community so if you're familiar with

00:13:19,529 --> 00:13:23,910
Adam show statics Dredd framework for

00:13:22,350 --> 00:13:25,559
threat modeling there's a lot to learn

00:13:23,910 --> 00:13:27,420
from the threat modeling space it is

00:13:25,559 --> 00:13:30,209
necessary but I would say insufficient

00:13:27,420 --> 00:13:32,249
for understanding because the two things

00:13:30,209 --> 00:13:33,749
you know we've got in the AI space or

00:13:32,249 --> 00:13:35,610
the unintended consequences and

00:13:33,749 --> 00:13:37,619
malicious actors you combine that with

00:13:35,610 --> 00:13:39,329
the fact that we are talking about a

00:13:37,619 --> 00:13:41,519
technology as meant to evolve and learn

00:13:39,329 --> 00:13:44,970
and has to be context specific

00:13:41,519 --> 00:13:45,990
unlike let's say certain software if you

00:13:44,970 --> 00:13:47,670
go through the Dredd framework

00:13:45,990 --> 00:13:49,379
essentially what it's saying the way to

00:13:47,670 --> 00:13:51,660
benchmark is you assess the amount of

00:13:49,379 --> 00:13:54,329
damage something could cause the

00:13:51,660 --> 00:13:56,369
reliability of the attack the ease at

00:13:54,329 --> 00:13:58,949
which an attacker can exploit or launch

00:13:56,369 --> 00:14:00,660
an attack the scope of affected users

00:13:58,949 --> 00:14:02,610
and the ease at which an attacker can

00:14:00,660 --> 00:14:04,499
discover the attack so how easy is it to

00:14:02,610 --> 00:14:06,360
find how bad can it be who's gonna be

00:14:04,499 --> 00:14:08,160
impacted it all seems pretty sensible

00:14:06,360 --> 00:14:10,860
right it is but it's actually quite

00:14:08,160 --> 00:14:12,449
difficult to do and one thing that at

00:14:10,860 --> 00:14:14,160
least I personally am working on is

00:14:12,449 --> 00:14:16,889
something I'm calling critical data

00:14:14,160 --> 00:14:19,079
science which is the Act the art of the

00:14:16,889 --> 00:14:20,879
pedagogy of critiquing data sign just

00:14:19,079 --> 00:14:24,600
actually not something data scientists

00:14:20,879 --> 00:14:26,850
are explicitly taught how to do so when

00:14:24,600 --> 00:14:28,860
we think about forensics so again how do

00:14:26,850 --> 00:14:30,829
you build in documentation to trace back

00:14:28,860 --> 00:14:33,509
to potential errors and vulnerabilities

00:14:30,829 --> 00:14:35,819
so there are some really great papers

00:14:33,509 --> 00:14:38,639
out there not really used in practice

00:14:35,819 --> 00:14:40,319
but still very solid papers coupled by

00:14:38,639 --> 00:14:42,689
Google one called data sheets for data

00:14:40,319 --> 00:14:44,339
data sets another one called model cards

00:14:42,689 --> 00:14:46,740
for models that essentially creates a

00:14:44,339 --> 00:14:48,839
standardized template for tracking

00:14:46,740 --> 00:14:51,569
particular things about your data and

00:14:48,839 --> 00:14:53,160
your model this has led to an initiative

00:14:51,569 --> 00:14:56,429
in this group called a partnership on AI

00:14:53,160 --> 00:15:00,839
the partnership on AI is an industry

00:14:56,429 --> 00:15:02,339
started but broadly attended group to

00:15:00,839 --> 00:15:03,689
understand the social implications of

00:15:02,339 --> 00:15:06,329
artificial intelligence so it's not just

00:15:03,689 --> 00:15:09,929
industry it's civil service groups

00:15:06,329 --> 00:15:10,949
nonprofits and you know so what what we

00:15:09,929 --> 00:15:12,929
do when we get together is have this

00:15:10,949 --> 00:15:14,069
amazing wide range of people all

00:15:12,929 --> 00:15:15,990
interested in the implications of

00:15:14,069 --> 00:15:17,339
artificial intelligence so I'm on the

00:15:15,990 --> 00:15:20,040
steering committee for this group called

00:15:17,339 --> 00:15:21,389
about ml and the goal of about ml is to

00:15:20,040 --> 00:15:23,730
create this sort of standardized

00:15:21,389 --> 00:15:25,740
documentation and then there's a

00:15:23,730 --> 00:15:27,959
language that is often used in a lot of

00:15:25,740 --> 00:15:29,939
the the legal space so the government

00:15:27,959 --> 00:15:31,110
space so the European Commission for

00:15:29,939 --> 00:15:32,209
example talks about things like

00:15:31,110 --> 00:15:35,399
explained

00:15:32,209 --> 00:15:36,540
auditability traceability the Government

00:15:35,399 --> 00:15:38,640
of Canada just came out with an

00:15:36,540 --> 00:15:41,160
algorithmic impact assessment in order

00:15:38,640 --> 00:15:43,320
to attempt to audit any model that is

00:15:41,160 --> 00:15:44,790
used by the Canadian government so there

00:15:43,320 --> 00:15:46,320
are these these early attempts at

00:15:44,790 --> 00:15:49,410
forensics that we see happening right

00:15:46,320 --> 00:15:50,880
now and then finally the notion of civil

00:15:49,410 --> 00:15:52,290
liberties I found this one to be the

00:15:50,880 --> 00:15:53,790
most is the most interesting this one's

00:15:52,290 --> 00:15:56,040
the hardest to grasp right for us

00:15:53,790 --> 00:15:58,110
technical folks you know it's not

00:15:56,040 --> 00:15:59,579
something that is programmable codable

00:15:58,110 --> 00:16:01,709
you got to read things you got to read

00:15:59,579 --> 00:16:03,600
the law things but it's really

00:16:01,709 --> 00:16:05,579
interesting cuz we've seen this already

00:16:03,600 --> 00:16:07,470
happen if you've heard of this in the

00:16:05,579 --> 00:16:08,880
in--this infamous gaydar paper that came

00:16:07,470 --> 00:16:11,519
out of stanford this was one of the

00:16:08,880 --> 00:16:14,310
first conversations we had about three

00:16:11,519 --> 00:16:16,560
years ago about how this technology a

00:16:14,310 --> 00:16:18,510
can be poorly designed and then be be

00:16:16,560 --> 00:16:20,579
used maliciously by a government actor

00:16:18,510 --> 00:16:23,850
so what these researchers at Stanford

00:16:20,579 --> 00:16:26,730
did was they scraped tinder photos took

00:16:23,850 --> 00:16:28,290
people's expressed sexuality and trained

00:16:26,730 --> 00:16:29,610
a machine learning model to say hey I

00:16:28,290 --> 00:16:32,730
can look at your face and tell you if

00:16:29,610 --> 00:16:34,019
you're straight or gay so many problems

00:16:32,730 --> 00:16:35,790
with that and the three and a half

00:16:34,019 --> 00:16:37,890
minutes I have left you I wouldn't have

00:16:35,790 --> 00:16:39,720
enough time if I had an hour left on all

00:16:37,890 --> 00:16:41,130
the things that were wrong on what they

00:16:39,720 --> 00:16:43,380
built and how they built it but what was

00:16:41,130 --> 00:16:44,940
scary about it was once they had built

00:16:43,380 --> 00:16:46,890
it there are countries in this world

00:16:44,940 --> 00:16:49,620
where it was literally illegal to be gay

00:16:46,890 --> 00:16:51,930
and you would be killed and what they

00:16:49,620 --> 00:16:53,640
did was create a tool that one of these

00:16:51,930 --> 00:16:56,640
governments could pick up and very

00:16:53,640 --> 00:16:58,350
easily use there for stifling civil

00:16:56,640 --> 00:17:00,450
liberties so thinking through what are

00:16:58,350 --> 00:17:02,970
illegal protections you know the UN

00:17:00,450 --> 00:17:05,429
actually has we have a list of civil

00:17:02,970 --> 00:17:07,709
liberties what we are owed as human

00:17:05,429 --> 00:17:09,150
beings our rights and what do and is

00:17:07,709 --> 00:17:09,600
what we're building violating those

00:17:09,150 --> 00:17:11,490
rights

00:17:09,600 --> 00:17:13,260
another example is actually facial

00:17:11,490 --> 00:17:14,880
recognition so we're seeing more and

00:17:13,260 --> 00:17:16,230
more protest movements all around the

00:17:14,880 --> 00:17:18,030
world and frankly I think it's an

00:17:16,230 --> 00:17:20,699
amazing thing we're seeing protests in

00:17:18,030 --> 00:17:22,980
Hong Kong in Lebanon right all around

00:17:20,699 --> 00:17:25,169
the world and as people cover their

00:17:22,980 --> 00:17:27,270
faces what governments are doing with

00:17:25,169 --> 00:17:28,710
Akkad in Hong Kong was ban the ability

00:17:27,270 --> 00:17:31,410
to cover your face that they can then

00:17:28,710 --> 00:17:33,780
track you and identify who you are using

00:17:31,410 --> 00:17:35,970
facial recognition technologies the

00:17:33,780 --> 00:17:37,679
stifling dissent movements and it's not

00:17:35,970 --> 00:17:39,510
just something as complicated as facial

00:17:37,679 --> 00:17:43,350
recognition to the surveillance state

00:17:39,510 --> 00:17:45,240
technologies are used in tandem to

00:17:43,350 --> 00:17:48,390
stifle the ability to have free

00:17:45,240 --> 00:17:50,429
speech the ability to congregate so we

00:17:48,390 --> 00:17:51,929
are stifling civil liberties with it

00:17:50,429 --> 00:17:53,820
with our creation of these technologies

00:17:51,929 --> 00:17:55,800
and that has to be a consideration we

00:17:53,820 --> 00:17:59,070
have when building and creating the

00:17:55,800 --> 00:18:01,410
tools that we have so we're in an era of

00:17:59,070 --> 00:18:02,520
massive impact massive growth and

00:18:01,410 --> 00:18:04,290
massive potential I don't want to

00:18:02,520 --> 00:18:05,670
diminish the value of what we're

00:18:04,290 --> 00:18:08,070
building the reason we haven't stopped

00:18:05,670 --> 00:18:10,380
building these technologies is that we

00:18:08,070 --> 00:18:12,750
are creating new and amazing things we

00:18:10,380 --> 00:18:14,130
are changing our paradigms where we're

00:18:12,750 --> 00:18:16,620
revolutionizing everything from

00:18:14,130 --> 00:18:18,150
education to transportation to the

00:18:16,620 --> 00:18:21,330
notion of government the notion of

00:18:18,150 --> 00:18:23,400
citizenship it's phenomenal but at the

00:18:21,330 --> 00:18:25,500
same time we can't just look towards a

00:18:23,400 --> 00:18:27,210
positive we actually have to embrace the

00:18:25,500 --> 00:18:29,070
potential negative consequences because

00:18:27,210 --> 00:18:31,940
the worst thing we could possibly do is

00:18:29,070 --> 00:18:34,500
recreate a world in which we've

00:18:31,940 --> 00:18:36,750
continued the inequality that exists

00:18:34,500 --> 00:18:38,990
today the goal of what we build is to

00:18:36,750 --> 00:18:41,460
democratize is to make things accessible

00:18:38,990 --> 00:18:42,900
to break down centralization of power

00:18:41,460 --> 00:18:46,650
and what we don't want to do is actually

00:18:42,900 --> 00:18:49,170
reenable malicious actors to do that

00:18:46,650 --> 00:18:51,480
kind of thing so as we think about the

00:18:49,170 --> 00:18:53,309
technologies we build my encouragement

00:18:51,480 --> 00:18:55,710
is to think through those three things

00:18:53,309 --> 00:18:57,450
right how do we create traceability in

00:18:55,710 --> 00:19:00,390
what we build how do we think about the

00:18:57,450 --> 00:19:02,220
civil liberties impact and also how do

00:19:00,390 --> 00:19:04,110
we make what we've built understandable

00:19:02,220 --> 00:19:05,460
and accessible to people so that we can

00:19:04,110 --> 00:19:08,680
be critiqued and we can build better

00:19:05,460 --> 00:19:14,449
products thank you

00:19:08,680 --> 00:19:14,449

YouTube URL: https://www.youtube.com/watch?v=oaEVp_JBS44


