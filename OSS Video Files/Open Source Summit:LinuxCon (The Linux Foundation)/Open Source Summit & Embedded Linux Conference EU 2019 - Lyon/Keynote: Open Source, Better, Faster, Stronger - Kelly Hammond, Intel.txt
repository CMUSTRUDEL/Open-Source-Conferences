Title: Keynote: Open Source, Better, Faster, Stronger - Kelly Hammond, Intel
Publication date: 2019-10-28
Playlist: Open Source Summit & Embedded Linux Conference EU 2019 - Lyon
Description: 
	Keynote: Open Source, Better, Faster, Stronger - Kelly Hammond, Senior Director of Engineering, Intel Corporation 

Where does time go? Itâ€™s the first question in creating performance solutions. Intel has a long history of leadership in open source projects and a reputation for creating performance products. In our journey for constant improvement, tunable parameters and a few patches are not enough. In this talk, Kelly Hammond, Senior Director of System Platform Software at Intel, will give you insights about the ways ever increasing amounts of data is driving demand for new types of products and new ways for creating and delivering performance with open source software.
Captions: 
	00:00:00,030 --> 00:00:06,680
hello Leone I would like to start just

00:00:02,129 --> 00:00:09,480
echoing what Greg said keep up to date

00:00:06,680 --> 00:00:13,769
and one of my good friends has a great

00:00:09,480 --> 00:00:16,770
line he says security is like doing the

00:00:13,769 --> 00:00:18,779
laundry or the dishes it's never done so

00:00:16,770 --> 00:00:21,119
this is not going away

00:00:18,779 --> 00:00:24,150
even from a software perspective right

00:00:21,119 --> 00:00:27,180
update okay

00:00:24,150 --> 00:00:29,210
so I'm excited today I got to tell you

00:00:27,180 --> 00:00:32,189
about how we're doing open source better

00:00:29,210 --> 00:00:36,120
making performance faster delivering

00:00:32,189 --> 00:00:38,340
strong results before I dive into my

00:00:36,120 --> 00:00:41,160
story I'd like to take a moment to

00:00:38,340 --> 00:00:42,809
recognize Intel's general manager for

00:00:41,160 --> 00:00:43,850
the open source Technology Center who's

00:00:42,809 --> 00:00:48,090
not with us today

00:00:43,850 --> 00:00:51,930
Ahmad Sousou he recently retired he

00:00:48,090 --> 00:00:54,449
spent over 20 years building open source

00:00:51,930 --> 00:00:58,050
at Intel building our strength in the

00:00:54,449 --> 00:01:02,910
Linux community and being a major

00:00:58,050 --> 00:01:05,250
advocate for open source if you want to

00:01:02,910 --> 00:01:08,520
see up a little bit about our evolution

00:01:05,250 --> 00:01:10,400
and how this group grew up you can check

00:01:08,520 --> 00:01:15,479
out the bitly link at until open source

00:01:10,400 --> 00:01:18,360
it's a great little story the

00:01:15,479 --> 00:01:22,799
cornerstone of our leadership spin the

00:01:18,360 --> 00:01:25,650
Linux kernel year-over-year leadership

00:01:22,799 --> 00:01:29,100
like this if we were in sports we'll be

00:01:25,650 --> 00:01:30,750
called the dynasty in a company renowned

00:01:29,100 --> 00:01:33,420
for setting the pace of compute

00:01:30,750 --> 00:01:34,650
technology through Moore's law when the

00:01:33,420 --> 00:01:39,000
software doesn't always get the

00:01:34,650 --> 00:01:42,600
spotlight but as you can see we've been

00:01:39,000 --> 00:01:48,750
setting the pace keeping pace with our

00:01:42,600 --> 00:01:50,610
hardware the same speed and we've grown

00:01:48,750 --> 00:01:54,030
from there right that's a cornerstone

00:01:50,610 --> 00:01:57,149
that's a beginning we've morphed into a

00:01:54,030 --> 00:02:00,000
variety of open source projects and

00:01:57,149 --> 00:02:05,090
technologies that we either have

00:02:00,000 --> 00:02:08,869
engineers in that we contribute to or

00:02:05,090 --> 00:02:11,910
sponsor or that we support indirectly

00:02:08,869 --> 00:02:13,560
through the many libraries or language

00:02:11,910 --> 00:02:15,240
optimizations that we do

00:02:13,560 --> 00:02:20,610
that feed into the broader open-source

00:02:15,240 --> 00:02:24,920
community we know that when we invest in

00:02:20,610 --> 00:02:27,440
an open-source engineer or in a project

00:02:24,920 --> 00:02:31,860
we're not just paying for a resource

00:02:27,440 --> 00:02:35,900
right we're investing in the return that

00:02:31,860 --> 00:02:35,900
we get from the community as a whole

00:02:36,050 --> 00:02:42,450
that being said I think we also have a

00:02:39,810 --> 00:02:44,459
pretty distinctive reputation and many

00:02:42,450 --> 00:02:47,519
of these circles for focus on

00:02:44,459 --> 00:02:49,260
performance so in this talk I'll go

00:02:47,519 --> 00:02:56,090
through some of the new ways we're using

00:02:49,260 --> 00:02:58,290
to deliver performance starts with this

00:02:56,090 --> 00:03:02,130
many of you may be familiar with this

00:02:58,290 --> 00:03:04,650
nexus tential question if a tree falls

00:03:02,130 --> 00:03:09,269
in a forest and no one is around to hear

00:03:04,650 --> 00:03:13,200
it does it make a sound this is how I

00:03:09,269 --> 00:03:17,280
think about performance because if my

00:03:13,200 --> 00:03:20,340
customer doesn't experience it even if

00:03:17,280 --> 00:03:24,660
it's written up in a white paper doesn't

00:03:20,340 --> 00:03:26,190
really matter let me tell you how we are

00:03:24,660 --> 00:03:28,920
delivering performance that you can

00:03:26,190 --> 00:03:33,650
experience and even give you some tips

00:03:28,920 --> 00:03:36,269
on how you can boost performance - first

00:03:33,650 --> 00:03:38,010
it's important to understand what the

00:03:36,269 --> 00:03:43,530
latest tools for performance are that

00:03:38,010 --> 00:03:45,570
Intel is delivering in our data driven

00:03:43,530 --> 00:03:48,510
world it's increasingly difficult to

00:03:45,570 --> 00:03:51,780
manage and leverage data to gain

00:03:48,510 --> 00:03:53,430
insights the two big pain points we hear

00:03:51,780 --> 00:03:56,220
a lot about from our customers are

00:03:53,430 --> 00:04:00,329
managing the data and pushing through

00:03:56,220 --> 00:04:03,180
compute fast enough in a data centric

00:04:00,329 --> 00:04:04,230
world performance isn't just about micro

00:04:03,180 --> 00:04:05,850
benchmarks

00:04:04,230 --> 00:04:08,730
it's about your workload it's about

00:04:05,850 --> 00:04:11,329
end-to-end performance and the data and

00:04:08,730 --> 00:04:13,980
the computer where the bottlenecks are

00:04:11,329 --> 00:04:15,600
good and bad news cuz now you know where

00:04:13,980 --> 00:04:18,410
to focus on those are the biggest

00:04:15,600 --> 00:04:18,410
performance payoffs

00:04:20,980 --> 00:04:26,770
when it comes to overwhelming amount of

00:04:23,020 --> 00:04:28,930
data being produced every day the

00:04:26,770 --> 00:04:33,100
challenge is optimized so that you can

00:04:28,930 --> 00:04:36,250
access more data faster and if you doubt

00:04:33,100 --> 00:04:39,910
the effect of what data can do to impact

00:04:36,250 --> 00:04:41,500
your performance calculation well maybe

00:04:39,910 --> 00:04:43,990
just take in some of the latest news I

00:04:41,500 --> 00:04:46,660
think most of us should be aware of

00:04:43,990 --> 00:04:51,460
Google's recent declaration of obtaining

00:04:46,660 --> 00:04:54,160
or getting to quantum supremacy by doing

00:04:51,460 --> 00:04:57,910
a 200 second calculation that would take

00:04:54,160 --> 00:05:00,550
10,000 years to do on a existing

00:04:57,910 --> 00:05:03,810
supercomputer I read an interesting

00:05:00,550 --> 00:05:06,310
article by The Verge where IBM said

00:05:03,810 --> 00:05:10,780
actually if you properly account for

00:05:06,310 --> 00:05:12,790
enough storage it's 2.5 days so I don't

00:05:10,780 --> 00:05:17,560
know which one exactly it is but I think

00:05:12,790 --> 00:05:20,770
the the takeaway that doing you're you

00:05:17,560 --> 00:05:24,430
know having a correct data solution can

00:05:20,770 --> 00:05:26,560
have 10,000 years of impact I think that

00:05:24,430 --> 00:05:31,240
really shows you how important and how

00:05:26,560 --> 00:05:33,910
critical this is to combat the

00:05:31,240 --> 00:05:37,000
challenges we face as datasets become

00:05:33,910 --> 00:05:40,570
large and larger intel has created a new

00:05:37,000 --> 00:05:42,850
tier of memory Intel obtain DC

00:05:40,570 --> 00:05:45,130
persistent memory is an innovative

00:05:42,850 --> 00:05:47,680
memory technology that delivers a unique

00:05:45,130 --> 00:05:51,550
combination of affordable large capacity

00:05:47,680 --> 00:05:53,590
and support for data persistence it's

00:05:51,550 --> 00:05:55,750
helping businesses get faster insights

00:05:53,590 --> 00:05:58,360
from their data intensive applications

00:05:55,750 --> 00:06:00,820
as well as deliver consistently improved

00:05:58,360 --> 00:06:04,840
service scalability with higher virtual

00:06:00,820 --> 00:06:08,910
machine and container density to speed

00:06:04,840 --> 00:06:12,790
up the compute we are adding a new

00:06:08,910 --> 00:06:15,370
instruction to the avx-512 this is the

00:06:12,790 --> 00:06:17,080
vector neural network instruction which

00:06:15,370 --> 00:06:19,650
significantly accelerates inference

00:06:17,080 --> 00:06:22,750
performance for deep learning workloads

00:06:19,650 --> 00:06:25,480
this acceleration speeds up int 8

00:06:22,750 --> 00:06:30,150
convolution in one instruction that used

00:06:25,480 --> 00:06:30,150
to take 3 in the previous generations

00:06:34,100 --> 00:06:39,960
cool now we have these tools and

00:06:36,480 --> 00:06:41,730
capabilities to go faster easy right all

00:06:39,960 --> 00:06:43,920
we need to do we've got these cool tools

00:06:41,730 --> 00:06:47,400
we just need to bring all of open source

00:06:43,920 --> 00:06:49,560
along with us which is sometimes easier

00:06:47,400 --> 00:06:51,270
said than done

00:06:49,560 --> 00:06:53,400
there's a saying that if you want to go

00:06:51,270 --> 00:06:57,780
fast go alone

00:06:53,400 --> 00:06:59,430
if you want to go far go together but

00:06:57,780 --> 00:07:01,920
they don't say much about going fast

00:06:59,430 --> 00:07:05,700
together and we've got a lot of

00:07:01,920 --> 00:07:08,430
dependencies and applications here so

00:07:05,700 --> 00:07:10,410
going faster an open source can

00:07:08,430 --> 00:07:13,440
sometimes feel like running a

00:07:10,410 --> 00:07:16,050
three-legged race right you want to

00:07:13,440 --> 00:07:18,210
enable your latest technology make it

00:07:16,050 --> 00:07:19,290
faster but you need to drag all of your

00:07:18,210 --> 00:07:22,230
dependencies and all of your

00:07:19,290 --> 00:07:24,450
applications along with you and somehow

00:07:22,230 --> 00:07:27,630
we need to win this race not end up in a

00:07:24,450 --> 00:07:29,520
tangle teep so how are we creating fast

00:07:27,630 --> 00:07:34,530
performance solutions through open

00:07:29,520 --> 00:07:37,350
source well I guess we can really

00:07:34,530 --> 00:07:40,440
reflect on even our recent discussion of

00:07:37,350 --> 00:07:43,020
creating better CI step one is creating

00:07:40,440 --> 00:07:45,900
a beat for your three-legged race right

00:07:43,020 --> 00:07:47,880
this is your backbone it's a regular

00:07:45,900 --> 00:07:49,950
cadence of automated performance testing

00:07:47,880 --> 00:07:53,160
across a broad range of workloads and

00:07:49,950 --> 00:07:57,000
micro benchmarks with our clear Linux

00:07:53,160 --> 00:08:00,060
operating system we test over 5300

00:07:57,000 --> 00:08:05,610
packages with over a hundred daily tests

00:08:00,060 --> 00:08:08,790
and 300 weekly tests for performance job

00:08:05,610 --> 00:08:10,710
number one is don't regress and that

00:08:08,790 --> 00:08:12,870
includes packages and dependencies from

00:08:10,710 --> 00:08:14,550
other projects in the open source

00:08:12,870 --> 00:08:18,360
letting them know if their latest

00:08:14,550 --> 00:08:20,040
updates have impacted performance the

00:08:18,360 --> 00:08:24,930
last thing you want to do is take one

00:08:20,040 --> 00:08:27,990
step forward and two two steps back now

00:08:24,930 --> 00:08:30,960
you can create performance without this

00:08:27,990 --> 00:08:33,560
infrastructure but you can't sustain it

00:08:30,960 --> 00:08:40,140
you can't continue to move it forward

00:08:33,560 --> 00:08:43,860
consistently except you ez optimize

00:08:40,140 --> 00:08:45,720
right we like to use the full stack

00:08:43,860 --> 00:08:46,529
that's why we've got our operating

00:08:45,720 --> 00:08:48,389
system that's why

00:08:46,529 --> 00:08:50,310
we create specialized workloads because

00:08:48,389 --> 00:08:56,220
it gives a whole us a whole lot of knobs

00:08:50,310 --> 00:08:58,139
to be able to turn the first step in

00:08:56,220 --> 00:09:02,009
this is to ask the simple question where

00:08:58,139 --> 00:09:04,800
does time go right we leverage all the

00:09:02,009 --> 00:09:05,999
knobs for specific workloads and we

00:09:04,800 --> 00:09:12,209
target workloads that we know are

00:09:05,999 --> 00:09:14,879
challenging to optimize we take in the

00:09:12,209 --> 00:09:17,009
latest and greatest GCC features we keep

00:09:14,879 --> 00:09:18,389
up to date right always taking the

00:09:17,009 --> 00:09:21,180
newest thing it gets you the performance

00:09:18,389 --> 00:09:25,110
it get used to gets you the security we

00:09:21,180 --> 00:09:26,999
use selectively enabling of avx-512 we

00:09:25,110 --> 00:09:28,559
consult experts for parameter tuning

00:09:26,999 --> 00:09:31,079
across kernel as well as runtime

00:09:28,559 --> 00:09:32,899
variables and we selectively pick

00:09:31,079 --> 00:09:35,759
scheduling policies aligned to workloads

00:09:32,899 --> 00:09:37,259
we also use tools like link top time

00:09:35,759 --> 00:09:43,860
optimization and profile guided

00:09:37,259 --> 00:09:47,430
optimization as needed step 3 finally we

00:09:43,860 --> 00:09:49,410
deliver we deliver the full stack

00:09:47,430 --> 00:09:51,959
because handing a data scientist a

00:09:49,410 --> 00:09:54,870
kernel parameter to tune probably isn't

00:09:51,959 --> 00:09:56,430
the best use of his or her time also

00:09:54,870 --> 00:09:58,379
there's inevitable trade-offs in

00:09:56,430 --> 00:10:00,809
performance like latency versus

00:09:58,379 --> 00:10:04,559
bandwidth settings for things like huge

00:10:00,809 --> 00:10:06,480
page or scheduling policies so to

00:10:04,559 --> 00:10:09,059
provide customers the ultimate in

00:10:06,480 --> 00:10:12,480
performance we deliver it all and to end

00:10:09,059 --> 00:10:16,949
in reference stacks there are currently

00:10:12,480 --> 00:10:18,750
three reference stacks the first is the

00:10:16,949 --> 00:10:20,759
deep learning reference stack this is

00:10:18,750 --> 00:10:22,199
created to help a I developers deliver

00:10:20,759 --> 00:10:24,329
the best experience on Intel

00:10:22,199 --> 00:10:26,069
architecture it allows you to quickly

00:10:24,329 --> 00:10:28,230
prototype and deploy deep learning

00:10:26,069 --> 00:10:29,819
workloads to production while

00:10:28,230 --> 00:10:33,149
maintaining the flexibility to customize

00:10:29,819 --> 00:10:34,769
solutions this stack is also the best

00:10:33,149 --> 00:10:39,709
way you can try out the deep learning

00:10:34,769 --> 00:10:43,350
boost performance for inference speed-up

00:10:39,709 --> 00:10:45,509
next is the data analytics stack this

00:10:43,350 --> 00:10:47,160
kiss' application developers a powerful

00:10:45,509 --> 00:10:49,319
way to store and process large amounts

00:10:47,160 --> 00:10:51,240
of data by using a distributed print

00:10:49,319 --> 00:10:53,339
processing framework to efficiently

00:10:51,240 --> 00:10:57,540
build big data solutions and solve

00:10:53,339 --> 00:10:59,970
domain-specific problems this leverages

00:10:57,540 --> 00:11:02,370
and deploys apache spark apache

00:10:59,970 --> 00:11:07,079
OOP and openjdk it allows you to

00:11:02,370 --> 00:11:09,660
customize your solution last but not

00:11:07,079 --> 00:11:11,910
least try out intel obtain DC persistent

00:11:09,660 --> 00:11:14,129
memory with a database reference stack

00:11:11,910 --> 00:11:17,399
also a second generation Intel Xeon

00:11:14,129 --> 00:11:19,110
scalable processors this workload

00:11:17,399 --> 00:11:22,230
optimized technology is designed to help

00:11:19,110 --> 00:11:25,829
businesses extract more insights from

00:11:22,230 --> 00:11:27,569
data its includes the latest database

00:11:25,829 --> 00:11:31,410
technologies including Apache Cassandra

00:11:27,569 --> 00:11:34,610
and Redis and should you want to extend

00:11:31,410 --> 00:11:36,629
your analytics we have a wide range of

00:11:34,610 --> 00:11:39,149
additional models that you can choose

00:11:36,629 --> 00:11:40,410
from and some great use cases where you

00:11:39,149 --> 00:11:42,990
can see where we worked with the likes

00:11:40,410 --> 00:11:45,449
of companies ranging from MasterCard to

00:11:42,990 --> 00:11:49,529
CERN on how they handle their big data

00:11:45,449 --> 00:11:50,519
and put together end-to-end solutions ok

00:11:49,529 --> 00:11:53,970
what happens when you put it all

00:11:50,519 --> 00:11:57,329
together and deliver performance world

00:11:53,970 --> 00:12:00,629
records for one ended in collaboration

00:11:57,329 --> 00:12:02,850
with Intel Alibaba recently published

00:12:00,629 --> 00:12:06,480
the industry's first 100 terabyte

00:12:02,850 --> 00:12:11,189
results for the TPC D s and the TPC xbb

00:12:06,480 --> 00:12:13,829
workloads these are standards benchmarks

00:12:11,189 --> 00:12:15,990
for big data workloads the previous

00:12:13,829 --> 00:12:18,120
records had been on 10 and 30 terabytes

00:12:15,990 --> 00:12:23,100
so this is a new level of record and a

00:12:18,120 --> 00:12:26,550
new level of scaling the next is

00:12:23,100 --> 00:12:30,089
sustained performance with our clear

00:12:26,550 --> 00:12:32,309
Linux OS we have kept the pace of

00:12:30,089 --> 00:12:35,449
performance kept ahead of the other

00:12:32,309 --> 00:12:38,129
operating systems for over 4 years

00:12:35,449 --> 00:12:40,019
winning more benchmarks than others by

00:12:38,129 --> 00:12:41,939
staying on the cutting edge a top

00:12:40,019 --> 00:12:44,730
adopting new Intel architecture

00:12:41,939 --> 00:12:47,040
technologies faster being ahead of the

00:12:44,730 --> 00:12:48,930
pack in adopting latest versions of open

00:12:47,040 --> 00:12:55,259
source packages including the latest

00:12:48,930 --> 00:12:57,750
kernels and also having a strict method

00:12:55,259 --> 00:13:00,689
for maintaining our security in pulling

00:12:57,750 --> 00:13:04,290
in security patches quickly this is a

00:13:00,689 --> 00:13:07,189
blueprint for implementing Intel

00:13:04,290 --> 00:13:09,929
features in a Linux operating system

00:13:07,189 --> 00:13:13,130
last but not least we deliver

00:13:09,929 --> 00:13:13,130
performance you can experience

00:13:13,330 --> 00:13:18,130
your defaults are slowing you down more

00:13:16,660 --> 00:13:19,750
than you realize and if you buy new

00:13:18,130 --> 00:13:22,870
hardware and aren't using the right

00:13:19,750 --> 00:13:26,290
software then you're leaving a lot on

00:13:22,870 --> 00:13:27,399
the table and if you don't feel like

00:13:26,290 --> 00:13:30,670
going out and building your own

00:13:27,399 --> 00:13:33,040
end-to-end stack you can use ours this

00:13:30,670 --> 00:13:36,220
is a great way to try out new features

00:13:33,040 --> 00:13:37,690
to try out performance or check out the

00:13:36,220 --> 00:13:43,060
code to build performance into your own

00:13:37,690 --> 00:13:45,970
products I invite you to experience

00:13:43,060 --> 00:13:48,360
performance I ask you to help keep the

00:13:45,970 --> 00:13:51,190
pace in building performance solutions

00:13:48,360 --> 00:13:54,279
because we have to bring everyone along

00:13:51,190 --> 00:13:56,170
together and we have to ensure that we

00:13:54,279 --> 00:13:58,779
don't regress and that we don't build up

00:13:56,170 --> 00:14:01,240
a backup backlog of bugs that we have to

00:13:58,779 --> 00:14:05,890
go figure out the best way is to stay on

00:14:01,240 --> 00:14:07,480
top of it and other than that I hope you

00:14:05,890 --> 00:14:10,269
enjoy the rest of the conference and

00:14:07,480 --> 00:14:11,890
your time here in Lyon

00:14:10,269 --> 00:14:14,410
we will have some of our stacks demoed

00:14:11,890 --> 00:14:16,720
and then tell booths as few as well as a

00:14:14,410 --> 00:14:19,160
few other exciting demos so please stop

00:14:16,720 --> 00:14:23,789
by thank you

00:14:19,160 --> 00:14:23,789

YouTube URL: https://www.youtube.com/watch?v=ip1w5vM5O_Y


