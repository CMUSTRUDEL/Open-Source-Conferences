Title: Keynote: Trusted AI: Bringing Trust Back into AI through Open Source - Todd Moore, IBM
Publication date: 2019-10-29
Playlist: Open Source Summit & Embedded Linux Conference EU 2019 - Lyon
Description: 
	Keynote: Trusted AI: Bringing Trust Back into AI through Open Source - Todd Moore, Vice President, Open Technology, Cognitive Applications, IBM

As businesses move beyond experimentation to full-blown AI projects across the enterprise, they are recognizing that thereâ€™s more to successful implementations than simply having the right datasets, AI models and scalability. Increasingly, dimensions of trust, including fairness, robustness and explainability, are important metrics that help evaluate AI model behavior. Here, IBM is going to discuss how we are leveraging the power of Open Source to bring trust back in AI.
Captions: 
	00:00:00,030 --> 00:00:06,810
well your mercy that's the extent of my

00:00:02,939 --> 00:00:10,500
French thank you all and thank you for

00:00:06,810 --> 00:00:11,940
having me and hopefully we can go from a

00:00:10,500 --> 00:00:14,700
little bit light-hearted to a little

00:00:11,940 --> 00:00:19,350
more serious topic which is really

00:00:14,700 --> 00:00:23,840
around trust in AI our world is changing

00:00:19,350 --> 00:00:26,939
AI is in invading every aspect of it and

00:00:23,840 --> 00:00:28,949
before I get into that I have to give a

00:00:26,939 --> 00:00:31,619
shout out to a dock maintainer some of

00:00:28,949 --> 00:00:32,969
my teams in the audience so Brad Topol

00:00:31,619 --> 00:00:35,370
you guys can tweet all this right

00:00:32,969 --> 00:00:39,719
shoutout to brad toko kubernetes dock

00:00:35,370 --> 00:00:42,719
maintainer de at IBM great guy and he's

00:00:39,719 --> 00:00:45,120
helping to run the docs sprint set at

00:00:42,719 --> 00:00:54,090
kubernetes so please give Brad a shout

00:00:45,120 --> 00:00:56,190
out thank you so think about it right

00:00:54,090 --> 00:00:58,879
and this is a quote from Kevin Kelly

00:00:56,190 --> 00:01:01,530
Kevin's a noted author anybody who saw

00:00:58,879 --> 00:01:04,799
Minority Report that was some of his

00:01:01,530 --> 00:01:06,240
work and and he's thought about AI and

00:01:04,799 --> 00:01:08,220
and it's really become very simple

00:01:06,240 --> 00:01:09,540
there'll be 10,000 new startups and

00:01:08,220 --> 00:01:11,159
guess what they're going to take

00:01:09,540 --> 00:01:14,070
something that we already do and they're

00:01:11,159 --> 00:01:16,049
gonna add AI technology to it and you

00:01:14,070 --> 00:01:18,990
can read his book the inevitable and see

00:01:16,049 --> 00:01:20,310
a little bit more on that and and it's

00:01:18,990 --> 00:01:22,560
anything right so a little you know

00:01:20,310 --> 00:01:24,689
light-hearted you can you can look at

00:01:22,560 --> 00:01:26,580
and this is really something that exists

00:01:24,689 --> 00:01:29,159
in Toronto it's a real picture of

00:01:26,580 --> 00:01:32,430
something one of my team did his

00:01:29,159 --> 00:01:36,630
daughter works in a dance studio it's at

00:01:32,430 --> 00:01:38,759
night the security guard is gone or off

00:01:36,630 --> 00:01:40,890
to the washroom something like that and

00:01:38,759 --> 00:01:44,070
and now somebody needs to be able to get

00:01:40,890 --> 00:01:46,619
in and get to class and they can just

00:01:44,070 --> 00:01:48,390
press the button talk to an application

00:01:46,619 --> 00:01:50,520
the application can verify who that

00:01:48,390 --> 00:01:52,079
person is open the door for them and let

00:01:50,520 --> 00:01:54,450
them in and they don't have to worry

00:01:52,079 --> 00:01:56,369
about where as a security person has

00:01:54,450 --> 00:01:56,909
gone after hours they've got something

00:01:56,369 --> 00:01:59,460
to do

00:01:56,909 --> 00:02:01,740
done very simply voice recognition

00:01:59,460 --> 00:02:04,740
technology text-to-speech

00:02:01,740 --> 00:02:06,930
to then go and play the response and and

00:02:04,740 --> 00:02:11,310
of course make the door open very simple

00:02:06,930 --> 00:02:13,020
smart things happened right but but we

00:02:11,310 --> 00:02:13,840
start to get more complex as we start

00:02:13,020 --> 00:02:15,819
using

00:02:13,840 --> 00:02:17,650
visual technologies as an example and

00:02:15,819 --> 00:02:20,440
there's adversarial attacks so that can

00:02:17,650 --> 00:02:22,209
exist as a result of just very simple

00:02:20,440 --> 00:02:25,090
things you can do to manipulate an image

00:02:22,209 --> 00:02:27,670
you can insert noise not a lot of noise

00:02:25,090 --> 00:02:31,090
in this case right giant panda becomes a

00:02:27,670 --> 00:02:32,860
monkey right done just by tricking it

00:02:31,090 --> 00:02:35,380
with a little bit of noise because the

00:02:32,860 --> 00:02:37,660
computer is looking at the little pixels

00:02:35,380 --> 00:02:40,180
around the things the elements that are

00:02:37,660 --> 00:02:42,160
inside of the image it's not looking

00:02:40,180 --> 00:02:44,170
necessarily at the big thing it can't

00:02:42,160 --> 00:02:47,500
make that sort of determination yet that

00:02:44,170 --> 00:02:49,390
we can but deep neural networks are

00:02:47,500 --> 00:02:51,459
evolving they're getting better there

00:02:49,390 --> 00:02:54,220
they're taking us down this path of

00:02:51,459 --> 00:02:56,739
helping us you know make decisions and

00:02:54,220 --> 00:02:58,269
of course you can then go and think

00:02:56,739 --> 00:03:01,840
about this in terms of your life well

00:02:58,269 --> 00:03:04,780
simple things added to as noise into an

00:03:01,840 --> 00:03:07,630
image can then hurt my bank account

00:03:04,780 --> 00:03:09,760
right somebody can very easily full a

00:03:07,630 --> 00:03:11,319
machine can you really trust that check

00:03:09,760 --> 00:03:14,860
scanner that you have going on in the

00:03:11,319 --> 00:03:16,930
background so as we get to more and more

00:03:14,860 --> 00:03:19,540
complex decisions that we have to go and

00:03:16,930 --> 00:03:22,030
make it becomes really necessary to

00:03:19,540 --> 00:03:24,700
think about what's going on credit

00:03:22,030 --> 00:03:28,000
applications admission to colleges

00:03:24,700 --> 00:03:30,220
employment applications your health care

00:03:28,000 --> 00:03:31,810
whether if you've committed a crime

00:03:30,220 --> 00:03:33,819
possibly how you're going to get

00:03:31,810 --> 00:03:37,329
sentenced all these things are being

00:03:33,819 --> 00:03:43,000
assisted by AI technology this can be

00:03:37,329 --> 00:03:44,859
kind of scary and and you really let me

00:03:43,000 --> 00:03:47,650
go back one here for a second quick too

00:03:44,859 --> 00:03:49,840
fast and what you have to do is look at

00:03:47,650 --> 00:03:52,780
is the data been tampered with has a

00:03:49,840 --> 00:03:55,359
model been tampered with is there's

00:03:52,780 --> 00:03:57,579
something inherently biased or unfair we

00:03:55,359 --> 00:03:59,980
heard a little bit about that yesterday

00:03:57,579 --> 00:04:00,940
in some of the discussion about ethics

00:03:59,980 --> 00:04:02,620
all right

00:04:00,940 --> 00:04:05,260
can you look at that model that black

00:04:02,620 --> 00:04:09,459
box and understand it in any any shape

00:04:05,260 --> 00:04:12,609
or fashion is what's being done tracked

00:04:09,459 --> 00:04:14,920
and accountable so how do we solve these

00:04:12,609 --> 00:04:16,359
problems well you know iBM has this long

00:04:14,920 --> 00:04:18,070
history of being out and available an

00:04:16,359 --> 00:04:20,289
open-source working in projects like

00:04:18,070 --> 00:04:22,659
Linux and others and other things before

00:04:20,289 --> 00:04:24,729
that and doing it across the variety of

00:04:22,659 --> 00:04:27,169
different organizations so that's our

00:04:24,729 --> 00:04:32,460
solution as well to

00:04:27,169 --> 00:04:34,380
and there we go so we've put out some

00:04:32,460 --> 00:04:36,510
open source projects adversarial

00:04:34,380 --> 00:04:38,250
robustness toolbox it's up there on

00:04:36,510 --> 00:04:40,979
github you can go and play with it it

00:04:38,250 --> 00:04:43,530
allows you to in look at your models

00:04:40,979 --> 00:04:45,060
inspect them look at the data make sure

00:04:43,530 --> 00:04:46,830
that nothing is being tampered with

00:04:45,060 --> 00:04:50,729
there's a set of tools there that you

00:04:46,830 --> 00:04:54,690
can go and use the AI fairness 360

00:04:50,729 --> 00:04:56,460
toolbox that's there helps you with 70

00:04:54,690 --> 00:04:58,979
different functions they're available to

00:04:56,460 --> 00:05:00,960
look at models and assess them in ten

00:04:58,979 --> 00:05:02,669
different ways to help you go and fix

00:05:00,960 --> 00:05:06,020
that model when it's broken in some way

00:05:02,669 --> 00:05:08,010
and then of course explain ability is

00:05:06,020 --> 00:05:10,139
something that we all have to deal with

00:05:08,010 --> 00:05:11,660
right when those models are being

00:05:10,139 --> 00:05:14,070
created these days we can't necessarily

00:05:11,660 --> 00:05:15,990
understand how they were put together

00:05:14,070 --> 00:05:17,850
they look like black boxes we have to be

00:05:15,990 --> 00:05:20,310
able to probe them we have to be able to

00:05:17,850 --> 00:05:21,600
understand whether the data is biased in

00:05:20,310 --> 00:05:23,850
some way we have to be able to make

00:05:21,600 --> 00:05:25,410
corrections and and so looking at that

00:05:23,850 --> 00:05:27,330
black box and turning it into a model

00:05:25,410 --> 00:05:29,190
that we humans can understand the

00:05:27,330 --> 00:05:31,200
regulator's can understand the consumers

00:05:29,190 --> 00:05:34,110
can understand is very important the

00:05:31,200 --> 00:05:36,090
toolbox there will help you so we apply

00:05:34,110 --> 00:05:37,950
these tools as well in our products

00:05:36,090 --> 00:05:39,930
what's an open scale has this as part of

00:05:37,950 --> 00:05:42,270
its dashboard allows you to track and

00:05:39,930 --> 00:05:43,289
measure your AI outcomes all throughout

00:05:42,270 --> 00:05:47,789
the lifecycle throughout that whole

00:05:43,289 --> 00:05:49,770
pipeline so one of the things you can do

00:05:47,789 --> 00:05:52,530
to get involved in this is to join the

00:05:49,770 --> 00:05:54,150
Linux Foundation ai Committee right

00:05:52,530 --> 00:05:55,800
there's a principals working group

00:05:54,150 --> 00:05:57,449
that's looking at the ethics and the

00:05:55,800 --> 00:06:00,240
principles of how we all need to operate

00:05:57,449 --> 00:06:02,400
as well as use cases and they're going

00:06:00,240 --> 00:06:04,139
to help really take these tough problems

00:06:02,400 --> 00:06:07,199
and and turn them into something that's

00:06:04,139 --> 00:06:08,520
you know we can all work with and it's

00:06:07,199 --> 00:06:10,500
course it's a worldwide group that's

00:06:08,520 --> 00:06:13,860
formed across North America Europe and

00:06:10,500 --> 00:06:16,860
Asia and lastly you can come join us at

00:06:13,860 --> 00:06:20,550
IBM at code a we did take a French term

00:06:16,860 --> 00:06:23,010
coder so you plus the work that we're

00:06:20,550 --> 00:06:25,670
doing in code a come join us join these

00:06:23,010 --> 00:06:31,249
projects be part of it thank you

00:06:25,670 --> 00:06:31,249

YouTube URL: https://www.youtube.com/watch?v=N6NGz4T7dkg


