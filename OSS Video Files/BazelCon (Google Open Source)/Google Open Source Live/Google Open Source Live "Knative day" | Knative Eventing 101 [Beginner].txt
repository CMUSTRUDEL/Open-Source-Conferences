Title: Google Open Source Live "Knative day" | Knative Eventing 101 [Beginner]
Publication date: 2020-11-02
Playlist: Google Open Source Live
Description: 
	Knative Eventing makes it easy to build event-driven serverless applications that connect disparate systems. In this session we will walkthrough an end-to-end event-driven application built with Knative Eventing, while introducing core concepts along the way.
Captions: 
	00:00:10,660 --> 00:00:13,550
Hello, and welcome to Knative Eventing 101.

00:00:13,550 --> 00:00:16,110
My name is Akash and I am an Engineering Manager at Google.

00:00:16,110 --> 00:00:19,029
I have with me Chen, who is an engineer at Google

00:00:19,029 --> 00:00:21,360
and we both work on Knative Eventing.

00:00:21,360 --> 00:00:23,349
This is a beginner-level session

00:00:23,349 --> 00:00:24,640
and we will introduce you to Knative Eventing concepts.

00:00:24,640 --> 00:00:28,880
So here is what we're going to cover today.

00:00:28,880 --> 00:00:33,180
We'll start with the rise of microservices

00:00:33,180 --> 00:00:34,629
and some of the issues inherent in them.

00:00:34,629 --> 00:00:37,559
Then we'll go over event-driven architectures

00:00:37,559 --> 00:00:39,830
and address some of those issues that we have introduced.

00:00:39,830 --> 00:00:44,320
We will then show you how Knative Eventing fits into all of this.

00:00:44,320 --> 00:00:48,260
Then we'll give you a demo of an event-driven application

00:00:48,260 --> 00:00:49,700
built using Knative

00:00:49,700 --> 00:00:52,790
that does some sentiment analysis on a complex image.

00:00:52,790 --> 00:00:56,989
We will then follow it up with a deep dive into the application architecture

00:00:56,989 --> 00:00:59,739
while introducing the core concepts of Eventing.

00:00:59,739 --> 00:01:03,940
Finally, we'll introduce you to our vibrant community of developers.

00:01:03,940 --> 00:01:07,790
So, first, let's talk about the rise of microservices.

00:01:07,790 --> 00:01:10,060
In the early days

00:01:10,060 --> 00:01:15,470
we saw large applications architected as monoliths.

00:01:15,470 --> 00:01:18,479
In early 2000 we saw a rise of service-oriented architectures.

00:01:18,479 --> 00:01:20,820
But they were still built as monoliths.

00:01:20,820 --> 00:01:25,190
You'll be surprised that even today many organizations have monoliths.

00:01:25,190 --> 00:01:26,890
So now, what is a monolith?

00:01:26,890 --> 00:01:30,860
A monolith is where there is a single application layer

00:01:30,860 --> 00:01:33,030
which contains your presentation layer

00:01:33,030 --> 00:01:35,729
your business project, and the database layer

00:01:35,729 --> 00:01:38,050
all integrated into a single platform.

00:01:38,050 --> 00:01:41,450
Take this e-commerce store as an example.

00:01:41,450 --> 00:01:44,729
All its five services run under a single application layer.

00:01:44,729 --> 00:01:49,890
Even though this looks simple, there are some inherent issues in it.

00:01:49,890 --> 00:01:54,440
First, as the incoming rates of request increase

00:01:54,440 --> 00:01:56,890
we will require more resources to process

00:01:56,890 --> 00:01:58,440
and hence scale the application.

00:01:58,440 --> 00:02:02,000
Now, a monolith can scale in only one direction

00:02:02,000 --> 00:02:03,160
and that is vertical.

00:02:03,160 --> 00:02:07,520
This means we can scale the application over a single machine

00:02:07,520 --> 00:02:12,590
by adding more and more hardware resources and there's always a limit to it.

00:02:12,590 --> 00:02:16,250
Scaling horizontally across machines is hard.

00:02:16,250 --> 00:02:21,860
As the application grows, we will have more services and more teams.

00:02:21,860 --> 00:02:24,030
It will soon become overwhelming for developers

00:02:24,030 --> 00:02:28,240
to build and maintain the application codebase.

00:02:28,240 --> 00:02:31,590
Not just growing the application, but even changing anything

00:02:31,590 --> 00:02:35,900
will require developers to build, test the entire application

00:02:35,900 --> 00:02:39,250
which is going to be a nightmare.

00:02:39,250 --> 00:02:44,420
To work on these problems, you could adopt a microservices architecture

00:02:44,420 --> 00:02:48,670
and start modularizing the application into small, standalone services

00:02:48,670 --> 00:02:53,780
that will be built, deployed, scaled, and maintained independently.

00:02:53,780 --> 00:02:57,230
We start by pulling two of these services and the database out.

00:02:57,230 --> 00:03:00,470
Followed by a few more services.

00:03:00,470 --> 00:03:05,120
Finally, we have all services independently pulled out.

00:03:05,120 --> 00:03:09,280
In reality, we'll break these services down into more modular microservices

00:03:09,280 --> 00:03:10,980
but for the simplicity of this presentation

00:03:10,980 --> 00:03:13,760
I have not done so.

00:03:13,760 --> 00:03:15,350
Now that each service is independent

00:03:15,350 --> 00:03:20,520
let's see what the advantages are that we get out of this architecture.

00:03:20,520 --> 00:03:26,030
First, microservices architecture introduces a concept of separation of concerns.

00:03:26,030 --> 00:03:29,330
This promotes agile development.

00:03:29,330 --> 00:03:32,340
Each microservice has its independent codebase

00:03:32,340 --> 00:03:36,170
and is built, maintained, and deployed independently by individual teams.

00:03:36,170 --> 00:03:42,840
Each team has the flexibility to build a service in the language of their choice.

00:03:42,840 --> 00:03:48,150
And then also update the tech stack with each service independently.

00:03:48,150 --> 00:03:50,870
On top of all of this, now we can scale our application

00:03:50,870 --> 00:03:53,220
in each individual service independently.

00:03:53,220 --> 00:03:55,800
Not just vertically, but even horizontally.

00:03:55,800 --> 00:04:00,740
And these are all advantages of a microservices architecture, which is good.

00:04:00,740 --> 00:04:03,459
But we also need to know about some of the drawbacks

00:04:03,459 --> 00:04:06,000
that are inherent in this architecture.

00:04:06,000 --> 00:04:09,830
First, if you look at the figure

00:04:09,830 --> 00:04:16,250
we have introduced a spider web of point-to-point integrations.

00:04:16,250 --> 00:04:19,709
Even though each service's codebase is decoupled with its own contract

00:04:19,709 --> 00:04:23,470
we have introduced coupling with direct service-to-service communication.

00:04:23,470 --> 00:04:28,540
Due to this inter-service communication, which is between two services

00:04:28,540 --> 00:04:31,530
adding or removing a service requires updating upstream

00:04:31,530 --> 00:04:33,870
as well as downstream dependencies.

00:04:33,870 --> 00:04:38,870
Now here is exactly where event-driven microservices come to rescue.

00:04:38,870 --> 00:04:42,320
But before I go into event-driven microservices architecture

00:04:42,320 --> 00:04:43,710
I just want to take a step back

00:04:43,710 --> 00:04:47,590
and take couple of minutes to go over what exactly an event is.

00:04:47,590 --> 00:04:50,190
Now, as a software system operates

00:04:50,190 --> 00:04:54,070
an occurrence is a capture of a statement of fact.

00:04:54,070 --> 00:04:58,510
An event is a record expressing the occurrence and its context.

00:04:58,510 --> 00:05:03,430
An action is executed when it is notified about the occurrence by receiving the event.

00:05:03,430 --> 00:05:06,200
So, in simple words, something happened.

00:05:06,200 --> 00:05:09,060
We captured that something happened in an event.

00:05:09,060 --> 00:05:12,090
So the event has the occurrence as well as the context data.

00:05:12,090 --> 00:05:16,500
And then one or more actions are triggered based on that data.

00:05:16,500 --> 00:05:19,690
It is important to note that events represent facts

00:05:19,690 --> 00:05:22,500
and therefore they do not include the destination.

00:05:22,500 --> 00:05:28,400
And the producer has no expectation of how the event is going to be handled.

00:05:28,400 --> 00:05:31,290
So now, back to event-driven architecture.

00:05:31,290 --> 00:05:35,040
An event-driven architecture has three main components.

00:05:35,040 --> 00:05:37,180
First, the event producers.

00:05:37,180 --> 00:05:41,100
They are exactly what their name says: they produce the events.

00:05:41,100 --> 00:05:44,180
Second, there is an intermediary that receives an event

00:05:44,180 --> 00:05:46,070
and routes it to the next receiver.

00:05:46,070 --> 00:05:49,389
This next receiver can be another intermediary

00:05:49,389 --> 00:05:51,480
or it can be a consumer.

00:05:51,480 --> 00:05:53,900
And that is what the third part is: a consumer.

00:05:53,900 --> 00:05:58,080
A consumer receives an event and acts on it.

00:05:58,080 --> 00:06:01,840
It uses the context and the data to execute some logic

00:06:01,840 --> 00:06:04,090
which might lead to the occurrence of new events.

00:06:04,090 --> 00:06:08,570
So now you know that a service can act as a consumer

00:06:08,570 --> 00:06:13,110
as well as a producer at the same time.

00:06:13,110 --> 00:06:15,199
So what are the benefits of this architecture?

00:06:15,199 --> 00:06:20,180
First, the services are not fully decoupled.

00:06:20,180 --> 00:06:22,479
No point-to-point communication.

00:06:22,479 --> 00:06:25,139
Producers have no idea about who will consume the event.

00:06:25,139 --> 00:06:27,110
Consumers just tell the intermediary

00:06:27,110 --> 00:06:29,430
that they are interested in a specific kind of event

00:06:29,430 --> 00:06:32,229
and the events just get delivered.

00:06:32,229 --> 00:06:35,830
This is highly scalable.

00:06:35,830 --> 00:06:38,200
Now you can extend it organically.

00:06:38,200 --> 00:06:40,790
Because there is no direct point-to-point communication

00:06:40,790 --> 00:06:42,660
between producers and consumers

00:06:42,660 --> 00:06:45,310
one can add producers and consumers independently.

00:06:45,310 --> 00:06:51,650
Now where does Knative Eventing fit into this architectural style?

00:06:51,650 --> 00:06:55,729
Knative Eventing is an intermediary.

00:06:55,729 --> 00:06:58,610
It is a set of composable primitives

00:06:58,610 --> 00:07:02,370
to enable late binding of producers and consumers.

00:07:02,370 --> 00:07:07,650
But, for an intermediary to work for every kind of application

00:07:07,650 --> 00:07:09,740
you need to standardize on something.

00:07:09,740 --> 00:07:12,979
And that's where we have standardized on the event envelope.

00:07:12,979 --> 00:07:17,800
Knative uses Cloud Events as the event envelope.

00:07:17,800 --> 00:07:20,320
Cloud Events is a vendor-neutral specification

00:07:20,320 --> 00:07:22,110
for defining the format of event data.

00:07:22,110 --> 00:07:24,389
It is a CNCF project.

00:07:24,389 --> 00:07:30,040
In Knative, we call the producers "event sources".

00:07:30,040 --> 00:07:33,060
These produce specifically Cloud Events.

00:07:33,060 --> 00:07:35,490
This could be your own service producing a Cloud Event

00:07:35,490 --> 00:07:39,430
or it could be a special source, such as a GitHub source or a Kafka source

00:07:39,430 --> 00:07:42,930
that can convert GitHub and Kafka events into Cloud Events

00:07:42,930 --> 00:07:45,840
and feed them into a Knative-based application.

00:07:45,840 --> 00:07:50,650
We'll go over some of these details in later slides.

00:07:50,650 --> 00:07:55,200
A consumer could be any service that can receive an event.

00:07:55,200 --> 00:07:58,660
And we'll also go over the details of what defines a consumer

00:07:58,660 --> 00:08:01,570
later in these slides.

00:08:01,570 --> 00:08:05,990
A broker represents an event mesh.

00:08:05,990 --> 00:08:09,120
Events are sent to the broker's ingress.

00:08:09,120 --> 00:08:13,080
And then sent to any subscribers that are interested in that event.

00:08:13,080 --> 00:08:19,039
A trigger is how services subscribe to the events from a specific broker.

00:08:19,039 --> 00:08:24,360
Along with the intermediary, we also have events and sources registry.

00:08:24,360 --> 00:08:27,150
Now, why do we need a registry?

00:08:27,150 --> 00:08:29,509
Think about examples like client tools

00:08:29,509 --> 00:08:32,140
trying to list all the sources available on the cluster.

00:08:32,140 --> 00:08:35,269
Or if you want to find out what other kinds of events are available on the cluster

00:08:35,269 --> 00:08:37,599
this is where the sources and events registry

00:08:37,599 --> 00:08:41,349
helps build the user experience.

00:08:41,349 --> 00:08:44,541
Broker and trigger are not the only primitives that we have.

00:08:44,541 --> 00:08:47,680
We have more, such as channels, and sequences, and parallels

00:08:47,680 --> 00:08:50,790
which you can use to stitch together a complex application.

00:08:50,790 --> 00:08:54,660
We'll go over all these later.

00:08:54,660 --> 00:08:57,819
Okay, so now enough talking.

00:08:57,819 --> 00:09:00,820
Let me hand over to Chen, who will show you Knative in action.

00:09:00,820 --> 00:09:03,209
Chen, go for it.

00:09:03,209 --> 00:09:04,399
Thanks, Akash.

00:09:04,399 --> 00:09:06,199
Hi, I'm Chen.

00:09:06,199 --> 00:09:09,749
I'm going to demo a sentiment analysis application.

00:09:09,749 --> 00:09:14,110
In this demo I will upload such an image to a Google storage bucket.

00:09:14,110 --> 00:09:18,499
The application will detect all the faces in the image, like here

00:09:18,499 --> 00:09:21,429
extract them out and determine emotion for them.

00:09:21,429 --> 00:09:24,550
We will see the results in the application's webpage

00:09:24,550 --> 00:09:28,040
soon after I upload the image.

00:09:28,040 --> 00:09:30,639
Here I'm on my Cloud Storage bucket page.

00:09:30,639 --> 00:09:34,519
At the moment, I have no image uploaded in my bucket.

00:09:34,519 --> 00:09:38,709
Here is the application's webpage.

00:09:38,709 --> 00:09:41,470
It's where we will see the final results from the processing.

00:09:41,470 --> 00:09:44,819
Of course, it's empty at the moment.

00:09:44,819 --> 00:09:49,939
Now, let me upload an image.

00:09:49,939 --> 00:09:54,589
The exact same image we have seen in the slide before.

00:09:54,589 --> 00:09:57,689
Okay, done.

00:09:57,689 --> 00:10:03,060
We should see the result here in a moment.

00:10:03,060 --> 00:10:06,429
The application will extract all the faces out from the original image

00:10:06,429 --> 00:10:16,259
and score them individually.

00:10:16,259 --> 00:10:18,029
Here you can see the result.

00:10:18,029 --> 00:10:22,569
Each face with their corresponding score.

00:10:22,569 --> 00:10:24,249
Let's look behind the scenes.

00:10:24,249 --> 00:10:27,970
The application consists of these components.

00:10:27,970 --> 00:10:31,559
We use the Cloud Storage bucket to store the images uploaded.

00:10:31,559 --> 00:10:34,930
We use a Firestore database to store the final results.

00:10:34,930 --> 00:10:39,779
We have five microservices deployed as Knative service on a GKE cluster.

00:10:39,779 --> 00:10:42,050
Each of them has a different responsibility.

00:10:42,050 --> 00:10:47,209
The first service is responsible for detecting faces in the original image.

00:10:47,209 --> 00:10:52,209
Essentially, it will generate the coordinates of the face bounding boxes.

00:10:52,209 --> 00:10:54,420
Given these coordinates

00:10:54,420 --> 00:11:00,050
the second service will extract the face boxes out as separate images.

00:11:00,050 --> 00:11:02,269
The third service will run an algorithm

00:11:02,269 --> 00:11:06,279
and generate the emotion score for a given face box.

00:11:06,279 --> 00:11:11,570
And the fourth service is responsible for writing the score to the Firestore database.

00:11:11,570 --> 00:11:19,399
And the fifth service is for the UI, which reads the results from the database.

00:11:19,399 --> 00:11:23,800
Now let's look at how these components are connected together.

00:11:23,800 --> 00:11:26,139
When I uploaded the image to the storage bucket

00:11:26,139 --> 00:11:28,589
we have a Cloud Storage source

00:11:28,589 --> 00:11:32,249
that imports the Cloud Storage events to the broker.

00:11:32,249 --> 00:11:35,019
The first trigger subscribes to such events

00:11:35,019 --> 00:11:38,449
and triggers the first service, which used the metadata in the event

00:11:38,449 --> 00:11:44,559
to retrieve the image and detect the faces' bounding boxes.

00:11:44,559 --> 00:11:48,759
For each face in the original image, it produced a new event back to the broker

00:11:48,759 --> 00:11:51,720
with the bounding box coordinates.

00:11:51,720 --> 00:11:54,059
The second trigger subscribed to such events

00:11:54,059 --> 00:11:56,050
and invoked the second service

00:11:56,050 --> 00:12:01,560
which used the coordinates to extract the face boxes as separate images.

00:12:01,560 --> 00:12:02,739
For each face image

00:12:02,739 --> 00:12:06,640
it produced a new event with a reference to the image.

00:12:06,640 --> 00:12:09,100
The third trigger subscribed to such events

00:12:09,100 --> 00:12:11,079
and invoked the third service

00:12:11,079 --> 00:12:13,989
which output the emotion score in events.

00:12:13,989 --> 00:12:17,089
The fourth trigger subscribed to such events

00:12:17,089 --> 00:12:20,709
and invoked the fourth service that wrote the results to the database.

00:12:20,709 --> 00:12:25,290
Eventually, the UI loaded the results from the database.

00:12:25,290 --> 00:12:28,749
As you can, see as a developer

00:12:28,749 --> 00:12:31,519
I only need to focus on implementing the services

00:12:31,519 --> 00:12:34,619
and defining the triggers that invoke them.

00:12:34,619 --> 00:12:40,589
The broker will take care of the rest of the communication.

00:12:40,589 --> 00:12:44,040
Now you have seen how these components are connected together

00:12:44,040 --> 00:12:48,009
let me give you a quick peek at what's really happening in the cluster.

00:12:48,009 --> 00:12:52,629
Here in my terminal, I'm tailing the logs from all the services

00:12:52,629 --> 00:12:53,889
except the one for the UI.

00:12:53,889 --> 00:12:56,149
They are in order.

00:12:56,149 --> 00:12:59,290
This is the first service that detects all the faces.

00:12:59,290 --> 00:13:02,869
This is the second service that extracts all the face boxes.

00:13:02,869 --> 00:13:04,910
This is the third one that determines the score.

00:13:04,910 --> 00:13:10,029
And this is the fourth one that writes the results to the database.

00:13:10,029 --> 00:13:12,579
Here, let me upload another image

00:13:12,579 --> 00:13:20,470
and you will see how they are triggered one after another.

00:13:20,470 --> 00:13:27,860
The first service got triggered.

00:13:27,860 --> 00:13:31,699
The second one got triggered.

00:13:31,699 --> 00:13:35,230
Now the third one.

00:13:35,230 --> 00:13:39,639
And now the fourth one.

00:13:39,639 --> 00:13:44,470
Here is a clear view of the logs.

00:13:44,470 --> 00:13:45,759
This is the first service.

00:13:45,759 --> 00:13:52,350
As we can see, it received a Cloud Storage event

00:13:52,350 --> 00:13:57,549
with the reference to the image I just uploaded.

00:13:57,549 --> 00:13:59,489
And it produced a new event

00:13:59,489 --> 00:14:03,550
where it has the coordinates of the face boxes.

00:14:03,550 --> 00:14:08,579
In the second service, it received those events

00:14:08,579 --> 00:14:13,299
and it produced a new event with the extracted face boxes

00:14:13,299 --> 00:14:18,399
with the reference, such as here.

00:14:18,399 --> 00:14:21,600
In the third service, it received those previous events

00:14:21,600 --> 00:14:25,129
and started to determine the sentiment scores.

00:14:25,129 --> 00:14:29,899
And here, that's one sentiment score for a face box.

00:14:29,899 --> 00:14:33,000
And it produced these new events.

00:14:33,000 --> 00:14:36,170
And in the fourth service, it received this event

00:14:36,170 --> 00:14:39,350
and wrote the data to the database.

00:14:39,350 --> 00:14:45,329
As mentioned earlier, Knative Eventing provides composable primitives

00:14:45,329 --> 00:14:49,230
to enable late-binding event producers and consumers.

00:14:49,230 --> 00:14:55,509
So now let's look at those concepts in more detail.

00:14:55,509 --> 00:14:58,040
A Knative Eventing source is a component

00:14:58,040 --> 00:15:02,059
that generates events or imports events from external producers.

00:15:02,059 --> 00:15:08,049
The main responsibility of a source is to produce events in Cloud Events format

00:15:08,049 --> 00:15:12,470
which is the standard format we uniformly adopt in the project.

00:15:12,470 --> 00:15:17,230
For example, GitHub can be configured to produce a lot of events

00:15:17,230 --> 00:15:19,980
but they are not in Cloud Events format.

00:15:19,980 --> 00:15:23,819
And we have a GitHub source that converts them to Cloud Events format.

00:15:23,819 --> 00:15:26,929
There are a lot of existing sources.

00:15:26,929 --> 00:15:29,429
Some are community owned, like Kafka source

00:15:29,429 --> 00:15:31,829
GitHub source, as I just mentioned.

00:15:31,829 --> 00:15:34,410
Some are vendor owned, such as Cloud Storage source

00:15:34,410 --> 00:15:36,730
as you have seen in the demo.

00:15:36,730 --> 00:15:40,899
TriggerMesh also supports AWS SQS source, etc.

00:15:40,899 --> 00:15:42,759
You can even write your own custom sources.

00:15:42,759 --> 00:15:46,249
For example, let's say you have a legacy system

00:15:46,249 --> 00:15:49,470
that generates the events in some legacy formats.

00:15:49,470 --> 00:15:50,669
You can write a custom source

00:15:50,669 --> 00:15:53,879
that converts those events into Cloud Events format

00:15:53,879 --> 00:15:56,189
and then with trigger and broker

00:15:56,189 --> 00:15:59,199
you can integrate it with the rest of your system.

00:15:59,199 --> 00:16:04,129
There will be another session that dives deep into Knative sources.

00:16:04,129 --> 00:16:10,519
Feel free to take a look if you're interested.

00:16:10,519 --> 00:16:14,839
Broker and trigger are the main events intermediary in Knative Eventing.

00:16:14,839 --> 00:16:18,639
You can treat a broker as a black box that you throw events into

00:16:18,639 --> 00:16:21,329
and use trigger to subscribe events from it.

00:16:21,329 --> 00:16:26,319
In a trigger, you can specify filters using Cloud Events attributes

00:16:26,319 --> 00:16:29,859
so that you only subscribe to events you're interested in.

00:16:29,859 --> 00:16:33,009
The event sink in a trigger could be any addressable.

00:16:33,009 --> 00:16:35,779
"Addressable" here means any resource reference

00:16:35,779 --> 00:16:40,920
that could be resolved to a URL, or just a plain URL.

00:16:40,920 --> 00:16:43,800
Knative Eventing provides a default broker implementation.

00:16:43,800 --> 00:16:46,649
There are alternative broker implementations

00:16:46,649 --> 00:16:51,920
such as GCP PubSub backed broker, Kafka-based Broker, etc.

00:16:51,920 --> 00:16:55,759
There will be another session that dives deep into broker and trigger.

00:16:55,759 --> 00:16:58,619
Again, feel free to take a look.

00:16:58,619 --> 00:17:06,000
Here are some lower-level primitives in Knative Eventing.

00:17:06,000 --> 00:17:09,010
Channel and subscription are the messaging primitives.

00:17:09,010 --> 00:17:13,209
A channel is an abstraction of a message transport

00:17:13,209 --> 00:17:17,400
which takes care of things like message persistence.

00:17:17,400 --> 00:17:21,610
And a subscription allows subscribing messages from a specific channel

00:17:21,610 --> 00:17:23,650
which takes care of message delivery.

00:17:23,650 --> 00:17:31,550
There are many implementations for them, such as Kafka, SQS, NATS, PubSub.

00:17:31,550 --> 00:17:34,170
The default broker implementation in Knative Eventing

00:17:34,170 --> 00:17:36,830
uses the messaging primitives.

00:17:36,830 --> 00:17:42,410
There is another category of primitives called "flows".

00:17:42,410 --> 00:17:45,920
There are sequence and parallel.

00:17:45,920 --> 00:17:51,570
They both allow invoking a list of services with the difference intuitively that

00:17:51,570 --> 00:17:56,360
sequence will invoke them sequentially, while parallel will invoke them in parallel.

00:17:56,360 --> 00:18:00,230
They could come in handy for certain use cases.

00:18:00,230 --> 00:18:06,270
For example, sequence makes it really easy to build an event augmenting flow

00:18:06,270 --> 00:18:10,270
where, at the beginning of the flow, you have a bare minimal event

00:18:10,270 --> 00:18:15,160
and in each service in the flow you add additional information to the event

00:18:15,160 --> 00:18:20,110
and in the end you have an event with much richer information.

00:18:20,110 --> 00:18:24,830
The benefit of using a flow is that you can treat it as a unit of work

00:18:24,830 --> 00:18:28,470
in which any intermediate events won't be visible from outside.

00:18:28,470 --> 00:18:31,890
This prevents unexpected events flowing into your system

00:18:31,890 --> 00:18:33,160
and causing unexpected behavior.

00:18:33,160 --> 00:18:39,120
A service in a flow is essentially an addressable

00:18:39,120 --> 00:18:43,290
that could consume events and potentially produce events also.

00:18:43,290 --> 00:18:47,830
By this definition, a service in a flow could be another flow

00:18:47,830 --> 00:18:50,580
or something like a broker.

00:18:50,580 --> 00:18:54,230
You can actually build more complex flows when you combine them.

00:18:54,230 --> 00:19:02,720
For example, in a parallel, you can invoke a sequence, a broker, and another parallel.

00:19:02,720 --> 00:19:05,600
Thank you everyone.

00:19:05,600 --> 00:19:07,400
Let me hand it back to Akash.

00:19:07,400 --> 00:19:10,400
Thank you, Chen.

00:19:10,400 --> 00:19:14,350
So, as I promised in the beginning of this presentation

00:19:14,350 --> 00:19:17,550
I will introduce you to our vibrant community of developers

00:19:17,550 --> 00:19:21,760
Knative is an Open Source project under Apache license

00:19:21,760 --> 00:19:24,490
and has a vibrant community of developers around the globe.

00:19:24,490 --> 00:19:28,160
Here are a few highlights about our community.

00:19:28,160 --> 00:19:32,100
We have two core components: Serving and Eventing.

00:19:32,100 --> 00:19:36,190
Today what we introduced you to was Eventing.

00:19:36,190 --> 00:19:40,040
Serving is another core piece that lets you build a microservice

00:19:40,040 --> 00:19:44,250
in the most simplistic manner and host it in any Kubernetes-based environment.

00:19:44,250 --> 00:19:48,980
We have ten active working groups with around 450 contributors.

00:19:48,980 --> 00:19:53,120
We have more than 15 active repositories.

00:19:53,120 --> 00:19:57,750
On top of all this development, we have seven different Knative-based offerings

00:19:57,750 --> 00:20:05,030
from vendors such as Google, IBM, Red Hat, TriggerMesh, and VMWare.

00:20:05,030 --> 00:20:08,540
Knative is maintained by the community

00:20:08,540 --> 00:20:13,730
which includes organizations such as Google, VMWare, SAP, Red Hat, IBM

00:20:13,730 --> 00:20:16,430
and a great ecosystem of start-ups.

00:20:16,430 --> 00:20:22,320
So, how do you get involved in all of this?

00:20:22,320 --> 00:20:26,410
If you just want to kick the tires, you can start with our docs

00:20:26,410 --> 00:20:29,690
and we have some easy-quick start examples.

00:20:29,690 --> 00:20:31,880
If you have questions or you're stuck with anything

00:20:31,880 --> 00:20:34,350
reach out to us on Slack.

00:20:34,350 --> 00:20:35,880
If you have any follow-up questions

00:20:35,880 --> 00:20:39,040
then please reach out to us on our community Slack channel.

00:20:39,040 --> 00:20:44,100
Or feel free to reach out directly to me or Chen on Twitter or LinkedIn.

00:20:44,100 --> 00:20:49,150
My Twitter handle is @_akashv and LinkedIn is akashrv.

00:20:49,150 --> 00:20:53,320
And Chen's Twitter is @chenoooooooo, with eight "o"s.

00:20:53,320 --> 00:20:54,680
And LinkedIn is cshou.

00:20:54,680 --> 00:20:57,300

YouTube URL: https://www.youtube.com/watch?v=e6Xw5ioM8y4


