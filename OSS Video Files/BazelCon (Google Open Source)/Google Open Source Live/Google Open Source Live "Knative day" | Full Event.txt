Title: Google Open Source Live "Knative day" | Full Event
Publication date: 2020-11-02
Playlist: Google Open Source Live
Description: 
	
Captions: 
	00:20:44,070 --> 00:20:52,210
[Music]

00:21:09,520 --> 00:21:12,480
good morning

00:21:10,400 --> 00:21:14,159
good afternoon and good evening thank

00:21:12,480 --> 00:21:16,480
you for joining the k-native day on

00:21:14,159 --> 00:21:18,400
google open source live

00:21:16,480 --> 00:21:20,640
this is an awesome monthly series of

00:21:18,400 --> 00:21:21,120
sessions led by open source experts from

00:21:20,640 --> 00:21:23,600
google

00:21:21,120 --> 00:21:25,200
and community leaders we really

00:21:23,600 --> 00:21:26,000
appreciate the ability to come together

00:21:25,200 --> 00:21:28,640
digitally

00:21:26,000 --> 00:21:30,159
and keep this community moving forward

00:21:28,640 --> 00:21:31,679
my name is brian zimmerman product

00:21:30,159 --> 00:21:33,919
manager with google cloud

00:21:31,679 --> 00:21:35,600
working on our eventing products

00:21:33,919 --> 00:21:36,480
currently i'm in cambridge ontario and

00:21:35,600 --> 00:21:39,280
canada

00:21:36,480 --> 00:21:41,039
enjoying a brisk 56 degree weather too

00:21:39,280 --> 00:21:44,240
cold to ride the bike unfortunately

00:21:41,039 --> 00:21:45,919
but otherwise a beautiful we have a

00:21:44,240 --> 00:21:48,000
really exciting session planned for you

00:21:45,919 --> 00:21:50,480
today we're going to go from zero to 100

00:21:48,000 --> 00:21:52,880
in terms of k-native eventing

00:21:50,480 --> 00:21:54,960
first akash and chen will take us from

00:21:52,880 --> 00:21:55,760
zero to 60 with an overview of k-native

00:21:54,960 --> 00:21:57,280
eventing

00:21:55,760 --> 00:21:59,280
starting with the history and advantages

00:21:57,280 --> 00:22:01,919
of microservice architecture

00:21:59,280 --> 00:22:03,840
to the problems they create to how

00:22:01,919 --> 00:22:04,640
event-driven architecture and k-native

00:22:03,840 --> 00:22:07,760
specifically

00:22:04,640 --> 00:22:09,440
can help solve those problems next

00:22:07,760 --> 00:22:11,120
nacho and adam will tell us everything

00:22:09,440 --> 00:22:13,280
we need to know about sources

00:22:11,120 --> 00:22:15,120
a key value to candidate eventing is the

00:22:13,280 --> 00:22:17,760
ability to benefit from sources

00:22:15,120 --> 00:22:19,280
published by the larger community in

00:22:17,760 --> 00:22:20,480
this talk we'll walk you through how

00:22:19,280 --> 00:22:21,760
they work

00:22:20,480 --> 00:22:24,320
and even how you can go about creating

00:22:21,760 --> 00:22:26,640
your own sources finally grant

00:22:24,320 --> 00:22:28,240
and song will deep dive into how brokers

00:22:26,640 --> 00:22:28,960
and triggers work this is our advanced

00:22:28,240 --> 00:22:30,240
topic

00:22:28,960 --> 00:22:33,440
and could leave you with a deep

00:22:30,240 --> 00:22:35,280
understanding of the eventing components

00:22:33,440 --> 00:22:37,039
this is an extremely exciting time for

00:22:35,280 --> 00:22:38,799
us at google as we have recently

00:22:37,039 --> 00:22:40,400
released a preview to our eventing

00:22:38,799 --> 00:22:42,799
product based off the k-native

00:22:40,400 --> 00:22:44,880
technology we're talking about today

00:22:42,799 --> 00:22:47,039
i hope you enjoy the session and we look

00:22:44,880 --> 00:22:49,440
forward to seeing with the after party

00:22:47,039 --> 00:22:51,039
couple housekeeping items first don't

00:22:49,440 --> 00:22:53,520
forget to put your questions in the live

00:22:51,039 --> 00:22:54,880
q a forum below the live stream window

00:22:53,520 --> 00:22:56,559
if you're viewing in full screen you'll

00:22:54,880 --> 00:22:58,559
need to exit full screen to see

00:22:56,559 --> 00:23:01,039
the live q a forum we really look

00:22:58,559 --> 00:23:02,559
forward to your questions

00:23:01,039 --> 00:23:04,480
our sessions have been pre-recorded to

00:23:02,559 --> 00:23:06,480
allow for accurate transcripts but also

00:23:04,480 --> 00:23:08,720
so the speakers can focus on answering

00:23:06,480 --> 00:23:11,120
your questions live

00:23:08,720 --> 00:23:13,039
and then of course once we're done don't

00:23:11,120 --> 00:23:14,159
forget to join us at the after party on

00:23:13,039 --> 00:23:15,600
google meet

00:23:14,159 --> 00:23:17,280
we'll share a link to the google me at

00:23:15,600 --> 00:23:19,760
the end of the last session

00:23:17,280 --> 00:23:23,840
without further ado let's hear from

00:23:19,760 --> 00:23:23,840
akash and chen

00:24:07,760 --> 00:24:10,880
hello and welcome to k native eventing

00:24:09,919 --> 00:24:12,559
00:24:10,880 --> 00:24:14,000
my name is akash and i am an engineering

00:24:12,559 --> 00:24:15,840
manager at google

00:24:14,000 --> 00:24:17,840
i have with me chen who is an engineer

00:24:15,840 --> 00:24:18,960
at google and we both work on canadian

00:24:17,840 --> 00:24:20,799
inventory

00:24:18,960 --> 00:24:22,480
this is a beginner level session and

00:24:20,799 --> 00:24:24,960
will introduce you to k-90 eventing

00:24:22,480 --> 00:24:26,240
concepts

00:24:24,960 --> 00:24:28,640
so here's what we're going to cover

00:24:26,240 --> 00:24:30,799
today we'll start with the rise of

00:24:28,640 --> 00:24:32,880
microservices architectures and some of

00:24:30,799 --> 00:24:34,240
the issues inherent in them

00:24:32,880 --> 00:24:35,840
then they'll go over even driven

00:24:34,240 --> 00:24:37,679
architectures and address some of those

00:24:35,840 --> 00:24:40,080
issues that we've introduced

00:24:37,679 --> 00:24:42,400
we'll then show you how kennedy eventing

00:24:40,080 --> 00:24:44,080
fits into all of this

00:24:42,400 --> 00:24:45,440
then we'll give you a demo of an

00:24:44,080 --> 00:24:47,679
event-driven application

00:24:45,440 --> 00:24:50,640
built using k native that does some

00:24:47,679 --> 00:24:52,559
sentiment analysis on the complex image

00:24:50,640 --> 00:24:54,320
we'll then follow it up with a deep dive

00:24:52,559 --> 00:24:56,080
into the application architecture

00:24:54,320 --> 00:24:57,360
while introducing the core concepts of

00:24:56,080 --> 00:24:59,360
eventing

00:24:57,360 --> 00:25:02,720
finally we'll introduce to a vibrant

00:24:59,360 --> 00:25:05,120
community of developers

00:25:02,720 --> 00:25:07,279
so first let's talk about the rise of

00:25:05,120 --> 00:25:08,960
microservices

00:25:07,279 --> 00:25:11,840
in the early days we saw large

00:25:08,960 --> 00:25:14,559
applications architected as monoliths

00:25:11,840 --> 00:25:16,159
in early 2000 we saw a rise of service

00:25:14,559 --> 00:25:18,400
oriented architectures

00:25:16,159 --> 00:25:20,240
but they were still built as monoliths

00:25:18,400 --> 00:25:22,400
you'll be surprised that even today many

00:25:20,240 --> 00:25:25,679
organizations have monoliths

00:25:22,400 --> 00:25:27,520
so now what is a monopoly a monolith is

00:25:25,679 --> 00:25:28,159
where there is a single application

00:25:27,520 --> 00:25:30,320
layer

00:25:28,159 --> 00:25:31,760
which contains your presentation layer

00:25:30,320 --> 00:25:34,159
your business logic

00:25:31,760 --> 00:25:36,080
and the database layer all integrated

00:25:34,159 --> 00:25:38,640
into a single platform

00:25:36,080 --> 00:25:40,720
take this e-commerce store as an example

00:25:38,640 --> 00:25:43,440
all its five services run under

00:25:40,720 --> 00:25:44,559
a single application layer even though

00:25:43,440 --> 00:25:47,840
this looks simple

00:25:44,559 --> 00:25:50,799
there are some inherent issues in it

00:25:47,840 --> 00:25:51,679
first as the incoming rate of request

00:25:50,799 --> 00:25:53,600
increase

00:25:51,679 --> 00:25:56,480
we will require more resources to

00:25:53,600 --> 00:25:58,799
process and hence scale the application

00:25:56,480 --> 00:25:59,679
now a monolith can scale in only one

00:25:58,799 --> 00:26:02,480
direction

00:25:59,679 --> 00:26:03,360
that is vertical this means we can scale

00:26:02,480 --> 00:26:06,000
the application

00:26:03,360 --> 00:26:07,600
over a single machine by adding more and

00:26:06,000 --> 00:26:10,159
more hardware resources

00:26:07,600 --> 00:26:14,400
and there's always a limit scaling

00:26:10,159 --> 00:26:14,400
horizontally across machines is hard

00:26:14,559 --> 00:26:19,039
as application grows we will have more

00:26:17,120 --> 00:26:20,880
services and more teams

00:26:19,039 --> 00:26:23,200
it will soon become overwhelming for

00:26:20,880 --> 00:26:26,400
developers to build and maintain the

00:26:23,200 --> 00:26:27,520
codebase not just growing the

00:26:26,400 --> 00:26:29,520
application

00:26:27,520 --> 00:26:31,120
but even changing anything will require

00:26:29,520 --> 00:26:33,760
developers to build

00:26:31,120 --> 00:26:36,720
test the entire application which is

00:26:33,760 --> 00:26:36,720
going to be nightmare

00:26:37,440 --> 00:26:41,760
to work on these problems you could

00:26:39,120 --> 00:26:43,760
adopt a micro services architecture

00:26:41,760 --> 00:26:45,919
and start modularizing the application

00:26:43,760 --> 00:26:49,120
into small stand-alone services

00:26:45,919 --> 00:26:51,919
that could be built deployed scaled

00:26:49,120 --> 00:26:53,520
and maintained independently we start by

00:26:51,919 --> 00:26:55,520
pulling two of these services and the

00:26:53,520 --> 00:26:59,039
database up

00:26:55,520 --> 00:27:00,240
followed by few more services finally we

00:26:59,039 --> 00:27:03,120
have all services

00:27:00,240 --> 00:27:05,039
independently pulled up in reality we

00:27:03,120 --> 00:27:06,559
will break these services down into more

00:27:05,039 --> 00:27:08,320
modular microservices

00:27:06,559 --> 00:27:10,960
but for simplicity of this presentation

00:27:08,320 --> 00:27:12,640
i have not done so

00:27:10,960 --> 00:27:14,000
now that each service is independent

00:27:12,640 --> 00:27:17,360
let's see what are the advantages that

00:27:14,000 --> 00:27:17,360
we get out of this architecture

00:27:17,440 --> 00:27:21,360
first microservices architecture

00:27:19,440 --> 00:27:23,120
introduces a concept of separation of

00:27:21,360 --> 00:27:26,480
concerns

00:27:23,120 --> 00:27:28,640
this promotes azure development

00:27:26,480 --> 00:27:29,520
each microservice has its independent

00:27:28,640 --> 00:27:31,919
code base

00:27:29,520 --> 00:27:34,720
and is built maintained and deployed

00:27:31,919 --> 00:27:36,960
independently by individual teams

00:27:34,720 --> 00:27:38,480
each team has the flexibility to build

00:27:36,960 --> 00:27:39,840
the service in the language of their

00:27:38,480 --> 00:27:42,240
choice

00:27:39,840 --> 00:27:43,120
and can also update the text type with

00:27:42,240 --> 00:27:46,559
each service

00:27:43,120 --> 00:27:48,240
independently on top of all of this now

00:27:46,559 --> 00:27:49,840
we can scale our application

00:27:48,240 --> 00:27:52,320
and each individual service

00:27:49,840 --> 00:27:54,559
independently not just vertically but

00:27:52,320 --> 00:27:55,840
even horizontally

00:27:54,559 --> 00:27:58,240
now these are all advantages of a

00:27:55,840 --> 00:27:59,760
microservices architecture which is good

00:27:58,240 --> 00:28:01,440
but we also need to know about some of

00:27:59,760 --> 00:28:04,640
the drawbacks that are

00:28:01,440 --> 00:28:07,440
inherent in this architecture

00:28:04,640 --> 00:28:08,320
first if you look at the figure we have

00:28:07,440 --> 00:28:11,039
introduced

00:28:08,320 --> 00:28:13,360
a spider web of point-to-point

00:28:11,039 --> 00:28:15,440
integrations

00:28:13,360 --> 00:28:17,360
even though each service's code base is

00:28:15,440 --> 00:28:19,279
decoupled with its own contract

00:28:17,360 --> 00:28:21,840
we have introduced coupling with direct

00:28:19,279 --> 00:28:23,840
service to service communication

00:28:21,840 --> 00:28:25,919
due to this inter-service communication

00:28:23,840 --> 00:28:27,840
which is between two services

00:28:25,919 --> 00:28:29,679
adding or removing a service requires

00:28:27,840 --> 00:28:32,000
updating upstream as well as downstream

00:28:29,679 --> 00:28:33,679
difference

00:28:32,000 --> 00:28:36,480
now here is exactly where event driven

00:28:33,679 --> 00:28:38,000
microservices come to rescue

00:28:36,480 --> 00:28:39,520
but before i go into even driven

00:28:38,000 --> 00:28:41,120
microservices architecture

00:28:39,520 --> 00:28:42,720
i just wanted to take a step back and

00:28:41,120 --> 00:28:45,120
take a couple of minutes to go over what

00:28:42,720 --> 00:28:47,840
exactly an event is

00:28:45,120 --> 00:28:48,559
now as a software system operates an

00:28:47,840 --> 00:28:51,679
occurrence

00:28:48,559 --> 00:28:53,919
is the capture for statement of fact

00:28:51,679 --> 00:28:56,320
an event is record expressing that

00:28:53,919 --> 00:28:58,159
occurrence and its context

00:28:56,320 --> 00:28:59,760
an action is executed when it is

00:28:58,159 --> 00:29:00,799
notified about the occurrence by

00:28:59,760 --> 00:29:03,679
receiving the event

00:29:00,799 --> 00:29:05,200
so in simple words something happens we

00:29:03,679 --> 00:29:06,240
capture that something happened in an

00:29:05,200 --> 00:29:08,159
event

00:29:06,240 --> 00:29:09,840
so the event has the occurrence as well

00:29:08,159 --> 00:29:11,279
as the context data

00:29:09,840 --> 00:29:13,679
and then one or more actions are

00:29:11,279 --> 00:29:15,520
triggered based on that

00:29:13,679 --> 00:29:17,039
it is important to know that events

00:29:15,520 --> 00:29:18,480
represent facts

00:29:17,039 --> 00:29:21,200
and therefore they do not include the

00:29:18,480 --> 00:29:22,080
destination and the producer has no

00:29:21,200 --> 00:29:26,399
expectation

00:29:22,080 --> 00:29:29,279
how the event is or will be handled

00:29:26,399 --> 00:29:31,279
so now back to even driven architecture

00:29:29,279 --> 00:29:32,880
and even driven architecture has three

00:29:31,279 --> 00:29:35,600
main components

00:29:32,880 --> 00:29:37,200
first they even produces they are

00:29:35,600 --> 00:29:38,480
exactly what the name says they produce

00:29:37,200 --> 00:29:40,640
the units

00:29:38,480 --> 00:29:41,520
second is an intermediary that receives

00:29:40,640 --> 00:29:44,320
an event

00:29:41,520 --> 00:29:45,760
and routes it to the next receiver this

00:29:44,320 --> 00:29:46,720
next receiver can be another

00:29:45,760 --> 00:29:49,200
intermediary

00:29:46,720 --> 00:29:50,159
or it can be a consumer and that is what

00:29:49,200 --> 00:29:53,679
the third part is

00:29:50,159 --> 00:29:55,279
a consumer a consumer receives an event

00:29:53,679 --> 00:29:57,600
and acts on it

00:29:55,279 --> 00:29:59,120
it uses the context and the data to

00:29:57,600 --> 00:30:00,720
execute some logic

00:29:59,120 --> 00:30:03,120
which might lead to occurrence of new

00:30:00,720 --> 00:30:06,159
waves so now you can know

00:30:03,120 --> 00:30:10,240
that a service can act as a consumer

00:30:06,159 --> 00:30:11,440
as well as a producer at the same time

00:30:10,240 --> 00:30:13,600
so what are the benefits of this

00:30:11,440 --> 00:30:16,559
architecture

00:30:13,600 --> 00:30:17,279
first the services are not fully

00:30:16,559 --> 00:30:19,279
decoupled

00:30:17,279 --> 00:30:21,360
no point-to-point communication

00:30:19,279 --> 00:30:22,640
producers have no idea about who will

00:30:21,360 --> 00:30:24,559
consume the event

00:30:22,640 --> 00:30:26,080
consumers just tell the intimidator that

00:30:24,559 --> 00:30:26,720
they are interested in a specific kind

00:30:26,080 --> 00:30:30,080
of event

00:30:26,720 --> 00:30:33,120
and the events just get delivered this

00:30:30,080 --> 00:30:35,440
is highly scalable

00:30:33,120 --> 00:30:36,799
now you can extend it organically

00:30:35,440 --> 00:30:38,320
because there is no direct

00:30:36,799 --> 00:30:40,000
point-to-point communication between

00:30:38,320 --> 00:30:42,000
producers and consumers

00:30:40,000 --> 00:30:44,559
one can add producers and consumers

00:30:42,000 --> 00:30:46,559
independently

00:30:44,559 --> 00:30:50,320
now where does kinetic eventing fit into

00:30:46,559 --> 00:30:53,679
this architectural style

00:30:50,320 --> 00:30:56,080
kennedy inventing is an intermediate

00:30:53,679 --> 00:30:58,480
it is a set of composable primitives to

00:30:56,080 --> 00:31:00,480
enable late binding of producers and

00:30:58,480 --> 00:31:03,279
consumers

00:31:00,480 --> 00:31:04,880
but for an intermediary to work for

00:31:03,279 --> 00:31:07,039
every kind of application

00:31:04,880 --> 00:31:08,640
we need to standardize on something and

00:31:07,039 --> 00:31:10,799
that's where we are standardized on the

00:31:08,640 --> 00:31:13,760
event envelope

00:31:10,799 --> 00:31:15,039
k native uses cloud event as the event

00:31:13,760 --> 00:31:16,799
envelope

00:31:15,039 --> 00:31:18,880
cloud events is a vendor neutral

00:31:16,799 --> 00:31:19,679
specification for defining the format of

00:31:18,880 --> 00:31:23,039
the unit

00:31:19,679 --> 00:31:25,600
it is a cncf project

00:31:23,039 --> 00:31:27,120
in k native we call the producers as

00:31:25,600 --> 00:31:30,240
event sources

00:31:27,120 --> 00:31:31,840
these produce specifically cloud events

00:31:30,240 --> 00:31:34,000
this could be your own service producing

00:31:31,840 --> 00:31:36,080
a cloud event or it could be a special

00:31:34,000 --> 00:31:37,039
source such as a github source or a

00:31:36,080 --> 00:31:39,360
kafka source

00:31:37,039 --> 00:31:41,279
that can convert github and kafka events

00:31:39,360 --> 00:31:42,960
into cloud events and feed them into a

00:31:41,279 --> 00:31:44,880
k-native-based application

00:31:42,960 --> 00:31:47,840
we'll go over some of these details in

00:31:44,880 --> 00:31:51,039
later steps

00:31:47,840 --> 00:31:51,760
a consumer could be any service that can

00:31:51,039 --> 00:31:53,600
receive an

00:31:51,760 --> 00:31:56,480
event and we'll also go with the details

00:31:53,600 --> 00:32:00,159
of what what defines the consumer later

00:31:56,480 --> 00:32:03,679
in slides

00:32:00,159 --> 00:32:06,240
a broker represents an event mesh

00:32:03,679 --> 00:32:06,880
events are sent to the broker's interest

00:32:06,240 --> 00:32:08,480
and then

00:32:06,880 --> 00:32:10,480
sent to any subscribers that are

00:32:08,480 --> 00:32:13,840
interested in that data

00:32:10,480 --> 00:32:17,039
a trigger is how services subscribe to

00:32:13,840 --> 00:32:19,840
the events from a specific broker

00:32:17,039 --> 00:32:20,399
along with intermediary we also have

00:32:19,840 --> 00:32:23,440
events

00:32:20,399 --> 00:32:24,559
and sources registered now why do we

00:32:23,440 --> 00:32:26,799
need a registry

00:32:24,559 --> 00:32:28,720
think about examples like client tools

00:32:26,799 --> 00:32:29,679
trying to list all the sources available

00:32:28,720 --> 00:32:31,039
on the cluster

00:32:29,679 --> 00:32:32,559
or if you want to find out what are the

00:32:31,039 --> 00:32:33,360
kind of events available on the cluster

00:32:32,559 --> 00:32:35,279
this is where

00:32:33,360 --> 00:32:38,320
the sources and events registry helps

00:32:35,279 --> 00:32:40,320
build the user experience around

00:32:38,320 --> 00:32:41,760
broker and trigger are not the only

00:32:40,320 --> 00:32:43,679
primitives that we have

00:32:41,760 --> 00:32:45,360
we have more such as channels and

00:32:43,679 --> 00:32:47,039
sequence and parallels

00:32:45,360 --> 00:32:48,960
which you can use to stitch together a

00:32:47,039 --> 00:32:51,200
complex application

00:32:48,960 --> 00:32:53,120
we'll go over all these in later later

00:32:51,200 --> 00:32:55,279
on

00:32:53,120 --> 00:32:57,039
okay so now enough of talking let me

00:32:55,279 --> 00:32:59,200
hand it over to cheng who will show you

00:32:57,039 --> 00:33:02,559
key native in action

00:32:59,200 --> 00:33:05,039
go forward thanks akash hi

00:33:02,559 --> 00:33:06,880
i'm chen i'm going to demo a sentiment

00:33:05,039 --> 00:33:09,679
analysis application

00:33:06,880 --> 00:33:11,760
in this demo i will upload such an image

00:33:09,679 --> 00:33:13,840
to a google storage bucket

00:33:11,760 --> 00:33:14,960
the application will detect all the

00:33:13,840 --> 00:33:17,679
faces in the image

00:33:14,960 --> 00:33:19,200
like here extract them out and determine

00:33:17,679 --> 00:33:21,200
the emotion for them

00:33:19,200 --> 00:33:25,120
we will see the results in applications

00:33:21,200 --> 00:33:28,240
web page soon after i upload the image

00:33:25,120 --> 00:33:30,559
here i'm on my cloud storage bucket page

00:33:28,240 --> 00:33:33,200
at the moment i have no image uploaded

00:33:30,559 --> 00:33:33,200
in my bucket

00:33:33,840 --> 00:33:38,159
here is applications webpage it's where

00:33:36,399 --> 00:33:39,360
we will see the final results from the

00:33:38,159 --> 00:33:42,880
processing

00:33:39,360 --> 00:33:46,240
of course it's empty at the moment

00:33:42,880 --> 00:33:46,240
now let me upload an image

00:33:47,679 --> 00:33:53,840
the exact same image we have seen in the

00:33:50,080 --> 00:33:53,840
slide before

00:33:55,120 --> 00:34:00,240
okay done we should see the result here

00:33:59,120 --> 00:34:01,840
in a moment

00:34:00,240 --> 00:34:04,000
the application will extract all the

00:34:01,840 --> 00:34:07,840
faces out from the original image and

00:34:04,000 --> 00:34:07,840
score them individually

00:34:13,359 --> 00:34:19,599
here you can see the result each face

00:34:16,000 --> 00:34:19,599
with their corresponding score

00:34:19,679 --> 00:34:25,119
let's look behind the scene the

00:34:21,679 --> 00:34:27,200
application consists of these components

00:34:25,119 --> 00:34:28,720
we use the cloud storage bucket to store

00:34:27,200 --> 00:34:30,800
the images uploaded

00:34:28,720 --> 00:34:32,159
we use a firestore database to store the

00:34:30,800 --> 00:34:34,320
final results

00:34:32,159 --> 00:34:35,280
we have five micro services deployed as

00:34:34,320 --> 00:34:37,839
canadian service

00:34:35,280 --> 00:34:40,639
on a gke cluster each of them has a

00:34:37,839 --> 00:34:42,399
different responsibility

00:34:40,639 --> 00:34:45,040
the first service is responsible for

00:34:42,399 --> 00:34:46,720
detecting faces in the original image

00:34:45,040 --> 00:34:50,240
essentially it will generate the

00:34:46,720 --> 00:34:52,240
coordinates of the face bounding boxes

00:34:50,240 --> 00:34:53,359
given these coordinates the second

00:34:52,240 --> 00:34:55,760
service will

00:34:53,359 --> 00:34:57,280
extract the face boxes out as separate

00:34:55,760 --> 00:34:59,599
images

00:34:57,280 --> 00:35:01,839
the third service will run algorithm and

00:34:59,599 --> 00:35:03,440
generate a motion score for a given face

00:35:01,839 --> 00:35:05,359
box

00:35:03,440 --> 00:35:07,200
and the forest service is responsible

00:35:05,359 --> 00:35:08,720
for writing the score to the firestore

00:35:07,200 --> 00:35:10,880
database

00:35:08,720 --> 00:35:14,480
and the fifth service is for the ui

00:35:10,880 --> 00:35:14,480
which reads the results from the

00:35:14,839 --> 00:35:18,800
database

00:35:16,640 --> 00:35:20,960
now let's look up how these components

00:35:18,800 --> 00:35:23,040
are connected together

00:35:20,960 --> 00:35:24,160
when i uploaded the image to the storage

00:35:23,040 --> 00:35:26,160
bucket

00:35:24,160 --> 00:35:27,920
we have a cloud storage source that

00:35:26,160 --> 00:35:29,440
imports the cloud storage events to the

00:35:27,920 --> 00:35:31,599
broker

00:35:29,440 --> 00:35:34,079
the first trigger subscribe to such

00:35:31,599 --> 00:35:35,920
events and trigger the first service

00:35:34,079 --> 00:35:37,119
which use the metadata in the event to

00:35:35,920 --> 00:35:41,280
retrieve the image

00:35:37,119 --> 00:35:41,280
and detect the faces bounding boxes

00:35:41,760 --> 00:35:45,359
for each face in the original image it

00:35:44,000 --> 00:35:46,079
produced the new event back to the

00:35:45,359 --> 00:35:49,119
broker

00:35:46,079 --> 00:35:50,000
with the bounding box coordinates the

00:35:49,119 --> 00:35:51,920
second trigger

00:35:50,000 --> 00:35:53,200
subscribed to such events and invoked

00:35:51,920 --> 00:35:55,040
the second service

00:35:53,200 --> 00:35:56,240
which used the coordinates to extract

00:35:55,040 --> 00:36:00,000
the face boxes

00:35:56,240 --> 00:36:02,240
as separate images for each face image

00:36:00,000 --> 00:36:03,839
it produced a new event with a reference

00:36:02,240 --> 00:36:05,760
to the image

00:36:03,839 --> 00:36:08,240
the third trigger subscribe to search

00:36:05,760 --> 00:36:12,079
events and invoke the third service

00:36:08,240 --> 00:36:14,079
which output emotion scoring events

00:36:12,079 --> 00:36:16,000
the first trigger subscribe to such

00:36:14,079 --> 00:36:18,960
events and invoke the for service that

00:36:16,000 --> 00:36:21,520
wrote the results to the database

00:36:18,960 --> 00:36:23,119
eventually the ui loaded results from

00:36:21,520 --> 00:36:26,480
the database

00:36:23,119 --> 00:36:28,240
as you can see as a developer i only

00:36:26,480 --> 00:36:30,480
need to focus on implementing the

00:36:28,240 --> 00:36:31,839
services and defining the triggers that

00:36:30,480 --> 00:36:34,000
invoke them

00:36:31,839 --> 00:36:36,800
the broker will take care of the rest of

00:36:34,000 --> 00:36:36,800
the communication

00:36:37,839 --> 00:36:41,359
now you have seen how these components

00:36:39,839 --> 00:36:43,200
are connected together

00:36:41,359 --> 00:36:46,480
let me give you a quick peek at what's

00:36:43,200 --> 00:36:48,800
really happening in the cluster

00:36:46,480 --> 00:36:50,000
here in my terminal i'm taking the logs

00:36:48,800 --> 00:36:52,560
from all the services

00:36:50,000 --> 00:36:53,280
except the one for the ui they are in

00:36:52,560 --> 00:36:55,119
order

00:36:53,280 --> 00:36:56,400
this is the first service that detects

00:36:55,119 --> 00:36:58,400
all the faces

00:36:56,400 --> 00:37:00,000
this is the second service that extract

00:36:58,400 --> 00:37:01,599
all the face boxes

00:37:00,000 --> 00:37:03,280
this is the third one that determines

00:37:01,599 --> 00:37:07,040
the score and this is the fourth one

00:37:03,280 --> 00:37:07,040
that writes the result to the database

00:37:07,440 --> 00:37:12,079
here let me upload another image and you

00:37:10,240 --> 00:37:23,040
will see how they are triggered one

00:37:12,079 --> 00:37:26,079
after another

00:37:23,040 --> 00:37:29,040
the first service got triggered the

00:37:26,079 --> 00:37:31,920
second one got triggered

00:37:29,040 --> 00:37:31,920
now the third one

00:37:35,200 --> 00:37:38,160
and now the fourth one

00:37:38,320 --> 00:37:44,720
here is a clear view of the logs

00:37:41,599 --> 00:37:49,599
this is the first service as we can see

00:37:44,720 --> 00:37:53,040
you receive a cloud storage event

00:37:49,599 --> 00:37:54,640
with the reference to the to the image i

00:37:53,040 --> 00:37:57,280
just uploaded

00:37:54,640 --> 00:37:58,720
and it produced the new events where it

00:37:57,280 --> 00:38:01,839
has the coordinates

00:37:58,720 --> 00:38:01,839
of the face boxes

00:38:02,400 --> 00:38:05,920
in a second service they received those

00:38:04,800 --> 00:38:07,839
events

00:38:05,920 --> 00:38:10,480
and they produced the new events with

00:38:07,839 --> 00:38:14,480
extracted face boxes

00:38:10,480 --> 00:38:14,480
with a reference such as here

00:38:15,599 --> 00:38:18,960
in the third service it receive those

00:38:17,760 --> 00:38:21,359
previous events

00:38:18,960 --> 00:38:22,320
and start to to determine the sentiment

00:38:21,359 --> 00:38:25,280
scores

00:38:22,320 --> 00:38:25,920
and here that's a that's one sentiment

00:38:25,280 --> 00:38:30,240
score for

00:38:25,920 --> 00:38:32,240
facebook and it produced such new events

00:38:30,240 --> 00:38:33,520
in india in the forest service you

00:38:32,240 --> 00:38:37,280
receive the such events

00:38:33,520 --> 00:38:37,280
and write the data to the database

00:38:37,920 --> 00:38:42,640
as mentioned earlier canadian event team

00:38:40,480 --> 00:38:45,119
provides composable primitives

00:38:42,640 --> 00:38:46,400
to enable late binding event producers

00:38:45,119 --> 00:38:49,119
and consumers

00:38:46,400 --> 00:38:51,760
so now let's look at those concepts in

00:38:49,119 --> 00:38:51,760
more details

00:38:52,720 --> 00:38:56,800
a canadian eventing source is a

00:38:54,720 --> 00:38:59,599
component that generates events or

00:38:56,800 --> 00:39:02,320
imports events from external producers

00:38:59,599 --> 00:39:04,480
the main responsibility of a source is

00:39:02,320 --> 00:39:05,359
to produce events in a cloud advance

00:39:04,480 --> 00:39:07,280
format

00:39:05,359 --> 00:39:10,079
which is the standard format we

00:39:07,280 --> 00:39:12,720
uniformly adopt in the project

00:39:10,079 --> 00:39:14,480
for example github can be configured to

00:39:12,720 --> 00:39:17,280
produce a lot of events

00:39:14,480 --> 00:39:18,079
but there are not in cloud events format

00:39:17,280 --> 00:39:19,680
and we have

00:39:18,079 --> 00:39:21,760
github source that converts them to

00:39:19,680 --> 00:39:24,320
cloud events format

00:39:21,760 --> 00:39:26,160
there are a lot of existing sources some

00:39:24,320 --> 00:39:26,720
are community owned like kafka cloud

00:39:26,160 --> 00:39:29,200
source

00:39:26,720 --> 00:39:30,240
github source as i just mentioned some

00:39:29,200 --> 00:39:32,240
are vendor-owned

00:39:30,240 --> 00:39:33,760
such as cloud storage source as you have

00:39:32,240 --> 00:39:36,160
seen in the demo

00:39:33,760 --> 00:39:38,079
trigger mesh also supports aws sql

00:39:36,160 --> 00:39:39,760
source exactly

00:39:38,079 --> 00:39:41,760
you can even write your own custom

00:39:39,760 --> 00:39:43,839
sources for example

00:39:41,760 --> 00:39:44,960
let's say you have a legacy system that

00:39:43,839 --> 00:39:47,359
generates events

00:39:44,960 --> 00:39:49,440
in some legacy formats you can write a

00:39:47,359 --> 00:39:52,000
custom source that converts those events

00:39:49,440 --> 00:39:52,480
into cloud events format and then with

00:39:52,000 --> 00:39:54,880
trigger

00:39:52,480 --> 00:39:57,520
and broker you can integrate with the

00:39:54,880 --> 00:39:59,359
rest of your system

00:39:57,520 --> 00:40:01,280
there will be another session that dives

00:39:59,359 --> 00:40:02,960
deep into cognitive sources

00:40:01,280 --> 00:40:05,520
feel free to take a look if you are

00:40:02,960 --> 00:40:05,520
interested

00:40:07,440 --> 00:40:12,000
broker and trigger are the main advanced

00:40:09,520 --> 00:40:14,480
intermediary in cognitive eventing

00:40:12,000 --> 00:40:16,000
you can treat a broker as a black box

00:40:14,480 --> 00:40:18,160
where you throw events into

00:40:16,000 --> 00:40:19,119
and use trigger to subscribe events from

00:40:18,160 --> 00:40:21,599
it

00:40:19,119 --> 00:40:23,599
in a trigger you can specify filters

00:40:21,599 --> 00:40:25,520
using cloud events attributes

00:40:23,599 --> 00:40:27,119
so that you only subscribe to events you

00:40:25,520 --> 00:40:29,280
are interested in

00:40:27,119 --> 00:40:30,560
the event sync in a trigger could be any

00:40:29,280 --> 00:40:32,560
addressable

00:40:30,560 --> 00:40:34,480
the addressable here means any resource

00:40:32,560 --> 00:40:35,359
reference that could be resolved to your

00:40:34,480 --> 00:40:39,200
url

00:40:35,359 --> 00:40:42,000
or just a plain url canadian eventing

00:40:39,200 --> 00:40:43,280
provides a default broker implementation

00:40:42,000 --> 00:40:45,359
there are alternative broker

00:40:43,280 --> 00:40:47,040
implementations such as gcp pub sub

00:40:45,359 --> 00:40:50,079
vector broker

00:40:47,040 --> 00:40:51,839
kafka based broker executor there will

00:40:50,079 --> 00:40:53,040
be another session that dives deep into

00:40:51,839 --> 00:40:56,720
broken trigger

00:40:53,040 --> 00:40:56,720
again feel free to take a look

00:40:58,640 --> 00:41:02,960
here are some lower level primitives in

00:41:01,040 --> 00:41:04,720
canadian eventing

00:41:02,960 --> 00:41:06,640
channel and subscription are the

00:41:04,720 --> 00:41:09,520
messaging primitives

00:41:06,640 --> 00:41:10,480
a channel is an abstraction of a message

00:41:09,520 --> 00:41:12,560
transport

00:41:10,480 --> 00:41:14,560
which takes care of things like message

00:41:12,560 --> 00:41:16,640
persistence

00:41:14,560 --> 00:41:18,960
and a subscription allows subscribing

00:41:16,640 --> 00:41:21,760
messages from a specific channel

00:41:18,960 --> 00:41:23,920
which takes care of message delivery

00:41:21,760 --> 00:41:27,119
there are many implementations for them

00:41:23,920 --> 00:41:29,839
such as kafka asqs nats

00:41:27,119 --> 00:41:31,920
pub sub the default broker

00:41:29,839 --> 00:41:35,839
implementation in canadian eventing uses

00:41:31,920 --> 00:41:35,839
the messaging primitives

00:41:36,480 --> 00:41:40,800
there is another category of primitives

00:41:38,560 --> 00:41:43,280
called flows

00:41:40,800 --> 00:41:46,240
there are sequence and parallel they

00:41:43,280 --> 00:41:49,200
both allow invoking a list of services

00:41:46,240 --> 00:41:50,960
with the difference intuitively sequence

00:41:49,200 --> 00:41:54,079
will invoke them sequentially while

00:41:50,960 --> 00:41:56,480
parallel will invoke them in parallel

00:41:54,079 --> 00:41:58,800
they could come in handy for certain use

00:41:56,480 --> 00:42:00,960
cases

00:41:58,800 --> 00:42:02,640
for example sequence makes it really

00:42:00,960 --> 00:42:05,280
easy to build an event

00:42:02,640 --> 00:42:06,960
augmenting flow where at the beginning

00:42:05,280 --> 00:42:07,839
of the flow you have a bare minimum

00:42:06,960 --> 00:42:10,319
event

00:42:07,839 --> 00:42:12,480
and in each service in the flow you add

00:42:10,319 --> 00:42:14,960
additional information to the event

00:42:12,480 --> 00:42:17,359
and in the end you have an event with

00:42:14,960 --> 00:42:19,760
much richer information

00:42:17,359 --> 00:42:22,160
the benefit of using a flow is that you

00:42:19,760 --> 00:42:24,319
can treat it as a unit of work

00:42:22,160 --> 00:42:26,240
in which any intermediate events won't

00:42:24,319 --> 00:42:28,319
be visible from outside

00:42:26,240 --> 00:42:30,560
this prevents unexpected events flow

00:42:28,319 --> 00:42:32,640
into your system and causing unexpected

00:42:30,560 --> 00:42:35,920
behavior

00:42:32,640 --> 00:42:38,400
a service in a flow is essentially an

00:42:35,920 --> 00:42:41,280
addressable that could consume events

00:42:38,400 --> 00:42:42,640
and potentially produce events also by

00:42:41,280 --> 00:42:45,760
this definition

00:42:42,640 --> 00:42:48,480
a in a flow could be another flow or

00:42:45,760 --> 00:42:50,160
something like a broker you can actually

00:42:48,480 --> 00:42:53,280
build more complex flows

00:42:50,160 --> 00:42:54,319
when you combine them for example in a

00:42:53,280 --> 00:42:57,520
parallel

00:42:54,319 --> 00:43:01,520
you can invoke a sequence

00:42:57,520 --> 00:43:01,520
a broker and another pearl

00:43:01,599 --> 00:43:06,240
thank you everyone let me hand it back

00:43:03,599 --> 00:43:06,240
to akash

00:43:06,800 --> 00:43:10,720
thank you jen so as i promised in the

00:43:09,520 --> 00:43:13,359
beginning of this

00:43:10,720 --> 00:43:15,680
presentation that i'll introduce you to

00:43:13,359 --> 00:43:17,920
our vibrant community of developers

00:43:15,680 --> 00:43:19,040
k native is an open source project and

00:43:17,920 --> 00:43:20,400
apache license

00:43:19,040 --> 00:43:22,880
and has a vibrant community of

00:43:20,400 --> 00:43:24,839
developers around the globe

00:43:22,880 --> 00:43:27,839
here are a few highlights about our

00:43:24,839 --> 00:43:31,200
community we have two core components

00:43:27,839 --> 00:43:33,200
serving and eventing today what we

00:43:31,200 --> 00:43:35,200
introduce you to was eventing

00:43:33,200 --> 00:43:37,680
serving is another core piece that lets

00:43:35,200 --> 00:43:39,760
you build a micro service in

00:43:37,680 --> 00:43:42,560
in the most simplistic manner and posted

00:43:39,760 --> 00:43:44,560
in any kubernetes based environment

00:43:42,560 --> 00:43:47,280
we have 10 active working groups with

00:43:44,560 --> 00:43:50,319
around 450 contributors

00:43:47,280 --> 00:43:52,560
we have more than 15 active repositories

00:43:50,319 --> 00:43:55,200
on top of all this development we have

00:43:52,560 --> 00:43:58,960
seven different knee two-based documents

00:43:55,200 --> 00:44:02,640
from vendors such as google ibm red hat

00:43:58,960 --> 00:44:02,640
trigger mesh and vmware

00:44:03,440 --> 00:44:07,680
k-native is maintained by the community

00:44:05,920 --> 00:44:08,560
which includes organizations such as

00:44:07,680 --> 00:44:12,000
google

00:44:08,560 --> 00:44:15,520
vmware sap red hat ibm

00:44:12,000 --> 00:44:17,200
and a great ecosystem of startups

00:44:15,520 --> 00:44:19,599
so how do you get involved in all of

00:44:17,200 --> 00:44:21,280
this you can

00:44:19,599 --> 00:44:23,760
if you just want to kick the tires you

00:44:21,280 --> 00:44:25,680
can start by our docks

00:44:23,760 --> 00:44:26,960
and we have some easy quick start

00:44:25,680 --> 00:44:28,480
examples

00:44:26,960 --> 00:44:31,599
if you have questions or you're stuck

00:44:28,480 --> 00:44:33,359
with anything reach out to us on slack

00:44:31,599 --> 00:44:34,720
if you have any follow-up questions then

00:44:33,359 --> 00:44:36,240
please reach out to us on our

00:44:34,720 --> 00:44:38,400
communities life channel

00:44:36,240 --> 00:44:40,079
or feel free to reach out directly to me

00:44:38,400 --> 00:44:42,800
or chen on twitter

00:44:40,079 --> 00:44:44,800
or linkedin my twitter handle is

00:44:42,800 --> 00:44:47,599
underscore akash v

00:44:44,800 --> 00:44:48,800
and linkedin is akash rv and chen's

00:44:47,599 --> 00:44:53,520
twitter handle is chan

00:44:48,800 --> 00:45:05,839
with eight o's and the linkedin is cshou

00:44:53,520 --> 00:45:05,839
thank you for joining us

00:45:10,000 --> 00:45:15,599
thanks akash and chen i love that demo

00:45:13,200 --> 00:45:16,880
very powerful application made much

00:45:15,599 --> 00:45:18,000
easier through a threat driven

00:45:16,880 --> 00:45:20,960
architecture using

00:45:18,000 --> 00:45:21,839
k-native this was a great summary of the

00:45:20,960 --> 00:45:24,319
problems

00:45:21,839 --> 00:45:26,560
that this solution solves for developers

00:45:24,319 --> 00:45:28,160
and why this is a great pattern to allow

00:45:26,560 --> 00:45:28,560
that separation of concerns that is so

00:45:28,160 --> 00:45:32,480
pro

00:45:28,560 --> 00:45:34,800
so important in modern microservices

00:45:32,480 --> 00:45:36,480
next we'll hear from adam and nacho

00:45:34,800 --> 00:45:47,839
about k-native sources

00:45:36,480 --> 00:45:47,839
take it away guys

00:46:22,160 --> 00:46:27,280
hi i'm adam hi i'm nacho

00:46:25,520 --> 00:46:29,520
we're both software engineers working at

00:46:27,280 --> 00:46:30,640
google on k-native specifically k-native

00:46:29,520 --> 00:46:32,800
eventing

00:46:30,640 --> 00:46:35,119
this session is about k-native sources

00:46:32,800 --> 00:46:36,400
which is a core construct in k-native

00:46:35,119 --> 00:46:37,760
eventing

00:46:36,400 --> 00:46:39,200
the previous session is more of an

00:46:37,760 --> 00:46:40,880
overview of event-driven architectures

00:46:39,200 --> 00:46:42,160
and k-native eventing in general

00:46:40,880 --> 00:46:44,079
and the following session is a deep

00:46:42,160 --> 00:46:46,720
technical dive on the broker component

00:46:44,079 --> 00:46:48,000
within canada preventing

00:46:46,720 --> 00:46:49,760
in case you haven't attended the

00:46:48,000 --> 00:46:52,480
previous session let's start with some

00:46:49,760 --> 00:46:54,880
brief context of k native

00:46:52,480 --> 00:46:56,720
k native is a kubernetes based platform

00:46:54,880 --> 00:46:57,760
to deploy and manage serverless

00:46:56,720 --> 00:46:59,920
workloads

00:46:57,760 --> 00:47:01,680
it's built on top of kubernetes with the

00:46:59,920 --> 00:47:03,520
idea of abstracting away

00:47:01,680 --> 00:47:05,760
complex kubernetes details from

00:47:03,520 --> 00:47:08,000
developers to enable them

00:47:05,760 --> 00:47:09,760
focus on what matters most their

00:47:08,000 --> 00:47:12,079
business logic

00:47:09,760 --> 00:47:13,760
it has two core components one is k

00:47:12,079 --> 00:47:15,119
native serving and the other one is k

00:47:13,760 --> 00:47:17,440
native event

00:47:15,119 --> 00:47:18,800
k native serving allows for rapid

00:47:17,440 --> 00:47:22,559
deployment of stateless

00:47:18,800 --> 00:47:25,359
http containers it supports auto scaling

00:47:22,559 --> 00:47:27,920
of those containers from 0 to n

00:47:25,359 --> 00:47:29,119
and it also deals with routing and

00:47:27,920 --> 00:47:32,319
managing

00:47:29,119 --> 00:47:34,400
traffic splits during deployment

00:47:32,319 --> 00:47:36,640
for example you can deploy a new version

00:47:34,400 --> 00:47:38,640
of your service and you only want 10

00:47:36,640 --> 00:47:40,720
of the traffic to go there then twenty

00:47:38,640 --> 00:47:42,400
percent and thirty and so on you can use

00:47:40,720 --> 00:47:45,119
canadian survey

00:47:42,400 --> 00:47:47,599
k native eventing in turn takes care of

00:47:45,119 --> 00:47:48,559
the subscription delivery and management

00:47:47,599 --> 00:47:50,880
of events

00:47:48,559 --> 00:47:53,359
and it enables event based triggering of

00:47:50,880 --> 00:47:54,720
your workloads

00:47:53,359 --> 00:47:56,319
both candidate server and cognitive

00:47:54,720 --> 00:47:58,400
eventing are great on their own but they

00:47:56,319 --> 00:47:59,920
also complement each other well

00:47:58,400 --> 00:48:01,839
using them together means that you can

00:47:59,920 --> 00:48:04,240
easily have canadian serving respond to

00:48:01,839 --> 00:48:05,920
things other than direct http requests

00:48:04,240 --> 00:48:07,359
and allows eventing to automatically

00:48:05,920 --> 00:48:09,119
scale up to the peaks of demand

00:48:07,359 --> 00:48:11,680
and scaling down to zero and no demand

00:48:09,119 --> 00:48:11,680
is present

00:48:11,839 --> 00:48:15,280
canada eventing is made up of composable

00:48:13,760 --> 00:48:16,400
primitives that allow late binding

00:48:15,280 --> 00:48:18,800
between event sources

00:48:16,400 --> 00:48:20,720
and event consumers whenever we speak of

00:48:18,800 --> 00:48:22,240
about an event inside kingdom eventing

00:48:20,720 --> 00:48:23,839
they always follow the cloud events

00:48:22,240 --> 00:48:25,599
standard

00:48:23,839 --> 00:48:27,599
event sources produce cloud events while

00:48:25,599 --> 00:48:29,760
event consumers consume them

00:48:27,599 --> 00:48:31,680
broker and trigger are intermediaries

00:48:29,760 --> 00:48:32,079
that allow interested event consumers to

00:48:31,680 --> 00:48:33,760
receive

00:48:32,079 --> 00:48:36,000
cloud events without the source having

00:48:33,760 --> 00:48:37,839
to know about each individual consumer

00:48:36,000 --> 00:48:39,119
as mentioned in the previous slide

00:48:37,839 --> 00:48:41,680
connective eventing

00:48:39,119 --> 00:48:43,119
speaks cloud events and these give us

00:48:41,680 --> 00:48:45,440
some advantages

00:48:43,119 --> 00:48:47,359
one is consistency because the lack of a

00:48:45,440 --> 00:48:49,359
common way of describing events

00:48:47,359 --> 00:48:51,440
would mean that developers will have to

00:48:49,359 --> 00:48:53,440
write their own event handling logic for

00:48:51,440 --> 00:48:55,520
every new source

00:48:53,440 --> 00:48:56,880
the other one is accessibility if you

00:48:55,520 --> 00:48:57,920
think about it if you don't have a

00:48:56,880 --> 00:49:00,559
common

00:48:57,920 --> 00:49:02,160
way of express of expressing events it

00:49:00,559 --> 00:49:03,599
means that there are no common libraries

00:49:02,160 --> 00:49:05,839
no common tooling and

00:49:03,599 --> 00:49:06,720
no common infrastructure for delivering

00:49:05,839 --> 00:49:09,359
event data

00:49:06,720 --> 00:49:10,800
across different environments it's worth

00:49:09,359 --> 00:49:12,720
noting that cloud events

00:49:10,800 --> 00:49:14,240
provide several sdks in different

00:49:12,720 --> 00:49:17,359
languages such as

00:49:14,240 --> 00:49:19,280
python go javascript etc

00:49:17,359 --> 00:49:21,359
and the other advantage is portability

00:49:19,280 --> 00:49:23,520
because your containers can be moved to

00:49:21,359 --> 00:49:25,280
other environments that also are cloud

00:49:23,520 --> 00:49:28,319
events friendly

00:49:25,280 --> 00:49:29,599
cloud events have contact attributes and

00:49:28,319 --> 00:49:33,200
data you can think of

00:49:29,599 --> 00:49:35,119
contact attributes as metadata and

00:49:33,200 --> 00:49:37,040
the contact attributes are designed in

00:49:35,119 --> 00:49:38,319
such a way that they can be serialized

00:49:37,040 --> 00:49:40,880
independently of

00:49:38,319 --> 00:49:42,640
of the event data this allows them to

00:49:40,880 --> 00:49:45,440
inspect it without having to

00:49:42,640 --> 00:49:46,800
deserialize the data for example to make

00:49:45,440 --> 00:49:49,440
routing decisions

00:49:46,800 --> 00:49:50,960
here we see an example where we see some

00:49:49,440 --> 00:49:53,920
context attributes such as

00:49:50,960 --> 00:49:54,640
type or the source where the event came

00:49:53,920 --> 00:49:57,760
from

00:49:54,640 --> 00:49:58,079
the spec version and so on there are

00:49:57,760 --> 00:50:00,400
also

00:49:58,079 --> 00:50:02,000
multiple protocol bindings that define

00:50:00,400 --> 00:50:04,319
how to map an event

00:50:02,000 --> 00:50:05,040
to a specific protocol message for

00:50:04,319 --> 00:50:07,200
example

00:50:05,040 --> 00:50:08,880
the http protocol binding for cloud

00:50:07,200 --> 00:50:11,040
events defines

00:50:08,880 --> 00:50:14,160
how client cloud events are mapped from

00:50:11,040 --> 00:50:16,480
http requests and response messages

00:50:14,160 --> 00:50:17,839
or the kafka protocol binding which

00:50:16,480 --> 00:50:20,160
defines how you can map

00:50:17,839 --> 00:50:23,040
events from kafka messages to cloud

00:50:20,160 --> 00:50:23,040
events and so on

00:50:23,920 --> 00:50:27,920
giving that general background we can

00:50:26,000 --> 00:50:28,800
now focus on the main topic of this

00:50:27,920 --> 00:50:31,599
session which is

00:50:28,800 --> 00:50:33,359
canadian sources here's the proposed

00:50:31,599 --> 00:50:35,200
agenda for the talk

00:50:33,359 --> 00:50:36,880
we will start with what are connective

00:50:35,200 --> 00:50:38,800
sources and why we need them

00:50:36,880 --> 00:50:40,240
as well as some of the design choices we

00:50:38,800 --> 00:50:42,079
made along the way

00:50:40,240 --> 00:50:43,920
we will also go over some of the sources

00:50:42,079 --> 00:50:44,960
that are available out there for you to

00:50:43,920 --> 00:50:47,359
use

00:50:44,960 --> 00:50:49,200
and we'll also go over more details on

00:50:47,359 --> 00:50:51,280
how you can implement your own source

00:50:49,200 --> 00:50:52,480
if none of the available ones fits your

00:50:51,280 --> 00:50:55,599
purpose

00:50:52,480 --> 00:50:57,040
and we'll finally conclude so let's get

00:50:55,599 --> 00:51:00,480
started

00:50:57,040 --> 00:51:00,480
with k native sources

00:51:00,960 --> 00:51:04,319
but let's start with the fundamental

00:51:02,640 --> 00:51:06,720
question of why we need cognitive

00:51:04,319 --> 00:51:09,200
sources and the answer is pretty simple

00:51:06,720 --> 00:51:10,720
canadian eventing adopted the usage of

00:51:09,200 --> 00:51:13,200
cloud events

00:51:10,720 --> 00:51:14,400
however most producers still use their

00:51:13,200 --> 00:51:17,200
own event formats

00:51:14,400 --> 00:51:19,599
so there's an impedance mismatch we need

00:51:17,200 --> 00:51:22,640
k native sources to be able to leverage

00:51:19,599 --> 00:51:24,640
existing non-cloud events producers into

00:51:22,640 --> 00:51:26,960
this cloud event world

00:51:24,640 --> 00:51:28,480
and if we want more breadth of producers

00:51:26,960 --> 00:51:31,520
then we need something that

00:51:28,480 --> 00:51:34,960
will allow us to convert event formats

00:51:31,520 --> 00:51:36,000
into cloud events and breath is very

00:51:34,960 --> 00:51:38,720
important here because

00:51:36,000 --> 00:51:40,720
at the end of the day eventing systems

00:51:38,720 --> 00:51:44,400
regardless of their quality are limited

00:51:40,720 --> 00:51:44,400
by the events that are available to them

00:51:44,720 --> 00:51:49,119
what are k native sources kennedy

00:51:46,960 --> 00:51:51,280
sources are constructs that produce or

00:51:49,119 --> 00:51:52,319
import events into the cluster as cloud

00:51:51,280 --> 00:51:55,040
events

00:51:52,319 --> 00:51:56,800
they convert incoming proprietary events

00:51:55,040 --> 00:51:57,920
into cloud events and sends them

00:51:56,800 --> 00:52:00,720
downstream

00:51:57,920 --> 00:52:02,720
and downstream can be any http

00:52:00,720 --> 00:52:05,119
addressable we also call them

00:52:02,720 --> 00:52:07,520
syncs for example it can be an end

00:52:05,119 --> 00:52:09,520
consumer such as a canadian service

00:52:07,520 --> 00:52:12,319
or a middleware like for example the

00:52:09,520 --> 00:52:14,400
kennedy broker

00:52:12,319 --> 00:52:16,880
we talked a bit about the why the what

00:52:14,400 --> 00:52:18,720
now let's delve more into the how

00:52:16,880 --> 00:52:21,119
specifically how are canadian sources

00:52:18,720 --> 00:52:23,280
implemented k-nativ is designed to be

00:52:21,119 --> 00:52:25,200
as native to kubernetes as possible and

00:52:23,280 --> 00:52:26,640
certainly k-native

00:52:25,200 --> 00:52:27,920
k-native uses the standard way of

00:52:26,640 --> 00:52:30,079
extending kubernetes the customer

00:52:27,920 --> 00:52:32,480
research definition or crd

00:52:30,079 --> 00:52:34,960
crds define new resource types similar

00:52:32,480 --> 00:52:37,119
to a class in a programming language

00:52:34,960 --> 00:52:39,040
we then can create instances of this crd

00:52:37,119 --> 00:52:40,800
called a custom object or co

00:52:39,040 --> 00:52:42,319
these are similar to instances in a

00:52:40,800 --> 00:52:43,680
programming language

00:52:42,319 --> 00:52:45,280
if you would like more information about

00:52:43,680 --> 00:52:46,000
this the link below has the official

00:52:45,280 --> 00:52:48,880
documentation

00:52:46,000 --> 00:52:49,440
about crds broadly speaking we have two

00:52:48,880 --> 00:52:51,280
types of

00:52:49,440 --> 00:52:52,640
sources push-based sources and

00:52:51,280 --> 00:52:54,079
poll-based sources

00:52:52,640 --> 00:52:56,240
push-based sources are ones where an

00:52:54,079 --> 00:52:57,839
upstream event producer pushes an event

00:52:56,240 --> 00:53:00,480
into our source

00:52:57,839 --> 00:53:03,760
such as github source where github makes

00:53:00,480 --> 00:53:05,520
a http request to our source

00:53:03,760 --> 00:53:07,200
this has the downside that we must

00:53:05,520 --> 00:53:09,280
expose an endpoint for

00:53:07,200 --> 00:53:10,800
our source to hit it doesn't necessarily

00:53:09,280 --> 00:53:11,920
have to be on the public internet but it

00:53:10,800 --> 00:53:13,680
does have to be somewhere that the

00:53:11,920 --> 00:53:15,119
producer can reach

00:53:13,680 --> 00:53:16,880
one of the large upsides is these are

00:53:15,119 --> 00:53:18,480
much easier to scale we can leverage

00:53:16,880 --> 00:53:21,440
things like k-native serving

00:53:18,480 --> 00:53:22,160
which already does this to scale up to

00:53:21,440 --> 00:53:23,599
meet demand

00:53:22,160 --> 00:53:26,400
and scale down to zero when there isn't

00:53:23,599 --> 00:53:28,559
any poll based sources on the other hand

00:53:26,400 --> 00:53:29,440
are pulling events from the upstream

00:53:28,559 --> 00:53:30,480
producer

00:53:29,440 --> 00:53:33,359
they can either be doing so on a

00:53:30,480 --> 00:53:35,040
continuous basis or on a periodic basis

00:53:33,359 --> 00:53:36,400
they need network access to the event

00:53:35,040 --> 00:53:38,480
producer but they don't necessarily have

00:53:36,400 --> 00:53:39,920
to expose an endpoint themselves

00:53:38,480 --> 00:53:42,240
in general these are more difficult to

00:53:39,920 --> 00:53:44,640
scale because we need our own way to

00:53:42,240 --> 00:53:46,319
define how congested it is

00:53:44,640 --> 00:53:47,680
it also means that something is either

00:53:46,319 --> 00:53:48,960
continuously running or at least

00:53:47,680 --> 00:53:51,680
periodically running

00:53:48,960 --> 00:53:51,680
in the background

00:53:52,880 --> 00:53:57,119
while developing k native we tried two

00:53:54,720 --> 00:53:58,319
different crd models for our sources

00:53:57,119 --> 00:54:00,559
the first we tried was called the

00:53:58,319 --> 00:54:02,880
provisioner model we had a single crd

00:54:00,559 --> 00:54:04,480
named source inside that inside the

00:54:02,880 --> 00:54:06,319
specification of that crd

00:54:04,480 --> 00:54:07,839
there was a field called provisioner

00:54:06,319 --> 00:54:09,440
every custom object that was made

00:54:07,839 --> 00:54:10,800
would fill in that field and that field

00:54:09,440 --> 00:54:11,440
would determine what kind of source it

00:54:10,800 --> 00:54:13,760
was

00:54:11,440 --> 00:54:14,720
it might be github or kafka or something

00:54:13,760 --> 00:54:16,640
else

00:54:14,720 --> 00:54:18,240
this made it very easy for our k native

00:54:16,640 --> 00:54:19,680
core code to interact with sources

00:54:18,240 --> 00:54:21,040
there's only one type and we knew it at

00:54:19,680 --> 00:54:22,559
compile time

00:54:21,040 --> 00:54:24,240
this tended to work pretty well as long

00:54:22,559 --> 00:54:26,559
as the parameters for our sources were

00:54:24,240 --> 00:54:27,520
very similar as each as the number of

00:54:26,559 --> 00:54:29,440
sources increased

00:54:27,520 --> 00:54:31,359
the parameters started to diverge we had

00:54:29,440 --> 00:54:32,960
to add a generic parameter map

00:54:31,359 --> 00:54:34,400
this made it very difficult for users to

00:54:32,960 --> 00:54:36,160
understand what

00:54:34,400 --> 00:54:37,760
they needed to configure or what even

00:54:36,160 --> 00:54:38,400
could be configured for a given source

00:54:37,760 --> 00:54:40,240
type

00:54:38,400 --> 00:54:41,599
it was also more more difficult to

00:54:40,240 --> 00:54:43,200
figure out what sources were even

00:54:41,599 --> 00:54:44,640
available to begin with

00:54:43,200 --> 00:54:46,319
the next model we tried was called the

00:54:44,640 --> 00:54:48,079
operator model

00:54:46,319 --> 00:54:49,839
in this model each source has its own

00:54:48,079 --> 00:54:52,240
crd type

00:54:49,839 --> 00:54:55,040
for example kafka source mongodb source

00:54:52,240 --> 00:54:56,480
and github source are all their own crds

00:54:55,040 --> 00:54:57,920
this made it much easier for users to

00:54:56,480 --> 00:54:59,040
understand what was installed in the

00:54:57,920 --> 00:55:00,480
cluster because they could use their

00:54:59,040 --> 00:55:03,839
native kubernetes

00:55:00,480 --> 00:55:04,799
interactions such as listing crds

00:55:03,839 --> 00:55:06,000
it also made it much more

00:55:04,799 --> 00:55:07,119
straightforward for users to understand

00:55:06,000 --> 00:55:09,040
how to

00:55:07,119 --> 00:55:10,559
use these sources because they could

00:55:09,040 --> 00:55:12,480
introspect the crd itself

00:55:10,559 --> 00:55:13,839
which told users exactly what fields and

00:55:12,480 --> 00:55:15,280
parameters were available for

00:55:13,839 --> 00:55:17,119
customization

00:55:15,280 --> 00:55:19,440
the big downside of this was that our

00:55:17,119 --> 00:55:21,119
k-native core code had to deal with

00:55:19,440 --> 00:55:23,839
objects at runtime that didn't know

00:55:21,119 --> 00:55:26,240
about it compile time

00:55:23,839 --> 00:55:27,040
in the end we decided to take that extra

00:55:26,240 --> 00:55:29,280
complexity

00:55:27,040 --> 00:55:30,400
on the k native core code and make life

00:55:29,280 --> 00:55:32,799
better for our users

00:55:30,400 --> 00:55:34,640
we decided to go with the operator model

00:55:32,799 --> 00:55:36,319
although we went with the operator model

00:55:34,640 --> 00:55:37,359
where we have different clds for

00:55:36,319 --> 00:55:40,319
different sources

00:55:37,359 --> 00:55:41,119
we also wanted to have all sources share

00:55:40,319 --> 00:55:43,599
some

00:55:41,119 --> 00:55:45,920
similar shape this would allow us to

00:55:43,599 --> 00:55:47,920
treat them polymorphically in code

00:55:45,920 --> 00:55:49,839
or for instance a cluster operator will

00:55:47,920 --> 00:55:51,280
be able to inspect the resources without

00:55:49,839 --> 00:55:52,319
having to be fully aware of the

00:55:51,280 --> 00:55:53,760
implementation

00:55:52,319 --> 00:55:55,599
and it will also allow us to build

00:55:53,760 --> 00:55:58,720
common libraries that could be

00:55:55,599 --> 00:56:01,520
reused all over for this

00:55:58,720 --> 00:56:02,240
we use duct typing duct typing in

00:56:01,520 --> 00:56:04,000
kubernetes

00:56:02,240 --> 00:56:05,839
world is a technique that allows you to

00:56:04,000 --> 00:56:09,200
define a partial schema of an

00:56:05,839 --> 00:56:09,599
object in the example below we we see

00:56:09,200 --> 00:56:11,760
three

00:56:09,599 --> 00:56:14,160
json objects the one on the left hand

00:56:11,760 --> 00:56:15,680
side has a four and a bar property the

00:56:14,160 --> 00:56:17,440
one in the middle has a bus

00:56:15,680 --> 00:56:19,599
property and the one on the right has a

00:56:17,440 --> 00:56:22,640
spam and a hand property

00:56:19,599 --> 00:56:25,520
but all of them they have this same

00:56:22,640 --> 00:56:26,720
k native property we have eventing and

00:56:25,520 --> 00:56:29,040
serving

00:56:26,720 --> 00:56:30,319
and if we want to reason about that

00:56:29,040 --> 00:56:32,559
canadian property

00:56:30,319 --> 00:56:34,480
in all of these different resources we

00:56:32,559 --> 00:56:38,720
can use that typing we can use that

00:56:34,480 --> 00:56:40,799
typing to extract that partial schema

00:56:38,720 --> 00:56:41,760
and that's exactly what we did with our

00:56:40,799 --> 00:56:43,520
sources

00:56:41,760 --> 00:56:45,839
although we have different cids and each

00:56:43,520 --> 00:56:48,720
crd has specific attributes

00:56:45,839 --> 00:56:50,799
all sources share a partial schema they

00:56:48,720 --> 00:56:53,119
all implement what we call the source

00:56:50,799 --> 00:56:54,160
duct type here i'm showing three

00:56:53,119 --> 00:56:56,559
different objects

00:56:54,160 --> 00:56:57,680
a kafka source a ping source and a

00:56:56,559 --> 00:56:59,599
mongodb source

00:56:57,680 --> 00:57:00,799
although they are different you can see

00:56:59,599 --> 00:57:05,440
that they all

00:57:00,799 --> 00:57:08,880
have this sync attribute marked in red

00:57:05,440 --> 00:57:11,119
and where the sources can specify

00:57:08,880 --> 00:57:12,000
the sync where they would send the event

00:57:11,119 --> 00:57:14,720
to

00:57:12,000 --> 00:57:15,839
they also can optionally specify a cloud

00:57:14,720 --> 00:57:18,480
event overrides

00:57:15,839 --> 00:57:20,319
which is in dotted red rectangles there

00:57:18,480 --> 00:57:21,119
which are attributes that are added or

00:57:20,319 --> 00:57:23,920
overridden

00:57:21,119 --> 00:57:26,160
in the outbound events although they

00:57:23,920 --> 00:57:28,720
share these common attributes

00:57:26,160 --> 00:57:30,720
the it's worth noting that each of these

00:57:28,720 --> 00:57:33,040
objects they find their own specific

00:57:30,720 --> 00:57:33,359
attributes for example the kafka one you

00:57:33,040 --> 00:57:36,400
can

00:57:33,359 --> 00:57:38,480
specify a consumer group in the spec or

00:57:36,400 --> 00:57:40,079
the ping source you can specify a

00:57:38,480 --> 00:57:41,839
current schedule

00:57:40,079 --> 00:57:46,000
even in the moment resource you can

00:57:41,839 --> 00:57:46,000
specify a database for instance

00:57:46,160 --> 00:57:49,920
having introduced the operator model and

00:57:48,480 --> 00:57:52,160
the source that type

00:57:49,920 --> 00:57:55,200
will now talk a little bit about what it

00:57:52,160 --> 00:57:56,880
means to be a compliant cognitive source

00:57:55,200 --> 00:57:59,040
in order to be a compliant cognitive

00:57:56,880 --> 00:58:01,599
source you need to conform to the source

00:57:59,040 --> 00:58:03,280
specification we added a link here for

00:58:01,599 --> 00:58:06,319
your reference

00:58:03,280 --> 00:58:09,280
roughly speaking the spec talks about

00:58:06,319 --> 00:58:10,960
crd level requirements and custom object

00:58:09,280 --> 00:58:13,760
level requirements

00:58:10,960 --> 00:58:15,440
regarding the crd level requirements we

00:58:13,760 --> 00:58:18,079
require the crds

00:58:15,440 --> 00:58:19,599
for sources have a particular label

00:58:18,079 --> 00:58:20,720
specifying that they are actually

00:58:19,599 --> 00:58:23,119
sources

00:58:20,720 --> 00:58:25,280
they have to have a sources category

00:58:23,119 --> 00:58:26,640
they should also have an annotation

00:58:25,280 --> 00:58:29,520
where they can specify

00:58:26,640 --> 00:58:31,760
the type of event they can produce

00:58:29,520 --> 00:58:33,599
regarding the custom object requirements

00:58:31,760 --> 00:58:36,079
they basically need to implement the

00:58:33,599 --> 00:58:38,720
source that type

00:58:36,079 --> 00:58:40,799
and it has the spec dot sync that we

00:58:38,720 --> 00:58:42,960
talked about in the previous slide

00:58:40,799 --> 00:58:44,799
plus some status condition that allows

00:58:42,960 --> 00:58:45,760
you to specify whether the source is

00:58:44,799 --> 00:58:49,680
ready or not

00:58:45,760 --> 00:58:50,880
as well as the resolve sync uri

00:58:49,680 --> 00:58:52,880
that we've talked a bit about what a

00:58:50,880 --> 00:58:55,200
source is let's see what sources are

00:58:52,880 --> 00:58:56,480
already available

00:58:55,200 --> 00:58:57,839
core sources are the ones that are

00:58:56,480 --> 00:58:58,960
maintained by the k native eventing

00:58:57,839 --> 00:59:02,400
project itself

00:58:58,960 --> 00:59:03,359
we have four of them api server source

00:59:02,400 --> 00:59:05,200
the source for

00:59:03,359 --> 00:59:06,400
kubernetes events a resource within the

00:59:05,200 --> 00:59:09,280
kubernetes cluster

00:59:06,400 --> 00:59:10,880
that represents the creation update or

00:59:09,280 --> 00:59:12,240
deletion of a resource within the

00:59:10,880 --> 00:59:14,640
cluster

00:59:12,240 --> 00:59:16,079
in order to create a api server source

00:59:14,640 --> 00:59:18,960
you can specify

00:59:16,079 --> 00:59:20,160
the highlighted attributes on the slide

00:59:18,960 --> 00:59:21,440
the service account name is the

00:59:20,160 --> 00:59:22,240
kubernetes service account that this

00:59:21,440 --> 00:59:24,079
will run out

00:59:22,240 --> 00:59:25,520
because this uses watches against the

00:59:24,079 --> 00:59:26,960
kubernetes api server

00:59:25,520 --> 00:59:29,200
it needs to have sufficient hardback

00:59:26,960 --> 00:59:32,000
permissions to do so

00:59:29,200 --> 00:59:33,280
the mode either resource or reference

00:59:32,000 --> 00:59:34,720
describes whether or not the event

00:59:33,280 --> 00:59:36,400
itself should contain the entire

00:59:34,720 --> 00:59:38,319
resource that's being modified

00:59:36,400 --> 00:59:40,480
or whether just a reference to that

00:59:38,319 --> 00:59:42,720
resource is going to be in the event

00:59:40,480 --> 00:59:44,839
finally there's a list of resources that

00:59:42,720 --> 00:59:47,200
are actually going to be

00:59:44,839 --> 00:59:50,079
watched

00:59:47,200 --> 00:59:50,799
the ping source sends events at a fixed

00:59:50,079 --> 00:59:53,440
schedule

00:59:50,799 --> 00:59:55,200
with a fixed payload this specific

00:59:53,440 --> 00:59:56,160
example uses the crown schedule for

00:59:55,200 --> 00:59:58,160
every minute

00:59:56,160 --> 00:59:59,760
and it always sends a message with the

00:59:58,160 --> 01:00:02,559
same data message

00:59:59,760 --> 01:00:02,559
is hello world

01:00:03,680 --> 01:00:06,799
sync binding isn't a direct source it's

01:00:05,760 --> 01:00:08,480
more of a meta source

01:00:06,799 --> 01:00:09,920
it doesn't generate any events itself

01:00:08,480 --> 01:00:12,079
but it makes it very quick and easy for

01:00:09,920 --> 01:00:13,839
you to make your own sources

01:00:12,079 --> 01:00:15,839
it can be used with the pod speckable

01:00:13,839 --> 01:00:17,760
duct type a pod speckable

01:00:15,839 --> 01:00:19,760
duct type object is one that has the

01:00:17,760 --> 01:00:22,079
spec.template field that is a pod

01:00:19,760 --> 01:00:24,240
template

01:00:22,079 --> 01:00:26,640
some examples are deployments canadian

01:00:24,240 --> 01:00:30,400
services and stateful sets

01:00:26,640 --> 01:00:33,520
a sync binding turns one a pod speckable

01:00:30,400 --> 01:00:36,960
into a source by providing

01:00:33,520 --> 01:00:37,599
in the sink binding the pod speckable

01:00:36,960 --> 01:00:40,480
you want to use

01:00:37,599 --> 01:00:41,280
it will inject the sink into that spot

01:00:40,480 --> 01:00:44,720
speckle

01:00:41,280 --> 01:00:46,319
as a environment variable this allows

01:00:44,720 --> 01:00:50,000
you to very quickly and easily

01:00:46,319 --> 01:00:50,000
get an ad hoc source up and running

01:00:50,480 --> 01:00:53,599
as we were using sync binding we

01:00:52,079 --> 01:00:55,359
realized that by far the most common

01:00:53,599 --> 01:00:57,599
usage was with the deployment

01:00:55,359 --> 01:00:59,119
container source is another meta source

01:00:57,599 --> 01:01:00,079
that combines a sync binding with a

01:00:59,119 --> 01:01:02,319
deployment

01:01:00,079 --> 01:01:03,200
by creating this one resource it will

01:01:02,319 --> 01:01:04,799
create

01:01:03,200 --> 01:01:07,280
the a deployment with the container

01:01:04,799 --> 01:01:10,559
image you specify in an injected

01:01:07,280 --> 01:01:14,079
sync uri this allows a very quick way

01:01:10,559 --> 01:01:16,480
to create ad hoc sources besides

01:01:14,079 --> 01:01:18,240
those four core sources that come out of

01:01:16,480 --> 01:01:19,680
the box with connective eventing

01:01:18,240 --> 01:01:21,440
there are other sources that are

01:01:19,680 --> 01:01:23,119
community owned and that needs to be

01:01:21,440 --> 01:01:25,359
installed separately

01:01:23,119 --> 01:01:26,799
we have things like the github source

01:01:25,359 --> 01:01:27,760
which will allow you to listen for

01:01:26,799 --> 01:01:29,200
github events

01:01:27,760 --> 01:01:31,119
for example if you want to get a

01:01:29,200 --> 01:01:33,119
notification with a pull request is

01:01:31,119 --> 01:01:35,520
created in a particular repo

01:01:33,119 --> 01:01:36,799
to trigger some ci cd pipeline you can

01:01:35,520 --> 01:01:38,880
use this

01:01:36,799 --> 01:01:40,720
or there's the kafka source that would

01:01:38,880 --> 01:01:41,359
allow you to receive events when some

01:01:40,720 --> 01:01:44,799
message

01:01:41,359 --> 01:01:45,119
is published into a kafka topic we also

01:01:44,799 --> 01:01:48,160
have

01:01:45,119 --> 01:01:50,079
non-sql based sources such as couchdb

01:01:48,160 --> 01:01:52,319
and mongodb where you can listen for

01:01:50,079 --> 01:01:53,599
changes in databases and collections and

01:01:52,319 --> 01:01:56,000
act accordingly

01:01:53,599 --> 01:01:58,559
for a more extensive list of sources you

01:01:56,000 --> 01:02:00,160
can follow that link

01:01:58,559 --> 01:02:02,480
besides the core sources and the

01:02:00,160 --> 01:02:05,599
community-owned sources we also help

01:02:02,480 --> 01:02:07,920
vendor-owned sources in particular

01:02:05,599 --> 01:02:10,160
we at google have been creating canadian

01:02:07,920 --> 01:02:12,240
sources for gcp services

01:02:10,160 --> 01:02:14,480
for example we have the cloud pub sap

01:02:12,240 --> 01:02:15,599
source which will allow you to receive

01:02:14,480 --> 01:02:18,640
events whenever you

01:02:15,599 --> 01:02:20,880
publish a message into a pub sub topic

01:02:18,640 --> 01:02:22,319
or we also have the cloud storage source

01:02:20,880 --> 01:02:24,400
that will allow you to listen for

01:02:22,319 --> 01:02:27,280
changes in gcs buckets

01:02:24,400 --> 01:02:29,359
for example when an image is uploaded in

01:02:27,280 --> 01:02:30,319
a gcs packet you can get an event and

01:02:29,359 --> 01:02:32,400
trigger some

01:02:30,319 --> 01:02:34,160
computer vision pipeline let's say to do

01:02:32,400 --> 01:02:38,000
object segmentation or

01:02:34,160 --> 01:02:40,160
image recognition we also have the cloud

01:02:38,000 --> 01:02:41,839
scheduler source which will send it an

01:02:40,160 --> 01:02:44,400
event based on a current schedule

01:02:41,839 --> 01:02:47,599
similar to ping source and some others

01:02:44,400 --> 01:02:48,079
a trigger mesh has been collaborating

01:02:47,599 --> 01:02:50,160
and

01:02:48,079 --> 01:02:51,760
contributing a bunch of sources

01:02:50,160 --> 01:02:55,039
especially for aws

01:02:51,760 --> 01:02:58,880
services such as sqs dynamodb and

01:02:55,039 --> 01:03:00,960
kinesis and finally vmware has also been

01:02:58,880 --> 01:03:02,160
contributing some connective sources

01:03:00,960 --> 01:03:05,359
such as

01:03:02,160 --> 01:03:06,799
b sphere now that we've looked at some

01:03:05,359 --> 01:03:08,240
of the available sources

01:03:06,799 --> 01:03:09,839
what if there's a source that you need

01:03:08,240 --> 01:03:12,720
that doesn't yet exist let's talk about

01:03:09,839 --> 01:03:14,480
implementing your own

01:03:12,720 --> 01:03:15,839
this is the kind of flowchart we think

01:03:14,480 --> 01:03:16,799
through whenever we think about creating

01:03:15,839 --> 01:03:18,559
a new source

01:03:16,799 --> 01:03:20,240
the first and most important question is

01:03:18,559 --> 01:03:21,839
do you want to make your own crd

01:03:20,240 --> 01:03:23,359
this makes it much easier for users to

01:03:21,839 --> 01:03:25,119
use your source but also

01:03:23,359 --> 01:03:27,119
increases the complexity of building

01:03:25,119 --> 01:03:28,640
everything if you make your own crd you

01:03:27,119 --> 01:03:30,079
do need to understand more about

01:03:28,640 --> 01:03:32,240
kubernetes and the kubernetes event

01:03:30,079 --> 01:03:33,440
model regardless of whether you choose

01:03:32,240 --> 01:03:34,960
to make your own crd

01:03:33,440 --> 01:03:37,039
you will need to write your own data

01:03:34,960 --> 01:03:39,119
plane your your own container image that

01:03:37,039 --> 01:03:40,880
sends the events themselves

01:03:39,119 --> 01:03:42,720
if you make a crd you also need to write

01:03:40,880 --> 01:03:44,799
a controller as well

01:03:42,720 --> 01:03:46,000
this flowchart can help determine what

01:03:44,799 --> 01:03:47,520
you should do

01:03:46,000 --> 01:03:48,720
if you want to make your own crd we

01:03:47,520 --> 01:03:49,760
recommend that you clone our sample

01:03:48,720 --> 01:03:52,720
source repository

01:03:49,760 --> 01:03:54,079
and modify it as necessary if you don't

01:03:52,720 --> 01:03:55,760
want your own crd

01:03:54,079 --> 01:03:57,440
then can you run your container images

01:03:55,760 --> 01:03:59,119
of deployment if so

01:03:57,440 --> 01:04:01,039
then use that container image and use

01:03:59,119 --> 01:04:02,880
make it container source

01:04:01,039 --> 01:04:04,400
if not can you run a container image in

01:04:02,880 --> 01:04:06,720
any pod speckable

01:04:04,400 --> 01:04:07,920
if so make that pod speckable resource

01:04:06,720 --> 01:04:10,160
and make a sync binding

01:04:07,920 --> 01:04:11,760
as well if not then once again we

01:04:10,160 --> 01:04:12,240
recommend you clone our sample source

01:04:11,760 --> 01:04:14,880
repo

01:04:12,240 --> 01:04:16,640
and modify sample sources are an example

01:04:14,880 --> 01:04:18,960
of how to build a source

01:04:16,640 --> 01:04:20,480
crd along with its controller web hook

01:04:18,960 --> 01:04:21,119
it follows all of k native's best

01:04:20,480 --> 01:04:23,119
practices

01:04:21,119 --> 01:04:26,000
and is set up to be easily modifiable to

01:04:23,119 --> 01:04:28,160
get your crd up and running quickly for

01:04:26,000 --> 01:04:29,599
experimentation or just ad hoc sources

01:04:28,160 --> 01:04:31,039
we recommend container source or sync

01:04:29,599 --> 01:04:32,319
binding because of just how quickly you

01:04:31,039 --> 01:04:34,079
can start

01:04:32,319 --> 01:04:35,280
as we mentioned if you use make your own

01:04:34,079 --> 01:04:36,720
crd then you're going to have to

01:04:35,280 --> 01:04:38,400
implement your own control plan

01:04:36,720 --> 01:04:40,079
the control plane is everything involved

01:04:38,400 --> 01:04:41,440
with getting a source ready to send

01:04:40,079 --> 01:04:42,720
events but not actually sending the

01:04:41,440 --> 01:04:44,319
events itself

01:04:42,720 --> 01:04:46,319
it's generally built up of two pieces

01:04:44,319 --> 01:04:48,240
the controller and the web hook

01:04:46,319 --> 01:04:50,240
the controller will read the spec of

01:04:48,240 --> 01:04:51,760
every custom object and then write out

01:04:50,240 --> 01:04:53,200
the status of every custom object

01:04:51,760 --> 01:04:55,039
running in a loop forever where it

01:04:53,200 --> 01:04:56,319
observes the real world this thoughts

01:04:55,039 --> 01:04:57,359
against the desired state from the

01:04:56,319 --> 01:04:58,960
custom object

01:04:57,359 --> 01:05:01,039
and acting to make the real world match

01:04:58,960 --> 01:05:04,240
the desired state

01:05:01,039 --> 01:05:06,480
the web hook will default validate

01:05:04,240 --> 01:05:08,319
and convert any of these resources as

01:05:06,480 --> 01:05:10,400
well the validation that the web book

01:05:08,319 --> 01:05:12,640
does is not on the events themselves

01:05:10,400 --> 01:05:13,520
rather on the custom object it can go

01:05:12,640 --> 01:05:15,920
beyond the

01:05:13,520 --> 01:05:17,039
level of validation available in crds by

01:05:15,920 --> 01:05:19,119
doing things like

01:05:17,039 --> 01:05:20,319
uh validating if two mutually exclusive

01:05:19,119 --> 01:05:22,160
fields are there

01:05:20,319 --> 01:05:25,200
they are not both present on the custom

01:05:22,160 --> 01:05:25,200
object at the same time

01:05:25,680 --> 01:05:30,880
the data plane is everything involved

01:05:27,920 --> 01:05:32,559
with sending the actual events

01:05:30,880 --> 01:05:34,480
we often call the pod that's sending the

01:05:32,559 --> 01:05:37,200
events the receive adapter

01:05:34,480 --> 01:05:38,400
what it does is it will receive an

01:05:37,200 --> 01:05:41,280
incoming event from

01:05:38,400 --> 01:05:41,760
somewhere this could be an external web

01:05:41,280 --> 01:05:43,760
request

01:05:41,760 --> 01:05:45,039
in the case of github source it could be

01:05:43,760 --> 01:05:47,039
an in-process timer

01:05:45,039 --> 01:05:48,799
in the case of ping source or it could

01:05:47,039 --> 01:05:50,880
be pulling from an external api in the

01:05:48,799 --> 01:05:52,720
case of api server source

01:05:50,880 --> 01:05:54,000
it will then take this event and convert

01:05:52,720 --> 01:05:56,079
it into a cloud event

01:05:54,000 --> 01:05:57,760
it can make any modifications it wants

01:05:56,079 --> 01:05:59,760
including the ce overrides we saw

01:05:57,760 --> 01:06:02,240
earlier

01:05:59,760 --> 01:06:03,839
it then sends this event to the sync

01:06:02,240 --> 01:06:05,039
that was passed in as an environment

01:06:03,839 --> 01:06:06,960
variable

01:06:05,039 --> 01:06:08,319
and finally if the original event

01:06:06,960 --> 01:06:10,000
producer would

01:06:08,319 --> 01:06:11,440
needs acknowledgements the receive

01:06:10,000 --> 01:06:14,000
adapter will then acknowledge

01:06:11,440 --> 01:06:15,520
upstream the picture we have here is of

01:06:14,000 --> 01:06:18,079
a pull based source

01:06:15,520 --> 01:06:19,280
receive adapter is pulling changes from

01:06:18,079 --> 01:06:21,119
the upstream producer

01:06:19,280 --> 01:06:23,359
and sending those as cloud events to the

01:06:21,119 --> 01:06:23,359
sync

01:06:24,079 --> 01:06:28,000
cloud events provides sdks in multiple

01:06:26,960 --> 01:06:29,599
languages

01:06:28,000 --> 01:06:32,480
here's an example of how we can send an

01:06:29,599 --> 01:06:34,240
event and go

01:06:32,480 --> 01:06:36,400
the first thing we do is we make a

01:06:34,240 --> 01:06:37,520
client in this case we want an http

01:06:36,400 --> 01:06:39,280
client and that's what the default

01:06:37,520 --> 01:06:41,440
happens to be

01:06:39,280 --> 01:06:43,839
next we make an event every event

01:06:41,440 --> 01:06:46,000
requires a source a type and an id

01:06:43,839 --> 01:06:49,039
we specify the source and type here and

01:06:46,000 --> 01:06:51,520
allow the sdk to default the id for us

01:06:49,039 --> 01:06:54,079
we also choose to add a data payload in

01:06:51,520 --> 01:06:56,480
this case hello world

01:06:54,079 --> 01:06:58,160
finally we look up where what the our

01:06:56,480 --> 01:06:58,799
sync is with the ksync environment

01:06:58,160 --> 01:07:02,559
variable

01:06:58,799 --> 01:07:04,720
and send to that address

01:07:02,559 --> 01:07:05,839
with that we have reached the end of the

01:07:04,720 --> 01:07:08,960
talk so let's

01:07:05,839 --> 01:07:08,960
jump to conclusions

01:07:09,119 --> 01:07:13,359
we hope you have a clear understanding

01:07:11,359 --> 01:07:15,680
what canadian sources are

01:07:13,359 --> 01:07:18,240
in general eventing sources are a key

01:07:15,680 --> 01:07:20,400
construct for any event driven system

01:07:18,240 --> 01:07:22,640
a canadian source is an abstraction that

01:07:20,400 --> 01:07:25,760
produces cloud events and send them

01:07:22,640 --> 01:07:28,640
downstream to any configurable sync

01:07:25,760 --> 01:07:29,359
canadian eventing comes with some core

01:07:28,640 --> 01:07:31,680
sources

01:07:29,359 --> 01:07:33,520
out of the box but there are multiple

01:07:31,680 --> 01:07:34,480
connective compliance sources out there

01:07:33,520 --> 01:07:36,960
for you to use

01:07:34,480 --> 01:07:38,319
both community-owned and vendor-owned

01:07:36,960 --> 01:07:40,640
but you can always create your own

01:07:38,319 --> 01:07:44,559
sources it's easy to do so and you don't

01:07:40,640 --> 01:07:44,559
need to fully understand kubernetes

01:07:44,640 --> 01:07:47,200
if you're interested in and want to

01:07:45,839 --> 01:07:48,799
learn more about any of the concepts

01:07:47,200 --> 01:07:50,160
we've talked about or just excited to

01:07:48,799 --> 01:07:52,079
start using canadian sources

01:07:50,160 --> 01:07:53,760
please visit our public docs if you have

01:07:52,079 --> 01:07:54,640
any questions please join our slack

01:07:53,760 --> 01:07:56,000
channel

01:07:54,640 --> 01:07:57,680
and if you just want to explore our code

01:07:56,000 --> 01:07:59,599
or our future plans please look at the k

01:07:57,680 --> 01:08:11,839
native github organization

01:07:59,599 --> 01:08:11,839
thank you very much thank you

01:08:16,560 --> 01:08:20,239
thanks adam and nacho for anyone

01:08:18,960 --> 01:08:20,880
interested in contributing their own

01:08:20,239 --> 01:08:22,960
sources

01:08:20,880 --> 01:08:25,199
that should get you started this

01:08:22,960 --> 01:08:27,199
solution is made even more powerful with

01:08:25,199 --> 01:08:30,799
a vibrant community

01:08:27,199 --> 01:08:30,799
of sources in the repository

01:08:31,040 --> 01:08:35,359
next up grant and song will take us

01:08:33,679 --> 01:08:36,719
through the nitty gritty details of the

01:08:35,359 --> 01:08:40,480
broker and the trigger

01:08:36,719 --> 01:08:42,640
this is our advanced topic as always

01:08:40,480 --> 01:08:43,600
put questions in the live q a forum as

01:08:42,640 --> 01:08:44,960
you think of them

01:08:43,600 --> 01:08:47,040
and start getting pumped for the post

01:08:44,960 --> 01:08:53,839
event party

01:08:47,040 --> 01:08:53,839
handing it over to you grant and song

01:09:40,799 --> 01:09:45,600
hi everyone how are you all doing i hope

01:09:43,679 --> 01:09:46,880
you're doing well

01:09:45,600 --> 01:09:49,920
today we're going to talk about the

01:09:46,880 --> 01:09:54,080
k-native broker let's get started

01:09:49,920 --> 01:09:56,159
first let me introduce myself i'm grant

01:09:54,080 --> 01:09:57,120
and i'm a k-native eventing working

01:09:56,159 --> 01:10:00,880
group lead

01:09:57,120 --> 01:10:00,880
and decay native toc member

01:10:01,360 --> 01:10:05,440
hi everyone my name is song i'm a

01:10:04,080 --> 01:10:08,000
software engineer at

01:10:05,440 --> 01:10:08,640
google i work actively on kennedy

01:10:08,000 --> 01:10:12,880
ranking

01:10:08,640 --> 01:10:12,880
and kinetic gcp open source projects

01:10:14,560 --> 01:10:18,640
in this talk we're not going to focus on

01:10:17,040 --> 01:10:22,080
explaining the broker concept

01:10:18,640 --> 01:10:23,920
or convincing you it's useful instead

01:10:22,080 --> 01:10:25,120
we'll explore the history and the

01:10:23,920 --> 01:10:28,239
current state

01:10:25,120 --> 01:10:29,920
of broker implementations we hope to

01:10:28,239 --> 01:10:31,280
shed some light on the challenges

01:10:29,920 --> 01:10:34,080
involved

01:10:31,280 --> 01:10:36,400
the solutions people have come up with

01:10:34,080 --> 01:10:39,679
and how one might decide which broker

01:10:36,400 --> 01:10:39,679
implementation to use

01:10:41,360 --> 01:10:45,679
we expect most people here are familiar

01:10:43,440 --> 01:10:48,000
with k-native eventing concepts

01:10:45,679 --> 01:10:50,000
but just in case here are the broker and

01:10:48,000 --> 01:10:51,679
trigger definitions from the original

01:10:50,000 --> 01:10:55,040
proposal

01:10:51,679 --> 01:10:57,199
publishers send their events to a broker

01:10:55,040 --> 01:10:59,199
and they expect the broker to reliably

01:10:57,199 --> 01:11:01,600
route those events to consumers who

01:10:59,199 --> 01:11:03,840
might be interested in them

01:11:01,600 --> 01:11:04,640
on the other side consumers create

01:11:03,840 --> 01:11:06,640
triggers

01:11:04,640 --> 01:11:07,920
to tell the broker which events are

01:11:06,640 --> 01:11:10,159
interesting

01:11:07,920 --> 01:11:13,840
and where those interesting events

01:11:10,159 --> 01:11:13,840
should be delivered

01:11:14,159 --> 01:11:19,120
to get a bit more concrete here are the

01:11:16,640 --> 01:11:22,320
yaml definitions for a basic broker

01:11:19,120 --> 01:11:22,960
and a basic trigger the broker has a

01:11:22,320 --> 01:11:26,080
name

01:11:22,960 --> 01:11:29,840
namespace and address the address

01:11:26,080 --> 01:11:32,960
is where publishers should send events

01:11:29,840 --> 01:11:34,480
the trigger specifies a broker name a

01:11:32,960 --> 01:11:38,080
filter definition

01:11:34,480 --> 01:11:40,159
and a subscriber address it says

01:11:38,080 --> 01:11:44,360
if an event arrives at the default

01:11:40,159 --> 01:11:46,320
broker with the type attribute set to

01:11:44,360 --> 01:11:48,320
com.example.object.delete

01:11:46,320 --> 01:11:51,280
then send that event to the delete

01:11:48,320 --> 01:11:51,280
processor service

01:11:53,120 --> 01:11:57,440
the first broker implementation didn't

01:11:55,679 --> 01:11:58,960
have a name at the time

01:11:57,440 --> 01:12:02,000
but we'll call it the single tenant

01:11:58,960 --> 01:12:02,000
channel broker for now

01:12:03,360 --> 01:12:08,480
its implementation looked like this

01:12:06,480 --> 01:12:09,520
an ingress pod receives events from

01:12:08,480 --> 01:12:12,159
publishers

01:12:09,520 --> 01:12:14,560
and forwards them to a channel the

01:12:12,159 --> 01:12:15,760
dispatcher pod receives every event from

01:12:14,560 --> 01:12:18,400
the channel

01:12:15,760 --> 01:12:19,199
checks it against each trigger filter

01:12:18,400 --> 01:12:22,560
and delivers

01:12:19,199 --> 01:12:24,480
any matches to the relevant consumers

01:12:22,560 --> 01:12:25,840
for simplicity the ingress and

01:12:24,480 --> 01:12:28,400
dispatcher pods

01:12:25,840 --> 01:12:29,440
only handle the events for a single

01:12:28,400 --> 01:12:31,840
broker

01:12:29,440 --> 01:12:37,840
and they run in the same namespace in

01:12:31,840 --> 01:12:37,840
which the broker object was created

01:12:39,040 --> 01:12:43,840
i'll pause for a second here to briefly

01:12:41,520 --> 01:12:46,239
talk about channels

01:12:43,840 --> 01:12:47,920
channel is a k-native eventing concept

01:12:46,239 --> 01:12:51,280
that's similar to broker

01:12:47,920 --> 01:12:53,679
but doesn't support filtering it's meant

01:12:51,280 --> 01:12:56,239
to be a simple interface to an external

01:12:53,679 --> 01:12:58,239
messaging backend

01:12:56,239 --> 01:13:01,120
publishers send events to a channel's

01:12:58,239 --> 01:13:05,600
address just like broker

01:13:01,120 --> 01:13:08,480
in this diagram that's b1-channel

01:13:05,600 --> 01:13:08,960
to receive events from a channel we

01:13:08,480 --> 01:13:11,840
create

01:13:08,960 --> 01:13:12,800
a subscription object which is like a

01:13:11,840 --> 01:13:16,159
trigger

01:13:12,800 --> 01:13:18,800
except it doesn't have a filter

01:13:16,159 --> 01:13:19,760
each subscription receives every event

01:13:18,800 --> 01:13:22,880
that's published

01:13:19,760 --> 01:13:24,800
to a channel the channel

01:13:22,880 --> 01:13:27,199
keeps track of event deliveries and

01:13:24,800 --> 01:13:28,080
failures to make sure that each event is

01:13:27,199 --> 01:13:33,520
delivered at least

01:13:28,080 --> 01:13:36,080
once to each subscriptions consumer

01:13:33,520 --> 01:13:38,600
now back to the broker the publisher

01:13:36,080 --> 01:13:41,440
delivers an event to ingress address

01:13:38,600 --> 01:13:44,080
b1-broker which forwards it to channel

01:13:41,440 --> 01:13:46,000
address b1-channel

01:13:44,080 --> 01:13:48,159
for every trigger there's a matching

01:13:46,000 --> 01:13:50,880
subscription that delivers events to the

01:13:48,159 --> 01:13:55,360
dispatcher at a trigger specific path

01:13:50,880 --> 01:13:57,920
slash t1 slash t2 etc

01:13:55,360 --> 01:13:58,800
the dispatcher filters each event for

01:13:57,920 --> 01:14:02,080
the trigger path

01:13:58,800 --> 01:14:04,640
it came in on then if it matches

01:14:02,080 --> 01:14:05,360
delivers the event to the consumer and

01:14:04,640 --> 01:14:09,600
reports

01:14:05,360 --> 01:14:09,600
success or failure back to the channel

01:14:10,239 --> 01:14:13,520
you can see in this example one of the

01:14:12,560 --> 01:14:16,320
challenges of this

01:14:13,520 --> 01:14:16,800
and other broker implementations i'll

01:14:16,320 --> 01:14:20,960
call it

01:14:16,800 --> 01:14:22,960
over delivery three red dot events are

01:14:20,960 --> 01:14:25,920
delivered to the dispatcher

01:14:22,960 --> 01:14:27,120
but only one makes it to a consumer the

01:14:25,920 --> 01:14:30,480
other two deliveries

01:14:27,120 --> 01:14:34,800
are wasted resources and that waste

01:14:30,480 --> 01:14:34,800
grows as the number of triggers grows

01:14:37,040 --> 01:14:40,159
the single tenancy of the single tenant

01:14:38,960 --> 01:14:43,120
channel broker was

01:14:40,159 --> 01:14:45,440
also a resource efficiency challenge

01:14:43,120 --> 01:14:46,560
since it didn't have scale to zero auto

01:14:45,440 --> 01:14:48,960
scaling

01:14:46,560 --> 01:14:50,320
the pods for ingress and dispatcher were

01:14:48,960 --> 01:14:53,760
always running

01:14:50,320 --> 01:14:55,520
even if the broker was idle for clusters

01:14:53,760 --> 01:14:57,840
with many idle brokers

01:14:55,520 --> 01:15:00,719
this was a lot of pods doing nothing but

01:14:57,840 --> 01:15:00,719
taking up space

01:15:02,239 --> 01:15:06,320
so we decided to replace the single

01:15:04,960 --> 01:15:11,840
tenant channel broker

01:15:06,320 --> 01:15:11,840
with the multi-tenant channel broker

01:15:12,000 --> 01:15:16,159
its implementation is similar to the

01:15:14,159 --> 01:15:18,560
single tenet channel broker

01:15:16,159 --> 01:15:21,760
but the ingress and dispatcher workloads

01:15:18,560 --> 01:15:24,960
are shared by all brokers in the cluster

01:15:21,760 --> 01:15:27,520
and they run in the system namespace

01:15:24,960 --> 01:15:28,239
auto scaling to zero isn't so critical

01:15:27,520 --> 01:15:30,239
anymore

01:15:28,239 --> 01:15:32,000
because our minimum footprint is now two

01:15:30,239 --> 01:15:34,880
pods per cluster

01:15:32,000 --> 01:15:36,840
instead of two pods per broker and

01:15:34,880 --> 01:15:39,199
because all brokers share the same

01:15:36,840 --> 01:15:40,800
infrastructure the aggregate resource

01:15:39,199 --> 01:15:42,400
efficiency of all brokers

01:15:40,800 --> 01:15:46,640
can theoretically be better than the

01:15:42,400 --> 01:15:46,640
same number of single tenant brokers

01:15:49,199 --> 01:15:53,199
the event flow through the multi-tenant

01:15:51,120 --> 01:15:54,239
broker is the same as the single tenant

01:15:53,199 --> 01:15:56,719
broker

01:15:54,239 --> 01:15:57,840
all events sent to a broker go to a

01:15:56,719 --> 01:15:59,920
single channel

01:15:57,840 --> 01:16:01,840
and are delivered once per trigger to

01:15:59,920 --> 01:16:03,440
the dispatcher

01:16:01,840 --> 01:16:06,239
the biggest difference is that

01:16:03,440 --> 01:16:07,280
publishers use a uri path to send events

01:16:06,239 --> 01:16:10,320
to a broker

01:16:07,280 --> 01:16:12,400
instead of a hostname

01:16:10,320 --> 01:16:15,280
note that the over delivery challenge

01:16:12,400 --> 01:16:17,360
isn't solved by this implementation

01:16:15,280 --> 01:16:19,280
and because the data plane is shared by

01:16:17,360 --> 01:16:21,679
all brokers in the cluster

01:16:19,280 --> 01:16:22,400
there's a risk that a single noisy

01:16:21,679 --> 01:16:24,080
broker

01:16:22,400 --> 01:16:25,679
will starve the other brokers of

01:16:24,080 --> 01:16:28,800
resources

01:16:25,679 --> 01:16:31,920
this is a trade-off multi-tenancy

01:16:28,800 --> 01:16:36,400
allows for better resource efficiency

01:16:31,920 --> 01:16:36,400
but isolation is harder to achieve

01:16:38,000 --> 01:16:41,520
now i'll hand off the song to talk about

01:16:40,480 --> 01:16:45,360
the other brokers

01:16:41,520 --> 01:16:48,800
in our k-native ecosystem

01:16:45,360 --> 01:16:50,880
thanks grant first up is the rapid mq

01:16:48,800 --> 01:16:53,120
broker

01:16:50,880 --> 01:16:54,560
the rapid mpq broker has a very

01:16:53,120 --> 01:16:57,280
straightforward design

01:16:54,560 --> 01:16:58,880
in fact you can find one-to-one mappings

01:16:57,280 --> 01:17:01,840
between rapid mq

01:16:58,880 --> 01:17:03,840
components and the cognitive concepts

01:17:01,840 --> 01:17:07,040
each connective broker object

01:17:03,840 --> 01:17:10,159
is mapped to a web mq exchange

01:17:07,040 --> 01:17:12,960
with an ingress service as the adapter

01:17:10,159 --> 01:17:13,600
quite similarly each kennedy trigger

01:17:12,960 --> 01:17:16,320
object

01:17:13,600 --> 01:17:18,000
corresponds to a rapid mq and a

01:17:16,320 --> 01:17:21,280
dispatcher

01:17:18,000 --> 01:17:24,960
recall that infinitive eventing triggers

01:17:21,280 --> 01:17:26,560
express interest to events by filtering

01:17:24,960 --> 01:17:29,679
on event attributes

01:17:26,560 --> 01:17:32,640
and this is implemented in rapid mq

01:17:29,679 --> 01:17:33,679
using headers exchange and bindings

01:17:32,640 --> 01:17:36,159
first of all

01:17:33,679 --> 01:17:37,520
event attributes are translated to

01:17:36,159 --> 01:17:39,760
message headers

01:17:37,520 --> 01:17:41,760
a binding then connects a queue for

01:17:39,760 --> 01:17:44,239
trigger to the exchange

01:17:41,760 --> 01:17:46,800
by applying filter attributes as binding

01:17:44,239 --> 01:17:46,800
arguments

01:17:47,280 --> 01:17:52,080
in rapid mq broker each broker has its

01:17:50,480 --> 01:17:55,280
own ingress deployment

01:17:52,080 --> 01:17:56,719
and exchange each trigger has its own

01:17:55,280 --> 01:17:59,600
dispatcher deployment

01:17:56,719 --> 01:18:01,440
and queue the ingress and dispatcher

01:17:59,600 --> 01:18:03,199
live in the user's namespace

01:18:01,440 --> 01:18:04,480
where the broker and trigger were

01:18:03,199 --> 01:18:06,239
created

01:18:04,480 --> 01:18:08,320
this is very similar to the single

01:18:06,239 --> 01:18:10,239
tenant channel broker that grant just

01:18:08,320 --> 01:18:12,719
talked about earlier

01:18:10,239 --> 01:18:13,679
since there is no risk of sharing the

01:18:12,719 --> 01:18:15,600
resource usage

01:18:13,679 --> 01:18:16,800
is something to keep in mind if you

01:18:15,600 --> 01:18:20,000
consider using

01:18:16,800 --> 01:18:21,440
the rapid mq broker especially when you

01:18:20,000 --> 01:18:24,880
need a lot of brokers

01:18:21,440 --> 01:18:26,000
or triggers there is no auto scaling on

01:18:24,880 --> 01:18:29,760
the ingress

01:18:26,000 --> 01:18:32,800
currently however on the dispatcher side

01:18:29,760 --> 01:18:35,040
kita is used to scale dispatchers

01:18:32,800 --> 01:18:35,920
currently it does not support multiple

01:18:35,040 --> 01:18:38,480
replicas

01:18:35,920 --> 01:18:39,120
but it does not it does support skill to

01:18:38,480 --> 01:18:42,239
zero

01:18:39,120 --> 01:18:43,600
so if most of your triggers are idle you

01:18:42,239 --> 01:18:47,360
are still fine

01:18:43,600 --> 01:18:49,360
in terms of resource usage next

01:18:47,360 --> 01:18:51,280
let me walk you through how event flow

01:18:49,360 --> 01:18:53,600
in webmq broker

01:18:51,280 --> 01:18:55,520
the publisher sends an event to the

01:18:53,600 --> 01:18:57,600
broker ingress endpoint

01:18:55,520 --> 01:18:59,440
the ingress translates the event to a

01:18:57,600 --> 01:19:01,920
rapid mq message

01:18:59,440 --> 01:19:03,199
and publishes it to the corresponding

01:19:01,920 --> 01:19:06,239
exchange

01:19:03,199 --> 01:19:09,920
a queue is created for each trigger and

01:19:06,239 --> 01:19:13,040
connected to the exchange via binding

01:19:09,920 --> 01:19:15,920
the binding filters the event based on

01:19:13,040 --> 01:19:16,719
message headers or event attributes

01:19:15,920 --> 01:19:19,040
finally

01:19:16,719 --> 01:19:19,840
if the event passes the filter the

01:19:19,040 --> 01:19:22,960
dispatcher

01:19:19,840 --> 01:19:24,320
pulls it from the queue and sends it to

01:19:22,960 --> 01:19:28,080
the consumer

01:19:24,320 --> 01:19:28,880
in rapid mq qs can share a single method

01:19:28,080 --> 01:19:30,800
store

01:19:28,880 --> 01:19:32,640
therefore there is not a lot of

01:19:30,800 --> 01:19:35,679
duplicate storage overhead

01:19:32,640 --> 01:19:36,880
by creating a cue per trigger and

01:19:35,679 --> 01:19:40,000
besides

01:19:36,880 --> 01:19:42,080
the filtering happens within revit mq

01:19:40,000 --> 01:19:43,760
and therefore there is no wasted

01:19:42,080 --> 01:19:47,120
delivery of events

01:19:43,760 --> 01:19:51,840
from the webmq to the dispatcher if they

01:19:47,120 --> 01:19:51,840
don't even pass the filter

01:19:52,320 --> 01:19:56,719
next let's talk about the kafka broker

01:19:58,800 --> 01:20:03,120
similar to the multi-tenant channel

01:20:00,560 --> 01:20:04,000
broker the kafka broker uses a

01:20:03,120 --> 01:20:07,199
multi-tenant

01:20:04,000 --> 01:20:10,159
shared data plan the ingress is sliced

01:20:07,199 --> 01:20:11,760
to serve multiple brokers by sharing a

01:20:10,159 --> 01:20:15,440
path

01:20:11,760 --> 01:20:17,840
a kafka topic is created for each broker

01:20:15,440 --> 01:20:20,639
like a channel is created for its broker

01:20:17,840 --> 01:20:23,040
in the multi-tenant channel broker case

01:20:20,639 --> 01:20:25,040
a multi-tenant dispatcher is responsible

01:20:23,040 --> 01:20:27,199
for pulling events from kafka

01:20:25,040 --> 01:20:30,239
applying the filter and delivering the

01:20:27,199 --> 01:20:32,880
events to the consumers

01:20:30,239 --> 01:20:34,719
if we look closer the thick arrow can be

01:20:32,880 --> 01:20:37,440
split into multiple ones

01:20:34,719 --> 01:20:38,880
each representing a consumer group for a

01:20:37,440 --> 01:20:41,199
trigger

01:20:38,880 --> 01:20:43,040
an interesting fact of the kafka broker

01:20:41,199 --> 01:20:46,000
is that the data plane is

01:20:43,040 --> 01:20:46,880
implemented in java because the kafka

01:20:46,000 --> 01:20:50,880
java client

01:20:46,880 --> 01:20:50,880
is better supported than the go client

01:20:52,880 --> 01:20:57,120
there is a single shared data plan that

01:20:55,120 --> 01:21:00,960
runs in the system namespace

01:20:57,120 --> 01:21:03,120
and it can be deployed by a yaml file

01:21:00,960 --> 01:21:06,719
there is no workload running in username

01:21:03,120 --> 01:21:10,239
space where brokers and triggers live

01:21:06,719 --> 01:21:13,840
as of today there is no skating strategy

01:21:10,239 --> 01:21:15,679
implemented let's take a more detailed

01:21:13,840 --> 01:21:17,760
look of the event flow

01:21:15,679 --> 01:21:20,239
the publisher sends an event to the

01:21:17,760 --> 01:21:22,719
endpoint of the target broker

01:21:20,239 --> 01:21:23,920
which points to the shared ingress

01:21:22,719 --> 01:21:27,040
you'll notice that

01:21:23,920 --> 01:21:30,080
the broker endpoint is a path of

01:21:27,040 --> 01:21:32,159
the ingress the ingress

01:21:30,080 --> 01:21:33,199
sends the event to the christian

01:21:32,159 --> 01:21:36,480
corresponding

01:21:33,199 --> 01:21:39,199
kafka topic the shared dispatcher

01:21:36,480 --> 01:21:40,320
pulls event from the kafka topics using

01:21:39,199 --> 01:21:43,199
trigger-specific

01:21:40,320 --> 01:21:44,960
consumer groups it finally sends the

01:21:43,199 --> 01:21:48,639
event to the target consumer

01:21:44,960 --> 01:21:48,639
if the event passes the filter

01:21:50,320 --> 01:21:57,280
now let's move on to the gcp broker the

01:21:56,159 --> 01:22:00,080
gcp broker

01:21:57,280 --> 01:22:01,280
uses google cloud pub sub as a messaging

01:22:00,080 --> 01:22:03,760
backend

01:22:01,280 --> 01:22:05,040
it is similar to kafka broker on the

01:22:03,760 --> 01:22:07,760
ingress side

01:22:05,040 --> 01:22:08,639
where it uses a shared multi-tenant

01:22:07,760 --> 01:22:11,600
ingress

01:22:08,639 --> 01:22:13,120
and a pub sub topic is created for each

01:22:11,600 --> 01:22:17,040
creative broker

01:22:13,120 --> 01:22:20,159
a unique design aspect of the gcp broker

01:22:17,040 --> 01:22:23,440
is that the dispatcher is split into

01:22:20,159 --> 01:22:26,880
separate fan out and retry stages

01:22:23,440 --> 01:22:29,600
for each broker find out pulls events

01:22:26,880 --> 01:22:30,000
from the corresponding pops up topic via

01:22:29,600 --> 01:22:33,440
a

01:22:30,000 --> 01:22:34,480
poor subscription only once and for all

01:22:33,440 --> 01:22:37,360
triggers

01:22:34,480 --> 01:22:39,040
it then attempts to deliver the events

01:22:37,360 --> 01:22:42,719
to all matching consumers

01:22:39,040 --> 01:22:44,960
only once for each failure or timeout

01:22:42,719 --> 01:22:45,920
the event is published to a trigger

01:22:44,960 --> 01:22:49,679
specific

01:22:45,920 --> 01:22:50,639
retry topic the retry stage periodically

01:22:49,679 --> 01:22:53,280
pulls events

01:22:50,639 --> 01:22:56,080
from those topics and attempts to

01:22:53,280 --> 01:22:58,719
redeliver to the consumers

01:22:56,080 --> 01:22:59,440
what motivated this design is that the

01:22:58,719 --> 01:23:03,040
pasta

01:22:59,440 --> 01:23:05,440
pricing model in pubsub users are built

01:23:03,040 --> 01:23:06,639
by the overall ingress and egress

01:23:05,440 --> 01:23:09,920
traffic

01:23:06,639 --> 01:23:12,320
while there is no storage cost therefore

01:23:09,920 --> 01:23:13,760
it is important to avoid duplicate

01:23:12,320 --> 01:23:17,199
methods deliveries

01:23:13,760 --> 01:23:18,239
from pub sub for multiple triggers in

01:23:17,199 --> 01:23:21,360
the happy pass

01:23:18,239 --> 01:23:22,719
and while in the ever pass when there is

01:23:21,360 --> 01:23:25,040
no storage cost

01:23:22,719 --> 01:23:27,920
for publishing field events to multiple

01:23:25,040 --> 01:23:27,920
reach high topics

01:23:30,400 --> 01:23:34,480
while multi-tenant broker designs are

01:23:32,400 --> 01:23:36,880
great at resource sharing

01:23:34,480 --> 01:23:38,000
a globally shared data plan reduces the

01:23:36,880 --> 01:23:40,880
flexibility

01:23:38,000 --> 01:23:41,440
to apply curated configurations and ask

01:23:40,880 --> 01:23:44,159
scopes

01:23:41,440 --> 01:23:46,000
for different use cases for example an

01:23:44,159 --> 01:23:47,199
operator may want to apply different

01:23:46,000 --> 01:23:50,719
resource coders

01:23:47,199 --> 01:23:53,120
for brokers owned by a specific team

01:23:50,719 --> 01:23:55,199
gcp broker solves this problem by

01:23:53,120 --> 01:23:57,760
allowing multiple data plans

01:23:55,199 --> 01:23:58,639
while each data plan is still

01:23:57,760 --> 01:24:01,600
multi-tenant

01:23:58,639 --> 01:24:03,840
and can still serve multiple brokers

01:24:01,600 --> 01:24:06,480
brokercell is a customer resource

01:24:03,840 --> 01:24:08,239
which manages resources for multi-tenant

01:24:06,480 --> 01:24:10,480
broker data plan

01:24:08,239 --> 01:24:11,360
a broker cell can be created and

01:24:10,480 --> 01:24:14,800
configured

01:24:11,360 --> 01:24:15,679
by an operator upfront or a default

01:24:14,800 --> 01:24:17,600
broker cell

01:24:15,679 --> 01:24:19,199
can be created automatically by the

01:24:17,600 --> 01:24:22,000
control plane

01:24:19,199 --> 01:24:22,800
the default broker cell is only created

01:24:22,000 --> 01:24:26,239
when there is

01:24:22,800 --> 01:24:29,040
at least one broker in this diagram

01:24:26,239 --> 01:24:30,480
broker 1 and 2 are assigned to the

01:24:29,040 --> 01:24:33,040
default broker cell

01:24:30,480 --> 01:24:33,920
while broker 3 is aligned to a customer

01:24:33,040 --> 01:24:36,159
broker cell

01:24:33,920 --> 01:24:37,280
which has an isolated data plan than the

01:24:36,159 --> 01:24:40,159
default

01:24:37,280 --> 01:24:42,480
note that the ability to assign brokers

01:24:40,159 --> 01:24:45,199
to non-default broker cell

01:24:42,480 --> 01:24:48,159
is not fully implemented yet but this is

01:24:45,199 --> 01:24:48,159
a low-hanging fruit

01:24:48,320 --> 01:24:52,239
the gcp broker currently uses hpa for

01:24:51,280 --> 01:24:55,040
auto scaling

01:24:52,239 --> 01:24:55,360
other skating mechanisms such as kida is

01:24:55,040 --> 01:24:59,920
an

01:24:55,360 --> 01:25:02,880
investigation back to you grant

01:24:59,920 --> 01:25:05,040
thanks song so what can we take away

01:25:02,880 --> 01:25:07,600
from all this

01:25:05,040 --> 01:25:09,120
well if you're a user wondering which

01:25:07,600 --> 01:25:12,800
broker to choose

01:25:09,120 --> 01:25:15,040
ask yourself a few simple questions

01:25:12,800 --> 01:25:16,080
are you more concerned about resource

01:25:15,040 --> 01:25:19,120
isolation

01:25:16,080 --> 01:25:21,679
or resource efficiency are you

01:25:19,120 --> 01:25:22,800
interested in using channels do you

01:25:21,679 --> 01:25:26,719
prefer kafka

01:25:22,800 --> 01:25:30,080
or pub sub there are no right answers

01:25:26,719 --> 01:25:31,600
and each choice has its own trade-offs

01:25:30,080 --> 01:25:34,320
but we hope you can find something that

01:25:31,600 --> 01:25:35,600
fits your needs

01:25:34,320 --> 01:25:37,440
if you're a contributor that's

01:25:35,600 --> 01:25:39,040
implementing a broker

01:25:37,440 --> 01:25:40,639
here are some basic principles to keep

01:25:39,040 --> 01:25:43,120
in mind

01:25:40,639 --> 01:25:43,920
first every messaging system has

01:25:43,120 --> 01:25:46,719
something that does

01:25:43,920 --> 01:25:48,560
really well try to optimize for those

01:25:46,719 --> 01:25:50,960
strengths

01:25:48,560 --> 01:25:53,520
second your broker should not be

01:25:50,960 --> 01:25:56,480
significantly slower or less efficient

01:25:53,520 --> 01:25:58,000
than using the messaging system directly

01:25:56,480 --> 01:25:59,840
to achieve this

01:25:58,000 --> 01:26:01,280
use the messaging system as little as

01:25:59,840 --> 01:26:04,480
possible

01:26:01,280 --> 01:26:05,199
filter uninteresting events early and

01:26:04,480 --> 01:26:08,800
avoid

01:26:05,199 --> 01:26:10,800
over delivery third

01:26:08,800 --> 01:26:13,520
be aware of the trade-off you're making

01:26:10,800 --> 01:26:15,679
between efficiency and isolation

01:26:13,520 --> 01:26:19,120
when you choose a multi-tenant versus a

01:26:15,679 --> 01:26:19,120
single tenant data plane

01:26:21,440 --> 01:26:25,120
finally if anything we talked about

01:26:23,520 --> 01:26:27,600
today was interesting to you

01:26:25,120 --> 01:26:29,760
and you'd like to learn more or just

01:26:27,600 --> 01:26:32,800
dive in and start contributing

01:26:29,760 --> 01:26:34,840
we'd love to continue the conversation

01:26:32,800 --> 01:26:36,080
join the eventing channel on k native

01:26:34,840 --> 01:26:37,679
slack

01:26:36,080 --> 01:26:40,159
or head to one of these github

01:26:37,679 --> 01:26:42,560
repositories to learn how you can get

01:26:40,159 --> 01:26:42,560
involved

01:26:42,800 --> 01:26:51,840
thank you everyone for your time

01:27:01,040 --> 01:27:05,199
thanks grant and song that should give

01:27:03,440 --> 01:27:06,800
you a good understanding of the details

01:27:05,199 --> 01:27:08,719
of this solution

01:27:06,800 --> 01:27:10,080
i hope you all found that as interesting

01:27:08,719 --> 01:27:12,320
as i do

01:27:10,080 --> 01:27:14,320
to learn more you can visit the k native

01:27:12,320 --> 01:27:16,400
documentation pages which we have linked

01:27:14,320 --> 01:27:18,239
in our q a section

01:27:16,400 --> 01:27:19,600
if you're a gcp customer you can check

01:27:18,239 --> 01:27:20,480
out our recently released beta

01:27:19,600 --> 01:27:23,040
implementation

01:27:20,480 --> 01:27:24,239
of eventing events for cloud run for

01:27:23,040 --> 01:27:26,800
anthos

01:27:24,239 --> 01:27:28,880
you can find this at cloud.google.com

01:27:26,800 --> 01:27:30,400
anthos slash run

01:27:28,880 --> 01:27:32,480
and link into the documentation from

01:27:30,400 --> 01:27:34,560
there we have our quick start

01:27:32,480 --> 01:27:36,000
and a quick lab that is linked to from

01:27:34,560 --> 01:27:37,360
our blog post

01:27:36,000 --> 01:27:39,040
that can help you get started with this

01:27:37,360 --> 01:27:40,800
solution on gcp

01:27:39,040 --> 01:27:42,400
and then as i mentioned the k-native

01:27:40,800 --> 01:27:44,480
documentation is your best place to

01:27:42,400 --> 01:27:45,199
learn more about k-native and canadian

01:27:44,480 --> 01:27:48,719
eventing

01:27:45,199 --> 01:27:50,800
in general okay so that's a wrap

01:27:48,719 --> 01:27:52,159
but not the end now it's time for the

01:27:50,800 --> 01:27:53,920
after party

01:27:52,159 --> 01:27:55,199
we've updated a google meet link to join

01:27:53,920 --> 01:27:56,960
the after party

01:27:55,199 --> 01:27:59,360
please look for a button in the agenda

01:27:56,960 --> 01:28:00,000
page today's speakers will also be

01:27:59,360 --> 01:28:02,159
joining

01:28:00,000 --> 01:28:03,760
we will have some quizzes interactive

01:28:02,159 --> 01:28:06,719
activity and a surprise

01:28:03,760 --> 01:28:15,840
guest i hope to see you all there thank

01:28:06,719 --> 01:28:15,840
you very much for joining us today

01:29:40,960 --> 01:29:43,040

YouTube URL: https://www.youtube.com/watch?v=FoCEh-YDfOk


