Title: Google Open Source Live presents Kubernetes (R) | Full Event
Publication date: 2021-01-11
Playlist: Google Open Source Live
Description: 
	
Captions: 
	00:00:00,540 --> 00:00:08,120
[Music]

00:00:28,840 --> 00:00:33,520
c

00:00:31,519 --> 00:00:35,280
good morning good afternoon and good

00:00:33,520 --> 00:00:37,280
evening thank you for joining the

00:00:35,280 --> 00:00:38,079
kubernetes day on google open source

00:00:37,280 --> 00:00:40,000
live

00:00:38,079 --> 00:00:41,120
this is a monthly series of sessions led

00:00:40,000 --> 00:00:44,079
by open source

00:00:41,120 --> 00:00:45,680
experts from google and for today good

00:00:44,079 --> 00:00:47,680
days community members

00:00:45,680 --> 00:00:49,200
now we really appreciate everyone taking

00:00:47,680 --> 00:00:50,399
the time to come here together

00:00:49,200 --> 00:00:51,600
digitally to learn a little bit more

00:00:50,399 --> 00:00:53,199
about you know really what makes

00:00:51,600 --> 00:00:55,280
kubernetes tick

00:00:53,199 --> 00:00:56,960
with that my name is bob killen also

00:00:55,280 --> 00:00:57,760
known as mr bobby tables across all the

00:00:56,960 --> 00:00:59,600
things

00:00:57,760 --> 00:01:02,239
i'm a program manager here at google and

00:00:59,600 --> 00:01:04,479
a member within the kubernetes project

00:01:02,239 --> 00:01:05,920
right now uh i'm i live just outside ann

00:01:04,479 --> 00:01:08,400
arbor and it was

00:01:05,920 --> 00:01:09,760
snowy earlier uh but today it's it's a

00:01:08,400 --> 00:01:13,680
little bit warmer so things are

00:01:09,760 --> 00:01:16,320
good it's just a little bit cool

00:01:13,680 --> 00:01:17,840
and hello everyone i'm caslin fields and

00:01:16,320 --> 00:01:19,360
i'm a developer advocate here at google

00:01:17,840 --> 00:01:21,520
cloud i'm also a member

00:01:19,360 --> 00:01:23,520
of the kubernetes special interest group

00:01:21,520 --> 00:01:25,360
for contributor experience

00:01:23,520 --> 00:01:27,040
and i'm based out of the seattle area

00:01:25,360 --> 00:01:28,960
where things are actually looking pretty

00:01:27,040 --> 00:01:30,880
nice and sunny today though a bit chilly

00:01:28,960 --> 00:01:33,119
at 35 degrees at 9am

00:01:30,880 --> 00:01:33,119
so

00:01:34,560 --> 00:01:39,360
hi everyone my name is tim i work on

00:01:36,880 --> 00:01:42,159
kubernetes and gke at google

00:01:39,360 --> 00:01:42,880
uh i'm currently in sunnyvale california

00:01:42,159 --> 00:01:44,479
where

00:01:42,880 --> 00:01:46,640
it's what californians think of as

00:01:44,479 --> 00:01:47,040
pretty darn cold and midwesterners like

00:01:46,640 --> 00:01:50,640
bob

00:01:47,040 --> 00:01:50,640
probably think of as short's weather

00:01:51,840 --> 00:01:55,200
we have some really interesting topics

00:01:53,680 --> 00:01:56,880
today if you're looking to learn

00:01:55,200 --> 00:01:58,240
more about some of the internals what

00:01:56,880 --> 00:01:59,200
makes kubernetes tick you're in the

00:01:58,240 --> 00:02:01,200
right place

00:01:59,200 --> 00:02:03,119
we'll kick it off with a talk from kevin

00:02:01,200 --> 00:02:05,040
delgado walking us through the

00:02:03,119 --> 00:02:07,520
life of a kubernetes api request from

00:02:05,040 --> 00:02:09,200
start to finish now but before that we

00:02:07,520 --> 00:02:10,720
have a couple housekeeping items i need

00:02:09,200 --> 00:02:13,840
to go over quick first

00:02:10,720 --> 00:02:15,840
um don't forget to put your questions in

00:02:13,840 --> 00:02:16,560
the live q a forum below the live stream

00:02:15,840 --> 00:02:18,160
window

00:02:16,560 --> 00:02:20,000
uh if you're viewing in full screen

00:02:18,160 --> 00:02:23,599
you'll actually need to exit out to

00:02:20,000 --> 00:02:25,120
see the q a um our sessions have been

00:02:23,599 --> 00:02:27,360
pre-recorded to

00:02:25,120 --> 00:02:28,879
allow the like accurate transcription as

00:02:27,360 --> 00:02:30,319
well as allowing speakers to focus on

00:02:28,879 --> 00:02:31,360
answering your questions live as they

00:02:30,319 --> 00:02:33,200
come up

00:02:31,360 --> 00:02:35,040
once we're done don't forget to join us

00:02:33,200 --> 00:02:36,959
in the after party today on google meet

00:02:35,040 --> 00:02:40,800
we will share a link to that

00:02:36,959 --> 00:02:40,800
at the end of the last session with that

00:02:48,840 --> 00:02:51,840
enjoy

00:03:28,239 --> 00:03:32,239
hey there i'm kevin delgado a software

00:03:30,720 --> 00:03:33,040
engineer that works on open source

00:03:32,239 --> 00:03:35,040
kubernetes

00:03:33,040 --> 00:03:36,959
here at google and today i'm going to

00:03:35,040 --> 00:03:39,440
take us through the life of a kubernetes

00:03:36,959 --> 00:03:41,440
api request

00:03:39,440 --> 00:03:43,120
the main focus here is to understand

00:03:41,440 --> 00:03:45,360
what happens when you type

00:03:43,120 --> 00:03:46,400
control create and pass in a kubernetes

00:03:45,360 --> 00:03:49,200
object with the dash

00:03:46,400 --> 00:03:50,480
f flag for example what happens when you

00:03:49,200 --> 00:03:52,959
call create and pass

00:03:50,480 --> 00:03:54,080
in a yaml file with a single replica set

00:03:52,959 --> 00:03:55,519
object

00:03:54,080 --> 00:03:57,519
this is targeted at people with some

00:03:55,519 --> 00:03:58,879
familiarity using cube control

00:03:57,519 --> 00:04:00,560
and so while you might know what should

00:03:58,879 --> 00:04:02,000
happen when you run this command maybe

00:04:00,560 --> 00:04:02,640
you haven't spent much time trying to

00:04:02,000 --> 00:04:05,840
understand

00:04:02,640 --> 00:04:07,439
how it actually happens here's the big

00:04:05,840 --> 00:04:08,959
overview of the main components we'll be

00:04:07,439 --> 00:04:11,519
looking at as we dive through

00:04:08,959 --> 00:04:12,080
the life of an api request as alluded to

00:04:11,519 --> 00:04:13,760
before

00:04:12,080 --> 00:04:15,519
our journey begins in the cube control

00:04:13,760 --> 00:04:17,040
command line tool for many

00:04:15,519 --> 00:04:19,280
this is the primary entry point for

00:04:17,040 --> 00:04:21,440
interacting with the kubernetes cluster

00:04:19,280 --> 00:04:23,919
from here our cube control command gets

00:04:21,440 --> 00:04:26,880
packaged up into an http request

00:04:23,919 --> 00:04:28,320
that gets sent over the wire to the cube

00:04:26,880 --> 00:04:30,400
api server

00:04:28,320 --> 00:04:32,479
the api server sits at the core of the

00:04:30,400 --> 00:04:34,960
kubernetes cluster in the control plane

00:04:32,479 --> 00:04:35,600
and is responsible for serving all

00:04:34,960 --> 00:04:37,440
requests

00:04:35,600 --> 00:04:38,960
for the many kubernetes resources that a

00:04:37,440 --> 00:04:40,720
cluster maintains

00:04:38,960 --> 00:04:43,040
the persistence of kubernetes objects is

00:04:40,720 --> 00:04:45,040
handled by the key value store at cd

00:04:43,040 --> 00:04:47,199
this is where the api server stores in

00:04:45,040 --> 00:04:48,000
our case the replica set object it has

00:04:47,199 --> 00:04:49,680
created

00:04:48,000 --> 00:04:51,520
if you're familiar with kubernetes

00:04:49,680 --> 00:04:52,960
noticeably absent from this picture is

00:04:51,520 --> 00:04:54,960
any mention of the controllers that

00:04:52,960 --> 00:04:56,240
operate on objects once they're created

00:04:54,960 --> 00:04:58,240
the nodes that actually run the

00:04:56,240 --> 00:05:00,320
containerized workloads and many other

00:04:58,240 --> 00:05:01,840
components of the kubernetes ecosystem

00:05:00,320 --> 00:05:03,919
those are all outside the scope of this

00:05:01,840 --> 00:05:06,000
talk as we really want to focus and dive

00:05:03,919 --> 00:05:08,880
deep on the path an api request takes

00:05:06,000 --> 00:05:10,240
through the system a fair warning that

00:05:08,880 --> 00:05:11,840
we are going to be taking a look

00:05:10,240 --> 00:05:13,759
at some of the source code that makes up

00:05:11,840 --> 00:05:15,440
each of these components for clarity and

00:05:13,759 --> 00:05:17,039
brevity the code is highly edited to

00:05:15,440 --> 00:05:18,639
emphasize the key pieces

00:05:17,039 --> 00:05:20,080
at the top of each snippet is a path

00:05:18,639 --> 00:05:21,199
where you can find it in source

00:05:20,080 --> 00:05:23,759
we're going to be looking at code from

00:05:21,199 --> 00:05:25,520
kubernetes version

00:05:23,759 --> 00:05:27,039
i'm well aware that a slide deck is not

00:05:25,520 --> 00:05:28,479
the best format to be code in

00:05:27,039 --> 00:05:30,080
but looking at the code really is one of

00:05:28,479 --> 00:05:33,120
the best ways to trace through

00:05:30,080 --> 00:05:34,960
what happens to an api request

00:05:33,120 --> 00:05:36,560
so starting with the cube control binary

00:05:34,960 --> 00:05:38,400
the few big points we'll touch on here

00:05:36,560 --> 00:05:39,680
are the setup around creating the cube

00:05:38,400 --> 00:05:41,840
control cli command

00:05:39,680 --> 00:05:43,759
the actual execution of that command the

00:05:41,840 --> 00:05:46,639
way in which we create an hd

00:05:43,759 --> 00:05:47,759
http request and execute that request

00:05:46,639 --> 00:05:50,639
sending to the ace

00:05:47,759 --> 00:05:52,320
sending to the api server and lastly the

00:05:50,639 --> 00:05:54,240
client-side serialization that takes

00:05:52,320 --> 00:05:56,080
place to encode the body of the request

00:05:54,240 --> 00:05:57,840
into a format that we can send across

00:05:56,080 --> 00:06:00,240
the wire that the api server can

00:05:57,840 --> 00:06:01,440
understand

00:06:00,240 --> 00:06:03,199
the first thing that happens when you

00:06:01,440 --> 00:06:05,360
run any queue control command is that

00:06:03,199 --> 00:06:07,919
all the cube control commands get built

00:06:05,360 --> 00:06:09,680
cubecontrol uses a cli a go cli command

00:06:07,919 --> 00:06:11,600
library called cobra

00:06:09,680 --> 00:06:13,280
and in the example on the left we see

00:06:11,600 --> 00:06:14,080
that to build the specific queue control

00:06:13,280 --> 00:06:15,919
create command

00:06:14,080 --> 00:06:17,680
we generate a new cobra command

00:06:15,919 --> 00:06:18,240
populating some of the documentation

00:06:17,680 --> 00:06:20,000
fields

00:06:18,240 --> 00:06:21,360
with things like what help info we want

00:06:20,000 --> 00:06:22,880
to get from the command

00:06:21,360 --> 00:06:24,560
but most importantly telling it how to

00:06:22,880 --> 00:06:27,199
run via its run field

00:06:24,560 --> 00:06:28,720
and when this is invoked it create calls

00:06:27,199 --> 00:06:30,400
a method run create

00:06:28,720 --> 00:06:33,120
on some options passing in something

00:06:30,400 --> 00:06:34,479
called a factory what is this factory

00:06:33,120 --> 00:06:36,319
well it's basically some drop in

00:06:34,479 --> 00:06:38,960
abstractions that are used

00:06:36,319 --> 00:06:40,400
when executing cube control for example

00:06:38,960 --> 00:06:40,960
we use the factory to get the various

00:06:40,400 --> 00:06:42,720
clients

00:06:40,960 --> 00:06:44,880
such as the rest client we need to make

00:06:42,720 --> 00:06:46,240
rest calls we also use this factory to

00:06:44,880 --> 00:06:49,440
retrieve something called the builder

00:06:46,240 --> 00:06:51,520
that we'll take a look at next

00:06:49,440 --> 00:06:52,960
that run create call we saw in the run

00:06:51,520 --> 00:06:54,639
method of the cobra command

00:06:52,960 --> 00:06:56,880
is what gets called when we want to

00:06:54,639 --> 00:06:58,720
execute the cube control create command

00:06:56,880 --> 00:07:00,400
after doing some initial setup the

00:06:58,720 --> 00:07:02,639
factory creates a builder

00:07:00,400 --> 00:07:04,400
now what this builder is responsible for

00:07:02,639 --> 00:07:06,319
is taking the data that you pass in via

00:07:04,400 --> 00:07:08,160
the dash app or dash k flags

00:07:06,319 --> 00:07:09,759
which is usually a yaml file of one or

00:07:08,160 --> 00:07:12,560
more kubernetes objects

00:07:09,759 --> 00:07:14,479
unpacks it and turns it into an iterable

00:07:12,560 --> 00:07:16,080
list of objects that for each object

00:07:14,479 --> 00:07:18,479
will have a generic rest operation

00:07:16,080 --> 00:07:19,120
performed on it this happens by calling

00:07:18,479 --> 00:07:21,039
visit

00:07:19,120 --> 00:07:22,960
on the resource created by the builder

00:07:21,039 --> 00:07:23,599
and for each resource it visits which in

00:07:22,960 --> 00:07:26,160
our case

00:07:23,599 --> 00:07:27,440
might just be the one replica set if

00:07:26,160 --> 00:07:29,840
that's all we passed in

00:07:27,440 --> 00:07:31,280
via the flag and for each resource it

00:07:29,840 --> 00:07:33,120
creates something called a helper

00:07:31,280 --> 00:07:35,599
and then calls the generic create on

00:07:33,120 --> 00:07:37,280
that helper with the object in question

00:07:35,599 --> 00:07:39,280
the helper is just a structure that

00:07:37,280 --> 00:07:39,759
helps that provides methods for running

00:07:39,280 --> 00:07:42,319
generic

00:07:39,759 --> 00:07:44,000
restful operations it has methods post

00:07:42,319 --> 00:07:46,080
get list delete etc

00:07:44,000 --> 00:07:47,120
that all map to corresponding http

00:07:46,080 --> 00:07:49,919
operations

00:07:47,120 --> 00:07:51,199
this helper executes the http request in

00:07:49,919 --> 00:07:53,360
the case of a post request

00:07:51,199 --> 00:07:54,800
like we see here a big part of that is

00:07:53,360 --> 00:07:57,759
building the body of the request

00:07:54,800 --> 00:07:58,400
and we'll take a look at that next as we

00:07:57,759 --> 00:08:00,240
saw

00:07:58,400 --> 00:08:02,000
the helper uses its rest client to build

00:08:00,240 --> 00:08:04,000
and execute a post request

00:08:02,000 --> 00:08:05,360
the rest client lives in client go which

00:08:04,000 --> 00:08:07,280
is the same go library that you would

00:08:05,360 --> 00:08:09,280
use if building some external program

00:08:07,280 --> 00:08:10,560
to interact with the kubernetes cluster

00:08:09,280 --> 00:08:12,400
prior to sending

00:08:10,560 --> 00:08:13,840
the prior to sending the request it

00:08:12,400 --> 00:08:15,840
builds the body of the request

00:08:13,840 --> 00:08:17,520
from an object into the format that gets

00:08:15,840 --> 00:08:19,280
sent across the wire and understood by

00:08:17,520 --> 00:08:21,360
the api server

00:08:19,280 --> 00:08:22,400
as you can see the rest client accepts a

00:08:21,360 --> 00:08:24,639
number of formats

00:08:22,400 --> 00:08:26,319
string bytes i o reader but if you're

00:08:24,639 --> 00:08:27,840
using the helper like we're doing here

00:08:26,319 --> 00:08:30,000
then the body is going to be created

00:08:27,840 --> 00:08:31,680
from a go kubernetes object in memory

00:08:30,000 --> 00:08:33,680
and must be serialized into the proper

00:08:31,680 --> 00:08:34,080
wire format for that we'll take a look

00:08:33,680 --> 00:08:35,919
at

00:08:34,080 --> 00:08:37,839
some interfaces from the api machinery

00:08:35,919 --> 00:08:39,680
runtime package in just a moment

00:08:37,839 --> 00:08:41,919
the last step is to actually send the

00:08:39,680 --> 00:08:44,560
http request across the wire

00:08:41,919 --> 00:08:45,760
which which it does via the do method

00:08:44,560 --> 00:08:48,640
using a vanilla go

00:08:45,760 --> 00:08:50,240
http client that then gets the response

00:08:48,640 --> 00:08:51,040
back from the server and bubbles it up

00:08:50,240 --> 00:08:53,040
to the user

00:08:51,040 --> 00:08:55,440
in the format outputted by the specific

00:08:53,040 --> 00:08:57,440
cube control command

00:08:55,440 --> 00:08:58,560
here we see the interfaces for

00:08:57,440 --> 00:09:00,399
serialization

00:08:58,560 --> 00:09:01,839
that get implemented by auto-generated

00:09:00,399 --> 00:09:03,680
api machinery code

00:09:01,839 --> 00:09:04,959
this serializer interface is just an

00:09:03,680 --> 00:09:06,720
encoder and a decoder

00:09:04,959 --> 00:09:08,880
and as you might expect the encoder is

00:09:06,720 --> 00:09:09,279
responsible for converting a kubernetes

00:09:08,880 --> 00:09:11,360
object

00:09:09,279 --> 00:09:12,560
ghoststruck in memory into the canonical

00:09:11,360 --> 00:09:14,720
wire format

00:09:12,560 --> 00:09:16,320
to send across the network to the api

00:09:14,720 --> 00:09:18,399
server and likewise

00:09:16,320 --> 00:09:20,000
the decoder takes that wire format and

00:09:18,399 --> 00:09:21,040
converts it back into a kubernetes

00:09:20,000 --> 00:09:23,279
object in go

00:09:21,040 --> 00:09:25,120
that lives in memory a few really

00:09:23,279 --> 00:09:27,120
interesting things happens here though

00:09:25,120 --> 00:09:29,200
first of all kubernetes accepts multiple

00:09:27,120 --> 00:09:30,959
wire types json and protobuf you'll

00:09:29,200 --> 00:09:32,880
notice the negotiated serializer

00:09:30,959 --> 00:09:34,000
which like the regular serializer offers

00:09:32,880 --> 00:09:36,160
encoding and decoding

00:09:34,000 --> 00:09:37,839
but serves as an abstraction around the

00:09:36,160 --> 00:09:39,920
multiple supported media types

00:09:37,839 --> 00:09:42,160
so not only in json and protobuf but

00:09:39,920 --> 00:09:43,440
also but also some more encoding options

00:09:42,160 --> 00:09:45,600
like content type header

00:09:43,440 --> 00:09:47,120
or json pretty printing you'll also

00:09:45,600 --> 00:09:49,760
notice the encoder 4

00:09:47,120 --> 00:09:51,839
version and decoder 2 version where we

00:09:49,760 --> 00:09:53,600
can get an encoder or decoder that are

00:09:51,839 --> 00:09:55,839
that are aware of how to serialize for a

00:09:53,600 --> 00:09:57,519
specific object version

00:09:55,839 --> 00:09:59,920
further you'll see that a codec is the

00:09:57,519 --> 00:10:01,680
exact same interface as a serializer

00:09:59,920 --> 00:10:03,279
the difference is that under the hood a

00:10:01,680 --> 00:10:05,200
serializer will not modify

00:10:03,279 --> 00:10:07,600
the kubernetes object being encoded or

00:10:05,200 --> 00:10:10,959
decoded while a codec can

00:10:07,600 --> 00:10:12,640
this is useful for understanding when

00:10:10,959 --> 00:10:14,320
when looking at how the api server

00:10:12,640 --> 00:10:16,480
handles the request it receives

00:10:14,320 --> 00:10:18,560
this serialization library straddles

00:10:16,480 --> 00:10:19,120
both the cube control client and the api

00:10:18,560 --> 00:10:20,959
server

00:10:19,120 --> 00:10:23,120
and on the server side is responsible

00:10:20,959 --> 00:10:24,560
not only for decoding the wire format

00:10:23,120 --> 00:10:25,920
into a ghost chuck but also for

00:10:24,560 --> 00:10:27,680
converting the object to the right

00:10:25,920 --> 00:10:29,120
version and defaulting any fields the

00:10:27,680 --> 00:10:31,200
server requires

00:10:29,120 --> 00:10:33,120
the idea behind conversion is that other

00:10:31,200 --> 00:10:34,720
clients is that older clients are

00:10:33,120 --> 00:10:36,079
expected to be able to communicate with

00:10:34,720 --> 00:10:37,440
new api servers

00:10:36,079 --> 00:10:39,279
in order for these older clients to

00:10:37,440 --> 00:10:42,000
continue to work the codec

00:10:39,279 --> 00:10:43,600
can encode or decode an object to or

00:10:42,000 --> 00:10:45,360
from a different version

00:10:43,600 --> 00:10:47,279
so something like something that is v1

00:10:45,360 --> 00:10:48,160
on the wire could be v2 in memory or

00:10:47,279 --> 00:10:49,839
vice versa

00:10:48,160 --> 00:10:52,000
there's also the concept of an internal

00:10:49,839 --> 00:10:53,200
version so the server or storage clients

00:10:52,000 --> 00:10:54,160
only need to know how to deal with a

00:10:53,200 --> 00:10:56,000
single version

00:10:54,160 --> 00:10:57,519
and the codec can convert from one

00:10:56,000 --> 00:10:58,880
version to the internal version

00:10:57,519 --> 00:11:00,959
and the back to a different version if

00:10:58,880 --> 00:11:02,959
needed similarly

00:11:00,959 --> 00:11:05,120
older clients may not be aware of new

00:11:02,959 --> 00:11:06,320
api fields and yet these api fields

00:11:05,120 --> 00:11:08,640
might be mandatory

00:11:06,320 --> 00:11:10,800
or only accept certain values with this

00:11:08,640 --> 00:11:12,000
machinery newer api servers are able to

00:11:10,800 --> 00:11:13,440
default those fields

00:11:12,000 --> 00:11:14,560
on requests that come in from other

00:11:13,440 --> 00:11:15,600
clients that might have the fields

00:11:14,560 --> 00:11:17,760
absent

00:11:15,600 --> 00:11:19,360
finally when we talk about the ghost

00:11:17,760 --> 00:11:20,720
struct of a kubernetes object

00:11:19,360 --> 00:11:22,399
what we're talking about is a ghost

00:11:20,720 --> 00:11:24,720
structure that satisfies the

00:11:22,399 --> 00:11:26,880
runtime.object interface

00:11:24,720 --> 00:11:30,000
that means that all kubernetes api

00:11:26,880 --> 00:11:32,160
objects replica sets pods services etc

00:11:30,000 --> 00:11:33,839
must implement this interface you'll see

00:11:32,160 --> 00:11:36,079
this runtime object

00:11:33,839 --> 00:11:38,000
type all throughout the code this

00:11:36,079 --> 00:11:39,760
interface used to be just one function

00:11:38,000 --> 00:11:40,480
called is a runtime object that didn't

00:11:39,760 --> 00:11:42,560
do anything

00:11:40,480 --> 00:11:43,839
other than signal if a struck was an api

00:11:42,560 --> 00:11:46,160
object now

00:11:43,839 --> 00:11:47,279
there are a couple functions that help

00:11:46,160 --> 00:11:49,200
remove some of the need

00:11:47,279 --> 00:11:50,560
for reflection and makes operation of

00:11:49,200 --> 00:11:53,440
the api machinery code

00:11:50,560 --> 00:11:53,440
much more efficient

00:11:53,600 --> 00:11:56,959
so that's it for the cube control client

00:11:55,440 --> 00:11:58,560
now that we have a request on the wire

00:11:56,959 --> 00:12:00,399
it's time to take a look at what happens

00:11:58,560 --> 00:12:02,079
server side

00:12:00,399 --> 00:12:04,320
within the cube api server we'll take a

00:12:02,079 --> 00:12:06,560
look at the api server aggregation layer

00:12:04,320 --> 00:12:07,680
and then see how the api server gets set

00:12:06,560 --> 00:12:08,399
up and configures routing and

00:12:07,680 --> 00:12:10,399
dispatching

00:12:08,399 --> 00:12:12,480
to the right endpoint and then we'll

00:12:10,399 --> 00:12:14,399
take a look at the business logic

00:12:12,480 --> 00:12:16,560
of how the request is actually handled

00:12:14,399 --> 00:12:20,560
and lastly we'll dive into how the newly

00:12:16,560 --> 00:12:20,560
created object is persistent to xcd

00:12:21,360 --> 00:12:25,360
the way the cube api server gets started

00:12:23,360 --> 00:12:27,519
is actually itself a cobra command 2.

00:12:25,360 --> 00:12:28,399
like all cobra commands it has a run

00:12:27,519 --> 00:12:30,399
function that is

00:12:28,399 --> 00:12:31,839
called when executed this run function

00:12:30,399 --> 00:12:34,000
kicks off a helper function

00:12:31,839 --> 00:12:36,240
that creates the server chain the server

00:12:34,000 --> 00:12:38,800
chain aggregates an extension server

00:12:36,240 --> 00:12:40,720
with the queue api server the api server

00:12:38,800 --> 00:12:41,839
aggregation is fairly new to kubernetes

00:12:40,720 --> 00:12:44,399
and provides some pretty nifty

00:12:41,839 --> 00:12:46,320
customization aggregated api servers let

00:12:44,399 --> 00:12:48,000
you do things with your api server

00:12:46,320 --> 00:12:50,480
that you wouldn't be able to achieve out

00:12:48,000 --> 00:12:52,399
of the box or even with custom resources

00:12:50,480 --> 00:12:53,680
such as using a different storage api

00:12:52,399 --> 00:12:55,279
instead of ncd

00:12:53,680 --> 00:12:57,440
that aside though for creating a simple

00:12:55,279 --> 00:13:00,240
replica set all we really need is a cube

00:12:57,440 --> 00:13:00,240
api server

00:13:00,880 --> 00:13:04,560
the cube api server that gets created

00:13:02,720 --> 00:13:06,880
holds alt state in a struct called a

00:13:04,560 --> 00:13:07,360
generic api server in addition to a lot

00:13:06,880 --> 00:13:10,399
of

00:13:07,360 --> 00:13:12,480
state to a lot of state and creating

00:13:10,399 --> 00:13:14,160
the generic the generic api server

00:13:12,480 --> 00:13:16,320
initializes the handler chain

00:13:14,160 --> 00:13:17,920
which is a series of http middlewares

00:13:16,320 --> 00:13:19,519
that every request goes through

00:13:17,920 --> 00:13:21,279
that is responsible for various things

00:13:19,519 --> 00:13:23,440
such as authorization cores

00:13:21,279 --> 00:13:25,200
timeouts or macs and flights and a

00:13:23,440 --> 00:13:27,519
handful of other functions

00:13:25,200 --> 00:13:29,440
additionally we call install api on the

00:13:27,519 --> 00:13:31,360
server so that we can serve requests

00:13:29,440 --> 00:13:32,720
for all the various kubernetes resources

00:13:31,360 --> 00:13:34,800
consistently

00:13:32,720 --> 00:13:36,480
this api installation sets up the

00:13:34,800 --> 00:13:37,120
routing and dispatch so that request

00:13:36,480 --> 00:13:38,639
urls

00:13:37,120 --> 00:13:40,240
gets sent to the correct resource

00:13:38,639 --> 00:13:41,360
handlers we'll take a look at routing

00:13:40,240 --> 00:13:43,360
and dispatch in a bit

00:13:41,360 --> 00:13:45,839
but first we'll see how the api server

00:13:43,360 --> 00:13:47,839
actually starts serving

00:13:45,839 --> 00:13:49,920
the generic api server exposes a run

00:13:47,839 --> 00:13:51,040
method that gets ran when the api server

00:13:49,920 --> 00:13:52,720
is invoked

00:13:51,040 --> 00:13:54,160
this sets up a shutdown delay so the

00:13:52,720 --> 00:13:55,040
server can gracefully shut down when

00:13:54,160 --> 00:13:56,880
terminated

00:13:55,040 --> 00:13:58,720
and calls serve on the server's secure

00:13:56,880 --> 00:14:00,959
serving info which sets

00:13:58,720 --> 00:14:03,120
up tls and finally invokes serve on a

00:14:00,959 --> 00:14:04,560
vanilla go http server

00:14:03,120 --> 00:14:07,839
which is the entry point for the socket

00:14:04,560 --> 00:14:09,519
to actually start listing and serving

00:14:07,839 --> 00:14:11,360
circling back to routing dispatch that

00:14:09,519 --> 00:14:13,519
we mentioned earlier we use an api

00:14:11,360 --> 00:14:14,160
installer to register the actual http

00:14:13,519 --> 00:14:16,800
handlers

00:14:14,160 --> 00:14:17,839
that process the request that comes in

00:14:16,800 --> 00:14:20,880
that install

00:14:17,839 --> 00:14:22,480
api method that we saw earlier

00:14:20,880 --> 00:14:24,720
called when creating the generic api

00:14:22,480 --> 00:14:26,560
server uses a library called go restful

00:14:24,720 --> 00:14:28,880
for setting up a muxer that matches the

00:14:26,560 --> 00:14:30,320
request path with its proper handler

00:14:28,880 --> 00:14:31,760
the way we configure the go restful

00:14:30,320 --> 00:14:33,839
muxer is by starting with something

00:14:31,760 --> 00:14:36,240
called an api group version struct

00:14:33,839 --> 00:14:37,600
api resources and kubernetes are divided

00:14:36,240 --> 00:14:39,440
by group version

00:14:37,600 --> 00:14:41,680
this indicates to the api installer

00:14:39,440 --> 00:14:43,519
which path these resources live at

00:14:41,680 --> 00:14:45,199
it contains a variety of useful fields

00:14:43,519 --> 00:14:47,519
like a negotiated serializer

00:14:45,199 --> 00:14:48,720
for encoding and decoding into and from

00:14:47,519 --> 00:14:50,399
the various formats

00:14:48,720 --> 00:14:52,160
as well as something called a storage

00:14:50,399 --> 00:14:54,079
which performs the various rest

00:14:52,160 --> 00:14:55,920
operations and wraps the actual client

00:14:54,079 --> 00:14:57,760
used to write to storage

00:14:55,920 --> 00:14:59,360
when we use the api group version to

00:14:57,760 --> 00:15:01,600
register the resource handlers we

00:14:59,360 --> 00:15:02,000
program go restful to link a resources

00:15:01,600 --> 00:15:05,519
path

00:15:02,000 --> 00:15:07,279
to its handler for every given http verb

00:15:05,519 --> 00:15:10,320
along with some other things like giving

00:15:07,279 --> 00:15:11,920
us auto-generated open api documentation

00:15:10,320 --> 00:15:13,199
the register resource handler snippet

00:15:11,920 --> 00:15:14,880
you see on the right hand side is

00:15:13,199 --> 00:15:16,320
actually a huge several hundred lines

00:15:14,880 --> 00:15:18,240
long function

00:15:16,320 --> 00:15:19,920
as a snippet indicates there's a switch

00:15:18,240 --> 00:15:22,320
statement with a case per

00:15:19,920 --> 00:15:24,480
http verb that sets up a route to the

00:15:22,320 --> 00:15:26,399
given handler

00:15:24,480 --> 00:15:28,240
so all we've seen so far is what gets

00:15:26,399 --> 00:15:28,959
ran when the api server binary is

00:15:28,240 --> 00:15:30,399
started

00:15:28,959 --> 00:15:31,759
we're done with that now and we've

00:15:30,399 --> 00:15:33,360
finally made it to the code that runs

00:15:31,759 --> 00:15:35,120
when an actual request comes through

00:15:33,360 --> 00:15:37,680
here in the create handler is what the

00:15:35,120 --> 00:15:38,560
http is where the http request gets

00:15:37,680 --> 00:15:40,639
handled

00:15:38,560 --> 00:15:41,920
as you can see we're using the decoder

00:15:40,639 --> 00:15:43,519
we've seen previously

00:15:41,920 --> 00:15:44,959
this decodes the body from the wire

00:15:43,519 --> 00:15:46,959
format into the correct version of the

00:15:44,959 --> 00:15:50,240
go runtime object struct in memory

00:15:46,959 --> 00:15:51,839
that we pass on to storage another

00:15:50,240 --> 00:15:53,360
interaction that takes place here

00:15:51,839 --> 00:15:55,440
are the calls out to the admission web

00:15:53,360 --> 00:15:56,959
hooks admission web hooks are calls that

00:15:55,440 --> 00:15:58,959
the api server makes

00:15:56,959 --> 00:16:00,800
to external servers that perform an

00:15:58,959 --> 00:16:02,240
action on the object being handled by

00:16:00,800 --> 00:16:03,680
the api server

00:16:02,240 --> 00:16:05,680
there are two kinds of admission web

00:16:03,680 --> 00:16:06,399
hooks mutating web hooks and validating

00:16:05,680 --> 00:16:08,000
web hooks

00:16:06,399 --> 00:16:10,000
the mutating commission web book gets

00:16:08,000 --> 00:16:10,560
called first and modifies the incoming

00:16:10,000 --> 00:16:12,320
object

00:16:10,560 --> 00:16:13,920
such as by adding custom defaults or

00:16:12,320 --> 00:16:15,040
annotations and then right before

00:16:13,920 --> 00:16:16,560
storing to add cd

00:16:15,040 --> 00:16:18,399
the object is passed to the validating

00:16:16,560 --> 00:16:20,079
commission webhook this can reject

00:16:18,399 --> 00:16:22,160
requests based on custom policies

00:16:20,079 --> 00:16:24,320
external to the api server

00:16:22,160 --> 00:16:26,320
lastly we actually persist the object to

00:16:24,320 --> 00:16:27,040
storage in etcd before returning the

00:16:26,320 --> 00:16:29,040
response

00:16:27,040 --> 00:16:30,800
of the request up the call stack how

00:16:29,040 --> 00:16:31,920
this object gets persisted to storage is

00:16:30,800 --> 00:16:33,519
what we'll look at next

00:16:31,920 --> 00:16:35,120
but take note here that it's under

00:16:33,519 --> 00:16:37,360
create call on something called a named

00:16:35,120 --> 00:16:39,360
creator

00:16:37,360 --> 00:16:40,560
the last part of our journey here we'll

00:16:39,360 --> 00:16:42,399
dive deep into

00:16:40,560 --> 00:16:44,240
how we go from the request handler

00:16:42,399 --> 00:16:46,000
processing the request body

00:16:44,240 --> 00:16:47,839
into the final state of the kubernetes

00:16:46,000 --> 00:16:51,440
object to be stored and persisting

00:16:47,839 --> 00:16:53,440
and persisting that object to ncd

00:16:51,440 --> 00:16:55,360
as mentioned previously the request

00:16:53,440 --> 00:16:56,560
handler calls the create method on a

00:16:55,360 --> 00:16:58,720
named creator to

00:16:56,560 --> 00:17:00,399
persist the kubernetes object at city

00:16:58,720 --> 00:17:01,040
the name creator and the more generic

00:17:00,399 --> 00:17:02,639
creator

00:17:01,040 --> 00:17:04,240
are interfaces that map to the

00:17:02,639 --> 00:17:07,839
corresponding http

00:17:04,240 --> 00:17:09,360
create method there exist interfaces for

00:17:07,839 --> 00:17:12,079
each of the restful verbs get

00:17:09,360 --> 00:17:13,039
watch create delete update that act on a

00:17:12,079 --> 00:17:14,640
single item

00:17:13,039 --> 00:17:16,079
as well as a separate set of interfaces

00:17:14,640 --> 00:17:17,280
for each of these verbs that act on a

00:17:16,079 --> 00:17:18,559
collection of items

00:17:17,280 --> 00:17:20,240
implementing this interface is not

00:17:18,559 --> 00:17:21,600
trivial though and on top of that

00:17:20,240 --> 00:17:22,880
there's a lot of shared functionality

00:17:21,600 --> 00:17:24,559
between how the create method should

00:17:22,880 --> 00:17:26,240
work across all resource types

00:17:24,559 --> 00:17:27,839
but there are also some differences to

00:17:26,240 --> 00:17:29,679
take away some of this complexity

00:17:27,839 --> 00:17:31,600
there's a built-in implementation called

00:17:29,679 --> 00:17:34,320
a store that in addition to

00:17:31,600 --> 00:17:34,960
holding the actual ncd client to fire

00:17:34,320 --> 00:17:37,840
off ncd

00:17:34,960 --> 00:17:39,280
transactions takes in a strategy for

00:17:37,840 --> 00:17:40,880
each type of rest action

00:17:39,280 --> 00:17:42,960
that is specific to the resource being

00:17:40,880 --> 00:17:44,160
operated on on the right hand side we

00:17:42,960 --> 00:17:46,960
see how the store implements

00:17:44,160 --> 00:17:48,480
create utilizing the create strategy

00:17:46,960 --> 00:17:49,440
prior to actually executing the

00:17:48,480 --> 00:17:51,039
transaction

00:17:49,440 --> 00:17:53,280
thus no matter what kind of object you

00:17:51,039 --> 00:17:55,120
are creating it runs this create method

00:17:53,280 --> 00:17:56,880
that really only differs by the create

00:17:55,120 --> 00:17:57,600
strategy it uses for the specific object

00:17:56,880 --> 00:17:59,600
type

00:17:57,600 --> 00:18:01,280
it then generates the ncd key before

00:17:59,600 --> 00:18:02,880
finally writing dead cd and calling

00:18:01,280 --> 00:18:04,480
after create on the object that has just

00:18:02,880 --> 00:18:06,240
been persisted

00:18:04,480 --> 00:18:08,400
before looking at the xcd3 storage

00:18:06,240 --> 00:18:09,760
client that executes the transaction

00:18:08,400 --> 00:18:12,840
we'll take a brief look at the create

00:18:09,760 --> 00:18:15,200
strategy specific to the replica set

00:18:12,840 --> 00:18:15,919
resource in general a rest create

00:18:15,200 --> 00:18:17,280
strategy

00:18:15,919 --> 00:18:18,960
must implement the methods you see on

00:18:17,280 --> 00:18:20,160
the left these are slightly different

00:18:18,960 --> 00:18:22,000
for each resource type

00:18:20,160 --> 00:18:23,919
but as you can see in the unedited

00:18:22,000 --> 00:18:24,480
prepare for create and validate methods

00:18:23,919 --> 00:18:26,240
on the right

00:18:24,480 --> 00:18:28,240
the specific strategy for a resource is

00:18:26,240 --> 00:18:28,960
pretty trivial as it should be in order

00:18:28,240 --> 00:18:30,480
to support

00:18:28,960 --> 00:18:32,240
the many different resource types that

00:18:30,480 --> 00:18:33,600
kubernetes has

00:18:32,240 --> 00:18:37,039
and the bulk of the work is done on the

00:18:33,600 --> 00:18:37,039
store that we just looked at previously

00:18:37,440 --> 00:18:41,200
one aspect that we glossed over when

00:18:39,440 --> 00:18:43,440
going through api server setup

00:18:41,200 --> 00:18:44,960
was how the actual replica set strategy

00:18:43,440 --> 00:18:47,120
is installed on the cluster

00:18:44,960 --> 00:18:49,280
taking a brief step back during api

00:18:47,120 --> 00:18:49,840
server setup the generic api server is

00:18:49,280 --> 00:18:51,280
wrapped

00:18:49,840 --> 00:18:52,960
in something called control plane

00:18:51,280 --> 00:18:54,880
instance this instance

00:18:52,960 --> 00:18:56,160
has an install api's method on it that

00:18:54,880 --> 00:18:58,080
confusingly is different

00:18:56,160 --> 00:19:01,039
than the install api method on the api

00:18:58,080 --> 00:19:03,280
installer to set up routing and dispatch

00:19:01,039 --> 00:19:05,440
this method wires together all the rest

00:19:03,280 --> 00:19:07,520
storage providers for every api group

00:19:05,440 --> 00:19:08,880
such as auto scaling batch and of course

00:19:07,520 --> 00:19:10,880
the apps group which is where the

00:19:08,880 --> 00:19:12,320
replica set storage provider lives

00:19:10,880 --> 00:19:14,400
this is where we configure the storage

00:19:12,320 --> 00:19:17,600
options and set the replica set strategy

00:19:14,400 --> 00:19:19,280
that we took a look at on the last slide

00:19:17,600 --> 00:19:21,440
finally the very last piece of code

00:19:19,280 --> 00:19:23,840
we'll look at is how the store uses the

00:19:21,440 --> 00:19:25,360
actual ncd3 client to execute the entity

00:19:23,840 --> 00:19:27,520
request

00:19:25,360 --> 00:19:29,120
once again we use a codec from the api

00:19:27,520 --> 00:19:31,679
machinery runtime package

00:19:29,120 --> 00:19:33,679
that is embedded into the encode call

00:19:31,679 --> 00:19:35,760
while an api server can handle requests

00:19:33,679 --> 00:19:37,200
for many different versions it writes

00:19:35,760 --> 00:19:39,200
all objects to storage

00:19:37,200 --> 00:19:42,080
as a single version this enables more

00:19:39,200 --> 00:19:44,320
control over storage format upgrades

00:19:42,080 --> 00:19:45,200
and lets you roll back or roll forward

00:19:44,320 --> 00:19:47,120
versions

00:19:45,200 --> 00:19:49,039
here the codec does that conversion

00:19:47,120 --> 00:19:50,080
before finally executing and committing

00:19:49,039 --> 00:19:53,360
the transaction

00:19:50,080 --> 00:19:53,760
and surfacing up any errors and that's

00:19:53,360 --> 00:19:56,160
it

00:19:53,760 --> 00:19:57,760
i hope you've gotten a better picture of

00:19:56,160 --> 00:19:59,520
now what happens when you make a

00:19:57,760 --> 00:20:00,960
kubernetes api request

00:19:59,520 --> 00:20:02,640
starting with the invocation of a cube

00:20:00,960 --> 00:20:03,440
control command and it's building an

00:20:02,640 --> 00:20:05,679
execution

00:20:03,440 --> 00:20:06,960
of an http request that gets sent to the

00:20:05,679 --> 00:20:08,679
cube api server

00:20:06,960 --> 00:20:10,080
handled and turned into an object

00:20:08,679 --> 00:20:21,840
persistent.cd

00:20:10,080 --> 00:20:21,840
thanks for taking the time to watch

00:20:51,679 --> 00:20:56,640
thanks kevin the api server is critical

00:20:54,720 --> 00:20:57,679
to everything we do with kubernetes so

00:20:56,640 --> 00:20:59,840
getting a stronger

00:20:57,679 --> 00:21:02,640
and how it works is sure no matter how

00:20:59,840 --> 00:21:02,640
you work with

00:21:05,040 --> 00:21:08,140
[Music]

00:21:08,799 --> 00:21:11,840
make sure you get those questions

00:21:10,000 --> 00:21:13,360
answered

00:21:11,840 --> 00:21:15,280
and to make things a little more

00:21:13,360 --> 00:21:17,039
interesting and interactive

00:21:15,280 --> 00:21:18,640
we'd like to hear from you we're going

00:21:17,039 --> 00:21:19,120
to be putting up a poll for you to

00:21:18,640 --> 00:21:21,360
answer

00:21:19,120 --> 00:21:24,720
so be sure to check it out the question

00:21:21,360 --> 00:21:27,840
is how many cube cons have you been to

00:21:24,720 --> 00:21:31,840
zero one two or three

00:21:27,840 --> 00:21:34,480
or four plus

00:21:31,840 --> 00:21:36,080
next we'll hear from cc huang about web

00:21:34,480 --> 00:21:45,840
hooks and kubernetes

00:21:36,080 --> 00:21:45,840
so take it away

00:22:42,400 --> 00:22:45,679
hi everyone thanks for coming to google

00:22:44,559 --> 00:22:47,840
open source

00:22:45,679 --> 00:22:50,240
i'm this huang and i'm currently working

00:22:47,840 --> 00:22:52,799
at google as a software engineer

00:22:50,240 --> 00:22:53,760
today the topic is admission web hooks

00:22:52,799 --> 00:22:57,600
put the power

00:22:53,760 --> 00:22:59,520
in your hands first let's take a look at

00:22:57,600 --> 00:23:00,960
today's agenda

00:22:59,520 --> 00:23:02,559
we are going to talk about what are

00:23:00,960 --> 00:23:05,120
admission web hooks

00:23:02,559 --> 00:23:06,320
why people need admission web hooks and

00:23:05,120 --> 00:23:08,880
how to config

00:23:06,320 --> 00:23:10,799
a custom admission web hook some best

00:23:08,880 --> 00:23:12,559
practices when you think about your use

00:23:10,799 --> 00:23:14,799
case and configurations

00:23:12,559 --> 00:23:16,799
and then the way to monitor your

00:23:14,799 --> 00:23:19,919
admission web hooks behavior

00:23:16,799 --> 00:23:20,640
and at last we will have a short demo so

00:23:19,919 --> 00:23:22,720
let's first

00:23:20,640 --> 00:23:25,039
take a look at what are animation web

00:23:22,720 --> 00:23:25,039
hooks

00:23:25,679 --> 00:23:29,360
here is an overview of the api request

00:23:28,080 --> 00:23:31,440
lifecycle

00:23:29,360 --> 00:23:33,039
to understand the mission web hooks

00:23:31,440 --> 00:23:34,840
first we need to understand

00:23:33,039 --> 00:23:36,159
what is the admission procedure in

00:23:34,840 --> 00:23:38,640
kubernetes

00:23:36,159 --> 00:23:39,440
an admission controller is a piece of

00:23:38,640 --> 00:23:41,520
code that

00:23:39,440 --> 00:23:42,640
intercepts requests to the kubernetes

00:23:41,520 --> 00:23:45,120
api server

00:23:42,640 --> 00:23:46,240
prior to persistence of the object but

00:23:45,120 --> 00:23:49,279
after the request

00:23:46,240 --> 00:23:51,760
is authenticated and authorized

00:23:49,279 --> 00:23:52,559
kubernetes ships with a lot of admission

00:23:51,760 --> 00:23:55,120
controllers

00:23:52,559 --> 00:23:55,760
and there are 17 of them enabled by

00:23:55,120 --> 00:23:59,279
default

00:23:55,760 --> 00:24:01,919
in wii 1.19 a lot of things

00:23:59,279 --> 00:24:03,120
which we assume are native to kubernetes

00:24:01,919 --> 00:24:06,400
are actually done

00:24:03,120 --> 00:24:07,840
by those animation controllers and

00:24:06,400 --> 00:24:09,760
they can be turned on with the

00:24:07,840 --> 00:24:13,279
kubernetes api server flag

00:24:09,760 --> 00:24:15,360
enabled admission plugins

00:24:13,279 --> 00:24:17,360
let's take a look at some admission

00:24:15,360 --> 00:24:19,840
controllers as examples which are shaped

00:24:17,360 --> 00:24:19,840
with kubernetes

00:24:20,000 --> 00:24:25,600
here is a namespace lifecycle controller

00:24:23,279 --> 00:24:26,960
and when the request come in try to

00:24:25,600 --> 00:24:29,360
create a deployment

00:24:26,960 --> 00:24:30,320
with the name my deploy in the namespace

00:24:29,360 --> 00:24:33,279
test

00:24:30,320 --> 00:24:35,279
and if the test namespace currently in

00:24:33,279 --> 00:24:37,039
on terminating status

00:24:35,279 --> 00:24:39,760
there's no point to create a new

00:24:37,039 --> 00:24:41,360
deployment so the namespace lifecycle

00:24:39,760 --> 00:24:44,159
controller will simply just

00:24:41,360 --> 00:24:46,080
fail the request and return to the user

00:24:44,159 --> 00:24:48,240
another example will be the obvious pro

00:24:46,080 --> 00:24:50,640
images controller

00:24:48,240 --> 00:24:51,440
we will check the image pull policy and

00:24:50,640 --> 00:24:55,440
update it

00:24:51,440 --> 00:24:55,440
to be always if it was not

00:24:55,520 --> 00:24:59,520
as we can see here the admission

00:24:57,520 --> 00:25:00,240
controllers can be categorized into two

00:24:59,520 --> 00:25:02,080
types

00:25:00,240 --> 00:25:04,159
mutated animation controllers and

00:25:02,080 --> 00:25:06,480
validating animation controllers

00:25:04,159 --> 00:25:07,279
and some may be both mutating

00:25:06,480 --> 00:25:09,840
controllers

00:25:07,279 --> 00:25:12,080
may modify the objects they admit but

00:25:09,840 --> 00:25:14,960
the dating controllers may not

00:25:12,080 --> 00:25:16,400
the mission controller process proceeds

00:25:14,960 --> 00:25:18,559
in two phases

00:25:16,400 --> 00:25:19,679
in the first phase mutated animation

00:25:18,559 --> 00:25:21,760
controller run

00:25:19,679 --> 00:25:23,919
and in the second phase validating

00:25:21,760 --> 00:25:25,919
animation controllers are run

00:25:23,919 --> 00:25:28,480
if any of the controllers in either

00:25:25,919 --> 00:25:29,120
phase reject the request the entire

00:25:28,480 --> 00:25:31,760
request

00:25:29,120 --> 00:25:32,320
is rejected immediately and an error is

00:25:31,760 --> 00:25:36,400
returned

00:25:32,320 --> 00:25:37,360
to the end user among those admission

00:25:36,400 --> 00:25:39,919
controllers

00:25:37,360 --> 00:25:40,880
shipped with kubernetes two take a

00:25:39,919 --> 00:25:42,720
special role

00:25:40,880 --> 00:25:44,320
because of their nearly limitless

00:25:42,720 --> 00:25:46,080
flexibility

00:25:44,320 --> 00:25:48,400
mutating animation web hooks and

00:25:46,080 --> 00:25:50,240
validating animation map hooks

00:25:48,400 --> 00:25:51,919
they do not implement any policy

00:25:50,240 --> 00:25:54,320
decision logic themselves

00:25:51,919 --> 00:25:56,159
instead the respective action is

00:25:54,320 --> 00:25:57,840
obtained from a rest endpoint

00:25:56,159 --> 00:26:00,159
of a service running inside of the

00:25:57,840 --> 00:26:02,480
cluster this approach

00:26:00,159 --> 00:26:04,640
decouples the admission controller logic

00:26:02,480 --> 00:26:06,960
from the kubernetes api server

00:26:04,640 --> 00:26:07,919
thus allowing users to implement

00:26:06,960 --> 00:26:10,480
customer logic

00:26:07,919 --> 00:26:11,200
to be executed whenever resources are

00:26:10,480 --> 00:26:13,600
created

00:26:11,200 --> 00:26:15,039
updated or deleted in a kubernetes

00:26:13,600 --> 00:26:17,039
cluster

00:26:15,039 --> 00:26:18,720
the difference between the two kinds of

00:26:17,039 --> 00:26:21,760
admission controller web hooks

00:26:18,720 --> 00:26:24,960
is pretty much self-explanatory

00:26:21,760 --> 00:26:27,039
multi-animation web hooks may mutate

00:26:24,960 --> 00:26:28,960
the objects while validating animation

00:26:27,039 --> 00:26:30,960
map hooks may not

00:26:28,960 --> 00:26:32,000
and the multi-animation map hooks are

00:26:30,960 --> 00:26:33,840
called in serial

00:26:32,000 --> 00:26:36,400
while when the data animation map hooks

00:26:33,840 --> 00:26:40,640
are called in parallel

00:26:36,400 --> 00:26:40,640
so why do we need admission map hooks

00:26:40,799 --> 00:26:46,960
the the admission web hooks um

00:26:43,919 --> 00:26:48,080
can help in certain areas so they can

00:26:46,960 --> 00:26:50,400
help

00:26:48,080 --> 00:26:52,000
they can help with increasing security

00:26:50,400 --> 00:26:53,840
by meditating a reasonable

00:26:52,000 --> 00:26:55,279
security baseline across the entire

00:26:53,840 --> 00:26:57,919
namespace or cluster

00:26:55,279 --> 00:26:58,480
for example we can only allow pulling

00:26:57,919 --> 00:27:01,840
image

00:26:58,480 --> 00:27:03,919
from 30 registry inside your corporation

00:27:01,840 --> 00:27:05,679
and we can disallow containers from

00:27:03,919 --> 00:27:08,720
running as root

00:27:05,679 --> 00:27:09,360
and secondly they can help to enforce

00:27:08,720 --> 00:27:12,000
the

00:27:09,360 --> 00:27:13,200
adherence to certain practices such as

00:27:12,000 --> 00:27:15,760
having good labels

00:27:13,200 --> 00:27:16,799
annotations resource limits or other

00:27:15,760 --> 00:27:18,640
settings

00:27:16,799 --> 00:27:21,200
like we can enforce some label

00:27:18,640 --> 00:27:22,880
validation or add annotations to objects

00:27:21,200 --> 00:27:26,480
like at a cost center

00:27:22,880 --> 00:27:28,080
cost center and third

00:27:26,480 --> 00:27:30,000
they can help with configuration

00:27:28,080 --> 00:27:32,000
management for example

00:27:30,000 --> 00:27:34,559
we can add resources limits or

00:27:32,000 --> 00:27:36,799
validating resources limits

00:27:34,559 --> 00:27:38,799
and also we can ensure image references

00:27:36,799 --> 00:27:42,640
used in production deployments

00:27:38,799 --> 00:27:42,640
are not using the latest tags

00:27:44,960 --> 00:27:51,520
then how can we configure our admission

00:27:48,960 --> 00:27:51,520
webhooks

00:27:52,000 --> 00:27:55,279
before we take a deep look at the

00:27:53,760 --> 00:27:58,159
configuration fields

00:27:55,279 --> 00:28:00,080
there are some prerequisites but better

00:27:58,159 --> 00:28:01,440
keep in mind while using the automation

00:28:00,080 --> 00:28:02,880
map hooks

00:28:01,440 --> 00:28:05,039
first we always have to check the

00:28:02,880 --> 00:28:05,520
community cluster versions it has to be

00:28:05,039 --> 00:28:09,679
at least

00:28:05,520 --> 00:28:12,080
we 1.16 for using admission registration

00:28:09,679 --> 00:28:15,120
dot kubernetes dot io slash v1

00:28:12,080 --> 00:28:16,960
and it should be at least we 1.9

00:28:15,120 --> 00:28:18,799
to use the admission registration dot

00:28:16,960 --> 00:28:22,159
communities dot io

00:28:18,799 --> 00:28:24,080
slash v1 beta 1 and we also have to

00:28:22,159 --> 00:28:26,480
ensure that ability and mission web hook

00:28:24,080 --> 00:28:29,120
and validated mission web hook admin

00:28:26,480 --> 00:28:30,000
admission controllers are enabled by the

00:28:29,120 --> 00:28:33,440
flag we talked

00:28:30,000 --> 00:28:37,120
earlier and also we should ensure

00:28:33,440 --> 00:28:37,120
that the api is enabled

00:28:38,320 --> 00:28:42,080
here is an example of a validating

00:28:40,480 --> 00:28:43,919
webhook configuration

00:28:42,080 --> 00:28:45,760
i will not go through all the fields in

00:28:43,919 --> 00:28:48,480
the detail basically

00:28:45,760 --> 00:28:50,559
it's a validation webhook configuration

00:28:48,480 --> 00:28:54,720
it only contains one webhook

00:28:50,559 --> 00:28:57,279
with the name pod policy.example.com

00:28:54,720 --> 00:29:00,320
with the rule triggered on every

00:28:57,279 --> 00:29:02,640
namespace port creation request

00:29:00,320 --> 00:29:05,039
the client config tells api server how

00:29:02,640 --> 00:29:07,520
to contact the webhook

00:29:05,039 --> 00:29:08,159
and this webhook does not have any side

00:29:07,520 --> 00:29:10,880
effects

00:29:08,159 --> 00:29:14,320
and it has 5 seconds timeout for the api

00:29:10,880 --> 00:29:16,880
server to call the webhook

00:29:14,320 --> 00:29:17,679
there are some other optional fields you

00:29:16,880 --> 00:29:19,840
can set

00:29:17,679 --> 00:29:21,679
which does not show here we will talk

00:29:19,840 --> 00:29:24,240
about them in more details in the

00:29:21,679 --> 00:29:26,720
following slides

00:29:24,240 --> 00:29:27,840
and there are always some best practices

00:29:26,720 --> 00:29:30,399
recommended

00:29:27,840 --> 00:29:32,399
in terms of thinking about your use case

00:29:30,399 --> 00:29:34,960
or think about how to configure your own

00:29:32,399 --> 00:29:34,960
web hook

00:29:35,120 --> 00:29:41,760
the first one is adam potence

00:29:38,720 --> 00:29:44,559
what is adam importance and importance

00:29:41,760 --> 00:29:45,360
is like it um after applied multiple

00:29:44,559 --> 00:29:48,480
times

00:29:45,360 --> 00:29:48,799
the state the status the result and all

00:29:48,480 --> 00:29:50,880
the

00:29:48,799 --> 00:29:54,000
and the side effects should be exactly

00:29:50,880 --> 00:29:56,240
the same as being applied once

00:29:54,000 --> 00:29:58,799
so here is an example of an important

00:29:56,240 --> 00:30:03,279
mutating animation map hooks

00:29:58,799 --> 00:30:05,039
always pull images so whenever like

00:30:03,279 --> 00:30:07,120
no matter how many times it's been

00:30:05,039 --> 00:30:08,320
called it will always check the image

00:30:07,120 --> 00:30:12,960
pro policy field

00:30:08,320 --> 00:30:16,080
and update it to be always if it was not

00:30:12,960 --> 00:30:17,039
another example shows a non-item potence

00:30:16,080 --> 00:30:20,159
mutating

00:30:17,039 --> 00:30:22,880
mission web hooks which is try to inject

00:30:20,159 --> 00:30:23,919
the sidecar with the timestamp in the

00:30:22,880 --> 00:30:26,399
name

00:30:23,919 --> 00:30:27,039
so when it's being called at the first

00:30:26,399 --> 00:30:29,679
time

00:30:27,039 --> 00:30:30,480
it will inject it will append a sidecar

00:30:29,679 --> 00:30:32,880
container

00:30:30,480 --> 00:30:33,919
in your container list with the current

00:30:32,880 --> 00:30:36,080
timestamp

00:30:33,919 --> 00:30:37,200
in the name but when it's been called

00:30:36,080 --> 00:30:39,760
the second time

00:30:37,200 --> 00:30:41,360
it will append another side container

00:30:39,760 --> 00:30:44,799
into your containers list

00:30:41,360 --> 00:30:48,799
with the timestamp which might

00:30:44,799 --> 00:30:48,799
not be something the users wanted

00:30:49,120 --> 00:30:55,360
so why we want the mutating web hook to

00:30:52,880 --> 00:30:58,159
be item potent

00:30:55,360 --> 00:31:01,519
because mutating web hook may be called

00:30:58,159 --> 00:31:03,919
trials by the api server

00:31:01,519 --> 00:31:04,799
a single ordering of multi-animation

00:31:03,919 --> 00:31:07,679
plugins

00:31:04,799 --> 00:31:08,399
just does not work for all cases because

00:31:07,679 --> 00:31:10,320
some other

00:31:08,399 --> 00:31:13,120
animation controllers may mutate the

00:31:10,320 --> 00:31:15,519
object and the decision may be different

00:31:13,120 --> 00:31:17,600
depending on the order let's take a look

00:31:15,519 --> 00:31:20,159
at an example

00:31:17,600 --> 00:31:21,200
here in the example we try to inject the

00:31:20,159 --> 00:31:24,000
side car

00:31:21,200 --> 00:31:25,200
and when this controller being called a

00:31:24,000 --> 00:31:29,279
sidecar container

00:31:25,200 --> 00:31:32,559
being appended into the port

00:31:29,279 --> 00:31:35,360
and then all these poor images being

00:31:32,559 --> 00:31:37,519
called so we will go ahead and check the

00:31:35,360 --> 00:31:38,399
image pro policy to all the containers

00:31:37,519 --> 00:31:42,000
and updated

00:31:38,399 --> 00:31:44,480
them to be obvious but what if the order

00:31:42,000 --> 00:31:47,200
is different

00:31:44,480 --> 00:31:49,440
if we first call the always pop images

00:31:47,200 --> 00:31:52,159
which will update our image pro policy

00:31:49,440 --> 00:31:52,159
to be always

00:31:52,799 --> 00:31:58,960
and then the inject sidecar being called

00:31:56,320 --> 00:31:59,440
so it will try to append a sidecar with

00:31:58,960 --> 00:32:02,080
the name

00:31:59,440 --> 00:32:03,760
sidecar in the containers list but the

00:32:02,080 --> 00:32:06,240
image pool policy

00:32:03,760 --> 00:32:09,120
is not said to be obvious which against

00:32:06,240 --> 00:32:09,120
our expectation

00:32:10,720 --> 00:32:14,320
so we try to apply the best effort

00:32:13,120 --> 00:32:17,600
reinvocation

00:32:14,320 --> 00:32:20,320
mechanism by setting by allowing

00:32:17,600 --> 00:32:22,080
to set the relocation policy field to be

00:32:20,320 --> 00:32:24,960
if needed

00:32:22,080 --> 00:32:27,440
so in the previous case we app the

00:32:24,960 --> 00:32:30,240
injector sidecar being called

00:32:27,440 --> 00:32:31,679
and they ingested a sidecar container

00:32:30,240 --> 00:32:34,880
into the container list

00:32:31,679 --> 00:32:37,039
and then the always port images

00:32:34,880 --> 00:32:39,120
is allowed to be called trials and

00:32:37,039 --> 00:32:39,760
updated the image pro policy to be

00:32:39,120 --> 00:32:43,840
always

00:32:39,760 --> 00:32:46,960
which meets the expectation

00:32:43,840 --> 00:32:48,799
and the second best practice is

00:32:46,960 --> 00:32:52,320
intercepting all versions

00:32:48,799 --> 00:32:54,399
of an object we all know that kubernetes

00:32:52,320 --> 00:32:57,919
support multiple api versions

00:32:54,399 --> 00:33:00,159
for example qbpi server allows creating

00:32:57,919 --> 00:33:01,760
updating deployments while extension

00:33:00,159 --> 00:33:04,480
slash v1 beta1

00:33:01,760 --> 00:33:07,200
apps slash weapon beta1 apps slash women

00:33:04,480 --> 00:33:09,760
beta 2 and app slash rewind apis

00:33:07,200 --> 00:33:11,360
if a webhook is configured to only

00:33:09,760 --> 00:33:15,519
intercept the deployment's

00:33:11,360 --> 00:33:18,480
request in apps v1 it will miss requests

00:33:15,519 --> 00:33:19,919
in the other versions it is recommended

00:33:18,480 --> 00:33:22,159
that admission webhooks

00:33:19,919 --> 00:33:23,120
should always intercept all versions of

00:33:22,159 --> 00:33:25,760
an object

00:33:23,120 --> 00:33:27,360
by setting the match policy field to be

00:33:25,760 --> 00:33:29,600
equivalent

00:33:27,360 --> 00:33:31,200
it is also recommended that admission

00:33:29,600 --> 00:33:33,279
web hooks should prefer

00:33:31,200 --> 00:33:34,559
registering for stable versions of

00:33:33,279 --> 00:33:36,559
resources

00:33:34,559 --> 00:33:38,000
and failure to intercept all versions of

00:33:36,559 --> 00:33:40,399
an object can result

00:33:38,000 --> 00:33:41,279
in admission policies not being forced

00:33:40,399 --> 00:33:44,720
for requests

00:33:41,279 --> 00:33:47,840
in 1330 versions

00:33:44,720 --> 00:33:50,320
the next best practice is availability

00:33:47,840 --> 00:33:51,440
the time calling webhook builds up time

00:33:50,320 --> 00:33:54,559
calculating the

00:33:51,440 --> 00:33:55,600
entire api request so it's always

00:33:54,559 --> 00:33:57,600
recommended that

00:33:55,600 --> 00:33:59,039
animation webhooks should evaluate as

00:33:57,600 --> 00:34:01,519
quickly as possible

00:33:59,039 --> 00:34:03,120
and is encouraged to use a small timeout

00:34:01,519 --> 00:34:06,240
for webhooks

00:34:03,120 --> 00:34:07,039
in this example the validating animation

00:34:06,240 --> 00:34:10,000
web hook

00:34:07,039 --> 00:34:12,480
sets a two seconds timeout if the web

00:34:10,000 --> 00:34:15,119
hook is being slow and doesn't respond

00:34:12,480 --> 00:34:16,000
in two seconds the api server will

00:34:15,119 --> 00:34:18,480
either reject

00:34:16,000 --> 00:34:21,839
the request or ignore the failure based

00:34:18,480 --> 00:34:24,240
on the failure policy set

00:34:21,839 --> 00:34:25,919
the next next best practice is

00:34:24,240 --> 00:34:29,280
guaranteeing the final state

00:34:25,919 --> 00:34:31,760
of the object as seen

00:34:29,280 --> 00:34:33,200
if you use a mutating webhook to enforce

00:34:31,760 --> 00:34:35,119
security policy

00:34:33,200 --> 00:34:37,679
make sure to use a validating web hook

00:34:35,119 --> 00:34:40,639
to ensure that because another web hook

00:34:37,679 --> 00:34:43,359
may be invoked after your admission web

00:34:40,639 --> 00:34:43,359
hookup in court

00:34:43,440 --> 00:34:49,119
and then the next best practice is

00:34:46,480 --> 00:34:51,679
avoiding deadlocks in self-hosted

00:34:49,119 --> 00:34:51,679
web hooks

00:34:53,599 --> 00:34:59,280
so a web hook running inside the cluster

00:34:57,040 --> 00:35:00,320
might causes deadlocks for its own

00:34:59,280 --> 00:35:02,079
deployment

00:35:00,320 --> 00:35:04,000
if it is configured to intercept the

00:35:02,079 --> 00:35:04,560
resources required to start its own

00:35:04,000 --> 00:35:06,320
course

00:35:04,560 --> 00:35:07,839
let's take a deep breath let's take an

00:35:06,320 --> 00:35:09,680
example

00:35:07,839 --> 00:35:11,280
a multiple admission map hook is

00:35:09,680 --> 00:35:13,760
configured to admit

00:35:11,280 --> 00:35:15,359
create port request only if a certain

00:35:13,760 --> 00:35:17,599
label is set in the port

00:35:15,359 --> 00:35:20,000
in this case we require the environment

00:35:17,599 --> 00:35:22,880
label to be set to production

00:35:20,000 --> 00:35:24,720
the webhoot server runs in a deployment

00:35:22,880 --> 00:35:26,160
which doesn't set the environment label

00:35:24,720 --> 00:35:28,640
to be production

00:35:26,160 --> 00:35:29,599
and then a node that runs the webhook

00:35:28,640 --> 00:35:32,480
server ports

00:35:29,599 --> 00:35:34,320
become unhealthy the webhook deployment

00:35:32,480 --> 00:35:35,680
will try to reschedule the ports to

00:35:34,320 --> 00:35:38,640
another node

00:35:35,680 --> 00:35:40,400
however the request will get rejected by

00:35:38,640 --> 00:35:42,400
the existing webhook server

00:35:40,400 --> 00:35:44,640
since the environment label is not set

00:35:42,400 --> 00:35:46,960
correctly and the migration cannot

00:35:44,640 --> 00:35:46,960
happen

00:35:47,920 --> 00:35:51,359
so we recommended to exclude the

00:35:50,560 --> 00:35:53,599
namespace

00:35:51,359 --> 00:35:54,720
variable webhook is running by setting

00:35:53,599 --> 00:35:58,560
the namespace

00:35:54,720 --> 00:36:01,119
selector fields

00:35:58,560 --> 00:36:02,400
and another best practice is side

00:36:01,119 --> 00:36:04,480
effects

00:36:02,400 --> 00:36:07,359
always try to avoid side effects if

00:36:04,480 --> 00:36:09,760
possible and if it cannot be avoided

00:36:07,359 --> 00:36:10,720
don't trigger the side effect in dry run

00:36:09,760 --> 00:36:14,000
i said the

00:36:10,720 --> 00:36:16,640
by set the side effects field to be

00:36:14,000 --> 00:36:19,280
known on dry run

00:36:16,640 --> 00:36:22,160
and the last best practice is avoiding

00:36:19,280 --> 00:36:24,320
operating on the cube system namespace

00:36:22,160 --> 00:36:27,599
the cubesystem namespace contains

00:36:24,320 --> 00:36:30,079
objects created by the kubernetes system

00:36:27,599 --> 00:36:31,280
like service or cons for on control

00:36:30,079 --> 00:36:34,240
plane components

00:36:31,280 --> 00:36:36,160
ports like cube dns accidentally

00:36:34,240 --> 00:36:38,160
mutating or rejecting requests

00:36:36,160 --> 00:36:39,920
in the equip system namespace may cause

00:36:38,160 --> 00:36:40,720
the control plane components to stop

00:36:39,920 --> 00:36:43,520
functioning

00:36:40,720 --> 00:36:44,800
or introduce anal behavior if your

00:36:43,520 --> 00:36:46,800
admission web hooks

00:36:44,800 --> 00:36:48,880
don't intend to modify the behavior of

00:36:46,800 --> 00:36:51,119
the kubernetes control plan

00:36:48,880 --> 00:36:51,920
exclude the cube system namespace from

00:36:51,119 --> 00:36:54,400
being

00:36:51,920 --> 00:36:55,839
intercepted using a namespace selector

00:36:54,400 --> 00:36:58,480
field

00:36:55,839 --> 00:36:59,280
here is an example only match the

00:36:58,480 --> 00:37:01,599
request

00:36:59,280 --> 00:37:02,560
in a namespace which the rule level

00:37:01,599 --> 00:37:05,599
value is not

00:37:02,560 --> 00:37:08,160
equal to 0 or 1.

00:37:05,599 --> 00:37:10,160
it also helps with efficiency since we

00:37:08,160 --> 00:37:13,040
mentioned earlier that every webhook

00:37:10,160 --> 00:37:14,960
invocation latency builds up the entire

00:37:13,040 --> 00:37:17,359
api request latency

00:37:14,960 --> 00:37:18,800
by excluding the keep system namespace

00:37:17,359 --> 00:37:23,760
the webhook invocation

00:37:18,800 --> 00:37:26,400
is scoped to only what it needed

00:37:23,760 --> 00:37:29,760
next let's talk about the ways to

00:37:26,400 --> 00:37:33,200
monitor the mission webhook behavior

00:37:29,760 --> 00:37:35,520
there are some questions the answers to

00:37:33,200 --> 00:37:37,200
certain questions can definitely help us

00:37:35,520 --> 00:37:38,240
to monitor the admission webhook

00:37:37,200 --> 00:37:40,640
behavior

00:37:38,240 --> 00:37:42,320
like which mutating webhook mutated the

00:37:40,640 --> 00:37:44,079
objecting api request

00:37:42,320 --> 00:37:45,359
what change did the mutating webhook

00:37:44,079 --> 00:37:46,960
applied to the object

00:37:45,359 --> 00:37:48,400
or which objects are frequently

00:37:46,960 --> 00:37:52,400
rejecting api requests

00:37:48,400 --> 00:37:52,400
and what's the reason for rejection

00:37:53,040 --> 00:37:59,440
the api server provides two ways

00:37:56,960 --> 00:38:01,280
two main ways to monitor the first is

00:37:59,440 --> 00:38:03,920
through auditing

00:38:01,280 --> 00:38:06,320
cube api server performs auditing on

00:38:03,920 --> 00:38:08,800
each mutating web hooking vocation

00:38:06,320 --> 00:38:10,400
each invocation generates an auditing

00:38:08,800 --> 00:38:13,040
annotation capturing

00:38:10,400 --> 00:38:14,079
if the request object is mutated by the

00:38:13,040 --> 00:38:16,720
invocation

00:38:14,079 --> 00:38:18,079
and optionally generates an annotation

00:38:16,720 --> 00:38:19,839
capturing the applied

00:38:18,079 --> 00:38:21,599
patch from the webhook admission

00:38:19,839 --> 00:38:24,720
response

00:38:21,599 --> 00:38:29,599
for the audit level metadata

00:38:24,720 --> 00:38:29,599
or higher an annotation with the key

00:38:30,440 --> 00:38:33,440
mutation.webhook.animation.communities.io

00:38:31,440 --> 00:38:36,560
slash round with round index

00:38:33,440 --> 00:38:38,240
index with order index gets locked with

00:38:36,560 --> 00:38:40,640
json play load indicating

00:38:38,240 --> 00:38:41,599
a web hook gets invoked for giving

00:38:40,640 --> 00:38:45,599
request

00:38:41,599 --> 00:38:48,960
and whether it mutate the object or not

00:38:45,599 --> 00:38:52,000
the example on the left shows

00:38:48,960 --> 00:38:54,640
an annotation gets recorded for webhook

00:38:52,000 --> 00:38:55,280
being invoked in the first round and the

00:38:54,640 --> 00:38:57,599
webhook

00:38:55,280 --> 00:39:00,160
is ordered the first in the multi-map

00:38:57,599 --> 00:39:02,960
hook chain

00:39:00,160 --> 00:39:05,040
and mutated the request object during

00:39:02,960 --> 00:39:09,280
the invocation

00:39:05,040 --> 00:39:12,800
for the um audit level request or hire

00:39:09,280 --> 00:39:12,800
an annotation with key

00:39:12,960 --> 00:39:16,880
dot kubernetes patch.webhook.mission.com

00:39:14,000 --> 00:39:19,599
io slash round round with round index

00:39:16,880 --> 00:39:21,760
index with order index gets locked with

00:39:19,599 --> 00:39:22,079
json play load indicating a web hook

00:39:21,760 --> 00:39:24,720
gets

00:39:22,079 --> 00:39:25,599
invoked for given request and what patch

00:39:24,720 --> 00:39:28,960
gets applied

00:39:25,599 --> 00:39:32,160
to the request object if we take a look

00:39:28,960 --> 00:39:33,040
at the right example the annotation gets

00:39:32,160 --> 00:39:36,160
recorded

00:39:33,040 --> 00:39:36,960
for webhook being reinvoked the webhook

00:39:36,160 --> 00:39:39,680
is ordered

00:39:36,960 --> 00:39:40,000
the first in the multitune webhook chain

00:39:39,680 --> 00:39:43,200
and

00:39:40,000 --> 00:39:45,520
responded with a json patch which got

00:39:43,200 --> 00:39:47,839
applied to the request body a request

00:39:45,520 --> 00:39:51,760
object

00:39:47,839 --> 00:39:55,440
and we can also monitoring the webhook

00:39:51,760 --> 00:39:58,079
by the matrix endpoint there are two

00:39:55,440 --> 00:39:59,760
metrics which highly related with the

00:39:58,079 --> 00:40:03,200
animation map hooks

00:39:59,760 --> 00:40:03,920
the first is api api server admission

00:40:03,200 --> 00:40:06,640
webhook

00:40:03,920 --> 00:40:07,520
admission duration seconds which is a

00:40:06,640 --> 00:40:10,560
historic

00:40:07,520 --> 00:40:14,480
histogram matrix tells you

00:40:10,560 --> 00:40:16,640
how long the web hook automation takes

00:40:14,480 --> 00:40:18,800
and the second is the api server

00:40:16,640 --> 00:40:21,440
animation web hook rejection count

00:40:18,800 --> 00:40:24,560
which is a counter matrix gopro by name

00:40:21,440 --> 00:40:27,359
operation type error type rejection code

00:40:24,560 --> 00:40:30,640
and below are the examples of how the

00:40:27,359 --> 00:40:30,640
metrics log look like

00:40:31,680 --> 00:40:35,440
and now let's take a look at a short

00:40:34,000 --> 00:40:38,560
demo

00:40:35,440 --> 00:40:41,280
so the demo use case is fairly simple

00:40:38,560 --> 00:40:42,240
we want to have a validating animation

00:40:41,280 --> 00:40:44,480
web hook

00:40:42,240 --> 00:40:46,160
which could ensure ports run as a

00:40:44,480 --> 00:40:49,760
non-root user

00:40:46,160 --> 00:40:50,960
i have four examples of ports try to be

00:40:49,760 --> 00:40:54,240
created

00:40:50,960 --> 00:40:57,119
the first one without the um

00:40:54,240 --> 00:40:58,319
right as noun root being cited at all

00:40:57,119 --> 00:41:02,000
and the second one

00:40:58,319 --> 00:41:05,119
we set the rise now root to be fourth

00:41:02,000 --> 00:41:07,119
the third one we even we set the right

00:41:05,119 --> 00:41:07,599
null route to be true but we are giving

00:41:07,119 --> 00:41:10,720
a

00:41:07,599 --> 00:41:13,760
mean user the third one with the

00:41:10,720 --> 00:41:16,480
rest now root to be true and with a

00:41:13,760 --> 00:41:19,440
given user as one two three

00:41:16,480 --> 00:41:20,720
so the community api should reject the

00:41:19,440 --> 00:41:24,880
first three requests

00:41:20,720 --> 00:41:27,280
and only allow the last one to be passed

00:41:24,880 --> 00:41:28,800
and here is our validating webhook

00:41:27,280 --> 00:41:31,040
configuration

00:41:28,800 --> 00:41:32,800
which we have a one web hook called

00:41:31,040 --> 00:41:35,599
kubernetes dash validation dashboard

00:41:32,800 --> 00:41:40,720
poke dot

00:41:35,599 --> 00:41:44,079
and for all the post creation requests

00:41:40,720 --> 00:41:47,680
happening in the namespace which has the

00:41:44,079 --> 00:41:47,680
webhook label to be enabled

00:41:47,760 --> 00:41:53,520
and here is a error

00:41:51,119 --> 00:41:54,480
here is the api behavior returned when

00:41:53,520 --> 00:41:57,680
we try to create

00:41:54,480 --> 00:42:00,319
the ports and i will not

00:41:57,680 --> 00:42:01,520
do a live demo in this talk but i have

00:42:00,319 --> 00:42:05,680
all the demo code

00:42:01,520 --> 00:42:07,680
uploaded into the github if anyone has

00:42:05,680 --> 00:42:09,839
if anyone is interested please feel free

00:42:07,680 --> 00:42:17,839
to take a look

00:42:09,839 --> 00:42:17,839
that's the end of the talk thank you

00:42:30,880 --> 00:42:34,640
thanks cece i hope everyone sees the

00:42:33,119 --> 00:42:36,000
power that web hooks offer

00:42:34,640 --> 00:42:38,319
whether you're trying to secure your

00:42:36,000 --> 00:42:39,760
clusters or building entirely new apis

00:42:38,319 --> 00:42:41,359
on top of kubernetes

00:42:39,760 --> 00:42:42,880
web hooks are a really important part of

00:42:41,359 --> 00:42:44,800
the solution we've

00:42:42,880 --> 00:42:46,640
seen the rise of operators in the last

00:42:44,800 --> 00:42:48,480
couple of years and web hooks are really

00:42:46,640 --> 00:42:50,160
central to that idea

00:42:48,480 --> 00:42:52,000
it's been great so far building up our

00:42:50,160 --> 00:42:53,440
understanding of how the kubernetes api

00:42:52,000 --> 00:42:55,200
machinery works and how all of these

00:42:53,440 --> 00:42:57,040
things fit together

00:42:55,200 --> 00:42:58,720
when we talk about kubernetes we often

00:42:57,040 --> 00:43:00,640
hear the word declarative

00:42:58,720 --> 00:43:02,079
it's an important aspect of our system

00:43:00,640 --> 00:43:03,839
and how it's all designed

00:43:02,079 --> 00:43:05,760
one of the things we haven't yet seen is

00:43:03,839 --> 00:43:07,359
how to take advantage of the declarative

00:43:05,760 --> 00:43:09,680
nature of kubernetes

00:43:07,359 --> 00:43:11,280
next up we'll hear from antoine police

00:43:09,680 --> 00:43:13,200
about how apply

00:43:11,280 --> 00:43:14,400
works including the newest work on

00:43:13,200 --> 00:43:27,839
server side apply

00:43:14,400 --> 00:43:27,839
take it away antoine

00:44:11,359 --> 00:44:16,640
welcome my name is antoine police and

00:44:14,640 --> 00:44:18,319
i'm a software engineer at google and

00:44:16,640 --> 00:44:21,839
i'm going to talk to you today about

00:44:18,319 --> 00:44:23,760
how apply works and doesn't and how

00:44:21,839 --> 00:44:26,720
hopefully societal play will save

00:44:23,760 --> 00:44:27,119
us all so today i'm going to talk about

00:44:26,720 --> 00:44:29,200
uh

00:44:27,119 --> 00:44:31,119
declarative configuration a lot because

00:44:29,200 --> 00:44:34,079
i think it's very very important to

00:44:31,119 --> 00:44:35,200
how kubernetes works and how

00:44:34,079 --> 00:44:38,000
foundational it

00:44:35,200 --> 00:44:39,760
to its success and how it's going to be

00:44:38,000 --> 00:44:42,240
successful in the future

00:44:39,760 --> 00:44:44,000
and so after describing what declarative

00:44:42,240 --> 00:44:45,200
configuration is i'm going to talk to

00:44:44,000 --> 00:44:47,839
you about the

00:44:45,200 --> 00:44:48,319
implementation that we have in kubecon

00:44:47,839 --> 00:44:51,280
which is

00:44:48,319 --> 00:44:52,480
how we initially implemented uh the

00:44:51,280 --> 00:44:54,480
mechanism

00:44:52,480 --> 00:44:56,240
and then i'm gonna talk about some of

00:44:54,480 --> 00:44:58,560
the limitations that we found

00:44:56,240 --> 00:45:00,319
and how we try to solve them with server

00:44:58,560 --> 00:45:02,960
side apply

00:45:00,319 --> 00:45:04,880
uh so what is declarative configuration

00:45:02,960 --> 00:45:07,520
and why is it so important

00:45:04,880 --> 00:45:09,839
um i think brian grant described it very

00:45:07,520 --> 00:45:11,359
well in 2014 when he said that

00:45:09,839 --> 00:45:13,839
we want to support management of

00:45:11,359 --> 00:45:17,040
services via declarative configuration

00:45:13,839 --> 00:45:18,720
i want to show that this is foundational

00:45:17,040 --> 00:45:20,560
to kubernetes this is back

00:45:18,720 --> 00:45:22,880
in the days very at the very beginning

00:45:20,560 --> 00:45:24,880
of the project in 2014

00:45:22,880 --> 00:45:26,319
and this is key to kubernetes and how

00:45:24,880 --> 00:45:29,040
it's configured

00:45:26,319 --> 00:45:29,440
um in my opinion kubernetes is not just

00:45:29,040 --> 00:45:32,400
about

00:45:29,440 --> 00:45:34,160
configuring workloads on the cloud it's

00:45:32,400 --> 00:45:36,960
very much about how

00:45:34,160 --> 00:45:39,359
configuration works and how you decide

00:45:36,960 --> 00:45:41,839
what you want the system to look like

00:45:39,359 --> 00:45:43,280
um and so i'm going to focus on that

00:45:41,839 --> 00:45:46,400
specifically today

00:45:43,280 --> 00:45:48,400
so what's declarative configuration um

00:45:46,400 --> 00:45:51,040
we can see on the on the left we have a

00:45:48,400 --> 00:45:53,920
configuration written as yamo

00:45:51,040 --> 00:45:56,640
um and this is decorative why is it

00:45:53,920 --> 00:46:00,160
decorative it's decorative because

00:45:56,640 --> 00:46:02,240
uh we do not specify how the number of

00:46:00,160 --> 00:46:05,200
replicas for example in this

00:46:02,240 --> 00:46:05,920
example is supposed to change but we

00:46:05,200 --> 00:46:08,319
describe

00:46:05,920 --> 00:46:10,400
what the value should be so if your

00:46:08,319 --> 00:46:13,040
system has currently one per

00:46:10,400 --> 00:46:14,640
and we apply this configuration we know

00:46:13,040 --> 00:46:16,400
that we want to increase the number of

00:46:14,640 --> 00:46:18,720
parts by two but this is not

00:46:16,400 --> 00:46:20,079
how we describe it we don't describe the

00:46:18,720 --> 00:46:21,760
fact that you have to increase the

00:46:20,079 --> 00:46:23,680
number of replicas by two we describe

00:46:21,760 --> 00:46:26,800
the fact that we want to have three

00:46:23,680 --> 00:46:28,000
and this is very important um this is

00:46:26,800 --> 00:46:29,839
very important for many

00:46:28,000 --> 00:46:31,040
reasons one of them is that this is

00:46:29,839 --> 00:46:34,400
repeatable

00:46:31,040 --> 00:46:37,040
we can do the same thing again and again

00:46:34,400 --> 00:46:37,680
again without knowing about the existing

00:46:37,040 --> 00:46:40,000
context

00:46:37,680 --> 00:46:41,280
of the cluster and so this is going to

00:46:40,000 --> 00:46:44,240
apply to any cluster

00:46:41,280 --> 00:46:45,280
in the exact same way so for example if

00:46:44,240 --> 00:46:48,800
you want to recover

00:46:45,280 --> 00:46:50,880
from a polygon cluster you're going to

00:46:48,800 --> 00:46:52,400
just have to reapply that configuration

00:46:50,880 --> 00:46:55,280
that you know works well

00:46:52,400 --> 00:46:56,720
and it's going to go back to that state

00:46:55,280 --> 00:46:57,760
that you know is good

00:46:56,720 --> 00:47:00,000
uh if you want to share the

00:46:57,760 --> 00:47:01,440
configuration with someone uh they can

00:47:00,000 --> 00:47:02,960
apply the configuration and they're

00:47:01,440 --> 00:47:05,359
going to have the same state

00:47:02,960 --> 00:47:06,160
at least they should have the same state

00:47:05,359 --> 00:47:08,720
um

00:47:06,160 --> 00:47:10,000
it's very very typical for people in

00:47:08,720 --> 00:47:12,079
companies to want to create

00:47:10,000 --> 00:47:14,240
testing environments and they usually

00:47:12,079 --> 00:47:15,200
want their environments to be very

00:47:14,240 --> 00:47:16,960
similar

00:47:15,200 --> 00:47:18,079
uh for production and testing so that

00:47:16,960 --> 00:47:18,800
they know they're testing the right

00:47:18,079 --> 00:47:21,440
thing

00:47:18,800 --> 00:47:22,800
and so again repeating the production

00:47:21,440 --> 00:47:25,119
cluster into

00:47:22,800 --> 00:47:26,400
a test cluster is very very convenient

00:47:25,119 --> 00:47:28,480
and frequent

00:47:26,400 --> 00:47:29,599
um migrations obviously if you want to

00:47:28,480 --> 00:47:31,440
migrate from one

00:47:29,599 --> 00:47:33,359
cloud provider or from one cluster to

00:47:31,440 --> 00:47:34,640
another that's very convenient that this

00:47:33,359 --> 00:47:37,680
is repeatable

00:47:34,640 --> 00:47:38,960
um because this is text and because this

00:47:37,680 --> 00:47:41,119
is easy to read

00:47:38,960 --> 00:47:43,839
it's very easy to review and commit to a

00:47:41,119 --> 00:47:47,119
good repository so this is very

00:47:43,839 --> 00:47:48,960
much done for the github workflows

00:47:47,119 --> 00:47:51,599
where you push the change to get

00:47:48,960 --> 00:47:54,160
repository and then it's reviewed by uh

00:47:51,599 --> 00:47:55,040
by your peers uh it's easy to review

00:47:54,160 --> 00:47:57,440
because we don't

00:47:55,040 --> 00:47:59,440
again we don't care about the context or

00:47:57,440 --> 00:48:00,960
the current state of the cluster we only

00:47:59,440 --> 00:48:02,960
care about what you want

00:48:00,960 --> 00:48:04,160
uh you want three replicas is that a

00:48:02,960 --> 00:48:05,920
good value or not

00:48:04,160 --> 00:48:08,000
uh the person who reviews doesn't need

00:48:05,920 --> 00:48:11,280
to know anything else that

00:48:08,000 --> 00:48:12,960
is this the value uh you want and it's

00:48:11,280 --> 00:48:14,079
also easy for tools to validate for

00:48:12,960 --> 00:48:16,800
example because

00:48:14,079 --> 00:48:18,640
they can just pass this file uh look at

00:48:16,800 --> 00:48:20,480
the value of replicas and make sure that

00:48:18,640 --> 00:48:23,440
it validates a specific policy

00:48:20,480 --> 00:48:25,440
so you could have a policy that says we

00:48:23,440 --> 00:48:27,359
must have the number of replicas bigger

00:48:25,440 --> 00:48:29,280
than three and less than ten

00:48:27,359 --> 00:48:30,960
and here it's very easy to write a tool

00:48:29,280 --> 00:48:32,559
that is going to pass this channel look

00:48:30,960 --> 00:48:33,680
at the number of replicas and make sure

00:48:32,559 --> 00:48:36,319
that it

00:48:33,680 --> 00:48:38,319
validates that policy uh because it's

00:48:36,319 --> 00:48:39,040
repeatable it's also very easy to roll

00:48:38,319 --> 00:48:42,240
back

00:48:39,040 --> 00:48:43,200
um and typically provides this kind of

00:48:42,240 --> 00:48:45,119
workflow

00:48:43,200 --> 00:48:46,880
so you just have to reverse your commit

00:48:45,119 --> 00:48:48,559
go back to a state that you know works

00:48:46,880 --> 00:48:48,800
well and hopefully everything is going

00:48:48,559 --> 00:48:51,680
to

00:48:48,800 --> 00:48:52,800
go back to a good state that we know

00:48:51,680 --> 00:48:55,119
works

00:48:52,800 --> 00:48:56,480
um i'm going to emphasize specifically

00:48:55,119 --> 00:48:59,280
that this is what i call

00:48:56,480 --> 00:48:59,760
data and why do i call this data rather

00:48:59,280 --> 00:49:03,599
than

00:48:59,760 --> 00:49:06,319
uh code for example is that this is

00:49:03,599 --> 00:49:07,119
very easy to pass deserialize and

00:49:06,319 --> 00:49:10,640
serialize

00:49:07,119 --> 00:49:11,839
in any common language um if we were to

00:49:10,640 --> 00:49:14,960
use

00:49:11,839 --> 00:49:18,319
python for the configuration i

00:49:14,960 --> 00:49:20,880
claim that it would be much harder for

00:49:18,319 --> 00:49:22,319
any language to be able to pass that

00:49:20,880 --> 00:49:25,599
it's very difficult

00:49:22,319 --> 00:49:26,559
in any language and let's say javascript

00:49:25,599 --> 00:49:29,520
for example

00:49:26,559 --> 00:49:30,240
to pass python look for the replicas

00:49:29,520 --> 00:49:32,160
field

00:49:30,240 --> 00:49:34,400
see that the value is actually a

00:49:32,160 --> 00:49:35,200
statement which may be a function for

00:49:34,400 --> 00:49:36,720
example

00:49:35,200 --> 00:49:38,400
and then if you want to see what the

00:49:36,720 --> 00:49:39,359
value is you have to evaluate this

00:49:38,400 --> 00:49:42,000
statement

00:49:39,359 --> 00:49:44,480
which you can do in javascript if the

00:49:42,000 --> 00:49:46,880
statement is python there is no

00:49:44,480 --> 00:49:48,319
uh python parser written in every

00:49:46,880 --> 00:49:51,520
language that we know of

00:49:48,319 --> 00:49:53,280
but there is a parser for yaml hopefully

00:49:51,520 --> 00:49:55,200
in most languages

00:49:53,280 --> 00:49:57,280
and it's they're usually very easy to

00:49:55,200 --> 00:50:00,000
use and changing a single value is

00:49:57,280 --> 00:50:00,640
typically very easy uh so now if you

00:50:00,000 --> 00:50:02,960
want to write

00:50:00,640 --> 00:50:03,680
a tool that is going to look for this

00:50:02,960 --> 00:50:06,000
value it's

00:50:03,680 --> 00:50:07,839
it's much easier than if the if the

00:50:06,000 --> 00:50:09,520
configuration was for eternal space and

00:50:07,839 --> 00:50:10,480
or any other language should be honest

00:50:09,520 --> 00:50:14,079
typically

00:50:10,480 --> 00:50:14,079
long wages are not easy to pass

00:50:14,319 --> 00:50:17,440
this is very convenient for

00:50:15,680 --> 00:50:20,079
collaboration uh

00:50:17,440 --> 00:50:21,680
because we want people to collaborate on

00:50:20,079 --> 00:50:23,200
their configuration and we want people

00:50:21,680 --> 00:50:25,359
to collaborate with machines

00:50:23,200 --> 00:50:27,040
under configuration and so we want

00:50:25,359 --> 00:50:28,079
people and tools to be able to look at

00:50:27,040 --> 00:50:31,119
the country

00:50:28,079 --> 00:50:32,000
make changes validate update as needed

00:50:31,119 --> 00:50:34,640
based maybe

00:50:32,000 --> 00:50:35,920
on a state that would be continuously

00:50:34,640 --> 00:50:38,800
reconciled

00:50:35,920 --> 00:50:40,640
and so an example of that would be a

00:50:38,800 --> 00:50:42,079
horizontal button scada

00:50:40,640 --> 00:50:43,839
which would look at the state of your

00:50:42,079 --> 00:50:45,599
cluster and update

00:50:43,839 --> 00:50:48,400
the value of replicas based on the

00:50:45,599 --> 00:50:51,440
workload on the cluster to

00:50:48,400 --> 00:50:51,440
suit the current need

00:50:52,240 --> 00:50:56,079
there are challenges with the cloud if

00:50:53,920 --> 00:50:58,800
configuration though um

00:50:56,079 --> 00:51:01,280
there is no typically endpoint uh in

00:50:58,800 --> 00:51:03,280
crowd apis for the cloud event you can

00:51:01,280 --> 00:51:04,240
create you can read you can update you

00:51:03,280 --> 00:51:06,160
can delete

00:51:04,240 --> 00:51:08,000
but there is no way to say hey this is

00:51:06,160 --> 00:51:10,640
what i want this is what i care

00:51:08,000 --> 00:51:11,359
about just do do the right thing do what

00:51:10,640 --> 00:51:14,559
i need

00:51:11,359 --> 00:51:17,119
there is no such thing with cut apis

00:51:14,559 --> 00:51:17,760
so one example of that is that we don't

00:51:17,119 --> 00:51:19,359
know

00:51:17,760 --> 00:51:21,280
if the resource needs to be created

00:51:19,359 --> 00:51:23,839
outdated you have to look

00:51:21,280 --> 00:51:25,280
does this resource exist if it doesn't

00:51:23,839 --> 00:51:27,040
then i'm gonna create it but then you

00:51:25,280 --> 00:51:29,200
can have a race condition

00:51:27,040 --> 00:51:30,640
uh this is much more complicated than it

00:51:29,200 --> 00:51:32,960
looks um

00:51:30,640 --> 00:51:34,400
if you want to update the resource now

00:51:32,960 --> 00:51:36,400
it's difficult because

00:51:34,400 --> 00:51:37,680
typically you updating a credit api is

00:51:36,400 --> 00:51:39,920
going to replace

00:51:37,680 --> 00:51:41,920
uh the entire object and if you only

00:51:39,920 --> 00:51:43,200
care about specific fields

00:51:41,920 --> 00:51:45,599
and because we want it to be

00:51:43,200 --> 00:51:47,119
collaborative uh you probably only care

00:51:45,599 --> 00:51:49,119
about some specific fields maybe some

00:51:47,119 --> 00:51:50,880
controllers care about other values in

00:51:49,119 --> 00:51:52,559
the same object so we never want to

00:51:50,880 --> 00:51:55,520
replace the entire

00:51:52,559 --> 00:51:57,440
object so we want to merge them uh for

00:51:55,520 --> 00:52:00,079
that we have patch

00:51:57,440 --> 00:52:01,920
but patch is very imperative in nature

00:52:00,079 --> 00:52:03,520
you have to look specifically at the

00:52:01,920 --> 00:52:06,480
changes that you want to make

00:52:03,520 --> 00:52:07,359
and then apply them there is chess on

00:52:06,480 --> 00:52:09,680
merch path

00:52:07,359 --> 00:52:11,440
patch that does that but it doesn't work

00:52:09,680 --> 00:52:14,160
very well in lists

00:52:11,440 --> 00:52:15,440
kubernetes has tons of lists for many

00:52:14,160 --> 00:52:18,480
reasons it uses

00:52:15,440 --> 00:52:20,400
associative list as you know this is the

00:52:18,480 --> 00:52:20,880
type of list well the name is actually

00:52:20,400 --> 00:52:24,319
the key

00:52:20,880 --> 00:52:26,559
the map hidden inside a list and json

00:52:24,319 --> 00:52:28,480
mulch patch just replaces list

00:52:26,559 --> 00:52:30,000
altogether if you want if you apply your

00:52:28,480 --> 00:52:30,400
list in chess and merge batch it's going

00:52:30,000 --> 00:52:32,480
to

00:52:30,400 --> 00:52:34,319
replace the entire list so if a

00:52:32,480 --> 00:52:34,960
controller has added anything to the

00:52:34,319 --> 00:52:38,319
list

00:52:34,960 --> 00:52:40,640
it's going to be of our reader so

00:52:38,319 --> 00:52:42,880
all of that to say that basically it's

00:52:40,640 --> 00:52:45,920
very hard not to hinder collaboration

00:52:42,880 --> 00:52:46,800
uh when creating such a system the

00:52:45,920 --> 00:52:49,280
initial

00:52:46,800 --> 00:52:49,839
implementation of coup curl apply is

00:52:49,280 --> 00:52:52,160
written

00:52:49,839 --> 00:52:53,680
on the client and tries to address some

00:52:52,160 --> 00:52:56,880
of these challenges

00:52:53,680 --> 00:52:59,920
um here we can see from this tweet

00:52:56,880 --> 00:53:01,359
that coup color apply is the solution to

00:52:59,920 --> 00:53:03,839
what i just described

00:53:01,359 --> 00:53:04,960
it's the declarative configuration tool

00:53:03,839 --> 00:53:08,079
it allows you

00:53:04,960 --> 00:53:09,040
to repeat you can take a cluster make

00:53:08,079 --> 00:53:11,440
some changes

00:53:09,040 --> 00:53:12,880
apply them to a different cluster and

00:53:11,440 --> 00:53:16,240
hopefully everything

00:53:12,880 --> 00:53:18,720
is going to just work this is

00:53:16,240 --> 00:53:20,640
uh obviously very different from reality

00:53:18,720 --> 00:53:22,160
where you have storage and clusters are

00:53:20,640 --> 00:53:23,359
typically slightly different from each

00:53:22,160 --> 00:53:25,839
other

00:53:23,359 --> 00:53:27,040
so you need to have some adjustments but

00:53:25,839 --> 00:53:30,079
basically this is

00:53:27,040 --> 00:53:31,760
it's still much easier than taking a lot

00:53:30,079 --> 00:53:33,440
of code to generate the config and make

00:53:31,760 --> 00:53:36,240
sure that it applies properly to a

00:53:33,440 --> 00:53:40,480
different context

00:53:36,240 --> 00:53:42,720
so how does this work um how do we apply

00:53:40,480 --> 00:53:43,760
we do apply by running keep call apply

00:53:42,720 --> 00:53:46,240
uh command

00:53:43,760 --> 00:53:47,920
the the command and you can apply these

00:53:46,240 --> 00:53:49,520
to many configurations this is

00:53:47,920 --> 00:53:51,280
convenient because

00:53:49,520 --> 00:53:53,520
again we don't have to look at each

00:53:51,280 --> 00:53:54,640
individual configuration and decide if

00:53:53,520 --> 00:53:59,040
they need to be

00:53:54,640 --> 00:54:01,359
uh created or updated um

00:53:59,040 --> 00:54:03,520
this is obviously not meant at all for

00:54:01,359 --> 00:54:04,720
uh imperative commands like you can't

00:54:03,520 --> 00:54:07,760
apply a

00:54:04,720 --> 00:54:09,839
charged credit card or open this door

00:54:07,760 --> 00:54:11,760
though if you wanted to do open doors

00:54:09,839 --> 00:54:14,160
you could rephrase this and say

00:54:11,760 --> 00:54:15,119
dal open and that would be either true

00:54:14,160 --> 00:54:16,880
or false

00:54:15,119 --> 00:54:19,280
and this would automatically become

00:54:16,880 --> 00:54:21,119
declarative

00:54:19,280 --> 00:54:22,880
this is much more collaborative too

00:54:21,119 --> 00:54:24,640
because it allows to change only

00:54:22,880 --> 00:54:26,880
specific fields

00:54:24,640 --> 00:54:27,760
uh rather than replacing the entire

00:54:26,880 --> 00:54:29,599
object

00:54:27,760 --> 00:54:31,280
uh we'll see in the next slide but

00:54:29,599 --> 00:54:33,680
basically you could all apply

00:54:31,280 --> 00:54:35,920
a sensor patch and it's literally trying

00:54:33,680 --> 00:54:38,960
to look at each individual fields

00:54:35,920 --> 00:54:41,280
see which ones have changed and update

00:54:38,960 --> 00:54:43,119
the object by sending a patch to change

00:54:41,280 --> 00:54:45,520
these values

00:54:43,119 --> 00:54:47,280
the way it decides on what has changed

00:54:45,520 --> 00:54:49,680
and what needs to change

00:54:47,280 --> 00:54:51,040
is based on what you had applied before

00:54:49,680 --> 00:54:53,839
and so for that

00:54:51,040 --> 00:54:54,480
we save the object the way you had it

00:54:53,839 --> 00:54:57,760
before

00:54:54,480 --> 00:54:59,520
in the object itself uh under this

00:54:57,760 --> 00:55:01,760
last applied annotation so as you can

00:54:59,520 --> 00:55:03,200
see here we have the entire object that

00:55:01,760 --> 00:55:05,760
we've seen before

00:55:03,200 --> 00:55:06,480
inside the annotation saved in the

00:55:05,760 --> 00:55:09,599
object

00:55:06,480 --> 00:55:10,880
this is literally how we save the uh old

00:55:09,599 --> 00:55:14,000
configuration

00:55:10,880 --> 00:55:15,680
in kubernetes we scoop karaoke

00:55:14,000 --> 00:55:17,680
how does it work so we can see on the

00:55:15,680 --> 00:55:19,520
left that we get the object from the

00:55:17,680 --> 00:55:20,960
server

00:55:19,520 --> 00:55:22,640
and we can see that the number of

00:55:20,960 --> 00:55:25,920
replicas is five

00:55:22,640 --> 00:55:27,920
the image is uh engine x142 and it has a

00:55:25,920 --> 00:55:29,599
sidecar container and i've removed the

00:55:27,920 --> 00:55:31,040
details of the sidecar because they're

00:55:29,599 --> 00:55:33,119
not relevant here

00:55:31,040 --> 00:55:34,640
but we can see that there is a side here

00:55:33,119 --> 00:55:36,319
and we can see

00:55:34,640 --> 00:55:38,799
the before which is coming from the

00:55:36,319 --> 00:55:42,160
annotation

00:55:38,799 --> 00:55:43,920
that we have three replicas before and

00:55:42,160 --> 00:55:46,559
so probably someone must have changed

00:55:43,920 --> 00:55:49,359
that value since we applied last time

00:55:46,559 --> 00:55:51,359
and uh the image hasn't changed we can

00:55:49,359 --> 00:55:54,400
also see that we didn't apply the

00:55:51,359 --> 00:55:56,240
sidecar initially so someone must have

00:55:54,400 --> 00:55:57,839
an opinion about this sidecar but it's

00:55:56,240 --> 00:56:00,160
definitely not us

00:55:57,839 --> 00:56:01,119
and then we can see what is being

00:56:00,160 --> 00:56:04,160
applied now

00:56:01,119 --> 00:56:05,119
in the third column we still want three

00:56:04,160 --> 00:56:07,200
replicas

00:56:05,119 --> 00:56:09,520
and we want to update the image to the

00:56:07,200 --> 00:56:12,799
nginx image to 143

00:56:09,520 --> 00:56:14,480
um we are going to look at these three

00:56:12,799 --> 00:56:15,599
different objects the current version

00:56:14,480 --> 00:56:17,599
the b4 version

00:56:15,599 --> 00:56:20,880
and the applied version and we're going

00:56:17,599 --> 00:56:23,040
to perform a three-way merge

00:56:20,880 --> 00:56:23,920
we're going to compute this new final

00:56:23,040 --> 00:56:26,240
object

00:56:23,920 --> 00:56:28,000
which has the number of replicas equal

00:56:26,240 --> 00:56:29,440
three which is the value that you said

00:56:28,000 --> 00:56:31,760
you wanted

00:56:29,440 --> 00:56:33,839
we're going to update the image to nginx

00:56:31,760 --> 00:56:36,400
143 as you wanted

00:56:33,839 --> 00:56:37,440
and we are going to conserve the sidecar

00:56:36,400 --> 00:56:38,960
container

00:56:37,440 --> 00:56:40,559
because you don't have an opinion about

00:56:38,960 --> 00:56:41,599
that so we don't believe that it should

00:56:40,559 --> 00:56:43,599
be removed

00:56:41,599 --> 00:56:45,280
once we have this new object we are

00:56:43,599 --> 00:56:46,400
going to compare it with the existing

00:56:45,280 --> 00:56:48,240
object

00:56:46,400 --> 00:56:49,839
compute the difference and send that

00:56:48,240 --> 00:56:52,640
difference as a patch

00:56:49,839 --> 00:56:52,640
to the server

00:56:53,119 --> 00:56:57,280
all of that sounds good but obviously

00:56:55,680 --> 00:57:00,160
there are limitations with this

00:56:57,280 --> 00:57:02,160
system the patch that we used to send to

00:57:00,160 --> 00:57:04,880
kubernetes has been made

00:57:02,160 --> 00:57:05,920
by the kubernetes community as the needs

00:57:04,880 --> 00:57:09,599
were coming

00:57:05,920 --> 00:57:11,839
we made it uh in a very homegoing way

00:57:09,599 --> 00:57:13,200
without thinking about all the times we

00:57:11,839 --> 00:57:16,240
could have with that

00:57:13,200 --> 00:57:20,000
and obviously we missed many use cases

00:57:16,240 --> 00:57:20,319
and um it became insanely hard to update

00:57:20,000 --> 00:57:22,480
so

00:57:20,319 --> 00:57:24,000
one of the prime with the patch strategy

00:57:22,480 --> 00:57:25,520
that we created that is called a

00:57:24,000 --> 00:57:28,640
strategic match patch

00:57:25,520 --> 00:57:31,280
is that um if we need to make an update

00:57:28,640 --> 00:57:32,799
to the strategic merge patch

00:57:31,280 --> 00:57:35,680
we don't have a way to version the

00:57:32,799 --> 00:57:37,599
strategy so if we update the client

00:57:35,680 --> 00:57:39,040
then we also need to update the server

00:57:37,599 --> 00:57:41,280
but we also need to make sure that the

00:57:39,040 --> 00:57:43,040
server and the and the client match

00:57:41,280 --> 00:57:44,640
and that they use the same version and

00:57:43,040 --> 00:57:46,480
if they don't then we need some sort of

00:57:44,640 --> 00:57:48,240
mechanism to know

00:57:46,480 --> 00:57:49,760
if they use the same version or what

00:57:48,240 --> 00:57:51,280
version they can use and we don't even

00:57:49,760 --> 00:57:53,599
know the version anyway so

00:57:51,280 --> 00:57:56,240
this was very very complicated we needed

00:57:53,599 --> 00:57:58,000
to make the changes of our many version

00:57:56,240 --> 00:58:01,200
of kubernetes any change

00:57:58,000 --> 00:58:02,960
required two or three releases to make

00:58:01,200 --> 00:58:04,079
sure that we could propagate the change

00:58:02,960 --> 00:58:07,440
everywhere

00:58:04,079 --> 00:58:09,359
and this was very very complicated so

00:58:07,440 --> 00:58:11,040
knowing that this was never going to be

00:58:09,359 --> 00:58:13,280
the good solution anyway we never

00:58:11,040 --> 00:58:15,280
implemented it for customer resources

00:58:13,280 --> 00:58:16,720
um so that doesn't even work for

00:58:15,280 --> 00:58:18,480
customer resources you can't

00:58:16,720 --> 00:58:20,400
you can cook karol applied some

00:58:18,480 --> 00:58:22,319
resources but you miss

00:58:20,400 --> 00:58:24,000
a lot of the possibilities that are

00:58:22,319 --> 00:58:27,359
available to you if you're

00:58:24,000 --> 00:58:29,200
using a built-in type um it also has

00:58:27,359 --> 00:58:29,839
limitations on the collaboration to be

00:58:29,200 --> 00:58:31,760
honest

00:58:29,839 --> 00:58:34,079
it's not solving all the use cases of

00:58:31,760 --> 00:58:35,839
collaboration that we had in mind

00:58:34,079 --> 00:58:38,319
and specifically because the

00:58:35,839 --> 00:58:40,880
implementation which is very complex

00:58:38,319 --> 00:58:41,599
is specifically made in coop cuddle in

00:58:40,880 --> 00:58:43,440
golang

00:58:41,599 --> 00:58:44,799
and if you want to use any other

00:58:43,440 --> 00:58:46,400
language than golang

00:58:44,799 --> 00:58:48,319
there is no way for you to use the

00:58:46,400 --> 00:58:50,720
algorithm you can maybe

00:58:48,319 --> 00:58:51,520
create a new process and you could

00:58:50,720 --> 00:58:53,760
curl

00:58:51,520 --> 00:58:55,359
but that's the only way you can't easily

00:58:53,760 --> 00:58:58,480
create a tool

00:58:55,359 --> 00:59:01,119
or library that is going to use

00:58:58,480 --> 00:59:01,920
apply if it's not written in curling or

00:59:01,119 --> 00:59:04,000
if it's not

00:59:01,920 --> 00:59:07,520
using the cube color binary and that's

00:59:04,000 --> 00:59:10,160
really detrimental for the ecosystem

00:59:07,520 --> 00:59:12,079
one of the other challenge is that only

00:59:10,160 --> 00:59:13,280
one actor can apply so because we have

00:59:12,079 --> 00:59:14,960
this annotation

00:59:13,280 --> 00:59:17,599
that says this is the last thing that

00:59:14,960 --> 00:59:19,280
you applied no one else can come and use

00:59:17,599 --> 00:59:20,480
the same annotation obviously because

00:59:19,280 --> 00:59:24,240
they would just

00:59:20,480 --> 00:59:26,000
not agree um so we could create maybe

00:59:24,240 --> 00:59:29,359
another annotation

00:59:26,000 --> 00:59:31,599
but clearly the people would

00:59:29,359 --> 00:59:33,760
take the chance and just override their

00:59:31,599 --> 00:59:35,200
fields constantly if you have an opinion

00:59:33,760 --> 00:59:36,079
about a field and i have a different

00:59:35,200 --> 00:59:39,119
opinion about

00:59:36,079 --> 00:59:40,799
the same field anytime i'm playing

00:59:39,119 --> 00:59:42,079
i'm going to overwrite your change and

00:59:40,799 --> 00:59:43,760
anytime you apply you're going to

00:59:42,079 --> 00:59:45,440
override my change and we're going to

00:59:43,760 --> 00:59:48,640
keep fighting like this

00:59:45,440 --> 00:59:49,280
in a way that is uh sometimes hard to

00:59:48,640 --> 00:59:51,680
detect

00:59:49,280 --> 00:59:53,359
and sometimes even if it's easy to

00:59:51,680 --> 00:59:55,040
detect it you might detect it because

00:59:53,359 --> 00:59:57,839
you've parked on your cluster

00:59:55,040 --> 01:00:00,160
um so the fields being overwritten

00:59:57,839 --> 01:00:02,400
unintentionally is definitely a big

01:00:00,160 --> 01:00:04,319
problem for collaboration

01:00:02,400 --> 01:00:06,720
one very good example of that again

01:00:04,319 --> 01:00:08,480
using the replicas field is that if you

01:00:06,720 --> 01:00:10,480
let's say you created the deployment

01:00:08,480 --> 01:00:12,799
with three replicas

01:00:10,480 --> 01:00:14,720
and later on you want to use an hpa to

01:00:12,799 --> 01:00:15,760
decide automatically how many replicas

01:00:14,720 --> 01:00:17,599
you want

01:00:15,760 --> 01:00:19,359
and let's say your system is very

01:00:17,599 --> 01:00:22,079
successful and you end up having

01:00:19,359 --> 01:00:24,000
a thousand replicas and now when you

01:00:22,079 --> 01:00:27,040
apply your config

01:00:24,000 --> 01:00:29,040
you actually apply three by mistake

01:00:27,040 --> 01:00:30,799
it's going to override this field the

01:00:29,040 --> 01:00:33,040
1000 olympics

01:00:30,799 --> 01:00:34,640
and you're going to end up with only

01:00:33,040 --> 01:00:37,040
three which may

01:00:34,640 --> 01:00:38,640
very well break uh the system and your

01:00:37,040 --> 01:00:40,960
customers use cases

01:00:38,640 --> 01:00:42,319
so that's actually quite a terrible bug

01:00:40,960 --> 01:00:46,160
that we want to avoid

01:00:42,319 --> 01:00:46,640
um finally one of the prime we had with

01:00:46,160 --> 01:00:48,400
the

01:00:46,640 --> 01:00:50,799
with this approach is again we didn't

01:00:48,400 --> 01:00:51,040
know exactly where we were going to end

01:00:50,799 --> 01:00:53,359
up

01:00:51,040 --> 01:00:55,119
when we started and so because of this

01:00:53,359 --> 01:00:57,599
lack of systemic approach

01:00:55,119 --> 01:00:58,799
there are many bugs that are very subtle

01:00:57,599 --> 01:01:00,559
and hard to fix

01:00:58,799 --> 01:01:03,359
and an example again about these

01:01:00,559 --> 01:01:05,760
replicas and take the same example

01:01:03,359 --> 01:01:07,760
if you have this hpa set the number of

01:01:05,760 --> 01:01:09,359
replicas to 1000

01:01:07,760 --> 01:01:10,480
and you actually remove it from your

01:01:09,359 --> 01:01:11,440
config because you don't want to

01:01:10,480 --> 01:01:13,040
overwrite it

01:01:11,440 --> 01:01:15,359
so you remove it from your config and

01:01:13,040 --> 01:01:17,359
apply it's actually going to believe

01:01:15,359 --> 01:01:18,240
that you want to remove the replicas

01:01:17,359 --> 01:01:19,760
field

01:01:18,240 --> 01:01:21,920
and the problem is when you remove the

01:01:19,760 --> 01:01:22,319
replicas field it gets defaulted to one

01:01:21,920 --> 01:01:25,520
so

01:01:22,319 --> 01:01:25,760
you break your customers anyway so there

01:01:25,520 --> 01:01:28,480
is

01:01:25,760 --> 01:01:30,319
actually no way of very only very

01:01:28,480 --> 01:01:32,000
complicated ways to solve this problem

01:01:30,319 --> 01:01:33,599
if you want to remove the epic skill

01:01:32,000 --> 01:01:34,319
it's probably going to break something

01:01:33,599 --> 01:01:37,200
and

01:01:34,319 --> 01:01:39,040
this is very complicated and unfortunate

01:01:37,200 --> 01:01:40,880
so we invented server side apply which

01:01:39,040 --> 01:01:43,440
is a new mechanism

01:01:40,880 --> 01:01:44,160
for which is a new mechanism for

01:01:43,440 --> 01:01:47,599
applying

01:01:44,160 --> 01:01:50,960
uh to a cluster this works by removing

01:01:47,599 --> 01:01:52,160
every logic we have in cube color and

01:01:50,960 --> 01:01:54,079
move it to the server

01:01:52,160 --> 01:01:55,280
so literally was tripping good color

01:01:54,079 --> 01:01:58,160
from any

01:01:55,280 --> 01:02:00,000
applied logic any merch logic and we are

01:01:58,160 --> 01:02:01,200
moving it to the cluster with some

01:02:00,000 --> 01:02:04,079
changes

01:02:01,200 --> 01:02:06,400
and so what do we do uh we created a new

01:02:04,079 --> 01:02:07,680
endpoint on the api solo or you can send

01:02:06,400 --> 01:02:09,760
your intent and say

01:02:07,680 --> 01:02:10,880
this is what i want and this is what i

01:02:09,760 --> 01:02:14,000
care about

01:02:10,880 --> 01:02:15,680
please do so uh do not overwrite

01:02:14,000 --> 01:02:16,319
anything that i'm not specifying in

01:02:15,680 --> 01:02:18,480
there

01:02:16,319 --> 01:02:20,480
if i've removed anything don't break

01:02:18,480 --> 01:02:21,839
everything please

01:02:20,480 --> 01:02:23,920
and the server is going to take care of

01:02:21,839 --> 01:02:25,359
all of that it's very very convenient

01:02:23,920 --> 01:02:27,039
because now you can write a client

01:02:25,359 --> 01:02:29,440
without doing anything you just have to

01:02:27,039 --> 01:02:31,359
send the object you just read the file

01:02:29,440 --> 01:02:34,240
send it to the proper endpoint and it's

01:02:31,359 --> 01:02:37,359
going to be applied to the cluster

01:02:34,240 --> 01:02:38,640
this it's as easy as doing curl curls

01:02:37,359 --> 01:02:41,119
send this file to this

01:02:38,640 --> 01:02:41,920
specific url and boom you have the file

01:02:41,119 --> 01:02:44,480
merged

01:02:41,920 --> 01:02:46,079
on the server properly uh we've been

01:02:44,480 --> 01:02:48,720
very careful to keep some

01:02:46,079 --> 01:02:49,520
compatibility with client-side apply and

01:02:48,720 --> 01:02:52,160
so

01:02:49,520 --> 01:02:54,559
you can still use these mechanisms and

01:02:52,160 --> 01:02:56,720
client-side apply at the same time

01:02:54,559 --> 01:02:58,720
uh and this is still going to work we're

01:02:56,720 --> 01:03:01,200
going to be very careful to maintain the

01:02:58,720 --> 01:03:04,160
last applied annotation as you do

01:03:01,200 --> 01:03:04,960
and you can go back and forth so that um

01:03:04,160 --> 01:03:06,319
so that

01:03:04,960 --> 01:03:08,079
things still work the way you would

01:03:06,319 --> 01:03:09,920
expect and if you want to go back and

01:03:08,079 --> 01:03:11,599
try one and go back to the other one

01:03:09,920 --> 01:03:14,000
it's still gonna work so that's i think

01:03:11,599 --> 01:03:16,400
that's a very good feature

01:03:14,000 --> 01:03:18,079
how does that work well because we want

01:03:16,400 --> 01:03:21,440
the credibility of approach

01:03:18,079 --> 01:03:24,240
we've decided to create managers so

01:03:21,440 --> 01:03:25,400
managers are actors of the system we

01:03:24,240 --> 01:03:28,720
believe that

01:03:25,400 --> 01:03:29,680
configurations have many actors acting

01:03:28,720 --> 01:03:32,640
on the same

01:03:29,680 --> 01:03:34,160
configuration and we want them to work

01:03:32,640 --> 01:03:36,319
together well

01:03:34,160 --> 01:03:38,240
and prevent any problem that could

01:03:36,319 --> 01:03:40,720
happen if someone is trying to change

01:03:38,240 --> 01:03:43,839
the field that you have an opinion about

01:03:40,720 --> 01:03:45,760
so each actor in the system has a name

01:03:43,839 --> 01:03:48,640
uh as you can see on this uh

01:03:45,760 --> 01:03:50,559
on the left we have the manager with uh

01:03:48,640 --> 01:03:54,240
coop call in this case

01:03:50,559 --> 01:03:56,160
and each actor has a list of field that

01:03:54,240 --> 01:03:59,200
they manage it's actually not a list

01:03:56,160 --> 01:04:00,000
we could call it a set um it's a set of

01:03:59,200 --> 01:04:02,400
fields

01:04:00,000 --> 01:04:03,920
and these are the fields that we believe

01:04:02,400 --> 01:04:05,599
you have an opinion about so if you

01:04:03,920 --> 01:04:06,480
apply the configuration that we saw

01:04:05,599 --> 01:04:08,000
before

01:04:06,480 --> 01:04:09,839
you're going to have an opinion about

01:04:08,000 --> 01:04:10,960
replicas you're going to have an opinion

01:04:09,839 --> 01:04:12,799
about

01:04:10,960 --> 01:04:15,039
the container image and the container

01:04:12,799 --> 01:04:15,039
name

01:04:15,440 --> 01:04:19,200
if someone else has an opinion about the

01:04:17,440 --> 01:04:20,160
same fields you're going to get a

01:04:19,200 --> 01:04:24,240
conflict

01:04:20,160 --> 01:04:26,640
so what happens is that if somebody

01:04:24,240 --> 01:04:28,319
for example the hpa has set the number

01:04:26,640 --> 01:04:30,480
of replicas to a different value

01:04:28,319 --> 01:04:31,839
we are going to notice that they already

01:04:30,480 --> 01:04:33,680
care about that value

01:04:31,839 --> 01:04:35,280
and so as you're trying to change the

01:04:33,680 --> 01:04:37,520
value of the field

01:04:35,280 --> 01:04:39,200
we will send back an error saying

01:04:37,520 --> 01:04:41,119
someone already has

01:04:39,200 --> 01:04:43,520
and manages this field they have an

01:04:41,119 --> 01:04:44,480
opinion are you sure you want to change

01:04:43,520 --> 01:04:46,640
the value

01:04:44,480 --> 01:04:48,880
because that can be catastrophic if you

01:04:46,640 --> 01:04:52,960
do and so the user has two

01:04:48,880 --> 01:04:55,520
options they can decide to

01:04:52,960 --> 01:04:57,440
remove the value from their file in

01:04:55,520 --> 01:04:59,200
which case they can then reapply and

01:04:57,440 --> 01:05:01,119
they're not gonna get a conflict because

01:04:59,200 --> 01:05:03,359
they literally said yeah indeed i don't

01:05:01,119 --> 01:05:05,440
have an opinion about replicas anymore

01:05:03,359 --> 01:05:06,400
let the system decide someone else has

01:05:05,440 --> 01:05:08,319
an opinion

01:05:06,400 --> 01:05:10,640
i trust them to have a better opinion

01:05:08,319 --> 01:05:12,799
than i let them do it

01:05:10,640 --> 01:05:14,000
uh or you can use the false flag which

01:05:12,799 --> 01:05:15,760
is going to

01:05:14,000 --> 01:05:17,440
force the value and in this case we are

01:05:15,760 --> 01:05:18,160
literally going to take the value of

01:05:17,440 --> 01:05:21,520
replicas

01:05:18,160 --> 01:05:22,880
from the manage field of the hpa we're

01:05:21,520 --> 01:05:24,720
gonna take it from them

01:05:22,880 --> 01:05:26,720
and we're gonna put it back in the

01:05:24,720 --> 01:05:29,760
fields that human edge

01:05:26,720 --> 01:05:31,760
um one interesting feature again

01:05:29,760 --> 01:05:33,039
is the backwards compatibility that we

01:05:31,760 --> 01:05:36,319
wanna maintain

01:05:33,039 --> 01:05:37,920
um we can't start breaking things that

01:05:36,319 --> 01:05:40,720
work today in kubernetes

01:05:37,920 --> 01:05:42,400
including controllers um it's difficult

01:05:40,720 --> 01:05:43,280
when there is an error in the controller

01:05:42,400 --> 01:05:46,480
to know about it

01:05:43,280 --> 01:05:48,839
and for a user to do anything about it

01:05:46,480 --> 01:05:51,039
so controllers are always sort of

01:05:48,839 --> 01:05:52,799
forcing and they're not using apply

01:05:51,039 --> 01:05:55,119
anyway because the plan is new so

01:05:52,799 --> 01:05:56,640
but even if they're not using apply and

01:05:55,119 --> 01:05:59,680
even if they're not forcing we

01:05:56,640 --> 01:06:02,160
always accept their changes and we only

01:05:59,680 --> 01:06:04,160
detect that they have a change because

01:06:02,160 --> 01:06:05,359
we can look at the difference so if the

01:06:04,160 --> 01:06:07,920
hpa change is

01:06:05,359 --> 01:06:08,880
just the replicas it's going to own just

01:06:07,920 --> 01:06:10,480
the epigas

01:06:08,880 --> 01:06:12,559
and we know that because it changed the

01:06:10,480 --> 01:06:14,559
value and whenever you change the value

01:06:12,559 --> 01:06:17,520
it's because you have an opinion

01:06:14,559 --> 01:06:19,520
and that's how we build the sets for

01:06:17,520 --> 01:06:21,680
controllers and types that

01:06:19,520 --> 01:06:23,599
did not know about server side apply or

01:06:21,680 --> 01:06:24,319
do not use server side apply default so

01:06:23,599 --> 01:06:26,319
we are

01:06:24,319 --> 01:06:28,880
completely maintaining this backward

01:06:26,319 --> 01:06:32,640
compatibility with existing workflows

01:06:28,880 --> 01:06:34,799
and we're just doing the right thing

01:06:32,640 --> 01:06:38,000
by detecting that uh they have an

01:06:34,799 --> 01:06:41,520
opinion because they changed something

01:06:38,000 --> 01:06:46,079
so now the question is how are objects

01:06:41,520 --> 01:06:49,359
merged um we've tried to keep the logic

01:06:46,079 --> 01:06:51,920
as simple as possible and by that i mean

01:06:49,359 --> 01:06:54,480
we literally take what you apply we take

01:06:51,920 --> 01:06:56,720
the existing object and we

01:06:54,480 --> 01:06:59,440
put it on top so any value that you

01:06:56,720 --> 01:07:01,920
specify is going to end up in the object

01:06:59,440 --> 01:07:03,920
and any value that you haven't specified

01:07:01,920 --> 01:07:06,079
is going to be kept the same

01:07:03,920 --> 01:07:07,760
um obviously one of the question when we

01:07:06,079 --> 01:07:08,960
do that is how do you remove a field

01:07:07,760 --> 01:07:10,319
because if you remove it from your

01:07:08,960 --> 01:07:12,319
country and apply it

01:07:10,319 --> 01:07:14,240
uh well it's not going to be removed

01:07:12,319 --> 01:07:16,799
it's going to keep the current value

01:07:14,240 --> 01:07:18,799
uh and so for that we just removed the

01:07:16,799 --> 01:07:19,520
fields if nobody has an opinion about it

01:07:18,799 --> 01:07:21,520
anymore

01:07:19,520 --> 01:07:23,599
so if no one has an opinion about

01:07:21,520 --> 01:07:25,520
replicas we just remove it

01:07:23,599 --> 01:07:27,520
um and what happens when an application

01:07:25,520 --> 01:07:31,680
is removed it goes back to

01:07:27,520 --> 01:07:33,200
one um but it only happens if no one has

01:07:31,680 --> 01:07:35,680
an opinion so if you remove it from your

01:07:33,200 --> 01:07:37,839
config but the hpa has said value

01:07:35,680 --> 01:07:39,280
we are going to know oh well the hp htl

01:07:37,839 --> 01:07:40,160
cares about that let's keep the value

01:07:39,280 --> 01:07:43,760
because we believe

01:07:40,160 --> 01:07:46,720
it's correct um how do we keep track of

01:07:43,760 --> 01:07:47,839
who and what i've discussed it a little

01:07:46,720 --> 01:07:49,440
bit before so for

01:07:47,839 --> 01:07:51,039
controllers we typically look at what

01:07:49,440 --> 01:07:54,240
has changed

01:07:51,039 --> 01:07:54,960
when you apply we look at all the fields

01:07:54,240 --> 01:07:56,960
that you have

01:07:54,960 --> 01:07:58,240
applied and we create a set a

01:07:56,960 --> 01:08:02,160
mathematical set

01:07:58,240 --> 01:08:04,799
of all the fields that you care about

01:08:02,160 --> 01:08:06,000
and at that time we look at all the

01:08:04,799 --> 01:08:08,799
manager sets

01:08:06,000 --> 01:08:10,319
in the list and we intersect them with

01:08:08,799 --> 01:08:14,400
your set and

01:08:10,319 --> 01:08:16,640
any intersection that comes as non-empty

01:08:14,400 --> 01:08:17,920
shows the conflict so we literally take

01:08:16,640 --> 01:08:20,319
all of these sets

01:08:17,920 --> 01:08:22,239
and we send them back to the user saying

01:08:20,319 --> 01:08:23,199
you have an intersection with this other

01:08:22,239 --> 01:08:24,799
person

01:08:23,199 --> 01:08:26,480
which means you care about the same

01:08:24,799 --> 01:08:28,640
field um

01:08:26,480 --> 01:08:30,799
you need to do something about these and

01:08:28,640 --> 01:08:32,719
as you could see before we actually save

01:08:30,799 --> 01:08:34,480
the name of the person and for some

01:08:32,719 --> 01:08:36,319
changes we also keep the date

01:08:34,480 --> 01:08:37,759
so that we know when someone changed the

01:08:36,319 --> 01:08:38,799
field that you're trying to edit so we

01:08:37,759 --> 01:08:40,560
can say hey

01:08:38,799 --> 01:08:42,080
uh you're trying to change the field

01:08:40,560 --> 01:08:45,520
that the hpa has set

01:08:42,080 --> 01:08:48,239
uh at this value this is also very

01:08:45,520 --> 01:08:50,719
important for of the out of band changes

01:08:48,239 --> 01:08:51,440
so let's say something is broken in your

01:08:50,719 --> 01:08:55,679
cluster

01:08:51,440 --> 01:08:58,080
and um someone goes and connect and

01:08:55,679 --> 01:09:00,880
directly edits one of the field

01:08:58,080 --> 01:09:03,040
um when you reapply maybe you're going

01:09:00,880 --> 01:09:04,400
to override that value and we obviously

01:09:03,040 --> 01:09:05,040
don't want to do that because if you do

01:09:04,400 --> 01:09:07,920
that

01:09:05,040 --> 01:09:09,839
your cluster is going to break right um

01:09:07,920 --> 01:09:12,159
and so at that time you're going to see

01:09:09,839 --> 01:09:13,759
a very nice conflict that says

01:09:12,159 --> 01:09:15,279
you are trying to change these fields

01:09:13,759 --> 01:09:16,080
said by this person are you sure you

01:09:15,279 --> 01:09:18,719
want to do so

01:09:16,080 --> 01:09:20,480
and um reasonably you should ask

01:09:18,719 --> 01:09:21,600
yourself if this is what you actually

01:09:20,480 --> 01:09:24,799
want to do

01:09:21,600 --> 01:09:26,960
um also finally

01:09:24,799 --> 01:09:28,000
this works for built-in types and custom

01:09:26,960 --> 01:09:30,239
resources

01:09:28,000 --> 01:09:31,679
but we're really trying to make customer

01:09:30,239 --> 01:09:34,640
resources work

01:09:31,679 --> 01:09:36,480
uh the way building types work um and

01:09:34,640 --> 01:09:39,040
for that we've tried to make everything

01:09:36,480 --> 01:09:40,719
look and behave as closely as possible

01:09:39,040 --> 01:09:43,600
so if you are writing a

01:09:40,719 --> 01:09:45,279
custom type all of these should work

01:09:43,600 --> 01:09:46,480
exactly the same way as it would if you

01:09:45,279 --> 01:09:48,159
had a built-in type

01:09:46,480 --> 01:09:50,239
we don't want you to miss out on any

01:09:48,159 --> 01:09:53,120
future

01:09:50,239 --> 01:09:53,520
what are the benefits of that uh well so

01:09:53,120 --> 01:09:56,080
it

01:09:53,520 --> 01:09:58,080
enables the ecosystem uh because it's so

01:09:56,080 --> 01:10:01,199
much easier to write

01:09:58,080 --> 01:10:03,600
apply um that

01:10:01,199 --> 01:10:04,480
it's very it's much easier to write a

01:10:03,600 --> 01:10:06,480
new tool

01:10:04,480 --> 01:10:08,000
you don't have to write any any any

01:10:06,480 --> 01:10:10,880
logic you just have to send

01:10:08,000 --> 01:10:12,159
the content to the api server so now if

01:10:10,880 --> 01:10:14,159
you want to write a tool it's

01:10:12,159 --> 01:10:16,000
it's it's a few lines of code just to

01:10:14,159 --> 01:10:18,000
update the object which is a massive

01:10:16,000 --> 01:10:19,679
change compared to today where you had

01:10:18,000 --> 01:10:21,840
to either use go or use the

01:10:19,679 --> 01:10:24,159
cube curl tool make sure it's shipped

01:10:21,840 --> 01:10:27,199
properly that the version is right

01:10:24,159 --> 01:10:28,880
uh this is much easier now um

01:10:27,199 --> 01:10:31,120
obviously the other change is that we

01:10:28,880 --> 01:10:34,480
are marketable you have multiple

01:10:31,120 --> 01:10:36,320
players we are starting to convert some

01:10:34,480 --> 01:10:37,760
of the existing controllers to use this

01:10:36,320 --> 01:10:40,480
mechanism because

01:10:37,760 --> 01:10:42,719
we believe it has benefits and it's much

01:10:40,480 --> 01:10:45,760
easier to implement and maintain

01:10:42,719 --> 01:10:48,320
um the friendly user

01:10:45,760 --> 01:10:49,760
the friendliers are much better and

01:10:48,320 --> 01:10:53,199
prevent you from

01:10:49,760 --> 01:10:55,760
breaking your clusters uh accidentally

01:10:53,199 --> 01:10:58,960
and of course it improves the human and

01:10:55,760 --> 01:11:01,520
machine collaboration because

01:10:58,960 --> 01:11:02,480
you can now change parts of the config

01:11:01,520 --> 01:11:05,520
while the machine

01:11:02,480 --> 01:11:08,400
changes other parts of the config

01:11:05,520 --> 01:11:09,120
without interacting with each other and

01:11:08,400 --> 01:11:10,560
without

01:11:09,120 --> 01:11:13,760
overriding the values that you don't

01:11:10,560 --> 01:11:17,280
want to override by accident

01:11:13,760 --> 01:11:19,280
now now that we have that so we know

01:11:17,280 --> 01:11:20,000
that we can apply a lot of configuration

01:11:19,280 --> 01:11:22,560
but

01:11:20,000 --> 01:11:23,920
i i suspect you're soon going to realize

01:11:22,560 --> 01:11:26,800
that you have too many

01:11:23,920 --> 01:11:29,360
configuration files it's it's very

01:11:26,800 --> 01:11:31,920
common for people to wonder

01:11:29,360 --> 01:11:33,600
what to do with all of these yaml files

01:11:31,920 --> 01:11:35,199
and i'm sure you've seen that before you

01:11:33,600 --> 01:11:38,480
have too many yaml files

01:11:35,199 --> 01:11:40,640
i've i've had this farm and often

01:11:38,480 --> 01:11:42,000
you have one version of the file and you

01:11:40,640 --> 01:11:43,840
want to play it for maybe

01:11:42,000 --> 01:11:45,679
a test environment as i talked about

01:11:43,840 --> 01:11:46,960
before the problem is that the test

01:11:45,679 --> 01:11:49,120
environment is not

01:11:46,960 --> 01:11:50,560
not the exact same environment as the

01:11:49,120 --> 01:11:51,920
production you want to use a different

01:11:50,560 --> 01:11:54,800
database i mean

01:11:51,920 --> 01:11:56,640
hopefully um you have different

01:11:54,800 --> 01:11:58,400
requirements you probably don't want as

01:11:56,640 --> 01:12:00,239
many pads you probably don't want it to

01:11:58,400 --> 01:12:02,000
be deployed to the same place

01:12:00,239 --> 01:12:03,840
there are many things that are actually

01:12:02,000 --> 01:12:05,360
different between the test environment

01:12:03,840 --> 01:12:08,719
and the product environment

01:12:05,360 --> 01:12:11,280
and so you have very small variations

01:12:08,719 --> 01:12:13,280
between these configurations that can be

01:12:11,280 --> 01:12:15,040
already made of like many many different

01:12:13,280 --> 01:12:16,400
files and if you just want to change one

01:12:15,040 --> 01:12:18,719
value out of these many

01:12:16,400 --> 01:12:19,520
different configurations you're gonna

01:12:18,719 --> 01:12:22,719
have

01:12:19,520 --> 01:12:24,719
this problem and it

01:12:22,719 --> 01:12:26,800
starts very quickly even having two

01:12:24,719 --> 01:12:27,679
configurations is already too much for

01:12:26,800 --> 01:12:29,760
me so

01:12:27,679 --> 01:12:31,120
i have this program constantly what

01:12:29,760 --> 01:12:32,320
we've made is that we've created

01:12:31,120 --> 01:12:34,560
customize because

01:12:32,320 --> 01:12:36,560
we believe it's it's as close as

01:12:34,560 --> 01:12:38,239
possible to the principles

01:12:36,560 --> 01:12:39,600
that we've described in this

01:12:38,239 --> 01:12:41,120
presentation

01:12:39,600 --> 01:12:42,880
we believe that customize is

01:12:41,120 --> 01:12:45,520
collaborative

01:12:42,880 --> 01:12:46,400
it lets you share the configuration

01:12:45,520 --> 01:12:50,000
easily

01:12:46,400 --> 01:12:52,400
but it's also data so i still believe

01:12:50,000 --> 01:12:54,640
that the configuration should be data

01:12:52,400 --> 01:12:56,239
and this is very difficult this is very

01:12:54,640 --> 01:12:59,760
very tempting to use a

01:12:56,239 --> 01:13:02,719
programming language to abstract away

01:12:59,760 --> 01:13:04,159
some of this redundancy uh but i i

01:13:02,719 --> 01:13:05,760
suspect there is

01:13:04,159 --> 01:13:08,159
there is something wrong because as soon

01:13:05,760 --> 01:13:09,920
as the as the configuration becomes code

01:13:08,159 --> 01:13:10,719
it's not possible to collaborate with

01:13:09,920 --> 01:13:13,840
machine

01:13:10,719 --> 01:13:17,360
i i do not believe that it's possible to

01:13:13,840 --> 01:13:21,120
have configurations written as code

01:13:17,360 --> 01:13:23,120
if you want machines to edit them

01:13:21,120 --> 01:13:24,960
so that's one of the big challenge that

01:13:23,120 --> 01:13:27,360
we have today

01:13:24,960 --> 01:13:28,560
and customize is one of the one of the

01:13:27,360 --> 01:13:31,280
solution but it

01:13:28,560 --> 01:13:32,000
has other limitations i know so many

01:13:31,280 --> 01:13:35,520
solutions

01:13:32,000 --> 01:13:36,239
exist uh out there but most of them do

01:13:35,520 --> 01:13:39,040
not treat

01:13:36,239 --> 01:13:41,040
uh the configuration as data and as

01:13:39,040 --> 01:13:42,880
collaborative as it should be

01:13:41,040 --> 01:13:44,080
and i think that's one of the problems

01:13:42,880 --> 01:13:47,600
that we want to save

01:13:44,080 --> 01:13:50,159
uh we want to fix for later

01:13:47,600 --> 01:13:50,960
that's it for me uh that's all i have

01:13:50,159 --> 01:14:01,840
for today

01:13:50,960 --> 01:14:01,840
thank you so much for listening

01:14:10,159 --> 01:14:14,880
thanks antoine the apply action is so

01:14:13,199 --> 01:14:16,480
integral to the magic of kubernetes

01:14:14,880 --> 01:14:19,040
declarative capabilities

01:14:16,480 --> 01:14:20,640
by implementing antoine's suggestions we

01:14:19,040 --> 01:14:22,640
can help make sure our kubernetes

01:14:20,640 --> 01:14:24,080
clusters are as scalable and easy to

01:14:22,640 --> 01:14:25,360
work with as possible

01:14:24,080 --> 01:14:28,560
i know i'm going to review that on

01:14:25,360 --> 01:14:30,719
demand and probably live tweet it later

01:14:28,560 --> 01:14:32,880
we've covered a lot of really essential

01:14:30,719 --> 01:14:34,880
topics to working with kubernetes

01:14:32,880 --> 01:14:36,640
and regarding how kubernetes itself

01:14:34,880 --> 01:14:38,480
works our next talk

01:14:36,640 --> 01:14:40,800
dives further into kubernetes inner

01:14:38,480 --> 01:14:42,640
workings by focusing on a component that

01:14:40,800 --> 01:14:44,640
i know i struggled with

01:14:42,640 --> 01:14:47,040
when i was first getting started trying

01:14:44,640 --> 01:14:48,719
to understand kubernetes

01:14:47,040 --> 01:14:50,159
i believe it's also the most recent

01:14:48,719 --> 01:14:53,600
project in the cloud native computing

01:14:50,159 --> 01:14:55,600
foundation to achieve graduated status

01:14:53,600 --> 01:14:57,199
but before we get to that for a bit of

01:14:55,600 --> 01:14:57,920
interactive fun we're sharing another

01:14:57,199 --> 01:15:02,640
poll

01:14:57,920 --> 01:15:05,440
and this one is a bit controversial

01:15:02,640 --> 01:15:07,120
check it out and tell us how do you

01:15:05,440 --> 01:15:11,040
pronounce the name of the kubernetes

01:15:07,120 --> 01:15:13,199
command line tool

01:15:11,040 --> 01:15:16,080
and for our final talk before the after

01:15:13,199 --> 01:15:25,840
party we'll hear from you chen zhou

01:15:16,080 --> 01:15:25,840
about cd take it away

01:16:25,360 --> 01:16:29,199
hey there welcome to the session about

01:16:27,360 --> 01:16:31,440
icd i'm yuchan

01:16:29,199 --> 01:16:34,880
i'm a software engineer from google and

01:16:31,440 --> 01:16:37,199
i'm a contributor to icd oss community

01:16:34,880 --> 01:16:38,480
after listening to the previous sessions

01:16:37,199 --> 01:16:40,960
about bernankes

01:16:38,480 --> 01:16:42,080
i'm sure you have already have a very

01:16:40,960 --> 01:16:44,560
clear

01:16:42,080 --> 01:16:45,920
or no overall knowledge and impression

01:16:44,560 --> 01:16:47,760
of kubernetes

01:16:45,920 --> 01:16:49,440
so finally we'll reach to the backend

01:16:47,760 --> 01:16:51,280
data of the kinetics

01:16:49,440 --> 01:16:52,960
so in this session we'll have the

01:16:51,280 --> 01:16:55,679
exploration of icd

01:16:52,960 --> 01:16:58,000
where the kubernetes data rests it'll

01:16:55,679 --> 01:16:58,800
give you an overview of architecture of

01:16:58,000 --> 01:17:01,280
lcd

01:16:58,800 --> 01:17:03,280
and some key components which i think is

01:17:01,280 --> 01:17:05,920
very good to know

01:17:03,280 --> 01:17:08,080
here is today's agenda first i'll do the

01:17:05,920 --> 01:17:10,480
overall introduction of cd to say

01:17:08,080 --> 01:17:11,840
what is actually and then i'll talk

01:17:10,480 --> 01:17:12,640
about the relationship between

01:17:11,840 --> 01:17:15,840
kubernetes

01:17:12,640 --> 01:17:19,120
and ncd and say why does kubernetes

01:17:15,840 --> 01:17:21,120
use the cd as a storage and finally

01:17:19,120 --> 01:17:22,159
let's have a deeper dive into the

01:17:21,120 --> 01:17:25,199
technical detail

01:17:22,159 --> 01:17:29,120
about how is the icd request that

01:17:25,199 --> 01:17:32,400
lifecycle looks like first thing first

01:17:29,120 --> 01:17:32,960
what is sav from the website official

01:17:32,400 --> 01:17:36,400
website

01:17:32,960 --> 01:17:38,480
of a cd std is a distributed reliable

01:17:36,400 --> 01:17:41,040
key value store for the most critical

01:17:38,480 --> 01:17:43,120
data of a distributed system

01:17:41,040 --> 01:17:44,080
the naming of the city comes from two

01:17:43,120 --> 01:17:46,880
parts

01:17:44,080 --> 01:17:48,800
the first part is sc sd comes from the

01:17:46,880 --> 01:17:51,440
acid folder in unix

01:17:48,800 --> 01:17:52,560
d stands for distributed so it's very

01:17:51,440 --> 01:17:54,800
easy to understand

01:17:52,560 --> 01:17:57,040
why std is designed for distributed

01:17:54,800 --> 01:17:57,679
system and storing the most critical

01:17:57,040 --> 01:18:00,960
data

01:17:57,679 --> 01:18:02,400
like the configuration data std is the

01:18:00,960 --> 01:18:05,040
open source project

01:18:02,400 --> 01:18:06,480
it has it has a rapid growth over the

01:18:05,040 --> 01:18:09,760
past five years

01:18:06,480 --> 01:18:11,440
scd is a cncf incubating project it is

01:18:09,760 --> 01:18:12,880
adopted by many different projects

01:18:11,440 --> 01:18:16,480
including kubernetes

01:18:12,880 --> 01:18:19,679
which is of course the most famous one

01:18:16,480 --> 01:18:20,640
std focused on some key features here i

01:18:19,679 --> 01:18:22,880
listed

01:18:20,640 --> 01:18:24,080
three of them which i think is the most

01:18:22,880 --> 01:18:26,960
important

01:18:24,080 --> 01:18:28,960
the first one is consistency as it

01:18:26,960 --> 01:18:31,760
ensures strict serial liability

01:18:28,960 --> 01:18:32,640
by default which is drone consistency

01:18:31,760 --> 01:18:35,280
model

01:18:32,640 --> 01:18:36,000
it means the operation appears to have

01:18:35,280 --> 01:18:38,320
occurred

01:18:36,000 --> 01:18:41,360
in some order consistent with the

01:18:38,320 --> 01:18:44,159
real-time ordering of those operations

01:18:41,360 --> 01:18:46,080
for example if operation a completes

01:18:44,159 --> 01:18:49,199
before operation b begins

01:18:46,080 --> 01:18:49,600
then a should appear to perceive b the

01:18:49,199 --> 01:18:52,320
second

01:18:49,600 --> 01:18:54,560
feature is a high availability the

01:18:52,320 --> 01:18:56,560
recommended sad cluster

01:18:54,560 --> 01:18:57,600
should have more than three cluster

01:18:56,560 --> 01:18:59,840
members and

01:18:57,600 --> 01:19:01,520
it is designed to replicate among

01:18:59,840 --> 01:19:04,400
cluster members

01:19:01,520 --> 01:19:06,159
sde leverage of a consensus algorithm to

01:19:04,400 --> 01:19:09,199
provide strong consistency

01:19:06,159 --> 01:19:10,000
and high availability which will improve

01:19:09,199 --> 01:19:12,880
the height

01:19:10,000 --> 01:19:14,080
fault tolerance by avoiding the failure

01:19:12,880 --> 01:19:16,960
from the single point

01:19:14,080 --> 01:19:18,560
or network partition it is obvious if

01:19:16,960 --> 01:19:21,840
the cluster helps remember

01:19:18,560 --> 01:19:25,440
if it was uh well stand for the

01:19:21,840 --> 01:19:28,000
bigger video std

01:19:25,440 --> 01:19:28,800
has a high usability it is very simple

01:19:28,000 --> 01:19:31,679
to use

01:19:28,800 --> 01:19:33,679
you can read or read data using standard

01:19:31,679 --> 01:19:37,040
http or json tools

01:19:33,679 --> 01:19:39,840
such as curo as a lightweight data store

01:19:37,040 --> 01:19:41,120
it also has a it also has a good

01:19:39,840 --> 01:19:43,600
performance

01:19:41,120 --> 01:19:44,320
here is a benchmark for as a universable

01:19:43,600 --> 01:19:46,320
rate

01:19:44,320 --> 01:19:47,760
actually if you don't mind receiving the

01:19:46,320 --> 01:19:50,719
possible style data

01:19:47,760 --> 01:19:52,080
in some condition you can specify to use

01:19:50,719 --> 01:19:55,199
the zero level

01:19:52,080 --> 01:19:57,920
rate it has better performance but it

01:19:55,199 --> 01:19:59,440
just reads data from the local machine

01:19:57,920 --> 01:20:00,480
which means it does not ensure

01:19:59,440 --> 01:20:03,199
consistency

01:20:00,480 --> 01:20:05,600
but other hand on the other hand it is

01:20:03,199 --> 01:20:07,800
it has lower latency

01:20:05,600 --> 01:20:09,520
std itself recommend to use the

01:20:07,800 --> 01:20:12,880
linearizability to read

01:20:09,520 --> 01:20:12,880
to ensure consistency

01:20:13,520 --> 01:20:16,719
consistency is one of the most important

01:20:15,920 --> 01:20:20,480
selling point

01:20:16,719 --> 01:20:22,159
of icd so how does sad ensure the strong

01:20:20,480 --> 01:20:24,960
consistency

01:20:22,159 --> 01:20:25,920
the first principle is that any action

01:20:24,960 --> 01:20:29,040
to take place

01:20:25,920 --> 01:20:31,199
has to be called which means everything

01:20:29,040 --> 01:20:32,159
should be decided by the majority of the

01:20:31,199 --> 01:20:34,719
cluster

01:20:32,159 --> 01:20:35,520
if the cluster has to remember to its

01:20:34,719 --> 01:20:38,239
majority

01:20:35,520 --> 01:20:39,440
so everything should be to be agreed

01:20:38,239 --> 01:20:43,120
among these two

01:20:39,440 --> 01:20:45,120
at least it is a reason why std suggests

01:20:43,120 --> 01:20:47,600
to use other size clusters

01:20:45,120 --> 01:20:48,239
as dimensional ball as it builds on top

01:20:47,600 --> 01:20:50,880
of

01:20:48,239 --> 01:20:52,639
draft consensus algorithm there are two

01:20:50,880 --> 01:20:56,400
rapid autosome property

01:20:52,639 --> 01:20:59,199
that ensures consensus leader election

01:20:56,400 --> 01:21:00,320
is not one you may be familiar in

01:20:59,199 --> 01:21:02,400
kubernetes

01:21:00,320 --> 01:21:03,840
it is to make sure there is only one

01:21:02,400 --> 01:21:06,560
member in the cluster

01:21:03,840 --> 01:21:07,360
who can make decision other member in

01:21:06,560 --> 01:21:09,920
the cluster

01:21:07,360 --> 01:21:10,880
are followers the leader needs to send

01:21:09,920 --> 01:21:13,679
out the harvest

01:21:10,880 --> 01:21:15,199
to his follower to keep his authority

01:21:13,679 --> 01:21:17,600
the leader goes offline

01:21:15,199 --> 01:21:19,760
because of networking issue the former

01:21:17,600 --> 01:21:21,280
will not receive his heartbeats

01:21:19,760 --> 01:21:23,199
then the follower will overturn the

01:21:21,280 --> 01:21:24,080
previous leadership and start a new

01:21:23,199 --> 01:21:26,880
leader election

01:21:24,080 --> 01:21:28,960
as a candidate basically the cluster

01:21:26,880 --> 01:21:30,480
should always have a leader or try to

01:21:28,960 --> 01:21:32,239
elect the leader

01:21:30,480 --> 01:21:34,480
the customer will not serve the client

01:21:32,239 --> 01:21:36,320
request unless it has the leader

01:21:34,480 --> 01:21:38,159
so sometimes the cluster has high

01:21:36,320 --> 01:21:40,480
leadership frequency

01:21:38,159 --> 01:21:41,520
it will strongly reduce the performance

01:21:40,480 --> 01:21:43,679
of the cluster

01:21:41,520 --> 01:21:46,480
and also signal the potential network

01:21:43,679 --> 01:21:48,800
issue or extensive loop

01:21:46,480 --> 01:21:49,760
log replication is another property of

01:21:48,800 --> 01:21:52,159
route

01:21:49,760 --> 01:21:54,400
log replication makes sure only the

01:21:52,159 --> 01:21:57,120
leader can manage the replicated log

01:21:54,400 --> 01:21:59,280
and replicate its log to the followers

01:21:57,120 --> 01:22:01,440
the replicated law which is named as

01:21:59,280 --> 01:22:02,800
warlock should be identical on each

01:22:01,440 --> 01:22:05,520
machine in cluster

01:22:02,800 --> 01:22:07,679
logically but the most time each cluster

01:22:05,520 --> 01:22:10,320
member can have different performance

01:22:07,679 --> 01:22:12,159
and some of them can may lay behind so

01:22:10,320 --> 01:22:14,239
warlock recovers a lot that are

01:22:12,159 --> 01:22:16,639
committed on the majority machine

01:22:14,239 --> 01:22:17,840
so for example if you have a cluster

01:22:16,639 --> 01:22:20,320
with remember

01:22:17,840 --> 01:22:23,840
warlock will cross along that are at

01:22:20,320 --> 01:22:26,400
least committed on two options

01:22:23,840 --> 01:22:27,199
the right here is architecture of the

01:22:26,400 --> 01:22:30,239
replicated

01:22:27,199 --> 01:22:33,280
state machine the

01:22:30,239 --> 01:22:35,280
firstly the client will send the request

01:22:33,280 --> 01:22:37,199
and the consensus mode on the server

01:22:35,280 --> 01:22:39,520
will receive this request

01:22:37,199 --> 01:22:41,600
the leader will manage the warlock and

01:22:39,520 --> 01:22:42,800
replicate the log entry to other

01:22:41,600 --> 01:22:45,440
followers

01:22:42,800 --> 01:22:47,360
then each custom member will apply the

01:22:45,440 --> 01:22:50,800
committee log entry locally

01:22:47,360 --> 01:22:52,960
and respond to the class client

01:22:50,800 --> 01:22:54,000
even if we have the above mechanism the

01:22:52,960 --> 01:22:56,480
cluster can still

01:22:54,000 --> 01:22:58,080
end up with corruption the crafted

01:22:56,480 --> 01:23:00,000
member is logically part of

01:22:58,080 --> 01:23:01,760
coral but the backing data is

01:23:00,000 --> 01:23:03,920
inconsistent with each other

01:23:01,760 --> 01:23:06,639
which means the corrupted member is

01:23:03,920 --> 01:23:09,840
viewed as healthy by the peers

01:23:06,639 --> 01:23:11,280
even if itself but its backend data is

01:23:09,840 --> 01:23:13,360
unhealthy

01:23:11,280 --> 01:23:14,400
actually it is allowed that the data on

01:23:13,360 --> 01:23:17,040
different members

01:23:14,400 --> 01:23:18,080
could have discrepancy at once because

01:23:17,040 --> 01:23:20,639
small machine

01:23:18,080 --> 01:23:22,320
has small that's different different

01:23:20,639 --> 01:23:23,120
machine can have different processing

01:23:22,320 --> 01:23:26,000
speeds

01:23:23,120 --> 01:23:26,960
however a strong consistent database

01:23:26,000 --> 01:23:29,199
should make sure

01:23:26,960 --> 01:23:30,480
the data at one division number should

01:23:29,199 --> 01:23:33,520
be identical

01:23:30,480 --> 01:23:36,960
revision number is a important concept

01:23:33,520 --> 01:23:40,400
in std i'll mention it

01:23:36,960 --> 01:23:42,639
later in our later slides corruption

01:23:40,400 --> 01:23:45,679
check is built on top of this property

01:23:42,639 --> 01:23:48,639
it does a harsh operation and then they

01:23:45,679 --> 01:23:49,760
compare the some of each member at one

01:23:48,639 --> 01:23:52,080
revision

01:23:49,760 --> 01:23:52,800
the check sum of the each member at one

01:23:52,080 --> 01:23:55,920
position

01:23:52,800 --> 01:23:57,199
should be identical to why the original

01:23:55,920 --> 01:23:59,920
cluster corruption

01:23:57,199 --> 01:24:02,320
we need to corruption check periodically

01:23:59,920 --> 01:24:05,360
to make sure the dick data is consistent

01:24:02,320 --> 01:24:05,360
is very important

01:24:05,840 --> 01:24:10,639
here let's move into our second part icd

01:24:09,120 --> 01:24:13,280
in kubernetes

01:24:10,639 --> 01:24:15,199
std plays an important role not only as

01:24:13,280 --> 01:24:16,800
a back-end data store but also for the

01:24:15,199 --> 01:24:19,360
service discovery

01:24:16,800 --> 01:24:20,159
kubernetes makes use of sd's watch

01:24:19,360 --> 01:24:23,600
operation

01:24:20,159 --> 01:24:24,719
to asynchronize monitor asynchronously

01:24:23,600 --> 01:24:27,199
monitor changes

01:24:24,719 --> 01:24:28,239
to reconfig cell from actual to desired

01:24:27,199 --> 01:24:30,719
states

01:24:28,239 --> 01:24:31,440
from the deployment perspective sad

01:24:30,719 --> 01:24:34,960
instances

01:24:31,440 --> 01:24:37,600
are deployed as pots on masters in the

01:24:34,960 --> 01:24:39,120
original kubernetes cluster like the

01:24:37,600 --> 01:24:41,840
picture shown on the right

01:24:39,120 --> 01:24:44,239
it has three master nodes and each one

01:24:41,840 --> 01:24:47,280
has one icd instance and

01:24:44,239 --> 01:24:50,080
this three scd instance can form as

01:24:47,280 --> 01:24:51,440
sd cluster which will communicate with

01:24:50,080 --> 01:24:53,520
each other

01:24:51,440 --> 01:24:55,040
about the leader election or other

01:24:53,520 --> 01:24:58,159
message

01:24:55,040 --> 01:25:00,480
so why std is important to coordinates

01:24:58,159 --> 01:25:02,239
firstly is because the data storing

01:25:00,480 --> 01:25:04,239
functionality for sure

01:25:02,239 --> 01:25:05,280
sad is designed to store the most

01:25:04,239 --> 01:25:07,920
critical data

01:25:05,280 --> 01:25:08,719
and it has rather high performance and

01:25:07,920 --> 01:25:12,239
it ensures

01:25:08,719 --> 01:25:13,679
strongest consistency secondly the watch

01:25:12,239 --> 01:25:16,080
operation opacity

01:25:13,679 --> 01:25:16,719
makes kubernetes easier to monitor the

01:25:16,080 --> 01:25:19,199
desired

01:25:16,719 --> 01:25:20,639
and extra state if they diverge

01:25:19,199 --> 01:25:24,000
kubernetes make use

01:25:20,639 --> 01:25:24,719
make changes to reconcile the actual

01:25:24,000 --> 01:25:27,840
states

01:25:24,719 --> 01:25:27,840
and the desired states

01:25:28,080 --> 01:25:32,480
then let's have a deeper dive into the

01:25:30,159 --> 01:25:35,440
std technical detail

01:25:32,480 --> 01:25:37,120
here is the sd architecture everything

01:25:35,440 --> 01:25:40,159
starts from the client

01:25:37,120 --> 01:25:41,199
the client of sad server could be the sd

01:25:40,159 --> 01:25:44,400
card

01:25:41,199 --> 01:25:47,440
or the which is command line 2 of icd

01:25:44,400 --> 01:25:49,199
or the api server of kubernetes

01:25:47,440 --> 01:25:50,639
the request goes through the client

01:25:49,199 --> 01:25:52,800
balancer and will

01:25:50,639 --> 01:25:54,400
visit one of the available nodes in the

01:25:52,800 --> 01:25:56,960
lcd cluster

01:25:54,400 --> 01:25:59,600
once the server receives the request it

01:25:56,960 --> 01:26:01,760
will send out the proposal to the route

01:25:59,600 --> 01:26:04,080
here we can just build raft as the black

01:26:01,760 --> 01:26:06,639
box and the output of raft

01:26:04,080 --> 01:26:07,520
is at the same lock while replicating

01:26:06,639 --> 01:26:10,880
both files

01:26:07,520 --> 01:26:11,600
on the majority machine the consensus is

01:26:10,880 --> 01:26:14,239
insured

01:26:11,600 --> 01:26:15,679
since all node photo is very long to

01:26:14,239 --> 01:26:18,719
apply the request

01:26:15,679 --> 01:26:19,120
so so the last step is that the nodes

01:26:18,719 --> 01:26:21,840
will

01:26:19,120 --> 01:26:24,239
apply the wall lock one by one and the

01:26:21,840 --> 01:26:28,560
process to the disk

01:26:24,239 --> 01:26:30,880
there are two official sd ports 2379

01:26:28,560 --> 01:26:33,159
is for client to request they sending

01:26:30,880 --> 01:26:36,639
the request from the client

01:26:33,159 --> 01:26:38,239
2380 is for peer communication

01:26:36,639 --> 01:26:39,840
the peers can use this part to

01:26:38,239 --> 01:26:43,040
communicate with each other

01:26:39,840 --> 01:26:43,040
and switch the messages

01:26:44,000 --> 01:26:49,679
if you have noticed in last slide the

01:26:46,639 --> 01:26:52,960
backend of a cd is called mvcc store

01:26:49,679 --> 01:26:55,520
aka multiversion concurrency control

01:26:52,960 --> 01:26:57,840
scd is designed to store infrequently

01:26:55,520 --> 01:26:58,719
updated data and provide reliable water

01:26:57,840 --> 01:27:01,600
queries

01:26:58,719 --> 01:27:02,880
which means sad recommendations as they

01:27:01,600 --> 01:27:07,120
recommend to have

01:27:02,880 --> 01:27:10,719
a low low rate but more rate

01:27:07,120 --> 01:27:14,080
so sdk previous version of kiwi pairs

01:27:10,719 --> 01:27:16,639
just to support a inexpensive snapshot

01:27:14,080 --> 01:27:18,000
and watch history events known as time

01:27:16,639 --> 01:27:20,239
travel query

01:27:18,000 --> 01:27:24,480
that is integration we want to use

01:27:20,239 --> 01:27:28,320
mvcdata model to address these use cases

01:27:24,480 --> 01:27:30,880
the critical concept in mvcc is revision

01:27:28,320 --> 01:27:32,560
revision is clustered while the counter

01:27:30,880 --> 01:27:35,520
it will increment each time

01:27:32,560 --> 01:27:36,239
the k space is modified which means only

01:27:35,520 --> 01:27:38,800
put

01:27:36,239 --> 01:27:40,960
delete and transaction operation will

01:27:38,800 --> 01:27:43,040
change the revision number

01:27:40,960 --> 01:27:45,600
the range request which is known as the

01:27:43,040 --> 01:27:47,120
read request will not touch the k space

01:27:45,600 --> 01:27:49,440
so it will not change the refrigerant

01:27:47,120 --> 01:27:51,440
number

01:27:49,440 --> 01:27:52,480
the revision is looks like the logical

01:27:51,440 --> 01:27:54,480
clock

01:27:52,480 --> 01:27:57,360
it makes sure every member in the

01:27:54,480 --> 01:27:59,920
cluster can keep pace with each other

01:27:57,360 --> 01:28:00,719
logically data is stored in that binary

01:27:59,920 --> 01:28:04,000
case space

01:28:00,719 --> 01:28:07,120
which has multiple revisions physically

01:28:04,000 --> 01:28:09,760
as it is stored physical data as kv pair

01:28:07,120 --> 01:28:10,480
in persistent bot db the key of this

01:28:09,760 --> 01:28:13,679
persistent

01:28:10,480 --> 01:28:15,840
train is a revision pair composed by the

01:28:13,679 --> 01:28:18,320
star revision and the identity of

01:28:15,840 --> 01:28:20,800
each key in the same store revision it

01:28:18,320 --> 01:28:23,440
is not the key from the request

01:28:20,800 --> 01:28:24,159
value is a kiwi peer along with a stereo

01:28:23,440 --> 01:28:26,400
data

01:28:24,159 --> 01:28:27,360
if they if the client requests to call

01:28:26,400 --> 01:28:30,159
this

01:28:27,360 --> 01:28:34,080
to fetch this kbp that is a scene which

01:28:30,159 --> 01:28:34,080
will be returned to the client finally

01:28:35,840 --> 01:28:40,560
as a database system a city has a bunch

01:28:39,280 --> 01:28:43,440
of operations

01:28:40,560 --> 01:28:44,400
here i only list the most important ones

01:28:43,440 --> 01:28:47,440
the first tab

01:28:44,400 --> 01:28:50,639
is kv operation the basic kv operation

01:28:47,440 --> 01:28:51,920
which works on the sdk space range is

01:28:50,639 --> 01:28:53,920
the rated operation

01:28:51,920 --> 01:28:54,960
which can read the range of kiwi peer

01:28:53,920 --> 01:28:57,040
tax play

01:28:54,960 --> 01:28:59,440
for the the retail operation and

01:28:57,040 --> 01:29:00,400
transaction is atomic if than else

01:28:59,440 --> 01:29:03,280
construct

01:29:00,400 --> 01:29:04,320
over the key radio store a transaction

01:29:03,280 --> 01:29:06,560
can automatically

01:29:04,320 --> 01:29:07,600
process multiple requests in a single

01:29:06,560 --> 01:29:10,239
request

01:29:07,600 --> 01:29:12,159
it is widely used in kubernetes since

01:29:10,239 --> 01:29:14,239
the range and the port are the most

01:29:12,159 --> 01:29:16,400
common and basic requests

01:29:14,239 --> 01:29:18,800
it'll explain more about the lifecycle

01:29:16,400 --> 01:29:21,360
also

01:29:18,800 --> 01:29:22,880
the second type is maintenance operation

01:29:21,360 --> 01:29:26,080
including compaction

01:29:22,880 --> 01:29:28,400
defragmentation and snapshot watch is to

01:29:26,080 --> 01:29:30,880
monitor the change of the kv pair

01:29:28,400 --> 01:29:31,840
this is to manage a temporary keys in

01:29:30,880 --> 01:29:33,760
kubernetes

01:29:31,840 --> 01:29:35,360
it is used to auto delete inventory

01:29:33,760 --> 01:29:38,560
resources

01:29:35,360 --> 01:29:41,360
std also has many hazards has many other

01:29:38,560 --> 01:29:42,400
operations such like the authentication

01:29:41,360 --> 01:29:44,400
operations

01:29:42,400 --> 01:29:45,520
which is used for authentication and

01:29:44,400 --> 01:29:47,199
authorization

01:29:45,520 --> 01:29:48,560
if you are interested in it you can go

01:29:47,199 --> 01:29:52,400
to the website and read

01:29:48,560 --> 01:29:54,719
the official documentation

01:29:52,400 --> 01:29:55,760
here is the life cycle of the pull

01:29:54,719 --> 01:29:58,239
request

01:29:55,760 --> 01:30:00,239
firstly this reservoir will receive this

01:29:58,239 --> 01:30:02,159
request from sap client

01:30:00,239 --> 01:30:04,400
it wraps the request and sends the

01:30:02,159 --> 01:30:07,440
router request to the rough cluster

01:30:04,400 --> 01:30:09,280
then on each local member zebra applies

01:30:07,440 --> 01:30:11,679
a committed graphic log entry

01:30:09,280 --> 01:30:13,440
to in-memory index train which maintains

01:30:11,679 --> 01:30:16,000
the logical view of lcd

01:30:13,440 --> 01:30:17,840
analyzer and then reads the qa pair to

01:30:16,000 --> 01:30:22,159
persistent about db

01:30:17,840 --> 01:30:25,440
in center route it has these four steps

01:30:22,159 --> 01:30:27,040
so if if the follower received the route

01:30:25,440 --> 01:30:29,920
if you save the rough request

01:30:27,040 --> 01:30:30,719
it will forward it to this either

01:30:29,920 --> 01:30:33,199
remember

01:30:30,719 --> 01:30:34,880
only the leader can make decision the

01:30:33,199 --> 01:30:36,960
forward cannot be decision

01:30:34,880 --> 01:30:39,600
so that's the reason the follower will

01:30:36,960 --> 01:30:42,239
forward the request to the leader

01:30:39,600 --> 01:30:44,080
leader will decide whether to create or

01:30:42,239 --> 01:30:46,239
append a new lock entry

01:30:44,080 --> 01:30:48,159
and then it will replicate this log

01:30:46,239 --> 01:30:50,400
entry to the followers

01:30:48,159 --> 01:30:51,360
follower leader will keep in touch with

01:30:50,400 --> 01:30:53,679
his followers

01:30:51,360 --> 01:30:56,400
to make sure they also commit the log

01:30:53,679 --> 01:30:58,960
entry and then update committee index

01:30:56,400 --> 01:31:01,120
after the majority high school committee

01:30:58,960 --> 01:31:04,080
entry

01:31:01,120 --> 01:31:04,719
after that the data entry will be

01:31:04,080 --> 01:31:08,080
applied

01:31:04,719 --> 01:31:10,400
into in-memory index 3 and also the bot

01:31:08,080 --> 01:31:10,400
db

01:31:10,639 --> 01:31:15,600
as for the range request normally it

01:31:13,520 --> 01:31:16,719
should go through the same precise as

01:31:15,600 --> 01:31:19,760
pull request

01:31:16,719 --> 01:31:20,639
to make sure consistency however since

01:31:19,760 --> 01:31:23,120
it will not

01:31:20,639 --> 01:31:24,719
mutate the kv space there is an

01:31:23,120 --> 01:31:26,719
optimization

01:31:24,719 --> 01:31:29,520
once a wizard server receives the

01:31:26,719 --> 01:31:32,159
request it will get current read index

01:31:29,520 --> 01:31:34,480
and wait until the local store applies

01:31:32,159 --> 01:31:37,199
all entries before the reading index

01:31:34,480 --> 01:31:38,080
and to make sure it is now still the

01:31:37,199 --> 01:31:40,719
root index

01:31:38,080 --> 01:31:43,360
is the committed index of the leader at

01:31:40,719 --> 01:31:45,840
the time the server receives the request

01:31:43,360 --> 01:31:48,000
the range request it stands for the

01:31:45,840 --> 01:31:51,199
latest days of the whole cluster

01:31:48,000 --> 01:31:54,239
so once the local store is not updated

01:31:51,199 --> 01:31:58,480
it is safe and fast read the keyway pair

01:31:54,239 --> 01:32:00,960
just from the local server now let's

01:31:58,480 --> 01:32:03,120
talk about the maintenance operation

01:32:00,960 --> 01:32:05,520
i think people usually get confused on

01:32:03,120 --> 01:32:08,719
compaction and the fragmentation

01:32:05,520 --> 01:32:11,280
both of them works on shrinking density

01:32:08,719 --> 01:32:12,800
but the integration and the object are

01:32:11,280 --> 01:32:15,920
different

01:32:12,800 --> 01:32:17,440
compaction works on k space and it is a

01:32:15,920 --> 01:32:20,320
linear reservoir operation

01:32:17,440 --> 01:32:22,000
which will be proposed to rot so why do

01:32:20,320 --> 01:32:24,880
we need compaction

01:32:22,000 --> 01:32:27,280
since std keeps the exact history of its

01:32:24,880 --> 01:32:28,880
k space we can known as a case-based

01:32:27,280 --> 01:32:30,639
group result control

01:32:28,880 --> 01:32:32,639
this history should be periodically

01:32:30,639 --> 01:32:33,679
compacted to a wide performance

01:32:32,639 --> 01:32:35,360
degradation

01:32:33,679 --> 01:32:38,400
and the eventual storage space

01:32:35,360 --> 01:32:41,120
exhaustion so how does compaction work

01:32:38,400 --> 01:32:42,400
compaction will specify a revision why

01:32:41,120 --> 01:32:43,120
do you set it before the compaction

01:32:42,400 --> 01:32:46,400
revision

01:32:43,120 --> 01:32:48,960
except the latest one will be removed

01:32:46,400 --> 01:32:49,679
compaction is done automatically by sad

01:32:48,960 --> 01:32:53,360
server

01:32:49,679 --> 01:32:57,199
array a couple of periods or every

01:32:53,360 --> 01:32:58,880
10 000 block entry but you can also

01:32:57,199 --> 01:33:02,159
request to do compaction

01:32:58,880 --> 01:33:02,719
yourself with a specific revision then

01:33:02,159 --> 01:33:06,080
the

01:33:02,719 --> 01:33:09,120
blending data will be compacted on this

01:33:06,080 --> 01:33:12,239
specific location

01:33:09,120 --> 01:33:14,159
the fragmentation applies on bulk db

01:33:12,239 --> 01:33:16,400
it is used to release the internal

01:33:14,159 --> 01:33:18,480
fragmentation back to the system

01:33:16,400 --> 01:33:20,000
the internal fragmentation comes from

01:33:18,480 --> 01:33:22,880
the compaction operation

01:33:20,000 --> 01:33:25,120
which is free to use by the backhand but

01:33:22,880 --> 01:33:27,360
i still consume the storage space

01:33:25,120 --> 01:33:29,120
compared to compaction the fragmentation

01:33:27,360 --> 01:33:31,840
physically reclaim the space

01:33:29,120 --> 01:33:32,320
on disk one thing we need to emphasize

01:33:31,840 --> 01:33:34,880
here

01:33:32,320 --> 01:33:35,760
is that the fragmentation will block the

01:33:34,880 --> 01:33:38,560
safety

01:33:35,760 --> 01:33:40,639
system from reading and writing data so

01:33:38,560 --> 01:33:40,960
if you like the whole cluster do the

01:33:40,639 --> 01:33:44,000
frag

01:33:40,960 --> 01:33:47,679
at the same time it will stop the word

01:33:44,000 --> 01:33:50,639
so uh to remember to not likely to do it

01:33:47,679 --> 01:33:51,600
at the same time or it will break the

01:33:50,639 --> 01:33:55,679
principle

01:33:51,600 --> 01:33:58,080
of high availability snapshot creates a

01:33:55,679 --> 01:34:00,000
durable backup on icd members that can

01:33:58,080 --> 01:34:02,639
database periodically

01:34:00,000 --> 01:34:03,600
compaction and fragmentation will remove

01:34:02,639 --> 01:34:06,320
the history

01:34:03,600 --> 01:34:07,600
so on the other hand the snapshot will

01:34:06,320 --> 01:34:10,400
recolor history

01:34:07,600 --> 01:34:11,840
and will be very useful to record some

01:34:10,400 --> 01:34:14,880
issues like

01:34:11,840 --> 01:34:17,360
crafting user snapshot operation

01:34:14,880 --> 01:34:18,080
is done locally so each member in the

01:34:17,360 --> 01:34:20,400
cluster

01:34:18,080 --> 01:34:22,239
can have different standpoint snapshot

01:34:20,400 --> 01:34:24,480
for specific time

01:34:22,239 --> 01:34:26,080
however do remember restoring the

01:34:24,480 --> 01:34:28,080
cluster from the backup

01:34:26,080 --> 01:34:29,600
will be dangerous because you may miss

01:34:28,080 --> 01:34:30,320
the data from the time you take a

01:34:29,600 --> 01:34:34,080
snapshot

01:34:30,320 --> 01:34:34,080
to the time to the current time

01:34:34,400 --> 01:34:38,320
that's all i want to share about this

01:34:36,000 --> 01:34:40,159
std today if you guys are interested in

01:34:38,320 --> 01:34:42,719
std you can go to the website

01:34:40,159 --> 01:34:44,560
and look at the official documentation

01:34:42,719 --> 01:35:01,840
or you can download it itself

01:34:44,560 --> 01:35:01,840
and play with it locally thank you

01:35:05,920 --> 01:35:09,920
thank you chen etcd is one of those

01:35:08,159 --> 01:35:10,960
things that's easy to take for granted

01:35:09,920 --> 01:35:13,119
for many it just

01:35:10,960 --> 01:35:15,119
works and that's a credit to the thought

01:35:13,119 --> 01:35:18,400
design and rigorous effort

01:35:15,119 --> 01:35:21,360
put into it by the cetera d maintainers

01:35:18,400 --> 01:35:23,280
with that okay that's a wrap and we're

01:35:21,360 --> 01:35:25,119
almost time for the after party

01:35:23,280 --> 01:35:26,719
i do have one quick announcement though

01:35:25,119 --> 01:35:28,880
before we can kick that off

01:35:26,719 --> 01:35:30,480
today we've launched a new video series

01:35:28,880 --> 01:35:31,920
learn kubernetes with google

01:35:30,480 --> 01:35:33,679
we'll be covering a wide variety of

01:35:31,920 --> 01:35:36,239
topics and we'll start with some video

01:35:33,679 --> 01:35:38,800
series on the horizontal pod auto scaler

01:35:36,239 --> 01:35:40,800
with that out of the way we can it's now

01:35:38,800 --> 01:35:42,320
time for the after party

01:35:40,800 --> 01:35:43,600
i have a couple little small

01:35:42,320 --> 01:35:44,400
housekeeping i was just going to cover

01:35:43,600 --> 01:35:47,119
first

01:35:44,400 --> 01:35:47,760
we'll uh send an updated google meet

01:35:47,119 --> 01:35:49,440
link to

01:35:47,760 --> 01:35:51,280
join the after party here in a sec

01:35:49,440 --> 01:35:52,719
you'll be able to see a button on the

01:35:51,280 --> 01:35:54,320
agenda page

01:35:52,719 --> 01:35:55,840
today's speakers will also be joining us

01:35:54,320 --> 01:35:56,719
and we'll have some quizzes and some

01:35:55,840 --> 01:35:58,000
other like

01:35:56,719 --> 01:36:11,840
fun activities and other things like

01:35:58,000 --> 01:36:11,840
that i hope to see you all there

01:36:14,719 --> 01:36:16,800

YouTube URL: https://www.youtube.com/watch?v=60fnBk14ifc


