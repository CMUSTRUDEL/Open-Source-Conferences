Title: Google Open Source Live presents Kubernetes (R) | Where the data rests: etcd
Publication date: 2021-01-11
Playlist: Google Open Source Live
Description: 
	
Captions: 
	00:00:10,560 --> 00:00:13,600
Hey there, welcome to the session of etcd.

00:00:13,600 --> 00:00:16,480
I'm Youchen, I'm a software engineer from Google

00:00:16,480 --> 00:00:19,520
and I'm a contributor to the etcd OSS community.

00:00:20,080 --> 00:00:23,680
After listening to the previous sessions about Kubernetes

00:00:23,680 --> 00:00:25,520
I'm sure you already have

00:00:25,520 --> 00:00:31,120
a very clear overall knowledge and impression of Kubernetes.

00:00:31,120 --> 00:00:34,880
So, finally we have reached to the backend data of Kubernetes.

00:00:34,880 --> 00:00:39,920
In this session we will explore etcd, where the Kubernetes data rests.

00:00:40,560 --> 00:00:44,000
I'll give you an overview of the architecture of etcd

00:00:44,000 --> 00:00:47,280
and some key components which I think are very good to know.

00:00:48,480 --> 00:00:50,160
Here is today's agenda.

00:00:50,160 --> 00:00:52,800
First, I'll give an overall introduction to etcd

00:00:52,800 --> 00:00:54,080
to say what is etcd?

00:00:54,640 --> 00:00:58,720
And then I'll talk about the relationship between Kubernetes and etcd

00:00:58,720 --> 00:01:02,640
and explain why Kubernetes uses etcd as its storage.

00:01:03,280 --> 00:01:08,000
And finally, let's have a deeper dive into the technical detail about

00:01:08,000 --> 00:01:11,840
what the etcd request lifecycle looks like.

00:01:13,360 --> 00:01:15,120
First things first, what is etcd?

00:01:16,080 --> 00:01:19,120
From the official website of etcd

00:01:19,120 --> 00:01:22,560
etcd is a distributed, reliable keyâ€“value store

00:01:22,560 --> 00:01:26,240
for the most critical data of a distributed system.

00:01:26,240 --> 00:01:28,640
The name "etcd" comes from two parts.

00:01:29,280 --> 00:01:31,040
The first part is "etc".

00:01:31,040 --> 00:01:33,920
"Etc" comes from the /etc folder in UNIX.

00:01:33,920 --> 00:01:36,000
"D" stands for distributed.

00:01:36,000 --> 00:01:40,640
So it's very easy to understand why etcd is designed for distributed systems

00:01:40,640 --> 00:01:44,400
and storing the most critical data like the configuration data.

00:01:45,280 --> 00:01:47,600
etcd is an open-source project.

00:01:47,600 --> 00:01:51,120
It has had rapid growth over the past five years.

00:01:51,680 --> 00:01:53,840
etcd is a CNCF incubating project.

00:01:54,560 --> 00:01:56,640
It's been adopted by many different projects

00:01:56,640 --> 00:02:00,000
including Kubernetes, which is of course the most famous one.

00:02:01,680 --> 00:02:04,320
etcd focuses on some key features.

00:02:04,320 --> 00:02:08,560
Here, I list three of them, which I think are the most important.

00:02:09,280 --> 00:02:11,200
The first one is consistency.

00:02:11,920 --> 00:02:15,200
etcd ensures strict serializability by default

00:02:15,200 --> 00:02:17,840
which is a strong consistency model.

00:02:17,840 --> 00:02:22,320
It means the operation appears to have occurred in some order

00:02:22,320 --> 00:02:25,920
consistent with the real-time ordering of those operations.

00:02:26,480 --> 00:02:31,280
For example, if operation A completes before operation B begins

00:02:31,280 --> 00:02:33,120
then A should appear to precede B.

00:02:34,160 --> 00:02:37,360
The second feature is high availability.

00:02:37,360 --> 00:02:42,480
The recommended etcd cluster should have more than three cluster members

00:02:42,480 --> 00:02:45,920
and it is designed to replicate data among cluster members.

00:02:46,960 --> 00:02:49,360
You leverage Raft consensus algorithm

00:02:49,360 --> 00:02:52,720
to provide strong consistency and high availability

00:02:53,280 --> 00:02:59,280
which will improve the fault tolerance by avoiding failure from a single point

00:02:59,280 --> 00:03:00,880
or network partitions.

00:03:00,880 --> 00:03:10,820
It is obvious if the cluster has three members

00:03:10,820 --> 00:03:10,872
it will withstand bigger failure.

00:03:10,872 --> 00:03:14,000
etcd has high usability; it is very simple to use.

00:03:14,000 --> 00:03:15,920
You can read or write data

00:03:15,920 --> 00:03:19,760
using standard HTTP or JSON tools such as curl.

00:03:20,320 --> 00:03:25,600
As a vital working datastore, it also has a good performance.

00:03:26,320 --> 00:03:29,520
Here is a benchmark for etcd linearizable read.

00:03:29,520 --> 00:03:33,920
Actually, if you don't mind receiving possible stale data in some condition

00:03:33,920 --> 00:03:37,600
you can specify to use the serializable read.

00:03:38,320 --> 00:03:39,840
It has better performance

00:03:39,840 --> 00:03:43,120
but it will just read data from the local machine

00:03:43,120 --> 00:03:45,600
which means it will allow you to ensure consistency.

00:03:45,600 --> 00:03:49,920
But on the other hand, it has lower latency.

00:03:50,800 --> 00:03:54,720
etcd itself recommends using linearizable rate

00:03:54,720 --> 00:03:56,080
to ensure consistency.

00:03:58,720 --> 00:04:03,040
Consistency is one of the most important selling points of etcd.

00:04:03,040 --> 00:04:06,400
So, how does etcd ensure strong consistency?

00:04:07,360 --> 00:04:11,920
The first principle is that any action to take place has to be quorum

00:04:12,720 --> 00:04:17,360
which means everything should be decided by the majority of the cluster.

00:04:17,360 --> 00:04:20,720
If the cluster has three members, two is a majority

00:04:20,720 --> 00:04:25,120
so everything should be agreed among these two at least.

00:04:25,920 --> 00:04:30,320
It is the reason why etcd suggests using autosized clusters.

00:04:30,320 --> 00:04:31,920
As I mentioned above

00:04:31,920 --> 00:04:34,800
etcd builds on top of Raft consensus algorithms.

00:04:35,360 --> 00:04:39,440
There are two raft algorithm properties that ensure consensus.

00:04:40,400 --> 00:04:45,520
Leader election is not one you may be familiar with in Kubernetes.

00:04:45,520 --> 00:04:49,040
It is to make sure there is only one member in the cluster

00:04:49,040 --> 00:04:50,160
who can make a decision.

00:04:50,880 --> 00:04:53,200
Other members in the cluster are followers.

00:04:53,840 --> 00:04:58,240
The leader needs to send out heartbeats to his followers to keep his authority.

00:04:58,240 --> 00:05:02,080
If the leader goes offline because of a network issue

00:05:02,080 --> 00:05:04,960
the follower will not receive his heartbeats.

00:05:04,960 --> 00:05:07,600
Then, the follower will overturn the previous leadership

00:05:07,600 --> 00:05:09,840
and start a new election as a candidate.

00:05:10,720 --> 00:05:13,440
Basically, the clusters will always have a leader

00:05:13,440 --> 00:05:14,960
or try to elect the leader.

00:05:15,680 --> 00:05:18,080
The cluster will now serve a client request

00:05:18,080 --> 00:05:19,680
unless it has a leader.

00:05:19,680 --> 00:05:22,640
So, sometimes the cluster has high leadership frequency.

00:05:23,280 --> 00:05:26,720
It will strongly reduce the performance of the cluster

00:05:26,720 --> 00:05:29,440
and also signal a potential network issue

00:05:29,440 --> 00:05:30,480
or [UNCLEAR].

00:05:31,600 --> 00:05:34,320
Log replication is another property of Raft.

00:05:34,960 --> 00:05:39,600
Log replication makes sure only the leader can manage the replicated log

00:05:39,600 --> 00:05:41,600
and replicate his log to the followers.

00:05:42,400 --> 00:05:45,120
The replicated log, which is named the WAL log

00:05:45,120 --> 00:05:48,960
should be identical on each machine in the cluster, logically.

00:05:48,960 --> 00:05:50,080
But most of the time

00:05:50,080 --> 00:05:52,880
each cluster member can have a different performance

00:05:52,880 --> 00:05:55,200
and some of them can lay behind.

00:05:55,200 --> 00:05:59,440
So, WAL log records a log that is committed on the majority of machines.

00:05:59,440 --> 00:06:02,640
So for example, if you have a cluster with three members

00:06:03,520 --> 00:06:07,280
WAL log records a log that is at least committed on two of them.

00:06:08,880 --> 00:06:13,120
On the right here is the architecture of the replicated state machine.

00:06:15,440 --> 00:06:18,400
First, the client will send a request

00:06:18,400 --> 00:06:21,840
and the consensus module on the server will receive this request.

00:06:22,400 --> 00:06:24,480
The leader will manage the WAL log

00:06:24,480 --> 00:06:27,280
and will replicate the log entry to other followers.

00:06:28,000 --> 00:06:32,560
Then, each cluster member will apply the committed log entry locally

00:06:32,560 --> 00:06:34,480
and respond to the cluster client.

00:06:36,000 --> 00:06:40,480
Even if we have the above mechanism, the cluster can still end up with corruption.

00:06:41,040 --> 00:06:43,920
The corrupted member is logically part of [UNCLEAR]

00:06:43,920 --> 00:06:46,880
but the backend data is in consensus with each other

00:06:46,880 --> 00:06:51,760
which means the corrupted member is viewed as healthy by the peers

00:06:51,760 --> 00:06:53,120
even if it's [UNCLEAR]

00:06:53,120 --> 00:06:55,600
but its backend data is unhealthy.

00:06:56,480 --> 00:06:59,600
Actually, it is allowed that data on different members

00:06:59,600 --> 00:07:01,840
could have discrepancy at once

00:07:01,840 --> 00:07:08,320
because different machines can have different processing speeds.

00:07:08,320 --> 00:07:11,200
However, a strong, consistent database

00:07:11,200 --> 00:07:15,680
should make sure that data under one revision number should be identical.

00:07:15,680 --> 00:07:20,320
Revision number is important in the concept of etcd.

00:07:21,280 --> 00:07:23,920
I'll mention it later in a different slide.

00:07:25,120 --> 00:07:27,840
Corruption check is built on top of this property.

00:07:27,840 --> 00:07:29,680
It does a hash operation

00:07:29,680 --> 00:07:34,320
and then compares the checksum of each member at one revision.

00:07:34,960 --> 00:07:38,960
The checksum of each member at one revision should be identical.

00:07:40,000 --> 00:07:44,400
To avoid original cluster corruption, we need to corruption check properly

00:07:45,120 --> 00:07:47,440
to make sure the data is consistent.

00:07:47,440 --> 00:07:48,560
It's very important.

00:07:51,360 --> 00:07:55,120
Let's move into our second part, etcd in Kubernetes.

00:07:55,760 --> 00:07:59,600
etcd plays an important role not only as a backend datastore

00:07:59,600 --> 00:08:01,440
but also for server discovery.

00:08:02,000 --> 00:08:05,360
Kubernetes makes use of etcd's "watch" operation

00:08:05,360 --> 00:08:09,920
to synchronize and monitor changes

00:08:09,920 --> 00:08:12,800
to reconfigure itself from "actual" to "desire" state.

00:08:13,440 --> 00:08:15,440
From the deployment perspective

00:08:15,440 --> 00:08:18,800
etcd instances are deployed as pods on the masters.

00:08:19,520 --> 00:08:24,320
In the original Kubernetes cluster, like in the picture on the right

00:08:24,320 --> 00:08:28,400
it has three master nodes and each one has one etcd instance.

00:08:29,120 --> 00:08:33,760
And these three etcd instances can form an etcd cluster

00:08:33,760 --> 00:08:40,240
where they communicate with each other about the leader election or other messages.

00:08:40,240 --> 00:08:43,360
So, why is etcd important to Kubernetes?

00:08:43,360 --> 00:08:46,560
First, because of the data storage function.

00:08:46,560 --> 00:08:50,480
etcd is designed to store the most critical data

00:08:50,480 --> 00:08:55,680
and it has rather high performance and ensures the strongest consistency.

00:08:56,320 --> 00:08:58,880
Second, the "watch" operation of etcd

00:08:58,880 --> 00:09:02,800
makes Kubernetes easier to monitor the "desire" and "actual" states.

00:09:03,360 --> 00:09:06,640
If they diverge, Kubernetes makes changes

00:09:07,360 --> 00:09:11,040
to reconcile the "actual" state and the "desire" state.

00:09:13,600 --> 00:09:16,800
Let's have a deeper dive into the etcd technical details.

00:09:17,600 --> 00:09:19,360
Here is the etcd architecture.

00:09:19,920 --> 00:09:22,320
Everything starts from the client.

00:09:22,320 --> 00:09:25,360
The client of an etcd server could be etcdctl

00:09:27,440 --> 00:09:29,600
which is a command line tool of etcd

00:09:29,600 --> 00:09:32,000
or the API server of Kubernetes.

00:09:32,640 --> 00:09:35,120
The request goes through the client balancer

00:09:35,120 --> 00:09:38,800
and will bring one of the available nodes in the etcd cluster.

00:09:39,600 --> 00:09:44,240
Once a server receives the request, it will send other proposal to the Raft.

00:09:44,800 --> 00:09:47,680
Here we can just build a Raft, as the black box.

00:09:47,680 --> 00:09:50,640
And the upload of Raft is at the same log

00:09:50,640 --> 00:09:54,000
we have replicated in more fails on the majority of the machines.

00:09:54,880 --> 00:10:00,400
The consensus is ensured since all nodes follow the WAL log to apply the request.

00:10:01,600 --> 00:10:06,800
So the last step is that the nodes will apply the WAL log one by one

00:10:06,800 --> 00:10:08,080
and then process to the disk.

00:10:09,440 --> 00:10:11,600
There are two official etcd pods.

00:10:12,320 --> 00:10:15,680
2379 is for client requests

00:10:15,680 --> 00:10:18,080
listening to requests from the client.

00:10:18,080 --> 00:10:21,040
2380 is for peer communication.

00:10:22,160 --> 00:10:24,960
Peers can use this port to communicate with each other

00:10:24,960 --> 00:10:26,240
and exchange messages.

00:10:29,360 --> 00:10:31,600
You may have noticed in the last slide

00:10:31,600 --> 00:10:37,200
the backend of etcd is called MVCC store, aka multi-version concurrency control.

00:10:38,160 --> 00:10:41,520
etcd is designed to store infrequently updating data

00:10:41,520 --> 00:10:43,920
and provide reliable watch queries

00:10:43,920 --> 00:10:47,280
which means etcd is recommended

00:10:47,280 --> 00:10:51,520
to have low write, but more read.

00:10:52,320 --> 00:10:55,680
So, etcd keeps previous versions of KV pairs

00:10:56,720 --> 00:10:59,280
to support inexpensive snapshot

00:10:59,280 --> 00:11:02,640
and a watch history known as time travel query.

00:11:03,200 --> 00:11:05,440
That is [UNCLEAR] wanted to use

00:11:05,440 --> 00:11:08,480
[UNCLEAR] model to address these use cases.

00:11:09,680 --> 00:11:12,880
The critical concept in MVCC is revision.

00:11:13,520 --> 00:11:16,000
Revision is a cluster-wide counter.

00:11:16,000 --> 00:11:19,600
It will increment each time the key space is modified

00:11:19,600 --> 00:11:23,680
which means only put, delete, and transaction operations

00:11:23,680 --> 00:11:25,200
will change the revision number.

00:11:26,160 --> 00:11:29,120
The range request, which is known as the read request

00:11:29,120 --> 00:11:30,800
will not touch the keyspace

00:11:30,800 --> 00:11:32,640
so it will not change the revision number.

00:11:34,560 --> 00:11:37,040
The revision looks like a logical clock.

00:11:37,680 --> 00:11:40,240
It makes sure every member in the cluster

00:11:40,240 --> 00:11:41,840
can keep pace with each other.

00:11:42,560 --> 00:11:45,920
Logically, data is stored in flat binary key space

00:11:45,920 --> 00:11:47,440
which has multiple revisions.

00:11:49,360 --> 00:11:54,080
etcd stores physical data as KV pairs in persistent BoltDB.

00:11:54,080 --> 00:11:57,920
The key of this persistent tree is a revision pair

00:11:57,920 --> 00:11:59,440
composed of the store revision

00:11:59,440 --> 00:12:03,200
as an entity of each key in the same store revision.

00:12:03,200 --> 00:12:05,280
It is not the key from the request.

00:12:06,000 --> 00:12:09,040
Value is a KV pair along with a string of data.

00:12:10,160 --> 00:12:14,320
If the client requests to fetch this KV pair

00:12:14,320 --> 00:12:17,280
that is the same that will be returned to the client ultimately.

00:12:21,040 --> 00:12:25,760
As a database system, etcd has a bunch of operations.

00:12:25,760 --> 00:12:28,640
Here I only list the most important ones.

00:12:28,640 --> 00:12:32,640
The first step is KV operations, the basic KV operations

00:12:32,640 --> 00:12:35,120
which works on the etcd key space.

00:12:35,120 --> 00:12:40,160
Range is the read operation that can read a range of KV pairs [UNCLEAR]

00:12:40,160 --> 00:12:42,000
Put is a read operation

00:12:42,000 --> 00:12:45,520
and Transaction is an is an atomic If/Then/Else construct

00:12:45,520 --> 00:12:47,520
over the keyâ€“value store.

00:12:47,520 --> 00:12:52,240
A transaction can atomically process multiple requests in a single request.

00:12:52,800 --> 00:12:55,120
It is widely used in Kubernetes.

00:12:55,120 --> 00:12:59,440
Since the range and the put are the most common and basic requests

00:12:59,440 --> 00:13:01,520
I will explain more about their life cycle.

00:13:04,000 --> 00:13:06,480
The second term is maintenance operation

00:13:06,480 --> 00:13:10,000
including compaction, defragmentation, and snapshot.

00:13:10,560 --> 00:13:13,600
Watch is to monitor the change of a KV pair.

00:13:13,600 --> 00:13:15,920
Lease is to manage temporary keys.

00:13:15,920 --> 00:13:19,840
In Kubernetes it is used to autodelete "event" resources.

00:13:20,560 --> 00:13:24,800
etcd also has many other operations

00:13:24,800 --> 00:13:27,520
such as authentication operation

00:13:27,520 --> 00:13:30,720
which is used for authentication and authorization.

00:13:30,720 --> 00:13:33,200
If you're interested, you can go to the website

00:13:33,200 --> 00:13:35,280
and read the official documentation.

00:13:37,600 --> 00:13:40,400
Here is the lifecycle of a put request.

00:13:40,960 --> 00:13:44,000
First, the etcd server will receive this request

00:13:44,000 --> 00:13:45,440
from the etcd client.

00:13:45,440 --> 00:13:46,800
It wraps the request

00:13:46,800 --> 00:13:48,960
and sends a Raft request to a Raft cluster.

00:13:49,600 --> 00:13:54,480
Then, each local member will apply the committed Raft log entry

00:13:54,480 --> 00:13:56,080
to the in-memory index tree

00:13:56,080 --> 00:13:58,400
which maintains a logical view of etcd

00:13:59,440 --> 00:14:02,400
and then requests a KV pair to persist on a BoltDB.

00:14:03,040 --> 00:14:05,600
Inside, a Raft has these four steps.

00:14:07,360 --> 00:14:12,240
So, if the follower receives the Raft request

00:14:12,240 --> 00:14:14,560
it will forward it to the leader.

00:14:15,120 --> 00:14:18,240
Remember, only the leader can make decisions.

00:14:18,240 --> 00:14:20,080
The followers cannot make decisions

00:14:20,080 --> 00:14:23,800
so that's the reason why the followers will forward the request to the leader.

00:14:23,800 --> 00:14:29,280
The leader will decide whether to create or append a new log entry

00:14:29,280 --> 00:14:32,560
and then it will replicate this log entry to the followers.

00:14:33,360 --> 00:14:36,560
The leader will keep in touch with his followers

00:14:36,560 --> 00:14:39,520
to make sure they also commit log entry

00:14:39,520 --> 00:14:44,480
and then update the committed index after the majority has committed the entry.

00:14:46,240 --> 00:14:52,080
After that, the log entry will be applied to the in-memory index tree

00:14:52,080 --> 00:14:53,600
and also to the BoltDB.

00:14:55,840 --> 00:14:57,840
As for the range request

00:14:57,840 --> 00:15:01,920
normally it should go through the same process as put requests

00:15:01,920 --> 00:15:03,840
to ensure consistency.

00:15:03,840 --> 00:15:07,680
However, since it will not mutate the KV space

00:15:07,680 --> 00:15:09,120
there is optimization.

00:15:09,920 --> 00:15:14,640
Once a V3 server receives the request, it will get the current read index

00:15:14,640 --> 00:15:19,680
and it will wait until local store applies all entries before the read index

00:15:19,680 --> 00:15:21,520
and to make sure it is not stale.

00:15:22,160 --> 00:15:25,760
The read index is a committed index of the leader

00:15:25,760 --> 00:15:30,080
at the time the server receives a range request.

00:15:30,080 --> 00:15:33,200
It stands for the latest days of the whole cluster

00:15:33,200 --> 00:15:36,400
so once a local store is not updated

00:15:36,400 --> 00:15:40,800
it is safe and fast to read the KV pair just from the local server.

00:15:43,120 --> 00:15:46,160
Now let's talk about the maintenance operation.

00:15:46,160 --> 00:15:50,160
I've seen people easily get confused on compaction and defragmentation.

00:15:50,720 --> 00:15:53,920
Both of them work on shrinking etcd

00:15:53,920 --> 00:15:56,880
but the duration and the object are different.

00:15:58,000 --> 00:15:59,840
Compaction works on keyspace.

00:16:00,720 --> 00:16:04,640
It is a linearizable operation that will be proposed to raft.

00:16:04,640 --> 00:16:06,560
So, why do we need the compaction?

00:16:07,200 --> 00:16:11,040
Since etcd keeps an exact history of its keyspace

00:16:11,040 --> 00:16:13,520
we cannot let the keyspace grow without control.

00:16:14,080 --> 00:16:16,640
This history should be periodically compacted

00:16:16,640 --> 00:16:21,120
to avoid performance degradation and eventual storage space exhaustion.

00:16:21,680 --> 00:16:23,600
So, how does compaction work?

00:16:23,600 --> 00:16:25,760
Compaction will specify a revision.

00:16:26,400 --> 00:16:30,320
Values set before the compaction revision except the latest one will be removed.

00:16:31,600 --> 00:16:34,880
Compaction is done automatically by the etcd server

00:16:34,880 --> 00:16:35,840
every [UNCLEAR]

00:16:36,720 --> 00:16:40,640
or every 10,000 log entries.

00:16:41,360 --> 00:16:44,960
But you can also request to do compaction yourself

00:16:44,960 --> 00:16:47,040
with a specific revision.

00:16:47,040 --> 00:16:52,240
Then the data will be compacted on the specific revision.

00:16:54,560 --> 00:16:56,880
Defragmentation applies on BoltDB.

00:16:57,440 --> 00:17:00,160
It is used to release the internal fragmentation

00:17:00,160 --> 00:17:01,600
back to the system.

00:17:01,600 --> 00:17:05,200
The internal fragmentation comes from the compaction operation

00:17:05,200 --> 00:17:09,680
which is free to use by the backend, but still consumes storage space.

00:17:10,320 --> 00:17:11,760
Compared to compaction

00:17:11,760 --> 00:17:14,800
defragmentation physically reclaims the space on disk.

00:17:15,360 --> 00:17:18,000
One thing we need to emphasize here is that

00:17:18,000 --> 00:17:23,520
defragmentation will block the system from reading and writing data.

00:17:23,520 --> 00:17:27,120
So, if you'd like the whole cluster to defrag at the same time

00:17:27,120 --> 00:17:28,320
it will stop the work.

00:17:29,200 --> 00:17:34,160
So, do remember to not do it at the same time

00:17:34,160 --> 00:17:38,160
or it will break the principle of high availability.

00:17:39,680 --> 00:17:41,920
Snapshot creates a durable backup

00:17:41,920 --> 00:17:44,640
on etcd members' backend database periodically.

00:17:45,200 --> 00:17:48,800
Compaction and defragmentation will remove the history

00:17:48,800 --> 00:17:52,800
but on the other hand, a snapshot will record the history

00:17:52,800 --> 00:17:56,160
and will be very useful to recall an issue

00:17:56,160 --> 00:17:57,920
like a corruption issue.

00:17:58,560 --> 00:18:01,360
Snapshot operation is done locally

00:18:01,360 --> 00:18:06,800
so each member in the cluster can have different snapshots for a specific time.

00:18:07,440 --> 00:18:09,040
However, do remember

00:18:09,040 --> 00:18:12,240
restoring the cluster from the backup will be dangerous

00:18:12,240 --> 00:18:13,840
because you may miss data

00:18:13,840 --> 00:18:17,280
from the time you take a snapshot to the current time.

00:18:19,600 --> 00:18:22,480
That's all I wanted to share about etcd today.

00:18:22,480 --> 00:18:25,360
If you guys are interested in etcd, you can go to the website

00:18:25,360 --> 00:18:27,920
and look at the official documentation

00:18:27,920 --> 00:18:31,440
or you can download it yourself and play with it locally.

00:18:31,440 --> 00:18:41,840

YouTube URL: https://www.youtube.com/watch?v=DY2KPS26k_w


