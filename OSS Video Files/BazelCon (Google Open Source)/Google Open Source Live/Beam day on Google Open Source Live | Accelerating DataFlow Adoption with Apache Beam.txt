Title: Beam day on Google Open Source Live | Accelerating DataFlow Adoption with Apache Beam
Publication date: 2021-05-06
Playlist: Google Open Source Live
Description: 
	
Captions: 
	00:00:00,790 --> 00:00:08,360
[Music]

00:00:10,719 --> 00:00:13,599
hey beam community

00:00:11,920 --> 00:00:15,040
will from wiseline here to talk about

00:00:13,599 --> 00:00:18,720
apache beam

00:00:15,040 --> 00:00:20,560
change data capture and p transforms

00:00:18,720 --> 00:00:22,400
over the last two years wiseline has

00:00:20,560 --> 00:00:24,640
been involved in several apache beam

00:00:22,400 --> 00:00:26,720
related initiatives

00:00:24,640 --> 00:00:27,840
during the apache beam user experience

00:00:26,720 --> 00:00:30,240
research project

00:00:27,840 --> 00:00:32,239
wiseline identified pain points learning

00:00:30,240 --> 00:00:32,960
gaps and opportunities to improve the

00:00:32,239 --> 00:00:36,480
adoption

00:00:32,960 --> 00:00:38,879
of the apache beam platform wiseline was

00:00:36,480 --> 00:00:40,640
able to provide a comprehensive list of

00:00:38,879 --> 00:00:43,200
actions to improve the apache beam

00:00:40,640 --> 00:00:46,000
platform

00:00:43,200 --> 00:00:47,200
the cdc and dibysium project this is the

00:00:46,000 --> 00:00:50,960
focus of today's

00:00:47,200 --> 00:00:52,800
topic this project was executed

00:00:50,960 --> 00:00:55,120
in six weeks and the final product was

00:00:52,800 --> 00:00:57,920
merged in the community

00:00:55,120 --> 00:01:00,320
we're also actively involved in beam sdk

00:00:57,920 --> 00:01:02,239
and infrastructure work

00:01:00,320 --> 00:01:04,159
wiseline is working on some enhancements

00:01:02,239 --> 00:01:06,159
to the core beam sdks

00:01:04,159 --> 00:01:07,520
these contributions are crucial in

00:01:06,159 --> 00:01:10,640
easing the adoption

00:01:07,520 --> 00:01:13,439
of apache beam as a solution now a

00:01:10,640 --> 00:01:16,560
little bit about change data capture

00:01:13,439 --> 00:01:18,560
change data capture or cdc is a term

00:01:16,560 --> 00:01:20,240
for a system that monitors and captures

00:01:18,560 --> 00:01:23,280
the changes in data so that other

00:01:20,240 --> 00:01:25,520
software can respond to those changes

00:01:23,280 --> 00:01:27,439
as some organizations have grown they

00:01:25,520 --> 00:01:28,799
have the need to pull data from a single

00:01:27,439 --> 00:01:30,560
source and distribute it to many

00:01:28,799 --> 00:01:32,640
different applications

00:01:30,560 --> 00:01:34,240
for example identifying changes in the

00:01:32,640 --> 00:01:37,759
database and sending them

00:01:34,240 --> 00:01:39,759
to both an erp and crm system

00:01:37,759 --> 00:01:42,240
traditionally data warehouses required

00:01:39,759 --> 00:01:45,200
custom cdc support to manage sync

00:01:42,240 --> 00:01:47,040
with the upstream oltp databases these

00:01:45,200 --> 00:01:48,880
solutions were custom and cumbersome to

00:01:47,040 --> 00:01:51,040
manage

00:01:48,880 --> 00:01:53,439
two primary tribes of cdc include

00:01:51,040 --> 00:01:54,880
trigger and log base

00:01:53,439 --> 00:01:57,119
and there are a variety of commercial

00:01:54,880 --> 00:01:59,119
cdc vendors available in the market

00:01:57,119 --> 00:02:02,000
today

00:01:59,119 --> 00:02:04,000
dibizium the open source offering is a

00:02:02,000 --> 00:02:05,920
is essentially a modern distributed

00:02:04,000 --> 00:02:07,920
change data capture platform

00:02:05,920 --> 00:02:09,679
that supports monitoring a variety of

00:02:07,920 --> 00:02:11,680
database systems

00:02:09,679 --> 00:02:14,239
debesium is an open source project that

00:02:11,680 --> 00:02:16,959
provides a low latency data streaming

00:02:14,239 --> 00:02:19,120
platform for change data capture

00:02:16,959 --> 00:02:20,800
you set up and configure dibysium to

00:02:19,120 --> 00:02:22,319
monitor your databases

00:02:20,800 --> 00:02:24,480
and then your applications consume

00:02:22,319 --> 00:02:25,840
events for each row level change made to

00:02:24,480 --> 00:02:27,760
the database

00:02:25,840 --> 00:02:29,280
only committed changes are visible so

00:02:27,760 --> 00:02:31,040
your application doesn't have to worry

00:02:29,280 --> 00:02:32,879
about transactions or changes that are

00:02:31,040 --> 00:02:35,120
rolled back

00:02:32,879 --> 00:02:37,120
debesium provides a single model for all

00:02:35,120 --> 00:02:39,200
change events so your application does

00:02:37,120 --> 00:02:40,879
not have to worry about the intricacies

00:02:39,200 --> 00:02:42,800
of each kind of database management

00:02:40,879 --> 00:02:45,440
system

00:02:42,800 --> 00:02:47,280
additionally since debesium records the

00:02:45,440 --> 00:02:48,720
history of data changes in durable

00:02:47,280 --> 00:02:50,319
replicated logs

00:02:48,720 --> 00:02:52,879
your application can be stopped and

00:02:50,319 --> 00:02:54,400
restarted at any time

00:02:52,879 --> 00:02:56,160
it will be able to consume all of the

00:02:54,400 --> 00:02:56,959
events it missed while it was not

00:02:56,160 --> 00:02:58,720
running

00:02:56,959 --> 00:03:00,720
ensuring all data are processed

00:02:58,720 --> 00:03:04,239
correctly and completely

00:03:00,720 --> 00:03:07,360
as of now mysql postgresql sql server

00:03:04,239 --> 00:03:08,560
and db2 are all capable of using cdc

00:03:07,360 --> 00:03:10,640
with beam

00:03:08,560 --> 00:03:13,040
now i'm going to talk a bit about the

00:03:10,640 --> 00:03:15,200
motivation for this work

00:03:13,040 --> 00:03:17,519
the primary motivation for this project

00:03:15,200 --> 00:03:18,800
was to add cdc functionality to apache

00:03:17,519 --> 00:03:20,720
beam

00:03:18,800 --> 00:03:22,400
the commercial offerings are expensive

00:03:20,720 --> 00:03:24,080
and are often closed restricting the

00:03:22,400 --> 00:03:27,680
developers and data engineers from

00:03:24,080 --> 00:03:30,319
building creative and efficient systems

00:03:27,680 --> 00:03:31,519
before the dabisium io beam users that

00:03:30,319 --> 00:03:34,159
wanted to consume

00:03:31,519 --> 00:03:35,040
change data streams from databases

00:03:34,159 --> 00:03:36,720
needed to take

00:03:35,040 --> 00:03:38,319
on one of the following architectural

00:03:36,720 --> 00:03:41,920
dependencies

00:03:38,319 --> 00:03:42,799
kafka by deploying to bzm pub sub and

00:03:41,920 --> 00:03:45,599
data catalog

00:03:42,799 --> 00:03:47,440
by deploying a custom connector or other

00:03:45,599 --> 00:03:49,840
commercial connectors

00:03:47,440 --> 00:03:51,680
by developing a p transform that

00:03:49,840 --> 00:03:53,519
provides this functionality directly in

00:03:51,680 --> 00:03:55,519
beam without any additional

00:03:53,519 --> 00:03:57,120
architectural requirements like like

00:03:55,519 --> 00:03:59,200
kafka or pub sub

00:03:57,120 --> 00:04:01,519
users can get started consuming change

00:03:59,200 --> 00:04:04,080
data streams from their database

00:04:01,519 --> 00:04:04,640
here you can see how using the dibysium

00:04:04,080 --> 00:04:08,319
i o

00:04:04,640 --> 00:04:09,599
reduces complexity in the architecture

00:04:08,319 --> 00:04:11,760
it's important to note that this

00:04:09,599 --> 00:04:15,280
architecture can run in parallel

00:04:11,760 --> 00:04:17,519
with your existing kafka implementations

00:04:15,280 --> 00:04:19,359
here are some details about the solution

00:04:17,519 --> 00:04:22,160
wise line develop

00:04:19,359 --> 00:04:23,600
dybysium connectors are implemented as a

00:04:22,160 --> 00:04:27,680
kafka source connector

00:04:23,600 --> 00:04:29,919
that splits into a single source task

00:04:27,680 --> 00:04:30,720
wiseline developed a dibyzium i o

00:04:29,919 --> 00:04:33,040
transformer

00:04:30,720 --> 00:04:34,400
as a splittable doe fin that reads from

00:04:33,040 --> 00:04:37,440
a variety of sql

00:04:34,400 --> 00:04:39,360
sources using kafka source connector and

00:04:37,440 --> 00:04:43,199
converts the read source record

00:04:39,360 --> 00:04:44,960
into a p collection in json format

00:04:43,199 --> 00:04:47,919
using the existing kafka source

00:04:44,960 --> 00:04:48,560
connector dibysium i o spreads out cdc

00:04:47,919 --> 00:04:51,280
data

00:04:48,560 --> 00:04:52,800
in a kafka record format or source

00:04:51,280 --> 00:04:54,479
record

00:04:52,800 --> 00:04:56,479
these records will be processed by the

00:04:54,479 --> 00:04:57,360
new splitable splitabledofin transformer

00:04:56,479 --> 00:05:00,400
to place them

00:04:57,360 --> 00:05:03,520
in a beam friendly format json

00:05:00,400 --> 00:05:05,440
inside a p collection this p collection

00:05:03,520 --> 00:05:09,199
will be used in the subsequent p

00:05:05,440 --> 00:05:10,960
transform now a bit about the flow of

00:05:09,199 --> 00:05:12,960
events

00:05:10,960 --> 00:05:15,120
beam lets you easily create data

00:05:12,960 --> 00:05:18,400
pipelines by connecting what they call

00:05:15,120 --> 00:05:20,160
transformations each transformation

00:05:18,400 --> 00:05:20,800
delivers a collection of something in

00:05:20,160 --> 00:05:23,759
this case

00:05:20,800 --> 00:05:26,240
records to the next transformation then

00:05:23,759 --> 00:05:28,560
the last transformation does the same

00:05:26,240 --> 00:05:30,639
and so on now let's focus on the

00:05:28,560 --> 00:05:33,520
transformations

00:05:30,639 --> 00:05:34,560
the pipeline has three important steps

00:05:33,520 --> 00:05:37,759
in step one

00:05:34,560 --> 00:05:40,080
the dibesium io or read transform

00:05:37,759 --> 00:05:42,320
reads and captures the cdc events then

00:05:40,080 --> 00:05:42,880
converts the kafka source record cdc

00:05:42,320 --> 00:05:46,320
event

00:05:42,880 --> 00:05:48,960
into a json element in step 2

00:05:46,320 --> 00:05:51,360
the json to table row transformer will

00:05:48,960 --> 00:05:54,160
take that cdc event as json

00:05:51,360 --> 00:05:57,199
and transform it into in this case

00:05:54,160 --> 00:05:59,680
bigquery's table row collection

00:05:57,199 --> 00:06:01,440
finally in step 3 the right transform

00:05:59,680 --> 00:06:02,400
will push out the resultant rows to

00:06:01,440 --> 00:06:05,440
bigquery

00:06:02,400 --> 00:06:06,800
into the specified output table

00:06:05,440 --> 00:06:08,800
here's a bit about wise line's

00:06:06,800 --> 00:06:10,560
engagement process

00:06:08,800 --> 00:06:11,840
first we gathered input from the apache

00:06:10,560 --> 00:06:13,680
beam community

00:06:11,840 --> 00:06:17,120
we identified that cdc could be

00:06:13,680 --> 00:06:19,199
implemented in beam via dibyzium

00:06:17,120 --> 00:06:21,280
artifacts were created in uml to

00:06:19,199 --> 00:06:22,720
validate our understanding

00:06:21,280 --> 00:06:24,160
we then kicked off our six-week

00:06:22,720 --> 00:06:26,800
development phase using agile

00:06:24,160 --> 00:06:28,720
methodologies

00:06:26,800 --> 00:06:30,080
we conducted periodic code reviews with

00:06:28,720 --> 00:06:32,840
the community

00:06:30,080 --> 00:06:34,560
and contributed via unit and integration

00:06:32,840 --> 00:06:36,240
testing

00:06:34,560 --> 00:06:38,400
after final review we were able to push

00:06:36,240 --> 00:06:39,840
the code to the community

00:06:38,400 --> 00:06:42,240
now we're going to take a brief look at

00:06:39,840 --> 00:06:43,840
the code we're looking at the latest

00:06:42,240 --> 00:06:47,919
debesium io

00:06:43,840 --> 00:06:48,240
dot java file in the beam repository

00:06:47,919 --> 00:06:51,280
here

00:06:48,240 --> 00:06:51,840
tobesium io declares its read t class

00:06:51,280 --> 00:06:55,520
function

00:06:51,840 --> 00:06:57,599
which extends from a p transform

00:06:55,520 --> 00:07:00,000
here we see the overridden expand

00:06:57,599 --> 00:07:03,520
function

00:07:00,000 --> 00:07:05,840
on line 213 a kafka source consumer

00:07:03,520 --> 00:07:06,880
function is instantiated passing the

00:07:05,840 --> 00:07:08,639
configuration

00:07:06,880 --> 00:07:09,919
format function and some other

00:07:08,639 --> 00:07:12,160
restrictions

00:07:09,919 --> 00:07:14,800
the kafka source consumer function is a

00:07:12,160 --> 00:07:16,400
splittable dofin used to process records

00:07:14,800 --> 00:07:18,240
fetched from supported to bezium

00:07:16,400 --> 00:07:20,880
connectors

00:07:18,240 --> 00:07:23,120
now we're going to review kafka source

00:07:20,880 --> 00:07:25,360
consumer process function

00:07:23,120 --> 00:07:26,960
kafka source consumer process function

00:07:25,360 --> 00:07:28,840
passes configuration

00:07:26,960 --> 00:07:30,080
and instantiates the kafka source

00:07:28,840 --> 00:07:33,039
connector

00:07:30,080 --> 00:07:34,720
on line 155 the connector instantiates

00:07:33,039 --> 00:07:36,319
the kafka source task

00:07:34,720 --> 00:07:38,080
which controls the execution of the

00:07:36,319 --> 00:07:41,759
connector

00:07:38,080 --> 00:07:43,680
on line 161 and 162 we initialize and

00:07:41,759 --> 00:07:47,360
start the task

00:07:43,680 --> 00:07:49,759
on line 164 record polling begins

00:07:47,360 --> 00:07:52,000
kafka source record from the connector's

00:07:49,759 --> 00:07:53,919
internal queue

00:07:52,000 --> 00:07:55,520
the kafka source record is then passed

00:07:53,919 --> 00:07:56,639
by the configured map function to

00:07:55,520 --> 00:07:59,039
transform it into

00:07:56,639 --> 00:08:00,240
a json representation which is the

00:07:59,039 --> 00:08:02,960
return type of the p

00:08:00,240 --> 00:08:04,000
collection now i'm going to show you how

00:08:02,960 --> 00:08:06,960
to stand up

00:08:04,000 --> 00:08:08,000
a very simple pipeline using apache beam

00:08:06,960 --> 00:08:12,160
debesium

00:08:08,000 --> 00:08:14,479
mysql and gcp here's a very high level

00:08:12,160 --> 00:08:15,759
of my environment i'm using a

00:08:14,479 --> 00:08:18,960
development workstation

00:08:15,759 --> 00:08:20,960
running debian 10 and maven and some

00:08:18,960 --> 00:08:22,639
other minor dependencies

00:08:20,960 --> 00:08:24,960
for testing make sure this environment

00:08:22,639 --> 00:08:26,639
has correct access to both source and

00:08:24,960 --> 00:08:29,599
target

00:08:26,639 --> 00:08:31,759
i have a mysql database via docker image

00:08:29,599 --> 00:08:32,959
as my source these test dockers

00:08:31,759 --> 00:08:35,120
are available in the debesium

00:08:32,959 --> 00:08:36,959
repositories

00:08:35,120 --> 00:08:40,560
i'm using bigquery as the destination

00:08:36,959 --> 00:08:42,159
for my sql data after transformation

00:08:40,560 --> 00:08:44,800
the first thing i'll do is configure my

00:08:42,159 --> 00:08:47,200
data source in this case i'm using one

00:08:44,800 --> 00:08:49,040
of the debesium example mysql containers

00:08:47,200 --> 00:08:51,279
version 1.3

00:08:49,040 --> 00:08:52,959
the example mysql docker has a very sim

00:08:51,279 --> 00:08:54,240
simple schema that is perfect for

00:08:52,959 --> 00:08:56,000
testing

00:08:54,240 --> 00:08:58,399
when considering your test setup you

00:08:56,000 --> 00:08:59,839
should make sure that tcp port access is

00:08:58,399 --> 00:09:03,680
available from your source

00:08:59,839 --> 00:09:05,120
to target sync the docker command

00:09:03,680 --> 00:09:06,720
defines the database connection

00:09:05,120 --> 00:09:09,760
parameters which will be used in

00:09:06,720 --> 00:09:13,519
standing up the pipeline in later steps

00:09:09,760 --> 00:09:16,000
now let's set up bigquery log into gcp

00:09:13,519 --> 00:09:17,040
from the bigquery explorer browse to

00:09:16,000 --> 00:09:20,640
your data set

00:09:17,040 --> 00:09:23,600
and create a new table under schema

00:09:20,640 --> 00:09:26,080
click add field and under name enter

00:09:23,600 --> 00:09:26,080
json

00:09:34,399 --> 00:09:38,240
let's prepare the pipeline configuration

00:09:36,240 --> 00:09:39,880
on the development workstation

00:09:38,240 --> 00:09:41,839
we are viewing step one in the

00:09:39,880 --> 00:09:43,839
project.java file

00:09:41,839 --> 00:09:46,800
in this case it's one of the samples

00:09:43,839 --> 00:09:48,800
provided by dibysia

00:09:46,800 --> 00:09:51,279
here step one defines the dibyzium

00:09:48,800 --> 00:09:53,120
connector configuration

00:09:51,279 --> 00:09:56,959
the connector configurations contain

00:09:53,120 --> 00:09:58,240
options class and property references

00:09:56,959 --> 00:10:01,440
these are the options that will be

00:09:58,240 --> 00:10:04,959
passed at the command line with maven

00:10:01,440 --> 00:10:08,000
let's take a look at steps two and three

00:10:04,959 --> 00:10:09,680
in step two the i o transforms the json

00:10:08,000 --> 00:10:11,839
into table row

00:10:09,680 --> 00:10:15,200
and in step three the table row data is

00:10:11,839 --> 00:10:17,519
appended to the bigquery output table

00:10:15,200 --> 00:10:19,200
now let's kick off the pipeline in order

00:10:17,519 --> 00:10:20,399
to do so we need to stage our maven

00:10:19,200 --> 00:10:22,560
command

00:10:20,399 --> 00:10:24,079
this command defines the template data

00:10:22,560 --> 00:10:27,839
runner google project

00:10:24,079 --> 00:10:29,760
source jdbc connection info and more

00:10:27,839 --> 00:10:31,839
notice the bigquery temporary directory

00:10:29,760 --> 00:10:34,320
this will need to be configured prior to

00:10:31,839 --> 00:10:34,320
testing

00:10:35,760 --> 00:10:40,320
after the maven command is set up i will

00:10:37,519 --> 00:10:41,760
launch the pipeline

00:10:40,320 --> 00:10:46,320
you will see the pipe building on the

00:10:41,760 --> 00:10:48,480
bottom of the screen

00:10:46,320 --> 00:10:50,800
now i will log into gcp data flow on the

00:10:48,480 --> 00:10:50,800
right

00:10:51,279 --> 00:10:55,200
you will notice the data flow job up and

00:10:58,839 --> 00:11:01,839
running

00:11:02,640 --> 00:11:05,920
let's check bigquery to see if we have

00:11:04,720 --> 00:11:08,640
data

00:11:05,920 --> 00:11:10,240
we do have data if you analyze the logs

00:11:08,640 --> 00:11:11,839
you can see verbose information about

00:11:10,240 --> 00:11:12,959
the pipeline and the transformations

00:11:11,839 --> 00:11:14,560
occurring

00:11:12,959 --> 00:11:16,160
you may have the need to convert strings

00:11:14,560 --> 00:11:17,120
of data in some manner before they land

00:11:16,160 --> 00:11:19,680
on the target

00:11:17,120 --> 00:11:21,440
for example your pipeline might receive

00:11:19,680 --> 00:11:24,000
a date value in pst

00:11:21,440 --> 00:11:25,120
and you would like to convert it to utc

00:11:24,000 --> 00:11:28,079
at your target

00:11:25,120 --> 00:11:29,680
this is possible with this technology

00:11:28,079 --> 00:11:31,519
within the pipeline logging you can see

00:11:29,680 --> 00:11:35,200
the before and after value of the data

00:11:31,519 --> 00:11:35,200
as it transfers the pipeline

00:11:35,600 --> 00:11:40,839
one thing to test is to make an insert

00:11:37,360 --> 00:11:43,040
at the source to see if it populates

00:11:40,839 --> 00:11:45,519
bigquery

00:11:43,040 --> 00:11:46,959
that wraps up my quick demo i hope that

00:11:45,519 --> 00:11:48,000
after going through this with me you

00:11:46,959 --> 00:11:50,880
have a better understanding of

00:11:48,000 --> 00:11:52,959
wiseline's contribution to apache beam

00:11:50,880 --> 00:11:54,959
developing pipelines using apache beam

00:11:52,959 --> 00:11:56,959
allows organizations many benefits

00:11:54,959 --> 00:12:00,160
including data portability

00:11:56,959 --> 00:12:02,079
flexibility and scalability wisely is

00:12:00,160 --> 00:12:04,639
actively working on apache beam related

00:12:02,079 --> 00:12:06,160
projects and will continue to contribute

00:12:04,639 --> 00:12:07,920
i have added some links at the bottom of

00:12:06,160 --> 00:12:09,040
this document if you would like to learn

00:12:07,920 --> 00:12:11,200
more

00:12:09,040 --> 00:12:25,680
thank you apache beam community and have

00:12:11,200 --> 00:12:25,680

YouTube URL: https://www.youtube.com/watch?v=cBm01670TF4


