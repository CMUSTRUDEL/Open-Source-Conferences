Title: Google Open Source Live "Knative day" | Knative Sources - Events from Everywhere [Intermediate]
Publication date: 2020-11-02
Playlist: Google Open Source Live
Description: 
	Eventing systems are only as powerful as the events they receive. Knative Sources generate or import events into the cluster as CloudEvents. Learn about existing Knative Sources, how they work, and how to write your own Sources.
Captions: 
	00:00:10,700 --> 00:00:12,080
Hi, I'm Adam.

00:00:12,080 --> 00:00:13,730
Hi' I'm Nacho.

00:00:13,730 --> 00:00:16,740
We're both software engineers working at Google on Knative

00:00:16,740 --> 00:00:18,630
specifically, Knative Eventing.

00:00:18,630 --> 00:00:21,250
This session is about Knative Sources

00:00:21,250 --> 00:00:24,910
which is a core construct in Knative Eventing.

00:00:24,910 --> 00:00:27,800
The previous session is more of an overview of event-driven architecture

00:00:27,800 --> 00:00:29,450
and Knative Eventing in general.

00:00:29,450 --> 00:00:31,520
The following session is a deep technical dive

00:00:31,520 --> 00:00:33,930
on the broker component within Knative Eventing.

00:00:33,930 --> 00:00:37,190
In case you haven't attended the previous session

00:00:37,190 --> 00:00:40,010
let's start with some brief context of Knative.

00:00:40,010 --> 00:00:43,250
Knative us a Kubernetes-based platform

00:00:43,250 --> 00:00:46,410
to deploy and manage serverless workloads.

00:00:46,410 --> 00:00:47,950
It's built on top of Kubernetes

00:00:47,950 --> 00:00:52,780
with the idea of abstracting away complex Kubernetes details from developers

00:00:52,780 --> 00:00:57,410
to enable them to focus on what matters most: their business logic.

00:00:57,410 --> 00:00:59,700
It has two core components.

00:00:59,700 --> 00:01:04,170
One is Knative Serving and the other is Knative Eventing.

00:01:04,170 --> 00:01:09,439
Knative Serving allows for rapid deployment of stateless HTTP containers

00:01:09,439 --> 00:01:13,820
it supports auto-scanning of those containers from 0 to n

00:01:13,820 --> 00:01:20,890
and it also deals with routing and managing traffic splits during deployment.

00:01:20,890 --> 00:01:23,530
For example, if you deploy a new version of your service

00:01:23,530 --> 00:01:26,640
and you only want 10% of the traffic to go there

00:01:26,640 --> 00:01:28,679
then 20%, then 30%, and so on

00:01:28,679 --> 00:01:31,340
you can use Knative Serving.

00:01:31,340 --> 00:01:34,509
Knative Eventing, in turn, takes care of the subscription

00:01:34,509 --> 00:01:38,289
delivery and management of events

00:01:38,289 --> 00:01:41,990
and it enables event-based triggering of your workloads.

00:01:41,990 --> 00:01:44,520
Both Knative Serving and Knative Eventing are great on their own

00:01:44,520 --> 00:01:47,090
but they also complement each other well.

00:01:47,090 --> 00:01:49,409
Using them together means that you can easily have

00:01:49,409 --> 00:01:52,869
Knative Serving respond to things other direct HTTP requests

00:01:52,869 --> 00:01:55,799
and allows Eventing to automatically scale up to the peaks of demand

00:01:55,799 --> 00:02:00,659
and scale down to zero when no demand is present.

00:02:00,659 --> 00:02:02,829
Knative Eventing is made up of composable primitives

00:02:02,829 --> 00:02:06,700
that allow late binding between event sources and event consumers.

00:02:06,700 --> 00:02:09,259
Whenever we speak about an event within Knative Eventing

00:02:09,259 --> 00:02:12,530
they always follow the Cloud Events standard.

00:02:12,530 --> 00:02:16,370
Event sources produce Cloud Events, while event consumers consume them.

00:02:16,370 --> 00:02:18,170
Broker and trigger are intermediaries

00:02:18,170 --> 00:02:21,220
that allow interested event consumers to receive Cloud Events

00:02:21,220 --> 00:02:24,450
without the source having to know about each individual consumer.

00:02:24,450 --> 00:02:26,780
As mentioned in the previous slide

00:02:26,780 --> 00:02:29,090
Knative Eventing speaks Cloud Events.

00:02:29,090 --> 00:02:31,739
This gives us some advantages.

00:02:31,739 --> 00:02:32,909
One is consistency

00:02:32,909 --> 00:02:35,790
because the lack of a common way of describing events

00:02:35,790 --> 00:02:39,500
will mean that developers have to write their own event-handling logic

00:02:39,500 --> 00:02:41,090
for every new source.

00:02:41,090 --> 00:02:43,810
The other one is accessibility.

00:02:43,810 --> 00:02:48,769
If you think about it, if you don't have a common way of expressing events

00:02:48,769 --> 00:02:51,540
it means there are no common libraries, no common tooling

00:02:51,540 --> 00:02:53,269
and no common infrastructure

00:02:53,269 --> 00:02:57,430
for delivering event data across different environments.

00:02:57,430 --> 00:03:00,760
It's worth noting that Cloud Events provides several SDKs

00:03:00,760 --> 00:03:02,439
in different languages

00:03:02,439 --> 00:03:05,819
such as Python, Go, JavaScript, etc.

00:03:05,819 --> 00:03:07,730
The other advantage is portability

00:03:07,730 --> 00:03:10,760
because your containers can be moved to other environments

00:03:10,760 --> 00:03:13,969
that are also Cloud Events-friendly.

00:03:13,969 --> 00:03:17,310
Cloud Events have context attributes and data.

00:03:17,310 --> 00:03:21,770
You can think of context attributes as metadata.

00:03:21,770 --> 00:03:23,989
The context attributes are designed in such a way

00:03:23,989 --> 00:03:28,090
that they can be serialized independently of the event data.

00:03:28,090 --> 00:03:32,750
This allows them to inspect it without having to deserialize the data

00:03:32,750 --> 00:03:34,989
for example, to make routing decisions.

00:03:34,989 --> 00:03:38,790
Here, we have an example where we see some context attributes

00:03:38,790 --> 00:03:42,930
such as type, or the source of where the event came from

00:03:42,930 --> 00:03:45,609
the spec version, and so on.

00:03:45,609 --> 00:03:48,269
There are also multiple protocol bindings

00:03:48,269 --> 00:03:52,640
that define how to map an event to a specific protocol message.

00:03:52,640 --> 00:03:56,799
For example, the HTTP Protocol Binding for Cloud Events

00:03:56,799 --> 00:04:02,239
defines how Cloud Events are mapped from HTTP requests and response messages

00:04:02,239 --> 00:04:04,680
or the Kafka Protocol Binding

00:04:04,680 --> 00:04:07,810
which defines how you can map events from Kafka messages

00:04:07,810 --> 00:04:12,629
to Cloud Events, and so on.

00:04:12,629 --> 00:04:14,139
Given that general background

00:04:14,139 --> 00:04:16,620
we can now focus on the main topic of this session

00:04:16,620 --> 00:04:19,370
which is Knative Sources.

00:04:19,370 --> 00:04:21,600
Here's the proposed agenda for the talk.

00:04:21,600 --> 00:04:25,330
We'll start with what Knative Sources are and why we need them

00:04:25,330 --> 00:04:29,000
as well as some of the design choices we made along the way.

00:04:29,000 --> 00:04:34,220
We'll also go over some other sources that are available out there for you to use.

00:04:34,220 --> 00:04:37,610
We'll also go over more details on how you can implement your own source

00:04:37,610 --> 00:04:40,640
if none of the available ones fit your purpose.

00:04:40,640 --> 00:04:42,920
And we'll finally conclude.

00:04:42,920 --> 00:04:49,780
So, let's get started with Knative Sources.

00:04:49,780 --> 00:04:53,220
Let's start with the fundamental question of why we need Knative Sources.

00:04:53,220 --> 00:04:55,620
The answer is pretty simple.

00:04:55,620 --> 00:04:59,040
Knative Eventing adopted usage of Cloud Events.

00:04:59,040 --> 00:05:02,970
However, most producers still use their own event formats

00:05:02,970 --> 00:05:05,400
so there's an impedance mismatch.

00:05:05,400 --> 00:05:10,790
We need Knative Sources to be able to leverage existing non-Cloud Events producers

00:05:10,790 --> 00:05:12,820
into this Cloud Events world.

00:05:12,820 --> 00:05:15,370
If we want more breadth of producers

00:05:15,370 --> 00:05:17,720
then we need something that would allow us to

00:05:17,720 --> 00:05:22,790
convert event formats into Cloud Events.

00:05:22,790 --> 00:05:24,080
Breadth is very important here

00:05:24,080 --> 00:05:28,410
because at the end of the day eventing systems, regardless of their quality

00:05:28,410 --> 00:05:31,730
are limited by the events that are available to them.

00:05:31,730 --> 00:05:35,340
What are Knative Sources?

00:05:35,340 --> 00:05:38,270
Knative Sources are constructs that produce or import events

00:05:38,270 --> 00:05:40,900
into the cluster as Cloud Events.

00:05:40,900 --> 00:05:44,380
They convert incoming proprietary events into Cloud Events

00:05:44,380 --> 00:05:47,210
and send them downstream.

00:05:47,210 --> 00:05:49,970
Downstream can be any HTTP addressable.

00:05:49,970 --> 00:05:52,380
We also call them "sinks."

00:05:52,380 --> 00:05:56,020
For example, you can be an end consumer, such as a Knative Service

00:05:56,020 --> 00:05:59,890
or a middleware, like, for example, the Knative broker.

00:05:59,890 --> 00:06:03,020
We talked a bit about the "why," the "what"â€¦

00:06:03,020 --> 00:06:05,330
Now let's delve more into the "how."

00:06:05,330 --> 00:06:07,750
Specifically, how are Knative Sources implemented?

00:06:07,750 --> 00:06:11,580
Knative is designed to be as native to Kubernetes as possible

00:06:11,580 --> 00:06:14,030
hence our name, "Knative."

00:06:14,030 --> 00:06:16,000
Knative uses the standard way of extending Kubernetes.

00:06:16,000 --> 00:06:18,500
The custom resource definition, or CRD.

00:06:18,500 --> 00:06:20,520
CRDs define new resource types

00:06:20,520 --> 00:06:23,000
similar to a class in a programming language.

00:06:23,000 --> 00:06:25,400
We then can create instances of this CRD

00:06:25,400 --> 00:06:27,150
called a custom object, or CO.

00:06:27,150 --> 00:06:30,830
These are similar to instances in a programming language.

00:06:30,830 --> 00:06:32,460
If you would like more information about this

00:06:32,460 --> 00:06:35,390
the link below has the official documentation about CRDs.

00:06:35,390 --> 00:06:39,080
Broadly speaking, we have two types of sources

00:06:39,080 --> 00:06:41,520
push-based and pull-based sources.

00:06:41,520 --> 00:06:42,520
Push-based sources are ones where

00:06:42,520 --> 00:06:46,240
an upstream event producer pushes an event into our source

00:06:46,240 --> 00:06:47,610
such as GitHubSource

00:06:47,610 --> 00:06:52,390
where GitHub makes an HTTP request to our source.

00:06:52,390 --> 00:06:54,990
This has the downside that we must expose an end point

00:06:54,990 --> 00:06:56,960
for our source to hit.

00:06:56,960 --> 00:06:59,080
It doesn't necessarily have to be on the public Internet

00:06:59,080 --> 00:07:01,260
but it does have to be somewhere that the producer can reach.

00:07:01,260 --> 00:07:04,440
One of the large upsides is these are much easier to scale.

00:07:04,440 --> 00:07:06,960
We can leverage things like Knative Serving

00:07:06,960 --> 00:07:10,620
which already does this, to scale up, to meet demand

00:07:10,620 --> 00:07:12,360
and scale down to zero when there isn't any.

00:07:12,360 --> 00:07:14,780
Pull-based sources, on the other hand

00:07:14,780 --> 00:07:17,240
are pulling events from the upstream producer.

00:07:17,240 --> 00:07:21,900
They can either be doing so on a continuous basis or on a periodic basis.

00:07:21,900 --> 00:07:23,900
They need network access to the event producer

00:07:23,900 --> 00:07:26,360
but they don't necessarily have to expose an end point themselves.

00:07:26,360 --> 00:07:29,090
In general, these are more difficult to scale

00:07:29,090 --> 00:07:33,260
because we need our own way to define how congested it is.

00:07:33,260 --> 00:07:35,780
It also means that something is either continuously running

00:07:35,780 --> 00:07:40,680
or at least periodically running in the background.

00:07:40,680 --> 00:07:45,680
While developing Knative, we tried two different CRD models for our sources.

00:07:45,680 --> 00:07:47,970
The first we tried was called the Provisioner Model.

00:07:47,970 --> 00:07:51,040
We had a single CRD, named "Source."

00:07:51,040 --> 00:07:55,000
Inside the specification of that CRD, there was a field called provisioner.

00:07:55,000 --> 00:07:57,240
Every custom object that was made would fill in that field

00:07:57,240 --> 00:07:59,520
and that field would determine what kind of source it was.

00:07:59,520 --> 00:08:03,390
It might be GitHub, or Kafka, or something else.

00:08:03,390 --> 00:08:05,600
This made it very easy for our Knative core code

00:08:05,600 --> 00:08:06,880
to interact with sources.

00:08:06,880 --> 00:08:09,680
There's only one type and we knew it at compile time.

00:08:09,680 --> 00:08:10,680
This tended to work pretty well

00:08:10,680 --> 00:08:14,050
as long as the parameters for our sources were very similar.

00:08:14,050 --> 00:08:16,060
As the number of sources increased

00:08:16,060 --> 00:08:17,750
the parameters started to diverge.

00:08:17,750 --> 00:08:19,980
We had to add a generic parameter map.

00:08:19,980 --> 00:08:24,040
This made it very difficult for users to understand what they needed to configure

00:08:24,040 --> 00:08:26,770
or what even could be configured for a given source type.

00:08:26,770 --> 00:08:29,320
It was also more difficult to figure out

00:08:29,320 --> 00:08:31,710
what sources were even available to begin with.

00:08:31,710 --> 00:08:33,899
The next model we tried was called the Operator Model.

00:08:33,899 --> 00:08:38,370
In this model, each source has its own CRD type.

00:08:38,370 --> 00:08:41,610
For example, KafkaSource, MongoDbSource, and GitHubSource

00:08:41,610 --> 00:08:43,090
are all their own CRDs.

00:08:43,090 --> 00:08:46,690
This made it much easier for users to understand what was installed in the cluster

00:08:46,690 --> 00:08:49,670
because they could use their native Kubernetes interactions

00:08:49,670 --> 00:08:51,960
such as listing CRDs.

00:08:51,960 --> 00:08:53,620
It also made it much more straightforward

00:08:53,620 --> 00:08:56,620
for users to understand how to use these sources

00:08:56,620 --> 00:08:58,930
because they could introspect the CRD itself

00:08:58,930 --> 00:09:01,590
which told users exactly what fields and parameters

00:09:01,590 --> 00:09:03,840
were available for customization.

00:09:03,840 --> 00:09:06,700
The big downside of this was that our Knative core code

00:09:06,700 --> 00:09:10,760
had to deal with objects at runtime that it didn't know about at compile time.

00:09:10,760 --> 00:09:15,390
In the end, we decided to take that extra complexity

00:09:15,390 --> 00:09:19,020
on the Knative core code and make life better for our users.

00:09:19,020 --> 00:09:21,320
We decided to go with the Operator Model.

00:09:21,320 --> 00:09:23,500
Although we went with the Operator Model

00:09:23,500 --> 00:09:25,710
where we have different CRDs for different sources

00:09:25,710 --> 00:09:31,170
we also wanted to have all sources share a similar shape.

00:09:31,170 --> 00:09:34,280
This would allow us to treat them polymorphically in code

00:09:34,280 --> 00:09:37,880
or for instance, a cluster operater would be able to inspect the resources

00:09:37,880 --> 00:09:40,380
without having to be fully aware of the implementation.

00:09:40,380 --> 00:09:43,000
It will also allow us to build common libraries

00:09:43,000 --> 00:09:46,590
that could be reused all over.

00:09:46,590 --> 00:09:48,670
For this, we use duck typing.

00:09:48,670 --> 00:09:51,130
Duck typing, in the Kubernetes world

00:09:51,130 --> 00:09:55,240
is a technique that allows you to define a partial schema of an object.

00:09:55,240 --> 00:09:59,010
In the example below, we see three json objects.

00:09:59,010 --> 00:10:02,300
The one on the left-hand side has a foo and bar property

00:10:02,300 --> 00:10:04,500
the one in the middle has a baz property

00:10:04,500 --> 00:10:08,180
and one on the right has a spam and ham property.

00:10:08,180 --> 00:10:12,310
But all of them have the same Knative property

00:10:12,310 --> 00:10:15,700
we have Eventing and Serving.

00:10:15,700 --> 00:10:18,450
If you want to reason about that Knative property

00:10:18,450 --> 00:10:21,980
in all of these different resources, we can use duck typing.

00:10:21,980 --> 00:10:27,360
We can use duck typing to extract that partial schema.

00:10:27,360 --> 00:10:29,800
And that's exactly what we did with our sources.

00:10:29,800 --> 00:10:34,380
Although we have different CRDs and each CRD has a specific attribute

00:10:34,380 --> 00:10:36,480
all sources share a partial schema.

00:10:36,480 --> 00:10:40,279
They all implement what we call the source duck type.

00:10:40,279 --> 00:10:42,290
Here, I'm showing three different objects

00:10:42,290 --> 00:10:45,740
a KafkaSource, a PingSource, and a MongoDbSource

00:10:45,740 --> 00:10:47,720
Although they are different

00:10:47,720 --> 00:10:54,740
you can see that they all have this sink attribute, marked in red

00:10:54,740 --> 00:11:01,040
where the sources can specify the sink where they will send the event to.

00:11:01,040 --> 00:11:04,320
They can optionally specify a Cloud Event Overrides

00:11:04,320 --> 00:11:06,910
which is in dotted red rectangles there.

00:11:06,910 --> 00:11:11,730
These are attributes that are added or overridden in the outbound events.

00:11:11,730 --> 00:11:15,100
Although they share these common attributes

00:11:15,100 --> 00:11:19,930
it's worth noting that each of these objects defines its own specific attributes.

00:11:19,930 --> 00:11:21,270
For example, with the Kafka one

00:11:21,270 --> 00:11:24,710
you can specify a consumerGroup in the spec

00:11:24,710 --> 00:11:27,890
or in PingSource, you can specify a cron schedule.

00:11:27,890 --> 00:11:34,860
Even in the MongoDBSource, you can specify a database, for instance.

00:11:34,860 --> 00:11:38,090
Having introduced the Operator Model and the source duck type

00:11:38,090 --> 00:11:40,270
we will now talk a little bit about

00:11:40,270 --> 00:11:42,840
what it means to be a compliant Knative Source.

00:11:42,840 --> 00:11:45,740
In order to be a compliant Knative source

00:11:45,740 --> 00:11:48,920
you need to conform to the sources specification.

00:11:48,920 --> 00:11:51,850
We added a link here for your reference.

00:11:51,850 --> 00:11:56,540
Roughly speaking, the spec talks about CRD-level requirements

00:11:56,540 --> 00:11:59,690
and custom-object-level requirements.

00:11:59,690 --> 00:12:01,900
Regarding the CRD-level requirements

00:12:01,900 --> 00:12:06,460
we required that CRDs for sources have a particular label

00:12:06,460 --> 00:12:08,950
specifying that they're actually sources.

00:12:08,950 --> 00:12:11,550
They have to have a sources category.

00:12:11,550 --> 00:12:13,800
They should also have an annotation

00:12:13,800 --> 00:12:18,270
where they can specify the type of event they can produce.

00:12:18,270 --> 00:12:20,170
Regarding the custom object requirements

00:12:20,170 --> 00:12:23,590
they basically need to implement the source duck type

00:12:23,590 --> 00:12:29,190
and it has that spec.sink that we talked about in the previous slide

00:12:29,190 --> 00:12:30,850
plus some status condition

00:12:30,850 --> 00:12:34,170
that allows you to specify whether the source is ready or not

00:12:34,170 --> 00:12:37,160
as well as the resolved sink URI.

00:12:37,160 --> 00:12:39,920
Now that we've talked a bit about what a source is

00:12:39,920 --> 00:12:43,800
let's see what sources are already available.

00:12:43,800 --> 00:12:47,560
Core sources are the ones that are maintained by the Knative Eventing project itself.

00:12:47,560 --> 00:12:50,960
We have four of them.

00:12:50,960 --> 00:12:52,720
The ApiServerSource is a source for Kubernetes Events

00:12:52,720 --> 00:12:54,890
a resource within the Kubernetes cluster

00:12:54,890 --> 00:13:00,830
that represents the creation, update, or deletion of a resource within the cluster.

00:13:00,830 --> 00:13:03,250
In order to create an ApiServerSource

00:13:03,250 --> 00:13:07,260
you can specify the highlighted attributes on the slide.

00:13:07,260 --> 00:13:09,550
The serviceAccountName is the Kubernetes service account

00:13:09,550 --> 00:13:10,920
that this will run as.

00:13:10,920 --> 00:13:14,020
Because this uses watches against the Kubernetes API server

00:13:14,020 --> 00:13:16,779
it needs to have sufficient RBAC permissions to do so.

00:13:16,779 --> 00:13:20,470
The mode, either resource or reference

00:13:20,470 --> 00:13:21,980
describes whether or not the event itself

00:13:21,980 --> 00:13:24,900
should contain the entire resource that's being modified

00:13:24,900 --> 00:13:29,090
or whether just a reference to that resource is going to be in the event.

00:13:29,090 --> 00:13:35,779
Finally, there's a list of resources that are actually going to be watched.

00:13:35,779 --> 00:13:41,370
The PingSource sends events on a fixed schedule, with a fixed payload.

00:13:41,370 --> 00:13:44,670
This specific example uses the cron schedule for every minute

00:13:44,670 --> 00:13:47,500
and it always sends a message with the same data.

00:13:47,500 --> 00:13:52,430
The message is "Hello world!"

00:13:52,430 --> 00:13:55,370
SinkBinding isn't a direct source, it's more of a meta source.

00:13:55,370 --> 00:13:57,000
It doesn't generate any events itself

00:13:57,000 --> 00:13:59,860
but it makes it very quick and easy for you to make your own sources.

00:13:59,860 --> 00:14:03,210
It can be used with the PodSpecable duck type.

00:14:03,210 --> 00:14:07,520
A PodSpecable duck type object is one that has the spec.template field

00:14:07,520 --> 00:14:10,690
that is a PodTemplate.

00:14:10,690 --> 00:14:15,160
Some examples are Deployments, Knative Services and StatefulSets.

00:14:15,160 --> 00:14:20,280
A SinkBinding turns a PodSpecable into a source.

00:14:20,280 --> 00:14:26,020
By providing, in the SinkBinding, the PodSpecable you want to use

00:14:26,020 --> 00:14:32,850
it will inject the sink into that PodSpecable as an environment variable.

00:14:32,850 --> 00:14:39,130
This allows you to very quickly and easily get an ad-hoc source up and running.

00:14:39,130 --> 00:14:40,370
As we were using SinkBinding

00:14:40,370 --> 00:14:44,170
we realized that, by far, the most common usage was with a Deployment.

00:14:44,170 --> 00:14:46,100
ContainerSource is another metasource

00:14:46,100 --> 00:14:48,680
that combines a SinkBinding with a Deployment.

00:14:48,680 --> 00:14:50,270
By creating this one resource

00:14:50,270 --> 00:14:54,580
it will create a Deployment with the container_image use specified

00:14:54,580 --> 00:14:57,000
and an injected sink URI.

00:14:57,000 --> 00:15:02,029
This allows a very quick way to create ad-hoc sources.

00:15:02,029 --> 00:15:06,320
Besides those four core sources that come out of the box with Knative Eventing

00:15:06,320 --> 00:15:09,050
there are other sources that are community-owned

00:15:09,050 --> 00:15:11,430
and that need to be installed separately.

00:15:11,430 --> 00:15:13,910
We have things like GitHubSource

00:15:13,910 --> 00:15:16,180
which will allow you to listen for GitHub events.

00:15:16,180 --> 00:15:18,320
For example, if you want to get a notification

00:15:18,320 --> 00:15:21,580
when a pull request is creating a particular repo

00:15:21,580 --> 00:15:23,710
to trigger some CI/CD pipeline

00:15:23,710 --> 00:15:24,770
you can use this.

00:15:24,770 --> 00:15:28,800
Or, there's the KafkaSource that would allow you to receive events

00:15:28,800 --> 00:15:31,800
when a message is published to a Kafka topic.

00:15:31,800 --> 00:15:37,600
We also have non-sql-based sources, such as CouchDb or MongoDb

00:15:37,600 --> 00:15:40,490
where you can listen for changes in databases and collections

00:15:40,490 --> 00:15:42,240
and act accordingly.

00:15:42,240 --> 00:15:47,050
For a more extensive list of sources, you can follow at that link.

00:15:47,050 --> 00:15:50,060
Besides the core and community-owned sources

00:15:50,060 --> 00:15:52,420
we also have vendor-owned sources.

00:15:52,420 --> 00:15:58,760
In particular, we, at Google, have been creating Knative Sources for GCP services.

00:15:58,760 --> 00:16:01,600
For example, we have the CloudPubSubSource

00:16:01,600 --> 00:16:03,520
which will allow you to receive events

00:16:03,520 --> 00:16:06,860
whenever you publish a message to a Pub/Sub topic.

00:16:06,860 --> 00:16:09,380
Or, we also have the CloudStorageSource

00:16:09,380 --> 00:16:12,980
that would allow you to listen for changes in GCS buckets.

00:16:12,980 --> 00:16:15,250
For example, when an image is uploaded

00:16:15,250 --> 00:16:20,149
to a GCS bucket, you can get an event and trigger a computer vision pipeline

00:16:20,149 --> 00:16:25,550
say to do object segmentation or image recognition.

00:16:25,550 --> 00:16:27,490
We also have the CloudSchedulerSource

00:16:27,490 --> 00:16:30,100
which will send an event based on a cron schedule

00:16:30,100 --> 00:16:33,930
similar to PingSource and some others.

00:16:33,930 --> 00:16:38,580
TriggerMesh has been collaborating and contributing a bunch of sources

00:16:38,580 --> 00:16:44,310
especially for AWS services, such as SQS, DynamoDB, and Kinesis.

00:16:44,310 --> 00:16:50,510
Finally, VMware has also been contributing some Knative Sources, such as vSphere.

00:16:50,510 --> 00:16:55,279
Now that we've looked at some of the available sources

00:16:55,279 --> 00:16:57,770
what if there's a source that you need that doesn't yet exist?

00:16:57,770 --> 00:17:01,310
Let's talk about implementing your own.

00:17:01,310 --> 00:17:03,090
This is the kind of flow chart we think through

00:17:03,090 --> 00:17:05,429
whenever we think about creating a new source.

00:17:05,429 --> 00:17:08,289
The first and most important question is: Do you want to make your own CRD?

00:17:08,289 --> 00:17:10,949
This makes it much easier for you just to use your source

00:17:10,949 --> 00:17:14,480
but it also increases the complexity of building everything.

00:17:14,480 --> 00:17:18,829
If you make your own CRD, you do need to understand more about Kubernetes

00:17:18,829 --> 00:17:19,970
and the Kubernetes event model.

00:17:19,970 --> 00:17:22,059
Regardless of whether you choose to make your own CRD

00:17:22,059 --> 00:17:23,629
you will need to write your own data plane

00:17:23,629 --> 00:17:27,739
with your own container image that sends the events themselves.

00:17:27,739 --> 00:17:31,360
If you make a CRD, you also need to write a controller as well.

00:17:31,360 --> 00:17:34,570
This flow chart can help determine what you should do.

00:17:34,570 --> 00:17:35,909
If you want to make your own CRD

00:17:35,909 --> 00:17:38,259
we recommend that you clone our sample-source repository

00:17:38,259 --> 00:17:40,869
and modify it as necessary.

00:17:40,869 --> 00:17:42,669
If you don't want your own CRD

00:17:42,669 --> 00:17:44,860
then can you run your container image as a Deployment?

00:17:44,860 --> 00:17:48,860
If so, then use that container image and make it ContainerSource.

00:17:48,860 --> 00:17:52,509
If not, can you run a container image in any PodSpecable?

00:17:52,509 --> 00:17:57,580
If so, make that PodSpecable resource, and make a SinkBinding as well.

00:17:57,580 --> 00:17:58,580
If not, then once again

00:17:58,580 --> 00:18:01,490
we recommend you clone our sample-source repo and modify.

00:18:01,490 --> 00:18:04,899
Sample source is our example of how to build a source CRD

00:18:04,899 --> 00:18:07,529
along with its controller and webhook.

00:18:07,529 --> 00:18:09,629
It follows all of Knative's best practices

00:18:09,629 --> 00:18:13,730
and is set up to be easily modifiable to get your CRD up and running quickly.

00:18:13,730 --> 00:18:16,570
For experimentation or just ad-hoc sources

00:18:16,570 --> 00:18:18,370
we recommend ContainerSource or SinkBinding

00:18:18,370 --> 00:18:20,929
because of just how quickly you can start.

00:18:20,929 --> 00:18:22,919
As we mentioned, if you make your own CRD

00:18:22,919 --> 00:18:25,119
then you'll have to implement your own control plane.

00:18:25,119 --> 00:18:28,960
The control plane is everything involved with getting a source ready to send events

00:18:28,960 --> 00:18:31,340
but not actually sending the event itself.

00:18:31,340 --> 00:18:35,240
It's generally built up of two pieces, the controller and the webhook.

00:18:35,240 --> 00:18:37,960
The controller will read the spec of every custom object

00:18:37,960 --> 00:18:40,269
and then write out all the status of every custom object

00:18:40,269 --> 00:18:43,070
running in a loop forever, where it observes the real world

00:18:43,070 --> 00:18:45,860
diffs that against the desired state from the custom object

00:18:45,860 --> 00:18:49,600
and acts to make the real world match the desired state.

00:18:49,600 --> 00:18:55,490
The webhook will default, validate, and convert any of these resources as well.

00:18:55,490 --> 00:18:58,619
The validation that the webhook does is not on the events themselves

00:18:58,619 --> 00:19:00,730
but rather on the custom object.

00:19:00,730 --> 00:19:04,039
It can go beyond the level of validation available in CRDs

00:19:04,039 --> 00:19:08,799
by doing things like validating, if two mutually exclusive fields are there

00:19:08,799 --> 00:19:12,759
they're not both present on the custom object at the same time.

00:19:12,759 --> 00:19:19,470
The data plane is everything involved with sending the actual events.

00:19:19,470 --> 00:19:23,100
We often call the pod that's sending the events the "Receive Adapter."

00:19:23,100 --> 00:19:28,179
What it does, is it will receive an incoming event from somewhere

00:19:28,179 --> 00:19:31,860
this could be an external web request, in the case of GitHubSource

00:19:31,860 --> 00:19:35,090
it could be an in-process timer, in the case of PingSource

00:19:35,090 --> 00:19:39,539
or it could be pulling from an external API, in the case of ApiServerSource.

00:19:39,539 --> 00:19:42,620
It'll then take this event and convert it into a Cloud Event.

00:19:42,620 --> 00:19:48,409
It can make any modifications it wants, including the CE Overrides we saw earlier.

00:19:48,409 --> 00:19:53,330
It then sends this event to the sink it was passed in as an environment variable.

00:19:53,330 --> 00:19:58,070
Finally, if the original event producer needs acknowledgements

00:19:58,070 --> 00:20:01,279
the Receive Adapter will then acknowledge it upstream.

00:20:01,279 --> 00:20:03,789
The picture we have here is of a pull-based source.

00:20:03,789 --> 00:20:07,779
The Receive Adapter is pulling changes from the upstream producer

00:20:07,779 --> 00:20:12,720
and sending those as Cloud Events to the sink.

00:20:12,720 --> 00:20:16,619
Cloud Events provides SDKs in multiple languages.

00:20:16,619 --> 00:20:21,040
Here's an example of how we can send an event in Go.

00:20:21,040 --> 00:20:23,159
The first thing we do is make a client.

00:20:23,159 --> 00:20:27,669
In this case, we want an HTTP client, and that's what the default happens to be.

00:20:27,669 --> 00:20:29,580
Next, we make an event.

00:20:29,580 --> 00:20:32,409
Every event requires a source, a type, and an ID.

00:20:32,409 --> 00:20:37,590
We specify the source and type here, and allow the SDK to default the ID for us.

00:20:37,590 --> 00:20:42,190
We also choose to add a data payload, in this case, "Hello world."

00:20:42,190 --> 00:20:47,279
Finally, we look up what our sink is with the K_SINK environment variable

00:20:47,279 --> 00:20:51,159
and send to that address.

00:20:51,159 --> 00:20:54,419
With that, we have reached the end of the talk.

00:20:54,419 --> 00:20:57,649
So, let's go to conclusions.

00:20:57,649 --> 00:21:01,429
We hope you have a clearer understanding of what Knative Sources are.

00:21:01,429 --> 00:21:06,590
In general, Eventing sources are a key construct for any event-driven system.

00:21:06,590 --> 00:21:10,259
A Knative Source is an abstraction that produces Cloud Events

00:21:10,259 --> 00:21:14,700
and sends them downstream to any configurable sink.

00:21:14,700 --> 00:21:19,139
Knative Eventing comes with some core sources out of the box

00:21:19,139 --> 00:21:21,510
but there are multiple Knative-compliant sources

00:21:21,510 --> 00:21:22,840
out there for you to use

00:21:22,840 --> 00:21:25,039
both community-owned and vendor-owned.

00:21:25,039 --> 00:21:27,450
But you can always create your own sources.

00:21:27,450 --> 00:21:28,450
Itâ€™s easy to do so

00:21:28,450 --> 00:21:33,159
and you don't need to fully understand Kubernetes.

00:21:33,159 --> 00:21:34,590
If you are interested in and want to learn more about

00:21:34,590 --> 00:21:36,529
any of the concepts we've talked about

00:21:36,529 --> 00:21:38,499
or are just excited to start using Knative Sources

00:21:38,499 --> 00:21:39,880
please visit our public docs.

00:21:39,880 --> 00:21:42,779
If you have any questions, please join our Slack channel.

00:21:42,779 --> 00:21:45,450
And if you just want to explore our code or future plans

00:21:45,450 --> 00:21:49,320
please look at the Knative GitHub organization.

00:21:49,320 --> 00:21:50,320
Thank you very much.

00:21:50,320 --> 00:21:50,820

YouTube URL: https://www.youtube.com/watch?v=uFPHdWhIRgs


