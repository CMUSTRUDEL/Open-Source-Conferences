Title: Disease Prediction Using the World's Largest Clinical Lab Dataset - Cristian Capdevila (Prognos)
Publication date: 2020-08-26
Playlist: JupyterCon 2018 Keynotes
Description: 
	Sponsored by Amazon Web Services - Cristian Capdevila explains how Prognos is predicting disease by applying a combination of modern machine learning techniques and clinical expertise to the world’s largest clinical lab database and how the company is leveraging Amazon SageMaker to accelerate model development, training, and deployment.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,000 --> 00:00:04,830
hi everyone as you've just heard I'm

00:00:02,370 --> 00:00:06,660
Kristin Capdevila I'm a data scientist

00:00:04,830 --> 00:00:08,760
at healthcare startup called prognosis

00:00:06,660 --> 00:00:11,969
and I'm here to tell you about what we

00:00:08,760 --> 00:00:16,740
do and how AWS stage maker helps us do

00:00:11,969 --> 00:00:18,240
it so this is our moon shot and as you

00:00:16,740 --> 00:00:20,039
can see we're an ambitious company with

00:00:18,240 --> 00:00:23,010
a very ambitious moonshot disease

00:00:20,039 --> 00:00:25,050
eradication more immediately as you can

00:00:23,010 --> 00:00:27,510
see after step one we're trying to learn

00:00:25,050 --> 00:00:29,070
to predict disease early and this is a

00:00:27,510 --> 00:00:31,859
problem obviously that has immense

00:00:29,070 --> 00:00:35,059
social and commercial impact as can be

00:00:31,859 --> 00:00:39,510
seen by a list of partners and investors

00:00:35,059 --> 00:00:41,719
here so we sell products into these

00:00:39,510 --> 00:00:44,940
three verticals life sciences

00:00:41,719 --> 00:00:46,860
diagnostics and payers here you can see

00:00:44,940 --> 00:00:50,730
some of our our bigger clients and

00:00:46,860 --> 00:00:53,460
investors and so the question is how are

00:00:50,730 --> 00:00:56,309
we doing this well we're combining three

00:00:53,460 --> 00:00:59,489
things one our teams of data scientists

00:00:56,309 --> 00:01:02,489
and engineers - an experienced group of

00:00:59,489 --> 00:01:05,010
clinical experts and three our unique

00:01:02,489 --> 00:01:08,460
data asset we have the world's largest

00:01:05,010 --> 00:01:11,159
clinical laboratory data set over 17

00:01:08,460 --> 00:01:12,720
billion de-identified records describing

00:01:11,159 --> 00:01:17,220
the patient journeys of 200 million

00:01:12,720 --> 00:01:20,430
people here in the United States so

00:01:17,220 --> 00:01:21,750
here's an overview of our predictive

00:01:20,430 --> 00:01:24,330
modeling pipeline

00:01:21,750 --> 00:01:27,450
I imagine this looks familiar to a lot

00:01:24,330 --> 00:01:30,090
of you out there we ingest data from

00:01:27,450 --> 00:01:32,640
disparate sources on the left and impose

00:01:30,090 --> 00:01:34,950
our schema on it then we use a

00:01:32,640 --> 00:01:37,020
combination of domain expertise and

00:01:34,950 --> 00:01:38,400
machine learning to harmonize it by

00:01:37,020 --> 00:01:40,380
cleaning up things like different naming

00:01:38,400 --> 00:01:43,500
conventions different units of measure

00:01:40,380 --> 00:01:45,890
and to organize and tag relegating sorry

00:01:43,500 --> 00:01:48,990
relevant information for use downstream

00:01:45,890 --> 00:01:52,320
extracting data from text fields is a

00:01:48,990 --> 00:01:54,180
good example and for the rest of this

00:01:52,320 --> 00:01:56,130
short talk I'll be focusing on this

00:01:54,180 --> 00:02:01,500
section in the bright pink box which

00:01:56,130 --> 00:02:04,619
runs on AWS sage maker so even after

00:02:01,500 --> 00:02:07,110
cleaning and harmonizing lab data it can

00:02:04,619 --> 00:02:09,479
be hard to work with it's irregular time

00:02:07,110 --> 00:02:11,760
series data different patients have

00:02:09,479 --> 00:02:12,930
different numbers of tests performed at

00:02:11,760 --> 00:02:16,049
irregular

00:02:12,930 --> 00:02:18,510
intervals it's also high-dimensional we

00:02:16,049 --> 00:02:21,180
have tagging logic for over 7,000

00:02:18,510 --> 00:02:23,010
different types of tests and a lot of

00:02:21,180 --> 00:02:25,890
those test records could have multiple

00:02:23,010 --> 00:02:28,379
ICD diagnostic codes of which there are

00:02:25,890 --> 00:02:30,959
roughly 68,000 and there's various other

00:02:28,379 --> 00:02:34,439
things attached and you end up in a very

00:02:30,959 --> 00:02:36,900
very large space and it's sparse some of

00:02:34,439 --> 00:02:42,180
these tests are quite rare so you aren't

00:02:36,900 --> 00:02:44,609
going to see them very often and a lot

00:02:42,180 --> 00:02:48,239
of you are probably also familiar with

00:02:44,609 --> 00:02:50,549
this or some variation on it if you're a

00:02:48,239 --> 00:02:53,819
data scientist the stuff that you're

00:02:50,549 --> 00:02:55,739
interested in is in orange but the vast

00:02:53,819 --> 00:02:58,260
majority of the initial man-hours and

00:02:55,739 --> 00:03:01,889
code are involved in all the blue stuff

00:02:58,260 --> 00:03:04,919
down below propping up those notebooks

00:03:01,889 --> 00:03:07,560
and the model code and like anything

00:03:04,919 --> 00:03:13,889
else all those pieces have to be

00:03:07,560 --> 00:03:15,930
maintained so sage maker for us has let

00:03:13,889 --> 00:03:19,620
us focus on the stuff that we care about

00:03:15,930 --> 00:03:22,889
and offload development and management

00:03:19,620 --> 00:03:25,199
of all the other pieces to Amazon we can

00:03:22,889 --> 00:03:27,599
prototype our models in a sage maker

00:03:25,199 --> 00:03:29,909
notebook and then very easily deploy

00:03:27,599 --> 00:03:33,150
that to a cluster of gpu-accelerated

00:03:29,909 --> 00:03:35,790
house for large scale training and once

00:03:33,150 --> 00:03:38,489
we've produced our model artifact we can

00:03:35,790 --> 00:03:40,290
deploy it as an endpoint in Sage Maker

00:03:38,489 --> 00:03:44,250
as well and we're ready to make

00:03:40,290 --> 00:03:46,139
predictions if however we do need to

00:03:44,250 --> 00:03:48,180
reach further down the stack we can

00:03:46,139 --> 00:03:50,250
provide our own docker images or Fork

00:03:48,180 --> 00:03:52,739
Amazon since they're on github and our

00:03:50,250 --> 00:03:56,639
own build of a framework and run those

00:03:52,739 --> 00:03:58,739
on their GPU docker hosts and use their

00:03:56,639 --> 00:04:00,750
deployment tools so that was a really

00:03:58,739 --> 00:04:03,180
big selling point for us that we're not

00:04:00,750 --> 00:04:06,090
necessarily locked into any particular

00:04:03,180 --> 00:04:11,159
tooling we can replace as much as we

00:04:06,090 --> 00:04:13,949
need when we need to and this has let us

00:04:11,159 --> 00:04:16,470
focus on this this big picture idea

00:04:13,949 --> 00:04:18,479
we've able to put our limited resources

00:04:16,470 --> 00:04:24,060
and time to what we really care about

00:04:18,479 --> 00:04:26,820
which is predicting disease that's the

00:04:24,060 --> 00:04:28,710
end of my talk we are higher

00:04:26,820 --> 00:04:30,600
ringg please send resumes I'll be

00:04:28,710 --> 00:04:33,330
walking around so if you have any

00:04:30,600 --> 00:04:35,050
questions come up in and talk to me

00:04:33,330 --> 00:04:39,059
thanks everyone

00:04:35,050 --> 00:04:39,059
[Applause]

00:04:44,100 --> 00:04:46,160

YouTube URL: https://www.youtube.com/watch?v=Zvvs1U7c17I


