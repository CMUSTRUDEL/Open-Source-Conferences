Title: How the Jupyter Notebook helped fast.ai teach deep learning to 50,000 students - Rachel Thomas
Publication date: 2020-08-24
Playlist: JupyterCon 2017 Keynotes
Description: 
	Although some claim you must start with advanced math to use deep learning, the best way for any coder to get started is with code. Rachel Thomas explains how fast.ai's Practical Deep Learning for Coders course uses Jupyter notebooks to provide an environment that encourages students to learn deep learning through experimentation.


Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Google: http://plus.google.com/+oreillymedia
Captions: 
	00:00:00,260 --> 00:00:07,980
Wow yeah and Rachel Thomas and

00:00:04,850 --> 00:00:10,410
co-founder of fast AI and researcher at

00:00:07,980 --> 00:00:12,509
the USF data Institute I'll be talking

00:00:10,410 --> 00:00:15,179
about the course practical deep learning

00:00:12,509 --> 00:00:17,039
for coders briefly my background I

00:00:15,179 --> 00:00:19,350
studied math and computer science and

00:00:17,039 --> 00:00:21,990
undergrad and then did a PhD in pure

00:00:19,350 --> 00:00:23,850
math and I worked as a quant and energy

00:00:21,990 --> 00:00:26,099
trading which is a lot of programming

00:00:23,850 --> 00:00:29,039
and working with data and then actually

00:00:26,099 --> 00:00:30,720
am O'Reilly strata 2012 as part of what

00:00:29,039 --> 00:00:33,059
convinced me to switch into tech and

00:00:30,720 --> 00:00:34,860
become a data scientist I was a data

00:00:33,059 --> 00:00:37,260
scientist and back-end engineer at uber

00:00:34,860 --> 00:00:39,210
and I taught full-stack software

00:00:37,260 --> 00:00:40,920
development at an all-woman coding

00:00:39,210 --> 00:00:42,719
academy hack Brian I really love

00:00:40,920 --> 00:00:44,940
teaching and I'll always return to it

00:00:42,719 --> 00:00:47,610
and then a year and a half ago Jeremy

00:00:44,940 --> 00:00:49,230
Howard and I founded fast AI with the

00:00:47,610 --> 00:00:52,410
goal of making deep learning more

00:00:49,230 --> 00:00:54,329
accessible and easier to use i'm on

00:00:52,410 --> 00:00:57,059
twitter at math underscore rachel and i

00:00:54,329 --> 00:00:59,640
blog about data science at fast i I I

00:00:57,059 --> 00:01:03,359
blog about diversity on medium at Rachel

00:00:59,640 --> 00:01:05,580
th show yeah so first just the

00:01:03,359 --> 00:01:07,920
vocabulary deep learning refers to

00:01:05,580 --> 00:01:10,350
multi-layered neural networks it's a

00:01:07,920 --> 00:01:12,479
very specific class of algorithms that's

00:01:10,350 --> 00:01:15,960
been given this ambiguous name that

00:01:12,479 --> 00:01:18,000
overloads two common words it's a subset

00:01:15,960 --> 00:01:21,450
of machine learning which is a subset of

00:01:18,000 --> 00:01:22,950
AI and AI is a very broad field but most

00:01:21,450 --> 00:01:24,930
of the advances that you hear about

00:01:22,950 --> 00:01:27,180
happening right now are all coming from

00:01:24,930 --> 00:01:31,350
deep learning this one family of

00:01:27,180 --> 00:01:33,689
algorithms so examples that you may have

00:01:31,350 --> 00:01:36,000
used of deep learning are Google photos

00:01:33,689 --> 00:01:37,500
here I've entered the search term purple

00:01:36,000 --> 00:01:39,170
flowers and it's showing me all the

00:01:37,500 --> 00:01:42,869
pictures I've taken of purple flowers

00:01:39,170 --> 00:01:47,340
this is powered by deep learning this is

00:01:42,869 --> 00:01:49,560
from analytic medical startup that can

00:01:47,340 --> 00:01:52,259
outperform a panel of expert human

00:01:49,560 --> 00:01:55,530
radiologists in identifying lung cancer

00:01:52,259 --> 00:01:57,450
and then last year the Chinese tech

00:01:55,530 --> 00:02:00,030
giant Baidu announced that their deep

00:01:57,450 --> 00:02:01,890
learning system is better at English and

00:02:00,030 --> 00:02:04,380
Mandarin speech recognition than most

00:02:01,890 --> 00:02:06,360
people so deep learning is being applied

00:02:04,380 --> 00:02:10,020
a ton right now to working with images

00:02:06,360 --> 00:02:12,360
and working with natural language this

00:02:10,020 --> 00:02:13,710
is a slide that Jeff Dean of the Google

00:02:12,360 --> 00:02:16,140
brain shared

00:02:13,710 --> 00:02:19,200
steer and it shows the growing use of

00:02:16,140 --> 00:02:23,340
deep learning at Google so the x-axis

00:02:19,200 --> 00:02:27,750
here is time going from beginning of

00:02:23,340 --> 00:02:29,790
2012 to mid 2016 and the y-axis is the

00:02:27,750 --> 00:02:31,650
number of unique projects at Google that

00:02:29,790 --> 00:02:34,950
we're using deep learning and so you can

00:02:31,650 --> 00:02:36,600
just see this exponential growth and he

00:02:34,950 --> 00:02:38,370
lists some of the areas that it was in

00:02:36,600 --> 00:02:41,100
and it's deep learnings being applied to

00:02:38,370 --> 00:02:44,250
drug discovery Gmail images natural

00:02:41,100 --> 00:02:46,800
language maps photos robots just this

00:02:44,250 --> 00:02:48,780
huge variety and the reason I think this

00:02:46,800 --> 00:02:51,150
is significant is that Google tends to

00:02:48,780 --> 00:02:52,530
be ahead of most of us and so we can

00:02:51,150 --> 00:02:54,900
expect other companies and other

00:02:52,530 --> 00:02:56,550
industries to follow this trend of

00:02:54,900 --> 00:03:01,110
applying deep learning to more and more

00:02:56,550 --> 00:03:02,820
problems so to get started this is from

00:03:01,110 --> 00:03:04,620
the most popular textbook on deep

00:03:02,820 --> 00:03:06,840
learning this is how we can gain

00:03:04,620 --> 00:03:09,210
intuition for back propagation through

00:03:06,840 --> 00:03:13,080
time so what do you think is this

00:03:09,210 --> 00:03:15,270
intuitive to you I have a PhD in math

00:03:13,080 --> 00:03:17,700
and this is not intuitive to me nor is

00:03:15,270 --> 00:03:19,400
it how I gain intuition I'm in this book

00:03:17,700 --> 00:03:21,690
I should say is a very useful resource

00:03:19,400 --> 00:03:25,920
but I don't recommend it as the best

00:03:21,690 --> 00:03:28,920
place to get started so a year ago

00:03:25,920 --> 00:03:30,690
Jeremy Howard and I decided to create a

00:03:28,920 --> 00:03:33,530
course practical deep learning for

00:03:30,690 --> 00:03:35,460
coders that would have no advanced math

00:03:33,530 --> 00:03:36,840
prerequisites and so this was very

00:03:35,460 --> 00:03:39,570
different from how other people were

00:03:36,840 --> 00:03:41,430
teaching deep learning we partnered with

00:03:39,570 --> 00:03:43,650
the University of San Francisco's data

00:03:41,430 --> 00:03:46,230
Institute the course was offered one

00:03:43,650 --> 00:03:48,930
evening a week most people in the class

00:03:46,230 --> 00:03:51,630
or working full-time as data scientist

00:03:48,930 --> 00:03:55,200
or developers and the only prerequisite

00:03:51,630 --> 00:03:58,140
was one year of coding experience we've

00:03:55,200 --> 00:04:00,090
had ton of success one of our students

00:03:58,140 --> 00:04:02,220
got into the Google brain residency who

00:04:00,090 --> 00:04:05,040
had only been coding for a year many

00:04:02,220 --> 00:04:07,920
others got new job offers won hackathons

00:04:05,040 --> 00:04:09,600
launched companies so it was worked

00:04:07,920 --> 00:04:12,840
really well we've released all the

00:04:09,600 --> 00:04:16,260
materials online for free at course fast

00:04:12,840 --> 00:04:18,239
I so you can check them out but yeah I'm

00:04:16,260 --> 00:04:20,370
not motto is if you can code you can do

00:04:18,239 --> 00:04:21,690
deep learning and it's all taught and do

00:04:20,370 --> 00:04:23,790
pa'dar notebooks and I'll say a little

00:04:21,690 --> 00:04:25,680
bit more about our kind of education

00:04:23,790 --> 00:04:27,630
philosophy in a moment

00:04:25,680 --> 00:04:29,070
I wanted to tell you the stories it's

00:04:27,630 --> 00:04:30,210
kind of a few of our students and

00:04:29,070 --> 00:04:32,729
they're just the huge variety of

00:04:30,210 --> 00:04:35,699
projects that they've worked on this is

00:04:32,729 --> 00:04:37,949
Shenzhen Lee of Akashi and exceed she's

00:04:35,699 --> 00:04:40,710
developing wearable devices for patients

00:04:37,949 --> 00:04:43,110
with Parkinson's disease typically a

00:04:40,710 --> 00:04:44,720
doctor would kind of visually assess a

00:04:43,110 --> 00:04:47,610
patient walking down the hallway to

00:04:44,720 --> 00:04:49,229
assess how the disease is progressing a

00:04:47,610 --> 00:04:51,120
wearable device is going to capture a

00:04:49,229 --> 00:04:52,949
lot more accurate more precise d than

00:04:51,120 --> 00:04:55,979
they're using deep learning to analyze

00:04:52,949 --> 00:04:58,680
it this is a team of data scientist

00:04:55,979 --> 00:05:00,720
Sarah hookers on the far right that's

00:04:58,680 --> 00:05:03,030
working with Delta analytics which pairs

00:05:00,720 --> 00:05:05,220
nonprofits with teams of data scientist

00:05:03,030 --> 00:05:07,410
and this particular team is with

00:05:05,220 --> 00:05:09,900
rainforests connection which puts

00:05:07,410 --> 00:05:13,440
recycled cell phones in endangered

00:05:09,900 --> 00:05:15,210
rainforest streams audio and then uses

00:05:13,440 --> 00:05:17,610
deep learning to identify chainsaw

00:05:15,210 --> 00:05:19,320
noises and they have responders they can

00:05:17,610 --> 00:05:21,449
send out if someone's illegally cutting

00:05:19,320 --> 00:05:23,340
down the forest there are a lot of

00:05:21,449 --> 00:05:25,800
interesting challenges apparently the

00:05:23,340 --> 00:05:28,349
frequency of chainsaws is the same as

00:05:25,800 --> 00:05:30,090
the frequency of mosquitoes that they're

00:05:28,349 --> 00:05:33,120
having they're having success with it

00:05:30,090 --> 00:05:35,460
with deep learning these images were

00:05:33,120 --> 00:05:38,370
sent to me by Cecile Singla a farm guide

00:05:35,460 --> 00:05:40,919
in India and thousands of farmers commit

00:05:38,370 --> 00:05:42,000
suicide every year in India in large

00:05:40,919 --> 00:05:44,010
part because they're taking very

00:05:42,000 --> 00:05:46,050
predatory loans from these thugs that

00:05:44,010 --> 00:05:47,610
harassed them and the farmers can't

00:05:46,050 --> 00:05:49,680
prove how much land they own or what

00:05:47,610 --> 00:05:51,389
type of crops they're growing so he'll

00:05:49,680 --> 00:05:54,030
and his team are scraping publicly

00:05:51,389 --> 00:05:56,370
available satellite data applying deep

00:05:54,030 --> 00:05:58,650
learning to identify the plots of lands

00:05:56,370 --> 00:06:00,810
and then creating better lending models

00:05:58,650 --> 00:06:03,349
so that the farmers can kind of qualify

00:06:00,810 --> 00:06:05,849
for better better loans and insurance

00:06:03,349 --> 00:06:08,460
this is Samar Haider who's a natural

00:06:05,849 --> 00:06:10,349
language researcher in Pakistan and the

00:06:08,460 --> 00:06:12,300
main language in pakistan urdu is

00:06:10,349 --> 00:06:14,250
relatively under-resourced as part of

00:06:12,300 --> 00:06:16,050
our class Samar gathered the largest

00:06:14,250 --> 00:06:19,110
corpus of Urdu text that's been

00:06:16,050 --> 00:06:20,639
assembled trained word of beddings on it

00:06:19,110 --> 00:06:22,500
and the style of google's word to back

00:06:20,639 --> 00:06:24,570
and this is a really useful resource for

00:06:22,500 --> 00:06:27,349
anyone working with you or translating

00:06:24,570 --> 00:06:31,229
materials into our out of the language

00:06:27,349 --> 00:06:35,070
this and you can start the video Jacque

00:06:31,229 --> 00:06:38,219
matiee bought two tons of used Legos on

00:06:35,070 --> 00:06:39,150
eBay because there's a lucrative like a

00:06:38,219 --> 00:06:40,650
resale Marv

00:06:39,150 --> 00:06:43,440
yet if you're willing to sort through

00:06:40,650 --> 00:06:46,199
them to remove the damaged pieces and to

00:06:43,440 --> 00:06:48,240
find the most valuable shapes he built a

00:06:46,199 --> 00:06:50,250
machine to do this for him and use his

00:06:48,240 --> 00:06:56,550
deep learning for identifying the shapes

00:06:50,250 --> 00:06:59,520
and this is this oh this is a picture of

00:06:56,550 --> 00:07:03,360
Kanye West drawn with miniature Captain

00:06:59,520 --> 00:07:05,910
Picard heads this is a new art technique

00:07:03,360 --> 00:07:07,590
that was invented by Brad penslar and

00:07:05,910 --> 00:07:08,970
he's written a great great blog post on

00:07:07,590 --> 00:07:15,840
it so deep learning is used to create

00:07:08,970 --> 00:07:17,699
art so a club Esmond who works at Splunk

00:07:15,840 --> 00:07:19,620
developed a new fraud detection

00:07:17,699 --> 00:07:22,110
algorithm based off of one of our

00:07:19,620 --> 00:07:25,590
lessons and got a patent for it and

00:07:22,110 --> 00:07:27,210
Splunk was very excited and then one of

00:07:25,590 --> 00:07:29,580
our students Tim on Glade worked for the

00:07:27,210 --> 00:07:31,260
HBO television show Silicon Valley if

00:07:29,580 --> 00:07:33,360
you've watched the most recent season

00:07:31,260 --> 00:07:36,300
there's an app that can identify if

00:07:33,360 --> 00:07:39,660
something is a hot dog or not a hot dog

00:07:36,300 --> 00:07:41,910
and while the the app is a bit of a joke

00:07:39,660 --> 00:07:43,800
on the show Tim actually did a really

00:07:41,910 --> 00:07:45,479
thorough job building it he contributed

00:07:43,800 --> 00:07:48,510
to core tensorflow as part of the

00:07:45,479 --> 00:07:50,220
process to reduce the size of tensorflow

00:07:48,510 --> 00:07:51,389
and Android and he's done a really great

00:07:50,220 --> 00:07:54,270
write-up of kind of the technology

00:07:51,389 --> 00:07:55,919
behind it and so I share so this is kind

00:07:54,270 --> 00:07:57,660
of eight stories of students working on

00:07:55,919 --> 00:07:59,880
very very different projects and it

00:07:57,660 --> 00:08:02,010
captures um I think the breadth of what

00:07:59,880 --> 00:08:06,240
what deep learning can do or some of

00:08:02,010 --> 00:08:08,970
that so about the class so it's all in

00:08:06,240 --> 00:08:10,680
Jupiter notebooks we use a lot of Kaggle

00:08:08,970 --> 00:08:12,300
competitions because they give you no

00:08:10,680 --> 00:08:14,639
good data sources and really clear

00:08:12,300 --> 00:08:17,280
metrics of kind of what and what good

00:08:14,639 --> 00:08:19,349
performance is we start off with cats

00:08:17,280 --> 00:08:21,990
versus dogs this has been a common

00:08:19,349 --> 00:08:23,880
competition twice and it involves

00:08:21,990 --> 00:08:26,970
getting a set of images and then having

00:08:23,880 --> 00:08:28,680
to determine if they're cats or dogs so

00:08:26,970 --> 00:08:32,190
here are the first three are cats and

00:08:28,680 --> 00:08:34,289
the fourth image is a dog so this is

00:08:32,190 --> 00:08:36,120
from our lesson one notebook you'll

00:08:34,289 --> 00:08:38,490
notice we get almost straight to the

00:08:36,120 --> 00:08:42,330
punchline a state of the art custom

00:08:38,490 --> 00:08:45,060
model in six lines of code and so this

00:08:42,330 --> 00:08:47,280
captures something pretty unique about

00:08:45,060 --> 00:08:49,860
our teaching philosophy and this was

00:08:47,280 --> 00:08:52,290
inspired by David Perkins this idea of

00:08:49,860 --> 00:08:52,950
the whole game and he uses an analogy

00:08:52,290 --> 00:08:54,720
with baseball

00:08:52,950 --> 00:08:57,210
we don't require children to memorize

00:08:54,720 --> 00:08:59,040
all the formal rules of baseball before

00:08:57,210 --> 00:09:00,210
we let them play but kids you know they

00:08:59,040 --> 00:09:01,950
have a sense of the game and they may

00:09:00,210 --> 00:09:04,530
not be playing a full nine innings or

00:09:01,950 --> 00:09:05,790
half full teams but they can play and

00:09:04,530 --> 00:09:08,730
have fun and as they get older they

00:09:05,790 --> 00:09:10,530
learn more and more details and so our

00:09:08,730 --> 00:09:12,510
goal was to get people using deep

00:09:10,530 --> 00:09:15,720
learning right away even though they

00:09:12,510 --> 00:09:18,090
don't know all the underlying and kind

00:09:15,720 --> 00:09:19,680
of techniques or principles but to get

00:09:18,090 --> 00:09:21,240
in there and to be applying it to

00:09:19,680 --> 00:09:24,420
problems they care about and then over

00:09:21,240 --> 00:09:26,430
time we drill into the details also

00:09:24,420 --> 00:09:29,250
you'll notice it says state of the art

00:09:26,430 --> 00:09:31,740
of the more practical tutorials out

00:09:29,250 --> 00:09:34,320
there we had found that many were kind

00:09:31,740 --> 00:09:36,240
of on work toy or sample problems and

00:09:34,320 --> 00:09:38,010
we're settling with good enough results

00:09:36,240 --> 00:09:40,440
but we really wanted to get to state of

00:09:38,010 --> 00:09:42,480
the art and there's a student that

00:09:40,440 --> 00:09:43,980
finished andrew eating just released a

00:09:42,480 --> 00:09:45,990
new deep learning class and there's a

00:09:43,980 --> 00:09:48,390
student that finished it in a week and

00:09:45,990 --> 00:09:50,970
Riaan tweeted about this and this is our

00:09:48,390 --> 00:09:52,710
arvind Nagaraj had also taken our course

00:09:50,970 --> 00:09:54,600
and he wrote a blog post comparing them

00:09:52,710 --> 00:09:56,400
and he recommends them both and he's as

00:09:54,600 --> 00:10:00,450
an analogy though of base or not

00:09:56,400 --> 00:10:03,570
baseball of driving and says that the

00:10:00,450 --> 00:10:05,220
fast day I course is like if you want to

00:10:03,570 --> 00:10:06,870
learn how to drive a car we kind of put

00:10:05,220 --> 00:10:08,190
you behind the wheel right away and we

00:10:06,870 --> 00:10:09,510
tell you like this is how to hold the

00:10:08,190 --> 00:10:11,820
steering wheel and this is how to press

00:10:09,510 --> 00:10:13,050
the brake and the accelerator and then

00:10:11,820 --> 00:10:14,760
you know in a few weeks we start talking

00:10:13,050 --> 00:10:16,380
about like okay this is how car engines

00:10:14,760 --> 00:10:18,330
work and then eventually we're like okay

00:10:16,380 --> 00:10:20,130
and then there's internal combustion and

00:10:18,330 --> 00:10:21,600
whereas Andrews course and the

00:10:20,130 --> 00:10:23,670
traditional model of teaching is much

00:10:21,600 --> 00:10:25,500
more to start with okay this is an

00:10:23,670 --> 00:10:27,540
internal combustion engine let's talk

00:10:25,500 --> 00:10:31,020
about the engineering of the car and

00:10:27,540 --> 00:10:32,040
then eventually then you can drive so we

00:10:31,020 --> 00:10:34,650
really have this kind of top-down

00:10:32,040 --> 00:10:36,060
approach that's kind of reverse order

00:10:34,650 --> 00:10:40,110
from I think a lot of a lot of the

00:10:36,060 --> 00:10:40,980
teaching I've had yeah and then the

00:10:40,110 --> 00:10:42,690
other thing I was just saying about

00:10:40,980 --> 00:10:45,210
working in Jupiter notebooks is that

00:10:42,690 --> 00:10:47,450
it's so great for experimenting on

00:10:45,210 --> 00:10:50,370
prototyping and I think this is true of

00:10:47,450 --> 00:10:53,160
most areas of data science and that

00:10:50,370 --> 00:10:54,630
includes deep learning if you haven't

00:10:53,160 --> 00:10:56,250
done deep learning before you might

00:10:54,630 --> 00:10:58,710
think that you need huge clusters of

00:10:56,250 --> 00:11:01,290
machines but really get started on a

00:10:58,710 --> 00:11:03,600
single GPU we have an Amazon machine

00:11:01,290 --> 00:11:05,779
image that we share with students to get

00:11:03,600 --> 00:11:07,040
them started with one GPU

00:11:05,779 --> 00:11:09,709
and part two of the course we encourage

00:11:07,040 --> 00:11:13,310
students to build their own box with a

00:11:09,709 --> 00:11:14,990
500 dollar GPO if they're able to but

00:11:13,310 --> 00:11:18,319
you can really get started in a Jupiter

00:11:14,990 --> 00:11:22,790
notebook using an AWS instance pretty

00:11:18,319 --> 00:11:25,160
easy relatively easily now this is a

00:11:22,790 --> 00:11:26,930
quote from a student I personally fell

00:11:25,160 --> 00:11:28,879
into the habit of watching the lectures

00:11:26,930 --> 00:11:31,399
too much and googling definitions too

00:11:28,879 --> 00:11:32,629
much without running the code at first I

00:11:31,399 --> 00:11:34,160
thought that I should read the code

00:11:32,629 --> 00:11:37,250
quickly and then spend time researching

00:11:34,160 --> 00:11:38,629
the theory behind it in retrospect I

00:11:37,250 --> 00:11:40,819
should have spent the majority of my

00:11:38,629 --> 00:11:42,560
time on the actual code and the Jupiter

00:11:40,819 --> 00:11:44,120
notebooks running it and seeing what

00:11:42,560 --> 00:11:47,329
goes into it and what comes out of it

00:11:44,120 --> 00:11:50,149
this is really good advice and it's not

00:11:47,329 --> 00:11:52,759
by listening or watching that you learn

00:11:50,149 --> 00:11:54,829
it's by doing and so it's so important

00:11:52,759 --> 00:11:56,839
to be changing the inputs and seeing how

00:11:54,829 --> 00:11:58,730
that modifies the outputs and writing

00:11:56,839 --> 00:12:02,269
code yourself and do paterno books are a

00:11:58,730 --> 00:12:05,990
really great environment for that this

00:12:02,269 --> 00:12:08,899
is s more of lesson one I'd say it's

00:12:05,990 --> 00:12:10,790
really good to be able in any sort of

00:12:08,899 --> 00:12:12,259
data science to look at your data to see

00:12:10,790 --> 00:12:15,740
what it looks like what are you getting

00:12:12,259 --> 00:12:18,050
wrong or right and if learning is often

00:12:15,740 --> 00:12:19,970
being applied to images or video or

00:12:18,050 --> 00:12:21,319
audio and jupiter notebooks give you a

00:12:19,970 --> 00:12:24,709
really great way to look at this data

00:12:21,319 --> 00:12:26,569
and so here we can look at the dogs and

00:12:24,709 --> 00:12:29,839
kind of to remember that the alternative

00:12:26,569 --> 00:12:30,860
to this would either be kind of doing

00:12:29,839 --> 00:12:32,600
something in the terminal and then

00:12:30,860 --> 00:12:34,160
having all these other windows open of

00:12:32,600 --> 00:12:36,350
maybe you have a separate image viewer

00:12:34,160 --> 00:12:38,930
and maybe a separate PDF with some law

00:12:36,350 --> 00:12:41,149
tech but you really have this all in one

00:12:38,930 --> 00:12:48,800
coherent narrative and I think is

00:12:41,149 --> 00:12:51,680
wonderful oh and I also I just want to

00:12:48,800 --> 00:12:53,540
highlight on this slide that the so we

00:12:51,680 --> 00:12:55,100
recommend using transfer learning and

00:12:53,540 --> 00:12:56,389
that's a big focus of the course and

00:12:55,100 --> 00:12:59,029
this is where you take a pre trained

00:12:56,389 --> 00:13:00,379
model from someone that probably has a

00:12:59,029 --> 00:13:02,990
lot more data than you and a lot more

00:13:00,379 --> 00:13:04,850
computational power and you retrain just

00:13:02,990 --> 00:13:06,649
the last few layers to fine-tune it to

00:13:04,850 --> 00:13:08,779
your problem and here we're doing that

00:13:06,649 --> 00:13:10,879
with imagenet which is involved a

00:13:08,779 --> 00:13:12,709
thousand categories and so it's

00:13:10,879 --> 00:13:15,019
identified this first dog as being very

00:13:12,709 --> 00:13:17,870
long alike which i think is accurate and

00:13:15,019 --> 00:13:20,180
then we go on to kind of fine-tune this

00:13:17,870 --> 00:13:22,649
to actually predict quatre

00:13:20,180 --> 00:13:24,779
this is from a later lesson using the

00:13:22,649 --> 00:13:26,820
coggle fisheries competition which was

00:13:24,779 --> 00:13:28,920
involved identifying the species of

00:13:26,820 --> 00:13:30,660
different fish and this is something

00:13:28,920 --> 00:13:33,990
that can help prevent or identify

00:13:30,660 --> 00:13:35,820
overfishing of endangered species and so

00:13:33,990 --> 00:13:37,829
in the top picture this is taken from

00:13:35,820 --> 00:13:40,110
above so you'll see on the you're

00:13:37,829 --> 00:13:41,790
looking down on a floor of a boat on the

00:13:40,110 --> 00:13:43,380
left and then the waters on the right

00:13:41,790 --> 00:13:45,149
and the fish that's been caught is near

00:13:43,380 --> 00:13:48,389
the top of the picture here we've

00:13:45,149 --> 00:13:50,040
overlaid that with a heat map showing

00:13:48,389 --> 00:13:51,630
the activations of the neural network

00:13:50,040 --> 00:13:53,670
and you can see where its most active

00:13:51,630 --> 00:13:55,860
there's kind of this bright pink spot

00:13:53,670 --> 00:13:58,620
that's on top of the fish and so this is

00:13:55,860 --> 00:14:00,149
a neat insight into kind of what the the

00:13:58,620 --> 00:14:01,949
neural network is doing if you're like

00:14:00,149 --> 00:14:05,100
it's activated in the area where the

00:14:01,949 --> 00:14:06,810
fish is and also do Paterno put gives us

00:14:05,100 --> 00:14:11,339
this night with nice way to kind of

00:14:06,810 --> 00:14:14,459
display that and look at it so this is

00:14:11,339 --> 00:14:16,709
stochastic gradient descent it's a core

00:14:14,459 --> 00:14:19,440
core part of training neural networks

00:14:16,709 --> 00:14:21,779
and so I think this video which was

00:14:19,440 --> 00:14:24,209
created with the animation module format

00:14:21,779 --> 00:14:27,540
plot lib give us a really accurate kind

00:14:24,209 --> 00:14:29,279
of picture of it so we've got a line of

00:14:27,540 --> 00:14:31,560
order we've got these points they're

00:14:29,279 --> 00:14:33,420
trying to find a line of best fit and we

00:14:31,560 --> 00:14:35,940
start off with a random guess which is

00:14:33,420 --> 00:14:38,010
not great and then we're taking steps

00:14:35,940 --> 00:14:39,390
generally in the right direction so

00:14:38,010 --> 00:14:42,149
there's some randomness it's not

00:14:39,390 --> 00:14:44,339
necessarily getting better each time but

00:14:42,149 --> 00:14:45,089
overall our line is headed headed in the

00:14:44,339 --> 00:14:48,829
right direction

00:14:45,089 --> 00:14:51,930
this is a good illustration of sdd I

00:14:48,829 --> 00:14:54,329
wanted to recognize my favorite jupiter

00:14:51,930 --> 00:14:56,880
extension which is collapsible headings

00:14:54,329 --> 00:14:59,130
these make it a lot easier to navigate

00:14:56,880 --> 00:15:02,850
very large notebooks and to keep them

00:14:59,130 --> 00:15:04,890
organized and the keyboard shortcuts are

00:15:02,850 --> 00:15:07,470
really terrific with this when you're in

00:15:04,890 --> 00:15:09,209
command mode within a section left takes

00:15:07,470 --> 00:15:11,430
you to the top of the section right

00:15:09,209 --> 00:15:13,829
takes you to the bottom and then on a

00:15:11,430 --> 00:15:16,079
section heading left folds it up right

00:15:13,829 --> 00:15:17,610
unfolds it and they've they've made it

00:15:16,079 --> 00:15:19,889
much easier to kind of produce these

00:15:17,610 --> 00:15:21,839
lip-balm you know like textbooks inside

00:15:19,889 --> 00:15:23,630
jupiter notebooks and to stay organized

00:15:21,839 --> 00:15:28,380
with different headings and subheadings

00:15:23,630 --> 00:15:29,610
much easier to navigate around yes I

00:15:28,380 --> 00:15:32,259
just wanted to thank everyone that

00:15:29,610 --> 00:15:33,549
contributes to jupiter notebooks

00:15:32,259 --> 00:15:35,589
they've definitely changed how I do my

00:15:33,549 --> 00:15:39,819
own work as a data scientist as well as

00:15:35,589 --> 00:15:42,279
how I teach in a huge direction for the

00:15:39,819 --> 00:15:44,259
better and I wanted to say I was just

00:15:42,279 --> 00:15:45,729
talking today about practical deep

00:15:44,259 --> 00:15:47,859
learning for coders that's one of our

00:15:45,729 --> 00:15:49,449
courses but we have three courses at

00:15:47,859 --> 00:15:51,850
fast AI that are all available

00:15:49,449 --> 00:15:54,369
completely for free online all taught

00:15:51,850 --> 00:15:56,619
within jupiter notebooks i'll talk with

00:15:54,369 --> 00:15:58,299
this top-down teaching philosophy I

00:15:56,619 --> 00:16:00,039
described of kind of getting you in the

00:15:58,299 --> 00:16:01,899
driver's seat right away and then

00:16:00,039 --> 00:16:04,149
digging into the details later and

00:16:01,899 --> 00:16:05,859
there's a part to cutting edge deep

00:16:04,149 --> 00:16:07,959
learning for coders that involves

00:16:05,859 --> 00:16:10,329
reading and implementing cutting-edge

00:16:07,959 --> 00:16:12,489
research and then there's a

00:16:10,329 --> 00:16:15,160
computational linear algebra class I

00:16:12,489 --> 00:16:16,959
taught this summer if you took a linear

00:16:15,160 --> 00:16:19,329
algebra course in college this is very

00:16:16,959 --> 00:16:21,999
different because a typical linear

00:16:19,329 --> 00:16:24,009
algebra class is focused on doing matrix

00:16:21,999 --> 00:16:25,749
computations by hand

00:16:24,009 --> 00:16:27,299
computational linear algebra is focused

00:16:25,749 --> 00:16:30,129
on how do we get computers to do this

00:16:27,299 --> 00:16:31,419
quickly and with enough accuracy and

00:16:30,129 --> 00:16:35,019
that's a whole different set of

00:16:31,419 --> 00:16:37,119
considerations I should also note if you

00:16:35,019 --> 00:16:38,799
live in the Bay Area will be doing

00:16:37,119 --> 00:16:41,559
another version of the in-person

00:16:38,799 --> 00:16:43,749
practical deep learning for coders with

00:16:41,559 --> 00:16:45,519
the updated version of the course that

00:16:43,749 --> 00:16:48,039
starts in late October and you can find

00:16:45,519 --> 00:16:49,389
information online about it so yeah

00:16:48,039 --> 00:16:52,299
again thank you and feel free to reach

00:16:49,389 --> 00:16:54,580
out to me on Twitter or in-person at

00:16:52,299 --> 00:16:59,349
this conference

00:16:54,580 --> 00:16:59,349

YouTube URL: https://www.youtube.com/watch?v=frc7FgheUj4


