Title: CppCon 2015: Grant Mercer & Danial Bourgeois “Parallelizing the C++ Standard Template Library”
Publication date: 2015-10-12
Playlist: CppCon 2015
Description: 
	http://www.Cppcon.org
—
Presentation Slides, PDFs, Source Code and other presenter materials are available at: https://github.com/cppcon/cppcon2015
—
As the era of frequency scaling comes to an end, multi-core parallelism has become an essential focal point in computational research. Mainstream languages, however, have not yet adapted to take full advantage of parallelism provided by the hardware. While new languages such as Rust and Swift are catching on and implementing multi-core algorithms in their libraries, C++ has only started to do so. A parallel Standard Library could bring with it many positive features that users can begin taking advantage of.

This talk will focus around two standards proposals, N4409 and N4406. N4409 outlines the details of a parallel Standard Library and features of these new parallel algorithms. The complementary N4406 outlines abstractions to take advantage of various mechanisms for parallel execution. We will cover the reasons why the new Standard Library would be beneficial to C++ users and our experience implementing these algorithms in HPX. The presentation will address what exactly the two proposals define, the challenges we faced, and the results we collected. In addition, we will discuss extensions made to these proposals and the C++11/14 standard in HPX to support these semantics in a distributed environment.
— 
Daniel Bourgeois:  I'm a student at LSU and have been working at the SET||AR research group helping implement parallel algorithms inside of HPX. Feel free to talk to me about new C++ features, parallel computing or anything else.
—
Videos Filmed & Edited by Bash Films: http://www.BashFilms.com
Captions: 
	00:00:00,000 --> 00:00:05,339
hear me all good all right so we can go

00:00:03,419 --> 00:00:07,560
ahead and start the presentation this is

00:00:05,339 --> 00:00:11,160
daniel a nice presentation on paralyzing

00:00:07,560 --> 00:00:13,200
the c++ 10 standard template library a

00:00:11,160 --> 00:00:15,780
little bit about myself first my name is

00:00:13,200 --> 00:00:17,460
grant mercer i'm currently a third year

00:00:15,780 --> 00:00:19,770
student at the University of Nevada Las

00:00:17,460 --> 00:00:23,519
Vegas I'm pursuing a bachelor's degree

00:00:19,770 --> 00:00:24,869
currently over the summer of 2014 I did

00:00:23,519 --> 00:00:27,029
some really cool work with the stellar

00:00:24,869 --> 00:00:29,039
research group and that was mainly

00:00:27,029 --> 00:00:33,360
implementing a standards proposal which

00:00:29,039 --> 00:00:35,219
is a n 4505 and this is a technical

00:00:33,360 --> 00:00:36,750
specification for extensions for

00:00:35,219 --> 00:00:38,190
parallelism and this is going to be one

00:00:36,750 --> 00:00:40,590
of the two main proposals we're going to

00:00:38,190 --> 00:00:42,420
talk about during this talk I worked on

00:00:40,590 --> 00:00:44,340
implementing this inside of our runtime

00:00:42,420 --> 00:00:47,010
system hpx which we'll get to in a

00:00:44,340 --> 00:00:49,440
second and my name is Daniel bajwa I

00:00:47,010 --> 00:00:52,140
started on the parallel algorithms after

00:00:49,440 --> 00:00:55,410
Grant had left so we have just met each

00:00:52,140 --> 00:00:56,789
other at this conference will we work

00:00:55,410 --> 00:01:00,120
together at afford to make this

00:00:56,789 --> 00:01:01,260
PowerPoint for y'all um hi so I'm

00:01:00,120 --> 00:01:04,799
currently working for the Stella

00:01:01,260 --> 00:01:06,810
research group at LSU and I worked on

00:01:04,799 --> 00:01:10,020
the parallel algorithms and in addition

00:01:06,810 --> 00:01:12,869
in 4406 which is parallel algorithms

00:01:10,020 --> 00:01:14,010
meet executors and so first grants going

00:01:12,869 --> 00:01:15,630
to take it off and then we're going to

00:01:14,010 --> 00:01:19,020
talk a little bit about executors and

00:01:15,630 --> 00:01:21,000
then I'll hand it back to grant okay so

00:01:19,020 --> 00:01:22,610
first some background information on

00:01:21,000 --> 00:01:25,110
what the stellar research group is

00:01:22,610 --> 00:01:27,030
stellar is all about shaping a scalable

00:01:25,110 --> 00:01:29,430
future with the new approach to parallel

00:01:27,030 --> 00:01:32,729
computation one of our notes most

00:01:29,430 --> 00:01:36,030
notable ongoing projects here is h PX +

00:01:32,729 --> 00:01:38,549
h PX is a general-purpose c++ runtime

00:01:36,030 --> 00:01:42,509
system for distributed applications of

00:01:38,549 --> 00:01:44,430
any scale so what hpx does is it enables

00:01:42,509 --> 00:01:46,320
programmers to write fully asynchronous

00:01:44,430 --> 00:01:48,689
code using hundreds of millions of

00:01:46,320 --> 00:01:50,430
threads it's the first open source

00:01:48,689 --> 00:01:53,549
implementation of the parallel X

00:01:50,430 --> 00:01:55,710
execution model and this execution model

00:01:53,549 --> 00:01:57,479
serves to overcome the four main

00:01:55,710 --> 00:01:59,969
barriers of parallelism which is

00:01:57,479 --> 00:02:03,180
starvation latin sees overhead and

00:01:59,969 --> 00:02:05,909
waiting some focus points we want to get

00:02:03,180 --> 00:02:07,799
to it during this talk are some reasons

00:02:05,909 --> 00:02:10,349
why we should actually paralyzed the stl

00:02:07,799 --> 00:02:11,550
and why this may be a good idea some of

00:02:10,349 --> 00:02:13,620
the features these algorithms should

00:02:11,550 --> 00:02:15,650
offer our experience

00:02:13,620 --> 00:02:17,879
implementing these algorithms at hpx

00:02:15,650 --> 00:02:19,260
some benchmarking results we want to

00:02:17,879 --> 00:02:22,019
make sure that these algorithms actually

00:02:19,260 --> 00:02:23,430
work as their intended to and some

00:02:22,019 --> 00:02:26,879
future work on what we want to do for

00:02:23,430 --> 00:02:28,470
these algorithms so why exactly should

00:02:26,879 --> 00:02:31,379
we paralyze the standard template

00:02:28,470 --> 00:02:34,530
library for one thing multiple cores are

00:02:31,379 --> 00:02:36,150
here to stay the old way of amping up

00:02:34,530 --> 00:02:39,390
your frequency and gain additional power

00:02:36,150 --> 00:02:41,970
has long passed back in the older days

00:02:39,390 --> 00:02:43,920
people thought that you needed to get a

00:02:41,970 --> 00:02:46,109
higher frequency to have a better

00:02:43,920 --> 00:02:48,150
computer and eventually they hit a mark

00:02:46,109 --> 00:02:51,420
where you start running into memory leg

00:02:48,150 --> 00:02:54,209
RC delay power consumption and number of

00:02:51,420 --> 00:02:56,310
other reasons so instead of getting more

00:02:54,209 --> 00:02:58,440
frequency they decided to make more

00:02:56,310 --> 00:02:59,940
cores instead and this was the birth of

00:02:58,440 --> 00:03:03,000
parallelism so now we're running into

00:02:59,940 --> 00:03:04,769
this new era of really great parallel

00:03:03,000 --> 00:03:08,430
computation and we need to start making

00:03:04,769 --> 00:03:09,690
taking advantage of parallelism another

00:03:08,430 --> 00:03:11,519
reason why we should paralyze the

00:03:09,690 --> 00:03:15,659
standard template library is some

00:03:11,519 --> 00:03:17,970
scalable performance gains so having a

00:03:15,659 --> 00:03:19,889
parallel SEL gives your user a ton of

00:03:17,970 --> 00:03:21,120
flexibility because now they don't need

00:03:19,889 --> 00:03:22,319
to reinvent the wheel when they have

00:03:21,120 --> 00:03:24,720
these parallel algorithms at their

00:03:22,319 --> 00:03:27,180
disposal so if you have a large data you

00:03:24,720 --> 00:03:29,280
would like to run some algorithms on you

00:03:27,180 --> 00:03:31,470
can simply say ok I'd like to run this

00:03:29,280 --> 00:03:32,910
in parallel now instead of sequential so

00:03:31,470 --> 00:03:35,010
we're still giving you a sequential

00:03:32,910 --> 00:03:37,440
option but now we're offering some new

00:03:35,010 --> 00:03:40,440
options for you and we want to build

00:03:37,440 --> 00:03:42,599
widespread existing practice for

00:03:40,440 --> 00:03:44,699
parallelism in the c++ standard template

00:03:42,599 --> 00:03:47,310
library and this is because we're seeing

00:03:44,699 --> 00:03:48,900
new languages like rust go and Swift all

00:03:47,310 --> 00:03:51,150
putting parallel ISM inside of their

00:03:48,900 --> 00:03:54,090
core library features and I think it'd

00:03:51,150 --> 00:03:56,220
be really cool if C++ could keep up with

00:03:54,090 --> 00:03:59,910
these other languages and also add some

00:03:56,220 --> 00:04:02,220
existing practice for parallelism here's

00:03:59,910 --> 00:04:04,379
a small graphic so everyone knows

00:04:02,220 --> 00:04:06,209
Moore's law the number of transistors in

00:04:04,379 --> 00:04:09,239
integrated circuit should double every

00:04:06,209 --> 00:04:13,049
year and so far it is but Moore's law

00:04:09,239 --> 00:04:16,620
will eventually come to an end there is

00:04:13,049 --> 00:04:18,180
some work saying that our transistors

00:04:16,620 --> 00:04:20,070
can only get so small soon we're going

00:04:18,180 --> 00:04:22,260
to start hitting the quantum level and

00:04:20,070 --> 00:04:24,630
that's when electrons and everything

00:04:22,260 --> 00:04:26,460
starts getting really weird so instead

00:04:24,630 --> 00:04:27,660
we're starting to see a new Moore's Law

00:04:26,460 --> 00:04:31,199
and this is for the number of cores

00:04:27,660 --> 00:04:33,030
inside of a computer as i mentioned

00:04:31,199 --> 00:04:34,680
we're seeing a frequency level out now

00:04:33,030 --> 00:04:36,180
so we're not trying to push up our

00:04:34,680 --> 00:04:38,550
frequency anymore because this gives us

00:04:36,180 --> 00:04:40,349
more power consumption so now our power

00:04:38,550 --> 00:04:42,870
is also leveling out so this is great

00:04:40,349 --> 00:04:45,360
for us our transistors are still trying

00:04:42,870 --> 00:04:47,430
to double which will eventually slow

00:04:45,360 --> 00:04:49,229
down but our course our course are

00:04:47,430 --> 00:04:50,669
getting more and more inside of our

00:04:49,229 --> 00:04:54,599
computers which is going to give us more

00:04:50,669 --> 00:04:57,330
power for parallelism and this is where

00:04:54,599 --> 00:05:00,570
the status proposal and 4505 comes in

00:04:57,330 --> 00:05:02,669
it's a technical specification for C++

00:05:00,570 --> 00:05:04,470
extensions for parallelism which pretty

00:05:02,669 --> 00:05:07,349
much means its implementation details

00:05:04,470 --> 00:05:09,240
for how to get a parallel stl when

00:05:07,349 --> 00:05:11,310
you're writing this it's important to

00:05:09,240 --> 00:05:13,289
realize that not all these algorithms

00:05:11,310 --> 00:05:15,960
can be parallelized we have stained

00:05:13,289 --> 00:05:17,810
accumulate for example where one value

00:05:15,960 --> 00:05:22,080
will depend on the previous so this

00:05:17,810 --> 00:05:24,090
cannot be parallelized theoretically so

00:05:22,080 --> 00:05:26,490
it's defined a list of algorithms which

00:05:24,090 --> 00:05:28,590
should now become paralyzed there's a

00:05:26,490 --> 00:05:32,159
huge range of them anywhere from copy to

00:05:28,590 --> 00:05:33,900
find to sort to transform so all these

00:05:32,159 --> 00:05:36,419
algorithms could now be offered to users

00:05:33,900 --> 00:05:39,630
in a parallel fashion and speed up a ton

00:05:36,419 --> 00:05:42,750
of new code it's aimed for acceptance

00:05:39,630 --> 00:05:45,060
into c plus plus 17 our implementation

00:05:42,750 --> 00:05:48,479
that hpx already takes advantage of c++

00:05:45,060 --> 00:05:51,479
11 and i believe some 14 the components

00:05:48,479 --> 00:05:53,490
of the specification right now are will

00:05:51,479 --> 00:05:56,070
lie in this standard experimental or

00:05:53,490 --> 00:05:58,770
standard parallel experimental v1 which

00:05:56,070 --> 00:06:00,389
is a mouthful but one standardized that

00:05:58,770 --> 00:06:03,240
we expect to be placed in standard just

00:06:00,389 --> 00:06:07,039
as the current stl is our implementation

00:06:03,240 --> 00:06:10,199
in HP X uses the HP X parallel namespace

00:06:07,039 --> 00:06:11,880
now all these algorithms inside this new

00:06:10,199 --> 00:06:13,860
proposed document will conform to their

00:06:11,880 --> 00:06:16,380
predecessors so any iterator

00:06:13,860 --> 00:06:18,570
requirements and return types of the

00:06:16,380 --> 00:06:19,740
current stl will remain the same we're

00:06:18,570 --> 00:06:23,610
not going to impose any new restrictions

00:06:19,740 --> 00:06:25,530
on these algorithms this is the simple

00:06:23,610 --> 00:06:27,389
search algorithms from cpp reference

00:06:25,530 --> 00:06:30,719
nothing will change inside of this new

00:06:27,389 --> 00:06:33,509
proposal minus one new argument and this

00:06:30,719 --> 00:06:36,210
new argument is an execution policy so

00:06:33,509 --> 00:06:38,289
an execution policy indicates the kinds

00:06:36,210 --> 00:06:40,180
of parallelism allowed

00:06:38,289 --> 00:06:42,279
expresses the consequent requirements on

00:06:40,180 --> 00:06:43,990
the element access functions so it's

00:06:42,279 --> 00:06:45,520
pretty much saying you can't run this in

00:06:43,990 --> 00:06:48,339
parallel you can go ahead and run this

00:06:45,520 --> 00:06:50,740
in sequential or some other options we

00:06:48,339 --> 00:06:52,629
officially support sequential parallel

00:06:50,740 --> 00:06:55,539
and parallel vectorize perla vectorize

00:06:52,629 --> 00:06:58,529
is to disambiguate on the tag so it'll

00:06:55,539 --> 00:07:01,719
specifically say this can be vectorized

00:06:58,529 --> 00:07:04,199
here's an example of how this new stl

00:07:01,719 --> 00:07:07,029
would work so you have your normal

00:07:04,199 --> 00:07:10,180
standard vector with view let's see if

00:07:07,029 --> 00:07:13,119
this pointer works a little bit and

00:07:10,180 --> 00:07:13,869
you'll have nope that doesn't work so

00:07:13,119 --> 00:07:30,550
we're going to go back to the

00:07:13,869 --> 00:07:40,959
presentation whoops jump right back up

00:07:30,550 --> 00:07:42,849
there okay so um the first two lines are

00:07:40,959 --> 00:07:45,279
the standard how your sort works

00:07:42,849 --> 00:07:47,379
currently and after that is when we're

00:07:45,279 --> 00:07:49,659
using the namespace hpx parallel these

00:07:47,379 --> 00:07:51,459
are our new algorithms were writing you

00:07:49,659 --> 00:07:53,769
have a standard sort and now you're

00:07:51,459 --> 00:07:57,099
explicitly saying sort this sequentially

00:07:53,769 --> 00:07:58,240
by passing in the seq tag after that and

00:07:57,099 --> 00:08:00,069
the line after that you're using

00:07:58,240 --> 00:08:02,589
parallel execution and parallel

00:08:00,069 --> 00:08:03,759
vectorized execution and one of the

00:08:02,589 --> 00:08:05,830
coolest things about this new proposal

00:08:03,759 --> 00:08:09,399
is the bottom part which will allow you

00:08:05,830 --> 00:08:11,529
to do dynamically selective execution so

00:08:09,399 --> 00:08:14,529
if you have some continuously increasing

00:08:11,529 --> 00:08:16,749
data size you can set a threshold and

00:08:14,529 --> 00:08:18,430
you can check and if the data size is

00:08:16,749 --> 00:08:20,619
over a certain threshold you can start

00:08:18,430 --> 00:08:22,659
parallelizing it so you don't need to

00:08:20,619 --> 00:08:24,669
know the type of parallel execution you

00:08:22,659 --> 00:08:27,159
want to do at compile time this can be a

00:08:24,669 --> 00:08:30,069
completely run time dependent operation

00:08:27,159 --> 00:08:31,990
and then down below I don't know if you

00:08:30,069 --> 00:08:34,839
guys can see that or I believe you can

00:08:31,990 --> 00:08:36,880
we're simply passing in this blank

00:08:34,839 --> 00:08:39,529
execution policy which has been set to

00:08:36,880 --> 00:08:41,810
either sequential or parallel

00:08:39,529 --> 00:08:43,579
some things to note about this new

00:08:41,810 --> 00:08:46,399
parallel execution policy inside these

00:08:43,579 --> 00:08:48,560
algorithms is that it's the caller's

00:08:46,399 --> 00:08:50,089
responsibility to ensure correctness so

00:08:48,560 --> 00:08:52,610
we're not doing any work to make sure

00:08:50,089 --> 00:08:54,230
this is thread-safe anything you do with

00:08:52,610 --> 00:08:55,970
your own code is your responsibility

00:08:54,230 --> 00:08:58,129
will simply try and parallel why's it

00:08:55,970 --> 00:09:00,350
for you so if you're throwing in a data

00:08:58,129 --> 00:09:01,639
race or a deadlock it's going to be just

00:09:00,350 --> 00:09:03,230
as you tried to write your own algorithm

00:09:01,639 --> 00:09:05,870
we're not going to try and fix anything

00:09:03,230 --> 00:09:07,999
for you so here's a quick example of

00:09:05,870 --> 00:09:09,889
what not to do if you're trying to push

00:09:07,999 --> 00:09:11,269
back multiple elements into a vector at

00:09:09,889 --> 00:09:15,290
the same time it's going to give you

00:09:11,269 --> 00:09:16,399
some undefined behavior a little bit

00:09:15,290 --> 00:09:19,249
more about these parallel execution

00:09:16,399 --> 00:09:20,990
policies is that what we're doing is

00:09:19,249 --> 00:09:22,759
we're permitting to run in parallel

00:09:20,990 --> 00:09:25,459
we're not forcing it to run in parallel

00:09:22,759 --> 00:09:26,809
so what this means is that as I

00:09:25,459 --> 00:09:28,569
mentioned before we're not going to post

00:09:26,809 --> 00:09:31,129
any new requirements on these algorithms

00:09:28,569 --> 00:09:32,959
this is in the case of an input iterator

00:09:31,129 --> 00:09:35,480
input iterators cannot be parallelized

00:09:32,959 --> 00:09:36,999
because one value you can't go forward

00:09:35,480 --> 00:09:40,009
or backward inside the input iterator

00:09:36,999 --> 00:09:41,209
what this means is that since we're

00:09:40,009 --> 00:09:42,829
still going to let you use all the

00:09:41,209 --> 00:09:44,660
iterator requirements if you pass

00:09:42,829 --> 00:09:47,089
parallel we're simply going to force

00:09:44,660 --> 00:09:48,529
back into the sequential execution so

00:09:47,089 --> 00:09:52,009
this is a case where you might call

00:09:48,529 --> 00:09:53,660
parallel on a data some iterators and

00:09:52,009 --> 00:09:57,680
these might be input so we'll go ahead

00:09:53,660 --> 00:10:00,529
and force them back to sequential now

00:09:57,680 --> 00:10:02,389
for the exception reporting behavior if

00:10:00,529 --> 00:10:03,889
temporary resources are acquired but you

00:10:02,389 --> 00:10:06,379
don't have them it'll throw a standard

00:10:03,889 --> 00:10:08,360
bad alloc that's pretty normal and now

00:10:06,379 --> 00:10:10,189
if the invocation of the ailment access

00:10:08,360 --> 00:10:12,860
function so what actually happens inside

00:10:10,189 --> 00:10:15,470
the parallelization terminates with an

00:10:12,860 --> 00:10:17,449
uncaught exception for par and for

00:10:15,470 --> 00:10:19,069
sequential all the uncaught exceptions

00:10:17,449 --> 00:10:22,730
will be contained in exception list that

00:10:19,069 --> 00:10:25,550
you can use after now what we did

00:10:22,730 --> 00:10:27,889
different hpx is since we're doing a lot

00:10:25,550 --> 00:10:30,860
of asynchronous programming we found a

00:10:27,889 --> 00:10:32,360
new policy would be really useful we

00:10:30,860 --> 00:10:34,009
haven't proposed this to the proposal

00:10:32,360 --> 00:10:36,319
yet but we found this really useful in

00:10:34,009 --> 00:10:39,199
our own runtime system and this is the

00:10:36,319 --> 00:10:40,910
task execution policy so what this does

00:10:39,199 --> 00:10:42,559
is it gives users an additional choice

00:10:40,910 --> 00:10:45,980
of when they want to join back in with

00:10:42,559 --> 00:10:48,139
the main thread it'll return a future of

00:10:45,980 --> 00:10:50,329
the result as opposed to just the result

00:10:48,139 --> 00:10:52,189
and this will allow you to continue

00:10:50,329 --> 00:10:52,480
we're doing work and then choosing when

00:10:52,189 --> 00:10:53,920
you

00:10:52,480 --> 00:10:56,500
we want to wait for this work to finish

00:10:53,920 --> 00:10:58,630
so it's useful in that if you have some

00:10:56,500 --> 00:11:00,040
big amount of work in you to do you'd

00:10:58,630 --> 00:11:02,110
also like to do some work while that's

00:11:00,040 --> 00:11:05,110
happening you can go ahead and send it

00:11:02,110 --> 00:11:06,880
off get a future for it do some of your

00:11:05,110 --> 00:11:10,540
extra work and when you need that value

00:11:06,880 --> 00:11:15,279
you can go ahead and wait for it then so

00:11:10,540 --> 00:11:19,089
let's meet Daniel so we've defined a way

00:11:15,279 --> 00:11:22,180
to interact with the algorithms via

00:11:19,089 --> 00:11:23,949
these execution policies but if we look

00:11:22,180 --> 00:11:25,889
at the code example the bottom that was

00:11:23,949 --> 00:11:28,480
in grand slide we're choosing

00:11:25,889 --> 00:11:30,459
dynamically at runtime whether or not we

00:11:28,480 --> 00:11:33,250
should do something sequentially or in

00:11:30,459 --> 00:11:35,110
parallel but there's more that we can

00:11:33,250 --> 00:11:37,779
give to the user or more that the user

00:11:35,110 --> 00:11:39,459
might want to provide and so this is

00:11:37,779 --> 00:11:42,160
what my section of the talk is going to

00:11:39,459 --> 00:11:45,220
be about and it starts with the proposal

00:11:42,160 --> 00:11:51,130
about executors to extend where work is

00:11:45,220 --> 00:11:53,529
going to be done and then we also want

00:11:51,130 --> 00:11:55,510
to be able to say how much work is going

00:11:53,529 --> 00:11:59,040
to get sent to each of our processors in

00:11:55,510 --> 00:12:01,410
other words control the grain size and

00:11:59,040 --> 00:12:03,670
in doing so we hope to develop

00:12:01,410 --> 00:12:06,430
abstractions that are useful for the

00:12:03,670 --> 00:12:08,709
parallel algorithm writers but also

00:12:06,430 --> 00:12:14,230
useful for other people who want to make

00:12:08,709 --> 00:12:17,800
use of these facilities so n 4406 came

00:12:14,230 --> 00:12:19,750
after the proposal about parallel

00:12:17,800 --> 00:12:23,290
algorithms and it's parallel algorithms

00:12:19,750 --> 00:12:26,260
need executors so the idea is that we

00:12:23,290 --> 00:12:28,920
want to be able to control where

00:12:26,260 --> 00:12:32,050
parallel or we want to be able to

00:12:28,920 --> 00:12:34,930
control where work is done on executors

00:12:32,050 --> 00:12:39,370
and we would like a way to do that so

00:12:34,930 --> 00:12:41,949
they proposed the dot on syntax so the

00:12:39,370 --> 00:12:44,139
dot on syntax attaches an executor to

00:12:41,949 --> 00:12:46,300
the policy so we're creating this

00:12:44,139 --> 00:12:48,639
interface to the user to gene interact

00:12:46,300 --> 00:12:51,100
with the parallel algorithms while still

00:12:48,639 --> 00:12:55,240
maintaining the same same extensibility

00:12:51,100 --> 00:12:56,889
as the original stl algorithms so we've

00:12:55,240 --> 00:13:00,240
got three examples on how this could be

00:12:56,889 --> 00:13:03,819
done and this is all defined it in 4406

00:13:00,240 --> 00:13:06,310
so we look at the first one we have for

00:13:03,819 --> 00:13:08,350
each call and we create a parallel

00:13:06,310 --> 00:13:10,450
policy but this time we attach an

00:13:08,350 --> 00:13:13,350
executor and in this case it's just the

00:13:10,450 --> 00:13:16,240
default parallel executor so in reality

00:13:13,350 --> 00:13:18,880
instead of doing paired on parallel

00:13:16,240 --> 00:13:21,700
executor you could just do pair because

00:13:18,880 --> 00:13:23,680
n rear and what you would expect of pair

00:13:21,700 --> 00:13:26,470
would be that it defaults to parallel

00:13:23,680 --> 00:13:29,200
executor nevertheless as an example I

00:13:26,470 --> 00:13:32,230
put it there um and you would expect the

00:13:29,200 --> 00:13:34,180
work to be done in parallel however we

00:13:32,230 --> 00:13:36,610
could also attach a sequential executor

00:13:34,180 --> 00:13:39,640
so maybe this is an execute that has one

00:13:36,610 --> 00:13:41,980
thread or require only does work in a

00:13:39,640 --> 00:13:44,890
way that maintains that order of what it

00:13:41,980 --> 00:13:47,230
when it was called then we are going to

00:13:44,890 --> 00:13:49,089
have sequential execution but we can

00:13:47,230 --> 00:13:53,050
still send it to a parallel execution

00:13:49,089 --> 00:13:55,210
policy because the policy only restricts

00:13:53,050 --> 00:13:57,400
that it can be done in parallel or

00:13:55,210 --> 00:13:59,860
sequentially but it doesn't say it has

00:13:57,400 --> 00:14:02,620
to be done in parallel so we want to

00:13:59,860 --> 00:14:05,650
allow that to happen but we don't want

00:14:02,620 --> 00:14:07,930
to allow a sequential policy to have a

00:14:05,650 --> 00:14:10,330
parallel executor so that does not make

00:14:07,930 --> 00:14:12,070
sense and so we're going to need a way

00:14:10,330 --> 00:14:14,230
for the executors to be able to

00:14:12,070 --> 00:14:16,630
advertise what sort of work it's done

00:14:14,230 --> 00:14:21,730
there what sort of work it implements or

00:14:16,630 --> 00:14:23,460
execute so the requirements to be met we

00:14:21,730 --> 00:14:26,980
want to be able to accept an executor

00:14:23,460 --> 00:14:29,589
advertise the restrictions and we'd also

00:14:26,980 --> 00:14:31,600
like at least for implementing these

00:14:29,589 --> 00:14:35,530
parallel algorithms or using executors

00:14:31,600 --> 00:14:38,080
in general an API that's easy to use and

00:14:35,530 --> 00:14:41,860
this is where executor traits for and

00:14:38,080 --> 00:14:44,290
4406 comes in there's four function

00:14:41,860 --> 00:14:47,500
calls and in other words two functions

00:14:44,290 --> 00:14:53,260
and they are a sink execute and execute

00:14:47,500 --> 00:14:55,390
a sink execute takes a function and it

00:14:53,260 --> 00:14:57,790
calls of that function with the past

00:14:55,390 --> 00:15:00,339
executor and it returns the future of a

00:14:57,790 --> 00:15:03,310
result of that function but there's also

00:15:00,339 --> 00:15:06,580
the bulk version of facing execute which

00:15:03,310 --> 00:15:09,640
can do which would just return a future

00:15:06,580 --> 00:15:12,459
void representing the function for each

00:15:09,640 --> 00:15:16,420
of a set of inputs where input supports

00:15:12,459 --> 00:15:19,779
beginning or supports an index range for

00:15:16,420 --> 00:15:22,050
and 4406 and execute

00:15:19,779 --> 00:15:25,029
is this analogous sequential or

00:15:22,050 --> 00:15:27,999
synchronous version so it can be it

00:15:25,029 --> 00:15:30,189
would just return a void for execute

00:15:27,999 --> 00:15:34,660
bulk but for the normal execute just the

00:15:30,189 --> 00:15:36,759
return of the function passed to it so

00:15:34,660 --> 00:15:40,120
here's an example of what we what we

00:15:36,759 --> 00:15:42,939
envisioned out of this we have some

00:15:40,120 --> 00:15:45,459
executor type we'll call it exec some in

00:15:42,939 --> 00:15:47,350
some shape type inputs and in this case

00:15:45,459 --> 00:15:50,920
will say that input supports it began in

00:15:47,350 --> 00:15:54,790
an end so we can do for auto and for

00:15:50,920 --> 00:15:57,309
auto v and inputs and so on we're going

00:15:54,790 --> 00:15:59,860
to have two functions f1 f2 and say they

00:15:57,309 --> 00:16:04,059
return a type the second one will have

00:15:59,860 --> 00:16:07,660
one argument and type def for traits to

00:16:04,059 --> 00:16:09,220
be called traits and so here are the

00:16:07,660 --> 00:16:14,399
four function calls just to make it

00:16:09,220 --> 00:16:18,069
clear and if we look at the second one

00:16:14,399 --> 00:16:20,230
that's that will call f2 once for each

00:16:18,069 --> 00:16:23,290
of the inputs and it will use the

00:16:20,230 --> 00:16:25,509
executor now what I personally find neat

00:16:23,290 --> 00:16:27,850
about this is that if our executor

00:16:25,509 --> 00:16:31,389
doesn't implement an async execute with

00:16:27,850 --> 00:16:35,500
this book overload it'll actually call

00:16:31,389 --> 00:16:37,600
um it'll actually synthesize the async

00:16:35,500 --> 00:16:41,160
execute bulk overload based off of the

00:16:37,600 --> 00:16:45,009
basic execute provided by the executor

00:16:41,160 --> 00:16:48,160
so what's neat about that is that it

00:16:45,009 --> 00:16:50,139
makes it easier to define the an

00:16:48,160 --> 00:16:53,230
executor all it has to do is implement a

00:16:50,139 --> 00:16:55,209
sink execute and same for execute and

00:16:53,230 --> 00:16:59,319
the bulk version of execute they don't

00:16:55,209 --> 00:17:00,939
need to define those functions because

00:16:59,319 --> 00:17:04,270
it can just be synthesized by a sink

00:17:00,939 --> 00:17:07,839
execute however in some cases it makes

00:17:04,270 --> 00:17:10,919
sense to define the eight in at bulk

00:17:07,839 --> 00:17:13,630
version of a sink execute because for an

00:17:10,919 --> 00:17:18,159
executor it might be easier to implement

00:17:13,630 --> 00:17:20,319
I create all of these function calls at

00:17:18,159 --> 00:17:23,260
once instead of just doing a simple for

00:17:20,319 --> 00:17:24,760
loop over the inputs in which case the

00:17:23,260 --> 00:17:30,210
executor traits would default to the

00:17:24,760 --> 00:17:32,580
executors function so once

00:17:30,210 --> 00:17:34,230
hpx not only as we implement the

00:17:32,580 --> 00:17:37,110
parallel algorithms but we afterwards

00:17:34,230 --> 00:17:41,250
and at the same time we started doing

00:17:37,110 --> 00:17:43,440
this proposal and we really like that we

00:17:41,250 --> 00:17:46,490
can extend our policies with the dot on

00:17:43,440 --> 00:17:49,169
syntax so that the user has more control

00:17:46,490 --> 00:17:51,870
it's a convenient uniform launch

00:17:49,169 --> 00:17:53,909
mechanism so all the way I'll only have

00:17:51,870 --> 00:17:59,220
to call create a parallel executor and

00:17:53,909 --> 00:18:02,240
executors with an async XE call um and

00:17:59,220 --> 00:18:05,580
we can do the work in book on quantities

00:18:02,240 --> 00:18:08,070
but we didn't quite like one feature and

00:18:05,580 --> 00:18:11,789
that feature was not maintaining the

00:18:08,070 --> 00:18:14,010
results for the book async execute so

00:18:11,789 --> 00:18:16,740
let's pretend that one of our function

00:18:14,010 --> 00:18:19,049
RF two calls to one of the inputs takes

00:18:16,740 --> 00:18:20,909
a really long time but all of the other

00:18:19,049 --> 00:18:24,029
calls doesn't take very much time and

00:18:20,909 --> 00:18:25,770
let's also suppose that if half of our

00:18:24,029 --> 00:18:29,010
calls are complete we can continue with

00:18:25,770 --> 00:18:30,929
something else so inside of using this

00:18:29,010 --> 00:18:34,640
version if we were to attach a

00:18:30,929 --> 00:18:37,770
continuation to this feature then we

00:18:34,640 --> 00:18:40,470
effectively insert a sequential portion

00:18:37,770 --> 00:18:42,570
of code because we have to wait for that

00:18:40,470 --> 00:18:45,390
one really long call potentially and

00:18:42,570 --> 00:18:48,179
that my tape calls a sequential point in

00:18:45,390 --> 00:18:51,020
the code so inside of hpx we decided to

00:18:48,179 --> 00:18:54,539
change it a little bit and we added the

00:18:51,020 --> 00:18:56,880
way to get the future results of our

00:18:54,539 --> 00:19:01,620
call and we do that with a vector of

00:18:56,880 --> 00:19:04,830
features and a bigot to REM adult and

00:19:01,620 --> 00:19:07,110
all off graph the idea is we want to

00:19:04,830 --> 00:19:11,220
give the user all the opportunity that

00:19:07,110 --> 00:19:14,580
they can to avoid sequential work and

00:19:11,220 --> 00:19:17,029
the idea is that if you have even a tiny

00:19:14,580 --> 00:19:20,940
amount of sequential work inside of your

00:19:17,029 --> 00:19:23,399
code then you're going to have a maximum

00:19:20,940 --> 00:19:28,200
theoretical speed up and that's what

00:19:23,399 --> 00:19:31,020
this graph shows so here's executor

00:19:28,200 --> 00:19:32,809
traits inside of HT PX just to give some

00:19:31,020 --> 00:19:37,470
more clarity about the things that does

00:19:32,809 --> 00:19:40,740
it exposes in X the executor type the

00:19:37,470 --> 00:19:43,370
execution category of the executor so is

00:19:40,740 --> 00:19:46,880
it sequential or is it parallel

00:19:43,370 --> 00:19:50,660
big ones also the future doesn't have to

00:19:46,880 --> 00:19:53,240
be a standard feature or even in HPA

00:19:50,660 --> 00:19:56,510
axis case the HP X future it can be

00:19:53,240 --> 00:19:59,180
defined by the executor but by default

00:19:56,510 --> 00:20:04,010
it would refer over to one of the

00:19:59,180 --> 00:20:07,460
defined features and we also implemented

00:20:04,010 --> 00:20:09,920
our async execute and execute and in

00:20:07,460 --> 00:20:13,010
addition there's apply execute so we

00:20:09,920 --> 00:20:15,320
just do that in so that it calls uses

00:20:13,010 --> 00:20:17,780
the executor to call an apply function

00:20:15,320 --> 00:20:19,580
which just sends off work and it doesn't

00:20:17,780 --> 00:20:21,830
require you to know what the return of

00:20:19,580 --> 00:20:25,030
that is and so you save any overhead you

00:20:21,830 --> 00:20:25,030
might have had from having a future

00:20:25,690 --> 00:20:31,100
however we want to know more information

00:20:29,179 --> 00:20:33,559
about these executors because they can

00:20:31,100 --> 00:20:36,710
do different types of things so we might

00:20:33,559 --> 00:20:40,070
have an executor that has three threads

00:20:36,710 --> 00:20:42,320
on it and it so it has three threads but

00:20:40,070 --> 00:20:45,380
we don't know that if we're calling them

00:20:42,320 --> 00:20:47,480
with the parallel algorithms so we also

00:20:45,380 --> 00:20:49,880
added an information traits where you

00:20:47,480 --> 00:20:54,440
attach an executor and by default it

00:20:49,880 --> 00:20:57,620
calls the so if by default looks for the

00:20:54,440 --> 00:21:00,650
executors function for get OS threads or

00:20:57,620 --> 00:21:05,120
get threads and it'll use that number to

00:21:00,650 --> 00:21:06,830
decide how to break up that work also

00:21:05,120 --> 00:21:08,510
those other traits just to show it's

00:21:06,830 --> 00:21:11,510
it's modular and there's different

00:21:08,510 --> 00:21:13,370
things you can expose to the user so

00:21:11,510 --> 00:21:17,480
there's time use act as executor traits

00:21:13,370 --> 00:21:21,850
um so do something on this executor at

00:21:17,480 --> 00:21:27,110
this time or after this length of time

00:21:21,850 --> 00:21:30,890
and here's our parallel executor I want

00:21:27,110 --> 00:21:32,179
to talk about the top part but first I'm

00:21:30,890 --> 00:21:35,270
going to talk about the bottom part to

00:21:32,179 --> 00:21:37,580
get there and this is just the

00:21:35,270 --> 00:21:39,679
implementation all it does is implement

00:21:37,580 --> 00:21:41,410
a sink execute and the rest is done by

00:21:39,679 --> 00:21:44,840
executor traits and that's at the bottom

00:21:41,410 --> 00:21:46,340
the template stuff at the out the

00:21:44,840 --> 00:21:48,590
template of the return type and all that

00:21:46,340 --> 00:21:52,940
says just the return type of the future

00:21:48,590 --> 00:21:57,230
of the function and then

00:21:52,940 --> 00:21:59,060
hpx a sink with a launch and that launch

00:21:57,230 --> 00:22:01,400
can be sent over in the constructor and

00:21:59,060 --> 00:22:03,890
so by default it does launch a sink and

00:22:01,400 --> 00:22:06,170
la chasing creates another thread for

00:22:03,890 --> 00:22:09,800
the future to be for the function to be

00:22:06,170 --> 00:22:12,230
run on but because hpx sort of manages

00:22:09,800 --> 00:22:14,420
the threads under the hood that we also

00:22:12,230 --> 00:22:16,490
expose something that the standard

00:22:14,420 --> 00:22:20,630
launch policies doesn't and we called

00:22:16,490 --> 00:22:22,400
that fork and so fork lets the main

00:22:20,630 --> 00:22:25,490
thread of execution take the function

00:22:22,400 --> 00:22:27,200
call F and then it lets other threads

00:22:25,490 --> 00:22:31,720
steal than the what was originally

00:22:27,200 --> 00:22:33,910
called on the main so that's

00:22:31,720 --> 00:22:36,590
continuation stealing as opposed to

00:22:33,910 --> 00:22:40,880
parents dealing and that that was made

00:22:36,590 --> 00:22:43,190
clear to me by n 3872 which is a primer

00:22:40,880 --> 00:22:45,620
on work stealing it was very useful for

00:22:43,190 --> 00:22:50,350
me because I was googling child stealing

00:22:45,620 --> 00:22:50,350
and not getting any results that helped

00:22:50,710 --> 00:22:57,170
so when you call it a sink you you're

00:22:55,100 --> 00:23:00,170
going to create another third of

00:22:57,170 --> 00:23:02,570
execution and so in in 3872 they have

00:23:00,170 --> 00:23:05,360
this example so you call E and then you

00:23:02,570 --> 00:23:09,140
spawn f and if you spawn f with in HPA

00:23:05,360 --> 00:23:12,050
or and a sink with a async launch policy

00:23:09,140 --> 00:23:14,690
then f will be up for grabs by other

00:23:12,050 --> 00:23:16,700
processes and then the main third of

00:23:14,690 --> 00:23:19,670
execution will go on to call it G and

00:23:16,700 --> 00:23:21,290
then when sink is called oh we're going

00:23:19,670 --> 00:23:24,350
to make sure that F is finished and then

00:23:21,290 --> 00:23:26,090
we can continue with H so if there's no

00:23:24,350 --> 00:23:30,350
other processors available to order of

00:23:26,090 --> 00:23:32,060
execution would be EG fh however if we

00:23:30,350 --> 00:23:35,630
were to use fork we could change that

00:23:32,060 --> 00:23:37,550
order of execution potentially so it

00:23:35,630 --> 00:23:40,610
would go e the main thread of execution

00:23:37,550 --> 00:23:43,700
would then execute f when we spawn it

00:23:40,610 --> 00:23:45,110
with a fork launch policy but then G

00:23:43,700 --> 00:23:46,970
would be up for grabs by other

00:23:45,110 --> 00:23:48,890
processors and if we didn't have any

00:23:46,970 --> 00:23:53,600
other processors the order of execution

00:23:48,890 --> 00:23:56,950
would have to go e f g h and i make this

00:23:53,600 --> 00:23:59,840
point um not just because it was a good

00:23:56,950 --> 00:24:01,550
primer on work stealing but because at

00:23:59,840 --> 00:24:04,250
the bottom and this is they explain why

00:24:01,550 --> 00:24:06,680
this is important if we have no other

00:24:04,250 --> 00:24:11,980
processes available and we're using

00:24:06,680 --> 00:24:16,430
a sink to launch our in F calls then

00:24:11,980 --> 00:24:19,880
that a sink launch policy will call F n

00:24:16,430 --> 00:24:21,350
times so o n operations of that function

00:24:19,880 --> 00:24:24,110
call so we're wasting a lot of memory

00:24:21,350 --> 00:24:28,190
and then once sink happens it has to go

00:24:24,110 --> 00:24:29,750
through and call f0 up to FN so we can

00:24:28,190 --> 00:24:31,790
avoid that and we can limit the maximum

00:24:29,750 --> 00:24:34,310
number of function calls to be the

00:24:31,790 --> 00:24:38,360
number of processors available if we use

00:24:34,310 --> 00:24:39,770
the fork and so with fork the main third

00:24:38,360 --> 00:24:41,870
of execution if there's no other

00:24:39,770 --> 00:24:48,320
processors just calls f0 all the way up

00:24:41,870 --> 00:24:49,880
to FN and then it sinks and so here are

00:24:48,320 --> 00:24:55,250
some of the types of executors that we

00:24:49,880 --> 00:24:56,990
have so far um obviously this thread

00:24:55,250 --> 00:24:59,210
executor does what you think it does it

00:24:56,990 --> 00:25:01,940
creates a thread and it sends its work

00:24:59,210 --> 00:25:04,370
to that thread we have the parallel

00:25:01,940 --> 00:25:07,880
sequential executors of course that are

00:25:04,370 --> 00:25:11,630
defaulted so if you use a sequel seek

00:25:07,880 --> 00:25:14,030
parallel asik policy then it uses a

00:25:11,630 --> 00:25:16,190
sequential executor by default and ditto

00:25:14,030 --> 00:25:19,100
for parallel policy then it uses the

00:25:16,190 --> 00:25:21,140
parallel executor by default they're

00:25:19,100 --> 00:25:23,870
also thread pool executors serves

00:25:21,140 --> 00:25:27,500
executors and I found this interesting

00:25:23,870 --> 00:25:29,660
because hbx does locality stuff so it

00:25:27,500 --> 00:25:32,540
does stuff on other nodes of computers

00:25:29,660 --> 00:25:35,570
um there is a notion of a distribution

00:25:32,540 --> 00:25:37,910
policy which existed before this idea of

00:25:35,570 --> 00:25:40,220
X of policies for the parallel

00:25:37,910 --> 00:25:43,610
algorithms but now we can have an

00:25:40,220 --> 00:25:45,200
executor that we create this would

00:25:43,610 --> 00:25:47,180
create this executor by passing and

00:25:45,200 --> 00:25:49,310
distribution policy and the distribution

00:25:47,180 --> 00:25:51,260
policy defines how work does is done on

00:25:49,310 --> 00:25:53,240
other localities and now we've just

00:25:51,260 --> 00:25:55,640
created an executor that does work on

00:25:53,240 --> 00:25:57,380
other localities and it was fairly

00:25:55,640 --> 00:26:02,570
straightforward to do and we can treat

00:25:57,380 --> 00:26:06,380
it like any other executor so taking a

00:26:02,570 --> 00:26:10,550
step back we've are through in 4406

00:26:06,380 --> 00:26:11,960
defined a way for launching work through

00:26:10,550 --> 00:26:16,160
the information traits we can have a

00:26:11,960 --> 00:26:18,020
flexible decision making um but we still

00:26:16,160 --> 00:26:19,940
haven't defined a general mechanism for

00:26:18,020 --> 00:26:20,519
great size control when we break up that

00:26:19,940 --> 00:26:23,700
work too

00:26:20,519 --> 00:26:27,179
down in parallel and this was done with

00:26:23,700 --> 00:26:30,929
the execution parameters so the idea is

00:26:27,179 --> 00:26:33,989
that it just has a function that defines

00:26:30,929 --> 00:26:35,609
how much work should be done given the

00:26:33,989 --> 00:26:38,700
number of processors available as

00:26:35,609 --> 00:26:41,369
defined by an executor so it's similar

00:26:38,700 --> 00:26:44,669
to the openmp dynamic static and guided

00:26:41,369 --> 00:26:47,159
but we've also used in auto chunk size

00:26:44,669 --> 00:26:49,049
and what that does is it measures an

00:26:47,159 --> 00:26:51,959
amount of work and then based off of

00:26:49,049 --> 00:26:54,209
that amount of work it create it decides

00:26:51,959 --> 00:26:59,969
what the rest of the work item should be

00:26:54,209 --> 00:27:02,279
done so here's how the user interacts

00:26:59,969 --> 00:27:05,369
with the pedal algorithms and it's via

00:27:02,279 --> 00:27:07,139
these sorts of policies um the idea is

00:27:05,369 --> 00:27:09,779
that it's easy to make your own type of

00:27:07,139 --> 00:27:12,899
executor or chunk size or executor

00:27:09,779 --> 00:27:15,899
parameters armed to the first one we

00:27:12,899 --> 00:27:17,789
have pair with and auto chunk size that

00:27:15,899 --> 00:27:22,049
is also equivalent to pair because

00:27:17,789 --> 00:27:24,899
that's the default chunk size um we can

00:27:22,049 --> 00:27:27,839
have our own so we can have pair with my

00:27:24,899 --> 00:27:32,879
executor and then I'll pair with my

00:27:27,839 --> 00:27:35,700
executor on my chunk size is that I

00:27:32,879 --> 00:27:39,389
think I got that backwards pair on an

00:27:35,700 --> 00:27:41,309
executor with a chunk size my bad um but

00:27:39,389 --> 00:27:44,099
at the same time the policies support

00:27:41,309 --> 00:27:45,869
tasks so we can also get future results

00:27:44,099 --> 00:27:49,109
of our parallel algorithms and we can

00:27:45,869 --> 00:27:54,239
use the task X cast task policy and it's

00:27:49,109 --> 00:27:58,739
just the same way so in doing this we

00:27:54,239 --> 00:28:01,139
sort of taken some concepts and created

00:27:58,739 --> 00:28:04,200
those concepts and so this is just a

00:28:01,139 --> 00:28:06,269
summarized there's restrictions so can

00:28:04,200 --> 00:28:08,219
this work be done in parallel can it be

00:28:06,269 --> 00:28:10,049
sequential and that's defined by the

00:28:08,219 --> 00:28:12,749
execution policy as it was originally

00:28:10,049 --> 00:28:14,459
intended but to make sure that the

00:28:12,749 --> 00:28:16,589
different things that can be done are

00:28:14,459 --> 00:28:19,889
separated then we have the other

00:28:16,589 --> 00:28:23,789
concepts so the executor defines where

00:28:19,889 --> 00:28:25,950
work is done and it also says hey I do

00:28:23,789 --> 00:28:29,039
my work in parallel or I do my work

00:28:25,950 --> 00:28:31,169
sequentially and then of course there's

00:28:29,039 --> 00:28:34,040
the grain size of work and if we want to

00:28:31,169 --> 00:28:36,830
do all everything on one note or

00:28:34,040 --> 00:28:39,200
one grant block of work or if we want to

00:28:36,830 --> 00:28:41,090
do on many blocks of work we can define

00:28:39,200 --> 00:28:43,160
that with the grain size and of course

00:28:41,090 --> 00:28:44,780
there's no cost of not using any of

00:28:43,160 --> 00:28:52,130
these and just using the pair and

00:28:44,780 --> 00:28:54,560
sequential all right so um we had Daniel

00:28:52,130 --> 00:28:56,180
talk about this great interface for

00:28:54,560 --> 00:28:58,550
these algorithms and how we now have all

00:28:56,180 --> 00:29:01,730
this flexibility for them but we need to

00:28:58,550 --> 00:29:03,470
write these algorithms so the thing with

00:29:01,730 --> 00:29:05,480
these parallel algorithms as defined is

00:29:03,470 --> 00:29:07,040
they will all take a range and they will

00:29:05,480 --> 00:29:09,860
all need to be partitioned and some

00:29:07,040 --> 00:29:11,980
action will be applied on this range of

00:29:09,860 --> 00:29:14,500
the important thing to note is that

00:29:11,980 --> 00:29:16,580
since you can do this you can create

00:29:14,500 --> 00:29:18,260
partition errs to solve this general

00:29:16,580 --> 00:29:20,300
problem with parallel algorithms and in

00:29:18,260 --> 00:29:24,520
hpx we wrote three different partition

00:29:20,300 --> 00:29:27,110
errs to implement all these algorithms

00:29:24,520 --> 00:29:29,480
our first partitioner is the for each

00:29:27,110 --> 00:29:31,790
partition ER and this is the simplest of

00:29:29,480 --> 00:29:34,280
the mall what it will simply do is it

00:29:31,790 --> 00:29:36,530
will take a range it will chunk up that

00:29:34,280 --> 00:29:38,000
range into equal partitions as defined

00:29:36,530 --> 00:29:40,700
by the amount of course you have in your

00:29:38,000 --> 00:29:44,090
computer or if you specify it a certain

00:29:40,700 --> 00:29:48,320
chunk size and it will apply some

00:29:44,090 --> 00:29:50,150
function f on each chunk so it's used in

00:29:48,320 --> 00:29:51,800
a lot of our really simple algorithms so

00:29:50,150 --> 00:29:56,630
implementing for each for example of

00:29:51,800 --> 00:29:58,580
course say films well it'll work by say

00:29:56,630 --> 00:30:02,480
you have a range of 12 elements and

00:29:58,580 --> 00:30:03,770
let's say you have 3 course so what the

00:30:02,480 --> 00:30:06,410
partitioner will do is it will

00:30:03,770 --> 00:30:09,130
distribute that range into three equal

00:30:06,410 --> 00:30:11,780
chunks containing four elements each and

00:30:09,130 --> 00:30:14,990
you've passed in a function f and this

00:30:11,780 --> 00:30:18,050
function f will be applied to each

00:30:14,990 --> 00:30:22,460
separate chunk and what that does is it

00:30:18,050 --> 00:30:25,030
allows us to execute on these algorithms

00:30:22,460 --> 00:30:28,670
in parallel and solve this problem so

00:30:25,030 --> 00:30:30,410
inside of for each n um the simple stuff

00:30:28,670 --> 00:30:33,980
is the 10 points we're defining at the

00:30:30,410 --> 00:30:35,180
top some just some extra stuff and where

00:30:33,980 --> 00:30:36,980
the reason we call it parallel is

00:30:35,180 --> 00:30:38,840
because for each n also has a sequential

00:30:36,980 --> 00:30:40,520
version because we also want to give the

00:30:38,840 --> 00:30:43,160
user a chance to just sequentially

00:30:40,520 --> 00:30:44,510
execute for each n and then in the

00:30:43,160 --> 00:30:45,890
middle is the important part when we're

00:30:44,510 --> 00:30:47,660
actually calling the partition

00:30:45,890 --> 00:30:51,650
wera forwarding any of the arguments we

00:30:47,660 --> 00:30:54,020
needed to and then this lambda function

00:30:51,650 --> 00:30:55,700
is what will be applied to each chunk so

00:30:54,020 --> 00:30:57,560
inside this lambda function we're

00:30:55,700 --> 00:30:59,270
looping through that chunk of data and

00:30:57,560 --> 00:31:01,340
we're applying that function f you

00:30:59,270 --> 00:31:04,400
passed into forage n on each separate

00:31:01,340 --> 00:31:06,680
element of the chunk at the very end

00:31:04,400 --> 00:31:09,110
we're returning in algorithm result of

00:31:06,680 --> 00:31:10,790
it so in the case that you asked for

00:31:09,110 --> 00:31:13,340
task will give you a future back that

00:31:10,790 --> 00:31:17,090
result and if not we'll simply give you

00:31:13,340 --> 00:31:19,520
the iterator back next up we have a

00:31:17,090 --> 00:31:21,290
partition ur so this is a little more

00:31:19,520 --> 00:31:25,070
complex than a forage partitioner it's

00:31:21,290 --> 00:31:28,070
got one extra step and what it'll do is

00:31:25,070 --> 00:31:30,560
that after we've partitioned this range

00:31:28,070 --> 00:31:32,930
into equal elements will apply some

00:31:30,560 --> 00:31:34,610
function onto each chunk and will store

00:31:32,930 --> 00:31:36,950
the result of that function on each

00:31:34,610 --> 00:31:38,660
chunk inside of a vector and then that

00:31:36,950 --> 00:31:40,790
vector will be passed to a second

00:31:38,660 --> 00:31:44,990
function so this is what you have to

00:31:40,790 --> 00:31:47,630
step algorithms like finder search give

00:31:44,990 --> 00:31:50,240
another example of 12 elements three

00:31:47,630 --> 00:31:53,270
cores it will evenly divide your data up

00:31:50,240 --> 00:31:54,770
first and what it'll do next is what's

00:31:53,270 --> 00:31:56,660
different here is that you have some

00:31:54,770 --> 00:31:59,380
vector which is set to the result of

00:31:56,660 --> 00:32:02,750
each function applied on these chunks

00:31:59,380 --> 00:32:04,580
and at the very end you have an

00:32:02,750 --> 00:32:09,050
additional function past in which will

00:32:04,580 --> 00:32:11,000
be applied to the vector V so um this is

00:32:09,050 --> 00:32:13,340
useful in an algorithm like reduce for

00:32:11,000 --> 00:32:15,110
example because what we're doing is will

00:32:13,340 --> 00:32:17,420
reduce is similar to accumulate but it

00:32:15,110 --> 00:32:19,010
can be parallelized you have the first

00:32:17,420 --> 00:32:21,260
lambda function like we had last time

00:32:19,010 --> 00:32:23,120
with the for each partitioner and what

00:32:21,260 --> 00:32:25,700
this will do is inside each chunk in

00:32:23,120 --> 00:32:28,400
parallel it'll grab that initial value

00:32:25,700 --> 00:32:30,020
and accumulate the rest of the chunk so

00:32:28,400 --> 00:32:32,750
now what we're trying to do is we have

00:32:30,020 --> 00:32:35,540
each chunk accumulated and we need to

00:32:32,750 --> 00:32:37,130
accumulate the chunks together and

00:32:35,540 --> 00:32:39,020
that's what this second lambda function

00:32:37,130 --> 00:32:42,430
does so this is the function that will

00:32:39,020 --> 00:32:45,170
be applied to the results of the vector

00:32:42,430 --> 00:32:49,550
an interesting thing that we had with

00:32:45,170 --> 00:32:51,230
reduce actually was that we had a

00:32:49,550 --> 00:32:53,420
coworker come in when we finished some

00:32:51,230 --> 00:32:55,880
of these algorithms and she needed to

00:32:53,420 --> 00:32:58,730
write a benchmark that computed the

00:32:55,880 --> 00:32:59,490
vector dot product in parallel and was

00:32:58,730 --> 00:33:01,170
so interesting

00:32:59,490 --> 00:33:03,179
is that since she heard about us she

00:33:01,170 --> 00:33:05,370
said okay we can go ahead and use reduce

00:33:03,179 --> 00:33:07,650
for example because if you were to do

00:33:05,370 --> 00:33:10,130
this sequentially let's say you have an

00:33:07,650 --> 00:33:13,050
array of X values and array of Y values

00:33:10,130 --> 00:33:14,550
to accumulate a dot product of this it

00:33:13,050 --> 00:33:16,710
would be really easy you could zip those

00:33:14,550 --> 00:33:19,020
to erase together you can set the

00:33:16,710 --> 00:33:21,600
initial value of the accumulation 20 and

00:33:19,020 --> 00:33:24,510
then inside each you could loop through

00:33:21,600 --> 00:33:26,250
it find the dot product that should

00:33:24,510 --> 00:33:28,440
actually be multiplied on the right hand

00:33:26,250 --> 00:33:31,320
side inside that lambda function give

00:33:28,440 --> 00:33:33,780
multiply the 2 x and y values together

00:33:31,320 --> 00:33:36,570
of their location and then add it on to

00:33:33,780 --> 00:33:38,309
the result but we found with reduce

00:33:36,570 --> 00:33:40,020
there's some requirements I'd that

00:33:38,309 --> 00:33:43,170
function that actually limit us and we

00:33:40,020 --> 00:33:44,880
can't do this so as you see here in the

00:33:43,170 --> 00:33:47,850
first lambda function we're setting the

00:33:44,880 --> 00:33:51,330
initial value that initial value t val

00:33:47,850 --> 00:33:53,730
is stuck being the type of the iterator

00:33:51,330 --> 00:33:55,260
that we passed in so when you have

00:33:53,730 --> 00:33:57,360
something like this and you have a zip

00:33:55,260 --> 00:33:59,820
iterator the type of the iterator will

00:33:57,360 --> 00:34:01,530
be a tuple in dereferenced so what this

00:33:59,820 --> 00:34:03,870
forced us to do when we wanted to write

00:34:01,530 --> 00:34:05,580
a dot product is we ended up having to

00:34:03,870 --> 00:34:07,290
make the initial value a tuple because

00:34:05,580 --> 00:34:09,450
when you dereference this zip iterator

00:34:07,290 --> 00:34:12,300
you're going to get a tuple of the x and

00:34:09,450 --> 00:34:13,889
y values respectively and then during

00:34:12,300 --> 00:34:16,109
the actual lambda function this also

00:34:13,889 --> 00:34:17,820
forced us to do tuple multiplication

00:34:16,109 --> 00:34:19,950
because now we had the result on the

00:34:17,820 --> 00:34:22,889
left side and then we had our x and y

00:34:19,950 --> 00:34:25,169
values it was a horribly hacky solution

00:34:22,889 --> 00:34:27,419
and we didn't like how we finished this

00:34:25,169 --> 00:34:30,210
this was the actual solution we didn't

00:34:27,419 --> 00:34:32,190
use it but we finished it and this is

00:34:30,210 --> 00:34:34,770
what led us to write a proposal on

00:34:32,190 --> 00:34:36,060
introducing transform reduce so this

00:34:34,770 --> 00:34:39,510
algorithm wasn't initially in the

00:34:36,060 --> 00:34:41,369
proposal but while we were doing this

00:34:39,510 --> 00:34:43,080
vector dot product problem we realized

00:34:41,369 --> 00:34:45,869
that this proposal you would need to

00:34:43,080 --> 00:34:49,020
transform reduce in cases like this and

00:34:45,869 --> 00:34:50,820
we proposed it in 4167 and it was

00:34:49,020 --> 00:34:53,700
implemented later on and it's now in the

00:34:50,820 --> 00:34:56,940
proposal but what transform reduced does

00:34:53,700 --> 00:34:58,980
is it'll add in an additional function

00:34:56,940 --> 00:35:01,619
you can pass and this function will be

00:34:58,980 --> 00:35:03,480
used as a convert function so now inside

00:35:01,619 --> 00:35:05,609
each chunk your respectively getting the

00:35:03,480 --> 00:35:08,190
initial value but it's going through a

00:35:05,609 --> 00:35:10,020
convert function first so in the case

00:35:08,190 --> 00:35:11,850
that your initial value is going to be

00:35:10,020 --> 00:35:12,370
of a tuple type you can pass it through

00:35:11,850 --> 00:35:13,930
convert

00:35:12,370 --> 00:35:15,550
function and get a double out of it and

00:35:13,930 --> 00:35:18,880
now this will allow you to accomplish

00:35:15,550 --> 00:35:21,580
anything you wanted to do as what you

00:35:18,880 --> 00:35:22,930
did with accumulate and then the

00:35:21,580 --> 00:35:24,670
accumulation function after we're

00:35:22,930 --> 00:35:26,260
grabbing the initial value we also have

00:35:24,670 --> 00:35:30,670
to pass in that convert as well to

00:35:26,260 --> 00:35:32,500
convert each subsequent element and we

00:35:30,670 --> 00:35:34,600
came up with a much more simplified dot

00:35:32,500 --> 00:35:37,630
product in this case so using transform

00:35:34,600 --> 00:35:40,480
reduce we were able to zip the to erase

00:35:37,630 --> 00:35:43,630
together set the initial value to zero

00:35:40,480 --> 00:35:45,910
this time didn't have to use a tuple our

00:35:43,630 --> 00:35:47,710
addition function was simply a standard

00:35:45,910 --> 00:35:50,410
plus so just add two doubles together

00:35:47,710 --> 00:35:53,650
and our convert functions so it'll grab

00:35:50,410 --> 00:35:55,150
that tuple of x and y values and it'll

00:35:53,650 --> 00:35:57,550
just find the dot product and return

00:35:55,150 --> 00:35:59,140
that so it's simply adding up a dot

00:35:57,550 --> 00:36:00,970
product which is found through convert

00:35:59,140 --> 00:36:02,650
function and we really like this

00:36:00,970 --> 00:36:06,220
solution so this was a good addition

00:36:02,650 --> 00:36:10,090
into the proposal and now onto the last

00:36:06,220 --> 00:36:11,680
one and the most complex the last one in

00:36:10,090 --> 00:36:14,770
two steps and now this one has three

00:36:11,680 --> 00:36:17,320
steps the scan partitioner will first

00:36:14,770 --> 00:36:18,940
partition the data and invoke a function

00:36:17,320 --> 00:36:21,310
on each equal partition as we do with

00:36:18,940 --> 00:36:23,260
the other ones now what's different in

00:36:21,310 --> 00:36:25,690
the second step is that will invoke a

00:36:23,260 --> 00:36:29,770
second function once the current and

00:36:25,690 --> 00:36:32,380
leftmost partition are ready so this

00:36:29,770 --> 00:36:35,110
will allow us to kind of overlap our

00:36:32,380 --> 00:36:37,540
second invocation and then on the third

00:36:35,110 --> 00:36:40,780
step we're going to voc a third function

00:36:37,540 --> 00:36:42,430
on the resultant vector of step 2 so

00:36:40,780 --> 00:36:44,530
this is really specific but it's for

00:36:42,430 --> 00:36:46,300
cases like copy if or inclusive

00:36:44,530 --> 00:36:49,230
exclusive scan and this allowed us to

00:36:46,300 --> 00:36:51,790
solve these problems so we'll do another

00:36:49,230 --> 00:36:55,900
example so you have your 12 elements

00:36:51,790 --> 00:36:58,120
again you'll partition that into equal

00:36:55,900 --> 00:37:00,760
chunks now let's say this is your

00:36:58,120 --> 00:37:03,250
current chunk so you just computed this

00:37:00,760 --> 00:37:05,380
current chunk once your leftmost one is

00:37:03,250 --> 00:37:08,710
also ready this will be stored in a

00:37:05,380 --> 00:37:10,750
vector V sub 0 and now let's say your

00:37:08,710 --> 00:37:13,540
move down the chain so now the rightmost

00:37:10,750 --> 00:37:16,000
one is your current chunk and that'll be

00:37:13,540 --> 00:37:19,660
V sub 1 will be equal to the leftmost

00:37:16,000 --> 00:37:21,340
end the current and then you can put

00:37:19,660 --> 00:37:23,230
those vectors together and apply some

00:37:21,340 --> 00:37:26,200
function on to that and that will be

00:37:23,230 --> 00:37:29,230
stored into R which is then finally

00:37:26,200 --> 00:37:31,240
applied to the function H so this is a

00:37:29,230 --> 00:37:34,000
three-step process a little more complex

00:37:31,240 --> 00:37:36,730
but it allowed us to solve a problem

00:37:34,000 --> 00:37:39,089
like copy if now the interesting thing

00:37:36,730 --> 00:37:40,839
when we were writing copy if at HP X is

00:37:39,089 --> 00:37:43,630
rina straight thought it was really

00:37:40,839 --> 00:37:46,000
simple I wrote it like the first week in

00:37:43,630 --> 00:37:47,560
half of starting and I left it and then

00:37:46,000 --> 00:37:49,660
maybe your month and a half later I

00:37:47,560 --> 00:37:53,020
realized how wrong I was when someone

00:37:49,660 --> 00:37:54,640
was trying to use it I'd initially only

00:37:53,020 --> 00:37:56,829
copied everything that matched the

00:37:54,640 --> 00:37:58,810
predicate but I didn't realize that when

00:37:56,829 --> 00:38:00,760
you're doing this in parallel you need

00:37:58,810 --> 00:38:03,040
to start squashing the array after and

00:38:00,760 --> 00:38:05,950
this became a problem because using the

00:38:03,040 --> 00:38:07,720
other partition errs I would have had to

00:38:05,950 --> 00:38:10,300
do two separate parallel calls that

00:38:07,720 --> 00:38:12,099
would have had a copy everything over in

00:38:10,300 --> 00:38:13,869
parallel and then squash everything in

00:38:12,099 --> 00:38:15,310
parallel and this was really inefficient

00:38:13,869 --> 00:38:17,619
because you don't want to construct

00:38:15,310 --> 00:38:19,450
resources execute something in parallel

00:38:17,619 --> 00:38:21,579
deconstruct the resources and then do

00:38:19,450 --> 00:38:24,310
that one more time so that's why we

00:38:21,579 --> 00:38:25,480
created the scan partitioner and I just

00:38:24,310 --> 00:38:27,190
so happened that this was really useful

00:38:25,480 --> 00:38:30,849
for stuff like inclusive and exclusive

00:38:27,190 --> 00:38:33,310
scan as well so inside the actual

00:38:30,849 --> 00:38:35,440
revision of copy if I couldn't fit the

00:38:33,310 --> 00:38:37,810
whole function on the screen but this is

00:38:35,440 --> 00:38:40,300
just the partitioner call so we're

00:38:37,810 --> 00:38:42,310
grabbing the scam partitioner and inside

00:38:40,300 --> 00:38:44,589
this first iteration which we apply it

00:38:42,310 --> 00:38:45,880
to each equal chunk we're simply

00:38:44,589 --> 00:38:48,819
flagging any of the elements to be

00:38:45,880 --> 00:38:50,530
copied in the second iteration which

00:38:48,819 --> 00:38:52,420
will be the current and leftmost chunk

00:38:50,530 --> 00:38:54,160
which will be applied to we want to

00:38:52,420 --> 00:38:56,470
determine the distance to advance the

00:38:54,160 --> 00:38:59,170
destination iterator for each partition

00:38:56,470 --> 00:39:01,420
and then our last which will be applied

00:38:59,170 --> 00:39:04,060
to the resultant of the vectors of step

00:39:01,420 --> 00:39:06,940
2 we're going to copy those elements

00:39:04,060 --> 00:39:09,280
into destination in parallel so this

00:39:06,940 --> 00:39:10,660
allowed us to tackle problems like copy

00:39:09,280 --> 00:39:16,270
if where you don't have a really

00:39:10,660 --> 00:39:17,560
intuitive solution to begin with as well

00:39:16,270 --> 00:39:19,780
designing some of these parallel

00:39:17,560 --> 00:39:21,550
algorithms we realize that we wanted to

00:39:19,780 --> 00:39:25,030
start simple and work up the grapevine

00:39:21,550 --> 00:39:26,800
so ways algorithms like for each it

00:39:25,030 --> 00:39:29,140
applies a really good base for building

00:39:26,800 --> 00:39:31,540
upon other algorithms and some of these

00:39:29,140 --> 00:39:34,300
future ones really easy to write

00:39:31,540 --> 00:39:36,099
actually so you have the case of like

00:39:34,300 --> 00:39:38,440
fill in for example we wrote that in

00:39:36,099 --> 00:39:39,420
technically one line so a lot of these

00:39:38,440 --> 00:39:42,369
algorithms are real

00:39:39,420 --> 00:39:45,220
related to each other in the sense that

00:39:42,369 --> 00:39:48,789
for Phil n all we need to do was call a

00:39:45,220 --> 00:39:50,140
parallel for regen and apply a custom

00:39:48,789 --> 00:39:52,539
lambda function to be applied to each

00:39:50,140 --> 00:39:55,480
element and all this would do is copy

00:39:52,539 --> 00:39:56,980
over the elements so what that for each

00:39:55,480 --> 00:39:59,980
end is actually doing with that boost

00:39:56,980 --> 00:40:02,589
MPL false stuff it's just we have some

00:39:59,980 --> 00:40:04,000
internal mechanism so this will call

00:40:02,589 --> 00:40:06,430
parallel without having to do any

00:40:04,000 --> 00:40:07,809
checking so we know we're executing

00:40:06,430 --> 00:40:09,880
parallel because we're calling the

00:40:07,809 --> 00:40:11,950
parallel fill n and then we're grabbing

00:40:09,880 --> 00:40:16,029
that parallel for each end and executing

00:40:11,950 --> 00:40:18,520
that as well as of today here are all

00:40:16,029 --> 00:40:20,049
the algorithms we've completed we're

00:40:18,520 --> 00:40:21,730
getting close to finishing up we still

00:40:20,049 --> 00:40:25,029
have some of the harder ones to do such

00:40:21,730 --> 00:40:26,670
as sort um and maybe a couple others but

00:40:25,029 --> 00:40:28,869
we aren't working on those currently

00:40:26,670 --> 00:40:30,460
alpha waiting at the end of the year or

00:40:28,869 --> 00:40:32,529
in the students future will have these

00:40:30,460 --> 00:40:34,089
all finished up and we'll be able to do

00:40:32,529 --> 00:40:36,250
some complete testing on all these to

00:40:34,089 --> 00:40:40,690
make sure what we're doing makes sense

00:40:36,250 --> 00:40:43,630
and then next up we wanted to measure

00:40:40,690 --> 00:40:44,829
these so after we finished about half

00:40:43,630 --> 00:40:46,329
our algorithms we wanted to make sure

00:40:44,829 --> 00:40:49,690
that what we're writing actually made

00:40:46,329 --> 00:40:51,700
sense and this is a quick example of one

00:40:49,690 --> 00:40:53,140
of the benchmarks we did to measure the

00:40:51,700 --> 00:40:56,579
performance this was a performance

00:40:53,140 --> 00:40:58,690
measure for for each we grabbed some

00:40:56,579 --> 00:41:00,640
parameters for the command line and

00:40:58,690 --> 00:41:02,950
we're looping through in applying some

00:41:00,640 --> 00:41:06,130
for each function on some data set which

00:41:02,950 --> 00:41:09,700
you specify and then we're averaging out

00:41:06,130 --> 00:41:11,440
the executions of it so when we were

00:41:09,700 --> 00:41:14,339
benchmarking we compared our sequential

00:41:11,440 --> 00:41:17,319
parallel and task execution policies

00:41:14,339 --> 00:41:18,730
it's important to mention that task we

00:41:17,319 --> 00:41:20,380
could do something special with them

00:41:18,730 --> 00:41:22,839
because how I mentioned that you can

00:41:20,380 --> 00:41:24,640
begin choosing when you want to join up

00:41:22,839 --> 00:41:27,339
with the threads with task we could

00:41:24,640 --> 00:41:30,160
overlap our executions so you have with

00:41:27,339 --> 00:41:31,599
a parallel execution policy if you want

00:41:30,160 --> 00:41:33,670
to call one parallel algorithm right

00:41:31,599 --> 00:41:35,890
after the other you would execute that

00:41:33,670 --> 00:41:37,990
parallel algorithm wait for that thread

00:41:35,890 --> 00:41:40,299
to finish join back up and then execute

00:41:37,990 --> 00:41:43,270
the next parallel algorithm but with

00:41:40,299 --> 00:41:46,240
futures and tasks what you can instead

00:41:43,270 --> 00:41:48,039
do is send both off at the same time and

00:41:46,240 --> 00:41:51,160
choose to wait for them after once

00:41:48,039 --> 00:41:53,680
you've sent both off so now you can do

00:41:51,160 --> 00:41:55,060
multiple pieces of work and you don't

00:41:53,680 --> 00:41:59,350
have to wait for that first one to

00:41:55,060 --> 00:42:00,580
finish until you wish to so in order to

00:41:59,350 --> 00:42:02,200
get out of the most out of performance

00:42:00,580 --> 00:42:04,930
we wanted to see the typical strong

00:42:02,200 --> 00:42:07,630
scaling graph what this means is that on

00:42:04,930 --> 00:42:10,150
the x-axis you have a grain size and on

00:42:07,630 --> 00:42:11,800
the y-axis you have a time scale the

00:42:10,150 --> 00:42:14,020
grain size means the amount of work

00:42:11,800 --> 00:42:15,670
you're putting on each thread if you

00:42:14,020 --> 00:42:17,020
have a really small grain size that

00:42:15,670 --> 00:42:18,850
means you're creating a lot of threads

00:42:17,020 --> 00:42:20,440
and not putting enough work on it and

00:42:18,850 --> 00:42:23,230
this is going to give you really bad

00:42:20,440 --> 00:42:25,120
performance because you'll get too much

00:42:23,230 --> 00:42:27,160
overhead of parallelism without enough

00:42:25,120 --> 00:42:30,160
actual work being executed to make up

00:42:27,160 --> 00:42:31,780
for this parallelism on the right if

00:42:30,160 --> 00:42:33,550
you're putting too much work on one

00:42:31,780 --> 00:42:34,900
thread you're not going to have any

00:42:33,550 --> 00:42:36,490
parallelism you're going to put all of

00:42:34,900 --> 00:42:38,650
your work on one thread and that's going

00:42:36,490 --> 00:42:41,620
to be sequential so there's a sweet spot

00:42:38,650 --> 00:42:43,840
for every algorithm in that you get the

00:42:41,620 --> 00:42:45,670
perfect amount of work at the perfect

00:42:43,840 --> 00:42:48,520
size and you'll get the most speed up

00:42:45,670 --> 00:42:50,350
now this isn't always known so we tried

00:42:48,520 --> 00:42:53,560
to just do an auto partition so we would

00:42:50,350 --> 00:42:54,970
grab the amount of cores you had we

00:42:53,560 --> 00:42:56,260
would split that up evenly with the

00:42:54,970 --> 00:42:59,740
amount of course to try and make use of

00:42:56,260 --> 00:43:01,270
all the threads but uh some executions

00:42:59,740 --> 00:43:04,240
if you don't know how much work actually

00:43:01,270 --> 00:43:05,860
occurs inside your for loops or anything

00:43:04,240 --> 00:43:07,750
it can be kind of hard to tweak so

00:43:05,860 --> 00:43:11,260
that's one of the things we are hoping

00:43:07,750 --> 00:43:13,330
to work on in the future we used a

00:43:11,260 --> 00:43:15,430
Marvin note on our harmony cluster so

00:43:13,330 --> 00:43:18,430
the important thing note is that it's

00:43:15,430 --> 00:43:20,740
got 16 cores so in a perfect world we

00:43:18,430 --> 00:43:23,740
wanted to see 16 times as fast parallel

00:43:20,740 --> 00:43:26,020
algorithms and while we didn't get 16

00:43:23,740 --> 00:43:29,680
times as fast we saw some decent results

00:43:26,020 --> 00:43:31,450
with some smaller data sizes so inside

00:43:29,680 --> 00:43:34,150
each for loop iteration if we had 500

00:43:31,450 --> 00:43:37,720
nano seconds of delay in a vector size

00:43:34,150 --> 00:43:39,370
of 10,000 we saw about 10 to 10 and a

00:43:37,720 --> 00:43:42,490
half times fast as a sequential

00:43:39,370 --> 00:43:44,830
algorithm which is still great it's not

00:43:42,490 --> 00:43:47,110
16 it's not perfect but it's still some

00:43:44,830 --> 00:43:48,970
great scaling that on the left you have

00:43:47,110 --> 00:43:52,450
the typical strong scaling graph where

00:43:48,970 --> 00:43:54,850
too much work to less work now we wanted

00:43:52,450 --> 00:43:57,640
to see if we could get even closer to 16

00:43:54,850 --> 00:43:59,890
times as fast so we went ahead and amped

00:43:57,640 --> 00:44:01,810
up the amount of work and what we did is

00:43:59,890 --> 00:44:04,060
we doubled the amount of delay per

00:44:01,810 --> 00:44:04,490
iteration and we also increase the

00:44:04,060 --> 00:44:08,210
vectors

00:44:04,490 --> 00:44:10,640
by 10 or 10 times the vector size so we

00:44:08,210 --> 00:44:13,340
had a vector size of almost or a hundred

00:44:10,640 --> 00:44:15,200
thousand and a thousand in a second

00:44:13,340 --> 00:44:17,390
delay and here we saw a lot better

00:44:15,200 --> 00:44:19,369
results so now that we have more work to

00:44:17,390 --> 00:44:22,130
do it makes more sense to paralyze it

00:44:19,369 --> 00:44:24,380
and now we're seeing about 14 kind of

00:44:22,130 --> 00:44:26,810
near 15 times as fast and this was

00:44:24,380 --> 00:44:29,170
really good for us because in a perfect

00:44:26,810 --> 00:44:31,430
world will see 16 and getting almost 15

00:44:29,170 --> 00:44:33,500
this pretty good sign that what we're

00:44:31,430 --> 00:44:35,000
doing is currently right and we can move

00:44:33,500 --> 00:44:37,910
forward with the rest of our algorithms

00:44:35,000 --> 00:44:39,860
and trying to finish this up now the

00:44:37,910 --> 00:44:41,930
cool thing with that task I mentioned is

00:44:39,860 --> 00:44:43,760
that now that you can overlap your

00:44:41,930 --> 00:44:45,970
executions you've almost got pseudo

00:44:43,760 --> 00:44:48,640
perfect scaling while it isn't actually

00:44:45,970 --> 00:44:51,290
16 times as fast what it means is that

00:44:48,640 --> 00:44:53,210
now that you're overlapping executions

00:44:51,290 --> 00:44:55,130
you no longer have to wait for previous

00:44:53,210 --> 00:44:57,260
executions to finish before you can

00:44:55,130 --> 00:44:59,060
launch more because i believe for this

00:44:57,260 --> 00:45:01,760
one we overlap two maybe three or four

00:44:59,060 --> 00:45:03,380
executions so we would launch three or

00:45:01,760 --> 00:45:05,180
four at a time until we waited for the

00:45:03,380 --> 00:45:07,580
first one and this gave us some really

00:45:05,180 --> 00:45:09,650
good scaling results and it made us

00:45:07,580 --> 00:45:11,359
happy because it actually makes sense to

00:45:09,650 --> 00:45:13,220
use a task in some scenarios when you

00:45:11,359 --> 00:45:14,810
have a lot of work and you may want to

00:45:13,220 --> 00:45:19,520
do some extra work before you actually

00:45:14,810 --> 00:45:22,660
wait for it one of my co-workers Martin

00:45:19,520 --> 00:45:25,520
stump also had a really cool

00:45:22,660 --> 00:45:29,150
implementation he had an open CL backend

00:45:25,520 --> 00:45:31,580
for HP X and he actually used one of our

00:45:29,150 --> 00:45:33,980
4-h algorithms inside of his work and

00:45:31,580 --> 00:45:35,510
this was for grouping work items into

00:45:33,980 --> 00:45:38,510
work packets and this was the actual

00:45:35,510 --> 00:45:41,000
algorithm he used it was a simple for

00:45:38,510 --> 00:45:43,160
each and he iterated over a range and

00:45:41,000 --> 00:45:46,670
did some function lambda function on

00:45:43,160 --> 00:45:49,280
each range and he saw some really good

00:45:46,670 --> 00:45:50,990
scaling as well so this was some of his

00:45:49,280 --> 00:45:54,830
research in that the speed-up verse

00:45:50,990 --> 00:45:56,840
sequential you have a good grain size

00:45:54,830 --> 00:45:58,130
and a good amount of work then you can

00:45:56,840 --> 00:46:00,290
see that you're getting really really

00:45:58,130 --> 00:46:02,270
good scaling with our algorithms then

00:46:00,290 --> 00:46:04,760
this could come in handy in hpx since

00:46:02,270 --> 00:46:07,010
we're all about large data asynchronous

00:46:04,760 --> 00:46:08,840
programming being able to suddenly take

00:46:07,010 --> 00:46:11,420
advantage of parallel algorithms without

00:46:08,840 --> 00:46:13,600
having to reinvent the wheel could be a

00:46:11,420 --> 00:46:17,430
big side bonus

00:46:13,600 --> 00:46:21,610
so we have all these good prose about

00:46:17,430 --> 00:46:23,380
C++ algorithms but we also had a little

00:46:21,610 --> 00:46:25,330
bit of downside and we have some future

00:46:23,380 --> 00:46:28,110
work that we want to do so one thing to

00:46:25,330 --> 00:46:31,300
note about these algorithms is that

00:46:28,110 --> 00:46:34,510
there's a fundamental design issue in

00:46:31,300 --> 00:46:36,580
that resources will not be shared with

00:46:34,510 --> 00:46:38,080
multiple parallel algorithms so it's

00:46:36,580 --> 00:46:40,390
important to note that if you're doing a

00:46:38,080 --> 00:46:41,920
lot of these algorithm calls it's going

00:46:40,390 --> 00:46:43,900
to construct these resources for a

00:46:41,920 --> 00:46:46,600
parallel algorithm go ahead and execute

00:46:43,900 --> 00:46:48,010
on those resources and then deconstruct

00:46:46,600 --> 00:46:49,930
those resources so if you're doing

00:46:48,010 --> 00:46:52,150
multiple calls of that it's going to be

00:46:49,930 --> 00:46:53,260
difficult to see some of the better

00:46:52,150 --> 00:46:56,080
performance if you're not sharing

00:46:53,260 --> 00:46:58,600
resources and this isn't necessarily a

00:46:56,080 --> 00:47:01,150
problem with how the algorithms are

00:46:58,600 --> 00:47:03,720
written it's just a design

00:47:01,150 --> 00:47:07,240
implementation to take into account that

00:47:03,720 --> 00:47:08,710
it's not as a not as efficient if you're

00:47:07,240 --> 00:47:11,770
calling many of these algorithms at the

00:47:08,710 --> 00:47:13,780
same time we also haven't finished all

00:47:11,770 --> 00:47:16,450
the algorithms so of course we want to

00:47:13,780 --> 00:47:18,160
make sure that we finish these and we

00:47:16,450 --> 00:47:19,780
also want to perform some additional

00:47:18,160 --> 00:47:22,030
benchmarking on these algorithms so we

00:47:19,780 --> 00:47:23,560
know where for each is doing great and a

00:47:22,030 --> 00:47:25,480
couple of other ones are doing great but

00:47:23,560 --> 00:47:28,690
we want to get into the more difficult

00:47:25,480 --> 00:47:30,610
ones like sort and we want to see our if

00:47:28,690 --> 00:47:32,080
our implementations are worthwhile and

00:47:30,610 --> 00:47:35,110
if we're still getting great speed up

00:47:32,080 --> 00:47:37,570
according to all the other algorithms as

00:47:35,110 --> 00:47:39,490
well grain size is important so right

00:47:37,570 --> 00:47:41,200
now we're simply just splitting up the

00:47:39,490 --> 00:47:44,170
amount of work into the amount of course

00:47:41,200 --> 00:47:46,510
available but sometimes if you have say

00:47:44,170 --> 00:47:48,450
a lot of work per iteration it might not

00:47:46,510 --> 00:47:51,160
be the smartest thing to just auto

00:47:48,450 --> 00:47:52,930
partition the grain size so we're doing

00:47:51,160 --> 00:47:55,360
some experiments of work into seeing if

00:47:52,930 --> 00:47:57,550
we can somewhat guess the grain size

00:47:55,360 --> 00:48:00,520
better and see if that will improve your

00:47:57,550 --> 00:48:02,310
performance as well and there's a lot of

00:48:00,520 --> 00:48:06,940
experimentation that we can do with

00:48:02,310 --> 00:48:09,460
custom policies so I've example that is

00:48:06,940 --> 00:48:12,220
it we don't have but if we have a GPU

00:48:09,460 --> 00:48:15,400
then do some work on the GPU otherwise

00:48:12,220 --> 00:48:19,660
do it on a NUMA domain with this chunk

00:48:15,400 --> 00:48:21,640
size um but also we can have make

00:48:19,660 --> 00:48:24,430
policies that connect with each other

00:48:21,640 --> 00:48:26,589
and use introspection tools that is

00:48:24,430 --> 00:48:27,729
inside of hpx already so

00:48:26,589 --> 00:48:30,249
there's performance counters that

00:48:27,729 --> 00:48:32,289
measure your power consumption we could

00:48:30,249 --> 00:48:35,759
also measure how long some function call

00:48:32,289 --> 00:48:37,779
takes inside of the async execute and

00:48:35,759 --> 00:48:40,210
based off of that we can have an

00:48:37,779 --> 00:48:42,789
executor that minimizes these things in

00:48:40,210 --> 00:48:46,089
some way when called with the function

00:48:42,789 --> 00:48:48,130
repeatedly or at least called with it

00:48:46,089 --> 00:48:51,009
some grain size inside of a parameters

00:48:48,130 --> 00:48:54,700
repeatedly and but those are just ideas

00:48:51,009 --> 00:49:12,789
at the moment um we can open the floor

00:48:54,700 --> 00:49:15,549
to any questions yes well we do have

00:49:12,789 --> 00:49:18,579
inner product I believe but we found

00:49:15,549 --> 00:49:21,400
that you have cases like transform

00:49:18,579 --> 00:49:23,759
reduce so the question was why would you

00:49:21,400 --> 00:49:28,239
not use inner product instead of

00:49:23,759 --> 00:49:30,579
transform reduce right or whoa yeah well

00:49:28,239 --> 00:49:32,950
we thought that transform reduce would

00:49:30,579 --> 00:49:34,509
be a necessary algorithm and that inner

00:49:32,950 --> 00:49:37,390
product could possibly solve that

00:49:34,509 --> 00:49:39,400
solution as well but um it's good

00:49:37,390 --> 00:49:42,609
designed to have some algorithm that

00:49:39,400 --> 00:49:43,900
could transform as you reduce over some

00:49:42,609 --> 00:49:46,029
range and we felt that would be useful

00:49:43,900 --> 00:49:48,279
to include in the algorithm list because

00:49:46,029 --> 00:49:49,769
I'm sure there are some other uses that

00:49:48,279 --> 00:49:55,859
transform reduce could have over the

00:49:49,769 --> 00:49:55,859
inner product any other questions

00:50:34,570 --> 00:50:38,830
mmm so the question was why would you

00:50:36,820 --> 00:50:41,260
not put the execution policy at the end

00:50:38,830 --> 00:50:44,020
instead at the beginning and I think

00:50:41,260 --> 00:50:45,640
that was just a design implementation at

00:50:44,020 --> 00:50:48,400
the proposal writer wanted because you

00:50:45,640 --> 00:50:50,350
wanted the how would execute to be the

00:50:48,400 --> 00:50:52,180
biggest priority in the algorithm so

00:50:50,350 --> 00:50:53,470
whether you want to execute in

00:50:52,180 --> 00:50:55,270
sequential parallel that should be your

00:50:53,470 --> 00:51:48,550
first stop before you put in the rest of

00:50:55,270 --> 00:51:50,380
your arguments Marshall and so the

00:51:48,550 --> 00:51:51,550
common was that there's different sizes

00:51:50,380 --> 00:51:53,350
in the argument so you want to

00:51:51,550 --> 00:51:56,760
disambiguate whether it's sequential

00:51:53,350 --> 00:51:56,760
parallel by placing it in the beginning

00:51:59,970 --> 00:52:05,110
the difficulties with sort are just the

00:52:03,280 --> 00:52:07,570
actual designing how the sort algorithm

00:52:05,110 --> 00:52:08,920
would work so I never actually got to

00:52:07,570 --> 00:52:12,010
the point where I would start tackling

00:52:08,920 --> 00:52:13,780
sort we apparently have one person at

00:52:12,010 --> 00:52:15,940
stellar currently attempting to write

00:52:13,780 --> 00:52:17,980
the sort algorithm but it's mostly

00:52:15,940 --> 00:52:19,870
communicating between these chunks

00:52:17,980 --> 00:52:21,640
because you can't simply sort each chunk

00:52:19,870 --> 00:52:24,010
because some data might be in reference

00:52:21,640 --> 00:52:26,980
to the first chunk and maybe the second

00:52:24,010 --> 00:52:28,060
one so we're likely gonna have to use

00:52:26,980 --> 00:52:29,140
something similar to the scam

00:52:28,060 --> 00:52:30,880
partitioner where you have these

00:52:29,140 --> 00:52:35,350
overlapping chunks that you can start

00:52:30,880 --> 00:52:36,850
communicating in between well I I was

00:52:35,350 --> 00:52:39,820
i'm communicating with the person who

00:52:36,850 --> 00:52:42,850
started doing that and i think how they

00:52:39,820 --> 00:52:45,200
did it was or how they're doing it is

00:52:42,850 --> 00:52:47,240
breaking it up into

00:52:45,200 --> 00:52:51,260
separate parts and then combining them

00:52:47,240 --> 00:52:53,150
back but I don't actually know um but it

00:52:51,260 --> 00:52:55,579
does lead into a problem with the

00:52:53,150 --> 00:52:58,400
executor parameters because then it's

00:52:55,579 --> 00:53:00,859
harder to define it does it could use

00:52:58,400 --> 00:53:02,990
recursion just in some sense so in that

00:53:00,859 --> 00:53:07,430
case we might be able to have the user

00:53:02,990 --> 00:53:09,920
control um how deep that recursion can

00:53:07,430 --> 00:53:12,200
get but at the same time that's not

00:53:09,920 --> 00:53:14,000
really the same as chunking into

00:53:12,200 --> 00:53:16,250
different partitions that the algorithms

00:53:14,000 --> 00:53:19,000
currently have and so that becomes a

00:53:16,250 --> 00:53:21,140
little bit tricky of how effective the

00:53:19,000 --> 00:53:23,180
executor parameters are or if we're just

00:53:21,140 --> 00:53:27,369
going to scrap them for some algorithms

00:53:23,180 --> 00:53:57,050
and their question was why is sort hard

00:53:27,369 --> 00:53:59,150
any other questions yeah so the question

00:53:57,050 --> 00:54:01,010
was why don't we just give a compiler

00:53:59,150 --> 00:54:02,990
error if we're trying to paralyze input

00:54:01,010 --> 00:54:04,460
iterators and that's because we don't

00:54:02,990 --> 00:54:06,230
want to place any new requirements on

00:54:04,460 --> 00:54:08,180
these algorithms so anything you could

00:54:06,230 --> 00:54:10,339
have done before simply by adding the

00:54:08,180 --> 00:54:13,160
part tag it should permit parallel

00:54:10,339 --> 00:54:14,960
execution so the design thing is we

00:54:13,160 --> 00:54:17,000
don't want to force the algorithm to

00:54:14,960 --> 00:54:20,540
parallel we're permitting a parallel

00:54:17,000 --> 00:54:21,619
execution so in cases like that you

00:54:20,540 --> 00:54:23,720
might not know the types of your

00:54:21,619 --> 00:54:25,400
iterators but you can say i want to

00:54:23,720 --> 00:54:27,200
permit parallel execution in this case

00:54:25,400 --> 00:54:29,810
and you know it won't give you a

00:54:27,200 --> 00:54:31,609
compiler error because the algorithm

00:54:29,810 --> 00:54:51,410
requirements are the same as before so

00:54:31,609 --> 00:54:55,190
that's why we decided that so the

00:54:51,410 --> 00:54:56,119
question was how is you need to term

00:54:55,190 --> 00:54:58,850
that the algorithm is run

00:54:56,119 --> 00:55:01,070
deterministically and

00:54:58,850 --> 00:55:06,500
I can't give you a solid answer for that

00:55:01,070 --> 00:55:24,650
you should see me after all right well

00:55:06,500 --> 00:55:26,660
oh so the question was could you see

00:55:24,650 --> 00:55:37,970
this tongue into daniks such as

00:55:26,660 --> 00:55:39,290
mapreduce and such um I can't say for

00:55:37,970 --> 00:55:41,180
sure because I'm not as experienced in

00:55:39,290 --> 00:55:43,250
this stuff I'm also new to parallel ism

00:55:41,180 --> 00:55:46,430
I'm simply just a student worker that

00:55:43,250 --> 00:55:48,170
really likes this stuff but uh um I

00:55:46,430 --> 00:55:50,000
could see having applications just due

00:55:48,170 --> 00:55:51,320
to its really broad and what we're doing

00:55:50,000 --> 00:55:53,360
and it could be applied to a lot of

00:55:51,320 --> 00:55:57,200
things and especially the runtime system

00:55:53,360 --> 00:56:00,050
the runtime system that we're doing this

00:55:57,200 --> 00:56:02,450
inside is applicable to applications of

00:56:00,050 --> 00:56:05,150
any scale so i could see the work being

00:56:02,450 --> 00:56:06,530
moved over a little bit and to speak to

00:56:05,150 --> 00:56:10,910
that there is some work at the stellar

00:56:06,530 --> 00:56:18,740
group about in parallelizing data and

00:56:10,910 --> 00:56:20,890
file systems so yes thank you guys for

00:56:18,740 --> 00:56:20,890

YouTube URL: https://www.youtube.com/watch?v=k6djT0a1q3E


