Title: CppCon 2015: Chandler Carruth "Tuning C++: Benchmarks, and CPUs, and Compilers! Oh My!"
Publication date: 2015-09-27
Playlist: CppCon 2015
Description: 
	http://www.Cppcon.org
—
A primary use case for C++ is low latency, low overhead, high performance code. But C++ does not give you these things for free, it gives you the tools to control these things and achieve them where needed. How do you realize this potential of the language? How do you tune your C++ code and achieve the necessary performance metrics?

This talk will walk through the process of tuning C++ code from benchmarking to performance analysis. It will focus on small scale performance problems ranging from loop kernels to data structures and algorithms. It will show you how to write benchmarks that effectively measure different aspects of performance even in the face of advanced compiler optimizations and bedeviling modern CPUs. It will also show how to analyze the performance of your benchmark, understand its behavior as well as the CPUs behavior, and use a wide array of tools available to isolate and pinpoint performance problems. The tools and some processor details will be Linux and x86 specific, but the techniques and concepts should be broadly applicable.
--
Chandler Carruth leads the Clang team at Google, building better diagnostics, tools, and more. Previously, he worked on several pieces of Google’s distributed build system. He makes guest appearances helping to maintain a few core C++ libraries across Google’s codebase, and is active in the LLVM and Clang open source communities. He received his M.S. and B.S. in Computer Science from Wake Forest University, but disavows all knowledge of the contents of his Master’s thesis. He is regularly found drinking Cherry Coke Zero in the daytime and pontificating over a single malt scotch in the evening.
--
Videos Filmed & Edited by Bash Films: http://www.BashFilms.com
Captions: 
	00:00:00,000 --> 00:00:03,260
So I'm going to turn it over to our headliner: Chandler Carruth.

00:00:03,280 --> 00:00:04,060
Thank you.

00:00:04,260 --> 00:00:09,460
(applause)

00:00:09,520 --> 00:00:10,880
CHANDLER: Morning everybody

00:00:11,360 --> 00:00:12,300
How are you guys doing?

00:00:13,080 --> 00:00:14,400
Excellent, excellent!

00:00:14,480 --> 00:00:18,800
So everyone who is at the back, you're going to want to scoot up a bit, OK?

00:00:18,880 --> 00:00:20,320
This is fair warning!

00:00:21,000 --> 00:00:23,260
Guys, you might want to start that shuffle process now...

00:00:23,260 --> 00:00:26,840
You're going to want to be able to read very little bitty text up here,

00:00:26,840 --> 00:00:27,920
for the whole talk, OK.

00:00:28,120 --> 00:00:30,820
So, my name is Chandler Carruth, I'm here from Google

00:00:31,360 --> 00:00:34,060
where I'm the C++ and LLVM tech lead

00:00:34,260 --> 00:00:36,380
I do a lot of fun compiler stuff,

00:00:36,540 --> 00:00:38,660
I do a lot of fun programming language stuff at Google

00:00:38,980 --> 00:00:41,840
and I'm here to talk to today about tuning C++.

00:00:41,840 --> 00:00:45,280
When I say tuning C++ I mean tuning C++ for performance -

00:00:45,280 --> 00:00:46,760
like runtime performance

00:00:46,760 --> 00:00:49,000
we want to have really fast and efficient code

00:00:49,000 --> 00:00:51,320
It's one of the primary reasons why we still use C++

00:00:51,360 --> 00:00:55,200
today is to get the last one and two percent of performance.

00:00:55,200 --> 00:00:58,000
I'm going to try to talk to you a little about how you can do that,

00:00:58,000 --> 00:00:59,480
I'm going to cover thing like benchmarking

00:00:59,980 --> 00:01:02,520
Some of the crazy things we see with CPUs

00:01:02,940 --> 00:01:04,500
Compilers (which are my bread and butter)

00:01:04,880 --> 00:01:06,400
All kind of crazy stuff

00:01:07,140 --> 00:01:07,940
Sound good?

00:01:08,620 --> 00:01:10,700
OK there are a couple of caveats.

00:01:10,700 --> 00:01:13,400
The first caveat is, and I actually do feel a little bit bad about this,

00:01:13,760 --> 00:01:15,920
This talk is going to be very Linux focussed

00:01:16,320 --> 00:01:19,920
Now I'm hopeful that the concepts are going to carry over to other operating systems

00:01:20,000 --> 00:01:23,020
I think there's similar software on almost every other operating system

00:01:23,020 --> 00:01:26,460
but I'm a Linux expert, I've got to teach from what I know

00:01:26,500 --> 00:01:28,140
It's going to be fairly Linux focussed

00:01:28,140 --> 00:01:30,060
It's also going to be x86 focussed

00:01:30,100 --> 00:01:33,400
Most of my performance work is on x86 these days

00:01:33,400 --> 00:01:34,900
and for better or worse

00:01:34,900 --> 00:01:37,240
And that means I'm going to talk a lot about x86

00:01:37,240 --> 00:01:38,700
There's going to be a lot of assembly

00:01:38,700 --> 00:01:40,400
and other stuff up on the projector

00:01:40,580 --> 00:01:44,060
So prepare yourself, right, for that part.

00:01:45,380 --> 00:01:47,640
The last thing I really want to caution you guys about is

00:01:47,680 --> 00:01:48,840
this is not going to be

00:01:49,500 --> 00:01:52,280
a talk similar to to Herb's talk or Bjarne's talk.

00:01:53,520 --> 00:01:55,900
I don't have the right answer here

00:01:56,120 --> 00:01:57,920
and that's not the point of this talk

00:01:57,980 --> 00:01:59,420
So you're going to see

00:01:59,460 --> 00:02:02,320
things that I'm not 100% proud of.

00:02:02,580 --> 00:02:04,500
You're going to see things that are very questionable

00:02:04,780 --> 00:02:06,700
but they're going to work (that's at least the hope).

00:02:06,860 --> 00:02:09,920
And to make sure that you believe me when I tell you that this stuff actually works

00:02:09,920 --> 00:02:12,540
and that you can do this if you go do this on your own,

00:02:12,920 --> 00:02:15,600
I'm going to do almost the entire talk live.

00:02:16,920 --> 00:02:18,240
So prepare yourself for that as well.

00:02:18,280 --> 00:02:19,640
Alright, so let's get started.

00:02:19,640 --> 00:02:21,440
So performance does actually matter

00:02:21,440 --> 00:02:25,200
How many folks here think that performance still matters today, that it's still relevant?

00:02:25,200 --> 00:02:27,280
Good - you guys have been paying attention!

00:02:27,280 --> 00:02:30,860
Now for anyone who is questioning, like really quickly

00:02:30,860 --> 00:02:34,080
do you have any guesses why performance still matters today?

00:02:35,100 --> 00:02:37,160
Anyone? You can shout stuff out, it's OK.

00:02:37,160 --> 00:02:38,240
(Audience mummers)

00:02:38,240 --> 00:02:39,620
Energy - I heard, yeah!

00:02:39,700 --> 00:02:46,320
So this guy - Nicola Tesla. We have this whole electricity thing, it's really cool, super-fun.

00:02:46,640 --> 00:02:48,660
And you know, electricity's great but

00:02:48,660 --> 00:02:50,900
this kind of gives the wrong impression about electricity

00:02:50,940 --> 00:02:53,880
because here electricity is just everywhere, right?

00:02:54,040 --> 00:02:55,680
It's just crazy... there's plenty of it

00:02:55,680 --> 00:02:57,920
Like we don't even know what to do with it, we're just studying it

00:02:58,340 --> 00:03:01,020
What we really have is a scarcity of electricity

00:03:01,480 --> 00:03:03,360
and the scarcity of electricity comes

00:03:03,360 --> 00:03:06,380
in almost every place where performance matters today.

00:03:06,660 --> 00:03:10,180
From the tiniest of little devices that we got to introduced to,

00:03:10,360 --> 00:03:13,800
that have these very frustrating batteries (sometimes),

00:03:14,460 --> 00:03:15,920
to the largest of data centers.

00:03:16,140 --> 00:03:21,100
And all of these things need electricity and they are all struggling to get it.

00:03:21,900 --> 00:03:25,180
That's the primary motivation behind performance work today.

00:03:25,180 --> 00:03:29,160
So this is what's in the back of my head constantly when I'm thinking about performance

00:03:29,620 --> 00:03:32,660
OK, the second thing I really want to talk at a high level about

00:03:32,660 --> 00:03:36,880
is exactly what C++ code you should spend your time tuning.

00:03:38,120 --> 00:03:39,860
Because, spending your time tuning C++ code -

00:03:39,940 --> 00:03:42,900
that introduces complexity, it's a hard task,

00:03:42,900 --> 00:03:45,260
you may make mistakes, you may even introduce bugs -

00:03:45,260 --> 00:03:47,160
we don't just want to tune anything.

00:03:48,360 --> 00:03:52,540
This actually became much more of a pressing concern for me

00:03:52,540 --> 00:03:56,380
when I first started at Google, OK?

00:03:56,380 --> 00:03:59,480
Very first- so I- you have to understand

00:03:59,480 --> 00:04:02,340
I started at Google almost eight years ago

00:04:02,340 --> 00:04:05,240
I started at Google right out of university

00:04:05,240 --> 00:04:08,080
I had no idea what I was doing

00:04:08,080 --> 00:04:10,480
I'm not kidding - I literally had no idea what I was doing

00:04:10,480 --> 00:04:12,860
I thought I was crazy for even going to Google...

00:04:13,820 --> 00:04:18,420
So on week one, they take you through this lovely orientation thing

00:04:18,420 --> 00:04:20,700
They say "Welcome to Google!", they give you funny hat to wear

00:04:20,700 --> 00:04:22,060
(Which doesn't go over too well)

00:04:22,960 --> 00:04:25,960
You get made fun of by Sergey Brin on stage

00:04:25,980 --> 00:04:27,700
It's a good experience on the whole

00:04:28,440 --> 00:04:30,220
It's a good experience, I was very happy...

00:04:30,680 --> 00:04:31,720
Then week two came around...

00:04:31,720 --> 00:04:38,420
(fumbles with mic)

00:04:38,680 --> 00:04:40,400
Sorry, my microphone doesn't like me.

00:04:42,380 --> 00:04:44,060
Hopefully it stays on this time

00:04:44,340 --> 00:04:48,300
So on week two, things get a little bit more interesting,

00:04:48,300 --> 00:04:50,760
I started to actually talk to my co-workers,

00:04:50,860 --> 00:04:52,700
figure out what team I was on

00:04:53,420 --> 00:04:54,320
That was fun,

00:04:54,320 --> 00:04:56,980
got to actually learn some stuff about what Google is actually doing

00:04:57,680 --> 00:05:00,360
And then I had a meeting with my manager at the time,

00:05:00,380 --> 00:05:01,440
great guy

00:05:03,160 --> 00:05:04,560
He had an interesting thing,

00:05:04,860 --> 00:05:05,860
He came to me and was like:

00:05:05,860 --> 00:05:06,940
"I've got a problem...

00:05:07,540 --> 00:05:10,320
"I've got a problem because

00:05:10,480 --> 00:05:14,400
"we have some code that we need someone to take a look at...

00:05:14,860 --> 00:05:17,620
"...and we thought you might be the perfect person to take a look at it"

00:05:17,620 --> 00:05:20,560
Which, by the way, for those of you who don't understand the translation,

00:05:20,560 --> 00:05:23,000
that was - "You're the new guy!"

00:05:24,300 --> 00:05:25,580
I didn't understand this at all...

00:05:25,580 --> 00:05:28,280
So he was like "We thought you'd be perfect". I was flattered....

00:05:28,280 --> 00:05:30,140
I was like: "Aw - of course I was!"

00:05:31,520 --> 00:05:33,680
And so he tells me:

00:05:34,580 --> 00:05:37,380
"We had this problem with some of our systems

00:05:37,580 --> 00:05:39,320
"they're really running into trouble...

00:05:39,320 --> 00:05:41,280
"The latency on this one system that we use internally

00:05:41,280 --> 00:05:43,140
for our internal development work"

00:05:43,140 --> 00:05:44,860
(I was on our internal developers tools team)

00:05:46,380 --> 00:05:50,700
"It's too slow, it's actually the bottleneck for almost all of our developers"

00:05:51,440 --> 00:05:52,380
That's a BIG problem!

00:05:52,380 --> 00:05:55,120
And this problem had bubbled up several levels of the company

00:05:55,120 --> 00:05:56,320
and across several

00:05:56,320 --> 00:05:57,580
different parts of the company

00:05:57,580 --> 00:05:58,400
and then down and back up

00:05:58,400 --> 00:06:00,100
and you know the whisper game?

00:06:00,240 --> 00:06:02,000
That you tell your kids...

00:06:02,000 --> 00:06:04,680
That's true in corporate politics as well

00:06:05,360 --> 00:06:09,120
So at some point it reached a totally different part of the company

00:06:09,120 --> 00:06:12,400
and had lost some small elements of truth to it

00:06:12,400 --> 00:06:18,260
and that part of the company had an extremely senior engineer who had decided to have a crack at this problem

00:06:18,260 --> 00:06:20,440
And you know he worked this problem

00:06:20,440 --> 00:06:22,340
but not talking to us, right?

00:06:22,340 --> 00:06:24,820
which is fine - he didn't even know he needed to talk to us

00:06:24,820 --> 00:06:28,960
because by the time he heard about the problem it was a totally differently described problem

00:06:31,220 --> 00:06:33,620
And then he sent my manager this code

00:06:33,620 --> 00:06:36,880
My manager pulls me in to ask me to take a look at this code

00:06:37,260 --> 00:06:39,440
Manager says to me "can you take a look at this code?

00:06:39,620 --> 00:06:43,620
"Ken Thompson sent it to us and he says it will this service twenty times faster

00:06:43,620 --> 00:06:46,920
"but we don't we really understand how, we need someone to step through it"

00:06:46,920 --> 00:06:48,380
Now I look at my manager

00:06:48,780 --> 00:06:51,100
Dead serious, I Iook at my manager and say -

00:06:51,400 --> 00:06:53,980
"Uh, sure but you know, who's Ken Thompson?"

00:06:54,360 --> 00:06:56,340
(audience laughter)

00:06:56,340 --> 00:06:58,880
I mentioned I didn't know anything at my second week at Google

00:06:59,020 --> 00:06:59,740
It was bad!

00:06:59,740 --> 00:07:04,900
I didn't actually know who Ken Thompson was and my manager very politely (perhaps too politely)

00:07:04,940 --> 00:07:07,900
smothered his burst of laughter and said:

00:07:07,960 --> 00:07:10,280
"A very senior engineer"

00:07:11,940 --> 00:07:15,200
Gave me the code, I went back, I went home, I got on Wikipedia, and looked into this...

00:07:15,200 --> 00:07:20,520
"Oh Ken Thompson! Oh OK, so the inventor of UNIX has sent me some code to look at..."

00:07:21,380 --> 00:07:22,400
That'll go over well...

00:07:23,960 --> 00:07:25,660
So I sat down, I figured

00:07:26,140 --> 00:07:28,020
It's my second week at Google - I've gotta do this!

00:07:28,020 --> 00:07:29,980
I can't fail now...

00:07:29,980 --> 00:07:31,000
So I sit down, stepping through this code

00:07:31,000 --> 00:07:33,040
Now the first thing I've got to say

00:07:33,040 --> 00:07:34,700
and I know this is a C++ conference,

00:07:34,700 --> 00:07:37,520
but Ken Thompson's code is actually what taught me

00:07:37,520 --> 00:07:40,320
that there was such a thing as beautiful C code

00:07:40,320 --> 00:07:42,400
I had always thought that C code was terrible

00:07:42,400 --> 00:07:43,680
Like - TERRIBLE!

00:07:43,680 --> 00:07:46,940
What I didn't know was that I was looking at terrible C code -

00:07:46,940 --> 00:07:48,860
it's an easy mistake to make!

00:07:49,540 --> 00:07:51,460
Terrible C code is terrible -

00:07:51,460 --> 00:07:53,300
it's not because it's in C it's because it's terrible code

00:07:53,300 --> 00:07:55,800
Ken Thompson knew how to actually write beautiful C code

00:07:55,800 --> 00:07:57,000
It was a wonderful experience,

00:07:57,000 --> 00:07:58,200
got to read through all this code

00:07:58,200 --> 00:07:59,600
understood the problem...

00:08:00,760 --> 00:08:02,260
The system we were trying to solve,

00:08:02,260 --> 00:08:09,380
it, was essentially serving out access control rules from this big long table of rules

00:08:09,480 --> 00:08:13,020
based on some pattern that you had to match in the table

00:08:13,020 --> 00:08:15,260
to say what access you were allowed to have.

00:08:15,260 --> 00:08:17,300
Straightforward problem, everybody has this -

00:08:17,300 --> 00:08:18,400
firewalls have this,

00:08:18,400 --> 00:08:21,580
filesystems have this - everybody has this problem it's a really common problem

00:08:21,600 --> 00:08:24,600
and Ken Thompson heard about this totally in the abstract

00:08:24,600 --> 00:08:30,260
and he noticed that this actually very similar to regular expression and finite automata construction

00:08:30,260 --> 00:08:32,880
That's one approach you can take to solving this problem

00:08:32,880 --> 00:08:37,200
And he's written a few regular expression engines in his life (a few!)

00:08:37,200 --> 00:08:38,220
Has some domain knowledge,

00:08:38,220 --> 00:08:40,060
and so he figured he'd help us out

00:08:40,060 --> 00:08:44,420
and he'd write us a finite automata based implementation of this rule matching

00:08:44,880 --> 00:08:50,000
And it used some of the regular expression finite automata code that he had written before

00:08:50,000 --> 00:08:52,120
and it was twenty times faster

00:08:52,120 --> 00:08:56,300
and in his email he says "this is twenty times faster, we can make it another twenty times faster

00:08:56,300 --> 00:08:58,200
but it would require substantially more memory

00:08:58,200 --> 00:09:02,280
"this seems like the sweet spot in terms of the tradeoff between time and space"

00:09:02,500 --> 00:09:03,620
Very reasonable.

00:09:03,620 --> 00:09:06,020
So I'm going through and I figure out all of this stuff,

00:09:06,020 --> 00:09:07,740
I learn all this stuff - it's wonderful!

00:09:08,660 --> 00:09:11,300
And then I send it to another one of my collegues

00:09:11,660 --> 00:09:14,620
who is actually responsible for the service

00:09:14,620 --> 00:09:16,520
and I explain to him how all this works

00:09:16,520 --> 00:09:17,840
and he says "well that's interesting,

00:09:17,840 --> 00:09:20,720
"so it really does get twenty times faster...

00:09:20,720 --> 00:09:24,320
"The funny thing is I was looking at our data the other day

00:09:24,320 --> 00:09:28,200
"for the queries we actually receive and which rules match them

00:09:28,200 --> 00:09:32,020
"and essentially every query matches against a single rule in the table

00:09:32,020 --> 00:09:35,320
"so I put that one first and it's one-hundred times faster now."

00:09:35,540 --> 00:09:38,520
(audience laughter)

00:09:39,400 --> 00:09:41,200
Well, that's unfortunate...

00:09:42,400 --> 00:09:46,880
So the moral of this story is - measure first and tune what matters

00:09:54,980 --> 00:09:56,360
and it actually solved the problem -

00:09:56,360 --> 00:09:58,240
it did exactly what he set out to do.

00:09:58,320 --> 00:10:01,600
Unfortunately, he didn't have all of the information

00:10:01,640 --> 00:10:06,860
and if you don't have all of the information you CAN'T make the right decision

00:10:06,860 --> 00:10:08,960
and you can't have the right information until you measure.

00:10:09,940 --> 00:10:13,980
So, that's my preachy bit of this talk. OK!

00:10:19,680 --> 00:10:20,860
I thought about spending a whole lot of time here

00:10:20,860 --> 00:10:24,740
and then I attended Bryce's talk [Benchmarking C++ Code - Bryce Adelstein Lelbach] yesterday and...

00:10:24,740 --> 00:10:26,480
Bryce where are you? Somewhere round here...

00:10:42,500 --> 00:10:45,440
We're going to talk about Micro Benchmarks.

00:10:45,820 --> 00:10:47,680
OK so micro benchmarks

00:10:47,680 --> 00:10:50,240
Who here knows what micro benchmarks even are?

00:10:50,660 --> 00:10:51,560
Got a few people...

00:10:51,820 --> 00:10:56,480
OK, so micro benchmarks are when you're benchmarking some really isolated part of your system

00:10:57,200 --> 00:10:59,660
Something whose performance matters desperately

00:10:59,660 --> 00:11:02,020
but is very very narrow in scope.

00:11:25,120 --> 00:11:27,440
But who here really loves slides?

00:11:27,600 --> 00:11:28,920
Anyone?

00:11:29,600 --> 00:11:32,600
Yeah, so I figured - let's just do it live!

00:11:33,160 --> 00:11:34,440
Alright (audience laugher)

00:11:35,480 --> 00:11:38,200
So the entire rest of this talk is going to be live

00:11:38,200 --> 00:11:40,860
Please bear with me, hopefully it's all going to work out well

00:11:42,200 --> 00:11:46,160
So what I'm going to try and do is I'm going to try and show you guys some microbenchmarking

00:11:46,160 --> 00:11:50,480
But before I do that I have to show you guys how we're going to write benchmarks

00:11:50,660 --> 00:11:52,420
If I can get this to work...

00:11:53,800 --> 00:11:56,120
There's this wonderful library,

00:11:56,280 --> 00:11:57,860
I can't take much credit for it

00:11:57,860 --> 00:11:59,680
But my company did this great thing

00:11:59,880 --> 00:12:01,840
we had the benchmarking library internally

00:12:01,840 --> 00:12:03,980
and we open-sourced it (this is all on Github)

00:12:04,020 --> 00:12:07,940
If you just search on Github for google/benchmark you'll find it - it's super easy

00:12:07,940 --> 00:12:09,800
This is a great little microbenchmark library

00:12:09,800 --> 00:12:12,200
I don't want to bore you guys with a detailed thing

00:12:12,200 --> 00:12:13,960
so I'm just going to skip all the fun stuff

00:12:14,300 --> 00:12:16,380
and point at some example usage

00:12:16,520 --> 00:12:19,340
So this is actually all on the webpage you can go and find it

00:12:19,340 --> 00:12:20,340
this is how this thing works

00:12:20,540 --> 00:12:22,380
You write a little function

00:12:22,500 --> 00:12:24,020
It takes a state object

00:12:25,360 --> 00:12:26,000
you run it

00:12:26,380 --> 00:12:27,100
for a while

00:12:27,500 --> 00:12:29,240
An unspecified amount of time

00:12:29,600 --> 00:12:31,140
And you do something in it

00:12:31,140 --> 00:12:32,180
And this benchmarks it

00:12:32,320 --> 00:12:34,160
Could, not, be, simpler - right?

00:12:34,440 --> 00:12:37,040
This is super simple, this is an entire benchmark

00:12:37,040 --> 00:12:40,300
Anybody have super-big questions about how this works?

00:12:41,320 --> 00:12:42,860
Please interrupt me at any point

00:12:42,980 --> 00:12:45,600
So let's look at a little bit more fanciness

00:12:45,600 --> 00:12:47,300
just so we understand how things work

00:12:47,300 --> 00:12:49,500
You can also do some other fun stuff with this benchmark library

00:12:50,300 --> 00:12:53,280
You can have... ranges of inputs

00:12:53,280 --> 00:12:56,000
So here we've got a range_x

00:12:56,000 --> 00:12:57,260
accessor on the state

00:12:57,860 --> 00:12:58,940
And what this is doing is...

00:12:58,940 --> 00:13:03,580
allowing you to control exactly what input space you benchmark

00:13:04,340 --> 00:13:05,840
you've got a bunch of arguments down at the bottom

00:13:06,680 --> 00:13:08,180
These go into x

00:13:08,860 --> 00:13:09,660
Make sense?

00:13:10,460 --> 00:13:11,840
Not rocket science here, right?

00:13:32,560 --> 00:13:34,640
we'll see lots of the fun output

00:15:17,360 --> 00:15:19,360
So we want to get a std::vector

00:15:19,680 --> 00:15:20,960
<int> like that...

00:15:21,520 --> 00:15:23,840
we're just going to push_back(42)

00:15:23,840 --> 00:15:29,600
Alright? Just to let us benchmark how long it takes to push_back an integer into a vector

00:15:29,600 --> 00:15:32,480
How many folks like this? Anyone like this?

00:15:32,480 --> 00:15:33,760
We've got a good benchmark going here

00:15:33,760 --> 00:15:36,320
Hmm, no-one's really happy about this...

00:15:36,640 --> 00:15:38,240
What's wrong with this?

00:15:38,240 --> 00:15:40,480
So we definitely need to fix our typos here

00:15:44,400 --> 00:15:44,960
Alright

00:15:46,160 --> 00:15:48,480
That seems plausible to me at least

00:15:48,480 --> 00:15:52,320
So the other thing we need to is figure out how to compile it

00:15:52,320 --> 00:15:55,520
Unfortunately compiling this thing requires a bunch of flags

00:15:56,720 --> 00:15:58,400
For which I'm kind of sorry

00:15:58,400 --> 00:15:59,680
We've got a bunch of flags here

00:15:59,680 --> 00:16:00,960
Optimisation flags

00:16:00,960 --> 00:16:03,520
I'm trying to turn on all the latest, greatest features

00:16:03,520 --> 00:16:05,440
We've got standard library flags

00:16:05,440 --> 00:16:07,040
But none of these really matter

00:16:07,040 --> 00:16:08,640
The only thing I'm really going to point out are a few things here

00:16:09,520 --> 00:16:12,160
So, I am turning off exceptions and RTTI

00:16:12,160 --> 00:16:13,120
Please don't throw anything at the stage

00:16:13,280 --> 00:16:16,960
But this is the environment I'm used to benchmarking in

00:16:16,960 --> 00:16:21,760
and so I'm trying to give you a faithful representation of what I see day in and day out

00:16:23,180 --> 00:16:23,820
Alright,

00:16:24,720 --> 00:16:26,620
We've got these lovely flags...

00:16:26,620 --> 00:16:28,260
So all I really need to do now...

00:16:28,260 --> 00:16:30,840
(I think I have a copy of this somewhere... here we go)

00:16:31,680 --> 00:16:33,280
Need to compile our code...

00:16:35,000 --> 00:16:37,140
Hopefully compiles... Cool, that worked well...

00:16:37,760 --> 00:16:38,860
And we can run it.

00:16:39,760 --> 00:16:41,920
And it's running, fantastic!

00:16:43,480 --> 00:16:47,380
So now this runs and does a bunch of cool stuff that's kind of under the hood

00:16:47,380 --> 00:16:48,920
So let's look at what we actually get out of this

00:16:49,220 --> 00:16:54,100
First thing is we get this nice column layout that tells us what benchmark we ran,

00:16:54,100 --> 00:16:56,640
how long that particular benchmark took,

00:16:56,740 --> 00:17:00,160
both in wall-time that's the first column here,

00:17:00,240 --> 00:17:01,720
and in CPU-time

00:17:01,740 --> 00:17:07,640
and CPU-time is interesting because we're discarding any kind of overhead where our thing got off the CPU -

00:17:07,640 --> 00:17:08,680
that's the CPU-time

00:17:09,340 --> 00:17:13,480
and to try and get some of what Bryce talked about in his benchmark thing

00:17:13,480 --> 00:17:16,800
This while library has a pile of internal logic

00:17:16,800 --> 00:17:21,600
To essentially run just amazing numbers of iterations until we get something reasonably stable

00:17:21,600 --> 00:17:24,000
We've go the iterations over here...

00:17:24,000 --> 00:17:25,280
So it ran... oh that's a big number

00:17:25,280 --> 00:17:26,720
22 million iterations

00:17:26,720 --> 00:17:30,360
in order to conclude that this operations takes 32 nanoseconds

00:17:30,880 --> 00:17:33,260
Now the one thing- I hate to rain on your parade-

00:17:33,280 --> 00:17:34,560
but if I run it again

00:17:34,560 --> 00:17:36,800
you're going to see some fluctuations

00:17:36,800 --> 00:17:39,840
So the wall-time here flucuated by three nanoseconds

00:17:39,840 --> 00:17:42,400
You're going to see the CPU-time fluctuate as well,

00:17:42,400 --> 00:17:47,160
even with tens of millions of runs we still have a noisy system.

00:17:47,840 --> 00:17:51,500
If you want to understand more of how to cope with that that's what Bryce was really talking about.

00:17:58,420 --> 00:18:00,840
Last little bit of statistical confidence

00:18:00,840 --> 00:18:01,960
of how fast things are

00:18:01,980 --> 00:18:03,820
Everybody happy with this benchmark now?

00:18:03,820 --> 00:18:05,200
How many folks are happy now?

00:18:05,360 --> 00:18:08,320
Why are you guys not happy with my benchmark?

00:18:09,820 --> 00:18:10,660
(audience member shouts "malloc")

00:18:10,800 --> 00:18:12,860
Malloc! Oh man!

00:18:13,020 --> 00:18:14,920
So I'm not benchmarking push_back

00:18:15,300 --> 00:18:16,520
You're totally right...

00:18:16,520 --> 00:18:17,840
So how do I actually compensate for that?

00:18:17,840 --> 00:18:20,740
Well, to compensate for that is actually hard...

00:18:48,000 --> 00:18:53,760
One thing that's a little bit annoying, if we just take this vector and pull it out of the loop, right?

00:18:53,760 --> 00:18:55,360
Then we're going to benchmark push_back,

00:18:55,360 --> 00:18:58,400
But we're going to run out of memory when we push_back

00:18:58,400 --> 00:18:59,500
too many millions and millions of integers -

00:18:59,500 --> 00:19:03,580
we're going to see other artefacts of the system because we're not actually clearing the memory.

00:19:03,780 --> 00:19:04,920
So we need to do something else

00:19:04,920 --> 00:19:06,760
Now, a good way of this is often

00:19:06,800 --> 00:19:09,120
to just, kind-of, set up a baseline

00:19:10,040 --> 00:19:12,260
Let's look at kind-of an example of this

00:19:13,780 --> 00:19:16,420
So here's a baseline benchmark so instead of push_back,

00:19:16,420 --> 00:19:20,040
we're going to benchmark just the creation of this vector

00:19:24,040 --> 00:19:25,380
Alight, is this making sense?

00:19:25,500 --> 00:19:26,620
So here we've got a new benchmark

00:19:26,620 --> 00:19:28,460
this is going to benchmark constructing the vector

00:19:28,460 --> 00:19:31,260
and then the second we're going to benchmark constructing it

00:19:31,420 --> 00:19:32,380
plus push_back

00:19:32,380 --> 00:19:35,640
we're going to... be able to look at the delta between these two

00:19:35,640 --> 00:19:37,640
to understand the performance impact of push_back

00:19:38,700 --> 00:19:41,720
So, any predictions on how fast the first one is going to run?

00:19:41,720 --> 00:19:45,000
Like what percentage of the time is this first one going to see?

00:19:46,860 --> 00:19:48,100
90%?

00:19:48,980 --> 00:19:49,860
0%!

00:19:56,480 --> 00:19:57,960
Everybody see the bottom here?

00:19:59,220 --> 00:20:01,800
Zero nanoseconds? ZERO nanoseconds!

00:20:01,820 --> 00:20:03,340
It's infinitely fast...

00:20:03,340 --> 00:20:05,340
It's amazing, right? I love code like this!

00:20:05,500 --> 00:20:06,780
Infinite speed...

00:20:06,940 --> 00:20:07,660
It's great!

00:20:11,240 --> 00:20:14,700
it's just the default constructor, it didn't do anything...

00:20:14,700 --> 00:20:17,380
So we've got to work a little bit harder to isolate this

00:20:17,820 --> 00:20:20,600
So what do we actually have to do to isolate this?

00:20:21,440 --> 00:20:23,020
We've got this nice create thing

00:20:23,300 --> 00:20:24,180
That's great

00:20:24,400 --> 00:20:25,920
And let's keep that around

00:20:26,320 --> 00:20:30,120
and we need something a little bit fancier to really benchmark this

00:20:30,720 --> 00:20:32,240
What if we use reserve?

00:20:33,360 --> 00:20:34,660
So we here can say

00:20:36,000 --> 00:20:39,140
And that's going to take the malloc, and actually benchmark the malloc

00:20:39,920 --> 00:20:41,600
That'll work, that'll work

00:20:43,240 --> 00:20:45,480
How many folks think this is going to actually work?

00:20:46,340 --> 00:20:48,760
Yeah, you're catching on to the theme of this talk

00:20:52,200 --> 00:20:54,920
So now reserve in our push_back benchmark

00:20:55,660 --> 00:20:58,100
and we also do just a blind reserve

00:20:58,100 --> 00:21:00,480
so that we should be able to see the delta now

00:21:01,840 --> 00:21:04,480
How much of the performance is this going to take?

00:21:05,160 --> 00:21:09,160
Is the reserve going to be 0% or more than 0% of the performance?

00:21:10,480 --> 00:21:11,520
Go on, shout it!

00:21:12,440 --> 00:21:14,360
4%! 4%?

00:21:15,980 --> 00:21:17,600
More! More than 90%!

00:21:17,600 --> 00:21:18,900
OK, there we go...

00:21:18,900 --> 00:21:21,140
So this is going to be all of the performance

00:21:22,680 --> 00:21:24,000
Let's find out

00:21:24,000 --> 00:21:26,460
So we call our thing, run it...

00:21:28,320 --> 00:21:31,000
Wait, now this is a little bit weird

00:21:32,840 --> 00:21:35,740
So I know reserve is good and all

00:21:35,740 --> 00:21:39,160
And you know I've read Scott Meyer's books and all these other books

00:21:41,240 --> 00:21:43,880
I even watched my own talk from last year

00:21:44,200 --> 00:21:47,200
and so I know I should do reserve whenever I can

00:21:47,200 --> 00:21:52,060
but I did not expect reserve to make push_back TEN TIMES faster

00:21:52,340 --> 00:21:54,260
That's a little bit weird...

00:22:46,240 --> 00:22:49,220
So my FAVOURITE profiler in the world

00:22:49,220 --> 00:22:51,280
(and I'm actually not being facetious here)

00:22:51,280 --> 00:22:52,760
is this little tool called perf

00:24:09,960 --> 00:24:11,580
That kind of stuff doesn't go into this

00:24:11,760 --> 00:24:14,060
and so wall-time is done at the bottom here,

00:24:14,060 --> 00:24:17,440
Is not always exactly equal to this task-clock

00:24:18,560 --> 00:24:20,840
But it also tells you how much precision this has

00:24:20,840 --> 00:24:22,900
This doesn't necessarily have perfect precision

00:25:15,760 --> 00:25:18,600
A coarse grained look at exactly what performance I'm seeing

00:25:18,760 --> 00:25:20,360
it's a fantastic tool

00:25:20,440 --> 00:25:22,680
But this doesn't actually tell us

00:25:22,680 --> 00:25:26,960
what on earth our code is doing that made it ten times faster when we reserved

00:25:27,280 --> 00:25:31,960
So we don't yet have the information - we need to do more detailed performance analysis

00:25:32,160 --> 00:25:34,400
To do that you can record with perf

00:25:34,400 --> 00:25:37,760
Now this is going to do a more traditional profiling run

00:25:38,200 --> 00:25:41,240
it's just going to record what takes place as your program executes

00:25:41,240 --> 00:25:43,300
and dump out this very dense,

00:25:43,300 --> 00:25:45,900
very strangely formatted data file at the end

00:25:46,020 --> 00:25:47,600
that contains the entire profile

00:25:47,680 --> 00:25:48,960
and once you do this

00:25:48,960 --> 00:25:51,780
There's a tool, the tool has a way to analyze it

00:25:52,480 --> 00:25:54,080
You can do "perf report"

00:25:54,380 --> 00:25:56,220
creates a nice little report

00:25:56,220 --> 00:25:58,020
perf report looks something like this

00:25:58,020 --> 00:25:59,980
it's going to show what you're dong here

00:25:59,980 --> 00:26:01,280
and so it's actually showing

00:26:01,540 --> 00:26:02,740
from the hardware

00:26:02,740 --> 00:26:04,000
exactly what functions

00:26:04,000 --> 00:26:05,940
were executing and how much of the time

00:26:06,160 --> 00:26:07,840
So this is your most basic profile -

00:26:07,840 --> 00:26:09,380
it's a flat profile, right?

00:26:09,620 --> 00:26:11,140
Here are the functions

00:26:11,140 --> 00:26:13,200
Here is how long we were actually inside of them

00:26:13,540 --> 00:26:14,700
Making some sense?

00:26:15,640 --> 00:26:16,360
Alright!

00:26:16,600 --> 00:26:19,920
So that's great but there's one kind of problem here

00:26:20,660 --> 00:26:26,180
There's no context to understand "is this function calling some other function?"

00:26:26,180 --> 00:26:29,000
Because we were trying to isolate out malloc()

00:26:29,620 --> 00:26:31,300
The allocation of memory

00:26:31,300 --> 00:26:32,640
from this benchmark

00:26:32,640 --> 00:26:34,360
But we don't actually get that information here -

00:26:34,360 --> 00:26:36,740
we don't know if there's memory allocation or something else going on...

00:26:36,740 --> 00:26:38,620
I see malloc down here, right?

00:26:38,920 --> 00:26:41,000
- we're spending some time in it

00:26:41,000 --> 00:26:43,860
but there's no connections between this stuff

00:26:43,860 --> 00:26:45,920
What we really want is a callgraph

00:26:46,400 --> 00:26:48,260
A dynamic callgraph that tell us

00:26:48,260 --> 00:26:50,860
well this function then calls this other function,

00:26:50,860 --> 00:26:52,100
which then calls this other function,

00:26:52,100 --> 00:26:54,540
and that's where the performance problem really lies.

00:27:59,140 --> 00:28:01,540
and when it interrupts your program

00:28:02,500 --> 00:28:05,220
it looks at the current state of the stack

00:28:05,220 --> 00:28:06,800
of the thread that is running

00:28:07,040 --> 00:28:11,280
and then it tries to figure out the call stack from that position

00:28:11,280 --> 00:28:16,280
Well, the problem here is that it doesn't have any way of finding that call stack

00:28:17,220 --> 00:28:22,820
Because I've optimized all of the information about the callgraph out of the binary

00:28:31,700 --> 00:28:33,700
You can actually fix this, OK?

00:28:35,580 --> 00:28:40,220
I know this is really ghetto - I'm using a text file for flag management

00:28:40,220 --> 00:28:41,980
But it fits in my presentation so...

00:28:42,600 --> 00:28:43,360
and I don't like Make

00:28:44,420 --> 00:28:46,180
There's this magical flag

00:28:48,320 --> 00:28:49,440
This flag...

00:28:49,700 --> 00:28:54,900
Is the performance analyzer's most endearing flag

00:28:54,900 --> 00:28:55,940
OK? Now what does it do?

00:28:55,940 --> 00:28:59,960
-fno-omit-frame-pointer is a very opaque flag name

00:28:59,960 --> 00:29:02,360
So what we're actually telling the compiler to do

00:29:02,540 --> 00:29:06,740
is to stop deleting the "frame pointer"

00:29:07,020 --> 00:29:08,860
Now the frame pointer on x86

00:29:08,860 --> 00:29:10,580
(and most other architectures)

00:29:10,660 --> 00:29:15,800
is a register which tells you where the bottom of stack frame is

00:29:15,800 --> 00:29:19,340
Or the top of the stack frame is depending on how it grows on your architecture

00:29:20,160 --> 00:29:23,000
So with the frame pointer you can kind of orient yourself.

00:29:23,000 --> 00:29:25,400
At any point in the program's execution you can orient yourself

00:29:25,400 --> 00:29:27,840
and understand how to walk up the stack

00:29:27,840 --> 00:29:32,180
and discover the call sequence that led to the place you're currently at.

00:29:32,820 --> 00:29:34,160
This is AWESOME!

00:31:31,920 --> 00:31:33,120
from the perf tool

00:31:33,120 --> 00:31:34,400
You can hit the 'a' key

00:31:36,160 --> 00:31:37,480
actually hold on, before I do that

00:31:38,560 --> 00:31:40,160
let's look at perf a little bit more...

00:31:40,160 --> 00:31:41,440
before we dive deep let's look at this a bit more

00:31:41,440 --> 00:31:43,360
There's one other thing that's a little bit confusing here

00:31:43,780 --> 00:31:47,980
So, you'll see if I expand this benchmark thing...

00:31:49,200 --> 00:31:51,040
Hold on, if I expand this benchmark thing

00:31:52,080 --> 00:31:57,440
It's not calling push_back... or create... or reserve...

00:31:57,440 --> 00:31:59,360
So we actually don't yet understand this

00:31:59,360 --> 00:32:01,920
This is actually my LEAST favourite thing about the perf tool

00:32:01,920 --> 00:32:05,120
So this callgraph is not the callgraph you expected

00:34:18,960 --> 00:34:21,360
what I naively expect it to look like

00:34:22,560 --> 00:34:24,080
OK, and now when I expand this I see

00:34:26,680 --> 00:34:28,460
We start the benchmark framework

00:34:28,820 --> 00:34:29,520
Cool

00:34:29,520 --> 00:34:31,320
and it calls three functions

00:34:31,600 --> 00:34:34,400
Ah yes, because we have three benchmark functions.

00:34:34,640 --> 00:34:38,600
Is that making sense to people now (that nice callgraph)? OK good, phew!

00:34:38,720 --> 00:34:40,880
Now you can keep expanding here,

00:34:41,520 --> 00:34:42,020
But,

00:34:42,080 --> 00:34:45,200
you're going to see strange artifacts if you do

00:34:46,900 --> 00:34:50,340
I strongly encourage you not to just keep expanding

00:34:50,540 --> 00:34:51,680
up here

00:34:51,680 --> 00:34:55,280
But drop down and find where this function is rooted

00:34:55,720 --> 00:34:56,760
right down here

00:34:56,760 --> 00:34:57,760
and look at that

00:34:57,760 --> 00:34:59,220
and you'll get slightly better information

00:34:59,220 --> 00:35:02,780
But you'll still be confused because it says it does weird stuff

00:35:03,780 --> 00:35:07,540
like, it seems to be calling this apic_timer_interrupt

00:35:07,540 --> 00:35:09,380
which doesn't make any sense

00:35:09,380 --> 00:35:11,060
but that's because it's in the kernel

00:35:11,200 --> 00:35:14,480
this stuff looks really confusing initially when you see it

00:35:14,480 --> 00:35:16,980
and the thing you have to understand is that

00:35:17,480 --> 00:35:20,040
the perf profiling tool isn't precise

00:35:21,360 --> 00:35:24,720
what it's doing is that it's sampling your program

00:35:24,860 --> 00:35:27,020
in the least intrusive way it can

00:35:27,020 --> 00:35:28,640
so that your program can keep running

00:35:29,140 --> 00:35:31,040
at blinding speed forward

00:37:00,560 --> 00:37:02,720
But it's not infinitely precise

00:37:02,720 --> 00:37:06,100
in fact there are a lot of artifacts and you're going to see them

00:37:06,100 --> 00:37:07,180
so it's really important to keep that in mind

00:37:07,780 --> 00:37:11,780
OK, so let's back-up and get back to what we were trying to do

00:37:11,780 --> 00:37:15,580
We're trying to understand something that should be simple

00:37:16,240 --> 00:37:20,000
We're trying to understand why this push_back function

00:37:20,160 --> 00:37:24,260
Is ten times faster when we reserve first

00:37:24,820 --> 00:37:28,100
and then push_back than it is when we just push_back

00:37:28,820 --> 00:37:29,320
Right?

00:37:29,320 --> 00:37:30,900
Everybody back on track?

00:37:30,900 --> 00:37:32,720
You know how to use a profiler?

00:37:33,040 --> 00:37:34,720
Well OK let's dig deep...

00:37:34,720 --> 00:37:36,240
So the way we dig deep in the perf tool is

00:37:36,720 --> 00:37:38,320
we hit the lovely "a" key

00:37:38,320 --> 00:37:40,680
and it's going to annotate this function

00:37:40,680 --> 00:37:42,960
I told you were were going to look at assembly

00:37:42,960 --> 00:37:44,020
so you've got to prepare yourself

00:39:21,120 --> 00:39:25,760
That's going to make the next few instructions look really suspcious

00:39:31,440 --> 00:39:32,320
We start here

00:39:34,120 --> 00:39:35,160
We execute down

00:39:35,920 --> 00:39:40,860
We do some computation which is basically just loading new data...

00:39:54,000 --> 00:39:56,160
That's why it's ten times faster

00:39:57,020 --> 00:39:58,540
So what happened here?

00:39:58,540 --> 00:40:00,520
So what happened here was the optimizer...

00:40:03,960 --> 00:40:08,040
Took one look at it and said, pff this code doesn't do anything

00:40:08,220 --> 00:40:09,660
I'm going to delete it

00:40:10,700 --> 00:40:12,140
That's its job right?

00:40:12,140 --> 00:40:12,960
It's doing its job

00:40:12,960 --> 00:40:17,840
Unfortunately it's making your job as a benchmarker or performance analyzer tremendously harder

00:40:41,240 --> 00:40:46,060
We've run into the first kind of real problem here and that's the optimisers

00:40:46,060 --> 00:40:48,080
How do we defeat an optimizer

00:40:48,080 --> 00:40:50,200
Now first off, we've got to pause for a minute

00:40:50,220 --> 00:40:53,660
I'm about to tell you all how to defeat the optimzier

00:40:53,660 --> 00:40:55,960
As a compiler and optimzer writer,

00:40:56,680 --> 00:40:59,000
please use these powers for good...

00:41:01,140 --> 00:41:05,700
Don't defeat the optimizer because you have undefined behaviour in your source code

00:41:05,700 --> 00:41:08,540
and you're like, "No, no, no optimizer. You're not going to miscompile MY code today!"

00:41:08,700 --> 00:41:11,260
Uh-uh, that's not actually going to work for you...

00:41:11,780 --> 00:41:12,980
it goes very badly

00:41:12,980 --> 00:41:15,940
OK but how do we defeat it when we need to for good reasons

00:41:15,940 --> 00:41:17,640
we need to for performance analysis reasons

00:41:17,640 --> 00:41:18,660
We need two functions

00:41:19,140 --> 00:41:20,780
These are kind of magical functions

00:41:20,780 --> 00:41:23,520
I would never expect anyone to know how to write these functions

00:41:24,040 --> 00:41:27,840
You might reasonably argue that I shouldn't be writing them

00:41:27,840 --> 00:41:28,920
they should be in my library

00:41:28,920 --> 00:41:31,060
but I want to show you guys how these things actually work

00:41:32,760 --> 00:41:34,060
OK we're already in trouble

00:41:34,060 --> 00:41:36,440
I'm using void *s - I know, I know...

00:41:36,440 --> 00:41:37,700
Type safety and all that...

00:41:37,980 --> 00:41:40,060
But there's actually a reason why I'm using void *s

00:41:40,060 --> 00:41:40,900
It's going to get a lot worse

00:41:41,300 --> 00:41:44,160
I'm going to next type the two scariest tokens

00:41:44,160 --> 00:41:47,680
from my compiler/implementor world.

00:41:47,680 --> 00:41:49,920
OK? Literally the two scariest tokens...

00:41:50,860 --> 00:41:51,940
(audience laughter)

00:41:52,000 --> 00:41:52,580
OK!

00:41:57,960 --> 00:41:59,180
And now I'm going to do something...

00:41:59,180 --> 00:42:00,840
this doesn't make any sense to anyone

00:42:03,220 --> 00:42:04,920
What on earth have I done?!

00:42:05,400 --> 00:42:06,760
OK so what is this?

00:42:07,200 --> 00:42:09,560
This is a magical escape function -

00:42:09,580 --> 00:42:10,960
it is truly magical!

00:42:11,480 --> 00:42:14,760
This has these unique and mysterious properties...

00:44:15,000 --> 00:44:16,160
It's going to assemble it

00:44:16,160 --> 00:44:17,640
It's going to build a representation for it

00:44:17,640 --> 00:44:19,820
and then it's going to notice it doesn't' DO anything...

00:44:20,140 --> 00:44:22,100
Or in my case that it's EMPTY.

00:44:22,560 --> 00:44:23,960
That doesn't do what I need.

00:44:23,960 --> 00:44:26,220
So I have to tell the compiler - "No, no, no,

00:44:26,220 --> 00:44:29,760
"this assembly has some crazy observable side effect."

00:44:30,020 --> 00:44:34,060
In my case the observable side effect is the benchmarking.

00:44:34,560 --> 00:44:35,840
The benchmark numbers.

00:44:35,840 --> 00:44:38,400
So it's actually doing what I want here

00:44:38,400 --> 00:44:43,520
This is a correct usage of volatile and in my opinion a correct usage of inline assembly.

00:44:44,360 --> 00:44:46,720
So the syntax for inline assembly,

00:44:46,800 --> 00:44:50,740
and GCC came up with the syntax (I think) and clang uses the same syntax,

00:44:51,120 --> 00:44:53,380
is kind of opaque - it's a little bit confusing.

00:44:53,380 --> 00:44:56,320
So you have a string literal that is your inline assembly

00:44:57,320 --> 00:44:58,320
You have a colon,

00:44:58,320 --> 00:45:00,640
you have a comma separated list of outputs,

00:45:00,640 --> 00:45:01,740
then you have a colon,

00:45:01,740 --> 00:45:03,420
you have a comma separated list of inputs,

00:45:03,440 --> 00:45:07,020
you have a colon and then a list of clobbers is what they're called,

00:45:07,020 --> 00:45:08,120
"Clobbers".

00:46:32,280 --> 00:46:34,440
STARTMISPLACEDSUBTITLES OK so the question is why

00:46:34,440 --> 00:46:35,640
when there is only one test

00:46:38,700 --> 00:46:42,540
did it take 20 nanoseconds

00:46:42,540 --> 00:46:44,280
and then iti got so much faster

00:46:45,420 --> 00:46:49,100
So...

00:47:42,740 --> 00:47:44,500
 

00:47:45,300 --> 00:47:46,980
and then I'm going to pick the modulus point at 32, 128, 224

00:47:47,320 --> 00:47:50,040
They'll be like 20%, 50% and 80% right?

00:47:50,040 --> 00:47:51,580
But all the numbers are the same.

00:47:51,580 --> 00:47:55,880
Anyone spot why?

00:47:55,880 --> 00:47:57,660
what I naively expect it to look like

00:47:57,960 --> 00:48:01,960
OK, and now when expand

00:48:08,500 --> 00:48:10,740
Are yes, because we have three functions.

00:48:10,740 --> 00:48:11,860
But,

00:50:27,760 --> 00:50:31,680
So the fact that create takes more time here is totally fine

00:50:32,800 --> 00:50:34,880
That's not what's interesting

00:50:35,600 --> 00:50:38,720
I want to actually look- what's reserve doing?

00:50:38,720 --> 00:50:41,280
This is actually pretty straightforward...

00:50:43,440 --> 00:50:45,760
Where's our loop? Here's our loop.

00:50:45,760 --> 00:50:47,040
We come in, we allocate some memory

00:50:48,320 --> 00:50:49,600
Not too surprising

00:50:49,600 --> 00:50:50,880
We delete some memory

00:50:51,680 --> 00:50:52,800
and then we loop.

00:50:52,800 --> 00:50:55,360
Now one thing that might frustrate you is

00:50:56,000 --> 00:50:57,920
(It very much frustrates me)

00:50:59,520 --> 00:51:02,400
Is why are these things still in my program?

00:51:05,040 --> 00:51:06,880
Ah! Because we MADE them be!

00:51:09,920 --> 00:51:11,360
Silently, inbetween

00:51:11,360 --> 00:51:12,640
in this empty space is our escape

00:51:13,420 --> 00:51:13,920
right?

00:51:13,920 --> 00:51:14,560
That's the cool thing about escape

00:51:14,560 --> 00:51:19,040
We can actually force something that the optimizer would love to just delete to stick around

00:51:19,040 --> 00:51:20,960
But we don't even have a single instruction in here

00:51:21,120 --> 00:51:25,440
So we actually get the lowest cost measurements we can - awesome!

00:53:18,780 --> 00:53:19,640
Sad story

00:53:19,680 --> 00:53:24,400
So when you explicitly write reserve and then you explicitly write pushback,

00:53:24,460 --> 00:53:28,720
the optimizer does something different than when you just write pushback

00:53:28,720 --> 00:53:32,580
even though pushback, behind a conditional, calls reserve

00:53:34,400 --> 00:53:35,900
The reason it does something different,

00:53:35,900 --> 00:53:38,300
is because it now inlines at different points

00:53:38,300 --> 00:53:41,880
Unfortunately, it's going to make a different inlining decision

00:53:42,100 --> 00:53:47,400
and this is just a horrible horrible deficiency of today's optimizers, right?

00:53:47,400 --> 00:53:51,020
This seemingly meaningless change causes such a dramatic performance swing

00:53:51,020 --> 00:53:52,900
It's not because it's faster or slower

00:53:52,900 --> 00:53:55,880
It's because in one case, the optimizer got very lucky

00:53:55,880 --> 00:53:58,980
and managed to inline all of these things and delete all the code

00:53:58,980 --> 00:54:02,380
In the other case the inliner didn't quite connect A-B-C-D

00:54:02,380 --> 00:54:04,680
even though theoretically it could

00:54:04,680 --> 00:54:08,000
and so it didn't delete everything and so we saw the real performance.

00:54:08,880 --> 00:54:09,380
Yes?

00:54:09,680 --> 00:54:15,900
< It that because of the the order that the compiler sees those functions?

00:54:15,900 --> 00:54:19,240
<I know (inaudible) from my own experience that I believe this is true

00:54:21,180 --> 00:54:24,300
< Visibility while it is running (inaudible) it always looks back it doesn't look forward (inaudible)

00:54:31,260 --> 00:54:33,460
No no no, nonono, no

00:54:33,460 --> 00:54:35,380
Not at all relevant, compiler doesn't care

00:54:47,780 --> 00:54:49,540
The allocation in reserve

00:55:15,360 --> 00:55:19,620
and I'm speaking as the author of it - I'm not trying to denigrate it -

00:55:19,620 --> 00:55:22,300
it's a very very fantastic thing but

00:55:22,580 --> 00:55:24,260
it gets tripped up easily

00:55:25,280 --> 00:55:26,400
This has been fun

00:55:27,000 --> 00:55:30,340
I want to briefly look at one other crazy example

00:55:37,500 --> 00:55:41,260
Because I thought, you know, I should have some fun stuff to look at

00:55:55,500 --> 00:55:58,580
but I'm going benchmark it first because I know I'm supposed to measure things

00:55:58,580 --> 00:56:01,060
So we have this generate_arg_pairs()

00:56:01,060 --> 00:56:02,420
This is going to generate our input space

00:56:02,800 --> 00:56:08,560
We're going to walk from 32 I think- no... 16, up to 1024

00:56:10,080 --> 00:56:14,340
then on the other dimension we're going to get the numbers 32, 128, 224

00:56:14,340 --> 00:56:18,160
These are kind of arbitrary - it's just a two dimensional space we're going to explore in our benchmark

00:56:18,160 --> 00:56:19,680
we'll come back to them more

00:56:27,040 --> 00:56:30,000
we pick a ceiling here which is 32, 128 and 224

00:56:30,540 --> 00:56:33,900
We create some vectors for our input and our output

00:56:37,440 --> 00:56:38,640
Mark this down, OK?

00:56:41,360 --> 00:56:44,960
We're going to take that input and if the input is greater than or equal to my ceiling,

00:56:44,960 --> 00:56:47,440
I'm going to do a modulo

00:56:47,440 --> 00:56:50,620
But if it's not I'm just going to use input directly

00:56:50,880 --> 00:56:51,840
See what I did?

00:56:51,840 --> 00:56:52,500
It's awesome!

00:56:52,500 --> 00:56:53,580
It's fantastic!

00:56:54,220 --> 00:56:56,160
I'm assuming all of these are positive numbers

00:56:56,160 --> 00:56:57,740
(that's not relevant to this example)

00:57:23,600 --> 00:57:26,320
Who thinks this is even going to compile?

00:57:27,480 --> 00:57:29,640
Hey look it compiles! It even runs, cool...

00:57:58,980 --> 00:58:01,540
What if 80% of the numbers are below it?

00:58:03,380 --> 00:58:06,340
I'm going to vary my numbers between 0 and 255

00:58:07,540 --> 00:58:12,300
and then I'm going to pick the modulus point at 32, 128, 224

00:58:12,660 --> 00:58:15,980
They'll be like 20%, 50% and 80%, right?

00:58:16,200 --> 00:58:18,860
So that should give me kind of a different- but all the numbers are the same...

00:58:19,660 --> 00:58:20,780
Anyone spot why?

01:01:31,600 --> 01:01:32,800
the actual sample

01:01:33,040 --> 01:01:34,720
for a very slow operation

01:01:34,720 --> 01:01:37,280
gets attributed after the operation completes

01:01:38,880 --> 01:01:41,120
So we're attributing all the cost

01:01:41,120 --> 01:01:43,040
of this actual modulo operation

01:01:44,160 --> 01:01:46,240
on to the store into memory, OK?

01:01:46,400 --> 01:01:48,800
So all of this cost - this whole thing

01:01:49,440 --> 01:01:50,720
that was super slow

01:01:50,720 --> 01:01:53,280
We're actually accounting for it here

01:01:56,320 --> 01:01:59,040
This can really throw you and confuse you

01:01:59,760 --> 01:02:03,520
So we are actually understanding how this profile works

01:02:04,720 --> 01:02:05,440
Cool thing

01:02:05,440 --> 01:02:06,720
Before we go though,

01:02:08,080 --> 01:02:10,560
The other thing that's a bit weird is:

01:02:10,560 --> 01:02:12,560
Why are there two copies here?

01:02:13,840 --> 01:02:16,960
Well the answer to that is of course very simple

01:02:16,960 --> 01:02:18,880
We have a nice optimizing compiler!

01:02:18,880 --> 01:02:20,800
It was like "you have a loop, it has like five instructions in it,

01:02:21,360 --> 01:02:27,840
"that's a waste of my time. I'm going to duplicate it so that we can actually do more things at a go."

01:02:28,720 --> 01:02:32,960
Well cool, I like it when my optimizing compiler does work for me

01:02:32,960 --> 01:02:34,880
But if two is good then I've got to figure that four is better right?

01:02:35,200 --> 01:02:37,440
Who's with me? C'mon, four better? Yeah!

01:02:37,520 --> 01:02:41,280
Thank you, thank you! There's one person still awake...

01:02:41,760 --> 01:02:43,200
(audience laughter)

01:02:43,440 --> 01:02:45,120
So four is clearly better

01:02:45,120 --> 01:02:47,040
Let's see what four looks like

01:02:49,040 --> 01:02:52,800
So unfortunately to get four (because I'm a lazy typist)

01:02:54,320 --> 01:02:56,640
I'm going to do something terrible

01:02:59,720 --> 01:03:02,620
Yeah I know - I'm sorry. I'm using a macro.

01:03:05,440 --> 01:03:07,340
How many folks here have used clang-format?

01:04:36,420 --> 01:04:37,220
Not really.

01:04:37,540 --> 01:04:40,100
So why is this not, like, crazy faster?

01:04:40,320 --> 01:04:42,160
get a look at what's going on

01:04:42,960 --> 01:04:46,700
For the record we have these numbers, they're good right?

01:04:46,700 --> 01:04:49,960
But I DOUBLED how many times we did it on each iteration

01:04:49,960 --> 01:04:51,840
so it should be twice as fast

01:04:51,880 --> 01:04:55,320
and it's a little faster but not a lot faster, right?

01:04:55,320 --> 01:04:57,600
What's going on here?

01:04:57,600 --> 01:05:00,160
This is, of course, where you break out your profiling tool

01:05:00,160 --> 01:05:02,720
So we want to profile this thing but when we do

01:05:02,880 --> 01:05:06,840
we actually want to look more specifically at one of these things

01:05:06,840 --> 01:05:09,120
so you can actually do this benchmark_filter thing

01:05:09,120 --> 01:05:11,680
(the benchmark library has nice filtering things)

01:05:13,360 --> 01:05:16,800
and we can do benchmark_fastmod- so it's just bench_fastmod...

01:05:18,220 --> 01:05:19,020
Alright?

01:05:22,200 --> 01:05:22,780
Cool!

01:05:23,200 --> 01:05:25,120
So it just ran one, this time it will be nice and consistent

01:05:25,580 --> 01:05:26,940
evenly distributed

01:05:27,520 --> 01:05:28,880
So we can look at this

01:05:29,600 --> 01:05:30,880
There's only one function

01:05:32,500 --> 01:05:33,140
Alright!

01:05:33,400 --> 01:05:34,740
So what's actually taking place here?

01:05:34,740 --> 01:05:37,240
Let's go up and get ourselves oriented

01:05:41,040 --> 01:05:42,940
you'll just have to trust me that I know this is actually the start of the loop

01:05:43,920 --> 01:05:46,240
We're going to load some input data

01:05:46,240 --> 01:05:47,520
we're going to compare it

01:05:47,520 --> 01:05:49,440
and if it's less we're going to jump over mod

01:05:49,440 --> 01:05:50,080
alright?

01:05:50,080 --> 01:05:53,920
We're going to load some data, compare it and jump over THAT mod...

01:05:54,020 --> 01:05:55,020
We just keep doing this

01:05:55,020 --> 01:05:55,660
Now why is this slow?

01:05:56,480 --> 01:06:02,240
The reason it's slow is because we've got these very small number of instructions and then we

01:06:02,240 --> 01:06:03,520
jump over even more instructions

01:06:03,520 --> 01:06:05,440
We're constantly jumping forward

01:06:06,480 --> 01:06:08,000
Now modern processors

01:06:08,000 --> 01:06:11,200
primary reason why they are slow is due to cache misses

01:06:11,200 --> 01:06:14,400
Like the processor is faster than any program you've ever run on it

01:06:14,400 --> 01:06:18,880
Unfortunately it's spending all its time waiting for your program to have data available for it

01:10:11,260 --> 01:10:14,100
And we're going to see a really large number here

01:10:14,100 --> 01:10:15,940
like fifteen here

01:10:16,280 --> 01:10:17,720
just jumping back up

01:10:18,100 --> 01:10:20,420
Now, this is not actually the jump back up.

01:10:20,460 --> 01:10:22,760
Again, right, our measurements are imprecise

01:10:22,760 --> 01:10:27,400
you'll note the utter lack of samples next to idiv over here, right?

01:10:27,400 --> 01:10:30,240
We're actually accounting for the cost of computing the modulo

01:10:30,260 --> 01:10:34,980
Even though we only spent 20% of our time computing modulos,

01:10:35,180 --> 01:10:38,780
that's hotter than the rest of our code...

01:10:39,260 --> 01:10:40,280
OK?

01:10:40,620 --> 01:10:44,300
Only 20% of our "numbers" needed the modulo computed

01:10:44,300 --> 01:10:48,280
but we actually spent most our time in those instructions

01:10:48,280 --> 01:10:49,880
which is really frustrating, right?

01:10:50,080 --> 01:10:54,560
But as we kind of have more and more dense kind of distributions

01:10:54,560 --> 01:10:58,040
towards under the ceiling, this is going to just get better and better

01:10:58,040 --> 01:11:00,140
because we're going to reach code less and less often.

01:11:00,280 --> 01:11:02,760
Make some sense? Fantastic!

01:11:02,820 --> 01:11:05,480
Now, before I stop and take some questions

01:11:05,580 --> 01:11:09,060
I bet all you guys have some burning- one burning question

01:11:09,060 --> 01:11:13,080
I spent a bunch of time talking about modulo and all this other stuff

01:11:13,860 --> 01:11:16,540
Any questions? What's the most important question right now?

01:11:16,540 --> 01:11:18,580
What have I failed utterly to do?

01:11:19,200 --> 01:11:20,140
(audience member says "baseline this")

01:11:20,140 --> 01:11:21,180
🎵Baseline!

01:11:21,500 --> 01:11:25,280
What does it look like if I just do this?

01:11:25,600 --> 01:11:27,760
Why do need all this complexity?

01:11:27,760 --> 01:11:29,740
OK, so we've got to actually do our homework here

01:11:30,960 --> 01:11:33,000
We've kind of being playing fast and loose...

01:11:34,080 --> 01:11:35,600
Because I was excited -

01:11:35,600 --> 01:11:37,980
I invented something folks, it was awesome!

01:11:38,460 --> 01:11:39,780
But now you're going to make me be honest

01:11:40,580 --> 01:11:43,660
OK, so we- I have- kicking around here...

01:11:43,660 --> 01:11:45,680
a benchmark that includes both

01:11:45,680 --> 01:11:50,760
(rapid typing)

01:15:49,680 --> 01:15:53,600
In that sense we're also getting different samples as well

01:15:53,600 --> 01:15:57,440
But you can fix the seed if you want - there are lots of techniques to pin this down harder

01:15:58,860 --> 01:15:59,360
Yes?

01:15:59,440 --> 01:16:00,760
AUDIENCE MEMBER: Ah yes, I was wondering,

01:16:00,760 --> 01:16:04,800
can you use instead just using the, uh, boost-

01:16:04,800 --> 01:16:07,340
the, uh, Google benchmark test

01:16:07,380 --> 01:16:12,760
could you kind of use, er, std::chronos to kind of get exactly

01:16:12,760 --> 01:16:20,020
what you really want to measure in a, er, fixed distribution?

01:16:20,020 --> 01:16:21,320
So if you're doing something,

01:16:21,320 --> 01:16:25,600
you can actually test how much time is actually running...

01:16:50,420 --> 01:16:51,440
AUDIENCE MEMBER: Two questions:

01:16:51,440 --> 01:16:57,960
First, why omit the frame pointer and not use the LBR unwind on Intel or DWARF unwinding?

01:16:58,380 --> 01:17:01,580
CHANDLER: So, why am I using -fno-omit-frame-pointer?

01:17:02,160 --> 01:17:07,680
The first option to- sorry. The first alternative to omitting the frame pointer is actually debug information

01:17:07,940 --> 01:17:09,380
like DWARF unwinding

01:17:09,380 --> 01:17:12,860
Unfortunately I have had terrible success with DWARF unwinding -

01:17:12,860 --> 01:17:15,840
it tends to miss frames and misattribute frames really really often,

01:17:15,840 --> 01:17:17,100
so I don't trust it.

01:17:17,180 --> 01:17:18,540
I just don't trust it.

01:17:18,540 --> 01:17:21,320
The frame pointer always gives me consistent results,

01:17:21,320 --> 01:17:23,260
even when it's a little bit hard to stare at the profile -

01:17:23,260 --> 01:17:25,760
it's much much more consistent than DWARF.

01:17:26,420 --> 01:17:28,020
LBR is a fantastic tool,

01:17:28,020 --> 01:17:37,360
But LBR magnifies the statistical... random errors that you see with the frame unwinding...

01:17:37,360 --> 01:17:42,060
LBR is a fantastic way to do statistical profiling, it's somewhat less useful for humans.

01:17:42,060 --> 01:17:45,060
Because it's even more imprecision in your measurements.

01:17:45,940 --> 01:17:47,820
That tends to be my preference,

01:17:47,820 --> 01:17:50,680
and I don't think you can use LBR with frame pointers...

01:17:51,200 --> 01:17:53,600
LBR has to be the unwind information

01:17:53,600 --> 01:17:55,120
and it has too many error cases

01:17:55,120 --> 01:17:57,760
AUDIENCE MEMBER: OK thanks. And speaking of precision,

01:17:57,760 --> 01:18:00,580
why not use precise counters which perf supports,

01:18:00,580 --> 01:18:06,300
or even better correlating more(?) counters like cycles with instruction counts and cache misses...

01:19:44,720 --> 01:19:48,240
So you had an example of where because of the sampling rate

01:19:48,240 --> 01:19:51,680
probably it was missing the div and making the jump look expensive

01:19:51,680 --> 01:19:57,120
So increasing the sampling rate helps and I personally don't think it would affect the benchmark

01:19:57,120 --> 01:19:58,400
because you run it enough etc etc.

01:19:58,400 --> 01:20:01,920
The sample rate won't help here, the reason it's missing div isn't the rate of sampling,

01:20:01,920 --> 01:20:05,440
it's the imprecision of actually collecting the sample data

01:20:06,640 --> 01:20:07,680
on the other (?)

01:20:08,340 --> 01:20:10,500
That to me is a solved problem but

01:20:11,040 --> 01:20:14,080
One of the more bothersome problems I've seen

01:20:14,080 --> 01:20:16,640
Especially with vectorization instructions is

01:20:16,640 --> 01:20:22,720
skid where basically the CPU counter's a little bit off and you can't quite trust them so you

01:20:22,720 --> 01:20:25,280
kinda of have to do low-level microcodey type things

01:20:26,640 --> 01:20:28,480
to the CPU to eliminate skid

01:20:28,800 --> 01:20:33,600
What are the strategies or tactics that you have for doing in perf versus

01:20:35,200 --> 01:20:36,800
a more complicated tool

01:20:36,960 --> 01:20:39,360
Or is perf not capable of doing that?

01:20:39,360 --> 01:20:40,640
Can you just talk about that a little bit?

01:20:40,640 --> 01:20:42,880
I have no idea if perf is capable of doing that

01:20:43,840 --> 01:20:48,640
The way I get around this is I read perf with a very very large grain of salt

01:20:48,880 --> 01:20:51,840
Because it's essentially giving me raw data

01:20:51,840 --> 01:20:52,800
and it's not correcting for it

01:20:53,040 --> 01:20:55,360
and I try to mentally correct for it

01:20:55,360 --> 01:20:58,240
Looking at the instructions, reasoning about what their latency should be

01:20:58,240 --> 01:21:01,440
and then how these samples actually match up with that

01:21:02,000 --> 01:21:04,320
The other tool that I'm a huge fan of

01:21:04,320 --> 01:21:09,120
because it's very easy to use and gives you a lot of this data is the Intel Architecture Code Analyser

01:21:09,920 --> 01:21:12,000
And you can essentially give it

01:21:12,000 --> 01:21:18,720
a snippet of instructions and it'll explain the Intel accurate model for how those instructions should execute

01:21:18,720 --> 01:21:20,000
and if you look at that

01:21:20,000 --> 01:21:22,240
and you look at your profile and the samples you're seeing

01:21:22,480 --> 01:21:23,200
in the wild

01:21:23,360 --> 01:21:27,360
you can get a pretty clear idea of where the samples came from

01:21:27,920 --> 01:21:29,600
even when they're skewed

01:21:30,240 --> 01:21:32,480
and so I do more of a mental mapping

01:21:32,480 --> 01:21:38,880
because I don't really trust any of the tools to undo any of the fundamental skew that's inherent in the counters

01:23:17,840 --> 01:23:19,680
They are very very general,

01:23:19,680 --> 01:23:21,280
you can almost always get them

01:23:21,280 --> 01:23:23,520
to preserve the piece of code you want...

01:23:23,520 --> 01:23:27,680
and they are unfortunately just a bit more powerful than a volatile varirable

01:23:27,680 --> 01:23:29,600
they're also a little bit easier to explain in my opinion

01:23:29,600 --> 01:23:32,160
I actually think that reasoning about things like escape

01:23:32,160 --> 01:23:34,400
and clobbering memory is fairly reasonable

01:23:34,640 --> 01:23:37,280
And pretty easy for folks to think about

01:23:37,280 --> 01:23:40,480
One of the other nice things about having clobber at the end of a loop iteration

01:23:40,480 --> 01:23:41,760
is that it prevents the compiler from doing

01:23:42,160 --> 01:23:44,000
loop computation analysis

01:23:44,000 --> 01:23:48,160
To just figure out the sequence of things you're going to store into this volatile memory

01:23:48,320 --> 01:23:52,000
and ahead of time prepare them and then just store them.

01:23:52,000 --> 01:23:53,600
And there are optimizers that will do this

01:23:53,840 --> 01:23:57,120
So the clobber isn't just helping with the escape

01:23:57,120 --> 01:23:58,720
it also tends to help with the inputs

01:23:58,720 --> 01:24:02,240
actually being new each time around the loop iteration

01:24:02,240 --> 01:24:03,520

YouTube URL: https://www.youtube.com/watch?v=nXaxk27zwlk


