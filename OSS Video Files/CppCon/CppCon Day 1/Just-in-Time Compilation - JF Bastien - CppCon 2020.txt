Title: Just-in-Time Compilation - JF Bastien - CppCon 2020
Publication date: 2020-09-25
Playlist: CppCon Day 1
Description: 
	https://cppcon.org/
https://github.com/CppCon/CppCon2020/blob/main/Presentations/justintime_compilation/justintime_compilation__jf_bastien__cppcon_2020.pdf
---

Just-in-Time compilers... we've all heard of them! What are they really? Why would anyone want them, are they actually a good idea, and how do they fit in with C++ since we all use Ahead-of-Time compilers?

In this talk I'll tell you about C++ AOT compiler, JITs for dynamic language, JITs for binary translation, and dive back 20, 30, 40, 50, 60 years, way back into compiler history and read wonderful academic papers about compilers. I'll illustrate how our view of compilers is really monolithic, and how compilers through time, and still today, are actually a continuum.

---
JF Bastien
Software Architect, Toyota Research Institute-Advanced Development
Compiler engineer, chair of the C++ committee's language evolution working group.

---
Streamed & Edited by Digital Medium Ltd - events.digital-medium.co.uk
events@digital-medium.co.uk
Captions: 
	00:00:08,960 --> 00:00:11,360
hey folks

00:00:09,679 --> 00:00:13,200
my name is jf today i'm going to talk to

00:00:11,360 --> 00:00:17,039
you about just-in-time compilation

00:00:13,200 --> 00:00:18,960
a lecture on the last 60 years

00:00:17,039 --> 00:00:21,039
so in this talk i'll go through a lot of

00:00:18,960 --> 00:00:23,119
papers you can look at them on github

00:00:21,039 --> 00:00:25,119
i posted all of them there and we also

00:00:23,119 --> 00:00:27,599
have a speedy con slack channel

00:00:25,119 --> 00:00:28,400
sig underscore jit cigars for special

00:00:27,599 --> 00:00:31,199
interest group

00:00:28,400 --> 00:00:31,840
so i'll cover about 20 some papers in

00:00:31,199 --> 00:00:33,600
the top

00:00:31,840 --> 00:00:35,040
uh and so that that's quite a bit now

00:00:33,600 --> 00:00:37,040
going through them mostly in

00:00:35,040 --> 00:00:38,800
chronological order covering the last 60

00:00:37,040 --> 00:00:40,960
years of research on jets

00:00:38,800 --> 00:00:42,559
uh in a way the stock isn't my usual

00:00:40,960 --> 00:00:44,000
talk because it's more of electronic

00:00:42,559 --> 00:00:46,320
compilers while outline

00:00:44,000 --> 00:00:47,440
papers that speak the most to me and i

00:00:46,320 --> 00:00:48,879
want to share with you

00:00:47,440 --> 00:00:51,360
so i have more text on the screen than

00:00:48,879 --> 00:00:52,800
usual from the papers and so i'll read

00:00:51,360 --> 00:00:56,800
through that with you

00:00:52,800 --> 00:00:59,120
now first some definitions first jit

00:00:56,800 --> 00:01:00,559
jit just in time compilation and with

00:00:59,120 --> 00:01:03,359
some artistic liberty

00:01:00,559 --> 00:01:04,000
uh folks nowadays usually think of jit

00:01:03,359 --> 00:01:06,720
as

00:01:04,000 --> 00:01:08,320
the executable code changes after the

00:01:06,720 --> 00:01:09,760
program is loaded into memory

00:01:08,320 --> 00:01:11,760
and the linkers and loader are done

00:01:09,760 --> 00:01:14,080
doing their thing right

00:01:11,760 --> 00:01:15,280
on modern systems so last 20 years or so

00:01:14,080 --> 00:01:18,400
what that means is you have

00:01:15,280 --> 00:01:19,439
pages that are mapped as executable at

00:01:18,400 --> 00:01:21,680
some point in time

00:01:19,439 --> 00:01:22,560
and they're modified while things

00:01:21,680 --> 00:01:24,640
execute

00:01:22,560 --> 00:01:26,640
right so that's roughly what people

00:01:24,640 --> 00:01:29,200
think of as gypsies and i'll stick to

00:01:26,640 --> 00:01:33,119
that definition

00:01:29,200 --> 00:01:36,159
in contrast ahead of time or aot uh

00:01:33,119 --> 00:01:36,799
is what people usually think of as the

00:01:36,159 --> 00:01:39,439
opposite

00:01:36,799 --> 00:01:41,520
of gypd right and roughly nowadays

00:01:39,439 --> 00:01:42,240
people think of cincy plus plus as an

00:01:41,520 --> 00:01:44,799
aot

00:01:42,240 --> 00:01:45,680
model right you compile code to a target

00:01:44,799 --> 00:01:47,920
machine

00:01:45,680 --> 00:01:49,680
then you run it right the the things

00:01:47,920 --> 00:01:51,280
you've mapped as executable

00:01:49,680 --> 00:01:52,799
don't change while you execute the

00:01:51,280 --> 00:01:55,439
program the link and loader might do

00:01:52,799 --> 00:01:58,880
stuff but afterwards it doesn't change

00:01:55,439 --> 00:02:00,240
and finally interpreters so

00:01:58,880 --> 00:02:01,600
you know you've all used interpreted

00:02:00,240 --> 00:02:02,560
languages like you know python or

00:02:01,600 --> 00:02:04,640
something like that and

00:02:02,560 --> 00:02:05,680
the question i want to get to is is an

00:02:04,640 --> 00:02:08,879
interpreter

00:02:05,680 --> 00:02:10,080
legit or is it ahead of time and what

00:02:08,879 --> 00:02:12,879
happens if the interpreter

00:02:10,080 --> 00:02:13,280
modifies its byte code as it executes

00:02:12,879 --> 00:02:15,680
right

00:02:13,280 --> 00:02:17,040
and i want to dig a bit into that so if

00:02:15,680 --> 00:02:19,200
you really think hard about it

00:02:17,040 --> 00:02:20,319
the cpu right so the machine you're

00:02:19,200 --> 00:02:22,879
running stuff on

00:02:20,319 --> 00:02:24,319
it executes machine code and really a

00:02:22,879 --> 00:02:26,720
cpu is an interpreter

00:02:24,319 --> 00:02:27,599
right it executes some form of

00:02:26,720 --> 00:02:29,680
instructions

00:02:27,599 --> 00:02:31,760
an interpreter itself executes byte

00:02:29,680 --> 00:02:32,879
codes right so i'd argue the cpu itself

00:02:31,760 --> 00:02:34,640
is an interpreter

00:02:32,879 --> 00:02:36,319
it's really the same thing right it's an

00:02:34,640 --> 00:02:38,720
interpreter for machine code

00:02:36,319 --> 00:02:39,360
now a compiler if you look at your

00:02:38,720 --> 00:02:41,440
compiler

00:02:39,360 --> 00:02:43,360
it can perform partial evaluation of the

00:02:41,440 --> 00:02:44,640
program your program the compiler can go

00:02:43,360 --> 00:02:45,760
in and look at stuff and partially

00:02:44,640 --> 00:02:47,200
evaluate it

00:02:45,760 --> 00:02:49,200
therefore the compiler itself can

00:02:47,200 --> 00:02:51,120
interpret things right the compiler

00:02:49,200 --> 00:02:52,959
itself you'll remember is a program so

00:02:51,120 --> 00:02:54,480
the compiler itself can be compiled

00:02:52,959 --> 00:02:56,000
which means the compiler compiler is the

00:02:54,480 --> 00:02:57,519
compiler interpreter

00:02:56,000 --> 00:02:59,360
right but with that i'm getting a bit

00:02:57,519 --> 00:03:01,360
ahead of myself with the future

00:02:59,360 --> 00:03:02,480
projections so we'll step back a bit

00:03:01,360 --> 00:03:03,200
we'll talk about things like that a bit

00:03:02,480 --> 00:03:06,000
later

00:03:03,200 --> 00:03:06,800
but i'll try to outline the goals in my

00:03:06,000 --> 00:03:09,280
talk for today

00:03:06,800 --> 00:03:10,159
right now i really have three goals goal

00:03:09,280 --> 00:03:12,000
number one

00:03:10,159 --> 00:03:14,879
is i want to convince you that jit and

00:03:12,000 --> 00:03:17,280
aot are really kind of a continuum

00:03:14,879 --> 00:03:18,319
right like i'm pretty sure i confuse you

00:03:17,280 --> 00:03:19,440
a bit with my explanation of

00:03:18,319 --> 00:03:21,040
interpreters and that was on

00:03:19,440 --> 00:03:23,519
on purpose i want to kind of wet your

00:03:21,040 --> 00:03:24,799
appetite to change the way we see things

00:03:23,519 --> 00:03:27,120
change the way we think about them

00:03:24,799 --> 00:03:29,599
because computation and compilation

00:03:27,120 --> 00:03:31,120
can occur in a bunch of places right and

00:03:29,599 --> 00:03:33,200
at different points in time

00:03:31,120 --> 00:03:35,200
and that's important and i'll have a

00:03:33,200 --> 00:03:37,120
little quote for you

00:03:35,200 --> 00:03:38,799
those who cannot remember the past are

00:03:37,120 --> 00:03:40,480
condemned to repeat it and that quote

00:03:38,799 --> 00:03:41,920
has been repeated quite a bit that's a

00:03:40,480 --> 00:03:42,480
great aphorism that explains a lot of

00:03:41,920 --> 00:03:45,360
stuff

00:03:42,480 --> 00:03:46,720
go to tony's talk next year at cppcon to

00:03:45,360 --> 00:03:48,319
learn more about them

00:03:46,720 --> 00:03:50,080
right but you've all heard this quote

00:03:48,319 --> 00:03:51,920
before and and

00:03:50,080 --> 00:03:53,280
the reason i have this quote is because

00:03:51,920 --> 00:03:54,480
it comes from a paper i'll talk to you

00:03:53,280 --> 00:03:56,640
about in a bit

00:03:54,480 --> 00:03:58,400
but it really outlines well my second

00:03:56,640 --> 00:04:00,959
goal of this talk

00:03:58,400 --> 00:04:01,439
i want to look at what have just done

00:04:00,959 --> 00:04:03,840
right

00:04:01,439 --> 00:04:05,360
so computation and compilation can occur

00:04:03,840 --> 00:04:07,439
at different places right

00:04:05,360 --> 00:04:09,519
in places and times they differ so we

00:04:07,439 --> 00:04:10,000
ought to look at where those places and

00:04:09,519 --> 00:04:12,319
times

00:04:10,000 --> 00:04:13,200
have historically been and why they've

00:04:12,319 --> 00:04:15,439
been there

00:04:13,200 --> 00:04:16,880
right so this is why we'll look at a

00:04:15,439 --> 00:04:17,840
bunch of published papers in the field

00:04:16,880 --> 00:04:19,519
of compilers

00:04:17,840 --> 00:04:21,280
not all of those papers are directly

00:04:19,519 --> 00:04:23,360
applicable to c plus plus but a lot of

00:04:21,280 --> 00:04:25,040
the ideas are relevant

00:04:23,360 --> 00:04:26,720
and that takes us to our third and my

00:04:25,040 --> 00:04:28,639
last goal today what

00:04:26,720 --> 00:04:30,160
could just do right so instead of just

00:04:28,639 --> 00:04:30,720
looking at like what have they done and

00:04:30,160 --> 00:04:32,800
why

00:04:30,720 --> 00:04:34,000
i also want to look at what could you do

00:04:32,800 --> 00:04:36,000
with them right

00:04:34,000 --> 00:04:37,280
and the reason i want to do that is when

00:04:36,000 --> 00:04:39,360
i talk to folks in the people's

00:04:37,280 --> 00:04:41,199
community uh we have the following

00:04:39,360 --> 00:04:42,000
perspective right it's it's mostly well

00:04:41,199 --> 00:04:45,040
i understand c

00:04:42,000 --> 00:04:46,479
plus plus and i kind of get assembly or

00:04:45,040 --> 00:04:48,400
i understand simply a lot

00:04:46,479 --> 00:04:50,479
because of tools like compiler explorer

00:04:48,400 --> 00:04:52,160
right i go on goggle.org and i look at

00:04:50,479 --> 00:04:53,600
stuff and i kind of understand it simply

00:04:52,160 --> 00:04:54,880
through that right like that's how i

00:04:53,600 --> 00:04:56,160
learned assembly i started debugging

00:04:54,880 --> 00:04:58,800
stuff and going to the assembly

00:04:56,160 --> 00:05:00,800
and looking at things right um and so

00:04:58,800 --> 00:05:03,520
our typical model of aot

00:05:00,800 --> 00:05:05,120
is what can what cmpc plus plus can do

00:05:03,520 --> 00:05:07,039
that's how we usually think of it

00:05:05,120 --> 00:05:09,680
and i want to expand the understanding

00:05:07,039 --> 00:05:11,840
for what other computation models exist

00:05:09,680 --> 00:05:13,520
right now i'll give you a warning i'll

00:05:11,840 --> 00:05:15,199
mostly avoid diving into the

00:05:13,520 --> 00:05:17,199
shortcomings and pitfalls of

00:05:15,199 --> 00:05:18,560
jit compilers in this talk but keep in

00:05:17,199 --> 00:05:20,479
mind that there's many of them

00:05:18,560 --> 00:05:22,320
right it would take an entire second top

00:05:20,479 --> 00:05:22,720
or third or fourth talk to go into those

00:05:22,320 --> 00:05:24,240
right

00:05:22,720 --> 00:05:26,160
so i'm not trying to get everyone to jit

00:05:24,240 --> 00:05:28,160
everything i just want to like you know

00:05:26,160 --> 00:05:29,280
excite you about jits and show you a few

00:05:28,160 --> 00:05:31,919
exciting things

00:05:29,280 --> 00:05:32,720
and in a way my talk could really have a

00:05:31,919 --> 00:05:34,880
different title

00:05:32,720 --> 00:05:35,840
uh i i call it alice's adventures in jet

00:05:34,880 --> 00:05:38,400
one right

00:05:35,840 --> 00:05:39,680
so so i really want to look at what can

00:05:38,400 --> 00:05:41,680
jits do

00:05:39,680 --> 00:05:43,680
right and alice in wonderland it

00:05:41,680 --> 00:05:46,080
challenges the reader's view i just read

00:05:43,680 --> 00:05:47,360
to my three-year-old kid and he was yeah

00:05:46,080 --> 00:05:48,960
didn't get all of it but

00:05:47,360 --> 00:05:51,360
it challenged a lot of stuff for him

00:05:48,960 --> 00:05:53,840
right and my goal is to expand our minds

00:05:51,360 --> 00:05:56,080
regarding what's possible with compilers

00:05:53,840 --> 00:05:57,600
all right so with that let's look at our

00:05:56,080 --> 00:05:58,639
first paper that i want to talk about

00:05:57,600 --> 00:06:01,039
today

00:05:58,639 --> 00:06:02,000
paper title is a brief history of just

00:06:01,039 --> 00:06:05,360
in time

00:06:02,000 --> 00:06:07,680
all right so this is a paper from 2003

00:06:05,360 --> 00:06:09,039
right so that's important so let's look

00:06:07,680 --> 00:06:11,039
at what it says

00:06:09,039 --> 00:06:12,639
it starts off with software systems have

00:06:11,039 --> 00:06:15,520
been using just-in-time compilation

00:06:12,639 --> 00:06:17,199
techniques since the 60s

00:06:15,520 --> 00:06:18,880
so broadly speaking jit compilation

00:06:17,199 --> 00:06:20,960
includes any translation performed

00:06:18,880 --> 00:06:22,639
dynamically after a program started

00:06:20,960 --> 00:06:24,479
execution

00:06:22,639 --> 00:06:25,759
and the paper examines the motivation

00:06:24,479 --> 00:06:27,680
behind the compilation

00:06:25,759 --> 00:06:29,280
and the constraints imposed on just

00:06:27,680 --> 00:06:31,039
compilation systems and presents a

00:06:29,280 --> 00:06:32,960
classification scheme for such

00:06:31,039 --> 00:06:35,280
systems this is a great paper if you

00:06:32,960 --> 00:06:36,639
read one of the papers i put on github

00:06:35,280 --> 00:06:38,160
that's the one you should read it has a

00:06:36,639 --> 00:06:39,120
lot of outlines it goes through a lot of

00:06:38,160 --> 00:06:41,360
interesting papers

00:06:39,120 --> 00:06:43,280
has really good references and a big

00:06:41,360 --> 00:06:44,000
chunk of this presentation borrows from

00:06:43,280 --> 00:06:47,199
the paper

00:06:44,000 --> 00:06:48,240
right now notice that it's almost 20

00:06:47,199 --> 00:06:50,800
years old

00:06:48,240 --> 00:06:52,720
okay so it covers roughly 40 years of

00:06:50,800 --> 00:06:54,400
research but there's another 20 that

00:06:52,720 --> 00:06:56,800
it's missing

00:06:54,400 --> 00:06:57,759
so so and you know looking back it's

00:06:56,800 --> 00:06:59,680
easier to look back

00:06:57,759 --> 00:07:01,440
you know 10 20 years in the past right

00:06:59,680 --> 00:07:02,000
so the last 20 years is a bit more muddy

00:07:01,440 --> 00:07:04,319
than the

00:07:02,000 --> 00:07:05,759
previous 40 bids right so i that's

00:07:04,319 --> 00:07:09,039
something to keep in mind here

00:07:05,759 --> 00:07:10,800
this is changing field all right now

00:07:09,039 --> 00:07:12,240
i mentioned alice in wonderland and

00:07:10,800 --> 00:07:14,240
there's really good insights in there

00:07:12,240 --> 00:07:18,080
for example

00:07:14,240 --> 00:07:20,720
this quote here how long is forever

00:07:18,080 --> 00:07:22,560
sometimes just one second right and this

00:07:20,720 --> 00:07:24,560
is a really good take on jit compilers

00:07:22,560 --> 00:07:25,759
now imagine if my jit compilation is in

00:07:24,560 --> 00:07:28,319
a foreground thread

00:07:25,759 --> 00:07:28,880
the locking interaction for an entire

00:07:28,319 --> 00:07:30,479
second

00:07:28,880 --> 00:07:32,240
right that animation that i just had on

00:07:30,479 --> 00:07:33,360
screen was kind of slow it was just one

00:07:32,240 --> 00:07:34,800
second right and you're like come on

00:07:33,360 --> 00:07:38,000
give me the words give me the words

00:07:34,800 --> 00:07:39,440
so contrast this with your usual c plus

00:07:38,000 --> 00:07:42,080
plus project build time

00:07:39,440 --> 00:07:43,759
one second sounds amazing right if you

00:07:42,080 --> 00:07:44,319
see plus plus project built in a second

00:07:43,759 --> 00:07:47,360
that would be

00:07:44,319 --> 00:07:48,960
so cool so clearly jit authors wouldn't

00:07:47,360 --> 00:07:50,479
want to block your interaction this way

00:07:48,960 --> 00:07:51,840
for like a long time because

00:07:50,479 --> 00:07:53,520
you're waiting for stuff to happen if

00:07:51,840 --> 00:07:55,360
it's in the foreground

00:07:53,520 --> 00:07:56,720
and that means the design space is

00:07:55,360 --> 00:07:58,319
really worth considering

00:07:56,720 --> 00:07:59,840
right it's not the same as just like

00:07:58,319 --> 00:08:00,879
well your compiler can take forever i

00:07:59,840 --> 00:08:02,720
can't

00:08:00,879 --> 00:08:04,000
right like it has a lot of stuff to do

00:08:02,720 --> 00:08:05,199
and it's waiting

00:08:04,000 --> 00:08:07,199
the user's waiting for something to

00:08:05,199 --> 00:08:09,039
happen all right so let's look at

00:08:07,199 --> 00:08:11,039
another quote

00:08:09,039 --> 00:08:14,319
strictly speaking jet compilation

00:08:11,039 --> 00:08:16,879
systems are completely unnecessary

00:08:14,319 --> 00:08:17,520
they're only a means to improve the time

00:08:16,879 --> 00:08:20,720
and space

00:08:17,520 --> 00:08:22,639
efficiency of the program after all

00:08:20,720 --> 00:08:23,919
the central problem jit systems address

00:08:22,639 --> 00:08:26,160
is a solid one

00:08:23,919 --> 00:08:27,680
translating programming languages into a

00:08:26,160 --> 00:08:29,039
form that is executable in a target

00:08:27,680 --> 00:08:32,000
platform that's all they do

00:08:29,039 --> 00:08:33,200
right alice really knows about that

00:08:32,000 --> 00:08:35,200
right

00:08:33,200 --> 00:08:37,760
so she knows how to trade off space and

00:08:35,200 --> 00:08:39,839
time right she drinks bottles gets big

00:08:37,760 --> 00:08:41,519
small follows a rabbit around she's out

00:08:39,839 --> 00:08:43,519
of time things like that

00:08:41,519 --> 00:08:45,200
and to root this into c plus plus

00:08:43,519 --> 00:08:47,519
imagine that you're templating

00:08:45,200 --> 00:08:49,120
all versions of function based on all

00:08:47,519 --> 00:08:50,560
valid integer parameters which could

00:08:49,120 --> 00:08:52,320
which it could receive

00:08:50,560 --> 00:08:53,920
that's a whole bunch of space and a

00:08:52,320 --> 00:08:55,360
whole bunch of time to compile it right

00:08:53,920 --> 00:08:56,000
your program's gonna be giant dormancy

00:08:55,360 --> 00:08:57,600
if you do that

00:08:56,000 --> 00:08:59,440
template explosion is the thing you got

00:08:57,600 --> 00:09:01,120
to be careful with it so clearly we want

00:08:59,440 --> 00:09:04,880
to do some tradeoffs here

00:09:01,120 --> 00:09:07,920
and and so to explain that i'll go into

00:09:04,880 --> 00:09:09,760
the four key benefits that jet systems

00:09:07,920 --> 00:09:10,640
have that they gain from aot and

00:09:09,760 --> 00:09:12,000
interpreters

00:09:10,640 --> 00:09:13,839
and there's really four of them and i'll

00:09:12,000 --> 00:09:17,040
dive into those so

00:09:13,839 --> 00:09:19,200
benefit number one compile programs run

00:09:17,040 --> 00:09:21,200
faster especially if they're compiled

00:09:19,200 --> 00:09:22,000
into a form that's directly executable

00:09:21,200 --> 00:09:25,200
on the underlying

00:09:22,000 --> 00:09:27,519
hardware makes sense static compilation

00:09:25,200 --> 00:09:28,880
can also devote an arbitrary amount of

00:09:27,519 --> 00:09:29,839
time to program analysis and

00:09:28,880 --> 00:09:33,519
optimizations

00:09:29,839 --> 00:09:35,519
by now invariably so this brings us to

00:09:33,519 --> 00:09:38,959
the primary constrained jit systems

00:09:35,519 --> 00:09:41,440
speed so jet system must not

00:09:38,959 --> 00:09:42,480
cause untoward pauses in normal program

00:09:41,440 --> 00:09:45,360
execution

00:09:42,480 --> 00:09:46,399
as a result of its operation now this is

00:09:45,360 --> 00:09:48,720
really broad

00:09:46,399 --> 00:09:50,080
what constitutes a nun toward pause

00:09:48,720 --> 00:09:51,360
really depends on the system really

00:09:50,080 --> 00:09:52,880
depends what the jit's doing

00:09:51,360 --> 00:09:54,640
whether it has background compilation or

00:09:52,880 --> 00:09:59,760
not a bunch of things

00:09:54,640 --> 00:10:01,839
right now advantage number two

00:09:59,760 --> 00:10:02,800
interpreted programs are typically

00:10:01,839 --> 00:10:04,320
smaller

00:10:02,800 --> 00:10:06,079
if only because the representation

00:10:04,320 --> 00:10:07,040
chosen is a higher level than machine

00:10:06,079 --> 00:10:08,720
code

00:10:07,040 --> 00:10:10,399
and it can carry much more semantic

00:10:08,720 --> 00:10:12,399
information implicitly

00:10:10,399 --> 00:10:14,240
now here think of examples where a

00:10:12,399 --> 00:10:16,640
single byte code instruction might do

00:10:14,240 --> 00:10:18,079
say a full matrix multiplication

00:10:16,640 --> 00:10:20,079
or it might change the prototype of a

00:10:18,079 --> 00:10:20,880
class there are much higher level

00:10:20,079 --> 00:10:23,760
primitives

00:10:20,880 --> 00:10:24,320
than even what a cis processor can do

00:10:23,760 --> 00:10:27,360
right

00:10:24,320 --> 00:10:28,160
now jits can benefit from this size

00:10:27,360 --> 00:10:30,240
savings by

00:10:28,160 --> 00:10:32,480
only compiling the code that matters and

00:10:30,240 --> 00:10:34,320
leaving the rest in a compressed form

00:10:32,480 --> 00:10:36,800
right so interpreters are really good at

00:10:34,320 --> 00:10:38,160
size a jit will compile stuff and expand

00:10:36,800 --> 00:10:38,959
it to the target architecture but the

00:10:38,160 --> 00:10:40,399
jit can say

00:10:38,959 --> 00:10:42,000
this is an important i'm just going to

00:10:40,399 --> 00:10:42,959
keep it for an interpreter and the rest

00:10:42,000 --> 00:10:45,519
i'm going to do

00:10:42,959 --> 00:10:48,399
so that's one trade off that just means

00:10:45,519 --> 00:10:50,399
now advantage number three

00:10:48,399 --> 00:10:51,440
interpreted programs tend to be more

00:10:50,399 --> 00:10:52,800
portable

00:10:51,440 --> 00:10:54,640
they work on different machines

00:10:52,800 --> 00:10:57,120
different architectures different uh

00:10:54,640 --> 00:10:59,040
operating systems more easily

00:10:57,120 --> 00:11:00,800
so assuming their machine independent

00:10:59,040 --> 00:11:02,720
representation such as a high-level

00:11:00,800 --> 00:11:04,560
source code or virtual machine

00:11:02,720 --> 00:11:06,079
only the interpreter needs to be

00:11:04,560 --> 00:11:06,959
supplied to run the program on a

00:11:06,079 --> 00:11:08,959
different machine

00:11:06,959 --> 00:11:10,800
right so the like i i don't know about

00:11:08,959 --> 00:11:12,480
you but like i've never ported python

00:11:10,800 --> 00:11:13,200
code to a different architecture it just

00:11:12,480 --> 00:11:15,360
kind of works

00:11:13,200 --> 00:11:17,360
there's stuff that cannot work but that

00:11:15,360 --> 00:11:18,959
usually works

00:11:17,360 --> 00:11:20,399
so of course the program still may be

00:11:18,959 --> 00:11:21,360
doing non-portable operations but that's

00:11:20,399 --> 00:11:23,440
a different kind

00:11:21,360 --> 00:11:24,959
right so if you expose say like system

00:11:23,440 --> 00:11:26,480
or something like that and you just

00:11:24,959 --> 00:11:27,680
shell out if the shell doesn't exist on

00:11:26,480 --> 00:11:28,720
the target platform of course you're not

00:11:27,680 --> 00:11:30,399
portable right but

00:11:28,720 --> 00:11:32,240
by and large interpreted languages are

00:11:30,399 --> 00:11:34,160
pretty important and that's a huge

00:11:32,240 --> 00:11:34,800
upside for programs that are entirely

00:11:34,160 --> 00:11:36,800
jitted

00:11:34,800 --> 00:11:38,640
even though the jit itself has to be

00:11:36,800 --> 00:11:39,920
compiled for the target architecture

00:11:38,640 --> 00:11:41,680
the rest of the program doesn't

00:11:39,920 --> 00:11:43,279
necessarily need to be

00:11:41,680 --> 00:11:45,040
but you can still expose non-portable

00:11:43,279 --> 00:11:47,120
stuff but there's kind of an ease where

00:11:45,040 --> 00:11:50,480
the the program that's injured

00:11:47,120 --> 00:11:52,000
can be easily portable let's look at the

00:11:50,480 --> 00:11:55,360
fourth and last upside

00:11:52,000 --> 00:11:56,000
digits interpreters have access to

00:11:55,360 --> 00:11:58,000
runtime

00:11:56,000 --> 00:11:59,760
information such as input parameters

00:11:58,000 --> 00:12:01,440
control flow and target machine

00:11:59,760 --> 00:12:02,639
specification and so like what machine

00:12:01,440 --> 00:12:03,839
are you running on what instructions

00:12:02,639 --> 00:12:06,079
does it have and stuff like that how

00:12:03,839 --> 00:12:07,920
many cores are there things like that

00:12:06,079 --> 00:12:10,320
this information may change from run to

00:12:07,920 --> 00:12:11,600
run or be unobtainable prior to runtime

00:12:10,320 --> 00:12:12,480
or you don't know what the control flow

00:12:11,600 --> 00:12:14,720
is going to be

00:12:12,480 --> 00:12:15,839
until after you run the program

00:12:14,720 --> 00:12:17,279
additionally

00:12:15,839 --> 00:12:18,959
gathering some types of information

00:12:17,279 --> 00:12:19,360
about a program before it runs may

00:12:18,959 --> 00:12:20,959
involve

00:12:19,360 --> 00:12:23,200
algorithms which are undecidable using

00:12:20,959 --> 00:12:24,959
static analysis there's a lot of

00:12:23,200 --> 00:12:26,480
problems in compilers the compiler has

00:12:24,959 --> 00:12:28,480
to stay too complicated

00:12:26,480 --> 00:12:29,920
can't solve that right like we're not

00:12:28,480 --> 00:12:31,760
trying to solve the whole thing problem

00:12:29,920 --> 00:12:33,360
in a lot of cases compilers will like

00:12:31,760 --> 00:12:34,480
you know iterate a bit and then be like

00:12:33,360 --> 00:12:35,839
yeah this is probably

00:12:34,480 --> 00:12:37,440
pretty complicated i figured out what i

00:12:35,839 --> 00:12:38,560
wanted i'm not going to prove the rest

00:12:37,440 --> 00:12:40,399
right

00:12:38,560 --> 00:12:42,480
whereas like if you run the program

00:12:40,399 --> 00:12:44,000
under a jit you can just kind of narrow

00:12:42,480 --> 00:12:45,040
to the execution that actually happens

00:12:44,000 --> 00:12:47,839
you don't need to prove

00:12:45,040 --> 00:12:49,279
entire things okay so interpreters

00:12:47,839 --> 00:12:51,279
simply know more about the program and

00:12:49,279 --> 00:12:53,440
just can control such information

00:12:51,279 --> 00:12:54,480
and optimize accordingly right another

00:12:53,440 --> 00:12:56,399
example think about

00:12:54,480 --> 00:12:58,000
lto or pgo right so link time

00:12:56,399 --> 00:12:59,600
optimization and pro profile data

00:12:58,000 --> 00:13:01,360
optimization in c plus plus

00:12:59,600 --> 00:13:03,360
it gives a lot more information to the

00:13:01,360 --> 00:13:04,000
compiler and that type of information is

00:13:03,360 --> 00:13:05,839
what a jit

00:13:04,000 --> 00:13:07,200
can gather at runtime same thing for an

00:13:05,839 --> 00:13:10,320
interpreter um

00:13:07,200 --> 00:13:12,399
another thing is is legit can speculate

00:13:10,320 --> 00:13:14,480
on certain facts being true

00:13:12,399 --> 00:13:16,399
optimize accordingly and then roll back

00:13:14,480 --> 00:13:18,160
if it turns out that it was wrong

00:13:16,399 --> 00:13:19,519
right so where static compilers don't do

00:13:18,160 --> 00:13:21,760
that as much they can't but

00:13:19,519 --> 00:13:23,839
it's a bit more complicated all right so

00:13:21,760 --> 00:13:25,519
let's let's go back in time now and look

00:13:23,839 --> 00:13:27,440
at the very first jit systems

00:13:25,519 --> 00:13:29,360
and back then they weren't called jits

00:13:27,440 --> 00:13:31,839
right but they weren't jit systems

00:13:29,360 --> 00:13:33,519
okay so now we have an idea of why you'd

00:13:31,839 --> 00:13:34,320
want chins right through our four key

00:13:33,519 --> 00:13:36,399
criteria

00:13:34,320 --> 00:13:38,079
so we'll go just roughly chronologically

00:13:36,399 --> 00:13:38,800
and see how stuff evolved over the last

00:13:38,079 --> 00:13:41,199
16 years

00:13:38,800 --> 00:13:42,320
now the first published thing that

00:13:41,199 --> 00:13:44,639
seemed to be a jib

00:13:42,320 --> 00:13:46,079
is this one recursive function of

00:13:44,639 --> 00:13:47,760
symbolic expressions and their

00:13:46,079 --> 00:13:49,120
computation by machines this is a paper

00:13:47,760 --> 00:13:50,880
that outlines lisp

00:13:49,120 --> 00:13:52,720
choreo language i think most of you know

00:13:50,880 --> 00:13:54,639
about it and the paper says

00:13:52,720 --> 00:13:56,480
values of compile functions are computed

00:13:54,639 --> 00:13:59,120
about 60 times as fast

00:13:56,480 --> 00:14:01,199
as they would if interpreted compilation

00:13:59,120 --> 00:14:02,000
is fast enough so it is not necessarily

00:14:01,199 --> 00:14:04,320
punch compiled

00:14:02,000 --> 00:14:05,519
programs for future use okay so this is

00:14:04,320 --> 00:14:06,480
the first published video that talks

00:14:05,519 --> 00:14:07,920
about jits

00:14:06,480 --> 00:14:09,440
there are quite a few like that in the

00:14:07,920 --> 00:14:10,399
60s and they're kind of point they do

00:14:09,440 --> 00:14:12,480
different things

00:14:10,399 --> 00:14:14,079
so for example uh when they speak of

00:14:12,480 --> 00:14:15,040
punching compiled programs they actually

00:14:14,079 --> 00:14:16,800
mean punch cards

00:14:15,040 --> 00:14:18,160
uh not punching the program because it's

00:14:16,800 --> 00:14:20,959
not fast enough

00:14:18,160 --> 00:14:22,079
okay so it's quaint it's a really cool

00:14:20,959 --> 00:14:23,279
paper it talks about

00:14:22,079 --> 00:14:25,440
s expressions and things like that it's

00:14:23,279 --> 00:14:27,040
worth watching all right

00:14:25,440 --> 00:14:28,800
so next paper i want to tell you about

00:14:27,040 --> 00:14:29,600
and it's not the second published paper

00:14:28,800 --> 00:14:31,440
on jits but

00:14:29,600 --> 00:14:32,959
it's one of the first uh there's this

00:14:31,440 --> 00:14:33,920
one on regular expressions search

00:14:32,959 --> 00:14:35,839
algorithms

00:14:33,920 --> 00:14:37,680
uh it was written by this guy called k

00:14:35,839 --> 00:14:38,720
thompson who probably did other stuff in

00:14:37,680 --> 00:14:40,240
his life

00:14:38,720 --> 00:14:42,079
but it's another quaint paper from the

00:14:40,240 --> 00:14:44,240
60s really interesting and it starts off

00:14:42,079 --> 00:14:46,480
by saying like the compiler consists of

00:14:44,240 --> 00:14:48,000
three concurrent running stages stage

00:14:46,480 --> 00:14:49,519
number one

00:14:48,000 --> 00:14:50,880
syntax sieve that allows only

00:14:49,519 --> 00:14:52,160
syntactically correct regular

00:14:50,880 --> 00:14:54,560
expressions to bet

00:14:52,160 --> 00:14:56,160
makes sense stage number two convert the

00:14:54,560 --> 00:14:57,199
regular expression to reverse polish

00:14:56,160 --> 00:14:59,680
notation

00:14:57,199 --> 00:15:01,600
final stage of the compiler object code

00:14:59,680 --> 00:15:02,959
producer which expects a syntactically

00:15:01,600 --> 00:15:04,160
correct reverse polish regular

00:15:02,959 --> 00:15:06,240
expression

00:15:04,160 --> 00:15:07,760
pretty simple legit three steps you

00:15:06,240 --> 00:15:08,399
gotta compile it that's pretty cool

00:15:07,760 --> 00:15:10,959
right

00:15:08,399 --> 00:15:12,079
now you know we don't always do regular

00:15:10,959 --> 00:15:14,320
expressions nowadays

00:15:12,079 --> 00:15:15,839
uh but you can get really good speed ups

00:15:14,320 --> 00:15:17,839
if you do

00:15:15,839 --> 00:15:19,440
all right next paper i want to tell you

00:15:17,839 --> 00:15:21,440
about we're going to jump a bit

00:15:19,440 --> 00:15:22,959
forward now this one is the official

00:15:21,440 --> 00:15:24,399
implementation of the small talk 80

00:15:22,959 --> 00:15:25,920
system from 84.

00:15:24,399 --> 00:15:27,760
it's a small talk 80 programming

00:15:25,920 --> 00:15:28,639
language includes dynamic storage

00:15:27,760 --> 00:15:31,360
allocation

00:15:28,639 --> 00:15:33,759
full upward final arts and universally

00:15:31,360 --> 00:15:35,680
polymorphic procedures

00:15:33,759 --> 00:15:37,839
the small talk 80 programming system

00:15:35,680 --> 00:15:39,519
features interactive execution with

00:15:37,839 --> 00:15:41,279
incremental compilation and

00:15:39,519 --> 00:15:43,839
implementation portability

00:15:41,279 --> 00:15:44,959
again portability is a key feature here

00:15:43,839 --> 00:15:46,800
these features of

00:15:44,959 --> 00:15:49,040
modern programming systems are among the

00:15:46,800 --> 00:15:51,040
most difficult to implement efficiently

00:15:49,040 --> 00:15:53,199
even individually and they have all of

00:15:51,040 --> 00:15:54,560
those wrapped into small talk

00:15:53,199 --> 00:15:56,639
so it's really interesting when you read

00:15:54,560 --> 00:15:58,800
this paper it's pretty short but you see

00:15:56,639 --> 00:16:00,079
what authors call out about their work

00:15:58,800 --> 00:16:00,720
right they thought this was worth

00:16:00,079 --> 00:16:02,480
publishing

00:16:00,720 --> 00:16:04,000
and they talked about what was actually

00:16:02,480 --> 00:16:05,199
considered hard back then now it might

00:16:04,000 --> 00:16:06,320
still be hard today but i think it's

00:16:05,199 --> 00:16:06,800
very interesting to see what they

00:16:06,320 --> 00:16:09,680
outline

00:16:06,800 --> 00:16:10,320
here and so in particular full upward

00:16:09,680 --> 00:16:12,079
fun art

00:16:10,320 --> 00:16:14,160
sounds really cool but it's allowing you

00:16:12,079 --> 00:16:15,440
to pass a function with its closure

00:16:14,160 --> 00:16:17,519
and that was considered really really

00:16:15,440 --> 00:16:19,440
hard back then and so

00:16:17,519 --> 00:16:21,600
the paper dives into important

00:16:19,440 --> 00:16:23,920
optimizations for small talk itself

00:16:21,600 --> 00:16:24,880
right so discuss this thing like macro

00:16:23,920 --> 00:16:27,920
expansion of

00:16:24,880 --> 00:16:29,600
a v code into encode with caching so in

00:16:27,920 --> 00:16:30,639
other words there's an ir before the

00:16:29,600 --> 00:16:32,480
machine code and that was pretty

00:16:30,639 --> 00:16:33,920
innovative back then

00:16:32,480 --> 00:16:36,480
all right let's look at another paper

00:16:33,920 --> 00:16:38,079
that's really interesting optimizing

00:16:36,480 --> 00:16:39,519
dynamically typed object-oriented

00:16:38,079 --> 00:16:40,240
languages with polymorphic inline

00:16:39,519 --> 00:16:42,959
caching

00:16:40,240 --> 00:16:43,839
1991 itself so palmer framing light

00:16:42,959 --> 00:16:47,759
caching picks

00:16:43,839 --> 00:16:49,680
achieve a median speed up of 11

00:16:47,759 --> 00:16:51,519
as an important side effect pics collect

00:16:49,680 --> 00:16:53,360
type information by recording all of the

00:16:51,519 --> 00:16:55,040
received types actually used at a given

00:16:53,360 --> 00:16:55,680
concept right so self is a dynamic

00:16:55,040 --> 00:16:57,600
language

00:16:55,680 --> 00:16:59,279
and and types can change over time right

00:16:57,600 --> 00:17:00,480
it's not like this class the class

00:16:59,279 --> 00:17:02,959
itself can change over time

00:17:00,480 --> 00:17:04,880
and picks collect the information and it

00:17:02,959 --> 00:17:06,319
optimizes based on them

00:17:04,880 --> 00:17:07,919
all right the compiler can exploit this

00:17:06,319 --> 00:17:10,240
type of information to generate better

00:17:07,919 --> 00:17:12,640
code when recompiling a method

00:17:10,240 --> 00:17:13,439
interesting an experimental version of

00:17:12,640 --> 00:17:16,079
such a system

00:17:13,439 --> 00:17:17,280
achieves a median speed up of 27 for a

00:17:16,079 --> 00:17:19,120
set of self programs

00:17:17,280 --> 00:17:21,360
reducing the number of non-inline

00:17:19,120 --> 00:17:22,959
messages sent by a factor of two

00:17:21,360 --> 00:17:24,319
right so self if you're not really

00:17:22,959 --> 00:17:24,959
familiar with it it's a precursor to

00:17:24,319 --> 00:17:26,559
javascript

00:17:24,959 --> 00:17:28,000
so it has prototypical inheritance which

00:17:26,559 --> 00:17:30,880
is kind of like changing the v

00:17:28,000 --> 00:17:32,960
table of a type at runtime so you can go

00:17:30,880 --> 00:17:33,919
in and say you had these methods now you

00:17:32,960 --> 00:17:37,200
have those methods

00:17:33,919 --> 00:17:38,720
right but the key thing is that rarely

00:17:37,200 --> 00:17:40,000
happens in practice you don't really

00:17:38,720 --> 00:17:42,960
change your your types

00:17:40,000 --> 00:17:45,120
uh uh behavior that much over time and

00:17:42,960 --> 00:17:46,960
so polymorphic inline caches come in

00:17:45,120 --> 00:17:48,880
and while you execute stuff you record

00:17:46,960 --> 00:17:50,160
what you see and you check against the

00:17:48,880 --> 00:17:52,080
common case runtime so

00:17:50,160 --> 00:17:53,760
runtime you need to say hey did you

00:17:52,080 --> 00:17:55,440
change anything and if you didn't change

00:17:53,760 --> 00:17:57,600
anything then you go to the fastpass

00:17:55,440 --> 00:17:59,200
right you can you can kind of expand a

00:17:57,600 --> 00:18:00,799
bunch of stuff so it's a really cute

00:17:59,200 --> 00:18:02,880
optimization which tries to make the

00:18:00,799 --> 00:18:04,720
language's dynamism more static

00:18:02,880 --> 00:18:07,039
when it actually is that so that's cool

00:18:04,720 --> 00:18:08,960
and you just cover it runtime

00:18:07,039 --> 00:18:10,400
all right so let's go back to our friend

00:18:08,960 --> 00:18:12,400
alice here

00:18:10,400 --> 00:18:14,640
alice says to the cat would you like to

00:18:12,400 --> 00:18:15,919
tell me please which way i ought to go

00:18:14,640 --> 00:18:19,120
from here

00:18:15,919 --> 00:18:19,840
the cat replies that depends a good deal

00:18:19,120 --> 00:18:23,760
on where

00:18:19,840 --> 00:18:26,400
you want to get to now as perplexed says

00:18:23,760 --> 00:18:28,480
i don't really care where the cat

00:18:26,400 --> 00:18:29,919
finishes with well

00:18:28,480 --> 00:18:31,760
it doesn't really much matter which way

00:18:29,919 --> 00:18:34,480
you go right

00:18:31,760 --> 00:18:36,160
the key inside here that cat has is

00:18:34,480 --> 00:18:38,400
figure out what you're trying to do

00:18:36,160 --> 00:18:39,360
and why it matters and there's a more

00:18:38,400 --> 00:18:42,000
grown-up

00:18:39,360 --> 00:18:43,840
way to say this it's in the paper uh by

00:18:42,000 --> 00:18:45,440
by er

00:18:43,840 --> 00:18:47,120
where he says in the course of our

00:18:45,440 --> 00:18:48,320
experiments we discovered that the

00:18:47,120 --> 00:18:50,880
trigger mechanism

00:18:48,320 --> 00:18:52,240
when is much less important for good

00:18:50,880 --> 00:18:55,679
recompilation results

00:18:52,240 --> 00:18:58,240
than the selection mechanism what

00:18:55,679 --> 00:18:59,360
so same as the cheshire cat first

00:18:58,240 --> 00:19:02,080
figured out

00:18:59,360 --> 00:19:03,360
that uh what you do matters more than

00:19:02,080 --> 00:19:05,760
when you do it

00:19:03,360 --> 00:19:07,280
and so what you optimize is way more

00:19:05,760 --> 00:19:09,520
important than when you optimize it

00:19:07,280 --> 00:19:11,200
for longer term payoffs so maybe you can

00:19:09,520 --> 00:19:12,320
take a bit more time and optimize the

00:19:11,200 --> 00:19:14,000
important functions

00:19:12,320 --> 00:19:15,440
take a bit more time look at what's

00:19:14,000 --> 00:19:16,240
actually hot look at what the behavior

00:19:15,440 --> 00:19:18,080
actually is

00:19:16,240 --> 00:19:19,679
and and what you need to include inside

00:19:18,080 --> 00:19:20,960
your optimization instead of optimizing

00:19:19,679 --> 00:19:24,080
the first themes

00:19:20,960 --> 00:19:25,679
that's kind of interesting all right so

00:19:24,080 --> 00:19:26,480
let's look at a bigger system to this

00:19:25,679 --> 00:19:29,120
optimization

00:19:26,480 --> 00:19:31,280
this system is called dpf it's a fast

00:19:29,120 --> 00:19:34,080
flexible message demultiplexing using

00:19:31,280 --> 00:19:36,799
dynamic code generation from 1994.

00:19:34,080 --> 00:19:37,440
so describe that as a new packet filter

00:19:36,799 --> 00:19:39,840
system

00:19:37,440 --> 00:19:41,679
dpf that provides both the traditional

00:19:39,840 --> 00:19:44,240
flexibility of packet filters and the

00:19:41,679 --> 00:19:45,840
speed of hand-crafted de-multiplexing

00:19:44,240 --> 00:19:48,480
routines

00:19:45,840 --> 00:19:50,240
it goes further dpf filters run 10 to 50

00:19:48,480 --> 00:19:52,240
times faster than the fastest packet

00:19:50,240 --> 00:19:53,039
filter reported in literature right so

00:19:52,240 --> 00:19:55,440
what it's doing

00:19:53,039 --> 00:19:56,160
is it's filtering network packets really

00:19:55,440 --> 00:19:59,440
really fast

00:19:56,160 --> 00:20:00,720
right that's cool so dps performance is

00:19:59,440 --> 00:20:02,559
either equivalent to

00:20:00,720 --> 00:20:05,360
or when it can explore runtime

00:20:02,559 --> 00:20:07,440
information superior to hand coded the

00:20:05,360 --> 00:20:09,600
multiplexers

00:20:07,440 --> 00:20:11,520
dpf achieves high performance by using

00:20:09,600 --> 00:20:12,880
carefully designed declarative package

00:20:11,520 --> 00:20:15,520
filter language that is aggressively

00:20:12,880 --> 00:20:17,520
optimized using dynamic code generation

00:20:15,520 --> 00:20:19,039
and so like what this does right when

00:20:17,520 --> 00:20:20,000
you want to filter packets coming in and

00:20:19,039 --> 00:20:22,559
you have a multi-user

00:20:20,000 --> 00:20:23,760
system is you want user space to say

00:20:22,559 --> 00:20:25,760
like this goes to me

00:20:23,760 --> 00:20:27,440
do this and do this on these packets do

00:20:25,760 --> 00:20:29,760
that thing and user space can

00:20:27,440 --> 00:20:32,159
dynamically generate packets filter code

00:20:29,760 --> 00:20:33,600
to steer the network packets directly in

00:20:32,159 --> 00:20:35,280
the kernel now the thing that's cool

00:20:33,600 --> 00:20:36,000
about this is this is safe right so the

00:20:35,280 --> 00:20:37,679
kernel

00:20:36,000 --> 00:20:39,679
needs access to all the packets that

00:20:37,679 --> 00:20:41,760
come in right and in general

00:20:39,679 --> 00:20:43,520
that's how traditional networking works

00:20:41,760 --> 00:20:44,559
right like the kernel has access to the

00:20:43,520 --> 00:20:46,799
networking hardware

00:20:44,559 --> 00:20:47,760
and user space asks the kernel to steer

00:20:46,799 --> 00:20:50,000
stuff

00:20:47,760 --> 00:20:51,760
dpf allows you as a user to inject

00:20:50,000 --> 00:20:53,520
directly into the kernel

00:20:51,760 --> 00:20:55,039
unknowing what the other user space

00:20:53,520 --> 00:20:55,360
processes are doing or whether packets

00:20:55,039 --> 00:20:57,039
are

00:20:55,360 --> 00:20:58,960
right now you want this to be fast and

00:20:57,039 --> 00:21:00,720
dpf was the first one to kind of explore

00:20:58,960 --> 00:21:02,159
doing that really quickly by having some

00:21:00,720 --> 00:21:04,640
form of legit that's safe

00:21:02,159 --> 00:21:06,080
in the kernel and dpf was part of a

00:21:04,640 --> 00:21:07,919
bigger project called

00:21:06,080 --> 00:21:09,679
called exokernels there's another paper

00:21:07,919 --> 00:21:10,799
exokernel operating system architecture

00:21:09,679 --> 00:21:11,440
for application level resource

00:21:10,799 --> 00:21:15,200
management

00:21:11,440 --> 00:21:17,440
in 1995 which says in the exokernel

00:21:15,200 --> 00:21:18,080
architecture a small kernel securely

00:21:17,440 --> 00:21:19,760
exports

00:21:18,080 --> 00:21:21,919
all hardware resources through a

00:21:19,760 --> 00:21:23,120
low-level interface to untrusted library

00:21:21,919 --> 00:21:24,640
operating systems

00:21:23,120 --> 00:21:26,720
hard-coding the implementation of these

00:21:24,640 --> 00:21:28,080
abstractions is inappropriate for three

00:21:26,720 --> 00:21:30,320
main reasons and so here

00:21:28,080 --> 00:21:31,200
they outline why exoplanetal is really

00:21:30,320 --> 00:21:33,840
cool right

00:21:31,200 --> 00:21:35,360
reason number one it denies applications

00:21:33,840 --> 00:21:36,559
the advantage of domain specific

00:21:35,360 --> 00:21:39,200
optimizations

00:21:36,559 --> 00:21:40,480
all right interesting two it discourages

00:21:39,200 --> 00:21:41,919
changes to the implementation of

00:21:40,480 --> 00:21:44,320
existing abstractions

00:21:41,919 --> 00:21:45,919
and number three and it restricts the

00:21:44,320 --> 00:21:48,159
flexibility of application builders

00:21:45,919 --> 00:21:49,520
since new abstractions can only be added

00:21:48,159 --> 00:21:51,520
by awkward emulation on top of the

00:21:49,520 --> 00:21:54,400
existing ones if they can be animal

00:21:51,520 --> 00:21:55,840
right so dpf is part of that system an

00:21:54,400 --> 00:21:58,640
exokernel has a bunch of

00:21:55,840 --> 00:22:00,799
small things and that it does to export

00:21:58,640 --> 00:22:02,799
hardware capabilities to user space

00:22:00,799 --> 00:22:04,240
right and the rationale for exposing

00:22:02,799 --> 00:22:05,760
hardware interfaces instead of

00:22:04,240 --> 00:22:07,280
abstracting them away are

00:22:05,760 --> 00:22:09,919
really really similar to the rationale

00:22:07,280 --> 00:22:11,679
we've discussed for using egypt right so

00:22:09,919 --> 00:22:14,080
this is the os level you're trying to

00:22:11,679 --> 00:22:17,039
give more flexibility and dynamism to

00:22:14,080 --> 00:22:20,240
the user space movements all right cool

00:22:17,039 --> 00:22:21,840
so talking of use of space programs

00:22:20,240 --> 00:22:23,919
there's another type of jit that i want

00:22:21,840 --> 00:22:26,320
to tell you about and the first really

00:22:23,919 --> 00:22:27,200
nicely pioneering paper that does that

00:22:26,320 --> 00:22:28,720
is called atom

00:22:27,200 --> 00:22:30,799
instead of showing you a quote from the

00:22:28,720 --> 00:22:31,919
paper i'll show you code so adam's from

00:22:30,799 --> 00:22:33,360
00:22:31,919 --> 00:22:35,520
here's what it does it starts off and

00:22:33,360 --> 00:22:37,120
you say instrument in it and you

00:22:35,520 --> 00:22:38,960
say like you do these things i'd call

00:22:37,120 --> 00:22:40,480
prototypes and whatever and then

00:22:38,960 --> 00:22:42,159
if you look at the loops right an

00:22:40,480 --> 00:22:44,159
instrument you say well

00:22:42,159 --> 00:22:45,679
guess get the the first procedure in the

00:22:44,159 --> 00:22:47,520
object for that

00:22:45,679 --> 00:22:48,799
get each block in the procedure and for

00:22:47,520 --> 00:22:49,679
that get each instruction in the

00:22:48,799 --> 00:22:52,159
procedure

00:22:49,679 --> 00:22:53,360
if the instruction type is a load add a

00:22:52,159 --> 00:22:55,600
call before it

00:22:53,360 --> 00:22:56,799
if the instruction type is a store add a

00:22:55,600 --> 00:22:59,679
call before that

00:22:56,799 --> 00:23:00,640
right so adam is assistant for building

00:22:59,679 --> 00:23:02,640
customized

00:23:00,640 --> 00:23:04,240
customized program analysis tools it's

00:23:02,640 --> 00:23:06,480
generally called dynamic binary

00:23:04,240 --> 00:23:08,240
instrumentation in other papers

00:23:06,480 --> 00:23:09,840
so this simple adam program like i'm

00:23:08,240 --> 00:23:12,159
just missing in maine here

00:23:09,840 --> 00:23:13,120
but this is the entire program it

00:23:12,159 --> 00:23:15,440
instruments all

00:23:13,120 --> 00:23:16,880
of an already compiled program's loads

00:23:15,440 --> 00:23:18,799
and stores right and it

00:23:16,880 --> 00:23:21,039
adds a call to a function a function

00:23:18,799 --> 00:23:23,440
call before every load and store

00:23:21,039 --> 00:23:24,799
so therefore adam disassembles a program

00:23:23,440 --> 00:23:27,440
all of the program

00:23:24,799 --> 00:23:28,480
instruments it then reassembles it right

00:23:27,440 --> 00:23:30,159
fixing up everything

00:23:28,480 --> 00:23:31,520
executes that's that's really really

00:23:30,159 --> 00:23:33,360
cute um

00:23:31,520 --> 00:23:34,559
and there's another paper called pin

00:23:33,360 --> 00:23:36,000
that does the same thing there's a lot

00:23:34,559 --> 00:23:36,400
of papers in that field i won't talk

00:23:36,000 --> 00:23:38,480
about

00:23:36,400 --> 00:23:39,840
the details from 2005 but it goes way

00:23:38,480 --> 00:23:43,520
further than what adams

00:23:39,840 --> 00:23:45,200
does in its worth reading all right so

00:23:43,520 --> 00:23:46,400
now that we know how to like instrument

00:23:45,200 --> 00:23:47,120
programs into a bunch of other stuff

00:23:46,400 --> 00:23:48,960
let's flip

00:23:47,120 --> 00:23:50,720
and use gifs to do something completely

00:23:48,960 --> 00:23:52,960
different there's this

00:23:50,720 --> 00:23:54,240
program called ambra fast and flexible

00:23:52,960 --> 00:23:57,200
machine simulations

00:23:54,240 --> 00:23:58,240
1996. so it's a simulator for the

00:23:57,200 --> 00:24:00,640
processor

00:23:58,240 --> 00:24:01,840
caches and memory system of uniprocessor

00:24:00,640 --> 00:24:05,279
and cache coherent

00:24:01,840 --> 00:24:07,039
multiprocessors it uses dynamic binary

00:24:05,279 --> 00:24:08,960
translation to generate code sequences

00:24:07,039 --> 00:24:11,039
which simulate the workload

00:24:08,960 --> 00:24:12,480
and embrac can simulate real workloads

00:24:11,039 --> 00:24:15,120
at speeds only

00:24:12,480 --> 00:24:17,200
three to nine times slower than native

00:24:15,120 --> 00:24:18,880
executables

00:24:17,200 --> 00:24:20,880
so you can customize its generated code

00:24:18,880 --> 00:24:22,000
to include a processor cache model which

00:24:20,880 --> 00:24:23,520
allows it to comp

00:24:22,000 --> 00:24:26,080
compute the cache misses and memory

00:24:23,520 --> 00:24:28,159
stalls a time of workload at a slowdown

00:24:26,080 --> 00:24:29,840
of only a factor 7 to 20.

00:24:28,159 --> 00:24:31,520
all right so simulation let's step back

00:24:29,840 --> 00:24:32,000
a bit what simulation it's the process

00:24:31,520 --> 00:24:34,240
of running

00:24:32,000 --> 00:24:36,159
a native executable machine code for one

00:24:34,240 --> 00:24:38,480
architecture on another architecture

00:24:36,159 --> 00:24:39,279
right so you say you have like some arm

00:24:38,480 --> 00:24:41,279
processor

00:24:39,279 --> 00:24:43,120
you have and you have an x86 machine you

00:24:41,279 --> 00:24:44,720
can simulate the entire arm processor in

00:24:43,120 --> 00:24:46,720
the x86 machine

00:24:44,720 --> 00:24:48,400
and dynamic wiring translation is the

00:24:46,720 --> 00:24:50,480
execution of the program from one

00:24:48,400 --> 00:24:52,880
instructions architecture an isa

00:24:50,480 --> 00:24:54,960
into another or the same iso and so it's

00:24:52,880 --> 00:24:56,559
performing the translation dynamically

00:24:54,960 --> 00:24:58,480
in other words it disassembles the

00:24:56,559 --> 00:24:59,840
binary as you execute it and reassembles

00:24:58,480 --> 00:25:03,200
a completely different binder

00:24:59,840 --> 00:25:04,080
and ambra was simulating mips rf 3000

00:25:03,200 --> 00:25:06,559
00:25:04,080 --> 00:25:07,520
on sgi systems and that's kind of

00:25:06,559 --> 00:25:10,960
interesting

00:25:07,520 --> 00:25:13,840
because the main reason this is useful

00:25:10,960 --> 00:25:15,679
is is to develop new hardware right

00:25:13,840 --> 00:25:16,960
either hardware for isis that exists but

00:25:15,679 --> 00:25:19,200
you're changing the hardware

00:25:16,960 --> 00:25:20,600
or for isis just don't exist right so

00:25:19,200 --> 00:25:22,720
say you're developing like

00:25:20,600 --> 00:25:24,400
armv20 or something right that doesn't

00:25:22,720 --> 00:25:25,760
exist yet but you still want to simulate

00:25:24,400 --> 00:25:27,200
it see how it works and things write a

00:25:25,760 --> 00:25:29,279
compiler for and target it

00:25:27,200 --> 00:25:31,120
you use a simulator for that right and

00:25:29,279 --> 00:25:32,799
and so when i started working on a cpu

00:25:31,120 --> 00:25:33,440
like 11 years ago it wasn't obvious to

00:25:32,799 --> 00:25:35,360
me that

00:25:33,440 --> 00:25:36,640
what you would do i don't know how spiel

00:25:35,360 --> 00:25:37,840
development work but that's that's in

00:25:36,640 --> 00:25:39,919
large parts what they do they use a

00:25:37,840 --> 00:25:41,760
simulator or multiple simulators

00:25:39,919 --> 00:25:44,559
and they simulate what the target

00:25:41,760 --> 00:25:46,400
hardware does with varying accuracy

00:25:44,559 --> 00:25:48,159
right so you might want to only simulate

00:25:46,400 --> 00:25:48,799
architectural states so registers in

00:25:48,159 --> 00:25:51,200
memory

00:25:48,799 --> 00:25:52,559
or as emperor says here you might want

00:25:51,200 --> 00:25:54,640
to simulate

00:25:52,559 --> 00:25:56,480
other details that are non-architectural

00:25:54,640 --> 00:25:57,840
such as you know the shadow registers

00:25:56,480 --> 00:25:59,039
the caches and other stuff

00:25:57,840 --> 00:26:00,640
right so the caches have some

00:25:59,039 --> 00:26:01,840
architectural behavior when you're like

00:26:00,640 --> 00:26:04,000
doing a multi-processor

00:26:01,840 --> 00:26:05,200
but by and large the reason you simulate

00:26:04,000 --> 00:26:07,600
caches in the

00:26:05,200 --> 00:26:09,200
uni processor system is to just like see

00:26:07,600 --> 00:26:12,080
how the cache system works

00:26:09,200 --> 00:26:13,679
right and um you also might want to do

00:26:12,080 --> 00:26:15,200
like timing of instructions right to see

00:26:13,679 --> 00:26:16,880
how fast your processor is going to be

00:26:15,200 --> 00:26:18,400
do caches memory and a bunch of other

00:26:16,880 --> 00:26:20,720
stuff and this is really

00:26:18,400 --> 00:26:21,520
really really slow to do an interpreter

00:26:20,720 --> 00:26:22,799
right

00:26:21,520 --> 00:26:24,240
because you do a lot of checks and a

00:26:22,799 --> 00:26:25,200
bunch of other stuff that's really

00:26:24,240 --> 00:26:27,200
redundant

00:26:25,200 --> 00:26:28,960
and ambra is the first machine simulator

00:26:27,200 --> 00:26:30,000
that use dynamic binary translation to

00:26:28,960 --> 00:26:31,520
do that instead

00:26:30,000 --> 00:26:32,960
so you don't like you know the process

00:26:31,520 --> 00:26:33,919
the simulator doesn't know what program

00:26:32,960 --> 00:26:35,679
it's going to run so you're going to

00:26:33,919 --> 00:26:36,240
boot linux on on your simulator or

00:26:35,679 --> 00:26:37,440
whatever

00:26:36,240 --> 00:26:38,640
like that might do a bunch of stuff you

00:26:37,440 --> 00:26:40,240
might go on the internet download a

00:26:38,640 --> 00:26:41,120
bunch of stuff and things like in in the

00:26:40,240 --> 00:26:43,039
simulator

00:26:41,120 --> 00:26:44,880
ambra can't hardcore all of that but you

00:26:43,039 --> 00:26:46,559
can jit it as it goes

00:26:44,880 --> 00:26:47,840
and so the speed numbers quoted here

00:26:46,559 --> 00:26:49,679
seem really slow but they're actually

00:26:47,840 --> 00:26:51,279
quite good right ambra can change the

00:26:49,679 --> 00:26:53,120
functionality of the simulator too which

00:26:51,279 --> 00:26:54,720
is cool right you can say like start up

00:26:53,120 --> 00:26:56,080
simulate the cache and then don't

00:26:54,720 --> 00:26:57,360
simulate the cache layer you can do

00:26:56,080 --> 00:26:58,320
those two things you just have one

00:26:57,360 --> 00:27:00,400
simulator

00:26:58,320 --> 00:27:02,400
that's kind of cool all right so let's

00:27:00,400 --> 00:27:05,039
look more at dynamic optimizations

00:27:02,400 --> 00:27:06,640
this system is kind of cool oberon 3 has

00:27:05,039 --> 00:27:07,760
a paper called dynamic optimizations

00:27:06,640 --> 00:27:10,080
from 1999

00:27:07,760 --> 00:27:11,360
and the description of it is is kisper

00:27:10,080 --> 00:27:13,039
looked at cache optimizations

00:27:11,360 --> 00:27:14,080
rearranging fields in the structure

00:27:13,039 --> 00:27:16,080
dynamically

00:27:14,080 --> 00:27:18,480
to optimize the program's data access

00:27:16,080 --> 00:27:20,399
patterns and a dynamic version of trace

00:27:18,480 --> 00:27:22,320
scheduling which optimizes based on the

00:27:20,399 --> 00:27:24,399
information about program's control flow

00:27:22,320 --> 00:27:26,720
during execution

00:27:24,399 --> 00:27:27,679
the continuous optimizer itself executes

00:27:26,720 --> 00:27:30,240
in the background

00:27:27,679 --> 00:27:31,919
as a separate low priority thread that's

00:27:30,240 --> 00:27:33,120
cool which executes only during a

00:27:31,919 --> 00:27:34,399
program's idle time

00:27:33,120 --> 00:27:36,799
so kessler who's the author of that

00:27:34,399 --> 00:27:38,159
paper used a more sophisticated metric

00:27:36,799 --> 00:27:40,000
than straightforward counters to

00:27:38,159 --> 00:27:41,919
determine when to optimize and observe

00:27:40,000 --> 00:27:43,120
that deciding what to optimize is highly

00:27:41,919 --> 00:27:44,559
optimization specific

00:27:43,120 --> 00:27:46,480
right that's an interesting optimization

00:27:44,559 --> 00:27:47,760
same one that burst did

00:27:46,480 --> 00:27:50,000
which is kind of interesting because

00:27:47,760 --> 00:27:50,480
first was on thomas kissler's phd at

00:27:50,000 --> 00:27:53,600
buyer's

00:27:50,480 --> 00:27:55,679
advisory board kind of makes sense um

00:27:53,600 --> 00:27:56,640
but but to to explain what oberon does

00:27:55,679 --> 00:27:58,720
here right

00:27:56,640 --> 00:28:00,559
uh c plus plus programmers are pretty

00:27:58,720 --> 00:28:01,520
familiar with doing a real struct

00:28:00,559 --> 00:28:02,960
destructive weight

00:28:01,520 --> 00:28:04,399
array transforms right you do that

00:28:02,960 --> 00:28:05,600
manually you have an array of structs

00:28:04,399 --> 00:28:07,120
you're like ah this is kind of slow

00:28:05,600 --> 00:28:08,799
because cache locality and stuff let's

00:28:07,120 --> 00:28:10,799
destructive arrays right

00:28:08,799 --> 00:28:12,559
but there's so much more theoretically

00:28:10,799 --> 00:28:14,080
that could be done if we could figure

00:28:12,559 --> 00:28:15,919
out the api concerns right so if you're

00:28:14,080 --> 00:28:16,480
able to change the layout of the class

00:28:15,919 --> 00:28:18,559
or

00:28:16,480 --> 00:28:20,320
arrays or whatever else not caring about

00:28:18,559 --> 00:28:22,000
the api you could change stuff and

00:28:20,320 --> 00:28:23,840
really optimize things really well based

00:28:22,000 --> 00:28:25,600
on the the you know the locality of

00:28:23,840 --> 00:28:28,159
accesses and stuff like that

00:28:25,600 --> 00:28:29,200
and so that's what this paper did right

00:28:28,159 --> 00:28:32,000
trace scheduling

00:28:29,200 --> 00:28:33,279
reorders code based on what's likely and

00:28:32,000 --> 00:28:35,039
unlikely can inline

00:28:33,279 --> 00:28:37,440
based on this information that's really

00:28:35,039 --> 00:28:38,480
really cool and this therefore changes

00:28:37,440 --> 00:28:41,840
the structure of

00:28:38,480 --> 00:28:44,320
data at runtime as well as the code

00:28:41,840 --> 00:28:45,840
structure of runtime right and that's a

00:28:44,320 --> 00:28:47,679
really cool aspect of jits right we've

00:28:45,840 --> 00:28:49,600
talked about just generating

00:28:47,679 --> 00:28:50,960
uh different code at runtime and

00:28:49,600 --> 00:28:52,399
changing it but

00:28:50,960 --> 00:28:54,399
it can also change the structure of your

00:28:52,399 --> 00:28:56,880
data runtime which is like a key

00:28:54,399 --> 00:28:58,080
thing that just do really well all right

00:28:56,880 --> 00:29:00,559
let's look at a similar system

00:28:58,080 --> 00:29:02,720
called dynamo it's a transparent dynamic

00:29:00,559 --> 00:29:04,559
compensation system from 2000.

00:29:02,720 --> 00:29:06,240
uh it's a software dynamic optimization

00:29:04,559 --> 00:29:08,000
system that's capable of transparently

00:29:06,240 --> 00:29:10,000
improving the performance of the native

00:29:08,000 --> 00:29:11,039
instruction stream as it executes on the

00:29:10,000 --> 00:29:13,360
process

00:29:11,039 --> 00:29:14,799
right it focuses its efforts on

00:29:13,360 --> 00:29:15,440
optimization opportunities that tend to

00:29:14,799 --> 00:29:17,679
manifest

00:29:15,440 --> 00:29:19,039
only at run time enhance opportunities

00:29:17,679 --> 00:29:20,399
that might be difficult for static

00:29:19,039 --> 00:29:22,399
compiler

00:29:20,399 --> 00:29:23,840
okay so we looked to previous systems

00:29:22,399 --> 00:29:25,520
that did translation from one

00:29:23,840 --> 00:29:27,360
architecture to the other or from a

00:29:25,520 --> 00:29:28,000
high-level language to like assembly and

00:29:27,360 --> 00:29:30,880
things like

00:29:28,000 --> 00:29:31,840
dynamo takes your binary and executes

00:29:30,880 --> 00:29:34,320
the same

00:29:31,840 --> 00:29:34,960
the binary on on the same cpu that you

00:29:34,320 --> 00:29:37,279
entered

00:29:34,960 --> 00:29:38,080
intended to run it on right so it

00:29:37,279 --> 00:29:40,080
translates from

00:29:38,080 --> 00:29:41,840
one i set to the same isa but it

00:29:40,080 --> 00:29:42,399
optimizes the code right whereas adam

00:29:41,840 --> 00:29:44,480
went in

00:29:42,399 --> 00:29:46,320
and used the jit to instrument the code

00:29:44,480 --> 00:29:48,399
dynamo just explodes your code

00:29:46,320 --> 00:29:49,679
says these instructions are useless

00:29:48,399 --> 00:29:51,120
throws them away and then

00:29:49,679 --> 00:29:52,320
assembles a faster version of your

00:29:51,120 --> 00:29:53,120
program that has exactly the same

00:29:52,320 --> 00:29:56,240
behavior

00:29:53,120 --> 00:29:57,840
right and can also optimize uh um say

00:29:56,240 --> 00:29:59,120
you run a program a new dynamo and that

00:29:57,840 --> 00:30:01,200
program's the jit itself

00:29:59,120 --> 00:30:02,720
dynamo can handle a jit within a gym

00:30:01,200 --> 00:30:05,360
right so you can jit the gym

00:30:02,720 --> 00:30:06,799
right and technically could jit itself

00:30:05,360 --> 00:30:08,640
right

00:30:06,799 --> 00:30:10,320
so it doesn't require user guidance to

00:30:08,640 --> 00:30:12,080
optimize nor multiple runs which is

00:30:10,320 --> 00:30:12,720
really cool right so it's not like a pgo

00:30:12,080 --> 00:30:14,000
and l2

00:30:12,720 --> 00:30:15,520
type thing you just kind of run it and

00:30:14,000 --> 00:30:17,279
then as you run it figures out

00:30:15,520 --> 00:30:18,799
properties of the program and optimizes

00:30:17,279 --> 00:30:20,240
those important points

00:30:18,799 --> 00:30:21,440
so you just run the program in dynamo

00:30:20,240 --> 00:30:22,320
you get faster stuff and so their

00:30:21,440 --> 00:30:24,000
numbers

00:30:22,320 --> 00:30:26,559
at the time they ran spec now you spec

00:30:24,000 --> 00:30:29,279
into 95 and it went from dash

00:30:26,559 --> 00:30:30,799
o level uh performance so o0 basically

00:30:29,279 --> 00:30:34,159
to the performance of o4

00:30:30,799 --> 00:30:36,159
right so you you if you run a o0 program

00:30:34,159 --> 00:30:38,000
under dynamo you get all four level

00:30:36,159 --> 00:30:39,600
performance right but you

00:30:38,000 --> 00:30:40,880
you know your static compiler runs for a

00:30:39,600 --> 00:30:41,600
short amount of time there's just no

00:30:40,880 --> 00:30:43,039
optimizations

00:30:41,600 --> 00:30:44,720
and then dynamo just sees what's

00:30:43,039 --> 00:30:45,760
important to optimize not uses that and

00:30:44,720 --> 00:30:48,080
nothing else

00:30:45,760 --> 00:30:49,360
that's kind of cool right now again like

00:30:48,080 --> 00:30:50,640
take this with a grain of salt

00:30:49,360 --> 00:30:51,919
right like maybe the static compiler

00:30:50,640 --> 00:30:54,000
wasn't that great or whatever else but

00:30:51,919 --> 00:30:56,559
like this is a cool system

00:30:54,000 --> 00:30:58,640
all right and so related to that there's

00:30:56,559 --> 00:31:01,519
a really good quote from another paper

00:30:58,640 --> 00:31:02,399
that says by far the fastest simulator

00:31:01,519 --> 00:31:04,240
of the cpu

00:31:02,399 --> 00:31:07,600
mmu and memory system of an sj

00:31:04,240 --> 00:31:10,000
multiprocessor is an sgi multiprocessor

00:31:07,600 --> 00:31:11,600
that sounds like a tautology right but

00:31:10,000 --> 00:31:13,039
in other words what this actually says

00:31:11,600 --> 00:31:14,399
is when the source and target

00:31:13,039 --> 00:31:15,760
architecture are the same

00:31:14,399 --> 00:31:17,440
as is the case when the goal is to

00:31:15,760 --> 00:31:18,000
dynamically optimize something right

00:31:17,440 --> 00:31:19,919
then the

00:31:18,000 --> 00:31:21,039
source program can be executed directly

00:31:19,919 --> 00:31:22,799
on the cpu

00:31:21,039 --> 00:31:24,880
right and so remember the slowdowns that

00:31:22,799 --> 00:31:26,720
we quoted in embraer about simulation if

00:31:24,880 --> 00:31:27,440
you simulate on the target architecture

00:31:26,720 --> 00:31:29,120
itself

00:31:27,440 --> 00:31:30,880
right then you can erase many of the

00:31:29,120 --> 00:31:32,240
performance costs and that's that's kind

00:31:30,880 --> 00:31:34,880
of cool right

00:31:32,240 --> 00:31:35,440
all right so i'm going to change topics

00:31:34,880 --> 00:31:37,440
a bit

00:31:35,440 --> 00:31:38,960
i'm going to apologize in advance i'm

00:31:37,440 --> 00:31:41,200
going to use a swear word

00:31:38,960 --> 00:31:41,200
okay

00:31:42,000 --> 00:31:45,679
i'm sorry i said it i said i said it i

00:31:43,840 --> 00:31:46,399
said java at a c process conference i'm

00:31:45,679 --> 00:31:48,720
sorry

00:31:46,399 --> 00:31:49,840
i said it but at least i didn't say ross

00:31:48,720 --> 00:31:51,840
okay

00:31:49,840 --> 00:31:52,880
all right so let's look at java for a

00:31:51,840 --> 00:31:54,399
bit it's pretty good

00:31:52,880 --> 00:31:56,640
it actually does cool stuff so let's

00:31:54,399 --> 00:31:58,720
look at it there's a cool paper called

00:31:56,640 --> 00:32:01,120
compiling java just in time

00:31:58,720 --> 00:32:03,120
right and their pitch was avoiding

00:32:01,120 --> 00:32:05,600
unnecessary overheads is crucial for

00:32:03,120 --> 00:32:08,000
fast compilation

00:32:05,600 --> 00:32:09,679
we'll talk about that in a bit in many

00:32:08,000 --> 00:32:11,919
compilers constructing an intermediate

00:32:09,679 --> 00:32:13,039
representation of a method is a standard

00:32:11,919 --> 00:32:15,360
process

00:32:13,039 --> 00:32:16,960
when compiling from java bytecode

00:32:15,360 --> 00:32:18,720
however we can eliminate

00:32:16,960 --> 00:32:21,200
that overheads the overhead of creating

00:32:18,720 --> 00:32:23,519
an ir but the bytecode themselves

00:32:21,200 --> 00:32:25,360
are in ir because they're primarily

00:32:23,519 --> 00:32:26,320
designed to be compact and to facilitate

00:32:25,360 --> 00:32:27,600
interpretation

00:32:26,320 --> 00:32:30,000
they're not the ideal error for

00:32:27,600 --> 00:32:31,440
compilation but they can easily be used

00:32:30,000 --> 00:32:34,159
for that purpose

00:32:31,440 --> 00:32:36,799
so if you remember back in the 90s early

00:32:34,159 --> 00:32:39,760
java was interpreted and it was slow

00:32:36,799 --> 00:32:40,640
right and so it had a fundamental design

00:32:39,760 --> 00:32:42,480
criteria

00:32:40,640 --> 00:32:43,679
which was java has to be portable and it

00:32:42,480 --> 00:32:45,919
has to be secure

00:32:43,679 --> 00:32:47,039
right and making that means that they

00:32:45,919 --> 00:32:49,440
designed an ir

00:32:47,039 --> 00:32:50,960
that was a byte code that had those

00:32:49,440 --> 00:32:52,880
fundamental properties

00:32:50,960 --> 00:32:54,559
and creating a compiler for that at the

00:32:52,880 --> 00:32:56,080
time was kind of tricky so it took a

00:32:54,559 --> 00:32:56,559
while but an interpreter was kind of

00:32:56,080 --> 00:33:00,000
easy

00:32:56,559 --> 00:33:02,080
but it was slow right so the key thing

00:33:00,000 --> 00:33:02,880
here is the byte code in java can't be

00:33:02,080 --> 00:33:04,320
trusted

00:33:02,880 --> 00:33:05,760
because it has to be secure you don't

00:33:04,320 --> 00:33:07,519
trust the bico you check a lot of

00:33:05,760 --> 00:33:08,159
invariants at runtime when you interpret

00:33:07,519 --> 00:33:10,080
it

00:33:08,159 --> 00:33:11,679
now realistically most most bytecode is

00:33:10,080 --> 00:33:13,279
not trying to mess with the program

00:33:11,679 --> 00:33:14,799
right and so you could actually do away

00:33:13,279 --> 00:33:16,159
with a lot of those things but they're

00:33:14,799 --> 00:33:17,679
dynamic properties you still have to

00:33:16,159 --> 00:33:18,720
check right like are you accessing or

00:33:17,679 --> 00:33:20,159
out of bounds and stuff like that you

00:33:18,720 --> 00:33:23,519
have to check all those things

00:33:20,159 --> 00:33:25,120
um and classes in java can be loaded

00:33:23,519 --> 00:33:26,799
runtime and that complicates things a

00:33:25,120 --> 00:33:28,640
lot right so it means that like

00:33:26,799 --> 00:33:30,720
everything in java is virtual basically

00:33:28,640 --> 00:33:31,440
right like it's as if all the methods

00:33:30,720 --> 00:33:33,360
have virtual

00:33:31,440 --> 00:33:35,120
like kind of inputs in them but if you

00:33:33,360 --> 00:33:37,440
can load classes runtime it means the

00:33:35,120 --> 00:33:38,799
dynamic dispatch is nearly impossible to

00:33:37,440 --> 00:33:40,640
determine statically

00:33:38,799 --> 00:33:42,159
right so there's all these kind of

00:33:40,640 --> 00:33:43,600
things in java that made it hard to

00:33:42,159 --> 00:33:46,320
optimize

00:33:43,600 --> 00:33:46,720
right and then what this paper talks

00:33:46,320 --> 00:33:50,240
about

00:33:46,720 --> 00:33:53,440
is is kind of saying well it's really

00:33:50,240 --> 00:33:55,200
key to a jits design which ir you use

00:33:53,440 --> 00:33:56,960
here they said we're not going to use an

00:33:55,200 --> 00:33:58,720
iron we have bytecode bytecode's good

00:33:56,960 --> 00:34:00,320
enough jitting

00:33:58,720 --> 00:34:02,720
it's fast right you don't need to take

00:34:00,320 --> 00:34:03,600
the bytecode create an ir and then do

00:34:02,720 --> 00:34:06,080
stuff with it

00:34:03,600 --> 00:34:08,000
right that's an interesting fact right

00:34:06,080 --> 00:34:09,760
like if you want to do stuff fast

00:34:08,000 --> 00:34:11,440
skip steps in a gym right if you don't

00:34:09,760 --> 00:34:14,000
need that step don't do it

00:34:11,440 --> 00:34:15,839
and this doing exactly what they did has

00:34:14,000 --> 00:34:17,359
deep effect on which optimizations

00:34:15,839 --> 00:34:18,960
are feasible that's really really

00:34:17,359 --> 00:34:21,280
important in understanding

00:34:18,960 --> 00:34:22,240
how jits designed their ir because when

00:34:21,280 --> 00:34:24,320
you design an ir

00:34:22,240 --> 00:34:26,480
you're trying to enable semantics as

00:34:24,320 --> 00:34:27,919
well as enable particular optimizations

00:34:26,480 --> 00:34:29,599
and there's a trade-off with speed and a

00:34:27,919 --> 00:34:31,440
bunch of other stuff there so in

00:34:29,599 --> 00:34:33,919
particular some information is lost when

00:34:31,440 --> 00:34:35,520
you translate from original source to ir

00:34:33,919 --> 00:34:37,919
in a lot of cases

00:34:35,520 --> 00:34:39,839
and also some irs have easier to analyze

00:34:37,919 --> 00:34:42,079
structures so for example

00:34:39,839 --> 00:34:43,440
you might have an ir which makes it

00:34:42,079 --> 00:34:45,040
super easy to say

00:34:43,440 --> 00:34:46,800
i have an instruction here creates a

00:34:45,040 --> 00:34:48,320
result who uses that result

00:34:46,800 --> 00:34:50,079
right in this function that's really

00:34:48,320 --> 00:34:50,720
easy to do say in ssa or something like

00:34:50,079 --> 00:34:52,240
that

00:34:50,720 --> 00:34:54,320
um another thing is if you have a nice

00:34:52,240 --> 00:34:54,960
control flow graph it's really easy to

00:34:54,320 --> 00:34:57,280
say like

00:34:54,960 --> 00:34:58,880
okay i'm here in the function how can i

00:34:57,280 --> 00:34:59,200
get there right you can say like i'm at

00:34:58,880 --> 00:35:00,560
this

00:34:59,200 --> 00:35:01,760
can this point in the program get there

00:35:00,560 --> 00:35:02,640
well if you have a control small graph

00:35:01,760 --> 00:35:04,240
it's kind of easy

00:35:02,640 --> 00:35:05,760
if you don't have a control flow graph

00:35:04,240 --> 00:35:08,079
it's much harder right so

00:35:05,760 --> 00:35:09,280
ir design is really key to jits and a

00:35:08,079 --> 00:35:11,520
lot of the java

00:35:09,280 --> 00:35:13,280
publications talk about ir design either

00:35:11,520 --> 00:35:16,320
saying we just have byte code whatever

00:35:13,280 --> 00:35:17,440
or by having really complicated items

00:35:16,320 --> 00:35:19,520
all right so let's look at another

00:35:17,440 --> 00:35:22,000
example of the later java

00:35:19,520 --> 00:35:23,760
uh jit this one's called hotspot i think

00:35:22,000 --> 00:35:26,079
you've all heard about it from 2006

00:35:23,760 --> 00:35:27,040
and this this paper is is about the

00:35:26,079 --> 00:35:29,760
design of

00:35:27,040 --> 00:35:31,200
hotspot client compiled for 6. and it's

00:35:29,760 --> 00:35:32,560
a bit complicated because like

00:35:31,200 --> 00:35:33,760
there's like the client and the server

00:35:32,560 --> 00:35:35,040
compiling a bunch of other stuff let's

00:35:33,760 --> 00:35:38,240
ignore that for now

00:35:35,040 --> 00:35:40,160
uh the paper itself talks about

00:35:38,240 --> 00:35:42,160
many many interesting aspects of the gym

00:35:40,160 --> 00:35:43,440
right so it has novel contributions to

00:35:42,160 --> 00:35:46,800
the field of jitting

00:35:43,440 --> 00:35:48,400
but his description of prior art is also

00:35:46,800 --> 00:35:50,240
super good right this is a really good

00:35:48,400 --> 00:35:52,079
great paper because it looks at

00:35:50,240 --> 00:35:53,680
stuff other people have done and it puts

00:35:52,079 --> 00:35:55,200
it in a hot spot explains how

00:35:53,680 --> 00:35:56,800
everything's integrated together

00:35:55,200 --> 00:35:58,800
right so even though the thing itself

00:35:56,800 --> 00:36:00,240
might not be novel how it integrates

00:35:58,800 --> 00:36:01,839
with a big system right like if you have

00:36:00,240 --> 00:36:04,400
a one trick pony paper

00:36:01,839 --> 00:36:06,000
uh this thing it's one trick is putting

00:36:04,400 --> 00:36:08,400
all the tricks into one trick

00:36:06,000 --> 00:36:09,680
right so that's cool uh so it refers

00:36:08,400 --> 00:36:10,960
back to publications that originally

00:36:09,680 --> 00:36:11,920
pioneered stuff so it's kind of cool

00:36:10,960 --> 00:36:14,079
there

00:36:11,920 --> 00:36:15,680
so the paper itself also also does other

00:36:14,079 --> 00:36:17,440
stuff it talks about like a fast

00:36:15,680 --> 00:36:18,160
algorithm for scale analysis which was

00:36:17,440 --> 00:36:19,599
novel

00:36:18,160 --> 00:36:21,359
automatic object inlining which was

00:36:19,599 --> 00:36:22,560
novel rebounds checking elimination

00:36:21,359 --> 00:36:25,119
which was novel

00:36:22,560 --> 00:36:26,880
um but where hotspot is really really

00:36:25,119 --> 00:36:28,640
cool is when everything comes together

00:36:26,880 --> 00:36:29,119
in a pretty complex just in runtime that

00:36:28,640 --> 00:36:30,880
has

00:36:29,119 --> 00:36:32,720
good performance right so let's look at

00:36:30,880 --> 00:36:34,640
interesting bits that it points out

00:36:32,720 --> 00:36:36,079
that aren't quite novel and i'll talk

00:36:34,640 --> 00:36:38,079
about that so

00:36:36,079 --> 00:36:39,920
it says if a method contains a

00:36:38,079 --> 00:36:41,839
long-running loop it may be compiled

00:36:39,920 --> 00:36:44,400
regardless of its identification

00:36:41,839 --> 00:36:45,920
frequency the fiend counts a number of

00:36:44,400 --> 00:36:48,320
backwards branches whereas when you

00:36:45,920 --> 00:36:49,760
go to the top of the loop and when a

00:36:48,320 --> 00:36:51,440
threshold is reached it suspends

00:36:49,760 --> 00:36:52,560
interpretation and compiles the running

00:36:51,440 --> 00:36:54,400
method

00:36:52,560 --> 00:36:56,079
right a new stack frame for the native

00:36:54,400 --> 00:36:57,440
method is set up and initialized to

00:36:56,079 --> 00:36:59,680
match the interpreter

00:36:57,440 --> 00:37:01,520
stack frame that's the key point an

00:36:59,680 --> 00:37:02,560
execution of the method then continues

00:37:01,520 --> 00:37:05,520
using the machine code

00:37:02,560 --> 00:37:07,520
of the native method switching from

00:37:05,520 --> 00:37:08,960
interpreted to compiled code in the

00:37:07,520 --> 00:37:11,119
middle of a running method

00:37:08,960 --> 00:37:12,560
is called on stack replacement so most

00:37:11,119 --> 00:37:13,520
of the time when we think of just

00:37:12,560 --> 00:37:16,400
jumping

00:37:13,520 --> 00:37:17,920
uh to code it's the entry of a function

00:37:16,400 --> 00:37:19,680
right in most cases

00:37:17,920 --> 00:37:21,200
uh you have a function it has a single

00:37:19,680 --> 00:37:22,240
entry then you might have a switch that

00:37:21,200 --> 00:37:22,960
goes into different parts of the

00:37:22,240 --> 00:37:24,640
function

00:37:22,960 --> 00:37:26,160
but if you're stuck in a hot glue right

00:37:24,640 --> 00:37:27,760
imagine you're in a really hot loop and

00:37:26,160 --> 00:37:29,280
that hot loop calls other things goes

00:37:27,760 --> 00:37:30,480
back all the things goes back

00:37:29,280 --> 00:37:31,920
you can't go to the start of that

00:37:30,480 --> 00:37:34,160
function you're in the hot loop already

00:37:31,920 --> 00:37:35,440
right you've decided let's look if this

00:37:34,160 --> 00:37:36,720
function is hot and then you never get

00:37:35,440 --> 00:37:38,240
out of the function so it's really hot

00:37:36,720 --> 00:37:40,960
but you're never getting out of it

00:37:38,240 --> 00:37:42,720
uh so this is where ysr comes in so osr

00:37:40,960 --> 00:37:43,040
is effectively turning a function inside

00:37:42,720 --> 00:37:44,800
out

00:37:43,040 --> 00:37:46,240
you're in that hot loop right or in

00:37:44,800 --> 00:37:47,359
multiple hot loops in the same function

00:37:46,240 --> 00:37:49,359
jumping back and forth

00:37:47,359 --> 00:37:50,560
and you kind of turn the function inside

00:37:49,359 --> 00:37:52,400
out allowing you to jump

00:37:50,560 --> 00:37:54,079
into the say top of the loop or

00:37:52,400 --> 00:37:54,880
something like this that's really cool

00:37:54,079 --> 00:37:57,040
right so then

00:37:54,880 --> 00:37:58,400
and what's cool is the interpreter has a

00:37:57,040 --> 00:38:01,920
state right which is pretty

00:37:58,400 --> 00:38:03,440
big and and osr in the jit matches the

00:38:01,920 --> 00:38:04,160
state of the interpreter so you can jump

00:38:03,440 --> 00:38:06,560
back and forth

00:38:04,160 --> 00:38:08,240
from that entry point either running the

00:38:06,560 --> 00:38:10,400
jit or the interpreter

00:38:08,240 --> 00:38:11,760
right so and it preserves all the

00:38:10,400 --> 00:38:12,880
semantics and that's a really key

00:38:11,760 --> 00:38:16,240
optimization there

00:38:12,880 --> 00:38:18,079
let's look at another one the compiler

00:38:16,240 --> 00:38:19,760
creates debugging information that maps

00:38:18,079 --> 00:38:21,440
the state of the compile method back to

00:38:19,760 --> 00:38:23,440
the state of the interpreter

00:38:21,440 --> 00:38:25,280
we talked about earlier this enables

00:38:23,440 --> 00:38:26,240
aggressive compiler optimizations

00:38:25,280 --> 00:38:29,280
because the vm

00:38:26,240 --> 00:38:30,640
can de-optimize back to a safe state

00:38:29,280 --> 00:38:34,079
where the assumptions under which an

00:38:30,640 --> 00:38:35,040
optimization was performed are invalid

00:38:34,079 --> 00:38:36,560
so garbage collection and

00:38:35,040 --> 00:38:37,920
de-optimization are allowed to occur

00:38:36,560 --> 00:38:40,160
only at some discrete

00:38:37,920 --> 00:38:42,240
points in the program called safe points

00:38:40,160 --> 00:38:43,760
such as backwards branches method calls

00:38:42,240 --> 00:38:45,359
return instructions and operations when

00:38:43,760 --> 00:38:47,839
they throw an exception

00:38:45,359 --> 00:38:49,280
all right so first i've always found

00:38:47,839 --> 00:38:51,040
that the concept of an optimizer

00:38:49,280 --> 00:38:54,720
de-optimizing isn't losing

00:38:51,040 --> 00:38:55,119
it's funny but here's a concrete example

00:38:54,720 --> 00:38:57,839
of

00:38:55,119 --> 00:38:59,119
the optimization uh when i was saying

00:38:57,839 --> 00:39:00,400
earlier java can load classes

00:38:59,119 --> 00:39:01,119
dynamically and you can do that through

00:39:00,400 --> 00:39:03,200
the network

00:39:01,119 --> 00:39:04,160
you launch a lot of java program you go

00:39:03,200 --> 00:39:06,320
get a

00:39:04,160 --> 00:39:07,599
thing like over the internet you load it

00:39:06,320 --> 00:39:09,440
and you execute on your program

00:39:07,599 --> 00:39:11,119
so statically you can't determine any of

00:39:09,440 --> 00:39:12,720
that right so some functions

00:39:11,119 --> 00:39:14,160
might have been inlined right because

00:39:12,720 --> 00:39:16,079
you're saying well like

00:39:14,160 --> 00:39:17,440
everything's virtual but i want speed so

00:39:16,079 --> 00:39:20,160
i'm going to inline some stuff

00:39:17,440 --> 00:39:21,280
right so it looks like this is final i

00:39:20,160 --> 00:39:22,320
don't see any of the versions of this

00:39:21,280 --> 00:39:22,800
method i'm just going to outline

00:39:22,320 --> 00:39:24,560
everything

00:39:22,800 --> 00:39:25,920
and then the programmer who's kind of a

00:39:24,560 --> 00:39:26,800
jerk comes in and loads stuff through

00:39:25,920 --> 00:39:28,640
the network

00:39:26,800 --> 00:39:30,240
some the the hard drive or something

00:39:28,640 --> 00:39:31,520
that's kind of a jerk move the compiler

00:39:30,240 --> 00:39:32,960
has to see that and it has to

00:39:31,520 --> 00:39:35,359
de-optimize it

00:39:32,960 --> 00:39:36,160
right but that really doesn't happen

00:39:35,359 --> 00:39:37,839
that often

00:39:36,160 --> 00:39:39,280
right so do you want to say i'm never

00:39:37,839 --> 00:39:41,040
going to learn anything in case the

00:39:39,280 --> 00:39:42,400
program is a jerk but you want to say

00:39:41,040 --> 00:39:44,240
i'll trust that you're not a jerk but

00:39:42,400 --> 00:39:45,280
i'll verify right that's that's what

00:39:44,240 --> 00:39:47,599
this does right

00:39:45,280 --> 00:39:49,119
so de-optimization kicks the executable

00:39:47,599 --> 00:39:50,560
back to the interpreter right so you

00:39:49,119 --> 00:39:52,640
have to super optimize thing

00:39:50,560 --> 00:39:54,880
the optimizer says whoa hold the horses

00:39:52,640 --> 00:39:56,880
something i don't like happened

00:39:54,880 --> 00:39:58,480
kick you back to interpret it right but

00:39:56,880 --> 00:39:59,040
in some cases you might actually have

00:39:58,480 --> 00:40:01,839
multiple

00:39:59,040 --> 00:40:03,200
tiers of compilation right so you don't

00:40:01,839 --> 00:40:04,240
just need an interpreter and

00:40:03,200 --> 00:40:06,400
code in some cases you have an

00:40:04,240 --> 00:40:08,079
interpreter you have like that

00:40:06,400 --> 00:40:10,400
fast dish pretty good optimizer and then

00:40:08,079 --> 00:40:12,000
like a really fast but like really good

00:40:10,400 --> 00:40:12,960
option a really slow but really good

00:40:12,000 --> 00:40:15,200
optimization right

00:40:12,960 --> 00:40:17,119
so you might actually like go from the

00:40:15,200 --> 00:40:19,359
interpreter to tier one to tier two

00:40:17,119 --> 00:40:20,319
back to tier one back to two two back to

00:40:19,359 --> 00:40:22,720
the interpreter like

00:40:20,319 --> 00:40:24,000
you can do that in g right obviously if

00:40:22,720 --> 00:40:25,359
you jump back and forth you're just

00:40:24,000 --> 00:40:26,480
spending your time optimizing stuff and

00:40:25,359 --> 00:40:27,440
jumping back and forth not doing

00:40:26,480 --> 00:40:29,760
anything useful

00:40:27,440 --> 00:40:31,599
but the key to being able to do that

00:40:29,760 --> 00:40:33,040
right is having this kind of debug

00:40:31,599 --> 00:40:34,319
information on the side and having

00:40:33,040 --> 00:40:35,839
specific points where you can

00:40:34,319 --> 00:40:37,280
de-optimize and it happens

00:40:35,839 --> 00:40:38,800
that it's also good for the garbage

00:40:37,280 --> 00:40:39,680
collection right because the state is

00:40:38,800 --> 00:40:41,599
really well known

00:40:39,680 --> 00:40:43,280
the chip can do a bunch of fancy stuff

00:40:41,599 --> 00:40:45,680
but then when the gc comes in

00:40:43,280 --> 00:40:46,720
it has a kind of like nice state of

00:40:45,680 --> 00:40:49,040
things to look at

00:40:46,720 --> 00:40:50,640
so that's a really really key part of

00:40:49,040 --> 00:40:53,680
certain vms

00:40:50,640 --> 00:40:55,200
all right so java brought the term

00:40:53,680 --> 00:40:57,200
just in time into common use in

00:40:55,200 --> 00:40:58,720
computing literature so that's gosling

00:40:57,200 --> 00:41:00,480
looking back at where things came from

00:40:58,720 --> 00:41:02,240
so thank you java for the good stuff you

00:41:00,480 --> 00:41:04,960
did and also thank you toyota for the

00:41:02,240 --> 00:41:07,839
borrowed word just in time

00:41:04,960 --> 00:41:09,760
okay stop talking about java all right

00:41:07,839 --> 00:41:11,119
let's look at another cool paper called

00:41:09,760 --> 00:41:13,200
and i'm not sure how to say it but i'll

00:41:11,119 --> 00:41:15,680
just say it fx bang

00:41:13,200 --> 00:41:16,880
32 i'm going to say fx bank 32. it's

00:41:15,680 --> 00:41:18,560
cooler than fx32

00:41:16,880 --> 00:41:20,319
so it's a profile guided binary

00:41:18,560 --> 00:41:23,280
translator from 1990

00:41:20,319 --> 00:41:23,760
now it starts with really really really

00:41:23,280 --> 00:41:26,160
big

00:41:23,760 --> 00:41:27,839
big moves here because digital alphas

00:41:26,160 --> 00:41:30,560
architecture provides the world's

00:41:27,839 --> 00:41:32,079
fastest processor many applications

00:41:30,560 --> 00:41:33,920
especially those requiring high

00:41:32,079 --> 00:41:34,400
processor performance has been ported to

00:41:33,920 --> 00:41:37,599
it

00:41:34,400 --> 00:41:40,560
cool however however

00:41:37,599 --> 00:41:42,160
and here i added the frownies however

00:41:40,560 --> 00:41:45,520
many other applications are available

00:41:42,160 --> 00:41:48,800
only under the x86 architecture right

00:41:45,520 --> 00:41:52,079
that's a bummer right like my alpha code

00:41:48,800 --> 00:41:54,240
okay but like x86 yeah okay but

00:41:52,079 --> 00:41:55,920
they designed digital fx bank 32 to make

00:41:54,240 --> 00:41:58,319
the complete set of applications both

00:41:55,920 --> 00:42:00,240
native and x86 available to alpha

00:41:58,319 --> 00:42:02,880
the goal for the software is to provide

00:42:00,240 --> 00:42:04,400
fast and transparent execution of x86

00:42:02,880 --> 00:42:07,359
when 32 applications on

00:42:04,400 --> 00:42:08,400
alpha systems fun fact there was windows

00:42:07,359 --> 00:42:11,440
32 on alpha

00:42:08,400 --> 00:42:13,280
did you know fx bank 32 achieves its

00:42:11,440 --> 00:42:13,920
goals by transparently running those

00:42:13,280 --> 00:42:16,000
applications

00:42:13,920 --> 00:42:17,119
at speeds comparable to high performance

00:42:16,000 --> 00:42:18,960
x86 platforms

00:42:17,119 --> 00:42:21,200
that's a cool thing right like they had

00:42:18,960 --> 00:42:25,200
they're running xcd x66

00:42:21,200 --> 00:42:26,720
alpha fast digital fx bank 32

00:42:25,200 --> 00:42:29,200
is a software utility that enables

00:42:26,720 --> 00:42:31,280
xd6132 applications to be run on windows

00:42:29,200 --> 00:42:33,440
and t slash alpha platforms

00:42:31,280 --> 00:42:35,760
once fx bank 32 has been installed

00:42:33,440 --> 00:42:37,680
almost all x86 applications can be run

00:42:35,760 --> 00:42:38,079
on alpha without special commands and

00:42:37,680 --> 00:42:41,359
with

00:42:38,079 --> 00:42:42,880
excellent performance three significant

00:42:41,359 --> 00:42:46,000
innovations of digital

00:42:42,880 --> 00:42:48,160
fx bank 32 include transparent operation

00:42:46,000 --> 00:42:49,280
interfaces native apis and most

00:42:48,160 --> 00:42:51,520
importantly

00:42:49,280 --> 00:42:53,599
profile directed binary translation so

00:42:51,520 --> 00:42:55,040
by now you know these words you've heard

00:42:53,599 --> 00:42:56,160
them right but they put them together in

00:42:55,040 --> 00:42:57,839
an interesting package

00:42:56,160 --> 00:42:59,359
right their claim is they're the first

00:42:57,839 --> 00:43:00,000
system to exploit this combination of

00:42:59,359 --> 00:43:01,680
emulation

00:43:00,000 --> 00:43:03,760
profile generation and binary

00:43:01,680 --> 00:43:05,599
translation now what's really cool about

00:43:03,760 --> 00:43:07,680
this is that uh applications run

00:43:05,599 --> 00:43:09,200
unmodified on a different architecture

00:43:07,680 --> 00:43:10,560
and effectively a different operating

00:43:09,200 --> 00:43:11,599
system if you know it's still windows

00:43:10,560 --> 00:43:12,480
the operating system is actually

00:43:11,599 --> 00:43:13,839
different

00:43:12,480 --> 00:43:15,760
and this makes transitioning

00:43:13,839 --> 00:43:17,520
architectures much easier especially if

00:43:15,760 --> 00:43:18,960
the destination is more powerful which

00:43:17,520 --> 00:43:21,280
is the claim here right they say alpha

00:43:18,960 --> 00:43:23,599
is the fastest processor in the world

00:43:21,280 --> 00:43:25,119
and like yeah at the time was pretty

00:43:23,599 --> 00:43:28,400
exciting

00:43:25,119 --> 00:43:28,880
so we're one to launch a system like

00:43:28,400 --> 00:43:32,079
this

00:43:28,880 --> 00:43:33,599
nowadays it would be a huge deal

00:43:32,079 --> 00:43:35,119
right like marketing would be involved

00:43:33,599 --> 00:43:35,520
they would talk about it they would have

00:43:35,119 --> 00:43:37,280
like

00:43:35,520 --> 00:43:38,720
key names for that product and things

00:43:37,280 --> 00:43:40,480
like that right like

00:43:38,720 --> 00:43:41,839
if if ever anyone launches something

00:43:40,480 --> 00:43:42,240
like this a new architecture and they

00:43:41,839 --> 00:43:44,079
port

00:43:42,240 --> 00:43:46,160
old code say from x86 their new

00:43:44,079 --> 00:43:46,560
architecture they make a big deal out of

00:43:46,160 --> 00:43:49,119
it

00:43:46,560 --> 00:43:50,480
right and this in 98 was quite a big

00:43:49,119 --> 00:43:52,880
deal it was really cool

00:43:50,480 --> 00:43:54,240
i still think it's really cool i still

00:43:52,880 --> 00:43:55,520
don't know how to say it though fx bank

00:43:54,240 --> 00:43:57,839
32 sounds weird

00:43:55,520 --> 00:43:58,720
okay let's get back to alice right alice

00:43:57,839 --> 00:44:00,480
has really

00:43:58,720 --> 00:44:03,280
really good insights into gyms so let's

00:44:00,480 --> 00:44:03,280
look at what she says

00:44:03,599 --> 00:44:06,839
why sometimes i've believed as many as

00:44:06,400 --> 00:44:10,640
six

00:44:06,839 --> 00:44:12,560
impossible things before breakfast right

00:44:10,640 --> 00:44:13,760
now lewis carroll really knew his jits

00:44:12,560 --> 00:44:16,319
right and here

00:44:13,760 --> 00:44:18,240
he's actually talking about speculation

00:44:16,319 --> 00:44:18,560
right so we talked about speculation a

00:44:18,240 --> 00:44:20,480
bit

00:44:18,560 --> 00:44:22,000
in the context of hotspot or we say the

00:44:20,480 --> 00:44:22,560
program is not a jerky they're not going

00:44:22,000 --> 00:44:24,079
to load

00:44:22,560 --> 00:44:25,359
like something from network a class and

00:44:24,079 --> 00:44:26,319
then i'm going to have to de-optimize

00:44:25,359 --> 00:44:27,920
right but

00:44:26,319 --> 00:44:29,599
there's a lot more speculation that you

00:44:27,920 --> 00:44:32,000
can do in jits so let's look

00:44:29,599 --> 00:44:33,119
at some deep speculation and much much

00:44:32,000 --> 00:44:36,240
cooler emulation

00:44:33,119 --> 00:44:38,400
right this is really cool

00:44:36,240 --> 00:44:40,160
transmeta the technology behind crusoe

00:44:38,400 --> 00:44:42,079
processor from 2000

00:44:40,160 --> 00:44:43,680
so their tagline the start of the white

00:44:42,079 --> 00:44:45,040
paper this isn't really an academic

00:44:43,680 --> 00:44:46,480
publication it's a whitebeard that

00:44:45,040 --> 00:44:48,880
exposes the architecture

00:44:46,480 --> 00:44:49,680
it's a low power x86 compatible

00:44:48,880 --> 00:44:51,680
processor

00:44:49,680 --> 00:44:52,880
implement it with and wait for it if you

00:44:51,680 --> 00:44:54,079
know what's coming just don't spoil it

00:44:52,880 --> 00:44:55,599
for the other people in the room it's

00:44:54,079 --> 00:44:58,720
implemented with

00:44:55,599 --> 00:45:02,160
code morphing software

00:44:58,720 --> 00:45:03,040
wow the new technology is fundamentally

00:45:02,160 --> 00:45:05,200
software based

00:45:03,040 --> 00:45:06,640
the power savings come from replacing a

00:45:05,200 --> 00:45:09,680
large number of transistors

00:45:06,640 --> 00:45:11,599
with software right so this

00:45:09,680 --> 00:45:13,119
transmeta was the hottest startup at the

00:45:11,599 --> 00:45:15,440
end of the 90s right so for those of you

00:45:13,119 --> 00:45:17,359
who remember slashdot

00:45:15,440 --> 00:45:18,560
slashdot was a buzz with this right it

00:45:17,359 --> 00:45:20,640
was really cool stuff and

00:45:18,560 --> 00:45:21,920
code morphing sorry to spoil it code

00:45:20,640 --> 00:45:23,359
morphing is just fancy where it's a

00:45:21,920 --> 00:45:25,440
dynamic binary translation

00:45:23,359 --> 00:45:27,280
right but but this is actually really

00:45:25,440 --> 00:45:28,079
cool imagine this is hardware that

00:45:27,280 --> 00:45:31,280
presents

00:45:28,079 --> 00:45:33,200
x86 but it's not actually x86 in fact

00:45:31,280 --> 00:45:34,880
they didn't have a license for x86 but

00:45:33,200 --> 00:45:37,680
they just simulated

00:45:34,880 --> 00:45:39,680
it's also hardware that can get faster

00:45:37,680 --> 00:45:41,599
through firmware updates which is either

00:45:39,680 --> 00:45:44,319
terrifying or amazingly cool

00:45:41,599 --> 00:45:44,720
it can be both at the same time actually

00:45:44,319 --> 00:45:47,839
and

00:45:44,720 --> 00:45:49,839
it's a stable seeming iso right x86

00:45:47,839 --> 00:45:51,760
with hardware that can radically change

00:45:49,839 --> 00:45:52,640
at each generation right and it did

00:45:51,760 --> 00:45:54,560
radically change

00:45:52,640 --> 00:45:55,760
through its both generations of

00:45:54,560 --> 00:45:56,960
transmitters processors

00:45:55,760 --> 00:45:58,160
and the third one that they never

00:45:56,960 --> 00:45:59,599
launched right but it had radical

00:45:58,160 --> 00:46:01,359
changes it's really cool

00:45:59,599 --> 00:46:02,720
all right so now i'm going to apologize

00:46:01,359 --> 00:46:04,240
again for the wall of text but it's a

00:46:02,720 --> 00:46:05,200
really exciting wall of tests let's look

00:46:04,240 --> 00:46:07,200
at it

00:46:05,200 --> 00:46:08,960
the vliw's native instruction said

00:46:07,200 --> 00:46:10,079
there's no resemblance the x86

00:46:08,960 --> 00:46:12,480
instruction said

00:46:10,079 --> 00:46:13,200
cool it has been designed purely for

00:46:12,480 --> 00:46:16,640
fast

00:46:13,200 --> 00:46:18,640
low power implementation the surrounding

00:46:16,640 --> 00:46:20,480
software layer gives x86 programs the

00:46:18,640 --> 00:46:22,480
impression that they're running on x86

00:46:20,480 --> 00:46:24,400
hardware

00:46:22,480 --> 00:46:26,400
the software layer is called code

00:46:24,400 --> 00:46:29,119
morphing software because it dynamically

00:46:26,400 --> 00:46:30,640
morphs x86 instructions into vliw

00:46:29,119 --> 00:46:32,240
instructions

00:46:30,640 --> 00:46:34,000
the code morphing software includes a

00:46:32,240 --> 00:46:36,960
number of advanced features to achieve

00:46:34,000 --> 00:46:38,640
good system level performance code

00:46:36,960 --> 00:46:40,240
morphing support facilities are also

00:46:38,640 --> 00:46:41,359
built into the underlying cpu and this

00:46:40,240 --> 00:46:43,200
sentence

00:46:41,359 --> 00:46:44,960
says a lot right this is really

00:46:43,200 --> 00:46:46,720
important

00:46:44,960 --> 00:46:48,960
in other words the transmeta designers

00:46:46,720 --> 00:46:49,760
have judiciously rendered some functions

00:46:48,960 --> 00:46:51,359
in hardware

00:46:49,760 --> 00:46:54,319
some in software according to the

00:46:51,359 --> 00:46:56,400
product design goals and constraints

00:46:54,319 --> 00:46:57,760
last in the wall of text different goals

00:46:56,400 --> 00:46:59,040
and constraints in future products may

00:46:57,760 --> 00:47:00,079
result in different hardware software

00:46:59,040 --> 00:47:03,200
partitioning

00:47:00,079 --> 00:47:05,200
so let's impact this vliw is

00:47:03,200 --> 00:47:06,400
means very long instruction board so on

00:47:05,200 --> 00:47:08,560
crusoe each

00:47:06,400 --> 00:47:10,560
actual instruction is really four

00:47:08,560 --> 00:47:13,040
different simple instructions

00:47:10,560 --> 00:47:15,520
right the hardware executes vliw what

00:47:13,040 --> 00:47:17,760
they call molecules in order so this is

00:47:15,520 --> 00:47:20,640
not a superscalar processor right it

00:47:17,760 --> 00:47:23,920
takes each molecule and executes them

00:47:20,640 --> 00:47:25,359
one at a time in order but that's four

00:47:23,920 --> 00:47:26,880
instructions at a time so there's some

00:47:25,359 --> 00:47:28,240
amount of stuff happening at the same

00:47:26,880 --> 00:47:29,839
time here

00:47:28,240 --> 00:47:31,520
the other thing is this is a binary

00:47:29,839 --> 00:47:34,319
translator so it's

00:47:31,520 --> 00:47:34,720
the instructions in vliw are executed in

00:47:34,319 --> 00:47:36,480
order

00:47:34,720 --> 00:47:38,720
but it's actually statically out of

00:47:36,480 --> 00:47:39,280
order from what the original x86 program

00:47:38,720 --> 00:47:42,240
contained

00:47:39,280 --> 00:47:44,480
because of binary translation right and

00:47:42,240 --> 00:47:46,559
so it means like the the translator

00:47:44,480 --> 00:47:48,079
moves stuff around and then executes an

00:47:46,559 --> 00:47:49,680
order right so you don't have

00:47:48,079 --> 00:47:51,280
all that super scalar hardware that

00:47:49,680 --> 00:47:52,640
tries to reorder stuff it has a ton of

00:47:51,280 --> 00:47:54,319
shadow registers and other things and

00:47:52,640 --> 00:47:57,520
renaming and things

00:47:54,319 --> 00:47:58,800
uh but but it's able to still do out of

00:47:57,520 --> 00:48:02,160
order things

00:47:58,800 --> 00:48:04,160
right so the hardware itself right

00:48:02,160 --> 00:48:05,920
supports transactional memory in

00:48:04,160 --> 00:48:06,880
hardware to enable speculative

00:48:05,920 --> 00:48:09,359
optimizations

00:48:06,880 --> 00:48:10,880
including a speculative reordering of

00:48:09,359 --> 00:48:12,960
loads and stores

00:48:10,880 --> 00:48:15,280
and x86 instructions are initially

00:48:12,960 --> 00:48:18,400
interpreted and if they're deemed hot

00:48:15,280 --> 00:48:20,160
the jit inside the cpu

00:48:18,400 --> 00:48:22,079
compiles them to a hidden part of the

00:48:20,160 --> 00:48:23,440
hardware so this is effectively running

00:48:22,079 --> 00:48:26,319
at ring minus one right

00:48:23,440 --> 00:48:28,000
the x86 code can't access the jit can't

00:48:26,319 --> 00:48:29,920
access the memory to which stuff is

00:48:28,000 --> 00:48:31,920
gym so i don't know about you but this

00:48:29,920 --> 00:48:33,119
wall of text gets me pretty excited

00:48:31,920 --> 00:48:34,960
right now at this point you're going to

00:48:33,119 --> 00:48:35,760
say well nothing can really impress me

00:48:34,960 --> 00:48:38,319
anymore well

00:48:35,760 --> 00:48:39,359
let's look at qmu right jmu is a fast

00:48:38,319 --> 00:48:41,119
and portable dynamic

00:48:39,359 --> 00:48:42,880
translator and they present the internal

00:48:41,119 --> 00:48:43,359
sequence we have a fast machine emulator

00:48:42,880 --> 00:48:46,559
using an

00:48:43,359 --> 00:48:49,680
original portable dynamic transmitter

00:48:46,559 --> 00:48:50,800
it emulates several cpus x86 power pc

00:48:49,680 --> 00:48:53,359
armaments parked on several

00:48:50,800 --> 00:48:54,240
hosts x86 power pc arm spark alpha and

00:48:53,359 --> 00:48:56,880
mips

00:48:54,240 --> 00:48:58,960
qmu supports full system emulation which

00:48:56,880 --> 00:49:00,800
a complete and unmodified operating

00:48:58,960 --> 00:49:02,800
system is run in a virtual machine and

00:49:00,800 --> 00:49:04,880
linux user mode emulation as well

00:49:02,800 --> 00:49:06,400
where a linux process just the process

00:49:04,880 --> 00:49:08,240
itself

00:49:06,400 --> 00:49:09,839
compiles for one target you can run on

00:49:08,240 --> 00:49:10,240
another one so you can emulate the whole

00:49:09,839 --> 00:49:12,000
machine

00:49:10,240 --> 00:49:13,920
or you can emulate just one program in

00:49:12,000 --> 00:49:15,920
that whole machine all right

00:49:13,920 --> 00:49:17,680
so qf sounds less granules are

00:49:15,920 --> 00:49:20,000
transmitted but it's a one-man

00:49:17,680 --> 00:49:20,960
uh uh a project initially it's grown a

00:49:20,000 --> 00:49:22,160
lot since then

00:49:20,960 --> 00:49:23,920
and it's really impressive that one

00:49:22,160 --> 00:49:25,520
person wrote all of that original thing

00:49:23,920 --> 00:49:26,880
the paper itself was really tiny really

00:49:25,520 --> 00:49:29,280
cool to read

00:49:26,880 --> 00:49:30,160
and train is super malleable right so it

00:49:29,280 --> 00:49:31,680
does a bunch of interesting

00:49:30,160 --> 00:49:33,520
optimizations it's super portable

00:49:31,680 --> 00:49:34,960
and the paper itself is short but what

00:49:33,520 --> 00:49:36,800
the project does is pretty close to

00:49:34,960 --> 00:49:37,599
magic which is why it's used in a lot of

00:49:36,800 --> 00:49:38,960
places these days

00:49:37,599 --> 00:49:40,480
right so i don't want to go into virtual

00:49:38,960 --> 00:49:41,200
machines and say what vmware and other

00:49:40,480 --> 00:49:42,880
people have done

00:49:41,200 --> 00:49:44,480
but there's a lot of interesting work in

00:49:42,880 --> 00:49:45,520
virtual machines in general right so

00:49:44,480 --> 00:49:48,000
particularly when you use

00:49:45,520 --> 00:49:49,200
special hardware to help virtualization

00:49:48,000 --> 00:49:50,960
uh uh

00:49:49,200 --> 00:49:52,880
like run the guest operating system so

00:49:50,960 --> 00:49:53,920
in particular uh in virtualization

00:49:52,880 --> 00:49:55,520
extensions uh

00:49:53,920 --> 00:49:57,200
you have hardware support to do a lot of

00:49:55,520 --> 00:49:58,720
interesting stuff as well as all the

00:49:57,200 --> 00:49:59,359
power virtualization techniques where

00:49:58,720 --> 00:50:01,920
you tell

00:49:59,359 --> 00:50:03,359
the guests that you know there's a host

00:50:01,920 --> 00:50:04,079
running them and they kind of cooperate

00:50:03,359 --> 00:50:06,079
to do stuff

00:50:04,079 --> 00:50:07,200
that's really cool all right that's

00:50:06,079 --> 00:50:09,040
another thing this one is pretty

00:50:07,200 --> 00:50:10,880
familiar most people valgrind

00:50:09,040 --> 00:50:12,319
it's a framework for heavyweight dynamic

00:50:10,880 --> 00:50:13,839
instrumentation

00:50:12,319 --> 00:50:15,680
and the paper says we focus on

00:50:13,839 --> 00:50:17,359
valgrind's unique support for shadow

00:50:15,680 --> 00:50:18,480
values it's a powerful but previously

00:50:17,359 --> 00:50:20,000
little studied and difficult to

00:50:18,480 --> 00:50:21,599
implement dynamic binary analysis

00:50:20,000 --> 00:50:22,160
technique which requires a tool to

00:50:21,599 --> 00:50:23,760
shadow

00:50:22,160 --> 00:50:25,520
every register and memory value with

00:50:23,760 --> 00:50:27,680
another value of describes it

00:50:25,520 --> 00:50:29,359
all right so what's cool about that is

00:50:27,680 --> 00:50:30,400
it kind of takes what adam does right so

00:50:29,359 --> 00:50:32,559
an instrumenting stuff

00:50:30,400 --> 00:50:34,480
and it goes way beyond in the analysis

00:50:32,559 --> 00:50:36,559
capabilities i didn't just let you add

00:50:34,480 --> 00:50:38,400
function calls and stuff but valgrind

00:50:36,559 --> 00:50:40,960
uses shadow values to track

00:50:38,400 --> 00:50:42,000
facts about registers about memory and

00:50:40,960 --> 00:50:43,520
things like that

00:50:42,000 --> 00:50:45,760
and knows about the guest operating

00:50:43,520 --> 00:50:47,920
system so folks are usually used to

00:50:45,760 --> 00:50:48,880
valgrind as a use after free tool or an

00:50:47,920 --> 00:50:51,440
out of bounds tool

00:50:48,880 --> 00:50:52,240
but that's just one tool that's part of

00:50:51,440 --> 00:50:53,680
the valgrind

00:50:52,240 --> 00:50:55,599
toolbox what's really cool about

00:50:53,680 --> 00:50:57,280
valgrind is is the tooling

00:50:55,599 --> 00:50:58,800
infrastructure around it right the way

00:50:57,280 --> 00:51:00,720
you write a tool is you don't

00:50:58,800 --> 00:51:02,640
really need to know about jits right you

00:51:00,720 --> 00:51:04,160
manipulate a program if you write a

00:51:02,640 --> 00:51:05,280
valgrinds tool without knowing about

00:51:04,160 --> 00:51:06,800
jits it just tells you there's

00:51:05,280 --> 00:51:08,559
programming there's a state

00:51:06,800 --> 00:51:10,000
tell me what to do with stuff right and

00:51:08,559 --> 00:51:11,760
that's what's really cool about valgrind

00:51:10,000 --> 00:51:14,880
and it talks about that in people

00:51:11,760 --> 00:51:16,800
all right here's another one

00:51:14,880 --> 00:51:18,880
javascript trace space just in time type

00:51:16,800 --> 00:51:22,000
supposedly specialization for dynamic

00:51:18,880 --> 00:51:22,800
languages so uh it identifies frequently

00:51:22,000 --> 00:51:24,480
executed

00:51:22,800 --> 00:51:26,480
loops based at runtime and then

00:51:24,480 --> 00:51:28,880
generates machine code on the fly that's

00:51:26,480 --> 00:51:30,000
specialized for the actual dynamic types

00:51:28,880 --> 00:51:33,119
occurring on each path

00:51:30,000 --> 00:51:35,119
through the loop they've implemented a

00:51:33,119 --> 00:51:36,240
dynamic compiler for javascript based on

00:51:35,119 --> 00:51:37,920
their techniques and they've measured

00:51:36,240 --> 00:51:40,079
speed up as 10x or more

00:51:37,920 --> 00:51:41,760
on certain benchmarks now there's a lot

00:51:40,079 --> 00:51:42,720
to say about the evolution compilers in

00:51:41,760 --> 00:51:45,040
the last like

00:51:42,720 --> 00:51:46,160
11 12 years once browsers started

00:51:45,040 --> 00:51:47,520
compiling javascript

00:51:46,160 --> 00:51:50,000
there's a lot to say there's a lot of

00:51:47,520 --> 00:51:51,359
blog posts not as many publications

00:51:50,000 --> 00:51:53,280
but there's many person decades that

00:51:51,359 --> 00:51:55,280
have gone into optimizing javascript and

00:51:53,280 --> 00:51:55,680
your multiple devices are way faster

00:51:55,280 --> 00:51:57,040
because

00:51:55,680 --> 00:51:58,880
or their batteries are much smaller

00:51:57,040 --> 00:52:00,559
that's really good so i'll only mention

00:51:58,880 --> 00:52:02,079
this paper because i think it's cool it

00:52:00,559 --> 00:52:04,400
uses trace compilation

00:52:02,079 --> 00:52:05,920
uh so it it's kind of like what oberon

00:52:04,400 --> 00:52:07,440
did instead of looking at

00:52:05,920 --> 00:52:09,520
a function at a time it just kind of

00:52:07,440 --> 00:52:11,839
traces through the actual execution

00:52:09,520 --> 00:52:12,559
which is cool so pin and transmitter

00:52:11,839 --> 00:52:14,160
also did that

00:52:12,559 --> 00:52:15,839
these are called traces or regions

00:52:14,160 --> 00:52:17,760
depending on what you do exactly

00:52:15,839 --> 00:52:18,880
and so uh it really traces through the

00:52:17,760 --> 00:52:20,559
execution and it

00:52:18,880 --> 00:52:22,160
kind of blows away what a function is

00:52:20,559 --> 00:52:23,920
and so it kind of implicitly does

00:52:22,160 --> 00:52:25,520
inlining as well as tracing through

00:52:23,920 --> 00:52:26,880
types right because if you follow a

00:52:25,520 --> 00:52:28,880
thing as it executes

00:52:26,880 --> 00:52:32,079
the type tends to change along certain

00:52:28,880 --> 00:52:33,359
paths so that's really really cool some

00:52:32,079 --> 00:52:34,800
stuff happened to traces after this

00:52:33,359 --> 00:52:36,319
paper but let's let's just ignore that

00:52:34,800 --> 00:52:39,599
the paper itself is cool

00:52:36,319 --> 00:52:41,280
all right so dynamic or just in time

00:52:39,599 --> 00:52:43,280
compilation is an old implementation

00:52:41,280 --> 00:52:44,640
technique with a fragmented history

00:52:43,280 --> 00:52:46,240
by collecting this historical

00:52:44,640 --> 00:52:47,839
information together we hope to shorten

00:52:46,240 --> 00:52:50,000
the voyage of rediscovery

00:52:47,839 --> 00:52:52,079
right so this completes our brief

00:52:50,000 --> 00:52:54,640
history of jet combines but

00:52:52,079 --> 00:52:55,280
there's one more thing i want to mention

00:52:54,640 --> 00:52:56,319
all right

00:52:55,280 --> 00:52:58,960
what's that one thing that i want to

00:52:56,319 --> 00:53:00,720
mention well security

00:52:58,960 --> 00:53:02,720
all right again let's look at the cat

00:53:00,720 --> 00:53:04,079
the cat only grinned when it saw alice

00:53:02,720 --> 00:53:06,960
that looked good nature

00:53:04,079 --> 00:53:08,800
and she thought still it had very long

00:53:06,960 --> 00:53:10,880
claws and great many teeth

00:53:08,800 --> 00:53:12,400
so she felt that it ought to be treated

00:53:10,880 --> 00:53:14,240
with respect

00:53:12,400 --> 00:53:15,680
and i said i wouldn't go into downsides

00:53:14,240 --> 00:53:17,280
of jit compilation too much but i want

00:53:15,680 --> 00:53:18,720
to just like quickly go through a few

00:53:17,280 --> 00:53:19,280
papers about security because it's kind

00:53:18,720 --> 00:53:20,800
of cool

00:53:19,280 --> 00:53:22,319
and and the reason i want to do that is

00:53:20,800 --> 00:53:23,920
the good news with the jit

00:53:22,319 --> 00:53:25,119
is that you're now shipping a compiler

00:53:23,920 --> 00:53:25,760
you can do all these things it's really

00:53:25,119 --> 00:53:28,559
really cool

00:53:25,760 --> 00:53:29,440
i'm excited about it um the bad news

00:53:28,559 --> 00:53:31,760
about jits

00:53:29,440 --> 00:53:33,599
is you're now shifting a compiler right

00:53:31,760 --> 00:53:36,160
so here's a few more publications

00:53:33,599 --> 00:53:37,680
i'll gloss over them a bit they're cool

00:53:36,160 --> 00:53:39,440
still negative

00:53:37,680 --> 00:53:40,960
area of research right so the first one

00:53:39,440 --> 00:53:42,720
i want to talk about is not actually

00:53:40,960 --> 00:53:44,240
it's native client it's a sandbox for

00:53:42,720 --> 00:53:46,480
portable untrusted x36

00:53:44,240 --> 00:53:48,160
native code so this uh native client

00:53:46,480 --> 00:53:50,079
this paper talks about a6

00:53:48,160 --> 00:53:51,200
32 but there's variants of it for

00:53:50,079 --> 00:53:54,079
example 6664

00:53:51,200 --> 00:53:55,760
32-bit arm v7 arm as well as mips 32.

00:53:54,079 --> 00:53:57,200
not covered in that paper there's like

00:53:55,760 --> 00:53:57,520
subsequent things on the internet about

00:53:57,200 --> 00:53:59,040
it

00:53:57,520 --> 00:54:00,800
so it's not technically legit but it's

00:53:59,040 --> 00:54:01,440
interesting because these seven rules

00:54:00,800 --> 00:54:02,640
right

00:54:01,440 --> 00:54:04,640
don't bother reading them they're kind

00:54:02,640 --> 00:54:05,599
of the material there's seven rules that

00:54:04,640 --> 00:54:07,359
you have to follow

00:54:05,599 --> 00:54:08,400
at the instruction level right so you'd

00:54:07,359 --> 00:54:09,359
assemble a program look at the

00:54:08,400 --> 00:54:11,440
instructions

00:54:09,359 --> 00:54:12,960
and you don't need to know much about

00:54:11,440 --> 00:54:14,079
the os you don't need to know anything

00:54:12,960 --> 00:54:16,000
about the compiler that

00:54:14,079 --> 00:54:19,119
generated that assembly you if these

00:54:16,000 --> 00:54:21,839
seven rules are followed in the binary

00:54:19,119 --> 00:54:22,960
then you know the the binary can't do

00:54:21,839 --> 00:54:25,200
anything malicious

00:54:22,960 --> 00:54:27,200
it's safe right that's really really

00:54:25,200 --> 00:54:29,280
cute right and further knackle uses

00:54:27,200 --> 00:54:31,520
segmentation to kind of enforce some of

00:54:29,280 --> 00:54:32,960
those things in x8632 and it effectively

00:54:31,520 --> 00:54:35,599
has a harvard architecture

00:54:32,960 --> 00:54:36,720
where code and data are separated right

00:54:35,599 --> 00:54:39,200
and so

00:54:36,720 --> 00:54:40,000
you can do this type of proof at the

00:54:39,200 --> 00:54:41,839
assembly level

00:54:40,000 --> 00:54:42,960
in jits right not necessarily these ones

00:54:41,839 --> 00:54:43,839
you don't necessarily need to use

00:54:42,960 --> 00:54:45,200
segmentation

00:54:43,839 --> 00:54:46,799
but there's a lot of really cool tricks

00:54:45,200 --> 00:54:48,799
that you can do if you prove particular

00:54:46,799 --> 00:54:50,079
properties about code right so your jibs

00:54:48,799 --> 00:54:52,640
could be completely untrusted

00:54:50,079 --> 00:54:53,920
completely cracked yet but if you have a

00:54:52,640 --> 00:54:55,440
program that checks with it

00:54:53,920 --> 00:54:57,200
generated afterwards you can actually

00:54:55,440 --> 00:54:58,240
make sure that the didn't generate

00:54:57,200 --> 00:54:59,520
anything too bad

00:54:58,240 --> 00:55:02,079
right that's one interesting thing that

00:54:59,520 --> 00:55:03,680
you can do um

00:55:02,079 --> 00:55:05,119
so so i i think that's really cool and

00:55:03,680 --> 00:55:06,880
then knackle as i said

00:55:05,119 --> 00:55:08,720
has versions for different architectures

00:55:06,880 --> 00:55:10,079
and there's a subsequent uh program

00:55:08,720 --> 00:55:10,799
called portable native clients so

00:55:10,079 --> 00:55:12,799
pinnacle

00:55:10,799 --> 00:55:14,160
uh that that made knackle portable now

00:55:12,799 --> 00:55:16,799
what it did is a bit like

00:55:14,160 --> 00:55:18,240
janky it runs lvm inside of the knuckle

00:55:16,799 --> 00:55:20,400
sandbox and then

00:55:18,240 --> 00:55:21,760
lvm which you really don't trust uh

00:55:20,400 --> 00:55:23,119
generates net call code

00:55:21,760 --> 00:55:24,559
inside your browser and then you

00:55:23,119 --> 00:55:25,839
validate the generated code before

00:55:24,559 --> 00:55:28,160
executing it because you don't need to

00:55:25,839 --> 00:55:29,920
trust lvm you just need to run it

00:55:28,160 --> 00:55:31,680
lvm itself is sandbox the code it

00:55:29,920 --> 00:55:32,799
generates you see that it follows the

00:55:31,680 --> 00:55:35,280
rules of the sandbox

00:55:32,799 --> 00:55:36,480
and it's safe right so it doesn't break

00:55:35,280 --> 00:55:38,160
out of the sandbox and

00:55:36,480 --> 00:55:40,000
like you know chrome has a vulnerability

00:55:38,160 --> 00:55:42,000
reward program this was part of chrome

00:55:40,000 --> 00:55:43,520
and as far as i know nobody's ever

00:55:42,000 --> 00:55:44,480
claimed any vulnerabilities on it right

00:55:43,520 --> 00:55:46,000
so it's it's it's

00:55:44,480 --> 00:55:47,599
you know it's had people poke at it a

00:55:46,000 --> 00:55:49,440
bit and nobody's found any flaws as far

00:55:47,599 --> 00:55:50,960
as anything so that's kind of cool

00:55:49,440 --> 00:55:52,480
and the techniques that it outlines are

00:55:50,960 --> 00:55:54,640
really cool right the idea of proving

00:55:52,480 --> 00:55:56,960
that a binary can't do anything

00:55:54,640 --> 00:55:58,559
now in fact some people have tried to

00:55:56,960 --> 00:56:00,000
attack jits right as soon as just

00:55:58,559 --> 00:56:00,400
started happening in browsers people

00:56:00,000 --> 00:56:01,680
started

00:56:00,400 --> 00:56:02,720
trying to break them and they still do

00:56:01,680 --> 00:56:03,680
all right so here's a paper that does

00:56:02,720 --> 00:56:05,280
that really well

00:56:03,680 --> 00:56:07,280
it's called attacking client side jib

00:56:05,280 --> 00:56:08,799
compilers 2011

00:56:07,280 --> 00:56:10,240
so you know a few years after it just

00:56:08,799 --> 00:56:11,839
started existing in browsers and so it

00:56:10,240 --> 00:56:13,760
says while the concept of a compiler

00:56:11,839 --> 00:56:15,520
producing correct code is not new

00:56:13,760 --> 00:56:17,119
 engines raise the stakes by

00:56:15,520 --> 00:56:19,520
performing this compilation

00:56:17,119 --> 00:56:21,040
at runtime with potentially under the

00:56:19,520 --> 00:56:22,160
influence of untrusted inputs right

00:56:21,040 --> 00:56:24,799
you just download javascript on the

00:56:22,160 --> 00:56:25,520
internet execute it right no idea what

00:56:24,799 --> 00:56:27,520
it is

00:56:25,520 --> 00:56:29,520
the common weakness and duration guide

00:56:27,520 --> 00:56:30,880
only contains one mention of compilation

00:56:29,520 --> 00:56:33,520
related vulnerabilities

00:56:30,880 --> 00:56:35,359
and that mentioned concerns compilers

00:56:33,520 --> 00:56:36,480
optimizing away a security check right

00:56:35,359 --> 00:56:40,160
so it doesn't talk about just

00:56:36,480 --> 00:56:41,760
at all so one concern with complex jit

00:56:40,160 --> 00:56:43,200
engines is a compiler producing

00:56:41,760 --> 00:56:44,799
codec runtime through either

00:56:43,200 --> 00:56:47,040
miscalculation of code locations

00:56:44,799 --> 00:56:48,720
mishandling register states bad point of

00:56:47,040 --> 00:56:49,040
dereference and a bunch of other stuff

00:56:48,720 --> 00:56:50,640
right

00:56:49,040 --> 00:56:52,640
so this paper is really cool because it

00:56:50,640 --> 00:56:54,960
breaks down a bunch of actual

00:56:52,640 --> 00:56:56,720
flaws in real production jits i think

00:56:54,960 --> 00:56:58,240
it's like four or five of them well

00:56:56,720 --> 00:57:00,559
there's like three that are the same ish

00:56:58,240 --> 00:57:02,880
but you know uh what's nice is that it

00:57:00,559 --> 00:57:04,319
also proposes concrete mitigations for

00:57:02,880 --> 00:57:05,359
the flaws that it outlines right and

00:57:04,319 --> 00:57:07,280
that's really cool

00:57:05,359 --> 00:57:09,520
and systems jits have changed

00:57:07,280 --> 00:57:10,799
significantly in some cases uh thanks to

00:57:09,520 --> 00:57:12,799
hardware support

00:57:10,799 --> 00:57:14,079
but if you look at publications there's

00:57:12,799 --> 00:57:15,839
a lot of stuff published by

00:57:14,079 --> 00:57:17,119
google project zero on their website

00:57:15,839 --> 00:57:18,799
they have a lot of really great deep

00:57:17,119 --> 00:57:20,400
dives into breaking various jits and

00:57:18,799 --> 00:57:22,640
stuff like that

00:57:20,400 --> 00:57:23,520
here's another cool paper lvm2

00:57:22,640 --> 00:57:25,920
javascript

00:57:23,520 --> 00:57:28,240
compiler called enscripting simscriptin

00:57:25,920 --> 00:57:30,160
is compiling lvm to javascript

00:57:28,240 --> 00:57:31,920
which opens up multiple opportunities to

00:57:30,160 --> 00:57:34,720
run code on the web

00:57:31,920 --> 00:57:36,319
right so l and scripting can be used to

00:57:34,720 --> 00:57:38,079
compile cnc plus bus code

00:57:36,319 --> 00:57:40,160
as well as like you can take say a

00:57:38,079 --> 00:57:41,200
python vm which is written in c compile

00:57:40,160 --> 00:57:43,599
it to unscript and

00:57:41,200 --> 00:57:44,400
to lvn to javascript and then run python

00:57:43,599 --> 00:57:46,000
code on the way

00:57:44,400 --> 00:57:47,280
you can do something with the one it has

00:57:46,000 --> 00:57:48,720
a lot of cool things and later on

00:57:47,280 --> 00:57:49,200
there's this thing called as in js that

00:57:48,720 --> 00:57:50,960
came in

00:57:49,200 --> 00:57:52,480
but it's basically hijacking javascript

00:57:50,960 --> 00:57:54,000
to do something it wasn't designed to do

00:57:52,480 --> 00:57:55,599
and with asm.js in particular

00:57:54,000 --> 00:57:57,520
which is which was published after the

00:57:55,599 --> 00:57:59,520
scripting paper uh it gives

00:57:57,520 --> 00:58:01,200
javascript a whole like c like type

00:57:59,520 --> 00:58:02,720
system even though javascript's dynamic

00:58:01,200 --> 00:58:03,440
it makes it static right so that's cool

00:58:02,720 --> 00:58:04,799
because

00:58:03,440 --> 00:58:06,559
and script doesn't need to know about

00:58:04,799 --> 00:58:08,000
security that much it just tells

00:58:06,559 --> 00:58:09,599
the browser you you take care of the

00:58:08,000 --> 00:58:11,040
security i'm going to give you code and

00:58:09,599 --> 00:58:13,359
then you get it and whatever else

00:58:11,040 --> 00:58:14,960
that's kind of a cool approach all right

00:58:13,359 --> 00:58:18,319
final thing i want to tell you about

00:58:14,960 --> 00:58:19,839
web assembly uh so there's paper that uh

00:58:18,319 --> 00:58:21,680
me and a few other people wrote called

00:58:19,839 --> 00:58:23,599
bringing the web up to what the web up

00:58:21,680 --> 00:58:24,720
to speed with webassembly so

00:58:23,599 --> 00:58:26,720
what we did is we looked at what

00:58:24,720 --> 00:58:28,000
unscripted did and as mgs and macon

00:58:26,720 --> 00:58:29,119
pinnacle and we're like this is kind of

00:58:28,000 --> 00:58:30,319
like all over the place let's make

00:58:29,119 --> 00:58:30,960
something that's principled and

00:58:30,319 --> 00:58:32,480
well-made

00:58:30,960 --> 00:58:34,880
so we got together four major bronze

00:58:32,480 --> 00:58:36,400
vendors and we designed a low-level byte

00:58:34,880 --> 00:58:37,839
code called word assembly

00:58:36,400 --> 00:58:39,599
it offers compact representation

00:58:37,839 --> 00:58:40,079
efficient validation and compilation and

00:58:39,599 --> 00:58:42,400
safe

00:58:40,079 --> 00:58:43,119
to uh and safe load to no overhead

00:58:42,400 --> 00:58:45,200
execution

00:58:43,119 --> 00:58:46,720
that's cool and rather than committing

00:58:45,200 --> 00:58:48,240
to a specific programming model

00:58:46,720 --> 00:58:49,920
webassembly is an abstraction over

00:58:48,240 --> 00:58:51,359
modern hardware making it

00:58:49,920 --> 00:58:53,280
language and hardware and platform

00:58:51,359 --> 00:58:53,839
independent with use cases beyond just

00:58:53,280 --> 00:58:55,599
the web

00:58:53,839 --> 00:58:57,200
right so it's neither web nor something

00:58:55,599 --> 00:58:59,040
it's not assembly it can target

00:58:57,200 --> 00:59:01,359
other stuff on the web let's ignore that

00:58:59,040 --> 00:59:02,880
part webassembly has been designed with

00:59:01,359 --> 00:59:04,799
formal semantics from the start which is

00:59:02,880 --> 00:59:06,319
pretty novel right like it has a formal

00:59:04,799 --> 00:59:08,400
model that explains

00:59:06,319 --> 00:59:09,599
how the vm works and that's that's

00:59:08,400 --> 00:59:11,520
really really novel

00:59:09,599 --> 00:59:13,040
uh and we describe the motivation design

00:59:11,520 --> 00:59:14,720
and formal semantics what assembly

00:59:13,040 --> 00:59:16,240
provides some preliminary experiences

00:59:14,720 --> 00:59:18,799
with implementation

00:59:16,240 --> 00:59:20,319
right and what's cool is web assembly

00:59:18,799 --> 00:59:21,760
programs run inside of browsers

00:59:20,319 --> 00:59:24,079
say chrome or firefox or safari or

00:59:21,760 --> 00:59:26,160
whatever and the virtual isa is well

00:59:24,079 --> 00:59:27,599
defined it can target all these os's and

00:59:26,160 --> 00:59:29,680
architectures differently

00:59:27,599 --> 00:59:31,040
right so it pretends to be a modern cpu

00:59:29,680 --> 00:59:32,960
but it's actually quite portable

00:59:31,040 --> 00:59:34,960
right and webassembly itself can be

00:59:32,960 --> 00:59:36,880
compiled when you get the program or

00:59:34,960 --> 00:59:38,400
like jit compiled as well while still

00:59:36,880 --> 00:59:39,520
being secure right because when you have

00:59:38,400 --> 00:59:41,680
stuff in the browser you don't want to

00:59:39,520 --> 00:59:44,400
be able to keep this

00:59:41,680 --> 00:59:45,200
all right so this concludes our talk

00:59:44,400 --> 00:59:47,200
thank you

00:59:45,200 --> 00:59:48,480
so i'm jeff i talked to you about

00:59:47,200 --> 00:59:49,920
just-in-time compilation it was a

00:59:48,480 --> 00:59:51,359
lecture on the last 60 years

00:59:49,920 --> 00:59:53,040
if you don't look at github all the

00:59:51,359 --> 00:59:56,400
papers are there again

00:59:53,040 --> 00:59:59,599
we have sig underscore sigms corjet

00:59:56,400 --> 01:00:01,040
on the slack channel and i'll take your

00:59:59,599 --> 01:00:02,400
questions afterwards maybe either on

01:00:01,040 --> 01:00:02,799
sick jet or we'll do an email or

01:00:02,400 --> 01:00:15,839
something

01:00:02,799 --> 01:00:15,839
thank you

01:00:23,359 --> 01:00:25,440

YouTube URL: https://www.youtube.com/watch?v=tWvaSkgVPpA


