Title: Taskflow: A Parallel and Heterogeneous Task Programming System Using Modern C++ - Tsung-Wei Huang
Publication date: 2020-10-04
Playlist: CppCon 2020 Day 5
Description: 
	https://cppcon.org/
https://github.com/CppCon/CppCon2020/blob/main/Presentations/taskflow_a_generalpurpose_parallel_and_heterogeneous_task_programming_system_using_modern_cpp/taskflow_a_generalpurpose_parallel_and_heterogeneous_task_programming_system_using_modern_cpp__tsungwei_huang__cppcon_2020.pdf
---
The Taskflow project addresses the long-standing question: "How can we make it easier for developers to write parallel and heterogeneous programs with high performance and simultaneous high productivity?" Modern scientific computing relies on a heterogeneous mix of computational patterns, domain algorithms, and specialized hardware to achieve key scientific milestones that go beyond traditional capabilities. However, programming these applications often requires complex expert-level tools and a deep understanding of software methodologies. Specifically, the lack of a suitable software environment that can overcome the complexity of programming large parallel and heterogeneous systems has posed a significant barrier for many organizations to facilitate transformational discoveries.

Taskflow develops a simple and powerful task programming model to enable efficient implementations of heterogeneous decomposition strategies. Our programming model empowers users with both static and dynamic task graph constructions to incorporate a broad range of computational patterns including hybrid CPU-GPU computing, dynamic control flow, and irregularity. We develop an efficient heterogeneous work-stealing strategy that adapts worker threads to available task parallelism at any time during the graph execution. We have demonstrated promising performance of Taskflow on both micro-benchmark and real-world applications. As an example, we solved a large machine learning workload by up to 1.5× faster, 1.6× less memory, and 1.7× fewer lines of code than two industrial-strength systems, oneTBB and StarPU, on a machine of 40 CPUs and 4 GPUs.

This talk will cover three aspects: (1) heterogeneous task programming model using modern C++, (2) an efficient work-stealing strategy generalizable to arbitrary heterogeneous domains, and (3) user experience we have obtained and suggested roadmap for C++ in face of future heterogeneity.

The Taskflow project is available at https://taskflow.github.io/

---
As a university faculty member, a central theme of my research is to make parallel computing easier to handle. I am passionate about using modern C++ technology to solve parallel and heterogeneous computing problems. One such effort is my Taskflow project (https://taskflow.github.io/), A General-purpose Parallel and Heterogeneous Task Programming System using Modern C++, which I developed to help developers quickly write parallel and heterogeneous programs with high performance and simultaneous high productivity.

---
Streamed & Edited by Digital Medium Ltd - events.digital-medium.co.uk
events@digital-medium.co.uk
Captions: 
	00:00:09,440 --> 00:00:12,400
hi

00:00:10,080 --> 00:00:13,200
i'm chong wei from the university of

00:00:12,400 --> 00:00:15,280
utah

00:00:13,200 --> 00:00:16,960
my research makes aerial computing

00:00:15,280 --> 00:00:19,600
easier to handle so you can quickly

00:00:16,960 --> 00:00:22,400
speed up your application performance

00:00:19,600 --> 00:00:24,560
today i'm going to show you cashflow an

00:00:22,400 --> 00:00:26,320
open source class plus opening system

00:00:24,560 --> 00:00:30,160
that can help you quickly ride payload

00:00:26,320 --> 00:00:32,239
and heterogeneous program

00:00:30,160 --> 00:00:34,079
you probably already know parallel

00:00:32,239 --> 00:00:35,520
computing is critical to advance your

00:00:34,079 --> 00:00:38,000
application performance

00:00:35,520 --> 00:00:38,879
for example a single siding machine

00:00:38,000 --> 00:00:41,280
learning program

00:00:38,879 --> 00:00:43,280
can take several hours to finish but it

00:00:41,280 --> 00:00:44,640
can be reduced only in a few minutes or

00:00:43,280 --> 00:00:47,120
even a few seconds

00:00:44,640 --> 00:00:49,280
if we are able to run it in parallel

00:00:47,120 --> 00:00:51,520
that's the power of payload computing

00:00:49,280 --> 00:00:53,440
by leveraging many core processing units

00:00:51,520 --> 00:00:53,760
we are able to speed up the performance

00:00:53,440 --> 00:00:58,239
by

00:00:53,760 --> 00:01:00,879
several orders of magnitude

00:00:58,239 --> 00:01:01,840
however writing a good payola program is

00:01:00,879 --> 00:01:03,280
very challenging

00:01:01,840 --> 00:01:05,199
because you need to deal with many

00:01:03,280 --> 00:01:07,119
difficult technical details such as

00:01:05,199 --> 00:01:09,600
standard concurrency control

00:01:07,119 --> 00:01:10,400
test dependency scheduling and data

00:01:09,600 --> 00:01:12,400
rates

00:01:10,400 --> 00:01:13,600
and many cpp developers have hard time

00:01:12,400 --> 00:01:15,119
in getting them right

00:01:13,600 --> 00:01:16,880
especially for those who do not have

00:01:15,119 --> 00:01:18,000
that much experience in parallel

00:01:16,880 --> 00:01:19,520
computing

00:01:18,000 --> 00:01:22,080
so it is really important for the

00:01:19,520 --> 00:01:24,080
library to make sure developer can focus

00:01:22,080 --> 00:01:25,280
on the application level development as

00:01:24,080 --> 00:01:27,040
much as possible

00:01:25,280 --> 00:01:30,880
instead of dealing with all these

00:01:27,040 --> 00:01:30,880
difficult payrollization details

00:01:32,640 --> 00:01:36,400
the tesla project is trying to offer the

00:01:34,400 --> 00:01:37,840
solution to this we address the

00:01:36,400 --> 00:01:40,400
following question

00:01:37,840 --> 00:01:42,560
how can we make it easier for cp cpt

00:01:40,400 --> 00:01:44,079
developers to quickly ride parallel and

00:01:42,560 --> 00:01:46,240
heterogeneous program

00:01:44,079 --> 00:01:47,840
with high performance and simultaneous

00:01:46,240 --> 00:01:49,600
high productivity

00:01:47,840 --> 00:01:51,360
and by high performance we mean the

00:01:49,600 --> 00:01:53,280
program has to run fast and scale to

00:01:51,360 --> 00:01:55,680
many core processing units

00:01:53,280 --> 00:01:56,560
including cpu and gpu by high

00:01:55,680 --> 00:01:58,799
productivity

00:01:56,560 --> 00:02:01,360
we reduce the time it takes to implement

00:01:58,799 --> 00:02:01,360
the program

00:02:03,040 --> 00:02:06,880
let's take a look at a lower example in

00:02:05,439 --> 00:02:09,599
pass flow

00:02:06,880 --> 00:02:10,319
suppose we want to do four things a b c

00:02:09,599 --> 00:02:13,599
d

00:02:10,319 --> 00:02:14,640
each representing a function or a task a

00:02:13,599 --> 00:02:17,920
has to run before b

00:02:14,640 --> 00:02:20,400
and c d has to run after b and c

00:02:17,920 --> 00:02:21,040
so when a finishes b and c can run in

00:02:20,400 --> 00:02:24,560
parallel

00:02:21,040 --> 00:02:27,840
when both b and c finish d can start

00:02:24,560 --> 00:02:30,080
this is how it looks in test flow

00:02:27,840 --> 00:02:32,640
only 15 lines of code to get a parallel

00:02:30,080 --> 00:02:34,720
test as fusion and this is all you need

00:02:32,640 --> 00:02:36,480
you create a test flow object to manage

00:02:34,720 --> 00:02:38,560
a task dependency graph

00:02:36,480 --> 00:02:40,640
and then you create an executor then

00:02:38,560 --> 00:02:42,239
manage a set of worker threads to run

00:02:40,640 --> 00:02:44,560
test flows

00:02:42,239 --> 00:02:45,440
here we use the method in place to

00:02:44,560 --> 00:02:48,959
create four

00:02:45,440 --> 00:02:51,200
tasks in terms of cpp london and assign

00:02:48,959 --> 00:02:53,760
b into d using the structure binding in

00:02:51,200 --> 00:02:56,720
c cos417

00:02:53,760 --> 00:02:57,040
to build our dependency we use precede

00:02:56,720 --> 00:02:59,440
to

00:02:57,040 --> 00:03:00,640
force a to run before b and c and

00:02:59,440 --> 00:03:04,080
succeed to force

00:03:00,640 --> 00:03:06,000
b to run after b and c finally we submit

00:03:04,080 --> 00:03:07,680
this task flow to the executive

00:03:06,000 --> 00:03:10,480
and it returns a future where we can

00:03:07,680 --> 00:03:12,800
wait on it to finish

00:03:10,480 --> 00:03:14,480
at this moment i believe many of you can

00:03:12,800 --> 00:03:16,640
fully understand what the code is doing

00:03:14,480 --> 00:03:20,720
and the code explains itself through an

00:03:16,640 --> 00:03:20,720
expressive crop description language

00:03:23,040 --> 00:03:27,040
here is how i'm going to do in the rest

00:03:24,720 --> 00:03:29,200
of the talk i'm going to show you

00:03:27,040 --> 00:03:30,080
how to express the parallelism in the

00:03:29,200 --> 00:03:32,000
right way

00:03:30,080 --> 00:03:35,120
by starting with the motivation behind

00:03:32,000 --> 00:03:37,440
test flow and the application we target

00:03:35,120 --> 00:03:39,120
then i will present how to use tesla to

00:03:37,440 --> 00:03:40,879
parallelize application

00:03:39,120 --> 00:03:43,519
and give you a rough idea about our

00:03:40,879 --> 00:03:45,599
workstation scheduling algorithm

00:03:43,519 --> 00:03:48,080
next i will demonstrate some real use

00:03:45,599 --> 00:03:50,000
cases of test flow and show you how it

00:03:48,080 --> 00:03:53,439
can boost the performance in large-scale

00:03:50,000 --> 00:03:55,439
health application finally i will share

00:03:53,439 --> 00:03:57,680
with you some of my experience in using

00:03:55,439 --> 00:03:59,519
c plus plus two parallelized large scale

00:03:57,680 --> 00:04:00,080
applications so we can enhance zipper

00:03:59,519 --> 00:04:01,439
spots

00:04:00,080 --> 00:04:04,879
to make it more amenable to

00:04:01,439 --> 00:04:04,879
heterogeneous parallelism

00:04:05,519 --> 00:04:10,959
let's start with number one i'm going to

00:04:08,400 --> 00:04:13,120
show you the motivation behind taskflow

00:04:10,959 --> 00:04:14,000
so you can express your parallelism in

00:04:13,120 --> 00:04:17,360
the right way

00:04:14,000 --> 00:04:17,360
depending on application

00:04:20,079 --> 00:04:25,120
in the past i have been developing

00:04:21,840 --> 00:04:28,160
parallel cache software for vlsi systems

00:04:25,120 --> 00:04:29,520
cad stands for computer idea design it

00:04:28,160 --> 00:04:30,960
is a software method

00:04:29,520 --> 00:04:32,639
to help people design integrated

00:04:30,960 --> 00:04:34,479
circuits or ic

00:04:32,639 --> 00:04:36,240
you start from a high level description

00:04:34,479 --> 00:04:38,000
of the hardware component

00:04:36,240 --> 00:04:40,479
and take this source to the cache

00:04:38,000 --> 00:04:41,120
software and the cat tool will generate

00:04:40,479 --> 00:04:42,960
a bunch of

00:04:41,120 --> 00:04:45,280
netize and synthesize them to the

00:04:42,960 --> 00:04:46,479
physical layout and you can tap it out

00:04:45,280 --> 00:04:49,120
to get a final chip

00:04:46,479 --> 00:04:49,520
that did inside your smartphone computer

00:04:49,120 --> 00:04:52,880
and all

00:04:49,520 --> 00:04:54,960
electronic devices this is a seriously

00:04:52,880 --> 00:04:56,000
complicated process for example if you

00:04:54,960 --> 00:04:58,320
take a look at layout

00:04:56,000 --> 00:05:00,720
generation process it require

00:04:58,320 --> 00:05:03,759
partitioning the graph circuit graph

00:05:00,720 --> 00:05:06,720
of impedance of nodes flow plan placing

00:05:03,759 --> 00:05:09,199
medians of cell in a very small area

00:05:06,720 --> 00:05:11,039
routing trillions of wires to establish

00:05:09,199 --> 00:05:12,800
the signal connection

00:05:11,039 --> 00:05:14,880
and analyze the timing of all these

00:05:12,800 --> 00:05:16,960
billions of components

00:05:14,880 --> 00:05:18,080
and the resulting computational test

00:05:16,960 --> 00:05:20,240
scrum

00:05:18,080 --> 00:05:22,639
in terms of encapsulated function code

00:05:20,240 --> 00:05:24,639
and task dependency can easily go up to

00:05:22,639 --> 00:05:26,320
millions of tasks with cycle

00:05:24,639 --> 00:05:27,680
dynamic control for irregular

00:05:26,320 --> 00:05:30,479
computational pattern

00:05:27,680 --> 00:05:33,440
it takes several days to finish so we

00:05:30,479 --> 00:05:35,840
want to answer a question

00:05:33,440 --> 00:05:36,800
how can we write efficient cpp parallel

00:05:35,840 --> 00:05:40,000
program for this

00:05:36,800 --> 00:05:42,720
monster computational test run that has

00:05:40,000 --> 00:05:43,280
medias of cpu gpu dependent tests along

00:05:42,720 --> 00:05:46,639
with

00:05:43,280 --> 00:05:46,639
algorithmic control flow

00:05:47,680 --> 00:05:51,680
because building a parallel capture is

00:05:49,919 --> 00:05:53,840
very complex we want to find a

00:05:51,680 --> 00:05:55,600
programming system that can assist us

00:05:53,840 --> 00:05:57,600
with the implementation and deployment

00:05:55,600 --> 00:06:00,160
of parallel cap algorithm

00:05:57,600 --> 00:06:00,800
to this end we have invested a lot in

00:06:00,160 --> 00:06:04,000
existing

00:06:00,800 --> 00:06:08,080
payroll computing system including p3

00:06:04,000 --> 00:06:10,319
openmp cpp flag intel tpb sequel star pu

00:06:08,080 --> 00:06:12,560
and low software from the ecp project by

00:06:10,319 --> 00:06:15,919
department of the image such as

00:06:12,560 --> 00:06:18,720
gold coast raja and parsec

00:06:15,919 --> 00:06:21,840
unfortunately we found very few of them

00:06:18,720 --> 00:06:21,840
can meet our purpose

00:06:24,400 --> 00:06:30,000
reason is as follows we summarize two

00:06:27,680 --> 00:06:31,680
big problems of existing two in this

00:06:30,000 --> 00:06:33,759
line first of all

00:06:31,680 --> 00:06:34,720
our problem defines very complex test

00:06:33,759 --> 00:06:38,000
dependencies

00:06:34,720 --> 00:06:40,160
for example analysis algorithm need to

00:06:38,000 --> 00:06:41,680
compute a certain network of billions of

00:06:40,160 --> 00:06:44,080
node dependencies

00:06:41,680 --> 00:06:45,440
that is the resulting task graph the

00:06:44,080 --> 00:06:47,360
resulting task from

00:06:45,440 --> 00:06:49,680
in terms of model functional and their

00:06:47,360 --> 00:06:53,039
dependency can go very irregular

00:06:49,680 --> 00:06:54,000
immediate but the problem is most of the

00:06:53,039 --> 00:06:56,400
existing tools

00:06:54,000 --> 00:06:57,840
are good at new parallelism but they are

00:06:56,400 --> 00:06:59,520
not very strong in expressing

00:06:57,840 --> 00:07:02,479
heterogeneous test graph

00:06:59,520 --> 00:07:03,360
at this large scale for example openmp

00:07:02,479 --> 00:07:05,759
is very good at

00:07:03,360 --> 00:07:08,720
regular parallel for loop but it's never

00:07:05,759 --> 00:07:11,680
efficient for irregular parallelism

00:07:08,720 --> 00:07:12,560
second our problem has very complex

00:07:11,680 --> 00:07:15,039
control flow

00:07:12,560 --> 00:07:17,360
for example optimization algorithm makes

00:07:15,039 --> 00:07:19,280
essential use of dynamic control flow

00:07:17,360 --> 00:07:20,720
to implement various computational

00:07:19,280 --> 00:07:22,319
patterns to carry out

00:07:20,720 --> 00:07:24,639
combinatorial optimization and

00:07:22,319 --> 00:07:26,960
analytical method

00:07:24,639 --> 00:07:29,360
and the problem is most of the existing

00:07:26,960 --> 00:07:30,400
tool required to describe a parallel

00:07:29,360 --> 00:07:32,560
workflow

00:07:30,400 --> 00:07:33,759
in a direct acidic ground and the ground

00:07:32,560 --> 00:07:35,840
cannot have cycle

00:07:33,759 --> 00:07:37,919
and they do not anticipate cycles or

00:07:35,840 --> 00:07:39,759
conditional dependencies

00:07:37,919 --> 00:07:41,039
the result of that is lacking

00:07:39,759 --> 00:07:42,319
engineering parallelism

00:07:41,039 --> 00:07:44,800
because you need to partition your

00:07:42,319 --> 00:07:45,680
workload or everything across a control

00:07:44,800 --> 00:07:47,280
flow

00:07:45,680 --> 00:07:50,560
and you synchronize your parallelism at

00:07:47,280 --> 00:07:50,560
a decision-making point

00:07:51,440 --> 00:07:54,879
to better understand this problem let's

00:07:53,360 --> 00:07:58,400
take a look at an example of an

00:07:54,879 --> 00:08:00,319
iterative optimization program

00:07:58,400 --> 00:08:02,560
the program has four tasks and start

00:08:00,319 --> 00:08:04,080
with a need tasks that initialize the

00:08:02,560 --> 00:08:06,479
data structure

00:08:04,080 --> 00:08:07,520
then enter the optimizer tabs to perform

00:08:06,479 --> 00:08:09,440
optimization

00:08:07,520 --> 00:08:11,360
for example solving a linear system

00:08:09,440 --> 00:08:14,319
using a gpu

00:08:11,360 --> 00:08:17,360
next it moves on to the converged task

00:08:14,319 --> 00:08:20,160
to check if the optimization converge

00:08:17,360 --> 00:08:21,039
if yes it goes to the output test and

00:08:20,160 --> 00:08:22,720
stop

00:08:21,039 --> 00:08:24,560
or otherwise it moves back to the

00:08:22,720 --> 00:08:25,840
optimization test again

00:08:24,560 --> 00:08:27,680
and you can see this problem has

00:08:25,840 --> 00:08:29,360
iterative control flow

00:08:27,680 --> 00:08:30,879
and doing this in a single threaded

00:08:29,360 --> 00:08:32,959
setting totally makes sense

00:08:30,879 --> 00:08:34,959
but it is very difficult to do in a

00:08:32,959 --> 00:08:36,719
parallel environment

00:08:34,959 --> 00:08:38,159
maybe we should ask this question so how

00:08:36,719 --> 00:08:40,479
can we describe

00:08:38,159 --> 00:08:41,680
this workload of dynamic controls we're

00:08:40,479 --> 00:08:44,640
using existing tool

00:08:41,680 --> 00:08:45,760
such as openmp in tpb start bu sql or

00:08:44,640 --> 00:08:47,839
cocos

00:08:45,760 --> 00:08:49,600
and what other situation we have meters

00:08:47,839 --> 00:08:50,800
of such tasks that overlap with each

00:08:49,600 --> 00:08:53,680
island each has

00:08:50,800 --> 00:08:55,120
a dynamic control flow what about a way

00:08:53,680 --> 00:08:56,000
to describe them with end-to-end

00:08:55,120 --> 00:08:58,160
parallelism

00:08:56,000 --> 00:08:59,519
is that of partitioning a parallelism

00:08:58,160 --> 00:09:01,600
across the control flow

00:08:59,519 --> 00:09:04,240
or synchronize your parallelism at each

00:09:01,600 --> 00:09:04,240
iteration

00:09:07,279 --> 00:09:12,160
after several years of research we

00:09:09,680 --> 00:09:13,760
arrive at a key conclusion we need a new

00:09:12,160 --> 00:09:16,720
cpp programming system

00:09:13,760 --> 00:09:19,200
and the takeaway here is while designing

00:09:16,720 --> 00:09:22,399
parallel algorithm is non-trivial

00:09:19,200 --> 00:09:22,959
what makes parallel an enormous

00:09:22,399 --> 00:09:25,839
challenge

00:09:22,959 --> 00:09:29,200
is the infrastructure work of how to

00:09:25,839 --> 00:09:31,120
efficiently express dependent tasks

00:09:29,200 --> 00:09:33,120
along with an algorithmic control flow

00:09:31,120 --> 00:09:37,200
and scheduling across heterogeneous

00:09:33,120 --> 00:09:38,880
computing resources

00:09:37,200 --> 00:09:41,120
that is the key motivation to support

00:09:38,880 --> 00:09:43,040
tesla project we want to strike a

00:09:41,120 --> 00:09:46,240
balance between three aspects

00:09:43,040 --> 00:09:48,000
performance productivity and portability

00:09:46,240 --> 00:09:50,399
we want to maximize the performance

00:09:48,000 --> 00:09:52,399
compared to handcrafted solution

00:09:50,399 --> 00:09:53,839
we want to maximize the productivity

00:09:52,399 --> 00:09:56,240
compared to handcrafted

00:09:53,839 --> 00:09:58,000
implementation time and we want to

00:09:56,240 --> 00:10:02,079
maximize the portability

00:09:58,000 --> 00:10:04,160
by leveraging the power of modern cpp

00:10:02,079 --> 00:10:05,920
more importantly we are not to replace

00:10:04,160 --> 00:10:07,760
anything we are not to replace the

00:10:05,920 --> 00:10:09,760
existing tool but we are trying to

00:10:07,760 --> 00:10:11,120
address that imitation on test graph

00:10:09,760 --> 00:10:13,200
parallelism

00:10:11,120 --> 00:10:14,160
and we keep our interface as compatible

00:10:13,200 --> 00:10:17,040
as possible

00:10:14,160 --> 00:10:18,240
so we can reuse their facilities because

00:10:17,040 --> 00:10:21,040
with bd

00:10:18,240 --> 00:10:25,519
only this we can deliver complementary

00:10:21,040 --> 00:10:25,519
advantage to advanced cpp parallelism

00:10:27,120 --> 00:10:32,959
next i'm going to show you how we can

00:10:30,320 --> 00:10:37,120
use taskflow to describe parallelism

00:10:32,959 --> 00:10:37,120
using a task drop in your application

00:10:39,519 --> 00:10:43,600
iso is a programming system and i cannot

00:10:42,079 --> 00:10:45,760
present a programming system without

00:10:43,600 --> 00:10:48,240
showing you some real code

00:10:45,760 --> 00:10:50,320
in the rest of the slide there will be a

00:10:48,240 --> 00:10:51,600
lot of intensive coding example and i

00:10:50,320 --> 00:10:53,440
hope this will not destroy you a

00:10:51,600 --> 00:10:55,519
beautiful morning

00:10:53,440 --> 00:10:56,800
programming is quite subjective so many

00:10:55,519 --> 00:10:59,360
arguments i'm going to

00:10:56,800 --> 00:11:00,079
present are based on my personal opinion

00:10:59,360 --> 00:11:02,640
no offense

00:11:00,079 --> 00:11:04,480
no criticism and it's all about seatbelt

00:11:02,640 --> 00:11:07,680
spots from both the users and the

00:11:04,480 --> 00:11:07,680
researchers perspective

00:11:10,240 --> 00:11:16,959
test flow defines five test types

00:11:13,680 --> 00:11:17,680
static tasks dynamic tasks cuda flow

00:11:16,959 --> 00:11:20,880
tasks

00:11:17,680 --> 00:11:22,640
condition tests and module tests static

00:11:20,880 --> 00:11:24,800
task is the most basic

00:11:22,640 --> 00:11:27,440
test type in test flow it takes a

00:11:24,800 --> 00:11:29,519
callable object and runs it

00:11:27,440 --> 00:11:31,440
a dynamic test let's just fund a test

00:11:29,519 --> 00:11:32,000
dependency graph during the execution of

00:11:31,440 --> 00:11:35,519
attack

00:11:32,000 --> 00:11:38,000
so we can do some dynamic parallelism

00:11:35,519 --> 00:11:40,079
a cooldown flow test lets you describe a

00:11:38,000 --> 00:11:43,200
gpu workload in a test flight

00:11:40,079 --> 00:11:45,839
and offloads it to a gpu

00:11:43,200 --> 00:11:46,480
a condition task lets you come and let

00:11:45,839 --> 00:11:48,720
you

00:11:46,480 --> 00:11:50,959
integrate control flow into a test

00:11:48,720 --> 00:11:52,880
ground so you can describe end-to-end

00:11:50,959 --> 00:11:55,440
parallelism

00:11:52,880 --> 00:11:56,720
a module type like to compose a large

00:11:55,440 --> 00:11:58,720
test flow graph

00:11:56,720 --> 00:12:00,480
through smaller test flow they are

00:11:58,720 --> 00:12:02,240
easier to optimize

00:12:00,480 --> 00:12:03,760
so in this hello world example we have

00:12:02,240 --> 00:12:07,440
seen before

00:12:03,760 --> 00:12:09,440
the four tasks abcb are static tasks

00:12:07,440 --> 00:12:10,800
they simply take for lambda's callback

00:12:09,440 --> 00:12:14,560
object and run them

00:12:10,800 --> 00:12:14,560
those are static types

00:12:15,360 --> 00:12:20,800
the same hello world example here by

00:12:17,600 --> 00:12:23,519
written in openmp's testing interface

00:12:20,800 --> 00:12:24,079
openmp is a language extension to

00:12:23,519 --> 00:12:27,040
describe

00:12:24,079 --> 00:12:29,279
parallelism using compiler directives it

00:12:27,040 --> 00:12:31,120
is a very popular programming library

00:12:29,279 --> 00:12:32,560
almost all people start with to learn

00:12:31,120 --> 00:12:35,279
parallel computing

00:12:32,560 --> 00:12:36,800
in this example we use omp's test syntax

00:12:35,279 --> 00:12:39,680
to create four tasks

00:12:36,800 --> 00:12:41,360
a c abcd and then we use the depend

00:12:39,680 --> 00:12:43,680
clause

00:12:41,360 --> 00:12:45,839
to specify the outgoing dependency and

00:12:43,680 --> 00:12:46,560
incoming dependency for each of the four

00:12:45,839 --> 00:12:49,600
tests

00:12:46,560 --> 00:12:50,480
for example in task a we need two

00:12:49,600 --> 00:12:53,279
variables a b

00:12:50,480 --> 00:12:53,839
and a to capture the constraint between

00:12:53,279 --> 00:12:57,040
a

00:12:53,839 --> 00:12:59,600
and b and a and c and we do the same

00:12:57,040 --> 00:13:02,480
thing for the other three tasks

00:12:59,600 --> 00:13:04,160
the code itself is not that complicated

00:13:02,480 --> 00:13:06,399
but it is static

00:13:04,160 --> 00:13:08,000
you need to explicitly define everything

00:13:06,399 --> 00:13:11,040
such that compiler would

00:13:08,000 --> 00:13:13,720
know how to generate a dependency

00:13:11,040 --> 00:13:14,959
another big problem is you are

00:13:13,720 --> 00:13:18,240
responsible

00:13:14,959 --> 00:13:20,320
for a proper order of writing tasks

00:13:18,240 --> 00:13:21,920
and order has to be consistent with the

00:13:20,320 --> 00:13:24,320
sequential execution

00:13:21,920 --> 00:13:26,240
because when you compile this program

00:13:24,320 --> 00:13:27,600
with a different compiler where openmvp

00:13:26,240 --> 00:13:29,040
has to be disabled

00:13:27,600 --> 00:13:31,040
everything falls back to sequential

00:13:29,040 --> 00:13:32,320
execution and you gotta make sure the

00:13:31,040 --> 00:13:33,920
order you roll

00:13:32,320 --> 00:13:36,079
these tasks is not breaking the

00:13:33,920 --> 00:13:37,760
dependency when they run in serial

00:13:36,079 --> 00:13:40,959
right so if you need to figure out a

00:13:37,760 --> 00:13:42,880
topological sorting order of the branch

00:13:40,959 --> 00:13:45,440
and this makes it very difficult to

00:13:42,880 --> 00:13:47,600
describe upfront parallels especially

00:13:45,440 --> 00:13:50,399
when dependencies are not known at the

00:13:47,600 --> 00:13:50,399
programming time

00:13:52,880 --> 00:13:58,079
set example again but written in intel's

00:13:55,760 --> 00:13:59,120
threading building block library or tpp

00:13:58,079 --> 00:14:01,519
for short

00:13:59,120 --> 00:14:03,199
the tpp is a general purpose object

00:14:01,519 --> 00:14:04,240
oriented parallel programming library in

00:14:03,199 --> 00:14:06,000
cpp

00:14:04,240 --> 00:14:07,600
it has been used by the industry for

00:14:06,000 --> 00:14:09,760
many years

00:14:07,600 --> 00:14:12,480
the idea of tdp is very similar to test

00:14:09,760 --> 00:14:15,440
flow in a sense it describes

00:14:12,480 --> 00:14:17,920
dependent tasks using a flow graph you

00:14:15,440 --> 00:14:20,480
create a flow graph then use the

00:14:17,920 --> 00:14:22,880
monster class continue node to create

00:14:20,480 --> 00:14:25,440
forecast abcd

00:14:22,880 --> 00:14:27,120
and then you use another big template

00:14:25,440 --> 00:14:31,279
continue message

00:14:27,120 --> 00:14:34,639
to describe dependency among a b c and d

00:14:31,279 --> 00:14:36,959
finally you identify the source task

00:14:34,639 --> 00:14:38,560
which is test a in this example from

00:14:36,959 --> 00:14:40,880
your graph and insert a

00:14:38,560 --> 00:14:44,399
into the schedule so the scheduler know

00:14:40,880 --> 00:14:45,760
where to start the execution according

00:14:44,399 --> 00:14:48,959
to our research

00:14:45,760 --> 00:14:50,720
tv has really excellent performance

00:14:48,959 --> 00:14:52,079
especially when you are running an intel

00:14:50,720 --> 00:14:54,639
architecture

00:14:52,079 --> 00:14:56,399
but it turns out many api defined by tpp

00:14:54,639 --> 00:14:57,680
is very complex and even though they are

00:14:56,399 --> 00:15:00,000
very powerful

00:14:57,680 --> 00:15:01,920
i would say the main drawback is mostly

00:15:00,000 --> 00:15:03,120
on the ease of use standpoint in terms

00:15:01,920 --> 00:15:06,160
of simplicity

00:15:03,120 --> 00:15:06,160
and explosivity

00:15:09,440 --> 00:15:14,800
final comparison is written in cocos

00:15:12,240 --> 00:15:16,800
cocos is a parallel computing library

00:15:14,800 --> 00:15:18,000
that has recently gained much attention

00:15:16,800 --> 00:15:19,920
from the community

00:15:18,000 --> 00:15:21,279
it's part of the excel skill computing

00:15:19,920 --> 00:15:24,560
ecp project

00:15:21,279 --> 00:15:27,279
under the department of energy

00:15:24,560 --> 00:15:29,440
coco's leveraged cpp style future object

00:15:27,279 --> 00:15:30,560
to handle test parallelism as far as

00:15:29,440 --> 00:15:32,560
unknown

00:15:30,560 --> 00:15:33,600
at the time of this presentation cocos

00:15:32,560 --> 00:15:36,240
does not support

00:15:33,600 --> 00:15:37,360
a support cpp lambda so in order to

00:15:36,240 --> 00:15:39,600
create tasks

00:15:37,360 --> 00:15:41,440
you need to define a test function of

00:15:39,600 --> 00:15:43,440
fixed memory they are

00:15:41,440 --> 00:15:45,440
everything is based on c plus plus

00:15:43,440 --> 00:15:47,279
future object and you launch your tester

00:15:45,440 --> 00:15:48,800
synchronously and get a future

00:15:47,279 --> 00:15:50,480
and that eventually will hold the result

00:15:48,800 --> 00:15:52,720
to that test

00:15:50,480 --> 00:15:54,160
test dependencies are described in terms

00:15:52,720 --> 00:15:57,519
of aggregated future

00:15:54,160 --> 00:16:00,160
object where you specify when one future

00:15:57,519 --> 00:16:01,920
finish another can start running and

00:16:00,160 --> 00:16:04,480
there is a lot of overhead happening

00:16:01,920 --> 00:16:06,880
during this shear state control

00:16:04,480 --> 00:16:08,399
also we need to explicitly tell the

00:16:06,880 --> 00:16:10,320
coco's roundtable all the scheduling

00:16:08,399 --> 00:16:13,120
details about the task dependencies

00:16:10,320 --> 00:16:14,720
using the function host spam and there

00:16:13,120 --> 00:16:15,440
are more scheduling code which i'm not

00:16:14,720 --> 00:16:17,440
able to

00:16:15,440 --> 00:16:19,920
present you know all the code at a

00:16:17,440 --> 00:16:21,839
single page

00:16:19,920 --> 00:16:24,079
so it turns out corpus is very powerful

00:16:21,839 --> 00:16:25,759
in describing a synchronous task but

00:16:24,079 --> 00:16:27,600
it's not very efficient in large test

00:16:25,759 --> 00:16:28,959
scrub due to the overhead of many of

00:16:27,600 --> 00:16:31,759
issues they control

00:16:28,959 --> 00:16:32,560
and also it is not as expressive as

00:16:31,759 --> 00:16:35,040
other library

00:16:32,560 --> 00:16:37,360
if you want to use it to describe a test

00:16:35,040 --> 00:16:37,360
run

00:16:39,680 --> 00:16:43,839
of course what i presented to you was my

00:16:41,600 --> 00:16:45,440
personal interpretation it can be biased

00:16:43,839 --> 00:16:46,959
so i took advantage of being a

00:16:45,440 --> 00:16:48,880
university faculty

00:16:46,959 --> 00:16:51,199
and gave a program assignment as the

00:16:48,880 --> 00:16:52,480
asking student about 100 and how would

00:16:51,199 --> 00:16:54,880
you like to

00:16:52,480 --> 00:16:55,519
express this slightly more complex test

00:16:54,880 --> 00:16:58,639
graph

00:16:55,519 --> 00:17:00,399
of 10 tasks 15 dependency and vote for

00:16:58,639 --> 00:17:03,120
the following five different library

00:17:00,399 --> 00:17:04,000
cash flow openmp tbp cocos and stu

00:17:03,120 --> 00:17:05,760
threat

00:17:04,000 --> 00:17:07,520
i did not tell you i'm an author of task

00:17:05,760 --> 00:17:09,839
flow otherwise the

00:17:07,520 --> 00:17:11,439
vote will be even more biased but what

00:17:09,839 --> 00:17:14,240
i'm trying to discover is how

00:17:11,439 --> 00:17:15,839
c plus bus learner think about this

00:17:14,240 --> 00:17:17,439
parallel computing library

00:17:15,839 --> 00:17:19,760
especially for those who are learning

00:17:17,439 --> 00:17:20,640
cpp to use parallelism in their

00:17:19,760 --> 00:17:23,760
application

00:17:20,640 --> 00:17:27,679
and this is how it looks about 74

00:17:23,760 --> 00:17:30,960
74 like cash flow and more importantly

00:17:27,679 --> 00:17:33,360
the number one concern they have is

00:17:30,960 --> 00:17:35,200
my application is already very complex

00:17:33,360 --> 00:17:36,880
and i don't i don't like the payroll

00:17:35,200 --> 00:17:37,600
programming library to become another

00:17:36,880 --> 00:17:40,400
burden

00:17:37,600 --> 00:17:42,080
to parallelize my application so this is

00:17:40,400 --> 00:17:44,480
the goal we need to pursue

00:17:42,080 --> 00:17:47,039
we need to let developers focus on high

00:17:44,480 --> 00:17:49,919
level algorithm as much as possible

00:17:47,039 --> 00:17:52,799
instead of wrestling with many parallel

00:17:49,919 --> 00:17:52,799
testing details

00:17:56,480 --> 00:18:01,679
the previous example was static testing

00:17:59,760 --> 00:18:03,840
you decide the task to spawn at the

00:18:01,679 --> 00:18:06,320
first level of the ground

00:18:03,840 --> 00:18:08,240
we can do dynamic testing you can spawn

00:18:06,320 --> 00:18:10,320
a task dependency graph

00:18:08,240 --> 00:18:14,000
from the execution of the test using a

00:18:10,320 --> 00:18:16,400
dedicated interface subflow

00:18:14,000 --> 00:18:18,080
in order to create a subfloor you in

00:18:16,400 --> 00:18:21,120
place a lambda

00:18:18,080 --> 00:18:21,840
with an argument on subflow and you

00:18:21,120 --> 00:18:23,760
describe

00:18:21,840 --> 00:18:25,200
another task dependency graph in that

00:18:23,760 --> 00:18:27,679
subfloor

00:18:25,200 --> 00:18:29,440
for example here we create another test

00:18:27,679 --> 00:18:30,640
dependency graph on the execution of

00:18:29,440 --> 00:18:33,600
task b

00:18:30,640 --> 00:18:34,400
and the software has three tasks b1 b2

00:18:33,600 --> 00:18:38,240
and b3

00:18:34,400 --> 00:18:40,640
where b3 runs after b1 and b2 so

00:18:38,240 --> 00:18:41,760
when the scheduler finishes a you start

00:18:40,640 --> 00:18:43,919
running b

00:18:41,760 --> 00:18:45,600
and then spawn another task dependency

00:18:43,919 --> 00:18:48,799
graph inside the subfloor

00:18:45,600 --> 00:18:50,880
and runs b1 and b2 and then b3

00:18:48,799 --> 00:18:52,000
eventually the subfloor joins its parent

00:18:50,880 --> 00:18:57,840
test b

00:18:52,000 --> 00:18:57,840
and then we move on to test d

00:18:59,280 --> 00:19:04,400
subfloor can be nested you can create a

00:19:01,760 --> 00:19:06,640
subflow from another subfloor and so on

00:19:04,400 --> 00:19:08,880
and this is the example of a test graph

00:19:06,640 --> 00:19:11,120
that finds the seven fibonacci number

00:19:08,880 --> 00:19:12,160
using subflow and this is a very common

00:19:11,120 --> 00:19:13,840
example for

00:19:12,160 --> 00:19:16,400
most of the parallel computing library

00:19:13,840 --> 00:19:18,640
to show recursive parallelism

00:19:16,400 --> 00:19:20,320
fibonacci number is a series of number

00:19:18,640 --> 00:19:21,760
where each number is the sum of the

00:19:20,320 --> 00:19:23,520
previous two number

00:19:21,760 --> 00:19:26,160
and it can be done with recursive

00:19:23,520 --> 00:19:26,160
parallels

00:19:31,039 --> 00:19:37,039
you can of low attach to gpu

00:19:34,160 --> 00:19:37,360
we manage heterogeneous cpu gpu tasking

00:19:37,039 --> 00:19:40,640
through

00:19:37,360 --> 00:19:44,880
a closure-based interface cuda flow

00:19:40,640 --> 00:19:45,760
a cuda flow is a task associated with a

00:19:44,880 --> 00:19:48,240
cuda graph

00:19:45,760 --> 00:19:50,000
which is a new feature in nvidia since

00:19:48,240 --> 00:19:52,320
2.10

00:19:50,000 --> 00:19:53,120
a cuda flow is very similar to subfloor

00:19:52,320 --> 00:19:55,280
it takes an

00:19:53,120 --> 00:19:57,679
argument of cuda flow and that will be

00:19:55,280 --> 00:19:59,039
created during execution of that product

00:19:57,679 --> 00:20:01,600
flow test

00:19:59,039 --> 00:20:03,520
within this cudaflow you describe

00:20:01,600 --> 00:20:06,640
multiple gpu work

00:20:03,520 --> 00:20:08,240
such as data transfer and kernel in a

00:20:06,640 --> 00:20:10,320
task dependency graph

00:20:08,240 --> 00:20:11,679
rather than aggregated operation using

00:20:10,320 --> 00:20:14,320
cuda stream

00:20:11,679 --> 00:20:14,799
in this case we have two copy tasks to

00:20:14,320 --> 00:20:18,240
move

00:20:14,799 --> 00:20:20,080
x and y from cpu to gpu

00:20:18,240 --> 00:20:21,840
and then we have a kernel we have a

00:20:20,080 --> 00:20:23,120
kernel task that offloads the

00:20:21,840 --> 00:20:25,840
computation

00:20:23,120 --> 00:20:27,360
of a written kernel onto a gpu and when

00:20:25,840 --> 00:20:30,159
a computation finishes we

00:20:27,360 --> 00:20:31,679
create another two copy tags to get the

00:20:30,159 --> 00:20:34,720
data from gpu

00:20:31,679 --> 00:20:36,240
of course if you are using unified share

00:20:34,720 --> 00:20:37,919
memory you don't need the digital copy

00:20:36,240 --> 00:20:39,360
text you can simply launch the kernel

00:20:37,919 --> 00:20:41,520
and build up a dependency between the

00:20:39,360 --> 00:20:43,840
kernels

00:20:41,520 --> 00:20:46,960
by leveraging the power of cooldock run

00:20:43,840 --> 00:20:49,120
we launch your entire gpu test run

00:20:46,960 --> 00:20:51,280
in a single kernel code instead of

00:20:49,120 --> 00:20:52,559
multiple one so we can largely reduce

00:20:51,280 --> 00:20:54,320
the overhead

00:20:52,559 --> 00:20:56,799
and when you finish your description of

00:20:54,320 --> 00:20:59,039
the gpu test graph

00:20:56,799 --> 00:21:01,280
test flow will automatically transform

00:20:59,039 --> 00:21:04,640
the test graph you described

00:21:01,280 --> 00:21:06,480
and transform it into a cuda graph

00:21:04,640 --> 00:21:08,080
so when you finish your description and

00:21:06,480 --> 00:21:10,960
we can you can pipeline the

00:21:08,080 --> 00:21:12,000
entire cuda flow with other cpu tests or

00:21:10,960 --> 00:21:13,600
other cudaflow

00:21:12,000 --> 00:21:16,080
and this is very important because the

00:21:13,600 --> 00:21:18,720
cpu gpu program always involves some

00:21:16,080 --> 00:21:20,640
data transformation tests and that comes

00:21:18,720 --> 00:21:22,799
with the expensive cost

00:21:20,640 --> 00:21:24,880
without a suitable task for interface

00:21:22,799 --> 00:21:25,760
it's very difficult to express this

00:21:24,880 --> 00:21:27,600
overlap

00:21:25,760 --> 00:21:29,360
and that we can take the most advantage

00:21:27,600 --> 00:21:32,480
of cpu gpu computing

00:21:29,360 --> 00:21:35,520
for example in this case i created llk x

00:21:32,480 --> 00:21:36,960
on cpu and another cpu test on lk1 and

00:21:35,520 --> 00:21:39,679
both can overlap

00:21:36,960 --> 00:21:41,280
and when they finish passport will fork

00:21:39,679 --> 00:21:45,840
a flat focal test graph

00:21:41,280 --> 00:21:45,840
to launch the cuda flow

00:21:46,400 --> 00:21:50,159
the design of cpu gpu heterogeneous

00:21:48,720 --> 00:21:53,280
testing

00:21:50,159 --> 00:21:54,240
has three key motivation first our

00:21:53,280 --> 00:21:57,120
closure

00:21:54,240 --> 00:21:59,039
enables state for interface we can

00:21:57,120 --> 00:22:01,919
capture data in reference to martial

00:21:59,039 --> 00:22:03,360
data exchange between cpu and gpu tests

00:22:01,919 --> 00:22:05,360
this is very important for graph

00:22:03,360 --> 00:22:06,480
parallelism because when everything is

00:22:05,360 --> 00:22:08,880
formulated as

00:22:06,480 --> 00:22:11,120
a test dependency graph we need to make

00:22:08,880 --> 00:22:11,679
sure the computational result of cpu

00:22:11,120 --> 00:22:14,320
tests

00:22:11,679 --> 00:22:16,240
is visible to gpu tests and vice versa

00:22:14,320 --> 00:22:17,360
for example cpu tests may compute the

00:22:16,240 --> 00:22:20,159
data size

00:22:17,360 --> 00:22:22,320
allocate the memory and then gpu tests

00:22:20,159 --> 00:22:24,000
know how to set up the kernel parameter

00:22:22,320 --> 00:22:25,600
so you gotta make sure the result you

00:22:24,000 --> 00:22:28,640
compute from the cpu test

00:22:25,600 --> 00:22:30,080
are visible to gpu tests so we can

00:22:28,640 --> 00:22:34,480
describe everything

00:22:30,080 --> 00:22:36,240
in a single graph entity second

00:22:34,480 --> 00:22:38,080
our closure interface can hide

00:22:36,240 --> 00:22:40,159
implementation detail to dishes

00:22:38,080 --> 00:22:41,760
it provides a lightweight abstraction

00:22:40,159 --> 00:22:44,720
over gpu implementation

00:22:41,760 --> 00:22:46,559
by default we use cuda run due to its

00:22:44,720 --> 00:22:48,640
excellent performance much faster than

00:22:46,559 --> 00:22:51,760
cuda stream opencl and sql

00:22:48,640 --> 00:22:54,080
based on our profile also

00:22:51,760 --> 00:22:56,240
this closure interface is extensible to

00:22:54,080 --> 00:22:57,280
a new accelerator type for example we

00:22:56,240 --> 00:23:00,559
may define

00:22:57,280 --> 00:23:01,280
sql flow opencl flow tpu flow or fpga

00:23:00,559 --> 00:23:03,120
flow

00:23:01,280 --> 00:23:06,080
where we can tell the graph interface

00:23:03,120 --> 00:23:07,520
for each of the accelerated time

00:23:06,080 --> 00:23:09,120
and one thing i would like to highlight

00:23:07,520 --> 00:23:11,919
is

00:23:09,120 --> 00:23:12,640
test flow does not simplify kernel

00:23:11,919 --> 00:23:15,760
programming

00:23:12,640 --> 00:23:17,760
but we focus on cpu gpu testing

00:23:15,760 --> 00:23:18,880
that affects the performance to a large

00:23:17,760 --> 00:23:21,280
extent

00:23:18,880 --> 00:23:22,000
same reason for data abstraction so

00:23:21,280 --> 00:23:23,840
tesla can

00:23:22,000 --> 00:23:26,640
complement many of the existing kernel

00:23:23,840 --> 00:23:26,640
programming system

00:23:31,120 --> 00:23:36,720
conditional testing is a powerful

00:23:33,919 --> 00:23:38,960
interface to enable dynamic control flow

00:23:36,720 --> 00:23:40,080
and we believe this is one of the key

00:23:38,960 --> 00:23:41,760
features

00:23:40,080 --> 00:23:44,799
that sends out test flow from other

00:23:41,760 --> 00:23:48,000
libraries you can create a condition

00:23:44,799 --> 00:23:50,480
test that returns a value indicating the

00:23:48,000 --> 00:23:52,640
next immediate successor to run

00:23:50,480 --> 00:23:54,159
of course you need to satisfy certain

00:23:52,640 --> 00:23:56,320
rules you cannot jump to any way you

00:23:54,159 --> 00:23:57,840
want and we do have a spatial scheduling

00:23:56,320 --> 00:24:00,240
policy for you to follow and we'll

00:23:57,840 --> 00:24:01,840
explain it later

00:24:00,240 --> 00:24:04,000
in this example of interactive

00:24:01,840 --> 00:24:06,400
optimization we have seen before

00:24:04,000 --> 00:24:09,440
i create a task run of four tasks in

00:24:06,400 --> 00:24:11,600
need optimizer converge and output

00:24:09,440 --> 00:24:14,240
in mini test initialize the data

00:24:11,600 --> 00:24:16,559
structure we need for optimization

00:24:14,240 --> 00:24:18,880
optimizer test launches the optimization

00:24:16,559 --> 00:24:20,640
routine such as solving a linear system

00:24:18,880 --> 00:24:23,440
in the matrix form

00:24:20,640 --> 00:24:25,600
converge tasks check if optimization

00:24:23,440 --> 00:24:27,279
converges and it forms the cycle back to

00:24:25,600 --> 00:24:29,360
the optimizer

00:24:27,279 --> 00:24:31,279
if we haven't reached a convergence

00:24:29,360 --> 00:24:34,559
point or move on

00:24:31,279 --> 00:24:37,120
to the output test and stop the converge

00:24:34,559 --> 00:24:40,159
task here is a condition test

00:24:37,120 --> 00:24:44,000
and you proceed it precedes two tasks

00:24:40,159 --> 00:24:47,120
optimizer and output with this order

00:24:44,000 --> 00:24:49,279
with this order when it returns zero

00:24:47,120 --> 00:24:50,240
when the converge condition test returns

00:24:49,279 --> 00:24:52,720
zero

00:24:50,240 --> 00:24:53,520
the schedule will be ask you to optimize

00:24:52,720 --> 00:24:56,559
the test

00:24:53,520 --> 00:24:58,880
again or it moves on to the output

00:24:56,559 --> 00:25:00,159
and we specify this in a return value of

00:24:58,880 --> 00:25:04,640
this condition test

00:25:00,159 --> 00:25:06,799
converge and you can see in this example

00:25:04,640 --> 00:25:08,960
there are ultimately four tasks ever

00:25:06,799 --> 00:25:09,760
created to describe this iterative

00:25:08,960 --> 00:25:12,000
workload

00:25:09,760 --> 00:25:14,000
in a whole graph entity there's no need

00:25:12,000 --> 00:25:16,799
to partition your and synchronize it at

00:25:14,000 --> 00:25:16,799
each iteration

00:25:18,320 --> 00:25:21,520
condition test can handle more

00:25:20,080 --> 00:25:23,760
complicated scenario

00:25:21,520 --> 00:25:24,799
such as nested and non-deterministic

00:25:23,760 --> 00:25:27,600
control flow

00:25:24,799 --> 00:25:29,039
in this example i create a test graph of

00:25:27,600 --> 00:25:31,360
five nested cycle

00:25:29,039 --> 00:25:32,640
each screen test is a condition test in

00:25:31,360 --> 00:25:35,440
this figure

00:25:32,640 --> 00:25:37,919
it does nothing but flips a binary coin

00:25:35,440 --> 00:25:40,640
to decide the next time

00:25:37,919 --> 00:25:41,440
either move on or go back if it returns

00:25:40,640 --> 00:25:43,679
zero

00:25:41,440 --> 00:25:44,880
it moves on to the next test if it

00:25:43,679 --> 00:25:47,679
returns one

00:25:44,880 --> 00:25:48,960
it goes back to the first condition pass

00:25:47,679 --> 00:25:51,919
and start everything

00:25:48,960 --> 00:25:54,080
from the scratch there are five such

00:25:51,919 --> 00:25:55,120
tests to flip a binary coin so you can

00:25:54,080 --> 00:25:57,200
infer that

00:25:55,120 --> 00:25:58,720
the average number of condition tasks

00:25:57,200 --> 00:25:59,919
you need to go through before reaching

00:25:58,720 --> 00:26:02,799
the test end

00:25:59,919 --> 00:26:03,600
is 32 because we got a probability of

00:26:02,799 --> 00:26:06,480
one over two

00:26:03,600 --> 00:26:08,720
at each condition test for us to move on

00:26:06,480 --> 00:26:11,200
and similar to a previous example

00:26:08,720 --> 00:26:12,480
ultimately we use only five condition

00:26:11,200 --> 00:26:15,440
tests to model this

00:26:12,480 --> 00:26:17,360
nested non-deterministic test execution

00:26:15,440 --> 00:26:18,640
even though we may end up with 32

00:26:17,360 --> 00:26:21,760
execution

00:26:18,640 --> 00:26:25,840
you know in average the schedule will

00:26:21,760 --> 00:26:25,840
will reduce these five condition tests

00:26:27,279 --> 00:26:30,799
now let's ask the question how do

00:26:28,960 --> 00:26:32,559
existing framework handle condition

00:26:30,799 --> 00:26:34,400
you know many of them are based on the

00:26:32,559 --> 00:26:35,120
replicity graph and they do not allow

00:26:34,400 --> 00:26:36,720
cycle

00:26:35,120 --> 00:26:38,159
and i'm not talking about the cycle you

00:26:36,720 --> 00:26:40,640
have in pipeline but

00:26:38,159 --> 00:26:41,520
a generic interface for you to describe

00:26:40,640 --> 00:26:44,640
control flow

00:26:41,520 --> 00:26:45,279
in a parallel test run the most common

00:26:44,640 --> 00:26:47,679
solution

00:26:45,279 --> 00:26:48,640
is to extend the test graph across fixed

00:26:47,679 --> 00:26:50,960
set iteration

00:26:48,640 --> 00:26:52,799
right for example if you happen to know

00:26:50,960 --> 00:26:54,240
a loop condition is going to step by

00:26:52,799 --> 00:26:56,799
iteration you can unroll

00:26:54,240 --> 00:26:57,279
the test one and the result of that is

00:26:56,799 --> 00:26:58,640
of course

00:26:57,279 --> 00:27:00,640
increased growth size and memory

00:26:58,640 --> 00:27:02,960
consumption

00:27:00,640 --> 00:27:04,000
but what about a loop of unknown

00:27:02,960 --> 00:27:06,880
iteration

00:27:04,000 --> 00:27:08,080
like non-deterministic conditions we saw

00:27:06,880 --> 00:27:10,799
in the previous slide

00:27:08,080 --> 00:27:12,559
you may spawn dynamic tests executing an

00:27:10,799 --> 00:27:14,240
if statement on the fly to decide the

00:27:12,559 --> 00:27:16,480
next execution path

00:27:14,240 --> 00:27:18,320
but that solution often gives you a

00:27:16,480 --> 00:27:20,640
rather complicated implementation with

00:27:18,320 --> 00:27:23,200
recursive parallelism

00:27:20,640 --> 00:27:25,039
in fact according to our research for

00:27:23,200 --> 00:27:26,799
generic condition

00:27:25,039 --> 00:27:31,120
existing frameworks suffer from

00:27:26,799 --> 00:27:31,120
exponential growth of coating capacity

00:27:33,279 --> 00:27:37,039
our testing interface is composable and

00:27:36,080 --> 00:27:38,720
this is the key

00:27:37,039 --> 00:27:41,360
element to improve programming

00:27:38,720 --> 00:27:43,919
productivity through composition

00:27:41,360 --> 00:27:45,360
you can create multiple test flow each

00:27:43,919 --> 00:27:48,000
representing a portion

00:27:45,360 --> 00:27:50,240
of your parallel decomposition strategy

00:27:48,000 --> 00:27:51,120
that is easier to optimize at a smaller

00:27:50,240 --> 00:27:53,039
scale

00:27:51,120 --> 00:27:55,279
then you can assemble these test flow

00:27:53,039 --> 00:27:55,840
smaller test flow to form a large test

00:27:55,279 --> 00:27:58,960
flow

00:27:55,840 --> 00:28:00,480
decompose correctly and efficiently in

00:27:58,960 --> 00:28:03,840
this example we create

00:28:00,480 --> 00:28:07,200
two test flows f1 and f2

00:28:03,840 --> 00:28:10,960
f1 has two tests f1 a and f1b

00:28:07,200 --> 00:28:13,919
f2 has four tests f2a f2b

00:28:10,960 --> 00:28:15,600
f2c and the module test composed of test

00:28:13,919 --> 00:28:18,080
flow f1

00:28:15,600 --> 00:28:19,840
and you can do this composition in just

00:28:18,080 --> 00:28:21,360
one line of code using the method

00:28:19,840 --> 00:28:24,640
compose off

00:28:21,360 --> 00:28:27,279
so if f1 here has two static tasks f2

00:28:24,640 --> 00:28:30,720
here has three static tasks

00:28:27,279 --> 00:28:31,120
the runtime will start from f2 a and f2

00:28:30,720 --> 00:28:34,080
b

00:28:31,120 --> 00:28:35,120
and then move on to the module test this

00:28:34,080 --> 00:28:37,360
spawn the

00:28:35,120 --> 00:28:39,520
test dependent test dependency graph

00:28:37,360 --> 00:28:43,120
describing test flow f1

00:28:39,520 --> 00:28:44,799
and then we run f1a f1b and then when

00:28:43,120 --> 00:28:47,279
they finish we go to f2c

00:28:44,799 --> 00:28:49,600
so this is how composable testing work

00:28:47,279 --> 00:28:50,480
we can partition a large test field into

00:28:49,600 --> 00:28:52,880
several

00:28:50,480 --> 00:28:58,159
a smaller test flow they are easier to

00:28:52,880 --> 00:29:00,480
compose and optimize individually

00:28:58,159 --> 00:29:02,320
the biggest advantage of tesla is

00:29:00,480 --> 00:29:04,480
everything is unified

00:29:02,320 --> 00:29:06,080
you use the method in place to create a

00:29:04,480 --> 00:29:08,960
test whether it is say

00:29:06,080 --> 00:29:09,600
static tests dynamic tests cuda flow

00:29:08,960 --> 00:29:12,720
tests

00:29:09,600 --> 00:29:13,279
condition tests or composition and when

00:29:12,720 --> 00:29:15,520
you then

00:29:13,279 --> 00:29:17,600
you use the single method proceed to

00:29:15,520 --> 00:29:19,279
relay the dependency between tests

00:29:17,600 --> 00:29:21,360
and you can create a really really

00:29:19,279 --> 00:29:22,880
complex graph that combines all the

00:29:21,360 --> 00:29:24,960
different test test type

00:29:22,880 --> 00:29:26,000
and integrate control flow into your

00:29:24,960 --> 00:29:28,000
test flow so

00:29:26,000 --> 00:29:29,279
everything runs end-to-end and our

00:29:28,000 --> 00:29:30,480
schedule will perform end-to-end

00:29:29,279 --> 00:29:34,240
optimization

00:29:30,480 --> 00:29:34,240
on one time energy efficiency and

00:29:38,840 --> 00:29:41,840
throughput

00:29:42,720 --> 00:29:46,880
this is another example of using test

00:29:45,440 --> 00:29:49,520
flow to describe

00:29:46,880 --> 00:29:50,559
k-mean clustering using cpu and gpu

00:29:49,520 --> 00:29:53,200
tests

00:29:50,559 --> 00:29:54,960
k-means is a clustering l we're going to

00:29:53,200 --> 00:29:57,760
try to find the best case center

00:29:54,960 --> 00:29:59,679
among a set of points it is interactive

00:29:57,760 --> 00:30:01,440
you have a cpu gpu test around

00:29:59,679 --> 00:30:04,640
interactively to computer

00:30:01,440 --> 00:30:05,679
k centroids here we use a single test

00:30:04,640 --> 00:30:08,399
flow to represent

00:30:05,679 --> 00:30:10,559
entire k-means workload we have one cuda

00:30:08,399 --> 00:30:11,840
flow for performing the host to device

00:30:10,559 --> 00:30:13,760
data transfer

00:30:11,840 --> 00:30:16,960
and then we have another cuda flow to

00:30:13,760 --> 00:30:18,480
compute to find the best case central of

00:30:16,960 --> 00:30:20,960
the current iteration

00:30:18,480 --> 00:30:22,640
and then we define a condition test to

00:30:20,960 --> 00:30:25,600
model iteration

00:30:22,640 --> 00:30:27,039
and when the iteration converge we have

00:30:25,600 --> 00:30:29,520
another cuda flow for

00:30:27,039 --> 00:30:31,600
performing device to host data transfer

00:30:29,520 --> 00:30:34,240
so you can see everything is unified

00:30:31,600 --> 00:30:36,000
we need only a single test flow it can

00:30:34,240 --> 00:30:38,159
represent a whole

00:30:36,000 --> 00:30:40,799
gaming clustering element of iterative

00:30:38,159 --> 00:30:40,799
control flow

00:30:43,919 --> 00:30:49,120
we have learned the core programming

00:30:46,159 --> 00:30:52,320
model of tesla to describe a parallel

00:30:49,120 --> 00:30:54,559
program using a task dependency graph

00:30:52,320 --> 00:30:56,080
being a cpp developer not only do we

00:30:54,559 --> 00:30:58,480
care about the expressivity of the

00:30:56,080 --> 00:31:00,480
library we also need performance

00:30:58,480 --> 00:31:02,559
and that means efficient scheduling

00:31:00,480 --> 00:31:04,000
algorithm so i'm going to show you our

00:31:02,559 --> 00:31:06,320
scheduling algorithm

00:31:04,000 --> 00:31:07,039
and how we can submit a written test

00:31:06,320 --> 00:31:09,360
flow

00:31:07,039 --> 00:31:12,559
with control flow into the execution

00:31:09,360 --> 00:31:12,559
engine executor

00:31:13,200 --> 00:31:16,640
an executor is where you submit a task

00:31:16,080 --> 00:31:19,279
flow to

00:31:16,640 --> 00:31:20,399
ask you tasks defined in that graph an

00:31:19,279 --> 00:31:22,559
example

00:31:20,399 --> 00:31:23,679
manages a set of workers flat to run

00:31:22,559 --> 00:31:25,919
test flow

00:31:23,679 --> 00:31:26,960
all as future methods are non-blocking

00:31:25,919 --> 00:31:28,480
and phrases

00:31:26,960 --> 00:31:30,159
so you can pretty much do whatever

00:31:28,480 --> 00:31:32,320
submissions you want

00:31:30,159 --> 00:31:33,919
let's take a look at this example

00:31:32,320 --> 00:31:36,000
suppose we have three

00:31:33,919 --> 00:31:37,760
tasks ground test row one test flow

00:31:36,000 --> 00:31:40,000
three and test flow three

00:31:37,760 --> 00:31:41,120
or tests for two and three and we have

00:31:40,000 --> 00:31:42,960
an example

00:31:41,120 --> 00:31:44,320
and we defined several methods to run

00:31:42,960 --> 00:31:47,200
the test flow

00:31:44,320 --> 00:31:49,600
for example you can use the method run

00:31:47,200 --> 00:31:52,240
to run a test flow by once

00:31:49,600 --> 00:31:53,919
or run in to run a test flow by end

00:31:52,240 --> 00:31:56,720
times

00:31:53,919 --> 00:31:57,600
or run a test flow until a stop

00:31:56,720 --> 00:32:01,039
predicate

00:31:57,600 --> 00:32:03,360
becomes true using run until

00:32:01,039 --> 00:32:06,240
for example in this line i create i can

00:32:03,360 --> 00:32:06,880
i i use the run until method to run test

00:32:06,240 --> 00:32:10,399
flow

00:32:06,880 --> 00:32:12,480
by five times in addition to

00:32:10,399 --> 00:32:14,000
running a test flow you can launch a

00:32:12,480 --> 00:32:17,440
test asynchronously using

00:32:14,000 --> 00:32:19,279
a stl style asynchronous function code

00:32:17,440 --> 00:32:20,880
and this makes it very handy if you want

00:32:19,279 --> 00:32:23,279
to create a test on the fly

00:32:20,880 --> 00:32:24,720
with no dependency and the scheduler

00:32:23,279 --> 00:32:26,799
will autonomously

00:32:24,720 --> 00:32:30,480
ask you all those tasks and decide which

00:32:26,799 --> 00:32:30,480
worker threat runs which pass

00:32:32,320 --> 00:32:38,240
our scheduling algorithm has two levels

00:32:35,600 --> 00:32:38,880
test level and worker level at the task

00:32:38,240 --> 00:32:41,760
level

00:32:38,880 --> 00:32:42,559
we decide how tasks are eq to the

00:32:41,760 --> 00:32:45,840
release

00:32:42,559 --> 00:32:48,720
under control flow at the worker level

00:32:45,840 --> 00:32:52,000
we decide how tasks are executed

00:32:48,720 --> 00:32:52,960
by which worker and the goal of task

00:32:52,000 --> 00:32:56,080
level schedule

00:32:52,960 --> 00:32:58,320
is 3-4 we want to ensure

00:32:56,080 --> 00:33:00,320
a feasible path to carry out the control

00:32:58,320 --> 00:33:02,720
flow and the key point here is

00:33:00,320 --> 00:33:04,799
we want to avoid test rates under acidic

00:33:02,720 --> 00:33:07,600
and condition execution so we can

00:33:04,799 --> 00:33:08,399
maximize the capability of a conditional

00:33:07,600 --> 00:33:10,480
testing

00:33:08,399 --> 00:33:12,880
for example you can describe a cycle in

00:33:10,480 --> 00:33:15,120
a graph but it is only schedulable when

00:33:12,880 --> 00:33:15,440
only one worker enters that cycle at any

00:33:15,120 --> 00:33:17,760
time

00:33:15,440 --> 00:33:19,919
otherwise the task will be erased by

00:33:17,760 --> 00:33:22,240
multiple workers

00:33:19,919 --> 00:33:24,080
on the other hand the goal of worker

00:33:22,240 --> 00:33:26,480
level schedule is to optimize

00:33:24,080 --> 00:33:27,679
the execution performance by leveraging

00:33:26,480 --> 00:33:30,559
work stealing

00:33:27,679 --> 00:33:31,440
to dynamically balance load and because

00:33:30,559 --> 00:33:33,760
the available

00:33:31,440 --> 00:33:35,440
parallelism changes from time to time

00:33:33,760 --> 00:33:35,919
depending on the point you are in a test

00:33:35,440 --> 00:33:38,080
graph

00:33:35,919 --> 00:33:39,760
for example you might have one task at

00:33:38,080 --> 00:33:40,480
this time and four tests at another time

00:33:39,760 --> 00:33:42,799
and next time you

00:33:40,480 --> 00:33:44,399
rerun the test book again and all the

00:33:42,799 --> 00:33:47,679
situation change

00:33:44,399 --> 00:33:50,159
so we need to adapt the number of worker

00:33:47,679 --> 00:33:51,279
threat to available test however this at

00:33:50,159 --> 00:33:53,279
any time

00:33:51,279 --> 00:33:55,279
rather than keeping them all busy so we

00:33:53,279 --> 00:33:57,760
can maximize the performance energy and

00:33:55,279 --> 00:33:57,760
throughput

00:33:59,360 --> 00:34:03,279
in order to schedule tasks under control

00:34:01,600 --> 00:34:06,399
flow

00:34:03,279 --> 00:34:07,279
we define two dependency type strong

00:34:06,399 --> 00:34:10,399
dependency

00:34:07,279 --> 00:34:11,760
and weak dependency a weak dependency is

00:34:10,399 --> 00:34:14,399
the dependency coming

00:34:11,760 --> 00:34:16,000
out of a condition test others are

00:34:14,399 --> 00:34:17,679
strong dependency for example the

00:34:16,000 --> 00:34:20,560
dependency from the

00:34:17,679 --> 00:34:20,879
in this example from the converge task

00:34:20,560 --> 00:34:23,119
to

00:34:20,879 --> 00:34:24,079
output and optimize the tasks are weak

00:34:23,119 --> 00:34:25,520
dependencies

00:34:24,079 --> 00:34:27,440
because they are coming out from the

00:34:25,520 --> 00:34:29,599
condition task

00:34:27,440 --> 00:34:30,639
and the test level scheduling flow is as

00:34:29,599 --> 00:34:33,359
follows

00:34:30,639 --> 00:34:34,399
when you submit a test graph it start

00:34:33,359 --> 00:34:36,720
with a test of

00:34:34,399 --> 00:34:38,560
no dependency including both strong and

00:34:36,720 --> 00:34:39,520
weak dependency in this case we will

00:34:38,560 --> 00:34:42,079
start from in mean

00:34:39,520 --> 00:34:43,200
test because a knee test has no

00:34:42,079 --> 00:34:46,720
dependency

00:34:43,200 --> 00:34:48,960
so schedule will start from a knee test

00:34:46,720 --> 00:34:49,919
when the scheduler runs a test from the

00:34:48,960 --> 00:34:52,639
queue

00:34:49,919 --> 00:34:53,599
it branches the execution depending on

00:34:52,639 --> 00:34:55,760
the test type

00:34:53,599 --> 00:34:56,960
if it is the condition task it invokes

00:34:55,760 --> 00:34:58,240
the callable

00:34:56,960 --> 00:35:00,839
and get a return value from the

00:34:58,240 --> 00:35:02,400
condition task and jump to the pointers

00:35:00,839 --> 00:35:05,920
assessor

00:35:02,400 --> 00:35:07,040
if it is not a condition test it invokes

00:35:05,920 --> 00:35:09,359
the callable

00:35:07,040 --> 00:35:10,560
decrement a strong dependency of all is

00:35:09,359 --> 00:35:13,280
successor by one

00:35:10,560 --> 00:35:14,480
and then in q low successor whatever

00:35:13,280 --> 00:35:16,960
dependency on men

00:35:14,480 --> 00:35:18,480
and you can infer that without condition

00:35:16,960 --> 00:35:20,800
test if you remove

00:35:18,480 --> 00:35:22,079
the condition test part from the

00:35:20,800 --> 00:35:24,079
scheduling flow

00:35:22,079 --> 00:35:30,560
the scheduling falls back to the normal

00:35:24,079 --> 00:35:32,400
direct acidic grass schedule

00:35:30,560 --> 00:35:34,320
commission text is very powerful for you

00:35:32,400 --> 00:35:34,960
to prescribe control flow in a test

00:35:34,320 --> 00:35:36,880
graph

00:35:34,960 --> 00:35:38,000
but it is also easy for you to make

00:35:36,880 --> 00:35:39,920
mistake

00:35:38,000 --> 00:35:42,480
the following example show two common

00:35:39,920 --> 00:35:44,079
pitfalls of using condition tests

00:35:42,480 --> 00:35:45,599
in the first test run we have a

00:35:44,079 --> 00:35:49,359
condition task a

00:35:45,599 --> 00:35:53,280
and you proceed three tasks excel test b

00:35:49,359 --> 00:35:55,520
and test c the problem of this graph is

00:35:53,280 --> 00:35:57,280
it won't get scheduled because there are

00:35:55,520 --> 00:35:59,040
no tasks for us to start with

00:35:57,280 --> 00:36:00,560
remember we always need to start with a

00:35:59,040 --> 00:36:02,720
task no dependencies

00:36:00,560 --> 00:36:03,760
including both strong and weak

00:36:02,720 --> 00:36:07,040
dependencies

00:36:03,760 --> 00:36:07,520
there are no such tasks fixed number one

00:36:07,040 --> 00:36:10,640
is to

00:36:07,520 --> 00:36:12,079
add a source test s so we can have it

00:36:10,640 --> 00:36:16,320
precede test a

00:36:12,079 --> 00:36:18,480
then we can start from the source test s

00:36:16,320 --> 00:36:20,240
and the second pitfall here is you may

00:36:18,480 --> 00:36:23,599
run into test rates

00:36:20,240 --> 00:36:25,599
both e and c has no dependencies right

00:36:23,599 --> 00:36:27,040
so they can both start at the same time

00:36:25,599 --> 00:36:30,000
when e finishes

00:36:27,040 --> 00:36:30,800
the enqueue test d at the same time if

00:36:30,000 --> 00:36:33,920
test b

00:36:30,800 --> 00:36:37,040
if test c returns zero and b

00:36:33,920 --> 00:36:40,079
test d is in queue and the scheduler

00:36:37,040 --> 00:36:42,320
the schedule may raise on test d

00:36:40,079 --> 00:36:44,320
to get it fixed we need to add an

00:36:42,320 --> 00:36:46,720
auxiliary node between test d

00:36:44,320 --> 00:36:48,480
and c and this in fact is to tell the

00:36:46,720 --> 00:36:52,160
schedule at the task c

00:36:48,480 --> 00:36:56,079
test d is conditioned by two situation

00:36:52,160 --> 00:36:57,920
both e finishes and c returns zero

00:36:56,079 --> 00:36:59,839
indeed this totally makes sense if you

00:36:57,920 --> 00:37:01,280
think about it as a normal single

00:36:59,839 --> 00:37:04,560
threaded control flow

00:37:01,280 --> 00:37:08,079
because we will not run test d until

00:37:04,560 --> 00:37:09,599
both conditions are true

00:37:08,079 --> 00:37:11,760
and for now it is the user's

00:37:09,599 --> 00:37:13,040
responsibility to ensure a tesla is

00:37:11,760 --> 00:37:14,800
properly conditioned

00:37:13,040 --> 00:37:16,480
of course in the future we may add some

00:37:14,800 --> 00:37:19,280
health check um

00:37:16,480 --> 00:37:20,480
api or element or functionality to help

00:37:19,280 --> 00:37:22,800
you diagnose

00:37:20,480 --> 00:37:25,839
potential pitfalls of a written

00:37:22,800 --> 00:37:25,839
conditional testing

00:37:28,160 --> 00:37:33,200
at the worker level we adopt the

00:37:30,880 --> 00:37:36,240
workstation to run tasks

00:37:33,200 --> 00:37:38,400
this uh this is a masterpiece of work

00:37:36,240 --> 00:37:40,079
we have spent a lot of effort on the

00:37:38,400 --> 00:37:41,760
research and unfortunately i won't be

00:37:40,079 --> 00:37:43,359
able to cover all the detail but i will

00:37:41,760 --> 00:37:45,040
try to give you an impression about how

00:37:43,359 --> 00:37:48,000
it works

00:37:45,040 --> 00:37:50,320
so what is work stating in a nutshell it

00:37:48,000 --> 00:37:52,400
is a dynamic scheduling algorithm

00:37:50,320 --> 00:37:53,599
i finished my job first and then i still

00:37:52,400 --> 00:37:55,599
draw from you

00:37:53,599 --> 00:37:57,520
so we can improve the performance

00:37:55,599 --> 00:37:59,200
through this decentralized dynamical

00:37:57,520 --> 00:38:01,200
balancing

00:37:59,200 --> 00:38:03,440
when a worker threat drains out of his

00:38:01,200 --> 00:38:05,359
task queue and try to get some more has

00:38:03,440 --> 00:38:08,240
from other workers

00:38:05,359 --> 00:38:09,200
and this is the essential point of work

00:38:08,240 --> 00:38:11,520
stealing

00:38:09,200 --> 00:38:14,880
and there is an excellent talk about the

00:38:11,520 --> 00:38:16,880
workstation at cppcon 2015 by

00:38:14,880 --> 00:38:20,800
pablo halpen and you are definitely

00:38:16,880 --> 00:38:20,800
encouraged to watch it for more details

00:38:22,720 --> 00:38:26,880
there are three challenges we tackle in

00:38:24,880 --> 00:38:29,280
the works worker level scheduling

00:38:26,880 --> 00:38:30,800
challenge number one distinct cpu gpu

00:38:29,280 --> 00:38:32,720
performance trade

00:38:30,800 --> 00:38:34,079
and we solve it by keeping a different

00:38:32,720 --> 00:38:37,200
set of worker

00:38:34,079 --> 00:38:38,240
per heterogeneous domain for example cpu

00:38:37,200 --> 00:38:42,000
workers for

00:38:38,240 --> 00:38:43,760
cpu tests and gpu workers for gpu tests

00:38:42,000 --> 00:38:45,680
and the reason is we need to separate

00:38:43,760 --> 00:38:47,200
execution between cpu and gpu tests

00:38:45,680 --> 00:38:48,000
because they have different execution

00:38:47,200 --> 00:38:49,760
time

00:38:48,000 --> 00:38:52,480
mixing them together can cause the

00:38:49,760 --> 00:38:54,640
problem of unpredictable delay

00:38:52,480 --> 00:38:56,160
challenge number two available task

00:38:54,640 --> 00:38:57,760
parallelism keeps changing during

00:38:56,160 --> 00:38:59,520
execution of test graph

00:38:57,760 --> 00:39:01,599
right the available tests you may have

00:38:59,520 --> 00:39:03,280
at any time are all different

00:39:01,599 --> 00:39:04,720
time point one you might have four tasks

00:39:03,280 --> 00:39:06,880
and the next time you may have

00:39:04,720 --> 00:39:08,880
ten tasks and you rerun the program

00:39:06,880 --> 00:39:10,400
again the situation change

00:39:08,880 --> 00:39:13,119
it all depends on the test class

00:39:10,400 --> 00:39:13,680
structure and this is also related to

00:39:13,119 --> 00:39:16,000
the

00:39:13,680 --> 00:39:18,000
next challenge number three that we need

00:39:16,000 --> 00:39:19,920
to properly control the worker they are

00:39:18,000 --> 00:39:21,839
making still attempt

00:39:19,920 --> 00:39:23,440
because in the work stating group you

00:39:21,839 --> 00:39:25,359
may fail to steal tests

00:39:23,440 --> 00:39:27,119
especially when multiple workers are

00:39:25,359 --> 00:39:29,200
stealing tests from the same worker only

00:39:27,119 --> 00:39:31,119
one can make it

00:39:29,200 --> 00:39:32,480
so we need to properly control the rest

00:39:31,119 --> 00:39:33,839
of our steel

00:39:32,480 --> 00:39:35,599
name and potentially degrade the

00:39:33,839 --> 00:39:37,680
performance and we solve these

00:39:35,599 --> 00:39:40,560
challenges by keeping an invariant

00:39:37,680 --> 00:39:40,880
they try to balance the active workers

00:39:40,560 --> 00:39:43,680
with

00:39:40,880 --> 00:39:45,359
available tax payers at any time during

00:39:43,680 --> 00:39:47,680
a broad execution

00:39:45,359 --> 00:39:49,359
and we bring workers to sleep when we

00:39:47,680 --> 00:39:51,920
don't have many tests to do

00:39:49,359 --> 00:39:52,560
and wake up workers to run tests when

00:39:51,920 --> 00:39:55,280
tests

00:39:52,560 --> 00:39:55,280
are abundant

00:39:55,599 --> 00:39:59,359
and this is the scheduling architecture

00:39:57,359 --> 00:40:01,760
of test flow there are two domains

00:39:59,359 --> 00:40:02,640
one for cpu workers and one for gpu

00:40:01,760 --> 00:40:04,560
worker

00:40:02,640 --> 00:40:06,800
and each domain keeps the set of worker

00:40:04,560 --> 00:40:09,440
to run tasks of the same domain

00:40:06,800 --> 00:40:12,240
a worker can only steal tests of the

00:40:09,440 --> 00:40:14,560
same domain from the other worker

00:40:12,240 --> 00:40:16,880
when you submit a task graph it goes

00:40:14,560 --> 00:40:18,720
through a globally shared task queue

00:40:16,880 --> 00:40:20,240
which can be either the cpu test queue

00:40:18,720 --> 00:40:21,839
or gpu test queue

00:40:20,240 --> 00:40:23,599
and then the workers start to steal

00:40:21,839 --> 00:40:25,520
tasks from the shared test queue

00:40:23,599 --> 00:40:28,720
and ask you that test including the

00:40:25,520 --> 00:40:31,040
following successes found from lag tests

00:40:28,720 --> 00:40:35,119
and in this architecture our schedule is

00:40:31,040 --> 00:40:35,119
generalizable to arbitrary heater genius

00:40:36,839 --> 00:40:39,520
domain

00:40:38,000 --> 00:40:41,200
we have understood the scheduling

00:40:39,520 --> 00:40:43,280
algorithm in cash flow

00:40:41,200 --> 00:40:44,800
next i'm going to present to you some of

00:40:43,280 --> 00:40:47,440
the results we have obtained from

00:40:44,800 --> 00:40:49,359
applying task flow to real application

00:40:47,440 --> 00:40:50,720
this performance data is another big

00:40:49,359 --> 00:40:53,839
thing users

00:40:50,720 --> 00:40:53,839
really care about

00:40:54,160 --> 00:40:57,839
the first experiment i'm showing you

00:40:55,839 --> 00:41:00,319
here is micro benchmark and the purpose

00:40:57,839 --> 00:41:01,920
of micro benchmark is to demonstrate a

00:41:00,319 --> 00:41:04,800
pure testing performance without too

00:41:01,920 --> 00:41:06,880
much bias from the application algorithm

00:41:04,800 --> 00:41:09,280
and we randomly generate a test run with

00:41:06,880 --> 00:41:11,599
even cpu gpu tests and each cpu test

00:41:09,280 --> 00:41:15,119
performs a constant time operation

00:41:11,599 --> 00:41:18,640
ax plus y and that is a very famous

00:41:15,119 --> 00:41:20,640
sexy pi operation with 1k element

00:41:18,640 --> 00:41:23,119
and each gpu test does the same thing

00:41:20,640 --> 00:41:25,040
but with 10k element

00:41:23,119 --> 00:41:27,040
we compare test flow with four very

00:41:25,040 --> 00:41:27,680
popular parallel programming library tbp

00:41:27,040 --> 00:41:30,640
starbu

00:41:27,680 --> 00:41:32,160
hpx and openmp and we are interested in

00:41:30,640 --> 00:41:34,400
the two questions so what is the

00:41:32,160 --> 00:41:37,599
turnaround time to program and what is

00:41:34,400 --> 00:41:39,760
the overhaul of taskbar parallelism

00:41:37,599 --> 00:41:42,079
to table 1 summarize the programming

00:41:39,760 --> 00:41:44,960
cost we use the very famous tool

00:41:42,079 --> 00:41:47,520
sloc count and they can analyze your

00:41:44,960 --> 00:41:50,800
source code in terms of rise of code

00:41:47,520 --> 00:41:52,640
numbers of token cyclomatic complexity

00:41:50,800 --> 00:41:54,720
and people and costs that may be

00:41:52,640 --> 00:41:57,280
involved you know in potentially

00:41:54,720 --> 00:41:59,280
in developing your program and we can

00:41:57,280 --> 00:42:00,720
see from this table tesla has the least

00:41:59,280 --> 00:42:04,240
cost across all

00:42:00,720 --> 00:42:06,480
reported by the soc count

00:42:04,240 --> 00:42:09,839
on the right table we summarize the

00:42:06,480 --> 00:42:11,839
overhead of test graph parallelism

00:42:09,839 --> 00:42:13,920
describing an application in a test run

00:42:11,839 --> 00:42:15,520
is convenient but it comes with some

00:42:13,920 --> 00:42:17,280
overhead right

00:42:15,520 --> 00:42:19,440
this overhead is very important for us

00:42:17,280 --> 00:42:20,800
to decide a granularity of tests when we

00:42:19,440 --> 00:42:23,040
want to use it

00:42:20,800 --> 00:42:24,079
and we can see both tesla and tbk are

00:42:23,040 --> 00:42:26,079
quite good

00:42:24,079 --> 00:42:28,640
and test flow is a little bit faster in

00:42:26,079 --> 00:42:31,839
creating a test and dependencies about

00:42:28,640 --> 00:42:34,160
30 to 40 nanoseconds faster and but the

00:42:31,839 --> 00:42:37,200
size of the test defined in tesla is

00:42:34,160 --> 00:42:40,319
a little bit larger than 1 db

00:42:37,200 --> 00:42:45,839
or tbp it's about 272 by

00:42:40,319 --> 00:42:47,920
and 132 136 by 444 tpp

00:42:45,839 --> 00:42:50,079
but when grasslands become larger as

00:42:47,920 --> 00:42:51,440
large as 40 tasks the overhead is less

00:42:50,079 --> 00:42:54,640
than one percent of the

00:42:51,440 --> 00:42:54,640
total exhibition time

00:42:55,200 --> 00:42:58,880
let's take a look at performance the top

00:42:57,040 --> 00:43:01,200
four figures show the performance on

00:42:58,880 --> 00:43:04,560
runtime memory energy and power

00:43:01,200 --> 00:43:07,599
blue line is test flow red light is tdp

00:43:04,560 --> 00:43:11,040
light blue is star pu black line is hp

00:43:07,599 --> 00:43:13,440
x and brown line is openmp in terms of

00:43:11,040 --> 00:43:15,359
runtime tesla is faster than the others

00:43:13,440 --> 00:43:16,960
and that is due to our scheduling

00:43:15,359 --> 00:43:19,119
algorithm which we always

00:43:16,960 --> 00:43:20,319
adapt the workers to available test

00:43:19,119 --> 00:43:22,480
parallelism for

00:43:20,319 --> 00:43:24,480
dynamic load balancing and these also

00:43:22,480 --> 00:43:25,440
translate to better energy and power

00:43:24,480 --> 00:43:27,599
efficiency

00:43:25,440 --> 00:43:28,560
when we major using the linux kernel

00:43:27,599 --> 00:43:30,400
profiler

00:43:28,560 --> 00:43:32,480
because we are able to use the minimal

00:43:30,400 --> 00:43:35,200
threading resources to finish the

00:43:32,480 --> 00:43:35,920
workload and in the workforce data loop

00:43:35,200 --> 00:43:39,920
we can

00:43:35,920 --> 00:43:42,160
control the number of westerval steel

00:43:39,920 --> 00:43:43,599
the downside of lag is of course higher

00:43:42,160 --> 00:43:45,760
memory consumption

00:43:43,599 --> 00:43:47,280
and tesla comes in a little bit higher

00:43:45,760 --> 00:43:50,160
memory than the others

00:43:47,280 --> 00:43:50,480
it is about uh full 100 if the cross

00:43:50,160 --> 00:43:53,760
size

00:43:50,480 --> 00:43:58,640
is as large as 1k 10k

00:43:53,760 --> 00:44:00,720
is about a 55 100 megabytes higher than

00:43:58,640 --> 00:44:02,400
the others

00:44:00,720 --> 00:44:03,760
and another attribute to this is our

00:44:02,400 --> 00:44:05,520
schedule architecture because

00:44:03,760 --> 00:44:09,839
we have a separate data structure for

00:44:05,520 --> 00:44:09,839
each domain cpu and gpu

00:44:11,040 --> 00:44:14,960
so the second experiment i'm going to

00:44:12,880 --> 00:44:16,880
demonstrate is a real application on

00:44:14,960 --> 00:44:18,720
machine learning here we try to compute

00:44:16,880 --> 00:44:19,920
the inference of a very large deep

00:44:18,720 --> 00:44:23,920
neural network

00:44:19,920 --> 00:44:26,800
it has 1920 layer of dna each of 6

00:44:23,920 --> 00:44:28,240
5 5 36 neuron the entire network can

00:44:26,800 --> 00:44:30,640
take up to 50

00:44:28,240 --> 00:44:32,079
gigabyte memory and this is also the

00:44:30,640 --> 00:44:35,119
problem given by the

00:44:32,079 --> 00:44:37,359
ieee hpec high performance computing

00:44:35,119 --> 00:44:38,800
community as their yearly growth

00:44:37,359 --> 00:44:40,880
challenge problem

00:44:38,800 --> 00:44:43,839
the figure here is a partial test flow

00:44:40,880 --> 00:44:45,920
graph of four cuda flow

00:44:43,839 --> 00:44:48,400
six study tests and eight condition

00:44:45,920 --> 00:44:51,280
cycle for this machine learning workload

00:44:48,400 --> 00:44:52,880
each cuda flow contains thousands of gpu

00:44:51,280 --> 00:44:53,520
tests because the network is too large

00:44:52,880 --> 00:44:56,160
to

00:44:53,520 --> 00:44:56,560
compute using a single kernel if you

00:44:56,160 --> 00:44:59,440
launch

00:44:56,560 --> 00:45:01,359
these thousands of gpu operations one by

00:44:59,440 --> 00:45:03,760
one using cuda stream

00:45:01,359 --> 00:45:05,280
the overhead becomes very very very

00:45:03,760 --> 00:45:06,800
significant

00:45:05,280 --> 00:45:08,480
and because the network is very large

00:45:06,800 --> 00:45:12,079
the advantage of test

00:45:08,480 --> 00:45:12,079
terrorism starts to come up

00:45:13,119 --> 00:45:17,599
these slides show the performance data

00:45:14,720 --> 00:45:19,440
where we compare tesla with dpps.pu

00:45:17,599 --> 00:45:20,240
because they both support test raw

00:45:19,440 --> 00:45:22,880
parallelism

00:45:20,240 --> 00:45:24,800
explicitly and this machine learning

00:45:22,880 --> 00:45:27,520
workload is iterative

00:45:24,800 --> 00:45:29,359
and we use condition tasks of test flow

00:45:27,520 --> 00:45:32,160
to modular control flow

00:45:29,359 --> 00:45:34,319
however tpb and stop you do not support

00:45:32,160 --> 00:45:34,880
control flow so we unroll their test

00:45:34,319 --> 00:45:37,359
graph

00:45:34,880 --> 00:45:39,440
across iteration form in hindsight and

00:45:37,359 --> 00:45:40,800
we implement cooldown bra for all the

00:45:39,440 --> 00:45:42,640
library

00:45:40,800 --> 00:45:45,040
the figure in the middle over here shows

00:45:42,640 --> 00:45:46,240
the runtime and memory at different cpu

00:45:45,040 --> 00:45:49,359
number again

00:45:46,240 --> 00:45:51,280
blue light is test flow red light is tpp

00:45:49,359 --> 00:45:53,440
and library start pu

00:45:51,280 --> 00:45:54,800
in general you can see test flow is much

00:45:53,440 --> 00:45:58,319
faster than tdp and stop

00:45:54,800 --> 00:46:02,079
us about 2x faster memory is about

00:45:58,319 --> 00:46:05,040
1.6 x less because we use condition test

00:46:02,079 --> 00:46:06,000
instead of unrolling a cross iteration

00:46:05,040 --> 00:46:07,920
and there is no

00:46:06,000 --> 00:46:09,760
explicit synchronization or client-side

00:46:07,920 --> 00:46:14,160
partition across the control flow

00:46:09,760 --> 00:46:15,839
by using condition tests

00:46:14,160 --> 00:46:18,319
the second application i'm going to show

00:46:15,839 --> 00:46:20,240
you is a boss placement workload we have

00:46:18,319 --> 00:46:22,319
applied testable to solve

00:46:20,240 --> 00:46:24,160
and vosi placement is a very important

00:46:22,319 --> 00:46:26,000
step in the circuit design flow it

00:46:24,160 --> 00:46:28,800
optimizes the sale location

00:46:26,000 --> 00:46:29,520
on the chip a cell is essentially again

00:46:28,800 --> 00:46:32,640
or gay

00:46:29,520 --> 00:46:34,480
naggy and gay and so on in modern design

00:46:32,640 --> 00:46:36,079
the armenians of such cell in a

00:46:34,480 --> 00:46:38,000
placement organization that can take

00:46:36,079 --> 00:46:40,000
several hours to finish

00:46:38,000 --> 00:46:41,599
and this is an optimization workload and

00:46:40,000 --> 00:46:43,839
it makes essential use of dynamic

00:46:41,599 --> 00:46:46,079
control flow to describe iteration

00:46:43,839 --> 00:46:47,920
to speed it up we implemented a gpu

00:46:46,079 --> 00:46:48,960
accelerated algorithm that leveraged

00:46:47,920 --> 00:46:52,079
cuda floating of

00:46:48,960 --> 00:46:54,319
low test gpu and condition tests

00:46:52,079 --> 00:46:56,079
to capture the iterative optimization

00:46:54,319 --> 00:46:58,160
control flow

00:46:56,079 --> 00:47:00,480
here is a partial test scrub of four

00:46:58,160 --> 00:47:02,720
cuda flow one condition cycle

00:47:00,480 --> 00:47:04,560
and twelve study tests that describe a

00:47:02,720 --> 00:47:06,560
tiny fraction of the graph

00:47:04,560 --> 00:47:11,119
and keep in mind the entire graph is

00:47:06,560 --> 00:47:13,520
much much larger than this

00:47:11,119 --> 00:47:16,160
again we compare the performance of test

00:47:13,520 --> 00:47:18,240
flow against our tppsw because they both

00:47:16,160 --> 00:47:19,920
support test scroll parallelism

00:47:18,240 --> 00:47:23,440
and we measure the performance in terms

00:47:19,920 --> 00:47:25,280
of runtime memory and power consumption

00:47:23,440 --> 00:47:27,200
in these three figures blue light is

00:47:25,280 --> 00:47:29,839
test flow red light is tpp

00:47:27,200 --> 00:47:31,359
and library star pu on the top left

00:47:29,839 --> 00:47:33,440
field a runtime plot

00:47:31,359 --> 00:47:34,640
you can see tesla is a bit faster and

00:47:33,440 --> 00:47:36,800
the difference starts to

00:47:34,640 --> 00:47:38,319
increase when the problem sites become

00:47:36,800 --> 00:47:42,240
larger and larger

00:47:38,319 --> 00:47:44,640
ol returns saturate at about 16

00:47:42,240 --> 00:47:46,400
cpu cores but before that respond test

00:47:44,640 --> 00:47:48,240
mode is always faster

00:47:46,400 --> 00:47:49,680
and this placement optimization workload

00:47:48,240 --> 00:47:51,599
is interactive like we say

00:47:49,680 --> 00:47:53,200
so we use condition test intestinal to

00:47:51,599 --> 00:47:55,599
model the control flow

00:47:53,200 --> 00:47:57,680
but for tpp and stop you we need to have

00:47:55,599 --> 00:47:58,319
no choice but to unroll their test

00:47:57,680 --> 00:48:00,640
ground

00:47:58,319 --> 00:48:02,240
and that comes at the cost of increased

00:48:00,640 --> 00:48:04,400
memory as you can see

00:48:02,240 --> 00:48:06,640
the increase the memory difference

00:48:04,400 --> 00:48:09,280
between test tesla and the other

00:48:06,640 --> 00:48:10,720
implementation is very very large and

00:48:09,280 --> 00:48:12,720
this of course can affect

00:48:10,720 --> 00:48:14,079
everything get together can affect your

00:48:12,720 --> 00:48:16,000
energy consumption

00:48:14,079 --> 00:48:21,760
so you can see in the power data test

00:48:16,000 --> 00:48:23,920
flow is much better than the others

00:48:21,760 --> 00:48:27,040
i would like to summarize this result

00:48:23,920 --> 00:48:27,040
with a key takeaway

00:48:29,359 --> 00:48:34,880
parallel property infrastructure is just

00:48:32,480 --> 00:48:36,079
as important as the parallel solution

00:48:34,880 --> 00:48:37,839
itself

00:48:36,079 --> 00:48:39,119
different models are going to give you

00:48:37,839 --> 00:48:41,839
different implementation

00:48:39,119 --> 00:48:43,599
and the parallel algorithm itself may

00:48:41,839 --> 00:48:45,680
run very fast

00:48:43,599 --> 00:48:46,960
but the parallel complete infrastructure

00:48:45,680 --> 00:48:49,440
you use to support

00:48:46,960 --> 00:48:50,960
that algorithm may dominate entire

00:48:49,440 --> 00:48:52,640
performance

00:48:50,960 --> 00:48:54,960
and this is especially important when

00:48:52,640 --> 00:48:56,640
you consider heterogeneous workload

00:48:54,960 --> 00:48:58,480
because control flow decisions

00:48:56,640 --> 00:49:00,960
frequently happen at the boundary

00:48:58,480 --> 00:49:02,800
between cpu and gpu tests

00:49:00,960 --> 00:49:04,880
if you don't have a dedicated interface

00:49:02,800 --> 00:49:05,599
for expressing cpu and gpu dependent

00:49:04,880 --> 00:49:08,079
tests

00:49:05,599 --> 00:49:10,079
along with the control flow the overhead

00:49:08,079 --> 00:49:12,240
to partition or synchronize your heater

00:49:10,079 --> 00:49:16,960
genius parallelism may outweigh

00:49:12,240 --> 00:49:19,680
its performance benefit

00:49:16,960 --> 00:49:21,280
so now i'm going to share with you some

00:49:19,680 --> 00:49:23,520
of the experience we have learned

00:49:21,280 --> 00:49:24,319
from using c plus plus to handle large

00:49:23,520 --> 00:49:26,559
scale

00:49:24,319 --> 00:49:29,359
payload application hopefully our

00:49:26,559 --> 00:49:31,119
experience can contribute to making cpp

00:49:29,359 --> 00:49:33,760
more amenable to future juniors

00:49:31,119 --> 00:49:33,760
paralysis

00:49:34,480 --> 00:49:39,040
paleo companies never stand alone it

00:49:37,040 --> 00:49:40,720
beats nothing if it doesn't apply

00:49:39,040 --> 00:49:42,319
no one will buy the payload computing

00:49:40,720 --> 00:49:44,800
tool without application

00:49:42,319 --> 00:49:48,319
we must bring parallelism to practice

00:49:44,800 --> 00:49:48,319
and apply it to application

00:49:48,720 --> 00:49:52,079
given a tremendous amount of application

00:49:51,440 --> 00:49:55,359
i don't

00:49:52,079 --> 00:49:58,400
believe a single model or api can

00:49:55,359 --> 00:49:59,359
express all parallelism we need multiple

00:49:58,400 --> 00:50:01,040
cpp apps

00:49:59,359 --> 00:50:02,480
in parallel computing because each of

00:50:01,040 --> 00:50:04,160
them has their own

00:50:02,480 --> 00:50:06,559
you know expertise in certain

00:50:04,160 --> 00:50:08,880
application for example we cannot rely

00:50:06,559 --> 00:50:11,040
on a single super powerful language

00:50:08,880 --> 00:50:12,000
or compiler to parallelize everything

00:50:11,040 --> 00:50:14,160
for us

00:50:12,000 --> 00:50:15,599
otherwise the scalability scalability

00:50:14,160 --> 00:50:17,280
becomes an issue

00:50:15,599 --> 00:50:18,640
on the other hand we cannot rely on

00:50:17,280 --> 00:50:20,319
heroic programmer

00:50:18,640 --> 00:50:22,319
right and ask them to do everything such

00:50:20,319 --> 00:50:24,400
as scheduling concurrency control

00:50:22,319 --> 00:50:27,040
workload partition for us you know

00:50:24,400 --> 00:50:29,760
there's no cpp man in the marvel

00:50:27,040 --> 00:50:33,760
we need library runtime models to assist

00:50:29,760 --> 00:50:33,760
developer with payrollization detail

00:50:35,200 --> 00:50:39,040
here is how i think about the current

00:50:36,960 --> 00:50:40,319
status of c plus plus parallels in an

00:50:39,040 --> 00:50:42,480
existing tool

00:50:40,319 --> 00:50:43,680
for me c-plus plus parallelism is still

00:50:42,480 --> 00:50:45,760
very primitive

00:50:43,680 --> 00:50:47,760
a su-3 is very powerful but it's

00:50:45,760 --> 00:50:48,079
considered very low level you can use

00:50:47,760 --> 00:50:51,440
through

00:50:48,079 --> 00:50:53,040
aac to launch a tester synchronously but

00:50:51,440 --> 00:50:54,240
there's no way for you to describe

00:50:53,040 --> 00:50:55,920
dependency between

00:50:54,240 --> 00:50:57,920
the launch test which turns out to be

00:50:55,920 --> 00:50:59,440
the more important thing

00:50:57,920 --> 00:51:01,119
there are no easy way to describe

00:50:59,440 --> 00:51:05,200
control flow parallelism

00:51:01,119 --> 00:51:07,520
using cpp and if you look at the c417

00:51:05,200 --> 00:51:09,280
parallel stl

00:51:07,520 --> 00:51:10,559
you the only possible payroll

00:51:09,280 --> 00:51:12,960
infrastructure you may use

00:51:10,559 --> 00:51:14,720
is box synchronous parallelism right you

00:51:12,960 --> 00:51:16,880
run something sequential first

00:51:14,720 --> 00:51:18,800
and you reach the parallel region and

00:51:16,880 --> 00:51:19,920
you float multiple threads to run that

00:51:18,800 --> 00:51:21,440
parallel algorithm

00:51:19,920 --> 00:51:23,680
and then you join together where the

00:51:21,440 --> 00:51:25,839
control flow decision need to make

00:51:23,680 --> 00:51:27,040
need to be made and you synchronize all

00:51:25,839 --> 00:51:28,800
those parallel work

00:51:27,040 --> 00:51:31,520
moving on to the next sequential block

00:51:28,800 --> 00:51:34,800
and repeat this by synchronous elevation

00:51:31,520 --> 00:51:37,119
over and over also there are no standard

00:51:34,800 --> 00:51:40,319
way to overflow tests to accelerate

00:51:37,119 --> 00:51:42,400
such a gpu existing third party tools

00:51:40,319 --> 00:51:44,319
have enabled the vast success

00:51:42,400 --> 00:51:46,160
in the payload company i really really

00:51:44,319 --> 00:51:47,839
appreciate that and learned a lot from

00:51:46,160 --> 00:51:49,440
all this effort

00:51:47,839 --> 00:51:51,280
but what i found is that they lack an

00:51:49,440 --> 00:51:54,000
easily expressed interface for

00:51:51,280 --> 00:51:56,000
parallelism that can be achieved by

00:51:54,000 --> 00:51:57,119
leveraging the power of modern sequels

00:51:56,000 --> 00:51:59,359
plus

00:51:57,119 --> 00:52:01,760
there are no efficient mechanism for

00:51:59,359 --> 00:52:03,520
modeling control flow

00:52:01,760 --> 00:52:05,359
most libraries are based on direct

00:52:03,520 --> 00:52:06,000
acidic ground they do not anticipate

00:52:05,359 --> 00:52:07,839
cycle

00:52:06,000 --> 00:52:09,280
when control flows happen it goes back

00:52:07,839 --> 00:52:11,520
to sequential flow

00:52:09,280 --> 00:52:13,599
right so finally according to our

00:52:11,520 --> 00:52:15,599
research existing runtime like an

00:52:13,599 --> 00:52:16,400
efficient executive for heterogeneous

00:52:15,599 --> 00:52:18,880
testing

00:52:16,400 --> 00:52:19,680
they are good at either cpu or gpu focus

00:52:18,880 --> 00:52:23,440
workload

00:52:19,680 --> 00:52:23,440
but really both simultaneously

00:52:24,559 --> 00:52:28,400
to sum up we have presented test flow as

00:52:27,359 --> 00:52:30,480
a

00:52:28,400 --> 00:52:32,400
general purpose payload testing tool it

00:52:30,480 --> 00:52:34,240
introduces a simple

00:52:32,400 --> 00:52:36,160
efficient and transparent testing model

00:52:34,240 --> 00:52:38,079
for c-plus bus developer

00:52:36,160 --> 00:52:40,000
to quickly write parallel program using

00:52:38,079 --> 00:52:42,240
minimal programming effort

00:52:40,000 --> 00:52:44,640
we also talked about the general idea

00:52:42,240 --> 00:52:46,720
about heterogeneous workstating

00:52:44,640 --> 00:52:48,400
executor and demonstrated the

00:52:46,720 --> 00:52:49,760
performance in large-scale machine

00:52:48,400 --> 00:52:52,880
learning and vosi cap

00:52:49,760 --> 00:52:54,880
application there are many excellent

00:52:52,880 --> 00:52:56,720
efforts from the c-plus bus community on

00:52:54,880 --> 00:52:58,880
parallelism

00:52:56,720 --> 00:53:00,000
task force is not to replace anyone but

00:52:58,880 --> 00:53:02,000
to complement

00:53:00,000 --> 00:53:03,839
the current state of the art and address

00:53:02,000 --> 00:53:04,880
their limitations on the test raw

00:53:03,839 --> 00:53:07,200
parallelism

00:53:04,880 --> 00:53:08,640
by leveraging the power of modern sequel

00:53:07,200 --> 00:53:10,720
spots

00:53:08,640 --> 00:53:12,720
we are very very open to collaboration

00:53:10,720 --> 00:53:15,119
and we believe collaborative effort is

00:53:12,720 --> 00:53:16,720
the only way to make c plus was amenable

00:53:15,119 --> 00:53:18,960
to parallel computing

00:53:16,720 --> 00:53:21,359
right now we are based on cooldown for

00:53:18,960 --> 00:53:23,520
gpu testing for various reasons

00:53:21,359 --> 00:53:24,400
but we definitely want to integrate

00:53:23,520 --> 00:53:27,760
opencl

00:53:24,400 --> 00:53:28,000
sql bpcp and so on we want to provide

00:53:27,760 --> 00:53:30,559
more

00:53:28,000 --> 00:53:31,280
high-level algorithm like the thrust

00:53:30,559 --> 00:53:33,599
library

00:53:31,280 --> 00:53:35,520
so developers can easily describe common

00:53:33,599 --> 00:53:36,400
parallel application and algorithm and

00:53:35,520 --> 00:53:39,599
integrate them

00:53:36,400 --> 00:53:43,040
into control flow and form a

00:53:39,599 --> 00:53:44,000
single test flow ground if you ever use

00:53:43,040 --> 00:53:45,839
tesla please

00:53:44,000 --> 00:53:49,920
let us know we want to learn from you

00:53:45,839 --> 00:53:49,920
and we want to broaden real use cases

00:53:51,040 --> 00:53:55,200
i would like to take this chance to say

00:53:53,520 --> 00:53:58,160
thank you to all the user

00:53:55,200 --> 00:53:59,760
and thank you cppcon for giving us this

00:53:58,160 --> 00:54:01,200
great opportunity to share our

00:53:59,760 --> 00:54:03,839
experience with

00:54:01,200 --> 00:54:05,680
many many excellent cpp developers there

00:54:03,839 --> 00:54:07,839
are quite a few people using tesla right

00:54:05,680 --> 00:54:10,319
now i'm very grateful for tremendously

00:54:07,839 --> 00:54:12,319
useful feedback from user thank you

00:54:10,319 --> 00:54:14,079
again i'm very open to collaboration

00:54:12,319 --> 00:54:14,720
whether you are interested in using test

00:54:14,079 --> 00:54:16,640
flow

00:54:14,720 --> 00:54:17,760
understanding more technical details or

00:54:16,640 --> 00:54:20,000
having me to

00:54:17,760 --> 00:54:22,160
present some technical innovation of

00:54:20,000 --> 00:54:23,680
tesla in your organization so we can

00:54:22,160 --> 00:54:27,280
talk about more cpp

00:54:23,680 --> 00:54:29,839
and heterogeneous parallelism

00:54:27,280 --> 00:54:31,839
here is my email feel free to ask me any

00:54:29,839 --> 00:54:33,119
questions you can also find more detail

00:54:31,839 --> 00:54:36,160
about tesco

00:54:33,119 --> 00:54:36,640
at this github link with that i'm going

00:54:36,160 --> 00:54:38,480
to

00:54:36,640 --> 00:54:39,839
cut here thank you very much for your

00:54:38,480 --> 00:54:43,599
participation

00:54:39,839 --> 00:54:43,599
i'll be happy to take any question

00:54:46,960 --> 00:54:52,240
hello tony yeah we

00:54:50,400 --> 00:54:53,680
have a couple of questions from the

00:54:52,240 --> 00:54:56,720
remote i

00:54:53,680 --> 00:54:57,200
post it in the comment section so you

00:54:56,720 --> 00:55:00,640
can

00:54:57,200 --> 00:55:04,160
check and there's still three live delay

00:55:00,640 --> 00:55:06,319
on their end but you can start

00:55:04,160 --> 00:55:07,920
replying those questions in the comments

00:55:06,319 --> 00:55:09,760
section you mean i need to

00:55:07,920 --> 00:55:12,319
go back to red more no you don't need to

00:55:09,760 --> 00:55:16,000
go to stick to the string yard

00:55:12,319 --> 00:55:18,079
okay i saw that yeah

00:55:16,000 --> 00:55:20,319
yeah remote audience still in the page

00:55:18,079 --> 00:55:20,319
uh

00:55:22,079 --> 00:55:25,440
i probably will not be able to answer

00:55:23,760 --> 00:55:28,480
all of them but i will

00:55:25,440 --> 00:55:30,319
stay in the in the in a remote zoom and

00:55:28,480 --> 00:55:32,839
you can definitely find it in the table

00:55:30,319 --> 00:55:34,160
on the first floor so the first question

00:55:32,839 --> 00:55:37,839
is

00:55:34,160 --> 00:55:41,760
what version of c plus plus is required

00:55:37,839 --> 00:55:44,880
right now we you need cpp

00:55:41,760 --> 00:55:48,480
14 c plus plus 14 that's the minimum

00:55:44,880 --> 00:55:50,799
standard we require

00:55:48,480 --> 00:55:52,559
so how the second question how does test

00:55:50,799 --> 00:55:55,040
flow respond to

00:55:52,559 --> 00:55:56,079
being given a circular dependency

00:55:55,040 --> 00:55:59,040
ordering

00:55:56,079 --> 00:55:59,920
in large system how with issues

00:55:59,040 --> 00:56:03,760
resulting from

00:55:59,920 --> 00:56:06,880
cycle being debugged well like i say

00:56:03,760 --> 00:56:09,359
like i say we use condition test

00:56:06,880 --> 00:56:11,520
that allows you to describe control flow

00:56:09,359 --> 00:56:13,359
and the control flow can be interactive

00:56:11,520 --> 00:56:15,040
in that case you will describe cycle in

00:56:13,359 --> 00:56:17,200
your test flow but

00:56:15,040 --> 00:56:19,119
the thing is you need to make sure right

00:56:17,200 --> 00:56:21,680
now this user's responsibility to make

00:56:19,119 --> 00:56:24,720
sure based on our test level scheduling

00:56:21,680 --> 00:56:25,520
only one task can enter that cycle at

00:56:24,720 --> 00:56:28,160
one time

00:56:25,520 --> 00:56:30,400
for example if you have a nasty cycle it

00:56:28,160 --> 00:56:32,400
turns out there is a dependency

00:56:30,400 --> 00:56:34,400
joined together between the two cycle

00:56:32,400 --> 00:56:37,680
then the scheduler will result

00:56:34,400 --> 00:56:38,400
undefined behavior unless the holding

00:56:37,680 --> 00:56:40,880
condition

00:56:38,400 --> 00:56:41,680
and the the acidic conditional

00:56:40,880 --> 00:56:44,079
dependency

00:56:41,680 --> 00:56:46,319
is strictly imposed by your application

00:56:44,079 --> 00:56:48,400
level algorithm

00:56:46,319 --> 00:56:50,400
how would issue resulting from cycle

00:56:48,400 --> 00:56:51,599
being debunked that is a really really

00:56:50,400 --> 00:56:54,559
good question

00:56:51,599 --> 00:56:56,400
right now in the way we debug the cycle

00:56:54,559 --> 00:56:58,799
is we visualize the problem

00:56:56,400 --> 00:57:00,319
you can dump the test flow graph and

00:56:58,799 --> 00:57:02,079
visualize it

00:57:00,319 --> 00:57:03,680
but of course in the future we may try

00:57:02,079 --> 00:57:05,280
to like i say in the presentation we

00:57:03,680 --> 00:57:07,760
want to come up with some

00:57:05,280 --> 00:57:09,040
library some other functionality that

00:57:07,760 --> 00:57:11,440
can help you diagnose

00:57:09,040 --> 00:57:12,319
a running ground in at both programming

00:57:11,440 --> 00:57:14,799
time and

00:57:12,319 --> 00:57:14,799
run time

00:57:17,040 --> 00:57:20,160
so i got another question from michael

00:57:19,280 --> 00:57:22,240
juan

00:57:20,160 --> 00:57:26,079
would you consider doing this with the

00:57:22,240 --> 00:57:29,680
sequel of course like i mentioned in the

00:57:26,079 --> 00:57:31,920
conclusion right now we are using cuda

00:57:29,680 --> 00:57:32,799
as our graph interface i definitely want

00:57:31,920 --> 00:57:35,920
to have

00:57:32,799 --> 00:57:37,760
more capability i want to suppose opencl

00:57:35,920 --> 00:57:39,119
and definitely the sql the unified

00:57:37,760 --> 00:57:40,799
program interface will be a great

00:57:39,119 --> 00:57:42,640
opportunity for me to integrate

00:57:40,799 --> 00:57:45,200
and maybe we should talk about that

00:57:42,640 --> 00:57:45,200
offline

00:57:46,720 --> 00:57:50,960
and the next question is is it possible

00:57:50,559 --> 00:57:54,079
for

00:57:50,960 --> 00:57:57,040
users to define their own flows without

00:57:54,079 --> 00:57:59,119
inheritance and have test flow work with

00:57:57,040 --> 00:58:00,960
them

00:57:59,119 --> 00:58:02,960
i'm not quite sure what you mean by

00:58:00,960 --> 00:58:05,040
inheritance

00:58:02,960 --> 00:58:07,680
are you talking about composition or

00:58:05,040 --> 00:58:10,079
inheritance

00:58:07,680 --> 00:58:13,359
maybe we can talk about this uh offline

00:58:10,079 --> 00:58:13,359
you can find me at the table

00:58:13,520 --> 00:58:18,079
so the next question is is it possible

00:58:15,680 --> 00:58:21,280
for users to define their own flows with

00:58:18,079 --> 00:58:23,839
oh sorry that's the same question again

00:58:21,280 --> 00:58:23,839
let me see

00:58:27,839 --> 00:58:34,319
what modification if any

00:58:31,839 --> 00:58:35,839
are required for non-closure codables

00:58:34,319 --> 00:58:38,640
function pointer

00:58:35,839 --> 00:58:40,960
function object to in place into a test

00:58:38,640 --> 00:58:40,960
flow

00:58:41,280 --> 00:58:45,440
while we do not so taskflow only handle

00:58:44,480 --> 00:58:47,280
testing

00:58:45,440 --> 00:58:49,599
we do not handle data abstraction we do

00:58:47,280 --> 00:58:52,160
not handle memory so everything

00:58:49,599 --> 00:58:53,839
is everything you are familiar with the

00:58:52,160 --> 00:58:56,319
written cpp using

00:58:53,839 --> 00:58:59,280
lambda closure function objects this is

00:58:56,319 --> 00:59:01,520
so applicable to tesla because we do not

00:58:59,280 --> 00:59:03,200
provide another abstraction over them we

00:59:01,520 --> 00:59:04,640
only deal with testing

00:59:03,200 --> 00:59:06,799
so i don't think there will be any

00:59:04,640 --> 00:59:09,599
modification for you to

00:59:06,799 --> 00:59:12,880
to to modify this an enclosure callable

00:59:09,599 --> 00:59:14,960
like a function point or function object

00:59:12,880 --> 00:59:16,319
because by default as long as the test

00:59:14,960 --> 00:59:20,000
is the portable object

00:59:16,319 --> 00:59:20,000
you can place it into textbook

00:59:21,280 --> 00:59:27,119
so the next question is can test flow be

00:59:24,240 --> 00:59:29,920
used to parallelize pipeline stages in a

00:59:27,119 --> 00:59:32,400
streaming computation

00:59:29,920 --> 00:59:32,960
that is an excellent question we have a

00:59:32,400 --> 00:59:35,920
lot of

00:59:32,960 --> 00:59:37,359
issues about github especially from the

00:59:35,920 --> 00:59:40,400
multimedia

00:59:37,359 --> 00:59:43,200
gaming industry asking us to provide a

00:59:40,400 --> 00:59:44,640
hyperline capability and the answer to

00:59:43,200 --> 00:59:45,680
this is right now we do not have this

00:59:44,640 --> 00:59:47,760
capability

00:59:45,680 --> 00:59:49,119
but we are working on that and we want

00:59:47,760 --> 00:59:50,960
to come up with a

00:59:49,119 --> 00:59:52,480
a more general interface that you

00:59:50,960 --> 00:59:54,480
describe a test flow

00:59:52,480 --> 00:59:55,760
and you can specify the data stream and

00:59:54,480 --> 00:59:58,400
you can pipeline

00:59:55,760 --> 00:59:59,280
this stream across it across the test

00:59:58,400 --> 01:00:01,839
flow

00:59:59,280 --> 01:00:02,720
instead of a linear channel of operation

01:00:01,839 --> 01:00:04,720
so then

01:00:02,720 --> 01:00:06,559
the entire pipeline test program will

01:00:04,720 --> 01:00:08,640
become more general

01:00:06,559 --> 01:00:09,920
but that requires lots of modification

01:00:08,640 --> 01:00:12,000
of the current

01:00:09,920 --> 01:00:15,839
facility and more important how we

01:00:12,000 --> 01:00:15,839
schedule them

01:00:20,000 --> 01:00:23,520
i think i'm a little bit running out of

01:00:21,520 --> 01:00:25,920
time maybe maybe we should stop here

01:00:23,520 --> 01:00:26,559
and i will find a table on the first

01:00:25,920 --> 01:00:28,799
floor

01:00:26,559 --> 01:00:30,640
in a a rainbow and you guys are

01:00:28,799 --> 01:00:43,839
definitely feel free to

01:00:30,640 --> 01:00:43,839
to come reach out to me

01:00:51,839 --> 01:00:53,920

YouTube URL: https://www.youtube.com/watch?v=MX15huP5DsM


