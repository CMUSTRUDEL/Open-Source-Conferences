Title: CppCon 2018: G. Nishanov “Nano-coroutines to the Rescue! (Using Coroutines TS, of Course)”
Publication date: 2018-10-08
Playlist: CppCon 2018
Description: 
	http://CppCon.org
“Memory Latency Troubles You? Nano-coroutines to the Rescue! (Using Coroutines TS, of Course)”
—
Presentation Slides, PDFs, Source Code and other presenter materials are available at: https://github.com/CppCon/CppCon2018
—
Are you doing memory lookups in a huge table? 
Does your embarrassingly random access to your lookup tables lead to memory stalls? 

Fear no more! 

We will explore techniques that allow us to do useful work while the prefetcher is busily working on bringing the requested cache lines from main memory, by utilizing nano-coroutines. 

And the best part, nano-coroutines can be easily implemented using Coroutines TS that is already available in MSVC and Clang compilers. With a little bit of library support we can utilize the coroutines to extract intra-thread parallelism and quadruple the speed up your lookups.
— 
Gor Nishanov
Software Engineer, Microsoft
Gor Nishanov is a Principal Software Design Engineer on the Microsoft C++ team. He works on design and standardization of C++ Coroutines, and on asynchronous programming models. Prior to joining C++ team, Gor was working on distributed systems in Windows Clustering team.
—
Videos Filmed & Edited by Bash Films: http://www.BashFilms.com
Captions: 
	00:00:00,030 --> 00:00:05,779
so my name is Gordon Hashanah

00:00:03,330 --> 00:00:09,450
I'm a developer in Visual C++ team and

00:00:05,779 --> 00:00:12,179
one of the things I work on is design

00:00:09,450 --> 00:00:15,480
and standardization of C++ corrosions

00:00:12,179 --> 00:00:17,550
most efficient most scalable most open

00:00:15,480 --> 00:00:22,109
crew teams of any programming language

00:00:17,550 --> 00:00:25,560
in existence and how many people

00:00:22,109 --> 00:00:28,109
attended any of the corrodium talks we

00:00:25,560 --> 00:00:31,260
have about oh let's do the other way

00:00:28,109 --> 00:00:33,719
around so we had probably I think every

00:00:31,260 --> 00:00:37,370
Silicon there is one or two coroutine

00:00:33,719 --> 00:00:43,200
talks who haven't been to any of them

00:00:37,370 --> 00:00:44,910
oops okay so this will not be an

00:00:43,200 --> 00:00:48,180
introduction to a quarantine I put a few

00:00:44,910 --> 00:00:50,700
slides to give a context but this still

00:00:48,180 --> 00:00:54,629
assumes that you are roughly aware how

00:00:50,700 --> 00:00:56,879
equals plus curtains will work but don't

00:00:54,629 --> 00:01:00,570
worry we'll have a few slides to get you

00:00:56,879 --> 00:01:03,840
up to speed so coroutines is a very very

00:01:00,570 --> 00:01:06,780
old abstraction it is it is a better

00:01:03,840 --> 00:01:11,340
function it was invented sixty years ago

00:01:06,780 --> 00:01:15,299
by Melvin Conway and Kluth gave it a

00:01:11,340 --> 00:01:18,479
more convenient definition as actually

00:01:15,299 --> 00:01:21,140
the opposite he said that the subroutine

00:01:18,479 --> 00:01:25,530
is a special case of a more general

00:01:21,140 --> 00:01:28,229
construct which is a corrosion and if we

00:01:25,530 --> 00:01:30,780
look at it go routines are just like

00:01:28,229 --> 00:01:32,939
functions but better because they get to

00:01:30,780 --> 00:01:35,329
more operations just like normal

00:01:32,939 --> 00:01:38,520
functions which get call and return

00:01:35,329 --> 00:01:43,170
coroutines get to more suspend and

00:01:38,520 --> 00:01:44,820
resume and in C++ we are roughly getting

00:01:43,170 --> 00:01:47,100
this or at least in terms of the

00:01:44,820 --> 00:01:49,530
corruption TS which is currently going

00:01:47,100 --> 00:01:52,530
through the standardization process so

00:01:49,530 --> 00:01:54,930
coroutine is a function which has an ax

00:01:52,530 --> 00:01:58,700
wait a he or healed expression or a

00:01:54,930 --> 00:02:02,549
special form of a quad-core adjourn

00:01:58,700 --> 00:02:04,770
statement and one of the common patterns

00:02:02,549 --> 00:02:08,849
people use with coroutines are

00:02:04,770 --> 00:02:12,660
generators you can think of them as a

00:02:08,849 --> 00:02:13,350
lazily produced sequence so in this case

00:02:12,660 --> 00:02:16,770
I have

00:02:13,350 --> 00:02:20,430
a hello generator which yields character

00:02:16,770 --> 00:02:23,040
one at the time and generator from the

00:02:20,430 --> 00:02:26,370
perspective of the consumer is just an

00:02:23,040 --> 00:02:30,360
iterable it has begin and and you can

00:02:26,370 --> 00:02:32,700
pass it to arrange base for you can pass

00:02:30,360 --> 00:02:35,340
it to a standard algorithm and what

00:02:32,700 --> 00:02:37,350
happens is that when the routine gets to

00:02:35,340 --> 00:02:39,240
the spent point in this case its yield

00:02:37,350 --> 00:02:41,070
which pushes the value out of the

00:02:39,240 --> 00:02:44,220
corrosion it gets suspended

00:02:41,070 --> 00:02:47,400
given all of its state intact and then

00:02:44,220 --> 00:02:49,980
when the main in this case will pull the

00:02:47,400 --> 00:02:55,230
next value will jump back in right where

00:02:49,980 --> 00:02:57,230
the yield was and keep going another

00:02:55,230 --> 00:03:00,210
common pattern with coroutines are

00:02:57,230 --> 00:03:03,420
asynchronous tasks in this case we have

00:03:00,210 --> 00:03:06,750
some activity which has a an iteration

00:03:03,420 --> 00:03:08,580
and expression which is not ready yet it

00:03:06,750 --> 00:03:10,530
could be some length separation like for

00:03:08,580 --> 00:03:13,470
example TCP connection established in

00:03:10,530 --> 00:03:16,080
TCP connection or maybe reading from a

00:03:13,470 --> 00:03:19,050
socket and instead of blocking the

00:03:16,080 --> 00:03:22,200
thread will suspend the coroutine while

00:03:19,050 --> 00:03:25,110
waiting for the result of that operation

00:03:22,200 --> 00:03:28,110
to become available and once available

00:03:25,110 --> 00:03:35,460
could be resumed from the point word was

00:03:28,110 --> 00:03:39,290
suspended and will keep going and many

00:03:35,460 --> 00:03:42,240
languages about eleven by my last count

00:03:39,290 --> 00:03:46,290
have acquired coroutines

00:03:42,240 --> 00:03:48,780
of roughly the same shape over the last

00:03:46,290 --> 00:03:51,600
four years ten or eleven included

00:03:48,780 --> 00:03:53,280
counting the ones which are still in the

00:03:51,600 --> 00:03:55,800
design phases and those which already

00:03:53,280 --> 00:04:00,650
shape the standard with the plan which

00:03:55,800 --> 00:04:04,710
it's dart JavaScript C sharp F sharp

00:04:00,650 --> 00:04:06,540
Python I think now rust is in progress

00:04:04,710 --> 00:04:08,220
of getting them Swift just getting in

00:04:06,540 --> 00:04:10,890
progress so anyway a lot of languages

00:04:08,220 --> 00:04:13,230
are getting those facilities what makes

00:04:10,890 --> 00:04:15,390
it was Pascal routines unique is the

00:04:13,230 --> 00:04:17,970
design principles that we established

00:04:15,390 --> 00:04:20,460
that wanted to guide the design of our

00:04:17,970 --> 00:04:23,760
corrosions so we wanted cartoons to be

00:04:20,460 --> 00:04:26,010
scalable not like hey we will solve 10k

00:04:23,760 --> 00:04:26,850
problem if you look it up there was this

00:04:26,010 --> 00:04:31,190
problem I think

00:04:26,850 --> 00:04:36,060
mostly UNIX related it's how many it's

00:04:31,190 --> 00:04:37,530
can you handle 10,000 connections on the

00:04:36,060 --> 00:04:38,100
same box without creating too many

00:04:37,530 --> 00:04:40,620
threats

00:04:38,100 --> 00:04:42,900
now we're talking billions of corrosion

00:04:40,620 --> 00:04:45,030
so the overhead of every individual

00:04:42,900 --> 00:04:47,790
coroutine is tiny it's just a few bytes

00:04:45,030 --> 00:04:50,640
so if you want to you can scale to

00:04:47,790 --> 00:04:53,190
billions so far I have not seen a very

00:04:50,640 --> 00:04:57,680
good example of an application which

00:04:53,190 --> 00:05:01,020
uses million collisions but we can hope

00:04:57,680 --> 00:05:03,480
now coroutines need to the efficient so

00:05:01,020 --> 00:05:07,830
the overhead of the span and resume is

00:05:03,480 --> 00:05:12,150
equivalent or roughly on the same order

00:05:07,830 --> 00:05:16,200
as function call and that actually gives

00:05:12,150 --> 00:05:17,750
a hint or kissed small him to you what

00:05:16,200 --> 00:05:20,520
this talk would be about because

00:05:17,750 --> 00:05:21,870
Malachor routines are playing off from

00:05:20,520 --> 00:05:22,650
the fact that the switch between

00:05:21,870 --> 00:05:25,650
co-routines

00:05:22,650 --> 00:05:29,570
is less than one nanosecond and we can

00:05:25,650 --> 00:05:32,640
do very funny things if that is the case

00:05:29,570 --> 00:05:35,940
we did not want to do any funny business

00:05:32,640 --> 00:05:39,510
with stack switch and change in TLS that

00:05:35,940 --> 00:05:41,310
for example fibers do that put any limit

00:05:39,510 --> 00:05:41,730
on what you can do inside of the

00:05:41,310 --> 00:05:44,520
corrosion

00:05:41,730 --> 00:05:47,100
moreover goroutines themselves can be

00:05:44,520 --> 00:05:50,490
passed through a sea binary fluid a day

00:05:47,100 --> 00:05:53,250
boundary as a void star and then from

00:05:50,490 --> 00:05:55,410
your third-party library from an OS hole

00:05:53,250 --> 00:05:57,390
in a call back you can reconstitute the

00:05:55,410 --> 00:05:59,640
collision back and resume it and that

00:05:57,390 --> 00:06:04,110
allows very nice interactions with

00:05:59,640 --> 00:06:05,820
legacy C code finally unlike most of the

00:06:04,110 --> 00:06:08,070
other languages which have co-routines

00:06:05,820 --> 00:06:11,880
we did not tie quarantines to any

00:06:08,070 --> 00:06:13,860
particular run time because requirements

00:06:11,880 --> 00:06:16,290
for the run time say the kernel mode or

00:06:13,860 --> 00:06:19,320
in embedded systems could be very

00:06:16,290 --> 00:06:23,910
different from requirements needed for

00:06:19,320 --> 00:06:27,090
say UI applications or some others so we

00:06:23,910 --> 00:06:29,550
made a quarantine open-ended you can

00:06:27,090 --> 00:06:31,320
define the meaning to them we just

00:06:29,550 --> 00:06:34,260
provide if you should children up a

00:06:31,320 --> 00:06:37,380
function into a nice state machine and

00:06:34,260 --> 00:06:38,910
finally we may we added efficient

00:06:37,380 --> 00:06:40,130
cancellation to corrosion so any

00:06:38,910 --> 00:06:43,400
corrosion can be can

00:06:40,130 --> 00:06:45,290
at Amiens has been point without relying

00:06:43,400 --> 00:06:47,900
on exceptions and there is mostly to

00:06:45,290 --> 00:06:50,780
increase applicability of corrosions

00:06:47,900 --> 00:06:54,560
there are environments like Google for

00:06:50,780 --> 00:06:56,960
example that does not use exceptions or

00:06:54,560 --> 00:06:59,360
at least in in the large part of your

00:06:56,960 --> 00:07:00,530
code base and and well you can use

00:06:59,360 --> 00:07:03,400
collisions there

00:07:00,530 --> 00:07:06,110
same with kernel like in Windows kernel

00:07:03,400 --> 00:07:08,140
exceptional generally band origin usable

00:07:06,110 --> 00:07:11,570
because runtime does not support them

00:07:08,140 --> 00:07:14,300
anyway so these this is what

00:07:11,570 --> 00:07:16,250
distinguishes coroutines in c++ from

00:07:14,300 --> 00:07:18,680
Kirkenes in any other language and a

00:07:16,250 --> 00:07:21,620
couple of years ago we went through the

00:07:18,680 --> 00:07:24,220
tail talk where I explained what

00:07:21,620 --> 00:07:27,020
compiler does with conversions how

00:07:24,220 --> 00:07:30,080
compiler synthesizes the Corinthians and

00:07:27,020 --> 00:07:31,850
we saw what magic compiler is doing to

00:07:30,080 --> 00:07:36,050
turn all of those wonderful coroutines

00:07:31,850 --> 00:07:38,900
on the left into a tiny single constant

00:07:36,050 --> 00:07:41,180
on the right so here you can see that we

00:07:38,900 --> 00:07:44,690
created a bunch of generator coroutines

00:07:41,180 --> 00:07:47,840
then we compose them together we pass

00:07:44,690 --> 00:07:49,600
them on to a standard accumulate and in

00:07:47,840 --> 00:07:51,740
the end we go to a single number

00:07:49,600 --> 00:07:55,400
wonderful so core teams are very

00:07:51,740 --> 00:07:57,740
efficient and optimizable in c++ last

00:07:55,400 --> 00:08:00,620
year we looked at interactions between

00:07:57,740 --> 00:08:02,900
the networking Jas and corrosions so we

00:08:00,620 --> 00:08:06,200
can take this wonderful server which is

00:08:02,900 --> 00:08:08,060
a little bit hard to read and this is

00:08:06,200 --> 00:08:10,670
the same server but rewritten as a

00:08:08,060 --> 00:08:15,500
corrosion which is the same efficiency

00:08:10,670 --> 00:08:19,430
but easier on your eyes so none of

00:08:15,500 --> 00:08:21,620
corrosions this is an application of

00:08:19,430 --> 00:08:24,920
coroutines in the field of databases and

00:08:21,620 --> 00:08:28,430
I have not actually discovered that all

00:08:24,920 --> 00:08:31,400
of the credit goes to researchers that

00:08:28,430 --> 00:08:34,969
presented and discovered that in vldb

00:08:31,400 --> 00:08:38,810
conferences and reading those papers so

00:08:34,969 --> 00:08:41,960
why database people why people that goes

00:08:38,810 --> 00:08:44,690
to vldb conference which is very large

00:08:41,960 --> 00:08:48,650
database conference care about

00:08:44,690 --> 00:08:51,080
corrosions especially in an ecology

00:08:48,650 --> 00:08:52,240
goroutines that can switch in

00:08:51,080 --> 00:08:57,100
nanoseconds

00:08:52,240 --> 00:09:01,569
well the trouble is very large database

00:08:57,100 --> 00:09:02,410
a lot right and this is a tiny time you

00:09:01,569 --> 00:09:04,929
CPU

00:09:02,410 --> 00:09:06,699
I'll show you well yes it's a full

00:09:04,929 --> 00:09:09,189
course it'd be you on my laptop on which

00:09:06,699 --> 00:09:14,110
I was running the benchmark look at

00:09:09,189 --> 00:09:20,019
those tiny tiny numbers l1 cache 32

00:09:14,110 --> 00:09:23,079
kilobytes l2 cache 236 l3 cache 8

00:09:20,019 --> 00:09:27,429
megabytes and then and then another

00:09:23,079 --> 00:09:35,199
memory and it takes 16 nanoseconds to

00:09:27,429 --> 00:09:37,420
bring a value from the memory to the to

00:09:35,199 --> 00:09:42,160
the CPU and because it's a very large

00:09:37,420 --> 00:09:45,339
database their indexes their gigabytes

00:09:42,160 --> 00:09:49,749
they're all memory and their heat at

00:09:45,339 --> 00:09:52,360
random spots so whatever whatever you do

00:09:49,749 --> 00:09:54,519
whatever wonderful Castle card you are

00:09:52,360 --> 00:09:56,559
trying to achieve when you start hitting

00:09:54,519 --> 00:10:00,970
those indexes your hidden memory it

00:09:56,559 --> 00:10:03,549
absolutely random spots and nothing can

00:10:00,970 --> 00:10:06,459
help you so let's see what happens and I

00:10:03,549 --> 00:10:10,869
will use the simplest algorithm we can

00:10:06,459 --> 00:10:14,290
have for index searching a binary search

00:10:10,869 --> 00:10:17,829
because it fits the screen databases use

00:10:14,290 --> 00:10:20,350
more complicated B trees and hashing and

00:10:17,829 --> 00:10:23,019
combination of those but this will be

00:10:20,350 --> 00:10:26,170
good enough for us so it is a version of

00:10:23,019 --> 00:10:29,439
the binary search mostly stolen from a

00:10:26,170 --> 00:10:32,230
lower bound with one extra check which

00:10:29,439 --> 00:10:36,850
allows us to bail out and early here

00:10:32,230 --> 00:10:41,769
than the standard algorithm so let's run

00:10:36,850 --> 00:10:45,399
it on a small array wonderful if it fits

00:10:41,769 --> 00:10:48,220
l1 we're getting roughly 400 seconds

00:10:45,399 --> 00:10:51,129
better look up over log 2 in this case I

00:10:48,220 --> 00:10:53,740
have this little make a metric which

00:10:51,129 --> 00:10:55,600
allows us to evaluate how good the

00:10:53,740 --> 00:10:58,809
algorithm performs by simply dividing

00:10:55,600 --> 00:11:01,419
how long did it take to loop this to do

00:10:58,809 --> 00:11:04,240
the search divided by the log of the

00:11:01,419 --> 00:11:05,300
size of the array which is roughly how

00:11:04,240 --> 00:11:08,899
many he

00:11:05,300 --> 00:11:12,860
into the memory it will do so wonderful

00:11:08,899 --> 00:11:15,050
area comfortably feeds l1 we are very

00:11:12,860 --> 00:11:17,660
heaven and in misses for milliseconds

00:11:15,050 --> 00:11:21,850
per to look up okay

00:11:17,660 --> 00:11:26,269
let's grow bigger okay 4.7 nanoseconds

00:11:21,850 --> 00:11:29,120
reasonable well half of the l1 cache

00:11:26,269 --> 00:11:32,990
misses but l2 if you see is just 3.5

00:11:29,120 --> 00:11:37,000
milliseconds latency it's so still not

00:11:32,990 --> 00:11:41,720
bad again even one the array feeds l3

00:11:37,000 --> 00:11:45,070
still wonderful now this is where we are

00:11:41,720 --> 00:11:50,870
getting with databases which databases

00:11:45,070 --> 00:11:53,829
the actual index is gigabytes so even

00:11:50,870 --> 00:11:58,070
with this tiny array of 256 megabytes

00:11:53,829 --> 00:12:00,680
suddenly we are having 0.21

00:11:58,070 --> 00:12:03,950
instructions per cycle the CPU can do

00:12:00,680 --> 00:12:06,019
four instructions per cycle here we're

00:12:03,950 --> 00:12:11,510
doing it the opposite it's like one

00:12:06,019 --> 00:12:16,040
fifth of the CPU speed and what can you

00:12:11,510 --> 00:12:18,470
do about it like people people people

00:12:16,040 --> 00:12:20,390
have those wonderful data structures

00:12:18,470 --> 00:12:22,970
trying to do cash optimizations but what

00:12:20,390 --> 00:12:27,110
if your memory access pattern is

00:12:22,970 --> 00:12:31,089
horribly random well if you want to do

00:12:27,110 --> 00:12:34,459
only one lookup nothing can help you

00:12:31,089 --> 00:12:36,829
nothing can help you but luckily in

00:12:34,459 --> 00:12:39,950
databases there is this wonderful join

00:12:36,829 --> 00:12:42,370
operation which does a lot of lookups

00:12:39,950 --> 00:12:45,020
for example here we have three tables

00:12:42,370 --> 00:12:47,779
containing some customer orders customer

00:12:45,020 --> 00:12:50,060
names and and and what what food

00:12:47,779 --> 00:12:51,220
customers are eating and now we're doing

00:12:50,060 --> 00:12:54,740
the join operation

00:12:51,220 --> 00:12:57,860
so probably database would go through

00:12:54,740 --> 00:13:00,350
one table in a nice linear fashion that

00:12:57,860 --> 00:13:02,990
will have wonderful cache performance it

00:13:00,350 --> 00:13:05,779
will go one by one they would prefetch

00:13:02,990 --> 00:13:08,240
you know big table of big pages so it

00:13:05,779 --> 00:13:10,790
will nicely go through all of them but

00:13:08,240 --> 00:13:14,959
then woman has to use the foreign key to

00:13:10,790 --> 00:13:18,710
heat other table oops it's random it's

00:13:14,959 --> 00:13:21,110
random the core will be very very

00:13:18,710 --> 00:13:24,560
said it will do just a little bit of

00:13:21,110 --> 00:13:29,690
work and then 16 and a seconds of

00:13:24,560 --> 00:13:32,300
nothing now 16 and a seconds is small

00:13:29,690 --> 00:13:34,220
enough so that it is not worth to switch

00:13:32,300 --> 00:13:39,200
into a different friend because switch

00:13:34,220 --> 00:13:42,230
to a different thread is what half a

00:13:39,200 --> 00:13:44,600
microsecond or more so it's much bigger

00:13:42,230 --> 00:13:48,050
so core is being wasted

00:13:44,600 --> 00:13:51,080
of course there is hyper threading but

00:13:48,050 --> 00:13:54,500
normally on CPUs how many hyper threads

00:13:51,080 --> 00:13:56,660
are - - right - okay

00:13:54,500 --> 00:13:59,210
okay so so core will be slightly more

00:13:56,660 --> 00:14:01,910
happy because what it will do it will

00:13:59,210 --> 00:14:06,020
run until it hits the stall memory stall

00:14:01,910 --> 00:14:08,600
then it will start running the next next

00:14:06,020 --> 00:14:12,800
thread but if it hits the stall again we

00:14:08,600 --> 00:14:13,540
are stuck so what can we do to make it

00:14:12,800 --> 00:14:20,120
better

00:14:13,540 --> 00:14:23,510
luckily CPU hardware vendors provided us

00:14:20,120 --> 00:14:27,230
with prefetch instructions and this is

00:14:23,510 --> 00:14:31,130
an intrinsic with a bunch of Hills which

00:14:27,230 --> 00:14:34,160
allows us to tell to a CPU hey CPU we're

00:14:31,130 --> 00:14:36,200
interested in that memory location and

00:14:34,160 --> 00:14:40,910
we can give it a bunch of hints for

00:14:36,200 --> 00:14:43,100
example if I say here T 0 it means I

00:14:40,910 --> 00:14:47,240
actually want that value to be fetched

00:14:43,100 --> 00:14:49,100
from memory and then go into L 3 L 2 L 1

00:14:47,240 --> 00:14:53,630
because I wanted to be all throughout

00:14:49,100 --> 00:14:56,150
and NCAA the last option it's actually

00:14:53,630 --> 00:15:00,470
the least disturbing flavor of prefetch

00:14:56,150 --> 00:15:02,570
it's where I have important data in in

00:15:00,470 --> 00:15:05,870
my caches and I don't want to evict it

00:15:02,570 --> 00:15:08,090
instead I want to prefetch the value

00:15:05,870 --> 00:15:12,290
from that memory location as close to

00:15:08,090 --> 00:15:15,290
CPU as I want as possible without trying

00:15:12,290 --> 00:15:19,640
to disturb other levels of caching so

00:15:15,290 --> 00:15:22,430
this is what we will be trying to do we

00:15:19,640 --> 00:15:25,340
will try to do instruction stream

00:15:22,430 --> 00:15:29,840
interleaving because our join operation

00:15:25,340 --> 00:15:31,600
needs to do multiple lookups we will try

00:15:29,840 --> 00:15:34,390
to do something like that

00:15:31,600 --> 00:15:36,880
we will start the first search and then

00:15:34,390 --> 00:15:39,490
at the point where our binary search he

00:15:36,880 --> 00:15:41,590
used to just read the value from memory

00:15:39,490 --> 00:15:46,810
we will issue a prefetch instruction and

00:15:41,590 --> 00:15:48,430
then switch to a different different

00:15:46,810 --> 00:15:50,020
search that we need still need to do

00:15:48,430 --> 00:15:51,460
because the join operation requires

00:15:50,020 --> 00:15:55,480
multiple lookups

00:15:51,460 --> 00:15:58,120
and then we had that run until we hit we

00:15:55,480 --> 00:16:00,880
hit a memory read from a random location

00:15:58,120 --> 00:16:03,430
or what issue prefetch switch to the

00:16:00,880 --> 00:16:06,490
next one and then when we have enough of

00:16:03,430 --> 00:16:08,440
those hopefully by the time when we did

00:16:06,490 --> 00:16:09,040
the last one the first one will

00:16:08,440 --> 00:16:11,980
prefetched

00:16:09,040 --> 00:16:14,590
is already in memory sorry it's already

00:16:11,980 --> 00:16:16,690
nearly CPU so we can switch to the

00:16:14,590 --> 00:16:20,200
original one so essentially what we'll

00:16:16,690 --> 00:16:24,760
be doing will be doing hyper phrasing in

00:16:20,200 --> 00:16:27,400
software ok it's actually not that hard

00:16:24,760 --> 00:16:29,950
it's just you have to be very very

00:16:27,400 --> 00:16:33,700
meticulous about how you're going to do

00:16:29,950 --> 00:16:36,250
it so this red line shows roughly how

00:16:33,700 --> 00:16:38,350
we're going to split that function and

00:16:36,250 --> 00:16:41,410
try to turn into the state machine so

00:16:38,350 --> 00:16:44,710
here we'll have this star middle that is

00:16:41,410 --> 00:16:48,220
trying to read from memory and we'll

00:16:44,710 --> 00:16:50,200
just partition it well since we need to

00:16:48,220 --> 00:16:52,960
make the state machine we first need to

00:16:50,200 --> 00:16:55,630
extract all of the state we need that

00:16:52,960 --> 00:16:59,830
currently inside of the function so do

00:16:55,630 --> 00:17:02,770
we need first yeah we needed because

00:16:59,830 --> 00:17:06,540
it's it is used both in before the red

00:17:02,770 --> 00:17:13,570
line in after headline same with last

00:17:06,540 --> 00:17:16,949
middle length half value they were

00:17:13,570 --> 00:17:19,510
evaluated with it we are keep comparing

00:17:16,949 --> 00:17:21,400
ok and we need a state and we need the

00:17:19,510 --> 00:17:23,770
state so that we know which part of the

00:17:21,400 --> 00:17:26,980
algorithm we are before the read or or

00:17:23,770 --> 00:17:28,360
or afterwards and we will add and we

00:17:26,980 --> 00:17:31,390
will run the state machine was just to

00:17:28,360 --> 00:17:33,340
function initial one which executes the

00:17:31,390 --> 00:17:36,640
first time we enter the function and

00:17:33,340 --> 00:17:38,980
until we hit the read or prefetch

00:17:36,640 --> 00:17:41,080
instruction and then the one that will

00:17:38,980 --> 00:17:45,460
run the rest of the loop but again we

00:17:41,080 --> 00:17:46,930
will speed it up so that we we stop

00:17:45,460 --> 00:17:50,410
before trying to read from the middle

00:17:46,930 --> 00:17:57,730
issue British instruction and then get

00:17:50,410 --> 00:18:00,070
out so very straightforward this is the

00:17:57,730 --> 00:18:01,900
initial entry we copied all of the

00:18:00,070 --> 00:18:06,010
values that were before that before the

00:18:01,900 --> 00:18:09,310
wild loop then well if the range is

00:18:06,010 --> 00:18:10,750
empty we will bail out and we'll mark

00:18:09,310 --> 00:18:13,330
the state as not found

00:18:10,750 --> 00:18:16,470
otherwise we will do all of this stuff

00:18:13,330 --> 00:18:19,030
which happens before the read namely

00:18:16,470 --> 00:18:21,570
half divided by three

00:18:19,030 --> 00:18:24,250
we divided the length stored in half

00:18:21,570 --> 00:18:26,980
computed the middle say that we need to

00:18:24,250 --> 00:18:31,410
keep going and we issued the prefetch

00:18:26,980 --> 00:18:36,400
instruction I showed earlier great now

00:18:31,410 --> 00:18:41,170
Ron will do the rest of of what happens

00:18:36,400 --> 00:18:42,850
once we read so now the value we think

00:18:41,170 --> 00:18:46,150
that value have been pre fetched

00:18:42,850 --> 00:18:49,300
so now it is safe to say start middle so

00:18:46,150 --> 00:18:51,250
we got the middle key we compare it is

00:18:49,300 --> 00:18:54,970
it on this side is it on that side

00:18:51,250 --> 00:18:57,190
we did the early bailout and one

00:18:54,970 --> 00:19:00,370
difference here is that before in our

00:18:57,190 --> 00:19:02,170
binary search boo or the return value

00:19:00,370 --> 00:19:06,310
was indicating whether we found the

00:19:02,170 --> 00:19:08,800
value or not here I'm using the return

00:19:06,310 --> 00:19:11,740
value of Ron to simply indicate do we

00:19:08,800 --> 00:19:14,500
need to run it more or we're done and

00:19:11,740 --> 00:19:16,840
the state that will be store and found

00:19:14,500 --> 00:19:20,080
or not found will be the thing that will

00:19:16,840 --> 00:19:24,880
be telling us whether we found a value

00:19:20,080 --> 00:19:30,640
or not and then we will prefetch the

00:19:24,880 --> 00:19:32,740
value again and now yeah I think we

00:19:30,640 --> 00:19:37,350
covered all of it and we'll mark that

00:19:32,740 --> 00:19:43,030
the state has not found it's tedious but

00:19:37,350 --> 00:19:44,830
doable right okay I see one one hand

00:19:43,030 --> 00:19:48,760
okay good good

00:19:44,830 --> 00:19:50,500
so what one had no deal so and we want

00:19:48,760 --> 00:19:52,510
to run it roughly like this we will

00:19:50,500 --> 00:19:55,510
create a bunch of those state machines

00:19:52,510 --> 00:19:58,240
and then we'll say run and eat on this

00:19:55,510 --> 00:20:01,450
one so it will initialize this

00:19:58,240 --> 00:20:03,580
hit the prefetch then do the same for

00:20:01,450 --> 00:20:09,130
the second frame and keep going

00:20:03,580 --> 00:20:12,100
now we issue initiated and prefetches we

00:20:09,130 --> 00:20:14,380
go back and call run we hope that by the

00:20:12,100 --> 00:20:17,230
time would get back to the first one

00:20:14,380 --> 00:20:19,510
prefetch has already completed so we

00:20:17,230 --> 00:20:22,660
will run it again until we hit the

00:20:19,510 --> 00:20:25,690
prefetch go to the next one and say this

00:20:22,660 --> 00:20:28,480
one either found a value or reach the

00:20:25,690 --> 00:20:31,030
end of the algorithm so we're done

00:20:28,480 --> 00:20:33,760
so we consume the result in whatever

00:20:31,030 --> 00:20:36,400
fashion the database wants and replace

00:20:33,760 --> 00:20:40,180
that frame by writing in it now for huge

00:20:36,400 --> 00:20:44,470
number n plus one and we will keep going

00:20:40,180 --> 00:20:49,810
I will bore you with roughish showing

00:20:44,470 --> 00:20:55,120
the code how it looks like suddenly like

00:20:49,810 --> 00:20:56,860
that we create a vector frames we go

00:20:55,120 --> 00:21:00,580
through the list of all of the hookups

00:20:56,860 --> 00:21:02,650
we need to do and then if the state in a

00:21:00,580 --> 00:21:05,820
particular frame is not equal keep

00:21:02,650 --> 00:21:08,980
running we will initialize it otherwise

00:21:05,820 --> 00:21:10,390
we will call around until it's either

00:21:08,980 --> 00:21:13,390
found or not found

00:21:10,390 --> 00:21:16,630
you know if either of those cases will

00:21:13,390 --> 00:21:19,750
initialize it again and so forth so on

00:21:16,630 --> 00:21:23,050
and in the very end we need to deal with

00:21:19,750 --> 00:21:27,040
stragglers what it means is that we are

00:21:23,050 --> 00:21:29,770
run through entire list of MU cups we

00:21:27,040 --> 00:21:32,110
need to do but we still have frames that

00:21:29,770 --> 00:21:34,690
have serious searchers that haven't been

00:21:32,110 --> 00:21:38,050
haven't completed yet so there will be a

00:21:34,690 --> 00:21:42,120
time you loop at the end that will take

00:21:38,050 --> 00:21:46,720
care of those okay let's see how we did

00:21:42,120 --> 00:21:51,130
so with naive search we were getting 26

00:21:46,720 --> 00:21:55,660
nanoseconds per look up over the log of

00:21:51,130 --> 00:21:59,590
an array size now we're getting

00:21:55,660 --> 00:22:02,800
something we just make two times to 2.5

00:21:59,590 --> 00:22:05,080
times faster and that is with

00:22:02,800 --> 00:22:08,290
interleaving with 16 streams essentially

00:22:05,080 --> 00:22:11,059
I ran it with different amount of frames

00:22:08,290 --> 00:22:12,799
and until I find that

00:22:11,059 --> 00:22:18,110
number of frames which is sufficient for

00:22:12,799 --> 00:22:19,759
us to to do enough work between the

00:22:18,110 --> 00:22:23,419
prefetches so that by the time we go to

00:22:19,759 --> 00:22:28,600
the first one it's already there but

00:22:23,419 --> 00:22:31,850
it's nice but that's a lot of code

00:22:28,600 --> 00:22:34,190
moreover we did a very very simple

00:22:31,850 --> 00:22:36,139
algorithm buying the research now

00:22:34,190 --> 00:22:40,429
imagine we want to apply that technique

00:22:36,139 --> 00:22:44,350
to more complicated algorithm that is

00:22:40,429 --> 00:22:47,539
not manageable can we do it better

00:22:44,350 --> 00:22:48,139
yes we can and we can do it with

00:22:47,539 --> 00:22:52,029
coroutines

00:22:48,139 --> 00:22:54,499
so at the very beginning we talked about

00:22:52,029 --> 00:22:57,440
two common patterns people use with

00:22:54,499 --> 00:23:00,889
karajan's one was generators lazily

00:22:57,440 --> 00:23:04,840
produced sequences another isync tasks

00:23:00,889 --> 00:23:09,169
that await for some my sinkers are your

00:23:04,840 --> 00:23:12,249
completions so which are those two kind

00:23:09,169 --> 00:23:17,539
of problems this problem reminds you of

00:23:12,249 --> 00:23:21,529
who thinks is generators - who thinks

00:23:17,539 --> 00:23:24,309
it's a sync tasks a lot and you are

00:23:21,529 --> 00:23:27,860
right and you are right moreover that

00:23:24,309 --> 00:23:32,049
coroutine based TCP server will looked

00:23:27,860 --> 00:23:35,720
at last year it actually shown us almost

00:23:32,049 --> 00:23:38,799
exactly the kind of patterns we are

00:23:35,720 --> 00:23:43,490
going to use to deal with this problem

00:23:38,799 --> 00:23:47,240
because the top part which is a session

00:23:43,490 --> 00:23:50,360
where we await waiting for TCP read and

00:23:47,240 --> 00:23:51,860
then we want read competitive we do a

00:23:50,360 --> 00:23:54,830
write and then they read it again right

00:23:51,860 --> 00:23:55,759
again it has the property that this

00:23:54,830 --> 00:23:58,129
corrosion

00:23:55,759 --> 00:24:00,379
doesn't go anywhere while we are

00:23:58,129 --> 00:24:02,749
awaiting for the result and that is

00:24:00,379 --> 00:24:04,279
roughly what binary search is doing

00:24:02,749 --> 00:24:07,100
because buying the research you can

00:24:04,279 --> 00:24:10,279
think about it read from memory oh

00:24:07,100 --> 00:24:12,830
that's an ion that is in a year that

00:24:10,279 --> 00:24:15,200
takes a lot of time 60 nanoseconds

00:24:12,830 --> 00:24:18,080
without corrosion so fast you know one

00:24:15,200 --> 00:24:22,879
on second OS switching we can do a lot

00:24:18,080 --> 00:24:24,110
and one change before I switch to a

00:24:22,879 --> 00:24:25,760
coroutine I

00:24:24,110 --> 00:24:28,820
we'll change the way how we deal with

00:24:25,760 --> 00:24:32,299
the result so here we return the result

00:24:28,820 --> 00:24:36,140
as a boolean I want to return the result

00:24:32,299 --> 00:24:39,410
as in a form of a function object and if

00:24:36,140 --> 00:24:42,679
you notice alex deep enough won't and

00:24:39,410 --> 00:24:44,900
ago discovered that this callback kind

00:24:42,679 --> 00:24:47,600
are the best kind if you have a function

00:24:44,900 --> 00:24:49,670
object compiler can actually inline it

00:24:47,600 --> 00:24:52,040
inside of the algorithm and it will be

00:24:49,670 --> 00:24:57,320
very very very efficient so that's why

00:24:52,040 --> 00:25:00,919
we're doing it this way and and you know

00:24:57,320 --> 00:25:01,809
what I'm going to with it because this

00:25:00,919 --> 00:25:04,970
is it

00:25:01,809 --> 00:25:09,520
this is how long it would take us to

00:25:04,970 --> 00:25:13,370
convert the algorithm from a plain one

00:25:09,520 --> 00:25:15,530
into in inter corrosion essentially we

00:25:13,370 --> 00:25:17,960
have some kind of corrosion type we

00:25:15,530 --> 00:25:21,530
changed void into a corrosion type and

00:25:17,960 --> 00:25:25,910
then we replace the plain read into a

00:25:21,530 --> 00:25:29,030
waiting on a prefetch operation that

00:25:25,910 --> 00:25:31,150
will suspend the call team while there

00:25:29,030 --> 00:25:34,850
has been prefetched and then hopefully

00:25:31,150 --> 00:25:38,299
resumed at the right time when the value

00:25:34,850 --> 00:25:40,100
is already in memory and yes we need to

00:25:38,299 --> 00:25:42,919
the replace return with score returns

00:25:40,100 --> 00:25:45,380
there was a battle two years ago and the

00:25:42,919 --> 00:25:47,320
core attorneys have one so we need to

00:25:45,380 --> 00:25:51,290
write code returns and karajan's

00:25:47,320 --> 00:25:53,320
okay so that took care of the algorithm

00:25:51,290 --> 00:25:58,570
now we need to write an Orchestrator

00:25:53,320 --> 00:26:01,790
let's get back to our TCP based server

00:25:58,570 --> 00:26:05,690
so the bottom part actually has the

00:26:01,790 --> 00:26:07,880
pattern we want because unlike the

00:26:05,690 --> 00:26:09,890
session which gets suspended while

00:26:07,880 --> 00:26:13,790
waiting for the result of the read or

00:26:09,890 --> 00:26:16,460
write we don't want the server to be

00:26:13,790 --> 00:26:19,640
suspended while waiting for a particular

00:26:16,460 --> 00:26:23,500
session to work we actually would like

00:26:19,640 --> 00:26:27,530
to spawn as many sessions as we want and

00:26:23,500 --> 00:26:30,559
we want them to live independently from

00:26:27,530 --> 00:26:34,429
the server itself and in this case

00:26:30,559 --> 00:26:37,580
because having detached activity right

00:26:34,429 --> 00:26:39,980
in wild like the text read is horrible

00:26:37,580 --> 00:26:42,530
never never never do it here we're

00:26:39,980 --> 00:26:45,950
actually associating the coroutine with

00:26:42,530 --> 00:26:48,410
another context so in networking yes our

00:26:45,950 --> 00:26:50,300
context is the thing which tracks all of

00:26:48,410 --> 00:26:52,700
the activity which is currently in

00:26:50,300 --> 00:26:55,010
flight so in this case we're spawning

00:26:52,700 --> 00:26:57,140
and we'll make the corrosion independent

00:26:55,010 --> 00:27:00,080
of a server but still there is a way to

00:26:57,140 --> 00:27:02,570
join them in the end or cancel and at

00:27:00,080 --> 00:27:07,190
the shutdown and that brings me to

00:27:02,570 --> 00:27:09,470
roughly this shape of an Orchestrator so

00:27:07,190 --> 00:27:11,690
here instead of our context we will have

00:27:09,470 --> 00:27:15,650
a throttle arm because we potentially

00:27:11,690 --> 00:27:19,970
doing a million lookups we don't really

00:27:15,650 --> 00:27:23,120
want to launch a million co-routines so

00:27:19,970 --> 00:27:25,310
the throttle er will just have enough

00:27:23,120 --> 00:27:29,300
goroutines to keep the prefetcher

00:27:25,310 --> 00:27:31,550
pipeline busy so and this multi lookup

00:27:29,300 --> 00:27:34,580
get the concurrency parameter were given

00:27:31,550 --> 00:27:37,820
to a throttle layer and then throttler

00:27:34,580 --> 00:27:40,550
will spawn enough goroutines to keep it

00:27:37,820 --> 00:27:43,600
busy and in the end we will do the joint

00:27:40,550 --> 00:27:46,910
so another name for this pattern is

00:27:43,600 --> 00:27:49,580
fork/join you probably have seen it in

00:27:46,910 --> 00:27:52,820
CBB and others because what we're trying

00:27:49,580 --> 00:27:54,920
to do is to spawn a whole bunch of stuff

00:27:52,820 --> 00:27:59,000
and then in the end we want to wait

00:27:54,920 --> 00:28:01,760
until all of it is done so this is how

00:27:59,000 --> 00:28:04,280
we imagine the simplest possible way to

00:28:01,760 --> 00:28:08,780
solve this problem does it look simple

00:28:04,280 --> 00:28:10,250
to you yeah yeah because it's very close

00:28:08,780 --> 00:28:12,440
to what we are doing the first was this

00:28:10,250 --> 00:28:14,810
is a server and second the algorithm

00:28:12,440 --> 00:28:17,750
itself is almost the same as a naive

00:28:14,810 --> 00:28:20,290
version so now let's see how much will

00:28:17,750 --> 00:28:23,500
it take us to actually make it work and

00:28:20,290 --> 00:28:26,210
there is actually a quite good approach

00:28:23,500 --> 00:28:27,680
frequently I advise when people say hey

00:28:26,210 --> 00:28:28,310
you know how can I do this with

00:28:27,680 --> 00:28:31,580
coroutines

00:28:28,310 --> 00:28:33,710
I will say well first think of the best

00:28:31,580 --> 00:28:35,630
way of solving this problem just imagine

00:28:33,710 --> 00:28:37,700
imagine the syntax and then you can make

00:28:35,630 --> 00:28:40,850
it work so that's roughly what we were

00:28:37,700 --> 00:28:43,580
doing here we with sort of a way how how

00:28:40,850 --> 00:28:46,850
how would we write it in a deal way and

00:28:43,580 --> 00:28:50,390
then figure out a way to make it as

00:28:46,850 --> 00:28:51,310
correct so the first part we need to

00:28:50,390 --> 00:28:57,740
make

00:28:51,310 --> 00:29:00,890
prefetch an away table and a quick

00:28:57,740 --> 00:29:04,640
reminder for those who haven't seen the

00:29:00,890 --> 00:29:08,360
other goroutine stalks away table is the

00:29:04,640 --> 00:29:11,590
thing with with three functions and wait

00:29:08,360 --> 00:29:14,540
radio it's a span and the wait resume so

00:29:11,590 --> 00:29:18,260
compiler expanded into something like

00:29:14,540 --> 00:29:22,640
that first it asks hey you a rate of

00:29:18,260 --> 00:29:25,040
expression are you ready and if it's

00:29:22,640 --> 00:29:27,350
ready when it precedes straight to the

00:29:25,040 --> 00:29:30,560
end into a way to resume with which

00:29:27,350 --> 00:29:32,810
unpacks the result and the result of

00:29:30,560 --> 00:29:38,600
entire await expression is whatever a

00:29:32,810 --> 00:29:41,810
way to resume have returned now if it's

00:29:38,600 --> 00:29:44,170
not ready then compiler calls await

00:29:41,810 --> 00:29:46,700
suspend and give it as a parameter a

00:29:44,170 --> 00:29:51,530
handle to a core routine it manufactured

00:29:46,700 --> 00:29:54,320
and the handler for await suspend can

00:29:51,530 --> 00:29:56,240
then do something with it so that it can

00:29:54,320 --> 00:29:59,030
resume or destroy the core routine later

00:29:56,240 --> 00:30:01,550
wouldn't know the result is available or

00:29:59,030 --> 00:30:04,280
if it know that we have to cancel and

00:30:01,550 --> 00:30:09,070
and get rid of all of this stuff so

00:30:04,280 --> 00:30:11,510
let's do that available for prefetch so

00:30:09,070 --> 00:30:12,920
we're going to try to implement this

00:30:11,510 --> 00:30:17,480
prefetch array table

00:30:12,920 --> 00:30:18,160
sympathised on T so first we need to

00:30:17,480 --> 00:30:20,270
remember

00:30:18,160 --> 00:30:23,240
tu-tu-tu-tu-tu-tu to have it to grab a

00:30:20,270 --> 00:30:26,000
reference to the value right so whatever

00:30:23,240 --> 00:30:28,550
is best in the constructor will just

00:30:26,000 --> 00:30:30,260
store a reference reference is cheap

00:30:28,550 --> 00:30:34,720
right it's just an address we're not

00:30:30,260 --> 00:30:38,330
reading anything now at the very end

00:30:34,720 --> 00:30:40,520
when our await is satisfied we will just

00:30:38,330 --> 00:30:43,610
return that reference to whomever and

00:30:40,520 --> 00:30:45,650
hopefully he will actually use it for

00:30:43,610 --> 00:30:50,060
good namely to the reference and get the

00:30:45,650 --> 00:30:52,430
value that we were looking for now in

00:30:50,060 --> 00:30:54,650
this case if you look at the prefetch

00:30:52,430 --> 00:30:56,390
instruction it doesn't tell us whether

00:30:54,650 --> 00:30:58,790
the value is already in the cache

00:30:56,390 --> 00:31:00,500
moreover probably that will not be a

00:30:58,790 --> 00:31:02,900
very good instruction because it will

00:31:00,500 --> 00:31:04,610
take a while what hardware to figure out

00:31:02,900 --> 00:31:06,380
whether it's in the cat or not

00:31:04,610 --> 00:31:07,700
we want to be very fast we want to cycle

00:31:06,380 --> 00:31:12,200
a lot of corrosion so we'll just say

00:31:07,700 --> 00:31:15,320
nope prefetch is never ready we assume

00:31:12,200 --> 00:31:17,840
that it is always in memory so we need

00:31:15,320 --> 00:31:20,120
to go on a fast man path and in the

00:31:17,840 --> 00:31:22,610
suspend path it's quite simple actually

00:31:20,120 --> 00:31:26,690
we will call that intrinsic that I

00:31:22,610 --> 00:31:29,210
showed earlier give it an address of the

00:31:26,690 --> 00:31:32,299
memory location that we want to prefetch

00:31:29,210 --> 00:31:34,670
and in this case I gave it a hint

00:31:32,299 --> 00:31:36,890
because I didn't want to pollute any

00:31:34,670 --> 00:31:38,809
other caches I want to just we hit at

00:31:36,890 --> 00:31:39,760
random spots every time will be

00:31:38,809 --> 00:31:44,299
different

00:31:39,760 --> 00:31:46,940
give us straight to the CPU and now I

00:31:44,299 --> 00:31:49,309
want to put this protein you see there

00:31:46,940 --> 00:31:51,260
is coated handle that was passed to

00:31:49,309 --> 00:31:53,809
every suspend and I will put it into

00:31:51,260 --> 00:31:59,660
some queue that will put it at the very

00:31:53,809 --> 00:32:01,100
end of the queue now a couple of years

00:31:59,660 --> 00:32:03,549
ago when we are talking about different

00:32:01,100 --> 00:32:07,100
cooking flavors we talked about

00:32:03,549 --> 00:32:09,970
symmetric and asymmetric origins so

00:32:07,100 --> 00:32:13,610
asymmetrical routines they support only

00:32:09,970 --> 00:32:17,330
suspend and resume operation so in order

00:32:13,610 --> 00:32:19,970
for well when you suspend you return

00:32:17,330 --> 00:32:21,650
back to the color and the resume is a

00:32:19,970 --> 00:32:23,900
different operation that will resume you

00:32:21,650 --> 00:32:26,480
so there is this unequal relationship

00:32:23,900 --> 00:32:29,660
between the guy who called you and the

00:32:26,480 --> 00:32:32,870
court in the suspend the lettuce bandit

00:32:29,660 --> 00:32:36,020
there is also a symmetrical routine

00:32:32,870 --> 00:32:40,340
variety which allows direct kuru team to

00:32:36,020 --> 00:32:43,669
coroutine transfer more like like threat

00:32:40,340 --> 00:32:46,730
to threat context switch and even though

00:32:43,669 --> 00:32:48,679
functionally those kind of corruptions

00:32:46,730 --> 00:32:51,980
are equivalent because we can convert

00:32:48,679 --> 00:32:55,970
easily one into another there is a cost

00:32:51,980 --> 00:32:59,240
of conversion asymmetrical routine into

00:32:55,970 --> 00:33:02,210
a symmetric one and I actually observe

00:32:59,240 --> 00:33:04,970
that post right here because we work in

00:33:02,210 --> 00:33:08,630
nanoseconds this was my first

00:33:04,970 --> 00:33:11,320
implementation where I simply added a

00:33:08,630 --> 00:33:13,880
corrosion handle to the skill and then I

00:33:11,320 --> 00:33:16,490
returned control back to the scheduler

00:33:13,880 --> 00:33:18,410
which runs the loop which DQ's now the

00:33:16,490 --> 00:33:21,170
corrosion from the front

00:33:18,410 --> 00:33:24,770
and resumes it well that's about three

00:33:21,170 --> 00:33:27,350
cycles and three cycles is about 0.75

00:33:24,770 --> 00:33:29,770
nanoseconds on that machine and we want

00:33:27,350 --> 00:33:34,010
to pack as many cartoons as we want and

00:33:29,770 --> 00:33:36,770
I wanted to eliminate that luckily we

00:33:34,010 --> 00:33:38,960
have a provision in collectivities which

00:33:36,770 --> 00:33:41,470
allows symmetric protein to protein

00:33:38,960 --> 00:33:45,980
transfer so what we want to do here is

00:33:41,470 --> 00:33:47,900
this that way we essentially eliminated

00:33:45,980 --> 00:33:50,660
the scheduler scheduler became just a

00:33:47,900 --> 00:33:54,290
very very simple dump queue we're saying

00:33:50,660 --> 00:33:56,050
put ourselves at the very end of the

00:33:54,290 --> 00:33:59,870
queue

00:33:56,050 --> 00:34:02,690
get the core routine at the front of the

00:33:59,870 --> 00:34:05,060
queue and that is the core routine I

00:34:02,690 --> 00:34:09,050
want to symmetrically transfer control

00:34:05,060 --> 00:34:12,890
to so this becomes simple jump from this

00:34:09,050 --> 00:34:21,220
point to a different routine okay

00:34:12,890 --> 00:34:21,220
so Q is boring it's anybody can write it

00:34:21,880 --> 00:34:27,679
yeah and at the very end is just try pop

00:34:24,830 --> 00:34:30,410
and run that will keep keep trying

00:34:27,679 --> 00:34:33,500
popping the corrosions until until there

00:34:30,410 --> 00:34:35,120
is no corrosion anymore and resume on

00:34:33,500 --> 00:34:37,429
the collecting handle is the thing you

00:34:35,120 --> 00:34:43,610
use to resume the coroutine which is

00:34:37,429 --> 00:34:47,679
suspended any question on the queue no

00:34:43,610 --> 00:34:51,669
no that's okay

00:34:47,679 --> 00:34:53,960
let's see what else is left to implement

00:34:51,669 --> 00:34:57,260
okay now we need to implement a

00:34:53,960 --> 00:35:01,340
throttler let's look at the binary

00:34:57,260 --> 00:35:04,250
search again I lied I said it's a task

00:35:01,340 --> 00:35:05,900
void and task void is something which

00:35:04,250 --> 00:35:08,450
currently goes through standardization

00:35:05,900 --> 00:35:13,120
processes a zero overhead future we

00:35:08,450 --> 00:35:16,070
talked about it two years ago in the

00:35:13,120 --> 00:35:17,990
working under the covers talk well this

00:35:16,070 --> 00:35:21,830
is not it I am going to hack it a little

00:35:17,990 --> 00:35:24,410
bit so it will be a root task because I

00:35:21,830 --> 00:35:27,530
want a root test to be aware of the

00:35:24,410 --> 00:35:29,690
throttler and it's a Yacky part I will

00:35:27,530 --> 00:35:30,200
show how to fix it later but I need this

00:35:29,690 --> 00:35:32,690
so

00:35:30,200 --> 00:35:36,170
my apologies in advance so what

00:35:32,690 --> 00:35:38,599
throttler needs to know is well what is

00:35:36,170 --> 00:35:41,020
the concurrency factor how many how many

00:35:38,599 --> 00:35:47,750
core teams it want to keep concurrently

00:35:41,020 --> 00:35:50,540
and in the spawn it will check if it

00:35:47,750 --> 00:35:53,750
populated the key with enough proteins

00:35:50,540 --> 00:35:55,160
it will start running them so

00:35:53,750 --> 00:35:58,099
essentially here what we are doing we

00:35:55,160 --> 00:36:00,410
will put in the first pop pop in the

00:35:58,099 --> 00:36:04,880
first quarantine from the queue and

00:36:00,410 --> 00:36:06,559
resume it and then it will start jumping

00:36:04,880 --> 00:36:09,140
around from coroutine to corrosion

00:36:06,559 --> 00:36:11,930
because we saw the a wait right a wait

00:36:09,140 --> 00:36:13,579
for prefetch is actually doing all of

00:36:11,930 --> 00:36:17,150
the scheduling and don't keep running

00:36:13,579 --> 00:36:19,670
until one of the core routines run to

00:36:17,150 --> 00:36:22,790
completion and that will create a room

00:36:19,670 --> 00:36:26,000
in the queue that's why if the limit is

00:36:22,790 --> 00:36:29,020
zero I will start running the first

00:36:26,000 --> 00:36:32,480
coroutine law run everything else

00:36:29,020 --> 00:36:34,670
otherwise I will associate this core

00:36:32,480 --> 00:36:37,069
routine with this throttler so that it

00:36:34,670 --> 00:36:40,579
knows to tell me when it's done so that

00:36:37,069 --> 00:36:44,900
I can bump up the limit push it into the

00:36:40,579 --> 00:36:47,470
queue and decrement the limit and on

00:36:44,900 --> 00:36:49,640
task down EDL is the callback that

00:36:47,470 --> 00:36:52,069
essentially the core routine will call

00:36:49,640 --> 00:36:58,510
at the end that will increment the limit

00:36:52,069 --> 00:37:02,059
and the rest of boring functions so I

00:36:58,510 --> 00:37:04,040
needed to do a few tweaks to the zero

00:37:02,059 --> 00:37:08,420
overhead future and this actually shows

00:37:04,040 --> 00:37:11,480
those tweaks first I need to add set

00:37:08,420 --> 00:37:16,010
owner which will disassociate the

00:37:11,480 --> 00:37:17,780
quality from this from this awry type

00:37:16,010 --> 00:37:20,930
which is going to destroy the core

00:37:17,780 --> 00:37:23,390
routine and it's destructor and transfer

00:37:20,930 --> 00:37:25,490
the ownership to the throttler because

00:37:23,390 --> 00:37:26,809
now this will be the guy who will be

00:37:25,490 --> 00:37:31,040
running the coroutines and will be

00:37:26,809 --> 00:37:33,380
destroying them in the end and I will

00:37:31,040 --> 00:37:36,799
remember the throttler in the promise

00:37:33,380 --> 00:37:39,770
type and I change my final suspend from

00:37:36,799 --> 00:37:42,140
suspend always to suspend never which

00:37:39,770 --> 00:37:43,250
means the co-rotating will self destroy

00:37:42,140 --> 00:37:45,530
itself

00:37:43,250 --> 00:37:50,110
in the end and we'll let know the owner

00:37:45,530 --> 00:37:52,640
the task is done so arranged completion

00:37:50,110 --> 00:37:54,620
finally because I'm creating a million

00:37:52,640 --> 00:37:57,110
of corrosions million potentially

00:37:54,620 --> 00:37:59,270
college ends I will use a recycling

00:37:57,110 --> 00:38:01,820
allocator so if you overwrite an

00:37:59,270 --> 00:38:04,670
operator new and delete on the corrosion

00:38:01,820 --> 00:38:07,280
promise you can supply your own

00:38:04,670 --> 00:38:09,500
allocator and that that helps a little

00:38:07,280 --> 00:38:11,810
bit you know extra few hundred seconds

00:38:09,500 --> 00:38:14,530
and again every one of second counts

00:38:11,810 --> 00:38:21,860
when we are doing this kind of iteration

00:38:14,530 --> 00:38:27,830
so let's see what happened so we have 26

00:38:21,860 --> 00:38:31,430
minutes seconds for 9:30 we have 10 and

00:38:27,830 --> 00:38:34,550
a seconds for a handcrafted version with

00:38:31,430 --> 00:38:38,710
16 streams and do you think it will be

00:38:34,550 --> 00:38:38,710
better or worse with corrosions

00:38:39,190 --> 00:38:49,700
yes yes it's actually better so we're

00:38:45,410 --> 00:38:52,700
getting to stick almost 3.5 times faster

00:38:49,700 --> 00:38:59,690
performance and we have to have 20

00:38:52,700 --> 00:39:01,340
streams to get it there thank you so you

00:38:59,690 --> 00:39:01,700
all have to leave early we're almost at

00:39:01,340 --> 00:39:06,760
the end

00:39:01,700 --> 00:39:11,270
so negative overhead abstraction again

00:39:06,760 --> 00:39:13,910
so Alex stefanov when he was in one of

00:39:11,270 --> 00:39:15,920
his interviews when he was asked how to

00:39:13,910 --> 00:39:18,950
design good abstractions

00:39:15,920 --> 00:39:22,070
he explained that it's not like you

00:39:18,950 --> 00:39:23,900
invent abstractions from just your brain

00:39:22,070 --> 00:39:26,600
from thin air no you start from a

00:39:23,900 --> 00:39:29,450
concrete problem and then you look at

00:39:26,600 --> 00:39:30,350
the most efficient way to solve that

00:39:29,450 --> 00:39:34,070
problem

00:39:30,350 --> 00:39:36,650
you know today okay at that point you

00:39:34,070 --> 00:39:39,950
can try to invent abstraction and now

00:39:36,650 --> 00:39:42,530
recode that problem using this new

00:39:39,950 --> 00:39:45,560
abstraction and see how much overhead

00:39:42,530 --> 00:39:47,450
have you introduced if not wonderful we

00:39:45,560 --> 00:39:51,120
have a good abstraction try applying to

00:39:47,450 --> 00:39:54,930
some other pattern and so far so on

00:39:51,120 --> 00:39:57,510
so what happened with goroutines and

00:39:54,930 --> 00:40:00,810
this is not the first time is that when

00:39:57,510 --> 00:40:03,570
we take the best possible pattern we do

00:40:00,810 --> 00:40:06,300
today say with handcraft state machine

00:40:03,570 --> 00:40:10,020
and then recode them with coroutines for

00:40:06,300 --> 00:40:12,510
some unknown reason it becomes faster so

00:40:10,020 --> 00:40:16,230
it is not just abstraction penalty is a

00:40:12,510 --> 00:40:19,140
negative overhead abstraction and I try

00:40:16,230 --> 00:40:21,180
to understand why it happened so I was

00:40:19,140 --> 00:40:23,790
looking at this assembly because pattern

00:40:21,180 --> 00:40:26,520
of access for both handcraft state

00:40:23,790 --> 00:40:29,490
machine and the core routine basic

00:40:26,520 --> 00:40:33,360
machine it is identical I am writing

00:40:29,490 --> 00:40:35,820
them on the same array with the same set

00:40:33,360 --> 00:40:40,880
of values to look up one of them is

00:40:35,820 --> 00:40:43,290
faster and I think it happens because of

00:40:40,880 --> 00:40:43,730
when Quarantine transformation takes

00:40:43,290 --> 00:40:49,320
place

00:40:43,730 --> 00:40:51,720
so both Microsoft compiler clang and a

00:40:49,320 --> 00:40:54,330
GCC implementation which is currently in

00:40:51,720 --> 00:40:56,790
progress discovered that the most

00:40:54,330 --> 00:40:59,700
profitable way to do a core routine is

00:40:56,790 --> 00:41:02,070
to actually let the core routine be

00:40:59,700 --> 00:41:04,650
observed by a compiler as a normal

00:41:02,070 --> 00:41:08,070
function for as long as it can so that

00:41:04,650 --> 00:41:09,840
optimizer can simplify the body can do

00:41:08,070 --> 00:41:12,240
the constant propagation and do the

00:41:09,840 --> 00:41:14,430
common expression elimination like do

00:41:12,240 --> 00:41:16,410
all of the awesome stuff compilers do

00:41:14,430 --> 00:41:18,690
in-line things discover after in lying

00:41:16,410 --> 00:41:21,690
in that some some other stuff disappears

00:41:18,690 --> 00:41:23,940
and once you finish simplifying the body

00:41:21,690 --> 00:41:25,890
of the function then you do the

00:41:23,940 --> 00:41:27,960
transformation of the core routine into

00:41:25,890 --> 00:41:30,360
a state machine so you chop it up into

00:41:27,960 --> 00:41:32,970
small pieces and then you optimize the

00:41:30,360 --> 00:41:37,590
individual pieces so what happened here

00:41:32,970 --> 00:41:41,280
we humans we did the transformation of a

00:41:37,590 --> 00:41:44,660
function into a state machine to earlier

00:41:41,280 --> 00:41:47,070
we did it at this very high level before

00:41:44,660 --> 00:41:49,890
compiler had any chance of doing

00:41:47,070 --> 00:41:51,480
optimizations and once a function is

00:41:49,890 --> 00:41:53,880
chopped up into pieces it becomes a

00:41:51,480 --> 00:41:56,820
state machine compilers are not very

00:41:53,880 --> 00:42:00,030
good at optimizing state machines so

00:41:56,820 --> 00:42:01,860
there is my guess why it's actually

00:42:00,030 --> 00:42:04,330
faster if you look at this assembly you

00:42:01,860 --> 00:42:07,270
will see that the

00:42:04,330 --> 00:42:10,280
the handcrafted machine puts too much

00:42:07,270 --> 00:42:13,460
register pressure on a compiler it is

00:42:10,280 --> 00:42:16,310
too complicated code it starts spilling

00:42:13,460 --> 00:42:18,980
and reloading values into the memory and

00:42:16,310 --> 00:42:21,290
into the registers whereas coroutines

00:42:18,980 --> 00:42:23,510
after it's chopped up it becomes very

00:42:21,290 --> 00:42:25,250
very simple it does almost the same

00:42:23,510 --> 00:42:28,250
thing that you would have done if you're

00:42:25,250 --> 00:42:30,500
trying to write this in assembly so

00:42:28,250 --> 00:42:33,619
that's not a core routine for you we're

00:42:30,500 --> 00:42:37,780
taking advantage of the fact that the

00:42:33,619 --> 00:42:41,030
switching from one column to another is

00:42:37,780 --> 00:42:44,770
taken less than a nanosecond and in some

00:42:41,030 --> 00:42:47,810
cases zero in case coroutine is in mind

00:42:44,770 --> 00:42:49,940
unlike for example fibres or stack focal

00:42:47,810 --> 00:42:51,800
routines which takes from 10 to 15

00:42:49,940 --> 00:42:55,760
milliseconds to switch depending on the

00:42:51,800 --> 00:42:58,700
OS architecture CPU kind and how many

00:42:55,760 --> 00:43:00,890
registers they have to save and restore

00:42:58,700 --> 00:43:02,750
when we do the context switch and

00:43:00,890 --> 00:43:04,609
naturally you cannot do that with

00:43:02,750 --> 00:43:08,050
threads because there were talking

00:43:04,609 --> 00:43:12,230
weather can you know close to a

00:43:08,050 --> 00:43:19,460
microsecond switch so this approach

00:43:12,230 --> 00:43:23,089
allows you to to to to deal with memory

00:43:19,460 --> 00:43:27,470
latency in those cases where nothing can

00:43:23,089 --> 00:43:29,770
help you no more packing of your data in

00:43:27,470 --> 00:43:32,210
a cache aware data structures can help

00:43:29,770 --> 00:43:34,609
nothing you hit the memory at random

00:43:32,210 --> 00:43:37,670
spots well if you have more than one

00:43:34,609 --> 00:43:40,220
look up you can try to split your code

00:43:37,670 --> 00:43:44,740
into several streams and then use

00:43:40,220 --> 00:43:47,450
coroutines to interleave them so

00:43:44,740 --> 00:43:49,369
coroutines current status proposal is

00:43:47,450 --> 00:43:53,660
going through the c++ standardization

00:43:49,369 --> 00:43:56,390
committee maybe it will be in c++ 20 we

00:43:53,660 --> 00:44:00,080
have up limitations in clang and a

00:43:56,390 --> 00:44:05,420
Visual Studio GCC implementation is in

00:44:00,080 --> 00:44:08,300
progress Nathan yes but yes progress

00:44:05,420 --> 00:44:10,790
he's I see he is not in there no he's

00:44:08,300 --> 00:44:12,380
leaving the room oh no he's coming here

00:44:10,790 --> 00:44:14,630
extra and so I think we have

00:44:12,380 --> 00:44:16,250
confirmation that UCC implementation is

00:44:14,630 --> 00:44:19,670
in progress

00:44:16,250 --> 00:44:21,110
and last year we had a talk where we

00:44:19,670 --> 00:44:23,300
talked about naked corrosions

00:44:21,110 --> 00:44:27,140
and that was one of the complaints that

00:44:23,300 --> 00:44:30,140
co-routines are just a compiler feature

00:44:27,140 --> 00:44:33,680
you always have to come up with your own

00:44:30,140 --> 00:44:36,590
types to play with them no longer naked

00:44:33,680 --> 00:44:38,060
we have the task type going through

00:44:36,590 --> 00:44:42,050
standardization we have the generator

00:44:38,060 --> 00:44:44,119
type that is supposed to cow in San

00:44:42,050 --> 00:44:48,430
Diego which is just in a few months and

00:44:44,119 --> 00:44:50,210
a bunch of wonderful wonderful wonderful

00:44:48,430 --> 00:44:56,420
algorithms that work on the weight of

00:44:50,210 --> 00:44:58,910
all types that will help to and this is

00:44:56,420 --> 00:45:02,510
just a reminder I am told to put this

00:44:58,910 --> 00:45:06,080
slide so it's there are other useful

00:45:02,510 --> 00:45:11,710
talk and we're at the end of Wednesday

00:45:06,080 --> 00:45:11,710
so I'll put it back again so questions

00:45:17,080 --> 00:45:22,700
yeah so go I just want correct starting

00:45:20,750 --> 00:45:25,280
and implementations I Facebook's hired a

00:45:22,700 --> 00:45:28,580
TTC engineer to work on this it started

00:45:25,280 --> 00:45:30,500
a week and a half ago so that we have a

00:45:28,580 --> 00:45:32,869
design document that I've been working

00:45:30,500 --> 00:45:35,690
with goron and the like so it's not from

00:45:32,869 --> 00:45:38,240
a standing start if you go to the GTC

00:45:35,690 --> 00:45:40,369
wiki page there's now a new project page

00:45:38,240 --> 00:45:43,360
where you can see what's happening but

00:45:40,369 --> 00:45:45,560
it'll be a while before there's a result

00:45:43,360 --> 00:45:46,070
wonderful do I need to repeat the

00:45:45,560 --> 00:45:49,280
question

00:45:46,070 --> 00:45:53,150
Oh excellent

00:45:49,280 --> 00:45:55,100
well it's awesome so I was not lying

00:45:53,150 --> 00:45:55,730
this is the implementation is in

00:45:55,100 --> 00:45:57,590
progress

00:45:55,730 --> 00:45:59,630
even though it's only week and a half

00:45:57,590 --> 00:46:05,600
it's still awesome it's still in

00:45:59,630 --> 00:46:08,180
progress so what's the performance of

00:46:05,600 --> 00:46:10,850
the Curtin implementation like without

00:46:08,180 --> 00:46:13,130
the allocator and to what extent does

00:46:10,850 --> 00:46:15,680
that allocator rely on the fact that all

00:46:13,130 --> 00:46:17,600
of those stack frames are expected to be

00:46:15,680 --> 00:46:19,940
exactly the same size ie

00:46:17,600 --> 00:46:23,210
if you were and in a place where you

00:46:19,940 --> 00:46:29,780
couldn't rely on that would it speed up

00:46:23,210 --> 00:46:32,810
be similar so I think it was about 10

00:46:29,780 --> 00:46:35,660
milliseconds so if I was surprised how

00:46:32,810 --> 00:46:39,410
well malloc actually behaved I didn't

00:46:35,660 --> 00:46:41,510
use TCL I just used very dumb exactly

00:46:39,410 --> 00:46:44,210
allocator but even without it

00:46:41,510 --> 00:46:46,040
malik was doing an amazing job but I

00:46:44,210 --> 00:46:48,890
wouldn't recommend because if you are

00:46:46,040 --> 00:46:51,890
going from nanoseconds you you better

00:46:48,890 --> 00:46:59,359
use some some some good recycling

00:46:51,890 --> 00:47:01,070
allocator for those of us who want to

00:46:59,359 --> 00:47:02,830
write applications that other people are

00:47:01,070 --> 00:47:06,710
going to run on the variety of machines

00:47:02,830 --> 00:47:09,680
that may have more cores or less cores

00:47:06,710 --> 00:47:11,210
or more cash with us cache what

00:47:09,680 --> 00:47:15,130
facilities are there going to be to make

00:47:11,210 --> 00:47:21,560
the throttle errs more well behaved

00:47:15,130 --> 00:47:24,080
without hand tuning them well with the

00:47:21,560 --> 00:47:26,660
current instruction sets as they exist

00:47:24,080 --> 00:47:29,370
today right that will not give you

00:47:26,660 --> 00:47:34,020
visibility of say how busy

00:47:29,370 --> 00:47:37,290
guess you have to tune it my hand but

00:47:34,020 --> 00:47:40,050
I've been talking to some intro

00:47:37,290 --> 00:47:42,600
engineers what kind of other facilities

00:47:40,050 --> 00:47:44,250
we could have for example there could be

00:47:42,600 --> 00:47:47,700
an elegant this is speed boiling it's

00:47:44,250 --> 00:47:48,990
not real no mmm nothing has happened

00:47:47,700 --> 00:47:51,150
what potential you could have an

00:47:48,990 --> 00:47:55,290
instruction can tell you hey prefetcher

00:47:51,150 --> 00:47:58,920
is is that busy and as you know you need

00:47:55,290 --> 00:48:01,080
to feed more to it and if it's if it's

00:47:58,920 --> 00:48:02,910
completely busy well then you don't have

00:48:01,080 --> 00:48:06,200
to spawn anymore it will not help you at

00:48:02,910 --> 00:48:08,730
all so this kind of help another one

00:48:06,200 --> 00:48:10,350
somebody suggested was to have a perfect

00:48:08,730 --> 00:48:13,260
instruction that will give you a hint

00:48:10,350 --> 00:48:15,720
whether the value is over the stuff

00:48:13,260 --> 00:48:17,580
somewhere close but that is likely to be

00:48:15,720 --> 00:48:19,050
an expense a more expensive instructor

00:48:17,580 --> 00:48:20,940
than a buoyant professor but you don't

00:48:19,050 --> 00:48:24,300
have to go and check in a bunch of

00:48:20,940 --> 00:48:28,260
places I would be even be happy to let

00:48:24,300 --> 00:48:30,390
the thrall or would ever initialize

00:48:28,260 --> 00:48:32,760
itself and run some kind of you know

00:48:30,390 --> 00:48:34,770
test once at the start of my program and

00:48:32,760 --> 00:48:38,060
then use that data if that's you know

00:48:34,770 --> 00:48:41,060
another way that this library could go

00:48:38,060 --> 00:48:41,060
absolutely

00:48:57,049 --> 00:49:00,919
I am interested if my own data

00:48:59,299 --> 00:49:02,630
structures and algorithms are cache

00:49:00,919 --> 00:49:04,880
friendly can you comment on the

00:49:02,630 --> 00:49:09,979
profiling toolkit you used to show your

00:49:04,880 --> 00:49:13,189
lane see I used births that it's Linux

00:49:09,979 --> 00:49:15,679
two births that to show me the info

00:49:13,189 --> 00:49:18,709
counters the CPU counters first that

00:49:15,679 --> 00:49:23,539
what was it called pure space yeah Dorf

00:49:18,709 --> 00:49:25,459
Oh sad okay are PE RF birth - it has a

00:49:23,539 --> 00:49:28,279
bunch of very useful primitives and if

00:49:25,459 --> 00:49:30,529
you want to look in more details I think

00:49:28,279 --> 00:49:34,669
Chandler has been given a couple of

00:49:30,529 --> 00:49:37,819
talks last EP pecan going nowhere faster

00:49:34,669 --> 00:49:41,929
and the previous one before that he

00:49:37,819 --> 00:49:44,659
showed how to use tor to to get the

00:49:41,929 --> 00:49:47,479
information about about caches about

00:49:44,659 --> 00:49:50,599
stalls from dense those back-end stalls

00:49:47,479 --> 00:49:53,359
your stat is a wonderful thing it also

00:49:50,599 --> 00:49:55,579
works as a provider you can say perfect

00:49:53,359 --> 00:49:58,789
chord and then per free part and you

00:49:55,579 --> 00:50:03,049
will see wonderful text-based UI which

00:49:58,789 --> 00:50:05,119
is pretty decent and on Windows you

00:50:03,049 --> 00:50:05,599
would use something like V tools but you

00:50:05,119 --> 00:50:08,119
have to

00:50:05,599 --> 00:50:12,399
is it VB Tunes right into but you have

00:50:08,119 --> 00:50:15,739
to pay on Linux it's cheap it's nothing

00:50:12,399 --> 00:50:18,199
so this looks super awesome but what if

00:50:15,739 --> 00:50:20,029
you put a breakpoint inside of a co

00:50:18,199 --> 00:50:23,259
routine and look at the call stack and

00:50:20,029 --> 00:50:27,309
in T bugger like is it a nightmare well

00:50:23,259 --> 00:50:31,939
if you put a breakpoint into equality

00:50:27,309 --> 00:50:39,739
you will see roughly in let me get to

00:50:31,939 --> 00:50:42,169
that okay hold on okay anyway it's too

00:50:39,739 --> 00:50:43,609
far to get to the code I want to show

00:50:42,169 --> 00:50:49,279
but what it will show you you will have

00:50:43,609 --> 00:50:50,779
a your binary search outer function

00:50:49,279 --> 00:50:53,649
right

00:50:50,779 --> 00:50:58,009
and then that function calls resume and

00:50:53,649 --> 00:50:59,719
that resume you will be in some in a

00:50:58,009 --> 00:51:01,399
core team so you'll have just two

00:50:59,719 --> 00:51:06,529
functions on the stack you will see a

00:51:01,399 --> 00:51:09,769
function the the multi lookup and the

00:51:06,529 --> 00:51:10,530
next one will be a core routine at that

00:51:09,769 --> 00:51:13,050
point

00:51:10,530 --> 00:51:16,400
it is currently executed so there's

00:51:13,050 --> 00:51:19,290
knowledge of like sort of impenetrable

00:51:16,400 --> 00:51:22,200
functions deep in some standard library

00:51:19,290 --> 00:51:24,870
in the Cossack no no every time you look

00:51:22,200 --> 00:51:25,860
you will see a normal thread and you

00:51:24,870 --> 00:51:30,420
will see the currently executing

00:51:25,860 --> 00:51:34,010
coroutine and what function invoked it

00:51:30,420 --> 00:51:36,930
and in this example it will be just the

00:51:34,010 --> 00:51:38,700
multi lookup and then a quarantine where

00:51:36,930 --> 00:51:41,100
it's suspended and if you want to see

00:51:38,700 --> 00:51:43,290
where other quarantines are then you

00:51:41,100 --> 00:51:45,870
need to go and look at the queue which

00:51:43,290 --> 00:51:48,960
has gotten handles and in Microsoft

00:51:45,870 --> 00:51:51,450
debugger we will actually tell you what

00:51:48,960 --> 00:51:55,740
functional points at what suspend point

00:51:51,450 --> 00:51:58,440
it points but we have plans both in M

00:51:55,740 --> 00:52:01,620
SVC and hopefully have extensions for

00:51:58,440 --> 00:52:03,150
other debuggers that will show you as if

00:52:01,620 --> 00:52:05,640
with the call stack so we'll show you a

00:52:03,150 --> 00:52:07,530
here is the coroutine suspended here and

00:52:05,640 --> 00:52:12,270
here's all of the locals it has at the

00:52:07,530 --> 00:52:14,430
moment right now you cannot examine

00:52:12,270 --> 00:52:16,520
goroutine handle in this fashion unless

00:52:14,430 --> 00:52:21,390
it's the currently executed corrosion

00:52:16,520 --> 00:52:24,510
thank you I'd like to know if there is

00:52:21,390 --> 00:52:28,560
anything to think about in terms of what

00:52:24,510 --> 00:52:32,250
the prefetch intrinsic does with regards

00:52:28,560 --> 00:52:34,650
to C++ memory model is there anything

00:52:32,250 --> 00:52:38,730
funky there with like a choir release

00:52:34,650 --> 00:52:41,400
semantics is it possible not to see the

00:52:38,730 --> 00:52:44,450
results of some memory operations done

00:52:41,400 --> 00:52:49,200
by some other thread

00:52:44,450 --> 00:52:52,460
well prefetch is just a hint right so

00:52:49,200 --> 00:52:56,220
the actual operation we are doing is

00:52:52,460 --> 00:52:58,380
dereference that is where we will start

00:52:56,220 --> 00:53:00,240
worrying about about the semantics it's

00:52:58,380 --> 00:53:02,820
we'll start trying to get that value

00:53:00,240 --> 00:53:06,150
from a cache of a memory prefetch is

00:53:02,820 --> 00:53:09,030
just a hint to a CPU hey try to get me a

00:53:06,150 --> 00:53:10,830
value the real work will be done when we

00:53:09,030 --> 00:53:12,810
also did not real work but at least the

00:53:10,830 --> 00:53:15,260
one that that have all of that semantics

00:53:12,810 --> 00:53:18,630
associated it is the actual dereference

00:53:15,260 --> 00:53:20,460
value so the memory access itself should

00:53:18,630 --> 00:53:22,320
be the subject of the same limitations

00:53:20,460 --> 00:53:23,360
as applies to any other program outside

00:53:22,320 --> 00:53:28,160
of coroutine

00:53:23,360 --> 00:53:31,130
case like putting memory barriers and so

00:53:28,160 --> 00:53:33,970
on actually I think I start I think I

00:53:31,130 --> 00:53:36,860
reapplied incorrectly to your to your

00:53:33,970 --> 00:53:41,720
question in a sense that yes we need to

00:53:36,860 --> 00:53:45,620
make sure that the prefetch is not moved

00:53:41,720 --> 00:53:47,330
around the actual read iteration and I'm

00:53:45,620 --> 00:53:52,000
not sure whether it's hardware caching

00:53:47,330 --> 00:53:55,190
it or it is an intrinsic which is marked

00:53:52,000 --> 00:53:59,210
somehow in the in the backend which

00:53:55,190 --> 00:54:03,770
prevents reordering and just another

00:53:59,210 --> 00:54:05,720
question it it sounds like execution ts

00:54:03,770 --> 00:54:07,850
is something that you don't really need

00:54:05,720 --> 00:54:09,740
to integrate with or care about because

00:54:07,850 --> 00:54:12,530
you're completely outside of the

00:54:09,740 --> 00:54:15,680
whatever threading model execution model

00:54:12,530 --> 00:54:17,210
is going to be in C++ because it seems

00:54:15,680 --> 00:54:21,020
to be like completely unrelated with

00:54:17,210 --> 00:54:25,190
anything threading right yes yes so

00:54:21,020 --> 00:54:27,920
coldians love executor x' carotenes love

00:54:25,190 --> 00:54:32,090
network in JS go routines interact with

00:54:27,920 --> 00:54:34,910
other things but by itself they do not

00:54:32,090 --> 00:54:38,120
require those facilities but if you want

00:54:34,910 --> 00:54:42,230
to interrupt interoperate with them they

00:54:38,120 --> 00:54:45,140
can and there is a paper in the if you

00:54:42,230 --> 00:54:47,360
know where to find the standardization

00:54:45,140 --> 00:54:51,350
mailings there is a paper of quarantine

00:54:47,360 --> 00:54:55,160
impact on library facilities you being

00:54:51,350 --> 00:54:56,840
this name you will see how it's supposed

00:54:55,160 --> 00:55:02,840
to interact with executor switch

00:54:56,840 --> 00:55:06,800
networking and other things so in my day

00:55:02,840 --> 00:55:08,960
job we use like a bunch of fibro

00:55:06,800 --> 00:55:12,320
managers to run on different threads and

00:55:08,960 --> 00:55:16,280
basically serve connections with clients

00:55:12,320 --> 00:55:19,130
and the reason a pain to use mutexes and

00:55:16,280 --> 00:55:23,600
statics because they can't grab log

00:55:19,130 --> 00:55:26,030
somewhere inside and like they basically

00:55:23,600 --> 00:55:28,010
block all the fibers on the same thread

00:55:26,030 --> 00:55:31,010
is it going to be the same with

00:55:28,010 --> 00:55:33,140
co-routines or you're going because like

00:55:31,010 --> 00:55:34,670
for fibers for you special fiber

00:55:33,140 --> 00:55:38,540
matrixes

00:55:34,670 --> 00:55:41,180
okay so one of the fundamental problems

00:55:38,540 --> 00:55:43,970
why you don't want to migrate fibers

00:55:41,180 --> 00:55:47,750
from one thread to another is related

00:55:43,970 --> 00:55:52,790
how thread-local storage code is

00:55:47,750 --> 00:55:55,760
generated so compiler is unaware of well

00:55:52,790 --> 00:55:57,590
at what point of your function you can

00:55:55,760 --> 00:56:01,550
have the fiber switch it will also

00:55:57,590 --> 00:56:05,690
switch the TLS so if you go to gut bolt

00:56:01,550 --> 00:56:08,060
and compile some arm code that uses TLS

00:56:05,690 --> 00:56:11,060
you will observe it it will cache in

00:56:08,060 --> 00:56:14,420
some non vult volatile registers the

00:56:11,060 --> 00:56:17,240
pages of TLS and thus you will be either

00:56:14,420 --> 00:56:21,200
in garbage or corrupt in the memory if

00:56:17,240 --> 00:56:23,270
the switch occurs kurutsu control flight

00:56:21,200 --> 00:56:26,150
the stack focused a class called even

00:56:23,270 --> 00:56:28,100
like like this suspension point is

00:56:26,150 --> 00:56:28,670
called out it is observable by the

00:56:28,100 --> 00:56:32,900
compiler

00:56:28,670 --> 00:56:36,140
so no TLS caching is performed now if

00:56:32,900 --> 00:56:39,530
you are actively trying to defeat or or

00:56:36,140 --> 00:56:42,110
write a bug you can but you can never

00:56:39,530 --> 00:56:45,110
get into a situation where you are using

00:56:42,110 --> 00:56:48,160
the facility like magic statics or maybe

00:56:45,110 --> 00:56:50,630
a thread catch an alligator or

00:56:48,160 --> 00:56:53,930
underscore value which is Undercovers

00:56:50,630 --> 00:56:56,510
uses the magic statics you will not end

00:56:53,930 --> 00:56:59,750
up in the case where you will trash the

00:56:56,510 --> 00:57:02,840
memory or or or or or read garbage with

00:56:59,750 --> 00:57:04,930
scorpions so they are safe to migrate as

00:57:02,840 --> 00:57:08,030
long as you're not actively writing bugs

00:57:04,930 --> 00:57:11,930
see one follow-up question what about

00:57:08,030 --> 00:57:15,800
your like primitives of STL are they

00:57:11,930 --> 00:57:19,640
going to be like variances which support

00:57:15,800 --> 00:57:22,160
coroutines okay so we have at the moment

00:57:19,640 --> 00:57:24,680
we don't have any synchronous are in the

00:57:22,160 --> 00:57:26,900
standard but there is a network in TS

00:57:24,680 --> 00:57:31,010
which is in flight which might land in

00:57:26,900 --> 00:57:33,980
20 or much might land slightly later so

00:57:31,010 --> 00:57:35,930
that code that network engineers will be

00:57:33,980 --> 00:57:38,930
interacting nicely with coroutines and

00:57:35,930 --> 00:57:42,110
if you look at CPP con talk called naked

00:57:38,930 --> 00:57:44,210
contains life from here last Vivek on

00:57:42,110 --> 00:57:47,240
there will be a demonstration how you

00:57:44,210 --> 00:57:48,319
can hook up networking I think retire

00:57:47,240 --> 00:57:50,259
and code

00:57:48,319 --> 00:57:52,999
I wish that I'm talking about like

00:57:50,259 --> 00:57:55,880
iostream for example well are your

00:57:52,999 --> 00:57:57,589
streams are not a synchronous yet when

00:57:55,880 --> 00:58:05,449
there will be we can start talking about

00:57:57,589 --> 00:58:08,929
goroutines as I understand the time when

00:58:05,449 --> 00:58:13,189
we await the result from prefetch

00:58:08,929 --> 00:58:19,459
depends on the size of should I talk to

00:58:13,189 --> 00:58:32,170
you so if it's not enough feature to

00:58:19,459 --> 00:58:37,359
between the prefetch and when it's real

00:58:32,170 --> 00:58:43,179
best of most you compare two

00:58:37,359 --> 00:58:49,630
realizations but in one case was sixteen

00:58:43,179 --> 00:58:52,309
sixteen streams and the case 20 frames

00:58:49,630 --> 00:58:59,150
working this all that it's on same

00:58:52,309 --> 00:59:02,119
amount of stamps okay so what happens is

00:58:59,150 --> 00:59:08,539
that the handcrafted state machine we're

00:59:02,119 --> 00:59:14,779
doing more work so there are four I only

00:59:08,539 --> 00:59:19,219
can launch it does more it consumes more

00:59:14,779 --> 00:59:26,599
cycles in that little squiggly area here

00:59:19,219 --> 00:59:28,839
sorry okay so this squiggly where we

00:59:26,599 --> 00:59:33,589
actually do do useful work between the

00:59:28,839 --> 00:59:38,239
stalls it was doing more work there are

00:59:33,589 --> 00:59:42,289
four I needed less of them to consume

00:59:38,239 --> 00:59:47,779
this empty space between between the

00:59:42,289 --> 00:59:50,719
squigglies whereas with compiler

00:59:47,779 --> 00:59:51,289
generated state machines that I did from

00:59:50,719 --> 00:59:53,299
co-routines

00:59:51,289 --> 00:59:55,849
it was actually using a less instruction

00:59:53,299 --> 00:59:59,540
to do the same amount of work so I was

00:59:55,849 --> 01:00:03,460
able to feed more co-routines to do

00:59:59,540 --> 01:00:06,880
more concurrency but III determined that

01:00:03,460 --> 01:00:10,520
empirically I just run them with

01:00:06,880 --> 01:00:12,820
different concurrency numbers and so

01:00:10,520 --> 01:00:17,210
which one gave the best performance

01:00:12,820 --> 01:00:19,550
thank you so I I wasn't trying on

01:00:17,210 --> 01:00:23,240
purpose to make the handcar to state

01:00:19,550 --> 01:00:27,520
machine wars it was actually the fastest

01:00:23,240 --> 01:00:27,520
I mean I bought what was 16 of them I

01:00:28,600 --> 01:00:33,500
wanted to ask about the binary search

01:00:31,450 --> 01:00:36,020
try to make it quick

01:00:33,500 --> 01:00:37,930
so in order to know what else you would

01:00:36,020 --> 01:00:40,730
need wouldn't you need to do some

01:00:37,930 --> 01:00:43,340
redundant work and bring in some stuff

01:00:40,730 --> 01:00:45,380
you might so to know which thing you're

01:00:43,340 --> 01:00:47,930
gonna need next you need to resolve the

01:00:45,380 --> 01:00:51,170
current so check so you need to sort of

01:00:47,930 --> 01:00:55,250
resolve do work that you never would

01:00:51,170 --> 01:00:57,370
actually need right well so as in and

01:00:55,250 --> 01:01:02,450
prefetch stuff that you wouldn't really

01:00:57,370 --> 01:01:05,330
require if you would just do it right

01:01:02,450 --> 01:01:08,690
like this linearly well this is exactly

01:01:05,330 --> 01:01:11,090
this sad situation because you if you

01:01:08,690 --> 01:01:13,730
knew in advance that you need that that

01:01:11,090 --> 01:01:17,090
there there and that one you can profess

01:01:13,730 --> 01:01:19,460
a whole bunch of them but you can't you

01:01:17,090 --> 01:01:21,740
first have to prefetch a value before

01:01:19,460 --> 01:01:25,220
you know where to go or in lettuce what

01:01:21,740 --> 01:01:27,710
make this this problem very set problem

01:01:25,220 --> 01:01:29,930
unless you do prefetch and combine with

01:01:27,710 --> 01:01:32,630
instruction stream interleaving my point

01:01:29,930 --> 01:01:36,500
is that you're pulling more data into

01:01:32,630 --> 01:01:38,600
the memory evacuating more stuff out of

01:01:36,500 --> 01:01:42,170
the memory than this thing right in some

01:01:38,600 --> 01:01:46,820
sense so in my testing I've been using

01:01:42,170 --> 01:01:49,670
the prefetch NTA which is trying to move

01:01:46,820 --> 01:01:52,700
the value directly into l1 cache without

01:01:49,670 --> 01:01:54,650
touching l2 + 3 so supposedly say if

01:01:52,700 --> 01:01:57,440
your database you have some real data

01:01:54,650 --> 01:02:00,290
you want to do work with and index is

01:01:57,440 --> 01:02:02,240
just a way to get to the data so in this

01:02:00,290 --> 01:02:05,900
case I'm trying to be very frugal and

01:02:02,240 --> 01:02:09,290
not leaked stuff from from big caches

01:02:05,900 --> 01:02:11,330
which have data and use prefetch MTA ok

01:02:09,290 --> 01:02:13,330
one was that can you put the slide back

01:02:11,330 --> 01:02:15,580
with all the presentations

01:02:13,330 --> 01:02:17,800
sayin which one this slide with all the

01:02:15,580 --> 01:02:21,280
presentation times you had a slide that

01:02:17,800 --> 01:02:25,450
says all the other talks yes sorry

01:02:21,280 --> 01:02:31,870
second-to-last this one nope the last

01:02:25,450 --> 01:02:38,230
the last one and hold on I think I know

01:02:31,870 --> 01:02:40,440
the better faster way to get there this

01:02:38,230 --> 01:02:40,440
one

01:02:40,610 --> 01:02:50,120
I wonder if to clarify the the question

01:02:46,190 --> 01:02:52,310
from before you you weren't optimizing a

01:02:50,120 --> 01:02:54,980
single search to be faster you were

01:02:52,310 --> 01:02:56,960
optimizing many searches to happen in at

01:02:54,980 --> 01:02:58,880
less amount of time so it's it wasn't

01:02:56,960 --> 01:03:00,680
the single search that he was

01:02:58,880 --> 01:03:03,350
prefetching he was just interleaving

01:03:00,680 --> 01:03:04,190
many other searches oh yeah thank you

01:03:03,350 --> 01:03:07,480
thank you

01:03:04,190 --> 01:03:12,020
yeah if you have to do only one search

01:03:07,480 --> 01:03:13,550
nothing nothing can help you and that is

01:03:12,020 --> 01:03:14,960
why database people are interested

01:03:13,550 --> 01:03:18,770
because they actually do multiple

01:03:14,960 --> 01:03:20,960
lookups what it'd be also pass possible

01:03:18,770 --> 01:03:25,010
to you up to my single search if you

01:03:20,960 --> 01:03:27,080
stopped prefetching sooner because well

01:03:25,010 --> 01:03:28,580
every time you access memory you are

01:03:27,080 --> 01:03:30,830
actually pulling a little bit more data

01:03:28,580 --> 01:03:32,450
into cache right nope so like let's say

01:03:30,830 --> 01:03:34,460
when you are making less steps in

01:03:32,450 --> 01:03:36,530
bearing research and when you're

01:03:34,460 --> 01:03:38,450
observing like lasts I don't know for

01:03:36,530 --> 01:03:41,150
value systems like that so you probably

01:03:38,450 --> 01:03:43,430
don't need to do it in separate steps

01:03:41,150 --> 01:03:45,170
can you speak slightly closer to the

01:03:43,430 --> 01:03:46,460
microphone for some reason I I'm not

01:03:45,170 --> 01:03:49,520
getting it here sure

01:03:46,460 --> 01:03:52,370
oh my question was when you prefetch

01:03:49,520 --> 01:03:54,080
something from memory so you are you

01:03:52,370 --> 01:03:56,420
know CPU is not pulling single byte

01:03:54,080 --> 01:03:59,360
right so it's pulling a couple more

01:03:56,420 --> 01:04:02,210
bites after that into cache probably you

01:03:59,360 --> 01:04:04,330
could stop prefetching for the last

01:04:02,210 --> 01:04:08,210
couple iterations of the binary search

01:04:04,330 --> 01:04:10,930
and by doing that so probably you would

01:04:08,210 --> 01:04:13,460
spit up a little bit individual search

01:04:10,930 --> 01:04:15,950
possibly I will put up the code on the

01:04:13,460 --> 01:04:21,650
github again and have fun party on it

01:04:15,950 --> 01:04:22,760
see if you can make it Wester okay thank

01:04:21,650 --> 01:04:27,260
you very much

01:04:22,760 --> 01:04:27,260

YouTube URL: https://www.youtube.com/watch?v=j9tlJAqMV7U


