Title: Design Patterns for Handling Reporting Errors in C++ - Parallel Algorithms & Executors -Mark Hoemmen
Publication date: 2020-09-30
Playlist: CppCon 2020 Day 3
Description: 
	https://cppcon.org/
https://github.com/CppCon/CppCon2020/blob/main/Presentations/design_patterns_for_error_handling/design_patterns_for_error_handling__mark_hoemmen__cppcon_2020.pdf
---
C++17 already has parallel algorithms.  Executors (P0443, hopefully in C++23) will add asynchronous execution to parallelism.  The usual C++ error handling approach, exceptions, will not work "out of the box."  We need to build up new patterns for error handling.

C++ parallel algorithms and executors have several challenges handling errors.  First, errors can happen nonlocally -- in parallel tasks other than my own -- yet they can affect correctness of an entire parallel algorithm.  Second, notification of errors may be deferred: I may not find out about an error right away.  Third, I may need to take explicit action to find out about an error.  Fourth, checking whether an error has occurred somewhere might have a performance cost.  Finally, letting exceptions propagate across certain boundaries may have unfortunate consequences: the program might crash without respecting destructors and stack unwinding, or it might even "hang" (become unresponsive).

I'm not offering canned solutions to these issues.  However, we don't have to start from scratch: this situation shares much in common with others that have been around for decades.  Examples include non-C++ interfaces to C++ libraries, boundaries between graphical user interface and back-end code, and distributed-memory parallel code.  My experience is mainly with distributed-memory parallel code that uses MPI, the Message Passing Interface that has seen nearly 30 years of continuous use and evolution.  In this talk, I'll show how design patterns already exist for parallel error handling, and I'll apply these design patterns to C++17 parallel algorithms, executors (P0443r13), and parallel algorithms for executors (P1897r3).

---
Mark Hoemmen has a B.S. in mathematics and computer science from the University of Illinois, and a PhD in computer science from the University of California Berkeley. His background is in numerical linear algebra and parallel computing. Mark has 20 years' professional experience as a C++ developer, and has been contributing to C++ Standard Committee proposals for about three years.

---
Streamed & Edited by Digital Medium Ltd - events.digital-medium.co.uk
events@digital-medium.co.uk
Captions: 
	00:00:09,120 --> 00:00:14,719
hello everyone

00:00:11,599 --> 00:00:16,080
okay great to have you all here um my

00:00:14,719 --> 00:00:18,240
name is mark coleman

00:00:16,080 --> 00:00:19,439
and i'll be speaking to you today about

00:00:18,240 --> 00:00:21,279
design patterns

00:00:19,439 --> 00:00:23,039
for error handling in c plus plus

00:00:21,279 --> 00:00:24,240
programs using parallel algorithms and

00:00:23,039 --> 00:00:27,439
executors

00:00:24,240 --> 00:00:27,840
and uh before i get started um this talk

00:00:27,439 --> 00:00:30,960
will be

00:00:27,840 --> 00:00:32,800
i'll take for about 25 minutes um if you

00:00:30,960 --> 00:00:34,880
could please um

00:00:32,800 --> 00:00:36,880
i'll hold answering questions until the

00:00:34,880 --> 00:00:37,760
end but uh please feel free during the

00:00:36,880 --> 00:00:39,760
talk to type

00:00:37,760 --> 00:00:41,760
in remote in the q a section to add

00:00:39,760 --> 00:00:44,160
questions there and if you could include

00:00:41,760 --> 00:00:45,280
um the slide number or title when you

00:00:44,160 --> 00:00:46,000
ask the question that would help me

00:00:45,280 --> 00:00:47,760
answer them

00:00:46,000 --> 00:00:49,120
i'll answer questions at the end

00:00:47,760 --> 00:00:51,680
immediately after the talk

00:00:49,120 --> 00:00:53,440
there will be an ama ask me anything um

00:00:51,680 --> 00:00:54,079
that will happen in the same room and i

00:00:53,440 --> 00:00:55,840
will just

00:00:54,079 --> 00:00:57,120
continue with the same stream and keep

00:00:55,840 --> 00:00:58,719
talking um

00:00:57,120 --> 00:01:00,239
so just if you're interested in

00:00:58,719 --> 00:01:01,760
attending the ama

00:01:00,239 --> 00:01:03,440
please just stay in the same room keep

00:01:01,760 --> 00:01:06,320
connected and we'll continue there

00:01:03,440 --> 00:01:08,000
thank you and my name is pronounced um i

00:01:06,320 --> 00:01:11,040
included some ipa there

00:01:08,000 --> 00:01:14,080
and my pronouns are he and him

00:01:11,040 --> 00:01:16,240
um so i have about 10 years experience

00:01:14,080 --> 00:01:17,439
after my phd are writing parallel c

00:01:16,240 --> 00:01:19,600
plus for science and engineering

00:01:17,439 --> 00:01:21,360
applications my background

00:01:19,600 --> 00:01:23,439
is in parallel algorithms for solving

00:01:21,360 --> 00:01:26,320
large linear algebra problems

00:01:23,439 --> 00:01:28,159
i'm really really new to see

00:01:26,320 --> 00:01:30,799
standardization processes

00:01:28,159 --> 00:01:32,799
my first standard committee meeting was

00:01:30,799 --> 00:01:35,200
at the end of 2017

00:01:32,799 --> 00:01:38,640
and i just started a new job at stellar

00:01:35,200 --> 00:01:38,640
science in march of this year

00:01:39,040 --> 00:01:43,360
um today i'll tell you about how first

00:01:41,360 --> 00:01:45,360
how parallelism makes it harder to do

00:01:43,360 --> 00:01:48,159
error handling to deal with errors

00:01:45,360 --> 00:01:50,079
and specifically c plus parallel

00:01:48,159 --> 00:01:51,759
algorithms and tasks make it harder to

00:01:50,079 --> 00:01:53,759
handle errors

00:01:51,759 --> 00:01:55,840
i'll then talk about the message passing

00:01:53,759 --> 00:01:57,439
interface for parallel programming in a

00:01:55,840 --> 00:01:59,360
distributed memory sense which has been

00:01:57,439 --> 00:02:02,799
around for about three decades

00:01:59,360 --> 00:02:04,719
mpi um experiences from mpi

00:02:02,799 --> 00:02:06,560
teach us design patterns to detect and

00:02:04,719 --> 00:02:07,520
handle recoverable errors and i'll

00:02:06,560 --> 00:02:10,479
outline some of those

00:02:07,520 --> 00:02:10,479
towards the end of the talk

00:02:10,640 --> 00:02:14,720
when i say parallel i mean using

00:02:12,879 --> 00:02:18,000
multiple hardware resources

00:02:14,720 --> 00:02:20,000
nodes cores vector units cmd units

00:02:18,000 --> 00:02:21,120
to accomplish more than one work item at

00:02:20,000 --> 00:02:22,720
the same time

00:02:21,120 --> 00:02:24,400
in order to improve performance

00:02:22,720 --> 00:02:27,920
performance could mean latency

00:02:24,400 --> 00:02:27,920
throughput or responsiveness

00:02:28,160 --> 00:02:32,080
parallelism hinders error handling and

00:02:31,040 --> 00:02:35,200
that's because

00:02:32,080 --> 00:02:37,040
parallelism relaxes execution order

00:02:35,200 --> 00:02:38,319
and it does so deliberately to improve

00:02:37,040 --> 00:02:40,879
performance

00:02:38,319 --> 00:02:42,000
i o to daisy holman whom you might know

00:02:40,879 --> 00:02:45,280
the distinction between

00:02:42,000 --> 00:02:47,760
parallelism and concurrency parallelism

00:02:45,280 --> 00:02:49,200
relaxes execution order to go faster

00:02:47,760 --> 00:02:51,120
concurrency constrains

00:02:49,200 --> 00:02:53,840
execution order in order to make it

00:02:51,120 --> 00:02:55,680
easier to reason about what's happening

00:02:53,840 --> 00:02:57,440
when an error happens it interrupts the

00:02:55,680 --> 00:02:59,200
normal flow of execution

00:02:57,440 --> 00:03:00,879
and in order to handle the error it

00:02:59,200 --> 00:03:02,959
constrains the order of execution you

00:03:00,879 --> 00:03:04,800
have to change the control flow

00:03:02,959 --> 00:03:06,640
errors could actually lead to deadlock

00:03:04,800 --> 00:03:08,640
which means waiting forever

00:03:06,640 --> 00:03:10,800
for example if one parallel worker drops

00:03:08,640 --> 00:03:12,560
out before a collective synchronization

00:03:10,800 --> 00:03:15,040
between multiple parallel workers

00:03:12,560 --> 00:03:16,959
the other workers may wait forever and

00:03:15,040 --> 00:03:18,319
the correct handling of errors requires

00:03:16,959 --> 00:03:20,560
communication

00:03:18,319 --> 00:03:22,480
when i say communication i mean either

00:03:20,560 --> 00:03:24,080
data movement or synchronization they're

00:03:22,480 --> 00:03:25,599
really the same thing data movement

00:03:24,080 --> 00:03:27,360
implies causality which implies

00:03:25,599 --> 00:03:29,040
synchronization

00:03:27,360 --> 00:03:30,560
and we might need to communicate for a

00:03:29,040 --> 00:03:32,000
couple reasons for example we might need

00:03:30,560 --> 00:03:33,280
to stop other workers from waiting

00:03:32,000 --> 00:03:35,360
forever on us

00:03:33,280 --> 00:03:36,879
or or we might need to propagate and

00:03:35,360 --> 00:03:39,360
combine error information from other

00:03:36,879 --> 00:03:39,360
workers

00:03:39,440 --> 00:03:43,360
now there's really no free lunch there's

00:03:40,959 --> 00:03:46,080
no zero overhead solution to this

00:03:43,360 --> 00:03:48,000
error handling requires communication

00:03:46,080 --> 00:03:49,360
communication is expensive it costs a

00:03:48,000 --> 00:03:50,480
lot especially on modern computer

00:03:49,360 --> 00:03:53,120
architectures

00:03:50,480 --> 00:03:55,280
making c plus plus do it for you

00:03:53,120 --> 00:03:57,680
automagically will not be free

00:03:55,280 --> 00:03:59,040
and so as c plus developers often want a

00:03:57,680 --> 00:04:00,959
zero overhead solution

00:03:59,040 --> 00:04:04,080
if that's what you want you the coder

00:04:00,959 --> 00:04:04,080
will need to handle errors

00:04:04,959 --> 00:04:08,959
now standard c plus plus offers a few

00:04:07,200 --> 00:04:11,439
different ways to get parallelism

00:04:08,959 --> 00:04:13,120
just in the standard first are the

00:04:11,439 --> 00:04:15,680
parallel algorithms that are in c

00:04:13,120 --> 00:04:16,799
plus 17 these include algorithms like

00:04:15,680 --> 00:04:19,919
for each reduce

00:04:16,799 --> 00:04:21,359
transform and sort the first argument of

00:04:19,919 --> 00:04:22,960
these parallel algorithms is an

00:04:21,359 --> 00:04:25,040
execution policy

00:04:22,960 --> 00:04:26,479
and that specifies the permitted changes

00:04:25,040 --> 00:04:29,440
in execution order

00:04:26,479 --> 00:04:31,199
from the normal sequential order and for

00:04:29,440 --> 00:04:32,400
all of the execution policies currently

00:04:31,199 --> 00:04:34,639
in the standard

00:04:32,400 --> 00:04:36,080
if you throw in your parallel loop body

00:04:34,639 --> 00:04:38,000
in the element access function that you

00:04:36,080 --> 00:04:41,040
give to the parallel algorithm

00:04:38,000 --> 00:04:43,759
terminate will be called there are also

00:04:41,040 --> 00:04:45,440
asynchronous tasks for example c plus 11

00:04:43,759 --> 00:04:47,600
has async

00:04:45,440 --> 00:04:49,280
and when you launch an asynchronous task

00:04:47,600 --> 00:04:50,320
uncaught exceptions in your task get

00:04:49,280 --> 00:04:52,160
captured

00:04:50,320 --> 00:04:55,040
and waiting on a result throws the past

00:04:52,160 --> 00:04:57,520
along exception in the future

00:04:55,040 --> 00:04:59,280
and there are also proposals to add um

00:04:57,520 --> 00:05:02,240
more general asynchronous tasks for

00:04:59,280 --> 00:05:04,160
example there's p0443 executors

00:05:02,240 --> 00:05:07,520
and it's companion paper the synchronous

00:05:04,160 --> 00:05:10,080
algorithms paper p1897

00:05:07,520 --> 00:05:11,759
and executors have a separate path of

00:05:10,080 --> 00:05:13,600
the so-called error channel

00:05:11,759 --> 00:05:16,080
for handling an ancestor task on caught

00:05:13,600 --> 00:05:16,560
exception there's also an algorithm when

00:05:16,080 --> 00:05:18,960
all

00:05:16,560 --> 00:05:20,960
and when all allows a task to express a

00:05:18,960 --> 00:05:24,160
dependency on more than one task

00:05:20,960 --> 00:05:27,360
so a task can depend on two or more

00:05:24,160 --> 00:05:27,840
or parents and when more than one parent

00:05:27,360 --> 00:05:30,720
task

00:05:27,840 --> 00:05:32,320
throws the when all algorithm captures

00:05:30,720 --> 00:05:34,800
only one of those exceptions it drops

00:05:32,320 --> 00:05:34,800
the rest

00:05:35,440 --> 00:05:40,240
so as you can see two examples of of how

00:05:38,720 --> 00:05:41,280
exceptions can cause trouble in your

00:05:40,240 --> 00:05:42,880
parallel code

00:05:41,280 --> 00:05:44,400
i mean a parallel algorithm if you throw

00:05:42,880 --> 00:05:45,840
in your loop body instead of the

00:05:44,400 --> 00:05:47,600
exception propagating as it does

00:05:45,840 --> 00:05:50,720
normally in a sequential algorithm

00:05:47,600 --> 00:05:52,639
terminate is called and in when all if

00:05:50,720 --> 00:05:55,840
more than one parent throws

00:05:52,639 --> 00:05:58,160
all but one of the exceptions are draw

00:05:55,840 --> 00:06:00,000
so you may argue well if i'm writing

00:05:58,160 --> 00:06:01,919
parallel code i care a lot about

00:06:00,000 --> 00:06:03,759
performance and so i shouldn't be having

00:06:01,919 --> 00:06:06,240
exceptions in my parallel code i should

00:06:03,759 --> 00:06:08,319
have written my code not to throw

00:06:06,240 --> 00:06:09,440
and that's an argument we can think

00:06:08,319 --> 00:06:11,199
about um

00:06:09,440 --> 00:06:13,039
exceptions are for recoverable errors

00:06:11,199 --> 00:06:14,560
really if we have non-recoverable errors

00:06:13,039 --> 00:06:16,639
maybe terminate is good

00:06:14,560 --> 00:06:18,240
but let's think about the kinds of code

00:06:16,639 --> 00:06:19,520
the code characteristics that can lead

00:06:18,240 --> 00:06:21,199
to us wanting to

00:06:19,520 --> 00:06:23,440
handle errors that can lead to us

00:06:21,199 --> 00:06:25,360
wanting to deal with exceptions

00:06:23,440 --> 00:06:27,039
i'll read this diagram this big chart

00:06:25,360 --> 00:06:29,680
from the bottom up

00:06:27,039 --> 00:06:31,440
and on the left side i see what is the

00:06:29,680 --> 00:06:32,080
status of exceptions in that level of

00:06:31,440 --> 00:06:34,000
code

00:06:32,080 --> 00:06:35,919
and on the right side we see the typical

00:06:34,000 --> 00:06:37,120
kinds of things that level of code might

00:06:35,919 --> 00:06:39,440
do

00:06:37,120 --> 00:06:40,560
so at the very lowest level of parallel

00:06:39,440 --> 00:06:42,240
code

00:06:40,560 --> 00:06:43,520
exceptions are not allowed even if we

00:06:42,240 --> 00:06:45,440
catch them you're just not allowed to

00:06:43,520 --> 00:06:47,759
throw or catch or do anything

00:06:45,440 --> 00:06:50,319
and that that happens in code that like

00:06:47,759 --> 00:06:52,160
explicit cmd intrinsics or

00:06:50,319 --> 00:06:53,840
dialects of c plus plus for running on

00:06:52,160 --> 00:06:55,039
gpus

00:06:53,840 --> 00:06:57,360
above there there's a level of

00:06:55,039 --> 00:06:58,319
abstraction where exceptions usually

00:06:57,360 --> 00:07:00,800
indicate bugs

00:06:58,319 --> 00:07:03,199
contract violations that kind of code

00:07:00,800 --> 00:07:04,479
looks like tight well-optimized loops

00:07:03,199 --> 00:07:06,720
i've thought really hard about how to

00:07:04,479 --> 00:07:08,560
make the code go fast

00:07:06,720 --> 00:07:09,759
and so any exception that i might

00:07:08,560 --> 00:07:12,240
encounter there is

00:07:09,759 --> 00:07:14,800
means i made a mistake i mean a bug but

00:07:12,240 --> 00:07:17,520
above that level of code

00:07:14,800 --> 00:07:19,039
we often see code where exceptions that

00:07:17,520 --> 00:07:20,000
are not bugs that are not contract

00:07:19,039 --> 00:07:21,520
violations

00:07:20,000 --> 00:07:23,759
and other kinds of recoverable errors

00:07:21,520 --> 00:07:25,599
are more likely this kind of code might

00:07:23,759 --> 00:07:26,400
call third party libraries of unknown

00:07:25,599 --> 00:07:28,720
provenance

00:07:26,400 --> 00:07:31,120
it might do input and output operations

00:07:28,720 --> 00:07:32,720
it might do speculative computations

00:07:31,120 --> 00:07:34,000
and i'll show you an example of code

00:07:32,720 --> 00:07:35,039
like that that wants to live in a

00:07:34,000 --> 00:07:38,000
parallel loop

00:07:35,039 --> 00:07:38,800
in the next slide this example is an

00:07:38,000 --> 00:07:40,800
algorithm called

00:07:38,800 --> 00:07:42,400
domain decomposition it's from my

00:07:40,800 --> 00:07:44,000
background in

00:07:42,400 --> 00:07:46,080
solving large sparse linear algebra

00:07:44,000 --> 00:07:47,360
problems domain decomposition is an

00:07:46,080 --> 00:07:49,280
algorithm for solving

00:07:47,360 --> 00:07:50,960
a big system of linear equations ax

00:07:49,280 --> 00:07:52,720
equals b

00:07:50,960 --> 00:07:54,960
and in domain decomposition we do that

00:07:52,720 --> 00:07:56,840
by decomposing the big linear system

00:07:54,960 --> 00:07:58,000
into many small linear systems called

00:07:56,840 --> 00:07:59,360
subdomains

00:07:58,000 --> 00:08:01,280
we solve those small systems

00:07:59,360 --> 00:08:02,400
independently and then we combine the

00:08:01,280 --> 00:08:04,639
results

00:08:02,400 --> 00:08:06,080
domain decomposition is an approximation

00:08:04,639 --> 00:08:07,120
and so we may have to repeat this

00:08:06,080 --> 00:08:08,879
process until we

00:08:07,120 --> 00:08:10,240
converge until we get close to the right

00:08:08,879 --> 00:08:12,560
answer

00:08:10,240 --> 00:08:14,000
now domain composition is not foolproof

00:08:12,560 --> 00:08:15,520
it might fail

00:08:14,000 --> 00:08:17,599
and it might fail for any of various

00:08:15,520 --> 00:08:19,280
reasons the entire problem might have no

00:08:17,599 --> 00:08:20,960
solution mathematically

00:08:19,280 --> 00:08:22,960
or it might take too many iterations to

00:08:20,960 --> 00:08:24,639
get to an accurate solution

00:08:22,960 --> 00:08:26,240
or some of the small systems may not

00:08:24,639 --> 00:08:28,560
have a solution at all mathematically

00:08:26,240 --> 00:08:30,639
even though the whole problem does

00:08:28,560 --> 00:08:32,159
or we might fail to solve the small

00:08:30,639 --> 00:08:34,080
systems for some reason

00:08:32,159 --> 00:08:36,800
other than their mathematical problems

00:08:34,080 --> 00:08:38,080
for example we may run out of resources

00:08:36,800 --> 00:08:40,399
now there are other algorithms that we

00:08:38,080 --> 00:08:42,479
can use to solve big linear systems

00:08:40,399 --> 00:08:43,760
these fallback algorithms like a sparse

00:08:42,479 --> 00:08:45,839
lu factorization

00:08:43,760 --> 00:08:47,600
can take a lot more memory and time so

00:08:45,839 --> 00:08:51,279
we would prefer for domain decomposition

00:08:47,600 --> 00:08:51,279
to work but still we do have a fallback

00:08:52,560 --> 00:08:57,040
on the left i show a sketch of how we

00:08:55,440 --> 00:08:58,800
might implement a domain decomposition

00:08:57,040 --> 00:09:01,839
in the non-parallel case so the

00:08:58,800 --> 00:09:03,519
the pre-parallelized example and

00:09:01,839 --> 00:09:06,320
we just wrap the whole thing in a try

00:09:03,519 --> 00:09:07,279
catch we set up for all the subdomain

00:09:06,320 --> 00:09:09,279
solves

00:09:07,279 --> 00:09:11,600
we do a for loop over the subdomain

00:09:09,279 --> 00:09:13,760
solves we solve each one in turn

00:09:11,600 --> 00:09:15,519
and then we clean up afterwards to

00:09:13,760 --> 00:09:18,160
combine all the results

00:09:15,519 --> 00:09:18,560
and no matter what solve throws if any

00:09:18,160 --> 00:09:20,800
of the

00:09:18,560 --> 00:09:22,000
if the any of the sub domain solves fail

00:09:20,800 --> 00:09:24,080
we know that we need to fall

00:09:22,000 --> 00:09:25,600
back to a different solver and so we

00:09:24,080 --> 00:09:28,160
just say that we fall back to a

00:09:25,600 --> 00:09:28,160
different solver

00:09:29,040 --> 00:09:32,880
now on the right i'm showing how we

00:09:30,959 --> 00:09:34,880
might parallelize this algorithm

00:09:32,880 --> 00:09:36,080
now we've done two things to parallelize

00:09:34,880 --> 00:09:38,160
and to optimize

00:09:36,080 --> 00:09:40,000
the first thing that we did is to use a

00:09:38,160 --> 00:09:42,560
fixed size memory pool

00:09:40,000 --> 00:09:44,240
this is because um memory allocation

00:09:42,560 --> 00:09:45,120
inside of a parallel loop can sometimes

00:09:44,240 --> 00:09:46,800
be slow and

00:09:45,120 --> 00:09:48,800
it kind of needs to synchronize if you

00:09:46,800 --> 00:09:50,800
think about it and so here

00:09:48,800 --> 00:09:53,200
i've used a fixed size memory pool to

00:09:50,800 --> 00:09:55,440
speed up that allocation process

00:09:53,200 --> 00:09:56,399
and i've also replaced the range for

00:09:55,440 --> 00:09:58,560
loop on the left

00:09:56,399 --> 00:09:59,839
with a c plus plus parallel algorithm

00:09:58,560 --> 00:10:02,320
called four each

00:09:59,839 --> 00:10:03,920
and i'm using the par unseek execution

00:10:02,320 --> 00:10:06,240
policy that means

00:10:03,920 --> 00:10:08,000
parallel and unsequenced so i'm allowed

00:10:06,240 --> 00:10:10,320
to execute the loop iterations in any

00:10:08,000 --> 00:10:10,320
order

00:10:13,279 --> 00:10:17,200
in our domain decomposition in our

00:10:15,279 --> 00:10:18,480
parallel domain decomposition algorithm

00:10:17,200 --> 00:10:21,440
we need to be able to distinguish

00:10:18,480 --> 00:10:23,519
between two different possible errors

00:10:21,440 --> 00:10:24,720
first it could be that the memory pool

00:10:23,519 --> 00:10:26,640
is too small

00:10:24,720 --> 00:10:28,320
if the memory pool is too small well we

00:10:26,640 --> 00:10:29,920
need to figure out how much additional

00:10:28,320 --> 00:10:32,160
space we need in the pool

00:10:29,920 --> 00:10:34,720
we need to reallocate the pool and then

00:10:32,160 --> 00:10:36,560
we need to retry domain decomposition

00:10:34,720 --> 00:10:37,839
but there's another kind of error and

00:10:36,560 --> 00:10:38,640
that could be the error in which it is

00:10:37,839 --> 00:10:41,279
mathematically

00:10:38,640 --> 00:10:42,720
impossible to solve one or more of the

00:10:41,279 --> 00:10:44,640
subdomain problems

00:10:42,720 --> 00:10:46,640
in that case we need to give up on

00:10:44,640 --> 00:10:49,040
domain decomposition and fall back to

00:10:46,640 --> 00:10:51,680
the slower solver

00:10:49,040 --> 00:10:54,000
however we have a bit of a problem

00:10:51,680 --> 00:10:57,040
inside of the parallel algorithm

00:10:54,000 --> 00:10:57,760
any kind of throw causes terminate to be

00:10:57,040 --> 00:10:59,279
called

00:10:57,760 --> 00:11:00,880
and that means that we can't have a try

00:10:59,279 --> 00:11:02,480
catch that distinguishes between

00:11:00,880 --> 00:11:04,160
the different types of errors by

00:11:02,480 --> 00:11:05,760
catching different exceptions

00:11:04,160 --> 00:11:08,079
so what do we do about this we can't

00:11:05,760 --> 00:11:09,920
distinguish between the kinds of errors

00:11:08,079 --> 00:11:14,320
so how do we tell what happened and

00:11:09,920 --> 00:11:16,399
recover from it

00:11:14,320 --> 00:11:18,000
hold that thought and i'll talk to you a

00:11:16,399 --> 00:11:20,480
little bit about the message passing

00:11:18,000 --> 00:11:20,480
interface

00:11:24,480 --> 00:11:30,560
the message passing interface or mpi

00:11:28,399 --> 00:11:32,480
is a c and a fortran interface for

00:11:30,560 --> 00:11:33,279
writing distributed memory parallel code

00:11:32,480 --> 00:11:36,880
it's a

00:11:33,279 --> 00:11:38,480
library interface so you call functions

00:11:36,880 --> 00:11:40,480
there might be macros involved in the c

00:11:38,480 --> 00:11:42,320
interface

00:11:40,480 --> 00:11:44,000
mpi has been a standard for three

00:11:42,320 --> 00:11:46,480
decades

00:11:44,000 --> 00:11:48,560
it unified divergent interfaces for

00:11:46,480 --> 00:11:51,279
distributed memory parallelism

00:11:48,560 --> 00:11:53,040
around the early 90s the first a version

00:11:51,279 --> 00:11:54,720
of the mpi standard was published in

00:11:53,040 --> 00:11:57,839
00:11:54,720 --> 00:12:00,880
but it's an ongoing evolving standard

00:11:57,839 --> 00:12:03,920
in 2015 uh version 3.1 came out and

00:12:00,880 --> 00:12:06,320
version 4.0 is pending people use mpi to

00:12:03,920 --> 00:12:07,440
solve enormous huge problems on enormous

00:12:06,320 --> 00:12:08,880
computers

00:12:07,440 --> 00:12:11,839
you can get millions away parallelism

00:12:08,880 --> 00:12:14,880
with mpi npi is a stable interface

00:12:11,839 --> 00:12:16,800
code from the 90s just works

00:12:14,880 --> 00:12:18,560
it has modest hardware requirements you

00:12:16,800 --> 00:12:20,160
really just need

00:12:18,560 --> 00:12:21,920
some kind of computers hooked up to a

00:12:20,160 --> 00:12:23,760
network you can run it on those on a

00:12:21,920 --> 00:12:25,440
raspberry pi cluster you can run on all

00:12:23,760 --> 00:12:27,279
sorts of computers

00:12:25,440 --> 00:12:29,040
and mpi cooperates with other

00:12:27,279 --> 00:12:29,279
programming models for example threads

00:12:29,040 --> 00:12:32,000
or

00:12:29,279 --> 00:12:32,000
for gpus

00:12:33,040 --> 00:12:36,560
now mpi is a distributed memory

00:12:35,120 --> 00:12:39,040
parallelism model

00:12:36,560 --> 00:12:41,200
and the idea is that you start with p

00:12:39,040 --> 00:12:42,959
parallel processes

00:12:41,200 --> 00:12:45,200
they have a fixed location in hardware

00:12:42,959 --> 00:12:46,639
there's a fixed number of them p

00:12:45,200 --> 00:12:48,720
and so it's a little bit like your

00:12:46,639 --> 00:12:49,680
entire program is running in a parallel

00:12:48,720 --> 00:12:51,519
for each

00:12:49,680 --> 00:12:53,279
over the integer range from zero to p

00:12:51,519 --> 00:12:55,120
minus one

00:12:53,279 --> 00:12:57,279
the processes are distributed they do

00:12:55,120 --> 00:12:59,360
not share memory at all

00:12:57,279 --> 00:13:01,360
and so in order for the processes to

00:12:59,360 --> 00:13:02,959
communicate you as the programmer need

00:13:01,360 --> 00:13:04,720
to do something explicitly

00:13:02,959 --> 00:13:06,320
and the main way that you do that in mpi

00:13:04,720 --> 00:13:08,480
is by messages

00:13:06,320 --> 00:13:10,399
these are explicit function calls you

00:13:08,480 --> 00:13:11,600
can call a function to send something to

00:13:10,399 --> 00:13:14,800
receive something

00:13:11,600 --> 00:13:17,279
or to communicate something altogether

00:13:14,800 --> 00:13:17,839
and the calls are two-sided all of the

00:13:17,279 --> 00:13:19,680
entities

00:13:17,839 --> 00:13:21,279
all of the processes that participate

00:13:19,680 --> 00:13:23,440
must call the function so

00:13:21,279 --> 00:13:24,720
if i'm sending i need to call something

00:13:23,440 --> 00:13:26,160
and you the receiver

00:13:24,720 --> 00:13:28,880
need to call another function that

00:13:26,160 --> 00:13:30,480
corresponds to my send

00:13:28,880 --> 00:13:32,320
messages can come in point-to-point

00:13:30,480 --> 00:13:33,680
forms send and receive

00:13:32,320 --> 00:13:35,680
or they can come in collective

00:13:33,680 --> 00:13:39,760
operations like reduces

00:13:35,680 --> 00:13:42,240
all reduces scatters and gathers

00:13:39,760 --> 00:13:43,279
and on the right side of this slide i

00:13:42,240 --> 00:13:46,160
show a diagram

00:13:43,279 --> 00:13:48,000
illustrating four mpi processes and

00:13:46,160 --> 00:13:50,000
they're sending and receiving messages

00:13:48,000 --> 00:13:51,120
the messages do not synchronize with

00:13:50,000 --> 00:13:53,519
respect to the

00:13:51,120 --> 00:13:54,160
collective communication and you can see

00:13:53,519 --> 00:13:56,800
that

00:13:54,160 --> 00:13:58,480
process three when it sends process zero

00:13:56,800 --> 00:13:59,680
receives process zero and process three

00:13:58,480 --> 00:14:00,959
both need to do something

00:13:59,680 --> 00:14:02,399
and all of the processes that

00:14:00,959 --> 00:14:05,360
participate in the collective need to

00:14:02,399 --> 00:14:05,360
call a function as well

00:14:07,360 --> 00:14:12,160
mpi is hostile to error handling

00:14:10,480 --> 00:14:14,399
i would call it a bit more hostile

00:14:12,160 --> 00:14:16,399
especially for c plus developers

00:14:14,399 --> 00:14:17,680
than even c plus parallel algorithms are

00:14:16,399 --> 00:14:19,920
hostile

00:14:17,680 --> 00:14:22,000
mpi in fact deliberately punted on error

00:14:19,920 --> 00:14:24,160
handling compared to

00:14:22,000 --> 00:14:26,720
competing programming models such as pvm

00:14:24,160 --> 00:14:28,959
the parallel virtual machine

00:14:26,720 --> 00:14:30,000
mpi's goal was really to portability and

00:14:28,959 --> 00:14:32,160
performance mpi

00:14:30,000 --> 00:14:34,480
wanted to run faster and it wanted to do

00:14:32,160 --> 00:14:36,560
so in a wider range of computer systems

00:14:34,480 --> 00:14:38,240
especially the the early distributed

00:14:36,560 --> 00:14:41,519
memory parallel computers

00:14:38,240 --> 00:14:42,720
were were very primitive in terms of

00:14:41,519 --> 00:14:44,160
their capabilities

00:14:42,720 --> 00:14:45,680
in terms of the kinds of things that

00:14:44,160 --> 00:14:46,560
their network and the processors could

00:14:45,680 --> 00:14:49,360
do

00:14:46,560 --> 00:14:50,720
and so the goal was to run run faster

00:14:49,360 --> 00:14:53,199
and to be able to run on all of those

00:14:50,720 --> 00:14:55,279
systems as well

00:14:53,199 --> 00:14:56,560
in fact the mpi equivalent of terminate

00:14:55,279 --> 00:14:58,800
in c plus plus

00:14:56,560 --> 00:15:00,800
is only best effort so if you call mpi

00:14:58,800 --> 00:15:02,480
abort on one process

00:15:00,800 --> 00:15:04,079
hopefully it stops all the other

00:15:02,480 --> 00:15:05,760
processes but there's no promise that it

00:15:04,079 --> 00:15:08,240
does so

00:15:05,760 --> 00:15:10,399
and if you have a c plus exception

00:15:08,240 --> 00:15:11,760
that's thrown and you don't catch it

00:15:10,399 --> 00:15:14,639
and it percolates all the way up to the

00:15:11,760 --> 00:15:16,000
top um that's undefined behavior for an

00:15:14,639 --> 00:15:18,320
mpi program

00:15:16,000 --> 00:15:19,839
and often in my experience is that if

00:15:18,320 --> 00:15:20,480
you have an uncaught exception on one

00:15:19,839 --> 00:15:22,160
process

00:15:20,480 --> 00:15:24,079
it causes deadlock because the other

00:15:22,160 --> 00:15:26,480
processes are waiting for that process

00:15:24,079 --> 00:15:27,839
to do something that it never does

00:15:26,480 --> 00:15:29,680
there are some ways around that you can

00:15:27,839 --> 00:15:30,639
get the terminate handler to call mpi

00:15:29,680 --> 00:15:33,839
aboard but it's

00:15:30,639 --> 00:15:33,839
again it's only best effort

00:15:34,800 --> 00:15:38,880
and so because mpi is such a hostile

00:15:36,639 --> 00:15:40,720
environment for error handling

00:15:38,880 --> 00:15:42,639
we have library and application

00:15:40,720 --> 00:15:43,839
developers who use mpi have learned some

00:15:42,639 --> 00:15:45,440
lessons from that

00:15:43,839 --> 00:15:47,839
and i think those lessons are good for

00:15:45,440 --> 00:15:50,000
sql plus parallel algorithms and tasks

00:15:47,839 --> 00:15:51,040
so i'll present those lessons to you now

00:15:50,000 --> 00:15:52,959
the first is

00:15:51,040 --> 00:15:54,800
to turn exceptions into values before

00:15:52,959 --> 00:15:57,519
they risk breaking control flow or

00:15:54,800 --> 00:16:01,199
percolating up without being caught

00:15:57,519 --> 00:16:03,440
the second is to turn a parallel four

00:16:01,199 --> 00:16:04,800
whose loop iterations might fail into a

00:16:03,440 --> 00:16:06,399
reduction

00:16:04,800 --> 00:16:08,000
and you can reduce on did everybody

00:16:06,399 --> 00:16:09,920
succeed and you can also

00:16:08,000 --> 00:16:11,199
reduce on other information that you can

00:16:09,920 --> 00:16:12,959
use to help with

00:16:11,199 --> 00:16:14,480
reporting the error and or recovering

00:16:12,959 --> 00:16:16,320
from it

00:16:14,480 --> 00:16:18,240
the third lesson is to prevent

00:16:16,320 --> 00:16:19,600
synchronization related deadlock

00:16:18,240 --> 00:16:21,279
if you find yourself needing to

00:16:19,600 --> 00:16:22,079
synchronize in a parallel algorithm or

00:16:21,279 --> 00:16:23,680
task

00:16:22,079 --> 00:16:25,040
you could use that as an opportunity to

00:16:23,680 --> 00:16:26,639
communicate error state because

00:16:25,040 --> 00:16:28,399
synchronization and communication are

00:16:26,639 --> 00:16:30,959
really the same thing

00:16:28,399 --> 00:16:34,880
and finally a lesson from mpi is to

00:16:30,959 --> 00:16:34,880
exploit out-of-band error reporting

00:16:35,360 --> 00:16:40,240
the first lesson is to turn exceptions

00:16:37,120 --> 00:16:42,880
to values and reduce over work items

00:16:40,240 --> 00:16:44,639
in c plus 17 parallel algorithms there's

00:16:42,880 --> 00:16:46,079
no good way to get an uncaught exception

00:16:44,639 --> 00:16:47,600
back to the caller it just calls

00:16:46,079 --> 00:16:49,199
terminate so you can't

00:16:47,600 --> 00:16:51,199
propagate that information well what do

00:16:49,199 --> 00:16:52,959
you do you turn that exception into a

00:16:51,199 --> 00:16:56,240
value like an error code

00:16:52,959 --> 00:16:59,360
by catching it and returning a value

00:16:56,240 --> 00:17:01,120
you then reduce over those return values

00:16:59,360 --> 00:17:02,880
to tell whether all the work items

00:17:01,120 --> 00:17:04,720
succeeded and you might do that

00:17:02,880 --> 00:17:06,240
by changing the algorithm so instead of

00:17:04,720 --> 00:17:07,679
calling for each for example

00:17:06,240 --> 00:17:10,160
you might call reduce or transform

00:17:07,679 --> 00:17:10,880
reduce now there are some algorithms

00:17:10,160 --> 00:17:12,559
that

00:17:10,880 --> 00:17:14,720
are not easily convertible into

00:17:12,559 --> 00:17:15,439
reductions sort is a good example of

00:17:14,720 --> 00:17:17,120
that

00:17:15,439 --> 00:17:19,520
and i'll explain later on how to handle

00:17:17,120 --> 00:17:22,720
algorithms like that

00:17:19,520 --> 00:17:24,880
now if you're using executors tasks

00:17:22,720 --> 00:17:27,760
with a when all algorithm that combines

00:17:24,880 --> 00:17:29,440
the results from multiple parent tasks

00:17:27,760 --> 00:17:31,280
the when all algorithm drops all but one

00:17:29,440 --> 00:17:33,280
exception from the parent tasks

00:17:31,280 --> 00:17:35,120
so if you need information from the

00:17:33,280 --> 00:17:36,400
other tasks exceptions

00:17:35,120 --> 00:17:38,400
what you need to do is turn those

00:17:36,400 --> 00:17:40,160
exceptions into values for example

00:17:38,400 --> 00:17:42,960
via the let error algorithm that's

00:17:40,160 --> 00:17:44,799
described in 18 97

00:17:42,960 --> 00:17:46,880
and then your next task will reduce over

00:17:44,799 --> 00:17:48,080
those values the transformation of those

00:17:46,880 --> 00:17:49,919
exceptions into values

00:17:48,080 --> 00:17:53,200
so it's really just like it's just like

00:17:49,919 --> 00:17:53,200
the parallel algorithms case

00:17:55,120 --> 00:17:59,039
i'll describe here how i can take the

00:17:57,440 --> 00:18:01,120
first attempt at a parallel domain

00:17:59,039 --> 00:18:03,039
decomposition in my previous slides

00:18:01,120 --> 00:18:04,799
and i can use this reduce over error

00:18:03,039 --> 00:18:08,080
information technique

00:18:04,799 --> 00:18:11,919
to get it to recover from errors so

00:18:08,080 --> 00:18:14,400
in this algorithm i change it first

00:18:11,919 --> 00:18:15,919
so that instead of the solves throwing

00:18:14,400 --> 00:18:18,400
they return a result

00:18:15,919 --> 00:18:20,320
result is a struct that has two fields

00:18:18,400 --> 00:18:21,679
and first there's a status field that's

00:18:20,320 --> 00:18:24,320
a bit field that explains

00:18:21,679 --> 00:18:25,280
why the solve failed and the second

00:18:24,320 --> 00:18:27,120
value in the struct

00:18:25,280 --> 00:18:28,320
is the number of bytes that we needed

00:18:27,120 --> 00:18:29,840
from the memory pool

00:18:28,320 --> 00:18:31,600
whether we got those bytes or not

00:18:29,840 --> 00:18:33,919
depends on whether the solving succeeded

00:18:31,600 --> 00:18:33,919
of course

00:18:34,000 --> 00:18:39,360
so the solves return error information

00:18:37,120 --> 00:18:41,440
and then we reduce over those error

00:18:39,360 --> 00:18:43,440
information over the solved results

00:18:41,440 --> 00:18:44,960
so i've transformed my for each

00:18:43,440 --> 00:18:46,400
algorithm into a transform reduce

00:18:44,960 --> 00:18:48,799
algorithm

00:18:46,400 --> 00:18:50,559
and i include um in the large red box i

00:18:48,799 --> 00:18:51,760
include this reduction that takes the

00:18:50,559 --> 00:18:53,280
two results

00:18:51,760 --> 00:18:55,280
and combines their bit fields using

00:18:53,280 --> 00:18:58,960
bitwise and and combines the number of

00:18:55,280 --> 00:18:58,960
bytes needed using using addition

00:18:59,039 --> 00:19:03,280
and so if the solve failed because we

00:19:01,280 --> 00:19:05,120
ran out of memory

00:19:03,280 --> 00:19:06,640
now we have the number of bytes needed

00:19:05,120 --> 00:19:10,960
total and so now we have the new

00:19:06,640 --> 00:19:13,200
required tool size we can try again

00:19:10,960 --> 00:19:15,919
now deadlock getting processes to wait

00:19:13,200 --> 00:19:17,360
forever is really easy in an mpi program

00:19:15,919 --> 00:19:19,360
that's because the entire program is a

00:19:17,360 --> 00:19:21,039
parallel for each and so you find

00:19:19,360 --> 00:19:22,799
yourself needing to communicate and

00:19:21,039 --> 00:19:24,960
synchronize more often

00:19:22,799 --> 00:19:26,559
than in c plus plus where you have

00:19:24,960 --> 00:19:29,520
parallel algorithms that are separate

00:19:26,559 --> 00:19:30,960
and have a four joint semantic so in mpi

00:19:29,520 --> 00:19:31,440
the entire program is a parallel for

00:19:30,960 --> 00:19:32,880
each

00:19:31,440 --> 00:19:35,200
you find yourself needing to communicate

00:19:32,880 --> 00:19:36,160
more each involved process must

00:19:35,200 --> 00:19:38,480
participate

00:19:36,160 --> 00:19:40,240
in the communication operation so that

00:19:38,480 --> 00:19:41,919
in the example diagram on the right

00:19:40,240 --> 00:19:44,320
i show the sequence four processes

00:19:41,919 --> 00:19:47,200
running process three

00:19:44,320 --> 00:19:49,200
sends to process zero if process zero

00:19:47,200 --> 00:19:51,200
throws an exception before it can post

00:19:49,200 --> 00:19:53,440
the corresponding receive operation

00:19:51,200 --> 00:19:54,880
and process zero will not participate in

00:19:53,440 --> 00:19:56,960
the send and receive

00:19:54,880 --> 00:19:58,400
that could make process three wait

00:19:56,960 --> 00:20:00,880
forever

00:19:58,400 --> 00:20:01,440
on the send the send will wait until the

00:20:00,880 --> 00:20:03,760
receive

00:20:01,440 --> 00:20:05,280
actually runs there's a bit of a

00:20:03,760 --> 00:20:06,640
subtlety there that i can explain in the

00:20:05,280 --> 00:20:08,400
ama later

00:20:06,640 --> 00:20:10,720
and so process three could wait forever

00:20:08,400 --> 00:20:11,840
there also process zero could wait

00:20:10,720 --> 00:20:13,840
forever

00:20:11,840 --> 00:20:15,840
because if process zero catches the

00:20:13,840 --> 00:20:17,520
throw but fails to post the receive

00:20:15,840 --> 00:20:19,520
and then process zero participates in

00:20:17,520 --> 00:20:21,760
the barrier process zero

00:20:19,520 --> 00:20:25,200
will wait for process three to wait on

00:20:21,760 --> 00:20:25,200
the send for process zero

00:20:25,919 --> 00:20:28,960
deadlock and parallel c plus plus as i

00:20:27,760 --> 00:20:31,440
mentioned is

00:20:28,960 --> 00:20:33,440
perhaps less likely because the way that

00:20:31,440 --> 00:20:34,960
you write code is more

00:20:33,440 --> 00:20:36,559
small parallel region after small

00:20:34,960 --> 00:20:38,080
parallel region

00:20:36,559 --> 00:20:39,679
versus mpi where everything is a

00:20:38,080 --> 00:20:42,240
parallel region

00:20:39,679 --> 00:20:43,520
but it's still possible so for example

00:20:42,240 --> 00:20:45,679
if you're if you're using a thread

00:20:43,520 --> 00:20:47,280
barrier where every thread must reach

00:20:45,679 --> 00:20:49,360
this barrier before any of the threads

00:20:47,280 --> 00:20:50,559
may proceed that's a way that you can

00:20:49,360 --> 00:20:53,039
have deadlock and parallel c

00:20:50,559 --> 00:20:54,240
plus plus an example of a thread barrier

00:20:53,039 --> 00:20:57,440
in c plus plus 20

00:20:54,240 --> 00:20:59,039
is std latch or std barrier

00:20:57,440 --> 00:21:00,480
those can be implemented with an atomic

00:20:59,039 --> 00:21:03,200
counter where you start with

00:21:00,480 --> 00:21:04,559
the number of agents of execution and

00:21:03,200 --> 00:21:07,360
each agent of execution

00:21:04,559 --> 00:21:08,640
decrements by one that counter and when

00:21:07,360 --> 00:21:10,000
the counter reaches zero

00:21:08,640 --> 00:21:12,080
all the threads know that it's safe to

00:21:10,000 --> 00:21:14,080
go on

00:21:12,080 --> 00:21:16,159
a good use for a thread barrier is in

00:21:14,080 --> 00:21:17,520
managing a shared resource for example

00:21:16,159 --> 00:21:19,360
you may want to make sure that all

00:21:17,520 --> 00:21:20,400
execution agents have stopped using the

00:21:19,360 --> 00:21:22,400
shared resource

00:21:20,400 --> 00:21:23,520
so that a coordinator agent can release

00:21:22,400 --> 00:21:26,320
the resource or

00:21:23,520 --> 00:21:27,919
reallocate it make it bigger and so in

00:21:26,320 --> 00:21:29,679
this example on the right

00:21:27,919 --> 00:21:31,120
agent zero throws before it could

00:21:29,679 --> 00:21:32,080
participate in the thread barrier and so

00:21:31,120 --> 00:21:35,919
all the other threads

00:21:32,080 --> 00:21:36,880
forever and the way that we can avoid

00:21:35,919 --> 00:21:38,799
deadlock

00:21:36,880 --> 00:21:40,400
in code like this is to is by going

00:21:38,799 --> 00:21:42,159
through the motions

00:21:40,400 --> 00:21:44,080
so we can think of a parallel loop body

00:21:42,159 --> 00:21:45,760
as a sequence of local blocks punctuated

00:21:44,080 --> 00:21:48,960
by synchronization

00:21:45,760 --> 00:21:50,640
always participate in synchronization so

00:21:48,960 --> 00:21:52,159
don't throw and prevent yourself from

00:21:50,640 --> 00:21:53,200
participating in synchronization

00:21:52,159 --> 00:21:55,039
catch it participate in the

00:21:53,200 --> 00:21:56,080
synchronization but give each of those

00:21:55,039 --> 00:21:58,720
local blocks

00:21:56,080 --> 00:21:59,520
a bypass if there's an error do nothing

00:21:58,720 --> 00:22:01,760
harmlessly

00:21:59,520 --> 00:22:03,760
and pass along the running error state

00:22:01,760 --> 00:22:06,640
this looks a lot like the error channel

00:22:03,760 --> 00:22:07,760
in p043 executors and there's a good

00:22:06,640 --> 00:22:09,919
reason for that you can

00:22:07,760 --> 00:22:11,360
kind of think of it that way you can

00:22:09,919 --> 00:22:13,280
also treat synchronization as an

00:22:11,360 --> 00:22:15,120
opportunity for error reporting

00:22:13,280 --> 00:22:17,840
for example if you are implementing a

00:22:15,120 --> 00:22:20,480
thread barrier using an atomic counter

00:22:17,840 --> 00:22:21,840
starting from zero instead of the s2d

00:22:20,480 --> 00:22:24,000
latch example

00:22:21,840 --> 00:22:25,440
each execution agent that encounters an

00:22:24,000 --> 00:22:26,960
error can add a big number instead of

00:22:25,440 --> 00:22:28,640
just one to that counter

00:22:26,960 --> 00:22:30,559
and so the final count is bigger than

00:22:28,640 --> 00:22:33,520
the number of agents that would tell you

00:22:30,559 --> 00:22:33,520
that some error occurred

00:22:34,720 --> 00:22:38,080
the final technique that we've learned

00:22:36,400 --> 00:22:38,960
from mpi for handling errors is

00:22:38,080 --> 00:22:42,159
something that i call

00:22:38,960 --> 00:22:44,320
out of band error reporting here

00:22:42,159 --> 00:22:45,520
when i say out of band i mean

00:22:44,320 --> 00:22:47,520
asynchronous or not

00:22:45,520 --> 00:22:49,360
synchronizing with respect to whatever

00:22:47,520 --> 00:22:52,000
else you're doing

00:22:49,360 --> 00:22:54,960
mpi has this interesting function called

00:22:52,000 --> 00:22:56,159
a non-blocking barrier mpi barrier

00:22:54,960 --> 00:22:58,480
that's kind of a funny phrase

00:22:56,159 --> 00:23:01,440
non-blocking barrier what does that mean

00:22:58,480 --> 00:23:02,799
it means something like you check in and

00:23:01,440 --> 00:23:04,320
when you're ready to wait on everyone

00:23:02,799 --> 00:23:06,080
else then you can wait so it's kind of a

00:23:04,320 --> 00:23:08,240
two-phase process

00:23:06,080 --> 00:23:09,600
now collectives in mti are not ordered

00:23:08,240 --> 00:23:10,799
with respect to other kinds of

00:23:09,600 --> 00:23:13,360
communication

00:23:10,799 --> 00:23:15,039
so messages it's not like a memory fence

00:23:13,360 --> 00:23:16,799
it's different messages can cross over

00:23:15,039 --> 00:23:19,600
the barrier

00:23:16,799 --> 00:23:21,200
um also when mpi says non-blocking it's

00:23:19,600 --> 00:23:22,320
really important to distinguish that

00:23:21,200 --> 00:23:24,080
from makes

00:23:22,320 --> 00:23:25,760
progress in the background makes

00:23:24,080 --> 00:23:28,480
asynchronous progress

00:23:25,760 --> 00:23:30,400
mpi does not promise anything about

00:23:28,480 --> 00:23:32,000
making progress in the background

00:23:30,400 --> 00:23:34,000
communication doesn't have to happen

00:23:32,000 --> 00:23:34,880
until you call a function that says wait

00:23:34,000 --> 00:23:37,039
or test

00:23:34,880 --> 00:23:39,280
until the thing is done so it's a little

00:23:37,039 --> 00:23:41,919
bit like in the c plus plus standard

00:23:39,280 --> 00:23:43,840
async if you don't specify a launch

00:23:41,919 --> 00:23:45,760
policy to async

00:23:43,840 --> 00:23:47,039
the standard could decide not to do

00:23:45,760 --> 00:23:49,679
anything until you

00:23:47,039 --> 00:23:50,640
call get on the resulting future however

00:23:49,679 --> 00:23:52,799
you can poll

00:23:50,640 --> 00:23:54,240
to force progress in fti so you can

00:23:52,799 --> 00:23:55,440
start the barrier and when you're ready

00:23:54,240 --> 00:23:57,919
to wait on the barrier

00:23:55,440 --> 00:23:58,880
you can call test test task test until

00:23:57,919 --> 00:24:01,279
it succeeds

00:23:58,880 --> 00:24:03,120
and mpi is required to make progress on

00:24:01,279 --> 00:24:05,679
that

00:24:03,120 --> 00:24:07,520
and so in this case we can use this

00:24:05,679 --> 00:24:08,799
non-blocking barrier technique to test

00:24:07,520 --> 00:24:11,679
whether an api process

00:24:08,799 --> 00:24:13,360
through or dropped out of communication

00:24:11,679 --> 00:24:14,240
i show that here in the diagram on the

00:24:13,360 --> 00:24:16,400
right

00:24:14,240 --> 00:24:19,360
so the way that that works is first each

00:24:16,400 --> 00:24:21,440
mpi process runs some risky local work

00:24:19,360 --> 00:24:23,120
some local work that might throw on the

00:24:21,440 --> 00:24:25,600
right i show this as this

00:24:23,120 --> 00:24:27,120
diagonally shaded blue diagram it shows

00:24:25,600 --> 00:24:27,520
up a little funny on the street view but

00:24:27,120 --> 00:24:30,559
it's

00:24:27,520 --> 00:24:32,240
kind of this blue region and

00:24:30,559 --> 00:24:33,600
those blue regions aren't synchronized

00:24:32,240 --> 00:24:35,039
across the processes

00:24:33,600 --> 00:24:37,520
that's why you can see that they end at

00:24:35,039 --> 00:24:40,240
different times and when each process

00:24:37,520 --> 00:24:41,760
finishes that local work it checks in by

00:24:40,240 --> 00:24:42,559
calling this non-blocking barrier

00:24:41,760 --> 00:24:43,919
function

00:24:42,559 --> 00:24:45,760
and so the different processes checking

00:24:43,919 --> 00:24:47,840
at different times

00:24:45,760 --> 00:24:49,760
and then the processes wait on whether

00:24:47,840 --> 00:24:51,679
the other processes have finished

00:24:49,760 --> 00:24:53,679
and they do that by spinning on the mpi

00:24:51,679 --> 00:24:55,039
test function with the timeout

00:24:53,679 --> 00:24:57,039
process can also do other work

00:24:55,039 --> 00:24:58,720
speculatively while spinning

00:24:57,039 --> 00:25:00,400
if there's a timeout you assume that the

00:24:58,720 --> 00:25:02,400
other process some other process may

00:25:00,400 --> 00:25:03,760
have died so you call npi abort to stop

00:25:02,400 --> 00:25:06,400
the program

00:25:03,760 --> 00:25:08,000
if if the test succeeds you know that

00:25:06,400 --> 00:25:09,200
all the processes have checked in so

00:25:08,000 --> 00:25:11,760
they all made it safely through the

00:25:09,200 --> 00:25:11,760
risky work

00:25:12,159 --> 00:25:16,799
in c plus um the equivalent of

00:25:15,279 --> 00:25:18,640
out-of-band communication or the

00:25:16,799 --> 00:25:19,360
generalization of that idea is atomic

00:25:18,640 --> 00:25:21,279
updates

00:25:19,360 --> 00:25:22,960
in c-plus-plus lock-free atomic updates

00:25:21,279 --> 00:25:25,039
do not block so you can use them

00:25:22,960 --> 00:25:26,640
anywhere in parallel algorithms or tasks

00:25:25,039 --> 00:25:28,400
and you could stash away some error

00:25:26,640 --> 00:25:30,400
information later

00:25:28,400 --> 00:25:31,600
and then read it when you're done and

00:25:30,400 --> 00:25:33,679
this is a good thing to do in

00:25:31,600 --> 00:25:35,039
sort or other reduction or other

00:25:33,679 --> 00:25:36,640
algorithms that are not easy to

00:25:35,039 --> 00:25:38,159
transform into reductions

00:25:36,640 --> 00:25:40,240
you can also use it if you don't want to

00:25:38,159 --> 00:25:40,960
pay for the reduction so if errors are

00:25:40,240 --> 00:25:42,159
rare it's

00:25:40,960 --> 00:25:44,000
it's not really a zero overhead

00:25:42,159 --> 00:25:47,120
abstraction to to call it a

00:25:44,000 --> 00:25:48,400
a reduction um it's true that atomic

00:25:47,120 --> 00:25:50,400
updates may hinder some kinds of

00:25:48,400 --> 00:25:53,039
compiler optimizations

00:25:50,400 --> 00:25:54,400
but um that may only be a concern if the

00:25:53,039 --> 00:25:56,000
loop is really too optimized for

00:25:54,400 --> 00:25:58,159
recoverable exceptions so it's less of a

00:25:56,000 --> 00:26:01,120
big deal

00:25:58,159 --> 00:26:03,120
so just to summarize um c plus parallel

00:26:01,120 --> 00:26:04,799
algorithms and asynchronous tasks make

00:26:03,120 --> 00:26:06,880
it harder to do and like error handling

00:26:04,799 --> 00:26:08,640
make it harder to

00:26:06,880 --> 00:26:10,000
discover errors that errors have

00:26:08,640 --> 00:26:13,440
occurred and make it harder

00:26:10,000 --> 00:26:15,039
to recover from them and so

00:26:13,440 --> 00:26:17,440
we can deal with that by using design

00:26:15,039 --> 00:26:19,120
patterns that mpi parallel programmers

00:26:17,440 --> 00:26:20,720
have developed over time

00:26:19,120 --> 00:26:22,240
for example we can turn exceptions into

00:26:20,720 --> 00:26:23,679
values we can reduce over error

00:26:22,240 --> 00:26:24,799
information

00:26:23,679 --> 00:26:26,559
we can avoid deadlock due to

00:26:24,799 --> 00:26:28,320
synchronization and use synchronization

00:26:26,559 --> 00:26:29,520
as an opportunity to communicate error

00:26:28,320 --> 00:26:31,279
information

00:26:29,520 --> 00:26:33,919
and we can exploit out-of-band

00:26:31,279 --> 00:26:36,480
communication such as atomic updates to

00:26:33,919 --> 00:26:38,400
detect and report errors and i'd like to

00:26:36,480 --> 00:26:38,960
thank my new employer stellar science

00:26:38,400 --> 00:26:40,480
for

00:26:38,960 --> 00:26:42,240
funding my efforts and giving me the

00:26:40,480 --> 00:26:44,080
opportunity to give you this talk

00:26:42,240 --> 00:26:45,840
and we're hiring modern c plus software

00:26:44,080 --> 00:26:52,159
engineers and uh

00:26:45,840 --> 00:26:54,320
thank you all for for visiting

00:26:52,159 --> 00:26:55,520
and just uh i'm going to answer

00:26:54,320 --> 00:26:57,919
questions now let's see if you have any

00:26:55,520 --> 00:26:57,919
questions

00:27:01,200 --> 00:27:07,279
do you have questions

00:27:04,240 --> 00:27:09,120
oh thank you

00:27:07,279 --> 00:27:12,080
yes please post your questions in the q

00:27:09,120 --> 00:27:12,080
a tab thank you

00:27:14,799 --> 00:27:17,919
well they don't have any questions

00:27:16,559 --> 00:27:21,840
aren't you curious about the spelling of

00:27:17,919 --> 00:27:21,840
meep meep

00:27:21,919 --> 00:27:25,360
they're not curious about the spelling

00:27:23,039 --> 00:27:28,240
of i'm going to tell the story anyway

00:27:25,360 --> 00:27:29,039
so paul julian was the background artist

00:27:28,240 --> 00:27:32,159
for the

00:27:29,039 --> 00:27:34,000
roadrunner cartoon with wiley coyote

00:27:32,159 --> 00:27:35,520
and the picture you see on the screen is

00:27:34,000 --> 00:27:36,559
actually a real road runner that's what

00:27:35,520 --> 00:27:38,720
they look like

00:27:36,559 --> 00:27:40,880
um i live in albuquerque we we have

00:27:38,720 --> 00:27:42,960
these in our yard they come and visit us

00:27:40,880 --> 00:27:44,880
in fact one of them raised a chick in

00:27:42,960 --> 00:27:47,360
our front yard because there was lots of

00:27:44,880 --> 00:27:48,399
grass in which you could hide and in the

00:27:47,360 --> 00:27:50,000
cartoon um the

00:27:48,399 --> 00:27:51,440
the background artist for the roadrunner

00:27:50,000 --> 00:27:52,720
cartoon spells

00:27:51,440 --> 00:27:56,559
the sound that the road runner in the

00:27:52,720 --> 00:28:03,840
cartoon makes says h-m-e-e-p

00:27:56,559 --> 00:28:03,840
now you know

00:28:07,520 --> 00:28:11,120
awesome and if you have any detailed

00:28:09,120 --> 00:28:14,159
questions i'm just going to stick around

00:28:11,120 --> 00:28:16,720
for the next half hour and i'll i'm not

00:28:14,159 --> 00:28:18,080
going to change anything i'll i'll keep

00:28:16,720 --> 00:28:19,120
connected in the same way and stay in

00:28:18,080 --> 00:28:22,240
the same room

00:28:19,120 --> 00:28:22,240
oh good you have a question

00:28:23,120 --> 00:28:31,360
how would one write a custom executor

00:28:26,640 --> 00:28:33,840
um uh i would recommend reading um

00:28:31,360 --> 00:28:35,520
the executor's paper for that um i'm not

00:28:33,840 --> 00:28:39,440
gonna give a tutorial on writing

00:28:35,520 --> 00:28:40,960
an executor um that's a little bit

00:28:39,440 --> 00:28:42,080
that's a little bit out of out of scope

00:28:40,960 --> 00:28:43,760
for this talk but it's a really

00:28:42,080 --> 00:28:46,399
interesting question

00:28:43,760 --> 00:28:47,679
i'm more interested as a as a parallel

00:28:46,399 --> 00:28:48,720
as someone who writes parallel

00:28:47,679 --> 00:28:50,880
algorithms

00:28:48,720 --> 00:28:52,880
i'm more interested in the programming

00:28:50,880 --> 00:28:54,320
model that's presented to me

00:28:52,880 --> 00:28:55,600
rather than implementing the programming

00:28:54,320 --> 00:28:56,960
model i think that's also a very

00:28:55,600 --> 00:28:58,880
interesting question but that's a little

00:28:56,960 --> 00:29:02,080
bit out of my expertise

00:28:58,880 --> 00:29:03,520
where is stellar science located um

00:29:02,080 --> 00:29:06,720
stellar science uh

00:29:03,520 --> 00:29:07,919
has two main offices um our biggest uh

00:29:06,720 --> 00:29:09,919
and our home office is here in

00:29:07,919 --> 00:29:12,240
albuquerque new mexico

00:29:09,919 --> 00:29:13,679
and we have another smaller office in

00:29:12,240 --> 00:29:16,640
virginia

00:29:13,679 --> 00:29:17,039
and um we have a lot of remote workers

00:29:16,640 --> 00:29:19,679
um

00:29:17,039 --> 00:29:21,600
i we and we've been hiring through this

00:29:19,679 --> 00:29:23,039
whole time and we're still hiring

00:29:21,600 --> 00:29:25,279
and so if you're interested uh get in

00:29:23,039 --> 00:29:26,799
touch with me

00:29:25,279 --> 00:29:29,200
i think we have a little over 100

00:29:26,799 --> 00:29:32,799
employees right now

00:29:29,200 --> 00:29:32,799
but we've been around for about 20 years

00:29:38,080 --> 00:29:41,840
okay well um looks like the end of my

00:29:40,240 --> 00:29:42,880
half hour um as i said i'm going to

00:29:41,840 --> 00:29:45,039
stick around here

00:29:42,880 --> 00:29:45,919
for the ama um thank you all for

00:29:45,039 --> 00:29:47,200
attending

00:29:45,919 --> 00:29:49,039
um if you want to go to the next talk

00:29:47,200 --> 00:29:51,039
and i want to stop you but feel free to

00:29:49,039 --> 00:29:55,840
stick around and ask me questions

00:29:51,039 --> 00:29:55,840
thank you

00:30:11,919 --> 00:30:14,000

YouTube URL: https://www.youtube.com/watch?v=DpLZ4pnrx0o


