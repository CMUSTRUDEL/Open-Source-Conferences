Title: CppCon 2017: Olivier Giroux "Designing (New) C++ Hardware”
Publication date: 2017-10-02
Playlist: CppCon 2017
Description: 
	http://CppCon.org
—
Presentation Slides, PDFs, Source Code and other presenter materials are available at: https://github.com/CppCon/CppCon2017
—
You can run C++ on any computer you want, as long as it pretends it is an 80’s computer. Conveniently most computers pretend to be 80’s computers – with extras, but nothing too radical – because they want to run C++. This contract isn’t written down anywhere, but both sides are absolutely bound by it. 

In this talk we’ll walk through the adaptation of the most radical new architecture to run C++ in decades, NVIDIA Volta. The talk contents will be divided into four parts that align to these abstract machine semantics: execution agents, progress guarantees, the object model, and the consistency model. 

We will close on new C++ features that make it easier, not harder, for hardware to support C++.
— 
Olivier Giroux: NVIDIA, Principal Architect

Olivier Giroux has worked on eight GPU and four SM architecture generations released by NVIDIA. Lately, he works to clarify the forms and semantics of valid GPU programs, present and future. He was the programming model lead for the new NVIDIA Volta architecture. He is a member of WG21, the ISO C++ committee, and is a passionate contributor to C++'s forward progress guarantees and memory model.
—
Videos Filmed & Edited by Bash Films: http://www.BashFilms.com
Captions: 
	00:00:00,030 --> 00:00:06,150
hello everybody to my talk my name is

00:00:03,179 --> 00:00:09,960
Nick a Jew who I work for in video I've

00:00:06,150 --> 00:00:12,480
been at Nvidia for 15 years now I've

00:00:09,960 --> 00:00:16,020
worked on eight different GPU

00:00:12,480 --> 00:00:18,359
architectures and more recently for

00:00:16,020 --> 00:00:20,730
different SM architectures within within

00:00:18,359 --> 00:00:24,269
these GPU architectures and SM in this

00:00:20,730 --> 00:00:26,460
case is a compute core so I guess that

00:00:24,269 --> 00:00:28,410
makes me old enough that there are some

00:00:26,460 --> 00:00:32,640
people here who've never used GPUs that

00:00:28,410 --> 00:00:35,370
I haven't worked on now I'm here to tell

00:00:32,640 --> 00:00:38,190
you something really exciting and that

00:00:35,370 --> 00:00:42,570
thing is that Nvidia actually really

00:00:38,190 --> 00:00:44,579
cares about C++ so much so that I can

00:00:42,570 --> 00:00:46,590
come out and say quite clearly that we

00:00:44,579 --> 00:00:48,840
designed this one which is Volta the one

00:00:46,590 --> 00:00:53,100
we announced this year specifically wood

00:00:48,840 --> 00:00:57,420
C++ in mind and so my talk designing C++

00:00:53,100 --> 00:01:00,210
hardware um hey I in my view of it it is

00:00:57,420 --> 00:01:03,420
both designing new hardware for C++ and

00:01:00,210 --> 00:01:06,540
designing new C++ or hardware and doing

00:01:03,420 --> 00:01:08,760
all that at the same time so to put me

00:01:06,540 --> 00:01:14,729
into some coordinates and space here for

00:01:08,760 --> 00:01:18,420
you I've said I work in Nvidia and the

00:01:14,729 --> 00:01:21,600
GPU architect I'm also a member of the

00:01:18,420 --> 00:01:23,880
g20 one which the contrast is nothing

00:01:21,600 --> 00:01:25,049
great but I'm I'm also a member of W 21

00:01:23,880 --> 00:01:28,350
I've been on the community for several

00:01:25,049 --> 00:01:30,810
years now and I'm a member of an even

00:01:28,350 --> 00:01:33,840
smaller community that deals in memory

00:01:30,810 --> 00:01:36,060
models these obtuse axiomatic

00:01:33,840 --> 00:01:38,490
definitions of what it means for memory

00:01:36,060 --> 00:01:41,850
can be memory okay so that's my

00:01:38,490 --> 00:01:44,790
perspective and the agenda today is

00:01:41,850 --> 00:01:47,939
largely not code and in fact there will

00:01:44,790 --> 00:01:50,070
be this is this is an accurate depiction

00:01:47,939 --> 00:01:52,320
of what we're gonna talk about from a

00:01:50,070 --> 00:01:55,729
high level when we do talk about coding

00:01:52,320 --> 00:01:58,229
will actually be fairly bad code I

00:01:55,729 --> 00:02:01,350
apologize in advance

00:01:58,229 --> 00:02:04,320
okay so now I have a question to start

00:02:01,350 --> 00:02:06,479
with and that question is what is it

00:02:04,320 --> 00:02:09,599
about sleepless bus that makes it

00:02:06,479 --> 00:02:12,269
portable or we like to think that C and

00:02:09,599 --> 00:02:16,160
C++ are languages that are portable to a

00:02:12,269 --> 00:02:18,930
wide variety of computer systems but

00:02:16,160 --> 00:02:19,980
when you look out today is there that

00:02:18,930 --> 00:02:23,069
much variety

00:02:19,980 --> 00:02:25,140
that's an evidence of that okay so what

00:02:23,069 --> 00:02:28,140
is it about C++ that makes it portable

00:02:25,140 --> 00:02:29,610
and I think that if we ask this question

00:02:28,140 --> 00:02:32,370
at the watercooler

00:02:29,610 --> 00:02:33,840
in most of your organizations we we

00:02:32,370 --> 00:02:37,049
might get a list that looks a bit like

00:02:33,840 --> 00:02:38,640
that like there are things like you

00:02:37,049 --> 00:02:40,380
don't really get to know how big char is

00:02:38,640 --> 00:02:44,220
you don't really get to know how big

00:02:40,380 --> 00:02:46,019
anything is your integers they might be

00:02:44,220 --> 00:02:47,700
two's complement they might not your

00:02:46,019 --> 00:02:50,099
floats they might be high Tripoli or

00:02:47,700 --> 00:02:52,110
they might not you don't really know

00:02:50,099 --> 00:02:54,599
about the Indian nough sub addressing

00:02:52,110 --> 00:02:56,370
you know you know you don't that get to

00:02:54,599 --> 00:02:58,500
control the alignment of certain things

00:02:56,370 --> 00:03:00,150
especially you cannot undercut the

00:02:58,500 --> 00:03:02,430
minimum alignment of a site for the

00:03:00,150 --> 00:03:04,799
platform shows and then there's this

00:03:02,430 --> 00:03:06,389
thing called segmented memory that you

00:03:04,799 --> 00:03:09,079
know that I see people supposed to

00:03:06,389 --> 00:03:13,650
supports through various arcane rules

00:03:09,079 --> 00:03:17,720
okay so in this view C++ has robust

00:03:13,650 --> 00:03:21,450
support for all of these weird things

00:03:17,720 --> 00:03:24,239
across the storage abstractions of the

00:03:21,450 --> 00:03:25,940
language across the earth mythic

00:03:24,239 --> 00:03:28,410
abstractions of a language largely

00:03:25,940 --> 00:03:32,819
through various forms of undefined

00:03:28,410 --> 00:03:34,350
behavior on you know on on you know on

00:03:32,819 --> 00:03:36,840
the everything to think of integers for

00:03:34,350 --> 00:03:39,720
example you know not that that they are

00:03:36,840 --> 00:03:43,440
not two's complement okay now I have

00:03:39,720 --> 00:03:45,090
some I have some this may not may come

00:03:43,440 --> 00:03:47,489
as a shock you not come as a shock but

00:03:45,090 --> 00:03:49,500
largely most of this also has you

00:03:47,489 --> 00:03:52,109
covered through the 20th century Trivial

00:03:49,500 --> 00:03:55,260
Pursuit meaning there are essentially no

00:03:52,109 --> 00:03:58,739
remaining well Alliance systems that

00:03:55,260 --> 00:03:59,880
care about any of this except for except

00:03:58,739 --> 00:04:02,970
for the bottom two which may surprise

00:03:59,880 --> 00:04:05,819
some of you so we've got this stuff in

00:04:02,970 --> 00:04:07,980
C++ because C++ has robust support for

00:04:05,819 --> 00:04:09,760
these things which are really you know

00:04:07,980 --> 00:04:14,010
is support for

00:04:09,760 --> 00:04:17,949
Parcells you know not not living systems

00:04:14,010 --> 00:04:20,530
okay so how useful is that in my

00:04:17,949 --> 00:04:22,600
experience of trying to implement or

00:04:20,530 --> 00:04:23,350
trying to create a new implementation of

00:04:22,600 --> 00:04:25,270
C++

00:04:23,350 --> 00:04:29,320
I haven't found these things to me very

00:04:25,270 --> 00:04:31,360
useful um and by the way the odds of

00:04:29,320 --> 00:04:32,800
making a genuinely new computer

00:04:31,360 --> 00:04:34,419
architecture that supports the bus

00:04:32,800 --> 00:04:36,729
closer are really stacked against you

00:04:34,419 --> 00:04:40,300
the odds are you're just gonna gravitate

00:04:36,729 --> 00:04:43,990
to copying one of the pre-existing okay

00:04:40,300 --> 00:04:46,060
so why is this why is most of this not

00:04:43,990 --> 00:04:48,699
helpful well it's because behind these

00:04:46,060 --> 00:04:51,610
things are either a lot of false choices

00:04:48,699 --> 00:04:54,070
or bad choices if you're going to create

00:04:51,610 --> 00:04:56,199
a new architecture today and say you're

00:04:54,070 --> 00:04:59,139
getting funding to create a new

00:04:56,199 --> 00:05:00,250
processor tape out well most of these

00:04:59,139 --> 00:05:03,580
options are really going to be dictated

00:05:00,250 --> 00:05:05,410
to you you know if you're making a new

00:05:03,580 --> 00:05:07,300
CPU pretty much everybody's gonna go

00:05:05,410 --> 00:05:10,180
match but they are 64 is doing because

00:05:07,300 --> 00:05:11,770
Aaron 64 went ahead and updated the

00:05:10,180 --> 00:05:13,600
Linux kernel with all the support they

00:05:11,770 --> 00:05:14,860
needed are you gonna do the same no

00:05:13,600 --> 00:05:19,180
probably not you're gonna want to like

00:05:14,860 --> 00:05:21,280
stream behind them okay so for a new CPU

00:05:19,180 --> 00:05:24,160
you'd probably look to matching your 64

00:05:21,280 --> 00:05:27,760
if you're making a GPU you have to match

00:05:24,160 --> 00:05:30,970
the hosts if the host is using a 32-bit

00:05:27,760 --> 00:05:33,580
two's complement int you also need to be

00:05:30,970 --> 00:05:35,710
using a 32-bit two's complement int

00:05:33,580 --> 00:05:37,590
because otherwise the GPM won't be

00:05:35,710 --> 00:05:40,750
interoperable with the CPU in fact

00:05:37,590 --> 00:05:44,560
receipt for GPUs what that means is that

00:05:40,750 --> 00:05:46,900
we end up matching all the hosts it used

00:05:44,560 --> 00:05:49,330
to be that nvidia gpus had to switch in

00:05:46,900 --> 00:05:50,639
them to switch them from little-endian

00:05:49,330 --> 00:05:52,510
to big-endian

00:05:50,639 --> 00:05:55,630
because we wanted to be attached to

00:05:52,510 --> 00:05:57,880
begin the insistence okay and that went

00:05:55,630 --> 00:06:01,539
away the beginning systems went away and

00:05:57,880 --> 00:06:04,210
so did this way and then other choices

00:06:01,539 --> 00:06:06,800
are just bad at this point I know sure

00:06:04,210 --> 00:06:09,110
you could build one

00:06:06,800 --> 00:06:12,469
sure you could build an on night relief

00:06:09,110 --> 00:06:14,930
local machine but at this point in 2017

00:06:12,469 --> 00:06:16,909
you would get really negligible area

00:06:14,930 --> 00:06:19,400
savings or power savings from that but

00:06:16,909 --> 00:06:22,610
you would you would pay on the software

00:06:19,400 --> 00:06:24,650
side your users would suffer from all

00:06:22,610 --> 00:06:27,050
the rediscovering all these knits which

00:06:24,650 --> 00:06:28,759
we know in the C++ community and it

00:06:27,050 --> 00:06:30,680
makes us really cool and the water

00:06:28,759 --> 00:06:32,870
cooler when we talk about that but um

00:06:30,680 --> 00:06:35,810
but most programmers don't really worry

00:06:32,870 --> 00:06:37,789
about it on a day-to-day basis okay

00:06:35,810 --> 00:06:41,080
so now I'm gonna make sort of a weird

00:06:37,789 --> 00:06:43,580
comparison I'm going to compare the

00:06:41,080 --> 00:06:47,870
manufacturing processes that nvidia tape

00:06:43,580 --> 00:06:50,090
outs used at the time of various c++

00:06:47,870 --> 00:06:51,409
revisions and I think this weird

00:06:50,090 --> 00:06:55,009
comparison makes sense in my head

00:06:51,409 --> 00:06:58,430
because in my view C++ implementations

00:06:55,009 --> 00:07:01,849
go all the way down to the method by

00:06:58,430 --> 00:07:04,039
meddling in a periodic table level yeah

00:07:01,849 --> 00:07:06,830
they don't like they don't stop at a

00:07:04,039 --> 00:07:09,169
particular arbitrary point in the middle

00:07:06,830 --> 00:07:11,770
called an ice ax the a C++

00:07:09,169 --> 00:07:13,939
implementation is not just the job of

00:07:11,770 --> 00:07:15,979
taking a program and lowering it to an

00:07:13,939 --> 00:07:18,050
ice ax it's taking a program and making

00:07:15,979 --> 00:07:22,219
a complete physical manifestation of

00:07:18,050 --> 00:07:25,460
that program all the way down okay so if

00:07:22,219 --> 00:07:28,009
you were to tape out C++ in those years

00:07:25,460 --> 00:07:31,460
what manufacturing process would you use

00:07:28,009 --> 00:07:34,279
and these are to skip okay so in 1998

00:07:31,460 --> 00:07:39,620
and video was using 350 nanometer

00:07:34,279 --> 00:07:43,370
lithography in C++ 17 we're using 12

00:07:39,620 --> 00:07:45,800
nanometer lithography a bunch of design

00:07:43,370 --> 00:07:48,080
you would think such a dramatic change

00:07:45,800 --> 00:07:50,180
in the landscape would affect certain

00:07:48,080 --> 00:07:52,759
design decisions there are design

00:07:50,180 --> 00:07:58,339
decisions you make differently when you

00:07:52,759 --> 00:08:00,919
have by now giant transistors or tiny

00:07:58,339 --> 00:08:03,560
transistors okay so at some point in

00:08:00,919 --> 00:08:04,219
there I Triple E floating point became a

00:08:03,560 --> 00:08:06,740
no-brainer

00:08:04,219 --> 00:08:11,300
now as far as we are concerned we would

00:08:06,740 --> 00:08:13,460
not bother taping out for our main math

00:08:11,300 --> 00:08:15,210
units for a primary math units we would

00:08:13,460 --> 00:08:17,050
not bother taking out

00:08:15,210 --> 00:08:20,740
nonequivalent it just doesn't really

00:08:17,050 --> 00:08:22,509
make sense the units are small uh are at

00:08:20,740 --> 00:08:26,310
around this time like around around

00:08:22,509 --> 00:08:28,599
between 130 and 40 it's just a non-issue

00:08:26,310 --> 00:08:30,789
so we're not interested in that freedom

00:08:28,599 --> 00:08:33,399
starting on that point and my argument

00:08:30,789 --> 00:08:35,500
here is most of the other things that

00:08:33,399 --> 00:08:37,810
are on the previous table they all fall

00:08:35,500 --> 00:08:38,529
to a similar argument at some point on

00:08:37,810 --> 00:08:40,390
the timeline

00:08:38,529 --> 00:08:43,560
they just no longer they're no longer a

00:08:40,390 --> 00:08:47,470
freedom you're interested in exercising

00:08:43,560 --> 00:08:51,040
okay so I would say that there are sort

00:08:47,470 --> 00:08:52,390
of these epochs of C++ design and by

00:08:51,040 --> 00:08:56,890
here I'm really talking about this is a

00:08:52,390 --> 00:08:58,600
language design in the early days in

00:08:56,890 --> 00:09:02,170
this revisionist history that I'm doing

00:08:58,600 --> 00:09:03,310
here in the early days priority is to

00:09:02,170 --> 00:09:05,380
make sure that you can make little

00:09:03,310 --> 00:09:07,300
systems legal systems that have decided

00:09:05,380 --> 00:09:09,370
to throw overboard some complexity

00:09:07,300 --> 00:09:11,350
because you're coming from a mini

00:09:09,370 --> 00:09:13,180
computer to a micro computer and you

00:09:11,350 --> 00:09:15,040
couldn't afford to have all these come

00:09:13,180 --> 00:09:18,270
all these units and have all of them

00:09:15,040 --> 00:09:23,020
behave the same way until the language

00:09:18,270 --> 00:09:25,329
enables lower qy options lower quality

00:09:23,020 --> 00:09:28,390
of implementation options but in the

00:09:25,329 --> 00:09:30,510
future we have a different problem our

00:09:28,390 --> 00:09:37,750
problem is making sure that C++

00:09:30,510 --> 00:09:38,890
legalizes really big systems okay so for

00:09:37,750 --> 00:09:41,320
the rest of the talk we're going to talk

00:09:38,890 --> 00:09:44,350
about sort of non classic portability

00:09:41,320 --> 00:09:46,480
and these are these two phases that I

00:09:44,350 --> 00:09:50,200
just that I just talked about you know a

00:09:46,480 --> 00:09:53,260
and then an era of transistor scarcity

00:09:50,200 --> 00:09:55,209
and now a completely different era of

00:09:53,260 --> 00:09:58,570
transistor abundance in seven nanometer

00:09:55,209 --> 00:10:01,540
you can put so many question is can see

00:09:58,570 --> 00:10:03,730
puss-puss actually use them if I if I if

00:10:01,540 --> 00:10:04,620
I fill a really big die with

00:10:03,730 --> 00:10:11,890
functionality

00:10:04,620 --> 00:10:14,740
how will C++ get to it okay um and I

00:10:11,890 --> 00:10:17,199
would say also slightly revisionist

00:10:14,740 --> 00:10:21,269
history I would say that in C++ 17 we

00:10:17,199 --> 00:10:21,269
have taken steps in that lurch

00:10:21,300 --> 00:10:29,310
okay so what is it about the next C++

00:10:24,930 --> 00:10:33,149
that makes it more portable I would like

00:10:29,310 --> 00:10:37,130
to talk to you about topology here's a

00:10:33,149 --> 00:10:39,899
really simplistic diagram of a computer

00:10:37,130 --> 00:10:44,279
and you see these diagrams in you know

00:10:39,899 --> 00:10:47,040
in in textbooks I have a line and I'm

00:10:44,279 --> 00:10:51,209
gonna call it bus and then I hook a CPU

00:10:47,040 --> 00:10:53,790
rectangle and a DRAM rectangle and and

00:10:51,209 --> 00:10:59,310
I'm gonna call this a 1980s type of

00:10:53,790 --> 00:11:02,370
topology okay so there's my 1980s

00:10:59,310 --> 00:11:04,110
computer oh and there's other things in

00:11:02,370 --> 00:11:06,060
my computer of course my computer's got

00:11:04,110 --> 00:11:10,110
lots of other features all of them are

00:11:06,060 --> 00:11:12,660
IO okay so there's the CPU and there's

00:11:10,110 --> 00:11:16,050
the RAM and then everything else in the

00:11:12,660 --> 00:11:18,240
world is IO and and really the interface

00:11:16,050 --> 00:11:22,110
here was probably invented to serve as

00:11:18,240 --> 00:11:25,050
tick drives but it's okay we'll work

00:11:22,110 --> 00:11:27,420
with it all right so then C++ comes in

00:11:25,050 --> 00:11:29,459
and obviously in C++ I'm not programming

00:11:27,420 --> 00:11:32,010
a CPU and dear my record I'm using

00:11:29,459 --> 00:11:34,829
extractions so c plus plus introduces

00:11:32,010 --> 00:11:36,750
layers of abstractions in fact c++ alone

00:11:34,829 --> 00:11:39,930
brings in at least three layers of

00:11:36,750 --> 00:11:42,209
abstraction for each each side we

00:11:39,930 --> 00:11:44,970
abstract the CPU using threads we

00:11:42,209 --> 00:11:48,260
virtualize it using threads we have this

00:11:44,970 --> 00:11:51,630
abstract notion of an execution engine

00:11:48,260 --> 00:11:53,610
that a thread is a concrete example of

00:11:51,630 --> 00:11:57,810
and then I have the thread of execution

00:11:53,610 --> 00:11:59,610
which is a a chain of value computations

00:11:57,810 --> 00:12:01,920
in your program one independent chain of

00:11:59,610 --> 00:12:03,540
value computation in your program is the

00:12:01,920 --> 00:12:06,089
thread of execution that at that point

00:12:03,540 --> 00:12:08,190
that's pretty much your code okay so

00:12:06,089 --> 00:12:09,779
those are layers of abstraction there

00:12:08,190 --> 00:12:13,140
and then on the dear hand side I have a

00:12:09,779 --> 00:12:15,390
similar thing I have allocations which

00:12:13,140 --> 00:12:21,810
have objects and then the objects hold

00:12:15,390 --> 00:12:24,560
values okay now from that time to more

00:12:21,810 --> 00:12:27,510
or less today here's what we've achieved

00:12:24,560 --> 00:12:30,899
we've been able to dice the Box on the

00:12:27,510 --> 00:12:34,579
left in two parts we have achieved that

00:12:30,899 --> 00:12:38,629
now they still pretty much have to be

00:12:34,579 --> 00:12:40,009
as one they have to be friends they have

00:12:38,629 --> 00:12:41,779
to be friends when they access the

00:12:40,009 --> 00:12:44,629
memory they need in order to make sure

00:12:41,779 --> 00:12:46,160
that that they're all coherent for

00:12:44,629 --> 00:12:49,220
example okay

00:12:46,160 --> 00:12:54,739
now now here's here's the problem that I

00:12:49,220 --> 00:12:57,019
have my design expands the topology of

00:12:54,739 --> 00:12:59,899
the system in a way that was not

00:12:57,019 --> 00:13:01,519
originally envisioned it was not

00:12:59,899 --> 00:13:04,239
originally envisioned in simple spots

00:13:01,519 --> 00:13:08,749
there are no abstractions to deal with

00:13:04,239 --> 00:13:10,730
another coprocessor of coprocessors went

00:13:08,749 --> 00:13:15,579
away when the 3d six integrated the FPU

00:13:10,730 --> 00:13:18,739
right so so there are no cool processors

00:13:15,579 --> 00:13:20,269
the like like the notion of whether

00:13:18,739 --> 00:13:22,509
they're equal or not this is like

00:13:20,269 --> 00:13:25,040
doesn't compute because there are not

00:13:22,509 --> 00:13:26,420
there are no abstractions for that but

00:13:25,040 --> 00:13:29,299
you know even at the fundamental level

00:13:26,420 --> 00:13:31,249
even in this in my computer architecture

00:13:29,299 --> 00:13:34,790
diagram it also doesn't work like how is

00:13:31,249 --> 00:13:37,579
microprocessor equivalent to IO and this

00:13:34,790 --> 00:13:41,480
is actually a real weird thing today

00:13:37,579 --> 00:13:44,689
operating systems program my GPUs using

00:13:41,480 --> 00:13:47,509
IO controls I up calls it is basically

00:13:44,689 --> 00:13:51,529
receiving these commands as if it were a

00:13:47,509 --> 00:13:54,559
disk drive you know okay but it's it's

00:13:51,529 --> 00:13:56,299
not it has it has a very large number of

00:13:54,559 --> 00:13:58,129
cores in it effectively it has a very

00:13:56,299 --> 00:13:59,899
large number of parts now I put two

00:13:58,129 --> 00:14:04,160
numbers there it sort of depends how you

00:13:59,899 --> 00:14:06,519
interpret it it has it can execute 50

00:14:04,160 --> 00:14:13,939
120 different instructions per clock

00:14:06,519 --> 00:14:15,199
every clock all different but they're

00:14:13,939 --> 00:14:20,269
gonna be like some of them are gonna be

00:14:15,199 --> 00:14:21,919
similar yeah yeah but it has an

00:14:20,269 --> 00:14:23,779
abstraction on top of that also a

00:14:21,919 --> 00:14:28,069
threads of which there's a hundred and

00:14:23,779 --> 00:14:29,959
six 3840 so I don't know how many of you

00:14:28,069 --> 00:14:32,389
have program have written concurrent

00:14:29,959 --> 00:14:34,309
programs with a hundred and sixty-three

00:14:32,389 --> 00:14:37,329
thousand eight hundred forty threads in

00:14:34,309 --> 00:14:41,839
them but that's basically been my summer

00:14:37,329 --> 00:14:44,209
awesome yeah okay so so the future

00:14:41,839 --> 00:14:46,660
skiing ability of C++ goes through

00:14:44,209 --> 00:14:49,190
showing some love for topology

00:14:46,660 --> 00:14:52,130
carrying about more interesting

00:14:49,190 --> 00:14:55,520
topologies that make bigger and bigger

00:14:52,130 --> 00:14:58,430
systems so this diagram here is not even

00:14:55,520 --> 00:15:00,740
particularly crazy this is a system that

00:14:58,430 --> 00:15:04,580
we're shipping there are some of you in

00:15:00,740 --> 00:15:05,930
the room who probably have one yeah so

00:15:04,580 --> 00:15:08,330
this is one of the systems that we're

00:15:05,930 --> 00:15:10,940
shipping that has eight GPUs and two

00:15:08,330 --> 00:15:12,290
CPUs and a bunch of network connections

00:15:10,940 --> 00:15:15,560
and then there's all kinds of

00:15:12,290 --> 00:15:25,040
connectivity in there um we can imagine

00:15:15,560 --> 00:15:29,360
way crazier okay all right so so far

00:15:25,040 --> 00:15:31,850
we've talked about things in C++ that

00:15:29,360 --> 00:15:33,860
make it more affordable and sort of

00:15:31,850 --> 00:15:34,490
future directions that where should we

00:15:33,860 --> 00:15:36,710
should care about

00:15:34,490 --> 00:15:38,330
now this saga stock is is called

00:15:36,710 --> 00:15:41,420
designing C++ hardware so now we're

00:15:38,330 --> 00:15:44,180
gonna switch to how did we design I made

00:15:41,420 --> 00:15:46,340
this statement up front that volta our

00:15:44,180 --> 00:15:48,650
GT architecture from this year the bolt

00:15:46,340 --> 00:15:51,290
of his design for signals bus so let's

00:15:48,650 --> 00:15:54,080
dive into that point I'm breaking it up

00:15:51,290 --> 00:15:56,990
into two parts we're gonna talk about

00:15:54,080 --> 00:15:59,300
how we're attacking the memory vertical

00:15:56,990 --> 00:16:01,370
right there were two verticals on my on

00:15:59,300 --> 00:16:03,350
my previous slide you know how are we

00:16:01,370 --> 00:16:06,080
attacking the memory vertical and he'll

00:16:03,350 --> 00:16:08,600
reattaching the execution burglar yeah

00:16:06,080 --> 00:16:10,250
we're gonna talk about those two so

00:16:08,600 --> 00:16:13,880
let's started but let's start with the

00:16:10,250 --> 00:16:15,770
the memory over at the diagram level and

00:16:13,880 --> 00:16:17,270
the theory level you know as long as I

00:16:15,770 --> 00:16:20,750
as long as I have a diagram with two

00:16:17,270 --> 00:16:24,230
boxes and three lines um you know it it

00:16:20,750 --> 00:16:25,670
appears it appears so simple there's a

00:16:24,230 --> 00:16:27,080
boss there's a bus and there's a

00:16:25,670 --> 00:16:29,030
processor and it's attached to the bus

00:16:27,080 --> 00:16:31,010
and and the memory is also attached to

00:16:29,030 --> 00:16:36,920
the bus and the processor goes and

00:16:31,010 --> 00:16:40,100
accesses the memory just work okay but

00:16:36,920 --> 00:16:42,020
the reality is a lot more like this you

00:16:40,100 --> 00:16:44,900
know the memory is really it's really

00:16:42,020 --> 00:16:47,930
attached to the CPU right it's not

00:16:44,900 --> 00:16:50,030
really on this bus like and and then the

00:16:47,930 --> 00:16:50,750
same he was a very special access to the

00:16:50,030 --> 00:16:54,290
memory and it

00:16:50,750 --> 00:16:55,490
it really and this is this is really you

00:16:54,290 --> 00:16:58,130
can quote me on that it's really my

00:16:55,490 --> 00:17:00,020
terminology I use it regularly ah it

00:16:58,130 --> 00:17:00,890
tastes like memory when the sleep of

00:17:00,020 --> 00:17:03,230
your ghosts memory

00:17:00,890 --> 00:17:07,790
it tastes like memory it has these

00:17:03,230 --> 00:17:09,949
attributes that you expect for example a

00:17:07,790 --> 00:17:13,100
series of loads and stores to the same

00:17:09,949 --> 00:17:16,160
address from the same thread never

00:17:13,100 --> 00:17:17,839
become reordered that sounds like pretty

00:17:16,160 --> 00:17:19,699
key like if I were polling on a memory

00:17:17,839 --> 00:17:22,189
location and I saw that memory location

00:17:19,699 --> 00:17:24,310
become one I would expect that I don't

00:17:22,189 --> 00:17:28,839
need to test it again after the polling

00:17:24,310 --> 00:17:28,839
daddy wouldn't just become zero again

00:17:30,820 --> 00:17:37,160
now that's really not the life

00:17:35,390 --> 00:17:39,890
experience of someone working on the

00:17:37,160 --> 00:17:41,960
other side of this line I said that

00:17:39,890 --> 00:17:45,650
everything below this line is treated as

00:17:41,960 --> 00:17:47,600
IO and so as we go to memory it's really

00:17:45,650 --> 00:17:50,990
like we're traversing this complicated

00:17:47,600 --> 00:17:53,510
maze which might be a series of

00:17:50,990 --> 00:17:55,970
different buses with exchanges and

00:17:53,510 --> 00:17:59,870
switches and it's really not that simple

00:17:55,970 --> 00:18:02,120
and somewhere along the way one of these

00:17:59,870 --> 00:18:06,890
switches just really likes to reorder

00:18:02,120 --> 00:18:09,440
things and so when I go to memory it

00:18:06,890 --> 00:18:12,710
doesn't taste like memory it tastes like

00:18:09,440 --> 00:18:14,540
DNA okay um

00:18:12,710 --> 00:18:16,520
we've done a lot of work on this

00:18:14,540 --> 00:18:19,730
actually you know all of this talk is

00:18:16,520 --> 00:18:22,460
notionally about Volta the next slide is

00:18:19,730 --> 00:18:25,400
mostly not bubble to the next slide is

00:18:22,460 --> 00:18:30,310
about the work we've done over a decade

00:18:25,400 --> 00:18:33,260
long period of making memory in CUDA C++

00:18:30,310 --> 00:18:37,370
live up to the expectations of C++

00:18:33,260 --> 00:18:39,890
programmers more and more okay so if you

00:18:37,370 --> 00:18:42,920
think of jeep new programming as being I

00:18:39,890 --> 00:18:45,020
use this GPU a lock and then I do GPU

00:18:42,920 --> 00:18:46,030
mem copy and I have to do do you mem

00:18:45,020 --> 00:18:49,030
cutting in

00:18:46,030 --> 00:18:51,940
the G human patent out you're really

00:18:49,030 --> 00:18:55,000
thinking of CUDA one daughter which at

00:18:51,940 --> 00:18:58,150
this point is ten years behind me okay

00:18:55,000 --> 00:19:02,350
um those were the first generations of

00:18:58,150 --> 00:19:04,900
GPUs that that had compute capability I

00:19:02,350 --> 00:19:06,490
said that I work on ATP architectures so

00:19:04,900 --> 00:19:08,530
before that I worked on ranking and

00:19:06,490 --> 00:19:09,610
Curie which did not have any compute K P

00:19:08,530 --> 00:19:15,070
but I think they were graphics only the

00:19:09,610 --> 00:19:16,270
best okay um so so here we are though

00:19:15,070 --> 00:19:19,750
that's that that's that's the first

00:19:16,270 --> 00:19:21,880
generation and you know CUDA made a very

00:19:19,750 --> 00:19:24,100
big splash and it was imitated far and

00:19:21,880 --> 00:19:27,220
wide and so this this model of CUDA

00:19:24,100 --> 00:19:29,010
window you can now find it on a variety

00:19:27,220 --> 00:19:32,340
of platforms we're a variety of vendors

00:19:29,010 --> 00:19:35,830
they're all more or less couples of that

00:19:32,340 --> 00:19:38,530
okay but we didn't stop there in

00:19:35,830 --> 00:19:39,970
capillary max well we created this of

00:19:38,530 --> 00:19:42,730
this different API called Coulomb at

00:19:39,970 --> 00:19:44,710
good amount of manage and khuddam how it

00:19:42,730 --> 00:19:46,450
managed uses this symmetric heap

00:19:44,710 --> 00:19:48,400
approach we're basically allocations

00:19:46,450 --> 00:19:51,370
that you make on the CPU with that

00:19:48,400 --> 00:19:53,020
become mirrored on the GPU when you

00:19:51,370 --> 00:19:55,090
launch some work and they are mirrored

00:19:53,020 --> 00:19:56,980
of the same address so as long as you

00:19:55,090 --> 00:19:58,870
don't have any data races between your

00:19:56,980 --> 00:20:02,710
CPU and GPU threads and so it we

00:19:58,870 --> 00:20:04,900
disallow your own custom synchronization

00:20:02,710 --> 00:20:07,090
between CPU and GPU and you have to use

00:20:04,900 --> 00:20:10,000
our heavyweight synchronization as long

00:20:07,090 --> 00:20:13,930
as you follow our rules then actually

00:20:10,000 --> 00:20:16,360
already at this point memory pretty much

00:20:13,930 --> 00:20:17,920
feels like memory but there is one big

00:20:16,360 --> 00:20:19,720
downside which is that we are making a

00:20:17,920 --> 00:20:21,520
symmetric copy of the heap at every

00:20:19,720 --> 00:20:25,480
invocation from the CPU to the GPU in

00:20:21,520 --> 00:20:27,370
back ok so we didn't stop there in

00:20:25,480 --> 00:20:29,770
Pascal is really when we introduced

00:20:27,370 --> 00:20:33,010
another mode of CUDA mouth managed where

00:20:29,770 --> 00:20:35,590
it can migrate the data on demand it's

00:20:33,010 --> 00:20:41,020
basic no it's notionally similar to a

00:20:35,590 --> 00:20:42,940
copy-on-write scheme ok and we followed

00:20:41,020 --> 00:20:45,310
when we we basically continued that

00:20:42,940 --> 00:20:47,950
we've continued that in Bolton and this

00:20:45,310 --> 00:20:51,580
I cannot wait for this to land there is

00:20:47,950 --> 00:20:53,620
a patch in the queue of the Linux kernel

00:20:51,580 --> 00:20:57,100
maintainer

00:20:53,620 --> 00:20:58,530
that adds effectively that merges this

00:20:57,100 --> 00:21:01,500
into the up

00:20:58,530 --> 00:21:04,860
system as an extra level of paging and

00:21:01,500 --> 00:21:07,920
then when you have that patch in your

00:21:04,860 --> 00:21:10,050
colonel bill basically now everything

00:21:07,920 --> 00:21:11,940
goes away it's just now it's just any

00:21:10,050 --> 00:21:14,640
other actually even memory that's not

00:21:11,940 --> 00:21:16,110
mela could be memory on your stack it

00:21:14,640 --> 00:21:17,670
could be a global variable aesthetic

00:21:16,110 --> 00:21:19,710
those would all be fine

00:21:17,670 --> 00:21:22,020
as basically at that point the memory

00:21:19,710 --> 00:21:23,850
that's backing the whole application not

00:21:22,020 --> 00:21:36,570
only is it payable to disk but it's also

00:21:23,850 --> 00:21:38,370
payable to the device okay yeah yeah the

00:21:36,570 --> 00:21:42,020
question was if there was if there was

00:21:38,370 --> 00:21:46,500
an nmap file could the GPU use it yes

00:21:42,020 --> 00:21:48,360
yeah yeah okay um yeah at that point

00:21:46,500 --> 00:21:50,160
when you have a Pascal and Volta or

00:21:48,360 --> 00:21:52,170
better and you have the end of the next

00:21:50,160 --> 00:21:53,790
hmm patch is either you have a custom

00:21:52,170 --> 00:21:55,620
build or it gets adopted and you have a

00:21:53,790 --> 00:21:58,860
kernel from after that date at that

00:21:55,620 --> 00:22:02,850
point the whole discussion of where the

00:21:58,860 --> 00:22:04,200
memory is it goes one okay um you'll

00:22:02,850 --> 00:22:05,640
notice that my table is weirdly shaped

00:22:04,200 --> 00:22:11,340
and it's because there's another part to

00:22:05,640 --> 00:22:15,320
it okay now um we we are not we're not

00:22:11,340 --> 00:22:18,660
satisfied with this you know this maze

00:22:15,320 --> 00:22:20,100
separating us from the memory so you may

00:22:18,660 --> 00:22:22,530
have heard that we also have this thing

00:22:20,100 --> 00:22:23,910
called envy link and and you think is

00:22:22,530 --> 00:22:27,060
the hardware capability that we've built

00:22:23,910 --> 00:22:30,030
and we've developed it together with IBM

00:22:27,060 --> 00:22:33,690
and IBM has implemented it in power

00:22:30,030 --> 00:22:35,880
eight and power nine and once we have an

00:22:33,690 --> 00:22:38,310
Envy link connection instead of a PCI

00:22:35,880 --> 00:22:40,890
connection suddenly all the memory

00:22:38,310 --> 00:22:44,040
tastes like memory again it does not

00:22:40,890 --> 00:22:47,040
taste like I do okay and so there's this

00:22:44,040 --> 00:22:49,110
other part here which is that if you're

00:22:47,040 --> 00:22:51,600
talking about an arm system in which

00:22:49,110 --> 00:22:55,290
case it would be an Nvidia arm an

00:22:51,600 --> 00:22:57,330
associate or a power system power eight

00:22:55,290 --> 00:23:01,080
or power 9 then there is also this other

00:22:57,330 --> 00:23:03,480
option down here and under this option

00:23:01,080 --> 00:23:09,190
down here there might not even be any

00:23:03,480 --> 00:23:14,950
paging it is the memory is at that point

00:23:09,190 --> 00:23:17,200
fully unified in Harlow okay all right

00:23:14,950 --> 00:23:20,980
um moving on so we talked about

00:23:17,200 --> 00:23:24,250
coherency well coherency is not enough

00:23:20,980 --> 00:23:26,050
you also need consistency and you'll

00:23:24,250 --> 00:23:27,580
recognize effectively this table is just

00:23:26,050 --> 00:23:30,850
a reproduction of your average

00:23:27,580 --> 00:23:32,410
implementation of C++ C seven you know

00:23:30,850 --> 00:23:35,020
there's some some of the combinations of

00:23:32,410 --> 00:23:36,370
loan stores and Atomics or not valid and

00:23:35,020 --> 00:23:37,750
some of the combinations are valid and

00:23:36,370 --> 00:23:41,200
then there's consume which everybody

00:23:37,750 --> 00:23:44,980
decays to acquire until such time as we

00:23:41,200 --> 00:23:47,050
as you want fixed into so there's your

00:23:44,980 --> 00:23:50,260
consistency for CP news so what should

00:23:47,050 --> 00:23:51,370
it be what should it be for GPUs well

00:23:50,260 --> 00:23:54,100
the answer is exactly the same

00:23:51,370 --> 00:23:55,870
absolutely exactly the same and that is

00:23:54,100 --> 00:23:59,800
a new thing in Volta although the

00:23:55,870 --> 00:24:03,490
coherency became a feature on Pascal the

00:23:59,800 --> 00:24:03,970
consistency model appears here with

00:24:03,490 --> 00:24:07,060
Volta

00:24:03,970 --> 00:24:09,010
okay we have the you know unfortunately

00:24:07,060 --> 00:24:11,400
the sign doesn't really do it justice

00:24:09,010 --> 00:24:14,560
it's a completely new memory model it

00:24:11,400 --> 00:24:15,970
you know at 10,000 foot sure resembles

00:24:14,560 --> 00:24:18,430
some other models for example it

00:24:15,970 --> 00:24:20,530
somewhat resembles power reasonable

00:24:18,430 --> 00:24:23,110
other models that have scopes that have

00:24:20,530 --> 00:24:26,440
appeared create news but it is

00:24:23,110 --> 00:24:30,190
fundamentally different from the other

00:24:26,440 --> 00:24:32,940
scope models in that it does not have

00:24:30,190 --> 00:24:36,250
undefined behavior for databases at the

00:24:32,940 --> 00:24:38,950
level of the instructions so it is in

00:24:36,250 --> 00:24:40,510
the vein of CPUs where programs that

00:24:38,950 --> 00:24:42,820
have databases have new C bounded

00:24:40,510 --> 00:24:44,050
behavior but not on the fine and then

00:24:42,820 --> 00:24:45,790
it's at the language level that you have

00:24:44,050 --> 00:24:47,620
on the find behavior I mean C++ it's

00:24:45,790 --> 00:24:48,760
undefined behavior there are races are

00:24:47,620 --> 00:24:52,930
undefined behavior in circles possible

00:24:48,760 --> 00:24:55,060
but um how bad can the machine get once

00:24:52,930 --> 00:24:56,620
you have data races it's still something

00:24:55,060 --> 00:24:59,320
that you want to care about like just

00:24:56,620 --> 00:25:01,330
the fact the fact that your C++ program

00:24:59,320 --> 00:25:04,300
is undefined if you have a data race is

00:25:01,330 --> 00:25:06,790
it's bad okay how much of that can be

00:25:04,300 --> 00:25:10,090
turned into an exploit somewhat depends

00:25:06,790 --> 00:25:14,170
on what the hardware will do and so it's

00:25:10,090 --> 00:25:18,580
very important to us that we adopt sort

00:25:14,170 --> 00:25:20,830
of the best known art for defining a CPU

00:25:18,580 --> 00:25:23,020
know processor memory models which have

00:25:20,830 --> 00:25:24,820
not been done before for a scope model

00:25:23,020 --> 00:25:26,290
there are a few concepts that didn't

00:25:24,820 --> 00:25:28,990
exist in the literature that we have to

00:25:26,290 --> 00:25:30,430
create and and so now I'm very lucky

00:25:28,990 --> 00:25:32,830
because this actually landed on the

00:25:30,430 --> 00:25:36,460
website I think two days ago I can now

00:25:32,830 --> 00:25:40,090
hand you a URL to our formal axiomatic

00:25:36,460 --> 00:25:42,670
memory ball that was in development for

00:25:40,090 --> 00:25:44,500
a very long time and so um that gives me

00:25:42,670 --> 00:25:46,510
an opportunity to tell everybody that oh

00:25:44,500 --> 00:25:47,890
my god I really love the c++ c levin

00:25:46,510 --> 00:25:51,670
memory well alright it's really

00:25:47,890 --> 00:25:52,960
fantastic um whereas earlier I enlisted

00:25:51,670 --> 00:25:55,750
some things that people might think

00:25:52,960 --> 00:25:57,880
helped the portability of C++ but don't

00:25:55,750 --> 00:25:59,980
really here's an example of a thing you

00:25:57,880 --> 00:26:04,410
might not think very much but really

00:25:59,980 --> 00:26:09,180
really helps the portability of C++ um

00:26:04,410 --> 00:26:14,290
you all know that volatile is a bad word

00:26:09,180 --> 00:26:16,900
but it is although it's bad for software

00:26:14,290 --> 00:26:19,810
developers to use volatile and is

00:26:16,900 --> 00:26:22,570
particularly oppressive for hardware

00:26:19,810 --> 00:26:24,640
architects when you use volatile because

00:26:22,570 --> 00:26:26,890
a program that has volatile in it does

00:26:24,640 --> 00:26:28,900
not actually have well-defined semantics

00:26:26,890 --> 00:26:30,820
it only has the semantics that the

00:26:28,900 --> 00:26:34,000
Machine you're running on gave it to

00:26:30,820 --> 00:26:36,640
David right so if most people do their

00:26:34,000 --> 00:26:38,620
development on x86 then it means that

00:26:36,640 --> 00:26:40,600
for most people the semantics of

00:26:38,620 --> 00:26:44,830
volatile is whatever Exodus X did with

00:26:40,600 --> 00:26:48,400
that and that would force us that would

00:26:44,830 --> 00:26:50,950
act and this is true story um we would

00:26:48,400 --> 00:26:52,150
end up in conversations endlessly where

00:26:50,950 --> 00:26:54,670
people are like what like cities did

00:26:52,150 --> 00:26:59,310
this you should do this and we're like

00:26:54,670 --> 00:27:01,450
no we can't like we don't have the the

00:26:59,310 --> 00:27:04,510
privileged access to the walled garden

00:27:01,450 --> 00:27:07,000
that owns the memory so it's not even an

00:27:04,510 --> 00:27:10,270
option it's not even an option for me to

00:27:07,000 --> 00:27:13,120
really go and match it what the C++ 11

00:27:10,270 --> 00:27:15,550
memory model did is it really clarified

00:27:13,120 --> 00:27:18,520
the semantics not only did it clarified

00:27:15,550 --> 00:27:21,610
the semantics it clarified them in a way

00:27:18,520 --> 00:27:25,120
that made it completely implementable to

00:27:21,610 --> 00:27:28,360
me so um so that was wonderful that was

00:27:25,120 --> 00:27:30,910
that that was very wonderful yeah and as

00:27:28,360 --> 00:27:32,850
it happens I joined the committee right

00:27:30,910 --> 00:27:35,399
after that and

00:27:32,850 --> 00:27:39,480
and then and then at that point I

00:27:35,399 --> 00:27:41,070
thought who we dodged a bullet there we

00:27:39,480 --> 00:27:43,110
were we were not really paying attention

00:27:41,070 --> 00:27:45,360
but um you dodged a bullet

00:27:43,110 --> 00:27:48,990
alright I have two open issues I'm not

00:27:45,360 --> 00:27:50,460
watching time too closely okay thank you

00:27:48,990 --> 00:27:53,580
okay

00:27:50,460 --> 00:27:55,919
um there are open issues remaining for

00:27:53,580 --> 00:27:57,870
simplices manner coherency and

00:27:55,919 --> 00:28:01,110
consistency unfortunately it was not all

00:27:57,870 --> 00:28:05,000
of it there is more so now I'd like you

00:28:01,110 --> 00:28:09,659
to consider the issue of footprint or

00:28:05,000 --> 00:28:12,870
working set or could be a synonym for so

00:28:09,659 --> 00:28:15,389
here I have some example numbers for

00:28:12,870 --> 00:28:17,250
that CPU box and these these different

00:28:15,389 --> 00:28:22,019
boxes and then volts box alright so I

00:28:17,250 --> 00:28:23,399
may have a CPU with 16 threads and 128

00:28:22,019 --> 00:28:24,629
gigs of ram that is a perfectly

00:28:23,399 --> 00:28:32,879
reasonable system for somebody to

00:28:24,629 --> 00:28:35,549
assemble and I have in my PC a Volta

00:28:32,879 --> 00:28:37,799
with a hundred and sixty-three thousand

00:28:35,549 --> 00:28:42,149
eight hundred and forty threads and it

00:28:37,799 --> 00:28:47,299
has 16 gigs of ram it let's take a

00:28:42,149 --> 00:28:56,039
second here and look at these ratios um

00:28:47,299 --> 00:28:59,309
these ratios are not very similar one to

00:28:56,039 --> 00:29:01,830
eight gig and one 200k these are not

00:28:59,309 --> 00:29:03,090
very similar so if you'll recall a few

00:29:01,830 --> 00:29:05,100
minutes ago we were looking at the

00:29:03,090 --> 00:29:07,879
transistors and I said you might imagine

00:29:05,100 --> 00:29:10,200
that with such a drastic difference in

00:29:07,879 --> 00:29:12,679
the amount of transistors you can have

00:29:10,200 --> 00:29:16,500
it might change some design decisions I

00:29:12,679 --> 00:29:18,799
wouldn't bring up to you that this would

00:29:16,500 --> 00:29:23,360
probably change your design decisions

00:29:18,799 --> 00:29:27,330
for libraries and even language design

00:29:23,360 --> 00:29:29,220
okay so here's a concern that are

00:29:27,330 --> 00:29:31,649
concerned that I have and this is really

00:29:29,220 --> 00:29:33,570
an evolution of c++ thing it's the thing

00:29:31,649 --> 00:29:35,909
as you wanted to talked about I'm gonna

00:29:33,570 --> 00:29:38,129
give you a number for a paper you can go

00:29:35,909 --> 00:29:40,980
read it and we'll find about it I would

00:29:38,129 --> 00:29:44,940
like that paper to get a future revision

00:29:40,980 --> 00:29:49,620
for instance okay shall we spend all of

00:29:44,940 --> 00:29:52,950
that on thread-local like that is a

00:29:49,620 --> 00:29:55,230
really significant risk people might use

00:29:52,950 --> 00:29:58,200
thread on their more local from C++ 11

00:29:55,230 --> 00:30:00,960
with relative abandon when it's one to

00:29:58,200 --> 00:30:04,169
eight gigabytes but when it's one to a

00:30:00,960 --> 00:30:08,220
hundred kilobytes that's just not going

00:30:04,169 --> 00:30:11,100
to work very well okay I would surmise

00:30:08,220 --> 00:30:14,909
that whoever came up with TLS didn't

00:30:11,100 --> 00:30:17,760
really think about systems with 163,000

00:30:14,909 --> 00:30:23,399
threads it probably does not occur to

00:30:17,760 --> 00:30:26,039
them okay so we have a problem we have

00:30:23,399 --> 00:30:30,779
some things gotta get we need we need

00:30:26,039 --> 00:30:33,330
some way to clarify that you know

00:30:30,779 --> 00:30:35,070
thread-local may sometimes have a

00:30:33,330 --> 00:30:37,919
different meaning or something I'm not

00:30:35,070 --> 00:30:40,620
sure that's wide open field we can write

00:30:37,919 --> 00:30:42,990
something you okay there's another issue

00:30:40,620 --> 00:30:45,149
a hundred and sixty three thousand eight

00:30:42,990 --> 00:30:50,250
hundred and forty stacks is a lot of

00:30:45,149 --> 00:30:52,679
stacks in that hundred kilobytes some of

00:30:50,250 --> 00:30:55,309
it goes to your step hopefully not all

00:30:52,679 --> 00:30:55,309
of it okay

00:30:57,980 --> 00:31:04,289
concretely in 2017 add in video right

00:31:01,889 --> 00:31:05,190
now we are forced to choose between

00:31:04,289 --> 00:31:07,110
having

00:31:05,190 --> 00:31:11,190
stack accesses that have high

00:31:07,110 --> 00:31:15,210
performance or complying with note 11 in

00:31:11,190 --> 00:31:16,200
clause 4.7 paragraph 1 what does that

00:31:15,210 --> 00:31:19,799
note say

00:31:16,200 --> 00:31:21,690
it says I'm paraphrasing that automatics

00:31:19,799 --> 00:31:23,129
are private to a thread however you can

00:31:21,690 --> 00:31:24,869
take their address and once you take

00:31:23,129 --> 00:31:27,000
their address you can share them with

00:31:24,869 --> 00:31:29,340
another threat and then other threads

00:31:27,000 --> 00:31:31,549
can access this object oh and be aware

00:31:29,340 --> 00:31:34,619
of their is it okay

00:31:31,549 --> 00:31:37,830
as it happens or current implementation

00:31:34,619 --> 00:31:40,769
is exceedingly efficient but it's not

00:31:37,830 --> 00:31:42,389
allow addressing of this memory by any

00:31:40,769 --> 00:31:46,710
thread other than the thread that owns

00:31:42,389 --> 00:31:48,269
it and I'm not sure how to reconcile

00:31:46,710 --> 00:31:50,340
these things and it's not because I

00:31:48,269 --> 00:31:53,539
haven't called on okay

00:31:50,340 --> 00:31:57,750
so memory progress report um

00:31:53,539 --> 00:32:00,840
coherency we're pretty much set from

00:31:57,750 --> 00:32:04,200
Pascal to volta consistency we're pretty

00:32:00,840 --> 00:32:05,639
much set on volta at the moment it's set

00:32:04,200 --> 00:32:06,929
in the assembly level I don't mean the

00:32:05,639 --> 00:32:08,519
private assembly that you don't have I

00:32:06,929 --> 00:32:11,850
mean the public assembly that we do have

00:32:08,519 --> 00:32:13,799
and you can use inline assembly in in

00:32:11,850 --> 00:32:15,510
critical spot so you can have access to

00:32:13,799 --> 00:32:17,010
all the primitives that you need to get

00:32:15,510 --> 00:32:20,190
the same semantics that are compatible

00:32:17,010 --> 00:32:23,519
with sequel cause when atomic

00:32:20,190 --> 00:32:26,940
T appears in CUDA C++ there's currently

00:32:23,519 --> 00:32:28,399
TDD but some people have particular

00:32:26,940 --> 00:32:32,460
implementations

00:32:28,399 --> 00:32:34,169
okay um that's on our side that's on the

00:32:32,460 --> 00:32:36,119
GPU side alright I really I don't know

00:32:34,169 --> 00:32:39,119
what to do about footprint working set

00:32:36,119 --> 00:32:42,299
this is this is effectively a problem

00:32:39,119 --> 00:32:44,220
that's just endemic to the scale and and

00:32:42,299 --> 00:32:45,990
my hunch is we're gonna need to have a

00:32:44,220 --> 00:32:46,309
way if we're simple supposed to speak to

00:32:45,990 --> 00:32:50,190
that

00:32:46,309 --> 00:32:52,169
okay then C++ this position there are

00:32:50,190 --> 00:32:54,059
some known bugs with the C++ 11 memory

00:32:52,169 --> 00:32:58,139
model nobody panic that's fine we're

00:32:54,059 --> 00:33:01,649
taking care of it you can read p06 a a

00:32:58,139 --> 00:33:04,019
fun adventure some of the bugs there

00:33:01,649 --> 00:33:06,510
were code discovered by Princeton and

00:33:04,019 --> 00:33:10,019
NVIDIA and a group in Germany at the Max

00:33:06,510 --> 00:33:11,369
Planck it's funny because they were all

00:33:10,019 --> 00:33:13,529
discovered at right about the same time

00:33:11,369 --> 00:33:14,909
and then and then everybody was like hi

00:33:13,529 --> 00:33:16,230
yeah I haven't think to I think to

00:33:14,909 --> 00:33:18,510
tell you you'll never guess what it is

00:33:16,230 --> 00:33:19,740
we're like wait the proof of conformance

00:33:18,510 --> 00:33:26,820
if this processor is invalid

00:33:19,740 --> 00:33:28,409
well yeah okay um so so there are

00:33:26,820 --> 00:33:30,570
recommended fixes we had a vote in

00:33:28,409 --> 00:33:32,220
Toronto to move ahead with the fixes so

00:33:30,570 --> 00:33:35,340
we just read like we need to revise the

00:33:32,220 --> 00:33:36,990
paper and and put it in and then we'll

00:33:35,340 --> 00:33:39,149
be will have a relatively short

00:33:36,990 --> 00:33:42,720
defect list on the memory model after

00:33:39,149 --> 00:33:43,919
that okay so that's the sequel supposed

00:33:42,720 --> 00:33:46,260
to this position for coherency and

00:33:43,919 --> 00:33:49,529
consistency I would invite people to

00:33:46,260 --> 00:33:51,840
look at p00 7 to half of which was

00:33:49,529 --> 00:33:54,119
adopted into C++ 17 and the other half

00:33:51,840 --> 00:33:55,919
was left on the shelf I'm basically

00:33:54,119 --> 00:33:58,080
saying the half that was left on the

00:33:55,919 --> 00:34:01,369
shelf we should think about it again and

00:33:58,080 --> 00:34:06,149
it is because it helps with the foot

00:34:01,369 --> 00:34:08,369
okay alright let's talk about designing

00:34:06,149 --> 00:34:10,859
voltar for C++ execution and this this

00:34:08,369 --> 00:34:13,529
year this year is we did something

00:34:10,859 --> 00:34:18,149
really huge really huge here

00:34:13,529 --> 00:34:22,190
okay so huge yeah um so I talked about

00:34:18,149 --> 00:34:24,599
these abstraction layers earlier C++ has

00:34:22,190 --> 00:34:26,760
the concept of a threat of execution

00:34:24,599 --> 00:34:28,619
which you have to use all three words

00:34:26,760 --> 00:34:30,480
probably - eight them because it's a

00:34:28,619 --> 00:34:32,639
term of art has a right of execution

00:34:30,480 --> 00:34:35,309
which is a chain of evaluations in your

00:34:32,639 --> 00:34:39,299
code it's sort of your code

00:34:35,309 --> 00:34:40,559
representation of it that runs on an

00:34:39,299 --> 00:34:42,869
execution agent which is just an

00:34:40,559 --> 00:34:48,990
abstract notion for that thing which

00:34:42,869 --> 00:34:51,000
runs your code and prior to C++ 17

00:34:48,990 --> 00:34:54,409
there was only one instantiation of this

00:34:51,000 --> 00:34:57,900
concept it was a concept with only one

00:34:54,409 --> 00:34:59,880
concretization thread and that's not

00:34:57,900 --> 00:35:02,910
just a particularly ownerís version of

00:34:59,880 --> 00:35:07,260
that really I mean the thread is is in

00:35:02,910 --> 00:35:09,510
C++ 11 fairly fast it has it has all

00:35:07,260 --> 00:35:11,430
these you know the thread local and it's

00:35:09,510 --> 00:35:12,960
behavior on the automatics and the it

00:35:11,430 --> 00:35:15,740
has it has a guarantee of forward

00:35:12,960 --> 00:35:18,119
progress and strong and all that okay so

00:35:15,740 --> 00:35:20,369
then what's lower down is see if you

00:35:18,119 --> 00:35:22,500
runs anything that's for sure and then

00:35:20,369 --> 00:35:27,359
the GPU is it's a thing that this is run

00:35:22,500 --> 00:35:31,109
ownerís things okay all right so in C++

00:35:27,359 --> 00:35:34,559
seventeen by adopting effectively that

00:35:31,109 --> 00:35:36,779
half of zero zero seven - we've done a

00:35:34,559 --> 00:35:39,539
lot of clarification we've clarified

00:35:36,779 --> 00:35:42,029
that actually execution agents might

00:35:39,539 --> 00:35:44,220
have more than one kind of instantiation

00:35:42,029 --> 00:35:46,020
and they're writing more than one kind

00:35:44,220 --> 00:35:48,480
of execution agent so we've created the

00:35:46,020 --> 00:35:50,670
concept of concurrent execution agents

00:35:48,480 --> 00:35:52,619
parallel execution agents and weakly

00:35:50,670 --> 00:35:54,930
parallel execution agents which is

00:35:52,619 --> 00:35:56,760
basically differ among each other by the

00:35:54,930 --> 00:35:59,220
amount of forward progress guarantee

00:35:56,760 --> 00:36:02,640
that they have a concurrent execution

00:35:59,220 --> 00:36:04,230
agent has all of the capabilities and

00:36:02,640 --> 00:36:07,109
all of the forward progress that has

00:36:04,230 --> 00:36:09,539
stood thread pass and also this special

00:36:07,109 --> 00:36:09,780
main thread that we can't tell you what

00:36:09,539 --> 00:36:13,980
its

00:36:09,780 --> 00:36:16,440
type is but it's a main threat um and so

00:36:13,980 --> 00:36:20,010
that sets up sort of the old C++ in

00:36:16,440 --> 00:36:23,250
eleven vertical is right now with the

00:36:20,010 --> 00:36:25,410
new types of execution agents parallel

00:36:23,250 --> 00:36:27,570
execution agents do not have a guarantee

00:36:25,410 --> 00:36:29,520
for when they will start until you block

00:36:27,570 --> 00:36:31,410
on their completion but once they

00:36:29,520 --> 00:36:32,730
started running they have a guarantee of

00:36:31,410 --> 00:36:34,980
forward progress you can execute

00:36:32,730 --> 00:36:37,530
concurrently returns that are a

00:36:34,980 --> 00:36:40,290
starvation Creek not just walk free and

00:36:37,530 --> 00:36:41,370
that is the most important distinction

00:36:40,290 --> 00:36:43,470
with the next class

00:36:41,370 --> 00:36:45,300
weakly parallel execution agents can

00:36:43,470 --> 00:36:47,550
only execute all rhythms that are locked

00:36:45,300 --> 00:36:50,970
free that is algorithms concurrent

00:36:47,550 --> 00:36:53,190
algorithms that are correct

00:36:50,970 --> 00:36:58,380
even if threads are indefinitely

00:36:53,190 --> 00:36:59,400
victimized by the scheduler okay now one

00:36:58,380 --> 00:37:00,870
of the things that you may already have

00:36:59,400 --> 00:37:03,480
noticed here is I've staked out this

00:37:00,870 --> 00:37:05,850
position that volta is pretty unique

00:37:03,480 --> 00:37:12,930
because volt is here and then all other

00:37:05,850 --> 00:37:15,950
GPUs are here okay this this was not

00:37:12,930 --> 00:37:19,620
business as usual at all this was a

00:37:15,950 --> 00:37:22,320
concerted effort by engineers who wanted

00:37:19,620 --> 00:37:23,250
to move the state of the art for vector

00:37:22,320 --> 00:37:26,670
command execution

00:37:23,250 --> 00:37:30,900
we literally throughout our entire

00:37:26,670 --> 00:37:33,150
implementation of GP threads took all

00:37:30,900 --> 00:37:36,390
that all that RTL code on that hardware

00:37:33,150 --> 00:37:38,640
code that unit tossed it out and did

00:37:36,390 --> 00:37:40,980
something completely new and when we

00:37:38,640 --> 00:37:42,750
designed it we were we were basically

00:37:40,980 --> 00:37:45,870
making sure every step along the way

00:37:42,750 --> 00:37:48,210
that we were conforming with the C++

00:37:45,870 --> 00:37:50,370
standard we were we were quite literally

00:37:48,210 --> 00:37:56,100
quoting the C++ standard at each other

00:37:50,370 --> 00:37:57,720
while designing Hardware okay okay and

00:37:56,100 --> 00:38:00,200
and that actually makes that actually

00:37:57,720 --> 00:38:03,060
makes Volta very much alone of its kind

00:38:00,200 --> 00:38:05,910
you know that the tradition of making

00:38:03,060 --> 00:38:12,720
GPU like threads is not all that new

00:38:05,910 --> 00:38:17,820
it's almost as old as I am by across

00:38:12,720 --> 00:38:21,720
that vast span of time across that span

00:38:17,820 --> 00:38:24,030
of time really nobody else

00:38:21,720 --> 00:38:26,700
managed to write to make a system that

00:38:24,030 --> 00:38:30,150
can execute starvation free concurrent

00:38:26,700 --> 00:38:33,660
algorithms at vector efficiency so that

00:38:30,150 --> 00:38:37,109
is completely new okay and that allows

00:38:33,660 --> 00:38:38,760
us to clarify 17 C++ cynthy is single

00:38:37,109 --> 00:38:40,320
instruction multiple thread that's the

00:38:38,760 --> 00:38:42,690
name that NVIDIA has given to this type

00:38:40,320 --> 00:38:47,340
of architecture and it's in comparison

00:38:42,690 --> 00:38:48,990
with Cindy alright alright now I'm gonna

00:38:47,340 --> 00:38:50,810
show me that bad code we're there we're

00:38:48,990 --> 00:38:53,099
at that place where I have the bad code

00:38:50,810 --> 00:38:54,770
so I'm going to show you some bad code

00:38:53,099 --> 00:38:59,190
that doesn't really have atomic T in it

00:38:54,770 --> 00:39:02,730
so now we can clarify this here I have a

00:38:59,190 --> 00:39:04,410
piece of code which is largely you know

00:39:02,730 --> 00:39:06,690
short of an annotation or two it's

00:39:04,410 --> 00:39:09,690
basically CUDA 9 it's physically valid

00:39:06,690 --> 00:39:12,090
code in I actually as written here it's

00:39:09,690 --> 00:39:18,200
valid CUDA for any revision of CUDA

00:39:12,090 --> 00:39:22,560
above 1.1 syntactically but semantically

00:39:18,200 --> 00:39:24,780
only 9.0 and only on volta and the

00:39:22,560 --> 00:39:27,260
reason for this is when you're running

00:39:24,780 --> 00:39:29,730
on a vectorized processor whether it's

00:39:27,260 --> 00:39:34,020
cindy with a compiler that inserted

00:39:29,730 --> 00:39:35,940
predication or whether it's simply with

00:39:34,020 --> 00:39:37,830
either a previous implementation into

00:39:35,940 --> 00:39:42,210
step based or this new implementation

00:39:37,830 --> 00:39:46,560
which is I'm not telling you um the

00:39:42,210 --> 00:39:49,710
problem is that the moral scheduler that

00:39:46,560 --> 00:39:52,050
runs these threads makes a choice after

00:39:49,710 --> 00:39:54,030
one of the threads exits the loop ok so

00:39:52,050 --> 00:39:56,099
this is just a spin lock I'm spinning on

00:39:54,030 --> 00:39:58,109
cast someone's going to succeed in

00:39:56,099 --> 00:40:01,040
switching the lock from unlock kalach

00:39:58,109 --> 00:40:03,750
and then they're gonna exit this loop

00:40:01,040 --> 00:40:04,470
when they exit this loop the machine has

00:40:03,750 --> 00:40:07,650
two paths

00:40:04,470 --> 00:40:09,869
I can either execute the other threads

00:40:07,650 --> 00:40:14,250
in the loop or I can execute the

00:40:09,869 --> 00:40:18,780
critical section the problem is any

00:40:14,250 --> 00:40:21,030
static compilation to predication or our

00:40:18,780 --> 00:40:22,560
previous architecture would commit to

00:40:21,030 --> 00:40:23,820
one of the two choices and then you

00:40:22,560 --> 00:40:26,310
would have to run that path to

00:40:23,820 --> 00:40:27,839
completion which in this case here means

00:40:26,310 --> 00:40:30,390
you potentially could choose a while

00:40:27,839 --> 00:40:32,580
loop the while loop is never going to

00:40:30,390 --> 00:40:34,060
succeed again because the thread that

00:40:32,580 --> 00:40:35,530
owns the lock is a ten

00:40:34,060 --> 00:40:37,750
to execute the critical section and it's

00:40:35,530 --> 00:40:40,600
being victimized indefinitely so we

00:40:37,750 --> 00:40:43,000
never make it to the unlock okay if you

00:40:40,600 --> 00:40:46,420
wrote this code in any revision of CUDA

00:40:43,000 --> 00:40:49,570
from 1.1 to even including 9.0 as long

00:40:46,420 --> 00:40:51,670
as you don't have the volta then then

00:40:49,570 --> 00:40:53,050
you could lock your GPU with this and

00:40:51,670 --> 00:40:56,320
you could I mean you could lock

00:40:53,050 --> 00:40:58,660
anybody's duty with this you could lock

00:40:56,320 --> 00:41:03,070
and you could lock compiler to

00:40:58,660 --> 00:41:07,150
predicated Sydney with this okay all

00:41:03,070 --> 00:41:09,040
right so what voltages did is is pretty

00:41:07,150 --> 00:41:12,130
huge and although I can't share the

00:41:09,040 --> 00:41:14,200
details of how we did it it's actually

00:41:12,130 --> 00:41:16,390
pretty wonderful in use you write this

00:41:14,200 --> 00:41:19,330
code it's just things you expect work in

00:41:16,390 --> 00:41:21,700
C++ and they just do and without fanfare

00:41:19,330 --> 00:41:24,430
it's like you wrote it and compile it

00:41:21,700 --> 00:41:25,060
ran really well a big deal this never

00:41:24,430 --> 00:41:26,830
works

00:41:25,060 --> 00:41:29,800
on a processor with this kind of

00:41:26,830 --> 00:41:31,930
efficiency before okay alright there's

00:41:29,800 --> 00:41:35,140
still some open issues with execution as

00:41:31,930 --> 00:41:37,090
well I talked about it I had some open

00:41:35,140 --> 00:41:38,860
issues with memory we have some open

00:41:37,090 --> 00:41:40,420
issues with execution and this is where

00:41:38,860 --> 00:41:44,940
we're going to talk about the degree of

00:41:40,420 --> 00:41:47,230
conformance or not that we have okay um

00:41:44,940 --> 00:41:49,510
so basically here's the short version of

00:41:47,230 --> 00:41:51,310
it I don't want to give an extra 30

00:41:49,510 --> 00:41:54,100
minute talk about all of the things that

00:41:51,310 --> 00:41:55,660
don't work but um but basically the

00:41:54,100 --> 00:41:57,340
state of heterogeneity hasn't really

00:41:55,660 --> 00:41:59,140
changed in quite a long time and I'm not

00:41:57,340 --> 00:42:00,430
here to announce anything you with

00:41:59,140 --> 00:42:03,250
respect to that that is basically

00:42:00,430 --> 00:42:04,840
there's annotation in CUDA 9 it takes

00:42:03,250 --> 00:42:06,580
the form of the on the born of our host

00:42:04,840 --> 00:42:08,920
or on the board of our device annotation

00:42:06,580 --> 00:42:13,990
where you say in which device to compile

00:42:08,920 --> 00:42:15,550
function 2 or both and this in

00:42:13,990 --> 00:42:17,170
combination with the lack of integration

00:42:15,550 --> 00:42:19,480
between the compilers you know the

00:42:17,170 --> 00:42:21,190
Nvidia compiler and GCC for ticular you

00:42:19,480 --> 00:42:23,530
know essentially do their jobs

00:42:21,190 --> 00:42:26,800
separately and they're not very deeply

00:42:23,530 --> 00:42:28,390
integrated um this is basically what's

00:42:26,800 --> 00:42:31,120
behind what's remaining in performance

00:42:28,390 --> 00:42:32,800
issues so you know our TTI this is

00:42:31,120 --> 00:42:34,390
because we don't know the internal

00:42:32,800 --> 00:42:36,250
implementation verticity I of every

00:42:34,390 --> 00:42:39,070
other compiler that we work with so

00:42:36,250 --> 00:42:41,080
that's just an example but more

00:42:39,070 --> 00:42:42,460
annoyingly for me and probably for many

00:42:41,080 --> 00:42:43,960
of you in the room is you know very

00:42:42,460 --> 00:42:45,230
simple things like stood the vector

00:42:43,960 --> 00:42:47,359
operator square bracket

00:42:45,230 --> 00:42:50,359
doesn't work and it's because it's just

00:42:47,359 --> 00:42:52,280
not annotated with device if it were it

00:42:50,359 --> 00:42:54,830
would work because there's nothing

00:42:52,280 --> 00:42:56,030
complicated in here but it's not so

00:42:54,830 --> 00:43:00,050
therefore it doesn't work and it's

00:42:56,030 --> 00:43:02,660
rejected but my personal experience

00:43:00,050 --> 00:43:05,359
writing CUDA all summer as someone who

00:43:02,660 --> 00:43:08,000
basically wrote essentially no CUDA

00:43:05,359 --> 00:43:13,820
before the summer I pretty much waiting

00:43:08,000 --> 00:43:15,830
for Voltas write CUDA um hey know all I

00:43:13,820 --> 00:43:17,359
all I need to know is that I need to

00:43:15,830 --> 00:43:19,910
stick device on functions and then

00:43:17,359 --> 00:43:21,740
everything will work and and and

00:43:19,910 --> 00:43:23,660
practically everything else does work

00:43:21,740 --> 00:43:27,050
you know most language features you can

00:43:23,660 --> 00:43:30,619
think of just work okay so here's the

00:43:27,050 --> 00:43:33,650
execution progress report um on the

00:43:30,619 --> 00:43:36,470
issue of forward progress we have

00:43:33,650 --> 00:43:40,820
greatly clarified cynthy progress with

00:43:36,470 --> 00:43:43,210
Volta and we simultaneously greatly

00:43:40,820 --> 00:43:46,359
clarified progress in C++ seventeen

00:43:43,210 --> 00:43:48,680
that's not an accident

00:43:46,359 --> 00:43:50,720
so that allowed us to clarify the

00:43:48,680 --> 00:43:53,590
meaning of simply simple spots that's a

00:43:50,720 --> 00:43:59,000
major progress and then on heterogeneity

00:43:53,590 --> 00:44:02,000
stay tuned alright so let me wrap up let

00:43:59,000 --> 00:44:07,369
me leave you to wrap up on the Volta C++

00:44:02,000 --> 00:44:09,140
architecture this is really the model

00:44:07,369 --> 00:44:12,859
that I want you guys to have in mind you

00:44:09,140 --> 00:44:15,290
know we we are sometimes you know just

00:44:12,859 --> 00:44:17,390
you're unjust me um people are sometimes

00:44:15,290 --> 00:44:19,550
upset that we don't give full specs for

00:44:17,390 --> 00:44:21,800
our systems other companies give full

00:44:19,550 --> 00:44:23,480
specs for their systems we reserve the

00:44:21,800 --> 00:44:25,520
right to be innovative in certain ways

00:44:23,480 --> 00:44:28,580
that are just not compatible with giving

00:44:25,520 --> 00:44:30,530
everybody our specs but I want everybody

00:44:28,580 --> 00:44:34,250
to know this that our platform is really

00:44:30,530 --> 00:44:36,550
a scalar platform like this earlier I

00:44:34,250 --> 00:44:40,790
said confining down to the metal meaning

00:44:36,550 --> 00:44:44,180
element metal for this slide I'm gonna

00:44:40,790 --> 00:44:46,930
say if you're compiling C++ to metal

00:44:44,180 --> 00:44:51,590
meaning instruction set above this line

00:44:46,930 --> 00:44:56,390
NVIDIA GPUs are scalar threaded machines

00:44:51,590 --> 00:44:58,260
they are not sending genes we do not

00:44:56,390 --> 00:45:00,630
present this in the abstraction at all

00:44:58,260 --> 00:45:02,820
in fact our instruction set both the

00:45:00,630 --> 00:45:05,720
abstract instruction set that you have

00:45:02,820 --> 00:45:08,520
access to called Pippa pts and the

00:45:05,720 --> 00:45:10,950
concrete instruction set which I work

00:45:08,520 --> 00:45:14,570
with on a daily basis both of them are

00:45:10,950 --> 00:45:19,130
inherently scalar threads we don't have

00:45:14,570 --> 00:45:22,500
FM a times 32 with a predicate we have

00:45:19,130 --> 00:45:25,470
FM a of a scalar and each thread

00:45:22,500 --> 00:45:28,350
executes its own copy of that FM a of a

00:45:25,470 --> 00:45:29,970
scalar and it's the job of this layer

00:45:28,350 --> 00:45:32,100
that we have in the silicon that I'm

00:45:29,970 --> 00:45:35,640
labeling thread virtualization it's the

00:45:32,100 --> 00:45:38,400
job of this layer to figure out how

00:45:35,640 --> 00:45:41,280
these operations can be gang so that the

00:45:38,400 --> 00:45:43,950
power saving is realized without having

00:45:41,280 --> 00:45:47,130
any undue consequence on how you would

00:45:43,950 --> 00:45:48,780
build a software stack above and I'm

00:45:47,130 --> 00:45:52,350
actually incredibly proud of this

00:45:48,780 --> 00:45:56,280
approach I think that it's supports

00:45:52,350 --> 00:45:59,130
really clean software stacks to map to

00:45:56,280 --> 00:46:02,910
our machine you know scalar compiler

00:45:59,130 --> 00:46:07,530
optimizations are you know old and well

00:46:02,910 --> 00:46:10,800
trusted and well verified and deliver

00:46:07,530 --> 00:46:13,109
the highest performance really we would

00:46:10,800 --> 00:46:15,180
not want to walk away from scaler

00:46:13,109 --> 00:46:20,540
optimizations because we prematurely

00:46:15,180 --> 00:46:24,619
committed to a vector execution okay so

00:46:20,540 --> 00:46:27,180
do these layers of abstraction today

00:46:24,619 --> 00:46:31,050
most of Cephas was 14 is supported by

00:46:27,180 --> 00:46:33,090
CUDA 9 there is just like oh the GCC and

00:46:31,050 --> 00:46:35,340
clang web pages there's a table of

00:46:33,090 --> 00:46:37,350
features and which papers they they took

00:46:35,340 --> 00:46:40,200
the future prime we have the exact same

00:46:37,350 --> 00:46:42,270
thing so you can come on the Nvidia

00:46:40,200 --> 00:46:44,369
developer website and see what the

00:46:42,270 --> 00:46:50,580
future support matrix is or a compiler

00:46:44,369 --> 00:46:57,119
and it is both the worst of any major

00:46:50,580 --> 00:47:00,240
compiler and by far the best of any GPU

00:46:57,119 --> 00:47:02,430
or throughput processor architecture so

00:47:00,240 --> 00:47:04,660
it's somewhere in this in this hectic

00:47:02,430 --> 00:47:07,480
log in this middle

00:47:04,660 --> 00:47:09,160
okay all right so we have that and that

00:47:07,480 --> 00:47:11,140
sits on the top of an array of scalar

00:47:09,160 --> 00:47:14,970
threads and the GPU is a scalar

00:47:11,140 --> 00:47:17,440
architecture okay at this point I

00:47:14,970 --> 00:47:19,780
actually have a really large number of

00:47:17,440 --> 00:47:21,720
backup slides that that dive into

00:47:19,780 --> 00:47:24,970
various kinds of details about the volta

00:47:21,720 --> 00:47:27,970
specifically but I'd like you to drive

00:47:24,970 --> 00:47:36,700
am I going there with your questions yes

00:47:27,970 --> 00:47:38,260
sir so what do I mean when I say that

00:47:36,700 --> 00:47:39,880
vector bracket bracket doesn't work um

00:47:38,260 --> 00:47:41,860
what you'll get is you'll get a compiler

00:47:39,880 --> 00:47:43,630
error it'll say when you're when you're

00:47:41,860 --> 00:47:44,950
writing so when you write your code

00:47:43,630 --> 00:47:46,510
you're gonna ruin some functions you're

00:47:44,950 --> 00:47:49,240
gonna designate some of them as being

00:47:46,510 --> 00:47:51,550
GPU functions when you write code inside

00:47:49,240 --> 00:47:53,860
of that function you could you can you

00:47:51,550 --> 00:47:55,450
can legitimately pass in a reference to

00:47:53,860 --> 00:47:57,850
a stood vector or a pointer to a stood

00:47:55,450 --> 00:47:59,890
vector that's fine but then when you

00:47:57,850 --> 00:48:02,140
attempt to invoke the square bracket

00:47:59,890 --> 00:48:04,600
operator on this vector you're gonna use

00:48:02,140 --> 00:48:06,550
a compile time error that says operator

00:48:04,600 --> 00:48:08,620
Cole Cole you know still vector call

00:48:06,550 --> 00:48:11,980
colon operator bracket bracket not the

00:48:08,620 --> 00:48:13,930
vise qualified and that effectively

00:48:11,980 --> 00:48:15,760
means that the compiler is not going to

00:48:13,930 --> 00:48:17,770
generate a device version of that a

00:48:15,760 --> 00:48:19,900
device version of the byte code the

00:48:17,770 --> 00:48:21,490
final executable bytecode for this

00:48:19,900 --> 00:48:23,770
function for the device and therefore

00:48:21,490 --> 00:48:25,930
you can't invoke it now there's this

00:48:23,770 --> 00:48:28,720
this is an experiment that I did years

00:48:25,930 --> 00:48:30,280
ago I opened up I have no idea if that

00:48:28,720 --> 00:48:33,220
violates Terms of Service but anyway huh

00:48:30,280 --> 00:48:35,290
I opened up my visual studio headers and

00:48:33,220 --> 00:48:39,930
I just started typing down the vise-like

00:48:35,290 --> 00:48:43,930
over the place hey worked just fine

00:48:39,930 --> 00:48:45,940
right now now here there be dragons even

00:48:43,930 --> 00:48:47,740
if you were to do that the standard

00:48:45,940 --> 00:48:49,240
library functions have no limits unlike

00:48:47,740 --> 00:48:51,160
how much special stuff you can use

00:48:49,240 --> 00:48:52,780
internally you can use x86 and line

00:48:51,160 --> 00:48:54,610
assembly for all the care and then at

00:48:52,780 --> 00:48:58,780
that point me sticking device on it is

00:48:54,610 --> 00:49:00,190
not gonna help so um okay so so that's

00:48:58,780 --> 00:49:03,570
what I mean there's there's actually a

00:49:00,190 --> 00:49:06,550
fairly big pile of problems there C++

00:49:03,570 --> 00:49:09,250
assumes all the code is going to be

00:49:06,550 --> 00:49:11,320
generated to one architecture the

00:49:09,250 --> 00:49:14,530
presence of another architecture in the

00:49:11,320 --> 00:49:18,010
abstract machine at the moment messed

00:49:14,530 --> 00:49:21,410
messes things up it would take very

00:49:18,010 --> 00:49:45,109
compiler studies to figure out new ways

00:49:21,410 --> 00:50:30,470
of Peter and then you sure it's

00:49:45,109 --> 00:50:32,359
difficult to do okay yeah okay so is

00:50:30,470 --> 00:50:34,250
that ratio likely to increase or

00:50:32,359 --> 00:50:36,440
decrease I think that ratio is likely to

00:50:34,250 --> 00:50:38,450
stay about the same and it does vary

00:50:36,440 --> 00:50:40,010
across the product lines at the end of

00:50:38,450 --> 00:50:42,290
the day like the number of threads and

00:50:40,010 --> 00:50:44,510
the amount of memory is dictated by the

00:50:42,290 --> 00:50:47,420
fact that the GPU has a very real day

00:50:44,510 --> 00:50:50,930
job and and and it needs to be

00:50:47,420 --> 00:50:52,730
profitable at that day job and so we so

00:50:50,930 --> 00:50:57,109
these numbers these numbers are picked

00:50:52,730 --> 00:50:59,510
you know based on availability of the

00:50:57,109 --> 00:51:02,240
memories from various memory

00:50:59,510 --> 00:51:04,069
manufacturers and how much yield we

00:51:02,240 --> 00:51:05,809
expect to have and then there's some

00:51:04,069 --> 00:51:07,160
performance that results out of that and

00:51:05,809 --> 00:51:09,319
then there's a very complicated

00:51:07,160 --> 00:51:12,200
multivariable optimization of the sides

00:51:09,319 --> 00:51:15,770
with the mixes you can't imagine that

00:51:12,200 --> 00:51:20,299
that ratio could swing by two to four X

00:51:15,770 --> 00:51:23,089
up probably not worse than 2 X down it's

00:51:20,299 --> 00:51:27,440
in that region it's in that region but I

00:51:23,089 --> 00:51:30,090
I don't see a near point in time where

00:51:27,440 --> 00:51:33,090
it would be drastically different

00:51:30,090 --> 00:51:39,830
because this other day job that the GPU

00:51:33,090 --> 00:51:39,830
has likes this balance yes sir

00:51:40,070 --> 00:51:44,970
could we ship in BCC with a standard

00:51:42,690 --> 00:51:49,470
library that has device everywhere we

00:51:44,970 --> 00:51:55,740
could it's a pretty monumental past just

00:51:49,470 --> 00:51:57,960
in its own right it also would affect

00:51:55,740 --> 00:51:59,790
customers who want to use a particular

00:51:57,960 --> 00:52:02,190
standing library that you would have to

00:51:59,790 --> 00:52:06,980
like my room library now you could use

00:52:02,190 --> 00:52:06,980
another one so it's a difficult choice

00:52:07,310 --> 00:52:10,460
yes sir

00:52:12,080 --> 00:52:19,200
can I speak to the differences between

00:52:14,190 --> 00:52:20,880
Cindy and Sinti yeah um well that could

00:52:19,200 --> 00:52:22,230
be another talk all right

00:52:20,880 --> 00:52:27,200
that could be another talk so that the

00:52:22,230 --> 00:52:30,750
main the main the main difference is is

00:52:27,200 --> 00:52:35,130
this D versus T distinction do you

00:52:30,750 --> 00:52:37,770
imagine that the operation is just

00:52:35,130 --> 00:52:43,760
mutating several pieces of data on by

00:52:37,770 --> 00:52:46,770
the same thread or are you executing

00:52:43,760 --> 00:52:50,490
multiple threads as part of this now the

00:52:46,770 --> 00:52:52,710
history of how GPUs came to this is long

00:52:50,490 --> 00:52:57,000
and interesting but it basically goes

00:52:52,710 --> 00:52:58,770
back to to the 80s to work that Pixar

00:52:57,000 --> 00:53:01,680
was doing Nick started developed this

00:52:58,770 --> 00:53:04,920
shading language called render man and

00:53:01,680 --> 00:53:08,220
in render man you wrote in a sealife

00:53:04,920 --> 00:53:12,210
language the math that each color

00:53:08,220 --> 00:53:15,240
channel would apply and then you your

00:53:12,210 --> 00:53:16,920
program would effectively get run for

00:53:15,240 --> 00:53:18,330
threads why maybe because you were

00:53:16,920 --> 00:53:21,870
writing in this C like language you were

00:53:18,330 --> 00:53:25,050
writing the scalar chain of evaluation

00:53:21,870 --> 00:53:29,670
for one color channel and then a Pixar

00:53:25,050 --> 00:53:33,090
wanted to run this in gangs and that

00:53:29,670 --> 00:53:36,960
sort of set the tone for what later

00:53:33,090 --> 00:53:39,450
became pixel shaders and then and then

00:53:36,960 --> 00:53:41,280
GPUs needed to run that they needed to

00:53:39,450 --> 00:53:42,599
run this execution model and so our

00:53:41,280 --> 00:53:45,509
review has never

00:53:42,599 --> 00:53:47,489
then that you're talking about multiple

00:53:45,509 --> 00:53:49,769
pieces of data in one thread or a view

00:53:47,489 --> 00:53:52,079
has always been that you were running

00:53:49,769 --> 00:53:54,450
multiple threads and then you want that

00:53:52,079 --> 00:53:56,130
vector efficiency so you add a little

00:53:54,450 --> 00:54:00,539
bit more Hardware there there's no

00:53:56,130 --> 00:54:02,789
question that sim team takes some extra

00:54:00,539 --> 00:54:08,519
hardware consideration that you would

00:54:02,789 --> 00:54:10,920
not have when your view is Cindy and and

00:54:08,519 --> 00:54:13,769
so that may seem to some group of people

00:54:10,920 --> 00:54:16,979
as an undo task there's definitely

00:54:13,769 --> 00:54:21,809
people who want the you know the

00:54:16,979 --> 00:54:24,299
absolute barest simplest silicon but in

00:54:21,809 --> 00:54:26,069
our view and and I would say in the view

00:54:24,299 --> 00:54:28,739
of the Pixar engineers who design render

00:54:26,069 --> 00:54:30,930
man there whether they really form this

00:54:28,739 --> 00:54:33,210
thought back then or you know but I'm

00:54:30,930 --> 00:54:34,650
gonna describe it to them anyway um you

00:54:33,210 --> 00:54:37,979
know they they saw that there is

00:54:34,650 --> 00:54:39,799
actually a balance between what is an

00:54:37,979 --> 00:54:43,019
elegant programming model that

00:54:39,799 --> 00:54:44,900
programmers will understand easily and

00:54:43,019 --> 00:54:47,609
it will feel comfortable and that

00:54:44,900 --> 00:54:50,269
hardware minimum cost and and this

00:54:47,609 --> 00:54:54,059
slight extra tax on the hardware which

00:54:50,269 --> 00:54:56,729
to be quite honest you know it's like a

00:54:54,059 --> 00:54:59,460
percent or something um that extra tax

00:54:56,729 --> 00:55:01,319
on the hardware is well worth it when

00:54:59,460 --> 00:55:04,979
you consider how much better you've made

00:55:01,319 --> 00:55:06,599
programmers lives and so in general you

00:55:04,979 --> 00:55:08,609
know people are afraid of port and were

00:55:06,599 --> 00:55:09,900
in the past afraid of porting to the GPU

00:55:08,609 --> 00:55:11,999
because it looked like it was a lot of

00:55:09,900 --> 00:55:13,979
work but most of the feedback we've

00:55:11,999 --> 00:55:17,989
received is that once you get over that

00:55:13,979 --> 00:55:20,940
initial fear you realize that it's

00:55:17,989 --> 00:55:23,729
actually quite easier to get higher

00:55:20,940 --> 00:55:26,729
performance using that programming

00:55:23,729 --> 00:55:29,069
abstraction relative to you know

00:55:26,729 --> 00:55:34,710
underbar underbar built-ins in your code

00:55:29,069 --> 00:55:38,239
top to bottom you know okay another

00:55:34,710 --> 00:55:38,239
question speak up

00:55:45,350 --> 00:56:05,730
yes yes right so all right so the

00:56:02,550 --> 00:56:07,650
question is if Linux and hmm and NV link

00:56:05,730 --> 00:56:11,640
and Volta all of that together it makes

00:56:07,650 --> 00:56:14,930
memory tape taste like memory how do we

00:56:11,640 --> 00:56:19,020
reconcile that with the TLS question um

00:56:14,930 --> 00:56:20,790
you can you can look at it as an issue

00:56:19,020 --> 00:56:24,210
between there are force for storage

00:56:20,790 --> 00:56:29,040
durations in in C++ there is static

00:56:24,210 --> 00:56:31,110
dynamic thread and automatic and static

00:56:29,040 --> 00:56:35,700
and dynamic storage durations are fully

00:56:31,110 --> 00:56:37,980
addressed by the next hmm Volta and the

00:56:35,700 --> 00:56:42,000
thread and automatic storage durations

00:56:37,980 --> 00:56:44,790
have some other issues so at the moment

00:56:42,000 --> 00:56:47,250
we don't allocate any threatens storage

00:56:44,790 --> 00:56:48,570
duration we really don't know what to do

00:56:47,250 --> 00:56:51,180
so we don't allocate it and if you try

00:56:48,570 --> 00:56:56,400
to access my local from a GPU thread it

00:56:51,180 --> 00:56:59,280
would just ball and certainly my

00:56:56,400 --> 00:57:00,900
preference is to find a way to explain

00:56:59,280 --> 00:57:06,720
this to you in an acceptable way in

00:57:00,900 --> 00:57:10,650
clause 4.7 maybe and then for automatics

00:57:06,720 --> 00:57:12,390
they work really great on GPU actually

00:57:10,650 --> 00:57:14,690
right up until the time where you try to

00:57:12,390 --> 00:57:16,710
share a pointer with one of your buttons

00:57:14,690 --> 00:57:18,720
but you can take pointers and the

00:57:16,710 --> 00:57:20,580
pointers do work and you can pass by a

00:57:18,720 --> 00:57:22,760
reference than that in a in at amount of

00:57:20,580 --> 00:57:25,020
execution there's no problem with that

00:57:22,760 --> 00:57:28,050
you just can't pass it with someone else

00:57:25,020 --> 00:57:32,369
and ok so so your question is is memory

00:57:28,050 --> 00:57:34,650
not really tasting like memory I guess

00:57:32,369 --> 00:57:36,480
I'm gonna narrow the claim to static and

00:57:34,650 --> 00:57:44,270
dynamic storage durations really do

00:57:36,480 --> 00:57:44,270
taste like ok you were slightly first

00:57:46,200 --> 00:58:16,690
yeah so so the ratio that I showed it is

00:58:14,049 --> 00:58:17,799
really just how much memory is on the

00:58:16,690 --> 00:58:20,499
board and how many threads are on the

00:58:17,799 --> 00:58:22,390
board's ratio of that if you know if

00:58:20,499 --> 00:58:24,039
your threads really only needed 20

00:58:22,390 --> 00:58:25,749
syllabi besiege then then everything

00:58:24,039 --> 00:58:39,579
else is just basically shared State

00:58:25,749 --> 00:58:40,509
right yes yes it's hard routine data

00:58:39,579 --> 00:58:42,279
memory quickly

00:58:40,509 --> 00:58:44,440
yeah yeah program can be over to really

00:58:42,279 --> 00:58:46,359
large it can be arbitrarily large and

00:58:44,440 --> 00:58:48,519
run arbitrarily long and run an

00:58:46,359 --> 00:58:51,400
arbitrary control flow I think there's

00:58:48,519 --> 00:58:52,479
no some people have a notion of GPAs

00:58:51,400 --> 00:58:53,529
can't do a recursion with the came to

00:58:52,479 --> 00:58:55,089
functions in the can't do function

00:58:53,529 --> 00:58:57,400
pointer this is like that hasn't been

00:58:55,089 --> 00:59:00,759
true for in video for like ten years

00:58:57,400 --> 00:59:02,109
so so you know you can have an

00:59:00,759 --> 00:59:04,599
arbitrarily large program that's

00:59:02,109 --> 00:59:19,319
arbitrarily gnarly it's just the amount

00:59:04,599 --> 00:59:22,479
of per thread state the program memory

00:59:19,319 --> 00:59:24,910
we live in one of the two but he doesn't

00:59:22,479 --> 00:59:27,369
know about yeah it's because no it's

00:59:24,910 --> 00:59:28,930
because it's my gradable like it it

00:59:27,369 --> 00:59:31,210
could start in one and move to the other

00:59:28,930 --> 00:59:34,119
a few microseconds into the job you know

00:59:31,210 --> 00:59:35,799
um you could canonically think of it as

00:59:34,119 --> 00:59:37,180
being in the GPU memory but it's

00:59:35,799 --> 00:59:38,680
relatively small compared to that

00:59:37,180 --> 00:59:41,349
sixteen gigabyte I'm not sure how many

00:59:38,680 --> 00:59:44,529
people have gigabyte object five I know

00:59:41,349 --> 00:59:47,829
I can nvidia has some but um but they're

00:59:44,529 --> 00:59:50,499
pretty rare they're pretty rare okay all

00:59:47,829 --> 00:59:52,539
right that's it so I'll take your

00:59:50,499 --> 00:59:53,330
questions offline at this point I'll

00:59:52,539 --> 00:59:58,540
sticker

00:59:53,330 --> 00:59:58,540

YouTube URL: https://www.youtube.com/watch?v=86seb-iZCnI


