Title: CppCon 2017: P. McKenney, M. Michael & M. Wong “Is Parallel Programming still hard? PART 1 of 2”
Publication date: 2017-10-08
Playlist: CppCon 2017
Description: 
	http://CppCon.org
—
Presentation Slides, PDFs, Source Code and other presenter materials are available at: https://github.com/CppCon/CppCon2017
—
Most embedded devices are multicore, and we see concurrency becoming ubiquitous for machine learning, machine vision, and self-driving cars. Thus the age of concurrency is upon us, so whether you like it or not, concurrency is now just part of the job. It is therefore time to stop being concurrency cowards and start on the path towards producing high-quality high-performance highly scalable concurrent software artifacts. After all, there was a time when sequential programming was considered mind-crushingly hard: In fact, in the late 1970s, Paul attended a talk where none other than Edsger Dijkstra argued, and not without reason, that programmers could not be trusted to correctly code simple sequential loops. However, these long-past perilous programming pitfalls are now easily avoided with improved programming models, heuristics, and tools. We firmly believe that concurrent and parallel programming will make this same transition. This talk will help you do just that. 

Besides, after more than a decade since the end of the hardware "free lunch", why should parallel programming still be hard?
— 
Paul E. McKenney: IBM Linux Technology Center, Distinguished Engineer

Paul E. McKenney has been coding for almost four decades, more than half of that on parallel hardware, where his work has earned him a reputation among some as a flaming heretic. Over the past decade, Paul has been an IBM Distinguished Engineer at the IBM Linux Technology Center. Paul maintains the RCU implementation within the Linux kernel, where the variety of workloads present highly entertaining performance, scalability, real-time response, and energy-efficiency challenges. Prior to that, he worked on the DYNIX/ptx kernel at Sequent, and prior to that on packet-radio and Internet protocols (but long before it was polite to mention Internet at cocktail parties), system administration, business applications, and real-time systems. His hobbies include what passes for running at his age along with the usual house-wife-and-kids habit.

Maged Michael: Facebook, Engineer

Maged Michael is a software engineer at Facebook. He is the inventor of hazard pointers, lock-free malloc and several algorithms for concurrent data structures. His work is included in several IBM products where he was a Research Staff Member at the IBM T.J. Watson Research Center. He received a Ph.D. in computer science from the University of Rochester. He is an ACM Distinguished Scientist and an ACM Distinguished Speaker. He is an elected member of the Connecticut Academy of Science and Engineering. He received the 2014 ACM SIGPLAN Most Influential PLDI Paper Award for his paper on Scalable Lock-Free Dynamic Memory Allocation.


Michael Wong: Codeplay, VP Research & Development

Michael Wong is VP of R&D at Codeplay Software. He is a current Director and VP of ISOCPP , and a senior member of the C++ Standards Committee with more then 15 years of experience. 
He chairs the WG21 SG5 Transactional Memory and SG14 Games Development/Low Latency/Financials C++ groups and is the co-author of a number C++/OpenMP/Transactional memory features including generalized attributes, user-defined literals, inheriting constructors, weakly ordered memory models, and explicit conversion operators. He has published numerous research papers and is the author of a book on C++11. He has been in invited speaker and keynote at numerous conferences. 

He is currently the editor of SG1 Concurrency TS and SG5 Transactional Memory TS. He is also the Chair of the SYCL standard and all Programming Languages for Standards Council of Canada. 

Previously, he was CEO of OpenMP involved with taking OpenMP toward Accelerator support and the Technical Strategy Architect responsible for moving IBM's compilers to Clang/LLVM after leading IBM’s XL C++ compiler team. 
—
Videos Filmed & Edited by Bash Films: http://www.BashFilms.com
Captions: 
	00:00:00,000 --> 00:00:06,120
hi everybody thank you very much for

00:00:02,490 --> 00:00:07,980
coming back after lunch my name is my

00:00:06,120 --> 00:00:10,620
name is Michael Wong I am I'm with

00:00:07,980 --> 00:00:12,509
Coldplay I'm presenting we're presenting

00:00:10,620 --> 00:00:14,549
this talk England construction with a

00:00:12,509 --> 00:00:16,500
number of people who have been involved

00:00:14,549 --> 00:00:18,180
in parallel programming for a number of

00:00:16,500 --> 00:00:20,220
years and we definitely have the gray

00:00:18,180 --> 00:00:21,449
hair and all the lack of hair to show

00:00:20,220 --> 00:00:24,000
for it

00:00:21,449 --> 00:00:25,650
my colleagues are Paul McKinney from IBM

00:00:24,000 --> 00:00:26,789
and Magath Michael from Facebook so

00:00:25,650 --> 00:00:29,580
you're definitely getting a cross

00:00:26,789 --> 00:00:31,650
spectrum of probably I would say at

00:00:29,580 --> 00:00:33,300
least half at least half a century of

00:00:31,650 --> 00:00:35,670
experience I would say I would expect

00:00:33,300 --> 00:00:37,860
the topic of our discussion is gonna be

00:00:35,670 --> 00:00:40,050
about its somewhat controversial we

00:00:37,860 --> 00:00:43,260
always believe or you've always heard

00:00:40,050 --> 00:00:45,090
that parallel programming is hard so

00:00:43,260 --> 00:00:46,289
what should you do about it we are

00:00:45,090 --> 00:00:49,500
turning that a little bit and we're

00:00:46,289 --> 00:00:54,539
saying is it still hard because if it is

00:00:49,500 --> 00:00:57,449
we need to do something about it again

00:00:54,539 --> 00:00:59,309
so wit as with any talks it's supported

00:00:57,449 --> 00:01:01,500
by a number of people in the background

00:00:59,309 --> 00:01:05,100
various standard committees various

00:01:01,500 --> 00:01:07,110
discussions that we've been having with

00:01:05,100 --> 00:01:08,760
anything I would say that there's always

00:01:07,110 --> 00:01:11,210
going to be some amount of errors and

00:01:08,760 --> 00:01:13,710
and anything that remains is all mine

00:01:11,210 --> 00:01:15,060
not anyone elses I would like to say

00:01:13,710 --> 00:01:16,590
it's all theirs I would like to think

00:01:15,060 --> 00:01:18,360
that all of the good stuff is all mine

00:01:16,590 --> 00:01:21,869
but that's unfortunately been proven too

00:01:18,360 --> 00:01:23,460
often not to be the case finally there's

00:01:21,869 --> 00:01:25,320
a usual legal disclaimer I won't bore

00:01:23,460 --> 00:01:26,820
you with it it's a standard template and

00:01:25,320 --> 00:01:29,100
of course the thing that my company

00:01:26,820 --> 00:01:30,450
always makes me put on not not a bad

00:01:29,100 --> 00:01:32,700
thing really it tells us that our

00:01:30,450 --> 00:01:36,090
company does a lot about what does a lot

00:01:32,700 --> 00:01:38,100
with heterogeneous computing in C++ with

00:01:36,090 --> 00:01:41,670
a language called sickle which were

00:01:38,100 --> 00:01:43,259
trying to put into the C++ standard so

00:01:41,670 --> 00:01:45,810
today what are you gonna hear about we

00:01:43,259 --> 00:01:48,420
we actually have three speakers hey

00:01:45,810 --> 00:01:53,040
Anthony don't you hate it when you walk

00:01:48,420 --> 00:01:56,040
in and people call your name it's okay

00:01:53,040 --> 00:01:57,600
we're good friends the business is such

00:01:56,040 --> 00:01:58,920
that you kind of know everybody by now

00:01:57,600 --> 00:02:00,869
you know all the all the parallel

00:01:58,920 --> 00:02:02,520
programmers who's who practitioner who's

00:02:00,869 --> 00:02:04,740
been in in it around around for all

00:02:02,520 --> 00:02:07,530
these years so our topic is to separate

00:02:04,740 --> 00:02:08,970
out into three groups I'm gonna I'm

00:02:07,530 --> 00:02:11,520
gonna bring you in the preamble about

00:02:08,970 --> 00:02:13,530
asking you what is parallel programming

00:02:11,520 --> 00:02:15,930
fundamentally still hard and

00:02:13,530 --> 00:02:18,240
let's show you what is because we've

00:02:15,930 --> 00:02:19,920
been involved in designing parallel

00:02:18,240 --> 00:02:21,750
programming models for almost twenty

00:02:19,920 --> 00:02:23,760
years we've had some experience and

00:02:21,750 --> 00:02:26,520
background in understanding where the

00:02:23,760 --> 00:02:28,890
weird fuzzy edges are so what does it

00:02:26,520 --> 00:02:31,380
mean to have a to be designing a

00:02:28,890 --> 00:02:32,640
parallel programming model Paul McKinney

00:02:31,380 --> 00:02:34,680
is gonna come in and look at what a

00:02:32,640 --> 00:02:36,360
hardware side so you look at though from

00:02:34,680 --> 00:02:37,980
the hardware side can it actually helped

00:02:36,360 --> 00:02:41,850
can it make can you make parallel

00:02:37,980 --> 00:02:43,709
programming easier and then Maggitt

00:02:41,850 --> 00:02:45,780
Michael is gonna come in and give you a

00:02:43,709 --> 00:02:47,190
really detailed example just so that you

00:02:45,780 --> 00:02:49,980
don't say well we didn't see any code

00:02:47,190 --> 00:02:52,590
well there is code but it's coming okay

00:02:49,980 --> 00:02:53,970
and it's not gonna be HS any code it's

00:02:52,590 --> 00:02:55,290
not just gonna be any little blob that

00:02:53,970 --> 00:02:56,970
you're gonna fit into a single slide

00:02:55,290 --> 00:02:58,860
it's going to be about a detailed

00:02:56,970 --> 00:03:00,720
breakdown about single producers single

00:02:58,860 --> 00:03:03,110
consumer buffer and it's gonna show you

00:03:00,720 --> 00:03:06,180
all the things we're going to talk about

00:03:03,110 --> 00:03:09,180
from the previous two two boobs talking

00:03:06,180 --> 00:03:11,220
about sharing parallelism about locality

00:03:09,180 --> 00:03:18,299
atomic operations and cache line

00:03:11,220 --> 00:03:20,760
bouncing so in a we figured this talk is

00:03:18,299 --> 00:03:22,860
aiming at a mid level of parallel

00:03:20,760 --> 00:03:25,170
programming not necessarily a total

00:03:22,860 --> 00:03:27,060
beginner level and not necessarily at

00:03:25,170 --> 00:03:29,100
the more advanced high level advanced

00:03:27,060 --> 00:03:30,120
level that we usually talk about so as a

00:03:29,100 --> 00:03:31,950
result we're not going to talk about

00:03:30,120 --> 00:03:33,480
lockrey programming we're not gonna

00:03:31,950 --> 00:03:34,799
I usually come up and talk about

00:03:33,480 --> 00:03:36,390
transactional memory you're not gonna

00:03:34,799 --> 00:03:39,239
hear any of that we're not gonna talk

00:03:36,390 --> 00:03:40,500
about hazard pointers or RCU we're not

00:03:39,239 --> 00:03:42,329
even gonna talk about computer

00:03:40,500 --> 00:03:44,640
architecture although some of many of

00:03:42,329 --> 00:03:46,890
the things on this screen are kind of

00:03:44,640 --> 00:03:48,570
required reading you know but this is

00:03:46,890 --> 00:03:50,310
one of those things were the people or

00:03:48,570 --> 00:03:51,750
the person on the stage is asking you go

00:03:50,310 --> 00:03:53,519
to just to go do your homework to some

00:03:51,750 --> 00:03:54,950
extent because it would be boring for us

00:03:53,519 --> 00:03:58,260
to actually go through all these things

00:03:54,950 --> 00:04:00,810
in fact we actually have slides for all

00:03:58,260 --> 00:04:02,190
these things we weren't quite exactly

00:04:00,810 --> 00:04:05,100
sure where to draw the line so we

00:04:02,190 --> 00:04:06,930
actually did all the slides and over

00:04:05,100 --> 00:04:10,170
over 200 of them and then we threw them

00:04:06,930 --> 00:04:13,290
all away this morning have you ever had

00:04:10,170 --> 00:04:15,060
that happen you know that morning you're

00:04:13,290 --> 00:04:17,760
like this is we're not gonna have time

00:04:15,060 --> 00:04:20,039
but no no this is all gone it's really

00:04:17,760 --> 00:04:21,720
good stuff though I promise you you

00:04:20,039 --> 00:04:22,919
would have been really impressed when

00:04:21,720 --> 00:04:24,390
you look at the stuff that we have

00:04:22,919 --> 00:04:26,490
talking about

00:04:24,390 --> 00:04:27,050
Flynn's classification talking about

00:04:26,490 --> 00:04:29,690
dependent

00:04:27,050 --> 00:04:31,940
cease talking about parallel patterns

00:04:29,690 --> 00:04:33,169
these are all exciting topics but

00:04:31,940 --> 00:04:35,360
they're not for this talk they're for

00:04:33,169 --> 00:04:37,550
other talks maybe out of people's talks

00:04:35,360 --> 00:04:39,169
we've hacked we have actually talked

00:04:37,550 --> 00:04:41,000
about them in some of our talks like

00:04:39,169 --> 00:04:44,680
transactional memory I'll see you has

00:04:41,000 --> 00:04:44,680
her pointers from previous years before

00:04:45,069 --> 00:04:52,699
all right so when you talk about

00:04:50,270 --> 00:04:54,800
parallel programming a number of perils

00:04:52,699 --> 00:04:57,860
comes to mind people talk about them all

00:04:54,800 --> 00:05:00,080
the time and we definitely have the gray

00:04:57,860 --> 00:05:01,759
hair and a lack of hair to show for it

00:05:00,080 --> 00:05:03,500
because we've been doing it for over 20

00:05:01,759 --> 00:05:06,020
years in some form or another

00:05:03,500 --> 00:05:08,180
I myself was started up started over

00:05:06,020 --> 00:05:10,699
about because of my involvement in IBM

00:05:08,180 --> 00:05:12,919
and as an IBM scientists we were doing

00:05:10,699 --> 00:05:13,190
high-performance computing I was the CEO

00:05:12,919 --> 00:05:15,710
of

00:05:13,190 --> 00:05:18,409
openmp so we work hard to develop the

00:05:15,710 --> 00:05:22,099
openmp which is a parallel compiler on

00:05:18,409 --> 00:05:23,719
top of C C++ and Fortran so we met all

00:05:22,099 --> 00:05:26,479
these things at some point in time

00:05:23,719 --> 00:05:30,050
whether they were data races or mutual

00:05:26,479 --> 00:05:31,759
exclusions or locks what happens if you

00:05:30,050 --> 00:05:34,130
have contention that would lead to poor

00:05:31,759 --> 00:05:35,599
poor scaling what happens you fall

00:05:34,130 --> 00:05:38,449
sharing that was probably the first

00:05:35,599 --> 00:05:41,120
thing you run into ok and then there's

00:05:38,449 --> 00:05:43,460
also you know like a locality references

00:05:41,120 --> 00:05:46,340
lack of you know low balancing issues

00:05:43,460 --> 00:05:50,029
they all lead to some consequence or

00:05:46,340 --> 00:05:51,770
another and certainly with communication

00:05:50,029 --> 00:05:59,990
overhead that just causes everything

00:05:51,770 --> 00:06:01,909
else to go to go bad unless

00:05:59,990 --> 00:06:04,250
as with any time I just want to step

00:06:01,909 --> 00:06:05,779
back very briefly to talk a little bit

00:06:04,250 --> 00:06:07,129
about just three not not a whole lot of

00:06:05,779 --> 00:06:10,129
time because I think many of you guys

00:06:07,129 --> 00:06:14,330
may know anyway but certainly you can

00:06:10,129 --> 00:06:17,330
see that by this chart the growth of

00:06:14,330 --> 00:06:20,479
transistors is pretty linear and

00:06:17,330 --> 00:06:24,229
consistent over the last many many years

00:06:20,479 --> 00:06:28,279
however the frequency clock has leveled

00:06:24,229 --> 00:06:31,669
off and the only way to keep that speed

00:06:28,279 --> 00:06:34,009
that that speed promised that the

00:06:31,669 --> 00:06:35,839
vendors have which is what teacher hook

00:06:34,009 --> 00:06:38,469
to bind their computer systems is to

00:06:35,839 --> 00:06:40,200
bike is to continually raise that core

00:06:38,469 --> 00:06:42,630
okay the

00:06:40,200 --> 00:06:44,850
problem is that even that has a problem

00:06:42,630 --> 00:06:48,210
because it's very soon you run into this

00:06:44,850 --> 00:06:50,790
heat death of your system and you've

00:06:48,210 --> 00:06:52,740
seen this before this is essentially the

00:06:50,790 --> 00:06:54,780
free lunch where the free lunch used to

00:06:52,740 --> 00:06:55,890
be you know you just keep adding you

00:06:54,780 --> 00:06:58,020
know you just keep buying new hardware

00:06:55,890 --> 00:07:00,450
and you get to get the free improvement

00:06:58,020 --> 00:07:01,970
now the reliance is in now with the

00:07:00,450 --> 00:07:04,680
reliance has to move into your software

00:07:01,970 --> 00:07:06,600
because the hardware has stopped getting

00:07:04,680 --> 00:07:09,210
faster so your software now has to get

00:07:06,600 --> 00:07:10,770
faster this leads to exactly the same

00:07:09,210 --> 00:07:14,010
graph except you'll notice I've added

00:07:10,770 --> 00:07:15,980
the yellow line there on power what what

00:07:14,010 --> 00:07:19,410
has essentially happened is that a

00:07:15,980 --> 00:07:21,690
hardware problem is now just became a

00:07:19,410 --> 00:07:23,280
software problem like a like every like

00:07:21,690 --> 00:07:24,930
a lot of things in the world bottlenecks

00:07:23,280 --> 00:07:27,900
just move from one place to another

00:07:24,930 --> 00:07:31,430
ok and this is what's driving the

00:07:27,900 --> 00:07:31,430
parallel computing architecture today

00:07:32,150 --> 00:07:45,240
that's it now I'm sure all you guys have

00:07:43,170 --> 00:07:46,650
a car know what to call hat what what a

00:07:45,240 --> 00:07:48,150
call it does but have you ever driven

00:07:46,650 --> 00:07:51,450
and have you ever been in one of the

00:07:48,150 --> 00:07:55,350
really old Model T Fords ok that's what

00:07:51,450 --> 00:07:57,300
it looks like inside it would not be

00:07:55,350 --> 00:07:59,070
impossible for you to drive it you could

00:07:57,300 --> 00:08:00,960
probably figure it out but the controls

00:07:59,070 --> 00:08:03,060
aren't exactly what it were you're used

00:08:00,960 --> 00:08:05,460
to for instance the left side left side

00:08:03,060 --> 00:08:07,020
levers sets the rear wheel parking brake

00:08:05,460 --> 00:08:09,960
and then puts the transmission into

00:08:07,020 --> 00:08:12,810
neutral the right lever is actually the

00:08:09,960 --> 00:08:14,640
throttle ok and then the the lever on

00:08:12,810 --> 00:08:16,850
the left of the steering column is for

00:08:14,640 --> 00:08:19,170
ignition timing and then the left pedal

00:08:16,850 --> 00:08:20,730
changed it to two Ford gears while the

00:08:19,170 --> 00:08:22,740
right pedal to Center pedal controls

00:08:20,730 --> 00:08:24,920
reversed and then the right pedal is the

00:08:22,740 --> 00:08:24,920
brake

00:08:30,430 --> 00:08:38,000
now at one time the ability to drive a

00:08:34,460 --> 00:08:40,130
car was actually only a ferry for very

00:08:38,000 --> 00:08:43,580
few people but now it's a fairly common

00:08:40,130 --> 00:08:46,310
it's it's fairly commonplace this

00:08:43,580 --> 00:08:49,580
drastic change come about for two very

00:08:46,310 --> 00:08:51,710
basic reasons okay the cars become

00:08:49,580 --> 00:08:56,300
cheaper and they are more readily

00:08:51,710 --> 00:08:58,280
available or and the cause simply became

00:08:56,300 --> 00:08:59,870
a lot easier to operate because they

00:08:58,280 --> 00:09:01,720
have a lot of assists for you right now

00:08:59,870 --> 00:09:04,430
a lot of automatic transmissions

00:09:01,720 --> 00:09:06,860
automatic chokes a lot of automatic

00:09:04,430 --> 00:09:11,540
starters they and they have greatly

00:09:06,860 --> 00:09:15,020
improved reliability as a result and

00:09:11,540 --> 00:09:17,990
today I bought this car about two years

00:09:15,020 --> 00:09:20,870
ago and it's now basically the classic

00:09:17,990 --> 00:09:22,160
bloat where I actually have stuff in

00:09:20,870 --> 00:09:24,080
there I still don't know what it does

00:09:22,160 --> 00:09:26,300
this is the manual that tells me what it

00:09:24,080 --> 00:09:28,010
does I think it can steer itself through

00:09:26,300 --> 00:09:29,690
the lanes and all those things never

00:09:28,010 --> 00:09:30,920
actually use it half of this stuff is

00:09:29,690 --> 00:09:33,170
blower I can still drive the car

00:09:30,920 --> 00:09:35,450
fundamentally without any of this stuff

00:09:33,170 --> 00:09:37,580
but basically this is this is a little

00:09:35,450 --> 00:09:40,670
bit of what's going on in the in terms

00:09:37,580 --> 00:09:43,100
of technological domain and it happens

00:09:40,670 --> 00:09:45,200
to computers I'm just the same it's no

00:09:43,100 --> 00:09:47,570
longer necessary for instance to or use

00:09:45,200 --> 00:09:50,080
the key punch I actually started by

00:09:47,570 --> 00:09:52,700
using a key punch in order to program

00:09:50,080 --> 00:09:54,500
spreadsheets now allow programmers

00:09:52,700 --> 00:09:56,300
non-programmers to get results from the

00:09:54,500 --> 00:10:00,980
computers that would require a team of

00:09:56,300 --> 00:10:03,770
specialists from before okay one of the

00:10:00,980 --> 00:10:06,860
most compelling example today about this

00:10:03,770 --> 00:10:09,260
easy ease of use of computers of cause

00:10:06,860 --> 00:10:11,240
web surfing and content creation believe

00:10:09,260 --> 00:10:13,130
it or not content creation would it

00:10:11,240 --> 00:10:15,050
required a team of scientists years ago

00:10:13,130 --> 00:10:18,770
today we just had together these slides

00:10:15,050 --> 00:10:21,220
Paul lives in Oregon actually I'm in

00:10:18,770 --> 00:10:24,440
Toronto but we found actually in Toronto

00:10:21,220 --> 00:10:25,820
maggot is in New York okay and we just

00:10:24,440 --> 00:10:27,740
we've been working at this for the last

00:10:25,820 --> 00:10:28,160
couple of months slowly putting it

00:10:27,740 --> 00:10:30,080
together

00:10:28,160 --> 00:10:32,180
it's amazing that this stuff is actually

00:10:30,080 --> 00:10:34,070
coming together and it's just it's

00:10:32,180 --> 00:10:35,750
fairly it's fairly taken for granted now

00:10:34,070 --> 00:10:39,260
years ago this would have been a

00:10:35,750 --> 00:10:40,700
research project so we're saying that

00:10:39,260 --> 00:10:43,610
the same thing is happening with

00:10:40,700 --> 00:10:47,420
parallel programming

00:10:43,610 --> 00:10:52,220
if you argue that parallel programming

00:10:47,420 --> 00:10:55,250
is difficult it's mostly because it's

00:10:52,220 --> 00:10:59,510
currently being perceived by many as

00:10:55,250 --> 00:11:01,670
being difficult let me explain what that

00:10:59,510 --> 00:11:03,650
means in some ways because one of the

00:11:01,670 --> 00:11:06,620
thing that you see computers have always

00:11:03,650 --> 00:11:09,470
been pretty much parallel but what's

00:11:06,620 --> 00:11:11,300
going on for the last 20 or 25 years or

00:11:09,470 --> 00:11:13,460
languages that's been given to us we've

00:11:11,300 --> 00:11:16,400
been forced by those languages to think

00:11:13,460 --> 00:11:19,310
serially when you create a loop for

00:11:16,400 --> 00:11:21,770
instance even though it you know what in

00:11:19,310 --> 00:11:23,810
turn innately what happens it loops

00:11:21,770 --> 00:11:25,820
around there's actually a lot of force

00:11:23,810 --> 00:11:27,980
ordering that actually up that is

00:11:25,820 --> 00:11:30,710
actually implied in there beyond what

00:11:27,980 --> 00:11:32,480
it's actually doing a lot of those

00:11:30,710 --> 00:11:34,700
things are necessary for in fact they're

00:11:32,480 --> 00:11:36,440
harmful to for parallel programming but

00:11:34,700 --> 00:11:39,110
the compiler cannot know those things

00:11:36,440 --> 00:11:41,660
and this is indeed one of the reason why

00:11:39,110 --> 00:11:45,010
that in order to think parallel you do

00:11:41,660 --> 00:11:48,290
have to go back to the reality of it

00:11:45,010 --> 00:11:50,000
okay all of you guys can drive and do a

00:11:48,290 --> 00:11:52,970
lot of things in parallel all of you

00:11:50,000 --> 00:11:54,890
guys can work work the book your car

00:11:52,970 --> 00:11:56,510
while texting on your cell phone I know

00:11:54,890 --> 00:11:58,190
it's not legal but I'm sure you all do

00:11:56,510 --> 00:11:59,570
it and I do it myself okay

00:11:58,190 --> 00:12:03,080
that's parallel prove that's parallel

00:11:59,570 --> 00:12:05,600
processing right there yet but computers

00:12:03,080 --> 00:12:08,090
somehow we're forced into this

00:12:05,600 --> 00:12:11,300
perception for and not your fault I

00:12:08,090 --> 00:12:12,470
would say of years of years of this of

00:12:11,300 --> 00:12:14,650
this guidance that we should try to

00:12:12,470 --> 00:12:17,030
think things in terms of steps of serial

00:12:14,650 --> 00:12:19,090
operations when it really is not

00:12:17,030 --> 00:12:22,310
necessary and this is actually what's

00:12:19,090 --> 00:12:24,980
causing the perception that parallel

00:12:22,310 --> 00:12:27,040
programming is difficult it's not

00:12:24,980 --> 00:12:29,770
parallel programming that's difficult

00:12:27,040 --> 00:12:32,450
programming itself is difficult period

00:12:29,770 --> 00:12:33,860
okay whether it's serial or parable

00:12:32,450 --> 00:12:35,930
you've been all trained to make it make

00:12:33,860 --> 00:12:41,630
make serial programming a lot easier

00:12:35,930 --> 00:12:44,260
okay so if you look back at many of the

00:12:41,630 --> 00:12:46,700
historical difficulties you see that

00:12:44,260 --> 00:12:48,890
historically there are many of these

00:12:46,700 --> 00:12:50,060
difficulties these problems that are

00:12:48,890 --> 00:12:52,220
well on the way they're becoming

00:12:50,060 --> 00:12:54,200
overcome first over the past few decades

00:12:52,220 --> 00:12:57,020
the cost of parallel systems have

00:12:54,200 --> 00:12:57,750
decreased tremendously from many

00:12:57,020 --> 00:13:00,390
multiples

00:12:57,750 --> 00:13:01,950
a house to a fraction of that of a

00:13:00,390 --> 00:13:03,990
bicycle in fact I dare you to find

00:13:01,950 --> 00:13:15,710
something that is not used that does not

00:13:03,990 --> 00:13:18,300
have parallel processors today okay so

00:13:15,710 --> 00:13:20,430
the second thing is that the advent the

00:13:18,300 --> 00:13:23,820
advent of low-cost and rarely available

00:13:20,430 --> 00:13:27,080
multi-core systems means that the once

00:13:23,820 --> 00:13:29,220
rare rarified experience of programming

00:13:27,080 --> 00:13:31,770
parallel systems is now putting much

00:13:29,220 --> 00:13:33,720
available to everyone this is pretty

00:13:31,770 --> 00:13:36,030
much the same process as an automobile

00:13:33,720 --> 00:13:37,890
in a way it's well on its way of being

00:13:36,030 --> 00:13:40,920
common for everyone to be able to do

00:13:37,890 --> 00:13:42,840
these kinds of things third in the 20th

00:13:40,920 --> 00:13:44,400
century their large systems of highly

00:13:42,840 --> 00:13:46,920
parallel software that was pretty much

00:13:44,400 --> 00:13:49,470
closely guarded proprietary things today

00:13:46,920 --> 00:13:52,350
on any given on any github you can

00:13:49,470 --> 00:13:55,620
easily be participating in a parallel

00:13:52,350 --> 00:13:57,150
programming project project the fourth

00:13:55,620 --> 00:13:59,340
thing even though the large-scale

00:13:57,150 --> 00:14:01,610
parallel programming projects in the 80s

00:13:59,340 --> 00:14:04,500
and the 90s were almost all proprietary

00:14:01,610 --> 00:14:07,710
projects these projects have essentially

00:14:04,500 --> 00:14:10,320
ceded a community with a massive group

00:14:07,710 --> 00:14:11,850
of developers who understand the

00:14:10,320 --> 00:14:16,470
engineering discipline that would be

00:14:11,850 --> 00:14:18,540
required so that actually helps to take

00:14:16,470 --> 00:14:20,010
us to the fifth spot and fortunate the

00:14:18,540 --> 00:14:22,230
fifth part is the one that is still a

00:14:20,010 --> 00:14:24,930
problem the high cost of communication

00:14:22,230 --> 00:14:28,520
relative to that of processing is still

00:14:24,930 --> 00:14:35,580
largely in place and this difficulty is

00:14:28,520 --> 00:14:37,970
essentially as been increasing every

00:14:35,580 --> 00:14:37,970
year actually

00:14:45,480 --> 00:14:50,710
so the owners I would claim is that if

00:14:48,730 --> 00:14:53,380
you believe parallel programming is

00:14:50,710 --> 00:14:56,710
difficult you have it is the onus is on

00:14:53,380 --> 00:14:59,230
you to come up as to the reason why if

00:14:56,710 --> 00:15:01,330
you really believe that it's exceedingly

00:14:59,230 --> 00:15:03,279
hard then you should have already some

00:15:01,330 --> 00:15:06,820
kind of an answer as the wise parallel

00:15:03,279 --> 00:15:09,010
programming heart and indeed you could

00:15:06,820 --> 00:15:10,750
list any number of reasons ranging from

00:15:09,010 --> 00:15:12,100
the things that I've shown in previous

00:15:10,750 --> 00:15:12,790
screens like dead logs or race

00:15:12,100 --> 00:15:14,890
conditions

00:15:12,790 --> 00:15:18,430
the real answer I've already mentioned

00:15:14,890 --> 00:15:22,120
is that it's really actually not all

00:15:18,430 --> 00:15:24,850
that hard if if if it would all that

00:15:22,120 --> 00:15:27,100
hard and how is it possible that we have

00:15:24,850 --> 00:15:31,300
all these large open source projects on

00:15:27,100 --> 00:15:33,160
Apache wood MySQL on the Linux on Linux

00:15:31,300 --> 00:15:34,350
kernels for instance and people have

00:15:33,160 --> 00:15:38,050
been able to do that

00:15:34,350 --> 00:15:39,850
okay so a better question might be why

00:15:38,050 --> 00:15:42,330
is parallel programming perceived to be

00:15:39,850 --> 00:15:42,330
so difficult

00:15:49,140 --> 00:15:53,820
so how could parallel programming ever

00:15:51,450 --> 00:15:56,130
be as easy as sequential programming you

00:15:53,820 --> 00:15:58,079
might ask well it all really all depends

00:15:56,130 --> 00:16:00,209
on the programming environment if you

00:15:58,079 --> 00:16:01,190
heard of the programming language SQL

00:16:00,209 --> 00:16:04,890
for databases

00:16:01,190 --> 00:16:07,230
it's an really underappreciated success

00:16:04,890 --> 00:16:09,060
story because it really allowed

00:16:07,230 --> 00:16:11,250
programmers who pretty much know nothing

00:16:09,060 --> 00:16:16,740
about parallelism to keep a large

00:16:11,250 --> 00:16:18,540
parallel system productively active with

00:16:16,740 --> 00:16:22,370
a fairly easy command that is very

00:16:18,540 --> 00:16:22,370
English that is very much English like

00:16:22,910 --> 00:16:28,500
MATLAB actually does something very

00:16:25,170 --> 00:16:30,209
similar and then on Linux systems take a

00:16:28,500 --> 00:16:34,410
look at the command here that I written

00:16:30,209 --> 00:16:38,519
using a get input with a grep and then a

00:16:34,410 --> 00:16:40,890
sword in this case the show essentially

00:16:38,519 --> 00:16:42,630
runs a pipeline the shell pipeline is

00:16:40,890 --> 00:16:45,120
sensory each engine essentially runs the

00:16:42,630 --> 00:16:48,209
get input grep and then sort the process

00:16:45,120 --> 00:16:49,950
in parallel now this is a parallel

00:16:48,209 --> 00:16:52,680
system right there it wasn't that hard

00:16:49,950 --> 00:16:54,660
so in short I would say that parallel

00:16:52,680 --> 00:16:57,060
programming is just as easy as

00:16:54,660 --> 00:16:58,890
sequential programming at least at these

00:16:57,060 --> 00:17:05,790
environments that hide the parallelism

00:16:58,890 --> 00:17:09,179
very well so after years of doing

00:17:05,790 --> 00:17:11,490
designs doing doing language designs

00:17:09,179 --> 00:17:15,209
some of the things that the the three of

00:17:11,490 --> 00:17:18,900
us have considered the design points

00:17:15,209 --> 00:17:21,059
that makes for a good parallel system so

00:17:18,900 --> 00:17:23,429
the goal of parallel programming over

00:17:21,059 --> 00:17:26,150
and above sequential programming of

00:17:23,429 --> 00:17:29,309
these trees in general you have

00:17:26,150 --> 00:17:32,309
performance you have productivity you

00:17:29,309 --> 00:17:32,669
have generality ok actually that's a

00:17:32,309 --> 00:17:33,929
mistake

00:17:32,669 --> 00:17:37,650
portability should really go with

00:17:33,929 --> 00:17:39,570
productivity but what it does form is

00:17:37,650 --> 00:17:41,580
this well-known iron triangle that if

00:17:39,570 --> 00:17:43,950
you are a team leader of a project you

00:17:41,580 --> 00:17:46,200
will probably have a lot of experience

00:17:43,950 --> 00:17:47,429
about balancing your teams and the tasks

00:17:46,200 --> 00:17:49,799
that needs to be done in terms of

00:17:47,429 --> 00:17:51,990
resources in terms of time in terms of

00:17:49,799 --> 00:17:54,299
personnel so when you're designing a

00:17:51,990 --> 00:17:55,650
language any language you have these

00:17:54,299 --> 00:17:57,150
things that consider now you may not be

00:17:55,650 --> 00:18:00,840
actually be thinking about them in fact

00:17:57,150 --> 00:18:02,850
most designers probably are not when

00:18:00,840 --> 00:18:04,320
you're examining a pair

00:18:02,850 --> 00:18:05,730
of programming language you could

00:18:04,320 --> 00:18:08,070
usually use something like this to

00:18:05,730 --> 00:18:10,020
characterize it and see where the design

00:18:08,070 --> 00:18:12,210
points are whether it's giving more

00:18:10,020 --> 00:18:14,520
favor to performance whether it's giving

00:18:12,210 --> 00:18:19,830
more favor to productivity or general or

00:18:14,520 --> 00:18:21,270
generality so what about some people

00:18:19,830 --> 00:18:23,520
might ask immediately what about

00:18:21,270 --> 00:18:25,290
correctness maintainability robustness

00:18:23,520 --> 00:18:27,740
and so on now these are important goals

00:18:25,290 --> 00:18:29,970
and they're just as important as or

00:18:27,740 --> 00:18:31,650
important for sequential programs as

00:18:29,970 --> 00:18:34,560
they are for parallel program and

00:18:31,650 --> 00:18:36,660
because of that we claim that important

00:18:34,560 --> 00:18:38,550
as they are they don't really belong on

00:18:36,660 --> 00:18:45,630
the list that we're looking at for

00:18:38,550 --> 00:18:48,030
parallel programming and if correctness

00:18:45,630 --> 00:18:50,060
maintainability and robustness don't

00:18:48,030 --> 00:18:53,310
make the list

00:18:50,060 --> 00:18:55,020
why do productivity and generality well

00:18:53,310 --> 00:18:57,810
given that parallel programming is

00:18:55,020 --> 00:18:59,580
perceived to be much harder than

00:18:57,810 --> 00:19:01,470
sequential programming productivity

00:18:59,580 --> 00:19:05,070
essentially scant amount and therefore

00:19:01,470 --> 00:19:07,260
has cannot be cannot be omitted high

00:19:05,070 --> 00:19:10,050
productivity parallel programming

00:19:07,260 --> 00:19:12,960
environments like SQL they serve a

00:19:10,050 --> 00:19:15,120
really special purpose so generality has

00:19:12,960 --> 00:19:16,920
to be also be added to the list are they

00:19:15,120 --> 00:19:18,630
specific one of the reason that they are

00:19:16,920 --> 00:19:21,560
high productivity is because they are

00:19:18,630 --> 00:19:25,020
very specific and very good at what they

00:19:21,560 --> 00:19:27,150
do and that gives you a clue of a

00:19:25,020 --> 00:19:29,490
language design the more general

00:19:27,150 --> 00:19:31,590
something is it's going to be much more

00:19:29,490 --> 00:19:35,130
difficult for it to serve a multiple

00:19:31,590 --> 00:19:38,930
large audience of needs but there's good

00:19:35,130 --> 00:19:38,930
reason to have general-purpose languages

00:19:39,920 --> 00:19:43,980
you wouldn't be here if you didn't you

00:19:42,180 --> 00:19:45,630
didn't know about that because C++ is

00:19:43,980 --> 00:19:47,970
one of the best general-purpose language

00:19:45,630 --> 00:19:49,710
there is now given that parallel

00:19:47,970 --> 00:19:52,290
programs are much harder to prove

00:19:49,710 --> 00:19:54,000
correct than sequential program you

00:19:52,290 --> 00:19:55,860
might still say that correctness really

00:19:54,000 --> 00:19:58,790
be a should be under the list now from

00:19:55,860 --> 00:20:01,620
an engineering point of view the

00:19:58,790 --> 00:20:03,630
difficulty in proving correctness either

00:20:01,620 --> 00:20:05,880
formally or informally it would be

00:20:03,630 --> 00:20:07,590
important so far as it impacts the

00:20:05,880 --> 00:20:09,750
primary goal productivity

00:20:07,590 --> 00:20:11,720
so in that case correctness proves are

00:20:09,750 --> 00:20:15,060
important and they're part of what

00:20:11,720 --> 00:20:16,830
productivity would be and finally I know

00:20:15,060 --> 00:20:19,620
that a lot of you guys general

00:20:16,830 --> 00:20:20,700
is C++ is something you love you're

00:20:19,620 --> 00:20:24,419
interesting but it might not necessarily

00:20:20,700 --> 00:20:26,370
be your day-to-day job so having fun so

00:20:24,419 --> 00:20:28,409
what about just having fun with C++ or

00:20:26,370 --> 00:20:30,929
any particular language well having fun

00:20:28,409 --> 00:20:33,090
is important but unless you are happiest

00:20:30,929 --> 00:20:37,230
you would not normally be a primary goal

00:20:33,090 --> 00:20:38,730
but if your hobbies go nuts on it let's

00:20:37,230 --> 00:20:43,860
take a look at the first criteria

00:20:38,730 --> 00:20:45,779
performance so the focus of performance

00:20:43,860 --> 00:20:52,169
has shifted from hardware to parallel

00:20:45,779 --> 00:20:54,750
software this change is basically due to

00:20:52,169 --> 00:20:57,840
Moore's law because it continues to

00:20:54,750 --> 00:21:00,000
deliver increases in transistor density

00:20:57,840 --> 00:21:01,649
it's basically ceased to provide the

00:21:00,000 --> 00:21:03,659
traditional single threaded performance

00:21:01,649 --> 00:21:06,120
so as you can see in this picture here

00:21:03,659 --> 00:21:07,860
on the side which shows that writing

00:21:06,120 --> 00:21:09,809
single threaded code and simply waiting

00:21:07,860 --> 00:21:11,940
a year or two for the CPU to catch up is

00:21:09,809 --> 00:21:14,010
no longer an option then given the

00:21:11,940 --> 00:21:18,360
recent trend on the part of all the

00:21:14,010 --> 00:21:21,000
major manufacturers parallelism is the

00:21:18,360 --> 00:21:23,730
only way transferring the hardware

00:21:21,000 --> 00:21:27,690
problem into the software problem even

00:21:23,730 --> 00:21:30,690
so the first goal is performance rather

00:21:27,690 --> 00:21:34,260
than scalability given that the easiest

00:21:30,690 --> 00:21:37,230
way to attain linear scalability is to

00:21:34,260 --> 00:21:39,090
reduce the performance of each CPU given

00:21:37,230 --> 00:21:41,700
a for CPU system let me give you an

00:21:39,090 --> 00:21:43,350
example which would you prefer a program

00:21:41,700 --> 00:21:48,720
that gives you a hundred transactions

00:21:43,350 --> 00:21:50,490
per second on a single CPU or a program

00:21:48,720 --> 00:21:53,970
that gives you ten transactions per

00:21:50,490 --> 00:21:58,110
second on a single CPU but scales

00:21:53,970 --> 00:22:00,419
perfectly the first programs definitely

00:21:58,110 --> 00:22:01,950
makes a better choice and even though

00:22:00,419 --> 00:22:06,419
the answer might change if you happen to

00:22:01,950 --> 00:22:08,010
have a thirty-two CPU system what I'm

00:22:06,419 --> 00:22:10,019
trying to say here is trying to bring

00:22:08,010 --> 00:22:11,639
you back down to the base case people

00:22:10,019 --> 00:22:13,440
are often too eager to start using

00:22:11,639 --> 00:22:14,789
parallel program right away parallel

00:22:13,440 --> 00:22:16,559
programming right away because you know

00:22:14,789 --> 00:22:18,990
we're all involved in it in my heads and

00:22:16,559 --> 00:22:21,179
our heads in it but just because you

00:22:18,990 --> 00:22:23,669
have multiple CPU doesn't necessarily

00:22:21,179 --> 00:22:26,399
mean that in and of itself is a reason

00:22:23,669 --> 00:22:27,870
to use them all especially given the

00:22:26,399 --> 00:22:31,049
recent decrease in price of these

00:22:27,870 --> 00:22:32,789
multiple CPU systems the key point

00:22:31,049 --> 00:22:35,129
that you want to take away from here is

00:22:32,789 --> 00:22:37,830
that it's just one potential

00:22:35,129 --> 00:22:40,200
optimizations of many if your program is

00:22:37,830 --> 00:22:42,749
fast enough to optimize either by

00:22:40,200 --> 00:22:44,759
parallelizing it or by applying any

00:22:42,749 --> 00:22:48,389
number of up to potential sequential

00:22:44,759 --> 00:22:53,960
optimizations you should definitely go

00:22:48,389 --> 00:22:53,960
with the sequential optimizations first

00:22:58,369 --> 00:23:05,789
let's look at productivity which is the

00:23:00,960 --> 00:23:07,499
next next pod now productivity has been

00:23:05,789 --> 00:23:10,169
becoming increasingly important in

00:23:07,499 --> 00:23:11,999
recent decades to see this think about

00:23:10,169 --> 00:23:14,580
the price of early computers that were

00:23:11,999 --> 00:23:16,710
tens of millions of dollars at a time

00:23:14,580 --> 00:23:18,889
when essentially engineering salaries

00:23:16,710 --> 00:23:22,009
back then were a few thousand dollars if

00:23:18,889 --> 00:23:24,809
dedicating a team of 10 engineers to

00:23:22,009 --> 00:23:27,269
this kind of machine would improve its

00:23:24,809 --> 00:23:30,210
performance by even 5 percent or 10

00:23:27,269 --> 00:23:31,830
percent then the salaries would be what

00:23:30,210 --> 00:23:36,539
would be repaid many many many times

00:23:31,830 --> 00:23:38,820
over one of the one such machine of

00:23:36,539 --> 00:23:40,830
nugget of all was this ciserek

00:23:38,820 --> 00:23:43,739
one of the oldest stored-program

00:23:40,830 --> 00:23:45,749
computers there but this computer had

00:23:43,739 --> 00:23:48,330
lots of transistors was built with

00:23:45,749 --> 00:23:53,879
thousands of vacuum tubes ok requires an

00:23:48,330 --> 00:23:56,460
army to maintain so today it would be

00:23:53,879 --> 00:23:58,580
quite difficult to indeed purchase a

00:23:56,460 --> 00:24:01,440
machine with so little computing power

00:23:58,580 --> 00:24:04,440
maybe you could buy some 8-bit embedded

00:24:01,440 --> 00:24:05,940
micro processors like a Zed 80 but even

00:24:04,440 --> 00:24:07,950
the in the old even back then when we

00:24:05,940 --> 00:24:09,869
were buying them as a kid you would you

00:24:07,950 --> 00:24:11,669
could probably buy them for two I don't

00:24:09,869 --> 00:24:13,139
remember like they were two dollars for

00:24:11,669 --> 00:24:15,259
a couple of thousand units or something

00:24:13,139 --> 00:24:21,119
like that so it really wasn't all that

00:24:15,259 --> 00:24:24,629
all that hard now today with the advent

00:24:21,119 --> 00:24:28,230
of multi-core CPUs this increase has

00:24:24,629 --> 00:24:29,009
allowed him to be continued basically

00:24:28,230 --> 00:24:31,259
unabated

00:24:29,009 --> 00:24:34,789
despite the clock frequency that we have

00:24:31,259 --> 00:24:34,789
been encountered in 2003

00:24:37,960 --> 00:24:43,539
so perhaps the takeaway from this is

00:24:40,299 --> 00:24:45,940
that maybe at one time the sole purpose

00:24:43,539 --> 00:24:49,899
of parallel software was performance but

00:24:45,940 --> 00:24:55,990
now I argue that productivity is much

00:24:49,899 --> 00:24:58,240
more important than just performance so

00:24:55,990 --> 00:24:59,740
one way to justify the cost of

00:24:58,240 --> 00:25:02,320
developing parallel software is

00:24:59,740 --> 00:25:05,649
basically this drive from maximal

00:25:02,320 --> 00:25:07,869
generality or else being equal the cost

00:25:05,649 --> 00:25:09,850
of a more general software audit

00:25:07,869 --> 00:25:14,730
artifact can be spread over more users

00:25:09,850 --> 00:25:17,470
than that of a less general one

00:25:14,730 --> 00:25:19,059
unfortunately generality often comes at

00:25:17,470 --> 00:25:21,549
the cost of something else and in this

00:25:19,059 --> 00:25:25,509
case one of the iron triangles I did the

00:25:21,549 --> 00:25:28,570
performance or the or the productivity

00:25:25,509 --> 00:25:33,970
or both now to see this think about a

00:25:28,570 --> 00:25:36,519
few popular languages if you look at C

00:25:33,970 --> 00:25:39,940
C++ locking and thread something we

00:25:36,519 --> 00:25:42,360
created for C++ 11 okay this cat which

00:25:39,940 --> 00:25:45,009
essentially is based on POSIX thread

00:25:42,360 --> 00:25:46,240
windows thread or some kernel threads

00:25:45,009 --> 00:25:48,730
kernel threads or something like that

00:25:46,240 --> 00:25:52,029
they essentially give you pretty pretty

00:25:48,730 --> 00:25:54,700
good performance and very good

00:25:52,029 --> 00:25:57,399
generality or in this case portability

00:25:54,700 --> 00:25:59,559
but it's still not very productive to

00:25:57,399 --> 00:26:01,450
program in that manner we've all pretty

00:25:59,559 --> 00:26:03,820
much been told that that you should not

00:26:01,450 --> 00:26:08,519
program in R all threads you should use

00:26:03,820 --> 00:26:08,519
some higher higher abstractions instead

00:26:08,940 --> 00:26:16,179
however I argue that with C++ 14 and 17

00:26:13,019 --> 00:26:18,070
these days we hope that we're designing

00:26:16,179 --> 00:26:20,860
something that can give you a choice in

00:26:18,070 --> 00:26:24,210
that giant triangle if you start using

00:26:20,860 --> 00:26:27,820
higher level abstractions of tasks

00:26:24,210 --> 00:26:30,730
higher levels of parallel parallelism ts

00:26:27,820 --> 00:26:33,100
using the parallel algorithms or higher

00:26:30,730 --> 00:26:36,340
levels of the concurrency TS which we

00:26:33,100 --> 00:26:38,230
hope will be coming for C++ 20 what

00:26:36,340 --> 00:26:40,629
happens then is that you can now start

00:26:38,230 --> 00:26:42,700
tuning gives you a little more tuning of

00:26:40,629 --> 00:26:45,249
the knobs as to whether you want more

00:26:42,700 --> 00:26:47,409
performance or whether you want more

00:26:45,249 --> 00:26:51,129
generality or more productivity more

00:26:47,409 --> 00:26:51,970
portability that's the real key that

00:26:51,129 --> 00:26:55,520
these

00:26:51,970 --> 00:26:57,830
new C++ parallelism capabilities are

00:26:55,520 --> 00:27:00,620
really giving you now you can all get

00:26:57,830 --> 00:27:04,760
down into the mud and talk about the

00:27:00,620 --> 00:27:06,980
fact that the parallelism TS allows you

00:27:04,760 --> 00:27:09,730
to do vector a vector and parallel and

00:27:06,980 --> 00:27:15,049
and parallel and vector eyes or just all

00:27:09,730 --> 00:27:16,580
paralyzed at the end of the day it's

00:27:15,049 --> 00:27:20,900
about the fact that you can essentially

00:27:16,580 --> 00:27:23,240
tune the knobs much better than before

00:27:20,900 --> 00:27:25,640
and this is unique among programming

00:27:23,240 --> 00:27:26,059
languages that's why these guys are in

00:27:25,640 --> 00:27:28,100
red

00:27:26,059 --> 00:27:32,750
you heard beyond it this morning talk

00:27:28,100 --> 00:27:36,440
about Python it's probably clear that

00:27:32,750 --> 00:27:38,900
Python is a high productivity language

00:27:36,440 --> 00:27:41,510
with great generality and in fact in

00:27:38,900 --> 00:27:44,720
this particular diagram on the on the

00:27:41,510 --> 00:27:47,270
right here the further down you get into

00:27:44,720 --> 00:27:49,940
the the hardware a hardware level the

00:27:47,270 --> 00:27:53,059
productivity generally it decreases it's

00:27:49,940 --> 00:27:54,860
getting more specific okay but you still

00:27:53,059 --> 00:27:56,540
that is the point where you can start

00:27:54,860 --> 00:27:58,490
generally get better and better

00:27:56,540 --> 00:28:00,590
performance and better spend it better

00:27:58,490 --> 00:28:02,990
and better generality so it's a result

00:28:00,590 --> 00:28:05,780
something like Java gives you great

00:28:02,990 --> 00:28:07,850
productivity and generality this is a

00:28:05,780 --> 00:28:11,480
general-purpose language inherent

00:28:07,850 --> 00:28:13,309
multi-threaded capability with a great

00:28:11,480 --> 00:28:16,730
garbage collector and a rich set of

00:28:13,309 --> 00:28:18,770
class libraries but the performance

00:28:16,730 --> 00:28:23,660
generally has been acknowledged as being

00:28:18,770 --> 00:28:26,030
poor MPI the message passing interface

00:28:23,660 --> 00:28:30,470
that the scientific community uses for

00:28:26,030 --> 00:28:32,270
no to know parallelism is something that

00:28:30,470 --> 00:28:34,010
has been around for a long time and it

00:28:32,270 --> 00:28:36,770
generally power is some of the the

00:28:34,010 --> 00:28:38,660
biggest computer clusters out there it

00:28:36,770 --> 00:28:42,080
gives you unparalleled performance and

00:28:38,660 --> 00:28:43,820
offers scalability so in scopus it's

00:28:42,080 --> 00:28:45,830
pretty general-purpose but it's

00:28:43,820 --> 00:28:48,140
essentially mainly used for the

00:28:45,830 --> 00:28:50,750
scientific and technical computing the

00:28:48,140 --> 00:28:56,360
productivity is by many believed by many

00:28:50,750 --> 00:28:59,299
is lower than C and C++ OpenMP gives you

00:28:56,360 --> 00:29:01,429
a set of compute of compiler directives

00:28:59,299 --> 00:29:04,400
using pragmas or comments and you can

00:29:01,429 --> 00:29:05,360
paste that over C C++ and Fortran in the

00:29:04,400 --> 00:29:07,100
same way which

00:29:05,360 --> 00:29:08,480
give you this commonality so it's

00:29:07,100 --> 00:29:11,480
especially if you're doing mix

00:29:08,480 --> 00:29:13,100
programming which many of the people in

00:29:11,480 --> 00:29:14,870
natural labs have they might have some

00:29:13,100 --> 00:29:16,790
code and see in fact most of the weather

00:29:14,870 --> 00:29:19,400
code that is running predicting your

00:29:16,790 --> 00:29:22,160
hurricane system everything around it is

00:29:19,400 --> 00:29:25,370
actually C++ code but the kernel is all

00:29:22,160 --> 00:29:26,809
still Fortran code and nobody can change

00:29:25,370 --> 00:29:30,160
that because they've been debugged over

00:29:26,809 --> 00:29:32,720
20 years and if you try to change that

00:29:30,160 --> 00:29:34,850
something will happen that is not gonna

00:29:32,720 --> 00:29:36,620
be good so as a result this is the kind

00:29:34,850 --> 00:29:39,530
of mixed world where something like

00:29:36,620 --> 00:29:43,850
openmp really works well and in this

00:29:39,530 --> 00:29:47,240
case it is quite specific to the tasks

00:29:43,850 --> 00:29:49,190
that it's been set forward now this

00:29:47,240 --> 00:29:52,070
specificity has a cost because it limits

00:29:49,190 --> 00:29:58,370
the performance in some ways but it is

00:29:52,070 --> 00:30:00,020
definitely easier to use than MPI then

00:29:58,370 --> 00:30:03,650
we come to a language that supports

00:30:00,020 --> 00:30:07,400
heterogeneous computing like OpenCL it's

00:30:03,650 --> 00:30:09,679
run by Kronos and they host a number of

00:30:07,400 --> 00:30:12,470
graphics initiative like OpenGL and now

00:30:09,679 --> 00:30:15,320
Vulcan and as well so the language cycle

00:30:12,470 --> 00:30:17,299
later following on on that it's one of

00:30:15,320 --> 00:30:19,610
the first language that allows you to

00:30:17,299 --> 00:30:21,919
dispatch to any number of GPU devices

00:30:19,610 --> 00:30:24,770
that's not Nvidia if it's Nvidia then

00:30:21,919 --> 00:30:29,120
you have to use things like CUDA it is

00:30:24,770 --> 00:30:31,640
also believed one of its major issue not

00:30:29,120 --> 00:30:33,220
to say its downfall but it's one of its

00:30:31,640 --> 00:30:35,540
major issue is that it's too low-level

00:30:33,220 --> 00:30:37,040
ok and because it's a little too

00:30:35,540 --> 00:30:40,000
low-level it's very low level it's

00:30:37,040 --> 00:30:42,590
productivity is not high so as a result

00:30:40,000 --> 00:30:44,960
definitely it falls in the generality

00:30:42,590 --> 00:30:48,799
and performance domain now sickled is

00:30:44,960 --> 00:30:51,500
actually the C++ language that is based

00:30:48,799 --> 00:30:53,090
on open CL that basically gives you the

00:30:51,500 --> 00:30:54,950
same ability it allows you to dispatch

00:30:53,090 --> 00:30:57,799
to any open CL devices

00:30:54,950 --> 00:31:00,470
that's not Nvidia although that could

00:30:57,799 --> 00:31:02,720
actually happen fairly easily so it also

00:31:00,470 --> 00:31:05,000
gives you great performance and general

00:31:02,720 --> 00:31:07,910
great performance and generality but

00:31:05,000 --> 00:31:10,010
because it's based on modern C++ it has

00:31:07,910 --> 00:31:12,620
the same capability allowing you to

00:31:10,010 --> 00:31:14,510
choose each point of the of that Iron

00:31:12,620 --> 00:31:16,700
Triangle you could tune it for higher

00:31:14,510 --> 00:31:19,010
productivity if that is what you wish if

00:31:16,700 --> 00:31:22,070
you believe C++ can give you that high

00:31:19,010 --> 00:31:25,370
product productivity okay

00:31:22,070 --> 00:31:27,590
finally with sql's the the structured

00:31:25,370 --> 00:31:30,650
query language it is probably one of the

00:31:27,590 --> 00:31:34,550
the top language out there for

00:31:30,650 --> 00:31:36,410
relational databases it's well measured

00:31:34,550 --> 00:31:38,600
by the transactional processing

00:31:36,410 --> 00:31:41,300
performance Council using TPC benchmarks

00:31:38,600 --> 00:31:43,160
so productivity is excellent in fact

00:31:41,300 --> 00:31:45,590
this parallel programming language

00:31:43,160 --> 00:31:48,020
essentially enables a lot of people to

00:31:45,590 --> 00:31:51,110
make good use of a large parallel system

00:31:48,020 --> 00:31:53,300
without having any particularly fine

00:31:51,110 --> 00:31:55,700
detail knowledge about parallel program

00:31:53,300 --> 00:31:57,470
these people do not know anything about

00:31:55,700 --> 00:32:00,260
sharing they definitely don't know

00:31:57,470 --> 00:32:01,730
anything about lock-free programming or

00:32:00,260 --> 00:32:03,380
have all hazard pointers or

00:32:01,730 --> 00:32:06,200
transactional memory and yet they're

00:32:03,380 --> 00:32:09,290
able to manipulate and get amazing

00:32:06,200 --> 00:32:10,640
results based on what they have so now

00:32:09,290 --> 00:32:12,200
so it's important to note that a

00:32:10,640 --> 00:32:14,240
trade-off between productivity and

00:32:12,200 --> 00:32:16,310
generality has essentially existed for

00:32:14,240 --> 00:32:19,160
centuries in a lot of in many fields for

00:32:16,310 --> 00:32:21,110
example in this particular case a nail

00:32:19,160 --> 00:32:23,570
gun is more productive than a hammer

00:32:21,110 --> 00:32:25,670
because a hammer can be used for many

00:32:23,570 --> 00:32:27,920
things if you have a hammer everything

00:32:25,670 --> 00:32:29,870
looks like a nail would say say so it

00:32:27,920 --> 00:32:31,700
should be no surprise to see that

00:32:29,870 --> 00:32:35,840
similar trade-off appear in the field

00:32:31,700 --> 00:32:38,630
parallel programming here in this case

00:32:35,840 --> 00:32:40,130
users 1 2 3 & 4 have specific jobs that

00:32:38,630 --> 00:32:41,930
they need to compute the computer to

00:32:40,130 --> 00:32:43,460
help them with and the most productive

00:32:41,930 --> 00:32:46,190
possible language environment for a

00:32:43,460 --> 00:32:50,330
given user is one that simply does that

00:32:46,190 --> 00:32:52,340
users job without actually requiring any

00:32:50,330 --> 00:32:55,990
particular programming configuration or

00:32:52,340 --> 00:32:55,990
setup excuse me

00:32:57,130 --> 00:33:02,570
unfortunately a system that does the job

00:32:59,960 --> 00:33:04,310
require as required by user 1 is

00:33:02,570 --> 00:33:06,290
unlikely to do the job that's required

00:33:04,310 --> 00:33:08,270
by users 2 so in other words the most

00:33:06,290 --> 00:33:10,880
productive language and environments are

00:33:08,270 --> 00:33:14,120
very domain-specific so by definition

00:33:10,880 --> 00:33:16,040
they lack generality another option is

00:33:14,120 --> 00:33:18,020
to tailor a given programming language

00:33:16,040 --> 00:33:19,880
to the environment or the environment to

00:33:18,020 --> 00:33:22,460
the hardware system for example low

00:33:19,880 --> 00:33:25,160
level languages like assembler C C++ or

00:33:22,460 --> 00:33:30,500
Java or to some abstraction like Haskell

00:33:25,160 --> 00:33:32,419
Prolog so that's is given or shown in

00:33:30,500 --> 00:33:36,409
the circular region and in

00:33:32,419 --> 00:33:39,139
Center there and these languages are

00:33:36,409 --> 00:33:41,600
considered to be general in the sense

00:33:39,139 --> 00:33:43,549
that they are equally ill suited to the

00:33:41,600 --> 00:33:46,009
jobs that's required by users one two

00:33:43,549 --> 00:33:48,350
three or four in other words their

00:33:46,009 --> 00:33:50,989
generality is bought at the expense of

00:33:48,350 --> 00:33:53,359
decreased productivity in that Iron

00:33:50,989 --> 00:33:54,980
Triangle when compared to

00:33:53,359 --> 00:33:57,710
domain-specific languages and

00:33:54,980 --> 00:33:59,149
environments worse yet basically a

00:33:57,710 --> 00:34:01,609
language that's tailored to a given

00:33:59,149 --> 00:34:02,960
abstraction is also likely to suffer

00:34:01,609 --> 00:34:06,440
from performance and scalability

00:34:02,960 --> 00:34:08,359
problems until somebody figures out how

00:34:06,440 --> 00:34:11,480
to efficiently map that abstraction to

00:34:08,359 --> 00:34:13,250
actual hardware so with these often

00:34:11,480 --> 00:34:15,679
conflicting parallel programming goals

00:34:13,250 --> 00:34:18,200
it's now take let's take a look at how

00:34:15,679 --> 00:34:20,299
to avoid these conflicts by looking at

00:34:18,200 --> 00:34:23,149
how to do them so what makes it hard

00:34:20,299 --> 00:34:25,220
these are essentially the aspects that

00:34:23,149 --> 00:34:27,230
you always have to go through as you're

00:34:25,220 --> 00:34:28,399
thinking about parallel programming you

00:34:27,230 --> 00:34:30,379
would probably have to do some work

00:34:28,399 --> 00:34:33,409
partitioning some parallel access

00:34:30,379 --> 00:34:36,049
control some resource we partitioning or

00:34:33,409 --> 00:34:42,379
replication or some interaction with the

00:34:36,049 --> 00:34:44,480
low-lying Hardware okay so what

00:34:42,379 --> 00:34:46,609
partitioning is absolutely required for

00:34:44,480 --> 00:34:48,349
parallel programming if there's just one

00:34:46,609 --> 00:34:50,629
global work then it can be executed

00:34:48,349 --> 00:34:54,700
essentially at most by one CPU which is

00:34:50,629 --> 00:34:58,010
by definition a sequential execution now

00:34:54,700 --> 00:34:59,720
the key point takeaway there is that

00:34:58,010 --> 00:35:01,520
allowing and the thing is permitting

00:34:59,720 --> 00:35:03,290
threads to execute concurrently allow

00:35:01,520 --> 00:35:05,359
you to greatly increase the program

00:35:03,290 --> 00:35:06,799
state space so this is about breaking up

00:35:05,359 --> 00:35:09,200
your work so that it can be partitioned

00:35:06,799 --> 00:35:11,390
it can be balanced across these threads

00:35:09,200 --> 00:35:13,670
the problem is that adding that that

00:35:11,390 --> 00:35:15,890
that very act adds to the amount of

00:35:13,670 --> 00:35:17,420
overheads and state space that you might

00:35:15,890 --> 00:35:20,059
be maintaining and this is something you

00:35:17,420 --> 00:35:23,109
would have to decide so as a result this

00:35:20,059 --> 00:35:25,549
can greatly decrease productivity

00:35:23,109 --> 00:35:27,650
parallel access control essentially

00:35:25,549 --> 00:35:29,569
means given a single threaded sequential

00:35:27,650 --> 00:35:31,309
program the single thread basically has

00:35:29,569 --> 00:35:35,000
the full access to all the programs

00:35:31,309 --> 00:35:37,670
resources these resources are things

00:35:35,000 --> 00:35:39,920
like IO memory computational

00:35:37,670 --> 00:35:41,660
accelerators files and things like that

00:35:39,920 --> 00:35:43,900
now the first parallel access control

00:35:41,660 --> 00:35:46,400
issue is whether the form of the access

00:35:43,900 --> 00:35:49,640
to a given resource depends

00:35:46,400 --> 00:35:50,839
on that resources location for example

00:35:49,640 --> 00:35:53,779
in some of the message passing

00:35:50,839 --> 00:35:55,609
environment local variable access is

00:35:53,779 --> 00:35:57,619
accessed using expressions and

00:35:55,609 --> 00:36:00,380
assignments which remote variable access

00:35:57,619 --> 00:36:01,730
uses a totally different mechanism these

00:36:00,380 --> 00:36:05,750
things call communicators

00:36:01,730 --> 00:36:08,240
so the other parallel access control

00:36:05,750 --> 00:36:10,579
issue is how threads essentially

00:36:08,240 --> 00:36:12,559
coordinate access to the resources this

00:36:10,579 --> 00:36:14,329
coordination is essentially carry out by

00:36:12,559 --> 00:36:16,130
a large number of synchronization

00:36:14,329 --> 00:36:18,890
mechanism provided by the various

00:36:16,130 --> 00:36:21,740
parallel languages okay and these things

00:36:18,890 --> 00:36:23,750
essentially can cause a decrease in

00:36:21,740 --> 00:36:25,670
performance because these traditional

00:36:23,750 --> 00:36:29,599
programming concerns like dead logs live

00:36:25,670 --> 00:36:32,510
locks coordination can essentially is

00:36:29,599 --> 00:36:36,140
what is adding to your trouble with the

00:36:32,510 --> 00:36:38,000
performance the third one has to do with

00:36:36,140 --> 00:36:40,430
resource partitioning your data your

00:36:38,000 --> 00:36:41,990
resource and replicating across nodes

00:36:40,430 --> 00:36:45,829
and this happens whether you're using

00:36:41,990 --> 00:36:48,260
seats a single CPU or multiple CPUs with

00:36:45,829 --> 00:36:50,359
GPUs you have to move that data around

00:36:48,260 --> 00:36:53,059
data would have to be spread over Numa

00:36:50,359 --> 00:36:54,410
nodes or cache lines okay indeed one of

00:36:53,059 --> 00:36:56,089
the biggest problem with heterogeneous

00:36:54,410 --> 00:36:58,220
computing one of the biggest issue with

00:36:56,089 --> 00:37:01,750
heterogeneous computing is when and how

00:36:58,220 --> 00:37:04,690
to move that data efficiently to the GPU

00:37:01,750 --> 00:37:07,670
so that it can be computed very quickly

00:37:04,690 --> 00:37:10,400
ok and worthwhile for the computation of

00:37:07,670 --> 00:37:12,980
the GPU which often is in a sim D manner

00:37:10,400 --> 00:37:15,230
so resore partition is basically is

00:37:12,980 --> 00:37:17,150
frequently application dependent but it

00:37:15,230 --> 00:37:19,730
generally reduces generality because

00:37:17,150 --> 00:37:21,650
what happens is is that in our research

00:37:19,730 --> 00:37:23,150
and work at code play we found that when

00:37:21,650 --> 00:37:24,920
we optimize that movement for one

00:37:23,150 --> 00:37:26,690
particular GPU it doesn't work that

00:37:24,920 --> 00:37:29,150
great with something with someone else's

00:37:26,690 --> 00:37:31,039
GPU like AMD or whether you might and

00:37:29,150 --> 00:37:32,599
then none of that actually works with an

00:37:31,039 --> 00:37:37,970
FPGA so you kind of have to throw your

00:37:32,599 --> 00:37:39,619
hand up and start over again the last

00:37:37,970 --> 00:37:41,960
one has to do with interacting with

00:37:39,619 --> 00:37:43,880
Hardware at some level you're gonna have

00:37:41,960 --> 00:37:45,500
to know that and and Hardware

00:37:43,880 --> 00:37:46,760
interaction is normally while it's

00:37:45,500 --> 00:37:48,950
normally the domain of the operating

00:37:46,760 --> 00:37:51,589
system the compilers or the libraries or

00:37:48,950 --> 00:37:53,900
other software environment structures

00:37:51,589 --> 00:37:55,609
the developers working with new hardware

00:37:53,900 --> 00:37:57,770
in component often have to work directly

00:37:55,609 --> 00:37:59,839
with them and direct access to the

00:37:57,770 --> 00:38:00,320
hardware is needed because that's where

00:37:59,839 --> 00:38:02,270
you're gonna

00:38:00,320 --> 00:38:04,370
squeeze out the last drop of performance

00:38:02,270 --> 00:38:06,620
yes it's great to have all these great

00:38:04,370 --> 00:38:08,630
abstractions but at some level you still

00:38:06,620 --> 00:38:11,620
need that hardware interaction to do

00:38:08,630 --> 00:38:15,440
that and direct access to the hardware

00:38:11,620 --> 00:38:17,600
it's great but it greatly reduces your

00:38:15,440 --> 00:38:22,550
productivity especially a portability is

00:38:17,600 --> 00:38:24,740
required so this gives you an idea of

00:38:22,550 --> 00:38:26,630
why these things are important and why

00:38:24,740 --> 00:38:28,760
every time you do a parallel program and

00:38:26,630 --> 00:38:31,190
we're gonna demonstrate that later on

00:38:28,760 --> 00:38:33,620
with the hardware examples Paul's gonna

00:38:31,190 --> 00:38:35,990
give you and the direct example that

00:38:33,620 --> 00:38:37,850
Maggie is gonna give you in terms of how

00:38:35,990 --> 00:38:40,700
to how he's gonna partition resources

00:38:37,850 --> 00:38:43,250
how he's gonna work with parallel access

00:38:40,700 --> 00:38:45,170
control so at the end of the day you've

00:38:43,250 --> 00:38:46,610
already seen the main theme I've been

00:38:45,170 --> 00:38:48,290
saying all along the reason it was it's

00:38:46,610 --> 00:38:50,810
now easier to drive the cars because

00:38:48,290 --> 00:38:52,100
automation is is there to help you so I

00:38:50,810 --> 00:38:53,840
would say that with language in

00:38:52,100 --> 00:38:55,370
environments automation is going to be

00:38:53,840 --> 00:39:02,150
the key for some of the killer

00:38:55,370 --> 00:39:03,650
applications I've talked about this

00:39:02,150 --> 00:39:05,300
already at the beginning so I've given

00:39:03,650 --> 00:39:06,560
it away so what makes parallel

00:39:05,300 --> 00:39:08,480
programming harder than serial

00:39:06,560 --> 00:39:11,210
programming how much of a lot of this is

00:39:08,480 --> 00:39:13,580
just simply adapting a new mindset many

00:39:11,210 --> 00:39:15,590
of us drive cars and many of our

00:39:13,580 --> 00:39:17,540
activities are in parallel when you play

00:39:15,590 --> 00:39:20,300
basketball and yet we've been forced

00:39:17,540 --> 00:39:22,460
down this path by programming languages

00:39:20,300 --> 00:39:24,740
a bit for us to think in a serial manner

00:39:22,460 --> 00:39:28,580
the tools in front of us are they are

00:39:24,740 --> 00:39:31,100
guiding us into doing this in a parallel

00:39:28,580 --> 00:39:32,690
in a serial manner so if you look back

00:39:31,100 --> 00:39:34,910
we can see that given the parallel

00:39:32,690 --> 00:39:36,590
systems have been in existence in for

00:39:34,910 --> 00:39:38,870
decades it's basically asking with

00:39:36,590 --> 00:39:40,550
asking why is it that they are causing

00:39:38,870 --> 00:39:42,380
so much fuss over the last few years

00:39:40,550 --> 00:39:44,120
this is pretty much toward the end I

00:39:42,380 --> 00:39:45,620
want to step back given the fact that

00:39:44,120 --> 00:39:47,510
some of us have a perspective on history

00:39:45,620 --> 00:39:49,880
and many of you guys do too if you lived

00:39:47,510 --> 00:39:51,470
through the 80s and the 70s this is not

00:39:49,880 --> 00:39:53,510
the first great parallel put to great

00:39:51,470 --> 00:39:56,110
programming crisis there was another one

00:39:53,510 --> 00:40:00,280
before back in the 1970s and 80s and

00:39:56,110 --> 00:40:03,800
eventually it led to a large number of

00:40:00,280 --> 00:40:05,960
of languages proliferation of language

00:40:03,800 --> 00:40:09,650
that was the good the fat and the ugly

00:40:05,960 --> 00:40:13,630
now there were a lot of ugly languages

00:40:09,650 --> 00:40:16,869
and fat languages and of the

00:40:13,630 --> 00:40:20,910
Fatty's languages that we had I would

00:40:16,869 --> 00:40:24,730
say that some of them things like Pascal

00:40:20,910 --> 00:40:27,369
because essentially it's too bad you

00:40:24,730 --> 00:40:29,019
know they gave a help great great

00:40:27,369 --> 00:40:31,000
promises but had these weird type

00:40:29,019 --> 00:40:34,990
systems that you have to work with none

00:40:31,000 --> 00:40:39,759
of it actually came to pretty much

00:40:34,990 --> 00:40:41,859
anything they essentially gave you small

00:40:39,759 --> 00:40:43,809
advantages in some limited area but

00:40:41,859 --> 00:40:47,200
essentially lost disadvantages in other

00:40:43,809 --> 00:40:49,960
areas when you look at the good

00:40:47,200 --> 00:40:52,480
languages they were actually quite a few

00:40:49,960 --> 00:40:54,339
heroes back then if you look back you

00:40:52,480 --> 00:40:57,700
have some of you guys might recognize

00:40:54,339 --> 00:41:00,059
this II calc you might look at you might

00:40:57,700 --> 00:41:03,700
recognize the a slide presentation

00:41:00,059 --> 00:41:05,410
manager you might recognize the PDP many

00:41:03,700 --> 00:41:08,289
computers there

00:41:05,410 --> 00:41:10,329
these were back in the 70s and 80s and

00:41:08,289 --> 00:41:14,230
they were the successful ones that allow

00:41:10,329 --> 00:41:16,089
you to solve the programming crisis they

00:41:14,230 --> 00:41:17,559
were the primary solutions there was not

00:41:16,089 --> 00:41:19,269
basically not high-minded tools or

00:41:17,559 --> 00:41:21,579
languages languages but rather the

00:41:19,269 --> 00:41:23,829
slowly process red sheets processors

00:41:21,579 --> 00:41:25,089
presentation managers that essentially

00:41:23,829 --> 00:41:28,000
allowed you to multiply your

00:41:25,089 --> 00:41:29,680
productivity 104 you didn't have to

00:41:28,000 --> 00:41:31,750
write code in order to get any of that

00:41:29,680 --> 00:41:37,259
stuff done you could essentially live

00:41:31,750 --> 00:41:39,670
through these these useful helpers so

00:41:37,259 --> 00:41:41,829
given that and I've been talking about

00:41:39,670 --> 00:41:43,720
this for a while I like to ask what do

00:41:41,829 --> 00:41:46,029
you guys think is the most successful

00:41:43,720 --> 00:41:48,099
parallel programming language what

00:41:46,029 --> 00:41:50,049
computer language that's used heavily in

00:41:48,099 --> 00:41:52,029
productions for decades essentially

00:41:50,049 --> 00:41:54,160
allowed to put developers to know

00:41:52,029 --> 00:41:55,359
nothing about threads locking or message

00:41:54,160 --> 00:41:57,460
passing or anything like that

00:41:55,359 --> 00:42:00,309
to keep a very large parallel system

00:41:57,460 --> 00:42:02,220
busy anybody have any thoughts there

00:42:00,309 --> 00:42:07,720
yeah go ahead

00:42:02,220 --> 00:42:10,619
Anna law airline thank you no yes over

00:42:07,720 --> 00:42:13,690
there SQL thank you

00:42:10,619 --> 00:42:15,670
SQL is actually one of the most amazing

00:42:13,690 --> 00:42:17,559
the most amazing languages that we have

00:42:15,670 --> 00:42:20,410
it's essentially one that gives you the

00:42:17,559 --> 00:42:22,420
high productivity at the end of the day

00:42:20,410 --> 00:42:25,150
it's all about either the language gives

00:42:22,420 --> 00:42:26,860
you gives enable a large percentage of

00:42:25,150 --> 00:42:29,290
people to be able to use it

00:42:26,860 --> 00:42:31,300
in a very productive way or itself

00:42:29,290 --> 00:42:33,700
enable a tremendous growth in

00:42:31,300 --> 00:42:35,100
productivity generally that the second

00:42:33,700 --> 00:42:38,170
part is much more difficult to achieve

00:42:35,100 --> 00:42:55,360
so with that I would like to pass this

00:42:38,170 --> 00:43:03,100
on to Paul McKinney now we'll see if the

00:42:55,360 --> 00:43:06,660
adapters work so I'm going to take it

00:43:03,100 --> 00:43:09,010
down a level a bit we're gonna go from

00:43:06,660 --> 00:43:11,260
prairie models and parallelism into a

00:43:09,010 --> 00:43:13,180
little bit of hardware software view of

00:43:11,260 --> 00:43:15,220
hardware this is not a presentation to

00:43:13,180 --> 00:43:17,110
help you design your computer I'm sorry

00:43:15,220 --> 00:43:18,640
if you came here for that I think

00:43:17,110 --> 00:43:20,620
there's other conferences that are into

00:43:18,640 --> 00:43:21,910
that sort of thing risk five people and

00:43:20,620 --> 00:43:24,700
some other people doing the open source

00:43:21,910 --> 00:43:26,800
hardware of course my employer likes to

00:43:24,700 --> 00:43:27,970
talk about their hardware but let's get

00:43:26,800 --> 00:43:34,090
on with this if I remember a password

00:43:27,970 --> 00:43:38,290
now of course success okay now let's see

00:43:34,090 --> 00:43:40,090
if we can convince this Hardware to be

00:43:38,290 --> 00:43:41,440
productive and see if I can find the

00:43:40,090 --> 00:43:44,460
right place to plug this in would be

00:43:41,440 --> 00:43:50,710
another step in the right direction

00:43:44,460 --> 00:43:58,570
there we go and ah okay something

00:43:50,710 --> 00:44:01,420
happened and there we go now let's try

00:43:58,570 --> 00:44:04,360
moving this over here and let's try

00:44:01,420 --> 00:44:06,070
doing that and it almost looks like

00:44:04,360 --> 00:44:12,880
something worked you know that's

00:44:06,070 --> 00:44:13,990
productivity sort of anyway okay so I'll

00:44:12,880 --> 00:44:16,600
be talking a little bit of hardware's

00:44:13,990 --> 00:44:19,320
have us again some my website and a

00:44:16,600 --> 00:44:21,790
place to find more information on this

00:44:19,320 --> 00:44:23,050
I'm gonna kind of riff on one of the

00:44:21,790 --> 00:44:24,640
things Michael did I'm gonna say that

00:44:23,050 --> 00:44:26,950
premature abstractions the real bowl

00:44:24,640 --> 00:44:28,680
evil now when I grew up was growing up

00:44:26,950 --> 00:44:32,110
when I was in my 20s and 30s it was

00:44:28,680 --> 00:44:33,880
premature optimization and in the case

00:44:32,110 --> 00:44:36,370
that case we had these really old

00:44:33,880 --> 00:44:39,190
machines machine I used as a freshman in

00:44:36,370 --> 00:44:40,300
college had a memory access time of 1.6

00:44:39,190 --> 00:44:43,120
microseconds

00:44:40,300 --> 00:44:44,620
the clock was something like what 600

00:44:43,120 --> 00:44:46,840
kilohertz or something like that all

00:44:44,620 --> 00:44:48,670
right and if you wanted the machine to

00:44:46,840 --> 00:44:50,050
do anything you optimized first and ask

00:44:48,670 --> 00:44:52,300
questions later and usually got in

00:44:50,050 --> 00:44:54,520
trouble doing that which is why Don

00:44:52,300 --> 00:44:57,880
Knuth came up with that phrase that we

00:44:54,520 --> 00:44:59,650
all have heard but this is quite a bit

00:44:57,880 --> 00:45:01,000
later we have machines with much more

00:44:59,650 --> 00:45:03,010
power that to instead of being

00:45:01,000 --> 00:45:04,270
refrigerator size like that one was but

00:45:03,010 --> 00:45:06,790
you know there's one thing we've lost

00:45:04,270 --> 00:45:10,660
and I think this is something that is

00:45:06,790 --> 00:45:12,460
really important the machine I was

00:45:10,660 --> 00:45:15,310
talking about the line I used that had

00:45:12,460 --> 00:45:18,730
the 1.6 microsecond access time had

00:45:15,310 --> 00:45:20,890
little ferrite cores this big the

00:45:18,730 --> 00:45:24,250
question I have for all of you is why

00:45:20,890 --> 00:45:27,790
should you trust bits of memory that you

00:45:24,250 --> 00:45:29,200
can't see with your own naked eye okay I

00:45:27,790 --> 00:45:30,490
mean you really got to ask yourself

00:45:29,200 --> 00:45:32,740
about that that's one of things we've

00:45:30,490 --> 00:45:34,870
lost although having you know multiple

00:45:32,740 --> 00:45:36,280
gigabytes of it 16 of them are on this

00:45:34,870 --> 00:45:37,990
machine is kind of handy I'll admit that

00:45:36,280 --> 00:45:40,810
so you know there's something that

00:45:37,990 --> 00:45:42,100
gained as well so one of things about

00:45:40,810 --> 00:45:43,600
premature in abstractions there's some

00:45:42,100 --> 00:45:47,290
things that are difficult to abstract

00:45:43,600 --> 00:45:50,950
away all right it's hard to abstract

00:45:47,290 --> 00:45:52,480
away the finite speed of light there

00:45:50,950 --> 00:45:53,860
there was this thing about neutrinos a

00:45:52,480 --> 00:45:56,080
while back but that turned out to be

00:45:53,860 --> 00:45:57,730
kind of a false alarm as far as we know

00:45:56,080 --> 00:45:59,860
we can't make the data go any faster

00:45:57,730 --> 00:46:03,490
than the speed of light and we'll see

00:45:59,860 --> 00:46:05,410
there's other limitations as well so you

00:46:03,490 --> 00:46:07,630
can abstract all you want but the laws

00:46:05,410 --> 00:46:08,980
of physics are a little unforgiving now

00:46:07,630 --> 00:46:11,410
one of the things we've done really well

00:46:08,980 --> 00:46:13,210
one of the reasons that old machine I

00:46:11,410 --> 00:46:17,740
used as a freshman in college was so

00:46:13,210 --> 00:46:19,060
horribly slow was it was big I mean if

00:46:17,740 --> 00:46:20,980
you think that it was bad that the

00:46:19,060 --> 00:46:23,020
little main memory were a little course

00:46:20,980 --> 00:46:25,390
about this big around the registers were

00:46:23,020 --> 00:46:28,060
about this big and they only had 12 bits

00:46:25,390 --> 00:46:29,260
in them and so it took some serious time

00:46:28,060 --> 00:46:31,540
to get stuff from one end of that

00:46:29,260 --> 00:46:32,800
machine to the other if you know being

00:46:31,540 --> 00:46:35,350
like this

00:46:32,800 --> 00:46:36,880
this guy's got a CPU it's got 16 cores

00:46:35,350 --> 00:46:39,330
excusing the 8 cores like I'm not

00:46:36,880 --> 00:46:41,680
getting maybe loudly an exponent get a

00:46:39,330 --> 00:46:44,800
16 Hardware threads excuse me and the

00:46:41,680 --> 00:46:46,630
chip is only about this big and so as a

00:46:44,800 --> 00:46:48,280
result this thing can run at gigahertz

00:46:46,630 --> 00:46:50,890
where the old thing ran at hundreds of

00:46:48,280 --> 00:46:52,390
kilohertz and that's that's wonderful

00:46:50,890 --> 00:46:53,590
that's something we've done we shrunk

00:46:52,390 --> 00:46:55,150
these down

00:46:53,590 --> 00:46:59,620
so that the speed of light is less of a

00:46:55,150 --> 00:47:01,720
problem unfortunately there's other laws

00:46:59,620 --> 00:47:06,370
of physics we get in the way that was

00:47:01,720 --> 00:47:08,830
exciting anybody want some water and the

00:47:06,370 --> 00:47:12,850
thing is is that atoms are uncomfortably

00:47:08,830 --> 00:47:14,980
big therein keenly large twelve years

00:47:12,850 --> 00:47:17,350
ago I saw a scanning electron microscope

00:47:14,980 --> 00:47:18,700
of a transistor and the base that's the

00:47:17,350 --> 00:47:20,650
part in the middle and the thinner of

00:47:18,700 --> 00:47:23,080
the base electrically thinner which is

00:47:20,650 --> 00:47:25,120
think of it just as thin right now is

00:47:23,080 --> 00:47:27,760
what controls how fast the transition to

00:47:25,120 --> 00:47:30,250
switch the thinner that base the faster

00:47:27,760 --> 00:47:32,500
the switching well twelve years ago they

00:47:30,250 --> 00:47:34,030
had transistors in production that had

00:47:32,500 --> 00:47:37,090
that we're about this many atoms across

00:47:34,030 --> 00:47:38,710
four or five atoms across the base they

00:47:37,090 --> 00:47:40,690
have made research transistors that look

00:47:38,710 --> 00:47:44,140
like that where they have one layer of

00:47:40,690 --> 00:47:45,970
atoms for the base now it is possible to

00:47:44,140 --> 00:47:48,940
split atoms we've done that for a long

00:47:45,970 --> 00:47:51,160
time but doing that kind of spoils them

00:47:48,940 --> 00:47:54,880
for electronics use in our experience

00:47:51,160 --> 00:47:56,080
and for much else insides now there are

00:47:54,880 --> 00:47:58,270
some other tricks that people are

00:47:56,080 --> 00:48:00,790
pulling I mean one trick is just not to

00:47:58,270 --> 00:48:02,290
have any atoms in the base and believe

00:48:00,790 --> 00:48:04,300
it or not there actually has been a

00:48:02,290 --> 00:48:06,190
research transistor constructed that

00:48:04,300 --> 00:48:07,960
does that you just have the source and

00:48:06,190 --> 00:48:09,810
the base with a gap between them it's

00:48:07,960 --> 00:48:12,520
actually a solid-state vacuum tube I

00:48:09,810 --> 00:48:15,310
mean it's really cool the atmosphere at

00:48:12,520 --> 00:48:16,780
those scales is so thin that it's for

00:48:15,310 --> 00:48:18,760
all intents and purposes of vacuum and

00:48:16,780 --> 00:48:21,250
so you make vacuum tubes out of

00:48:18,760 --> 00:48:22,870
semiconductors except the way they did

00:48:21,250 --> 00:48:26,620
that was it made a huge pile of one less

00:48:22,870 --> 00:48:27,940
chip and a few of them worked which is a

00:48:26,620 --> 00:48:29,560
wonderful there's a really cool

00:48:27,940 --> 00:48:31,300
demonstration of capability I mean Oh

00:48:29,560 --> 00:48:33,940
get me wrong but we need to have

00:48:31,300 --> 00:48:35,830
billions of on that ship all working and

00:48:33,940 --> 00:48:37,540
you kind of have to plan place the atoms

00:48:35,830 --> 00:48:38,860
and well maybe we'll get there sometimes

00:48:37,540 --> 00:48:41,190
we aren't there right now so for right

00:48:38,860 --> 00:48:45,600
now we're kind of stuck with this and

00:48:41,190 --> 00:48:48,130
that's kind of a problem so here we are

00:48:45,600 --> 00:48:50,130
this is kind of a cartoony picture of

00:48:48,130 --> 00:48:54,160
what a system might look like

00:48:50,130 --> 00:48:55,330
we've got sockets with CPUs on them they

00:48:54,160 --> 00:48:56,260
could be multi-threaded I didn't show

00:48:55,330 --> 00:48:57,490
that here there's some kind of

00:48:56,260 --> 00:48:59,200
interconnect there's memory the memory

00:48:57,490 --> 00:49:00,430
might be associated with the sockets or

00:48:59,200 --> 00:49:03,040
might be off on the side let me show

00:49:00,430 --> 00:49:07,300
here there's a little bit of variety the

00:49:03,040 --> 00:49:07,900
problem is at two gigahertz the speed of

00:49:07,300 --> 00:49:10,720
light going

00:49:07,900 --> 00:49:12,490
and coming back it's about that far okay

00:49:10,720 --> 00:49:14,230
last not so bad I mean you know look at

00:49:12,490 --> 00:49:15,880
my laptop you know that's most a laptop

00:49:14,230 --> 00:49:18,609
right and if you put all the stuff in

00:49:15,880 --> 00:49:21,309
the middle shouldn't be a problem except

00:49:18,609 --> 00:49:24,819
that we haven't used vacuums or

00:49:21,309 --> 00:49:25,869
computers since I was very small and I'm

00:49:24,819 --> 00:49:27,490
one of the guys Michael was talking

00:49:25,869 --> 00:49:30,940
about with a gray hair all right and I

00:49:27,490 --> 00:49:34,359
do have this honestly and those vacuum

00:49:30,940 --> 00:49:36,279
tube computers are pretty slow and and

00:49:34,359 --> 00:49:38,049
the electrons in these things are not

00:49:36,279 --> 00:49:39,130
going through vacuum I'm sorry they're

00:49:38,049 --> 00:49:40,029
going through silicon they're going

00:49:39,130 --> 00:49:42,250
through copper they're going through

00:49:40,029 --> 00:49:44,020
luminometer and if you're going through

00:49:42,250 --> 00:49:45,849
a conductor like copper aluminum you

00:49:44,020 --> 00:49:48,220
might get a third of the speed of light

00:49:45,849 --> 00:49:50,470
that's a lot that bad about like this

00:49:48,220 --> 00:49:52,210
that's as big of the chip right except

00:49:50,470 --> 00:49:53,500
if you're inside a transistor you're

00:49:52,210 --> 00:49:57,220
about three percent the speed of light

00:49:53,500 --> 00:49:59,160
and so you're taking forever in terms of

00:49:57,220 --> 00:50:02,319
clock rate to get across your chip and

00:49:59,160 --> 00:50:04,299
that's just the physics once you get

00:50:02,319 --> 00:50:06,309
past the physics you got mathematics yet

00:50:04,299 --> 00:50:07,720
got to have protocols to make sure the

00:50:06,309 --> 00:50:09,190
data gets understood correctly if the

00:50:07,720 --> 00:50:11,049
other side and responded to and change

00:50:09,190 --> 00:50:15,730
the state correctly and that adds extra

00:50:11,049 --> 00:50:17,559
layers of overhead so you know the

00:50:15,730 --> 00:50:18,819
hardware guy is uh you know huh it may

00:50:17,559 --> 00:50:22,029
be very inconvenient what they're doing

00:50:18,819 --> 00:50:23,529
to us but you know the laws of physics

00:50:22,029 --> 00:50:26,349
are being much more nasty to them than

00:50:23,529 --> 00:50:28,119
they're being to us so maybe we need to

00:50:26,349 --> 00:50:30,640
give some sympathy and maybe seeping

00:50:28,119 --> 00:50:32,619
help so let's just take a look at what

00:50:30,640 --> 00:50:33,760
the effects this has a kind of cartoony

00:50:32,619 --> 00:50:35,200
thing you know this is kind of the

00:50:33,760 --> 00:50:36,400
marketing message for CPU clock

00:50:35,200 --> 00:50:37,869
frequency cranked it up and we'll go

00:50:36,400 --> 00:50:39,819
faster you know just run the race we'll

00:50:37,869 --> 00:50:42,480
get there the faster CPU wins bigger

00:50:39,819 --> 00:50:45,520
clock rate everything is wonderful and

00:50:42,480 --> 00:50:47,670
until one of them just submits the

00:50:45,520 --> 00:50:51,400
particular and

00:50:47,670 --> 00:50:52,539
problem is you know you think the way

00:50:51,400 --> 00:50:54,130
you think about it what you'd hope it

00:50:52,539 --> 00:50:55,569
working it got something running at two

00:50:54,130 --> 00:50:58,359
gigahertz or even up to five gigahertz

00:50:55,569 --> 00:50:59,589
or liquid nitrogen on these these guys

00:50:58,359 --> 00:51:01,059
doing the overclocking eat up the light

00:50:59,589 --> 00:51:02,260
I think after seven gigahertz a while

00:51:01,059 --> 00:51:04,900
back maybe they got higher I don't know

00:51:02,260 --> 00:51:06,549
haven't tracked it for a while yeah you

00:51:04,900 --> 00:51:07,750
think you know that's that's really cool

00:51:06,549 --> 00:51:09,130
again instructions going along we

00:51:07,750 --> 00:51:12,160
getting you know with two gigahertz we

00:51:09,130 --> 00:51:13,569
two instructions per nanosecond more

00:51:12,160 --> 00:51:15,369
than that if we're superscalar one life

00:51:13,569 --> 00:51:17,829
is wonderful but that's not really the

00:51:15,369 --> 00:51:21,250
way they work it's kind of like there's

00:51:17,829 --> 00:51:22,570
this this this ocean or pool of stuff

00:51:21,250 --> 00:51:24,160
and the instructions kind of filter in

00:51:22,570 --> 00:51:27,190
and sometime leader come out the other

00:51:24,160 --> 00:51:29,950
side it's all very parallel inside the

00:51:27,190 --> 00:51:31,390
chip as well and so the thing is you've

00:51:29,950 --> 00:51:32,800
got a lot of branches in the program and

00:51:31,390 --> 00:51:34,210
this hardware guys really hate the fact

00:51:32,800 --> 00:51:35,860
we do that we have branches all over the

00:51:34,210 --> 00:51:37,090
place they really like to have straight

00:51:35,860 --> 00:51:38,770
line code so they can just go charging

00:51:37,090 --> 00:51:40,900
through it governance throw it in there

00:51:38,770 --> 00:51:43,270
a up and some group and get it done

00:51:40,900 --> 00:51:45,550
but we got these branches and so they

00:51:43,270 --> 00:51:47,200
have to predict them and they're

00:51:45,550 --> 00:51:49,300
actually rad predictions are pretty good

00:51:47,200 --> 00:51:50,320
in general but sometimes they get it

00:51:49,300 --> 00:51:51,490
wrong and when they do you've got a

00:51:50,320 --> 00:51:53,500
bunch of work you did soon you're gonna

00:51:51,490 --> 00:51:55,270
go one way and then software just wax

00:51:53,500 --> 00:51:57,180
your head and goes the other way and you

00:51:55,270 --> 00:52:00,340
gotta throw it all away and start over

00:51:57,180 --> 00:52:01,900
this isn't that bad though really let's

00:52:00,340 --> 00:52:07,840
go on some other things a memory

00:52:01,900 --> 00:52:10,150
reference we've got these CPUs running a

00:52:07,840 --> 00:52:12,300
gigahertz but it still takes many tens

00:52:10,150 --> 00:52:14,590
of nanoseconds to get out to main memory

00:52:12,300 --> 00:52:16,870
now used to be hundreds of nanoseconds

00:52:14,590 --> 00:52:18,430
not that long ago it's gotten better but

00:52:16,870 --> 00:52:20,680
it hasn't gotten better as fast as the

00:52:18,430 --> 00:52:22,240
CPU rates have and they're still kind of

00:52:20,680 --> 00:52:24,940
catching up and they aren't catching up

00:52:22,240 --> 00:52:26,410
very quickly so if you have to do a real

00:52:24,940 --> 00:52:29,140
memory reference going to real memory

00:52:26,410 --> 00:52:31,560
life is hard you're gonna it's gonna

00:52:29,140 --> 00:52:31,560
take some time

00:52:31,980 --> 00:52:36,640
atomic instructions read mine what I'm

00:52:34,090 --> 00:52:40,890
this I want to be very clear this is not

00:52:36,640 --> 00:52:43,240
necessarily a C++ 11 atomic operation

00:52:40,890 --> 00:52:45,370
because those miner might not involve

00:52:43,240 --> 00:52:47,950
read-modify-write atomic instruction

00:52:45,370 --> 00:52:49,450
okay where you're looking at some piece

00:52:47,950 --> 00:52:53,080
of data atomically changing it and

00:52:49,450 --> 00:52:54,960
putting the result back and there's a

00:52:53,080 --> 00:52:57,220
lot of constraints on these things

00:52:54,960 --> 00:52:59,740
there's a bunch of optimization as I'll

00:52:57,220 --> 00:53:01,690
cover later that do not work well with

00:52:59,740 --> 00:53:05,170
atomic operations because they have to

00:53:01,690 --> 00:53:06,820
be atomic it is not nice for other CPUs

00:53:05,170 --> 00:53:09,610
to see atomic operation halfway through

00:53:06,820 --> 00:53:11,590
that would not be atomic and so there's

00:53:09,610 --> 00:53:13,120
extra overhead associate ease which has

00:53:11,590 --> 00:53:14,590
by the way gotten much better over the

00:53:13,120 --> 00:53:16,390
decades these things just to be really

00:53:14,590 --> 00:53:19,350
painful Pentium 4 was just a nightmare

00:53:16,390 --> 00:53:24,190
ok but they've gotten better but still

00:53:19,350 --> 00:53:27,700
they cause trouble memory barriers these

00:53:24,190 --> 00:53:29,380
are fences and they're needed to restore

00:53:27,700 --> 00:53:30,670
ordering we'll get into why you need to

00:53:29,380 --> 00:53:31,930
remove why would you ever do anything

00:53:30,670 --> 00:53:33,490
out of order in first place well we've

00:53:31,930 --> 00:53:34,390
talked a little bit about it with the C

00:53:33,490 --> 00:53:36,400
of instructions like

00:53:34,390 --> 00:53:37,869
get executed in parallel inside the chip

00:53:36,400 --> 00:53:40,089
that means things will get out of order

00:53:37,869 --> 00:53:41,859
we'll see some of the reasons later if

00:53:40,089 --> 00:53:43,359
you put a memory barrier saying hey the

00:53:41,859 --> 00:53:44,680
stuff before has to happened first and

00:53:43,359 --> 00:53:47,559
then the stuff later just get it right

00:53:44,680 --> 00:53:48,099
and suddenly the CPU has to do something

00:53:47,559 --> 00:53:49,510
about that

00:53:48,099 --> 00:53:51,269
now the harbor designers have gotten

00:53:49,510 --> 00:53:53,349
really really really clever about

00:53:51,269 --> 00:53:56,140
cheating on this and we'll talk about

00:53:53,349 --> 00:53:58,000
that later - which is mostly to the good

00:53:56,140 --> 00:53:59,230
the thing is though that these things

00:53:58,000 --> 00:54:02,980
aren't created equal and I'm gonna

00:53:59,230 --> 00:54:04,930
switch to C++ eleven terminology here so

00:54:02,980 --> 00:54:06,579
clinical consistency barrier or an

00:54:04,930 --> 00:54:09,130
atomic operation based on it is gonna be

00:54:06,579 --> 00:54:11,019
expensive acquire release and acquire

00:54:09,130 --> 00:54:13,720
acquire release combination or be

00:54:11,019 --> 00:54:15,220
cheaper consume if it was implemented

00:54:13,720 --> 00:54:16,690
the way I'd like it to maybe we'll get

00:54:15,220 --> 00:54:18,700
there someday would be cheaper yet and

00:54:16,690 --> 00:54:21,010
relaxed of course is on most platforms

00:54:18,700 --> 00:54:24,579
almost free now this is kind of cartoony

00:54:21,010 --> 00:54:26,559
again if you're talking x86 relax

00:54:24,579 --> 00:54:28,150
consume and acquire in theory anyway

00:54:26,559 --> 00:54:31,029
would you all be essentially just the

00:54:28,150 --> 00:54:37,900
instruction almost zero cost over the

00:54:31,029 --> 00:54:39,940
ordering but something like a arm or a

00:54:37,900 --> 00:54:42,339
MIPS or power bc would have lightweight

00:54:39,940 --> 00:54:44,079
barriers that can do the acquire release

00:54:42,339 --> 00:54:46,089
and the consume and the consume except

00:54:44,079 --> 00:54:47,650
on alpha is just instructions again but

00:54:46,089 --> 00:54:50,019
this gives you a kind of a rough idea

00:54:47,650 --> 00:54:54,880
across CPU families have kind of sort of

00:54:50,019 --> 00:54:57,549
what to expect okay yeah now if I hit

00:54:54,880 --> 00:54:59,259
the right button the worst a much worse

00:54:57,549 --> 00:55:01,900
one these other ones were bad

00:54:59,259 --> 00:55:05,109
this one's much worse the reason that

00:55:01,900 --> 00:55:07,900
the CPUs can move along reasonably well

00:55:05,109 --> 00:55:10,119
is that they have caches and can keep

00:55:07,900 --> 00:55:11,259
the data close as we said earlier if you

00:55:10,119 --> 00:55:13,150
have to actually go to memory that's

00:55:11,259 --> 00:55:15,490
hard if you keep in the crashes that's

00:55:13,150 --> 00:55:16,779
fine but if the tittie you need isn't in

00:55:15,490 --> 00:55:18,549
the cache it's either in memory in

00:55:16,779 --> 00:55:20,440
somebody else's cash well when you the

00:55:18,549 --> 00:55:22,660
one CPU in front has memory as cache is

00:55:20,440 --> 00:55:24,190
doing happy and is snarking off at the

00:55:22,660 --> 00:55:28,740
cpu that's got to pay the cost of the

00:55:24,190 --> 00:55:35,200
cache miss and if you think that's bad

00:55:28,740 --> 00:55:38,619
if you think that's bad try IO I mean

00:55:35,200 --> 00:55:40,630
you know a cache miss might consume a

00:55:38,619 --> 00:55:41,740
few hundred nanoseconds or maybe a small

00:55:40,630 --> 00:55:44,440
number of microseconds on a really

00:55:41,740 --> 00:55:46,240
really big machine if you're using

00:55:44,440 --> 00:55:47,710
rotating rust you know the standard

00:55:46,240 --> 00:55:49,540
spinning disks

00:55:47,710 --> 00:55:51,840
those things have been milliseconds you

00:55:49,540 --> 00:55:54,130
know several orders of magnitude worse

00:55:51,840 --> 00:55:55,450
SSD solid state disks make that a little

00:55:54,130 --> 00:55:57,610
bit better quite a bit better actually

00:55:55,450 --> 00:55:59,500
but even so if you're using networking

00:55:57,610 --> 00:56:00,580
you know if you're talking from here to

00:55:59,500 --> 00:56:02,740
I don't know

00:56:00,580 --> 00:56:04,240
Bangalore India oh I'm sorry but that's

00:56:02,740 --> 00:56:06,910
gonna cost you hurt milliseconds just

00:56:04,240 --> 00:56:08,950
like speed of light okay let alone going

00:56:06,910 --> 00:56:13,570
through glass and being amplified and

00:56:08,950 --> 00:56:16,270
switched and everything else so IO can

00:56:13,570 --> 00:56:17,620
be painful and there's a hole that's why

00:56:16,270 --> 00:56:19,060
there's this cottage industry and things

00:56:17,620 --> 00:56:21,610
like memcache D and other things like

00:56:19,060 --> 00:56:26,110
that to try to localize things and avoid

00:56:21,610 --> 00:56:27,460
the need for unnecessary i/o so I'm

00:56:26,110 --> 00:56:30,010
gonna do this slide and then I think

00:56:27,460 --> 00:56:31,630
it's gonna be time for break this let me

00:56:30,010 --> 00:56:33,130
just take a look and see if I'm yes so

00:56:31,630 --> 00:56:36,310
we're gonna do that so this is where

00:56:33,130 --> 00:56:38,320
rents table this just kind of goes

00:56:36,310 --> 00:56:41,680
through cost of operations on a fairly

00:56:38,320 --> 00:56:43,930
old but not that old system and the

00:56:41,680 --> 00:56:45,520
operations aren't exactly lined up with

00:56:43,930 --> 00:56:47,770
the previous things because it's

00:56:45,520 --> 00:56:49,150
difficult to separate out of Brac grant

00:56:47,770 --> 00:56:50,770
how much is due to brandsmart addiction

00:56:49,150 --> 00:56:54,040
how much you do that to the other so i

00:56:50,770 --> 00:56:56,110
basically looked at going across threads

00:56:54,040 --> 00:56:57,700
in a core across cores and across socket

00:56:56,110 --> 00:57:00,130
support for the most part and i also

00:56:57,700 --> 00:57:03,760
looked at caste versus locking for the

00:57:00,130 --> 00:57:08,470
simple cases so this particular system

00:57:03,760 --> 00:57:10,570
had a clock period of 400 picosecond 0.4

00:57:08,470 --> 00:57:13,690
nanoseconds and so we have the

00:57:10,570 --> 00:57:15,730
operations on the Left we have the cost

00:57:13,690 --> 00:57:17,710
and nanoseconds down the middle and the

00:57:15,730 --> 00:57:18,940
ratio to the clock period now there's

00:57:17,710 --> 00:57:20,500
the call number of clock period required

00:57:18,940 --> 00:57:22,420
to complete the opera's ation down the

00:57:20,500 --> 00:57:24,160
right and of course hardware's gonna

00:57:22,420 --> 00:57:26,170
vary you could take your hardware you

00:57:24,160 --> 00:57:26,980
get different numbers and somebody else

00:57:26,170 --> 00:57:29,170
take their hard we get different numbers

00:57:26,980 --> 00:57:33,460
that's fine this is more or less

00:57:29,170 --> 00:57:34,870
representative the key point if you're

00:57:33,460 --> 00:57:37,210
just doing normal instructions you

00:57:34,870 --> 00:57:38,440
should be able to crank them out at more

00:57:37,210 --> 00:57:41,050
than one per clock if you're doing well

00:57:38,440 --> 00:57:44,680
maybe one every second clock if you're

00:57:41,050 --> 00:57:46,510
not doing quite as well if you are doing

00:57:44,680 --> 00:57:48,250
complicated operations like compare and

00:57:46,510 --> 00:57:49,870
swap which is a read-modify-write atomic

00:57:48,250 --> 00:57:51,130
operation that we saw the CPU tripping

00:57:49,870 --> 00:57:53,080
over back there with the electronic

00:57:51,130 --> 00:57:54,790
crowd around his foot or a locking

00:57:53,080 --> 00:57:57,790
you're taking more than order magnitude

00:57:54,790 --> 00:57:59,830
hit for that that's costing you because

00:57:57,790 --> 00:58:01,750
the CPU is having to deal with that now

00:57:59,830 --> 00:58:03,070
this is a fairly old CPU newly

00:58:01,750 --> 00:58:06,100
cpu's might get you a little bit better

00:58:03,070 --> 00:58:08,620
but it's not going to be perfect cache

00:58:06,100 --> 00:58:10,360
misses if you're within a core one

00:58:08,620 --> 00:58:12,220
thread to the other it's not too bad

00:58:10,360 --> 00:58:14,140
still you're over a magnet or a mag too

00:58:12,220 --> 00:58:15,820
but not to avange dude if you're going

00:58:14,140 --> 00:58:17,800
for one core to another within the same

00:58:15,820 --> 00:58:21,520
piece of silicon you're almost tours of

00:58:17,800 --> 00:58:23,320
magnitude so just having the data flow

00:58:21,520 --> 00:58:25,120
from one CP to another one core to

00:58:23,320 --> 00:58:27,610
another within the same chip in this

00:58:25,120 --> 00:58:29,590
within the same socket is almost tourism

00:58:27,610 --> 00:58:32,800
management more expensive than a clock

00:58:29,590 --> 00:58:36,520
cycle and if you're going off cycle

00:58:32,800 --> 00:58:38,260
you're well over towards a magnitude so

00:58:36,520 --> 00:58:40,720
this really does matter

00:58:38,260 --> 00:58:43,480
it takes you've got to be careful if you

00:58:40,720 --> 00:58:44,740
spend a sh miss to go off socket you'd

00:58:43,480 --> 00:58:48,070
better be doing a bunch of other stuff

00:58:44,740 --> 00:58:49,780
to make up for that because just do the

00:58:48,070 --> 00:58:51,670
multiplication if every other

00:58:49,780 --> 00:58:54,490
instruction is a cache miss you're

00:58:51,670 --> 00:58:55,900
taking 200 times slowdown that means

00:58:54,490 --> 00:58:58,720
that you're parallel application needs

00:58:55,900 --> 00:59:00,760
200 CPUs to keep up with one CPU I don't

00:58:58,720 --> 00:59:02,470
know about you guys but the idea I had

00:59:00,760 --> 00:59:05,890
when adding more CPUs to make go faster

00:59:02,470 --> 00:59:07,810
not slower and so when we come back from

00:59:05,890 --> 00:59:10,000
break I think was break starts about now

00:59:07,810 --> 00:59:11,800
in 15 minutes if I got that right I'll

00:59:10,000 --> 00:59:12,460
take a little bit of a look at what we

00:59:11,800 --> 00:59:14,410
can do about that

00:59:12,460 --> 00:59:16,510
and after that Magid will take us

00:59:14,410 --> 00:59:17,740
through a given algorithm to show

00:59:16,510 --> 00:59:19,690
exactly how we did it for that

00:59:17,740 --> 00:59:21,730
particular algorithm thank you brush for

00:59:19,690 --> 00:59:23,080
your time attention enjoy break see you

00:59:21,730 --> 00:59:26,380
back here in 15 minutes

00:59:23,080 --> 00:59:26,380

YouTube URL: https://www.youtube.com/watch?v=YM8Xy6oKVQg


