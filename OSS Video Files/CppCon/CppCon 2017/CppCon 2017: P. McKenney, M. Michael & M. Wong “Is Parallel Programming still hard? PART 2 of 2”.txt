Title: CppCon 2017: P. McKenney, M. Michael & M. Wong “Is Parallel Programming still hard? PART 2 of 2”
Publication date: 2017-10-08
Playlist: CppCon 2017
Description: 
	http://CppCon.org
—
Presentation Slides, PDFs, Source Code and other presenter materials are available at: https://github.com/CppCon/CppCon2017
—
Most embedded devices are multicore, and we see concurrency becoming ubiquitous for machine learning, machine vision, and self-driving cars. Thus the age of concurrency is upon us, so whether you like it or not, concurrency is now just part of the job. It is therefore time to stop being concurrency cowards and start on the path towards producing high-quality high-performance highly scalable concurrent software artifacts. After all, there was a time when sequential programming was considered mind-crushingly hard: In fact, in the late 1970s, Paul attended a talk where none other than Edsger Dijkstra argued, and not without reason, that programmers could not be trusted to correctly code simple sequential loops. However, these long-past perilous programming pitfalls are now easily avoided with improved programming models, heuristics, and tools. We firmly believe that concurrent and parallel programming will make this same transition. This talk will help you do just that. 

Besides, after more than a decade since the end of the hardware "free lunch", why should parallel programming still be hard?
— 
Paul E. McKenney: IBM Linux Technology Center, Distinguished Engineer

Paul E. McKenney has been coding for almost four decades, more than half of that on parallel hardware, where his work has earned him a reputation among some as a flaming heretic. Over the past decade, Paul has been an IBM Distinguished Engineer at the IBM Linux Technology Center. Paul maintains the RCU implementation within the Linux kernel, where the variety of workloads present highly entertaining performance, scalability, real-time response, and energy-efficiency challenges. Prior to that, he worked on the DYNIX/ptx kernel at Sequent, and prior to that on packet-radio and Internet protocols (but long before it was polite to mention Internet at cocktail parties), system administration, business applications, and real-time systems. His hobbies include what passes for running at his age along with the usual house-wife-and-kids habit.

Maged Michael: Facebook, Engineer

Maged Michael is a software engineer at Facebook. He is the inventor of hazard pointers, lock-free malloc and several algorithms for concurrent data structures. His work is included in several IBM products where he was a Research Staff Member at the IBM T.J. Watson Research Center. He received a Ph.D. in computer science from the University of Rochester. He is an ACM Distinguished Scientist and an ACM Distinguished Speaker. He is an elected member of the Connecticut Academy of Science and Engineering. He received the 2014 ACM SIGPLAN Most Influential PLDI Paper Award for his paper on Scalable Lock-Free Dynamic Memory Allocation.


Michael Wong: Codeplay, VP Research & Development

Michael Wong is VP of R&D at Codeplay Software. He is a current Director and VP of ISOCPP , and a senior member of the C++ Standards Committee with more then 15 years of experience. 
He chairs the WG21 SG5 Transactional Memory and SG14 Games Development/Low Latency/Financials C++ groups and is the co-author of a number C++/OpenMP/Transactional memory features including generalized attributes, user-defined literals, inheriting constructors, weakly ordered memory models, and explicit conversion operators. He has published numerous research papers and is the author of a book on C++11. He has been in invited speaker and keynote at numerous conferences. 

He is currently the editor of SG1 Concurrency TS and SG5 Transactional Memory TS. He is also the Chair of the SYCL standard and all Programming Languages for Standards Council of Canada. 

Previously, he was CEO of OpenMP involved with taking OpenMP toward Accelerator support and the Technical Strategy Architect responsible for moving IBM's compilers to Clang/LLVM after leading IBM’s XL C++ compiler team. 
—
Videos Filmed & Edited by Bash Films: http://www.BashFilms.com
Captions: 
	00:00:00,030 --> 00:00:04,470
so we've talked a lot about laws of

00:00:02,610 --> 00:00:06,299
physics and how they're inconvenient for

00:00:04,470 --> 00:00:09,240
Hardware people and thus inconvenient

00:00:06,299 --> 00:00:11,580
for software people but you know one

00:00:09,240 --> 00:00:13,139
thing about laws almost all laws have

00:00:11,580 --> 00:00:15,960
loopholes okay

00:00:13,139 --> 00:00:17,430
and the laws of physics are no exception

00:00:15,960 --> 00:00:19,850
there are some loopholes we take

00:00:17,430 --> 00:00:23,250
advantage of in some cases unfortunately

00:00:19,850 --> 00:00:25,320
like loopholes and normal legal laws

00:00:23,250 --> 00:00:27,630
there are limits to these loopholes but

00:00:25,320 --> 00:00:28,859
still they are loopholes and we should

00:00:27,630 --> 00:00:30,689
take advantage of them full advantage

00:00:28,859 --> 00:00:33,660
from wherever we can so let's go through

00:00:30,689 --> 00:00:36,510
some of them one really cool one that

00:00:33,660 --> 00:00:39,059
I've taken shameless advantage of over a

00:00:36,510 --> 00:00:41,160
period of more than 25 years is the fact

00:00:39,059 --> 00:00:43,710
that read-only data is replicated in all

00:00:41,160 --> 00:00:46,500
the caches so if a whole pile of

00:00:43,710 --> 00:00:48,539
different CPUs read the same data that

00:00:46,500 --> 00:00:50,579
data gets pulled into all those CPUs

00:00:48,539 --> 00:00:53,610
caches and that means all the CPUs have

00:00:50,579 --> 00:00:55,289
high-speed access to that data and we

00:00:53,610 --> 00:00:56,730
show that here we've got the little

00:00:55,289 --> 00:00:59,129
black squares that are in that little

00:00:56,730 --> 00:01:01,920
green area the same variable just

00:00:59,129 --> 00:01:04,140
everywhere and so all the CPUs can get

00:01:01,920 --> 00:01:05,460
access to that and of course that makes

00:01:04,140 --> 00:01:07,799
think kind of strange especially if

00:01:05,460 --> 00:01:09,659
you're as old as I am I mean I I grew up

00:01:07,799 --> 00:01:11,970
in the time when we had this thing

00:01:09,659 --> 00:01:13,729
called UNIX is before a Linux I ran it

00:01:11,970 --> 00:01:17,130
on the one he's really fancy pdp-11 s

00:01:13,729 --> 00:01:20,130
the thing was you had to configure by

00:01:17,130 --> 00:01:23,100
hand your kernel to exactly what

00:01:20,130 --> 00:01:25,799
hardware you had exactly okay

00:01:23,100 --> 00:01:27,479
and that meant that if you didn't

00:01:25,799 --> 00:01:29,460
compile in the fact that a floppy drive

00:01:27,479 --> 00:01:30,840
you couldn't access your floppy drive

00:01:29,460 --> 00:01:33,869
and and maybe your system wouldn't boot

00:01:30,840 --> 00:01:36,420
even you had to compile into the kernel

00:01:33,869 --> 00:01:37,799
where the swap partition was and there

00:01:36,420 --> 00:01:39,119
were three in fact you had to put get

00:01:37,799 --> 00:01:40,229
three different places in the kernel

00:01:39,119 --> 00:01:42,180
correct as to where the swap partition

00:01:40,229 --> 00:01:45,780
was otherwise it would do interesting

00:01:42,180 --> 00:01:53,130
things I know that because I only found

00:01:45,780 --> 00:01:55,079
two of them once and and in fact if you

00:01:53,130 --> 00:01:56,369
forgot the mouth of floppy driving you

00:01:55,079 --> 00:01:58,950
tried to use a floppy drive that was a

00:01:56,369 --> 00:02:00,659
kernel panic which usually irritated

00:01:58,950 --> 00:02:03,180
everybody else who was trying to use the

00:02:00,659 --> 00:02:07,500
machine at the time I know that because

00:02:03,180 --> 00:02:09,599
I did that too well things have changed

00:02:07,500 --> 00:02:11,039
what happens now you got a laptop you

00:02:09,599 --> 00:02:13,620
know you slam a memory stick into it and

00:02:11,039 --> 00:02:13,930
no memory sticks here you know you can

00:02:13,620 --> 00:02:15,939
off

00:02:13,930 --> 00:02:18,819
CPUs of using Linux probably Windows - I

00:02:15,939 --> 00:02:20,439
don't know you can connect to the

00:02:18,819 --> 00:02:21,819
network or disconnect the network stuff

00:02:20,439 --> 00:02:24,189
could like appear and disappear and the

00:02:21,819 --> 00:02:25,390
system just takes just notice it you

00:02:24,189 --> 00:02:26,890
know some easing like you can do if you

00:02:25,390 --> 00:02:30,400
have more than 64 K of address space

00:02:26,890 --> 00:02:32,909
alright but what this means is there's a

00:02:30,400 --> 00:02:35,379
lot more data that almost never changes

00:02:32,909 --> 00:02:37,359
there's a bunch of data that keeps track

00:02:35,379 --> 00:02:39,040
of what hardware you have because the

00:02:37,359 --> 00:02:41,379
kernel is built just to take whatever

00:02:39,040 --> 00:02:43,299
shows up mostly and so it has to keep

00:02:41,379 --> 00:02:45,849
track of what do I really have versus

00:02:43,299 --> 00:02:47,200
what I might have and that could change

00:02:45,849 --> 00:02:48,669
at any time you could throw a memory

00:02:47,200 --> 00:02:52,090
stick into it or pull memory stick out

00:02:48,669 --> 00:02:55,689
anytime you wanted but you don't do that

00:02:52,090 --> 00:02:57,639
very often and so that data if you were

00:02:55,689 --> 00:02:59,650
really heavily using your i/o would tend

00:02:57,639 --> 00:03:01,719
to be replicated in the caches and your

00:02:59,650 --> 00:03:03,730
access to those data structures

00:03:01,719 --> 00:03:06,629
describing what the system had and

00:03:03,730 --> 00:03:08,859
didn't have would be quite fast so

00:03:06,629 --> 00:03:11,260
what's happened is that larger memory

00:03:08,859 --> 00:03:12,959
has changed the way that the machines

00:03:11,260 --> 00:03:15,969
actually operate there's much more data

00:03:12,959 --> 00:03:17,560
lying around in the system describing

00:03:15,969 --> 00:03:19,269
what's there whereas the old days that

00:03:17,560 --> 00:03:21,010
was all code that was either there or

00:03:19,269 --> 00:03:23,979
not compiled in you had to very

00:03:21,010 --> 00:03:25,690
carefully select what you had wifes much

00:03:23,979 --> 00:03:29,769
better this way than it was then believe

00:03:25,690 --> 00:03:31,209
me okay and in real estate of course the

00:03:29,769 --> 00:03:34,239
most three most important things are

00:03:31,209 --> 00:03:36,180
location location location and in

00:03:34,239 --> 00:03:39,970
parallel computing is very similar

00:03:36,180 --> 00:03:42,220
locality locality locality if you can

00:03:39,970 --> 00:03:44,139
make it so that you split your problem

00:03:42,220 --> 00:03:45,849
up nicely so that the data can be local

00:03:44,139 --> 00:03:47,769
to a given CPU a given sock at a given

00:03:45,849 --> 00:03:53,769
system your life is gonna be much better

00:03:47,769 --> 00:03:55,479
things are gonna ok one question is well

00:03:53,769 --> 00:03:57,009
jeez you know we all lose Hardware you

00:03:55,479 --> 00:04:00,669
all these transistors can't the hardware

00:03:57,009 --> 00:04:04,449
help us and in fact it does quite a bit

00:04:00,669 --> 00:04:08,079
and these are just five of the larger

00:04:04,449 --> 00:04:10,060
big animal optimizations it does you

00:04:08,079 --> 00:04:11,290
know big caches of course is a very

00:04:10,060 --> 00:04:13,389
popular thing you've seen these things

00:04:11,290 --> 00:04:15,900
grow from small numbers of kilobytes to

00:04:13,389 --> 00:04:18,400
many many tens of megabytes these days

00:04:15,900 --> 00:04:20,470
we'll talk about store buffers specula

00:04:18,400 --> 00:04:21,849
of execution big cache lines are kind of

00:04:20,470 --> 00:04:23,979
a two-edged sword in fact a lot of these

00:04:21,849 --> 00:04:26,229
a optimization in general it's kind of a

00:04:23,979 --> 00:04:27,490
two-edged sword and we'll see that some

00:04:26,229 --> 00:04:29,560
of these are - cache

00:04:27,490 --> 00:04:33,849
is another one that can be very good or

00:04:29,560 --> 00:04:36,190
very bad all right so the good thing

00:04:33,849 --> 00:04:38,860
about big cash lines so you got CPU zero

00:04:36,190 --> 00:04:40,240
he wants to read variable a well

00:04:38,860 --> 00:04:41,919
variable a happens to being a cache line

00:04:40,240 --> 00:04:42,729
down there in the bottom just got a B is

00:04:41,919 --> 00:04:44,979
C and D in it

00:04:42,729 --> 00:04:46,660
that's a CPU one right now because maybe

00:04:44,979 --> 00:04:49,599
CPU one was the last one to write to it

00:04:46,660 --> 00:04:52,060
so CPU zero says hey I want to read a

00:04:49,599 --> 00:04:53,740
give me the cache line and then that's

00:04:52,060 --> 00:04:55,330
the arrow coming down so that's why we

00:04:53,740 --> 00:04:57,550
remember we had over and back speed of

00:04:55,330 --> 00:04:58,900
light well that's the over part and then

00:04:57,550 --> 00:05:01,030
see if you one says oh yeah a cache line

00:04:58,900 --> 00:05:03,069
okay here it is and that's the back part

00:05:01,030 --> 00:05:05,680
so there's this big delay this long

00:05:03,069 --> 00:05:07,389
latency between the time the CPU zero

00:05:05,680 --> 00:05:10,000
says hey I want to read the value of a

00:05:07,389 --> 00:05:12,430
and the request goes out for the cache

00:05:10,000 --> 00:05:14,349
line containing a and that cache line

00:05:12,430 --> 00:05:16,990
comes back and now once it's back there

00:05:14,349 --> 00:05:18,880
CPU zero actually can read a the cool

00:05:16,990 --> 00:05:21,970
thing is is if CPU zero was next gonna

00:05:18,880 --> 00:05:23,560
read B C and D do it right now just

00:05:21,970 --> 00:05:25,599
that's right there you've got it

00:05:23,560 --> 00:05:27,669
almost no additional latency to get the

00:05:25,599 --> 00:05:29,050
rest of the data in that cache line so

00:05:27,669 --> 00:05:30,190
having a really big cache line would

00:05:29,050 --> 00:05:31,840
mean you could just get all sorts of

00:05:30,190 --> 00:05:33,969
data if you pay the price once that

00:05:31,840 --> 00:05:36,490
speedlights gonna get you once but once

00:05:33,969 --> 00:05:37,750
it's once you've paid that price you get

00:05:36,490 --> 00:05:40,030
your money's worth the rest of the data

00:05:37,750 --> 00:05:41,759
in the cache line well that's a good

00:05:40,030 --> 00:05:46,840
sign

00:05:41,759 --> 00:05:48,610
there's also a bad nug lee side now

00:05:46,840 --> 00:05:50,469
could be that we've got a b and c in

00:05:48,610 --> 00:05:54,460
there and cpu zero wants to write to a

00:05:50,469 --> 00:05:56,130
and cpu one wants to write to d well

00:05:54,460 --> 00:05:58,360
they're both on the same cache line and

00:05:56,130 --> 00:06:00,009
so that cache line is gonna painting

00:05:58,360 --> 00:06:02,319
back and forth between those two CPUs

00:06:00,009 --> 00:06:03,490
and this is called false sharing how

00:06:02,319 --> 00:06:05,199
many people have heard of false sharing

00:06:03,490 --> 00:06:06,639
or like run into it how many people have

00:06:05,199 --> 00:06:07,750
had to beat their head against the wall

00:06:06,639 --> 00:06:09,819
to figure out what the heck was being

00:06:07,750 --> 00:06:13,810
false shared and fix it yeah yeah you

00:06:09,819 --> 00:06:15,729
know what I'm talking about and the

00:06:13,810 --> 00:06:17,380
bigger you make your cache lines the

00:06:15,729 --> 00:06:19,719
more likely you end up with a situation

00:06:17,380 --> 00:06:21,039
where several CPUs are fighting over the

00:06:19,719 --> 00:06:24,250
same cache line because they want to

00:06:21,039 --> 00:06:26,229
modify it at the same time all right so

00:06:24,250 --> 00:06:27,969
it's it's an optimization it's the same

00:06:26,229 --> 00:06:29,710
as our software optimizations you know

00:06:27,969 --> 00:06:30,759
you use a hash table that's fast on the

00:06:29,710 --> 00:06:33,159
other hand you don't get to do range

00:06:30,759 --> 00:06:34,960
searches anymore it's the same kind of

00:06:33,159 --> 00:06:37,210
trade-off they're facing that we're

00:06:34,960 --> 00:06:37,479
facing and software all the time all

00:06:37,210 --> 00:06:39,940
right

00:06:37,479 --> 00:06:41,740
so big cache lines they can help thing

00:06:39,940 --> 00:06:43,669
her but they're getting larger

00:06:41,740 --> 00:06:45,470
alignment directives are what we do to

00:06:43,669 --> 00:06:48,710
get around that so what we could do here

00:06:45,470 --> 00:06:50,330
is we might use alignment directives to

00:06:48,710 --> 00:06:52,490
make sure that a and B are on one cache

00:06:50,330 --> 00:06:54,710
line is C in the air on another once

00:06:52,490 --> 00:06:56,270
we've done that well CP is zero still

00:06:54,710 --> 00:06:58,819
takes a long latency to get that cache

00:06:56,270 --> 00:07:00,800
line from CP 1 to begin with but once

00:06:58,819 --> 00:07:02,150
that's happened it can write to a all

00:07:00,800 --> 00:07:05,150
alikes it's just right there

00:07:02,150 --> 00:07:08,599
likewise CP 1 has C and D and it can do

00:07:05,150 --> 00:07:10,550
whatever wants there as well so the big

00:07:08,599 --> 00:07:14,090
way to get around cache false sharing

00:07:10,550 --> 00:07:16,490
ecz is the is alignment ok well that's

00:07:14,090 --> 00:07:19,250
really nice we've also got prefetching

00:07:16,490 --> 00:07:21,169
which could also be really good it was

00:07:19,250 --> 00:07:24,169
good when we picked up a and got B and C

00:07:21,169 --> 00:07:25,789
and D for free if we have prefetching we

00:07:24,169 --> 00:07:28,129
can still have the two cache lines and

00:07:25,789 --> 00:07:30,169
maybe have them with different CPUs but

00:07:28,129 --> 00:07:31,639
we can also if they're both of the same

00:07:30,169 --> 00:07:33,500
place and it's clear the other guys

00:07:31,639 --> 00:07:34,250
going sequentially we can send both of

00:07:33,500 --> 00:07:36,259
them at once

00:07:34,250 --> 00:07:37,759
we kind of cheat the speed of light with

00:07:36,259 --> 00:07:39,319
a second cache line because we

00:07:37,759 --> 00:07:42,800
anticipate that the other CPU is gonna

00:07:39,319 --> 00:07:44,689
need it and so then it reads a and then

00:07:42,800 --> 00:07:46,789
waits for a to show up and then it gets

00:07:44,689 --> 00:07:48,319
to read all a B through F like that

00:07:46,789 --> 00:07:51,919
because the prefetching happened and

00:07:48,319 --> 00:07:55,960
that's wonderful except that just got

00:07:51,919 --> 00:07:57,370
the same downside if the no matter how

00:07:55,960 --> 00:07:59,629
don't get me wrong

00:07:57,370 --> 00:08:02,870
hardware these days has really really

00:07:59,629 --> 00:08:04,479
clever tree fetching heuristics and they

00:08:02,870 --> 00:08:07,430
would generally avoid this sort of thing

00:08:04,479 --> 00:08:08,990
but you show me hard worth whatever

00:08:07,430 --> 00:08:11,089
heuristics and I'll show you software

00:08:08,990 --> 00:08:12,800
that will break those heuristics okay I

00:08:11,089 --> 00:08:14,419
mean that's that's that's the way of

00:08:12,800 --> 00:08:16,849
life you win by the heuristic you

00:08:14,419 --> 00:08:17,990
wouldn't lose by the heuristic and you

00:08:16,849 --> 00:08:19,580
can end up with a sort of thing where

00:08:17,990 --> 00:08:20,779
they fight for a little bit over the you

00:08:19,580 --> 00:08:21,889
know you prefetch it but you shouldn't

00:08:20,779 --> 00:08:23,360
have sent the second one over because

00:08:21,889 --> 00:08:25,009
he's not gonna go to it you need it back

00:08:23,360 --> 00:08:26,960
where you were to start from and you can

00:08:25,009 --> 00:08:27,589
take an extra latency because of that

00:08:26,960 --> 00:08:30,080
prefetching

00:08:27,589 --> 00:08:31,669
so it's an optimization again it can

00:08:30,080 --> 00:08:37,399
work very well in some cases it can hurt

00:08:31,669 --> 00:08:40,190
in others store buffers well one of the

00:08:37,399 --> 00:08:42,229
problems if you just did a store and you

00:08:40,190 --> 00:08:43,519
didn't have a store buffer you would try

00:08:42,229 --> 00:08:45,350
to the store you'd have to wait for the

00:08:43,519 --> 00:08:46,820
cache line to come to you in this case

00:08:45,350 --> 00:08:53,029
the cache lines of CPU won and would

00:08:46,820 --> 00:08:54,800
have to get to CPU zero and you had

00:08:53,029 --> 00:08:56,930
you'd have to wait that time

00:08:54,800 --> 00:08:58,580
what a store buffer allows you to do is

00:08:56,930 --> 00:09:00,019
say okay we're going to store this so

00:08:58,580 --> 00:09:01,700
we'll just record the address and the

00:09:00,019 --> 00:09:03,410
value in my little store buffer locally

00:09:01,700 --> 00:09:05,450
we'll ask for the cache line it'll get

00:09:03,410 --> 00:09:07,010
here sometime or another and when it

00:09:05,450 --> 00:09:09,380
gets here then we'll do the store and

00:09:07,010 --> 00:09:11,120
the cool thing about that is the CPU can

00:09:09,380 --> 00:09:13,790
proceed with this other operations while

00:09:11,120 --> 00:09:15,649
the store is pending and some machines

00:09:13,790 --> 00:09:17,300
have really big store buffers you know

00:09:15,649 --> 00:09:19,519
thousand any thousands of entries in

00:09:17,300 --> 00:09:21,950
fact on some machines they call it the

00:09:19,519 --> 00:09:24,200
level 0 cache it's really a giant store

00:09:21,950 --> 00:09:25,370
buffer ok depending on some machines is

00:09:24,200 --> 00:09:27,170
that way some it isn't but there are

00:09:25,370 --> 00:09:30,649
some machines or they call the level 0

00:09:27,170 --> 00:09:35,839
cache is really a huge store buffer and

00:09:30,649 --> 00:09:37,750
that's really nice but as you might

00:09:35,839 --> 00:09:44,120
expect from the previous several slides

00:09:37,750 --> 00:09:46,339
this is only half the story this is a

00:09:44,120 --> 00:09:48,290
big source of memory miss ordering and

00:09:46,339 --> 00:09:50,120
why we have to have these memory models

00:09:48,290 --> 00:09:53,839
and everything else because let's

00:09:50,120 --> 00:09:55,760
suppose the CPU 0 writes a 42 it gets

00:09:53,839 --> 00:09:56,959
stuck on the store buffer and then it

00:09:55,760 --> 00:09:59,899
goes and it does a bunch of stuff and

00:09:56,959 --> 00:10:00,829
reads a well CPU zero says oh yeah well

00:09:59,899 --> 00:10:06,050
I've got that in the store buffer right

00:10:00,829 --> 00:10:08,980
here it's 42 I just wrote it and so as

00:10:06,050 --> 00:10:14,180
far as its concerned it wrote 42 to a

00:10:08,980 --> 00:10:15,980
before it read from Seyi okay the other

00:10:14,180 --> 00:10:17,570
CPUs though they don't see that right

00:10:15,980 --> 00:10:19,760
until the cache line gets there and gets

00:10:17,570 --> 00:10:21,589
back to them so they're gonna have

00:10:19,760 --> 00:10:24,589
there's many disagreement about whether

00:10:21,589 --> 00:10:28,399
the store to a or the Reedy II happen

00:10:24,589 --> 00:10:31,070
first CPU zero knows it's stored a first

00:10:28,399 --> 00:10:32,480
and then it ready but there's no

00:10:31,070 --> 00:10:34,160
evidence of that store going to any

00:10:32,480 --> 00:10:36,740
other part of the system until much

00:10:34,160 --> 00:10:43,130
later and that means the other CPUs

00:10:36,740 --> 00:10:45,350
think that a happened after E ok and

00:10:43,130 --> 00:10:48,740
this is even things even the tightly

00:10:45,350 --> 00:10:51,380
couple systems like our excuse me type

00:10:48,740 --> 00:10:54,589
memory models like X a6 or the mainframe

00:10:51,380 --> 00:10:56,870
or spark or any other ones that are TSO

00:10:54,589 --> 00:10:58,130
they do this because they have store

00:10:56,870 --> 00:11:00,290
buffers because the store buffer

00:10:58,130 --> 00:11:02,540
optimization allowing the system to

00:11:00,290 --> 00:11:05,420
recede aligns CP to proceed with these

00:11:02,540 --> 00:11:07,660
pending stores is so valuable they can't

00:11:05,420 --> 00:11:10,420
do without it

00:11:07,660 --> 00:11:13,150
okay well what this means is we have to

00:11:10,420 --> 00:11:15,220
use ordering directives in C 11 of

00:11:13,150 --> 00:11:16,960
course these are the memory order you

00:11:15,220 --> 00:11:19,390
just get sequential with assistant by

00:11:16,960 --> 00:11:21,160
default and you have memory or require

00:11:19,390 --> 00:11:23,530
remember order release memory order act

00:11:21,160 --> 00:11:26,050
realm a river is consumed a minute order

00:11:23,530 --> 00:11:27,760
relaxed although can relax isn't gonna

00:11:26,050 --> 00:11:32,920
help you much here it's not gonna do

00:11:27,760 --> 00:11:34,750
much ordering for you okay and so what's

00:11:32,920 --> 00:11:36,610
happening this is one of the cool things

00:11:34,750 --> 00:11:38,530
about the c-plus with memory model is

00:11:36,610 --> 00:11:40,030
this allowing for the pretty much the

00:11:38,530 --> 00:11:42,280
first time I see you could argue job you

00:11:40,030 --> 00:11:43,690
got there first okay

00:11:42,280 --> 00:11:46,290
but it was only sequentially his

00:11:43,690 --> 00:11:49,690
assistant early on it added the weaker

00:11:46,290 --> 00:11:51,100
options after CLM and did the really

00:11:49,690 --> 00:11:52,900
cool thing about see 11 memory models

00:11:51,100 --> 00:11:54,940
allows portability for ordering and

00:11:52,900 --> 00:11:57,220
pretty good performance for it I mean

00:11:54,940 --> 00:11:58,210
it's you might be able to write tightly

00:11:57,220 --> 00:12:02,260
code as assembly and get a little bit

00:11:58,210 --> 00:12:04,510
better but in many situations just using

00:12:02,260 --> 00:12:08,530
the straight things I think a theater

00:12:04,510 --> 00:12:10,090
will be talking about that later but it

00:12:08,530 --> 00:12:11,530
allows us to do that what we have is we

00:12:10,090 --> 00:12:14,260
have the hardware which is often very

00:12:11,530 --> 00:12:17,470
different arm PowerPC

00:12:14,260 --> 00:12:19,360
like a neum mips x86 mainframe and so on

00:12:17,470 --> 00:12:22,240
or had very different hardware memory

00:12:19,360 --> 00:12:24,220
models we have a tool chain on top of

00:12:22,240 --> 00:12:28,300
them and that tool chain is responsible

00:12:24,220 --> 00:12:30,820
for providing the C 11 C 14 c-17 memory

00:12:28,300 --> 00:12:32,470
model and so it unifies provides a

00:12:30,820 --> 00:12:35,650
unified view of all that hardware and

00:12:32,470 --> 00:12:37,660
that means to do some of these ordering

00:12:35,650 --> 00:12:39,070
things in a portable manner and not lose

00:12:37,660 --> 00:12:43,110
that much performance in some cases

00:12:39,070 --> 00:12:45,220
maybe none maybe lose no performance

00:12:43,110 --> 00:12:46,900
okay so the last thing we'll talk about

00:12:45,220 --> 00:12:50,010
is big cache and speculative execution

00:12:46,900 --> 00:12:52,630
big caches actually work pretty well

00:12:50,010 --> 00:12:54,310
it's hard to find fault with them and

00:12:52,630 --> 00:12:55,780
left you happen to be one of these

00:12:54,310 --> 00:12:58,000
people that are using a battery-powered

00:12:55,780 --> 00:12:59,500
system in which case the bigger your

00:12:58,000 --> 00:13:03,450
cache the shorter your battery or

00:12:59,500 --> 00:13:05,380
lifetime up to a point but even so the

00:13:03,450 --> 00:13:06,970
people doing those things have gotten

00:13:05,380 --> 00:13:10,060
very clever about turning the caches off

00:13:06,970 --> 00:13:12,280
on the system's not being used but so

00:13:10,060 --> 00:13:13,870
it's it helps a lot you have the bigger

00:13:12,280 --> 00:13:16,150
the cache the more data fits in cache

00:13:13,870 --> 00:13:19,030
the closer it is to the CPU and life is

00:13:16,150 --> 00:13:21,240
better however if you have bigger caches

00:13:19,030 --> 00:13:23,370
usually you have to have a taller cache

00:13:21,240 --> 00:13:25,050
Archy the bigger the cash the more time

00:13:23,370 --> 00:13:26,790
it takes to figure out which element of

00:13:25,050 --> 00:13:30,540
the catch you want and so that's we have

00:13:26,790 --> 00:13:31,890
this l0 l1 l2 l3 sort of a thing and of

00:13:30,540 --> 00:13:33,570
course that means you have to check more

00:13:31,890 --> 00:13:35,220
levels of caches you're trying to figure

00:13:33,570 --> 00:13:36,839
out whether the thing is in a cache and

00:13:35,220 --> 00:13:39,120
your cache miss Leighton so you can get

00:13:36,839 --> 00:13:41,220
larger so it's an optimization there are

00:13:39,120 --> 00:13:46,050
trade-offs but it generally works pretty

00:13:41,220 --> 00:13:48,899
well a spectrum of execution is also it

00:13:46,050 --> 00:13:51,300
can hide Layton sees and we'll see an

00:13:48,899 --> 00:13:55,020
example here in a bit but the trick is

00:13:51,300 --> 00:13:56,550
that one thing you could do is every

00:13:55,020 --> 00:13:58,380
time you see some kind of ordering

00:13:56,550 --> 00:14:00,660
directive the cpg say okay I'm gonna

00:13:58,380 --> 00:14:02,190
wait until I until I know the stuff I

00:14:00,660 --> 00:14:04,920
did before is visible everywhere and

00:14:02,190 --> 00:14:06,570
only then will I go ahead well that

00:14:04,920 --> 00:14:09,240
might have been a successful strategy in

00:14:06,570 --> 00:14:12,620
1990 but if you're a Hardware player

00:14:09,240 --> 00:14:16,350
today that's probably not a winning play

00:14:12,620 --> 00:14:18,360
what they tend to do is go ahead and

00:14:16,350 --> 00:14:20,910
execute anyway and then if they see

00:14:18,360 --> 00:14:22,950
evidence that somebody saw or had the

00:14:20,910 --> 00:14:25,230
potential to see something backwards

00:14:22,950 --> 00:14:29,190
they cancel the speculation and start

00:14:25,230 --> 00:14:31,470
over more carefully now what that means

00:14:29,190 --> 00:14:33,300
is that in many cases you can hide the

00:14:31,470 --> 00:14:35,640
latency you would otherwise see from

00:14:33,300 --> 00:14:38,310
directives required to enforce memory

00:14:35,640 --> 00:14:40,980
ordering the downside is that you have

00:14:38,310 --> 00:14:42,810
worse worst case latency because you may

00:14:40,980 --> 00:14:45,540
have to do work cancel and do the work

00:14:42,810 --> 00:14:47,370
again possibly multiple times and also

00:14:45,540 --> 00:14:49,470
doing that work multiple times is going

00:14:47,370 --> 00:14:51,600
to give you poorer energy efficiency and

00:14:49,470 --> 00:14:53,339
so again the battery part people would

00:14:51,600 --> 00:14:57,209
be perhaps less aggressive about this

00:14:53,339 --> 00:15:00,180
sort of thing here's an example

00:14:57,209 --> 00:15:02,310
I hope the is this little door in the

00:15:00,180 --> 00:15:03,750
back there this is a standard

00:15:02,310 --> 00:15:06,029
message-passing type of a thing you've

00:15:03,750 --> 00:15:08,010
probably seen it before so we have a

00:15:06,029 --> 00:15:09,570
data and a flag we set the data to a

00:15:08,010 --> 00:15:12,779
value and we set the flag to one to say

00:15:09,570 --> 00:15:14,820
the data is available and we have memory

00:15:12,779 --> 00:15:16,620
order released there to say you know

00:15:14,820 --> 00:15:18,750
make sure that the flag happens after

00:15:16,620 --> 00:15:21,540
all the stuff before it on the other

00:15:18,750 --> 00:15:23,370
side we have a while loop you usually

00:15:21,540 --> 00:15:26,370
wouldn't spin waiting for the data but

00:15:23,370 --> 00:15:28,649
let's they get P bit simple so while the

00:15:26,370 --> 00:15:31,170
flag is not set we sit there waiting

00:15:28,649 --> 00:15:32,910
once the flag is set we pick up the data

00:15:31,170 --> 00:15:34,680
we use a memory order require for the

00:15:32,910 --> 00:15:37,170
flag load and therefore

00:15:34,680 --> 00:15:40,200
the data is fetched after the flag is

00:15:37,170 --> 00:15:42,810
red which means we get 42 we get the

00:15:40,200 --> 00:15:44,310
value stored in the data reliably if we

00:15:42,810 --> 00:15:46,350
lift those directives off we made a book

00:15:44,310 --> 00:15:47,640
through relaxed you can actually there

00:15:46,350 --> 00:15:49,080
are tool sets allow you to run that on

00:15:47,640 --> 00:15:51,870
the machine and see what happens on an

00:15:49,080 --> 00:15:54,930
x86 it won't work too badly on other

00:15:51,870 --> 00:15:57,060
systems you will see things get out of

00:15:54,930 --> 00:15:58,680
order and you'll see the flag being one

00:15:57,060 --> 00:16:00,630
and the data being the pre

00:15:58,680 --> 00:16:05,220
initialization garbage there was in it

00:16:00,630 --> 00:16:08,460
before thing is though if the system is

00:16:05,220 --> 00:16:10,680
aware if both the cache lines both flag

00:16:08,460 --> 00:16:12,660
and data are sitting on the first CPU

00:16:10,680 --> 00:16:14,400
and it knows that there and knows nobody

00:16:12,660 --> 00:16:16,830
else has access to them they can do

00:16:14,400 --> 00:16:18,570
those stores in any order once because

00:16:16,830 --> 00:16:22,800
nobody can see the intermediate state

00:16:18,570 --> 00:16:25,230
unless it lets them okay similarly on

00:16:22,800 --> 00:16:27,120
the other side if both the cache lines

00:16:25,230 --> 00:16:29,070
are sitting there and it knows that

00:16:27,120 --> 00:16:31,280
nobody else has access to them again it

00:16:29,070 --> 00:16:33,570
can do the reads in any order wants and

00:16:31,280 --> 00:16:39,360
because nobody else can be changing it

00:16:33,570 --> 00:16:42,240
because it's got the only copy so the

00:16:39,360 --> 00:16:44,250
hardware can you look into the cache

00:16:42,240 --> 00:16:45,840
clearance protocol the protocol used to

00:16:44,250 --> 00:16:48,240
keep the values constant to vevey the

00:16:45,840 --> 00:16:51,090
memory model across the machine and can

00:16:48,240 --> 00:16:53,010
determine when it's safe to cheat do

00:16:51,090 --> 00:16:54,840
things out of order and if it senses a

00:16:53,010 --> 00:16:56,370
condition that might allow the software

00:16:54,840 --> 00:16:59,670
see something bad it can cancel the

00:16:56,370 --> 00:17:00,870
speculation and start over a lot of

00:16:59,670 --> 00:17:02,190
people get kind of annoyed at me because

00:17:00,870 --> 00:17:03,660
I tell them the compiler can't do

00:17:02,190 --> 00:17:04,950
something without messing me up as a

00:17:03,660 --> 00:17:06,600
parallel programmer the old hardware

00:17:04,950 --> 00:17:08,730
does it well the reason the harbor gets

00:17:06,600 --> 00:17:10,320
away with it is because the hardware has

00:17:08,730 --> 00:17:12,570
a lot more visibility into the state of

00:17:10,320 --> 00:17:13,980
the machine and it can sense when

00:17:12,570 --> 00:17:16,560
something's going bad and can pull

00:17:13,980 --> 00:17:18,510
itself back out if you've got a compiler

00:17:16,560 --> 00:17:20,340
that can do that too okay great but

00:17:18,510 --> 00:17:23,060
until you do there are some things the

00:17:20,340 --> 00:17:24,570
hard work can get away with you can't

00:17:23,060 --> 00:17:26,550
okay

00:17:24,570 --> 00:17:29,940
we'll take a look at malign lurkers

00:17:26,550 --> 00:17:31,980
locking this is kind of leading some

00:17:29,940 --> 00:17:34,140
things just some simple things lead into

00:17:31,980 --> 00:17:35,790
maggots presentation a little bit this

00:17:34,140 --> 00:17:37,590
is a stupidly simple lock out position

00:17:35,790 --> 00:17:42,480
but it's one that has been used in

00:17:37,590 --> 00:17:45,510
production you just do a exchange with a

00:17:42,480 --> 00:17:46,680
and I apologize for the C but I managed

00:17:45,510 --> 00:17:48,460
to fix it on a previous slide we didn't

00:17:46,680 --> 00:17:50,860
notice this one

00:17:48,460 --> 00:17:55,179
I am after all C programmer work on the

00:17:50,860 --> 00:17:59,830
lathe colonel so we spend waiting for

00:17:55,179 --> 00:18:01,390
the value to be not to be 0 the basic

00:17:59,830 --> 00:18:03,460
ideas the lock word starts out being

00:18:01,390 --> 00:18:04,809
zero that means nobody holds it you're

00:18:03,460 --> 00:18:06,820
doing an exchange you take whatever is

00:18:04,809 --> 00:18:08,080
there and put a one there if there was

00:18:06,820 --> 00:18:09,460
already a one there you get a one back

00:18:08,080 --> 00:18:11,860
you know somebody already held it so you

00:18:09,460 --> 00:18:13,480
if you do it again if you get back a

00:18:11,860 --> 00:18:15,010
zero that means that nobody held the

00:18:13,480 --> 00:18:20,350
lock and now you hold it and they value

00:18:15,010 --> 00:18:21,880
is one the release is pretty

00:18:20,350 --> 00:18:23,710
straightforward you do an atomic store

00:18:21,880 --> 00:18:25,900
memory order release of the zero to set

00:18:23,710 --> 00:18:29,440
it back and this can implement a lock

00:18:25,900 --> 00:18:31,870
although I it has really really really

00:18:29,440 --> 00:18:34,000
terrible high contention behavior you

00:18:31,870 --> 00:18:36,370
know if you do this if you implement

00:18:34,000 --> 00:18:37,809
your lock this way it's on you to make

00:18:36,370 --> 00:18:39,070
sure you have almost no lock contention

00:18:37,809 --> 00:18:41,350
because if you do have lock attention

00:18:39,070 --> 00:18:42,910
it's gonna hurt but still this is

00:18:41,350 --> 00:18:44,890
something you can do we can make

00:18:42,910 --> 00:18:46,179
something a little smarter by instead of

00:18:44,890 --> 00:18:47,500
the problem was we were doing an

00:18:46,179 --> 00:18:48,669
exchange over and over and over again so

00:18:47,500 --> 00:18:51,669
we were just batting the cash lying

00:18:48,669 --> 00:18:53,650
around for all the CPUs and even so if

00:18:51,669 --> 00:18:54,880
some guy gets the lock and tries to

00:18:53,650 --> 00:18:57,910
release it it's probably somewhere else

00:18:54,880 --> 00:19:01,380
and it's hard what we can do is we can

00:18:57,910 --> 00:19:03,580
just do a while loop and just say just

00:19:01,380 --> 00:19:07,600
essentially spin waiting and only once

00:19:03,580 --> 00:19:09,370
we see that it's released then we try to

00:19:07,600 --> 00:19:13,299
do it I think I haven't got those files

00:19:09,370 --> 00:19:16,240
nested right but I'll fix that later so

00:19:13,299 --> 00:19:18,010
what we're doing is instead of doing a

00:19:16,240 --> 00:19:19,809
remodel I write on the cache line each

00:19:18,010 --> 00:19:22,299
time we're spinning waiting for the

00:19:19,809 --> 00:19:23,919
value to change and then once we see

00:19:22,299 --> 00:19:25,840
it's we have a chance of getting it then

00:19:23,919 --> 00:19:27,970
and only then do we do the atomic

00:19:25,840 --> 00:19:29,919
read-modify-write operation that Yanks

00:19:27,970 --> 00:19:32,110
it out of Rios's cache line and tries to

00:19:29,919 --> 00:19:34,630
acquire it this is still suboptimal

00:19:32,110 --> 00:19:38,950
though and an example of where it can go

00:19:34,630 --> 00:19:43,480
bad thing is is that you lock it but and

00:19:38,950 --> 00:19:47,290
you say see it's locked and CPU one sees

00:19:43,480 --> 00:19:50,860
that that it's that is held as well so

00:19:47,290 --> 00:19:52,419
see CPU 1 2 3 CPU sorry the thing is the

00:19:50,860 --> 00:19:54,280
CPU one knows it can't lock it but it's

00:19:52,419 --> 00:19:56,140
got a read copy of the cache line that

00:19:54,280 --> 00:19:57,970
means the CPU 0 can't release it until

00:19:56,140 --> 00:20:00,700
it gets exclusive ownership of the cache

00:19:57,970 --> 00:20:01,450
line because you don't want the same

00:20:00,700 --> 00:20:04,950
variable to have

00:20:01,450 --> 00:20:07,360
values into her partial machine usually

00:20:04,950 --> 00:20:08,799
not with atomic instructions be okay

00:20:07,360 --> 00:20:09,789
with stores they could have to store in

00:20:08,799 --> 00:20:10,809
different store buffers that'd be all

00:20:09,789 --> 00:20:12,429
right but this is atomic

00:20:10,809 --> 00:20:14,260
read-modify-write and it's supposed to

00:20:12,429 --> 00:20:18,460
look at Tomic so everybody has to agree

00:20:14,260 --> 00:20:19,899
on the value now what happens is that

00:20:18,460 --> 00:20:21,190
when somebody it does unlock there's a

00:20:19,899 --> 00:20:22,720
whole bunch of bus traffic that's to

00:20:21,190 --> 00:20:24,519
happen for the cache line get yanked out

00:20:22,720 --> 00:20:25,779
from the readers and gets sent to the

00:20:24,519 --> 00:20:30,519
guy that wants to release it so the

00:20:25,779 --> 00:20:36,549
readers can actually get it and one way

00:20:30,519 --> 00:20:38,950
to fix this is use cued locks and the

00:20:36,549 --> 00:20:40,059
idea is that you actually make a cue I'm

00:20:38,950 --> 00:20:41,320
not going to go through in the code in

00:20:40,059 --> 00:20:43,240
detail we don't have time for that

00:20:41,320 --> 00:20:44,860
there's a paper or your along there

00:20:43,240 --> 00:20:48,460
these slightly close you look at it with

00:20:44,860 --> 00:20:50,950
the kind of the fundamental paper on MCS

00:20:48,460 --> 00:20:51,580
locks but the idea is you make a cue out

00:20:50,950 --> 00:20:53,649
of these things

00:20:51,580 --> 00:20:55,690
so they got CP 0 holding lock so we got

00:20:53,649 --> 00:20:58,360
three states and two transitions between

00:20:55,690 --> 00:21:00,610
them CPU zero holds the lock CPU one is

00:20:58,360 --> 00:21:05,440
spinning on its own little node there

00:21:00,610 --> 00:21:08,049
and CP 2 is spinning on its node so when

00:21:05,440 --> 00:21:10,299
it comes time to release the lock CPU

00:21:08,049 --> 00:21:12,279
zero has to fight only with CPU one not

00:21:10,299 --> 00:21:16,179
all the CPUs of course there's only

00:21:12,279 --> 00:21:19,210
three CPUs they're saying so what I have

00:21:16,179 --> 00:21:21,549
had problems on systems of 4096 CPUs and

00:21:19,210 --> 00:21:23,440
it believe me if you have 4095 CPUs

00:21:21,549 --> 00:21:25,630
fighting with you you're in a world of

00:21:23,440 --> 00:21:27,789
hurt and so things like this can make it

00:21:25,630 --> 00:21:29,830
make it work better and then that that

00:21:27,789 --> 00:21:32,679
once it hands it off CP 1 holds a lock

00:21:29,830 --> 00:21:34,269
and then it hands off CPU 2 and again

00:21:32,679 --> 00:21:35,889
you have most two CPUs contending for a

00:21:34,269 --> 00:21:39,519
given location and life is a lot easier

00:21:35,889 --> 00:21:40,570
in the high contention realm you know

00:21:39,519 --> 00:21:41,110
except that there's one thing that's

00:21:40,570 --> 00:21:45,299
even better

00:21:41,110 --> 00:21:47,830
and that's not to have high contention

00:21:45,299 --> 00:21:49,269
the the queued locks usually have a

00:21:47,830 --> 00:21:50,860
little bit more overhead and being set

00:21:49,269 --> 00:21:52,419
up you have to do one more operation and

00:21:50,860 --> 00:21:54,250
that cost you in the low contention case

00:21:52,419 --> 00:21:56,230
if you can make your algorithms just

00:21:54,250 --> 00:22:00,820
have low contention you're in a lot

00:21:56,230 --> 00:22:02,409
better position in addition just

00:22:00,820 --> 00:22:04,059
spinning all the time may not be what

00:22:02,409 --> 00:22:05,320
you want especially userspace code and

00:22:04,059 --> 00:22:07,149
the kernel I get away with it because

00:22:05,320 --> 00:22:07,720
I've got to hold the CPU and it's not

00:22:07,149 --> 00:22:09,789
going anywhere

00:22:07,720 --> 00:22:11,110
and userspace code you can block while

00:22:09,789 --> 00:22:12,669
you're spinning you can guide holding

00:22:11,110 --> 00:22:13,270
lock get blocked and horrible things can

00:22:12,669 --> 00:22:15,409
happen

00:22:13,270 --> 00:22:16,490
so you really would like an adaptive

00:22:15,409 --> 00:22:18,140
spin sleep strategy

00:22:16,490 --> 00:22:19,370
I could probably this is one of the

00:22:18,140 --> 00:22:21,850
things about throwing away the slides I

00:22:19,370 --> 00:22:23,990
could talk about this for a long time

00:22:21,850 --> 00:22:25,610
but one thing you do is have the

00:22:23,990 --> 00:22:28,100
algorithm use futex system calls the

00:22:25,610 --> 00:22:30,530
Linux kernel other systems have similar

00:22:28,100 --> 00:22:32,539
things in order to spin for a little bit

00:22:30,530 --> 00:22:34,280
and then go to sleep so that if it's

00:22:32,539 --> 00:22:35,990
available right now you get it or it's

00:22:34,280 --> 00:22:38,240
available soon you get it if it's gonna

00:22:35,990 --> 00:22:42,320
be a long time you get out of the way of

00:22:38,240 --> 00:22:44,840
the people that have it ok we read a

00:22:42,320 --> 00:22:46,850
rider locking you know it sounds

00:22:44,840 --> 00:22:48,110
wonderful read side parallelism you know

00:22:46,850 --> 00:22:49,700
if you have read mostly stuff use a

00:22:48,110 --> 00:22:54,200
reader read lock and lock and life is

00:22:49,700 --> 00:22:55,549
wonderful right so what you normally

00:22:54,200 --> 00:22:57,559
have for these things there's a there's

00:22:55,549 --> 00:22:58,880
been papers there's been a huge amount

00:22:57,559 --> 00:23:00,080
of ink spilled on how you can make

00:22:58,880 --> 00:23:02,059
different kinds of reader idle locks

00:23:00,080 --> 00:23:04,220
this is just kind of a canonical single

00:23:02,059 --> 00:23:05,659
variable thing you love and there's a

00:23:04,220 --> 00:23:06,950
bunch of variations on this the flag

00:23:05,659 --> 00:23:08,539
bits may have all sorts of things in

00:23:06,950 --> 00:23:10,130
them but you have some number of readers

00:23:08,539 --> 00:23:12,200
yes I'm the writers you have some some

00:23:10,130 --> 00:23:14,780
flag bits for fairness if you want to

00:23:12,200 --> 00:23:15,860
read you at the very least have to say

00:23:14,780 --> 00:23:16,940
you're there you have to increment the

00:23:15,860 --> 00:23:18,530
number of readers in the word and

00:23:16,940 --> 00:23:19,970
because you have multiple people doing

00:23:18,530 --> 00:23:22,309
this this needs to be a

00:23:19,970 --> 00:23:23,870
read-modify-write instruction that many

00:23:22,309 --> 00:23:25,549
people I said one way or another might

00:23:23,870 --> 00:23:28,820
be compare and swap might be atomic

00:23:25,549 --> 00:23:31,309
fetch an ad or something like that and

00:23:28,820 --> 00:23:33,350
the problem here is that the readers are

00:23:31,309 --> 00:23:35,150
having to write to memory write to

00:23:33,350 --> 00:23:37,460
contended memory in order to announce

00:23:35,150 --> 00:23:39,679
their presence and so you know put

00:23:37,460 --> 00:23:41,440
something like that every time you go

00:23:39,679 --> 00:23:43,610
get the read lock you get a cache miss

00:23:41,440 --> 00:23:45,620
now if you remember from a slide way

00:23:43,610 --> 00:23:47,690
earlier cache misses from one sock

00:23:45,620 --> 00:23:50,179
another are way worse than tours of

00:23:47,690 --> 00:23:52,280
magnitude more expensive than just a

00:23:50,179 --> 00:23:53,870
normal operation and so unless you have

00:23:52,280 --> 00:23:55,700
really really big read site critical

00:23:53,870 --> 00:23:57,980
sections with thousands of instructions

00:23:55,700 --> 00:24:00,590
of them you're taking a lot of overhead

00:23:57,980 --> 00:24:03,289
in fact it's likely that you might as

00:24:00,590 --> 00:24:05,090
well use a exclusive lock because by the

00:24:03,289 --> 00:24:07,549
time you acquire and release lock and do

00:24:05,090 --> 00:24:09,679
something you aren't over actually

00:24:07,549 --> 00:24:10,970
overlapping with anybody else's critical

00:24:09,679 --> 00:24:13,039
section because the critical sections

00:24:10,970 --> 00:24:17,750
are short the acquisition of the and the

00:24:13,039 --> 00:24:19,640
release the lock are long so one thing

00:24:17,750 --> 00:24:21,620
we can do what it comes down to the key

00:24:19,640 --> 00:24:23,360
thing here we mentioned that the

00:24:21,620 --> 00:24:25,370
hardware people are up against the laws

00:24:23,360 --> 00:24:26,730
of physics and that means they need a

00:24:25,370 --> 00:24:28,440
little bit of help from the hard

00:24:26,730 --> 00:24:29,940
there's a bunch of ways of doing this

00:24:28,440 --> 00:24:32,190
I'm just gonna give one fairly simple

00:24:29,940 --> 00:24:35,970
one and that's a thread-local storage

00:24:32,190 --> 00:24:38,390
variant of a breeder idle lock what we

00:24:35,970 --> 00:24:41,340
do is we give each thread its own lock

00:24:38,390 --> 00:24:43,200
so you do a read lock by acquiring your

00:24:41,340 --> 00:24:45,030
threads lock and only your threads lock

00:24:43,200 --> 00:24:47,730
and you release it by releasing that

00:24:45,030 --> 00:24:49,890
lock so everybody's reading they're

00:24:47,730 --> 00:24:51,840
requiring each their own locks their

00:24:49,890 --> 00:24:53,580
lock remains in their cache and they get

00:24:51,840 --> 00:24:55,500
fast access to acquiring release in that

00:24:53,580 --> 00:24:56,880
lock yeah there's memory very overhead

00:24:55,500 --> 00:24:59,760
there's atomic instruction overhead but

00:24:56,880 --> 00:25:02,640
you don't have cache misses of course

00:24:59,760 --> 00:25:04,679
this is like any other optimization the

00:25:02,640 --> 00:25:06,480
for this to work the writing threads

00:25:04,679 --> 00:25:08,309
have to acquire all the locks for all

00:25:06,480 --> 00:25:10,290
the threads and then release all the

00:25:08,309 --> 00:25:12,360
locks for all the threads so many things

00:25:10,290 --> 00:25:15,270
very nice to the readers and you know

00:25:12,360 --> 00:25:16,620
too bad for those poor writers and this

00:25:15,270 --> 00:25:18,240
is kind of a picture of what's happening

00:25:16,620 --> 00:25:19,740
instead of having these big cache miss

00:25:18,240 --> 00:25:22,140
explosions like we had on the slide a

00:25:19,740 --> 00:25:23,850
couple times ago each CPU is just

00:25:22,140 --> 00:25:28,590
looking at its own cache and things

00:25:23,850 --> 00:25:29,730
would go very fast again what we've got

00:25:28,590 --> 00:25:30,960
is we've got something that is a reader

00:25:29,730 --> 00:25:32,940
so I had to lock that works very well

00:25:30,960 --> 00:25:34,380
for the readers but we've lost a little

00:25:32,940 --> 00:25:35,940
bit of generality because the writers

00:25:34,380 --> 00:25:37,610
get such horrible performance especially

00:25:35,940 --> 00:25:40,380
if you have large numbers of threads and

00:25:37,610 --> 00:25:42,179
that by the way is one of the reasons

00:25:40,380 --> 00:25:43,770
that meg and Michael came with hazard

00:25:42,179 --> 00:25:45,720
pointers I canif large you to deal with

00:25:43,770 --> 00:25:47,340
that situation but that's another

00:25:45,720 --> 00:25:49,980
presentation and we've given those

00:25:47,340 --> 00:25:51,390
before at this point it's time for me to

00:25:49,980 --> 00:25:54,290
hand off to Megan and he's gonna give us

00:25:51,390 --> 00:26:02,840
a single producer single Zoomer buffer

00:25:54,290 --> 00:26:02,840
thank you hello okay

00:26:30,670 --> 00:26:38,770
yeah so I I selected really a simple

00:26:36,090 --> 00:26:42,870
data structure with very simple

00:26:38,770 --> 00:26:47,440
interface to be able to dive deep into

00:26:42,870 --> 00:26:49,330
how we design it how we deal with the

00:26:47,440 --> 00:26:52,690
trade-offs between performance

00:26:49,330 --> 00:26:57,670
complexity and generality and to apply

00:26:52,690 --> 00:27:00,340
the concepts and techniques that Michael

00:26:57,670 --> 00:27:02,670
and Paul spoke about and complements

00:27:00,340 --> 00:27:08,710
some of the issues that were not covered

00:27:02,670 --> 00:27:10,720
so basically it's had a single producing

00:27:08,710 --> 00:27:13,030
single current superb consumer buffer

00:27:10,720 --> 00:27:16,030
risk for communication between a single

00:27:13,030 --> 00:27:19,090
producer and single consumer it's a

00:27:16,030 --> 00:27:21,550
buffer with a bounded size and this is

00:27:19,090 --> 00:27:24,280
the interface that we want to provide at

00:27:21,550 --> 00:27:28,690
least for now I'll get to blocking later

00:27:24,280 --> 00:27:31,630
so this is a non-blocking interface the

00:27:28,690 --> 00:27:35,800
the simplest implementation would be a

00:27:31,630 --> 00:27:39,220
simple a single lock so there is one

00:27:35,800 --> 00:27:42,760
lock that each of the operations has to

00:27:39,220 --> 00:27:44,680
acquire and to operate to operate in

00:27:42,760 --> 00:27:47,740
their structure using just the

00:27:44,680 --> 00:27:50,920
sequential algorithm that that you'd

00:27:47,740 --> 00:27:53,380
expect in if the thread is just

00:27:50,920 --> 00:27:57,700
operating by itself under the

00:27:53,380 --> 00:27:59,830
instructions the problem so this is what

00:27:57,700 --> 00:28:03,310
it looks like you have the you know the

00:27:59,830 --> 00:28:05,230
buffer and and the pointers but then you

00:28:03,310 --> 00:28:12,910
have the lock that's the only thing that

00:28:05,230 --> 00:28:14,140
is allowing concurrency so the problem

00:28:12,910 --> 00:28:17,740
with this is that there is no

00:28:14,140 --> 00:28:20,380
parallelism like the producer cannot

00:28:17,740 --> 00:28:22,509
operate with concurrently with in

00:28:20,380 --> 00:28:25,960
parallel with a consumer

00:28:22,509 --> 00:28:29,049
another problem is that the that lock

00:28:25,960 --> 00:28:32,200
itself whatever implementation like like

00:28:29,049 --> 00:28:35,590
for example what polls showed there

00:28:32,200 --> 00:28:38,019
we'll both threads will be writing to it

00:28:35,590 --> 00:28:40,359
back and forth and the cache line will

00:28:38,019 --> 00:28:42,309
keep bouncing because there are both

00:28:40,359 --> 00:28:44,679
writing to it

00:28:42,309 --> 00:28:47,499
actually it's enough that one of them is

00:28:44,679 --> 00:28:50,769
writing but they are both right another

00:28:47,499 --> 00:28:53,349
problem is that it's it's a lock

00:28:50,769 --> 00:28:56,499
acquisition requires either a read

00:28:53,349 --> 00:28:59,349
modify write instruction or offense most

00:28:56,499 --> 00:29:02,019
in practically it is really fire right

00:28:59,349 --> 00:29:04,269
atomic instruction so that's the least

00:29:02,019 --> 00:29:06,549
of the problems but it is still like an

00:29:04,269 --> 00:29:10,359
order of magnitude slower than regular

00:29:06,549 --> 00:29:12,099
loads and stores so these are the

00:29:10,359 --> 00:29:13,239
problem with the with that single lock

00:29:12,099 --> 00:29:16,029
and the measure

00:29:13,239 --> 00:29:19,509
so I ran that and the measurements on my

00:29:16,029 --> 00:29:22,960
machine with on a single socket with 2.2

00:29:19,509 --> 00:29:26,080
gigahertz is like 4.3 million

00:29:22,960 --> 00:29:27,969
handoffs per second so hopefully we can

00:29:26,080 --> 00:29:30,700
do better than this of course this

00:29:27,969 --> 00:29:32,489
solution is very general like you were

00:29:30,700 --> 00:29:36,820
just using standard length you know

00:29:32,489 --> 00:29:41,259
standard language nothing not portable

00:29:36,820 --> 00:29:45,039
at all now we get to Lamport algorithm

00:29:41,259 --> 00:29:48,460
the classic single producer-consumer

00:29:45,039 --> 00:29:50,049
algorithm instead of the lock it is

00:29:48,460 --> 00:29:52,690
dealing with all it is dealing with

00:29:50,049 --> 00:29:55,299
these kind of atomic variables and has

00:29:52,690 --> 00:29:59,919
all these loads and stores to it that

00:29:55,299 --> 00:30:03,190
can go concurrent in parallel and this

00:29:59,919 --> 00:30:11,229
what gives us gives us the safety of

00:30:03,190 --> 00:30:12,879
this idea structure yeah and so let's

00:30:11,229 --> 00:30:15,279
look at what what are the features that

00:30:12,879 --> 00:30:16,899
we get from this what we get is like

00:30:15,279 --> 00:30:18,639
okay we lost the lock we don't have the

00:30:16,899 --> 00:30:21,339
lock anymore which is with all the

00:30:18,639 --> 00:30:22,989
negative things about it this especially

00:30:21,339 --> 00:30:24,820
a single lock it's not fine-grain

00:30:22,989 --> 00:30:26,109
locking it's actually coarse-grained

00:30:24,820 --> 00:30:29,289
locking with all the negative things

00:30:26,109 --> 00:30:31,989
about it so we'll see from this that

00:30:29,289 --> 00:30:33,460
there is parallelism there's nothing

00:30:31,989 --> 00:30:36,090
really prevents that producer and

00:30:33,460 --> 00:30:39,040
consumer from proceeding and

00:30:36,090 --> 00:30:41,460
that's good the other thing and let's

00:30:39,040 --> 00:30:45,240
look at all this instruction all these

00:30:41,460 --> 00:30:52,200
upload and store operations they are

00:30:45,240 --> 00:30:56,590
load acquire and store release these are

00:30:52,200 --> 00:31:02,490
quite cheap on x86 and not as expensive

00:30:56,590 --> 00:31:05,710
as full fences on x86 and other

00:31:02,490 --> 00:31:07,630
platforms but there's a caveat I mean

00:31:05,710 --> 00:31:11,320
this these instructions are yeah they

00:31:07,630 --> 00:31:13,900
are fast but only if the cache if you

00:31:11,320 --> 00:31:17,590
have good cache locality the problem

00:31:13,900 --> 00:31:22,930
here is that we have cache line bouncing

00:31:17,590 --> 00:31:26,530
and that's because we have data that is

00:31:22,930 --> 00:31:29,440
being written by one thread and read by

00:31:26,530 --> 00:31:33,130
the other threads and vice versa so this

00:31:29,440 --> 00:31:35,650
is this is the downside of lamport's

00:31:33,130 --> 00:31:37,750
algorithm but it is much better we have

00:31:35,650 --> 00:31:41,650
as you said there's parallelism and

00:31:37,750 --> 00:31:44,650
we're avoiding atomic read-modify-write

00:31:41,650 --> 00:31:49,300
instructions so what we get in

00:31:44,650 --> 00:31:52,480
performance we improved a bit let's see

00:31:49,300 --> 00:31:57,670
if we can get better performance at the

00:31:52,480 --> 00:32:01,960
cost of more complexity and less

00:31:57,670 --> 00:32:05,520
generality so there's a diagram by

00:32:01,960 --> 00:32:09,250
Jacques ammonia tell from 2008 and

00:32:05,520 --> 00:32:12,030
basically they switch things so instead

00:32:09,250 --> 00:32:15,430
of the indices the the pointers being

00:32:12,030 --> 00:32:19,300
atomic they made the pointers like you

00:32:15,430 --> 00:32:21,610
know they are private but instead the

00:32:19,300 --> 00:32:24,250
buffer itself the the items in the

00:32:21,610 --> 00:32:26,200
buffer itself each is atomic and this

00:32:24,250 --> 00:32:29,470
example I just to simplify things I made

00:32:26,200 --> 00:32:32,020
it like a point you know a pointer but

00:32:29,470 --> 00:32:36,850
it's actually you know it's quite easy

00:32:32,020 --> 00:32:41,560
to change it to general data with a flag

00:32:36,850 --> 00:32:43,870
like like what polls showed so the

00:32:41,560 --> 00:32:47,290
features here we we do have parallelism

00:32:43,870 --> 00:32:49,090
we didn't lose that we have fast

00:32:47,290 --> 00:32:49,720
instructions as you can see the load and

00:32:49,090 --> 00:32:56,350
store

00:32:49,720 --> 00:32:58,630
you know acquire and release and we have

00:32:56,350 --> 00:33:05,170
no cash line bouncing because you look

00:32:58,630 --> 00:33:09,250
at that the the producer is only dealing

00:33:05,170 --> 00:33:12,340
with the tail in a pointer and consumer

00:33:09,250 --> 00:33:15,400
is dealing with is managing the the head

00:33:12,340 --> 00:33:19,030
pointer so there's no cash lining cash

00:33:15,400 --> 00:33:22,510
line balancing another thing is that we

00:33:19,030 --> 00:33:23,170
have less I mean we have improved full

00:33:22,510 --> 00:33:24,970
sharing

00:33:23,170 --> 00:33:27,820
let's pull cheering and that's because

00:33:24,970 --> 00:33:30,460
but in four we the price we pay here is

00:33:27,820 --> 00:33:35,590
like we have to do let's do something

00:33:30,460 --> 00:33:38,830
kinda more less general of like aligning

00:33:35,590 --> 00:33:42,370
the data to cache lines to avoid false

00:33:38,830 --> 00:33:44,500
sharing without that we Dooley's lose

00:33:42,370 --> 00:33:46,990
performance so let's look at the

00:33:44,500 --> 00:33:48,670
performance results that we get if we're

00:33:46,990 --> 00:33:52,710
doing if we didn't do that alignment

00:33:48,670 --> 00:33:55,150
actually we do worse than than Lamport

00:33:52,710 --> 00:33:57,330
because the you know the head and tail

00:33:55,150 --> 00:33:59,830
even though they are managed separately

00:33:57,330 --> 00:34:02,680
they're actually if they are in the same

00:33:59,830 --> 00:34:05,170
cache line there will be false sharing

00:34:02,680 --> 00:34:07,900
and the cache line will keep bouncing so

00:34:05,170 --> 00:34:10,300
by aligning them separately avoid false

00:34:07,900 --> 00:34:14,350
sharing we were able to double the

00:34:10,300 --> 00:34:20,710
performance of Lamport so the lessons we

00:34:14,350 --> 00:34:24,190
learn from this is that we can achieve

00:34:20,710 --> 00:34:29,590
high performance at the cost of hard

00:34:24,190 --> 00:34:33,460
complexity and reduced in reality and we

00:34:29,590 --> 00:34:37,000
looked at you know how cache cache

00:34:33,460 --> 00:34:39,310
locality for sharing alignment these

00:34:37,000 --> 00:34:41,740
kind of issues now let's switch gears

00:34:39,310 --> 00:34:44,470
and look at what if we want to I'm so so

00:34:41,740 --> 00:34:47,980
far that was non blocking like basically

00:34:44,470 --> 00:34:49,530
the consumer had to be spinning even if

00:34:47,980 --> 00:34:52,450
it's doing some kind of them you know

00:34:49,530 --> 00:34:54,730
like less expensive like pausing or

00:34:52,450 --> 00:35:01,210
thing but it's still consuming CPU

00:34:54,730 --> 00:35:03,230
cycles to await like so like you know

00:35:01,210 --> 00:35:06,380
items produced by the

00:35:03,230 --> 00:35:11,660
producer so they're not all applications

00:35:06,380 --> 00:35:16,130
can can utilize that and some might use

00:35:11,660 --> 00:35:18,619
might need to have blocking consumer so

00:35:16,130 --> 00:35:20,330
we look at this and the first thing

00:35:18,619 --> 00:35:23,690
we'll look is like okay we'll use a

00:35:20,330 --> 00:35:25,820
single lock with condition variables

00:35:23,690 --> 00:35:27,140
with a condition variable so this is

00:35:25,820 --> 00:35:29,840
kind of like the straightforward

00:35:27,140 --> 00:35:32,000
solution and we added a condition

00:35:29,840 --> 00:35:34,580
variable but then the problem with this

00:35:32,000 --> 00:35:40,100
is that we have a notify a quite

00:35:34,580 --> 00:35:43,490
expensive like step that is in the

00:35:40,100 --> 00:35:46,250
critical path of the producer so that's

00:35:43,490 --> 00:35:50,000
the downside and the performance is

00:35:46,250 --> 00:35:52,670
quite bad it's it because of the single

00:35:50,000 --> 00:35:55,910
lock is the non blocking implementation

00:35:52,670 --> 00:36:01,280
is it's quite expensive it doesn't look

00:35:55,910 --> 00:36:04,520
as you know it but it is worse let's

00:36:01,280 --> 00:36:06,770
look at I didn't like her so let's do

00:36:04,520 --> 00:36:09,170
something kind of more complex with

00:36:06,770 --> 00:36:10,970
futex and that's so now we're losing

00:36:09,170 --> 00:36:14,420
generality it's not something that is

00:36:10,970 --> 00:36:21,050
available in every system and with it's

00:36:14,420 --> 00:36:23,510
not like language supported so let's

00:36:21,050 --> 00:36:26,450
apply to Jack Imani I mean it's I

00:36:23,510 --> 00:36:29,690
already applied to Lamport but I'm not

00:36:26,450 --> 00:36:32,030
I'm not going to show that so the

00:36:29,690 --> 00:36:36,140
difference here is that we added a flag

00:36:32,030 --> 00:36:41,600
or state you know atomic variable to the

00:36:36,140 --> 00:36:44,810
items in the buffer and every every in

00:36:41,600 --> 00:36:49,010
queue operation it you know it it will

00:36:44,810 --> 00:36:52,040
wake up the the consumer in case it is

00:36:49,010 --> 00:36:54,830
asleep it doesn't know and the state is

00:36:52,040 --> 00:36:59,080
just like it's either empty or full so

00:36:54,830 --> 00:37:02,300
as we see here that the steps are

00:36:59,080 --> 00:37:05,030
similar to logic the non blocking but

00:37:02,300 --> 00:37:10,510
then what is what is added is that now

00:37:05,030 --> 00:37:10,510
the producer will wake up the consumer

00:37:10,930 --> 00:37:15,310
the consumer doesn't necessarily always

00:37:14,140 --> 00:37:18,190
weigh

00:37:15,310 --> 00:37:19,660
you know go to sleep it's only when you

00:37:18,190 --> 00:37:22,360
needed it will so it's not in the

00:37:19,660 --> 00:37:25,810
critical path so let's look at the

00:37:22,360 --> 00:37:28,780
performance here and I applied it to

00:37:25,810 --> 00:37:31,210
Lampert and it is you know it is

00:37:28,780 --> 00:37:33,640
slightly better than the like the

00:37:31,210 --> 00:37:36,400
condition variable and Jacqueline is

00:37:33,640 --> 00:37:41,650
like it is really slightly better but

00:37:36,400 --> 00:37:44,220
really not not that much let's apply

00:37:41,650 --> 00:37:47,140
something a little bit more complicated

00:37:44,220 --> 00:37:50,560
but we want to avoid this very expensive

00:37:47,140 --> 00:37:55,660
step of waking up the consumer from the

00:37:50,560 --> 00:37:58,270
critical path so for this we change the

00:37:55,660 --> 00:38:03,820
algorithm to use three values instead of

00:37:58,270 --> 00:38:07,600
two and it's conditional so that the

00:38:03,820 --> 00:38:12,460
variable is being you know set by both

00:38:07,600 --> 00:38:14,560
the producer consumer and the like so as

00:38:12,460 --> 00:38:17,920
you can as I'll show these are a little

00:38:14,560 --> 00:38:20,680
bit more expensive or complex but then

00:38:17,920 --> 00:38:23,140
the wakeup is only conditioner it's

00:38:20,680 --> 00:38:25,630
being like under you know being

00:38:23,140 --> 00:38:28,720
selective when to wake up the the

00:38:25,630 --> 00:38:32,050
consumer so that's that's that will show

00:38:28,720 --> 00:38:35,830
that'll be a good advantage there's a

00:38:32,050 --> 00:38:38,470
slight disadvantage here because both of

00:38:35,830 --> 00:38:42,160
them might be both the producer and the

00:38:38,470 --> 00:38:46,300
consumer might be actually updating the

00:38:42,160 --> 00:38:50,590
state concurrently then we need to have

00:38:46,300 --> 00:38:55,030
a read modify write operation so this

00:38:50,590 --> 00:38:59,080
will limit the gains that we can get and

00:38:55,030 --> 00:39:02,770
it is in the critical path so we have

00:38:59,080 --> 00:39:06,040
compared change operation that the

00:39:02,770 --> 00:39:08,290
producer has to do every time just to

00:39:06,040 --> 00:39:10,870
make sure that it didn't miss that the

00:39:08,290 --> 00:39:15,550
quiz where the consumer went to sleep or

00:39:10,870 --> 00:39:23,110
doing good or not let's look at the

00:39:15,550 --> 00:39:25,330
algorithm for the consumer it's

00:39:23,110 --> 00:39:28,660
basically it's doing everything and only

00:39:25,330 --> 00:39:32,380
wait like after it gives up and it's

00:39:28,660 --> 00:39:36,700
want to wait it will block by dozens

00:39:32,380 --> 00:39:40,440
just go straight to to wait it has to

00:39:36,700 --> 00:39:43,680
actually make sure that the state is set

00:39:40,440 --> 00:39:46,630
using an original fire right operation

00:39:43,680 --> 00:39:50,140
inkay in case there is kind of a race

00:39:46,630 --> 00:39:52,000
condition with the producer again this

00:39:50,140 --> 00:39:55,690
is not in the critical path so it really

00:39:52,000 --> 00:39:57,640
doesn't affect performance that much now

00:39:55,690 --> 00:39:59,230
let's look at how the performance is

00:39:57,640 --> 00:40:02,309
affected

00:39:59,230 --> 00:40:04,960
Lamport I didn't show the code here but

00:40:02,309 --> 00:40:07,150
Lamport basically because I said since

00:40:04,960 --> 00:40:10,770
central I you know the the

00:40:07,150 --> 00:40:14,079
synchronization is done on centralized

00:40:10,770 --> 00:40:17,859
variables it actually doesn't doesn't

00:40:14,079 --> 00:40:20,460
really scale or improve that much but

00:40:17,859 --> 00:40:23,740
jakkuman because were doing this

00:40:20,460 --> 00:40:28,079
synchronization on every item in the

00:40:23,740 --> 00:40:32,859
buffer we're actually able to achieve

00:40:28,079 --> 00:40:37,000
much better scalability that being said

00:40:32,859 --> 00:40:39,520
it's actually it also shows us that how

00:40:37,000 --> 00:40:41,980
the choice of being like if someone says

00:40:39,520 --> 00:40:43,510
do you want to have blocking supported

00:40:41,980 --> 00:40:45,910
or not it's actually you have to be

00:40:43,510 --> 00:40:49,089
really careful what you know we're

00:40:45,910 --> 00:40:51,220
losing 4x performance just for the

00:40:49,089 --> 00:40:54,309
possibility of blocking even without

00:40:51,220 --> 00:40:56,559
using blocking just just for the

00:40:54,309 --> 00:40:59,829
possibility that the consumer might want

00:40:56,559 --> 00:41:01,779
to block we are losing that much so

00:40:59,829 --> 00:41:04,359
actually if you're a designing a library

00:41:01,779 --> 00:41:07,000
or you should you should provide both

00:41:04,359 --> 00:41:08,619
because it's like some user might

00:41:07,000 --> 00:41:10,059
actually have a very streamlined

00:41:08,619 --> 00:41:12,430
communication between the producer and

00:41:10,059 --> 00:41:14,740
consumer and actually want the consumer

00:41:12,430 --> 00:41:17,770
to be it they can afford to have the

00:41:14,740 --> 00:41:20,950
consumer to be awake all the time and

00:41:17,770 --> 00:41:23,680
and this the stream of communication

00:41:20,950 --> 00:41:27,339
between the two threads is actually

00:41:23,680 --> 00:41:34,089
keeping the consumer occupied so that

00:41:27,339 --> 00:41:37,119
that's another lesson here and to

00:41:34,089 --> 00:41:41,470
summarize what what we went through the

00:41:37,119 --> 00:41:42,430
in this talk like so Michael discussed

00:41:41,470 --> 00:41:45,819
like the landscape

00:41:42,430 --> 00:41:47,500
of parallel programming and how it is

00:41:45,819 --> 00:41:50,559
changing and now it seemed you know

00:41:47,500 --> 00:41:53,260
becoming easier but not quite easier and

00:41:50,559 --> 00:41:56,559
the challenges and Paul discussed the

00:41:53,260 --> 00:41:59,020
hardware and software issues that still

00:41:56,559 --> 00:42:01,510
remain and how they improved and how

00:41:59,020 --> 00:42:05,160
they interact and I want to through this

00:42:01,510 --> 00:42:07,660
example that hopefully demonstrated what

00:42:05,160 --> 00:42:11,319
some of the concepts that we discussed

00:42:07,660 --> 00:42:16,390
and don't say how these kind of

00:42:11,319 --> 00:42:19,050
trade-offs can be worked out to talk you

00:42:16,390 --> 00:42:23,710
know to balance generality and

00:42:19,050 --> 00:42:27,640
complexity and performance and so thank

00:42:23,710 --> 00:42:51,220
you and you have any questions for Paul

00:42:27,640 --> 00:42:52,630
and Michael and myself we have

00:42:51,220 --> 00:43:01,740
microphones there and there I think if

00:42:52,630 --> 00:43:01,740
anybody's got questions if oh okay

00:43:02,819 --> 00:43:07,210
microphones is demonstrating loaded

00:43:04,869 --> 00:43:08,980
mouths just so I know all right

00:43:07,210 --> 00:43:11,260
yes sir could you maybe my god could you

00:43:08,980 --> 00:43:14,710
point to any open source for

00:43:11,260 --> 00:43:18,670
high-quality SP SCE or EMC cues that you

00:43:14,710 --> 00:43:23,730
would recommend I mean I could I plan to

00:43:18,670 --> 00:43:31,059
provide that so I haven't yeah right so

00:43:23,730 --> 00:43:33,880
probably soon yeah so a question for you

00:43:31,059 --> 00:43:37,930
about the times that you were showing

00:43:33,880 --> 00:43:41,170
the performance times was that with high

00:43:37,930 --> 00:43:42,670
contention or low contention or yeah no

00:43:41,170 --> 00:43:45,420
I mean basically there are two threads

00:43:42,670 --> 00:43:49,690
oh yeah I should say the buffer was like

00:43:45,420 --> 00:43:52,540
1,024 items it was like of course two

00:43:49,690 --> 00:43:54,819
threads I limited to one socket exactly

00:43:52,540 --> 00:43:56,000
it was we were losing performance on

00:43:54,819 --> 00:43:59,060
multi socket I mean it's

00:43:56,000 --> 00:44:01,820
there's no need for that and it was yeah

00:43:59,060 --> 00:44:04,190
I mean basically it's equivalent to like

00:44:01,820 --> 00:44:07,040
you know the producer giving the

00:44:04,190 --> 00:44:11,050
consumer a pointer and the consumer can

00:44:07,040 --> 00:44:13,250
like you know depositing somewhere that

00:44:11,050 --> 00:44:15,950
unless it turning around to get the

00:44:13,250 --> 00:44:17,840
other item so it's quite high contention

00:44:15,950 --> 00:44:19,550
yes okay with the relative performance

00:44:17,840 --> 00:44:22,010
of the different solutions change

00:44:19,550 --> 00:44:25,940
depending on the degree of contention in

00:44:22,010 --> 00:44:28,100
your system of course if let's say

00:44:25,940 --> 00:44:29,870
there's imbalance between the producer

00:44:28,100 --> 00:44:31,640
and consumer then yeah I mean that

00:44:29,870 --> 00:44:33,500
that'll be like a different bottleneck

00:44:31,640 --> 00:44:35,900
but actually I mean what you expected

00:44:33,500 --> 00:44:37,700
from some like this is like the producer

00:44:35,900 --> 00:44:41,210
and consumer really balanced otherwise

00:44:37,700 --> 00:44:43,460
there would be or either their best or

00:44:41,210 --> 00:44:48,470
the consumer I mean the producer is

00:44:43,460 --> 00:44:50,450
actually just you know like he has more

00:44:48,470 --> 00:44:52,100
pauses and then the consumer would just

00:44:50,450 --> 00:44:58,630
have to go to sleep and in that case

00:44:52,100 --> 00:44:58,630
would want to use the blocking version

00:45:00,940 --> 00:45:06,230
so my question is probably for power

00:45:03,770 --> 00:45:08,690
more so the question is you are saying

00:45:06,230 --> 00:45:10,580
that if we have bigger caches of course

00:45:08,690 --> 00:45:13,340
performance is better because most of

00:45:10,580 --> 00:45:16,010
the time CPU is waiting for data from

00:45:13,340 --> 00:45:20,810
the memory so and at the same time the

00:45:16,010 --> 00:45:24,770
size of the l1 l2 l3 is not changing in

00:45:20,810 --> 00:45:27,590
recent years and so if we send a web it

00:45:24,770 --> 00:45:30,620
says that it's expensive to build bigger

00:45:27,590 --> 00:45:33,380
caches but how much it expensive what

00:45:30,620 --> 00:45:37,280
kind of tests you did that entail and

00:45:33,380 --> 00:45:38,780
what are the prognosis well for Intel I

00:45:37,280 --> 00:45:41,360
have to directly somebody works for

00:45:38,780 --> 00:45:44,030
Intel I'm actually right be M o for I

00:45:41,360 --> 00:45:46,220
bet that's a or in the market to talk

00:45:44,030 --> 00:45:49,400
but I can tell you kind of in general

00:45:46,220 --> 00:45:52,220
what what they tend to do they have

00:45:49,400 --> 00:45:53,690
analytic models but they also run traces

00:45:52,220 --> 00:45:55,520
on a budget for applications and

00:45:53,690 --> 00:45:57,560
actually look at okay if we had this

00:45:55,520 --> 00:46:00,320
geometry what would happen there's

00:45:57,560 --> 00:46:02,030
trade-offs if you and the upper level

00:46:00,320 --> 00:46:06,590
caches tend to range small because

00:46:02,030 --> 00:46:08,570
people want what happens is that the if

00:46:06,590 --> 00:46:09,710
you have a physically tagged cache in

00:46:08,570 --> 00:46:12,109
other words

00:46:09,710 --> 00:46:13,430
what happens you have a cache line you

00:46:12,109 --> 00:46:15,230
have to have the address that the cache

00:46:13,430 --> 00:46:17,540
line corresponds to in its data and

00:46:15,230 --> 00:46:19,250
there's a question should that address

00:46:17,540 --> 00:46:20,480
be a virtual address that the user is

00:46:19,250 --> 00:46:22,550
using or should be a physical address

00:46:20,480 --> 00:46:24,440
that the hardware's using and there was

00:46:22,550 --> 00:46:26,630
quite a bit of controversy thirty years

00:46:24,440 --> 00:46:28,430
ago twenty to thirty years ago and the

00:46:26,630 --> 00:46:31,940
physically tagged people one pretty much

00:46:28,430 --> 00:46:35,030
as far as I can tell what that means is

00:46:31,940 --> 00:46:37,010
that if you if the cache is big enough

00:46:35,030 --> 00:46:39,890
that you need to use bits that get

00:46:37,010 --> 00:46:42,080
translated so if you have a 4k page size

00:46:39,890 --> 00:46:43,339
if the cache is big enough that you need

00:46:42,080 --> 00:46:44,660
some of the bits that are translated you

00:46:43,339 --> 00:46:46,430
have you put the translation on the

00:46:44,660 --> 00:46:49,880
critical path so that innermost cache

00:46:46,430 --> 00:46:51,410
and that really slows things down and so

00:46:49,880 --> 00:46:52,910
they tend to keep the closed cache is

00:46:51,410 --> 00:46:54,290
small so that they can just grab the

00:46:52,910 --> 00:46:56,480
lower bits that don't get translated

00:46:54,290 --> 00:46:58,490
anyway applies the cache and be

00:46:56,480 --> 00:47:00,290
translating the address in parallel so

00:46:58,490 --> 00:47:02,320
that's the that's one of the reasons why

00:47:00,290 --> 00:47:08,300
those top caches tend to remain tiny

00:47:02,320 --> 00:47:09,710
they could of course have put more it's

00:47:08,300 --> 00:47:11,780
kind of like a hash table a hardware

00:47:09,710 --> 00:47:13,040
hash table so you kind of have buckets

00:47:11,780 --> 00:47:14,930
and you can have more stuff in the

00:47:13,040 --> 00:47:16,910
buckets but they don't do linked lists

00:47:14,930 --> 00:47:18,230
very well in hardware and the problem is

00:47:16,910 --> 00:47:21,080
if you if you have more stuff in a

00:47:18,230 --> 00:47:22,550
bucket you have to have do more

00:47:21,080 --> 00:47:24,890
comparisons you have to have bigger

00:47:22,550 --> 00:47:26,990
bigger fan into the and fan out to the

00:47:24,890 --> 00:47:29,570
gates that are checking the addresses

00:47:26,990 --> 00:47:31,970
and that starts hurting your performance

00:47:29,570 --> 00:47:34,220
and also increasing energy consumption

00:47:31,970 --> 00:47:36,050
so those are I can't give you a really

00:47:34,220 --> 00:47:37,849
exact answer to it because but that's

00:47:36,050 --> 00:47:43,849
from a software guy's viewpoint that's

00:47:37,849 --> 00:47:45,640
kind of where it is thank you any more

00:47:43,849 --> 00:47:48,170
questions

00:47:45,640 --> 00:47:52,040
well if not you guys have an unfair

00:47:48,170 --> 00:47:54,099
advantage for break thank you thanks

00:47:52,040 --> 00:47:54,099

YouTube URL: https://www.youtube.com/watch?v=74QjNwYAJ7M


