Title: CppCon 2016: Timur Doumler “Want fast C++? Know your hardware!"
Publication date: 2016-10-02
Playlist: CppCon 2016
Description: 
	http://CppCon.org
—
Presentation Slides, PDFs, Source Code and other presenter materials are available at: https://github.com/cppcon/cppcon2016
—
As C++ evolves, it provides us with better and more powerful tools for optimal performance. But often, knowing the language very well is not enough. It is just as important to know your hardware. Modern computer architectures have many properties that can impact the performance of C++ code, such as cache locality, cache associativity, true and false sharing between cores, memory alignment, the branch predictor, the instruction pipeline, denormals, and SIMD. In this talk, I will give an overview over these properties, using C++ code. I will present a series of code examples, highlighting different effects, and benchmark their performance on different machines with different compilers, sometimes with surprising results. The talk will draw a picture of what every C++ developer needs to know about hardware architecture, provide guidelines on how to write modern C++ code that is cache-friendly, pipeline-friendly, and well-vectorisable, and highlight what to look for when profiling it.
— 
Timur Doumler
ROLI Ltd.
JUCE Senior Software Engineer
London, UK
Timur Doumler is Senior Software Developer at London-based technology company ROLI. He is currently working on JUCE, the leading cross-platform framework for creating audio applications used by hundreds of companies in the audio industry. After five years of writing high-performance code in Fortran, C, and C++ for numerical simulations of the cosmic structure formation, Timur became committed to audio and music production software. Before joining ROLI, he worked on various projects at market-leading company Native Instruments, such as KONTAKT, the industry standard sampling platform used by the majority of music producers and composers for film score, games, and contemporary popular music. Timur holds a PhD in astrophysics and is passionate about well-written code, modern C++ techniques, science-fiction, learning languages, and progressive rock music.
—
Videos Filmed & Edited by Bash Films: http://www.BashFilms.com
Captions: 
	00:00:00,030 --> 00:00:05,190
hello my name is timid owner I work at

00:00:02,730 --> 00:00:06,899
Rolly in London UK we make audio

00:00:05,190 --> 00:00:09,690
software audio hardware and music

00:00:06,899 --> 00:00:10,740
technology and we also make juice which

00:00:09,690 --> 00:00:13,290
is an open-source cross-platform

00:00:10,740 --> 00:00:15,089
framework written in C++

00:00:13,290 --> 00:00:16,949
it runs on major all major desktop and

00:00:15,089 --> 00:00:19,109
mobile platforms it really does

00:00:16,949 --> 00:00:21,180
everything you need and it's the leading

00:00:19,109 --> 00:00:23,340
framework in this particular pro audio

00:00:21,180 --> 00:00:25,019
and music production software industry

00:00:23,340 --> 00:00:26,630
you can also use it for many other

00:00:25,019 --> 00:00:29,220
things so this is sort of my background

00:00:26,630 --> 00:00:31,140
now in an audio in music we are

00:00:29,220 --> 00:00:32,910
essentially in a very low latency real

00:00:31,140 --> 00:00:35,370
time environment so what does that mean

00:00:32,910 --> 00:00:36,870
some people you do games so you render

00:00:35,370 --> 00:00:38,870
60 frames per second or something like

00:00:36,870 --> 00:00:41,610
that well we have we have to render

00:00:38,870 --> 00:00:43,500
44,000 audio samples per second yeah and

00:00:41,610 --> 00:00:46,140
that in real-time so that's really

00:00:43,500 --> 00:00:47,250
fascinating a problem if you want to

00:00:46,140 --> 00:00:49,770
know more about that and how it actually

00:00:47,250 --> 00:00:51,300
works you can check out my last CPP

00:00:49,770 --> 00:00:53,969
contour from last year which is called

00:00:51,300 --> 00:00:56,160
C++ in the audio industry where I talk a

00:00:53,969 --> 00:00:58,109
little bit more about that so but the

00:00:56,160 --> 00:01:01,890
bottom line is we use C++ and we care

00:00:58,109 --> 00:01:05,610
about performance yeah and one thing

00:01:01,890 --> 00:01:07,020
that I found is that if you want to know

00:01:05,610 --> 00:01:09,060
about performance and you know your

00:01:07,020 --> 00:01:10,500
non-coding then you know you know that

00:01:09,060 --> 00:01:12,510
for example quicksort is quicker than

00:01:10,500 --> 00:01:13,979
bubble sort or if you don't see pass

00:01:12,510 --> 00:01:15,600
pass you know that the arrays you move

00:01:13,979 --> 00:01:17,610
idiom is faster than you know

00:01:15,600 --> 00:01:19,140
individually removing elements from from

00:01:17,610 --> 00:01:21,330
a vector bond by one and things like

00:01:19,140 --> 00:01:22,950
that and you can understand all this by

00:01:21,330 --> 00:01:23,939
knowing the language to C++ language

00:01:22,950 --> 00:01:26,759
very well and knowing the data

00:01:23,939 --> 00:01:28,080
structures were you well but there's one

00:01:26,759 --> 00:01:30,270
thing that I found really frustrating

00:01:28,080 --> 00:01:31,710
which is no matter how well you know the

00:01:30,270 --> 00:01:34,350
language and the data structures

00:01:31,710 --> 00:01:35,280
that's just sometimes not enough so you

00:01:34,350 --> 00:01:36,090
also really need to know how the

00:01:35,280 --> 00:01:37,350
hardware works

00:01:36,090 --> 00:01:39,439
you know in order to explain why

00:01:37,350 --> 00:01:44,070
something performs a doesn't perform and

00:01:39,439 --> 00:01:45,210
I find this notoriously hard so our code

00:01:44,070 --> 00:01:46,770
runs on something like this you know

00:01:45,210 --> 00:01:49,079
this is how a toad chip looks like and

00:01:46,770 --> 00:01:50,520
but if you're a C++ programmer what does

00:01:49,079 --> 00:01:53,100
that mean for you you know what what you

00:01:50,520 --> 00:01:55,500
need to know and then you start googling

00:01:53,100 --> 00:01:56,880
and then you find diagrams like that of

00:01:55,500 --> 00:02:00,329
you know for example some intel

00:01:56,880 --> 00:02:01,979
architecture you find threads at Stack

00:02:00,329 --> 00:02:04,469
Overflow discussing this stuff you find

00:02:01,979 --> 00:02:06,780
optimization manuals which are very hard

00:02:04,469 --> 00:02:09,750
to read you find books about hardware

00:02:06,780 --> 00:02:11,550
science computer science books but but

00:02:09,750 --> 00:02:13,390
why do you C++ books not talk about this

00:02:11,550 --> 00:02:15,250
stuff I mean they say

00:02:13,390 --> 00:02:17,230
things about being cash friendly and

00:02:15,250 --> 00:02:20,080
data structures and things like that but

00:02:17,230 --> 00:02:23,200
normally it's not very useful if you

00:02:20,080 --> 00:02:25,300
just want to write C++ code and this is

00:02:23,200 --> 00:02:27,940
where my talk today fits in I want to be

00:02:25,300 --> 00:02:29,610
a little bit better than that so I want

00:02:27,940 --> 00:02:32,200
to talk about performance and hardware

00:02:29,610 --> 00:02:33,670
and just to make it feel this is an

00:02:32,200 --> 00:02:36,490
introduction talk yeah so this is sort

00:02:33,670 --> 00:02:38,170
of like an over you if you know all the

00:02:36,490 --> 00:02:39,910
stuff already then you know maybe it's

00:02:38,170 --> 00:02:43,420
really useful as a refresher or at least

00:02:39,910 --> 00:02:44,530
I hope it will be entertaining I'm going

00:02:43,420 --> 00:02:47,320
to present a few Hardware related

00:02:44,530 --> 00:02:50,050
effects a few easy C++ snippets that

00:02:47,320 --> 00:02:53,140
demonstrate you know what's going on and

00:02:50,050 --> 00:02:55,620
I'm going to show some benchmarks and

00:02:53,140 --> 00:02:58,150
really the goal of this is to have a

00:02:55,620 --> 00:03:01,989
mental model of hardware architecture

00:02:58,150 --> 00:03:04,690
which is easier than than this slide and

00:03:01,989 --> 00:03:06,940
useful for your daily C++ job if you're

00:03:04,690 --> 00:03:09,310
in any of the you know industries that

00:03:06,940 --> 00:03:12,040
do this number crunching thing in C++

00:03:09,310 --> 00:03:15,120
you know like audio or games or finance

00:03:12,040 --> 00:03:19,180
or scientific computing or any of these

00:03:15,120 --> 00:03:21,400
so let me start for a long time when I

00:03:19,180 --> 00:03:22,600
started programming my mental model of

00:03:21,400 --> 00:03:26,620
the computer was something like that

00:03:22,600 --> 00:03:28,239
yeah so you have memory and we have your

00:03:26,620 --> 00:03:29,769
numbers which are in memory they have an

00:03:28,239 --> 00:03:31,989
address and you have a CPU it's doing

00:03:29,769 --> 00:03:33,640
all the computations and it can fetch

00:03:31,989 --> 00:03:35,260
you know the data from that memory and

00:03:33,640 --> 00:03:39,400
write it back and inside it's doing its

00:03:35,260 --> 00:03:40,930
stuff so and you can get quite far with

00:03:39,400 --> 00:03:43,090
this you can learn C++ it can do all

00:03:40,930 --> 00:03:45,100
this stuff but it turns out that

00:03:43,090 --> 00:03:46,360
sometimes it's not quite enough and I

00:03:45,100 --> 00:03:48,519
remember when I was a student like the

00:03:46,360 --> 00:03:51,820
first time I discovered a situation

00:03:48,519 --> 00:03:53,680
where that was not enough is when I had

00:03:51,820 --> 00:03:56,440
to traverse a 2d array I think that's

00:03:53,680 --> 00:03:59,110
like a very classical example of this so

00:03:56,440 --> 00:04:01,410
this is my first code snippet so you

00:03:59,110 --> 00:04:03,700
have a two-dimensional array and you

00:04:01,410 --> 00:04:05,709
need to traverse that and touch every

00:04:03,700 --> 00:04:08,680
element and there's two two ways you can

00:04:05,709 --> 00:04:10,239
you can loop through that array either

00:04:08,680 --> 00:04:14,590
in row major order or in column major

00:04:10,239 --> 00:04:15,970
order and you know in C++ if you write

00:04:14,590 --> 00:04:18,190
it like that then the column major order

00:04:15,970 --> 00:04:19,870
will be this way and the row mate sorry

00:04:18,190 --> 00:04:21,370
the row major order way this way and the

00:04:19,870 --> 00:04:23,530
column major order will be that way and

00:04:21,370 --> 00:04:25,260
of course this way is the way the 2d

00:04:23,530 --> 00:04:26,820
arrays actually laid out in memory

00:04:25,260 --> 00:04:30,000
it's actually one contiguous data

00:04:26,820 --> 00:04:34,290
structure and this would end up with you

00:04:30,000 --> 00:04:35,460
jumping back and forth in memory so so

00:04:34,290 --> 00:04:37,950
this is sort of the first the first

00:04:35,460 --> 00:04:39,300
simple example so for the rest of the

00:04:37,950 --> 00:04:40,770
talk I'm going to show snippets like

00:04:39,300 --> 00:04:44,160
that I'm going to show micro benchmarks

00:04:40,770 --> 00:04:45,900
now what I want to say about micro

00:04:44,160 --> 00:04:47,760
benchmarks like you want to know how

00:04:45,900 --> 00:04:50,040
fast the loop runs for example a that's

00:04:47,760 --> 00:04:51,060
a micro benchmark so I want to say what

00:04:50,040 --> 00:04:52,710
micro benchmarks that they're very

00:04:51,060 --> 00:04:55,620
dangerous

00:04:52,710 --> 00:04:56,760
so if you do micro benchmarks you know

00:04:55,620 --> 00:04:59,070
there's a thing called confirmation bias

00:04:56,760 --> 00:05:00,800
where you tend to find the thing that

00:04:59,070 --> 00:05:03,420
you were looking for in the beginning or

00:05:00,800 --> 00:05:06,090
you know you can write code snippets and

00:05:03,420 --> 00:05:07,230
then the optimizer optimizes away the

00:05:06,090 --> 00:05:09,630
thing that you were actually looking for

00:05:07,230 --> 00:05:11,970
and you know things like that so you

00:05:09,630 --> 00:05:13,680
know don't do that or be very very

00:05:11,970 --> 00:05:15,810
careful when you do that

00:05:13,680 --> 00:05:17,250
so today I'll be the guy who does that

00:05:15,810 --> 00:05:18,870
but do you please don't do that measure

00:05:17,250 --> 00:05:20,220
your real code because that's the only

00:05:18,870 --> 00:05:23,880
way to actually find out where the

00:05:20,220 --> 00:05:25,440
problem is the other thing is the last

00:05:23,880 --> 00:05:26,880
time I did a similar talk like this I

00:05:25,440 --> 00:05:29,100
had for the benchmarks I had like my own

00:05:26,880 --> 00:05:30,870
like handwritten macros with some timers

00:05:29,100 --> 00:05:32,990
but since then I discovered Google

00:05:30,870 --> 00:05:37,020
benchmark which is really really great

00:05:32,990 --> 00:05:38,900
and I think last CPP Con last year

00:05:37,020 --> 00:05:41,400
Chandler Carruth gave a keynote about

00:05:38,900 --> 00:05:44,790
you know a few few things about Google

00:05:41,400 --> 00:05:46,980
benchmark and how to use it and thanks

00:05:44,790 --> 00:05:48,780
Chandler I since switched to Google

00:05:46,980 --> 00:05:50,850
benchmark and it's a great tool I can

00:05:48,780 --> 00:05:54,870
really recommend you to use that for

00:05:50,850 --> 00:05:57,630
micro benchmarks so the other thing I

00:05:54,870 --> 00:06:01,350
want to say is that for these benchmarks

00:05:57,630 --> 00:06:03,270
I usually I take the three compiler set

00:06:01,350 --> 00:06:06,030
you know I'm working with so it's clang

00:06:03,270 --> 00:06:07,560
GCC and Visual Studio the most recent

00:06:06,030 --> 00:06:09,240
versions of that and I'm using the

00:06:07,560 --> 00:06:11,520
default settings with full optimizations

00:06:09,240 --> 00:06:13,020
on so something like oh three yeah so of

00:06:11,520 --> 00:06:14,790
course there's a lot you can tweak with

00:06:13,020 --> 00:06:17,910
compiler flags and things like that and

00:06:14,790 --> 00:06:20,190
results will change a lot but I think

00:06:17,910 --> 00:06:21,930
90% of people they just open Xcode or

00:06:20,190 --> 00:06:25,140
Visual Studio or you know whatever and

00:06:21,930 --> 00:06:26,550
hit build or unreleased so you know

00:06:25,140 --> 00:06:29,060
that's what most people do I think so

00:06:26,550 --> 00:06:31,440
this is what I'm going to do as well

00:06:29,060 --> 00:06:33,660
also the other thing is that the

00:06:31,440 --> 00:06:35,190
benchmarks I ran this on this very

00:06:33,660 --> 00:06:37,380
MacBook that I'm running right here and

00:06:35,190 --> 00:06:38,110
also another one which is a bit older

00:06:37,380 --> 00:06:41,230
with a different

00:06:38,110 --> 00:06:43,420
processor architecture you know I could

00:06:41,230 --> 00:06:44,920
have included also arm and you know

00:06:43,420 --> 00:06:46,030
other architectures in this job but is

00:06:44,920 --> 00:06:48,010
really not the point the points we just

00:06:46,030 --> 00:06:50,110
give like a very sort of like quick

00:06:48,010 --> 00:06:53,170
overview over different aspects of

00:06:50,110 --> 00:06:54,550
hardware and performance so and also I'm

00:06:53,170 --> 00:06:56,230
only going to mention these different

00:06:54,550 --> 00:06:57,880
compilers and different machines if

00:06:56,230 --> 00:06:59,800
there's actually significant difference

00:06:57,880 --> 00:07:01,510
in the results otherwise if I'm just

00:06:59,800 --> 00:07:02,770
going to show like one one bar or

00:07:01,510 --> 00:07:08,100
whatever then you can assume that it

00:07:02,770 --> 00:07:12,760
looks similar on all of these now ok so

00:07:08,100 --> 00:07:14,740
let's go back to this array let's you

00:07:12,760 --> 00:07:16,510
know take an array that's maybe 10

00:07:14,740 --> 00:07:18,010
megabytes large maybe yeah something

00:07:16,510 --> 00:07:21,910
like you know a picture of your cat or

00:07:18,010 --> 00:07:23,530
something and then loop through it the

00:07:21,910 --> 00:07:26,740
right way in the wrong way yeah and then

00:07:23,530 --> 00:07:28,000
you see that you know if you look at the

00:07:26,740 --> 00:07:30,280
wrong way around it's going to be 30

00:07:28,000 --> 00:07:31,960
times about 30 35 times slower on this

00:07:30,280 --> 00:07:34,270
particular machine on the other one I

00:07:31,960 --> 00:07:36,070
think it was like 40 times slower so

00:07:34,270 --> 00:07:37,480
this is like a typical number so this is

00:07:36,070 --> 00:07:40,900
obviously very very bad for performance

00:07:37,480 --> 00:07:42,460
and the first time I saw this many years

00:07:40,900 --> 00:07:44,200
ago my supervisor told me yeah well

00:07:42,460 --> 00:07:46,510
that's because processors are really

00:07:44,200 --> 00:07:48,040
good at you know looping continuously

00:07:46,510 --> 00:07:49,810
through memory and and they're very bad

00:07:48,040 --> 00:07:51,910
at jumping which is what happens if you

00:07:49,810 --> 00:07:54,210
loop the wrong way around and that is

00:07:51,910 --> 00:07:56,500
true processes are much better at

00:07:54,210 --> 00:08:00,310
scanning through contiguous memory yeah

00:07:56,500 --> 00:08:02,140
this is the reason why mostly like

00:08:00,310 --> 00:08:03,760
vector and array are so good and so

00:08:02,140 --> 00:08:05,500
better than all the other containers and

00:08:03,760 --> 00:08:07,660
this is also reason why if you need like

00:08:05,500 --> 00:08:09,880
a sorted container a sorted vector or

00:08:07,660 --> 00:08:15,220
something like boost flat set usually

00:08:09,880 --> 00:08:16,960
outperforms a stood set but to really

00:08:15,220 --> 00:08:20,010
understand this it's not enough to just

00:08:16,960 --> 00:08:23,530
know that this is contiguous memory so

00:08:20,010 --> 00:08:25,300
we have to revise our mental model here

00:08:23,530 --> 00:08:27,670
to really understand this and of course

00:08:25,300 --> 00:08:31,510
as you will know there was a cache app

00:08:27,670 --> 00:08:35,140
so because main memories we slow there

00:08:31,510 --> 00:08:36,700
was a cache in between and a cache holds

00:08:35,140 --> 00:08:38,800
less memory but is much faster and

00:08:36,700 --> 00:08:40,979
closer to the CPU and if a structure

00:08:38,800 --> 00:08:43,479
like the area doesn't fit into the cache

00:08:40,979 --> 00:08:46,240
then you know if you loop it if you look

00:08:43,479 --> 00:08:47,470
through the wrong way around then you're

00:08:46,240 --> 00:08:48,970
going to get cache misses and then you

00:08:47,470 --> 00:08:51,399
have to retrieve stuff from main memory

00:08:48,970 --> 00:08:56,180
and that's going to be very slow

00:08:51,399 --> 00:08:59,810
so here's another example so we take an

00:08:56,180 --> 00:09:01,339
area that is very big is 512 megabytes

00:08:59,810 --> 00:09:04,130
so it doesn't fit into the cache and

00:09:01,339 --> 00:09:07,190
then we loop through it and then we just

00:09:04,130 --> 00:09:10,220
touch every nth element yeah so we touch

00:09:07,190 --> 00:09:11,720
either every end it since right yet in

00:09:10,220 --> 00:09:13,310
so we touch either every inch or we

00:09:11,720 --> 00:09:15,110
touch every second in and let me touch

00:09:13,310 --> 00:09:18,110
every 4th and and so on and so forth and

00:09:15,110 --> 00:09:22,130
then you know you would expect that the

00:09:18,110 --> 00:09:23,779
less ins you have to touch the less time

00:09:22,130 --> 00:09:25,880
it takes the less work it is and it

00:09:23,779 --> 00:09:30,829
turns out that's mostly true except that

00:09:25,880 --> 00:09:34,190
if you are there is there's one place

00:09:30,829 --> 00:09:35,300
there were at 16 ins or 64 bytes because

00:09:34,190 --> 00:09:38,240
that's the cache line size in this

00:09:35,300 --> 00:09:39,740
particular machine and if something's on

00:09:38,240 --> 00:09:42,649
the same cache line then basically it's

00:09:39,740 --> 00:09:47,360
just as cheap to touch every end as to

00:09:42,649 --> 00:09:49,730
touch every 16th int yeah so here we see

00:09:47,360 --> 00:09:51,889
that memory comes in cache lines so you

00:09:49,730 --> 00:09:53,089
don't just access an individual int but

00:09:51,889 --> 00:09:55,279
if the cache gets something from memory

00:09:53,089 --> 00:09:58,069
it gets a whole cache line and then if

00:09:55,279 --> 00:09:59,540
you if you touch one end on that cash on

00:09:58,069 --> 00:10:03,319
you get you know all the other data

00:09:59,540 --> 00:10:06,079
that's on that cache line for free um so

00:10:03,319 --> 00:10:07,430
let's come back to this case where we

00:10:06,079 --> 00:10:11,720
loop through the array the wrong way

00:10:07,430 --> 00:10:13,370
around so um so this is a 10 megabyte

00:10:11,720 --> 00:10:16,250
area but what we can do obviously is we

00:10:13,370 --> 00:10:18,529
can see how this changes if we change

00:10:16,250 --> 00:10:21,649
the size of the array yeah so that's

00:10:18,529 --> 00:10:24,160
going to do the same plot but vary the

00:10:21,649 --> 00:10:26,660
size of the array and the solid line is

00:10:24,160 --> 00:10:28,300
where we loop through the array the

00:10:26,660 --> 00:10:29,899
right way around and the dashed line is

00:10:28,300 --> 00:10:33,709
the wrong way around

00:10:29,899 --> 00:10:36,110
and you see that you know you have a

00:10:33,709 --> 00:10:38,029
base sort of penalty because you don't

00:10:36,110 --> 00:10:40,699
loop contiguously so probably you know

00:10:38,029 --> 00:10:43,399
the optimizer can't can't vectorize this

00:10:40,699 --> 00:10:46,639
or something but then also you have

00:10:43,399 --> 00:10:49,550
these steps and these steps occur at

00:10:46,639 --> 00:10:50,990
certain places where you know if the

00:10:49,550 --> 00:10:54,560
area gets so big that it doesn't fit

00:10:50,990 --> 00:10:56,540
into level 2 cache then you get level 2

00:10:54,560 --> 00:10:58,699
cache misses and then your performance

00:10:56,540 --> 00:11:00,199
penalty goes up and then if you ever get

00:10:58,699 --> 00:11:01,490
even bigger at some point it doesn't fit

00:11:00,199 --> 00:11:03,410
into a level 3 cache anymore you get

00:11:01,490 --> 00:11:05,620
level 3 cache misses which is much more

00:11:03,410 --> 00:11:05,620
expensive

00:11:05,850 --> 00:11:14,650
so you're going to get another another

00:11:09,070 --> 00:11:18,070
bump in penalty in performance so cache

00:11:14,650 --> 00:11:19,900
is hierarchical yeah so um we have

00:11:18,070 --> 00:11:21,790
typically on a desktop machine maybe

00:11:19,900 --> 00:11:25,180
something like level 1 2 & 3 caches and

00:11:21,790 --> 00:11:27,820
everyone is every one of those levels is

00:11:25,180 --> 00:11:30,970
gets basically there's one order in

00:11:27,820 --> 00:11:32,590
magnitude in size between these so and

00:11:30,970 --> 00:11:34,570
the bigger they are the slower they are

00:11:32,590 --> 00:11:37,060
so if the level 2 cache is 10 times

00:11:34,570 --> 00:11:38,350
bigger and typically also it's going to

00:11:37,060 --> 00:11:41,830
be something of an order of magnitude

00:11:38,350 --> 00:11:43,270
slower and by slower I mean latency so

00:11:41,830 --> 00:11:46,060
not throughput right so it's not about

00:11:43,270 --> 00:11:47,620
how much data per second you can move

00:11:46,060 --> 00:11:49,720
between these levels it's not about that

00:11:47,620 --> 00:11:50,830
it's about how many how much time how

00:11:49,720 --> 00:11:53,890
many instructions is it does it

00:11:50,830 --> 00:11:56,020
initially take to fetch you know one int

00:11:53,890 --> 00:11:57,760
from level 2 cache and get to level 1

00:11:56,020 --> 00:11:59,950
cache yeah that's that's very important

00:11:57,760 --> 00:12:01,390
like how much does it initially take if

00:11:59,950 --> 00:12:03,850
you get a cache miss to retrieve it from

00:12:01,390 --> 00:12:05,400
a higher level and that gets slower if

00:12:03,850 --> 00:12:11,470
you go up by about an order of magnitude

00:12:05,400 --> 00:12:14,230
um so let's go back to the studio a for

00:12:11,470 --> 00:12:15,430
a minute so you know this benchmark is

00:12:14,230 --> 00:12:16,990
really boring because you don't really

00:12:15,430 --> 00:12:20,620
do anything here I've just loops to the

00:12:16,990 --> 00:12:22,720
area and we the incremented or we added

00:12:20,620 --> 00:12:25,360
we do like 1 addition so that's not a

00:12:22,720 --> 00:12:27,550
very realistic case right so let's let's

00:12:25,360 --> 00:12:29,560
add some work here so this is just some

00:12:27,550 --> 00:12:34,240
made-up stuff where you know each time

00:12:29,560 --> 00:12:36,040
you access a number in this area you

00:12:34,240 --> 00:12:37,690
just do some computation yeah so you do

00:12:36,040 --> 00:12:39,490
like a multiplication in addition and a

00:12:37,690 --> 00:12:40,660
hash and a square which it doesn't

00:12:39,490 --> 00:12:42,310
actually mean anything it just means

00:12:40,660 --> 00:12:46,270
that you keep the CPU busy so it's going

00:12:42,310 --> 00:12:48,070
to compute stuff every time and then

00:12:46,270 --> 00:12:49,240
let's do the same benchmark let's loop

00:12:48,070 --> 00:12:50,890
through the array the right way around

00:12:49,240 --> 00:12:54,850
at the wrong way around and now it's

00:12:50,890 --> 00:12:56,950
going to look like this and I think this

00:12:54,850 --> 00:12:59,410
is interesting because you see that for

00:12:56,950 --> 00:13:01,690
most so again the every size increases

00:12:59,410 --> 00:13:02,950
if you go to the right and I think this

00:13:01,690 --> 00:13:06,240
is interesting because for most of it

00:13:02,950 --> 00:13:06,240
you don't see a difference here right

00:13:08,100 --> 00:13:14,860
but then only if you go to very large

00:13:11,860 --> 00:13:16,030
every size this all of a sudden the

00:13:14,860 --> 00:13:16,510
version where you loop the wrong way

00:13:16,030 --> 00:13:18,130
around

00:13:16,510 --> 00:13:22,089
the dashed one

00:13:18,130 --> 00:13:23,410
gets lower and basically here you can

00:13:22,089 --> 00:13:24,850
you can draw a line here and I think

00:13:23,410 --> 00:13:28,269
this this is really important where

00:13:24,850 --> 00:13:29,649
basically to the left of that line the

00:13:28,269 --> 00:13:31,029
execution time if your program is bound

00:13:29,649 --> 00:13:33,699
by the computation that you're doing

00:13:31,029 --> 00:13:35,440
right so basically the hash in the

00:13:33,699 --> 00:13:36,699
square root and you know whatever under

00:13:35,440 --> 00:13:40,839
number quantity you would do in your

00:13:36,699 --> 00:13:43,000
program so it's determined by how fast

00:13:40,839 --> 00:13:46,480
the CPU computes these numbers whereas

00:13:43,000 --> 00:13:47,800
to the right of the of this line the

00:13:46,480 --> 00:13:49,509
execution time of your program is

00:13:47,800 --> 00:13:51,880
entirely determined by how fast you can

00:13:49,509 --> 00:13:53,380
get this data from the cache yeah

00:13:51,880 --> 00:13:56,800
because you have to retrieve it from the

00:13:53,380 --> 00:13:59,649
cache here and it doesn't really matter

00:13:56,800 --> 00:14:01,209
anymore how fast the computation is

00:13:59,649 --> 00:14:03,339
because you're bound by the data access

00:14:01,209 --> 00:14:06,040
and you can see this even more clearly

00:14:03,339 --> 00:14:08,019
if you if you overlay these two yeah so

00:14:06,040 --> 00:14:09,459
let's the black ones are the ones where

00:14:08,019 --> 00:14:11,199
we just loop through the array we don't

00:14:09,459 --> 00:14:13,509
do anything apart from one addition and

00:14:11,199 --> 00:14:15,940
the red ones are where we do all this

00:14:13,509 --> 00:14:17,410
like number crunching stuff on top then

00:14:15,940 --> 00:14:18,940
you see that if you have small areas

00:14:17,410 --> 00:14:21,339
then obviously the number crunching one

00:14:18,940 --> 00:14:23,709
it's much slower but as you get to the

00:14:21,339 --> 00:14:25,930
right and the error gets bigger the

00:14:23,709 --> 00:14:29,439
version where you have the bad way of

00:14:25,930 --> 00:14:31,689
traversing the data if you draw this

00:14:29,439 --> 00:14:32,920
line again actually the version where

00:14:31,689 --> 00:14:34,089
you compute a lot of stuff in the

00:14:32,920 --> 00:14:36,310
version where you don't compute anything

00:14:34,089 --> 00:14:38,110
and you just access the data they they

00:14:36,310 --> 00:14:39,819
run with the same speed because you're

00:14:38,110 --> 00:14:42,939
not bound by your computation time

00:14:39,819 --> 00:14:44,920
you're bound by your data access and I

00:14:42,939 --> 00:14:47,620
think that that's really important to

00:14:44,920 --> 00:14:49,029
keep in mind especially if you do have a

00:14:47,620 --> 00:14:51,029
performance problem in your code you

00:14:49,029 --> 00:14:56,259
know what what do people do people

00:14:51,029 --> 00:14:58,720
people profile so but then this is not

00:14:56,259 --> 00:15:00,519
always helpful um if you do a time

00:14:58,720 --> 00:15:03,939
profile for example what most people do

00:15:00,519 --> 00:15:06,610
so if you time profile this code the the

00:15:03,939 --> 00:15:08,769
red one so this is for example what what

00:15:06,610 --> 00:15:10,959
xcode shows you um i mean we can do the

00:15:08,769 --> 00:15:13,420
same thing in Visual Studio um it would

00:15:10,959 --> 00:15:16,060
look very very similar so this time

00:15:13,420 --> 00:15:19,230
profile tells you that no unsurprisingly

00:15:16,060 --> 00:15:21,910
you spend 100% of your time in main and

00:15:19,230 --> 00:15:24,310
then 13 percent of that time you spend

00:15:21,910 --> 00:15:25,600
in this template template stuff down

00:15:24,310 --> 00:15:28,930
there which is essentially the the

00:15:25,600 --> 00:15:31,270
square root so the hash the hash one is

00:15:28,930 --> 00:15:33,550
the other one is missing here right

00:15:31,270 --> 00:15:35,880
so you can say okay you know maybe the

00:15:33,550 --> 00:15:37,839
the compiler has in line the hash

00:15:35,880 --> 00:15:39,279
because that's the only other thing

00:15:37,839 --> 00:15:43,450
that's computation the expensive right

00:15:39,279 --> 00:15:45,520
that's going on there so you know what

00:15:43,450 --> 00:15:48,459
you do is you go off and you optimize

00:15:45,520 --> 00:15:49,510
your hash function yeah and then after

00:15:48,459 --> 00:15:51,220
we could come back with the hash

00:15:49,510 --> 00:15:52,690
function that's two times faster and you

00:15:51,220 --> 00:15:54,550
run your program again and you realize

00:15:52,690 --> 00:15:56,709
it runs just as slow as I did before

00:15:54,550 --> 00:15:57,940
that because it doesn't matter how fast

00:15:56,709 --> 00:16:01,209
your computation is because you're bound

00:15:57,940 --> 00:16:02,860
by the data access and the worst thing

00:16:01,209 --> 00:16:04,390
about this is that these on time

00:16:02,860 --> 00:16:06,670
profilers they don't show you this thing

00:16:04,390 --> 00:16:09,940
they show you how much time you spend in

00:16:06,670 --> 00:16:11,440
a function um then we're showing you

00:16:09,940 --> 00:16:13,060
that you actually spent most of the time

00:16:11,440 --> 00:16:13,720
waiting for data yeah they're not

00:16:13,060 --> 00:16:16,000
telling you that

00:16:13,720 --> 00:16:18,580
and for example Xcode does have a

00:16:16,000 --> 00:16:21,130
counter somewhere it's quite hidden away

00:16:18,580 --> 00:16:24,310
away you can tell you how many cache

00:16:21,130 --> 00:16:26,980
misses per second you have but you know

00:16:24,310 --> 00:16:29,080
again that's a number yeah so 1 million

00:16:26,980 --> 00:16:34,029
cache misses per second no is that a lot

00:16:29,080 --> 00:16:35,920
I don't know so it's really difficult to

00:16:34,029 --> 00:16:37,630
I find it very difficult to actually

00:16:35,920 --> 00:16:40,390
profile that stuff and and find out

00:16:37,630 --> 00:16:42,339
what's going on and the key to all this

00:16:40,390 --> 00:16:44,470
is you know in this case you have the

00:16:42,339 --> 00:16:45,640
right data structure and the right to

00:16:44,470 --> 00:16:47,950
access the data that you need in the

00:16:45,640 --> 00:16:50,680
right order so if you want to traverse a

00:16:47,950 --> 00:16:52,660
2d array it's going to be in that you're

00:16:50,680 --> 00:16:54,730
going to traverse it in this way like

00:16:52,660 --> 00:16:57,310
you just scan sequentially through the

00:16:54,730 --> 00:16:59,410
memory if you have an image file for

00:16:57,310 --> 00:17:01,329
example and you want to you want to edit

00:16:59,410 --> 00:17:04,089
an image file maybe you want to edit

00:17:01,329 --> 00:17:06,069
like always you want to always edit some

00:17:04,089 --> 00:17:07,959
sections that some parts in that image

00:17:06,069 --> 00:17:09,910
file maybe that's not the most optimal

00:17:07,959 --> 00:17:11,110
way to store an image maybe it will be

00:17:09,910 --> 00:17:13,480
more something like that where you have

00:17:11,110 --> 00:17:15,339
like tires yeah so every time you you

00:17:13,480 --> 00:17:18,370
change something in a tile that's going

00:17:15,339 --> 00:17:20,559
to be sort of close together in space in

00:17:18,370 --> 00:17:21,760
the memory or maybe for another problem

00:17:20,559 --> 00:17:24,579
you know this would be the best way of

00:17:21,760 --> 00:17:25,929
traversing your data it doesn't actually

00:17:24,579 --> 00:17:28,569
matter that much it doesn't have to be

00:17:25,929 --> 00:17:31,750
the best it just has to be good enough

00:17:28,569 --> 00:17:33,700
so you know you're not bound by you know

00:17:31,750 --> 00:17:34,990
waiting for that data to be fetched so

00:17:33,700 --> 00:17:39,460
that's that's the thing you really want

00:17:34,990 --> 00:17:42,280
and that leads to things like for

00:17:39,460 --> 00:17:44,780
example if you have this is this is a

00:17:42,280 --> 00:17:46,850
made-up example yeah so ah if you

00:17:44,780 --> 00:17:50,210
have two different classes like foo and

00:17:46,850 --> 00:17:52,910
bar and you know you have areas of them

00:17:50,210 --> 00:17:54,620
and you tend to use always one foo and a

00:17:52,910 --> 00:17:56,390
bar together in a function you have this

00:17:54,620 --> 00:17:58,700
do something function there then

00:17:56,390 --> 00:18:00,410
probably it's a better idea to actually

00:17:58,700 --> 00:18:02,270
stick that fluent that bar into one

00:18:00,410 --> 00:18:05,090
struck so they come just one after the

00:18:02,270 --> 00:18:08,000
other in memory and then then use that

00:18:05,090 --> 00:18:10,400
and then you know the data that you

00:18:08,000 --> 00:18:12,740
actually use is going to be close

00:18:10,400 --> 00:18:14,420
together in memory and then it's less

00:18:12,740 --> 00:18:16,490
likely that your program is going to be

00:18:14,420 --> 00:18:18,590
bound by the time it takes to fetch the

00:18:16,490 --> 00:18:22,700
right data and then you can go off and

00:18:18,590 --> 00:18:24,560
optimize your actual computation the

00:18:22,700 --> 00:18:27,290
same holds in time it's something that

00:18:24,560 --> 00:18:28,640
we call temporal cache coherency so you

00:18:27,290 --> 00:18:30,410
know if you use the green piece of data

00:18:28,640 --> 00:18:33,050
here and then the red and blue one and

00:18:30,410 --> 00:18:34,850
then you know the green one again by the

00:18:33,050 --> 00:18:37,570
time you do that maybe it might have

00:18:34,850 --> 00:18:39,950
been already evicted from the cache so

00:18:37,570 --> 00:18:41,540
what you ideally want to do is you want

00:18:39,950 --> 00:18:42,800
to you know work with the green data and

00:18:41,540 --> 00:18:45,410
then work with the red data and then

00:18:42,800 --> 00:18:46,970
work with the blue data and basically if

00:18:45,410 --> 00:18:50,450
you generate some data use it

00:18:46,970 --> 00:18:52,280
immediately and not let it hang in there

00:18:50,450 --> 00:18:53,480
in the cache and hoping that it's going

00:18:52,280 --> 00:18:58,880
to it's going to still be around later

00:18:53,480 --> 00:19:02,030
on because maybe it won't okay one last

00:18:58,880 --> 00:19:03,980
thing about traversing arrays so we saw

00:19:02,030 --> 00:19:06,230
our you know we can do it this way which

00:19:03,980 --> 00:19:09,740
is the correct way we can you can jump

00:19:06,230 --> 00:19:12,680
around what about if we jump around

00:19:09,740 --> 00:19:14,360
randomly yeah because this is often how

00:19:12,680 --> 00:19:19,820
how it looks like if you traversing a

00:19:14,360 --> 00:19:22,040
stood list or a still map so at last

00:19:19,820 --> 00:19:24,350
last year meeting C++ there was a great

00:19:22,040 --> 00:19:26,740
talk by Klaus Eagleburger it's called

00:19:24,350 --> 00:19:29,060
obtaining the performance beast where he

00:19:26,740 --> 00:19:31,520
benchmarks a lot of different STL

00:19:29,060 --> 00:19:34,100
containers so I can really recommend

00:19:31,520 --> 00:19:37,990
this talk this is great I'm not going to

00:19:34,100 --> 00:19:41,630
talk a lot about STL containers here

00:19:37,990 --> 00:19:43,370
just one thing you know basically to

00:19:41,630 --> 00:19:45,260
avoid this situation if you have if you

00:19:43,370 --> 00:19:46,550
have to use like a list or a map you

00:19:45,260 --> 00:19:48,020
know don't keep inserting and deleting

00:19:46,550 --> 00:19:50,420
because then you're going to end up with

00:19:48,020 --> 00:19:52,940
the structure looks like this but it's

00:19:50,420 --> 00:19:54,680
always better to you know allocate the

00:19:52,940 --> 00:19:56,300
nodes close together initially and then

00:19:54,680 --> 00:19:57,220
reuse those nodes rather than inserting

00:19:56,300 --> 00:19:58,990
entity

00:19:57,220 --> 00:20:04,480
you can do that for example by writing a

00:19:58,990 --> 00:20:06,670
custom educator but yeah let's not talk

00:20:04,480 --> 00:20:09,180
about list and map here let's talk about

00:20:06,670 --> 00:20:11,560
arrays because then when you're doing um

00:20:09,180 --> 00:20:13,150
you know scientific company was doing

00:20:11,560 --> 00:20:14,860
scientific computing we were doing you

00:20:13,150 --> 00:20:16,630
were using arrays most of the time and

00:20:14,860 --> 00:20:19,000
now I mean audio technology and we use

00:20:16,630 --> 00:20:21,460
areas most of the time so it's really

00:20:19,000 --> 00:20:22,990
important to understand arrays so I want

00:20:21,460 --> 00:20:25,330
to model this thing using the same the

00:20:22,990 --> 00:20:27,820
same array and so basically the next

00:20:25,330 --> 00:20:31,930
example is where the third one here

00:20:27,820 --> 00:20:33,430
where I do the same thing but except for

00:20:31,930 --> 00:20:35,800
just traversing the array the wrong way

00:20:33,430 --> 00:20:40,360
around I'm actually accessing random

00:20:35,800 --> 00:20:42,040
random elements each time now and you

00:20:40,360 --> 00:20:43,870
might think that the second and the

00:20:42,040 --> 00:20:44,980
third would be probably simp similar in

00:20:43,870 --> 00:20:47,230
terms of performance because they're

00:20:44,980 --> 00:20:48,730
both jumping around a lot and they're

00:20:47,230 --> 00:20:50,020
just not accessing the data in the right

00:20:48,730 --> 00:20:51,790
way and you're getting cache misses all

00:20:50,020 --> 00:20:53,310
the time and that is true but if you

00:20:51,790 --> 00:20:57,130
benchmark it it turns out that actually

00:20:53,310 --> 00:20:58,990
the the random one is much worse even

00:20:57,130 --> 00:21:01,810
than the the wrong column major order

00:20:58,990 --> 00:21:03,370
one and actually this factor like so the

00:21:01,810 --> 00:21:05,980
random is more than a hundred times

00:21:03,370 --> 00:21:08,710
slower than if you accept it access the

00:21:05,980 --> 00:21:10,510
array in the right order and actually if

00:21:08,710 --> 00:21:13,630
you look at how fast caches are and how

00:21:10,510 --> 00:21:15,070
the latency of a cache is this 100

00:21:13,630 --> 00:21:17,500
something factors actually what you

00:21:15,070 --> 00:21:19,990
would expect yeah so this is this is

00:21:17,500 --> 00:21:24,670
actually the fracture how much slower it

00:21:19,990 --> 00:21:27,130
is if you have a bad data structure but

00:21:24,670 --> 00:21:30,270
now the question is why why is the red

00:21:27,130 --> 00:21:32,530
one not quite as bad as the blue one and

00:21:30,270 --> 00:21:34,390
you know there are probably different

00:21:32,530 --> 00:21:37,390
reasons for this but one reasons for one

00:21:34,390 --> 00:21:39,400
reason for this is that even though the

00:21:37,390 --> 00:21:42,280
red one is a very bad while you're

00:21:39,400 --> 00:21:44,320
fitting an array you still use the same

00:21:42,280 --> 00:21:46,000
stride every time right so you jump but

00:21:44,320 --> 00:21:48,580
you jump by the same amount of bytes

00:21:46,000 --> 00:21:53,020
each time in the in the column-major

00:21:48,580 --> 00:21:55,840
example at the red one now modern

00:21:53,020 --> 00:21:59,910
hardware has a thing which is called the

00:21:55,840 --> 00:22:03,520
prefetcher which is helping a lot with

00:21:59,910 --> 00:22:07,150
optimizing performance in this case

00:22:03,520 --> 00:22:09,100
without even people noticing so what the

00:22:07,150 --> 00:22:10,540
prefecture does is it's the thing that

00:22:09,100 --> 00:22:13,630
sits between the CPU

00:22:10,540 --> 00:22:14,890
in the cache and it just listens to you

00:22:13,630 --> 00:22:16,810
know all the traffic that goes on

00:22:14,890 --> 00:22:18,640
between those two and whenever it

00:22:16,810 --> 00:22:21,760
notices a pattern as for example you

00:22:18,640 --> 00:22:23,530
using a constant stride it sort of

00:22:21,760 --> 00:22:27,130
latches onto that pattern and starts

00:22:23,530 --> 00:22:29,050
prefetching things now and this is why

00:22:27,130 --> 00:22:31,960
if you use the regular pattern to access

00:22:29,050 --> 00:22:33,790
data even if it's not you know even if

00:22:31,960 --> 00:22:37,780
you expect it to be slow the prefetcher

00:22:33,790 --> 00:22:39,670
can still speed it up somewhat and so

00:22:37,780 --> 00:22:41,380
that's a very interesting sort of like

00:22:39,670 --> 00:22:43,590
hardware a technique to speed things up

00:22:41,380 --> 00:22:45,940
and I read somewhere a number that in

00:22:43,590 --> 00:22:47,560
practice that can actually speed up code

00:22:45,940 --> 00:22:49,090
about ten to thirty percent and like

00:22:47,560 --> 00:22:52,900
typical high-performance applications

00:22:49,090 --> 00:22:55,030
like that so that so that's really

00:22:52,900 --> 00:22:57,280
interesting I think that actually the

00:22:55,030 --> 00:23:03,940
hardware helps you here and you don't

00:22:57,280 --> 00:23:06,880
even notice it so let's talk about one

00:23:03,940 --> 00:23:08,890
one last example about the cache here so

00:23:06,880 --> 00:23:11,110
let's go to this case where we have an

00:23:08,890 --> 00:23:13,200
array and it's a big array and then you

00:23:11,110 --> 00:23:15,790
access every nth element in the array

00:23:13,200 --> 00:23:18,820
but not now what I want to do now is I

00:23:15,790 --> 00:23:20,980
want to basically loop through this loop

00:23:18,820 --> 00:23:23,860
and I want to access every second or

00:23:20,980 --> 00:23:28,420
every fourth or every sixth item in the

00:23:23,860 --> 00:23:29,980
and the array but the amount of int I

00:23:28,420 --> 00:23:31,810
want to access is the same time so I

00:23:29,980 --> 00:23:34,000
want to access ten ten thousand ins

00:23:31,810 --> 00:23:35,260
every time but sometimes it's going to

00:23:34,000 --> 00:23:38,950
be every second in some time it's going

00:23:35,260 --> 00:23:41,290
to be every every twentieth in yeah and

00:23:38,950 --> 00:23:42,550
you would expect that you know at least

00:23:41,290 --> 00:23:44,830
if every end is on a different cache

00:23:42,550 --> 00:23:47,110
line you know ten thousand accesses

00:23:44,830 --> 00:23:53,050
would take more or less the same amount

00:23:47,110 --> 00:23:56,680
of time yeah well it turns out that is

00:23:53,050 --> 00:23:59,140
the case if you touch every sixteenth

00:23:56,680 --> 00:24:02,140
and yeah it's taking the same amount of

00:23:59,140 --> 00:24:05,410
time is if you touch every 128th and but

00:24:02,140 --> 00:24:07,480
when you get to 256 as you stride all of

00:24:05,410 --> 00:24:11,920
a sudden performance drops by a factor

00:24:07,480 --> 00:24:13,600
of twenty yeah and then and then it goes

00:24:11,920 --> 00:24:17,530
down again and it's fine again

00:24:13,600 --> 00:24:21,460
until you get to 512 and then it again

00:24:17,530 --> 00:24:22,600
drops by a factor of 20 so yeah when I

00:24:21,460 --> 00:24:24,220
first saw this I found this really

00:24:22,600 --> 00:24:26,620
surprising um

00:24:24,220 --> 00:24:28,480
because for some reason choosing a

00:24:26,620 --> 00:24:30,850
stride of a power of two seems to be a

00:24:28,480 --> 00:24:32,759
very bad idea and I think this an

00:24:30,850 --> 00:24:34,929
intuitive in a way because you know

00:24:32,759 --> 00:24:40,059
computers are really good with powers of

00:24:34,929 --> 00:24:54,820
two right so who can tell me why this

00:24:40,059 --> 00:24:56,230
happens right exactly um so it's not so

00:24:54,820 --> 00:24:57,669
every end is on a different cache on

00:24:56,230 --> 00:25:01,539
anyway but what you're referring to is

00:24:57,669 --> 00:25:03,370
called cache associativity so let me try

00:25:01,539 --> 00:25:04,870
to explain that in one minute let's see

00:25:03,370 --> 00:25:07,509
let's see how good how good I can do

00:25:04,870 --> 00:25:10,870
this imagine all your bytes in memory

00:25:07,509 --> 00:25:12,879
are cars and you have a hundred thousand

00:25:10,870 --> 00:25:15,639
of them and that's that's your main

00:25:12,879 --> 00:25:18,940
memory and then every car has a license

00:25:15,639 --> 00:25:20,019
plate that's the memory address now so

00:25:18,940 --> 00:25:22,629
you have a hundred thousand memory

00:25:20,019 --> 00:25:25,480
addresses and then your cache you have a

00:25:22,629 --> 00:25:28,659
parking lot with ten parking spaces

00:25:25,480 --> 00:25:31,529
yeah that's your cache so you can park

00:25:28,659 --> 00:25:33,399
ten cars in this in this cache now

00:25:31,529 --> 00:25:36,429
there's different ways to build this

00:25:33,399 --> 00:25:37,750
cache one way is to say any car can park

00:25:36,429 --> 00:25:39,120
anywhere on that parking lot

00:25:37,750 --> 00:25:41,620
that's called a fully associative cache

00:25:39,120 --> 00:25:46,090
that's really good because we flexible

00:25:41,620 --> 00:25:47,440
you don't suffer from you know cache

00:25:46,090 --> 00:25:49,509
line misses that much because you can

00:25:47,440 --> 00:25:51,399
really optimize the you can really use

00:25:49,509 --> 00:25:54,309
the cash on a very very optimized way

00:25:51,399 --> 00:25:56,350
but the problem is that it's difficult

00:25:54,309 --> 00:25:58,419
to build them or they require more

00:25:56,350 --> 00:26:00,250
hardware so they're going to be more

00:25:58,419 --> 00:26:01,480
expensive and also they're going to be

00:26:00,250 --> 00:26:03,279
slower because they require more

00:26:01,480 --> 00:26:05,259
circuitry right the other way if

00:26:03,279 --> 00:26:07,539
building a cache is what's called a

00:26:05,259 --> 00:26:10,059
direct mapped cache where you can say

00:26:07,539 --> 00:26:11,919
okay of like if you have a car and the

00:26:10,059 --> 00:26:14,860
last digit is 2 it can only go to the

00:26:11,919 --> 00:26:17,440
parking space with a number 2 you have a

00:26:14,860 --> 00:26:19,659
car and the license plate ends on 3 it

00:26:17,440 --> 00:26:22,029
can go only to the space now label 3 and

00:26:19,659 --> 00:26:23,320
this is much easier to build yeah

00:26:22,029 --> 00:26:26,379
because it's much easier to build a

00:26:23,320 --> 00:26:28,210
silicon to route every like date piece

00:26:26,379 --> 00:26:29,350
of data to to its right place so it's

00:26:28,210 --> 00:26:31,299
going to be faster it's going to be

00:26:29,350 --> 00:26:33,730
cheaper the problem is that it's going

00:26:31,299 --> 00:26:35,799
to suffer from more cache misses now

00:26:33,730 --> 00:26:36,550
because you can't always keep the things

00:26:35,799 --> 00:26:40,090
in

00:26:36,550 --> 00:26:41,920
- if you need to so most desktop

00:26:40,090 --> 00:26:45,070
machines have like a compromise kind of

00:26:41,920 --> 00:26:46,750
thing where which is called an n way set

00:26:45,070 --> 00:26:49,210
associative cache where you divide the

00:26:46,750 --> 00:26:52,390
cache and sets and every cache every set

00:26:49,210 --> 00:26:53,920
individually will be a we be like like

00:26:52,390 --> 00:26:55,570
the direct mapped cache so in this case

00:26:53,920 --> 00:26:59,110
you have two sets so every car can go

00:26:55,570 --> 00:27:02,710
into two slots now now if you do the

00:26:59,110 --> 00:27:03,940
math for example on your machine you can

00:27:02,710 --> 00:27:05,770
actually look it up there's like always

00:27:03,940 --> 00:27:08,110
like a command line thing you can run to

00:27:05,770 --> 00:27:09,490
find out about your cache so let's say

00:27:08,110 --> 00:27:12,880
for example you have a level two cache

00:27:09,490 --> 00:27:15,250
which is 128 kilobytes and it's 8 way

00:27:12,880 --> 00:27:16,780
associative yeah that's fairly typical I

00:27:15,250 --> 00:27:20,710
think so it means that you have 8 sets

00:27:16,780 --> 00:27:24,100
and each set is 16 kilobytes big now

00:27:20,710 --> 00:27:27,130
imagine that you have 20 numbers 20 ins

00:27:24,100 --> 00:27:30,460
and there each of them is like the 16

00:27:27,130 --> 00:27:32,950
kilobytes apart and now you want to

00:27:30,460 --> 00:27:34,390
access those 20 ins it's not much yeah

00:27:32,950 --> 00:27:35,950
it's just 20 numbers but it turns out

00:27:34,390 --> 00:27:38,260
that those 20 numbers end up competing

00:27:35,950 --> 00:27:40,150
for the same eight slots in the cache

00:27:38,260 --> 00:27:42,910
there's only eight slots where these 20

00:27:40,150 --> 00:27:44,230
numbers can go ah which means that

00:27:42,910 --> 00:27:47,920
they're just going to keep evicting each

00:27:44,230 --> 00:27:49,300
other out all the time and you're going

00:27:47,920 --> 00:27:50,500
to get a cache miss on every single one

00:27:49,300 --> 00:27:53,500
of them and this is why the performance

00:27:50,500 --> 00:27:57,550
goes down by a factor of 20 so so this

00:27:53,500 --> 00:27:59,170
is this is cache associativity um I

00:27:57,550 --> 00:28:01,810
found a web page on the internet which

00:27:59,170 --> 00:28:04,210
is called a gallery of cache effects so

00:28:01,810 --> 00:28:05,440
by a guy called eager Ostrovsky so I can

00:28:04,210 --> 00:28:06,790
really recommend you to Google that

00:28:05,440 --> 00:28:09,250
that's pretty cool so his examples are

00:28:06,790 --> 00:28:11,800
in c-sharp so not all of that translates

00:28:09,250 --> 00:28:14,830
to c++ the same way but there's like a

00:28:11,800 --> 00:28:17,560
lot of more a lot more cache effects

00:28:14,830 --> 00:28:20,790
there so I think it's quite interesting

00:28:17,560 --> 00:28:23,350
so one last note about the cache is that

00:28:20,790 --> 00:28:24,850
there's a date there's also data caching

00:28:23,350 --> 00:28:25,990
and instruction cache so if you execute

00:28:24,850 --> 00:28:27,910
code and you need to fetch the

00:28:25,990 --> 00:28:29,290
instructions to execute then actually

00:28:27,910 --> 00:28:32,050
they will go to the same kind of

00:28:29,290 --> 00:28:33,730
hierarchy in the cache so the

00:28:32,050 --> 00:28:35,920
instruction cache is not that much of an

00:28:33,730 --> 00:28:37,510
issue in practice because I will

00:28:35,920 --> 00:28:39,460
essentially code that is bad for the

00:28:37,510 --> 00:28:42,640
instruction caches code that you know

00:28:39,460 --> 00:28:43,870
jumps a lot and branches a lot we're

00:28:42,640 --> 00:28:45,340
going to talk about branches later

00:28:43,870 --> 00:28:47,950
because there's actually bigger problems

00:28:45,340 --> 00:28:50,480
there but for now let's talk about

00:28:47,950 --> 00:28:53,350
memory first so

00:28:50,480 --> 00:28:55,310
you've talked about the cash but if the

00:28:53,350 --> 00:28:57,620
the other thing is that if you look at

00:28:55,310 --> 00:29:01,460
memory sort of like that's what I you

00:28:57,620 --> 00:29:04,040
know also that's what I thought for a

00:29:01,460 --> 00:29:07,430
long time is that okay so you have these

00:29:04,040 --> 00:29:09,140
caches but then you know memory are

00:29:07,430 --> 00:29:11,210
essentially these slots in one way or

00:29:09,140 --> 00:29:13,610
another and then you know if you have

00:29:11,210 --> 00:29:15,170
different data types they just they can

00:29:13,610 --> 00:29:16,700
go to these different slots so you know

00:29:15,170 --> 00:29:21,100
if you have an image then it can be

00:29:16,700 --> 00:29:24,080
anywhere anywhere in that memory and

00:29:21,100 --> 00:29:26,570
then actually then I found out that

00:29:24,080 --> 00:29:28,280
that's not actually how memory works

00:29:26,570 --> 00:29:31,100
that's not how memory looks like memory

00:29:28,280 --> 00:29:33,260
looks much more like this now where

00:29:31,100 --> 00:29:35,930
every type has an alignment requirement

00:29:33,260 --> 00:29:37,970
and there is a word size and there's

00:29:35,930 --> 00:29:39,770
these slots yeah for example an int so

00:29:37,970 --> 00:29:41,690
you can find that out in C++ by doing

00:29:39,770 --> 00:29:44,570
align off that's the key word to get the

00:29:41,690 --> 00:29:46,010
alignment requirement so you find that

00:29:44,570 --> 00:29:49,100
you know an int is not only four bytes

00:29:46,010 --> 00:29:52,670
big but it can only be at a position in

00:29:49,100 --> 00:29:55,370
memory which is aligned by four bytes so

00:29:52,670 --> 00:29:57,320
and the word size for example in a

00:29:55,370 --> 00:30:00,190
32-bit machine that would be 4 bytes on

00:29:57,320 --> 00:30:03,170
a 64-bit machine would be 8 bytes and

00:30:00,190 --> 00:30:08,330
and memory is organized in this way now

00:30:03,170 --> 00:30:10,190
and this leads to a curious property

00:30:08,330 --> 00:30:14,210
that for example if you have a struct

00:30:10,190 --> 00:30:15,590
you declare a struct then with different

00:30:14,210 --> 00:30:18,590
members and then these members will be

00:30:15,590 --> 00:30:20,600
laid out in memory aligned like this but

00:30:18,590 --> 00:30:23,210
then if you actually change the order of

00:30:20,600 --> 00:30:25,820
these members then sometimes the size of

00:30:23,210 --> 00:30:29,300
your the size of your strap can change

00:30:25,820 --> 00:30:31,190
yeah so that's the memory layout of a

00:30:29,300 --> 00:30:33,050
struct and the compilers do that for you

00:30:31,190 --> 00:30:35,540
for example clang-clang as a thing

00:30:33,050 --> 00:30:38,120
called F dumb memory layout where you

00:30:35,540 --> 00:30:41,780
can it can display like the memory

00:30:38,120 --> 00:30:42,950
layout of your classes so that's what

00:30:41,780 --> 00:30:44,780
the compiler is going to do you write

00:30:42,950 --> 00:30:48,650
portable C++ code you're always going to

00:30:44,780 --> 00:30:52,580
get this now this is just the way memory

00:30:48,650 --> 00:30:54,530
is organized and actually you can

00:30:52,580 --> 00:30:57,410
override this and this is now platform

00:30:54,530 --> 00:30:59,420
specific you know clang for example has

00:30:57,410 --> 00:31:02,810
attribute packed or I think in visual

00:30:59,420 --> 00:31:04,250
Studios pragma pack where you can tell

00:31:02,810 --> 00:31:06,110
the compiler no don't do that

00:31:04,250 --> 00:31:08,030
do packed structs and sometimes that's

00:31:06,110 --> 00:31:09,650
actually required in different

00:31:08,030 --> 00:31:13,400
applications

00:31:09,650 --> 00:31:15,200
now that really depends on the hardware

00:31:13,400 --> 00:31:19,640
you know what the computer will do with

00:31:15,200 --> 00:31:23,679
this because fundamentally memory is

00:31:19,640 --> 00:31:25,760
organized in this in this aligned way so

00:31:23,679 --> 00:31:28,250
if you force the computer to do

00:31:25,760 --> 00:31:30,080
underline memory access then for example

00:31:28,250 --> 00:31:32,750
one thing could happen is that it just

00:31:30,080 --> 00:31:36,919
can't do that so if you in this case the

00:31:32,750 --> 00:31:38,590
the sort of the read for squares sorry

00:31:36,919 --> 00:31:40,850
the read eight squares are the eight

00:31:38,590 --> 00:31:44,600
bytes of the double and in this case

00:31:40,850 --> 00:31:45,950
like the D and this is now underlined so

00:31:44,600 --> 00:31:48,140
if you want to read or write to that

00:31:45,950 --> 00:31:49,490
then sometimes what what some

00:31:48,140 --> 00:31:51,710
architectures will have to do is they

00:31:49,490 --> 00:31:53,600
have to write read like a two line bytes

00:31:51,710 --> 00:31:58,130
and then eight align by it's after that

00:31:53,600 --> 00:32:00,110
and then you know shift shift both of

00:31:58,130 --> 00:32:01,880
them and then combine them into into the

00:32:00,110 --> 00:32:05,059
double and that's the only way to

00:32:01,880 --> 00:32:07,309
actually access that unaligned double in

00:32:05,059 --> 00:32:10,039
there so for some architecture is going

00:32:07,309 --> 00:32:11,419
to look like that and then if you if you

00:32:10,039 --> 00:32:14,900
need to write it me to do the same thing

00:32:11,419 --> 00:32:18,169
the other way in Reverse now so for some

00:32:14,900 --> 00:32:21,049
architectures they do this for some

00:32:18,169 --> 00:32:22,460
architectures you know they the CPU

00:32:21,049 --> 00:32:24,080
can't do that for you and you would do

00:32:22,460 --> 00:32:25,370
that and so like the operating system go

00:32:24,080 --> 00:32:30,470
to that and software obviously that

00:32:25,370 --> 00:32:31,820
would be slow and that really depends on

00:32:30,470 --> 00:32:33,890
on your architecture as well this is for

00:32:31,820 --> 00:32:36,289
example one reason why reinterpret cast

00:32:33,890 --> 00:32:37,340
is undefined behavior yeah this is

00:32:36,289 --> 00:32:39,409
something that really struggles

00:32:37,340 --> 00:32:41,000
understanding for awhile I thought why

00:32:39,409 --> 00:32:42,500
can't you just you know there's like

00:32:41,000 --> 00:32:44,390
eight bytes there why can't you just

00:32:42,500 --> 00:32:47,210
take them and interpret them as a double

00:32:44,390 --> 00:32:48,799
why is that undefined behavior like just

00:32:47,210 --> 00:32:51,950
just do it that's what computers do they

00:32:48,799 --> 00:32:53,600
interpret bits and bytes right well it

00:32:51,950 --> 00:32:55,220
turns out that you know one of the

00:32:53,600 --> 00:32:57,289
reasons an event behavior said they

00:32:55,220 --> 00:33:00,500
might not be aligned so that you can

00:32:57,289 --> 00:33:02,090
actually read double from it you know

00:33:00,500 --> 00:33:03,890
the other reason why our interpret cost

00:33:02,090 --> 00:33:06,470
and if I behavior is aliasing but that's

00:33:03,890 --> 00:33:09,110
another topic but basically I mean this

00:33:06,470 --> 00:33:10,460
is is important and we will come back to

00:33:09,110 --> 00:33:13,159
that so it's an it's an important

00:33:10,460 --> 00:33:15,049
concept so I did a benchmark with that

00:33:13,159 --> 00:33:17,030
as well I'm just going to skip skip over

00:33:15,049 --> 00:33:18,410
the benchmark because the details are an

00:33:17,030 --> 00:33:21,020
important

00:33:18,410 --> 00:33:23,330
but actually I found that on this

00:33:21,020 --> 00:33:26,500
machine here which is a modern MacBook

00:33:23,330 --> 00:33:29,440
um actually that's not an issue like the

00:33:26,500 --> 00:33:32,570
newest generation of these Intel

00:33:29,440 --> 00:33:35,080
machines they just do unaligned access

00:33:32,570 --> 00:33:37,910
and island access with the same speed

00:33:35,080 --> 00:33:39,320
actually the red one is too packed the

00:33:37,910 --> 00:33:40,520
underlined one actually it was in this

00:33:39,320 --> 00:33:43,040
particular benchmark it was a bit faster

00:33:40,520 --> 00:33:44,480
because the structure and they was was

00:33:43,040 --> 00:33:46,760
more compact yeah because it was packed

00:33:44,480 --> 00:33:49,100
but then if I ran the same test on on

00:33:46,760 --> 00:33:50,419
the slightly older MacBook with the Core

00:33:49,100 --> 00:33:52,309
2 Duo architecture I found that

00:33:50,419 --> 00:33:54,799
unaligned memory access actually is

00:33:52,309 --> 00:33:56,840
three times slower which is sort of

00:33:54,799 --> 00:33:59,150
dislike to reads and then the

00:33:56,840 --> 00:34:00,890
combination so that's the massive

00:33:59,150 --> 00:34:02,630
performance impact and that that was the

00:34:00,890 --> 00:34:04,250
exact same binary just running on two

00:34:02,630 --> 00:34:09,050
different Mac books yeah three times

00:34:04,250 --> 00:34:10,760
slower in one case so basically the

00:34:09,050 --> 00:34:12,139
messages here alignment is important

00:34:10,760 --> 00:34:13,520
we'll come back to that let's look at

00:34:12,139 --> 00:34:15,320
something else for let's look at

00:34:13,520 --> 00:34:19,340
something more fun because I think

00:34:15,320 --> 00:34:21,200
alignment is is maybe a bit dry let's

00:34:19,340 --> 00:34:24,320
look at this example here so that that's

00:34:21,200 --> 00:34:28,399
something else now so we have a vector

00:34:24,320 --> 00:34:30,290
of floats 32,000 of them so that it fits

00:34:28,399 --> 00:34:33,919
into the cache and then I'm going to

00:34:30,290 --> 00:34:35,690
fill them with random numbers either

00:34:33,919 --> 00:34:37,220
minus 1 or 1 yeah that's what this to

00:34:35,690 --> 00:34:39,619
generate does so I'm going to take a

00:34:37,220 --> 00:34:41,629
vector and fill it with minus 1 1 1 1

00:34:39,619 --> 00:34:45,409
minus 1 minus 1 1 minus 1 1 minus 1

00:34:41,629 --> 00:34:48,050
random yeah and then this really silly

00:34:45,409 --> 00:34:49,490
coded by the way um so then what I want

00:34:48,050 --> 00:34:51,350
to do now is I want to count how many

00:34:49,490 --> 00:34:53,570
ones I have so how many elements are

00:34:51,350 --> 00:34:59,390
bigger than bigger than than 0 yeah

00:34:53,570 --> 00:35:01,960
that's the counted so so that's the code

00:34:59,390 --> 00:35:07,010
and now the thing that I going to do

00:35:01,960 --> 00:35:11,960
next is I'm going to sort the array

00:35:07,010 --> 00:35:13,609
before I do it and obviously and so now

00:35:11,960 --> 00:35:15,920
I'm going to measure how long the count

00:35:13,609 --> 00:35:19,070
will take the count if and obviously

00:35:15,920 --> 00:35:20,480
each time whether we sort it or not it's

00:35:19,070 --> 00:35:21,980
going to be the same number of positive

00:35:20,480 --> 00:35:24,530
elements right so the count if we're

00:35:21,980 --> 00:35:29,700
going to do exactly the same number of

00:35:24,530 --> 00:35:31,920
comparisons and additions in both cases

00:35:29,700 --> 00:35:37,880
but does it make a difference if I sort

00:35:31,920 --> 00:35:45,839
it before yes is it slower or faster

00:35:37,880 --> 00:35:50,490
how much faster 1.5 - so someone thinks

00:35:45,839 --> 00:35:55,920
it's more than that okay who thinks it's

00:35:50,490 --> 00:35:59,250
going to be five times faster okay three

00:35:55,920 --> 00:36:01,560
four people let's see I benchmarked it

00:35:59,250 --> 00:36:03,150
here the results it depends on the

00:36:01,560 --> 00:36:04,140
compiler yeah that's the first one we're

00:36:03,150 --> 00:36:06,570
different compilers give you different

00:36:04,140 --> 00:36:08,970
results so it turns out that I'm clang

00:36:06,570 --> 00:36:11,070
and GCC it doesn't matter and if you do

00:36:08,970 --> 00:36:13,020
it version studio then the sorted one

00:36:11,070 --> 00:36:15,240
runs six times faster yeah that's the

00:36:13,020 --> 00:36:17,250
the red bar at six times shorter so that

00:36:15,240 --> 00:36:19,380
if you don't sort it if it's random it's

00:36:17,250 --> 00:36:22,380
going to be six times slower yeah

00:36:19,380 --> 00:36:24,300
although it's the exact same number of

00:36:22,380 --> 00:36:25,680
instructions you like the exact same

00:36:24,300 --> 00:36:29,099
number of comparisons and additions

00:36:25,680 --> 00:36:30,839
you're executing like why is that okay

00:36:29,099 --> 00:36:32,339
let's look let's look at this loop yeah

00:36:30,839 --> 00:36:36,119
so this is just a basically a fancy way

00:36:32,339 --> 00:36:39,329
of writing a for X if X bigger than zero

00:36:36,119 --> 00:36:43,079
account class bus yeah so what is it

00:36:39,329 --> 00:36:45,000
about this loop so if different

00:36:43,079 --> 00:36:46,530
compilers give you different results one

00:36:45,000 --> 00:36:48,780
way to find out what's going on is to

00:36:46,530 --> 00:36:50,640
look at the assembly output so I did

00:36:48,780 --> 00:36:53,099
that if you look at the visual studio

00:36:50,640 --> 00:36:55,230
one it looks like that where the

00:36:53,099 --> 00:36:57,089
highlighted red section is basically yep

00:36:55,230 --> 00:37:01,470
it compares if it's smaller than zero

00:36:57,089 --> 00:37:05,310
and then if yes it jumps otherwise it

00:37:01,470 --> 00:37:06,780
increases a counter yeah so that's what

00:37:05,310 --> 00:37:09,030
what the the Microsoft compiler

00:37:06,780 --> 00:37:10,920
generates if you look at the GCC one

00:37:09,030 --> 00:37:15,119
it's slightly different it's also

00:37:10,920 --> 00:37:16,349
comparing that X to zero and then

00:37:15,119 --> 00:37:17,339
depending on the result of that

00:37:16,349 --> 00:37:19,680
comparison it's going to do a

00:37:17,339 --> 00:37:21,690
conditional move and so that's the C

00:37:19,680 --> 00:37:24,660
move a instruction and then it's going

00:37:21,690 --> 00:37:26,609
to carry on so there's no jump here and

00:37:24,660 --> 00:37:32,430
then if I look at what clang produces

00:37:26,609 --> 00:37:34,290
then it's some SSE instructions so

00:37:32,430 --> 00:37:37,560
there's even though I think there's no

00:37:34,290 --> 00:37:40,150
counter in there so I this works somehow

00:37:37,560 --> 00:37:43,120
magically this code does what I want

00:37:40,150 --> 00:37:44,770
I'm sure if you stare at it for like an

00:37:43,120 --> 00:37:48,010
hour it would be possible to understand

00:37:44,770 --> 00:37:50,200
how it works but I I wasn't patient

00:37:48,010 --> 00:37:51,540
enough to do that but it does what you

00:37:50,200 --> 00:37:54,160
what you want

00:37:51,540 --> 00:37:59,320
I'm sure someone here can read this

00:37:54,160 --> 00:38:02,340
stuff but anyway they all do the same

00:37:59,320 --> 00:38:04,780
thing now why does the Microsoft version

00:38:02,340 --> 00:38:07,090
have this like difference between sorted

00:38:04,780 --> 00:38:13,870
and unsorted and the reason is to jump

00:38:07,090 --> 00:38:17,470
right so the reason lies in the pipeline

00:38:13,870 --> 00:38:18,730
which is the part of the CPU that does

00:38:17,470 --> 00:38:20,800
your computations where all the

00:38:18,730 --> 00:38:23,230
instructions go through and this is the

00:38:20,800 --> 00:38:25,150
picture from Wikipedia if you just

00:38:23,230 --> 00:38:28,030
google CPU pipeline you're going to see

00:38:25,150 --> 00:38:29,200
that picture so you see how like the

00:38:28,030 --> 00:38:31,510
different instructions goes to this

00:38:29,200 --> 00:38:33,160
pipeline unfortunately in reality it's

00:38:31,510 --> 00:38:34,840
not quite like that because you don't

00:38:33,160 --> 00:38:36,490
have four stages but you have more like

00:38:34,840 --> 00:38:38,410
something like 20 I think and more than

00:38:36,490 --> 00:38:39,910
CPUs and also you don't just have

00:38:38,410 --> 00:38:41,950
instructions but the decomposed further

00:38:39,910 --> 00:38:45,060
into like micro instructions it's really

00:38:41,950 --> 00:38:47,320
really complicated and but anyway

00:38:45,060 --> 00:38:49,870
actually you could you could use that

00:38:47,320 --> 00:38:51,340
benchmark to estimate the length of the

00:38:49,870 --> 00:38:56,710
pipeline but you know that's something

00:38:51,340 --> 00:39:00,430
so so the point here is that basically

00:38:56,710 --> 00:39:01,600
um if you have too blue instruction you

00:39:00,430 --> 00:39:03,340
know it goes through the pipeline and

00:39:01,600 --> 00:39:05,770
the next one is the red one if you have

00:39:03,340 --> 00:39:08,200
a jump basically it depends on the

00:39:05,770 --> 00:39:10,120
result of the blue instruction which one

00:39:08,200 --> 00:39:11,410
will be the red instruction the next one

00:39:10,120 --> 00:39:12,610
so if you look at this pipeline

00:39:11,410 --> 00:39:15,070
obviously this is not going to work

00:39:12,610 --> 00:39:17,140
because by the time you've evaluated the

00:39:15,070 --> 00:39:19,180
result of the blue instruction the red

00:39:17,140 --> 00:39:21,220
instructions already in the pipeline so

00:39:19,180 --> 00:39:23,530
you already need to know no much earlier

00:39:21,220 --> 00:39:25,360
what the next instruction is and if you

00:39:23,530 --> 00:39:28,440
don't know then you have to wait so the

00:39:25,360 --> 00:39:30,880
pipe that has to stall it has to are

00:39:28,440 --> 00:39:32,590
introduced what's called a pipe and

00:39:30,880 --> 00:39:34,170
bubble by you your weight or you do

00:39:32,590 --> 00:39:36,250
something else maybe in the mean time

00:39:34,170 --> 00:39:37,630
but sometimes if you're in a hot loop

00:39:36,250 --> 00:39:39,430
there's nothing else to do so you just

00:39:37,630 --> 00:39:42,580
have to you know wait and that's going

00:39:39,430 --> 00:39:44,590
to hit performance so that's really bad

00:39:42,580 --> 00:39:46,780
for performance so you know one thing

00:39:44,590 --> 00:39:48,310
that hardware people you know do to

00:39:46,780 --> 00:39:50,530
solve this is there is a thing in your

00:39:48,310 --> 00:39:52,060
CPU called a branch predictor which is

00:39:50,530 --> 00:39:53,590
going to guess you know what the next

00:39:52,060 --> 00:39:55,390
instruction is going to be and

00:39:53,590 --> 00:39:57,310
speculatively execute what it thinks the

00:39:55,390 --> 00:39:59,110
next instructions could be and this is

00:39:57,310 --> 00:40:02,440
really really complicated as well how

00:39:59,110 --> 00:40:04,600
this thing works but most of the time it

00:40:02,440 --> 00:40:05,980
works really well and it works

00:40:04,600 --> 00:40:08,920
especially well with things like this

00:40:05,980 --> 00:40:10,720
where you know these are branches that

00:40:08,920 --> 00:40:13,030
are not not problematic like if you had

00:40:10,720 --> 00:40:14,260
a loop and you have a loop condition you

00:40:13,030 --> 00:40:16,480
know it's going to be true all the time

00:40:14,260 --> 00:40:19,090
except the last way the last time around

00:40:16,480 --> 00:40:20,620
or if you do like a check for some error

00:40:19,090 --> 00:40:21,820
and then you return that's something

00:40:20,620 --> 00:40:22,930
that's not most of the time it's not

00:40:21,820 --> 00:40:25,270
going to happen yeah so the branch

00:40:22,930 --> 00:40:27,220
predictor can easily pick up that most

00:40:25,270 --> 00:40:28,540
of the time you know that's the result

00:40:27,220 --> 00:40:31,420
of the condition and it's going to

00:40:28,540 --> 00:40:32,830
insert the right instruction so

00:40:31,420 --> 00:40:35,620
obviously if it fails to predict the

00:40:32,830 --> 00:40:37,720
future then what it has to do is it has

00:40:35,620 --> 00:40:40,120
to basically flush the pipeline fetch

00:40:37,720 --> 00:40:41,380
the right instruction and then you know

00:40:40,120 --> 00:40:43,330
that's going to take a while but most of

00:40:41,380 --> 00:40:44,920
the time that's not going to happen and

00:40:43,330 --> 00:40:46,210
I see branch predictors that even better

00:40:44,920 --> 00:40:48,790
than that they can even support patterns

00:40:46,210 --> 00:40:51,580
these days yeah but one thing they can't

00:40:48,790 --> 00:40:53,620
do is predict random numbers so that's

00:40:51,580 --> 00:40:55,690
the one thing that branch predictors no

00:40:53,620 --> 00:40:57,520
matter how good they are can never do if

00:40:55,690 --> 00:40:59,560
it's really random whether your branch

00:40:57,520 --> 00:41:01,330
is going to be true like where you're

00:40:59,560 --> 00:41:02,860
going to jump then no branch predictor

00:41:01,330 --> 00:41:06,730
in the world can can figure out what to

00:41:02,860 --> 00:41:08,620
do here so for example if I write an

00:41:06,730 --> 00:41:10,360
audio algorithm I'm not going to write

00:41:08,620 --> 00:41:12,040
an if in there yeah I'm going to I'm

00:41:10,360 --> 00:41:13,840
going to use like bitmask operations for

00:41:12,040 --> 00:41:15,310
example floating points you know they

00:41:13,840 --> 00:41:17,250
have a sign bit and you can do some math

00:41:15,310 --> 00:41:21,630
with that so to avoid this kind of

00:41:17,250 --> 00:41:21,630
condition and branch kind of thing

00:41:21,780 --> 00:41:27,100
because there are cases where in real

00:41:24,940 --> 00:41:29,620
world in the real world it does hit

00:41:27,100 --> 00:41:31,750
performance so another another example

00:41:29,620 --> 00:41:34,780
for for when branch has happened our

00:41:31,750 --> 00:41:37,570
virtual function calls yeah so in this

00:41:34,780 --> 00:41:40,960
example we have a very simple sort of

00:41:37,570 --> 00:41:43,030
made up our class a hierarchy yeah we

00:41:40,960 --> 00:41:44,620
have your mammals and dogs and cats I

00:41:43,030 --> 00:41:46,330
think this very popular example from

00:41:44,620 --> 00:41:48,670
computing books and they have a virtual

00:41:46,330 --> 00:41:51,550
function which just returns a different

00:41:48,670 --> 00:41:55,120
number that's very very simple and what

00:41:51,550 --> 00:41:57,070
we do now is we we put bits Givi we have

00:41:55,120 --> 00:41:59,080
like a vector of mammal mammals and then

00:41:57,070 --> 00:41:59,860
we're going to put 10,000 mammals in

00:41:59,080 --> 00:42:01,360
there and then you're going to put

00:41:59,860 --> 00:42:03,100
10,000 dogs and darling when you put

00:42:01,360 --> 00:42:04,810
10,000 cats in there and I'm going to

00:42:03,100 --> 00:42:06,280
loop through all of them and add up the

00:42:04,810 --> 00:42:06,960
different numbers at different virtual

00:42:06,280 --> 00:42:11,310
functions

00:42:06,960 --> 00:42:12,930
right and before we do that so that's

00:42:11,310 --> 00:42:15,150
one way of doing it and then the the

00:42:12,930 --> 00:42:17,580
other way is before we do that we just

00:42:15,150 --> 00:42:19,650
shuffle it just shuffle the array now by

00:42:17,580 --> 00:42:22,110
the way I'm really sad that C++ not

00:42:19,650 --> 00:42:23,700
deprecated random shuffle because four

00:42:22,110 --> 00:42:28,050
things are easy things like that I think

00:42:23,700 --> 00:42:29,460
it's a very handy function but but you

00:42:28,050 --> 00:42:30,780
don't actually care about like whether

00:42:29,460 --> 00:42:36,750
it's in resent whist I just want to

00:42:30,780 --> 00:42:39,330
shuffle stuff but anyway so yeah so if

00:42:36,750 --> 00:42:40,860
you do that it's the same thing right

00:42:39,330 --> 00:42:44,550
you always have ten thousand dogs and

00:42:40,860 --> 00:42:46,470
ten thousand cats and you always have to

00:42:44,550 --> 00:42:48,240
call this like virtual function each

00:42:46,470 --> 00:42:52,800
time around in the loop ah

00:42:48,240 --> 00:42:54,690
and then if you benchmark this it turns

00:42:52,800 --> 00:42:56,820
out that you know if you shuffle them

00:42:54,690 --> 00:42:58,560
you're going to get this branch perform

00:42:56,820 --> 00:42:59,820
branch predictor of performance it again

00:42:58,560 --> 00:43:02,730
but this time you're going to get it in

00:42:59,820 --> 00:43:04,170
all of the compilers because there's no

00:43:02,730 --> 00:43:06,480
way compiler can optimize away your

00:43:04,170 --> 00:43:08,090
virtual func because it's not it's okay

00:43:06,480 --> 00:43:10,320
sometimes it can actually optimize a way

00:43:08,090 --> 00:43:12,270
polymorphism but that's that's most of

00:43:10,320 --> 00:43:14,550
the time that's not possible and the

00:43:12,270 --> 00:43:15,960
compiler is not going to do that so

00:43:14,550 --> 00:43:18,240
you're going to suffer this hit in this

00:43:15,960 --> 00:43:20,280
case and you know that's the thing

00:43:18,240 --> 00:43:21,660
people sometimes claim that virtual

00:43:20,280 --> 00:43:23,610
functions and V tables are bad for

00:43:21,660 --> 00:43:25,500
performance I would say that 99% of the

00:43:23,610 --> 00:43:28,200
time that is not going to be a problem

00:43:25,500 --> 00:43:29,670
but if it is a lot of the time the

00:43:28,200 --> 00:43:32,640
problem will not be the indirection and

00:43:29,670 --> 00:43:34,830
the offset but the additional branch

00:43:32,640 --> 00:43:39,980
yeah that you're introducing by figuring

00:43:34,830 --> 00:43:44,610
out which function to call okay so

00:43:39,980 --> 00:43:46,860
that's about the branch predictor um let

00:43:44,610 --> 00:43:53,330
me jump to the next example

00:43:46,860 --> 00:43:59,360
ah let's see how much time we have left

00:43:53,330 --> 00:43:59,360
fifteen minutes okay

00:44:00,420 --> 00:44:12,000
okay let me skip the arm really quickly

00:44:05,609 --> 00:44:14,400
go through the full sharing one so

00:44:12,000 --> 00:44:16,079
that's that's another thing Cassius

00:44:14,400 --> 00:44:19,980
actually shared between cores right so

00:44:16,079 --> 00:44:22,049
in this example we have atomic counter

00:44:19,980 --> 00:44:23,519
and we have a function work that just

00:44:22,049 --> 00:44:26,220
going to do something silly with that

00:44:23,519 --> 00:44:29,309
counter it's going to increase it and

00:44:26,220 --> 00:44:30,599
then you access this counter from you

00:44:29,309 --> 00:44:35,869
know four different threads in this case

00:44:30,599 --> 00:44:38,910
and they all you'll use this counter and

00:44:35,869 --> 00:44:42,029
then they're all doing the same work and

00:44:38,910 --> 00:44:44,819
then it turns out that the more on the

00:44:42,029 --> 00:44:48,089
more threads you have the slower that

00:44:44,819 --> 00:44:51,180
the thing gets so actually on in this

00:44:48,089 --> 00:44:53,000
case it's actually slower it's actually

00:44:51,180 --> 00:44:55,349
going to be faster to run the threads in

00:44:53,000 --> 00:44:59,700
success like one after the other and to

00:44:55,349 --> 00:45:02,309
run them in parallel and the reason for

00:44:59,700 --> 00:45:04,950
this is that if you have a something

00:45:02,309 --> 00:45:08,640
that's shared between like a piece of

00:45:04,950 --> 00:45:10,559
data shared between two course so you

00:45:08,640 --> 00:45:12,599
know for example you have this variable

00:45:10,559 --> 00:45:15,809
a you know it's present in all these

00:45:12,599 --> 00:45:18,299
different levels and to course access it

00:45:15,809 --> 00:45:21,480
then if one could modifies it you know

00:45:18,299 --> 00:45:22,470
it's going to flush so if it's going to

00:45:21,480 --> 00:45:25,109
be used somewhere else it's going to

00:45:22,470 --> 00:45:26,609
flush that to the to the shared cache

00:45:25,109 --> 00:45:29,430
and then that's going to invalidate the

00:45:26,609 --> 00:45:31,559
copy of a that core to has and the next

00:45:29,430 --> 00:45:33,150
time cord to are wants to access a it's

00:45:31,559 --> 00:45:35,609
going to have a cache miss and it's

00:45:33,150 --> 00:45:39,059
going to have to fetch a from on the

00:45:35,609 --> 00:45:40,440
higher level and that's going to that's

00:45:39,059 --> 00:45:42,119
going to be performance penalty and this

00:45:40,440 --> 00:45:43,890
is this is the reason why why stead

00:45:42,119 --> 00:45:48,809
atomic is sometimes so slower compared

00:45:43,890 --> 00:45:50,910
to mutexes so so this is this is a well

00:45:48,809 --> 00:45:53,730
known thing and but then one interesting

00:45:50,910 --> 00:45:56,460
effect here that you can sometimes

00:45:53,730 --> 00:45:58,079
observe axials in real code is something

00:45:56,460 --> 00:46:03,059
that's called full sharing where in this

00:45:58,079 --> 00:46:04,799
example you have not won a but you have

00:46:03,059 --> 00:46:07,890
four different four different variables

00:46:04,799 --> 00:46:10,079
yeah and each thread works on another

00:46:07,890 --> 00:46:12,420
one of those so you see this no data at

00:46:10,079 --> 00:46:14,039
all shared between the the shared

00:46:12,420 --> 00:46:18,130
between the threads here

00:46:14,039 --> 00:46:21,390
but then if you run this you see that

00:46:18,130 --> 00:46:23,920
still you you suffer this immense

00:46:21,390 --> 00:46:25,359
performance penalty where if you run

00:46:23,920 --> 00:46:26,859
these four threads in parallel it's

00:46:25,359 --> 00:46:28,900
going to be 12 times slower and it's

00:46:26,859 --> 00:46:31,299
actually faster to run them one after

00:46:28,900 --> 00:46:32,499
one after the other although they don't

00:46:31,299 --> 00:46:40,119
share any data at all

00:46:32,499 --> 00:46:41,440
so that's surprising right and there's

00:46:40,119 --> 00:46:42,969
an article by hops at are called

00:46:41,440 --> 00:46:44,499
affective concurrency eliminated for

00:46:42,969 --> 00:46:47,469
sharing right here's like an example of

00:46:44,499 --> 00:46:50,170
like how how this happens actually in

00:46:47,469 --> 00:46:51,700
real code as well and the reason for

00:46:50,170 --> 00:46:54,670
this if you go back to you know

00:46:51,700 --> 00:46:56,380
alignment and how memory is organized it

00:46:54,670 --> 00:46:58,630
turns out that you know if that's what

00:46:56,380 --> 00:47:00,279
memory looks like you know a cache line

00:46:58,630 --> 00:47:01,599
remember data comes in cache lines right

00:47:00,279 --> 00:47:04,150
a cache line is like one of those

00:47:01,599 --> 00:47:05,920
bookshelf right and then memory is

00:47:04,150 --> 00:47:09,069
basically many of those bookshelves and

00:47:05,920 --> 00:47:11,650
then it turns out well if one of them is

00:47:09,069 --> 00:47:13,809
a cache line well guess what it's also

00:47:11,650 --> 00:47:18,819
aligned it's aligned by the sides of a

00:47:13,809 --> 00:47:20,380
cache line which means that in this

00:47:18,819 --> 00:47:21,759
example it turns out that all of these

00:47:20,380 --> 00:47:23,380
four variables there are on the same

00:47:21,759 --> 00:47:25,180
cache line although there are four

00:47:23,380 --> 00:47:27,190
different variables you know in this

00:47:25,180 --> 00:47:29,380
case you know everything that if the

00:47:27,190 --> 00:47:31,359
cache line size is 64 bytes then

00:47:29,380 --> 00:47:34,089
everything with addresses like blah blah

00:47:31,359 --> 00:47:35,920
blah 0:02 blah blah blah 3f it's going

00:47:34,089 --> 00:47:38,049
to be in the same cache line and that

00:47:35,920 --> 00:47:42,039
thing well that works in cache lines

00:47:38,049 --> 00:47:42,759
right so if you have different variables

00:47:42,039 --> 00:47:43,809
but they're on the same cache line

00:47:42,759 --> 00:47:45,999
they're going to stuff for the same

00:47:43,809 --> 00:47:50,109
thing as if you access the same variable

00:47:45,999 --> 00:47:53,499
from different course one way to prevent

00:47:50,109 --> 00:47:57,160
this is to align your data structure by

00:47:53,499 --> 00:47:58,779
the size of the cache line so that you

00:47:57,160 --> 00:48:00,579
know different data that's used by

00:47:58,779 --> 00:48:02,589
different course to force it to be on

00:48:00,579 --> 00:48:04,509
different cache lines and if you do that

00:48:02,589 --> 00:48:06,609
you get the red bars where you see that

00:48:04,509 --> 00:48:10,900
you basically don't have you don't have

00:48:06,609 --> 00:48:14,469
that performance penalty anymore um okay

00:48:10,900 --> 00:48:15,549
let's look at this again so the thing

00:48:14,469 --> 00:48:17,349
that we saw earlier with a branch

00:48:15,549 --> 00:48:18,940
predictor where you don't know which is

00:48:17,349 --> 00:48:21,130
the next instruction to fetch as a thing

00:48:18,940 --> 00:48:23,319
called branch hazard if you're in this

00:48:21,130 --> 00:48:25,809
pipeline here there's another type of

00:48:23,319 --> 00:48:27,380
hazard which is a data hazard which

00:48:25,809 --> 00:48:29,030
means basically

00:48:27,380 --> 00:48:32,990
you have the blue instruction for

00:48:29,030 --> 00:48:34,910
example which goes through and then the

00:48:32,990 --> 00:48:37,640
result of the blue instruction is used

00:48:34,910 --> 00:48:40,910
by the red instruction yeah and you can

00:48:37,640 --> 00:48:42,770
see how how that in this case will also

00:48:40,910 --> 00:48:44,720
be a problem because by the you know the

00:48:42,770 --> 00:48:47,690
red structure has to be fetched and

00:48:44,720 --> 00:48:52,370
executed way before you get the result

00:48:47,690 --> 00:48:53,300
of the instruction before that and this

00:48:52,370 --> 00:48:55,370
is something that's called instruction

00:48:53,300 --> 00:49:00,010
level parallelism and this is one of the

00:48:55,370 --> 00:49:02,180
reasons why CPUs are so fast and

00:49:00,010 --> 00:49:04,280
basically most of time we don't have

00:49:02,180 --> 00:49:06,110
this problem because even though you

00:49:04,280 --> 00:49:07,550
know you write your code such that one

00:49:06,110 --> 00:49:09,980
instruction come after the other first

00:49:07,550 --> 00:49:11,930
the optimizer it's going to see oh

00:49:09,980 --> 00:49:13,730
here's a data dependency there and it's

00:49:11,930 --> 00:49:15,140
going to reorder your instructions yeah

00:49:13,730 --> 00:49:16,580
and then the next thing is that in

00:49:15,140 --> 00:49:18,140
Hardware actually the hardware will also

00:49:16,580 --> 00:49:20,000
reorder your instructions it's another

00:49:18,140 --> 00:49:22,370
whole like level of complexity that's

00:49:20,000 --> 00:49:24,650
going on there but sometimes you know

00:49:22,370 --> 00:49:26,120
the only thing you execute is the blue

00:49:24,650 --> 00:49:28,400
instruction than the red instruction

00:49:26,120 --> 00:49:29,720
then you do that in a hot loop and then

00:49:28,400 --> 00:49:32,600
and then you have a performance problem

00:49:29,720 --> 00:49:36,530
right there so this is one example here

00:49:32,600 --> 00:49:39,590
where um this is a function that takes

00:49:36,530 --> 00:49:43,550
basically string and translates that to

00:49:39,590 --> 00:49:45,650
an integer so that's an example from a

00:49:43,550 --> 00:49:48,830
talk by andrea alexandra school from

00:49:45,650 --> 00:49:51,230
last year called writing fast code which

00:49:48,830 --> 00:49:53,000
i really recommend you to check out so

00:49:51,230 --> 00:49:55,100
the problem here is that it is the only

00:49:53,000 --> 00:49:57,470
thing here that we're doing is to to do

00:49:55,100 --> 00:49:59,720
this thing in a loop right and you see

00:49:57,470 --> 00:50:02,510
that you know every new number depends

00:49:59,720 --> 00:50:04,370
on the result of the previous one so

00:50:02,510 --> 00:50:07,820
that's an example for this thing

00:50:04,370 --> 00:50:09,920
andrea i think he he talks about half an

00:50:07,820 --> 00:50:11,450
hour and his talk about how to how to

00:50:09,920 --> 00:50:12,860
optimize this how to change the

00:50:11,450 --> 00:50:15,770
implementation such that you get rid of

00:50:12,860 --> 00:50:17,510
data dependency and you can make it

00:50:15,770 --> 00:50:19,040
three times faster i'm not going to talk

00:50:17,510 --> 00:50:22,480
about this because he's already done

00:50:19,040 --> 00:50:27,080
that so what's his talk I'm going to

00:50:22,480 --> 00:50:30,650
present another example here where you

00:50:27,080 --> 00:50:31,130
have a loop um and it's a bit of a silly

00:50:30,650 --> 00:50:35,060
loop

00:50:31,130 --> 00:50:36,620
yeah but um you see here that so you

00:50:35,060 --> 00:50:40,650
have three arrays here and then you see

00:50:36,620 --> 00:50:42,880
that if you compute

00:50:40,650 --> 00:50:46,450
so you compute a and then you compute B

00:50:42,880 --> 00:50:48,040
and the next time around for the next

00:50:46,450 --> 00:50:51,640
day you're going to use the previous B

00:50:48,040 --> 00:50:56,700
right so there's a data dependency here

00:50:51,640 --> 00:51:02,440
can you yeah you can see that right now

00:50:56,700 --> 00:51:05,890
that's a problem right um because um you

00:51:02,440 --> 00:51:07,660
compute B and then and then and then you

00:51:05,890 --> 00:51:09,099
keep looping around next time you need

00:51:07,660 --> 00:51:11,680
that so that that's that's a data

00:51:09,099 --> 00:51:15,280
dependency here how can you make this

00:51:11,680 --> 00:51:19,200
better and you want to just a better way

00:51:15,280 --> 00:51:19,200
of writing this exact same same loop

00:51:20,130 --> 00:51:26,319
yeah well but then you're going to have

00:51:24,309 --> 00:51:27,760
two or three loops right so that's

00:51:26,319 --> 00:51:34,030
probably going to be slower than one

00:51:27,760 --> 00:51:35,380
loop yeah so the suggestion was just

00:51:34,030 --> 00:51:35,770
repeat the same thing in the loop three

00:51:35,380 --> 00:51:37,000
times

00:51:35,770 --> 00:51:38,349
that's called loop unrolling that's

00:51:37,000 --> 00:51:39,960
something that the compiler already the

00:51:38,349 --> 00:51:44,020
optimizing compiler only does for you

00:51:39,960 --> 00:51:47,559
but yeah sometimes sometimes it helps

00:51:44,020 --> 00:51:49,450
in this particular case actually the

00:51:47,559 --> 00:51:56,230
best way is to just slightly rewrite it

00:51:49,450 --> 00:51:59,349
like that um we're basically the right

00:51:56,230 --> 00:52:01,240
version is doing the same thing right so

00:51:59,349 --> 00:52:03,670
basically instead of having like the

00:52:01,240 --> 00:52:04,900
first and a second line in one loop and

00:52:03,670 --> 00:52:06,430
then the third and fourth and one loop

00:52:04,900 --> 00:52:07,630
you have to first and then the second

00:52:06,430 --> 00:52:09,400
the third will be one loop and then the

00:52:07,630 --> 00:52:11,829
fourth and the fifth of you on loop so

00:52:09,400 --> 00:52:13,420
you just shift where the loop is by one

00:52:11,829 --> 00:52:15,819
by one line yeah can everyone see that

00:52:13,420 --> 00:52:19,720
the right thing is the same as the left

00:52:15,819 --> 00:52:21,640
thing now you still have the data

00:52:19,720 --> 00:52:24,329
dependency there right so you have in

00:52:21,640 --> 00:52:27,220
the right loop you have to be I plus one

00:52:24,329 --> 00:52:30,400
is is set and then in the very next line

00:52:27,220 --> 00:52:33,280
of code the B plus one is used again so

00:52:30,400 --> 00:52:34,180
that's the same data dependency there so

00:52:33,280 --> 00:52:38,130
you could argue that this is not

00:52:34,180 --> 00:52:38,130
actually better but why is this better

00:52:38,190 --> 00:52:47,500
does anyone know why the right version

00:52:40,359 --> 00:52:50,520
is better than the left version yes the

00:52:47,500 --> 00:52:53,610
values are going to be in a register so

00:52:50,520 --> 00:52:56,640
the computations are the same in both

00:52:53,610 --> 00:53:00,200
cases so I'm so I didn't I didn't

00:52:56,640 --> 00:53:00,200
observe this particular thing here a

00:53:02,150 --> 00:53:14,640
locality no I mean the data will be in

00:53:04,680 --> 00:53:16,110
the same place every time right ah so

00:53:14,640 --> 00:53:30,650
the suggestion was the compiler can see

00:53:16,110 --> 00:53:33,090
that these are the same statements hmm

00:53:30,650 --> 00:53:35,210
maybe okay I didn't

00:53:33,090 --> 00:53:36,750
maybe you're right I don't I don't quite

00:53:35,210 --> 00:53:45,090
understand it

00:53:36,750 --> 00:53:47,550
oh it's the same amount so oh hang on

00:53:45,090 --> 00:53:50,190
actually in the okay there's a type of

00:53:47,550 --> 00:53:57,570
there in the right case this there's one

00:53:50,190 --> 00:53:59,430
one loop iteration less right yeah so

00:53:57,570 --> 00:54:02,430
actually in the first one it would be go

00:53:59,430 --> 00:54:04,380
from four and I equals 0 to 9 9 H and

00:54:02,430 --> 00:54:06,600
then the right one is from 1 to 9 9 8

00:54:04,380 --> 00:54:08,490
sorry that that's a type of there but

00:54:06,600 --> 00:54:10,260
actually the reason is different the

00:54:08,490 --> 00:54:13,620
reason is yes it's the exact same stuff

00:54:10,260 --> 00:54:15,750
but now each iteration of the loop

00:54:13,620 --> 00:54:19,290
doesn't depend on the previous iteration

00:54:15,750 --> 00:54:20,400
of the loop right and if one iteration

00:54:19,290 --> 00:54:22,160
of the loop doesn't depend on the

00:54:20,400 --> 00:54:26,100
previous iteration of the loop

00:54:22,160 --> 00:54:29,430
what can the compiler do with that it

00:54:26,100 --> 00:54:31,140
can vectorize it right and surprisingly

00:54:29,430 --> 00:54:33,210
none of the compilers that have checked

00:54:31,140 --> 00:54:34,920
has done this automatically if you write

00:54:33,210 --> 00:54:37,020
the loop in this like wrong way right

00:54:34,920 --> 00:54:39,600
but you write it like you do it right

00:54:37,020 --> 00:54:42,180
yes it can vectorize it because in the

00:54:39,600 --> 00:54:47,040
modern compiler in modern CPU you have

00:54:42,180 --> 00:54:50,940
Zim d you can yeah you can do a single

00:54:47,040 --> 00:54:53,430
instruction multiple data so you have

00:54:50,940 --> 00:54:55,380
all these things like SSE and AVX and on

00:54:53,430 --> 00:54:56,400
an arm you have neon where you can you

00:54:55,380 --> 00:55:00,390
know do four additions for one

00:54:56,400 --> 00:55:02,160
instruction and if I do that I see yes

00:55:00,390 --> 00:55:04,410
the right hand side of the loop is going

00:55:02,160 --> 00:55:06,830
to perform three times faster than the

00:55:04,410 --> 00:55:09,860
left one and just to double check

00:55:06,830 --> 00:55:11,750
ah that's the assembly that that clang

00:55:09,860 --> 00:55:13,220
produces and yes in the left version is

00:55:11,750 --> 00:55:14,810
just going to do some ads and moves

00:55:13,220 --> 00:55:20,680
whereas on the right it's going to

00:55:14,810 --> 00:55:24,230
insert some nice SSE instructions so

00:55:20,680 --> 00:55:26,300
there's one KVUE there so if you do this

00:55:24,230 --> 00:55:29,180
SSE thing remember that memory is

00:55:26,300 --> 00:55:31,790
aligned well yeah you also have to align

00:55:29,180 --> 00:55:33,740
to the SSE register size so if you if

00:55:31,790 --> 00:55:36,380
you if you have as a si instructions

00:55:33,740 --> 00:55:38,450
that takes 4 into the x 16 bytes you

00:55:36,380 --> 00:55:42,800
really want those 16 bytes to be aligned

00:55:38,450 --> 00:55:44,480
to a 16 byte boundary right so here's

00:55:42,800 --> 00:55:47,540
another example demonstrating that so I

00:55:44,480 --> 00:55:49,430
have have two buffers here I have two to

00:55:47,540 --> 00:55:50,930
erase with floats and then I do a

00:55:49,430 --> 00:55:53,450
multiply add that's a really really

00:55:50,930 --> 00:55:55,880
common thing to do in an audio code for

00:55:53,450 --> 00:55:58,160
example where it's like in mixing for

00:55:55,880 --> 00:56:00,170
example yeah where you have one area you

00:55:58,160 --> 00:56:01,490
have another area that you multiply the

00:56:00,170 --> 00:56:03,020
secondary and you add it to the first

00:56:01,490 --> 00:56:04,130
area and you do that in the loop and

00:56:03,020 --> 00:56:08,360
this is something that every compiler

00:56:04,130 --> 00:56:13,160
will vectorize yeah but now if I do this

00:56:08,360 --> 00:56:15,050
thing but then I offset the erase by one

00:56:13,160 --> 00:56:16,820
inch so I offset one array by one end

00:56:15,050 --> 00:56:19,250
and then offset another array by two

00:56:16,820 --> 00:56:21,010
ends so it's not underlined data but

00:56:19,250 --> 00:56:23,240
it's not aligned to like a sim D

00:56:21,010 --> 00:56:26,110
register boundary and then I'm going to

00:56:23,240 --> 00:56:29,930
measure that turns out yep it's slower

00:56:26,110 --> 00:56:32,270
actually on the new Mac here it's about

00:56:29,930 --> 00:56:35,660
20% slower which is very significant if

00:56:32,270 --> 00:56:38,210
I do this on the slightly older Mac it's

00:56:35,660 --> 00:56:39,770
actually three times slower or 2.5 times

00:56:38,210 --> 00:56:41,500
slower which completely negates like the

00:56:39,770 --> 00:56:47,030
benefit you would get from vectorization

00:56:41,500 --> 00:56:48,050
now so um so that's one effect here okay

00:56:47,030 --> 00:56:50,690
I have three minutes left

00:56:48,050 --> 00:56:57,260
um so let me get to my very very last

00:56:50,690 --> 00:57:00,470
example um this is a fun one so I have a

00:56:57,260 --> 00:57:02,210
float and I'm just going to do 10,000

00:57:00,470 --> 00:57:03,530
multiplications on it with a number

00:57:02,210 --> 00:57:07,220
that's not going to change the value of

00:57:03,530 --> 00:57:08,960
the float very much now so I'm just

00:57:07,220 --> 00:57:10,130
going to take a float and do 10,000

00:57:08,960 --> 00:57:11,480
multiplications and I'm going to do that

00:57:10,130 --> 00:57:14,210
with different floats I'm going to do it

00:57:11,480 --> 00:57:17,420
with one I'm going to do it with one

00:57:14,210 --> 00:57:20,950
divided by zero which is what in

00:57:17,420 --> 00:57:24,500
I'm going to do it with 0/0 what's that

00:57:20,950 --> 00:57:26,240
nan I'm going to do but - zero which is

00:57:24,500 --> 00:57:27,349
actually something else than plus zero

00:57:26,240 --> 00:57:31,940
and then I'm going to do it with a very

00:57:27,349 --> 00:57:36,770
small number so is any one of those

00:57:31,940 --> 00:57:39,170
going to give us a problem the very

00:57:36,770 --> 00:57:46,670
small number right how much lower will

00:57:39,170 --> 00:57:48,290
it be like how much yep it's going to be

00:57:46,670 --> 00:57:50,690
about thirty times slower on this

00:57:48,290 --> 00:57:53,170
particular Mac here and the reason is

00:57:50,690 --> 00:57:55,280
that it's a D normal and D normals are

00:57:53,170 --> 00:57:56,780
yeah if you have floating-point numbers

00:57:55,280 --> 00:57:59,450
the normals are the ones where the

00:57:56,780 --> 00:58:01,190
exponent is zero and then the fraction

00:57:59,450 --> 00:58:03,680
is nonzero of the red portion and then

00:58:01,190 --> 00:58:05,480
basically the way Hardware handles

00:58:03,680 --> 00:58:08,630
floats with the normative it's not

00:58:05,480 --> 00:58:10,490
really the then we fit in there so I

00:58:08,630 --> 00:58:12,170
think I think I'm not sure what the

00:58:10,490 --> 00:58:14,089
actual like reason is why it's so slow

00:58:12,170 --> 00:58:16,280
in hardware but it is and from the

00:58:14,089 --> 00:58:17,270
software you can disable the normals so

00:58:16,280 --> 00:58:19,339
you know if you know about this thing

00:58:17,270 --> 00:58:21,230
you can there's a switch you can you can

00:58:19,339 --> 00:58:26,720
say flush them to zero and then you're

00:58:21,230 --> 00:58:27,859
not going to have this problem but you

00:58:26,720 --> 00:58:30,290
know if you're not aware of it then you

00:58:27,859 --> 00:58:31,849
will and I actually in my job in audio

00:58:30,290 --> 00:58:33,049
software I'm aware of like two times

00:58:31,849 --> 00:58:34,430
where this was actually causing

00:58:33,049 --> 00:58:36,650
significant performance problems in

00:58:34,430 --> 00:58:38,180
production code because you know there

00:58:36,650 --> 00:58:40,040
was some algorithm generating two

00:58:38,180 --> 00:58:42,140
normals and performance would go with

00:58:40,040 --> 00:58:43,640
plummet and then you know you would

00:58:42,140 --> 00:58:47,690
spend like two days trying to figure out

00:58:43,640 --> 00:58:50,180
what's actually going on so yeah these

00:58:47,690 --> 00:58:53,470
are the normals so that was my last

00:58:50,180 --> 00:58:57,049
example so that concludes my talk so

00:58:53,470 --> 00:58:59,660
here's what we talked about and here's a

00:58:57,049 --> 00:59:00,920
summary slide if you want fast super

00:58:59,660 --> 00:59:03,500
supposed to be nice to your heart where

00:59:00,920 --> 00:59:06,650
you know be conscious about whether you

00:59:03,500 --> 00:59:08,869
bought by data or by computation don't

00:59:06,650 --> 00:59:10,280
be bound by data access you really want

00:59:08,869 --> 00:59:12,650
to be like in the sweet spot all right

00:59:10,280 --> 00:59:15,589
we do data access and computation

00:59:12,650 --> 00:59:17,030
ideally prefer data that's contiguous if

00:59:15,589 --> 00:59:19,339
you can't prefer the constants Trice

00:59:17,030 --> 00:59:20,210
randomness keep data close together in

00:59:19,339 --> 00:59:22,250
space and time

00:59:20,210 --> 00:59:24,770
avoid dependencies between successive

00:59:22,250 --> 00:59:26,450
computations avoid hard to predict

00:59:24,770 --> 00:59:28,460
branches be aware of cache lines and

00:59:26,450 --> 00:59:30,700
alignment minimize the number of cache

00:59:28,460 --> 00:59:33,220
lines that access pad multiple threads

00:59:30,700 --> 00:59:36,190
um you know and don't be surprised if

00:59:33,220 --> 00:59:40,030
you're hot but does weird stuff yeah so

00:59:36,190 --> 00:59:42,390
that's that's a summary here thank you

00:59:40,030 --> 00:59:42,390

YouTube URL: https://www.youtube.com/watch?v=BP6NxVxDQIs


