Title: CppCon 2016: Hartmut Kaiser “Parallelism in Modern C++"
Publication date: 2016-10-01
Playlist: CppCon 2016
Description: 
	http://CppCon.org
—
Presentation Slides, PDFs, Source Code and other presenter materials are available at: https://github.com/cppcon/cppcon2016
—
The traditionally used and well established parallel programming models OpenMP and MPI are both targeting lower level parallelism and are meant to be as language agnostic as possible. For a long time, those models were the only widely available portable options for developing parallel C++ applications beyond using plain threads. This has strongly limited the optimization capabilities of compilers, has inhibited extensibility and genericity, and has restricted the use of those models together with other, modern higher level abstractions introduced by the C++11 and C++14 standards. 
The recent revival of interest in the industry and wider community for the C++ language has also spurred a remarkable amount of standardization proposals and technical specifications being developed. Those efforts however have so far failed to build a vision on how to seamlessly integrate various types of parallelism, such as iterative parallel execution, task-based parallelism, asynchronous many-task execution flows, continuation style computation, or explicit fork-join control flow of independent and non-homogeneous code paths. 

In this talk we present the results of developing higher level parallelization facilities in HPX, a general purpose C++ runtime system for applications of any scale. The developed higher-level parallelization APIs have been designed aiming at overcoming the limitations of today's prevalently used programming models in C++ codes. 
HPX exposes a uniform higher-level API which gives the application programmer syntactic and semantic equivalence of various types of on-node and off-node parallelism, all of which are well integrated into the C++ type system. We show that these higher level facilities are fully aligned with modern C++ programming concepts, they are easily extensible, fully generic, and enable highly efficient parallelization on par with or better than what existing equivalent applications based on OpenMP and/or MPI can achieve. 
— 
Hartmut Kaiser
Hartmut is a member of the faculty at the CS department at Louisiana State University (LSU) and a senior research scientist at LSU's Center for Computation and Technology (CCT). He received his doctorate from the Technical University of Chemnitz (Germany) in 1988. He is probably best known through his involvement in open source software projects, mainly as the author of several C++ libraries he has contributed to Boost, which are in use by thousands of developers worldwide. His current research is focused on leading the STE||AR group at CCT working on the practical design and implementation of future execution models and programming methods. His research interests are focused on the complex interaction of compiler technologies, runtime systems, active libraries, and modern system's architectures. His goal is to enable the creation of a new generation of scientific applications in powerful, though complex environments, such as high performance computing, distributed and grid computing, spatial information systems, and compiler technologies.

Website: https://www.cct.lsu.edu/~hkaiser/
—
Videos Filmed & Edited by Bash Films: http://www.BashFilms.com
Captions: 
	00:00:00,030 --> 00:00:06,120
hello everybody my name is Hamid Kaiser

00:00:02,820 --> 00:00:10,340
I'm working at Louisiana State

00:00:06,120 --> 00:00:14,670
University and I'm leading group there

00:00:10,340 --> 00:00:19,980
it's called stellar group where we focus

00:00:14,670 --> 00:00:23,150
on writing or focus on parallelism and

00:00:19,980 --> 00:00:25,710
everything around C++ and that we go

00:00:23,150 --> 00:00:28,550
today I want to talk about our

00:00:25,710 --> 00:00:32,460
experience was implementing different

00:00:28,550 --> 00:00:39,239
kind of parallelism in C++ very much

00:00:32,460 --> 00:00:41,730
oriented on the ongoing standardization

00:00:39,239 --> 00:00:46,460
forward and starting with the

00:00:41,730 --> 00:00:46,460
parallelism TS and other other things I

00:00:47,239 --> 00:00:54,300
essentially what what the main outcome

00:00:50,300 --> 00:00:58,620
we what we learned when while

00:00:54,300 --> 00:01:00,989
implementing all of that is that you can

00:00:58,620 --> 00:01:03,870
use task based parallelism as a basis

00:01:00,989 --> 00:01:07,950
for all higher-level api's and C++ and

00:01:03,870 --> 00:01:10,860
that works very well even if you can

00:01:07,950 --> 00:01:13,500
implement it all kind of a very broad

00:01:10,860 --> 00:01:15,810
kind of parallelism like a synchrony

00:01:13,500 --> 00:01:18,509
continuation based parallelism for joint

00:01:15,810 --> 00:01:22,259
parallelism and so on I would like to

00:01:18,509 --> 00:01:24,869
focus on the parallelism TS today so

00:01:22,259 --> 00:01:27,360
those parallel algorithms which now have

00:01:24,869 --> 00:01:37,200
been or will be added to C++ 17

00:01:27,360 --> 00:01:40,619
hopefully okay but first allow me please

00:01:37,200 --> 00:01:43,799
allow me to to give you three slides

00:01:40,619 --> 00:01:45,450
about HP X which is a runtime system

00:01:43,799 --> 00:01:48,149
we've been using to implement all of

00:01:45,450 --> 00:01:51,329
that it's a library we've been

00:01:48,149 --> 00:01:54,149
developing for years and I really give

00:01:51,329 --> 00:01:55,920
you these slides about HP X just to give

00:01:54,149 --> 00:01:58,740
you a background where all of that has

00:01:55,920 --> 00:02:01,380
been implemented and to show you that

00:01:58,740 --> 00:02:04,979
what you can use to to play with it

00:02:01,380 --> 00:02:07,259
today to take the parallel algorithms as

00:02:04,979 --> 00:02:09,629
they will be in support for 17 today and

00:02:07,259 --> 00:02:11,730
just try them out run write your

00:02:09,629 --> 00:02:13,590
applications with them and later on all

00:02:11,730 --> 00:02:15,810
you have to do is to change see

00:02:13,590 --> 00:02:17,160
name space from HP X kalaam kalaam to

00:02:15,810 --> 00:02:19,290
stomach : :

00:02:17,160 --> 00:02:24,269
and you get exactly the same behavior

00:02:19,290 --> 00:02:26,700
same semantics HP X is a general-purpose

00:02:24,269 --> 00:02:29,790
runtime system for applications of any

00:02:26,700 --> 00:02:33,480
skill and by that I mean applications

00:02:29,790 --> 00:02:37,920
which run on the Raspberry Pi on one end

00:02:33,480 --> 00:02:40,410
or on the petascale machines on the

00:02:37,920 --> 00:02:43,410
National Labs on the other end hundreds

00:02:40,410 --> 00:02:45,540
of thousands of cores and two cores on

00:02:43,410 --> 00:02:50,250
the Raspberry Pi and the same runtime

00:02:45,540 --> 00:02:52,530
system is usable on all of those it's a

00:02:50,250 --> 00:02:54,269
general-purpose runtime system for

00:02:52,530 --> 00:02:59,450
applications of any scale as already

00:02:54,269 --> 00:03:03,090
said we we made a very large effort to

00:02:59,450 --> 00:03:05,489
implement a API which is absolutely

00:03:03,090 --> 00:03:11,010
stunning conformant in terms of

00:03:05,489 --> 00:03:13,560
semantics and syntax so it gives you a

00:03:11,010 --> 00:03:17,010
very uniform and standards oriented api

00:03:13,560 --> 00:03:18,930
to write your applications on the other

00:03:17,010 --> 00:03:21,060
hand you can write fully a synchronous

00:03:18,930 --> 00:03:24,810
code using hundreds of millions of

00:03:21,060 --> 00:03:28,290
threats and even on my laptop I can run

00:03:24,810 --> 00:03:31,670
applications which run hundred million

00:03:28,290 --> 00:03:31,670
threats without any problems

00:03:33,950 --> 00:03:39,420
the interesting part here is that since

00:03:37,739 --> 00:03:41,700
we want to run on the big machines as

00:03:39,420 --> 00:03:45,419
well not only on on the on desktop zone

00:03:41,700 --> 00:03:47,400
on smaller embedded devices we provide

00:03:45,419 --> 00:03:51,090
you a unified syntax to write

00:03:47,400 --> 00:03:53,940
applications which abstract the notion

00:03:51,090 --> 00:03:56,940
of locality from you so you code looks

00:03:53,940 --> 00:03:58,470
the same regardless whether the code DB

00:03:56,940 --> 00:04:01,319
the operation has to be performed

00:03:58,470 --> 00:04:07,169
locally to the invocation or remotely on

00:04:01,319 --> 00:04:10,290
a different node just a short rundown

00:04:07,169 --> 00:04:12,269
what HP X actually consists of and you

00:04:10,290 --> 00:04:14,459
will see that essentially what we

00:04:12,269 --> 00:04:17,280
implemented there is a set of features a

00:04:14,459 --> 00:04:21,150
set of facilities which are known for

00:04:17,280 --> 00:04:23,729
decades and just by putting them

00:04:21,150 --> 00:04:25,979
together we kind of get new completely

00:04:23,729 --> 00:04:26,580
completely new emergent properties which

00:04:25,979 --> 00:04:29,009
allow us

00:04:26,580 --> 00:04:32,419
- to implement these nice nice things

00:04:29,009 --> 00:04:35,430
which I will talk to about in this talk

00:04:32,419 --> 00:04:38,639
it sits on top of a global global

00:04:35,430 --> 00:04:40,770
address space that might not be that

00:04:38,639 --> 00:04:43,020
important in the context of this con and

00:04:40,770 --> 00:04:44,370
the ill and the context of this talk but

00:04:43,020 --> 00:04:46,050
it's very important when you are talking

00:04:44,370 --> 00:04:47,969
about distributed applications running

00:04:46,050 --> 00:04:49,979
on thousands of nodes because what you

00:04:47,969 --> 00:04:52,949
want to do you want to do local global

00:04:49,979 --> 00:04:54,300
load balancing of your data and that

00:04:52,949 --> 00:04:57,389
means you want to move around things

00:04:54,300 --> 00:04:59,669
from one node to another and by

00:04:57,389 --> 00:05:01,530
providing a global address space that

00:04:59,669 --> 00:05:03,569
moving around of data doesn't change the

00:05:01,530 --> 00:05:06,090
address of that data item which is very

00:05:03,569 --> 00:05:07,800
important on the other hand it's built

00:05:06,090 --> 00:05:10,080
on top of fine grained parallelism and

00:05:07,800 --> 00:05:12,440
very lightweight synchronization the

00:05:10,080 --> 00:05:15,090
overhead for creating is thread

00:05:12,440 --> 00:05:17,280
scheduling it running it destroying it

00:05:15,090 --> 00:05:19,770
is in there in the range of 800

00:05:17,280 --> 00:05:25,319
nanoseconds so it's very cheap to create

00:05:19,770 --> 00:05:27,120
a thread and to run things on that we

00:05:25,319 --> 00:05:29,250
combine that with a work queue based

00:05:27,120 --> 00:05:31,919
message different computation not only

00:05:29,250 --> 00:05:35,460
local locally like you know it from TBB

00:05:31,919 --> 00:05:37,529
or from silk but also remotely so it's a

00:05:35,460 --> 00:05:39,500
departure from the common bulk

00:05:37,529 --> 00:05:42,479
synchronous programming paradigm which

00:05:39,500 --> 00:05:44,460
people use nowadays to write their

00:05:42,479 --> 00:05:48,360
distributed applications mostly based on

00:05:44,460 --> 00:05:49,589
MPI I already mentioned - for semantic

00:05:48,360 --> 00:05:52,740
equivalence of local and remote

00:05:49,589 --> 00:05:57,839
execution and we have explicit support

00:05:52,740 --> 00:05:59,250
for accelerators and and vectorization

00:05:57,839 --> 00:06:03,870
and they will show you a couple of

00:05:59,250 --> 00:06:05,520
examples later on it's completely

00:06:03,870 --> 00:06:09,509
open-source you can download it from

00:06:05,520 --> 00:06:11,520
from github it's being released under

00:06:09,509 --> 00:06:13,860
the boost license so no strings attached

00:06:11,520 --> 00:06:16,349
whatsoever you can use it forever for

00:06:13,860 --> 00:06:18,270
whatever you want we certainly would

00:06:16,349 --> 00:06:21,539
like to hear back when you use it but

00:06:18,270 --> 00:06:24,089
you don't have to towers even and the

00:06:21,539 --> 00:06:26,190
nice thing we believe why it might be

00:06:24,089 --> 00:06:28,319
interesting for you to to look into HP

00:06:26,190 --> 00:06:29,580
exits you can very nicely use it for

00:06:28,319 --> 00:06:32,279
platform for research and

00:06:29,580 --> 00:06:34,020
experimentation and you can on the other

00:06:32,279 --> 00:06:35,729
hand already today we'll play with the

00:06:34,020 --> 00:06:37,590
features which your compilers will give

00:06:35,729 --> 00:06:40,290
you hopefully tomorrow

00:06:37,590 --> 00:06:42,600
a quick overview about what the

00:06:40,290 --> 00:06:46,140
structure of hpx is but I'm not going to

00:06:42,600 --> 00:06:50,840
drill down too much into the details for

00:06:46,140 --> 00:06:53,970
subsystems shredding local shredding

00:06:50,840 --> 00:06:57,300
synchronization the address space and

00:06:53,970 --> 00:07:00,330
the networking layer sitting on top of

00:06:57,300 --> 00:07:02,640
the operating system and exposing a C++

00:07:00,330 --> 00:07:06,810
standards conforming parallelism API on

00:07:02,640 --> 00:07:08,580
top all of that combined was a

00:07:06,810 --> 00:07:10,260
performance counter framework which

00:07:08,580 --> 00:07:12,720
allows you to measure all kinds of

00:07:10,260 --> 00:07:14,070
different things in in the runtime

00:07:12,720 --> 00:07:15,870
system in the application and the

00:07:14,070 --> 00:07:19,830
operating system hardware performance

00:07:15,870 --> 00:07:22,850
counters as provided by the chips expose

00:07:19,830 --> 00:07:26,790
through a uniform interface and some

00:07:22,850 --> 00:07:28,560
tooling support which allows to tie in

00:07:26,790 --> 00:07:29,970
what we call policies which look at

00:07:28,560 --> 00:07:31,980
these performance counters and allowed

00:07:29,970 --> 00:07:35,580
to and to do the runtime adaptive

00:07:31,980 --> 00:07:37,080
decisions on what to do next and turn

00:07:35,580 --> 00:07:41,450
the knobs in the system itself

00:07:37,080 --> 00:07:43,860
things like hues lines white times

00:07:41,450 --> 00:07:46,710
memory distributions and so on and so on

00:07:43,860 --> 00:07:47,190
and you can can analysis at very nice

00:07:46,710 --> 00:07:50,490
name

00:07:47,190 --> 00:07:53,790
in this talk today I will focus on the

00:07:50,490 --> 00:07:56,490
parallelism ap is and is specifically on

00:07:53,790 --> 00:08:01,530
the parallelism TS they are parallel

00:07:56,490 --> 00:08:05,280
algorithms in general the API hpx

00:08:01,530 --> 00:08:09,390
exposes as I said is oriented strictly

00:08:05,280 --> 00:08:11,850
on the existing standardization

00:08:09,390 --> 00:08:14,790
standards and on this standardization

00:08:11,850 --> 00:08:16,740
effort so we try to track many of the

00:08:14,790 --> 00:08:20,190
proposals related to parallelism and

00:08:16,740 --> 00:08:22,950
concurrency in hpx early on and give

00:08:20,190 --> 00:08:25,980
feedback develop extensions which we

00:08:22,950 --> 00:08:29,040
feel are very interesting for us and our

00:08:25,980 --> 00:08:30,630
application development and try to feed

00:08:29,040 --> 00:08:34,470
that back into the standardization

00:08:30,630 --> 00:08:36,780
process well in short essentially

00:08:34,470 --> 00:08:39,030
everything you have under standard you

00:08:36,780 --> 00:08:41,220
have something very equivalent in hpx

00:08:39,030 --> 00:08:43,170
and you might ask hey why did you

00:08:41,220 --> 00:08:46,230
reimplemented stuff why did you

00:08:43,170 --> 00:08:47,160
reimplemented new text futures async and

00:08:46,230 --> 00:08:48,870
so on and so on

00:08:47,160 --> 00:08:49,860
well there are several reasons why we

00:08:48,870 --> 00:08:53,820
did that

00:08:49,860 --> 00:08:55,680
first we wanted to support distributed

00:08:53,820 --> 00:08:57,510
computing from the very beginning and we

00:08:55,680 --> 00:08:59,640
wanted to provide these facilities in

00:08:57,510 --> 00:09:03,930
distributed computing so that you could

00:08:59,640 --> 00:09:05,640
just bind a function on one node send it

00:09:03,930 --> 00:09:12,630
over the wire and invoke it on the other

00:09:05,640 --> 00:09:14,339
node or just do a async which gives you

00:09:12,630 --> 00:09:16,110
a lightweight future which is

00:09:14,339 --> 00:09:18,810
implemented on top of a very lightweight

00:09:16,110 --> 00:09:21,240
writing system and gives you the Layton

00:09:18,810 --> 00:09:26,180
sees and the overheads very the low

00:09:21,240 --> 00:09:28,560
overheads I already mentioned we

00:09:26,180 --> 00:09:31,019
implemented a couple of extensions to

00:09:28,560 --> 00:09:33,089
those done it's a pis and as I said we

00:09:31,019 --> 00:09:38,540
are trying to defeat that information

00:09:33,089 --> 00:09:38,540
back into our work was sg1 was SG 14

00:09:39,920 --> 00:09:46,350
okay so if you look at the picture at

00:09:44,399 --> 00:09:47,880
the big picture what does it mean to

00:09:46,350 --> 00:09:49,470
have parallelism and and what are the

00:09:47,880 --> 00:09:51,560
things you're interested in controlling

00:09:49,470 --> 00:09:54,600
when you are when you want to expose

00:09:51,560 --> 00:10:01,290
various facets of the parallelism to you

00:09:54,600 --> 00:10:02,940
to you using on the uppermost level the

00:10:01,290 --> 00:10:06,810
application probably will deal with

00:10:02,940 --> 00:10:10,370
fairly high level things like parallel

00:10:06,810 --> 00:10:13,440
algorithms some fork/join parallelism

00:10:10,370 --> 00:10:16,500
some essentially some continuation style

00:10:13,440 --> 00:10:19,230
programming and so on and so on all of

00:10:16,500 --> 00:10:23,310
that we have implemented on top of three

00:10:19,230 --> 00:10:27,180
facilities essentially futures async and

00:10:23,310 --> 00:10:28,890
a function which we call dataflow and I

00:10:27,180 --> 00:10:34,260
will talk about what dataflow actually

00:10:28,890 --> 00:10:35,160
does in a second and these three

00:10:34,260 --> 00:10:40,199
facilities

00:10:35,160 --> 00:10:43,890
kindness sit on top of a three concepts

00:10:40,199 --> 00:10:46,199
the execution policies that are those

00:10:43,890 --> 00:10:47,730
objects which are already defined in the

00:10:46,199 --> 00:10:52,170
parallelism tiers and which have been

00:10:47,730 --> 00:10:55,350
added to super 417 and those execution

00:10:52,170 --> 00:10:57,899
policies are there to express

00:10:55,350 --> 00:11:00,060
restrictions of your of your code you

00:10:57,899 --> 00:11:01,770
want to run a parallel essentially if

00:11:00,060 --> 00:11:02,880
you use the sequential execution policy

00:11:01,770 --> 00:11:05,190
that means

00:11:02,880 --> 00:11:07,590
it tells a system hey the code I provide

00:11:05,190 --> 00:11:10,770
as a lambda for my parallel for loop

00:11:07,590 --> 00:11:12,960
does not is not allowed to run in

00:11:10,770 --> 00:11:15,690
parallel or concurrently it has to run

00:11:12,960 --> 00:11:18,540
sequentially if I use a parallel

00:11:15,690 --> 00:11:20,720
execution policy I allowed the system to

00:11:18,540 --> 00:11:23,220
run things concurrently in parallel

00:11:20,720 --> 00:11:25,470
execution policies are tied to

00:11:23,220 --> 00:11:27,270
executives that's an ongoing discussion

00:11:25,470 --> 00:11:30,030
and currently there's a lot of work

00:11:27,270 --> 00:11:32,010
being done to coin out the details of

00:11:30,030 --> 00:11:34,380
those executives what we have

00:11:32,010 --> 00:11:37,980
implemented is an early proposal which

00:11:34,380 --> 00:11:41,270
was developed mostly by Nvidia and which

00:11:37,980 --> 00:11:43,230
is a basis for the executor

00:11:41,270 --> 00:11:46,140
standardization work which is currently

00:11:43,230 --> 00:11:48,540
being discussed executives are

00:11:46,140 --> 00:11:51,570
essentially objects which represent the

00:11:48,540 --> 00:11:53,880
where and when of execution where do I

00:11:51,570 --> 00:11:57,930
want to run this particular task and one

00:11:53,880 --> 00:12:00,840
do I want to run it what we edit you to

00:11:57,930 --> 00:12:03,960
it is what we call executive parameters

00:12:00,840 --> 00:12:06,260
that sings which allow us to further

00:12:03,960 --> 00:12:09,720
modify the behavior of their executives

00:12:06,260 --> 00:12:11,640
other things like grain size so how many

00:12:09,720 --> 00:12:15,930
of the iterations of this for loop do I

00:12:11,640 --> 00:12:18,150
want to run and one go and how do I want

00:12:15,930 --> 00:12:19,740
to chunk up the work this kind of things

00:12:18,150 --> 00:12:24,000
and many other things and I will show a

00:12:19,740 --> 00:12:26,910
couple of examples later on if you look

00:12:24,000 --> 00:12:28,560
at what the standardization how the

00:12:26,910 --> 00:12:30,750
standardization landscape looks today

00:12:28,560 --> 00:12:33,570
then we see we have the parallelism

00:12:30,750 --> 00:12:36,810
tiers which is mostly concerned with

00:12:33,570 --> 00:12:39,720
iterative parallelism which has been

00:12:36,810 --> 00:12:42,740
moved to be included in C++ 17 and I

00:12:39,720 --> 00:12:47,270
really hope it will make it into C++ 17

00:12:42,740 --> 00:12:51,900
the concurrency TS which is defining

00:12:47,270 --> 00:12:54,830
extensions for a synchrony for task

00:12:51,900 --> 00:12:58,230
based continuation style parallelism

00:12:54,830 --> 00:12:59,940
there are task blocks for fork/join

00:12:58,230 --> 00:13:02,220
parallelism of heterogeneous tasks

00:12:59,940 --> 00:13:04,260
that's a proposal below submitted by

00:13:02,220 --> 00:13:09,060
Intel which is currently being discussed

00:13:04,260 --> 00:13:11,360
as already mentioned executives and last

00:13:09,060 --> 00:13:16,170
but not least resumable functions or

00:13:11,360 --> 00:13:16,680
coroutines with compiler support for

00:13:16,170 --> 00:13:18,960
quality

00:13:16,680 --> 00:13:20,370
which are very important for us as well

00:13:18,960 --> 00:13:23,100
and it will show a couple of examples

00:13:20,370 --> 00:13:24,750
for that what's currently missing in the

00:13:23,100 --> 00:13:27,480
in the standardization from our

00:13:24,750 --> 00:13:29,760
perspective is the integration of all of

00:13:27,480 --> 00:13:31,920
the buff so these things are all very

00:13:29,760 --> 00:13:35,459
interesting but they are not very well

00:13:31,920 --> 00:13:37,589
aligned one with it another there's a

00:13:35,459 --> 00:13:39,089
question of creating or extending the

00:13:37,589 --> 00:13:41,310
the Rangers proposal

00:13:39,089 --> 00:13:43,800
into the parallel world nobody is

00:13:41,310 --> 00:13:46,320
working on that as far as Inc as I can

00:13:43,800 --> 00:13:49,800
see so far vectorization is already

00:13:46,320 --> 00:13:52,050
being discussed and there are still

00:13:49,800 --> 00:13:53,670
extensions for GPUs many core

00:13:52,050 --> 00:13:58,649
distributed or high performance

00:13:53,670 --> 00:14:01,440
computing and that's where we try to to

00:13:58,649 --> 00:14:03,540
define and find abstractions which can

00:14:01,440 --> 00:14:07,320
be used efficiently to integrate all of

00:14:03,540 --> 00:14:09,060
these things into one big package in my

00:14:07,320 --> 00:14:11,490
point the goal has to be to make

00:14:09,060 --> 00:14:15,089
parallelism in C++ independent from

00:14:11,490 --> 00:14:21,930
helper libraries like open MP or open

00:14:15,089 --> 00:14:23,970
ACC if I had a wish by C plus 20 I'm not

00:14:21,930 --> 00:14:27,270
sure if that's possible but that's what

00:14:23,970 --> 00:14:29,490
what we are striving for hpx itself as a

00:14:27,270 --> 00:14:31,860
library it allows us to make C++

00:14:29,490 --> 00:14:34,740
independent of MPI as the underlying

00:14:31,860 --> 00:14:40,160
networking or programming model for

00:14:34,740 --> 00:14:42,990
distributed applications as well okay I

00:14:40,160 --> 00:14:45,180
mentioned futures and just to be sure

00:14:42,990 --> 00:14:46,500
that everybody is on the same page here

00:14:45,180 --> 00:14:49,380
and everybody understands what the

00:14:46,500 --> 00:14:52,230
future is I and because futures are for

00:14:49,380 --> 00:14:56,459
us at least in hpx kind of the central

00:14:52,230 --> 00:14:58,770
very crucial facility we use to build

00:14:56,459 --> 00:15:01,920
all kinds of parallelism all kind of

00:14:58,770 --> 00:15:06,720
synchronization on I would like to talk

00:15:01,920 --> 00:15:09,209
about those for a second so in essence a

00:15:06,720 --> 00:15:11,370
future is an object which represent it

00:15:09,209 --> 00:15:15,839
represents a result which has not been

00:15:11,370 --> 00:15:18,089
computed yet essentially what happens is

00:15:15,839 --> 00:15:21,120
you have one thread of execution which

00:15:18,089 --> 00:15:23,760
creates that future object and when that

00:15:21,120 --> 00:15:25,709
future object is created a new thread is

00:15:23,760 --> 00:15:29,040
being spawned which executes another

00:15:25,709 --> 00:15:29,970
task and the original thread can go

00:15:29,040 --> 00:15:33,089
forward

00:15:29,970 --> 00:15:35,129
executing other work until two point

00:15:33,089 --> 00:15:38,879
when it actually needs a result of that

00:15:35,129 --> 00:15:40,680
spawn threat and at this point it looks

00:15:38,879 --> 00:15:43,139
at the future and if the future has

00:15:40,680 --> 00:15:45,180
received the result of that parallel

00:15:43,139 --> 00:15:47,339
execution already then it just goes

00:15:45,180 --> 00:15:50,610
ahead if not then the current sweat is

00:15:47,339 --> 00:15:54,240
put it's put to sleep

00:15:50,610 --> 00:15:57,029
and will be resumed when the when the

00:15:54,240 --> 00:16:00,569
result arrives this is a very nice

00:15:57,029 --> 00:16:02,850
synchronization model and allows us to

00:16:00,569 --> 00:16:05,730
completely abstract ourselves from the

00:16:02,850 --> 00:16:07,589
notion of a thread which is very

00:16:05,730 --> 00:16:11,089
important because I personally believe

00:16:07,589 --> 00:16:13,879
that dealing with low-level threads is

00:16:11,089 --> 00:16:16,889
very very painful and people shouldn't

00:16:13,879 --> 00:16:19,139
well people are just not wired to think

00:16:16,889 --> 00:16:21,389
in terms of threats and if you have to

00:16:19,139 --> 00:16:23,759
deal with two threads or ten threats

00:16:21,389 --> 00:16:25,230
that might be okay but as soon as you

00:16:23,759 --> 00:16:27,509
have to deal with millions of threats

00:16:25,230 --> 00:16:28,649
this is completely impossible to to

00:16:27,509 --> 00:16:33,449
model in your head

00:16:28,649 --> 00:16:36,439
how that works so this kind of model of

00:16:33,449 --> 00:16:38,910
all using future objects enables us very

00:16:36,439 --> 00:16:41,399
transparent synchronization with a

00:16:38,910 --> 00:16:42,899
producer it hides the notion of dealing

00:16:41,399 --> 00:16:44,879
with threats which is in my book the

00:16:42,899 --> 00:16:47,279
most important thing it makes a

00:16:44,879 --> 00:16:50,490
synchrony manageable and it allows for

00:16:47,279 --> 00:16:53,730
composition of several independent tasks

00:16:50,490 --> 00:16:56,610
in building a task dependency tree as we

00:16:53,730 --> 00:16:59,399
will see in a second the last point

00:16:56,610 --> 00:17:01,439
turns concurrency into parallelism I put

00:16:59,399 --> 00:17:03,779
that in parenthesis because futures

00:17:01,439 --> 00:17:06,240
actually don't do that but as soon as

00:17:03,779 --> 00:17:07,949
you start using futures for the

00:17:06,240 --> 00:17:10,679
synchronization you kind of end up with

00:17:07,949 --> 00:17:12,659
a programming model where you have to do

00:17:10,679 --> 00:17:14,970
less with concurrency and you can focus

00:17:12,659 --> 00:17:18,419
more on the parallelism so it's a very

00:17:14,970 --> 00:17:21,839
very nice way to to write code where you

00:17:18,419 --> 00:17:24,240
where you in my experience have a lot

00:17:21,839 --> 00:17:27,390
less concurrency problems like data

00:17:24,240 --> 00:17:33,390
risks and so on and so on so a small

00:17:27,390 --> 00:17:35,700
example we want to calculate the

00:17:33,390 --> 00:17:38,820
universal answer to life to universe and

00:17:35,700 --> 00:17:40,130
everything else while we know that it's

00:17:38,820 --> 00:17:42,680
00:17:40,130 --> 00:17:45,590
but we also know that calculating that

00:17:42,680 --> 00:17:47,840
answer requires 7.5 million years

00:17:45,590 --> 00:17:52,340
unfortunately if you follow the

00:17:47,840 --> 00:17:54,230
literature so let's so let's create a

00:17:52,340 --> 00:17:56,540
function which we call deep thought

00:17:54,230 --> 00:17:58,810
that's our computer and what that

00:17:56,540 --> 00:18:01,820
computer does it just cause acing and

00:17:58,810 --> 00:18:03,770
launches that I think that it universal

00:18:01,820 --> 00:18:07,880
function that function which calculates

00:18:03,770 --> 00:18:10,850
a result on a new thread async returns

00:18:07,880 --> 00:18:13,130
as a future object which represents

00:18:10,850 --> 00:18:16,640
answer which the function will deliver

00:18:13,130 --> 00:18:19,460
at some point in the future then we go

00:18:16,640 --> 00:18:21,020
on with our lives for 7.5 million years

00:18:19,460 --> 00:18:22,760
because well we have lots of other

00:18:21,020 --> 00:18:24,770
things to do while this answer is being

00:18:22,760 --> 00:18:26,780
computed and at some point we come back

00:18:24,770 --> 00:18:28,400
and say hey I'm now the 7.5 million

00:18:26,780 --> 00:18:31,310
years are over now I want to get the

00:18:28,400 --> 00:18:33,890
result and what you do you call get on

00:18:31,310 --> 00:18:36,200
the future and two things can happen

00:18:33,890 --> 00:18:38,180
either the result has been computed and

00:18:36,200 --> 00:18:40,640
has delivered has been delivered back to

00:18:38,180 --> 00:18:43,670
the to the future object then this will

00:18:40,640 --> 00:18:46,190
just grab the value and go ahead or the

00:18:43,670 --> 00:18:48,950
value has not been calculated yet it has

00:18:46,190 --> 00:18:51,470
not been returned to the future and then

00:18:48,950 --> 00:18:54,470
get will internally suspend the current

00:18:51,470 --> 00:18:57,440
thread completely invisible for the

00:18:54,470 --> 00:19:01,700
caller and assume it as a value is

00:18:57,440 --> 00:19:03,920
delivered this read will be resumed and

00:19:01,700 --> 00:19:06,350
the the call returns from get and

00:19:03,920 --> 00:19:09,350
returns and continues as if nothing

00:19:06,350 --> 00:19:12,080
happened so the caller will not even

00:19:09,350 --> 00:19:14,510
know whether the synchronization whether

00:19:12,080 --> 00:19:16,430
the thread was suspended or not all it

00:19:14,510 --> 00:19:18,680
knows I said one get returns it has a

00:19:16,430 --> 00:19:20,680
result of the of the function and that's

00:19:18,680 --> 00:19:24,470
a very nice way of abstracting these

00:19:20,680 --> 00:19:27,380
anonymous consumer produces scenarios

00:19:24,470 --> 00:19:30,590
and as you can see there's not a single

00:19:27,380 --> 00:19:32,390
notion of threat or of synchronization

00:19:30,590 --> 00:19:34,960
of whatever

00:19:32,390 --> 00:19:38,630
it's very nice straightforward code and

00:19:34,960 --> 00:19:41,740
in HP X we use futures as a central

00:19:38,630 --> 00:19:43,610
building block for all kind of

00:19:41,740 --> 00:19:45,500
synchronization for all kind of

00:19:43,610 --> 00:19:52,930
parallelism and that worked very well

00:19:45,500 --> 00:19:52,930
for us ok Pirlo algorithms

00:19:54,389 --> 00:20:06,749
you might know that the parallelism key

00:19:59,200 --> 00:20:10,859
s essentially defines to implement probe

00:20:06,749 --> 00:20:14,109
specifies these eighty algorithms also

00:20:10,859 --> 00:20:18,849
which we all know and love from from the

00:20:14,109 --> 00:20:22,749
STL and it specifies them to to be

00:20:18,849 --> 00:20:24,789
parallel or to be paralyzed about what

00:20:22,749 --> 00:20:26,769
the essence there is that they are very

00:20:24,789 --> 00:20:29,409
similar to standard library facilities

00:20:26,769 --> 00:20:31,629
we know for years but they have one

00:20:29,409 --> 00:20:35,799
additional first argument which is the

00:20:31,629 --> 00:20:37,389
execution policy and the standard

00:20:35,799 --> 00:20:39,219
currently defines three execution

00:20:37,389 --> 00:20:41,950
policies if I remember correctly I will

00:20:39,219 --> 00:20:45,039
focus on two of them at this point the

00:20:41,950 --> 00:20:49,419
most important ones are power and SEC

00:20:45,039 --> 00:20:51,940
well power means that the code is

00:20:49,419 --> 00:20:53,830
allowed to run in parallel my my lambda

00:20:51,940 --> 00:20:57,489
I passed to the to the for each is

00:20:53,830 --> 00:21:00,219
allowed to be to run in parallel and seq

00:20:57,489 --> 00:21:02,889
just means I know parallelization is

00:21:00,219 --> 00:21:07,139
allowed I'm not allowed to run that code

00:21:02,889 --> 00:21:10,779
concurrently execution policies have a

00:21:07,139 --> 00:21:13,239
standard default executor associated

00:21:10,779 --> 00:21:16,929
with them and this is how we implemented

00:21:13,239 --> 00:21:18,759
it in in HP X so essentially power is

00:21:16,929 --> 00:21:22,629
associated with a default parallel

00:21:18,759 --> 00:21:26,799
executor and seq sack as associated with

00:21:22,629 --> 00:21:29,529
a default sequential executor what we

00:21:26,799 --> 00:21:32,109
also implemented is that you can rebind

00:21:29,529 --> 00:21:34,419
the executor and the executor parameters

00:21:32,109 --> 00:21:37,179
to the execution policies and you can do

00:21:34,419 --> 00:21:40,719
that in a very simple way that code

00:21:37,179 --> 00:21:44,409
example shows you a parallel till I left

00:21:40,719 --> 00:21:46,839
that namespace parallel in front of it

00:21:44,409 --> 00:21:49,239
even if the standard doesn't do that

00:21:46,839 --> 00:21:51,729
under standard it still stood : : for

00:21:49,239 --> 00:21:53,469
each o state : hello and fill but just

00:21:51,729 --> 00:21:54,700
to make it clear that I'm talking about

00:21:53,469 --> 00:21:56,859
the parallel versions of those

00:21:54,700 --> 00:21:59,190
algorithms I usually have the parallel

00:21:56,859 --> 00:22:02,400
kolonko on the namespace in front of it

00:21:59,190 --> 00:22:06,150
so as you can see the last arguments are

00:22:02,400 --> 00:22:09,150
the same as for the old venerable state

00:22:06,150 --> 00:22:12,270
find fill we we know but you pass that

00:22:09,150 --> 00:22:14,550
additional argument there as a first

00:22:12,270 --> 00:22:16,590
argument which means hey that fill can

00:22:14,550 --> 00:22:19,170
be actually executed in power law as

00:22:16,590 --> 00:22:21,360
simple as it is so if you want to use a

00:22:19,170 --> 00:22:26,750
different executor you can do that very

00:22:21,360 --> 00:22:29,880
simply by doing power dot on dot on

00:22:26,750 --> 00:22:31,800
allows you to bind a new executor and I

00:22:29,880 --> 00:22:35,910
will talk about executives a bit more

00:22:31,800 --> 00:22:37,320
detail later a new instance of an

00:22:35,910 --> 00:22:41,400
executor to this parallel execution

00:22:37,320 --> 00:22:44,220
policy which will and if that executor

00:22:41,400 --> 00:22:46,680
for instance limits execution to a

00:22:44,220 --> 00:22:50,100
particular Numa domain in your system

00:22:46,680 --> 00:22:52,260
then this standard fill algorithm will

00:22:50,100 --> 00:22:54,720
run code only on this particular new

00:22:52,260 --> 00:22:59,910
model main or if that executor limits

00:22:54,720 --> 00:23:02,280
execution to a GPU then sted fill will

00:22:59,910 --> 00:23:04,440
run the code on a GPU and so on and so

00:23:02,280 --> 00:23:06,270
on and the nice thing is you as a user

00:23:04,440 --> 00:23:08,370
can provide your own and develop your

00:23:06,270 --> 00:23:10,320
own executive objects which will have a

00:23:08,370 --> 00:23:12,510
very simple interface and you can

00:23:10,320 --> 00:23:14,880
control the the operation of this

00:23:12,510 --> 00:23:17,430
parallel algorithms by providing your

00:23:14,880 --> 00:23:19,110
own executor object which encapsulates

00:23:17,430 --> 00:23:21,420
your particular needs in terms of

00:23:19,110 --> 00:23:23,460
execution where remember executives

00:23:21,420 --> 00:23:28,560
means where and when our thing's

00:23:23,460 --> 00:23:30,930
executed the dot voice is an extension

00:23:28,560 --> 00:23:34,530
we implemented in HP X which allows you

00:23:30,930 --> 00:23:36,600
to add additional parameters to the

00:23:34,530 --> 00:23:38,880
executor to tell the executive

00:23:36,600 --> 00:23:41,070
additional parameterization things like

00:23:38,880 --> 00:23:44,070
what's the chunk size what's your

00:23:41,070 --> 00:23:46,050
chunking policy and and so on and so on

00:23:44,070 --> 00:23:52,590
and I will have a couple of of other

00:23:46,050 --> 00:23:55,440
examples in on the next slides so again

00:23:52,590 --> 00:23:58,530
what you can do take our Batory

00:23:55,440 --> 00:24:01,140
execution policy rebind only the

00:23:58,530 --> 00:24:03,180
executive instance or rebind only the

00:24:01,140 --> 00:24:05,160
executor parameters when you want to

00:24:03,180 --> 00:24:07,110
rely on the default executors which is

00:24:05,160 --> 00:24:10,020
already pre bound to the parallel

00:24:07,110 --> 00:24:13,059
execution policy or you can rebind both

00:24:10,020 --> 00:24:16,080
it's as easy as ad

00:24:13,059 --> 00:24:21,389
or if you go to our power example again

00:24:16,080 --> 00:24:23,499
so power power was a rebound executor or

00:24:21,389 --> 00:24:27,070
was a rebound

00:24:23,499 --> 00:24:35,440
both of them rebound to your executor

00:24:27,070 --> 00:24:38,769
and your parameters object okay

00:24:35,440 --> 00:24:41,470
what we additionally edit in in HP X to

00:24:38,769 --> 00:24:43,360
those execution policies which the

00:24:41,470 --> 00:24:46,149
standard defines and there's currently a

00:24:43,360 --> 00:24:50,399
proposal which is being discussed and

00:24:46,149 --> 00:24:53,169
sg-1 which we submitted the paper a a

00:24:50,399 --> 00:24:57,129
proposals currently being discussed to

00:24:53,169 --> 00:24:59,559
turn the algorithms into a synchronous

00:24:57,129 --> 00:25:01,960
algorithms normally when you invoke a

00:24:59,559 --> 00:25:04,029
parallel algorithm it will return only

00:25:01,960 --> 00:25:06,519
once all the algorithm has been finished

00:25:04,029 --> 00:25:09,279
computing but wouldn't it be nice if you

00:25:06,519 --> 00:25:11,830
could just kick off the computation let

00:25:09,279 --> 00:25:15,309
it run somewhere and get a future back

00:25:11,830 --> 00:25:17,710
which represent the the the finalization

00:25:15,309 --> 00:25:20,499
of that particular algorithm execution

00:25:17,710 --> 00:25:26,590
and the way we we did that is just by

00:25:20,499 --> 00:25:28,330
allowing to specify a task modifier with

00:25:26,590 --> 00:25:31,690
the execution policy so if you write

00:25:28,330 --> 00:25:33,850
power of task then the algorithm

00:25:31,690 --> 00:25:37,210
actually will not run sequentially at

00:25:33,850 --> 00:25:40,269
our synchronously as you know it but it

00:25:37,210 --> 00:25:42,309
will just kick off the operation and

00:25:40,269 --> 00:25:43,990
give you a future which represents you

00:25:42,309 --> 00:25:46,090
the all execution of the of the

00:25:43,990 --> 00:25:48,279
algorithm and the same we have

00:25:46,090 --> 00:25:51,369
implemented for the sequential execution

00:25:48,279 --> 00:25:54,070
policy as well that gives you two new

00:25:51,369 --> 00:25:55,990
execution policies a parallel task

00:25:54,070 --> 00:25:58,960
execution policy and a sequential task

00:25:55,990 --> 00:26:00,909
execution policy in all those cases

00:25:58,960 --> 00:26:04,869
formally synchronous functions written

00:26:00,909 --> 00:26:07,629
on the future the parallel construct

00:26:04,869 --> 00:26:10,690
will be exit exit executive executed

00:26:07,629 --> 00:26:13,179
asynchronously on a side and you can go

00:26:10,690 --> 00:26:15,940
ahead was on the current sweat was other

00:26:13,179 --> 00:26:19,749
operations and synchronize on your own

00:26:15,940 --> 00:26:21,639
time was it was a return future which

00:26:19,749 --> 00:26:23,529
allow us to integrate these parallel

00:26:21,639 --> 00:26:26,139
algorithms very nicely into other a

00:26:23,529 --> 00:26:26,820
synchronous control flows which is very

00:26:26,139 --> 00:26:30,630
very power

00:26:26,820 --> 00:26:36,539
for feature another thing we did there

00:26:30,630 --> 00:26:37,950
based on the data power work which is

00:26:36,539 --> 00:26:41,700
currently done to integrate

00:26:37,950 --> 00:26:43,830
vectorization with with C++ we

00:26:41,700 --> 00:26:45,990
implemented two other or four other

00:26:43,830 --> 00:26:48,929
execution policies which we for now

00:26:45,990 --> 00:26:51,360
called data power and data port task

00:26:48,929 --> 00:26:53,639
that might conflict with other names so

00:26:51,360 --> 00:26:55,380
please take all those names with a grain

00:26:53,639 --> 00:26:57,269
of salt we can discuss those we can

00:26:55,380 --> 00:26:59,309
change those but just for the sake of

00:26:57,269 --> 00:27:01,259
the discussion let's assume that the

00:26:59,309 --> 00:27:04,049
execution policies are called data power

00:27:01,259 --> 00:27:06,779
and data SEC and those execution

00:27:04,049 --> 00:27:09,179
policies don't do anything in terms of

00:27:06,779 --> 00:27:11,309
parallelization or not polarization but

00:27:09,179 --> 00:27:13,320
what they do they transform the code in

00:27:11,309 --> 00:27:18,320
a way so that your lambda can be

00:27:13,320 --> 00:27:20,399
vectorized so if you have mostly

00:27:18,320 --> 00:27:22,919
arithmetic operations or other

00:27:20,399 --> 00:27:26,549
vectorizable operations in in your in

00:27:22,919 --> 00:27:28,620
your lambda then the code will be

00:27:26,549 --> 00:27:30,690
transformed in a way that your lambda is

00:27:28,620 --> 00:27:34,259
not invoked with the value type of your

00:27:30,690 --> 00:27:37,500
containers but with a special type which

00:27:34,259 --> 00:27:38,580
is a vector tag essentially which is pre

00:27:37,500 --> 00:27:40,529
initialized by the underlying

00:27:38,580 --> 00:27:42,929
implementation and your lambda will work

00:27:40,529 --> 00:27:45,029
on the vector pack instead of the

00:27:42,929 --> 00:27:48,110
original value type and I will give you

00:27:45,029 --> 00:27:50,610
some examples later on about that

00:27:48,110 --> 00:27:53,009
currently we have built that on top of

00:27:50,610 --> 00:27:55,889
an external library which is we see it's

00:27:53,009 --> 00:27:58,139
a library floating by matías fretes one

00:27:55,889 --> 00:28:00,179
of the authors of office of one of the

00:27:58,139 --> 00:28:02,370
vectorization proposal and which is

00:28:00,179 --> 00:28:04,440
currently being discussed i am in

00:28:02,370 --> 00:28:08,850
contact with joe it's Joel here Joel

00:28:04,440 --> 00:28:12,509
focu I think he has a talk and talk yeah

00:28:08,850 --> 00:28:14,700
he's talking currently and we we want to

00:28:12,509 --> 00:28:16,559
do the same integration with boost's MD

00:28:14,700 --> 00:28:19,799
which is an equivalent library he is

00:28:16,559 --> 00:28:21,809
currently been developing the only

00:28:19,799 --> 00:28:24,899
caveat des said it requires the use of

00:28:21,809 --> 00:28:28,110
generic lambdas or polymorphic function

00:28:24,899 --> 00:28:31,169
objects because the algorithm transforms

00:28:28,110 --> 00:28:33,240
the code and the data types your your

00:28:31,169 --> 00:28:35,549
lambda now has to be instantiated with

00:28:33,240 --> 00:28:37,230
several different data types for the

00:28:35,549 --> 00:28:39,910
plain value type and for the pack and

00:28:37,230 --> 00:28:42,640
and so on and so on but that is

00:28:39,910 --> 00:28:46,780
shouldn't be a problem it's just nice to

00:28:42,640 --> 00:28:50,080
know so a bit more about executives what

00:28:46,780 --> 00:28:52,480
is an executor executives must implement

00:28:50,080 --> 00:28:55,299
one function in our implementation at

00:28:52,480 --> 00:28:56,980
least note that might change as I said

00:28:55,299 --> 00:28:58,600
the there is currently a lot of

00:28:56,980 --> 00:29:00,640
discussion going on in the standards

00:28:58,600 --> 00:29:02,799
community we have a weekly phone call

00:29:00,640 --> 00:29:05,230
where these things are being discussed

00:29:02,799 --> 00:29:09,789
Michael talked about that this morning a

00:29:05,230 --> 00:29:11,289
bit more detail for us executives need

00:29:09,789 --> 00:29:14,549
to implement one function which is

00:29:11,289 --> 00:29:17,950
called async execute and you give it a

00:29:14,549 --> 00:29:19,929
very radical or in this case it's an

00:29:17,950 --> 00:29:22,750
honorary function object but we

00:29:19,929 --> 00:29:24,640
implemented a very etic interface which

00:29:22,750 --> 00:29:25,960
launches that function with a given set

00:29:24,640 --> 00:29:29,500
of parameters and gives you a future

00:29:25,960 --> 00:29:32,049
back and that's how that is done it's

00:29:29,500 --> 00:29:35,530
completely to the up to the executor

00:29:32,049 --> 00:29:38,860
instance you're dealing with and we

00:29:35,530 --> 00:29:42,100
access those executive functions through

00:29:38,860 --> 00:29:45,490
a trades object and this might change as

00:29:42,100 --> 00:29:47,350
well people in the Ennis and the current

00:29:45,490 --> 00:29:49,750
discussion might go in the direction

00:29:47,350 --> 00:29:51,700
that there will not be a single straight

00:29:49,750 --> 00:29:54,190
object but a special overload for each

00:29:51,700 --> 00:29:57,520
of the of the functions but that's

00:29:54,190 --> 00:30:02,590
details so and and that trades object is

00:29:57,520 --> 00:30:04,690
important because what you might they're

00:30:02,590 --> 00:30:06,730
essentially four functions you or five

00:30:04,690 --> 00:30:09,400
functionalities you want to get from an

00:30:06,730 --> 00:30:11,679
executive a fire-and-forget operation

00:30:09,400 --> 00:30:14,260
run that at some point I don't care

00:30:11,679 --> 00:30:17,020
about the result run that as

00:30:14,260 --> 00:30:19,480
synchronously give me a future of which

00:30:17,020 --> 00:30:21,970
represents a result of that function run

00:30:19,480 --> 00:30:25,240
that synchronously I will wait for it

00:30:21,970 --> 00:30:28,299
and two functions for bulk async and

00:30:25,240 --> 00:30:30,250
bulk sync operations which allowed to

00:30:28,299 --> 00:30:32,710
run many tasks which is important in the

00:30:30,250 --> 00:30:35,830
GPU context or or for loops and things

00:30:32,710 --> 00:30:39,309
like that and what the traits do the

00:30:35,830 --> 00:30:41,740
traits are able to simulate part of that

00:30:39,309 --> 00:30:43,480
functionality if the executor doesn't

00:30:41,740 --> 00:30:45,669
implement all of those functions so you

00:30:43,480 --> 00:30:47,260
it's very simple to implement an

00:30:45,669 --> 00:30:49,260
executor in the end because all you have

00:30:47,260 --> 00:30:52,510
to do you implement one function and

00:30:49,260 --> 00:30:52,890
everything else can be emulated on top

00:30:52,510 --> 00:30:55,530
of the

00:30:52,890 --> 00:31:01,830
one function and the traits will do that

00:30:55,530 --> 00:31:05,790
for you yes Oh both of the async calls

00:31:01,830 --> 00:31:08,490
return a future the single function

00:31:05,790 --> 00:31:10,980
async and debulk async return the future

00:31:08,490 --> 00:31:12,720
so you can synchronize with with the

00:31:10,980 --> 00:31:14,900
work just to give you a couple of

00:31:12,720 --> 00:31:17,520
examples of executives we implemented

00:31:14,900 --> 00:31:19,260
sequential executors obviously parallel

00:31:17,520 --> 00:31:21,120
executor that's the default ones which

00:31:19,260 --> 00:31:23,670
are bound to the execution policies is

00:31:21,120 --> 00:31:27,180
this read executor which guarantees that

00:31:23,670 --> 00:31:29,160
swings are executed on this read a

00:31:27,180 --> 00:31:31,350
distribution policy executor which

00:31:29,160 --> 00:31:35,190
allows you to tell hey I want to run it

00:31:31,350 --> 00:31:36,960
over there on this node or please here's

00:31:35,190 --> 00:31:40,590
a set of nodes you decide on your own

00:31:36,960 --> 00:31:43,350
where you want to run things a parallel

00:31:40,590 --> 00:31:46,560
executor I will talk about that a bit

00:31:43,350 --> 00:31:48,600
later in more detail which allows you to

00:31:46,560 --> 00:31:50,310
specify the new model main or constrain

00:31:48,600 --> 00:31:53,030
the set of course you actually want to

00:31:50,310 --> 00:31:57,120
run that code on very important or a

00:31:53,030 --> 00:31:59,400
kuda executor which you can use to run

00:31:57,120 --> 00:32:02,880
things on a particular device connected

00:31:59,400 --> 00:32:07,800
some anywhere a new system and so on and

00:32:02,880 --> 00:32:11,400
so on ok a bit worried about the

00:32:07,800 --> 00:32:14,940
executor parameters we implemented the

00:32:11,400 --> 00:32:18,300
same scheme as for the executor executor

00:32:14,940 --> 00:32:23,610
traits pair we call the parameters and

00:32:18,300 --> 00:32:26,880
executor parameters traits various

00:32:23,610 --> 00:32:29,100
execution parameters are possibly you

00:32:26,880 --> 00:32:32,220
can be specified some examples there are

00:32:29,100 --> 00:32:34,290
as I already said control the grain size

00:32:32,220 --> 00:32:36,660
of work how do you chunk up your your

00:32:34,290 --> 00:32:38,760
loop when you paralyze it that's very

00:32:36,660 --> 00:32:43,100
similar to what open Peters was the

00:32:38,760 --> 00:32:45,300
static dynamic guided chunking policies

00:32:43,100 --> 00:32:47,100
but it allows a much more fine control

00:32:45,300 --> 00:32:49,170
because you can define your own champion

00:32:47,100 --> 00:32:51,630
policy easily just by implementing one

00:32:49,170 --> 00:32:56,640
function which tells the the algorithm

00:32:51,630 --> 00:32:59,190
how to chunk up things we use it for our

00:32:56,640 --> 00:33:01,740
parallel algorithms obviously to to be

00:32:59,190 --> 00:33:05,950
able to control how how things are being

00:33:01,740 --> 00:33:09,350
chunked another example if you deal with

00:33:05,950 --> 00:33:11,270
certain systems and and GPU integration

00:33:09,350 --> 00:33:13,760
and the compiler toolchains I was not

00:33:11,270 --> 00:33:18,740
too great nowadays we have to deal with

00:33:13,760 --> 00:33:22,010
several vendors cura is one circle from

00:33:18,740 --> 00:33:24,710
code players one HC or MD has their own

00:33:22,010 --> 00:33:27,080
tool chain to integrate GPUs and all of

00:33:24,710 --> 00:33:30,470
them have some quirks where you have to

00:33:27,080 --> 00:33:33,110
specify or give give your caramel which

00:33:30,470 --> 00:33:35,090
has to be run on the GPU a unique name

00:33:33,110 --> 00:33:37,580
because compiler is not able to figure

00:33:35,090 --> 00:33:40,190
that out so the programmer has to give

00:33:37,580 --> 00:33:43,250
that name and that would be for instance

00:33:40,190 --> 00:33:46,850
one of the executive parameters giving

00:33:43,250 --> 00:33:48,710
the GPU kernel a name another example

00:33:46,850 --> 00:33:50,600
and I will should have an example in the

00:33:48,710 --> 00:33:55,070
very end some measurements but for that

00:33:50,600 --> 00:33:58,700
is do simple array of prefetching so

00:33:55,070 --> 00:34:04,160
that you can automatically prefetch

00:33:58,700 --> 00:34:05,990
certain iterations ahead of time so that

00:34:04,160 --> 00:34:08,389
when when the execution comes to that

00:34:05,990 --> 00:34:10,190
iteration the values will be already

00:34:08,389 --> 00:34:12,260
loaded into the caches and things like

00:34:10,190 --> 00:34:14,629
that again you can write your own

00:34:12,260 --> 00:34:17,210
executive parameters and implementations

00:34:14,629 --> 00:34:21,230
of that easily and customize the way

00:34:17,210 --> 00:34:24,710
these algorithms actually work ok that's

00:34:21,230 --> 00:34:26,899
one part namely placement of execution

00:34:24,710 --> 00:34:30,710
where to run things how to run things

00:34:26,899 --> 00:34:33,409
went to run things another problem when

00:34:30,710 --> 00:34:35,600
you deal and when you want to make

00:34:33,409 --> 00:34:37,210
parallelism efficient is that you have

00:34:35,600 --> 00:34:42,080
to be able to control data placement

00:34:37,210 --> 00:34:44,060
where is my data which is a spec on a

00:34:42,080 --> 00:34:45,830
single note that might not be that

00:34:44,060 --> 00:34:48,440
important as long as you're not dealing

00:34:45,830 --> 00:34:50,389
with new mud effects in your algorithms

00:34:48,440 --> 00:34:52,730
because no matter where the data is

00:34:50,389 --> 00:34:55,370
placed all course can access all all

00:34:52,730 --> 00:34:58,190
data in the main memory but when you're

00:34:55,370 --> 00:35:00,470
dealing with five GPUs connected to your

00:34:58,190 --> 00:35:02,150
system you better place the data on the

00:35:00,470 --> 00:35:05,300
GPU where you're going to run the code

00:35:02,150 --> 00:35:08,660
otherwise it will fail so data placement

00:35:05,300 --> 00:35:11,030
isn't very important second facet and I

00:35:08,660 --> 00:35:12,980
would like to give you some ideas what

00:35:11,030 --> 00:35:16,400
we can do in that regard and how we can

00:35:12,980 --> 00:35:20,109
expose data placement to the user in a

00:35:16,400 --> 00:35:20,109
very very generic very nice way

00:35:20,309 --> 00:35:24,920
generally there are different strategies

00:35:21,869 --> 00:35:28,170
you have to implement for different

00:35:24,920 --> 00:35:31,140
platforms as I already said for

00:35:28,170 --> 00:35:33,119
Newmarket Texas you use one way for GPUs

00:35:31,140 --> 00:35:35,640
used something completely different and

00:35:33,119 --> 00:35:38,430
for distributed systems if you want to

00:35:35,640 --> 00:35:40,559
do data make data placement efficient

00:35:38,430 --> 00:35:43,079
and and do load balancing across nodes

00:35:40,559 --> 00:35:46,349
then in distributed systems you have to

00:35:43,079 --> 00:35:49,619
do completely different things

00:35:46,349 --> 00:35:51,569
what we decided to go for is again the

00:35:49,619 --> 00:35:53,609
same as we did with everything else we

00:35:51,569 --> 00:35:55,260
just took what this done it has in this

00:35:53,609 --> 00:35:58,470
case is done at a locator and the

00:35:55,260 --> 00:36:00,599
allocator traits and extended those for

00:35:58,470 --> 00:36:03,599
for the use cases we had in mind to make

00:36:00,599 --> 00:36:07,650
make it more efficient essentially what

00:36:03,599 --> 00:36:09,750
we did we added additional functions to

00:36:07,650 --> 00:36:13,079
the standard a locator for bulk

00:36:09,750 --> 00:36:15,599
operations for allocation the allocation

00:36:13,079 --> 00:36:17,400
construction destruction which is very

00:36:15,599 --> 00:36:24,859
useful when you want to work with data

00:36:17,400 --> 00:36:27,569
on the GPU for instance but generally

00:36:24,859 --> 00:36:30,809
it's it's really just a a linear

00:36:27,569 --> 00:36:32,250
extension of the of the a locator

00:36:30,809 --> 00:36:34,079
interface

00:36:32,250 --> 00:36:38,160
additionally we edit our own

00:36:34,079 --> 00:36:41,010
implementation of or yeah vector I'd say

00:36:38,160 --> 00:36:43,589
implementation and a data type which we

00:36:41,010 --> 00:36:46,740
called partition vector we added the

00:36:43,589 --> 00:36:49,559
vector with a special yeah was full

00:36:46,740 --> 00:36:52,430
allocated support and which which takes

00:36:49,559 --> 00:36:56,640
advantage of the extensions of our of

00:36:52,430 --> 00:36:58,650
the allocator extensions we implemented

00:36:56,640 --> 00:37:00,990
to give you the same interface as a

00:36:58,650 --> 00:37:02,460
standard vector which allows to manage

00:37:00,990 --> 00:37:05,520
the data localities through that a

00:37:02,460 --> 00:37:07,500
locator and which uses the execution

00:37:05,520 --> 00:37:10,950
target and I will talk about targets in

00:37:07,500 --> 00:37:13,140
a minute for the data placement the nice

00:37:10,950 --> 00:37:15,390
thing here is that it allows to do

00:37:13,140 --> 00:37:18,630
direct manipulation of data on a GPU for

00:37:15,390 --> 00:37:21,539
instance from the GPU and from the CPU

00:37:18,630 --> 00:37:23,490
at the same time so it has special magic

00:37:21,539 --> 00:37:26,279
implemented so that we can access the

00:37:23,490 --> 00:37:28,950
data from both ends partition vector is

00:37:26,279 --> 00:37:31,650
the data structure we implemented mostly

00:37:28,950 --> 00:37:32,750
for the distributed case essentially

00:37:31,650 --> 00:37:35,010
it's a

00:37:32,750 --> 00:37:37,079
object which looks like a standard

00:37:35,010 --> 00:37:40,260
vector but the data is not contiguous

00:37:37,079 --> 00:37:42,420
its chunked up into partitions and those

00:37:40,260 --> 00:37:45,299
partitions can be located on different

00:37:42,420 --> 00:37:49,260
nodes in a system or on different GPUs

00:37:45,299 --> 00:37:53,700
in the system and the the the chunks or

00:37:49,260 --> 00:37:56,339
the partitions taken together form the

00:37:53,700 --> 00:37:58,650
data managed by this petition vector so

00:37:56,339 --> 00:38:01,079
it's a segmented data store where the

00:37:58,650 --> 00:38:03,390
segment's themselves are the hpx vectors

00:38:01,079 --> 00:38:05,309
again obviously and you can use

00:38:03,390 --> 00:38:07,319
different things to decide how to

00:38:05,309 --> 00:38:08,849
distribute these these partitions over

00:38:07,319 --> 00:38:12,589
over the machine and so on and so on I

00:38:08,849 --> 00:38:12,589
will give you a code example in a minute

00:38:14,210 --> 00:38:19,589
yeah as I already mentioned it involves

00:38:17,450 --> 00:38:22,079
extending the allocator traits as well

00:38:19,589 --> 00:38:24,839
well if you extend the allocator you

00:38:22,079 --> 00:38:28,410
want to extend the traits as well which

00:38:24,839 --> 00:38:31,589
was functionality which allows us to

00:38:28,410 --> 00:38:34,260
copy data that's very trivial for CPUs

00:38:31,589 --> 00:38:36,299
but as you know when you work with GPUs

00:38:34,260 --> 00:38:39,000
you you have to explicitly move the data

00:38:36,299 --> 00:38:42,210
from CPU to the GPU in order to work on

00:38:39,000 --> 00:38:43,980
it and we extend the allocated trades

00:38:42,210 --> 00:38:46,529
which allows us to do that in a very

00:38:43,980 --> 00:38:49,680
very efficient way allowing to overlap

00:38:46,529 --> 00:38:53,160
the the move the data movement from the

00:38:49,680 --> 00:38:56,819
CPU to the device and back was with

00:38:53,160 --> 00:38:59,069
local work on the on a CPU well and

00:38:56,819 --> 00:39:02,519
distributed that maps onto network pause

00:38:59,069 --> 00:39:05,549
possibly a remote DMA operations and so

00:39:02,519 --> 00:39:08,549
on and that's implemented in our our

00:39:05,549 --> 00:39:12,720
networking layer I'm not going into any

00:39:08,549 --> 00:39:16,109
details here we added the functionality

00:39:12,720 --> 00:39:18,990
access single elements again on a CPU

00:39:16,109 --> 00:39:24,359
that's trivial on the GPU if you want to

00:39:18,990 --> 00:39:26,579
access a single element over the from

00:39:24,359 --> 00:39:28,259
the device from the CPU that's certainly

00:39:26,579 --> 00:39:30,269
expensive but for debugging purposes

00:39:28,259 --> 00:39:31,950
sometimes that's nice the same is true

00:39:30,269 --> 00:39:34,799
for the network if you really want to

00:39:31,950 --> 00:39:36,240
grab only one value you can do that you

00:39:34,799 --> 00:39:38,190
might not want to do that all the time

00:39:36,240 --> 00:39:42,740
but for debugging purposes that might be

00:39:38,190 --> 00:39:42,740
useful okay

00:39:44,210 --> 00:39:50,270
we introduce the type which we call

00:39:46,400 --> 00:39:51,080
execution target and again the name can

00:39:50,270 --> 00:39:53,720
be changed

00:39:51,080 --> 00:39:55,670
we are not attached to it but what we

00:39:53,720 --> 00:39:57,530
need but I mean by a target is

00:39:55,670 --> 00:40:00,770
essentially just a place on the system

00:39:57,530 --> 00:40:02,810
where the data can be located and the

00:40:00,770 --> 00:40:07,010
execution is kind of connected to it and

00:40:02,810 --> 00:40:07,850
we use these target objects to a create

00:40:07,010 --> 00:40:09,800
an alligator

00:40:07,850 --> 00:40:12,410
to do the data placement and be to

00:40:09,800 --> 00:40:14,360
create an executor to co-locate the

00:40:12,410 --> 00:40:19,550
execution was with was a site where

00:40:14,360 --> 00:40:21,860
where the data is located several types

00:40:19,550 --> 00:40:24,620
of those targets for instance kewda

00:40:21,860 --> 00:40:26,690
target or a host target or a distributed

00:40:24,620 --> 00:40:30,020
target and so on and you can implement

00:40:26,690 --> 00:40:32,510
your own then we initialize a locator

00:40:30,020 --> 00:40:35,180
from those targets which gives us a new

00:40:32,510 --> 00:40:39,260
model main target when we use a host

00:40:35,180 --> 00:40:42,590
argot or a GPU device a locator which

00:40:39,260 --> 00:40:46,280
allows to allocate data on on GPU or a

00:40:42,590 --> 00:40:50,120
locality based target allocate data over

00:40:46,280 --> 00:40:52,010
there on this node place the same target

00:40:50,120 --> 00:40:54,790
object will be used for creating the

00:40:52,010 --> 00:40:58,670
executor and that guarantees that the

00:40:54,790 --> 00:41:00,200
executor will run things exactly what

00:40:58,670 --> 00:41:02,210
the data is located and that's what you

00:41:00,200 --> 00:41:06,050
want to achieve in the end so this

00:41:02,210 --> 00:41:07,910
target allows us to abstract the data

00:41:06,050 --> 00:41:10,340
placement and the execution placement

00:41:07,910 --> 00:41:14,600
and give the user full control well

00:41:10,340 --> 00:41:16,820
still utilize all the pre defined

00:41:14,600 --> 00:41:19,490
facilities and the parallel algorithms

00:41:16,820 --> 00:41:21,440
on other paralyzation facilities in the

00:41:19,490 --> 00:41:23,780
system so it's really just a set of

00:41:21,440 --> 00:41:27,560
customization points essentially we give

00:41:23,780 --> 00:41:29,540
you as a user so that you can control

00:41:27,560 --> 00:41:33,200
what the parallel algorithms and all the

00:41:29,540 --> 00:41:36,470
other parallel facilities we expose do

00:41:33,200 --> 00:41:41,270
and how they work and and what are the

00:41:36,470 --> 00:41:43,670
specifics ok let me give you some some

00:41:41,270 --> 00:41:49,940
results and that might might clarify a

00:41:43,670 --> 00:41:53,840
lot of of these things this example is

00:41:49,940 --> 00:41:56,660
taken from Shawn parents talk a couple

00:41:53,840 --> 00:41:57,960
of years ago and what what he did there

00:41:56,660 --> 00:42:00,750
is said ok

00:41:57,960 --> 00:42:02,820
I have a sequence of elements and some

00:42:00,750 --> 00:42:04,260
of the elements have a flag said and

00:42:02,820 --> 00:42:08,190
some of the elements have not arrived

00:42:04,260 --> 00:42:10,230
the flag not set and what I want to do I

00:42:08,190 --> 00:42:13,380
want to grab those elements I want to

00:42:10,230 --> 00:42:15,420
plug them into the middle for instance I

00:42:13,380 --> 00:42:17,369
mark up my emails and my email client

00:42:15,420 --> 00:42:19,070
click click click click click drag them

00:42:17,369 --> 00:42:21,450
and put them somewhere else

00:42:19,070 --> 00:42:23,550
so if you want to implement this kind of

00:42:21,450 --> 00:42:25,830
algorithm you probably would come up

00:42:23,550 --> 00:42:28,260
with a naive implementation which would

00:42:25,830 --> 00:42:30,690
look horrible but as it turned out you

00:42:28,260 --> 00:42:33,720
can very simply implement that by

00:42:30,690 --> 00:42:36,420
splitting that problem into two problems

00:42:33,720 --> 00:42:38,130
the problem above the red line and below

00:42:36,420 --> 00:42:39,810
the red line and now you can see that

00:42:38,130 --> 00:42:42,660
it's really just two partitioning steps

00:42:39,810 --> 00:42:44,400
I want to do above the red line I want

00:42:42,660 --> 00:42:46,200
to get all elements which are not marked

00:42:44,400 --> 00:42:47,820
first and then the marked ones and below

00:42:46,200 --> 00:42:49,500
the red lines I want to get all of those

00:42:47,820 --> 00:42:51,930
which are marked and then the unmarked

00:42:49,500 --> 00:42:54,450
ones and luckily the standard template

00:42:51,930 --> 00:42:57,359
library already gives us a facility for

00:42:54,450 --> 00:43:00,630
that which is called stable petition and

00:42:57,359 --> 00:43:04,170
I created or Sean created a function

00:43:00,630 --> 00:43:07,410
which he called Gaza which is called

00:43:04,170 --> 00:43:10,950
with four arguments the first the last

00:43:07,410 --> 00:43:13,020
of the whole sequence PSC required

00:43:10,950 --> 00:43:15,150
insertion point where I want to drop the

00:43:13,020 --> 00:43:18,180
marked elements and the binary predictor

00:43:15,150 --> 00:43:19,500
the the the predicate is a function

00:43:18,180 --> 00:43:22,020
which will be called for each element

00:43:19,500 --> 00:43:26,250
which will decide whether that thing has

00:43:22,020 --> 00:43:28,830
has been marked or not so cos stable

00:43:26,250 --> 00:43:32,160
petition from beginning to the insertion

00:43:28,830 --> 00:43:34,680
point and that's a partition was he was

00:43:32,160 --> 00:43:36,030
inverted predicates and then the stable

00:43:34,680 --> 00:43:38,810
partition from the insertion point to

00:43:36,030 --> 00:43:41,390
the end was it was a straight predicate

00:43:38,810 --> 00:43:45,810
stable partition gives you back the

00:43:41,390 --> 00:43:47,070
point of the of the partitioning so the

00:43:45,810 --> 00:43:49,410
overall function gatha

00:43:47,070 --> 00:43:52,200
gives you back the region of the object

00:43:49,410 --> 00:43:54,630
where the the marked objects have been

00:43:52,200 --> 00:43:56,310
inserted into the overall sequence it's

00:43:54,630 --> 00:44:00,210
just two iterators which would give you

00:43:56,310 --> 00:44:02,760
those now what I want to show is that

00:44:00,210 --> 00:44:05,640
what you can do and how you can create a

00:44:02,760 --> 00:44:07,830
synchronous algorithm which does the

00:44:05,640 --> 00:44:10,800
same thing but in a synchronous manner

00:44:07,830 --> 00:44:12,270
so that you can launch that thing let it

00:44:10,800 --> 00:44:14,250
run on the side and you get back

00:44:12,270 --> 00:44:18,540
the future which will present the

00:44:14,250 --> 00:44:20,820
execution of these two things well I

00:44:18,540 --> 00:44:23,040
just called the function get gaza async

00:44:20,820 --> 00:44:24,869
now and the difference is they are all

00:44:23,040 --> 00:44:27,180
the arguments are the same but it now

00:44:24,869 --> 00:44:32,310
returns a future representing the pair

00:44:27,180 --> 00:44:35,250
of iterators we use stable partition

00:44:32,310 --> 00:44:37,530
with the task execution policy that

00:44:35,250 --> 00:44:39,869
means that this will be launched on a

00:44:37,530 --> 00:44:42,599
side I get a future which will present

00:44:39,869 --> 00:44:44,970
the whole result for the one partition I

00:44:42,599 --> 00:44:47,550
do the same for the other partition I

00:44:44,970 --> 00:44:50,099
now have two futures representing the

00:44:47,550 --> 00:44:51,960
two executions of the two partitioning

00:44:50,099 --> 00:44:54,690
steps and the interesting thing is that

00:44:51,960 --> 00:44:57,119
I already get a additional

00:44:54,690 --> 00:44:58,980
parallelization effect here because not

00:44:57,119 --> 00:45:00,900
only each of the algorithms runs in

00:44:58,980 --> 00:45:03,480
parallel but the two algorithms run at

00:45:00,900 --> 00:45:04,770
the same time because they really just

00:45:03,480 --> 00:45:08,580
kick off the operation of those

00:45:04,770 --> 00:45:11,310
petitions and the next code looks a bit

00:45:08,580 --> 00:45:13,109
more complex and I will explain it and I

00:45:11,310 --> 00:45:16,320
will simplify it and in a second so

00:45:13,109 --> 00:45:19,170
don't get too upset about it what

00:45:16,320 --> 00:45:21,260
happens here is dataflow is a function

00:45:19,170 --> 00:45:24,930
which is very much like async

00:45:21,260 --> 00:45:28,680
it gets a function to call on a new

00:45:24,930 --> 00:45:30,570
thread and a set of arguments and if one

00:45:28,680 --> 00:45:33,810
or more of the arguments to dataflow in

00:45:30,570 --> 00:45:36,780
this case f1 and f2 our futures then

00:45:33,810 --> 00:45:39,240
dataflow will wait with launching the

00:45:36,780 --> 00:45:41,910
new thread until all the futures have

00:45:39,240 --> 00:45:44,490
become ready so it will just delay the

00:45:41,910 --> 00:45:47,790
the scheduling of the function which in

00:45:44,490 --> 00:45:50,280
our case is that return make are 1 or 2

00:45:47,790 --> 00:45:53,099
and it will pass those arguments

00:45:50,280 --> 00:45:56,460
directly to the function the unrep

00:45:53,099 --> 00:45:58,890
facility here you can see that lambda is

00:45:56,460 --> 00:46:01,230
put into an unwrapped just gets rid of

00:45:58,890 --> 00:46:03,450
the futures adjust since we know that

00:46:01,230 --> 00:46:05,369
the function will be invoked only want

00:46:03,450 --> 00:46:07,339
that at the point when the futures have

00:46:05,369 --> 00:46:09,839
become ready we can simply call get and

00:46:07,339 --> 00:46:12,750
unwrap does as for us and forwards the

00:46:09,839 --> 00:46:15,930
actual values so dataflow will return a

00:46:12,750 --> 00:46:18,089
future to result of the function which

00:46:15,930 --> 00:46:21,240
in our case is the future to the pair of

00:46:18,089 --> 00:46:24,180
of iterators we want to have well you

00:46:21,240 --> 00:46:26,740
might say that's complex code I don't

00:46:24,180 --> 00:46:31,240
want to write this kind of code

00:46:26,740 --> 00:46:35,260
well.here course proposal of coroutines

00:46:31,240 --> 00:46:39,130
comes in because if you use coal wait

00:46:35,260 --> 00:46:41,410
which is the one part of the of the

00:46:39,130 --> 00:46:44,380
co-routine proposal you get exactly the

00:46:41,410 --> 00:46:46,930
same semantics in an absolutely straight

00:46:44,380 --> 00:46:49,750
way essentially what we've done is we

00:46:46,930 --> 00:46:51,850
turned our straight code which Sean

00:46:49,750 --> 00:46:54,880
wrote three years ago into the

00:46:51,850 --> 00:46:57,250
equivalent a synchronous code just by

00:46:54,880 --> 00:46:59,650
changing two small things we changed the

00:46:57,250 --> 00:47:02,109
execution policy and we added the co way

00:46:59,650 --> 00:47:06,010
to to get the values out of all those

00:47:02,109 --> 00:47:07,780
futures and we did another thing if you

00:47:06,010 --> 00:47:10,480
think about it what does that function

00:47:07,780 --> 00:47:12,460
do that function gives me future or

00:47:10,480 --> 00:47:15,990
presenting the pair of iterators and

00:47:12,460 --> 00:47:18,730
that future depends on two other futures

00:47:15,990 --> 00:47:21,220
which are returned from the partitioning

00:47:18,730 --> 00:47:24,430
steps so essentially this function gives

00:47:21,220 --> 00:47:26,500
you an execution tree which represents

00:47:24,430 --> 00:47:30,280
the execution of your algorithm you have

00:47:26,500 --> 00:47:32,560
inside so we turned our file our code

00:47:30,280 --> 00:47:36,070
which was straight synchronous execution

00:47:32,560 --> 00:47:37,960
into a code which does not do anything

00:47:36,070 --> 00:47:40,450
but giving you an execution tree

00:47:37,960 --> 00:47:43,119
representing the algorithm and when that

00:47:40,450 --> 00:47:45,310
execution tree is executed you actually

00:47:43,119 --> 00:47:47,100
do the work and you get the result so

00:47:45,310 --> 00:47:50,320
essentially it's out of parallelization

00:47:47,100 --> 00:47:54,340
because the execution of that execution

00:47:50,320 --> 00:47:56,050
tree can be performed at full speed it

00:47:54,340 --> 00:47:58,660
will just flow there will be no

00:47:56,050 --> 00:48:01,180
synchronization it will just fully

00:47:58,660 --> 00:48:04,210
paralyzed utilize a system in them to

00:48:01,180 --> 00:48:07,390
the full extent and that technique which

00:48:04,210 --> 00:48:10,180
we call future ization can be applied to

00:48:07,390 --> 00:48:12,760
arbitrary complex algorithms so you take

00:48:10,180 --> 00:48:14,320
your textbook algorithm apply the future

00:48:12,760 --> 00:48:16,780
ization technique and you get a fully

00:48:14,320 --> 00:48:20,170
automatically paralyzed algorithm just

00:48:16,780 --> 00:48:22,780
like that just by using futures as see

00:48:20,170 --> 00:48:26,400
data dependencies because futures

00:48:22,780 --> 00:48:30,359
represent data dependencies nothing else

00:48:26,400 --> 00:48:32,410
okay another couple of another examples

00:48:30,359 --> 00:48:34,810
you might have heard of the stream

00:48:32,410 --> 00:48:37,960
benchmark the stream benchmark is

00:48:34,810 --> 00:48:38,980
usually used to assess the memory

00:48:37,960 --> 00:48:41,410
bandwidth available

00:48:38,980 --> 00:48:43,420
on a particular architecture the

00:48:41,410 --> 00:48:45,280
original stream benchmark is available

00:48:43,420 --> 00:48:46,930
on the web and usually uses for

00:48:45,280 --> 00:48:50,650
assessing the memory bandwidth on a

00:48:46,930 --> 00:48:53,170
single no it usually uses openmp so I

00:48:50,650 --> 00:48:55,780
will compare these stream benchmark

00:48:53,170 --> 00:48:57,970
results with the original open P

00:48:55,780 --> 00:49:00,490
benchmark was implementing the same

00:48:57,970 --> 00:49:02,109
thing without parallel algorithms the

00:49:00,490 --> 00:49:05,500
stream benchmark essentially does four

00:49:02,109 --> 00:49:09,550
things it's for loops well it's three

00:49:05,500 --> 00:49:12,910
years and you perform four loops over

00:49:09,550 --> 00:49:16,390
those arrays you do copy step a scale

00:49:12,910 --> 00:49:18,100
step you add two of the race and then

00:49:16,390 --> 00:49:20,230
you have a try out step where you access

00:49:18,100 --> 00:49:21,700
to arrays and multiply one of the

00:49:20,230 --> 00:49:24,250
factors with it and assign it to the

00:49:21,700 --> 00:49:26,230
third one so the best possible

00:49:24,250 --> 00:49:28,090
performance in this case on a single

00:49:26,230 --> 00:49:29,680
node is certainly only possible when you

00:49:28,090 --> 00:49:32,560
have new more awareness when you when

00:49:29,680 --> 00:49:34,180
you place a data close to your Numa to

00:49:32,560 --> 00:49:36,040
your socket while you run the code on

00:49:34,180 --> 00:49:37,690
because as soon as you have crossed Numa

00:49:36,040 --> 00:49:42,520
domain traffic you you just kill the

00:49:37,690 --> 00:49:44,500
performance in Oatman P that's done

00:49:42,520 --> 00:49:46,930
implicitly by a so-called first touch

00:49:44,500 --> 00:49:49,510
policy so you have to write a rely on

00:49:46,930 --> 00:49:51,520
very arcane low-level features to make

00:49:49,510 --> 00:49:53,080
sure that this read has really run on

00:49:51,520 --> 00:49:55,650
the same socket where they where the

00:49:53,080 --> 00:49:58,960
corresponding data is located and you

00:49:55,650 --> 00:50:03,730
use that pragma OpenMP parallel and you

00:49:58,960 --> 00:50:05,890
schedule statically and we start with

00:50:03,730 --> 00:50:08,020
parallel algorithms that looks like this

00:50:05,890 --> 00:50:12,040
you just have three vectors you init

00:50:08,020 --> 00:50:14,560
data you run the copy step who is a

00:50:12,040 --> 00:50:16,600
parallel execution policy you run the

00:50:14,560 --> 00:50:19,119
transform which is doing the scaling

00:50:16,600 --> 00:50:21,070
another transform which is doing well a

00:50:19,119 --> 00:50:22,810
binary transform which is adding the two

00:50:21,070 --> 00:50:24,330
arrays and another binary transform

00:50:22,810 --> 00:50:26,670
which is performing the triad step

00:50:24,330 --> 00:50:30,280
nothing else so it's really just for

00:50:26,670 --> 00:50:32,830
parallel for loops or for parallel

00:50:30,280 --> 00:50:36,010
algorithms one after another and that

00:50:32,830 --> 00:50:39,270
mimics hundred-percent the way of my

00:50:36,010 --> 00:50:42,670
peers is measuring that memory bandwidth

00:50:39,270 --> 00:50:45,190
now let's do that on it on on a CPU

00:50:42,670 --> 00:50:48,100
target I create an object which is

00:50:45,190 --> 00:50:50,109
called host target give it some

00:50:48,100 --> 00:50:53,050
initialization tell it a run on new

00:50:50,109 --> 00:50:58,990
model main zero for instance

00:50:53,050 --> 00:51:02,260
I create a executor and a allocator from

00:50:58,990 --> 00:51:05,140
their target which will make sure that

00:51:02,260 --> 00:51:08,740
the code will be executed on on Numa

00:51:05,140 --> 00:51:11,470
domain zero and the data is allocated on

00:51:08,740 --> 00:51:14,619
the same very close to that socket and

00:51:11,470 --> 00:51:16,210
then I create three vectors a B and C

00:51:14,619 --> 00:51:18,849
using that a locator which makes sure

00:51:16,210 --> 00:51:21,599
that the data is located on that memory

00:51:18,849 --> 00:51:25,240
close to this new model main zero and

00:51:21,599 --> 00:51:28,240
then I create a execution policy where

00:51:25,240 --> 00:51:31,180
rebind the executor and tell it to do

00:51:28,240 --> 00:51:33,130
static chunking well and the rest is

00:51:31,180 --> 00:51:35,140
history the rest is really the same code

00:51:33,130 --> 00:51:37,540
as before except that instead of the

00:51:35,140 --> 00:51:41,589
power execution policy I use my rebound

00:51:37,540 --> 00:51:44,109
execution policy and that's it if you

00:51:41,589 --> 00:51:48,160
look at what we get when we run it on

00:51:44,109 --> 00:51:50,470
one Numa domain the Green Line is what

00:51:48,160 --> 00:51:52,390
we get with the parallel algorithms the

00:51:50,470 --> 00:51:54,640
blue line above is what the original

00:51:52,390 --> 00:51:58,420
openmp benchmark gives us so we reach

00:51:54,640 --> 00:52:00,220
about 97% of the performance and we

00:51:58,420 --> 00:52:02,230
haven't looked too closely into it to

00:52:00,220 --> 00:52:05,410
optimize it even further but I think we

00:52:02,230 --> 00:52:06,640
can still squeeze out the remaining 3%

00:52:05,410 --> 00:52:09,220
so essentially we get equivalent

00:52:06,640 --> 00:52:11,380
performance to what openmp gives us

00:52:09,220 --> 00:52:14,200
today was full control of a memory

00:52:11,380 --> 00:52:15,820
placement from a very high level when we

00:52:14,200 --> 00:52:18,369
run the same code on to nuuma domains

00:52:15,820 --> 00:52:20,800
you you see very similar behavior there

00:52:18,369 --> 00:52:23,890
are still a 5% abstraction penalty I'd

00:52:20,800 --> 00:52:25,960
say we currently have to deal with but I

00:52:23,890 --> 00:52:28,750
think we we should be able to and we

00:52:25,960 --> 00:52:30,430
will be able to find or to optimize that

00:52:28,750 --> 00:52:31,900
and and reach the full of memory

00:52:30,430 --> 00:52:34,119
performance

00:52:31,900 --> 00:52:36,940
I know beyond a set this morning we want

00:52:34,119 --> 00:52:39,849
to have a speed-up of 10 I would like to

00:52:36,940 --> 00:52:42,990
have a speed-up of 10 as well but please

00:52:39,849 --> 00:52:45,520
keep in mind that openmp really

00:52:42,990 --> 00:52:47,770
perfectly utilizes a memory band was

00:52:45,520 --> 00:52:49,930
here so we are dealing with hardware

00:52:47,770 --> 00:52:51,940
limitations not was code limitations so

00:52:49,930 --> 00:52:56,170
reaching the open p performance is the

00:52:51,940 --> 00:52:59,020
best we can do now let's run the same

00:52:56,170 --> 00:53:01,150
thing on a GPU this time we create a

00:52:59,020 --> 00:53:04,839
target where we specify which device

00:53:01,150 --> 00:53:06,010
actually to use create a different type

00:53:04,839 --> 00:53:09,850
of allocator and the

00:53:06,010 --> 00:53:14,320
type of executor create local data on a

00:53:09,850 --> 00:53:18,160
CPU which we will copy over to the to

00:53:14,320 --> 00:53:20,710
the device create our three vectors a B

00:53:18,160 --> 00:53:25,240
and C these three will be allocated on

00:53:20,710 --> 00:53:30,010
the device then we copy the data from

00:53:25,240 --> 00:53:32,680
the CPU over to the device and look we

00:53:30,010 --> 00:53:34,450
use standard copy and that's done that

00:53:32,680 --> 00:53:36,880
copy just copies the local data over the

00:53:34,450 --> 00:53:39,040
two device and please note if we were

00:53:36,880 --> 00:53:42,100
using instead of power if we were using

00:53:39,040 --> 00:53:43,750
power task as the execution policy that

00:53:42,100 --> 00:53:45,460
copy operation would give us a future

00:53:43,750 --> 00:53:47,470
which would allow us to completely

00:53:45,460 --> 00:53:49,390
overlap the the operation of copying

00:53:47,470 --> 00:53:51,730
data from this CPU over to the device

00:53:49,390 --> 00:53:54,610
behind code on the CPU so we wouldn't

00:53:51,730 --> 00:53:56,770
have to do anything special there and

00:53:54,610 --> 00:54:00,430
then what the stream management as it

00:53:56,770 --> 00:54:02,350
was before there's no change there so

00:54:00,430 --> 00:54:04,300
almost the same code the only difference

00:54:02,350 --> 00:54:06,100
is that we have two different types for

00:54:04,300 --> 00:54:08,650
the executor and for the allocator here

00:54:06,100 --> 00:54:11,170
and things will run on a GPU and if you

00:54:08,650 --> 00:54:14,140
look at the data we get here we can see

00:54:11,170 --> 00:54:16,600
that for very small array sizes the

00:54:14,140 --> 00:54:19,030
overheads off are significant and that's

00:54:16,600 --> 00:54:22,480
probably because of and that's compared

00:54:19,030 --> 00:54:25,870
with was native CUDA the overheads come

00:54:22,480 --> 00:54:31,810
mostly from the synchronization with

00:54:25,870 --> 00:54:34,810
with the CPU code where we rely on CUDA

00:54:31,810 --> 00:54:37,810
invoking a callback function in the user

00:54:34,810 --> 00:54:41,740
code and this relies on operating system

00:54:37,810 --> 00:54:42,700
context switching kewda itself doesn't

00:54:41,740 --> 00:54:45,010
rely on that

00:54:42,700 --> 00:54:47,260
so the overheads come from here but for

00:54:45,010 --> 00:54:48,550
decently sized the race you can see that

00:54:47,260 --> 00:54:49,810
we essentially reach the same

00:54:48,550 --> 00:54:52,810
performance as a native CUDA

00:54:49,810 --> 00:54:55,840
implementation of the of the string

00:54:52,810 --> 00:54:57,960
benchmark for on the right hand side so

00:54:55,840 --> 00:55:02,250
if you zoom in on the on the largest

00:54:57,960 --> 00:55:07,480
array sizes here you can see that for

00:55:02,250 --> 00:55:11,140
compared to Oatman P we reach 95% as

00:55:07,480 --> 00:55:14,170
said and for kewda will reach 99% of the

00:55:11,140 --> 00:55:15,640
performance possible so again the

00:55:14,170 --> 00:55:18,790
simplest possible

00:55:15,640 --> 00:55:19,750
how powerful they are and as you can see

00:55:18,790 --> 00:55:21,430
they're very

00:55:19,750 --> 00:55:24,280
versatile and completely under your

00:55:21,430 --> 00:55:26,260
control don't impose any significant

00:55:24,280 --> 00:55:29,670
overhead over the native implementations

00:55:26,260 --> 00:55:32,800
of the underlying runtime systems which

00:55:29,670 --> 00:55:35,620
companies have been developing for 25

00:55:32,800 --> 00:55:38,650
years I don't know okay

00:55:35,620 --> 00:55:44,110
the last example vectorization and I

00:55:38,650 --> 00:55:47,650
think I'm five minutes right let's do a

00:55:44,110 --> 00:55:50,080
dot product of two vectors just floats

00:55:47,650 --> 00:55:52,030
for simplicity here and we can use in a

00:55:50,080 --> 00:55:54,520
product which is parallel algorithms

00:55:52,030 --> 00:55:57,040
which is part of the parallelism Cheers

00:55:54,520 --> 00:56:01,180
and I hope everybody knows what a dot

00:55:57,040 --> 00:56:03,490
product is you take pairwise you

00:56:01,180 --> 00:56:06,520
multiply the two vectors pairwise and

00:56:03,490 --> 00:56:08,560
add up all the results essentially so

00:56:06,520 --> 00:56:11,020
it's it's a basic operation needed for

00:56:08,560 --> 00:56:12,970
metrics of multiplications mainly and

00:56:11,020 --> 00:56:17,320
and for for other linear algebra

00:56:12,970 --> 00:56:22,510
programs so the inner product gets two

00:56:17,320 --> 00:56:25,030
we pass two operators to it the operator

00:56:22,510 --> 00:56:26,830
to be used to do the pairwise operation

00:56:25,030 --> 00:56:28,930
which is the second one and the operator

00:56:26,830 --> 00:56:30,880
which is used to sum up all the results

00:56:28,930 --> 00:56:32,560
which is the first of the two operator

00:56:30,880 --> 00:56:36,340
so it's essentially plus and multiplies

00:56:32,560 --> 00:56:42,370
and please note I specifically used C++

00:56:36,340 --> 00:56:44,620
14 generic lambdas here and this is just

00:56:42,370 --> 00:56:46,360
giving you parallel performance parallel

00:56:44,620 --> 00:56:48,520
execution of that dot product which is

00:56:46,360 --> 00:56:51,670
very nice but if you change just

00:56:48,520 --> 00:56:53,950
execution policy from power to data

00:56:51,670 --> 00:56:55,270
power you get in addition to

00:56:53,950 --> 00:56:58,480
parallelization you get vectorization

00:56:55,270 --> 00:57:01,330
and now these lambdas are being invoked

00:56:58,480 --> 00:57:02,860
with vector packs so with with a special

00:57:01,330 --> 00:57:05,620
data structure which consists out of

00:57:02,860 --> 00:57:07,870
four of the of the values and the

00:57:05,620 --> 00:57:10,900
operations are performed vectorized on

00:57:07,870 --> 00:57:14,140
these four or eight depending on the on

00:57:10,900 --> 00:57:17,800
the underlying hardware platform values

00:57:14,140 --> 00:57:19,930
so you you essentially gain value you

00:57:17,800 --> 00:57:24,580
get vectorization for free so let me

00:57:19,930 --> 00:57:29,400
show some data here this is what you see

00:57:24,580 --> 00:57:34,529
for two vectors of 100,000 floats

00:57:29,400 --> 00:57:37,170
for the the y-axis is see they speed up

00:57:34,529 --> 00:57:40,950
and the x-axis is a number of course

00:57:37,170 --> 00:57:43,229
this loop runs when you run it on war on

00:57:40,950 --> 00:57:45,059
one core you can see that just by

00:57:43,229 --> 00:57:47,489
changing the execution policy you get a

00:57:45,059 --> 00:57:50,039
speed-up of three which is significant

00:57:47,489 --> 00:57:51,839
so all you have to do we change power

00:57:50,039 --> 00:57:55,440
for data power and you get a speed-up of

00:57:51,839 --> 00:57:58,410
three isn't that great in addition when

00:57:55,440 --> 00:58:01,559
you run this parallel loop on more than

00:57:58,410 --> 00:58:03,119
one core you still see a speed-up but

00:58:01,559 --> 00:58:04,829
the speed up is not as significant

00:58:03,119 --> 00:58:06,719
anymore and the red dotted line gives

00:58:04,829 --> 00:58:08,700
you the ratio between the blue and the

00:58:06,719 --> 00:58:10,769
green lines so the speed-up from

00:58:08,700 --> 00:58:15,950
parallel to vector eyes and parallel

00:58:10,769 --> 00:58:20,400
code is in between 1.3 and n3 roughly

00:58:15,950 --> 00:58:24,299
this is a fairly small array 100,000

00:58:20,400 --> 00:58:26,249
points and here the parallelization

00:58:24,299 --> 00:58:28,349
overhead you can see that the

00:58:26,249 --> 00:58:30,859
paralyzation already takes in fairly

00:58:28,349 --> 00:58:36,869
fairly massively so you don't really

00:58:30,859 --> 00:58:40,229
scale up as well as as you might want so

00:58:36,869 --> 00:58:42,809
if you look at the same for a array size

00:58:40,229 --> 00:58:46,049
of 1 million you have almost perfect

00:58:42,809 --> 00:58:48,569
scaling for the parallel case and you

00:58:46,049 --> 00:58:50,670
see now that the the speed-up of the

00:58:48,569 --> 00:58:53,549
vectorization isn't breaking that fast

00:58:50,670 --> 00:58:56,099
anymore this problem still fits into

00:58:53,549 --> 00:58:58,199
level 3 cache so you can see for certain

00:58:56,099 --> 00:59:00,390
array sizes you can really gain a lot

00:58:58,199 --> 00:59:03,420
from from just changing that that

00:59:00,390 --> 00:59:05,609
execution policy and the last one is for

00:59:03,420 --> 00:59:07,410
10 million points and here the problem

00:59:05,609 --> 00:59:11,699
doesn't fit into the caches anymore so

00:59:07,410 --> 00:59:14,009
additional memory access overheads have

00:59:11,699 --> 00:59:18,269
to be accounted for so the fact is not

00:59:14,009 --> 00:59:20,489
as as as massive anything anymore as

00:59:18,269 --> 00:59:22,559
before but you still can just by

00:59:20,489 --> 00:59:24,449
changing the execution policy you can

00:59:22,559 --> 00:59:28,319
still see you speed up of at least one

00:59:24,449 --> 00:59:29,789
point 3 to 2.5 or so very nice result

00:59:28,319 --> 00:59:32,069
because it's it's essentially cheap

00:59:29,789 --> 00:59:33,839
right all you do is just instead of Tory

00:59:32,069 --> 00:59:36,209
write data per and you get some speed up

00:59:33,839 --> 00:59:38,099
that might not be completely optimal but

00:59:36,209 --> 00:59:41,059
you get a very very nice first step in

00:59:38,099 --> 00:59:41,059
optimizing your code

00:59:42,920 --> 00:59:50,130
let me yeah so I'll skip the the example

00:59:47,700 --> 00:59:52,490
on was there was a partition vector

00:59:50,130 --> 00:59:54,750
because time's running out

00:59:52,490 --> 00:59:56,910
essentially what what this code shows

00:59:54,750 --> 00:59:59,010
you how to you can use a partition

00:59:56,910 --> 01:00:02,180
vector and distribute the partitions

00:59:59,010 --> 01:00:07,080
over several notes and by changing the

01:00:02,180 --> 01:00:09,540
target object do the same on a bunch of

01:00:07,080 --> 01:00:11,250
GPUs so no code change and you

01:00:09,540 --> 01:00:12,900
distribute the partitions over the GPUs

01:00:11,250 --> 01:00:15,920
and then you run the parallel algorithms

01:00:12,900 --> 01:00:18,000
on those partitions in parallel was was

01:00:15,920 --> 01:00:20,520
without you having to deal with the

01:00:18,000 --> 01:00:24,210
partitions and so on and so on so very

01:00:20,520 --> 01:00:28,020
nice the very last example and then now

01:00:24,210 --> 01:00:30,300
I'll stop if you have this for loop and

01:00:28,020 --> 01:00:32,400
you have power execution policy and as

01:00:30,300 --> 01:00:35,580
you can see this loop works on three

01:00:32,400 --> 01:00:37,890
arrays wouldn't it be nice if we could

01:00:35,580 --> 01:00:40,530
prefetch B and see a couple of

01:00:37,890 --> 01:00:42,600
iterations ahead of time so that the the

01:00:40,530 --> 01:00:45,780
Hardware prefetching could make sure

01:00:42,600 --> 01:00:47,970
that B and C are in the caches when we

01:00:45,780 --> 01:00:51,540
need them well the easiest solution is

01:00:47,970 --> 01:00:55,050
this we just say power was prefetch bin

01:00:51,540 --> 01:00:58,320
C and that's it so you can write very

01:00:55,050 --> 01:01:00,720
powerful means of customizing that the

01:00:58,320 --> 01:01:03,450
way these parallel algorithms work and

01:01:00,720 --> 01:01:06,240
you can really drill in and optimize

01:01:03,450 --> 01:01:08,450
things and turn knobs as you wish and

01:01:06,240 --> 01:01:11,930
and get very very nice performance

01:01:08,450 --> 01:01:15,300
because what this prefetching does is

01:01:11,930 --> 01:01:18,140
just gives you about 10-15 percent speed

01:01:15,300 --> 01:01:22,200
up just by prefetching these two arrays

01:01:18,140 --> 01:01:23,790
and I think I'll skip that I think

01:01:22,200 --> 01:01:26,040
that's it so I thank you for your

01:01:23,790 --> 01:01:28,940
attention if you have any questions I'd

01:01:26,040 --> 01:01:28,940

YouTube URL: https://www.youtube.com/watch?v=6Z3_qaFYF84


