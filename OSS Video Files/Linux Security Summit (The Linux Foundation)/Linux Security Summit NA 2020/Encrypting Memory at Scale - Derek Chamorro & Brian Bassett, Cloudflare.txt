Title: Encrypting Memory at Scale - Derek Chamorro & Brian Bassett, Cloudflare
Publication date: 2020-09-08
Playlist: Linux Security Summit NA 2020
Description: 
	Encrypting Memory at Scale - Derek Chamorro & Brian Bassett, Cloudflare
Captions: 
	00:00:13,840 --> 00:00:17,279
howdy

00:00:14,559 --> 00:00:18,080
we are here from cloudflare and we are

00:00:17,279 --> 00:00:22,400
here to discuss

00:00:18,080 --> 00:00:22,400
our story on memory encryption

00:00:24,880 --> 00:00:28,480
i'm derek and i work on the

00:00:27,039 --> 00:00:30,960
infrastructure security team

00:00:28,480 --> 00:00:32,719
here at cloudflare and i'm based out of

00:00:30,960 --> 00:00:35,440
austin texas

00:00:32,719 --> 00:00:37,360
and i'm brian i'm a hardware engineer on

00:00:35,440 --> 00:00:40,719
cloudflare's hardware team

00:00:37,360 --> 00:00:43,600
and i'm also based out of austin

00:00:40,719 --> 00:00:44,000
our primary focus is on designing what

00:00:43,600 --> 00:00:46,000
our

00:00:44,000 --> 00:00:47,840
next generation server platform looks

00:00:46,000 --> 00:00:49,600
like and how we can make it

00:00:47,840 --> 00:00:52,239
highly secure without impacting

00:00:49,600 --> 00:00:52,239
performance

00:00:52,960 --> 00:00:58,160
so before we get started uh there is one

00:00:55,680 --> 00:00:59,840
quick slide on who we are as a company

00:00:58,160 --> 00:01:02,320
and our global presence to help

00:00:59,840 --> 00:01:03,359
visualize what we are doing in order to

00:01:02,320 --> 00:01:05,040
understand

00:01:03,359 --> 00:01:08,479
the level of work it takes when we

00:01:05,040 --> 00:01:08,479
design our server platforms

00:01:09,760 --> 00:01:13,920
so a little bit about who we are our

00:01:12,400 --> 00:01:17,439
network spans across

00:01:13,920 --> 00:01:20,799
200 cities in more than 95 countries

00:01:17,439 --> 00:01:23,520
including 17 cities in mainland china

00:01:20,799 --> 00:01:24,640
we have interconnects with over 8 800

00:01:23,520 --> 00:01:27,759
networks globally

00:01:24,640 --> 00:01:30,320
including major isps cloud services

00:01:27,759 --> 00:01:31,520
and enterprises we have internet

00:01:30,320 --> 00:01:34,320
properties

00:01:31,520 --> 00:01:36,320
that are over 27 million and used by

00:01:34,320 --> 00:01:39,200
approximately 13 percent

00:01:36,320 --> 00:01:41,280
of the fortune 1000 more than one

00:01:39,200 --> 00:01:44,320
billion unique ip addresses pass through

00:01:41,280 --> 00:01:47,920
cloudflare's network every day

00:01:44,320 --> 00:01:50,240
we operate within 100 milliseconds of 99

00:01:47,920 --> 00:01:51,600
of the internet connected population in

00:01:50,240 --> 00:01:54,000
the developed world

00:01:51,600 --> 00:01:55,840
and over 95 of the internet connected

00:01:54,000 --> 00:01:58,399
population globally

00:01:55,840 --> 00:02:01,439
just for context the blink of an eye is

00:01:58,399 --> 00:02:04,560
300 to 400 milliseconds

00:02:01,439 --> 00:02:05,920
we serve 14 million http requests per

00:02:04,560 --> 00:02:09,119
second on average

00:02:05,920 --> 00:02:11,280
with more than 17 million http requests

00:02:09,119 --> 00:02:14,400
per second at peak

00:02:11,280 --> 00:02:14,879
we consistently do approximately 4.6

00:02:14,400 --> 00:02:18,480
million

00:02:14,879 --> 00:02:20,560
dns queries per second that's around 400

00:02:18,480 --> 00:02:23,840
billion queries per day

00:02:20,560 --> 00:02:25,680
and about 11.9 trillion queries per

00:02:23,840 --> 00:02:27,680
month

00:02:25,680 --> 00:02:30,480
we wanted to talk about these numbers

00:02:27,680 --> 00:02:34,840
because everything we do is at scale

00:02:30,480 --> 00:02:37,120
at the same time security is of utmost

00:02:34,840 --> 00:02:37,760
importance so we wanted to talk a little

00:02:37,120 --> 00:02:39,840
about

00:02:37,760 --> 00:02:42,720
encryption and how we handle encryption

00:02:39,840 --> 00:02:42,720
here at cloudflare

00:02:43,840 --> 00:02:47,440
we do encrypt different data states we

00:02:46,959 --> 00:02:51,440
encrypt

00:02:47,440 --> 00:02:53,200
data at rest including our cash on disk

00:02:51,440 --> 00:02:56,080
one of our engineers recently made a

00:02:53,200 --> 00:02:58,959
post about patching the dmcrypt

00:02:56,080 --> 00:03:01,120
linux module because he found that while

00:02:58,959 --> 00:03:02,000
the cost of ssd and flash drives went

00:03:01,120 --> 00:03:03,760
down

00:03:02,000 --> 00:03:05,440
the modules were still built for

00:03:03,760 --> 00:03:07,920
spinning disks

00:03:05,440 --> 00:03:08,879
so a patch was created to remove all the

00:03:07,920 --> 00:03:11,280
extra cueing

00:03:08,879 --> 00:03:13,360
in a synchronous behavior and revert the

00:03:11,280 --> 00:03:16,720
encrypt to its original purpose

00:03:13,360 --> 00:03:18,720
simply encrypt decrypt io crests

00:03:16,720 --> 00:03:20,879
as they pass through since we're using

00:03:18,720 --> 00:03:23,599
faster storage mechanisms than we were

00:03:20,879 --> 00:03:23,599
10 years ago

00:03:24,879 --> 00:03:29,599
we also encrypt data in transit we've

00:03:27,920 --> 00:03:31,840
even collaborated with the internet

00:03:29,599 --> 00:03:34,879
engineering task force on evolving and

00:03:31,840 --> 00:03:36,879
standardizing the latest version of tls

00:03:34,879 --> 00:03:38,640
this helped create this helps to address

00:03:36,879 --> 00:03:41,519
some of the older cryptographic

00:03:38,640 --> 00:03:43,200
problems and design flaws with tls that

00:03:41,519 --> 00:03:44,239
created the conditions for attacks like

00:03:43,200 --> 00:03:47,840
park bleed

00:03:44,239 --> 00:03:47,840
poodle and berserk

00:03:48,239 --> 00:03:53,360
but what about data and use this is data

00:03:51,440 --> 00:03:54,720
that is being processed by one or more

00:03:53,360 --> 00:03:56,720
applications

00:03:54,720 --> 00:03:57,760
and data that's currently in the process

00:03:56,720 --> 00:04:00,640
of being created

00:03:57,760 --> 00:04:01,280
updated appended or deleted it also

00:04:00,640 --> 00:04:03,280
includes

00:04:01,280 --> 00:04:05,680
data that is being viewed by users

00:04:03,280 --> 00:04:07,519
accessing it through various endpoints

00:04:05,680 --> 00:04:08,720
and is the data that is susceptible to

00:04:07,519 --> 00:04:10,879
different kinds of threats

00:04:08,720 --> 00:04:12,959
depending on where it is in the system

00:04:10,879 --> 00:04:14,959
and who is able to use it

00:04:12,959 --> 00:04:16,560
while we use different methods to

00:04:14,959 --> 00:04:18,239
protect data in use

00:04:16,560 --> 00:04:21,840
we're always challenging ourselves with

00:04:18,239 --> 00:04:21,840
better protection modes

00:04:23,360 --> 00:04:26,639
so one of our concerns is that someone

00:04:25,680 --> 00:04:28,479
could come in

00:04:26,639 --> 00:04:30,720
and steal one of our servers out of a

00:04:28,479 --> 00:04:33,360
data center in our colo

00:04:30,720 --> 00:04:36,400
but it doesn't necessarily have to be a

00:04:33,360 --> 00:04:38,400
mission impossible style snatch and grab

00:04:36,400 --> 00:04:41,360
how many of you have received reports of

00:04:38,400 --> 00:04:44,479
a rack or racks that were left unlocked

00:04:41,360 --> 00:04:45,600
or worse missing a door i joke around

00:04:44,479 --> 00:04:47,840
about these things

00:04:45,600 --> 00:04:48,880
but they do happen rex can be left

00:04:47,840 --> 00:04:51,680
unprotected

00:04:48,880 --> 00:04:52,320
and sometimes controls can be bypassed

00:04:51,680 --> 00:04:54,320
so

00:04:52,320 --> 00:04:56,479
if someone were to steal one of our

00:04:54,320 --> 00:05:00,160
servers the question becomes

00:04:56,479 --> 00:05:00,160
what exactly could they pull off of it

00:05:01,199 --> 00:05:04,960
so when we just started discussing the

00:05:02,720 --> 00:05:07,039
concept of protecting data and use or

00:05:04,960 --> 00:05:08,320
further protecting data in use

00:05:07,039 --> 00:05:10,400
we wanted to address how we could

00:05:08,320 --> 00:05:12,720
protect memory at our current and future

00:05:10,400 --> 00:05:12,720
scale

00:05:13,919 --> 00:05:17,199
and the reason this is important is that

00:05:16,320 --> 00:05:20,000
data is

00:05:17,199 --> 00:05:21,600
stored in the clear this can leave data

00:05:20,000 --> 00:05:23,840
vulnerable to snooping by

00:05:21,600 --> 00:05:26,160
unauthorized administrators or different

00:05:23,840 --> 00:05:28,639
methods of probing

00:05:26,160 --> 00:05:30,400
dim memory modules when powered down

00:05:28,639 --> 00:05:31,600
gradually lose data over time as they

00:05:30,400 --> 00:05:33,840
lose power

00:05:31,600 --> 00:05:35,600
but do not immediately lose all data

00:05:33,840 --> 00:05:37,919
when power is lost

00:05:35,600 --> 00:05:40,080
we've seen from a flux of research paper

00:05:37,919 --> 00:05:40,560
is that memory modules can potentially

00:05:40,080 --> 00:05:42,960
retain

00:05:40,560 --> 00:05:45,199
at least some data for up to 90 minutes

00:05:42,960 --> 00:05:47,280
after power loss

00:05:45,199 --> 00:05:49,759
well a reboot will generally take care

00:05:47,280 --> 00:05:51,360
of flushing memory caches right

00:05:49,759 --> 00:05:52,880
that's what you know cold boot attack

00:05:51,360 --> 00:05:55,039
works to feed

00:05:52,880 --> 00:05:56,319
dump the contents of pre-boot retained

00:05:55,039 --> 00:05:58,639
physical memory

00:05:56,319 --> 00:06:01,919
firmware modifications which an attacker

00:05:58,639 --> 00:06:04,880
can then use to inspect that data

00:06:01,919 --> 00:06:06,400
newer non-volatile memory technologies

00:06:04,880 --> 00:06:09,280
exacerbate this problem

00:06:06,400 --> 00:06:10,800
but problem since an nv dim chip can be

00:06:09,280 --> 00:06:12,000
physically removed from a system with

00:06:10,800 --> 00:06:14,240
the data intact

00:06:12,000 --> 00:06:15,280
as it uses nand flash to store a copy of

00:06:14,240 --> 00:06:17,680
its contents

00:06:15,280 --> 00:06:18,319
similar to our hard drive without

00:06:17,680 --> 00:06:20,080
encryption

00:06:18,319 --> 00:06:21,440
any stored information such as sensitive

00:06:20,080 --> 00:06:26,000
data passwords

00:06:21,440 --> 00:06:29,120
or secret keys can be easily compromised

00:06:26,000 --> 00:06:30,560
so do these attacks really happen cold

00:06:29,120 --> 00:06:32,639
boot attacks as mentioned

00:06:30,560 --> 00:06:34,639
previously first talked about more than

00:06:32,639 --> 00:06:36,800
a decade ago i've started making a

00:06:34,639 --> 00:06:38,720
comeback with recent research papers

00:06:36,800 --> 00:06:41,039
introducing new methods to defeat ddr

00:06:38,720 --> 00:06:43,199
memory scrambling technologies

00:06:41,039 --> 00:06:45,039
that were used to obfuscate data written

00:06:43,199 --> 00:06:47,280
across the memory bus

00:06:45,039 --> 00:06:48,160
by monitoring memory bus transactions

00:06:47,280 --> 00:06:49,680
attackers are

00:06:48,160 --> 00:06:51,280
listening and looking for objects that

00:06:49,680 --> 00:06:54,400
can be secret in nature

00:06:51,280 --> 00:06:56,560
think passwords tls keys etc

00:06:54,400 --> 00:06:58,560
since the data was merely obfuscated via

00:06:56,560 --> 00:07:00,840
xor and not encrypted

00:06:58,560 --> 00:07:02,080
these attacks themselves were not very

00:07:00,840 --> 00:07:03,520
sophisticated

00:07:02,080 --> 00:07:06,160
leaving dram exposed to member

00:07:03,520 --> 00:07:08,800
extraction techniques

00:07:06,160 --> 00:07:10,240
and then we had rambly which allowed an

00:07:08,800 --> 00:07:11,759
unprivileged attacker to read out

00:07:10,240 --> 00:07:12,800
certain memory belonging to other

00:07:11,759 --> 00:07:14,639
processes

00:07:12,800 --> 00:07:16,160
by leveraging the row hammer and bit

00:07:14,639 --> 00:07:17,919
flipping effect

00:07:16,160 --> 00:07:19,599
common hardware mitigations such as

00:07:17,919 --> 00:07:21,599
targeted raw refresh

00:07:19,599 --> 00:07:23,360
introduce other potential attack vectors

00:07:21,599 --> 00:07:25,599
like trespass

00:07:23,360 --> 00:07:26,800
increasing the dram refresh rate leads

00:07:25,599 --> 00:07:28,840
to fewer bit flips

00:07:26,800 --> 00:07:30,960
but there is a power and performance

00:07:28,840 --> 00:07:33,520
trade-off and while ecc

00:07:30,960 --> 00:07:36,400
memory does complicate the attack it

00:07:33,520 --> 00:07:36,400
does not prevent it

00:07:37,440 --> 00:07:41,440
so we started looking into ways of

00:07:39,599 --> 00:07:43,599
better protecting memory and we started

00:07:41,440 --> 00:07:45,440
looking at enclaves

00:07:43,599 --> 00:07:47,120
its memory encryption and isolation can

00:07:45,440 --> 00:07:49,199
be achieved with enclaves

00:07:47,120 --> 00:07:52,240
it can be done in software only but

00:07:49,199 --> 00:07:54,080
hardware manufacturers made

00:07:52,240 --> 00:07:55,840
hardware assisted trusted execution

00:07:54,080 --> 00:07:56,879
environments to help create security

00:07:55,840 --> 00:07:59,360
boundaries

00:07:56,879 --> 00:08:00,240
by isolating software execution at

00:07:59,360 --> 00:08:02,240
runtime

00:08:00,240 --> 00:08:04,319
so that sensitive data can be processed

00:08:02,240 --> 00:08:06,639
in a trusted environment such as a

00:08:04,319 --> 00:08:12,479
secure area inside an existing processor

00:08:06,639 --> 00:08:15,759
or trusted platform module

00:08:12,479 --> 00:08:18,080
but enclaves were really meant to only

00:08:15,759 --> 00:08:19,759
process and run small pieces of code not

00:08:18,080 --> 00:08:21,120
an entire os

00:08:19,759 --> 00:08:23,360
while there have been research papers

00:08:21,120 --> 00:08:24,840
that have shown how you can do it

00:08:23,360 --> 00:08:26,720
they have come with performance

00:08:24,840 --> 00:08:31,199
trade-offs on

00:08:26,720 --> 00:08:32,240
page class is also limited to 128 to 256

00:08:31,199 --> 00:08:35,519
megabytes of

00:08:32,240 --> 00:08:37,839
uh of cash um and it still is a

00:08:35,519 --> 00:08:39,599
performance trade-off by enabling that

00:08:37,839 --> 00:08:41,200
and at the same time application

00:08:39,599 --> 00:08:43,200
refactoring

00:08:41,200 --> 00:08:46,800
is required in order not just to enable

00:08:43,200 --> 00:08:49,680
but also to use the enclave itself

00:08:46,800 --> 00:08:51,519
and there have been a string of recent

00:08:49,680 --> 00:08:53,760
vulnerabilities that have come out

00:08:51,519 --> 00:08:56,240
things like load value injection which

00:08:53,760 --> 00:08:58,480
are transient execution attacks

00:08:56,240 --> 00:09:00,480
that inject attacker data into a victim

00:08:58,480 --> 00:09:02,560
program and steal sensitive data and

00:09:00,480 --> 00:09:04,720
keys from an enclave

00:09:02,560 --> 00:09:06,480
recently cash out which is a newer

00:09:04,720 --> 00:09:08,560
speculative execution attack that is

00:09:06,480 --> 00:09:10,959
capable of leaking data from

00:09:08,560 --> 00:09:13,440
caching mechanisms including enclaves

00:09:10,959 --> 00:09:15,839
and sgax which is a further evolution

00:09:13,440 --> 00:09:18,720
of cash out in the form of uh enclave

00:09:15,839 --> 00:09:18,720
side channel attack

00:09:20,880 --> 00:09:26,080
so we made a series of blog posts uh

00:09:24,240 --> 00:09:27,519
earlier in march regarding our next

00:09:26,080 --> 00:09:30,240
generation server hardware

00:09:27,519 --> 00:09:31,040
that we labeled gen x for the 10th

00:09:30,240 --> 00:09:32,640
generation

00:09:31,040 --> 00:09:34,959
and it's based off of the amd rom

00:09:32,640 --> 00:09:36,080
architecture we spoke about thermal

00:09:34,959 --> 00:09:38,800
design power

00:09:36,080 --> 00:09:40,080
improvements in l3 cache in overall

00:09:38,800 --> 00:09:41,360
performance tuning

00:09:40,080 --> 00:09:43,360
but we were surprised at some of the

00:09:41,360 --> 00:09:44,480
included security features which weren't

00:09:43,360 --> 00:09:47,279
readily available from other

00:09:44,480 --> 00:09:47,279
manufacturers

00:09:48,160 --> 00:09:51,200
in this case it was secure memory

00:09:49,680 --> 00:09:53,440
encryption um

00:09:51,200 --> 00:09:54,800
secure memory encryption is an x86

00:09:53,440 --> 00:09:57,920
instruction set

00:09:54,800 --> 00:09:59,680
extension introduced by amd in 2016. so

00:09:57,920 --> 00:10:01,680
it's been around for a few years

00:09:59,680 --> 00:10:03,680
for page granular memory encryption

00:10:01,680 --> 00:10:04,480
support using a single ephemeral key at

00:10:03,680 --> 00:10:06,160
boot

00:10:04,480 --> 00:10:08,000
with the new key generated by the

00:10:06,160 --> 00:10:09,680
processor on every boot

00:10:08,000 --> 00:10:11,760
a page that is marked encrypted will

00:10:09,680 --> 00:10:14,320
automatically will be automatically

00:10:11,760 --> 00:10:17,200
decrypted when read from mem from dram

00:10:14,320 --> 00:10:18,320
and encrypted when written to dram and

00:10:17,200 --> 00:10:20,240
while there have been a handful of

00:10:18,320 --> 00:10:22,000
presentations and papers on secure

00:10:20,240 --> 00:10:24,320
encrypted virtualization

00:10:22,000 --> 00:10:25,200
also known as sev it wasn't a feature we

00:10:24,320 --> 00:10:27,120
would use as we

00:10:25,200 --> 00:10:29,839
typically do not isolate with with

00:10:27,120 --> 00:10:29,839
hypervisors

00:10:30,880 --> 00:10:34,560
the smb components are fairly

00:10:32,800 --> 00:10:37,600
straightforward there is an

00:10:34,560 --> 00:10:39,440
aes 128-bit encryption engine that is

00:10:37,600 --> 00:10:41,279
embedded in the memory controllers and

00:10:39,440 --> 00:10:42,079
is able to transparently encrypt and

00:10:41,279 --> 00:10:44,399
decrypt

00:10:42,079 --> 00:10:46,079
data in main memory when an encryption

00:10:44,399 --> 00:10:47,760
key has been provided via the secure

00:10:46,079 --> 00:10:50,000
processor

00:10:47,760 --> 00:10:51,360
then you have the amd secure processor

00:10:50,000 --> 00:10:54,560
which is an on-dye

00:10:51,360 --> 00:10:55,760
32-bit arm cortex a5 cpu

00:10:54,560 --> 00:10:58,240
that provides cryptographic

00:10:55,760 --> 00:11:00,240
functionality for secure key generation

00:10:58,240 --> 00:11:02,079
and key management you could think of

00:11:00,240 --> 00:11:02,640
this like a mini hardware security

00:11:02,079 --> 00:11:04,320
module

00:11:02,640 --> 00:11:07,440
that uses a hardware random number

00:11:04,320 --> 00:11:10,480
generator to generate the 128-bit aes

00:11:07,440 --> 00:11:12,880
keys used by the encryption engine the

00:11:10,480 --> 00:11:15,040
as algorithm uses a physical address as

00:11:12,880 --> 00:11:17,440
a type of knots

00:11:15,040 --> 00:11:20,160
it is hardware isolated so keys are

00:11:17,440 --> 00:11:22,480
never sent in the clear outside of the

00:11:20,160 --> 00:11:25,600
system on a chip and it runs its own

00:11:22,480 --> 00:11:25,600
secure os and curl

00:11:26,880 --> 00:11:32,399
so how it works it works by requiring

00:11:29,920 --> 00:11:34,240
and then by enabling a model specific

00:11:32,399 --> 00:11:36,160
register which is a control register

00:11:34,240 --> 00:11:39,200
responsible for executing

00:11:36,160 --> 00:11:41,120
the x86 instruction sets this enables

00:11:39,200 --> 00:11:43,600
the ability to set a page table

00:11:41,120 --> 00:11:45,200
entry encryption but here we have the

00:11:43,600 --> 00:11:50,079
documentation officially from

00:11:45,200 --> 00:11:52,079
the amd developers manual

00:11:50,079 --> 00:11:53,600
support for sme can be determined

00:11:52,079 --> 00:11:57,360
through the following cpu

00:11:53,600 --> 00:12:00,240
id function bit 0 indicates support for

00:11:57,360 --> 00:12:02,959
sme and again the relevant amp

00:12:00,240 --> 00:12:02,959
documentation

00:12:04,000 --> 00:12:07,360
here you can see it on a test box the

00:12:05,519 --> 00:12:09,440
validation output you can

00:12:07,360 --> 00:12:11,279
validate that it's turned on by viewing

00:12:09,440 --> 00:12:13,120
the message buffer output by gripping

00:12:11,279 --> 00:12:15,680
for sme

00:12:13,120 --> 00:12:17,279
you can view the eax register contents

00:12:15,680 --> 00:12:20,639
by using the cpu

00:12:17,279 --> 00:12:23,040
cpu id utility to show support for

00:12:20,639 --> 00:12:24,880
the instruction in the processor and

00:12:23,040 --> 00:12:28,240
validating that bit 23

00:12:24,880 --> 00:12:28,240
in the msr is present

00:12:29,600 --> 00:12:34,000
so how it works for an actual write

00:12:32,079 --> 00:12:36,480
after memory encryption is enabled

00:12:34,000 --> 00:12:38,320
a physical address bit also known as the

00:12:36,480 --> 00:12:40,480
c bit for encrypted bit

00:12:38,320 --> 00:12:41,600
is utilized to mark if a memory page is

00:12:40,480 --> 00:12:43,600
protected

00:12:41,600 --> 00:12:44,959
the operating system sets the bit of a

00:12:43,600 --> 00:12:47,600
physical address to

00:12:44,959 --> 00:12:49,360
one in the page table entry to indicate

00:12:47,600 --> 00:12:50,959
the page should be encrypted

00:12:49,360 --> 00:12:52,399
this causes any data assigned to that

00:12:50,959 --> 00:12:54,240
memory space to automatically be

00:12:52,399 --> 00:12:56,639
encrypted when written in memory

00:12:54,240 --> 00:12:58,000
so a page will be allocated that page

00:12:56,639 --> 00:13:00,240
has zeroized

00:12:58,000 --> 00:13:01,839
the encryption bit in the pte if it's

00:13:00,240 --> 00:13:04,720
set clear it or if it's been

00:13:01,839 --> 00:13:06,880
cleared set it then with series of

00:13:04,720 --> 00:13:08,320
instructions is flushing the translation

00:13:06,880 --> 00:13:11,279
look aside buffer

00:13:08,320 --> 00:13:14,560
flush memory caches update the pta and

00:13:11,279 --> 00:13:14,560
then flush the tlb again

00:13:15,760 --> 00:13:20,079
and when data is red the secure

00:13:18,160 --> 00:13:21,760
processor provides the key to the as

00:13:20,079 --> 00:13:23,680
engine to decrypt the data

00:13:21,760 --> 00:13:25,279
the operating system sets the bit of the

00:13:23,680 --> 00:13:26,320
physical address to zero in the page

00:13:25,279 --> 00:13:28,399
table and

00:13:26,320 --> 00:13:30,320
page table entry to indicate the page

00:13:28,399 --> 00:13:33,120
should be decrypted

00:13:30,320 --> 00:13:34,000
and this is how standard sme works and

00:13:33,120 --> 00:13:36,560
while it would be great

00:13:34,000 --> 00:13:37,440
to mark the pages we want encrypted ad

00:13:36,560 --> 00:13:39,360
hoc

00:13:37,440 --> 00:13:42,480
we wanted to ensure that all memory was

00:13:39,360 --> 00:13:42,480
encrypted by default

00:13:43,199 --> 00:13:46,560
and so that's when we looked into

00:13:44,320 --> 00:13:48,720
transparent me sme

00:13:46,560 --> 00:13:49,920
and as the name suggests all memory is

00:13:48,720 --> 00:13:51,440
encrypted and it's performed

00:13:49,920 --> 00:13:54,399
transparently the background

00:13:51,440 --> 00:13:56,160
invisible to the os all traffic going to

00:13:54,399 --> 00:13:57,839
the memory controller is encrypted

00:13:56,160 --> 00:14:00,079
regardless of the value of the encrypt

00:13:57,839 --> 00:14:02,800
bit on any particular page

00:14:00,079 --> 00:14:03,680
this includes instruction pages data

00:14:02,800 --> 00:14:05,600
pages

00:14:03,680 --> 00:14:06,880
pages corresponding to the page table

00:14:05,600 --> 00:14:09,440
itself

00:14:06,880 --> 00:14:10,560
and no applications were required so no

00:14:09,440 --> 00:14:13,440
need to refactor

00:14:10,560 --> 00:14:14,560
any applications uh to ensure that the

00:14:13,440 --> 00:14:17,839
applications themselves

00:14:14,560 --> 00:14:17,839
are using encrypted memory

00:14:18,320 --> 00:14:21,519
it's a bios uefi option that when

00:14:20,959 --> 00:14:24,880
enabled

00:14:21,519 --> 00:14:26,639
sets the msr bit to active then your os

00:14:24,880 --> 00:14:27,600
can activate memory encryption by

00:14:26,639 --> 00:14:29,920
default

00:14:27,600 --> 00:14:31,120
by setting the following kernel flag and

00:14:29,920 --> 00:14:33,440
by supplying

00:14:31,120 --> 00:14:36,399
uh mem encrypt uh equals on on the

00:14:33,440 --> 00:14:36,399
kernel command line

00:14:38,160 --> 00:14:44,800
so now that we know that it's active we

00:14:41,279 --> 00:14:47,279
wanted to test and see if it worked so

00:14:44,800 --> 00:14:50,079
we built built and loaded a kernel

00:14:47,279 --> 00:14:53,920
module specifically for memory testing

00:14:50,079 --> 00:14:53,920
that allocates a page of memory

00:14:54,160 --> 00:14:59,120
zeroes out the allocated memory and

00:14:58,160 --> 00:15:01,440
issues a

00:14:59,120 --> 00:15:03,680
set memory decrypted function call

00:15:01,440 --> 00:15:06,800
against allocated memory

00:15:03,680 --> 00:15:07,440
this specific function call is called to

00:15:06,800 --> 00:15:09,279
remove

00:15:07,440 --> 00:15:11,279
the encryption bit associated with the

00:15:09,279 --> 00:15:12,639
buffer under test

00:15:11,279 --> 00:15:14,399
this doesn't actually decrypt the

00:15:12,639 --> 00:15:15,680
contents of the memory buffer but we'll

00:15:14,399 --> 00:15:17,760
just mark it as not

00:15:15,680 --> 00:15:20,720
encrypted this can then be used to

00:15:17,760 --> 00:15:24,839
compare against the reference buffer

00:15:20,720 --> 00:15:26,079
and determine the state of secure memory

00:15:24,839 --> 00:15:28,160
encryption

00:15:26,079 --> 00:15:29,680
then we check if the allocated memory is

00:15:28,160 --> 00:15:32,880
still zero

00:15:29,680 --> 00:15:36,320
if sme is enabled memory will still be

00:15:32,880 --> 00:15:39,040
all zeros if sme is disabled memory will

00:15:36,320 --> 00:15:39,040
not be zeros

00:15:39,759 --> 00:15:43,360
so here we load the specific kernel

00:15:42,720 --> 00:15:45,600
module

00:15:43,360 --> 00:15:46,959
uh and then we get a error and we do

00:15:45,600 --> 00:15:48,880
that intentionally

00:15:46,959 --> 00:15:50,639
we want the load specifically to

00:15:48,880 --> 00:15:52,480
intentionally fail so that the module

00:15:50,639 --> 00:15:53,680
doesn't have to be unloaded before we're

00:15:52,480 --> 00:15:57,440
running the test

00:15:53,680 --> 00:15:57,440
while still capturing the debug output

00:15:58,560 --> 00:16:03,440
so with the module failure we can still

00:16:01,040 --> 00:16:06,000
see the contents of the memory buffer

00:16:03,440 --> 00:16:07,360
we can view the module output to console

00:16:06,000 --> 00:16:09,920
where we can see the print

00:16:07,360 --> 00:16:10,959
the printout of the actual hex dump the

00:16:09,920 --> 00:16:12,800
printout shows

00:16:10,959 --> 00:16:15,199
the beginning of the buffer before the

00:16:12,800 --> 00:16:17,759
call to set memory decrypted

00:16:15,199 --> 00:16:18,959
and that checks buffer buffer reference

00:16:17,759 --> 00:16:22,320
and page size

00:16:18,959 --> 00:16:25,680
is still set to zero and after

00:16:22,320 --> 00:16:25,680
where the buffers do not match

00:16:28,000 --> 00:16:32,639
so now we know it works how old does it

00:16:30,639 --> 00:16:34,320
perform to our performance testing

00:16:32,639 --> 00:16:35,920
as well as rolling it out to production

00:16:34,320 --> 00:16:37,839
and i'll hand it off to you brian to go

00:16:35,920 --> 00:16:41,920
over the results

00:16:37,839 --> 00:16:43,920
all right thanks um so

00:16:41,920 --> 00:16:45,360
uh yeah as derek said now that we knew

00:16:43,920 --> 00:16:48,639
that the feature worked

00:16:45,360 --> 00:16:50,320
our next step was to test um

00:16:48,639 --> 00:16:52,880
how if at any it would affect

00:16:50,320 --> 00:16:55,920
performance

00:16:52,880 --> 00:16:57,519
so we ran a series of benchmarks in the

00:16:55,920 --> 00:17:00,639
lab

00:16:57,519 --> 00:17:03,199
and then based on the results of those

00:17:00,639 --> 00:17:04,959
we took into production

00:17:03,199 --> 00:17:07,839
the gen x servers that we're running

00:17:04,959 --> 00:17:11,120
this on have eight 32 gig dimms

00:17:07,839 --> 00:17:15,360
running at 29 33 megahertz

00:17:11,120 --> 00:17:18,480
and we're using the epic 7642

00:17:15,360 --> 00:17:19,839
processor which has 48 cores and 96

00:17:18,480 --> 00:17:22,000
threads

00:17:19,839 --> 00:17:23,919
and we're running in notes for socket

00:17:22,000 --> 00:17:26,400
equals 4 mode

00:17:23,919 --> 00:17:27,760
we're a debian shop we run debian 9 on

00:17:26,400 --> 00:17:33,840
these servers

00:17:27,760 --> 00:17:33,840
and our kernel version is 5.4.12.

00:17:35,760 --> 00:17:41,840
so the first test we ran was the stream

00:17:38,240 --> 00:17:44,080
industry standard memory bandwidth test

00:17:41,840 --> 00:17:45,360
we used this the standard stream.c

00:17:44,080 --> 00:17:46,720
available from the university of

00:17:45,360 --> 00:17:48,960
virginia

00:17:46,720 --> 00:17:50,160
but what one change we'd make was to

00:17:48,960 --> 00:17:53,039
increase the

00:17:50,160 --> 00:17:54,799
data set size to be around five gig

00:17:53,039 --> 00:17:57,760
gigabytes

00:17:54,799 --> 00:18:00,000
and the reason why is that these these

00:17:57,760 --> 00:18:02,320
processors have a large 256

00:18:00,000 --> 00:18:03,120
megabyte level three cache and we didn't

00:18:02,320 --> 00:18:06,400
want that

00:18:03,120 --> 00:18:08,480
that big cache skewing the results

00:18:06,400 --> 00:18:10,400
so the graph that you um you're looking

00:18:08,480 --> 00:18:12,880
at here you can see that depending on

00:18:10,400 --> 00:18:16,240
which sub benchmark you look at

00:18:12,880 --> 00:18:21,360
we saw anywhere from a 2.6 to a 4.2

00:18:16,240 --> 00:18:21,360
performance loss from implementing sme

00:18:24,960 --> 00:18:31,280
um the next test we ran was the

00:18:28,240 --> 00:18:33,360
setup command um this

00:18:31,280 --> 00:18:34,640
tool is normally used to encrypt disks

00:18:33,360 --> 00:18:36,720
but in this case

00:18:34,640 --> 00:18:38,559
it has a built-in cryptography benchmark

00:18:36,720 --> 00:18:40,880
that we can use to

00:18:38,559 --> 00:18:42,480
as a quick test of cpu and memory

00:18:40,880 --> 00:18:45,440
performance

00:18:42,480 --> 00:18:47,039
and on this test we saw less than one

00:18:45,440 --> 00:18:50,240
percent performance loss from

00:18:47,039 --> 00:18:52,320
from activating sme so um

00:18:50,240 --> 00:18:54,320
this this uh benchmark is not

00:18:52,320 --> 00:18:57,840
particularly memory bandwidth

00:18:54,320 --> 00:18:57,840
um constrained

00:19:00,160 --> 00:19:04,080
and then finally we ran a custom web

00:19:02,799 --> 00:19:07,679
traffic benchmark that

00:19:04,080 --> 00:19:10,559
was developed by our performance team um

00:19:07,679 --> 00:19:12,559
this uses cloud for workers to generate

00:19:10,559 --> 00:19:13,840
web traffic from one host to another in

00:19:12,559 --> 00:19:16,880
the lab

00:19:13,840 --> 00:19:19,280
and again here we saw um

00:19:16,880 --> 00:19:20,480
roughly one percent performance hit when

00:19:19,280 --> 00:19:24,240
um

00:19:20,480 --> 00:19:27,280
transferring this small 10 kilobit by

00:19:24,240 --> 00:19:30,720
image from one host to another uh it

00:19:27,280 --> 00:19:33,840
uses 256 concurrent clients to do that

00:19:30,720 --> 00:19:35,440
so uh encouraged by these results we

00:19:33,840 --> 00:19:38,640
went ahead and

00:19:35,440 --> 00:19:41,760
activated sme on host in production

00:19:38,640 --> 00:19:42,960
and then compared it to the performance

00:19:41,760 --> 00:19:44,960
of a host that's

00:19:42,960 --> 00:19:46,960
sitting right next to it in the rack so

00:19:44,960 --> 00:19:49,440
they're both in the same polo

00:19:46,960 --> 00:19:52,320
uh just one with sme off and one with

00:19:49,440 --> 00:19:55,919
sme on

00:19:52,320 --> 00:20:00,160
this graph is a snapshot of a recent

00:19:55,919 --> 00:20:03,600
interval of web traffic to the server

00:20:00,160 --> 00:20:04,960
this is nginx request service and

00:20:03,600 --> 00:20:08,400
you can see the performance of the two

00:20:04,960 --> 00:20:10,400
servers track each other pretty closely

00:20:08,400 --> 00:20:11,520
we are averaging over the time period

00:20:10,400 --> 00:20:13,520
that you see here

00:20:11,520 --> 00:20:15,120
around five percent fewer requests per

00:20:13,520 --> 00:20:21,520
second service by the

00:20:15,120 --> 00:20:26,159
host that has sme enabled

00:20:21,520 --> 00:20:28,320
thanks for that brian um so what's next

00:20:26,159 --> 00:20:30,480
so some of our future work uh includes

00:20:28,320 --> 00:20:33,520
doing this at a fleetwide rollout

00:20:30,480 --> 00:20:36,880
uh we currently have this uh enabled on

00:20:33,520 --> 00:20:38,400
in a colo and we are pleased with like a

00:20:36,880 --> 00:20:39,919
lot of the results so

00:20:38,400 --> 00:20:41,840
uh planning on rolling this out feet

00:20:39,919 --> 00:20:43,679
wide

00:20:41,840 --> 00:20:46,080
also uh putting the ball in intel's

00:20:43,679 --> 00:20:48,400
court for total memory encryption

00:20:46,080 --> 00:20:49,360
the spec has been released or was

00:20:48,400 --> 00:20:52,559
released

00:20:49,360 --> 00:20:55,360
back in 2017 um and uh

00:20:52,559 --> 00:20:56,159
we've recently uh seen uh intel make

00:20:55,360 --> 00:20:57,520
some progress

00:20:56,159 --> 00:20:59,360
and deploying this in some of their

00:20:57,520 --> 00:21:02,559
future processors

00:20:59,360 --> 00:21:05,200
so we'll be excited to test this as well

00:21:02,559 --> 00:21:05,760
at the same time more research uh you

00:21:05,200 --> 00:21:07,360
know we

00:21:05,760 --> 00:21:09,280
love testing cpu so we're looking to see

00:21:07,360 --> 00:21:10,320
if arm has a risk equivalent as we

00:21:09,280 --> 00:21:11,600
believe

00:21:10,320 --> 00:21:14,080
full memory encryption to be a

00:21:11,600 --> 00:21:16,640
technology that will be widely adopted

00:21:14,080 --> 00:21:17,600
um also looking into some newer amd

00:21:16,640 --> 00:21:19,840
features

00:21:17,600 --> 00:21:21,520
for memory encryption uh when it comes

00:21:19,840 --> 00:21:23,440
to secure nested paging

00:21:21,520 --> 00:21:25,919
and seeing if it can protect container

00:21:23,440 --> 00:21:25,919
runtimes

00:21:27,600 --> 00:21:33,280
so to summarize first

00:21:31,120 --> 00:21:35,200
memory text will happen they will

00:21:33,280 --> 00:21:36,640
continue to get more sophisticated even

00:21:35,200 --> 00:21:39,360
as we continue to create counter

00:21:36,640 --> 00:21:39,360
measures for them

00:21:40,080 --> 00:21:44,080
full memory encryption is available this

00:21:42,400 --> 00:21:44,720
is an added security feature that

00:21:44,080 --> 00:21:46,640
doesn't

00:21:44,720 --> 00:21:48,320
require code refactoring and it's

00:21:46,640 --> 00:21:51,200
something that was surprisingly easy to

00:21:48,320 --> 00:21:51,200
turn on and test

00:21:51,520 --> 00:21:55,679
and the overhead isn't as bad as we

00:21:53,280 --> 00:21:56,400
thought in the majority of test results

00:21:55,679 --> 00:21:58,559
performance

00:21:56,400 --> 00:22:00,080
decreased by a nominal amount actually

00:21:58,559 --> 00:22:02,880
less than we expected

00:22:00,080 --> 00:22:04,559
amd's official white paper on sme even

00:22:02,880 --> 00:22:05,520
states that encryption and decryption of

00:22:04,559 --> 00:22:07,360
memory

00:22:05,520 --> 00:22:09,120
through the aes engine does incur a

00:22:07,360 --> 00:22:10,799
small amount of additional latency for

00:22:09,120 --> 00:22:13,039
dram memory access

00:22:10,799 --> 00:22:14,960
although it is dependent on the workload

00:22:13,039 --> 00:22:16,720
across all 11 data points

00:22:14,960 --> 00:22:19,280
our average performance drag was only

00:22:16,720 --> 00:22:21,200
down by 0.699 percent

00:22:19,280 --> 00:22:23,120
even at scale enabling this feature

00:22:21,200 --> 00:22:27,840
reduces the worry that any data could be

00:22:23,120 --> 00:22:27,840
excreted from a stolen server

00:22:27,919 --> 00:22:31,679
so for up-to-date info on what we're

00:22:29,760 --> 00:22:32,400
working on please feel free to follow

00:22:31,679 --> 00:22:34,240
our blog

00:22:32,400 --> 00:22:36,400
where we are consistently publishing

00:22:34,240 --> 00:22:38,240
content on new technologies

00:22:36,400 --> 00:22:40,880
and topics that are relevant in this day

00:22:38,240 --> 00:22:45,760
and age

00:22:40,880 --> 00:22:45,760
and with that we thank you thanks for

00:22:52,840 --> 00:22:55,840
watching

00:22:59,919 --> 00:23:03,679
hi um derek and i are here to answer

00:23:02,159 --> 00:23:04,480
questions we had some good ones coming

00:23:03,679 --> 00:23:07,520
in through the q

00:23:04,480 --> 00:23:08,720
a uh angie asks what's the overhead of

00:23:07,520 --> 00:23:10,559
encrypting memory

00:23:08,720 --> 00:23:12,559
uh hopefully we address that to your

00:23:10,559 --> 00:23:14,559
satisfaction angie

00:23:12,559 --> 00:23:16,240
we ran a bunch of synthetic tests and

00:23:14,559 --> 00:23:18,320
saw anywhere from zero percent

00:23:16,240 --> 00:23:20,559
up to about four point six percent uh

00:23:18,320 --> 00:23:22,159
hit on the test that we were running

00:23:20,559 --> 00:23:24,400
and then on our live production

00:23:22,159 --> 00:23:27,039
environment um it's anywhere like

00:23:24,400 --> 00:23:28,880
four to five percent overhead um as in

00:23:27,039 --> 00:23:30,000
fewer requests per second service by

00:23:28,880 --> 00:23:36,080
nginx

00:23:30,000 --> 00:23:37,600
uh with sme feature turned on

00:23:36,080 --> 00:23:38,880
fernando asked does the chip

00:23:37,600 --> 00:23:39,679
manufacturer have similar instructions

00:23:38,880 --> 00:23:41,200
like amd

00:23:39,679 --> 00:23:42,960
intel arm do you mind just on smaller

00:23:41,200 --> 00:23:46,480
devices and not desktops

00:23:42,960 --> 00:23:48,159
um so yeah we addressed it intel uh

00:23:46,480 --> 00:23:49,600
has a spec for total memory encryption

00:23:48,159 --> 00:23:51,840
as well as multi-key total memory

00:23:49,600 --> 00:23:54,159
encryption which is similar to the mpsf

00:23:51,840 --> 00:23:55,840
um and they do have it on some smaller

00:23:54,159 --> 00:23:59,360
chipsets too

00:23:55,840 --> 00:24:00,960
actually amd does um as far as arm

00:23:59,360 --> 00:24:02,559
we're still trying to investigate um

00:24:00,960 --> 00:24:05,520
they said they don't share

00:24:02,559 --> 00:24:07,760
the same x86 instruction sets that that

00:24:05,520 --> 00:24:09,120
intel and uh ambi do so

00:24:07,760 --> 00:24:12,880
um but we're in discussion to find out

00:24:09,120 --> 00:24:14,559
if they have like a wrong equipment

00:24:12,880 --> 00:24:16,080
yeah and there were some press releases

00:24:14,559 --> 00:24:18,559
from intel based on their

00:24:16,080 --> 00:24:19,840
um summit that they had a security

00:24:18,559 --> 00:24:22,320
summit earlier this year

00:24:19,840 --> 00:24:23,840
where they said that they are kind of

00:24:22,320 --> 00:24:25,760
getting the infrastructure in place for

00:24:23,840 --> 00:24:28,320
their implementation of this

00:24:25,760 --> 00:24:29,600
and um apaches to the linux kernel and

00:24:28,320 --> 00:24:31,360
stuff like that

00:24:29,600 --> 00:24:33,440
and that the the feature would be

00:24:31,360 --> 00:24:34,240
supported in hardware on just future

00:24:33,440 --> 00:24:36,640
chips

00:24:34,240 --> 00:24:38,000
so they haven't really made a specific

00:24:36,640 --> 00:24:40,799
announcement about

00:24:38,000 --> 00:24:40,799
when that'll happen

00:24:41,279 --> 00:24:45,440
but that's a good question we got that

00:24:42,480 --> 00:24:48,000
one from a couple people

00:24:45,440 --> 00:24:49,600
yeah um any experience i'm using said

00:24:48,000 --> 00:24:51,600
with vms or containers you can share

00:24:49,600 --> 00:24:53,279
um so not looking this philly itself

00:24:51,600 --> 00:24:55,200
because i've um

00:24:53,279 --> 00:24:57,039
although we've seen extensions that for

00:24:55,200 --> 00:24:57,840
for containers we are more interested in

00:24:57,039 --> 00:24:59,919
like

00:24:57,840 --> 00:25:01,760
the um the secure nested paging uh

00:24:59,919 --> 00:25:02,400
feature within amd chipsets that we're

00:25:01,760 --> 00:25:04,559
looking into

00:25:02,400 --> 00:25:07,039
is being able to protect those container

00:25:04,559 --> 00:25:07,039
runtimes

00:25:09,840 --> 00:25:14,240
uh not sure if you can answer but has cf

00:25:12,799 --> 00:25:14,640
has coffer notice servers that various

00:25:14,240 --> 00:25:15,840
clouds

00:25:14,640 --> 00:25:16,880
disappearing completely or we're

00:25:15,840 --> 00:25:18,080
suddenly going offline and they're

00:25:16,880 --> 00:25:19,760
coming back online

00:25:18,080 --> 00:25:21,120
um not that we can really answer it but

00:25:19,760 --> 00:25:23,440
it's a concern of

00:25:21,120 --> 00:25:24,720
uh of us and hence we wanted to protect

00:25:23,440 --> 00:25:26,720
kind of like the

00:25:24,720 --> 00:25:28,960
the some of the physical attack vectors

00:25:26,720 --> 00:25:29,840
um this being one of them that if this

00:25:28,960 --> 00:25:32,000
were to happen

00:25:29,840 --> 00:25:33,840
you know how could we ensure that that

00:25:32,000 --> 00:25:36,400
um that our hardware

00:25:33,840 --> 00:25:38,799
itself was was being protected as best

00:25:36,400 --> 00:25:38,799
as possible

00:25:40,000 --> 00:25:43,600
um is there any reason to use uh

00:25:42,480 --> 00:25:46,159
transparent

00:25:43,600 --> 00:25:48,159
sme instead of sme if the os linux

00:25:46,159 --> 00:25:49,679
supports sme and then we wanted to just

00:25:48,159 --> 00:25:54,880
do it transferring the background

00:25:49,679 --> 00:25:54,880
and have it be more bios controlled so

00:25:55,360 --> 00:25:58,799
you know we didn't want to just turn it

00:25:57,600 --> 00:26:02,080
on ad hoc

00:25:58,799 --> 00:26:03,520
um and since a lot of what we do is when

00:26:02,080 --> 00:26:04,880
we deploy our hardware we

00:26:03,520 --> 00:26:06,559
automate a lot of the configurations we

00:26:04,880 --> 00:26:11,840
figured it was a lot easier

00:26:06,559 --> 00:26:11,840
to turn this flag on

00:26:15,279 --> 00:26:18,880
and then we already answered the info

00:26:22,840 --> 00:26:26,320
question

00:26:24,000 --> 00:26:27,360
uh the comparison between sgx and that's

00:26:26,320 --> 00:26:30,480
me um

00:26:27,360 --> 00:26:32,000
is not right here we

00:26:30,480 --> 00:26:33,840
you're right it is actually a different

00:26:32,000 --> 00:26:34,240
threat model the reason we brought that

00:26:33,840 --> 00:26:38,159
on

00:26:34,240 --> 00:26:39,760
is that it was more or less from uh from

00:26:38,159 --> 00:26:41,760
some of the beyonce side

00:26:39,760 --> 00:26:43,360
we didn't want the specific overhead of

00:26:41,760 --> 00:26:44,720
actually enabling

00:26:43,360 --> 00:26:46,720
enclaves because there were limitations

00:26:44,720 --> 00:26:47,279
if we were looking at protecting all

00:26:46,720 --> 00:26:49,919
memory

00:26:47,279 --> 00:26:50,840
um we didn't want to have that limited

00:26:49,919 --> 00:26:55,360
memory space

00:26:50,840 --> 00:26:55,760
um uh most sjx machines from what we

00:26:55,360 --> 00:26:57,919
found

00:26:55,760 --> 00:26:59,840
out you know just allocate a maximum of

00:26:57,919 --> 00:27:03,440
128 megs of memory

00:26:59,840 --> 00:27:05,200
for the epc phase cache uh and that

00:27:03,440 --> 00:27:06,559
would be shared amongst all enclaves so

00:27:05,200 --> 00:27:07,919
we didn't arbitrarily want to

00:27:06,559 --> 00:27:10,400
choose what we wanted to run in the

00:27:07,919 --> 00:27:12,559
enclave we just were more consistently

00:27:10,400 --> 00:27:14,720
worried about memory protection

00:27:12,559 --> 00:27:17,840
so while not the same threat model it

00:27:14,720 --> 00:27:17,840
was still the same concern

00:27:18,840 --> 00:27:21,840
um

00:27:22,799 --> 00:27:28,960
i think that's all the questions we have

00:27:25,919 --> 00:27:31,200
yeah i think we addressed them all

00:27:28,960 --> 00:27:31,200
okay

00:27:32,840 --> 00:27:35,840
oh

00:27:36,960 --> 00:27:40,799
let's try and get to some more of these

00:27:38,320 --> 00:27:40,799
if we can

00:27:40,960 --> 00:27:45,840
oh okay you see at the top where you can

00:27:43,679 --> 00:27:45,840
click

00:27:46,320 --> 00:27:49,679
yeah yeah would it make sense to

00:27:48,159 --> 00:27:50,240
provision per user memory encryption

00:27:49,679 --> 00:27:53,440
keys

00:27:50,240 --> 00:27:54,240
um that's that's something that we have

00:27:53,440 --> 00:27:57,520
thought about

00:27:54,240 --> 00:27:59,919
um uh something we're still looking into

00:27:57,520 --> 00:28:02,240
um so yeah that's that's that's

00:27:59,919 --> 00:28:04,000
potentially a use case too

00:28:02,240 --> 00:28:05,919
can virtual machines choose to utilize

00:28:04,000 --> 00:28:07,440
this independent independent of the host

00:28:05,919 --> 00:28:08,960
yeah that's a feature of uh secure

00:28:07,440 --> 00:28:13,039
encrypted virtualization

00:28:08,960 --> 00:28:16,240
so if you are again running hypervisors

00:28:13,039 --> 00:28:16,559
um you'd have the the host os run its

00:28:16,240 --> 00:28:19,600
own

00:28:16,559 --> 00:28:23,520
encryption key and every vm have um

00:28:19,600 --> 00:28:23,520
uh encryption key assigned to it

00:28:26,240 --> 00:28:29,039
uh fme doesn't have cryptographic

00:28:27,600 --> 00:28:29,679
integrity can you give some thoughts on

00:28:29,039 --> 00:28:31,679
whether your throat

00:28:29,679 --> 00:28:33,760
includes the tactic of integrity if it

00:28:31,679 --> 00:28:37,200
does if it doesn't include should it

00:28:33,760 --> 00:28:39,679
um yeah that's a tough question um

00:28:37,200 --> 00:28:39,679
we we

00:28:40,320 --> 00:28:45,039
it does um and i think that's something

00:28:43,600 --> 00:28:48,640
that we've kind of addressed with

00:28:45,039 --> 00:28:51,279
the vendor um uh specifically

00:28:48,640 --> 00:28:53,120
um is that if we're if we're trying to

00:28:51,279 --> 00:28:54,480
do like some form of measurement then

00:28:53,120 --> 00:28:57,200
it's either gonna be a combination

00:28:54,480 --> 00:28:58,000
of some other features that we are going

00:28:57,200 --> 00:29:00,000
to enable

00:28:58,000 --> 00:29:02,159
or something we defer back to the vendor

00:29:00,000 --> 00:29:02,159
too

00:29:03,840 --> 00:29:08,399
so michael asks you're using 32 gig

00:29:06,080 --> 00:29:08,399
dimms

00:29:08,559 --> 00:29:12,399
okay you're using 32 gig dimms do you

00:29:10,880 --> 00:29:13,840
anticipate any additional problem with

00:29:12,399 --> 00:29:15,440
higher density memory

00:29:13,840 --> 00:29:16,880
i i don't think the feature is going to

00:29:15,440 --> 00:29:19,919
work significantly different

00:29:16,880 --> 00:29:21,600
with a different memory config i like

00:29:19,919 --> 00:29:23,360
i don't have any measurements debate to

00:29:21,600 --> 00:29:25,919
base that on just um

00:29:23,360 --> 00:29:27,360
because it is all in hardware and it's

00:29:25,919 --> 00:29:28,640
all built into the memory controllers i

00:29:27,360 --> 00:29:29,679
would expect it would work the same way

00:29:28,640 --> 00:29:32,080
with 64 gig

00:29:29,679 --> 00:29:32,080
dimms

00:29:34,320 --> 00:29:36,559
and

00:29:37,760 --> 00:29:41,039
so the last one i'll take is can i in

00:29:39,440 --> 00:29:41,679
fact edit the boot rx disabled memory

00:29:41,039 --> 00:29:44,720
encryption

00:29:41,679 --> 00:29:46,480
um no we're layering that with other um

00:29:44,720 --> 00:29:49,760
with other features so things like

00:29:46,480 --> 00:29:52,240
um silicon based hardwood trusts um

00:29:49,760 --> 00:29:53,360
uh and other features to kill to uh to

00:29:52,240 --> 00:29:56,320
prevent that so

00:29:53,360 --> 00:29:56,720
um uh that's the biggest thing too and

00:29:56,320 --> 00:30:00,399
and

00:29:56,720 --> 00:30:02,399
some models that you can do um uh

00:30:00,399 --> 00:30:03,679
to kind of like limit that as well um

00:30:02,399 --> 00:30:07,679
and that's something that we're

00:30:03,679 --> 00:30:11,200
gonna look at sharing in a future talk

00:30:07,679 --> 00:30:23,840
thanks everybody for the questions yep

00:30:11,200 --> 00:30:23,840
thank you

00:30:25,520 --> 00:30:27,600

YouTube URL: https://www.youtube.com/watch?v=Vz7IQsRaC9Y


