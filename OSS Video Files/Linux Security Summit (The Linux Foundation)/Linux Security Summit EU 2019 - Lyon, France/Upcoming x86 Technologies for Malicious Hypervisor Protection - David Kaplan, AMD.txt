Title: Upcoming x86 Technologies for Malicious Hypervisor Protection - David Kaplan, AMD
Publication date: 2019-11-04
Playlist: Linux Security Summit EU 2019 - Lyon, France
Description: 
	Upcoming x86 Technologies for Malicious Hypervisor Protection - David Kaplan, AMD
Forum 1

Speakers: David Kaplan
This talk will introduce AMD SEV-SNP (Secure Nested Paging), the next generation of AMDâ€™s x86 virtualization isolation technology. Building upon the existing AMD SEV and AMD SEV-ES features released in 2017, SEV-SNP provides additional hardware security that is designed to protect VMs from malicious hypervisors. SEV-SNP adds new memory integrity protection, new use models, and more flexibility in attestation and VM management when working with protected VMs in hostile environments.

This talk will delve into the specific security that is provided by the SEV-SNP architecture, the stronger threat model that it supports, and the new hardware structures and x86 instructions being added to implement these protections. Finally, this talk will discuss the impacts of these changes to the open source ecosystem and identify areas where Linux may desire to take advantage of these new protections.
Captions: 
	                              all right well thank you everyone for                               coming so my name is David Kaplan I'm a                               security architect with AMD and today                               I'm excited we're going to be revealing                               actually for the first time publicly                               some new x                                                          we've been working on related to virtual                               machine protection and before I start I                               was asked by the lawyers to put this up                                which basically says this is all                                preliminary information so it may change                                etc so what we're going after here is a                                problem that is sometimes referred to as                                confidential computing basically the                                idea that you want to run a workload in                                an untrusted hosting environment there                                are a number of potential applications                                for this kind of the most obvious one is                                public cloud a case where you want to                                have security of your data but you might                                want to run it in a remote location or                                somewhere where you don't have much                                control what we find is that this is a                                pretty easy sell in general because the                                customers especially in scenarios like                                public cloud do want additional security                                for their data which often can be                                sensitive in nature but the cloud                                providers often don't really want to see                                what the customers are doing and so                                being able to offer some hardware                                protection for this is is where we've                                been developing and this is not our                                first foray into this space if you've                                been at this conference before you've                                probably heard me talk about our secure                                and vert secure encrypted virtualization                                technology which we've been shipping now                                for a couple generations in our server                                product line and which we first revealed                                I think maybe in                                                        been going through a generational                                improvement so we started with what we                                call SCV and this is supported in                                upstream linux starting about                                        also have a feature called sv ES which I                                talked about last year which is SUV with                                encrypted state so that adds protection                                around CPU registers and we have some                                patches on our public github for that                                and they should be getting upstream soon                                ish what I'm gonna be talking about                                today is the next generation of this                                technology that we call SNP or secure                                nested paging and this is a additional                                layer of security protection that                                focuses on memory integrity and a number                                of other attack factors that all gothe                                excuse me that I'll go through and we                                feel that this raises the security bar                                significantly and builds on top of the                                existing SUV technologies we've                                previously developed so in our                                environment we typically have these                                blocks we have what was called an AMD                                secure processor as one of the trusted                                components this is our embedded security                                subsystem that runs an arm core and it's                                executing sign firmware it has a Linux                                device driver from a software standpoint                                it looks like a PCI device and it has an                                M mio interface and this secure                                processor exposes an API related to VM                                lifecycle management and including                                launching migrating and so on the                                hypervisor interfaces with this is API                                and is responsible for not only the                                lifecycle tasks but also the scheduling                                and the allocation of the system                                resources inside of the guest we have                                the guest operating system kernel which                                is enlightened and does some things like                                choosing what memory it wants to keep                                private versus memory it wants to share                                with the rest of the world for things                                like DMA and then we have guest                                applications which are generally are                                really completely unaware of the                                security going on here so we don't have                                to modify the applications in this                                technology rather all of the special                                enlightenment is done at the operating                                system layer and we've tried to make                                this technology relatively generic it is                                tied to the CPU the x                                                 instructions and we've tried to keep the                                performance overhead down as much as                                possible                                I believe if you were to benchmark some                                 of our current technology you're talking                                 about kind of low single-digit overhead                                 in most cases now with the SNP                                 technology we are taking a stronger                                 threat model approach than we have with                                 the previous technologies if you heard                                 me talk in past years I would talk about                                 something called a benign but vulnerable                                 hypervisor which basically means that we                                 think it's working but there might be                                 bugs and so we want to have to some                                 extra protection against those bugs in                                 the S&P model we're taking that a step                                 further and we're saying we're not going                                 to trust the hypervisor and related                                 components at all we're going to assume                                 they can be malicious they can be                                 conspiring with one another and we want                                 to protect the guest in all of these                                 cases so the components that are trusted                                 here is our hardware and our sign                                 firmware and I'll talk later about how                                 that stuff can be attested and then the                                 operating system software running inside                                 of your protected VM everything else                                 including BIOS SMM other VMs and whatnot                                 is untrusted so let me go into a bit of                                 detail about what exactly we are and are                                 not protecting against in each of these                                 generations and I'll start with the                                 confidentiality protection so in our                                 previous features we have used main                                 memory encryption as a key technology we                                 have a a s                                                          controllers and that engine is capable                                 of encrypting and decrypting                                 the guest memory at very fast speed in                                 the SE ves feature we added                                 confidentiality protection for the CPU                                 register state so whenever you switch                                 out of the VM all of the registers are                                 similarly encrypted in a way the                                 hypervisor can't see them and we also                                 have DMA protection so we don't allow                                 devices to get to this memory so that                                 that's been what we've had before the                                 big new thing that we're adding now is                                 what we call its integrity protection                                 and this is specifically around software                                 based integrity attacks so things like                                 replay                                 protection data corruption memory                                 aliasing things of that sort the way                                 that I try to summarize this is that                                 integrity means that if one of these VMs                                 reads a page of memory then one of its                                 private pages of memory then it must                                 always read the value at Pass Road now                                 there's a catch here says that if it's                                 able to read that memory and I'll talk                                 later about how there may be denial of                                 service cases where it is not able to                                 but either we want to return the correct                                 result or we want to return it here and                                 because there may be many things                                 happening without the guests knowledge                                 like memory swapping migration whatnot                                 this integrity protection has to hold                                 across all those cases and so we have                                 support in the architecture for that                                 from a availability standpoint we do                                 care about certain types of availability                                 guarantees in particular we want to                                 maintain the guarantee that exists today                                 in most virtualization systems that a                                 malicious guest cannot cause a denial of                                 service on the system and so it should                                 always be possible for a hypervisor to                                 terminate a guest at any time without                                 the consent of that guest and all of our                                 features do you support that we do not                                 support any availability guarantees in                                 the other direction because the                                 hypervisor can always choose just not to                                 schedule a guest and that's their                                 prerogative and from a physical attack                                 standpoint we do include protection for                                 what we call offline DRAM attacks so I                                 called boot attacks because the memory                                 is encrypted then we have protection for                                 that sort of thing we do not have                                 protection in these generations for what                                 I call your active DRAM corruption if                                 you're physically on the board that's a                                 much more difficult attack that is                                 currently beyond our our scope besides                                 those big ones we have also added a                                 number of other miscellaneous                                 protections in the SMP generation some                                 of which are on by default and some of                                 which are optional we have added more                                 support for we call TCB rollback so                                 basically reverting the                                 firmware that involved in this feature                                 and I'll talk a bit later about how that                                 works                                 we also have some optional controls                                 related to protection against malicious                                 interrupt injection certain kinds of                                 speculative side channel attacks that                                 have recently been in the news CPU ID                                 spoofing and so on we do not have                                 protection in any of these features for                                 what I like to call architectural side                                 channels so you know we you still have a                                 cache you still have a TLB and if your                                 algorithm is susceptible to side channel                                 attacks in that way there is nothing                                 extra that we are doing in the hardware                                 here to protect against that similarly                                 we do not have protection in these                                 features for things like performance                                 counter monitoring the rationale that                                 we've taken here is that we're primarily                                 trying to protect the data inside of                                 virtual machines we are not as worried                                 about protecting the code so if someone                                 is able to figure out that you're                                 running a database okay but we don't                                 want them to be able to figure out what                                 the data is in that database that is the                                 higher priority information so how do we                                 do all this well let's start with the                                 integrity promise and this is enforced                                 through a new structure that we call the                                 reverse map table and this is a large                                 structure that is allocated at boot time                                 it is a single global structure and                                 basically it contains one entry for                                 every page of memory and included in                                 each entry are various bits that                                 indicate who that page belongs to so we                                 can have hypervisor owned pages which is                                 the default we can have pages that are                                 assigned to specific guests we can also                                 have pages that are reserved for our                                 firmware use for various lifecycle tasks                                 there are new x                                                      added that I'll talk through in a bit                                 that are used to manipulate these                                 entries so the table cannot be directly                                 manipulated by software the primary                                 purpose of the RMP is to indicate                                 ownership and therefore write ability so                                 we do not want to ever allow software to                                 write two pages which it does not which                                 is it isn't                                 the proper owner of the way that this is                                 enforced is as part of our CPU and iommu                                 table lock behavior so in a native                                 non-virtualized mode we will translate                                 an address from a virtual to physical                                 and then we use that physical to Index                                 this RMP table and we will read out the                                 entry corresponding to that and check                                 who the owner is and if it does not say                                 that it's a hypervisor page then you get                                 a page fault with a new bit that says                                 that this was a costly error if you are                                 in the guest the check is a little bit                                 more complicated in this case we have                                 two level paging so we translate a guest                                 virtual to a guest physical and then to                                 a system physical again we go and index                                 the table and in this case the table                                 should say not only that this page                                 belongs to a certain guest but also what                                 physical address it was supposed to be                                 mapped at in the guest address space and                                 so we take that value we compare it                                 against what we just encountered in the                                 walk and if anything there is not cool                                 then we generate a fault                                 this check is done for any writes                                 because of course we have to protect                                 against corruption we check it for reads                                 in some cases specifically for guests                                 but we do not check the RMP for reads in                                 general basically for performance                                 reasons in part because we already have                                 the memory encryption going on so if                                 you're in hypervisor mode and you                                 attempt to read memory that as part of                                 guest you're going to see cipher text                                 anyways and we're cool with that                                 so the RMP by its construction directly                                 protects against things like data                                 corruption replay because of this new                                 check it also protects against memory                                 aliasing because you can't have a single                                 page map to two guests at the same time                                 there's another aspect here too which is                                 has to do with memory remapping and we                                 address this problem through a technique                                 we call page validation and essentially                                 what this means is that adding a page to                                 the guest address space is a two-step                                 process the first thing is that the                                 hypervisor has to donate the page of the                                 guest and it does this using the what's                                 called RMP update instruction and that                                 lets it write to an entry inside the RMP                                 and that puts the page into a state we                                 call guest invalid and when the page is                                 in that state it is not usable by the                                 hypervisor because it's giving it away                                 it's also not usable by the guest                                 because it hasn't accepted it yet the                                 guest can then execute its own                                 instruction called P validate page                                 validate which will set a bit in the RMP                                 indicating that the page is now in the                                 guest valid state and it can be read and                                 written by the guest the key thing is                                 that the guest is expected to only                                 validate each page in its guest physical                                 address space once and this is required                                 in order to maintain the appropriate                                 security around the mapping between the                                 guest physical space and the system                                 physical space and this is probably a                                 little bit easier explained with a                                 picture here so the idea is that say we                                 have a guest physical address a and it                                 is initially mapped to system physical                                 address X through the standard nested                                 page tables the guest boots up and it                                 says all right I want to use address a                                 so it does p validate on that the                                 hardware in turn sets the appropriate                                 validated bit and it can use it if the                                 malicious hypervisor at some point here                                 tries to remap this page to a different                                 location it could go and it could try to                                 create an RNP entry at a new location                                 let's say for page Y and it could flip                                 the nested page tables over there but                                 the catch is that when our MP update is                                 executed the hardware clears this                                 validated bit to zero and if the guests                                 were to try to access it the hardware                                 will generate a fault on the guest                                 access and that fault actually goes to                                 the guest and basically says you                                 accessed a page that was not validated                                 and the guest at this point could say                                 well that's very strange because I had                                 previously validated address a and                                 therefore I think that I'm under attack                                 and so this is what I mentioned earlier                                 about how the guest may not always be                                 able to read a page in memory this is                                 one case where that could happen in this                                 case we get a fault saying that the                                 harbor                                 unable to figure out what the correct                                 data was so we're just going to tell you                                 that we failed                                 so I mentioned there's like a guest                                 valid guest invalid state we actually                                 have about eight total page states in                                 this architecture I'm not going to go                                 through all of these but these are used                                 at various points in the guests                                 lifecycle we also have certain pages                                 that are used around metadata when you                                 go to swap pages to disk and things like                                 this in order to maintain all the                                 integrity guarantees some of the sub                                 pages change stayed in three ways as you                                 can see here so there's the RMP update                                 and P validate x                                                       just mentioned and then all the rest of                                 the the green arrows here are due to                                 various API calls to that AMD secure                                 processor another feature that we have                                 added into this architecture that I                                 think is is pretty interesting is                                 something we call virtual machine                                 privilege levels or V MPLS and this is                                 an optional feature but it allows for                                 dividing a guest address space into up                                 to four different levels so we have we                                 call VM pl                                                       privilege and VM pl                                                      privilege and each V CPU of the guest                                 may be associated with a VM PL level and                                 that's like a static definition for that                                 V CPU each entry in the RMP in turn is                                 extended with permission bits various                                 read/write/execute bits for each page at                                 each level and so the idea is that you                                 can have different V CPUs with different                                 access to parts of against address space                                 and the guest can change these                                 permission bits using a new x                                   instruction we call our MT adjust so why                                 would you want to do this there's a few                                 architectures that we have in mind and                                 you guys may think of some others as                                 well one case here is that you may want                                 to have a security enforcement layer                                 that exists on top of a rich OS so in a                                 standard compute                                 you could do this using virtualization                                 using a hypervisor but once you move to                                 a cloud environment than you know the                                 hypervisor is already there you can't                                 control that so in this model VM pl                                      do a lot of that same security                                 enforcement by marking certain pages                                 read-only or not executable in a trusted                                 manner on top of the rich OS so in a                                 sense it's like a form of nested                                 virtualization there's a couple other                                 use cases that we see this as being                                 interesting for one of them is around                                 interrupt protection so it is likely                                 that you know Linux and other operating                                 systems make assumptions about interrupt                                 behavior based on bare metal systems you                                 may assume that you don't get interrupts                                 when interrupts are masked or that you                                 don't get a UD exception on an add                                 instruction because that wouldn't make                                 any sense all of those behaviors however                                 are possible under a malicious                                 hypervisor and they could take a guest                                 out of let's say it's design space so                                 what we have defined as a couple new                                 modes that are designed to be used in                                 conjunction with this VM PL architecture                                 and the idea is that you have the VM PL                                                                                                      layer which can communicate with the                                 hypervisor through some sort of a                                 paravirtualized interface using a                                 doorbell so instead of a hypervisor                                 injecting interrupts directly it would                                 say using some memory I've got some                                 events for you and it would inject a                                 doorbell using a new exception vector we                                 call pound HV in turn the VM PL                                         then go and actually inject those                                 interrupts when it deems appropriate                                 into the rich OS so if you imagine that                                 you have some VM PL                                                      v CP you could go and set some bits in a                                 protected area that say inject a certain                                 interrupt a certain exception and so on                                 so combined together this basically                                 turns into secure epoch emulation we've                                 moved the epoch emulation inside of the                                 which is now inside of the trust                                 boundary and so if this is a security                                 concern for a particular use case than                                 this can help address that                                 another interesting capability of the                                 VMP architecture is that it enables                                 better support for unenlightened guests                                 so earlier I mentioned that the guest                                 operating system had to be enlightened                                 in order to work with this SCV                                 technology but of course there's a lot                                 of workloads out there that that's not                                 practical and with the VMP architecture                                 we can basically use a higher privilege                                 level like vm PL                                                 whenever an event occurs at a lower                                 privilege level we can actually exit                                 high Peyser can go and invoke the shim                                 and say figure out what went on here and                                 that shim can take care of doing all the                                 appropriate instruction emulation                                 instruction cracking and anything else                                 necessary so this won't be as fast as if                                 you had the native enlightenment but we                                 think it's an interesting approach to                                 possibly extending this type of security                                 protection to workloads that otherwise                                 wouldn't be able to benefit from it and                                 the final thing here I want to mention                                 on the hardware capabilities is we call                                 it trusted platform information                                 so traditionally CPU ID is the x                                   instruction that's used to discover                                 capabilities of the platform and it's                                 typically intercepted by the hypervisor                                 and the hypervisor may choose to provide                                 different information with the native                                 system supports in the S&P architecture                                 we've added a capability we call CPI D                                 filtering that can allow us to make sure                                 that the hypervisor is not supplying                                 data that could cause a security problem                                 so in particular there are certain CPI D                                 leaves that specify things like                                 floating-point save areas which directly                                 turn into buffer allocations and                                 software and so those really need to be                                 correct                                 there's also instructions and                                 capabilities that if the CPI D says is                                 there the guest will try to use it and                                 I get very confused if it's not actually                                 there so in the SNP architecture we                                 basically added a way for the secured                                 processor to filter this and this can                                 either be done at boot time and like a                                 prefilled block of information or at                                 runtime as the VM discovers capabilities                                 and when it after this data gets                                 filtered then it will be it will be more                                 secure it will it will not allow the                                 hypervisor to lie about the capabilities                                 of a platform it will allow the                                 hypervisor to restrict the capabilities                                 of a platform in case it wants to for                                 migration compatibility but they can't                                 go the other direction so let me finish                                 up here by talking a bit about lifecycle                                 management so to start off with how do                                 we launch one of these guests well it's                                 a three step process so it's very simple                                 it's basically the same as in our                                 previous technology so we start with an                                 unencrypted image and first the                                 hypervisor would ask the secure                                 processor to bassy create a context                                 create a random encryption key and then                                 it will provide the unencrypted image                                 which gets encrypted by the secure                                 processor and measured and then at the                                 end the hypervisor closes the context                                 and a new thing that we've added here is                                 we allow the association of what's                                 called an ID block and an ID block is a                                 piece of information that is signed by                                 the owner of that VM so that might be                                 like the customer in a public cloud                                 environment and then ID block contains                                 things like what measurement you expect                                 to get it contains various information                                 about who you are based on public key                                 and whatnot policy information and so on                                 and assuming that this stuff passes at                                 this time of launch then this ID                                 information is bound to that guest and                                 will be useful in attestation now before                                 we get all the way attestation there's                                 another key component here which is a                                 TCP versioning that I alluded to earlier                                 so we have things like CPU microcode                                 patch we have firmware that runs on the                                 secure processor these are all mutable                                 components they can all be upgraded in                                 the case of a security vulnerability and                                 an enhancement that we've been we've                                 made in this generation is something we                                 call a version chip endorsement key                                 where basically we take the authority of                                 AMD which is fused into the chips and we                                 combine that using a cryptographic                                 method with a specific version number of                                 all these components and we produce a                                 version chip endorsement key and this is                                 done in such a way that it is                                 irreversible so a compromised component                                 cannot lie about being a newer version                                 of that component so putting this all                                 together after a vm boots we can then do                                 add a station this is a different                                 attestation flow than what we've had                                 before but based on feedback we've                                 wanted to provide more flexibility with                                 our attestation model at runtime the                                 guest machine can ask for an attestation                                 report directly from the security                                 processor and they're able to talk using                                 a set of communication keys that we                                 provision at the time that the guest                                 started when the guest asks for this                                 attestation report it supplies some                                 amount of arbitrary data and that data                                 will be included in the report gets back                                 so a typical use case might be that the                                 VM would generate a public private key                                 it would publish the public key and it                                 would create an attestation report with                                 a hash of the public key and that                                 attestation report contains all that                                 identity information from launch                                 it contains TCP information the supplied                                 data and it's all signed with as version                                                                                                          then to a remote party who is able to                                 check everything here and determine if                                 they trust the versions of a tcp that                                 this thing is running at and then it                                 knows that ok this key actually is                                 associated with this particular VM and                                 now I can communicate with it by                                 encrypting stuff with this public key                                 and this can't this attestation port can                                 be regenerated whenever is desired and                                 it could be countersigned by the cloud                                 or anything else                                 so that's kind of the startup and                                 attestation flow at a station is very                                 important because of course you need to                                 supply secrets into when he's guest                                 because they start up unencrypted so                                 things like disk decryption keys and                                 whatnot                                 once the guest is running of course you                                 may want to migrate it and my colleague                                 Tom gave a talk at kam forum yesterday                                 with more details about migration but                                 I'll just summarize here there's two                                 primary aspects to migration what we                                 call authentication and data movement so                                 authentication is can the destination                                 receive Machine receive my VM because                                 it's in compliance with policy and then                                 of course you actually have to move the                                 data a new concept in the SNP                                 architecture is what we call a migration                                 agent which is a separate virtual                                 machine that runs under all the same                                 protection that is responsible for the                                 first part is for the authentication                                 policy enforcement previously we've had                                 relatively simple migration policies                                 that kind of say migrate or don't                                 migrate and a couple other things the                                 migration agent being a piece of code                                 can enforce an arbitrarily complex                                 policy and that's more flexibility the                                 migration agent is bound to the virtual                                 machine at creation time and information                                 about it is included in that attestation                                 report because it is considered part of                                 the TCB and the migration agent itself                                 does not migrate so the idea is that you                                 start up one of these things on both                                 machines they communicate figure out if                                 everything is ok and then if so then the                                 migration agent on the source machine                                 will send the relevant secret                                 information to the one on the                                 destination the data movement piece we                                 can handle through the security                                 processor or there's also a concept                                 called a migration helper which we talk                                 about KVM form yesterday and that's kind                                 of some in guest code to make migration                                 go faster and that's something that can                                 work with this architecture as well                                 all right and the final thing I want to                                 mention here is just a little bit about                                 side channels because this is a hot                                 topic AMD has added support for                                 mitigations against some of the various                                 specter side channels especially spectre                                 v                                                                     affect AMD products this includes things                                 like the speck control MSR which has                                 IVRS and STI BP and bits like that that                                 MSR is fully virtualized in our                                 architecture so the guests can choose                                 its own policies for that but that's not                                 always enough so in this model where the                                 hypervisor is not trusted we are also                                 concerned about the hypervisor poisoning                                 the BTB                                 of the guest we don't want it to be able                                 to launch a speculative execution attack                                 against the guest and so we have an                                 optional protection for this where we                                 can track whose predictions are in the                                 BTB that's the branch prediction buffer                                 and if we determine that they do not                                 belong to the current guest that we're                                 about to enter then we will do a flush                                 and hardware so we'll guarantee that you                                 never use predictions that are not yours                                 and this is this requires a little bit                                 of care from the hypervisor side to                                 avoid being a huge performance issue but                                 we do have some support for this and                                 this is something guests can choose to                                 opt into the final thing we have related                                 to side channels is we do have a policy                                 bit that we've added in the architecture                                 around SMT so if there is a guest that                                 decides it is not comfortable with SMT                                 for any reason we will enforce and                                 hardware that it can only be run on a                                 machine with SMT disabled so                                 it's a big new feature there there's a                                 lot of information here I hope it was                                 able to make some sense we do have a                                 white paper that I'd hope to get out                                 today but it's probably going to be                                 Monday that is going to be posted on our                                 developer site that's developer AMD com                                 slash SUV and that white white paper                                 goes into more information about the                                 material that I presented here and about                                 the new capabilities of the SNP                                 architecture we're pretty excited about                                 it buy-in that we think it offers                                 significantly stronger security                                 protection as well as more flexibility                                 for different use cases and that's in                                 the form of the integrity protection the                                 VMP architecture more support around at                                 a station in memory over commit things                                 like that I did want to say also that                                 the the reason that we're talking about                                 this here is that we want to begin                                 engaging at this point with the open                                 source community around linux support                                 for some of these capabilities and in                                 particular figuring out if and how it                                 makes sense for linux to take advantage                                 of some of these capabilities things                                 like the VMP o-- architecture so if                                 that's something that anyone here is                                 interested in or has opinions about I                                 love it if you could come find me maybe                                 at the break and I'm trying to sort of                                 gather the stakeholders and figure out                                 who wants to be involved in the                                 discussion and so that'd be really                                 really appreciative if you are                                 interested in this space Wow                                 I was also asked to say by our Linux                                 manager on a somewhat related note that                                 we're also hiring so if you're                                 interested in that talk to me as well                                 and with that I think we have a couple                                 minutes for questions                                 [Applause]                                 actually I can start with one question                                 what can you tell about her overhead                                 about one overhead there's nothing that                                 we're prepared to say this point about                                 overhead other than you know we believe                                 that performance is important in the                                 presence of security features and if                                 it's too much then people won't turn it                                 on and so we're certainly do everything                                 we can to keep the overhead minimalized                                 so questions                                 thank you very much for the nice                                 presentation could I ask you to turn                                 back to swipe number                                                   actually my report I think had a talk                                 about isolating clinics namespaces with                                 different page tables do you think that                                 could help implement different page                                 tables in hard in hardware system manner                                 in order to isolate Linux namespaces                                 it's an idea it's not a direction that                                 we have really pursued too much in part                                 because in this model we basically try                                 not to trust the page tables we don't                                 have any way of protecting them and it's                                 very difficult to do so given their                                 structure so that's why we basically                                 done all the protection at the very end                                 we said you know however you want to get                                 to a physical address that's fine but at                                 the end of it we're gonna make sure that                                 it's safe for you to use we haven't                                 tried to kind of control the page tables                                 as much know that quite answers your                                 question okay okay and another question                                 when dos abuse will be available on the                                 market                                 any idea you're asking when this is                                 gonna be available in the market yeah                                 we're not prepared at this point to                                 discuss that we're just disclosing the                                 technology so we can start working with                                 the community but more information will                                 come in the future okay thank you                                 a question say in what situations do                                 consider a hypervisor to be malicious                                 so that's not necessary for us to judge                                 we're trying to just offer the                                 protection where as a cloud provider                                 someone you can basically offer the                                 promise that the hardware prevents us                                 from looking at your data so it's not                                 necessarily that we expect the                                 hypervisor to be malicious we just want                                 to have that protection especially in                                 the case where say there is a software                                 bug or some sort of compromise and it                                 now has become malicious because someone                                 has taken it over                                 so we're wanting to add an additional                                 layer of protection for that how the                                 questions thanks for the talk of the RAM                                 some researchers about Intel                                 Architecture which allow using JTAG and                                 other develop develop debugging                                 technologies on Intel platform and I                                 guess MD has the same some JTAG sounds                                 like that and did you work on separating                                 this security mitigations in md platform                                 with debugging capabilities yeah so let                                 me answer that a couple ways so we do                                 have JTAG debugging capabilities on our                                 platforms like all chip vendors however                                 all the parts that we ship for                                 production have those disabled and so                                 that is not a capability that customers                                 outside of of our labs are able to                                 utilize so JTAG is and I'm not sure how                                 other companies work with that but at                                 least for us JTAG is not a customer                                 visible jate debug feature instead we do                                 have a debug policy bit that guests that                                 run in this mode can choose to use and                                 when they opt into that then we allow                                 the secure processor to decrypt and                                 encrypt their data basically at the                                 hypervisors discretion and provide it                                 back and so                                 if you want to run GDB or something like                                 that on one of these VMs you can as long                                 as the guest has opted into allowing                                 that that debugging does that make sense                                 I think our questions                                 how does this interact with RO hammer                                 and things along those lines it seems                                 like it would be hard to attest the                                 goodness of your DRAM yeah so this was                                 not designed specifically for RO hammer                                 but there is encrypting the data and erm                                 does help in some regards the encryption                                 does happen before things like ECC                                 protections so you don't lose out on                                 your ECC by using memory encryption and                                 we do have other mitigations for ro                                 hammer in our memory controllers which                                 I'm not an expert in but I know that                                 there's some new capabilities in ddr                                  that help with that based on some papers                                 that I have seen that have dealt with ro                                 hammer attacks memory encryption which                                 by the ways is available today in our                                 hardware can help in many cases because                                 of the fact that if you do flip one bit                                 you effectively flip                                                     not designed specifically for that                                 hey you mentioned that the host is able                                 to read the ciphertext of the VMF new                                 time is there any temporal protection to                                 prevent fingerprinting attacks for                                 example if the host reads a memory                                 location repeatedly seeing whether or                                 not whether that value has changed                                 whether it changes back to a previous                                 value yeah a temporal protection we                                 don't have any fingering protection in                                 this generation it's certainly something                                 that we're thinking about we we do have                                 like the encryption algorithm is tweaked                                 so the same plaintext at different                                 locations will appear different but if                                 you're looking at one location and just                                 whether that changes or not that's not                                 something in our current scope questions                                 if not let's thank David for talk                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=yr56SaJ_0QI


