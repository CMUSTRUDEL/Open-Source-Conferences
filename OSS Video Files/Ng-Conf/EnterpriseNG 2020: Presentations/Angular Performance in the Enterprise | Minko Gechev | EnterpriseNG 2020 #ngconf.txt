Title: Angular Performance in the Enterprise | Minko Gechev | EnterpriseNG 2020Â #ngconf
Publication date: 2021-04-29
Playlist: EnterpriseNG 2020: Presentations
Description: 
	In this presentation, weâ€™re going to focus on the runtime performance of Angular applications. First, weâ€™ll learn how to profile an app using Chrome DevTools. After that, weâ€™ll identify different patterns looking into the profilerâ€™s output. For each one of them, weâ€™ll discuss strategies for reducing the runtime performance and their consequence.

Learn the best ways to build reliable web applications, write quality code, choose scalable architectures, and create effective automated tests at the Reliable Web Summit this August 26-27, 2021. Powered by the team at ng-conf.
Get your ticket ðŸ‘‰ https://reliablewebsummit.com/

ng-conf is a multi-day Angular conference focused on delivering the highest quality training in the Angular JavaScript framework. 1000's of developers from across the globe join together to attend talks and workshops by the Angular team and other community experts.

Follow us on twitter https://twitter.com/ngconfâ€‹ 
Official Website: https://www.ng-conf.org/
Captions: 
	00:00:00,120 --> 00:00:16,720
[Music]

00:00:03,840 --> 00:00:19,039
let us

00:00:16,720 --> 00:00:20,000
hello everyone my name is miku gachev

00:00:19,039 --> 00:00:22,720
i'm working on

00:00:20,000 --> 00:00:23,680
angular at google today i want to share

00:00:22,720 --> 00:00:25,920
with you a couple of

00:00:23,680 --> 00:00:26,880
insights on how you can optimize your

00:00:25,920 --> 00:00:29,599
applications

00:00:26,880 --> 00:00:31,439
runtime performance first we're going to

00:00:29,599 --> 00:00:33,600
look into how to diagnose

00:00:31,439 --> 00:00:35,760
a few common performance problems by

00:00:33,600 --> 00:00:38,800
using chrome devtools

00:00:35,760 --> 00:00:41,120
i'll explain what plane charts are and

00:00:38,800 --> 00:00:42,719
how we can use them to find performance

00:00:41,120 --> 00:00:44,559
pitfalls

00:00:42,719 --> 00:00:46,399
as the next step we're going to discuss

00:00:44,559 --> 00:00:49,600
how to optimize our apps

00:00:46,399 --> 00:00:51,280
and make them faster finally we're going

00:00:49,600 --> 00:00:52,399
to look into the javascript virtual

00:00:51,280 --> 00:00:54,960
machine runtime

00:00:52,399 --> 00:00:57,199
and explore how it could impact our

00:00:54,960 --> 00:00:59,120
app's performance

00:00:57,199 --> 00:01:01,600
i've been doing a lot of work in this

00:00:59,120 --> 00:01:04,799
space over the past couple of years

00:01:01,600 --> 00:01:05,519
often at events or on the internet folks

00:01:04,799 --> 00:01:08,400
ask me

00:01:05,519 --> 00:01:08,799
how can i make my application run faster

00:01:08,400 --> 00:01:10,720
well

00:01:08,799 --> 00:01:12,080
the high level answer to this question

00:01:10,720 --> 00:01:15,360
is pretty simple

00:01:12,080 --> 00:01:17,520
just do less this advice is valid not

00:01:15,360 --> 00:01:19,759
only in the context of angular

00:01:17,520 --> 00:01:21,759
but for any framework or programming

00:01:19,759 --> 00:01:24,720
language out there

00:01:21,759 --> 00:01:27,520
to make our apps run faster we should

00:01:24,720 --> 00:01:30,159
just do fewer things

00:01:27,520 --> 00:01:31,040
at ngcoff 2018 i gave the talk

00:01:30,159 --> 00:01:33,439
optimizing

00:01:31,040 --> 00:01:35,040
an angular application where i explained

00:01:33,439 --> 00:01:38,400
several practices

00:01:35,040 --> 00:01:41,119
that can make angular do less improving

00:01:38,400 --> 00:01:42,960
our apps performance

00:01:41,119 --> 00:01:44,479
things haven't changed much over the

00:01:42,960 --> 00:01:47,600
past few years and

00:01:44,479 --> 00:01:49,360
these practices are still valid in fact

00:01:47,600 --> 00:01:51,840
i would recommend you watching this

00:01:49,360 --> 00:01:55,680
video to get more value out of

00:01:51,840 --> 00:01:57,600
this current one we can memoize

00:01:55,680 --> 00:01:59,680
calculations using pure pipes

00:01:57,600 --> 00:02:00,960
or storing the results out of

00:01:59,680 --> 00:02:03,040
calculations

00:02:00,960 --> 00:02:04,320
we can skip change detection using

00:02:03,040 --> 00:02:07,119
onpush or running

00:02:04,320 --> 00:02:09,679
code outside of the angular zone and

00:02:07,119 --> 00:02:13,360
clearly we can render fewer components

00:02:09,679 --> 00:02:15,680
using virtual scrolling or pagination

00:02:13,360 --> 00:02:17,520
in this video we're going to classify

00:02:15,680 --> 00:02:20,800
common performance problems

00:02:17,520 --> 00:02:23,120
into several categories learn how to

00:02:20,800 --> 00:02:24,239
recognize them using the chrome devtools

00:02:23,120 --> 00:02:26,480
profiler

00:02:24,239 --> 00:02:27,599
and apply these practices that we

00:02:26,480 --> 00:02:31,200
already know

00:02:27,599 --> 00:02:34,840
to speed up our apps let us first

00:02:31,200 --> 00:02:36,400
look into how we can profile an

00:02:34,840 --> 00:02:38,319
application

00:02:36,400 --> 00:02:39,840
for the video i have built this

00:02:38,319 --> 00:02:42,160
dashboard

00:02:39,840 --> 00:02:42,879
here we have a few different charts a

00:02:42,160 --> 00:02:45,120
widget

00:02:42,879 --> 00:02:46,800
showing an overall score for the data we

00:02:45,120 --> 00:02:50,080
have a table

00:02:46,800 --> 00:02:52,480
and at the bottom just a bunch of links

00:02:50,080 --> 00:02:54,080
to profile this app we need to keep in

00:02:52,480 --> 00:02:57,120
mind the following

00:02:54,080 --> 00:02:59,360
three essential preconditions

00:02:57,120 --> 00:03:00,400
when building the project we need to

00:02:59,360 --> 00:03:03,760
ensure the cli

00:03:00,400 --> 00:03:06,239
is using its production environment

00:03:03,760 --> 00:03:07,519
running a production build is required

00:03:06,239 --> 00:03:09,519
because otherwise

00:03:07,519 --> 00:03:11,920
the cli will not remove coal that

00:03:09,519 --> 00:03:15,040
angular uses only during development

00:03:11,920 --> 00:03:15,280
to guard us against common mistakes such

00:03:15,040 --> 00:03:18,480
as

00:03:15,280 --> 00:03:21,200
circular bindings for example next

00:03:18,480 --> 00:03:22,080
we need to make sure we're not mangling

00:03:21,200 --> 00:03:25,440
the output

00:03:22,080 --> 00:03:27,040
of the cli this precondition is not as

00:03:25,440 --> 00:03:29,200
critical as the first one

00:03:27,040 --> 00:03:30,560
but ensuring we have readable methods

00:03:29,200 --> 00:03:33,519
and property names

00:03:30,560 --> 00:03:36,080
will help us identify the cause of

00:03:33,519 --> 00:03:38,080
issues we find

00:03:36,080 --> 00:03:39,360
finally we need to make sure we're

00:03:38,080 --> 00:03:42,959
profiling the app

00:03:39,360 --> 00:03:45,440
without any browser extensions enabled

00:03:42,959 --> 00:03:46,799
extensions can add extra noise to the

00:03:45,440 --> 00:03:49,280
profiler

00:03:46,799 --> 00:03:52,799
and even skew the results if they plug

00:03:49,280 --> 00:03:55,280
into the app's execution lifecycle

00:03:52,799 --> 00:03:58,560
the easiest way to do this is to open an

00:03:55,280 --> 00:04:01,280
app in the incognito mode

00:03:58,560 --> 00:04:03,280
alright now let me prepare our dashboard

00:04:01,280 --> 00:04:05,599
for debugging

00:04:03,280 --> 00:04:07,680
making sure it follows these three

00:04:05,599 --> 00:04:10,560
preconditions

00:04:07,680 --> 00:04:12,239
we can make sure we disable mangling by

00:04:10,560 --> 00:04:15,920
setting the ng build

00:04:12,239 --> 00:04:18,400
mango environment variable to false

00:04:15,920 --> 00:04:19,919
after that we need to invoke ng build

00:04:18,400 --> 00:04:21,759
with dash dash prot

00:04:19,919 --> 00:04:23,520
to build the app in the production

00:04:21,759 --> 00:04:25,840
environment

00:04:23,520 --> 00:04:27,680
look at this beautiful output from the

00:04:25,840 --> 00:04:29,759
angular cli here

00:04:27,680 --> 00:04:31,440
notice we are exceeding the maximum

00:04:29,759 --> 00:04:33,840
bundle budget here

00:04:31,440 --> 00:04:34,639
that is because we are using strict mode

00:04:33,840 --> 00:04:37,280
so we have

00:04:34,639 --> 00:04:38,160
lower thresholds and we also disabled

00:04:37,280 --> 00:04:40,320
mangling

00:04:38,160 --> 00:04:41,680
so our bundles will be larger because of

00:04:40,320 --> 00:04:44,160
that

00:04:41,680 --> 00:04:46,080
disabling mangling can negatively impact

00:04:44,160 --> 00:04:47,600
the profiler's output

00:04:46,080 --> 00:04:49,680
because the javascript virtual machine

00:04:47,600 --> 00:04:51,440
needs to parse more code

00:04:49,680 --> 00:04:53,600
but it shouldn't skew the metrics

00:04:51,440 --> 00:04:56,080
dramatically

00:04:53,600 --> 00:04:58,560
next we can go to this directory and

00:04:56,080 --> 00:05:01,520
start a static file server

00:04:58,560 --> 00:05:03,520
i really love using surf since it is

00:05:01,520 --> 00:05:05,360
aware of the client-side routing

00:05:03,520 --> 00:05:08,240
and when it starts a server it

00:05:05,360 --> 00:05:11,680
automatically puts the url of the app

00:05:08,240 --> 00:05:12,160
into the clipboard now to preview the

00:05:11,680 --> 00:05:14,560
app

00:05:12,160 --> 00:05:17,919
we can open an incognito chrome window

00:05:14,560 --> 00:05:20,320
and paste the url in the address bar

00:05:17,919 --> 00:05:21,520
to profile the application first go to

00:05:20,320 --> 00:05:23,280
the performance tab

00:05:21,520 --> 00:05:24,560
and after that click on the record

00:05:23,280 --> 00:05:26,720
button

00:05:24,560 --> 00:05:28,320
we can start interacting with the app to

00:05:26,720 --> 00:05:31,680
capture application

00:05:28,320 --> 00:05:32,240
usage scenarios in the profiler once we

00:05:31,680 --> 00:05:34,080
are ready

00:05:32,240 --> 00:05:35,919
we can stop the profiling and preview

00:05:34,080 --> 00:05:37,759
the flame chart

00:05:35,919 --> 00:05:39,039
here it is necessary to notice that

00:05:37,759 --> 00:05:43,759
chrome dev 2 shows

00:05:39,039 --> 00:05:46,160
us the estimated frame rate over time

00:05:43,759 --> 00:05:48,320
see how where the rate is lower there is

00:05:46,160 --> 00:05:50,960
a red line on top

00:05:48,320 --> 00:05:52,320
devtools follows the rail model it

00:05:50,960 --> 00:05:54,080
indicates risks

00:05:52,320 --> 00:05:55,759
that the frame rate drops to a level

00:05:54,080 --> 00:05:58,240
that would not allow the ui

00:05:55,759 --> 00:06:00,160
to respond within 50 milliseconds to

00:05:58,240 --> 00:06:02,479
user interaction

00:06:00,160 --> 00:06:03,440
as the next step let us look at what

00:06:02,479 --> 00:06:07,280
flame graphs

00:06:03,440 --> 00:06:09,919
are and how can we read them

00:06:07,280 --> 00:06:11,600
here is an example of a flame graph it

00:06:09,919 --> 00:06:14,800
visualizes the execution

00:06:11,600 --> 00:06:16,080
of a program over some time each

00:06:14,800 --> 00:06:18,960
rectangle size

00:06:16,080 --> 00:06:20,240
is proportional to the number of times

00:06:18,960 --> 00:06:22,400
the corresponding

00:06:20,240 --> 00:06:23,360
call ended up being part of the call

00:06:22,400 --> 00:06:26,800
stack

00:06:23,360 --> 00:06:28,960
during the profiler sampling

00:06:26,800 --> 00:06:30,880
brandon greck a performance engineer at

00:06:28,960 --> 00:06:32,479
netflix originally developed this

00:06:30,880 --> 00:06:35,520
visualization method

00:06:32,479 --> 00:06:37,840
of profiler's output all right so now

00:06:35,520 --> 00:06:38,960
let us trace the execution of a program

00:06:37,840 --> 00:06:41,199
and sample it

00:06:38,960 --> 00:06:42,400
to preview it with flame graph to get a

00:06:41,199 --> 00:06:44,400
better understanding of this

00:06:42,400 --> 00:06:47,759
visualization

00:06:44,400 --> 00:06:51,120
here we have a few functions

00:06:47,759 --> 00:06:53,919
a which calls b ends a1

00:06:51,120 --> 00:06:55,360
b which does some work and right after

00:06:53,919 --> 00:06:58,560
that calls d

00:06:55,360 --> 00:07:02,160
d which calls e and the function a1

00:06:58,560 --> 00:07:06,240
and e we first call the function

00:07:02,160 --> 00:07:07,919
a and right after that we call a1

00:07:06,240 --> 00:07:09,919
at the beginning we'll first call the

00:07:07,919 --> 00:07:12,800
function a

00:07:09,919 --> 00:07:13,199
when the profiler takes a sample it will

00:07:12,800 --> 00:07:16,400
find

00:07:13,199 --> 00:07:18,000
a in the call stack and record of this

00:07:16,400 --> 00:07:21,599
fact

00:07:18,000 --> 00:07:24,319
after that it will call b will have

00:07:21,599 --> 00:07:25,680
a and b onto the call stack in the next

00:07:24,319 --> 00:07:29,680
sample

00:07:25,680 --> 00:07:31,919
continuing we will get a b and d

00:07:29,680 --> 00:07:34,000
and at the following sample d will

00:07:31,919 --> 00:07:36,880
invoke e

00:07:34,000 --> 00:07:37,280
once the execution completes we'll get e

00:07:36,880 --> 00:07:40,080
d

00:07:37,280 --> 00:07:40,960
and b out of the call stack and we're

00:07:40,080 --> 00:07:44,400
going to invoke

00:07:40,960 --> 00:07:46,639
a1 as an example

00:07:44,400 --> 00:07:48,160
we would have completed a and the

00:07:46,639 --> 00:07:51,840
profiler will capture

00:07:48,160 --> 00:07:52,479
a1 onto the call stack the primary

00:07:51,840 --> 00:07:54,639
purpose

00:07:52,479 --> 00:07:56,879
of the flame graphs is to capture how

00:07:54,639 --> 00:07:58,160
many samples a given function occurred

00:07:56,879 --> 00:08:00,479
in

00:07:58,160 --> 00:08:02,080
since this could potentially be in a

00:08:00,479 --> 00:08:04,240
multi-threaded environment

00:08:02,080 --> 00:08:05,280
the order of execution is not something

00:08:04,240 --> 00:08:08,319
that we can express

00:08:05,280 --> 00:08:09,599
accurately with just a single graph to

00:08:08,319 --> 00:08:11,680
improve the visualization

00:08:09,599 --> 00:08:13,919
we can sort the samples in alphabetical

00:08:11,680 --> 00:08:16,000
order and merge the rectangles

00:08:13,919 --> 00:08:16,639
corresponding to a specific function

00:08:16,000 --> 00:08:19,919
call

00:08:16,639 --> 00:08:21,759
into 1. we can see that we spent a

00:08:19,919 --> 00:08:23,199
decent amount of time in b

00:08:21,759 --> 00:08:25,440
so there might be a place for

00:08:23,199 --> 00:08:27,680
optimization here

00:08:25,440 --> 00:08:30,080
well enough about flame graphs now let's

00:08:27,680 --> 00:08:32,479
talk about flame charts

00:08:30,080 --> 00:08:34,159
which is something different when the

00:08:32,479 --> 00:08:34,959
chrome dev tools team worked on their

00:08:34,159 --> 00:08:36,719
pro fiber

00:08:34,959 --> 00:08:38,159
they decided to reuse the flame graph

00:08:36,719 --> 00:08:39,760
visualization because they found it

00:08:38,159 --> 00:08:41,919
particularly useful

00:08:39,760 --> 00:08:43,760
however since their main focus was the

00:08:41,919 --> 00:08:45,920
main javascript thread

00:08:43,760 --> 00:08:49,600
they changed the format a little bit to

00:08:45,920 --> 00:08:51,440
show also the execution over time

00:08:49,600 --> 00:08:53,200
let us look into the flame chart from

00:08:51,440 --> 00:08:54,959
the profile link we did just a few

00:08:53,200 --> 00:08:56,399
minutes ago

00:08:54,959 --> 00:08:59,200
notice the calls from the angular

00:08:56,399 --> 00:09:02,399
runtime for example refresh component

00:08:59,200 --> 00:09:04,080
refresh view etc at the bottom

00:09:02,399 --> 00:09:06,560
we can find the execution of the

00:09:04,080 --> 00:09:08,959
component's template function

00:09:06,560 --> 00:09:09,839
when we select this call drag the bottom

00:09:08,959 --> 00:09:11,600
bar up

00:09:09,839 --> 00:09:14,240
and here we can see a link to the

00:09:11,600 --> 00:09:16,399
templates function exact location

00:09:14,240 --> 00:09:18,000
within the formatted source file

00:09:16,399 --> 00:09:20,720
clicking on it will take us

00:09:18,000 --> 00:09:21,120
directly to the right spot here we can

00:09:20,720 --> 00:09:22,959
find

00:09:21,120 --> 00:09:24,240
all the iv instruction rendering this

00:09:22,959 --> 00:09:26,480
template

00:09:24,240 --> 00:09:28,320
clicking on the bottom up tab we can

00:09:26,480 --> 00:09:30,959
preview all the functions

00:09:28,320 --> 00:09:31,760
the template function code and see how

00:09:30,959 --> 00:09:34,399
much time

00:09:31,760 --> 00:09:35,760
we spent in them which corresponds to

00:09:34,399 --> 00:09:38,880
the number of samples

00:09:35,760 --> 00:09:40,640
the profiler captured them in

00:09:38,880 --> 00:09:42,480
now let us use this knowledge to

00:09:40,640 --> 00:09:43,360
understand what triggers the change

00:09:42,480 --> 00:09:46,480
detection

00:09:43,360 --> 00:09:48,560
and find redundant calls

00:09:46,480 --> 00:09:50,320
based on the many apps i've profiled

00:09:48,560 --> 00:09:52,000
some of the most frequent redundant

00:09:50,320 --> 00:09:54,720
change detection triggers come from

00:09:52,000 --> 00:09:57,040
set timeout set interval and request

00:09:54,720 --> 00:09:59,200
animation frame

00:09:57,040 --> 00:10:01,360
often these calls are in third-party

00:09:59,200 --> 00:10:02,399
libraries so it is not immediately

00:10:01,360 --> 00:10:05,760
apparent

00:10:02,399 --> 00:10:08,880
that they occurred well

00:10:05,760 --> 00:10:09,279
notice at the bottom here before we even

00:10:08,880 --> 00:10:11,200
get

00:10:09,279 --> 00:10:12,480
into the angular runtime there is a

00:10:11,200 --> 00:10:16,079
rectangle that says

00:10:12,480 --> 00:10:19,279
event click this event is what triggered

00:10:16,079 --> 00:10:20,800
the cycle of change detection the event

00:10:19,279 --> 00:10:23,120
maps directly to our

00:10:20,800 --> 00:10:24,880
click on the hamburger menu toggling the

00:10:23,120 --> 00:10:27,360
site navigation

00:10:24,880 --> 00:10:28,399
scrolling down we can see the detect

00:10:27,360 --> 00:10:30,720
changes call

00:10:28,399 --> 00:10:33,440
that will later indirectly invoke the

00:10:30,720 --> 00:10:36,959
component's template functions

00:10:33,440 --> 00:10:40,160
zooming out however notice that we have

00:10:36,959 --> 00:10:44,640
many similar change detection calls

00:10:40,160 --> 00:10:44,640
many more than the clicks with it

00:10:44,880 --> 00:10:50,880
zooming in we can see a timer event

00:10:48,800 --> 00:10:52,640
judging based on the equal intervals

00:10:50,880 --> 00:10:55,760
we're on change detection in here

00:10:52,640 --> 00:10:58,320
this seems like a leaked set interval

00:10:55,760 --> 00:10:59,839
if this behavior was not intended we can

00:10:58,320 --> 00:11:03,760
just wrap the invocation

00:10:59,839 --> 00:11:05,440
inside of ng zone run outside angular

00:11:03,760 --> 00:11:08,800
just to remove redundant change

00:11:05,440 --> 00:11:11,519
detection calls and optimize our app

00:11:08,800 --> 00:11:13,120
okay well as a next step let us look

00:11:11,519 --> 00:11:16,240
into how we can detect

00:11:13,120 --> 00:11:17,680
long calls long calls could be

00:11:16,240 --> 00:11:19,680
particularly harmful

00:11:17,680 --> 00:11:21,839
to our applications performance

00:11:19,680 --> 00:11:24,240
especially if there are in templates

00:11:21,839 --> 00:11:26,560
or lifecycle hooks that angular invokes

00:11:24,240 --> 00:11:28,560
during change detection

00:11:26,560 --> 00:11:30,640
going back to the flame chart we can see

00:11:28,560 --> 00:11:32,959
that we have a getter called

00:11:30,640 --> 00:11:34,720
aggregate at the bottom of one of the

00:11:32,959 --> 00:11:36,480
calls

00:11:34,720 --> 00:11:38,720
clicking on the bottom up tab we can

00:11:36,480 --> 00:11:40,800
find this piece of code's exact location

00:11:38,720 --> 00:11:42,959
in the source tab

00:11:40,800 --> 00:11:44,800
to see if we're spending sufficient time

00:11:42,959 --> 00:11:45,760
in the aggregate getter as part of the

00:11:44,800 --> 00:11:47,920
change detection

00:11:45,760 --> 00:11:48,959
we can just go back to the top of the

00:11:47,920 --> 00:11:51,519
flame chart

00:11:48,959 --> 00:11:54,240
click on any of the calls there and just

00:11:51,519 --> 00:11:56,240
explore the bottom-up tab again

00:11:54,240 --> 00:11:58,720
here we can see that we have spent over

00:11:56,240 --> 00:12:01,839
50 percent of the execution time

00:11:58,720 --> 00:12:03,279
only in the aggregate getter well that

00:12:01,839 --> 00:12:05,519
is a lot of time

00:12:03,279 --> 00:12:07,440
here we have a couple of options in

00:12:05,519 --> 00:12:09,360
order to optimize the code

00:12:07,440 --> 00:12:10,639
clearly we can use memorization for

00:12:09,360 --> 00:12:12,959
example

00:12:10,639 --> 00:12:15,120
since the call occurs in the template we

00:12:12,959 --> 00:12:17,040
can even use a pure pipe

00:12:15,120 --> 00:12:19,760
all of these approaches are definitely

00:12:17,040 --> 00:12:22,800
valid at the same time however

00:12:19,760 --> 00:12:25,279
the code seems to be quite expensive

00:12:22,800 --> 00:12:25,920
so even if we apply memoization or pure

00:12:25,279 --> 00:12:27,360
pipes

00:12:25,920 --> 00:12:29,680
we'll still have to perform the

00:12:27,360 --> 00:12:32,720
calculation at least once

00:12:29,680 --> 00:12:35,839
which will hurt the initial performance

00:12:32,720 --> 00:12:37,760
and initial rendering of our app

00:12:35,839 --> 00:12:40,720
what we could do instead is move the

00:12:37,760 --> 00:12:42,639
calculation into a web worker

00:12:40,720 --> 00:12:44,240
let us go to the terminal and just run

00:12:42,639 --> 00:12:47,920
ngg

00:12:44,240 --> 00:12:50,480
web worker specifying the worker's name

00:12:47,920 --> 00:12:52,480
now open the worker file and let us

00:12:50,480 --> 00:12:54,720
replace its content

00:12:52,480 --> 00:12:57,440
here i'm using a snippet but let me

00:12:54,720 --> 00:12:59,760
quickly go through the code

00:12:57,440 --> 00:13:01,040
we declared a message listener and in

00:12:59,760 --> 00:13:03,839
the callback we get

00:13:01,040 --> 00:13:06,480
a message id and an array over which

00:13:03,839 --> 00:13:09,519
we're going to perform the calculation

00:13:06,480 --> 00:13:12,079
we use id just to ensure we return the

00:13:09,519 --> 00:13:14,959
result associated with a correct

00:13:12,079 --> 00:13:15,839
worker message at the bottom of the

00:13:14,959 --> 00:13:18,160
function

00:13:15,839 --> 00:13:20,000
we just pull the result back associating

00:13:18,160 --> 00:13:23,839
it with the message id we received

00:13:20,000 --> 00:13:25,760
earlier to use the worker i'm going to

00:13:23,839 --> 00:13:28,160
create a very simple service

00:13:25,760 --> 00:13:28,959
this way we can quickly mock it and

00:13:28,160 --> 00:13:31,600
cache

00:13:28,959 --> 00:13:33,360
different calls here we first

00:13:31,600 --> 00:13:35,760
instantiate the worker

00:13:33,360 --> 00:13:37,120
after that adds an event listener to

00:13:35,760 --> 00:13:39,199
process the response

00:13:37,120 --> 00:13:42,160
with the calculated result and at the

00:13:39,199 --> 00:13:44,720
bottom we send a message to the worker

00:13:42,160 --> 00:13:45,600
before that ensuring that there are no

00:13:44,720 --> 00:13:49,040
other pending

00:13:45,600 --> 00:13:50,800
calls finally we can just update the

00:13:49,040 --> 00:13:52,800
getter to reuse the service which

00:13:50,800 --> 00:13:54,959
communicates with the worker

00:13:52,800 --> 00:13:57,360
first we're going to inject it into the

00:13:54,959 --> 00:13:59,680
constructor of the home component

00:13:57,360 --> 00:14:00,720
after that we'll invoke its calculate

00:13:59,680 --> 00:14:03,920
method passing

00:14:00,720 --> 00:14:05,920
the required parameters if we get a

00:14:03,920 --> 00:14:07,680
number we're just going to return it

00:14:05,920 --> 00:14:09,440
alternatively we want to return the

00:14:07,680 --> 00:14:11,440
string calculating

00:14:09,440 --> 00:14:13,440
since well this is an asynchronous

00:14:11,440 --> 00:14:15,519
calculation

00:14:13,440 --> 00:14:17,120
here we rely on the fact that anywhere

00:14:15,519 --> 00:14:19,120
will cause change detection

00:14:17,120 --> 00:14:20,399
when the microtask queue of the browser

00:14:19,120 --> 00:14:22,800
is empty

00:14:20,399 --> 00:14:24,720
this way the aggregate getter will

00:14:22,800 --> 00:14:27,760
return the numeric value

00:14:24,720 --> 00:14:29,279
at the last change detection co and

00:14:27,760 --> 00:14:31,199
we're going to just make sure that we

00:14:29,279 --> 00:14:33,440
have consistent state of the view this

00:14:31,199 --> 00:14:33,440
way

00:14:34,480 --> 00:14:39,120
we can now preview the results notice

00:14:36,399 --> 00:14:39,600
that we get a calculating label for a

00:14:39,120 --> 00:14:41,760
bet

00:14:39,600 --> 00:14:44,720
until it changes to the course result in

00:14:41,760 --> 00:14:46,959
just a few milliseconds

00:14:44,720 --> 00:14:48,480
let us now look into the final pattern

00:14:46,959 --> 00:14:49,920
that we're going to describe in today's

00:14:48,480 --> 00:14:51,920
video

00:14:49,920 --> 00:14:52,959
in this scenario we have a really large

00:14:51,920 --> 00:14:56,079
component tree

00:14:52,959 --> 00:14:58,079
with many cheap calculations for example

00:14:56,079 --> 00:15:01,920
very simple templates and life cycle

00:14:58,079 --> 00:15:04,959
hooks without any heavy calculations

00:15:01,920 --> 00:15:06,959
here is one such flame chart we can see

00:15:04,959 --> 00:15:09,199
that there is still a frame drop

00:15:06,959 --> 00:15:11,120
that can impact the user experience but

00:15:09,199 --> 00:15:12,079
most calls here are taking less than one

00:15:11,120 --> 00:15:15,279
millisecond so

00:15:12,079 --> 00:15:16,639
what could we do when angular calls the

00:15:15,279 --> 00:15:18,880
app's change detection

00:15:16,639 --> 00:15:20,079
it will start from the parent component

00:15:18,880 --> 00:15:22,720
and check its children

00:15:20,079 --> 00:15:23,920
after that it is also essential to

00:15:22,720 --> 00:15:25,680
notice that depending

00:15:23,920 --> 00:15:27,440
on the change detection strategy

00:15:25,680 --> 00:15:30,399
components using onpush

00:15:27,440 --> 00:15:32,399
could be cheaper than others having a

00:15:30,399 --> 00:15:34,959
parent component with many children

00:15:32,399 --> 00:15:35,759
using onpush could be relatively cheap

00:15:34,959 --> 00:15:37,600
as soon as

00:15:35,759 --> 00:15:39,360
change in the children's inputs doesn't

00:15:37,600 --> 00:15:42,399
trigger change detection

00:15:39,360 --> 00:15:43,759
in contrast however if many children are

00:15:42,399 --> 00:15:45,040
using the default change detection

00:15:43,759 --> 00:15:48,160
strategy

00:15:45,040 --> 00:15:50,320
the execution could be much slower

00:15:48,160 --> 00:15:51,600
a refactoring could be used here to

00:15:50,320 --> 00:15:53,279
improve the performance you're just

00:15:51,600 --> 00:15:54,560
creating a new parent component that

00:15:53,279 --> 00:15:57,199
uses onpush

00:15:54,560 --> 00:15:58,000
and move as many of the components using

00:15:57,199 --> 00:16:00,000
the default

00:15:58,000 --> 00:16:01,920
change detection strategy as its

00:16:00,000 --> 00:16:03,519
children

00:16:01,920 --> 00:16:05,440
this way we're going to prevent change

00:16:03,519 --> 00:16:06,320
detection running in entire component

00:16:05,440 --> 00:16:09,120
subtrees

00:16:06,320 --> 00:16:11,040
and have faster execution since we're

00:16:09,120 --> 00:16:13,440
going to do less

00:16:11,040 --> 00:16:15,040
however keep in mind that this could

00:16:13,440 --> 00:16:16,160
bring improvements during change

00:16:15,040 --> 00:16:19,680
detection

00:16:16,160 --> 00:16:21,759
but not necessary at initial rendering

00:16:19,680 --> 00:16:23,040
angular will still have to render all

00:16:21,759 --> 00:16:25,680
the components

00:16:23,040 --> 00:16:28,959
and the more components we have well the

00:16:25,680 --> 00:16:30,959
slower the rendering would be

00:16:28,959 --> 00:16:32,160
the way to fix this is to render fewer

00:16:30,959 --> 00:16:33,920
components

00:16:32,160 --> 00:16:35,279
virtual scrolling is a way to achieve

00:16:33,920 --> 00:16:37,600
this if we have

00:16:35,279 --> 00:16:39,440
thousands of items in the list virtual

00:16:37,600 --> 00:16:40,720
scrolling could help us render fewer

00:16:39,440 --> 00:16:42,160
components

00:16:40,720 --> 00:16:44,959
pagination is clearly another

00:16:42,160 --> 00:16:47,600
alternative a more advanced strategy

00:16:44,959 --> 00:16:49,360
is implementing on-demand rendering

00:16:47,600 --> 00:16:50,639
depending on what is currently visible

00:16:49,360 --> 00:16:52,240
in the viewport

00:16:50,639 --> 00:16:55,120
for the purpose we can use the

00:16:52,240 --> 00:16:57,519
intersection observer api

00:16:55,120 --> 00:16:59,440
well the chances are that you'll be able

00:16:57,519 --> 00:17:00,959
to speed up your application's runtime

00:16:59,440 --> 00:17:02,839
performance if you're following

00:17:00,959 --> 00:17:04,240
all the practices that you already

00:17:02,839 --> 00:17:05,679
mentioned

00:17:04,240 --> 00:17:07,360
especially during initial rendering

00:17:05,679 --> 00:17:08,799
however there are occasions

00:17:07,360 --> 00:17:11,360
when the javascript virtual machine

00:17:08,799 --> 00:17:14,559
runtime could bring some extra weight

00:17:11,360 --> 00:17:16,240
and make things more difficult instead

00:17:14,559 --> 00:17:16,880
of interpreting all the source code we

00:17:16,240 --> 00:17:18,959
provide

00:17:16,880 --> 00:17:20,720
the javascript virtual machine compiles

00:17:18,959 --> 00:17:22,799
it to native codes to improve

00:17:20,720 --> 00:17:24,559
performance

00:17:22,799 --> 00:17:27,120
this technique is known as just-in-time

00:17:24,559 --> 00:17:29,200
compilation or jet

00:17:27,120 --> 00:17:30,559
often jet relies on assumptions about

00:17:29,200 --> 00:17:32,400
the source code

00:17:30,559 --> 00:17:33,840
and when these assumptions turn out to

00:17:32,400 --> 00:17:35,760
be incorrect

00:17:33,840 --> 00:17:36,960
the vm needs to deoptimize the source

00:17:35,760 --> 00:17:38,799
code

00:17:36,960 --> 00:17:41,440
well we have optimized the internals of

00:17:38,799 --> 00:17:44,880
angular well for such situations

00:17:41,440 --> 00:17:47,600
but jit on its own can bring extra cost

00:17:44,880 --> 00:17:48,240
during execution especially for cold

00:17:47,600 --> 00:17:51,440
cold

00:17:48,240 --> 00:17:53,520
that hasn't been compiled yet well now

00:17:51,440 --> 00:17:55,360
let us visualize this in practice

00:17:53,520 --> 00:17:58,320
to do that we need to enable an

00:17:55,360 --> 00:18:01,600
experimental setting in chrome devtools

00:17:58,320 --> 00:18:05,280
go to the gear icon select experiments

00:18:01,600 --> 00:18:08,080
and enable timeline v8 runtime call

00:18:05,280 --> 00:18:10,240
stats on timeline

00:18:08,080 --> 00:18:11,280
enabling the setting will require a

00:18:10,240 --> 00:18:14,720
restart

00:18:11,280 --> 00:18:15,679
of devtools now when we go to

00:18:14,720 --> 00:18:17,679
performance

00:18:15,679 --> 00:18:19,440
and profile the app we're going to see

00:18:17,679 --> 00:18:21,919
something interesting

00:18:19,440 --> 00:18:23,440
let us zoom in in the first part of the

00:18:21,919 --> 00:18:25,360
timeline

00:18:23,440 --> 00:18:27,440
when we magnify further we're going to

00:18:25,360 --> 00:18:30,559
see many compile and parse

00:18:27,440 --> 00:18:32,400
calls in the flame chart

00:18:30,559 --> 00:18:34,400
these are all places where a javascript

00:18:32,400 --> 00:18:36,960
vm compiles code

00:18:34,400 --> 00:18:38,160
during code execution until jit happens

00:18:36,960 --> 00:18:41,200
some functions could take

00:18:38,160 --> 00:18:42,640
5x or even 10x the time they will take

00:18:41,200 --> 00:18:44,160
once the javascript virtual machine

00:18:42,640 --> 00:18:46,320
compiles them

00:18:44,160 --> 00:18:48,400
we can see that when we move towards the

00:18:46,320 --> 00:18:51,120
end of the timeline

00:18:48,400 --> 00:18:51,600
notice how we have almost zero compile

00:18:51,120 --> 00:18:53,600
calls

00:18:51,600 --> 00:18:55,120
and all the functions are taking much

00:18:53,600 --> 00:18:57,039
shorter

00:18:55,120 --> 00:18:58,799
here is one compile called later on

00:18:57,039 --> 00:19:01,760
because the javascript virtual machine

00:18:58,799 --> 00:19:03,600
performs jit on demand this function

00:19:01,760 --> 00:19:07,760
hasn't been called in the past

00:19:03,600 --> 00:19:09,200
so we just need to compile it right here

00:19:07,760 --> 00:19:11,200
well that was pretty much everything i

00:19:09,200 --> 00:19:12,960
had for today i hope this presentation

00:19:11,200 --> 00:19:14,559
clarifies what's happening under the

00:19:12,960 --> 00:19:16,240
hood of your app's runtime

00:19:14,559 --> 00:19:18,240
and how you can diagnose typical

00:19:16,240 --> 00:19:20,720
performance issues

00:19:18,240 --> 00:19:21,679
we explained three main patterns

00:19:20,720 --> 00:19:24,000
identifying

00:19:21,679 --> 00:19:25,200
redundant change detection triggers

00:19:24,000 --> 00:19:28,160
detecting

00:19:25,200 --> 00:19:29,039
and optimizing expensive calls using web

00:19:28,160 --> 00:19:31,600
workers

00:19:29,039 --> 00:19:33,679
and refactoring applications with large

00:19:31,600 --> 00:19:35,360
component hierarchies

00:19:33,679 --> 00:19:37,280
in the end we peek into javascript

00:19:35,360 --> 00:19:38,880
virtual machine runtime and so how

00:19:37,280 --> 00:19:39,919
function calls could be way more

00:19:38,880 --> 00:19:42,240
expensive

00:19:39,919 --> 00:19:44,080
before a javascript virtual machine

00:19:42,240 --> 00:19:45,360
compiles them

00:19:44,080 --> 00:19:47,799
thank you very much for watching this

00:19:45,360 --> 00:19:50,799
video see you next time and happy

00:19:47,799 --> 00:19:50,799

YouTube URL: https://www.youtube.com/watch?v=6_5kTeTeal4


