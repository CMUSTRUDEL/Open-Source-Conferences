Title: #6 Open Source Forum 2019 Adam Gibson
Publication date: 2020-01-29
Playlist: Open Source Forum 2019
Description: 
	
Captions: 
	00:00:09,769 --> 00:00:16,640
all right hello thank you for having me

00:00:12,559 --> 00:00:18,590
so while we are not eclipse or well

00:00:16,640 --> 00:00:19,939
we're not links just like Chris I've

00:00:18,590 --> 00:00:22,099
heard of a look at the Eclipse I

00:00:19,939 --> 00:00:24,020
mentioned actually but what we do

00:00:22,099 --> 00:00:25,820
heavily come with the the Linux

00:00:24,020 --> 00:00:28,970
ecosystem and we're looking forward to

00:00:25,820 --> 00:00:30,890
working with you so what I'm gonna do

00:00:28,970 --> 00:00:32,410
just a quick overview I'm going to give

00:00:30,890 --> 00:00:34,220
a little bit about us since we're not

00:00:32,410 --> 00:00:36,290
we're not as well-known with some of the

00:00:34,220 --> 00:00:39,050
bigger companies out there again I'll

00:00:36,290 --> 00:00:42,680
define an ecosystem and you'll see a lot

00:00:39,050 --> 00:00:44,570
of relevant themes today that are very

00:00:42,680 --> 00:00:47,570
similar to other presentations in this

00:00:44,570 --> 00:00:50,449
within the speeches today I'll talk a

00:00:47,570 --> 00:00:51,049
bit about open source frameworks so like

00:00:50,449 --> 00:00:52,519
tensorflow

00:00:51,049 --> 00:00:54,920
pine torch for those of you who are

00:00:52,519 --> 00:00:57,110
familiar with it if not I'll give a high

00:00:54,920 --> 00:01:01,220
level overview of those different file

00:00:57,110 --> 00:01:03,439
formats so some of the Catholic again no

00:01:01,220 --> 00:01:05,900
chris mentioned earlier you know a lot

00:01:03,439 --> 00:01:07,189
of you know i think a lot of foundations

00:01:05,900 --> 00:01:09,860
work together at different different

00:01:07,189 --> 00:01:11,840
things so there's there's file formats

00:01:09,860 --> 00:01:13,280
and specs at the all the different

00:01:11,840 --> 00:01:15,710
foundations that all kind of worked at

00:01:13,280 --> 00:01:18,350
work together Interop i'll talk about

00:01:15,710 --> 00:01:21,100
open accelerators so one thing kind of

00:01:18,350 --> 00:01:23,030
missing from the ecosystem today is

00:01:21,100 --> 00:01:24,560
kubernetes actually doesn't work with

00:01:23,030 --> 00:01:26,899
the PGA's or customer accelerators very

00:01:24,560 --> 00:01:27,829
well and it's still a very able to find

00:01:26,899 --> 00:01:30,770
area

00:01:27,829 --> 00:01:33,049
so schedulers today are not very open to

00:01:30,770 --> 00:01:34,789
edge there's a lot of work to do as kind

00:01:33,049 --> 00:01:37,159
of illustrated in the previous

00:01:34,789 --> 00:01:37,939
blueprints so a lot of that's still kind

00:01:37,159 --> 00:01:40,159
of in progress

00:01:37,939 --> 00:01:42,259
and so I'm hoping to make you present

00:01:40,159 --> 00:01:44,600
some newer ideas here today and then

00:01:42,259 --> 00:01:45,750
finally another another area that's not

00:01:44,600 --> 00:01:47,580
well-defined yet

00:01:45,750 --> 00:01:51,800
and I think will be relevant to LFA a

00:01:47,580 --> 00:01:56,670
people is open standards for any metrics

00:01:51,800 --> 00:01:59,010
and so to start so kind of it if we

00:01:56,670 --> 00:02:01,980
develop we're having developer of open

00:01:59,010 --> 00:02:03,540
source software so my open source life

00:02:01,980 --> 00:02:06,990
mainly comes from eclipse t pointing for

00:02:03,540 --> 00:02:08,040
J but we also heavily develop standards

00:02:06,990 --> 00:02:09,330
we work with other open source

00:02:08,040 --> 00:02:11,760
communities I'm also on the chair of

00:02:09,330 --> 00:02:13,680
Karis so I'm not a dependent care is

00:02:11,760 --> 00:02:16,200
chair as well as we work heavily with

00:02:13,680 --> 00:02:18,720
the tencel community in tensorflow Java

00:02:16,200 --> 00:02:20,730
so that's not a woman effect but we're

00:02:18,720 --> 00:02:22,170
we're actually we actually put pretty

00:02:20,730 --> 00:02:24,209
decent open-source footprint

00:02:22,170 --> 00:02:26,580
I'm also the author i'm austin or any

00:02:24,209 --> 00:02:28,500
author for deep learning and we have a

00:02:26,580 --> 00:02:30,690
few when were distributed all around the

00:02:28,500 --> 00:02:34,920
globe with our team predominantly in

00:02:30,690 --> 00:02:38,010
asia instead of the start the definition

00:02:34,920 --> 00:02:39,900
of open source PA ecosystem so basically

00:02:38,010 --> 00:02:41,940
to start the components necessary to

00:02:39,900 --> 00:02:45,000
build a fellow a and execute algorithms

00:02:41,940 --> 00:02:47,459
so Anton rhythms for those of you who

00:02:45,000 --> 00:02:51,239
don't know are primarily based on this

00:02:47,459 --> 00:02:54,120
idea of operations so operations can be

00:02:51,239 --> 00:02:56,280
something complex like something in

00:02:54,120 --> 00:03:00,000
signal processing or it can just be two

00:02:56,280 --> 00:03:01,320
plus two subtraction multiplication you

00:03:00,000 --> 00:03:03,000
know there are different Hardware

00:03:01,320 --> 00:03:05,489
different hardware but all these

00:03:03,000 --> 00:03:07,320
operations and something and they're

00:03:05,489 --> 00:03:09,300
different hardware has decays from say

00:03:07,320 --> 00:03:12,269
let's mention arm since they're here

00:03:09,300 --> 00:03:14,519
today having a permanent you know

00:03:12,269 --> 00:03:16,980
there's other vendors like that kale DNA

00:03:14,519 --> 00:03:19,470
include enn and basically the turns out

00:03:16,980 --> 00:03:22,080
the the chip vendors are better than us

00:03:19,470 --> 00:03:23,820
at writing math for their own ship and

00:03:22,080 --> 00:03:27,060
writing writing math code is a

00:03:23,820 --> 00:03:28,680
specialized area that only people who

00:03:27,060 --> 00:03:29,370
generally work in physics or something

00:03:28,680 --> 00:03:31,799
like that and

00:03:29,370 --> 00:03:34,200
having or Kevin C and C++ typically do

00:03:31,799 --> 00:03:35,730
these are not things that normal

00:03:34,200 --> 00:03:37,830
software developers will typically touch

00:03:35,730 --> 00:03:40,620
you know there are frameworks for these

00:03:37,830 --> 00:03:41,670
things which I'll cover as well so you

00:03:40,620 --> 00:03:43,230
know that's your tensorflow your

00:03:41,670 --> 00:03:46,170
pike-perch you know kind of these these

00:03:43,230 --> 00:03:48,930
Python level interfaces to these seats

00:03:46,170 --> 00:03:51,180
of these little math routines the file

00:03:48,930 --> 00:03:53,819
formats you know so there's you know we

00:03:51,180 --> 00:03:55,310
think of a protocols but you know we

00:03:53,819 --> 00:03:57,780
need we need to store our data somehow

00:03:55,310 --> 00:03:59,700
so there there's there's there's file

00:03:57,780 --> 00:04:01,739
formats out there again at different

00:03:59,700 --> 00:04:04,200
open-source foundations you know you

00:04:01,739 --> 00:04:07,200
have put up a few have a patch of arrow

00:04:04,200 --> 00:04:08,370
you have you have things built that do

00:04:07,200 --> 00:04:11,370
have protocols built on top of those

00:04:08,370 --> 00:04:13,440
things like G RPC so you know we use

00:04:11,370 --> 00:04:16,680
these file formats for storing data for

00:04:13,440 --> 00:04:18,750
storing in for storing models the later

00:04:16,680 --> 00:04:21,120
today on X will be mentioned as well

00:04:18,750 --> 00:04:22,830
it's actually a portal of these it's

00:04:21,120 --> 00:04:24,870
actually a part of the beast format and

00:04:22,830 --> 00:04:26,400
that cesspits aspect implemented as part

00:04:24,870 --> 00:04:29,820
of both so you need to know

00:04:26,400 --> 00:04:31,410
there's also hdf5 for carrots as well so

00:04:29,820 --> 00:04:35,520
again I'm given all these in a little

00:04:31,410 --> 00:04:37,710
bit open accelerators so one thing

00:04:35,520 --> 00:04:40,710
that's really hard to do is actually do

00:04:37,710 --> 00:04:42,300
cross-platform Ã«the that's not you know

00:04:40,710 --> 00:04:44,240
each each chip vendor has their own way

00:04:42,300 --> 00:04:46,889
you know their own different kind of

00:04:44,240 --> 00:04:48,960
nuances for implementing things the way

00:04:46,889 --> 00:04:51,450
they do so different different chips or

00:04:48,960 --> 00:04:53,520
different use cases are are based

00:04:51,450 --> 00:04:55,800
architectures are predominantly for you

00:04:53,520 --> 00:04:58,500
know kind of the edge edge based

00:04:55,800 --> 00:05:00,750
computing you typically have Nvidia and

00:04:58,500 --> 00:05:03,960
worked more on their own applications

00:05:00,750 --> 00:05:06,240
like TV use by Google among others and

00:05:03,960 --> 00:05:08,070
you can you have other HPC vendors like

00:05:06,240 --> 00:05:09,690
xfinity in Japan working on their own

00:05:08,070 --> 00:05:12,240
kind of specialized

00:05:09,690 --> 00:05:14,669
the ISO the readers for you know and

00:05:12,240 --> 00:05:17,160
they're all very different things like

00:05:14,669 --> 00:05:18,990
so if you do cnn's turns out you're a

00:05:17,160 --> 00:05:22,139
better computer vision but other chips

00:05:18,990 --> 00:05:23,580
might be better at NLP so it's not so

00:05:22,139 --> 00:05:25,289
there's a reason there's performance

00:05:23,580 --> 00:05:27,389
reasons and use these reasons why

00:05:25,289 --> 00:05:30,000
different chips only implement various

00:05:27,389 --> 00:05:32,160
operations or only implement

00:05:30,000 --> 00:05:34,440
optimized versions of certain routines

00:05:32,160 --> 00:05:36,690
so being able to have a cross platform

00:05:34,440 --> 00:05:37,800
SDK for that would be interesting so

00:05:36,690 --> 00:05:39,030
something that can work with all these

00:05:37,800 --> 00:05:40,410
different things you don't have to

00:05:39,030 --> 00:05:42,360
that's kind of look the deep learning

00:05:40,410 --> 00:05:44,400
frameworks take care of but even though

00:05:42,360 --> 00:05:46,440
they only do half the job so it's still

00:05:44,400 --> 00:05:48,060
so community efforts like the open

00:05:46,440 --> 00:05:49,440
blueprints that were discussed in the

00:05:48,060 --> 00:05:51,540
prior presentation are crucial to

00:05:49,440 --> 00:05:55,890
actually bridging a lot of things from

00:05:51,540 --> 00:05:58,140
research to production and finally get

00:05:55,890 --> 00:06:00,180
into metrics you know so you know we

00:05:58,140 --> 00:06:02,960
don't we don't I know that the general

00:06:00,180 --> 00:06:05,970
public talks a lot about bias and a I

00:06:02,960 --> 00:06:07,650
you know so racist models is something

00:06:05,970 --> 00:06:09,960
that comes up a lot and also accuracy

00:06:07,650 --> 00:06:13,020
you know eventually once bi is actually

00:06:09,960 --> 00:06:15,120
deployed everywhere then how how do we

00:06:13,020 --> 00:06:16,830
know what accuracy is you know so you

00:06:15,120 --> 00:06:18,990
have you have a lot of programs out

00:06:16,830 --> 00:06:20,610
there you know LFE high being one of

00:06:18,990 --> 00:06:22,740
them it's trying to build out these

00:06:20,610 --> 00:06:25,290
standards but there's you know there's

00:06:22,740 --> 00:06:26,640
also the there's also the AIA a

00:06:25,290 --> 00:06:29,490
partnership like Google and Microsoft

00:06:26,640 --> 00:06:31,200
and open AI as well so there's there's

00:06:29,490 --> 00:06:32,910
several different standards bodies kind

00:06:31,200 --> 00:06:34,650
of working on these things and it's

00:06:32,910 --> 00:06:36,240
still it's still very early yet though

00:06:34,650 --> 00:06:38,580
so what is you know what is the notion

00:06:36,240 --> 00:06:40,500
of accuracy for different use cases you

00:06:38,580 --> 00:06:42,060
know how do we know how do we know what

00:06:40,500 --> 00:06:43,620
a general you know generalize model

00:06:42,060 --> 00:06:46,590
actually looks like how do we know

00:06:43,620 --> 00:06:48,350
something too robust so for example one

00:06:46,590 --> 00:06:50,940
in one area recommender engines

00:06:48,350 --> 00:06:53,130
recommender engines are inherently based

00:06:50,940 --> 00:06:55,020
on user preferences so how do you know

00:06:53,130 --> 00:06:57,030
what you consider accurate for a given

00:06:55,020 --> 00:06:59,100
user base Netflix will be different from

00:06:57,030 --> 00:07:01,050
Amazon you know so for example we have

00:06:59,100 --> 00:07:03,960
recommender engines out there and as

00:07:01,050 --> 00:07:05,790
users we all know that we get

00:07:03,960 --> 00:07:08,280
recommended the second you buy a vacuum

00:07:05,790 --> 00:07:10,020
cleaner every sink will recommend to you

00:07:08,280 --> 00:07:13,140
vacuum cleaners and they do such a film

00:07:10,020 --> 00:07:15,270
that will say buy five more so you know

00:07:13,140 --> 00:07:17,220
as users we know that there's work to do

00:07:15,270 --> 00:07:19,410
in terms of you know recommending

00:07:17,220 --> 00:07:22,140
recommender engines with you know with

00:07:19,410 --> 00:07:25,050
e-commerce right so how do we know

00:07:22,140 --> 00:07:27,240
what we as users consider to be accurate

00:07:25,050 --> 00:07:28,620
it's hard to define right so these these

00:07:27,240 --> 00:07:30,420
things are it's still an open and

00:07:28,620 --> 00:07:31,680
research problem for many different

00:07:30,420 --> 00:07:33,300
kinds of AI problems whether it's

00:07:31,680 --> 00:07:35,190
something's you know something like

00:07:33,300 --> 00:07:39,420
recommender and it's or all the way to

00:07:35,190 --> 00:07:41,010
some random cars instead to start so

00:07:39,420 --> 00:07:43,380
this is this is an example of kind of

00:07:41,010 --> 00:07:45,390
the whole supply chain you know so so

00:07:43,380 --> 00:07:47,430
starting from starting from the top you

00:07:45,390 --> 00:07:49,050
have distributed front times you know so

00:07:47,430 --> 00:07:50,850
Nvidia for example they have they have

00:07:49,050 --> 00:07:53,550
their own they have they have their own

00:07:50,850 --> 00:07:55,080
MPI based clustering stuff you know by

00:07:53,550 --> 00:07:56,630
the Apache foundation you have a pension

00:07:55,080 --> 00:07:58,950
spark which is typically used for

00:07:56,630 --> 00:08:01,470
different different kinds of distributed

00:07:58,950 --> 00:08:03,240
as well queries and ETL and then

00:08:01,470 --> 00:08:05,280
something something that's not as well

00:08:03,240 --> 00:08:07,980
known but heavily used in finance

00:08:05,280 --> 00:08:11,220
actually real-time training is Apache

00:08:07,980 --> 00:08:13,560
Iran or Iran which is used in a lot of

00:08:11,220 --> 00:08:15,870
its by some consultants out of London

00:08:13,560 --> 00:08:18,630
heavily used in in the aquifer mark and

00:08:15,870 --> 00:08:20,660
a few other areas when we use it as our

00:08:18,630 --> 00:08:22,560
primary communication layer for our

00:08:20,660 --> 00:08:23,340
distributed spark for training and deep

00:08:22,560 --> 00:08:26,610
learning for Jane

00:08:23,340 --> 00:08:28,620
so our framework obviously is one

00:08:26,610 --> 00:08:31,140
implementation of this so we focus on

00:08:28,620 --> 00:08:33,210
providing you know we you know we

00:08:31,140 --> 00:08:35,100
started as kind of this Java thing and

00:08:33,210 --> 00:08:36,420
we kind of in and that's that's the main

00:08:35,100 --> 00:08:39,540
that's the main thing we found like

00:08:36,420 --> 00:08:43,110
today but again we also work heavily we

00:08:39,540 --> 00:08:44,310
also have a very compact C++ layer then

00:08:43,110 --> 00:08:46,260
it drops with every deep learning

00:08:44,310 --> 00:08:47,310
framework out there so we can read and

00:08:46,260 --> 00:08:50,840
write tensorflow

00:08:47,310 --> 00:08:53,250
carrots and soon-to-be plugins as well

00:08:50,840 --> 00:08:55,380
and then again as I mentioned the file

00:08:53,250 --> 00:08:56,700
format so from left to right you have

00:08:55,380 --> 00:08:58,380
Onix

00:08:56,700 --> 00:09:00,930
so that's that's kind of trying to be a

00:08:58,380 --> 00:09:01,790
neutral standard for defining neural

00:09:00,930 --> 00:09:05,190
networks

00:09:01,790 --> 00:09:06,690
Karis intentions law Kerris

00:09:05,190 --> 00:09:09,180
even though people think it supportive

00:09:06,690 --> 00:09:11,100
tensorflow it is but they also has its

00:09:09,180 --> 00:09:13,230
own file format even though it's the

00:09:11,100 --> 00:09:15,150
same interface it's actually there's

00:09:13,230 --> 00:09:17,520
actually a higher-level file format and

00:09:15,150 --> 00:09:19,860
a lower level one people don't even know

00:09:17,520 --> 00:09:21,900
people don't even know that server you

00:09:19,860 --> 00:09:23,520
observe a fully use of to Treblinka

00:09:21,900 --> 00:09:25,170
you need two different file formats a

00:09:23,520 --> 00:09:27,210
lot of people don't do that rightly so

00:09:25,170 --> 00:09:29,700
it's just it's just it's just local

00:09:27,210 --> 00:09:32,300
things like this that only hits you

00:09:29,700 --> 00:09:34,200
after you wanted to play your model so

00:09:32,300 --> 00:09:36,060
another thing people don't know for

00:09:34,200 --> 00:09:37,770
example is that teal white and

00:09:36,060 --> 00:09:39,930
tensorflow are actually fairly different

00:09:37,770 --> 00:09:42,480
it's a completely different runtime and

00:09:39,930 --> 00:09:46,500
everything and so the general purpose

00:09:42,480 --> 00:09:48,330
one and the global only one that our

00:09:46,500 --> 00:09:51,060
partner with Google on are actually very

00:09:48,330 --> 00:09:53,190
different so you know so a tie itself

00:09:51,060 --> 00:09:54,390
seems like it's it's well defined but

00:09:53,190 --> 00:09:56,400
it's actually not and there's actually a

00:09:54,390 --> 00:09:58,200
lot of fragmentation stuff

00:09:56,400 --> 00:10:00,540
there's also efforts in their compiler

00:09:58,200 --> 00:10:02,430
and by our space so there's there's one

00:10:00,540 --> 00:10:05,610
there's one there's one initiative at

00:10:02,430 --> 00:10:07,320
the Apache foundation called TBM so it's

00:10:05,610 --> 00:10:10,350
it's basically in height and intervening

00:10:07,320 --> 00:10:11,940
your representation which is basically a

00:10:10,350 --> 00:10:13,950
way it's basically wait before you get

00:10:11,940 --> 00:10:15,930
to assembly and somebody for compilers

00:10:13,950 --> 00:10:18,150
to admit something that that's platform

00:10:15,930 --> 00:10:20,340
neutral and then you basically directly

00:10:18,150 --> 00:10:22,320
mountable to assembly on a particular

00:10:20,340 --> 00:10:25,110
system there's also

00:10:22,320 --> 00:10:27,840
LLVM has something called from Google

00:10:25,110 --> 00:10:30,030
called FML IR that's being continued to

00:10:27,840 --> 00:10:33,030
the LLVM foundation that's meant to be

00:10:30,030 --> 00:10:35,940
the kind of base a way for Calvin to

00:10:33,030 --> 00:10:37,530
implement machine learning code now so

00:10:35,940 --> 00:10:40,140
and then this will allow you to target

00:10:37,530 --> 00:10:43,110
different accelerators both TVM and MLA

00:10:40,140 --> 00:10:45,180
on our both are both kind of competing

00:10:43,110 --> 00:10:46,530
standards in that space I don't know

00:10:45,180 --> 00:10:47,520
which one win I don't know if there's

00:10:46,530 --> 00:10:50,250
gonna be a cover name so if there's

00:10:47,520 --> 00:10:52,110
going to be multiple for partner you

00:10:50,250 --> 00:10:54,780
know there's there's there's lower level

00:10:52,110 --> 00:10:57,980
vendors such as camera con Intel and

00:10:54,780 --> 00:11:00,720
then the more mainstream ones are Nvidia

00:10:57,980 --> 00:11:03,930
the TPU IV in power

00:11:00,720 --> 00:11:05,820
NEC and then I know Fujitsu just has

00:11:03,930 --> 00:11:07,950
their deep learning chip as well so you

00:11:05,820 --> 00:11:10,080
know the the you know the you know East

00:11:07,950 --> 00:11:11,670
Asian things I think is some of them are

00:11:10,080 --> 00:11:13,350
big some of them are are based but some

00:11:11,670 --> 00:11:15,540
of them are just their own custom

00:11:13,350 --> 00:11:16,920
instruction sets the you know all these

00:11:15,540 --> 00:11:18,270
firms are implementing especially if

00:11:16,920 --> 00:11:20,250
they are accelerators some targeted

00:11:18,270 --> 00:11:22,200
computer diffident others targeting you

00:11:20,250 --> 00:11:23,460
know as time series these cases but

00:11:22,200 --> 00:11:25,590
there there's there's a lot of different

00:11:23,460 --> 00:11:26,259
ways to to think about this and finally

00:11:25,590 --> 00:11:27,819
packaging

00:11:26,259 --> 00:11:30,609
so packaging is generally

00:11:27,819 --> 00:11:31,899
language-specific so you know we have

00:11:30,609 --> 00:11:34,529
our own you know so as I mentioned

00:11:31,899 --> 00:11:37,059
Google uses us for four tensorflow Java

00:11:34,529 --> 00:11:39,999
so we're the were the main way of

00:11:37,059 --> 00:11:41,290
packaging any C code you AC code you

00:11:39,999 --> 00:11:43,419
think of as only having a Python

00:11:41,290 --> 00:11:44,949
remember we provide a Java we provide an

00:11:43,419 --> 00:11:47,229
automatically generated equivalent with

00:11:44,949 --> 00:11:50,470
built a garbage collection and a bunch

00:11:47,229 --> 00:11:52,809
of other things so we're going it's our

00:11:50,470 --> 00:11:56,619
format that's also within source it's

00:11:52,809 --> 00:11:58,179
been used since 2012 for for packaging

00:11:56,619 --> 00:12:00,459
Java projects but you know depending on

00:11:58,179 --> 00:12:02,829
whether you go to Python you know you

00:12:00,459 --> 00:12:05,199
have you know your Python you paint on

00:12:02,829 --> 00:12:06,399
our condoms you have go which is you

00:12:05,199 --> 00:12:07,839
know provide you of that by hearing

00:12:06,399 --> 00:12:08,889
there's all sorts of ways you can patch

00:12:07,839 --> 00:12:12,519
things and generally its language

00:12:08,889 --> 00:12:14,799
specific so again this is this kind of

00:12:12,519 --> 00:12:17,559
our overview if you have questions about

00:12:14,799 --> 00:12:19,629
other other standards in the space I

00:12:17,559 --> 00:12:20,970
probably know I probably know about it

00:12:19,629 --> 00:12:23,079
but maybe just didn't include it here

00:12:20,970 --> 00:12:25,179
but I just want to give you because I'm

00:12:23,079 --> 00:12:26,679
mainly for brevity but if you have

00:12:25,179 --> 00:12:28,209
because I know like the I know sometimes

00:12:26,679 --> 00:12:30,009
architecture diagrams get overwhelming

00:12:28,209 --> 00:12:31,629
so if there's anything at but if you're

00:12:30,009 --> 00:12:35,019
going to zoom you know just let me know

00:12:31,629 --> 00:12:38,259
later so starting with open source

00:12:35,019 --> 00:12:40,209
frameworks so you know right now we're

00:12:38,259 --> 00:12:43,179
kind of we just had an unfortunate kind

00:12:40,209 --> 00:12:45,519
of consolidation in the space the PFN

00:12:43,179 --> 00:12:48,629
folks moved to moved from junior to pay

00:12:45,519 --> 00:12:53,379
torch so now we're down to kind of a

00:12:48,629 --> 00:12:55,149
duopoly in the the framework space you

00:12:53,379 --> 00:12:56,709
know with us kind of being more of a

00:12:55,149 --> 00:12:58,509
backing framework you know so you kind

00:12:56,709 --> 00:13:00,339
of have two ways to build to build

00:12:58,509 --> 00:13:01,899
frameworks and to build neural networks

00:13:00,339 --> 00:13:05,139
now tourism tensorflow

00:13:01,899 --> 00:13:07,720
so but what most what most I think end

00:13:05,139 --> 00:13:09,879
users don't know is that actually those

00:13:07,720 --> 00:13:11,709
are actually see based code bases so

00:13:09,879 --> 00:13:13,509
pipe arch is actually just a Python

00:13:11,709 --> 00:13:16,419
interface for the old

00:13:13,509 --> 00:13:18,909
Lulla Lulla based the the blue or

00:13:16,419 --> 00:13:20,619
Rhapsody code base from torch the same

00:13:18,909 --> 00:13:23,790
is true for tensorflow it's actually

00:13:20,619 --> 00:13:25,410
mainly a C++ library that has

00:13:23,790 --> 00:13:29,400
that has a lot of logic from Python but

00:13:25,410 --> 00:13:31,620
all the persecution happens in C++ you

00:13:29,400 --> 00:13:33,270
have and then you have you have a

00:13:31,620 --> 00:13:35,100
language specific unity of server

00:13:33,270 --> 00:13:36,780
middleware which is anything where you

00:13:35,100 --> 00:13:38,400
brain at web server or things of course

00:13:36,780 --> 00:13:40,260
you know a system language like go the

00:13:38,400 --> 00:13:42,270
kubernetes for example those routines is

00:13:40,260 --> 00:13:44,700
where things go so you have a lot of

00:13:42,270 --> 00:13:46,920
middleware Java and c-sharp kind of in

00:13:44,700 --> 00:13:48,300
the enterprise space so generally

00:13:46,920 --> 00:13:50,880
generally these generally these

00:13:48,300 --> 00:13:53,070
frameworks will have SDKs for using

00:13:50,880 --> 00:13:54,660
models from those languages we provide

00:13:53,070 --> 00:13:56,370
one like I said we provide one for

00:13:54,660 --> 00:13:58,320
attention flow for example in Java and

00:13:56,370 --> 00:13:59,910
then find that you have the you know

00:13:58,320 --> 00:14:01,920
we're actually most of our developers

00:13:59,910 --> 00:14:02,940
even though we look like we look like

00:14:01,920 --> 00:14:04,440
your job but we're actually being like

00:14:02,940 --> 00:14:07,350
again most of our developers actually C

00:14:04,440 --> 00:14:11,970
and C++ people so framework development

00:14:07,350 --> 00:14:14,670
is specialized costly and generally

00:14:11,970 --> 00:14:16,410
requires a breadth of expert of breadth

00:14:14,670 --> 00:14:18,750
of extra piece it's with expertise to

00:14:16,410 --> 00:14:20,640
get right and and and to implement right

00:14:18,750 --> 00:14:22,200
to approach to middle there you know in

00:14:20,640 --> 00:14:24,690
this case just to things like schedulers

00:14:22,200 --> 00:14:26,610
you know like spark and all that it

00:14:24,690 --> 00:14:28,440
requires a very specific specific and

00:14:26,610 --> 00:14:32,280
specialized team across multiple

00:14:28,440 --> 00:14:33,480
disciplines to to work well so sorry

00:14:32,280 --> 00:14:35,550
you're just backing a little bit detail

00:14:33,480 --> 00:14:37,890
frameworks so I mentioned Apache spark

00:14:35,550 --> 00:14:39,210
is one there's also a more traditional

00:14:37,890 --> 00:14:42,150
ones for common air data like pandas a

00:14:39,210 --> 00:14:44,280
desk so you know so there's lots of ways

00:14:42,150 --> 00:14:46,950
to run what all this called distributed

00:14:44,280 --> 00:14:50,190
jobs like running this run a large SQL

00:14:46,950 --> 00:14:52,560
query on my - three bucket there's lots

00:14:50,190 --> 00:14:54,330
of different ways you can do that those

00:14:52,560 --> 00:14:55,620
are some the public ones in Python and

00:14:54,330 --> 00:14:59,130
in schedulers

00:14:55,620 --> 00:14:59,730
so kubernetes is obviously the Linux

00:14:59,130 --> 00:15:03,420
Foundation

00:14:59,730 --> 00:15:04,140
ray is fairly new Ria's they started as

00:15:03,420 --> 00:15:07,620
we think is true

00:15:04,140 --> 00:15:10,200
it's it's it's built by the by what used

00:15:07,620 --> 00:15:12,030
to be a maja which created spark so if

00:15:10,200 --> 00:15:13,200
you're not familiar with reg it's it's

00:15:12,030 --> 00:15:15,240
kind of helping coming it's not an

00:15:13,200 --> 00:15:16,530
accommodation yet but the Linux

00:15:15,240 --> 00:15:19,500
Foundation might want to target this one

00:15:16,530 --> 00:15:21,120
just on that out there but in general

00:15:19,500 --> 00:15:22,530
life they think it'll be you know does

00:15:21,120 --> 00:15:24,390
have an open Garland's model and I think

00:15:22,530 --> 00:15:25,620
they will be I think they're there for

00:15:24,390 --> 00:15:29,070
you know what to do with it it's almost

00:15:25,620 --> 00:15:30,660
on the lab now yarn so that's that's

00:15:29,070 --> 00:15:31,230
kind of a yard of me so that's kind of

00:15:30,660 --> 00:15:33,180
old school

00:15:31,230 --> 00:15:35,190
patchy foundation it allows kind of the

00:15:33,180 --> 00:15:36,810
legacy hey dude days they're still

00:15:35,190 --> 00:15:38,280
around even if they're even if they're

00:15:36,810 --> 00:15:40,920
not growing they're still around they're

00:15:38,280 --> 00:15:42,090
still used sometimes you run you know

00:15:40,920 --> 00:15:43,560
sometimes you run these things on top of

00:15:42,090 --> 00:15:45,360
the program user you Interop with it

00:15:43,560 --> 00:15:47,610
just depends on the use case it depends

00:15:45,360 --> 00:15:50,550
on what's there so machine learning

00:15:47,610 --> 00:15:51,900
frameworks so not deep learning you know

00:15:50,550 --> 00:15:53,370
they're there there is more people

00:15:51,900 --> 00:15:55,080
earning still you know just the more

00:15:53,370 --> 00:15:56,250
standard or those the more standard

00:15:55,080 --> 00:15:57,450
models that are actually still be used

00:15:56,250 --> 00:15:58,500
by mostly in the street like random

00:15:57,450 --> 00:16:00,750
forests and things like that

00:15:58,500 --> 00:16:03,180
seeing that this key learn and XG boost

00:16:00,750 --> 00:16:06,270
deep learning frameworks tensorflow

00:16:03,180 --> 00:16:08,760
Kerris painters departing for J and then

00:16:06,270 --> 00:16:10,380
serving middleware so there's there's

00:16:08,760 --> 00:16:13,560
this does mean there's a lot of open

00:16:10,380 --> 00:16:15,720
source surfing that we're out there so

00:16:13,560 --> 00:16:17,910
there's some there's tensorflow there's

00:16:15,720 --> 00:16:19,080
ten small serving and then it's a kind

00:16:17,910 --> 00:16:21,090
of serving as well so that there's a lot

00:16:19,080 --> 00:16:23,040
of open source kind of serving

00:16:21,090 --> 00:16:24,330
middleware out there you know that

00:16:23,040 --> 00:16:25,830
there's different ways of implementing

00:16:24,330 --> 00:16:27,660
this and it's basically taking your

00:16:25,830 --> 00:16:30,690
model and exposing it as a REST API or

00:16:27,660 --> 00:16:32,580
something over over a network that's all

00:16:30,690 --> 00:16:33,780
it turns out it's you know it turns out

00:16:32,580 --> 00:16:36,270
as a hard problem especially if you're

00:16:33,780 --> 00:16:38,310
it's a safety point at the edge so for

00:16:36,270 --> 00:16:41,040
example serving them where doesn't run

00:16:38,310 --> 00:16:42,720
well on edge chips and generally

00:16:41,040 --> 00:16:44,430
generally these things also don't

00:16:42,720 --> 00:16:46,260
generally beat these things only bundle

00:16:44,430 --> 00:16:48,540
other libraries and you don't generally

00:16:46,260 --> 00:16:51,360
have integration with the serving going

00:16:48,540 --> 00:16:53,250
where plus a custom accelerator that's

00:16:51,360 --> 00:16:56,250
actually a niche case yeah but if you

00:16:53,250 --> 00:16:58,410
want to be deployed for optimal speeds

00:16:56,250 --> 00:17:00,960
and and middle latency in production

00:16:58,410 --> 00:17:02,190
turns out that you have to do more than

00:17:00,960 --> 00:17:03,360
shrink the models you actually have to

00:17:02,190 --> 00:17:05,429
integrate the file

00:17:03,360 --> 00:17:07,049
where that developers talk to you and so

00:17:05,429 --> 00:17:08,579
that's still kind of a token of an

00:17:07,049 --> 00:17:10,980
open-ended problem yeah that's actually

00:17:08,579 --> 00:17:14,670
the the main thing we work is just that

00:17:10,980 --> 00:17:16,140
part okay

00:17:14,670 --> 00:17:18,480
so here's an example of a lot of

00:17:16,140 --> 00:17:19,939
different file formats so again there

00:17:18,480 --> 00:17:22,799
there's a lot of different foundations

00:17:19,939 --> 00:17:25,410
that that where all this work kind of

00:17:22,799 --> 00:17:27,510
spans you know you have Apache girl so

00:17:25,410 --> 00:17:29,309
again this is a this is a memory file

00:17:27,510 --> 00:17:33,270
format for just data interchange between

00:17:29,309 --> 00:17:34,740
same Python and spark so that's a that

00:17:33,270 --> 00:17:36,390
was the use cases was developed with it

00:17:34,740 --> 00:17:38,910
but basically if you've ever heard of a

00:17:36,390 --> 00:17:40,530
data frame and are or even if you just

00:17:38,910 --> 00:17:43,590
know an Excel spreadsheet is with with

00:17:40,530 --> 00:17:45,570
columns with columns and then had type

00:17:43,590 --> 00:17:49,049
some kind of that you have Apache arrow

00:17:45,570 --> 00:17:52,590
it's an interchange format either the

00:17:49,049 --> 00:17:54,990
arrow Park a again those those were both

00:17:52,590 --> 00:17:57,210
developed in the Hadoop ecosystem work

00:17:54,990 --> 00:18:00,000
as well so these are all different ways

00:17:57,210 --> 00:18:01,410
of storing of storing data sometimes

00:18:00,000 --> 00:18:03,450
they're used as configuration file

00:18:01,410 --> 00:18:04,410
formats part of off further thoughts is

00:18:03,450 --> 00:18:07,350
an example of that

00:18:04,410 --> 00:18:10,049
so onyx is part about based developed by

00:18:07,350 --> 00:18:14,040
Google and then JSON I think we all know

00:18:10,049 --> 00:18:16,169
JSON is deal frameworks so it turns out

00:18:14,040 --> 00:18:18,120
you know there's there's actually a lot

00:18:16,169 --> 00:18:20,340
of different ways you can save models so

00:18:18,120 --> 00:18:20,820
there's a reporter Buffy's based file

00:18:20,340 --> 00:18:23,640
formats

00:18:20,820 --> 00:18:25,410
there's hdf5 which is Carris and then

00:18:23,640 --> 00:18:28,049
you have another one now as flatbuffers

00:18:25,410 --> 00:18:31,620
built by the guy who's who who did

00:18:28,049 --> 00:18:33,900
protocol buffers so there's n +1 file

00:18:31,620 --> 00:18:36,059
format standards each better than the

00:18:33,900 --> 00:18:38,309
last sometimes some use for legacy

00:18:36,059 --> 00:18:40,080
reasons so onyx for example picked

00:18:38,309 --> 00:18:41,549
protobuf is because it was widely

00:18:40,080 --> 00:18:44,370
adopted not because it was the best one

00:18:41,549 --> 00:18:45,780
to use and then you have other lower

00:18:44,370 --> 00:18:47,490
level you have other lower level

00:18:45,780 --> 00:18:49,980
communications on top of those file

00:18:47,490 --> 00:18:54,720
formats so in this case GRDC in around

00:18:49,980 --> 00:18:56,760
as I mentioned before so accelerators so

00:18:54,720 --> 00:18:59,520
you know you have basic so you can

00:18:56,760 --> 00:19:02,400
imagine application developers taking an

00:18:59,520 --> 00:19:04,860
open standard like onyx and then wanting

00:19:02,400 --> 00:19:07,800
to run up on a target chip and then that

00:19:04,860 --> 00:19:10,590
that ship will have their own optimized

00:19:07,800 --> 00:19:12,120
a way of running certain kinds of you

00:19:10,590 --> 00:19:14,160
know may become huge neural networks

00:19:12,120 --> 00:19:15,900
your computer vision our answer time

00:19:14,160 --> 00:19:17,580
series you know or whatever whatever

00:19:15,900 --> 00:19:22,110
optimizing libraries might be present on

00:19:17,580 --> 00:19:23,220
that metric lenders check so what you

00:19:22,110 --> 00:19:25,230
might want to do is you might want to

00:19:23,220 --> 00:19:27,330
have cross puffer remember that knows

00:19:25,230 --> 00:19:30,180
the difference between what runs well on

00:19:27,330 --> 00:19:32,280
Intel it runs well on arm so that way

00:19:30,180 --> 00:19:34,560
you as a developer can is up let's say

00:19:32,280 --> 00:19:35,730
an onyx model and then they know you're

00:19:34,560 --> 00:19:37,980
going to get that performance on

00:19:35,730 --> 00:19:39,840
whatever chip you know one of the

00:19:37,980 --> 00:19:41,460
biggest so the reason I bring this up is

00:19:39,840 --> 00:19:43,260
because one of the biggest problems in

00:19:41,460 --> 00:19:45,330
most I think most people aren't from

00:19:43,260 --> 00:19:47,730
there with us today and the Onyx people

00:19:45,330 --> 00:19:49,880
should talk about is this idea of pop

00:19:47,730 --> 00:19:52,140
coverage so as I mentioned before

00:19:49,880 --> 00:19:54,030
certain certain chips will wind

00:19:52,140 --> 00:19:55,710
influence certain operations because

00:19:54,030 --> 00:19:57,540
they're targeted certain use cases

00:19:55,710 --> 00:19:59,640
sonar whenever you hear about tensorflow

00:19:57,540 --> 00:20:02,070
tensorflow actually itself is a

00:19:59,640 --> 00:20:04,290
fragmented thing that runs differently

00:20:02,070 --> 00:20:06,030
on a different chips most people don't

00:20:04,290 --> 00:20:08,010
know that so for example if you run on

00:20:06,030 --> 00:20:09,270
the TPU they actually open up only a

00:20:08,010 --> 00:20:12,180
small subset of tensorflow

00:20:09,270 --> 00:20:15,030
TF white is a small subset of tensorflow

00:20:12,180 --> 00:20:16,710
niche is for mobile rightly so just

00:20:15,030 --> 00:20:18,150
don't always implement the latest

00:20:16,710 --> 00:20:19,950
architectures either so for example if

00:20:18,150 --> 00:20:21,720
you go if you go to use the coop DNN

00:20:19,950 --> 00:20:24,810
from nvidia they don't generally have

00:20:21,720 --> 00:20:26,190
the latest CNN architectures either so

00:20:24,810 --> 00:20:28,380
this is a fairly common problem in the

00:20:26,190 --> 00:20:29,820
space the so these highly optimized

00:20:28,380 --> 00:20:31,710
libraries are generally behind the

00:20:29,820 --> 00:20:33,060
cutting edge so it's hard to build

00:20:31,710 --> 00:20:34,410
something general-purpose and it's no

00:20:33,060 --> 00:20:36,420
you know that's not their fault you know

00:20:34,410 --> 00:20:38,160
you have you have you know you have a

00:20:36,420 --> 00:20:39,840
lot of time being being spent on these

00:20:38,160 --> 00:20:41,340
things and so you need to pick carefully

00:20:39,840 --> 00:20:43,830
what what you put in a roadmap to

00:20:41,340 --> 00:20:46,680
optimize so I'm having something general

00:20:43,830 --> 00:20:48,930
purpose that knows what to use where can

00:20:46,680 --> 00:20:50,760
be really helpful and so then just this

00:20:48,930 --> 00:20:52,620
integrated list this may be integrating

00:20:50,760 --> 00:20:54,720
these things is is it actually still a

00:20:52,620 --> 00:20:55,860
hard problem because you have you know

00:20:54,720 --> 00:20:56,590
you kind of have a lot of things

00:20:55,860 --> 00:20:59,230
happening in the

00:20:56,590 --> 00:21:01,539
and then they build the model but then

00:20:59,230 --> 00:21:03,159
it turns out that the without a lot of

00:21:01,539 --> 00:21:03,639
extra free processing so for example

00:21:03,159 --> 00:21:05,980
tensorflow

00:21:03,639 --> 00:21:07,899
in order to actually deploy a tensor

00:21:05,980 --> 00:21:09,399
flow model there's a lot of there's

00:21:07,899 --> 00:21:12,279
actually a water pre-processing you have

00:21:09,399 --> 00:21:13,990
to do most people don't know that so you

00:21:12,279 --> 00:21:16,240
actually need to massage with what we

00:21:13,990 --> 00:21:18,159
call massaging the graph so to speak the

00:21:16,240 --> 00:21:20,529
graph is kind of the the the instruction

00:21:18,159 --> 00:21:22,539
set so it's a the like you need to add

00:21:20,529 --> 00:21:24,129
and subtract and multiply that's a graph

00:21:22,539 --> 00:21:25,659
essentially it's just a list of

00:21:24,129 --> 00:21:28,720
operations that you need to run in order

00:21:25,659 --> 00:21:32,200
to perform some target operation in AI

00:21:28,720 --> 00:21:34,539
as we call it so also in this case it

00:21:32,200 --> 00:21:36,669
turns out when you call a TFSA you

00:21:34,539 --> 00:21:38,139
procure a save there's actually a lot of

00:21:36,669 --> 00:21:39,999
stuff in there and actually it won't run

00:21:38,139 --> 00:21:42,369
so just because you can save the model

00:21:39,999 --> 00:21:43,480
doesn't mean it'll run properly so

00:21:42,369 --> 00:21:45,279
they're like I said there's a lot of

00:21:43,480 --> 00:21:46,929
noise and then depending on if you want

00:21:45,279 --> 00:21:48,669
to run it on a different chip or not you

00:21:46,929 --> 00:21:49,869
might need to modify the model you might

00:21:48,669 --> 00:21:52,749
need to the puzzle process up in some

00:21:49,869 --> 00:21:54,700
form so that post-processing work is a

00:21:52,749 --> 00:21:56,289
black art to most to most people who

00:21:54,700 --> 00:21:58,899
build models that stuff is a black art

00:21:56,289 --> 00:22:00,490
and it's not clear it's not clear what

00:21:58,899 --> 00:22:02,200
would work in one scenario so sometimes

00:22:00,490 --> 00:22:03,759
you might have one model you build for

00:22:02,200 --> 00:22:04,899
one scenario and then I did completely

00:22:03,759 --> 00:22:06,909
different model that sings of the

00:22:04,899 --> 00:22:08,649
surface of saying these days but maybe

00:22:06,909 --> 00:22:10,779
in what you're doing is you want to make

00:22:08,649 --> 00:22:13,960
accuracy and you might begin to see

00:22:10,779 --> 00:22:15,580
trade-offs and just so it can run most

00:22:13,960 --> 00:22:17,470
again most people don't know that so

00:22:15,580 --> 00:22:20,559
that that integration problem and

00:22:17,470 --> 00:22:22,899
actually making models run especially at

00:22:20,559 --> 00:22:24,460
the accelerator level it's not a wall

00:22:22,899 --> 00:22:26,619
it's not a wall so a problem yet most

00:22:24,460 --> 00:22:29,590
people just so any any time you see a

00:22:26,619 --> 00:22:31,119
name a company I vendor what they do is

00:22:29,590 --> 00:22:32,409
they just use other people's software

00:22:31,119 --> 00:22:34,330
and then rapid kubernetes

00:22:32,409 --> 00:22:35,889
that's what most people do and then they

00:22:34,330 --> 00:22:37,149
say they did their own thing actually

00:22:35,889 --> 00:22:38,080
the biggest one with a bunch of stuff

00:22:37,149 --> 00:22:39,909
and they if they don't know how the

00:22:38,080 --> 00:22:41,379
underlying silicon works and that's

00:22:39,909 --> 00:22:43,480
actually a big problem if we're going to

00:22:41,379 --> 00:22:45,519
deploy models at the edge and have it

00:22:43,480 --> 00:22:47,860
actually kind of go after this

00:22:45,519 --> 00:22:49,149
ubiquitous computing problem you need to

00:22:47,860 --> 00:22:49,840
know how this stuff works you can't

00:22:49,149 --> 00:22:51,700
handle a ver

00:22:49,840 --> 00:22:53,499
so you need to work with the community

00:22:51,700 --> 00:22:54,909
you need to work of a lot of these a lot

00:22:53,499 --> 00:22:56,020
of these efforts and that's that's

00:22:54,909 --> 00:22:57,520
essentially what we're trying to

00:22:56,020 --> 00:23:00,730
just trance off the integration problem

00:22:57,520 --> 00:23:01,900
this is the only thing I do so if you if

00:23:00,730 --> 00:23:05,590
you have any questions about that please

00:23:01,900 --> 00:23:08,050
do let me know and I'll end with metrics

00:23:05,590 --> 00:23:09,429
so you know these these are Linux

00:23:08,050 --> 00:23:11,800
Foundation practice so these are Linux

00:23:09,429 --> 00:23:14,170
innovation projects or widely used open

00:23:11,800 --> 00:23:17,559
source you know kind of visualization

00:23:14,170 --> 00:23:19,780
based tools you know so one thing I want

00:23:17,559 --> 00:23:21,610
to say is you want to you know depending

00:23:19,780 --> 00:23:24,100
on whether you're implementing different

00:23:21,610 --> 00:23:26,470
kinds of you know model accuracy f-word

00:23:24,100 --> 00:23:28,000
a model biased at work whenever you do

00:23:26,470 --> 00:23:29,890
that you typically want to use open

00:23:28,000 --> 00:23:32,200
standards so it turns out Prometheus and

00:23:29,890 --> 00:23:34,150
the a lot of the things in the Linux

00:23:32,200 --> 00:23:36,610
ecosystem are actually very good for

00:23:34,150 --> 00:23:38,620
this and so you know we typically think

00:23:36,610 --> 00:23:40,270
at least for DevOps but what if you have

00:23:38,620 --> 00:23:42,880
the same kind of DevOps tile that word

00:23:40,270 --> 00:23:45,100
for a model suffer from for modeling

00:23:42,880 --> 00:23:46,179
accuracy and bias and then what if you

00:23:45,100 --> 00:23:50,200
get a scientist could then use that

00:23:46,179 --> 00:23:51,429
output may see and say okay you know

00:23:50,200 --> 00:23:54,700
this is the real world version this is

00:23:51,429 --> 00:23:56,410
how what we're doing otherwise otherwise

00:23:54,700 --> 00:23:58,510
you know we're okay so they would never

00:23:56,410 --> 00:23:59,830
know great because one of the things

00:23:58,510 --> 00:24:01,270
that data you know because it's because

00:23:59,830 --> 00:24:03,400
data saying this week one thing out

00:24:01,270 --> 00:24:05,380
there is they want to build the latest

00:24:03,400 --> 00:24:06,720
model but it doesn't necessarily mean

00:24:05,380 --> 00:24:08,740
it's gonna work in their word world

00:24:06,720 --> 00:24:11,320
there's an you know machine learning

00:24:08,740 --> 00:24:13,660
models they also have technical debt the

00:24:11,320 --> 00:24:15,910
world changes what do you do in that

00:24:13,660 --> 00:24:18,070
habits you know so that you know small

00:24:15,910 --> 00:24:19,690
example that could be from detection you

00:24:18,070 --> 00:24:22,720
know so frog be a fraud protection

00:24:19,690 --> 00:24:23,890
changes every day so what do you do but

00:24:22,720 --> 00:24:25,690
you might build machine learning models

00:24:23,890 --> 00:24:27,160
to detect this kind of fraud but you

00:24:25,690 --> 00:24:29,410
almost always need to retrain the models

00:24:27,160 --> 00:24:31,720
because again the world changes so this

00:24:29,410 --> 00:24:33,520
is called this is this is this is

00:24:31,720 --> 00:24:35,770
compound one one or one way to describe

00:24:33,520 --> 00:24:37,510
this is something from concept drift so

00:24:35,770 --> 00:24:39,700
you have the world the root world is one

00:24:37,510 --> 00:24:41,530
concept and then what your model thinks

00:24:39,700 --> 00:24:43,330
is the real world is another one of

00:24:41,530 --> 00:24:48,130
those two things differ how do you know

00:24:43,330 --> 00:24:50,730
you visualize it and with that I'll take

00:24:48,130 --> 00:24:50,730

YouTube URL: https://www.youtube.com/watch?v=DKVXxh_NtJI


