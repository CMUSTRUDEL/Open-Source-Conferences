Title: !!Con West 2020: Naomi Saphra - Get Hooked On Neural Net Inspection! That was a pun!
Publication date: 2020-03-23
Playlist: !!Con West 2020
Description: 
	Presented at !!Con West 2020: http://bangbangcon.com/west

Sometimes you want to find out some information about a neural net in PyTorch while it runs. Like what if you wanted to do some math of your own on the math that happens inside the network? You can do that with hooks! Hooks can go on each module of a neural network to spy on the black box and whisper its secrets to you. No neural net will keep its secrets hidden from your perceptive functions.


#bangbangcon #bangbangconwest #bangbangconwest2020
Captions: 
	00:00:27,260 --> 00:00:27,980
Okay.

00:00:27,980 --> 00:00:31,920
So first, let me talk about myself for just a second.

00:00:32,660 --> 00:00:38,200
I'm a current PhD student in natural language processing at the University of Edinburgh

00:00:38,200 --> 00:00:42,260
in Scotland, which has its own strike going on right now.

00:00:42,260 --> 00:00:44,120
Solidarity!

00:00:44,120 --> 00:00:46,400
(applause)

00:00:47,720 --> 00:00:50,240
I'm currently in training at Google.

00:00:50,240 --> 00:00:51,290
Temporarily.

00:00:51,290 --> 00:00:52,530
In New York.

00:00:52,530 --> 00:00:58,860
I dictate all my code, because due to a disability, it's hard for me to type much.

00:00:58,860 --> 00:01:07,750
My roller derby name is Gaussian Retribution, and my Twitter handle is @nsaphra.

00:01:07,750 --> 00:01:10,180
I just got a tonsillectomy a few days ago.

00:01:10,180 --> 00:01:18,010
So usually my voice sounds authoritative but alluring, like a sultry James Earl Jones but

00:01:18,010 --> 00:01:22,040
today I sound like a Muppet.

00:01:22,040 --> 00:01:23,520
Alright.

00:01:23,520 --> 00:01:27,680
So there are a lot of kinds of deep neural networks, right?

00:01:27,680 --> 00:01:32,390
So the simplest kind, the first thing I'll introduce you to is a feed forward neural

00:01:32,390 --> 00:01:33,390
network.

00:01:33,390 --> 00:01:37,780
So you have an input x, that is some vector, that goes into some function that involves

00:01:37,780 --> 00:01:43,180
some matrix multiplication followed by some non-linear thing... output from that goes

00:01:43,180 --> 00:01:49,200
into some other function, that's a different module, and eventually you get generally like a prediction

00:01:49,200 --> 00:01:50,200
vector, Å·.

00:01:50,200 --> 00:01:52,040
Erm.

00:01:52,040 --> 00:01:54,080
There are other kinds, like recurrent.

00:01:54,080 --> 00:01:55,990
Recurrent networks are like...

00:01:55,990 --> 00:01:59,500
You can iterate over a bunch of items in a sequence.

00:01:59,500 --> 00:02:06,300
Because the same module gets repeatedly applied to each item in that sequence.

00:02:06,300 --> 00:02:07,460
Okay.

00:02:10,000 --> 00:02:13,260
They can work in parallel.

00:02:13,260 --> 00:02:16,000
They have like...

00:02:16,000 --> 00:02:18,280
Generally x is not actually gonna be a vector.

00:02:18,290 --> 00:02:23,799
It's actually gonna be like a matrix because... it's processing a bunch of inputs in parallel.

00:02:23,799 --> 00:02:24,939
So.

00:02:24,939 --> 00:02:30,079
Erm, generally you have your forward pass, which is what happens during inference, when

00:02:30,079 --> 00:02:31,080
you're trying to produce output.

00:02:31,080 --> 00:02:32,489
So it goes in.

00:02:32,489 --> 00:02:34,879
It goes up the computation graph.

00:02:34,879 --> 00:02:36,650
And produces your output.

00:02:36,650 --> 00:02:41,500
And then when you're training, you're taking the derivative of the error, and you're passing

00:02:41,500 --> 00:02:46,519
it back through each module, so you're training the weights of the matrices that are inside

00:02:46,519 --> 00:02:48,519
those modules.

00:02:48,520 --> 00:02:50,200
Down to the bottom.

00:02:50,200 --> 00:02:50,840
So.

00:02:52,420 --> 00:02:57,980
I don't really get what's happening, you know, during my training or inference time.

00:02:57,980 --> 00:03:00,019
Like, while it's running, right?

00:03:00,019 --> 00:03:02,159
I want to see what's happening with the representations.

00:03:02,159 --> 00:03:06,650
Maybe I want to save some heat maps of the different activations.

00:03:06,650 --> 00:03:09,090
Maybe I want to look at the concentration of gradients.

00:03:09,090 --> 00:03:13,230
Which is something where like it indicates whether your network is kind of memorizing

00:03:13,230 --> 00:03:16,980
as opposed to learning a general function.

00:03:16,980 --> 00:03:21,870
Maybe I just want to see the magnitude of the error at each module.

00:03:21,870 --> 00:03:27,059
But all I have is inputs and some PyTorch modules that are trained or not.

00:03:27,059 --> 00:03:28,059
So...

00:03:28,060 --> 00:03:29,100
What am I gonna do?

00:03:29,180 --> 00:03:30,040
Hooks!

00:03:30,360 --> 00:03:31,560
Yeah!

00:03:32,480 --> 00:03:38,370
So what happens with a hook is it's a function that you associate with a particular module.

00:03:38,370 --> 00:03:45,379
So that when the actual function of that module gets run, it simultaneously passes its inputs

00:03:45,379 --> 00:03:47,480
and outputs to the hook.

00:03:47,480 --> 00:03:50,389
So this is what would happen during a forward pass.

00:03:50,389 --> 00:03:52,219
With a forward hook... right?

00:03:52,219 --> 00:03:57,989
And this is what would happen during a backward pass with a backward hook.

00:03:57,989 --> 00:03:59,459
So...

00:03:59,459 --> 00:04:00,459
You can...

00:04:00,459 --> 00:04:05,530
So in this case, the gradients or derivatives are actually getting passed.

00:04:05,530 --> 00:04:12,860
Both the gradient at the input and the gradient at the output, are both getting passed to the hook.

00:04:12,940 --> 00:04:13,940
So it gets both inputs.

00:04:14,300 --> 00:04:23,220
But that's actually a lot of, basically, raw data that the hook is getting passed, and

00:04:23,230 --> 00:04:25,980
the type of that data is just like...

00:04:25,980 --> 00:04:28,440
Ugh the matrix!

00:04:28,440 --> 00:04:32,380
Uh, tensor that's got this dimension and this dimension and this dimension!

00:04:32,380 --> 00:04:35,000
And you have no idea what any of them are doing.

00:04:35,000 --> 00:04:36,220
Usually.

00:04:36,420 --> 00:04:44,500
So I have a little trick that I use, which is that I set every single hyperparameter

00:04:44,510 --> 00:04:48,020
associated with some dimension to a prime number.

00:04:48,020 --> 00:04:53,470
So even if two of them get collapsed, if you've set something to 3 and something to 7, and

00:04:53,470 --> 00:05:00,650
then you get some kind of input, that's, like, dimension 21, you know where that number came

00:05:00,650 --> 00:05:01,650
from!

00:05:01,650 --> 00:05:04,160
You can just reshape it accordingly.

00:05:04,160 --> 00:05:08,520
So in this case, you can tell where each of the...

00:05:08,520 --> 00:05:11,330
Like, this is like -- I'm just using a dummy hook.

00:05:11,330 --> 00:05:14,570
The dummy hook that I wrote just prints out the actual...

00:05:14,570 --> 00:05:20,400
You know, information about the input and the output types.

00:05:20,400 --> 00:05:25,390
And you can tell what hyperparameters are associated with what types.

00:05:25,390 --> 00:05:34,240
Erm, but then it's like... uh! I've got that, like, transformer model from the beginning.

00:05:34,240 --> 00:05:36,720
There's so many modules.

00:05:36,720 --> 00:05:41,000
And am I really gonna have to add a hook for every single one of them?

00:05:42,060 --> 00:05:43,320
Erm, no.

00:05:43,320 --> 00:05:45,900
You can just add them recursively, actually.

00:05:46,600 --> 00:05:52,060
You've got this little function named_children(), so named_children() is just going to find

00:05:52,070 --> 00:05:57,750
you all the child modules of a particular module, and you can just recursively go through

00:05:57,750 --> 00:06:00,880
the entire computation graph like that.

00:06:02,820 --> 00:06:07,560
And that's what I like to do, because I don't like to manually associate a hook with each

00:06:07,560 --> 00:06:08,560
module.

00:06:08,560 --> 00:06:10,150
All right.

00:06:10,150 --> 00:06:17,100
I'm gonna just end by talking about the grossest thing I've ever done with a hook.

00:06:17,100 --> 00:06:24,760
Which is... that... uh, there's this model, long short-term memory networks, LSTMs...

00:06:24,760 --> 00:06:29,260
PyTorch makes it completely opaque what's going on inside, but I really wanted to look at what

00:06:29,270 --> 00:06:34,570
was happening in the gating mechanisms, but I didn't actually trust myself to write an

00:06:34,570 --> 00:06:40,090
LSTM module that was definitely going to give the same outputs as the real one.

00:06:40,090 --> 00:06:49,830
So I just wrote, like, an LSTM module that ran at the same time as the real LSTM module,

00:06:49,830 --> 00:06:54,060
and just asserted that the outputs were the same.

00:06:54,060 --> 00:06:56,690
And that's the most disgusting thing I've ever done with hooks.

00:06:56,690 --> 00:06:58,260
That's my confession.

00:06:59,620 --> 00:07:00,820
And actually, that's it!

00:07:02,020 --> 00:07:02,920
Ha ha!

00:07:02,920 --> 00:07:05,700

YouTube URL: https://www.youtube.com/watch?v=H17OQk7UeFM


