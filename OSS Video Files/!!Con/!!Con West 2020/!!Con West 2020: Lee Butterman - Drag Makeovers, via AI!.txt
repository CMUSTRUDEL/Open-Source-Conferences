Title: !!Con West 2020: Lee Butterman - Drag Makeovers, via AI!
Publication date: 2020-03-23
Playlist: !!Con West 2020
Description: 
	Presented at !!Con West 2020: http://bangbangcon.com/west

A new project, StyleGAN, represents faces in high-fidelity in 9000 dimensions, and can produce high-quality interpolation between those faces! They even provide pre-trained models on 70k faces from Flickr! These 9000 dimensions represent everything from facial pose to hair styles to background to skin tone and more. We can try to find encodings of several drag queens, in drag and out of drag, and use those differences to interpolate our own faces. This is available online as well, with your webcam, at https://leebutterman.com/drag-makeover.


#bangbangcon #bangbangconwest #bangbangconwest2020
Captions: 
	00:00:27,100 --> 00:00:35,040
I'm super excited to be here to share some adventures I've been doing in drag makeovers,

00:00:35,040 --> 00:00:38,060
using facial models in AI.

00:00:38,060 --> 00:00:44,980
And they're all using the facial model StyleGAN.

00:00:44,980 --> 00:00:52,180
StyleGAN and Flickr Face's high quality dataset powers thispersondoesnotexist.com.

00:00:52,180 --> 00:00:54,540
Which you may have seen.

00:00:54,540 --> 00:01:05,480
It's quality faces generated from a dataset of 70,000 people.

00:01:05,480 --> 00:01:13,260
And they're all realistic, and they're all generated kind of on-demand.

00:01:13,420 --> 00:01:19,180
The ecosystem of StyleGAN is flexible enough to support drag queen makeovers out of box.

00:01:19,820 --> 00:01:24,560
There's a publicly available demo of it.

00:01:24,560 --> 00:01:27,080
It took a lot of work to do so.

00:01:27,090 --> 00:01:31,259
maybe it's worth thinking about ways to make it easier.

00:01:31,260 --> 00:01:41,840
StyleGAN uses some ideas from Style Transfer and kind of progressively growing networks to separate

00:01:41,840 --> 00:01:45,560
generating backgrounds of images from foregrounds of images.

00:01:45,560 --> 00:01:50,520
If you're doing faces, you can separate a blurry background from like your eyes and

00:01:50,520 --> 00:01:52,180
your teeth and what-not.

00:01:52,189 --> 00:01:58,450
And you can smoothly interpolate in the 9000-dimensional space it works in.

00:01:58,450 --> 00:02:07,939
All of these faces are somewhere close to the zero face in this 9000-dimensional space.

00:02:07,939 --> 00:02:09,540
The code is available.

00:02:09,540 --> 00:02:12,910
The training data is available from Flickr.

00:02:12,910 --> 00:02:15,840
There are several pretrained models available.

00:02:15,840 --> 00:02:22,600
And if you have a week or so on a machine that costs $25 an hour, you too can make your

00:02:22,600 --> 00:02:24,140
own models.

00:02:24,140 --> 00:02:30,960
If you want to generate your own training data, it's even more expensive.

00:02:32,760 --> 00:02:40,720
And the idea is that it comes from 70,000 faces, extracted from Flickr.

00:02:40,720 --> 00:02:43,830
They're all 1024x1024.

00:02:43,830 --> 00:02:52,730
They get aligned and cropped in various ways with statues and paintings and photos pruned

00:02:52,730 --> 00:02:54,080
out.

00:02:54,080 --> 00:02:59,010
So there definitely is a bias in that.

00:02:59,010 --> 00:03:05,210
But they say it contains considerable variation in terms of age, ethnicity, and image background.

00:03:05,210 --> 00:03:12,310
There's a good coverage of accessories, such as eyeglasses, sunglasses, hats, et cetera.

00:03:12,310 --> 00:03:14,560
These are seven sample images.

00:03:14,560 --> 00:03:20,480
But accessories are more than just a pair of glasses.

00:03:20,480 --> 00:03:28,040
The idea is that when you're encoding, you crop an image to a face.

00:03:28,040 --> 00:03:33,820
You choose a loss function, as we saw in the last talk.

00:03:33,820 --> 00:03:39,110
You have to gauge where you are with where you want to be.

00:03:39,110 --> 00:03:45,360
Is it just have your parameters explode to a prime number close to infinity?

00:03:45,360 --> 00:03:47,960
Is it the just sheer pixel difference?

00:03:47,960 --> 00:03:50,050
Is it some statistical model?

00:03:50,050 --> 00:03:57,400
And once you know how to improve, there are ways to optimize, where you are.

00:03:57,400 --> 00:04:06,150
And the zero face that you want to, that a lot of facial models kind of hem towards,

00:04:06,150 --> 00:04:17,030
a lot of the models from thispersondoesnotexist keep pretty close to the zero face.

00:04:17,030 --> 00:04:22,450
It's possible to get kind of glitch art, if you go too far off.

00:04:22,450 --> 00:04:25,870
But you also don't want to stay too close to that.

00:04:25,870 --> 00:04:28,460
Maybe you want something more exciting.

00:04:28,460 --> 00:04:35,750
Maybe you want Ezra Miller's seven eyes from the Met Gala in 2019.

00:04:35,750 --> 00:04:44,520
Maybe you need Dana Martina's adversarial poinsettia fascinator with industrial holiday

00:04:44,520 --> 00:04:47,370
cheer gift bows.

00:04:47,370 --> 00:04:50,200
That didn't register as a face.

00:04:50,200 --> 00:04:52,430
Even though we can all see it is.

00:04:52,430 --> 00:05:03,720
So I attempted to compile some faces of reasonable photography subjects that seemed worth including.

00:05:03,720 --> 00:05:07,200
PRs welcome.

00:05:07,200 --> 00:05:15,700
And I attempted to encode all of those, and I plotted the loss over time of encoding it,

00:05:15,700 --> 00:05:20,030
compared to encoding that zero face.

00:05:20,030 --> 00:05:28,510
There are three selfies I took with just like a webcam in my office.

00:05:28,510 --> 00:05:36,270
Just like waking up in the morning, and then there's Billy Porter as the Sun God.

00:05:36,270 --> 00:05:38,230
With more of a loss.

00:05:38,230 --> 00:05:40,680
You can see he's resplendent.

00:05:40,680 --> 00:05:43,139
So that's interesting.

00:05:43,139 --> 00:05:48,880
Because it is proportional to how well you conform to the training data.

00:05:48,880 --> 00:05:49,880
Which is great.

00:05:49,880 --> 00:05:55,400
If you feel like going out and being a bit more genderqueer, you can take a selfie and see like

00:05:55,520 --> 00:06:01,940
This is too close to the average of 70,000 people on Flickr!

00:06:01,940 --> 00:06:06,889
This is a way to just shift it up a bit.

00:06:06,889 --> 00:06:08,550
If you're in control.

00:06:08,550 --> 00:06:13,680
If you're not in control, if that runs on a CC-TV, this is a bad idea.

00:06:13,680 --> 00:06:17,639
But either way, running it is expensive.

00:06:17,640 --> 00:06:26,190
And it's tough to model a Sun God look, Bob the Drag Queen's blue wig, hoop earrings and

00:06:26,190 --> 00:06:36,240
a snarl, a smoking water pistol, and what-not, and there are ways of kind of seeing how the

00:06:36,250 --> 00:06:45,240
end product maps to the beginning, and you can see ways that the encoding progresses

00:06:45,240 --> 00:06:46,240
over time.

00:06:46,240 --> 00:06:51,210
And it makes more sense if you see it kind of exponentially sped up.

00:06:51,210 --> 00:06:55,020
Because you need to run it longer than you would expect.

00:06:55,020 --> 00:07:02,470
Just to converge to something that resembles to what a human observer would consider to

00:07:02,470 --> 00:07:08,000
be a reasonable encoding.

00:07:08,460 --> 00:07:18,840
So the point of this is that there's a lot of ability to do vector space manipulations.

00:07:18,840 --> 00:07:22,449
The original StyleGAN encoder I used had three dimensions.

00:07:22,449 --> 00:07:23,449
One was age.

00:07:23,449 --> 00:07:26,290
You add age and it becomes younger.

00:07:26,290 --> 00:07:31,639
You add gender and it becomes more butch, and you add smile.

00:07:31,639 --> 00:07:36,550
No one really needs to have a computer tell their selfie to smile.

00:07:36,550 --> 00:07:38,520
And all of those are straightforward enough.

00:07:38,520 --> 00:07:44,620
But why not try a dimension of a performer into and out of a costume?

00:07:44,620 --> 00:07:47,180
So why not add a drag makeup tutorial?

00:07:47,180 --> 00:07:55,330
Furthermore, if you have computationally defined what a makeover is, you can say, "How do I

00:07:55,330 --> 00:07:58,080
look out of the drag I've got on?"

00:07:58,080 --> 00:08:00,720
So you can go in the negative dimension.

00:08:00,720 --> 00:08:03,210
Which is not exactly intuitive.

00:08:03,210 --> 00:08:04,810
But that's how that goes.

00:08:04,810 --> 00:08:06,380
So here I am.

00:08:06,380 --> 00:08:09,350
Doing a Trixie Mattel look.

00:08:09,350 --> 00:08:11,550
Going into and out of drag.

00:08:11,550 --> 00:08:17,530
I've got like a C3PO zinc robot fantasy knockoff.

00:08:17,530 --> 00:08:25,060
And either way, I've got kind of a Chernobyl-like skin care routine, when I'm going out of drag.

00:08:25,060 --> 00:08:32,690
But it's not immediately clear, seeing this, that what I'm doing is going into a Bob the

00:08:32,690 --> 00:08:33,789
Drag Queen look.

00:08:33,789 --> 00:08:44,640
And what we can do is we can take a... we can take a...

00:08:44,640 --> 00:08:52,460
We can take a selfie that I just did a moment ago.

00:08:52,460 --> 00:08:57,180
And that is a zero face of StyleGAN 2.

00:08:57,199 --> 00:08:58,199
And it is uploading.

00:08:58,200 --> 00:09:02,779
As far as I'm aware.

00:09:04,040 --> 00:09:08,100
Doing a thing.

00:09:08,100 --> 00:09:15,579
So that is... up, not, well, all right.

00:09:15,580 --> 00:09:19,720
The demo gods are not with me.

00:09:20,920 --> 00:09:22,000
Let's get...

00:09:34,640 --> 00:09:38,040
There we are.

00:09:38,040 --> 00:09:40,029
So I've got a selfie.

00:09:40,029 --> 00:09:46,259
I've got the dimension from Bryan Fergus to Trixie Mattel.

00:09:46,259 --> 00:09:50,869
That is the same human being in a lot of makeup.

00:09:50,869 --> 00:09:53,480
And I'm encoding right now.

00:09:53,480 --> 00:09:56,439
You can see it kind of progressing slowly.

00:09:56,439 --> 00:09:58,350
This is me going out of drag.

00:09:58,350 --> 00:10:02,260
This is me going into drag.

00:10:02,260 --> 00:10:06,579
It's not exactly prime time quality.

00:10:06,579 --> 00:10:10,050
But you can also see that there are...

00:10:10,050 --> 00:10:13,220
That it gets kind of glitch-art, if you go too far.

00:10:13,220 --> 00:10:23,439
If I go 110%, there you've got Chernobyl skin care routine and you've got, hah, turning

00:10:23,439 --> 00:10:24,689
into plasma.

00:10:24,689 --> 00:10:29,790
So don't turn into plasma if you can avoid it.

00:10:29,790 --> 00:10:33,949
It's not very sanitary.

00:10:33,949 --> 00:10:46,410
(laughter) So there are a lot of uses for this, as kind of fun glitch art ways to do

00:10:46,410 --> 00:10:49,720
a makeover.

00:10:49,720 --> 00:10:53,880
It's also worth thinking about kind of larger scale impact, though.

00:10:53,880 --> 00:10:56,380
Because this is all fun and games.

00:10:56,380 --> 00:11:00,649
But it takes a while to curate a dataset, to provision GPU machines, and the GPU machines

00:11:00,649 --> 00:11:02,019
are super expensive.

00:11:02,019 --> 00:11:04,670
To tweak all the parameters.

00:11:04,670 --> 00:11:08,300
And this is not just for computer vision.

00:11:08,300 --> 00:11:14,920
We've invented 180 voices in Google Cloud text-to-speech, and they're either male or

00:11:14,920 --> 00:11:15,920
female.

00:11:15,920 --> 00:11:19,190
And the California State ID is more gender diverse!

00:11:19,190 --> 00:11:21,240
How often does that happen?

00:11:21,240 --> 00:11:23,360
Compared to like...

00:11:23,360 --> 00:11:25,580
Voices cooked out of thin air?!

00:11:25,589 --> 00:11:33,509
So rather than doing homophobia whack-a-mole, why not analyze the field as a system, and

00:11:33,509 --> 00:11:40,449
Donella Meadows in '97 published 12 leverage points of kind of places to intervene in a

00:11:40,449 --> 00:11:44,579
system with varying amounts of leverage.

00:11:44,579 --> 00:11:47,830
So the first one -- you just kind of tweak the hyperparameters.

00:11:47,830 --> 00:11:50,759
You like encode it for twice as long.

00:11:50,759 --> 00:11:54,639
You change your models in this and that.

00:11:54,639 --> 00:11:55,899
And that's effective.

00:11:55,899 --> 00:11:58,980
But not as effective as tweaking the buffer sizes.

00:11:58,980 --> 00:12:01,699
So build new facial models.

00:12:01,699 --> 00:12:04,839
Build new notebooks with more inclusive training data.

00:12:04,839 --> 00:12:05,839
Which is effective.

00:12:05,839 --> 00:12:12,869
But not as effective as varying the structure of material stocks and flows.

00:12:12,869 --> 00:12:16,329
Using diverse datasets, or having diverse hiring practices.

00:12:16,329 --> 00:12:20,639
Which is effective, but not as effective as the delay per rate of change.

00:12:20,639 --> 00:12:28,929
So run a continuous integration benchmark to assess kind of how inclusive your encoders

00:12:28,929 --> 00:12:29,929
are.

00:12:29,929 --> 00:12:33,649
Which is effective, but not as effective as, like, slowing negative feedback loops, and

00:12:33,649 --> 00:12:35,899
slowing positive feedback loops.

00:12:35,899 --> 00:12:44,579
Final PRs to make your AI more inclusive, and fix bugs and add features in inclusive

00:12:44,579 --> 00:12:48,589
projects, but not to the exclusion of other projects.

00:12:48,589 --> 00:12:56,069
Which is effective, but not as effective as structuring information flow, so raising awareness of systems

00:12:56,069 --> 00:13:01,189
of issues of inclusion, which is effective, but not as effective as changing the rules

00:13:01,189 --> 00:13:02,620
of the system.

00:13:02,620 --> 00:13:09,800
So like amplifying voices raising awareness, or putting on some adversarial fashion, to

00:13:09,800 --> 00:13:19,240
insert junk snippets from the 4th Amendment into license plate scanners, driving around.

00:13:19,240 --> 00:13:24,240
So step 5 and step 4... 6 and 5 are what we're doing right now.

00:13:25,500 --> 00:13:28,140
4. Kind of organize system structure.

00:13:28,140 --> 00:13:30,509
So make a culture of open data.

00:13:30,509 --> 00:13:32,350
Have reproducible results.

00:13:32,350 --> 00:13:35,949
And be aware of how things go.

00:13:35,949 --> 00:13:37,910
And change the goal of the system.

00:13:37,910 --> 00:13:41,220
Like, maybe AI should be more inclusive and ethical.

00:13:41,220 --> 00:13:48,760
Which is effective, but not as effective as break down the mindset and paradigms of the system

00:13:48,840 --> 00:13:52,160
Challenge anomalies and failures of assumptions.

00:13:52,160 --> 00:13:55,660
Like where does the training data come from?

00:13:55,670 --> 00:13:59,559
Are human benchmarks the best benchmarks?

00:13:59,559 --> 00:14:01,040
GNU Freedom 0.

00:14:01,040 --> 00:14:06,500
Should you have the freedom to run the program for any purpose on a drone with a gun?

00:14:06,500 --> 00:14:07,540
How about sensitive groups?

00:14:07,540 --> 00:14:09,920
How are sensitive groups impacted?

00:14:09,920 --> 00:14:14,759
And after you've got that, there's the power to transcend paradigms.

00:14:14,759 --> 00:14:19,730
So like if you teach people to code, if you drop the price of GPUs massively, with like Google

00:14:19,730 --> 00:14:26,199
Collab and what-not, then there are ways of decentralizing advancement of queer liberation.

00:14:26,199 --> 00:14:27,920
So thank you.

00:14:27,920 --> 00:14:29,879
And let me know what you think.

00:14:29,879 --> 00:14:30,879
It's been super exciting.

00:14:30,879 --> 00:14:31,879
Thank you.

00:14:31,879 --> 00:14:31,880

YouTube URL: https://www.youtube.com/watch?v=ESyuJyskDQo


