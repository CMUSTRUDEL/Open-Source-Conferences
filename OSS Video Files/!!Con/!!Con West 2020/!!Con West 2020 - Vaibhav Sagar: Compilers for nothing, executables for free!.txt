Title: !!Con West 2020 - Vaibhav Sagar: Compilers for nothing, executables for free!
Publication date: 2020-03-23
Playlist: !!Con West 2020
Description: 
	Presented at !!Con West 2020: http://bangbangcon.com/west

I’m going to talk about a less commonly known, but surprisingly useful, class of programs known as specialisers, and what you can do with them! I will also discuss the Futamura projections, which are mind-bending applications of this idea that have recently become practically useful!

#bangbangcon #bangbangconwest #bangbangconwest2020
Captions: 
	00:00:26,609 --> 00:00:29,279
So I have a problem.

00:00:29,279 --> 00:00:33,390
And my problem is that I use Vim.

00:00:33,390 --> 00:00:35,630
(laughter)

00:00:35,630 --> 00:00:42,489
And to get Vim to do anything remotely fancy or customize it to my needs, I need a vimrc.

00:00:42,489 --> 00:00:46,320
And every time Vim lags with a large file or it's just a bit slower than I would like,

00:00:46,320 --> 00:00:48,300
I keep thinking how like...

00:00:48,300 --> 00:00:51,250
I don't really need my Vim to handle every possible vimrc.

00:00:51,250 --> 00:00:53,710
I just need it to handle the vimrc that I have.

00:00:53,710 --> 00:00:56,570
There's a whole bunch of functionality in Vim that I don't use.

00:00:56,570 --> 00:01:01,250
There's a whole tab engine, for example, that I just don't care about.

00:01:01,250 --> 00:01:04,110
The only tab settings I use for my tab length are 4.

00:01:04,110 --> 00:01:06,259
Why does it need to handle a tab length of 3?

00:01:06,259 --> 00:01:09,400
Who uses tab length of 3?

00:01:09,400 --> 00:01:12,470
This is the kind of code I would imagine is inside Vim.

00:01:12,470 --> 00:01:13,470
Like...

00:01:13,470 --> 00:01:15,540
Someone set the tab length to 4.

00:01:15,540 --> 00:01:18,180
Handle it and set the number of spaces accordingly.

00:01:18,180 --> 00:01:19,630
So I kept thinking about...

00:01:19,630 --> 00:01:25,439
Wouldn't it be cool if I could take Vim and take my vimrc and jam my vimrc into Vim and

00:01:25,439 --> 00:01:31,170
do a whole bunch of unrolling and inlining and take out the stuff I would never use?

00:01:31,170 --> 00:01:35,110
So at the end of the day, it would be smaller because it doesn't have stuff I don't use

00:01:35,110 --> 00:01:38,590
and faster because it doesn't have stuff I don't use.

00:01:38,590 --> 00:01:39,590
Right?

00:01:39,590 --> 00:01:43,550
The idea of doing this in general is called partial evaluation.

00:01:43,550 --> 00:01:48,540
So favorite example of this is: You think of a program as taking static input and dynamic

00:01:48,540 --> 00:01:49,540
input.

00:01:49,540 --> 00:01:52,670
Static input is essentially known in advance before the program starts executing and dynamic

00:01:52,670 --> 00:01:57,020
input is only known once the program starts executing, to produce some output.

00:01:57,020 --> 00:02:00,460
A favorite example is anything with a configuration file.

00:02:00,460 --> 00:02:04,619
I use Vim, but emacs has the same problem.

00:02:04,619 --> 00:02:06,220
Your web servers.

00:02:06,220 --> 00:02:11,939
So wouldn't it be cool if we could actually take the program, take the static input, and

00:02:11,939 --> 00:02:16,080
jam it in, like I was describing, and do simple transformations like inline constants and

00:02:16,080 --> 00:02:17,879
unroll loops?

00:02:17,879 --> 00:02:18,879
What does that look like?

00:02:18,879 --> 00:02:24,180
Going back to the example I had earlier, if the tab length is not used anywhere else,

00:02:24,180 --> 00:02:26,720
I can just do that.

00:02:26,720 --> 00:02:27,720
Right?

00:02:27,720 --> 00:02:31,500
And that saves me a variable lookup and it saves me, like, allocating space that's not

00:02:31,500 --> 00:02:33,819
needed and all that other fun stuff.

00:02:33,819 --> 00:02:34,989
That's constant inlining.

00:02:34,989 --> 00:02:36,590
And what about loop unrolling?

00:02:36,590 --> 00:02:38,160
Again, same code.

00:02:38,160 --> 00:02:42,810
If I know I'm gonna insert a space four times, why don't I just insert a space four times?

00:02:42,810 --> 00:02:45,910
Pretty straightforward, I think.

00:02:45,910 --> 00:02:48,520
At the end of this, I would have something called a residual program.

00:02:48,520 --> 00:02:52,350
You can think of this as evaporating away all the parts of the program I don't need.

00:02:52,350 --> 00:02:58,230
It behaves exactly the same as my original program on that static input but it's faster.

00:02:58,230 --> 00:03:00,849
The program that would do this is called a specialiser.

00:03:00,849 --> 00:03:05,489
A specialiser takes a program and static input, jams the static input into the program, inlines,

00:03:05,489 --> 00:03:11,349
unrolls, does some other simple program transformations, and gives me a residual program that can take

00:03:11,349 --> 00:03:15,349
the dynamic input and produce the same output that the original program would have produced

00:03:15,349 --> 00:03:18,250
given the static and dynamic input.

00:03:18,250 --> 00:03:22,239
We call these programs specialisers and say in the terminology a specialiser is a specialised

00:03:22,240 --> 00:03:25,600
program with respect to some input.

00:03:25,600 --> 00:03:29,920
So let's take a slight detour and talk about interpreters versus compilers.

00:03:29,920 --> 00:03:31,940
So Python, for example, is a language that's usually interpreted.

00:03:31,940 --> 00:03:37,340
An interpreter usually looks at instructions one and one and interprets them, changes a

00:03:37,340 --> 00:03:39,379
state, and performs some action in the outside world.

00:03:39,379 --> 00:03:43,230
And these programs are relatively simple to write but they're also slow.

00:03:43,230 --> 00:03:46,190
Because you have to essentially consider each instruction one at a time.

00:03:46,190 --> 00:03:49,379
In contrast, we have languages like C, which are typically compiled.

00:03:49,379 --> 00:03:54,349
A compiler will kind of look at a whole source program, do a whole bunch of work, and generate

00:03:54,349 --> 00:03:57,650
another program that's usually in a different language.

00:03:57,650 --> 00:04:01,099
And these are harder to write, because you have to handle two different languages at

00:04:01,099 --> 00:04:05,720
once, but the output they produce is usually faster.

00:04:05,720 --> 00:04:08,410
So wouldn't it be nice if we could have both?

00:04:08,410 --> 00:04:12,590
Something that's both simple to write, as well as fast enough for our purposes?

00:04:12,590 --> 00:04:18,030
So a researcher called Yoshiko Futamura was thinking about these things in the early '80s.

00:04:18,030 --> 00:04:23,259
Because what he wanted was he wanted to write software in BASIC, not the fastest language,

00:04:23,259 --> 00:04:25,900
interpreted, but wanted the output to be fast, like Fortran.

00:04:25,900 --> 00:04:29,599
But he didn't want to write Fortran.

00:04:29,599 --> 00:04:34,940
Then he was thinking about this, so if I take an interpreter and specialise an interpreter with respect

00:04:34,940 --> 00:04:40,970
to some source program and do inlining and unrolling, and get a residual program which

00:04:40,970 --> 00:04:44,900
is a cut down version of the interpreter that only knows how to do things that the source

00:04:44,900 --> 00:04:49,500
program knows how to do, which can take dynamic input it needs and produce some output.

00:04:49,500 --> 00:04:54,900
But this program, the residual program, does not have any dependency on the old interpreter

00:04:54,900 --> 00:04:56,770
and runs by itself.

00:04:56,770 --> 00:04:59,270
So we've created a standalone program.

00:04:59,270 --> 00:05:00,270
That's pretty cool.

00:05:00,270 --> 00:05:01,820
We've compiled it somehow.

00:05:01,820 --> 00:05:04,809
Without involving a compiler.

00:05:04,809 --> 00:05:06,129
So he thought about this some more.

00:05:06,129 --> 00:05:07,129
He thought...

00:05:07,129 --> 00:05:08,129
Okay.

00:05:08,129 --> 00:05:09,129
Let's take this one step further.

00:05:09,129 --> 00:05:12,099
What if you specialized the specialiser itself with respect to an interpreter?

00:05:12,099 --> 00:05:15,849
You take a specialiser and jam an interpreter into it and do a whole bunch of inlining and

00:05:15,849 --> 00:05:21,349
unrolling and at the end you can take source code that the interpreter knows how to interpret

00:05:21,349 --> 00:05:26,080
and output a standalone executable just like in step one.

00:05:26,080 --> 00:05:30,120
Essentially we've made a compiler, because that’s what a compiler does.

00:05:30,120 --> 00:05:35,090
A compiler takes some source code for a program and outputs a standalone executable.

00:05:35,090 --> 00:05:38,949
So say the Python interpreter is written in C and we have a specialiser who knows how

00:05:38,949 --> 00:05:43,039
to specialise C, it takes a Python interpreter and outputs something that can then take Python

00:05:43,039 --> 00:05:45,939
source code and output a standalone executable.

00:05:45,939 --> 00:05:48,960
This is cool.

00:05:48,960 --> 00:05:50,419
Final trivial step.

00:05:50,419 --> 00:05:51,809
What if you specialise a specialiser?

00:05:51,809 --> 00:05:56,249
You take a specialiser, another specialiser, jam one specialiser into the other, do a bunch

00:05:56,249 --> 00:06:00,289
of inlining and unrolling and at the end you have a cool program that can take any interpreter

00:06:00,289 --> 00:06:04,900
that is written in a language that a specialiser knows how to specialise and output an equivalent

00:06:04,900 --> 00:06:05,900
compiler.

00:06:05,900 --> 00:06:08,169
Sp we've made a compiler compiler.

00:06:08,169 --> 00:06:09,949
That's pretty cool.

00:06:09,949 --> 00:06:11,340
Pretty cool.

00:06:11,340 --> 00:06:12,870
So why would someone want to do this?

00:06:12,870 --> 00:06:18,300
In the '80s, when Yoshika Futamura came up with these ideas, people were like...

00:06:18,300 --> 00:06:21,919
Let's write general purpose specialisers, but it turned out they're very difficult to

00:06:21,919 --> 00:06:22,919
write.

00:06:22,919 --> 00:06:23,919
Not easy programs to write.

00:06:23,919 --> 00:06:27,860
And the speedups provided were not as fast as people were hoping for.

00:06:27,860 --> 00:06:34,139
Still, the idea of specialisation gives us an interesting insight into the difference

00:06:34,139 --> 00:06:35,999
between an interpreter and a compiler.

00:06:35,999 --> 00:06:38,809
Because a specialiser isn't either, but it sits somewhere in between.

00:06:38,809 --> 00:06:42,069
The fact that you can use a specialiser to turn an interpreter into a compiler is pretty

00:06:42,069 --> 00:06:43,069
cool.

00:06:43,069 --> 00:06:47,180
It means it allows you, instead of writing a compiler, which is sometimes hard and error

00:06:47,180 --> 00:06:51,479
prone and tedious to write by hand, to just write an interpreter, which is relatively

00:06:51,479 --> 00:06:57,069
straightforward and a specialiser, and jam them together and get a compiler for free.

00:06:57,069 --> 00:07:01,960
So this is also important when it comes to correctness, because compiler bugs are pretty

00:07:01,960 --> 00:07:08,169
frustrating, and if you didn't have to worry about that at all, that would be pretty great.

00:07:08,169 --> 00:07:12,319
And so far, I've avoided the use of the word "optimisation", but has what we've been doing

00:07:12,319 --> 00:07:13,319
been optimisation?

00:07:13,319 --> 00:07:14,319
I would argue yes.

00:07:14,319 --> 00:07:18,629
You have to statically analyze a program and do loop unrolling and constant lining.

00:07:18,629 --> 00:07:20,871
This is a good way to think about what optimisation is.

00:07:20,871 --> 00:07:25,159
So this is a good entry point into learning about those things if you're curious.

00:07:25,159 --> 00:07:29,050
So I mentioned in the '80s this idea didn't go anywhere, but in the present day, we have

00:07:29,050 --> 00:07:32,369
things like PyPy, Truffle/Graal, LLVM, and other things whose entire value proposition

00:07:32,369 --> 00:07:36,960
is that if you write your language a certain way, you can get a just-in-time compiler for

00:07:36,960 --> 00:07:37,960
free.

00:07:37,960 --> 00:07:40,830
What is a just-in-time compiler but a second Futamura projection?

00:07:40,830 --> 00:07:46,159
It lazily partially evaluates parts of your program and turns things that were previously

00:07:46,159 --> 00:07:50,699
interpreted into things that are now compiled and also just gives you these programs.

00:07:50,699 --> 00:07:53,080
I would still like this specialised Vim.

00:07:53,080 --> 00:07:56,439
It would make me very happy.

00:07:56,439 --> 00:07:57,580
That's essentially all I have.

00:07:57,580 --> 00:07:59,490
Here are some resources.

00:07:59,490 --> 00:08:01,439
There's the original paper here.

00:08:01,439 --> 00:08:03,330
There'll be a link to my slides shortly.

00:08:03,330 --> 00:08:05,999
The Wikipedia page is always good.

00:08:05,999 --> 00:08:09,840
There's a much longer talk which in some ways this talk is a summary of called compilers

00:08:09,840 --> 00:08:14,669
for free which goes into details and gives you a sample language to talk about.

00:08:14,669 --> 00:08:17,689
But I think that talk is either 30 minutes or an hour long.

00:08:17,689 --> 00:08:19,819
So we didn't really have enough time for that.

00:08:19,819 --> 00:08:25,309
There's a textbook by people who did some interesting research into partial evaluation

00:08:25,309 --> 00:08:26,309
and were like...

00:08:26,309 --> 00:08:27,970
Here's all this stuff for free.

00:08:27,970 --> 00:08:30,800
Some other resources.

00:08:30,800 --> 00:08:34,120
And finally this really interesting talk about a fourth Futamura projection, going even further

00:08:34,120 --> 00:08:36,599
than the three we've just covered.

00:08:36,599 --> 00:08:40,770
So this is all I want you to take away from this talk.

00:08:40,770 --> 00:08:48,939

YouTube URL: https://www.youtube.com/watch?v=tylozRo9_hU


