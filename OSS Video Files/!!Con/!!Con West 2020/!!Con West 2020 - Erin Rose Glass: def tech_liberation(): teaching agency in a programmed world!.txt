Title: !!Con West 2020 - Erin Rose Glass: def tech_liberation(): teaching agency in a programmed world!
Publication date: 2020-03-23
Playlist: !!Con West 2020
Description: 
	Presented at !!Con West 2020: http://bangbangcon.com/west

Erin Rose Glass's keynote, def tech_liberation(): teaching agency in a programmed world!

#bangbangcon #bangbangconwest #bangbangconwest2020
Captions: 
	00:00:26,760 --> 00:00:28,200
Hi, everyone!

00:00:28,240 --> 00:00:30,080
My name is Erin, as you know.

00:00:30,080 --> 00:00:34,580
And I'm really excited to be here with you all.

00:00:34,580 --> 00:00:39,230
In fact, you're all so smart, so funny, so creative, it's kind of hard to follow you

00:00:39,230 --> 00:00:40,230
this morning.

00:00:40,230 --> 00:00:41,370
So bear with me.

00:00:41,370 --> 00:00:45,910
But thank you so much to the conference organizers, the other speakers, to the sponsors, for putting

00:00:45,910 --> 00:00:48,920
on this terrific conference.

00:00:48,920 --> 00:00:53,950
So I've been working at the intersection of higher education and technology for about

00:00:53,950 --> 00:00:55,220
10 years.

00:00:55,220 --> 00:00:57,220
Most recently in libraries.

00:00:57,220 --> 00:01:02,189
And I can't tell you how excited I was to learn of this conference that's devoted to

00:01:02,189 --> 00:01:06,140
the joy and surprise and excitement of computing.

00:01:06,140 --> 00:01:09,800
Because I think that's a vision that we desperately need more of, right?

00:01:09,800 --> 00:01:15,000
It's a vision that's being threatened by a very different kind of use of computers.

00:01:15,000 --> 00:01:16,000
Right?

00:01:16,000 --> 00:01:20,850
The use of computers to control and exploit human beings.

00:01:20,850 --> 00:01:23,430
And unfortunately we're seeing a lot more of that.

00:01:23,430 --> 00:01:24,430
Right?

00:01:24,430 --> 00:01:25,430
Every day.

00:01:25,430 --> 00:01:26,430
In our personal lives, in our workplace.

00:01:26,430 --> 00:01:28,300
In institutions of higher education.

00:01:28,300 --> 00:01:31,800
And also more recently on this very campus.

00:01:31,800 --> 00:01:37,561
And so I wanted to just start this talk by expressing solidarity with the graduate student

00:01:37,561 --> 00:01:43,000
workers at the University of California, Santa Cruz, and across the UC system, and across

00:01:43,000 --> 00:01:48,700
the country, who are fighting against unjust labor practices and for their right to a living

00:01:48,700 --> 00:01:49,840
wage.

00:01:49,840 --> 00:01:56,420
As we heard from Sohum Banerjea yesterday, more than 50 graduate students were fired,

00:01:56,420 --> 00:02:01,930
and that's putting their right to be in the US at risk, many of those folks, and jeopardizing

00:02:01,930 --> 00:02:03,830
their education as well.

00:02:03,830 --> 00:02:09,259
But I think we should really thank these graduate students, because their action reminds us

00:02:09,259 --> 00:02:14,459
of the type of agency that we have, when we work collectively.

00:02:14,459 --> 00:02:19,810
I'm mentioning that because, you know, we're on the campus of UC Santa Cruz, but I'm also

00:02:19,810 --> 00:02:23,620
mentioning that because it happens to be the theme of my talk.

00:02:23,620 --> 00:02:26,590
So today I want to talk about three things.

00:02:26,590 --> 00:02:30,380
Technology, education, and agency.

00:02:30,380 --> 00:02:35,700
And by agency, I mean our ability to critically understand the world around us.

00:02:35,700 --> 00:02:38,340
And to shape it according to our own interests.

00:02:38,340 --> 00:02:39,340
Right?

00:02:39,340 --> 00:02:44,410
Not according to the interests of the status quo, of the powers that be.

00:02:44,410 --> 00:02:49,870
And what I want us to think about, really, is: What does liberation look like in an increasingly

00:02:49,870 --> 00:02:51,030
programmed world?

00:02:51,030 --> 00:02:58,490
In a world where more and more of our activities are being monitored and influenced by computational

00:02:58,490 --> 00:03:00,120
technologies?

00:03:00,120 --> 00:03:05,849
Or if liberation was a Python function, as I imagined it in my title, which of course

00:03:05,849 --> 00:03:09,710
it is not, how would we define it?

00:03:09,710 --> 00:03:14,130
And how would we teach it?

00:03:14,130 --> 00:03:15,130
And just to be clear...

00:03:15,130 --> 00:03:16,130
You know, let me start off.

00:03:16,130 --> 00:03:17,340
I am not a programmer.

00:03:17,340 --> 00:03:19,210
I'm a bit of an outsider at this conference.

00:03:19,210 --> 00:03:20,880
I'm not a developer.

00:03:20,880 --> 00:03:24,340
I don't work in the industry.

00:03:24,340 --> 00:03:26,830
I'm really coming to you as a messenger from higher education.

00:03:26,830 --> 00:03:32,910
A messenger from academic libraries, to really tell you that we need your joyful approach

00:03:32,910 --> 00:03:36,300
of computing in higher education.

00:03:36,300 --> 00:03:41,459
Or to say it in sort of the soap opera language of this conference…

00:03:41,459 --> 00:03:44,120
(laughter and applause)

00:03:44,120 --> 00:03:45,930
You know?

00:03:45,930 --> 00:03:49,030
I had no idea that soap operas were a programming language.

00:03:49,030 --> 00:03:50,510
But this is what you speak, right?

00:03:50,510 --> 00:03:51,510
So anyway, I'm here.

00:03:51,510 --> 00:03:52,510
I'm asking you.

00:03:52,510 --> 00:03:57,120
Look, y'all are really smart and we need your help, because things are getting pretty scary.

00:03:57,120 --> 00:03:58,120
You might ask...

00:03:58,120 --> 00:03:59,120
How scary?

00:03:59,120 --> 00:04:00,460
What do you mean?

00:04:00,460 --> 00:04:02,480
Have you ever seen the Neverending Story?

00:04:02,480 --> 00:04:05,800
We’re talking The Nothing type scare level.

00:04:05,800 --> 00:04:09,250
Aren't we supposed to be talking about joy at this conference?

00:04:09,250 --> 00:04:10,250
Don't worry.

00:04:10,250 --> 00:04:11,599
We'll get to that.

00:04:11,599 --> 00:04:14,650
But really quickly about me...

00:04:14,650 --> 00:04:16,190
Um...

00:04:16,190 --> 00:04:20,810
So I'm a digital scholarship librarian at UCSD.

00:04:20,810 --> 00:04:22,000
Everyone always asks me...

00:04:22,000 --> 00:04:23,819
What does that even mean?!

00:04:23,819 --> 00:04:29,900
I'm really helping educators, students, researchers think about applying emerging technologies

00:04:29,900 --> 00:04:31,250
in their research and teaching.

00:04:31,250 --> 00:04:33,680
I have a PhD in the humanities.

00:04:33,680 --> 00:04:34,680
Okay?

00:04:34,680 --> 00:04:37,659
I'm not trained as a computational person.

00:04:37,659 --> 00:04:38,729
This all happened by accident.

00:04:38,729 --> 00:04:40,830
I could tell you about that later.

00:04:40,830 --> 00:04:45,669
And I'm really invested in Open Source technology, and community driven software.

00:04:45,669 --> 00:04:52,249
I think a lot about ethics and surveillance and control and I'm also a nice person, I

00:04:52,249 --> 00:04:53,309
think.

00:04:53,309 --> 00:04:54,309
I try.

00:04:54,309 --> 00:04:55,599
Let's get back to the strike.

00:04:55,599 --> 00:04:59,960
It's great to root our conversations in what's happening right now.

00:04:59,960 --> 00:05:05,830
As some of you know, as Joshua knew, there's been concerns in the last few weeks about

00:05:05,830 --> 00:05:09,160
how educational technology is being used to disempower workers.

00:05:09,160 --> 00:05:15,819
Our administrators Canvas, the learning management system, to spy on graduate students, controlling

00:05:15,819 --> 00:05:20,110
behavior, getting undergrads to snitch on their TAs.

00:05:20,110 --> 00:05:21,380
Someone said...

00:05:21,380 --> 00:05:24,689
I think NYU did that, didn’t they do that in Blackboard?

00:05:24,689 --> 00:05:25,990
They did.

00:05:25,990 --> 00:05:32,310
In 2005, grad students in NYU were striking and they were using Blackboard, the learning

00:05:32,310 --> 00:05:37,809
management system there, to kind of spy on the TAs' communications with students.

00:05:37,809 --> 00:05:38,839
Okay.

00:05:38,839 --> 00:05:39,869
Interesting.

00:05:39,869 --> 00:05:43,680
Oh, another thing...

00:05:43,680 --> 00:05:49,410
One of the main sites for the strike these past few weeks has been, I guess, blocked

00:05:49,410 --> 00:05:51,530
by the university network.

00:05:51,530 --> 00:05:52,530
Okay.

00:05:52,530 --> 00:05:58,770
One thing I want to point out, though, is that student concern with technology and particularly

00:05:58,770 --> 00:06:01,330
educational technology is not new.

00:06:01,330 --> 00:06:04,979
There's been a kind of conversation about this for quite a while.

00:06:04,979 --> 00:06:08,860
So it came up in the free speech movement in 1964.

00:06:08,860 --> 00:06:13,029
Here we have a picture of the movement happening at the University of California.

00:06:13,029 --> 00:06:22,009
And one of the student leaders, Mario Savio, on December 2nd, 1964, stood on the steps

00:06:22,009 --> 00:06:27,599
of Sproul Hall at UC Berkeley, and he talked about the machine, talked about this really

00:06:27,599 --> 00:06:34,229
terrible machine, and used it as a metaphor to describe the alienating forces that were

00:06:34,229 --> 00:06:35,229
happening.

00:06:35,229 --> 00:06:36,229
And of course, the student movement...

00:06:36,229 --> 00:06:38,099
I know you're all familiar...

00:06:38,099 --> 00:06:41,830
Was fighting against...

00:06:41,830 --> 00:06:43,889
Fighting for free speech.

00:06:43,889 --> 00:06:45,960
Expressing opposition to The Vietnam War.

00:06:45,960 --> 00:06:51,780
And I want to show you a video of how Mario Savio was sort of imagining The Machine.

00:06:51,780 --> 00:06:58,169
>> A time when the operation of the Machine becomes so odious, makes you so sick at heart

00:06:58,169 --> 00:07:02,069
that you can't take part.

00:07:02,080 --> 00:07:08,600
You can't even passively take part, and you've got to put your bodies upon the gears, upon

00:07:08,600 --> 00:07:13,280
the wheels, upon the levers, upon all the apparatus, and you've got to make it stop.

00:07:13,280 --> 00:07:19,280
You've got to indicate to the people who run it, who own it, that unless you're free, the

00:07:19,280 --> 00:07:22,200
machine will be prevented from working at all.

00:07:22,200 --> 00:07:24,840
>> Not joyful!

00:07:24,840 --> 00:07:26,960
Really not joyful!

00:07:27,040 --> 00:07:30,760
So on that day, 800 students were arrested.

00:07:30,770 --> 00:07:31,770
Right?

00:07:31,770 --> 00:07:35,800
Basically launching the free speech movement into national consciousness.

00:07:35,800 --> 00:07:40,270
And it's just interesting that the machine sort of metaphorically played a role in the

00:07:40,270 --> 00:07:45,729
imagination about the sort of dehumanization that was happening to students and then of

00:07:45,729 --> 00:07:47,789
course more broadly.

00:07:47,789 --> 00:07:49,360
But the thing is actually...

00:07:49,360 --> 00:07:51,389
They weren't just using machines as a metaphor.

00:07:51,389 --> 00:07:52,389
Right?

00:07:52,389 --> 00:07:57,059
Computing technology at the time was becoming an increasingly important tool in managing

00:07:57,059 --> 00:07:58,300
workers and students.

00:07:58,300 --> 00:08:02,259
And one of the core computing technologies at the time is of course you all know what

00:08:02,259 --> 00:08:03,360
this is on the left side.

00:08:03,360 --> 00:08:04,360
Right?

00:08:04,360 --> 00:08:06,349
It's a -- yeah, it’s a punch card!

00:08:06,349 --> 00:08:13,289
And so students were sort of imagining themselves as punch cards, being churned through the

00:08:13,289 --> 00:08:15,460
American university.

00:08:15,460 --> 00:08:21,610
And as Fred Turner tells us in his wonderful history From Counterculture To Cyberculture

00:08:21,610 --> 00:08:27,149
-- if you have not read this book, you should read it -- students were using the punch cards

00:08:27,149 --> 00:08:32,360
and reappropriating them for expressions of resistance.

00:08:32,360 --> 00:08:33,360
Here's the strike.

00:08:33,360 --> 00:08:34,770
It’s punched through the punch card.

00:08:34,770 --> 00:08:37,960
And this is Dusty Miller wearing it.

00:08:37,960 --> 00:08:43,000
Fred Turner tells us -- there's no photo of this, not that I found, but one student had

00:08:43,000 --> 00:08:45,790
a sign that said: I am a student of the University of California.

00:08:45,790 --> 00:08:51,960
Please do not fold, spindle, or mutilate me, obviously referring to the instructions of

00:08:51,960 --> 00:08:53,900
punch cards.

00:08:53,900 --> 00:08:55,390
Okay.

00:08:55,390 --> 00:08:59,440
So why should you care about this?

00:08:59,440 --> 00:09:02,880
I know probably not all of you are working in higher education.

00:09:02,880 --> 00:09:04,340
Maybe very few of you.

00:09:04,340 --> 00:09:09,350
In fact, raise your hand if you are associated with higher education in some way.

00:09:09,350 --> 00:09:10,350
Okay.

00:09:10,350 --> 00:09:11,850
So a fair number of you.

00:09:11,850 --> 00:09:12,850
That's awesome.

00:09:12,850 --> 00:09:14,220
That's great.

00:09:14,220 --> 00:09:19,780
So what I want to kind of convince you all, though, of is that whether you're working

00:09:19,780 --> 00:09:26,850
in higher education or not, what's happening technologically in universities matters!

00:09:26,850 --> 00:09:32,700
Because in many ways, the sort of tools, the values, the practices that we expose students

00:09:32,700 --> 00:09:38,830
to today, in relation to computing technology, will become dominant tomorrow.

00:09:38,830 --> 00:09:43,070
And if you don't believe me, just ask this guy.

00:09:43,070 --> 00:09:50,570
In an oral history, in 1995, he said: One of the things that built Apple II computers

00:09:50,570 --> 00:09:52,090
was schools buying them.

00:09:52,090 --> 00:09:53,090
Right?

00:09:53,090 --> 00:09:54,090
What does he mean by that?

00:09:54,090 --> 00:10:00,320
Steve Jobs was really frustrated at the sort of slow pace that people were picking up personal

00:10:00,320 --> 00:10:01,320
computing.

00:10:01,320 --> 00:10:05,420
So he ended up developing all these contracts with the universities and schools to get them

00:10:05,420 --> 00:10:07,980
to buy Apple II computers.

00:10:07,980 --> 00:10:10,360
Thus financing the company.

00:10:10,360 --> 00:10:18,700
But also training a generation of students to really value Apple II's vision of computing.

00:10:18,700 --> 00:10:19,700
Okay?

00:10:19,700 --> 00:10:23,070
And so what this anecdote highlights, whether you love Apple computers or not is not really

00:10:23,070 --> 00:10:24,130
the point.

00:10:24,130 --> 00:10:31,180
But the types of technologies that we expose students to is going to train their technological

00:10:31,180 --> 00:10:36,290
imagination about what's possible, what's normal, and so forth.

00:10:36,290 --> 00:10:40,680
And so I think we all know, right, there are some really exciting possibilities.

00:10:40,680 --> 00:10:41,680
Right?

00:10:41,680 --> 00:10:42,850
We've seen plenty of talks yesterday.

00:10:42,850 --> 00:10:44,950
We're gonna see more today.

00:10:44,950 --> 00:10:47,860
About the sort of exciting future that we might have.

00:10:47,860 --> 00:10:49,610
Exciting digital future.

00:10:49,610 --> 00:10:53,500
But we also know there's some really scary and dystopic things happening.

00:10:53,500 --> 00:10:58,350
And I thought, just so we're all on the same page, let's go through them really quickly.

00:10:58,350 --> 00:11:02,150
We're gonna do them quickly, because this is a conference about joy, not dystopia.

00:11:02,150 --> 00:11:06,470
But in order to get to joy, we have to face dystopia.

00:11:06,470 --> 00:11:08,820
One of the major concerns right now is surveillance.

00:11:08,820 --> 00:11:10,100
You all know this.

00:11:10,100 --> 00:11:13,490
Let's go quickly through what people are talking about.

00:11:13,490 --> 00:11:19,250
Edward Snowden a few years ago revealed that the NSA, in partnership with several technology

00:11:19,250 --> 00:11:24,110
companies, created this massive global surveillance infrastructure.

00:11:24,110 --> 00:11:26,980
People were understandably really upset about that.

00:11:26,980 --> 00:11:27,980
What's happened since?

00:11:27,980 --> 00:11:34,780
Well, now we just also have surveillance sort of ubiquitously embedded in all of our devices

00:11:34,780 --> 00:11:36,180
and tools that we use.

00:11:36,180 --> 00:11:38,200
Like the phone, of course.

00:11:38,200 --> 00:11:39,700
Like your television.

00:11:39,700 --> 00:11:41,570
Like your vacuum cleaner.

00:11:41,570 --> 00:11:48,000
Your AirBnB, your personal assistant, your social media, your search engine, your grocery

00:11:48,000 --> 00:11:49,000
store.

00:11:49,000 --> 00:11:50,000
Right?

00:11:50,000 --> 00:11:54,070
And then we even have more tools that are specifically for neighbors to spy on other

00:11:54,070 --> 00:11:55,070
people.

00:11:55,070 --> 00:11:56,070
Your dating apps.

00:11:56,070 --> 00:11:57,070
Etcetera, etcetera.

00:11:57,070 --> 00:11:58,070
Okay.

00:11:58,070 --> 00:11:59,070
We all know that.

00:11:59,070 --> 00:12:02,930
And another thing we're seeing, right -- it's not just about -- people aren't just concerned

00:12:02,930 --> 00:12:04,160
about the surveillance in itself.

00:12:04,160 --> 00:12:09,820
It's what this is enabling companies to do, through their tools.

00:12:09,820 --> 00:12:15,230
So we all know about the Facebook mood manipulation experiment.

00:12:15,230 --> 00:12:18,310
Which might be valuable, if we can manipulate moods.

00:12:18,310 --> 00:12:22,550
Turns out certain moods make people more susceptible to advertising.

00:12:22,550 --> 00:12:24,830
That's interesting.

00:12:24,830 --> 00:12:28,030
We know companies can delete things off of your machine.

00:12:28,030 --> 00:12:32,840
They can slow down your devices' batteries.

00:12:32,840 --> 00:12:37,150
They can devise things in hopes of making them more addictive.

00:12:37,150 --> 00:12:42,260
And then they can possibly even manipulate public discourse.

00:12:42,260 --> 00:12:44,030
This is debatable, but...

00:12:44,030 --> 00:12:45,030
Right?

00:12:45,030 --> 00:12:50,190
We're seeing that actually our communication platforms really have quite an effect on the

00:12:50,190 --> 00:12:55,350
way information is sort of passed on and how that shapes public understanding of current

00:12:55,350 --> 00:12:56,350
events.

00:12:56,350 --> 00:12:57,350
Do you remember this?

00:12:57,350 --> 00:13:04,420
Last fall, Elizabeth Warren was sort of calling out Facebook's policy to allow candidates

00:13:04,420 --> 00:13:08,130
or politicians to make false ads on Facebook.

00:13:08,130 --> 00:13:09,160
She said...

00:13:09,160 --> 00:13:10,160
Okay.

00:13:10,160 --> 00:13:11,160
You really want to do that?

00:13:11,160 --> 00:13:16,100
I'm gonna create an ad that claims that Zuckerberg endorsed Trump's reelection.

00:13:16,100 --> 00:13:19,320
So people are right now pointing to China.

00:13:19,320 --> 00:13:21,020
Wow, this is really scary.

00:13:21,020 --> 00:13:29,080
China is using facial recognition and other computational tools to basically monitor and

00:13:29,080 --> 00:13:30,430
influence the behavior of its citizens.

00:13:30,430 --> 00:13:32,170
You know, isn't that scary?

00:13:32,170 --> 00:13:33,170
And...

00:13:33,170 --> 00:13:37,240
Hey, actually, maybe we're also doing something similar here.

00:13:37,240 --> 00:13:38,970
Okay.

00:13:38,970 --> 00:13:40,910
But what's the problem?

00:13:40,910 --> 00:13:41,910
So...

00:13:41,910 --> 00:13:42,910
(laughter)

00:13:42,910 --> 00:13:45,330
I'm glad you're laughing!

00:13:45,330 --> 00:13:46,810
That's reassuring.

00:13:46,810 --> 00:13:48,630
But we have to talk about it.

00:13:48,630 --> 00:13:50,890
What do we think about this?

00:13:50,890 --> 00:13:55,650
There's been just an absolute explosion of literature from journalists, from scholars,

00:13:55,650 --> 00:14:03,380
from concerned citizens, about some of the problems that stem from these forms of surveillance.

00:14:03,380 --> 00:14:04,380
And control.

00:14:04,380 --> 00:14:05,400
I'll just go over a few of them.

00:14:05,400 --> 00:14:06,400
Right?

00:14:06,400 --> 00:14:07,470
Are any of you familiar with this book?

00:14:07,470 --> 00:14:08,470
Yeah.

00:14:08,470 --> 00:14:09,470
It's a great book.

00:14:09,470 --> 00:14:10,470
Right?

00:14:10,470 --> 00:14:12,100
Safiya Noble wrote this book called Algorithms of Oppression.

00:14:12,100 --> 00:14:16,160
She said look, search engines can reinforce racism.

00:14:16,160 --> 00:14:22,120
We don't have the right people sort of helping build these systems.

00:14:22,120 --> 00:14:25,330
Virginia Eubanks wrote a book called Automating Inequality.

00:14:25,330 --> 00:14:29,430
How high-tech tools profile police and punish the poor.

00:14:29,430 --> 00:14:30,430
Right?

00:14:30,430 --> 00:14:35,570
Another example of how these technologies are exasperating existing forms of social

00:14:35,570 --> 00:14:38,120
inequity and injustice.

00:14:38,120 --> 00:14:41,650
Human rights organizations that said: You know, these surveillance giants are really

00:14:41,650 --> 00:14:44,230
threatening human rights.

00:14:44,230 --> 00:14:47,930
Here's another book that made a big splash in 2019.

00:14:47,930 --> 00:14:55,170
Controversial, but just to quickly sum up its thesis: Shoshana Zuboff describes this

00:14:55,170 --> 00:14:57,580
thing she calls Surveillance Capitalism.

00:14:57,580 --> 00:15:03,360
She says this is a new economic logic that aims to predict and modify human behavior

00:15:03,360 --> 00:15:06,850
as a means to produce revenue, market, and control.

00:15:06,850 --> 00:15:09,890
And really what she's saying is all those forms of surveillance and control that we

00:15:09,890 --> 00:15:11,400
just looked at -- those aren't bugs.

00:15:11,400 --> 00:15:13,300
We hear this all the time.

00:15:13,300 --> 00:15:14,690
It's not a bug.

00:15:14,690 --> 00:15:16,080
It's a feature.

00:15:16,080 --> 00:15:17,130
This is the logic.

00:15:17,130 --> 00:15:18,130
Right?

00:15:18,130 --> 00:15:21,640
Driving a lot of our technology today.

00:15:21,640 --> 00:15:26,290
And she says: What this is causing is the rise of instrumentarian power.

00:15:26,290 --> 00:15:27,290
Right?

00:15:27,290 --> 00:15:32,310
The power to sort of manipulate behaviors of populations, at scale.

00:15:32,310 --> 00:15:34,560
And she interviews a bunch of people that say...

00:15:34,560 --> 00:15:35,560
Yeah.

00:15:35,560 --> 00:15:36,560
Quite frankly, that's what we're doing.

00:15:36,560 --> 00:15:41,970
The goal of everything we do is to change people's actual behavior at scale.

00:15:41,970 --> 00:15:42,970
And okay...

00:15:42,970 --> 00:15:44,650
What's wrong with instrumentarian power?

00:15:44,650 --> 00:15:49,140
Well, it threatens individual autonomy and democratic processes.

00:15:49,140 --> 00:15:54,430
But, you know, I don't know if you relate to this.

00:15:54,430 --> 00:15:56,060
What do you do?

00:15:56,060 --> 00:15:57,870
Have we really changed our behaviors?

00:15:57,870 --> 00:16:01,029
Have we given up tools that we don't think are ethical?

00:16:01,029 --> 00:16:04,820
I mean, no, not at all.

00:16:04,820 --> 00:16:06,000
Sorry.

00:16:06,000 --> 00:16:08,360
Let me...

00:16:08,360 --> 00:16:13,890
I moved on from my notes here.

00:16:13,890 --> 00:16:20,320
Because despite our awareness, we still...

00:16:20,320 --> 00:16:22,050
We still embrace these tools.

00:16:22,050 --> 00:16:23,050
Right?

00:16:23,050 --> 00:16:24,490
And Rob Horning writes about this.

00:16:24,490 --> 00:16:29,870
He says despite the steady flow of complaints in more and more high profile media outlets

00:16:29,870 --> 00:16:35,140
about surveillance capitalism, the way it erodes privacy, abolishes trust, extends bias,

00:16:35,140 --> 00:16:38,920
foments anxiety and invidious competition, on and on and on -- this is a great list,

00:16:38,920 --> 00:16:45,190
by the way -- despite all this, many people are still going to go with the flow, and get

00:16:45,190 --> 00:16:49,690
the latest gadgets, because why not?

00:16:49,690 --> 00:16:52,940
Or as some journalists have pointed out, it's actually impossible.

00:16:52,940 --> 00:16:55,350
I don't know if anyone saw this article.

00:16:55,350 --> 00:16:57,279
It's by Kashmir Hill.

00:16:57,279 --> 00:17:03,400
She took five weeks and had a whole team of expert technologists, and a budget to try

00:17:03,400 --> 00:17:06,559
to figure out how to disentangle herself from these technologies.

00:17:06,559 --> 00:17:09,120
And she said she actually literally could not do it.

00:17:09,120 --> 00:17:10,120
Right?

00:17:10,120 --> 00:17:14,110
So who has five weeks and a surplus budget to sort of disconnect?

00:17:14,110 --> 00:17:15,970
Not me, anyway.

00:17:15,970 --> 00:17:16,970
Okay.

00:17:16,970 --> 00:17:17,970
So...

00:17:17,970 --> 00:17:21,819
These were the things that I was thinking about.

00:17:21,819 --> 00:17:24,289
This leaves us with a lot of questions, right?

00:17:24,289 --> 00:17:25,539
Is technology bad?

00:17:25,539 --> 00:17:26,740
Is computing bad?

00:17:26,740 --> 00:17:27,740
You know?

00:17:27,740 --> 00:17:30,639
Is Western civilization bad?

00:17:30,639 --> 00:17:32,360
Are humans bad?

00:17:32,360 --> 00:17:33,590
Is capitalism bad?

00:17:33,590 --> 00:17:36,149
Are we just totally screwed, you know?

00:17:36,149 --> 00:17:39,009
What do we do with this information?

00:17:39,009 --> 00:17:40,009
Right?

00:17:40,009 --> 00:17:44,100
And so that's what I was thinking about as a graduate student in the humanities.

00:17:44,100 --> 00:17:45,270
Thinking...

00:17:45,270 --> 00:17:47,419
Maybe the humanities can help us out here somehow.

00:17:47,419 --> 00:17:48,740
I don't know why or how.

00:17:48,740 --> 00:17:50,879
And I came across this book.

00:17:50,879 --> 00:17:54,140
Does anyone recognize this book?

00:17:54,140 --> 00:17:55,210
A few people?

00:17:55,210 --> 00:17:56,210
Cool.

00:17:56,210 --> 00:17:57,210
Awesome.

00:17:57,210 --> 00:17:58,210
Awesome.

00:17:58,210 --> 00:17:59,210
Yeah.

00:17:59,210 --> 00:18:00,210
Pedagogy of the Oppressed.

00:18:00,210 --> 00:18:05,590
I thought some of you might recognize it, because even though it's a book that was originally

00:18:05,590 --> 00:18:11,299
published in Brazil, in the 1970s, it had a great -- and still today -- had great influence

00:18:11,299 --> 00:18:13,650
on US educators here.

00:18:13,650 --> 00:18:17,610
And launched a movement called critical pedagogy.

00:18:17,610 --> 00:18:24,169
Anyway, I want to share a little bit with you about this book, because even though it

00:18:24,169 --> 00:18:32,419
has nothing to do with technology, it's about pedagogy, teaching, education, it surprisingly

00:18:32,419 --> 00:18:39,679
and joyfully offers some ideas about how we might respond to the sort of technological

00:18:39,679 --> 00:18:44,220
dystopia that we see rising up all around us.

00:18:44,220 --> 00:18:46,549
So I thought I'd tell you a little bit about this book.

00:18:46,549 --> 00:18:53,049
It was written by a Brazilian educator and philosopher named Paulo Freire.

00:18:53,049 --> 00:19:03,889
Who basically in the early 1960s was directing a national literacy program in Brazil that

00:19:03,889 --> 00:19:11,080
aimed to basically teach millions and millions of impoverished Brazilians how to read and

00:19:11,080 --> 00:19:12,140
write.

00:19:12,140 --> 00:19:14,269
Adults, essentially.

00:19:14,269 --> 00:19:20,120
And if you're not familiar with this period in Brazil's history, this was a decade of

00:19:20,120 --> 00:19:22,790
great political unrest.

00:19:22,790 --> 00:19:28,170
In fact, Paulo Freire's work was cut short in 1964, because there was a military coup

00:19:28,170 --> 00:19:33,369
and he was arrested and eventually exiled to Chile.

00:19:33,369 --> 00:19:37,389
But I wanted to show you these documents, because I think they highlight sort of where

00:19:37,389 --> 00:19:39,610
the country was at the moment.

00:19:39,610 --> 00:19:47,350
On the left, this is a photo taken by an American photographer of one of the Brazilian rural

00:19:47,350 --> 00:19:48,419
poor child...

00:19:48,419 --> 00:19:56,309
And on the left is a still from a film called Vidas Secas, Barren Lives, which was one of

00:19:56,309 --> 00:20:00,169
the many films coming out at the time that was really illustrating the sort of struggles

00:20:00,169 --> 00:20:02,309
of these people.

00:20:02,309 --> 00:20:03,779
Okay.

00:20:03,779 --> 00:20:08,799
So while Freire was educating these people, he came up with this really interesting theory

00:20:08,799 --> 00:20:13,029
between education and oppression.

00:20:13,029 --> 00:20:14,649
And he says...

00:20:14,649 --> 00:20:16,049
Look.

00:20:16,049 --> 00:20:22,669
So much of what passes for education is really in fact teaching people to accept the world

00:20:22,669 --> 00:20:24,559
view of an oppressive class.

00:20:24,559 --> 00:20:25,559
Right?

00:20:25,559 --> 00:20:29,820
It's preventing people from questioning the status quo.

00:20:29,820 --> 00:20:36,630
From realizing their oppression, and from developing greater agency in their lives.

00:20:36,630 --> 00:20:42,720
And I found this little cartoon on the internet, sort of depicting what Paulo Freire calls

00:20:42,720 --> 00:20:45,120
the banking model of education.

00:20:45,120 --> 00:20:46,220
And he says really...

00:20:46,220 --> 00:20:53,269
What we're doing in the classroom is the authoritative instructor is filling the minds of students

00:20:53,269 --> 00:20:59,879
with this world view, and the orange Kool-Aid, in this case, and then when you're done with

00:20:59,879 --> 00:21:02,929
education, you just think that the world is orange Kool-Aid.

00:21:02,929 --> 00:21:04,779
You're not actually asking...

00:21:04,779 --> 00:21:05,890
Who is that guy?

00:21:05,890 --> 00:21:06,929
Why is he up there?

00:21:06,929 --> 00:21:08,200
What's the orange Kool-Aid?

00:21:08,200 --> 00:21:09,249
Why is he pouring it in here?

00:21:09,249 --> 00:21:14,360
Is there something better that we could do in this case?

00:21:14,360 --> 00:21:22,720
So for Freire, oppression is denying people the right to critically understand and transform

00:21:22,720 --> 00:21:28,290
the world and surprisingly, the thesis of his book: Education is a very powerful tool

00:21:28,290 --> 00:21:31,440
in doing that.

00:21:31,440 --> 00:21:32,490
Okay.

00:21:32,490 --> 00:21:34,360
But it doesn't have to be that way.

00:21:34,360 --> 00:21:35,710
Let me look at time.

00:21:35,710 --> 00:21:36,710
Okay.

00:21:36,710 --> 00:21:37,710
It doesn't have to be that way.

00:21:37,710 --> 00:21:42,450
And so he spends his book sort of describing this different theory of education.

00:21:42,450 --> 00:21:44,040
Education as liberatory practice.

00:21:44,040 --> 00:21:47,470
How do you teach people to not accept the orange Kool-Aid?

00:21:47,470 --> 00:21:52,049
To take off the aquarium and to critically understand the world and transform it according

00:21:52,049 --> 00:21:54,360
to their own needs and interests?

00:21:54,360 --> 00:21:58,039
How do we teach people to have agency?

00:21:58,039 --> 00:22:02,799
Sorry, I'm lost.

00:22:02,799 --> 00:22:04,389
Okay.

00:22:04,389 --> 00:22:11,029
And surprisingly, as I was mentioning before, this book became very popular in the US.

00:22:11,029 --> 00:22:18,799
Even though the 1960s and '70s college student was very different than the rural poor in

00:22:18,799 --> 00:22:20,679
Brazil at the time.

00:22:20,679 --> 00:22:27,409
And yet, analyzing education for the ways that it teaches people to conform to the status

00:22:27,409 --> 00:22:31,480
quo is something that you can apply to educational institutions anywhere.

00:22:31,480 --> 00:22:32,480
Right?

00:22:32,480 --> 00:22:38,669
Oppression does not need to be an explicitly violent act.

00:22:38,669 --> 00:22:41,891
And just for an example, right, the grade, the letter grade, that's something that we

00:22:41,891 --> 00:22:45,360
probably all think is sort of natural and neutral.

00:22:45,360 --> 00:22:49,269
You can also look at the letter grade and see it as a tool that reinforces individualism.

00:22:49,269 --> 00:22:50,269
Right?

00:22:50,269 --> 00:22:55,950
That reinforces sort of competitive self-serving interests, when you're learning, and so actually

00:22:55,950 --> 00:22:58,600
Santa Cruz -- it has grades now.

00:22:58,600 --> 00:22:59,600
Right?

00:22:59,600 --> 00:23:01,049
But historically, it did not have grades at some point.

00:23:01,049 --> 00:23:02,049
Is that true?

00:23:02,049 --> 00:23:03,049
Can anyone confirm that?

00:23:03,049 --> 00:23:04,049
Right?

00:23:04,049 --> 00:23:05,049
And that's really interesting.

00:23:05,049 --> 00:23:11,429
Because how is this grade teaching us to go out into the world and continue to try to

00:23:11,429 --> 00:23:17,450
only serve our own learning and our own selves, instead of working collaboratively with other

00:23:17,450 --> 00:23:18,620
people?

00:23:18,620 --> 00:23:22,210
We could talk all day about different ways...

00:23:22,210 --> 00:23:25,260
Ideological principles are embodied in education.

00:23:25,260 --> 00:23:30,690
But what I want to talk about is that: I find these ideas really relevant to thinking about

00:23:30,690 --> 00:23:35,299
how computers have been adopted by higher education.

00:23:35,299 --> 00:23:42,200
And one observation I've made is that outside of computer science departments, computers

00:23:42,200 --> 00:23:45,720
have been predominantly adopted as just a neutral utility.

00:23:45,720 --> 00:23:50,070
As something to make the university processes quicker, more efficient, easier.

00:23:50,070 --> 00:23:51,070
Right?

00:23:51,070 --> 00:23:53,029
Think of learning management systems.

00:23:53,029 --> 00:23:54,029
Word processors.

00:23:54,029 --> 00:23:55,029
Email.

00:23:55,029 --> 00:23:56,029
Web search.

00:23:56,029 --> 00:23:57,029
Etcetera, etcetera.

00:23:57,029 --> 00:23:58,029
Right?

00:23:58,029 --> 00:24:00,179
And in many ways, they've been...

00:24:00,179 --> 00:24:01,330
People really like these tools.

00:24:01,330 --> 00:24:05,730
They've made things easier to do.

00:24:05,730 --> 00:24:10,100
But at the same time, something really sneaky has happened.

00:24:10,100 --> 00:24:11,100
Right?

00:24:11,100 --> 00:24:15,639
As we're adopting these technologies, as we're adopting the Apple II in the '80s, we're also

00:24:15,639 --> 00:24:20,020
adopting the sort of dominant view of computing at the time.

00:24:20,020 --> 00:24:21,020
Right?

00:24:21,020 --> 00:24:24,749
Which is a black box form of computing where users have little expectation of being able

00:24:24,749 --> 00:24:33,179
to go into their machines and to change them according to their own needs and interests.

00:24:33,179 --> 00:24:38,960
And of course, you know, I often speak to folks who only know this model of computing.

00:24:38,960 --> 00:24:39,960
Right?

00:24:39,960 --> 00:24:42,649
But we're at a conference now where all of you are tinkerers, and so of course, you know

00:24:42,649 --> 00:24:48,119
that there have been many other models of computing throughout the years.

00:24:48,119 --> 00:24:53,759
The Scandinavian trade unions in the 1960s advocated for the rights of workers to co-design

00:24:53,759 --> 00:24:56,489
information technologies in their workplace.

00:24:56,489 --> 00:25:02,380
Alan Kay had this idea for a Dynabook where students would program their computers from

00:25:02,380 --> 00:25:03,720
the ground up.

00:25:03,720 --> 00:25:05,250
Ted Nelson.

00:25:05,250 --> 00:25:08,359
Any nitwit can understand computers, and many do.

00:25:08,359 --> 00:25:12,639
(laughter) It's a good quote, right?

00:25:12,639 --> 00:25:16,610
Unfortunately, due to ridiculous historical circumstances, computers have made it in this

00:25:16,610 --> 00:25:18,890
form to most of the world.

00:25:18,890 --> 00:25:23,970
I'm low on time, so I'm gonna go quickly into the Free Software Foundation, of course.

00:25:23,970 --> 00:25:26,230
More recently, platform co-op movements...

00:25:26,230 --> 00:25:28,429
I'm just gonna speed right away.

00:25:28,429 --> 00:25:34,690
And one thing that I think we've lost by adopting this dominant model of computing is we've

00:25:34,690 --> 00:25:41,629
lost the ability to give sort of the larger student body the ability to creatively tinker

00:25:41,629 --> 00:25:43,190
with their tools.

00:25:43,190 --> 00:25:45,989
And one thing that's really interesting is you go back in the history, in the '80s, and

00:25:45,989 --> 00:25:50,119
there were in fact a lot of educators and students who were not in the computer science

00:25:50,119 --> 00:25:55,419
department who were doing all sorts of creative things with their computing technologies.

00:25:55,419 --> 00:25:56,809
Do you want to see one?

00:25:56,809 --> 00:25:57,809
Yeah!

00:25:57,809 --> 00:25:58,809
Okay.

00:25:58,809 --> 00:25:59,809
Cool.

00:25:59,809 --> 00:26:00,809
Good.

00:26:00,809 --> 00:26:01,809
Oh, sorry.

00:26:01,809 --> 00:26:02,809
Before that...

00:26:02,809 --> 00:26:03,809
Right...

00:26:03,809 --> 00:26:06,450
They were aware of this, because Fred Kemp says in 1987: Why is it after more than seven

00:26:06,450 --> 00:26:13,289
years of imaginative and innovative work by all these people on writing programs specifically...

00:26:13,289 --> 00:26:17,220
We still have this really boring word processor?

00:26:17,220 --> 00:26:21,970
Universities are just adopting word processors that are all about grammar and things like

00:26:21,970 --> 00:26:22,970
that.

00:26:22,970 --> 00:26:23,970
So here is one.

00:26:23,970 --> 00:26:25,250
I want to show you -- I'm really excited about this.

00:26:25,250 --> 00:26:26,250
There’s many more.

00:26:26,250 --> 00:26:27,799
We don't have much time.

00:26:27,799 --> 00:26:33,809
Hugh Burns was a graduate student in composition in 1979, and he created a writing program

00:26:33,809 --> 00:26:36,649
inspired by a philosopher, inspired by Aristotle.

00:26:36,649 --> 00:26:38,879
What does that even mean?

00:26:38,879 --> 00:26:40,049
It's sort of a chat program.

00:26:40,049 --> 00:26:45,700
Students, while they're working on their essays, would chat with this bot, and sort of develop

00:26:45,700 --> 00:26:49,190
ideas for their paper.

00:26:49,190 --> 00:26:53,620
And this is -- sorry, it's sort of a poor copy here, but one thing I wanted to point

00:26:53,620 --> 00:26:56,870
out is: it's a fun, it's a joyful program.

00:26:56,870 --> 00:26:57,870
It's hilarious.

00:26:57,870 --> 00:27:01,369
Like here, he's talking at this point -- or the computer is talking at this point -- with

00:27:01,369 --> 00:27:04,779
a student about their topic on the fear of death.

00:27:04,779 --> 00:27:06,799
And the computer says: Holy electronics!

00:27:06,799 --> 00:27:07,799
That's weird.

00:27:07,799 --> 00:27:11,340
I used to date a computer interested in the fear of death!

00:27:11,340 --> 00:27:12,340
Right?

00:27:12,340 --> 00:27:13,340
(laughter)

00:27:13,340 --> 00:27:14,799
I mean, how fun?!

00:27:14,799 --> 00:27:17,450
So I hope showing this to you makes you realize...

00:27:17,450 --> 00:27:21,950
Wouldn't it be great if everyone was developing weird writing programs based on their favorite

00:27:21,950 --> 00:27:26,139
philosophers or artists or things like that?

00:27:26,139 --> 00:27:31,840
So I think we've sort of lost a joyful approach in computing for the masses.

00:27:31,840 --> 00:27:32,879
Certainly not for you guys.

00:27:32,879 --> 00:27:35,830
But for the broader population.

00:27:35,830 --> 00:27:38,630
But I think that's not the only thing we've lost.

00:27:38,630 --> 00:27:42,860
We've also lost the more kind of critical participatory relationship with our tools

00:27:42,860 --> 00:27:49,049
that is allowing really scary things to happen in educational technology more broadly.

00:27:49,049 --> 00:27:52,700
Because all those forms of surveillance and control that we just looked at are also right

00:27:52,700 --> 00:27:54,980
now entering education.

00:27:54,980 --> 00:27:56,129
Right?

00:27:56,129 --> 00:28:00,259
So the EFF just published this two days ago.

00:28:00,259 --> 00:28:04,659
Schools are operating as test beds for mass surveillance with no evidence of and no way

00:28:04,659 --> 00:28:05,659
to opt out.

00:28:05,659 --> 00:28:09,869
Let me just show you quickly a few of these.

00:28:09,869 --> 00:28:15,179
I'm gonna go quickly, because I only have a few more minutes, but what is driving this

00:28:15,179 --> 00:28:16,179
surveillance?

00:28:16,179 --> 00:28:17,619
The value of student data.

00:28:17,619 --> 00:28:18,619
Right?

00:28:18,619 --> 00:28:21,110
So in 2013, McKinsey Global Institute said...

00:28:21,110 --> 00:28:22,110
You know what?

00:28:22,110 --> 00:28:26,399
That student data might be worth as much as $890 billion to $1.2 trillion annually.

00:28:26,399 --> 00:28:28,399
That's a lot of money.

00:28:28,399 --> 00:28:30,570
So of course, companies are moving in on that.

00:28:30,570 --> 00:28:34,309
Have you heard of TurnItIn, the plagiarism company?

00:28:34,309 --> 00:28:39,159
Has over 33 million student papers locked in on its server.

00:28:39,159 --> 00:28:44,999
It sold last year for $1.75 billion, to a media company, not even an educational company.

00:28:44,999 --> 00:28:51,690
Not sure if you're aware, but Instructure, the company that provides Canvas, the learning

00:28:51,690 --> 00:28:55,799
management system, is right now negotiating a deal for $2 billion.

00:28:55,799 --> 00:28:59,350
The value of these companies is not just the software.

00:28:59,350 --> 00:29:02,230
It's the data that they have.

00:29:02,230 --> 00:29:06,080
Even our scholarly publishers are no longer calling themselves scholarly publishers.

00:29:06,080 --> 00:29:09,979
They're calling themselves research analytics companies.

00:29:09,979 --> 00:29:11,950
There's a lot of scary stuff here.

00:29:11,950 --> 00:29:13,080
I'm gonna pass over...

00:29:13,080 --> 00:29:16,210
You can just look at it.

00:29:16,210 --> 00:29:17,210
Wrist bands.

00:29:17,210 --> 00:29:18,210
Ugh.

00:29:18,210 --> 00:29:19,210
It's too much.

00:29:19,210 --> 00:29:20,720
It's too much.

00:29:20,720 --> 00:29:22,090
Oh, this one.

00:29:22,090 --> 00:29:28,179
Gaggle monitors all student email and digital communication to make sure you're not a threat

00:29:28,179 --> 00:29:29,889
to other students.

00:29:29,889 --> 00:29:33,350
That one is fun.

00:29:33,350 --> 00:29:34,350
Okay.

00:29:34,350 --> 00:29:35,350
Okay.

00:29:35,350 --> 00:29:36,350
Okay.

00:29:36,350 --> 00:29:38,500
What do we do?

00:29:38,500 --> 00:29:39,500
Okay.

00:29:39,500 --> 00:29:41,490
First I just want to tell you...

00:29:41,490 --> 00:29:43,970
I'm not the only person talking about this.

00:29:43,970 --> 00:29:50,889
There are a lot of really exciting people working in education or as writers or librarians

00:29:50,889 --> 00:29:53,260
that are doing really important work.

00:29:53,260 --> 00:29:56,389
This is only a few of them.

00:29:56,389 --> 00:30:02,130
Chris Gilliard, who's right here, he writes about this, and says: You know, actually,

00:30:02,130 --> 00:30:06,200
students are really angry about these forms of surveillance, but they just don't see any

00:30:06,200 --> 00:30:07,460
alternatives.

00:30:07,460 --> 00:30:12,739
If higher education is to, quote, save the web, we really need to enact this better web

00:30:12,739 --> 00:30:13,739
in the classroom.

00:30:13,739 --> 00:30:16,110
And we're certainly not doing that right now.

00:30:16,110 --> 00:30:20,669
Here's Jessamyn; Jessamyn is up there on the left, Jessamyn is a Vermont librarian doing

00:30:20,669 --> 00:30:28,919
all this really exciting library activism around digital technology.

00:30:28,919 --> 00:30:30,749
There's a hybrid pedagogy lab.

00:30:30,749 --> 00:30:33,039
Look, there's Paulo Freire right there!

00:30:33,039 --> 00:30:38,399
They're also thinking about more critical approaches to digital technology and higher

00:30:38,399 --> 00:30:39,440
ed.

00:30:39,440 --> 00:30:40,970
So here's a lot of projects.

00:30:40,970 --> 00:30:44,909
Digital library federation has a technologies of surveillance group.

00:30:44,909 --> 00:30:47,350
I'm gonna speed along.

00:30:47,350 --> 00:30:50,710
What these are all doing, I think, is they're showing the importance of exposing students

00:30:50,710 --> 00:30:54,009
to different models and values of technological practice.

00:30:54,009 --> 00:30:55,640
I've been doing this kind of work too.

00:30:55,640 --> 00:31:01,549
When I was a graduate student, I helped develop a platform for socializing student writing

00:31:01,549 --> 00:31:02,549
and feedback.

00:31:02,549 --> 00:31:07,259
It was really fun, just for the functionality, but what was really important to me was that

00:31:07,259 --> 00:31:11,190
it was a space where students can actually start designing the technologies that they

00:31:11,190 --> 00:31:16,461
were using in their education, and although we didn't get to enact this, you know, it

00:31:16,461 --> 00:31:20,409
was sort of striving to also allow students to govern these platforms.

00:31:20,409 --> 00:31:26,230
To give them experience in sort of creating the policies around data and things like that.

00:31:26,230 --> 00:31:31,359
And so here's another screenshot of the activity feed and so forth.

00:31:31,359 --> 00:31:39,289
Right now, I lead a non-commercial digital commons in San Diego, for five different institutions

00:31:39,289 --> 00:31:40,530
of higher education.

00:31:40,530 --> 00:31:47,159
We have about 2400 members, and it's really a place where students can create these discussion

00:31:47,159 --> 00:31:50,619
groups and create websites and things like that.

00:31:50,619 --> 00:31:55,549
But what I'm really excited about is that we have a student research group that gets

00:31:55,549 --> 00:31:59,590
together, and we read about critical issues and technology and educational technology,

00:31:59,590 --> 00:32:04,590
and then we think about: How would we like to further develop KNIT, according to our

00:32:04,590 --> 00:32:10,119
own creative interests and our sort of political concerns about educational technology?

00:32:10,119 --> 00:32:16,919
I also run ethical ed tech, with Nathan Schneider, at the University of Colorado, Boulder.

00:32:16,919 --> 00:32:20,049
This is a wiki and a learning community.

00:32:20,049 --> 00:32:27,870
The wiki is a place where we invite the community to put information up about more ethical alternatives

00:32:27,870 --> 00:32:33,900
to technologies than the sort of surveillance technologies we see in education right now.

00:32:33,900 --> 00:32:40,179
We also do webinars and invite people working in this space to talk about their experiences.

00:32:40,179 --> 00:32:46,429
This was a really great editathon on strategic refusal, where we heard from ed tech workers

00:32:46,429 --> 00:32:52,019
about their experiences in refusing to provide certain tools to universities, based on the

00:32:52,019 --> 00:32:56,720
different types of surveillance they embodied.

00:32:56,720 --> 00:33:02,669
And what are the risks of that, as an ed tech worker, in refusing to provide a tool that

00:33:02,669 --> 00:33:05,940
your employer is telling you to use?

00:33:05,940 --> 00:33:10,220
And we also collaborate with other activists in this space.

00:33:10,220 --> 00:33:17,529
Cristina Colquhoun wrote a petition to Instructure, that company we just talked about, about their

00:33:17,529 --> 00:33:22,940
use of student data, and that in fact got this EdSurge article written up about their

00:33:22,940 --> 00:33:26,900
concerns about student data that I told you just a minute ago.

00:33:26,900 --> 00:33:29,340
And finally, another project I worked on.

00:33:29,340 --> 00:33:31,399
And I don't think this is just about technology.

00:33:31,399 --> 00:33:33,509
It's about raising awareness.

00:33:33,509 --> 00:33:40,019
I worked with Autumm Caines on creating a remixable student privacy statement that any

00:33:40,019 --> 00:33:46,489
educator could put on their syllabus, to sort of just let students know that they should

00:33:46,489 --> 00:33:52,220
maybe be thinking about how their student data is being used by these companies, and

00:33:52,220 --> 00:33:57,299
hopefully just trying to not normalize the fact that we're allowing surveillance to kind

00:33:57,299 --> 00:34:00,220
of happen in our educational spaces.

00:34:00,220 --> 00:34:08,230
So these are ways that I've been trying to apply Freire's liberatory practice to educational

00:34:08,230 --> 00:34:09,369
technology.

00:34:09,369 --> 00:34:15,490
I see higher education as a site for liberating the technological imagination from the digital

00:34:15,490 --> 00:34:17,510
status quo.

00:34:17,510 --> 00:34:22,650
And what I mean just by liberatory technological consciousness is the right and capacity to

00:34:22,650 --> 00:34:27,800
critically understand and transform the tech that we use, the belief that collective governance

00:34:27,800 --> 00:34:30,730
and development of technology matters.

00:34:30,730 --> 00:34:34,850
The study of the influence of this technology on our practices.

00:34:34,850 --> 00:34:36,280
From the point of view of the users.

00:34:36,280 --> 00:34:38,710
Not just the administrators.

00:34:38,710 --> 00:34:43,780
And the belief that technology is always an unfinished project in need of our continued

00:34:43,780 --> 00:34:47,610
collective understanding and transformation.

00:34:47,610 --> 00:34:48,679
Why does it matter?

00:34:48,679 --> 00:34:54,780
Well, because we need students to be prepared to shape a better digital future.

00:34:54,780 --> 00:34:56,330
And not this.

00:34:56,330 --> 00:34:57,720
Not that.

00:34:57,720 --> 00:35:01,400
Not that.

00:35:01,400 --> 00:35:02,820
And yeah.

00:35:02,820 --> 00:35:03,820
I think...

00:35:03,820 --> 00:35:07,670
As I said, the imagination is a really important part of this.

00:35:07,670 --> 00:35:12,170
Which is why I was so excited to see Rachel Carson quoted yesterday.

00:35:12,170 --> 00:35:13,670
In the opening talk.

00:35:13,670 --> 00:35:14,920
She's talking about wonder, right?

00:35:14,920 --> 00:35:15,920
It's wonder.

00:35:15,920 --> 00:35:16,920
It's joy.

00:35:16,920 --> 00:35:17,920
It's imagination.

00:35:17,920 --> 00:35:23,610
We really need to preserve that vision of computing.

00:35:23,610 --> 00:35:26,610
And why I think it's so exciting that you brought up Rachel Carson...

00:35:26,610 --> 00:35:29,300
She of course is the author of Silent Spring.

00:35:29,300 --> 00:35:31,030
Are you all familiar with Silent Spring?

00:35:31,030 --> 00:35:36,400
It was one of the books that sort of kicked off the environmental movement in the US.

00:35:36,400 --> 00:35:42,220
And sparked the imagination and made people care!

00:35:42,220 --> 00:35:47,530
And I think her work has really led to the profound movement that we see happening today.

00:35:47,530 --> 00:35:50,560
And I think we need something similar for computing.

00:35:50,560 --> 00:35:51,560
Right?

00:35:51,560 --> 00:35:55,170
And this is what I think you all are already doing.

00:35:55,170 --> 00:35:56,170
Right?

00:35:56,170 --> 00:36:03,280
By exploring the sort of joyful possibilities of what we can do with these tools, you know,

00:36:03,280 --> 00:36:09,640
when you have power and agency over these tools, and showing people the value of that,

00:36:09,640 --> 00:36:11,720
that's really important.

00:36:11,720 --> 00:36:18,730
So what I just want to invite you to do is see what you're doing as part of Freire's

00:36:18,730 --> 00:36:20,490
liberatory practice.

00:36:20,490 --> 00:36:24,480
And see that your joy of computing isn't just for fun.

00:36:24,480 --> 00:36:25,930
It's for a better future.

00:36:25,930 --> 00:36:26,930
So...

00:36:26,930 --> 00:36:27,930
Thank you very much.

00:36:27,930 --> 00:36:32,100

YouTube URL: https://www.youtube.com/watch?v=hmFO5_llRLM


