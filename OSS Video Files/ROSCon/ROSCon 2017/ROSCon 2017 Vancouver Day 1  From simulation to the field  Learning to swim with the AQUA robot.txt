Title: ROSCon 2017 Vancouver Day 1  From simulation to the field  Learning to swim with the AQUA robot
Publication date: 2021-03-28
Playlist: ROSCon 2017
Description: 
	Unaltered video by Open Robotics from http://roscon.ros.org/2017 under the Attribution-NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) License
Captions: 
	00:00:00,210 --> 00:00:05,609
all right so my name is Juan Camilo

00:00:03,060 --> 00:00:08,849
Juan Camilo and I'm going to be talking

00:00:05,609 --> 00:00:10,920
about the tips the tricks and hacks and

00:00:08,849 --> 00:00:15,120
other like principle design decisions

00:00:10,920 --> 00:00:21,330
that we used to train underwater robot

00:00:15,120 --> 00:00:24,420
how to swim so the goal of our lab is to

00:00:21,330 --> 00:00:28,500
develop rapid systems that autonomous

00:00:24,420 --> 00:00:30,000
systems for data collection so the idea

00:00:28,500 --> 00:00:33,300
is that we would like to provide tools

00:00:30,000 --> 00:00:35,309
for scientists to collect data in

00:00:33,300 --> 00:00:38,340
situations where it would be really hard

00:00:35,309 --> 00:00:42,210
to control a robot with a joypad or some

00:00:38,340 --> 00:00:45,570
other user interface so in this talk I

00:00:42,210 --> 00:00:48,899
will talk about mainly three things I

00:00:45,570 --> 00:00:50,550
will describe the Aqua robot then I'll

00:00:48,899 --> 00:00:52,850
describe our simulation environment

00:00:50,550 --> 00:00:56,550
which we develop on top of gazebo and

00:00:52,850 --> 00:00:59,399
finally I'll talk about a bit about our

00:00:56,550 --> 00:01:02,149
experience with applying reinforcement

00:00:59,399 --> 00:01:06,210
learning algorithms to to the Aqua robot

00:01:02,149 --> 00:01:08,189
so the Aqua robot is an Evo six legged

00:01:06,210 --> 00:01:12,210
robot capable capable of autonomous

00:01:08,189 --> 00:01:14,340
operation it's its propulsion propulsion

00:01:12,210 --> 00:01:16,790
system it's based on six flippers so it

00:01:14,340 --> 00:01:18,509
just moved its foot person and it gets a

00:01:16,790 --> 00:01:21,689
propulsion in the opposite direction

00:01:18,509 --> 00:01:25,740
it's based on the Rex walking platform

00:01:21,689 --> 00:01:28,829
and but it was this underwater version

00:01:25,740 --> 00:01:31,530
was developed at McGill University we

00:01:28,829 --> 00:01:33,360
used to be one of the only two labs in

00:01:31,530 --> 00:01:35,369
the world using this robot but in the

00:01:33,360 --> 00:01:38,670
past few years there's more people

00:01:35,369 --> 00:01:42,390
getting them so I encourage you to go

00:01:38,670 --> 00:01:45,439
visit their websites to see what they're

00:01:42,390 --> 00:01:47,909
up to how they're using the Aqua robot

00:01:45,439 --> 00:01:49,590
the acrobat the hardware is very similar

00:01:47,909 --> 00:01:51,570
to what you would find inside a drone

00:01:49,590 --> 00:01:53,490
it's essentially a low-level computer

00:01:51,570 --> 00:01:56,070
that interacts with the motors internal

00:01:53,490 --> 00:01:58,799
sensors power management high-level

00:01:56,070 --> 00:02:02,219
computer that be used for a vision tasks

00:01:58,799 --> 00:02:06,360
and some where we actually put all our

00:02:02,219 --> 00:02:08,819
raw stack and all of it is connected

00:02:06,360 --> 00:02:11,640
through Ethernet switch which we can

00:02:08,819 --> 00:02:13,800
also connect to some external payload

00:02:11,640 --> 00:02:16,590
for example we connected two acoustic

00:02:13,800 --> 00:02:19,890
devices for localization underwater we

00:02:16,590 --> 00:02:21,860
also have a fiber-optic to Ethernet link

00:02:19,890 --> 00:02:26,280
which allows us to connect an external

00:02:21,860 --> 00:02:29,820
PC for monitoring or deploying other

00:02:26,280 --> 00:02:31,080
algorithms so I'll talk a little bit

00:02:29,820 --> 00:02:33,420
about the software that we use for

00:02:31,080 --> 00:02:35,790
control so we have the control tech tab

00:02:33,420 --> 00:02:39,000
it's running one kilohertz a real time

00:02:35,790 --> 00:02:42,930
loop this is what interacts directly

00:02:39,000 --> 00:02:45,959
with the with the sensors the low-level

00:02:42,930 --> 00:02:49,560
sensors and motors then we have the

00:02:45,959 --> 00:02:54,300
vision stack where we drop all our Russ

00:02:49,560 --> 00:02:57,000
software so we have some Russ interface

00:02:54,300 --> 00:02:59,520
for the hardware some respect use for

00:02:57,000 --> 00:03:04,290
navigation and some package for user

00:02:59,520 --> 00:03:05,940
interface and then another bit part of

00:03:04,290 --> 00:03:08,370
what I'm going to be presenting today

00:03:05,940 --> 00:03:10,980
it's our controller learning

00:03:08,370 --> 00:03:14,630
architecture which is also built on top

00:03:10,980 --> 00:03:17,760
of rust so the hardware interface is

00:03:14,630 --> 00:03:20,400
essentially one package that defines all

00:03:17,760 --> 00:03:22,769
the messages that we use internally for

00:03:20,400 --> 00:03:28,049
passing data around about the robot

00:03:22,769 --> 00:03:30,060
state and one package it acts as the

00:03:28,049 --> 00:03:33,900
layer between the low level control and

00:03:30,060 --> 00:03:36,900
Russ for navigation we have the echo to

00:03:33,900 --> 00:03:39,690
pilot package which implements

00:03:36,900 --> 00:03:41,790
trajectory tracking via Y points so we

00:03:39,690 --> 00:03:44,280
control the robots pose by standing by

00:03:41,790 --> 00:03:48,209
points on its 6 DOF

00:03:44,280 --> 00:03:50,760
world and we use some extra packages to

00:03:48,209 --> 00:03:55,350
transform the sensor streams into the

00:03:50,760 --> 00:03:58,230
Ross and TF convert conventions for the

00:03:55,350 --> 00:04:03,630
user interface we used to use we

00:03:58,230 --> 00:04:04,920
modified the joy package to also to

00:04:03,630 --> 00:04:06,630
interact directly with our autopilot

00:04:04,920 --> 00:04:10,320
with this idea of selling weapons to

00:04:06,630 --> 00:04:12,930
change the robots pose so all these Ross

00:04:10,320 --> 00:04:17,070
based software has enabled us to for

00:04:12,930 --> 00:04:20,609
example program the robot to perform

00:04:17,070 --> 00:04:23,160
certain maneuvers 22.8 sensors in any

00:04:20,609 --> 00:04:25,800
direction that we want we can do this

00:04:23,160 --> 00:04:27,449
using just the autopilot

00:04:25,800 --> 00:04:29,190
but we have to spend a lot of time

00:04:27,449 --> 00:04:32,910
cleaning all the controller parameters

00:04:29,190 --> 00:04:34,470
to get all these maneuvers so tuning

00:04:32,910 --> 00:04:39,750
these controllers it probably took a

00:04:34,470 --> 00:04:41,610
couple I would say days so that's why

00:04:39,750 --> 00:04:45,030
we're looking into learning this

00:04:41,610 --> 00:04:47,310
controllers instead another application

00:04:45,030 --> 00:04:50,150
that we were going to be presenting that

00:04:47,310 --> 00:04:53,430
I Ross it's a convoying with the robot

00:04:50,150 --> 00:04:55,919
so the idea is that we can have one

00:04:53,430 --> 00:04:58,169
robot that deals with global

00:04:55,919 --> 00:05:01,380
localization using an acoustic sensor

00:04:58,169 --> 00:05:04,349
and we let a robot behind it carry

00:05:01,380 --> 00:05:06,720
another sensor payload and not worry

00:05:04,349 --> 00:05:09,479
about a lot about localization or

00:05:06,720 --> 00:05:13,289
control we just put in there really

00:05:09,479 --> 00:05:17,190
simple tracking controller reactive

00:05:13,289 --> 00:05:19,530
controller to follow the leader so now

00:05:17,190 --> 00:05:21,599
we can go into the description of our

00:05:19,530 --> 00:05:23,699
simulated environment that enable a lot

00:05:21,599 --> 00:05:28,590
of the development and debugging of this

00:05:23,699 --> 00:05:31,440
software so going back to that software

00:05:28,590 --> 00:05:33,210
overview we just swap out our control

00:05:31,440 --> 00:05:36,320
stack and our vision stack and we just

00:05:33,210 --> 00:05:39,330
put gazebo in so we develop a couple

00:05:36,320 --> 00:05:42,120
plugins and using a couple other plugins

00:05:39,330 --> 00:05:44,669
that existed already to emulate the

00:05:42,120 --> 00:05:46,110
simulate the emulate the Aqua robot the

00:05:44,669 --> 00:05:49,889
hardwood and simulate the underwater

00:05:46,110 --> 00:05:52,349
robot dynamics that thing is running at

00:05:49,889 --> 00:05:55,259
one kilohertz just as the real hardware

00:05:52,349 --> 00:05:59,460
and then developing machine we just run

00:05:55,259 --> 00:06:01,979
the rest of our software stack so

00:05:59,460 --> 00:06:05,159
emulating the echo hardware consists of

00:06:01,979 --> 00:06:09,990
a plug-in that just influenced the same

00:06:05,159 --> 00:06:12,750
interfaces or aqua hardware package we

00:06:09,990 --> 00:06:14,990
use the IMU sensor plugin from Casey

00:06:12,750 --> 00:06:18,539
Borras that works fine for our

00:06:14,990 --> 00:06:20,550
simulations then we add two plugins to

00:06:18,539 --> 00:06:24,270
simulate the additional underwater

00:06:20,550 --> 00:06:25,830
forces and the propulsion model since

00:06:24,270 --> 00:06:30,870
we're not simulating the fluid around

00:06:25,830 --> 00:06:32,940
the robot so for simulating the

00:06:30,870 --> 00:06:36,050
hydrodynamics what we do it's very

00:06:32,940 --> 00:06:39,160
similar to what the

00:06:36,050 --> 00:06:41,840
dragged from the presentation on

00:06:39,160 --> 00:06:44,990
soldering cars we just editing as

00:06:41,840 --> 00:06:48,349
external for external forces we let od'd

00:06:44,990 --> 00:06:53,629
do its own integration loop as usual and

00:06:48,349 --> 00:06:57,860
then we model drag added mass buoyancy

00:06:53,629 --> 00:07:00,949
as forces that we need to add to the

00:06:57,860 --> 00:07:02,810
simulation so we have two ways of

00:07:00,949 --> 00:07:04,729
simulating the hydrodynamics one is

00:07:02,810 --> 00:07:07,819
using a box model so we just go through

00:07:04,729 --> 00:07:11,569
all the geometry of the robot and just

00:07:07,819 --> 00:07:14,650
put boxes around every link and compute

00:07:11,569 --> 00:07:18,229
the drag assuming that every link is box

00:07:14,650 --> 00:07:20,720
we also implemented one idea that was

00:07:18,229 --> 00:07:25,069
publishing SIGGRAPH a couple years ago

00:07:20,720 --> 00:07:27,199
which is based on the mesh of the of the

00:07:25,069 --> 00:07:31,849
geometry so I'll explain a little bit

00:07:27,199 --> 00:07:33,710
more about the about the mesh so for the

00:07:31,849 --> 00:07:37,490
mesh matter so for buoyancy what we do

00:07:33,710 --> 00:07:40,280
is we traverse the whole geometry three

00:07:37,490 --> 00:07:42,319
of the of the robot we compute the

00:07:40,280 --> 00:07:43,940
volume for each link and we applied the

00:07:42,319 --> 00:07:48,050
buoyancy force that it's proportional to

00:07:43,940 --> 00:07:49,460
the volume of displaced liquid and that

00:07:48,050 --> 00:07:50,990
forces in going in the opposite

00:07:49,460 --> 00:07:53,479
direction of of the gravity

00:07:50,990 --> 00:07:56,870
gravitational force ideally you should

00:07:53,479 --> 00:07:58,969
be not adding two different forces like

00:07:56,870 --> 00:08:03,819
gravity and buoyancy and then

00:07:58,969 --> 00:08:07,400
integrating that but rather you should

00:08:03,819 --> 00:08:09,650
consider in like OD internally

00:08:07,400 --> 00:08:11,840
integrates gravity separately so we

00:08:09,650 --> 00:08:14,690
should be putting buoyancy there but

00:08:11,840 --> 00:08:17,090
since that's not directly available

00:08:14,690 --> 00:08:21,319
through the API we just did it with an

00:08:17,090 --> 00:08:23,630
external force for added mass and drag

00:08:21,319 --> 00:08:28,729
what we do is we take the collision

00:08:23,630 --> 00:08:31,370
geometry of each link so the collision

00:08:28,729 --> 00:08:33,440
geometry in our case it's a really

00:08:31,370 --> 00:08:36,800
really really simplified version of the

00:08:33,440 --> 00:08:39,770
visual geometry so it's like one percent

00:08:36,800 --> 00:08:40,310
of the vertices and then we can

00:08:39,770 --> 00:08:43,459
pre-compute

00:08:40,310 --> 00:08:45,380
two matrices that essentially there's

00:08:43,459 --> 00:08:48,800
our linear relationships between

00:08:45,380 --> 00:08:49,790
velocities and forces so one is the

00:08:48,800 --> 00:08:53,420
direct answer

00:08:49,790 --> 00:08:54,860
it says drag times velocity it produces

00:08:53,420 --> 00:08:58,190
some force in the opposite direction of

00:08:54,860 --> 00:09:01,160
the displacement so model side the

00:08:58,190 --> 00:09:04,790
hydrodynamic drag and we have added mass

00:09:01,160 --> 00:09:05,330
which means when the robot is moving

00:09:04,790 --> 00:09:08,300
under water

00:09:05,330 --> 00:09:10,280
it's also displacing some liquid so when

00:09:08,300 --> 00:09:12,800
it's stopping it has to all stop the

00:09:10,280 --> 00:09:17,990
liquid so it acts the water around it

00:09:12,800 --> 00:09:20,660
add mask to the system so this is model

00:09:17,990 --> 00:09:23,150
as a matrix multiplication that produces

00:09:20,660 --> 00:09:25,100
momentum so essentially added mass here

00:09:23,150 --> 00:09:28,780
it's mass times velocity

00:09:25,100 --> 00:09:31,790
it gives momentum to get it as a

00:09:28,780 --> 00:09:33,860
external force we just integrated over

00:09:31,790 --> 00:09:38,260
time and we get an expression for the

00:09:33,860 --> 00:09:38,260
force that we need to apply to each link

00:09:38,470 --> 00:09:44,720
for the flippers we could just say use

00:09:42,680 --> 00:09:47,090
the hydrodynamics that we're already

00:09:44,720 --> 00:09:50,210
simulating and see is that this is trust

00:09:47,090 --> 00:09:51,680
well for periodic motion all these

00:09:50,210 --> 00:09:54,740
forces will cancel out and the robot

00:09:51,680 --> 00:09:57,560
will not move propulsion the way it

00:09:54,740 --> 00:09:59,690
happens with fish and with flexible

00:09:57,560 --> 00:10:02,630
flippers in our case is that when the

00:09:59,690 --> 00:10:06,410
flipper is moving it's shedding vortices

00:10:02,630 --> 00:10:08,300
and when the flipper hits one of the

00:10:06,410 --> 00:10:10,160
vorticity that it has shed before that

00:10:08,300 --> 00:10:13,610
produces some force some propulsive

00:10:10,160 --> 00:10:15,590
force in the forward direction so since

00:10:13,610 --> 00:10:17,510
we are not simulating the fluid and in

00:10:15,590 --> 00:10:21,020
simulating the fluid will take a long

00:10:17,510 --> 00:10:24,230
time we just use a very simple empirical

00:10:21,020 --> 00:10:26,780
model that essentially produces a

00:10:24,230 --> 00:10:29,150
propulsive force that is proportional to

00:10:26,780 --> 00:10:30,740
the amplitude of the periodic motion and

00:10:29,150 --> 00:10:36,470
inversely proportional to the frequency

00:10:30,740 --> 00:10:39,320
so that we reuse the parameters that

00:10:36,470 --> 00:10:45,650
were found on the on the reference by

00:10:39,320 --> 00:10:47,870
Hamilton and now and it seems to work at

00:10:45,650 --> 00:10:49,670
least it gives possible results so

00:10:47,870 --> 00:10:52,220
here's an example of a simulation on

00:10:49,670 --> 00:10:54,350
where we were just dropping the robot

00:10:52,220 --> 00:10:58,070
under water and seeing how it behaved

00:10:54,350 --> 00:11:00,650
and so our idea is that one of these

00:10:58,070 --> 00:11:02,550
experiments like learning how to

00:11:00,650 --> 00:11:04,920
stabilize

00:11:02,550 --> 00:11:07,620
if we wanted to run that experiment on

00:11:04,920 --> 00:11:09,240
the real robot we would have to pick up

00:11:07,620 --> 00:11:11,040
the robot from it into the pool

00:11:09,240 --> 00:11:13,830
go to the bottom of the pool pick up the

00:11:11,040 --> 00:11:16,650
robot again throw it again so the using

00:11:13,830 --> 00:11:20,570
simulation enables us to avoid that kind

00:11:16,650 --> 00:11:22,440
of work so is this simulation accurate

00:11:20,570 --> 00:11:25,530
probably not because we're not

00:11:22,440 --> 00:11:28,380
simulating vortex shedding or or the

00:11:25,530 --> 00:11:31,070
whole fluid that's moving around the

00:11:28,380 --> 00:11:35,100
robot system but it gives a reasonable

00:11:31,070 --> 00:11:40,140
or plausible behavior prediction of how

00:11:35,100 --> 00:11:42,450
the system would behave this we've found

00:11:40,140 --> 00:11:44,610
surprisingly that some of the controller

00:11:42,450 --> 00:11:46,530
parameters for the autopilot on the real

00:11:44,610 --> 00:11:48,570
robot actually worked on the simulation

00:11:46,530 --> 00:11:51,360
so that must mean that maybe the

00:11:48,570 --> 00:11:53,910
simulation is capturing some of the some

00:11:51,360 --> 00:11:58,350
of the effects that we see in the real

00:11:53,910 --> 00:12:02,010
robot and well we are also looking at

00:11:58,350 --> 00:12:03,900
improving the these models with real

00:12:02,010 --> 00:12:05,520
experiment data so the idea is to use

00:12:03,900 --> 00:12:09,330
the simulation that some sort of

00:12:05,520 --> 00:12:15,080
informative mean prior and and then add

00:12:09,330 --> 00:12:17,250
corrections from the stochastic world

00:12:15,080 --> 00:12:19,140
finally I'm going to talk a little bit

00:12:17,250 --> 00:12:25,950
about the architecture for motor control

00:12:19,140 --> 00:12:28,110
learning so I guess the whole idea is

00:12:25,950 --> 00:12:32,360
that we're going to use reinforcement

00:12:28,110 --> 00:12:35,090
learning where an agent a software agent

00:12:32,360 --> 00:12:38,480
it's presented with two streams

00:12:35,090 --> 00:12:43,080
uninterpreted streams of sensor data and

00:12:38,480 --> 00:12:46,440
control data and it's going to learn how

00:12:43,080 --> 00:12:49,470
to act or how to perform a task we buy a

00:12:46,440 --> 00:12:52,140
trial and error and we usually specify

00:12:49,470 --> 00:12:54,690
the task with our some objective or

00:12:52,140 --> 00:12:57,270
reward function and the problem here is

00:12:54,690 --> 00:12:59,910
then the agent has to find a controller

00:12:57,270 --> 00:13:06,060
that realizes the task or maximizing

00:12:59,910 --> 00:13:08,100
reward so now what the whole idea of

00:13:06,060 --> 00:13:11,370
reinforcement learning algorithms is

00:13:08,100 --> 00:13:13,470
that you replace your design effort

00:13:11,370 --> 00:13:14,880
where you're trying to figure out what

00:13:13,470 --> 00:13:15,410
are the situations that the robot is

00:13:14,880 --> 00:13:17,300
going to

00:13:15,410 --> 00:13:20,769
contrary in the future with letting the

00:13:17,300 --> 00:13:20,769
robot do experiments by itself

00:13:20,870 --> 00:13:24,949
well the prom with trial and error is

00:13:22,850 --> 00:13:27,470
that again going back to that robot

00:13:24,949 --> 00:13:30,620
diving example is that if your robot

00:13:27,470 --> 00:13:34,939
does not know how to recover then you

00:13:30,620 --> 00:13:36,470
need a person to go pick it up so an

00:13:34,939 --> 00:13:38,149
alternative to robot experiment and is

00:13:36,470 --> 00:13:40,009
like the vision of where we want to get

00:13:38,149 --> 00:13:41,449
to is that we first learned the basics

00:13:40,009 --> 00:13:44,329
on a local simulator like for example

00:13:41,449 --> 00:13:46,250
there on some airplane simulator where

00:13:44,329 --> 00:13:48,439
people are pushing you and then once

00:13:46,250 --> 00:13:50,360
your instructor thinks you're ready then

00:13:48,439 --> 00:13:53,079
you can try your skills under on the

00:13:50,360 --> 00:13:56,480
real deal and see try to minimize

00:13:53,079 --> 00:14:01,550
failures by minimizing interactions with

00:13:56,480 --> 00:14:03,879
the real system so one thing that we

00:14:01,550 --> 00:14:06,110
found when trying to implement

00:14:03,879 --> 00:14:08,839
reinforcement learning algorithms with

00:14:06,110 --> 00:14:11,720
Ross enable robot is that there's no

00:14:08,839 --> 00:14:13,639
straightforward way like you you have

00:14:11,720 --> 00:14:17,329
all this generic reinforcement learning

00:14:13,639 --> 00:14:22,970
libraries and some very specific robot

00:14:17,329 --> 00:14:24,470
software it it was not clear for us how

00:14:22,970 --> 00:14:26,240
to glue everything together so here's

00:14:24,470 --> 00:14:30,589
like our attempted solution at that

00:14:26,240 --> 00:14:33,759
problem so for my own research I

00:14:30,589 --> 00:14:36,199
developed this library of a suite of

00:14:33,759 --> 00:14:38,660
reinforcement learning algorithms that I

00:14:36,199 --> 00:14:42,259
think applicable to robotics because of

00:14:38,660 --> 00:14:44,269
their data efficiency so and I

00:14:42,259 --> 00:14:46,670
implemented a bunch of different

00:14:44,269 --> 00:14:50,029
controllers for different tasks I'm

00:14:46,670 --> 00:14:51,769
going to be releasing this code because

00:14:50,029 --> 00:14:53,569
I like people to reproduce the

00:14:51,769 --> 00:14:56,420
experiments that I've been doing on my

00:14:53,569 --> 00:14:58,399
research so but if it's not there yet

00:14:56,420 --> 00:15:02,540
but if you want it just contact me and I

00:14:58,399 --> 00:15:05,630
can send you the code the other building

00:15:02,540 --> 00:15:10,279
block of our learning architecture are

00:15:05,630 --> 00:15:12,980
this marshaling task marshaling module

00:15:10,279 --> 00:15:15,470
and the glue that connects the

00:15:12,980 --> 00:15:21,050
reinforcement learning library with Ross

00:15:15,470 --> 00:15:23,809
so our what we usually do in our

00:15:21,050 --> 00:15:25,220
experiments is a we we don't learn just

00:15:23,809 --> 00:15:27,290
one learning experiment because

00:15:25,220 --> 00:15:28,060
deployments are costly we won't want to

00:15:27,290 --> 00:15:29,890
learn multiple

00:15:28,060 --> 00:15:32,560
tasks at the same time so we led the

00:15:29,890 --> 00:15:34,990
system tried task 1 then task 2

00:15:32,560 --> 00:15:38,010
sequentially and offload all the

00:15:34,990 --> 00:15:40,900
learning to happen in some remote server

00:15:38,010 --> 00:15:43,740
since all this learning can happen at

00:15:40,900 --> 00:15:46,210
different times we need some way of

00:15:43,740 --> 00:15:48,940
triggering when the robot is ready to

00:15:46,210 --> 00:15:51,970
gather more experience so that's what a

00:15:48,940 --> 00:15:55,320
commercial does and then the negative us

00:15:51,970 --> 00:15:58,360
package what it does it converting the

00:15:55,320 --> 00:16:00,340
interpreted topics from from the ross

00:15:58,360 --> 00:16:04,360
world into the uninterpreted sensor

00:16:00,340 --> 00:16:08,200
streams that our algorithm expects and

00:16:04,360 --> 00:16:11,290
the way we do this is we just wrote a

00:16:08,200 --> 00:16:14,680
simple note that converts topics to

00:16:11,290 --> 00:16:17,260
sensor stream to RL streams essentially

00:16:14,680 --> 00:16:20,290
we have a node that reads the mo file

00:16:17,260 --> 00:16:24,250
that tells it subscribe to all these

00:16:20,290 --> 00:16:26,560
topics and put them out in some desired

00:16:24,250 --> 00:16:29,530
format usually just a single vector and

00:16:26,560 --> 00:16:31,120
we have a similar thing like for any

00:16:29,530 --> 00:16:33,400
control command that comes from the

00:16:31,120 --> 00:16:37,680
controller we can tell it publish it to

00:16:33,400 --> 00:16:39,940
all these different control topics

00:16:37,680 --> 00:16:43,980
finally like this is like how we

00:16:39,940 --> 00:16:50,380
connected everything together so all our

00:16:43,980 --> 00:16:52,980
the whole PI point is we have a robot

00:16:50,380 --> 00:16:55,660
that it's executing some controllers

00:16:52,980 --> 00:16:57,610
then using our learning interface with

00:16:55,660 --> 00:17:00,310
some experience they taught by out to

00:16:57,610 --> 00:17:02,290
the outside learning server and then

00:17:00,310 --> 00:17:06,310
over time we're getting new parameters

00:17:02,290 --> 00:17:07,990
to try out and we use a joypad to be

00:17:06,310 --> 00:17:10,570
able to drive the robot to some initial

00:17:07,990 --> 00:17:14,560
state and then say like now you can try

00:17:10,570 --> 00:17:17,620
it again so here's an example of a

00:17:14,560 --> 00:17:19,750
learning run for a single task here the

00:17:17,620 --> 00:17:23,110
robot is trying to learn how to do a

00:17:19,750 --> 00:17:25,210
barrel roll while swimming so it's

00:17:23,110 --> 00:17:27,940
getting you data it's building a new

00:17:25,210 --> 00:17:31,830
model and using this model to optimize a

00:17:27,940 --> 00:17:35,110
controller and over time you see it it's

00:17:31,830 --> 00:17:37,960
pretty quickly it can it can get to

00:17:35,110 --> 00:17:40,610
learn the task here swimming like doing

00:17:37,960 --> 00:17:43,610
that called screw maneuver

00:17:40,610 --> 00:17:49,160
the main problem with this is that

00:17:43,610 --> 00:17:51,350
learning still takes a long time so even

00:17:49,160 --> 00:17:53,960
if we were able to maximize the amount

00:17:51,350 --> 00:17:56,420
of minimize the idle time of the robot

00:17:53,960 --> 00:17:59,210
in the pool still these experience could

00:17:56,420 --> 00:18:04,820
take up to four or five hours and that

00:17:59,210 --> 00:18:05,840
can be quite tiring for the people for

00:18:04,820 --> 00:18:10,490
the graduate students running the

00:18:05,840 --> 00:18:15,290
experiments so what's next on our world

00:18:10,490 --> 00:18:18,010
will we have the tools for training

00:18:15,290 --> 00:18:23,210
robots in simulation and now we want to

00:18:18,010 --> 00:18:25,130
run the experiments that will close the

00:18:23,210 --> 00:18:27,320
loop between the two like learning

00:18:25,130 --> 00:18:29,230
simulation gather new experience in the

00:18:27,320 --> 00:18:34,310
real world update the simulator and

00:18:29,230 --> 00:18:37,910
continue improving over time one of the

00:18:34,310 --> 00:18:39,770
things that we found is like the machine

00:18:37,910 --> 00:18:42,590
learning community has done a lot of

00:18:39,770 --> 00:18:45,050
progress because they have tools for

00:18:42,590 --> 00:18:49,360
easily implementing learning algorithms

00:18:45,050 --> 00:18:51,560
and running experiments with them and

00:18:49,360 --> 00:18:54,080
the question is like is what they're

00:18:51,560 --> 00:18:56,390
doing robotics well some people would

00:18:54,080 --> 00:19:00,320
say no because it's missing all if this

00:18:56,390 --> 00:19:02,900
our idealized world so I guess one thing

00:19:00,320 --> 00:19:04,580
that we would like I would like to see

00:19:02,900 --> 00:19:07,010
in the communities like more of these

00:19:04,580 --> 00:19:09,140
ease of running experiments on Ross

00:19:07,010 --> 00:19:11,000
enabled robots just as you would run

00:19:09,140 --> 00:19:15,350
some experiment on one of these robots

00:19:11,000 --> 00:19:17,270
cool robots so in summary I gave an

00:19:15,350 --> 00:19:20,030
overview of the packages that we use for

00:19:17,270 --> 00:19:21,980
motor control on the robot I told you a

00:19:20,030 --> 00:19:24,290
little bit about about our simulation

00:19:21,980 --> 00:19:26,450
environment and also give you a brief

00:19:24,290 --> 00:19:28,220
description of our reinforcement

00:19:26,450 --> 00:19:30,260
learning pipeline and how we plan to use

00:19:28,220 --> 00:19:32,780
it for simulator simulator to robot

00:19:30,260 --> 00:19:36,410
transfer we will be releasing some of

00:19:32,780 --> 00:19:38,390
these packages in our github so I think

00:19:36,410 --> 00:19:40,190
in the next couple of weeks you should

00:19:38,390 --> 00:19:42,020
be seen some of them again if you want

00:19:40,190 --> 00:19:44,990
to get them now just contact me and

00:19:42,020 --> 00:19:47,270
we'll see what we can do so I would like

00:19:44,990 --> 00:19:49,130
to thank you the audience and also all

00:19:47,270 --> 00:19:50,780
my colleagues at the model robotics

00:19:49,130 --> 00:19:54,130
laboratory who have contributed to this

00:19:50,780 --> 00:19:54,130
project thank you

00:19:57,650 --> 00:20:03,270
Thank You Juan Camila guys general Han

00:20:01,650 --> 00:20:04,440
can you come up start getting set up I

00:20:03,270 --> 00:20:05,000
think we have time for one quick

00:20:04,440 --> 00:20:12,900
question

00:20:05,000 --> 00:20:14,970
fairness questions so you showed your

00:20:12,900 --> 00:20:17,429
learning with this aqua robot but with

00:20:14,970 --> 00:20:19,350
the interface between learning and Ross

00:20:17,429 --> 00:20:22,920
and also your learning code work on

00:20:19,350 --> 00:20:26,400
other platforms already right now yes

00:20:22,920 --> 00:20:28,620
because our our code it's a robotic

00:20:26,400 --> 00:20:32,820
nastic like we're all converting all

00:20:28,620 --> 00:20:35,070
these topics into just uninterpreted

00:20:32,820 --> 00:20:40,350
sensor streams so you could potentially

00:20:35,070 --> 00:20:41,920
use it with any other robot okay one

00:20:40,350 --> 00:20:45,319
more thank you for one

00:20:41,920 --> 00:20:45,319

YouTube URL: https://www.youtube.com/watch?v=wfUa98Cu22w


