Title: ROSCon 2017 Vancouver Day 1  How to select a 3D sensor technology
Publication date: 2021-03-28
Playlist: ROSCon 2017
Description: 
	Unaltered video by Open Robotics from http://roscon.ros.org/2017 under the Attribution-NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) License
Captions: 
	00:00:00,740 --> 00:00:06,060
I'm Chris osteoid CTO of carne robotics

00:00:04,410 --> 00:00:08,670
and they'll be talking today about how

00:00:06,060 --> 00:00:11,700
we select 3d sensors in the systems that

00:00:08,670 --> 00:00:13,500
we build first just one slide on the

00:00:11,700 --> 00:00:14,820
company that I'm with we're seven year

00:00:13,500 --> 00:00:16,800
old spin-out from the National abadox

00:00:14,820 --> 00:00:18,840
Engineering Center which is part of CMU

00:00:16,800 --> 00:00:20,970
and the robotics Institute there but

00:00:18,840 --> 00:00:22,710
we're a for-profit company and we build

00:00:20,970 --> 00:00:24,720
sensors as well as custom robotic

00:00:22,710 --> 00:00:26,550
systems we have about 70 employees a

00:00:24,720 --> 00:00:29,039
half engineering and half sort of

00:00:26,550 --> 00:00:30,390
engineering sport that's our building

00:00:29,039 --> 00:00:32,460
which is an old steel mill in Pittsburgh

00:00:30,390 --> 00:00:36,450
that's been converted into this new

00:00:32,460 --> 00:00:38,670
high-tech space that we operated so the

00:00:36,450 --> 00:00:40,649
talk today is going to start with some

00:00:38,670 --> 00:00:42,360
criterion definitions of how we think

00:00:40,649 --> 00:00:43,890
about sensors and I'm gonna go through

00:00:42,360 --> 00:00:45,750
that very very briefly because of a

00:00:43,890 --> 00:00:48,719
woman a time and the bulk of the

00:00:45,750 --> 00:00:51,300
presentation is on 3d sensor modalities

00:00:48,719 --> 00:00:52,469
how they work and how we have evaluated

00:00:51,300 --> 00:00:55,500
them both qualitatively and

00:00:52,469 --> 00:00:57,780
quantitatively and hopefully this helps

00:00:55,500 --> 00:00:59,840
all of you select 3d sensors in future

00:00:57,780 --> 00:01:02,430
systems that you're building

00:00:59,840 --> 00:01:04,619
so we separate performance criteria or

00:01:02,430 --> 00:01:06,540
criteria into performance criteria and

00:01:04,619 --> 00:01:07,590
non performance criteria and the

00:01:06,540 --> 00:01:09,840
weighting of all these different

00:01:07,590 --> 00:01:12,360
criterias changes dramatically based on

00:01:09,840 --> 00:01:14,130
the platform the ear system is running

00:01:12,360 --> 00:01:17,189
the kind of application that you're

00:01:14,130 --> 00:01:18,540
developing for and the exact use of the

00:01:17,189 --> 00:01:20,549
3d sensor that you're trying to select

00:01:18,540 --> 00:01:22,560
so the field of view or the density or

00:01:20,549 --> 00:01:24,119
the data rate or the depth accuracy or

00:01:22,560 --> 00:01:26,549
the angular field of view or the angular

00:01:24,119 --> 00:01:28,110
density all of these things may have

00:01:26,549 --> 00:01:29,790
very different requirements for your

00:01:28,110 --> 00:01:33,060
application than other people's

00:01:29,790 --> 00:01:34,320
application and the importance of each

00:01:33,060 --> 00:01:36,509
of these is also going to change

00:01:34,320 --> 00:01:39,030
dramatically by all these sort of

00:01:36,509 --> 00:01:42,240
applications specific factors as well

00:01:39,030 --> 00:01:43,470
and I put these up here not to go

00:01:42,240 --> 00:01:45,240
through each one and point by point in

00:01:43,470 --> 00:01:47,850
terms of what is important where but

00:01:45,240 --> 00:01:50,009
just to urge all of you to think very

00:01:47,850 --> 00:01:51,630
critically about which are the critical

00:01:50,009 --> 00:01:54,390
ones for the application and the

00:01:51,630 --> 00:01:56,009
platform and the youth and the use that

00:01:54,390 --> 00:01:57,509
you are building for just because

00:01:56,009 --> 00:01:59,549
someone else is using a particular kind

00:01:57,509 --> 00:02:01,649
of sensor on the same platform if you

00:01:59,549 --> 00:02:04,250
have a different application you may be

00:02:01,649 --> 00:02:06,689
better served with a different 3d sensor

00:02:04,250 --> 00:02:09,119
and in terms of non performance criteria

00:02:06,689 --> 00:02:10,530
there's a whole host of other factors

00:02:09,119 --> 00:02:13,120
that we look at when we integrate

00:02:10,530 --> 00:02:16,450
sensors size weight power costs

00:02:13,120 --> 00:02:17,950
swap see is a big part of mobile robots

00:02:16,450 --> 00:02:20,260
especially things that are operating

00:02:17,950 --> 00:02:22,510
with their own power but ceiling

00:02:20,260 --> 00:02:24,010
vibration resistance the communication

00:02:22,510 --> 00:02:25,450
interface for this the sense of the

00:02:24,010 --> 00:02:28,120
you're selecting all these things can

00:02:25,450 --> 00:02:29,920
dramatically change the integration time

00:02:28,120 --> 00:02:33,370
and cost and should be considered

00:02:29,920 --> 00:02:36,549
carefully while building your system so

00:02:33,370 --> 00:02:39,370
going into some technologies 3d lighters

00:02:36,549 --> 00:02:41,500
are fairly simple from an architecture

00:02:39,370 --> 00:02:43,209
perspective they're very very tricky

00:02:41,500 --> 00:02:45,160
when you get down to the physics of it

00:02:43,209 --> 00:02:47,590
but you're basically sending out a pulse

00:02:45,160 --> 00:02:49,420
of light or a continuous wave of light

00:02:47,590 --> 00:02:52,180
and then you're doing some measurement

00:02:49,420 --> 00:02:54,599
on both your mission and the return of

00:02:52,180 --> 00:02:57,190
that emission from the environment

00:02:54,599 --> 00:02:59,230
normally these are built with multiple

00:02:57,190 --> 00:03:00,849
arrays of single point sensors and you

00:02:59,230 --> 00:03:03,190
actuate them in one or two degrees of

00:03:00,849 --> 00:03:06,489
freedom to build a 3d sensor from a 1d

00:03:03,190 --> 00:03:08,049
sensor or from a 2d sensor and at some

00:03:06,489 --> 00:03:10,860
point hopefully in the near future salt

00:03:08,049 --> 00:03:12,910
state lidar will be a more reliable and

00:03:10,860 --> 00:03:15,400
purchasable product we're not quite

00:03:12,910 --> 00:03:17,980
there yet but now we exciting time when

00:03:15,400 --> 00:03:19,829
that happens the really nice thing about

00:03:17,980 --> 00:03:22,329
ladar is that has a constant error model

00:03:19,829 --> 00:03:24,670
even in very long distances the one

00:03:22,329 --> 00:03:27,190
sigma accuracies are very good they can

00:03:24,670 --> 00:03:29,230
be very long range sensors but you have

00:03:27,190 --> 00:03:30,880
real limitations in data rate and in

00:03:29,230 --> 00:03:32,799
horizontal and vertical density and

00:03:30,880 --> 00:03:35,560
depending on the task of your system

00:03:32,799 --> 00:03:37,540
they may prevent or limit the use of 3d

00:03:35,560 --> 00:03:40,750
lidar which is where 3d time-of-flight

00:03:37,540 --> 00:03:43,060
cameras and stereo cameras can sometimes

00:03:40,750 --> 00:03:46,349
be more effective so time-of-flight

00:03:43,060 --> 00:03:50,290
cameras are similar to 3d lasers from a

00:03:46,349 --> 00:03:50,919
machine and detection perspective excuse

00:03:50,290 --> 00:03:54,310
me

00:03:50,919 --> 00:03:58,060
but instead of having a laser emitter

00:03:54,310 --> 00:04:00,160
they have a modulated IR emission and

00:03:58,060 --> 00:04:02,829
then you're actually doing that phase

00:04:00,160 --> 00:04:05,889
based extraction on every single pixel

00:04:02,829 --> 00:04:07,599
in a photo array instead of on a single

00:04:05,889 --> 00:04:09,299
Avalanche photodiode

00:04:07,599 --> 00:04:12,190
this means that you get much better

00:04:09,299 --> 00:04:13,780
density but because you're using a much

00:04:12,190 --> 00:04:15,970
smaller detector that's much lower power

00:04:13,780 --> 00:04:17,829
and because you're emitting a broader

00:04:15,970 --> 00:04:19,810
field of view into the environment

00:04:17,829 --> 00:04:21,640
instead of a single point laser the

00:04:19,810 --> 00:04:23,960
signal of noise extraction that a

00:04:21,640 --> 00:04:26,810
time-of-flight camera has to do is much

00:04:23,960 --> 00:04:28,310
here than a 3d laser and they this

00:04:26,810 --> 00:04:32,330
limits the range of these sensors as

00:04:28,310 --> 00:04:34,520
well as their accuracies there's also

00:04:32,330 --> 00:04:36,470
some range ambiguity problems due to the

00:04:34,520 --> 00:04:37,669
modulated waves and that means that you

00:04:36,470 --> 00:04:39,620
either have to operate these at lower

00:04:37,669 --> 00:04:41,539
data rates and have different

00:04:39,620 --> 00:04:44,330
frequencies that they operate at or they

00:04:41,539 --> 00:04:46,460
actually have fixed distances that you

00:04:44,330 --> 00:04:49,430
just can't see or that mask and appear

00:04:46,460 --> 00:04:52,340
to be other distances these are

00:04:49,430 --> 00:04:55,370
generally much denser than 3d lasers but

00:04:52,340 --> 00:04:57,139
have a higher power for the range that

00:04:55,370 --> 00:04:59,199
they're perceiving do the higher density

00:04:57,139 --> 00:05:02,630
and the way that are electrically built

00:04:59,199 --> 00:05:05,000
they have generally intensity outputs

00:05:02,630 --> 00:05:06,199
they almost act like a camera and they

00:05:05,000 --> 00:05:09,169
bring their own illumination with their

00:05:06,199 --> 00:05:10,880
IR emissions but they don't really have

00:05:09,169 --> 00:05:12,080
camera parameters that you can tune and

00:05:10,880 --> 00:05:12,889
this is both an advantage and a

00:05:12,080 --> 00:05:14,180
disadvantage

00:05:12,889 --> 00:05:16,310
depending on the environment that you're

00:05:14,180 --> 00:05:18,080
in sometimes you run into problems where

00:05:16,310 --> 00:05:19,789
an object very close to the time of

00:05:18,080 --> 00:05:22,159
flight camera returns so much energy

00:05:19,789 --> 00:05:23,659
back to the detector array that it has a

00:05:22,159 --> 00:05:26,120
hard time doing that signal-to-noise

00:05:23,659 --> 00:05:29,960
extraction process on far the range

00:05:26,120 --> 00:05:32,000
materials in the scene they're also

00:05:29,960 --> 00:05:34,460
susceptible to multi echo returns

00:05:32,000 --> 00:05:36,050
especially with things with specular

00:05:34,460 --> 00:05:39,409
highlights and I have some examples of

00:05:36,050 --> 00:05:41,870
this on later slides the last limitation

00:05:39,409 --> 00:05:43,639
and this again this may or may not be a

00:05:41,870 --> 00:05:45,620
limitation depending on the exact

00:05:43,639 --> 00:05:47,360
application that you're building for but

00:05:45,620 --> 00:05:49,940
they have a limited horizontal field of

00:05:47,360 --> 00:05:52,280
view it's very hard to both admit and

00:05:49,940 --> 00:05:54,380
detect this sort of information beyond

00:05:52,280 --> 00:05:56,500
you know sixty to eighty degrees though

00:05:54,380 --> 00:05:59,270
that may change in the future

00:05:56,500 --> 00:06:00,349
so stereo cameras work in a

00:05:59,270 --> 00:06:02,389
fundamentally different way they look

00:06:00,349 --> 00:06:05,000
they work the way our eyes do by

00:06:02,389 --> 00:06:06,440
visually detecting texture in the

00:06:05,000 --> 00:06:08,960
environment and then comparing that

00:06:06,440 --> 00:06:11,419
texture between left and right camera

00:06:08,960 --> 00:06:13,759
and the horizontal shift and texture

00:06:11,419 --> 00:06:15,710
position is inversely proportional to

00:06:13,759 --> 00:06:17,300
the distance that that texture is away

00:06:15,710 --> 00:06:18,440
so when you close one eye and then you

00:06:17,300 --> 00:06:19,759
close the other eye things that are

00:06:18,440 --> 00:06:21,289
close to your head move horizontally

00:06:19,759 --> 00:06:23,060
more than things that are very far away

00:06:21,289 --> 00:06:25,550
things that infinity don't move at all

00:06:23,060 --> 00:06:27,229
and if you have a very accurate model of

00:06:25,550 --> 00:06:29,210
the distortion of your camera lens and a

00:06:27,229 --> 00:06:30,860
very good understanding of the

00:06:29,210 --> 00:06:32,120
particular arrangement of the two lenses

00:06:30,860 --> 00:06:34,460
their distances and their are mounting

00:06:32,120 --> 00:06:36,440
angles you can relatively accurately

00:06:34,460 --> 00:06:37,430
construct the 3d scene that the cameras

00:06:36,440 --> 00:06:39,830
are looking at

00:06:37,430 --> 00:06:41,120
one thing that it's critical when you're

00:06:39,830 --> 00:06:43,400
looking at stereo cameras is their

00:06:41,120 --> 00:06:45,350
disparity search range this is the

00:06:43,400 --> 00:06:46,759
number of pixels in the scene that the

00:06:45,350 --> 00:06:48,590
camera is actually searching before it

00:06:46,759 --> 00:06:50,690
decides there's no match and I'm going

00:06:48,590 --> 00:06:52,490
to give up and the more pixels that you

00:06:50,690 --> 00:06:53,990
search across in a camera the closer to

00:06:52,490 --> 00:06:57,680
the camera that you're actually able to

00:06:53,990 --> 00:07:00,350
observe 3d objects so beyond or within

00:06:57,680 --> 00:07:02,240
that things that are closer than that

00:07:00,350 --> 00:07:06,500
minimum range are generally invisible to

00:07:02,240 --> 00:07:07,880
the camera the advantage of stereo is

00:07:06,500 --> 00:07:10,639
that you have very high resolution

00:07:07,880 --> 00:07:14,030
intensity but unlike these other forms

00:07:10,639 --> 00:07:17,000
of 3d sensors their accuracy is limited

00:07:14,030 --> 00:07:19,100
at range it's no longer a standard

00:07:17,000 --> 00:07:21,470
linear or constant error model it's

00:07:19,100 --> 00:07:23,780
actually a quadratic error model and as

00:07:21,470 --> 00:07:25,100
objects get farther away the cameras

00:07:23,780 --> 00:07:27,289
ability to perceive them accurately

00:07:25,100 --> 00:07:28,849
decreases just like my ability to

00:07:27,289 --> 00:07:30,860
perceive the distances to these chairs

00:07:28,849 --> 00:07:33,020
versus the back of the hall

00:07:30,860 --> 00:07:35,240
you know distance really affects the

00:07:33,020 --> 00:07:36,889
accuracy these cameras are totally

00:07:35,240 --> 00:07:38,270
passive which helps a lot in high

00:07:36,889 --> 00:07:39,919
security environments if you're

00:07:38,270 --> 00:07:43,190
operating outdoors with blowing dust or

00:07:39,919 --> 00:07:45,349
rain or snow or fog if you visually can

00:07:43,190 --> 00:07:46,940
see through the environment to what the

00:07:45,349 --> 00:07:48,770
camera needs to see so can the camera

00:07:46,940 --> 00:07:50,030
it's not emitting anything into the

00:07:48,770 --> 00:07:52,610
world that it's trying to extract

00:07:50,030 --> 00:07:56,090
information from so it's very robust to

00:07:52,610 --> 00:07:57,979
these sorts of experience but it's a

00:07:56,090 --> 00:07:59,510
very computationally expensive process

00:07:57,979 --> 00:08:00,740
to do the stereo matching this

00:07:59,510 --> 00:08:03,770
correspondence problem between the two

00:08:00,740 --> 00:08:05,510
cameras so if you're doing stereo

00:08:03,770 --> 00:08:07,190
processing on a general-purpose computer

00:08:05,510 --> 00:08:10,880
you're limited in resolution or

00:08:07,190 --> 00:08:13,580
framerate or over the it's very search

00:08:10,880 --> 00:08:15,409
range that's part of why we embedded the

00:08:13,580 --> 00:08:17,960
stereo matching process inside of an

00:08:15,409 --> 00:08:20,659
FPGA and our cameras that are the top

00:08:17,960 --> 00:08:22,010
two cameras here that reduces the power

00:08:20,659 --> 00:08:24,860
required to do the stereo matching

00:08:22,010 --> 00:08:29,720
process as well as reduce the latency to

00:08:24,860 --> 00:08:31,639
get the 3d data I talked earlier about

00:08:29,720 --> 00:08:33,229
the nonlinear error model here this

00:08:31,639 --> 00:08:35,270
gives you some sense of what I'm talking

00:08:33,229 --> 00:08:36,890
about the purple and the bottom is lidar

00:08:35,270 --> 00:08:38,450
and then all the other curves here are

00:08:36,890 --> 00:08:39,500
different stereo cameras with different

00:08:38,450 --> 00:08:41,870
bass lines and different horizontal

00:08:39,500 --> 00:08:43,610
fields of view and you can see that you

00:08:41,870 --> 00:08:45,860
know depending on the range you could

00:08:43,610 --> 00:08:47,750
have better or worse accuracy and this

00:08:45,860 --> 00:08:49,579
isn't a game this isn't a showstopper

00:08:47,750 --> 00:08:50,840
for lots of applications but the

00:08:49,579 --> 00:08:52,730
software you're building really needs

00:08:50,840 --> 00:08:57,590
understand the Sara model to accurately

00:08:52,730 --> 00:09:01,220
use and reasonably use stereo data I'm

00:08:57,590 --> 00:09:02,600
just gonna skip over that for time so I

00:09:01,220 --> 00:09:04,430
have here a couple slides on a

00:09:02,600 --> 00:09:06,950
qualitative comparison between we do LD

00:09:04,430 --> 00:09:08,930
we did between these sensors all these

00:09:06,950 --> 00:09:10,670
scenes are similar where we have a cart

00:09:08,930 --> 00:09:12,770
with a chessboard on it person walks

00:09:10,670 --> 00:09:14,300
into the scene bends down picks up a

00:09:12,770 --> 00:09:16,190
chess piece moves it to another spot and

00:09:14,300 --> 00:09:18,860
then walks out of the scene this is a

00:09:16,190 --> 00:09:20,720
rotating 3d laser first with 8 seconds

00:09:18,860 --> 00:09:22,130
of persisted data and then we speed up

00:09:20,720 --> 00:09:24,410
the rotation rate and just show one

00:09:22,130 --> 00:09:26,780
second of persisted data and you see

00:09:24,410 --> 00:09:28,100
intuitively the wide horizontal and

00:09:26,780 --> 00:09:31,400
vertical field of view of this rotating

00:09:28,100 --> 00:09:33,770
planar laser but because of the density

00:09:31,400 --> 00:09:36,470
and the persistence it's actually very

00:09:33,770 --> 00:09:38,780
difficult to make out the person and you

00:09:36,470 --> 00:09:42,350
have very different density in plane or

00:09:38,780 --> 00:09:46,040
in scan as well as scan to scan with the

00:09:42,350 --> 00:09:47,540
higher data rate you get more accurate

00:09:46,040 --> 00:09:50,180
information about the world temporarily

00:09:47,540 --> 00:09:52,190
but it becomes harder and harder to pick

00:09:50,180 --> 00:09:55,070
out different features in the scene

00:09:52,190 --> 00:09:56,870
because the density reductions so trying

00:09:55,070 --> 00:09:59,180
to write run an object classifier on

00:09:56,870 --> 00:10:00,860
this sort of data it's possible but

00:09:59,180 --> 00:10:02,270
you're you're adding a lot of difficulty

00:10:00,860 --> 00:10:04,070
your process and you also see that

00:10:02,270 --> 00:10:06,410
between the cart and the ground there's

00:10:04,070 --> 00:10:07,940
actually returns that are in the middle

00:10:06,410 --> 00:10:10,040
of the year these are mixed pixel

00:10:07,940 --> 00:10:11,720
returns where the laser beam it's partly

00:10:10,040 --> 00:10:12,740
on the card partly on the floor and you

00:10:11,720 --> 00:10:15,080
get a return in mid-space

00:10:12,740 --> 00:10:16,370
that's not actually a real return it's

00:10:15,080 --> 00:10:19,340
balanced between those two different

00:10:16,370 --> 00:10:21,710
objects this is the same scene from a

00:10:19,340 --> 00:10:24,080
low-cost time-of-flight camera sort of

00:10:21,710 --> 00:10:25,610
consumer grade and you see the much

00:10:24,080 --> 00:10:27,740
lower resolution much lower field of

00:10:25,610 --> 00:10:30,140
view much lower data rate in comparison

00:10:27,740 --> 00:10:31,760
to the star through the lighter that we

00:10:30,140 --> 00:10:33,980
were just looking at you see summer

00:10:31,760 --> 00:10:37,850
turns in the middle of the air that are

00:10:33,980 --> 00:10:40,850
just completely false and not not actual

00:10:37,850 --> 00:10:42,530
returns and you also see that about half

00:10:40,850 --> 00:10:45,050
the floor is missing and I can't really

00:10:42,530 --> 00:10:46,610
explain this this is shiny concrete

00:10:45,050 --> 00:10:49,850
floor you know a couple of meters away

00:10:46,610 --> 00:10:52,130
it doesn't really make any sense why

00:10:49,850 --> 00:10:53,480
half the floor is missing you see that

00:10:52,130 --> 00:10:56,180
the back of the card is very bowed and

00:10:53,480 --> 00:10:57,650
here we have flipped to a intensity view

00:10:56,180 --> 00:10:59,210
of the same data instead of an access

00:10:57,650 --> 00:11:00,800
coloring and you see the sort of

00:10:59,210 --> 00:11:02,130
flashlight effect you get when you're

00:11:00,800 --> 00:11:05,790
viewing this data because

00:11:02,130 --> 00:11:07,890
the ir admitted data are they the IR

00:11:05,790 --> 00:11:11,760
light that the camera is basing all of

00:11:07,890 --> 00:11:13,740
its Ranger turns off of this is an

00:11:11,760 --> 00:11:15,270
industrial camera and when someone walks

00:11:13,740 --> 00:11:17,490
in front of it it completely distorts

00:11:15,270 --> 00:11:18,540
the scene if you didn't know that was

00:11:17,490 --> 00:11:19,710
gonna happen that could really mess up

00:11:18,540 --> 00:11:21,930
the map that you're building of this

00:11:19,710 --> 00:11:23,670
environment and the similar thing

00:11:21,930 --> 00:11:25,440
happens to the background when the

00:11:23,670 --> 00:11:27,240
person's walking in front of it it will

00:11:25,440 --> 00:11:29,100
flip to a top-down view in a second and

00:11:27,240 --> 00:11:32,700
you actually see that it sort of ripples

00:11:29,100 --> 00:11:35,220
and rolls and waves the background again

00:11:32,700 --> 00:11:38,970
this is maybe due to multipath returns

00:11:35,220 --> 00:11:40,500
but it's not really explainable you also

00:11:38,970 --> 00:11:42,360
see a bunch of data in between the

00:11:40,500 --> 00:11:44,400
person and the back wall that they're

00:11:42,360 --> 00:11:46,230
standing in front of sort of the same

00:11:44,400 --> 00:11:48,690
sort of mixed pixel return problem that

00:11:46,230 --> 00:11:52,320
allowed our cameras have or that a 3d

00:11:48,690 --> 00:11:54,330
light arts have and you also see some

00:11:52,320 --> 00:11:59,610
pretty severe bowing to the concrete

00:11:54,330 --> 00:12:01,770
floor caused by multipath this is the

00:11:59,610 --> 00:12:03,150
same data from a stereo camera this is

00:12:01,770 --> 00:12:05,280
from the multi sense that camera that we

00:12:03,150 --> 00:12:08,580
build and you see that there's a lot

00:12:05,280 --> 00:12:11,070
higher density here but you now start to

00:12:08,580 --> 00:12:12,540
have that that range accuracy problem

00:12:11,070 --> 00:12:14,070
you know the data close to the cameras

00:12:12,540 --> 00:12:15,720
much more accurate than the data at that

00:12:14,070 --> 00:12:17,490
back wall which is three or four meters

00:12:15,720 --> 00:12:20,520
away there's a lot more range variance

00:12:17,490 --> 00:12:22,710
on that stack of panels behind our

00:12:20,520 --> 00:12:24,420
cameras can be built with color imagers

00:12:22,710 --> 00:12:25,830
so you have one-to-one perfectly

00:12:24,420 --> 00:12:28,530
correlated color with range which is

00:12:25,830 --> 00:12:30,510
useful in some applications but you also

00:12:28,530 --> 00:12:31,950
see a hole in this data in the floor and

00:12:30,510 --> 00:12:34,680
a bunch of returns below the floor

00:12:31,950 --> 00:12:37,020
surface this is actually an invalid

00:12:34,680 --> 00:12:38,670
returned the cameras seeing a light

00:12:37,020 --> 00:12:41,340
reflection of a light on the concrete

00:12:38,670 --> 00:12:43,170
floor and it can't differentiate that

00:12:41,340 --> 00:12:44,850
reflection from reality so it's actually

00:12:43,170 --> 00:12:46,470
giving us returns as though that light

00:12:44,850 --> 00:12:50,130
were reflected underneath the floor

00:12:46,470 --> 00:12:51,720
surface like a a mirror

00:12:50,130 --> 00:12:53,340
one nice feature about our cameras is

00:12:51,720 --> 00:12:56,460
you can dynamically adjust the stereo

00:12:53,340 --> 00:12:58,890
post threshold and change between very

00:12:56,460 --> 00:13:00,750
dense and less accurate data or you can

00:12:58,890 --> 00:13:02,760
tell the camera actually I just want the

00:13:00,750 --> 00:13:04,800
really confident data and I want less of

00:13:02,760 --> 00:13:06,030
it and you can dynamically change those

00:13:04,800 --> 00:13:08,640
sorts of variables as the cameras

00:13:06,030 --> 00:13:10,980
operating and you see here just the the

00:13:08,640 --> 00:13:13,530
density and the accuracy of close range

00:13:10,980 --> 00:13:16,100
of this data it's very useful in these

00:13:13,530 --> 00:13:16,100
sorts of ranges

00:13:16,680 --> 00:13:20,500
one other interesting thing about stereo

00:13:19,089 --> 00:13:22,300
is that it doesn't have to be totally

00:13:20,500 --> 00:13:24,430
passive you can build active stereo

00:13:22,300 --> 00:13:26,980
systems and here this is the same camera

00:13:24,430 --> 00:13:29,170
and we're turning on and off an IR V IR

00:13:26,980 --> 00:13:31,150
pattern projector the stereo algorithm

00:13:29,170 --> 00:13:33,370
has new knowledge of this pattern it can

00:13:31,150 --> 00:13:35,140
be totally random and arbitrary but it's

00:13:33,370 --> 00:13:36,820
just providing additional visual texture

00:13:35,140 --> 00:13:38,410
to the world which allows the stereo

00:13:36,820 --> 00:13:41,380
accurate algorithm to be more accurate

00:13:38,410 --> 00:13:43,930
and to get better information in low

00:13:41,380 --> 00:13:46,510
texture and lower texture parts of the

00:13:43,930 --> 00:13:48,640
scene this is really important for

00:13:46,510 --> 00:13:51,430
stereo operations indoors things like

00:13:48,640 --> 00:13:54,190
drywall and indoor paint are very smooth

00:13:51,430 --> 00:13:56,710
and without any visual texture stereo

00:13:54,190 --> 00:14:01,089
algorithms you'll fall over and can't

00:13:56,710 --> 00:14:03,400
give you any data you can also build

00:14:01,089 --> 00:14:04,450
stereo cameras from thermal imagers you

00:14:03,400 --> 00:14:07,720
don't need to use the visible spectrum

00:14:04,450 --> 00:14:10,000
and this is a bimodal optical and

00:14:07,720 --> 00:14:12,550
thermal stereo camera we built the top

00:14:10,000 --> 00:14:14,320
set of images here is data from the

00:14:12,550 --> 00:14:16,990
visual pair the bottom set of images is

00:14:14,320 --> 00:14:18,040
data from the thermal pair and you see

00:14:16,990 --> 00:14:19,750
that the ground doesn't have enough

00:14:18,040 --> 00:14:21,640
thermal texture to give us range returns

00:14:19,750 --> 00:14:23,380
but the people and the cars do and

00:14:21,640 --> 00:14:25,300
there's actually enough temperature

00:14:23,380 --> 00:14:27,700
change in the air around the powerlines

00:14:25,300 --> 00:14:29,589
that we're getting Range returns off of

00:14:27,700 --> 00:14:34,630
those that big diagonal stripe across

00:14:29,589 --> 00:14:38,230
the sky is actually the power lines okay

00:14:34,630 --> 00:14:40,420
so going into some quantitative testing

00:14:38,230 --> 00:14:41,890
this is a material test board that we

00:14:40,420 --> 00:14:43,180
built up and we've done a bunch of

00:14:41,890 --> 00:14:44,529
different testing on different sensors

00:14:43,180 --> 00:14:46,779
and I just wanted to highlight a few

00:14:44,529 --> 00:14:48,430
things there's a lot of information on

00:14:46,779 --> 00:14:50,140
these slides and not enough information

00:14:48,430 --> 00:14:51,970
not enough time to go through all of it

00:14:50,140 --> 00:14:54,010
but I just wanted to show the effect of

00:14:51,970 --> 00:14:55,870
a couple of different changes to the

00:14:54,010 --> 00:14:57,010
environment this is with sort of normal

00:14:55,870 --> 00:14:59,050
for us at wedding and then high

00:14:57,010 --> 00:15:01,630
intensity halogen lighting the laser

00:14:59,050 --> 00:15:04,690
from this vendor saw a shift of 2.5

00:15:01,630 --> 00:15:06,100
centimeters on the white areas similar

00:15:04,690 --> 00:15:08,680
kind of laser from a different vendor

00:15:06,100 --> 00:15:10,060
had 50% more noise in the low lighting

00:15:08,680 --> 00:15:12,730
environment but then when you turn on

00:15:10,060 --> 00:15:16,390
the high halogen or the high ambient

00:15:12,730 --> 00:15:18,700
excuse me the high IR from the halogen

00:15:16,390 --> 00:15:22,300
lights the range accuracy drops

00:15:18,700 --> 00:15:25,030
dramatically for this sensor similar

00:15:22,300 --> 00:15:26,290
kind of 3d perception technology

00:15:25,030 --> 00:15:29,010
different vendors have different

00:15:26,290 --> 00:15:31,890
implementations and have a these

00:15:29,010 --> 00:15:33,540
of regressions potentially this is a

00:15:31,890 --> 00:15:34,830
time of flight camera and you see that

00:15:33,540 --> 00:15:37,290
there's higher range noise on dark

00:15:34,830 --> 00:15:39,120
objects and that turning on that halogen

00:15:37,290 --> 00:15:40,590
light with high IR increases the range

00:15:39,120 --> 00:15:44,010
can always by a factor of two to five

00:15:40,590 --> 00:15:45,510
depending on the material a stereo

00:15:44,010 --> 00:15:49,860
camera doesn't have those sorts of

00:15:45,510 --> 00:15:51,690
intensity based accuracy reductions but

00:15:49,860 --> 00:15:53,580
it does have accuracy reductions as you

00:15:51,690 --> 00:15:55,530
pull the camera away so this is a

00:15:53,580 --> 00:15:57,840
visualization of range accuracy and

00:15:55,530 --> 00:16:00,860
variance at close range and at long

00:15:57,840 --> 00:16:03,420
range for that seven centimeter camera

00:16:00,860 --> 00:16:06,510
sensors can also change over time this

00:16:03,420 --> 00:16:09,210
is two different units we put them up on

00:16:06,510 --> 00:16:11,100
a flat wall and did scans of this flat

00:16:09,210 --> 00:16:13,590
wall and slowly pulled the sensor away

00:16:11,100 --> 00:16:16,050
so each line of data here horizontally

00:16:13,590 --> 00:16:17,940
is pulling that 3d laser five

00:16:16,050 --> 00:16:20,040
centimeters away from the wall and you

00:16:17,940 --> 00:16:22,950
see that this one unit unit a actually

00:16:20,040 --> 00:16:24,900
changed in accuracy from the first time

00:16:22,950 --> 00:16:27,420
we tested it January 20 tested again in

00:16:24,900 --> 00:16:28,530
July and the distortion that it was

00:16:27,420 --> 00:16:30,570
seeing on that wall is actually a

00:16:28,530 --> 00:16:31,560
mission angle and range dependent it

00:16:30,570 --> 00:16:33,720
wasn't just that there was a bad

00:16:31,560 --> 00:16:35,760
distance there was a bad clocking angle

00:16:33,720 --> 00:16:38,460
but the combination those two factors

00:16:35,760 --> 00:16:40,470
had like bad lobes of data within this

00:16:38,460 --> 00:16:44,100
field of view which was very surprising

00:16:40,470 --> 00:16:45,690
and not expected and then a similar the

00:16:44,100 --> 00:16:47,310
exact same model but a different actual

00:16:45,690 --> 00:16:50,240
physical unit from that same vendor

00:16:47,310 --> 00:16:53,430
didn't have those sorts of problems

00:16:50,240 --> 00:16:55,560
we've also done we're sort of instilled

00:16:53,430 --> 00:16:57,240
a process with this but we've been

00:16:55,560 --> 00:16:59,700
trying to develop better tools that let

00:16:57,240 --> 00:17:01,920
us understand 3d resolution accuracy

00:16:59,700 --> 00:17:03,780
both in depth and well as horizontally

00:17:01,920 --> 00:17:05,400
and vertically so we've built a 3d

00:17:03,780 --> 00:17:08,280
target modeled after than the US Air

00:17:05,400 --> 00:17:12,000
Force 1951 a visual target that's used

00:17:08,280 --> 00:17:14,490
for MTF and 2d camera testing and we've

00:17:12,000 --> 00:17:16,350
recreated that target in 3d space with

00:17:14,490 --> 00:17:18,810
vertical horizontal bars of different

00:17:16,350 --> 00:17:22,140
sizes and at different distant standoffs

00:17:18,810 --> 00:17:24,150
from the backboard and this is what this

00:17:22,140 --> 00:17:26,579
target looks like through an industrial

00:17:24,150 --> 00:17:28,980
time-of-flight camera you see horizontal

00:17:26,579 --> 00:17:31,380
and vertical bars the left image is an

00:17:28,980 --> 00:17:34,470
error image white is the model and the

00:17:31,380 --> 00:17:36,510
measure lining up red is the model is in

00:17:34,470 --> 00:17:38,580
front of the measure and blue is the

00:17:36,510 --> 00:17:40,900
model is behind the measure and then on

00:17:38,580 --> 00:17:42,520
the left side is just a raw range image

00:17:40,900 --> 00:17:45,309
showing at two-and-a-half d heightmap a

00:17:42,520 --> 00:17:47,650
one thing that's really interesting in

00:17:45,309 --> 00:17:49,809
this is we added this slanted target you

00:17:47,650 --> 00:17:52,240
see below the bullseye this sort of a

00:17:49,809 --> 00:17:54,460
three by six inch wedge on the target

00:17:52,240 --> 00:17:56,410
and that totally disappears you really

00:17:54,460 --> 00:17:58,360
can't see it in the range image the top

00:17:56,410 --> 00:17:59,860
half is visible and then the bottom half

00:17:58,360 --> 00:18:01,960
of that ramp is completely gone and you

00:17:59,860 --> 00:18:04,120
see that reflected in that big red

00:18:01,960 --> 00:18:06,790
stripe in the model versus measure error

00:18:04,120 --> 00:18:07,960
image we believe this is happening

00:18:06,790 --> 00:18:09,430
because that slant is actually

00:18:07,960 --> 00:18:10,990
reflecting IR energy away from the

00:18:09,430 --> 00:18:12,790
camera back down to the floor so we're

00:18:10,990 --> 00:18:14,650
not getting a good return off that wedge

00:18:12,790 --> 00:18:16,230
but we get a much better return off the

00:18:14,650 --> 00:18:19,120
things that are parallel to the camera

00:18:16,230 --> 00:18:20,980
you also see that there's sort of equal

00:18:19,120 --> 00:18:23,140
performance of this camera both

00:18:20,980 --> 00:18:24,820
horizontally and vertically but it's

00:18:23,140 --> 00:18:26,410
relatively low resolution even these

00:18:24,820 --> 00:18:28,450
large targets only have about three

00:18:26,410 --> 00:18:30,700
pixels of data on them and then you can

00:18:28,450 --> 00:18:33,250
see the whole top left corner the very

00:18:30,700 --> 00:18:34,809
small features completely blend into the

00:18:33,250 --> 00:18:36,280
background there they don't have enough

00:18:34,809 --> 00:18:38,290
size horizontally or vertically to be

00:18:36,280 --> 00:18:40,500
visible and the depth accuracy of the

00:18:38,290 --> 00:18:42,760
camera can't pick them out either in

00:18:40,500 --> 00:18:44,770
comparison stereo is much higher

00:18:42,760 --> 00:18:47,740
resolution so we're able to see the

00:18:44,770 --> 00:18:49,630
smaller targets but because of this very

00:18:47,740 --> 00:18:51,550
matching process we see very different

00:18:49,630 --> 00:18:53,620
performance in horizontal and vertical

00:18:51,550 --> 00:18:55,929
bars the horizontal bars at the

00:18:53,620 --> 00:18:58,600
bottom-left you actually see the blue

00:18:55,929 --> 00:19:01,300
background between them they're sort of

00:18:58,600 --> 00:19:02,679
they're isolated and individual and then

00:19:01,300 --> 00:19:04,179
the vertical bars actually webbed

00:19:02,679 --> 00:19:06,429
together because of the horizontal

00:19:04,179 --> 00:19:09,250
stereo matching process you're searching

00:19:06,429 --> 00:19:10,660
left to right and that the accuracy of

00:19:09,250 --> 00:19:12,610
the camera in that direction is worse

00:19:10,660 --> 00:19:14,920
vertically so you ended up you end up

00:19:12,610 --> 00:19:16,960
blending features sort of smearing them

00:19:14,920 --> 00:19:19,929
horizontally and that blends those

00:19:16,960 --> 00:19:21,370
vertical bars together that actually is

00:19:19,929 --> 00:19:22,929
advantageous when you're trying to see

00:19:21,370 --> 00:19:25,150
the small vertical bars which are

00:19:22,929 --> 00:19:27,820
directly above that large one at the top

00:19:25,150 --> 00:19:30,190
left features here even the very

00:19:27,820 --> 00:19:31,809
smallest vertical bars are visible to

00:19:30,190 --> 00:19:36,309
the stereo camera because of this

00:19:31,809 --> 00:19:38,380
blending effect so there is no perfect

00:19:36,309 --> 00:19:40,630
censor they all have limitations they

00:19:38,380 --> 00:19:41,860
all have edge cases and you really need

00:19:40,630 --> 00:19:43,960
to think carefully about your

00:19:41,860 --> 00:19:47,110
environment the application the platform

00:19:43,960 --> 00:19:48,060
and balance all these requirements and

00:19:47,110 --> 00:19:50,950
balance the sensor performance

00:19:48,060 --> 00:19:52,120
appropriately it's really important to

00:19:50,950 --> 00:19:53,230
collect data from your environment

00:19:52,120 --> 00:19:53,929
really important to test things

00:19:53,230 --> 00:19:55,249
thoroughly

00:19:53,929 --> 00:19:57,769
to look at data sets that are available

00:19:55,249 --> 00:19:59,840
if datasets aren't available please ask

00:19:57,769 --> 00:20:02,330
your sensor manufacturer like us or like

00:19:59,840 --> 00:20:04,879
other folks to provide that to you that

00:20:02,330 --> 00:20:08,240
you really helps you understand the

00:20:04,879 --> 00:20:10,340
performance in the environment and I you

00:20:08,240 --> 00:20:12,590
know I welcome people to publish more

00:20:10,340 --> 00:20:14,179
data sets it's very useful to sort of

00:20:12,590 --> 00:20:16,369
share these sorts of things with the

00:20:14,179 --> 00:20:18,259
community and to learn for each other's

00:20:16,369 --> 00:20:21,169
experience in testing and integrating

00:20:18,259 --> 00:20:23,480
these systems we have all these videos

00:20:21,169 --> 00:20:24,529
replicated on our website and then

00:20:23,480 --> 00:20:27,139
there's more information about our

00:20:24,529 --> 00:20:29,149
products on our website as well and I'll

00:20:27,139 --> 00:20:30,950
also say we're about to launch some new

00:20:29,149 --> 00:20:32,809
versions of our products through there's

00:20:30,950 --> 00:20:35,090
a sign up for our newsletter link on

00:20:32,809 --> 00:20:36,799
that page and when these new product

00:20:35,090 --> 00:20:38,779
announcements are available I'd be happy

00:20:36,799 --> 00:20:40,369
to share that with you guys thanks so

00:20:38,779 --> 00:20:42,370
much for your time and I have a few

00:20:40,369 --> 00:20:47,040
seconds for questions

00:20:42,370 --> 00:20:47,040
[Applause]

00:20:47,460 --> 00:20:50,970
just got time for one quick question I

00:20:49,800 --> 00:20:52,740
think you've made it to a mic first

00:20:50,970 --> 00:20:58,320
sorry about that but we'll have time in

00:20:52,740 --> 00:21:00,780
the evening if you contact Chris okay

00:20:58,320 --> 00:21:04,680
hey sir I'm Brandon Shrewsbury we've

00:21:00,780 --> 00:21:07,080
been emailing quite a bit so I've been

00:21:04,680 --> 00:21:09,060
wondering in the multi since SL when you

00:21:07,080 --> 00:21:12,600
do a calibration check it checks the

00:21:09,060 --> 00:21:15,660
stereo to the lidar and I've always

00:21:12,600 --> 00:21:17,660
wondered do you use ground truth when

00:21:15,660 --> 00:21:21,360
you're setting up your calibrations

00:21:17,660 --> 00:21:24,330
in-house because you've noticed that the

00:21:21,360 --> 00:21:26,430
lidar obviously has different error

00:21:24,330 --> 00:21:27,840
depending on the texture and I had

00:21:26,430 --> 00:21:30,870
noticed that the stereo had the same

00:21:27,840 --> 00:21:33,030
thing we with our material

00:21:30,870 --> 00:21:35,310
yeah the stereo accuracy can change

00:21:33,030 --> 00:21:36,840
based on material and the texture when

00:21:35,310 --> 00:21:39,090
we do the stereo to light our

00:21:36,840 --> 00:21:41,070
calibration we actually synthetically

00:21:39,090 --> 00:21:43,770
generate sort of a fake disparity image

00:21:41,070 --> 00:21:45,990
from the lidar data and we do all of our

00:21:43,770 --> 00:21:49,140
estimation in disparity space which

00:21:45,990 --> 00:21:50,430
balances out the different error models

00:21:49,140 --> 00:21:52,290
between the two different sensors which

00:21:50,430 --> 00:21:55,430
is also really important if you try to

00:21:52,290 --> 00:21:57,750
align stereo and light are in Xyz space

00:21:55,430 --> 00:21:59,880
you have a you know much greater

00:21:57,750 --> 00:22:03,180
difficulty because of the error model of

00:21:59,880 --> 00:22:04,730
stereo data and in terms of the

00:22:03,180 --> 00:22:07,140
materials that you should use

00:22:04,730 --> 00:22:08,700
you need something has a visual texture

00:22:07,140 --> 00:22:10,830
for the stereo to work off of and you

00:22:08,700 --> 00:22:13,560
want something that doesn't cause

00:22:10,830 --> 00:22:16,620
distortion to the laser laser data so

00:22:13,560 --> 00:22:19,410
checker boards are great for calibrating

00:22:16,620 --> 00:22:20,940
camera data but most laser data that we

00:22:19,410 --> 00:22:22,650
looked at you actually see the checker

00:22:20,940 --> 00:22:23,970
board in the laser data you know the

00:22:22,650 --> 00:22:25,740
white and the black appear to be

00:22:23,970 --> 00:22:28,350
different distances away so when you're

00:22:25,740 --> 00:22:30,570
doing these laser to stereo calibrations

00:22:28,350 --> 00:22:32,850
it's important to pick materials that

00:22:30,570 --> 00:22:34,830
don't have as much visual contrast

00:22:32,850 --> 00:22:37,200
between them and then you aren't

00:22:34,830 --> 00:22:39,950
introducing that sort of range error in

00:22:37,200 --> 00:22:39,950

YouTube URL: https://www.youtube.com/watch?v=sexmG5Ks3YQ


