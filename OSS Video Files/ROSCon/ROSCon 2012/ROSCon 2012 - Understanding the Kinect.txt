Title: ROSCon 2012 - Understanding the Kinect
Publication date: 2014-08-31
Playlist: ROSCon 2012
Description: 
	Patrick Mihelich
Understanding the Kinect
Captions: 
	00:00:02,139 --> 00:00:09,440
everyone so I've spent a good chunk of

00:00:07,249 --> 00:00:11,360
the last year and a half working with

00:00:09,440 --> 00:00:14,030
the Kinect and working on the Ross

00:00:11,360 --> 00:00:16,760
drivers for the kinect and I'm here to

00:00:14,030 --> 00:00:19,279
tell you how awesome it is but many of

00:00:16,760 --> 00:00:20,960
you already know that the kinect had

00:00:19,279 --> 00:00:26,510
quite a splash when it came on the

00:00:20,960 --> 00:00:29,480
market late 2010 sold millions of units

00:00:26,510 --> 00:00:32,150
it was the fastest selling piece of

00:00:29,480 --> 00:00:33,560
electronics ever and people in the

00:00:32,150 --> 00:00:36,200
robust community got really excited

00:00:33,560 --> 00:00:40,180
about it because they said this is the

00:00:36,200 --> 00:00:40,180
3d sensor that we've been waiting for

00:00:40,840 --> 00:00:45,770
thing is you really need 3d perception

00:00:43,940 --> 00:00:50,050
for robotics if you want to do anything

00:00:45,770 --> 00:00:54,140
beyond very scripted open-loop behavior

00:00:50,050 --> 00:00:59,899
so for example if you want to do mapping

00:00:54,140 --> 00:01:02,840
and navigation this is a map that's

00:00:59,899 --> 00:01:05,090
built by octo map you need through your

00:01:02,840 --> 00:01:07,310
perception if you want to pick up things

00:01:05,090 --> 00:01:08,720
recognize objects you also need to

00:01:07,310 --> 00:01:10,940
figure out where they are in 3d

00:01:08,720 --> 00:01:14,990
everything that we're doing in robotics

00:01:10,940 --> 00:01:17,270
is manipulating the real world so we

00:01:14,990 --> 00:01:21,560
need it but three perception is hard and

00:01:17,270 --> 00:01:23,750
it's been expensive in the entire field

00:01:21,560 --> 00:01:28,010
of computer vision basically exists to

00:01:23,750 --> 00:01:31,070
turn 2d images into 3d inferences about

00:01:28,010 --> 00:01:32,990
the world and we have all kinds of

00:01:31,070 --> 00:01:35,240
technologies for doing this we have

00:01:32,990 --> 00:01:38,390
stereo we have time of flight we have

00:01:35,240 --> 00:01:41,540
structured light we also have laser

00:01:38,390 --> 00:01:44,300
scanners of course some of these work

00:01:41,540 --> 00:01:49,130
better than others but they've all been

00:01:44,300 --> 00:01:51,580
hard to do and have been costly and then

00:01:49,130 --> 00:01:54,320
the kinect came along and it's 150 bucks

00:01:51,580 --> 00:01:57,470
so what are we looking for a no 3d

00:01:54,320 --> 00:02:03,410
sensor so here are four things that we'd

00:01:57,470 --> 00:02:07,479
like cheap is good the more resolution

00:02:03,410 --> 00:02:10,340
the more detail you get the kinect is

00:02:07,479 --> 00:02:14,330
640 x 480 which is actually pretty good

00:02:10,340 --> 00:02:15,710
as think these things go speed is

00:02:14,330 --> 00:02:18,850
important if you have

00:02:15,710 --> 00:02:20,900
any kind of movement or dynamic scene

00:02:18,850 --> 00:02:23,360
there are technologies that give you

00:02:20,900 --> 00:02:26,300
very accurate 3d scans like on the pier

00:02:23,360 --> 00:02:29,900
to we have a pic OH a laser on a tilt

00:02:26,300 --> 00:02:32,210
platform which gives you a very rich set

00:02:29,900 --> 00:02:34,160
of accurate 3d data but it takes a few

00:02:32,210 --> 00:02:36,080
seconds to collect that if you have a

00:02:34,160 --> 00:02:39,340
moving object in there it's gone by the

00:02:36,080 --> 00:02:44,530
time you've seen it from the data and

00:02:39,340 --> 00:02:44,530
under quality I'll I lump in things like

00:02:44,650 --> 00:02:48,920
coverage of of the field of view in

00:02:47,480 --> 00:02:50,720
terms of the actual depths you get the

00:02:48,920 --> 00:02:53,660
accuracy and precision of the depths and

00:02:50,720 --> 00:02:56,300
the Kinect does pretty well in all of

00:02:53,660 --> 00:02:59,600
these categories especially price it was

00:02:56,300 --> 00:03:01,040
really an outlier when it came out this

00:02:59,600 --> 00:03:02,390
is a very active area of hardware

00:03:01,040 --> 00:03:04,700
research so I'm hoping that we'll see

00:03:02,390 --> 00:03:07,430
more devices coming out that are

00:03:04,700 --> 00:03:09,680
comparable or even better but for the

00:03:07,430 --> 00:03:14,150
last year the Kinect has really been

00:03:09,680 --> 00:03:17,540
where the game has been at and just one

00:03:14,150 --> 00:03:19,150
example of what this is enabled a couple

00:03:17,540 --> 00:03:21,860
years ago you would have been lucky to

00:03:19,150 --> 00:03:24,830
spend 14 hundred dollars and get a

00:03:21,860 --> 00:03:28,040
decent sensor now you can get an entire

00:03:24,830 --> 00:03:35,060
robot with a mobile base and you know a

00:03:28,040 --> 00:03:36,950
little bit of processing power so a few

00:03:35,060 --> 00:03:39,320
months after the kinect came out we'd

00:03:36,950 --> 00:03:41,840
been hacking away like crazy to build up

00:03:39,320 --> 00:03:43,460
support for an in ross we did a 3d

00:03:41,840 --> 00:03:45,650
challenge where we did a call to the

00:03:43,460 --> 00:03:49,460
community you know do something cool

00:03:45,650 --> 00:03:52,940
make a video send it to us we awarded

00:03:49,460 --> 00:03:57,470
prizes and so a bunch of people did

00:03:52,940 --> 00:04:00,500
exactly this I'm just gonna show a few

00:03:57,470 --> 00:04:02,240
of the videos that came out of this at

00:04:00,500 --> 00:04:05,720
the bottom right this is a video from

00:04:02,240 --> 00:04:08,240
Garrett who's here somewhere this is an

00:04:05,720 --> 00:04:10,130
early Billy bought prototype that's

00:04:08,240 --> 00:04:12,650
following him around by using the

00:04:10,130 --> 00:04:16,850
skeleton tracking capabilities that made

00:04:12,650 --> 00:04:20,299
the Kinect so attractive for gaming up

00:04:16,850 --> 00:04:23,510
top we see also using the skeleton

00:04:20,299 --> 00:04:26,990
tracking teleoperation of a humanoid

00:04:23,510 --> 00:04:29,180
robot and on the bottom left

00:04:26,990 --> 00:04:31,430
you can even mount one of these things

00:04:29,180 --> 00:04:36,979
on a quadrotor and here it's being used

00:04:31,430 --> 00:04:38,840
to travel down a corridor staying safely

00:04:36,979 --> 00:04:48,560
in the middle without veering into the

00:04:38,840 --> 00:04:50,720
walls of the ceiling so the rest of the

00:04:48,560 --> 00:04:53,960
talk is going to in three parts first

00:04:50,720 --> 00:04:55,699
I'm going to go over the hardware so how

00:04:53,960 --> 00:04:58,669
the Kinect actually works and what that

00:04:55,699 --> 00:05:01,130
means for you as a user and how we get

00:04:58,669 --> 00:05:05,780
data off of it then I'm going to talk

00:05:01,130 --> 00:05:07,099
about how it's integrated in Ross what

00:05:05,780 --> 00:05:10,130
the driver looks like we're all the

00:05:07,099 --> 00:05:12,380
tools that you can use and I'll end by

00:05:10,130 --> 00:05:15,259
giving you some flavor of what you can

00:05:12,380 --> 00:05:20,659
do with our 2d 3d processing libraries

00:05:15,259 --> 00:05:23,120
in particular opencv and pcl so this is

00:05:20,659 --> 00:05:27,650
what the Kinect looks like if you manage

00:05:23,120 --> 00:05:30,319
to rip the skin off of it there is an IR

00:05:27,650 --> 00:05:32,690
camera up there on the right and an IR

00:05:30,319 --> 00:05:34,520
projector those are the two components

00:05:32,690 --> 00:05:37,639
that are actually used to compute the

00:05:34,520 --> 00:05:39,979
depth and also because we all love color

00:05:37,639 --> 00:05:45,050
images there's an RGB camera well that's

00:05:39,979 --> 00:05:47,419
that's not used for depth sensing so the

00:05:45,050 --> 00:05:51,229
to briefly review how conventional

00:05:47,419 --> 00:05:53,659
stereo works because it's relevant you

00:05:51,229 --> 00:05:56,500
see some point out in the world from two

00:05:53,659 --> 00:05:59,259
cameras left from the right camera and

00:05:56,500 --> 00:06:02,569
you're trying to detect its depth Z

00:05:59,259 --> 00:06:04,789
based on triangulation so you know

00:06:02,569 --> 00:06:10,699
there's some baseline distance between

00:06:04,789 --> 00:06:13,580
the cameras T and because the cameras

00:06:10,699 --> 00:06:17,330
are separated that point out in space

00:06:13,580 --> 00:06:19,659
appears on their image plane slightly

00:06:17,330 --> 00:06:22,580
offset and the two images that you get

00:06:19,659 --> 00:06:24,860
so these cameras are there calibrates

00:06:22,580 --> 00:06:28,759
that their image planes are parallel to

00:06:24,860 --> 00:06:31,130
each other and you can correlate some

00:06:28,759 --> 00:06:33,469
point between both images and say oh

00:06:31,130 --> 00:06:36,770
well it shifted a little bit between the

00:06:33,469 --> 00:06:38,990
left and the right and that's called

00:06:36,770 --> 00:06:40,370
that value that shift amount is called

00:06:38,990 --> 00:06:43,850
the disparity

00:06:40,370 --> 00:06:45,740
so here it's x1 plus x2 and if you do

00:06:43,850 --> 00:06:48,320
the if you work out the similar

00:06:45,740 --> 00:06:49,910
triangles here then you find that you

00:06:48,320 --> 00:06:53,419
have a very simple formula for distance

00:06:49,910 --> 00:06:55,360
it's proportional to the focal length it

00:06:53,419 --> 00:06:58,040
is the distance of the image plane and

00:06:55,360 --> 00:07:00,860
the baseline and it's inversely related

00:06:58,040 --> 00:07:04,639
to this pixel disparity that you detect

00:07:00,860 --> 00:07:08,630
by matching pixels across the images so

00:07:04,639 --> 00:07:10,479
on the pier to pre connect we had

00:07:08,630 --> 00:07:14,000
actually two pairs of stereo cameras

00:07:10,479 --> 00:07:16,610
this was about the best we could do at

00:07:14,000 --> 00:07:20,900
the time and we added one more element

00:07:16,610 --> 00:07:24,650
there which was this texture projector

00:07:20,900 --> 00:07:27,229
so this is a passive vision system it's

00:07:24,650 --> 00:07:29,600
just you know taking light in from the

00:07:27,229 --> 00:07:32,780
world and now we're turning it into an

00:07:29,600 --> 00:07:35,810
active one where the point of this is

00:07:32,780 --> 00:07:38,210
that you put a point of passive stereo

00:07:35,810 --> 00:07:39,770
system at say a blank wall you can't

00:07:38,210 --> 00:07:42,080
match anything up between the images so

00:07:39,770 --> 00:07:44,419
you don't get any depth but if you

00:07:42,080 --> 00:07:45,950
project some texture onto it well now

00:07:44,419 --> 00:07:51,169
you have something for the correlation

00:07:45,950 --> 00:07:53,620
algorithm to hook onto then primeSense

00:07:51,169 --> 00:07:57,320
which is the company that developed the

00:07:53,620 --> 00:07:59,300
depth sensing technology microsoft that

00:07:57,320 --> 00:08:01,220
microsoft license really connect they

00:07:59,300 --> 00:08:03,169
had this very nice insight that if

00:08:01,220 --> 00:08:05,180
you're already projecting texture on the

00:08:03,169 --> 00:08:08,840
scene you actually don't need the right

00:08:05,180 --> 00:08:13,910
camera you can kind of use the projector

00:08:08,840 --> 00:08:15,410
itself as your right camera and so what

00:08:13,910 --> 00:08:18,500
they're doing is they're projecting this

00:08:15,410 --> 00:08:22,190
IR speckle pattern out into the aliens

00:08:18,500 --> 00:08:26,330
little world and during the factory

00:08:22,190 --> 00:08:28,310
calibration you point this at some wall

00:08:26,330 --> 00:08:31,220
if it's a known reference distance R

00:08:28,310 --> 00:08:33,740
away and you record an image of what the

00:08:31,220 --> 00:08:35,900
speckle pattern is and this this takes

00:08:33,740 --> 00:08:37,520
the place of your right stereo image you

00:08:35,900 --> 00:08:42,020
just put this in flash memory on the

00:08:37,520 --> 00:08:44,930
camera and then you take the image

00:08:42,020 --> 00:08:47,029
League from your irm it ir camera do the

00:08:44,930 --> 00:08:51,709
exact same stereo correlation kind of

00:08:47,029 --> 00:08:52,710
stuff you get some shift value s in the

00:08:51,709 --> 00:08:58,830
reference plane

00:08:52,710 --> 00:09:00,990
and you can oops you can turn that into

00:08:58,830 --> 00:09:03,540
a distance so it's very very very

00:09:00,990 --> 00:09:04,680
similar to stereo and a lot of the

00:09:03,540 --> 00:09:08,340
conclusions that you can make about

00:09:04,680 --> 00:09:10,230
stereo also apply here and finally I'm

00:09:08,340 --> 00:09:12,510
because there is that extra RGB camera

00:09:10,230 --> 00:09:14,790
there's an extra calibration step to

00:09:12,510 --> 00:09:17,400
take the depths they've detected in the

00:09:14,790 --> 00:09:19,980
IR camera and reproject those into the

00:09:17,400 --> 00:09:25,860
RGB camera frame if you want those to

00:09:19,980 --> 00:09:27,600
line up right so we did some some rough

00:09:25,860 --> 00:09:29,220
experiments to figure out how accurate

00:09:27,600 --> 00:09:32,880
this thing is and it's pretty good

00:09:29,220 --> 00:09:35,280
actually we couldn't we couldn't really

00:09:32,880 --> 00:09:37,590
detect very much error like maybe plus

00:09:35,280 --> 00:09:43,350
or minus a millimeter relative error

00:09:37,590 --> 00:09:46,700
within the like meter and a half to six

00:09:43,350 --> 00:09:46,700
meters where you get good information

00:09:47,300 --> 00:09:52,710
measuring with a tape measure and or

00:09:50,600 --> 00:09:55,800
doing a post calculation from a

00:09:52,710 --> 00:09:58,650
checkerboard it's pretty much within the

00:09:55,800 --> 00:10:02,340
range of error that we could detect it

00:09:58,650 --> 00:10:03,750
can be bias biased some systematic bias

00:10:02,340 --> 00:10:06,060
up to a centimeter you really only

00:10:03,750 --> 00:10:08,520
notice that if you're calibrating it to

00:10:06,060 --> 00:10:10,020
one or more other cameras and you're

00:10:08,520 --> 00:10:13,170
trying to figure out why the the depth

00:10:10,020 --> 00:10:15,840
is a little bit further or shorter than

00:10:13,170 --> 00:10:18,150
it should be I'm not totally sure why

00:10:15,840 --> 00:10:20,640
that is maybe just primeSense and I

00:10:18,150 --> 00:10:23,520
disagree a little bit on where the the

00:10:20,640 --> 00:10:27,060
optical center is most of the time you

00:10:23,520 --> 00:10:30,750
won't notice that though and for stereo

00:10:27,060 --> 00:10:32,460
systems the error and the depths that

00:10:30,750 --> 00:10:34,470
you get increases quadratically with the

00:10:32,460 --> 00:10:35,730
distance and we see exactly the same

00:10:34,470 --> 00:10:38,960
thing with the Kinect since it's

00:10:35,730 --> 00:10:41,760
essentially the same principle so I'm

00:10:38,960 --> 00:10:45,810
here we're fitting a couple of curves to

00:10:41,760 --> 00:10:48,420
the error that we get detect by pointing

00:10:45,810 --> 00:10:51,030
the connected of plain fitting a plane

00:10:48,420 --> 00:10:54,180
to all the data and then seeing how much

00:10:51,030 --> 00:10:56,550
the points vary from that what this what

00:10:54,180 --> 00:10:59,310
this chart also shows is that because

00:10:56,550 --> 00:11:01,320
the Kinect is active if you have to

00:10:59,310 --> 00:11:03,430
connects looking at the same scene that

00:11:01,320 --> 00:11:06,130
I are speckle pattern from both

00:11:03,430 --> 00:11:08,649
can interfere with each other and so

00:11:06,130 --> 00:11:12,490
that increases your error also it's not

00:11:08,649 --> 00:11:14,770
too bad for for many applications is

00:11:12,490 --> 00:11:19,000
probably okay and it may be useful to

00:11:14,770 --> 00:11:21,420
get that extra data lastly i just want

00:11:19,000 --> 00:11:23,890
to mention a couple of gotchas with USB

00:11:21,420 --> 00:11:26,380
i've seen a bunch of people get excited

00:11:23,890 --> 00:11:28,779
with actually putting multiple connects

00:11:26,380 --> 00:11:31,270
into the same system which you can do

00:11:28,779 --> 00:11:33,160
but you there's some challenges there

00:11:31,270 --> 00:11:37,420
for one thing these things are just

00:11:33,160 --> 00:11:39,670
firing data at the computer if you're

00:11:37,420 --> 00:11:42,490
streaming RGB and depth when we connect

00:11:39,670 --> 00:11:45,010
it's basically taking an entire USB bus

00:11:42,490 --> 00:11:48,459
to do that so you can only get two

00:11:45,010 --> 00:11:50,410
streams over one USB controller so if

00:11:48,459 --> 00:11:52,180
you want all the streams multiple

00:11:50,410 --> 00:11:55,060
connects you need multiple controllers

00:11:52,180 --> 00:11:59,760
if necessary you can add them via

00:11:55,060 --> 00:12:02,860
expansion slots and for some reason usb3

00:11:59,760 --> 00:12:04,180
the new USB standard doesn't really seem

00:12:02,860 --> 00:12:07,779
to work with these things I don't know

00:12:04,180 --> 00:12:11,140
why maybe a firmer issue or something so

00:12:07,779 --> 00:12:13,390
watch out for that so I keep saying

00:12:11,140 --> 00:12:17,610
connect but that's actually not your

00:12:13,390 --> 00:12:20,110
only option asus has come out with an

00:12:17,610 --> 00:12:22,750
almost identical device it's the same

00:12:20,110 --> 00:12:26,050
technology lessons from primeSense you

00:12:22,750 --> 00:12:28,660
can use the same drivers with it and so

00:12:26,050 --> 00:12:33,029
here's a little little rundown of the

00:12:28,660 --> 00:12:37,930
relevant differences I like the asus

00:12:33,029 --> 00:12:40,420
it's a smaller form factor it runs just

00:12:37,930 --> 00:12:43,839
off USB power whereas the kinect has

00:12:40,420 --> 00:12:48,520
this weird proprietary adapter that you

00:12:43,839 --> 00:12:52,029
also have to plug in the kinect has a

00:12:48,520 --> 00:12:54,640
tilt motor an accelerometer most of the

00:12:52,029 --> 00:12:57,490
time we're doing a fixed mount onto a

00:12:54,640 --> 00:13:00,040
robot where we already have other

00:12:57,490 --> 00:13:04,149
sensors to take care of that so we don't

00:13:00,040 --> 00:13:08,200
care and also the the asus has a couple

00:13:04,149 --> 00:13:10,270
of nice hardware capabilities non-prime

00:13:08,200 --> 00:13:13,990
sense reference design that microsoft

00:13:10,270 --> 00:13:15,740
ripped out asus can actually do the

00:13:13,990 --> 00:13:18,920
registration

00:13:15,740 --> 00:13:21,440
putting the depth image into the RGB

00:13:18,920 --> 00:13:24,170
camera frame and hardware which is nice

00:13:21,440 --> 00:13:25,970
saves you some software computation and

00:13:24,170 --> 00:13:29,000
can also synchronize those two data

00:13:25,970 --> 00:13:30,410
streams which is also important if

00:13:29,000 --> 00:13:33,110
you're doing a lot of movement we're

00:13:30,410 --> 00:13:35,900
trying to detect moving objects the

00:13:33,110 --> 00:13:38,930
Kinect has a bigger microphone array if

00:13:35,900 --> 00:13:40,370
you care about that and then a recently

00:13:38,930 --> 00:13:42,470
Microsoft's put out this kinect for

00:13:40,370 --> 00:13:44,510
windows device which is pretty much

00:13:42,470 --> 00:13:50,470
exactly the same thing but a hundred

00:13:44,510 --> 00:13:53,630
dollars more expensive and then the the

00:13:50,470 --> 00:13:55,550
briefly look at the driver library so

00:13:53,630 --> 00:13:57,680
you can use lip free next was the first

00:13:55,550 --> 00:13:59,690
on the scene I'm like right after they

00:13:57,680 --> 00:14:01,340
connect came out people started trying

00:13:59,690 --> 00:14:04,490
to hack it and very quickly were

00:14:01,340 --> 00:14:06,080
successful so the very very first Ross

00:14:04,490 --> 00:14:10,010
driver that we did was slippery neck

00:14:06,080 --> 00:14:14,480
based works now with kinect for xbox

00:14:10,010 --> 00:14:17,060
from the asus cameras it gives you the

00:14:14,480 --> 00:14:19,160
data streams it's it's nice for hacking

00:14:17,060 --> 00:14:21,530
because it's small and simple you can

00:14:19,160 --> 00:14:25,160
dive down just a little bit and get

00:14:21,530 --> 00:14:27,110
straight to the usb calls in Ross were

00:14:25,160 --> 00:14:29,350
actually now using open and I which is

00:14:27,110 --> 00:14:32,840
an open source project from primeSense

00:14:29,350 --> 00:14:35,690
so prompt this came out like a month

00:14:32,840 --> 00:14:38,120
later and it was like oh my god now we

00:14:35,690 --> 00:14:39,440
have the answer key we don't have to do

00:14:38,120 --> 00:14:42,020
any was any more of this reverse

00:14:39,440 --> 00:14:44,270
engineering and it has some some more

00:14:42,020 --> 00:14:50,960
nice features it has skeleton tracking

00:14:44,270 --> 00:14:53,690
and then there's the kinect sdk from

00:14:50,960 --> 00:14:57,220
microsoft which was late to the party i

00:14:53,690 --> 00:14:59,720
haven't really looked at very much

00:14:57,220 --> 00:15:01,370
currently it's the only thing that does

00:14:59,720 --> 00:15:03,290
the kinect for windows device although

00:15:01,370 --> 00:15:06,920
that could be added to the open source

00:15:03,290 --> 00:15:08,450
libraries the it also has skeleton

00:15:06,920 --> 00:15:10,610
tracking of course the big

00:15:08,450 --> 00:15:12,470
distinguishing feature of the kinect sdk

00:15:10,610 --> 00:15:14,630
is it can do some interesting things

00:15:12,470 --> 00:15:17,300
with the microphone array so source

00:15:14,630 --> 00:15:18,590
localization speech recognition whereas

00:15:17,300 --> 00:15:21,740
the other libraries don't i really have

00:15:18,590 --> 00:15:23,300
a good story for that but there is

00:15:21,740 --> 00:15:26,480
another open source library that does

00:15:23,300 --> 00:15:27,630
there's this library called hark from

00:15:26,480 --> 00:15:29,850
japan

00:15:27,630 --> 00:15:32,870
that does open source robot audition and

00:15:29,850 --> 00:15:36,360
they've actually reverse engineered the

00:15:32,870 --> 00:15:39,720
kinect audio devices so they can bring

00:15:36,360 --> 00:15:41,390
those in as a Linux alsa device and do

00:15:39,720 --> 00:15:44,370
these kinds of source localization

00:15:41,390 --> 00:15:49,890
speech recognition features they're also

00:15:44,370 --> 00:15:52,860
in Ross to just nice okay so that was

00:15:49,890 --> 00:15:55,770
that was the hardware now we'll talk

00:15:52,860 --> 00:15:59,480
about what this looks like in Ross so

00:15:55,770 --> 00:16:04,470
here's the stack layout that's relevant

00:15:59,480 --> 00:16:07,140
and the dependencies this is as of where

00:16:04,470 --> 00:16:09,840
teh there's there's some movement

00:16:07,140 --> 00:16:13,200
between electric and forte so an

00:16:09,840 --> 00:16:15,750
electric the three open and eyes tracker

00:16:13,200 --> 00:16:17,670
camera and launch we're all packages in

00:16:15,750 --> 00:16:19,170
the open and I connect stack and the

00:16:17,670 --> 00:16:22,440
dependencies weren't as nice and

00:16:19,170 --> 00:16:24,990
separated as they are here in forte all

00:16:22,440 --> 00:16:28,770
three of those are now unary stacks open

00:16:24,990 --> 00:16:31,260
and I connect is basically empty it's a

00:16:28,770 --> 00:16:37,410
meta stack that depends on all three of

00:16:31,260 --> 00:16:40,860
those and then of the processing that

00:16:37,410 --> 00:16:42,930
requires our big libraries like opencv

00:16:40,860 --> 00:16:48,630
npc i was pushed out into the image

00:16:42,930 --> 00:16:50,160
pipeline so open eye tracker is

00:16:48,630 --> 00:16:54,270
basically a very simple wrapper around

00:16:50,160 --> 00:16:57,180
opening eyes skeleton tracking and it

00:16:54,270 --> 00:17:00,120
publishes all the the joints that the

00:16:57,180 --> 00:17:03,180
skeleton tracker sees to TF what you

00:17:00,120 --> 00:17:05,130
heard about earlier so that you know

00:17:03,180 --> 00:17:09,720
this is the kind of thing that a couple

00:17:05,130 --> 00:17:12,770
attacks we saw earlier based off of then

00:17:09,720 --> 00:17:16,439
open and I camera is the camera driver

00:17:12,770 --> 00:17:20,490
so it I hope follows all the best

00:17:16,439 --> 00:17:25,589
practices that we heard about it's very

00:17:20,490 --> 00:17:27,480
minimal as a forte it just publishes the

00:17:25,589 --> 00:17:29,670
raw data streams its following this

00:17:27,480 --> 00:17:31,920
principle they should take as much

00:17:29,670 --> 00:17:34,050
processing as you can out of the out of

00:17:31,920 --> 00:17:36,690
the camera driver so you can reuse it

00:17:34,050 --> 00:17:39,060
with other with other cameras and also

00:17:36,690 --> 00:17:41,040
simplifies the camera driver which is

00:17:39,060 --> 00:17:46,170
still pretty complex for

00:17:41,040 --> 00:17:49,370
the Kinect so you get the the raw Bayer

00:17:46,170 --> 00:17:54,470
y UV RGB stream depending on the device

00:17:49,370 --> 00:18:00,120
yet the depth stream open eyes format is

00:17:54,470 --> 00:18:01,170
16-bit integer depths in millimeters and

00:18:00,120 --> 00:18:06,060
the rest of the Ross we use

00:18:01,170 --> 00:18:07,770
floating-point ups but it's okay and you

00:18:06,060 --> 00:18:10,230
can also get the IR stream what you need

00:18:07,770 --> 00:18:13,830
it which you really never do unless

00:18:10,230 --> 00:18:15,570
you're calibrating and then I see there

00:18:13,830 --> 00:18:17,850
there are two namespaces for depth and

00:18:15,570 --> 00:18:22,190
depth registered those those to me are

00:18:17,850 --> 00:18:24,270
semantically different and in the driver

00:18:22,190 --> 00:18:25,680
open and I will actually do the

00:18:24,270 --> 00:18:28,050
registration for you based on the

00:18:25,680 --> 00:18:29,550
factory calibration which is nice and if

00:18:28,050 --> 00:18:31,790
you're using the Asus that will even

00:18:29,550 --> 00:18:34,680
happen in hardware which is even better

00:18:31,790 --> 00:18:38,940
so to enable that you just use dynamic

00:18:34,680 --> 00:18:41,970
in figure bring up the connect the

00:18:38,940 --> 00:18:45,420
drivers panel and enable this depth

00:18:41,970 --> 00:18:48,420
registration check box and so you can

00:18:45,420 --> 00:18:51,000
see it kind of crumples up that depth

00:18:48,420 --> 00:18:55,650
image a little bit to make it match up

00:18:51,000 --> 00:18:57,840
with the RGB image then an image

00:18:55,650 --> 00:19:00,600
practice part of the image pipeline this

00:18:57,840 --> 00:19:02,850
does basic TD processing of camera

00:19:00,600 --> 00:19:05,970
images things like deep a ring and y UV

00:19:02,850 --> 00:19:09,660
conversion rectifying images to correct

00:19:05,970 --> 00:19:12,900
for distortion and then the the new

00:19:09,660 --> 00:19:16,970
stuff is in depth image Brock so this

00:19:12,900 --> 00:19:20,100
was all written for the kinect basically

00:19:16,970 --> 00:19:24,180
so a couple of things will do is take a

00:19:20,100 --> 00:19:26,100
depth image convert to disparity if you

00:19:24,180 --> 00:19:31,560
need to interrupt with stereo processing

00:19:26,100 --> 00:19:34,890
notes or to a point cloud and it will

00:19:31,560 --> 00:19:37,080
also if you have a good calibration so

00:19:34,890 --> 00:19:39,600
they are the internal camera camera

00:19:37,080 --> 00:19:42,510
parameters and the transform between two

00:19:39,600 --> 00:19:45,870
cameras it can register a depth image to

00:19:42,510 --> 00:19:49,020
a new camera frame so this is exactly

00:19:45,870 --> 00:19:52,170
what open eye is doing for the connects

00:19:49,020 --> 00:19:53,880
ir and the kinect RGB cameras and then

00:19:52,170 --> 00:19:56,550
you can combine that with an

00:19:53,880 --> 00:19:59,580
be image and get a nice color point

00:19:56,550 --> 00:20:03,030
cloud so that depth limits Brock is

00:19:59,580 --> 00:20:06,030
written during development for the

00:20:03,030 --> 00:20:07,950
connect but depth images now are well

00:20:06,030 --> 00:20:10,140
defined in Ross there was an RTP about

00:20:07,950 --> 00:20:12,840
them depth image products is an image

00:20:10,140 --> 00:20:15,420
pipeline so this is perfectly usable

00:20:12,840 --> 00:20:17,280
with any other depth device that you

00:20:15,420 --> 00:20:19,500
care to write a driver for whether

00:20:17,280 --> 00:20:21,780
that's stereo or time of flight or

00:20:19,500 --> 00:20:26,040
whatever anything that will produce the

00:20:21,780 --> 00:20:28,470
depth image and this is all this is all

00:20:26,040 --> 00:20:30,510
written as node 'let's so we just heard

00:20:28,470 --> 00:20:33,180
a little bit about those this is very

00:20:30,510 --> 00:20:38,510
important to actually make all the sea

00:20:33,180 --> 00:20:41,730
fishin so then open and i launch

00:20:38,510 --> 00:20:44,520
basically its job is to combine all of

00:20:41,730 --> 00:20:48,060
these node 'let's into some coherent

00:20:44,520 --> 00:20:50,310
system and if if you saw my lightning

00:20:48,060 --> 00:20:52,800
talk yesterday if you want to see all

00:20:50,310 --> 00:20:55,680
those tricks taken to their logical or

00:20:52,800 --> 00:20:58,920
not so logical conclusion you look in

00:20:55,680 --> 00:21:01,320
open i launch which has a whole set of

00:20:58,920 --> 00:21:03,320
launch files that build up this this

00:21:01,320 --> 00:21:06,600
computation graph using node 'let's a

00:21:03,320 --> 00:21:09,450
middle also provide a default TF tree

00:21:06,600 --> 00:21:11,880
linking the cameras on the sensor and

00:21:09,450 --> 00:21:13,830
forte that's optional you can sub in

00:21:11,880 --> 00:21:17,130
something better based on calibration if

00:21:13,830 --> 00:21:19,620
you want to and this command is

00:21:17,130 --> 00:21:25,740
basically what you use to bring up the

00:21:19,620 --> 00:21:28,020
connect and it's very easy it publishes

00:21:25,740 --> 00:21:31,470
a bunch of topics so you can check out

00:21:28,020 --> 00:21:33,420
the reference api but these are the ones

00:21:31,470 --> 00:21:36,750
that you would commonly actually use you

00:21:33,420 --> 00:21:41,220
have the point cloud topics with just X

00:21:36,750 --> 00:21:43,740
Y Z or X Y Z RGB you have your RGB

00:21:41,220 --> 00:21:46,400
images generally when you're processing

00:21:43,740 --> 00:21:49,650
you should always use rectified images

00:21:46,400 --> 00:21:53,730
so that projection actually works

00:21:49,650 --> 00:21:57,290
correctly and also the rod depth images

00:21:53,730 --> 00:22:02,610
if you prefer to work on those directly

00:21:57,290 --> 00:22:05,010
so I'm this division into the node

00:22:02,610 --> 00:22:07,740
'let's this wasn't always how it was and

00:22:05,010 --> 00:22:09,480
diamondback that connect driver was this

00:22:07,740 --> 00:22:12,330
monolithic thing that did all the

00:22:09,480 --> 00:22:14,160
processing and in one node and this

00:22:12,330 --> 00:22:16,230
proved to be pretty inflexible like

00:22:14,160 --> 00:22:19,460
their use cases that just weren't

00:22:16,230 --> 00:22:21,809
satisfied that by that something that

00:22:19,460 --> 00:22:25,230
I've heard several times that people

00:22:21,809 --> 00:22:27,780
want to do is have the driver just run

00:22:25,230 --> 00:22:30,900
on some resource-limited device and do

00:22:27,780 --> 00:22:33,240
all the processing on a separate beefier

00:22:30,900 --> 00:22:35,910
computer and you can do that actually

00:22:33,240 --> 00:22:41,670
quite easily with the with a nodal it

00:22:35,910 --> 00:22:44,880
based Ross open and I driver so you run

00:22:41,670 --> 00:22:48,390
the the open and i know'd that's just

00:22:44,880 --> 00:22:51,300
wrapping the minimal driver Noblet on

00:22:48,390 --> 00:22:55,380
your on your resource-limited device and

00:22:51,300 --> 00:22:58,740
then you do open an i launch but tell it

00:22:55,380 --> 00:23:01,050
not to load the driver this stuff is

00:22:58,740 --> 00:23:02,610
forte you can do the same kind of thing

00:23:01,050 --> 00:23:07,950
an electric of course but just looks a

00:23:02,610 --> 00:23:09,960
little bit different and recording bag

00:23:07,950 --> 00:23:13,020
files looks very similar to that

00:23:09,960 --> 00:23:18,809
actually when you record back file you

00:23:13,020 --> 00:23:20,340
want to record the raw topics a lot of

00:23:18,809 --> 00:23:22,290
times your first instinct is to record

00:23:20,340 --> 00:23:24,960
the point clouds but those are huge on

00:23:22,290 --> 00:23:28,590
their bandwidth hogs and in fact if you

00:23:24,960 --> 00:23:30,720
try to record XYZ RGB you actually can't

00:23:28,590 --> 00:23:34,740
write them to the hard disk fast enough

00:23:30,720 --> 00:23:37,050
as they're coming in but the image raw

00:23:34,740 --> 00:23:40,410
for the kinect is bare so it's one byte

00:23:37,050 --> 00:23:43,940
per pixel the raw depth image is two

00:23:40,410 --> 00:23:47,490
bytes per pixel so this this is very

00:23:43,940 --> 00:23:49,650
this is very handleable and if you

00:23:47,490 --> 00:23:52,530
really want to bring that down even

00:23:49,650 --> 00:23:55,350
further you can use the compression

00:23:52,530 --> 00:23:57,300
facilities offered by image transport so

00:23:55,350 --> 00:24:00,179
the RGB you can you know use whatever

00:23:57,300 --> 00:24:02,340
compression you want with depth you

00:24:00,179 --> 00:24:06,320
should really really only suitable thing

00:24:02,340 --> 00:24:08,550
we have is PNG for losses encoding

00:24:06,320 --> 00:24:11,760
compression techniques for color don't

00:24:08,550 --> 00:24:13,890
really map so well on to depth images

00:24:11,760 --> 00:24:15,420
they're not designed for that and there

00:24:13,890 --> 00:24:19,830
there's a tutorial for how to do this

00:24:15,420 --> 00:24:21,090
and it would not launch so I I spent a

00:24:19,830 --> 00:24:23,040
bunch of time

00:24:21,090 --> 00:24:24,930
working on connect calibration so I'm

00:24:23,040 --> 00:24:27,000
going to talk about a little bit but

00:24:24,930 --> 00:24:29,400
most of the time you actually shouldn't

00:24:27,000 --> 00:24:31,770
do this the factory calibration that's

00:24:29,400 --> 00:24:34,710
baked into the Kinect is actually pretty

00:24:31,770 --> 00:24:37,530
good so there are only a couple reasons

00:24:34,710 --> 00:24:39,300
to calibrate if you want to squeeze out

00:24:37,530 --> 00:24:41,550
like that last bit a little bit of

00:24:39,300 --> 00:24:44,220
accuracy maybe you can do a little bit

00:24:41,550 --> 00:24:46,470
better with the user calibration or if

00:24:44,220 --> 00:24:49,650
you want to calibrate the connect to

00:24:46,470 --> 00:24:51,240
other cameras in your system then this

00:24:49,650 --> 00:24:55,800
becomes interesting and we have tools to

00:24:51,240 --> 00:24:57,720
do that so first thing you always need

00:24:55,800 --> 00:25:00,720
to do is do the intrinsic calibration of

00:24:57,720 --> 00:25:02,760
all of your cameras these are the

00:25:00,720 --> 00:25:05,280
internal parameters that tell you how to

00:25:02,760 --> 00:25:11,910
project some point in space into the

00:25:05,280 --> 00:25:14,970
image plane and oops here we go so yet

00:25:11,910 --> 00:25:16,890
we have a GUI tool and camera Kelly

00:25:14,970 --> 00:25:18,930
camera calibration package that will do

00:25:16,890 --> 00:25:21,690
this you need a checkerboard target you

00:25:18,930 --> 00:25:23,570
get a bunch of use for it and it knows

00:25:21,690 --> 00:25:26,370
the dimensions and it can calculate

00:25:23,570 --> 00:25:28,640
focal length optical center of the image

00:25:26,370 --> 00:25:32,850
distortion parameters based on those

00:25:28,640 --> 00:25:34,020
those captured images well the IR cam

00:25:32,850 --> 00:25:35,760
you have to be a little bit careful

00:25:34,020 --> 00:25:37,800
because that speckle pattern will mess

00:25:35,760 --> 00:25:40,080
up your your corners on the checkerboard

00:25:37,800 --> 00:25:41,850
but you can just like put up post it

00:25:40,080 --> 00:25:49,950
over the projector and it kind of

00:25:41,850 --> 00:25:52,290
diffuses it out and it works and then

00:25:49,950 --> 00:25:55,710
the extrinsic calibration when you have

00:25:52,290 --> 00:25:58,950
multiple cameras it gets interesting I'm

00:25:55,710 --> 00:26:02,090
skipping slides here so here's a simpler

00:25:58,950 --> 00:26:05,100
setup and what we did here is we have a

00:26:02,090 --> 00:26:08,790
5-megapixel priscilla camera mounted

00:26:05,100 --> 00:26:11,220
directly underneath the IR camera you

00:26:08,790 --> 00:26:12,840
might want to do this if the RGB camera

00:26:11,220 --> 00:26:15,690
that's built in just isn't cutting it

00:26:12,840 --> 00:26:18,330
for you so this way we got a lot more

00:26:15,690 --> 00:26:20,790
resolution and a lot more control over

00:26:18,330 --> 00:26:23,040
what the RGB camera is doing in terms of

00:26:20,790 --> 00:26:26,550
white balance exposure all these nice

00:26:23,040 --> 00:26:29,340
things or you can go really nuts this is

00:26:26,550 --> 00:26:32,130
a setup where has two connects to

00:26:29,340 --> 00:26:33,030
priscilla cos and then a webcam just for

00:26:32,130 --> 00:26:35,580
kicks

00:26:33,030 --> 00:26:37,710
and I don't know if you can actually see

00:26:35,580 --> 00:26:40,470
in the RV's window but this stuff

00:26:37,710 --> 00:26:42,270
actually is calibrated to each other so

00:26:40,470 --> 00:26:46,230
that it looks like it all meshes

00:26:42,270 --> 00:26:49,620
together pretty well so the tool that we

00:26:46,230 --> 00:26:53,130
use to do that is in the camera post

00:26:49,620 --> 00:26:56,790
stack and basically what you do is you

00:26:53,130 --> 00:27:02,280
take a checkerboard and you hold it very

00:26:56,790 --> 00:27:04,160
very still and this server looks finds

00:27:02,280 --> 00:27:07,140
the checkerboard in all the images and

00:27:04,160 --> 00:27:09,540
finds a time interval where it's still

00:27:07,140 --> 00:27:12,450
and all the images so you get a really

00:27:09,540 --> 00:27:14,490
robust view of it and then each time you

00:27:12,450 --> 00:27:17,160
get a new view it reruns this

00:27:14,490 --> 00:27:19,470
optimization to refine where the cameras

00:27:17,160 --> 00:27:24,320
are with respect to each other and it

00:27:19,470 --> 00:27:27,150
publishes of that transform tree over TF

00:27:24,320 --> 00:27:35,490
so you can actually do live calibration

00:27:27,150 --> 00:27:38,220
even on your running system so it does

00:27:35,490 --> 00:27:41,340
summarize that part what is Ross give

00:27:38,220 --> 00:27:43,860
you you get easy device access yet great

00:27:41,340 --> 00:27:45,710
visualization tools you have data

00:27:43,860 --> 00:27:48,590
logging playback through backfiles

00:27:45,710 --> 00:27:51,480
compression with image transport

00:27:48,590 --> 00:27:54,030
calibration with multi camera support if

00:27:51,480 --> 00:27:55,470
you need it and getting into the last

00:27:54,030 --> 00:27:59,490
part of the talk it integrates nicely

00:27:55,470 --> 00:28:05,760
with opencv and pcl and of the open and

00:27:59,490 --> 00:28:07,980
I niceties are dealt with for you so for

00:28:05,760 --> 00:28:11,060
actually processing all this data we

00:28:07,980 --> 00:28:13,680
have a couple answers for that also

00:28:11,060 --> 00:28:17,820
there's open CV which has been around

00:28:13,680 --> 00:28:22,190
for a long time for two division and pcl

00:28:17,820 --> 00:28:26,360
which is new and exciting for 3d vision

00:28:22,190 --> 00:28:29,810
so opencv it's got tons of stuff in it

00:28:26,360 --> 00:28:33,930
all kinds of general image processing

00:28:29,810 --> 00:28:36,710
transforms a bunch of the 2d stuff that

00:28:33,930 --> 00:28:39,540
I've showed here like debating

00:28:36,710 --> 00:28:43,160
rectification most of the camera

00:28:39,540 --> 00:28:43,160
calibration that's done through opencv

00:28:43,610 --> 00:28:46,790
the thing is the opencv really is for

00:28:46,350 --> 00:28:50,030
two

00:28:46,790 --> 00:28:52,130
division so that you can use it to your

00:28:50,030 --> 00:28:54,200
heart's content on the RGB image do all

00:28:52,130 --> 00:28:56,540
kinds of interesting processing on that

00:28:54,200 --> 00:28:58,790
and it's tempting to process the depth

00:28:56,540 --> 00:29:02,600
image directly because this in many

00:28:58,790 --> 00:29:04,940
cases this could be faster than working

00:29:02,600 --> 00:29:07,280
on this bigger point cloud you have to

00:29:04,940 --> 00:29:08,780
be careful though most of the algorithms

00:29:07,280 --> 00:29:11,270
an open CV that you might want to use

00:29:08,780 --> 00:29:13,760
won't do the right thing out of the box

00:29:11,270 --> 00:29:15,830
on depth images because they um they

00:29:13,760 --> 00:29:17,510
aren't written to handle invalid points

00:29:15,830 --> 00:29:20,330
and in any depth image you're going to

00:29:17,510 --> 00:29:22,640
have a lot of invalid points you also

00:29:20,330 --> 00:29:26,810
don't want to do things like blur over

00:29:22,640 --> 00:29:28,160
depth discontinuities there is one

00:29:26,810 --> 00:29:31,010
exception that I want to point out

00:29:28,160 --> 00:29:33,950
though the newest version of opencv

00:29:31,010 --> 00:29:36,170
that's in forte has this line mod object

00:29:33,950 --> 00:29:38,660
recognition algorithm that one of our

00:29:36,170 --> 00:29:41,930
interns Stefano interstice are devised

00:29:38,660 --> 00:29:44,720
and this is very cool this is working on

00:29:41,930 --> 00:29:48,130
the RGB image and the depth image it's

00:29:44,720 --> 00:29:53,690
multimodal it detects color gradients in

00:29:48,130 --> 00:29:58,430
the RGB image it detects normal

00:29:53,690 --> 00:30:01,760
gradients or we'll normals and the depth

00:29:58,430 --> 00:30:03,620
image and fits template does template

00:30:01,760 --> 00:30:07,250
matching over both of those at the same

00:30:03,620 --> 00:30:08,930
time and it's extremely fast and you can

00:30:07,250 --> 00:30:13,310
see it here working well even on these

00:30:08,930 --> 00:30:16,370
very untextured objects which most

00:30:13,310 --> 00:30:18,680
methods for doing object range

00:30:16,370 --> 00:30:20,600
recognition on color images just fail on

00:30:18,680 --> 00:30:23,270
because there's there's not a lot there

00:30:20,600 --> 00:30:24,770
to find but if you have depth then you

00:30:23,270 --> 00:30:29,960
can actually see the boundaries and you

00:30:24,770 --> 00:30:32,900
can do a lot better so most of the time

00:30:29,960 --> 00:30:34,790
for 3d processing pc elsewhere it's at

00:30:32,900 --> 00:30:38,690
the point cloud library this does all

00:30:34,790 --> 00:30:41,630
kinds of awesome things for you 3d

00:30:38,690 --> 00:30:43,670
features object recognition has some

00:30:41,630 --> 00:30:47,450
spatial data structures like Katie trees

00:30:43,670 --> 00:30:49,250
and octrees if you have multiple 3d

00:30:47,450 --> 00:30:51,470
point cloud views of a scene you can

00:30:49,250 --> 00:30:55,580
register to those to each other and

00:30:51,470 --> 00:30:58,370
combine them you can fit planes and

00:30:55,580 --> 00:30:59,500
cylinders and I don't know other

00:30:58,370 --> 00:31:02,410
connects

00:30:59,500 --> 00:31:04,690
to point clouds and you can it has some

00:31:02,410 --> 00:31:07,420
nice segmentation capabilities too so

00:31:04,690 --> 00:31:10,540
I've just got some videos that show off

00:31:07,420 --> 00:31:13,120
what pcl can do this is connect fusion

00:31:10,540 --> 00:31:15,520
this is an algorithm I came out of

00:31:13,120 --> 00:31:18,480
Microsoft and there's an open source

00:31:15,520 --> 00:31:21,910
implementation of it in pcl this was

00:31:18,480 --> 00:31:24,250
cooperation between people willow people

00:31:21,910 --> 00:31:27,670
at Nvidia and some of our opencv

00:31:24,250 --> 00:31:30,040
developers in Russia and what is doing

00:31:27,670 --> 00:31:32,470
here is you can see you know the the

00:31:30,040 --> 00:31:34,480
color image just for reference on the

00:31:32,470 --> 00:31:36,730
bottom right the actual depth image

00:31:34,480 --> 00:31:40,860
that's getting in and then it's fusing

00:31:36,730 --> 00:31:43,300
that data over time to get a really nice

00:31:40,860 --> 00:31:45,910
reconstruction of the world in the top

00:31:43,300 --> 00:31:48,970
right and this is being done in real

00:31:45,910 --> 00:31:57,880
time all of the heavy computation is

00:31:48,970 --> 00:31:59,920
being done on the GPU and here here's

00:31:57,880 --> 00:32:03,040
some work from another one of our

00:31:59,920 --> 00:32:07,300
interns Julius cameral he developed an

00:32:03,040 --> 00:32:10,030
octree library for pcl just this spatial

00:32:07,300 --> 00:32:13,150
structure so you divide your point cloud

00:32:10,030 --> 00:32:16,120
into space and then those subspace those

00:32:13,150 --> 00:32:18,670
eight regions into sub eight regions and

00:32:16,120 --> 00:32:22,000
this is kind of like a binary tree for

00:32:18,670 --> 00:32:24,730
looking up 3d locations finding what

00:32:22,000 --> 00:32:28,240
points are around it and his interest

00:32:24,730 --> 00:32:30,370
was in compression so he actually came

00:32:28,240 --> 00:32:33,970
up with a very neat compression

00:32:30,370 --> 00:32:36,880
algorithm using the sock tree and then

00:32:33,970 --> 00:32:39,550
was able to further ease it for doing

00:32:36,880 --> 00:32:41,170
compression over time by detecting which

00:32:39,550 --> 00:32:43,750
points have actually changed from frame

00:32:41,170 --> 00:32:47,860
to frame and so here what you're seeing

00:32:43,750 --> 00:32:49,810
is actually a cross atlantic 3d skype

00:32:47,860 --> 00:32:54,640
session kind of where they're using this

00:32:49,810 --> 00:33:00,300
compression to send the kinect 3d video

00:32:54,640 --> 00:33:03,120
feed to each other so very cool and

00:33:00,300 --> 00:33:06,040
finally this is some of the most recent

00:33:03,120 --> 00:33:08,650
segmentation stuff from PCO right now

00:33:06,040 --> 00:33:10,540
it's doing planar segmentation so it's

00:33:08,650 --> 00:33:12,350
finding the table the computer screen

00:33:10,540 --> 00:33:15,020
and

00:33:12,350 --> 00:33:17,210
and a second it should turn on yeah

00:33:15,020 --> 00:33:19,880
object segmentation so it's finding

00:33:17,210 --> 00:33:23,600
things that are sticking up from the

00:33:19,880 --> 00:33:31,190
from the planes and fitting nice little

00:33:23,600 --> 00:33:33,250
little regions to those and of course

00:33:31,190 --> 00:33:35,720
these are nicely integrated in Ross

00:33:33,250 --> 00:33:39,799
there are some bridge packages Siri

00:33:35,720 --> 00:33:43,250
bridge pcl Ross for matching Ross

00:33:39,799 --> 00:33:46,340
messages to peace to the native data

00:33:43,250 --> 00:33:48,890
types and in fact the pcl point cloud

00:33:46,340 --> 00:33:50,780
type is actually registered as a message

00:33:48,890 --> 00:33:54,980
in Ross so you can pass shared pointers

00:33:50,780 --> 00:33:57,500
to pcl point clouds around directly so

00:33:54,980 --> 00:34:00,429
we heard about no lets you know it's

00:33:57,500 --> 00:34:02,840
great to use them whenever possible

00:34:00,429 --> 00:34:06,950
point clouds are huge images are also

00:34:02,840 --> 00:34:08,389
big so for the best efficiency one thing

00:34:06,950 --> 00:34:10,250
you can do to optimize this write your

00:34:08,389 --> 00:34:12,859
code is no glitz and load them into the

00:34:10,250 --> 00:34:14,750
same manager that's running the kinect

00:34:12,859 --> 00:34:20,119
driver and all the other processing

00:34:14,750 --> 00:34:23,119
that's going on and so as I said like

00:34:20,119 --> 00:34:26,419
these like the kinect is just like a

00:34:23,119 --> 00:34:29,919
fire hose of data to your computer so no

00:34:26,419 --> 00:34:32,840
let's are one way of optimizing but

00:34:29,919 --> 00:34:35,330
often what you want to do is just turn

00:34:32,840 --> 00:34:37,520
down that stream make it a little bit

00:34:35,330 --> 00:34:40,550
easier to handle so there are a few ways

00:34:37,520 --> 00:34:42,950
that you can do that best of all is to

00:34:40,550 --> 00:34:49,340
do it on the device itself and hardware

00:34:42,950 --> 00:34:53,300
so you can get down to a 320 x 240 image

00:34:49,340 --> 00:34:55,429
I think at least on the asus and in

00:34:53,300 --> 00:34:58,070
parenthesis I've cropping this isn't

00:34:55,429 --> 00:35:01,730
actually implemented in the Ross driver

00:34:58,070 --> 00:35:04,310
it would be nice if it if it were open

00:35:01,730 --> 00:35:06,890
and I has some some cropping capability

00:35:04,310 --> 00:35:10,700
that I hope is actually implemented in

00:35:06,890 --> 00:35:12,470
hardware on these devices and if you can

00:35:10,700 --> 00:35:14,869
reduce the frame rate and hardware it's

00:35:12,470 --> 00:35:17,859
nice to do that too I don't I don't know

00:35:14,869 --> 00:35:21,950
that they can act or the asus actually

00:35:17,859 --> 00:35:24,650
let you do that very much alternately

00:35:21,950 --> 00:35:27,470
when the driver you can just drop frames

00:35:24,650 --> 00:35:29,809
and so in dynamic reconfigure for the

00:35:27,470 --> 00:35:32,119
driver know that there is like a data

00:35:29,809 --> 00:35:35,329
skip parameter that you can use to just

00:35:32,119 --> 00:35:40,390
have it drop frames and software and not

00:35:35,329 --> 00:35:43,400
do any further processing on those if

00:35:40,390 --> 00:35:45,440
binning or cropping and hardware is

00:35:43,400 --> 00:35:48,500
isn't working for you you can also do

00:35:45,440 --> 00:35:53,359
that in software image proc has a crop

00:35:48,500 --> 00:35:55,609
decimate node lit where you can do a do

00:35:53,359 --> 00:35:57,319
decimation basically throw out the

00:35:55,609 --> 00:36:01,160
points that you don't need or you can do

00:35:57,319 --> 00:36:05,960
a better resizing operation from opencv

00:36:01,160 --> 00:36:08,480
and crop it but that is actually kind of

00:36:05,960 --> 00:36:10,339
nice for bayer images if you subsample

00:36:08,480 --> 00:36:12,500
bayer or if you do bending on bear

00:36:10,339 --> 00:36:14,240
images and hardware you average out all

00:36:12,500 --> 00:36:16,220
of your color samples so you just get a

00:36:14,240 --> 00:36:18,170
monochrome image but if you do in

00:36:16,220 --> 00:36:19,730
software then you can you know pick and

00:36:18,170 --> 00:36:25,250
choose the data that you need and still

00:36:19,730 --> 00:36:27,230
get color and finally you can use pcl to

00:36:25,250 --> 00:36:30,470
down sample the point cloud you that you

00:36:27,230 --> 00:36:33,289
get with voxel grid down sampling and

00:36:30,470 --> 00:36:37,579
this is nice because it actually does

00:36:33,289 --> 00:36:42,829
down sampling in space and so you you

00:36:37,579 --> 00:36:47,869
get a much better you get a much better

00:36:42,829 --> 00:36:50,809
like distribution of down sample points

00:36:47,869 --> 00:36:52,760
for pc LS processing purposes but the

00:36:50,809 --> 00:36:56,150
further down you get on on this diagram

00:36:52,760 --> 00:37:01,279
you know the more you've paid and cpu

00:36:56,150 --> 00:37:03,609
time so thanks for listening any

00:37:01,279 --> 00:37:03,609
questions

00:37:11,940 --> 00:37:19,150
okay so the first question was how can

00:37:14,980 --> 00:37:24,730
you skip the surrender step when using

00:37:19,150 --> 00:37:26,560
the tracker so right right now to start

00:37:24,730 --> 00:37:29,260
to initialize the tracker you have to do

00:37:26,560 --> 00:37:32,560
like this this surrender pose so that it

00:37:29,260 --> 00:37:35,830
sees you and in the newer versions of

00:37:32,560 --> 00:37:38,230
open and I this this step has been

00:37:35,830 --> 00:37:42,940
lifted unfortunately the version of open

00:37:38,230 --> 00:37:46,210
I that's used in Ross right now is old

00:37:42,940 --> 00:37:47,620
enough that it doesn't have that new

00:37:46,210 --> 00:37:50,380
feature that you don't have to do the

00:37:47,620 --> 00:37:52,060
surrender um so I don't know that

00:37:50,380 --> 00:37:54,040
version will get updated at some point

00:37:52,060 --> 00:37:56,170
with the newer versions we had some

00:37:54,040 --> 00:37:57,640
issue with multiple connects so we

00:37:56,170 --> 00:38:02,500
weren't we weren't in a big hurry to do

00:37:57,640 --> 00:38:06,010
that and the the second question was if

00:38:02,500 --> 00:38:08,710
you want to get just like one image on

00:38:06,010 --> 00:38:12,580
request right instead of subscribing to

00:38:08,710 --> 00:38:17,830
the stream there are a couple of answers

00:38:12,580 --> 00:38:20,620
to that you can you can do like a Ross

00:38:17,830 --> 00:38:23,920
wait for message and Ross CPP there's

00:38:20,620 --> 00:38:26,110
some function call where it wraps it up

00:38:23,920 --> 00:38:28,330
but basically it creates a subscriber

00:38:26,110 --> 00:38:33,640
ways to get a message in and immediately

00:38:28,330 --> 00:38:35,680
unsubscribes so you can do that you can

00:38:33,640 --> 00:38:37,960
also do that on the driver side there is

00:38:35,680 --> 00:38:40,540
in fact there's an API defined for this

00:38:37,960 --> 00:38:42,820
the polled camera API where you do a

00:38:40,540 --> 00:38:46,420
service call requesting the driver to

00:38:42,820 --> 00:38:48,430
send an image on some topic so for I

00:38:46,420 --> 00:38:50,590
think the Priscilla driver is the only

00:38:48,430 --> 00:38:53,800
thing that actually implements that it's

00:38:50,590 --> 00:38:56,140
it's really intended for high-resolution

00:38:53,800 --> 00:38:58,870
cameras where you want to do like some

00:38:56,140 --> 00:39:04,390
very finely controlled like binning and

00:38:58,870 --> 00:39:10,390
on ROI kind of operations okay yeah so

00:39:04,390 --> 00:39:14,560
I'm pointing out that pcl if you go to

00:39:10,390 --> 00:39:16,450
like the pcl PPA and get their their

00:39:14,560 --> 00:39:18,370
version of pcl and isles dependencies

00:39:16,450 --> 00:39:20,170
they have a more recent version of open

00:39:18,370 --> 00:39:22,500
and I and there so you can try it out

00:39:20,170 --> 00:39:22,500
from that

00:39:23,680 --> 00:39:30,590
so oh it's pcl has its own

00:39:27,950 --> 00:39:35,720
implementation of skeleton tracking okay

00:39:30,590 --> 00:39:39,110
pcl has skeleton tracking yeah that's

00:39:35,720 --> 00:39:41,210
that demo is someone holding a connect

00:39:39,110 --> 00:39:43,490
in their hand and waving it around the

00:39:41,210 --> 00:39:46,490
scene that's it's the only sensor that's

00:39:43,490 --> 00:39:48,740
being used and so it's yeah it's some

00:39:46,490 --> 00:39:52,850
collecting point clouds registering it

00:39:48,740 --> 00:39:55,910
to its seen before and you know seeing

00:39:52,850 --> 00:39:58,430
from what it sees across multiple views

00:39:55,910 --> 00:40:02,990
it's guessing at what the true geometry

00:39:58,430 --> 00:40:05,030
is and there's a paper on that from

00:40:02,990 --> 00:40:11,380
Microsoft that you can check out for

00:40:05,030 --> 00:40:16,790
details so does the asus work on USB 3.0

00:40:11,380 --> 00:40:19,610
i am not sure about anything with these

00:40:16,790 --> 00:40:25,520
devices USB 3 0 but i think that people

00:40:19,610 --> 00:40:29,240
have had problems with it what was the

00:40:25,520 --> 00:40:33,410
first part of the question Oh for hark

00:40:29,240 --> 00:40:35,510
Oh for hark localization what's the

00:40:33,410 --> 00:40:42,650
actress she's like I actually don't know

00:40:35,510 --> 00:40:45,350
um I haven't tried that out myself right

00:40:42,650 --> 00:40:47,930
um yeah when we were doing this like

00:40:45,350 --> 00:40:49,970
crazy like calibrate five cameras to

00:40:47,930 --> 00:40:51,320
each other kind of thing they'd find

00:40:49,970 --> 00:40:53,390
that like everything worked out nicely

00:40:51,320 --> 00:40:55,040
except the depths would like point out a

00:40:53,390 --> 00:40:57,050
little bit further than they should and

00:40:55,040 --> 00:40:58,520
we didn't like that was like the one

00:40:57,050 --> 00:41:00,440
piece that we didn't really have control

00:40:58,520 --> 00:41:03,680
over because we were getting the depths

00:41:00,440 --> 00:41:05,540
from open and I and so we end up doing

00:41:03,680 --> 00:41:08,510
was adding some parameter to dynamic

00:41:05,540 --> 00:41:11,000
reconfigure where it just applies some

00:41:08,510 --> 00:41:13,970
constant offset to the depths that you

00:41:11,000 --> 00:41:23,450
get and then then everything lined up

00:41:13,970 --> 00:41:26,060
very nicely um yeah if it's like if it's

00:41:23,450 --> 00:41:29,300
like a keyboard or something maybe

00:41:26,060 --> 00:41:31,190
but like night if you're streaming like

00:41:29,300 --> 00:41:33,500
both data streams from they connect it

00:41:31,190 --> 00:41:40,490
really does pretty much take over that

00:41:33,500 --> 00:41:42,260
that controller oh so a good question

00:41:40,490 --> 00:41:45,350
can you control the motor and the

00:41:42,260 --> 00:41:48,260
accelerometer and the Kinect and that's

00:41:45,350 --> 00:41:51,110
not exposed in open and I but lib

00:41:48,260 --> 00:41:53,780
three-necked does that and in fact you

00:41:51,110 --> 00:41:56,450
can use those those functions side by

00:41:53,780 --> 00:41:59,750
side so you can use open and I to stream

00:41:56,450 --> 00:42:02,270
off the data and then the accelerometer

00:41:59,750 --> 00:42:05,420
and the tilt motor are separate USB

00:42:02,270 --> 00:42:08,500
devices so there is there's actually a

00:42:05,420 --> 00:42:13,280
Ross package for this called connect ox

00:42:08,500 --> 00:42:15,860
connect aux and that that should let you

00:42:13,280 --> 00:42:18,280
get to the accelerometer data and the

00:42:15,860 --> 00:42:18,280
tilt motor

00:42:24,720 --> 00:42:26,780

YouTube URL: https://www.youtube.com/watch?v=ZTR16W0DsEM


