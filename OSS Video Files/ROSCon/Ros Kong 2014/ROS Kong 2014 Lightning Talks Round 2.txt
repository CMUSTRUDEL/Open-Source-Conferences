Title: ROS Kong 2014 Lightning Talks Round 2
Publication date: 2014-07-17
Playlist: Ros Kong 2014
Description: 
	Unaltered video by Open Robotics from https://roscon.ros.org/hk/2014/ under the Attribution-NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) License https://creativecommons.org/licenses/by-nc-nd/3.0/
Captions: 
	00:00:00,000 --> 00:00:05,400
all right my name is Nicholas art from

00:00:02,970 --> 00:00:07,350
technical university in Munich and I

00:00:05,400 --> 00:00:11,130
want to present to you today the Ross

00:00:07,350 --> 00:00:14,250
web controller it's a versatile system

00:00:11,130 --> 00:00:17,640
for creating gooeys in graphical user

00:00:14,250 --> 00:00:21,090
interfaces similar to actually tools but

00:00:17,640 --> 00:00:25,680
not as far by drag-and-drop in the

00:00:21,090 --> 00:00:27,779
browser we target mainly we work mainly

00:00:25,680 --> 00:00:33,809
with simple robots such as the turtle

00:00:27,779 --> 00:00:35,910
but well it's an html5-based thing a

00:00:33,809 --> 00:00:37,800
system of course no Rose is required on

00:00:35,910 --> 00:00:39,989
the client site as Daniel order already

00:00:37,800 --> 00:00:43,230
mentioned we are mainly based on

00:00:39,989 --> 00:00:45,539
rosbridge raw slip chess and checker you

00:00:43,230 --> 00:00:48,059
I so another project working with the

00:00:45,539 --> 00:00:50,340
robot web tools right now we have

00:00:48,059 --> 00:00:53,969
different widgets for publishing showing

00:00:50,340 --> 00:00:57,149
messages parameters services images and

00:00:53,969 --> 00:01:00,359
plots and the system here is pretty much

00:00:57,149 --> 00:01:03,000
determined by the robot web tools ross

00:01:00,359 --> 00:01:05,540
and ross bridge which connects to the RO

00:01:03,000 --> 00:01:08,400
system using web sockets and chasing and

00:01:05,540 --> 00:01:11,430
in the browser we on our web controller

00:01:08,400 --> 00:01:15,299
with several widgets and also compatible

00:01:11,430 --> 00:01:19,890
the tablet will slide so it's available

00:01:15,299 --> 00:01:21,900
on github since the day if you're

00:01:19,890 --> 00:01:25,500
interested in that I feel free to

00:01:21,900 --> 00:01:28,890
contact me the code is still a bit messy

00:01:25,500 --> 00:01:30,240
like typical check very code structure

00:01:28,890 --> 00:01:33,390
you get without any additional

00:01:30,240 --> 00:01:35,490
frameworks so if you if any of you is

00:01:33,390 --> 00:01:37,770
experienced in backbone for instance or

00:01:35,490 --> 00:01:41,250
some other frameworks would be nice to

00:01:37,770 --> 00:01:43,950
talk to you next slide I want to show

00:01:41,250 --> 00:01:47,310
you a like a demo on this video here we

00:01:43,950 --> 00:01:49,229
have turtle sim running it's the window

00:01:47,310 --> 00:01:53,210
on the right is not part of of the web

00:01:49,229 --> 00:01:55,049
controller but i can drop we create a

00:01:53,210 --> 00:01:57,479
parameter widget which says the

00:01:55,049 --> 00:01:59,460
parameter and on the left our service

00:01:57,479 --> 00:02:03,840
which it would just change the color of

00:01:59,460 --> 00:02:06,210
our turtle sim the next thing is a quick

00:02:03,840 --> 00:02:07,710
way to set up a controller for this

00:02:06,210 --> 00:02:09,270
turtle sim basically you see the

00:02:07,710 --> 00:02:12,510
different buttons which send out

00:02:09,270 --> 00:02:13,710
different messages the cheetah and the

00:02:12,510 --> 00:02:16,560
videos somehow

00:02:13,710 --> 00:02:18,810
related to the screen capture and some

00:02:16,560 --> 00:02:23,100
of which is here showing you a topic and

00:02:18,810 --> 00:02:26,850
a plot this is a parameter window which

00:02:23,100 --> 00:02:29,070
configures individual regions now here's

00:02:26,850 --> 00:02:32,270
a video showing a live demo in a robot

00:02:29,070 --> 00:02:34,650
again we have some buttons which move

00:02:32,270 --> 00:02:39,240
the robot itself the platform and the

00:02:34,650 --> 00:02:41,340
PTU in it we see the image here taken

00:02:39,240 --> 00:02:46,650
from the robot camera its stream using

00:02:41,340 --> 00:02:48,780
mjpeg server right now this user

00:02:46,650 --> 00:02:51,750
interfaces you create you can save as a

00:02:48,780 --> 00:02:54,690
JSON file and just load in the browser

00:02:51,750 --> 00:02:57,510
even just by bookmarks another short

00:02:54,690 --> 00:02:59,970
demo here we have the dynamic cells and

00:02:57,510 --> 00:03:02,580
we plot the forces buried out of them

00:02:59,970 --> 00:03:10,860
guess that's pretty good thing for

00:03:02,580 --> 00:03:13,380
debugging the system thank you hello I'm

00:03:10,860 --> 00:03:16,560
Louise po well I'm an intern at the open

00:03:13,380 --> 00:03:18,450
source of Robotics foundation I started

00:03:16,560 --> 00:03:22,740
my internship with the outreach program

00:03:18,450 --> 00:03:25,890
for women organized by genome and I was

00:03:22,740 --> 00:03:29,460
working on jeezy web which is a web

00:03:25,890 --> 00:03:33,090
client for gazebo both desktop and bio

00:03:29,460 --> 00:03:35,910
versions so I started working on the

00:03:33,090 --> 00:03:37,830
mobile version for the internship in the

00:03:35,910 --> 00:03:40,320
outreach program for women so for

00:03:37,830 --> 00:03:41,790
example if you're running and simulation

00:03:40,320 --> 00:03:43,830
on cloud seem for example you can

00:03:41,790 --> 00:03:47,040
visualize your simulation and directly

00:03:43,830 --> 00:03:51,390
on the browser it uses some technology

00:03:47,040 --> 00:03:56,370
like HTML html5 WebGL a lot of 3 j's

00:03:51,390 --> 00:04:02,280
uses also rosli pjs jquery UI jquery

00:03:56,370 --> 00:04:05,930
mobile thanks so this is what i did for

00:04:02,280 --> 00:04:09,660
the the internship we already had a

00:04:05,930 --> 00:04:14,820
jeezy web for desktop and i adapted the

00:04:09,660 --> 00:04:18,390
basic functionality to mobile and always

00:04:14,820 --> 00:04:20,790
doing iteration with user tests so we

00:04:18,390 --> 00:04:22,380
would implement something give the users

00:04:20,790 --> 00:04:24,960
to use to see if they were comfortable

00:04:22,380 --> 00:04:26,190
with it and then come back and change

00:04:24,960 --> 00:04:28,950
and adapt

00:04:26,190 --> 00:04:32,550
as it was necessary I also implemented

00:04:28,950 --> 00:04:35,910
some some code to simplify the meshes

00:04:32,550 --> 00:04:38,580
because you have mobile devices are not

00:04:35,910 --> 00:04:39,960
as powerful as computer so for the

00:04:38,580 --> 00:04:41,940
simulation to run faster you need

00:04:39,960 --> 00:04:44,310
lighter mashes so for example you can

00:04:41,940 --> 00:04:46,950
simplify the meshes like as much as you

00:04:44,310 --> 00:04:48,960
want according to to how fast you want

00:04:46,950 --> 00:04:54,120
your your simulation to run and so on

00:04:48,960 --> 00:04:57,240
and the work is actually continuing so

00:04:54,120 --> 00:04:59,490
now we're I'm still collaborating with

00:04:57,240 --> 00:05:02,070
the open-source robotics foundation and

00:04:59,490 --> 00:05:04,620
we're adding a lot of functionality to

00:05:02,070 --> 00:05:06,570
have the goals to have as much

00:05:04,620 --> 00:05:11,100
functionality as jeezy client which is

00:05:06,570 --> 00:05:13,760
gazebos main minghui so now we can

00:05:11,100 --> 00:05:16,260
insert some models you can visualize

00:05:13,760 --> 00:05:18,180
like model transparent and so on so

00:05:16,260 --> 00:05:20,370
we're still adding a lot of things we're

00:05:18,180 --> 00:05:22,800
going to add lights and and so on so

00:05:20,370 --> 00:05:25,050
there's a lot happening right now and if

00:05:22,800 --> 00:05:27,840
anyone wants to calibrate there's a lot

00:05:25,050 --> 00:05:29,940
of information in gazebo wiki and on my

00:05:27,840 --> 00:05:31,260
personal blog or you can just look

00:05:29,940 --> 00:05:40,950
directly in the code it's all on

00:05:31,260 --> 00:05:42,450
bitbucket so that's it hello my name is

00:05:40,950 --> 00:05:45,660
Mike chunk from a university of

00:05:42,450 --> 00:05:48,600
washington i wasn't planning on

00:05:45,660 --> 00:05:51,090
attending arroz con before so it's kind

00:05:48,600 --> 00:05:56,480
of super live so please excuse any of

00:05:51,090 --> 00:05:58,919
the inaccuracies or problems so for last

00:05:56,480 --> 00:06:01,770
month or two I worked on this project

00:05:58,919 --> 00:06:03,780
called the archetype active a lot of

00:06:01,770 --> 00:06:07,470
times I wanted to compare two different

00:06:03,780 --> 00:06:09,600
sensors from the rtt back and then it

00:06:07,470 --> 00:06:12,030
was extremely hard to use just use the

00:06:09,600 --> 00:06:14,310
archetype have to do that so most what

00:06:12,030 --> 00:06:17,660
as a machine learning people we want to

00:06:14,310 --> 00:06:20,490
either for example wanted to build a up

00:06:17,660 --> 00:06:22,110
tour opening detector for example turtle

00:06:20,490 --> 00:06:24,840
bug is roaming around and we want to

00:06:22,110 --> 00:06:27,090
make sure that it can detect the doors

00:06:24,840 --> 00:06:29,940
or maybe some other kind of events and

00:06:27,090 --> 00:06:31,830
we want to lively compare or we want to

00:06:29,940 --> 00:06:34,620
look at the data in our kitty bag or

00:06:31,830 --> 00:06:37,890
something and compare the two important

00:06:34,620 --> 00:06:39,720
spots like for example when sensor data

00:06:37,890 --> 00:06:42,650
when the door is opening and then

00:06:39,720 --> 00:06:45,180
when door is closing or something so the

00:06:42,650 --> 00:06:47,280
conventional way that people usually do

00:06:45,180 --> 00:06:49,890
is just dump all the data in the matlab

00:06:47,280 --> 00:06:52,250
and then load it from there and then try

00:06:49,890 --> 00:06:54,810
to analyzing using a matlab a

00:06:52,250 --> 00:06:57,660
visualization it's similarly for Python

00:06:54,810 --> 00:06:59,820
and so forth it's it's a bit difficult

00:06:57,660 --> 00:07:01,680
when you have like a 10 gigabytes of

00:06:59,820 --> 00:07:03,630
data and then try and and you have when

00:07:01,680 --> 00:07:06,330
you have a lot of data it's pretty tough

00:07:03,630 --> 00:07:08,340
so our QT is really nice because I mean

00:07:06,330 --> 00:07:13,050
our kitty roz bag is really nice because

00:07:08,340 --> 00:07:14,880
uh it can it reads it it reads the back

00:07:13,050 --> 00:07:17,250
file in real time and it's not worrying

00:07:14,880 --> 00:07:18,270
about the memory and also it works

00:07:17,250 --> 00:07:24,300
really well with the other

00:07:18,270 --> 00:07:27,060
visualizations like our vids and all the

00:07:24,300 --> 00:07:30,120
other and our QT imageview and all these

00:07:27,060 --> 00:07:32,100
other thing so I can quickly look at

00:07:30,120 --> 00:07:34,710
what the sensor data really looks like

00:07:32,100 --> 00:07:39,210
by jumping around however it doesn't

00:07:34,710 --> 00:07:42,270
allow allow me to give give a nice way

00:07:39,210 --> 00:07:43,980
to compare two time points so if you

00:07:42,270 --> 00:07:47,310
scroll down a little bit go to our

00:07:43,980 --> 00:07:50,130
approach so we came up with this thing

00:07:47,310 --> 00:07:52,130
called our QT backed if we implemented

00:07:50,130 --> 00:07:55,380
the diff like functionality for our QT

00:07:52,130 --> 00:07:58,260
what it allows you to do is you can

00:07:55,380 --> 00:08:00,900
display to our QT back at the same time

00:07:58,260 --> 00:08:04,229
think about like get de for any kind of

00:08:00,900 --> 00:08:06,870
diff you know and then play play it

00:08:04,229 --> 00:08:09,000
through and jump around to the places

00:08:06,870 --> 00:08:10,740
where you are interested in so for

00:08:09,000 --> 00:08:12,600
example like if I'm interested in two

00:08:10,740 --> 00:08:15,419
different time points where one is a

00:08:12,600 --> 00:08:17,310
door opening door closing and then well

00:08:15,419 --> 00:08:18,840
one is a tour of one instance of the

00:08:17,310 --> 00:08:21,390
door opening and then the other one is

00:08:18,840 --> 00:08:23,520
that for example here that where the red

00:08:21,390 --> 00:08:26,250
bar is like where they are where sensor

00:08:23,520 --> 00:08:27,750
day that the sensor data is about we're

00:08:26,250 --> 00:08:30,240
opening and then one is door closing

00:08:27,750 --> 00:08:33,240
then I can play them and then subscribe

00:08:30,240 --> 00:08:35,940
our RVs or other viewers can subscribe

00:08:33,240 --> 00:08:38,099
to two different of two different data

00:08:35,940 --> 00:08:39,630
last sensor source and then I can

00:08:38,099 --> 00:08:41,820
visually look at how they are different

00:08:39,630 --> 00:08:44,159
and this was really nice because I can

00:08:41,820 --> 00:08:46,530
get like intuitive idea how the how

00:08:44,159 --> 00:08:51,410
those two are different and maybe even

00:08:46,530 --> 00:08:51,410
form how to build a classifier on online

00:08:52,660 --> 00:08:59,529
yes so that's actually that's about it

00:08:56,110 --> 00:09:01,480
there are more more smaller features I

00:08:59,529 --> 00:09:04,060
can talk about for example you can dine

00:09:01,480 --> 00:09:05,529
up so what the problem is like you want

00:09:04,060 --> 00:09:07,120
to figure out what are the dynamic

00:09:05,529 --> 00:09:10,269
points that you're interested in for

00:09:07,120 --> 00:09:11,769
example when for maybe I have some

00:09:10,269 --> 00:09:13,779
markers that saying that these are the

00:09:11,769 --> 00:09:15,879
points of the doors open and doors are

00:09:13,779 --> 00:09:19,149
closed then i can create the some

00:09:15,879 --> 00:09:21,220
filters for extracting those time points

00:09:19,149 --> 00:09:23,199
so that i can jump around between two

00:09:21,220 --> 00:09:26,259
time points and compare and there are

00:09:23,199 --> 00:09:29,589
some smaller features that are supported

00:09:26,259 --> 00:09:31,060
by this and yeah that's it it's it's

00:09:29,589 --> 00:09:33,370
somewhat useful but there's a lot of

00:09:31,060 --> 00:09:39,490
bugs if you want to actually use it talk

00:09:33,370 --> 00:09:50,160
to me after thank you and available on

00:09:39,490 --> 00:09:53,680
github i'll post somehow hello hi I'm

00:09:50,160 --> 00:09:56,019
dr. David Hanson I I'm with Hanson

00:09:53,680 --> 00:09:58,899
robotics and we developed these

00:09:56,019 --> 00:10:01,420
extremely lifelike humanoid robots such

00:09:58,899 --> 00:10:03,250
as Diego's on at the University of

00:10:01,420 --> 00:10:07,000
California at San Diego machine

00:10:03,250 --> 00:10:09,339
perception lab so on my team and I built

00:10:07,000 --> 00:10:14,019
the face the body was actually built by

00:10:09,339 --> 00:10:15,579
the kokoro company out of tokyo so now

00:10:14,019 --> 00:10:17,670
there's this question why human-like

00:10:15,579 --> 00:10:19,660
right I mean there's lots of human-like

00:10:17,670 --> 00:10:21,730
robots in the world but lots of other

00:10:19,660 --> 00:10:23,860
ones well the main reason is that the

00:10:21,730 --> 00:10:25,720
human leg face is very attractive to

00:10:23,860 --> 00:10:28,269
people that's why you see human-like

00:10:25,720 --> 00:10:30,220
faces and movies and cartoons and so

00:10:28,269 --> 00:10:33,189
forth you see them actually used in

00:10:30,220 --> 00:10:35,559
theme parks medical education simulation

00:10:33,189 --> 00:10:37,319
medical simulation robots they're useful

00:10:35,559 --> 00:10:40,059
for these kinds of simulation and

00:10:37,319 --> 00:10:42,250
education applications and of course

00:10:40,059 --> 00:10:44,230
consumer products are kind of a

00:10:42,250 --> 00:10:49,089
well-known application for this stuff

00:10:44,230 --> 00:10:52,779
this is a one of a number of my robots

00:10:49,089 --> 00:10:56,559
uh-oh let's see if seems like that the

00:10:52,779 --> 00:10:58,689
connection here is a little wonky so you

00:10:56,559 --> 00:11:01,870
can see some of the natural looking

00:10:58,689 --> 00:11:03,910
facial expressions that my robots have

00:11:01,870 --> 00:11:06,040
generated they've got cameras in eyes

00:11:03,910 --> 00:11:06,400
they simulate a full range of facial

00:11:06,040 --> 00:11:07,990
express

00:11:06,400 --> 00:11:10,870
they're extremely lightweight say they

00:11:07,990 --> 00:11:17,410
mount on walking robot bodies on such as

00:11:10,870 --> 00:11:19,090
the hue bo robotic Geist and so we with

00:11:17,410 --> 00:11:20,710
the cameras in the eyes and algorithms

00:11:19,090 --> 00:11:22,420
for detecting faces they make eye

00:11:20,710 --> 00:11:25,050
contact with you they can have a

00:11:22,420 --> 00:11:27,940
conversation so we set up these sort of

00:11:25,050 --> 00:11:32,230
intelligent algorithms but also some

00:11:27,940 --> 00:11:35,080
theater and mix so so we simulate like

00:11:32,230 --> 00:11:38,080
the sci-fi writer philip k dick we also

00:11:35,080 --> 00:11:40,600
have a natural auto tutor software that

00:11:38,080 --> 00:11:42,220
at one point we from memphis university

00:11:40,600 --> 00:11:44,890
of memphis we connected einstein it was

00:11:42,220 --> 00:11:46,120
teaching physics so now that said with

00:11:44,890 --> 00:11:49,210
all this stuff it's been really

00:11:46,120 --> 00:11:52,690
difficult to connect in new features and

00:11:49,210 --> 00:11:55,450
extend its oh so we switched to Ross

00:11:52,690 --> 00:11:58,270
through with open cog that's been

00:11:55,450 --> 00:11:59,890
working to interface their AGI software

00:11:58,270 --> 00:12:01,120
with Ross they're located here in Hong

00:11:59,890 --> 00:12:05,020
Kong and we've been working with them

00:12:01,120 --> 00:12:07,630
quite a bit so we've developed also a

00:12:05,020 --> 00:12:09,850
binding between Ross and blender and

00:12:07,630 --> 00:12:12,220
that's the main thing that we're talking

00:12:09,850 --> 00:12:14,110
about today with the blender you have

00:12:12,220 --> 00:12:17,770
movie quality computer animation

00:12:14,110 --> 00:12:21,430
software it's really come of age it's

00:12:17,770 --> 00:12:24,550
quite mature so you can author inverse

00:12:21,430 --> 00:12:27,430
kinematics in Maya in in blender and

00:12:24,550 --> 00:12:29,980
it's very easy to put that ik chains

00:12:27,430 --> 00:12:32,710
together you can author all kinds of

00:12:29,980 --> 00:12:36,100
complex motions now there is a blender

00:12:32,710 --> 00:12:37,690
Ross binding that exists in a form

00:12:36,100 --> 00:12:39,250
called Moore's but that's with the

00:12:37,690 --> 00:12:43,840
blender game engine it doesn't have the

00:12:39,250 --> 00:12:47,770
complete set of features with blender so

00:12:43,840 --> 00:12:50,200
we created a blender binding and also

00:12:47,770 --> 00:12:55,060
with Moore's blender simulation sort of

00:12:50,200 --> 00:12:57,130
a replacement or parallel to gazebo and

00:12:55,060 --> 00:12:59,980
we're going to set up a demo in a few

00:12:57,130 --> 00:13:03,460
minutes and you can come see a real live

00:12:59,980 --> 00:13:05,920
Einstein head that will interact with

00:13:03,460 --> 00:13:08,370
you and track this guy's face and facial

00:13:05,920 --> 00:13:12,209
expressions as a kind of telex

00:13:08,370 --> 00:13:14,269
demonstration thank you so much

00:13:12,209 --> 00:13:14,269

YouTube URL: https://www.youtube.com/watch?v=uBl1CNATjMU


