Title: NUMA Optimizations in the FreeBSD Network Stack
Publication date: 2019-10-27
Playlist: EuroBSDCon 2019, Norway
Description: 
	I will discuss optimizations to keep network connections and their resources local to NUMA domains. These changes include:

Allocating NUMA local memory to back files sent via sendfile(9).
Allocating NUMA local memory for Kernel TLS crypto buffers.
Directing connections to TCP Pacers and kTLS workers bound to the local domain.
Directing incoming connections to Nginx workers bound to the local domain via modifications to SO_REUSEPORT_LB listen sockets.
I will present data from real Netflix servers showing an improvement of almost 2x on AMD EPYC (85Gbs - 165Gbs), and 1.3x on Intel Xeon (140Gb/s - 180Gbs). I will present data from the Xeon system showing a 50% reduction in cross-domain traffic.

Drew Gallatin

Drew started working on FreeBSD at Duke in the 90s, and was one of the people behind the FreeBSD/alpha port. He worked on zero-copy TCP optimizations for FreeBSD and was sending data at over 1Gb/s before gigabit Ethernet was generally available. He spent a decade at Myricom, optimizing their drivers. After a brief hiatus at Google, he landed at Netflix, where he works on optimizing the FreeBSD kernel and network stack for content delivery. He worked on the optimizations to serve unencrypted Netflix traffic at 100Gb/s, and then on more optimizations to send encrypted traffic at 100Gb/s.
Captions: 
	00:00:03,649 --> 00:00:07,950
so for those of you who weren't here in

00:00:06,390 --> 00:00:10,290
the last talk

00:00:07,950 --> 00:00:12,959
I'm drew Gallatin I've been a FreeBSD

00:00:10,290 --> 00:00:14,700
committer since the 90s I really really

00:00:12,959 --> 00:00:16,890
like fast stuff and making things go

00:00:14,700 --> 00:00:18,630
fast so the first thing I worked on in

00:00:16,890 --> 00:00:20,609
FreeBSD was the FreeBSD ports of the dec

00:00:18,630 --> 00:00:22,170
alpha with Doug Robson and I kicked

00:00:20,609 --> 00:00:24,720
around doing stupid stuff in the network

00:00:22,170 --> 00:00:26,820
stack and now I'm really lucky because I

00:00:24,720 --> 00:00:28,710
work for Netflix and I get to play with

00:00:26,820 --> 00:00:30,060
really fast machines that serve real

00:00:28,710 --> 00:00:32,759
traffic to real people on the real

00:00:30,060 --> 00:00:35,760
internet so I'm here to talk to you

00:00:32,759 --> 00:00:39,480
about web what I'm calling Numa siloing

00:00:35,760 --> 00:00:41,160
and the FreeBSD Network stack and we're

00:00:39,480 --> 00:00:44,039
really really what this is is to serve

00:00:41,160 --> 00:00:46,980
200 gigabits a second of TLS to Netflix

00:00:44,039 --> 00:00:52,079
customers from a single machine using

00:00:46,980 --> 00:00:54,469
FreeBSD of course so why do we want to

00:00:52,079 --> 00:00:57,930
serve this much traffic basically since

00:00:54,469 --> 00:01:00,660
2016 we've been serving at roughly 100

00:00:57,930 --> 00:01:03,179
gigabits a second with with kernel TLS

00:01:00,660 --> 00:01:07,230
from you know single what we call a

00:01:03,179 --> 00:01:10,500
flash appliance and we want to continue

00:01:07,230 --> 00:01:12,180
to drive our costs down and consolidate

00:01:10,500 --> 00:01:13,920
things and reduce density so we want to

00:01:12,180 --> 00:01:18,150
try to do chaordic if it's a second from

00:01:13,920 --> 00:01:19,710
a single box so in order to explain why

00:01:18,150 --> 00:01:23,280
this is a challenge I need to first talk

00:01:19,710 --> 00:01:26,640
a little bit about our workload so we

00:01:23,280 --> 00:01:28,409
use FreeBSD current and we're basically

00:01:26,640 --> 00:01:31,680
we're basically a web server we use the

00:01:28,409 --> 00:01:35,939
nginx web server and we serve all of our

00:01:31,680 --> 00:01:37,590
video the send file and kernel TLS and

00:01:35,939 --> 00:01:39,930
like like if you're here in the last

00:01:37,590 --> 00:01:45,869
talk you know we even able kernel TLS

00:01:39,930 --> 00:01:47,420
now with this TCP Tilak's TS TX TLS

00:01:45,869 --> 00:01:49,920
enable try to say that five times fast

00:01:47,420 --> 00:01:51,930
so why do we need 200 gig if it's a

00:01:49,920 --> 00:01:53,579
second why do we need new myrrh for 200

00:01:51,930 --> 00:01:55,079
gigabits a second and in fact what is

00:01:53,579 --> 00:01:56,310
new Mouse I'll explain you about in a

00:01:55,079 --> 00:02:00,030
little bit but first let me talk about

00:01:56,310 --> 00:02:02,460
why about where we are with a hundred

00:02:00,030 --> 00:02:05,250
and where we need to be for 200 so 400

00:02:02,460 --> 00:02:08,340
we started off with a Broadwell Xeon for

00:02:05,250 --> 00:02:11,940
our original hundred G and mm 2016 or so

00:02:08,340 --> 00:02:14,890
and that has about 60 gigabytes a second

00:02:11,940 --> 00:02:17,770
or about 400 gigabits a second of memory

00:02:14,890 --> 00:02:21,220
than with about 40 lanes of PCI Express

00:02:17,770 --> 00:02:24,010
and now we've moved on to newer Intel

00:02:21,220 --> 00:02:26,319
generation skylake in cascade Lake which

00:02:24,010 --> 00:02:28,390
have 90 gigabytes a second of memory

00:02:26,319 --> 00:02:31,320
bandwidth which if you noticed isn't

00:02:28,390 --> 00:02:33,640
quite - isn't quite 800 gigabits and

00:02:31,320 --> 00:02:39,940
they have a little bit more PCIe gen3

00:02:33,640 --> 00:02:42,700
but not you know not enough and so the

00:02:39,940 --> 00:02:43,840
this mace this diagram will seem a

00:02:42,700 --> 00:02:45,310
little bit familiar if you were here for

00:02:43,840 --> 00:02:47,260
the last talk and if I can figure this

00:02:45,310 --> 00:02:51,270
laser pointer at all try to annotate it

00:02:47,260 --> 00:02:55,840
I have my own laser pointer least I did

00:02:51,270 --> 00:02:57,700
so basically the the workflow for

00:02:55,840 --> 00:03:01,780
criminal classes I as I mentioned before

00:02:57,700 --> 00:03:05,050
is send file will pull data in from the

00:03:01,780 --> 00:03:06,580
disks into memory and then to encrypt it

00:03:05,050 --> 00:03:08,650
you've got to read it into the CPU and

00:03:06,580 --> 00:03:09,850
then to read it and then - once you've

00:03:08,650 --> 00:03:11,860
encrypted it you've got to write it back

00:03:09,850 --> 00:03:13,420
to memory and then once it's been

00:03:11,860 --> 00:03:15,730
written in memory the network interface

00:03:13,420 --> 00:03:18,100
card needs to read it to send it and so

00:03:15,730 --> 00:03:19,420
if you add all these if you add all

00:03:18,100 --> 00:03:22,209
these twenty-fives up it's pretty easy

00:03:19,420 --> 00:03:23,530
math you get you get to 100 gigabytes a

00:03:22,209 --> 00:03:25,030
second and memory bandwidth is what you

00:03:23,530 --> 00:03:26,650
need and from the last from the last

00:03:25,030 --> 00:03:30,790
slide you could see that the Xeon only

00:03:26,650 --> 00:03:34,000
had 90 gigabytes a second so how do we

00:03:30,790 --> 00:03:35,049
get - how do we get that how do we get

00:03:34,000 --> 00:03:36,130
that much memory bandwidth well the

00:03:35,049 --> 00:03:38,590
simplest thing to do is just throw

00:03:36,130 --> 00:03:40,480
another CPU socket at it so basically

00:03:38,590 --> 00:03:42,549
you know it's it you double everything

00:03:40,480 --> 00:03:45,120
you've got twice as much memory

00:03:42,549 --> 00:03:49,540
bandwidth twice as many PCIe lanes and

00:03:45,120 --> 00:03:51,400
you've got two UPI links I'll go into

00:03:49,540 --> 00:03:56,200
more detail about that later connecting

00:03:51,400 --> 00:03:58,920
the two sockets and you know on this on

00:03:56,200 --> 00:04:02,500
these prototype machines we have eight

00:03:58,920 --> 00:04:05,680
really fast and DME drives and we have

00:04:02,500 --> 00:04:08,379
two 100 Gig NICs

00:04:05,680 --> 00:04:10,030
and we thought well why not get AMD a

00:04:08,379 --> 00:04:12,519
chance so let's build a prototype around

00:04:10,030 --> 00:04:15,129
AMD and when we first started this we

00:04:12,519 --> 00:04:18,970
were looking at the AMD Naples series

00:04:15,129 --> 00:04:20,799
machines with the interesting thing is

00:04:18,970 --> 00:04:24,280
here is that B is that you can do this

00:04:20,799 --> 00:04:27,789
in a single socket with AMD and just

00:04:24,280 --> 00:04:31,360
like the just like the the Intel we have

00:04:27,789 --> 00:04:32,949
eight nvme drives but the on the AMD we

00:04:31,360 --> 00:04:34,479
actually have four NICs and I'll get

00:04:32,949 --> 00:04:37,240
into why and a little bit later in the

00:04:34,479 --> 00:04:38,800
presentation but we're not running 400

00:04:37,240 --> 00:04:43,270
gigabits we're running you know 4 times

00:04:38,800 --> 00:04:44,560
50 basically so you know once we doubled

00:04:43,270 --> 00:04:46,449
everything we're like yeah we're gonna

00:04:44,560 --> 00:04:48,940
get a lot of performance boost but

00:04:46,449 --> 00:04:51,910
actually the performance went down so

00:04:48,940 --> 00:04:53,440
you know our normal workflow or normal

00:04:51,910 --> 00:04:55,990
workload was we were getting about 85

00:04:53,440 --> 00:04:58,780
gigs on AMD and about 130 gigs on Intel

00:04:55,990 --> 00:05:01,090
at 80 percent CPU and crazy stuff was

00:04:58,780 --> 00:05:03,699
happening you know we could crazy a

00:05:01,090 --> 00:05:06,490
distance these spikes that would drive

00:05:03,699 --> 00:05:10,080
our engine X latency way up which would

00:05:06,490 --> 00:05:12,550
cause clients to run away in terror and

00:05:10,080 --> 00:05:14,380
and I should mention by the way that in

00:05:12,550 --> 00:05:17,229
case it wasn't clear before all the

00:05:14,380 --> 00:05:18,550
testing I do is with real Netflix well I

00:05:17,229 --> 00:05:19,990
mean that's not the very beginning

00:05:18,550 --> 00:05:21,639
testing but they're the most of the real

00:05:19,990 --> 00:05:24,610
test thing I do is with real Netflix

00:05:21,639 --> 00:05:27,580
clients so if you live in San Jose or if

00:05:24,610 --> 00:05:30,010
you live in Chicago you've probably

00:05:27,580 --> 00:05:33,900
you've probably been served a video from

00:05:30,010 --> 00:05:33,900
from one of my machines and I apologize

00:05:34,710 --> 00:05:39,970
so anyway with with no optimization Numa

00:05:38,229 --> 00:05:42,099
is just wasn't it just was a non-starter

00:05:39,970 --> 00:05:43,960
we threw more hardware at it and they

00:05:42,099 --> 00:05:46,210
got either negative results or not

00:05:43,960 --> 00:05:47,889
enough positive results to matter so we

00:05:46,210 --> 00:05:49,810
didn't consider doing doing Numa for a

00:05:47,889 --> 00:05:51,039
long time because of earlier results

00:05:49,810 --> 00:05:56,860
that were very similar to these that we

00:05:51,039 --> 00:05:58,570
did in 2014 to 2015 so now we've got to

00:05:56,860 --> 00:05:59,889
understand the problem what's what's new

00:05:58,570 --> 00:06:03,030
moe what is this Numa stuff mean

00:05:59,889 --> 00:06:05,020
basically means non-uniform memory

00:06:03,030 --> 00:06:07,090
architecture or memory access depending

00:06:05,020 --> 00:06:09,789
on who you talk to so basically it means

00:06:07,090 --> 00:06:11,409
that stuff can be closer to one CPU than

00:06:09,789 --> 00:06:14,020
another like back in the good old days

00:06:11,409 --> 00:06:16,029
like you know 15 years ago before AMD

00:06:14,020 --> 00:06:19,360
did hypertransport and before intel did

00:06:16,029 --> 00:06:20,680
did qpi basically the way i multi

00:06:19,360 --> 00:06:23,110
it's system look was kind of like this

00:06:20,680 --> 00:06:24,550
you've got you know the central IO hub

00:06:23,110 --> 00:06:26,050
our North Ridge or whatever you want to

00:06:24,550 --> 00:06:28,120
call it sitting in the middle all the

00:06:26,050 --> 00:06:29,710
CPUs plug in equally all the memory

00:06:28,120 --> 00:06:31,090
plugs in equally all the disks plug in

00:06:29,710 --> 00:06:33,729
equally all the network cards plug in

00:06:31,090 --> 00:06:35,169
equally everybody has equal access to

00:06:33,729 --> 00:06:38,860
everything it doesn't matter if you're

00:06:35,169 --> 00:06:41,349
on why can't I figure this out there we

00:06:38,860 --> 00:06:43,240
go if you're on this CPU and you want to

00:06:41,349 --> 00:06:44,740
talk to this disk hey great go for it if

00:06:43,240 --> 00:06:46,469
you want if you want to store it in that

00:06:44,740 --> 00:06:49,389
memory yeah it doesn't really matter the

00:06:46,469 --> 00:06:51,280
problem is that these were slow and

00:06:49,389 --> 00:06:53,919
expensive and complicated to build and

00:06:51,280 --> 00:06:55,599
so CPU manufacturers figured out that it

00:06:53,919 --> 00:06:58,330
is better to basically sort of build a

00:06:55,599 --> 00:07:00,729
network on the on the motherboard and

00:06:58,330 --> 00:07:02,979
you wind up with something looks kind of

00:07:00,729 --> 00:07:06,430
like this where basically you have

00:07:02,979 --> 00:07:08,440
what's essentially two separate two

00:07:06,430 --> 00:07:10,770
separate systems that are tied together

00:07:08,440 --> 00:07:14,199
by this thing we call a Numa bus and

00:07:10,770 --> 00:07:16,120
what that really means is that stuff on

00:07:14,199 --> 00:07:18,159
the left side is basically his own

00:07:16,120 --> 00:07:19,569
computer and stuff on the right side is

00:07:18,159 --> 00:07:22,389
his own computer and these red circles

00:07:19,569 --> 00:07:24,099
we call them a locality call these

00:07:22,389 --> 00:07:26,259
localities owns a Numa domaine or a Numa

00:07:24,099 --> 00:07:30,879
node and so what that really means is

00:07:26,259 --> 00:07:32,919
that if you are on this CPU and you want

00:07:30,879 --> 00:07:35,680
to read something from this disk it's

00:07:32,919 --> 00:07:37,810
got to go across and ideally into your

00:07:35,680 --> 00:07:41,349
own memory or and if you're on the CPU

00:07:37,810 --> 00:07:43,270
and you want to access this memory it's

00:07:41,349 --> 00:07:44,440
got to go across this Numa bus or if

00:07:43,270 --> 00:07:46,419
you're on if you want to send something

00:07:44,440 --> 00:07:47,860
on this network card and it's stored in

00:07:46,419 --> 00:07:49,690
this memory it's got to go across this

00:07:47,860 --> 00:07:53,050
Numa bus and the problem is there's only

00:07:49,690 --> 00:07:55,029
so much bandwidth on this Numa bus and

00:07:53,050 --> 00:07:56,319
then once you get at AMD you get into

00:07:55,029 --> 00:07:58,150
something that looks even weirder and

00:07:56,319 --> 00:08:00,460
this is why we've got four network cards

00:07:58,150 --> 00:08:02,949
on AMD so we can have one each one of

00:08:00,460 --> 00:08:06,539
these red circles but basic with AMD you

00:08:02,949 --> 00:08:08,469
have you know Numa links between the

00:08:06,539 --> 00:08:12,279
different the four different neumann

00:08:08,469 --> 00:08:13,810
notes on a chip and you've got four Numa

00:08:12,279 --> 00:08:15,339
notes which is kind of a disaster which

00:08:13,810 --> 00:08:18,920
is why the AMD performance actually went

00:08:15,339 --> 00:08:21,750
down so much compared to the Intel

00:08:18,920 --> 00:08:24,240
so there's a there's a latency penalty

00:08:21,750 --> 00:08:25,440
to go across these links it's you know

00:08:24,240 --> 00:08:26,550
from everything I've read and what I've

00:08:25,440 --> 00:08:28,350
seen it's you're depending on

00:08:26,550 --> 00:08:30,180
manufacturer and revisions and stuff

00:08:28,350 --> 00:08:32,730
it's about 50 nanoseconds give or take

00:08:30,180 --> 00:08:34,980
give or take 50 nanoseconds the real

00:08:32,730 --> 00:08:37,320
problem is when you're sending a lot of

00:08:34,980 --> 00:08:38,820
both data across these links 50

00:08:37,320 --> 00:08:40,710
nanoseconds can turn into 500

00:08:38,820 --> 00:08:42,210
nanoseconds can turn into you know even

00:08:40,710 --> 00:08:44,190
milliseconds in some cases which is

00:08:42,210 --> 00:08:46,200
really really bad if what you're trying

00:08:44,190 --> 00:08:47,400
to do is read you know kernel text it's

00:08:46,200 --> 00:08:49,890
on the other domain or if you're trying

00:08:47,400 --> 00:08:51,150
to write to you know a global variable

00:08:49,890 --> 00:08:52,620
if you're trying to redo the end page

00:08:51,150 --> 00:08:54,930
and you better grow up you've got to

00:08:52,620 --> 00:08:57,240
wait for some bulk data transfer to pass

00:08:54,930 --> 00:09:02,640
that's really really bad and the CPU

00:08:57,240 --> 00:09:04,620
utilization goes crazy so and the

00:09:02,640 --> 00:09:07,530
bandwidth is speaking of both data is

00:09:04,620 --> 00:09:08,760
you know roughly from what I've read and

00:09:07,530 --> 00:09:10,620
they try to obscure these things by

00:09:08,760 --> 00:09:13,050
talking about Giga transfers per

00:09:10,620 --> 00:09:14,340
fortnight or something which makes it

00:09:13,050 --> 00:09:16,260
really wish makes it really really hard

00:09:14,340 --> 00:09:17,550
to figure out what you what you guys

00:09:16,260 --> 00:09:19,020
should get in bandwidth but from what

00:09:17,550 --> 00:09:22,170
I'd be able to figure out it's about 20

00:09:19,020 --> 00:09:24,000
gigabytes a second per UPI link or about

00:09:22,170 --> 00:09:26,670
40 gigabytes a second per Infinity

00:09:24,000 --> 00:09:28,140
fabric link and the AMD is even more

00:09:26,670 --> 00:09:30,360
complicated because it depends on the

00:09:28,140 --> 00:09:32,340
memory speed and on the new ones there's

00:09:30,360 --> 00:09:36,390
multiple there's multiplying factors and

00:09:32,340 --> 00:09:38,910
it's kind of crazy so anyway what I came

00:09:36,390 --> 00:09:41,370
up with was after playing around with

00:09:38,910 --> 00:09:42,930
lots of little optimizations to do

00:09:41,370 --> 00:09:44,400
things like move to the in page array to

00:09:42,930 --> 00:09:46,740
make the VM page array be backed by

00:09:44,400 --> 00:09:48,870
local memory on each domain I decided

00:09:46,740 --> 00:09:50,130
well I'm just kind of playing with the

00:09:48,870 --> 00:09:51,510
small stuff and but I really need to do

00:09:50,130 --> 00:09:53,010
is figure out a way to organize things

00:09:51,510 --> 00:09:56,400
and keep the bulk data off the Numa

00:09:53,010 --> 00:09:57,810
links because the bulk data like I was

00:09:56,400 --> 00:09:59,100
saying will congest the Numa Lincoln

00:09:57,810 --> 00:10:03,300
will slow down anything that you haven't

00:09:59,100 --> 00:10:05,340
managed it to localize so I'm going to

00:10:03,300 --> 00:10:06,900
go through basically the worst case if

00:10:05,340 --> 00:10:22,980
you do everything you possibly can

00:10:06,900 --> 00:10:24,360
traumatize incented out the network so

00:10:22,980 --> 00:10:26,010
he starts reading he starts reading from

00:10:24,360 --> 00:10:28,320
the disk whoops

00:10:26,010 --> 00:10:30,330
but whoops it goes across the numeral

00:10:28,320 --> 00:10:31,890
ank and into the other nodes memory

00:10:30,330 --> 00:10:32,410
because he wasn't paying attention money

00:10:31,890 --> 00:10:35,889
allocated

00:10:32,410 --> 00:10:37,420
memory and then he wants to encrypt it

00:10:35,889 --> 00:10:40,449
so he's gonna have to read it back

00:10:37,420 --> 00:10:42,579
across the name of us and whoops

00:10:40,449 --> 00:10:44,079
he forgot delicated on the right note

00:10:42,579 --> 00:10:45,550
again so he's gonna write it back into

00:10:44,079 --> 00:10:47,800
the he's gonna write it back into the

00:10:45,550 --> 00:10:50,230
wrong notes memory and then he wants to

00:10:47,800 --> 00:10:51,339
send it out on the network and you know

00:10:50,230 --> 00:10:53,709
maybe he should be using this network

00:10:51,339 --> 00:10:55,350
card up there but whoops he's gonna send

00:10:53,709 --> 00:10:58,300
it out this this other network card so

00:10:55,350 --> 00:11:01,029
we end up crossing the the Numa bus four

00:10:58,300 --> 00:11:05,319
times and you end up burning basically

00:11:01,029 --> 00:11:06,550
100 gigabytes a second a bandwidth so at

00:11:05,319 --> 00:11:09,639
this point the fabric is going to

00:11:06,550 --> 00:11:11,199
saturate and you have CPU stalls you'll

00:11:09,639 --> 00:11:15,069
have latency spikes you'll have all

00:11:11,199 --> 00:11:17,620
kinds of crazy stuff so the best case is

00:11:15,069 --> 00:11:20,290
basically the case that I showed you

00:11:17,620 --> 00:11:22,089
kind of at the beginning where you read

00:11:20,290 --> 00:11:25,360
from you read from the disk and in the

00:11:22,089 --> 00:11:27,459
close memory the CPU reads reads it from

00:11:25,360 --> 00:11:29,980
the close memory encrypts it writes it

00:11:27,459 --> 00:11:32,259
into closed memory and then sends it out

00:11:29,980 --> 00:11:34,569
the network card that's closest to him

00:11:32,259 --> 00:11:37,480
and that's beautiful there's no Numa

00:11:34,569 --> 00:11:38,649
crossings this is how you know AMD and

00:11:37,480 --> 00:11:44,350
Intel would really like you to use these

00:11:38,649 --> 00:11:46,769
machines in an ideal world so how can we

00:11:44,350 --> 00:11:49,329
get as close as we can to this best case

00:11:46,769 --> 00:11:50,769
basically all the simplest idea well

00:11:49,329 --> 00:11:52,209
let's just forget about let's just

00:11:50,769 --> 00:11:55,209
pretend us two machines let's have one V

00:11:52,209 --> 00:11:57,699
let's have one VM turn per Numa node and

00:11:55,209 --> 00:11:59,380
pass everything through except if you do

00:11:57,699 --> 00:12:02,050
that you're gonna double your ipv4

00:11:59,380 --> 00:12:07,509
address space and every IP v4 address is

00:12:02,050 --> 00:12:09,850
precious and so it Netflix when you get

00:12:07,509 --> 00:12:12,880
a when you get a movie or a video or a

00:12:09,850 --> 00:12:15,790
TV show or whatever you press play and

00:12:12,880 --> 00:12:17,980
your client talked to Netflix stuff

00:12:15,790 --> 00:12:19,839
running in the Amazon Cloud and that

00:12:17,980 --> 00:12:23,259
stuff and the Amazon Cloud figures out

00:12:19,839 --> 00:12:26,470
where the where the where which which

00:12:23,259 --> 00:12:28,269
machine has the file you want and if

00:12:26,470 --> 00:12:30,160
it's closed which is closest to you and

00:12:28,269 --> 00:12:31,569
which is next closest to you and next

00:12:30,160 --> 00:12:33,160
closest to you and so on and gives you a

00:12:31,569 --> 00:12:37,720
list of URLs where you can find that

00:12:33,160 --> 00:12:39,160
that file so if we double the number of

00:12:37,720 --> 00:12:41,529
machines then we're kind of doubling the

00:12:39,160 --> 00:12:43,269
work that we have to do in AWS in fact

00:12:41,529 --> 00:12:45,009
if we're running VMs we're kind of more

00:12:43,269 --> 00:12:46,240
than doubling it because now you got the

00:12:45,009 --> 00:12:49,220
hypervisor to manage

00:12:46,240 --> 00:12:52,759
so it's kind of a non-starter for that

00:12:49,220 --> 00:12:54,589
reason and the next idea

00:12:52,759 --> 00:12:56,600
well what if we used multiple IP

00:12:54,589 --> 00:12:58,610
addresses wait a sec I multiplied P

00:12:56,600 --> 00:13:01,550
addresses we don't want to do that

00:12:58,610 --> 00:13:03,829
so the same reason as before basically

00:13:01,550 --> 00:13:05,959
so basically how can we get as close to

00:13:03,829 --> 00:13:09,050
the best case as possible while using

00:13:05,959 --> 00:13:11,300
lag and LACP to combine the Knicks and

00:13:09,050 --> 00:13:14,060
just to just use one IP address and

00:13:11,300 --> 00:13:15,889
while keeping the catalog the same so

00:13:14,060 --> 00:13:19,790
that you know AWS doesn't have to do any

00:13:15,889 --> 00:13:22,759
extra work so we need to somehow impose

00:13:19,790 --> 00:13:24,529
order on this chaos and the first idea I

00:13:22,759 --> 00:13:27,319
came up with which was not the winner

00:13:24,529 --> 00:13:29,269
was what I called disc centric siloing

00:13:27,319 --> 00:13:31,579
which is basically try to do everything

00:13:29,269 --> 00:13:34,339
you can on the Numa node where the

00:13:31,579 --> 00:13:36,829
content actually lives and the other

00:13:34,339 --> 00:13:38,509
idea I came up with was network centric

00:13:36,829 --> 00:13:41,389
siloing which was try to do everything

00:13:38,509 --> 00:13:43,970
local to the network card that the

00:13:41,389 --> 00:13:45,649
connection came in on and if you don't

00:13:43,970 --> 00:13:48,139
know anything about LACP basically what

00:13:45,649 --> 00:13:51,920
you need to know is that when you're

00:13:48,139 --> 00:13:53,630
speaking at least LACP your your switch

00:13:51,920 --> 00:13:55,579
or router that you're talking to will

00:13:53,630 --> 00:13:59,810
take a connection and will hash it based

00:13:55,579 --> 00:14:02,750
on either the based on on some end tuple

00:13:59,810 --> 00:14:04,370
and it will decide which of the lagged

00:14:02,750 --> 00:14:06,139
ports that you're connected to that it

00:14:04,370 --> 00:14:07,750
will the traffic will go over so you

00:14:06,139 --> 00:14:11,149
have no control over that

00:14:07,750 --> 00:14:13,610
so basically we try to in in the network

00:14:11,149 --> 00:14:15,589
where the network centric styling we try

00:14:13,610 --> 00:14:17,810
to do as much work as we can on the Numa

00:14:15,589 --> 00:14:19,310
know that Canada where the LACP partner

00:14:17,810 --> 00:14:21,889
decided that the connection was going to

00:14:19,310 --> 00:14:25,010
live so let's talk about the thing that

00:14:21,889 --> 00:14:26,750
didn't work first so basically the idea

00:14:25,010 --> 00:14:29,930
was to associate a disk controller or an

00:14:26,750 --> 00:14:33,319
nvme really an nvm you drive with a Numa

00:14:29,930 --> 00:14:35,839
node and then to basically propagate the

00:14:33,319 --> 00:14:37,550
Numa affinity through the VFS layer

00:14:35,839 --> 00:14:39,199
until we got to a point where if we

00:14:37,550 --> 00:14:40,639
looked at a file if we looked at a tea

00:14:39,199 --> 00:14:43,310
node we know what Numa note it was

00:14:40,639 --> 00:14:45,130
associated with and again but again we

00:14:43,310 --> 00:14:47,649
have to do all the work to associate

00:14:45,130 --> 00:14:52,490
network connections with Numa nodes and

00:14:47,649 --> 00:14:54,529
the idea is we want to move the network

00:14:52,490 --> 00:14:56,329
connection to be as close to the to the

00:14:54,529 --> 00:14:59,480
to the content as we can

00:14:56,329 --> 00:14:59,900
so that if it comes in on one lag port

00:14:59,480 --> 00:15:03,320
it'll

00:14:59,900 --> 00:15:04,340
end up going out on the other so after

00:15:03,320 --> 00:15:07,250
we move everything there's going to be

00:15:04,340 --> 00:15:09,290
zero Numa crossings for a bulk data the

00:15:07,250 --> 00:15:10,610
problems with this was that like like I

00:15:09,290 --> 00:15:12,650
said there's no way to tell the LIC

00:15:10,610 --> 00:15:14,060
partner you know I don't want to come in

00:15:12,650 --> 00:15:18,110
this note I wanted to come in this note

00:15:14,060 --> 00:15:19,490
you can't do that so basically while

00:15:18,110 --> 00:15:21,590
you're setting up the connection while

00:15:19,490 --> 00:15:23,360
you're doing the get you're gonna have

00:15:21,590 --> 00:15:25,010
your admin before you before you know

00:15:23,360 --> 00:15:27,470
what content you're talking about your

00:15:25,010 --> 00:15:29,210
acts are gonna be going and your replies

00:15:27,470 --> 00:15:30,260
remain going out one port then as soon

00:15:29,210 --> 00:15:31,940
as you figure it out it's gonna be going

00:15:30,260 --> 00:15:33,620
out the other port so you can have

00:15:31,940 --> 00:15:35,570
stuffing on going out both ports and

00:15:33,620 --> 00:15:37,640
with TCP that can lead to reordering and

00:15:35,570 --> 00:15:41,720
that's kind of bad news and I think

00:15:37,640 --> 00:15:46,970
Randall would be upset if I did that so

00:15:41,720 --> 00:15:49,430
the other problem is that unbeknownst to

00:15:46,970 --> 00:15:50,930
me clients will actually reuse the

00:15:49,430 --> 00:15:52,610
connection and make multiple make

00:15:50,930 --> 00:15:55,790
multiple requests in the same connection

00:15:52,610 --> 00:15:57,320
for those of you that love or hate the

00:15:55,790 --> 00:15:58,760
newish feature where if you're on the

00:15:57,320 --> 00:16:02,390
Netflix homepage just crap starts

00:15:58,760 --> 00:16:03,770
playing all the time back but it'll

00:16:02,390 --> 00:16:05,510
reuse connections for all that junk

00:16:03,770 --> 00:16:07,280
that's that's that's that's playing all

00:16:05,510 --> 00:16:09,020
the time and so you'll end up having

00:16:07,280 --> 00:16:11,180
stuff coming from all that from all the

00:16:09,020 --> 00:16:12,500
Numa nodes on the same connection so I

00:16:11,180 --> 00:16:15,110
was seeing connections being moved

00:16:12,500 --> 00:16:17,420
around willy-nilly and TCP retransmits

00:16:15,110 --> 00:16:19,790
going crazy and I said it was a bad idea

00:16:17,420 --> 00:16:21,200
so I went back to the other idea which

00:16:19,790 --> 00:16:24,590
was the network centric siloing

00:16:21,200 --> 00:16:25,730
which is basically it's basically just

00:16:24,590 --> 00:16:28,910
done plumbing and that's good because

00:16:25,730 --> 00:16:29,930
I'm just de plumber so essentially you

00:16:28,910 --> 00:16:32,510
have to associate the network

00:16:29,930 --> 00:16:34,610
connections with the Numa nodes and you

00:16:32,510 --> 00:16:37,730
have to allocate local memory to two

00:16:34,610 --> 00:16:40,970
back-to-back the media files and you

00:16:37,730 --> 00:16:45,140
allocate local memory for crypto and you

00:16:40,970 --> 00:16:48,350
run the TCP pacers and on the local on

00:16:45,140 --> 00:16:53,330
the local node and you manage to choose

00:16:48,350 --> 00:16:57,980
a local node to send the to send the

00:16:53,330 --> 00:16:59,570
data on so how do we do all this to

00:16:57,980 --> 00:17:01,520
associate the network connections with

00:16:59,570 --> 00:17:02,660
the Numa nodes basically I'm going to go

00:17:01,520 --> 00:17:04,790
through some kind of nitty-gritty

00:17:02,660 --> 00:17:06,830
details of what's been committed and

00:17:04,790 --> 00:17:08,990
what's in review and all that kind of

00:17:06,830 --> 00:17:11,300
stuff so if you're not a developer you

00:17:08,990 --> 00:17:16,710
may want to check your phone

00:17:11,300 --> 00:17:19,080
so basically I added a Newman ode to a

00:17:16,710 --> 00:17:20,790
new beau domain no to the struck down

00:17:19,080 --> 00:17:24,180
buff there was just a tiny a little bit

00:17:20,790 --> 00:17:27,510
of room and I stole it and that was

00:17:24,180 --> 00:17:30,810
added a few months ago and I also added

00:17:27,510 --> 00:17:33,870
a new modem into the if' net struct also

00:17:30,810 --> 00:17:36,500
a few months ago and this is kind of all

00:17:33,870 --> 00:17:39,120
groundwork so try to stay awake

00:17:36,500 --> 00:17:45,180
basically and and what's that once I did

00:17:39,120 --> 00:17:47,430
this when a driver received a packet he

00:17:45,180 --> 00:17:50,340
can tag that packet as he receives it

00:17:47,430 --> 00:17:52,530
with his Numa node and that's in the

00:17:50,340 --> 00:17:54,680
deaths in the tree too and I also added

00:17:52,530 --> 00:17:59,940
a Numa domaine to the InP CB struct

00:17:54,680 --> 00:18:01,950
which is also in the tree and basically

00:17:59,940 --> 00:18:04,260
the idea is that when the TCP connection

00:18:01,950 --> 00:18:06,500
connections before is born when instance

00:18:04,260 --> 00:18:10,170
in cache in the same cache expansion

00:18:06,500 --> 00:18:11,820
you've got a neumann node there in the

00:18:10,170 --> 00:18:13,320
EM buff that caused the the kinetics

00:18:11,820 --> 00:18:16,890
because of the connection to get

00:18:13,320 --> 00:18:21,600
established and you can then propagate

00:18:16,890 --> 00:18:23,970
it in the on PCB table and so the next

00:18:21,600 --> 00:18:25,710
the next trick is to make sure that you

00:18:23,970 --> 00:18:27,420
give that connection to the right nginx

00:18:25,710 --> 00:18:33,450
worker and I'll detail that in a little

00:18:27,420 --> 00:18:35,130
bit so the other and the other trick is

00:18:33,450 --> 00:18:36,900
what I thought was going to be a hard

00:18:35,130 --> 00:18:40,800
job which is to allocate local memory

00:18:36,900 --> 00:18:42,690
for the for send file to back to video

00:18:40,800 --> 00:18:46,590
files and I actually came up with this

00:18:42,690 --> 00:18:48,690
gigantic patch to plumb you know all the

00:18:46,590 --> 00:18:51,330
way from from send file down into done

00:18:48,690 --> 00:18:57,600
into the VM page allocation routines a

00:18:51,330 --> 00:18:59,520
Numa node and it turns out that I don't

00:18:57,600 --> 00:19:02,160
need any of that stuff basically if you

00:18:59,520 --> 00:19:05,190
have a first touch policy and nginx is

00:19:02,160 --> 00:19:07,080
bound to the the right domain then

00:19:05,190 --> 00:19:11,580
everything just works automatically and

00:19:07,080 --> 00:19:14,930
I want to thank Alan Cox and Constantine

00:19:11,580 --> 00:19:16,880
for pointing out my stupidity and

00:19:14,930 --> 00:19:19,180
making me realize that the vm system

00:19:16,880 --> 00:19:21,290
already did everything I needed it to do

00:19:19,180 --> 00:19:23,680
so that was two weeks of my life I'll

00:19:21,290 --> 00:19:23,680
never get back

00:19:24,010 --> 00:19:32,180
so the other trick is to allocate the

00:19:29,150 --> 00:19:36,020
local memory for the for the TLS buffers

00:19:32,180 --> 00:19:37,010
so basically we run the TLS worker

00:19:36,020 --> 00:19:39,170
threads I mentioned in the last

00:19:37,010 --> 00:19:42,470
presentation we basically have a thread

00:19:39,170 --> 00:19:44,600
pool of four CPU TLS workers and the

00:19:42,470 --> 00:19:47,059
idea is that normally connections are

00:19:44,600 --> 00:19:49,190
just hashed to them using using just

00:19:47,059 --> 00:19:51,380
software had using something software

00:19:49,190 --> 00:19:52,820
hashing on the on the end tuple so that

00:19:51,380 --> 00:19:55,820
the same connection goes through the

00:19:52,820 --> 00:19:57,890
same you know TLS worker but what I did

00:19:55,820 --> 00:20:00,380
was add a filter based on Numa domain in

00:19:57,890 --> 00:20:01,970
front of that so that connections that

00:20:00,380 --> 00:20:03,559
were associated with node 0 will go to a

00:20:01,970 --> 00:20:04,820
worker that's run will be hashed to a

00:20:03,559 --> 00:20:09,910
worker that's running on the CPU and

00:20:04,820 --> 00:20:14,330
unknown 0 and no similar with node 1 so

00:20:09,910 --> 00:20:18,970
and I also set the the K TLS workers to

00:20:14,330 --> 00:20:21,440
have a domain allocation policy so that

00:20:18,970 --> 00:20:24,860
they'll allocate stuff local to their

00:20:21,440 --> 00:20:26,870
domain so that way we'll we're doing the

00:20:24,860 --> 00:20:28,400
crypto on the same domain the connection

00:20:26,870 --> 00:20:31,760
lives on and we're doing a crypto into

00:20:28,400 --> 00:20:34,960
and out of local memory and this the

00:20:31,760 --> 00:20:38,210
Katie left stop is in review currently

00:20:34,960 --> 00:20:42,080
so how do we choose the right lag port

00:20:38,210 --> 00:20:43,910
to go out of so like I said earlier M

00:20:42,080 --> 00:20:47,390
buffs are tagged can be tagged with the

00:20:43,910 --> 00:20:51,140
Numa domain so when we go through out

00:20:47,390 --> 00:20:53,870
the output or ip6 output we we tagged

00:20:51,140 --> 00:20:56,120
the outgoing M buffs and I've organized

00:20:53,870 --> 00:20:59,120
I've done a patch to lag which is in the

00:20:56,120 --> 00:21:02,179
tree which is enabled if you have the

00:20:59,120 --> 00:21:04,280
used Numa option set for lag where

00:21:02,179 --> 00:21:06,170
basically you've got this high similarly

00:21:04,280 --> 00:21:08,540
k TLS you've got this hierarchy rather

00:21:06,170 --> 00:21:11,059
than just hashing directly to to any lag

00:21:08,540 --> 00:21:12,830
board in the system first you you filter

00:21:11,059 --> 00:21:15,020
by neumann domain and then you only

00:21:12,830 --> 00:21:16,880
choose a lag port in that dome that's

00:21:15,020 --> 00:21:19,460
connected to a NIC and that domain and

00:21:16,880 --> 00:21:21,230
obviously if there's no NIC in that

00:21:19,460 --> 00:21:23,150
domain it'll fall back to just hashing

00:21:21,230 --> 00:21:25,100
to anything so that you can still send

00:21:23,150 --> 00:21:26,149
even if there's even if that lag ports

00:21:25,100 --> 00:21:30,649
down

00:21:26,149 --> 00:21:32,299
and that's in the tree so how do you

00:21:30,649 --> 00:21:36,499
choose the right nginx worker this is

00:21:32,299 --> 00:21:38,179
this was the hard part for me so right

00:21:36,499 --> 00:21:40,039
now we've got this ester to reuse port

00:21:38,179 --> 00:21:42,769
stuff that came in I don't know about a

00:21:40,039 --> 00:21:46,059
year ago or so where essentially what

00:21:42,769 --> 00:21:48,439
that means is that you can have multiple

00:21:46,059 --> 00:21:51,049
threads multiple processes share the

00:21:48,439 --> 00:21:54,169
same list and socket and again it's kind

00:21:51,049 --> 00:21:55,939
of like lag things are hashed fairly the

00:21:54,169 --> 00:21:57,709
connection new connections are hashed

00:21:55,939 --> 00:21:59,719
fairly to these lists and sockets and

00:21:57,709 --> 00:22:01,669
that allows you to have you know a bunch

00:21:59,719 --> 00:22:06,259
of ngx workers all listening on port 80

00:22:01,669 --> 00:22:08,119
and port 443 and so what so the the

00:22:06,259 --> 00:22:09,199
obvious thing to do is

00:22:08,119 --> 00:22:12,319
and everything's obvious in hindsight

00:22:09,199 --> 00:22:15,979
the obvious thing to do is to filter

00:22:12,319 --> 00:22:21,109
that by Numa domain so that you end up

00:22:15,979 --> 00:22:22,729
with you end up with you know you've you

00:22:21,109 --> 00:22:24,799
end up with a new socket option

00:22:22,729 --> 00:22:28,579
unfortunately because of the way ng next

00:22:24,799 --> 00:22:31,389
works and I can go into deep California

00:22:28,579 --> 00:22:34,519
yeah I'd look but sure why not why not

00:22:31,389 --> 00:22:36,529
so the way nginx works is the master

00:22:34,519 --> 00:22:38,809
process start starts up creates all the

00:22:36,529 --> 00:22:41,659
lists and sockets and then Forks off as

00:22:38,809 --> 00:22:42,949
children and at least you know for a

00:22:41,659 --> 00:22:44,269
mere mortal reading the entry next

00:22:42,949 --> 00:22:45,439
source code there's no way to tell which

00:22:44,269 --> 00:22:47,659
listing sock is going to go to which

00:22:45,439 --> 00:22:50,119
child to which domain so the easiest

00:22:47,659 --> 00:22:52,579
thing for me to do was to make a new

00:22:50,119 --> 00:22:54,409
socket option which was called after the

00:22:52,579 --> 00:22:55,609
childhood in here had had inherited his

00:22:54,409 --> 00:22:57,469
list in socket and sort of taken

00:22:55,609 --> 00:23:02,599
possession of it and after he bound

00:22:57,469 --> 00:23:03,979
himself to 2-dose cpu then I can call a

00:23:02,599 --> 00:23:05,989
sock an option when the colonel says ah

00:23:03,979 --> 00:23:08,029
you're running on this CPU which is on

00:23:05,989 --> 00:23:09,439
this domain and you want to you and you

00:23:08,029 --> 00:23:12,469
want your listen socket filtered there

00:23:09,439 --> 00:23:14,419
so that basically builds up another one

00:23:12,469 --> 00:23:16,189
of these hierarchical models where first

00:23:14,419 --> 00:23:18,499
you filter Purnima domain into a list

00:23:16,189 --> 00:23:20,269
and socket and then I eat you hash among

00:23:18,499 --> 00:23:22,099
all the different workers that are that

00:23:20,269 --> 00:23:24,499
are listening on that domain and like

00:23:22,099 --> 00:23:25,999
lag there's a fall backwards if there's

00:23:24,499 --> 00:23:28,489
nobody on that domain it'll go back to

00:23:25,999 --> 00:23:30,200
hashing among all the listing sockets on

00:23:28,489 --> 00:23:36,620
that for it globally

00:23:30,200 --> 00:23:38,240
and that's also in review and so let's

00:23:36,620 --> 00:23:41,990
go back to that same diagram where I

00:23:38,240 --> 00:23:43,760
talked about the the worst case so in

00:23:41,990 --> 00:23:45,860
this model the worst case is basically

00:23:43,760 --> 00:23:47,990
if you always cheat if you always get

00:23:45,860 --> 00:23:50,240
unlucky and your content is always on

00:23:47,990 --> 00:23:52,490
the wrong domain so you know going back

00:23:50,240 --> 00:23:54,110
to what we talked about before we're

00:23:52,490 --> 00:23:57,080
running on the bottom noon on the bottom

00:23:54,110 --> 00:23:59,480
numa numa domain on the bottom CPU and

00:23:57,080 --> 00:24:00,410
we wanted we inserted a request comes in

00:23:59,480 --> 00:24:02,720
and we're reading that we're reading

00:24:00,410 --> 00:24:05,270
data from this disk on the top so we go

00:24:02,720 --> 00:24:08,300
for one Numa bus crossing read it into

00:24:05,270 --> 00:24:09,410
local memory and then we and then we

00:24:08,300 --> 00:24:11,390
read it out of local memory and yay

00:24:09,410 --> 00:24:14,000
we're encrypting it on the right on the

00:24:11,390 --> 00:24:15,650
right CPU and now we're writing it back

00:24:14,000 --> 00:24:17,650
to a crypto buffer that we were smart

00:24:15,650 --> 00:24:20,390
enough to allocate on the right CPU and

00:24:17,650 --> 00:24:22,250
now we are going to send it on on the

00:24:20,390 --> 00:24:23,840
local Nick because the connection came

00:24:22,250 --> 00:24:25,960
in on this on this bottom domain

00:24:23,840 --> 00:24:30,220
originally so now in the worst case

00:24:25,960 --> 00:24:34,760
we've got one Numa domain crossing and

00:24:30,220 --> 00:24:37,010
so basically you're doing 100% of the

00:24:34,760 --> 00:24:39,620
the disk reads the nvme reads across

00:24:37,010 --> 00:24:43,940
Numa which is about 25 gigabytes a

00:24:39,620 --> 00:24:46,130
second on the fabric which is much less

00:24:43,940 --> 00:24:49,460
than 40 gigabytes a second the second of

00:24:46,130 --> 00:24:52,160
the fabric bandwidth but the nice thing

00:24:49,460 --> 00:24:53,990
is the average case which is better the

00:24:52,160 --> 00:24:55,400
average case is about a half a Numa

00:24:53,990 --> 00:24:56,780
crossing because you're you're gonna get

00:24:55,400 --> 00:24:58,040
green getting it right about half the

00:24:56,780 --> 00:25:00,590
time you're gonna get an unlucky about

00:24:58,040 --> 00:25:02,960
half the time so it's about 50 percent

00:25:00,590 --> 00:25:04,670
across the fabric and that's about 12

00:25:02,960 --> 00:25:09,470
and a half gigabytes of data on the on

00:25:04,670 --> 00:25:11,270
the fabric and the nice thing is in this

00:25:09,470 --> 00:25:17,510
case the CPU doesn't saturate and we had

00:25:11,270 --> 00:25:18,830
190 gigs so for the 4 node it's the

00:25:17,510 --> 00:25:21,140
average case is a little bit worse

00:25:18,830 --> 00:25:25,060
because you've only got a 25% chance of

00:25:21,140 --> 00:25:27,170
getting lucky so 75% is across Numa and

00:25:25,060 --> 00:25:28,550
you get a little bit higher bandwidth

00:25:27,170 --> 00:25:31,190
going across the new my bus but that's

00:25:28,550 --> 00:25:33,170
still less than 40 gigabytes a second

00:25:31,190 --> 00:25:35,210
and we can still get better than 190

00:25:33,170 --> 00:25:39,890
gigs

00:25:35,210 --> 00:25:39,890
so here's what everybody's here to see

00:25:39,920 --> 00:25:43,950
one thing I should mention before I go

00:25:41,970 --> 00:25:47,460
into the performance results this is

00:25:43,950 --> 00:25:48,780
sort of a game of moving goal posts when

00:25:47,460 --> 00:25:51,720
I first started looking into this we

00:25:48,780 --> 00:25:53,610
were looking at the at the then the

00:25:51,720 --> 00:25:56,840
Naples the first version of AMD and we

00:25:53,610 --> 00:26:00,000
were looking at the skylake Intel and

00:25:56,840 --> 00:26:01,710
since then you know both of these

00:26:00,000 --> 00:26:03,330
motherboards have had their CPUs swapped

00:26:01,710 --> 00:26:05,880
to the to the latest and greatest from

00:26:03,330 --> 00:26:08,220
the different manufacturers so those

00:26:05,880 --> 00:26:12,360
first initial results were from FreeBSD

00:26:08,220 --> 00:26:15,450
from like you know fall of 2018 ish with

00:26:12,360 --> 00:26:23,630
the older CPUs these new results are

00:26:15,450 --> 00:26:31,560
from just last week with a AMD Rome Rome

00:26:23,630 --> 00:26:33,570
CPU and Intel cascade Lake CPU so in and

00:26:31,560 --> 00:26:36,180
this is why the xeon performance is

00:26:33,570 --> 00:26:39,030
lower that's something that I don't

00:26:36,180 --> 00:26:41,010
quite understand the way I got these

00:26:39,030 --> 00:26:42,870
numbers was to basically go through and

00:26:41,010 --> 00:26:46,680
intentionally torpedo all the Optimates

00:26:42,870 --> 00:26:49,140
all the optimizations I've done and when

00:26:46,680 --> 00:26:51,600
I did that I was surprised a little bit

00:26:49,140 --> 00:26:54,660
by the fact that it's 105 rather than

00:26:51,600 --> 00:26:59,100
130 and I think some of that is some of

00:26:54,660 --> 00:27:00,420
the work that Marc and Jeff have done to

00:26:59,100 --> 00:27:02,820
make things to make things better for

00:27:00,420 --> 00:27:04,260
Numa where I guess if you make things if

00:27:02,820 --> 00:27:05,580
you make things better you kind of make

00:27:04,260 --> 00:27:08,940
things worse if it makes any sense

00:27:05,580 --> 00:27:10,950
there's some stuff in um a that that we

00:27:08,940 --> 00:27:17,250
have turned on at Netflix which will try

00:27:10,950 --> 00:27:18,840
to sort basically it will try to if you

00:27:17,250 --> 00:27:20,760
do a um a allocation of like an M buff

00:27:18,840 --> 00:27:22,470
or something on one domain and you do a

00:27:20,760 --> 00:27:25,260
free on the other domain it'll try to

00:27:22,470 --> 00:27:27,270
return the memory to the proper domain

00:27:25,260 --> 00:27:29,190
rather than mixing up the UM a zone so

00:27:27,270 --> 00:27:30,470
that so that you can still have a nice

00:27:29,190 --> 00:27:33,100
Numa

00:27:30,470 --> 00:27:36,770
zone but the problem is that once you've

00:27:33,100 --> 00:27:38,690
once you have freed a lot of stuff on

00:27:36,770 --> 00:27:40,160
the wrong domain that option gets really

00:27:38,690 --> 00:27:41,750
expensive because you're taking a lock

00:27:40,160 --> 00:27:43,940
and you're moving things back to the

00:27:41,750 --> 00:27:45,620
proper domain and then and when you're

00:27:43,940 --> 00:27:46,580
doing things right it's awesome and

00:27:45,620 --> 00:27:47,930
you're not when you're doing things

00:27:46,580 --> 00:27:49,400
writing you're not doing a lot of cross

00:27:47,930 --> 00:27:50,240
domain freeze it's it's great but when

00:27:49,400 --> 00:27:56,650
you're doing a lot of cross domain

00:27:50,240 --> 00:27:59,090
freeze that's expensive and so basically

00:27:56,650 --> 00:28:02,390
I've actually measured with the Intel

00:27:59,090 --> 00:28:04,550
PCM tools the fact the the qpi

00:28:02,390 --> 00:28:06,050
utilization if they give you this this

00:28:04,550 --> 00:28:08,630
metric that tells you how much of the

00:28:06,050 --> 00:28:11,210
memory controller accesses were remote

00:28:08,630 --> 00:28:16,400
versus local and it goes from 40 percent

00:28:11,210 --> 00:28:18,680
to 13 percent and on epic because the

00:28:16,400 --> 00:28:20,950
four nodes things are even worse so the

00:28:18,680 --> 00:28:26,930
you go for even better I guess you'd say

00:28:20,950 --> 00:28:28,610
so you go from 68 gigs to 194 gigs and

00:28:26,930 --> 00:28:31,010
for people who like visual

00:28:28,610 --> 00:28:34,970
representations this is the the Xeon

00:28:31,010 --> 00:28:41,270
before and after so roughly a hundred to

00:28:34,970 --> 00:28:44,120
roughly 200 and the and the this is the

00:28:41,270 --> 00:28:45,920
the utilization on the on the qpi bus

00:28:44,120 --> 00:28:49,370
again going from about forty percent to

00:28:45,920 --> 00:28:52,700
about 13 percent and here's the

00:28:49,370 --> 00:29:00,470
bandwidth on the on the AMD going from

00:28:52,700 --> 00:29:02,090
you know 60-ish gigs to 195 gigs and for

00:29:00,470 --> 00:29:06,140
people who like green screens with raw

00:29:02,090 --> 00:29:08,990
data this is the output from PCM X

00:29:06,140 --> 00:29:12,470
showing the memory controller traffic as

00:29:08,990 --> 00:29:14,660
I was as I was mentioning this is the

00:29:12,470 --> 00:29:16,310
EPI data traffic memory control over

00:29:14,660 --> 00:29:21,970
over a memory memory controller traffic

00:29:16,310 --> 00:29:25,040
which is 0.4 and that's that's bad and

00:29:21,970 --> 00:29:28,510
this for people who aren't familiar with

00:29:25,040 --> 00:29:30,740
this I wrote it so it's my favorite tool

00:29:28,510 --> 00:29:33,110
it's something called I call n stat

00:29:30,740 --> 00:29:35,450
which is a I got sick of having a window

00:29:33,110 --> 00:29:37,880
for vmstat in a window for for a net

00:29:35,450 --> 00:29:41,090
stat and either running that stat with a

00:29:37,880 --> 00:29:42,290
with a with the delay of 8 seconds or

00:29:41,090 --> 00:29:43,390
doing the conversion in my head to

00:29:42,290 --> 00:29:45,910
convert from

00:29:43,390 --> 00:29:50,320
from the front bytes to bits so I wrote

00:29:45,910 --> 00:29:52,090
a tool that that does it spits out all

00:29:50,320 --> 00:29:54,040
the stuff I care about so it's my tool I

00:29:52,090 --> 00:29:57,700
can do what I want it's import stuff so

00:29:54,040 --> 00:29:59,679
anybody can use it but basically it's

00:29:57,700 --> 00:30:01,929
this is the output gigabits per second

00:29:59,679 --> 00:30:04,570
the important fields here the number of

00:30:01,929 --> 00:30:06,940
TCP connections the percent CPU and

00:30:04,570 --> 00:30:08,590
things like system calls and how many

00:30:06,940 --> 00:30:10,530
interrupts and context switches and how

00:30:08,590 --> 00:30:12,820
much memory is free in the machine and

00:30:10,530 --> 00:30:15,850
input and output and million millions of

00:30:12,820 --> 00:30:19,240
packets a second so that's this is the

00:30:15,850 --> 00:30:23,980
this is of course the before and this is

00:30:19,240 --> 00:30:26,650
the after you can see the 13% remote and

00:30:23,980 --> 00:30:30,910
that's a that's a good number and you

00:30:26,650 --> 00:30:34,450
can see the hundred and ninety ish 191

00:30:30,910 --> 00:30:39,760
gigs with 150 thousand TCP connections

00:30:34,450 --> 00:30:41,740
and in the 70ish percent cpu with you

00:30:39,760 --> 00:30:46,380
know a hundred thousand contexts which

00:30:41,740 --> 00:30:49,480
is a second thank you tcp pacing and and

00:30:46,380 --> 00:30:52,080
for people who like looking at internal

00:30:49,480 --> 00:30:56,260
netflix metrics this is our internal

00:30:52,080 --> 00:30:59,320
bandwidth graphs showing each showing

00:30:56,260 --> 00:31:01,179
each link separately stacking to about a

00:30:59,320 --> 00:31:05,410
hundred 190 when the machines finished

00:31:01,179 --> 00:31:08,980
ramping up and here's the same stuff

00:31:05,410 --> 00:31:11,080
from the AMD and i've crossed out the

00:31:08,980 --> 00:31:12,929
the model because it's not a released

00:31:11,080 --> 00:31:16,150
model

00:31:12,929 --> 00:31:17,290
it's a roughly equivalent to the the

00:31:16,150 --> 00:31:18,669
model number i said at the beginning of

00:31:17,290 --> 00:31:21,309
the presentation except it has a lower

00:31:18,669 --> 00:31:22,780
clock speed so the actual AMD results

00:31:21,309 --> 00:31:25,570
would be better than this because the

00:31:22,780 --> 00:31:28,510
actual AMD the real AMD cpu that's like

00:31:25,570 --> 00:31:30,040
this would be higher clocked so this may

00:31:28,510 --> 00:31:32,470
be doing AMD a slight to service by

00:31:30,040 --> 00:31:34,990
mentioning this but i would imagine the

00:31:32,470 --> 00:31:36,549
cpu number would probably be maybe eight

00:31:34,990 --> 00:31:41,650
or ten percent lower on the on the real

00:31:36,549 --> 00:31:43,179
AMD part and again here's since then the

00:31:41,650 --> 00:31:45,070
other the big frustration with AMD is

00:31:43,179 --> 00:31:46,690
that they don't export enough counters

00:31:45,070 --> 00:31:49,179
for us to be able to measure the fabric

00:31:46,690 --> 00:31:51,340
utilization and we've complained about

00:31:49,179 --> 00:31:52,720
it to them and i've heard the linux

00:31:51,340 --> 00:31:55,990
folks who are also complaining because

00:31:52,720 --> 00:31:56,830
linux doesn't have it either so if you

00:31:55,990 --> 00:31:58,750
happen to

00:31:56,830 --> 00:32:02,950
a good relationship with AMD complain

00:31:58,750 --> 00:32:06,250
about it too please anyway so here is

00:32:02,950 --> 00:32:09,880
the the data from the green screen data

00:32:06,250 --> 00:32:13,420
showing 194 gigabits a second when it's

00:32:09,880 --> 00:32:15,790
getting close to ramping up and this is

00:32:13,420 --> 00:32:17,860
not nearly as pretty because we're not

00:32:15,790 --> 00:32:19,540
used to the way these things are

00:32:17,860 --> 00:32:21,400
numbered they were numbered there were

00:32:19,540 --> 00:32:25,960
two port NICs to they're numbered you

00:32:21,400 --> 00:32:29,050
know 0 2 4 and 6 and no other machine

00:32:25,960 --> 00:32:30,430
has that many NICs so it doesn't fit it

00:32:29,050 --> 00:32:33,970
doesn't fit and nobody's ever picked a

00:32:30,430 --> 00:32:36,730
color for its but this this bar is the

00:32:33,970 --> 00:32:38,620
you know roughly 200 200 gig line and it

00:32:36,730 --> 00:32:41,170
goes up to 400 because there's because

00:32:38,620 --> 00:32:43,480
there's for 100 gig links active in the

00:32:41,170 --> 00:32:45,730
lag but it's not really going to go up

00:32:43,480 --> 00:32:50,440
to 400 because some of the merchant gen

00:32:45,730 --> 00:32:51,910
3x8 links so that's it I've rambled on

00:32:50,440 --> 00:32:53,680
for a long time about something really

00:32:51,910 --> 00:32:57,300
simple so if anybody has any questions

00:32:53,680 --> 00:32:57,300
this is this would be the time

00:32:57,640 --> 00:33:07,729
[Applause]

00:33:20,540 --> 00:33:23,619
[Music]

00:33:28,779 --> 00:33:46,089
some is for management in different part

00:33:31,089 --> 00:33:48,039
of the world it's above my pay grade in

00:33:46,089 --> 00:33:49,719
terms of like you're worried about like

00:33:48,039 --> 00:33:51,489
you know like a million connections on

00:33:49,719 --> 00:34:00,190
one domain and no connections on the

00:33:51,489 --> 00:34:02,080
other or the we deal in you know orders

00:34:00,190 --> 00:34:04,089
of thousands or tens of thousands of

00:34:02,080 --> 00:34:08,109
connections hundreds of thousands of

00:34:04,089 --> 00:34:09,549
connections and it on that level it's

00:34:08,109 --> 00:34:12,450
rough it's it's gonna it's roughly gonna

00:34:09,549 --> 00:34:14,740
be fair because lag is gonna be you know

00:34:12,450 --> 00:34:16,929
hashing to the different nicks in a in a

00:34:14,740 --> 00:34:18,490
fair way obviously if one link goes down

00:34:16,929 --> 00:34:20,589
then you're going to lose half your

00:34:18,490 --> 00:34:22,089
bandwidth but you've still got enough

00:34:20,589 --> 00:34:23,859
capacity in that in that neumann node

00:34:22,089 --> 00:34:25,109
where the link is up that you're gonna

00:34:23,859 --> 00:34:27,779
be fine

00:34:25,109 --> 00:34:29,889
does that kind of answer the question

00:34:27,779 --> 00:34:33,549
because i think it would be a different

00:34:29,889 --> 00:34:36,819
story if if a you receipt be you

00:34:33,549 --> 00:34:41,559
constrained because you were doing a lot

00:34:36,819 --> 00:34:42,789
of work that wasn't basically if the

00:34:41,559 --> 00:34:44,289
connection was doing more work than you

00:34:42,789 --> 00:34:45,250
anticipated i guess i guess would be the

00:34:44,289 --> 00:34:47,020
way the way to say it

00:34:45,250 --> 00:34:48,879
right if one connections could somehow

00:34:47,020 --> 00:34:50,020
or a small number of connections could

00:34:48,879 --> 00:34:52,029
somehow cause an inordinate number

00:34:50,020 --> 00:34:54,990
amount of cpu use but that's not

00:34:52,029 --> 00:34:54,990
something that can really happen

00:34:59,320 --> 00:35:05,240
on the AMD you've got four NICs so you

00:35:02,270 --> 00:35:08,630
have a theoretical bandwidth of 400 well

00:35:05,240 --> 00:35:11,030
300 actually well because it's it's it's

00:35:08,630 --> 00:35:14,240
a it's a an older motherboard so it's

00:35:11,030 --> 00:35:19,880
only a PCI gen3 so they're not they're

00:35:14,240 --> 00:35:27,350
not hooked up with full bandwidth reason

00:35:19,880 --> 00:35:29,900
why actually I mean in when I was

00:35:27,350 --> 00:35:33,290
testing earlier if I if I let that guy

00:35:29,900 --> 00:35:35,660
ramp up I think I could get it I I think

00:35:33,290 --> 00:35:39,080
I know I got over 200 the problem is

00:35:35,660 --> 00:35:41,480
that when you do that if you have if

00:35:39,080 --> 00:35:44,900
flag is hashing everything fairly which

00:35:41,480 --> 00:35:46,940
which it is then you are screwing over

00:35:44,900 --> 00:35:48,820
the people that come in on the on the on

00:35:46,940 --> 00:35:50,990
the links that are limited to 50 gigs

00:35:48,820 --> 00:35:53,410
because they're gonna be bandwidth

00:35:50,990 --> 00:35:55,640
constraint and TCP is gonna be you know

00:35:53,410 --> 00:35:57,170
gonna be sort of seeing congestion

00:35:55,640 --> 00:36:03,670
because if the NIC is going to be

00:35:57,170 --> 00:36:03,670
dropping packets on the way out hey Kirk

00:36:08,260 --> 00:36:15,980
100% of them so for capacity planning

00:36:12,410 --> 00:36:17,930
purposes we we have to prepare capacity

00:36:15,980 --> 00:36:24,590
planning purposes and for my performance

00:36:17,930 --> 00:36:28,960
work we do everything with 100% TLS 660

00:36:24,590 --> 00:36:28,960
ish percent on AMD and 70 ishani Intel

00:36:30,280 --> 00:36:37,880
we that's come down over the years the

00:36:35,410 --> 00:36:40,100
the CPU use now for a hundred percent

00:36:37,880 --> 00:36:42,860
TLS thanks to a lot of the work that's

00:36:40,100 --> 00:36:47,270
been done in the VM system by Jeff and

00:36:42,860 --> 00:36:51,860
Mark and Konstantin is down in the upper

00:36:47,270 --> 00:36:53,210
50s and that you see the Broadwell

00:36:51,860 --> 00:36:56,000
machines that I was talking about

00:36:53,210 --> 00:36:58,130
earlier at the they are so close to the

00:36:56,000 --> 00:36:59,480
memory bandwidth limit limit that the

00:36:58,130 --> 00:37:01,430
performance it's like kind of like a

00:36:59,480 --> 00:37:03,200
hockey stick whereas the memory

00:37:01,430 --> 00:37:05,630
bandwidth this is the memory bandwidth

00:37:03,200 --> 00:37:08,030
on this axis and on this axis and then

00:37:05,630 --> 00:37:10,100
the CPUs is like this as you get like

00:37:08,030 --> 00:37:11,270
the limit the hard limit is like 60

00:37:10,100 --> 00:37:12,859
gigabytes a second

00:37:11,270 --> 00:37:15,380
but as you get much of the further over

00:37:12,859 --> 00:37:18,650
e-50 you get the some more you sort to

00:37:15,380 --> 00:37:21,050
climb up on this hockey stick and the

00:37:18,650 --> 00:37:22,970
the any little thing like every cache

00:37:21,050 --> 00:37:26,390
line on those machines like any cache

00:37:22,970 --> 00:37:28,700
line is sacred so so basically any cache

00:37:26,390 --> 00:37:30,500
miss you can eat you can avoid you you

00:37:28,700 --> 00:37:32,720
move further and further down that

00:37:30,500 --> 00:37:35,060
hockey stick and as you you save an

00:37:32,720 --> 00:37:37,550
inordinate amount of CPU like you could

00:37:35,060 --> 00:37:39,740
eliminate I mean I there was an early

00:37:37,550 --> 00:37:41,510
optimization I did where I eliminated

00:37:39,740 --> 00:37:44,000
you know looking at the third line that

00:37:41,510 --> 00:37:45,859
cache line of a ten buff which saved

00:37:44,000 --> 00:37:48,470
like two or three percent CPU on those

00:37:45,859 --> 00:37:50,089
machines the same optimization on you

00:37:48,470 --> 00:37:51,619
know a cascade lake we probably save

00:37:50,089 --> 00:37:54,530
almost nothing because it's got excess

00:37:51,619 --> 00:37:56,589
excess bandwidth does that answer your

00:37:54,530 --> 00:37:56,589
question

00:38:02,920 --> 00:38:15,350
Adrian where a single listen call would

00:38:11,480 --> 00:38:19,610
return 16 sockets the on the per CPU

00:38:15,350 --> 00:38:22,340
PCBs and then he added like a call where

00:38:19,610 --> 00:38:26,060
the worker could get the socket and

00:38:22,340 --> 00:38:28,790
query on which CPU PCB this socket

00:38:26,060 --> 00:38:31,430
wasn't then find the worker there okay

00:38:28,790 --> 00:38:35,420
an approach like that help you for the

00:38:31,430 --> 00:38:36,050
nginx matching of the work of threads it

00:38:35,420 --> 00:38:37,850
might

00:38:36,050 --> 00:38:41,330
I would this with this was part of his

00:38:37,850 --> 00:38:47,930
RSS work I think but it never made the

00:38:41,330 --> 00:38:49,280
tree I think because of UDP I maybe we

00:38:47,930 --> 00:38:52,390
can talk afterward because I'm not

00:38:49,280 --> 00:38:52,390
familiar with that piece of it

00:39:19,550 --> 00:39:26,680
going once going twice

00:39:24,070 --> 00:39:35,370
alright I think I'm done thank you

00:39:26,680 --> 00:39:35,370
[Applause]

00:39:41,670 --> 00:39:43,730

YouTube URL: https://www.youtube.com/watch?v=8NSzkYSX5nY


