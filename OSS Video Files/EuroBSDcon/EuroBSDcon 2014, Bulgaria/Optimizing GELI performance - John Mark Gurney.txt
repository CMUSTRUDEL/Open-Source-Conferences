Title: Optimizing GELI performance - John Mark Gurney
Publication date: 2019-10-14
Playlist: EuroBSDcon 2014, Bulgaria
Description: 
	Abstract:

Features, like encryption, need to have minimal overhead for them to be widely adopted. If the performance is to slow, few people will use it. The first iteration of AES-XTS using AES-NI in FreeBSD was not much faster than the software version of it. The talk will describe why the AES-XTS algorithm was slow and what was done to improve it. It will cover topics from intrinsics, adding them to gcc and advantages of using them over assembly to how to use HWPC that are included in most modern processors to evaluate performance to identify performance bottle necks.

Speaker biography:

John-Mark Gurney has been using FreeBSD since 1994 and a committer since 1997. His last 10 years has been spent working in the security industry, at first, nCircle (now TripWire) and then at Cryptography Research. He is now doing independent consulting, looking to continue to improve FreeBSD.
Captions: 
	00:00:08,220 --> 00:00:13,410
let me introduce you to John Mike gurney

00:00:10,889 --> 00:00:16,619
with freebies decameter and I've been

00:00:13,410 --> 00:00:19,619
working lately on improving crypto stuff

00:00:16,619 --> 00:00:26,970
and especially July performance on

00:00:19,619 --> 00:00:28,680
FreeBSD Thank You Olivier so as he said

00:00:26,970 --> 00:00:33,360
my name is John Mark Carney I've been a

00:00:28,680 --> 00:00:36,059
committer since 1997 I've been a user of

00:00:33,360 --> 00:00:38,489
FreeBSD since actually this year

00:00:36,059 --> 00:00:41,640
sometime this year will be my 20th year

00:00:38,489 --> 00:00:43,820
anniversary of using FreeBSD so when

00:00:41,640 --> 00:00:46,829
Jordan put up this slide of installing

00:00:43,820 --> 00:00:50,039
FreeBSD from floppy I actually installed

00:00:46,829 --> 00:00:54,360
FreeBSD one one five one from 1.4 for

00:00:50,039 --> 00:00:58,800
Meg floppies so I I remember that quite

00:00:54,360 --> 00:01:00,989
fun way so as everybody you know the

00:00:58,800 --> 00:01:03,359
FreeBSD that we see today is

00:01:00,989 --> 00:01:07,440
dramatically different than the one that

00:01:03,359 --> 00:01:10,170
we had back then encryption I mean that

00:01:07,440 --> 00:01:13,530
was a new thing ssh didn't even exist

00:01:10,170 --> 00:01:15,630
when back in 1 1 5 1 that it had meant

00:01:13,530 --> 00:01:17,819
we who are still using telnet in our sh

00:01:15,630 --> 00:01:22,429
so it's awesome to see the progress that

00:01:17,819 --> 00:01:28,819
we've done so what okay

00:01:22,429 --> 00:01:32,459
so jelly is a disk encryption layer for

00:01:28,819 --> 00:01:35,130
FreeBSD it was created by PJ D who's

00:01:32,459 --> 00:01:38,849
somewhere around here I'm not sure if

00:01:35,130 --> 00:01:43,950
he's here but you know it did its thing

00:01:38,849 --> 00:01:45,450
it did it thing I created and I put

00:01:43,950 --> 00:01:48,840
together a new server that I was using

00:01:45,450 --> 00:01:51,840
for home storage that consisted of eight

00:01:48,840 --> 00:01:55,289
disks and when I it was a whole new

00:01:51,840 --> 00:01:57,780
server replacing my old one and you know

00:01:55,289 --> 00:02:00,840
why why encrypt everything one of the

00:01:57,780 --> 00:02:03,030
main main advantages is with hard disk

00:02:00,840 --> 00:02:05,640
if you want to RMA it if the drive is

00:02:03,030 --> 00:02:09,030
dead there's no way to destroy your data

00:02:05,640 --> 00:02:11,069
on it you know so not that helpful you

00:02:09,030 --> 00:02:18,550
know you have sensitive data you know

00:02:11,069 --> 00:02:22,540
whatever might be a so yeah and

00:02:18,550 --> 00:02:25,320
so the original server was using

00:02:22,540 --> 00:02:28,650
software crypto and I was getting like

00:02:25,320 --> 00:02:31,720
150 megabytes per second on it and then

00:02:28,650 --> 00:02:34,000
that was just too slow eight disks

00:02:31,720 --> 00:02:36,130
should perform much better yes they're

00:02:34,000 --> 00:02:37,900
all SATA but you know each individual

00:02:36,130 --> 00:02:39,850
disc hundred megabytes per second plus

00:02:37,900 --> 00:02:42,580
you know I should be getting a little

00:02:39,850 --> 00:02:44,800
bit better six core machine that's a lot

00:02:42,580 --> 00:02:50,410
of cores but still only 150 Meg's a

00:02:44,800 --> 00:02:53,140
second so after some research I did not

00:02:50,410 --> 00:02:55,090
realize that my aim D Opteron had a si

00:02:53,140 --> 00:02:57,780
instructions when I finally discovered

00:02:55,090 --> 00:03:00,610
it after a little bit of research turned

00:02:57,780 --> 00:03:03,370
loaded they ass 9 modules and the

00:03:00,610 --> 00:03:07,300
performance almost did not change I was

00:03:03,370 --> 00:03:10,450
like what you know I am that my previous

00:03:07,300 --> 00:03:13,330
company CRI they worked on reviewing the

00:03:10,450 --> 00:03:16,620
ASM a sni instructions when Intel was

00:03:13,330 --> 00:03:20,020
creating them so I knew for a fact that

00:03:16,620 --> 00:03:24,190
100 megabytes per second on 6 cores is

00:03:20,020 --> 00:03:26,230
much too slow and it's like and and and

00:03:24,190 --> 00:03:27,790
there were there are issues which I'll

00:03:26,230 --> 00:03:30,160
talk a little bit about more but

00:03:27,790 --> 00:03:32,709
depending upon the cipher mode you can

00:03:30,160 --> 00:03:36,190
have not-so-great performance but with

00:03:32,709 --> 00:03:38,950
XTS it should be significantly better

00:03:36,190 --> 00:03:41,320
and obviously if it's slow people won't

00:03:38,950 --> 00:03:44,320
use it I actually sometimes don't use

00:03:41,320 --> 00:03:46,450
SSH at home because well my servers can

00:03:44,320 --> 00:03:49,120
only get about 12 15 megabytes per

00:03:46,450 --> 00:03:51,280
second on a Gigabit LAN so if I need to

00:03:49,120 --> 00:03:53,470
transfer files I'll use and see you know

00:03:51,280 --> 00:03:56,170
because I can get 50 70 megabytes per

00:03:53,470 --> 00:03:58,690
second so and then another goal

00:03:56,170 --> 00:04:01,510
obviously is maintainability you know

00:03:58,690 --> 00:04:03,640
this is no longer the 1990s where you

00:04:01,510 --> 00:04:06,190
spend five minutes on a project you hack

00:04:03,640 --> 00:04:09,910
out some C code and you just use it so

00:04:06,190 --> 00:04:11,800
as I mentioned making sure you use the

00:04:09,910 --> 00:04:14,980
correct cipher mode is the most

00:04:11,800 --> 00:04:18,430
important thing the interesting part

00:04:14,980 --> 00:04:22,630
sadly again somebody's using the Linux

00:04:18,430 --> 00:04:25,060
penguin if you use a very bad encryption

00:04:22,630 --> 00:04:26,979
mode this is what you can get yes it is

00:04:25,060 --> 00:04:29,320
encrypted but you can still see that it

00:04:26,979 --> 00:04:30,830
is clearly tux the penguin but if you

00:04:29,320 --> 00:04:32,900
use a proper mode

00:04:30,830 --> 00:04:35,419
it should look completely random and

00:04:32,900 --> 00:04:38,870
then also sector size becomes a

00:04:35,419 --> 00:04:41,810
performance impact like if you use 4k

00:04:38,870 --> 00:04:44,659
sectors you can actually get good decent

00:04:41,810 --> 00:04:47,479
performance and the 4k limitation is the

00:04:44,659 --> 00:04:50,419
fact that that is a page size if you use

00:04:47,479 --> 00:04:52,849
512 sectors you have to do a lot more

00:04:50,419 --> 00:04:54,349
overhead in the crypto because now

00:04:52,849 --> 00:04:57,919
you're doing eight times as many

00:04:54,349 --> 00:05:01,069
function calls so like if you're doing

00:04:57,919 --> 00:05:03,740
performance measurements on GE zero

00:05:01,069 --> 00:05:05,599
you're looking at like 300 megabytes a

00:05:03,740 --> 00:05:08,389
second versus nine hundred megabytes a

00:05:05,599 --> 00:05:10,669
second by going simply from 512 to 4k

00:05:08,389 --> 00:05:12,680
sectors and then obviously key size

00:05:10,669 --> 00:05:14,780
believe it or not does not make that

00:05:12,680 --> 00:05:19,039
much of a difference especially with a

00:05:14,780 --> 00:05:22,669
S&I you are doing 14 rounds on 256 bit

00:05:19,039 --> 00:05:27,069
key but that's not much more than just

00:05:22,669 --> 00:05:29,659
than the ten rounds and 128 bit so

00:05:27,069 --> 00:05:34,030
performance as I mentioned is very

00:05:29,659 --> 00:05:38,419
variable and modes is another one

00:05:34,030 --> 00:05:43,190
geli supports a CBC mode and as you can

00:05:38,419 --> 00:05:45,880
see ECB d encrypt is very slow this is

00:05:43,190 --> 00:05:51,289
open SSL performance using a s and I

00:05:45,880 --> 00:05:53,509
instructions and this is on my 3.8

00:05:51,289 --> 00:05:57,319
gigahertz AMD Opteron and as you can see

00:05:53,509 --> 00:05:59,539
the other modes like counter and XTS all

00:05:57,319 --> 00:06:03,199
get very good performance and this is

00:05:59,539 --> 00:06:07,729
because both a XTS and counter are able

00:06:03,199 --> 00:06:11,000
to be fully paralyzed so cipher mode as

00:06:07,729 --> 00:06:13,580
I mentioned CBC as you notice if you

00:06:11,000 --> 00:06:15,500
have as you encrypt each block of

00:06:13,580 --> 00:06:17,930
plaintext you go through the block

00:06:15,500 --> 00:06:20,300
cipher operation but then this cipher

00:06:17,930 --> 00:06:23,389
output goes to next and so there is no

00:06:20,300 --> 00:06:27,279
way that you can pipeline this and so

00:06:23,389 --> 00:06:29,990
instead with a mode like XTS each

00:06:27,279 --> 00:06:34,969
plaintext each block cipher operation is

00:06:29,990 --> 00:06:39,080
completely independent and this this is

00:06:34,969 --> 00:06:41,930
the the tweak tweak factor which is a

00:06:39,080 --> 00:06:44,550
Galois lfsr which gets incremented and

00:06:41,930 --> 00:06:47,669
this operation is

00:06:44,550 --> 00:06:50,130
is very cheap if you know how to do it

00:06:47,669 --> 00:06:54,060
properly and this one is the one that

00:06:50,130 --> 00:06:58,199
can consume many cycles and have to do

00:06:54,060 --> 00:07:00,270
that so any and feel free to interrupt

00:06:58,199 --> 00:07:05,759
me if you have any questions or if you

00:07:00,270 --> 00:07:08,550
want to go into more detail so now the

00:07:05,759 --> 00:07:11,220
other mode that I mentioned is counter

00:07:08,550 --> 00:07:13,860
cipher mode and that is again very

00:07:11,220 --> 00:07:15,750
easily to be paralyzed because of the

00:07:13,860 --> 00:07:19,050
fact New York mere fact that that plane

00:07:15,750 --> 00:07:20,820
to the counter gets encrypted which is

00:07:19,050 --> 00:07:23,340
just simply encrypted as you go along

00:07:20,820 --> 00:07:25,500
and then the plaintext gets a ex-ored

00:07:23,340 --> 00:07:27,389
with the cipher text so again the

00:07:25,500 --> 00:07:31,050
counter mode does not have that and that

00:07:27,389 --> 00:07:34,500
as we discussed as you can see they have

00:07:31,050 --> 00:07:38,250
very similar performance profiles on

00:07:34,500 --> 00:07:40,949
that oh and the I'll actually talk about

00:07:38,250 --> 00:07:44,550
the reason why EC CVC decrypt is

00:07:40,949 --> 00:07:47,490
actually able to be performed in a very

00:07:44,550 --> 00:07:50,250
much faster than encrypt is because as

00:07:47,490 --> 00:07:52,380
you can see in order to decrypt you can

00:07:50,250 --> 00:07:54,300
do all these decrypt operations in

00:07:52,380 --> 00:07:55,949
parallel and then after you've done the

00:07:54,300 --> 00:07:58,349
decrypt you do the extra for the

00:07:55,949 --> 00:08:01,260
ciphertext so the inverse is fully

00:07:58,349 --> 00:08:06,139
paralyzed Abul but not the encrypt so

00:08:01,260 --> 00:08:06,139
reading data is fast but not writing so

00:08:07,220 --> 00:08:15,840
I went through so as I mentioned one of

00:08:10,860 --> 00:08:17,210
the big things is that with AES if if

00:08:15,840 --> 00:08:21,030
you don't have the instructions

00:08:17,210 --> 00:08:22,740
pipelining lets your AAS has ten twelve

00:08:21,030 --> 00:08:26,250
or fourteen rounds depending upon key

00:08:22,740 --> 00:08:30,210
size and each round depends upon the

00:08:26,250 --> 00:08:32,219
previous one there's there's a whole set

00:08:30,210 --> 00:08:35,839
of operations that you do for each round

00:08:32,219 --> 00:08:40,919
there's a sub byte surround there's a

00:08:35,839 --> 00:08:44,880
substitute rotate rows rotate mix

00:08:40,919 --> 00:08:47,329
columns and then a and then that's for

00:08:44,880 --> 00:08:52,410
those and so the thing is is that if you

00:08:47,329 --> 00:08:55,050
do that the original as9 code was doing

00:08:52,410 --> 00:08:56,660
only one AES block at a time and so you

00:08:55,050 --> 00:08:58,819
ended up with this simple

00:08:56,660 --> 00:09:02,000
things so like after three instructions

00:08:58,819 --> 00:09:05,269
this is a each of one of these as seven

00:09:02,000 --> 00:09:08,899
cycles so after three rounds that would

00:09:05,269 --> 00:09:12,670
take 21 clock cycles but the AAS and I

00:09:08,899 --> 00:09:16,730
instructions are able to be fully

00:09:12,670 --> 00:09:20,000
pipelined meaning that as depending upon

00:09:16,730 --> 00:09:22,009
other factors even though the

00:09:20,000 --> 00:09:23,810
instruction may take seven clock cycles

00:09:22,009 --> 00:09:25,910
to complete before you can get the

00:09:23,810 --> 00:09:27,860
results and use it you can actually

00:09:25,910 --> 00:09:30,800
schedule another one immediately on the

00:09:27,860 --> 00:09:33,769
next clock cycle so obviously if you're

00:09:30,800 --> 00:09:36,170
doing you know one after another that's

00:09:33,769 --> 00:09:38,389
not very good but as you see in this

00:09:36,170 --> 00:09:41,089
when we do three separate instructions

00:09:38,389 --> 00:09:43,879
each time we schedule a new clock cycle

00:09:41,089 --> 00:09:46,459
we can actually get another thing and so

00:09:43,879 --> 00:09:48,980
with this we've now done three blocks in

00:09:46,459 --> 00:09:54,139
merely two more clock cycles than we did

00:09:48,980 --> 00:09:56,240
a one one block the S&I implementation

00:09:54,139 --> 00:09:59,990
that I did actually does eight blocks at

00:09:56,240 --> 00:10:01,959
a time there's it's kind of processor

00:09:59,990 --> 00:10:04,399
optimization is always very tricky

00:10:01,959 --> 00:10:06,470
sometimes it's better to do only four

00:10:04,399 --> 00:10:08,689
blocks in parallel but I tried to do

00:10:06,470 --> 00:10:11,209
this target for all intel and AMD

00:10:08,689 --> 00:10:14,569
processors and some of the older Intel

00:10:11,209 --> 00:10:16,910
processors have less than ideal Layton

00:10:14,569 --> 00:10:19,850
sees and throughputs and so doing eight

00:10:16,910 --> 00:10:22,069
seemed about the best ideal to get the

00:10:19,850 --> 00:10:23,990
best performance and you know every

00:10:22,069 --> 00:10:28,370
processor you change something and

00:10:23,990 --> 00:10:30,589
things so the other thing is is that my

00:10:28,370 --> 00:10:34,130
measured performance was as I mentioned

00:10:30,589 --> 00:10:36,769
there's a tweak factor in XTS and this

00:10:34,130 --> 00:10:38,860
tweak factor is necessary in order to

00:10:36,769 --> 00:10:42,949
ensure that the data remains

00:10:38,860 --> 00:10:44,689
confidential if the if the tweak factor

00:10:42,949 --> 00:10:46,759
was not there then you would actually

00:10:44,689 --> 00:10:49,759
end up the problem with similar to the

00:10:46,759 --> 00:10:52,730
ECB attack where if you wrote one set of

00:10:49,759 --> 00:10:55,670
block of data and then you rewrote the

00:10:52,730 --> 00:10:57,410
next one you could actually if you knew

00:10:55,670 --> 00:10:58,970
the plaintext and you knew the cipher

00:10:57,410 --> 00:11:00,170
and you could observe the ciphertext so

00:10:58,970 --> 00:11:02,480
you could actually XOR the two

00:11:00,170 --> 00:11:03,860
ciphertext and you would actually be

00:11:02,480 --> 00:11:06,740
able to see the difference between the

00:11:03,860 --> 00:11:08,089
plaintext so sorry I if you only know

00:11:06,740 --> 00:11:10,160
the ciphertext

00:11:08,089 --> 00:11:12,589
and you were able to be able to get the

00:11:10,160 --> 00:11:15,379
X or difference between the two and so

00:11:12,589 --> 00:11:18,680
obviously if you're writing data to your

00:11:15,379 --> 00:11:23,509
hard disk confidentiality is necessary

00:11:18,680 --> 00:11:26,540
so the tweak factor also takes a little

00:11:23,509 --> 00:11:29,329
bit of work the original code I forget

00:11:26,540 --> 00:11:32,180
how long ago did the tweak factor in a

00:11:29,329 --> 00:11:35,209
very inefficient manner it did it each

00:11:32,180 --> 00:11:39,860
byte and the tweak factor is 16 bytes

00:11:35,209 --> 00:11:43,279
long and every for every every byte it

00:11:39,860 --> 00:11:45,259
would actually have to do it have to do

00:11:43,279 --> 00:11:48,829
a branch to test to whether there was a

00:11:45,259 --> 00:11:51,769
carry or not between the two things so

00:11:48,829 --> 00:11:53,689
we now have a 16 iteration loop with a

00:11:51,769 --> 00:11:56,019
branch and even with modern branch

00:11:53,689 --> 00:11:58,910
predicting processors that will never be

00:11:56,019 --> 00:12:03,470
predicted right so your performance will

00:11:58,910 --> 00:12:05,870
be very less than ideal and so if the

00:12:03,470 --> 00:12:09,110
high bit was actually necessary then we

00:12:05,870 --> 00:12:10,910
add the we add the reduction step I'm

00:12:09,110 --> 00:12:13,819
not going to get into gawa filled math

00:12:10,910 --> 00:12:17,509
it's not that interesting

00:12:13,819 --> 00:12:19,639
so then PJD did some improvements

00:12:17,509 --> 00:12:21,499
because that believe it or not was

00:12:19,639 --> 00:12:25,220
actually a significant performance

00:12:21,499 --> 00:12:27,829
problem and he improved it up to 64 bits

00:12:25,220 --> 00:12:30,860
in width and this obviously is much

00:12:27,829 --> 00:12:33,170
better now we have you know pull out the

00:12:30,860 --> 00:12:35,509
carry now we only have one if statement

00:12:33,170 --> 00:12:38,899
instead of 16 if statements on each

00:12:35,509 --> 00:12:44,269
iteration and add in the appropriate

00:12:38,899 --> 00:12:47,600
tweak factor so it turns out that even I

00:12:44,269 --> 00:12:50,559
work at CRI I worked with a friend of

00:12:47,600 --> 00:12:52,879
mine called Mike Hamburg and he does

00:12:50,559 --> 00:12:54,860
elliptic curve cryptography and those

00:12:52,879 --> 00:12:56,779
other really low-level stuff and it

00:12:54,860 --> 00:12:58,879
turns out that I was talking to him

00:12:56,779 --> 00:13:02,480
about various performance improvements

00:12:58,879 --> 00:13:06,290
and he actually came up with a way to do

00:13:02,480 --> 00:13:09,019
the tweak factor in five SSE

00:13:06,290 --> 00:13:12,110
instructions with no branches branching

00:13:09,019 --> 00:13:14,809
as I mentioned earlier if you branch in

00:13:12,110 --> 00:13:18,080
a processor you have to throw lots of

00:13:14,809 --> 00:13:20,360
state away so avoiding branches can make

00:13:18,080 --> 00:13:22,310
huge performance gains even if it seems

00:13:20,360 --> 00:13:23,990
that you have to do more work the

00:13:22,310 --> 00:13:27,970
pipeline will still be filled and you'll

00:13:23,990 --> 00:13:31,340
get better performance so in order to do

00:13:27,970 --> 00:13:35,030
but the tricky part is is that in SSC

00:13:31,340 --> 00:13:37,040
there SSE all of those instructions are

00:13:35,030 --> 00:13:39,920
designed to break up a large register

00:13:37,040 --> 00:13:43,520
into small sections like 32-bit words

00:13:39,920 --> 00:13:45,560
64-bit words you know for 32-bit

00:13:43,520 --> 00:13:47,660
floating points or two 64-bit

00:13:45,560 --> 00:13:50,660
floating-point numbers but they are not

00:13:47,660 --> 00:13:53,480
actually designed to operate as a

00:13:50,660 --> 00:13:55,760
128-bit number you'd think there would

00:13:53,480 --> 00:13:57,890
have some of that but it turns out that

00:13:55,760 --> 00:14:02,890
they don't so think simple things like

00:13:57,890 --> 00:14:07,100
you know a shift shift left by one bit

00:14:02,890 --> 00:14:09,440
you actually end up shifting 34 32-bit

00:14:07,100 --> 00:14:11,420
words one bit to the left but then you

00:14:09,440 --> 00:14:13,490
lose all those high bits so the question

00:14:11,420 --> 00:14:16,340
is is how do you actually implement some

00:14:13,490 --> 00:14:21,110
of these functions and so with a little

00:14:16,340 --> 00:14:25,220
bit of magic you can do that there's we

00:14:21,110 --> 00:14:26,660
put in a constant together I have an the

00:14:25,220 --> 00:14:29,540
next slide will have a nice little

00:14:26,660 --> 00:14:33,110
diagram on this we shuffle some things

00:14:29,540 --> 00:14:34,970
together we shift right arithmetic by 31

00:14:33,110 --> 00:14:39,590
bits which actually spreads everything

00:14:34,970 --> 00:14:42,950
over add in the alpha mask and then we

00:14:39,590 --> 00:14:45,590
shift left by one and then we return so

00:14:42,950 --> 00:14:48,610
this is a diagram of actually what is

00:14:45,590 --> 00:14:52,160
happening so we have the inputs and

00:14:48,610 --> 00:14:54,560
these are the four different 32-bit

00:14:52,160 --> 00:14:58,570
words we shuffle them around which is

00:14:54,560 --> 00:15:02,390
the equivalent of a left barrel shift by

00:14:58,570 --> 00:15:05,240
one 32-bit word then as you see the

00:15:02,390 --> 00:15:09,350
right shift arithmetic smears the high

00:15:05,240 --> 00:15:13,100
bit down so that all 32 bit registers

00:15:09,350 --> 00:15:14,990
contain it so this will either be a 0 or

00:15:13,100 --> 00:15:17,810
1 depending upon whether the high bit is

00:15:14,990 --> 00:15:21,080
1 and then when we add in the mask we

00:15:17,810 --> 00:15:24,320
will now have the equivalent high bit

00:15:21,080 --> 00:15:26,870
set so now this high bit magically gets

00:15:24,320 --> 00:15:30,020
transported to here and if the high bit

00:15:26,870 --> 00:15:31,610
needed to do the reduction stuff because

00:15:30,020 --> 00:15:34,100
this high bit was 1 we

00:15:31,610 --> 00:15:37,610
now add in the appropriate alpha mask to

00:15:34,100 --> 00:15:40,370
be reduced and then now we do the shift

00:15:37,610 --> 00:15:43,100
left by one and which vacates all of

00:15:40,370 --> 00:15:45,200
these places and then we XOR in the

00:15:43,100 --> 00:15:48,470
combination and since we know this will

00:15:45,200 --> 00:15:52,820
be 0 this will this will make the shift

00:15:48,470 --> 00:15:56,990
come out perfectly fine so the other

00:15:52,820 --> 00:16:00,649
step that I decided was well you know

00:15:56,990 --> 00:16:02,930
there's almost everybody has done low

00:16:00,649 --> 00:16:06,620
levels or there's been lots of

00:16:02,930 --> 00:16:08,870
implementations of using AES using

00:16:06,620 --> 00:16:10,880
low-level assembly but the problem is is

00:16:08,870 --> 00:16:15,050
who wants to maintain assembly code

00:16:10,880 --> 00:16:17,089
raise their hand nobody raised hands I

00:16:15,050 --> 00:16:19,190
mean we're getting to the point where

00:16:17,089 --> 00:16:21,560
even when you're doing supporting to an

00:16:19,190 --> 00:16:24,140
arm board it's like ok I'll do the

00:16:21,560 --> 00:16:25,790
minimum amount of work and assembly to

00:16:24,140 --> 00:16:27,740
the point where I can call in and have a

00:16:25,790 --> 00:16:29,570
proper C stack and then I'm doing all

00:16:27,740 --> 00:16:31,610
the rest of the work and C because you

00:16:29,570 --> 00:16:35,600
know assembly is just annoying to work

00:16:31,610 --> 00:16:38,390
with so in its you know sees almost

00:16:35,600 --> 00:16:40,880
low-level enough so there's some I

00:16:38,390 --> 00:16:44,480
decided to go with intrinsic for this

00:16:40,880 --> 00:16:46,790
project and there's well-established

00:16:44,480 --> 00:16:49,839
intrinsics that are cross-platform even

00:16:46,790 --> 00:16:53,120
between Windows and Linux and FreeBSD

00:16:49,839 --> 00:16:55,279
that allow you to get access to all of

00:16:53,120 --> 00:16:57,949
these intrinsics and there's some good

00:16:55,279 --> 00:17:00,110
advantages to using intrinsics some of

00:16:57,949 --> 00:17:04,699
it is works around there a bi limitation

00:17:00,110 --> 00:17:11,449
on amd64 the number of xmm registers

00:17:04,699 --> 00:17:14,059
that you can pass in to into a function

00:17:11,449 --> 00:17:15,679
is 8 which is a good amount of registers

00:17:14,059 --> 00:17:18,890
but the problem is is that when you

00:17:15,679 --> 00:17:21,500
return from the function you can only

00:17:18,890 --> 00:17:24,350
pass back to xmm registers now if you're

00:17:21,500 --> 00:17:27,530
doing a function that will do 8 blocks

00:17:24,350 --> 00:17:29,840
of encryption you would like to pass in

00:17:27,530 --> 00:17:31,580
8 registers which is the 8 blocks of the

00:17:29,840 --> 00:17:35,450
data that you're encrypting and then

00:17:31,580 --> 00:17:37,549
you'd like to pass back those 8 a box of

00:17:35,450 --> 00:17:38,750
data in the registers too as opposed to

00:17:37,549 --> 00:17:41,300
spelling to the stack

00:17:38,750 --> 00:17:44,929
even though modern processors are very

00:17:41,300 --> 00:17:45,470
very good at optimizing away stack reads

00:17:44,929 --> 00:17:47,480
and right

00:17:45,470 --> 00:17:49,610
you know even though l1 cache is still

00:17:47,480 --> 00:17:52,009
slower than the registers so being able

00:17:49,610 --> 00:17:53,840
to work around that you know and this if

00:17:52,009 --> 00:17:55,909
you can inline functions you can get

00:17:53,840 --> 00:17:56,509
some decent performance improvements and

00:17:55,909 --> 00:17:59,600
other stuff

00:17:56,509 --> 00:18:04,490
the other interesting thing we now have

00:17:59,600 --> 00:18:06,620
a single source of the a SSDs XTS

00:18:04,490 --> 00:18:10,190
implementation that works both between

00:18:06,620 --> 00:18:13,070
i386 and AMD 64 which means anybody who

00:18:10,190 --> 00:18:17,269
does performance improvements on AMD 64

00:18:13,070 --> 00:18:20,000
I 386 gets them for free if we had been

00:18:17,269 --> 00:18:23,360
doing this with assembly since all the

00:18:20,000 --> 00:18:25,789
local 32 64-bit registers don't match

00:18:23,360 --> 00:18:28,299
you'd have to rewrite to port it to two

00:18:25,789 --> 00:18:30,769
different to both I 386 and AMD 64

00:18:28,299 --> 00:18:33,409
admittedly not many people are going to

00:18:30,769 --> 00:18:36,110
be running a S&I on I 386 but there

00:18:33,409 --> 00:18:38,000
might be some people who do it and as we

00:18:36,110 --> 00:18:40,610
demonstrated nobody wants to write

00:18:38,000 --> 00:18:45,320
assembly so maintainable maintainability

00:18:40,610 --> 00:18:47,269
is nice obviously assembly when you know

00:18:45,320 --> 00:18:49,190
your processor architecture you can make

00:18:47,269 --> 00:18:52,879
sure all the instructions get scheduled

00:18:49,190 --> 00:18:54,529
in the right order modern compilers are

00:18:52,879 --> 00:18:57,710
getting better at this but they do not

00:18:54,529 --> 00:19:00,019
have the perfect idea of when should I

00:18:57,710 --> 00:19:02,059
run various instructions you know they

00:19:00,019 --> 00:19:04,789
don't know that oh if I run this

00:19:02,059 --> 00:19:07,700
multiply of schedule put this multiply

00:19:04,789 --> 00:19:11,840
few instructions earlier the data will

00:19:07,700 --> 00:19:13,700
be ready early enough and yes processors

00:19:11,840 --> 00:19:16,340
have look-ahead buffers in order to do

00:19:13,700 --> 00:19:19,940
out of order execution but that's not a

00:19:16,340 --> 00:19:24,200
cure-all for everything and so there's

00:19:19,940 --> 00:19:25,639
some advantages there assembly since

00:19:24,200 --> 00:19:28,850
you're writing everything it's much

00:19:25,639 --> 00:19:32,179
easier to do non-aligned loads yes

00:19:28,850 --> 00:19:35,450
even though x86 is a not in most loads

00:19:32,179 --> 00:19:39,490
do not require alignment SSE has both

00:19:35,450 --> 00:19:42,200
versions that do handle aligned may

00:19:39,490 --> 00:19:47,210
require alignment and will fault it's

00:19:42,200 --> 00:19:50,240
not and also unaligned memory loads now

00:19:47,210 --> 00:19:53,899
i've done some research and pretty much

00:19:50,240 --> 00:19:56,919
half within one generation all the

00:19:53,899 --> 00:19:58,800
processors had performance where

00:19:56,919 --> 00:20:00,840
unaligned and the line

00:19:58,800 --> 00:20:04,230
we're exactly the same performance there

00:20:00,840 --> 00:20:07,020
was no disadvantage except for obviously

00:20:04,230 --> 00:20:09,960
with unaligned you might cross a cache

00:20:07,020 --> 00:20:12,600
line but otherwise the performance is

00:20:09,960 --> 00:20:14,790
almost the same so it's almost why don't

00:20:12,600 --> 00:20:17,300
the compilers just use the on the line

00:20:14,790 --> 00:20:20,750
instruction by default but they don't

00:20:17,300 --> 00:20:26,030
the other issue was the original port

00:20:20,750 --> 00:20:30,450
binutils uses the a s assembler for GCC

00:20:26,030 --> 00:20:33,450
did not support a SMI intrinsics clang

00:20:30,450 --> 00:20:36,360
did but when I did the work FreeBSD was

00:20:33,450 --> 00:20:40,560
still trying to support GCC on both a

00:20:36,360 --> 00:20:43,860
386 and AMD 64 and we are old Binni

00:20:40,560 --> 00:20:48,050
tales from many many years ago and GCC

00:20:43,860 --> 00:20:51,600
did not have a S&I instruction support

00:20:48,050 --> 00:20:55,110
and so when a part of the project was

00:20:51,600 --> 00:20:58,200
actually adding that to GCC luckily

00:20:55,110 --> 00:20:59,850
other people had done it so I found

00:20:58,200 --> 00:21:05,780
their patches with a little bit of work

00:20:59,850 --> 00:21:07,950
managed to do it so not too bad so and

00:21:05,780 --> 00:21:10,440
actually when I last gave this

00:21:07,950 --> 00:21:12,540
presentation David Chu Snell mentioned

00:21:10,440 --> 00:21:14,100
that within inline assembly you can

00:21:12,540 --> 00:21:16,230
actually create your own Coleen and

00:21:14,100 --> 00:21:18,600
convention so you could actually work

00:21:16,230 --> 00:21:21,510
around the ABI limitations and still do

00:21:18,600 --> 00:21:23,880
the assembly version but then you still

00:21:21,510 --> 00:21:29,010
now have to maintain the assembly source

00:21:23,880 --> 00:21:31,890
code adding a sni to the tool chain as I

00:21:29,010 --> 00:21:35,340
mentioned claimed did not really need

00:21:31,890 --> 00:21:37,920
any work GCC which luckily hopefully

00:21:35,340 --> 00:21:40,740
will go away soon but is really really

00:21:37,920 --> 00:21:43,320
ancient we're based on four to one I

00:21:40,740 --> 00:21:44,610
heard that the for a GCC branch is

00:21:43,320 --> 00:21:47,040
frozen and I think the current

00:21:44,610 --> 00:21:51,240
development is 5 so just to give you an

00:21:47,040 --> 00:21:52,800
example of how old are been new to GCC

00:21:51,240 --> 00:21:54,990
binya tools I mean don't remember the

00:21:52,800 --> 00:21:58,140
exact version of our binutils but it's

00:21:54,990 --> 00:22:04,230
not much newer than 4 - if it is that

00:21:58,140 --> 00:22:09,900
much newer and so it turns out that we

00:22:04,230 --> 00:22:11,410
in order the we needed to add the AAS

00:22:09,900 --> 00:22:13,450
and I instructions too

00:22:11,410 --> 00:22:16,570
been details in order to do inline

00:22:13,450 --> 00:22:18,310
assembly in that we also needed GCC

00:22:16,570 --> 00:22:19,810
needed the new headers in order to

00:22:18,310 --> 00:22:22,900
provide the additional intrinsics

00:22:19,810 --> 00:22:26,620
luckily it turns out clangs headers I

00:22:22,900 --> 00:22:28,240
used one of those headers without any

00:22:26,620 --> 00:22:30,970
troubles and then I'd created the other

00:22:28,240 --> 00:22:34,270
one another instruction that came along

00:22:30,970 --> 00:22:39,190
with the AAS and I was an instruction

00:22:34,270 --> 00:22:41,170
called PCL mul qdq and are actually

00:22:39,190 --> 00:22:43,210
actually I must have that wrong on my

00:22:41,170 --> 00:22:47,110
slide because I'm pretty sure it's qdq

00:22:43,210 --> 00:22:50,740
not DQ D so I'll have to take a look at

00:22:47,110 --> 00:22:54,460
that this what I added that at the time

00:22:50,740 --> 00:22:56,290
I was doing this work XTS does not use

00:22:54,460 --> 00:22:58,990
this instruction I'm actually working

00:22:56,290 --> 00:23:01,990
right now working on a near completion

00:22:58,990 --> 00:23:06,340
of a freebsd foundation sponsor project

00:23:01,990 --> 00:23:08,470
to add ASG CM to the crypto to the open

00:23:06,340 --> 00:23:12,960
crypto framework and that ends up using

00:23:08,470 --> 00:23:17,500
the PCL mul qdq instruction because that

00:23:12,960 --> 00:23:18,910
does a Galois field multiply in for XTS

00:23:17,500 --> 00:23:22,240
we only needed to do what is the

00:23:18,910 --> 00:23:24,400
equivalent of what is called increment

00:23:22,240 --> 00:23:26,500
so to say a multiply by two and that is

00:23:24,400 --> 00:23:30,310
much easier but this one does a full

00:23:26,500 --> 00:23:32,310
64-bit multiply and so that actually

00:23:30,310 --> 00:23:37,480
ended up being useful for that

00:23:32,310 --> 00:23:40,900
so the original assembly that came with

00:23:37,480 --> 00:23:44,620
Deus and I done way way long ago looked

00:23:40,900 --> 00:23:47,770
like this and this was a simple ECB

00:23:44,620 --> 00:23:49,720
encrypt no pipelining and that was

00:23:47,770 --> 00:23:52,150
actually the other issue as the original

00:23:49,720 --> 00:23:54,880
assembly had absolutely no pipelining

00:23:52,150 --> 00:23:56,770
done and so my big big performance

00:23:54,880 --> 00:24:00,130
increase was simply being able to do

00:23:56,770 --> 00:24:04,510
eight blocks at a time now if you might

00:24:00,130 --> 00:24:07,270
notice this AES ink ink instruction is

00:24:04,510 --> 00:24:09,010
content commented out that is because we

00:24:07,270 --> 00:24:12,540
did not support assembling that

00:24:09,010 --> 00:24:15,040
instruction and so some the author

00:24:12,540 --> 00:24:17,260
basically hand assembled it and put in

00:24:15,040 --> 00:24:19,430
the raw byte codes in order to emulate

00:24:17,260 --> 00:24:23,360
that

00:24:19,430 --> 00:24:26,730
do you think this is maintainable no and

00:24:23,360 --> 00:24:28,470
obviously well if our GCC YouTube in

00:24:26,730 --> 00:24:30,390
details didn't support assembling it

00:24:28,470 --> 00:24:32,820
well and you want the new feature are

00:24:30,390 --> 00:24:35,580
you gonna spend the time to update GCC

00:24:32,820 --> 00:24:36,570
or make this you know you only have to

00:24:35,580 --> 00:24:41,880
do it in a few places

00:24:36,570 --> 00:24:44,640
so the intrinsics provides in c a native

00:24:41,880 --> 00:24:48,630
128-bit datatype there's other things

00:24:44,640 --> 00:24:51,120
the M 128 I is for integers there's also

00:24:48,630 --> 00:24:54,690
one for like single single precision

00:24:51,120 --> 00:24:58,170
floating point and also double precision

00:24:54,690 --> 00:24:59,970
floating point you can choose the

00:24:58,170 --> 00:25:02,640
intrinsic can be implemented either as

00:24:59,970 --> 00:25:05,060
an inline assembly function or a

00:25:02,640 --> 00:25:07,140
built-in the built-in actually

00:25:05,060 --> 00:25:10,080
implementing it as a built-in there are

00:25:07,140 --> 00:25:11,970
some advantages I ran in when I was

00:25:10,080 --> 00:25:14,880
doing some of my testing I ran into

00:25:11,970 --> 00:25:18,060
issues where if you passed an and

00:25:14,880 --> 00:25:20,430
constant that was in line that was

00:25:18,060 --> 00:25:22,710
passed through like two or three

00:25:20,430 --> 00:25:26,400
different functions even though the

00:25:22,710 --> 00:25:28,620
outer line was constant the optimizer

00:25:26,400 --> 00:25:30,660
could not actually detect through the

00:25:28,620 --> 00:25:33,570
different function calls that this was a

00:25:30,660 --> 00:25:36,840
constant and some of the instructions

00:25:33,570 --> 00:25:39,870
required a constant to be passed in line

00:25:36,840 --> 00:25:43,310
to the inline assembly otherwise it

00:25:39,870 --> 00:25:45,660
would fail and so when it was doing the

00:25:43,310 --> 00:25:47,340
optimization it would say sorry I can't

00:25:45,660 --> 00:25:49,440
compile this because it's not a constant

00:25:47,340 --> 00:25:52,980
even though it actually was a constant

00:25:49,440 --> 00:25:54,870
so but with the built-in it turns out

00:25:52,980 --> 00:25:57,060
that there was I did not actually

00:25:54,870 --> 00:26:00,990
implement any built-ins partly because

00:25:57,060 --> 00:26:03,990
well GCC is dying and at least for

00:26:00,990 --> 00:26:06,150
FreeBSD and so didn't want to do as much

00:26:03,990 --> 00:26:08,520
work so there are a few cases where an

00:26:06,150 --> 00:26:10,890
intrinsic will not work with the

00:26:08,520 --> 00:26:14,670
existing GCC but it will work on other

00:26:10,890 --> 00:26:17,580
cases the features that we added must be

00:26:14,670 --> 00:26:21,570
enabled via feature flags you know if

00:26:17,580 --> 00:26:24,180
you do a standard C C it will not

00:26:21,570 --> 00:26:26,430
include support for like SSE or the MMX

00:26:24,180 --> 00:26:31,590
instructions and so that we added

00:26:26,430 --> 00:26:32,460
additional mas and MPC LM all compiler

00:26:31,590 --> 00:26:35,610
flags in order

00:26:32,460 --> 00:26:38,759
to support the additional features and

00:26:35,610 --> 00:26:41,759
as I mentioned supporting unlined access

00:26:38,759 --> 00:26:44,070
was not very easy to do in the C code

00:26:41,759 --> 00:26:46,529
there's one where you could do the

00:26:44,070 --> 00:26:48,899
explicit load you you've run a sign

00:26:46,529 --> 00:26:50,940
there's also a load a if I remember

00:26:48,899 --> 00:26:54,230
correctly for loading the line but well

00:26:50,940 --> 00:26:54,230
you might as well just do a pointer

00:26:59,840 --> 00:27:05,360
dereferences tell the compiler that all

00:27:03,299 --> 00:27:08,789
sense all the members in the structure

00:27:05,360 --> 00:27:10,499
could be at any location and you know

00:27:08,789 --> 00:27:13,889
like if you follow a char by an integer

00:27:10,499 --> 00:27:16,590
that integer will be unlined it actually

00:27:13,889 --> 00:27:18,419
isn't smart enough to figure out that it

00:27:16,590 --> 00:27:20,610
assumes that the entire structure is

00:27:18,419 --> 00:27:23,009
unaligned even though it contains all

00:27:20,610 --> 00:27:27,059
properly aligned data structures and so

00:27:23,009 --> 00:27:29,369
the compiler will properly emit proper

00:27:27,059 --> 00:27:31,289
unaligned accesses when accessing

00:27:29,369 --> 00:27:34,850
through a pack structure when you're

00:27:31,289 --> 00:27:37,110
trying to access eight 128-bit registers

00:27:34,850 --> 00:27:39,059
it's much easier to use memory

00:27:37,110 --> 00:27:44,700
dereferences as opposed to having to

00:27:39,059 --> 00:27:48,659
call load you eight times so this is the

00:27:44,700 --> 00:27:50,999
intrinsic code for a simple AES encrypt

00:27:48,659 --> 00:27:53,129
round how many people think this is much

00:27:50,999 --> 00:27:58,379
better to read than the AES assembly

00:27:53,129 --> 00:28:00,029
earlier well yes it is still hard to

00:27:58,379 --> 00:28:02,039
read because this is pretty low-level

00:28:00,029 --> 00:28:05,279
stuff but now you can actually very

00:28:02,039 --> 00:28:09,840
easily see the fact that we XOR in the

00:28:05,279 --> 00:28:12,299
key schedule we do the ten twelve or

00:28:09,840 --> 00:28:14,460
fourteen rounds and then we just do the

00:28:12,299 --> 00:28:17,309
in class because the last round of AIS

00:28:14,460 --> 00:28:19,980
is special and we do that and we're done

00:28:17,309 --> 00:28:24,210
so you know this is much easier to work

00:28:19,980 --> 00:28:27,119
with so now the next thing is this now

00:28:24,210 --> 00:28:30,720
that we've had we have and when I did

00:28:27,119 --> 00:28:33,629
almost all of my testing this testing I

00:28:30,720 --> 00:28:36,539
did was pretty much all done in userland

00:28:33,629 --> 00:28:38,820
because I don't have to reboot kernels

00:28:36,539 --> 00:28:41,549
or reload modules or if I accidentally

00:28:38,820 --> 00:28:43,980
get a pointer wrong I dereference random

00:28:41,549 --> 00:28:44,970
memory data and crashed my machine I did

00:28:43,980 --> 00:28:46,799
all of the

00:28:44,970 --> 00:28:50,610
development work in userland which was

00:28:46,799 --> 00:28:52,409
very convenient to do but the next thing

00:28:50,610 --> 00:28:53,909
is this well it's not very useful in

00:28:52,409 --> 00:28:57,120
user land if we're going to be using it

00:28:53,909 --> 00:28:59,130
with for the online thing so now we need

00:28:57,120 --> 00:29:01,620
to add it to the colonel Colonel

00:28:59,130 --> 00:29:04,799
compiled the trickiness here is is that

00:29:01,620 --> 00:29:09,120
all the our FreeBSD kernel when it is

00:29:04,799 --> 00:29:11,309
compiled they disable all the standard

00:29:09,120 --> 00:29:13,799
includes because we do not want like

00:29:11,309 --> 00:29:15,780
standard i/o to be polluting the kernel

00:29:13,799 --> 00:29:17,820
compile the kernel has a completely

00:29:15,780 --> 00:29:20,370
different interface it implements its

00:29:17,820 --> 00:29:23,010
own like you know printf functions

00:29:20,370 --> 00:29:25,950
they're like you know F open doesn't

00:29:23,010 --> 00:29:27,539
exist in the kernel so all of those we

00:29:25,950 --> 00:29:30,450
don't want to pull in those header files

00:29:27,539 --> 00:29:34,320
and so but except the problem is is that

00:29:30,450 --> 00:29:36,480
the ASI intrinsic header files are only

00:29:34,320 --> 00:29:40,950
in user land they are not actually in

00:29:36,480 --> 00:29:43,200
the kernel so I have to remove the no

00:29:40,950 --> 00:29:46,740
standard Inc flag which is normally

00:29:43,200 --> 00:29:50,460
passed turn on increased optimization

00:29:46,740 --> 00:29:53,190
because in my testing Oh 203 gave better

00:29:50,460 --> 00:29:54,570
performance than Oh to the interesting

00:29:53,190 --> 00:29:57,090
thing is some other code that I was

00:29:54,570 --> 00:29:59,490
working on around the same time Oh 2

00:29:57,090 --> 00:30:03,600
actually gave better performance than oh

00:29:59,490 --> 00:30:06,539
3 and that gets into some really deep

00:30:03,600 --> 00:30:08,730
dark compiler magic and then obviously

00:30:06,539 --> 00:30:13,830
we have to turn on the AES instructions

00:30:08,730 --> 00:30:17,580
and the appropriate MMX and SSE so in my

00:30:13,830 --> 00:30:22,049
performance test has how many people

00:30:17,580 --> 00:30:24,030
here have used mini stat those who

00:30:22,049 --> 00:30:26,789
haven't used mini set it is an extremely

00:30:24,030 --> 00:30:28,950
useful tool that phk wrote a number of

00:30:26,789 --> 00:30:31,650
years ago the problem is is when you're

00:30:28,950 --> 00:30:35,549
benchmarking stuff if you run it once

00:30:31,650 --> 00:30:37,740
and you run it again computers do not

00:30:35,549 --> 00:30:39,360
always even though they execute the

00:30:37,740 --> 00:30:41,520
exact same thing interrupts and other

00:30:39,360 --> 00:30:44,179
stuff will change the performance and

00:30:41,520 --> 00:30:46,530
you will not get exactly the same

00:30:44,179 --> 00:30:48,990
runtime for every time you run

00:30:46,530 --> 00:30:51,690
instructions the prenda problem is is

00:30:48,990 --> 00:30:54,090
that in some cases the variability may

00:30:51,690 --> 00:30:56,100
be high enough and the performance may

00:30:54,090 --> 00:30:58,290
only be a few percent difference that

00:30:56,100 --> 00:31:00,780
you cannot actually clearly tell

00:30:58,290 --> 00:31:03,830
which performed better mini stat lets

00:31:00,780 --> 00:31:05,970
you run statistical analysis on your

00:31:03,830 --> 00:31:08,310
benchmark data to actually give you

00:31:05,970 --> 00:31:11,130
confident results that yes the change

00:31:08,310 --> 00:31:13,020
that I'd made did make a difference now

00:31:11,130 --> 00:31:16,470
admittedly there are some cases where

00:31:13,020 --> 00:31:18,450
recompiling the kernel when you add new

00:31:16,470 --> 00:31:20,700
code will shift things around so that

00:31:18,450 --> 00:31:22,680
your change may not actually have been

00:31:20,700 --> 00:31:25,440
the responsible change and changing

00:31:22,680 --> 00:31:28,080
around instruction layout can actually

00:31:25,440 --> 00:31:30,090
improve it but you know with a little

00:31:28,080 --> 00:31:33,330
bit of work you can actually figure this

00:31:30,090 --> 00:31:36,600
out so in this case the original

00:31:33,330 --> 00:31:39,000
software performance and if I remember

00:31:36,600 --> 00:31:41,130
quickly this was in megabytes per second

00:31:39,000 --> 00:31:44,550
or bytes per second the original

00:31:41,130 --> 00:31:50,760
software was not very good then I added

00:31:44,550 --> 00:31:54,440
the AAS and I and and it is very

00:31:50,760 --> 00:31:57,060
interesting when you are working on

00:31:54,440 --> 00:32:00,270
performing and increasing performance of

00:31:57,060 --> 00:32:01,920
software you really need to understand

00:32:00,270 --> 00:32:05,240
all of the systems that you interact

00:32:01,920 --> 00:32:08,070
with one of the problem when I was doing

00:32:05,240 --> 00:32:11,670
performance measurements and stuff I

00:32:08,070 --> 00:32:14,550
used some tools to actually visualize

00:32:11,670 --> 00:32:17,460
this and one of the things that I found

00:32:14,550 --> 00:32:21,210
out was that I had a random thread that

00:32:17,460 --> 00:32:23,910
was doing some some work and it didn't

00:32:21,210 --> 00:32:24,920
make sense and it turns out I'll talk a

00:32:23,910 --> 00:32:27,540
little bit more about this later

00:32:24,920 --> 00:32:31,830
enabling one little flag that basically

00:32:27,540 --> 00:32:34,650
said this is a synchronous synchronous

00:32:31,830 --> 00:32:36,540
call not to use the thread gave me a

00:32:34,650 --> 00:32:39,360
nice little boost of performance and as

00:32:36,540 --> 00:32:43,950
you can see you know I almost doubled

00:32:39,360 --> 00:32:46,560
doubled performance in this case so the

00:32:43,950 --> 00:32:48,900
thing is is benchmark getting standard

00:32:46,560 --> 00:32:51,150
timing benchmarks or bytes per second

00:32:48,900 --> 00:32:55,560
that's pretty easy but the question is

00:32:51,150 --> 00:32:57,720
is now when you're this that assumes you

00:32:55,560 --> 00:32:59,970
already know what you want to improve

00:32:57,720 --> 00:33:03,350
performance the next question is how do

00:32:59,970 --> 00:33:07,370
you even identify what you need to do

00:33:03,350 --> 00:33:11,549
provide for performance turns out that

00:33:07,370 --> 00:33:13,830
FreeBSD has a couple very useful tools

00:33:11,549 --> 00:33:18,899
in order to improve performance one of

00:33:13,830 --> 00:33:20,639
the first ones is PMC stat this I don't

00:33:18,899 --> 00:33:23,190
remember how long they've been around

00:33:20,639 --> 00:33:26,159
it's session I think well over 10 years

00:33:23,190 --> 00:33:29,039
but modern Intel processors and even

00:33:26,159 --> 00:33:31,440
other architecture processors have what

00:33:29,039 --> 00:33:34,019
are called performance monitoring

00:33:31,440 --> 00:33:36,269
counters in the silicon and you can

00:33:34,019 --> 00:33:39,029
enable them and you can actually count

00:33:36,269 --> 00:33:42,179
things like l1 cache misses l2 cache

00:33:39,029 --> 00:33:44,669
misses instruction pipeline stalls how

00:33:42,179 --> 00:33:46,649
many my crops were executed the

00:33:44,669 --> 00:33:50,299
instructions that were executed there

00:33:46,649 --> 00:33:53,129
are there's a huge huge host of

00:33:50,299 --> 00:33:56,639
instructions and if you and the nice

00:33:53,129 --> 00:34:00,779
thing is is that if you load the HW PMC

00:33:56,639 --> 00:34:03,059
module this all of these functions are

00:34:00,779 --> 00:34:04,919
available to user land as a normal user

00:34:03,059 --> 00:34:06,840
you don't have to be route you can run

00:34:04,919 --> 00:34:08,730
PMC stat to gather your stats as a

00:34:06,840 --> 00:34:11,609
normal user as long as you've already

00:34:08,730 --> 00:34:14,369
loaded the HW PMC module which is very

00:34:11,609 --> 00:34:18,000
convenient and in this case we basically

00:34:14,369 --> 00:34:19,919
say how many time measure all the all

00:34:18,000 --> 00:34:22,679
the times that the CPU clock is

00:34:19,919 --> 00:34:28,079
unhaunted and out and run my test

00:34:22,679 --> 00:34:33,179
perform my test turns out that PMC stat

00:34:28,079 --> 00:34:36,750
is a general utility and they're pre

00:34:33,179 --> 00:34:41,129
preceding pmc was jeep Roth has anybody

00:34:36,750 --> 00:34:43,770
used jeep Roth that's pretty good

00:34:41,129 --> 00:34:46,349
problem is is if you enable jeep Roth

00:34:43,770 --> 00:34:48,960
your performance goes to crap every

00:34:46,349 --> 00:34:51,899
single function gets annotated with an M

00:34:48,960 --> 00:34:54,149
count and stores all this data and so

00:34:51,899 --> 00:34:56,159
now instant instantly every function

00:34:54,149 --> 00:34:57,630
call you make now already takes a

00:34:56,159 --> 00:34:59,280
hundred clock cycles so if you had a

00:34:57,630 --> 00:35:01,859
tight function that was taking 10 clock

00:34:59,280 --> 00:35:05,220
cycles now it's 10 times worse you know

00:35:01,859 --> 00:35:07,500
and it just gets not very manageable

00:35:05,220 --> 00:35:09,900
your program runs 5 times slower

00:35:07,500 --> 00:35:12,510
the beauty with PMC since it's hardware

00:35:09,900 --> 00:35:14,010
it collects all the registers every once

00:35:12,510 --> 00:35:18,510
in a while interrupts to dump the data

00:35:14,010 --> 00:35:21,049
your program runs pretty much at native

00:35:18,510 --> 00:35:22,790
speed I don't you cannot tell that

00:35:21,049 --> 00:35:26,600
slowdown on that

00:35:22,790 --> 00:35:29,690
the only issue is is that G prof is one

00:35:26,600 --> 00:35:33,530
of the data output formats turns out G

00:35:29,690 --> 00:35:37,640
prof is in program a relic of the 1990s

00:35:33,530 --> 00:35:40,070
it the counters are only 16 bits and

00:35:37,640 --> 00:35:44,690
size so if you run anything for very

00:35:40,070 --> 00:35:48,890
long or very large you will overflow 64

00:35:44,690 --> 00:35:52,550
K turns out that a call tree format does

00:35:48,890 --> 00:35:56,990
not have this problem and call tree is

00:35:52,550 --> 00:35:59,270
used by K crash K cash grind to figure

00:35:56,990 --> 00:36:03,380
to be able to handle that so in my

00:35:59,270 --> 00:36:05,480
performance I had various functions that

00:36:03,380 --> 00:36:09,020
were well over a hundred thousand I

00:36:05,480 --> 00:36:14,150
forget the exact number so has anybody

00:36:09,020 --> 00:36:15,740
used K cash grind Wow actually there's a

00:36:14,150 --> 00:36:19,430
good number of people who have used it

00:36:15,740 --> 00:36:22,670
it one of the nice little graphs that it

00:36:19,430 --> 00:36:24,440
prints out that you can do is this and

00:36:22,670 --> 00:36:27,980
if you're actually running the

00:36:24,440 --> 00:36:30,320
application you can mouse over this and

00:36:27,980 --> 00:36:32,840
all this big areas where you want to

00:36:30,320 --> 00:36:35,930
optimize obviously it's kind of hard to

00:36:32,840 --> 00:36:38,780
optimize the idle CPU but this you can

00:36:35,930 --> 00:36:42,500
actually see the perm the majority of my

00:36:38,780 --> 00:36:45,350
work is done in a SN 8 which is the 8

00:36:42,500 --> 00:36:47,510
block encrypt and considering that we're

00:36:45,350 --> 00:36:50,150
doing that's the most innermost loop

00:36:47,510 --> 00:36:52,640
that's a good thing to see you can't

00:36:50,150 --> 00:36:55,460
optimize that anymore but the thing is

00:36:52,640 --> 00:36:57,380
is that you can optimize all the call

00:36:55,460 --> 00:37:00,410
parent calls around it and as you can

00:36:57,380 --> 00:37:03,680
see there's actually a lot of area that

00:37:00,410 --> 00:37:06,140
you can optimize and I I know I have a

00:37:03,680 --> 00:37:08,000
bigger version but Oh see this one I'm

00:37:06,140 --> 00:37:10,850
not sure what happened but this is the

00:37:08,000 --> 00:37:12,770
cranking the lfsr and you can that

00:37:10,850 --> 00:37:15,170
actually ends up using about two point

00:37:12,770 --> 00:37:16,970
six eight percent and if it's already

00:37:15,170 --> 00:37:19,100
and I think this was on my optimized

00:37:16,970 --> 00:37:20,480
version if even if on the optimized

00:37:19,100 --> 00:37:22,370
version that was using two point six

00:37:20,480 --> 00:37:27,080
eight percent you can imagine how big

00:37:22,370 --> 00:37:30,620
this was back on the by taxes one and so

00:37:27,080 --> 00:37:32,570
I was poking around seeing what was

00:37:30,620 --> 00:37:35,120
going on and other stuff and I think

00:37:32,570 --> 00:37:36,140
this is the thread that I did but with a

00:37:35,120 --> 00:37:38,210
little bit of poking

00:37:36,140 --> 00:37:40,550
I found out that there was this random

00:37:38,210 --> 00:37:43,040
thread that I wasn't expecting called

00:37:40,550 --> 00:37:46,910
crypto rat proc that was consuming

00:37:43,040 --> 00:37:49,610
approximately 15% of the CPU and upon

00:37:46,910 --> 00:37:52,850
further investigation that was the whole

00:37:49,610 --> 00:37:55,660
sync flag that I was talking about this

00:37:52,850 --> 00:37:58,820
was one of the first real non

00:37:55,660 --> 00:38:01,790
algorithmic changes that I made and by

00:37:58,820 --> 00:38:04,400
simply adding it was a one-line diff

00:38:01,790 --> 00:38:06,830
that was when I was registering a s and

00:38:04,400 --> 00:38:10,970
I driver in with open crypto framework

00:38:06,830 --> 00:38:13,010
if I said hey you know call me I don't

00:38:10,970 --> 00:38:15,860
need to schedule in a separate thread if

00:38:13,010 --> 00:38:18,770
you just do a sync call perform I got a

00:38:15,860 --> 00:38:20,620
27% performance increase simply by

00:38:18,770 --> 00:38:22,910
adding that that was one of the most

00:38:20,620 --> 00:38:25,880
happy patches that I committed you know

00:38:22,910 --> 00:38:27,860
it's kind of hard not to add you know

00:38:25,880 --> 00:38:30,830
what is it however many characters that

00:38:27,860 --> 00:38:34,540
is plus the pipe to and get a nice

00:38:30,830 --> 00:38:38,480
almost a third percent performance so

00:38:34,540 --> 00:38:41,950
the other utility that you can use to

00:38:38,480 --> 00:38:48,740
measure performance in freebsd is DTrace

00:38:41,950 --> 00:38:51,770
dtrace goes dtrace how many people have

00:38:48,740 --> 00:38:56,690
heard of DTrace so how many people have

00:38:51,770 --> 00:39:02,180
used DTrace it's very very very useful

00:38:56,690 --> 00:39:05,540
utility actually in another in another

00:39:02,180 --> 00:39:08,540
project a PCIe driver I was writing I

00:39:05,540 --> 00:39:11,540
actually discovered an interrupt storm

00:39:08,540 --> 00:39:13,760
that was only possible to discover with

00:39:11,540 --> 00:39:16,340
printf the code worked perfectly fine

00:39:13,760 --> 00:39:18,170
with or without with the printf sand

00:39:16,340 --> 00:39:20,510
without the prin s it worked perfectly

00:39:18,170 --> 00:39:22,760
fine but there was an interrupt that was

00:39:20,510 --> 00:39:25,610
firing repeatedly that I wasn't shutting

00:39:22,760 --> 00:39:28,040
him down but I could not tell that but

00:39:25,610 --> 00:39:29,900
with dtrace the overhead was so little

00:39:28,040 --> 00:39:31,790
that I was actually able to see an

00:39:29,900 --> 00:39:34,220
interrupt storm and it go away

00:39:31,790 --> 00:39:37,730
and so dtrace is a very beautiful thing

00:39:34,220 --> 00:39:41,390
the other useful thing is is that dtrace

00:39:37,730 --> 00:39:43,370
you can pull full stacks so this is what

00:39:41,390 --> 00:39:46,630
is called a flame graph if you do a

00:39:43,370 --> 00:39:49,200
google for flame graph the readme on

00:39:46,630 --> 00:39:52,020
generating the flame graph works

00:39:49,200 --> 00:39:55,190
exactly as is in the read meet just as

00:39:52,020 --> 00:39:58,740
long as you have loaded the D trace so

00:39:55,190 --> 00:40:00,780
the in the case of the dtrace you

00:39:58,740 --> 00:40:03,290
basically set up a profiling Hertz like

00:40:00,780 --> 00:40:06,030
every thousand times a second it will

00:40:03,290 --> 00:40:08,790
basically sample what the kernel stack

00:40:06,030 --> 00:40:10,829
is and so this is a standard way to do a

00:40:08,790 --> 00:40:13,619
statistical performance monitoring and

00:40:10,829 --> 00:40:16,500
figure out exactly what is and so this

00:40:13,619 --> 00:40:19,200
is really just kind of like a one or two

00:40:16,500 --> 00:40:20,940
dimensional version of the exact pretty

00:40:19,200 --> 00:40:25,559
much the exact same graph that we saw

00:40:20,940 --> 00:40:26,849
here but it's pretty to look at and

00:40:25,559 --> 00:40:29,099
gives you the exact same information

00:40:26,849 --> 00:40:33,000
because as you can see there's that I

00:40:29,099 --> 00:40:35,750
think that's the AAS Inc eight and then

00:40:33,000 --> 00:40:38,280
you can also see these other stuff other

00:40:35,750 --> 00:40:41,940
other pits of working so this is

00:40:38,280 --> 00:40:43,619
actually the system calls user land side

00:40:41,940 --> 00:40:46,859
of things this is all the crypto I

00:40:43,619 --> 00:40:49,349
actually removed the idle thread on this

00:40:46,859 --> 00:40:52,410
just because the idle thread was like

00:40:49,349 --> 00:40:54,599
over half the graph and that you know

00:40:52,410 --> 00:40:59,579
the data that I care about is this data

00:40:54,599 --> 00:41:01,829
so in this case there's also the gm0

00:40:59,579 --> 00:41:05,549
thread which is kind of like def zero

00:41:01,829 --> 00:41:08,520
but for disk devices which gives is much

00:41:05,549 --> 00:41:11,670
lighter weight than the using like a dev

00:41:08,520 --> 00:41:14,480
MD and so turn off witness if you're

00:41:11,670 --> 00:41:18,059
running head so that you don't have that

00:41:14,480 --> 00:41:21,599
the other thing that when I was working

00:41:18,059 --> 00:41:24,809
on jelly jelly doing an original

00:41:21,599 --> 00:41:29,549
software crypto you want to use as many

00:41:24,809 --> 00:41:31,380
cores as possible so jelly by default

00:41:29,549 --> 00:41:34,530
will actually find out how many cores

00:41:31,380 --> 00:41:36,359
your machine has and launch that number

00:41:34,530 --> 00:41:39,480
of threads in order to handle it

00:41:36,359 --> 00:41:42,990
that way when if you it receives a 128

00:41:39,480 --> 00:41:46,680
kilobyte read requests it will split

00:41:42,990 --> 00:41:49,319
that 128 kilobits into 8 km to four

00:41:46,680 --> 00:41:51,450
kilobyte sectors so will generate 32

00:41:49,319 --> 00:41:55,109
requests and ship them off that means

00:41:51,450 --> 00:41:56,060
that you know all six 824 cores or

00:41:55,109 --> 00:41:58,850
however many

00:41:56,060 --> 00:42:02,930
your processor has can do some of the

00:41:58,850 --> 00:42:05,180
work and then it'll react together the

00:42:02,930 --> 00:42:09,020
thing is is that as I originally stated

00:42:05,180 --> 00:42:12,560
on the I was using this on an eight disk

00:42:09,020 --> 00:42:16,190
subsystem with six cores so that and

00:42:12,560 --> 00:42:19,100
then oh my root is also jelly encrypted

00:42:16,190 --> 00:42:22,220
my Zilla is also jelly encrypted my l2

00:42:19,100 --> 00:42:25,100
arc is also encrypted and my swap

00:42:22,220 --> 00:42:28,490
partition is also encrypted so if I I

00:42:25,100 --> 00:42:31,880
think I had like 15 jelly volumes on the

00:42:28,490 --> 00:42:33,890
system and if they all fired off six

00:42:31,880 --> 00:42:36,620
cores you're looking at like around 100

00:42:33,890 --> 00:42:38,660
threads that you know and the thing

00:42:36,620 --> 00:42:41,120
think about that even just when I access

00:42:38,660 --> 00:42:44,270
to did a read or write to the eight disk

00:42:41,120 --> 00:42:46,490
ZFS raid you know eight times six

00:42:44,270 --> 00:42:47,930
threads would suddenly you have this

00:42:46,490 --> 00:42:50,060
thundering herd problems of all these

00:42:47,930 --> 00:42:52,250
threads wanting to do work so I actually

00:42:50,060 --> 00:42:55,430
got significant performance by just

00:42:52,250 --> 00:42:57,470
restricting it to one obviously totally

00:42:55,430 --> 00:42:59,720
depends upon your workload and so

00:42:57,470 --> 00:43:02,930
current performance right now is around

00:42:59,720 --> 00:43:05,300
900 megabytes per second oh and other

00:43:02,930 --> 00:43:07,580
option is is that you can tell jelly not

00:43:05,300 --> 00:43:09,410
to actually zero the data just to get

00:43:07,580 --> 00:43:11,270
that little bit of performance and so

00:43:09,410 --> 00:43:13,100
we're getting around 900 megabytes per

00:43:11,270 --> 00:43:16,310
second on my three point four gigahertz

00:43:13,100 --> 00:43:20,630
AMD processor that will turbo boost up

00:43:16,310 --> 00:43:25,460
to four gigahertz there are some

00:43:20,630 --> 00:43:27,470
continued improvements the only calls

00:43:25,460 --> 00:43:30,260
through the open crypto framework are

00:43:27,470 --> 00:43:33,620
improved there's a direct rinrin doll

00:43:30,260 --> 00:43:35,660
calls that a few parts of the kernel use

00:43:33,620 --> 00:43:39,160
that will not receive performance part

00:43:35,660 --> 00:43:42,560
of this is is that in the kernel that

00:43:39,160 --> 00:43:44,840
SSE instructions use the FPU freebsd

00:43:42,560 --> 00:43:48,370
kernel by default does not actually save

00:43:44,840 --> 00:43:51,800
the FPU registers because right now with

00:43:48,370 --> 00:43:54,020
SSE instructions the entire FPU safe

00:43:51,800 --> 00:43:56,630
state is about a kilobyte and with

00:43:54,020 --> 00:43:59,060
upcoming avx2 instructions that will

00:43:56,630 --> 00:44:00,800
increase to two kilobytes so having to

00:43:59,060 --> 00:44:02,750
save two kilobytes of data in addition

00:44:00,800 --> 00:44:04,940
to all the registers on every context

00:44:02,750 --> 00:44:07,520
which would become very very expensive

00:44:04,940 --> 00:44:09,260
and so the kernel by default does not

00:44:07,520 --> 00:44:10,700
allow you to use FPU

00:44:09,260 --> 00:44:13,310
which is part of the reason why there's

00:44:10,700 --> 00:44:18,280
a special an ID driver in order to do

00:44:13,310 --> 00:44:22,040
the work and so with that so yeah that's

00:44:18,280 --> 00:44:24,020
and and the problem is is that the

00:44:22,040 --> 00:44:26,360
direct software implementation sometimes

00:44:24,020 --> 00:44:28,400
is called in a place where other locks

00:44:26,360 --> 00:44:32,200
are held and so you cannot actually

00:44:28,400 --> 00:44:36,140
allocate memory to to I would have the

00:44:32,200 --> 00:44:38,270
FPU safe state in or in non sleepable

00:44:36,140 --> 00:44:41,570
context so there's some work that I'd

00:44:38,270 --> 00:44:45,080
like to get done on that partly also SSC

00:44:41,570 --> 00:44:49,460
256 there's a improved algorithm using

00:44:45,080 --> 00:44:53,870
SSE registers to improve sha 256 using

00:44:49,460 --> 00:44:57,050
bestest e there's Jeff just actually

00:44:53,870 --> 00:44:59,270
about a month ago commit was made to

00:44:57,050 --> 00:45:04,460
increase our memory pool such that large

00:44:59,270 --> 00:45:06,380
allocations up to 64 K are now cached as

00:45:04,460 --> 00:45:09,200
opposed to up to 4 K this actually

00:45:06,380 --> 00:45:12,350
improves things a little bit because

00:45:09,200 --> 00:45:14,360
jelly makes some of the times it makes

00:45:12,350 --> 00:45:16,970
very large memory allocations to the

00:45:14,360 --> 00:45:19,370
tune of 128 K or greater the problem is

00:45:16,970 --> 00:45:21,770
is that freebsd whenever you do a large

00:45:19,370 --> 00:45:23,690
memory allocation it actually has to do

00:45:21,770 --> 00:45:26,720
send out a notification to all the other

00:45:23,690 --> 00:45:29,680
cores to invalidate the page tables for

00:45:26,720 --> 00:45:31,940
that range so that that way if you're

00:45:29,680 --> 00:45:34,190
that new and what as soon as you use

00:45:31,940 --> 00:45:36,500
that new allocation if your thread gets

00:45:34,190 --> 00:45:39,460
scheduled for another processor that it

00:45:36,500 --> 00:45:42,590
actually has the proper virtual memory

00:45:39,460 --> 00:45:44,720
the other option is is since a ESX GS

00:45:42,590 --> 00:45:47,450
uses two different keys

00:45:44,720 --> 00:45:50,090
you could pipeline the key schedule but

00:45:47,450 --> 00:45:51,470
since it gets done so early the

00:45:50,090 --> 00:45:54,890
performance would not actually be

00:45:51,470 --> 00:45:58,100
improved and as I mentioned the AES GCM

00:45:54,890 --> 00:45:59,990
work is coming along some of it's being

00:45:58,100 --> 00:46:03,920
reviewed right now and then also I

00:45:59,990 --> 00:46:07,280
mentioned the sha-256 for ZFS and as I

00:46:03,920 --> 00:46:08,840
talked with pgaede at the dev summit and

00:46:07,280 --> 00:46:10,670
there are lots of other improvements to

00:46:08,840 --> 00:46:14,450
the open crypto framework that I'd like

00:46:10,670 --> 00:46:17,350
to make so so does anybody have any

00:46:14,450 --> 00:46:17,350
questions for me

00:46:18,880 --> 00:46:33,470
so Oh microphone ITAR I just wanted to

00:46:30,319 --> 00:46:39,799
ask when you told about how many rounds

00:46:33,470 --> 00:46:42,680
of ia 60s you make other in effect n

00:46:39,799 --> 00:46:47,240
plus one round yes because the last one

00:46:42,680 --> 00:46:50,599
doesn't count as M so the no action so

00:46:47,240 --> 00:46:54,410
the AES Inc last let's see here let me

00:46:50,599 --> 00:47:01,880
go you're talking about like this code

00:46:54,410 --> 00:47:03,829
right yes so it's I'm trying to read

00:47:01,880 --> 00:47:08,359
it's been a while since I remember that

00:47:03,829 --> 00:47:11,029
it's really confusing because the let's

00:47:08,359 --> 00:47:13,759
hear the key schedule is one there's a

00:47:11,029 --> 00:47:18,740
so like in a 10-round you have 12 of

00:47:13,759 --> 00:47:20,809
these so I thought I'd have to take a

00:47:18,740 --> 00:47:25,039
look at the Wikipedia to remind myself

00:47:20,809 --> 00:47:28,190
but I think that even that in class is

00:47:25,039 --> 00:47:30,589
actually the 10th round if I remember

00:47:28,190 --> 00:47:32,150
correctly and so like the nine is normal

00:47:30,589 --> 00:47:35,559
and then the 10th isn't it extra it's

00:47:32,150 --> 00:47:47,539
not like ten normal and then eleventh so

00:47:35,559 --> 00:47:49,310
but any other questions alright thank

00:47:47,539 --> 00:47:52,200
you for your time thank you

00:47:49,310 --> 00:47:58,579
[Applause]

00:47:52,200 --> 00:47:58,579

YouTube URL: https://www.youtube.com/watch?v=j4ICLdGAwVw


