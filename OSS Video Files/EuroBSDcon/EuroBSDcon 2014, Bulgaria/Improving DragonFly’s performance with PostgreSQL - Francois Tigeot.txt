Title: Improving DragonFly’s performance with PostgreSQL - Francois Tigeot
Publication date: 2019-10-13
Playlist: EuroBSDcon 2014, Bulgaria
Description: 
	Abstract:

Since its creation, the DragonFly operating system had contained a giant lock in its kernel, preventing more than one thread to use kernel resources at the same time. This big lock was finally removed after the 2.10 release. I then started to look for a good way to test the multi-processor performance and scalability of the operating system. PostgreSQL seemed to fit the bill with its PGbench tool.This talk will focus on how PostgreSQL was used as a benchmarking tool to check and improve DragonFly’s performance and scalability.

Speaker biography:

Independent consultant, sysadmin, *BSD and PostgreSQL user since 1999, DragonFly developer since 2011
Captions: 
	00:00:02,780 --> 00:00:24,570
is it working yet ello everyone sorry

00:00:23,760 --> 00:00:27,240
for the delay

00:00:24,570 --> 00:00:30,949
I am from Swati Joe and I will talk

00:00:27,240 --> 00:00:34,950
about PostgreSQL and oh we used PG bench

00:00:30,949 --> 00:00:36,899
to benchmark to check and improve the

00:00:34,950 --> 00:00:41,309
scalability of the dragonfly bsd

00:00:36,899 --> 00:00:45,149
operating system so i'm an independent

00:00:41,309 --> 00:00:48,149
consultant and season me for mercy seat

00:00:45,149 --> 00:00:51,750
release engineer I work in the data flow

00:00:48,149 --> 00:00:55,410
registry you have been a BSD and

00:00:51,750 --> 00:00:58,140
PostgreSQL user for a long time I've

00:00:55,410 --> 00:01:02,609
introduced FreeBSD for name servers of

00:00:58,140 --> 00:01:11,850
the dot F or g3 um I've been a dragonfly

00:01:02,609 --> 00:01:14,210
developer for a few years now dragonfly

00:01:11,850 --> 00:01:20,880
bsd is a unix-like operating system

00:01:14,210 --> 00:01:24,530
based on freebsd it was forked in 2003

00:01:20,880 --> 00:01:27,470
by matthew Dillon from FreeBSD four

00:01:24,530 --> 00:01:31,439
there were many goals at the beginning

00:01:27,470 --> 00:01:34,409
but most important one are about

00:01:31,439 --> 00:01:38,220
performance multiprocessors scalability

00:01:34,409 --> 00:01:44,759
and for that dragonfly uses per core

00:01:38,220 --> 00:01:47,130
replicated resources and messaging it is

00:01:44,759 --> 00:01:51,000
bit different from most of the other

00:01:47,130 --> 00:01:56,479
unix-like operating systems it doesn't

00:01:51,000 --> 00:02:00,329
use complicating algorithm and by using

00:01:56,479 --> 00:02:03,210
replicated resources per core biological

00:02:00,329 --> 00:02:07,040
core on hyper threading machines many

00:02:03,210 --> 00:02:07,040
operations are naturally lot less

00:02:07,700 --> 00:02:13,580
and dragonfly are the pure innovative

00:02:10,940 --> 00:02:16,280
features which can be very useful for

00:02:13,580 --> 00:02:19,790
some workloads and particular database

00:02:16,280 --> 00:02:23,420
loads for example we have web cache

00:02:19,790 --> 00:02:26,360
which is a signal level file cache using

00:02:23,420 --> 00:02:30,260
this weapon for a structure and optimize

00:02:26,360 --> 00:02:33,250
versus DS this is a relative particular

00:02:30,260 --> 00:02:41,470
performance with and wizards web cache

00:02:33,250 --> 00:02:45,970
the blue curve it was registered on the

00:02:41,470 --> 00:02:49,280
workstation with a limited amount memory

00:02:45,970 --> 00:02:52,340
by someone wanted to map and process

00:02:49,280 --> 00:02:55,730
maps for a good chunk of the North

00:02:52,340 --> 00:02:59,930
American continent use the OpenStreetMap

00:02:55,730 --> 00:03:02,480
project if I'm not mistaken and it's

00:02:59,930 --> 00:03:05,620
machine just couldn't keep up it didn't

00:03:02,480 --> 00:03:10,150
add enough number but by using swap cash

00:03:05,620 --> 00:03:13,850
it was able to just buy a small SSD and

00:03:10,150 --> 00:03:18,140
process all the amount of mapping data

00:03:13,850 --> 00:03:25,760
22 with a much less server performance

00:03:18,140 --> 00:03:29,750
degradation basically it avoided to buy

00:03:25,760 --> 00:03:37,430
a new workstation and only had to spend

00:03:29,750 --> 00:03:41,930
about $300 instead of 3,000 I will now

00:03:37,430 --> 00:03:47,540
talk about the PG ben-judah j-- what I

00:03:41,930 --> 00:03:52,030
did historically in details the first PG

00:03:47,540 --> 00:03:56,690
benched Rams were made in November 2011

00:03:52,030 --> 00:03:59,600
at the time dragonfly the last stable

00:03:56,690 --> 00:04:00,500
release of dragonfly still had a Big

00:03:59,600 --> 00:04:03,739
Mike

00:04:00,500 --> 00:04:08,450
multi persistent Lodge and we just

00:04:03,739 --> 00:04:11,019
removed it in the development version I

00:04:08,450 --> 00:04:17,299
have the kitchen to use a drowsy machine

00:04:11,019 --> 00:04:19,410
it could run 24 hour threads and I was

00:04:17,299 --> 00:04:21,420
looking for benchmarks

00:04:19,410 --> 00:04:24,330
which could show improvements and

00:04:21,420 --> 00:04:26,700
multiprocessor scalability from the

00:04:24,330 --> 00:04:28,260
development version to this table from

00:04:26,700 --> 00:04:33,630
the stable version to the development

00:04:28,260 --> 00:04:38,400
version so PG bench was a good fit PG

00:04:33,630 --> 00:04:40,460
bench is Postgres specific it can be

00:04:38,400 --> 00:04:42,900
used in different ways

00:04:40,460 --> 00:04:45,320
there's read-only

00:04:42,900 --> 00:04:48,030
workload which is very useful for

00:04:45,320 --> 00:04:51,510
showing multiprocessor scalability and

00:04:48,030 --> 00:04:55,440
this is one I used it doesn't touch

00:04:51,510 --> 00:04:57,060
touch file systems well does use file

00:04:55,440 --> 00:05:00,300
systems but it doesn't touch disks

00:04:57,060 --> 00:05:06,240
directly all five systems operation can

00:05:00,300 --> 00:05:08,330
be made from memory and we don't have

00:05:06,240 --> 00:05:14,970
the problem of fire bottlenecks

00:05:08,330 --> 00:05:18,060
polishing the benchmark results we had

00:05:14,970 --> 00:05:25,010
some problems immediately from the start

00:05:18,060 --> 00:05:28,170
I I loathe the Dragonfly kernel crashed

00:05:25,010 --> 00:05:32,460
we add some weird birds named cash bug

00:05:28,170 --> 00:05:36,660
for example where some files were

00:05:32,460 --> 00:05:42,000
supposed to exist that couldn't be found

00:05:36,660 --> 00:05:45,840
by PG bench for example these birds were

00:05:42,000 --> 00:05:50,730
fixed quite quickly generally less than

00:05:45,840 --> 00:05:56,300
a day it was mostly a matter of writings

00:05:50,730 --> 00:05:59,900
you write lock directives in the kernel

00:05:56,300 --> 00:06:03,480
we are dead logs in the VM subsystem

00:05:59,900 --> 00:06:10,520
overflows in memory allocation

00:06:03,480 --> 00:06:13,860
subsystems receives all of this was

00:06:10,520 --> 00:06:15,750
generally locking problems and we're

00:06:13,860 --> 00:06:19,410
exposed by the removal of the big

00:06:15,750 --> 00:06:21,460
multiprocessor lock they were relatively

00:06:19,410 --> 00:06:27,420
objective

00:06:21,460 --> 00:06:32,260
so for comparison purpose I'll also use

00:06:27,420 --> 00:06:37,000
scientific Linux which is sort of or

00:06:32,260 --> 00:06:40,510
reddit enterprise derivative so for

00:06:37,000 --> 00:06:40,960
several workloads I felt it was a good

00:06:40,510 --> 00:06:44,830
fit

00:06:40,960 --> 00:06:46,960
we have dragonfly in the bottom the

00:06:44,830 --> 00:06:55,890
stable version and scientific Linux

00:06:46,960 --> 00:06:55,890
which is obviously doing much better we

00:06:55,980 --> 00:07:06,970
immediately found many bottlenecks and

00:07:01,620 --> 00:07:11,170
changes worm read just in the days

00:07:06,970 --> 00:07:14,080
following the first results with pond we

00:07:11,170 --> 00:07:18,630
had another big multiprocessor lock in

00:07:14,080 --> 00:07:22,600
the system 5 semaphore car system 5

00:07:18,630 --> 00:07:24,790
share memory segments are used by post

00:07:22,600 --> 00:07:28,150
rest were used by Postgres in this

00:07:24,790 --> 00:07:32,680
version so this was very important fix

00:07:28,150 --> 00:07:38,080
that subsystem we had bottlenecks in

00:07:32,680 --> 00:07:44,170
select for many performance issues in

00:07:38,080 --> 00:07:46,450
memory allocation path and we had to fix

00:07:44,170 --> 00:07:52,570
the VM subsystems virtual memory

00:07:46,450 --> 00:07:56,820
subsystem to improve the number of page

00:07:52,570 --> 00:07:56,820
faults we could process at one time

00:07:58,560 --> 00:08:04,150
Postgres uses big memory share a big

00:08:02,020 --> 00:08:07,060
share memory segment so it exercises

00:08:04,150 --> 00:08:11,290
memory allocation cut paths virtual

00:08:07,060 --> 00:08:13,210
memory cut passes and well you will keep

00:08:11,290 --> 00:08:15,460
finding problems we will keep finding

00:08:13,210 --> 00:08:18,850
problems in these subsystems for a long

00:08:15,460 --> 00:08:22,810
time so after the first improvements

00:08:18,850 --> 00:08:28,330
dragonfly to the 13 what here so

00:08:22,810 --> 00:08:31,080
intermediary between linux the original

00:08:28,330 --> 00:08:31,080
stable version

00:08:33,360 --> 00:08:40,510
the new development version of I don't

00:08:37,180 --> 00:08:48,040
like could more or less scale according

00:08:40,510 --> 00:08:55,510
to the amount of physical course still

00:08:48,040 --> 00:08:57,430
have problems there sometime damn a few

00:08:55,510 --> 00:09:02,140
years later I had access to an assumed

00:08:57,430 --> 00:09:04,959
you all their machine I decided to run

00:09:02,140 --> 00:09:09,420
new benchmarks with a new post-grad

00:09:04,959 --> 00:09:12,580
fashion new dragonfly questions

00:09:09,420 --> 00:09:15,390
basically I read much mark other people

00:09:12,580 --> 00:09:20,950
and found bottlenecks in the kernel all

00:09:15,390 --> 00:09:25,020
other people fix them or met twigs to

00:09:20,950 --> 00:09:29,020
the kernel and this was constantly

00:09:25,020 --> 00:09:31,360
running process we were communicating on

00:09:29,020 --> 00:09:34,930
IRC male leaf so as not to lose too much

00:09:31,360 --> 00:09:38,470
time but it still took a very long time

00:09:34,930 --> 00:09:40,990
to learn all the benchmarks and do all

00:09:38,470 --> 00:09:45,130
the improvements so at first we had

00:09:40,990 --> 00:09:47,680
dragonfly suite at O which was already

00:09:45,130 --> 00:09:51,970
using the improvements from the previous

00:09:47,680 --> 00:09:54,040
development version but it was much much

00:09:51,970 --> 00:09:57,940
worse than a Linux based operating

00:09:54,040 --> 00:10:00,190
system used as reference and one of the

00:09:57,940 --> 00:10:04,600
problems was the new version of course

00:10:00,190 --> 00:10:06,790
rescue L used a different share memory

00:10:04,600 --> 00:10:11,200
allocation technique it didn't used

00:10:06,790 --> 00:10:18,130
system 5 share memory signals anymore

00:10:11,200 --> 00:10:21,310
first quest no used n map nobody loves

00:10:18,130 --> 00:10:24,160
system 5 share memory many operating

00:10:21,310 --> 00:10:28,060
systems still have default straight from

00:10:24,160 --> 00:10:31,870
Z in 1980s where else you can allocate

00:10:28,060 --> 00:10:35,709
Moore's and a few megabytes of memory by

00:10:31,870 --> 00:10:40,089
default so people have to tune their

00:10:35,709 --> 00:10:41,970
system to use this control possibly

00:10:40,089 --> 00:10:45,209
recompile kernels on some

00:10:41,970 --> 00:10:47,490
some systems and this is a big mess so

00:10:45,209 --> 00:10:49,470
that's why possibly as people are trying

00:10:47,490 --> 00:10:52,860
to use em map by default and move away

00:10:49,470 --> 00:10:56,160
from system-wide share memories after

00:10:52,860 --> 00:11:01,139
some time we still identified mini

00:10:56,160 --> 00:11:04,829
bottle next improved cannot performance

00:11:01,139 --> 00:11:07,490
so dragonfly sweetie to which was a new

00:11:04,829 --> 00:11:10,439
X stable version was much improved and

00:11:07,490 --> 00:11:17,129
more or less and poor with the reference

00:11:10,439 --> 00:11:20,040
Linux based system first details of the

00:11:17,129 --> 00:11:24,480
improvement we are one bright summer of

00:11:20,040 --> 00:11:30,449
coeur student add CPU topology awareness

00:11:24,480 --> 00:11:32,370
to the camel the all bsd scheduler was

00:11:30,449 --> 00:11:33,529
changed to take this information into

00:11:32,370 --> 00:11:36,899
account

00:11:33,529 --> 00:11:39,769
knowing the machine topology is very

00:11:36,899 --> 00:11:43,620
important and new multiprocessor systems

00:11:39,769 --> 00:11:47,839
new MP machines are not symmetric

00:11:43,620 --> 00:11:51,540
anymore they are Numa for sharp

00:11:47,839 --> 00:11:57,949
non-uniform memory access but of the

00:11:51,540 --> 00:12:01,319
memory is allocated to one processor

00:11:57,949 --> 00:12:04,319
where the memory to another one the

00:12:01,319 --> 00:12:09,259
different processors communicate with

00:12:04,319 --> 00:12:14,040
iOS subsystems or memory differently and

00:12:09,259 --> 00:12:17,689
most importantly each processor of each

00:12:14,040 --> 00:12:21,449
core in a process of tsukete as its own

00:12:17,689 --> 00:12:25,379
individual hardware resources so we have

00:12:21,449 --> 00:12:28,709
to know when to migrate processes when

00:12:25,379 --> 00:12:33,269
or when not to make read them so as to

00:12:28,709 --> 00:12:36,899
keep caches hot and this is very very

00:12:33,269 --> 00:12:39,509
important for performance and we found

00:12:36,899 --> 00:12:41,910
out the original be as the scheduler

00:12:39,509 --> 00:12:44,550
even when it adds the topology

00:12:41,910 --> 00:12:48,660
information still add significant

00:12:44,550 --> 00:12:51,970
botanist it itself was single threaded

00:12:48,660 --> 00:12:57,459
and add to be riveted

00:12:51,970 --> 00:13:00,399
so Madeleine Rajan your scheduler she

00:12:57,459 --> 00:13:02,319
Julia schedules processes as close as

00:13:00,399 --> 00:13:05,739
possible to the place where last one on

00:13:02,319 --> 00:13:09,959
to keep catch his hut to keep to also

00:13:05,739 --> 00:13:13,929
keep translation lookaside buffer hat

00:13:09,959 --> 00:13:16,869
TLB is our serve caches for the virtual

00:13:13,929 --> 00:13:19,029
memory aware subsystems so it's very

00:13:16,869 --> 00:13:23,169
important when you use huge amount

00:13:19,029 --> 00:13:27,189
memory to keep sales caches as hot as

00:13:23,169 --> 00:13:28,839
possible we also on the other hand we

00:13:27,189 --> 00:13:31,449
also try to avoid unnecessary

00:13:28,839 --> 00:13:35,829
competition for resources when we have

00:13:31,449 --> 00:13:39,909
to pursue circuits with huge amount

00:13:35,829 --> 00:13:41,499
cache per circuit and only to persuade

00:13:39,909 --> 00:13:44,679
client for example it doesn't make sense

00:13:41,499 --> 00:13:49,059
to runs it to Postgres clients on the

00:13:44,679 --> 00:13:50,229
same circuit it's best to run one on the

00:13:49,059 --> 00:13:53,529
first it's ok

00:13:50,229 --> 00:13:57,419
and the other one is the others ok for

00:13:53,529 --> 00:14:00,429
you doubles the amount of cache memory

00:13:57,419 --> 00:14:02,439
effectively used so there are many

00:14:00,429 --> 00:14:07,479
trade-offs we have to know we have to

00:14:02,439 --> 00:14:10,239
make C is the same kind of problem

00:14:07,479 --> 00:14:14,649
between individual course on each

00:14:10,239 --> 00:14:20,079
processor circuit and I per stress on

00:14:14,649 --> 00:14:23,139
the same CPU core so we have to keep

00:14:20,079 --> 00:14:25,959
caches earth band also globally balance

00:14:23,139 --> 00:14:28,449
the load and try to avoid experts and

00:14:25,959 --> 00:14:33,399
performance bottlenecks only particular

00:14:28,449 --> 00:14:36,129
hardware research works other

00:14:33,399 --> 00:14:40,539
improvements we found out many default

00:14:36,129 --> 00:14:43,329
values were tuned for 32-bit machines in

00:14:40,539 --> 00:14:47,109
particular the amount of file cache

00:14:43,329 --> 00:14:55,869
memories we could use and so on so we

00:14:47,109 --> 00:14:59,849
changed that yeah patron memory was also

00:14:55,869 --> 00:14:59,849
a big problem

00:14:59,890 --> 00:15:08,950
because it itself has to use page tables

00:15:04,420 --> 00:15:13,560
which are in memory descriptive

00:15:08,950 --> 00:15:18,579
resources used to describe where a

00:15:13,560 --> 00:15:21,579
virtual address points to so when you

00:15:18,579 --> 00:15:24,399
seen for example 32 gigabytes of virtual

00:15:21,579 --> 00:15:25,029
memory we have to manage possibly

00:15:24,399 --> 00:15:29,140
androids

00:15:25,029 --> 00:15:32,649
of megabytes even gigabytes of page

00:15:29,140 --> 00:15:34,810
tables just to describe were in the

00:15:32,649 --> 00:15:40,959
physical world these virtual memory

00:15:34,810 --> 00:15:44,769
pages allocated so we add two different

00:15:40,959 --> 00:15:48,370
thing and that something was called P

00:15:44,769 --> 00:15:53,649
map and my optimization basically is you

00:15:48,370 --> 00:15:57,610
cannot tries to keep some of the page

00:15:53,649 --> 00:16:02,350
descriptive tables common between

00:15:57,610 --> 00:16:04,829
different processes I'm not sure if I'm

00:16:02,350 --> 00:16:09,510
very clear on that

00:16:04,829 --> 00:16:14,890
what we try to avoid duplication in the

00:16:09,510 --> 00:16:18,459
vegetables and possibly avoiding the use

00:16:14,890 --> 00:16:21,250
of gigabytes or even tens of gigabytes

00:16:18,459 --> 00:16:25,449
memory when you have a huge process to

00:16:21,250 --> 00:16:27,899
share anymore segment and finally we

00:16:25,449 --> 00:16:32,260
also found out we could directly read

00:16:27,899 --> 00:16:36,360
file data from the VM Kashyap system so

00:16:32,260 --> 00:16:36,360
that's also called red rocket

00:16:37,050 --> 00:16:44,079
this graph shows a few of the

00:16:39,940 --> 00:16:49,899
improvements individually so this was

00:16:44,079 --> 00:16:54,899
the best dragonfly 3-0 performance by

00:16:49,899 --> 00:16:59,010
using share vegetable information we

00:16:54,899 --> 00:17:01,839
already improve performance tremendously

00:16:59,010 --> 00:17:06,069
reading file data directly from the

00:17:01,839 --> 00:17:08,289
virtual memory cache help us improve

00:17:06,069 --> 00:17:12,110
performance

00:17:08,289 --> 00:17:18,019
even better and finally the new

00:17:12,110 --> 00:17:20,980
scheduler is the green curve and gives

00:17:18,019 --> 00:17:35,059
us something which was more or less

00:17:20,980 --> 00:17:36,769
equal to the only next performance we

00:17:35,059 --> 00:17:38,450
also found out there was these

00:17:36,769 --> 00:17:40,700
performance improvements when not was

00:17:38,450 --> 00:17:44,299
very specific in many workloads were

00:17:40,700 --> 00:17:47,720
improved in general the number of

00:17:44,299 --> 00:17:51,409
multiprocessor virtual memory in

00:17:47,720 --> 00:17:54,919
validations was much reduced so the

00:17:51,409 --> 00:17:58,759
Biggers machine the less it has to wait

00:17:54,919 --> 00:18:05,059
for was a processor to process virtual

00:17:58,759 --> 00:18:09,200
memory mapping information file

00:18:05,059 --> 00:18:12,889
operations were much improved with the

00:18:09,200 --> 00:18:16,370
read system call generally is the

00:18:12,889 --> 00:18:19,549
Moore's mission whether did less it had

00:18:16,370 --> 00:18:25,940
to wait so we first brought improvements

00:18:19,549 --> 00:18:28,909
performance and all load I am recently

00:18:25,940 --> 00:18:32,379
at the occasion of running Postgres

00:18:28,909 --> 00:18:36,460
benchmarks again this time it wasn't 40

00:18:32,379 --> 00:18:41,779
Hardware thread machine also duel them

00:18:36,460 --> 00:18:44,059
in marshal this year we didn't really do

00:18:41,779 --> 00:18:45,950
any Postgres specific performance

00:18:44,059 --> 00:18:50,779
improvements this time it was just to

00:18:45,950 --> 00:18:53,000
check if dragonfly was still performing

00:18:50,779 --> 00:18:58,370
adequately with rest and PostgreSQL

00:18:53,000 --> 00:19:01,539
versions we found out with to add some

00:18:58,370 --> 00:19:06,320
improvements but this time the worm

00:19:01,539 --> 00:19:11,019
most likely caused by improvement we had

00:19:06,320 --> 00:19:14,360
to make foreign input here which is the

00:19:11,019 --> 00:19:16,880
FreeBSD best package building system

00:19:14,360 --> 00:19:26,480
created by batiste which is

00:19:16,880 --> 00:19:28,850
yeah this is career is very reserved for

00:19:26,480 --> 00:19:35,470
exact and i/o intensive it really just

00:19:28,850 --> 00:19:40,460
is all kinds of kernel subsystems so um

00:19:35,470 --> 00:19:45,380
we found out we were no better than

00:19:40,460 --> 00:19:47,960
Linux for much of the curve dragon flies

00:19:45,380 --> 00:19:50,420
in green and for reference you have two

00:19:47,960 --> 00:19:55,310
Linux based operating systems Libyan and

00:19:50,420 --> 00:19:58,400
centers dragonfly is faster than Linux

00:19:55,310 --> 00:20:01,720
and scales better than Linux as long as

00:19:58,400 --> 00:20:03,740
you have available hardware resources

00:20:01,720 --> 00:20:06,680
performance you've really shared bit

00:20:03,740 --> 00:20:09,440
more severe than Linux once you have

00:20:06,680 --> 00:20:18,020
more progress plans than are where

00:20:09,440 --> 00:20:22,040
resources but this is known we are well

00:20:18,020 --> 00:20:29,750
we wanted to to keep interactive

00:20:22,040 --> 00:20:32,260
performance to najib read too much

00:20:29,750 --> 00:20:36,260
interactive performance and a lot so

00:20:32,260 --> 00:20:44,050
this most severe degradation the Linux

00:20:36,260 --> 00:20:47,300
once our resources are gone

00:20:44,050 --> 00:20:50,570
avoid well it's a consequence of wanting

00:20:47,300 --> 00:20:52,700
to avoid waiting for fifteen seconds

00:20:50,570 --> 00:20:55,640
after you have typed a key for example

00:20:52,700 --> 00:20:58,750
and see the results on the screen so

00:20:55,640 --> 00:20:58,750
this is a compromise

00:20:59,680 --> 00:21:22,490
so I'm don't you have any questions so

00:21:19,790 --> 00:21:24,320
the swap cash that is more of a file

00:21:22,490 --> 00:21:26,840
system cache so that when you have a

00:21:24,320 --> 00:21:28,340
spinning disk instead of reading it

00:21:26,840 --> 00:21:30,650
again from the spinning disk you

00:21:28,340 --> 00:21:33,679
actually read it from the SSD as opposed

00:21:30,650 --> 00:21:35,630
to just put in swap on it correct is

00:21:33,679 --> 00:21:38,150
that more or less the idea well a swept

00:21:35,630 --> 00:21:40,970
cache is a sort of signal level five

00:21:38,150 --> 00:21:46,160
cache and it uses the swept

00:21:40,970 --> 00:21:52,640
infrastructure it really puts cache data

00:21:46,160 --> 00:21:56,990
on in the swept area it can be used with

00:21:52,640 --> 00:22:00,080
regular harvest you're really the spirit

00:21:56,990 --> 00:22:04,160
you can improve performance a bit like

00:22:00,080 --> 00:22:10,570
if you had more discs in a red pool but

00:22:04,160 --> 00:22:15,080
it's really optimized for SSD it has

00:22:10,570 --> 00:22:18,170
right amount a get tries to not write

00:22:15,080 --> 00:22:25,370
too much data at the same time so as not

00:22:18,170 --> 00:22:28,400
to wear out the SSD and once the cache

00:22:25,370 --> 00:22:32,590
has been populated it starts to read

00:22:28,400 --> 00:22:37,190
back pages directly from this cash

00:22:32,590 --> 00:22:39,980
instantly so yeah the idea is really to

00:22:37,190 --> 00:22:45,530
use it more as a rate cache and not to

00:22:39,980 --> 00:22:47,780
wear out SSDs you can have and read of

00:22:45,530 --> 00:22:51,730
gigabytes of second level file cache in

00:22:47,780 --> 00:23:01,010
that way so kind of similar to the l2r

00:22:51,730 --> 00:23:04,070
but well it's yeah and well CFS that

00:23:01,010 --> 00:23:08,990
things differently if I'm not mistaken

00:23:04,070 --> 00:23:12,500
ZFS uses SSDs for white kitchen

00:23:08,990 --> 00:23:15,110
so ZFS has both l2 arc which is

00:23:12,500 --> 00:23:17,029
effectively a read cache for the main

00:23:15,110 --> 00:23:19,580
data set but then there's also the Zil

00:23:17,029 --> 00:23:21,140
which is the intent log writing which is

00:23:19,580 --> 00:23:24,789
separate so you actually have both

00:23:21,140 --> 00:23:29,260
effectively a write cache memory cache

00:23:24,789 --> 00:23:33,590
yeah so that point is mostly a real cash

00:23:29,260 --> 00:23:36,440
you can do almost everything with it we

00:23:33,590 --> 00:23:44,539
have specific file attributes you can

00:23:36,440 --> 00:23:47,659
use too if you want to decide to put in

00:23:44,539 --> 00:23:50,450
cache only a subdirectory your key you

00:23:47,659 --> 00:23:54,559
add this file attribute to the directory

00:23:50,450 --> 00:23:58,820
you want to cache you can try to only

00:23:54,559 --> 00:24:02,270
cache file metadata you can cache file

00:23:58,820 --> 00:24:04,399
concerns well all kinds of variations

00:24:02,270 --> 00:24:09,789
are possible and this is controlled by

00:24:04,399 --> 00:24:09,789
sis controls and file attribute

00:24:16,120 --> 00:24:20,960
so I'm going to get a bit sidetrack

00:24:18,769 --> 00:24:23,600
obviously you mentioned food we have our

00:24:20,960 --> 00:24:25,490
clothes did you notice anything really

00:24:23,600 --> 00:24:27,440
interesting like I know that it's all

00:24:25,490 --> 00:24:35,629
over the place but it's about something

00:24:27,440 --> 00:24:38,899
which is more just to optimize well

00:24:35,629 --> 00:24:42,830
problem is food we are exercised all

00:24:38,899 --> 00:24:47,299
kinds of kernel subsystems at the same

00:24:42,830 --> 00:24:50,720
time and we had really really wet burbs

00:24:47,299 --> 00:24:54,559
for example we had a rest condition in

00:24:50,720 --> 00:24:58,539
the tty subsystem just by trying to do a

00:24:54,559 --> 00:25:01,549
LS or print what was happening on the

00:24:58,539 --> 00:25:07,309
screen at the same time per year and we

00:25:01,549 --> 00:25:10,399
are a cannot panic yeah and I think most

00:25:07,309 --> 00:25:15,379
canons have these kinds of bug suburbs

00:25:10,399 --> 00:25:18,409
but only put reader was able to exercise

00:25:15,379 --> 00:25:22,759
so many subsistence at the same time so

00:25:18,409 --> 00:25:24,259
as to make them obvious most of the

00:25:22,759 --> 00:25:30,250
problems weren't in virtual memory

00:25:24,259 --> 00:25:35,090
system the i/o subsystem by reading

00:25:30,250 --> 00:25:43,129
mostly winning directory access name

00:25:35,090 --> 00:25:44,659
cache multiprocessor by itself was

00:25:43,129 --> 00:25:49,120
already a problem having so many

00:25:44,659 --> 00:25:52,519
processes run at the same time we are

00:25:49,120 --> 00:25:57,460
locking issues in some part of the

00:25:52,519 --> 00:26:03,820
camera like really exercise system calls

00:25:57,460 --> 00:26:06,500
for example well we had read I think we

00:26:03,820 --> 00:26:10,460
select or some common sets of terms

00:26:06,500 --> 00:26:13,159
which are looking problems but no

00:26:10,460 --> 00:26:16,090
program was able to make them obvious

00:26:13,159 --> 00:26:16,090
before probably

00:26:19,600 --> 00:26:25,400
which was everyone and the bulk of the

00:26:23,450 --> 00:26:28,240
bags and the shoes were in the virtual

00:26:25,400 --> 00:26:39,950
memory and name cache subsystem if I

00:26:28,240 --> 00:26:42,770
remember correctly so I know that this

00:26:39,950 --> 00:26:45,470
kind of benchmarking is quite time

00:26:42,770 --> 00:26:48,200
consuming and requires a lot of effort

00:26:45,470 --> 00:26:52,460
so I have a two-part questions the first

00:26:48,200 --> 00:26:56,240
part is I'm wondering if you intend to

00:26:52,460 --> 00:26:59,540
do another round of investigation of

00:26:56,240 --> 00:27:04,190
Postgres specifically at some point in

00:26:59,540 --> 00:27:08,210
the future and secondly have you given

00:27:04,190 --> 00:27:10,309
thought to methods of automating this

00:27:08,210 --> 00:27:13,100
sort of benchmarking or some sort of way

00:27:10,309 --> 00:27:15,650
that it could be run on an ongoing basis

00:27:13,100 --> 00:27:19,340
because I think one of the tricks is its

00:27:15,650 --> 00:27:20,750
it may not be if you're not directly

00:27:19,340 --> 00:27:22,760
following an upstream project like

00:27:20,750 --> 00:27:25,130
Postgres in this case when the men when

00:27:22,760 --> 00:27:28,100
the EM maps which changed it may not be

00:27:25,130 --> 00:27:30,890
immediately obvious oh 9.2 to 9.3 maybe

00:27:28,100 --> 00:27:32,630
an important point to do a huge

00:27:30,890 --> 00:27:37,070
benchmarking run right so I I'm not sure

00:27:32,630 --> 00:27:38,990
how how to effectively find out when

00:27:37,070 --> 00:27:42,169
these sorts of regressions happen

00:27:38,990 --> 00:27:42,500
against upstream projects yeah that's a

00:27:42,169 --> 00:27:44,870
problem

00:27:42,500 --> 00:27:47,990
well actually automating this benchmark

00:27:44,870 --> 00:27:52,580
and running continuously is not

00:27:47,990 --> 00:27:58,120
difficult already automated it when I

00:27:52,580 --> 00:27:58,120
ran when I collected the data points

00:27:58,510 --> 00:28:07,190
each point was made from three different

00:28:01,610 --> 00:28:09,530
measures and I had to round the

00:28:07,190 --> 00:28:11,510
benchmark many times at least three

00:28:09,530 --> 00:28:16,580
times for each operating system and each

00:28:11,510 --> 00:28:26,510
point so I created shell script and ran

00:28:16,580 --> 00:28:27,669
it but madly for for the entire except

00:28:26,510 --> 00:28:32,210
this

00:28:27,669 --> 00:28:33,950
x-axis yeah so automating this kind of

00:28:32,210 --> 00:28:36,380
benchmarks in they're difficult the

00:28:33,950 --> 00:28:40,789
problem is more about big machines

00:28:36,380 --> 00:28:43,039
availability every time I ran the

00:28:40,789 --> 00:28:45,710
mishmash this benchmark I added well

00:28:43,039 --> 00:28:48,710
seein system available for at least a

00:28:45,710 --> 00:28:54,620
week before it could be put into

00:28:48,710 --> 00:28:58,909
production so that was really an

00:28:54,620 --> 00:29:03,679
opportunity we could do run this kind of

00:28:58,909 --> 00:29:07,269
benchmark continuously but we had to buy

00:29:03,679 --> 00:29:11,210
a machine especially dedicated for it

00:29:07,269 --> 00:29:12,649
okay so is the but the installation of

00:29:11,210 --> 00:29:14,679
the OSS and everything was still

00:29:12,649 --> 00:29:16,549
relatively manual for your testing

00:29:14,679 --> 00:29:19,039
installation of the u.s. is not a

00:29:16,549 --> 00:29:22,970
problem for we can prepare a disk and

00:29:19,039 --> 00:29:27,350
just plug it into the machine when you

00:29:22,970 --> 00:29:30,519
want to run the benchmark so this can be

00:29:27,350 --> 00:29:30,519
done offline no way

00:29:36,120 --> 00:29:43,669
yeah part of the reason and only run

00:29:39,090 --> 00:29:43,669
read-only benchmarks yes

00:29:44,330 --> 00:29:51,980
trying to use discs for measuring

00:29:48,960 --> 00:30:00,090
PostgreSQL rien right these performance

00:29:51,980 --> 00:30:07,400
would have taken much much more time any

00:30:00,090 --> 00:30:13,579
more questions thank you

00:30:07,400 --> 00:30:13,579

YouTube URL: https://www.youtube.com/watch?v=jLMeK-sTymM


