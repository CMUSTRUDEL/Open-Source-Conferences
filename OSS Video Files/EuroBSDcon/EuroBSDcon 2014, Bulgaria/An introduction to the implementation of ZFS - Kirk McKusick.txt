Title: An introduction to the implementation of ZFS - Kirk McKusick
Publication date: 2019-10-14
Playlist: EuroBSDcon 2014, Bulgaria
Description: 
	Abstract:

Much has been documented about how to use ZFS, but little has been written about how it is implemented. This talk pulls back the covers to describe the design and implementation of ZFS. The content of this talk was developed by scouring through blog posts, tracking down unpublished papers, hours of reading through the quarter-million lines of code that implement ZFS, and endless email with the ZFS developers themselves. The result is a concise description of an elegant and powerful system.

Speaker biography:

Dr. Marshall Kirk McKusickâ€™s work with Unix and BSD development spans over four decades. It begins with his first paper on the implementation of Berkeley Pascal in 1979, goes on to his pioneering work in the eighties on the BSD Fast File System, the BSD virtual memory system, the final release of 4.4BSD-Lite from the UC Berkeley Computer Systems Research Group, and carries on with his work on FreeBSD. A key figure in Unix and BSD development, his experiences chronicle not only the innovative technical achievements but also the interesting personalities and philosophical debates in Unix over the past thirty-five years.
Captions: 
	00:00:08,170 --> 00:00:14,110
so I love everybody I don't think I need

00:00:11,830 --> 00:00:17,650
to really introduce you to Kirk but

00:00:14,110 --> 00:00:21,340
there it is maybe and traditions to

00:00:17,650 --> 00:00:23,619
Marshall makuu sick by deficient DBS D

00:00:21,340 --> 00:00:27,250
hacker is going to talk to you about the

00:00:23,619 --> 00:00:32,079
design and implementation of DFS thank

00:00:27,250 --> 00:00:33,579
you so we were working on this little

00:00:32,079 --> 00:00:37,450
book thing that you probably know about

00:00:33,579 --> 00:00:38,980
and I had originally thought that ZFS

00:00:37,450 --> 00:00:40,780
was just going to be sort of a section

00:00:38,980 --> 00:00:43,870
in the chapter that talks about file

00:00:40,780 --> 00:00:46,149
systems and then at some point one of my

00:00:43,870 --> 00:00:50,530
reviewers looked at it and said you only

00:00:46,149 --> 00:00:52,600
have six pages on ZFS that's not enough

00:00:50,530 --> 00:00:54,670
and I thought about it for a little bit

00:00:52,600 --> 00:00:56,140
and I thought yeah you're you're right

00:00:54,670 --> 00:00:58,390
about that I guess I have to do a whole

00:00:56,140 --> 00:01:00,760
chapter on it and so we reorganized the

00:00:58,390 --> 00:01:03,429
way the whole book was done and there I

00:01:00,760 --> 00:01:07,180
was staring at a chapter than all of the

00:01:03,429 --> 00:01:09,190
texts that I had in it was the title so

00:01:07,180 --> 00:01:11,170
how do you go about actually figuring

00:01:09,190 --> 00:01:12,700
this out well there's got to be all

00:01:11,170 --> 00:01:14,020
kinds of documentation about I mean

00:01:12,700 --> 00:01:17,320
there's tons of stuff out there about

00:01:14,020 --> 00:01:19,659
ZFS but it's all about how to use a ZFS

00:01:17,320 --> 00:01:24,939
and not about how it's actually put

00:01:19,659 --> 00:01:26,860
together and so I found a paper that had

00:01:24,939 --> 00:01:28,630
been written for the Lisa conference but

00:01:26,860 --> 00:01:30,340
it hadn't been accepted and so there

00:01:28,630 --> 00:01:33,820
were some sort of copies of it floating

00:01:30,340 --> 00:01:36,820
around there's some slide decks that had

00:01:33,820 --> 00:01:39,189
been used to talk to customers about it

00:01:36,820 --> 00:01:40,719
and then I finally found the resources

00:01:39,189 --> 00:01:42,729
that I really needed which was Matt

00:01:40,719 --> 00:01:45,219
Aaron's who would actually answer my

00:01:42,729 --> 00:01:46,630
emails and actually if I bought him

00:01:45,219 --> 00:01:48,850
lunch would sit down and we could

00:01:46,630 --> 00:01:50,530
actually I can show them prototypes of

00:01:48,850 --> 00:01:51,880
things and then of course there's the

00:01:50,530 --> 00:01:53,350
quarter million lines of code that you

00:01:51,880 --> 00:01:55,149
can read if you really want to figure

00:01:53,350 --> 00:01:57,159
out how things work so I would try and

00:01:55,149 --> 00:01:58,960
read the code and I'd come to this lunch

00:01:57,159 --> 00:02:00,310
and Matt would say yeah that's an

00:01:58,960 --> 00:02:02,979
interesting way of thinking of it but no

00:02:00,310 --> 00:02:05,200
that's not the way it works so really

00:02:02,979 --> 00:02:08,710
that chapter came about because Matt

00:02:05,200 --> 00:02:11,620
basically led me through and and made it

00:02:08,710 --> 00:02:15,040
happen and so a great debt to him for

00:02:11,620 --> 00:02:17,860
doing that at any rate this according to

00:02:15,040 --> 00:02:19,840
Matt is the only complete documentation

00:02:17,860 --> 00:02:21,790
on how ZFS works in considering it's

00:02:19,840 --> 00:02:24,750
only a 30 page chapter

00:02:21,790 --> 00:02:28,480
is obviously just a brief introduction

00:02:24,750 --> 00:02:32,319
so anyway I then had to put together

00:02:28,480 --> 00:02:34,540
some slides for my tutorial and in doing

00:02:32,319 --> 00:02:37,150
that I said well I can just take some of

00:02:34,540 --> 00:02:38,800
those for this talk and I did that and

00:02:37,150 --> 00:02:40,569
then I did a dry run of the talk and it

00:02:38,800 --> 00:02:42,970
went on for an hour and 15 minutes

00:02:40,569 --> 00:02:44,440
so last night I whacked out four more

00:02:42,970 --> 00:02:47,680
slides and I'll try and go through it a

00:02:44,440 --> 00:02:51,269
little more quickly than my dry run so

00:02:47,680 --> 00:02:54,790
let me just give you the overview of ZFS

00:02:51,269 --> 00:02:57,580
the basic idea is that it's this sort of

00:02:54,790 --> 00:03:00,190
new generation of file system technology

00:02:57,580 --> 00:03:02,980
which is the never overwriting then the

00:03:00,190 --> 00:03:05,920
copy-on-write if you will file system so

00:03:02,980 --> 00:03:07,569
once something gets written you don't go

00:03:05,920 --> 00:03:09,100
back and change it if you have a

00:03:07,569 --> 00:03:11,950
traditional file system like the fast

00:03:09,100 --> 00:03:13,630
file system you you you know seek to a

00:03:11,950 --> 00:03:15,489
point in a file and you write we just

00:03:13,630 --> 00:03:16,959
read the old thing in change the bytes

00:03:15,489 --> 00:03:18,430
you want to change and write it right

00:03:16,959 --> 00:03:22,000
back to where it came from

00:03:18,430 --> 00:03:24,640
so that you update whereas in ZFS as

00:03:22,000 --> 00:03:27,910
you'll see if you if you modify an

00:03:24,640 --> 00:03:29,980
existing file that block that modified

00:03:27,910 --> 00:03:32,440
block will be placed somewhere else and

00:03:29,980 --> 00:03:35,890
then the inode will be changed to go

00:03:32,440 --> 00:03:38,859
point to that new copy of the block all

00:03:35,890 --> 00:03:40,630
right because it's a non overrating

00:03:38,859 --> 00:03:41,950
filesystem you don't have the problems

00:03:40,630 --> 00:03:43,540
that you do with the traditional file

00:03:41,950 --> 00:03:46,120
systems where it can become inconsistent

00:03:43,540 --> 00:03:49,750
you've updated some things but not other

00:03:46,120 --> 00:03:52,269
things and so the file system has to be

00:03:49,750 --> 00:03:55,450
either rolled back using a log or a

00:03:52,269 --> 00:03:57,880
journal or fsck or whatever your poison

00:03:55,450 --> 00:04:00,579
is that you happen to like in the case

00:03:57,880 --> 00:04:02,680
of a non overwriting file system it is

00:04:00,579 --> 00:04:04,329
always consistent because if you have a

00:04:02,680 --> 00:04:07,420
consistent version of the file system

00:04:04,329 --> 00:04:09,220
and now you're going to move from one

00:04:07,420 --> 00:04:10,750
consistent version to another so what

00:04:09,220 --> 00:04:14,019
will happen is you'll write everything

00:04:10,750 --> 00:04:17,620
out that needs to be changed and the

00:04:14,019 --> 00:04:19,239
last step is that you just you actually

00:04:17,620 --> 00:04:22,600
create the checkpoint then move forward

00:04:19,239 --> 00:04:24,070
by just writing that a new uber block in

00:04:22,600 --> 00:04:27,820
this case think of it as sort of a

00:04:24,070 --> 00:04:29,530
glorified super block and so either that

00:04:27,820 --> 00:04:31,300
rate has occurred or it hasn't if it

00:04:29,530 --> 00:04:32,860
hasn't yet occurred then you have the

00:04:31,300 --> 00:04:34,150
old consistent version and if the rate

00:04:32,860 --> 00:04:36,130
is completed then you

00:04:34,150 --> 00:04:37,600
the new consistent version but you're

00:04:36,130 --> 00:04:40,150
never at a point where the file system

00:04:37,600 --> 00:04:42,190
is inconsistent okay so you just

00:04:40,150 --> 00:04:46,139
atomically step forward checkpoint by

00:04:42,190 --> 00:04:49,300
checkpoint it has things like snapshots

00:04:46,139 --> 00:04:51,760
which are read-only clones which are

00:04:49,300 --> 00:04:53,199
read/write so if you want to clone you

00:04:51,760 --> 00:04:56,050
have to take a snapshot and then you

00:04:53,199 --> 00:04:59,800
make a clone of the snapshot and you can

00:04:56,050 --> 00:05:01,660
then you know modify away and if you

00:04:59,800 --> 00:05:03,490
know one of the common uses for clones

00:05:01,660 --> 00:05:05,949
is that you'll make a clone and do an up

00:05:03,490 --> 00:05:07,870
update or an upgrade of the system and

00:05:05,949 --> 00:05:09,729
if it all works out then you just say

00:05:07,870 --> 00:05:10,930
alright that's now the file system if it

00:05:09,729 --> 00:05:12,479
doesn't work out you say yep just to

00:05:10,930 --> 00:05:15,910
throw it away let me try again

00:05:12,479 --> 00:05:18,100
okay with a non overwriting file system

00:05:15,910 --> 00:05:20,530
is really easy to do snapshots it's

00:05:18,100 --> 00:05:23,320
really easy to do clones you can have

00:05:20,530 --> 00:05:25,060
you know piles of them in ZFS there are

00:05:23,320 --> 00:05:29,949
no limits other than the amount of disk

00:05:25,060 --> 00:05:32,860
space you have to throw at them ZFS has

00:05:29,949 --> 00:05:34,630
a lot of metadata redundancy data check

00:05:32,860 --> 00:05:37,120
sums we'll see that when we talk about

00:05:34,630 --> 00:05:40,210
the way the block pointers are

00:05:37,120 --> 00:05:44,080
implemented we have selective data

00:05:40,210 --> 00:05:47,680
compression and deduplication data

00:05:44,080 --> 00:05:49,570
compression and deduplication requires a

00:05:47,680 --> 00:05:51,729
lot of memory to hold the d dupe table

00:05:49,570 --> 00:05:54,990
if you just make the whole file system

00:05:51,729 --> 00:05:57,940
you know the whole file system

00:05:54,990 --> 00:06:01,630
deduplicated you can often blow out your

00:05:57,940 --> 00:06:03,669
memory and it gets very slow so ZFS

00:06:01,630 --> 00:06:05,770
gives you the ability to selectively say

00:06:03,669 --> 00:06:09,460
what's being deduplicated what's being

00:06:05,770 --> 00:06:12,910
compressed and you can just do it for

00:06:09,460 --> 00:06:14,860
the things where it makes sense unlike a

00:06:12,910 --> 00:06:16,090
traditional file system where you give

00:06:14,860 --> 00:06:18,310
it a certain amount of space when you

00:06:16,090 --> 00:06:21,130
create the file system and that's it

00:06:18,310 --> 00:06:23,949
with ZFS do you have a pool of blocks

00:06:21,130 --> 00:06:26,110
and that pool blocks is shared among all

00:06:23,949 --> 00:06:31,349
the file systems and clones and so on

00:06:26,110 --> 00:06:34,419
that are running in that pool you have

00:06:31,349 --> 00:06:37,810
mirroring and also single double and

00:06:34,419 --> 00:06:39,490
triple parity raid I don't have time to

00:06:37,810 --> 00:06:41,280
pull the raid stuff because it just

00:06:39,490 --> 00:06:45,070
takes too long to explain

00:06:41,280 --> 00:06:47,110
unfortunately but it's in the book space

00:06:45,070 --> 00:06:48,010
management you can put quotas on on

00:06:47,110 --> 00:06:50,470
users

00:06:48,010 --> 00:06:52,900
and you can also reserve space you can

00:06:50,470 --> 00:06:54,810
say make sure that there is this amount

00:06:52,900 --> 00:06:58,090
of space available to this file system

00:06:54,810 --> 00:06:59,530
so that you don't have like one file

00:06:58,090 --> 00:07:02,370
system go crazy on you and then

00:06:59,530 --> 00:07:04,960
everybody else dies a horrible death and

00:07:02,370 --> 00:07:06,790
there's fast remote replication and

00:07:04,960 --> 00:07:09,340
backups which I also won't talk about

00:07:06,790 --> 00:07:12,660
all right so let's start with the

00:07:09,340 --> 00:07:14,470
structural organization there's sort of

00:07:12,660 --> 00:07:18,970
let's come on now

00:07:14,470 --> 00:07:22,810
you were working just a minute there we

00:07:18,970 --> 00:07:25,210
go okay just whack it okay so we have

00:07:22,810 --> 00:07:27,370
these sort of two main layers here we

00:07:25,210 --> 00:07:30,100
have what's called the meta object set

00:07:27,370 --> 00:07:31,780
layer and the object set layer the meta

00:07:30,100 --> 00:07:34,000
object set layer this is the thing that

00:07:31,780 --> 00:07:36,190
essentially is the pool so in

00:07:34,000 --> 00:07:38,560
traditional file systems all the block

00:07:36,190 --> 00:07:40,540
allocation and space map management's

00:07:38,560 --> 00:07:42,130
done down by the file system here the

00:07:40,540 --> 00:07:43,750
file systems don't mess with that stuff

00:07:42,130 --> 00:07:46,660
when they need a block they just come up

00:07:43,750 --> 00:07:48,220
to the pool here and say give me a block

00:07:46,660 --> 00:07:49,660
and they use it and when they're done

00:07:48,220 --> 00:07:54,580
with it they just hand it back to the

00:07:49,660 --> 00:07:57,550
pool and so at the top of all this is an

00:07:54,580 --> 00:07:59,110
uber block and that's the thing that's

00:07:57,550 --> 00:08:00,670
actually taking the checkpoint so when

00:07:59,110 --> 00:08:03,100
we talk about moving from a consistent

00:08:00,670 --> 00:08:05,980
state to from one to another

00:08:03,100 --> 00:08:08,230
we're in fact not getting just a stay

00:08:05,980 --> 00:08:10,390
consistent State for a particular file

00:08:08,230 --> 00:08:12,970
system you get a consistent state for

00:08:10,390 --> 00:08:15,220
that every file system every clone every

00:08:12,970 --> 00:08:17,470
everything that's in the pool so what

00:08:15,220 --> 00:08:19,270
will happen is in essence when we get

00:08:17,470 --> 00:08:21,310
ready to take a checkpoint we will go to

00:08:19,270 --> 00:08:23,470
each file system have them write out the

00:08:21,310 --> 00:08:25,330
stuff that they need to write out and

00:08:23,470 --> 00:08:28,240
when they've all when all the things in

00:08:25,330 --> 00:08:30,130
the pool have done that then we write

00:08:28,240 --> 00:08:32,410
out anything that then has to be updated

00:08:30,130 --> 00:08:34,450
in the pool and then finally we update

00:08:32,410 --> 00:08:36,940
the uber block at the top and that's

00:08:34,450 --> 00:08:38,229
when the checkpoint actually happens and

00:08:36,940 --> 00:08:40,870
again I'm going to show you a little

00:08:38,229 --> 00:08:43,720
example of how all that happens there

00:08:40,870 --> 00:08:45,790
okay so in these pictures when you see

00:08:43,720 --> 00:08:48,280
something that's just an arrow that is a

00:08:45,790 --> 00:08:49,710
single block pointer and when you see

00:08:48,280 --> 00:08:54,700
something that's one of these triangles

00:08:49,710 --> 00:08:57,250
that is a set of blocks and indirect

00:08:54,700 --> 00:09:00,520
blocks and so on so all of these things

00:08:57,250 --> 00:09:01,329
that have the triangles on them are can

00:09:00,520 --> 00:09:04,079
grow sort of are

00:09:01,329 --> 00:09:07,029
fairly large just think of it it really

00:09:04,079 --> 00:09:08,739
sort of like an inode that allows files

00:09:07,029 --> 00:09:11,949
to get arbitrarily large you just keep

00:09:08,739 --> 00:09:14,139
adding indirect block pointers and until

00:09:11,949 --> 00:09:17,889
you have enough to map out whatever you

00:09:14,139 --> 00:09:21,519
want so and then an object set is is the

00:09:17,889 --> 00:09:24,790
thing that sort of describes is used to

00:09:21,519 --> 00:09:27,939
describe whatever this object is that's

00:09:24,790 --> 00:09:30,790
being drawn okay so at the meta object

00:09:27,939 --> 00:09:33,970
layer we've got this this file if you

00:09:30,790 --> 00:09:36,459
will or this set of things the first

00:09:33,970 --> 00:09:39,249
thing in there is always the master and

00:09:36,459 --> 00:09:41,139
that's where we store various properties

00:09:39,249 --> 00:09:44,110
about whatever the object is so you'll

00:09:41,139 --> 00:09:46,509
see pretty much always we always have a

00:09:44,110 --> 00:09:48,100
master thing at the front to store

00:09:46,509 --> 00:09:50,730
properties and that's things like for a

00:09:48,100 --> 00:09:52,959
file system where is it mounted and

00:09:50,730 --> 00:09:57,369
things about how the privileges are

00:09:52,959 --> 00:10:01,059
being managed etc then here you can see

00:09:57,369 --> 00:10:05,259
each of the things in this is some

00:10:01,059 --> 00:10:10,989
particular underlying either a snapshot

00:10:05,259 --> 00:10:14,709
or file system or clone or as evil as if

00:10:10,989 --> 00:10:18,399
all is it looks sort of like a raw disk

00:10:14,709 --> 00:10:21,040
partition okay and then finally at the

00:10:18,399 --> 00:10:23,860
very end we have the space map and the

00:10:21,040 --> 00:10:25,779
space map is keeping track it's think of

00:10:23,860 --> 00:10:28,389
it as a big bitmap with one bit per

00:10:25,779 --> 00:10:32,169
block so it's it's where we're keeping

00:10:28,389 --> 00:10:33,910
track of what space is available okay so

00:10:32,169 --> 00:10:36,249
in this particular example here I've

00:10:33,910 --> 00:10:38,790
taken a file system so this again is

00:10:36,249 --> 00:10:42,220
just a pointer to another object set and

00:10:38,790 --> 00:10:45,160
this this thing the object set that we

00:10:42,220 --> 00:10:47,709
use for a file system is essentially a

00:10:45,160 --> 00:10:50,199
set of I nodes so if you just think of

00:10:47,709 --> 00:10:52,929
it logically we've got all the eye nodes

00:10:50,199 --> 00:10:55,209
that make up the file system and they're

00:10:52,929 --> 00:10:57,790
just in an array so you just index into

00:10:55,209 --> 00:11:02,169
this array so here we have directory

00:10:57,790 --> 00:11:04,679
file a symbolic link etc and for a file

00:11:02,169 --> 00:11:07,929
of course it's going to have set of

00:11:04,679 --> 00:11:11,009
indirect block pointers to describe the

00:11:07,929 --> 00:11:14,050
data that's the contents of the file

00:11:11,009 --> 00:11:16,630
okay so uber block anchors

00:11:14,050 --> 00:11:19,480
Poole the meta object layer here is

00:11:16,630 --> 00:11:21,279
going to have an array of all of the

00:11:19,480 --> 00:11:24,820
objects all the file systems clone

00:11:21,279 --> 00:11:28,899
snapshots and so on then each of the of

00:11:24,820 --> 00:11:31,630
the objects in here references the an

00:11:28,899 --> 00:11:34,120
object set which describes its set of

00:11:31,630 --> 00:11:35,800
whatever it is it holds so in case of a

00:11:34,120 --> 00:11:37,630
file system all the eye nodes that

00:11:35,800 --> 00:11:41,589
reference all the bits and pieces that

00:11:37,630 --> 00:11:48,010
make up that file system alright

00:11:41,589 --> 00:11:52,959
so block pointers in when we went from

00:11:48,010 --> 00:11:57,100
ffs 1 to ffs 2 this is where we realized

00:11:52,959 --> 00:11:59,079
that 32 bit block pointers weren't big

00:11:57,100 --> 00:12:03,579
enough to deal with large discs and so

00:11:59,079 --> 00:12:07,000
we heaven forbid went to 8 byte 64 bit

00:12:03,579 --> 00:12:10,510
block pointers we have nothing over

00:12:07,000 --> 00:12:12,700
block pointers in ZFS the ZFS block

00:12:10,510 --> 00:12:18,790
pointers are the Titanic of block

00:12:12,700 --> 00:12:21,029
pointers 256 bytes of block pointer this

00:12:18,790 --> 00:12:23,920
brief block pointer in the file system

00:12:21,029 --> 00:12:26,680
ok so what can we possibly do with all

00:12:23,920 --> 00:12:29,680
that stuff well the first thing you get

00:12:26,680 --> 00:12:32,020
here is this is where the redundancy can

00:12:29,680 --> 00:12:34,870
come in because you see we have 3

00:12:32,020 --> 00:12:39,940
different potentially up to 3 different

00:12:34,870 --> 00:12:42,730
pointers to something on a disk so a a

00:12:39,940 --> 00:12:46,149
reference to the disk here we first of

00:12:42,730 --> 00:12:47,770
all have a 64 bit offset actually 63

00:12:46,149 --> 00:12:49,480
bits because there's this thing called a

00:12:47,770 --> 00:12:51,940
gang block if the disk gets too

00:12:49,480 --> 00:12:53,829
fragmented we don't actually have it all

00:12:51,940 --> 00:12:56,829
in one place then it's it's made up of

00:12:53,829 --> 00:13:00,130
some smaller pieces we have the device

00:12:56,829 --> 00:13:06,459
on which it resides grid is just saved

00:13:00,130 --> 00:13:08,529
not currently used and the the size the

00:13:06,459 --> 00:13:13,540
size of the the thing that we're

00:13:08,529 --> 00:13:15,399
pointing at out on the on the disk this

00:13:13,540 --> 00:13:18,399
this you'll notice there's three sizes a

00:13:15,399 --> 00:13:20,500
sized piece size and L size a size is

00:13:18,399 --> 00:13:23,920
how much actual disk space is being used

00:13:20,500 --> 00:13:25,899
including for example any raid parity

00:13:23,920 --> 00:13:27,430
blocks and you know all the other stuff

00:13:25,899 --> 00:13:29,230
that we need

00:13:27,430 --> 00:13:32,440
or you know how much actual physical

00:13:29,230 --> 00:13:35,260
disk are we using and so if we're gonna

00:13:32,440 --> 00:13:39,520
have redundancy and we by default any

00:13:35,260 --> 00:13:42,640
metadata and ZFS has at least two copies

00:13:39,520 --> 00:13:44,410
made so it an indirect block will always

00:13:42,640 --> 00:13:46,420
have at least two copies of that

00:13:44,410 --> 00:13:48,820
indirect block and you can crank it up

00:13:46,420 --> 00:13:51,490
to three if you want so the first copy

00:13:48,820 --> 00:13:53,649
is referenced by this first one the

00:13:51,490 --> 00:13:55,779
second copy is referenced by the second

00:13:53,649 --> 00:13:58,740
one and if there's a third copy it will

00:13:55,779 --> 00:14:03,130
be referenced by the third one here and

00:13:58,740 --> 00:14:05,080
ZFS carefully tries to make sure that if

00:14:03,130 --> 00:14:06,940
you have multiple copies and and if you

00:14:05,080 --> 00:14:09,480
in fact have multiple disks that make up

00:14:06,940 --> 00:14:14,170
the pool that they will be on different

00:14:09,480 --> 00:14:17,950
media different discs so that if one

00:14:14,170 --> 00:14:19,209
particular disc goes down that you'll be

00:14:17,950 --> 00:14:24,940
able to pick it up off of one of the

00:14:19,209 --> 00:14:28,360
other discs okay level just tells us

00:14:24,940 --> 00:14:30,250
where we are in the in if what level of

00:14:28,360 --> 00:14:35,709
indirect block single double triple

00:14:30,250 --> 00:14:37,779
whatever they check some here tells us

00:14:35,709 --> 00:14:39,970
what algorithm we're using for the

00:14:37,779 --> 00:14:43,480
checksum you'll notice down here at the

00:14:39,970 --> 00:14:45,490
bottom we have the checksum of the you

00:14:43,480 --> 00:14:48,820
know the contents of the block note that

00:14:45,490 --> 00:14:50,920
the checksum is not stored with the rest

00:14:48,820 --> 00:14:53,080
of the block it is stored here and of

00:14:50,920 --> 00:14:54,520
course we need just one even if we have

00:14:53,080 --> 00:14:57,100
multiple copies because they should all

00:14:54,520 --> 00:15:01,360
have the same checksum value and in fact

00:14:57,100 --> 00:15:04,860
if you have for example two levels or

00:15:01,360 --> 00:15:07,450
three three sets of redundancy you can

00:15:04,860 --> 00:15:09,910
you you pull in the first one and the

00:15:07,450 --> 00:15:13,089
checksum fails you can pull in these

00:15:09,910 --> 00:15:16,360
other two and you know verify that the

00:15:13,089 --> 00:15:17,980
checksum does in fact work and then use

00:15:16,360 --> 00:15:21,430
that to decide that this one should be

00:15:17,980 --> 00:15:24,810
updated okay so by not storing the

00:15:21,430 --> 00:15:27,760
checksum where the data is stored

00:15:24,810 --> 00:15:30,160
essentially if the data somehow gets

00:15:27,760 --> 00:15:33,399
corrupted you because the checksum is

00:15:30,160 --> 00:15:34,959
stored elsewhere you hopefully that that

00:15:33,399 --> 00:15:38,290
the checksum will not have gotten

00:15:34,959 --> 00:15:41,470
corrupted all right then you then have

00:15:38,290 --> 00:15:44,440
the physical size and the logic

00:15:41,470 --> 00:15:46,930
sighs these will normally be the same

00:15:44,440 --> 00:15:48,850
but if you're doing compression the

00:15:46,930 --> 00:15:50,260
logical size is typically going to be

00:15:48,850 --> 00:15:52,660
bigger than the actual amount of space

00:15:50,260 --> 00:15:54,370
that you need to store the data because

00:15:52,660 --> 00:15:58,030
the compression has made the physical

00:15:54,370 --> 00:15:59,470
side go down compression here just tells

00:15:58,030 --> 00:16:02,920
you what algorithm you used for the

00:15:59,470 --> 00:16:05,980
compression we also have the birth times

00:16:02,920 --> 00:16:11,470
and I'm not going to go into all of the

00:16:05,980 --> 00:16:16,420
details but these times are not wall

00:16:11,470 --> 00:16:19,750
clock time rather they are which of the

00:16:16,420 --> 00:16:21,220
checkpoints in which checkpoint did this

00:16:19,750 --> 00:16:24,160
thing get created

00:16:21,220 --> 00:16:26,080
so if we start off at checkpoint 0 when

00:16:24,160 --> 00:16:27,370
we first create the pool and then that

00:16:26,080 --> 00:16:29,290
checkpoint just keeps getting

00:16:27,370 --> 00:16:31,360
incremented each time we do another

00:16:29,290 --> 00:16:33,370
checkpoint we choose to use the

00:16:31,360 --> 00:16:35,050
checkpoint number rather than time

00:16:33,370 --> 00:16:37,500
because if you took two checkpoints

00:16:35,050 --> 00:16:41,560
within the same second you might end up

00:16:37,500 --> 00:16:47,260
with these things matching and you don't

00:16:41,560 --> 00:16:49,420
want that to happen okay so whenever I

00:16:47,260 --> 00:16:53,380
talk about a block pointer I'm talking

00:16:49,420 --> 00:16:59,230
about one of these things and because

00:16:53,380 --> 00:17:00,670
they're very large you want to be you

00:16:59,230 --> 00:17:03,580
would like them to refer to something

00:17:00,670 --> 00:17:05,980
fairly big because of the overhead that

00:17:03,580 --> 00:17:12,689
you're incurring by having them so most

00:17:05,980 --> 00:17:16,270
blocks in ZFS are 128 K now small files

00:17:12,689 --> 00:17:18,790
can use smaller pieces so if you're

00:17:16,270 --> 00:17:21,490
running on disks with 4k sectors a file

00:17:18,790 --> 00:17:23,079
may use just a single 4k sector so in

00:17:21,490 --> 00:17:26,650
fact this thing may just point to

00:17:23,079 --> 00:17:29,770
something that's 4k but typically when a

00:17:26,650 --> 00:17:32,410
file anytime a file has grown over 128

00:17:29,770 --> 00:17:39,720
kilobytes in size it will be made up of

00:17:32,410 --> 00:17:39,720
some number of 128 kilobytes blocks okay

00:17:40,539 --> 00:17:46,250
so management of blocks as I've already

00:17:44,360 --> 00:17:49,580
said the blocks are all kept in a pool

00:17:46,250 --> 00:17:51,559
and we have multiple file systems and

00:17:49,580 --> 00:17:54,110
all their snapshots and clones are also

00:17:51,559 --> 00:17:56,140
held in that pool and then blocks from

00:17:54,110 --> 00:17:58,880
the pool are given to the file system as

00:17:56,140 --> 00:18:01,340
they're needed and then they're

00:17:58,880 --> 00:18:04,070
reclaimed back to the pool when they get

00:18:01,340 --> 00:18:06,260
freed now it turns out that actually the

00:18:04,070 --> 00:18:08,360
freeing of blocks turns out to be one of

00:18:06,260 --> 00:18:13,390
the more difficult things to deal with

00:18:08,360 --> 00:18:16,220
in ZFS not difficult in the sense of

00:18:13,390 --> 00:18:19,240
it's hard to get it right but it's it it

00:18:16,220 --> 00:18:21,200
takes a lot of code and a lot of rather

00:18:19,240 --> 00:18:22,970
interesting algorithms to be able to

00:18:21,200 --> 00:18:25,490
figure out when a block is really free

00:18:22,970 --> 00:18:28,970
because what will happen is you you

00:18:25,490 --> 00:18:31,070
first allocate the the file and then

00:18:28,970 --> 00:18:33,950
you've taken some number of snapshots

00:18:31,070 --> 00:18:36,169
and all of the snapshots are also gonna

00:18:33,950 --> 00:18:39,530
have I notice that refer to that block

00:18:36,169 --> 00:18:41,210
and so just because if you remove the

00:18:39,530 --> 00:18:43,370
file in the file system doesn't mean

00:18:41,210 --> 00:18:46,309
that we can actually free the block we

00:18:43,370 --> 00:18:48,919
can't free the block until the it's both

00:18:46,309 --> 00:18:51,740
free in the file system and we've gotten

00:18:48,919 --> 00:18:56,390
rid of all the snapshots that reference

00:18:51,740 --> 00:18:58,520
it so that's what a quick but not

00:18:56,390 --> 00:18:59,990
completely accurate way that I can

00:18:58,520 --> 00:19:03,700
describe this is with what are called

00:18:59,990 --> 00:19:05,900
dead lists and so when you remove a file

00:19:03,700 --> 00:19:08,390
we go through we take find all the

00:19:05,900 --> 00:19:11,360
blocks that are in that file and then we

00:19:08,390 --> 00:19:14,360
say okay are there any snapshots of this

00:19:11,360 --> 00:19:16,280
file system if so they'll be on a list

00:19:14,360 --> 00:19:18,169
sorted from the one that was created

00:19:16,280 --> 00:19:21,230
most recently to the one that was

00:19:18,169 --> 00:19:23,299
created furthest in the past and so we

00:19:21,230 --> 00:19:26,059
passed the blocks that we want to free

00:19:23,299 --> 00:19:28,100
down to the next snapshot and if that

00:19:26,059 --> 00:19:31,130
snapshot is still referencing the block

00:19:28,100 --> 00:19:33,559
it says oh well I'll hold on to that and

00:19:31,130 --> 00:19:36,049
if it's not then they just trickles down

00:19:33,559 --> 00:19:37,940
until it's all the snapshots I've had an

00:19:36,049 --> 00:19:40,159
opportunity to look at it and if none of

00:19:37,940 --> 00:19:42,679
them want it then and only then does it

00:19:40,159 --> 00:19:47,450
get handed back to the pool and actually

00:19:42,679 --> 00:19:49,039
made free ok so we'd already talked you

00:19:47,450 --> 00:19:52,490
can reserve space to make sure it'll be

00:19:49,039 --> 00:19:53,450
there and you can impose quotas alright

00:19:52,490 --> 00:19:56,450
so that same pick

00:19:53,450 --> 00:19:59,690
sure that you saw before I've now drawn

00:19:56,450 --> 00:20:02,450
in a bit more detail so again we've got

00:19:59,690 --> 00:20:04,909
the uber block at the top we have the

00:20:02,450 --> 00:20:08,659
object set which you saw before embedded

00:20:04,909 --> 00:20:11,659
in that is a D node and a D node to a

00:20:08,659 --> 00:20:12,830
first-order approximation is what we

00:20:11,659 --> 00:20:15,019
would call in the traditional file

00:20:12,830 --> 00:20:17,960
system and I note it's the data

00:20:15,019 --> 00:20:21,830
structure that keeps track of certain

00:20:17,960 --> 00:20:24,740
properties about the node and also and

00:20:21,830 --> 00:20:27,649
primarily keeps track of the block

00:20:24,740 --> 00:20:29,840
pointers now unlike the traditional file

00:20:27,649 --> 00:20:31,700
system where we have direct blocks and

00:20:29,840 --> 00:20:36,860
single indirect and double indirect and

00:20:31,700 --> 00:20:39,649
so on and ZFS we we start with a swell

00:20:36,860 --> 00:20:41,870
we have a single pointer and the rigid

00:20:39,649 --> 00:20:45,289
so you have one direct block pointer and

00:20:41,870 --> 00:20:48,350
then if the file gets bigger than 128 K

00:20:45,289 --> 00:20:49,700
so it needs two pointers instead of

00:20:48,350 --> 00:20:53,090
keeping the direct block pointer and

00:20:49,700 --> 00:20:55,940
then creating a single indirect block we

00:20:53,090 --> 00:20:57,950
just promote that the direct block we

00:20:55,940 --> 00:20:59,899
allocate an indirect block and when we

00:20:57,950 --> 00:21:01,909
make the what was previously the direct

00:20:59,899 --> 00:21:04,039
block pointer is the first entry in the

00:21:01,909 --> 00:21:06,649
single indirect and then we just start

00:21:04,039 --> 00:21:08,690
growing the single indirect and if we

00:21:06,649 --> 00:21:10,399
fill it up then we allocate a double

00:21:08,690 --> 00:21:11,690
indirect we take that single indirect

00:21:10,399 --> 00:21:15,110
and make it the first entry of the

00:21:11,690 --> 00:21:17,480
double indirect it so on so a file

00:21:15,110 --> 00:21:20,529
either has a single direct block pointer

00:21:17,480 --> 00:21:23,269
or it has a pointer to a single indirect

00:21:20,529 --> 00:21:24,980
block or it has a pointer to a double

00:21:23,269 --> 00:21:26,570
indirect block or it has a pointer to a

00:21:24,980 --> 00:21:29,539
triple indirect block it doesn't have

00:21:26,570 --> 00:21:32,960
that as we do in the regular file in the

00:21:29,539 --> 00:21:36,500
traditional file system all three mix

00:21:32,960 --> 00:21:38,210
together okay so we have some number

00:21:36,500 --> 00:21:41,480
some number of levels of indirect block

00:21:38,210 --> 00:21:43,940
pointers here and then in fact what we

00:21:41,480 --> 00:21:48,200
end up with is this array of these D

00:21:43,940 --> 00:21:51,409
nodes and then the D node has a sort of

00:21:48,200 --> 00:21:52,880
an area at the end of it that is sort of

00:21:51,409 --> 00:21:55,370
free space that can be used for

00:21:52,880 --> 00:21:57,500
different purposes so depending on what

00:21:55,370 --> 00:21:59,120
it is that the d node reference is we

00:21:57,500 --> 00:22:01,760
put different things into that free

00:21:59,120 --> 00:22:04,669
space at the end of it we use a thing

00:22:01,760 --> 00:22:05,500
called a data set when we are referring

00:22:04,669 --> 00:22:07,580
to

00:22:05,500 --> 00:22:10,150
most of the

00:22:07,580 --> 00:22:14,990
things like file systems and snapshots

00:22:10,150 --> 00:22:17,120
okay so the we have the the original

00:22:14,990 --> 00:22:18,590
master node here and since it can have

00:22:17,120 --> 00:22:21,679
sort of an arbitrary amount of stuff in

00:22:18,590 --> 00:22:24,010
it we use the D node to you know scale

00:22:21,679 --> 00:22:28,220
that up to however big it needs to be

00:22:24,010 --> 00:22:30,140
for a a file system or a clone this

00:22:28,220 --> 00:22:32,539
thing is going to point down to an

00:22:30,140 --> 00:22:36,020
object set which actually has 3d nodes

00:22:32,539 --> 00:22:38,090
in it so the two of them are used for

00:22:36,020 --> 00:22:40,820
the user quota and the group quota and

00:22:38,090 --> 00:22:44,150
the other one is used to describe the

00:22:40,820 --> 00:22:46,850
array of all of the AI nodes or D nodes

00:22:44,150 --> 00:22:49,549
actually that are describing the files

00:22:46,850 --> 00:22:50,960
and directories and so on here you also

00:22:49,549 --> 00:22:51,799
see this little pointer off the side

00:22:50,960 --> 00:22:56,510
this

00:22:51,799 --> 00:23:00,860
Zil is the ZFS intent log one of the

00:22:56,510 --> 00:23:02,659
issues that we have with ZFS is that

00:23:00,860 --> 00:23:05,900
although the filesystem is always

00:23:02,659 --> 00:23:08,240
consistent we've had some number of

00:23:05,900 --> 00:23:12,020
changes that have occurred since the

00:23:08,240 --> 00:23:15,440
last check point was taken and if the

00:23:12,020 --> 00:23:17,090
system crashes then when we reboot we

00:23:15,440 --> 00:23:18,470
leave the state that we get back is

00:23:17,090 --> 00:23:22,190
whatever the state was for the last

00:23:18,470 --> 00:23:25,370
check point so if you start making other

00:23:22,190 --> 00:23:27,740
changes unless you have some kind of a

00:23:25,370 --> 00:23:31,039
log you're going to lose those after

00:23:27,740 --> 00:23:34,340
crash and the reason that the the intent

00:23:31,039 --> 00:23:36,679
log is particularly important is so that

00:23:34,340 --> 00:23:38,120
you can implement F sync when you see

00:23:36,679 --> 00:23:40,640
how much work it is to take a check

00:23:38,120 --> 00:23:43,070
point you'll understand that we can't

00:23:40,640 --> 00:23:45,110
implement F sync by simply taking a

00:23:43,070 --> 00:23:47,690
check point a check point is a big deal

00:23:45,110 --> 00:23:49,850
it takes a long time so you could do a

00:23:47,690 --> 00:23:52,250
few of those a second but you couldn't

00:23:49,850 --> 00:23:54,350
do hundreds of them a second and you may

00:23:52,250 --> 00:23:57,020
if you're running something like an SMTP

00:23:54,350 --> 00:23:58,640
server be doing hundreds of F syncs a

00:23:57,020 --> 00:24:03,650
second based on the rate of incoming

00:23:58,640 --> 00:24:07,970
mail and so we need to use this ZFS

00:24:03,650 --> 00:24:11,090
intent log to essentially log the the F

00:24:07,970 --> 00:24:13,159
sync so that after a crash we will be

00:24:11,090 --> 00:24:14,539
able to start from the stable version of

00:24:13,159 --> 00:24:16,580
the file system and then run through the

00:24:14,539 --> 00:24:21,110
intent log to make sure that we get back

00:24:16,580 --> 00:24:24,440
at least everything that we

00:24:21,110 --> 00:24:28,460
was going to be there okay so this is

00:24:24,440 --> 00:24:30,830
not a traditional journal a journal only

00:24:28,460 --> 00:24:33,500
tracks the metadata changes this is a

00:24:30,830 --> 00:24:35,420
full blog so into the intent log here

00:24:33,500 --> 00:24:37,460
has to go not only the metadata changes

00:24:35,420 --> 00:24:41,150
that have happened but also any data

00:24:37,460 --> 00:24:45,049
because appsync is committing data of

00:24:41,150 --> 00:24:47,900
course all right so when you take a

00:24:45,049 --> 00:24:51,080
snapshot all you really do is just take

00:24:47,900 --> 00:24:52,730
a reference to this object here so over

00:24:51,080 --> 00:24:54,410
here you see a snapshot yeah you notice

00:24:52,730 --> 00:24:56,660
there's just a single denote shown in

00:24:54,410 --> 00:25:00,140
this picture but there are in fact three

00:24:56,660 --> 00:25:02,030
here the there's a frozen version of the

00:25:00,140 --> 00:25:04,700
group quote and a frozen version of the

00:25:02,030 --> 00:25:06,679
user quota and that's convenient because

00:25:04,700 --> 00:25:09,799
then later if you take a clone again you

00:25:06,679 --> 00:25:11,780
just take a snapshot you know you take a

00:25:09,799 --> 00:25:13,400
reference basically to this object and

00:25:11,780 --> 00:25:15,500
then as it changes of course it'll

00:25:13,400 --> 00:25:18,080
create a new one and leave the old

00:25:15,500 --> 00:25:22,190
unmodified one behind so the snapshot

00:25:18,080 --> 00:25:26,990
doesn't change for XIV all the things

00:25:22,190 --> 00:25:28,549
that look like a disk partition it's a

00:25:26,990 --> 00:25:32,150
much simpler thing it just has

00:25:28,549 --> 00:25:34,160
underneath it 2d nodes here one of which

00:25:32,150 --> 00:25:36,020
holds the master information and the

00:25:34,160 --> 00:25:39,830
other is the array of blocks that make

00:25:36,020 --> 00:25:41,210
up the disk partition so and then of

00:25:39,830 --> 00:25:44,179
course once you have one of these you

00:25:41,210 --> 00:25:47,990
can run a database on what it thinks is

00:25:44,179 --> 00:25:49,760
just a raw disk or you can format a

00:25:47,990 --> 00:25:51,860
traditional filesystem in there if you

00:25:49,760 --> 00:25:54,049
want but you have the benefit that you

00:25:51,860 --> 00:25:55,460
can take a snapshot of yours evolve just

00:25:54,049 --> 00:25:59,960
like you take a snapshot of a filesystem

00:25:55,460 --> 00:26:01,070
which is and make clones of it and you

00:25:59,960 --> 00:26:03,260
can do you have all the functionality

00:26:01,070 --> 00:26:05,330
that you normally get out of the the

00:26:03,260 --> 00:26:10,700
moss layer up here for your disk

00:26:05,330 --> 00:26:14,470
partition okay so far so good haven't

00:26:10,700 --> 00:26:16,820
lost anybody not everybody okay so

00:26:14,470 --> 00:26:19,250
checkpoints are sort of the key thing

00:26:16,820 --> 00:26:21,230
here and so I'm going to spend most of

00:26:19,250 --> 00:26:24,110
the rest of my time talking about how

00:26:21,230 --> 00:26:27,860
you actually take a ZFS checkpoint so

00:26:24,110 --> 00:26:30,050
you start when ZFS is running your

00:26:27,860 --> 00:26:32,570
you're collecting all of the changes

00:26:30,050 --> 00:26:34,130
that are happening in memory and if

00:26:32,570 --> 00:26:34,759
things are you know a lot of writing is

00:26:34,130 --> 00:26:35,929
happening

00:26:34,759 --> 00:26:39,769
you're chewing through memory and a

00:26:35,929 --> 00:26:40,999
pretty good clip so at any rate you're

00:26:39,769 --> 00:26:42,919
not writing anything to the file system

00:26:40,999 --> 00:26:45,739
obviously you're just collecting it all

00:26:42,919 --> 00:26:47,479
together in memory as the modifications

00:26:45,739 --> 00:26:49,759
happen so you're you're collecting the

00:26:47,479 --> 00:26:51,649
new data that's being written when you

00:26:49,759 --> 00:26:54,169
grow a file of course you have to update

00:26:51,649 --> 00:26:56,119
the inode with the new size and

00:26:54,169 --> 00:26:58,849
potentially with new block pointers etc

00:26:56,119 --> 00:27:00,769
and all of those changes that I know

00:26:58,849 --> 00:27:02,659
changes the actual data that's being

00:27:00,769 --> 00:27:06,049
done all that is just being collected in

00:27:02,659 --> 00:27:08,839
memory not being written to disk okay

00:27:06,049 --> 00:27:10,309
so now you say all right I don't want to

00:27:08,839 --> 00:27:11,719
take a checkpoint and there's the

00:27:10,309 --> 00:27:12,919
reasons for a checkpoint is a certain

00:27:11,719 --> 00:27:14,440
amount of time has passed or you've

00:27:12,919 --> 00:27:17,869
gotten a certain amount of dirty data

00:27:14,440 --> 00:27:20,119
and that triggers a checkpoint happening

00:27:17,869 --> 00:27:21,499
or a checkpoint will also happen if the

00:27:20,119 --> 00:27:23,599
administrator comes in and says I want

00:27:21,499 --> 00:27:25,219
to take a snapshot you start by taking a

00:27:23,599 --> 00:27:30,829
checkpoint and then you create the

00:27:25,219 --> 00:27:33,229
snapshot of that checkpoint okay so you

00:27:30,829 --> 00:27:36,729
gather together all these things that

00:27:33,229 --> 00:27:40,489
have changed and you go find a chunk of

00:27:36,729 --> 00:27:42,319
available disk space and you write it

00:27:40,489 --> 00:27:44,690
and if there's a lot of contiguous space

00:27:42,319 --> 00:27:47,269
all of the modifications that have been

00:27:44,690 --> 00:27:49,639
made get written in one big write and

00:27:47,269 --> 00:27:52,039
this is the reason that writing in ZFS

00:27:49,639 --> 00:27:53,299
is so fast because unlike the

00:27:52,039 --> 00:27:54,619
traditional file system where you want

00:27:53,299 --> 00:27:55,609
to update oh well you know I've got to

00:27:54,619 --> 00:27:57,349
go over here and write the data for this

00:27:55,609 --> 00:27:58,159
file and over here radius I know know

00:27:57,349 --> 00:28:00,079
over that or do something with a

00:27:58,159 --> 00:28:02,329
directory and so instead of having these

00:28:00,079 --> 00:28:06,679
rights scattered all over the disk it's

00:28:02,329 --> 00:28:10,309
just all in one place and once you do

00:28:06,679 --> 00:28:12,169
that then you have you know all that

00:28:10,309 --> 00:28:15,229
that i/o completes you know it's all

00:28:12,169 --> 00:28:17,469
there do you know any raid Z stuff has

00:28:15,229 --> 00:28:21,079
all been dealt with then and only then

00:28:17,469 --> 00:28:24,619
the last step is that you write the Ebor

00:28:21,079 --> 00:28:26,659
block and the blood is not just one

00:28:24,619 --> 00:28:29,179
hoover block there's actually typically

00:28:26,659 --> 00:28:31,519
several thousand of them and so but i

00:28:29,179 --> 00:28:34,339
did not have time to explain how we

00:28:31,519 --> 00:28:35,899
manage rubra blocks but anyway the or

00:28:34,339 --> 00:28:38,719
uber block that represents this

00:28:35,899 --> 00:28:40,699
checkpoint gets written okay so the

00:28:38,719 --> 00:28:42,409
entire pool is always consistent because

00:28:40,699 --> 00:28:43,840
when we write that Hoover block either

00:28:42,409 --> 00:28:45,130
we haven't written it and we

00:28:43,840 --> 00:28:48,520
the old version we have written and

00:28:45,130 --> 00:28:50,500
we've got the new version okay so as a

00:28:48,520 --> 00:28:51,909
bird he said the checkpoint affects all

00:28:50,500 --> 00:28:54,220
the file systems all the clones

00:28:51,909 --> 00:28:58,600
everything in the pool gets snapshot at

00:28:54,220 --> 00:29:00,820
at once okay and as I also said you need

00:28:58,600 --> 00:29:04,480
to log changes between the checkpoints

00:29:00,820 --> 00:29:07,150
in order to have persistence all right

00:29:04,480 --> 00:29:09,340
recovery starts from the last checkpoint

00:29:07,150 --> 00:29:11,049
so you find you come up you find that

00:29:09,340 --> 00:29:14,169
the uber block for the most recent

00:29:11,049 --> 00:29:17,799
checkpoint and you find the in the

00:29:14,169 --> 00:29:19,600
intent log and I mean you find the

00:29:17,799 --> 00:29:22,330
intent logs there's one for every file

00:29:19,600 --> 00:29:24,010
system and every XIV all and then you

00:29:22,330 --> 00:29:26,350
just roll forward through the log and

00:29:24,010 --> 00:29:27,909
this you know as you go through the log

00:29:26,350 --> 00:29:29,500
it's like great this do that do these

00:29:27,909 --> 00:29:31,330
other things it just builds up a whole

00:29:29,500 --> 00:29:33,880
bunch of stuff in memory just like you

00:29:31,330 --> 00:29:35,559
would from normal operation and when

00:29:33,880 --> 00:29:38,260
you've got all that done then you do a

00:29:35,559 --> 00:29:41,559
checkpoint and say ok boom we're now all

00:29:38,260 --> 00:29:46,559
caught up and we can reset the logs

00:29:41,559 --> 00:29:50,559
because we're ready to move on ok so

00:29:46,559 --> 00:29:52,330
what actually is involved so in in you

00:29:50,559 --> 00:29:54,700
know I say we've got all this dirty data

00:29:52,330 --> 00:29:57,100
but let's just look at what we actually

00:29:54,700 --> 00:30:01,090
have to do so this is what you would

00:29:57,100 --> 00:30:03,419
have to do if one file had one block

00:30:01,090 --> 00:30:06,070
added to it just to give you an idea

00:30:03,419 --> 00:30:08,950
there's nine things that we're gonna

00:30:06,070 --> 00:30:11,799
have to be changed well we start out

00:30:08,950 --> 00:30:14,049
down here in step one is there's the

00:30:11,799 --> 00:30:17,309
actual new data that got written ok

00:30:14,049 --> 00:30:19,990
that's not too surprising but since we

00:30:17,309 --> 00:30:22,480
have added another block to the file

00:30:19,990 --> 00:30:24,309
that means that let's say we have a

00:30:22,480 --> 00:30:26,200
single level indirect block pointer here

00:30:24,309 --> 00:30:27,940
we have to update that single level

00:30:26,200 --> 00:30:29,830
indirect block corner with the new

00:30:27,940 --> 00:30:32,140
pointer you know with the pointer to the

00:30:29,830 --> 00:30:34,179
new data and we can't change the

00:30:32,140 --> 00:30:35,890
existing single indirect block so we

00:30:34,179 --> 00:30:38,049
have to make a copy of the single

00:30:35,890 --> 00:30:39,970
indirect block with the update made to

00:30:38,049 --> 00:30:42,940
it and if it was a double indirect block

00:30:39,970 --> 00:30:45,190
we then have to potentially be the the

00:30:42,940 --> 00:30:47,289
block above it would change because this

00:30:45,190 --> 00:30:48,490
has changed therefore the thing that

00:30:47,289 --> 00:30:50,470
points had it changed so you have to

00:30:48,490 --> 00:30:52,149
trickle your way all the way up through

00:30:50,470 --> 00:30:54,340
all the indirect levels until you

00:30:52,149 --> 00:30:56,260
finally get to the inode and now of

00:30:54,340 --> 00:31:00,100
course the inode is pointing to a new

00:30:56,260 --> 00:31:04,090
block so the inode the gnudi node has to

00:31:00,100 --> 00:31:06,400
be written and then that because this

00:31:04,090 --> 00:31:08,320
has changed that is going to change the

00:31:06,400 --> 00:31:10,120
effect of this file which means we have

00:31:08,320 --> 00:31:14,530
to trickle all the changes up through

00:31:10,120 --> 00:31:17,530
all the indirect blocks here up to the

00:31:14,530 --> 00:31:19,480
object set now the object set has a new

00:31:17,530 --> 00:31:21,820
pointer so the object set has to be

00:31:19,480 --> 00:31:23,620
rewritten the object sets been rewritten

00:31:21,820 --> 00:31:26,140
so therefore the files the thing that

00:31:23,620 --> 00:31:27,670
points to it has to be rewritten which

00:31:26,140 --> 00:31:30,040
means this thing has changed so we have

00:31:27,670 --> 00:31:32,110
to change all the indirect blocks that

00:31:30,040 --> 00:31:34,480
go all the way back up to the top here

00:31:32,110 --> 00:31:37,510
that has now changed so we have to make

00:31:34,480 --> 00:31:40,120
a new copy of that and then finally the

00:31:37,510 --> 00:31:42,940
last step is to point to that thing so

00:31:40,120 --> 00:31:45,190
we figure out all the blocks that have

00:31:42,940 --> 00:31:48,130
changed all the way down from steps 1

00:31:45,190 --> 00:31:50,710
through 8 we gather all of those now

00:31:48,130 --> 00:31:53,290
modified blocks we write them out and

00:31:50,710 --> 00:31:55,900
once we we get confirmation that they've

00:31:53,290 --> 00:31:57,580
all been written then finally we update

00:31:55,900 --> 00:32:00,070
the ubirr block so the youger block is

00:31:57,580 --> 00:32:09,100
the only thing that we ever overwrite in

00:32:00,070 --> 00:32:11,650
a in a ZFS filesystem sorry yes oh yes

00:32:09,100 --> 00:32:13,840
the point he's pointed out that I

00:32:11,650 --> 00:32:16,720
allocated a block out of the space map

00:32:13,840 --> 00:32:19,450
so the space map has to change and if

00:32:16,720 --> 00:32:23,130
you back on this previous slide here

00:32:19,450 --> 00:32:26,620
come on you'll see it the space map is

00:32:23,130 --> 00:32:28,090
it's a file so you know we've changed

00:32:26,620 --> 00:32:29,710
one thing here so we had to change all

00:32:28,090 --> 00:32:32,170
the indirect things up to here so this

00:32:29,710 --> 00:32:34,510
changed and that that'll come in with

00:32:32,170 --> 00:32:36,670
this the change that happened here as we

00:32:34,510 --> 00:32:39,280
trickle them up through there so yeah

00:32:36,670 --> 00:32:43,210
another you know three four blocks have

00:32:39,280 --> 00:32:48,580
to be allocated and dealt with okay so

00:32:43,210 --> 00:32:51,940
the you can see why we can't implement F

00:32:48,580 --> 00:32:53,200
sync simply by taking a check point the

00:32:51,940 --> 00:32:54,370
the amount of work that we have to do

00:32:53,200 --> 00:32:57,780
the amount of space that we need to

00:32:54,370 --> 00:33:01,150
allocate is such that it would be just

00:32:57,780 --> 00:33:02,800
way too inefficient to do that now it

00:33:01,150 --> 00:33:04,300
looks really bad because of all the

00:33:02,800 --> 00:33:06,490
things that trickle up here but

00:33:04,300 --> 00:33:09,730
supposing we had two files that changed

00:33:06,490 --> 00:33:10,010
in this filesystem there'd be a little a

00:33:09,730 --> 00:33:12,230
few

00:33:10,010 --> 00:33:14,020
extra blocks for that file but then all

00:33:12,230 --> 00:33:16,190
the rest of the stuff trickling up here

00:33:14,020 --> 00:33:17,870
we've already had to change it all

00:33:16,190 --> 00:33:20,210
already because of this first file so

00:33:17,870 --> 00:33:22,070
it's you don't get this much stuff for

00:33:20,210 --> 00:33:24,170
every modification that occurred and

00:33:22,070 --> 00:33:27,050
that's why if you aggregate together a

00:33:24,170 --> 00:33:29,150
bunch of changes the overall the the

00:33:27,050 --> 00:33:31,400
cost of this trickle up is not nearly as

00:33:29,150 --> 00:33:32,870
bad you know we've already had to update

00:33:31,400 --> 00:33:35,390
the space map we've already had to

00:33:32,870 --> 00:33:38,270
update all of these things and so just

00:33:35,390 --> 00:33:40,640
updating one more file is not nearly as

00:33:38,270 --> 00:33:46,250
bad as the first file that we had to

00:33:40,640 --> 00:33:49,220
change all right so I just want to sort

00:33:46,250 --> 00:33:52,580
of finish up by summarizing sort of the

00:33:49,220 --> 00:33:56,090
strengths and weaknesses of ZFS ZFS is

00:33:52,580 --> 00:34:00,470
not going to replace ffs because it

00:33:56,090 --> 00:34:03,230
requires a lot of resources and it you

00:34:00,470 --> 00:34:07,760
know it's very well designed for sort of

00:34:03,230 --> 00:34:11,000
large large pools of systems or large

00:34:07,760 --> 00:34:13,100
pools of data and lots of file systems

00:34:11,000 --> 00:34:15,440
but if all you have is a little embedded

00:34:13,100 --> 00:34:16,940
appliance and you need a file system to

00:34:15,440 --> 00:34:18,889
sort of manage a small amount of stuff

00:34:16,940 --> 00:34:20,510
you're not going to do that with ZFS I

00:34:18,889 --> 00:34:22,550
mean that's just overkill and you don't

00:34:20,510 --> 00:34:24,649
have the resources to do it so for this

00:34:22,550 --> 00:34:26,540
sort of embedded appliance where you

00:34:24,649 --> 00:34:29,659
have sort of us usually a single disk

00:34:26,540 --> 00:34:31,700
drive ffs continues to be sort of the

00:34:29,659 --> 00:34:35,210
right solution where you've got large

00:34:31,700 --> 00:34:36,860
pools ZFS just blows ffs out of the

00:34:35,210 --> 00:34:38,870
water because it has all these

00:34:36,860 --> 00:34:42,500
redundancy and checking and other

00:34:38,870 --> 00:34:45,169
capabilities that ffs could only dream

00:34:42,500 --> 00:34:47,480
about okay so what are its strengths the

00:34:45,169 --> 00:34:48,020
high right throughput as already

00:34:47,480 --> 00:34:49,639
mentioned

00:34:48,020 --> 00:34:51,860
instead of having to scatter stuff all

00:34:49,639 --> 00:34:57,590
over the place it's just getting chunked

00:34:51,860 --> 00:34:59,990
down in one place because it uses raid-z

00:34:57,590 --> 00:35:01,370
which I haven't had a chance to describe

00:34:59,990 --> 00:35:05,690
to you but

00:35:01,370 --> 00:35:09,410
with raid-z essentially each block since

00:35:05,690 --> 00:35:12,710
the raid is integrated in with ZFS each

00:35:09,410 --> 00:35:16,220
block just gets its own raid stripe and

00:35:12,710 --> 00:35:16,820
so you don't have partial stripes that

00:35:16,220 --> 00:35:20,030
you're filling

00:35:16,820 --> 00:35:23,150
you're always filling exactly the block

00:35:20,030 --> 00:35:25,309
and so the upshot of this is that

00:35:23,150 --> 00:35:27,170
when you reconstruct you don't have to

00:35:25,309 --> 00:35:29,029
go through when you want to rebuild a

00:35:27,170 --> 00:35:30,859
disk you don't have to go through and

00:35:29,029 --> 00:35:32,480
reconstruct every block on the disk

00:35:30,859 --> 00:35:35,059
because you know which blocks are being

00:35:32,480 --> 00:35:37,730
used the way you reconstruct raid-z is

00:35:35,059 --> 00:35:40,670
you just do a walk across the the all

00:35:37,730 --> 00:35:42,980
the file systems in the pool and figure

00:35:40,670 --> 00:35:46,220
out you know which blocks they are using

00:35:42,980 --> 00:35:48,710
and then you reconstruct those blocks so

00:35:46,220 --> 00:35:51,289
if you have a pool that has a relatively

00:35:48,710 --> 00:35:53,809
low utilization it's actually faster to

00:35:51,289 --> 00:35:56,289
reconstruct a raid-z pool than it is to

00:35:53,809 --> 00:35:58,940
reconstruct the whole physical medium

00:35:56,289 --> 00:36:01,309
unfortunately if your pool is mostly

00:35:58,940 --> 00:36:03,289
full it actually takes considerably

00:36:01,309 --> 00:36:04,819
longer to reconstruct the raid-z because

00:36:03,289 --> 00:36:07,460
there's a lot of random access stuff

00:36:04,819 --> 00:36:14,180
whereas a traditional raid rebuild

00:36:07,460 --> 00:36:16,190
happens sequentially across the disk ZFS

00:36:14,180 --> 00:36:19,130
doesn't have the right whole problem

00:36:16,190 --> 00:36:21,289
would raid the right hole is where

00:36:19,130 --> 00:36:22,730
you've you're updating a stripe and

00:36:21,289 --> 00:36:24,950
you've written some pieces of it but not

00:36:22,730 --> 00:36:27,380
other parts of it and you have to have

00:36:24,950 --> 00:36:28,819
some NVRAM or some other way so that the

00:36:27,380 --> 00:36:31,339
power fails when it comes back up you

00:36:28,819 --> 00:36:33,289
know how to finish writing that ZFS

00:36:31,339 --> 00:36:35,390
doesn't need to worry about that because

00:36:33,289 --> 00:36:37,130
it's not going to change the check point

00:36:35,390 --> 00:36:40,010
until it knows all of the stripes are

00:36:37,130 --> 00:36:42,970
completely written so it never has any

00:36:40,010 --> 00:36:47,380
block that's referenced by a filesystem

00:36:42,970 --> 00:36:47,380
that is incomplete

00:36:47,900 --> 00:36:51,619
you can blue move blocks between file

00:36:50,029 --> 00:36:54,349
systems as needed you know how many

00:36:51,619 --> 00:36:56,329
times have you statically created ffs

00:36:54,349 --> 00:36:58,579
file systems and then wished that you'd

00:36:56,329 --> 00:37:01,099
put more in one file blocks in one file

00:36:58,579 --> 00:37:02,240
system and then you'd in another it'd be

00:37:01,099 --> 00:37:03,500
really nice if I could just reach over

00:37:02,240 --> 00:37:05,059
into that other file system that's got

00:37:03,500 --> 00:37:07,880
all that free space and borrow some of

00:37:05,059 --> 00:37:10,099
it but obviously you can't do that

00:37:07,880 --> 00:37:12,529
where are as we'd CFS they're just

00:37:10,099 --> 00:37:14,150
blocks in the pool and so if a file

00:37:12,529 --> 00:37:15,470
system needs a lot of space it gets it

00:37:14,150 --> 00:37:17,000
and if it doesn't need it it gives it

00:37:15,470 --> 00:37:21,410
back in some other file system can use

00:37:17,000 --> 00:37:22,339
it it's monolithic and I have a long

00:37:21,410 --> 00:37:24,500
list of things I don't like about

00:37:22,339 --> 00:37:25,819
monolithic but one of the benefits of

00:37:24,500 --> 00:37:29,869
monolithic is that everything's

00:37:25,819 --> 00:37:32,029
integrated together so the the those

00:37:29,869 --> 00:37:33,720
master nodes for example keep track of

00:37:32,029 --> 00:37:35,880
where things are mounted

00:37:33,720 --> 00:37:38,040
what things are exported and what the

00:37:35,880 --> 00:37:39,870
properties of those exports are so you

00:37:38,040 --> 00:37:42,240
don't have to maintain all these other

00:37:39,870 --> 00:37:47,430
files like etcetera FS tab and etc

00:37:42,240 --> 00:37:49,500
remote are its was export and ZFS just

00:37:47,430 --> 00:37:51,240
tracks all that it knows where things

00:37:49,500 --> 00:37:54,990
are and it just makes all the mounts

00:37:51,240 --> 00:37:59,970
happen exports happen as they should so

00:37:54,990 --> 00:38:04,770
it eases the administration ok so where

00:37:59,970 --> 00:38:08,130
does it fall down well the if you write

00:38:04,770 --> 00:38:10,260
a file slowly then it's blocks are going

00:38:08,130 --> 00:38:12,840
to end up scattered all over the disk so

00:38:10,260 --> 00:38:15,630
think of a log that's being written over

00:38:12,840 --> 00:38:17,670
several days time the the blocks that

00:38:15,630 --> 00:38:19,740
make up that log are going to be all

00:38:17,670 --> 00:38:21,600
over the disk because they're written

00:38:19,740 --> 00:38:24,960
temporally they're written as they're

00:38:21,600 --> 00:38:27,270
creating and so if you now want to grep

00:38:24,960 --> 00:38:29,160
that log it's going to take a long time

00:38:27,270 --> 00:38:32,220
to run all over the disk to pick up all

00:38:29,160 --> 00:38:34,440
those pieces the way CFS deals with this

00:38:32,220 --> 00:38:37,800
is it simply makes sure that it has

00:38:34,440 --> 00:38:39,570
enough cash so that any files that

00:38:37,800 --> 00:38:41,190
you've read over the last day or so are

00:38:39,570 --> 00:38:42,420
just going to be in the cache and then

00:38:41,190 --> 00:38:45,210
in fact that they're not well laid out

00:38:42,420 --> 00:38:47,970
on the disk doesn't matter one of the

00:38:45,210 --> 00:38:49,590
reasons that ZFS wants you know 8 16

00:38:47,970 --> 00:38:54,090
gigabytes of memory to be available is

00:38:49,590 --> 00:38:56,010
to mitigate these problems I've already

00:38:54,090 --> 00:38:59,280
talked about reconstructing a nearly

00:38:56,010 --> 00:39:03,030
full pool can go up to 10 times slower

00:38:59,280 --> 00:39:04,800
than if you just had physical media the

00:39:03,030 --> 00:39:07,860
block cache has to fit in the kernels

00:39:04,800 --> 00:39:09,750
address space so in the traditional file

00:39:07,860 --> 00:39:11,700
system we just map the blocks into the

00:39:09,750 --> 00:39:15,450
kernel when we need to look at them so

00:39:11,700 --> 00:39:17,730
you can have a 32-bit processor with 16

00:39:15,450 --> 00:39:19,740
gigabytes of memory and you can use all

00:39:17,730 --> 00:39:21,600
of that memory as the buffer cache in

00:39:19,740 --> 00:39:25,020
the cases of ZFS the way it's

00:39:21,600 --> 00:39:27,180
implemented it wants all of physical

00:39:25,020 --> 00:39:28,530
memory to be mapped into the kernel so

00:39:27,180 --> 00:39:30,540
if you're running on a 32-bit address

00:39:28,530 --> 00:39:32,880
space your cache can't really be bigger

00:39:30,540 --> 00:39:35,520
than about a gigabyte no matter how much

00:39:32,880 --> 00:39:36,990
physical memory is on the machine so the

00:39:35,520 --> 00:39:39,450
answer to this is if you're going to run

00:39:36,990 --> 00:39:44,100
ZFS just make sure you're on a 64-bit

00:39:39,450 --> 00:39:46,360
processor FreeBSD will support it on

00:39:44,100 --> 00:39:48,490
32-bit processors quiet

00:39:46,360 --> 00:39:51,040
you will not be happy so just don't do

00:39:48,490 --> 00:39:54,640
that okay

00:39:51,040 --> 00:39:57,040
if the pool gets more than about 75%

00:39:54,640 --> 00:39:59,200
full the allocations start to get very

00:39:57,040 --> 00:40:02,980
very painful and it's because it wants

00:39:59,200 --> 00:40:05,590
these 128 K blocks and if there's not

00:40:02,980 --> 00:40:07,060
enough space to have those then it has

00:40:05,590 --> 00:40:08,530
to start taking smaller pieces and

00:40:07,060 --> 00:40:10,990
putting them together in these things

00:40:08,530 --> 00:40:15,490
called gang blocks and that's just

00:40:10,990 --> 00:40:16,930
painful so don't don't really plan to

00:40:15,490 --> 00:40:20,020
run more than about 75 percent

00:40:16,930 --> 00:40:21,280
utilization the good news is if you see

00:40:20,020 --> 00:40:22,780
you're getting too high you can always

00:40:21,280 --> 00:40:25,180
just add more disks because that just

00:40:22,780 --> 00:40:26,560
adds more space to the pool and it can

00:40:25,180 --> 00:40:28,360
then just be handed out to all the other

00:40:26,560 --> 00:40:30,430
file systems so unlike a traditional

00:40:28,360 --> 00:40:32,410
file system where you say oh I'm getting

00:40:30,430 --> 00:40:34,330
too full it's like well no just you can

00:40:32,410 --> 00:40:36,370
just add disk to it which is something

00:40:34,330 --> 00:40:40,030
that not a property that you typically

00:40:36,370 --> 00:40:42,700
have with the regular file system okay

00:40:40,030 --> 00:40:45,550
by contrast ffs will go to 95 percent

00:40:42,700 --> 00:40:49,230
quite happily and will go to about 99

00:40:45,550 --> 00:40:53,110
percent somewhat less happily all right

00:40:49,230 --> 00:40:55,060
raid Z is has a lot of good properties

00:40:53,110 --> 00:40:57,160
associated with it but one of the less

00:40:55,060 --> 00:41:00,100
nice properties is if you're using 4k

00:40:57,160 --> 00:41:02,050
blocks then you're you have a 50 percent

00:41:00,100 --> 00:41:08,230
overhead if you're doing a single

00:41:02,050 --> 00:41:11,200
redundancy raid and so the 4k blocks are

00:41:08,230 --> 00:41:14,710
typically used by Z vowels or databases

00:41:11,200 --> 00:41:17,050
and so you are going to have a high

00:41:14,710 --> 00:41:20,470
overhead if you choose to say use 4k

00:41:17,050 --> 00:41:22,270
blocks and finally the thing that's

00:41:20,470 --> 00:41:24,070
really a pain is that the blocks that

00:41:22,270 --> 00:41:27,130
are cached in memory are not part of the

00:41:24,070 --> 00:41:29,860
unified buffer cache it's got its own

00:41:27,130 --> 00:41:32,650
little world with the ark and so if

00:41:29,860 --> 00:41:34,000
you're doing m mapping or sun file you

00:41:32,650 --> 00:41:35,410
end up doing an extra memory to memory

00:41:34,000 --> 00:41:38,860
copy every time you read or write

00:41:35,410 --> 00:41:41,050
something okay and I'm being yelled at

00:41:38,860 --> 00:41:44,800
to be done so I'll push the button and

00:41:41,050 --> 00:41:46,990
get to questions any questions they do

00:41:44,800 --> 00:41:49,690
ask that you talk on the microphone so

00:41:46,990 --> 00:41:52,420
that people walk in not in the room that

00:41:49,690 --> 00:41:57,790
are listening can can hear your very

00:41:52,420 --> 00:42:00,090
insightful questions surely somebody has

00:41:57,790 --> 00:42:00,090
a question

00:42:11,759 --> 00:42:16,630
so what about sibling performance and

00:42:15,609 --> 00:42:21,729
implementation

00:42:16,630 --> 00:42:23,170
what about siblings symlinks why what

00:42:21,729 --> 00:42:25,479
would be particularly difficult about

00:42:23,170 --> 00:42:28,479
them I mean that's just a path name that

00:42:25,479 --> 00:42:36,279
points to a file so yeah but there were

00:42:28,479 --> 00:42:38,469
places in the the headers you're talking

00:42:36,279 --> 00:42:41,680
about like having to use a whole block

00:42:38,469 --> 00:42:44,680
to hold a symlink is that the issue the

00:42:41,680 --> 00:42:46,509
thing is that in ufs if the sim link is

00:42:44,680 --> 00:42:50,369
small enough it's stored inside the

00:42:46,509 --> 00:42:53,709
inode and not as a file yes okay so

00:42:50,369 --> 00:42:57,369
remember I said that the D node had this

00:42:53,709 --> 00:42:59,170
extra space at the end of it so that

00:42:57,369 --> 00:43:01,569
extra space can be used to hold assembly

00:42:59,170 --> 00:43:10,739
so they pretty much do the same game as

00:43:01,569 --> 00:43:20,229
we do in a pass for that anyone else

00:43:10,739 --> 00:43:23,439
yeah I can tell you all one no I think

00:43:20,229 --> 00:43:28,569
almost everybody in this room knows for

00:43:23,439 --> 00:43:33,699
the difficulty in installing and adding

00:43:28,569 --> 00:43:37,599
devices to ZFS pools with 4 Kbytes

00:43:33,699 --> 00:43:44,709
physical sectors I mean those Advanced

00:43:37,599 --> 00:43:48,400
Format disks it it is used not devices

00:43:44,709 --> 00:43:52,719
are made and then they are added to the

00:43:48,400 --> 00:43:58,479
pool otherwise DFS reads the devices

00:43:52,719 --> 00:44:01,929
properties and starts with 50 15 1,500

00:43:58,479 --> 00:44:04,779
bytes sectors so in the next version

00:44:01,929 --> 00:44:06,959
versions are there any improvements in

00:44:04,779 --> 00:44:06,959
this

00:44:08,920 --> 00:44:13,789
well I can only speculate on what the

00:44:11,990 --> 00:44:19,730
ZFS developers are going to do since I

00:44:13,789 --> 00:44:21,380
don't actually develop ZFS but given the

00:44:19,730 --> 00:44:22,999
issue that you have raised I would be

00:44:21,380 --> 00:44:28,039
very surprised if that's not something

00:44:22,999 --> 00:44:31,789
that they plan to do I'm told you have

00:44:28,039 --> 00:44:34,039
to speak right there's a Cissy TL called

00:44:31,789 --> 00:44:35,990
minimum a shift that you can set so that

00:44:34,039 --> 00:44:39,339
you can force any device added to your

00:44:35,990 --> 00:44:42,079
pool to be treated with block size of 4k

00:44:39,339 --> 00:44:46,130
okay and that you said when did that

00:44:42,079 --> 00:44:54,559
show up it's already in 9.3 and will be

00:44:46,130 --> 00:44:56,779
in 10.1 okay so gherkin can we expect

00:44:54,559 --> 00:44:58,999
you join ZFS development team any site

00:44:56,779 --> 00:45:04,249
any time can you expect me to join the

00:44:58,999 --> 00:45:06,410
ZFS development team probably not when I

00:45:04,249 --> 00:45:08,029
was younger I used to take on projects

00:45:06,410 --> 00:45:10,670
you you probably know about this because

00:45:08,029 --> 00:45:12,349
you do the ZFS and you think you're done

00:45:10,670 --> 00:45:13,430
and then they keep coming back to you

00:45:12,349 --> 00:45:14,989
and say well how about this and how

00:45:13,430 --> 00:45:18,410
about that and can't you do this and

00:45:14,989 --> 00:45:21,380
can't you do that so this has happened

00:45:18,410 --> 00:45:23,480
to me I I had early on I took on the

00:45:21,380 --> 00:45:26,329
fast file system the VM system and NFS

00:45:23,480 --> 00:45:29,450
and I've managed to shed the vm system

00:45:26,329 --> 00:45:31,759
and NFS thankfully but I'm stolen the

00:45:29,450 --> 00:45:33,739
hook pretty much for ffs but so I'm not

00:45:31,759 --> 00:45:37,759
into finding another project that I

00:45:33,739 --> 00:45:39,410
could be on the hook for this spoken by

00:45:37,759 --> 00:45:43,339
the person that did the original port of

00:45:39,410 --> 00:45:45,140
ZFS to FreeBSD let me say alright well

00:45:43,339 --> 00:45:47,440
it's definitely time for us to have some

00:45:45,140 --> 00:45:53,480
lunch so thank you very much thank you

00:45:47,440 --> 00:45:59,890
[Applause]

00:45:53,480 --> 00:45:59,890

YouTube URL: https://www.youtube.com/watch?v=TQe-nnJPNF8


