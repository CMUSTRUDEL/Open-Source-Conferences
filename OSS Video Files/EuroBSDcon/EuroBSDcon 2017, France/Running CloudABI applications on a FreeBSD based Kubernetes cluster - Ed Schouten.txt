Title: Running CloudABI applications on a FreeBSD based Kubernetes cluster - Ed Schouten
Publication date: 2019-10-16
Playlist: EuroBSDcon 2017, France
Description: 
	Description:

Two years ago, I gave a talk at EuroBSDCon in Stockholm, where I
presented a project I started working on that same year, called
CloudABI. CloudABI is a framework that allows you to design POSIX-like
programs that are very strongly sandboxed. CloudABI is comparable to
FreeBSD’s sandboxing technique Capsicum, but differs in the sense that
sandboxing has to be turned on as soon as the first instruction of
your program starts. This has a couple of interesting implications:

 You can safely run programs that you don’t trust at all, as long as
you don’t provide it access to file descriptors of resources that
should remain off-limits. This makes it a very useful building block
for a multi-tenant cloud/cluster computing service.
By having Capsicum always enabled, we can remove all of the features
that conflict with Capsicum. This allows you to modify applications to
work well with sandboxing a lot more easily. It is easy to make an
inventory of which modification need to be made, simply by looking at
compiler errors generated by the absence of the incompatible features.
Software becomes easier to test and manage. This effectively brings
the principle of Dependency Injection from object oriented programming
to full-scale programs.

In this talk, I’m going to discuss how I’ve added support for running
CloudABI-based applications directly on top of Kubernetes, an Open
Source cluster management suite. An interesting aspect of this is that
it effectively removes the dependency on Docker and makes Kubernetes
work on FreeBSD. After giving a crash course on Kubernetes, I will
present the software that I have developed to make this work.

Speaker biography:

Ed Schouten has been a developer at the FreeBSD project since 2008.
Initially, he focussed on terminals, TTYs and console drivers. Later
on he maintained a branch of FreeBSD called ClangBSD, whose purpose it
was to replace FreeBSD’s system compiler, GCC, with Clang. Nowadays,
he spends most of his time working on CloudABI.
Captions: 
	00:00:00,030 --> 00:00:04,319
so good morning everyone I'm glad to see

00:00:02,820 --> 00:00:06,240
that you guys at least made it to the

00:00:04,319 --> 00:00:10,440
second talk of this day I mean that

00:00:06,240 --> 00:00:12,210
you're not still in bed before I start

00:00:10,440 --> 00:00:14,280
my talk I quickly sort of want to like

00:00:12,210 --> 00:00:17,490
you guys who raise your hand who has

00:00:14,280 --> 00:00:19,320
ever heard of kubernetes before oh whoa

00:00:17,490 --> 00:00:22,350
okay that's a lot more than expected

00:00:19,320 --> 00:00:22,710
then I can just skip 10 slides no just

00:00:22,350 --> 00:00:25,619
kidding

00:00:22,710 --> 00:00:28,170
so who views heard of free beezy well of

00:00:25,619 --> 00:00:29,420
course everybody who has heard of cloud

00:00:28,170 --> 00:00:33,360
ABI

00:00:29,420 --> 00:00:43,260
okay well still hands race that's pretty

00:00:33,360 --> 00:00:47,969
impressive so oh oh okay I'll speak even

00:00:43,260 --> 00:00:49,739
closer should not swallow it so so yeah

00:00:47,969 --> 00:00:53,489
my name is ed scouted I've been a

00:00:49,739 --> 00:00:57,590
FreeBSD developer since 2008 initially I

00:00:53,489 --> 00:00:59,760
used to work on terminal TTY stuff

00:00:57,590 --> 00:01:02,670
people nowadays still sent me bug

00:00:59,760 --> 00:01:05,129
reports and hope that I fix stuff there

00:01:02,670 --> 00:01:07,890
but too busy nowadays sorry about that

00:01:05,129 --> 00:01:10,439
guys nowadays are more focusing on

00:01:07,890 --> 00:01:13,409
security cluster computing etc which is

00:01:10,439 --> 00:01:15,090
why I'm giving today's talk so this like

00:01:13,409 --> 00:01:17,280
the title of the talk is also going to

00:01:15,090 --> 00:01:19,110
be sort of my outline for today I didn't

00:01:17,280 --> 00:01:21,659
have a separate outline slide first I'm

00:01:19,110 --> 00:01:24,299
going to talk about kubernetes what it

00:01:21,659 --> 00:01:26,580
is give like a short introduction then

00:01:24,299 --> 00:01:28,770
I'm going to talk how kubernetes is

00:01:26,580 --> 00:01:31,140
related to freebsd or how we could get

00:01:28,770 --> 00:01:32,670
it running on freebsd and then later on

00:01:31,140 --> 00:01:34,350
i'm going to talk about cloud API and

00:01:32,670 --> 00:01:37,170
i'm throwing all of these three

00:01:34,350 --> 00:01:39,380
different components in the mix and why

00:01:37,170 --> 00:01:42,020
we should be doing it in the first place

00:01:39,380 --> 00:01:45,240
so let's first start off with kubernetes

00:01:42,020 --> 00:01:47,430
so kubernetes is a cluster management

00:01:45,240 --> 00:01:49,500
system that has originally been

00:01:47,430 --> 00:01:51,899
developed by Google and it sort of

00:01:49,500 --> 00:01:53,939
inspired by their sort of proprietary

00:01:51,899 --> 00:01:56,610
cluster management system called Borg

00:01:53,939 --> 00:01:59,939
that I also used a lot in the past while

00:01:56,610 --> 00:02:01,740
I was working for them in Munich the

00:01:59,939 --> 00:02:03,869
main difference between Borg and

00:02:01,740 --> 00:02:06,689
kubernetes is said kubernetes is written

00:02:03,869 --> 00:02:08,250
in go and this was sort of where's Borg

00:02:06,689 --> 00:02:10,920
is written in C++ and this is sort of

00:02:08,250 --> 00:02:13,110
done on purpose because they there were

00:02:10,920 --> 00:02:14,140
like a lot of design flaws things that

00:02:13,110 --> 00:02:16,240
could have been improved

00:02:14,140 --> 00:02:17,800
on board so what they did is they picked

00:02:16,240 --> 00:02:19,450
a different language so they were sort

00:02:17,800 --> 00:02:21,670
of required to rewrite all of it and

00:02:19,450 --> 00:02:22,840
sort of cherry-pick all of the things

00:02:21,670 --> 00:02:26,110
that were good and leave out all of the

00:02:22,840 --> 00:02:28,690
things that were bad so what happened is

00:02:26,110 --> 00:02:30,880
that Google like wrote is initially you

00:02:28,690 --> 00:02:33,790
know release it as open-source put it on

00:02:30,880 --> 00:02:34,870
github and then later on donated it to

00:02:33,790 --> 00:02:36,700
something called the cloud native

00:02:34,870 --> 00:02:38,320
computing foundation and the cloud

00:02:36,700 --> 00:02:39,850
native computing foundation is sort of a

00:02:38,320 --> 00:02:42,670
branch of the Linux Foundation we're

00:02:39,850 --> 00:02:44,860
sort of all projects end up that are

00:02:42,670 --> 00:02:47,470
related to cloud computing cluster

00:02:44,860 --> 00:02:48,670
computing so other projects that are

00:02:47,470 --> 00:02:50,290
part of the cloud native computing

00:02:48,670 --> 00:02:52,510
foundation are Prometheus a monitoring

00:02:50,290 --> 00:02:55,350
system which i think is pretty awesome

00:02:52,510 --> 00:02:59,080
and also used on like day to day basis

00:02:55,350 --> 00:03:02,410
and another pretty popular one is G RPC

00:02:59,080 --> 00:03:05,110
so that's google's RPC framework that is

00:03:02,410 --> 00:03:06,430
built on top of protobuf which is also

00:03:05,110 --> 00:03:08,019
sort of a reimplementation of a

00:03:06,430 --> 00:03:10,480
proprietary system they had internally

00:03:08,019 --> 00:03:12,459
called stubby but stubby was too hard to

00:03:10,480 --> 00:03:13,690
open-source because it also sort of tied

00:03:12,459 --> 00:03:16,090
in with a lot of other Google

00:03:13,690 --> 00:03:17,500
infrastructure too tightly so that's why

00:03:16,090 --> 00:03:21,489
they sort of made a clean slate approach

00:03:17,500 --> 00:03:23,320
and donated to the CNC F so like almost

00:03:21,489 --> 00:03:25,090
all of them I think all of the projects

00:03:23,320 --> 00:03:27,820
that are part of the CNC efforts apache2

00:03:25,090 --> 00:03:29,680
licensed so that's pretty faithful

00:03:27,820 --> 00:03:32,799
favourable for us of course I mean it's

00:03:29,680 --> 00:03:36,310
not the BSD license but still better

00:03:32,799 --> 00:03:39,130
than the GPL in my opinion so to sort of

00:03:36,310 --> 00:03:41,590
give like an explanation of like the

00:03:39,130 --> 00:03:44,680
model behind a kubernetes cluster you

00:03:41,590 --> 00:03:46,450
know you you have some sort of get used

00:03:44,680 --> 00:03:48,100
to the terminology that kubernetes uses

00:03:46,450 --> 00:03:51,280
to sort of understand it properly how it

00:03:48,100 --> 00:03:52,690
works so one of the common phrases you

00:03:51,280 --> 00:03:55,950
often hear when people talk about

00:03:52,690 --> 00:03:58,690
kubernetes is nodes and nodes are just

00:03:55,950 --> 00:04:00,640
Linux servers just your average Linux

00:03:58,690 --> 00:04:02,440
installation it doesn't matter whether

00:04:00,640 --> 00:04:04,810
it's a virtual server a physical server

00:04:02,440 --> 00:04:06,700
whether it's running on AWS or on

00:04:04,810 --> 00:04:09,700
Google's own cloud computing platform or

00:04:06,700 --> 00:04:12,220
on your own Hardware in your your your

00:04:09,700 --> 00:04:13,510
basement it doesn't matter it's as long

00:04:12,220 --> 00:04:18,910
as it's running a Linux kernel

00:04:13,510 --> 00:04:20,979
it's a node so on top of those nodes you

00:04:18,910 --> 00:04:24,039
want to be able to run containers docker

00:04:20,979 --> 00:04:26,680
containers in the case of kubernetes and

00:04:24,039 --> 00:04:28,030
a container in my opinion can be defined

00:04:26,680 --> 00:04:30,370
as like a group of unique

00:04:28,030 --> 00:04:32,970
processes that share the same process

00:04:30,370 --> 00:04:35,110
process / file system namespace so

00:04:32,970 --> 00:04:37,780
process is running in a container can

00:04:35,110 --> 00:04:39,460
all like send signals to each other they

00:04:37,780 --> 00:04:41,200
can store file somewhere in the file

00:04:39,460 --> 00:04:42,580
system that have become visible to other

00:04:41,200 --> 00:04:46,000
processors running in the same container

00:04:42,580 --> 00:04:48,220
etc kubernetes has sort of added like

00:04:46,000 --> 00:04:50,410
one layer on top of containers called

00:04:48,220 --> 00:04:51,760
pubs and pods are groups of containers

00:04:50,410 --> 00:04:53,680
that need to be scheduled together on

00:04:51,760 --> 00:04:54,880
one node so they're like the smallest

00:04:53,680 --> 00:04:57,910
thing that can be scheduled on the

00:04:54,880 --> 00:04:59,200
cluster so it's impossible to start just

00:04:57,910 --> 00:05:01,150
a single container on the cluster

00:04:59,200 --> 00:05:03,520
you always have to create a pod that is

00:05:01,150 --> 00:05:04,870
running one or more containers not

00:05:03,520 --> 00:05:09,160
several containers that's pretty much

00:05:04,870 --> 00:05:12,060
useless so every pod has its own RFC

00:05:09,160 --> 00:05:14,740
1918 ipv4 address so ten point oh point

00:05:12,060 --> 00:05:17,470
something you can just configure which

00:05:14,740 --> 00:05:19,450
range needs to be used and all of the

00:05:17,470 --> 00:05:21,820
processes that are running in like the

00:05:19,450 --> 00:05:23,800
containers in that pod all have to make

00:05:21,820 --> 00:05:25,090
use of that single IP address so you

00:05:23,800 --> 00:05:26,080
can't have two containers that both

00:05:25,090 --> 00:05:27,940
listen on port 80

00:05:26,080 --> 00:05:31,630
they must be listening on a different

00:05:27,940 --> 00:05:33,520
port then you're often like when you do

00:05:31,630 --> 00:05:38,110
cluster computer you want to start a

00:05:33,520 --> 00:05:40,510
whole bunch of pods that are well pretty

00:05:38,110 --> 00:05:41,919
much identical to each other so for

00:05:40,510 --> 00:05:43,780
example you've got a web application

00:05:41,919 --> 00:05:46,150
that you want to spot like spin up ten

00:05:43,780 --> 00:05:48,130
twenty times maybe a hundred times maybe

00:05:46,150 --> 00:05:49,750
ten thousand times it all needs to be

00:05:48,130 --> 00:05:51,910
sort of the same template of a part that

00:05:49,750 --> 00:05:54,310
you want to start and this is what

00:05:51,910 --> 00:05:56,950
kubernetes calls a deployment so a

00:05:54,310 --> 00:05:58,720
deployment is sort of a just literally a

00:05:56,950 --> 00:06:00,130
template for what a pop should look like

00:05:58,720 --> 00:06:02,500
and then you're just saying - kubernetes

00:06:00,130 --> 00:06:04,990
i want to start like a hundred thousand

00:06:02,500 --> 00:06:07,120
of those and it will well if you only

00:06:04,990 --> 00:06:08,620
have a small number of servers be you

00:06:07,120 --> 00:06:10,479
know won't be able to schedule it but if

00:06:08,620 --> 00:06:13,419
you pick a same number then it will spin

00:06:10,479 --> 00:06:15,580
it up properly so all of these objects

00:06:13,419 --> 00:06:17,830
are configured through Jason or yamo

00:06:15,580 --> 00:06:19,300
files it doesn't really matter you can

00:06:17,830 --> 00:06:21,850
write Jason you can write Jamo

00:06:19,300 --> 00:06:24,039
kubernetes accept both jamol's a bit

00:06:21,850 --> 00:06:25,660
more easy to read because the files tend

00:06:24,039 --> 00:06:28,060
to be rather big and then just having

00:06:25,660 --> 00:06:32,280
like Jason all smash them to one line

00:06:28,060 --> 00:06:36,880
that's pretty much unreadable so oh wait

00:06:32,280 --> 00:06:39,750
today it's crawling instead of going to

00:06:36,880 --> 00:06:39,750
the next slide properly

00:06:39,820 --> 00:06:44,500
yes so here's a picture of what a simple

00:06:42,400 --> 00:06:45,940
cluster might look like so this is a

00:06:44,500 --> 00:06:48,250
cluster consisting of three different

00:06:45,940 --> 00:06:52,720
nodes and these three different nodes

00:06:48,250 --> 00:06:54,130
aren't running four pods and these were

00:06:52,720 --> 00:06:56,910
instantiated by two different

00:06:54,130 --> 00:06:59,230
deployments one of them is called DB

00:06:56,910 --> 00:07:01,480
simple deployment that spawns like a

00:06:59,230 --> 00:07:03,130
maybe a my sequel container that just

00:07:01,480 --> 00:07:05,500
serves incoming requests you know

00:07:03,130 --> 00:07:07,060
handles sequel queries and maybe you

00:07:05,500 --> 00:07:10,510
have some kind of maybe background

00:07:07,060 --> 00:07:12,370
scrubbing fsck like job that I don't

00:07:10,510 --> 00:07:14,290
know you know my sequel doesn't eat this

00:07:12,370 --> 00:07:15,970
but if you would have maybe a sort of

00:07:14,290 --> 00:07:17,410
somewhat more complex database system

00:07:15,970 --> 00:07:19,270
you might have some kind of background

00:07:17,410 --> 00:07:21,610
scrubbing job that scrapes over the data

00:07:19,270 --> 00:07:23,800
set and removes prunes that data for

00:07:21,610 --> 00:07:25,450
example and then there's the second

00:07:23,800 --> 00:07:27,700
deployment at the top that sort of being

00:07:25,450 --> 00:07:30,760
spread out across multiple nodes in the

00:07:27,700 --> 00:07:33,070
cluster and this deployment was called

00:07:30,760 --> 00:07:35,380
www and contains one container engine

00:07:33,070 --> 00:07:37,690
called nginx and what happens if you

00:07:35,380 --> 00:07:40,030
spawn this up then it creates multiple

00:07:37,690 --> 00:07:42,010
pods but because all pods have to have a

00:07:40,030 --> 00:07:44,169
unique name it adds some Sun garbage at

00:07:42,010 --> 00:07:48,640
the end so you see that one pot was

00:07:44,169 --> 00:07:51,669
called wwa for DD etc some kind of

00:07:48,640 --> 00:07:53,260
random hash you know it's it just needs

00:07:51,669 --> 00:07:55,120
to add something to make it unique so

00:07:53,260 --> 00:07:57,070
that's where all that random garbage

00:07:55,120 --> 00:07:59,020
comes from if you take a look at a

00:07:57,070 --> 00:08:01,390
kubernetes cluster at the pots that are

00:07:59,020 --> 00:08:02,530
running so on all of the nodes in the

00:08:01,390 --> 00:08:04,270
cluster you have a process running

00:08:02,530 --> 00:08:07,120
called a couplet and a couplet is

00:08:04,270 --> 00:08:09,669
basically a tool that looks at like the

00:08:07,120 --> 00:08:11,440
stuff stored in the API server so the

00:08:09,669 --> 00:08:12,640
API service sort of like a database that

00:08:11,440 --> 00:08:15,250
keeps track of everything that needs to

00:08:12,640 --> 00:08:16,840
be running and sort of compares against

00:08:15,250 --> 00:08:18,430
what's running on the system itself and

00:08:16,840 --> 00:08:21,400
if there's any discrepancies it just

00:08:18,430 --> 00:08:23,500
spins up more containers and reports

00:08:21,400 --> 00:08:26,979
back status on whether that's successful

00:08:23,500 --> 00:08:28,930
or not so by default the cube API server

00:08:26,979 --> 00:08:30,820
doesn't have any event loops in it it's

00:08:28,930 --> 00:08:32,080
just a sort of static server and the

00:08:30,820 --> 00:08:33,909
only thing it can do is it can keep

00:08:32,080 --> 00:08:36,460
track of what needs to be running on the

00:08:33,909 --> 00:08:38,789
cluster so in order to spawn up jobs

00:08:36,460 --> 00:08:41,169
properly you need to run to other jobs

00:08:38,789 --> 00:08:43,510
somewhere they can even be run on the

00:08:41,169 --> 00:08:44,770
cluster somehow but you can also run on

00:08:43,510 --> 00:08:46,810
a separate server doesn't really matter

00:08:44,770 --> 00:08:48,459
and one of them is called the cube

00:08:46,810 --> 00:08:51,280
scheduler and it takes a look at all of

00:08:48,459 --> 00:08:53,140
the pods that that are sort of

00:08:51,280 --> 00:08:55,180
registered in the API server

00:08:53,140 --> 00:08:57,250
and looks at the ones that are not being

00:08:55,180 --> 00:09:00,300
scheduled on the note right now and then

00:08:57,250 --> 00:09:02,740
takes a note like it has some kind of

00:09:00,300 --> 00:09:04,270
bin packing algorithm or some kind of

00:09:02,740 --> 00:09:06,250
algorithm in place to determine what's

00:09:04,270 --> 00:09:08,560
like the best place to run a certain

00:09:06,250 --> 00:09:09,850
node and then there's not a note called

00:09:08,560 --> 00:09:12,370
the queue controller manager and that

00:09:09,850 --> 00:09:14,650
one is sort of also responsible for like

00:09:12,370 --> 00:09:17,260
all the miscellaneous event loops

00:09:14,650 --> 00:09:18,760
actions that need to be run on nodes in

00:09:17,260 --> 00:09:21,580
the cluster so for example this is the

00:09:18,760 --> 00:09:24,250
job that hands out IP addresses on the

00:09:21,580 --> 00:09:25,810
cluster so this one this job basically

00:09:24,250 --> 00:09:27,490
says like you know it gets a list from

00:09:25,810 --> 00:09:29,680
the API server of all the nodes and says

00:09:27,490 --> 00:09:35,170
this specific node in the cluster needs

00:09:29,680 --> 00:09:36,820
to make use of this IP or ipv4 range so

00:09:35,170 --> 00:09:38,650
here's a simple example of how you would

00:09:36,820 --> 00:09:40,450
like spawn a simple part on the cluster

00:09:38,650 --> 00:09:42,700
so not using a deployment but just a

00:09:40,450 --> 00:09:44,860
single set of containers that needs to

00:09:42,700 --> 00:09:46,840
be scheduled on some node you just write

00:09:44,860 --> 00:09:48,340
it a yamo file where you say like this

00:09:46,840 --> 00:09:49,840
yellow file declares a pod

00:09:48,340 --> 00:09:53,650
it doesn't declare a deployment but just

00:09:49,840 --> 00:09:55,120
a part and this part should consist of

00:09:53,650 --> 00:09:57,940
these containers over here in this case

00:09:55,120 --> 00:10:00,160
just a simple nginx container so in this

00:09:57,940 --> 00:10:01,780
case you know for the image you can

00:10:00,160 --> 00:10:04,030
specify a very simple name if you just

00:10:01,780 --> 00:10:07,180
specify a plain name like this it will

00:10:04,030 --> 00:10:08,860
go to docker hub and just download the

00:10:07,180 --> 00:10:11,170
image under that name you can also

00:10:08,860 --> 00:10:14,020
specify a full URL and then it points to

00:10:11,170 --> 00:10:19,450
your own on premise container registry

00:10:14,020 --> 00:10:21,190
service so another in sort of important

00:10:19,450 --> 00:10:24,730
aspect of of kubernetes is how they do

00:10:21,190 --> 00:10:26,050
networking this is again done using like

00:10:24,730 --> 00:10:28,300
some separate tools some separate

00:10:26,050 --> 00:10:31,420
concepts one of them is called surfaces

00:10:28,300 --> 00:10:33,370
and a surface is basically a an object

00:10:31,420 --> 00:10:37,090
you also register in humanities and it's

00:10:33,370 --> 00:10:39,430
sort of a match a matching on parts that

00:10:37,090 --> 00:10:41,740
sort of says like these parts they

00:10:39,430 --> 00:10:43,150
together form some kind of like uniform

00:10:41,740 --> 00:10:46,690
surface that needs to receive load

00:10:43,150 --> 00:10:49,870
balanced traffic and if you create a

00:10:46,690 --> 00:10:52,090
service it adds like an additional ipv4

00:10:49,870 --> 00:10:54,070
address to the cluster and if you

00:10:52,090 --> 00:10:55,570
communicate over that ipv4 address you

00:10:54,070 --> 00:10:57,700
don't end up on a single part but you

00:10:55,570 --> 00:10:59,070
end up being load balanced across all of

00:10:57,700 --> 00:11:01,270
the parts that are part of that service

00:10:59,070 --> 00:11:02,710
to make all of that work there are two

00:11:01,270 --> 00:11:04,480
separate daemons that you also need to

00:11:02,710 --> 00:11:06,480
run on your system one of them is called

00:11:04,480 --> 00:11:08,550
MQ proxy

00:11:06,480 --> 00:11:11,040
q proxies not really a proxy but it's a

00:11:08,550 --> 00:11:12,870
tool that basically scrapes the state

00:11:11,040 --> 00:11:15,090
from the API server on which services

00:11:12,870 --> 00:11:19,200
are being registered and generates like

00:11:15,090 --> 00:11:23,340
a whole bunch of IP tables rules to do

00:11:19,200 --> 00:11:24,690
load balancing across the nodes so yeah

00:11:23,340 --> 00:11:26,430
this is just a job that runs in the

00:11:24,690 --> 00:11:29,940
background on this on your server you

00:11:26,430 --> 00:11:31,440
can you could run it through kubernetes

00:11:29,940 --> 00:11:33,990
as well through some hacks or you could

00:11:31,440 --> 00:11:36,510
just set it up with an init script on

00:11:33,990 --> 00:11:38,130
the node itself to run on startup then

00:11:36,510 --> 00:11:40,710
there's another service called cube DNS

00:11:38,130 --> 00:11:43,950
and what that thing does is it allows

00:11:40,710 --> 00:11:45,300
you to resolve services by name so in

00:11:43,950 --> 00:11:48,690
the case of that web server if I would

00:11:45,300 --> 00:11:52,610
create a service called www then cube

00:11:48,690 --> 00:11:55,380
DNS allows you to resolve the hostname

00:11:52,610 --> 00:11:59,460
wwm spaced or clustered up local or with

00:11:55,380 --> 00:12:02,810
some kind of suffix to the RFC 1918

00:11:59,460 --> 00:12:02,810
address you have a question

00:12:19,630 --> 00:12:31,269
so the question is is there a reason why

00:12:21,759 --> 00:12:33,490
it's an RFC 19 address yeah so you need

00:12:31,269 --> 00:12:35,019
to use an RFC 1918 address here because

00:12:33,490 --> 00:12:37,480
the entire idea behind kubernetes is

00:12:35,019 --> 00:12:40,389
that you want the cluster to be like

00:12:37,480 --> 00:12:42,880
like internal you shouldn't be like

00:12:40,389 --> 00:12:44,589
literally exposing a cluster with all of

00:12:42,880 --> 00:12:47,560
its internal addresses to the public

00:12:44,589 --> 00:12:48,910
internet directly why it's using ipv4

00:12:47,560 --> 00:12:50,740
it's mainly because the Google

00:12:48,910 --> 00:12:53,350
developers are too lazy to add ipv6

00:12:50,740 --> 00:12:55,089
support that's that's the main reason

00:12:53,350 --> 00:12:57,040
there is a ticket open on github for

00:12:55,089 --> 00:12:58,980
adding ipv6 support and it's been

00:12:57,040 --> 00:13:03,370
opening for open for a couple years so

00:12:58,980 --> 00:13:06,579
they're still using ipv4 yeah it's like

00:13:03,370 --> 00:13:11,680
why why are bananas curved that's little

00:13:06,579 --> 00:13:13,959
we can do about it right so then this is

00:13:11,680 --> 00:13:15,730
only about like how sort of cluster

00:13:13,959 --> 00:13:17,620
internal trick metric traffic works a

00:13:15,730 --> 00:13:19,480
very important thing is how do you get

00:13:17,620 --> 00:13:21,339
external traffic coming to the close to

00:13:19,480 --> 00:13:22,750
the cluster well they have some separate

00:13:21,339 --> 00:13:24,730
concepts for that which I'm not going to

00:13:22,750 --> 00:13:27,310
discuss today called ingress controllers

00:13:24,730 --> 00:13:31,000
and this allows you to spawn jobs that

00:13:27,310 --> 00:13:33,279
take traffic from public ipv4 ipv6

00:13:31,000 --> 00:13:38,139
addresses and route it into the cluster

00:13:33,279 --> 00:13:39,639
over the internal ipv4 addresses so what

00:13:38,139 --> 00:13:41,699
are the weaknesses of kubernetes I'm

00:13:39,639 --> 00:13:45,100
first going to start off with that

00:13:41,699 --> 00:13:47,319
so the networking ipv4 mentioned it

00:13:45,100 --> 00:13:50,019
before it's a shame that it's not using

00:13:47,319 --> 00:13:52,420
ipv6 also the fact that it's allocating

00:13:50,019 --> 00:13:55,029
a single slash 24 for every node to like

00:13:52,420 --> 00:13:57,519
schedule like to four addresses to

00:13:55,029 --> 00:14:01,389
attach to pods you may actually run out

00:13:57,519 --> 00:14:05,860
of addresses quite quickly I mean say if

00:14:01,389 --> 00:14:09,130
you own a cluster with 256 nodes you're

00:14:05,860 --> 00:14:10,509
already using up a 16 just for yeah

00:14:09,130 --> 00:14:13,630
running some parts which is quite

00:14:10,509 --> 00:14:18,189
aggressive another problem you have is

00:14:13,630 --> 00:14:22,209
because there's quite a sort of they

00:14:18,189 --> 00:14:24,040
make use of NAT proxying all the kind of

00:14:22,209 --> 00:14:25,509
stuff to make load balancing work it's

00:14:24,040 --> 00:14:28,389
actually pretty hard to trace traffic

00:14:25,509 --> 00:14:30,550
properly so some kind of front-end job a

00:14:28,389 --> 00:14:32,559
web service doing that like a sequel

00:14:30,550 --> 00:14:33,520
sending a sequel statement over to a

00:14:32,559 --> 00:14:36,100
database back-end but

00:14:33,520 --> 00:14:37,720
it's not working properly I mean how do

00:14:36,100 --> 00:14:40,330
we know which server the traffic was

00:14:37,720 --> 00:14:44,500
sent to you only see traffic going from

00:14:40,330 --> 00:14:46,810
the pods IP address to a service address

00:14:44,500 --> 00:14:48,370
of the database system and how do you

00:14:46,810 --> 00:14:50,380
know which database back-end was being

00:14:48,370 --> 00:14:52,630
used well you see that a lot of people

00:14:50,380 --> 00:14:53,860
adds all sorts of hacks you know for

00:14:52,630 --> 00:14:55,360
example if your back-end is the web

00:14:53,860 --> 00:14:58,210
service you add some kind of HTTP

00:14:55,360 --> 00:14:59,710
response header to know which back-end

00:14:58,210 --> 00:15:02,290
you're making use of which is with

00:14:59,710 --> 00:15:03,760
sloppy in my opinion something we've

00:15:02,290 --> 00:15:05,350
noticed that cumin aware we're making

00:15:03,760 --> 00:15:07,870
use of kubernetes in combination with

00:15:05,350 --> 00:15:09,760
docker on linux is q proxy can actually

00:15:07,870 --> 00:15:12,310
get stale and then you sometimes see

00:15:09,760 --> 00:15:13,900
traffic being misrouted to like random

00:15:12,310 --> 00:15:15,760
other backends in the cluster

00:15:13,900 --> 00:15:17,710
you know addresses are being reused

00:15:15,760 --> 00:15:19,300
quite aggressively in the cluster so

00:15:17,710 --> 00:15:21,310
there's one time we actually at a

00:15:19,300 --> 00:15:23,370
problem where we were like restarting

00:15:21,310 --> 00:15:26,530
some jobs on the cluster and then we saw

00:15:23,370 --> 00:15:29,680
user traffic actually ending up on the

00:15:26,530 --> 00:15:31,420
staging setup even on the wrong job it's

00:15:29,680 --> 00:15:33,220
just because they're reusing addresses

00:15:31,420 --> 00:15:35,920
so aggressively traffic and can go to

00:15:33,220 --> 00:15:37,420
the wrong jobs initially kubernetes

00:15:35,920 --> 00:15:39,040
didn't have any support for network

00:15:37,420 --> 00:15:40,630
policy so all of the containers running

00:15:39,040 --> 00:15:42,910
on the cluster could basically chat with

00:15:40,630 --> 00:15:45,880
each other so security was was pretty

00:15:42,910 --> 00:15:48,130
bad they later on solved this by adding

00:15:45,880 --> 00:15:50,440
network policy support but that can only

00:15:48,130 --> 00:15:52,960
limit incoming traffic so you can't

00:15:50,440 --> 00:15:55,000
specify which traffic a container can

00:15:52,960 --> 00:15:56,950
generate you can only specify which

00:15:55,000 --> 00:15:59,890
traffic a container can receive which is

00:15:56,950 --> 00:16:02,440
somewhat of an improvement but still not

00:15:59,890 --> 00:16:05,200
ideal so if you're using sort of

00:16:02,440 --> 00:16:08,230
kubernetes to run a multi-tenant cluster

00:16:05,200 --> 00:16:12,640
I would strongly advise against doing

00:16:08,230 --> 00:16:14,290
that it's not pretty wise also like the

00:16:12,640 --> 00:16:17,230
computing side so just ignoring the

00:16:14,290 --> 00:16:19,270
networking for a minute containers they

00:16:17,230 --> 00:16:21,310
don't really take the complexity away so

00:16:19,270 --> 00:16:25,900
every container is a full UNIX

00:16:21,310 --> 00:16:28,360
environment having all of its 1980s

00:16:25,900 --> 00:16:30,220
unique features in it and what I've

00:16:28,360 --> 00:16:31,930
noticed is we we sometimes get like

00:16:30,220 --> 00:16:33,640
junior systems administrators and we can

00:16:31,930 --> 00:16:35,890
like really easily explain to them how

00:16:33,640 --> 00:16:37,930
kubernetes works you know you just run a

00:16:35,890 --> 00:16:39,400
single command you could spawn a dozen

00:16:37,930 --> 00:16:43,030
jobs but then you sort of have to help

00:16:39,400 --> 00:16:45,610
them debug unix issues explaining them

00:16:43,030 --> 00:16:46,840
why standard out on the container needs

00:16:45,610 --> 00:16:48,330
to or on the job

00:16:46,840 --> 00:16:51,430
running in container needs to be made

00:16:48,330 --> 00:16:52,720
unbuffered otherwise messages don't end

00:16:51,430 --> 00:16:56,020
up in logs all that kind of nonsense

00:16:52,720 --> 00:16:58,330
that sort of remains and it's sort of

00:16:56,020 --> 00:17:00,580
slightly annoying also the attack

00:16:58,330 --> 00:17:02,380
surface with the kernels quite huge you

00:17:00,580 --> 00:17:04,900
know it's a linux is an operating system

00:17:02,380 --> 00:17:06,910
that supports what 300 400 system calls

00:17:04,900 --> 00:17:09,160
and all of those need to be container

00:17:06,910 --> 00:17:10,990
aware you have special file systems like

00:17:09,160 --> 00:17:13,360
sash proc being mounted in containers

00:17:10,990 --> 00:17:15,670
and you know Linux proc is polluted with

00:17:13,360 --> 00:17:16,960
a lot of random files you need to sort

00:17:15,670 --> 00:17:22,380
of be certain that there's nothing in

00:17:16,960 --> 00:17:25,650
there that should remain hidden so again

00:17:22,380 --> 00:17:27,700
for running completely untrusted jobs

00:17:25,650 --> 00:17:30,460
you know for building a cloud computing

00:17:27,700 --> 00:17:32,430
service and using kubernetes in the

00:17:30,460 --> 00:17:34,900
backend I actually wouldn't you know

00:17:32,430 --> 00:17:36,400
think it's secure enough to operate a

00:17:34,900 --> 00:17:40,390
single cluster where you're running jobs

00:17:36,400 --> 00:17:42,340
for for multiple customers and also like

00:17:40,390 --> 00:17:44,170
the final thing that I think is quite a

00:17:42,340 --> 00:17:46,150
weakness of kubernetes or containers in

00:17:44,170 --> 00:17:47,710
general said it actually creates a sort

00:17:46,150 --> 00:17:49,960
of cargo called programming culture

00:17:47,710 --> 00:17:52,030
where a lot of food containers need to

00:17:49,960 --> 00:17:54,130
copy paste a lot of garbage over to make

00:17:52,030 --> 00:17:55,270
it work you know docker files at a

00:17:54,130 --> 00:17:58,660
hundred lines long

00:17:55,270 --> 00:18:00,430
shell scripts that that are only useful

00:17:58,660 --> 00:18:02,560
for starting up a binary in the end that

00:18:00,430 --> 00:18:04,270
are also a hundred lines long combined

00:18:02,560 --> 00:18:07,090
with docker images there are hundreds of

00:18:04,270 --> 00:18:10,930
megabytes in size containing a whole set

00:18:07,090 --> 00:18:12,790
of like G lips EE core utils only to run

00:18:10,930 --> 00:18:16,480
a very simple web application I mean

00:18:12,790 --> 00:18:18,970
that's just copy/paste programming waste

00:18:16,480 --> 00:18:21,310
a lot of this space adds a lot of

00:18:18,970 --> 00:18:22,930
security issues of course it's it should

00:18:21,310 --> 00:18:24,970
be a lot simpler people should just

00:18:22,930 --> 00:18:26,770
write a small web application just write

00:18:24,970 --> 00:18:28,240
some code and then just press the play

00:18:26,770 --> 00:18:30,600
button and run it on their cluster they

00:18:28,240 --> 00:18:34,180
shouldn't be thinking about you know

00:18:30,600 --> 00:18:37,150
just running entire Linux distros inside

00:18:34,180 --> 00:18:37,930
of containers still there are some

00:18:37,150 --> 00:18:40,650
things that are pretty good about

00:18:37,930 --> 00:18:43,000
communities it all works quite reliable

00:18:40,650 --> 00:18:46,600
the automatic rescheduling works pretty

00:18:43,000 --> 00:18:48,010
well we use that kumon ah and we we

00:18:46,600 --> 00:18:49,510
don't get paged in the middle of the

00:18:48,010 --> 00:18:52,360
night at often anymore you know whenever

00:18:49,510 --> 00:18:54,430
a system crashes you know some some disk

00:18:52,360 --> 00:18:56,500
breaks down or something goes wrong with

00:18:54,430 --> 00:18:58,720
the like the networking interface unless

00:18:56,500 --> 00:19:00,400
some node it just gets disconnected from

00:18:58,720 --> 00:19:02,350
the rest of the cluster and

00:19:00,400 --> 00:19:03,660
he's just sponsor job on some other node

00:19:02,350 --> 00:19:06,100
in the cluster and everything's alright

00:19:03,660 --> 00:19:10,150
cube CTL the tools really friendly to

00:19:06,100 --> 00:19:12,220
use you know if you've used it for a day

00:19:10,150 --> 00:19:13,450
or so then you basically understand 90%

00:19:12,220 --> 00:19:16,930
of its functionality which is pretty

00:19:13,450 --> 00:19:18,490
good docker hub is also pretty good in

00:19:16,930 --> 00:19:20,140
the sense that you have packages for

00:19:18,490 --> 00:19:21,250
anything you can think of you have dr.

00:19:20,140 --> 00:19:23,890
I'm just for anything you can think of

00:19:21,250 --> 00:19:25,600
and the project so the kuba nice project

00:19:23,890 --> 00:19:27,610
has like a lot of funding and momentum

00:19:25,600 --> 00:19:32,730
behind it it isn't going to disappear

00:19:27,610 --> 00:19:32,730
anytime soon so FreeBSD

00:19:50,430 --> 00:19:54,910
yeah so the question is is there any way

00:19:52,930 --> 00:19:56,110
to properly trace what's going on to

00:19:54,910 --> 00:19:58,480
figure out what's going on when a

00:19:56,110 --> 00:20:02,860
container has failed yes

00:19:58,480 --> 00:20:04,960
so what's pretty cool is that containers

00:20:02,860 --> 00:20:08,110
aren't like the state of a container

00:20:04,960 --> 00:20:10,570
isn't being thrown away immediately you

00:20:08,110 --> 00:20:13,060
can always run cube CTL describe to run

00:20:10,570 --> 00:20:15,430
to like to describe a container that is

00:20:13,060 --> 00:20:17,680
already terminated and then you just get

00:20:15,430 --> 00:20:19,510
sort of a couple of screens of text

00:20:17,680 --> 00:20:21,160
giving all sorts of metadata about the

00:20:19,510 --> 00:20:22,840
container when I was started when it was

00:20:21,160 --> 00:20:25,210
terminated why it was terminated and

00:20:22,840 --> 00:20:27,520
also some log entries related to that so

00:20:25,210 --> 00:20:29,710
those are not log entries generated by

00:20:27,520 --> 00:20:31,390
the program but log entries generated by

00:20:29,710 --> 00:20:33,820
kubernetes in the process of starting

00:20:31,390 --> 00:20:36,070
and tearing down that specific container

00:20:33,820 --> 00:20:37,780
in addition to that there's also a

00:20:36,070 --> 00:20:40,480
logging facility in community so you can

00:20:37,780 --> 00:20:42,370
run cube CTL logs and then the name of a

00:20:40,480 --> 00:20:44,320
pod and then you can actually take a

00:20:42,370 --> 00:20:47,680
look at the pod standard out standard

00:20:44,320 --> 00:20:49,840
error and it also has flags like - f + -

00:20:47,680 --> 00:20:51,490
- time stamps to prefix timestamps to

00:20:49,840 --> 00:20:55,300
the output and also follow the output

00:20:51,490 --> 00:20:58,180
while it's being generated so at first

00:20:55,300 --> 00:20:59,170
you when you start using it you might

00:20:58,180 --> 00:21:01,900
have the feeling that you're sort of

00:20:59,170 --> 00:21:04,090
losing control and that things might be

00:21:01,900 --> 00:21:06,730
coming non-transparent but in practice I

00:21:04,090 --> 00:21:08,800
haven't really run into those issues a

00:21:06,730 --> 00:21:11,290
lot I mean you get you sued after some

00:21:08,800 --> 00:21:13,840
time instead of browsing through far log

00:21:11,290 --> 00:21:15,790
you now have to use cube CTL and that's

00:21:13,840 --> 00:21:18,910
that that's all pretty well it's

00:21:15,790 --> 00:21:22,570
perfectly manageable did that answer

00:21:18,910 --> 00:21:23,680
your question ok so now the first

00:21:22,570 --> 00:21:26,340
question we should be asking ourselves

00:21:23,680 --> 00:21:30,070
could we port kubernetes through freebsd

00:21:26,340 --> 00:21:31,570
well we likely could I mean some people

00:21:30,070 --> 00:21:34,540
are already working on getting docker

00:21:31,570 --> 00:21:37,690
working on freebsd and I heard from some

00:21:34,540 --> 00:21:39,370
people that it's a hack job but some

00:21:37,690 --> 00:21:40,780
other people are will probably step up

00:21:39,370 --> 00:21:42,640
to clean it up and then it might work

00:21:40,780 --> 00:21:43,990
after some time we could also make

00:21:42,640 --> 00:21:46,660
combat Linux and the kernel more

00:21:43,990 --> 00:21:48,250
complete to be more linux like so jobs

00:21:46,660 --> 00:21:51,100
running in a container have less of an

00:21:48,250 --> 00:21:52,720
idea that they're running on like some

00:21:51,100 --> 00:21:53,650
broken version of Linux but actually

00:21:52,720 --> 00:21:56,440
think that they're running on the real

00:21:53,650 --> 00:21:58,360
version of Linux we could maybe even

00:21:56,440 --> 00:22:00,670
adopt some more Linux specific

00:21:58,360 --> 00:22:04,560
frameworks like C groups -

00:22:00,670 --> 00:22:09,970
make resource limiting and network

00:22:04,560 --> 00:22:11,620
policies like make it work now well we

00:22:09,970 --> 00:22:13,750
could also maybe extend communities to

00:22:11,620 --> 00:22:15,580
support PF and certify PE tables to do

00:22:13,750 --> 00:22:17,440
the networking so there's like a whole

00:22:15,580 --> 00:22:18,940
bunch of options all other things we can

00:22:17,440 --> 00:22:21,070
do to make kubernetes work on freebsd

00:22:18,940 --> 00:22:23,070
but next slide

00:22:21,070 --> 00:22:25,960
should we port kubernetes to freebsd

00:22:23,070 --> 00:22:27,820
well consider this discussion somebody

00:22:25,960 --> 00:22:30,190
like from the linux world says like why

00:22:27,820 --> 00:22:31,990
bother I'm going to startup Linux based

00:22:30,190 --> 00:22:34,180
containers anyway well then your answer

00:22:31,990 --> 00:22:35,950
would be yes but at least our cluster is

00:22:34,180 --> 00:22:39,250
based on FreeBSD which is awesome it's

00:22:35,950 --> 00:22:40,330
BSD tech BSD license yada yada then the

00:22:39,250 --> 00:22:41,890
other person says like what's the

00:22:40,330 --> 00:22:43,780
advantage of that and then you say well

00:22:41,890 --> 00:22:46,030
in practice not much and then are there

00:22:43,780 --> 00:22:48,520
any disadvantages well yeah the jobs

00:22:46,030 --> 00:22:50,290
crash every couple of hours and you know

00:22:48,520 --> 00:22:52,750
this doesn't work and you know this

00:22:50,290 --> 00:22:54,400
nodejs container or something popular no

00:22:52,750 --> 00:22:55,780
doesn't work no you can't run rusts

00:22:54,400 --> 00:22:58,840
programs because we haven't implemented

00:22:55,780 --> 00:23:00,550
that yet so we could go in that

00:22:58,840 --> 00:23:02,350
direction but I think the end result is

00:23:00,550 --> 00:23:04,540
this will only make BSD look bad and

00:23:02,350 --> 00:23:06,490
also like I'm creative we're only trying

00:23:04,540 --> 00:23:09,270
to follow you catch up with Linux

00:23:06,490 --> 00:23:12,100
instead of doing something awesome so

00:23:09,270 --> 00:23:14,770
what I think we should do instead simply

00:23:12,100 --> 00:23:17,470
accept that people use one to use Linux

00:23:14,770 --> 00:23:20,620
servers to run Linux containers don't

00:23:17,470 --> 00:23:22,300
even try to compete with it so try go

00:23:20,620 --> 00:23:24,760
out of you know don't get into these

00:23:22,300 --> 00:23:26,770
arguments you know that that don't make

00:23:24,760 --> 00:23:28,750
any sense you know where it's just us

00:23:26,770 --> 00:23:30,400
trying to catch up with BSD it's a

00:23:28,750 --> 00:23:33,640
cat-and-mouse game and will always lose

00:23:30,400 --> 00:23:35,860
that instead we should see if we could

00:23:33,640 --> 00:23:37,870
at least integrate with kubernetes in a

00:23:35,860 --> 00:23:40,240
certain way and with that I mean trying

00:23:37,870 --> 00:23:44,680
to see if there is a place for FreeBSD

00:23:40,240 --> 00:23:46,510
nodes inside of a kubernetes cluster see

00:23:44,680 --> 00:23:49,450
if we could come up with like kubernetes

00:23:46,510 --> 00:23:52,480
nodes that actually provides some

00:23:49,450 --> 00:23:55,900
additional value over simple plain

00:23:52,480 --> 00:23:57,970
docker containers focus on niche martens

00:23:55,900 --> 00:24:02,410
markets instead of focusing on like you

00:23:57,970 --> 00:24:04,210
know that 90% of users focus on like the

00:24:02,410 --> 00:24:05,980
10% of companies that actually do

00:24:04,210 --> 00:24:07,930
software development in-house and start

00:24:05,980 --> 00:24:10,630
of firing a plain Apache and nginx

00:24:07,930 --> 00:24:11,800
containers and actually try to appease

00:24:10,630 --> 00:24:13,779
those kinds of people

00:24:11,800 --> 00:24:15,309
so in

00:24:13,779 --> 00:24:18,190
process we should try to tackle the

00:24:15,309 --> 00:24:20,200
weaknesses that too Vanitas on on Linux

00:24:18,190 --> 00:24:22,179
has and with that I mean try to improve

00:24:20,200 --> 00:24:24,519
security and try to make things more

00:24:22,179 --> 00:24:26,529
minimal you know development headcount

00:24:24,519 --> 00:24:27,759
on FreeBSD will always remain less than

00:24:26,529 --> 00:24:30,219
Linux or at least for the foreseeable

00:24:27,759 --> 00:24:33,700
future so we should try to keep things

00:24:30,219 --> 00:24:35,499
simple and you know not add a lot of

00:24:33,700 --> 00:24:39,340
garbage that the people in the end don't

00:24:35,499 --> 00:24:43,299
use just yes yeah kiss that's basically

00:24:39,340 --> 00:24:45,639
what I want so now I'm going to talk

00:24:43,299 --> 00:24:46,869
about cloudy API and you know later on

00:24:45,639 --> 00:24:48,729
discussed where I think the cloud be

00:24:46,869 --> 00:24:51,399
quite a bi face it fits in this picture

00:24:48,729 --> 00:24:53,349
so this is sort of like a reap recap of

00:24:51,399 --> 00:24:55,929
my cloud ABI talks that I've given in

00:24:53,349 --> 00:24:57,940
the past so cloud EBI is sort of a

00:24:55,929 --> 00:25:01,359
heavily stripped POSIX like programming

00:24:57,940 --> 00:25:03,849
environment and the entire goal behind

00:25:01,359 --> 00:25:06,339
it is to sort of make programs behave

00:25:03,849 --> 00:25:08,259
like black boxes so programs can't just

00:25:06,339 --> 00:25:10,839
open arbitrary paths on disk they can't

00:25:08,259 --> 00:25:12,519
create arbitrary network connections to

00:25:10,839 --> 00:25:15,489
the outside world they're really just

00:25:12,519 --> 00:25:17,589
black boxes that need to be plugged in

00:25:15,489 --> 00:25:19,809
before they can be started that's sort

00:25:17,589 --> 00:25:21,460
of the entire goal behind it so these

00:25:19,809 --> 00:25:23,379
dependencies on the outside world are

00:25:21,460 --> 00:25:25,149
expressed as file descriptors so for

00:25:23,379 --> 00:25:27,369
example if you want to make a program

00:25:25,149 --> 00:25:29,529
communicate with the network then you

00:25:27,369 --> 00:25:31,779
must make sure that you start it up with

00:25:29,529 --> 00:25:33,489
a socket injected into it if you want

00:25:31,779 --> 00:25:35,109
this program to access parts of the file

00:25:33,489 --> 00:25:36,609
system that you need to inject file

00:25:35,109 --> 00:25:39,099
descriptors of directories into it to

00:25:36,609 --> 00:25:41,289
make it work so all in all you can think

00:25:39,099 --> 00:25:43,359
of it as a sort of capsicum like

00:25:41,289 --> 00:25:44,710
programming environment you the only

00:25:43,359 --> 00:25:47,710
difference is that capsicum is

00:25:44,710 --> 00:25:49,539
essentially always turned on well this

00:25:47,710 --> 00:25:51,820
model has a couple of advantages so

00:25:49,539 --> 00:25:53,200
first of all one thing that's not on the

00:25:51,820 --> 00:25:54,759
slide but what was pretty awesome it

00:25:53,200 --> 00:25:56,649
said it makes it easier to port software

00:25:54,759 --> 00:25:58,210
over because all of the features that

00:25:56,649 --> 00:26:00,249
are incompatible with capsicum etham is

00:25:58,210 --> 00:26:02,710
stripped out you can just compile your

00:26:00,249 --> 00:26:04,419
software against cloud ABI 90% chance it

00:26:02,710 --> 00:26:05,799
won't build but then you at least have

00:26:04,419 --> 00:26:07,239
sort of an inventory of all the things

00:26:05,799 --> 00:26:07,899
that need to be patched up to make it

00:26:07,239 --> 00:26:10,179
work properly

00:26:07,899 --> 00:26:11,859
whereas with capsicum everything just

00:26:10,179 --> 00:26:14,469
builds out of the box but as soon as you

00:26:11,859 --> 00:26:16,570
started it it doesn't work because you

00:26:14,469 --> 00:26:18,210
know the the sandboxing that you're

00:26:16,570 --> 00:26:23,379
trying to apply to it is far too strong

00:26:18,210 --> 00:26:25,479
so these programs can be like really

00:26:23,379 --> 00:26:27,490
tightly sandboxed they can also easily

00:26:25,479 --> 00:26:29,650
be tested because the nice thing is

00:26:27,490 --> 00:26:31,120
they don't try to sort of open arbitrary

00:26:29,650 --> 00:26:32,679
paths on disk you really have to sort of

00:26:31,120 --> 00:26:34,450
feed them thousand directories that they

00:26:32,679 --> 00:26:37,150
can use so if you want to instantiate

00:26:34,450 --> 00:26:38,440
cloud API programs multiple times well

00:26:37,150 --> 00:26:40,390
it's pretty easy to just inject

00:26:38,440 --> 00:26:41,770
different kinds of resources and you can

00:26:40,390 --> 00:26:44,410
be quite certain that those programs

00:26:41,770 --> 00:26:46,450
won't conflict in any way and also

00:26:44,410 --> 00:26:47,920
because all of the dependencies are sort

00:26:46,450 --> 00:26:50,020
of known upfront you can deploy them a

00:26:47,920 --> 00:26:51,730
lot easier because you know you sort of

00:26:50,020 --> 00:26:54,309
have an inventory for what kind of

00:26:51,730 --> 00:26:56,410
things they use so on the next slide

00:26:54,309 --> 00:26:59,470
there's an example a simple C++ web

00:26:56,410 --> 00:27:01,090
server that is built on top of cloud ABI

00:26:59,470 --> 00:27:03,700
some of the includes have been left away

00:27:01,090 --> 00:27:05,860
and some of the details missing but it

00:27:03,700 --> 00:27:08,080
just consists of two parts so first of

00:27:05,860 --> 00:27:09,910
all there and instead of trying to open

00:27:08,080 --> 00:27:12,550
like network sockets and directories

00:27:09,910 --> 00:27:14,260
directly it gets it out of a structure

00:27:12,550 --> 00:27:15,550
called the art data so the entry point

00:27:14,260 --> 00:27:18,010
no longer has string command line

00:27:15,550 --> 00:27:19,630
arguments it sort of has a llamo like

00:27:18,010 --> 00:27:21,520
tree of attributes that can be passed

00:27:19,630 --> 00:27:23,200
into the program and a neat thing is

00:27:21,520 --> 00:27:25,540
that file descriptors can be attached to

00:27:23,200 --> 00:27:28,929
this tree so it's not just like plain

00:27:25,540 --> 00:27:31,179
scalars strings in etc it's also file

00:27:28,929 --> 00:27:33,520
descriptors can be added as attributes

00:27:31,179 --> 00:27:34,929
pushed into the program so this program

00:27:33,520 --> 00:27:37,150
loops over all of the attributes that

00:27:34,929 --> 00:27:39,880
are received and extracts their HTTP

00:27:37,150 --> 00:27:41,110
socket and a root directory so once all

00:27:39,880 --> 00:27:43,090
of that stuff is finished that can go to

00:27:41,110 --> 00:27:44,620
second loop where it starts to process

00:27:43,090 --> 00:27:47,290
incoming network connections using

00:27:44,620 --> 00:27:49,750
except here could be some code to parse

00:27:47,290 --> 00:27:51,370
HTTP GET requests and in the end maybe

00:27:49,750 --> 00:27:53,350
some file needs to be served back to the

00:27:51,370 --> 00:27:58,059
user so it uses OpenNet to open a

00:27:53,350 --> 00:27:59,559
specific file on disk so in this this

00:27:58,059 --> 00:28:01,090
slide sort of explains how this programs

00:27:59,559 --> 00:28:02,230
can be started up first of all we need

00:28:01,090 --> 00:28:03,790
to run them through a separate cross

00:28:02,230 --> 00:28:07,809
compiler that you can stall us from

00:28:03,790 --> 00:28:09,820
FreeBSD ports and once we have it

00:28:07,809 --> 00:28:12,640
compiled we need to load a certain

00:28:09,820 --> 00:28:14,500
kernel module called cloud ABI 64 and

00:28:12,640 --> 00:28:17,230
this adds support for running 64-bit

00:28:14,500 --> 00:28:18,490
cloud ABI processors so of course

00:28:17,230 --> 00:28:20,620
there's also kernel module called cloud

00:28:18,490 --> 00:28:23,350
API 32 that allows you to run the 32 bit

00:28:20,620 --> 00:28:25,000
processes this configuration then

00:28:23,350 --> 00:28:27,730
explains how the program needs to be

00:28:25,000 --> 00:28:30,190
started up this is just some some llamó

00:28:27,730 --> 00:28:32,140
way of referring to certain namespace a

00:28:30,190 --> 00:28:33,820
certain tag namespaces it's called but

00:28:32,140 --> 00:28:36,130
this is like the most interesting part

00:28:33,820 --> 00:28:37,780
what we're saying here is HTTP socket

00:28:36,130 --> 00:28:39,520
needs to be a socket that's bound to

00:28:37,780 --> 00:28:40,990
port 80 and this will sort of be

00:28:39,520 --> 00:28:43,360
replaced by a file descriptor

00:28:40,990 --> 00:28:45,130
and also for the root directory the root

00:28:43,360 --> 00:28:46,750
directory needs to be like replaced by a

00:28:45,130 --> 00:28:48,580
file descriptor and this is sort of all

00:28:46,750 --> 00:28:51,039
opossum to the root to the program when

00:28:48,580 --> 00:28:54,010
we're running cloud ABI run and passing

00:28:51,039 --> 00:28:57,940
in the Yama file so is this clear to

00:28:54,010 --> 00:28:59,710
people you're sitting in the audience so

00:28:57,940 --> 00:29:02,409
what are the changes in cloud EBI since

00:28:59,710 --> 00:29:06,610
2015 the ABI is now formally specified

00:29:02,409 --> 00:29:08,409
we have like a cloudy bi dot exe 2,000

00:29:06,610 --> 00:29:10,090
lines long describing all of the system

00:29:08,409 --> 00:29:11,770
calls and data types and this allows

00:29:10,090 --> 00:29:15,370
people to reuse cloud AV on different

00:29:11,770 --> 00:29:17,380
operating systems so we automatically

00:29:15,370 --> 00:29:19,330
generate C header files from it we could

00:29:17,380 --> 00:29:21,010
even generate bindings for different

00:29:19,330 --> 00:29:25,029
programming languages so for example

00:29:21,010 --> 00:29:26,770
rust etc support for more hardware

00:29:25,029 --> 00:29:29,370
architectures when I first announced

00:29:26,770 --> 00:29:32,130
cloud a B I'd only worked on x86 64

00:29:29,370 --> 00:29:34,299
nowadays it runs on 4 architectures

00:29:32,130 --> 00:29:35,890
FreeBSD 11 has been released in the mean

00:29:34,299 --> 00:29:38,039
time so support for cloud API sort of

00:29:35,890 --> 00:29:40,809
integrated into it just install FreeBSD

00:29:38,039 --> 00:29:42,159
11.0 or 0.1 and then you have some

00:29:40,809 --> 00:29:45,880
proper support for running cloudy API

00:29:42,159 --> 00:29:47,980
software also pretty awesome we can also

00:29:45,880 --> 00:29:49,390
emulate it in userspace nowaday so even

00:29:47,980 --> 00:29:51,039
if your operating system doesn't provide

00:29:49,390 --> 00:29:53,200
native support you can at least run like

00:29:51,039 --> 00:29:55,960
an unsecured version of cloud ABI in

00:29:53,200 --> 00:29:58,450
user space at least test tests around

00:29:55,960 --> 00:30:00,309
with it and also more software has been

00:29:58,450 --> 00:30:01,870
ported in the meantime so if you want to

00:30:00,309 --> 00:30:05,559
know more details about this free BC

00:30:01,870 --> 00:30:06,850
journal from May 2017 has a pretty good

00:30:05,559 --> 00:30:10,960
article on it what has sort of changed

00:30:06,850 --> 00:30:14,110
over time so now sort of with the the

00:30:10,960 --> 00:30:15,610
introduction ahead now we're actually

00:30:14,110 --> 00:30:17,770
going to look at the the interesting

00:30:15,610 --> 00:30:20,020
part like the huge long build-up for all

00:30:17,770 --> 00:30:20,559
of this so now we're going to take

00:30:20,020 --> 00:30:22,539
kubernetes

00:30:20,559 --> 00:30:27,480
and replace Linux with FreeBSD and

00:30:22,539 --> 00:30:27,480
docker with cloud API how did I do it

00:30:31,940 --> 00:30:37,460
so when we add cumin I started using

00:30:35,870 --> 00:30:40,809
kubernetes we were still making use of

00:30:37,460 --> 00:30:44,120
kubernetes 1.3 pretty long time ago and

00:30:40,809 --> 00:30:46,070
back then kubernetes was still pretty

00:30:44,120 --> 00:30:48,350
simple if you sort of looked at how the

00:30:46,070 --> 00:30:50,600
couplet daemon was implemented what it

00:30:48,350 --> 00:30:51,289
is for every pod that needed to be

00:30:50,600 --> 00:30:54,080
created

00:30:51,289 --> 00:30:56,950
it invoked Linux system calls to create

00:30:54,080 --> 00:30:59,360
C groups set up the networking etc and

00:30:56,950 --> 00:31:01,519
then when I wanted to create containers

00:30:59,360 --> 00:31:03,799
it would call into the docker daemon and

00:31:01,519 --> 00:31:05,360
the talker daemon would then download

00:31:03,799 --> 00:31:09,529
the docker image she would specify in

00:31:05,360 --> 00:31:12,470
the in the pod Yama file and once

00:31:09,529 --> 00:31:15,919
downloaded it would spawn containers

00:31:12,470 --> 00:31:19,789
inside of the pods that kubernetes set

00:31:15,919 --> 00:31:21,649
up so there were actually two things

00:31:19,789 --> 00:31:23,360
sort of wrong or annoying about this

00:31:21,649 --> 00:31:25,580
model so first of all it's sort of

00:31:23,360 --> 00:31:27,529
strongly tied against C groups you know

00:31:25,580 --> 00:31:28,970
this contain all sorts of Linux specific

00:31:27,529 --> 00:31:30,710
code to create C groups who set up the

00:31:28,970 --> 00:31:33,500
networking and the Cooper need these

00:31:30,710 --> 00:31:34,909
people weren't happy with that also it's

00:31:33,500 --> 00:31:37,700
strongly dependent on the docker demon

00:31:34,909 --> 00:31:38,840
within the the kubernetes community

00:31:37,700 --> 00:31:40,610
there was also some discussion about

00:31:38,840 --> 00:31:44,750
using different container formats so

00:31:40,610 --> 00:31:46,340
rocket OCIE containers etc so what they

00:31:44,750 --> 00:31:48,980
did is in company G is 1.5 they

00:31:46,340 --> 00:31:52,279
introduced a an API called the container

00:31:48,980 --> 00:31:53,509
runtime interface CRI for short and what

00:31:52,279 --> 00:31:56,509
happened is that they just split it up

00:31:53,509 --> 00:31:57,980
in multiple demons so cubelet now

00:31:56,509 --> 00:31:59,960
doesn't have any understanding about

00:31:57,980 --> 00:32:02,210
container format anymore it also has no

00:31:59,960 --> 00:32:05,600
understanding about cgroups anymore it's

00:32:02,210 --> 00:32:07,820
just a random go app that connects to

00:32:05,600 --> 00:32:09,169
the API server takes a look at what it

00:32:07,820 --> 00:32:12,529
needs to be doing and then forwards

00:32:09,169 --> 00:32:14,269
those requests to to our pcs and it now

00:32:12,529 --> 00:32:15,500
makes use of two separate daemons one of

00:32:14,269 --> 00:32:18,860
them is called the image service and

00:32:15,500 --> 00:32:20,629
with the image service it can the

00:32:18,860 --> 00:32:23,450
couplet sends over our pcs to download

00:32:20,629 --> 00:32:25,429
docker images and maybe even remove them

00:32:23,450 --> 00:32:26,960
so it also takes a look at the free disk

00:32:25,429 --> 00:32:29,090
space on the node and if that's getting

00:32:26,960 --> 00:32:30,440
too low it might send our pcs over to

00:32:29,090 --> 00:32:32,509
the image server to throw away certain

00:32:30,440 --> 00:32:34,610
large container images that haven't been

00:32:32,509 --> 00:32:36,350
used for a long time and there's the

00:32:34,610 --> 00:32:38,149
runtime service and the runtime service

00:32:36,350 --> 00:32:40,549
implements sort of the old logic of

00:32:38,149 --> 00:32:44,720
creating those C groups and it sends our

00:32:40,549 --> 00:32:45,350
pcs over to rocket OCI or docker etc to

00:32:44,720 --> 00:32:49,010
spawn

00:32:45,350 --> 00:32:51,559
containers inside so what I decided is

00:32:49,010 --> 00:32:54,770
to make use of this container runtime

00:32:51,559 --> 00:32:56,990
interface feature to add support for for

00:32:54,770 --> 00:32:58,910
cloud API so what I've written I've

00:32:56,990 --> 00:33:01,700
written a couple of demons collectively

00:32:58,910 --> 00:33:06,110
called scuba scuba means secure

00:33:01,700 --> 00:33:07,850
kubernetes scuba and it consists of like

00:33:06,110 --> 00:33:10,520
two separate processes namely a runtime

00:33:07,850 --> 00:33:13,429
and an image service written in as

00:33:10,520 --> 00:33:15,559
cloudy api processes so so these jobs

00:33:13,429 --> 00:33:17,750
already run as cloud API processes on a

00:33:15,559 --> 00:33:20,059
freebsd box and the image serves is

00:33:17,750 --> 00:33:21,650
quite simple instead of downloading like

00:33:20,059 --> 00:33:22,850
a fully fledged docker image the goal

00:33:21,650 --> 00:33:25,429
behind the image serves is to download

00:33:22,850 --> 00:33:27,760
an elf file a cloud ABI executable

00:33:25,429 --> 00:33:30,020
stored in some kind of directory on disk

00:33:27,760 --> 00:33:31,970
then there's the runtime service and

00:33:30,020 --> 00:33:34,580
what it does is the only thing it does

00:33:31,970 --> 00:33:38,059
basically it's just fork and run those

00:33:34,580 --> 00:33:39,830
elf files that are being provided I've

00:33:38,059 --> 00:33:42,620
added an extension to kubernetes that

00:33:39,830 --> 00:33:44,090
instead of using command line arguments

00:33:42,620 --> 00:33:46,130
environment variables that kind of stuff

00:33:44,090 --> 00:33:48,679
the arc data

00:33:46,130 --> 00:33:51,590
yamo specification can actually be

00:33:48,679 --> 00:33:54,140
placed inside of the kubernetes pub

00:33:51,590 --> 00:33:55,669
specification so normally when starting

00:33:54,140 --> 00:33:58,730
a cloud API application you would make

00:33:55,669 --> 00:34:00,500
use of tags like socket and file to

00:33:58,730 --> 00:34:02,570
access the disk and networking

00:34:00,500 --> 00:34:04,789
well these tags are gone when you're

00:34:02,570 --> 00:34:06,890
using scuba instead you just get some

00:34:04,789 --> 00:34:09,710
tags that are really specific to

00:34:06,890 --> 00:34:11,060
kubernetes to the environment at hand so

00:34:09,710 --> 00:34:13,580
for example there's a tag called

00:34:11,060 --> 00:34:16,669
kubernetes / container log and if you

00:34:13,580 --> 00:34:19,190
place this one in the llamo file you the

00:34:16,669 --> 00:34:20,629
process basically gets a pipe in which

00:34:19,190 --> 00:34:21,950
it can write and everything that's

00:34:20,629 --> 00:34:24,800
written in there ends up in a container

00:34:21,950 --> 00:34:26,570
log for networking I've also added

00:34:24,800 --> 00:34:28,820
various tags like kubernetes like server

00:34:26,570 --> 00:34:30,379
and kubernetes - client where you can

00:34:28,820 --> 00:34:35,510
say this is a program that needs to be

00:34:30,379 --> 00:34:37,310
started with a network service so the

00:34:35,510 --> 00:34:38,840
this will add sort of sock this will

00:34:37,310 --> 00:34:41,210
hand over sockets through the program on

00:34:38,840 --> 00:34:43,369
which they can do networking then

00:34:41,210 --> 00:34:44,899
there's another tag called a community /

00:34:43,369 --> 00:34:49,369
mount and if you use that one you can

00:34:44,899 --> 00:34:51,770
refer to disks or to paths on the system

00:34:49,369 --> 00:34:53,419
so for example kubernetes has built in

00:34:51,770 --> 00:34:55,310
support for doing NFS mounts or

00:34:53,419 --> 00:34:59,180
attaching Amazon

00:34:55,310 --> 00:35:00,829
EBS volumes all of the kind of stuff is

00:34:59,180 --> 00:35:02,690
boarded and you can attach those

00:35:00,829 --> 00:35:04,460
directories those those mount points

00:35:02,690 --> 00:35:07,210
over you can hand them over to the cloud

00:35:04,460 --> 00:35:11,180
API process using kubernetes slash mount

00:35:07,210 --> 00:35:13,490
so this is sort of like a picture like

00:35:11,180 --> 00:35:16,040
of what my setup looks like so instead

00:35:13,490 --> 00:35:18,349
of using docker it's just a couplet

00:35:16,040 --> 00:35:21,050
talking to scuba image service and scuba

00:35:18,349 --> 00:35:22,099
runtime service so scuba image service

00:35:21,050 --> 00:35:24,349
downloads an image from the internet

00:35:22,099 --> 00:35:26,329
places it in a certain directory in disk

00:35:24,349 --> 00:35:28,160
and then the scuba runtime service gets

00:35:26,329 --> 00:35:34,490
the Delft from this location and spawns

00:35:28,160 --> 00:35:37,910
the jobs over there yeah so I got this

00:35:34,490 --> 00:35:40,430
working initially I got some very simple

00:35:37,910 --> 00:35:42,380
jobs to run I could run a very simple

00:35:40,430 --> 00:35:44,750
HTTP server I could run a sleep

00:35:42,380 --> 00:35:47,089
executable that would just stay in a

00:35:44,750 --> 00:35:51,079
loop write some entries into the log

00:35:47,089 --> 00:35:53,270
file or nothing more I started to

00:35:51,079 --> 00:35:57,770
realize after some time that the way

00:35:53,270 --> 00:36:00,470
that like cloud API did its networking

00:35:57,770 --> 00:36:02,930
or kubernetes does its networking for

00:36:00,470 --> 00:36:05,359
this worth is sort of fairly suboptimal

00:36:02,930 --> 00:36:06,829
and it's basically boils back to some of

00:36:05,359 --> 00:36:07,460
the slides I gave earlier during my talk

00:36:06,829 --> 00:36:11,390
that like

00:36:07,460 --> 00:36:13,819
ipv4 is easy to exhaust etcetera so the

00:36:11,390 --> 00:36:17,930
problem with API is like binding Connect

00:36:13,819 --> 00:36:20,000
sort of the traditional API for binding

00:36:17,930 --> 00:36:21,470
to a certain port number or connecting

00:36:20,000 --> 00:36:23,119
to some odd rows on the network is that

00:36:21,470 --> 00:36:25,609
they require a lot of security

00:36:23,119 --> 00:36:28,220
frameworks to be secure you need I P

00:36:25,609 --> 00:36:29,839
tables or PF to actually generate you

00:36:28,220 --> 00:36:32,059
know hundreds of lines or maybe

00:36:29,839 --> 00:36:34,160
thousands of lines on an average no to

00:36:32,059 --> 00:36:37,460
sort of come out with a secure policy

00:36:34,160 --> 00:36:39,920
not a problem if these these functions

00:36:37,460 --> 00:36:41,450
is that they also require extra kernel

00:36:39,920 --> 00:36:43,520
frameworks to do tracing and debugging

00:36:41,450 --> 00:36:45,349
you know the fact that I always have to

00:36:43,520 --> 00:36:46,940
log in on Linux servers running docker

00:36:45,349 --> 00:36:49,250
containers you know in the kubernetes

00:36:46,940 --> 00:36:50,630
cluster to run TCP dump it's all fairly

00:36:49,250 --> 00:36:53,059
you know because you get so much garbage

00:36:50,630 --> 00:36:54,829
in there there's no easy way where you

00:36:53,059 --> 00:36:56,869
can just say like I want to capture only

00:36:54,829 --> 00:36:58,609
this traffic for this specific part or

00:36:56,869 --> 00:37:01,220
container just using a polar containers

00:36:58,609 --> 00:37:03,470
name there's also no support for

00:37:01,220 --> 00:37:06,109
metadata passing so all of these API

00:37:03,470 --> 00:37:07,400
czar ipv4 address based whenever a

00:37:06,109 --> 00:37:09,559
container receives an incoming

00:37:07,400 --> 00:37:10,569
connection from some other part on the

00:37:09,559 --> 00:37:13,070
cluster

00:37:10,569 --> 00:37:16,340
it only gets an ipv4

00:37:13,070 --> 00:37:18,920
and it needs to somehow do a reserve

00:37:16,340 --> 00:37:20,810
sorry reverse lookup to actually figure

00:37:18,920 --> 00:37:23,000
out which container tried to contact it

00:37:20,810 --> 00:37:26,330
so what I've done is I've written a

00:37:23,000 --> 00:37:27,950
demon called flower which I jokingly

00:37:26,330 --> 00:37:31,430
sometimes called sockets as a service

00:37:27,950 --> 00:37:35,630
SAS sometimes also call it like a dating

00:37:31,430 --> 00:37:36,770
service for unique saps and it's nothing

00:37:35,630 --> 00:37:40,070
more than a daemon where you can send

00:37:36,770 --> 00:37:42,590
our pcs to register register yourself as

00:37:40,070 --> 00:37:44,330
a service and you can send other our pcs

00:37:42,590 --> 00:37:47,570
to just connect to those services that

00:37:44,330 --> 00:37:49,010
are being registered so what flower does

00:37:47,570 --> 00:37:51,110
whenever there is sort of like a match

00:37:49,010 --> 00:37:52,520
you know you've got one server running

00:37:51,110 --> 00:37:54,950
and one client trying to connect to it

00:37:52,520 --> 00:37:56,420
it creates a unique socket pair and then

00:37:54,950 --> 00:37:58,100
hands out two file descriptor to both

00:37:56,420 --> 00:38:00,890
ends so it uses file descriptor passing

00:37:58,100 --> 00:38:02,630
and this project is sort of unrelated to

00:38:00,890 --> 00:38:04,640
coup Benitez and cloud ABI I mean it's

00:38:02,630 --> 00:38:06,920
in practice you'd likely want to use

00:38:04,640 --> 00:38:08,720
those two in combination but in theory

00:38:06,920 --> 00:38:09,350
you could use it separately so how does

00:38:08,720 --> 00:38:11,180
it work

00:38:09,350 --> 00:38:12,920
first of all you start a process called

00:38:11,180 --> 00:38:16,280
the switchboard and just like a

00:38:12,920 --> 00:38:17,600
traditional patch panel and what you can

00:38:16,280 --> 00:38:19,850
then do is you can run these other

00:38:17,600 --> 00:38:21,770
commands flower cat which is a bit like

00:38:19,850 --> 00:38:24,680
NAT cat to like listen on the

00:38:21,770 --> 00:38:26,720
switchboard and connect to to a process

00:38:24,680 --> 00:38:28,340
listening on the switchboard and you can

00:38:26,720 --> 00:38:30,410
use these labels to sort of identify

00:38:28,340 --> 00:38:32,570
multiple processes listening on the

00:38:30,410 --> 00:38:34,190
switchboard so this is all like a bit

00:38:32,570 --> 00:38:36,860
lame doesn't really look exciting so

00:38:34,190 --> 00:38:38,210
here's a more sort of practical example

00:38:36,860 --> 00:38:40,190
file you could be using it in practice

00:38:38,210 --> 00:38:42,590
so again you start the switchboard but

00:38:40,190 --> 00:38:44,570
instead of using flower cat you could

00:38:42,590 --> 00:38:46,820
use cloudy I run to spawn a cloud API

00:38:44,570 --> 00:38:48,320
process there's a simple demo web server

00:38:46,820 --> 00:38:50,450
for nowadays that's already prepackaged

00:38:48,320 --> 00:38:52,220
that you can use and inside of the yamo

00:38:50,450 --> 00:38:54,110
which I haven't put in this slide you

00:38:52,220 --> 00:38:55,730
refer to the switchboard listening on

00:38:54,110 --> 00:38:59,390
tnp flower and you say I want to run

00:38:55,730 --> 00:39:00,590
this process lasing on flower then

00:38:59,390 --> 00:39:02,600
there's not a process called flower

00:39:00,590 --> 00:39:06,770
ingress except and what that one does is

00:39:02,600 --> 00:39:08,150
it binds on a TCP port number and it

00:39:06,770 --> 00:39:10,370
calls except in a loop to accept

00:39:08,150 --> 00:39:12,170
incoming connections but then it pushes

00:39:10,370 --> 00:39:13,730
the file descriptors into the flower

00:39:12,170 --> 00:39:15,800
switchboard which and can then hand it

00:39:13,730 --> 00:39:17,360
over to a process on the other side so

00:39:15,800 --> 00:39:19,670
when you run all of these free commands

00:39:17,360 --> 00:39:20,960
you can finally run curl localhost and

00:39:19,670 --> 00:39:23,690
you get a simple response from this

00:39:20,960 --> 00:39:25,370
batch server so far this doesn't really

00:39:23,690 --> 00:39:26,510
look that exciting but this is really

00:39:25,370 --> 00:39:28,130
where sort of I'm going to

00:39:26,510 --> 00:39:32,720
describe the actual sort of value that's

00:39:28,130 --> 00:39:34,310
attitude to this system is the matching

00:39:32,720 --> 00:39:35,840
that's being done on the labels it's not

00:39:34,310 --> 00:39:37,310
like an exact matching so it doesn't

00:39:35,840 --> 00:39:38,480
require that both sides present the same

00:39:37,310 --> 00:39:41,360
side of labels the clients in the

00:39:38,480 --> 00:39:42,830
servers but a valid match is constructed

00:39:41,360 --> 00:39:44,720
when there are no contradicting labels

00:39:42,830 --> 00:39:47,720
and this allows both parties to actually

00:39:44,720 --> 00:39:50,930
attach more labels than necessary in the

00:39:47,720 --> 00:39:53,450
matching process so clients can provide

00:39:50,930 --> 00:39:56,900
extra metadata speccing specifying who

00:39:53,450 --> 00:39:58,340
they are and servers can also add some

00:39:56,900 --> 00:40:00,020
extra metadata under sense so they can

00:39:58,340 --> 00:40:02,360
say like this is a web server that's

00:40:00,020 --> 00:40:07,400
running nginx this is a web server with

00:40:02,360 --> 00:40:09,800
version whatever so also in this case if

00:40:07,400 --> 00:40:11,330
the ingress it will attach the IP

00:40:09,800 --> 00:40:14,180
address and the port number of the pier

00:40:11,330 --> 00:40:19,700
as extra labels before forwarding it

00:40:14,180 --> 00:40:22,250
over to flower so the process oh okay so

00:40:19,700 --> 00:40:24,890
the process will then know like the IP

00:40:22,250 --> 00:40:26,780
address of the connecting client so it's

00:40:24,890 --> 00:40:28,190
also capability based namely the handles

00:40:26,780 --> 00:40:30,170
that are porting over to the switch

00:40:28,190 --> 00:40:31,940
pointing to the switchboard they can be

00:40:30,170 --> 00:40:33,530
duplicated and can be constrained and

00:40:31,940 --> 00:40:35,060
some extra labels can be attached so

00:40:33,530 --> 00:40:37,460
what you can do is you can actually

00:40:35,060 --> 00:40:38,990
enforce at a program that makes use of

00:40:37,460 --> 00:40:41,840
the switchboard always has to identify

00:40:38,990 --> 00:40:45,250
itself it always has to provide a label

00:40:41,840 --> 00:40:49,820
for every request saying my pop name is

00:40:45,250 --> 00:40:52,040
WWE etc so this is sort of resulting

00:40:49,820 --> 00:40:53,390
picture so Scooba runtime now is a

00:40:52,040 --> 00:40:54,980
connection to the switchboard and for

00:40:53,390 --> 00:40:56,900
every container and pod it starts up

00:40:54,980 --> 00:40:59,300
that has a kubernetes client or server

00:40:56,900 --> 00:41:01,070
tag it also creates additional handles

00:40:59,300 --> 00:41:03,320
to the switchboard so those are these

00:41:01,070 --> 00:41:05,540
lines over here and then whenever the

00:41:03,320 --> 00:41:08,780
nginx mi sequel want to communicate with

00:41:05,540 --> 00:41:10,040
each other they nginx sends a request to

00:41:08,780 --> 00:41:12,410
the switchboard which and creates a

00:41:10,040 --> 00:41:14,390
socket pair and hands out both ends to

00:41:12,410 --> 00:41:18,680
my sequel nginx so that's this line over

00:41:14,390 --> 00:41:24,200
here so the question is does all of this

00:41:18,680 --> 00:41:25,970
actually work well let's see so let me

00:41:24,200 --> 00:41:29,210
try to do this while holding the

00:41:25,970 --> 00:41:30,740
microphone so I hope everyone can read

00:41:29,210 --> 00:41:35,260
this on the left there is a free BC

00:41:30,740 --> 00:41:35,260
server the font size a bit

00:41:45,130 --> 00:41:49,690
okay well then I'll I'll try to hold it

00:41:47,559 --> 00:41:52,319
like this so now I need to type in my

00:41:49,690 --> 00:41:52,319
pseudo password

00:42:00,460 --> 00:42:03,310
so what I've done now in the right

00:42:01,990 --> 00:42:07,090
terminal I've started up a kubernetes

00:42:03,310 --> 00:42:09,310
server or sorry like an API server and

00:42:07,090 --> 00:42:10,780
on the Left I've started up a couplet so

00:42:09,310 --> 00:42:13,530
now we see that the couplet this is a

00:42:10,780 --> 00:42:16,590
freebsd vm registered itself as being

00:42:13,530 --> 00:42:18,370
couplet read or tehsil cloudy by etc and

00:42:16,590 --> 00:42:20,770
there on the right you see that

00:42:18,370 --> 00:42:25,090
kubernetes is crashing so that's not a

00:42:20,770 --> 00:42:26,590
good time but well yeah yeah it does it

00:42:25,090 --> 00:42:31,240
sometimes but now at least what I can do

00:42:26,590 --> 00:42:33,190
is I can run a cube CTL get nodes and

00:42:31,240 --> 00:42:34,960
you see that there's now one node in the

00:42:33,190 --> 00:42:36,790
cluster called couplet free so this is a

00:42:34,960 --> 00:42:37,870
freebies denote I could also describe it

00:42:36,790 --> 00:42:40,390
and then you can actually see that as a

00:42:37,870 --> 00:42:45,130
freebies you know but now what I can do

00:42:40,390 --> 00:42:52,030
is I can run cube CTL create - F many

00:42:45,130 --> 00:42:55,090
fests slash web server mo and nothing is

00:42:52,030 --> 00:42:58,600
happening over there oh man

00:42:55,090 --> 00:43:00,520
this is annoying so what I'll do I'll

00:42:58,600 --> 00:43:03,460
first finish the remainder of my slides

00:43:00,520 --> 00:43:04,810
and then I'll come back to the demo so

00:43:03,460 --> 00:43:08,530
the annoying thing is I'm making use of

00:43:04,810 --> 00:43:10,240
like a like kubernetes master and I've

00:43:08,530 --> 00:43:11,920
seen that the API server sometimes

00:43:10,240 --> 00:43:16,060
crashes unrelated to any of the cloud

00:43:11,920 --> 00:43:22,570
Evi things so yeah that's a shame

00:43:16,060 --> 00:43:23,320
umm phew slideshow so we'll come back to

00:43:22,570 --> 00:43:26,320
that in a minute

00:43:23,320 --> 00:43:28,840
so wrapping up well if it would have

00:43:26,320 --> 00:43:29,410
worked there is something called

00:43:28,840 --> 00:43:31,360
kubernetes

00:43:29,410 --> 00:43:33,370
I think we at 3bc should be using it as

00:43:31,360 --> 00:43:35,710
well if we're not just trying to catch

00:43:33,370 --> 00:43:37,540
up with Linux we have some components in

00:43:35,710 --> 00:43:39,370
freebsd already or some readily

00:43:37,540 --> 00:43:42,100
available that that allow us to do that

00:43:39,370 --> 00:43:44,410
one of them is called cloud Evi easily

00:43:42,100 --> 00:43:46,450
sandboxing with scuba we can run it on

00:43:44,410 --> 00:43:48,150
the cluster and with flower we can allow

00:43:46,450 --> 00:43:51,490
them to communicate over the network

00:43:48,150 --> 00:43:53,980
so what's why wish list for 2018 or the

00:43:51,490 --> 00:43:56,140
remainder of 2017 as well I mean this so

00:43:53,980 --> 00:43:58,960
far has been like a solo effort there

00:43:56,140 --> 00:44:00,670
are some number of people IRC are also

00:43:58,960 --> 00:44:02,890
hacking on cloud API a bit but the

00:44:00,670 --> 00:44:05,800
entire kubernetes thing is just me on my

00:44:02,890 --> 00:44:07,660
own I can only work on this part-time I

00:44:05,800 --> 00:44:09,460
also have bills to pay of course so

00:44:07,660 --> 00:44:11,650
that's why I also do some consulting

00:44:09,460 --> 00:44:13,180
work on the side it would be really

00:44:11,650 --> 00:44:14,800
awesome if I could somehow do this fault

00:44:13,180 --> 00:44:16,599
full time but at the same

00:44:14,800 --> 00:44:18,220
I'm also really hoping for participation

00:44:16,599 --> 00:44:19,390
from the community so it would be

00:44:18,220 --> 00:44:22,900
awesome if other people could use and

00:44:19,390 --> 00:44:24,520
test this hack and ports stuff also help

00:44:22,900 --> 00:44:26,710
me document this and promote this at

00:44:24,520 --> 00:44:28,599
other conferences etc and also

00:44:26,710 --> 00:44:31,599
eventually fun and if vessel is it can

00:44:28,599 --> 00:44:33,310
become something sustainable so most of

00:44:31,599 --> 00:44:35,560
this work you know this is there's only

00:44:33,310 --> 00:44:37,150
works for FreeBSD but in theory this

00:44:35,560 --> 00:44:40,119
girls will be done for the other puz so

00:44:37,150 --> 00:44:42,099
even if you're not a FreeBSD fan then

00:44:40,119 --> 00:44:43,210
and you're still interested in cloudy

00:44:42,099 --> 00:44:45,250
high please get in touch

00:44:43,210 --> 00:44:47,980
I mean we can also always get this to

00:44:45,250 --> 00:44:50,140
work another beasties so here's a bunch

00:44:47,980 --> 00:44:52,060
of links to github repositories so

00:44:50,140 --> 00:44:54,730
they're patched up kubernetes source the

00:44:52,060 --> 00:44:56,050
cloud API definitions scuba have flour a

00:44:54,730 --> 00:44:58,540
lot of these things are already in

00:44:56,050 --> 00:45:00,310
FreeBSD ports and if they're not a main

00:44:58,540 --> 00:45:02,590
reason for them is because they are

00:45:00,310 --> 00:45:04,210
cloudy bi binaries and those are

00:45:02,590 --> 00:45:08,040
packaged in a separate repository that

00:45:04,210 --> 00:45:10,690
you can just add to package config file

00:45:08,040 --> 00:45:14,710
so this is actually all my slides I'm

00:45:10,690 --> 00:45:19,109
now going to going to look at a few so

00:45:14,710 --> 00:45:19,109
still time for the demo or shall we

00:45:20,760 --> 00:45:29,619
shall we do the demo okay so what I now

00:45:27,070 --> 00:45:31,930
need to do is actually not all that hard

00:45:29,619 --> 00:45:34,030
I just need to shut down all of

00:45:31,930 --> 00:45:36,010
kubernetes and throw away all of its

00:45:34,030 --> 00:45:43,030
state so I'm just going to do that now

00:45:36,010 --> 00:45:45,369
with two hands because I need time no no

00:45:43,030 --> 00:45:47,020
no no this is some this is just a

00:45:45,369 --> 00:45:54,390
testing cluster because this thing of

00:45:47,020 --> 00:45:54,390
two nodes yeah

00:45:56,800 --> 00:46:01,300
so the question is does the API server

00:45:59,050 --> 00:46:03,700
also always crash in production and the

00:46:01,300 --> 00:46:05,470
answer to that to that is no the reason

00:46:03,700 --> 00:46:08,260
why this one crashes is that this is

00:46:05,470 --> 00:46:10,330
just like get master that I checked out

00:46:08,260 --> 00:46:12,430
at some point in time and had to patch

00:46:10,330 --> 00:46:13,870
up to make work on FreeBSD so there's a

00:46:12,430 --> 00:46:16,420
small number of changes to actually make

00:46:13,870 --> 00:46:19,530
it like to add the FreeBSD bits to make

00:46:16,420 --> 00:46:22,150
work and also some cloudy API changes

00:46:19,530 --> 00:46:25,480
that said this version of the API server

00:46:22,150 --> 00:46:29,650
always crashes like with 30 20 percent

00:46:25,480 --> 00:46:31,510
chance so yeah it's I just picked an

00:46:29,650 --> 00:46:33,070
unlucky reversion revision but the

00:46:31,510 --> 00:46:35,220
eventual version shouldn't crash all day

00:46:33,070 --> 00:46:35,220
long

00:46:50,670 --> 00:46:56,410
so now I've restarted kubernetes again

00:46:53,170 --> 00:46:57,790
on the master so the right terminal is a

00:46:56,410 --> 00:47:00,760
Linux system running the Keneally's

00:46:57,790 --> 00:47:03,700
master and the left one is the free BC

00:47:00,760 --> 00:47:04,810
system running the couplet and now you

00:47:03,700 --> 00:47:06,820
can see that it has successfully

00:47:04,810 --> 00:47:08,380
registered again and now we don't see

00:47:06,820 --> 00:47:13,240
this ugly back trace in this terminal

00:47:08,380 --> 00:47:15,700
which is good so now in on the linux

00:47:13,240 --> 00:47:17,560
server I can run cube CTL get nodes and

00:47:15,700 --> 00:47:20,290
indeed we see that Q plus tree has

00:47:17,560 --> 00:47:23,110
registered itself in the cluster cube

00:47:20,290 --> 00:47:24,190
CTL create - F we now see some output

00:47:23,110 --> 00:47:25,900
here in the terminal that's because it

00:47:24,190 --> 00:47:32,650
has now started a couple of jobs if I

00:47:25,900 --> 00:47:34,120
run Q CTL get pod you can see that it

00:47:32,650 --> 00:47:36,460
now has spawned three web server

00:47:34,120 --> 00:47:37,900
processes on this node in the cluster if

00:47:36,460 --> 00:47:40,270
I would describe them I would get more

00:47:37,900 --> 00:47:41,530
info you know you could actually see in

00:47:40,270 --> 00:47:43,120
which node in the cluster it's running

00:47:41,530 --> 00:47:46,420
when it was started how much resources

00:47:43,120 --> 00:47:48,150
it is using etc one thing I could for

00:47:46,420 --> 00:47:51,040
example show you is how to sort of

00:47:48,150 --> 00:48:00,780
increase the number of jobs so if I run

00:47:51,040 --> 00:48:04,330
cube CTL edit deployment web server I

00:48:00,780 --> 00:48:05,830
just get spawned in vim and I could head

00:48:04,330 --> 00:48:07,990
over to this line indicating the number

00:48:05,830 --> 00:48:09,460
of replicas running on the cluster if I

00:48:07,990 --> 00:48:12,640
would just say like let's change this to

00:48:09,460 --> 00:48:14,950
five and exit vim dam voila you seen the

00:48:12,640 --> 00:48:16,210
left terminal that it generated some

00:48:14,950 --> 00:48:18,240
more output in the meantime but now we

00:48:16,210 --> 00:48:21,400
have five notes running on the cluster

00:48:18,240 --> 00:48:23,200
so one thing that I could now do is like

00:48:21,400 --> 00:48:24,280
for the sake of this demo expose one

00:48:23,200 --> 00:48:25,960
over the network and you can actually

00:48:24,280 --> 00:48:28,540
see what this webserver said I Sparta

00:48:25,960 --> 00:48:33,250
spawned on the cluster so I'm now going

00:48:28,540 --> 00:48:34,570
to run sudo flower I'll just type it in

00:48:33,250 --> 00:48:36,690
first and then explain to you what it

00:48:34,570 --> 00:48:36,690
does

00:48:53,870 --> 00:48:58,200
so this come on over here I say I'm now

00:48:56,490 --> 00:49:00,240
going to start an ingress that's going

00:48:58,200 --> 00:49:02,130
to listen on port 80 and send all

00:49:00,240 --> 00:49:04,350
traffic over to one specific part in the

00:49:02,130 --> 00:49:07,170
cluster namely server kubernetes pop

00:49:04,350 --> 00:49:09,390
name is webserver etc if I'm now going

00:49:07,170 --> 00:49:12,350
to start this and this web server will

00:49:09,390 --> 00:49:12,350
be listening on port 80

00:49:20,690 --> 00:49:24,560
which is now worked it has found my web

00:49:22,850 --> 00:49:27,410
server that's like surfing some kind of

00:49:24,560 --> 00:49:28,700
silly HTML page on the to the browser

00:49:27,410 --> 00:49:30,560
that I found on it's quite impressive

00:49:28,700 --> 00:49:33,080
there's one kilobyte of JavaScript and

00:49:30,560 --> 00:49:34,550
it's printing itself so right now you

00:49:33,080 --> 00:49:36,500
see that like the networking part

00:49:34,550 --> 00:49:39,260
through flower is sort of is sort of

00:49:36,500 --> 00:49:40,760
working the only thing that's sort of

00:49:39,260 --> 00:49:42,050
missing right now is load balancing

00:49:40,760 --> 00:49:44,000
support this is still something that

00:49:42,050 --> 00:49:45,890
needs to be added to flower so you could

00:49:44,000 --> 00:49:47,450
see when I started the ingress that I

00:49:45,890 --> 00:49:49,580
directed all traffic to one specific

00:49:47,450 --> 00:49:50,720
instance of the part this of course

00:49:49,580 --> 00:49:52,490
needs to be extended that you can

00:49:50,720 --> 00:49:53,990
provide a name of a service to redirect

00:49:52,490 --> 00:49:56,150
all traffic for a service to all of its

00:49:53,990 --> 00:49:58,430
back-end so this is still on the to do

00:49:56,150 --> 00:50:00,050
so this is just a really tiny tiny demo

00:49:58,430 --> 00:50:02,210
to show you that like in fact this does

00:50:00,050 --> 00:50:04,490
work this does spawn jobs in a cluster

00:50:02,210 --> 00:50:06,710
and these are all sandbox all these web

00:50:04,490 --> 00:50:08,590
servers over here please hack them there

00:50:06,710 --> 00:50:11,150
might be security bugs in them

00:50:08,590 --> 00:50:12,590
the fortunate thing about it is these

00:50:11,150 --> 00:50:14,330
jobs are all started up in such a way

00:50:12,590 --> 00:50:17,360
that they can only communicate with the

00:50:14,330 --> 00:50:19,370
kubernetes log and the switchboard so

00:50:17,360 --> 00:50:23,350
the impact of that would be fairly

00:50:19,370 --> 00:50:23,350
minimal aren't any questions

00:50:27,710 --> 00:50:35,400
yeah thanks for a talk you said that the

00:50:31,440 --> 00:50:39,630
peyten was ordered to support cloud ibi

00:50:35,400 --> 00:50:42,270
right does the Titan program need to

00:50:39,630 --> 00:50:46,080
support it as well so that's a really

00:50:42,270 --> 00:50:50,070
good question so we have a version of Pi

00:50:46,080 --> 00:50:51,750
Python it works on cloud ABI D there is

00:50:50,070 --> 00:50:54,390
a Python executable but it doesn't have

00:50:51,750 --> 00:50:56,250
like the same startup process as a

00:50:54,390 --> 00:50:58,830
traditional - one so it's not just -

00:50:56,250 --> 00:51:02,280
space filename what you do is you write

00:50:58,830 --> 00:51:04,710
a llamó file in which you specify which

00:51:02,280 --> 00:51:07,530
include paths that Python is allowed to

00:51:04,710 --> 00:51:09,110
use and you specify the file name of the

00:51:07,530 --> 00:51:11,580
Python script that needs to be run and

00:51:09,110 --> 00:51:13,410
maybe some other resources on which your

00:51:11,580 --> 00:51:15,990
Python script depends and then you can

00:51:13,410 --> 00:51:18,150
use cloud ABI - run on the Python

00:51:15,990 --> 00:51:20,070
interpreter specify of you know giving

00:51:18,150 --> 00:51:22,230
it the config file that lists all the

00:51:20,070 --> 00:51:25,530
resources that it's allowed to use does

00:51:22,230 --> 00:51:27,480
it answer your question what about the

00:51:25,530 --> 00:51:29,850
likes network sockets and file

00:51:27,480 --> 00:51:32,490
descriptors open files and stuff like

00:51:29,850 --> 00:51:35,430
that so you're not allowed to call just

00:51:32,490 --> 00:51:36,930
plain open inside of Python but all of

00:51:35,430 --> 00:51:38,910
the resources like directories in which

00:51:36,930 --> 00:51:41,220
you depend on network sockets they can

00:51:38,910 --> 00:51:43,380
be passed as arguments on to the Python

00:51:41,220 --> 00:51:45,450
script and what happens is that inside a

00:51:43,380 --> 00:51:49,140
Fife and there's no sister arc V anymore

00:51:45,450 --> 00:51:51,270
there's no sin stopped and environ but

00:51:49,140 --> 00:51:53,310
there is a system arc data that gives

00:51:51,270 --> 00:51:57,330
you like access to all of the attributes

00:51:53,310 --> 00:51:59,280
that you pass in in the mo file but I

00:51:57,330 --> 00:52:01,980
can always use a new maintainer or more

00:51:59,280 --> 00:52:03,990
people who want a hack on our pipe

00:52:01,980 --> 00:52:07,200
import I mean at one point I was looking

00:52:03,990 --> 00:52:08,610
into porting Jango that sort of stall

00:52:07,200 --> 00:52:10,920
halfway along because I got distracted

00:52:08,610 --> 00:52:12,480
by all of this kubernetes work but could

00:52:10,920 --> 00:52:16,470
always use more people looking into it

00:52:12,480 --> 00:52:18,390
Thanks hi I have two questions if you

00:52:16,470 --> 00:52:20,580
permit me first

00:52:18,390 --> 00:52:23,940
well you have to pass powder script in

00:52:20,580 --> 00:52:26,490
advance you can pass a descriptor to a

00:52:23,940 --> 00:52:28,560
directory in cloud ABI and then you can

00:52:26,490 --> 00:52:31,200
use OpenNet beneath a directory to

00:52:28,560 --> 00:52:33,450
subsequently open arbitrary files so we

00:52:31,200 --> 00:52:38,970
can run interpreted languages like PHP

00:52:33,450 --> 00:52:40,290
fpm and stuff yeah so if you would want

00:52:38,970 --> 00:52:42,840
to like have a website

00:52:40,290 --> 00:52:44,970
that spawns up a separate PHP process to

00:52:42,840 --> 00:52:46,590
run a script then you would also need to

00:52:44,970 --> 00:52:49,050
pass in a file descriptor of the

00:52:46,590 --> 00:52:51,600
executable of the PHP interpreter but

00:52:49,050 --> 00:52:53,700
passing in directories means that you

00:52:51,600 --> 00:52:55,620
can also access any files within and

00:52:53,700 --> 00:52:57,570
generate new file descriptors based on

00:52:55,620 --> 00:52:59,160
those things so it's it's not as if

00:52:57,570 --> 00:53:00,960
you're limited to the file descriptors

00:52:59,160 --> 00:53:02,790
that are being passed on or startup it's

00:53:00,960 --> 00:53:06,380
only that those limit to what you can

00:53:02,790 --> 00:53:08,640
actually infer so any directory

00:53:06,380 --> 00:53:10,620
underneath a certain directory can have

00:53:08,640 --> 00:53:13,770
its own separate directory descriptor

00:53:10,620 --> 00:53:15,450
that you can open later on great and why

00:53:13,770 --> 00:53:19,140
would you want to put the load balancing

00:53:15,450 --> 00:53:24,600
into flour instead of buying something

00:53:19,140 --> 00:53:25,760
like well that's that's a really good

00:53:24,600 --> 00:53:30,750
question

00:53:25,760 --> 00:53:32,880
maybe we should so one of the things

00:53:30,750 --> 00:53:34,980
that I also so this is sort of slightly

00:53:32,880 --> 00:53:37,620
unrelated to well it is somewhat relate

00:53:34,980 --> 00:53:40,710
to load balancing I also want to add so

00:53:37,620 --> 00:53:42,540
support for DNS lookups not added inside

00:53:40,710 --> 00:53:44,790
a flower but added as a separate process

00:53:42,540 --> 00:53:46,530
so one of the things that I didn't show

00:53:44,790 --> 00:53:48,630
my slice is that there's also in in

00:53:46,530 --> 00:53:50,250
addition to an in an ingress there's an

00:53:48,630 --> 00:53:52,530
egress that allows you to make outgoing

00:53:50,250 --> 00:53:54,990
connections so cloud API processes could

00:53:52,530 --> 00:53:56,850
connect to jobs on the internet for that

00:53:54,990 --> 00:53:58,320
we also need to have DNS support of

00:53:56,850 --> 00:54:00,150
course because only connecting by IP

00:53:58,320 --> 00:54:03,150
address is just plain ugly doesn't make

00:54:00,150 --> 00:54:04,680
any sense and that is also load

00:54:03,150 --> 00:54:06,180
balancing in a certain way you know

00:54:04,680 --> 00:54:08,640
you're resolving a certain hostname

00:54:06,180 --> 00:54:11,400
google.com it may return return multiple

00:54:08,640 --> 00:54:13,620
IP addresses and then flour needs to

00:54:11,400 --> 00:54:15,600
sort of take a look at all of those

00:54:13,620 --> 00:54:17,820
results and pick the first one or use

00:54:15,600 --> 00:54:20,520
some kind of logic to to make a

00:54:17,820 --> 00:54:22,470
successful connection so I think that

00:54:20,520 --> 00:54:24,690
like load balancing and DNS support are

00:54:22,470 --> 00:54:26,820
sort of somewhat strongly related to

00:54:24,690 --> 00:54:29,330
each other and therefore we might see

00:54:26,820 --> 00:54:31,560
that this is being solved in Flour but

00:54:29,330 --> 00:54:34,530
this is still on the drawing board if

00:54:31,560 --> 00:54:36,180
you want to help out you know have a

00:54:34,530 --> 00:54:39,030
nice chat about it give your input then

00:54:36,180 --> 00:54:46,280
you know just counts for everyone just

00:54:39,030 --> 00:54:46,280
ping me and you

00:54:49,609 --> 00:54:56,579
could you go back to the sed slide for a

00:54:53,069 --> 00:55:04,460
moment I want to take a picture sorry

00:54:56,579 --> 00:55:04,460
sorry which slide so sorry this said oh

00:55:05,930 --> 00:55:17,880
oh this one yeah yeah the Reg axis yeah

00:55:09,510 --> 00:55:21,660
yeah I get it sorry yeah there's another

00:55:17,880 --> 00:55:24,990
question back there are we going to

00:55:21,660 --> 00:55:27,990
upload the slide somewhere so I can show

00:55:24,990 --> 00:55:29,880
you two linux people crazy things you're

00:55:27,990 --> 00:55:33,599
doing and what might be actually

00:55:29,880 --> 00:55:35,700
interesting yeah so my plan is whenever

00:55:33,599 --> 00:55:38,369
conferences have some kind of facility

00:55:35,700 --> 00:55:40,799
for uploading slides I will so I'll

00:55:38,369 --> 00:55:43,410
probably send it over to the program

00:55:40,799 --> 00:55:47,299
committee yeah just send it yeah and

00:55:43,410 --> 00:55:47,299
then it will appear on the website yeah

00:55:48,680 --> 00:56:06,690
any other questions why do bananas have

00:55:54,240 --> 00:56:08,970
occurred you mentioned the lack of

00:56:06,690 --> 00:56:11,369
traceability of applications in the

00:56:08,970 --> 00:56:14,190
traditionary given it a set up and it's

00:56:11,369 --> 00:56:17,130
there any support for tracing like a

00:56:14,190 --> 00:56:21,839
simplified TCP dump or some similar

00:56:17,130 --> 00:56:25,349
command for doing this with flour there

00:56:21,839 --> 00:56:28,289
might be some third party automation

00:56:25,349 --> 00:56:29,789
around it kubernetes doesn't really

00:56:28,289 --> 00:56:31,619
provide something like that officially

00:56:29,789 --> 00:56:35,549
and I think one of the main reasons is

00:56:31,619 --> 00:56:39,150
because they instead of focusing on

00:56:35,549 --> 00:56:40,799
looking at TCP packets they are looking

00:56:39,150 --> 00:56:43,519
at the bigger picture in a certain way

00:56:40,799 --> 00:56:46,529
and they're more interested in tracing

00:56:43,519 --> 00:56:48,630
requests tracing our pcs over a larger

00:56:46,529 --> 00:56:51,210
cluster you know our PC gets sent from A

00:56:48,630 --> 00:56:54,029
to B gets forward from B to C and there

00:56:51,210 --> 00:56:56,970
is an sort of an open source tracing

00:56:54,029 --> 00:56:58,599
framework that's also part of the cloud

00:56:56,970 --> 00:57:09,819
native computing foundation but it is

00:56:58,599 --> 00:57:12,039
'we focus on tcp dumping so right now

00:57:09,819 --> 00:57:14,829
what kind of strategy you have for the

00:57:12,039 --> 00:57:16,420
load balancing part because now you get

00:57:14,829 --> 00:57:20,140
control of all that you can maybe

00:57:16,420 --> 00:57:25,599
implement whatever you want do you know

00:57:20,140 --> 00:57:27,970
any good strategies if so you can maybe

00:57:25,599 --> 00:57:32,109
implemented for implementation and and

00:57:27,970 --> 00:57:34,690
based on the experience that we get try

00:57:32,109 --> 00:57:38,140
something else and have it as a

00:57:34,690 --> 00:57:42,249
playground for ya so one thing I did

00:57:38,140 --> 00:57:44,710
think about is that all of the like load

00:57:42,249 --> 00:57:46,359
balancing or at least announcing which

00:57:44,710 --> 00:57:48,249
targets to which you can connect can be

00:57:46,359 --> 00:57:51,999
placed in a separate process that hands

00:57:48,249 --> 00:57:53,349
that info to switchboard and in addition

00:57:51,999 --> 00:57:55,479
to just giving the names of the things

00:57:53,349 --> 00:57:59,170
to which you can connect we could also

00:57:55,479 --> 00:58:00,729
add like weight scores so if you would

00:57:59,170 --> 00:58:01,900
want to make sort of a more complex load

00:58:00,729 --> 00:58:04,509
balancing thing you could have some

00:58:01,900 --> 00:58:07,150
dynamic weight computation in it so that

00:58:04,509 --> 00:58:08,950
if one back-end discovers that it's sort

00:58:07,150 --> 00:58:11,769
of being overloaded then it will reduce

00:58:08,950 --> 00:58:15,839
its weight causing that less traffic's

00:58:11,769 --> 00:58:15,839
being sent over something like that

00:58:21,210 --> 00:58:31,930
point of presence to deal by zone is

00:58:25,089 --> 00:58:33,969
geolocation yeah so I have to confess I

00:58:31,930 --> 00:58:36,549
I haven't given it like entire load been

00:58:33,969 --> 00:58:37,569
surfing too much but yet this is all

00:58:36,549 --> 00:58:40,509
open for discussion

00:58:37,569 --> 00:58:42,249
yeah okay referring to last question did

00:58:40,509 --> 00:58:44,200
I get you correctly that flower load

00:58:42,249 --> 00:58:47,589
balancing and flower network switching

00:58:44,200 --> 00:58:51,789
is all about a single note so it's load

00:58:47,589 --> 00:59:01,479
balancing services so so it wouldn't

00:58:51,789 --> 00:59:04,359
help in your global tea krauser lets say

00:59:01,479 --> 00:59:07,599
globalize the in different data center

00:59:04,359 --> 00:59:10,269
oh like there is giants I tried to

00:59:07,599 --> 00:59:12,550
implement such kind of things using the

00:59:10,269 --> 00:59:14,700
docker API and they do that

00:59:12,550 --> 00:59:19,590
you know not you have Moochie Thailand

00:59:14,700 --> 00:59:23,200
and globalized the cluster so yeah so I

00:59:19,590 --> 00:59:24,490
think you could argue that it's all the

00:59:23,200 --> 00:59:26,770
same problem it doesn't matter whether

00:59:24,490 --> 00:59:29,890
it's within a data center or cross data

00:59:26,770 --> 00:59:33,040
center but so I think our focus should

00:59:29,890 --> 00:59:35,440
first be on like in data center load

00:59:33,040 --> 00:59:38,620
balancing because it's it's unwise to

00:59:35,440 --> 00:59:43,330
smear one kubernetes cluster over

00:59:38,620 --> 00:59:45,580
multiple data centers anyway I mean what

00:59:43,330 --> 00:59:47,320
happens is that like the API server will

00:59:45,580 --> 00:59:48,880
still sort of remain a single point of

00:59:47,320 --> 00:59:50,020
failure and things slow down if you

00:59:48,880 --> 00:59:52,420
would actually try to spread it out a

00:59:50,020 --> 00:59:53,800
lot it's it's often a lot myself like in

00:59:52,420 --> 00:59:57,340
every data center one kubernetes cluster

00:59:53,800 --> 00:59:59,560
and therefore I think that flower should

00:59:57,340 --> 01:00:01,180
for now mainly look at we should look

00:59:59,560 --> 01:00:02,950
into adding in data center load

01:00:01,180 --> 01:00:05,140
balancing and across data center load

01:00:02,950 --> 01:00:08,200
balancing that also involves BGP and

01:00:05,140 --> 01:00:09,820
maybe all sorts of other like Network

01:00:08,200 --> 01:00:12,580
logistical problems that are sort of out

01:00:09,820 --> 01:00:14,100
of scope for flower yeah did it answer

01:00:12,580 --> 01:00:18,370
your question okay

01:00:14,100 --> 01:00:20,610
okay so new it and no it's for time for

01:00:18,370 --> 01:00:20,610

YouTube URL: https://www.youtube.com/watch?v=D13u9Gsz_R8


