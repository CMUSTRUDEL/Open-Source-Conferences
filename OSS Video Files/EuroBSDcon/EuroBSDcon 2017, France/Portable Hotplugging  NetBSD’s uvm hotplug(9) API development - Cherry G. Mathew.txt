Title: Portable Hotplugging  NetBSD’s uvm hotplug(9) API development - Cherry G. Mathew
Publication date: 2019-10-16
Playlist: EuroBSDcon 2017, France
Description: 
	Description:

This presentation is based upon the work of two authors: Cherry G. Mathew and
Santhosh Raju.
The current implementation of uvm(9) uses a static array to “manage”
memory segments. The uvm hotplug(9) API enables dynamically “managed”
memory segments allowing for the possibility of hot plugging and unplugging of
memory. During the process of implementing uvm hotplug(9) we used a Test
Driven Development methodology and Pair Programming to achieve our goal.

This talk focuses on how to re-organize the code for testing, test design
strategy for correctness and performance evaluation and the possibilities of
testing kernel code in userspace, specifically code pertaining to uvm(9). The
talk will also cover the methodology we used to achieve TDD on an existing
code base which lacked any prior formal written tests. In addition to the
above there will also be a small section on how tests(7) was used as a tool
to measure performance by load testing.

Speaker biography:

Cherry has been a NetBSD user since 2005 and a developer since 2006. His
first project was to import the ia64 FreeBSD sources to NetBSD.
Later he turned his attention to minor OF tweaks to the ibook G3 he
owned.

His serious contributions to NetBSD came after an internship with what
was then Xensource in the U.Cambridge startup scene. He committed SMP
support for NetBSD/Xen in 2011, the Xen memory ballooning driver for
NetBSD and the uvm hotplug interface.

Cherry also got FreeBSD to boot single user to Xen in its
Paravirtualised avatar – however this project was made redundant by the
excellent PVHVM support by royger@

Cherry likes to play with kernel code, electronics, walk up mountains,
travel footloose and hang out with the locals, pretend to cook, do a
bit of gardening / small scale farming, teach, take things apart, and
generally pretend that he is an intelligent sort.
Captions: 
	00:00:01,050 --> 00:00:07,890
Thank You Steven it's good to be back at

00:00:06,000 --> 00:00:13,469
a PSD conference after about ten years

00:00:07,890 --> 00:00:18,690
so glad to be here and meet familiar

00:00:13,469 --> 00:00:22,410
faces and new faces in person the stock

00:00:18,690 --> 00:00:28,619
is more or less identical to the talk

00:00:22,410 --> 00:00:32,640
that my co programmer Santosh Raju

00:00:28,619 --> 00:00:36,030
presented at Asia BSD Khan earlier

00:00:32,640 --> 00:00:38,280
earlier this year and we've done a

00:00:36,030 --> 00:00:41,010
little bit more work on it but nothing

00:00:38,280 --> 00:00:43,469
to show for it that we've published so

00:00:41,010 --> 00:00:49,340
I'll just stick to the to the you know

00:00:43,469 --> 00:00:55,289
it's the the baseline of the talk and

00:00:49,340 --> 00:00:56,820
essentially what I'm hoping to do with

00:00:55,289 --> 00:01:01,710
the stock is not really to present the

00:00:56,820 --> 00:01:06,900
guts of the development cycle or the

00:01:01,710 --> 00:01:09,390
experience but rather to kind of share

00:01:06,900 --> 00:01:11,549
the opportunity that I realized after 10

00:01:09,390 --> 00:01:13,439
years of kernel programming that a

00:01:11,549 --> 00:01:15,299
certain methodology or approach to

00:01:13,439 --> 00:01:19,350
coding can actually bring to the kernel

00:01:15,299 --> 00:01:23,909
programming and I feel like because this

00:01:19,350 --> 00:01:29,360
is this code was quite intrusive and at

00:01:23,909 --> 00:01:31,500
the deepest part of the VM subsystem

00:01:29,360 --> 00:01:35,210
typically you'd expect a lot of chaos

00:01:31,500 --> 00:01:38,939
because you know nebby is these VM

00:01:35,210 --> 00:01:41,670
powers about I think it's about 11 CPU

00:01:38,939 --> 00:01:44,100
architectures and about 70 platforms so

00:01:41,670 --> 00:01:46,680
it's it's really like a huge amount of

00:01:44,100 --> 00:01:47,820
dependencies and a lot of things can go

00:01:46,680 --> 00:01:53,009
wrong

00:01:47,820 --> 00:01:58,790
so really most of the technical details

00:01:53,009 --> 00:02:03,930
are in the man pages we won't cover that

00:01:58,790 --> 00:02:08,429
but I I just wanted to kind of go over

00:02:03,930 --> 00:02:10,709
the the broad sort of storyline with the

00:02:08,429 --> 00:02:12,870
hope that there that I'm not preaching

00:02:10,709 --> 00:02:13,930
to the choir and actually there are

00:02:12,870 --> 00:02:17,769
people from

00:02:13,930 --> 00:02:20,709
you know other BS DS and also diversity

00:02:17,769 --> 00:02:22,420
of port maintenance in here so it's a

00:02:20,709 --> 00:02:28,930
little bit of evangelism as well if you

00:02:22,420 --> 00:02:31,780
bear with me all right so this is the

00:02:28,930 --> 00:02:35,799
background the background is that all

00:02:31,780 --> 00:02:38,140
the current PS DS have a very simple

00:02:35,799 --> 00:02:40,659
implementation for keeping track of RAM

00:02:38,140 --> 00:02:48,760
on the machine essentially it maps to a

00:02:40,659 --> 00:02:53,430
very simple static array and this is the

00:02:48,760 --> 00:02:56,590
famous VM fistic max how to define that

00:02:53,430 --> 00:02:59,560
you know has come up for discussion when

00:02:56,590 --> 00:03:01,930
large amounts of RAM show up or the in

00:02:59,560 --> 00:03:05,579
the x86 world the EA 20 map is very

00:03:01,930 --> 00:03:09,190
fragmented and then things happen and

00:03:05,579 --> 00:03:13,150
this is obviously from a really you know

00:03:09,190 --> 00:03:15,250
a long time ago when Ram was you know

00:03:13,150 --> 00:03:17,319
not so much and the layouts were not as

00:03:15,250 --> 00:03:22,329
complicated as they are on modern

00:03:17,319 --> 00:03:25,540
machines and so the the discussion about

00:03:22,329 --> 00:03:29,169
this or the the idea for this project

00:03:25,540 --> 00:03:32,769
came up from the need for virtual memory

00:03:29,169 --> 00:03:35,919
machines to have memory added and

00:03:32,769 --> 00:03:38,250
removed dynamically at runtime and right

00:03:35,919 --> 00:03:43,359
now we have a balloon driver which is a

00:03:38,250 --> 00:03:48,609
hack to kind of remove allocated memory

00:03:43,359 --> 00:03:51,310
from a virtual machine but there's no

00:03:48,609 --> 00:03:56,079
way to put more in fernette bsd at least

00:03:51,310 --> 00:03:58,480
and for the other BS DS I believe so so

00:03:56,079 --> 00:04:00,549
we said okay how do we take on this

00:03:58,480 --> 00:04:02,139
problem this is really you know the

00:04:00,549 --> 00:04:05,319
tentacles go really all over the place

00:04:02,139 --> 00:04:07,000
it's really hard to you know pull things

00:04:05,319 --> 00:04:07,510
out and separate them how where do we

00:04:07,000 --> 00:04:10,720
start

00:04:07,510 --> 00:04:12,340
you know I mean there's there's VM fish

00:04:10,720 --> 00:04:14,349
mem is a global variable and it's strewn

00:04:12,340 --> 00:04:15,849
all over the place and it's used you

00:04:14,349 --> 00:04:18,370
know by every bit of the VM called

00:04:15,849 --> 00:04:20,440
everywhere so the first job obviously

00:04:18,370 --> 00:04:22,960
was not to try and do any hot plugging

00:04:20,440 --> 00:04:24,789
but to clean up and so we said okay well

00:04:22,960 --> 00:04:25,790
you know we want to use something more

00:04:24,789 --> 00:04:27,500
dynamic then and

00:04:25,790 --> 00:04:28,880
but we want to clean up so that we have

00:04:27,500 --> 00:04:31,430
hold on

00:04:28,880 --> 00:04:33,020
sort of you know the complexity

00:04:31,430 --> 00:04:37,930
otherwise it just explode so we said

00:04:33,020 --> 00:04:40,400
okay we'll clean it up and so and so

00:04:37,930 --> 00:04:43,070
that's when you know the idealist you

00:04:40,400 --> 00:04:44,990
know shows up and okay so now we've got

00:04:43,070 --> 00:04:47,360
a chance for a brand new API so I went

00:04:44,990 --> 00:04:50,180
away and designed this massive API with

00:04:47,360 --> 00:04:52,760
like tags and cache attributes and

00:04:50,180 --> 00:04:55,630
everything and then I had it all Specter

00:04:52,760 --> 00:04:58,940
out and we wrote ATF tests for it and

00:04:55,630 --> 00:05:00,860
then we ran it by people who actually

00:04:58,940 --> 00:05:03,080
had a lot of experience designing and

00:05:00,860 --> 00:05:04,940
building these things and they basically

00:05:03,080 --> 00:05:08,810
came back and said sorry I mean this is

00:05:04,940 --> 00:05:10,100
everything looks interesting so and we

00:05:08,810 --> 00:05:14,030
had a you know it hadn't gotten the

00:05:10,100 --> 00:05:15,620
kernel and so Chuck Silva was really

00:05:14,030 --> 00:05:18,080
helpful with a lot of a lot of feedback

00:05:15,620 --> 00:05:20,480
he helped with the initial integration

00:05:18,080 --> 00:05:22,900
of UVM the virtual memory management

00:05:20,480 --> 00:05:27,850
manager for their PhD into the kernel

00:05:22,900 --> 00:05:32,020
and so you know idealism gave way to

00:05:27,850 --> 00:05:34,550
sort of pragmatism and we decided to

00:05:32,020 --> 00:05:38,480
kind of work the other way so we thought

00:05:34,550 --> 00:05:41,450
okay well we'll try and put what exists

00:05:38,480 --> 00:05:43,570
and what we've cleaned up to kind of

00:05:41,450 --> 00:05:48,950
interface with the rest of the kernel

00:05:43,570 --> 00:05:50,960
through simpler API and so that would

00:05:48,950 --> 00:05:53,540
mean that we'd separate out the existing

00:05:50,960 --> 00:05:55,910
API and expose it via header files and

00:05:53,540 --> 00:05:57,890
hide everything else in file scope and

00:05:55,910 --> 00:06:00,140
then most importantly have a way to test

00:05:57,890 --> 00:06:02,180
unit test all these functions the first

00:06:00,140 --> 00:06:05,360
time in I think maybe 30 years since the

00:06:02,180 --> 00:06:07,750
BSD projects have begun that actually

00:06:05,360 --> 00:06:13,760
these functions were tested actually

00:06:07,750 --> 00:06:20,900
unit tests so so it was you know it was

00:06:13,760 --> 00:06:23,120
kind of interesting and and we yeah so a

00:06:20,900 --> 00:06:25,580
lot of the a lot of the detail was

00:06:23,120 --> 00:06:28,940
basically trying to get this situation

00:06:25,580 --> 00:06:32,000
going so like clean up the code you know

00:06:28,940 --> 00:06:35,360
sort them into separate files and then

00:06:32,000 --> 00:06:37,250
try and organize getting the specific

00:06:35,360 --> 00:06:39,560
functions the unit tested in us in a

00:06:37,250 --> 00:06:43,400
disciplined manner and

00:06:39,560 --> 00:06:45,890
we made the choice to kind of do this

00:06:43,400 --> 00:06:49,880
first so like this was in the live

00:06:45,890 --> 00:06:51,650
current tree so see you know because you

00:06:49,880 --> 00:06:53,840
can you can go away on a limb and and

00:06:51,650 --> 00:06:55,130
try and kind of you know create api's

00:06:53,840 --> 00:06:56,810
and then everything breaks when you try

00:06:55,130 --> 00:06:58,340
and integrate so we used the approach

00:06:56,810 --> 00:06:59,990
that before we try and unit test

00:06:58,340 --> 00:07:02,750
everything we'll try and do the

00:06:59,990 --> 00:07:04,790
reorganization in the live source in a

00:07:02,750 --> 00:07:07,100
way that doesn't change functionality

00:07:04,790 --> 00:07:09,290
but then we can also sort of put the

00:07:07,100 --> 00:07:12,169
tests on top of them and so we initially

00:07:09,290 --> 00:07:14,390
got UVM fist second dot C and dot h the

00:07:12,169 --> 00:07:16,910
code reorg into current before the tests

00:07:14,390 --> 00:07:18,590
were in place and now it was in a

00:07:16,910 --> 00:07:20,600
position where we could start looking at

00:07:18,590 --> 00:07:22,310
the unit tests and sometimes actually

00:07:20,600 --> 00:07:25,340
did this entire project on a Windows

00:07:22,310 --> 00:07:31,750
laptop with cygwin so that's testament e

00:07:25,340 --> 00:07:34,660
Nephi s DS of code code cleanliness so

00:07:31,750 --> 00:07:37,419
so yeah I mean so the organization

00:07:34,660 --> 00:07:40,940
didn't really affect the ability for

00:07:37,419 --> 00:07:47,990
nebby SD to boot and you know life went

00:07:40,940 --> 00:07:50,570
on as usual in current we didn't we

00:07:47,990 --> 00:07:52,970
didn't mess with that but then this is

00:07:50,570 --> 00:07:54,890
where sort of the modifications now

00:07:52,970 --> 00:07:58,460
started getting interesting so we

00:07:54,890 --> 00:08:00,080
essentially figured out how to now-now

00:07:58,460 --> 00:08:02,930
everything everything to do with the

00:08:00,080 --> 00:08:05,150
array management the segment's are a

00:08:02,930 --> 00:08:07,280
management went into that file you VM

00:08:05,150 --> 00:08:09,020
fist sexy and so now you've got like a

00:08:07,280 --> 00:08:11,419
little module that you can actually test

00:08:09,020 --> 00:08:15,669
and so we literally did a hash include

00:08:11,419 --> 00:08:18,800
UVM fist egg dot C inside of a userland

00:08:15,669 --> 00:08:20,900
you know test rig and so now we had

00:08:18,800 --> 00:08:22,790
enough to compile a separate user land

00:08:20,900 --> 00:08:26,570
binary which then we could run unit

00:08:22,790 --> 00:08:28,070
tests on and the UVM fist egg not see

00:08:26,570 --> 00:08:30,410
that we're using was not a copy it was a

00:08:28,070 --> 00:08:32,060
live copy that was in the kernel so you

00:08:30,410 --> 00:08:33,589
know everything that we were testing was

00:08:32,060 --> 00:08:34,849
already in the kernel but we were

00:08:33,589 --> 00:08:38,570
testing it completely in New Zealand

00:08:34,849 --> 00:08:41,390
which was pretty neat in that process

00:08:38,570 --> 00:08:44,120
obviously you know you have to export a

00:08:41,390 --> 00:08:46,550
subset of the kernel API to the to

00:08:44,120 --> 00:08:48,170
userland rompe does this by namespace

00:08:46,550 --> 00:08:50,150
rehashing and so on and so forth if

00:08:48,170 --> 00:08:51,950
people are familiar with ROM on a BSD

00:08:50,150 --> 00:08:53,300
but this is much simpler

00:08:51,950 --> 00:08:56,690
you know this code does

00:08:53,300 --> 00:08:59,930
really use you know a lot of API

00:08:56,690 --> 00:09:01,850
infrastructure so you know essentially

00:08:59,930 --> 00:09:04,580
it was came in malloc being stopped by

00:09:01,850 --> 00:09:11,240
malloc things like that so it's fairly

00:09:04,580 --> 00:09:13,279
straightforward and so so so now we

00:09:11,240 --> 00:09:17,779
could focus on the guts of the of the

00:09:13,279 --> 00:09:20,990
move from you know static array to RB

00:09:17,779 --> 00:09:22,220
tree and so we said okay well you know

00:09:20,990 --> 00:09:23,930
what are the other options and we went

00:09:22,220 --> 00:09:26,029
through the iterations of that and and

00:09:23,930 --> 00:09:28,370
essentially boiled down the decision

00:09:26,029 --> 00:09:30,019
about RB trees boiled down to you know

00:09:28,370 --> 00:09:31,399
what was available that was easiest to

00:09:30,019 --> 00:09:33,380
use and gave us the best promise

00:09:31,399 --> 00:09:35,899
performance and literally you know that

00:09:33,380 --> 00:09:38,870
B is these RB tree implementations right

00:09:35,899 --> 00:09:41,000
there waiting for us to use it and it

00:09:38,870 --> 00:09:43,370
made technical sense because the

00:09:41,000 --> 00:09:45,019
previous code had I mean an array if you

00:09:43,370 --> 00:09:46,880
want to insert

00:09:45,019 --> 00:09:50,269
you know things into an array you have

00:09:46,880 --> 00:09:52,490
to copy over you know bits of data and

00:09:50,269 --> 00:09:53,839
then insert and so there's a lot of

00:09:52,490 --> 00:09:55,850
overhead and so there were a couple of

00:09:53,839 --> 00:09:58,040
algorithms using hash defined so you

00:09:55,850 --> 00:10:01,660
could you know there was hash the hash

00:09:58,040 --> 00:10:04,550
define for binary searching for specific

00:10:01,660 --> 00:10:05,779
items in the array or for random search

00:10:04,550 --> 00:10:07,339
and so on and so there were three

00:10:05,779 --> 00:10:09,079
options and we said well you know all of

00:10:07,339 --> 00:10:10,490
that goes away and the RB tree takes

00:10:09,079 --> 00:10:12,260
care of that and so we just have one

00:10:10,490 --> 00:10:14,240
there's no compile time option now for

00:10:12,260 --> 00:10:18,079
searching in this array of segments of

00:10:14,240 --> 00:10:20,660
memory segments so there are no multiple

00:10:18,079 --> 00:10:23,930
strategies for maintaining segments and

00:10:20,660 --> 00:10:24,680
obviously less code clutter and yeah

00:10:23,930 --> 00:10:26,720
there were a couple of other options

00:10:24,680 --> 00:10:28,339
queue is anyway not a not an option

00:10:26,720 --> 00:10:30,800
because it was worst of both worlds but

00:10:28,339 --> 00:10:32,540
yeah RB tree was there and we thought

00:10:30,800 --> 00:10:34,430
we'd go with it and we also thought

00:10:32,540 --> 00:10:36,589
we'll do some performance analysis from

00:10:34,430 --> 00:10:39,740
user space because hey that's cool so

00:10:36,589 --> 00:10:45,680
we'll come to that at the end some of

00:10:39,740 --> 00:10:47,060
the details so essentially the thing

00:10:45,680 --> 00:10:50,660
that's interesting for the rest of the

00:10:47,060 --> 00:10:57,110
VM system is you know what does this bit

00:10:50,660 --> 00:10:58,880
of what is this page offset in RAM point

00:10:57,110 --> 00:11:01,699
to what would the the VM page structure

00:10:58,880 --> 00:11:03,500
basically so I want to look given a

00:11:01,699 --> 00:11:06,589
given a physical address I want to have

00:11:03,500 --> 00:11:07,190
the pointer to the data structure that

00:11:06,589 --> 00:11:10,930
represents

00:11:07,190 --> 00:11:15,920
that page of memory that's essentially

00:11:10,930 --> 00:11:20,540
what what this array gives you and so we

00:11:15,920 --> 00:11:24,290
essentially abstracted the idea of the

00:11:20,540 --> 00:11:28,250
index into the segment with a sort of

00:11:24,290 --> 00:11:31,190
abstract sort of data structure called

00:11:28,250 --> 00:11:33,290
Vivian physic underscore T but initially

00:11:31,190 --> 00:11:35,510
we before we did that we ensure that

00:11:33,290 --> 00:11:41,750
this abstraction was sitting on top of

00:11:35,510 --> 00:11:43,220
the array itself and so so yeah I mean I

00:11:41,750 --> 00:11:45,670
went over this again it was quite

00:11:43,220 --> 00:11:49,670
interesting it's a huge number of

00:11:45,670 --> 00:11:52,610
architectures to port to so so yeah so

00:11:49,670 --> 00:11:54,020
some of the challenges here and then

00:11:52,610 --> 00:11:56,900
obviously the performance indications

00:11:54,020 --> 00:12:00,950
implications so we look at that later

00:11:56,900 --> 00:12:02,800
so we introduced you know certain sets

00:12:00,950 --> 00:12:05,030
of abstraction so as you can see

00:12:02,800 --> 00:12:08,900
previously you had global variables

00:12:05,030 --> 00:12:11,360
being you know used outside of the

00:12:08,900 --> 00:12:13,760
module and then you would have sort of

00:12:11,360 --> 00:12:15,770
iteration through a macro and then we

00:12:13,760 --> 00:12:19,060
kind of hid everything behind explicit

00:12:15,770 --> 00:12:22,610
function calls and it can be argued that

00:12:19,060 --> 00:12:24,920
you know this is more less performance

00:12:22,610 --> 00:12:27,140
but then the set up and many of these

00:12:24,920 --> 00:12:30,530
loops are actually run only at boot time

00:12:27,140 --> 00:12:32,090
or during hot-plug itself but when it

00:12:30,530 --> 00:12:34,250
comes to actually searching for things

00:12:32,090 --> 00:12:35,540
in when the system is running you know

00:12:34,250 --> 00:12:37,610
so there's not really so much of a hot

00:12:35,540 --> 00:12:40,460
path so we thought well you know we'll

00:12:37,610 --> 00:12:43,120
run with it and then we'll see how it

00:12:40,460 --> 00:12:45,050
goes in terms of performance this

00:12:43,120 --> 00:12:47,420
function came up because it was

00:12:45,050 --> 00:12:51,020
interesting to understand what a valid

00:12:47,420 --> 00:12:54,050
segments was a lot of systems code sort

00:12:51,020 --> 00:12:56,870
of ad hoc puts in a placeholder value

00:12:54,050 --> 00:12:58,640
like minus 1 or you know null or

00:12:56,870 --> 00:13:01,880
something to signify that you know an

00:12:58,640 --> 00:13:04,040
entry is invalid and it got interesting

00:13:01,880 --> 00:13:07,430
because we had to make this explicit and

00:13:04,040 --> 00:13:11,030
say ok what does this index point to is

00:13:07,430 --> 00:13:13,190
this you know is this a valid address or

00:13:11,030 --> 00:13:15,589
is it a valid index and why is it a

00:13:13,190 --> 00:13:18,080
valid index or not so we wanted to pull

00:13:15,589 --> 00:13:19,640
out that assumption in terms of tests so

00:13:18,080 --> 00:13:20,720
literally the approach was if there's

00:13:19,640 --> 00:13:23,389
anything that we had

00:13:20,720 --> 00:13:26,389
doubt in terms of semantics or state we

00:13:23,389 --> 00:13:31,790
write a test for it and make it explicit

00:13:26,389 --> 00:13:34,519
and this was this was really useful

00:13:31,790 --> 00:13:37,970
because I think this kind of took care

00:13:34,519 --> 00:13:39,350
of the idea of the hero that we got that

00:13:37,970 --> 00:13:42,019
we got introduced to this morning

00:13:39,350 --> 00:13:45,889
because the hero can be reduced to ATF

00:13:42,019 --> 00:13:48,350
tests I think this is my discovery so

00:13:45,889 --> 00:13:49,819
you know a lot of the heroism is holding

00:13:48,350 --> 00:13:52,040
the state in your head and you know

00:13:49,819 --> 00:13:54,949
being able to process that data and make

00:13:52,040 --> 00:13:56,930
a call on design but I feel like if you

00:13:54,949 --> 00:13:59,029
can make it explicit in these tests and

00:13:56,930 --> 00:14:01,730
you can automate the open source hero

00:13:59,029 --> 00:14:03,920
into into test by and large I'm not

00:14:01,730 --> 00:14:05,899
making this claim but at least from my

00:14:03,920 --> 00:14:11,839
own ego it was interesting to you know I

00:14:05,899 --> 00:14:14,870
became less relevant basically so so we

00:14:11,839 --> 00:14:17,600
you know it goes to doesn't need to be

00:14:14,870 --> 00:14:19,550
mentioned that we did quite a bit of

00:14:17,600 --> 00:14:22,490
test coverage and all the tests you know

00:14:19,550 --> 00:14:25,309
there live tested using an eater in the

00:14:22,490 --> 00:14:29,889
NEB ESD build system so these are part

00:14:25,309 --> 00:14:32,329
of the formal test rig for a PhD and

00:14:29,889 --> 00:14:34,970
essentially what we did was we got the

00:14:32,329 --> 00:14:36,589
tests running so that the the the

00:14:34,970 --> 00:14:38,350
objective was for the semantics and the

00:14:36,589 --> 00:14:40,939
state information to be exported

00:14:38,350 --> 00:14:42,939
explicitly in these tests and we made

00:14:40,939 --> 00:14:45,230
sure that the current implementation

00:14:42,939 --> 00:14:47,269
could be tested rigorously first before

00:14:45,230 --> 00:14:48,769
we touched it with our b-tree we said we

00:14:47,269 --> 00:14:53,949
want the current implementation to be

00:14:48,769 --> 00:14:53,949
you know running under these tests and

00:14:54,639 --> 00:15:09,189
and so yeah and so yeah we had some

00:15:04,730 --> 00:15:17,230
interesting situation so the original

00:15:09,189 --> 00:15:19,250
idea for VM fish said the array was

00:15:17,230 --> 00:15:22,100
explicitly meant for boot time so you

00:15:19,250 --> 00:15:24,680
boot up your populate your array after

00:15:22,100 --> 00:15:26,180
machine-dependent code passes through

00:15:24,680 --> 00:15:28,129
your memory Hardware dependent memory

00:15:26,180 --> 00:15:30,019
tables and then you populate the array

00:15:28,129 --> 00:15:32,959
and then that's it you don't touch it

00:15:30,019 --> 00:15:34,340
afterwards but we were interested

00:15:32,959 --> 00:15:36,740
obviously in

00:15:34,340 --> 00:15:39,440
you know being able to plug in new

00:15:36,740 --> 00:15:43,370
memory and pull out bits of existing

00:15:39,440 --> 00:15:45,140
memory and so immediately that brings a

00:15:43,370 --> 00:15:46,930
lot of interesting issues especially

00:15:45,140 --> 00:15:51,200
fragmentation and how to manage stuff

00:15:46,930 --> 00:15:53,870
and in the process of this integration

00:15:51,200 --> 00:15:56,839
you know there were certain assumptions

00:15:53,870 --> 00:15:58,610
like for example pate fish load was just

00:15:56,839 --> 00:16:00,350
expected to succeed because if you

00:15:58,610 --> 00:16:02,510
couldn't you know if you couldn't

00:16:00,350 --> 00:16:04,310
register a physical page during boot

00:16:02,510 --> 00:16:06,589
time then that's like something serious

00:16:04,310 --> 00:16:08,060
is wrong so you panic basically and so

00:16:06,589 --> 00:16:09,920
that was the assumption in the boot code

00:16:08,060 --> 00:16:11,960
it was it was a void you VM page for his

00:16:09,920 --> 00:16:14,210
load well that's not great for testing

00:16:11,960 --> 00:16:16,730
because we need to know if this function

00:16:14,210 --> 00:16:18,740
actually you know did the right thing or

00:16:16,730 --> 00:16:20,330
not and the function should have some

00:16:18,740 --> 00:16:23,690
way to return that information so we

00:16:20,330 --> 00:16:26,150
essentially edited the function you know

00:16:23,690 --> 00:16:28,100
to return what happened to that load and

00:16:26,150 --> 00:16:32,570
so there were similar kinds of things

00:16:28,100 --> 00:16:35,150
that were you know modifications that we

00:16:32,570 --> 00:16:38,020
that we did so yeah so sorry I'm

00:16:35,150 --> 00:16:42,650
speaking I'm going ahead of myself here

00:16:38,020 --> 00:16:44,560
but essentially again just repeat that

00:16:42,650 --> 00:16:47,270
reiterate at that point it was to make

00:16:44,560 --> 00:16:48,950
you know the test framework pull out all

00:16:47,270 --> 00:16:50,900
these assumptions that were inside a

00:16:48,950 --> 00:16:52,460
programmers heads you know and just you

00:16:50,900 --> 00:16:59,600
know put it down in writing and make it

00:16:52,460 --> 00:17:03,680
as explicit as possible so so so here's

00:16:59,600 --> 00:17:06,589
here's a like a slightly subtle one we

00:17:03,680 --> 00:17:09,530
kept getting this failure for get

00:17:06,589 --> 00:17:13,670
previous segments so given an array

00:17:09,530 --> 00:17:16,310
offset in that fist seg array can you

00:17:13,670 --> 00:17:21,980
get the the function is asked can you

00:17:16,310 --> 00:17:24,140
give me the previous you know entry and

00:17:21,980 --> 00:17:27,560
obviously you would be in an array would

00:17:24,140 --> 00:17:31,640
be just you know - index minus one but

00:17:27,560 --> 00:17:33,260
this was failing for us and this was

00:17:31,640 --> 00:17:35,660
kind of a situation in which that was

00:17:33,260 --> 00:17:37,400
happening and for a minute it had me

00:17:35,660 --> 00:17:40,420
going wondering what the heck is going

00:17:37,400 --> 00:17:42,950
on and at this point we had the RB tree

00:17:40,420 --> 00:17:44,570
implementation as well so we were

00:17:42,950 --> 00:17:47,240
testing both together side by side we

00:17:44,570 --> 00:17:48,110
had the test running on the static array

00:17:47,240 --> 00:17:52,460
implementation

00:17:48,110 --> 00:17:56,809
the arbitrary implementation and what we

00:17:52,460 --> 00:17:59,120
discovered was that the reason that get

00:17:56,809 --> 00:18:02,360
probe was failing was because the

00:17:59,120 --> 00:18:05,750
assumption of what of the of the

00:18:02,360 --> 00:18:06,980
property of the of the handle that we

00:18:05,750 --> 00:18:09,170
were passing through that to that

00:18:06,980 --> 00:18:11,510
function so for the array it would be

00:18:09,170 --> 00:18:13,220
the index value but for the RB tree it

00:18:11,510 --> 00:18:15,770
would be a pointer and there were

00:18:13,220 --> 00:18:17,750
certain implications for that which I'll

00:18:15,770 --> 00:18:20,299
talk about in a second so this is the

00:18:17,750 --> 00:18:22,669
static array implementation so B is a

00:18:20,299 --> 00:18:24,679
loaded segment the system knows about it

00:18:22,669 --> 00:18:26,840
it's already been given physic loaded

00:18:24,679 --> 00:18:29,000
and therefore the index if you were

00:18:26,840 --> 00:18:30,950
inserting segments would be in

00:18:29,000 --> 00:18:32,929
increasing monotonically increasing

00:18:30,950 --> 00:18:35,510
order so zero and then if a new one

00:18:32,929 --> 00:18:37,340
where a was inserted what would happen

00:18:35,510 --> 00:18:39,860
would be that that would get copied over

00:18:37,340 --> 00:18:43,429
and then a would go in and zero so you

00:18:39,860 --> 00:18:46,490
can you can see here that to refer to B

00:18:43,429 --> 00:18:50,210
initially you would want to refer to the

00:18:46,490 --> 00:18:53,179
index value as 0 but then once the

00:18:50,210 --> 00:18:54,500
insert has been done 0 refers to a so

00:18:53,179 --> 00:18:55,880
this means that the handle was not

00:18:54,500 --> 00:18:57,410
immutable we were making the implicit

00:18:55,880 --> 00:18:59,570
assumption that a handle was immutable

00:18:57,410 --> 00:19:01,760
but in this case the handle was not

00:18:59,570 --> 00:19:04,160
immutable in this implementation and so

00:19:01,760 --> 00:19:05,480
you know this brought it out where for

00:19:04,160 --> 00:19:07,790
the RB tree there's no problem because

00:19:05,480 --> 00:19:12,010
the pointer remains the same you know

00:19:07,790 --> 00:19:16,309
the handle is immutable and so

00:19:12,010 --> 00:19:17,840
essentially again we exported that you

00:19:16,309 --> 00:19:19,790
know assumption into a test and said ok

00:19:17,840 --> 00:19:24,290
well slap a test onto that so that we

00:19:19,790 --> 00:19:27,590
know exactly what's going on so yeah so

00:19:24,290 --> 00:19:30,470
now the the property I mean the the

00:19:27,590 --> 00:19:33,940
behavior of both implementations is

00:19:30,470 --> 00:19:37,760
explicit and as explicitly tested for

00:19:33,940 --> 00:19:40,850
right at that point of time we were also

00:19:37,760 --> 00:19:42,440
supporting conditional compiles of this

00:19:40,850 --> 00:19:43,850
static in fact we still do have

00:19:42,440 --> 00:19:46,460
conditional compiles of the static so

00:19:43,850 --> 00:19:48,980
you can use both in BSD right now so you

00:19:46,460 --> 00:19:50,299
don't need to actually use RB tree you

00:19:48,980 --> 00:19:54,169
can also use the old static

00:19:50,299 --> 00:19:56,500
implementation right so guess what

00:19:54,169 --> 00:19:56,500
happens

00:19:59,000 --> 00:20:05,250
yeah so so at this point I was on my own

00:20:03,360 --> 00:20:07,200
sometimes there wasn't very familiar

00:20:05,250 --> 00:20:09,840
with the net bsd system itself and it

00:20:07,200 --> 00:20:12,870
was i was pre pleased that you know i

00:20:09,840 --> 00:20:14,460
was able to break down the problem in in

00:20:12,870 --> 00:20:15,630
a language and in a context that he

00:20:14,460 --> 00:20:17,580
could understand as a user space

00:20:15,630 --> 00:20:19,590
programmer and he was happy that he

00:20:17,580 --> 00:20:23,490
could actually get some you know worked

00:20:19,590 --> 00:20:25,560
on on on kernel code but at this point i

00:20:23,490 --> 00:20:26,850
was on my own basically and i must give

00:20:25,560 --> 00:20:28,170
him a lot of credit he's an amazing

00:20:26,850 --> 00:20:33,590
program and one of the best i've met

00:20:28,170 --> 00:20:36,840
actually and well that happened and

00:20:33,590 --> 00:20:40,230
normally at this point i'd be panicking

00:20:36,840 --> 00:20:42,660
because like if i didn't have all the

00:20:40,230 --> 00:20:45,090
state information out there you know

00:20:42,660 --> 00:20:47,120
it's the vm system there's like a whole

00:20:45,090 --> 00:20:51,360
lot of things that could be going wrong

00:20:47,120 --> 00:20:53,250
but that wasn't the case i mean we had

00:20:51,360 --> 00:20:56,060
the tests and we knew exactly what was

00:20:53,250 --> 00:20:58,530
going on so we could actually flip

00:20:56,060 --> 00:21:00,690
assumptions inside the tests to figure

00:20:58,530 --> 00:21:04,200
out what was going on and we quickly

00:21:00,690 --> 00:21:06,870
realized that the tests themselves

00:21:04,200 --> 00:21:08,280
assumed that malloc would always be

00:21:06,870 --> 00:21:11,130
available obviously because the tests

00:21:08,280 --> 00:21:14,340
run in userspace but obviously the

00:21:11,130 --> 00:21:17,040
kernel doesn't have malloc running when

00:21:14,340 --> 00:21:20,040
it starts off you have to figure out how

00:21:17,040 --> 00:21:21,810
to present malloc you know to the kernel

00:21:20,040 --> 00:21:24,510
that's booting so that's the bootstrap

00:21:21,810 --> 00:21:27,480
problem that's very peculiar to you know

00:21:24,510 --> 00:21:32,280
a boot up situation I think FreeBSD has

00:21:27,480 --> 00:21:33,690
a VM API kind of which does is in a very

00:21:32,280 --> 00:21:37,530
cloudy way I forget the name of the API

00:21:33,690 --> 00:21:41,430
now but anyway but that's in a different

00:21:37,530 --> 00:21:45,870
context this anyway so we essentially

00:21:41,430 --> 00:21:48,840
what we did was to kind of provide so we

00:21:45,870 --> 00:21:52,290
tweaked the the the the code a little

00:21:48,840 --> 00:21:54,030
bit so that we could have back-end

00:21:52,290 --> 00:21:55,440
functions to do the allocation a little

00:21:54,030 --> 00:21:56,610
bit like the pool allocator I don't know

00:21:55,440 --> 00:21:59,730
if people have used the pool alligator

00:21:56,610 --> 00:22:01,110
but pool cash alligator but basically

00:21:59,730 --> 00:22:03,960
your slab alligators I think have this

00:22:01,110 --> 00:22:06,720
backing function thing where you can you

00:22:03,960 --> 00:22:08,850
know allocate either device addressable

00:22:06,720 --> 00:22:10,890
memory or actual you know system RAM

00:22:08,850 --> 00:22:12,270
memory but anyway we use the same kind

00:22:10,890 --> 00:22:13,950
of approach so we had like

00:22:12,270 --> 00:22:15,720
callback function that would get

00:22:13,950 --> 00:22:19,290
initialized at boot time which did the

00:22:15,720 --> 00:22:21,390
static allocation for boot time Ram and

00:22:19,290 --> 00:22:23,640
then once we figured out that page in it

00:22:21,390 --> 00:22:25,020
done was you know Don and came him Alec

00:22:23,640 --> 00:22:27,240
could work then we'd switched the

00:22:25,020 --> 00:22:29,520
alligators to the regular kernel memory

00:22:27,240 --> 00:22:33,480
allocator so that was a little bit of a

00:22:29,520 --> 00:22:35,070
twist in the in the story but but yeah I

00:22:33,480 --> 00:22:37,860
mean again we got that

00:22:35,070 --> 00:22:40,170
so we X again that assumption we put it

00:22:37,860 --> 00:22:41,730
out in tests so we explicitly put the

00:22:40,170 --> 00:22:44,700
you know the assumption of boot time

00:22:41,730 --> 00:22:49,800
allocation versus you know run time

00:22:44,700 --> 00:22:54,240
allocation inside of tests so at this

00:22:49,800 --> 00:22:56,070
point more or less x86 was working we

00:22:54,240 --> 00:22:58,290
had an integrated code yet mind because

00:22:56,070 --> 00:23:00,420
you can't do that I mean there's 70

00:22:58,290 --> 00:23:04,950
architectures gonna get breaking if you

00:23:00,420 --> 00:23:06,660
start pushing code so so we started

00:23:04,950 --> 00:23:09,900
looking at okay so we're trying to you

00:23:06,660 --> 00:23:14,040
know fragment memory and sorry har plug

00:23:09,900 --> 00:23:15,480
and and replug memory and we started

00:23:14,040 --> 00:23:18,300
looking at the other fragmentation

00:23:15,480 --> 00:23:21,330
problem so so you have a segment and

00:23:18,300 --> 00:23:25,309
then the so how net bsd does this is

00:23:21,330 --> 00:23:30,690
that there is an array that essentially

00:23:25,309 --> 00:23:33,000
holds the physical data structures of

00:23:30,690 --> 00:23:35,429
all the system pages in memory

00:23:33,000 --> 00:23:37,320
now segments are not at page resolution

00:23:35,429 --> 00:23:40,470
segments are it can be like a one gig

00:23:37,320 --> 00:23:42,480
segment followed by one megabyte segment

00:23:40,470 --> 00:23:44,910
followed by so on and so forth but pages

00:23:42,480 --> 00:23:49,200
are for K pages on a BSD I think on most

00:23:44,910 --> 00:23:52,200
architectures so PDS is an array of VM

00:23:49,200 --> 00:23:53,970
page structures and not pointers these

00:23:52,200 --> 00:23:56,700
are actual structures that's where the

00:23:53,970 --> 00:24:00,600
actual data about the pages are stored

00:23:56,700 --> 00:24:06,630
so PGs is really important because again

00:24:00,600 --> 00:24:09,090
we go back to the array problem where as

00:24:06,630 --> 00:24:12,059
we saw in vm segment you know that was

00:24:09,090 --> 00:24:13,679
an area and then PGs is an array as well

00:24:12,059 --> 00:24:17,100
so now we've got the same problem coming

00:24:13,679 --> 00:24:18,929
back at us and we were like oh my gosh

00:24:17,100 --> 00:24:20,280
now are we going to have to use the RB

00:24:18,929 --> 00:24:25,600
tree again have to go over the whole

00:24:20,280 --> 00:24:29,340
thing again but extend

00:24:25,600 --> 00:24:31,960
the rescue so extent is a really cool

00:24:29,340 --> 00:24:33,370
extent manager so any namespace you give

00:24:31,960 --> 00:24:37,299
it it'll manage the fragmentation

00:24:33,370 --> 00:24:39,039
insert/remove operations on any any

00:24:37,299 --> 00:24:40,990
namespace it doesn't have to be memory

00:24:39,039 --> 00:24:43,059
so it doesn't make any assumptions about

00:24:40,990 --> 00:24:46,299
backing or you can tell it not to make

00:24:43,059 --> 00:24:48,100
assumptions about backing memory and so

00:24:46,299 --> 00:24:50,880
that was it would just plug that into

00:24:48,100 --> 00:24:53,020
the PGS array management so instead of

00:24:50,880 --> 00:24:55,720
manually going and trying to mess with

00:24:53,020 --> 00:25:00,520
the PGS array we just said okay well

00:24:55,720 --> 00:25:03,880
will will will use extent will register

00:25:00,520 --> 00:25:05,710
the entire PGs array name space into the

00:25:03,880 --> 00:25:07,059
extent and then just tell the extent

00:25:05,710 --> 00:25:08,380
manager to do it for us and it was

00:25:07,059 --> 00:25:14,679
brilliant it just worked he was plug and

00:25:08,380 --> 00:25:17,409
play literally and so at this point we

00:25:14,679 --> 00:25:20,020
were ready so yeah at this point things

00:25:17,409 --> 00:25:23,370
were things were going well and Nick

00:25:20,020 --> 00:25:26,020
Hudson mostly Amaya helped me integrate

00:25:23,370 --> 00:25:27,970
most of the code and again like what

00:25:26,020 --> 00:25:31,330
could have been a real mess it was like

00:25:27,970 --> 00:25:33,970
two weeks and most ports were like done

00:25:31,330 --> 00:25:36,610
I mean 70 70 architectures I was pretty

00:25:33,970 --> 00:25:38,860
impressed i it could have been a

00:25:36,610 --> 00:25:45,159
nightmare but you know the methodology

00:25:38,860 --> 00:25:47,890
really I think performance is really

00:25:45,159 --> 00:25:50,080
interesting I mean it's really simple

00:25:47,890 --> 00:25:51,880
performance testing on for this code

00:25:50,080 --> 00:25:54,789
base because oh we all we are interested

00:25:51,880 --> 00:25:57,669
in is how fast can you find you know a

00:25:54,789 --> 00:25:59,440
segment given a physical offset that is

00:25:57,669 --> 00:26:03,280
literally the fast path operation in

00:25:59,440 --> 00:26:05,470
this whole API and so it was really and

00:26:03,280 --> 00:26:07,900
there's a there's a essentially a macro

00:26:05,470 --> 00:26:09,340
that wraps over this function calls so

00:26:07,900 --> 00:26:11,320
literally we just had two performance

00:26:09,340 --> 00:26:14,740
tests one function and that was pretty

00:26:11,320 --> 00:26:19,260
straightforward so what did we do well

00:26:14,740 --> 00:26:21,880
we had you know the these these tests

00:26:19,260 --> 00:26:26,470
had to factor in the fact that there

00:26:21,880 --> 00:26:28,570
were fragments memory segments because

00:26:26,470 --> 00:26:30,700
this was not the case previously before

00:26:28,570 --> 00:26:31,960
unplug was available you know all the

00:26:30,700 --> 00:26:34,270
segments would be contiguous there's no

00:26:31,960 --> 00:26:35,770
big deal you know finding out the same

00:26:34,270 --> 00:26:38,080
algorithm could find them out but here

00:26:35,770 --> 00:26:40,510
if your segments were fragmented then

00:26:38,080 --> 00:26:43,120
your RB tree starts getting

00:26:40,510 --> 00:26:47,800
you know filled up in not necessarily

00:26:43,120 --> 00:26:51,130
symmetric ways right so so we wanted to

00:26:47,800 --> 00:26:54,160
see what impact that had on performance

00:26:51,130 --> 00:26:58,750
and so what we did was we kind of faked

00:26:54,160 --> 00:27:03,160
the so we used random to essentially

00:26:58,750 --> 00:27:05,440
fake the the effect of a running system

00:27:03,160 --> 00:27:10,300
so we use the random system system

00:27:05,440 --> 00:27:16,540
called essentially sort of spread out

00:27:10,300 --> 00:27:18,130
the the physical address offsets so okay

00:27:16,540 --> 00:27:20,710
so you could test it saying okay from

00:27:18,130 --> 00:27:23,620
zero to the end of RAM you know run

00:27:20,710 --> 00:27:28,660
through a loop and see what paid fist

00:27:23,620 --> 00:27:30,370
like fine gives you but in this case you

00:27:28,660 --> 00:27:31,780
know that you know that's one way to do

00:27:30,370 --> 00:27:33,730
it or we could just sort of randomly

00:27:31,780 --> 00:27:37,000
look up offsets and that's that's what

00:27:33,730 --> 00:27:38,740
we did so so we did like you know ten

00:27:37,000 --> 00:27:43,360
you know hundred calls to ten million

00:27:38,740 --> 00:27:47,380
calls kind of kind of breadth of calls

00:27:43,360 --> 00:27:50,410
and and again you know random so on a

00:27:47,380 --> 00:27:52,630
real running system physical addresses

00:27:50,410 --> 00:27:54,760
and not necessarily randomly looked up

00:27:52,630 --> 00:27:55,690
right I mean there's a lot of caching a

00:27:54,760 --> 00:27:58,450
lot of code that keeps it running

00:27:55,690 --> 00:27:59,770
repetitively so this is not really an

00:27:58,450 --> 00:28:02,260
indicator of the actual system

00:27:59,770 --> 00:28:04,990
performance and this is sort of work to

00:28:02,260 --> 00:28:06,250
do or maybe a comment about how this

00:28:04,990 --> 00:28:07,660
kind of testing can be done in a

00:28:06,250 --> 00:28:11,290
slightly different way load testing I

00:28:07,660 --> 00:28:18,130
think is not really mapping straight on

00:28:11,290 --> 00:28:20,200
to ATF testing but essentially yeah we

00:28:18,130 --> 00:28:21,840
just you know we used what we had and

00:28:20,200 --> 00:28:27,370
all this stuff was done in user space

00:28:21,840 --> 00:28:30,450
and so yeah so we you know we applied

00:28:27,370 --> 00:28:32,590
different scenarios a fixed size segment

00:28:30,450 --> 00:28:36,600
fragmented segments just to see what

00:28:32,590 --> 00:28:36,600
effects it had on the performance and

00:28:37,050 --> 00:28:42,760
but yeah so this is what it would look

00:28:41,170 --> 00:28:46,420
like numbers will be pulled out like

00:28:42,760 --> 00:28:49,210
this and then yeah and then we got

00:28:46,420 --> 00:28:52,059
tables and you're welcome to look at

00:28:49,210 --> 00:28:52,870
that later on but you can yeah I mean so

00:28:52,059 --> 00:28:54,250
the the

00:28:52,870 --> 00:28:57,039
you can see the you know you can make a

00:28:54,250 --> 00:28:58,600
quick scan through the tables but the

00:28:57,039 --> 00:28:59,740
top one is our bee tree and the bottom

00:28:58,600 --> 00:29:01,029
one is static area and you can

00:28:59,740 --> 00:29:04,120
immediately see that the arbitrary

00:29:01,029 --> 00:29:08,049
implementation is taking more time you

00:29:04,120 --> 00:29:11,049
know to find segments so we wanted to

00:29:08,049 --> 00:29:15,730
see you know how bad it is so yeah this

00:29:11,049 --> 00:29:17,799
is just you know testing between 10

00:29:15,730 --> 00:29:23,289
million segments and 100 million

00:29:17,799 --> 00:29:25,090
segments kind of thing so yeah so this

00:29:23,289 --> 00:29:27,730
was the statistical kind of methodology

00:29:25,090 --> 00:29:29,890
we use because we were using random so

00:29:27,730 --> 00:29:32,559
we had to kind of compensate for that by

00:29:29,890 --> 00:29:35,980
using making a statistical approximation

00:29:32,559 --> 00:29:37,690
about how you know how much confidence

00:29:35,980 --> 00:29:41,260
do we have the actual number we're

00:29:37,690 --> 00:29:44,490
getting and so we kind of you know we

00:29:41,260 --> 00:29:48,100
stuck to a margin of error of 95 percent

00:29:44,490 --> 00:29:50,309
I didn't do this work so if you have any

00:29:48,100 --> 00:29:55,929
questions I'll redirect them to Santos

00:29:50,309 --> 00:29:58,179
but anyway so that's the number five

00:29:55,929 --> 00:30:01,659
point five nine percent degradation and

00:29:58,179 --> 00:30:03,490
performance which you know depending on

00:30:01,659 --> 00:30:06,669
how important it is for you to plug your

00:30:03,490 --> 00:30:08,770
unplug your memory live is a design

00:30:06,669 --> 00:30:12,760
choice that you know somebody can manage

00:30:08,770 --> 00:30:19,649
I can take so so there we are

00:30:12,760 --> 00:30:23,049
and right okay so that's the details of

00:30:19,649 --> 00:30:26,500
fragmentation and how we did the tests

00:30:23,049 --> 00:30:30,760
and colorful graphs but you can kind of

00:30:26,500 --> 00:30:32,320
get an idea yeah so those are graphs of

00:30:30,760 --> 00:30:35,950
the average maximum and minimum times

00:30:32,320 --> 00:30:40,090
for different you know numbers of

00:30:35,950 --> 00:30:42,880
segments that were being tested and yeah

00:30:40,090 --> 00:30:44,860
just to go over some of the things that

00:30:42,880 --> 00:30:49,510
we were probably missing originally when

00:30:44,860 --> 00:30:51,700
I designed the API the ideal API that

00:30:49,510 --> 00:30:53,799
had all the you know cache attributes

00:30:51,700 --> 00:30:54,279
and this and that we actually looked at

00:30:53,799 --> 00:30:56,559
rump

00:30:54,279 --> 00:30:58,149
testing testing on ROM but the problem

00:30:56,559 --> 00:31:00,279
with rump was there was a whole load of

00:30:58,149 --> 00:31:01,720
assumptions inside of rump and a whole

00:31:00,279 --> 00:31:04,360
load of dependencies that we'd have to

00:31:01,720 --> 00:31:06,309
bring in and so we abandoned that but I

00:31:04,360 --> 00:31:07,030
think for load testing rompers pretty

00:31:06,309 --> 00:31:10,000
good because it

00:31:07,030 --> 00:31:11,920
an interface with actual user space

00:31:10,000 --> 00:31:14,650
workloads and exercise your cold paths

00:31:11,920 --> 00:31:17,520
so I think rump is interesting for load

00:31:14,650 --> 00:31:20,620
testing of this kind of behavior

00:31:17,520 --> 00:31:22,120
yeah cold well we could we could have a

00:31:20,620 --> 00:31:24,180
bit more code coverage I mean that's the

00:31:22,120 --> 00:31:26,110
question of how much time you have to

00:31:24,180 --> 00:31:29,380
address but more importantly I think

00:31:26,110 --> 00:31:32,800
maybe if we could get some live numbers

00:31:29,380 --> 00:31:34,720
as well to make a comparison to the

00:31:32,800 --> 00:31:38,860
numbers we got with user space on

00:31:34,720 --> 00:31:40,690
testing with the dtrace I'm not sure

00:31:38,860 --> 00:31:42,580
DTrace I haven't used deep trace on net

00:31:40,690 --> 00:31:45,610
bsd but i don't know what the state of

00:31:42,580 --> 00:31:47,530
it is anyway maybe we will get some

00:31:45,610 --> 00:31:50,710
feedback on that later so yeah this is

00:31:47,530 --> 00:31:52,200
the the big you know moment of

00:31:50,710 --> 00:31:55,120
enlightenment

00:31:52,200 --> 00:31:57,610
you know existing techniques nothing

00:31:55,120 --> 00:31:58,960
fancy you know hero isn't required just

00:31:57,610 --> 00:32:01,570
you know apply interest you know

00:31:58,960 --> 00:32:05,110
standard techniques and you can do cool

00:32:01,570 --> 00:32:07,600
stuff I definitely had a much less

00:32:05,110 --> 00:32:12,990
stressful experience than debugging Xen

00:32:07,600 --> 00:32:16,300
intra panelist so this is really cool

00:32:12,990 --> 00:32:19,180
and obviously you know it did help that

00:32:16,300 --> 00:32:20,710
there were API is inside of net BSD that

00:32:19,180 --> 00:32:23,350
made the least amount of assumptions

00:32:20,710 --> 00:32:24,640
about what those API is the context in

00:32:23,350 --> 00:32:27,040
which those APRs could be used for

00:32:24,640 --> 00:32:28,420
example you know RB tree could be it's a

00:32:27,040 --> 00:32:30,010
you know it's a user space library but

00:32:28,420 --> 00:32:32,950
they can be used in the kernel or the

00:32:30,010 --> 00:32:34,450
extent you know API does not assume

00:32:32,950 --> 00:32:36,400
anything about the nature of the backing

00:32:34,450 --> 00:32:38,440
Ram for example so really cool to have

00:32:36,400 --> 00:32:39,070
that kind of tooling available inside of

00:32:38,440 --> 00:32:42,340
the kernel

00:32:39,070 --> 00:32:44,950
so I mean literally this sort of pitch

00:32:42,340 --> 00:32:48,250
is for people who have access to or know

00:32:44,950 --> 00:32:50,410
about hardware that can do hot-plug

00:32:48,250 --> 00:32:52,180
outside of virtual virtualized

00:32:50,410 --> 00:32:55,750
environment or even inside virtualized

00:32:52,180 --> 00:32:57,490
environment I think VirtualBox qmu has

00:32:55,750 --> 00:32:59,080
an ACPI interface for hot plugging but I

00:32:57,490 --> 00:33:01,240
had to look at but it was wait you know

00:32:59,080 --> 00:33:02,710
our ACPI stuff is a bit shaky and at the

00:33:01,240 --> 00:33:03,520
moment so a little bit more work

00:33:02,710 --> 00:33:05,380
required for that

00:33:03,520 --> 00:33:09,910
Xen is the easiest to do and I've done

00:33:05,380 --> 00:33:12,490
like a really simple stub plug

00:33:09,910 --> 00:33:15,970
implementation what's missing is that if

00:33:12,490 --> 00:33:18,460
you want to unplug code your driver

00:33:15,970 --> 00:33:20,110
needs to are kind of in so there's a

00:33:18,460 --> 00:33:20,860
little bit more work required for unplug

00:33:20,110 --> 00:33:23,080
because

00:33:20,860 --> 00:33:24,850
obviously you can't just unplug memory

00:33:23,080 --> 00:33:28,270
that's being DM aid to or like you know

00:33:24,850 --> 00:33:29,610
memory that you know is being used for

00:33:28,270 --> 00:33:33,670
something so there needs to be a way to

00:33:29,610 --> 00:33:37,330
figure out you know if if a page that

00:33:33,670 --> 00:33:40,270
you want to unplug is in use or not and

00:33:37,330 --> 00:33:43,090
that you know this can be you know this

00:33:40,270 --> 00:33:45,160
enables that but it it doesn't do it for

00:33:43,090 --> 00:33:46,600
you so if you're writing a driver

00:33:45,160 --> 00:33:47,800
obviously you have to you know there's a

00:33:46,600 --> 00:33:49,600
little bit more work for that and also

00:33:47,800 --> 00:33:53,350
exporting this functionality to use a

00:33:49,600 --> 00:33:55,570
space you know you use if you have a ram

00:33:53,350 --> 00:33:57,580
control kind of utility that talks to

00:33:55,570 --> 00:33:59,380
the kernel and says pull out this offset

00:33:57,580 --> 00:34:01,870
of ram or whatever so that api needs to

00:33:59,380 --> 00:34:03,610
be designed and so on but the basic

00:34:01,870 --> 00:34:06,490
interface with the virtual memory

00:34:03,610 --> 00:34:08,560
manager is in place and it's reason to

00:34:06,490 --> 00:34:10,600
be robust I think I haven't seen any

00:34:08,560 --> 00:34:13,000
complaints of weird page faults or

00:34:10,600 --> 00:34:15,390
panics on various architectures so far

00:34:13,000 --> 00:34:18,130
at least so I'm keeping an eye on it but

00:34:15,390 --> 00:34:20,170
so yeah and the cool thing is that the

00:34:18,130 --> 00:34:23,710
VM system is probably the you know the

00:34:20,170 --> 00:34:26,080
the most one of the oldest in pedigree

00:34:23,710 --> 00:34:28,690
because obviously all the BSD started

00:34:26,080 --> 00:34:30,100
off from a similar vm system so I feel

00:34:28,690 --> 00:34:34,060
like other Beasley's could actually

00:34:30,100 --> 00:34:35,470
benefit from this work so freebsd

00:34:34,060 --> 00:34:38,169
especially i've had a look at their vm

00:34:35,470 --> 00:34:41,740
code mm-hmm but uh you know they could

00:34:38,169 --> 00:34:45,250
benefit from this stuff OpenBSD I think

00:34:41,740 --> 00:34:46,450
uses you VM so I think it should be

00:34:45,250 --> 00:34:47,770
fairly straightforward for them but

00:34:46,450 --> 00:34:49,270
Phoebe SD might need to put a little bit

00:34:47,770 --> 00:34:52,240
more work into it

00:34:49,270 --> 00:34:57,100
thank you all these people Philips here

00:34:52,240 --> 00:34:59,830
and yeah foundation we coded this by the

00:34:57,100 --> 00:35:03,730
beach we spent four months who's

00:34:59,830 --> 00:35:06,610
brilliant I highly recommend it knows

00:35:03,730 --> 00:35:10,570
about it needs means in the venue yeah

00:35:06,610 --> 00:35:16,960
and yeah that's that's the credits and

00:35:10,570 --> 00:35:19,650
yeah thank you everyone we've got five

00:35:16,960 --> 00:35:19,650
minutes for questions

00:35:22,750 --> 00:35:30,890
can we kill the balloon driver now

00:35:26,770 --> 00:35:36,020
well the balloon driver could use hot

00:35:30,890 --> 00:35:38,690
lug so yeah right now the balloon driver

00:35:36,020 --> 00:35:40,579
uses the hack that it uses came in Alec

00:35:38,690 --> 00:35:42,980
to reserve memory and then you know keep

00:35:40,579 --> 00:35:45,770
it away from the rest of UVM from

00:35:42,980 --> 00:35:47,930
reusing it but it also interfaces with

00:35:45,770 --> 00:35:49,609
the hypervisor you know so there's an

00:35:47,930 --> 00:35:51,349
assumption there about how that API

00:35:49,609 --> 00:35:54,530
works so we could reduce it to an API

00:35:51,349 --> 00:35:55,970
that's kind of named balloon but does

00:35:54,530 --> 00:36:00,890
something else should probably be called

00:35:55,970 --> 00:36:07,880
hop like now but yeah they came in Alec

00:36:00,890 --> 00:36:09,770
stuff would change for the benefit of

00:36:07,880 --> 00:36:13,520
the video can you explain what Anita is

00:36:09,770 --> 00:36:16,599
and how it worked when you workflow yeah

00:36:13,520 --> 00:36:21,380
so Anita is a continuous integration

00:36:16,599 --> 00:36:23,809
setup than their BSD has where all the

00:36:21,380 --> 00:36:27,170
ATF tests inside of the net BSD source

00:36:23,809 --> 00:36:29,720
tree correct me if I'm wrong are kind of

00:36:27,170 --> 00:36:32,299
so your source is built and then all the

00:36:29,720 --> 00:36:37,819
tests are run I'm not sure for what the

00:36:32,299 --> 00:36:40,880
frequency is sorry and if there's a

00:36:37,819 --> 00:36:45,799
Python you know wrapper that actually

00:36:40,880 --> 00:36:48,710
takes a build and after that it spawns

00:36:45,799 --> 00:36:51,470
up either a UVM or another emulator runs

00:36:48,710 --> 00:36:53,750
a net BSD install you know from scratch

00:36:51,470 --> 00:36:55,670
and then starts running with tests so it

00:36:53,750 --> 00:36:58,309
builds a new environment a new system

00:36:55,670 --> 00:37:00,500
make sure that everything works from you

00:36:58,309 --> 00:37:03,260
know the Installer to actually running

00:37:00,500 --> 00:37:06,559
the full test suite and it completes and

00:37:03,260 --> 00:37:06,950
runs there is in rail line net BSD to

00:37:06,559 --> 00:37:10,190
torg

00:37:06,950 --> 00:37:13,250
you can go and follow the link and it

00:37:10,190 --> 00:37:16,160
has several architectures that run daily

00:37:13,250 --> 00:37:17,720
and others that run on emulations daily

00:37:16,160 --> 00:37:19,849
and others that run on real hardware

00:37:17,720 --> 00:37:30,980
you know weekly or whenever people get

00:37:19,849 --> 00:37:33,740
random 86 runs eight times a day spark

00:37:30,980 --> 00:37:38,830
we aim to run twice a day

00:37:33,740 --> 00:37:42,260
and others about it the same frequency

00:37:38,830 --> 00:37:45,860
right I hope that answers your question

00:37:42,260 --> 00:37:48,260
now the question real question is so

00:37:45,860 --> 00:37:53,510
first of all I think that we don't

00:37:48,260 --> 00:37:55,700
really unpleasant that doesn't happen

00:37:53,510 --> 00:37:58,369
too often so optimizing the whole thing

00:37:55,700 --> 00:38:00,619
to an RB 3 seems to me like kind of an

00:37:58,369 --> 00:38:02,660
overkill right because you know you

00:38:00,619 --> 00:38:03,970
could just say stop the world and

00:38:02,660 --> 00:38:07,160
rebuild your array

00:38:03,970 --> 00:38:09,790
isn't that a feasible thing or what what

00:38:07,160 --> 00:38:11,720
what other thing that's arbitrary by you

00:38:09,790 --> 00:38:13,490
that's the first part of the question

00:38:11,720 --> 00:38:15,350
and the second part is that the unplug

00:38:13,490 --> 00:38:17,990
is the most interesting part of it and

00:38:15,350 --> 00:38:20,090
you know the the reverse lookup is what

00:38:17,990 --> 00:38:22,760
you want to have really is to basically

00:38:20,090 --> 00:38:24,500
have the subsystem ask ok who's using

00:38:22,760 --> 00:38:26,390
those segments that I'm trying to unplug

00:38:24,500 --> 00:38:28,430
right now and if it's a device driver

00:38:26,390 --> 00:38:30,830
device driver it needs to relinquish

00:38:28,430 --> 00:38:32,240
them and if it's a process we have to

00:38:30,830 --> 00:38:34,070
wait for the process to finish so we

00:38:32,240 --> 00:38:36,980
decide to kill it or something right and

00:38:34,070 --> 00:38:42,740
that's have you given any thought of

00:38:36,980 --> 00:38:46,190
that yeah this is the trick question ok

00:38:42,740 --> 00:38:50,359
so I I have fleetingly thought about it

00:38:46,190 --> 00:38:56,510
and and we when we did the work we

00:38:50,359 --> 00:38:58,460
really had to have deliverables we

00:38:56,510 --> 00:39:00,640
wanted to you know put it at a point

00:38:58,460 --> 00:39:03,230
where we actually had some functionality

00:39:00,640 --> 00:39:06,619
and so initially I did actually go

00:39:03,230 --> 00:39:09,560
around and write a very rudimentary Ram

00:39:06,619 --> 00:39:12,440
control tool just to think about how a

00:39:09,560 --> 00:39:15,710
user would interface with you know the

00:39:12,440 --> 00:39:17,510
kernel but it turned out that that is a

00:39:15,710 --> 00:39:23,030
house of cards that's going to fall very

00:39:17,510 --> 00:39:25,700
hard so my thought about this is that if

00:39:23,030 --> 00:39:26,690
we can incrementally star so the RB tree

00:39:25,700 --> 00:39:29,500
question I'll come back to in a minute

00:39:26,690 --> 00:39:35,330
but my thought is that if we could have

00:39:29,500 --> 00:39:38,210
if we if we could have users so for

00:39:35,330 --> 00:39:40,850
example Zen or any virtualized

00:39:38,210 --> 00:39:43,250
environment is the immediate one that I

00:39:40,850 --> 00:39:46,609
can think of and then you know there's

00:39:43,250 --> 00:39:49,099
heavy metal architectures out there that

00:39:46,609 --> 00:39:50,660
have a CPI based hot-plug and I'm not

00:39:49,099 --> 00:39:53,170
sure of the other architecture spark or

00:39:50,660 --> 00:39:56,170
and so on but I'm sure that the server

00:39:53,170 --> 00:39:59,239
industry has these hardware mediated Ram

00:39:56,170 --> 00:40:02,749
plug and plug things so if we have if we

00:39:59,239 --> 00:40:04,670
could have just the basic sort of plug

00:40:02,749 --> 00:40:08,599
on plug operation for specific cases

00:40:04,670 --> 00:40:13,210
that are maybe that's memory that is you

00:40:08,599 --> 00:40:15,859
know not diem aid or memory that is only

00:40:13,210 --> 00:40:17,390
allocated for a restricted set of things

00:40:15,859 --> 00:40:20,509
that can be tracked so for example

00:40:17,390 --> 00:40:23,779
things like maybe the mmm some kind of

00:40:20,509 --> 00:40:25,069
cache maybe I'm not sure which ones but

00:40:23,779 --> 00:40:27,440
we'll have to you know do the analysis

00:40:25,069 --> 00:40:29,150
but a restricted set of these pages

00:40:27,440 --> 00:40:31,160
could then be looked at because the

00:40:29,150 --> 00:40:34,339
state tracking again it's you know it's

00:40:31,160 --> 00:40:36,140
that's a huge effort to figure out the

00:40:34,339 --> 00:40:38,119
reverse lookup of what page is used by

00:40:36,140 --> 00:40:39,739
what subsystem and then to keep track of

00:40:38,119 --> 00:40:41,599
all of that and then to make sure that

00:40:39,739 --> 00:40:43,249
this particular page is not being

00:40:41,599 --> 00:40:45,410
accessed by anybody that that is a

00:40:43,249 --> 00:40:58,309
significant search problem in terms of

00:40:45,410 --> 00:41:02,630
space so yeah Taylor is it often the

00:40:58,309 --> 00:41:06,349
case that you will want to use memory

00:41:02,630 --> 00:41:07,579
that you have for say DMA or kind of you

00:41:06,349 --> 00:41:12,109
know driver specific specialized

00:41:07,579 --> 00:41:14,599
purposes like that or could you prefer

00:41:12,109 --> 00:41:17,720
to allocate DMA memory from what isn't

00:41:14,599 --> 00:41:20,539
provided initially at boot and prefer to

00:41:17,720 --> 00:41:22,910
provide hot plug memory to say user

00:41:20,539 --> 00:41:25,130
processes where you couldn't just you

00:41:22,910 --> 00:41:26,660
know you you could unmapped them and

00:41:25,130 --> 00:41:29,509
move the page data to another page and

00:41:26,660 --> 00:41:32,749
then remap them and let the user use a

00:41:29,509 --> 00:41:34,670
space continue yeah that's that's a

00:41:32,749 --> 00:41:37,220
pretty that's a leading question

00:41:34,670 --> 00:41:39,410
actually so I think bus DMA already has

00:41:37,220 --> 00:41:41,809
and this is a machine dependent question

00:41:39,410 --> 00:41:43,039
because perhaps one can imagine that you

00:41:41,809 --> 00:41:46,190
know there are architectures where hot

00:41:43,039 --> 00:41:47,329
plug memory can be used equivalent to

00:41:46,190 --> 00:41:49,220
boot time memory and then there are

00:41:47,329 --> 00:41:50,960
other architectures like x86 where the

00:41:49,220 --> 00:41:53,150
first 16 Meg's is you know only

00:41:50,960 --> 00:41:54,799
accessible to certain types of DMA

00:41:53,150 --> 00:41:58,609
controllers and so on so I think that is

00:41:54,799 --> 00:42:00,390
a machine dependent question sorry I

00:41:58,609 --> 00:42:03,779
missed the main part of your questions

00:42:00,390 --> 00:42:06,750
question is could you use this as a way

00:42:03,779 --> 00:42:10,470
to avoid having to do the search the

00:42:06,750 --> 00:42:12,359
reverse lookup problem yes yes make the

00:42:10,470 --> 00:42:14,819
problem simpler by pretending harder

00:42:12,359 --> 00:42:15,900
cases don't exist yeah more yeah so

00:42:14,819 --> 00:42:17,849
that's kind of what I was leading to

00:42:15,900 --> 00:42:20,130
with when I was saying that we should

00:42:17,849 --> 00:42:23,069
probably try and restrict where unplug

00:42:20,130 --> 00:42:27,119
is done inside of certain categories of

00:42:23,069 --> 00:42:30,960
memory or certain memory management API

00:42:27,119 --> 00:42:32,910
is like bus DMA or I can't think of

00:42:30,960 --> 00:42:36,180
something else you know at the top of my

00:42:32,910 --> 00:42:40,319
head but yeah so restricted categories

00:42:36,180 --> 00:42:42,960
of RAM could be you know that's a start

00:42:40,319 --> 00:42:44,609
I feel like and then you know different

00:42:42,960 --> 00:42:46,200
subsystems can then decide whether they

00:42:44,609 --> 00:42:48,900
want to use this or not and then

00:42:46,200 --> 00:42:51,089
eventually we could kind of have a sort

00:42:48,900 --> 00:42:53,309
of global it's a complex problem I don't

00:42:51,089 --> 00:42:54,960
have all the answers right now but it

00:42:53,309 --> 00:42:56,640
needs definitely needs analysis but I

00:42:54,960 --> 00:42:58,140
would say I would like to actually when

00:42:56,640 --> 00:42:59,670
I get spare time this was a funded

00:42:58,140 --> 00:43:01,230
project thanks to the foundation but I

00:42:59,670 --> 00:43:02,849
would actually like to look at various

00:43:01,230 --> 00:43:05,880
use cases and start doing this you know

00:43:02,849 --> 00:43:08,849
bits of categories of RAM and then see

00:43:05,880 --> 00:43:10,140
if this API can be exercised got two

00:43:08,849 --> 00:43:13,289
minutes to answer that Chris tosses the

00:43:10,140 --> 00:43:16,740
original question which which was why

00:43:13,289 --> 00:43:19,109
are b-tree I think the simple answer to

00:43:16,740 --> 00:43:25,829
that was it was available it was easy to

00:43:19,109 --> 00:43:27,420
use and yes we yeah I mean we could use

00:43:25,829 --> 00:43:29,490
something that was already available so

00:43:27,420 --> 00:43:33,089
in in theory we could go back to the

00:43:29,490 --> 00:43:34,890
array implementation and make it do what

00:43:33,089 --> 00:43:36,539
the RB tree does now because we have the

00:43:34,890 --> 00:43:38,160
testing infrastructure in place to make

00:43:36,539 --> 00:43:39,480
sure that it doesn't fall apart so

00:43:38,160 --> 00:43:41,400
actually there's nothing stopping us

00:43:39,480 --> 00:43:43,259
from going back and reworking the array

00:43:41,400 --> 00:43:45,990
implementation to do exactly what the RB

00:43:43,259 --> 00:43:47,910
tree stuff does so yeah so it's possible

00:43:45,990 --> 00:43:49,920
it's just that that was convenient and

00:43:47,910 --> 00:43:51,930
we thought that it it was it was not a

00:43:49,920 --> 00:43:53,009
very organized decision decision but we

00:43:51,930 --> 00:43:54,240
kind of asked around about the

00:43:53,009 --> 00:43:56,490
performance implications and we thought

00:43:54,240 --> 00:43:58,470
well we'll just find that number five

00:43:56,490 --> 00:44:00,089
points X percent and throw it out and

00:43:58,470 --> 00:44:01,529
see if people complain too much and if

00:44:00,089 --> 00:44:03,900
they don't then we're just gonna you

00:44:01,529 --> 00:44:05,430
know push it in but we're not forcing

00:44:03,900 --> 00:44:07,670
you to use it you can you know you can

00:44:05,430 --> 00:44:10,380
optionally use the array implementation

00:44:07,670 --> 00:44:12,509
without ha plug and then the arbitrary

00:44:10,380 --> 00:44:13,920
implementation with hot-plug and if

00:44:12,509 --> 00:44:16,109
someone's up for

00:44:13,920 --> 00:44:18,420
you know doing a hot-plug implementation

00:44:16,109 --> 00:44:18,869
of you know using the array you're more

00:44:18,420 --> 00:44:22,190
than welcome

00:44:18,869 --> 00:44:28,900
I'm happy to do you know help with that

00:44:22,190 --> 00:44:33,050
so thank you very much

00:44:28,900 --> 00:44:33,050

YouTube URL: https://www.youtube.com/watch?v=K0Sry5laFvs


