Title: A Fault Aware Global Server Load Balancer in DNS - Stefan D. Caunter, Allan C. Jude, EuroBSDcon 2012
Publication date: 2012-12-07
Playlist: EuroBSDcon 2012, Poland
Description: 
	EuroBSDcon 2012
Warsaw, Poland 18-21 October
Captions: 
	                              ok I think everyone is here it's going                               to be here so                               get started it's a great pleasure to be                               here in Warsaw I'm lovely time last                               night I I've had lots fun at weddings                               we're here to talk about our fault aware                               global server load balancer that we do                               in DNS that we use for our CDN just a                               few words about who our target audiences                                for this talk if you have an                                organization that has more than one                                point of presence it's for you if you or                                your application would benefit from a                                geographic segmentation of traffic if                                you could use the ability to deploy                                simple failover active passive or                                multi-node load balancing of your                                traffic do you want your server                                monitoring to automatically update your                                DNS you probably do gdn SD is the                                subject of this talk we put in a port as                                our contribution and basically wrote                                this talk proposal on the train back                                from bsd can last May the deport up in                                the summer gdns d was originally built                                by a programmer working at logitech and                                he got them to open source it which was                                nice so they used it to manage directing                                users to nearby driver download mirrors                                and works nicely okay a little bit about                                us where the scale engine guys where a                                CDN                                as Alan Alan Jude I've been a freebsd                                admin for quite a while I built all the                                architecture stuff at scale engine                                including our CDN and the video                                streaming network before that I was a                                professor at Mohawk College teaching                                computer science and IT and then I hosts                                a podcast called tech snap which is a                                the systems network in admin podcast so                                if you're into that it's might be                                interesting and i've been a bsd server                                admin for a long time i did the varnish                                implementation for the Toronto Star                                newspaper in those exciting times in                                                                                                     Canada and I guess this is the story of                                how we came to use this gsl be so just                                an overview of what we're going to talk                                about we'll give you an introduction on                                what we do talk about some of the                                challenges we've had with with growth as                                people have started using us to carry                                their their their traffic will discuss                                what is a global server load balancer                                examine some of the currently available                                solutions both proprietary and                                non-proprietary and of course we prefer                                the open source solution that's why                                we're here and then we'll get into the                                actual gdns d implementation in l and                                we'll walk you through an example so                                combining the port with this talk and                                the slides should get you set up pretty                                nicely we're going to look at response                                policy in other words what do you do                                when when a client does it look up okay                                so what's the dns server going to say                                advanced response policies with geoip so                                making a geo geographically aware use                                cases and examples agents monitoring                                adding capacity in other words on demand                                capacity based on what's happening in                                DNS the e DNS client sudden that                                implementation and challenges they're in                                and workarounds for it what you have to                                do if you're not on the whitelist okay                                what is a scale engine it's a global CDN                                well it is now we do a lot of video                                streaming we do a lot of HTTP objects                                and we do some application hosting we                                are entirely powered by freebsd and have                                been for many years this so we do a                                number of things we have edge side                                caching and that's a varnish                                implementation a CDN for global caching                                of anything over HTTP anything else vsn                                is a video streaming so it's live                                streaming for events and on-demand                                streaming to desktop using rtmp and to                                mobile devices lots of iPads and iPhones                                and androids and so on mostly with HLS                                using the cupertino chunking we also                                have a an Origin web cluster owc which                                is a PHP MySQL mostly and of course we                                have the gsl be that powers it you want                                to talk with ya at the moment we have                                about                                                             different data centers in nine different                                 countries and so balancing load between                                 those and dealing with failures that                                 happen started become much more                                 administrative headache and we looked at                                 ways to automate it and in aggregate we                                 can push about                                                           to the internet between those different                                 hosts all of which run freebsd nine and                                 are managed with the puppet Thank You                                 Edward tan for introducing us to that                                 and we also make extensive uses of jails                                 with easy jail to make it easier to                                 deploy the same containerized config for                                 some of our applications on those                                 different hosts and to move things                                 between hosts when required well we'll                                 do that let's do a little bit of stats                                 the main the biggest one is that in the                                 last in September we uploaded about                                     terabytes to the Internet at peak time                                 that was about                                                        and i think our varnish serves about                                   or this month it'll be about                                           requests but at peak time that's as many                                 as                                                                   spread across the different geo zones as                                 everyone knows the varnish can do that                                 with its eyes closed on one box but the                                 challenge is actually locating it around                                 the world so yeah we push a fair amount                                 of bits in in terms of video we have                                 very spiky event-driven traffic's where                                 we see in a space of five minutes things                                 go from                                                             video is a low request rate situation                                 extremely high bandwidth sometimes                                    and a half megabit to each client and                                 the sessions are long-lived so that's                                 where we see our peak network load in                                 comparison hgp is smooth it tends to                                 follow when people are awake                                 our largest CDN HTTP object client is                                 runs on lots and lots of news sites and                                 so you know people read these people                                 read the news when they're at work so                                 the graph goes up when they work in it                                 that's how it goes there's a graph of it                                 coming up yeah we'll have a graph of                                 that so yeah we started a CDN kind of                                 vitis out of necessity and this is a few                                 years ago and we were just doing hosting                                 and one of our hosting customers                                 quadruple in size so yay but we were on                                 a fixed commit that an expensive North                                 American hosting provider and we had a                                 very modest                                                          could burst to                                                          max it out with this suddenly large                                 customer so we had to get creative we                                 had to we have limitations that's fine                                 and you use that to get better or you                                 die so you can get servers from cheaper                                 viride providers and we did that they                                 had cheaper network in some cases better                                 network and we just created a sub-domain                                 instant CDN so your image content is                                 offloaded and we were able to push out                                 enough to avoid expensive overage                                 charges on premium transit so a little                                 bit about HTTP and subdomains your                                 modern browsers will parallel download                                 if you serve content from additional                                 domains so that's good like normally a                                 browser will only connect to the same                                 site two or three times to download                                 objects and then any objects after that                                 get queued but if you make all the CSS                                 come from one sub domain the JavaScript                                 from another domain and images from a                                 couple of other domains are even sub                                 domains that will start loading more of                                 them at once and since most people have                                 a broadband connection now and it's more                                 the latency that's limiting this loading                                 speed rather than the                                 the transfer rate we can load more of                                 the page at once and be finished sooner                                 so this will improve speed if the user                                 is reasonably close geographically to                                 the server and conversely it will not                                 improve speed it will make it worse if                                 the user is far away from the server you                                 get the ability to strip cookies which                                 can speed up user experience and it just                                 lets us cash popular content it's kind                                 of on demand and that's the beauty of                                 varnish here's your typical HTTP graph                                 so people are all awake in the middle of                                 the day and looking at news and they go                                 to sleep in the middle of the night and                                 nobody reads the news and it's                                 interesting to watch those graphs across                                 the geographically distributed servers                                 because the hump and the graph is offset                                 at different times depending where the                                 server is so the challenge is that it                                 just creating a couple of C names for a                                 sub domain is suboptimal and it quickly                                 becomes cumbersome to manage this on a                                 large scale users don't benefit from any                                 Geographic awareness with just straight                                 for straight round robin DNS we also                                 found that some resolvers were sorting                                 results alphabetically so we couldn't                                 really point people at a domain anyway                                 an IP that starts with the one always                                 got more traffic so we're benefiting by                                 offloading Network right yes but user                                 experiences is not as good as we want so                                 here's just part of growing pains right                                 so your your your business is starting                                 to get used and you need to sort them                                 sort some things out external providers                                 are cheaper for bandwidth and sold in                                 large amounts which is what we need                                 right but also they sold us train                                 transfer per terabyte of transfer rather                                 than                                                                  for our bursty type of traffic it was                                 much easier to                                 get what we wanted for a reasonable                                 price so you want to talk about and the                                 other advantage with going with a bunch                                 of separate external providers was that                                 we could get different locations instead                                 of having to have a colocation and try                                 to manage that at all the different                                 points on the globe right so now we                                 don't have we have two racks in our data                                 center near our offices but we have nine                                 other data centers around the globe                                 where we are sorry                                                       on the globe where we have servers and                                 then that means that we cover more of                                 the globe with a server near them but                                 also we're having a lot of different                                 transit providers that way so that                                 there's one that's topographically close                                 to you on the internet there's somewhere                                 there's peering near you so you also get                                 a better connection that way I'm not                                 diverse so early struggles with this                                 were to manually managed DNS suboptimal                                 used bind views which inflated the size                                 of the zone alive it was I basically                                 took a geoip database and summarize the                                 subnets into larger blocks and made                                 access lists out of those and then                                 define them as different views so I had                                 views for different regions like East                                 Coast West Coast Europe Asia and so on                                 and each of those would have to have a                                 separate zone filed and bind with                                 different records so when you went to                                 this website if you fell into this zone                                 then you were sent to this IP and feel                                 into the Sony went to a different IP but                                 you know when one of those service goes                                 down it was we had to edit the zone                                 files and market and it was cumbersome                                 to manage at all time and it often took                                 too long um we thought about doing any                                 cast it but our core expertise is with                                 server applications and we also had                                 providers that were frankly reluctant to                                 do that with us the basic technique then                                 initially is to add IPS more than once                                 in two                                 a record in DNS and that just compounds                                 problems because you run into the DNS                                 size response limit so ed and s                                      original RFC stated that if response is                                 larger than                                                              TCP not UDP in                                                  introduced and allowed for DNS responses                                 up to                                                          fragmentation and the problem with that                                 is I basically a server will only return                                 to respond a larger response over UDP if                                 the client requests it so that clients                                 that don't support it wouldn't be sent a                                 response they wouldn't understand the                                 problem is some older firewalls will                                 have rules that say if the DNS response                                 aboard and                                                            client behind that doesn't know that                                 that's there and so request the response                                 to be sent with ed and s                                                 a larger response but then some                                 intermediary firewall would block the                                 response on the way back cisco ASA's yes                                 and so they wouldn't get the response                                 that they asked for ah whereas if they                                 hadn't set the flag we would have sent                                 the response in a way that would have                                 got through the firewall but because the                                 client was requesting a feature that                                 technically was being blocked somewhere                                 in front of the client it resulted in                                 when our dns responses got too big when                                 we were at like                                                     america it meant that all of a sudden                                 some clients mostly at corporations and                                 since news sites and mostly browse by                                 people at work all of a sudden they                                 couldn't resolve the domain and we                                 couldn't figure out why at first and                                 then we found out about these evil                                 firewalls telling them to install                                 pfsense didn't work                                 so we start getting a lot of requests to                                 do video streaming and we start playing                                 around with it we get a lot of these                                 requests from Europe entirely different                                 scaling problems now with video it                                 becomes link capacity that is the issue                                                                                                     anymore yeah so you have to have a                                 gigabit up links you need providers with                                 lots of bandwidth and hey that's Europe                                 surprise bandwidth is even cheaper in                                 Europe some providers have better                                 transit than others to North America we                                 learned you're scaling is completely                                 unpredictable with video compared to                                 http so you don't have those nice smooth                                 graphs there's no trend it's just all of                                 a sudden there's something and all                                 sudden it's gone yeah the other the                                 technical issue is that you cannot have                                 contention for the wire in video if you                                 do that it breaks the experience for                                 everyone right like with HTTP if you                                 have demand for bandwidth greater than                                 your hundred mega bits or whatever then                                 everybody will just get this filed or                                 downloading just slower but with video                                 if you're using most of your Gigabit and                                 everybody's getting the video at one and                                 a half megabits or whatever and they're                                 happy and then you add a couple more                                 users and you all sudden push over the                                 limit of how much you can push through                                 that line now all                                                      trying to watch I have their live stream                                 stuttering and breaks yeah so TCP tries                                 to be helpful and just breaks there                                 before even UDP would have the same                                 problem that so as soon as you hit that                                 limit you're not just slowing everybody                                 down or something are you not just                                 having a problem for the new people                                 connecting you basically ruin the                                 experience for everyone connected to                                 that server as soon as you try to exceed                                 the link capacity okay so the example of                                 news sports and so on one day there was                                 an airport emergency in Iceland so the                                 whole country went and visited the state                                 broadcaster would carry the video for                                 them and we carry all foreign video for                                 ruv debt is and an airplane had two                                 broken landing gear or something and                                 good headline circle the airport for                                 four hours in burn off                                 fuel and apparently there was like                                                                                                          live stream of this happening and it was                                 just I'm president scaling challenge for                                 us um so we needed to factor geography                                 into the load balancing it was very                                 important though to not send viewers to                                 overloaded servers okay and this is why                                 we we start doing it out of DNS like the                                 the first load balancing solution we                                 tried was one that was based on the                                 number of viewers on each server so just                                 knew how many people were watching                                 videos on each different server and                                 would just send the next viewer to the                                 least loaded server but not everybody is                                 watching the same video some videos are                                 lower bitrate some of them are only                                 audio so that meant that just because                                 that server had fewer viewers didn't                                 mean to have more available bandwidth                                 and so we had to work out something more                                 intelligent than that yep so we have to                                 measure network and application health                                 before we send someone to a server and                                 this this lets us do that here's your                                 here's your spiky spiky video graph and                                 use your very very spiky videograph so                                 yeah people just it very sharp it just                                 goes from nothing to                                                   out of nowhere okay so what's a global                                 server load balancer it handles the                                 direction of traffic to different nodes                                 with a focus on geolocation and                                 depending on the vendor there's a bunch                                 of talk and waving in cliches about high                                 availability an optimal response time in                                 our case our global server load balancer                                 routes traffic to edge servers near the                                 request or to provide lower latency and                                 to spread load between a number of data                                 centers that's important for us as well                                 as automatically diverting traffic from                                 down servers whether it's down meaning                                 your you're maxed out you're at your                                     we're not sending anybody more to you                                 the hard drive data yeah yeah so we                                 first looked at what was out there                                 there's a bunch of different commercial                                 solutions like your f                                                  so on                                 the Barracuda though required that we                                 have a barracuda device at each                                 different location and when you make a                                 configuration change there's no way to                                 have that replicated you have to                                 manually load the new configuration on                                 each of the devices and that wasn't very                                 helpful and it meant that it really                                 couldn't be automated and also the                                 response policies we come up with we're                                 quite limited it was basically we can                                 decide on by region or geoip but that                                 was about it and yeah there was no auto                                 updating and also the monitoring was                                 fairly limited a lot of times it was                                 just is this port open rather than can                                 you actually check this the health of                                 the application it's actually completely                                 limited in terms of what you can do with                                 this with GT and SD gdns t just blows                                 away that particular proprietary                                 solution and it's expensive okay okay so                                 we looked at what was available open                                 source and originally we tried bind with                                 views basically summarized the subnets                                 in a smaller set because otherwise it                                 would have eaten all of the RAM on the                                 machine as it was it was pretty heavy to                                 have all these huge acl's with different                                 cider subnets and it also made it take                                 about                                                               config and it required a separate buying                                 config for each different zone and as we                                 kept adding more zones it just became                                 even more cumbersome to manage and the                                 worst part was that it broke master                                 slave replication because when you and a                                 xfr the slave would get the copy of the                                 zone for whatever region it happened to                                 fall in whichever view it was in it                                 wouldn't get all of the views so then we                                 had to come up with our own way to to                                 push the zones out and update serial                                 numbers and so on we looked at power DNS                                 which is basically a something like bind                                 except for the storage back end is                                 usually something like MySQL but it's                                 geoip thing doesn't read the max mine                                 files that we have they took some other                                 format for DNS block list and then we                                 look to any cast but it kind of had some                                 limited flexibility it wasn't                                 application-aware unless we ran like                                 openpgp d on each machine and had some                                 helper script or something to withdraw                                 the route if our app wasn't healthy and                                 only a third of our providers were                                 willing to establish a BGP session with                                 each of our servers so I did a little                                 chart here showing the different things                                 and the other thing was that the e DNS                                 client subnet is not supported in mind                                 when I looked I don't know it might be                                 better now power DNS didn't really                                 support it yet either and so basically                                 gdns d we found when we were looking for                                 something that supported EDS so what the                                 global server debt load balancer does is                                 we have a situation like this where our                                 us viewers are being directed to a US                                 server UK Germany and France or whatever                                 and then the load balancer is monitoring                                 each of those servers and checking                                 itself when it detects that the French                                 server all of a sudden doesn't have a                                 connection to the internet anymore or is                                 down or whatever then the French viewers                                 are actually redirected to the next                                 closest server or whatever business                                 rules we've applied through the config                                 our response policy so we send them to                                 Germany but then in a different                                 situation we have our cluster of servers                                 in the UK and they're reaching their                                 load limit they they can't handle any                                 more viewers without degrading the                                 performance for everyone rather than                                 just sending the viewers to the next                                 closest server which if we're having a                                 lot of load in one area it's likely                                 covering the whole region instead of                                 just sending it to the next closest                                 server basically knocking that over and                                 then the next the second close deserver                                 knocking that over and continuing that                                 because you know its dns so there is                                 that little cash in delay we and our                                 load spikes really really quickly as                                 soon as we detect a trend that we're                                 going to overload the UK we fall back                                 and redirect traffic to the entire                                 region and spread the traffic around all                                 the servers in the region so that we can                                 handle a bigger spike yeah so in the                                 example config which we will get to                                 you'll see how we define different zones                                 and how to fail and your stages of                                 failing                                 up instead of over so gdns d basically                                 came to the rescue it's a thora tative                                 only dns server doesn't have all the                                 features but it has most of what you                                 need importantly it supports ed NS                                 clients subnet it's a draft                                 specification engineer at Google came up                                 with and the basic idea is when you do a                                 DNS lookup the server doesn't see the                                 clients IP address they only see the                                 requests coming from the recursive DNS                                 server and when that was that most                                 people's ISP that worked pretty well                                 because the ISPs DNS aerobie generally                                 located near the customers but as people                                 started moving to using things like Open                                 DNS and Google's public dns I the                                 problem was that geolocation points all                                 of google's IPS to the Googleplex in                                 California even if that server happens                                 to be in the Netherlands and so people                                 would notice things like iTunes user CDN                                 called Akamai and if you weren't in                                 California you would get lower speeds                                 especially in Europe because if you're                                 using google dns Akamai solder who has                                 come from google so they thought you                                 were in California so they send all the                                 traffic to the California node that came                                 through google and so II DNS client                                 subnet is an extension that as part of                                 the DNS request the recursive server                                 passes along the /                                                     not the whole IP address for privacy                                 reasons but it tells you what subnet the                                 client is in so you can geolocate based                                 on that rather than the request the                                 recursive DNS servers I key because                                 normally you wouldn't know what the                                 client IP is in DNS ah it reads the max                                 mind geoip database directly through the                                 capi so you can use like geo light                                 cities or whatever or you can buy the                                 more expensive one that's more accurate                                 it has monitoring plugins that can                                 actually monitor your servers and                                 importantly is also include flapping                                 detection so we'll get into what the                                 rules are like but it allows you to make                                 sure that a server isn't going up and                                 down constantly because if it is there's                                 probably something wrong and you want to                                 back off from that server not flip on it                                 and it has a number of different                                 response plugins plus an API for you to                                 write your own so simple failover you                                 basically just have a primary and a                                 secondary defined for any response                                 record and it returns the primary unless                                 it fails in which case it returns the                                 secondary with multi failover you                                 basically give it a pool of IP addresses                                 and it spreads the load between those                                 and removes any of those IPS if they're                                 monitoring before so they're down with                                 weighted you assign each of the servers                                 in a pool of weight and it allows you to                                 have unfair load balancing you know if                                 one of your servers has more capacity                                 than the other than you want it to get                                 more of the load and then the geographic                                 multi failover the same thing except for                                 you use the geoip database as well it                                 reads basically standard bind zones so                                 you don't have to rewrite your zone you                                 just copy it in but you get a couple of                                 new record types importantly it also led                                 us limit the number of addresses that                                 returned in response so it's just a                                 little macro you put in your zone file                                 and say for this set of Records never                                 return more than four ip's in the                                 response and that let us make sure our                                 records are responses were under that                                                                                                        so we made one so the two new record                                 types you get our Dinah and I'm see                                 which are basically dynamic a records in                                 NC names so importantly being able to do                                 dynamic cnames is actually pretty cool                                 and it also supports the include macro                                 and the record size limits so when                                 you're defining response policy there's                                 a bunch of things you can consider ah if                                 the failover and the load balancing I by                                 default you would just have basically                                 what's called binary node health just                                 checking if it's up or down like it has                                 a HTTP status monitor that checks for                                                                                                          we kind of developed more advanced note                                 health checking that actually checks                                 other factors to decide if the server                                 should be a candidate for being returned                                 in a response and then it factors in the                                 location ah what nodes to use at what                                 time and                                 how many nodes to return in each message                                 right do we want to return just one or                                 an hour of two or eight or whatever so                                 the basic config probably little too                                 small to read but you basically define a                                 service or a resource like public www                                 and then you define your monitoring                                 types and basically a list of IP                                 addresses it will check the service on                                 each one of those you can even define                                 multiple services like here and only if                                 they're all up is the IP a candidate and                                 then it'll return those three IP                                 addresses and it can also do ipv                                      can specify separate monitoring for                                 example if your DNS server doesn't                                 support ipv                                                         doesn't have a v                                                      your services have v                                                 monitor the v                                                         suit of service that's always up or                                 whatever another interesting thing here                                 it has an up threshold basically is a                                 failover in case something's wrong with                                 the monitoring or and it determines that                                 all of your servers are down rather than                                 not returning anything you basically set                                 a threshold and you say in this case if                                 less than seventy percent of our servers                                 are up then it falls back to just                                 returning all the servers because it                                 soon something's wrong rather than                                 that's assume something's wrong with the                                 monitoring rather than something's wrong                                 with all of your servers at once there's                                 a number of things you run into when                                 you're using geoip the first one is that                                 the data is not always accurate for                                 example the IP address of our colocation                                 shows up in New York rather than Toronto                                 so sometimes the country isn't even                                 right let alone the city level or state                                 level and you know the same thing was                                 obviously happening with Google's ip's                                 right they use any cast for the address                                 that you interface with                                                 but the actual addresses the request                                 comes from a regular unicast IPS and                                 they're spread all over the world but                                 all of them geoip back to google flags                                 and yes then as                                 said the source IP that your dns server                                 sees is that of the recursive resolver                                 not the client so your geo-locating the                                 dns server they're using not the user so                                 there's going to be an error in there of                                 how far away the user is from their dns                                 server especially if they're using                                 something like a corporate VPN or                                 something and it's good yeah ah the                                 other thing is you can in addition to                                 loading the geo IP database gdns d lets                                 you define a list of overrides so you                                 can say you know for example our                                 specific subnet for our own internal                                 ip's goes to a specific server all the                                 time even if the geolocation would send                                 it somewhere else so when you're using                                 geoip there's two different methods the                                 first is the automatic you basically                                 define a list of your data centers and                                 then for each one you define the                                 latitude and longitude of location and                                 it will do locate the user and find the                                 closest data center and return that and                                 then you see you can do overrides we can                                 say you know our subnet always goes to                                 Toronto and you see here we have a comma                                 separated list of data centers it'll try                                 the first one and then only if all the                                 servers in that are down it'll then fall                                 over to the second one and so on and you                                 can have a unlimited length of that list                                 in a more advanced configuration we                                 define a list of our data centers and                                 then we do a map from the Geo ip's                                 databases actual geolocation                                 yeah that's in the automatic one and                                 that's why we have this more advanced                                 one where you can be more specific right                                 you can have specific subnets be                                 override to a certain location or you                                 can have other stuff but mostly we were                                 just worried about not serving users                                 from Europe in north america in North                                 America and Europe because why go                                 transatlantic when you don't have to do                                 that cuts                                                                a big difference so in this one you can                                 you can even drill down to the city                                 level if you're gia VoIP data is good                                 enough ah and add addition you can also                                 do subnets if you have specific places                                 that are having a problem so in this                                 case we say for Africa use the European                                 data centers for Asia is Asian data                                 center but we don't have very many of                                 those so fall over to the backup pool                                 for year up we have a default of a fool                                 of all our servers in Europe but we have                                 specific data centers in Germany France                                 the Netherlands and Great Britain and                                 you see each of those in Netherlands we                                 preferred it separate from the                                 Netherlands but if those servers are                                 down or unreachable then use this the                                 regular pool so that's the map you                                 define and then you do your resources so                                 you know you say which map to use you                                 set up the services that you want to                                 monitor and then we map that those data                                 centers recreated like North America and                                 we say you know server in Seattle it's                                 IP Los Angeles Phoenix Dallas and so on                                 and in Europe the other advantage we get                                 here is unlike combined where we had the                                 problem you can't have in a round-robin                                 you can't have the same IP twice because                                 it just creates it into a hash here we                                 can have the same IP twice to double up                                 the load to do accomplish that in x and                                 we basically had to have two different                                 IPS on the machine to be able to double                                 up and we have a naming convention here                                 that factors in a little bit later and                                 then for your monitoring you just we're                                 just using the HTTP status module but                                 you can there are a few others and you                                 can write your own                                 we set the V host in the path and so on                                 but importantly we have the thresholds                                 here so we only consider it server up if                                 the last                                                             returned the up status so if one of                                 those checks in the middle somewhere                                 returned a Down status the servers still                                 not up ah if the server was in the                                 danger state which happens as soon as                                 there's one failure but before there's                                 enough failures to be down then it takes                                 five good checks in a row to get back to                                 the Upstate so if a server is being                                 monitored and it has one failure it                                 drops from up to danger then once it has                                 five goods again it's back to up once a                                 server has had two downs in the                                 monitoring period then it moves from the                                 dangerous date to down and then we pull                                 it out of the pool and stop returning it                                 to users so that they're not going to a                                 server that's either too busy or not                                 even up the nice thing about this is                                 it's something that is pushed out from                                 the actual location of where the                                 application is so this is all happening                                 in DNS and you're directing clients                                 really you're anticipating your                                 application health as this happens one                                 thing we haven't mentioned is that you                                 actually get a statistics output from                                 gdns d sorry we don't have an example of                                 it but that's also yours there's a                                 little web server built into G in it GD                                 and SD that lets you see the status of                                 everything and it also has a CSV upper                                 which we make use of so we've created a                                 couple of custom monitoring modules or                                 on the not as part of Gd and SD they're                                 just on our side and so for a video                                 server you hit a certain an HTTP URL and                                 it checks how much bandwidth the servers                                 you putting out right now and once                                 that's over a threshold our HTTP                                 provider returns an error                                                a                                                                      down so when its servers too busy it                                 gets marked as down and we direct this                                 traffic elsewhere and we do the same                                 thing for                                 HTTP except for recheck disgaea has when                                 after a cold rebook our varnish cast is                                 hitting the disc so hard that it starts                                 causing latency for responses so all                                 that worked until we landed the                                 situation where we don't have enough                                 servers for the amount of bandwidth we                                 need anymore so we can fail over to a                                 different region but we kind of wanted                                 to avoid that so I came up with the hate                                 algorithm high availability through ec                                  that's we hate to use Amazon because                                 they actually charge more per gigabit                                 than or per gigabyte than we do so using                                 them cost us money so really hate to do                                 that but we hate being down even more so                                 we wrote a little capacity manager                                 script that basically pulls the CSV                                 stats interface of the DNS server and                                 checks the health of all of our servers                                 and because of our naming convention we                                 can tell that look we're you know eighty                                 percent of our servers in Europe or                                 marked is down because of load so we                                 need to get some more capacity in Europe                                 right away so call the Amazon ec                                      Europe spin up some freebsd ec                                        they come up and start taking over                                 basically we have the elastic IPS which                                 are basically static ip's they can move                                 between virtual machines and they're set                                 up as the very last data center on the                                 list for each region normally go all                                 down but once we start them and boot                                 them up they start coming up and all the                                 load goes to amazon and then once our                                 servers are back up you know the events                                 / or whatever we hold the hour HTTP                                 provider on each of those servers and                                 check how many viewers are still using                                 amazon once that falls below a threshold                                 of about                                                                 again we can kill off those amazon                                 instances to save money                                 so in conclusion we found that                                 especially with HTTP distance adds                                 latency and that can hurt our object                                 delivery for HTTP flesh because HTTP has                                 a couple of round trips and I usually                                 the objects are so small it's not                                 transfer time that's killing us it's                                 just the setup of the sockets and things                                 like that we also found that video                                 performance legs if we have TCP                                 retransmits because flash and HLS                                 especially our over tcp they pause if                                 there's a Miss packet and have to wait                                 for it and so trying to avoid that as                                 much as possible makes a big difference                                 and proximity to the user we already                                 said that the proximity is less                                 important for video because once your                                 connections been alive for                                            it's usually settled down a bit but the                                 closer to the user we are the more                                 reliable the packet transmission is so                                 it means better quality video and yes we                                 found that our cache warming once we had                                 millions of objects in our varnish when                                 you're cold restart varnish it hits the                                 disc really hard so having back off the                                 traffic on that as when the disa starts                                 to get busy and then ease it back in one                                 and a couple of times until the cache is                                 hot hey and um it just to wrap it up                                 because we are almost done um this is a                                 solution that fits in the wonderful                                 world of limitations and so you you're                                 good at running freebsd servers you                                 create a solution using what you're good                                 at and so it's an application layer                                 solution it we find it means having lots                                 of network transit providers we get to                                 leverage that and and and make things                                 better by making it an advantage instead                                 of a limitation if we lose transit we                                 can drop something out and a lot of                                 times something goes down because of                                 poor transit                                 so we get to automatically recover we                                 get to distribute we have this complete                                 global view of services and we get to                                 stage the distribution we have load                                 stages so especially for live video                                 because of we're doing like an origin                                 edge type setup we have an origin server                                 and as to feed one copy of the stream to                                 each edge server ah when we only have                                 say a hundred viewers watching a stream                                 it's not advantageous to spread those                                 out over                                                               we're taking                                                         stream so certain customers are on an                                 elastic pool that uses a smaller number                                 of servers and then once the if the load                                 goes hi there then it kicks over to                                 using the rest of the network so that we                                 have some level of efficiency in under                                 lower load as well and so we used that                                 list of data centers to say used a small                                 pool and then once that's full use the                                 medium pool and then switch to the full                                 pool okay um and I think of if there is                                 a question we could take a question or                                 two and                                 you may want to have a look at the thing                                 called bgp DNS I did that                                              about and you can find it through a                                 simple google search and it uses bgp                                 information for the proximity of the                                 client instead of playing this                                 geolocation stuff who divided invite                                 night before in ellicott maybe i missed                                 a small detail but i don't understand                                 why you need to manually configure                                 google with jyp can you just look at the                                 a dns information and always use that to                                 write i did miss to say that on google                                 doesn't send the ETNs client subnet                                 information unless you're on their white                                 list of servers that support it they                                 didn't want to risk breaking the                                 internet so unless you're on google's                                 very very small white list of                                 authoritative servers then they won't                                 send the client subnet to you so you                                 don't have that information and yeah                                 yeah so you don't have that information                                 so what we do is we have from google we                                 found a post that Google put out a list                                 of all of their dns servers and the                                 actual location and so we do an override                                 in the of the geoip information for that                                 list of Google servers and for the list                                 of open dns servers did you check what                                 it takes to get onto the waitlist yeah                                 it's not that easy they're like email us                                 at a faster internet com but then they                                 they don't really get back to you and                                 what's the value of the TTLs you are                                 sending out to the clients usually RT                                 dl's about five minutes because we want                                 to have really fast and we have like                                 eight of these g DNS servers so we're                                 not really worried about the DNS load                                 we'd rather have a high DNS load then to                                 have a longer TTL where we serve up a                                 dead IP for                                                            need it to be short because you know our                                 load for a video can spike like                                    gigabits in five minutes so we need to                                 be able to reposition that traffic very                                 very quickly                                 so it's not fun when certain ISPs impose                                 a minimum TTL any other questions do you                                 want to list those ISPs I don't have a                                 list of somebody does I would like to                                 know so I could shoot them does the                                 stats web page does it'll show show just                                  statistics of the queries I'll ask rage                                  or only of the punching some stats can                                  you pull that up really quick gdns I                                  don't remember what port it's on it's on                                  weird port I we're going to pull up the                                  stats page really quick so you can see                                  but any other point I think someone over                                  here at a question than they yeah ah so                                  here's the stats page you can see we see                                  how many responses we had with no errors                                  how many we refused because they were                                  silly people looking up stuff that                                  doesn't exist drop packets and see that                                  most of our queries it's at                                               out of                                                                  packet up to                                                              don't know how many of those behind a                                  firewall that's the actually going to                                  block it if we tried to use it I can we                                  see the uws how much you see only                                      TCP requests out of                                                 million requests since the server                                  rebooted                                                                here's our monitoring you see certain                                  servers are up certain servers are down                                  ah and there's also an orange danger                                  state but mostly were upright and so a                                  danger state is a server that has had a                                  failure but not enough to be actually                                  considered down because sometimes you                                  have just a small hiccup or whatever we                                  use a very short time out so sometimes                                  it's just there                                  okay thank you very much                                  you
YouTube URL: https://www.youtube.com/watch?v=WF75IGx9svM


