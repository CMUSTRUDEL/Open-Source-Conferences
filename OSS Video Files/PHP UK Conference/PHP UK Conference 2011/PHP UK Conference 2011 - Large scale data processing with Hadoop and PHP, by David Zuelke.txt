Title: PHP UK Conference 2011 - Large scale data processing with Hadoop and PHP, by David Zuelke
Publication date: 2012-03-15
Playlist: PHP UK Conference 2011
Description: 
	
Captions: 
	00:00:03,910 --> 00:00:10,190
hi thanks for the introduction I've

00:00:08,299 --> 00:00:12,620
called it how Dube now so at the end of

00:00:10,190 --> 00:00:14,030
the talk will have a vote and you guys

00:00:12,620 --> 00:00:16,309
have to tell me what it should be with

00:00:14,030 --> 00:00:18,200
Hadoop and PHP or with MapReduce and PHP

00:00:16,309 --> 00:00:20,779
I'll explain what how do this and what

00:00:18,200 --> 00:00:22,640
MapReduce is in a second my name is

00:00:20,779 --> 00:00:25,399
already said is David silk and the weird

00:00:22,640 --> 00:00:28,099
dots over the name giveaway that I am

00:00:25,399 --> 00:00:30,020
indeed German but that shouldn't

00:00:28,099 --> 00:00:35,660
frighten any of you because we're all

00:00:30,020 --> 00:00:39,560
friends now all differences forgotten

00:00:35,660 --> 00:00:41,840
right so I'm from a beautiful city of

00:00:39,560 --> 00:00:43,190
Munich as was always said which is most

00:00:41,840 --> 00:00:44,600
famous for the Oktoberfest and a good

00:00:43,190 --> 00:00:45,920
beer that comes from their work there is

00:00:44,600 --> 00:00:50,240
a founder of a company called beta

00:00:45,920 --> 00:00:52,910
extender we do scalability System

00:00:50,240 --> 00:00:54,320
Architect the web stuff I do among other

00:00:52,910 --> 00:00:56,720
things a lot of rest I give a lot of

00:00:54,320 --> 00:00:58,310
rest towards friends I'm also delete

00:00:56,720 --> 00:01:00,230
developer of the garvey framework which

00:00:58,310 --> 00:01:01,790
is awesome this is my Twitter nickname

00:01:00,230 --> 00:01:03,320
if you want to bash me or praise me

00:01:01,790 --> 00:01:04,820
while I'm giving this talk if the

00:01:03,320 --> 00:01:08,450
internet works which I think it doesn't

00:01:04,820 --> 00:01:10,520
because this is not Germany no just

00:01:08,450 --> 00:01:12,620
kidding it also breaks down in Germany

00:01:10,520 --> 00:01:13,970
curiously here's the fun thing the only

00:01:12,620 --> 00:01:15,560
conference I've ever been to that was

00:01:13,970 --> 00:01:17,390
reasonably large with an internet that

00:01:15,560 --> 00:01:24,770
didn't break down was Belgium of all

00:01:17,390 --> 00:01:26,360
places so I have a lot of slides so I'll

00:01:24,770 --> 00:01:28,430
go through them and explain stuff and

00:01:26,360 --> 00:01:30,830
then there's a hands-on and the handsome

00:01:28,430 --> 00:01:34,280
will be a complete disaster it'll be my

00:01:30,830 --> 00:01:36,619
water right because i've i've written

00:01:34,280 --> 00:01:39,590
you kept mate I've this cardinal sin of

00:01:36,619 --> 00:01:41,119
its new code I haven't tested it five

00:01:39,590 --> 00:01:43,190
minutes ago is literally just hacking

00:01:41,119 --> 00:01:48,439
away on it and I don't think it'll work

00:01:43,190 --> 00:01:50,650
but we'll see so at overview of like

00:01:48,439 --> 00:01:54,350
what distributed and parallel computing

00:01:50,650 --> 00:01:56,780
requires is is this part of the

00:01:54,350 --> 00:02:00,290
presentation the the key challenge we

00:01:56,780 --> 00:02:03,110
face well I'll assume is we want to

00:02:00,290 --> 00:02:05,000
process data all right lots of it but

00:02:03,110 --> 00:02:07,430
how much is a lot this is the first

00:02:05,000 --> 00:02:09,500
question here's some numbers that are

00:02:07,430 --> 00:02:11,959
breathing outdated because these

00:02:09,500 --> 00:02:15,690
companies in question don't publish them

00:02:11,959 --> 00:02:18,570
all the time so they are like a year old

00:02:15,690 --> 00:02:22,380
Facebook gets a lot of new data per day

00:02:18,570 --> 00:02:25,890
in 2007 2008 there was 200 gigabytes of

00:02:22,380 --> 00:02:28,500
data like logs new statistical

00:02:25,890 --> 00:02:30,390
information new data about metadata

00:02:28,500 --> 00:02:33,000
about people they had to process and

00:02:30,390 --> 00:02:35,190
somehow make sense off but they are

00:02:33,000 --> 00:02:36,540
rapidly growing as we all know so by

00:02:35,190 --> 00:02:38,460
april two thousand nine was two

00:02:36,540 --> 00:02:41,880
terabytes a day ten times this amount

00:02:38,460 --> 00:02:44,430
and by it was four terabytes by October

00:02:41,880 --> 00:02:46,020
and it was guelph terabytes by march two

00:02:44,430 --> 00:02:49,170
thousand twelve so they've grown from

00:02:46,020 --> 00:02:52,710
200 gigabytes a day to 12 terabytes a

00:02:49,170 --> 00:02:57,240
day which is lots of data and Google

00:02:52,710 --> 00:03:00,600
they process 400 and 400 petabytes a day

00:02:57,240 --> 00:03:03,120
a month that was in 2007 I haven't seen

00:03:00,600 --> 00:03:07,680
more recent number since and the average

00:03:03,120 --> 00:03:09,810
job site for them is 180 gigabytes this

00:03:07,680 --> 00:03:12,810
MapReduce approach that i will be

00:03:09,810 --> 00:03:15,720
showing is something that Google used to

00:03:12,810 --> 00:03:18,000
use to produce their index Yahoo still

00:03:15,720 --> 00:03:20,250
use it to build the surgeon next Google

00:03:18,000 --> 00:03:23,310
have since moved away from it um it is

00:03:20,250 --> 00:03:25,080
still a very very cool thing and as you

00:03:23,310 --> 00:03:26,550
can see Google takes this massive amount

00:03:25,080 --> 00:03:28,080
of data and they chunk it up into

00:03:26,550 --> 00:03:31,590
smaller slices and then they process

00:03:28,080 --> 00:03:34,739
these smaller slices but what if you

00:03:31,590 --> 00:03:37,290
have much data right let's say one

00:03:34,739 --> 00:03:39,660
percent just one percent of this amount

00:03:37,290 --> 00:03:41,850
that google ad or Facebook had a long

00:03:39,660 --> 00:03:44,010
time ago so you say two gigabytes maybe

00:03:41,850 --> 00:03:46,170
let's say you have 50 gigabytes okay you

00:03:44,010 --> 00:03:48,690
say that's no problem here's the cool

00:03:46,170 --> 00:03:50,970
thing about this I donated gigabytes the

00:03:48,690 --> 00:03:53,820
average job sighs that Google does is

00:03:50,970 --> 00:03:56,640
going to take you 45 minutes to read off

00:03:53,820 --> 00:03:58,080
at hard drive SSDs don't really change

00:03:56,640 --> 00:04:00,959
that because what you want to achieve in

00:03:58,080 --> 00:04:03,060
the end is like is our it's just I oath

00:04:00,959 --> 00:04:07,019
is throughput which means you'll have

00:04:03,060 --> 00:04:10,050
raids okay so um and then at some point

00:04:07,019 --> 00:04:11,610
the interface you reach the the peak

00:04:10,050 --> 00:04:14,670
bandwidth of the interface it's really

00:04:11,610 --> 00:04:17,310
annoying this the it blinds me can you

00:04:14,670 --> 00:04:20,489
switch off the spotlight is that

00:04:17,310 --> 00:04:23,390
possible thank you ah hello I see you

00:04:20,489 --> 00:04:23,390
now sir good

00:04:23,770 --> 00:04:29,259
and um so this is the challenge like the

00:04:27,490 --> 00:04:30,759
I oh right you need to have you need you

00:04:29,259 --> 00:04:33,580
want to read sequentially because that's

00:04:30,759 --> 00:04:35,530
faster because at the scales of which ad

00:04:33,580 --> 00:04:38,020
with which these guys operate buying

00:04:35,530 --> 00:04:39,729
SSDs is still a little bit expensive

00:04:38,020 --> 00:04:41,470
even though Facebook what Facebook not

00:04:39,729 --> 00:04:42,879
but Google have too much money on the

00:04:41,470 --> 00:04:45,280
hand and they don't know how to spend it

00:04:42,879 --> 00:04:47,979
I guarantee that if they bought SSDs

00:04:45,280 --> 00:04:52,240
instead of hard drives there will be

00:04:47,979 --> 00:04:54,039
expensive even for them so you have big

00:04:52,240 --> 00:04:55,479
amounts of data that takes a long time

00:04:54,039 --> 00:04:58,030
to read and then you have the problem

00:04:55,479 --> 00:04:59,710
that maybe you have 16 gigabytes of ram

00:04:58,030 --> 00:05:01,360
/ computer maybe have 64 it doesn't

00:04:59,710 --> 00:05:02,919
matter the whole data set that you want

00:05:01,360 --> 00:05:07,180
to process does not fit into room into

00:05:02,919 --> 00:05:10,449
memory typically right and in general

00:05:07,180 --> 00:05:12,550
nowadays computers can process data

00:05:10,449 --> 00:05:16,690
orders of magnitude faster than they can

00:05:12,550 --> 00:05:18,909
read it I computers nowadays are Rio

00:05:16,690 --> 00:05:20,830
bound pretty much in all cases unless

00:05:18,909 --> 00:05:22,120
you're doing like have a like a

00:05:20,830 --> 00:05:24,699
computing class that some kind of

00:05:22,120 --> 00:05:26,560
university like folding molecules to do

00:05:24,699 --> 00:05:29,229
cancer research which is really really

00:05:26,560 --> 00:05:31,500
really computational expensive and then

00:05:29,229 --> 00:05:34,300
you you know you move the move the

00:05:31,500 --> 00:05:37,870
complex jobs to run on the GPU or

00:05:34,300 --> 00:05:40,000
something but we as web developers we

00:05:37,870 --> 00:05:44,380
don't have these cases typically we deal

00:05:40,000 --> 00:05:47,050
with data and we need to as a solution

00:05:44,380 --> 00:05:49,000
the only way to speed things up is have

00:05:47,050 --> 00:05:51,370
power little processes that do this

00:05:49,000 --> 00:05:53,949
right so you don't have one computer

00:05:51,370 --> 00:05:57,490
that performs these operations but you

00:05:53,949 --> 00:05:59,620
will have you want 100 but the problem

00:05:57,490 --> 00:06:01,000
is that once you do this you need to

00:05:59,620 --> 00:06:06,300
start coordinating what you're doing

00:06:01,000 --> 00:06:09,310
right you need to somehow organize our

00:06:06,300 --> 00:06:10,840
what computer does what keep them in

00:06:09,310 --> 00:06:13,029
check make sure that the results are

00:06:10,840 --> 00:06:16,210
fine that everything is orderly and

00:06:13,029 --> 00:06:19,449
German and works at that hard that's

00:06:16,210 --> 00:06:21,849
really hard whatever node dies like one

00:06:19,449 --> 00:06:23,469
of the uuuu right some sort of like

00:06:21,849 --> 00:06:24,400
custom code maybe with gierman or

00:06:23,469 --> 00:06:26,199
something and then it runs in the

00:06:24,400 --> 00:06:27,880
background and blind move stuff to other

00:06:26,199 --> 00:06:31,690
computers but then wasn't for the

00:06:27,880 --> 00:06:33,399
computers breaks down for Google every

00:06:31,690 --> 00:06:35,710
minute a computer probably fails in

00:06:33,399 --> 00:06:37,409
their data centers Google when when a

00:06:35,710 --> 00:06:38,669
hard drive fails at Google server

00:06:37,409 --> 00:06:40,229
don't replace the hard drive they just

00:06:38,669 --> 00:06:41,759
switch off the computer they can't be

00:06:40,229 --> 00:06:43,379
bothered it takes way too long they have

00:06:41,759 --> 00:06:45,659
spent containers that are full of

00:06:43,379 --> 00:06:48,239
computers computer fails they just leave

00:06:45,659 --> 00:06:51,089
it dying that's fine in that case if

00:06:48,239 --> 00:06:53,099
this computer was processing a small

00:06:51,089 --> 00:06:54,599
amount of your data another computer

00:06:53,099 --> 00:06:55,860
needs to take over right but then you

00:06:54,599 --> 00:06:57,089
have io again because maybe that

00:06:55,860 --> 00:07:00,239
computer needs to read the data from

00:06:57,089 --> 00:07:03,749
somewhere tetra etc etc so if there's no

00:07:00,239 --> 00:07:06,119
ties the data is it last right what

00:07:03,749 --> 00:07:08,279
happens then what about the other nodes

00:07:06,119 --> 00:07:10,079
are they does the whole process have to

00:07:08,279 --> 00:07:12,629
restart or can you live with just

00:07:10,079 --> 00:07:17,610
there's one problem part of the job

00:07:12,629 --> 00:07:20,610
failing and can it be restarted and how

00:07:17,610 --> 00:07:22,169
do you coordinate this that is MapReduce

00:07:20,610 --> 00:07:23,639
well our kind of MapReduce is the

00:07:22,169 --> 00:07:27,649
approach an implementation will

00:07:23,639 --> 00:07:31,589
coordinator for you but the idea is that

00:07:27,649 --> 00:07:33,989
many years or decades ago when well

00:07:31,589 --> 00:07:35,399
large computer clusters were mostly or

00:07:33,989 --> 00:07:38,939
mainframes were mostly run by people

00:07:35,399 --> 00:07:40,860
with really long beards you you would

00:07:38,939 --> 00:07:44,759
distribute the workloads across a grid

00:07:40,860 --> 00:07:46,499
right because that was the times when

00:07:44,759 --> 00:07:48,659
there was not a lot of data but still

00:07:46,499 --> 00:07:51,839
computationally expensive tasks one

00:07:48,659 --> 00:07:54,599
thing would be weather forecasting you

00:07:51,839 --> 00:07:57,179
mostly you're mostly crunch numbers you

00:07:54,599 --> 00:07:59,069
have input for that sensor data input

00:07:57,179 --> 00:08:01,469
etc but there's so much statistical

00:07:59,069 --> 00:08:03,689
stuff and you need to do like these the

00:08:01,469 --> 00:08:06,749
the algorithms they they are CPU

00:08:03,689 --> 00:08:08,449
intensive that worked very very well in

00:08:06,749 --> 00:08:10,709
the olden days you had like be wolf

00:08:08,449 --> 00:08:13,860
Linux clusters right and stuff like this

00:08:10,709 --> 00:08:16,739
super computers but they ship the data

00:08:13,860 --> 00:08:18,869
around between nodes okay which was

00:08:16,739 --> 00:08:20,699
typically stored centrally ons like a

00:08:18,869 --> 00:08:22,679
storage attached network and a network

00:08:20,699 --> 00:08:24,209
storage of some sort and that is an i/o

00:08:22,679 --> 00:08:26,489
bottleneck as we already figured out

00:08:24,209 --> 00:08:28,169
because it'll take so and so long to

00:08:26,489 --> 00:08:29,849
read it off disk this has a really fast

00:08:28,169 --> 00:08:31,769
interface then you need to ship it of a

00:08:29,849 --> 00:08:33,659
Gigabit Ethernet on which other

00:08:31,769 --> 00:08:35,069
communication is also happening then you

00:08:33,659 --> 00:08:37,319
have fibre channel and all these things

00:08:35,069 --> 00:08:39,539
but that costs lots of money what you

00:08:37,319 --> 00:08:43,079
actually want is off-the-shelf hardware

00:08:39,539 --> 00:08:47,550
in a network and it'll just be blazing

00:08:43,079 --> 00:08:49,319
fast so in 2004 Google they had this

00:08:47,550 --> 00:08:50,209
publication called simplify data

00:08:49,319 --> 00:08:51,800
processing on

00:08:50,209 --> 00:08:53,689
clusters they called it MapReduce

00:08:51,800 --> 00:08:56,929
because it's built fundamentally on

00:08:53,689 --> 00:08:59,029
these two map and reduce functions of

00:08:56,929 --> 00:09:02,569
that functional programming languages so

00:08:59,029 --> 00:09:05,059
love we look at it in a second and the

00:09:02,569 --> 00:09:07,759
key is that now the data is spread out

00:09:05,059 --> 00:09:11,059
across the computers in a distributed

00:09:07,759 --> 00:09:12,499
file system and you move the processing

00:09:11,059 --> 00:09:15,319
jobs to where the data already is

00:09:12,499 --> 00:09:17,869
because moving a few kilobytes of code

00:09:15,319 --> 00:09:20,600
in place on every computer and then

00:09:17,869 --> 00:09:22,519
starting is much cheaper than shuffling

00:09:20,600 --> 00:09:25,009
50 to go out of data around when I know

00:09:22,519 --> 00:09:26,959
dies you simply tell a note that or it

00:09:25,009 --> 00:09:29,899
also has the data of the other note that

00:09:26,959 --> 00:09:31,339
failed restart this job need redundancy

00:09:29,899 --> 00:09:34,730
in the storage etc but we'll cover that

00:09:31,339 --> 00:09:36,860
in the moment and the cool thing is this

00:09:34,730 --> 00:09:39,920
is this is key to scalability this is

00:09:36,860 --> 00:09:43,009
actually something that modern Network

00:09:39,920 --> 00:09:45,889
whose am a couch to be user who has you

00:09:43,009 --> 00:09:48,379
heard of couch to be 00 quite a few

00:09:45,889 --> 00:09:53,420
people you guys Rock cuz I'm who's a

00:09:48,379 --> 00:09:56,869
MongoDB user my condolences crap it's a

00:09:53,420 --> 00:09:59,269
crap database but so CouchDB no really

00:09:56,869 --> 00:10:00,949
it is it's I keep saying it's designed

00:09:59,269 --> 00:10:02,779
by a committee of clowns and stuff but

00:10:00,949 --> 00:10:07,009
I'm get like I can tell this story later

00:10:02,779 --> 00:10:09,949
so um couchdb is a really good example

00:10:07,009 --> 00:10:15,290
for the trade-offs that we have we have

00:10:09,949 --> 00:10:17,540
learned to make in recent years b/c it's

00:10:15,290 --> 00:10:19,249
particularly difficult in Germany to

00:10:17,540 --> 00:10:22,639
sell to a manager that something might

00:10:19,249 --> 00:10:25,279
go wrong like yo the data might be gone

00:10:22,639 --> 00:10:26,420
like we might lose Oh point 0 1 percent

00:10:25,279 --> 00:10:29,389
of the data per day and they will say

00:10:26,420 --> 00:10:31,879
1999 this is assists for Bolton you must

00:10:29,389 --> 00:10:35,329
you must not lose data but if you want

00:10:31,879 --> 00:10:38,569
to scale up you cannot have everything

00:10:35,329 --> 00:10:41,299
in sync you cannot notify everything

00:10:38,569 --> 00:10:43,189
that something had like the web is the

00:10:41,299 --> 00:10:45,379
best example the reason why the World

00:10:43,189 --> 00:10:47,420
Wide Web became the first planetary

00:10:45,379 --> 00:10:50,749
scale computer network that there is as

00:10:47,420 --> 00:10:54,679
has ever been is the link the hyperlink

00:10:50,749 --> 00:10:57,259
and more generically hypermedia because

00:10:54,679 --> 00:11:00,049
you can link to another resource and if

00:10:57,259 --> 00:11:02,299
that resource is gone you'll get a 404

00:11:00,049 --> 00:11:03,620
from the other server there is no

00:11:02,299 --> 00:11:05,450
mechanism

00:11:03,620 --> 00:11:07,760
when the resource is deleted on the web

00:11:05,450 --> 00:11:09,710
all the links that point to it somehow

00:11:07,760 --> 00:11:12,800
get notified that something happens

00:11:09,710 --> 00:11:14,540
there is no locking on the web that is

00:11:12,800 --> 00:11:20,930
why it works so well this is the only

00:11:14,540 --> 00:11:22,400
reason why I became successful but what

00:11:20,930 --> 00:11:25,460
we have in MapReduce is it's kind of

00:11:22,400 --> 00:11:27,380
it's basically it's the same philosophy

00:11:25,460 --> 00:11:29,360
is the same approaches you have a share

00:11:27,380 --> 00:11:31,339
nothing architecture individual nodes on

00:11:29,360 --> 00:11:35,360
on this cluster do not know anything

00:11:31,339 --> 00:11:38,420
about other classes other computers so

00:11:35,360 --> 00:11:40,279
you have a hundred 100 computers each

00:11:38,420 --> 00:11:42,350
working on a small chunk of your data

00:11:40,279 --> 00:11:44,270
processing job right but they are not

00:11:42,350 --> 00:11:46,850
aware that there are 99 others they

00:11:44,270 --> 00:11:48,589
don't know how far along the process is

00:11:46,850 --> 00:11:50,570
on these other computers it's a share

00:11:48,589 --> 00:11:52,850
nothing architecture which dramatically

00:11:50,570 --> 00:11:55,640
decreases the amount of coordination you

00:11:52,850 --> 00:11:57,500
need between computers and that allows

00:11:55,640 --> 00:12:00,529
scale because whether you do this with

00:11:57,500 --> 00:12:02,000
10 or a thousand nodes doesn't matter it

00:12:00,529 --> 00:12:04,610
will still work unlike other

00:12:02,000 --> 00:12:06,320
technologies we're at some point the

00:12:04,610 --> 00:12:08,360
cost of communication between knows

00:12:06,320 --> 00:12:11,360
become so expensive that if you can't

00:12:08,360 --> 00:12:15,650
afford getting up anymore the basic

00:12:11,360 --> 00:12:19,640
principle is that a mapper first reads a

00:12:15,650 --> 00:12:22,339
record let's say an Apache log file okay

00:12:19,640 --> 00:12:24,470
and extracts a quina value and if each

00:12:22,339 --> 00:12:27,110
line in the access log is treated as a

00:12:24,470 --> 00:12:30,860
record then we would say read the client

00:12:27,110 --> 00:12:34,310
IP address okay and use that as the key

00:12:30,860 --> 00:12:35,540
and we use the number of bytes that were

00:12:34,310 --> 00:12:37,190
transferred in the request because

00:12:35,540 --> 00:12:40,029
that's an Apache log file right you've

00:12:37,190 --> 00:12:45,620
all seen Apache log files hopefully cool

00:12:40,029 --> 00:12:51,520
wait you gotta ask and we omit that as

00:12:45,620 --> 00:12:54,620
value arm so we have localhost 10 bytes

00:12:51,520 --> 00:12:58,940
some other IP address 150 bytes etc etc

00:12:54,620 --> 00:13:02,000
okay and this gets emitted and the cool

00:12:58,940 --> 00:13:03,920
thing is like a very basic approach of

00:13:02,000 --> 00:13:06,140
splitting up this workload across many

00:13:03,920 --> 00:13:07,940
computers would be you rotate logs in

00:13:06,140 --> 00:13:09,560
such a way that you have one log file

00:13:07,940 --> 00:13:11,390
per hour and then you can have 24

00:13:09,560 --> 00:13:14,450
computers working on analyzing the data

00:13:11,390 --> 00:13:15,920
for one day and practice as much it's

00:13:14,450 --> 00:13:17,360
much better than that

00:13:15,920 --> 00:13:19,550
the hardwood distributed file system

00:13:17,360 --> 00:13:21,380
actually is aware of the nature of the

00:13:19,550 --> 00:13:24,019
file so it says okay there's a txt file

00:13:21,380 --> 00:13:27,649
its splittable it'll split it up into

00:13:24,019 --> 00:13:29,720
smaller chunks so you can have one big

00:13:27,649 --> 00:13:31,760
log file but it'll still be able to do

00:13:29,720 --> 00:13:35,300
this in parallel on more than one

00:13:31,760 --> 00:13:37,940
instance so now you have a lot of IP

00:13:35,300 --> 00:13:39,709
address above bites I appear that none

00:13:37,940 --> 00:13:43,190
of bites lines essentially right and

00:13:39,709 --> 00:13:47,360
let's say across a few files and then

00:13:43,190 --> 00:13:49,100
you bucket them into one bucket per IP

00:13:47,360 --> 00:13:51,230
address so the key becomes essentially

00:13:49,100 --> 00:13:53,389
the collection this is then hand to the

00:13:51,230 --> 00:13:55,370
reducer the redditor receives this key

00:13:53,389 --> 00:13:57,649
the IP address let's say localhost and

00:13:55,370 --> 00:14:05,600
all the number of bytes that we received

00:13:57,649 --> 00:14:07,100
and the cool thing is this is this

00:14:05,600 --> 00:14:10,519
actually recruit this step requires some

00:14:07,100 --> 00:14:12,829
coordination so you have 10 computers

00:14:10,519 --> 00:14:14,959
doing this by producing IP bytes I he

00:14:12,829 --> 00:14:18,139
bites ap bytes and then it'll collect

00:14:14,959 --> 00:14:21,800
all this all the bite numbers for one IP

00:14:18,139 --> 00:14:23,540
address on one computer and then there

00:14:21,800 --> 00:14:25,339
you can sum them up and then you know

00:14:23,540 --> 00:14:27,890
which climbs across your cluster of a

00:14:25,339 --> 00:14:30,260
hundred web service which clients did

00:14:27,890 --> 00:14:31,730
the most traffic I can also do this with

00:14:30,260 --> 00:14:36,910
the request URL which would be much more

00:14:31,730 --> 00:14:39,829
practical this is just an example and

00:14:36,910 --> 00:14:41,300
you we can so we send this up and if you

00:14:39,829 --> 00:14:44,990
if you look at this step by step if you

00:14:41,300 --> 00:14:47,089
assume that this is your input okay then

00:14:44,990 --> 00:14:49,850
you have a few IP addresses I have a

00:14:47,089 --> 00:14:52,610
laser IP addresses and buy it awesome

00:14:49,850 --> 00:14:57,589
right I wish this looked like a shark

00:14:52,610 --> 00:14:59,390
with a friggin laser beam but so um say

00:14:57,589 --> 00:15:00,709
of IP addresses and so this is just two

00:14:59,390 --> 00:15:02,300
different IPS but as you can see that

00:15:00,709 --> 00:15:04,880
there's no order right because it's an

00:15:02,300 --> 00:15:08,690
Apache log file and then you can group

00:15:04,880 --> 00:15:10,399
it like this okay and now you can send

00:15:08,690 --> 00:15:12,500
this off to one computer and this to

00:15:10,399 --> 00:15:15,519
another and what they do is they take

00:15:12,500 --> 00:15:18,019
all these bites and sum them up boom

00:15:15,519 --> 00:15:22,160
that is trivial right if you do to one

00:15:18,019 --> 00:15:23,810
computer if you have 100 web service you

00:15:22,160 --> 00:15:25,190
could ship it all to one box which takes

00:15:23,810 --> 00:15:28,130
forever because it's a couple of

00:15:25,190 --> 00:15:28,970
gigabytes per log file at the log files

00:15:28,130 --> 00:15:30,649
person

00:15:28,970 --> 00:15:35,389
if you have 100 web servers you have a

00:15:30,649 --> 00:15:38,029
lot of traffic I suppose and that's just

00:15:35,389 --> 00:15:40,009
I oh I oh I oh so this is the thing

00:15:38,029 --> 00:15:41,990
that's MapReduce and its implementation

00:15:40,009 --> 00:15:45,529
hard absolves we're going to look at

00:15:41,990 --> 00:15:46,759
that later if my code works so if you

00:15:45,529 --> 00:15:48,680
put this into code it kind of looks like

00:15:46,759 --> 00:15:53,029
this you have a map okay you get the

00:15:48,680 --> 00:15:54,379
line number and the line text the line

00:15:53,029 --> 00:15:55,879
number is not really important it could

00:15:54,379 --> 00:15:57,829
also be a byte offset or maybe let's

00:15:55,879 --> 00:15:59,959
just ignore this because with treating

00:15:57,829 --> 00:16:01,910
text files and having a key for the map

00:15:59,959 --> 00:16:04,730
it makes not a lot of sense but how does

00:16:01,910 --> 00:16:05,930
it anyway so we parse the Apache log and

00:16:04,730 --> 00:16:08,480
we say okay this returns some sort of

00:16:05,930 --> 00:16:11,870
array this is an imaginary function

00:16:08,480 --> 00:16:14,360
right and so we have the IP and the

00:16:11,870 --> 00:16:16,250
bytes so we use the IP as the key and

00:16:14,360 --> 00:16:17,839
devices the value and then some magic

00:16:16,250 --> 00:16:19,160
happens and then our reduce function is

00:16:17,839 --> 00:16:21,829
called with a key and an array of values

00:16:19,160 --> 00:16:24,050
and we do in a race on this is actually

00:16:21,829 --> 00:16:26,569
bad because the problem is you'd have to

00:16:24,050 --> 00:16:29,240
load all values into memory so better is

00:16:26,569 --> 00:16:30,800
to have an iterator that doesn't have to

00:16:29,240 --> 00:16:33,949
do this but this only affects when you

00:16:30,800 --> 00:16:35,569
do PHP the java stuff that how do comes

00:16:33,949 --> 00:16:39,550
with doesn't have this problem anyway

00:16:35,569 --> 00:16:42,850
but this is what you use his input okay

00:16:39,550 --> 00:16:45,079
basically and this will be the output

00:16:42,850 --> 00:16:47,389
this was sorted we could actually just

00:16:45,079 --> 00:16:48,800
read from standard in and there Emma to

00:16:47,389 --> 00:16:50,360
standard out that rich from standard in

00:16:48,800 --> 00:16:53,360
and the midst of standard out here and

00:16:50,360 --> 00:16:55,639
then use unix pipes I could demonstrate

00:16:53,360 --> 00:16:57,819
this later if you want but theoretically

00:16:55,639 --> 00:17:01,189
that would work already you could just

00:16:57,819 --> 00:17:04,159
catch the file into the mapper pipe it

00:17:01,189 --> 00:17:06,110
out into the reducer the only problem is

00:17:04,159 --> 00:17:09,049
that the keys do not get grouped in the

00:17:06,110 --> 00:17:10,850
in the step between that so you wouldn't

00:17:09,049 --> 00:17:13,760
get this exact results but this is the

00:17:10,850 --> 00:17:18,770
result that you get in theory okay so IP

00:17:13,760 --> 00:17:22,360
address and the some bites hold on

00:17:18,770 --> 00:17:22,360
broken slide is broken

00:17:30,730 --> 00:17:40,900
what did I do I'm just deleting slides

00:17:34,600 --> 00:17:42,580
that make no sense okay so now to do all

00:17:40,900 --> 00:17:46,210
this I mean map produces that's just an

00:17:42,580 --> 00:17:48,520
idea right it's it's a it's a system

00:17:46,210 --> 00:17:50,410
architecture of source but you want to

00:17:48,520 --> 00:17:51,549
do you want to implement something using

00:17:50,410 --> 00:17:54,220
this approach but that you've used

00:17:51,549 --> 00:17:57,820
Apache hard oh it's a yellow elephant so

00:17:54,220 --> 00:18:01,929
it is Java but it's still miss an

00:17:57,820 --> 00:18:04,000
elephant it is a MapReduce framework

00:18:01,929 --> 00:18:06,460
which actually actually also has a

00:18:04,000 --> 00:18:09,250
massive massive massive ecosystem of

00:18:06,460 --> 00:18:11,950
auxiliary projects that somehow

00:18:09,250 --> 00:18:13,419
complements this general MapReduce idea

00:18:11,950 --> 00:18:15,840
you have a distributed file system you

00:18:13,419 --> 00:18:18,610
have stuff serializing data you have

00:18:15,840 --> 00:18:20,980
systems that are able to coordinate

00:18:18,610 --> 00:18:25,240
distributive workloads all these things

00:18:20,980 --> 00:18:26,919
it's really really cool and the what how

00:18:25,240 --> 00:18:29,679
do potentially does is it allows us it

00:18:26,919 --> 00:18:31,960
takes it does all the heavy lifting the

00:18:29,679 --> 00:18:33,880
coronation of jobs they're sending data

00:18:31,960 --> 00:18:35,559
back and forth if necessary the

00:18:33,880 --> 00:18:37,530
distributed file system so we can focus

00:18:35,559 --> 00:18:41,020
on writing mappers and reducers okay and

00:18:37,530 --> 00:18:42,610
it works extremely well because if you

00:18:41,020 --> 00:18:46,720
look at some of the users for instance

00:18:42,610 --> 00:18:49,380
facebook they use it mostly with a

00:18:46,720 --> 00:18:52,860
technology called hive hive essentially

00:18:49,380 --> 00:18:56,080
what you saw earlier we emitted IP tab

00:18:52,860 --> 00:18:57,760
number of bytes right with hive you can

00:18:56,080 --> 00:19:00,760
treat that essentially as a sequel table

00:18:57,760 --> 00:19:03,010
and then make sequel like queries

00:19:00,760 --> 00:19:07,270
against it and it'll transform this SQL

00:19:03,010 --> 00:19:09,490
into into mapper and reducer code

00:19:07,270 --> 00:19:11,770
compile it ship it out to clines and do

00:19:09,490 --> 00:19:13,900
stuff and like an hour later it gets

00:19:11,770 --> 00:19:15,429
back to you with the result so with

00:19:13,900 --> 00:19:17,559
MapReduce and with how do we don't get

00:19:15,429 --> 00:19:18,850
instant results the stuff that i showed

00:19:17,559 --> 00:19:20,830
you if you want to do it across 100 web

00:19:18,850 --> 00:19:22,390
servers with many many gigabytes of logs

00:19:20,830 --> 00:19:25,510
that's probably going to run for an hour

00:19:22,390 --> 00:19:28,120
too okay but you can speed it up if you

00:19:25,510 --> 00:19:30,160
want to run twice as fast you can just

00:19:28,120 --> 00:19:32,110
add a bunch of process service ideally

00:19:30,160 --> 00:19:34,120
you run hard OOP on each of your web

00:19:32,110 --> 00:19:36,880
service at least the descriptor file

00:19:34,120 --> 00:19:38,860
system because most web servers do not

00:19:36,880 --> 00:19:43,299
really have a high amount of CPU

00:19:38,860 --> 00:19:44,290
utilization so they are oftentimes idle

00:19:43,299 --> 00:19:46,360
anyway

00:19:44,290 --> 00:19:49,050
so you can give them something to do by

00:19:46,360 --> 00:19:53,320
installing hard of instances on them

00:19:49,050 --> 00:19:57,670
Facebook arm mostly most of their tasks

00:19:53,320 --> 00:20:00,820
are hive so it's SQL arm but basic

00:19:57,670 --> 00:20:03,610
principle is the same they have 8400 cpu

00:20:00,820 --> 00:20:05,860
cores with 12.5 petabytes of combined

00:20:03,610 --> 00:20:09,040
storage that is also at least the number

00:20:05,860 --> 00:20:12,820
or a year old is number so it's a lot I

00:20:09,040 --> 00:20:18,940
suppose it's that'll be 4100 service I

00:20:12,820 --> 00:20:21,670
guess for core service I'm sorry 2100 um

00:20:18,940 --> 00:20:23,650
oh no actually it isn't because it says

00:20:21,670 --> 00:20:26,380
here eight cores 12 terabytes of storage

00:20:23,650 --> 00:20:31,150
sorry and 32 gigabytes of RAM per node

00:20:26,380 --> 00:20:34,090
okay they have and here's the cool thing

00:20:31,150 --> 00:20:37,510
they have gigabit ethernet for each of

00:20:34,090 --> 00:20:39,130
in iraq and each rack has four gigabit

00:20:37,510 --> 00:20:42,400
ethernet like a trunk connection right

00:20:39,130 --> 00:20:45,010
with fork with four lanes to from the

00:20:42,400 --> 00:20:48,160
rack switch to the core switch and this

00:20:45,010 --> 00:20:50,590
is a key thing because hardwood is aware

00:20:48,160 --> 00:20:53,350
of your network topology when you say

00:20:50,590 --> 00:20:55,750
hey how do i have i have 10 computers

00:20:53,350 --> 00:20:57,880
you can actually tell it these five are

00:20:55,750 --> 00:21:00,490
in iraq and these five are in iraq so

00:20:57,880 --> 00:21:02,350
when you store data and you say i want a

00:21:00,490 --> 00:21:04,360
replication factor of three for the date

00:21:02,350 --> 00:21:06,070
i store for data safety and in case

00:21:04,360 --> 00:21:08,050
something fails that other nodes can

00:21:06,070 --> 00:21:10,060
take over it will store the data on a

00:21:08,050 --> 00:21:12,340
server it will store the same a copy of

00:21:10,060 --> 00:21:14,800
the data on another server in the same

00:21:12,340 --> 00:21:17,440
rack and then it will store a third copy

00:21:14,800 --> 00:21:20,260
of the data in a rack that's as far away

00:21:17,440 --> 00:21:24,750
as possible in case something blows up

00:21:20,260 --> 00:21:26,950
right so this is pretty cool because

00:21:24,750 --> 00:21:29,320
they essentially rely on the fact that

00:21:26,950 --> 00:21:32,110
if there is data that one node needs it

00:21:29,320 --> 00:21:33,910
can copy that locally but if data needs

00:21:32,110 --> 00:21:36,910
to be shuffled around between nodes many

00:21:33,910 --> 00:21:39,280
nodes need to do this at once so they

00:21:36,910 --> 00:21:42,820
have like the backbone across the racks

00:21:39,280 --> 00:21:45,010
has a higher capacity they do 25

00:21:42,820 --> 00:21:46,960
terabytes of logging data that they lets

00:21:45,010 --> 00:21:49,270
this is what they ingest with a

00:21:46,960 --> 00:21:52,930
technology called scribe scribe it has

00:21:49,270 --> 00:21:55,780
anyone ever heard thrift are a few has

00:21:52,930 --> 00:21:56,980
anyone ever heard scribe have you used

00:21:55,780 --> 00:22:00,880
scribe

00:21:56,980 --> 00:22:03,790
did it make you angry good have you

00:22:00,880 --> 00:22:06,520
heard of flume cloudera flu it's an ah

00:22:03,790 --> 00:22:09,070
perfect i'll talk about this later so

00:22:06,520 --> 00:22:11,669
scribe is essentially a log aggregation

00:22:09,070 --> 00:22:14,500
service what it does it is you start and

00:22:11,669 --> 00:22:16,000
like a listener on each lock on each box

00:22:14,500 --> 00:22:17,950
and we each web server and you make

00:22:16,000 --> 00:22:20,380
local this is a local socket connection

00:22:17,950 --> 00:22:22,990
through a protocol called thrift thrift

00:22:20,380 --> 00:22:25,900
is like Google's protocol buffers it's a

00:22:22,990 --> 00:22:27,340
it's a serialization format you can use

00:22:25,900 --> 00:22:33,669
it to do our piece remote procedure

00:22:27,340 --> 00:22:36,580
calls and so you you have like code that

00:22:33,669 --> 00:22:38,740
logs from like a PHP code or Java

00:22:36,580 --> 00:22:41,290
whatever it uses this thrift interface

00:22:38,740 --> 00:22:43,150
because the scribe interface it said

00:22:41,290 --> 00:22:44,830
there must be a category for the message

00:22:43,150 --> 00:22:48,580
and a message there's no time stamp

00:22:44,830 --> 00:22:49,840
which is very clever and so and then you

00:22:48,580 --> 00:22:51,460
can say okay so this is a service

00:22:49,840 --> 00:22:53,950
definition and I want to generate code

00:22:51,460 --> 00:22:56,110
for PHP or java or c++ and then you have

00:22:53,950 --> 00:22:58,840
generated code that can simply log staff

00:22:56,110 --> 00:23:00,970
to this service this service means it's

00:22:58,840 --> 00:23:02,410
a local agent on every box if you have a

00:23:00,970 --> 00:23:04,929
hundred web service you have 100 local

00:23:02,410 --> 00:23:06,730
agents you lock to them they buffer this

00:23:04,929 --> 00:23:09,429
stuff on disk in the background and

00:23:06,730 --> 00:23:12,100
occasionally they will set the stuff to

00:23:09,429 --> 00:23:14,380
a local add to a central scribe log

00:23:12,100 --> 00:23:15,910
aggregation server which is very cool

00:23:14,380 --> 00:23:18,309
because the logging call that you make

00:23:15,910 --> 00:23:20,380
from your PHP web server software for

00:23:18,309 --> 00:23:23,410
instance there non-blocking they just

00:23:20,380 --> 00:23:27,040
they return immediately and then after a

00:23:23,410 --> 00:23:30,490
while the data will end up in some sort

00:23:27,040 --> 00:23:32,620
of central storage system they put this

00:23:30,490 --> 00:23:35,080
the data they collect with scribe into

00:23:32,620 --> 00:23:40,750
hard HDFS into heart of distributed file

00:23:35,080 --> 00:23:41,950
system the flume is like an alternative

00:23:40,750 --> 00:23:42,910
product that I'll cover later which is

00:23:41,950 --> 00:23:44,860
much more modern has a much nicer

00:23:42,910 --> 00:23:47,049
architecture and it's better suited to

00:23:44,860 --> 00:23:52,120
that but anyway they have 25 terabytes

00:23:47,049 --> 00:23:55,240
of data and they scan 135 terabytes of

00:23:52,120 --> 00:23:56,830
data roughly a day with their processing

00:23:55,240 --> 00:23:59,500
jobs that run they touch her in thirty

00:23:56,830 --> 00:24:01,929
five terabytes of compressed data

00:23:59,500 --> 00:24:05,409
because everything's gzipped in are dope

00:24:01,929 --> 00:24:08,760
typically they run around seven seven

00:24:05,409 --> 00:24:08,760
half thousand hive jobs and

00:24:10,309 --> 00:24:17,880
these together consumer about 80,000

00:24:13,230 --> 00:24:21,270
compute hours per day and the new data

00:24:17,880 --> 00:24:24,539
they have is 200 gigabytes per day in in

00:24:21,270 --> 00:24:26,130
2008 two terabytes in 2009 I think I

00:24:24,539 --> 00:24:29,399
already showed you this yeah there

00:24:26,130 --> 00:24:32,850
wasn't an earlier flight um Yahoo on the

00:24:29,399 --> 00:24:34,710
other hand yeah who does pretty much

00:24:32,850 --> 00:24:36,299
everything with hardwood they even built

00:24:34,710 --> 00:24:40,200
this search index with it as I mentioned

00:24:36,299 --> 00:24:42,419
earlier and their biggest cost is they

00:24:40,200 --> 00:24:45,000
have 25,000 computers with 100,000 CPUs

00:24:42,419 --> 00:24:47,370
and that's numbers from 2009 or 2010 so

00:24:45,000 --> 00:24:50,610
it's a lot more now probably oh maybe

00:24:47,370 --> 00:24:52,710
not because yahoo is i'm not going to

00:24:50,610 --> 00:24:54,510
say a sinking ship but the biggest class

00:24:52,710 --> 00:25:00,779
do they have is they have 4,000 nodes

00:24:54,510 --> 00:25:03,690
okay and 2 times 4 cpu cores each and 16

00:25:00,779 --> 00:25:05,850
gigabytes of ram i think this is the

00:25:03,690 --> 00:25:08,159
cluster they use to win the terror sort

00:25:05,850 --> 00:25:12,510
so we sort a terabyte of data in the

00:25:08,159 --> 00:25:14,250
smallest amount of time possible after

00:25:12,510 --> 00:25:16,260
which a lot of people stood like you

00:25:14,250 --> 00:25:19,770
know university people with beards said

00:25:16,260 --> 00:25:23,789
but it's not efficient like you need so

00:25:19,770 --> 00:25:25,890
many computers and the algorithm is but

00:25:23,789 --> 00:25:28,080
this misses the point right because you

00:25:25,890 --> 00:25:30,480
can't scale it up this is the of course

00:25:28,080 --> 00:25:33,059
like sorting a terabyte of data how do

00:25:30,480 --> 00:25:35,880
is not an efficient thing you can easily

00:25:33,059 --> 00:25:38,010
have I don't know twenty five percent

00:25:35,880 --> 00:25:42,000
improvement fifty percent maybe if you

00:25:38,010 --> 00:25:44,520
did it like with custom code you know

00:25:42,000 --> 00:25:47,340
Taylor to all to do to the to the exact

00:25:44,520 --> 00:25:49,770
amount of data that you have at a given

00:25:47,340 --> 00:25:51,360
point in time but you can't easily scale

00:25:49,770 --> 00:25:53,130
it up you can't add computing notes to

00:25:51,360 --> 00:25:58,529
make it faster to cope with more data

00:25:53,130 --> 00:26:01,169
with higher loads so um yeah they want

00:25:58,529 --> 00:26:03,539
this terabytes sorting competition with

00:26:01,169 --> 00:26:05,309
with such a cluster of course it's fast

00:26:03,539 --> 00:26:07,289
but they could easily double the

00:26:05,309 --> 00:26:09,899
performance if they just double the

00:26:07,289 --> 00:26:19,100
number of computer notes right getting a

00:26:09,899 --> 00:26:23,750
call not anymore and forty percent

00:26:19,100 --> 00:26:26,670
forty percent of these jobs run using a

00:26:23,750 --> 00:26:29,250
system called pig you know earlier on

00:26:26,670 --> 00:26:32,460
facebook users hive which is sequel like

00:26:29,250 --> 00:26:36,000
pig is is a is used as a language called

00:26:32,460 --> 00:26:38,280
Pig Latin and you right processing tasks

00:26:36,000 --> 00:26:40,530
essentially you give just line by line

00:26:38,280 --> 00:26:44,100
instructions on take this data split it

00:26:40,530 --> 00:26:45,990
up by tab then put it into I don't know

00:26:44,100 --> 00:26:47,220
these in these fields and then use these

00:26:45,990 --> 00:26:49,290
in these fields and group them and blah

00:26:47,220 --> 00:26:51,570
blah blah so you explain step by step

00:26:49,290 --> 00:26:53,760
how you want your data processed by them

00:26:51,570 --> 00:26:55,920
hard web system and again it takes these

00:26:53,760 --> 00:26:59,070
instructions generates code out of them

00:26:55,920 --> 00:27:02,570
and well creates a task and hard up and

00:26:59,070 --> 00:27:04,830
started across your processing cluster a

00:27:02,570 --> 00:27:09,690
few other notable users will be Twitter

00:27:04,830 --> 00:27:13,830
they use it for storage logging they use

00:27:09,690 --> 00:27:20,790
pig a lot Rackspace mostly has leucine

00:27:13,830 --> 00:27:22,770
and solar fallout 4 log analysis so the

00:27:20,790 --> 00:27:25,710
results get put it to leucine and solar

00:27:22,770 --> 00:27:30,000
so they can search over it the results

00:27:25,710 --> 00:27:31,530
of their processing jobs linkedin uses

00:27:30,000 --> 00:27:33,330
friend suggestions so they analyze the

00:27:31,530 --> 00:27:37,260
social graph with MapReduce and then

00:27:33,330 --> 00:27:40,440
produce the recommendations why is this

00:27:37,260 --> 00:27:42,330
always advancing to one's last of em

00:27:40,440 --> 00:27:43,950
does that does its charts like this

00:27:42,330 --> 00:27:46,050
which is a very is the perfect use case

00:27:43,950 --> 00:27:47,910
computing charts because you simply

00:27:46,050 --> 00:27:52,410
count the number of times and song was

00:27:47,910 --> 00:27:54,420
played and that's it basically and also

00:27:52,410 --> 00:27:57,540
a/b testing so they have they log the

00:27:54,420 --> 00:27:59,220
data and they also log like this feature

00:27:57,540 --> 00:28:01,200
variant a feature variant be and then

00:27:59,220 --> 00:28:02,910
they figure out okay so with this

00:28:01,200 --> 00:28:03,990
feature we had more use with this

00:28:02,910 --> 00:28:06,300
version of the feature we have more

00:28:03,990 --> 00:28:08,820
users and then they figure out how to

00:28:06,300 --> 00:28:10,140
improve the usability on their side the

00:28:08,820 --> 00:28:11,760
New York Times did a very interesting

00:28:10,140 --> 00:28:16,110
thing they had thought terabytes of

00:28:11,760 --> 00:28:18,990
scans of old newspaper issues and they

00:28:16,110 --> 00:28:21,630
used hard oop to essentially just you

00:28:18,990 --> 00:28:25,080
know have a power huge parallel cluster

00:28:21,630 --> 00:28:28,320
they use Amazon's ec2 and they had these

00:28:25,080 --> 00:28:31,659
they ran OCR software but like on

00:28:28,320 --> 00:28:34,490
thousand instances or something

00:28:31,659 --> 00:28:37,940
so this is hard ooh which is really

00:28:34,490 --> 00:28:40,429
awesome are really convenient it gets so

00:28:37,940 --> 00:28:42,590
much so much so many new features and

00:28:40,429 --> 00:28:45,110
bug fixes every day yahoo is a is a big

00:28:42,590 --> 00:28:47,119
contributor to to harder but also other

00:28:45,110 --> 00:28:50,210
companies facebook users that lots

00:28:47,119 --> 00:28:52,519
facebook is very active in very much

00:28:50,210 --> 00:28:54,590
involved in the hive development for

00:28:52,519 --> 00:28:57,399
instance but there's one problem which

00:28:54,590 --> 00:28:59,929
is that you need to write Java code so

00:28:57,399 --> 00:29:04,759
what we have to do is use how tubes

00:28:59,929 --> 00:29:06,889
streaming feature the idea is that it

00:29:04,759 --> 00:29:09,049
uses any kind of script as the mapper

00:29:06,889 --> 00:29:10,700
and reducer instead of a java class i

00:29:09,049 --> 00:29:12,379
showed you PHP code earlier you'd have

00:29:10,700 --> 00:29:15,320
to write it in java but you can also

00:29:12,379 --> 00:29:18,590
have PHP it simply starts a process then

00:29:15,320 --> 00:29:20,269
uses some you can also configure it blah

00:29:18,590 --> 00:29:22,850
blah blah blah but it uses standard in

00:29:20,269 --> 00:29:24,710
and standard out so it simply opens to

00:29:22,850 --> 00:29:27,619
invokes a PHP script once for the mapper

00:29:24,710 --> 00:29:32,059
and then just sends it lines lines lines

00:29:27,619 --> 00:29:35,360
lines lines or stuff and your mappers

00:29:32,059 --> 00:29:39,409
then are expected to accept typically

00:29:35,360 --> 00:29:45,289
well this is actually an old version I'm

00:29:39,409 --> 00:29:46,999
sorry I can't fix this now I'll fix it

00:29:45,289 --> 00:29:49,369
before I sell other slice it needs

00:29:46,999 --> 00:29:52,190
explanation but it used to send the byte

00:29:49,369 --> 00:29:55,129
offset of the text followed red tab line

00:29:52,190 --> 00:29:58,460
now it just sends the line and then your

00:29:55,129 --> 00:30:02,029
your mapper is expected to produce a key

00:29:58,460 --> 00:30:04,850
tap the value new line so IP address tab

00:30:02,029 --> 00:30:06,950
number of bytes new line that we

00:30:04,850 --> 00:30:11,059
extracted from this line of apache logs

00:30:06,950 --> 00:30:14,480
that we received and the reduces then

00:30:11,059 --> 00:30:17,149
get key key key key which is the value

00:30:14,480 --> 00:30:19,369
each and it does not start a new reducer

00:30:17,149 --> 00:30:25,490
process for a new key so you need to

00:30:19,369 --> 00:30:27,259
watch out okay but are there is code

00:30:25,490 --> 00:30:28,100
that takes this off your shoulders so

00:30:27,259 --> 00:30:33,129
you don't have to deal with these

00:30:28,100 --> 00:30:36,649
aspects of it I'll show it later hdfs is

00:30:33,129 --> 00:30:39,679
one of the core things of Hadoop is its

00:30:36,649 --> 00:30:41,980
distributed file system and it stores

00:30:39,679 --> 00:30:43,360
data in individual blocks

00:30:41,980 --> 00:30:45,580
this is something that's very important

00:30:43,360 --> 00:30:47,260
because you need copies of the data

00:30:45,580 --> 00:30:49,540
across your computing clusters so you

00:30:47,260 --> 00:30:52,059
can actually send the processing jobs to

00:30:49,540 --> 00:30:54,040
where the data is right the default

00:30:52,059 --> 00:30:55,960
block sizes 64 megabytes you can

00:30:54,040 --> 00:30:58,210
configure this but it's important to get

00:30:55,960 --> 00:31:00,309
as close to that as possible if you have

00:30:58,210 --> 00:31:02,020
small logs it's probably easier to

00:31:00,309 --> 00:31:04,870
aggregate them somewhere first that the

00:31:02,020 --> 00:31:07,330
second where cloud eros flume would come

00:31:04,870 --> 00:31:11,650
in handy or you should group up log

00:31:07,330 --> 00:31:13,929
files because if you're if a file you

00:31:11,650 --> 00:31:16,540
store in this HDFS is just one megabyte

00:31:13,929 --> 00:31:18,790
it'll still consume 64 megabytes of disk

00:31:16,540 --> 00:31:24,460
space because this is the block size

00:31:18,790 --> 00:31:26,770
HDFS has and the reason why it's 64

00:31:24,460 --> 00:31:29,320
megabytes is that this kind of the sweet

00:31:26,770 --> 00:31:31,210
spot where most hard drives really kick

00:31:29,320 --> 00:31:34,360
into their sequential reading mode and

00:31:31,210 --> 00:31:36,250
they are allocated in blocks so when you

00:31:34,360 --> 00:31:38,980
read these 64 megabytes the disk is just

00:31:36,250 --> 00:31:40,720
reading the head is moving barely as the

00:31:38,980 --> 00:31:43,870
disk spins you don't have many sikhs

00:31:40,720 --> 00:31:45,400
because seikhs are slow that's your i/o

00:31:43,870 --> 00:31:48,700
bound again so you want to you want to

00:31:45,400 --> 00:31:50,559
stream data as much as you can it's

00:31:48,700 --> 00:31:51,880
designed for very very large data sets

00:31:50,559 --> 00:31:53,950
that you can store terabytes and

00:31:51,880 --> 00:31:55,570
petabytes of data in a in a in a

00:31:53,950 --> 00:31:57,790
distributed file system without issues

00:31:55,570 --> 00:31:59,580
you can simply add notes and it'll

00:31:57,790 --> 00:32:03,280
you'll have you'll increase your storage

00:31:59,580 --> 00:32:04,419
and 64 megabytes is the reason why it's

00:32:03,280 --> 00:32:08,049
designed for streaming rather than

00:32:04,419 --> 00:32:10,330
random reads it's right once read many

00:32:08,049 --> 00:32:12,250
although now a pending is also possible

00:32:10,330 --> 00:32:14,080
initially it was designed as only you

00:32:12,250 --> 00:32:15,429
can only write a file once if you want

00:32:14,080 --> 00:32:17,140
to replace if you want i append to a

00:32:15,429 --> 00:32:21,910
file you need to read it depends put the

00:32:17,140 --> 00:32:23,230
whole thing back and it can do

00:32:21,910 --> 00:32:24,940
compression transparently which is

00:32:23,230 --> 00:32:26,980
really cool so you store something and

00:32:24,940 --> 00:32:29,320
it's transparently gzipped but when you

00:32:26,980 --> 00:32:35,679
read it out again it's just text again

00:32:29,320 --> 00:32:37,419
um a few um a few of the core concepts

00:32:35,679 --> 00:32:40,510
I've already mentioned this the large

00:32:37,419 --> 00:32:42,640
blocks that minimize seeks right the

00:32:40,510 --> 00:32:44,440
blocks are stored redundantly so by

00:32:42,640 --> 00:32:46,780
default you have three records of each

00:32:44,440 --> 00:32:49,299
chunk of Peters so storing 64 megabytes

00:32:46,780 --> 00:32:53,200
on if I cross give three times 64

00:32:49,299 --> 00:32:55,529
megabytes and it's aware of the

00:32:53,200 --> 00:32:58,600
infrastructure as I already explained

00:32:55,529 --> 00:33:02,919
each node that holds data is called a

00:32:58,600 --> 00:33:04,840
data node very clever so this is storage

00:33:02,919 --> 00:33:07,090
right you can put this on any computer

00:33:04,840 --> 00:33:11,049
you want and then you have a thing

00:33:07,090 --> 00:33:13,480
called the name node the name node is in

00:33:11,049 --> 00:33:15,580
an HDFS cluster it's a really critical

00:33:13,480 --> 00:33:16,690
component if you need to rely on the

00:33:15,580 --> 00:33:18,190
stuff in production you should have

00:33:16,690 --> 00:33:20,529
failed overs for that it's all possible

00:33:18,190 --> 00:33:24,279
it's just this is kind of this is a bit

00:33:20,529 --> 00:33:26,820
of a pain to keep up and running because

00:33:24,279 --> 00:33:29,580
the problem is that the data node

00:33:26,820 --> 00:33:32,529
essentially the the the data nodes are

00:33:29,580 --> 00:33:34,989
stupid storage they are not even aware

00:33:32,529 --> 00:33:36,460
what files are on them if you want to

00:33:34,989 --> 00:33:38,739
read a file you need to ask the data no

00:33:36,460 --> 00:33:40,690
data node I want to read this file can

00:33:38,739 --> 00:33:42,369
you tell me where it is and it'll say

00:33:40,690 --> 00:33:44,980
it's on that computer offset such and

00:33:42,369 --> 00:33:47,289
such and then you can read it which also

00:33:44,980 --> 00:33:48,730
means that later know the name node

00:33:47,289 --> 00:33:51,340
needs to know about all the all the

00:33:48,730 --> 00:33:54,820
blocks all the files stored in across

00:33:51,340 --> 00:33:57,759
the entire cluster and all I think still

00:33:54,820 --> 00:33:59,919
this the entire file system tree

00:33:57,759 --> 00:34:01,720
essentially so the info bad every fun

00:33:59,919 --> 00:34:04,600
where it's located needs to fit into

00:34:01,720 --> 00:34:07,749
memory so big cluster needs a name node

00:34:04,600 --> 00:34:09,790
with lots of RAM which is another reason

00:34:07,749 --> 00:34:12,040
why you should be as close to the 64

00:34:09,790 --> 00:34:13,899
megabytes as possible because many small

00:34:12,040 --> 00:34:16,270
files mean that the name node will

00:34:13,899 --> 00:34:18,849
quickly fill up with just little junk

00:34:16,270 --> 00:34:20,349
files that it needs to manage whereas if

00:34:18,849 --> 00:34:23,429
you have bigger files because you

00:34:20,349 --> 00:34:25,720
aggregate log data before you write it

00:34:23,429 --> 00:34:29,740
you gets a better bang for the buck

00:34:25,720 --> 00:34:34,540
essentially so when it's time for how do

00:34:29,740 --> 00:34:36,940
to do to process a data job okay how

00:34:34,540 --> 00:34:38,889
does it work pretty much like I already

00:34:36,940 --> 00:34:41,200
described because it's just MapReduce

00:34:38,889 --> 00:34:44,079
right which is awesome the basic rules

00:34:41,200 --> 00:34:46,629
are it uses a so called input format

00:34:44,079 --> 00:34:48,280
which you can configure to split up data

00:34:46,629 --> 00:34:50,889
into single records so you can have like

00:34:48,280 --> 00:34:53,409
a text input format so it knows ok so

00:34:50,889 --> 00:34:56,829
this file has like 10 million new lines

00:34:53,409 --> 00:34:58,809
so I can spread it out across 10

00:34:56,829 --> 00:35:00,730
computers 10 computers each reads a

00:34:58,809 --> 00:35:03,520
million new lines or something so it'll

00:35:00,730 --> 00:35:05,500
not it can even store the file when it

00:35:03,520 --> 00:35:07,900
stores them it can split up the file and

00:35:05,500 --> 00:35:10,559
put parts of the file on different

00:35:07,900 --> 00:35:10,559
places in the class

00:35:10,680 --> 00:35:15,160
you can up there's a way of optimizing

00:35:13,119 --> 00:35:17,800
things what I've explained earlier when

00:35:15,160 --> 00:35:19,809
we when we count when we count the

00:35:17,800 --> 00:35:21,760
number of bytes what we did is IP

00:35:19,809 --> 00:35:23,470
address by its IP address by its IP

00:35:21,760 --> 00:35:26,109
address bites right and then we would

00:35:23,470 --> 00:35:28,809
ship all same I all the identical IP

00:35:26,109 --> 00:35:30,550
addresses to one reducer instance but

00:35:28,809 --> 00:35:32,829
that's actually inefficient because we

00:35:30,550 --> 00:35:35,200
would have 10,000 instances of the same

00:35:32,829 --> 00:35:39,430
IP address and a number of bytes what we

00:35:35,200 --> 00:35:41,140
can do is on a mapper like on one

00:35:39,430 --> 00:35:44,140
computer that there's the mapping for a

00:35:41,140 --> 00:35:47,410
part of the data we can already reduce

00:35:44,140 --> 00:35:49,089
right we can take the number of bytes

00:35:47,410 --> 00:35:53,079
and sum them up and then we to the

00:35:49,089 --> 00:35:54,700
central reducer we only send one one IP

00:35:53,079 --> 00:35:56,470
address instance and the some advice

00:35:54,700 --> 00:35:58,000
course is not possible in all

00:35:56,470 --> 00:36:00,010
circumstances you can do this when you

00:35:58,000 --> 00:36:02,230
want to sum up something but if you want

00:36:00,010 --> 00:36:06,760
to compute an average doing that would

00:36:02,230 --> 00:36:08,500
well ruin with the results obviously you

00:36:06,760 --> 00:36:10,829
can control the partitioning of the map

00:36:08,500 --> 00:36:14,790
yourself so you say okay so the mapper

00:36:10,829 --> 00:36:18,280
gets this in this input and the keys are

00:36:14,790 --> 00:36:20,680
like I have a key tab key tab key tab

00:36:18,280 --> 00:36:22,990
value so the keys consists of three

00:36:20,680 --> 00:36:25,059
elements but only the first two are

00:36:22,990 --> 00:36:26,799
needed to group stuff together etc so

00:36:25,059 --> 00:36:30,450
you can really fine-tune your processing

00:36:26,799 --> 00:36:30,450
job so they get the best performance

00:36:30,720 --> 00:36:34,960
this however is really useful because

00:36:33,040 --> 00:36:37,450
typically it just takes the keys makes a

00:36:34,960 --> 00:36:40,020
hash and decides what reduce it to pass

00:36:37,450 --> 00:36:43,000
it to but sometimes it can make sense

00:36:40,020 --> 00:36:44,950
and there's also built in other config

00:36:43,000 --> 00:36:48,520
options but they don't really they don't

00:36:44,950 --> 00:36:52,359
really matter and if you have ever won

00:36:48,520 --> 00:36:54,730
that we're hard tube got its name it's a

00:36:52,359 --> 00:37:00,369
child word because this guy dark cutting

00:36:54,730 --> 00:37:04,299
who founded the project his his son I

00:37:00,369 --> 00:37:06,190
think had yeah stuffed yellow elephant

00:37:04,299 --> 00:37:10,510
and there's that kid called it hi goob

00:37:06,190 --> 00:37:12,130
that's why hard it was all hard now I

00:37:10,510 --> 00:37:13,510
mentioned earlier that this whole

00:37:12,130 --> 00:37:16,270
streaming stuff you know you're like you

00:37:13,510 --> 00:37:18,130
get a key tab value keytab value another

00:37:16,270 --> 00:37:20,799
key tab value in the reducers so you

00:37:18,130 --> 00:37:22,010
need to our new key and then start the

00:37:20,799 --> 00:37:23,330
reducing all over

00:37:22,010 --> 00:37:25,670
that can be made easier there's a bunch

00:37:23,330 --> 00:37:27,350
of there's actually no toolkits you

00:37:25,670 --> 00:37:31,940
typically write it yourself but I've

00:37:27,350 --> 00:37:33,560
written one so how do PHP who is a

00:37:31,940 --> 00:37:37,850
little framework to help with writing

00:37:33,560 --> 00:37:40,910
MapReduce jobs in PHP and it takes care

00:37:37,850 --> 00:37:43,970
of the input splitting for you it does

00:37:40,910 --> 00:37:47,420
the basic decoding for you and in

00:37:43,970 --> 00:37:49,100
essentially it's um it simply then

00:37:47,420 --> 00:37:51,860
invokes a map and a reduced function in

00:37:49,100 --> 00:37:53,180
a class you define and that's it it

00:37:51,860 --> 00:37:55,790
makes it very convenient the next

00:37:53,180 --> 00:37:58,550
version will also use an iterator rather

00:37:55,790 --> 00:38:00,230
than grouping up all the values for a

00:37:58,550 --> 00:38:01,430
key first because that's much more

00:38:00,230 --> 00:38:04,040
efficient but that's just an

00:38:01,430 --> 00:38:06,170
implementation of detail but the really

00:38:04,040 --> 00:38:09,500
cool thing it does is it packages your

00:38:06,170 --> 00:38:10,910
job as one far file because what you

00:38:09,500 --> 00:38:12,440
need to do is when you have let's say

00:38:10,910 --> 00:38:14,660
you have a hundred PHP classes because

00:38:12,440 --> 00:38:16,480
you have a really complex processing job

00:38:14,660 --> 00:38:18,800
they don't need to tell how the

00:38:16,480 --> 00:38:20,000
streaming and here's the first file you

00:38:18,800 --> 00:38:21,440
need to package and center every

00:38:20,000 --> 00:38:23,780
computer this is the second this is the

00:38:21,440 --> 00:38:25,820
third fourth fifth and so forth and then

00:38:23,780 --> 00:38:29,060
invoke this command which is a complete

00:38:25,820 --> 00:38:32,210
pain so this just makes one little fire

00:38:29,060 --> 00:38:34,210
file that contains the all the code

00:38:32,210 --> 00:38:37,580
needed to do the data processing and

00:38:34,210 --> 00:38:40,700
then also creates a simple file that

00:38:37,580 --> 00:38:45,260
will invoke hardip and ship it out to

00:38:40,700 --> 00:38:46,700
our two different computers so you write

00:38:45,260 --> 00:38:48,500
a map a class you write a reducer class

00:38:46,700 --> 00:38:50,660
maybe you set some options then you run

00:38:48,500 --> 00:38:52,490
this compiler and it'll create a shell

00:38:50,660 --> 00:38:55,840
script for you you run the shell script

00:38:52,490 --> 00:38:59,480
and that just boom starts the processing

00:38:55,840 --> 00:39:06,890
it is as I said written by me I wrote it

00:38:59,480 --> 00:39:09,800
however yesterday and this morning but

00:39:06,890 --> 00:39:11,450
it's uh it's it's still going to be

00:39:09,800 --> 00:39:13,370
awesome it's I mean it's I'm German

00:39:11,450 --> 00:39:16,390
right so you know it won't let you down

00:39:13,370 --> 00:39:19,310
and it'll still work in a thousand years

00:39:16,390 --> 00:39:20,450
unlike a like a PHP like a French

00:39:19,310 --> 00:39:23,170
implementation of this would probably

00:39:20,450 --> 00:39:23,170
surrender

00:39:23,740 --> 00:39:31,550
because it says all too much data so

00:39:28,460 --> 00:39:33,590
this is this is not this doesn't exist

00:39:31,550 --> 00:39:35,720
yet I'll i need to upload this later

00:39:33,590 --> 00:39:39,260
because i was i had problems with the

00:39:35,720 --> 00:39:43,070
stuff this morning i want who I wanted

00:39:39,260 --> 00:39:47,180
to um you know I needed to document

00:39:43,070 --> 00:39:49,430
everything like dutifully every method

00:39:47,180 --> 00:39:51,110
needs a full signature api docs and

00:39:49,430 --> 00:39:53,780
stuff because it's jeremy think it can't

00:39:51,110 --> 00:39:56,930
help it is my OCD so um but I hope this

00:39:53,780 --> 00:39:59,270
should be online this afternoon and with

00:39:56,930 --> 00:40:08,750
this stuff'll is untested code I'm now

00:39:59,270 --> 00:40:11,230
going to do our hands on I have to sync

00:40:08,750 --> 00:40:11,230
the screens

00:40:16,160 --> 00:40:34,190
okay mmm so okay so lets them I'll show

00:40:22,200 --> 00:40:41,790
some basics first ah we have absolutely

00:40:34,190 --> 00:40:43,080
i'll do a general set is this better you

00:40:41,790 --> 00:40:45,660
better have to do it on every file so

00:40:43,080 --> 00:40:49,230
I'm doing a central is this readable for

00:40:45,660 --> 00:40:51,740
everybody or still too small I was

00:40:49,230 --> 00:40:53,880
asking the people in the back Derrick

00:40:51,740 --> 00:40:59,540
can you guys see this at the back or

00:40:53,880 --> 00:40:59,540
should I make bigger yeah no complaints

00:41:05,090 --> 00:41:10,470
so this is like the basic configuration

00:41:07,710 --> 00:41:12,990
that you need for Hadoop arm you simply

00:41:10,470 --> 00:41:15,480
download hardik i typically go to a site

00:41:12,990 --> 00:41:17,280
called cloudera com it'll be on the last

00:41:15,480 --> 00:41:19,920
slide on the in the further reading

00:41:17,280 --> 00:41:22,950
section because they have a distribution

00:41:19,920 --> 00:41:24,720
for how to that you can that simply you

00:41:22,950 --> 00:41:26,790
download it you extract it and you're

00:41:24,720 --> 00:41:28,500
ready to go and they they keep adding

00:41:26,790 --> 00:41:30,150
patches too hard when they make regular

00:41:28,500 --> 00:41:31,740
releases and they have lots of extra

00:41:30,150 --> 00:41:33,450
software like this flume thing for

00:41:31,740 --> 00:41:35,660
aggregating log data and getting data

00:41:33,450 --> 00:41:39,230
into this how to distributed file system

00:41:35,660 --> 00:41:42,090
so you download this then you extracted

00:41:39,230 --> 00:41:44,310
you add this isn't a quick start guide

00:41:42,090 --> 00:41:46,020
so you don't have to write this down or

00:41:44,310 --> 00:41:49,200
something you simply say I can have one

00:41:46,020 --> 00:41:52,620
law jobtracker locally I have one name

00:41:49,200 --> 00:41:54,510
node locally just one replication factor

00:41:52,620 --> 00:41:56,310
one because i have just one node for

00:41:54,510 --> 00:42:01,590
data right so i don't need a number for

00:41:56,310 --> 00:42:03,750
3 and 4 the core system needs to know

00:42:01,590 --> 00:42:05,550
that the file system is located here and

00:42:03,750 --> 00:42:07,110
then you can simply start it i actually

00:42:05,550 --> 00:42:09,840
have have it already running you should

00:42:07,110 --> 00:42:11,730
run this hard up and i think or how to

00:42:09,840 --> 00:42:15,210
start all there's a script and that

00:42:11,730 --> 00:42:16,890
starts a name node a data node a job

00:42:15,210 --> 00:42:18,540
tracker everything so you have a

00:42:16,890 --> 00:42:21,480
descriptive file system running locally

00:42:18,540 --> 00:42:23,640
you have the jobtracker everything runs

00:42:21,480 --> 00:42:25,650
locally it's called the pseudo

00:42:23,640 --> 00:42:26,910
distributed mode it's not actually

00:42:25,650 --> 00:42:29,160
distributed because it is just one

00:42:26,910 --> 00:42:37,230
machine but some

00:42:29,160 --> 00:42:41,220
if yes why it's the default port I'm

00:42:37,230 --> 00:42:45,839
sorry change your party but hard was

00:42:41,220 --> 00:42:49,559
bigger than next oh I don't know how to

00:42:45,839 --> 00:42:52,109
change the setting sir it's good i'm

00:42:49,559 --> 00:42:53,819
running not running XD bug because i

00:42:52,109 --> 00:43:01,740
hate it because i don't like its Dutch

00:42:53,819 --> 00:43:03,420
why would I use it so wha so I'm a shell

00:43:01,740 --> 00:43:07,640
and there's lots of I'll need this

00:43:03,420 --> 00:43:09,809
documentation later because yeah ok so

00:43:07,640 --> 00:43:12,710
to demonstrate that there's something

00:43:09,809 --> 00:43:17,270
running here are we actually this is

00:43:12,710 --> 00:43:22,680
yeah this is as big as I can make it um

00:43:17,270 --> 00:43:28,770
that's what she says sorry I'm sorry so

00:43:22,680 --> 00:43:32,130
um we have hard do something running

00:43:28,770 --> 00:43:34,619
here which doesn't really matter because

00:43:32,130 --> 00:43:37,710
I have a dollar Hadoop home set and if

00:43:34,619 --> 00:43:42,180
we run this um we can run the fastest

00:43:37,710 --> 00:43:45,420
sorry been harder file system list there

00:43:42,180 --> 00:43:47,670
is nothing because I my user doesn't

00:43:45,420 --> 00:43:48,960
exist one thing you need for this for

00:43:47,670 --> 00:43:50,549
this mode you can also have a purely

00:43:48,960 --> 00:43:52,079
local mode but there's not as much fun

00:43:50,549 --> 00:43:53,849
because it doesn't really start like a

00:43:52,079 --> 00:43:56,849
name node and all the stuff all you need

00:43:53,849 --> 00:43:58,859
before this is your local user needs to

00:43:56,849 --> 00:44:01,799
be able to ssh to localhost so you need

00:43:58,859 --> 00:44:05,069
to add your own key to allow what is it

00:44:01,799 --> 00:44:08,240
authorized keys but that's it so here we

00:44:05,069 --> 00:44:08,240
have a test one directory

00:44:12,410 --> 00:44:17,680
and there's an input file we can just

00:44:14,090 --> 00:44:24,590
cat this this heart of home bin hardip

00:44:17,680 --> 00:44:32,510
FS cat test1 I think I called it in put

00:44:24,590 --> 00:44:34,220
up excess log yeah so this is

00:44:32,510 --> 00:44:35,720
essentially one file one big file is

00:44:34,220 --> 00:44:39,850
like five megabytes or something with my

00:44:35,720 --> 00:44:42,110
local my local code as you can see here

00:44:39,850 --> 00:44:42,980
yeah this there's not a lot of stuff in

00:44:42,110 --> 00:44:47,600
there so it's not going to be that

00:44:42,980 --> 00:44:50,060
entertaining to work with but at least

00:44:47,600 --> 00:44:56,920
it works and it has some data we have

00:44:50,060 --> 00:44:59,300
here we've seen this is on 5030 and on

00:44:56,920 --> 00:45:01,400
5070 you'll have web interfaces that

00:44:59,300 --> 00:45:03,020
give you on basic statistics for the

00:45:01,400 --> 00:45:06,020
system that you're running so here we

00:45:03,020 --> 00:45:08,840
have the name node I'm sorry here we

00:45:06,020 --> 00:45:10,250
have the name node if we refresh this we

00:45:08,840 --> 00:45:12,560
can also browse the file system from

00:45:10,250 --> 00:45:14,150
inside the browser here's test one and

00:45:12,560 --> 00:45:20,660
so forth so this is the file system

00:45:14,150 --> 00:45:23,840
right this is the access log we see we

00:45:20,660 --> 00:45:27,830
have 5.5 nine megabytes utilized etc etc

00:45:23,840 --> 00:45:30,290
blah we have just we have just one data

00:45:27,830 --> 00:45:32,780
node live in a bigger cluster would be

00:45:30,290 --> 00:45:34,760
more and this is the MapReduce status

00:45:32,780 --> 00:45:36,590
interface there is no tasks running

00:45:34,760 --> 00:45:41,690
right now which is what we're going to

00:45:36,590 --> 00:45:44,270
change so we'll try and see this wax god

00:45:41,690 --> 00:45:46,160
that went it's so the problem is I've

00:45:44,270 --> 00:45:49,430
written this curve before like I've I've

00:45:46,160 --> 00:45:51,680
given this a similar talk before but it

00:45:49,430 --> 00:45:54,710
wasn't as good like the code that I

00:45:51,680 --> 00:45:57,200
wrote and in the meantime for a bunch of

00:45:54,710 --> 00:45:58,910
clients I also improved like this basic

00:45:57,200 --> 00:46:00,470
framework and then I said hey I should

00:45:58,910 --> 00:46:02,090
really open source this don't you think

00:46:00,470 --> 00:46:04,150
and they said yeah why not and so this

00:46:02,090 --> 00:46:07,640
is what I started doing like yesterday

00:46:04,150 --> 00:46:08,900
but it was more complex than I imagined

00:46:07,640 --> 00:46:10,100
it to be like changing all the file

00:46:08,900 --> 00:46:13,850
names and changing a bunch of things

00:46:10,100 --> 00:46:15,260
around so what we have here I'm sorry

00:46:13,850 --> 00:46:17,660
I'm actually going to use this shell

00:46:15,260 --> 00:46:22,390
because it's already the right directory

00:46:17,660 --> 00:46:24,600
so have OSS and i called it hard to PHP

00:46:22,390 --> 00:46:26,700
why is it not there

00:46:24,600 --> 00:46:30,480
I don't have symphony on my hard drive

00:46:26,700 --> 00:46:34,370
you didn't see that stupid symphony so

00:46:30,480 --> 00:46:38,430
you have an example a hit counter okay

00:46:34,370 --> 00:46:43,620
here's the map of PHP file and here's

00:46:38,430 --> 00:46:45,960
the reducer the mapper simply uses a

00:46:43,620 --> 00:46:49,080
method called parse Apache log line and

00:46:45,960 --> 00:46:51,360
it emits the request-uri and a one

00:46:49,080 --> 00:46:53,160
because it's one hit right we could also

00:46:51,360 --> 00:46:54,750
use the number of bytes here this is

00:46:53,160 --> 00:46:56,430
something that is actually bundled so

00:46:54,750 --> 00:47:02,070
parsing and apache log line is in a you

00:46:56,430 --> 00:47:04,410
till class now we can we can try and run

00:47:02,070 --> 00:47:08,640
this locally if this was in the include

00:47:04,410 --> 00:47:10,130
path but we can simply compile it this

00:47:08,640 --> 00:47:12,750
is the examples directory and in bin

00:47:10,130 --> 00:47:17,280
you'll have a compile PHP so we can

00:47:12,750 --> 00:47:25,710
theoretically run go to the examples hit

00:47:17,280 --> 00:47:30,660
counter run PHP on compile PHP where is

00:47:25,710 --> 00:47:32,600
it so it'll tell us we need to you

00:47:30,660 --> 00:47:35,160
provide the path to the drops folder

00:47:32,600 --> 00:47:37,770
which the map has a map and reduce the

00:47:35,160 --> 00:47:40,770
file and we need to tell it where to

00:47:37,770 --> 00:47:44,850
generate the far and the shell script to

00:47:40,770 --> 00:47:47,040
we can also do dash I and then some sort

00:47:44,850 --> 00:47:49,560
of path and it will import that whole

00:47:47,040 --> 00:47:52,890
tree into your fire file as well so when

00:47:49,560 --> 00:47:55,320
you have like classes with generic code

00:47:52,890 --> 00:47:57,180
that you use as utilities you can say

00:47:55,320 --> 00:47:58,620
include this in this far package I have

00:47:57,180 --> 00:48:00,210
not tested them so I don't know if it

00:47:58,620 --> 00:48:01,740
works and we don't need it but we can

00:48:00,210 --> 00:48:06,410
say because we're in the hit counter

00:48:01,740 --> 00:48:08,820
directory use the local input and add

00:48:06,410 --> 00:48:14,550
generated locally which is the stupid

00:48:08,820 --> 00:48:16,500
ideas i just realized because it'll

00:48:14,550 --> 00:48:18,510
include these files as well so it grows

00:48:16,500 --> 00:48:22,080
bigger and bigger every for a few

00:48:18,510 --> 00:48:23,580
minutes so let's do it here so now we

00:48:22,080 --> 00:48:30,590
have a hit car into the far and a hit

00:48:23,580 --> 00:48:30,590
counter SH change mods

00:48:32,039 --> 00:48:42,269
um the the shell script might actually

00:48:36,670 --> 00:48:42,269
not work because here's the problem um

00:48:43,710 --> 00:48:49,990
if I do this it just gives me a buzzer

00:48:48,099 --> 00:48:51,579
question marks because of the stupid

00:48:49,990 --> 00:48:52,660
detect unicode feature that nobody

00:48:51,579 --> 00:48:55,660
understands Derek would you like to

00:48:52,660 --> 00:49:00,759
comment this happens what I went around

00:48:55,660 --> 00:49:02,529
a far file do you see what happens it

00:49:00,759 --> 00:49:06,730
prints a bunch of Crescent question

00:49:02,529 --> 00:49:11,950
marks what I need to do is minus D check

00:49:06,730 --> 00:49:17,289
unicode off or something hold on yeah it

00:49:11,950 --> 00:49:25,029
does it's 53 free yep version that does

00:49:17,289 --> 00:49:32,249
the job and we and not anymore with

00:49:25,029 --> 00:49:32,249
version 2.6 you can do it without ya I

00:49:33,450 --> 00:49:39,960
know I didn't say to you maybe

00:49:41,339 --> 00:49:48,660
hey so good for you guys apparently that

00:49:46,920 --> 00:49:50,670
was that made people happy over there

00:49:48,660 --> 00:49:53,039
okay so they're cheering so we can even

00:49:50,670 --> 00:50:00,239
try now is I'm simply going to catch a

00:49:53,039 --> 00:50:09,930
VAR log Apache to access log and pipe it

00:50:00,239 --> 00:50:16,170
into this stupid track that oh that's

00:50:09,930 --> 00:50:22,650
not working Oh actually it is yeah so um

00:50:16,170 --> 00:50:24,440
let's see hit counter mapper it's

00:50:22,650 --> 00:50:33,289
supposed to read the request your aye

00:50:24,440 --> 00:50:33,289
but it's not reading let's try this

00:50:47,980 --> 00:50:54,410
yeah that works so there's a bug in the

00:50:50,569 --> 00:51:15,079
script it extracts the wrong thing hold

00:50:54,410 --> 00:51:18,650
on so I what am i oh yes thank you thank

00:51:15,079 --> 00:51:20,900
you awesome this is like it's good that

00:51:18,650 --> 00:51:25,430
you guys haven't fallen asleep good

00:51:20,900 --> 00:51:32,029
great great success so hits counter okay

00:51:25,430 --> 00:51:42,279
and we can recompile this boom and give

00:51:32,029 --> 00:51:42,279
it another shot no why it's not working

00:51:45,249 --> 00:51:57,400
are you gonna believe me when i say

00:51:47,420 --> 00:51:57,400
dwork before thank yeah maybe damn it

00:51:58,480 --> 00:52:03,230
there should be a request your aye so I

00:52:01,460 --> 00:52:08,509
don't know why it doesn't work it might

00:52:03,230 --> 00:52:11,150
be that this is actually oh yeah aria so

00:52:08,509 --> 00:52:12,859
I think this is actually modified to

00:52:11,150 --> 00:52:15,140
read engine X log files which are

00:52:12,859 --> 00:52:17,180
slightly different so it's returning

00:52:15,140 --> 00:52:21,380
false or it's returning null and it

00:52:17,180 --> 00:52:26,599
can't read stuff okay um I don't know

00:52:21,380 --> 00:52:31,839
any to do something else like image yeah

00:52:26,599 --> 00:52:31,839
like between 0 and 1 and can we somehow

00:52:32,890 --> 00:52:54,160
what no and I Yuri compile it yeah that

00:52:42,529 --> 00:52:57,979
sucks because I kind of okay that works

00:52:54,160 --> 00:52:59,089
pretty lame too if I do oh and five is

00:52:57,979 --> 00:53:03,529
it going to have is it going to

00:52:59,089 --> 00:53:09,380
introduce gonna be integers what is it

00:53:03,529 --> 00:53:10,640
saying nothing why do you always sit in

00:53:09,380 --> 00:53:14,239
my talks and not done pay attention

00:53:10,640 --> 00:53:19,119
Derek seriously a funny defensive it's

00:53:14,239 --> 00:53:19,119
boring did you say your mom is boring

00:53:19,390 --> 00:53:26,210
okay so this is working this is working

00:53:23,180 --> 00:53:27,739
so let's try let's try and run it on the

00:53:26,210 --> 00:53:34,069
distributed file system I don't know if

00:53:27,739 --> 00:53:36,619
that works I need to edit this so what

00:53:34,069 --> 00:53:38,029
it it creates this okay so you provided

00:53:36,619 --> 00:53:40,519
with an H of S input directory and an

00:53:38,029 --> 00:53:42,650
output directory it'll also see is there

00:53:40,519 --> 00:53:45,170
a harder poem to find if not it assumes

00:53:42,650 --> 00:53:47,450
like a standard value and you see this

00:53:45,170 --> 00:53:49,549
the map and the reducer actually already

00:53:47,450 --> 00:53:51,890
has the detect unicode off because in

00:53:49,549 --> 00:53:53,809
the directory here i also put a file

00:53:51,890 --> 00:53:55,279
called arguments typically you don't

00:53:53,809 --> 00:53:57,890
need this so if you have a PHP

00:53:55,279 --> 00:53:59,450
installation that works with even though

00:53:57,890 --> 00:54:02,200
the Unicode detective new code is broken

00:53:59,450 --> 00:54:04,609
that would work in my case I have to

00:54:02,200 --> 00:54:07,729
override the mapper and the reducer

00:54:04,609 --> 00:54:09,739
thingies okay I could also here to find

00:54:07,729 --> 00:54:12,920
other Street part of streaming options

00:54:09,739 --> 00:54:14,839
like used don't use tab I'd use a comma

00:54:12,920 --> 00:54:17,660
as a separator or a space character for

00:54:14,839 --> 00:54:20,660
keys and values or the key is actually a

00:54:17,660 --> 00:54:23,390
value to have value and then only the

00:54:20,660 --> 00:54:24,769
value starts after another tab key tap

00:54:23,390 --> 00:54:25,999
value something like this stuff like

00:54:24,769 --> 00:54:30,349
that it's all of the streaming

00:54:25,999 --> 00:54:34,400
documentation so theoretically this

00:54:30,349 --> 00:54:36,170
should work hit counter and our input

00:54:34,400 --> 00:54:39,559
directory was if i remember correctly

00:54:36,170 --> 00:54:40,349
test one input and you want to get have

00:54:39,559 --> 00:54:43,130
it in test

00:54:40,349 --> 00:54:43,130
output

00:54:50,660 --> 00:54:56,660
so it's actually not working on these

00:54:52,340 --> 00:55:00,890
you can render numbers now but oh and in

00:54:56,660 --> 00:55:04,960
the meantime we could boom here or the

00:55:00,890 --> 00:55:07,430
map has complete but it's reducing still

00:55:04,960 --> 00:55:08,750
so i'm only running one map instance in

00:55:07,430 --> 00:55:10,010
one reduce instances so this is not

00:55:08,750 --> 00:55:12,680
technically distributed the data

00:55:10,010 --> 00:55:23,270
processing with PHP I'm sorry so I was

00:55:12,680 --> 00:55:28,640
lying to you this okay so mm hardwood FS

00:55:23,270 --> 00:55:43,250
list in test two oopsie did I we need

00:55:28,640 --> 00:55:49,580
test two yep and there's a part 000 and

00:55:43,250 --> 00:55:53,990
we can count this damn it okay one last

00:55:49,580 --> 00:55:55,790
straw here what we can do this is

00:55:53,990 --> 00:55:57,830
actually pretty cool instead of having a

00:55:55,790 --> 00:56:00,800
reducer that is written in PHP we can

00:55:57,830 --> 00:56:03,800
use a native hard oop reducer and what

00:56:00,800 --> 00:56:10,420
we do is we cheat we say make a long

00:56:03,800 --> 00:56:17,720
value some and in arguments we say

00:56:10,420 --> 00:56:22,250
reducer aggregate and that's it so I'm a

00:56:17,720 --> 00:56:24,080
trinary compile this there must be

00:56:22,250 --> 00:56:28,640
something wrong with my Medusa for some

00:56:24,080 --> 00:56:30,910
reason harder poem bin need to compile

00:56:28,640 --> 00:56:30,910
it

00:56:34,770 --> 00:56:42,340
there's ton of executable Commission's

00:56:36,880 --> 00:56:45,310
it so in this case we can do test one

00:56:42,340 --> 00:56:47,440
output this will work in the meantime I

00:56:45,310 --> 00:56:49,390
can figure out what it's wrong I realize

00:56:47,440 --> 00:56:51,790
we're running over time I know I'll be

00:56:49,390 --> 00:56:53,380
right down return an Aries summer values

00:56:51,790 --> 00:56:59,820
maybe I have to admit the area some of

00:56:53,380 --> 00:56:59,820
values let's see

00:57:03,140 --> 00:57:24,089
handle yep I have to do all of this

00:57:09,750 --> 00:57:30,510
emits that'll fix it but maybe please

00:57:24,089 --> 00:57:35,160
work yeah awesome so it's not really

00:57:30,510 --> 00:57:38,190
random there is quite the yeah okay so

00:57:35,160 --> 00:57:42,240
um whilst I do this is there any

00:57:38,190 --> 00:57:44,970
questions right now yes please all the

00:57:42,240 --> 00:57:47,280
examples you mentioned are basically

00:57:44,970 --> 00:57:49,410
about processing data that's text data

00:57:47,280 --> 00:57:51,299
that's on HDFS does that make sense to

00:57:49,410 --> 00:57:53,039
use MapReduce like say I have lots of

00:57:51,299 --> 00:57:55,260
data in a central database and use my

00:57:53,039 --> 00:57:58,980
parts to connect to that database and no

00:57:55,260 --> 00:58:00,960
big because then what you can there's

00:57:58,980 --> 00:58:03,329
nothing stopping you from like in the

00:58:00,960 --> 00:58:05,250
mapper process opening a like a somehow

00:58:03,329 --> 00:58:07,020
our connection to the my skill database

00:58:05,250 --> 00:58:10,529
but the problem is that you have then a

00:58:07,020 --> 00:58:13,559
hundred nodes accessing this data and

00:58:10,529 --> 00:58:15,029
your your IO bound again what you do in

00:58:13,559 --> 00:58:16,650
this case is you use a utility that

00:58:15,029 --> 00:58:19,260
these Cloudera guys there's going to be

00:58:16,650 --> 00:58:21,599
a link in the next slide road is called

00:58:19,260 --> 00:58:24,660
scoop and it allows you to extract a

00:58:21,599 --> 00:58:28,319
table or a part of a table and put it

00:58:24,660 --> 00:58:30,240
into HDFS as text files like with tab

00:58:28,319 --> 00:58:32,130
separation or even as a hive table and

00:58:30,240 --> 00:58:33,839
then you can run queries on that or

00:58:32,130 --> 00:58:38,250
MapReduce jobs on that so what I've done

00:58:33,839 --> 00:58:41,220
mostly for my clients is written basic

00:58:38,250 --> 00:58:43,349
map reduce tasks that use either one

00:58:41,220 --> 00:58:45,089
thing is log files typically from web

00:58:43,349 --> 00:58:46,920
service and the other thing is stuff

00:58:45,089 --> 00:58:48,720
that an application logged which for

00:58:46,920 --> 00:58:52,500
which I use JSON so you'd simply log a

00:58:48,720 --> 00:58:54,390
line of JSON to a file and using this

00:58:52,500 --> 00:58:56,099
cloudera flume thing that then buffer

00:58:54,390 --> 00:58:57,690
stuff up until you hit a few dozen

00:58:56,099 --> 00:59:00,089
megabytes and then it writes it to earn

00:58:57,690 --> 00:59:02,730
to a distributed file system and then

00:59:00,089 --> 00:59:04,650
each map just takes the input drums JSON

00:59:02,730 --> 00:59:06,420
encode over it figures out a so this is

00:59:04,650 --> 00:59:09,089
this kind of event this is that kind of

00:59:06,420 --> 00:59:10,710
event and then acts accordingly because

00:59:09,089 --> 00:59:12,089
you don't know what it's in the log file

00:59:10,710 --> 00:59:15,859
right is it could be random log files

00:59:12,089 --> 00:59:18,200
and then basic processing jobs

00:59:15,859 --> 00:59:20,960
take these take this JSON stuff and

00:59:18,200 --> 00:59:23,210
produce intermediary records with this

00:59:20,960 --> 00:59:25,579
so maybe they just transform the JSON to

00:59:23,210 --> 00:59:26,900
something like keytab value or maybe

00:59:25,579 --> 00:59:29,960
they do some sort of aggregation and

00:59:26,900 --> 00:59:31,730
then this intermediary result is stored

00:59:29,960 --> 00:59:34,970
locally because it may be that happens

00:59:31,730 --> 00:59:37,279
once a day and then you can load a whole

00:59:34,970 --> 00:59:40,519
months of data which is already pre

00:59:37,279 --> 00:59:43,130
processed into hive and just run a

00:59:40,519 --> 00:59:46,640
simple select query do legal like select

00:59:43,130 --> 00:59:49,009
from all the month tables and group by

00:59:46,640 --> 00:59:50,569
day and count the number of hits and

00:59:49,009 --> 00:59:51,799
then you see like the number of hits and

00:59:50,569 --> 00:59:53,599
then you can load that into Excel and

00:59:51,799 --> 00:59:55,460
make a nice pie chart you can even to

00:59:53,599 --> 00:59:56,960
hive even has a jdbc in an ODBC

00:59:55,460 --> 00:59:59,359
interface so you can use like crystal

00:59:56,960 --> 01:00:01,489
reports of other reporting software and

00:59:59,359 --> 01:00:04,099
connect to hive and run sequel queries

01:00:01,489 --> 01:00:05,779
on this thing it'll take a few minutes a

01:00:04,099 --> 01:00:10,190
few hours but it'll eventually get back

01:00:05,779 --> 01:00:11,599
to you so it was pretty cool yes other

01:00:10,190 --> 01:00:17,239
questions this is this is answer your

01:00:11,599 --> 01:00:19,460
question yes I think I've you'll have to

01:00:17,239 --> 01:00:21,589
put the other questions offline and just

01:00:19,460 --> 01:00:26,029
okay grab yourself yeah okay Cheers

01:00:21,589 --> 01:00:28,489
that's fine um let me finish the

01:00:26,029 --> 01:00:33,980
presentation slides there's no more

01:00:28,489 --> 01:00:36,170
olive oil stuff Oh hairy little tour

01:00:33,980 --> 01:00:38,420
Apache Avro I've mentioned is a date of

01:00:36,170 --> 01:00:40,190
civilization format in the streaming

01:00:38,420 --> 01:00:41,779
stuff and like actual implementation

01:00:40,190 --> 01:00:43,160
little stuff doesn't use that it's just

01:00:41,779 --> 01:00:46,220
used something that they use a lot

01:00:43,160 --> 01:00:48,319
cloudera flume is this log aggregation

01:00:46,220 --> 01:00:50,989
software so if you run scribe get rid of

01:00:48,319 --> 01:00:52,910
scribe install flume you can say on each

01:00:50,989 --> 01:00:54,650
server there's a local agent that buffer

01:00:52,910 --> 01:00:57,049
stuff and occasionally sense to a

01:00:54,650 --> 01:00:58,789
collector tier which then sends it

01:00:57,049 --> 01:01:00,440
further down further and further down so

01:00:58,789 --> 01:01:02,690
it really scares really well you can say

01:01:00,440 --> 01:01:05,150
this log message must be must be written

01:01:02,690 --> 01:01:07,609
to the file system to like 10 computers

01:01:05,150 --> 01:01:09,079
away before it returns or you can do it

01:01:07,609 --> 01:01:10,910
completely as synchronously like it as a

01:01:09,079 --> 01:01:13,309
best-effort thing it's awesome it's

01:01:10,910 --> 01:01:15,799
super scalable it work it's extreme is

01:01:13,309 --> 01:01:17,720
rock-solid that's great HBase is kind of

01:01:15,799 --> 01:01:19,730
a maybe you've heard of Google's big

01:01:17,720 --> 01:01:21,049
table it's a big table implementation so

01:01:19,730 --> 01:01:23,660
if you want to analyze the data that way

01:01:21,049 --> 01:01:25,640
you can HDFS is distributed file system

01:01:23,660 --> 01:01:27,739
which is you could use it for other

01:01:25,640 --> 01:01:29,740
purposes storing images in a distributed

01:01:27,739 --> 01:01:32,820
fashion high f is an SQL

01:01:29,740 --> 01:01:36,340
face pig is this processing language

01:01:32,820 --> 01:01:39,340
zookeeper organizes your distributed

01:01:36,340 --> 01:01:42,460
applications if you want so this is

01:01:39,340 --> 01:01:44,380
projects that parts of MapReduce users

01:01:42,460 --> 01:01:46,150
are erhard abusers sometimes it does

01:01:44,380 --> 01:01:48,130
sometimes doesn't but it all fits into

01:01:46,150 --> 01:01:49,510
this parallel processing stuff so

01:01:48,130 --> 01:01:51,910
there's lots of software to leverage

01:01:49,510 --> 01:01:54,940
from and and use from this ecosystem

01:01:51,910 --> 01:01:56,800
that would be the end this is a bit of

01:01:54,940 --> 01:01:58,780
further reading this is a really good

01:01:56,800 --> 01:02:01,360
tutorial the cloud era guys have this

01:01:58,780 --> 01:02:03,340
distribution for Hadoop is really easy

01:02:01,360 --> 01:02:07,590
to install it comes with hard with hive

01:02:03,340 --> 01:02:07,590

YouTube URL: https://www.youtube.com/watch?v=elOaZdQCWEY


