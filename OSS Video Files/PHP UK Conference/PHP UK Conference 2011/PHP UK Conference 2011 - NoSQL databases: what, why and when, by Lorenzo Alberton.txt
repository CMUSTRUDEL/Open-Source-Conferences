Title: PHP UK Conference 2011 - NoSQL databases: what, why and when, by Lorenzo Alberton
Publication date: 2012-03-15
Playlist: PHP UK Conference 2011
Description: 
	
Captions: 
	00:00:04,850 --> 00:00:14,070
can you all hear me yeah so let's start

00:00:09,870 --> 00:00:16,189
with this strange topic I bet many of

00:00:14,070 --> 00:00:20,040
you heard the term unless you have been

00:00:16,189 --> 00:00:22,230
hiding behind a rock and you must have

00:00:20,040 --> 00:00:25,590
noticed how nice field of daisies now

00:00:22,230 --> 00:00:28,829
are taking over the entire world more or

00:00:25,590 --> 00:00:32,780
less and you might wonder why this is

00:00:28,829 --> 00:00:37,199
happening why are they a better solution

00:00:32,780 --> 00:00:40,379
versus traditional that ways so listen

00:00:37,199 --> 00:00:44,870
let's first see some trends that justify

00:00:40,379 --> 00:00:49,260
their usage in the latest years we've

00:00:44,870 --> 00:00:54,059
experienced a huge increase in big data

00:00:49,260 --> 00:00:58,550
in data log load so we happen to get

00:00:54,059 --> 00:01:02,309
process and store a huge amount of data

00:00:58,550 --> 00:01:04,530
then we also have to serve a lot of data

00:01:02,309 --> 00:01:07,740
concurrently so this is something that

00:01:04,530 --> 00:01:10,410
didn't happen a few years ago and data

00:01:07,740 --> 00:01:14,010
is more and more connected it's not just

00:01:10,410 --> 00:01:18,320
that it's not just a hyperlink but we've

00:01:14,010 --> 00:01:22,800
been moving slowly towards reading real

00:01:18,320 --> 00:01:27,810
structure first links then we have weird

00:01:22,800 --> 00:01:30,480
RSS wikis tags folksonomies ontologies

00:01:27,810 --> 00:01:33,810
and finally we also have graphs which

00:01:30,480 --> 00:01:37,830
are the ultimate solution when you have

00:01:33,810 --> 00:01:40,980
complicated and complex relationships we

00:01:37,830 --> 00:01:43,440
also ceased it to a huge diversity in

00:01:40,980 --> 00:01:47,040
the applications they are not all the

00:01:43,440 --> 00:01:49,590
same anymore any longer so a single

00:01:47,040 --> 00:01:54,390
standard solution might not fit all

00:01:49,590 --> 00:01:57,030
these problems and finally knowledge

00:01:54,390 --> 00:02:01,590
coming from a peer-to-peer systems and

00:01:57,030 --> 00:02:04,020
cloud computing also started a movement

00:02:01,590 --> 00:02:06,900
to reconsider the relational database

00:02:04,020 --> 00:02:09,569
the concept behind a power relationship

00:02:06,900 --> 00:02:14,490
that leaves and they're trying to apply

00:02:09,569 --> 00:02:17,550
them to a new britain so first of all

00:02:14,490 --> 00:02:21,720
why is a religion that base

00:02:17,550 --> 00:02:24,500
not suitable and if any longer first of

00:02:21,720 --> 00:02:26,910
all we all know that whilst of the

00:02:24,500 --> 00:02:30,840
transaction volume and the data volume

00:02:26,910 --> 00:02:34,400
girl linearly that the latency and the

00:02:30,840 --> 00:02:38,550
response time don't grow exponentially

00:02:34,400 --> 00:02:40,860
so now change that and consider that

00:02:38,550 --> 00:02:43,710
volume and data growth usually grow

00:02:40,860 --> 00:02:48,360
exponentially imagine what happens to

00:02:43,710 --> 00:02:50,250
wrestle times and and latency it's just

00:02:48,360 --> 00:02:53,220
not manageable so what we started doing

00:02:50,250 --> 00:02:54,900
is we started putting a lot of cash is

00:02:53,220 --> 00:02:58,500
like my gosh the in front of our

00:02:54,900 --> 00:03:00,660
databases we started considering

00:02:58,500 --> 00:03:04,590
solutions like master slave or master

00:03:00,660 --> 00:03:07,470
master even clustering and that's not

00:03:04,590 --> 00:03:10,890
his easy to handle right and sometimes

00:03:07,470 --> 00:03:14,370
it's fine for translate transactional

00:03:10,890 --> 00:03:16,770
load but not so fine for data growth so

00:03:14,370 --> 00:03:20,250
at one point data doesn't fit one

00:03:16,770 --> 00:03:22,940
machine any longer so you start looking

00:03:20,250 --> 00:03:24,959
into federated tables charting or a

00:03:22,940 --> 00:03:27,420
full-fledged distributed that

00:03:24,959 --> 00:03:30,420
distributed database which is available

00:03:27,420 --> 00:03:33,840
but is usually very expensive and not

00:03:30,420 --> 00:03:37,050
really easy to handle so the relational

00:03:33,840 --> 00:03:41,910
database was fine until now but it's

00:03:37,050 --> 00:03:44,130
starting to show some weaknesses and the

00:03:41,910 --> 00:03:48,420
complexity that that is put onto

00:03:44,130 --> 00:03:51,060
developer is way too much now so as I

00:03:48,420 --> 00:03:53,580
said we are assisting to an explosion of

00:03:51,060 --> 00:03:55,410
different search solutions that depart

00:03:53,580 --> 00:03:59,040
quite happily from the traditional

00:03:55,410 --> 00:04:02,370
arguments i'm going to show you the a

00:03:59,040 --> 00:04:05,430
very rough overview of what we can

00:04:02,370 --> 00:04:11,100
gather from the internet by reading

00:04:05,430 --> 00:04:12,930
blogs and seen a boss so as for you are

00:04:11,100 --> 00:04:17,459
all familiar with this video is very

00:04:12,930 --> 00:04:20,790
funny and the it was made because

00:04:17,459 --> 00:04:22,350
sometimes some products show numbers

00:04:20,790 --> 00:04:25,810
that are probably too good to be true

00:04:22,350 --> 00:04:28,030
and they look impressive but you

00:04:25,810 --> 00:04:31,270
under what conditions they were taken

00:04:28,030 --> 00:04:34,060
and what compromise is the they made to

00:04:31,270 --> 00:04:37,720
original those numbers or sometimes we

00:04:34,060 --> 00:04:39,280
read about real use cases sign companies

00:04:37,720 --> 00:04:42,010
starting for the first time to

00:04:39,280 --> 00:04:45,639
experiment with a new product new SQL

00:04:42,010 --> 00:04:49,300
database and the outcome is not always

00:04:45,639 --> 00:04:52,450
stellar so if the implementation phase

00:04:49,300 --> 00:04:54,280
they usually blame the product but again

00:04:52,450 --> 00:04:56,980
it's not real clear if it's the product

00:04:54,280 --> 00:04:59,410
to be flawed or rather their

00:04:56,980 --> 00:05:04,030
implementation of the product within the

00:04:59,410 --> 00:05:06,280
architecture and finally we tend to

00:05:04,030 --> 00:05:10,360
believe to blindly believed to be high

00:05:06,280 --> 00:05:13,690
and we time to consider those bucks as

00:05:10,360 --> 00:05:16,600
solid amateur and it's not always the

00:05:13,690 --> 00:05:20,350
case but this stock doesn't want to be a

00:05:16,600 --> 00:05:23,200
pointless comparison all speed between

00:05:20,350 --> 00:05:25,120
different non-school databases you can

00:05:23,200 --> 00:05:29,220
find them all over the Internet they are

00:05:25,120 --> 00:05:29,220
all rubbish pizza they are not

00:05:30,660 --> 00:05:36,760
expressing any bring your use case they

00:05:32,979 --> 00:05:39,210
are just random numbers instead I want

00:05:36,760 --> 00:05:43,900
to take a step back and introduce you

00:05:39,210 --> 00:05:47,169
feel solid principle principles of

00:05:43,900 --> 00:05:49,680
distributed databases and I'm going to

00:05:47,169 --> 00:05:52,570
do these by taking a step back and

00:05:49,680 --> 00:05:56,889
introduce to some relation that based on

00:05:52,570 --> 00:05:59,470
sex pert so first of all we know that a

00:05:56,889 --> 00:06:02,770
relational database is acid compliant

00:05:59,470 --> 00:06:07,090
what does exit me basically they

00:06:02,770 --> 00:06:10,080
guarantee that whatever you do is done

00:06:07,090 --> 00:06:13,150
either in its entirety or not at all

00:06:10,080 --> 00:06:16,180
it's consistent that I that is the

00:06:13,150 --> 00:06:20,020
database is never in an inconsistent

00:06:16,180 --> 00:06:22,630
state in at any given moment so it never

00:06:20,020 --> 00:06:25,930
breaks relational constraints or nothing

00:06:22,630 --> 00:06:28,960
like that it's always consistent in

00:06:25,930 --> 00:06:32,740
guarantee is that multiple concurrent

00:06:28,960 --> 00:06:35,910
transactions don't affect each other so

00:06:32,740 --> 00:06:39,060
if you are running transaction

00:06:35,910 --> 00:06:41,400
the partially modified data which is

00:06:39,060 --> 00:06:43,440
hasn't been committed yet will not be

00:06:41,400 --> 00:06:46,470
seen by other transactions at the same

00:06:43,440 --> 00:06:49,170
time and finally database usually

00:06:46,470 --> 00:06:52,470
guarantee is that once the data has been

00:06:49,170 --> 00:06:57,210
committed it's there forever or in case

00:06:52,470 --> 00:07:00,870
of a crusher or disaster you can reply

00:06:57,210 --> 00:07:05,130
reply the redo log and recover all the

00:07:00,870 --> 00:07:08,280
data without losing anything what we

00:07:05,130 --> 00:07:12,590
don't usually pay much attention to is

00:07:08,280 --> 00:07:17,280
that this is not an absolute concept so

00:07:12,590 --> 00:07:21,600
the acid properties are always usually

00:07:17,280 --> 00:07:24,270
tunable starting from an isolation we

00:07:21,600 --> 00:07:29,550
can have different levels of isolation

00:07:24,270 --> 00:07:34,790
and strange things can happen if we

00:07:29,550 --> 00:07:39,120
loosen up the constraints of it so we

00:07:34,790 --> 00:07:43,380
can add stuff like contemplates we can

00:07:39,120 --> 00:07:46,980
read dirty date data if we still if

00:07:43,380 --> 00:07:48,660
you'll allow data partially being

00:07:46,980 --> 00:07:52,980
processed by one transition to be seen

00:07:48,660 --> 00:07:57,510
by another transaction and in order to

00:07:52,980 --> 00:07:59,300
avoid all these those pitfalls usually

00:07:57,510 --> 00:08:02,340
you have to lock a lot of stuff

00:07:59,300 --> 00:08:06,419
depending on how much you lock either a

00:08:02,340 --> 00:08:09,810
row or a page range or the entire table

00:08:06,419 --> 00:08:14,390
you can guarantee those properties to be

00:08:09,810 --> 00:08:17,040
more straight of course this is good for

00:08:14,390 --> 00:08:22,380
consistency but it's not very good for

00:08:17,040 --> 00:08:26,040
performance so what usually the database

00:08:22,380 --> 00:08:28,910
people do is to use a new party

00:08:26,040 --> 00:08:32,330
hardening dick instead of locking stuff

00:08:28,910 --> 00:08:35,160
they invent this multi version

00:08:32,330 --> 00:08:39,419
concurrency control system which is

00:08:35,160 --> 00:08:41,789
method that the database provide in

00:08:39,419 --> 00:08:45,060
order to allow concurrent access to the

00:08:41,789 --> 00:08:47,900
database so what happens is when you

00:08:45,060 --> 00:08:49,710
want to modify some data in the database

00:08:47,900 --> 00:08:51,630
basically you have

00:08:49,710 --> 00:08:53,340
nutshot of the database your bit of the

00:08:51,630 --> 00:08:58,800
part of the database if you are

00:08:53,340 --> 00:09:01,410
modifying allow reads on the older

00:08:58,800 --> 00:09:04,890
version and once you're ready you'll

00:09:01,410 --> 00:09:09,060
simply switch the pointer from the old

00:09:04,890 --> 00:09:12,960
one to the new branch and this is nice

00:09:09,060 --> 00:09:15,210
because it's a you can write in one

00:09:12,960 --> 00:09:19,350
transaction I leave all the other green

00:09:15,210 --> 00:09:22,050
transactions not a lot the drawback of

00:09:19,350 --> 00:09:24,980
this is usually you have to do some sort

00:09:22,050 --> 00:09:30,270
of compaction if you've been to the

00:09:24,980 --> 00:09:33,420
innodb talk you know now that sense that

00:09:30,270 --> 00:09:36,180
the bases are quite smart in the way the

00:09:33,420 --> 00:09:41,400
lease is done so it's not always a

00:09:36,180 --> 00:09:45,720
severe problem so this is fine for one

00:09:41,400 --> 00:09:47,520
single node it's quite easy it's not

00:09:45,720 --> 00:09:49,830
easy but it's relatively easy to

00:09:47,520 --> 00:09:52,970
maintain I see the compliance on one

00:09:49,830 --> 00:09:57,930
node what is you start you need to grow

00:09:52,970 --> 00:10:01,440
above one node you still can maintain

00:09:57,930 --> 00:10:04,140
full AC compliance but there are many

00:10:01,440 --> 00:10:07,410
things you have to consider and latency

00:10:04,140 --> 00:10:11,250
starts really growing a lot so there are

00:10:07,410 --> 00:10:14,220
some protocols to guarantee consistency

00:10:11,250 --> 00:10:17,160
and and they are usually under the

00:10:14,220 --> 00:10:20,280
umbrella of the two faced with me terms

00:10:17,160 --> 00:10:24,300
of transaction protocol and what they

00:10:20,280 --> 00:10:28,560
usually do is they have to face approach

00:10:24,300 --> 00:10:33,120
first the coordinator will instruct all

00:10:28,560 --> 00:10:36,150
the participants to apply to to run our

00:10:33,120 --> 00:10:40,290
transaction but not committed and only

00:10:36,150 --> 00:10:43,410
if all of them agree on the outcome then

00:10:40,290 --> 00:10:46,020
the commit action is actually propagated

00:10:43,410 --> 00:10:50,300
and done until that point it's only

00:10:46,020 --> 00:10:54,830
written in the riddle riddle riddle lock

00:10:50,300 --> 00:10:58,310
this is working both ways both for

00:10:54,830 --> 00:11:03,560
success a successful transaction and for

00:10:58,310 --> 00:11:06,680
because when a rod Beck is you but it

00:11:03,560 --> 00:11:08,870
has many problems first of all if one

00:11:06,680 --> 00:11:12,170
node is down especially the coordinator

00:11:08,870 --> 00:11:17,180
erased locking the entire base the

00:11:12,170 --> 00:11:20,690
entire cluster in best case you can be a

00:11:17,180 --> 00:11:24,860
very conservative and if the nodes don't

00:11:20,690 --> 00:11:27,100
respond within a certain timeframe then

00:11:24,860 --> 00:11:31,250
there is a timeout which rolls back the

00:11:27,100 --> 00:11:34,040
transaction on all the nodes and it's

00:11:31,250 --> 00:11:38,540
very slow in any case there are some

00:11:34,040 --> 00:11:43,850
algorithms that allow for better wave

00:11:38,540 --> 00:11:47,660
handling some sort of tolerance 22 notes

00:11:43,850 --> 00:11:50,600
being down and they are usually under

00:11:47,660 --> 00:11:53,360
the family of the passes algorithm I

00:11:50,600 --> 00:11:55,880
will not explain it its bit too complex

00:11:53,360 --> 00:11:57,529
to be explained now but you can read it

00:11:55,880 --> 00:11:59,930
read up on Wikipedia there are many

00:11:57,529 --> 00:12:03,580
papers the idea is it's based on our

00:11:59,930 --> 00:12:09,830
consensus rather than an absolute agreed

00:12:03,580 --> 00:12:12,470
and is more tolerant to failures so we I

00:12:09,830 --> 00:12:17,390
hope we all agree that basic properties

00:12:12,470 --> 00:12:19,940
are desirable all the time but nowadays

00:12:17,390 --> 00:12:23,270
we also have some more pressing things

00:12:19,940 --> 00:12:27,020
to consider so availability is probably

00:12:23,270 --> 00:12:30,500
starting to be a better value to have

00:12:27,020 --> 00:12:33,260
rather than absolute consistency so if

00:12:30,500 --> 00:12:35,390
until the 90s the approach was it's

00:12:33,260 --> 00:12:38,000
better to have the system down than to

00:12:35,390 --> 00:12:40,820
have it in an inconsistent state now we

00:12:38,000 --> 00:12:45,800
are started we consider this this

00:12:40,820 --> 00:12:49,550
approach the question here to ask is

00:12:45,800 --> 00:12:53,240
what should you trade a higher

00:12:49,550 --> 00:12:55,430
availability against what should you can

00:12:53,240 --> 00:12:57,950
you give up even just a little bit of it

00:12:55,430 --> 00:13:05,180
in order to get a higher availability

00:12:57,950 --> 00:13:09,020
and the the nosc your movement we can we

00:13:05,180 --> 00:13:10,480
can say started from a conjecture the

00:13:09,020 --> 00:13:13,880
professor brewer

00:13:10,480 --> 00:13:18,040
in 2000 year year 2000 and formalized

00:13:13,880 --> 00:13:22,340
into a form of erm two years later and

00:13:18,040 --> 00:13:25,240
what he says is that at any given time

00:13:22,340 --> 00:13:29,420
you can only guarantee out of

00:13:25,240 --> 00:13:31,670
consistency from network partitions and

00:13:29,420 --> 00:13:34,160
availability you can only guarantee to

00:13:31,670 --> 00:13:37,160
it doesn't mean you only you can only

00:13:34,160 --> 00:13:40,279
guarantee two of the auditory it means

00:13:37,160 --> 00:13:44,120
at any given time you can only go into

00:13:40,279 --> 00:13:45,710
two of those of course the goal is

00:13:44,120 --> 00:13:50,120
always to guarantee all three of them

00:13:45,710 --> 00:13:54,140
okay so let's have a look at what those

00:13:50,120 --> 00:13:56,890
three things mean I'm gonna talk about

00:13:54,140 --> 00:13:59,210
partitioning tolerance and availability

00:13:56,890 --> 00:14:03,730
together because they are more or less

00:13:59,210 --> 00:14:06,650
the two sides of the same coin they are

00:14:03,730 --> 00:14:09,290
different in a way but the result is

00:14:06,650 --> 00:14:11,960
quite similar so if you want to read

00:14:09,290 --> 00:14:14,600
more about the legal distinctions you

00:14:11,960 --> 00:14:18,050
can read in those blog posts which are

00:14:14,600 --> 00:14:22,190
really great but ideally this is what

00:14:18,050 --> 00:14:24,620
they mean in your network your cluster

00:14:22,190 --> 00:14:27,050
at any given time there could be a

00:14:24,620 --> 00:14:31,310
network partition so one board part of

00:14:27,050 --> 00:14:33,500
the nose can't can see the other part so

00:14:31,310 --> 00:14:37,360
this happens all the time it's quite

00:14:33,500 --> 00:14:41,270
common in the highly distributed systems

00:14:37,360 --> 00:14:45,080
and so what you usually have given

00:14:41,270 --> 00:14:50,000
Network partitions are given they can't

00:14:45,080 --> 00:14:52,220
you can't really do the assumption that

00:14:50,000 --> 00:14:55,370
they don't have and because they do you

00:14:52,220 --> 00:14:57,580
can choose whether you want to have a

00:14:55,370 --> 00:15:01,510
privilege on consistency over

00:14:57,580 --> 00:15:05,810
availability or the other way around

00:15:01,510 --> 00:15:08,150
just not usually a highlight latency is

00:15:05,810 --> 00:15:11,300
considered to be a network partition as

00:15:08,150 --> 00:15:12,350
well okay it's like de node is not

00:15:11,300 --> 00:15:18,920
available to rest

00:15:12,350 --> 00:15:23,780
season again we said we we have to do

00:15:18,920 --> 00:15:27,410
some compromises in order to get the

00:15:23,780 --> 00:15:29,450
higher availability and the compromise

00:15:27,410 --> 00:15:32,780
doesn't need to be black and white we

00:15:29,450 --> 00:15:34,850
can adjust the level of resistance we

00:15:32,780 --> 00:15:37,580
want we can add either strong

00:15:34,850 --> 00:15:39,920
consistency which is as in the

00:15:37,580 --> 00:15:43,190
relational database model or we can add

00:15:39,920 --> 00:15:48,800
up the flora less strong or weak

00:15:43,190 --> 00:15:51,680
consistently these will consent a small

00:15:48,800 --> 00:15:54,680
window or inconsistency in the system

00:15:51,680 --> 00:15:56,870
and there is not just one level you can

00:15:54,680 --> 00:16:00,050
have in many different levels of

00:15:56,870 --> 00:16:02,930
inconsistencies the there is you should

00:16:00,050 --> 00:16:05,690
do is to guarantee that when someone

00:16:02,930 --> 00:16:08,030
writes something that person is able to

00:16:05,690 --> 00:16:10,370
read it same data but not a different

00:16:08,030 --> 00:16:13,220
person not a previous version so that's

00:16:10,370 --> 00:16:16,810
the minimum expectation but you can do

00:16:13,220 --> 00:16:21,290
better even in an eventual consistency

00:16:16,810 --> 00:16:24,200
state potato on the server side you can

00:16:21,290 --> 00:16:28,400
also tune the consistency thanks to

00:16:24,200 --> 00:16:31,430
three magic numbers those r and w and

00:16:28,400 --> 00:16:34,970
our and represents the number of

00:16:31,430 --> 00:16:38,090
replicas that pulled pork for your data

00:16:34,970 --> 00:16:42,020
so let's suppose your data is replicated

00:16:38,090 --> 00:16:45,350
on three different nodes so n is equal

00:16:42,020 --> 00:16:49,150
to three in the case and then you add W

00:16:45,350 --> 00:16:54,500
and our W is the minimum number of nodes

00:16:49,150 --> 00:16:58,220
that can have to agree to a response in

00:16:54,500 --> 00:17:01,540
order for the way to be rightly access

00:16:58,220 --> 00:17:06,560
successful and r is the other thing so

00:17:01,540 --> 00:17:09,800
you have to at least our nose must agree

00:17:06,560 --> 00:17:14,540
on the value of the node for it will be

00:17:09,800 --> 00:17:18,560
returned successfully if r & w overlap

00:17:14,540 --> 00:17:20,540
if the sound is bigger than n then you

00:17:18,560 --> 00:17:24,250
have a strong consistency basically you

00:17:20,540 --> 00:17:27,310
have a quorum so if the majority of the

00:17:24,250 --> 00:17:31,660
knows respond with the same answer then

00:17:27,310 --> 00:17:33,910
that one must be true answer okay it's

00:17:31,660 --> 00:17:39,100
like voting it's a poor the majority

00:17:33,910 --> 00:17:43,530
wins both in when writing and reading

00:17:39,100 --> 00:17:46,660
okay if those two sets don't overlap

00:17:43,530 --> 00:17:49,570
then you only have weak consistency

00:17:46,660 --> 00:17:54,130
which is acceptable in some cases but

00:17:49,570 --> 00:17:58,000
not always I'm going faster if you had

00:17:54,130 --> 00:18:03,220
problems please raise a hand okay is

00:17:58,000 --> 00:18:06,460
more or less clears apart okay so this

00:18:03,220 --> 00:18:09,420
work was a very brief overview of the

00:18:06,460 --> 00:18:13,840
problems that are dbms is had and how

00:18:09,420 --> 00:18:17,500
and what's at stake here so now fast

00:18:13,840 --> 00:18:21,520
forward to tell 2007 in 2007 amazon

00:18:17,500 --> 00:18:25,330
published my paper which is the dino

00:18:21,520 --> 00:18:29,850
paper which is which was for the first

00:18:25,330 --> 00:18:34,120
time an accessible description of solid

00:18:29,850 --> 00:18:38,740
distribute distribute principles so for

00:18:34,120 --> 00:18:41,140
the first time some the the fundamental

00:18:38,740 --> 00:18:43,270
blocks of a distributed system we're

00:18:41,140 --> 00:18:46,720
presenting an accessible way it's not

00:18:43,270 --> 00:18:49,270
like anything of this stuff is new all

00:18:46,720 --> 00:18:51,160
these concepts were known since the 70s

00:18:49,270 --> 00:18:53,230
but for the first time they were

00:18:51,160 --> 00:18:56,200
presenting all together in an accessible

00:18:53,230 --> 00:18:59,710
way and these principles are consistent

00:18:56,200 --> 00:19:02,370
hashing vector clocks ragazza protocol

00:18:59,710 --> 00:19:05,340
hinted at and offs and will you bear

00:19:02,370 --> 00:19:08,140
let's start with consistent national

00:19:05,340 --> 00:19:12,190
okay so let's suppose we have n nodes

00:19:08,140 --> 00:19:15,910
and each of the node contains some of

00:19:12,190 --> 00:19:18,160
our data okay we need to spread the data

00:19:15,910 --> 00:19:20,830
or the load across different nodes how

00:19:18,160 --> 00:19:25,750
do we choose which node a piece of data

00:19:20,830 --> 00:19:29,650
should go to the simple solution is to

00:19:25,750 --> 00:19:33,850
have a module based hashing so you

00:19:29,650 --> 00:19:36,999
simply get the key module in under

00:19:33,850 --> 00:19:39,609
servers and that's that gives you the

00:19:36,999 --> 00:19:43,569
index of the node you should put the

00:19:39,609 --> 00:19:47,309
piece of data to int'l what happens if

00:19:43,569 --> 00:19:51,129
one node dies or if you hadn't one node

00:19:47,309 --> 00:19:55,359
the problem with this is number of

00:19:51,129 --> 00:19:58,539
servers changes so you have to reshuffle

00:19:55,359 --> 00:20:00,789
reach three hash every single piece of

00:19:58,539 --> 00:20:03,219
data in the entire cluster because the

00:20:00,789 --> 00:20:07,089
otherwise you can't find anything any

00:20:03,219 --> 00:20:08,919
longer so it's a bit expensive if you if

00:20:07,089 --> 00:20:10,539
you want to add node you to take the

00:20:08,919 --> 00:20:15,039
entire system down we calculate the

00:20:10,539 --> 00:20:19,779
hashes every time and then take it up

00:20:15,039 --> 00:20:22,629
again so the idea here is a brilliant it

00:20:19,779 --> 00:20:26,619
is one of those one on central is that

00:20:22,629 --> 00:20:29,619
is simple and very effective instead of

00:20:26,619 --> 00:20:32,739
using module head by Baeza hashim here

00:20:29,619 --> 00:20:36,039
we use a green space which is our key

00:20:32,739 --> 00:20:40,419
space and very important we use the same

00:20:36,039 --> 00:20:44,139
hash function for both the service

00:20:40,419 --> 00:20:47,559
location and the data so let's start

00:20:44,139 --> 00:20:50,259
with the nose we have a few servers we

00:20:47,559 --> 00:20:53,979
hash the key of these servers and we

00:20:50,259 --> 00:20:55,509
place them onto the written ok this can

00:20:53,979 --> 00:20:57,489
be a random location or we can

00:20:55,509 --> 00:21:00,969
distribute to the service informally and

00:20:57,489 --> 00:21:04,449
then using the same hashing function we

00:21:00,969 --> 00:21:09,699
put data onto the regular ok so this can

00:21:04,449 --> 00:21:12,059
be anywhere in the beam from the point

00:21:09,699 --> 00:21:16,479
where we store the data we walk

00:21:12,059 --> 00:21:19,689
clockwise and the first node that we

00:21:16,479 --> 00:21:22,149
encounter that is the note that we

00:21:19,689 --> 00:21:25,389
consider to be the canonical home for

00:21:22,149 --> 00:21:28,929
the data the piece of data ok so the

00:21:25,389 --> 00:21:33,009
entire segment between a D and B will be

00:21:28,929 --> 00:21:36,039
stored physically on node 2 okay why did

00:21:33,009 --> 00:21:38,979
why is this good but because it's no big

00:21:36,039 --> 00:21:42,659
is done then you don't have to rehash

00:21:38,979 --> 00:21:45,940
the entire key set you only have to

00:21:42,659 --> 00:21:46,500
reassign the keys that we're store do

00:21:45,940 --> 00:21:51,360
not be

00:21:46,500 --> 00:21:55,940
into the next available node is this

00:21:51,360 --> 00:22:00,270
clear it's very simple very actually and

00:21:55,940 --> 00:22:02,580
this is a this has a very important

00:22:00,270 --> 00:22:06,150
plication it means that as long as you

00:22:02,580 --> 00:22:10,050
have some nodes up price will never fail

00:22:06,150 --> 00:22:14,130
so that's a huge important thing for

00:22:10,050 --> 00:22:16,200
high availability okay so this is the

00:22:14,130 --> 00:22:20,550
only Ranger keys that you have to reach

00:22:16,200 --> 00:22:23,070
shuffle around consistent hashing makes

00:22:20,550 --> 00:22:26,040
it very easy to handle replication as

00:22:23,070 --> 00:22:28,470
well because with the same idea we can

00:22:26,040 --> 00:22:31,680
also handle replication we can decide

00:22:28,470 --> 00:22:35,390
where to place replicas so we simply

00:22:31,680 --> 00:22:38,520
replicate the data onto the three

00:22:35,390 --> 00:22:41,040
success and successors also on up to the

00:22:38,520 --> 00:22:43,830
three nodes that follow the location of

00:22:41,040 --> 00:22:47,070
the data okay they're simple it's very

00:22:43,830 --> 00:22:49,080
easy way of distributing the data you

00:22:47,070 --> 00:22:50,760
just walk clockwise the first three

00:22:49,080 --> 00:22:58,430
nodes you in character of course three

00:22:50,760 --> 00:23:03,270
is configurable what is an old changes

00:22:58,430 --> 00:23:05,550
like one node goes down then the

00:23:03,270 --> 00:23:08,100
replicas for that node will be in a

00:23:05,550 --> 00:23:10,230
number which is lower than the replicas

00:23:08,100 --> 00:23:13,020
for all the other nodes so what happens

00:23:10,230 --> 00:23:16,110
is whenever a failure in a door notice

00:23:13,020 --> 00:23:18,390
detected the subsequent three knows will

00:23:16,110 --> 00:23:21,110
take care of the data in the previous

00:23:18,390 --> 00:23:23,730
segment so individual three segments

00:23:21,110 --> 00:23:26,610
okay so the number of replicas is

00:23:23,730 --> 00:23:34,140
consistent at any time eventually

00:23:26,610 --> 00:23:37,620
persisted all times okay when we talk

00:23:34,140 --> 00:23:40,530
about a load we also think about load

00:23:37,620 --> 00:23:44,610
distribution because it's quite clear

00:23:40,530 --> 00:23:47,220
that if you have a random distribution

00:23:44,610 --> 00:23:49,980
over your nose then you can have hot

00:23:47,220 --> 00:23:53,870
spots in the system so some nodes might

00:23:49,980 --> 00:23:56,380
be more loaded than others so how do we

00:23:53,870 --> 00:24:00,100
take care of load

00:23:56,380 --> 00:24:03,010
distribution well the dynamo paper shows

00:24:00,100 --> 00:24:05,620
three different strategies but more can

00:24:03,010 --> 00:24:08,380
be decided the first strategy is very

00:24:05,620 --> 00:24:10,780
simple first of all instead of thinking

00:24:08,380 --> 00:24:14,080
about physical nodes we think about

00:24:10,780 --> 00:24:17,860
virtual nodes so a physical node can

00:24:14,080 --> 00:24:21,040
contain many virtual nodes and then we

00:24:17,860 --> 00:24:23,470
spread the virtual nodes that are

00:24:21,040 --> 00:24:25,930
physically stored in one server across

00:24:23,470 --> 00:24:28,870
the cluster intertwined with the virtual

00:24:25,930 --> 00:24:33,490
nodes of other nodes okay so this way we

00:24:28,870 --> 00:24:38,800
avoid the case where an entire range of

00:24:33,490 --> 00:24:41,560
keys is only on one server so we kind of

00:24:38,800 --> 00:24:44,980
help spread the load more uniformly

00:24:41,560 --> 00:24:48,460
across different physical machines a

00:24:44,980 --> 00:24:50,970
slightly different strategy this is

00:24:48,460 --> 00:24:55,090
sorta to another preview pretty ready

00:24:50,970 --> 00:24:58,750
Dino paper he is to have to split the

00:24:55,090 --> 00:25:02,350
ring space in our pre fixed number of

00:24:58,750 --> 00:25:05,440
equal size partitions and then all the

00:25:02,350 --> 00:25:07,720
physical nodes will claim a more or less

00:25:05,440 --> 00:25:11,230
uniform number of those partitions

00:25:07,720 --> 00:25:13,930
across the ring ok so this guarantee is

00:25:11,230 --> 00:25:17,220
an even better distribution of the load

00:25:13,930 --> 00:25:22,810
across the different physical notes Oh

00:25:17,220 --> 00:25:26,860
clear a very important concept of the

00:25:22,810 --> 00:25:29,560
dynamo paper is vector clocks if you

00:25:26,860 --> 00:25:33,100
think about a distributed system then

00:25:29,560 --> 00:25:35,560
you quite quickly realize that time stem

00:25:33,100 --> 00:25:39,370
day is the resolution of conflicts is no

00:25:35,560 --> 00:25:43,000
longer enough if the time the wall time

00:25:39,370 --> 00:25:45,670
of different nodes isn't in sync then

00:25:43,000 --> 00:25:48,370
you can't decide whether a certain

00:25:45,670 --> 00:25:51,520
version of a document is newer than the

00:25:48,370 --> 00:25:53,250
other one you can't so vector clocks try

00:25:51,520 --> 00:25:57,310
to solve this problem by putting

00:25:53,250 --> 00:26:00,790
causality in the decision whether one no

00:25:57,310 --> 00:26:02,800
one version of document is subsequent to

00:26:00,790 --> 00:26:07,030
the other one how did it how do they

00:26:02,800 --> 00:26:09,290
work it's again quite simple there is a

00:26:07,030 --> 00:26:12,590
piece of metadata associated with

00:26:09,290 --> 00:26:15,890
each document so we have the document in

00:26:12,590 --> 00:26:18,470
version one written by notable a not a

00:26:15,890 --> 00:26:23,870
will increment its encounter for that

00:26:18,470 --> 00:26:27,200
node from 0 to 1 then a subsequent time

00:26:23,870 --> 00:26:29,870
the same node a will date to the same

00:26:27,200 --> 00:26:34,400
document and will increment the counter

00:26:29,870 --> 00:26:36,380
so that's the revision of the document

00:26:34,400 --> 00:26:41,690
according to an asset specific knowledge

00:26:36,380 --> 00:26:44,960
and of course each node will have its

00:26:41,690 --> 00:26:47,510
own counter for each document how do we

00:26:44,960 --> 00:26:49,670
detect a conflict let's suppose we have

00:26:47,510 --> 00:26:54,160
a network split so we have two copies of

00:26:49,670 --> 00:26:58,850
our data our design document and one

00:26:54,160 --> 00:27:00,710
node will update one for one copy and in

00:26:58,850 --> 00:27:03,110
the other side of the the network split

00:27:00,710 --> 00:27:05,840
there is another node updating the same

00:27:03,110 --> 00:27:08,270
document with different data when the

00:27:05,840 --> 00:27:11,870
partition goes away how do you reconcile

00:27:08,270 --> 00:27:16,430
it to document well with vector cloths

00:27:11,870 --> 00:27:19,760
you are guaranteed that if the counters

00:27:16,430 --> 00:27:22,520
of the two documents are one this sub

00:27:19,760 --> 00:27:27,230
suffers superset of the other then you

00:27:22,520 --> 00:27:29,660
know that the first one is newer if they

00:27:27,230 --> 00:27:32,630
are not one is super set the other then

00:27:29,660 --> 00:27:34,700
you have a conflict the vector clocks

00:27:32,630 --> 00:27:37,040
alone will not tell you how to solve the

00:27:34,700 --> 00:27:40,880
conflict but they are able to detect it

00:27:37,040 --> 00:27:44,270
so the application itself can decide how

00:27:40,880 --> 00:27:45,860
to reconciliate the conflict such as it

00:27:44,270 --> 00:27:49,420
could merge the two versions of the

00:27:45,860 --> 00:27:52,130
document think that a dynamo was

00:27:49,420 --> 00:27:56,900
invented by amazon for their shopping

00:27:52,130 --> 00:27:59,300
cart so let's suppose someone updates I

00:27:56,900 --> 00:28:03,560
put some items into the shopping cart

00:27:59,300 --> 00:28:07,100
then there is a network splitter and on

00:28:03,560 --> 00:28:08,870
one side of the network I have a the

00:28:07,100 --> 00:28:11,150
vision of the shopping cart is in a

00:28:08,870 --> 00:28:14,320
certain state and then in the other

00:28:11,150 --> 00:28:17,270
partition I keep adding stuff so the

00:28:14,320 --> 00:28:19,580
conservative behavior when you merge the

00:28:17,270 --> 00:28:21,290
change the changes is to add all the

00:28:19,580 --> 00:28:24,590
items in both cards

00:28:21,290 --> 00:28:30,020
ok so you don't want to lose any item

00:28:24,590 --> 00:28:33,790
just because you had a partition another

00:28:30,020 --> 00:28:36,920
key concept of the diamond paper is

00:28:33,790 --> 00:28:40,430
gossip because it's protocol pair with

00:28:36,920 --> 00:28:43,040
the painted and off how does it work

00:28:40,430 --> 00:28:44,990
well you have to think about the rain

00:28:43,040 --> 00:28:47,270
and he knows has a very chatty

00:28:44,990 --> 00:28:50,180
environment so all the notes will

00:28:47,270 --> 00:28:53,210
constantly talk to all the others in a

00:28:50,180 --> 00:28:56,150
random way about their own vision of the

00:28:53,210 --> 00:29:00,770
network at that point in time so he's

00:28:56,150 --> 00:29:03,260
not a notice is that not be is down it

00:29:00,770 --> 00:29:06,710
will declare oh I can't see not be and

00:29:03,260 --> 00:29:10,340
as he replies oh I can't see it either

00:29:06,710 --> 00:29:13,130
and now d replies oh I consider either

00:29:10,340 --> 00:29:15,770
then node B must be done so let's

00:29:13,130 --> 00:29:20,060
discard it for now let's not consider it

00:29:15,770 --> 00:29:23,750
as available so we will take load for

00:29:20,060 --> 00:29:28,280
instead of the pinole and here the

00:29:23,750 --> 00:29:31,450
handouts is a related concept so let's

00:29:28,280 --> 00:29:34,730
suppose we have to add new node that

00:29:31,450 --> 00:29:37,280
given the hashing function is intended

00:29:34,730 --> 00:29:39,110
for B but B is not available at this

00:29:37,280 --> 00:29:42,380
point in time it might be temporarily

00:29:39,110 --> 00:29:46,940
unavailable in my stomach up again so

00:29:42,380 --> 00:29:51,560
what happens here oops sorry what

00:29:46,940 --> 00:29:53,660
happens here be basically will will be

00:29:51,560 --> 00:29:55,970
down and see the next of a good note

00:29:53,660 --> 00:29:59,840
we'll take the request will store the

00:29:55,970 --> 00:30:05,780
note along with the hint that when we

00:29:59,840 --> 00:30:08,360
can suck back again then C will give the

00:30:05,780 --> 00:30:11,470
node DD document to the intended

00:30:08,360 --> 00:30:17,070
knowledge okay so this is a self-healing

00:30:11,470 --> 00:30:21,870
property okay

00:30:17,070 --> 00:30:24,690
and finally we add the concept of read

00:30:21,870 --> 00:30:28,769
repairs so what happens when decline

00:30:24,690 --> 00:30:31,919
tries and branded a key this is an

00:30:28,769 --> 00:30:34,909
example of a tunable consistency this

00:30:31,919 --> 00:30:38,820
client wants to have document key

00:30:34,909 --> 00:30:41,700
Express with a key K and wants to have a

00:30:38,820 --> 00:30:44,220
quorum of two so at least two notes must

00:30:41,700 --> 00:30:47,070
correspond with subversion d of the

00:30:44,220 --> 00:30:50,610
document in order to be a successful

00:30:47,070 --> 00:30:52,889
brain so if we send de request through

00:30:50,610 --> 00:30:55,259
all the replicas of the nose containing

00:30:52,889 --> 00:30:58,860
a replica and those nodes will respond

00:30:55,259 --> 00:31:00,809
to requests two of those nodes have the

00:30:58,860 --> 00:31:03,210
same provision of the document one has a

00:31:00,809 --> 00:31:07,200
previous relationship document they will

00:31:03,210 --> 00:31:10,590
all reply and the client will take a

00:31:07,200 --> 00:31:12,899
quorum based decision majority wins so

00:31:10,590 --> 00:31:15,000
it will reply with it latest it will

00:31:12,899 --> 00:31:17,429
return the latest version of the

00:31:15,000 --> 00:31:20,940
document but at the same time it will

00:31:17,429 --> 00:31:23,639
also instruct note C to update the

00:31:20,940 --> 00:31:28,080
document with latest version again it's

00:31:23,639 --> 00:31:32,429
another self healing system okay enough

00:31:28,080 --> 00:31:36,440
I already annoyed you quite bored you

00:31:32,429 --> 00:31:39,299
quite a lot now let's see some real

00:31:36,440 --> 00:31:42,980
products I think it was important to

00:31:39,299 --> 00:31:46,710
show the architectural decisions behind

00:31:42,980 --> 00:31:48,750
those any products because otherwise you

00:31:46,710 --> 00:31:51,809
fail to understand this strong points of

00:31:48,750 --> 00:31:54,299
the different systems one of my biggest

00:31:51,809 --> 00:31:57,919
rights with know as well is that we

00:31:54,299 --> 00:32:01,320
think about no SQL as set of

00:31:57,919 --> 00:32:06,690
interchangeable components so Cassandra

00:32:01,320 --> 00:32:09,269
or CouchDB or whatever they are all the

00:32:06,690 --> 00:32:11,580
same not they are not it's not like the

00:32:09,269 --> 00:32:14,129
nor SQL world where yeah there are some

00:32:11,580 --> 00:32:16,289
slight differences between different

00:32:14,129 --> 00:32:19,860
vendors but the model is more less the

00:32:16,289 --> 00:32:21,990
same the one ranked point at some point

00:32:19,860 --> 00:32:25,320
of choice of masculine that basis is

00:32:21,990 --> 00:32:29,220
that they are a big variety of solutions

00:32:25,320 --> 00:32:33,659
okay and we can group them in four major

00:32:29,220 --> 00:32:37,620
families the first they are the Aimable

00:32:33,659 --> 00:32:40,710
is of course DD in the the selling point

00:32:37,620 --> 00:32:42,659
they are all focused on scalability but

00:32:40,710 --> 00:32:44,940
the goal is different depending on the

00:32:42,659 --> 00:32:47,610
family so on one side we have key value

00:32:44,940 --> 00:32:49,860
stores those are excellent for

00:32:47,610 --> 00:32:53,220
scalability you can distribute them

00:32:49,860 --> 00:32:55,830
across a usual number of nodes and this

00:32:53,220 --> 00:32:58,590
is beautifully but they are very poor

00:32:55,830 --> 00:33:01,169
data model you only have keys and values

00:32:58,590 --> 00:33:03,659
which is not what you build your

00:33:01,169 --> 00:33:05,879
application on top of usually then we

00:33:03,659 --> 00:33:08,210
have coming from families talk me into

00:33:05,879 --> 00:33:11,100
databases and with increasing complexity

00:33:08,210 --> 00:33:13,559
graph databases so at this point we have

00:33:11,100 --> 00:33:18,330
a very rich data model but it's not as

00:33:13,559 --> 00:33:21,360
scalable when data size grows okay so

00:33:18,330 --> 00:33:24,690
let's start from the keyboard stores the

00:33:21,360 --> 00:33:26,100
first product is Voldemort this was

00:33:24,690 --> 00:33:28,830
developed by linkedin and is a

00:33:26,100 --> 00:33:31,879
by-the-book implementation dynamo so all

00:33:28,830 --> 00:33:38,659
I said about a consistent fashion or the

00:33:31,879 --> 00:33:38,659
ratepayer it's all in here ok so the

00:33:38,929 --> 00:33:48,389
Voldemort is behind an HTTP or API it

00:33:45,000 --> 00:33:51,570
handles conflicts both a read and write

00:33:48,389 --> 00:33:55,019
time can detect them and you can decide

00:33:51,570 --> 00:33:57,570
what to do with a conflict it supports

00:33:55,019 --> 00:34:02,879
different data storage civilization

00:33:57,570 --> 00:34:07,409
systems you can store roll java objects

00:34:02,879 --> 00:34:10,290
or byte arrays or user session protocol

00:34:07,409 --> 00:34:14,599
and like Google's protocol buffers or

00:34:10,290 --> 00:34:17,099
fifth row row and the good thing about

00:34:14,599 --> 00:34:19,379
Voldemort is that it has a pluggable

00:34:17,099 --> 00:34:22,500
data storage again at the bottom of the

00:34:19,379 --> 00:34:25,319
stack so depending on what type of data

00:34:22,500 --> 00:34:28,649
you want to store you can use either my

00:34:25,319 --> 00:34:30,270
sequel or you can use birth ladies which

00:34:28,649 --> 00:34:32,819
are more efficient for there is more

00:34:30,270 --> 00:34:35,010
files so depending on little so you can

00:34:32,819 --> 00:34:38,040
choose which one which actually beckoned

00:34:35,010 --> 00:34:38,669
to use but this is on top of course it's

00:34:38,040 --> 00:34:41,549
the trashing

00:34:38,669 --> 00:34:44,099
all the good stuff to do you want on top

00:34:41,549 --> 00:34:48,710
of that Voldemort is probably the only

00:34:44,099 --> 00:34:51,510
one is family guarantees some sort of

00:34:48,710 --> 00:34:53,720
transactions across different rows most

00:34:51,510 --> 00:34:58,650
of them just going to transactions

00:34:53,720 --> 00:35:00,960
atomicity on a single roll and there is

00:34:58,650 --> 00:35:06,119
also support for MapReduce by an adult

00:35:00,960 --> 00:35:08,460
here another cable store is in days the

00:35:06,119 --> 00:35:12,030
biggest selling point of mayonnaise is

00:35:08,460 --> 00:35:15,119
that is API compatible with memcache d

00:35:12,030 --> 00:35:17,700
so if you know how to use my HD you know

00:35:15,119 --> 00:35:21,809
how to use Monday's it's the same idea

00:35:17,700 --> 00:35:25,859
actually membres itself has a concept of

00:35:21,809 --> 00:35:28,410
the buckets that our data stores of two

00:35:25,859 --> 00:35:33,329
different types one is linkage d so it's

00:35:28,410 --> 00:35:36,930
an in-memory distributed store and the

00:35:33,329 --> 00:35:42,829
other one AD persistency and replication

00:35:36,930 --> 00:35:42,829
with phil over heaven high availability

00:35:43,880 --> 00:35:50,480
it doesn't have it doesn't use

00:35:46,260 --> 00:35:54,450
consistent hashing it's using our very

00:35:50,480 --> 00:35:58,200
custom way of solely services this same

00:35:54,450 --> 00:36:03,650
problem which is as solid but it kind of

00:35:58,200 --> 00:36:06,480
worked and one thing that can offer is

00:36:03,650 --> 00:36:08,819
it doesn't a handle failover

00:36:06,480 --> 00:36:12,319
automatically so if one node goes down

00:36:08,819 --> 00:36:17,220
it doesn't reconfigure the network

00:36:12,319 --> 00:36:20,190
automatically but it gives the client to

00:36:17,220 --> 00:36:22,980
choose what to do why is this a good

00:36:20,190 --> 00:36:26,099
thing well because maybe the client is

00:36:22,980 --> 00:36:28,829
being something outside of the data

00:36:26,099 --> 00:36:31,680
store might have access to other

00:36:28,829 --> 00:36:34,819
monitoring tools and decide whether a

00:36:31,680 --> 00:36:38,540
node is really down or maybe it's just

00:36:34,819 --> 00:36:41,579
under a lot of load so it might postpone

00:36:38,540 --> 00:36:46,349
the update for a while until denote

00:36:41,579 --> 00:36:49,870
recover okay overall this is a good

00:36:46,349 --> 00:36:52,900
protocol a good software it's

00:36:49,870 --> 00:36:55,330
memcache the base is portable so it's

00:36:52,900 --> 00:36:59,500
easy to implement it doesn't look as

00:36:55,330 --> 00:37:01,830
mature as the others a very cool

00:36:59,500 --> 00:37:06,400
implementation implementation dynamo is

00:37:01,830 --> 00:37:11,410
react this is again based on consistent

00:37:06,400 --> 00:37:16,120
hashing it has fixed side partitions it

00:37:11,410 --> 00:37:19,660
can man it supports also links between

00:37:16,120 --> 00:37:23,550
keys so you can express relations

00:37:19,660 --> 00:37:26,800
between nodes somehow so it can traverse

00:37:23,550 --> 00:37:29,380
multiple keys in in a way so it's adding

00:37:26,800 --> 00:37:33,970
a bit of relations on top of that simple

00:37:29,380 --> 00:37:36,280
cable to store it supports MapReduce but

00:37:33,970 --> 00:37:38,650
it can be targeted so it doesn't need to

00:37:36,280 --> 00:37:40,600
be run on the entire cluster you can

00:37:38,650 --> 00:37:43,390
decide on which nodes to run amok reduce

00:37:40,600 --> 00:37:46,710
so if you are only interested in getting

00:37:43,390 --> 00:37:50,410
data from settle knows you can do so and

00:37:46,710 --> 00:37:54,510
this one again has a tunable consistency

00:37:50,410 --> 00:37:57,670
level on appropriate we're based basis

00:37:54,510 --> 00:38:01,690
the last one of these family is radius

00:37:57,670 --> 00:38:05,230
radius is little jam this is not

00:38:01,690 --> 00:38:07,750
distributed this in-memory data

00:38:05,230 --> 00:38:10,240
structure server it's not just a cable

00:38:07,750 --> 00:38:12,460
store its production reductant to call

00:38:10,240 --> 00:38:17,350
it a key value store it can support sex

00:38:12,460 --> 00:38:19,660
maps queues and you can add linked list

00:38:17,350 --> 00:38:22,960
and you can have the celebrations so you

00:38:19,660 --> 00:38:27,760
can do unions you can do difference ADIZ

00:38:22,960 --> 00:38:31,660
you can have counters you can have also

00:38:27,760 --> 00:38:34,780
expired on a key I can add up published

00:38:31,660 --> 00:38:38,680
shooting sorry a publisher subscriber

00:38:34,780 --> 00:38:40,900
model it's very fast very very fast

00:38:38,680 --> 00:38:45,000
being in memory this is what you would

00:38:40,900 --> 00:38:48,220
expect it does support persistently

00:38:45,000 --> 00:38:51,700
persistency via is not shopping which

00:38:48,220 --> 00:38:54,850
means it's a snapshot to the system so

00:38:51,700 --> 00:38:57,580
it's not a continuous it not is not

00:38:54,850 --> 00:39:00,310
continuously back to this is only done

00:38:57,580 --> 00:39:02,380
periodically and it doesn't support

00:39:00,310 --> 00:39:04,780
distribution out of the box

00:39:02,380 --> 00:39:09,120
unless the client itself supports a

00:39:04,780 --> 00:39:12,430
consistent hashing ok the next family of

00:39:09,120 --> 00:39:14,710
products is derived from the google

00:39:12,430 --> 00:39:19,540
BigTable paper this is another seminal

00:39:14,710 --> 00:39:23,170
paper in the noisy l word it's been this

00:39:19,540 --> 00:39:26,050
data store is using over 60 different

00:39:23,170 --> 00:39:28,000
products within google and the data

00:39:26,050 --> 00:39:31,390
model is radically different from what

00:39:28,000 --> 00:39:34,840
you might expect from database so there

00:39:31,390 --> 00:39:36,880
is a concept of rows as a constable

00:39:34,840 --> 00:39:39,220
tables which is more or less like a

00:39:36,880 --> 00:39:43,000
database in the relational database work

00:39:39,220 --> 00:39:49,210
then we are bros and we have comms

00:39:43,000 --> 00:39:51,370
because can be in the billions up there

00:39:49,210 --> 00:39:53,650
I said it's a sparse distributed

00:39:51,370 --> 00:39:57,040
persistency multinational sorted map

00:39:53,650 --> 00:40:00,160
that's a lot of stuff distributed means

00:39:57,040 --> 00:40:02,440
that it sits on top of our gfs which is

00:40:00,160 --> 00:40:05,410
a disability supply system Percy

00:40:02,440 --> 00:40:08,710
something is back back on this

00:40:05,410 --> 00:40:12,130
multi-dimensional because it doesn't it

00:40:08,710 --> 00:40:14,110
also has a concept of common families so

00:40:12,130 --> 00:40:19,330
you can group the columns into a family

00:40:14,110 --> 00:40:21,250
and also it has a time stamp for each

00:40:19,330 --> 00:40:26,020
revision of the document so you know

00:40:21,250 --> 00:40:30,060
which version is the latest one and at

00:40:26,020 --> 00:40:33,370
coma family level you can have a CLS the

00:40:30,060 --> 00:40:36,340
update of an entire roll our atomic and

00:40:33,370 --> 00:40:38,950
finally there is an automatic garbage

00:40:36,340 --> 00:40:41,980
collection system to and reclaim this

00:40:38,950 --> 00:40:46,720
space this is the building block of a

00:40:41,980 --> 00:40:48,490
google BigTable system it's our that

00:40:46,720 --> 00:40:51,550
structure which is a series of blocks

00:40:48,490 --> 00:40:56,380
with a look at the index to make it fast

00:40:51,550 --> 00:40:58,630
to retrieve the data which and this is

00:40:56,380 --> 00:41:03,640
grouped into a tablet which is a range

00:40:58,630 --> 00:41:07,660
of rows ok and the table is a collection

00:41:03,640 --> 00:41:13,030
of ranges so this is why it's a

00:41:07,660 --> 00:41:15,700
multi-dimensional ok d are your model is

00:41:13,030 --> 00:41:16,210
works like this every bride right

00:41:15,700 --> 00:41:19,960
request

00:41:16,210 --> 00:41:23,440
is first goes through a log first and

00:41:19,960 --> 00:41:25,210
then written in memory once the memory

00:41:23,440 --> 00:41:29,410
structure reaches a certain threshold

00:41:25,210 --> 00:41:33,400
then the data is dumped onto this into

00:41:29,410 --> 00:41:36,190
an immutable data structure the weeds

00:41:33,400 --> 00:41:39,310
will then do a merge between what's in

00:41:36,190 --> 00:41:43,089
memory and what's in the indeed on disco

00:41:39,310 --> 00:41:46,570
and will return leading the data there

00:41:43,089 --> 00:41:49,180
is a major compaction done on the

00:41:46,570 --> 00:41:52,599
physical types to reclaim space this

00:41:49,180 --> 00:41:56,440
done regularly and there is a single

00:41:52,599 --> 00:41:59,230
master that has knowledge about where to

00:41:56,440 --> 00:42:04,810
find data in the cluster so this is a

00:41:59,230 --> 00:42:07,660
single point of failure HBase is a noble

00:42:04,810 --> 00:42:11,109
source implementation of google big

00:42:07,660 --> 00:42:15,540
table there are some differences namely

00:42:11,109 --> 00:42:19,390
its using huddle and HDFS instead of the

00:42:15,540 --> 00:42:22,240
deal instead of a Google file system

00:42:19,390 --> 00:42:25,330
it's using zookeeper instead of champion

00:42:22,240 --> 00:42:26,859
and it does support multi masters so it

00:42:25,330 --> 00:42:32,050
it doesn't have this single point of

00:42:26,859 --> 00:42:34,990
failure as as is one it can actually sit

00:42:32,050 --> 00:42:37,690
on top of different file systems so you

00:42:34,990 --> 00:42:43,270
can actually put it on amazon s3 and it

00:42:37,690 --> 00:42:45,640
works just fine it also it's a perfect

00:42:43,270 --> 00:42:49,150
candidate for any MapReduce job this is

00:42:45,640 --> 00:42:50,890
excellent for a lot of data processing

00:42:49,150 --> 00:42:54,339
stuff it's not necessarily good for

00:42:50,890 --> 00:42:58,150
real-time data serving but it's

00:42:54,339 --> 00:43:02,650
excellent for server processing a lot of

00:42:58,150 --> 00:43:06,310
data okay hyper table is an open source

00:43:02,650 --> 00:43:09,070
implementation of people Google big

00:43:06,310 --> 00:43:12,400
table it's more or less the same it does

00:43:09,070 --> 00:43:14,320
use a different coordinator it's not the

00:43:12,400 --> 00:43:16,359
people is not childish called hyperspace

00:43:14,320 --> 00:43:18,250
but more or less it's the same thing is

00:43:16,359 --> 00:43:20,800
just implementing a sicko splashing cell

00:43:18,250 --> 00:43:23,230
instead of Java and it's usually faster

00:43:20,800 --> 00:43:25,540
this is used by by judy google search

00:43:23,230 --> 00:43:28,510
engine the google competitor in china

00:43:25,540 --> 00:43:33,880
and then we have

00:43:28,510 --> 00:43:37,270
hybrid cassandra is a hybrid between big

00:43:33,880 --> 00:43:40,330
table and Daniel it uses the data model

00:43:37,270 --> 00:43:44,350
of big table and the distribution of

00:43:40,330 --> 00:43:47,170
surg model of dynamo so it's a bit

00:43:44,350 --> 00:43:51,010
complex to get it but it's also quite

00:43:47,170 --> 00:43:53,500
cool I'd be based level lavate column

00:43:51,010 --> 00:43:56,350
which is a pair of a column name and

00:43:53,500 --> 00:43:58,600
value plus the time stamp then you have

00:43:56,350 --> 00:44:01,900
a super column which is a group of

00:43:58,600 --> 00:44:04,620
columns you can have a chrome family

00:44:01,900 --> 00:44:06,820
which is an another group of cons and

00:44:04,620 --> 00:44:08,920
then you can have a super common

00:44:06,820 --> 00:44:12,640
families this is a bit complex the first

00:44:08,920 --> 00:44:15,910
time you see it but the main idea to

00:44:12,640 --> 00:44:18,780
take away is you had a row and then you

00:44:15,910 --> 00:44:24,420
have three different levels of hierarchy

00:44:18,780 --> 00:44:28,000
beneath it okay this again is using the

00:44:24,420 --> 00:44:30,790
consistent hashing peer-to-peer goes in

00:44:28,000 --> 00:44:34,380
protocol all the good stuff tunable a

00:44:30,790 --> 00:44:38,010
consistency level on query bases and

00:44:34,380 --> 00:44:42,100
this is very important it does support a

00:44:38,010 --> 00:44:44,170
partition air to know how to distribute

00:44:42,100 --> 00:44:46,810
the load across different nodes you can

00:44:44,170 --> 00:44:50,020
either use a random partitioner so it's

00:44:46,810 --> 00:44:52,750
basically using md5 to decide which know

00:44:50,020 --> 00:44:55,840
to send the data to or you have assorted

00:44:52,750 --> 00:44:58,990
partitioner which means you can then do

00:44:55,840 --> 00:45:01,900
scan ranges of the data so if you want

00:44:58,990 --> 00:45:06,010
to do scale ranges sorted scan range is

00:45:01,900 --> 00:45:08,680
very strong so sorry you can choose this

00:45:06,010 --> 00:45:11,290
other partitioner the drawback is the

00:45:08,680 --> 00:45:15,850
distribution is not as good because you

00:45:11,290 --> 00:45:19,660
might have a hot spots then we have

00:45:15,850 --> 00:45:24,400
documented that basis these are derived

00:45:19,660 --> 00:45:27,970
from lotus notes yes the first one is

00:45:24,400 --> 00:45:30,160
very popular it is kuch bhi instead of

00:45:27,970 --> 00:45:35,200
just keys and values you actually can

00:45:30,160 --> 00:45:38,010
store an entire JSON document it has the

00:45:35,200 --> 00:45:40,280
concept of views as a materialized

00:45:38,010 --> 00:45:43,140
MapReduce job

00:45:40,280 --> 00:45:46,740
it supports B plus three indexes which

00:45:43,140 --> 00:45:51,630
are very fast is it has a very good

00:45:46,740 --> 00:45:54,630
building a replication system and it

00:45:51,630 --> 00:45:56,640
doesn't handle conflicts at all all the

00:45:54,630 --> 00:46:01,880
conflicts are pushed to the developer to

00:45:56,640 --> 00:46:05,760
the application also being MVCC based

00:46:01,880 --> 00:46:08,700
database it means that it's very good

00:46:05,760 --> 00:46:11,010
for concurrent weeds but it this base

00:46:08,700 --> 00:46:13,710
growth is significant we see you know

00:46:11,010 --> 00:46:15,870
means what what really happens so this

00:46:13,710 --> 00:46:21,270
happens you have to do compaction quite

00:46:15,870 --> 00:46:22,770
a lot with Lane all the space and for

00:46:21,270 --> 00:46:25,830
performance if there are some tricks to

00:46:22,770 --> 00:46:28,470
speed up the device this is an example

00:46:25,830 --> 00:46:30,600
of what happens during compaction this

00:46:28,470 --> 00:46:33,510
is a real world examples about a year

00:46:30,600 --> 00:46:37,950
ago we implemented a catch to be at the

00:46:33,510 --> 00:46:40,350
BBC and this is what we we noticed when

00:46:37,950 --> 00:46:43,470
doing a compaction as you can see the

00:46:40,350 --> 00:46:45,780
load putting this server is in really

00:46:43,470 --> 00:46:48,180
considerable you just can't ignore so

00:46:45,780 --> 00:46:51,240
don't wait doing compaction when it's

00:46:48,180 --> 00:46:52,680
late okay do it regularly if you want

00:46:51,240 --> 00:46:56,340
good performances or other single and

00:46:52,680 --> 00:46:59,250
consistent performances the other

00:46:56,340 --> 00:47:02,400
document or in that base I want to show

00:46:59,250 --> 00:47:04,920
you is MongoDB it's both similar in the

00:47:02,400 --> 00:47:07,530
way it stores the the documents but also

00:47:04,920 --> 00:47:10,140
they are different first of all it does

00:47:07,530 --> 00:47:12,540
support Charlie how to be is not

00:47:10,140 --> 00:47:14,130
distributed at all it's all disability

00:47:12,540 --> 00:47:15,510
in the sense that is behind the

00:47:14,130 --> 00:47:19,040
restaurant interface but it doesn't

00:47:15,510 --> 00:47:24,150
spread the data across different nodes

00:47:19,040 --> 00:47:26,640
mongrel you can can do it it doesn't

00:47:24,150 --> 00:47:29,610
support and this mvcc so updates are

00:47:26,640 --> 00:47:32,880
done in place it has very nice features

00:47:29,610 --> 00:47:35,970
like geospatial indexes and persistency

00:47:32,880 --> 00:47:38,850
is a bit scary because it's handled by a

00:47:35,970 --> 00:47:40,980
snapshot team so everything is kept in

00:47:38,850 --> 00:47:45,180
memory as long as possible and then

00:47:40,980 --> 00:47:47,960
flash to these every now and then it has

00:47:45,180 --> 00:47:50,240
a supposed MapReduce but not producing

00:47:47,960 --> 00:47:51,980
the among gonna be is why different from

00:47:50,240 --> 00:47:54,980
what you expect normal for MapReduce is

00:47:51,980 --> 00:47:57,230
more a way of doing aggregation by

00:47:54,980 --> 00:48:01,730
default there is no knowledge on rice

00:47:57,230 --> 00:48:04,190
that's why it's still and this last

00:48:01,730 --> 00:48:07,070
version there is a flag so if you start

00:48:04,190 --> 00:48:12,380
the server with this flag they guarantee

00:48:07,070 --> 00:48:14,420
some durability on the data yay and the

00:48:12,380 --> 00:48:18,970
latest model I want to talk about is

00:48:14,420 --> 00:48:20,960
graph databases this is the most complex

00:48:18,970 --> 00:48:23,690
representation of the data you can add

00:48:20,960 --> 00:48:27,589
from cable to store to document or in

00:48:23,690 --> 00:48:30,650
two different levels of kin values to

00:48:27,589 --> 00:48:33,050
document our databases we get to the

00:48:30,650 --> 00:48:35,599
full graph representation of your data

00:48:33,050 --> 00:48:38,170
which is awesome so don't think about

00:48:35,599 --> 00:48:41,089
tables think about graphs with nodes

00:48:38,170 --> 00:48:46,070
relationships and properties on both

00:48:41,089 --> 00:48:49,869
nodes and the links it does support a

00:48:46,070 --> 00:48:51,980
cluster for with zookeeper but not for

00:48:49,869 --> 00:48:54,440
distributional data is just distribution

00:48:51,980 --> 00:48:55,849
of load it's actually replication on

00:48:54,440 --> 00:48:59,020
different notes just to handle the

00:48:55,849 --> 00:49:01,780
transaction though not the data growth

00:48:59,020 --> 00:49:07,640
physically on disk it's stored as a

00:49:01,780 --> 00:49:10,089
linked list it does support sparkle

00:49:07,640 --> 00:49:13,970
which is a nice rdf based query language

00:49:10,089 --> 00:49:15,950
and it's incredibly fast for graph based

00:49:13,970 --> 00:49:18,290
operations so if you are building a

00:49:15,950 --> 00:49:21,200
social network this is what we want this

00:49:18,290 --> 00:49:23,630
is this can be thousands of times faster

00:49:21,200 --> 00:49:27,589
than any another solution the drawback

00:49:23,630 --> 00:49:30,680
is it only fits in on one node it has to

00:49:27,589 --> 00:49:33,170
fit fit on one node and this is how the

00:49:30,680 --> 00:49:34,640
work it's not like it's a separate piece

00:49:33,170 --> 00:49:36,770
of your architecture it's actually

00:49:34,640 --> 00:49:39,920
integrated in your application so you

00:49:36,770 --> 00:49:41,990
have to actually write code to express

00:49:39,920 --> 00:49:43,700
your data it's not separate from the

00:49:41,990 --> 00:49:46,849
application is embedded in your

00:49:43,700 --> 00:49:49,339
application so this is what you do you

00:49:46,849 --> 00:49:51,050
can create a node you set properties we

00:49:49,339 --> 00:49:53,150
create another node the other properties

00:49:51,050 --> 00:49:55,940
and that you can create a relationship

00:49:53,150 --> 00:49:58,099
between those nodes and then you can

00:49:55,940 --> 00:49:59,880
traverse those relationships of course

00:49:58,099 --> 00:50:02,250
you can add different types of reversal

00:49:59,880 --> 00:50:05,190
you can choose when to stop you can

00:50:02,250 --> 00:50:08,339
follow a certain kind of like

00:50:05,190 --> 00:50:11,910
relationships and ignore the others so

00:50:08,339 --> 00:50:17,789
it's perfect for graph based

00:50:11,910 --> 00:50:20,460
representation your data I want to just

00:50:17,789 --> 00:50:23,569
one minute talk about some

00:50:20,460 --> 00:50:27,359
considerations that argue at this point

00:50:23,569 --> 00:50:32,039
think most people think about Noel as a

00:50:27,359 --> 00:50:35,160
rejection of SQL SQL is seen as black

00:50:32,039 --> 00:50:39,089
magic my son but it's not really the

00:50:35,160 --> 00:50:42,119
case I bet that most of the database did

00:50:39,089 --> 00:50:45,380
not feel database if we saw today would

00:50:42,119 --> 00:50:47,930
die to add SQL interface on top of it

00:50:45,380 --> 00:50:51,809
because the alternative is not easier

00:50:47,930 --> 00:50:54,839
everyone thinks that nice girl means new

00:50:51,809 --> 00:50:57,839
kid on the block easy fast all positive

00:50:54,839 --> 00:51:01,710
sides well it's not really the case not

00:50:57,839 --> 00:51:04,440
always the meal standard in queering

00:51:01,710 --> 00:51:08,039
these kind of languages that is slowly

00:51:04,440 --> 00:51:11,730
emerging emerging is MapReduce MapReduce

00:51:08,039 --> 00:51:14,250
is awesome it really is also to

00:51:11,730 --> 00:51:16,049
distribute blowed of the computation on

00:51:14,250 --> 00:51:18,630
different notes without moving data

00:51:16,049 --> 00:51:20,279
around you do the computation on the

00:51:18,630 --> 00:51:24,569
separate nose and then you combine the

00:51:20,279 --> 00:51:26,759
results but can be very complex so there

00:51:24,569 --> 00:51:30,390
are some interfaces on top of MapReduce

00:51:26,759 --> 00:51:35,190
like big the niqab and cascading that

00:51:30,390 --> 00:51:38,759
help writing MapReduce in an easier way

00:51:35,190 --> 00:51:42,960
just to show you this is a sample

00:51:38,759 --> 00:51:46,549
MapReduce job this is lipid equivalent

00:51:42,960 --> 00:51:49,259
version this is more sql-like

00:51:46,549 --> 00:51:54,269
representation of the same thing which

00:51:49,259 --> 00:51:56,730
one did you prefer so it if we think

00:51:54,269 --> 00:51:59,279
about the data model in in might look we

00:51:56,730 --> 00:52:02,579
are taking a step backwards because we

00:51:59,279 --> 00:52:06,269
had relationships in our databases we

00:52:02,579 --> 00:52:07,980
had strong data models and going back to

00:52:06,269 --> 00:52:11,030
key value stores is

00:52:07,980 --> 00:52:14,550
and given up a consistency giving up

00:52:11,030 --> 00:52:18,180
transito transactions even up all these

00:52:14,550 --> 00:52:22,200
nice nice progress properties mast much

00:52:18,180 --> 00:52:24,030
load like a step backwards 38 if you

00:52:22,200 --> 00:52:26,040
want scalability and hang high

00:52:24,030 --> 00:52:28,650
availability you are to give up

00:52:26,040 --> 00:52:30,840
something so all these different

00:52:28,650 --> 00:52:34,790
products attack the problem from a

00:52:30,840 --> 00:52:39,109
different perspective and you have a an

00:52:34,790 --> 00:52:42,480
army of tools you can choose from work

00:52:39,109 --> 00:52:45,270
one last thing we not we now live in a

00:52:42,480 --> 00:52:48,810
word where we have to collect an immense

00:52:45,270 --> 00:52:51,600
amount of data and more often than not

00:52:48,810 --> 00:52:54,750
we don't know which questions we are

00:52:51,600 --> 00:52:59,100
going to ask before still collect me the

00:52:54,750 --> 00:53:02,369
data so not all the SQL database are

00:52:59,100 --> 00:53:07,190
good for all this for this case because

00:53:02,369 --> 00:53:13,530
if you think about a big table derived

00:53:07,190 --> 00:53:16,910
databases like Cassandra or HBase they

00:53:13,530 --> 00:53:19,080
are mostly able to answer one question

00:53:16,910 --> 00:53:21,810
if you try to answer a different

00:53:19,080 --> 00:53:24,840
question after modeling your application

00:53:21,810 --> 00:53:27,890
then the performance you usually expect

00:53:24,840 --> 00:53:31,770
from from them are completely different

00:53:27,890 --> 00:53:34,530
okay so be aware of how you're going to

00:53:31,770 --> 00:53:36,990
put your data MongoDB aur kuch bhi are

00:53:34,530 --> 00:53:41,430
more flexible in that way but not as

00:53:36,990 --> 00:53:44,550
efficient in other cases oh I mentioned

00:53:41,430 --> 00:53:46,680
Cassandra we now have a London group

00:53:44,550 --> 00:53:49,109
user group now it's awesome check out

00:53:46,680 --> 00:53:53,640
the gardener if you are interested is

00:53:49,109 --> 00:53:54,630
quite a nice tool Wow okay if you are

00:53:53,640 --> 00:53:56,700
interested in playing with these

00:53:54,630 --> 00:53:58,380
technologies we are playing with them

00:53:56,700 --> 00:54:01,590
all the time so if you're interested

00:53:58,380 --> 00:54:03,950
current talk to me do you have any

00:54:01,590 --> 00:54:03,950
questions

00:54:04,430 --> 00:54:13,970
I know I I really had to rush that's

00:54:12,560 --> 00:54:15,860
what I was going to say alpha chi we're

00:54:13,970 --> 00:54:19,400
running a little bit side of town so you

00:54:15,860 --> 00:54:21,710
got any questions we have to be quick no

00:54:19,400 --> 00:54:24,680
I saw the time we're going down slide

00:54:21,710 --> 00:54:26,510
please russian and i think i can come

00:54:24,680 --> 00:54:29,350
back and explain something is it with

00:54:26,510 --> 00:54:29,350
more detail with the fact

00:54:37,010 --> 00:54:44,960
hi hi if how would you choose between

00:54:42,890 --> 00:54:46,670
them what sort of criteria do you use if

00:54:44,960 --> 00:54:48,560
you're looking at a problem and

00:54:46,670 --> 00:54:50,660
realizing you have too much data to use

00:54:48,560 --> 00:54:52,040
a relational database what kind of

00:54:50,660 --> 00:54:54,950
criteria would you look for when

00:54:52,040 --> 00:54:57,950
choosing on those solutions yeah this is

00:54:54,950 --> 00:55:01,300
something I actually hope to express

00:54:57,950 --> 00:55:03,530
with the projector but it's quite simple

00:55:01,300 --> 00:55:06,520
depending on how you are going to query

00:55:03,530 --> 00:55:09,650
the data and what's the load you expect

00:55:06,520 --> 00:55:11,990
then you have different options let's

00:55:09,650 --> 00:55:15,770
suppose you have to collect a lot of

00:55:11,990 --> 00:55:18,440
data for background processing in that

00:55:15,770 --> 00:55:22,430
case a solution like HBase is perfect

00:55:18,440 --> 00:55:26,390
because it's distributed across the

00:55:22,430 --> 00:55:28,670
cluster uniform distribution and it is

00:55:26,390 --> 00:55:32,360
very good right and low latency

00:55:28,670 --> 00:55:35,120
characteristics if you have something

00:55:32,360 --> 00:55:36,440
more dynamic like you have to do a lot

00:55:35,120 --> 00:55:38,720
of rice but at the same time a lot of

00:55:36,440 --> 00:55:40,550
reads as well then cassandra is probably

00:55:38,720 --> 00:55:43,640
better in that case if you have some

00:55:40,550 --> 00:55:46,540
real data to process and and real

00:55:43,640 --> 00:55:49,640
queries in real time if you don't know

00:55:46,540 --> 00:55:52,610
what what you are going to ask from the

00:55:49,640 --> 00:55:54,400
database then there are other solutions

00:55:52,610 --> 00:55:56,990
there are more complex like

00:55:54,400 --> 00:55:58,670
document-oriented ladies I talked about

00:55:56,990 --> 00:56:01,760
couch TV and mongodb but there are

00:55:58,670 --> 00:56:05,540
others like ravendb and door are more

00:56:01,760 --> 00:56:08,150
flexible in a way not all them handle

00:56:05,540 --> 00:56:10,840
distribution of the data but at least

00:56:08,150 --> 00:56:14,660
all of them handle distribution of the

00:56:10,840 --> 00:56:16,870
transaction load so depending on which

00:56:14,660 --> 00:56:20,900
one is your constraint you might choose

00:56:16,870 --> 00:56:23,330
couch be if you are not concerned about

00:56:20,900 --> 00:56:26,750
data growth or MongoDB if you are

00:56:23,330 --> 00:56:30,680
concerned about data growth key value

00:56:26,750 --> 00:56:34,100
stores are the ultimate solution for

00:56:30,680 --> 00:56:36,080
true scalability because their

00:56:34,100 --> 00:56:38,630
distribution model is awesome there are

00:56:36,080 --> 00:56:40,240
so easy to distribute we just put them

00:56:38,630 --> 00:56:43,840
behind our consistent hash

00:56:40,240 --> 00:56:47,470
algorithm you're done but as I said the

00:56:43,840 --> 00:56:49,690
data model is bit crappy it's just key

00:56:47,470 --> 00:56:52,870
values so you have to handle the entire

00:56:49,690 --> 00:56:55,720
logic into the application you have to

00:56:52,870 --> 00:56:58,120
we build the constraints between

00:56:55,720 --> 00:57:01,180
different pieces of data in your

00:56:58,120 --> 00:57:03,790
application something you use to thank

00:57:01,180 --> 00:57:06,810
think as a given when you consider a

00:57:03,790 --> 00:57:08,950
relational database you have

00:57:06,810 --> 00:57:11,880
relationships it's a relation that ways

00:57:08,950 --> 00:57:16,330
not just dump data store so if you need

00:57:11,880 --> 00:57:21,130
those and yet it obviously a key value

00:57:16,330 --> 00:57:25,530
store is not as good graph databases are

00:57:21,130 --> 00:57:30,370
awesome for social networks indole the

00:57:25,530 --> 00:57:33,160
kind of cases where you you really had a

00:57:30,370 --> 00:57:37,530
very rich set of properties on the nose

00:57:33,160 --> 00:57:37,530
and a very integrated set of the

00:57:37,590 --> 00:57:43,300
relationship knows it can if the nose

00:57:40,870 --> 00:57:46,810
our people then you can you can add

00:57:43,300 --> 00:57:51,430
relationship like I know this person I'm

00:57:46,810 --> 00:57:55,180
a business partner I yeah whatever you

00:57:51,430 --> 00:58:00,580
can imagine I'm the the uncle is person

00:57:55,180 --> 00:58:03,100
whatever and yet maximum flexibility on

00:58:00,580 --> 00:58:05,830
how to express those relationships but

00:58:03,100 --> 00:58:08,140
they are very very ethical to distribute

00:58:05,830 --> 00:58:09,940
across different nodes the only

00:58:08,140 --> 00:58:13,360
distribution you have at the moment is

00:58:09,940 --> 00:58:15,610
via replication so again you distribute

00:58:13,360 --> 00:58:22,810
the transaction load but not the data

00:58:15,610 --> 00:58:24,280
growth I am we thank Lorenzo for be so

00:58:22,810 --> 00:58:27,010
consistent because it's very very

00:58:24,280 --> 00:58:28,900
complex it's okay so I'm afraid we don't

00:58:27,010 --> 00:58:33,330
have time for any further questions but

00:58:28,900 --> 00:58:33,330

YouTube URL: https://www.youtube.com/watch?v=l7n2NyKoS2s


