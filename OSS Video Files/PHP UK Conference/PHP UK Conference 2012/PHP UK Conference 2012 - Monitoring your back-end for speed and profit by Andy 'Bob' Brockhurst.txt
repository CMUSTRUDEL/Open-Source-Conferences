Title: PHP UK Conference 2012 - Monitoring your back-end for speed and profit by Andy 'Bob' Brockhurst
Publication date: 2012-05-03
Playlist: PHP UK Conference 2012
Description: 
	At the BBC we are preparing for the some big events in the coming year (the Olympics amongst them), as we don't have to cash to splash on new hardware in the current economic climate and our frozen license fee.

The BBC runs approximate 300 websites all contained with bbc.co.uk and running on the same hardware. This means that a spike in traffic on /weather affects /iplayer and all other sites hosted on our platform.

This talk will show some of the things we've been doing to benchmark our platform (the PHP, ZF portion anyway) and highlight poor performing sections of our site and address them.

I will explain some of the tools we've written and technologies we've used to achieve this on a relatively short timescale with limited budget.

This will cover our experience of using XHProf for the first time and augmenting Zend Framework to generate HAR (Http Archive) format files to expose the service calls our platform makes and how we've approached optimising them.
Captions: 
	00:00:07,400 --> 00:00:14,110
before they merge to drag me off so as

00:00:10,760 --> 00:00:14,110
lunch is next on

00:00:15,220 --> 00:00:23,710
anything i'll try and pretty much motor

00:00:18,040 --> 00:00:27,100
through this so um hi I'm Bob I'm a

00:00:23,710 --> 00:00:33,370
principal engineer at the BBC and I work

00:00:27,100 --> 00:00:39,130
in the frameworks team and we are a team

00:00:33,370 --> 00:00:43,930
of six people and we look after PHP

00:00:39,130 --> 00:00:45,730
apache varnish we look after zend

00:00:43,930 --> 00:00:47,290
framework that we have a framework stuck

00:00:45,730 --> 00:00:51,010
on top of that which we call the PAL

00:00:47,290 --> 00:00:52,390
which is the page assembly layer then we

00:00:51,010 --> 00:00:54,850
also have on top of that to further

00:00:52,390 --> 00:00:57,760
framework or BBC micro then we have a

00:00:54,850 --> 00:00:59,260
templating language called spectrum then

00:00:57,760 --> 00:01:01,750
we're also responsible for the global

00:00:59,260 --> 00:01:03,790
and header and footer for the BBC which

00:01:01,750 --> 00:01:08,080
is called bar lesk we also look after

00:01:03,790 --> 00:01:12,040
jquery and ah the list goes on we also

00:01:08,080 --> 00:01:14,770
look after a Java service layer which

00:01:12,040 --> 00:01:18,790
I'll come into in a few moments so the

00:01:14,770 --> 00:01:21,790
BBC runs on lamp but without the end we

00:01:18,790 --> 00:01:24,550
don't have any access directly from PHP

00:01:21,790 --> 00:01:28,480
to any data layers we have to go through

00:01:24,550 --> 00:01:30,550
a tomcat service layer and I've

00:01:28,480 --> 00:01:34,960
mentioned zend framework already and the

00:01:30,550 --> 00:01:38,020
rest of it and so how does the BBC work

00:01:34,960 --> 00:01:42,220
we have one domain we are a shared

00:01:38,020 --> 00:01:45,130
platform we kind of do have a bit more

00:01:42,220 --> 00:01:47,409
than one platter m1 domain but generally

00:01:45,130 --> 00:01:51,520
everything is served off dub dub dub BBC

00:01:47,409 --> 00:01:54,100
co uk we have two technology stacks we

00:01:51,520 --> 00:01:56,140
have clean the Powell layer which is the

00:01:54,100 --> 00:01:58,540
PHP layer that are talked about then we

00:01:56,140 --> 00:02:00,750
also have some proxy passes proxy

00:01:58,540 --> 00:02:03,340
policies sorry that go back to our

00:02:00,750 --> 00:02:08,409
legacy technology stack but everything

00:02:03,340 --> 00:02:11,019
is moving off that and onto the PAL all

00:02:08,409 --> 00:02:13,810
our data transfer internally is done

00:02:11,019 --> 00:02:17,830
with certificates so client certificates

00:02:13,810 --> 00:02:21,400
talking to SSL endpoints and proxy

00:02:17,830 --> 00:02:23,709
passes I mentioned so we have about

00:02:21,400 --> 00:02:28,290
three hundred and sixty different apps

00:02:23,709 --> 00:02:32,590
that run on WBC that code it k so I

00:02:28,290 --> 00:02:35,110
player is four and so we have I player

00:02:32,590 --> 00:02:37,450
desktop which is just /i player we also

00:02:35,110 --> 00:02:40,209
have I player for the TV we have I

00:02:37,450 --> 00:02:43,540
player for the TV version 2 which is the

00:02:40,209 --> 00:02:46,780
older version 2 is actually the older

00:02:43,540 --> 00:02:50,230
version we also have I player for the

00:02:46,780 --> 00:02:53,370
mobile so if you add all of those up for

00:02:50,230 --> 00:02:55,989
CBBC for sport the weather there's about

00:02:53,370 --> 00:03:01,180
360 and they're all set on the same

00:02:55,989 --> 00:03:05,860
servers we had at Christmas we had about

00:03:01,180 --> 00:03:09,970
where we had 42 PHP servers running the

00:03:05,860 --> 00:03:12,340
whole of the BBC so that's not a lot

00:03:09,970 --> 00:03:15,640
considering we do about 30 million

00:03:12,340 --> 00:03:19,390
unique users a day doing about half a

00:03:15,640 --> 00:03:23,410
billion page impressions so we run quite

00:03:19,390 --> 00:03:25,660
a tight ship and everybody's ring on the

00:03:23,410 --> 00:03:29,170
same platform so it's a shared host so a

00:03:25,660 --> 00:03:31,180
surge on BBC weather can take down I

00:03:29,170 --> 00:03:33,340
player and anybody on the home page and

00:03:31,180 --> 00:03:34,870
everybody else so we're very careful

00:03:33,340 --> 00:03:39,340
about performance where we have to be

00:03:34,870 --> 00:03:42,970
and so a little bit more about our

00:03:39,340 --> 00:03:48,209
network topology we have two data

00:03:42,970 --> 00:03:53,290
centers at the minute we have one in

00:03:48,209 --> 00:03:55,870
Watford with the cleverly named WTF data

00:03:53,290 --> 00:03:59,440
center and then we have London telewest

00:03:55,870 --> 00:04:01,989
house I think it is as well and we don't

00:03:59,440 --> 00:04:04,650
run any data center affinity so your

00:04:01,989 --> 00:04:07,629
request could be served out of either

00:04:04,650 --> 00:04:10,090
well there is one caveat there are a

00:04:07,629 --> 00:04:11,980
couple of apps that do have data center

00:04:10,090 --> 00:04:13,330
affinity but we don't like to talk about

00:04:11,980 --> 00:04:17,019
them because we're trying to get them to

00:04:13,330 --> 00:04:22,690
stop it this is roughly what I network

00:04:17,019 --> 00:04:25,030
looks like but I've got a simpler

00:04:22,690 --> 00:04:29,220
version so basically all the traffic

00:04:25,030 --> 00:04:31,500
that comes into the BBC goes through and

00:04:29,220 --> 00:04:34,260
one of several

00:04:31,500 --> 00:04:36,570
traffic routing patterns we either go

00:04:34,260 --> 00:04:39,390
from the ZX TMS which is our traffic

00:04:36,570 --> 00:04:41,100
routing servers that's Zeus traffic

00:04:39,390 --> 00:04:43,410
managers whatever if anyone doesn't know

00:04:41,100 --> 00:04:45,210
they are they go straight to our page

00:04:43,410 --> 00:04:48,000
assembly layer which is our PHP layer

00:04:45,210 --> 00:04:50,970
and we serve content straighten straight

00:04:48,000 --> 00:04:55,020
from there so at the minute we have I

00:04:50,970 --> 00:04:56,820
player whether BBC homepage there's

00:04:55,020 --> 00:04:59,940
about six or seven applications that are

00:04:56,820 --> 00:05:02,280
going through varnish so they go from

00:04:59,940 --> 00:05:04,560
ours lxd em into varnish and then they

00:05:02,280 --> 00:05:08,790
go out of varnish back to a Zed XDM and

00:05:04,560 --> 00:05:11,580
then on to our Powell servers again and

00:05:08,790 --> 00:05:15,150
if our pal ever tries to access any data

00:05:11,580 --> 00:05:17,490
so if we want to talk to our programs

00:05:15,150 --> 00:05:20,490
informations data services if we want to

00:05:17,490 --> 00:05:22,350
talk to our identity services they hit

00:05:20,490 --> 00:05:24,840
yet another zetex TM that goes on to the

00:05:22,350 --> 00:05:26,820
to the service layer and we've actually

00:05:24,840 --> 00:05:32,480
started putting varnish between that as

00:05:26,820 --> 00:05:35,880
well so the most common scenario is for

00:05:32,480 --> 00:05:38,130
dynamite which is our programs data that

00:05:35,880 --> 00:05:42,780
generates huge XML responses in the

00:05:38,130 --> 00:05:44,040
order of 1 maybe 2 Meg and previously

00:05:42,780 --> 00:05:46,590
they were going straight through zetex

00:05:44,040 --> 00:05:50,700
TMZ XDM only has about 2 gigabytes of

00:05:46,590 --> 00:05:53,130
cash and it would expire it even though

00:05:50,700 --> 00:05:55,050
we were sending headers back that said

00:05:53,130 --> 00:05:57,360
it could be cached for an hour or two

00:05:55,050 --> 00:06:00,419
hours generally it was only cash for

00:05:57,360 --> 00:06:03,330
about five minutes so varnish has got 64

00:06:00,419 --> 00:06:04,830
gigabytes of cash in there so and has

00:06:03,330 --> 00:06:08,160
got plenty of room to store that so we

00:06:04,830 --> 00:06:11,910
get a lot better cache hit ratio so and

00:06:08,160 --> 00:06:15,330
this is roughly what um what happens

00:06:11,910 --> 00:06:17,160
when you so the ZX TMS are generally the

00:06:15,330 --> 00:06:19,440
same cedex gems that you hit at the

00:06:17,160 --> 00:06:21,210
front end and we do loop back and same

00:06:19,440 --> 00:06:22,890
as the varnish instances are actually

00:06:21,210 --> 00:06:25,620
the same varnishes so we're basically

00:06:22,890 --> 00:06:30,660
doing lots of loops around and then we

00:06:25,620 --> 00:06:32,310
have and we now have we do the routing

00:06:30,660 --> 00:06:33,950
in the ZX GM based on headers that are

00:06:32,310 --> 00:06:36,950
sent from

00:06:33,950 --> 00:06:39,800
varnish so when you come into the ZX TM

00:06:36,950 --> 00:06:42,410
at the top that will set a header saying

00:06:39,800 --> 00:06:43,880
this is a request to a power box and

00:06:42,410 --> 00:06:45,320
then when it comes out of varnish

00:06:43,880 --> 00:06:48,710
there's an XDM or then root it's the

00:06:45,320 --> 00:06:51,770
local correct location so I was going to

00:06:48,710 --> 00:06:54,770
talk a little bit about caching because

00:06:51,770 --> 00:06:56,570
without our caching we really wouldn't

00:06:54,770 --> 00:06:58,370
be able to serve on the 40-odd servers

00:06:56,570 --> 00:07:00,500
that we have we now have about 70

00:06:58,370 --> 00:07:02,600
servers as of just after Christmas and

00:07:00,500 --> 00:07:04,880
that's mainly because we're scaling up

00:07:02,600 --> 00:07:08,570
to serve all the traffic for the

00:07:04,880 --> 00:07:11,510
Olympics so the caching that we

00:07:08,570 --> 00:07:14,960
previously had was mod cash sitting in

00:07:11,510 --> 00:07:16,970
Apache that was quite often defeated by

00:07:14,960 --> 00:07:22,280
bar lesk which is our global header and

00:07:16,970 --> 00:07:24,320
footer because that used to render

00:07:22,280 --> 00:07:27,260
different versions based on whether you

00:07:24,320 --> 00:07:28,880
were inside or outside of the UK which

00:07:27,260 --> 00:07:30,410
meant that we couldn't cash it therefore

00:07:28,880 --> 00:07:35,540
everything was sent as cash control

00:07:30,410 --> 00:07:37,790
private which so every time you could

00:07:35,540 --> 00:07:39,500
still cash inside mod cash occasionally

00:07:37,790 --> 00:07:43,160
but generally think generally everything

00:07:39,500 --> 00:07:44,510
else was cash control private so we were

00:07:43,160 --> 00:07:47,750
still hitting Apache for pretty much

00:07:44,510 --> 00:07:49,820
every request so the reason that vile

00:07:47,750 --> 00:07:52,370
exchanges that behavior is because if

00:07:49,820 --> 00:07:55,850
you hit the BBC property from outside

00:07:52,370 --> 00:07:58,640
the UK you'll get served an advert and

00:07:55,850 --> 00:08:00,590
you may well get redirected to BBC calm

00:07:58,640 --> 00:08:03,980
which is worldwide our commercial arm

00:08:00,590 --> 00:08:07,700
and so to try and get some better

00:08:03,980 --> 00:08:10,070
cashing out of that we used in another

00:08:07,700 --> 00:08:14,210
Apache module called mod annotate and

00:08:10,070 --> 00:08:17,660
what this would do is when you hit that

00:08:14,210 --> 00:08:20,240
I'll go on and come back so when you hit

00:08:17,660 --> 00:08:22,940
Apache with your request mod annotate

00:08:20,240 --> 00:08:26,180
would hit two data services we could hit

00:08:22,940 --> 00:08:29,660
more but generally hit hit 21 which was

00:08:26,180 --> 00:08:31,940
a gip service Java service layer which

00:08:29,660 --> 00:08:35,660
is clover which I think was mentioned in

00:08:31,940 --> 00:08:39,020
one of the talks yesterday and it also

00:08:35,660 --> 00:08:42,830
hits a service called Wuerffel well we

00:08:39,020 --> 00:08:44,930
call it Werfel Demi but it's the open

00:08:42,830 --> 00:08:46,970
source were for which is device data

00:08:44,930 --> 00:08:51,279
device metadata

00:08:46,970 --> 00:08:55,100
that adds some headers on to the request

00:08:51,279 --> 00:08:58,550
generally we roll up the IP info to just

00:08:55,100 --> 00:09:02,509
two headers one which is are you inside

00:08:58,550 --> 00:09:05,629
the UK yes or no and what type of device

00:09:02,509 --> 00:09:07,910
you are sorry for the gip it's whether

00:09:05,629 --> 00:09:09,560
you're inside the UK and whether we

00:09:07,910 --> 00:09:11,779
should serve adverts to you because you

00:09:09,560 --> 00:09:14,540
could still be outside the UK but we're

00:09:11,779 --> 00:09:16,459
not confident enough about your IP to

00:09:14,540 --> 00:09:19,220
actually show you adverts so there's a

00:09:16,459 --> 00:09:22,490
compromise in there the were full data

00:09:19,220 --> 00:09:27,319
set returns whether you're a desktop and

00:09:22,490 --> 00:09:29,149
mobile a television a tablet pc it also

00:09:27,319 --> 00:09:32,350
returns some metadata about whether your

00:09:29,149 --> 00:09:34,459
input type is touch keyboard mouse

00:09:32,350 --> 00:09:37,279
scroll wheel and all sorts of things

00:09:34,459 --> 00:09:39,800
like that that allows our PHP

00:09:37,279 --> 00:09:42,649
applications to then send a very back to

00:09:39,800 --> 00:09:44,540
say I very on whether I'm inside or

00:09:42,649 --> 00:09:48,350
outside the UK to change the header and

00:09:44,540 --> 00:09:49,879
footer and i also have four different

00:09:48,350 --> 00:09:52,399
versions to say whether it's a desktop

00:09:49,879 --> 00:09:56,480
version the mobile version or a TV

00:09:52,399 --> 00:09:59,120
version particularly iPlayer use that a

00:09:56,480 --> 00:10:02,149
lot we would get virtually no caching on

00:09:59,120 --> 00:10:04,430
I play without that because they serve

00:10:02,149 --> 00:10:07,699
up they actually do slightly more they

00:10:04,430 --> 00:10:10,550
the waffle also different returns a mod

00:10:07,699 --> 00:10:12,529
make and model of the television we

00:10:10,550 --> 00:10:16,100
spent quite a lot of time modifying

00:10:12,529 --> 00:10:17,899
Wuerffel to get manufacturer details for

00:10:16,100 --> 00:10:20,629
samsung and LG televisions and things

00:10:17,899 --> 00:10:22,550
every time every once a month we add

00:10:20,629 --> 00:10:25,309
about seven or eight new televisions to

00:10:22,550 --> 00:10:27,500
that and and then the iPlayer team

00:10:25,309 --> 00:10:30,829
decide whether you get version 4 or

00:10:27,500 --> 00:10:32,569
version 2 which is the old one and so

00:10:30,829 --> 00:10:35,389
yeah that goes then into mod cash and

00:10:32,569 --> 00:10:38,779
then because mod cash knows that the

00:10:35,389 --> 00:10:41,149
application varies on geoip and on the

00:10:38,779 --> 00:10:43,639
device type it can actually cash it and

00:10:41,149 --> 00:10:46,610
rather than invoke PHP it will just

00:10:43,639 --> 00:10:48,879
return it straight out of disk cache and

00:10:46,610 --> 00:10:52,490
then they get sent back to the response

00:10:48,879 --> 00:10:56,040
so this is what we used to do and

00:10:52,490 --> 00:10:58,200
we found that you're still hitting

00:10:56,040 --> 00:10:59,670
Apache and you're still incurring all

00:10:58,200 --> 00:11:04,230
the network latency and everything else

00:10:59,670 --> 00:11:06,230
and we had some it's not very easy to

00:11:04,230 --> 00:11:10,470
reconfigure mod cash it's literally a

00:11:06,230 --> 00:11:14,250
straight HTTP comply RFC compliant HTTP

00:11:10,470 --> 00:11:19,350
cache so and we wanted to do something a

00:11:14,250 --> 00:11:22,140
bit more special with it so we we

00:11:19,350 --> 00:11:24,660
introduced varnish so varnish was

00:11:22,140 --> 00:11:26,430
originally brought in to serve iPlayer

00:11:24,660 --> 00:11:30,410
content because they wanted to root for

00:11:26,430 --> 00:11:32,940
the televisions and mobile devices and

00:11:30,410 --> 00:11:36,600
varnish has a configuration language

00:11:32,940 --> 00:11:39,270
called vcl which quite handily you can

00:11:36,600 --> 00:11:41,310
drop down to see and you can embed your

00:11:39,270 --> 00:11:44,610
own libraries and your own c code within

00:11:41,310 --> 00:11:47,160
varnish so typical things we do in there

00:11:44,610 --> 00:11:53,280
is we normalize headers we normalize

00:11:47,160 --> 00:11:56,550
accept-encoding we normalize quite a few

00:11:53,280 --> 00:11:58,590
things we strip out some cookies and so

00:11:56,550 --> 00:12:00,870
the other thing is if you hit a BBC

00:11:58,590 --> 00:12:03,990
website so this is the other thing that

00:12:00,870 --> 00:12:07,890
mod cash fell down on if you hit a BBC

00:12:03,990 --> 00:12:09,840
website the first time you hit it if you

00:12:07,890 --> 00:12:12,960
don't have a cookie that's called BBC

00:12:09,840 --> 00:12:15,570
UID we issue a set cookie back to set

00:12:12,960 --> 00:12:18,900
you a random cookie not entirely sure

00:12:15,570 --> 00:12:21,750
what it's used for but it's actually an

00:12:18,900 --> 00:12:24,210
extension into Apache to set this cookie

00:12:21,750 --> 00:12:26,040
and it kind of defeats caching quite a

00:12:24,210 --> 00:12:28,470
lot because a set cookie cookie becomes

00:12:26,040 --> 00:12:32,730
a private cache-control private no cash

00:12:28,470 --> 00:12:35,160
request and so we've actually moved this

00:12:32,730 --> 00:12:37,770
logic into our varnish layer so varnish

00:12:35,160 --> 00:12:40,620
will rather than hit the back end go get

00:12:37,770 --> 00:12:42,690
them some content it will return that

00:12:40,620 --> 00:12:44,940
from cache and it will then modify the

00:12:42,690 --> 00:12:46,380
response to set the cookie and set the

00:12:44,940 --> 00:12:49,560
cache control to private again so

00:12:46,380 --> 00:12:51,360
upstream cashews don't cash it we strip

00:12:49,560 --> 00:12:53,300
out cookies from stuff that really don't

00:12:51,360 --> 00:12:57,709
need to have cookies I

00:12:53,300 --> 00:13:00,490
and we also have a problem a huge

00:12:57,709 --> 00:13:05,420
problem with people using cash busters

00:13:00,490 --> 00:13:07,370
so we discovered last week or two weeks

00:13:05,420 --> 00:13:09,380
ago that we were actually only doing

00:13:07,370 --> 00:13:11,630
this for the iPlayer I think parts of

00:13:09,380 --> 00:13:14,630
the iPlayer site will have a polling

00:13:11,630 --> 00:13:17,029
mechanism for now and next and because

00:13:14,630 --> 00:13:20,450
of problems in IE six seven eight and

00:13:17,029 --> 00:13:23,420
probably every other version if you use

00:13:20,450 --> 00:13:27,200
if you use an AJAX request to go fetch

00:13:23,420 --> 00:13:29,089
and json even though you tell in the

00:13:27,200 --> 00:13:32,600
response from the server cache control

00:13:29,089 --> 00:13:34,310
no cash ie aggressively caches that and

00:13:32,600 --> 00:13:37,430
the fix that everyone seems to do is

00:13:34,310 --> 00:13:39,230
append a random number on the end so we

00:13:37,430 --> 00:13:40,700
strip that out in our varnish layer so

00:13:39,230 --> 00:13:43,579
that we don't actually hit the back end

00:13:40,700 --> 00:13:45,950
because we know that we can cash it for

00:13:43,579 --> 00:13:48,320
ten seconds but the clients want to go

00:13:45,950 --> 00:13:51,070
and refresh it so that saved is quite a

00:13:48,320 --> 00:13:55,899
lot to load and we discovered that when

00:13:51,070 --> 00:13:59,649
sport pushed a new release of their oh

00:13:55,899 --> 00:14:03,380
it was the cricket the test match

00:13:59,649 --> 00:14:05,990
started a couple of weeks ago and the

00:14:03,380 --> 00:14:07,279
sport site found their homepage had a

00:14:05,990 --> 00:14:10,220
little ticker that got the latest

00:14:07,279 --> 00:14:12,730
cricket scores they had a cash plus the

00:14:10,220 --> 00:14:14,959
URL on the end of it and that generated

00:14:12,730 --> 00:14:17,990
15,000 requests per second to our

00:14:14,959 --> 00:14:21,440
back-end so we now implement this cash

00:14:17,990 --> 00:14:23,180
buster stripping on every request the

00:14:21,440 --> 00:14:26,000
other thing we can do with varnish which

00:14:23,180 --> 00:14:30,320
is quite nice is es is which is edge

00:14:26,000 --> 00:14:32,540
side includes so we can render the main

00:14:30,320 --> 00:14:36,020
page which has your customization on it

00:14:32,540 --> 00:14:38,810
that says hi Bob but then things like

00:14:36,020 --> 00:14:40,339
the weather is specific to London so

00:14:38,810 --> 00:14:42,260
everyone in London you get that and

00:14:40,339 --> 00:14:45,740
what's now and next is specific to

00:14:42,260 --> 00:14:48,709
everyone in the UK so we stick some tags

00:14:45,740 --> 00:14:51,019
in the page that says the page itself is

00:14:48,709 --> 00:14:52,880
only is not cacheable at all but these

00:14:51,019 --> 00:14:54,860
little sections of the page at cacheable

00:14:52,880 --> 00:14:58,220
for 30 seconds or the weather is

00:14:54,860 --> 00:15:00,350
cacheable for an hour or so and the

00:14:58,220 --> 00:15:02,690
varnish layer will go and fetch

00:15:00,350 --> 00:15:05,240
the page itself but then it will push in

00:15:02,690 --> 00:15:07,190
all the content from weather and things

00:15:05,240 --> 00:15:08,900
like that and it will serve it out in it

00:15:07,190 --> 00:15:10,940
or assemble the page in the cache and

00:15:08,900 --> 00:15:12,440
serve it out so you're only serving the

00:15:10,940 --> 00:15:14,060
bits of the page that need to be served

00:15:12,440 --> 00:15:18,440
everything else is still served from

00:15:14,060 --> 00:15:20,240
cache and whereas normally so if varnish

00:15:18,440 --> 00:15:22,100
disappeared what happens in this

00:15:20,240 --> 00:15:23,840
situation is we have to go and fetch

00:15:22,100 --> 00:15:26,570
that data for every single person

00:15:23,840 --> 00:15:28,340
assemble the page in PHP and push it out

00:15:26,570 --> 00:15:31,190
and it then becomes private because it's

00:15:28,340 --> 00:15:32,930
customized to me whereas this we can

00:15:31,190 --> 00:15:34,580
customize only the bits of the page that

00:15:32,930 --> 00:15:41,510
need to be customized everything else is

00:15:34,580 --> 00:15:46,840
cached again we do the geoip data and we

00:15:41,510 --> 00:15:49,610
do the device data inside varnish and

00:15:46,840 --> 00:15:53,620
what that looks like is so this is a

00:15:49,610 --> 00:15:56,540
normal request hitting BBC iplayer and

00:15:53,620 --> 00:15:58,250
so this the top one is your request from

00:15:56,540 --> 00:16:00,680
the browser the bottom one is the

00:15:58,250 --> 00:16:03,020
response from the server we're saying

00:16:00,680 --> 00:16:07,040
it's cashable for 30 seconds the user

00:16:03,020 --> 00:16:09,920
agent is whatever we have the accept

00:16:07,040 --> 00:16:12,290
type when we have accept encoding so if

00:16:09,920 --> 00:16:14,690
in a normal cash if you're accepting

00:16:12,290 --> 00:16:16,340
coding there was g5g deflate first and

00:16:14,690 --> 00:16:18,890
then gzip you had two different

00:16:16,340 --> 00:16:20,600
variations of your paging you cash so we

00:16:18,890 --> 00:16:23,120
normalize that and I think we check for

00:16:20,600 --> 00:16:25,160
gzip first if you support gzip you get

00:16:23,120 --> 00:16:29,900
DS it if you don't support gzip but you

00:16:25,160 --> 00:16:32,810
should support deflate you get deflate I

00:16:29,900 --> 00:16:35,780
think there's really the only two things

00:16:32,810 --> 00:16:38,030
we do and that's your normal response if

00:16:35,780 --> 00:16:43,640
you're then going through varnish what

00:16:38,030 --> 00:16:46,010
you'll see is what the PHP services we

00:16:43,640 --> 00:16:48,860
get the normal user agent information as

00:16:46,010 --> 00:16:52,130
before but now we have this device was

00:16:48,860 --> 00:16:55,190
actually a desktop it was inside it was

00:16:52,130 --> 00:16:59,360
actually in the country UK so it is UK

00:16:55,190 --> 00:17:02,620
combined that so Northern Ireland or

00:16:59,360 --> 00:17:05,600
Scotland Wales all the rest of it to

00:17:02,620 --> 00:17:07,280
turn to be UK combined I think there

00:17:05,600 --> 00:17:11,060
might be some overseas territories that

00:17:07,280 --> 00:17:11,709
also get UK combined and then we tell it

00:17:11,060 --> 00:17:15,760
that we

00:17:11,709 --> 00:17:19,959
want to serve adverts to this page sorry

00:17:15,760 --> 00:17:22,240
and then in the response the server is

00:17:19,959 --> 00:17:24,760
now told the cache which is varnish in

00:17:22,240 --> 00:17:27,279
this case that I'm going to vary on the

00:17:24,760 --> 00:17:30,100
device type and is UK combined so

00:17:27,279 --> 00:17:32,200
basically I have a international version

00:17:30,100 --> 00:17:34,360
and I have a domestic version and I've

00:17:32,200 --> 00:17:37,120
bearing on whether it's a mobile device

00:17:34,360 --> 00:17:39,159
or whether it's a desktop so that will

00:17:37,120 --> 00:17:43,210
also create different versions in the

00:17:39,159 --> 00:17:46,659
cache so we'll have to two copies for is

00:17:43,210 --> 00:17:49,929
UK combined so we'll have the two copies

00:17:46,659 --> 00:17:53,169
for internal domestic and international

00:17:49,929 --> 00:17:54,850
and then we'll have up to four copies so

00:17:53,169 --> 00:17:56,080
who multiply those together you've got

00:17:54,850 --> 00:17:59,140
eight different copies of the page

00:17:56,080 --> 00:18:01,899
cached but we can serve out of the cash

00:17:59,140 --> 00:18:04,600
in half a millisecond whereas hitting

00:18:01,899 --> 00:18:11,799
the power to render the page is 200 to

00:18:04,600 --> 00:18:13,090
300 milliseconds so that's how we try

00:18:11,799 --> 00:18:15,190
and make our site is cashable as

00:18:13,090 --> 00:18:17,440
possible and we can use less servers

00:18:15,190 --> 00:18:20,169
because we can deliver as much content

00:18:17,440 --> 00:18:22,120
as we can out to cash one of the

00:18:20,169 --> 00:18:25,020
problems we got at the minute is we

00:18:22,120 --> 00:18:31,620
don't really know what's going on and

00:18:25,020 --> 00:18:37,000
our monitoring system is the xenos and

00:18:31,620 --> 00:18:40,539
we basically log data as SNMP style key

00:18:37,000 --> 00:18:42,820
performance indicators into xenos and we

00:18:40,539 --> 00:18:44,140
have apache error logs well and that's

00:18:42,820 --> 00:18:46,600
pretty much all the monitoring we have

00:18:44,140 --> 00:18:50,710
at the minute so if your application

00:18:46,600 --> 00:18:52,690
errors when you get to 500 we won't know

00:18:50,710 --> 00:18:54,809
about it for five minutes and then all

00:18:52,690 --> 00:19:00,120
will get is a little blip on the graph

00:18:54,809 --> 00:19:02,649
and it's really not very suitable so

00:19:00,120 --> 00:19:04,059
what we really want is we want purpose

00:19:02,649 --> 00:19:08,820
so we do get that information per

00:19:04,059 --> 00:19:12,460
application so we see there were 20 20

00:19:08,820 --> 00:19:13,929
sorry 20 error 500 so I player in the

00:19:12,460 --> 00:19:17,350
last five minutes but we don't get much

00:19:13,929 --> 00:19:18,880
more than that and a lot of the

00:19:17,350 --> 00:19:20,409
applications can log as much data as

00:19:18,880 --> 00:19:21,620
they like so they cut a number of

00:19:20,409 --> 00:19:24,980
service calls they make

00:19:21,620 --> 00:19:29,230
it took but the frequency thing is

00:19:24,980 --> 00:19:29,230
really killing us so and

00:19:30,520 --> 00:19:35,310
so what are we doing to improve this

00:19:39,490 --> 00:19:47,110
so we're still using the apache logs we

00:19:42,520 --> 00:19:48,850
looked at and using I forget there's an

00:19:47,110 --> 00:19:50,500
internal PHP function that I've

00:19:48,850 --> 00:19:52,390
forgotten the name of now they allows

00:19:50,500 --> 00:19:56,230
you to augment extra data into the

00:19:52,390 --> 00:19:59,620
apache logs but you're still limited to

00:19:56,230 --> 00:20:00,790
a single log line and it gets very

00:19:59,620 --> 00:20:02,110
difficult to try and pass that

00:20:00,790 --> 00:20:04,030
information and then you have to have

00:20:02,110 --> 00:20:06,250
data warehouse to process it we don't

00:20:04,030 --> 00:20:10,480
have all that information so we came up

00:20:06,250 --> 00:20:12,010
with some other prototypes and so we

00:20:10,480 --> 00:20:16,929
came up we thought we'd do this in

00:20:12,010 --> 00:20:19,990
varnish because we can get access to low

00:20:16,929 --> 00:20:23,650
level stuff using C libraries inside

00:20:19,990 --> 00:20:25,780
varnish so what we've done is we are now

00:20:23,650 --> 00:20:28,390
implementing per application statistics

00:20:25,780 --> 00:20:29,860
in varnish varnish out the box is really

00:20:28,390 --> 00:20:32,920
designed to sit in front of one website

00:20:29,860 --> 00:20:34,960
which technically we are but you

00:20:32,920 --> 00:20:38,470
basically get a big blob of data that is

00:20:34,960 --> 00:20:40,600
cache hits cache misses and things like

00:20:38,470 --> 00:20:42,100
that and we wanted more granularity on

00:20:40,600 --> 00:20:44,320
what exactly is happening inside our

00:20:42,100 --> 00:20:46,900
caching layer we'd also like to know

00:20:44,320 --> 00:20:48,910
that if we look at varnish at the minute

00:20:46,900 --> 00:20:52,510
we see we get about a thirty percent hit

00:20:48,910 --> 00:20:55,570
rate which seems really really low and

00:20:52,510 --> 00:20:57,160
we don't know why that is so we can't

00:20:55,570 --> 00:21:01,090
really investigate because the varnish

00:20:57,160 --> 00:21:03,010
logs are huge and incomprehensible so

00:21:01,090 --> 00:21:05,230
we've implemented something we called

00:21:03,010 --> 00:21:07,240
stats D which we've also found has been

00:21:05,230 --> 00:21:09,640
implemented by other companies like Etsy

00:21:07,240 --> 00:21:11,500
as well and ours is very similar and

00:21:09,640 --> 00:21:12,870
we're kind of migrating to make it even

00:21:11,500 --> 00:21:17,920
more similar so they're interchangeable

00:21:12,870 --> 00:21:21,220
and what we've done is we do hit missing

00:21:17,920 --> 00:21:23,170
pass ratios inside varnish and we do

00:21:21,220 --> 00:21:25,960
interquartile ranges and we do it

00:21:23,170 --> 00:21:28,120
updating every 10 seconds which is

00:21:25,960 --> 00:21:32,930
significantly better than the five

00:21:28,120 --> 00:21:36,590
minutes we get out of xenos

00:21:32,930 --> 00:21:41,930
so this is a quick look at our part of

00:21:36,590 --> 00:21:43,520
our varnish vcl for the BBC so we drop

00:21:41,930 --> 00:21:46,130
in to see the curly bracket with the sea

00:21:43,520 --> 00:21:48,290
just says I'm now dropping out of vcl

00:21:46,130 --> 00:21:52,160
language Aminta see we include a whole

00:21:48,290 --> 00:21:56,500
load of standard C libraries we set a

00:21:52,160 --> 00:22:00,920
purport to our to a socket which is a

00:21:56,500 --> 00:22:02,960
FIFO on disk and then we have a cook we

00:22:00,920 --> 00:22:04,700
basically just have three functions we

00:22:02,960 --> 00:22:06,590
have one function called get file

00:22:04,700 --> 00:22:08,360
descriptor we have one that says

00:22:06,590 --> 00:22:12,980
increase a counter and another that says

00:22:08,360 --> 00:22:15,580
close the file descriptor the get file

00:22:12,980 --> 00:22:18,860
descriptor and so varnish is

00:22:15,580 --> 00:22:20,420
multi-threaded we have about 4,000 or

00:22:18,860 --> 00:22:23,360
six thousand threads running inside

00:22:20,420 --> 00:22:27,530
varnish so we have to what we have to

00:22:23,360 --> 00:22:28,910
make sure that we we don't step on any

00:22:27,530 --> 00:22:31,970
with other threads toes while we're

00:22:28,910 --> 00:22:36,080
doing this so we really don't care too

00:22:31,970 --> 00:22:37,580
much if we throw data away so i won't go

00:22:36,080 --> 00:22:39,860
through this particularly but all it's

00:22:37,580 --> 00:22:43,160
doing is opening a socket to a FIFO on

00:22:39,860 --> 00:22:47,720
disk and then it's returning a status to

00:22:43,160 --> 00:22:51,320
say yeah here's your 50 when we increase

00:22:47,720 --> 00:22:53,150
the counter what we do is we pass a blob

00:22:51,320 --> 00:22:55,550
of texting with a bit on the front that

00:22:53,150 --> 00:23:00,200
says this is the application this is

00:22:55,550 --> 00:23:01,910
what we're doing I'll just skip this

00:23:00,200 --> 00:23:03,230
really if you want this the slides are

00:23:01,910 --> 00:23:04,640
on get healthy you can have a look at

00:23:03,230 --> 00:23:08,630
the code and have a look at our

00:23:04,640 --> 00:23:11,300
implementation and but this is really

00:23:08,630 --> 00:23:13,100
what we do so we have in vcl deliver

00:23:11,300 --> 00:23:18,500
which is when we're pushing content back

00:23:13,100 --> 00:23:22,520
to the client if we what we've done in

00:23:18,500 --> 00:23:24,860
our PHP layer is the PHP layer sends

00:23:22,520 --> 00:23:27,230
back to varnish an extra header which we

00:23:24,860 --> 00:23:29,360
called ex web app that identifies the

00:23:27,230 --> 00:23:32,230
page that is being served so in this

00:23:29,360 --> 00:23:35,450
case it will be I player or home page or

00:23:32,230 --> 00:23:37,100
CBBC or something like that so that we

00:23:35,450 --> 00:23:38,530
know exactly which a web application was

00:23:37,100 --> 00:23:42,980
being served out of varnish

00:23:38,530 --> 00:23:45,440
and then we we just pass into our

00:23:42,980 --> 00:23:50,000
increased counter that with this was a

00:23:45,440 --> 00:23:54,620
cash pass or this was a so we we

00:23:50,000 --> 00:23:56,720
maintain more accurate and technically

00:23:54,620 --> 00:23:59,390
more rocky I guess whatever hit miss and

00:23:56,720 --> 00:24:01,400
pass ratios the reason that we do this

00:23:59,390 --> 00:24:03,830
is so varnish will just give you a

00:24:01,400 --> 00:24:06,590
hit-and-miss ratio we want to know why

00:24:03,830 --> 00:24:10,280
things missed and we also want to know

00:24:06,590 --> 00:24:12,559
why things past I don't go cover this

00:24:10,280 --> 00:24:15,880
terminology a pass in varnish language

00:24:12,559 --> 00:24:19,400
is something that would have been a hit

00:24:15,880 --> 00:24:21,020
but for some rule in your logic that

00:24:19,400 --> 00:24:25,160
tells you how to manage your your real

00:24:21,020 --> 00:24:26,630
requests you decided not to serve it as

00:24:25,160 --> 00:24:28,820
a hit and you actually went back to the

00:24:26,630 --> 00:24:32,419
backend and got the data normally that's

00:24:28,820 --> 00:24:34,040
because the backend said that this page

00:24:32,419 --> 00:24:36,860
is not cashable so I want you to always

00:24:34,040 --> 00:24:39,200
go fetch it or it could be because the

00:24:36,860 --> 00:24:40,610
server issued a set cookie because

00:24:39,200 --> 00:24:43,400
somebody changed their user preferences

00:24:40,610 --> 00:24:45,169
or logged in or something like that so

00:24:43,400 --> 00:24:47,390
we record all of these reasons and we

00:24:45,169 --> 00:24:49,130
push them into our stats so that we can

00:24:47,390 --> 00:24:52,460
actually see that the reason that that

00:24:49,130 --> 00:24:56,030
CBBC is not cashing it well and varnish

00:24:52,460 --> 00:24:57,799
is because they keep issuing set cookies

00:24:56,030 --> 00:25:02,059
on every page request or something like

00:24:57,799 --> 00:25:04,910
that which we have found before and then

00:25:02,059 --> 00:25:10,730
also we do when we push that into our

00:25:04,910 --> 00:25:13,370
stats monitoring system which wants the

00:25:10,730 --> 00:25:15,470
peace yeah this was just to cover all

00:25:13,370 --> 00:25:17,360
the bases so this is what happens when

00:25:15,470 --> 00:25:18,950
we actually do get a hit we just pushing

00:25:17,360 --> 00:25:21,380
so yeah we've got a hit on this

00:25:18,950 --> 00:25:26,860
application we also got a hit in total

00:25:21,380 --> 00:25:26,860
for varnish right

00:25:31,620 --> 00:25:39,080
so now we're generating all this data

00:25:34,050 --> 00:25:41,520
and what are we doing with it so it's

00:25:39,080 --> 00:25:42,660
all very well to have this stuff

00:25:41,520 --> 00:25:43,680
generated but we've got to put it

00:25:42,660 --> 00:25:45,840
somewhere I'm going to find out what

00:25:43,680 --> 00:25:49,110
exactly is happening so the other thing

00:25:45,840 --> 00:25:55,890
we've implemented is XH kroff which is

00:25:49,110 --> 00:25:58,380
the facebooks kind of slim down cash

00:25:55,890 --> 00:25:59,820
grind XD bug type output but it is

00:25:58,380 --> 00:26:03,720
designed to be run into prison

00:25:59,820 --> 00:26:06,929
production and so we are actually doing

00:26:03,720 --> 00:26:09,030
that we're sampling one in every 10,000

00:26:06,929 --> 00:26:11,880
requests at the minute because we only

00:26:09,030 --> 00:26:14,309
have a kind of development environment

00:26:11,880 --> 00:26:17,370
setup for handling the storage of this

00:26:14,309 --> 00:26:19,860
at the minute once we've sampled it and

00:26:17,370 --> 00:26:22,290
we've seen how much data we're getting

00:26:19,860 --> 00:26:24,059
and how easy it is to process we may

00:26:22,290 --> 00:26:26,550
scale that up I mean ideally we'd like

00:26:24,059 --> 00:26:29,880
to process every single request but that

00:26:26,550 --> 00:26:33,120
may probably not be practical and we're

00:26:29,880 --> 00:26:37,290
using the XH prof GUI which is the pool

00:26:33,120 --> 00:26:38,730
Rhine Homer clone of it on github we've

00:26:37,290 --> 00:26:41,730
made some amendments to it that we're

00:26:38,730 --> 00:26:43,050
going to be pushing back I'll just give

00:26:41,730 --> 00:26:47,340
you a quick demo that because I think

00:26:43,050 --> 00:26:51,260
I've got it up already so this is on our

00:26:47,340 --> 00:26:51,260
test environment that looks nice

00:26:54,190 --> 00:26:58,179
so this is actually running on our

00:26:56,049 --> 00:27:03,629
testing but environment it really

00:26:58,179 --> 00:27:03,629
doesn't look very pretty but if we look

00:27:04,230 --> 00:27:09,549
down here so you see we get some really

00:27:06,610 --> 00:27:13,389
huge URLs and things in here but if we

00:27:09,549 --> 00:27:17,980
look at say I player so this is all the

00:27:13,389 --> 00:27:19,539
iPlayer requests and we can see the CPU

00:27:17,980 --> 00:27:22,029
time and these are all individual

00:27:19,539 --> 00:27:24,159
processes and how much CPU and how much

00:27:22,029 --> 00:27:28,679
memory was taken and we can drill into

00:27:24,159 --> 00:27:28,679
here with any luck

00:27:29,630 --> 00:27:33,770
profile

00:27:31,370 --> 00:27:35,270
and we then can go down and look at an

00:27:33,770 --> 00:27:36,830
individual request we get our nice

00:27:35,270 --> 00:27:39,250
pretty cool graph which looks a bit

00:27:36,830 --> 00:27:42,530
better if your screen resolutions higher

00:27:39,250 --> 00:27:47,680
we can even drill down and see BBC

00:27:42,530 --> 00:27:50,240
autoloader is called 139 times we get

00:27:47,680 --> 00:27:52,280
which which cook which functions were

00:27:50,240 --> 00:27:53,930
calling it so if you're used to looking

00:27:52,280 --> 00:27:55,880
at cash grind outputs or things like

00:27:53,930 --> 00:27:59,180
that it's pretty much the same sort of

00:27:55,880 --> 00:28:00,350
thing but in a web interface so we're

00:27:59,180 --> 00:28:03,740
storing that at the minute in a my

00:28:00,350 --> 00:28:05,980
sequel database and we're looking at

00:28:03,740 --> 00:28:09,380
other alternatives for storing that

00:28:05,980 --> 00:28:14,450
we're aggregating this across all of the

00:28:09,380 --> 00:28:16,940
BBC platforms we don't actually store

00:28:14,450 --> 00:28:18,470
this on disk and collect it off the

00:28:16,940 --> 00:28:24,110
server's we use something called

00:28:18,470 --> 00:28:26,690
teleport d so we have a FIFO poor FIFO

00:28:24,110 --> 00:28:28,480
pipe on disk so we're not actually

00:28:26,690 --> 00:28:31,630
touching the disk at all with this

00:28:28,480 --> 00:28:35,179
teleport D is a transport mechanism

00:28:31,630 --> 00:28:37,910
basically it talked to a FIFO on the

00:28:35,179 --> 00:28:40,250
disk any data you push in it sticks out

00:28:37,910 --> 00:28:42,170
over a TCP connection if there's nothing

00:28:40,250 --> 00:28:44,900
listening at the other end it just

00:28:42,170 --> 00:28:46,910
dropped it and it but if there is

00:28:44,900 --> 00:28:48,770
something is so you have 50 on your

00:28:46,910 --> 00:28:51,710
destination and you have a client

00:28:48,770 --> 00:28:53,780
listening and it just pull stuff out we

00:28:51,710 --> 00:28:56,030
we send all of this information to

00:28:53,780 --> 00:28:59,830
something we call internally the

00:28:56,030 --> 00:29:02,000
Schrodinger logs which is basically if

00:28:59,830 --> 00:29:04,820
nobody's there to look at it it doesn't

00:29:02,000 --> 00:29:07,809
happen because like I said teleport d if

00:29:04,820 --> 00:29:11,360
nothing's listening just drops it so and

00:29:07,809 --> 00:29:14,990
we then have our server that is running

00:29:11,360 --> 00:29:17,360
this the XH prof GUI as a Python a

00:29:14,990 --> 00:29:21,770
script that sits and listens to the pipe

00:29:17,360 --> 00:29:25,490
and we've had to come across some

00:29:21,770 --> 00:29:28,040
hurdles because in PHP way in fact any

00:29:25,490 --> 00:29:32,570
language pushing data to a pipe on disk

00:29:28,040 --> 00:29:35,900
or in fact any file descriptor if you do

00:29:32,570 --> 00:29:39,320
a F right it doesn't necessarily write

00:29:35,900 --> 00:29:42,020
the whole content in one go depending on

00:29:39,320 --> 00:29:42,530
your operating system you may get 64

00:29:42,020 --> 00:29:45,740
bytes

00:29:42,530 --> 00:29:49,640
or you may get 512 bytes or whatever so

00:29:45,740 --> 00:29:52,580
we have to we have to find out a decent

00:29:49,640 --> 00:29:54,230
way to flush that to disk which is well

00:29:52,580 --> 00:29:56,120
that's pretty simple the problem we had

00:29:54,230 --> 00:30:01,100
was actually decoding it at the other

00:29:56,120 --> 00:30:05,060
end and because we're multiplexing

00:30:01,100 --> 00:30:07,990
streams from all of our 40 boxes and we

00:30:05,060 --> 00:30:10,250
the logging portal combines all those

00:30:07,990 --> 00:30:12,470
five rows from all the different servers

00:30:10,250 --> 00:30:14,630
into one port so we don't have to make

00:30:12,470 --> 00:30:17,300
40 connections to listen to the logs we

00:30:14,630 --> 00:30:18,650
only make one connection and but it

00:30:17,300 --> 00:30:21,830
means that they're all inter spliced

00:30:18,650 --> 00:30:28,190
with each other so I'll talk very

00:30:21,830 --> 00:30:29,180
briefly on there on how we do that sorry

00:30:28,190 --> 00:30:32,210
I haven't actually done this

00:30:29,180 --> 00:30:33,860
presentation before so i apologize if

00:30:32,210 --> 00:30:38,780
things are a little bit garbled and out

00:30:33,860 --> 00:30:41,990
of order and at the minute we generate

00:30:38,780 --> 00:30:46,250
the profile ID which is the unique

00:30:41,990 --> 00:30:48,590
reference to a single trace in the PHP

00:30:46,250 --> 00:30:51,290
layer and we pass that into our data

00:30:48,590 --> 00:30:53,150
transport to identify this stream and we

00:30:51,290 --> 00:30:57,350
pass it over the teleport d system we're

00:30:53,150 --> 00:31:02,990
talking about because x h prof generates

00:30:57,350 --> 00:31:05,150
a json object it can be quite large if

00:31:02,990 --> 00:31:07,190
you saw the call trace information all

00:31:05,150 --> 00:31:09,830
that timing information memory Delta's

00:31:07,190 --> 00:31:12,440
who called what when how many times all

00:31:09,830 --> 00:31:15,380
that is encoding a JSON object it tends

00:31:12,440 --> 00:31:22,360
to be and getting on for a Meg of

00:31:15,380 --> 00:31:24,320
gzipped Jason so yeah it's it's not tiny

00:31:22,360 --> 00:31:26,690
that's the other reason that we don't

00:31:24,320 --> 00:31:28,280
hit disk with it because we don't want

00:31:26,690 --> 00:31:30,170
to incur the latency on our front ends

00:31:28,280 --> 00:31:32,030
we just stick it straight out over the

00:31:30,170 --> 00:31:35,070
TCP connection

00:31:32,030 --> 00:31:36,630
so we gzip Ouija zip it and then we add

00:31:35,070 --> 00:31:41,310
a data framing so we generate the

00:31:36,630 --> 00:31:43,590
profile ID we then send a line that says

00:31:41,310 --> 00:31:45,900
this is the profile ID this is how much

00:31:43,590 --> 00:31:48,960
data I'm going to send you we send that

00:31:45,900 --> 00:31:55,890
straight off then after that we chunk up

00:31:48,960 --> 00:31:58,920
all the data we chunk up all the data on

00:31:55,890 --> 00:32:01,700
new lines and then we insert on the

00:31:58,920 --> 00:32:04,320
front of the new line we insert the

00:32:01,700 --> 00:32:06,960
profile ID or I think the thing is the

00:32:04,320 --> 00:32:08,880
profile ID then a bar and then we send

00:32:06,960 --> 00:32:11,310
the rest of the data then the script at

00:32:08,880 --> 00:32:14,720
the other end picks out the profile ID

00:32:11,310 --> 00:32:18,930
says oh I'm expecting 50 4 bytes of data

00:32:14,720 --> 00:32:21,630
54 kilobytes of data and then it loop

00:32:18,930 --> 00:32:24,930
then it basically assigns that into a

00:32:21,630 --> 00:32:26,220
bucket and then it d-mail demultiplex is

00:32:24,930 --> 00:32:28,740
all the different lines as they come in

00:32:26,220 --> 00:32:31,200
looks at the profile IDs the pen strips

00:32:28,740 --> 00:32:33,000
off the data framing header and appends

00:32:31,200 --> 00:32:36,600
it all to the content once we've got a

00:32:33,000 --> 00:32:38,340
full frame as in we've got our 5456

00:32:36,600 --> 00:32:42,210
kilobytes of data or whatever we're

00:32:38,340 --> 00:32:44,100
expecting within mg zip it and then json

00:32:42,210 --> 00:32:47,280
decode it and then we write it into

00:32:44,100 --> 00:32:51,540
whatever system we're going to store it

00:32:47,280 --> 00:32:54,330
on and we write as much as we can and we

00:32:51,540 --> 00:32:55,590
just throw away any errors because at

00:32:54,330 --> 00:32:58,140
the end of the day is profiling

00:32:55,590 --> 00:33:00,750
information with something 1 in 10,000

00:32:58,140 --> 00:33:03,000
or whatever if we miss one it really

00:33:00,750 --> 00:33:04,980
doesn't matter if there's any errors in

00:33:03,000 --> 00:33:06,570
the gzip being we just don't care we

00:33:04,980 --> 00:33:08,010
don't want to investigate errors

00:33:06,570 --> 00:33:10,290
particularly unless it all stops

00:33:08,010 --> 00:33:12,990
completely so we pretty much throw away

00:33:10,290 --> 00:33:16,170
everything and this is the amendment we

00:33:12,990 --> 00:33:22,830
made on to the end of the PHP XH prof

00:33:16,170 --> 00:33:24,570
stuff so we open open our pipe and if we

00:33:22,830 --> 00:33:26,190
actually do login error but we just say

00:33:24,570 --> 00:33:29,760
couldn't open the pipe it just gives us

00:33:26,190 --> 00:33:32,280
something to look at we set say that we

00:33:29,760 --> 00:33:34,920
are going to not block so we don't want

00:33:32,280 --> 00:33:37,410
to cause any blocking on here we G

00:33:34,920 --> 00:33:40,440
zipping a JSON encode our data we'd use

00:33:37,410 --> 00:33:41,600
it that we work out the length of it and

00:33:40,440 --> 00:33:44,059
then we

00:33:41,600 --> 00:33:47,240
right the size of the data that we've

00:33:44,059 --> 00:33:48,650
got and then we do so once we've gzipped

00:33:47,240 --> 00:33:50,299
all our data there's going to be

00:33:48,650 --> 00:33:52,280
carriage returns in there because just

00:33:50,299 --> 00:33:54,980
random garbage you've things being

00:33:52,280 --> 00:33:59,419
translated into gzip so whenever we find

00:33:54,980 --> 00:34:01,460
a new line we prefix it with the size of

00:33:59,419 --> 00:34:03,200
the profile and then we flushed out the

00:34:01,460 --> 00:34:05,030
buffer and then once we done and if

00:34:03,200 --> 00:34:08,570
anything there we close the socket them

00:34:05,030 --> 00:34:10,250
and if anything goes wrong we do

00:34:08,570 --> 00:34:14,720
actually log an error I didn't think we

00:34:10,250 --> 00:34:16,940
did but there you go so that's how we

00:34:14,720 --> 00:34:20,389
handle our XH broth and we get that into

00:34:16,940 --> 00:34:23,990
our ath prof GUI and we thought we could

00:34:20,389 --> 00:34:27,260
do better than this because XH prof will

00:34:23,990 --> 00:34:29,960
give you and typically for most of our

00:34:27,260 --> 00:34:33,889
applications it will show you that you

00:34:29,960 --> 00:34:36,590
spent you made a request to curl three

00:34:33,889 --> 00:34:40,609
times in your page and you spent 200

00:34:36,590 --> 00:34:42,980
milliseconds in curl and we didn't know

00:34:40,609 --> 00:34:47,629
kind of what we wanted to find out what

00:34:42,980 --> 00:34:50,600
was really happening so and we can cross

00:34:47,629 --> 00:34:53,950
something called the HTTP archive format

00:34:50,600 --> 00:34:56,750
which if you've ever used Firebug on

00:34:53,950 --> 00:34:59,300
Firefox is the waterfall diagram you get

00:34:56,750 --> 00:35:00,920
that shows you your CSS and JavaScript

00:34:59,300 --> 00:35:02,210
loading and how long it took to download

00:35:00,920 --> 00:35:05,000
an image and how many things are done in

00:35:02,210 --> 00:35:06,950
parallel all that kind of information so

00:35:05,000 --> 00:35:10,520
that is all encapsulated in a format

00:35:06,950 --> 00:35:12,830
called HTTP archive so as we already

00:35:10,520 --> 00:35:14,480
have this transport available to us for

00:35:12,830 --> 00:35:17,750
teleport d we thought we'd see what we

00:35:14,480 --> 00:35:23,210
could do with this and what we generate

00:35:17,750 --> 00:35:25,550
is we've extended so we're using zend

00:35:23,210 --> 00:35:28,880
framework and we have our power layer on

00:35:25,550 --> 00:35:32,240
top of this and we have a BBC HTTP

00:35:28,880 --> 00:35:35,050
client that extends the zend HTTP client

00:35:32,240 --> 00:35:38,510
so we hooked in some metrics in there

00:35:35,050 --> 00:35:41,450
and we basically start a micro timer

00:35:38,510 --> 00:35:43,160
before we make a request and then when

00:35:41,450 --> 00:35:45,920
with request is finished we stop the

00:35:43,160 --> 00:35:48,650
micro timer we also take a copy of the

00:35:45,920 --> 00:35:53,010
curl handle and pass that into our

00:35:48,650 --> 00:35:55,550
performance monitoring class and then

00:35:53,010 --> 00:35:59,910
that generates an HTTP archive format

00:35:55,550 --> 00:36:04,740
file for what is happening in we also

00:35:59,910 --> 00:36:06,570
did the same for zend cash call so we

00:36:04,740 --> 00:36:09,840
now get all the metrics for what's

00:36:06,570 --> 00:36:13,230
happening in memcache and rather than

00:36:09,840 --> 00:36:14,700
post this data into a my sequel database

00:36:13,230 --> 00:36:16,500
we're actually posting you this into our

00:36:14,700 --> 00:36:18,870
key value store which is actually

00:36:16,500 --> 00:36:21,480
couchdb but with a Mickey Mouse front

00:36:18,870 --> 00:36:27,210
end on it that basically only allows you

00:36:21,480 --> 00:36:28,410
to post and fetch and delete reason

00:36:27,210 --> 00:36:30,600
we're doing this is that we'd like to

00:36:28,410 --> 00:36:33,780
you take advantage of cash DB views to

00:36:30,600 --> 00:36:36,060
actually and get an idea of what is

00:36:33,780 --> 00:36:38,910
happening with our requests and who the

00:36:36,060 --> 00:36:43,290
most expensive requests are so this is a

00:36:38,910 --> 00:36:47,700
very very sport with a cache cluster on

00:36:43,290 --> 00:36:50,330
the end and so this is a very quick view

00:36:47,700 --> 00:36:54,840
of the kind of output that we get from

00:36:50,330 --> 00:36:57,660
our this is a single page request so we

00:36:54,840 --> 00:37:00,270
they were making a call to memcache that

00:36:57,660 --> 00:37:03,240
did an increment on a counter that took

00:37:00,270 --> 00:37:05,220
0 milliseconds the the next one down is

00:37:03,240 --> 00:37:07,680
a get so they're actually fetching data

00:37:05,220 --> 00:37:11,010
you can't quite see it but it was making

00:37:07,680 --> 00:37:13,530
call to API BBC and the status on there

00:37:11,010 --> 00:37:16,440
is 30 for mem cache hit and it took Milt

00:37:13,530 --> 00:37:18,120
six milliseconds and then they did

00:37:16,440 --> 00:37:20,490
actually her fetching something from

00:37:18,120 --> 00:37:22,680
memcache as well which took another five

00:37:20,490 --> 00:37:27,330
milliseconds and if we have a look at

00:37:22,680 --> 00:37:30,570
that expanded the get request would have

00:37:27,330 --> 00:37:34,400
been a curl request but because of the

00:37:30,570 --> 00:37:37,940
way we've extended the zend HTTP client

00:37:34,400 --> 00:37:42,420
we've stuck some rappers in there to

00:37:37,940 --> 00:37:44,520
enable our client SSL connections

00:37:42,420 --> 00:37:45,810
because I don't think was ends does that

00:37:44,520 --> 00:37:48,060
well certainly the version has end with

00:37:45,810 --> 00:37:49,520
it running doesn't do that by default so

00:37:48,060 --> 00:37:53,099
I'm

00:37:49,520 --> 00:37:55,859
we've also augmented it so that by

00:37:53,099 --> 00:37:57,480
default it will look up the data in

00:37:55,859 --> 00:38:00,570
memcache before he actually makes the

00:37:57,480 --> 00:38:02,220
curl request so if somebody on their PHP

00:38:00,570 --> 00:38:05,579
application is making lots of requests

00:38:02,220 --> 00:38:07,470
to the same curl resource when it

00:38:05,579 --> 00:38:10,470
receives the response if the response is

00:38:07,470 --> 00:38:12,720
cacheable it will take a hash of the URL

00:38:10,470 --> 00:38:14,730
and it will push it into memcache so

00:38:12,720 --> 00:38:16,170
before we make a curl request we go

00:38:14,730 --> 00:38:18,450
check memcache to see if there's a cache

00:38:16,170 --> 00:38:21,150
copy in there and we return that and we

00:38:18,450 --> 00:38:24,500
save ourselves about 100 milliseconds

00:38:21,150 --> 00:38:27,210
normally so this was a cached response

00:38:24,500 --> 00:38:30,359
but we can see that we actually got the

00:38:27,210 --> 00:38:32,820
response and request headers that were

00:38:30,359 --> 00:38:35,250
actually sent or were sent originally to

00:38:32,820 --> 00:38:38,579
the the back-end service so you can see

00:38:35,250 --> 00:38:41,940
the max age 60 it was cached for 60

00:38:38,579 --> 00:38:46,890
seconds there was an e-tag in there and

00:38:41,940 --> 00:38:50,280
all that kind of stuff right so I said

00:38:46,890 --> 00:38:52,260
we are extending zend HTTP client I

00:38:50,280 --> 00:38:54,359
think I've already basically mentioned

00:38:52,260 --> 00:38:57,380
all of that I've probably already

00:38:54,359 --> 00:38:57,380
mentioned this one as well

00:38:58,860 --> 00:39:04,200
yeah so we extent we already extended

00:39:01,320 --> 00:39:08,220
BBC cash core as well to have to

00:39:04,200 --> 00:39:10,350
implement our own hashing algorithm so

00:39:08,220 --> 00:39:13,350
we've now just added on performance

00:39:10,350 --> 00:39:18,360
information we can also get information

00:39:13,350 --> 00:39:20,550
on a PC because APC in in zend is

00:39:18,360 --> 00:39:24,120
actually an extend an implementation of

00:39:20,550 --> 00:39:27,330
Zen cached or however we found that they

00:39:24,120 --> 00:39:30,960
will take zero milliseconds because it's

00:39:27,330 --> 00:39:33,120
an in-memory swap and it you just end up

00:39:30,960 --> 00:39:40,170
with 60 or 70 of them in your page it's

00:39:33,120 --> 00:39:42,650
really not very useful so I thought I

00:39:40,170 --> 00:39:42,650
had some

00:39:44,260 --> 00:39:48,810
so that was a brief look under the hood

00:39:46,270 --> 00:39:51,520
of what we're doing inside the BBC and

00:39:48,810 --> 00:39:53,710
what we're currently doing is yeah we're

00:39:51,520 --> 00:39:55,810
trying to data-mine all this all this

00:39:53,710 --> 00:39:58,600
data that we're starting to get so the

00:39:55,810 --> 00:40:00,910
idea is that now that we are aggregating

00:39:58,600 --> 00:40:02,920
across the whole of the BBC who is

00:40:00,910 --> 00:40:05,700
making service calls to what service

00:40:02,920 --> 00:40:09,880
endpoints how long they're cacheable for

00:40:05,700 --> 00:40:12,250
how big the data was how big how long it

00:40:09,880 --> 00:40:14,620
took to deliver the content we can now

00:40:12,250 --> 00:40:17,470
start to rank all the applications in

00:40:14,620 --> 00:40:20,110
the BBC all these 360 applications and

00:40:17,470 --> 00:40:23,020
we can rank them by frequency so like I

00:40:20,110 --> 00:40:25,120
player and home page a hit lots but

00:40:23,020 --> 00:40:27,370
things like the Performing Arts web

00:40:25,120 --> 00:40:30,670
application page is probably hit once a

00:40:27,370 --> 00:40:33,790
week if that and we can rank them and we

00:40:30,670 --> 00:40:36,430
can say your page is very expensive for

00:40:33,790 --> 00:40:37,480
us to serve on a platform and we're

00:40:36,430 --> 00:40:39,160
trying to come up with some kind of

00:40:37,480 --> 00:40:41,260
metrics that we can kind of say the

00:40:39,160 --> 00:40:43,810
average cost to deliver a page on the

00:40:41,260 --> 00:40:45,190
BBC and then we can equate which are the

00:40:43,810 --> 00:40:48,010
good performing and the bad performing

00:40:45,190 --> 00:40:50,500
sites we can also look at all the who is

00:40:48,010 --> 00:40:53,800
making calls to particular APA API

00:40:50,500 --> 00:40:55,390
endpoints what frequency whether they're

00:40:53,800 --> 00:40:58,090
cashing them correctly we've already

00:40:55,390 --> 00:41:02,740
spotted some sites that are bypassing

00:40:58,090 --> 00:41:04,570
the BBC HTTP car call to hit memcache

00:41:02,740 --> 00:41:07,480
first and they're actually going direct

00:41:04,570 --> 00:41:09,850
every time and it's just one line change

00:41:07,480 --> 00:41:12,970
to remove a switch when they make their

00:41:09,850 --> 00:41:15,040
request or set their there HTTP client

00:41:12,970 --> 00:41:18,340
up to actually say go to memcache first

00:41:15,040 --> 00:41:19,840
and that would probably save as hundreds

00:41:18,340 --> 00:41:22,510
of milliseconds per page requests for

00:41:19,840 --> 00:41:25,120
their application and basically we're

00:41:22,510 --> 00:41:27,100
generating huge list that we can go

00:41:25,120 --> 00:41:30,220
around and we can poke people in the BBC

00:41:27,100 --> 00:41:35,630
and say fix your

00:41:30,220 --> 00:41:38,180
so what's happening next and so the

00:41:35,630 --> 00:41:40,880
minute we're only doing this for the PHP

00:41:38,180 --> 00:41:45,200
layer we're now going to start passing

00:41:40,880 --> 00:41:47,660
the profile ID that is generated in to

00:41:45,200 --> 00:41:50,180
record the profile information so i

00:41:47,660 --> 00:41:53,060
didn't mention it we actually send back

00:41:50,180 --> 00:41:55,490
to the client a header so no we only do

00:41:53,060 --> 00:41:59,600
this so on every environment other than

00:41:55,490 --> 00:42:02,120
live so on intestine stage you can set a

00:41:59,600 --> 00:42:04,760
cookie or you can set a header and you

00:42:02,120 --> 00:42:07,070
can hit your page and it will force a

00:42:04,760 --> 00:42:09,470
profile and then it will send you back

00:42:07,070 --> 00:42:12,290
in the request to your browser it will

00:42:09,470 --> 00:42:15,620
send you a header which is X BBC profile

00:42:12,290 --> 00:42:18,050
ID and then it's a blob of data that you

00:42:15,620 --> 00:42:18,950
can then go and cut and paste although

00:42:18,050 --> 00:42:20,480
we're thinking of something like a

00:42:18,950 --> 00:42:22,520
greasemonkey script that will then give

00:42:20,480 --> 00:42:25,280
you a little button at the bottom to say

00:42:22,520 --> 00:42:28,340
view my trace and that will take you to

00:42:25,280 --> 00:42:29,960
the XH prof GUI page and it will take

00:42:28,340 --> 00:42:33,020
you to the hard data page so that you

00:42:29,960 --> 00:42:34,880
can then see that page profile

00:42:33,020 --> 00:42:37,910
information we're actually going to pass

00:42:34,880 --> 00:42:39,530
this information the X profile ID on to

00:42:37,910 --> 00:42:41,540
the service our Java service layer

00:42:39,530 --> 00:42:42,920
because at the minute now we can see

00:42:41,540 --> 00:42:45,860
we're making two calls to our Java

00:42:42,920 --> 00:42:47,510
service layer took 200 milliseconds so

00:42:45,860 --> 00:42:50,150
what was the job service layer doing to

00:42:47,510 --> 00:42:52,880
take 200 milliseconds so we're going to

00:42:50,150 --> 00:42:55,160
get the Java service guys to implement

00:42:52,880 --> 00:42:58,100
the same data format this HTTP archive

00:42:55,160 --> 00:43:01,670
so then we'll be able to see all the way

00:42:58,100 --> 00:43:04,730
for it through this page request it did

00:43:01,670 --> 00:43:08,690
all of this data inside PHP made these

00:43:04,730 --> 00:43:11,030
curl requests to Java what was Java

00:43:08,690 --> 00:43:13,190
doing oh it was making ten calls to my

00:43:11,030 --> 00:43:16,070
sequel this was a long-running sequel

00:43:13,190 --> 00:43:18,980
query and all that information all in

00:43:16,070 --> 00:43:20,960
the same format so again you post your

00:43:18,980 --> 00:43:22,280
ID that you get back into your page or

00:43:20,960 --> 00:43:26,240
you click your button or whatever and

00:43:22,280 --> 00:43:27,770
you will get a whole stack look at what

00:43:26,240 --> 00:43:30,920
your application was doing and what's

00:43:27,770 --> 00:43:32,660
taking the time we're also looking to

00:43:30,920 --> 00:43:35,270
see if we can actually push this further

00:43:32,660 --> 00:43:38,120
and rather than get PHP to force the

00:43:35,270 --> 00:43:39,950
profile we go because we own varnish

00:43:38,120 --> 00:43:42,020
we're going to do it in varnish

00:43:39,950 --> 00:43:43,490
which then means we can probably put

00:43:42,020 --> 00:43:45,620
some interesting metrics as to what

00:43:43,490 --> 00:43:47,540
happened the caching layer we're also

00:43:45,620 --> 00:43:49,850
try we ran in discussion with our

00:43:47,540 --> 00:43:51,890
network operations team to see if we can

00:43:49,850 --> 00:43:55,100
actually get the random select to to

00:43:51,890 --> 00:43:57,560
profile thing done at the Z XDM layer so

00:43:55,100 --> 00:43:59,060
we also see what traffic routing calls

00:43:57,560 --> 00:44:01,580
were hit in our traffic management

00:43:59,060 --> 00:44:03,320
server and then we need to implement an

00:44:01,580 --> 00:44:09,610
interface to aggregate all this data

00:44:03,320 --> 00:44:19,070
together and that is the end of my talk

00:44:09,610 --> 00:44:20,270
thank you very much for listening yeah

00:44:19,070 --> 00:44:23,630
just wondered if you could like

00:44:20,270 --> 00:44:25,370
summarize the things that varnish gives

00:44:23,630 --> 00:44:37,220
you that the Zeus traffic manager

00:44:25,370 --> 00:44:39,850
doesn't so basically it gives you and I

00:44:37,220 --> 00:44:39,850
guess

00:44:40,900 --> 00:44:47,380
I wasn't that I wasn't there when

00:44:43,360 --> 00:44:49,150
varnish was introduced and it kind of I

00:44:47,380 --> 00:44:52,150
got handed this as a legacy when i

00:44:49,150 --> 00:44:54,370
joined the BBC as it happens I'm very

00:44:52,150 --> 00:44:56,290
very impressed with it I think the main

00:44:54,370 --> 00:44:59,280
thing it gives us we probably could

00:44:56,290 --> 00:45:03,060
implement most these things in Zeus and

00:44:59,280 --> 00:45:06,150
but the thing that it does give us is

00:45:03,060 --> 00:45:09,400
flexibility to be able to change it

00:45:06,150 --> 00:45:10,390
ourselves because otherwise we have to

00:45:09,400 --> 00:45:13,420
go through hoops with the network

00:45:10,390 --> 00:45:15,700
operations teams so this sits as a

00:45:13,420 --> 00:45:17,590
separate caching layer the other thing

00:45:15,700 --> 00:45:21,040
is we can scale it as much as we like

00:45:17,590 --> 00:45:25,990
we've got 64 gig boxes a 64 gig of ram

00:45:21,040 --> 00:45:29,710
boxes and and we are currently cinch for

00:45:25,990 --> 00:45:31,390
cash is about 10 gigabytes so we've got

00:45:29,710 --> 00:45:35,350
lots of room for expansion in there I

00:45:31,390 --> 00:45:37,570
think the Zeus's tend to use most of

00:45:35,350 --> 00:45:40,090
their so the other thing we do is we

00:45:37,570 --> 00:45:43,000
send a header back from varnish to help

00:45:40,090 --> 00:45:45,300
tell the Zeus is not to cash it because

00:45:43,000 --> 00:45:47,980
otherwise they would cash it as well and

00:45:45,300 --> 00:45:49,810
they are limited on RAM i think they

00:45:47,980 --> 00:45:52,600
normally have about 4 gigabytes of ram

00:45:49,810 --> 00:45:55,930
available for cash and most of that is

00:45:52,600 --> 00:45:58,480
used serving static assets images CSS

00:45:55,930 --> 00:46:00,730
javascript and i think the general

00:45:58,480 --> 00:46:03,220
general consensus is that they prefer to

00:46:00,730 --> 00:46:08,020
keep it that way and the dynamic stuff

00:46:03,220 --> 00:46:10,870
that is casual is by far larger volume

00:46:08,020 --> 00:46:13,650
data and they'd prefer to keep that in a

00:46:10,870 --> 00:46:13,650
separate caching layer

00:46:16,740 --> 00:46:21,780
right a little bit to expand on the

00:46:18,960 --> 00:46:24,690
question worlds and framework bearing in

00:46:21,780 --> 00:46:27,840
mind the version 2 is coming out at some

00:46:24,690 --> 00:46:30,750
point you might send framework to I'm

00:46:27,840 --> 00:46:33,480
it's more about the framework to add do

00:46:30,750 --> 00:46:36,960
you make any plans to to my greater than

00:46:33,480 --> 00:46:39,180
framework to or so the other problem

00:46:36,960 --> 00:46:42,660
with running shared hosting platform

00:46:39,180 --> 00:46:44,340
with 360 applications some of which were

00:46:42,660 --> 00:46:47,310
developed two or three years ago and

00:46:44,340 --> 00:46:52,140
haven't been touched since we had a huge

00:46:47,310 --> 00:46:55,320
problem getting up to PHP 5.3 it took us

00:46:52,140 --> 00:46:57,930
three attempts I think to do it before

00:46:55,320 --> 00:47:00,600
we actually got it to stick because of

00:46:57,930 --> 00:47:03,150
just random crap that was two and a half

00:47:00,600 --> 00:47:05,100
years old and nobody had looked at we

00:47:03,150 --> 00:47:10,050
did manage to upgrade zend framework to

00:47:05,100 --> 00:47:12,180
the latest i think it's 16 something we

00:47:10,050 --> 00:47:14,280
did manage to upgrade but again we

00:47:12,180 --> 00:47:16,380
didn't we did take us to two attempts

00:47:14,280 --> 00:47:22,130
before we got that we tried going to the

00:47:16,380 --> 00:47:27,830
latest a zend framework one I'm sorry

00:47:22,130 --> 00:47:32,160
yeah and that that broke so many things

00:47:27,830 --> 00:47:34,500
we are committed to as the frameworks

00:47:32,160 --> 00:47:36,150
team we are responsible for zend and the

00:47:34,500 --> 00:47:38,670
things that sit on top as end but not

00:47:36,150 --> 00:47:40,740
the actual websites so we have we've

00:47:38,670 --> 00:47:43,410
actually built a separate separate so we

00:47:40,740 --> 00:47:45,660
have an int test and a stage environment

00:47:43,410 --> 00:47:47,070
so integration is where developers

00:47:45,660 --> 00:47:49,050
commit their stuff and it gets built

00:47:47,070 --> 00:47:50,460
onto their test is when they're happy to

00:47:49,050 --> 00:47:53,460
test it once they're happy it's ready

00:47:50,460 --> 00:47:55,140
for release it goes to stage intern test

00:47:53,460 --> 00:47:56,760
and managed by developers they can roll

00:47:55,140 --> 00:47:59,100
and push their products out there as

00:47:56,760 --> 00:48:01,500
often as they like everything stage

00:47:59,100 --> 00:48:04,710
upwards is managed by the network

00:48:01,500 --> 00:48:07,859
operations and delivery team their site

00:48:04,710 --> 00:48:13,980
operations team so and that's when it

00:48:07,859 --> 00:48:15,990
becomes more production ready so we've

00:48:13,980 --> 00:48:17,130
built a separate environment called we

00:48:15,990 --> 00:48:18,869
called it the engineering environment

00:48:17,130 --> 00:48:21,630
which is actually a model of the live

00:48:18,869 --> 00:48:24,330
environment just so that we can test new

00:48:21,630 --> 00:48:25,950
releases of apache new releases a PHP

00:48:24,330 --> 00:48:29,790
and zend framework

00:48:25,950 --> 00:48:31,740
and all we can do is roll it push it out

00:48:29,790 --> 00:48:33,960
see what breaks and then just log

00:48:31,740 --> 00:48:36,359
tickets against I player and BBC home

00:48:33,960 --> 00:48:38,579
page and say fix your stuff and then

00:48:36,359 --> 00:48:42,510
we'll try again but that takes a long

00:48:38,579 --> 00:48:44,490
time because their releases don't

00:48:42,510 --> 00:48:47,280
necessarily tie up with our time scales

00:48:44,490 --> 00:48:49,140
so and yeah we're committed to try and

00:48:47,280 --> 00:48:52,200
doing it because we need to keep up to

00:48:49,140 --> 00:48:54,630
date but I'm not sure whether we'll be

00:48:52,200 --> 00:48:55,950
able to in the short term and will also

00:48:54,630 --> 00:48:58,079
be coming up to the Olympics and

00:48:55,950 --> 00:49:01,140
there'll be a freeze on major impact

00:48:58,079 --> 00:49:03,060
changing on the infrastructure so

00:49:01,140 --> 00:49:06,680
hopefully by the end of the year maybe

00:49:03,060 --> 00:49:06,680
but I'm not holding my breath

00:49:10,240 --> 00:49:19,060
oh sorry yeah what's your approach to

00:49:14,080 --> 00:49:21,550
cash in validation we don't have a cache

00:49:19,060 --> 00:49:24,310
invalidation mechanism with varnish so

00:49:21,550 --> 00:49:27,720
it's up to the individual application

00:49:24,310 --> 00:49:29,920
owners to set their cash Magnus age and

00:49:27,720 --> 00:49:34,000
appropriate etag headers and things like

00:49:29,920 --> 00:49:38,830
that we do do varnish grace periods so

00:49:34,000 --> 00:49:41,380
we have stale while revalidate which

00:49:38,830 --> 00:49:43,810
reduces the load dramatically because

00:49:41,380 --> 00:49:46,060
you only the first request that has

00:49:43,810 --> 00:49:47,950
reached the max age hits the back end

00:49:46,060 --> 00:49:51,190
everybody else is served stale data

00:49:47,950 --> 00:49:53,410
until it refreshes but it's we don't

00:49:51,190 --> 00:49:55,330
have a cache invalidation mechanism we

00:49:53,410 --> 00:49:59,320
could implement one we haven't had the

00:49:55,330 --> 00:50:02,070
need for it just yet we've got time for

00:49:59,320 --> 00:50:02,070
one more of them

00:50:05,380 --> 00:50:11,080
I look look I you've obviously done an

00:50:08,500 --> 00:50:13,540
awful lot of research into performance

00:50:11,080 --> 00:50:14,920
and caching that kind of thing if you

00:50:13,540 --> 00:50:16,510
were talking to an organization that's

00:50:14,920 --> 00:50:18,640
got pretty high traffic on their site

00:50:16,510 --> 00:50:20,200
and they've done nothing so far so it's

00:50:18,640 --> 00:50:22,600
a lamp stack and they've got no caching

00:50:20,200 --> 00:50:29,650
at all and you could recommend one thing

00:50:22,600 --> 00:50:31,330
for them to do what would it be well

00:50:29,650 --> 00:50:33,580
actually yeah there is something to be

00:50:31,330 --> 00:50:36,760
said for just sticking varnish in front

00:50:33,580 --> 00:50:39,280
of a site just out the box generally you

00:50:36,760 --> 00:50:42,190
get where as long as you're not setting

00:50:39,280 --> 00:50:44,050
cache-control private or setting cookies

00:50:42,190 --> 00:50:46,930
on every request or something stupid

00:50:44,050 --> 00:50:50,560
like that you probably get some benefit

00:50:46,930 --> 00:50:52,810
out of it and what what cause does a lot

00:50:50,560 --> 00:50:54,970
of problems was our international

00:50:52,810 --> 00:50:57,400
domestic switch for the header and

00:50:54,970 --> 00:50:59,680
footer and it's taken as almost a year

00:50:57,400 --> 00:51:01,390
to solve that sufficiently that we're

00:50:59,680 --> 00:51:03,700
happy with it and can cat we're now

00:51:01,390 --> 00:51:05,710
actually in the process of sticking

00:51:03,700 --> 00:51:08,260
varnish in front of every single BBC

00:51:05,710 --> 00:51:13,480
site so we will be using varnish for

00:51:08,260 --> 00:51:15,840
everything very shortly but we've had to

00:51:13,480 --> 00:51:18,610
do a lot of proving but yeah I mean just

00:51:15,840 --> 00:51:22,120
make sure that your understand how

00:51:18,610 --> 00:51:24,730
caches work and that you don't make your

00:51:22,120 --> 00:51:30,280
page uncatchable by setting cookies or

00:51:24,730 --> 00:51:34,950
setting yeah disabled PHP sessions is

00:51:30,280 --> 00:51:40,420
probably a good one as well okay will um

00:51:34,950 --> 00:51:42,700
okay maybe one just one more you'll your

00:51:40,420 --> 00:51:46,120
service virtualized to do you run now

00:51:42,700 --> 00:51:49,960
we're all commodity servers we don't do

00:51:46,120 --> 00:51:52,750
virtualized we have some of our testing

00:51:49,960 --> 00:51:54,550
environments that are not our standard

00:51:52,750 --> 00:51:57,430
testing environment so we have something

00:51:54,550 --> 00:51:59,050
we called in incubators which are for

00:51:57,430 --> 00:52:02,350
like test projects they're virtualized

00:51:59,050 --> 00:52:05,800
we're looking where we run red hat and

00:52:02,350 --> 00:52:08,980
and we've been looking at the Red Hat

00:52:05,800 --> 00:52:10,900
virtualization system it's I mean I

00:52:08,980 --> 00:52:12,160
think it we will eventually but for the

00:52:10,900 --> 00:52:15,490
time being we're all running on

00:52:12,160 --> 00:52:19,000
commodity servers for everything in our

00:52:15,490 --> 00:52:21,070
own data centers excellent thanks again

00:52:19,000 --> 00:52:23,310
and guys we've got lunch outside sir

00:52:21,070 --> 00:52:23,310

YouTube URL: https://www.youtube.com/watch?v=iTveazbmzF8


