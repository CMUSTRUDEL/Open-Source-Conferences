Title: Microservices Gone Wrong - Anthony Ferrara - PHP UK Conference 2019
Publication date: 2019-03-22
Playlist: PHP UK Conference 2019
Description: 
	Microservices are the latest architectural trend to take the PHP community by storm. Is it a good pattern? How can you use it effectively? In this talk, we'll explore real world experience building out a large scale application based around microservices: what worked really well, what didn't work at all, and what we learned along the way. Spoiler alert: we got a lot wrong.
Captions: 
	00:00:00,030 --> 00:00:09,179
I want to do something quite a bit

00:00:06,690 --> 00:00:12,150
different than probably every other talk

00:00:09,179 --> 00:00:14,790
here and probably the majority of the

00:00:12,150 --> 00:00:17,400
talks you've ever been to before rather

00:00:14,790 --> 00:00:19,410
than focus on what I think you should do

00:00:17,400 --> 00:00:24,119
or rather than focus on teaching you new

00:00:19,410 --> 00:00:26,010
technology or explaining something I'm

00:00:24,119 --> 00:00:27,930
gonna do the opposite I'm gonna tell you

00:00:26,010 --> 00:00:30,269
everything that you should not do and

00:00:27,930 --> 00:00:32,669
I'm gonna tell you this story based on a

00:00:30,269 --> 00:00:34,410
lesson from personal experience based on

00:00:32,669 --> 00:00:37,920
something that I did that I worked with

00:00:34,410 --> 00:00:40,440
the team on and we got a lot right don't

00:00:37,920 --> 00:00:43,649
get me wrong but we got a lot a lot

00:00:40,440 --> 00:00:46,770
wrong and I want to share you today this

00:00:43,649 --> 00:00:49,320
story of what we got wrong how we fixed

00:00:46,770 --> 00:00:51,989
it what we learned and hopefully a

00:00:49,320 --> 00:00:56,340
couple takeaways on how to prevent you

00:00:51,989 --> 00:00:58,680
making those same style mistakes so this

00:00:56,340 --> 00:01:01,859
journey is going to be assisted by my

00:00:58,680 --> 00:01:03,449
cute little puppy at a Douglas she's

00:01:01,859 --> 00:01:04,500
gonna be in some of the interludes but

00:01:03,449 --> 00:01:07,680
really I want to take you on the

00:01:04,500 --> 00:01:09,450
background around what the team was what

00:01:07,680 --> 00:01:12,210
the technological and business problems

00:01:09,450 --> 00:01:14,460
that we were trying to solve for the

00:01:12,210 --> 00:01:16,979
architecture involved what we chose why

00:01:14,460 --> 00:01:19,470
we chose what we chose how we actually

00:01:16,979 --> 00:01:22,350
got this platform up and running from

00:01:19,470 --> 00:01:24,450
the local dividend environment getting

00:01:22,350 --> 00:01:26,340
it into production dealing with the

00:01:24,450 --> 00:01:28,619
changes that eventually came from

00:01:26,340 --> 00:01:30,060
product and from bugs bugs and things

00:01:28,619 --> 00:01:32,850
like that the refactoring we needed to

00:01:30,060 --> 00:01:36,420
do and kind of finish it off with some

00:01:32,850 --> 00:01:39,270
of the lessons that we learned to give

00:01:36,420 --> 00:01:42,240
you some background when I joined the

00:01:39,270 --> 00:01:44,460
team it was nine developers they had

00:01:42,240 --> 00:01:46,350
just recently grown from five developers

00:01:44,460 --> 00:01:48,750
in the path in the year beforehand the

00:01:46,350 --> 00:01:50,689
original five were there for about four

00:01:48,750 --> 00:01:53,420
years beforehand and built the entire

00:01:50,689 --> 00:01:56,759
monolithic application that existed and

00:01:53,420 --> 00:02:00,600
so we grew to twenty three by the time

00:01:56,759 --> 00:02:03,329
this story starts we went from five to

00:02:00,600 --> 00:02:05,189
nine in about a year and then 9 to 23 in

00:02:03,329 --> 00:02:06,960
about nine months

00:02:05,189 --> 00:02:11,039
recently closing around a funding of

00:02:06,960 --> 00:02:13,830
aggressive growth and so we had a decent

00:02:11,039 --> 00:02:17,640
sized team the team had a fair

00:02:13,830 --> 00:02:20,220
bit of experience range everything from

00:02:17,640 --> 00:02:22,200
quite junior to some of the most senior

00:02:20,220 --> 00:02:25,830
in the industry that we could find very

00:02:22,200 --> 00:02:30,300
very experienced developers about half

00:02:25,830 --> 00:02:32,730
the team was PHP focused a couple of

00:02:30,300 --> 00:02:36,240
those people had experience with golang

00:02:32,730 --> 00:02:38,940
and Scala etc we had a number of

00:02:36,240 --> 00:02:41,730
front-end engineers mostly focused on

00:02:38,940 --> 00:02:44,430
JavaScript react was the new thing at

00:02:41,730 --> 00:02:46,770
the time and we had hired a very very

00:02:44,430 --> 00:02:50,190
good react team and built a react team

00:02:46,770 --> 00:02:53,459
under us we also had data engineers as

00:02:50,190 --> 00:02:55,580
well as dev ops as well as so test

00:02:53,459 --> 00:02:59,070
engineers I mean DevOps is kind of a

00:02:55,580 --> 00:03:00,780
loaded term operational engineers a

00:02:59,070 --> 00:03:01,970
couple people focused on infrastructure

00:03:00,780 --> 00:03:05,040
and an SRE

00:03:01,970 --> 00:03:07,860
to help actually run the existing

00:03:05,040 --> 00:03:12,030
platform that we had and so it's a very

00:03:07,860 --> 00:03:14,340
new team the team existed for less than

00:03:12,030 --> 00:03:18,269
three months as the whole before we

00:03:14,340 --> 00:03:21,239
kicked off this project the background

00:03:18,269 --> 00:03:24,030
the legacy system so I mentioned we had

00:03:21,239 --> 00:03:26,160
a monolithic application this monolithic

00:03:24,030 --> 00:03:29,430
application had bits of Zend framework

00:03:26,160 --> 00:03:32,130
one CodeIgniter Symphony cake and a

00:03:29,430 --> 00:03:34,830
bunch of PHP for in pair code in it and

00:03:32,130 --> 00:03:38,040
to be fair it worked pretty well it

00:03:34,830 --> 00:03:42,930
powered a multi multi multi-million

00:03:38,040 --> 00:03:45,120
dollar a year business for years but it

00:03:42,930 --> 00:03:46,590
was kind of a nightmare to work with

00:03:45,120 --> 00:03:49,410
we started doing quite a bit of

00:03:46,590 --> 00:03:51,239
refactoring you know as you see symphony

00:03:49,410 --> 00:03:55,080
which started to refactor prior to me

00:03:51,239 --> 00:03:57,120
coming in to try to condense onto one

00:03:55,080 --> 00:03:59,519
framework but it was just such a large

00:03:57,120 --> 00:04:02,519
application about half a million lines

00:03:59,519 --> 00:04:06,600
of code that that's not an easy thing to

00:04:02,519 --> 00:04:09,030
do the biggest problem though is at the

00:04:06,600 --> 00:04:12,750
data layer the application as you see

00:04:09,030 --> 00:04:16,530
here had about 600 database tables this

00:04:12,750 --> 00:04:18,840
is not a complicated application 600

00:04:16,530 --> 00:04:21,570
tables for something that you could

00:04:18,840 --> 00:04:23,580
probably do in about 40 should give you

00:04:21,570 --> 00:04:26,850
an idea of kind of the level of

00:04:23,580 --> 00:04:27,360
complexity in here and this thousand

00:04:26,850 --> 00:04:29,759
cron job

00:04:27,360 --> 00:04:32,069
number that is not an exaggeration I

00:04:29,759 --> 00:04:33,389
believe the exact number was like eleven

00:04:32,069 --> 00:04:37,289
hundred and fifty or something like that

00:04:33,389 --> 00:04:39,270
and the scariest part was the

00:04:37,289 --> 00:04:43,530
application was designed over many many

00:04:39,270 --> 00:04:46,560
years and so reporting so reporting was

00:04:43,530 --> 00:04:47,699
a really important area that we have our

00:04:46,560 --> 00:04:52,199
application that we did that we shipped

00:04:47,699 --> 00:04:54,060
to our clients and multiple parts of the

00:04:52,199 --> 00:04:56,699
application would write to the reporting

00:04:54,060 --> 00:04:58,699
area the reporting tables except they

00:04:56,699 --> 00:05:00,960
would over it write slightly differently

00:04:58,699 --> 00:05:02,520
partially because of bugs partially

00:05:00,960 --> 00:05:06,199
because of differences in design over

00:05:02,520 --> 00:05:09,060
the years and so the stats were wrong

00:05:06,199 --> 00:05:11,789
because the database was updated in a

00:05:09,060 --> 00:05:13,590
weird way and so rather than fixing in

00:05:11,789 --> 00:05:16,439
common izing and putting a library as

00:05:13,590 --> 00:05:17,969
most developers would do the solution

00:05:16,439 --> 00:05:20,879
was well let's just install a cron job

00:05:17,969 --> 00:05:22,860
to fix this and so we had cron jobs that

00:05:20,879 --> 00:05:26,069
ran every five minutes every 20 minutes

00:05:22,860 --> 00:05:28,800
every 15 minutes to clean and rectify

00:05:26,069 --> 00:05:30,300
the data it brings a whole new

00:05:28,800 --> 00:05:34,889
perspective to the term eventually

00:05:30,300 --> 00:05:36,569
consistent so this was actually a huge

00:05:34,889 --> 00:05:38,690
huge problem and really slowed

00:05:36,569 --> 00:05:42,870
everything down working with this system

00:05:38,690 --> 00:05:45,900
was a nightmare and so when I joined

00:05:42,870 --> 00:05:48,360
when I when I was hired they were just

00:05:45,900 --> 00:05:51,659
ending a nine-month product freeze and

00:05:48,360 --> 00:05:54,659
what that means is no product work

00:05:51,659 --> 00:05:57,960
happened at the company for nine months

00:05:54,659 --> 00:06:00,300
they focused purely on refactoring on

00:05:57,960 --> 00:06:01,860
moving the infrastructure away from the

00:06:00,300 --> 00:06:06,930
dedicated hosts that they were on

00:06:01,860 --> 00:06:08,370
beforehand on into AWS on a couple

00:06:06,930 --> 00:06:11,370
security things that had to be taken

00:06:08,370 --> 00:06:13,919
care of for compliance reasons and on

00:06:11,370 --> 00:06:16,050
trying to make the system easier to work

00:06:13,919 --> 00:06:18,210
with at the same time

00:06:16,050 --> 00:06:19,860
sales did not stop so we had

00:06:18,210 --> 00:06:22,500
increasingly increasing scale

00:06:19,860 --> 00:06:25,289
requirements we had new customers that

00:06:22,500 --> 00:06:27,509
were closing with ten times the number

00:06:25,289 --> 00:06:29,879
of users as our price bigger cost but

00:06:27,509 --> 00:06:33,449
biggest customer and the product wasn't

00:06:29,879 --> 00:06:35,279
support to scale for that and so we

00:06:33,449 --> 00:06:37,169
needed to ship faster we needed to ship

00:06:35,279 --> 00:06:39,209
better code we needed to ship products

00:06:37,169 --> 00:06:41,100
that were stable and the business was

00:06:39,209 --> 00:06:45,840
really demanding and so we kind of hit

00:06:41,100 --> 00:06:48,780
this precipice point where we had to

00:06:45,840 --> 00:06:52,080
make a decision do we continue trying to

00:06:48,780 --> 00:06:54,750
refactor this do we continue trying to

00:06:52,080 --> 00:06:57,930
solve the application's problems in the

00:06:54,750 --> 00:07:00,690
code base that exists or do we throw

00:06:57,930 --> 00:07:06,960
away the existing platform and build a

00:07:00,690 --> 00:07:09,120
new one and rewrite it and so the way we

00:07:06,960 --> 00:07:10,410
approach this question I think is really

00:07:09,120 --> 00:07:13,020
really interesting and so I want to take

00:07:10,410 --> 00:07:15,360
a little tangent to kind of go into this

00:07:13,020 --> 00:07:18,980
process a little bit because the exactly

00:07:15,360 --> 00:07:18,980
answer we got to is quite unexpected

00:07:19,250 --> 00:07:24,120
when we face this question we walked

00:07:22,350 --> 00:07:26,880
into a room and when I say a room I'm

00:07:24,120 --> 00:07:29,100
talking about maybe a 2 meter by 2 meter

00:07:26,880 --> 00:07:32,010
room with 4 whiteboard walls with

00:07:29,100 --> 00:07:33,510
whiteboards on all four walls we walked

00:07:32,010 --> 00:07:35,970
in the head of product head of design

00:07:33,510 --> 00:07:39,180
myself and two other engineers cramped

00:07:35,970 --> 00:07:41,520
into this tiny little room and we got

00:07:39,180 --> 00:07:44,460
out of our whiteboard or our pens and we

00:07:41,520 --> 00:07:45,930
started listing every core assumption

00:07:44,460 --> 00:07:48,740
that we could think of of the of the

00:07:45,930 --> 00:07:52,890
product now when I say a core assumption

00:07:48,740 --> 00:07:55,530
think of like your users table when you

00:07:52,890 --> 00:07:57,510
have a user you probably have an email

00:07:55,530 --> 00:08:00,540
address associated with that user right

00:07:57,510 --> 00:08:03,570
common email on the the user table you

00:08:00,540 --> 00:08:06,990
probably have a unique index on that

00:08:03,570 --> 00:08:09,690
email address right that's an assumption

00:08:06,990 --> 00:08:13,260
you're assuming that an email is

00:08:09,690 --> 00:08:15,600
uniquely identifies a user if you want

00:08:13,260 --> 00:08:18,270
it in your application to change that

00:08:15,600 --> 00:08:19,770
assumption if you wanted to say I want

00:08:18,270 --> 00:08:21,960
to have two users be able to have the

00:08:19,770 --> 00:08:25,410
same email address how difficult would

00:08:21,960 --> 00:08:27,180
that be for you probably not terribly

00:08:25,410 --> 00:08:28,740
difficult yeah you'd have to change some

00:08:27,180 --> 00:08:30,720
of your authentication logic if you're

00:08:28,740 --> 00:08:32,849
using emails for authentication you

00:08:30,720 --> 00:08:34,650
probably have to change a little bit

00:08:32,849 --> 00:08:36,539
around like password resets and account

00:08:34,650 --> 00:08:38,430
management and stuff like that but for

00:08:36,539 --> 00:08:41,300
the most part that should be a pretty

00:08:38,430 --> 00:08:44,580
straightforward refactor in our case

00:08:41,300 --> 00:08:47,010
about 60% of the system relied on the

00:08:44,580 --> 00:08:49,320
fact that emails were unique some of the

00:08:47,010 --> 00:08:51,480
reporting pieces looked at email rather

00:08:49,320 --> 00:08:54,310
than user ID and trying to tease that

00:08:51,480 --> 00:08:55,840
apart and refactor to use user ID

00:08:54,310 --> 00:08:58,990
would have been incredibly incredibly

00:08:55,840 --> 00:09:02,020
difficult so what we did is we filled

00:08:58,990 --> 00:09:04,990
all of the whiteboards with everything

00:09:02,020 --> 00:09:08,650
that we could think of that the product

00:09:04,990 --> 00:09:10,839
assumed okay that took us probably about

00:09:08,650 --> 00:09:12,850
three hours to do and so we filled out

00:09:10,839 --> 00:09:15,490
all of this information then we took a

00:09:12,850 --> 00:09:16,930
different color marker and turned to to

00:09:15,490 --> 00:09:19,630
product brought the rest of the product

00:09:16,930 --> 00:09:21,370
managers in and said this assumption

00:09:19,630 --> 00:09:24,520
here the email is unique within the

00:09:21,370 --> 00:09:27,400
system per user do you want that to

00:09:24,520 --> 00:09:29,200
change within the next year and if they

00:09:27,400 --> 00:09:31,540
said yes we drew a line through it if

00:09:29,200 --> 00:09:34,060
they said no we left it in the color if

00:09:31,540 --> 00:09:36,610
they said now maybe eventually we left

00:09:34,060 --> 00:09:38,710
it alone we did this process for every

00:09:36,610 --> 00:09:40,330
single one of them and we were left with

00:09:38,710 --> 00:09:42,970
three assumptions that we did not want

00:09:40,330 --> 00:09:45,010
to change I would have four walls worth

00:09:42,970 --> 00:09:48,029
of assumptions like we basically wanted

00:09:45,010 --> 00:09:52,870
to change everything about the product

00:09:48,029 --> 00:09:56,200
without changing it so that led us to

00:09:52,870 --> 00:09:59,350
the decision to do neither we don't want

00:09:56,200 --> 00:10:01,180
to refactor because that's going to be a

00:09:59,350 --> 00:10:03,610
nightmare trying to change everything

00:10:01,180 --> 00:10:06,790
about the program I mean even if it was

00:10:03,610 --> 00:10:08,740
a really clean application trying to

00:10:06,790 --> 00:10:11,170
refactor that much would have been an

00:10:08,740 --> 00:10:14,170
insane challenge but we also don't want

00:10:11,170 --> 00:10:16,660
to rebuild because rebuilding would mean

00:10:14,170 --> 00:10:19,930
rewriting the existing application and

00:10:16,660 --> 00:10:23,470
so what we chose was to build a second

00:10:19,930 --> 00:10:25,600
product at the highest of high levels it

00:10:23,470 --> 00:10:29,320
did the same thing it solved the same

00:10:25,600 --> 00:10:31,570
stupidly high level business problem but

00:10:29,320 --> 00:10:33,910
at every layer below the way it did it

00:10:31,570 --> 00:10:36,640
the approach that it took how it solved

00:10:33,910 --> 00:10:39,250
it from the user perspective how it was

00:10:36,640 --> 00:10:43,680
administrated how it was maintained the

00:10:39,250 --> 00:10:43,680
technology everything else was different

00:10:44,580 --> 00:10:49,810
so we basically went to the board we

00:10:48,040 --> 00:10:52,150
pitched this let's build the second

00:10:49,810 --> 00:10:54,580
product let's invest as much as we can

00:10:52,150 --> 00:10:56,620
into this once it's up high enough we

00:10:54,580 --> 00:10:58,510
can migrate our existing customers over

00:10:56,620 --> 00:11:03,640
and throw away the exists the existing

00:10:58,510 --> 00:11:05,440
one so this is what we did so everything

00:11:03,640 --> 00:11:07,450
from this point forward is going to be

00:11:05,440 --> 00:11:08,259
talking about that new product that we

00:11:07,450 --> 00:11:10,569
decided to build

00:11:08,259 --> 00:11:14,470
this is literally a greenfield project

00:11:10,569 --> 00:11:16,509
we walked in with a team about 23 at the

00:11:14,470 --> 00:11:19,029
start about 35 towards towards the end

00:11:16,509 --> 00:11:21,040
of what I'm going to talk about here and

00:11:19,029 --> 00:11:23,410
so we needed to start off from scratch

00:11:21,040 --> 00:11:25,629
and how do you start off from scratch

00:11:23,410 --> 00:11:28,119
well there were this new thing this is

00:11:25,629 --> 00:11:31,059
in the 2014 era there's this new thing

00:11:28,119 --> 00:11:33,279
called start 2015 new thing called micro

00:11:31,059 --> 00:11:34,899
services which was all the rage we had

00:11:33,279 --> 00:11:37,779
experienced building one of these micro

00:11:34,899 --> 00:11:39,220
services so we sat down and said what if

00:11:37,779 --> 00:11:40,899
we build the entire application off of

00:11:39,220 --> 00:11:43,839
Iker services what would that look like

00:11:40,899 --> 00:11:44,949
and so we started drawing diagrams and

00:11:43,839 --> 00:11:48,459
we started building into the

00:11:44,949 --> 00:11:54,369
relationship and we started doing design

00:11:48,459 --> 00:11:58,559
sprints and we started doing the event

00:11:54,369 --> 00:12:02,589
storming and we came up with a plan for

00:11:58,559 --> 00:12:06,879
the UI the architecture as well as the

00:12:02,589 --> 00:12:10,389
application design this is actually

00:12:06,879 --> 00:12:12,569
pretty close to one of the the initial

00:12:10,389 --> 00:12:15,850
stabs at the architecture that we took

00:12:12,569 --> 00:12:18,579
and what we wound up finally building at

00:12:15,850 --> 00:12:20,619
this point actually I should caveat some

00:12:18,579 --> 00:12:22,029
details have been changed from the

00:12:20,619 --> 00:12:23,859
reality some things I'm going to

00:12:22,029 --> 00:12:27,609
exaggerate a little bit for effect and

00:12:23,859 --> 00:12:31,539
some things aren't going to be identical

00:12:27,609 --> 00:12:34,989
to what we did but it is still I think

00:12:31,539 --> 00:12:36,309
in spirit good enough and it's also some

00:12:34,989 --> 00:12:39,069
of the things that have been tweaked or

00:12:36,309 --> 00:12:40,629
to make the point across better so if

00:12:39,069 --> 00:12:42,669
anybody is watching from the company

00:12:40,629 --> 00:12:46,689
from the team I hope that the rest of

00:12:42,669 --> 00:12:48,999
this aligns so we have this architecture

00:12:46,689 --> 00:12:53,859
I think the most interesting initial

00:12:48,999 --> 00:12:56,649
part was we went API first by design so

00:12:53,859 --> 00:12:59,230
we had a front-end server in a single

00:12:56,649 --> 00:13:00,699
page front-end app actually a series um

00:12:59,230 --> 00:13:03,339
I think there were about three front end

00:13:00,699 --> 00:13:05,259
apps for different usages that were

00:13:03,339 --> 00:13:07,600
based on react so we had a front-end

00:13:05,259 --> 00:13:09,819
server that ran node that rent

00:13:07,600 --> 00:13:11,619
pre-rendered react and served it up to

00:13:09,819 --> 00:13:13,119
the client worked absolutely

00:13:11,619 --> 00:13:15,100
phenomenally was really really stable

00:13:13,119 --> 00:13:16,720
everything worked great so I'm not going

00:13:15,100 --> 00:13:18,519
to talk about the front-end at all from

00:13:16,720 --> 00:13:21,159
here point but this point forward it was

00:13:18,519 --> 00:13:22,120
an amazing part of what we built but

00:13:21,159 --> 00:13:23,350
this talk is

00:13:22,120 --> 00:13:24,760
about things that we did wrong not

00:13:23,350 --> 00:13:27,130
things that we did right and so let's

00:13:24,760 --> 00:13:30,460
skip that so the first thing I want to

00:13:27,130 --> 00:13:32,500
focus on here is this big green thing in

00:13:30,460 --> 00:13:36,790
the center this thing called an API

00:13:32,500 --> 00:13:39,190
gateway to put time and perspective here

00:13:36,790 --> 00:13:43,440
when we decided to go with this

00:13:39,190 --> 00:13:47,230
architecture was exactly one day before

00:13:43,440 --> 00:13:50,920
Amazon launched its AWS gateway API

00:13:47,230 --> 00:13:52,900
gateway to private beta an hour

00:13:50,920 --> 00:13:54,580
AWS representative reached out to us the

00:13:52,900 --> 00:13:57,100
day after we made the decision to go to

00:13:54,580 --> 00:13:58,480
this route and said hey we have this new

00:13:57,100 --> 00:14:00,970
feature that you would you like to be a

00:13:58,480 --> 00:14:02,080
private beta member and so we stopped

00:14:00,970 --> 00:14:04,480
and we looked at it and we had been

00:14:02,080 --> 00:14:06,940
researching other technologies and we

00:14:04,480 --> 00:14:10,510
decided not to go with AWS gateway but

00:14:06,940 --> 00:14:12,400
that should put the rest of this story

00:14:10,510 --> 00:14:14,170
in the context because we ran into that

00:14:12,400 --> 00:14:16,510
problem a lot where we made decisions

00:14:14,170 --> 00:14:18,250
because we had to make a decision and

00:14:16,510 --> 00:14:20,080
then very shortly after a better

00:14:18,250 --> 00:14:23,700
alternative came up that we should have

00:14:20,080 --> 00:14:26,620
probably taken so we went with Tyco

00:14:23,700 --> 00:14:29,470
it's a open-source tool written in go

00:14:26,620 --> 00:14:32,589
lang he uses MongoDB behind the hood

00:14:29,470 --> 00:14:34,209
which it worked phenomenally well but

00:14:32,589 --> 00:14:38,940
you know take what you want what you

00:14:34,209 --> 00:14:44,529
wish with no offense to anybody

00:14:38,940 --> 00:14:47,470
so what Tyco is is a API gateway it

00:14:44,529 --> 00:14:50,320
terminates and separates external

00:14:47,470 --> 00:14:52,480
requests from internal requests so in

00:14:50,320 --> 00:14:57,550
the public domain anything outside of

00:14:52,480 --> 00:15:00,190
your firewall hits Tyco Tyco handles

00:14:57,550 --> 00:15:02,440
oweth for you so you're off to

00:15:00,190 --> 00:15:04,990
termination all of your secrets all of

00:15:02,440 --> 00:15:07,029
your key management is all done by Tyco

00:15:04,990 --> 00:15:09,940
nothing in your application needs to

00:15:07,029 --> 00:15:11,950
even know about OAuth it's completely

00:15:09,940 --> 00:15:13,420
agnostic to it it doesn't have to know

00:15:11,950 --> 00:15:16,600
about rate limits it doesn't have to

00:15:13,420 --> 00:15:18,190
know about anything to do with security

00:15:16,600 --> 00:15:21,490
of the request Tyco terminates

00:15:18,190 --> 00:15:24,940
everything publicly and then just simply

00:15:21,490 --> 00:15:27,220
adds a header that says I validated that

00:15:24,940 --> 00:15:29,830
through other services that this user is

00:15:27,220 --> 00:15:32,829
user ID X that they have a valid token

00:15:29,830 --> 00:15:33,970
that here's the exploration time of the

00:15:32,829 --> 00:15:36,040
token in case you need it

00:15:33,970 --> 00:15:38,950
etc etc etc

00:15:36,040 --> 00:15:40,510
and so your application can focus on the

00:15:38,950 --> 00:15:41,550
application and doesn't have to worry

00:15:40,510 --> 00:15:46,060
about these things

00:15:41,550 --> 00:15:48,280
if you use Symphony or Zen or laravel

00:15:46,060 --> 00:15:50,260
this is very very similar to the

00:15:48,280 --> 00:15:52,420
middleware pattern applied at the

00:15:50,260 --> 00:15:55,090
services layer this is a middleware

00:15:52,420 --> 00:15:59,740
that's only for public requests for

00:15:55,090 --> 00:16:03,370
external requests internally every

00:15:59,740 --> 00:16:05,530
service use our pcs and use these custom

00:16:03,370 --> 00:16:07,780
headers to identify authentication

00:16:05,530 --> 00:16:10,690
information etc which made it really

00:16:07,780 --> 00:16:13,360
really easy to iterate the private

00:16:10,690 --> 00:16:15,370
servers behind the scene very very fast

00:16:13,360 --> 00:16:16,600
we were deploying these servers in

00:16:15,370 --> 00:16:18,820
production once we got everything up and

00:16:16,600 --> 00:16:20,890
running probably about four or five

00:16:18,820 --> 00:16:23,890
times a day and changing API is

00:16:20,890 --> 00:16:27,010
potentially as much as often whereas

00:16:23,890 --> 00:16:31,180
because of Tyco we were able to keep the

00:16:27,010 --> 00:16:36,250
public API the public REST API quite

00:16:31,180 --> 00:16:38,260
slow and quite stable phenomenal design

00:16:36,250 --> 00:16:40,570
pattern it worked really really well for

00:16:38,260 --> 00:16:42,010
us this is one of those type of

00:16:40,570 --> 00:16:44,950
decisions that I think that we got

00:16:42,010 --> 00:16:46,660
really really right early on and if I

00:16:44,950 --> 00:16:49,450
was building a system of this scale

00:16:46,660 --> 00:16:51,550
again I would absolutely use a API

00:16:49,450 --> 00:16:54,460
gateway of some sort we had fantastic

00:16:51,550 --> 00:16:55,930
luck with Tyco and Tyco worked well but

00:16:54,460 --> 00:17:01,750
I know there's plenty of other

00:16:55,930 --> 00:17:03,580
alternatives so that separates outside

00:17:01,750 --> 00:17:05,350
the firewall to inside the firewall and

00:17:03,580 --> 00:17:08,070
note that the front end server is

00:17:05,350 --> 00:17:11,080
outside the firewall and so that means

00:17:08,070 --> 00:17:13,870
nothing about the front end depended

00:17:11,080 --> 00:17:15,610
upon anything private about the back end

00:17:13,870 --> 00:17:18,280
we were developing against our public

00:17:15,610 --> 00:17:20,830
api's so when we released our API to

00:17:18,280 --> 00:17:23,470
clients to build off of as a third party

00:17:20,830 --> 00:17:25,420
API we knew that it worked because we

00:17:23,470 --> 00:17:26,950
had the front end the test of it we knew

00:17:25,420 --> 00:17:29,410
it solved the business problems because

00:17:26,950 --> 00:17:31,390
we had an active team of developers who

00:17:29,410 --> 00:17:33,400
was developing against it and we knew it

00:17:31,390 --> 00:17:35,350
solved the use cases completely because

00:17:33,400 --> 00:17:38,230
what we built our application off of it

00:17:35,350 --> 00:17:40,690
so aside from some like really really

00:17:38,230 --> 00:17:45,550
narrow use cases we were pretty

00:17:40,690 --> 00:17:50,289
confident in our public API the next set

00:17:45,550 --> 00:17:53,919
of layers is the backbone of this thing

00:17:50,289 --> 00:18:00,100
the message bus or RabbitMQ as we reach

00:17:53,919 --> 00:18:04,120
owes one of the problems we ran into we

00:18:00,100 --> 00:18:07,929
used RabbitMQ as a pseudo event sourcing

00:18:04,120 --> 00:18:09,940
database meaning every change to the

00:18:07,929 --> 00:18:13,360
application so if you update your

00:18:09,940 --> 00:18:16,149
username gets emitted as an event from

00:18:13,360 --> 00:18:17,559
the user service that event goes back

00:18:16,149 --> 00:18:20,350
into the

00:18:17,559 --> 00:18:23,169
rabbitmq other services can listen to it

00:18:20,350 --> 00:18:25,929
and adjust their state effectively and

00:18:23,169 --> 00:18:30,820
so we had only one set of shared state

00:18:25,929 --> 00:18:33,549
which was RabbitMQ I should note this we

00:18:30,820 --> 00:18:37,740
chose RabbitMQ specifically because at

00:18:33,549 --> 00:18:40,629
the time it had the best bindings in PHP

00:18:37,740 --> 00:18:42,759
today if I were to do this again I would

00:18:40,629 --> 00:18:45,789
choose Kafka in a heartbeat just because

00:18:42,759 --> 00:18:48,940
Kafka gets one of the biggest issues we

00:18:45,789 --> 00:18:51,250
ran into with rabbit it makes Kafka

00:18:48,940 --> 00:18:54,669
makes non-existent which is ordering of

00:18:51,250 --> 00:18:56,019
messages rabbit has the subtle property

00:18:54,669 --> 00:18:58,059
and it's not subtle it's actually

00:18:56,019 --> 00:19:00,129
blurred in big bold letters but we just

00:18:58,059 --> 00:19:03,399
kind of ignored it for some reason that

00:19:00,129 --> 00:19:05,740
you can't trust message order and so we

00:19:03,399 --> 00:19:07,929
had this archive service that would take

00:19:05,740 --> 00:19:10,090
the messages that every single message

00:19:07,929 --> 00:19:12,549
that went on the rabbitmq and put it

00:19:10,090 --> 00:19:14,620
into an archive well when you have

00:19:12,549 --> 00:19:16,149
unordered messages and you have an

00:19:14,620 --> 00:19:18,279
archive of unordered messages and the

00:19:16,149 --> 00:19:20,289
messages are changed or changes to the

00:19:18,279 --> 00:19:24,399
application what happens if those

00:19:20,289 --> 00:19:29,139
messages get reversed yeah that caused

00:19:24,399 --> 00:19:32,590
us a met a metric ton of pain and the

00:19:29,139 --> 00:19:34,629
biggest reason for that was these

00:19:32,590 --> 00:19:38,110
certainties were not directly querying

00:19:34,629 --> 00:19:40,450
by any of the applications so we had

00:19:38,110 --> 00:19:43,720
this event source database if you will

00:19:40,450 --> 00:19:47,350
in this archive but no applications

00:19:43,720 --> 00:19:49,120
actually bound to the event store and so

00:19:47,350 --> 00:19:50,559
if you study an event sourcing or if

00:19:49,120 --> 00:19:52,299
you've gone to any of the talks that's

00:19:50,559 --> 00:19:54,309
one of the most critical aspects to

00:19:52,299 --> 00:19:56,500
keeping an event source system

00:19:54,309 --> 00:19:58,690
consistent and so we got that really

00:19:56,500 --> 00:20:01,929
wrong and paid quite a bit of a price

00:19:58,690 --> 00:20:02,990
for that but the thing it did allow us

00:20:01,929 --> 00:20:05,299
to do

00:20:02,990 --> 00:20:07,880
was keep every application complete

00:20:05,299 --> 00:20:09,740
every service completely separated every

00:20:07,880 --> 00:20:12,529
service had its own caching system had

00:20:09,740 --> 00:20:15,470
its own database and was really its own

00:20:12,529 --> 00:20:18,320
isolated pod and it used rabbit to

00:20:15,470 --> 00:20:21,380
communicate and we communicated in two

00:20:18,320 --> 00:20:23,240
different types of methods one is an

00:20:21,380 --> 00:20:27,309
event which is kind of those application

00:20:23,240 --> 00:20:30,020
changes that I talked about the user

00:20:27,309 --> 00:20:35,059
updated their username username change

00:20:30,020 --> 00:20:36,919
event comes out and then our PCs we did

00:20:35,059 --> 00:20:38,929
quite a bit of work with video and video

00:20:36,919 --> 00:20:42,169
transcoding and so one of the common

00:20:38,929 --> 00:20:43,429
operations was a user uploads a video we

00:20:42,169 --> 00:20:45,640
needed transcode it into a bunch of

00:20:43,429 --> 00:20:48,380
different things we need to extract

00:20:45,640 --> 00:20:50,029
closed captions etc all of those

00:20:48,380 --> 00:20:53,350
processes they all take a lot of time

00:20:50,029 --> 00:20:56,809
and so you would send off an RPC request

00:20:53,350 --> 00:20:58,700
to RabbitMQ to tell it hey do the

00:20:56,809 --> 00:21:00,799
transcoding and then at some point in

00:20:58,700 --> 00:21:03,380
the future you would get a response

00:21:00,799 --> 00:21:05,720
message back saying the transcoding is

00:21:03,380 --> 00:21:08,899
done do the next thing that our PC

00:21:05,720 --> 00:21:13,480
system was actually quite reliable it

00:21:08,899 --> 00:21:16,490
was a phenomenal use of the platform so

00:21:13,480 --> 00:21:19,039
the final layer that we'll spend a good

00:21:16,490 --> 00:21:22,669
bit of time talking about today is the

00:21:19,039 --> 00:21:24,529
service layer the names here domain

00:21:22,669 --> 00:21:27,039
service async service and meta service

00:21:24,529 --> 00:21:30,490
or stuff that we came up with to try to

00:21:27,039 --> 00:21:33,679
distinguish the way applications behaved

00:21:30,490 --> 00:21:36,649
we had an internal platform that we used

00:21:33,679 --> 00:21:40,220
I wouldn't call it a framework I would

00:21:36,649 --> 00:21:42,429
call it a set of Composer files that

00:21:40,220 --> 00:21:44,480
brought in and wired together

00:21:42,429 --> 00:21:47,809
applications differently for each one of

00:21:44,480 --> 00:21:50,659
these services so for example async

00:21:47,809 --> 00:21:52,520
services need to talk to rabbitmq a lot

00:21:50,659 --> 00:21:54,590
and so we had PHP services that were

00:21:52,520 --> 00:21:57,559
built on this and so we had a base

00:21:54,590 --> 00:21:59,390
package for PHP services that listened

00:21:57,559 --> 00:22:02,419
to rabbit pulled it together so they

00:21:59,390 --> 00:22:06,820
weren't really framework although some

00:22:02,419 --> 00:22:06,820
of them did use open source frameworks

00:22:07,750 --> 00:22:13,190
the difference between these are kind of

00:22:10,490 --> 00:22:15,799
what purpose they serve inside of the

00:22:13,190 --> 00:22:16,910
platform the domain service is named

00:22:15,799 --> 00:22:19,970
domain

00:22:16,910 --> 00:22:21,950
because it's a domain entity if you've

00:22:19,970 --> 00:22:24,050
looked at domain driven design you look

00:22:21,950 --> 00:22:26,840
at the different entities you have we

00:22:24,050 --> 00:22:30,400
basically took every domain entity and

00:22:26,840 --> 00:22:33,650
stuck it in its own service give or take

00:22:30,400 --> 00:22:37,160
all communicate over HTTP and they have

00:22:33,650 --> 00:22:40,040
their own persistence I mentioned we do

00:22:37,160 --> 00:22:41,900
a lot of batch jobs async services they

00:22:40,040 --> 00:22:44,809
only communicate over APO

00:22:41,900 --> 00:22:47,690
RPC the only HTTP endpoint that they

00:22:44,809 --> 00:22:49,370
open up is a health check and then the

00:22:47,690 --> 00:22:50,720
meta services kind of fill the gaps

00:22:49,370 --> 00:22:52,550
between the two and I'll give you a

00:22:50,720 --> 00:22:55,190
concrete example of what that looks like

00:22:52,550 --> 00:22:57,410
so this is a simplified version of the

00:22:55,190 --> 00:23:00,410
domain that we were working with you

00:22:57,410 --> 00:23:03,230
have users you have assignments and a

00:23:00,410 --> 00:23:05,420
history object and then lessons with

00:23:03,230 --> 00:23:07,580
content and with assets so assets would

00:23:05,420 --> 00:23:10,280
be video files or images or anything

00:23:07,580 --> 00:23:12,410
like that content has many assets a

00:23:10,280 --> 00:23:14,750
lesson has many pieces of content and

00:23:12,410 --> 00:23:17,110
assignment has many lessons and then we

00:23:14,750 --> 00:23:20,150
keep track of what lessons the user sees

00:23:17,110 --> 00:23:22,850
not terribly complex model the reality

00:23:20,150 --> 00:23:25,460
was a little bit different but what we

00:23:22,850 --> 00:23:29,120
did was we built a service around each

00:23:25,460 --> 00:23:31,640
one of these primary entities and so we

00:23:29,120 --> 00:23:33,920
turned it into a user service and the

00:23:31,640 --> 00:23:36,230
Simon service a history service and

00:23:33,920 --> 00:23:40,100
lesson service a content service and an

00:23:36,230 --> 00:23:42,050
asset service that's a lot of services

00:23:40,100 --> 00:23:43,550
and especially when you throw that out

00:23:42,050 --> 00:23:46,370
to the rest of the application and the

00:23:43,550 --> 00:23:50,030
rest of the design we had probably about

00:23:46,370 --> 00:23:51,770
50 services within a year and it was

00:23:50,030 --> 00:23:57,470
growing quickly and the demand was quite

00:23:51,770 --> 00:23:59,600
high so the question comes let's say you

00:23:57,470 --> 00:24:02,660
want to show an assignment on the front

00:23:59,600 --> 00:24:05,450
end so the assignment remember has a set

00:24:02,660 --> 00:24:08,780
of users that it's associated to has a

00:24:05,450 --> 00:24:11,059
set of lessons etc on the front end in

00:24:08,780 --> 00:24:14,110
an administrative portal how would you

00:24:11,059 --> 00:24:17,630
render that well the naive way would be

00:24:14,110 --> 00:24:19,280
to call for the assignment and then loop

00:24:17,630 --> 00:24:21,710
over each one of the other objects and

00:24:19,280 --> 00:24:24,170
do another call and have you know a

00:24:21,710 --> 00:24:25,429
couple hundred thousand HTTP requests

00:24:24,170 --> 00:24:28,090
from the front-end in order to render

00:24:25,429 --> 00:24:30,170
the page we decided not to do that

00:24:28,090 --> 00:24:32,120
rightfully so

00:24:30,170 --> 00:24:35,049
we also decided not to go with something

00:24:32,120 --> 00:24:41,210
like graph QL to do that for us because

00:24:35,049 --> 00:24:42,950
two reasons one we saw the that basic

00:24:41,210 --> 00:24:45,350
the graph QL would just shift the

00:24:42,950 --> 00:24:47,690
problem to inside of our firewall rather

00:24:45,350 --> 00:24:50,630
than outside of our firewall but to and

00:24:47,690 --> 00:24:53,929
more importantly the assignment concept

00:24:50,630 --> 00:24:56,390
is a concept that in vote that comes

00:24:53,929 --> 00:25:00,860
over all of this so when we're shipping

00:24:56,390 --> 00:25:03,260
a public API I wanted to show that

00:25:00,860 --> 00:25:06,320
concept in that API so we wanted to have

00:25:03,260 --> 00:25:09,110
an API that actually returned the domain

00:25:06,320 --> 00:25:11,960
information for all of this and so what

00:25:09,110 --> 00:25:13,880
we built was a meta service the meta

00:25:11,960 --> 00:25:16,640
service knows how to talk to all of

00:25:13,880 --> 00:25:19,160
these services but it's also listening

00:25:16,640 --> 00:25:20,660
for changes on RabbitMQ so anytime you

00:25:19,160 --> 00:25:23,450
update some of one of these things it

00:25:20,660 --> 00:25:25,400
updates its local state so requesting

00:25:23,450 --> 00:25:27,980
from the front-end only has to hit

00:25:25,400 --> 00:25:29,570
there's one service which 99.9 percent

00:25:27,980 --> 00:25:31,850
of the time just fetches things from

00:25:29,570 --> 00:25:33,190
cache and pushes out of the cache so it

00:25:31,850 --> 00:25:36,080
wound up speeding up the application

00:25:33,190 --> 00:25:37,940
drastically and it wound up making it

00:25:36,080 --> 00:25:41,900
really really simple to work with from

00:25:37,940 --> 00:25:44,750
an API layer the problem comes what if

00:25:41,900 --> 00:25:46,220
we changed the question what if instead

00:25:44,750 --> 00:25:48,410
you want to get a list of lessons

00:25:46,220 --> 00:25:55,340
ordered by content author name

00:25:48,410 --> 00:25:59,870
I mean naively you would get every

00:25:55,340 --> 00:26:02,299
single user get every single lesson do

00:25:59,870 --> 00:26:05,270
the join and then order it we could

00:26:02,299 --> 00:26:06,919
create a lesson sorting service meta

00:26:05,270 --> 00:26:09,710
service but that would be really weird

00:26:06,919 --> 00:26:11,960
we could do a whole bunch of other

00:26:09,710 --> 00:26:14,630
things but every single solution is

00:26:11,960 --> 00:26:16,940
dirty and is weird the actual solution

00:26:14,630 --> 00:26:19,220
we eventually deployed was elastic

00:26:16,940 --> 00:26:21,320
search so we built a search service that

00:26:19,220 --> 00:26:26,059
would do these type of of highly

00:26:21,320 --> 00:26:28,130
aggregate queries for you so that's kind

00:26:26,059 --> 00:26:30,380
of the infrastructure of what we

00:26:28,130 --> 00:26:32,950
designed and started to build so let's

00:26:30,380 --> 00:26:37,070
talk about the infrastructure for a bit

00:26:32,950 --> 00:26:38,630
at the time there was a project that was

00:26:37,070 --> 00:26:41,750
just announced but was not actually

00:26:38,630 --> 00:26:43,250
released yet called kubernetes and so we

00:26:41,750 --> 00:26:43,820
actually couldn't run kubernetes because

00:26:43,250 --> 00:26:46,340
there was no

00:26:43,820 --> 00:26:49,060
Bernays to run and so we started to look

00:26:46,340 --> 00:26:51,560
at we knew we wanted to do containers

00:26:49,060 --> 00:26:52,790
primarily because we have go we have PHP

00:26:51,560 --> 00:26:57,650
we've got Scala

00:26:52,790 --> 00:26:59,000
we've got spark we've got Hadoop that

00:26:57,650 --> 00:27:01,310
we're all running in this in this

00:26:59,000 --> 00:27:03,650
infrastructure so we wanted some kind of

00:27:01,310 --> 00:27:06,950
a job worker to manage all of this

00:27:03,650 --> 00:27:08,930
I mentioned spark we had a production

00:27:06,950 --> 00:27:11,270
spark infrastructure on the prior

00:27:08,930 --> 00:27:13,280
application so we had a good bit of

00:27:11,270 --> 00:27:16,250
experience running a product called

00:27:13,280 --> 00:27:18,740
Apache mesas Apache may sauce is like

00:27:16,250 --> 00:27:22,190
kubernetes but not for containers for

00:27:18,740 --> 00:27:24,230
abstract jobs so it's a layer down so

00:27:22,190 --> 00:27:26,480
maysa runs on all of the different

00:27:24,230 --> 00:27:27,920
servers in hybrid environments so you

00:27:26,480 --> 00:27:30,350
can give it servers in multiple

00:27:27,920 --> 00:27:32,870
different data centers etc and then you

00:27:30,350 --> 00:27:34,760
give it jobs and it figures out how to

00:27:32,870 --> 00:27:36,920
run those jobs and handles that for you

00:27:34,760 --> 00:27:39,470
and there's also another open source

00:27:36,920 --> 00:27:43,250
software called marathon which basically

00:27:39,470 --> 00:27:45,950
binds meso s-- into docker so we've ran

00:27:43,250 --> 00:27:49,100
marathon on top of meso s-- and we could

00:27:45,950 --> 00:27:51,080
have one cluster that ran all of our

00:27:49,100 --> 00:27:53,870
data jobs as well as all of our HTTP

00:27:51,080 --> 00:27:55,700
jobs and so we wound up having was about

00:27:53,870 --> 00:27:58,910
I think it was about 40 to 50 servers

00:27:55,700 --> 00:28:01,790
that could deploy containers to and we

00:27:58,910 --> 00:28:03,470
basically we just had an API so when I

00:28:01,790 --> 00:28:06,200
wanted to deploy a new service I could

00:28:03,470 --> 00:28:08,330
just call an API and the new container

00:28:06,200 --> 00:28:11,200
was spun up I told it I want four

00:28:08,330 --> 00:28:14,810
instances meso figures out the rest we

00:28:11,200 --> 00:28:16,700
coded in rules that instance if there's

00:28:14,810 --> 00:28:19,880
multiple instances of a service they

00:28:16,700 --> 00:28:22,310
must ban data centers because again meso

00:28:19,880 --> 00:28:24,860
so smart it knows about where the

00:28:22,310 --> 00:28:27,980
servers are it knows how to orchestrate

00:28:24,860 --> 00:28:30,200
everything and so this is kind of what

00:28:27,980 --> 00:28:31,940
we had at the network layer we've got a

00:28:30,200 --> 00:28:36,140
whole bunch of servers that may Sosa's

00:28:31,940 --> 00:28:39,560
is running marathon is orchestrating how

00:28:36,140 --> 00:28:42,290
we route networks we used L because L

00:28:39,560 --> 00:28:44,540
beed Amazon EO B's are insanely reliable

00:28:42,290 --> 00:28:47,330
but they're also slow to update so we

00:28:44,540 --> 00:28:50,240
used H a proxy which could update in the

00:28:47,330 --> 00:28:53,060
span of milliseconds and so we built

00:28:50,240 --> 00:28:56,480
this tiered system to be as reliable yet

00:28:53,060 --> 00:28:57,560
as performant as possible and so a

00:28:56,480 --> 00:29:00,260
request

00:28:57,560 --> 00:29:03,260
would come in it would hit the elbe the

00:29:00,260 --> 00:29:05,330
EOB would call H a proxy H a proxy would

00:29:03,260 --> 00:29:06,800
then call Tyco which is running in a

00:29:05,330 --> 00:29:09,410
container just like any other service

00:29:06,800 --> 00:29:10,970
and then Tyco would do OAuth it would do

00:29:09,410 --> 00:29:13,070
rate limiting it would do all that fun

00:29:10,970 --> 00:29:15,890
stuff and it would figure out this is a

00:29:13,070 --> 00:29:19,550
call for an internal service so it calls

00:29:15,890 --> 00:29:21,950
down to that domain name which then hits

00:29:19,550 --> 00:29:24,740
that load balancer which then hits that

00:29:21,950 --> 00:29:29,450
H a proxy instance which then calls up

00:29:24,740 --> 00:29:32,000
into the service adds maybe 10

00:29:29,450 --> 00:29:33,740
milliseconds of latency yet we've tried

00:29:32,000 --> 00:29:36,620
to kill it we literally went in and

00:29:33,740 --> 00:29:38,960
killed maysa nodes we killed H a proxy

00:29:36,620 --> 00:29:43,100
nodes while load balancing it and it was

00:29:38,960 --> 00:29:44,600
surprisingly robust internal requests

00:29:43,100 --> 00:29:46,580
worked the same way except they skipped

00:29:44,600 --> 00:29:49,070
this to skip a step so if a wants to

00:29:46,580 --> 00:29:50,870
call a different service it just calls

00:29:49,070 --> 00:29:52,940
that service goes through the EOB and

00:29:50,870 --> 00:29:56,870
comes back into the service so what this

00:29:52,940 --> 00:29:58,580
means is if we want statistics on what's

00:29:56,870 --> 00:30:00,590
going on inside of our network we only

00:29:58,580 --> 00:30:02,210
need to look at the load balancers the

00:30:00,590 --> 00:30:05,030
load balancers will tell us every piece

00:30:02,210 --> 00:30:06,740
of communication because no one service

00:30:05,030 --> 00:30:09,110
can talk directly to eat to another

00:30:06,740 --> 00:30:11,990
service so we knew what was going on in

00:30:09,110 --> 00:30:14,030
the stack speaking of knowing what's

00:30:11,990 --> 00:30:17,540
going on in the stack let's talk about

00:30:14,030 --> 00:30:20,180
monitoring and logging we have Mae sews

00:30:17,540 --> 00:30:23,630
a host that's running a series of

00:30:20,180 --> 00:30:25,610
containers well we're running docker and

00:30:23,630 --> 00:30:28,370
so we want to collect the log so we put

00:30:25,610 --> 00:30:30,170
blog spouts in each host and log spouts

00:30:28,370 --> 00:30:33,440
pushed off to a Kafka queue which then

00:30:30,170 --> 00:30:35,180
pushed off into Data Dogg and s3 logs

00:30:33,440 --> 00:30:37,790
were available instantly for every

00:30:35,180 --> 00:30:39,650
single service and you didn't have to do

00:30:37,790 --> 00:30:42,680
a thing about it it was all automated

00:30:39,650 --> 00:30:44,180
for you even which service that came

00:30:42,680 --> 00:30:45,410
from was automated we had the docker

00:30:44,180 --> 00:30:48,700
container information and the host

00:30:45,410 --> 00:30:51,680
information so you could correlate this

00:30:48,700 --> 00:30:53,480
we also wanted to collect metrics so we

00:30:51,680 --> 00:30:55,610
deployed a stats D collector on every

00:30:53,480 --> 00:30:57,380
host so every host is tell every

00:30:55,610 --> 00:30:59,900
container is talking to a local deploy

00:30:57,380 --> 00:31:03,740
of stats D and so we got our metrics

00:30:59,900 --> 00:31:06,260
from our application and finally we

00:31:03,740 --> 00:31:09,050
wanted some tracing information and what

00:31:06,260 --> 00:31:10,590
that means is if one service calls

00:31:09,050 --> 00:31:12,659
another service

00:31:10,590 --> 00:31:15,960
we want to know that and we want to be

00:31:12,659 --> 00:31:18,990
able to see in debug well this request

00:31:15,960 --> 00:31:21,360
failed what was the original request and

00:31:18,990 --> 00:31:23,610
be able to trace backwards or this

00:31:21,360 --> 00:31:25,769
service is slow so let's see what else

00:31:23,610 --> 00:31:27,539
why it was slow what was slow in the in

00:31:25,769 --> 00:31:31,409
the query path and so we use something

00:31:27,539 --> 00:31:34,080
called Zipkin it was at the time really

00:31:31,409 --> 00:31:35,909
difficult to work with because it was

00:31:34,080 --> 00:31:37,889
right in the transition period before a

00:31:35,909 --> 00:31:39,960
major fork and so the open source

00:31:37,889 --> 00:31:42,779
version of Zipkin was difficult to run

00:31:39,960 --> 00:31:45,629
and didn't wasn't really stable yet but

00:31:42,779 --> 00:31:47,669
the closed source but available package

00:31:45,629 --> 00:31:48,960
was also really difficult to run so we

00:31:47,669 --> 00:31:52,259
spent a lot of time trying to get

00:31:48,960 --> 00:31:53,759
zipping up and running and towards the

00:31:52,259 --> 00:31:54,960
end we actually wound up did get it and

00:31:53,759 --> 00:31:56,940
getting it up and running I did wind up

00:31:54,960 --> 00:31:58,049
working out quite well but that's

00:31:56,940 --> 00:32:02,100
something I wish we would have done in

00:31:58,049 --> 00:32:04,289
quiter a bit earlier the final piece of

00:32:02,100 --> 00:32:06,990
the infrastructure puzzle is the

00:32:04,289 --> 00:32:09,389
services that JSON or service that JSON

00:32:06,990 --> 00:32:12,539
config file so I mentioned we had like

00:32:09,389 --> 00:32:14,759
50 services every service has its own

00:32:12,539 --> 00:32:16,769
docker can dev sorry github repository

00:32:14,759 --> 00:32:18,990
and in the root of that github

00:32:16,769 --> 00:32:22,860
repository we defined a service that

00:32:18,990 --> 00:32:25,440
JSON file define what the name of the

00:32:22,860 --> 00:32:27,889
service was what other services this

00:32:25,440 --> 00:32:30,960
depended upon so we knew what to spin up

00:32:27,889 --> 00:32:33,419
what type of data stores it needs note

00:32:30,960 --> 00:32:35,039
there's no authentication information

00:32:33,419 --> 00:32:37,259
because all of that was generated by

00:32:35,039 --> 00:32:39,029
deployment so we would automatically

00:32:37,259 --> 00:32:41,009
know that you needed a Postgres database

00:32:39,029 --> 00:32:43,710
so we'd spin up the Postgres instance

00:32:41,009 --> 00:32:45,769
create a database name user table etc

00:32:43,710 --> 00:32:48,179
and set all the environment variables

00:32:45,769 --> 00:32:51,119
100% automated so humans don't actually

00:32:48,179 --> 00:32:53,429
even have access to that information to

00:32:51,119 --> 00:32:55,080
find your health checks your service in

00:32:53,429 --> 00:32:58,559
type the public endpoints that you had

00:32:55,080 --> 00:33:01,350
and the deployment variables and so if

00:32:58,559 --> 00:33:03,809
towards the end the goal was if you

00:33:01,350 --> 00:33:07,019
wanted to deploy a new service all you

00:33:03,809 --> 00:33:09,269
have to do is create this file and then

00:33:07,019 --> 00:33:12,419
tell our central deployment system about

00:33:09,269 --> 00:33:14,399
that github repo and it did the rest it

00:33:12,419 --> 00:33:16,440
deployed it spun up all the instances

00:33:14,399 --> 00:33:18,330
every time you push to master it knew

00:33:16,440 --> 00:33:21,960
that it needed to rebuild a container it

00:33:18,330 --> 00:33:25,340
handled CI for you the goal was really

00:33:21,960 --> 00:33:27,840
quite doing everything

00:33:25,340 --> 00:33:29,940
so with all of this said this entire

00:33:27,840 --> 00:33:32,580
infrastructure section does any of this

00:33:29,940 --> 00:33:34,200
look familiar I mentioned at the start

00:33:32,580 --> 00:33:36,090
there was this new project called

00:33:34,200 --> 00:33:37,320
kubernetes that was that came out in the

00:33:36,090 --> 00:33:39,510
beginning right when we were starting

00:33:37,320 --> 00:33:41,520
this program but wasn't actually open

00:33:39,510 --> 00:33:45,060
source yet it was just announced this is

00:33:41,520 --> 00:33:47,040
basically kubernetes we built in large

00:33:45,060 --> 00:33:50,880
part of kubernetes and a large part of

00:33:47,040 --> 00:33:55,920
the architecture decisions we made by

00:33:50,880 --> 00:33:58,560
luck by knowledge of the team etc wound

00:33:55,920 --> 00:34:01,350
up being almost identical to the way

00:33:58,560 --> 00:34:02,550
kubernetes actually runs and so all of

00:34:01,350 --> 00:34:05,130
this stuff that idea showed you that we

00:34:02,550 --> 00:34:06,480
designed built and worked on we could

00:34:05,130 --> 00:34:09,900
have thrown away and just replaced with

00:34:06,480 --> 00:34:11,400
one spin up today at the top point in

00:34:09,900 --> 00:34:13,980
time again we were just a little bit too

00:34:11,400 --> 00:34:15,900
early to use to use scuba nets but today

00:34:13,980 --> 00:34:17,580
just use kubernetes and forget about

00:34:15,900 --> 00:34:19,050
that it doesn't mean you don't need an

00:34:17,580 --> 00:34:21,480
ops team it doesn't mean you don't need

00:34:19,050 --> 00:34:23,280
automation it just means the the

00:34:21,480 --> 00:34:28,770
infrastructure is handled for you and

00:34:23,280 --> 00:34:32,250
you need to maintain that piece so let's

00:34:28,770 --> 00:34:34,410
talk about local dev I mentioned we

00:34:32,250 --> 00:34:37,110
kicked off this whole process we had a

00:34:34,410 --> 00:34:40,080
three month window where we were looking

00:34:37,110 --> 00:34:41,790
to build the Minimum Viable Product so

00:34:40,080 --> 00:34:44,220
we built the first I think it was about

00:34:41,790 --> 00:34:46,440
ten services built the infrastructure

00:34:44,220 --> 00:34:48,120
got everything up and running and spent

00:34:46,440 --> 00:34:50,220
the last month of that three months

00:34:48,120 --> 00:34:53,429
trying to get it running in production

00:34:50,220 --> 00:34:54,960
and what we realized was that's going to

00:34:53,429 --> 00:34:56,610
be a nightmare we knew that's going to

00:34:54,960 --> 00:34:58,560
be a nightmare so let's try to make a

00:34:56,610 --> 00:35:00,930
local Devon spirit experience that's

00:34:58,560 --> 00:35:04,560
going to mimic what we're expecting in

00:35:00,930 --> 00:35:06,960
production what we intended and what we

00:35:04,560 --> 00:35:09,810
actually did build build was a

00:35:06,960 --> 00:35:10,890
command-line tool that you would run so

00:35:09,810 --> 00:35:13,710
you would check out that repository

00:35:10,890 --> 00:35:15,510
you'd run this command line tool it

00:35:13,710 --> 00:35:19,020
would look for the service definition

00:35:15,510 --> 00:35:24,330
and it would figure out well I need to

00:35:19,020 --> 00:35:26,940
spin up a database I need to spin up the

00:35:24,330 --> 00:35:28,890
dependent services etc and so it

00:35:26,940 --> 00:35:33,060
basically generated a docker compose

00:35:28,890 --> 00:35:36,540
file and then ran docker compose up and

00:35:33,060 --> 00:35:38,099
just spun up everything locally it all

00:35:36,540 --> 00:35:39,690
is designed to work frictionless

00:35:38,099 --> 00:35:41,339
was designed that every time you spun up

00:35:39,690 --> 00:35:43,289
these services it would run your

00:35:41,339 --> 00:35:45,569
migrations for you so when you had other

00:35:43,289 --> 00:35:47,400
services that you depended upon that

00:35:45,569 --> 00:35:49,440
we're moving quickly they stayed up to

00:35:47,400 --> 00:35:51,920
date their databases stayed up to date

00:35:49,440 --> 00:35:54,420
the data in them stayed up to date etc

00:35:51,920 --> 00:35:57,089
and it was all designed to wire

00:35:54,420 --> 00:36:00,210
everything together the same way we

00:35:57,089 --> 00:36:03,229
wanted to wire together fraud and what

00:36:00,210 --> 00:36:05,700
actually happened was that was not used

00:36:03,229 --> 00:36:08,789
every developer ran their own service

00:36:05,700 --> 00:36:11,940
natively they ran Apache or nginx on

00:36:08,789 --> 00:36:14,160
their local machine PHP fpm built the

00:36:11,940 --> 00:36:16,140
service if they needed to talk to

00:36:14,160 --> 00:36:18,660
another service nine times out of ten

00:36:16,140 --> 00:36:20,009
they just like kind of faked it and made

00:36:18,660 --> 00:36:23,759
their own little endpoints that they

00:36:20,009 --> 00:36:25,859
could call or mocked out that portion of

00:36:23,759 --> 00:36:30,329
the application so that they didn't call

00:36:25,859 --> 00:36:33,150
the other services they never ran

00:36:30,329 --> 00:36:34,890
sometimes they would actually spin up

00:36:33,150 --> 00:36:36,359
the service they depended upon on their

00:36:34,890 --> 00:36:38,430
local machine they would talk to the dev

00:36:36,359 --> 00:36:41,670
of that other service and say help me

00:36:38,430 --> 00:36:45,529
get this thing up and running and what

00:36:41,670 --> 00:36:48,779
happened was they got stale so service a

00:36:45,529 --> 00:36:50,700
dependent upon service B so I get it up

00:36:48,779 --> 00:36:52,829
and running on my machine today next

00:36:50,700 --> 00:36:55,049
week when I go to ship my service I'm

00:36:52,829 --> 00:36:58,019
depending on an old version so old that

00:36:55,049 --> 00:36:59,640
when I try to deploy I need to go fix

00:36:58,019 --> 00:37:01,920
all of those inconsistencies between the

00:36:59,640 --> 00:37:03,380
two it was a nightmare it was an

00:37:01,920 --> 00:37:05,759
absolute nightmare

00:37:03,380 --> 00:37:08,160
so we started we stopped and we thought

00:37:05,759 --> 00:37:11,729
about well why why is no one using the

00:37:08,160 --> 00:37:14,160
the local dev system and it turns out

00:37:11,729 --> 00:37:16,650
there were a couple key reasons it was

00:37:14,160 --> 00:37:18,930
excruciatingly least-loved spinning up

00:37:16,650 --> 00:37:21,119
because it had to configure sometimes up

00:37:18,930 --> 00:37:22,950
to like five to ten other containers and

00:37:21,119 --> 00:37:24,599
other services and build them all

00:37:22,950 --> 00:37:27,269
including database containers and

00:37:24,599 --> 00:37:29,789
everything the spin up time was about 15

00:37:27,269 --> 00:37:31,589
minutes from the time that you said give

00:37:29,789 --> 00:37:32,729
me a new dev environment to the time

00:37:31,589 --> 00:37:34,829
that your new dev environment is ready

00:37:32,729 --> 00:37:37,499
to work about 15 minutes my codes

00:37:34,829 --> 00:37:39,569
compiling yeah it was difficult to use

00:37:37,499 --> 00:37:41,430
it frequently broke or frequently got

00:37:39,569 --> 00:37:44,569
into inconsistent states that you had to

00:37:41,430 --> 00:37:46,499
manually just kill it all and rebuild it

00:37:44,569 --> 00:37:49,710
and it's just kind of really just

00:37:46,499 --> 00:37:52,140
unreliable but that's not why it wasn't

00:37:49,710 --> 00:37:54,290
used it wasn't you

00:37:52,140 --> 00:37:58,020
because it was someone else's problem a

00:37:54,290 --> 00:38:00,600
person on the team was picked to build

00:37:58,020 --> 00:38:03,510
the local dev experience and to maintain

00:38:00,600 --> 00:38:05,700
that local Devon's experience and they

00:38:03,510 --> 00:38:08,640
did that but all the rest of the devs

00:38:05,700 --> 00:38:10,170
were told to just use this thing but

00:38:08,640 --> 00:38:12,780
were never given any ownership over it

00:38:10,170 --> 00:38:14,160
so they never had any investment to fix

00:38:12,780 --> 00:38:16,860
any of the problems when something went

00:38:14,160 --> 00:38:20,100
wrong well I got to get my work done so

00:38:16,860 --> 00:38:22,230
hey you go fix the problem I'm gonna go

00:38:20,100 --> 00:38:24,030
work over here locally and so it just

00:38:22,230 --> 00:38:26,370
never got adoption because it was always

00:38:24,030 --> 00:38:29,070
someone else's problem that's I think

00:38:26,370 --> 00:38:30,630
the biggest mistake from the

00:38:29,070 --> 00:38:33,540
infrastructure side and from the ops

00:38:30,630 --> 00:38:35,840
side that we made was not making local

00:38:33,540 --> 00:38:38,400
dev every single developers problem

00:38:35,840 --> 00:38:42,500
making it someone else's tool that they

00:38:38,400 --> 00:38:44,610
maintained really really bit us hard and

00:38:42,500 --> 00:38:48,510
so trying to get this thing up and

00:38:44,610 --> 00:38:51,870
running was quite a bit of fun and quite

00:38:48,510 --> 00:38:53,610
a bit of a challenge I mentioned we we

00:38:51,870 --> 00:38:55,220
got the first set of services up in two

00:38:53,610 --> 00:38:57,660
months out of that three month window

00:38:55,220 --> 00:39:01,170
getting the first word production

00:38:57,660 --> 00:39:03,180
request took another month think about

00:39:01,170 --> 00:39:05,550
that we're code complete everything

00:39:03,180 --> 00:39:08,010
works locally getting it up and running

00:39:05,550 --> 00:39:14,190
in a production infrastructure took 30

00:39:08,010 --> 00:39:16,110
days ouch it failed for a whole bunch of

00:39:14,190 --> 00:39:17,970
reasons some of which are probably the

00:39:16,110 --> 00:39:20,460
text book you're looking at going up yep

00:39:17,970 --> 00:39:23,340
I've been there been there some of which

00:39:20,460 --> 00:39:25,760
were due to the the failures of local

00:39:23,340 --> 00:39:28,140
dev and consistent local dev experiences

00:39:25,760 --> 00:39:31,410
and some of it was just bad management

00:39:28,140 --> 00:39:34,800
on my part and the part of my team where

00:39:31,410 --> 00:39:37,140
we built services to idealized behavior

00:39:34,800 --> 00:39:39,570
we didn't actually have real mettle to

00:39:37,140 --> 00:39:42,090
try this stuff on to learn how it works

00:39:39,570 --> 00:39:43,200
so there were some just learning kinks

00:39:42,090 --> 00:39:45,570
that we had to get out

00:39:43,200 --> 00:39:48,450
I mentioned services were built in

00:39:45,570 --> 00:39:51,720
isolation but the biggest problem was we

00:39:48,450 --> 00:39:54,060
got to code complete at month two but we

00:39:51,720 --> 00:39:55,530
still had more product to build so all

00:39:54,060 --> 00:39:57,630
of the production all of the product

00:39:55,530 --> 00:40:00,570
engineers were off building new services

00:39:57,630 --> 00:40:02,700
while infra was finished finishing and

00:40:00,570 --> 00:40:05,910
finalizing the infrastructure and by the

00:40:02,700 --> 00:40:08,310
time we ran into problems devs were off

00:40:05,910 --> 00:40:11,340
those services for a week or two and

00:40:08,310 --> 00:40:13,140
that context switching back to oppai got

00:40:11,340 --> 00:40:15,540
to fix this bug okay nope I've got to

00:40:13,140 --> 00:40:18,570
fix that cost us a ludicrous amount of

00:40:15,540 --> 00:40:20,880
time and the API is we're evolving

00:40:18,570 --> 00:40:25,440
rapidly which led to all sorts of

00:40:20,880 --> 00:40:28,320
problems trying to maintain it I'm not

00:40:25,440 --> 00:40:30,090
going to read every one of these this is

00:40:28,320 --> 00:40:32,340
kind of like a dump of some of the big

00:40:30,090 --> 00:40:34,140
takeaways from getting it up and running

00:40:32,340 --> 00:40:36,390
in prod take a look at the slides

00:40:34,140 --> 00:40:38,790
afterwards or take a picture here the

00:40:36,390 --> 00:40:41,850
one I want to really call out is this

00:40:38,790 --> 00:40:44,160
guy RabbitMQ events for missing data or

00:40:41,850 --> 00:40:48,930
structure so I mentioned we used

00:40:44,160 --> 00:40:51,450
RabbitMQ as and event source well what

00:40:48,930 --> 00:40:52,560
happens if your event source UJ all of

00:40:51,450 --> 00:40:56,280
your changes that the entire application

00:40:52,560 --> 00:40:58,770
relies upon or missing data that's a

00:40:56,280 --> 00:41:01,650
pretty critical issue the problem was

00:40:58,770 --> 00:41:03,900
our services were all tested incredibly

00:41:01,650 --> 00:41:05,430
well we had about we had over a hundred

00:41:03,900 --> 00:41:06,450
percent coverage I say over a hundred

00:41:05,430 --> 00:41:08,400
percent coverage because we had a

00:41:06,450 --> 00:41:10,710
hundred percent line coverage but we

00:41:08,400 --> 00:41:12,960
actually had very very reviewed unit

00:41:10,710 --> 00:41:15,480
tests for every single service the

00:41:12,960 --> 00:41:18,480
services were insanely well tested from

00:41:15,480 --> 00:41:20,880
a unit test perspective so we tested the

00:41:18,480 --> 00:41:23,850
when an HTTP request comes in that it

00:41:20,880 --> 00:41:27,480
triggers the change that we expected to

00:41:23,850 --> 00:41:28,920
the model to the model layer we tested

00:41:27,480 --> 00:41:30,390
that when the model interchange model

00:41:28,920 --> 00:41:31,560
layer changes correctly we get the

00:41:30,390 --> 00:41:34,080
change persistence the database

00:41:31,560 --> 00:41:36,180
correctly we know that when that model

00:41:34,080 --> 00:41:37,590
layer changes we get an event that gets

00:41:36,180 --> 00:41:39,840
emitted to the events to work correctly

00:41:37,590 --> 00:41:42,990
with the correct information all of that

00:41:39,840 --> 00:41:45,570
was tested but we didn't test was all of

00:41:42,990 --> 00:41:48,510
that put together we had no smoke test

00:41:45,570 --> 00:41:51,330
that when an HTTP request came in it did

00:41:48,510 --> 00:41:53,790
all of those things together and what

00:41:51,330 --> 00:41:57,150
happened was we had a wiring error we're

00:41:53,790 --> 00:41:59,190
in a couple services the wiring from the

00:41:57,150 --> 00:42:02,130
request coming in to the event going out

00:41:59,190 --> 00:42:03,930
was just ever so slightly misconfigured

00:42:02,130 --> 00:42:07,110
and so the events were still going out

00:42:03,930 --> 00:42:09,570
they just had no data and so user

00:42:07,110 --> 00:42:13,440
changes we're admitting events that were

00:42:09,570 --> 00:42:14,970
empty how do we find this out well I

00:42:13,440 --> 00:42:16,470
would love to sit here and say well we

00:42:14,970 --> 00:42:19,140
had instrumentation we had a service

00:42:16,470 --> 00:42:19,800
that identified this and error reporting

00:42:19,140 --> 00:42:21,630
was great

00:42:19,800 --> 00:42:24,150
we found out because we went to try to

00:42:21,630 --> 00:42:25,620
replay the events to restore to build up

00:42:24,150 --> 00:42:27,690
a new service and we're starting to get

00:42:25,620 --> 00:42:30,600
errors building that new service after

00:42:27,690 --> 00:42:32,100
about a month of uptime and went Oh crud

00:42:30,600 --> 00:42:34,800
this is not good

00:42:32,100 --> 00:42:36,510
we lost the production data we did mine

00:42:34,800 --> 00:42:39,480
it managed to resolve that we did

00:42:36,510 --> 00:42:42,540
rebuild but it cost us a pretty

00:42:39,480 --> 00:42:45,060
significant amount of time the lesson is

00:42:42,540 --> 00:42:47,190
always smoke test in addition so I'm not

00:42:45,060 --> 00:42:48,570
saying smoke test every path but make

00:42:47,190 --> 00:42:50,280
sure that the application is wired

00:42:48,570 --> 00:42:53,010
together correctly unit tests are not

00:42:50,280 --> 00:42:57,050
enough especially when doing something

00:42:53,010 --> 00:42:59,430
as complicated as a distributed system

00:42:57,050 --> 00:43:02,310
the final major point that I want to

00:42:59,430 --> 00:43:03,870
cover is dealing with change so we have

00:43:02,310 --> 00:43:07,590
this thing out in prod we're serving

00:43:03,870 --> 00:43:10,740
customer production traffic and product

00:43:07,590 --> 00:43:12,660
changes come in could be because we got

00:43:10,740 --> 00:43:14,250
client feedback that says the thing that

00:43:12,660 --> 00:43:16,170
we built wasn't exactly what we need

00:43:14,250 --> 00:43:17,490
that happens that's good we should do

00:43:16,170 --> 00:43:20,280
that we should respond to that it

00:43:17,490 --> 00:43:22,530
partially was executive stakeholders

00:43:20,280 --> 00:43:25,080
coming in saying I want X or e I think

00:43:22,530 --> 00:43:26,580
we need to do Y which is fine and

00:43:25,080 --> 00:43:28,520
natural I don't think any of those were

00:43:26,580 --> 00:43:31,800
like egregious or anything it's just

00:43:28,520 --> 00:43:33,390
natural course of product progress and

00:43:31,800 --> 00:43:35,880
sometimes we just didn't get it right

00:43:33,390 --> 00:43:37,800
the first time and we needed to iterate

00:43:35,880 --> 00:43:41,610
a couple times a hundred percent natural

00:43:37,800 --> 00:43:44,610
change is natural to software let's take

00:43:41,610 --> 00:43:48,390
an example of a real change let's say

00:43:44,610 --> 00:43:50,370
this is a hierarchy of data and let's

00:43:48,390 --> 00:43:54,000
just say each one of these is a service

00:43:50,370 --> 00:43:57,260
it wasn't in our case but let's just do

00:43:54,000 --> 00:44:00,510
this for for for example purposes a

00:43:57,260 --> 00:44:03,150
program has many topics each topic as

00:44:00,510 --> 00:44:04,970
many lessons each lesson has many cards

00:44:03,150 --> 00:44:08,970
in each card has many assets

00:44:04,970 --> 00:44:12,660
what happens if product asks you to add

00:44:08,970 --> 00:44:14,100
in a new layer to this and when I say a

00:44:12,660 --> 00:44:16,260
new layer I'm not talking about like

00:44:14,100 --> 00:44:18,240
these are categories that are just a

00:44:16,260 --> 00:44:20,370
taxonomy like these are each domain

00:44:18,240 --> 00:44:21,180
objects with actual business rules built

00:44:20,370 --> 00:44:23,130
against them

00:44:21,180 --> 00:44:26,430
how would you refactor that in a

00:44:23,130 --> 00:44:29,370
monolith that's actually probably pretty

00:44:26,430 --> 00:44:31,760
easy you just add some of the new logic

00:44:29,370 --> 00:44:33,630
and then do a migration and you're done

00:44:31,760 --> 00:44:35,820
what about removing one

00:44:33,630 --> 00:44:39,390
that's probably even easier in an in a

00:44:35,820 --> 00:44:40,650
monolithic application context but when

00:44:39,390 --> 00:44:46,320
you talk about services

00:44:40,650 --> 00:44:50,610
how would you refactor this well it's

00:44:46,320 --> 00:44:52,650
not easy the refactoring actually was

00:44:50,610 --> 00:44:54,450
one of the most difficult parts because

00:44:52,650 --> 00:44:58,200
we had to do three stage where factors

00:44:54,450 --> 00:45:00,960
stage one was well actually four stages

00:44:58,200 --> 00:45:03,240
stage one was figure out what the change

00:45:00,960 --> 00:45:05,310
was that we wanted to do so we wanted to

00:45:03,240 --> 00:45:08,400
change these four api's and create this

00:45:05,310 --> 00:45:10,890
this fifth API we needed to go to every

00:45:08,400 --> 00:45:13,650
service that depended upon the one we

00:45:10,890 --> 00:45:15,420
wanted to change make it so that that

00:45:13,650 --> 00:45:17,010
service could accept either version

00:45:15,420 --> 00:45:20,010
either the old version or the new

00:45:17,010 --> 00:45:23,760
version deploy all of those make sure

00:45:20,010 --> 00:45:25,530
that's up and running stable then deploy

00:45:23,760 --> 00:45:27,810
the new version of the target service

00:45:25,530 --> 00:45:29,790
and then finally go back and clean up

00:45:27,810 --> 00:45:31,740
and remove all of those conditional

00:45:29,790 --> 00:45:36,090
objects to depend to allow that the old

00:45:31,740 --> 00:45:39,270
format what would have been a simple one

00:45:36,090 --> 00:45:41,330
day change that was just involved in ETL

00:45:39,270 --> 00:45:44,790
on your application and a migration

00:45:41,330 --> 00:45:47,000
became major major coordinated surgery

00:45:44,790 --> 00:45:49,860
that required almost the entire team

00:45:47,000 --> 00:45:52,650
that I think was the biggest failure of

00:45:49,860 --> 00:45:54,480
the architecture was it made it really

00:45:52,650 --> 00:45:56,580
easy to respond to change the

00:45:54,480 --> 00:45:59,040
micro-level it made it nearly impossible

00:45:56,580 --> 00:46:03,300
to respond to change at the macro level

00:45:59,040 --> 00:46:04,920
at the larger level and so wrapping up

00:46:03,300 --> 00:46:09,870
only summarize a couple of the key

00:46:04,920 --> 00:46:12,150
takeaways first off I mentioned that we

00:46:09,870 --> 00:46:14,300
designed our services really small at

00:46:12,150 --> 00:46:17,880
the entity level we didn't appreciate

00:46:14,300 --> 00:46:20,460
how unreliable service calls are I mean

00:46:17,880 --> 00:46:23,670
you say it out loud and it's yeah we

00:46:20,460 --> 00:46:27,170
know HTTP is unreliable but how

00:46:23,670 --> 00:46:30,690
unreliable is it let me ask a question

00:46:27,170 --> 00:46:32,790
how often do you expect a method call to

00:46:30,690 --> 00:46:36,060
fail I'm not saying what the method does

00:46:32,790 --> 00:46:39,960
I'm saying the method call itself to not

00:46:36,060 --> 00:46:43,470
execute the code in that method one of

00:46:39,960 --> 00:46:46,200
maybe a billion requests when I started

00:46:43,470 --> 00:46:47,099
one in a billion method calls no they're

00:46:46,200 --> 00:46:50,249
more reliable

00:46:47,099 --> 00:46:54,450
that one in whatever that large number

00:46:50,249 --> 00:46:58,319
is incidentally this this large number

00:46:54,450 --> 00:47:01,200
actually does have a a root if you look

00:46:58,319 --> 00:47:03,119
at Symphony and you take a like a hello

00:47:01,200 --> 00:47:05,519
world style or sorry not a hello world a

00:47:03,119 --> 00:47:07,319
real world Symphony application and just

00:47:05,519 --> 00:47:10,619
look at the number of methods it

00:47:07,319 --> 00:47:16,079
executes in one request and then

00:47:10,619 --> 00:47:17,880
multiply that by 99.999% up time you

00:47:16,079 --> 00:47:21,119
wind up getting roughly that number and

00:47:17,880 --> 00:47:23,069
so if the reason for a symphony

00:47:21,119 --> 00:47:26,099
application failing was a method call

00:47:23,069 --> 00:47:27,779
failing and not like HTTP request or

00:47:26,099 --> 00:47:29,160
database issue or anything like that

00:47:27,779 --> 00:47:31,920
you would get something about that that

00:47:29,160 --> 00:47:33,690
method and reliability in practice

00:47:31,920 --> 00:47:34,920
though you don't think about it we don't

00:47:33,690 --> 00:47:36,450
think about it when we do

00:47:34,920 --> 00:47:38,039
object-oriented design when you talk

00:47:36,450 --> 00:47:40,259
about solid when you talk about design

00:47:38,039 --> 00:47:41,729
patterns every single one of those

00:47:40,259 --> 00:47:44,430
things with the exception of like the

00:47:41,729 --> 00:47:46,529
proxy design pattern is based on the

00:47:44,430 --> 00:47:51,930
fundamental assumption that method calls

00:47:46,529 --> 00:47:53,759
are 100% reliable I guarantee you none

00:47:51,930 --> 00:47:55,410
of you when writing code ever think

00:47:53,759 --> 00:47:56,940
about well what if this code method

00:47:55,410 --> 00:48:00,089
doesn't actually enter the code that I'm

00:47:56,940 --> 00:48:02,700
going into yet with a service call that

00:48:00,089 --> 00:48:05,069
is absolutely going to happen over and

00:48:02,700 --> 00:48:08,460
over and over again even if you're able

00:48:05,069 --> 00:48:10,349
to do five nines of uptime that's one

00:48:08,460 --> 00:48:13,349
even every 100,000 requests is gonna

00:48:10,349 --> 00:48:17,940
fail and so the biggest error that we

00:48:13,349 --> 00:48:20,160
made was how big of a service should you

00:48:17,940 --> 00:48:22,979
make we went for the domain size because

00:48:20,160 --> 00:48:24,239
we were really good at domain design and

00:48:22,979 --> 00:48:26,579
we were really good at object-oriented

00:48:24,239 --> 00:48:27,660
design and we decided to treat micro

00:48:26,579 --> 00:48:31,229
services and service-oriented

00:48:27,660 --> 00:48:34,710
architecture as objects and the reality

00:48:31,229 --> 00:48:36,259
is that was a horrible idea and so what

00:48:34,710 --> 00:48:38,999
I would have done him what we actually

00:48:36,259 --> 00:48:43,589
refactored to at the very very end and

00:48:38,999 --> 00:48:46,380
after I had left was this so have a

00:48:43,589 --> 00:48:48,420
couple of services where the domain

00:48:46,380 --> 00:48:52,200
boundaries are very very narrow and very

00:48:48,420 --> 00:48:54,739
very defined and that's it the lesson

00:48:52,200 --> 00:48:58,469
service handles everything about lessons

00:48:54,739 --> 00:49:01,170
one of the guys on our team coined the

00:48:58,469 --> 00:49:02,609
term micro with and use the

00:49:01,170 --> 00:49:05,849
Micra lists expression to define this

00:49:02,609 --> 00:49:08,609
start your services as micro lists as

00:49:05,849 --> 00:49:11,490
large as possible because it is far

00:49:08,609 --> 00:49:14,400
easier to split apart a large system

00:49:11,490 --> 00:49:17,069
than it is to stitch two dynamic systems

00:49:14,400 --> 00:49:18,869
together especially when there's a lot

00:49:17,069 --> 00:49:21,150
of dependencies and a lot of network

00:49:18,869 --> 00:49:22,710
traversal between them build it large

00:49:21,150 --> 00:49:24,900
and then when you need to split or

00:49:22,710 --> 00:49:30,680
refactor something out and do that it's

00:49:24,900 --> 00:49:35,730
far easier my number one recommendation

00:49:30,680 --> 00:49:36,990
do not do micro services in reality

00:49:35,730 --> 00:49:38,579
there is a caveat here I'm not

00:49:36,990 --> 00:49:40,410
suggesting micro services are a bad

00:49:38,579 --> 00:49:42,240
pattern obviously there's lots of very

00:49:40,410 --> 00:49:43,890
large teams and very successful teams

00:49:42,240 --> 00:49:47,880
who do it very very well

00:49:43,890 --> 00:49:50,130
but do not do it period unless you have

00:49:47,880 --> 00:49:51,990
a dedicated team to tooling and

00:49:50,130 --> 00:49:53,970
infrastructure and I'm not just talking

00:49:51,990 --> 00:49:56,130
about production pooling I am talking

00:49:53,970 --> 00:49:58,770
about developer tooling if it is not

00:49:56,130 --> 00:50:00,809
easy for your build systems to work and

00:49:58,770 --> 00:50:03,750
for you to run tests and testing

00:50:00,809 --> 00:50:06,470
infrastructure and do your own local dev

00:50:03,750 --> 00:50:10,109
in a hundred percent automated fashion

00:50:06,470 --> 00:50:12,890
you will have a bad time at scale with

00:50:10,109 --> 00:50:16,140
this type of a complexity system I

00:50:12,890 --> 00:50:18,599
mentioned start with big services it's

00:50:16,140 --> 00:50:22,440
easier to split a large instance and it

00:50:18,599 --> 00:50:26,010
is to stitch together to automate

00:50:22,440 --> 00:50:30,000
everything deployment is the obvious one

00:50:26,010 --> 00:50:31,829
but I'm talking far lower than that your

00:50:30,000 --> 00:50:34,500
spin up getting a new service into your

00:50:31,829 --> 00:50:36,420
infrastructure automate that automated

00:50:34,500 --> 00:50:38,299
deployment automate your migration are

00:50:36,420 --> 00:50:41,040
you using a migration tool that's

00:50:38,299 --> 00:50:44,359
dependent upon your service we're using

00:50:41,040 --> 00:50:47,640
one tool for all of your services

00:50:44,359 --> 00:50:52,020
automate your backups state restoration

00:50:47,640 --> 00:50:54,569
is a huge one if you want to reproduce a

00:50:52,020 --> 00:50:57,059
production state locally without

00:50:54,569 --> 00:50:57,619
downloading production data how do you

00:50:57,059 --> 00:51:00,030
do that

00:50:57,619 --> 00:51:02,579
how can you set up all of the objects

00:51:00,030 --> 00:51:06,260
all the services in the right state it's

00:51:02,579 --> 00:51:08,970
actually a pretty difficult problem

00:51:06,260 --> 00:51:11,250
don't plan for failure live it this is

00:51:08,970 --> 00:51:13,530
the biggest takeaway I mentioned don't

00:51:11,250 --> 00:51:15,050
do micro services I still use service

00:51:13,530 --> 00:51:16,940
architectures and I still use

00:51:15,050 --> 00:51:19,100
you serve services and I still would

00:51:16,940 --> 00:51:20,990
recommend people using services the

00:51:19,100 --> 00:51:23,540
biggest change to how I approach writing

00:51:20,990 --> 00:51:26,120
that code is I write the failure case

00:51:23,540 --> 00:51:28,400
first so when I have a service that's

00:51:26,120 --> 00:51:31,010
going to call a second service the first

00:51:28,400 --> 00:51:32,930
thing I write and I test and I deploy is

00:51:31,010 --> 00:51:35,900
what happens if that service was missing

00:51:32,930 --> 00:51:39,350
how am I going to recover how am I going

00:51:35,900 --> 00:51:41,690
to gracefully handle that issue get that

00:51:39,350 --> 00:51:43,790
in test it and make sure that solid and

00:51:41,690 --> 00:51:46,100
then handle the happy path because if

00:51:43,790 --> 00:51:47,750
you focus on the happy path first you're

00:51:46,100 --> 00:51:49,880
going to code yourself into a corner and

00:51:47,750 --> 00:51:52,730
you're going to have a bad time at some

00:51:49,880 --> 00:51:53,840
point in time and the finest one would

00:51:52,730 --> 00:51:56,330
the final one which I didn't really

00:51:53,840 --> 00:51:59,270
touch too much here is around service

00:51:56,330 --> 00:52:01,160
level objectives if you haven't read the

00:51:59,270 --> 00:52:04,040
Google SRA book I would highly highly

00:52:01,160 --> 00:52:05,420
recommend it it's a phenomenal book but

00:52:04,040 --> 00:52:07,990
one of the big things that they focus on

00:52:05,420 --> 00:52:11,360
is business service level objectives

00:52:07,990 --> 00:52:13,640
what I mean by that is for every service

00:52:11,360 --> 00:52:16,070
we tracked a whole bunch of metrics we

00:52:13,640 --> 00:52:19,660
tracked time to first buy we tracked

00:52:16,070 --> 00:52:22,880
average response time we tracked

00:52:19,660 --> 00:52:24,740
requests per second etc but the business

00:52:22,880 --> 00:52:26,990
doesn't care about requests per second

00:52:24,740 --> 00:52:28,840
to your users table they care about the

00:52:26,990 --> 00:52:31,820
number of successful authentications

00:52:28,840 --> 00:52:34,310
they care about the ratio of

00:52:31,820 --> 00:52:35,840
authentication failures they care about

00:52:34,310 --> 00:52:37,910
the number of lessons you're actually

00:52:35,840 --> 00:52:40,160
able to deliver they care about all of

00:52:37,910 --> 00:52:41,960
these other metrics the technical

00:52:40,160 --> 00:52:44,930
metrics are important I'm not saying not

00:52:41,960 --> 00:52:46,850
to measure those but think about what is

00:52:44,930 --> 00:52:48,590
the business case for this service why

00:52:46,850 --> 00:52:52,070
does the business care that this service

00:52:48,590 --> 00:52:53,480
exists define those objectives and get

00:52:52,070 --> 00:52:54,650
them into code get them in the code

00:52:53,480 --> 00:52:57,260
early get them in your monitoring

00:52:54,650 --> 00:52:58,820
systems and be sure that your service is

00:52:57,260 --> 00:53:04,190
operating to what the business needs of

00:52:58,820 --> 00:53:06,200
it not what the geeky technical need the

00:53:04,190 --> 00:53:11,120
biggest takeaway that I've got from this

00:53:06,200 --> 00:53:12,350
entire experience is what we do as

00:53:11,120 --> 00:53:15,350
software engineers or software

00:53:12,350 --> 00:53:17,840
developers as programmers on one level

00:53:15,350 --> 00:53:20,720
is writing the code to solve a problem

00:53:17,840 --> 00:53:23,990
but when you get beyond that I think our

00:53:20,720 --> 00:53:27,080
number one job is managing complexity we

00:53:23,990 --> 00:53:27,990
can write complexity into our source

00:53:27,080 --> 00:53:31,710
code

00:53:27,990 --> 00:53:33,150
of us the 1998 way of doing things where

00:53:31,710 --> 00:53:36,869
you just have a single-page spaghetti

00:53:33,150 --> 00:53:38,640
app and the complexity of the system is

00:53:36,869 --> 00:53:40,110
really really small but there's a

00:53:38,640 --> 00:53:40,860
complexity of what you're looking at is

00:53:40,110 --> 00:53:43,080
quite high

00:53:40,860 --> 00:53:44,970
you can refactor and may use more

00:53:43,080 --> 00:53:47,250
object-oriented design where the object

00:53:44,970 --> 00:53:51,180
that you're looking at is quite simple

00:53:47,250 --> 00:53:55,530
but at the expense of more complicated

00:53:51,180 --> 00:53:58,800
code elsewhere you can use modern

00:53:55,530 --> 00:54:01,110
frameworks which or you know there was a

00:53:58,800 --> 00:54:03,330
controversial post about visual debt a

00:54:01,110 --> 00:54:04,710
couple years ago where just remove all

00:54:03,330 --> 00:54:06,869
type hints from your file it makes it

00:54:04,710 --> 00:54:10,230
look simpler and it does it makes what

00:54:06,869 --> 00:54:11,760
you're looking at simpler it makes that

00:54:10,230 --> 00:54:13,980
complexity goes somewhere else in the

00:54:11,760 --> 00:54:15,869
application and so that's I think the

00:54:13,980 --> 00:54:18,540
biggest thing is you can never eliminate

00:54:15,869 --> 00:54:22,590
complexity you can only move it around

00:54:18,540 --> 00:54:24,240
or edit and so when you're dealing with

00:54:22,590 --> 00:54:27,780
change when you're dealing with systems

00:54:24,240 --> 00:54:29,940
and applications and software be

00:54:27,780 --> 00:54:32,369
cognizant of where the complexity you're

00:54:29,940 --> 00:54:35,310
introducing goes and what trade-offs

00:54:32,369 --> 00:54:38,070
that you're that you're exercising with

00:54:35,310 --> 00:54:41,700
respect to that complexity had a really

00:54:38,070 --> 00:54:43,920
good conversation last night with a

00:54:41,700 --> 00:54:46,080
couple people and there was a discussion

00:54:43,920 --> 00:54:48,510
about the difference in writing software

00:54:46,080 --> 00:54:51,210
for a library that has three million

00:54:48,510 --> 00:54:54,630
active users and writing a software for

00:54:51,210 --> 00:54:55,560
a team of 12 to maintain fundamentally

00:54:54,630 --> 00:54:59,100
it's the same thing we're writing

00:54:55,560 --> 00:55:01,470
software for both but the complexity

00:54:59,100 --> 00:55:03,869
trade-offs are vastly different you can

00:55:01,470 --> 00:55:05,640
deal with and get away with a lot more

00:55:03,869 --> 00:55:07,380
when you're dealing with a small team so

00:55:05,640 --> 00:55:09,600
you have to understand your audience in

00:55:07,380 --> 00:55:13,830
the context with it with it within your

00:55:09,600 --> 00:55:16,200
running if the final takeaway is it's

00:55:13,830 --> 00:55:20,310
far far far too easy we got bit by this

00:55:16,200 --> 00:55:22,260
we got the battle scars for it to create

00:55:20,310 --> 00:55:24,390
a system that is so complicated that you

00:55:22,260 --> 00:55:26,430
cannot understand it and if you can't

00:55:24,390 --> 00:55:28,859
understand it how can you possibly hope

00:55:26,430 --> 00:55:30,660
to run it in production to debug it to

00:55:28,859 --> 00:55:35,800
maintain it and to keep it alive and

00:55:30,660 --> 00:55:44,390
actually work with it quickly thank you

00:55:35,800 --> 00:55:47,130
[Applause]

00:55:44,390 --> 00:55:54,120
we have a few minutes for questions if

00:55:47,130 --> 00:55:57,030
there's any questions in the room did

00:55:54,120 --> 00:55:58,950
you ever end up refactoring to take on

00:55:57,030 --> 00:56:02,520
some of those technologies you kind of

00:55:58,950 --> 00:56:04,530
missed going out of the door so did we

00:56:02,520 --> 00:56:08,100
wind up refactoring to check things so

00:56:04,530 --> 00:56:09,480
things like Kafka things like yeah not

00:56:08,100 --> 00:56:11,730
to my knowledge not by the time I had

00:56:09,480 --> 00:56:13,620
left the team this is not the team I am

00:56:11,730 --> 00:56:17,240
with right now this is a prior a prior

00:56:13,620 --> 00:56:19,830
organization by the time I had left no

00:56:17,240 --> 00:56:21,270
I've kept in touch with a number of them

00:56:19,830 --> 00:56:24,270
I do not believe that they have

00:56:21,270 --> 00:56:26,160
refactored in those directions but you

00:56:24,270 --> 00:56:28,860
know when you have a system that's up

00:56:26,160 --> 00:56:31,080
and running the trade-offs of moving off

00:56:28,860 --> 00:56:33,810
of on to those other systems if the

00:56:31,080 --> 00:56:35,460
infrastructure was built so the benefit

00:56:33,810 --> 00:56:38,100
to moving on to kubernetes would have

00:56:35,460 --> 00:56:41,250
been relatively low given that all the

00:56:38,100 --> 00:56:47,450
tooling was built and was automated but

00:56:41,250 --> 00:56:52,470
yeah some of it we did but most of it no

00:56:47,450 --> 00:56:54,950
other back there microphones coming

00:56:52,470 --> 00:56:54,950
it'll be there

00:57:04,349 --> 00:57:12,819
thank you so about the problem you've

00:57:09,940 --> 00:57:15,670
mentioned when you had to go all over in

00:57:12,819 --> 00:57:17,890
services in support simultaneously both

00:57:15,670 --> 00:57:19,569
versions of the message what do you

00:57:17,890 --> 00:57:22,240
think of an idea I think they call it

00:57:19,569 --> 00:57:26,410
message translator or something from the

00:57:22,240 --> 00:57:31,029
enterprise integration patterns so you

00:57:26,410 --> 00:57:33,880
just put version support for your

00:57:31,029 --> 00:57:36,460
messages from the very start so if your

00:57:33,880 --> 00:57:39,640
current message if your current service

00:57:36,460 --> 00:57:41,680
does not support version each as

00:57:39,640 --> 00:57:44,380
declines the request and doesn't process

00:57:41,680 --> 00:57:47,019
it and so when you refactor your

00:57:44,380 --> 00:57:50,769
messages yep you just put an extra

00:57:47,019 --> 00:57:53,980
service that duplicates the messages and

00:57:50,769 --> 00:57:56,140
translates the new version into an old

00:57:53,980 --> 00:57:58,900
one and then you refactor all of your

00:57:56,140 --> 00:58:01,119
services one by one and just remove the

00:57:58,900 --> 00:58:04,630
message translator after that does that

00:58:01,119 --> 00:58:08,140
work yeah so for the message layer for

00:58:04,630 --> 00:58:11,500
the event was for rabbitmq we actually

00:58:08,140 --> 00:58:13,630
had a different we we had a solution to

00:58:11,500 --> 00:58:15,940
that problem where every service was

00:58:13,630 --> 00:58:18,490
required to handle all of the versions

00:58:15,940 --> 00:58:21,400
of every event because we had that

00:58:18,490 --> 00:58:24,789
message store so instead of trying to

00:58:21,400 --> 00:58:26,680
upgrade in real time on the fly and only

00:58:24,789 --> 00:58:28,630
give the current version of the event we

00:58:26,680 --> 00:58:30,880
actually had mandated support for every

00:58:28,630 --> 00:58:32,289
event and that was tested so we didn't

00:58:30,880 --> 00:58:34,450
run into that problem on the service

00:58:32,289 --> 00:58:36,250
layer where we ran into that problem or

00:58:34,450 --> 00:58:38,859
sorry I'm the event layer we ran into

00:58:36,250 --> 00:58:41,019
the problem was on the HTTP layer with

00:58:38,859 --> 00:58:43,809
HTTP service calls and could we have

00:58:41,019 --> 00:58:47,079
implemented another service to backwards

00:58:43,809 --> 00:58:47,619
translate absolutely and that actually

00:58:47,079 --> 00:58:49,740
probably would have been a

00:58:47,619 --> 00:58:53,769
halfway-decent solution to that problem

00:58:49,740 --> 00:58:55,450
but we focused more on the nimble side

00:58:53,769 --> 00:58:58,450
of trying to keep the application up to

00:58:55,450 --> 00:59:01,029
date and the type of changes that I'm

00:58:58,450 --> 00:59:03,160
talking about most of them are more

00:59:01,029 --> 00:59:07,180
business logic changes than just

00:59:03,160 --> 00:59:10,329
structural changes so and having a

00:59:07,180 --> 00:59:12,519
single proxy turn a and the B would work

00:59:10,329 --> 00:59:13,750
for structural changes but I don't know

00:59:12,519 --> 00:59:16,059
that it would necessarily off-the-cuff

00:59:13,750 --> 00:59:17,010
work for the level of business changes

00:59:16,059 --> 00:59:19,890
that we had

00:59:17,010 --> 00:59:22,829
that we would need I think we have

00:59:19,890 --> 00:59:24,450
another like that so you've given us a

00:59:22,829 --> 00:59:26,070
river health warning on mic receivers

00:59:24,450 --> 00:59:29,190
that's much appreciated

00:59:26,070 --> 00:59:32,040
what would you take be then on using

00:59:29,190 --> 00:59:34,380
that architectural pattern of a message

00:59:32,040 --> 00:59:37,740
bus to glue existing services together

00:59:34,380 --> 00:59:40,770
and to also be the glue for new services

00:59:37,740 --> 00:59:43,109
sa CRM to talk to instead of direct way

00:59:40,770 --> 00:59:44,520
to all these services that CRM has been

00:59:43,109 --> 00:59:47,040
talking to go through the message bus

00:59:44,520 --> 00:59:50,400
instead and have you got any steer and

00:59:47,040 --> 00:59:53,339
tips on that biggest tip is what is your

00:59:50,400 --> 00:59:56,849
single source of truth for data is your

00:59:53,339 --> 00:59:59,849
single source of truth a API is it that

00:59:56,849 --> 01:00:02,430
message bus itself and or is it

00:59:59,849 --> 01:00:04,290
something else have a single source of

01:00:02,430 --> 01:00:06,119
truth for every piece of data it doesn't

01:00:04,290 --> 01:00:08,490
have to be the same one but every piece

01:00:06,119 --> 01:00:11,160
of data should have exactly one source

01:00:08,490 --> 01:00:14,160
of truth and make sure that that source

01:00:11,160 --> 01:00:16,710
of truth is row busted and anytime you

01:00:14,160 --> 01:00:18,270
need truth go to that source of truth so

01:00:16,710 --> 01:00:20,820
if you're using a message bus to

01:00:18,270 --> 01:00:22,980
communicate changes that's perfectly

01:00:20,820 --> 01:00:24,329
fine like I'm not saying rabbitmq should

01:00:22,980 --> 01:00:26,819
not exist or anything like that for an

01:00:24,329 --> 01:00:29,160
application layer absolutely should but

01:00:26,819 --> 01:00:31,500
how do you handle a miss message do you

01:00:29,160 --> 01:00:34,140
have a met a mechanism for going back

01:00:31,500 --> 01:00:36,859
and reconstructing from a source of

01:00:34,140 --> 01:00:39,060
truth that's tested and that's robust

01:00:36,859 --> 01:00:42,030
that's I think the biggest piece I would

01:00:39,060 --> 01:00:44,220
look at again it's handling the failure

01:00:42,030 --> 01:00:46,650
case and so if your source of truth is

01:00:44,220 --> 01:00:51,270
that bus then handling that failure case

01:00:46,650 --> 01:00:53,160
could be just replay from the bus and so

01:00:51,270 --> 01:00:56,150
you rely on the bus to deliver your

01:00:53,160 --> 01:00:56,150

YouTube URL: https://www.youtube.com/watch?v=5QIpzNPVDaY


