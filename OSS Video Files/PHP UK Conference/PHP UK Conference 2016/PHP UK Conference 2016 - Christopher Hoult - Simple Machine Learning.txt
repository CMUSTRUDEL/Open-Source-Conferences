Title: PHP UK Conference 2016 - Christopher Hoult - Simple Machine Learning
Publication date: 2016-03-16
Playlist: PHP UK Conference 2016
Description: 
	Want to separate the signal from the noise, but have too much input to deal with? Fed up with reading everything yourself? Mechanical Turk got you down? Then perhaps you need to apply some machine learning! In this talk, Christopher will cover some basic approaches to machine-learned classification as well as demonstrate a real-life application of it in PHP.
Captions: 
	00:00:04,990 --> 00:00:11,120
so my job here is to tell you about

00:00:08,660 --> 00:00:12,590
simple machine learning quick caveat I'm

00:00:11,120 --> 00:00:17,360
not an expert and that's why this should

00:00:12,590 --> 00:00:20,270
hopefully be a bit more interesting so

00:00:17,360 --> 00:00:23,689
who am i I'm a senior software engineer

00:00:20,270 --> 00:00:25,160
at DataSift I look after the web

00:00:23,689 --> 00:00:27,770
application for controlling everything

00:00:25,160 --> 00:00:30,259
the public API and a bunch of internal

00:00:27,770 --> 00:00:33,079
services prior to that I was at Time Out

00:00:30,259 --> 00:00:35,480
magazine and working on crm systems for

00:00:33,079 --> 00:00:38,900
secured loans companies before that

00:00:35,480 --> 00:00:42,020
industry fell through I'm also the

00:00:38,900 --> 00:00:45,680
founder and co-organizer of PHP

00:00:42,020 --> 00:00:49,160
Berkshire okay another co-organizer

00:00:45,680 --> 00:00:51,320
there and yeah if you want to head out

00:00:49,160 --> 00:00:53,930
to reading then we're more than happy to

00:00:51,320 --> 00:00:57,530
to welcome you all the way all those 20

00:00:53,930 --> 00:01:01,820
long minutes away from Paddington I also

00:00:57,530 --> 00:01:04,009
act for the record it's not group

00:01:01,820 --> 00:01:09,139
therapy it's not individual therapy it's

00:01:04,009 --> 00:01:17,420
me being a dog I do have the hair for it

00:01:09,139 --> 00:01:20,350
so so yes so the challenge I call this

00:01:17,420 --> 00:01:23,840
Hawk simple machine learning because

00:01:20,350 --> 00:01:27,289
machine learning itself at its basic is

00:01:23,840 --> 00:01:29,060
actually quite straightforward there's a

00:01:27,289 --> 00:01:30,859
bit of math in there when you get to

00:01:29,060 --> 00:01:33,619
some of the classification stuff there's

00:01:30,859 --> 00:01:35,209
a bit of probability theory and stuff

00:01:33,619 --> 00:01:36,950
that we can just say well here's an

00:01:35,209 --> 00:01:40,310
equation let's chuck stuff in and forget

00:01:36,950 --> 00:01:44,229
about it but in essence it's a very

00:01:40,310 --> 00:01:46,609
straightforward process but in order to

00:01:44,229 --> 00:01:49,099
kind of apply this challenge I've got to

00:01:46,609 --> 00:01:50,719
have a case study and the one that kind

00:01:49,099 --> 00:01:55,849
of kicked me off in thinking about this

00:01:50,719 --> 00:01:58,939
now and in PHP and so on was a blog post

00:01:55,849 --> 00:02:02,299
by Larry Gura field in August when he

00:01:58,939 --> 00:02:06,829
looked at the the number of new speakers

00:02:02,299 --> 00:02:10,880
at PHP conferences throughout well since

00:02:06,829 --> 00:02:13,489
2011 I believe he basically grabbed a

00:02:10,880 --> 00:02:16,970
bunch of data which I'll show on a sec

00:02:13,489 --> 00:02:18,290
the joined in API he used he grabbed all

00:02:16,970 --> 00:02:21,400
the events all the top

00:02:18,290 --> 00:02:24,530
and he used that given that it's vastly

00:02:21,400 --> 00:02:26,810
PHP biased because it's written in PHP

00:02:24,530 --> 00:02:30,739
it's promoted by people from PHP and we

00:02:26,810 --> 00:02:33,310
all love our own he did an analysis of

00:02:30,739 --> 00:02:36,379
first time PHP speakers across all these

00:02:33,310 --> 00:02:40,129
all these conferences and he came up

00:02:36,379 --> 00:02:43,819
with a number of about fifty fifty one

00:02:40,129 --> 00:02:46,280
percent new speakers I've run that code

00:02:43,819 --> 00:02:48,650
again in the past well past few hours

00:02:46,280 --> 00:02:53,090
and that's actually running now about

00:02:48,650 --> 00:02:55,400
forty-eight forty-nine percent so still

00:02:53,090 --> 00:02:59,269
doing pretty well but one of the things

00:02:55,400 --> 00:03:02,120
that joined in is it's not just PHP

00:02:59,269 --> 00:03:04,700
focused it's built in PHP as I said it's

00:03:02,120 --> 00:03:06,560
promoted at PHP events but there's some

00:03:04,700 --> 00:03:09,049
rather large conferences in there

00:03:06,560 --> 00:03:13,280
there's that don't say anything about

00:03:09,049 --> 00:03:16,579
PHP there are there's javascript stuff

00:03:13,280 --> 00:03:20,120
in there c plus plus there are away days

00:03:16,579 --> 00:03:22,909
for people like in victor in vika and so

00:03:20,120 --> 00:03:25,879
on so i'm a lot of that data is actually

00:03:22,909 --> 00:03:30,590
not quite focused on PHP in the way that

00:03:25,879 --> 00:03:32,690
that core assumptions made so there's a

00:03:30,590 --> 00:03:35,900
tagging system in enjoying it and you

00:03:32,690 --> 00:03:38,269
can say this talk is about or this event

00:03:35,900 --> 00:03:41,799
is about PHP this one's about JavaScript

00:03:38,269 --> 00:03:46,549
and so on but we have a bit of a problem

00:03:41,799 --> 00:03:48,979
we have only out of a thousand 84 event

00:03:46,549 --> 00:03:51,799
we only have four hundred and nine of

00:03:48,979 --> 00:03:55,340
those are tagged the other the majority

00:03:51,799 --> 00:03:57,829
are untagged so we can't just go and run

00:03:55,340 --> 00:03:59,629
this analysis by looking at the tags and

00:03:57,829 --> 00:04:03,340
seeing everything that's tagged with PHP

00:03:59,629 --> 00:04:05,810
and improving our results in that way so

00:04:03,340 --> 00:04:08,959
so we got to look at how do we make up

00:04:05,810 --> 00:04:11,030
for the 675 on tags and then going

00:04:08,959 --> 00:04:13,459
forward unless we start enforcing tags

00:04:11,030 --> 00:04:16,579
on joined in how do new events start

00:04:13,459 --> 00:04:20,239
getting tags and how do we kind of keep

00:04:16,579 --> 00:04:24,470
classifying things as we go through so

00:04:20,239 --> 00:04:29,210
there are a few solutions number one the

00:04:24,470 --> 00:04:30,950
most boring one is to go through 1,100

00:04:29,210 --> 00:04:33,620
talks by hand

00:04:30,950 --> 00:04:35,690
and tag them individually if you

00:04:33,620 --> 00:04:37,310
estimate one a minute you can do it a

00:04:35,690 --> 00:04:40,790
bit faster but if you estimate a one a

00:04:37,310 --> 00:04:42,920
minute that's 11 hours of work you might

00:04:40,790 --> 00:04:46,300
find typos or have typos and your tags

00:04:42,920 --> 00:04:50,180
you might not apply your criteria

00:04:46,300 --> 00:04:53,300
consistently and as such you're into a

00:04:50,180 --> 00:04:55,160
bit of error there then once you finish

00:04:53,300 --> 00:04:58,460
classifying everything that's great but

00:04:55,160 --> 00:05:00,230
20 new conferences or 15 new user groups

00:04:58,460 --> 00:05:03,890
have sprung up in the time it's taken

00:05:00,230 --> 00:05:06,650
you to tag those events so that solution

00:05:03,890 --> 00:05:07,970
isn't exactly one that goes forward and

00:05:06,650 --> 00:05:09,740
it's not necessarily one that really

00:05:07,970 --> 00:05:11,060
scales no one really wants to be there

00:05:09,740 --> 00:05:12,920
going right so I've got to spend an hour

00:05:11,060 --> 00:05:15,980
tonight tagging over linkedin or the

00:05:12,920 --> 00:05:19,460
joined in event it's not fun it's not

00:05:15,980 --> 00:05:23,390
cool it's not an appropriate use of our

00:05:19,460 --> 00:05:24,890
time perhaps we have mechanical turk I

00:05:23,390 --> 00:05:27,920
hands up if you've heard of Mechanical

00:05:24,890 --> 00:05:30,460
Turk that's about half of you so

00:05:27,920 --> 00:05:33,020
mechanical turk is a system kind of a

00:05:30,460 --> 00:05:35,990
system that's put forward by Amazon

00:05:33,020 --> 00:05:39,260
where you put in kind of micro jobs you

00:05:35,990 --> 00:05:41,090
say here's some background here's the

00:05:39,260 --> 00:05:44,600
output that I want from that it

00:05:41,090 --> 00:05:46,310
distributes it to real humans and then

00:05:44,600 --> 00:05:49,130
they do your work for you in little bit

00:05:46,310 --> 00:05:51,820
so it's quite scalable it's done by

00:05:49,130 --> 00:05:55,610
humans and it's applying that kind of

00:05:51,820 --> 00:05:58,730
the human eye that we'd like but again

00:05:55,610 --> 00:06:01,010
you have to pay for such a service it

00:05:58,730 --> 00:06:04,820
can get expensive if you need to get a

00:06:01,010 --> 00:06:06,500
lot of a lot of people contributing and

00:06:04,820 --> 00:06:10,520
you need to get a lot of stuff checked

00:06:06,500 --> 00:06:14,240
as you go through it is fun it is cool

00:06:10,520 --> 00:06:16,550
but again expensive and you're relying

00:06:14,240 --> 00:06:17,960
on others and their interpretation it

00:06:16,550 --> 00:06:20,450
could just land in the lap of someone

00:06:17,960 --> 00:06:21,880
who has no idea what PHP is and couldn't

00:06:20,450 --> 00:06:26,270
tell you the symphony talk for example

00:06:21,880 --> 00:06:28,040
is about PHP of its core so our third

00:06:26,270 --> 00:06:33,430
solution and the one that you're here

00:06:28,040 --> 00:06:36,710
for is machine learning yay it's fast

00:06:33,430 --> 00:06:39,320
well it's fast when you apply it as

00:06:36,710 --> 00:06:40,850
we'll see in a bit it's consistent so

00:06:39,320 --> 00:06:42,560
once you've got your model it can be

00:06:40,850 --> 00:06:44,530
applied time and time again and you'll

00:06:42,560 --> 00:06:49,189
get the same results

00:06:44,530 --> 00:06:52,250
automated and it is fun and cool there

00:06:49,189 --> 00:06:56,800
we go that's our selection criteria for

00:06:52,250 --> 00:06:59,569
our solution so before we pick up on

00:06:56,800 --> 00:07:02,090
machine learning and what that is and

00:06:59,569 --> 00:07:04,099
how that works let's have a think about

00:07:02,090 --> 00:07:05,810
how we as humans learn because

00:07:04,099 --> 00:07:09,349
ultimately what we're trying to do is

00:07:05,810 --> 00:07:12,530
come up with a model that models the way

00:07:09,349 --> 00:07:16,310
we as humans look at the the input data

00:07:12,530 --> 00:07:18,770
and and how we can understand and

00:07:16,310 --> 00:07:21,909
interpret it interpret that and apply

00:07:18,770 --> 00:07:26,659
our learnings as such so how do we as

00:07:21,909 --> 00:07:28,819
humans learn pattern recognition is a

00:07:26,659 --> 00:07:32,840
huge part of what goes on up here and

00:07:28,819 --> 00:07:36,830
not just this one ball yours we spot

00:07:32,840 --> 00:07:39,620
patterns we see cause and effect we

00:07:36,830 --> 00:07:42,259
associate well this happened then this

00:07:39,620 --> 00:07:45,199
happened so if we see that enough times

00:07:42,259 --> 00:07:47,810
we make that link that that cause cause

00:07:45,199 --> 00:07:52,099
that effect and correlation in that in

00:07:47,810 --> 00:07:54,050
our brain anyway is causation and the

00:07:52,099 --> 00:07:57,139
mere fact that we know that that's not

00:07:54,050 --> 00:07:59,599
the case and that's being propagated

00:07:57,139 --> 00:08:01,699
correlation is not causation is quite

00:07:59,599 --> 00:08:05,659
you know it's the phrase that's because

00:08:01,699 --> 00:08:07,069
we have to subvert that innate way that

00:08:05,659 --> 00:08:09,860
we think and the way that we make those

00:08:07,069 --> 00:08:11,900
connections for example pigeons you can

00:08:09,860 --> 00:08:14,779
make them superstitious if you feed them

00:08:11,900 --> 00:08:17,629
every time they turn 360 when they're

00:08:14,779 --> 00:08:19,520
hungry they'll turn 360 just to get the

00:08:17,629 --> 00:08:21,259
food if you do that enough times and

00:08:19,520 --> 00:08:24,949
that at the basis is the way that we

00:08:21,259 --> 00:08:28,460
learn we also look at past experience we

00:08:24,949 --> 00:08:30,440
build a mental model of what we're how

00:08:28,460 --> 00:08:32,360
we experience the world and the people

00:08:30,440 --> 00:08:35,120
and the places and the things we

00:08:32,360 --> 00:08:36,740
encounter and we create stereotypes and

00:08:35,120 --> 00:08:40,219
again we were talking about our hearing

00:08:36,740 --> 00:08:42,289
about biases in the in the keynotes and

00:08:40,219 --> 00:08:44,899
some of that is learned from our

00:08:42,289 --> 00:08:46,519
experience and we might have some bias

00:08:44,899 --> 00:08:49,310
from our own experience and some

00:08:46,519 --> 00:08:51,019
stereotypes from our own experience but

00:08:49,310 --> 00:08:53,000
that's the way that we deal with the

00:08:51,019 --> 00:08:57,319
world in a general sense in our kind of

00:08:53,000 --> 00:08:58,140
type one immediate sense and one of the

00:08:57,319 --> 00:09:01,080
examples

00:08:58,140 --> 00:09:02,850
of this pattern recognition is something

00:09:01,080 --> 00:09:09,540
called baader-meinhof not the terrorist

00:09:02,850 --> 00:09:12,810
gang but another kind of another way of

00:09:09,540 --> 00:09:15,780
looking at things it's basically where

00:09:12,810 --> 00:09:18,030
you learn a new word or you encounter a

00:09:15,780 --> 00:09:21,180
new brands or you shop at a new shop

00:09:18,030 --> 00:09:22,970
you've never heard of it before it's

00:09:21,180 --> 00:09:26,360
never been important to you before

00:09:22,970 --> 00:09:30,210
suddenly it's become important to you

00:09:26,360 --> 00:09:32,520
and you're you're very aware of it and

00:09:30,210 --> 00:09:34,320
you step out of the shop with your bag

00:09:32,520 --> 00:09:36,690
that's got the name of the shop on it

00:09:34,320 --> 00:09:38,010
and you go I'm really cool it's the

00:09:36,690 --> 00:09:40,890
first time I've seen this new shop and

00:09:38,010 --> 00:09:43,200
you look around everyone's got bags with

00:09:40,890 --> 00:09:45,960
the name of the shop on you learn a new

00:09:43,200 --> 00:09:48,510
word and suddenly you see that word

00:09:45,960 --> 00:09:50,550
everywhere and you wonder why haven't I

00:09:48,510 --> 00:09:52,830
fit encountered that before and that's

00:09:50,550 --> 00:09:56,430
because we attach importance to the new

00:09:52,830 --> 00:09:58,950
things that we've encountered the other

00:09:56,430 --> 00:10:01,560
way or another way that we learn is

00:09:58,950 --> 00:10:04,500
through shared experience and teaching

00:10:01,560 --> 00:10:05,640
so being instructed about things being

00:10:04,500 --> 00:10:08,730
told that this is the way that the world

00:10:05,640 --> 00:10:10,770
works being told this causes that and

00:10:08,730 --> 00:10:14,820
getting that information from an

00:10:10,770 --> 00:10:17,700
authoritative source it gives you that

00:10:14,820 --> 00:10:19,890
gateway into the ideas and because it

00:10:17,700 --> 00:10:22,350
comes from a teacher you know that well

00:10:19,890 --> 00:10:23,790
you hope assuming you're in a good

00:10:22,350 --> 00:10:25,470
educational system that there's

00:10:23,790 --> 00:10:27,540
Authority there and that what they're

00:10:25,470 --> 00:10:29,510
telling you is correct and from all that

00:10:27,540 --> 00:10:31,920
stuff you've learned and picked up on

00:10:29,510 --> 00:10:34,610
you can apply that model that you've

00:10:31,920 --> 00:10:37,320
been taught to the rest of the world

00:10:34,610 --> 00:10:40,530
because you've got the authority you

00:10:37,320 --> 00:10:42,540
then have accuracy we hope and also

00:10:40,530 --> 00:10:44,100
because you're doing it in conjunction

00:10:42,540 --> 00:10:46,320
with a teacher you have those

00:10:44,100 --> 00:10:49,010
opportunities to fail and to be

00:10:46,320 --> 00:10:51,540
corrected and that kind of feedback loop

00:10:49,010 --> 00:10:56,460
helps us to learn and to refine our

00:10:51,540 --> 00:11:00,900
model as we go through so machine

00:10:56,460 --> 00:11:03,360
learning it's not as scary as it sounds

00:11:00,900 --> 00:11:07,470
some of you might not think it's scary

00:11:03,360 --> 00:11:11,150
but it's straightforward it's a chunk of

00:11:07,470 --> 00:11:13,580
break down a problem figure out how

00:11:11,150 --> 00:11:15,530
to build a model of it and how to apply

00:11:13,580 --> 00:11:17,420
that model to get the output that you're

00:11:15,530 --> 00:11:18,830
after the things we're going to talk

00:11:17,420 --> 00:11:20,420
about aren't going to be as advanced as

00:11:18,830 --> 00:11:23,750
neural nets mainly because I don't know

00:11:20,420 --> 00:11:26,480
that stuff I'm going to pick through a

00:11:23,750 --> 00:11:29,630
one example of how we build classifiers

00:11:26,480 --> 00:11:31,790
and how we apply stuff but one of the

00:11:29,630 --> 00:11:33,740
things that I realized when I started

00:11:31,790 --> 00:11:35,920
tackling this is actually I've been

00:11:33,740 --> 00:11:39,620
doing this on and off in various guises

00:11:35,920 --> 00:11:41,990
for years I've been looking at how often

00:11:39,620 --> 00:11:45,230
things pop up I look at the correlation

00:11:41,990 --> 00:11:47,510
between some some items and then I can

00:11:45,230 --> 00:11:49,850
make a prediction of the back of it so I

00:11:47,510 --> 00:11:51,650
was doing this it's a big buzzword now

00:11:49,850 --> 00:11:54,170
but I was doing this without calling it

00:11:51,650 --> 00:11:56,000
as such 12 years ago trying to do image

00:11:54,170 --> 00:11:58,600
recognition at the University in that

00:11:56,000 --> 00:12:01,700
project that I don't understand anymore

00:11:58,600 --> 00:12:04,790
but that kind of the fact that it's been

00:12:01,700 --> 00:12:08,110
around well pretty much as an idea since

00:12:04,790 --> 00:12:10,910
the 60s and 70s it's only just becoming

00:12:08,110 --> 00:12:14,300
scalable and practical as an approach

00:12:10,910 --> 00:12:15,770
that we can pick up there are a couple

00:12:14,300 --> 00:12:18,710
of different types of machine learning

00:12:15,770 --> 00:12:21,590
there is unsupervised learning and

00:12:18,710 --> 00:12:23,900
supervised learning so unsupervised is

00:12:21,590 --> 00:12:26,030
where you just say here's all the input

00:12:23,900 --> 00:12:29,210
here all your documents all your events

00:12:26,030 --> 00:12:32,240
from joined in go off and figure out

00:12:29,210 --> 00:12:35,990
something just leave it to its own

00:12:32,240 --> 00:12:39,650
devices and the way that kind of works

00:12:35,990 --> 00:12:41,840
is it you might think take all the other

00:12:39,650 --> 00:12:43,580
words that come in look at the similar

00:12:41,840 --> 00:12:46,670
words that come up and group all the

00:12:43,580 --> 00:12:48,950
events under similar words then pass it

00:12:46,670 --> 00:12:51,710
off to a human to label what those

00:12:48,950 --> 00:12:53,480
clusters are find similarities between

00:12:51,710 --> 00:12:56,540
documents and grunt lump them together

00:12:53,480 --> 00:12:58,490
and then present those or find inherent

00:12:56,540 --> 00:13:00,730
connections between those clusters and

00:12:58,490 --> 00:13:04,160
present that is kind of a network

00:13:00,730 --> 00:13:06,440
diagram of all the events there it does

00:13:04,160 --> 00:13:08,960
help uncover hidden structure one of the

00:13:06,440 --> 00:13:11,510
cool ones that has been described in the

00:13:08,960 --> 00:13:13,970
past few years and is now being done at

00:13:11,510 --> 00:13:15,980
scale is called word tyvek and

00:13:13,970 --> 00:13:18,740
effectively what that ends up doing is

00:13:15,980 --> 00:13:20,870
you give it a load of words a load of

00:13:18,740 --> 00:13:23,090
English words if you're like loads of

00:13:20,870 --> 00:13:23,670
text it starts figuring out the

00:13:23,090 --> 00:13:27,090
relationship

00:13:23,670 --> 00:13:30,660
between individual words and it's above

00:13:27,090 --> 00:13:34,320
me and over my head but what you get at

00:13:30,660 --> 00:13:38,960
the end of it is this kind of spatial

00:13:34,320 --> 00:13:42,050
model of words in our language and it

00:13:38,960 --> 00:13:46,850
inherently without actually knowing that

00:13:42,050 --> 00:13:50,610
male and female exists it can start to

00:13:46,850 --> 00:13:54,630
cluster the words around there and start

00:13:50,610 --> 00:13:57,840
to I say gender words so it can connect

00:13:54,630 --> 00:14:01,620
female to Queen and male to King and

00:13:57,840 --> 00:14:04,680
royal to king and queen and then you can

00:14:01,620 --> 00:14:07,770
do some fun stuff like give me all the

00:14:04,680 --> 00:14:10,070
words that are royal minus mail and it

00:14:07,770 --> 00:14:12,510
would give you queen of the back of that

00:14:10,070 --> 00:14:14,070
like I said over my head so that's my

00:14:12,510 --> 00:14:16,230
simplistic understanding of it but

00:14:14,070 --> 00:14:19,730
that's a good example when you come

00:14:16,230 --> 00:14:23,370
across a of unsupervised learning

00:14:19,730 --> 00:14:25,890
supervised learning is where you

00:14:23,370 --> 00:14:28,110
actually do that teaching you say here

00:14:25,890 --> 00:14:31,140
all the documents but here's a set of

00:14:28,110 --> 00:14:33,540
documents I've done for you here's a set

00:14:31,140 --> 00:14:35,670
that I've actually pre labeled in our

00:14:33,540 --> 00:14:37,610
joint in example we've already got some

00:14:35,670 --> 00:14:40,590
events that are tagged as I said before

00:14:37,610 --> 00:14:43,110
so we can think of that as being our

00:14:40,590 --> 00:14:47,100
source of authority and good set of

00:14:43,110 --> 00:14:51,360
labeling we build our model from those

00:14:47,100 --> 00:14:55,620
known accurate knowns to borrow a phrase

00:14:51,360 --> 00:14:58,830
from Donald Rumsfeld and we take that

00:14:55,620 --> 00:15:01,350
model that we build from it we refer to

00:14:58,830 --> 00:15:03,330
that and we take all of our assumptions

00:15:01,350 --> 00:15:05,610
from that and we apply that to new

00:15:03,330 --> 00:15:07,680
documents that come in new things that

00:15:05,610 --> 00:15:09,860
we come in and take that stereotype that

00:15:07,680 --> 00:15:13,380
we built and apply it to the new stuff

00:15:09,860 --> 00:15:15,390
and it's important to know that it's the

00:15:13,380 --> 00:15:17,640
fact that we've attached metadata in

00:15:15,390 --> 00:15:20,160
those labels in those tags to our

00:15:17,640 --> 00:15:22,560
documents that's a supervised part you

00:15:20,160 --> 00:15:24,930
can have interactive and non-interactive

00:15:22,560 --> 00:15:26,880
supervised learning and the important

00:15:24,930 --> 00:15:28,320
thing is that you've told about some of

00:15:26,880 --> 00:15:30,620
the relationships already and the kind

00:15:28,320 --> 00:15:33,240
of relationships you want to bring out

00:15:30,620 --> 00:15:36,090
there are a few types of output that we

00:15:33,240 --> 00:15:37,320
can look at from our machine learning so

00:15:36,090 --> 00:15:40,570
we have regression

00:15:37,320 --> 00:15:43,750
which is basically line of best fit so

00:15:40,570 --> 00:15:47,710
you you plug a load of data in you say

00:15:43,750 --> 00:15:50,290
here are my independent variables or are

00:15:47,710 --> 00:15:52,960
they are they dependent and figure out

00:15:50,290 --> 00:15:56,560
exactly what the relationship is without

00:15:52,960 --> 00:15:58,570
understanding that upfront so if you

00:15:56,560 --> 00:16:02,440
know that the temperature of a room goes

00:15:58,570 --> 00:16:04,120
up when a lot of people are in and you

00:16:02,440 --> 00:16:07,720
have a number of people and you have the

00:16:04,120 --> 00:16:09,400
data that you could correlate the number

00:16:07,720 --> 00:16:11,020
of people in the rumors that the ambient

00:16:09,400 --> 00:16:13,120
temperature in the room then you could

00:16:11,020 --> 00:16:15,850
build a model via regression linear

00:16:13,120 --> 00:16:17,230
regression to actually predict the the

00:16:15,850 --> 00:16:20,200
temperature of the room given the number

00:16:17,230 --> 00:16:21,880
of people come in without actually doing

00:16:20,200 --> 00:16:24,130
that working out for you it's a very

00:16:21,880 --> 00:16:25,900
simple example but if you imagine

00:16:24,130 --> 00:16:28,240
extending that to a lot of different

00:16:25,900 --> 00:16:30,550
variables where you'd be there all day

00:16:28,240 --> 00:16:32,790
by hand plugging all this stuff in and

00:16:30,550 --> 00:16:35,380
looking for those different pieces

00:16:32,790 --> 00:16:37,500
machine learning that does regression is

00:16:35,380 --> 00:16:40,330
one way of figuring all that stuff out

00:16:37,500 --> 00:16:42,790
and it's used for those continuous

00:16:40,330 --> 00:16:44,590
values as i talked about temperature but

00:16:42,790 --> 00:16:46,540
about anything on a spectrum basically

00:16:44,590 --> 00:16:48,760
where it's not discreet this is a this

00:16:46,540 --> 00:16:53,110
this is of that and so on it here's a

00:16:48,760 --> 00:16:55,390
predicted output value then this

00:16:53,110 --> 00:16:58,390
classification which is what we were

00:16:55,390 --> 00:17:00,670
looking at tagging so you say these sets

00:16:58,390 --> 00:17:03,120
of documents belong to this class this

00:17:00,670 --> 00:17:06,160
set of documents belong to this class

00:17:03,120 --> 00:17:08,980
and as a result when I give a new

00:17:06,160 --> 00:17:11,620
document which classes are belong to you

00:17:08,980 --> 00:17:14,410
can think of it as which bit in the

00:17:11,620 --> 00:17:16,540
library do I put this book into what's

00:17:14,410 --> 00:17:20,080
the Dewey Decimal Classification number

00:17:16,540 --> 00:17:24,640
of that or what mammal is this based on

00:17:20,080 --> 00:17:26,410
the attributes of that and a mammal so

00:17:24,640 --> 00:17:28,209
it requires those known labels it

00:17:26,410 --> 00:17:31,180
requires that teaching upfront that

00:17:28,209 --> 00:17:32,770
supervised style to actually come up

00:17:31,180 --> 00:17:35,950
with this classification that that's

00:17:32,770 --> 00:17:39,100
certainly would that we're after

00:17:35,950 --> 00:17:41,590
so our approach given that we have a

00:17:39,100 --> 00:17:45,160
bunch of documents given that we have

00:17:41,590 --> 00:17:48,340
existing tags that we have the output

00:17:45,160 --> 00:17:50,670
that we want to get new tags and improve

00:17:48,340 --> 00:17:52,810
the ones that don't have tags at all

00:17:50,670 --> 00:17:55,960
we're going to go for a supervised

00:17:52,810 --> 00:17:57,790
classification we're going to use the

00:17:55,960 --> 00:18:00,550
joint in tags that are already there and

00:17:57,790 --> 00:18:03,160
then I'll do a demo we're actually using

00:18:00,550 --> 00:18:05,410
the interactive tagging where I'll

00:18:03,160 --> 00:18:06,700
classify stuff on stage and hit go and

00:18:05,410 --> 00:18:10,240
we'll wait and we'll see what the

00:18:06,700 --> 00:18:12,970
results are i'm going to show you what's

00:18:10,240 --> 00:18:16,660
called in very long words multivariate

00:18:12,970 --> 00:18:18,730
naive bayesian classification which is

00:18:16,660 --> 00:18:20,770
less scary than it sounds especially

00:18:18,730 --> 00:18:24,550
when you just take it as read that one

00:18:20,770 --> 00:18:26,050
of the calculations is good what that

00:18:24,550 --> 00:18:28,750
means is you've got lots of variables

00:18:26,050 --> 00:18:30,730
coming in and then naive part is the

00:18:28,750 --> 00:18:34,060
fact that we make some assumptions to

00:18:30,730 --> 00:18:36,850
reduce the problem someone said to me

00:18:34,060 --> 00:18:39,130
yesterday that classify classifier is a

00:18:36,850 --> 00:18:40,930
machine learning they're all solving the

00:18:39,130 --> 00:18:42,370
same problem some of them just do it

00:18:40,930 --> 00:18:44,670
more efficiently than others and it's

00:18:42,370 --> 00:18:48,100
all about reducing the problem space

00:18:44,670 --> 00:18:50,650
will also have a very quick look or an

00:18:48,100 --> 00:18:53,080
example of a Bernoulli multivariate new

00:18:50,650 --> 00:18:55,660
nave I can't even say at this time in

00:18:53,080 --> 00:18:59,140
the morning Bernoulli multivariate naive

00:18:55,660 --> 00:19:01,300
bayesian classifier kind of worked which

00:18:59,140 --> 00:19:04,420
is a similar kind of approach but it

00:19:01,300 --> 00:19:09,430
takes information about when something

00:19:04,420 --> 00:19:11,830
isn't there alongside when it is sort of

00:19:09,430 --> 00:19:13,870
steps in machine learning I vaguely

00:19:11,830 --> 00:19:15,070
touched on this before but they're

00:19:13,870 --> 00:19:17,550
really straightforward you've got a

00:19:15,070 --> 00:19:20,890
couple of phases you have the training

00:19:17,550 --> 00:19:23,290
side which is feature extraction and

00:19:20,890 --> 00:19:25,390
then you're modeling and then the

00:19:23,290 --> 00:19:27,100
prediction which is taking the same

00:19:25,390 --> 00:19:30,850
features outs which I'll talk about in a

00:19:27,100 --> 00:19:32,200
sec and applying that model so once

00:19:30,850 --> 00:19:34,120
you've got the model built which is the

00:19:32,200 --> 00:19:38,200
tough bit the actual application of it

00:19:34,120 --> 00:19:44,220
it's a pretty straightforward affair so

00:19:38,200 --> 00:19:46,360
training the supervised learning part

00:19:44,220 --> 00:19:48,400
we're we're actually plugging in those

00:19:46,360 --> 00:19:52,270
documents and plugging in those labels

00:19:48,400 --> 00:19:55,570
as we go through we start off with

00:19:52,270 --> 00:19:57,970
feature extraction now one of the things

00:19:55,570 --> 00:20:00,430
about machines about computers is

00:19:57,970 --> 00:20:02,110
there's no inherent understanding of any

00:20:00,430 --> 00:20:04,630
of the stuff we took at it they're just

00:20:02,110 --> 00:20:07,410
ASCII characters is the bytes underneath

00:20:04,630 --> 00:20:10,660
and so on we've got to find a way of

00:20:07,410 --> 00:20:16,260
describing a document in a in an

00:20:10,660 --> 00:20:18,910
objective sense rather than saying it

00:20:16,260 --> 00:20:21,460
rather than just kind of saying others

00:20:18,910 --> 00:20:23,410
about PHP we need to actually say well

00:20:21,460 --> 00:20:26,560
it's got the word PHP in it if you like

00:20:23,410 --> 00:20:29,620
so we boil a document an image or any

00:20:26,560 --> 00:20:32,950
set of data down into numbers and we do

00:20:29,620 --> 00:20:36,730
that in a consistent fashion so that we

00:20:32,950 --> 00:20:40,780
have this multi-dimensional concept of

00:20:36,730 --> 00:20:42,610
what a document is take it to the

00:20:40,780 --> 00:20:44,230
numbers and then the computer can work

00:20:42,610 --> 00:20:46,030
with those add those together multiply

00:20:44,230 --> 00:20:47,340
whatever we want to do with them but the

00:20:46,030 --> 00:20:50,530
important thing is that it's consistent

00:20:47,340 --> 00:20:51,820
and that once we've applied to that one

00:20:50,530 --> 00:20:55,210
of our documents we're going to take

00:20:51,820 --> 00:20:56,890
those same descriptions those same

00:20:55,210 --> 00:20:58,930
features that we're extracting and apply

00:20:56,890 --> 00:21:01,030
them across all our documents so that we

00:20:58,930 --> 00:21:04,840
put them into the same kind of space and

00:21:01,030 --> 00:21:06,090
we normalize them if you like there are

00:21:04,840 --> 00:21:08,440
a few well there are many approaches

00:21:06,090 --> 00:21:11,020
basically anything that you can apply to

00:21:08,440 --> 00:21:12,820
a set number of documents that turns

00:21:11,020 --> 00:21:16,420
into a number you can make it a feature

00:21:12,820 --> 00:21:18,130
extractor so term frequency how often

00:21:16,420 --> 00:21:19,840
does this word appear how often does

00:21:18,130 --> 00:21:23,200
that would appear and so on in my line

00:21:19,840 --> 00:21:25,050
of text we've got a fun one term

00:21:23,200 --> 00:21:28,390
frequency by inverse document frequency

00:21:25,050 --> 00:21:29,950
which is basically how important is a

00:21:28,390 --> 00:21:33,880
words to the document you're looking at

00:21:29,950 --> 00:21:36,190
verses all the documents so this kind of

00:21:33,880 --> 00:21:38,350
house search engine starts it off I

00:21:36,190 --> 00:21:40,630
don't know if Google every used it but

00:21:38,350 --> 00:21:43,510
i'm pretty sure altavista started off

00:21:40,630 --> 00:21:46,720
using it basically you know that the

00:21:43,510 --> 00:21:48,160
word the is really common so you need to

00:21:46,720 --> 00:21:50,470
the fact that it appears in all your

00:21:48,160 --> 00:21:52,330
documents while the fact that it appears

00:21:50,470 --> 00:21:55,090
really often in the document might make

00:21:52,330 --> 00:21:56,679
you think naively that it's important

00:21:55,090 --> 00:21:59,019
the fact that it

00:21:56,679 --> 00:22:01,840
years in all the documents a lot needs

00:21:59,019 --> 00:22:03,669
to be ruled out so in that respect we

00:22:01,840 --> 00:22:07,419
can actually bubble up important

00:22:03,669 --> 00:22:11,590
keywords as we come out n grams which

00:22:07,419 --> 00:22:14,740
basically one two three words in a row

00:22:11,590 --> 00:22:16,990
so it might be that instead of saying if

00:22:14,740 --> 00:22:18,580
a specific word appears we want to see

00:22:16,990 --> 00:22:22,570
if two words appear next to each other

00:22:18,580 --> 00:22:26,019
or a three so by grams and trigrams we

00:22:22,570 --> 00:22:28,450
can infer well well we can construct

00:22:26,019 --> 00:22:30,869
something so if we know if we're

00:22:28,450 --> 00:22:34,240
classifying people and we know their

00:22:30,869 --> 00:22:38,740
date of birth and we know that the date

00:22:34,240 --> 00:22:41,169
is the 18th of februari 2016 then we can

00:22:38,740 --> 00:22:44,019
in calculate their age and that can be a

00:22:41,169 --> 00:22:46,600
number that comes out basically as I say

00:22:44,019 --> 00:22:48,249
anything that creates a number and what

00:22:46,600 --> 00:22:50,590
we're trying to do is look at the data

00:22:48,249 --> 00:22:52,960
that's come in and figure out where

00:22:50,590 --> 00:22:56,019
those numbers and those dimensions

00:22:52,960 --> 00:23:00,190
correlate with the with the tag that

00:22:56,019 --> 00:23:01,419
we've we've come up with so I mentioned

00:23:00,190 --> 00:23:07,679
about thinking about things in

00:23:01,419 --> 00:23:10,119
multi-dimensional space 3d 4d 5d ND the

00:23:07,679 --> 00:23:12,669
the kind of cool thing about doing this

00:23:10,119 --> 00:23:13,840
is you have a lot of side effects you

00:23:12,669 --> 00:23:15,940
can think about it in a really

00:23:13,840 --> 00:23:18,309
interesting sense anyway a document

00:23:15,940 --> 00:23:20,649
might be over here in 3d space another

00:23:18,309 --> 00:23:22,119
document might be over here and the very

00:23:20,649 --> 00:23:25,659
fact that they're quite distant means

00:23:22,119 --> 00:23:27,519
they're not similar so we can use what's

00:23:25,659 --> 00:23:29,830
called cosine similarity to take two

00:23:27,519 --> 00:23:31,679
documents that we described with these

00:23:29,830 --> 00:23:34,360
feature vectors these collections of

00:23:31,679 --> 00:23:36,340
feature features that we've extracted

00:23:34,360 --> 00:23:37,960
and we can look at the two of them and

00:23:36,340 --> 00:23:40,119
say how similar they are because we've

00:23:37,960 --> 00:23:42,129
modeled them an n-dimensional space done

00:23:40,119 --> 00:23:43,899
a bit of Pythagoras and figured out that

00:23:42,129 --> 00:23:46,059
they're far apart they're not similar or

00:23:43,899 --> 00:23:49,299
they're very very similar they use very

00:23:46,059 --> 00:23:52,929
similar words so we can start thinking

00:23:49,299 --> 00:23:54,820
about things in a spatial sense I bring

00:23:52,929 --> 00:23:56,919
that up only because it's really cool

00:23:54,820 --> 00:23:58,330
and it's way that we can take the inputs

00:23:56,919 --> 00:24:01,600
to machine learning and figure out

00:23:58,330 --> 00:24:03,039
relevancy as we go through the other

00:24:01,600 --> 00:24:05,499
thing is that we can use the scary

00:24:03,039 --> 00:24:07,509
matrix multiplication stuff again I've

00:24:05,499 --> 00:24:11,249
forgotten since my GCSEs and a-levels

00:24:07,509 --> 00:24:11,249
but i will remember one day

00:24:11,560 --> 00:24:17,900
so I talked about feature extraction the

00:24:15,010 --> 00:24:20,480
the text on the the right might be a bit

00:24:17,900 --> 00:24:23,810
small for some of you at the back but

00:24:20,480 --> 00:24:27,650
effectively we step through each part of

00:24:23,810 --> 00:24:32,060
our input document and this example this

00:24:27,650 --> 00:24:33,860
talk we can take out some of the common

00:24:32,060 --> 00:24:37,460
words that's called stop words and

00:24:33,860 --> 00:24:41,120
removing those and from and have and 2

00:24:37,460 --> 00:24:43,640
and so on we can take the words

00:24:41,120 --> 00:24:45,560
themselves and reduce some of the

00:24:43,640 --> 00:24:48,200
problems that even further by doing

00:24:45,560 --> 00:24:50,630
something called stemming which is where

00:24:48,200 --> 00:24:52,820
you take you do your conjugation if

00:24:50,630 --> 00:24:54,560
anyone ever did Latin you do your

00:24:52,820 --> 00:24:57,350
conjugation on the words and you come up

00:24:54,560 --> 00:24:59,870
with a common root of each word so apply

00:24:57,350 --> 00:25:02,270
and applying boil down to the same kind

00:24:59,870 --> 00:25:04,550
of token if you like and then we kind of

00:25:02,270 --> 00:25:07,910
count them as we go through so we build

00:25:04,550 --> 00:25:10,310
up a list of features event title at PHP

00:25:07,910 --> 00:25:13,570
for example we've got one up there and

00:25:10,310 --> 00:25:16,880
so on and we build that as we go through

00:25:13,570 --> 00:25:25,100
so so we come up with a simple feature

00:25:16,880 --> 00:25:26,690
vector have I lost the mic nope so so

00:25:25,100 --> 00:25:28,820
that's kind of an example of what we've

00:25:26,690 --> 00:25:30,770
done there what we can actually do is

00:25:28,820 --> 00:25:33,680
again we can take two words we can go

00:25:30,770 --> 00:25:36,080
with reading everything as the by Graham

00:25:33,680 --> 00:25:41,300
example and that becomes another measure

00:25:36,080 --> 00:25:46,010
that we're going through I seem to have

00:25:41,300 --> 00:25:49,580
copy to slide there that's wonderful so

00:25:46,010 --> 00:25:53,450
what we do after that point is how it

00:25:49,580 --> 00:25:55,220
done that no there we go so with the

00:25:53,450 --> 00:25:57,230
supervised learning we have what we call

00:25:55,220 --> 00:25:58,820
a training set so we have that

00:25:57,230 --> 00:26:01,220
information where we've got all these

00:25:58,820 --> 00:26:04,130
documents that are already tagged we

00:26:01,220 --> 00:26:07,520
have these known knowns and that model

00:26:04,130 --> 00:26:09,140
that we're going to build from and we

00:26:07,520 --> 00:26:11,530
start to make the assumption and this is

00:26:09,140 --> 00:26:16,700
where some of the naive element of that

00:26:11,530 --> 00:26:18,230
that bayesian classifier comes in we

00:26:16,700 --> 00:26:20,920
start to make the assumption that the

00:26:18,230 --> 00:26:24,100
tags that are in our training set

00:26:20,920 --> 00:26:28,960
appear just as often in there as they

00:26:24,100 --> 00:26:31,660
would in the whole of our set so if the

00:26:28,960 --> 00:26:35,830
word PHP appears in three quarters of

00:26:31,660 --> 00:26:38,680
all the documents on joined in then in

00:26:35,830 --> 00:26:40,600
three quarters of the new ones will find

00:26:38,680 --> 00:26:44,680
that word appearing three-quarters of

00:26:40,600 --> 00:26:46,390
the times again so we we take that

00:26:44,680 --> 00:26:47,710
assumption and we start counting them as

00:26:46,390 --> 00:26:49,030
I've just shown there and eventually

00:26:47,710 --> 00:26:52,360
we'll take the number of documents

00:26:49,030 --> 00:26:53,710
they'd appear they've appeared in and

00:26:52,360 --> 00:26:55,390
divide that through by the number of

00:26:53,710 --> 00:26:57,670
documents we have as a whole and figure

00:26:55,390 --> 00:27:01,120
out there there the number of times that

00:26:57,670 --> 00:27:02,980
it appears across the whole document so

00:27:01,120 --> 00:27:04,510
in that learning stage we extract those

00:27:02,980 --> 00:27:06,520
features we go through those step for

00:27:04,510 --> 00:27:08,830
all of our things there we take those

00:27:06,520 --> 00:27:11,830
tags that we know already and we build

00:27:08,830 --> 00:27:13,810
that correlation so we're counting this

00:27:11,830 --> 00:27:15,370
tag appears when this word appears and

00:27:13,810 --> 00:27:21,130
doesn't appear when this word appears

00:27:15,370 --> 00:27:23,290
and so on so again many methods of

00:27:21,130 --> 00:27:27,310
probability but if we're assuming that

00:27:23,290 --> 00:27:30,550
our training set is representative of

00:27:27,310 --> 00:27:33,970
all documents that can never be put into

00:27:30,550 --> 00:27:37,270
joined in then we can say that the

00:27:33,970 --> 00:27:40,030
probability that a tag predicts a

00:27:37,270 --> 00:27:44,410
feature so the probability that the word

00:27:40,030 --> 00:27:47,590
PHP as a tag predicts that the word PHP

00:27:44,410 --> 00:27:50,800
appears in the document that's a number

00:27:47,590 --> 00:27:54,130
of items with PHP in the document and

00:27:50,800 --> 00:27:59,020
PHP is a tag divided by the number of

00:27:54,130 --> 00:28:00,430
items with PHP in the whole thing it

00:27:59,020 --> 00:28:04,000
might be a little early in the morning

00:28:00,430 --> 00:28:06,340
it's almost afternoon but you know it's

00:28:04,000 --> 00:28:08,520
pretty straightforward a better example

00:28:06,340 --> 00:28:12,130
because that's pretty obvious might be

00:28:08,520 --> 00:28:15,460
the probability that the tag PHP

00:28:12,130 --> 00:28:18,310
indicates that symphony is in that

00:28:15,460 --> 00:28:20,020
document is the number of items with PHP

00:28:18,310 --> 00:28:22,330
as a tag and symphony in the document

00:28:20,020 --> 00:28:26,830
divided by the number of items with

00:28:22,330 --> 00:28:29,250
symphony in the document we can also

00:28:26,830 --> 00:28:32,960
write that as the probability that

00:28:29,250 --> 00:28:36,320
feature given the tag

00:28:32,960 --> 00:28:39,830
occurs is that so I'm going to use some

00:28:36,320 --> 00:28:41,870
of that note some of that notation as we

00:28:39,830 --> 00:28:43,669
go through some of the equations that

00:28:41,870 --> 00:28:50,809
I'm going to say just trust me on this

00:28:43,669 --> 00:28:52,429
one and encode again and might be a bit

00:28:50,809 --> 00:28:54,440
small for you at the back but

00:28:52,429 --> 00:28:55,789
effectively what we're doing is we're

00:28:54,440 --> 00:28:58,909
looping through the number of things

00:28:55,789 --> 00:29:03,169
that we've that we've identified in

00:28:58,909 --> 00:29:05,000
there we we have think we've for each of

00:29:03,169 --> 00:29:09,080
the tags that we found we built up the

00:29:05,000 --> 00:29:13,520
number of the number of features that

00:29:09,080 --> 00:29:15,799
occur against that tag so that's in our

00:29:13,520 --> 00:29:20,500
feature list as we go through there and

00:29:15,799 --> 00:29:20,500
we're just there doing this division

00:29:20,529 --> 00:29:27,830
with a bit of cheating so see how we've

00:29:25,909 --> 00:29:29,899
got our list we're looping through each

00:29:27,830 --> 00:29:32,049
one of those we're filling it out so

00:29:29,899 --> 00:29:34,909
that we have zeros at the point of which

00:29:32,049 --> 00:29:36,740
feature hasn't occurred so that even

00:29:34,909 --> 00:29:38,779
though that word in the current document

00:29:36,740 --> 00:29:39,980
it occurred somewhere in all our

00:29:38,779 --> 00:29:44,120
documents so that we're filling it in

00:29:39,980 --> 00:29:45,470
with zero we do what we call what's

00:29:44,120 --> 00:29:47,149
called Laplace smoothing which is

00:29:45,470 --> 00:29:51,770
basically make sure that we never get a

00:29:47,149 --> 00:29:55,490
zero by adding one to the the item on

00:29:51,770 --> 00:29:56,600
the top then we do a bit of cheating to

00:29:55,490 --> 00:30:01,159
make up for the fact that we've just

00:29:56,600 --> 00:30:04,580
massively biased our numbers and in

00:30:01,159 --> 00:30:06,620
reality i could find probably a lot

00:30:04,580 --> 00:30:10,340
better ways of doing that mathematically

00:30:06,620 --> 00:30:12,919
but again it's beyond me so so just

00:30:10,340 --> 00:30:16,730
adding that kind of tweak there is what

00:30:12,919 --> 00:30:18,110
we're doing is cheating and effectively

00:30:16,730 --> 00:30:20,179
what we're doing is we're setting

00:30:18,110 --> 00:30:23,750
ourselves up for the application of

00:30:20,179 --> 00:30:28,490
Bayes theorem which is the probability

00:30:23,750 --> 00:30:31,279
that a tag is predicted by a feature so

00:30:28,490 --> 00:30:33,710
that the tag PHP is predicted by the

00:30:31,279 --> 00:30:37,549
occurrence of the word symphony in our

00:30:33,710 --> 00:30:39,529
document is equal to the probability of

00:30:37,549 --> 00:30:44,029
the tag appearing across all our

00:30:39,529 --> 00:30:45,389
documents times that thing we worked out

00:30:44,029 --> 00:30:46,950
a second ago

00:30:45,389 --> 00:30:50,369
the probability that a feature is

00:30:46,950 --> 00:30:52,559
predicted by a tag divided by the

00:30:50,369 --> 00:30:56,940
probability of that feature existing in

00:30:52,559 --> 00:31:00,509
all our documents you don't have to

00:30:56,940 --> 00:31:04,049
remember that I've said it I've won the

00:31:00,509 --> 00:31:06,389
points that's good now this is Bayes

00:31:04,049 --> 00:31:10,229
theorem I've missed out the

00:31:06,389 --> 00:31:13,769
multivariable I wrote a really nice blog

00:31:10,229 --> 00:31:18,690
post that explains it and that's what

00:31:13,769 --> 00:31:21,629
you get so the probability that a tag is

00:31:18,690 --> 00:31:24,619
predicted by features one two and three

00:31:21,629 --> 00:31:27,329
is the probability of that tag appearing

00:31:24,619 --> 00:31:29,159
times the probability that the feature

00:31:27,329 --> 00:31:38,339
is predicted by tag featured predicted

00:31:29,159 --> 00:31:40,799
and so on / that was a genuine yawn yes

00:31:38,339 --> 00:31:42,839
and you can find that basically all the

00:31:40,799 --> 00:31:45,119
steps as I was working out I wrote it

00:31:42,839 --> 00:31:46,709
down and it might make absolutely no

00:31:45,119 --> 00:31:51,119
sense to anyone but you can see I've at

00:31:46,709 --> 00:31:53,029
least on the working what we can do is

00:31:51,119 --> 00:31:55,849
we can identify bits this is the prior

00:31:53,029 --> 00:31:59,549
so this is the number of documents

00:31:55,849 --> 00:32:02,219
tagged with that tag / all the documents

00:31:59,549 --> 00:32:04,049
we've got there remember I was talking

00:32:02,219 --> 00:32:06,299
about we're going to apply the the

00:32:04,049 --> 00:32:08,820
assumption that the tags appearing in

00:32:06,299 --> 00:32:12,349
our training sets are equally

00:32:08,820 --> 00:32:15,809
distributed against our tags everywhere

00:32:12,349 --> 00:32:19,409
the number at the bottom which is the

00:32:15,809 --> 00:32:22,489
probability that all of these three

00:32:19,409 --> 00:32:26,070
features exist in the same document

00:32:22,489 --> 00:32:27,329
that's going to be fixed that's just a

00:32:26,070 --> 00:32:29,669
number that's not going to change

00:32:27,329 --> 00:32:32,789
whatever tag that we're applying this to

00:32:29,669 --> 00:32:35,129
so we can kind of cheat that again there

00:32:32,789 --> 00:32:37,709
and like I said classifiers are about

00:32:35,129 --> 00:32:38,909
reducing complexity to try and make

00:32:37,709 --> 00:32:41,639
things a bit faster in a bit more

00:32:38,909 --> 00:32:44,700
applicable so we just replace that with

00:32:41,639 --> 00:32:46,589
a Z we can just mess about with the

00:32:44,700 --> 00:32:48,119
numbers as we took that in we can assume

00:32:46,589 --> 00:32:50,339
it's one that they're always going to be

00:32:48,119 --> 00:32:51,839
there or we can find a nice balance that

00:32:50,339 --> 00:32:54,389
bring stuff out at the end of the day

00:32:51,839 --> 00:32:55,889
it's just like removing a constant and

00:32:54,389 --> 00:32:58,139
frequently that's combined with the

00:32:55,889 --> 00:32:58,659
prior because again that one's fixed

00:32:58,139 --> 00:33:05,019
every

00:32:58,659 --> 00:33:06,940
we were going through or encode again I

00:33:05,019 --> 00:33:09,820
said simple machine learning I think the

00:33:06,940 --> 00:33:14,289
first slide was of code was about 10 12

00:33:09,820 --> 00:33:16,330
lines this one's again about eight it

00:33:14,289 --> 00:33:19,450
doesn't do anything very much special

00:33:16,330 --> 00:33:21,879
but again we're calculating our prior at

00:33:19,450 --> 00:33:23,830
this point so the number of times the

00:33:21,879 --> 00:33:30,070
label occurs versus the number of

00:33:23,830 --> 00:33:33,159
documents in the whole training set we

00:33:30,070 --> 00:33:36,429
then going through do we have that

00:33:33,159 --> 00:33:39,970
feature in in the model as we're doing

00:33:36,429 --> 00:33:42,909
this prediction stage and the if the

00:33:39,970 --> 00:33:44,590
answer is yes then what we do is we

00:33:42,909 --> 00:33:47,649
actually just grab that probability that

00:33:44,590 --> 00:33:50,349
we've calculated previously how often

00:33:47,649 --> 00:33:53,979
does the feature occur for the tag and

00:33:50,349 --> 00:33:56,019
then we just multiply it together so

00:33:53,979 --> 00:33:58,479
again probability the tag is predicted

00:33:56,019 --> 00:34:00,399
by feature and like I said we then

00:33:58,479 --> 00:34:05,409
multiply the problem with multiplication

00:34:00,399 --> 00:34:08,500
and and all these things is we're

00:34:05,409 --> 00:34:10,510
calculating for big numbers of documents

00:34:08,500 --> 00:34:13,569
we're calculating really small numbers

00:34:10,510 --> 00:34:15,399
which is great for a probability when

00:34:13,569 --> 00:34:17,020
you're doing it once but as soon as you

00:34:15,399 --> 00:34:18,399
start multiplying it gets worse and

00:34:17,020 --> 00:34:22,200
worse and worse and smaller and smaller

00:34:18,399 --> 00:34:25,210
and smaller and dashi 27 and 80 and blah

00:34:22,200 --> 00:34:31,000
so if we actually switch using log we

00:34:25,210 --> 00:34:32,409
can turn that into addition not lose out

00:34:31,000 --> 00:34:33,819
on any of the numbers it's just a

00:34:32,409 --> 00:34:35,740
different bit of transformation that we

00:34:33,819 --> 00:34:37,539
can do there and we'll see a bit later

00:34:35,740 --> 00:34:41,440
that the numbers are very different they

00:34:37,539 --> 00:34:47,069
also end up very negative again beyond

00:34:41,440 --> 00:34:47,069
me oh dear

00:34:47,490 --> 00:34:54,850
so I'd love to build this up and say

00:34:50,200 --> 00:34:57,310
it's spectacular it's wonderful it's the

00:34:54,850 --> 00:34:59,110
best thing you'll see all day it might

00:34:57,310 --> 00:35:04,060
be the best thing you've seen since the

00:34:59,110 --> 00:35:11,070
keynote I'll go with that because it

00:35:04,060 --> 00:35:11,070
certainly doesn't beat that uh-huh so

00:35:13,830 --> 00:35:21,910
here's a little little web app that

00:35:18,700 --> 00:35:24,460
basically takes everything I've shown

00:35:21,910 --> 00:35:27,060
you model to all in the background for

00:35:24,460 --> 00:35:30,100
all those thousands and forty eight

00:35:27,060 --> 00:35:33,730
events and all those huge numbers of

00:35:30,100 --> 00:35:37,560
tags 350 something like that builds a

00:35:33,730 --> 00:35:41,470
model then applies the classifier and

00:35:37,560 --> 00:35:45,750
then just gives you the probability that

00:35:41,470 --> 00:35:48,910
a given tag is right for for the event

00:35:45,750 --> 00:35:51,760
so what we've done is taken the title of

00:35:48,910 --> 00:35:53,500
the event and the body we've broken it

00:35:51,760 --> 00:35:56,710
down this is a bit of a better one dumb

00:35:53,500 --> 00:36:00,460
code we broken it down into the features

00:35:56,710 --> 00:36:01,900
that we have there I lied namespace them

00:36:00,460 --> 00:36:06,010
just so that I could use more than the

00:36:01,900 --> 00:36:08,140
engrams that are saying in the past we

00:36:06,010 --> 00:36:10,450
then do all that multiplication together

00:36:08,140 --> 00:36:12,490
we apply all those probabilities we've

00:36:10,450 --> 00:36:15,250
worked out in our model and we work out

00:36:12,490 --> 00:36:22,360
that that's something that this is PHP

00:36:15,250 --> 00:36:24,010
is a really small number the important

00:36:22,360 --> 00:36:26,140
thing is because that we're multiplying

00:36:24,010 --> 00:36:28,630
for each word in the document

00:36:26,140 --> 00:36:30,580
effectively or each unique word in the

00:36:28,630 --> 00:36:32,560
document that number is going to be

00:36:30,580 --> 00:36:34,600
relative to the whole thing so we're

00:36:32,560 --> 00:36:36,160
looking for big differences so we can

00:36:34,600 --> 00:36:38,920
see there's a huge level of difference

00:36:36,160 --> 00:36:43,500
between DOM code being about PHP which

00:36:38,920 --> 00:36:47,530
it it mentions PHP we have a result

00:36:43,500 --> 00:36:48,820
versus it being about the web is quite

00:36:47,530 --> 00:36:50,740
high but you can see how we're actually

00:36:48,820 --> 00:36:52,450
picking up some of the ideas we've seen

00:36:50,740 --> 00:36:54,970
some correlation about symphony and

00:36:52,450 --> 00:36:57,250
Drupal as we go through I was going to

00:36:54,970 --> 00:36:59,430
go through and figure out what a really

00:36:57,250 --> 00:37:02,619
good page to show you on was

00:36:59,430 --> 00:37:05,079
Madison PHP is thirteen percent likely

00:37:02,619 --> 00:37:08,619
to be about PHP which is a result and

00:37:05,079 --> 00:37:09,910
it's a consistent result look so

00:37:08,619 --> 00:37:12,010
effectively that's what we're doing

00:37:09,910 --> 00:37:13,420
we're taking those tags that we already

00:37:12,010 --> 00:37:15,940
have existing we're doing all the

00:37:13,420 --> 00:37:21,549
multiplication together I mentioned it

00:37:15,940 --> 00:37:23,980
was about 350 tags this is only done

00:37:21,549 --> 00:37:26,289
over 20 because as I'll show you a

00:37:23,980 --> 00:37:29,140
little bit later there are some slight

00:37:26,289 --> 00:37:32,890
memory issues in doing this kind of

00:37:29,140 --> 00:37:36,420
thing and again that's I've reduced the

00:37:32,890 --> 00:37:39,970
problem by chopping out 330 of the tags

00:37:36,420 --> 00:37:44,200
for this model I mentioned about using

00:37:39,970 --> 00:37:46,029
logarithms so instead we can have actual

00:37:44,200 --> 00:37:47,200
numbers just shifts the problem into

00:37:46,029 --> 00:37:51,039
something that we can deal with a bit

00:37:47,200 --> 00:37:54,549
better and so on as we go through now

00:37:51,039 --> 00:37:57,099
this model when I built it talk about 23

00:37:54,549 --> 00:38:01,089
seconds I'm now going to switch it to

00:37:57,099 --> 00:38:03,039
using Engram by grams I'm now going to

00:38:01,089 --> 00:38:06,760
hit build the model and then we're going

00:38:03,039 --> 00:38:09,099
to wait it's going to fail I'm going to

00:38:06,760 --> 00:38:11,410
have waffle for 23 seconds for no good

00:38:09,099 --> 00:38:13,539
reason I'm going to cry and then switch

00:38:11,410 --> 00:38:15,430
to another demo just to show you some of

00:38:13,539 --> 00:38:18,700
the interactive kind of ideas as we go

00:38:15,430 --> 00:38:22,049
through I think we still got about 15

00:38:18,700 --> 00:38:24,130
seconds left on this so I'll keep going

00:38:22,049 --> 00:38:27,099
one of the things I found in through

00:38:24,130 --> 00:38:31,390
doing this is I'm not necessarily the

00:38:27,099 --> 00:38:34,000
best of it yet I'm getting better yay

00:38:31,390 --> 00:38:38,079
memory issue let's go back to where we

00:38:34,000 --> 00:38:41,039
were we can't lovely like I said it

00:38:38,079 --> 00:38:43,599
failed and this is an important lesson

00:38:41,039 --> 00:38:50,170
don't go off-piste when you're doing a

00:38:43,599 --> 00:38:51,099
talk skip to the next demo one of the

00:38:50,170 --> 00:38:53,140
things I didn't talk about there

00:38:51,099 --> 00:38:55,720
actually was the fact that we've got the

00:38:53,140 --> 00:38:57,460
multivariate naive Bayes which is doing

00:38:55,720 --> 00:38:59,289
all that multiplication together but

00:38:57,460 --> 00:39:03,609
that's basically saying if a word exists

00:38:59,289 --> 00:39:06,640
there what we do is actually multiply

00:39:03,609 --> 00:39:08,950
when we encounter it Bernoulli that I

00:39:06,640 --> 00:39:11,319
mentioned earlier actually models for

00:39:08,950 --> 00:39:12,339
all the words that you've got and if it

00:39:11,319 --> 00:39:15,400
works out the coral

00:39:12,339 --> 00:39:18,489
not only of when the tag exists and a

00:39:15,400 --> 00:39:20,769
word exists but when it tags doesn't

00:39:18,489 --> 00:39:25,479
exist and the word exists and tries to

00:39:20,769 --> 00:39:26,950
figure out where you have the

00:39:25,479 --> 00:39:29,529
probability that something doesn't

00:39:26,950 --> 00:39:31,839
predict and the probability that

00:39:29,529 --> 00:39:34,450
something does predict as we go through

00:39:31,839 --> 00:39:36,339
and merge that together it's a fairly

00:39:34,450 --> 00:39:37,809
straightforward process again I'm going

00:39:36,339 --> 00:39:39,999
to give you a link to the code later so

00:39:37,809 --> 00:39:42,400
you can pick through that as we go

00:39:39,999 --> 00:39:45,910
through this one is more interactive and

00:39:42,400 --> 00:39:48,400
this shows kind of the interactive

00:39:45,910 --> 00:39:51,190
supervised learning side of things so

00:39:48,400 --> 00:39:52,960
this is halfway house to that machine

00:39:51,190 --> 00:39:54,880
that human tagging I was talking about

00:39:52,960 --> 00:39:56,529
being boring as we go through this is

00:39:54,880 --> 00:39:59,289
how I know it's not quite a minute to do

00:39:56,529 --> 00:40:02,829
each one because that's about PHP that's

00:39:59,289 --> 00:40:04,599
about PHP that's about PHP are you

00:40:02,829 --> 00:40:08,499
giving a scene that one isn't excellence

00:40:04,599 --> 00:40:12,369
and so on as we go through we can then

00:40:08,499 --> 00:40:13,630
submit that through we can build our

00:40:12,369 --> 00:40:17,469
model as we go through this one's just

00:40:13,630 --> 00:40:19,390
working down on the actual titles as we

00:40:17,469 --> 00:40:22,420
go through just to speed up for the demo

00:40:19,390 --> 00:40:24,039
and so on we can see that using the

00:40:22,420 --> 00:40:26,529
Bernoulli side of things we're saying

00:40:24,039 --> 00:40:29,200
it's either PHP or not PHP and we can

00:40:26,529 --> 00:40:31,960
see that because we have an interesting

00:40:29,200 --> 00:40:34,960
mix of languages in here and place names

00:40:31,960 --> 00:40:38,460
we can see that Friday apparently is a

00:40:34,960 --> 00:40:43,630
hundred percent correlated with PHP

00:40:38,460 --> 00:40:46,839
whereas a jas days yay is positively

00:40:43,630 --> 00:40:48,339
correlated with not PHP um there are a

00:40:46,839 --> 00:40:49,749
lot more tags as we go through here but

00:40:48,339 --> 00:40:51,640
this just kind of gives a flavor of what

00:40:49,749 --> 00:40:54,489
we're building up in the background I've

00:40:51,640 --> 00:40:57,819
constructed a confidence score which is

00:40:54,489 --> 00:41:01,779
relatively random because we're taking

00:40:57,819 --> 00:41:04,299
eighty percent of our knowns we're

00:41:01,779 --> 00:41:06,339
building our model from that and as a

00:41:04,299 --> 00:41:08,410
result of that were then correlated

00:41:06,339 --> 00:41:10,329
checking that against our twenty percent

00:41:08,410 --> 00:41:12,999
left over of knowns to actually figure

00:41:10,329 --> 00:41:14,739
out how well we're doing if I had a bit

00:41:12,999 --> 00:41:16,539
more time in preparing this I might have

00:41:14,739 --> 00:41:19,539
actually created a feedback loop that

00:41:16,539 --> 00:41:20,799
you could use that confidence score how

00:41:19,539 --> 00:41:22,450
many times did I get it right against

00:41:20,799 --> 00:41:24,069
what I know versus how much how many

00:41:22,450 --> 00:41:26,000
times I didn't as a kind of a fitness

00:41:24,069 --> 00:41:28,010
function to help

00:41:26,000 --> 00:41:29,960
say that this is a good model and this

00:41:28,010 --> 00:41:31,940
is a bad model and so on and start

00:41:29,960 --> 00:41:34,400
tweaking the inputs some of those

00:41:31,940 --> 00:41:36,170
numbers the cheating numbers some of the

00:41:34,400 --> 00:41:38,810
weights if you like so that we can start

00:41:36,170 --> 00:41:40,580
building up a more complex picture but

00:41:38,810 --> 00:41:42,310
for the purposes of this we can see

00:41:40,580 --> 00:41:46,760
where our successes of failures are

00:41:42,310 --> 00:41:49,490
including in CSS and where we're getting

00:41:46,760 --> 00:41:53,450
stuff right and totally wrong as we go

00:41:49,490 --> 00:41:55,850
through so so that was the scary and

00:41:53,450 --> 00:41:59,140
didn't quite work demo as we go through

00:41:55,850 --> 00:42:01,730
all that codes is gone now on the floor

00:41:59,140 --> 00:42:03,830
all of that codes is going to be up I'll

00:42:01,730 --> 00:42:05,210
give a link to a later especially after

00:42:03,830 --> 00:42:08,390
i finished the hacking that'll make it

00:42:05,210 --> 00:42:11,480
look good so you can follow that through

00:42:08,390 --> 00:42:13,070
so given that I've now demonstrated a

00:42:11,480 --> 00:42:15,560
bit failure having been so positive

00:42:13,070 --> 00:42:17,330
going on the way through we had a bit of

00:42:15,560 --> 00:42:19,400
human learning to do as I went through

00:42:17,330 --> 00:42:22,610
this process and i talked about how i

00:42:19,400 --> 00:42:24,770
learned or how we learn as humans but

00:42:22,610 --> 00:42:28,610
this experience is fed into that their

00:42:24,770 --> 00:42:30,860
cause and effect side of things so

00:42:28,610 --> 00:42:35,440
lessons I've learned bayes theorem is

00:42:30,860 --> 00:42:37,760
annoying just trust that it works

00:42:35,440 --> 00:42:40,070
because at the end of the day it's all

00:42:37,760 --> 00:42:41,630
about multiplication division it's just

00:42:40,070 --> 00:42:45,400
those kind of figure out the numbers

00:42:41,630 --> 00:42:48,860
plug it all in let it fly do it stuff

00:42:45,400 --> 00:42:51,680
joined in it turns out that Larry

00:42:48,860 --> 00:42:54,970
Garfield was very correct in considering

00:42:51,680 --> 00:42:57,770
it to be a fairly accurate proxy for PHP

00:42:54,970 --> 00:43:01,700
conferences because it's very heavily

00:42:57,770 --> 00:43:03,350
PHP biased we're I've built something

00:43:01,700 --> 00:43:07,520
that pretty much labels everything is

00:43:03,350 --> 00:43:10,580
PHP it's not that's uncharitable it's

00:43:07,520 --> 00:43:12,320
because at the end of the day most of

00:43:10,580 --> 00:43:14,000
the data coming in is about PHP we've

00:43:12,320 --> 00:43:16,160
made that assumption that it's

00:43:14,000 --> 00:43:17,900
representative of what's actually out

00:43:16,160 --> 00:43:20,000
there in the wild and so if we've got a

00:43:17,900 --> 00:43:22,070
lot of prevalence of PHP in our training

00:43:20,000 --> 00:43:25,070
set that's going to be reflected in our

00:43:22,070 --> 00:43:28,070
output as we go through a lot of the

00:43:25,070 --> 00:43:29,930
tags are noisy I could have stepped

00:43:28,070 --> 00:43:33,530
through and shown you that the members

00:43:29,930 --> 00:43:37,220
of so flow PHP really love to tag

00:43:33,530 --> 00:43:38,680
themselves with the so flow PHP tag as

00:43:37,220 --> 00:43:41,290
do PHP Hampshire

00:43:38,680 --> 00:43:42,970
PHP hants and as a results because it

00:43:41,290 --> 00:43:45,010
appears quite a lot in our training set

00:43:42,970 --> 00:43:49,810
we start thinking that things in Paris

00:43:45,010 --> 00:43:51,190
are in southern Florida again that's not

00:43:49,810 --> 00:43:53,350
something that we can create correct

00:43:51,190 --> 00:43:54,820
overnight with the data coming in but it

00:43:53,350 --> 00:43:57,610
gives us an opportunity to actually

00:43:54,820 --> 00:43:59,920
figure out what features might better to

00:43:57,610 --> 00:44:02,920
describe stuff and kind of quilts that

00:43:59,920 --> 00:44:05,320
noise as we go through all this approach

00:44:02,920 --> 00:44:07,270
were building up lots of grids lots of

00:44:05,320 --> 00:44:11,680
numbers lots of multi-dimensional arrays

00:44:07,270 --> 00:44:13,180
that takes a lot of memory 350 tags

00:44:11,680 --> 00:44:19,260
which is what I was trying to calculate

00:44:13,180 --> 00:44:23,800
this with when I started by about 1300

00:44:19,260 --> 00:44:28,540
13,000 sorry features 76 bytes to put

00:44:23,800 --> 00:44:30,610
one item in a PHP array that's lots of

00:44:28,540 --> 00:44:33,340
memory that's 300 and something

00:44:30,610 --> 00:44:34,840
megabytes worth of memory and that's

00:44:33,340 --> 00:44:38,020
only going to get worse as you add more

00:44:34,840 --> 00:44:40,600
names so one of the things that we're

00:44:38,020 --> 00:44:42,940
learning by the hard way is that we need

00:44:40,600 --> 00:44:45,250
to actually be a bit more discerning are

00:44:42,940 --> 00:44:47,400
we building classifiers for one tag are

00:44:45,250 --> 00:44:49,990
we only going to pick out the actual

00:44:47,400 --> 00:44:52,870
signal features so the ones that appear

00:44:49,990 --> 00:44:56,110
lots versus some of the ones that don't

00:44:52,870 --> 00:44:59,350
appear much at all or the ones that that

00:44:56,110 --> 00:45:00,910
have high or very low correlation and

00:44:59,350 --> 00:45:02,770
ignore the ones that are middling and so

00:45:00,910 --> 00:45:05,080
on to reduce that problem like I said

00:45:02,770 --> 00:45:07,000
classification is actually the

00:45:05,080 --> 00:45:10,090
difference between classifiers ends up

00:45:07,000 --> 00:45:11,710
being how you can reduce that problem

00:45:10,090 --> 00:45:13,240
set so you can fit it in memory or you

00:45:11,710 --> 00:45:17,380
can fit it into the constraints that you

00:45:13,240 --> 00:45:19,540
already have so my next steps as I said

00:45:17,380 --> 00:45:22,120
I need to improve on my feature

00:45:19,540 --> 00:45:26,130
selection I need to actually start

00:45:22,120 --> 00:45:29,710
picking out bits from that data set that

00:45:26,130 --> 00:45:31,540
that are actually indicative of signal

00:45:29,710 --> 00:45:33,400
and stuff that I can combine in the

00:45:31,540 --> 00:45:36,220
better way and improve that confidence

00:45:33,400 --> 00:45:38,080
score that I've shown you there to

00:45:36,220 --> 00:45:40,480
implement different types of classifiers

00:45:38,080 --> 00:45:43,150
so I can't just stand up here and say to

00:45:40,480 --> 00:45:45,760
you bernoulli multivariate naive

00:45:43,150 --> 00:45:49,030
bayesian classifiers and go on a bit

00:45:45,760 --> 00:45:51,869
more and talk a bit better on that you

00:45:49,030 --> 00:45:53,730
build the model into joined in

00:45:51,869 --> 00:45:56,999
model even though I now lost the numbers

00:45:53,730 --> 00:45:58,440
was applying itself very quickly it was

00:45:56,999 --> 00:46:01,289
applying itself in about 30 milliseconds

00:45:58,440 --> 00:46:02,460
once we've actually loaded up passing a

00:46:01,289 --> 00:46:04,410
new document it would take 30

00:46:02,460 --> 00:46:06,329
milliseconds and you've got a tag or a

00:46:04,410 --> 00:46:08,519
set of tags out of the back of it so all

00:46:06,329 --> 00:46:10,259
the pain is in that creating the model

00:46:08,519 --> 00:46:11,999
maybe in some of the loading of the

00:46:10,259 --> 00:46:13,859
model but when it's applied it's really

00:46:11,999 --> 00:46:16,109
quick so if we found some way of

00:46:13,859 --> 00:46:19,380
applying that as you submit your

00:46:16,109 --> 00:46:20,670
documents that would be a lot quite

00:46:19,380 --> 00:46:23,249
useful hopefully in the future

00:46:20,670 --> 00:46:26,309
classification of joined in talks and

00:46:23,249 --> 00:46:28,410
again improve the performance actually

00:46:26,309 --> 00:46:31,140
figure out how matrices work actually

00:46:28,410 --> 00:46:32,700
add caching and do stuff that's a bit

00:46:31,140 --> 00:46:35,430
more rigorous than I happen in spite of

00:46:32,700 --> 00:46:40,589
this one of the things that did come out

00:46:35,430 --> 00:46:44,490
having applied having applied all these

00:46:40,589 --> 00:46:47,130
classifiers and given it a shot is this

00:46:44,490 --> 00:46:49,319
is the original output well I said the

00:46:47,130 --> 00:46:52,829
original id's it was calculated this

00:46:49,319 --> 00:46:54,599
morning it's the original form and the

00:46:52,829 --> 00:46:57,420
original methods that Larry Garfield

00:46:54,599 --> 00:47:00,299
came up this is where I was picking out

00:46:57,420 --> 00:47:05,220
the fact that we have j/s day popping up

00:47:00,299 --> 00:47:08,249
with 24 talks and fifty-seven percent

00:47:05,220 --> 00:47:10,859
new speakers you've got I'm not entirely

00:47:08,249 --> 00:47:14,430
sure what that conference is there but

00:47:10,859 --> 00:47:16,259
it's 98 talks and a hundred percent new

00:47:14,430 --> 00:47:17,940
speakers that's quite a lot of noise and

00:47:16,259 --> 00:47:20,670
that's the problem i was starting to

00:47:17,940 --> 00:47:23,940
look at and you can see down the bottom

00:47:20,670 --> 00:47:25,890
here the overall number currently as of

00:47:23,940 --> 00:47:31,140
this morning is about forty nine percent

00:47:25,890 --> 00:47:34,049
of new speakers in our proxy PHP side of

00:47:31,140 --> 00:47:36,119
things if we actually look at something

00:47:34,049 --> 00:47:38,849
I've classified which has taken out some

00:47:36,119 --> 00:47:42,059
of those some of that noise it's left in

00:47:38,849 --> 00:47:44,489
a few of them but it's taken out some of

00:47:42,059 --> 00:47:47,700
it and we actually see that our results

00:47:44,489 --> 00:47:49,920
starts to shrink down to forty four

00:47:47,700 --> 00:47:51,839
percent and when I actually get the

00:47:49,920 --> 00:47:53,880
classifier is right I'll publish the

00:47:51,839 --> 00:47:55,319
numbers there and feed that back and we

00:47:53,880 --> 00:47:57,569
can actually look at how we've we've

00:47:55,319 --> 00:48:00,480
gone through and improved that diversity

00:47:57,569 --> 00:48:05,220
of our new time speakers our first time

00:48:00,480 --> 00:48:07,530
speakers so you can

00:48:05,220 --> 00:48:09,540
find I promised the code I'm creating a

00:48:07,530 --> 00:48:12,810
library called the enamel which has

00:48:09,540 --> 00:48:14,849
currently multivariate and Bernoulli

00:48:12,810 --> 00:48:16,980
multivariate stuff in there and some of

00:48:14,849 --> 00:48:18,390
the the ways in which you can actually

00:48:16,980 --> 00:48:21,210
feed your own documents in and you're in

00:48:18,390 --> 00:48:22,800
tags and so on they joined in audit

00:48:21,210 --> 00:48:25,980
stuff created as I said bye Larry

00:48:22,800 --> 00:48:27,900
Garfield I've made some alterations to

00:48:25,980 --> 00:48:30,840
provide that web hadn't to bring in

00:48:27,900 --> 00:48:33,090
enamel and so on into it but effectively

00:48:30,840 --> 00:48:34,890
you can go to those places and check my

00:48:33,090 --> 00:48:37,020
working and check that the figures

00:48:34,890 --> 00:48:41,480
actually match what I've actually shown

00:48:37,020 --> 00:48:44,580
you not doing today so with that thank

00:48:41,480 --> 00:48:56,970
you very much and have I got any

00:48:44,580 --> 00:48:59,760
questions Billy similar things in

00:48:56,970 --> 00:49:01,560
elastic search and generate the same

00:48:59,760 --> 00:49:05,119
slot of statistics using aggregates I

00:49:01,560 --> 00:49:08,160
was wondering how your code compares to

00:49:05,119 --> 00:49:10,470
something like elastic search which has

00:49:08,160 --> 00:49:12,119
been like around for a little while how

00:49:10,470 --> 00:49:14,339
does it perform in comparison you get

00:49:12,119 --> 00:49:15,869
the same sort results um I assume he'd

00:49:14,339 --> 00:49:17,220
get the same sort of results he just

00:49:15,869 --> 00:49:20,849
would have something that's a hell of a

00:49:17,220 --> 00:49:23,010
lot less performance so this is more an

00:49:20,849 --> 00:49:25,050
exercise in not in making a fast clip

00:49:23,010 --> 00:49:27,750
fast thing but actually implementing

00:49:25,050 --> 00:49:28,740
myself I'm putting it forward yes there

00:49:27,750 --> 00:49:30,839
are a hell of a lot better stuff out

00:49:28,740 --> 00:49:32,849
there there's scikit-learn in in Python

00:49:30,839 --> 00:49:35,400
which is the industry standard for the

00:49:32,849 --> 00:49:36,960
coding side of things you can get better

00:49:35,400 --> 00:49:39,180
storage and this is backed with my

00:49:36,960 --> 00:49:42,900
sequel you can get all that kind of

00:49:39,180 --> 00:49:44,580
thing there in a way that PHP in the way

00:49:42,900 --> 00:49:46,740
that over it now anyway is a love a lot

00:49:44,580 --> 00:49:50,190
less performant about but this is an

00:49:46,740 --> 00:49:53,240
example of showing that hopefully anyone

00:49:50,190 --> 00:49:53,240
can do it because it's so simple

00:49:58,300 --> 00:50:05,180
just questioned by the interpretation of

00:50:02,300 --> 00:50:09,890
that thirteen percent that came up

00:50:05,180 --> 00:50:13,070
earlier that given something is tagged

00:50:09,890 --> 00:50:17,000
as PHP there's a thirteen percent chance

00:50:13,070 --> 00:50:19,640
that it actually is about PHP and so

00:50:17,000 --> 00:50:22,190
it's more that was the cumulative

00:50:19,640 --> 00:50:24,110
probability and because we're

00:50:22,190 --> 00:50:26,230
multiplying in that example without

00:50:24,110 --> 00:50:28,880
correct thing for it we're multiplying

00:50:26,230 --> 00:50:30,140
the fact that it's not just got PHP in

00:50:28,880 --> 00:50:32,360
there but it's got all these other words

00:50:30,140 --> 00:50:34,160
together that are existing in other

00:50:32,360 --> 00:50:35,990
documents that reduces that number down

00:50:34,160 --> 00:50:39,260
even further so the way the numbers

00:50:35,990 --> 00:50:41,060
represented is more about the comparison

00:50:39,260 --> 00:50:45,860
between the different tags themselves

00:50:41,060 --> 00:50:48,320
for that document rather than as a whole

00:50:45,860 --> 00:50:51,350
so it's reduced by the fact that I know

00:50:48,320 --> 00:50:54,560
you had a common word that showed up in

00:50:51,350 --> 00:50:55,850
JavaScript say talks in different

00:50:54,560 --> 00:50:57,830
documents so it's not necessarily

00:50:55,850 --> 00:51:05,870
there's only thirteen percent

00:50:57,830 --> 00:51:08,570
correlation in interpreted how should

00:51:05,870 --> 00:51:11,690
that be interpreted it's it's the fact

00:51:08,570 --> 00:51:14,480
that its most likely of all your tags to

00:51:11,690 --> 00:51:15,980
be applied so PHP was at the top it was

00:51:14,480 --> 00:51:18,350
the highest number even though it was a

00:51:15,980 --> 00:51:20,960
relatively low number it's the highest

00:51:18,350 --> 00:51:23,870
the highest correlation for that given

00:51:20,960 --> 00:51:26,900
document a better approach might be to

00:51:23,870 --> 00:51:30,620
say normalized for that we've got very

00:51:26,900 --> 00:51:32,900
different sizes of document there so a

00:51:30,620 --> 00:51:35,840
better approach might lead to say have

00:51:32,900 --> 00:51:37,970
some kind of if you're heavily weighted

00:51:35,840 --> 00:51:40,220
there versus all the other tags its most

00:51:37,970 --> 00:51:42,020
likely to be about that and as a result

00:51:40,220 --> 00:51:43,760
the actual number itself doesn't matter

00:51:42,020 --> 00:51:45,230
but the comparison with all the other

00:51:43,760 --> 00:51:47,120
numbers that you've generated for that

00:51:45,230 --> 00:51:49,270
model would be a better way of doing

00:51:47,120 --> 00:51:49,270
that

00:51:52,780 --> 00:52:00,619
hi I mean III saw the time it takes to

00:51:58,310 --> 00:52:02,420
actually execute the result do you have

00:52:00,619 --> 00:52:05,150
any thoughts about the memory management

00:52:02,420 --> 00:52:07,580
on this case like for for example this

00:52:05,150 --> 00:52:11,000
using pc may not be enough for the

00:52:07,580 --> 00:52:13,930
hardware and the approach I've done here

00:52:11,000 --> 00:52:17,210
you can probably do it in a more

00:52:13,930 --> 00:52:18,920
scalable side of things you could do all

00:52:17,210 --> 00:52:21,440
sorts of things to actually spread the

00:52:18,920 --> 00:52:23,900
load out the better approach in my

00:52:21,440 --> 00:52:25,760
example in would actually just to be

00:52:23,900 --> 00:52:27,740
reduced the amount of information being

00:52:25,760 --> 00:52:29,270
calculated all the way through so I'm

00:52:27,740 --> 00:52:31,190
not actually trying to use the memory in

00:52:29,270 --> 00:52:32,570
the first place so as opposed to using

00:52:31,190 --> 00:52:34,460
it more efficiently I use them more

00:52:32,570 --> 00:52:38,450
efficiently by not using as much of it

00:52:34,460 --> 00:52:41,089
if that kind of makes sense so instead

00:52:38,450 --> 00:52:46,280
of saying calculating all that data for

00:52:41,089 --> 00:52:48,290
the rehabil reduce the number of

00:52:46,280 --> 00:52:50,630
features on calculating so that I've

00:52:48,290 --> 00:52:52,339
only got I've got less information that

00:52:50,630 --> 00:52:55,960
I'm trying to calculate for each of

00:52:52,339 --> 00:52:55,960
those tags thank you

00:52:58,700 --> 00:53:11,660
I wonder when it might be useful if it's

00:53:09,920 --> 00:53:14,180
useful to manually edit the model after

00:53:11,660 --> 00:53:16,099
its generated for example you've said

00:53:14,180 --> 00:53:18,530
that fridays and represent correlated

00:53:16,099 --> 00:53:19,820
with BP is that something you might want

00:53:18,530 --> 00:53:22,550
to manually delete after you've

00:53:19,820 --> 00:53:24,320
generated a muddle or not and so there's

00:53:22,550 --> 00:53:27,079
no reason why you couldn't do that I'm

00:53:24,320 --> 00:53:28,730
so kind of prune what's going in I we

00:53:27,079 --> 00:53:31,160
can the advantages we know that's a

00:53:28,730 --> 00:53:33,619
ridiculous her thing but one of the

00:53:31,160 --> 00:53:37,520
interesting things of machine learning

00:53:33,619 --> 00:53:40,970
is the fact that it might be it might be

00:53:37,520 --> 00:53:45,290
that PHP only ever gets talked about and

00:53:40,970 --> 00:53:47,210
Fridays and you might assume that we

00:53:45,290 --> 00:53:49,400
talk about it every day but an actual

00:53:47,210 --> 00:53:51,890
fact on joined in it only ever gets

00:53:49,400 --> 00:53:53,660
talked about on Fridays and machine

00:53:51,890 --> 00:53:55,250
learning doing this and showing the

00:53:53,660 --> 00:53:58,520
actual numbers and calculating that

00:53:55,250 --> 00:54:02,000
might actually show you a correlation

00:53:58,520 --> 00:54:05,270
and a results that's entirely counter

00:54:02,000 --> 00:54:07,910
intuitive but might be entirely accurate

00:54:05,270 --> 00:54:09,650
so yes you could manipulate the model

00:54:07,910 --> 00:54:13,040
that you've created and that would be

00:54:09,650 --> 00:54:14,359
valid certainly in that example but

00:54:13,040 --> 00:54:16,670
that's making the assumption that we

00:54:14,359 --> 00:54:24,170
know right and not the thing that's

00:54:16,670 --> 00:54:26,359
actually crunch the numbers so I think

00:54:24,170 --> 00:54:29,359
there's only a couple of minutes left to

00:54:26,359 --> 00:54:31,430
actually going at a coffee so when you

00:54:29,359 --> 00:54:33,260
take a break now thank you very much for

00:54:31,430 --> 00:54:35,380
your questions if you'd like to get in

00:54:33,260 --> 00:54:35,380

YouTube URL: https://www.youtube.com/watch?v=tSUjgzCQB_4


