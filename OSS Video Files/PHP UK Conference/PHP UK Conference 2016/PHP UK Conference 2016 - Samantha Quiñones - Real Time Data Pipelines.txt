Title: PHP UK Conference 2016 - Samantha Quiñones - Real Time Data Pipelines
Publication date: 2016-03-16
Playlist: PHP UK Conference 2016
Description: 
	Taking a data-driven approach to application management starts with instrumentation, but storing, analyzing, and distributing that data to the people who need it introduces a unique set of problems. Discover how one of the world’s top digital media platforms handles the massive stream of analytics pouring in from millions of consumer devices. We'll dive into the technologies that allow us to collect, route, store, and find meaning in data at 20 MB/s.
Captions: 
	00:00:05,259 --> 00:00:08,780
my name is Samantha can be honest I'm

00:00:07,250 --> 00:00:11,510
talk to you little bit today about data

00:00:08,780 --> 00:00:12,650
pipelines a little bit about me if

00:00:11,510 --> 00:00:14,179
you're not familiar with me i'm a

00:00:12,650 --> 00:00:14,960
software engineer I've been in the

00:00:14,179 --> 00:00:17,599
business since about nineteen

00:00:14,960 --> 00:00:20,180
ninety-seven I usually just be a cobol

00:00:17,599 --> 00:00:23,060
programmer so I can talk to you about

00:00:20,180 --> 00:00:25,279
COBOL I just did about JavaScript some

00:00:23,060 --> 00:00:27,080
Java maybe even a little Python my could

00:00:25,279 --> 00:00:29,480
be talked a lot about PHP just up maybe

00:00:27,080 --> 00:00:31,039
a little bit I've been in the media

00:00:29,480 --> 00:00:32,449
space since 2012 so I spent the first

00:00:31,039 --> 00:00:35,989
half of my career in financial services

00:00:32,449 --> 00:00:38,210
i worked for visa for a decade left

00:00:35,989 --> 00:00:39,379
financial services and and moved into

00:00:38,210 --> 00:00:41,170
the media space and they've been with

00:00:39,379 --> 00:00:45,200
AOL which is a digital media company

00:00:41,170 --> 00:00:48,110
since 2014 i'm also the organizer of the

00:00:45,200 --> 00:00:50,090
washington DC PHP user group so if

00:00:48,110 --> 00:00:52,430
you're ever in in Washington DC in the

00:00:50,090 --> 00:00:54,739
States please come by we'd love to have

00:00:52,430 --> 00:00:56,780
you if you're interested in following me

00:00:54,739 --> 00:01:00,260
on Twitter which is the best place to

00:00:56,780 --> 00:01:02,660
reach me I eat killer bees or you can

00:01:00,260 --> 00:01:06,770
find me on my website if you can somehow

00:01:02,660 --> 00:01:09,080
remember how to spell my last name so i

00:01:06,770 --> 00:01:10,640
mentioned that i work from AOL i can

00:01:09,080 --> 00:01:14,390
kind of see you guys over the lights who

00:01:10,640 --> 00:01:15,530
is not at all familiar with AOL awesome

00:01:14,390 --> 00:01:23,179
who thought al went out of business

00:01:15,530 --> 00:01:25,610
years ago yeah so uh AOL was merged with

00:01:23,179 --> 00:01:27,890
time warner and then in 2009 the two

00:01:25,610 --> 00:01:30,920
entities separated and AOL was spun off

00:01:27,890 --> 00:01:33,440
as its own company and we are in the the

00:01:30,920 --> 00:01:34,819
digital media space so we actually own a

00:01:33,440 --> 00:01:36,910
bunch of media properties like The

00:01:34,819 --> 00:01:39,410
Huffington Post and gadget TechCrunch

00:01:36,910 --> 00:01:41,569
we're also a big player in the the video

00:01:39,410 --> 00:01:45,560
advertising market all right so we do

00:01:41,569 --> 00:01:47,000
demand-side advertisement stuff yeah

00:01:45,560 --> 00:01:48,770
we're why did we have those annoying

00:01:47,000 --> 00:01:53,030
interstitial video ads on your mobile

00:01:48,770 --> 00:01:56,929
devices that's like all us sorry we're

00:01:53,030 --> 00:01:58,310
actually a very very large company but

00:01:56,929 --> 00:02:01,160
for the last few years we've kind of put

00:01:58,310 --> 00:02:02,629
the brands like being gadget brand

00:02:01,160 --> 00:02:04,489
Huffington Post brand in front of the

00:02:02,629 --> 00:02:05,720
AOL brand so most people haven't heard

00:02:04,489 --> 00:02:09,530
of us although we are still one of the

00:02:05,720 --> 00:02:10,729
large players in digital media space so

00:02:09,530 --> 00:02:12,440
the reason that I asked that question

00:02:10,729 --> 00:02:14,810
when AOL reached out to me a few years

00:02:12,440 --> 00:02:17,120
ago or a year and a half ago to come and

00:02:14,810 --> 00:02:19,220
join them

00:02:17,120 --> 00:02:23,209
I said why I don't why would I do that I

00:02:19,220 --> 00:02:27,500
don't really I don't really want to work

00:02:23,209 --> 00:02:29,540
for a dead internet dinosaur um but

00:02:27,500 --> 00:02:30,560
actually I i took the interview because

00:02:29,540 --> 00:02:33,829
actually the person who reached out to

00:02:30,560 --> 00:02:34,760
me was really very interesting and in my

00:02:33,829 --> 00:02:36,709
interview was that's a really

00:02:34,760 --> 00:02:41,390
interesting question so who here like

00:02:36,709 --> 00:02:42,769
hires engineers I see a few hands this

00:02:41,390 --> 00:02:44,989
is a really great question to ask

00:02:42,769 --> 00:02:46,280
engineering candidates in my opinion and

00:02:44,989 --> 00:02:47,440
I kind of like to ask questions like

00:02:46,280 --> 00:02:49,459
this when I'm interviewing candidates

00:02:47,440 --> 00:02:50,900
how would you let editors test how well

00:02:49,459 --> 00:02:53,780
different headlines perform for the same

00:02:50,900 --> 00:02:55,069
piece of content it's a cool question

00:02:53,780 --> 00:02:57,310
because you can kind of look at how a

00:02:55,069 --> 00:02:59,540
person works through a problem

00:02:57,310 --> 00:03:01,310
technically without kind of relying on

00:02:59,540 --> 00:03:03,980
like weird CS questions and silliness

00:03:01,310 --> 00:03:05,180
that we like to do in this industry but

00:03:03,980 --> 00:03:06,230
I want you to keep this question in the

00:03:05,180 --> 00:03:07,400
back of your mind because it's actually

00:03:06,230 --> 00:03:12,640
turned out to be a very important

00:03:07,400 --> 00:03:12,640
question when it comes to my time at AOL

00:03:13,450 --> 00:03:16,579
obviously that's an example of

00:03:15,140 --> 00:03:17,750
multivariate testing multivariate

00:03:16,579 --> 00:03:19,489
testing is one way that we can use

00:03:17,750 --> 00:03:20,359
metrics to improve our applications

00:03:19,489 --> 00:03:21,769
right so there's a lot of different

00:03:20,359 --> 00:03:23,660
traditional kind of metrics that we look

00:03:21,769 --> 00:03:25,669
at stuff like request response time

00:03:23,660 --> 00:03:27,530
cache hit rates resource utilization

00:03:25,669 --> 00:03:28,880
things are we're looking at to make sure

00:03:27,530 --> 00:03:31,579
that we're using the hardware that we

00:03:28,880 --> 00:03:34,720
have efficiently that we're providing a

00:03:31,579 --> 00:03:37,250
good level of performance for our users

00:03:34,720 --> 00:03:38,239
we use tools like New Relic I feel like

00:03:37,250 --> 00:03:41,359
this is perfect cuz we're in the new

00:03:38,239 --> 00:03:42,980
relic room new relic track right we used

00:03:41,359 --> 00:03:47,389
tools like New Relic to look at things

00:03:42,980 --> 00:03:49,519
like our CPU time per request versus

00:03:47,389 --> 00:03:51,859
throughput so how much as we as we were

00:03:49,519 --> 00:03:55,489
having more and more throughput how is

00:03:51,859 --> 00:03:57,049
our CPU times per request changing so we

00:03:55,489 --> 00:04:00,410
see that it kind of reduces as caches

00:03:57,049 --> 00:04:01,699
get hot right so you can you can get a

00:04:00,410 --> 00:04:02,870
lot of really interesting insight into

00:04:01,699 --> 00:04:04,760
how your application is performing we

00:04:02,870 --> 00:04:09,639
can look at our full stack see where

00:04:04,760 --> 00:04:13,430
we're spending a lot of time yay PHP

00:04:09,639 --> 00:04:15,680
actually a lot of this is is actually

00:04:13,430 --> 00:04:17,150
caching layer stuff so we have a very

00:04:15,680 --> 00:04:19,039
complex caching thing that I won't get

00:04:17,150 --> 00:04:20,959
into but we can kind of look and see

00:04:19,039 --> 00:04:25,310
where potential bottlenecks are in our

00:04:20,959 --> 00:04:30,020
stack this is all really important

00:04:25,310 --> 00:04:32,180
because perceived performance is vital

00:04:30,020 --> 00:04:34,250
so when users are interacting with your

00:04:32,180 --> 00:04:36,530
application anything under 100

00:04:34,250 --> 00:04:39,560
milliseconds is almost imperceptible to

00:04:36,530 --> 00:04:41,030
the to the human brain as you move up to

00:04:39,560 --> 00:04:42,650
about 300 milliseconds there's a small

00:04:41,030 --> 00:04:44,629
perceptible delay but it doesn't rarely

00:04:42,650 --> 00:04:48,740
register as a problem it's when you get

00:04:44,629 --> 00:04:52,009
to around a second that the brain starts

00:04:48,740 --> 00:04:53,509
to think that there is a that maybe it's

00:04:52,009 --> 00:04:57,379
not just the system working hard and

00:04:53,509 --> 00:04:58,819
actually that kind of delay area can

00:04:57,379 --> 00:05:01,639
sometimes be important so have you ever

00:04:58,819 --> 00:05:02,750
worked on an application where you maybe

00:05:01,639 --> 00:05:04,099
you've not worked on an application but

00:05:02,750 --> 00:05:06,880
it's not uncommon for applications to

00:05:04,099 --> 00:05:09,550
inject an artificial delay during log in

00:05:06,880 --> 00:05:13,699
when password hat and password

00:05:09,550 --> 00:05:16,400
negotiation is too quick right when you

00:05:13,699 --> 00:05:18,440
get past a second up to about 10,000 up

00:05:16,400 --> 00:05:20,419
to about 10 seconds the system is

00:05:18,440 --> 00:05:23,419
perceived as slow eventually broken and

00:05:20,419 --> 00:05:24,710
after 10 seconds generally suck at 10

00:05:23,419 --> 00:05:29,479
second response time generally speaking

00:05:24,710 --> 00:05:32,330
girl you'll have lost your user this can

00:05:29,479 --> 00:05:35,300
have a real impact on revenue so Amazon

00:05:32,330 --> 00:05:37,599
did a study in I'm trying to get my

00:05:35,300 --> 00:05:40,159
notes they're so so super tiny 2006

00:05:37,599 --> 00:05:42,289
where they looked at the effect of

00:05:40,159 --> 00:05:43,819
latency on sales they saw that with each

00:05:42,289 --> 00:05:45,050
hundred milliseconds of increased

00:05:43,819 --> 00:05:47,419
latency they saw about a one-percent

00:05:45,050 --> 00:05:48,800
drop-off in sales so I don't know what I

00:05:47,419 --> 00:05:52,159
don't know what Amazon makes every

00:05:48,800 --> 00:05:54,560
second probably two or three thousand

00:05:52,159 --> 00:05:57,349
dollars i would imagine in revenue that

00:05:54,560 --> 00:06:00,020
can be a huge amount of revenue lost

00:05:57,349 --> 00:06:03,099
right and for a company like Amazon that

00:06:00,020 --> 00:06:05,690
actually keeps you know stock like those

00:06:03,099 --> 00:06:07,340
margins are very very small for google

00:06:05,690 --> 00:06:09,800
they did a search experiment where they

00:06:07,340 --> 00:06:11,719
into the injected artificial latency

00:06:09,800 --> 00:06:14,389
into some of their search search

00:06:11,719 --> 00:06:18,190
responses and found that as the latency

00:06:14,389 --> 00:06:21,500
increased the number of total searches

00:06:18,190 --> 00:06:23,090
went down right so Google makes their

00:06:21,500 --> 00:06:25,759
money the same way that AOL makes our

00:06:23,090 --> 00:06:28,550
money right we put ads for you know

00:06:25,759 --> 00:06:30,020
Honda Civics on the internet and the

00:06:28,550 --> 00:06:31,310
more of those ads we put in front of

00:06:30,020 --> 00:06:32,990
your eyeballs the more money we make

00:06:31,310 --> 00:06:34,250
right so Google's in that same business

00:06:32,990 --> 00:06:36,620
so for us all about page impressions

00:06:34,250 --> 00:06:39,259
right so you can see that you can have a

00:06:36,620 --> 00:06:40,550
tremendous drop off in your revenue by

00:06:39,259 --> 00:06:41,870
having a very small I mean this is

00:06:40,550 --> 00:06:43,260
talking about you know additional

00:06:41,870 --> 00:06:45,090
milliseconds 150

00:06:43,260 --> 00:06:48,630
milliseconds of latency can actually

00:06:45,090 --> 00:06:50,040
have real bottom bottom line impact and

00:06:48,630 --> 00:06:51,720
then to go back to what I talked about a

00:06:50,040 --> 00:06:53,190
few minutes ago multivariate testing

00:06:51,720 --> 00:06:55,110
right this is where you take your users

00:06:53,190 --> 00:06:56,310
you sort them into groups right you have

00:06:55,110 --> 00:06:58,320
a control group and you have treatment

00:06:56,310 --> 00:07:00,630
groups and you give them different

00:06:58,320 --> 00:07:02,340
content and then you measure behavioral

00:07:00,630 --> 00:07:04,920
statistics like click-through rate and

00:07:02,340 --> 00:07:08,160
abandoned rate right to see how

00:07:04,920 --> 00:07:09,870
different types of content are received

00:07:08,160 --> 00:07:11,550
by the end user right so this is a way

00:07:09,870 --> 00:07:14,070
of looking at the performance of design

00:07:11,550 --> 00:07:20,550
elements or even the performance of

00:07:14,070 --> 00:07:21,780
content itself something that doesn't

00:07:20,550 --> 00:07:23,880
often get talked about when we talk

00:07:21,780 --> 00:07:26,160
about metrics state monitoring all right

00:07:23,880 --> 00:07:27,960
so logging so who has a distributed

00:07:26,160 --> 00:07:31,860
application but does not use centralized

00:07:27,960 --> 00:07:35,880
logging I'm glad I don't see any I see a

00:07:31,860 --> 00:07:38,400
couple hands it's vital to understand it

00:07:35,880 --> 00:07:40,260
is a vital aspect of performance monitor

00:07:38,400 --> 00:07:41,760
monitoring to understand what exceptions

00:07:40,260 --> 00:07:44,340
are being thrown in your application

00:07:41,760 --> 00:07:46,710
what is the general load at the at the

00:07:44,340 --> 00:07:48,240
individual web head right what is the

00:07:46,710 --> 00:07:49,740
system performance what is the cache

00:07:48,240 --> 00:07:51,600
performance so we get a lot of this out

00:07:49,740 --> 00:07:57,300
of having applications that log to a

00:07:51,600 --> 00:07:58,500
centralized logging repository and a big

00:07:57,300 --> 00:08:00,210
thing for us especially because we're in

00:07:58,500 --> 00:08:01,650
the advertising business and I'm going

00:08:00,210 --> 00:08:04,860
to try not to make it sound like we're

00:08:01,650 --> 00:08:07,740
too creepy just a little creepy a good

00:08:04,860 --> 00:08:08,760
amount of creepy traffic metrics so

00:08:07,740 --> 00:08:10,170
trying to understand the people that are

00:08:08,760 --> 00:08:14,370
interacting with our content like who

00:08:10,170 --> 00:08:15,750
they are what are their maybe what's

00:08:14,370 --> 00:08:18,000
their gender what's their age what's

00:08:15,750 --> 00:08:19,200
their income range are they more

00:08:18,000 --> 00:08:20,970
interested in seeing ads for Honda

00:08:19,200 --> 00:08:26,820
Civics or you know for nice Bentley

00:08:20,970 --> 00:08:28,950
right we want to look at the user

00:08:26,820 --> 00:08:30,360
behavior user behavior and experience so

00:08:28,950 --> 00:08:32,790
we want to understand like what is the

00:08:30,360 --> 00:08:36,300
user clicking on just off my microphone

00:08:32,790 --> 00:08:38,340
off what does the user clicking on where

00:08:36,300 --> 00:08:40,470
are they coming from where do they visit

00:08:38,340 --> 00:08:42,510
when they hit the site where is their

00:08:40,470 --> 00:08:43,890
mouths going right so we can click we

00:08:42,510 --> 00:08:45,780
can follow where their mouse is on the

00:08:43,890 --> 00:08:48,810
page we do a lot of really user

00:08:45,780 --> 00:08:50,520
monitoring so monitoring how long it

00:08:48,810 --> 00:08:53,190
takes for the JavaScript on the page to

00:08:50,520 --> 00:08:55,950
load how long do all the dns lookups

00:08:53,190 --> 00:08:56,600
take are we causing an excessive delay

00:08:55,950 --> 00:08:58,730
when

00:08:56,600 --> 00:09:01,579
users browsers are loading our content

00:08:58,730 --> 00:09:04,579
right how many of you have seen this

00:09:01,579 --> 00:09:07,370
this is the dreamscape multimedia did

00:09:04,579 --> 00:09:09,589
this I think in I don't remember it was

00:09:07,370 --> 00:09:10,790
awhile ago where they did this is

00:09:09,589 --> 00:09:12,529
actually not mouse tracking this is

00:09:10,790 --> 00:09:14,420
eyeball tracking right so they're

00:09:12,529 --> 00:09:16,220
tracking the where the eye is looking at

00:09:14,420 --> 00:09:21,380
the screen and the kind of found that

00:09:16,220 --> 00:09:23,990
this pattern that this pattern exposes

00:09:21,380 --> 00:09:25,519
itself over and over again we're sort of

00:09:23,990 --> 00:09:26,839
the eye is drawn to the upper left

00:09:25,519 --> 00:09:29,089
corner right so if you look at the way

00:09:26,839 --> 00:09:30,920
that Google kind of lays out there their

00:09:29,089 --> 00:09:32,300
page their search results which is I

00:09:30,920 --> 00:09:33,680
mean this is where google makes their

00:09:32,300 --> 00:09:37,190
money right this is there this is their

00:09:33,680 --> 00:09:38,540
money page there their most important

00:09:37,190 --> 00:09:40,790
stuff is right where the eyeball goes

00:09:38,540 --> 00:09:44,899
first all right so this is a way that we

00:09:40,790 --> 00:09:46,310
can use you know user monitoring to

00:09:44,899 --> 00:09:47,630
understand how we can improve the

00:09:46,310 --> 00:09:51,380
performance in this case the revenue

00:09:47,630 --> 00:09:53,120
performance of our applications and we

00:09:51,380 --> 00:09:55,190
do a lot of demographic collection so we

00:09:53,120 --> 00:09:57,560
collect information about like I said

00:09:55,190 --> 00:09:58,730
the that you know the an anonymized user

00:09:57,560 --> 00:10:01,040
information so we're trying to figure

00:09:58,730 --> 00:10:03,259
out who a user is not like who that

00:10:01,040 --> 00:10:05,120
person is like it's Joe Smith but like

00:10:03,259 --> 00:10:06,920
this is an individual human being and

00:10:05,120 --> 00:10:08,180
well maybe try and figure out some facts

00:10:06,920 --> 00:10:09,230
about that human being we're not really

00:10:08,180 --> 00:10:11,930
that interested in who that person

00:10:09,230 --> 00:10:13,189
really is what devices they're using

00:10:11,930 --> 00:10:15,050
what is peas they're using the

00:10:13,189 --> 00:10:16,430
geographic location that they're in if

00:10:15,050 --> 00:10:18,470
we can't get that at least the region

00:10:16,430 --> 00:10:23,029
are they in the UK are they in the EU or

00:10:18,470 --> 00:10:24,709
in the United States right so we do all

00:10:23,029 --> 00:10:26,750
this as part of something called the AOL

00:10:24,709 --> 00:10:27,589
media platform all right so I mentioned

00:10:26,750 --> 00:10:28,819
the before that we have a whole bunch of

00:10:27,589 --> 00:10:30,139
brands all the brands of this big

00:10:28,819 --> 00:10:32,209
multi-tenant system called the AL a

00:10:30,139 --> 00:10:35,600
media platform so it provides content

00:10:32,209 --> 00:10:37,220
management distributed rendering wow so

00:10:35,600 --> 00:10:40,459
this is a PHP stack sort of a couple

00:10:37,220 --> 00:10:42,079
hundred million page views a day we

00:10:40,459 --> 00:10:43,459
don't use any front side caching so

00:10:42,079 --> 00:10:45,319
every time you hit one of our properties

00:10:43,459 --> 00:10:48,250
you there's PHP being executed right so

00:10:45,319 --> 00:10:50,959
this big big massive rendering farm

00:10:48,250 --> 00:10:53,329
these websites are developed generally

00:10:50,959 --> 00:10:56,870
speaking using a custom dsl inside of an

00:10:53,329 --> 00:10:58,670
IDE that we develop and maintain we have

00:10:56,870 --> 00:10:59,930
a content aggregation platform we have a

00:10:58,670 --> 00:11:01,730
machine distributed meat machine

00:10:59,930 --> 00:11:03,290
learning platform so we we aggregate

00:11:01,730 --> 00:11:05,839
content from all of our brands and also

00:11:03,290 --> 00:11:07,040
from external partners and then we run

00:11:05,839 --> 00:11:08,600
it all through a machine learning

00:11:07,040 --> 00:11:10,100
platform to try to understand what it's

00:11:08,600 --> 00:11:12,390
talking about

00:11:10,100 --> 00:11:14,250
stuff like that and like I said this is

00:11:12,390 --> 00:11:15,269
a big multi-tenant system so we've lots

00:11:14,250 --> 00:11:19,470
and lots of brands living in there

00:11:15,269 --> 00:11:23,850
together at the same time I want to talk

00:11:19,470 --> 00:11:28,079
to you about basically what I was

00:11:23,850 --> 00:11:31,260
brought into a ll to work on right so

00:11:28,079 --> 00:11:33,269
when I started with AOL these are the

00:11:31,260 --> 00:11:34,890
four things that we had from metrics and

00:11:33,269 --> 00:11:37,500
analytics so we use omniture for revenue

00:11:34,890 --> 00:11:39,540
analytics New Relic for application

00:11:37,500 --> 00:11:41,370
performance monitoring we also have a

00:11:39,540 --> 00:11:42,899
lot of elk infrastructure is everybody

00:11:41,370 --> 00:11:45,540
familiar with elk elasticsearch log

00:11:42,899 --> 00:11:47,430
stash and cabana okay so it's kind of

00:11:45,540 --> 00:11:48,870
like an open-source way of collecting

00:11:47,430 --> 00:11:51,000
like open-source belonged if you're

00:11:48,870 --> 00:11:52,980
familiar with splunk and then we have a

00:11:51,000 --> 00:11:55,140
perp I a proprietary data platform

00:11:52,980 --> 00:11:56,160
called the called the data layer which

00:11:55,140 --> 00:11:57,959
is used for collecting rum and

00:11:56,160 --> 00:11:59,940
demographic traffic information right

00:11:57,959 --> 00:12:03,170
this is a massively distributed data

00:11:59,940 --> 00:12:07,800
collection network it's based on Hadoop

00:12:03,170 --> 00:12:09,180
so essentially we have a big giant data

00:12:07,800 --> 00:12:11,760
store where we just collect petabytes

00:12:09,180 --> 00:12:13,230
and petabytes of data we use Cassandra

00:12:11,760 --> 00:12:16,079
to slice some of that data off to make

00:12:13,230 --> 00:12:18,810
it more readily accessible to different

00:12:16,079 --> 00:12:21,390
consumers part of this is a vertica

00:12:18,810 --> 00:12:23,329
installation where we ingest a lot of

00:12:21,390 --> 00:12:27,300
data from Omniture and then sort of

00:12:23,329 --> 00:12:29,459
massage it to make it fit with the kind

00:12:27,300 --> 00:12:30,720
of reporting that we need and then

00:12:29,459 --> 00:12:32,610
there's also a streaming interface for

00:12:30,720 --> 00:12:36,209
raw data alright so it ends up looking

00:12:32,610 --> 00:12:37,649
like this at the very front we have

00:12:36,209 --> 00:12:40,290
these things called beacon servers and

00:12:37,649 --> 00:12:41,850
if you hit one of our websites you might

00:12:40,290 --> 00:12:44,160
see in few open like the chrome

00:12:41,850 --> 00:12:47,070
inspector connections to something

00:12:44,160 --> 00:12:49,920
called ba ll com or beaten gadget com

00:12:47,070 --> 00:12:51,029
these are the the beacon servers and

00:12:49,920 --> 00:12:52,949
these are just web servers that are

00:12:51,029 --> 00:12:55,980
sitting there connect their internet

00:12:52,949 --> 00:12:58,500
facing you throw JSON payloads at them

00:12:55,980 --> 00:13:00,779
and they push them onto a RabbitMQ but

00:12:58,500 --> 00:13:02,399
be there fairly simple they have some

00:13:00,779 --> 00:13:05,010
kind of filtering like to try and filter

00:13:02,399 --> 00:13:06,449
out garbage but they're fairly simple so

00:13:05,010 --> 00:13:08,279
all of that goes into a giant RabbitMQ

00:13:06,449 --> 00:13:11,040
farm and then from RabbitMQ there are

00:13:08,279 --> 00:13:13,829
services that pull that data out push it

00:13:11,040 --> 00:13:15,959
into couchbase push it into Cassandra

00:13:13,829 --> 00:13:18,660
push it into vertica everything gets

00:13:15,959 --> 00:13:20,220
logged into Hadoop and then all that

00:13:18,660 --> 00:13:22,589
stuff also gets pushed into a stream or

00:13:20,220 --> 00:13:23,160
farm so this is what what drives these

00:13:22,589 --> 00:13:28,050
data layers

00:13:23,160 --> 00:13:30,300
streamers right over here on the right

00:13:28,050 --> 00:13:31,860
side is a I said that raw data streaming

00:13:30,300 --> 00:13:33,540
interface so all that data that's coming

00:13:31,860 --> 00:13:37,050
into the be considered beacon servers up

00:13:33,540 --> 00:13:41,339
here comes out over here exactly as it

00:13:37,050 --> 00:13:42,480
came in the beacon payloads themself

00:13:41,339 --> 00:13:44,730
looks something like this now they're

00:13:42,480 --> 00:13:47,189
generally much larger than this but

00:13:44,730 --> 00:13:49,019
they're just JSON objects if you see

00:13:47,189 --> 00:13:50,180
them in your chrome inspector you can

00:13:49,019 --> 00:13:52,800
see what's in them there's nothing

00:13:50,180 --> 00:13:56,009
particularly interesting will capture

00:13:52,800 --> 00:13:59,850
the user agent this makes sense to us

00:13:56,009 --> 00:14:02,610
has a Content vertical kind of

00:13:59,850 --> 00:14:04,850
information everything gets assigned an

00:14:02,610 --> 00:14:08,490
anonymous ID if you if you allow cookies

00:14:04,850 --> 00:14:12,360
that will persist across different AOL

00:14:08,490 --> 00:14:14,459
properties we get the refer locate your

00:14:12,360 --> 00:14:16,079
geographic location if we can figure it

00:14:14,459 --> 00:14:17,660
out from your isp information stuff like

00:14:16,079 --> 00:14:20,100
that and then we can actually inject

00:14:17,660 --> 00:14:28,079
things like multivariate test payloads

00:14:20,100 --> 00:14:32,189
into here so we collected this

00:14:28,079 --> 00:14:33,779
tremendous amount of data each one of

00:14:32,189 --> 00:14:36,380
these these payloads which we call

00:14:33,779 --> 00:14:39,509
events contain about 40 different keys

00:14:36,380 --> 00:14:43,170
they're roughly about 1.6 kilobytes in

00:14:39,509 --> 00:14:46,589
size so they're fairly sizable we get

00:14:43,170 --> 00:14:49,380
15,000 of them a second writes about 25

00:14:46,589 --> 00:14:52,019
megabytes of data every second and in

00:14:49,380 --> 00:14:53,790
the course of a day we log about 1.3

00:14:52,019 --> 00:14:55,769
billion events or about 2 terabytes of

00:14:53,790 --> 00:14:58,829
data it's just a very large amount of

00:14:55,769 --> 00:15:00,509
data not very large I in the terms that

00:14:58,829 --> 00:15:06,059
in the maybe in the sense of the whole

00:15:00,509 --> 00:15:07,290
internet but for us pretty big so this

00:15:06,059 --> 00:15:08,730
is all great and all of the stuff goes

00:15:07,290 --> 00:15:10,800
into do but you can go into Hadoop and

00:15:08,730 --> 00:15:13,199
you can rob bread and I'm you can run

00:15:10,800 --> 00:15:14,699
hive jobs in Hadoop and like extract

00:15:13,199 --> 00:15:16,949
amazing amounts of data out of the

00:15:14,699 --> 00:15:19,529
system but it can take a long time so

00:15:16,949 --> 00:15:20,850
that's a time shared system we have to

00:15:19,529 --> 00:15:21,660
share that with other developers for

00:15:20,850 --> 00:15:23,819
trying to get at this information

00:15:21,660 --> 00:15:25,529
sometimes it takes a couple of hours to

00:15:23,819 --> 00:15:28,019
pull information out generally speaking

00:15:25,529 --> 00:15:29,670
from the time that data hits a beacon

00:15:28,019 --> 00:15:33,350
server until the time it's available in

00:15:29,670 --> 00:15:35,009
Hadoop is a couple of hours right so

00:15:33,350 --> 00:15:36,960
realistically speaking to get

00:15:35,009 --> 00:15:39,410
information if you go right now to

00:15:36,960 --> 00:15:42,240
to engadget com and click on something

00:15:39,410 --> 00:15:44,310
for that data to be available in a

00:15:42,240 --> 00:15:48,510
report coming out of Hadoop could be six

00:15:44,310 --> 00:15:50,940
hours right our editors don't care what

00:15:48,510 --> 00:15:52,350
happens six hours ago right they care

00:15:50,940 --> 00:15:55,470
about what's happening in the past hour

00:15:52,350 --> 00:15:56,820
two hours they want to know social

00:15:55,470 --> 00:16:02,070
engagement trends like who's tweeting

00:15:56,820 --> 00:16:05,730
about something right now they're very

00:16:02,070 --> 00:16:07,770
very time where every time oriented and

00:16:05,730 --> 00:16:09,360
then also we have to remember we have

00:16:07,770 --> 00:16:11,100
all of these developers who are our

00:16:09,360 --> 00:16:14,070
customers so we're not just developing a

00:16:11,100 --> 00:16:16,530
product for content creators we're

00:16:14,070 --> 00:16:17,700
developing a platform for for software

00:16:16,530 --> 00:16:20,190
developers right so they need to

00:16:17,700 --> 00:16:21,630
understand a lot of those application

00:16:20,190 --> 00:16:23,940
performance things that I talked about

00:16:21,630 --> 00:16:26,400
earlier how are their API queries

00:16:23,940 --> 00:16:29,610
performing are there is how is their

00:16:26,400 --> 00:16:31,800
cash rate cache hit rate what kind of

00:16:29,610 --> 00:16:33,180
exceptions is their code throwing right

00:16:31,800 --> 00:16:35,130
so they need to have this information to

00:16:33,180 --> 00:16:36,570
and they don't have the luxury of direct

00:16:35,130 --> 00:16:41,340
access to something like you relic or

00:16:36,570 --> 00:16:44,540
elk right we live in a 24-hour media

00:16:41,340 --> 00:16:46,650
cycle so you know media trends change

00:16:44,540 --> 00:16:48,720
hour to hour and it's very very

00:16:46,650 --> 00:16:52,700
important for us to be like the

00:16:48,720 --> 00:16:56,100
difference for for a breaking news story

00:16:52,700 --> 00:16:57,450
like if if some some information comes

00:16:56,100 --> 00:17:01,860
out about Miley Cyrus I will talk about

00:16:57,450 --> 00:17:03,330
Miley Cyrus so much I'm sorry if there's

00:17:01,860 --> 00:17:06,300
a breaking news story about Miley Cyrus

00:17:03,330 --> 00:17:09,030
if cambio calm gets to it first or or

00:17:06,300 --> 00:17:10,310
gawker com gets to it first whoever gets

00:17:09,030 --> 00:17:12,750
to it first is going to make the money

00:17:10,310 --> 00:17:17,630
from that story so it's important to be

00:17:12,750 --> 00:17:20,459
on top of these things so anyway I

00:17:17,630 --> 00:17:21,270
joined AOL because of this project I

00:17:20,459 --> 00:17:22,410
thought it was a really really

00:17:21,270 --> 00:17:23,940
interesting project and this is actually

00:17:22,410 --> 00:17:27,270
a unique experience for me and that in

00:17:23,940 --> 00:17:29,940
the weeks leading up to my kind of

00:17:27,270 --> 00:17:31,620
between my old job and the new job I was

00:17:29,940 --> 00:17:34,410
in daily communication with my new boss

00:17:31,620 --> 00:17:36,360
and we were planning and designing and

00:17:34,410 --> 00:17:37,650
and discussing kind of what this new

00:17:36,360 --> 00:17:40,350
platform would look like this real-time

00:17:37,650 --> 00:17:42,030
data platform would look like so i had

00:17:40,350 --> 00:17:44,550
the really cool experience of going to

00:17:42,030 --> 00:17:47,390
work on my very first day getting my

00:17:44,550 --> 00:17:50,100
software so i could sign it and then

00:17:47,390 --> 00:17:53,880
immediately starting to work

00:17:50,100 --> 00:17:55,380
on this project and the first question

00:17:53,880 --> 00:17:56,549
we want an answer was how would you let

00:17:55,380 --> 00:17:57,929
editors test how well different

00:17:56,549 --> 00:17:59,600
headlines perform for the same piece of

00:17:57,929 --> 00:18:04,710
content so that's that multivariate

00:17:59,600 --> 00:18:07,679
testing quite a question right so I want

00:18:04,710 --> 00:18:12,240
to tell you how we did this with just

00:18:07,679 --> 00:18:15,600
three easy failures started with a proof

00:18:12,240 --> 00:18:18,990
of concept so over here on the left side

00:18:15,600 --> 00:18:22,140
of the screen we have those streamers so

00:18:18,990 --> 00:18:25,159
those are the real time data layer

00:18:22,140 --> 00:18:25,159
streamers that I talked about earlier

00:18:25,490 --> 00:18:30,059
connecting to that we have a receiver so

00:18:28,230 --> 00:18:32,520
these things generate an HTTP stream you

00:18:30,059 --> 00:18:35,100
connect to them over HTTP and they just

00:18:32,520 --> 00:18:37,980
start sending you lying delimited JSON

00:18:35,100 --> 00:18:39,090
objects so you need something that's

00:18:37,980 --> 00:18:41,090
going to actually do that can make that

00:18:39,090 --> 00:18:43,350
connection and consume that data and

00:18:41,090 --> 00:18:44,700
then my initial thought was to push all

00:18:43,350 --> 00:18:47,669
of that into a stats d question it's

00:18:44,700 --> 00:18:49,980
familiar with stats d okay I see a lot

00:18:47,669 --> 00:18:52,559
of hands so stats t is Canada out of a

00:18:49,980 --> 00:18:53,820
etsy it's actually a really cool little

00:18:52,559 --> 00:18:55,679
tool it's very very simple node.js

00:18:53,820 --> 00:18:57,450
application that just runs collects data

00:18:55,679 --> 00:18:58,799
aggregates it in memory and then pushes

00:18:57,450 --> 00:19:00,030
it off to something else you usually

00:18:58,799 --> 00:19:01,740
graph I but you can push it to

00:19:00,030 --> 00:19:04,590
elasticsearch or a bunch of different

00:19:01,740 --> 00:19:06,000
backends and in this case I was pushing

00:19:04,590 --> 00:19:07,110
it into elasticsearch and I wanted to

00:19:06,000 --> 00:19:09,870
use elasticsearch because we already

00:19:07,110 --> 00:19:13,770
have a lot of familiarity with cabana

00:19:09,870 --> 00:19:17,190
right so cabana's the visualization tool

00:19:13,770 --> 00:19:18,539
that lives on top of elastic search from

00:19:17,190 --> 00:19:20,039
a logical perspective it kind of look

00:19:18,539 --> 00:19:23,280
like this so we have a streaming HTTP

00:19:20,039 --> 00:19:26,039
client this is built-in nodejs so it's

00:19:23,280 --> 00:19:28,350
using a native node stream we receive

00:19:26,039 --> 00:19:30,809
the data layer stream break it off at

00:19:28,350 --> 00:19:33,600
each line break create new records for

00:19:30,809 --> 00:19:35,909
each data layer event push that on to a

00:19:33,600 --> 00:19:37,230
message bus and then a bunch of event

00:19:35,909 --> 00:19:41,520
listeners that look for those events to

00:19:37,230 --> 00:19:43,169
get created and then edits them right so

00:19:41,520 --> 00:19:45,299
it extracts the information out of them

00:19:43,169 --> 00:19:46,590
that it wants and generate statistics

00:19:45,299 --> 00:19:51,870
objects which then get pushed out to

00:19:46,590 --> 00:19:54,690
stats d right there was one good idea

00:19:51,870 --> 00:19:57,059
here that we had and that was to kind of

00:19:54,690 --> 00:19:59,940
encapsulate this editing it's tiny

00:19:57,059 --> 00:20:02,010
little services that were shareable and

00:19:59,940 --> 00:20:03,420
reusable this is the only code in this

00:20:02,010 --> 00:20:05,610
entire system that

00:20:03,420 --> 00:20:08,940
that survived from prototype to

00:20:05,610 --> 00:20:13,800
production it's just parsing out a you a

00:20:08,940 --> 00:20:15,240
user agent but using this method in

00:20:13,800 --> 00:20:18,350
about a week I was able to produce

00:20:15,240 --> 00:20:20,490
something that was fairly usable in that

00:20:18,350 --> 00:20:23,160
so I took the screenshot on my seventh

00:20:20,490 --> 00:20:25,290
day in AOL where we could actually see

00:20:23,160 --> 00:20:27,150
on a map this is not a video

00:20:25,290 --> 00:20:29,300
unfortunately the different countries

00:20:27,150 --> 00:20:32,120
the darkness would change as there were

00:20:29,300 --> 00:20:34,170
the number of hits in the past second

00:20:32,120 --> 00:20:37,860
would change for each of those different

00:20:34,170 --> 00:20:39,030
countries right so if nothing else it

00:20:37,860 --> 00:20:41,130
proved that we could do something

00:20:39,030 --> 00:20:48,150
somewhat meaningful with this stream of

00:20:41,130 --> 00:20:50,610
data right so the proof concept while it

00:20:48,150 --> 00:20:52,680
did work we were only able to consume

00:20:50,610 --> 00:20:53,970
300 messages a second so remember we're

00:20:52,680 --> 00:20:55,500
talking about a total of five hundred

00:20:53,970 --> 00:20:59,190
fifteen thousand messages a second

00:20:55,500 --> 00:21:02,190
sometimes that spikes up to 20 so we

00:20:59,190 --> 00:21:03,960
would have needed around 70 receivers in

00:21:02,190 --> 00:21:06,870
order to do this which is an unwieldy

00:21:03,960 --> 00:21:07,980
number of receivers and then stats t

00:21:06,870 --> 00:21:09,930
actually imposed a bunch of other

00:21:07,980 --> 00:21:12,270
limitations so we have these very rich

00:21:09,930 --> 00:21:16,620
payloads and these payloads will tell us

00:21:12,270 --> 00:21:19,410
that someone in into in Denton Texas on

00:21:16,620 --> 00:21:21,000
an iPad looked at this content what we

00:21:19,410 --> 00:21:22,650
see once it gets broken up into stats T

00:21:21,000 --> 00:21:24,180
is that someone in Denton Texas looked

00:21:22,650 --> 00:21:25,590
at this content and someone on my iPad

00:21:24,180 --> 00:21:31,740
looked at this content we lose that

00:21:25,590 --> 00:21:33,060
connection between those data points it

00:21:31,740 --> 00:21:34,860
was around this time that I realized

00:21:33,060 --> 00:21:36,510
that in order to build an efficient

00:21:34,860 --> 00:21:38,430
real-time data pathway you had to have a

00:21:36,510 --> 00:21:39,960
network transits and terminals this is

00:21:38,430 --> 00:21:43,260
not any kind of real terminology this is

00:21:39,960 --> 00:21:45,870
BS that I made up the idea here being

00:21:43,260 --> 00:21:47,700
that you have no single know that acts

00:21:45,870 --> 00:21:50,940
both as its transit and a terminal at

00:21:47,700 --> 00:21:53,930
the same time that is to say transits

00:21:50,940 --> 00:21:56,550
are short-term volatile or data lives

00:21:53,930 --> 00:21:59,780
date with a very short lifespan while

00:21:56,550 --> 00:22:02,010
it's in route to something else and

00:21:59,780 --> 00:22:07,190
terminals are where data goes to either

00:22:02,010 --> 00:22:07,190
be destroyed stored or retransmitted

00:22:09,170 --> 00:22:14,960
so the first thing that we needed to do

00:22:10,940 --> 00:22:17,180
was replace that's D as a transit right

00:22:14,960 --> 00:22:19,070
so two of the tools that we looked at

00:22:17,180 --> 00:22:21,740
work Kafka and RabbitMQ so who's

00:22:19,070 --> 00:22:24,350
familiar with Kafka and how about

00:22:21,740 --> 00:22:25,790
RabbitMQ okay so a lot more people

00:22:24,350 --> 00:22:29,510
continue it familiar with RabbitMQ than

00:22:25,790 --> 00:22:31,850
Kafka i'll talk about Kafka first so

00:22:29,510 --> 00:22:33,740
Kafka is a pub sub message broker it

00:22:31,850 --> 00:22:38,450
came out of LinkedIn a number of years

00:22:33,740 --> 00:22:39,890
ago it's an Apache project now it's very

00:22:38,450 --> 00:22:41,720
similar to rabbit and cue in that it's

00:22:39,890 --> 00:22:44,030
it's a you know a publish and subscribe

00:22:41,720 --> 00:22:45,530
message broker but it's very focused on

00:22:44,030 --> 00:22:49,520
message integrity so it's persistent

00:22:45,530 --> 00:22:50,720
first message order is preserved so you

00:22:49,520 --> 00:22:53,450
get messages out of it and the same

00:22:50,720 --> 00:22:59,810
order they go in and it's highly fault

00:22:53,450 --> 00:23:00,860
tolerant rabbitmq on the other hand as a

00:22:59,810 --> 00:23:02,630
little bit more mature it's been around

00:23:00,860 --> 00:23:06,560
since 2007 it's kind of considered the

00:23:02,630 --> 00:23:09,380
de facto amqp implementation it's much

00:23:06,560 --> 00:23:12,230
more general purpose it allows routing

00:23:09,380 --> 00:23:13,910
you don't necessarily have a guarantee

00:23:12,230 --> 00:23:18,620
of message order integrity although you

00:23:13,910 --> 00:23:20,180
can you can get that it uses Federation

00:23:18,620 --> 00:23:23,960
for high availability as opposed to

00:23:20,180 --> 00:23:26,110
persistence and sort of sharding so when

00:23:23,960 --> 00:23:28,160
you think about our our requirements

00:23:26,110 --> 00:23:29,420
we're getting these payloads they're

00:23:28,160 --> 00:23:31,220
very rich payloads and they're

00:23:29,420 --> 00:23:34,040
time-stamped so it doesn't matter what

00:23:31,220 --> 00:23:34,970
order we receive them in when we analyze

00:23:34,040 --> 00:23:36,680
them they're going to be analyzed

00:23:34,970 --> 00:23:38,510
they're going to be if we like do a

00:23:36,680 --> 00:23:40,760
histogram obviously we're going to use

00:23:38,510 --> 00:23:44,600
the timestamp anyway so the order

00:23:40,760 --> 00:23:46,220
doesn't matter some data loss is

00:23:44,600 --> 00:23:47,900
acceptable right we're collecting 1.3

00:23:46,220 --> 00:23:49,100
billion events every day if we lose ten

00:23:47,900 --> 00:23:51,710
percent of that we still have a

00:23:49,100 --> 00:23:57,470
significant sample size to do analytics

00:23:51,710 --> 00:23:59,780
against we have different consumers that

00:23:57,470 --> 00:24:02,930
may want small subsets of the data right

00:23:59,780 --> 00:24:05,150
so being able to take data and and route

00:24:02,930 --> 00:24:06,440
it based on different parameters is

00:24:05,150 --> 00:24:10,370
something that would be very very useful

00:24:06,440 --> 00:24:11,600
for us we need to have a broad support

00:24:10,370 --> 00:24:13,580
for different languages right so

00:24:11,600 --> 00:24:16,810
generally speaking our big three

00:24:13,580 --> 00:24:19,250
languages at ARL or Java Python and PHP

00:24:16,810 --> 00:24:20,900
we have a lot of know Jas lately too so

00:24:19,250 --> 00:24:21,670
having support for those languages would

00:24:20,900 --> 00:24:25,150
be really really

00:24:21,670 --> 00:24:27,070
for us and we need to route data between

00:24:25,150 --> 00:24:28,390
lots of different data centers and like

00:24:27,070 --> 00:24:30,040
we're right in the process right now

00:24:28,390 --> 00:24:31,540
closing our physical data centers down

00:24:30,040 --> 00:24:33,520
and moving everything to AWS we've got

00:24:31,540 --> 00:24:34,930
like one foot in each world so we're

00:24:33,520 --> 00:24:36,760
constantly like shuttling data back and

00:24:34,930 --> 00:24:39,070
forth over this network bound reasons

00:24:36,760 --> 00:24:43,300
it's every bit as horrifying as you're

00:24:39,070 --> 00:24:45,400
imagining right so I ended up choosing

00:24:43,300 --> 00:24:46,960
rabbitmq right because the priorities

00:24:45,400 --> 00:24:48,550
were very similar to mine rat I had the

00:24:46,960 --> 00:24:50,020
Federation over at least once delivery I

00:24:48,550 --> 00:24:52,540
don't care if I lose a couple messages

00:24:50,020 --> 00:24:57,040
it's not a big deal I can route messages

00:24:52,540 --> 00:24:58,900
I Confederate over network boundaries

00:24:57,040 --> 00:25:01,750
even though like you really shouldn't do

00:24:58,900 --> 00:25:03,550
this you can especially if you're if

00:25:01,750 --> 00:25:07,660
you're routing very specific chains of

00:25:03,550 --> 00:25:11,580
messages across very small volume there

00:25:07,660 --> 00:25:14,890
were mature clients for all of our major

00:25:11,580 --> 00:25:17,170
application stacks right and obviously

00:25:14,890 --> 00:25:20,500
pivotal's behind it so that makes it

00:25:17,170 --> 00:25:21,760
attractive to a big companies right so

00:25:20,500 --> 00:25:25,200
that was the first decision so we just I

00:25:21,760 --> 00:25:27,310
decided to replace stats d with RabbitMQ

00:25:25,200 --> 00:25:29,800
right so now we have the data layer

00:25:27,310 --> 00:25:32,890
streamer the receiver the node.js

00:25:29,800 --> 00:25:36,570
application pushing into RabbitMQ and

00:25:32,890 --> 00:25:36,570
then elasticsearch on the other side

00:25:39,180 --> 00:25:44,950
made a couple of other changes in

00:25:41,290 --> 00:25:46,060
version one I moved away from the

00:25:44,950 --> 00:25:49,390
observer pattern so if you remember

00:25:46,060 --> 00:25:52,090
before I showed you each time a message

00:25:49,390 --> 00:25:52,900
came in we would generate an event and

00:25:52,090 --> 00:25:54,190
there would be a bunch of event

00:25:52,900 --> 00:25:56,560
listeners that would pick that that

00:25:54,190 --> 00:25:58,990
event up extract data out of it and

00:25:56,560 --> 00:26:02,470
generate a stat record moved away from

00:25:58,990 --> 00:26:03,910
that why so no Jas is really really

00:26:02,470 --> 00:26:07,240
great at handling events it's a very

00:26:03,910 --> 00:26:09,190
very fast engine but when you're

00:26:07,240 --> 00:26:11,860
generating a tremendous number of events

00:26:09,190 --> 00:26:16,120
all at the same time you can cause a lot

00:26:11,860 --> 00:26:18,190
of memory blow right so while that's

00:26:16,120 --> 00:26:19,690
tunable and like if you go if you go to

00:26:18,190 --> 00:26:20,950
the J's community and like say hey I got

00:26:19,690 --> 00:26:22,330
this problem with like I'm generating

00:26:20,950 --> 00:26:23,980
too many messages like well you can

00:26:22,330 --> 00:26:25,120
write a native module and you like okay

00:26:23,980 --> 00:26:28,300
you can you can't get around this

00:26:25,120 --> 00:26:29,800
limitations but rather than doing that I

00:26:28,300 --> 00:26:31,840
wanted to just use the backpressure

00:26:29,800 --> 00:26:34,720
method mechanisms in the native node

00:26:31,840 --> 00:26:36,039
streams to sort of regulate

00:26:34,720 --> 00:26:39,130
the input rate so it ended up looking

00:26:36,039 --> 00:26:40,299
kind of like this we have streaming HTTP

00:26:39,130 --> 00:26:42,210
client just like before we're still

00:26:40,299 --> 00:26:44,559
breaking apart messages as they come in

00:26:42,210 --> 00:26:47,559
now they go on to a process chain and

00:26:44,559 --> 00:26:49,840
each one takes the record performs an

00:26:47,559 --> 00:26:51,340
edit to it and then passes it on and

00:26:49,840 --> 00:26:52,480
then we get this enriched edited record

00:26:51,340 --> 00:26:57,640
that comes out the other side and goes

00:26:52,480 --> 00:27:00,039
in to grab at mq right so this was fine

00:26:57,640 --> 00:27:01,360
this work great you do start running

00:27:00,039 --> 00:27:03,520
into weird things where you're kind of

00:27:01,360 --> 00:27:05,650
like looking at operations like I need

00:27:03,520 --> 00:27:07,809
to pull the first element off of an

00:27:05,650 --> 00:27:12,610
array in a tight loop which one is

00:27:07,809 --> 00:27:14,080
faster right and just as a general rule

00:27:12,610 --> 00:27:16,500
in software engineering when you find

00:27:14,080 --> 00:27:19,000
yourself kind of thinking about this

00:27:16,500 --> 00:27:21,000
either you're a systems programmer or

00:27:19,000 --> 00:27:23,620
you've done something horribly wrong

00:27:21,000 --> 00:27:24,970
this is a this is this is am I telling

00:27:23,620 --> 00:27:30,610
this should be the picture next to micro

00:27:24,970 --> 00:27:32,320
optimization in the in the dictionary so

00:27:30,610 --> 00:27:34,720
very the one I've actually doubled the

00:27:32,320 --> 00:27:37,120
performance 600 message is a second

00:27:34,720 --> 00:27:38,620
inversion one which means we cut the

00:27:37,120 --> 00:27:42,789
number of receivers down to 35 which is

00:27:38,620 --> 00:27:45,940
too bad but two main problems having to

00:27:42,789 --> 00:27:49,150
do this micro optimization of code was

00:27:45,940 --> 00:27:50,890
getting really really sketchy and also

00:27:49,150 --> 00:27:52,299
was having to constantly add code to

00:27:50,890 --> 00:27:55,780
handle weird edge cases like we would

00:27:52,299 --> 00:27:58,750
get weird data especially from from some

00:27:55,780 --> 00:27:59,950
of our users in east asia where

00:27:58,750 --> 00:28:01,270
sometimes like people with older

00:27:59,950 --> 00:28:03,340
computers they're sending data that's

00:28:01,270 --> 00:28:06,120
encoded strangely we're not strangely

00:28:03,340 --> 00:28:08,380
but just encoded in non UTF formats and

00:28:06,120 --> 00:28:10,539
it would I mean it literally just cause

00:28:08,380 --> 00:28:12,159
node to crash because there would be

00:28:10,539 --> 00:28:13,960
like something that node thought was it

00:28:12,159 --> 00:28:17,260
no and they would just we would fall

00:28:13,960 --> 00:28:19,299
over so constantly having to like like

00:28:17,260 --> 00:28:23,710
write code to handle these weird little

00:28:19,299 --> 00:28:25,630
edge cases so like I said when you're

00:28:23,710 --> 00:28:26,799
when you're having to constantly deal

00:28:25,630 --> 00:28:28,270
with good edge cases and micro

00:28:26,799 --> 00:28:29,830
optimizations you know as an engineer

00:28:28,270 --> 00:28:31,210
that you've done something horribly

00:28:29,830 --> 00:28:36,330
horribly wrong so you have to kind of

00:28:31,210 --> 00:28:40,570
reevaluate and I want to point out that

00:28:36,330 --> 00:28:41,860
me not knowing how to use node to its

00:28:40,570 --> 00:28:43,690
fullest extent doesn't mean there's

00:28:41,860 --> 00:28:45,039
anything wrong with note and i mean i

00:28:43,690 --> 00:28:47,260
realized i don't have to defend node to

00:28:45,039 --> 00:28:49,740
you guys because you're mostly mostly

00:28:47,260 --> 00:28:53,170
you are PHP developers

00:28:49,740 --> 00:28:54,970
sometimes it's very easy to dismiss a

00:28:53,170 --> 00:28:57,280
tool i think this happens a lot to PHP

00:28:54,970 --> 00:28:59,620
right like people like say uh PHP's

00:28:57,280 --> 00:29:01,030
garbage because their experience with

00:28:59,620 --> 00:29:02,110
PHP is 10 years out of date right

00:29:01,030 --> 00:29:06,970
doesn't necessarily mean the tool itself

00:29:02,110 --> 00:29:08,080
is broken so throughout this time that I

00:29:06,970 --> 00:29:12,150
realize I needed to get very very

00:29:08,080 --> 00:29:12,150
serious about what I was building

00:29:13,440 --> 00:29:18,100
receiving data editing it and routing it

00:29:16,030 --> 00:29:19,809
all on the same step violates my transit

00:29:18,100 --> 00:29:24,910
in terminal separation policy right so

00:29:19,809 --> 00:29:30,940
I'm doing that over here right this

00:29:24,910 --> 00:29:32,799
guy's doing too many things the receiver

00:29:30,940 --> 00:29:35,650
should be simple should just be a

00:29:32,799 --> 00:29:38,110
transit that consumes data off of the

00:29:35,650 --> 00:29:40,240
high-pressure RabbitMQ are the high

00:29:38,110 --> 00:29:42,669
pressure data streamer and pushes it

00:29:40,240 --> 00:29:47,380
into the low pressure RabbitMQ farm that

00:29:42,669 --> 00:29:49,360
we control right it would be really nice

00:29:47,380 --> 00:29:51,940
to build this in something that has

00:29:49,360 --> 00:29:56,410
static and dynamic optimization a nice

00:29:51,940 --> 00:29:57,549
multiprocessing model because would be

00:29:56,410 --> 00:29:59,350
much more efficient if we could do this

00:29:57,549 --> 00:30:02,650
sort of in-memory edits in a number of

00:29:59,350 --> 00:30:04,510
multiple threads and something would

00:30:02,650 --> 00:30:08,100
that's good at managing like very large

00:30:04,510 --> 00:30:10,210
very volatile data sets in memory right

00:30:08,100 --> 00:30:11,500
so I had a different number of different

00:30:10,210 --> 00:30:13,480
choices I could go with obvious I could

00:30:11,500 --> 00:30:17,169
stick with nodejs which is very simple

00:30:13,480 --> 00:30:18,460
and fast and that we have a lot of

00:30:17,169 --> 00:30:20,530
institutional knowledge about you know

00:30:18,460 --> 00:30:23,350
deploying and distributing we could use

00:30:20,530 --> 00:30:25,000
Go Go is really attractive for this

00:30:23,350 --> 00:30:26,140
because of its native concurrency model

00:30:25,000 --> 00:30:27,850
right so if you're not familiar with go

00:30:26,140 --> 00:30:31,600
like it's very very simple to do like

00:30:27,850 --> 00:30:33,640
very lightweight threads and go a good

00:30:31,600 --> 00:30:35,290
memory management it compiles FAFSA for

00:30:33,640 --> 00:30:39,990
a compiled language like go is really

00:30:35,290 --> 00:30:43,600
really very interactive goodies rust

00:30:39,990 --> 00:30:45,059
could use Java Java has got a very very

00:30:43,600 --> 00:30:47,260
powerful static and dynamic optimization

00:30:45,059 --> 00:30:50,020
it's got really great memory management

00:30:47,260 --> 00:30:52,480
it's got multithreading especially if

00:30:50,020 --> 00:30:55,000
you use libraries we could use C or C++

00:30:52,480 --> 00:30:56,620
also really really fast there's good

00:30:55,000 --> 00:31:00,210
libraries for handling concurrency and

00:30:56,620 --> 00:31:00,210
for handling memory management

00:31:00,290 --> 00:31:04,370
a lot of downsides though for nodejs I

00:31:02,840 --> 00:31:08,660
just need too many instances to manage

00:31:04,370 --> 00:31:13,280
production flow without hyper tuning the

00:31:08,660 --> 00:31:15,200
the v8 engine right while I was really

00:31:13,280 --> 00:31:16,490
pumped to use go and some of the people

00:31:15,200 --> 00:31:19,250
on my team were like yeah that would be

00:31:16,490 --> 00:31:21,860
fun it's kind of selfish to like I want

00:31:19,250 --> 00:31:24,950
to build this and go and then I'll be

00:31:21,860 --> 00:31:26,930
the only person who can support it rust

00:31:24,950 --> 00:31:29,360
very very similar except no one was like

00:31:26,930 --> 00:31:34,010
I said hey who wants to learn rust and

00:31:29,360 --> 00:31:35,540
just cricket chirping I know I cuz so I

00:31:34,010 --> 00:31:36,770
speak a lot of PHP conferences I know if

00:31:35,540 --> 00:31:38,930
I did this in Java I was going to be fun

00:31:36,770 --> 00:31:44,660
of me and I just don't hate myself

00:31:38,930 --> 00:31:48,940
enough to do it and see I think it's

00:31:44,660 --> 00:31:52,030
really important as a software architect

00:31:48,940 --> 00:31:54,260
to understand other people's vision

00:31:52,030 --> 00:31:56,240
before expressing your own I think like

00:31:54,260 --> 00:31:58,730
kind of like growing into that role

00:31:56,240 --> 00:32:03,650
means learning to understand how other

00:31:58,730 --> 00:32:05,150
people see a solution to a problem when

00:32:03,650 --> 00:32:06,440
I first started this project and I was

00:32:05,150 --> 00:32:07,070
talking to the data layer team so

00:32:06,440 --> 00:32:08,450
they're the people that manage

00:32:07,070 --> 00:32:11,030
everything to the left of those

00:32:08,450 --> 00:32:12,050
streamers in that early diagram they

00:32:11,030 --> 00:32:14,480
said you know what you're probably going

00:32:12,050 --> 00:32:17,210
to want to build this in Java and I said

00:32:14,480 --> 00:32:18,770
no it's 2014 people don't build things

00:32:17,210 --> 00:32:21,410
in Java anymore I don't smell my right

00:32:18,770 --> 00:32:24,100
my life writing giant XML files it's an

00:32:21,410 --> 00:32:34,760
oj s world you know they beget with it

00:32:24,100 --> 00:32:36,830
yeah I use Java so version 2 same

00:32:34,760 --> 00:32:40,850
streamers before receiver now written in

00:32:36,830 --> 00:32:42,290
Java rabbitmq a new component called the

00:32:40,850 --> 00:32:43,790
process of router on this side so the

00:32:42,290 --> 00:32:46,700
process of router is essentially all of

00:32:43,790 --> 00:32:48,590
that nodejs logic that does the editing

00:32:46,700 --> 00:32:51,500
of records the massaging of data

00:32:48,590 --> 00:32:53,570
extraction of data encapsulated in its

00:32:51,500 --> 00:32:55,640
own processes over on the other side so

00:32:53,570 --> 00:32:57,620
I what I guess what I'm trying to point

00:32:55,640 --> 00:32:59,810
out here is that this is a very high

00:32:57,620 --> 00:33:03,830
pressure connection like there's this is

00:32:59,810 --> 00:33:07,040
we can't back that up that is a fire

00:33:03,830 --> 00:33:08,870
hose over here this is a very low

00:33:07,040 --> 00:33:10,400
pressure connection we can drop records

00:33:08,870 --> 00:33:12,960
out of this if we get backed up and drop

00:33:10,400 --> 00:33:16,640
some records it's fine no big deal

00:33:12,960 --> 00:33:20,130
so having the the nodejs server

00:33:16,640 --> 00:33:21,720
processes over there we don't run into

00:33:20,130 --> 00:33:24,890
that same performance issue of trying to

00:33:21,720 --> 00:33:27,929
keep up with this massive stream of data

00:33:24,890 --> 00:33:29,730
and the Java itself who hasn't worked in

00:33:27,929 --> 00:33:34,260
Java ever or hasn't worked in Java in a

00:33:29,730 --> 00:33:36,690
long time yeah so job is actually not

00:33:34,260 --> 00:33:40,039
that bad actually looks a lot like PHP i

00:33:36,690 --> 00:33:42,330
should say PHP looks a lot like Java

00:33:40,039 --> 00:33:45,450
this application is written in very very

00:33:42,330 --> 00:33:47,190
bare-bones Java no frameworks simple

00:33:45,450 --> 00:33:52,200
route wrapper around a native Java inch

00:33:47,190 --> 00:33:53,640
dreamer create a bunch of threads each

00:33:52,200 --> 00:33:57,179
thread maintains a connection to

00:33:53,640 --> 00:33:59,520
rabbitmq we have a very very simple pass

00:33:57,179 --> 00:34:01,919
through so we just basically read the

00:33:59,520 --> 00:34:04,260
record off the stream attempt to parson

00:34:01,919 --> 00:34:07,919
is JSON if it parses we push it over the

00:34:04,260 --> 00:34:09,450
rabbit the rabbit very very simple I

00:34:07,919 --> 00:34:12,270
described it as a bunch of ketchup

00:34:09,450 --> 00:34:16,200
bottles we have net work input a bunch

00:34:12,270 --> 00:34:19,940
of cues and then network output so it's

00:34:16,200 --> 00:34:21,960
a very very minimalistic application

00:34:19,940 --> 00:34:24,060
from a logical perspective it looks

00:34:21,960 --> 00:34:25,589
looks very very similar to what it did

00:34:24,060 --> 00:34:29,639
before so we have a streaming HTTP

00:34:25,589 --> 00:34:31,379
client we cut records at new lines push

00:34:29,639 --> 00:34:34,470
it onto the work queue worker threads

00:34:31,379 --> 00:34:36,119
pick them up and generate the wrong send

00:34:34,470 --> 00:34:43,020
the rail records onto amqp if they're

00:34:36,119 --> 00:34:44,310
valid JSON so I expected to get some

00:34:43,020 --> 00:34:48,149
performance benefit out of this new

00:34:44,310 --> 00:34:49,950
version and I did that it's about 2,600

00:34:48,149 --> 00:34:51,929
records a second which means I only

00:34:49,950 --> 00:34:53,040
needed would need about 10 receivers in

00:34:51,929 --> 00:34:57,420
production and I in production now I

00:34:53,040 --> 00:34:59,700
have 12 it's pretty close validity

00:34:57,420 --> 00:35:01,770
filtering is almost free in the Java

00:34:59,700 --> 00:35:04,200
receiver if it can't parse JSON we just

00:35:01,770 --> 00:35:05,970
drop it right and then the processor the

00:35:04,200 --> 00:35:07,440
router service can select only the

00:35:05,970 --> 00:35:09,270
messages that it wants everything else

00:35:07,440 --> 00:35:13,349
gets dropped on the floor or left for

00:35:09,270 --> 00:35:16,320
another service to collect right so huge

00:35:13,349 --> 00:35:18,420
win right we're getting that data out of

00:35:16,320 --> 00:35:21,150
this massive fire hose and into a pool

00:35:18,420 --> 00:35:22,800
where we can use it but this is where it

00:35:21,150 --> 00:35:24,359
just starts because without consumers

00:35:22,800 --> 00:35:26,190
this pipeline is useless we have all

00:35:24,359 --> 00:35:28,050
this data now flowing through RabbitMQ

00:35:26,190 --> 00:35:29,119
we going to do with it so I want to talk

00:35:28,050 --> 00:35:33,150
to you about some of the applications

00:35:29,119 --> 00:35:36,690
that we're building on top of this

00:35:33,150 --> 00:35:38,280
pipeline first thing is our real-time

00:35:36,690 --> 00:35:41,819
analytics service so we want to do math

00:35:38,280 --> 00:35:46,230
at all this data I guess i should say

00:35:41,819 --> 00:35:48,060
mass it just sounds so weird so the

00:35:46,230 --> 00:35:51,210
goals here provide real-time statistics

00:35:48,060 --> 00:35:52,349
metrics and analytics for the editorial

00:35:51,210 --> 00:35:53,910
staff so these are the people for the

00:35:52,349 --> 00:35:55,380
secured mostly towards the people who

00:35:53,910 --> 00:35:58,220
are generating content they're writing

00:35:55,380 --> 00:36:00,750
stories we want to allow them to do a

00:35:58,220 --> 00:36:03,810
statistical evaluation of arbitrary

00:36:00,750 --> 00:36:05,460
variables right and we want to provide a

00:36:03,810 --> 00:36:06,990
simple interface for the developers who

00:36:05,460 --> 00:36:09,569
are working in the publishing stack so

00:36:06,990 --> 00:36:13,380
these are PHP developers so we want to

00:36:09,569 --> 00:36:15,180
provide a an interface that lets them

00:36:13,380 --> 00:36:18,060
get at this data without having to go

00:36:15,180 --> 00:36:19,260
through too many hoops right so you know

00:36:18,060 --> 00:36:21,200
it's been kind of like hanging on the

00:36:19,260 --> 00:36:24,390
right side of the screen all day

00:36:21,200 --> 00:36:27,300
elasticsearch that big bubble who's

00:36:24,390 --> 00:36:29,520
familiar with elastic search awesome i

00:36:27,300 --> 00:36:32,190
love elasticsearch i give like 18,000

00:36:29,520 --> 00:36:35,849
talks about elasticsearch i kind of want

00:36:32,190 --> 00:36:38,280
to marry it elasticsearch has a high

00:36:35,849 --> 00:36:39,810
performance no SQL documents store right

00:36:38,280 --> 00:36:42,750
that has it does a high-availability

00:36:39,810 --> 00:36:45,119
Vidya intelligent clustering it's like

00:36:42,750 --> 00:36:46,920
got rack awareness data center awareness

00:36:45,119 --> 00:36:51,390
built-in it's got a really really

00:36:46,920 --> 00:36:52,560
powerful query DSL I guess it also does

00:36:51,390 --> 00:36:55,050
some full-text searching or something I

00:36:52,560 --> 00:36:58,920
don't know it's a very very powerful

00:36:55,050 --> 00:37:00,300
analytics engine it clusters out really

00:36:58,920 --> 00:37:02,569
really nicely so remember before I was

00:37:00,300 --> 00:37:04,800
saying that one of our big issues is

00:37:02,569 --> 00:37:07,470
moving data around between all of these

00:37:04,800 --> 00:37:10,349
different data centers and AWS right so

00:37:07,470 --> 00:37:12,869
we can have clusters that live in AWS we

00:37:10,349 --> 00:37:18,599
can ask them that live in elle's us data

00:37:12,869 --> 00:37:20,400
centers in our EU data centers it does

00:37:18,599 --> 00:37:23,780
replication and it does elastic

00:37:20,400 --> 00:37:27,319
replication and so if you decide to add

00:37:23,780 --> 00:37:29,579
nodes to a cluster it will rebalance the

00:37:27,319 --> 00:37:32,040
location of these individual shards

00:37:29,579 --> 00:37:38,440
right so it's a very very powerful very

00:37:32,040 --> 00:37:40,850
easy to to scale out data store

00:37:38,440 --> 00:37:43,250
for search applications it's got a

00:37:40,850 --> 00:37:45,370
really simple dsl right so this is uh

00:37:43,250 --> 00:37:49,520
this is actual real code from from

00:37:45,370 --> 00:37:51,350
there's Miley again hey Miley from one

00:37:49,520 --> 00:37:53,480
of our sites cambio calm which is a site

00:37:51,350 --> 00:37:57,800
a US based site that's a kind of geared

00:37:53,480 --> 00:38:00,620
towards teenage teenage girls analytics

00:37:57,800 --> 00:38:03,410
queries are a little bit more complex so

00:38:00,620 --> 00:38:05,630
we do things like multi-level

00:38:03,410 --> 00:38:09,340
aggregations so in here what we're doing

00:38:05,630 --> 00:38:12,890
is we're searching for specific types of

00:38:09,340 --> 00:38:14,180
events that happen in specific post so

00:38:12,890 --> 00:38:15,710
these are referring to individual

00:38:14,180 --> 00:38:19,940
articles that live in the system and

00:38:15,710 --> 00:38:23,200
then we're extracting out what search

00:38:19,940 --> 00:38:27,920
referrals led to those individual

00:38:23,200 --> 00:38:29,510
articles and then or should say we're

00:38:27,920 --> 00:38:30,920
parsing out the actual referrals and

00:38:29,510 --> 00:38:33,760
then making a decision if it is a search

00:38:30,920 --> 00:38:36,140
refer trying to get the search terms

00:38:33,760 --> 00:38:38,990
back out right so we can do these

00:38:36,140 --> 00:38:42,350
multi-level aggregations this stuff lets

00:38:38,990 --> 00:38:43,640
us build dynamic interfaces inside of

00:38:42,350 --> 00:38:45,050
our CMS that look like this so this

00:38:43,640 --> 00:38:48,710
comes from engadget I took the

00:38:45,050 --> 00:38:49,880
screenshot a couple of months ago where

00:38:48,710 --> 00:38:51,800
they can come into our into their

00:38:49,880 --> 00:38:56,540
dashboard and they see these numbers

00:38:51,800 --> 00:39:00,710
just scrolling live showing the the page

00:38:56,540 --> 00:39:01,940
views and then for us internally these

00:39:00,710 --> 00:39:03,800
columns you see over here on the right

00:39:01,940 --> 00:39:05,450
social search internal and other refer

00:39:03,800 --> 00:39:07,670
to where the referrals came from so

00:39:05,450 --> 00:39:09,320
whether it came from a social or a

00:39:07,670 --> 00:39:11,360
search site or internal meaning from

00:39:09,320 --> 00:39:14,330
another AOL property right so we try

00:39:11,360 --> 00:39:16,880
very hard to maximize how much we drive

00:39:14,330 --> 00:39:23,270
traffic from TechCrunch to engadget and

00:39:16,880 --> 00:39:25,910
stuff like that right client access is

00:39:23,270 --> 00:39:30,560
really easy so we can we can we index on

00:39:25,910 --> 00:39:31,790
the on the no j s side of things alright

00:39:30,560 --> 00:39:35,600
so we can index a whole bunch of stuff

00:39:31,790 --> 00:39:38,510
just create big bulk index requests and

00:39:35,600 --> 00:39:40,730
shove them off with a wire PHP super

00:39:38,510 --> 00:39:42,770
simple to connect to elasticsearch using

00:39:40,730 --> 00:39:44,680
they have a really great library that

00:39:42,770 --> 00:39:48,650
that elastic search actually maintains

00:39:44,680 --> 00:39:53,630
so you can perform searches

00:39:48,650 --> 00:39:55,339
so very very early on in this talk I was

00:39:53,630 --> 00:39:59,000
mentioned I mentioned a multivariate

00:39:55,339 --> 00:40:01,119
testing all right so allowing editors to

00:39:59,000 --> 00:40:03,230
test the performance of any concrete

00:40:01,119 --> 00:40:07,099
element so things like Dex leads

00:40:03,230 --> 00:40:08,930
headlines sub leads hero images right so

00:40:07,099 --> 00:40:11,569
all the different things that aren't a

00:40:08,930 --> 00:40:14,059
page and we wanted to be able to let

00:40:11,569 --> 00:40:16,160
editors to do editors do this create

00:40:14,059 --> 00:40:18,170
tests start them stop them and evaluate

00:40:16,160 --> 00:40:19,670
them without actually having to talk to

00:40:18,170 --> 00:40:26,299
one of their developers right because

00:40:19,670 --> 00:40:29,119
developer time is expensive so what can

00:40:26,299 --> 00:40:30,770
we do how can we do this well each new

00:40:29,119 --> 00:40:32,720
visitor that comes into a site gets

00:40:30,770 --> 00:40:35,510
cooked that's how we assign them to a

00:40:32,720 --> 00:40:37,520
test group we used to inject test

00:40:35,510 --> 00:40:40,250
markers into that beacon payload that

00:40:37,520 --> 00:40:42,470
comes back over and then we compare the

00:40:40,250 --> 00:40:45,200
click-through rate to calculate

00:40:42,470 --> 00:40:47,599
performance right so we have a test

00:40:45,200 --> 00:40:49,250
population identifier on a test ID so

00:40:47,599 --> 00:40:50,960
this makes it makes sense to our

00:40:49,250 --> 00:40:52,760
back-end databases that understand what

00:40:50,960 --> 00:40:55,369
these tests are these are again

00:40:52,760 --> 00:40:57,230
user-defined tests so as a content

00:40:55,369 --> 00:40:59,000
editor or content writer I can come in

00:40:57,230 --> 00:41:00,079
here and create a test i could say i'm

00:40:59,000 --> 00:41:01,160
going to write an article i'm going to

00:41:00,079 --> 00:41:02,390
give it two different headlines and i

00:41:01,160 --> 00:41:09,950
want to see which headline gets more

00:41:02,390 --> 00:41:12,589
clicks so then on the on the analytic

00:41:09,950 --> 00:41:17,960
side from all that raw data we can look

00:41:12,589 --> 00:41:22,339
for page views we can create a nested

00:41:17,960 --> 00:41:24,619
aggregation that looks for records that

00:41:22,339 --> 00:41:27,319
have multivariate tests pulls out the

00:41:24,619 --> 00:41:28,819
test ID pulls out the population ID and

00:41:27,319 --> 00:41:32,960
then determines whether it was a clicker

00:41:28,819 --> 00:41:35,660
or a a page view and then we can do some

00:41:32,960 --> 00:41:40,579
very very complex mathematics to

00:41:35,660 --> 00:41:44,299
calculate the click-through rate and we

00:41:40,579 --> 00:41:45,500
end up with an interface like this so

00:41:44,299 --> 00:41:47,119
they can actually come in here there's

00:41:45,500 --> 00:41:48,859
no developer involvement here this works

00:41:47,119 --> 00:41:51,079
automatically on any one of our content

00:41:48,859 --> 00:41:53,089
plath in one of our content brands any

00:41:51,079 --> 00:41:55,460
editor can come in there and create four

00:41:53,089 --> 00:41:57,559
different headlines for their article so

00:41:55,460 --> 00:41:58,200
what's why this is important for them

00:41:57,559 --> 00:41:59,839
it's not

00:41:58,200 --> 00:42:02,010
we couldn't do this under the old system

00:41:59,839 --> 00:42:03,300
before they could look they can create

00:42:02,010 --> 00:42:05,339
an article and then maybe six hours

00:42:03,300 --> 00:42:09,180
later or tomorrow going to see how the

00:42:05,339 --> 00:42:10,619
different headlines performed now if you

00:42:09,180 --> 00:42:13,230
go and click on one of these headlines

00:42:10,619 --> 00:42:15,030
these numbers change within five seconds

00:42:13,230 --> 00:42:23,820
it's five seconds from browser to

00:42:15,030 --> 00:42:25,950
browser right another big thing mostly

00:42:23,820 --> 00:42:31,190
that my boss was was interested in my

00:42:25,950 --> 00:42:35,460
boss is a really interesting guy he's

00:42:31,190 --> 00:42:37,380
he's all about real-time native

00:42:35,460 --> 00:42:39,869
real-time native real time he wants to

00:42:37,380 --> 00:42:44,310
see things moving and popping and

00:42:39,869 --> 00:42:47,670
shining and making noise he wanted wall

00:42:44,310 --> 00:42:50,040
maps right so how can we create wall

00:42:47,670 --> 00:42:51,750
maps with this raw data well we can

00:42:50,040 --> 00:42:54,150
create a socket i/o server use a

00:42:51,750 --> 00:42:56,730
WebSocket server let browsers connect to

00:42:54,150 --> 00:42:58,260
that with a JavaScript client and stream

00:42:56,730 --> 00:43:01,109
data right into the browser now

00:42:58,260 --> 00:43:03,839
obviously we have to select a subset of

00:43:01,109 --> 00:43:05,280
messages you can't stream 15,000

00:43:03,839 --> 00:43:10,410
messages a second to a browser I've

00:43:05,280 --> 00:43:14,550
tried it crashes chrome right so this is

00:43:10,410 --> 00:43:16,770
where RabbitMQ comes into play we can

00:43:14,550 --> 00:43:19,400
select out a small subset of messages

00:43:16,770 --> 00:43:21,839
push it out to the visualization service

00:43:19,400 --> 00:43:26,280
right and then we can pump it into d3

00:43:21,839 --> 00:43:28,800
who's used III okay see a few hands so

00:43:26,280 --> 00:43:31,440
we can pump this into d3 we can use d3

00:43:28,800 --> 00:43:33,000
to create a map projection and then draw

00:43:31,440 --> 00:43:35,430
on that map where each of these

00:43:33,000 --> 00:43:39,690
geographic points are and it ends up

00:43:35,430 --> 00:43:41,130
looking like this right so I showed this

00:43:39,690 --> 00:43:42,569
to my boss is like oh wow that's really

00:43:41,130 --> 00:43:43,770
cool and I said yeah that sure is a cool

00:43:42,569 --> 00:43:49,230
map of the english-speaking population

00:43:43,770 --> 00:43:50,640
of planet Earth which is what it is I

00:43:49,230 --> 00:43:52,920
shouldn't say that I mean we do have

00:43:50,640 --> 00:43:54,839
like we do have actually like a like

00:43:52,920 --> 00:43:57,089
Korean like language properties and

00:43:54,839 --> 00:43:58,680
japanese language properties but

00:43:57,089 --> 00:44:01,589
actually this is this is this means a

00:43:58,680 --> 00:44:04,470
lot more to our editors they put a lot

00:44:01,589 --> 00:44:08,940
of heart and soul into writing this

00:44:04,470 --> 00:44:10,170
content and for them numbers don't mean

00:44:08,940 --> 00:44:12,480
as much

00:44:10,170 --> 00:44:14,640
this feels to them much more live and

00:44:12,480 --> 00:44:17,820
real they could have an emotional

00:44:14,640 --> 00:44:20,780
connection almost to this sort of living

00:44:17,820 --> 00:44:23,310
breathing visualization of their content

00:44:20,780 --> 00:44:26,130
out there on the internet being consumed

00:44:23,310 --> 00:44:27,600
it's kind of cool I tried to make this a

00:44:26,130 --> 00:44:30,180
little bit more useful by color-coding

00:44:27,600 --> 00:44:33,180
the dots right so I color code them

00:44:30,180 --> 00:44:34,740
based on the property but you don't

00:44:33,180 --> 00:44:37,230
really I know I mean like the green dots

00:44:34,740 --> 00:44:38,940
are of dailyfinance calm so that covers

00:44:37,230 --> 00:44:40,470
like the stock market's international

00:44:38,940 --> 00:44:42,600
currency markets and stuff like that so

00:44:40,470 --> 00:44:45,030
I mean big surprise popular in London

00:44:42,600 --> 00:44:46,740
New York San Francisco right autoblog

00:44:45,030 --> 00:44:48,570
like really popular in like Russia

00:44:46,740 --> 00:44:53,430
they're like super into cars there for

00:44:48,570 --> 00:44:55,140
some reason one thing that's really cool

00:44:53,430 --> 00:44:59,610
is like the ability to create embeddable

00:44:55,140 --> 00:45:01,170
visualizations right so this is a tiny

00:44:59,610 --> 00:45:03,570
little visualization that we can

00:45:01,170 --> 00:45:06,180
actually embed next to a post inside of

00:45:03,570 --> 00:45:07,650
our CMS right I know if you can see this

00:45:06,180 --> 00:45:10,740
with the light it's kind of dim can you

00:45:07,650 --> 00:45:12,840
guys see this okay now I won't call this

00:45:10,740 --> 00:45:15,930
a graph because if you'll notice there's

00:45:12,840 --> 00:45:19,710
no numbers on either axis this is not a

00:45:15,930 --> 00:45:22,320
graph all this is is I'll show you what

00:45:19,710 --> 00:45:25,170
this is all this is is we're counting

00:45:22,320 --> 00:45:27,630
the number of events that happen each

00:45:25,170 --> 00:45:28,650
tick and then we're moving the line up

00:45:27,630 --> 00:45:30,690
and down and doing a spline

00:45:28,650 --> 00:45:36,150
transformation so that it goes all

00:45:30,690 --> 00:45:37,470
squiggly right it's very very simple but

00:45:36,150 --> 00:45:39,480
this allows people to have a connection

00:45:37,470 --> 00:45:40,980
to this data sitting right next there to

00:45:39,480 --> 00:45:42,720
next to their posts they could say wow

00:45:40,980 --> 00:45:46,050
people are really like looking at my

00:45:42,720 --> 00:45:47,520
stuff look at it it's moving this point

00:45:46,050 --> 00:45:49,350
here like jumps really really high and

00:45:47,520 --> 00:45:50,820
it looks really impressive it's probably

00:45:49,350 --> 00:45:52,620
a network glitch where like we kind of

00:45:50,820 --> 00:45:55,170
missed counting for a second it's not a

00:45:52,620 --> 00:45:59,100
really big deal but these kind of

00:45:55,170 --> 00:46:00,990
visualizations have a huge impact you

00:45:59,100 --> 00:46:02,460
know what here there it is in a way that

00:46:00,990 --> 00:46:04,140
I think sometimes we as engineers miss

00:46:02,460 --> 00:46:09,660
right because we tend to be much more

00:46:04,140 --> 00:46:11,250
like evidence-based and I want to give

00:46:09,660 --> 00:46:14,370
some love to the developers to so like I

00:46:11,250 --> 00:46:17,220
said we have all these developers we

00:46:14,370 --> 00:46:20,390
have about a dozen developers who work

00:46:17,220 --> 00:46:22,920
in my team working on the core platform

00:46:20,390 --> 00:46:24,390
and then probably you know a couple of

00:46:22,920 --> 00:46:26,160
hundred developers who are actually

00:46:24,390 --> 00:46:27,870
building on top of the platform actually

00:46:26,160 --> 00:46:31,410
building user-facing sites right so

00:46:27,870 --> 00:46:33,570
they're really very important to us want

00:46:31,410 --> 00:46:36,660
to give them the ability to profile

00:46:33,570 --> 00:46:39,810
their code live right so they use our

00:46:36,660 --> 00:46:41,130
API they have two ways of building on

00:46:39,810 --> 00:46:44,190
our platform they can connect to our

00:46:41,130 --> 00:46:47,610
HTTP API and they can build using

00:46:44,190 --> 00:46:49,950
whatever stack they like they can use

00:46:47,610 --> 00:46:53,040
PHP they can use Ruby they can use

00:46:49,950 --> 00:46:55,650
nodejs whatever but they have to manage

00:46:53,040 --> 00:46:56,520
their own infrastructure if they want

00:46:55,650 --> 00:46:58,530
they can use our managed hosting

00:46:56,520 --> 00:47:00,330
platform right so we handle all the

00:46:58,530 --> 00:47:02,280
scaling we handle all the cash and we do

00:47:00,330 --> 00:47:04,080
all the hard work so they can focus on

00:47:02,280 --> 00:47:06,000
their stuff so the vast majority of our

00:47:04,080 --> 00:47:07,140
customers do this right the only

00:47:06,000 --> 00:47:12,810
downside is that you have to work in a

00:47:07,140 --> 00:47:14,400
custom dsl now it's better than it used

00:47:12,810 --> 00:47:17,490
to be used to be you had to work inside

00:47:14,400 --> 00:47:18,930
of our IDE now we have the ability to

00:47:17,490 --> 00:47:22,140
let people create their own git

00:47:18,930 --> 00:47:24,900
repositories they write their code they

00:47:22,140 --> 00:47:27,300
use these this thing called the amp

00:47:24,900 --> 00:47:28,950
client and then they can sink that to a

00:47:27,300 --> 00:47:30,390
live machine that's out there on the

00:47:28,950 --> 00:47:32,280
internet or they can do testing is it's

00:47:30,390 --> 00:47:33,720
really really cool I keep trying to get

00:47:32,280 --> 00:47:35,760
Ralph still I give and talk about this

00:47:33,720 --> 00:47:38,490
really really fascinating it's all built

00:47:35,760 --> 00:47:40,050
in PHP all built on twig like the other

00:47:38,490 --> 00:47:41,730
user facing language is just based on

00:47:40,050 --> 00:47:43,170
twigs to the extent instead of a series

00:47:41,730 --> 00:47:50,760
of twig extensions that generate all

00:47:43,170 --> 00:47:52,170
this stuff so to profile this code what

00:47:50,760 --> 00:47:54,440
we do is essentially the dev starts up a

00:47:52,170 --> 00:47:57,210
profiler session so they're in their

00:47:54,440 --> 00:48:00,000
management console and they started a

00:47:57,210 --> 00:48:01,500
profiler session that gives them a query

00:48:00,000 --> 00:48:03,390
parameter so they hit their website we

00:48:01,500 --> 00:48:04,980
used with this query parameter appended

00:48:03,390 --> 00:48:06,330
to the URL they can do this in

00:48:04,980 --> 00:48:09,330
production or they can do it in divot

00:48:06,330 --> 00:48:12,870
and development it doesn't matter using

00:48:09,330 --> 00:48:14,370
that token activates the profiling so

00:48:12,870 --> 00:48:16,350
all of these event messages that get

00:48:14,370 --> 00:48:18,120
generated by activities that are

00:48:16,350 --> 00:48:21,020
happening inside of the system get

00:48:18,120 --> 00:48:23,190
tagged with that profile session ID

00:48:21,020 --> 00:48:25,410
anything that's tagged appropriately

00:48:23,190 --> 00:48:27,570
gets routed by RabbitMQ to the profiler

00:48:25,410 --> 00:48:29,370
service alright so the profiler service

00:48:27,570 --> 00:48:31,890
just sits there waits for messages when

00:48:29,370 --> 00:48:34,410
it gets messages that its Caesar meant

00:48:31,890 --> 00:48:35,250
for a logged in developer it routes them

00:48:34,410 --> 00:48:38,260
to the console

00:48:35,250 --> 00:48:43,420
so they can view a waterfall of their

00:48:38,260 --> 00:48:45,040
code executing right it's not baby the

00:48:43,420 --> 00:48:50,130
best example like wow that is a really

00:48:45,040 --> 00:48:52,060
slow HTTP request nine seconds bad times

00:48:50,130 --> 00:48:53,410
but they can do this like i said i think

00:48:52,060 --> 00:48:54,580
it is in production and they can do this

00:48:53,410 --> 00:48:57,340
in development so they can actually go

00:48:54,580 --> 00:49:00,130
and look at their code executing live on

00:48:57,340 --> 00:49:01,330
some you know one of our 1200 render

00:49:00,130 --> 00:49:03,340
servers they're sitting out there

00:49:01,330 --> 00:49:06,420
without knowing which server it's on

00:49:03,340 --> 00:49:06,420
without having to go and look at logs

00:49:07,170 --> 00:49:14,440
the last piece I'll talk about is

00:49:10,320 --> 00:49:15,550
cross-platform eventing right so I

00:49:14,440 --> 00:49:17,859
mentioned that we work in a bunch of

00:49:15,550 --> 00:49:18,910
different stacks and one of the things

00:49:17,859 --> 00:49:20,920
that we thought would be really cool be

00:49:18,910 --> 00:49:23,290
allowed developers to dispatch native

00:49:20,920 --> 00:49:27,040
events in one stack and then observe

00:49:23,290 --> 00:49:30,340
them on another right so in this case

00:49:27,040 --> 00:49:31,720
our CMS is built in PHP it's not built

00:49:30,340 --> 00:49:33,070
using symphony but we use a lot of

00:49:31,720 --> 00:49:34,270
different libraries one of the libraries

00:49:33,070 --> 00:49:37,390
that we use is a symphony event

00:49:34,270 --> 00:49:41,680
dispatcher right so we want to allow a

00:49:37,390 --> 00:49:43,750
PHP developer to dispatch events using

00:49:41,680 --> 00:49:45,220
the symphony event dispatcher and then

00:49:43,750 --> 00:49:47,770
maybe have another developer who's

00:49:45,220 --> 00:49:51,100
working in nodejs somewhere consume that

00:49:47,770 --> 00:49:58,090
event natively right so to distributed

00:49:51,100 --> 00:49:59,320
event handling without PHP workers so

00:49:58,090 --> 00:50:00,460
this is kind of how that routing happens

00:49:59,320 --> 00:50:02,230
and we've added in another another

00:50:00,460 --> 00:50:03,520
direction so we have these messages that

00:50:02,230 --> 00:50:08,410
go out to the event handler service

00:50:03,520 --> 00:50:09,910
right The Dispatch works basically just

00:50:08,410 --> 00:50:12,160
by overriding the native dispatch that

00:50:09,910 --> 00:50:15,369
comes out of the symphony event

00:50:12,160 --> 00:50:16,900
dispatcher so we look for events that

00:50:15,369 --> 00:50:21,070
are forwardable that implement this

00:50:16,900 --> 00:50:25,020
forwardable interface and when we see

00:50:21,070 --> 00:50:25,020
them we push them out onto the RabbitMQ

00:50:27,910 --> 00:50:34,510
I talk before I clicked right so that we

00:50:31,960 --> 00:50:38,079
forward them out onto rabbitmq so then

00:50:34,510 --> 00:50:40,839
over in in the know Jas world all we're

00:50:38,079 --> 00:50:42,670
doing here is creating a listener so

00:50:40,839 --> 00:50:43,690
there's some bootstrapping code that

00:50:42,670 --> 00:50:47,440
actually comes in here and reads this

00:50:43,690 --> 00:50:52,420
out of the exports but we're here

00:50:47,440 --> 00:50:56,470
dispatching an event name damp post save

00:50:52,420 --> 00:50:58,450
and then here in a lot in in nodejs we

00:50:56,470 --> 00:51:03,339
are listening for an event called aunt

00:50:58,450 --> 00:51:06,609
post save right when that event gets

00:51:03,339 --> 00:51:09,010
sent out over the pipeline it gets

00:51:06,609 --> 00:51:16,270
picked up by the event handlers and can

00:51:09,010 --> 00:51:17,140
be consumed natively in nodejs so we

00:51:16,270 --> 00:51:19,599
have a bunch of other stuff that we're

00:51:17,140 --> 00:51:21,760
building on top of this to kind of

00:51:19,599 --> 00:51:24,490
summarize what we're talking about where

00:51:21,760 --> 00:51:27,130
we are right now 1.3 billion events that

00:51:24,490 --> 00:51:30,609
we're consuming a day we're out all of

00:51:27,130 --> 00:51:32,619
this stuff through through rabbitmq two

00:51:30,609 --> 00:51:35,230
different microservice consumers right

00:51:32,619 --> 00:51:36,700
we're doing real-time analytics on a

00:51:35,230 --> 00:51:39,549
data set that's about two hundred and

00:51:36,700 --> 00:51:41,700
fifty gigabytes of raw data per day so

00:51:39,549 --> 00:51:43,539
of that two terabytes that we consume

00:51:41,700 --> 00:51:48,010
250 gets ratted into our real-time

00:51:43,539 --> 00:51:50,559
analytics service we're visualizing 1.3

00:51:48,010 --> 00:51:52,630
million events a day and generating live

00:51:50,559 --> 00:51:54,760
profiles 450 property so we have about

00:51:52,630 --> 00:51:56,950
50 different developers at any given

00:51:54,760 --> 00:52:00,309
time that are in there doing live

00:51:56,950 --> 00:52:02,559
profiling of code and we're using we're

00:52:00,309 --> 00:52:04,480
using cross-platform eventing to handle

00:52:02,559 --> 00:52:09,010
that 10,000 elasticsearch search index

00:52:04,480 --> 00:52:11,980
updates every day all this is built in

00:52:09,010 --> 00:52:14,980
Java nodejs a lot of PHP is especially

00:52:11,980 --> 00:52:16,829
in the front end pieces Python on the

00:52:14,980 --> 00:52:24,609
backend Hadoop and RabbitMQ

00:52:16,829 --> 00:52:26,529
elasticsearch and vertica 4 2016 and the

00:52:24,609 --> 00:52:27,670
rest of the year so one of the big

00:52:26,529 --> 00:52:30,400
things is cool is that we're moving this

00:52:27,670 --> 00:52:33,069
all into AWS which in the one hand is

00:52:30,400 --> 00:52:34,240
horrifying because petabytes of data so

00:52:33,069 --> 00:52:36,400
we give you one of those cool companies

00:52:34,240 --> 00:52:37,809
that gets like FedEx like boxes of hard

00:52:36,400 --> 00:52:39,770
drives to Amazon that's going to be

00:52:37,809 --> 00:52:41,690
exciting

00:52:39,770 --> 00:52:44,480
a lot of this is going to change because

00:52:41,690 --> 00:52:45,800
amazon kinesis right so I don't feel

00:52:44,480 --> 00:52:47,570
familiar but amazon has this really

00:52:45,800 --> 00:52:50,030
really cool new streaming service called

00:52:47,570 --> 00:52:51,620
Kinesis some of the things that we're

00:52:50,030 --> 00:52:52,670
looking to do a real-time sentiment

00:52:51,620 --> 00:52:54,790
analysis and predictive performance

00:52:52,670 --> 00:52:57,230
analysis so we have this plan to create

00:52:54,790 --> 00:53:00,260
content bootstrapping to let our content

00:52:57,230 --> 00:53:02,450
creators know what topics are trending

00:53:00,260 --> 00:53:05,600
heavily out there in the media world

00:53:02,450 --> 00:53:07,940
right now that they're not covering so

00:53:05,600 --> 00:53:11,270
if they're a cambio and there's like hot

00:53:07,940 --> 00:53:13,280
Justin Bieber news that cambio needs to

00:53:11,270 --> 00:53:15,770
get on now right we want to be able to

00:53:13,280 --> 00:53:17,180
tell them hey like we don't know what it

00:53:15,770 --> 00:53:19,820
is but there is some gesture Justin

00:53:17,180 --> 00:53:21,740
Bieber stuff going down right now you

00:53:19,820 --> 00:53:23,240
need to get on it right so we can do

00:53:21,740 --> 00:53:26,150
predictive analysis social sentiment

00:53:23,240 --> 00:53:28,370
analysis of what's kind of happening in

00:53:26,150 --> 00:53:29,570
the social world in real time and

00:53:28,370 --> 00:53:32,270
continue with some bed embeddable

00:53:29,570 --> 00:53:34,820
visuals visualizations the pipeline if

00:53:32,270 --> 00:53:38,330
you look at it as map ends up looking

00:53:34,820 --> 00:53:39,940
like this so we have different the AOL

00:53:38,330 --> 00:53:42,680
media platform in the AL data platform

00:53:39,940 --> 00:53:44,360
that are generating or inputting into

00:53:42,680 --> 00:53:46,390
the pipeline and then the analytics

00:53:44,360 --> 00:53:48,230
service visualizations profiler

00:53:46,390 --> 00:53:49,430
religions is our machine learning

00:53:48,230 --> 00:53:50,840
platform so we're in the process right

00:53:49,430 --> 00:53:52,310
now connecting these two things to

00:53:50,840 --> 00:53:53,930
actually take this real-time data and

00:53:52,310 --> 00:53:56,690
push it into that machine learning

00:53:53,930 --> 00:53:57,770
platform that I mentioned earlier so

00:53:56,690 --> 00:53:59,480
2016 is gonna be really really

00:53:57,770 --> 00:54:02,480
interesting year I would love hopefully

00:53:59,480 --> 00:54:04,460
to come back next year and tell you

00:54:02,480 --> 00:54:07,760
where we are then because I think it's

00:54:04,460 --> 00:54:10,010
really very exciting it's a very very

00:54:07,760 --> 00:54:11,720
exciting topic and it's really kind of

00:54:10,010 --> 00:54:16,220
cool to be at the kind of the bleeding

00:54:11,720 --> 00:54:17,720
edge of of media right so that's really

00:54:16,220 --> 00:54:19,190
all that I have we have a couple of

00:54:17,720 --> 00:54:23,270
minutes for questions please make sure

00:54:19,190 --> 00:54:25,430
that you take a moment and hit joined in

00:54:23,270 --> 00:54:26,720
give me some feedback leave feedback for

00:54:25,430 --> 00:54:28,280
all of the other speakers that you've

00:54:26,720 --> 00:54:31,700
seen is really really very important to

00:54:28,280 --> 00:54:35,170
us helps us make better talks for you

00:54:31,700 --> 00:54:35,170
guys oh thank you

00:54:39,470 --> 00:54:50,789
many questions that's a very interest

00:54:47,849 --> 00:54:54,180
talk as the very good to hear the

00:54:50,789 --> 00:54:57,329
experience we're using Kafka as well so

00:54:54,180 --> 00:55:00,180
and we're using spark streaming for the

00:54:57,329 --> 00:55:02,190
real time not analytics but statistics

00:55:00,180 --> 00:55:05,460
with the latter sir so just have a

00:55:02,190 --> 00:55:07,289
question of interest the you still use

00:55:05,460 --> 00:55:08,940
rubbing and cute have you considered to

00:55:07,289 --> 00:55:11,609
replace this with cup cards for all this

00:55:08,940 --> 00:55:16,740
good use cases for probably am q in

00:55:11,609 --> 00:55:18,420
these other scenarios um I mean we kind

00:55:16,740 --> 00:55:19,890
of compared the two when I was building

00:55:18,420 --> 00:55:22,619
the part of the system i went with

00:55:19,890 --> 00:55:25,579
rabbitmq mostly because i don't need the

00:55:22,619 --> 00:55:27,809
persistence that rabbit that Kafka gives

00:55:25,579 --> 00:55:31,049
Kafka's actually can handle a much

00:55:27,809 --> 00:55:32,549
larger load of data but we're not at the

00:55:31,049 --> 00:55:34,440
point where we're kind of pushing up

00:55:32,549 --> 00:55:37,200
against rabbitmq limitation so it didn't

00:55:34,440 --> 00:55:38,250
really matter in 2016 like I said we're

00:55:37,200 --> 00:55:41,130
going to be moving all this stuff into

00:55:38,250 --> 00:55:43,440
Kinesis in amazon so it's going to be

00:55:41,130 --> 00:55:47,099
interesting to see how that how that

00:55:43,440 --> 00:55:58,369
plays out but probably not Kafka anytime

00:55:47,099 --> 00:55:58,369
soon any others sweet go eat lunch

00:55:59,099 --> 00:56:02,490

YouTube URL: https://www.youtube.com/watch?v=1i1Zoy2JpcY


