Title: Introduction to Time Series - David McKay - PHP UK 2020
Publication date: 2020-03-11
Playlist: PHP UK Conference 2020
Description: 
	Time-Series has been the fastest growing database category, rated, by DBEngines, for over 2 years; yet, less than 15% store their time-series data in a time-series database. Do you?
Captions: 
	00:00:02,730 --> 00:00:06,990
my name is David I'm a developer

00:00:04,980 --> 00:00:11,190
advocate for a company called influx

00:00:06,990 --> 00:00:14,280
data our primary product is f.lux DB and

00:00:11,190 --> 00:00:18,230
open source time series database anyone

00:00:14,280 --> 00:00:20,670
familiar before I begin okay a few hands

00:00:18,230 --> 00:00:23,939
all right first we'll start with me so

00:00:20,670 --> 00:00:26,730
I'm Scottish I have a load of animals in

00:00:23,939 --> 00:00:29,279
my house I really like esoteric

00:00:26,730 --> 00:00:31,380
programming languages this is the

00:00:29,279 --> 00:00:33,570
question I ask every time and I've not

00:00:31,380 --> 00:00:36,239
had a hand so I'm looking for someone

00:00:33,570 --> 00:00:39,410
here but has anyone ever heard of a

00:00:36,239 --> 00:00:39,410
Retton elaine of pune

00:00:39,530 --> 00:00:44,039
all right that's your mission after this

00:00:42,000 --> 00:00:46,530
talk go look up Pony Liang very very

00:00:44,039 --> 00:00:48,120
cool and rust usually gets a few more

00:00:46,530 --> 00:00:53,070
hands still kinda esoteric but a very

00:00:48,120 --> 00:00:55,019
cool language - I am quite into the

00:00:53,070 --> 00:00:56,670
container ecosystem I'm a member of the

00:00:55,019 --> 00:00:58,050
kubernetes organization and I've been

00:00:56,670 --> 00:00:59,429
involved in a release team for the last

00:00:58,050 --> 00:01:01,170
year so if anyone wants to talk about

00:00:59,429 --> 00:01:03,870
kubernetes or containers any point

00:01:01,170 --> 00:01:06,240
please come and speak to me and I am a

00:01:03,870 --> 00:01:07,950
practicing stoic so if you're not

00:01:06,240 --> 00:01:11,580
familiar with stoicism it's just the

00:01:07,950 --> 00:01:12,690
idea that being uncomfortable with being

00:01:11,580 --> 00:01:17,820
comfortable with uncomfortable

00:01:12,690 --> 00:01:19,290
situations like this and my animals so

00:01:17,820 --> 00:01:22,860
I've got a small video does anyone know

00:01:19,290 --> 00:01:25,470
what a chinchilla is so I have to Degas

00:01:22,860 --> 00:01:27,840
510 cellos a free-roaming ferret a dog

00:01:25,470 --> 00:01:29,220
some fish and some other stuff but

00:01:27,840 --> 00:01:36,120
aren't they just the cutest animals in

00:01:29,220 --> 00:01:38,300
the world no it's stuck there we go okay

00:01:36,120 --> 00:01:40,470
so the talk introduction to time series

00:01:38,300 --> 00:01:43,140
before we begin I'm going to start with

00:01:40,470 --> 00:01:44,550
a small pop quiz right when we talk

00:01:43,140 --> 00:01:47,310
about time series we generally think

00:01:44,550 --> 00:01:51,540
about monitoring and IOT we talk about

00:01:47,310 --> 00:01:52,950
financial trading etc but there's a

00:01:51,540 --> 00:01:54,960
whole bunch of fundamental concepts of

00:01:52,950 --> 00:01:57,450
time series that are actually much much

00:01:54,960 --> 00:01:59,100
older so we're gonna play the invented

00:01:57,450 --> 00:02:00,870
when game and I'm just going to cleave

00:01:59,100 --> 00:02:02,010
it myself I'm not going to make anyone

00:02:00,870 --> 00:02:04,260
participate that doesn't want to

00:02:02,010 --> 00:02:06,300
participate but as developers or

00:02:04,260 --> 00:02:08,060
operators or anything like that we have

00:02:06,300 --> 00:02:10,140
to deal with the concept of encoding

00:02:08,060 --> 00:02:12,930
probably on a daily basis whether that

00:02:10,140 --> 00:02:15,239
be JSON encoding or other formats but

00:02:12,930 --> 00:02:17,689
when do we think encoding was first used

00:02:15,239 --> 00:02:20,099
and modern civilized

00:02:17,689 --> 00:02:24,269
the surprising answer may be that

00:02:20,099 --> 00:02:27,239
actually it goes back to 410 BC oh not

00:02:24,269 --> 00:02:29,730
that year the encoding system back then

00:02:27,239 --> 00:02:33,180
was documented in Plutarch and it's just

00:02:29,730 --> 00:02:36,840
ancient Rome kind of I guess studies

00:02:33,180 --> 00:02:39,299
diary maxed you were both and what they

00:02:36,840 --> 00:02:41,639
said and this book was that there was a

00:02:39,299 --> 00:02:44,670
mercenary someone who had a boat with a

00:02:41,639 --> 00:02:47,189
crew and the ability to fete code Alka

00:02:44,670 --> 00:02:50,069
biddies who didn't fight for any one

00:02:47,189 --> 00:02:52,530
particular Sade in any war what he used

00:02:50,069 --> 00:02:54,959
to do is show up to the bow in a school

00:02:52,530 --> 00:02:56,970
though who's who and then raise a flag

00:02:54,959 --> 00:02:58,950
to see which one he was gonna kill that

00:02:56,970 --> 00:03:00,750
was his entire premise and I'm sure he

00:02:58,950 --> 00:03:01,739
made a lot of money during that now

00:03:00,750 --> 00:03:03,689
maybe it's a bit of a stretch to

00:03:01,739 --> 00:03:05,579
consider this encoding but he used a

00:03:03,689 --> 00:03:08,459
very simple flag based system to

00:03:05,579 --> 00:03:09,870
indicate to a much greater audience what

00:03:08,459 --> 00:03:13,590
save of that battle he was gonna

00:03:09,870 --> 00:03:15,689
participate on now we're talking about

00:03:13,590 --> 00:03:19,829
the 14th century so think about that as

00:03:15,689 --> 00:03:21,659
1500 years and at minimum and the system

00:03:19,829 --> 00:03:24,299
hadn't really evolved much in that time

00:03:21,659 --> 00:03:28,169
so they're now in a system with one flag

00:03:24,299 --> 00:03:31,769
or two flags 1500 years to decide we

00:03:28,169 --> 00:03:33,239
could add another flag but then shortly

00:03:31,769 --> 00:03:35,340
after that just another hundred years

00:03:33,239 --> 00:03:37,650
things started to get a little bit more

00:03:35,340 --> 00:03:41,099
sophisticated they actually had 15 flags

00:03:37,650 --> 00:03:42,780
to represent different letters or or

00:03:41,099 --> 00:03:45,150
actions that they were going to partake

00:03:42,780 --> 00:03:47,340
in the war and then just a couple of

00:03:45,150 --> 00:03:49,620
hundred years later in the 17th century

00:03:47,340 --> 00:03:52,319
this system is actually still used today

00:03:49,620 --> 00:03:53,939
by most Navy ships so we have a system

00:03:52,319 --> 00:03:54,810
that represents numbers and some letters

00:03:53,939 --> 00:03:57,659
with flags and they have different

00:03:54,810 --> 00:04:00,659
colors and that's it right so encoding

00:03:57,659 --> 00:04:04,680
not new very very old and has been used

00:04:00,659 --> 00:04:08,280
across the last 2,000 years so if I ask

00:04:04,680 --> 00:04:10,560
you when sharding is is anyone brave

00:04:08,280 --> 00:04:13,049
enough to think it's modern hopefully no

00:04:10,560 --> 00:04:15,810
sharding is also very old but when we

00:04:13,049 --> 00:04:17,609
talk about it in a technology sense we

00:04:15,810 --> 00:04:20,699
may think of sharting as as new thing

00:04:17,609 --> 00:04:24,389
that people ten years before but

00:04:20,699 --> 00:04:27,270
actually dates back to 150 BC wasn't

00:04:24,389 --> 00:04:29,760
documented until many years later but

00:04:27,270 --> 00:04:31,500
the concept of sharding may not be what

00:04:29,760 --> 00:04:35,020
we think of to

00:04:31,500 --> 00:04:37,690
what the Romans did was take the Roman

00:04:35,020 --> 00:04:40,600
alphabet spread across five tablets with

00:04:37,690 --> 00:04:42,100
five letters on each tablet and the

00:04:40,600 --> 00:04:44,440
reason they did this is because the way

00:04:42,100 --> 00:04:46,660
you won any war two thousand years ago

00:04:44,440 --> 00:04:48,699
was how fast you could communicate with

00:04:46,660 --> 00:04:50,320
your trips and different posts being

00:04:48,699 --> 00:04:54,669
able to use tactics to your advantage to

00:04:50,320 --> 00:04:55,960
attack your you know your enemy so I

00:04:54,669 --> 00:04:58,180
honestly find this 2,000 year old photo

00:04:55,960 --> 00:05:00,250
let's his hold it at the encoding and

00:04:58,180 --> 00:05:02,860
they used the flame system or smoke

00:05:00,250 --> 00:05:04,419
system so on the left wall they would

00:05:02,860 --> 00:05:07,180
have five people all with the flame

00:05:04,419 --> 00:05:09,190
torch and it would end the Kate which

00:05:07,180 --> 00:05:11,500
tablet the message was going to come

00:05:09,190 --> 00:05:14,229
from if they raised the four smoke

00:05:11,500 --> 00:05:16,000
flames they would know fourth tablet on

00:05:14,229 --> 00:05:17,710
the right hand side they would do the

00:05:16,000 --> 00:05:19,630
same to indicate which letter on the

00:05:17,710 --> 00:05:22,000
tablet and they were able to transmit

00:05:19,630 --> 00:05:24,160
messages really really far and really

00:05:22,000 --> 00:05:26,020
really fast and that was the biggest

00:05:24,160 --> 00:05:29,650
advantage of the Roman Empire had when

00:05:26,020 --> 00:05:32,410
it came to fighting their enemies if

00:05:29,650 --> 00:05:34,570
like me you appreciate learning about

00:05:32,410 --> 00:05:36,310
these kind of historical points and and

00:05:34,570 --> 00:05:38,320
how technology and maybe isn't as new as

00:05:36,310 --> 00:05:39,610
we like to believe there's a really good

00:05:38,320 --> 00:05:41,530
book called early history of data

00:05:39,610 --> 00:05:43,210
networks most of the examples I spoke

00:05:41,530 --> 00:05:45,760
about here are in this and there are so

00:05:43,210 --> 00:05:47,500
much more so if you enjoyed that buy

00:05:45,760 --> 00:05:49,949
this book read it you'll have a great

00:05:47,500 --> 00:05:49,949
time okay

00:05:50,560 --> 00:05:56,979
no the same series that has to be new

00:05:52,930 --> 00:05:59,320
right can't be old same series must be

00:05:56,979 --> 00:06:01,090
new unfortunately attempt series is also

00:05:59,320 --> 00:06:04,659
really old the Romans had a concept of

00:06:01,090 --> 00:06:07,870
time series in fact the Romans used to

00:06:04,659 --> 00:06:09,729
have IPOs the Romans used to have stock

00:06:07,870 --> 00:06:13,419
markets the Romans used to trade shares

00:06:09,729 --> 00:06:14,889
and companies where their peers people

00:06:13,419 --> 00:06:16,599
were tracking the price and

00:06:14,889 --> 00:06:19,539
profitability and value of all of these

00:06:16,599 --> 00:06:20,949
companies going to the Roman Marcus and

00:06:19,539 --> 00:06:23,889
trading the shares and all other

00:06:20,949 --> 00:06:25,690
companies making some extra money and I

00:06:23,889 --> 00:06:27,250
find that fascinating right nothing we

00:06:25,690 --> 00:06:29,500
do is you the Romans that everything

00:06:27,250 --> 00:06:34,270
2,000 years ago but we can learn a lot

00:06:29,500 --> 00:06:36,610
of lessons from them so again many many

00:06:34,270 --> 00:06:38,349
many years later the Netherlands were

00:06:36,610 --> 00:06:41,940
the first to have the first IPO that we

00:06:38,349 --> 00:06:41,940
have on record I was in 1602

00:06:41,979 --> 00:06:47,539
the US were lagging behind of course and

00:06:44,900 --> 00:06:49,370
they had the first I appeal then no I

00:06:47,539 --> 00:06:52,160
think what's important here but these

00:06:49,370 --> 00:06:54,710
two events is that the concept of time

00:06:52,160 --> 00:06:58,780
series as a phrase has never been used

00:06:54,710 --> 00:07:02,389
in any public racing ever at this point

00:06:58,780 --> 00:07:05,960
in fact it was an 1884 that someone

00:07:02,389 --> 00:07:09,050
actually coined the term time series and

00:07:05,960 --> 00:07:12,460
what they were interested in was every

00:07:09,050 --> 00:07:16,220
amp or silk and cotton and other

00:07:12,460 --> 00:07:18,229
textiles from around the world does that

00:07:16,220 --> 00:07:19,639
have any correlation with the export

00:07:18,229 --> 00:07:23,330
price that we were getting on her wheat

00:07:19,639 --> 00:07:26,000
and the answer was yes I would not

00:07:23,330 --> 00:07:28,220
suggest reading that article and it

00:07:26,000 --> 00:07:33,650
hasn't aged well it's very dry but you

00:07:28,220 --> 00:07:36,860
know it's there it's available but as as

00:07:33,650 --> 00:07:38,659
a Terra memories as an old 1884 okay so

00:07:36,860 --> 00:07:41,120
that's the date that none of us relies

00:07:38,659 --> 00:07:42,770
but not too long ago why is this

00:07:41,120 --> 00:07:44,569
important well it was the first paper

00:07:42,770 --> 00:07:46,820
that ever actually used the dimension of

00:07:44,569 --> 00:07:48,770
time and any statistical mathematics or

00:07:46,820 --> 00:07:51,889
analysis of numbers and that's what

00:07:48,770 --> 00:07:54,770
we're going to talk about today okay so

00:07:51,889 --> 00:07:58,130
my CTO hates it when I put a slate in

00:07:54,770 --> 00:07:59,599
but on my first week of the job I spoke

00:07:58,130 --> 00:08:01,310
with him and he told me that most data

00:07:59,599 --> 00:08:02,930
is best understood on a dimension of

00:08:01,310 --> 00:08:05,449
time and that has stuck with me for the

00:08:02,930 --> 00:08:06,680
last 15 months and it really opens your

00:08:05,449 --> 00:08:08,360
eyes when you think about that

00:08:06,680 --> 00:08:09,650
especially when you go back to your jobs

00:08:08,360 --> 00:08:10,639
on Monday and you're starting to look at

00:08:09,650 --> 00:08:13,219
the amount of theories that you have

00:08:10,639 --> 00:08:14,870
start to think about it's a dimension of

00:08:13,219 --> 00:08:16,909
time and how that applies to your data

00:08:14,870 --> 00:08:19,849
and the whole world the possibilities

00:08:16,909 --> 00:08:21,319
are going to open up so now we're

00:08:19,849 --> 00:08:24,199
actually going to start the talk that's

00:08:21,319 --> 00:08:25,190
the history bit done what are we going

00:08:24,199 --> 00:08:28,370
to cover if we were going to talk about

00:08:25,190 --> 00:08:29,960
what time series did it is I am going to

00:08:28,370 --> 00:08:31,669
talk to you a little bit about TSD B's

00:08:29,960 --> 00:08:33,320
or why you should use a time tears

00:08:31,669 --> 00:08:37,250
database instead of a general purpose

00:08:33,320 --> 00:08:39,229
database I will use some examples with n

00:08:37,250 --> 00:08:40,370
flux DB because that should pays me and

00:08:39,229 --> 00:08:42,620
that's the product I am most familiar

00:08:40,370 --> 00:08:44,660
with what I will tell you is that

00:08:42,620 --> 00:08:46,160
everything I cover is very much agnostic

00:08:44,660 --> 00:08:48,800
and will work with Prometheus or any

00:08:46,160 --> 00:08:50,779
other tsdp that you may be using and

00:08:48,800 --> 00:08:52,459
we'll talk a little bit about the value

00:08:50,779 --> 00:08:53,930
and not just a value but the cost of

00:08:52,459 --> 00:08:54,590
time series data there's a lot to

00:08:53,930 --> 00:08:56,990
understand

00:08:54,590 --> 00:08:58,730
and mostly because generally you have a

00:08:56,990 --> 00:09:00,440
lot of time series data and you have to

00:08:58,730 --> 00:09:03,830
understand the trade-off of storing it

00:09:00,440 --> 00:09:06,410
all or storing some and then I'll talk a

00:09:03,830 --> 00:09:08,300
little bit about as developers and

00:09:06,410 --> 00:09:10,040
operators where we have moved in the

00:09:08,300 --> 00:09:11,540
last ten years and why time series data

00:09:10,040 --> 00:09:13,310
is so important to understand that

00:09:11,540 --> 00:09:16,510
migration from monolithic the cloud

00:09:13,310 --> 00:09:19,910
native to micro services and servers

00:09:16,510 --> 00:09:22,670
okay so what is time series data nice

00:09:19,910 --> 00:09:25,880
and simple any piece of data we have a

00:09:22,670 --> 00:09:27,890
timestamp as time series data that's it

00:09:25,880 --> 00:09:29,720
there's nothing fancy here if you have

00:09:27,890 --> 00:09:31,280
something and a fail or written to a

00:09:29,720 --> 00:09:33,920
database that has some sort of created

00:09:31,280 --> 00:09:37,460
at last updated at any of that that is

00:09:33,920 --> 00:09:38,570
considered time series data so I'm going

00:09:37,460 --> 00:09:40,250
to kind of walk you through an example

00:09:38,570 --> 00:09:42,560
to understand why we need the dimension

00:09:40,250 --> 00:09:44,540
of time when we talk about data and what

00:09:42,560 --> 00:09:46,160
I have here is just random scenarios

00:09:44,540 --> 00:09:48,350
that everyone here should hopefully be

00:09:46,160 --> 00:09:52,160
familiar with inside of our

00:09:48,350 --> 00:09:55,670
infrastructure so and the late red color

00:09:52,160 --> 00:09:57,950
we have the memory hadn't 100% that

00:09:55,670 --> 00:09:58,970
would be a cause for concern if we see

00:09:57,950 --> 00:10:01,730
the memory usage a hundred percent

00:09:58,970 --> 00:10:03,320
something may be wrong next we have a

00:10:01,730 --> 00:10:05,990
health check feeling if there is a

00:10:03,320 --> 00:10:08,690
health check feeling and our production

00:10:05,990 --> 00:10:10,310
infrastructure something is wrong and of

00:10:08,690 --> 00:10:13,580
course if we see a port killed by the

00:10:10,310 --> 00:10:14,960
memory that is a bad thing

00:10:13,580 --> 00:10:16,370
we don't want pods to be dying or

00:10:14,960 --> 00:10:19,760
processes whatever you want to think of

00:10:16,370 --> 00:10:22,220
there and then in the orange a yellow

00:10:19,760 --> 00:10:24,350
color what we have our potential causal

00:10:22,220 --> 00:10:27,020
events these are not dangerous on their

00:10:24,350 --> 00:10:29,240
own but they could be a trigger for the

00:10:27,020 --> 00:10:30,680
dangerous stuff so if we have a database

00:10:29,240 --> 00:10:31,760
migration that runs that could cause

00:10:30,680 --> 00:10:34,340
problems that could break our

00:10:31,760 --> 00:10:36,050
application if a part gets restarted in

00:10:34,340 --> 00:10:37,970
kubernetes that could lead to potential

00:10:36,050 --> 00:10:41,060
problems too if we deploy a new version

00:10:37,970 --> 00:10:43,400
Ripper app I spoken to developers we are

00:10:41,060 --> 00:10:46,250
likely to introduce bugs new versions

00:10:43,400 --> 00:10:49,130
equal new problems ever see I passed the

00:10:46,250 --> 00:10:50,930
starties may not necessarily be a causal

00:10:49,130 --> 00:10:54,950
event but could trigger something else

00:10:50,930 --> 00:10:56,390
in our infrastructure and the purple we

00:10:54,950 --> 00:10:58,850
have things that should have absolutely

00:10:56,390 --> 00:11:00,350
no correlation whatsoever to any of the

00:10:58,850 --> 00:11:02,630
dangerous stuff and our empress dowager

00:11:00,350 --> 00:11:05,120
so if you do a git commit or the CPU is

00:11:02,630 --> 00:11:06,320
hovering around the 12 percent mark that

00:11:05,120 --> 00:11:07,279
should don't give you any cause for

00:11:06,320 --> 00:11:09,529
concern

00:11:07,279 --> 00:11:11,240
and the red herring and pink Scotland

00:11:09,529 --> 00:11:17,509
qualifying for the World Cup I've lost

00:11:11,240 --> 00:11:21,110
all hope of that ever happening now it's

00:11:17,509 --> 00:11:23,660
really difficult from here to understand

00:11:21,110 --> 00:11:25,160
what happened in the system but when we

00:11:23,660 --> 00:11:28,189
apply the dimension of time things

00:11:25,160 --> 00:11:31,069
actually begin to make sense what we see

00:11:28,189 --> 00:11:33,829
here is a memory a hundred percent that

00:11:31,069 --> 00:11:35,569
process was killed by the arm we

00:11:33,829 --> 00:11:37,339
triggered a new deployment because of

00:11:35,569 --> 00:11:39,439
that now if you're an container land

00:11:37,339 --> 00:11:40,639
that could be used in a latest AG very

00:11:39,439 --> 00:11:41,959
dangerous you're always going to pull

00:11:40,639 --> 00:11:43,910
stuff so you don't necessarily know

00:11:41,959 --> 00:11:45,170
what's running in production when that

00:11:43,910 --> 00:11:47,720
happened that caused a database

00:11:45,170 --> 00:11:49,009
migration to run which broke all of our

00:11:47,720 --> 00:11:50,749
tables because we weren't supposed to

00:11:49,009 --> 00:11:53,120
release it yet our health check is now

00:11:50,749 --> 00:11:54,970
failing and now our application is in

00:11:53,120 --> 00:11:58,370
some sort of crash look back off state

00:11:54,970 --> 00:12:00,379
so understanding what happened when it

00:11:58,370 --> 00:12:04,540
happened and the order that happened is

00:12:00,379 --> 00:12:04,540
how we make sense of complex situations

00:12:05,110 --> 00:12:10,730
everyone familiar with this screen no

00:12:09,379 --> 00:12:14,059
matter what you use for storing logs

00:12:10,730 --> 00:12:15,319
logs our classical time series data was

00:12:14,059 --> 00:12:19,399
the first thing we put at the beginning

00:12:15,319 --> 00:12:21,949
of every log line it tames them these

00:12:19,399 --> 00:12:23,629
are events now I'm not going to talk a

00:12:21,949 --> 00:12:25,279
lot about structure and events but if

00:12:23,629 --> 00:12:27,110
you're not writing your event or log

00:12:25,279 --> 00:12:29,089
data and JSON that can be possible by a

00:12:27,110 --> 00:12:31,399
TS DB or other system you're losing out

00:12:29,089 --> 00:12:33,139
in a lot of intrinsic value and there'll

00:12:31,399 --> 00:12:34,129
be some examples of that later but if

00:12:33,139 --> 00:12:35,269
you want to talk about structured log

00:12:34,129 --> 00:12:40,040
and just come and find me after this

00:12:35,269 --> 00:12:41,600
talk so what is time series data well we

00:12:40,040 --> 00:12:42,860
know there's anything with a timestamp

00:12:41,600 --> 00:12:44,959
but there are actually two

00:12:42,860 --> 00:12:47,809
classifications of time series data as

00:12:44,959 --> 00:12:49,879
well so what we have first is regular

00:12:47,809 --> 00:12:51,230
time series data and I'm going to try

00:12:49,879 --> 00:12:53,720
and refer to this as metrics for the

00:12:51,230 --> 00:12:56,600
rest of this talk what makes it a metric

00:12:53,720 --> 00:12:58,339
is predictably available so regular time

00:12:56,600 --> 00:13:01,220
series should always be available in the

00:12:58,339 --> 00:13:04,699
same interval examples of that would be

00:13:01,220 --> 00:13:06,920
the CPU usage and so forth a regular

00:13:04,699 --> 00:13:08,660
time series is not predictable I cannot

00:13:06,920 --> 00:13:10,370
tell you when that next event will

00:13:08,660 --> 00:13:12,649
happen and we're going to call that

00:13:10,370 --> 00:13:17,660
events it's unpredictable and

00:13:12,649 --> 00:13:19,100
inconsistent so an example of regular

00:13:17,660 --> 00:13:21,200
metrics or regular time series of

00:13:19,100 --> 00:13:22,550
metrics CPU is each

00:13:21,200 --> 00:13:24,200
mamrie your seats we're talking about

00:13:22,550 --> 00:13:26,810
Lennox system infrastructure monitoring

00:13:24,200 --> 00:13:28,040
here we could be tracking the Peng fame

00:13:26,810 --> 00:13:29,899
or laughing state to some sort of

00:13:28,040 --> 00:13:32,389
external service and we do that every

00:13:29,899 --> 00:13:33,860
ten seconds that is predictable and if

00:13:32,389 --> 00:13:35,839
we want to track the number of processes

00:13:33,860 --> 00:13:37,519
running on any machine again if we

00:13:35,839 --> 00:13:42,410
request that value every ten seconds it

00:13:37,519 --> 00:13:44,209
will always be available a regular time

00:13:42,410 --> 00:13:47,660
series of things that we cannot protect

00:13:44,209 --> 00:13:52,160
can you tell me when the next login

00:13:47,660 --> 00:13:53,720
event will happen in your system no you

00:13:52,160 --> 00:13:55,970
do not know when the next user is going

00:13:53,720 --> 00:13:57,829
to get the password incorrect nor when

00:13:55,970 --> 00:13:59,120
the CI is going to finish or if someone

00:13:57,829 --> 00:14:00,889
who's going to trip over a network cable

00:13:59,120 --> 00:14:02,810
and your data center and take that a few

00:14:00,889 --> 00:14:04,399
systems these are things that are all

00:14:02,810 --> 00:14:06,380
valuably important and should be stored

00:14:04,399 --> 00:14:07,910
somewhere but you can never predict it

00:14:06,380 --> 00:14:09,980
and the way that we handle these two

00:14:07,910 --> 00:14:15,500
different types of CDs time series data

00:14:09,980 --> 00:14:17,779
has to has to be different okay so if we

00:14:15,500 --> 00:14:20,720
have a football game there is time

00:14:17,779 --> 00:14:22,910
series data everywhere and firestarters

00:14:20,720 --> 00:14:25,339
time series data in this room as a

00:14:22,910 --> 00:14:26,839
metric any interval I can ask how many

00:14:25,339 --> 00:14:29,029
people are not in this room and I will

00:14:26,839 --> 00:14:32,149
get a number from zero to whatever

00:14:29,029 --> 00:14:34,130
so in a football game we have the exact

00:14:32,149 --> 00:14:36,170
same there are a few different types of

00:14:34,130 --> 00:14:38,420
time series data here first we have the

00:14:36,170 --> 00:14:41,060
number of people in the stadium so that

00:14:38,420 --> 00:14:44,060
would be a metric and regular we also

00:14:41,060 --> 00:14:45,440
have goals scored so they have a time

00:14:44,060 --> 00:14:46,640
stamp we know that at fifty of four

00:14:45,440 --> 00:14:48,560
minutes and seven minutes in this game

00:14:46,640 --> 00:14:48,860
there is a goal scored we know who did

00:14:48,560 --> 00:14:50,899
it

00:14:48,860 --> 00:14:52,430
that is an event and it's irregular we

00:14:50,899 --> 00:14:53,990
cannot predict when the next goal will

00:14:52,430 --> 00:14:57,980
come if we could we would all be a lot

00:14:53,990 --> 00:14:59,810
richer we also have aggregate and that's

00:14:57,980 --> 00:15:02,060
really important and time series data as

00:14:59,810 --> 00:15:03,860
well so we are actually able to

00:15:02,060 --> 00:15:07,190
calculate an aggregate score over

00:15:03,860 --> 00:15:08,329
multiple games we're now going to talk

00:15:07,190 --> 00:15:10,880
about aggregates too much today but

00:15:08,329 --> 00:15:15,529
there are a special form of metric and

00:15:10,880 --> 00:15:17,690
in fact there is some more event based

00:15:15,529 --> 00:15:19,459
data here if we take the number of

00:15:17,690 --> 00:15:21,860
people and the audience that is an

00:15:19,459 --> 00:15:23,209
aggregation of a set of events how many

00:15:21,860 --> 00:15:25,040
people come into the stadium how many

00:15:23,209 --> 00:15:27,230
people left us do if you track every one

00:15:25,040 --> 00:15:30,190
person then in one person a you have an

00:15:27,230 --> 00:15:30,190
aggregate or a metric

00:15:31,100 --> 00:15:34,520
you have to understand the difference

00:15:32,330 --> 00:15:36,830
between metrics and events and what's

00:15:34,520 --> 00:15:41,150
important is that all metrics are

00:15:36,830 --> 00:15:43,000
aggregations everything even the CPU

00:15:41,150 --> 00:15:45,650
load on your machine is an aggregation

00:15:43,000 --> 00:15:48,110
we just don't find a lot of value in

00:15:45,650 --> 00:15:50,660
store in every CPU instruction sent

00:15:48,110 --> 00:15:52,310
every instruction sent to the CPU it

00:15:50,660 --> 00:15:54,530
would be very costly because it would be

00:15:52,310 --> 00:15:58,910
billions and billions but all metrics

00:15:54,530 --> 00:16:00,530
are aggregations so how do we collect

00:15:58,910 --> 00:16:03,410
time series data I'm going to talk about

00:16:00,530 --> 00:16:05,000
a couple of ways and just from an Fox DB

00:16:03,410 --> 00:16:08,090
and for such a monitoring point of view

00:16:05,000 --> 00:16:09,830
of course you can have client libraries

00:16:08,090 --> 00:16:11,870
for PHP through right to your TS DB of

00:16:09,830 --> 00:16:13,220
choice but a lot of this work has

00:16:11,870 --> 00:16:15,950
already been done and is already

00:16:13,220 --> 00:16:17,900
available to you to be consumed so there

00:16:15,950 --> 00:16:20,090
is a project from n flux data called

00:16:17,900 --> 00:16:22,010
Telegraph it has a whole bunch of inputs

00:16:20,090 --> 00:16:23,840
and outputs the reason I'm less than the

00:16:22,010 --> 00:16:26,840
outputs is that we are a very open

00:16:23,840 --> 00:16:28,430
company we don't really care or mind if

00:16:26,840 --> 00:16:31,070
you want to use Telegraph to rate to

00:16:28,430 --> 00:16:33,740
another tsdp we don't lock you in to

00:16:31,070 --> 00:16:35,420
influx DBM we support input cyclone

00:16:33,740 --> 00:16:37,520
watch the last research Kafka jenkins

00:16:35,420 --> 00:16:38,360
kubernetes anything you have running and

00:16:37,520 --> 00:16:40,520
your infrastructure

00:16:38,360 --> 00:16:42,500
Azamat same time series data and the

00:16:40,520 --> 00:16:44,390
tooling to collect that is there and a

00:16:42,500 --> 00:16:45,620
couple of lanes of tamil so you don't

00:16:44,390 --> 00:16:48,590
even need to write anything complicated

00:16:45,620 --> 00:16:50,630
to start or nesting series if you come

00:16:48,590 --> 00:16:51,890
from a Prometheus background then

00:16:50,630 --> 00:16:53,330
there's a whole bunch of exporters that

00:16:51,890 --> 00:16:56,150
are very similar to the Telegraph

00:16:53,330 --> 00:16:57,740
plugins as well basically no matter what

00:16:56,150 --> 00:16:59,270
you run in your infrastructure there's

00:16:57,740 --> 00:17:02,930
something there to collect that data so

00:16:59,270 --> 00:17:05,030
you should begin to leverage it and

00:17:02,930 --> 00:17:06,500
anyone who has already been doing a

00:17:05,030 --> 00:17:09,020
little bit of time series research

00:17:06,500 --> 00:17:10,640
there's a bit of a dogmatic war should I

00:17:09,020 --> 00:17:10,970
pull or should I push my time series

00:17:10,640 --> 00:17:12,770
data

00:17:10,970 --> 00:17:14,420
you know Prometheus goes down the

00:17:12,770 --> 00:17:17,210
essence of you should always pull and

00:17:14,420 --> 00:17:19,010
flux DB actually supports both and the

00:17:17,210 --> 00:17:22,910
reason for that is you really do need to

00:17:19,010 --> 00:17:24,560
support both right for metrics yes you

00:17:22,910 --> 00:17:26,900
can pull that on a regular predictable

00:17:24,560 --> 00:17:28,400
interval the value will always be there

00:17:26,900 --> 00:17:29,540
and you're going to have a really good

00:17:28,400 --> 00:17:32,140
time you're going to collect the data

00:17:29,540 --> 00:17:35,660
and learn those event sites for the

00:17:32,140 --> 00:17:37,520
UNPROFOR the events and unpredictable

00:17:35,660 --> 00:17:39,290
interval stuff you cannot pull it you

00:17:37,520 --> 00:17:41,060
don't know when it's going to happen for

00:17:39,290 --> 00:17:42,020
that we do need a push approach so

00:17:41,060 --> 00:17:44,419
you're going to have to instrument your

00:17:42,020 --> 00:17:46,940
code to push those events to a tsdp you

00:17:44,419 --> 00:17:51,230
so there's not one right way you have to

00:17:46,940 --> 00:17:53,809
kind of a dog both use cases for time

00:17:51,230 --> 00:17:55,039
series data as developers you may be

00:17:53,809 --> 00:17:56,899
very familiar with monitoring your

00:17:55,039 --> 00:17:59,529
application of course the Linux machines

00:17:56,899 --> 00:18:02,869
to run on any cloud VMs and so forth

00:17:59,529 --> 00:18:04,429
IOT masses amount of time series data

00:18:02,869 --> 00:18:06,409
there you know if you have a digital

00:18:04,429 --> 00:18:07,759
thermostat in your house it's constantly

00:18:06,409 --> 00:18:10,159
checking the temperature in a manner

00:18:07,759 --> 00:18:13,159
it's constantly tweaking the water

00:18:10,159 --> 00:18:14,989
pressure and the boiler etc and of

00:18:13,159 --> 00:18:17,600
course real-time analytics have you have

00:18:14,989 --> 00:18:19,340
a one-lane and people browse to it you

00:18:17,600 --> 00:18:20,749
maybe want to track them when they go

00:18:19,340 --> 00:18:22,190
there how long they stay there when they

00:18:20,749 --> 00:18:25,249
leave where they go to where they came

00:18:22,190 --> 00:18:27,080
from all time series data the time

00:18:25,249 --> 00:18:29,809
series data literally is everywhere and

00:18:27,080 --> 00:18:35,299
our stack already and you just have to

00:18:29,809 --> 00:18:37,369
identify it now the database bit so why

00:18:35,299 --> 00:18:40,580
should I use a TS DB over any other

00:18:37,369 --> 00:18:42,379
system well the way that time series

00:18:40,580 --> 00:18:44,929
database stores data is in a very

00:18:42,379 --> 00:18:46,789
particular format right the way that we

00:18:44,929 --> 00:18:49,279
read the data is very very unique as

00:18:46,789 --> 00:18:50,869
well time series data is not small did

00:18:49,279 --> 00:18:52,759
generally we're talking millions to

00:18:50,869 --> 00:18:55,249
billions of points on a regular

00:18:52,759 --> 00:18:58,309
frequency that means that we have to

00:18:55,249 --> 00:18:59,989
have high velocity rates a wreaths are

00:18:58,309 --> 00:19:02,269
very different and unique right you

00:18:59,989 --> 00:19:04,279
would never go to a TS DB and say give

00:19:02,269 --> 00:19:07,090
me every value for this serious across

00:19:04,279 --> 00:19:09,440
all time that would be really painful

00:19:07,090 --> 00:19:11,450
generally 99% of the time you're gonna

00:19:09,440 --> 00:19:13,820
go I want all the values for the series

00:19:11,450 --> 00:19:15,200
within a specific window so the way the

00:19:13,820 --> 00:19:19,129
sharding has to happen the way the

00:19:15,200 --> 00:19:22,220
indexes are bell is all around time and

00:19:19,129 --> 00:19:24,230
because there's a very heavy cost to

00:19:22,220 --> 00:19:26,419
storing time series data just because of

00:19:24,230 --> 00:19:29,330
the pure volume and size we have to have

00:19:26,419 --> 00:19:30,739
time to live or some sort of lifecycle

00:19:29,330 --> 00:19:32,869
management for that data to either

00:19:30,739 --> 00:19:34,429
expire render upper timestamp light and

00:19:32,869 --> 00:19:36,460
so forth and we will talk about that in

00:19:34,429 --> 00:19:39,409
a minute

00:19:36,460 --> 00:19:42,529
so this graph comes from DB engines comm

00:19:39,409 --> 00:19:44,960
and they track a whole wealth of

00:19:42,529 --> 00:19:47,119
statistics about Twitter mentions Google

00:19:44,960 --> 00:19:49,399
searches active repositories on github

00:19:47,119 --> 00:19:51,049
all this other data and they try to

00:19:49,399 --> 00:19:52,940
track the growth curves of each

00:19:51,049 --> 00:19:56,059
individual data base but also databases

00:19:52,940 --> 00:19:57,739
as a category the blue line that's

00:19:56,059 --> 00:19:58,400
storming away at the top here is tame

00:19:57,739 --> 00:20:00,740
sir

00:19:58,400 --> 00:20:02,900
it's the fastest-growing category of

00:20:00,740 --> 00:20:05,480
database for the last two years so maybe

00:20:02,900 --> 00:20:07,070
a little bit longer and I think the

00:20:05,480 --> 00:20:09,170
reason this is the fastest-growing

00:20:07,070 --> 00:20:11,570
database is we're now starting to change

00:20:09,170 --> 00:20:12,770
our architectures I will make you show

00:20:11,570 --> 00:20:14,180
your hands but there are going to be a

00:20:12,770 --> 00:20:15,950
number of people in this room who have a

00:20:14,180 --> 00:20:17,720
monolithic application are talking about

00:20:15,950 --> 00:20:20,150
containerization micro services and

00:20:17,720 --> 00:20:21,620
cloud native and what we do when we make

00:20:20,150 --> 00:20:23,480
the architectural switch is that we

00:20:21,620 --> 00:20:24,470
remove a lot of complex a that we used

00:20:23,480 --> 00:20:26,810
to keep in our code to the

00:20:24,470 --> 00:20:28,520
infrastructure layer and it's that

00:20:26,810 --> 00:20:29,750
complications and infrastructure layer

00:20:28,520 --> 00:20:34,700
that have led people to make their

00:20:29,750 --> 00:20:36,740
monitoring a lot more advanced so the

00:20:34,700 --> 00:20:38,990
new stack to the study and they asked

00:20:36,740 --> 00:20:40,820
people or developers you know do you

00:20:38,990 --> 00:20:43,700
have time series data and ever you do do

00:20:40,820 --> 00:20:46,790
you store in a time series database 12%

00:20:43,700 --> 00:20:48,080
suggests so 88% of the people that had

00:20:46,790 --> 00:20:49,640
time series data weren't using a

00:20:48,080 --> 00:20:52,280
general-purpose database instead of a

00:20:49,640 --> 00:20:55,340
time series database no I think that's

00:20:52,280 --> 00:20:57,740
mildly confusing um and I don't think

00:20:55,340 --> 00:20:59,210
it's entirely accurate and what I want

00:20:57,740 --> 00:21:01,580
to show you besides just using a really

00:20:59,210 --> 00:21:03,290
cool reking more image is that you

00:21:01,580 --> 00:21:06,980
probably already are using the time

00:21:03,290 --> 00:21:10,910
series data and just we're nowhere so if

00:21:06,980 --> 00:21:14,840
your company has too much money you may

00:21:10,910 --> 00:21:16,130
be using New Relic if you have lately

00:21:14,840 --> 00:21:19,760
that's a little bit less money

00:21:16,130 --> 00:21:22,180
you may be using data doc and probably

00:21:19,760 --> 00:21:25,010
everyone who's using google analytics

00:21:22,180 --> 00:21:26,570
these are all time series databases just

00:21:25,010 --> 00:21:28,310
of a different UI showing you different

00:21:26,570 --> 00:21:30,820
stuff and metrics and measures about

00:21:28,310 --> 00:21:33,890
your applications or your websites

00:21:30,820 --> 00:21:36,200
excuse me so you probably already are

00:21:33,890 --> 00:21:39,130
using a TS DB so I think that that's

00:21:36,200 --> 00:21:41,660
that study was just a little bit flawed

00:21:39,130 --> 00:21:46,070
nice question who's Justin Cooper

00:21:41,660 --> 00:21:48,490
Nettie's okay only a few hands I sent a

00:21:46,070 --> 00:21:51,710
poll on Twitter I think it was around

00:21:48,490 --> 00:21:53,480
March last year and I said and I run

00:21:51,710 --> 00:21:56,600
kubernetes in production and I monitor

00:21:53,480 --> 00:21:58,190
with there's 74 percent of the people

00:21:56,600 --> 00:22:00,290
that responded said the realism

00:21:58,190 --> 00:22:02,060
Prometheus wait and that's great at

00:22:00,290 --> 00:22:04,370
least they have some sort of metrics of

00:22:02,060 --> 00:22:06,470
monitoring in place and because we faced

00:22:04,370 --> 00:22:08,590
is a CNC F project that number is always

00:22:06,470 --> 00:22:11,060
going to be higher than any other TS DB

00:22:08,590 --> 00:22:11,630
we have 10% using some sort of SAS

00:22:11,060 --> 00:22:15,440
linear

00:22:11,630 --> 00:22:18,950
databook we have 3% use an influx DB and

00:22:15,440 --> 00:22:20,210
13% they're using nothing now I don't

00:22:18,950 --> 00:22:23,000
know what your label of knowledge of

00:22:20,210 --> 00:22:26,060
kubernetes is but I did see a production

00:22:23,000 --> 00:22:27,650
and 13% of those people are not

00:22:26,060 --> 00:22:30,920
monitoring it and it's a big scary

00:22:27,650 --> 00:22:32,810
system so if you are in that 13% right

00:22:30,920 --> 00:22:35,360
what you learned today hopefully you can

00:22:32,810 --> 00:22:39,230
take away run and flux DB opera me visit

00:22:35,360 --> 00:22:40,220
or ever and begin to monitor that all

00:22:39,230 --> 00:22:41,600
right so it's not too late

00:22:40,220 --> 00:22:43,700
this is where we start talking about

00:22:41,600 --> 00:22:44,390
time series with an Fox DB as a few

00:22:43,700 --> 00:22:46,430
examples

00:22:44,390 --> 00:22:49,760
remember it's agnostic you don't need to

00:22:46,430 --> 00:22:54,950
use any flux DB it's just the one I'm

00:22:49,760 --> 00:22:57,140
more comfortable with okay so and feisty

00:22:54,950 --> 00:22:58,250
P is the time series database what's

00:22:57,140 --> 00:23:00,650
really cool about the company I work for

00:22:58,250 --> 00:23:03,790
and flux data is everything is open

00:23:00,650 --> 00:23:07,700
source and we have a very very small

00:23:03,790 --> 00:23:10,160
enterprise code base just to make money

00:23:07,700 --> 00:23:13,130
to keep us taken over but we are a full

00:23:10,160 --> 00:23:14,480
stack time series company well that

00:23:13,130 --> 00:23:16,340
means is we have Telegraph for

00:23:14,480 --> 00:23:18,320
collecting your metrics we give you

00:23:16,340 --> 00:23:20,360
cronograph for visualizing that for

00:23:18,320 --> 00:23:21,530
dashboarding and other tools you can use

00:23:20,360 --> 00:23:23,210
graph an ax if you're already

00:23:21,530 --> 00:23:25,610
comfortable with that it works Rayo the

00:23:23,210 --> 00:23:27,320
box we also provide capacitor which

00:23:25,610 --> 00:23:29,540
allows you to do real-time streaming and

00:23:27,320 --> 00:23:32,570
anomaly detection on top of your time to

00:23:29,540 --> 00:23:36,500
read data we are currently in a process

00:23:32,570 --> 00:23:39,710
of working on v2 of n flux DB as on beta

00:23:36,500 --> 00:23:42,680
4 as of last week and will be hat and GE

00:23:39,710 --> 00:23:45,260
within the next three months and andfox

00:23:42,680 --> 00:23:46,430
db2 is bail around containerization and

00:23:45,260 --> 00:23:48,620
kubernetes and all the other cool stuff

00:23:46,430 --> 00:23:50,470
so if you are doing that migration make

00:23:48,620 --> 00:23:54,440
it a lot of value from playing with that

00:23:50,470 --> 00:23:55,760
that's the sales pitch bit and when we

00:23:54,440 --> 00:23:57,620
talk about time series dinner we talked

00:23:55,760 --> 00:24:00,230
about points the best way to think about

00:23:57,620 --> 00:24:04,630
that is that at some point in time this

00:24:00,230 --> 00:24:06,650
equaled this if we use an example

00:24:04,630 --> 00:24:08,540
hopefully we're all relatively familiar

00:24:06,650 --> 00:24:12,200
with Linux but it has this concept of

00:24:08,540 --> 00:24:16,130
Luud averages I have a small series key

00:24:12,200 --> 00:24:18,290
on the left here the series key is made

00:24:16,130 --> 00:24:20,570
up of a measurement name so here it's

00:24:18,290 --> 00:24:22,370
called load we then have a couple of tag

00:24:20,570 --> 00:24:25,250
values so we're saying the host that's

00:24:22,370 --> 00:24:27,440
load was collected from is called VM one

00:24:25,250 --> 00:24:28,790
and the blue we have the fields so

00:24:27,440 --> 00:24:30,410
that's just the one minute the five

00:24:28,790 --> 00:24:31,760
minute and a 15-minute lute everage and

00:24:30,410 --> 00:24:33,650
then and the yellow we have the time

00:24:31,760 --> 00:24:37,370
stamp on the end because all time series

00:24:33,650 --> 00:24:39,380
data needs a time stamp a couple of

00:24:37,370 --> 00:24:41,270
examples here we could have the stock

00:24:39,380 --> 00:24:44,090
tickers so the measurement name is stock

00:24:41,270 --> 00:24:46,460
price and yellow the series key is new

00:24:44,090 --> 00:24:47,750
market and the ticker so the market we

00:24:46,460 --> 00:24:49,340
need to know which market the data came

00:24:47,750 --> 00:24:50,750
from and the ticker is a company we're

00:24:49,340 --> 00:24:52,730
tracking and there would be some value

00:24:50,750 --> 00:24:54,320
in the end and of course you can use

00:24:52,730 --> 00:24:55,850
that format for anything within your

00:24:54,320 --> 00:24:57,470
infrastructure so if you are micro

00:24:55,850 --> 00:24:59,120
service-based you could just track users

00:24:57,470 --> 00:25:03,590
which service they're having with their

00:24:59,120 --> 00:25:06,500
API calls and so forth just to cement

00:25:03,590 --> 00:25:08,660
this when we talk about the CDs what

00:25:06,500 --> 00:25:11,480
we're saying is that the tag set is the

00:25:08,660 --> 00:25:13,250
same so even though the market is Nasdaq

00:25:11,480 --> 00:25:16,580
on both of these these are different

00:25:13,250 --> 00:25:18,050
series because the teka is different so

00:25:16,580 --> 00:25:19,790
if I wanted to pull out all of Google's

00:25:18,050 --> 00:25:21,710
pricing for the last 24 hours

00:25:19,790 --> 00:25:27,020
that would be a series of points and a

00:25:21,710 --> 00:25:28,250
series now why don't you need to

00:25:27,020 --> 00:25:31,550
understand the difference between tags

00:25:28,250 --> 00:25:33,260
and fields well tags are always indexed

00:25:31,550 --> 00:25:35,360
which means they can only ever be

00:25:33,260 --> 00:25:37,490
strings that just means that when I want

00:25:35,360 --> 00:25:40,750
to pull out one very specific CDs from

00:25:37,490 --> 00:25:43,070
the TS DB is really really fast

00:25:40,750 --> 00:25:45,170
but we do want to store multiple types

00:25:43,070 --> 00:25:46,940
of data across that CDs so you can use

00:25:45,170 --> 00:25:49,280
fields which are not indexed but they do

00:25:46,940 --> 00:25:50,960
support multiple data types so they can

00:25:49,280 --> 00:25:53,270
also be strings they could be integers

00:25:50,960 --> 00:25:54,920
they could be billions and so forth but

00:25:53,270 --> 00:25:56,540
they are not indexed and doing

00:25:54,920 --> 00:25:58,580
aggregations across them can be really

00:25:56,540 --> 00:26:04,910
slow so you generally use always work

00:25:58,580 --> 00:26:06,470
with the CDs and your filter by tag so

00:26:04,910 --> 00:26:09,920
let's talk about this value and cost

00:26:06,470 --> 00:26:12,410
about time series data and in order to

00:26:09,920 --> 00:26:14,270
understand how costly time series data

00:26:12,410 --> 00:26:16,220
as we have to understand one of the most

00:26:14,270 --> 00:26:19,250
important concepts in sensitive time

00:26:16,220 --> 00:26:21,110
series and that is resolution so when I

00:26:19,250 --> 00:26:23,570
spoke about metrics earlier I spoke

00:26:21,110 --> 00:26:26,270
about predictable intervals we call that

00:26:23,570 --> 00:26:28,640
the resolution of the data so if I'm

00:26:26,270 --> 00:26:30,940
going to collect a lude average every 10

00:26:28,640 --> 00:26:33,440
seconds that is a 10-second resolution

00:26:30,940 --> 00:26:35,270
evacuate the loge average of over 30

00:26:33,440 --> 00:26:38,360
seconds or one minute that would be a 30

00:26:35,270 --> 00:26:39,070
second or 1 minute resolution and more

00:26:38,360 --> 00:26:40,779
sophisticated

00:26:39,070 --> 00:26:42,519
we get with that resolution is going to

00:26:40,779 --> 00:26:47,559
determine how expensive it as to store

00:26:42,519 --> 00:26:49,299
that theater so the value of time series

00:26:47,559 --> 00:26:51,399
data is directly correlated with the

00:26:49,299 --> 00:26:53,529
resolution and how many points of data

00:26:51,399 --> 00:26:55,750
we have you can actually reuse a formula

00:26:53,529 --> 00:26:58,149
and say 10 second resolution across 10

00:26:55,750 --> 00:26:59,500
metrics multiply them together and I

00:26:58,149 --> 00:27:00,880
have some sort of storage requirements

00:26:59,500 --> 00:27:04,299
of how many points I want to store and

00:27:00,880 --> 00:27:05,740
you can calculate that but it's more

00:27:04,299 --> 00:27:08,980
important to understand this through an

00:27:05,740 --> 00:27:10,539
example I feel so let's assume we're

00:27:08,980 --> 00:27:11,860
doing Lennox temperature monitoring I'm

00:27:10,539 --> 00:27:14,139
gonna keep this really really simple to

00:27:11,860 --> 00:27:19,120
start with we're gonna track one value

00:27:14,139 --> 00:27:22,299
so if I have one machine one CDs I have

00:27:19,120 --> 00:27:26,190
one value that's the CPU at one second

00:27:22,299 --> 00:27:28,809
resolution that means I'm going to store

00:27:26,190 --> 00:27:31,870
86,400 points per day and side of my

00:27:28,809 --> 00:27:35,529
database now in order for this example

00:27:31,870 --> 00:27:37,570
to really strike home with you what I

00:27:35,529 --> 00:27:39,039
would suggest is think about storing

00:27:37,570 --> 00:27:41,259
this number of rows and a

00:27:39,039 --> 00:27:46,090
general-purpose database as the number

00:27:41,259 --> 00:27:48,100
increases if we have two machines in our

00:27:46,090 --> 00:27:50,409
infrastructure still one single

00:27:48,100 --> 00:27:51,909
measurement at one second resolution we

00:27:50,409 --> 00:27:58,419
have a hundred seventy two thousand

00:27:51,909 --> 00:28:01,210
eight hundred points per day a modest

00:27:58,419 --> 00:28:04,059
production infrastructure may have ten

00:28:01,210 --> 00:28:05,470
machines which is ten series and you're

00:28:04,059 --> 00:28:08,409
going to want to track more than the CPU

00:28:05,470 --> 00:28:09,490
CPU memory disk i/o a couple other

00:28:08,409 --> 00:28:11,679
fields so we're gonna have five

00:28:09,490 --> 00:28:15,159
measurements we're still gonna track at

00:28:11,679 --> 00:28:16,659
once at one second resolution and all of

00:28:15,159 --> 00:28:19,450
a sudden we're jumping up to four

00:28:16,659 --> 00:28:20,620
million points per day again think of a

00:28:19,450 --> 00:28:22,389
lesson concept of a general-purpose

00:28:20,620 --> 00:28:25,240
database er are you comfortable store in

00:28:22,389 --> 00:28:28,570
formalin points that idea depends on the

00:28:25,240 --> 00:28:30,490
database but maybe not now let's jump to

00:28:28,570 --> 00:28:32,740
a real example of time series data we

00:28:30,490 --> 00:28:34,179
have financial trading on the Nasdaq

00:28:32,740 --> 00:28:36,610
there are three thousand three hundred

00:28:34,179 --> 00:28:38,019
companies let's just assume we're only

00:28:36,610 --> 00:28:40,210
tracking the cost of the ticker and

00:28:38,019 --> 00:28:42,669
because it's financial trading we may

00:28:40,210 --> 00:28:45,549
have sub-second resolution here we're

00:28:42,669 --> 00:28:46,960
going to use one millisecond realistic

00:28:45,549 --> 00:28:49,029
lamb and I speak to financial companies

00:28:46,960 --> 00:28:50,070
they are using nanosecond resolution on

00:28:49,029 --> 00:28:51,960
the trading data

00:28:50,070 --> 00:28:53,220
so

00:28:51,960 --> 00:28:55,980
I just couldn't offend that number on

00:28:53,220 --> 00:28:59,490
the screen but it's a really big number

00:28:55,980 --> 00:29:02,130
pair D right very very very difficult

00:28:59,490 --> 00:29:04,380
for anyone to store that metadata and a

00:29:02,130 --> 00:29:06,059
single database but time series

00:29:04,380 --> 00:29:10,470
databases are built to do lists and they

00:29:06,059 --> 00:29:13,050
will handle it much better so what

00:29:10,470 --> 00:29:14,540
happens if we take the one millisecond

00:29:13,050 --> 00:29:16,890
resolution and drop it to one minute

00:29:14,540 --> 00:29:20,760
well we're back down to the millions

00:29:16,890 --> 00:29:23,100
which is good what if we drop it to an

00:29:20,760 --> 00:29:24,720
error and seventy-nine thousand points

00:29:23,100 --> 00:29:26,850
right we're now and the comfortable

00:29:24,720 --> 00:29:28,710
territory regardless of what database we

00:29:26,850 --> 00:29:30,030
use we are feeling a little bit more

00:29:28,710 --> 00:29:34,230
comfortable that store not on a daily

00:29:30,030 --> 00:29:37,110
basis and if we jump to a sex our

00:29:34,230 --> 00:29:38,910
resolution we're at 13,000 points you

00:29:37,110 --> 00:29:40,800
could store that in a JSON fail on the

00:29:38,910 --> 00:29:42,570
desk and load it every time and you

00:29:40,800 --> 00:29:45,600
would still be happy with that

00:29:42,570 --> 00:29:49,400
so sex our resolution were very very

00:29:45,600 --> 00:29:49,400
comfortable now this is good

00:29:49,520 --> 00:29:54,420
here's my wonderful draft and to do this

00:29:53,880 --> 00:29:58,440
myself

00:29:54,420 --> 00:30:01,230
it's not my kid but what's important to

00:29:58,440 --> 00:30:04,490
understand here we have the dimension of

00:30:01,230 --> 00:30:07,470
time and we have the value of the data

00:30:04,490 --> 00:30:09,510
and what happens is if I have one

00:30:07,470 --> 00:30:12,360
millisecond of fact I have 10 seconds

00:30:09,510 --> 00:30:14,940
resolution that data is valuable to a

00:30:12,360 --> 00:30:17,040
certain point and then that level of

00:30:14,940 --> 00:30:18,809
granularity a resolution no longer is

00:30:17,040 --> 00:30:20,970
that important to me anymore you know

00:30:18,809 --> 00:30:23,820
once my time series data as well a day

00:30:20,970 --> 00:30:26,370
or a week old do I really need that

00:30:23,820 --> 00:30:28,280
level of resolution no for real-time

00:30:26,370 --> 00:30:31,620
observer Bel Air debug ability

00:30:28,280 --> 00:30:36,390
definitely so then we can maybe move it

00:30:31,620 --> 00:30:39,390
and we go to one-hour resolution so the

00:30:36,390 --> 00:30:41,040
Purple Line is the the resolution being

00:30:39,390 --> 00:30:42,480
changed the green one is just your

00:30:41,040 --> 00:30:44,790
values without ever changed in a

00:30:42,480 --> 00:30:46,410
resolution after an error were still

00:30:44,790 --> 00:30:48,210
getting more value and what we really

00:30:46,410 --> 00:30:50,640
want to achieve is this top four plane

00:30:48,210 --> 00:30:52,140
we do want to change the resolution and

00:30:50,640 --> 00:30:52,679
we want to maintain as much value as

00:30:52,140 --> 00:30:54,870
possible

00:30:52,679 --> 00:30:56,460
but the longer we store the data when

00:30:54,870 --> 00:30:58,910
it's not important to us anymore we have

00:30:56,460 --> 00:31:01,230
to down sample it or delete it and

00:30:58,910 --> 00:31:03,720
that's what it's called and time series

00:31:01,230 --> 00:31:05,630
data you have to down sample you may

00:31:03,720 --> 00:31:08,240
pain some systems talk about

00:31:05,630 --> 00:31:09,980
rolling up the data calculating averages

00:31:08,240 --> 00:31:11,630
is all the same thing

00:31:09,980 --> 00:31:13,580
so don't sampling data with time Sears

00:31:11,630 --> 00:31:15,860
as one of the most important things that

00:31:13,580 --> 00:31:19,300
you can do especially as you move from

00:31:15,860 --> 00:31:22,630
the thousands to the millions of points

00:31:19,300 --> 00:31:26,000
an Fox DB makes us really really easy

00:31:22,630 --> 00:31:28,430
this is called a continuous query it

00:31:26,000 --> 00:31:30,490
runs on a regular interval that pulls

00:31:28,430 --> 00:31:32,570
out data from a protectable of wendel

00:31:30,490 --> 00:31:35,540
calculates the average and then stores

00:31:32,570 --> 00:31:38,750
and a different retention policy for

00:31:35,540 --> 00:31:40,160
longer so what most people use in flux

00:31:38,750 --> 00:31:42,800
DB do is collect data at high

00:31:40,160 --> 00:31:45,680
resolutions in one second it'll keep

00:31:42,800 --> 00:31:48,110
that for one day and if it also done a

00:31:45,680 --> 00:31:50,680
day they calculate the average for the

00:31:48,110 --> 00:31:53,090
day and store it for a week or a month

00:31:50,680 --> 00:31:54,740
and if an older than a month they

00:31:53,090 --> 00:31:57,410
calculate the average for a week and

00:31:54,740 --> 00:31:59,660
store that for a year so you have that

00:31:57,410 --> 00:32:01,790
nice value curve going down slowly and

00:31:59,660 --> 00:32:02,780
you're reducing the points the same as

00:32:01,790 --> 00:32:04,940
we dealt with the Nasdaq

00:32:02,780 --> 00:32:08,450
you know we went from the 200 billion

00:32:04,940 --> 00:32:10,940
points per day to 18,000 and it's as

00:32:08,450 --> 00:32:13,820
simple as a 5-line continuous query like

00:32:10,940 --> 00:32:15,560
that so it's really important it's the

00:32:13,820 --> 00:32:17,960
one thing that I always say to people if

00:32:15,560 --> 00:32:20,360
you leave this talk with nothing but yes

00:32:17,960 --> 00:32:23,980
that's good right roll-ups don't

00:32:20,360 --> 00:32:23,980
something really really important

00:32:24,280 --> 00:32:29,120
unfortunately for events and irregular

00:32:27,080 --> 00:32:31,130
time series you cannot calculate the

00:32:29,120 --> 00:32:33,140
average right there's no predictability

00:32:31,130 --> 00:32:35,630
there the average would tell is nothing

00:32:33,140 --> 00:32:37,790
so there's an entire wealth of

00:32:35,630 --> 00:32:39,440
information on doing normally an outlier

00:32:37,790 --> 00:32:41,270
detection you can Google for that with

00:32:39,440 --> 00:32:42,980
an Fox DB there are plenty of Doc's I

00:32:41,270 --> 00:32:44,720
definitely do not have time to go into

00:32:42,980 --> 00:32:46,220
this today there are approaches used to

00:32:44,720 --> 00:32:49,430
do it so if you want to store your logs

00:32:46,220 --> 00:32:50,780
and say it event fox DB you can and

00:32:49,430 --> 00:32:56,420
there's a really good way to sample that

00:32:50,780 --> 00:32:57,680
over time ok so now that we have all of

00:32:56,420 --> 00:32:59,390
this time series data what does that

00:32:57,680 --> 00:33:00,830
mean for the monitoring or for

00:32:59,390 --> 00:33:04,340
applications on our infrastructure what

00:33:00,830 --> 00:33:05,750
can we do with it let's talk about the

00:33:04,340 --> 00:33:08,630
most simplest architecture in the world

00:33:05,750 --> 00:33:11,630
we have one application that speaks to

00:33:08,630 --> 00:33:14,840
one database this is the monolithic

00:33:11,630 --> 00:33:17,030
architecture if we want to monitor this

00:33:14,840 --> 00:33:18,410
system and I'm only going to speak for

00:33:17,030 --> 00:33:19,130
my own experience here I'm not saying

00:33:18,410 --> 00:33:22,160
this is necessary

00:33:19,130 --> 00:33:23,860
everyone does but nearly 2,000 what we

00:33:22,160 --> 00:33:26,900
would do is self check based monitoring

00:33:23,860 --> 00:33:28,970
check based monitoring says F the CPU

00:33:26,900 --> 00:33:32,600
ever goes above eighty percent I'm gonna

00:33:28,970 --> 00:33:34,760
reboot the machine as the memory usage

00:33:32,600 --> 00:33:37,490
ever goes above eighty percent I'm going

00:33:34,760 --> 00:33:39,470
to reboot the machine if the response

00:33:37,490 --> 00:33:41,180
time of my application has about 300

00:33:39,470 --> 00:33:44,960
milliseconds I'm gonna reboot the

00:33:41,180 --> 00:33:47,510
machine Black Friday I used to work on

00:33:44,960 --> 00:33:48,890
e-commerce stores we would go out we

00:33:47,510 --> 00:33:50,450
would buy a whole bunch of new machines

00:33:48,890 --> 00:33:53,150
we'd stick them on our racks and then we

00:33:50,450 --> 00:33:56,090
sell them two weeks later that's how we

00:33:53,150 --> 00:33:58,370
handled scale back in those days this

00:33:56,090 --> 00:33:59,960
was just really simple right when you

00:33:58,370 --> 00:34:02,350
have one application speak into one

00:33:59,960 --> 00:34:07,270
database life is easy

00:34:02,350 --> 00:34:10,340
kinda so in this system

00:34:07,270 --> 00:34:13,639
when do we send a message to our DevOps

00:34:10,340 --> 00:34:15,020
or sre our operations people and

00:34:13,639 --> 00:34:16,669
generally we just have a health check on

00:34:15,020 --> 00:34:19,159
our application and if that begins to

00:34:16,669 --> 00:34:20,330
fail we page someone and they have to go

00:34:19,159 --> 00:34:22,639
look at the logs and work out why that's

00:34:20,330 --> 00:34:24,830
failing again things are a lot simpler

00:34:22,639 --> 00:34:26,360
in the early 2000s and if you're lucky

00:34:24,830 --> 00:34:29,840
enough to still be on this architecture

00:34:26,360 --> 00:34:31,159
today and very envious but then of

00:34:29,840 --> 00:34:32,360
course we started to do horizontal

00:34:31,159 --> 00:34:33,770
scaling we'd have more than one

00:34:32,360 --> 00:34:35,659
applications they'll speak into one

00:34:33,770 --> 00:34:38,659
database and that got a little bit

00:34:35,659 --> 00:34:40,810
trickier how do we send a page from

00:34:38,659 --> 00:34:43,490
written horizontal scalability does a

00:34:40,810 --> 00:34:47,300
health check feeling on one of our nodes

00:34:43,490 --> 00:34:48,889
mean we should page someone know what we

00:34:47,300 --> 00:34:51,050
actually used to do was just kind of

00:34:48,889 --> 00:34:53,149
work out how many 500s we had and if it

00:34:51,050 --> 00:34:54,379
was a certain quantile eventually we

00:34:53,149 --> 00:34:56,810
would page someone and they would have

00:34:54,379 --> 00:34:59,320
to go and look and see what was wrong so

00:34:56,810 --> 00:35:04,280
things were still okay at this point

00:34:59,320 --> 00:35:06,520
fast forward for the modern day and this

00:35:04,280 --> 00:35:10,000
is generally what we're working with

00:35:06,520 --> 00:35:12,530
so we have microservices now we have

00:35:10,000 --> 00:35:14,210
independent scalable services service B

00:35:12,530 --> 00:35:17,120
has two instances here we could service

00:35:14,210 --> 00:35:18,770
one service C has a canary deployment

00:35:17,120 --> 00:35:20,330
because we wanted to do traffic shaping

00:35:18,770 --> 00:35:23,080
and sent ten percent of our traffic to

00:35:20,330 --> 00:35:25,340
ones and dividual a new version of

00:35:23,080 --> 00:35:27,260
course all our networks are virtualized

00:35:25,340 --> 00:35:28,460
now whether we're in the cloud are using

00:35:27,260 --> 00:35:29,840
kubernetes CNAs

00:35:28,460 --> 00:35:31,710
everything goes through some sort of

00:35:29,840 --> 00:35:33,750
Software Defined Networking there

00:35:31,710 --> 00:35:36,480
each micro-service genuinely speaks to

00:35:33,750 --> 00:35:39,420
its own database and those databases

00:35:36,480 --> 00:35:40,500
vary in different types and of course

00:35:39,420 --> 00:35:42,300
we've got the service mesh because

00:35:40,500 --> 00:35:44,250
everybody really needs a service mess

00:35:42,300 --> 00:35:46,040
now so over proxy in all of our traffic

00:35:44,250 --> 00:35:48,839
through that as well

00:35:46,040 --> 00:35:54,300
how do we even begin to understand if

00:35:48,839 --> 00:35:55,950
this system is healthy so there is a

00:35:54,300 --> 00:35:59,369
cloud native convenience versus cost

00:35:55,950 --> 00:36:01,859
argument I guess it's really difficult

00:35:59,369 --> 00:36:04,230
when you make this migration to micro

00:36:01,859 --> 00:36:06,839
services to continue to in check based

00:36:04,230 --> 00:36:08,940
monitoring in fact you can't there is no

00:36:06,839 --> 00:36:10,560
way to tell the system is healthy using

00:36:08,940 --> 00:36:11,849
check based monitoring and I'm talking

00:36:10,560 --> 00:36:13,710
about systems like Maggie Austin this

00:36:11,849 --> 00:36:17,099
thing right we have to spit start using

00:36:13,710 --> 00:36:18,810
TS TPS that's why the DB engines graph

00:36:17,099 --> 00:36:20,520
is going up at the fastest-growing rate

00:36:18,810 --> 00:36:23,190
because many many companies and teams

00:36:20,520 --> 00:36:25,589
are adopting this so we can no longer

00:36:23,190 --> 00:36:28,050
treat the symptoms we really need to

00:36:25,589 --> 00:36:30,210
understand the system as a whole and

00:36:28,050 --> 00:36:34,050
determine how to fix it so we have to

00:36:30,210 --> 00:36:36,420
upgrade our monitoring and what we're

00:36:34,050 --> 00:36:39,630
talking about as causality how can i

00:36:36,420 --> 00:36:41,070
pain the root cause and fix it and not

00:36:39,630 --> 00:36:43,109
just that but how can I do that quickly

00:36:41,070 --> 00:36:46,170
and efficiently with a major impact to

00:36:43,109 --> 00:36:48,890
my users or customers if I had an answer

00:36:46,170 --> 00:36:52,619
for that that would be great wouldn't it

00:36:48,890 --> 00:36:54,210
but when we use a ten juries database we

00:36:52,619 --> 00:36:57,119
have access to weeks months or years of

00:36:54,210 --> 00:36:59,220
data at different resolution so we can

00:36:57,119 --> 00:37:02,490
leverage that for statistic modeling to

00:36:59,220 --> 00:37:03,869
understand our systems we have tags that

00:37:02,490 --> 00:37:06,330
are indexed on all of our time series

00:37:03,869 --> 00:37:08,070
data not only do we understand the type

00:37:06,330 --> 00:37:10,080
of user we have what services they're

00:37:08,070 --> 00:37:11,910
having what region they're coming from

00:37:10,080 --> 00:37:13,560
all of this is tagged based data that

00:37:11,910 --> 00:37:16,050
can reapply to the time series or the

00:37:13,560 --> 00:37:17,820
events and we have a wealth of

00:37:16,050 --> 00:37:19,650
statistical methods available to ism'

00:37:17,820 --> 00:37:21,810
Prometheus and n flux and data dog and

00:37:19,650 --> 00:37:24,089
Uralic well that's just a list of sex

00:37:21,810 --> 00:37:25,800
there are dozens of statistical methods

00:37:24,089 --> 00:37:28,410
you do not need to know how they work

00:37:25,800 --> 00:37:29,790
and you just call the function you pass

00:37:28,410 --> 00:37:31,950
in some data and it's going to give you

00:37:29,790 --> 00:37:33,390
some sort of distribution or graph or

00:37:31,950 --> 00:37:36,480
chart that allows you to understand your

00:37:33,390 --> 00:37:40,320
system so I've got a couple of examples

00:37:36,480 --> 00:37:44,700
and a previous life I was an SRE

00:37:40,320 --> 00:37:46,970
I wasn't a good one but I was an SRE

00:37:44,700 --> 00:37:50,880
and my biggest complaint about that job

00:37:46,970 --> 00:37:53,730
was being paged at 4:00 a.m. with a desk

00:37:50,880 --> 00:37:55,589
earlier hopefully a few people have had

00:37:53,730 --> 00:37:57,299
this you get woken up at 4 a.m. because

00:37:55,589 --> 00:38:00,569
the desk is just across ninety percent

00:37:57,299 --> 00:38:03,210
capacity now because I was a terrible

00:38:00,569 --> 00:38:05,849
sre I have it fumble through it my

00:38:03,210 --> 00:38:09,420
office get my laptop Association to the

00:38:05,849 --> 00:38:10,799
Machine and tape RM minus RF and delete

00:38:09,420 --> 00:38:13,160
as much as humanly possible from that

00:38:10,799 --> 00:38:15,750
machine so that I could go back to bed I

00:38:13,160 --> 00:38:19,140
mean I deleted for temp and then I

00:38:15,750 --> 00:38:21,030
deleted for cash and then I deleted var

00:38:19,140 --> 00:38:25,020
log because I'm never gonna need a log

00:38:21,030 --> 00:38:27,329
theater anymore but it worked my machine

00:38:25,020 --> 00:38:29,640
will be back to 30% desk utilization I

00:38:27,329 --> 00:38:30,089
go back to bed mental note I'll fix up

00:38:29,640 --> 00:38:34,079
tomorrow

00:38:30,089 --> 00:38:37,440
I never effects that tomorrow and that

00:38:34,079 --> 00:38:39,210
just repeats it this happens a lot no

00:38:37,440 --> 00:38:41,010
what if we had this this time series

00:38:39,210 --> 00:38:43,500
data what if I wasn't doing check based

00:38:41,010 --> 00:38:46,319
monitoring well I could actually track

00:38:43,500 --> 00:38:47,579
the growth on that desk over time which

00:38:46,319 --> 00:38:49,200
means I can use linear prediction

00:38:47,579 --> 00:38:52,430
algorithms that are all provided in

00:38:49,200 --> 00:38:54,660
every TS DB to protect ahead of time

00:38:52,430 --> 00:38:56,160
hopefully one or two p.m. in the

00:38:54,660 --> 00:38:59,490
afternoon when I've got my good brain on

00:38:56,160 --> 00:39:01,440
and see there's a really highly probable

00:38:59,490 --> 00:39:04,470
chance this desk is gonna fill up at 40

00:39:01,440 --> 00:39:06,240
M I'm gonna page you know and that's

00:39:04,470 --> 00:39:09,210
what we need this is the value in time

00:39:06,240 --> 00:39:11,760
series data using that monitoring of

00:39:09,210 --> 00:39:13,710
growth or regression tracking over time

00:39:11,760 --> 00:39:16,859
and sending alerts to actually improve

00:39:13,710 --> 00:39:20,220
your infrastructure and your job and of

00:39:16,859 --> 00:39:22,049
course if we use loads of tags that show

00:39:20,220 --> 00:39:23,700
which process is right into the desk at

00:39:22,049 --> 00:39:25,559
what speeds we can ask to track down the

00:39:23,700 --> 00:39:27,839
problem demand this problem process the

00:39:25,559 --> 00:39:32,670
problem application and work it wise

00:39:27,839 --> 00:39:35,400
producing so many logs the next thing I

00:39:32,670 --> 00:39:38,280
want to talk about is as we adopt micro

00:39:35,400 --> 00:39:39,809
services how do we understand every

00:39:38,280 --> 00:39:42,630
users are having a happy time on our

00:39:39,809 --> 00:39:46,190
website and histograms are really really

00:39:42,630 --> 00:39:50,089
important the problem of histograms is

00:39:46,190 --> 00:39:52,859
there are two different variations know

00:39:50,089 --> 00:39:55,799
some of you may be very familiar with

00:39:52,859 --> 00:39:57,390
this and this is how we instrument our

00:39:55,799 --> 00:39:58,140
cords at present using tools like

00:39:57,390 --> 00:40:01,079
prometheus

00:39:58,140 --> 00:40:04,109
and a few other reasons we define these

00:40:01,079 --> 00:40:07,349
buckets and our applications so the

00:40:04,109 --> 00:40:08,970
request comes then we say oh we're about

00:40:07,349 --> 00:40:12,029
to respond to this the time says the

00:40:08,970 --> 00:40:13,470
requests come n was 311 milliseconds we

00:40:12,029 --> 00:40:15,950
stick it into a bucket in our

00:40:13,470 --> 00:40:18,450
application and send that to her tsdp

00:40:15,950 --> 00:40:21,529
the problem with this approach is we're

00:40:18,450 --> 00:40:24,150
pre aggregating the event based data as

00:40:21,529 --> 00:40:25,890
my application always gonna respond to

00:40:24,150 --> 00:40:27,690
one of those buckets are there any

00:40:25,890 --> 00:40:30,390
scenarios where I can change how do I

00:40:27,690 --> 00:40:33,539
make that dynamic if you want to change

00:40:30,390 --> 00:40:36,390
this you have to deploy and then you

00:40:33,539 --> 00:40:37,859
lose all previous data well at least

00:40:36,390 --> 00:40:40,079
your previous data will have the old

00:40:37,859 --> 00:40:42,089
buckets your application has had massive

00:40:40,079 --> 00:40:43,349
speed improvements and now everything is

00:40:42,089 --> 00:40:45,990
in the first bucket there's no value

00:40:43,349 --> 00:40:48,539
there there's no distribution so we

00:40:45,990 --> 00:40:50,309
cannot use histograms and an effective

00:40:48,539 --> 00:40:52,079
method immortan architectures with pre

00:40:50,309 --> 00:40:55,529
aggregated buckets so that's your

00:40:52,079 --> 00:40:57,660
warning what we really want to be able

00:40:55,529 --> 00:41:00,150
to do is do the histogram at query time

00:40:57,660 --> 00:41:02,640
pass it in a time series of data using

00:41:00,150 --> 00:41:04,980
dynamic buckets to understand we're

00:41:02,640 --> 00:41:07,680
looking at 24 hours of data calculate

00:41:04,980 --> 00:41:10,109
some means do Duke we retain bucketing

00:41:07,680 --> 00:41:11,609
but not only that we's Emily forward all

00:41:10,109 --> 00:41:13,769
of that data for something called mood

00:41:11,609 --> 00:41:15,450
what does more than I was to do well it

00:41:13,769 --> 00:41:17,220
allows us to generate distributions from

00:41:15,450 --> 00:41:19,859
highly available tag sets within that

00:41:17,220 --> 00:41:22,910
data so if we talk about that in a more

00:41:19,859 --> 00:41:26,190
practical example if we have a

00:41:22,910 --> 00:41:28,859
percentage of our users that are

00:41:26,190 --> 00:41:32,309
experienced and one-second response time

00:41:28,859 --> 00:41:34,410
that's bad if we grab that data and

00:41:32,309 --> 00:41:36,059
zoomin to it pack up that bucket past a

00:41:34,410 --> 00:41:37,710
certain mode it's going to analyze all

00:41:36,059 --> 00:41:39,240
of the tag sets for that data and tell

00:41:37,710 --> 00:41:42,029
us which tags are most frequently used

00:41:39,240 --> 00:41:44,069
and it may be that for our premium

00:41:42,029 --> 00:41:45,930
customers we inject an extra set of

00:41:44,069 --> 00:41:48,000
scripts of JavaScript it does some sort

00:41:45,930 --> 00:41:49,619
of extra tracking which is delay in some

00:41:48,000 --> 00:41:51,509
sort of response or luteum on the client

00:41:49,619 --> 00:41:52,980
side of the browser and just by using

00:41:51,509 --> 00:41:54,930
the modality function we can identify

00:41:52,980 --> 00:42:00,960
that really really quickly and that

00:41:54,930 --> 00:42:04,019
query time and finally because I have a

00:42:00,960 --> 00:42:06,660
bit of an e-commerce background one of

00:42:04,019 --> 00:42:08,069
the toughest things we had was under how

00:42:06,660 --> 00:42:09,630
many servers we were gonna need to

00:42:08,069 --> 00:42:10,740
handle events like Black Friday or

00:42:09,630 --> 00:42:12,870
Christmas and so

00:42:10,740 --> 00:42:15,270
and there's still a really difficult

00:42:12,870 --> 00:42:17,730
problem I also used to work for a media

00:42:15,270 --> 00:42:20,250
company that there'd rock and metal news

00:42:17,730 --> 00:42:22,380
and the biggest rock and metal news in

00:42:20,250 --> 00:42:25,350
the last 20 years was probably the death

00:42:22,380 --> 00:42:28,740
of Lemmy and we could not scale for that

00:42:25,350 --> 00:42:30,720
but fortunately well no unfortunately if

00:42:28,740 --> 00:42:32,460
we were using a tank series database we

00:42:30,720 --> 00:42:34,980
could have ran scenarios through

00:42:32,460 --> 00:42:36,990
something called Holt winters protection

00:42:34,980 --> 00:42:38,760
major events and trying to have it taken

00:42:36,990 --> 00:42:41,250
at a series that they are from lakin

00:42:38,760 --> 00:42:43,170
lake events and applying some sort of

00:42:41,250 --> 00:42:45,840
scale to it to work out how many servers

00:42:43,170 --> 00:42:47,850
or machines or VMs I would need to

00:42:45,840 --> 00:42:50,130
handle certain failure scenarios so

00:42:47,850 --> 00:42:53,220
whole Welters is again available in most

00:42:50,130 --> 00:42:55,290
ESD B's you just pass on data and look

00:42:53,220 --> 00:42:58,280
for SEC like they are things that repeat

00:42:55,290 --> 00:43:02,040
very very handy and very very convenient

00:42:58,280 --> 00:43:04,230
and the whole point of this is that once

00:43:02,040 --> 00:43:06,480
we have claim series data allows us to

00:43:04,230 --> 00:43:08,700
build our automation determine root

00:43:06,480 --> 00:43:11,280
cause analysis use historical data and

00:43:08,700 --> 00:43:13,080
that's just the start

00:43:11,280 --> 00:43:14,460
right we can then start to do protection

00:43:13,080 --> 00:43:15,150
of machine learning on that stream of

00:43:14,460 --> 00:43:16,800
data as well

00:43:15,150 --> 00:43:19,530
there are wealth of tools so who can in

00:43:16,800 --> 00:43:21,390
flux VB Prometheus through Kafka that

00:43:19,530 --> 00:43:24,510
run with machine learning models on your

00:43:21,390 --> 00:43:27,990
time series data to do security threat

00:43:24,510 --> 00:43:29,210
detection anomalies and so forth very

00:43:27,990 --> 00:43:33,270
very cool

00:43:29,210 --> 00:43:34,680
so in summary please use a TS DB do not

00:43:33,270 --> 00:43:37,740
use a general purpose database do time

00:43:34,680 --> 00:43:39,600
series data please roll up and down some

00:43:37,740 --> 00:43:41,070
of your data as much as possible all

00:43:39,600 --> 00:43:43,140
right it's very expensive to score

00:43:41,070 --> 00:43:44,880
billions very expensive to serve

00:43:43,140 --> 00:43:46,770
billions of points so you have to

00:43:44,880 --> 00:43:49,280
understand how long is able to you and

00:43:46,770 --> 00:43:51,750
plan your done something appropriately

00:43:49,280 --> 00:43:53,760
you can run out IR an event based

00:43:51,750 --> 00:43:57,000
detection over an outlier and anomaly

00:43:53,760 --> 00:43:58,950
detection on your events and really just

00:43:57,000 --> 00:44:00,660
build as much tooling dashboarding and

00:43:58,950 --> 00:44:02,040
automation as possible with this data

00:44:00,660 --> 00:44:03,900
there's no point in storing it for it

00:44:02,040 --> 00:44:07,530
they're not to be consumed and used to

00:44:03,900 --> 00:44:10,050
your advantage so that's all I've got

00:44:07,530 --> 00:44:11,910
thank you very much and I hope your

00:44:10,050 --> 00:44:14,060
interest in territories did up a little

00:44:11,910 --> 00:44:14,060

YouTube URL: https://www.youtube.com/watch?v=_Wul7Qg6icM


