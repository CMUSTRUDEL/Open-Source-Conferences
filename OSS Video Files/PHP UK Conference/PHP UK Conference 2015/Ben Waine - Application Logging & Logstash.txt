Title: Ben Waine - Application Logging & Logstash
Publication date: 2015-04-15
Playlist: PHP UK Conference 2015
Description: 
	Modern web applications are complex, have many layers and usually integrate many technologies. A well structured application log is invaluable for debugging your application in development and monitoring it in production. All this being said itâ€™s amazing how many applications don't have an application log or have a poor, inconsistent one which is little use to anyone! This session takes a brief look at the basics of logging (libraries and tools) and moves on to look at how to plan an effective logging strategy for your application. Questions like: What to log, logging levels, how much to log and how long to keep historical logs are all addressed. In the second half of the session we consider logs as a stream of events and how we can use Logstash and Kibana to surface a wealth of interesting information about our applications.
Captions: 
	00:00:05,470 --> 00:00:10,850
so my name is Ben wayna I've worked with

00:00:08,120 --> 00:00:13,160
PHP for about five years I'm currently a

00:00:10,850 --> 00:00:14,470
software engineer at Sainsbury's and

00:00:13,160 --> 00:00:16,310
this is the plug bit of the talk

00:00:14,470 --> 00:00:18,140
Sainsbury's are currently building out a

00:00:16,310 --> 00:00:20,480
whole new team we're using technologies

00:00:18,140 --> 00:00:23,840
like log stash so we'd love to hear from

00:00:20,480 --> 00:00:25,910
you at this our store I also

00:00:23,840 --> 00:00:27,410
occasionally dabble in DevOps which the

00:00:25,910 --> 00:00:31,849
amusement of the people we actually pay

00:00:27,410 --> 00:00:33,860
to do that job at Sainsbury's um this is

00:00:31,849 --> 00:00:35,720
the joined in link to the talk so if

00:00:33,860 --> 00:00:38,570
you've got any feedback at the end or in

00:00:35,720 --> 00:00:42,649
the middle and please do send some

00:00:38,570 --> 00:00:45,859
feedback my way so without further ado

00:00:42,649 --> 00:00:48,350
moving on to logging when I talk about

00:00:45,859 --> 00:00:49,460
logging and logging your application I'm

00:00:48,350 --> 00:00:51,769
actually talking about two different

00:00:49,460 --> 00:00:54,320
types of logging log stash is going to

00:00:51,769 --> 00:00:56,179
help us with both of those and the first

00:00:54,320 --> 00:00:58,570
one is a quick win really because it's

00:00:56,179 --> 00:01:01,519
just the system logs which all of your

00:00:58,570 --> 00:01:05,600
technology choices generates so for

00:01:01,519 --> 00:01:08,330
example MySQL slow query log Apache and

00:01:05,600 --> 00:01:09,650
nginx access logs and for information

00:01:08,330 --> 00:01:12,890
about what type of requests are coming

00:01:09,650 --> 00:01:16,250
in also you've got PHP fpm where a lot

00:01:12,890 --> 00:01:17,720
of errors creep in and then all of the

00:01:16,250 --> 00:01:20,060
other pieces of your infrastructure will

00:01:17,720 --> 00:01:21,950
generate logs and you should be

00:01:20,060 --> 00:01:24,020
capturing those logs in order to solve

00:01:21,950 --> 00:01:26,690
problems and also to look at trends in

00:01:24,020 --> 00:01:29,030
your infrastructure so these bits come

00:01:26,690 --> 00:01:31,610
for free and the next bit takes a little

00:01:29,030 --> 00:01:34,880
bit more effort to implement which is

00:01:31,610 --> 00:01:36,860
the application log a while ago I did a

00:01:34,880 --> 00:01:39,290
talk about application logging and I was

00:01:36,860 --> 00:01:41,630
surprised by how many people don't have

00:01:39,290 --> 00:01:44,630
a log for their application it's really

00:01:41,630 --> 00:01:45,950
useful in terms of debugging and so how

00:01:44,630 --> 00:01:49,100
do you get information about your

00:01:45,950 --> 00:01:51,320
application at runtime when you can't

00:01:49,100 --> 00:01:53,510
just fire up a debugging session but

00:01:51,320 --> 00:01:55,850
it's also really useful for looking at

00:01:53,510 --> 00:02:00,290
trends of your of your application over

00:01:55,850 --> 00:02:02,600
time so what should we be putting in an

00:02:00,290 --> 00:02:03,710
application log the first thing the

00:02:02,600 --> 00:02:05,690
first and most important thing is

00:02:03,710 --> 00:02:07,610
debugging information so at some point

00:02:05,690 --> 00:02:09,649
it will be four o'clock in the morning

00:02:07,610 --> 00:02:11,269
some of your servers will be on fire and

00:02:09,649 --> 00:02:13,850
you'll be trying to figure out what the

00:02:11,269 --> 00:02:15,920
hell's going on so you should be logging

00:02:13,850 --> 00:02:17,010
errors and these normally are things

00:02:15,920 --> 00:02:19,049
like connection

00:02:17,010 --> 00:02:21,510
to a remote server which isn't there

00:02:19,049 --> 00:02:23,849
anymore that could cause an error on

00:02:21,510 --> 00:02:25,860
core business exceptions so perhaps a

00:02:23,849 --> 00:02:28,530
situation has arisen in your app that

00:02:25,860 --> 00:02:30,239
you didn't think to handle those should

00:02:28,530 --> 00:02:32,010
be in your log and also resource

00:02:30,239 --> 00:02:33,959
exhaustion how many times have we logged

00:02:32,010 --> 00:02:36,209
into a server and looked at the log to

00:02:33,959 --> 00:02:37,500
find out and they tried to allocate some

00:02:36,209 --> 00:02:39,840
memory that it couldn't because it run

00:02:37,500 --> 00:02:42,599
out or that you exceeded the number of

00:02:39,840 --> 00:02:43,799
file descriptors available so really

00:02:42,599 --> 00:02:47,909
important information that you want to

00:02:43,799 --> 00:02:51,930
that you want to log i'm narrative

00:02:47,909 --> 00:02:53,190
information i'm so information about the

00:02:51,930 --> 00:02:55,049
kind of method calls that have been made

00:02:53,190 --> 00:02:58,709
in your system how long it takes to

00:02:55,049 --> 00:03:00,480
execute a particular method all really

00:02:58,709 --> 00:03:02,879
useful at runtime when you're trying to

00:03:00,480 --> 00:03:04,290
figure out what's going on and also

00:03:02,879 --> 00:03:05,430
event triggers as well so if you're

00:03:04,290 --> 00:03:07,650
triggering events with an event

00:03:05,430 --> 00:03:11,220
dispatcher it's really easy to hang

00:03:07,650 --> 00:03:14,099
logging information off of that and then

00:03:11,220 --> 00:03:16,650
finally this one tends to get overlooked

00:03:14,099 --> 00:03:19,409
i'm but logging business events in a

00:03:16,650 --> 00:03:22,079
separate log next to your application is

00:03:19,409 --> 00:03:23,519
a really worthwhile so Facebook and Etsy

00:03:22,079 --> 00:03:26,519
are kind of pioneers of this technique

00:03:23,519 --> 00:03:29,790
they log these events like purchases

00:03:26,519 --> 00:03:31,019
logins registrations unsubscribes things

00:03:29,790 --> 00:03:33,180
they were important to the business and

00:03:31,019 --> 00:03:35,099
they logged them they count them and

00:03:33,180 --> 00:03:37,199
then they display that information to

00:03:35,099 --> 00:03:39,510
the business also really useful because

00:03:37,199 --> 00:03:41,849
after a deploy if any of these kind of

00:03:39,510 --> 00:03:43,260
soft business e metrics go up or down

00:03:41,849 --> 00:03:46,799
you know you've either done a really

00:03:43,260 --> 00:03:48,329
good or a really bad job and so I'm

00:03:46,799 --> 00:03:52,560
going to demonstrate some of those in

00:03:48,329 --> 00:03:55,199
action shortly so we've all been in this

00:03:52,560 --> 00:03:59,010
situation servers are on fire and we've

00:03:55,199 --> 00:04:01,230
logged into various boxes the web server

00:03:59,010 --> 00:04:03,690
at the top they're tailing the nginx

00:04:01,230 --> 00:04:05,099
access log and perhaps telling your own

00:04:03,690 --> 00:04:07,739
log to try and figure out what's going

00:04:05,099 --> 00:04:09,299
on problem is endure through login to

00:04:07,739 --> 00:04:12,629
another server and after that another

00:04:09,299 --> 00:04:14,669
one trying to identify this problem and

00:04:12,629 --> 00:04:16,620
the point of this story is that this

00:04:14,669 --> 00:04:19,079
isn't really feasible you might be able

00:04:16,620 --> 00:04:20,519
to solve this problem once when it's

00:04:19,079 --> 00:04:22,470
actually happening to you wouldn't it be

00:04:20,519 --> 00:04:25,349
great to kind of preempt this problem I

00:04:22,470 --> 00:04:28,020
have one place to go when times are hard

00:04:25,349 --> 00:04:30,000
and the servers are on fire and also

00:04:28,020 --> 00:04:34,890
look at trends over time

00:04:30,000 --> 00:04:38,550
for this log information now the answer

00:04:34,890 --> 00:04:41,280
to that I think is is the lq stack which

00:04:38,550 --> 00:04:42,960
consists of elasticsearch log stash and

00:04:41,280 --> 00:04:44,760
Cabana those are the three parts that

00:04:42,960 --> 00:04:46,350
I'm going to discuss today there are

00:04:44,760 --> 00:04:48,750
some kind of paid for alternatives to

00:04:46,350 --> 00:04:50,370
this but at Sainsbury's we've had really

00:04:48,750 --> 00:04:53,580
good resorts just using these open

00:04:50,370 --> 00:04:54,000
source free tools so what are these

00:04:53,580 --> 00:04:57,180
things

00:04:54,000 --> 00:04:59,370
I'm elasticsearch is a open source

00:04:57,180 --> 00:05:02,730
search engine you can basically pump it

00:04:59,370 --> 00:05:04,680
full of data and then do clever searches

00:05:02,730 --> 00:05:08,370
and aggregations on that data at a later

00:05:04,680 --> 00:05:09,390
point lasting searchers originally just

00:05:08,370 --> 00:05:11,760
a piece of software but it's now a

00:05:09,390 --> 00:05:14,040
company that was formed around that and

00:05:11,760 --> 00:05:17,000
they owned these other two technologies

00:05:14,040 --> 00:05:20,160
or sponsor the project these other two

00:05:17,000 --> 00:05:22,140
Tech's logstash

00:05:20,160 --> 00:05:24,480
sits in in the middle of this setup you

00:05:22,140 --> 00:05:26,730
can think of logstash as a bus because

00:05:24,480 --> 00:05:29,460
it has a range of inputs some middleware

00:05:26,730 --> 00:05:30,450
and a range of outputs so in the example

00:05:29,460 --> 00:05:33,720
I'm going to show you today I'm going to

00:05:30,450 --> 00:05:36,510
show you a simple tool which tails log

00:05:33,720 --> 00:05:37,530
files and gets it into log stash but

00:05:36,510 --> 00:05:41,520
there's a range of other inputs like

00:05:37,530 --> 00:05:43,320
Redis is another one the various M

00:05:41,520 --> 00:05:44,610
queues so if you if you've got an

00:05:43,320 --> 00:05:46,470
interesting setup and you want to get

00:05:44,610 --> 00:05:49,970
data into log stash go to the website

00:05:46,470 --> 00:05:52,380
there's so many options and then on the

00:05:49,970 --> 00:05:53,760
output side of log stash you've got

00:05:52,380 --> 00:05:55,620
things like elastic search which is

00:05:53,760 --> 00:05:59,100
pretty much the default but you've also

00:05:55,620 --> 00:06:02,370
got things like stats D graphite the M

00:05:59,100 --> 00:06:03,300
cues like 0 mq rabbitmq and then in the

00:06:02,370 --> 00:06:06,360
middle of that you've got a

00:06:03,300 --> 00:06:09,570
transformation layer which can filter

00:06:06,360 --> 00:06:10,860
that data tokenize it and do interesting

00:06:09,570 --> 00:06:13,290
things which I'll demonstrate in a

00:06:10,860 --> 00:06:17,160
minute so think of it as a way to get

00:06:13,290 --> 00:06:18,419
data into various outputs and then on

00:06:17,160 --> 00:06:22,020
the end of this tool chain we've got

00:06:18,419 --> 00:06:24,360
Cabana and Cabana is a way to quickly

00:06:22,020 --> 00:06:27,290
interrogate elasticsearch for log data

00:06:24,360 --> 00:06:31,140
we also allows you to build dashboards

00:06:27,290 --> 00:06:33,300
again that's in the demo in a moment and

00:06:31,140 --> 00:06:38,220
that's that's the end result this is the

00:06:33,300 --> 00:06:40,530
kind of Holy Grail really surfacing log

00:06:38,220 --> 00:06:42,720
data in a way that means something to

00:06:40,530 --> 00:06:43,920
the business so the titles of these logs

00:06:42,720 --> 00:06:47,280
are things like load

00:06:43,920 --> 00:06:49,410
by request a title here is total bytes

00:06:47,280 --> 00:06:51,360
per second by extension so you could

00:06:49,410 --> 00:06:54,750
imagine that being a critical dashboard

00:06:51,360 --> 00:06:56,340
for somebody that runs a CDN and that's

00:06:54,750 --> 00:06:58,020
what we're going to build about halfway

00:06:56,340 --> 00:07:01,080
through this presentation but before

00:06:58,020 --> 00:07:03,360
getting to that point and we need some

00:07:01,080 --> 00:07:05,850
way of generating these logs and so I'm

00:07:03,360 --> 00:07:07,830
going to go back to first principles and

00:07:05,850 --> 00:07:13,140
talk about how we capture logs in a PHP

00:07:07,830 --> 00:07:14,760
application so if a while ago I did that

00:07:13,140 --> 00:07:16,530
presentation where I showed you lots of

00:07:14,760 --> 00:07:18,420
different types of ways of logging data

00:07:16,530 --> 00:07:20,460
for anything from you know file put

00:07:18,420 --> 00:07:22,470
contents into a file various different

00:07:20,460 --> 00:07:24,150
logging libraries but in my old age I've

00:07:22,470 --> 00:07:26,910
become a lot more opinionated and I

00:07:24,150 --> 00:07:28,410
think monologue is such a mature product

00:07:26,910 --> 00:07:29,880
it's such a pleasure to use I've been

00:07:28,410 --> 00:07:32,100
using for about three years it's the

00:07:29,880 --> 00:07:33,680
go-to logger for symphony I highly

00:07:32,100 --> 00:07:36,180
recommend you go and have a look at it

00:07:33,680 --> 00:07:39,210
and all the examples I'm gonna show her

00:07:36,180 --> 00:07:41,160
in monologue so here's what I'm going to

00:07:39,210 --> 00:07:44,040
show and the basics of monologue and

00:07:41,160 --> 00:07:45,090
some important features that make it

00:07:44,040 --> 00:07:48,360
compatible with logstash

00:07:45,090 --> 00:07:54,780
and a little code snippet on logging

00:07:48,360 --> 00:07:57,630
business events so this is a all the

00:07:54,780 --> 00:07:59,490
code which is required to bootstrap a

00:07:57,630 --> 00:08:01,860
monologue logger which is the tool you

00:07:59,490 --> 00:08:03,210
use that you get logs into a log file so

00:08:01,860 --> 00:08:04,410
there's about eight lines they're not

00:08:03,210 --> 00:08:07,050
really normally just sits in the

00:08:04,410 --> 00:08:08,640
bootstrap section of your application so

00:08:07,050 --> 00:08:12,750
I'm going to go through these make them

00:08:08,640 --> 00:08:14,220
a little bit bigger and so the first

00:08:12,750 --> 00:08:16,770
thing that I do in this bootstrap

00:08:14,220 --> 00:08:17,850
process is to pick a log level and I'm

00:08:16,770 --> 00:08:19,230
going to go through the various log

00:08:17,850 --> 00:08:20,550
levels in a minute but at the moment you

00:08:19,230 --> 00:08:23,220
can think about it as a way of choosing

00:08:20,550 --> 00:08:24,810
how verbose your log should be and it

00:08:23,220 --> 00:08:26,430
ranges from really verbose I want to

00:08:24,810 --> 00:08:30,150
know everything about my application to

00:08:26,430 --> 00:08:31,800
only surface the most critical events so

00:08:30,150 --> 00:08:34,830
why am i doing that and using an

00:08:31,800 --> 00:08:39,330
environment variable rather than using

00:08:34,830 --> 00:08:41,010
config or hard coding that value using

00:08:39,330 --> 00:08:43,260
an environment variable lets you log

00:08:41,010 --> 00:08:44,880
into a server maybe it's a problem

00:08:43,260 --> 00:08:46,260
server in your infrastructure and just

00:08:44,880 --> 00:08:48,990
change the environment just that one

00:08:46,260 --> 00:08:50,850
server is running in so it allows you to

00:08:48,990 --> 00:08:52,410
switch that level down to debug but

00:08:50,850 --> 00:08:53,760
without affecting the rest of the web

00:08:52,410 --> 00:08:55,290
tier for example if you're logging into

00:08:53,760 --> 00:08:56,790
the web tier so that's been really

00:08:55,290 --> 00:08:57,270
useful for us when we're debugging

00:08:56,790 --> 00:09:01,950
problems

00:08:57,270 --> 00:09:03,780
specific machines and then in the next

00:09:01,950 --> 00:09:06,810
line I'm just setting a sensible default

00:09:03,780 --> 00:09:10,260
for that level and then creating a new

00:09:06,810 --> 00:09:12,480
logger which is the kind of object of

00:09:10,260 --> 00:09:14,490
interest here once you have a logger

00:09:12,480 --> 00:09:18,420
you can call various methods to write

00:09:14,490 --> 00:09:21,840
some logs so in the next section here

00:09:18,420 --> 00:09:24,210
we've got a stream handler um and a

00:09:21,840 --> 00:09:25,950
fingers-crossed handler so handlers are

00:09:24,210 --> 00:09:28,170
really pointers to some kind of output

00:09:25,950 --> 00:09:30,990
so this stream handler at the top here

00:09:28,170 --> 00:09:33,180
it points to a file and the second

00:09:30,990 --> 00:09:36,120
argument is a log level which have

00:09:33,180 --> 00:09:39,000
hard-coded so why the hard-coded it that

00:09:36,120 --> 00:09:41,280
goes against what I just said so the

00:09:39,000 --> 00:09:43,110
output here would be any any kind of log

00:09:41,280 --> 00:09:45,000
entry would go into this file there's no

00:09:43,110 --> 00:09:48,150
filtering at all and the reason I

00:09:45,000 --> 00:09:49,910
hard-coded it there is because this next

00:09:48,150 --> 00:09:54,530
handler the fingers-crossed handler

00:09:49,910 --> 00:09:57,870
wraps another handler and will only let

00:09:54,530 --> 00:09:59,880
log messages through to it if a log

00:09:57,870 --> 00:10:01,980
entry of a certain level comes along so

00:09:59,880 --> 00:10:04,230
to put that in an example imagine you

00:10:01,980 --> 00:10:06,750
have a request where you logged ten

00:10:04,230 --> 00:10:07,470
debug lines in a row normally you're not

00:10:06,750 --> 00:10:09,980
gonna want to see that information

00:10:07,470 --> 00:10:12,510
because that request was a success

00:10:09,980 --> 00:10:15,060
storing all that information you know

00:10:12,510 --> 00:10:17,010
why would you bother but if a critical

00:10:15,060 --> 00:10:19,320
event happened at event number 11

00:10:17,010 --> 00:10:21,240
suddenly all those are the ten log lines

00:10:19,320 --> 00:10:24,060
become really relevant so fingers

00:10:21,240 --> 00:10:26,010
crossed enables us to capture all the

00:10:24,060 --> 00:10:29,730
data but only when we need it so it's a

00:10:26,010 --> 00:10:33,210
really useful construct and then you

00:10:29,730 --> 00:10:35,580
push that handler onto the app log that

00:10:33,210 --> 00:10:39,780
we just created and then you're ready to

00:10:35,580 --> 00:10:41,820
go so then you have this log object here

00:10:39,780 --> 00:10:44,280
with methods like debug to our debug log

00:10:41,820 --> 00:10:47,040
or emergency to write an emergency log

00:10:44,280 --> 00:10:48,680
and you can then pass that around your

00:10:47,040 --> 00:10:57,150
system in whatever way you see fit so

00:10:48,680 --> 00:11:00,720
dependency injection or similar okay so

00:10:57,150 --> 00:11:03,390
that's the basics of creating a logger

00:11:00,720 --> 00:11:05,190
so that's useful but it's much more

00:11:03,390 --> 00:11:08,910
useful if we follow these next two

00:11:05,190 --> 00:11:11,040
approaches so the first one of those is

00:11:08,910 --> 00:11:13,290
tagging and it allows you to

00:11:11,040 --> 00:11:16,140
metadata to every single log line that

00:11:13,290 --> 00:11:20,460
goes through one of the loggers so

00:11:16,140 --> 00:11:23,820
metadata could be an IP address some geo

00:11:20,460 --> 00:11:25,710
data the date and time allows you to tag

00:11:23,820 --> 00:11:27,330
all of those with with that data which

00:11:25,710 --> 00:11:32,580
is useful later on further down the

00:11:27,330 --> 00:11:34,680
chain and then there's a formatter so

00:11:32,580 --> 00:11:36,720
formatter and so many different log

00:11:34,680 --> 00:11:40,040
formats it's like syslog former maybe

00:11:36,720 --> 00:11:42,240
you want to out log your logs as Jason

00:11:40,040 --> 00:11:44,610
so rather than trying to write that

00:11:42,240 --> 00:11:47,430
logic itself you just attach a formatter

00:11:44,610 --> 00:11:55,110
and to do it for you so let's take a

00:11:47,430 --> 00:11:57,840
look at these in detail so I'm creating

00:11:55,110 --> 00:12:00,930
a logger as in the previous example and

00:11:57,840 --> 00:12:03,630
a handler as in the previous example but

00:12:00,930 --> 00:12:05,640
this time I am setting a formatter on

00:12:03,630 --> 00:12:06,270
that handler which is a log stash

00:12:05,640 --> 00:12:09,300
formatter

00:12:06,270 --> 00:12:11,700
it's a pre-written piece of code which

00:12:09,300 --> 00:12:14,070
will format all of our logs in Jason and

00:12:11,700 --> 00:12:16,590
attach an important tag and to that

00:12:14,070 --> 00:12:19,170
Jason and so that when we tail it and

00:12:16,590 --> 00:12:20,910
send it to log stash it's in the right

00:12:19,170 --> 00:12:24,900
format reduces the work we have to do

00:12:20,910 --> 00:12:28,080
later on and then um push the handler

00:12:24,900 --> 00:12:33,150
onto the log object as in the previous

00:12:28,080 --> 00:12:35,870
example so this is an example of tagging

00:12:33,150 --> 00:12:39,780
so this is a pattern that I often follow

00:12:35,870 --> 00:12:42,600
when when logging so say you are

00:12:39,780 --> 00:12:44,340
generating 10 log lines in a request but

00:12:42,600 --> 00:12:46,470
you're actually accepting lots of

00:12:44,340 --> 00:12:48,690
requests at the same time so in your log

00:12:46,470 --> 00:12:51,690
file all these log entries get jumbled

00:12:48,690 --> 00:12:53,400
up and what's really useful is say we're

00:12:51,690 --> 00:12:55,770
doing some load testing and one or two

00:12:53,400 --> 00:12:57,630
requests errored you want to be able to

00:12:55,770 --> 00:13:02,250
identify all of the log lines from that

00:12:57,630 --> 00:13:04,380
particular request that errored so I've

00:13:02,250 --> 00:13:07,800
got varnish at the front of this setup

00:13:04,380 --> 00:13:09,420
varnish drops and ID in the HTTP headers

00:13:07,800 --> 00:13:11,880
before it sends it over to nginx or

00:13:09,420 --> 00:13:15,060
Apache and then here I'm grabbing that

00:13:11,880 --> 00:13:17,460
ID feeding it into a task a tag

00:13:15,060 --> 00:13:20,790
processor and pushing that processor

00:13:17,460 --> 00:13:23,850
onto the log object so now I've got a UD

00:13:20,790 --> 00:13:24,730
unique ID which links all of my logs to

00:13:23,850 --> 00:13:26,920
a particular request

00:13:24,730 --> 00:13:31,360
very useful I'm going to demonstrate

00:13:26,920 --> 00:13:34,360
that later on so log levels have been

00:13:31,360 --> 00:13:37,150
talking about them in general but they

00:13:34,360 --> 00:13:40,240
do have a specific specification so in

00:13:37,150 --> 00:13:43,680
RFC five four to four which is one of

00:13:40,240 --> 00:13:49,030
the more interesting rfcs it describes

00:13:43,680 --> 00:13:50,740
it describes a scheme for logging or lot

00:13:49,030 --> 00:13:52,930
of levels at the bottom of that scheme

00:13:50,740 --> 00:13:54,250
is debug and you're not normally going

00:13:52,930 --> 00:13:57,070
to want to see these in production but

00:13:54,250 --> 00:13:58,210
it's useful in development or it's

00:13:57,070 --> 00:14:00,430
useful if something's going wrong and

00:13:58,210 --> 00:14:05,590
you need some really granular data in

00:14:00,430 --> 00:14:07,360
the middle of that scale is error and so

00:14:05,590 --> 00:14:09,340
an error could still be kind of isolated

00:14:07,360 --> 00:14:11,980
to a specific request it might be an

00:14:09,340 --> 00:14:13,660
uncaught exception something like that

00:14:11,980 --> 00:14:15,310
or it could be symptomatic of a wider

00:14:13,660 --> 00:14:18,010
problem which is affecting more things

00:14:15,310 --> 00:14:20,230
but anything from error upwards like

00:14:18,010 --> 00:14:22,500
critical critical conditions alert

00:14:20,230 --> 00:14:25,210
action must be taken immediately an

00:14:22,500 --> 00:14:26,830
emergency system is unstable it's these

00:14:25,210 --> 00:14:29,260
kind of messages which should be waking

00:14:26,830 --> 00:14:31,300
up your ops team and sounding alarms and

00:14:29,260 --> 00:14:33,580
then on the side they're represented by

00:14:31,300 --> 00:14:36,850
codes which are normally in constants in

00:14:33,580 --> 00:14:39,610
the various log libraries but with it

00:14:36,850 --> 00:14:43,480
being a PHP talk it would be remiss of

00:14:39,610 --> 00:14:46,120
me not to mention PRS oh three the PHP

00:14:43,480 --> 00:14:48,340
logging interface standard so they

00:14:46,120 --> 00:14:51,790
define the same set of conditions

00:14:48,340 --> 00:14:54,340
however they use phrases instead of kind

00:14:51,790 --> 00:14:55,750
of codes and a monologue will support

00:14:54,340 --> 00:14:57,880
both of these things depending on what

00:14:55,750 --> 00:14:59,530
you want to do and I do think it's a bit

00:14:57,880 --> 00:15:02,320
a bit kind of like this we already

00:14:59,530 --> 00:15:06,910
specified some really useful codes why

00:15:02,320 --> 00:15:10,720
specify another set so a last kind of

00:15:06,910 --> 00:15:12,430
thing on logging in PHP now earlier on I

00:15:10,720 --> 00:15:15,850
mentioned it was really important to log

00:15:12,430 --> 00:15:18,700
business events so if you're using the

00:15:15,850 --> 00:15:20,470
pattern of an event dispatcher and a

00:15:18,700 --> 00:15:22,390
kind of event-driven system you're

00:15:20,470 --> 00:15:24,940
probably doing things like a

00:15:22,390 --> 00:15:27,070
pre-registration and a post registration

00:15:24,940 --> 00:15:30,160
hook and you can hook various things

00:15:27,070 --> 00:15:32,710
into that process and if you were to

00:15:30,160 --> 00:15:33,910
create a separate business logger using

00:15:32,710 --> 00:15:37,390
the same kind of style that we've just

00:15:33,910 --> 00:15:38,380
demonstrated you could hang a log file

00:15:37,390 --> 00:15:39,820
off of that so that

00:15:38,380 --> 00:15:42,040
every time we receive a registration

00:15:39,820 --> 00:15:44,020
event it logs it in that file that a

00:15:42,040 --> 00:15:45,580
customer's being registered later on we

00:15:44,020 --> 00:15:47,770
can push that into log stash and show

00:15:45,580 --> 00:15:49,450
some interesting information about real

00:15:47,770 --> 00:15:51,850
time business events in our system

00:15:49,450 --> 00:15:54,220
things like registrations per minute or

00:15:51,850 --> 00:15:57,730
amount of money spent in the last hour

00:15:54,220 --> 00:16:03,850
that kind of thing and then at the

00:15:57,730 --> 00:16:06,790
bottom just dispatch that event so that

00:16:03,850 --> 00:16:09,510
is logging in PHP or an opinionated view

00:16:06,790 --> 00:16:13,390
on logging in PHP how does that fit into

00:16:09,510 --> 00:16:14,800
the log stash or the ELQ stack so they

00:16:13,390 --> 00:16:18,850
say this is an infrastructure diagram

00:16:14,800 --> 00:16:21,340
with a web server at the bottom and the

00:16:18,850 --> 00:16:23,590
web server is running the log stash

00:16:21,340 --> 00:16:27,010
forwarding agent and that is a tool

00:16:23,590 --> 00:16:29,680
which will tail files on your system and

00:16:27,010 --> 00:16:32,200
send those files send those lines from

00:16:29,680 --> 00:16:35,130
those files to log stash which is the

00:16:32,200 --> 00:16:37,750
next thing in the chain log stash will

00:16:35,130 --> 00:16:40,920
do some kind of filtering process on

00:16:37,750 --> 00:16:43,600
them and then send them to elasticsearch

00:16:40,920 --> 00:16:45,310
and back at the kind of front end of the

00:16:43,600 --> 00:16:48,960
infrastructure you've got key abana

00:16:45,310 --> 00:16:52,150
which is a front-end to elasticsearch

00:16:48,960 --> 00:16:53,890
I've done a example code base which I'm

00:16:52,150 --> 00:16:55,690
going to show you in a minute and in the

00:16:53,890 --> 00:16:58,240
example code base log stash

00:16:55,690 --> 00:16:59,980
elasticsearch and cabana are all on one

00:16:58,240 --> 00:17:02,140
box so if you have a small

00:16:59,980 --> 00:17:04,560
infrastructure you can kind of pack all

00:17:02,140 --> 00:17:07,750
of this onto one machine cost-effective

00:17:04,560 --> 00:17:09,250
but if you've got a big infrastructure

00:17:07,750 --> 00:17:10,870
you're likely going to be generating a

00:17:09,250 --> 00:17:13,780
lot of logs so you might need something

00:17:10,870 --> 00:17:16,110
a little more complicated so there are a

00:17:13,780 --> 00:17:19,960
number of different shipping models in

00:17:16,110 --> 00:17:22,480
log stash but one of them is Redis so

00:17:19,960 --> 00:17:25,290
you could have n number of web servers

00:17:22,480 --> 00:17:27,790
or ship shipping logs to a Redis cluster

00:17:25,290 --> 00:17:30,970
log stash reads from that Redis cluster

00:17:27,790 --> 00:17:35,410
and writes to a cluster of elasticsearch

00:17:30,970 --> 00:17:40,210
instances which are read by multiple

00:17:35,410 --> 00:17:42,430
Cabana front ends so how you want to set

00:17:40,210 --> 00:17:44,140
up that is set up that infrastructure is

00:17:42,430 --> 00:17:45,520
up to you so if you're generating a lot

00:17:44,140 --> 00:17:48,700
of logs probably go for something like

00:17:45,520 --> 00:17:52,430
that if you're just kind of testing ELQ

00:17:48,700 --> 00:17:55,470
then maybe set it up on one instance

00:17:52,430 --> 00:17:57,690
so there's a demo coming up but let me

00:17:55,470 --> 00:17:59,490
just tell you about joined in here if

00:17:57,690 --> 00:18:02,010
you if you're feeling warm things about

00:17:59,490 --> 00:18:05,010
this presentation already why wait until

00:18:02,010 --> 00:18:09,960
the end to to post some feedback why not

00:18:05,010 --> 00:18:13,580
do it now so it looks nice demo um all

00:18:09,960 --> 00:18:17,370
of the code for this demo is on github

00:18:13,580 --> 00:18:18,690
application logging with log stash and

00:18:17,370 --> 00:18:23,340
I'm going to take a walk through that

00:18:18,690 --> 00:18:27,260
codebase now okay

00:18:23,340 --> 00:18:34,670
so let's increase the size a little bit

00:18:27,260 --> 00:18:42,150
and how's that for size a bit bigger or

00:18:34,670 --> 00:18:45,150
okay oh yeah sure so URL is a live

00:18:42,150 --> 00:18:47,280
software slash application - logging

00:18:45,150 --> 00:18:49,410
with logs - that's a good point so you

00:18:47,280 --> 00:18:55,170
can read along with the code now if you

00:18:49,410 --> 00:18:57,180
go to that website okay um

00:18:55,170 --> 00:19:01,770
so the first thing I want to say is that

00:18:57,180 --> 00:19:03,870
the code base is vagrant ice-t-- so

00:19:01,770 --> 00:19:06,060
there's a vagrant file which will do two

00:19:03,870 --> 00:19:08,160
VMs so if you want to try this out one

00:19:06,060 --> 00:19:11,160
of those VMs is the elk stack the other

00:19:08,160 --> 00:19:12,990
one has a test web application this is a

00:19:11,160 --> 00:19:15,030
particularly interesting file which is

00:19:12,990 --> 00:19:17,100
provision - log stash in the build

00:19:15,030 --> 00:19:19,770
directory and that's basically a shell

00:19:17,100 --> 00:19:22,710
script which will set up log stash from

00:19:19,770 --> 00:19:26,220
first principles so all the Java log

00:19:22,710 --> 00:19:28,320
stash elasticsearch it will set

00:19:26,220 --> 00:19:30,840
everything up including nginx which is

00:19:28,320 --> 00:19:32,490
used to proxy elastic search so if

00:19:30,840 --> 00:19:33,360
you're interested in how it's all how

00:19:32,490 --> 00:19:36,480
it's all set up

00:19:33,360 --> 00:19:37,920
then that's the file to go to and then

00:19:36,480 --> 00:19:40,410
also there's a provision web which

00:19:37,920 --> 00:19:42,150
provisions the web server the most

00:19:40,410 --> 00:19:44,220
interesting thing here is that it's also

00:19:42,150 --> 00:19:46,350
setting up the log stash forwarder

00:19:44,220 --> 00:19:51,210
so those are the two moving parts if you

00:19:46,350 --> 00:19:53,490
want to join in later on so in order to

00:19:51,210 --> 00:19:55,560
demonstrate a log a logging tool I need

00:19:53,490 --> 00:19:57,210
some actual logs so I'll take you

00:19:55,560 --> 00:19:59,910
through what this little tiny

00:19:57,210 --> 00:20:02,670
application does and the first thing it

00:19:59,910 --> 00:20:04,590
does is on the kind of slash root it

00:20:02,670 --> 00:20:05,070
returns a string saying hello I'm

00:20:04,590 --> 00:20:07,050
generate

00:20:05,070 --> 00:20:09,560
logs as we speak that's never going to

00:20:07,050 --> 00:20:12,510
fail that's gonna show us some good logs

00:20:09,560 --> 00:20:15,570
and then you'll see here if I just

00:20:12,510 --> 00:20:16,860
highlight this section this is the code

00:20:15,570 --> 00:20:18,810
that I showed you on the slides earlier

00:20:16,860 --> 00:20:21,390
on so this is bootstrapping an instance

00:20:18,810 --> 00:20:25,170
of monologue game you're working with

00:20:21,390 --> 00:20:25,800
the tag processor and also the log stash

00:20:25,170 --> 00:20:29,730
formatter

00:20:25,800 --> 00:20:32,070
and then after that there is this funky

00:20:29,730 --> 00:20:35,570
piece of code which defies all kind of

00:20:32,070 --> 00:20:39,930
testing it will randomly return either

00:20:35,570 --> 00:20:43,650
404 not found a 500 with loads of logs

00:20:39,930 --> 00:20:46,260
and a really latent response so this one

00:20:43,650 --> 00:20:48,780
is sleeping so there's three seconds

00:20:46,260 --> 00:20:52,740
worth of latency here a normal response

00:20:48,780 --> 00:20:53,910
and a bad gateway response so hopefully

00:20:52,740 --> 00:20:57,900
it provides us with a little bit of

00:20:53,910 --> 00:21:00,210
variety to our logs and then just while

00:20:57,900 --> 00:21:02,760
I'm here if you want to play it play

00:21:00,210 --> 00:21:04,340
with this after there is an atty Ted

00:21:02,760 --> 00:21:07,500
example of the fingers-crossed

00:21:04,340 --> 00:21:09,980
demo showing you how to set that up and

00:21:07,500 --> 00:21:13,800
then underneath there is an annotated

00:21:09,980 --> 00:21:15,660
example of logging with business events

00:21:13,800 --> 00:21:23,610
but they're not too important for this

00:21:15,660 --> 00:21:26,550
demo so without further ado okay so as

00:21:23,610 --> 00:21:28,650
you know so this is when you open Cubana

00:21:26,550 --> 00:21:30,870
this is what you get a screen with some

00:21:28,650 --> 00:21:32,550
text on and the first thing you want to

00:21:30,870 --> 00:21:34,890
do is go to the log stash dashboard

00:21:32,550 --> 00:21:38,220
which is a template and log stash

00:21:34,890 --> 00:21:43,130
implementation and then what you get on

00:21:38,220 --> 00:21:47,580
here is a histogram full of log entries

00:21:43,130 --> 00:21:50,270
over time and then you get a list of all

00:21:47,580 --> 00:21:52,380
the log entries that have been captured

00:21:50,270 --> 00:21:57,030
so let's have a look at how this might

00:21:52,380 --> 00:21:59,400
and kind of affect the front-end so here

00:21:57,030 --> 00:22:01,920
is the front-end of the website so it

00:21:59,400 --> 00:22:03,690
says hello I'm gathering logs so if I

00:22:01,920 --> 00:22:05,880
just request this quite a lot what I'm

00:22:03,690 --> 00:22:10,430
hoping to see when I go back to logs -

00:22:05,880 --> 00:22:10,430
as some nginx access logs appear there

00:22:12,500 --> 00:22:18,960
okay the last five minutes so you can

00:22:15,210 --> 00:22:20,309
see I was that that last view was on

00:22:18,960 --> 00:22:21,990
six-hour view so you could see I was

00:22:20,309 --> 00:22:23,429
playing with it earlier on but right

00:22:21,990 --> 00:22:26,210
over in the right-hand corner we've now

00:22:23,429 --> 00:22:30,120
got some new events in the 5-minute view

00:22:26,210 --> 00:22:32,610
and if I kind of drag and drop drop if I

00:22:30,120 --> 00:22:35,610
drag to the right and highlight that and

00:22:32,610 --> 00:22:38,309
it will reset the time to be something a

00:22:35,610 --> 00:22:41,039
bit more granular so let's zoom in a

00:22:38,309 --> 00:22:42,240
little bit and see what what the logs

00:22:41,039 --> 00:22:43,460
actually look like when they've reached

00:22:42,240 --> 00:22:46,500
logstash

00:22:43,460 --> 00:22:48,600
so this is a list of all those home

00:22:46,500 --> 00:22:50,669
pages that I just viewed you can click

00:22:48,600 --> 00:22:54,809
on an item here and I'll just increase

00:22:50,669 --> 00:22:57,120
the size there again I'm and when you

00:22:54,809 --> 00:23:00,149
actually expanded a log item you can see

00:22:57,120 --> 00:23:04,919
things like agent or number of bytes in

00:23:00,149 --> 00:23:07,500
the request the client IP the file host

00:23:04,919 --> 00:23:10,110
basically everything that is in the

00:23:07,500 --> 00:23:11,159
original message here but it's been

00:23:10,110 --> 00:23:15,690
tokenized

00:23:11,159 --> 00:23:19,169
and it's been given meaning by logs - so

00:23:15,690 --> 00:23:22,140
that is useful as a way to quickly

00:23:19,169 --> 00:23:23,730
interrogate logs but one of the other

00:23:22,140 --> 00:23:25,770
goals that I saw at the start of this

00:23:23,730 --> 00:23:28,289
would be able to isolate faults as they

00:23:25,770 --> 00:23:30,390
happened so for that we need a really

00:23:28,289 --> 00:23:33,480
ropey endpoint which is going to give us

00:23:30,390 --> 00:23:36,870
some errors which is this flat the

00:23:33,480 --> 00:23:38,970
endpoint and so I can kind of flick

00:23:36,870 --> 00:23:42,210
through this it's going to give me a

00:23:38,970 --> 00:23:44,850
slow response now not found something

00:23:42,210 --> 00:23:50,850
terrible has happened so let's get the

00:23:44,850 --> 00:23:54,870
console open like so so the console

00:23:50,850 --> 00:23:56,370
shows us here which you can't really see

00:23:54,870 --> 00:23:59,669
there and it doesn't magnify with the

00:23:56,370 --> 00:24:02,039
rest of the screen but it shows us the

00:23:59,669 --> 00:24:04,740
error code here and if we click in it we

00:24:02,039 --> 00:24:07,830
can also see the varnish header because

00:24:04,740 --> 00:24:10,200
varnish returns that unique ID to the

00:24:07,830 --> 00:24:12,330
client now armed with that kind of

00:24:10,200 --> 00:24:14,730
unique ID we can then go back to logs -

00:24:12,330 --> 00:24:16,950
and pull back all of the events that

00:24:14,730 --> 00:24:19,350
happened in the system the correspond to

00:24:16,950 --> 00:24:21,210
that ID so it's getting more interesting

00:24:19,350 --> 00:24:23,760
one to do that with something terrible

00:24:21,210 --> 00:24:26,870
has happened generates lots of logs so

00:24:23,760 --> 00:24:31,370
let's click through and then I'll copy

00:24:26,870 --> 00:24:37,520
copy this ID over here

00:24:31,370 --> 00:24:45,690
you know if I do a search for request ID

00:24:37,520 --> 00:24:47,810
equals a reset the time okay so you can

00:24:45,690 --> 00:24:50,970
see now that I've done a search for the

00:24:47,810 --> 00:24:53,460
request ID in the top corner and that

00:24:50,970 --> 00:24:56,790
corresponds to exactly one event here

00:24:53,460 --> 00:25:03,150
which is a number of events because I've

00:24:56,790 --> 00:25:05,940
been clicking through and that request

00:25:03,150 --> 00:25:10,020
ID is present here

00:25:05,940 --> 00:25:13,560
so it's come through in the it's come

00:25:10,020 --> 00:25:16,440
through as a token here in the log

00:25:13,560 --> 00:25:19,920
response so if I want to then do a

00:25:16,440 --> 00:25:22,590
search for everything that contains a

00:25:19,920 --> 00:25:25,200
reference to that number then I can see

00:25:22,590 --> 00:25:28,890
now that I've got one two three four

00:25:25,200 --> 00:25:30,720
five different logs and so just to give

00:25:28,890 --> 00:25:32,370
you a sample of one of those logs this

00:25:30,720 --> 00:25:34,830
one is from the actual application log

00:25:32,370 --> 00:25:38,490
and it says it's caused an error oh god

00:25:34,830 --> 00:25:40,890
no in your application now I'll probably

00:25:38,490 --> 00:25:46,290
mean something like the database is

00:25:40,890 --> 00:25:48,180
timed out and so on and so that is an

00:25:46,290 --> 00:25:49,740
easy way to isolate a particular error

00:25:48,180 --> 00:25:53,160
providing you can get hold of the

00:25:49,740 --> 00:25:54,990
varnish ID header but it would be good

00:25:53,160 --> 00:25:58,440
if we could surface kind of more general

00:25:54,990 --> 00:26:00,450
errors so let's do a response which is

00:25:58,440 --> 00:26:02,610
an alias to the response code

00:26:00,450 --> 00:26:06,660
let's do response greater than or equal

00:26:02,610 --> 00:26:10,260
to 500 and then let's set the time to be

00:26:06,660 --> 00:26:12,030
something like at the last kind of 12

00:26:10,260 --> 00:26:13,770
hours so I was playing with this this

00:26:12,030 --> 00:26:16,260
morning and generated some error

00:26:13,770 --> 00:26:17,550
responses so if you were from an

00:26:16,260 --> 00:26:19,110
infrastructure perspective if you were

00:26:17,550 --> 00:26:21,390
kind of surveying what's happened in the

00:26:19,110 --> 00:26:24,480
last 6 hours you could look for all the

00:26:21,390 --> 00:26:28,980
500 responses and they're all listed

00:26:24,480 --> 00:26:31,080
here you could click in and then you

00:26:28,980 --> 00:26:33,630
could go and find that request ID field

00:26:31,080 --> 00:26:35,220
for one of the 500 responses pull back

00:26:33,630 --> 00:26:36,810
all the logs that relate to it and

00:26:35,220 --> 00:26:38,820
really start doing some in-depth

00:26:36,810 --> 00:26:42,540
analysis as to what what's going on in

00:26:38,820 --> 00:26:44,330
your system and another one worth

00:26:42,540 --> 00:26:52,910
showing is

00:26:44,330 --> 00:26:57,710
response response time greater than one

00:26:52,910 --> 00:26:59,330
yeah so in the demo code I show this in

00:26:57,710 --> 00:27:01,130
the github repository I've added some

00:26:59,330 --> 00:27:03,110
custom headers one of those is upstream

00:27:01,130 --> 00:27:07,880
response time so that tells you how long

00:27:03,110 --> 00:27:10,070
PHP is spending with your code so this

00:27:07,880 --> 00:27:12,380
this graph shows anything greater than

00:27:10,070 --> 00:27:19,730
one second worth of time is spent being

00:27:12,380 --> 00:27:23,030
spent there okay so that's all very well

00:27:19,730 --> 00:27:25,760
for solving problems and kind of

00:27:23,030 --> 00:27:27,260
debugging but one of cabaÃ±as real

00:27:25,760 --> 00:27:29,840
strengths is that it lets you build

00:27:27,260 --> 00:27:31,929
dashboards with historical data that you

00:27:29,840 --> 00:27:35,570
can of harvest from your log files and

00:27:31,929 --> 00:27:38,090
so let's give that a go so I'm going to

00:27:35,570 --> 00:27:42,740
hide the other two the graph and the

00:27:38,090 --> 00:27:46,100
events now all the all the kind of

00:27:42,740 --> 00:27:48,230
graphs and panels are powered by these

00:27:46,100 --> 00:27:50,299
queries at the top so we have to think

00:27:48,230 --> 00:27:51,679
of some queries and that will give us

00:27:50,299 --> 00:27:56,330
some meaningful data which we can then

00:27:51,679 --> 00:27:58,039
graph before I do that I'll just add

00:27:56,330 --> 00:28:01,940
some additional rows here so that we can

00:27:58,039 --> 00:28:05,179
add some more graphs so configure the

00:28:01,940 --> 00:28:08,090
dashboard and then let's create some new

00:28:05,179 --> 00:28:09,770
rows and so you can create as many of

00:28:08,090 --> 00:28:14,900
these as you want you can create them

00:28:09,770 --> 00:28:18,440
big small and grow that and I will just

00:28:14,900 --> 00:28:22,070
move the existing rows down and so there

00:28:18,440 --> 00:28:24,770
are new rows are on top if I save that

00:28:22,070 --> 00:28:26,450
now we've got some blank space there

00:28:24,770 --> 00:28:29,210
that we can hopefully fill with some

00:28:26,450 --> 00:28:30,980
interesting data so the first thing I'm

00:28:29,210 --> 00:28:33,049
going to do is specify a type of logs

00:28:30,980 --> 00:28:38,200
that I'm interested in so I'm going to

00:28:33,049 --> 00:28:38,200
do nginx access if I spell that right

00:28:45,240 --> 00:28:50,520
okay yes did spell that right so that's

00:28:47,920 --> 00:28:53,440
going to show us all of the access logs

00:28:50,520 --> 00:28:55,960
now let's also look for all of the

00:28:53,440 --> 00:28:58,690
access logs but only the lines that

00:28:55,960 --> 00:29:03,820
errored so if I do type engine X is

00:28:58,690 --> 00:29:06,250
access and the response is greater than

00:29:03,820 --> 00:29:09,220
or equal to 500 so that's the error

00:29:06,250 --> 00:29:11,140
series of status codes and then

00:29:09,220 --> 00:29:12,610
underneath in this graph which I like to

00:29:11,140 --> 00:29:13,810
keep open just to make sure that there

00:29:12,610 --> 00:29:17,050
are actually events which match my

00:29:13,810 --> 00:29:19,110
queries you can see success and then

00:29:17,050 --> 00:29:26,410
also on top of that stacked up our

00:29:19,110 --> 00:29:28,600
errors so let's add a pie chart so you

00:29:26,410 --> 00:29:31,390
can once you've got rows you then add

00:29:28,600 --> 00:29:35,740
columns and columns stack up left to

00:29:31,390 --> 00:29:37,900
right so this this panel type is called

00:29:35,740 --> 00:29:40,750
terms and terms is actually an elastic

00:29:37,900 --> 00:29:42,760
search and kind of name for a process

00:29:40,750 --> 00:29:46,090
for anything that you can book it so if

00:29:42,760 --> 00:29:48,220
you imagine HTTP status codes they're

00:29:46,090 --> 00:29:50,200
quite small in number and unique so you

00:29:48,220 --> 00:29:53,440
could have buckets of them and then put

00:29:50,200 --> 00:29:55,890
that into a pie chart equally endpoints

00:29:53,440 --> 00:29:59,200
on your infrastructure so like slash

00:29:55,890 --> 00:30:00,880
slash flappy slash register they're also

00:29:59,200 --> 00:30:04,810
quite small in number so you could make

00:30:00,880 --> 00:30:07,470
pie charts built on those so let's do

00:30:04,810 --> 00:30:12,070
the first one as an error distribution

00:30:07,470 --> 00:30:13,780
so I'm going to show all of the show pie

00:30:12,070 --> 00:30:23,110
chart of where all of the errors are

00:30:13,780 --> 00:30:24,700
occurring in the system ok the field

00:30:23,110 --> 00:30:27,490
that I'm going to do the terms on is

00:30:24,700 --> 00:30:29,590
request which maps to the string at the

00:30:27,490 --> 00:30:31,210
end of the URL and I'm going to select a

00:30:29,590 --> 00:30:36,520
specific query to do and I'm going to

00:30:31,210 --> 00:30:38,920
pick the error responses one thing that

00:30:36,520 --> 00:30:43,210
I forgot to do is to make sure that it

00:30:38,920 --> 00:30:45,280
was a pie chart ok ok so this is

00:30:43,210 --> 00:30:47,950
interesting I mean we've got a really

00:30:45,280 --> 00:30:50,410
trivial example here but all of the

00:30:47,950 --> 00:30:52,150
errors are from the slash floppy

00:30:50,410 --> 00:30:54,100
endpoint which is to be expected because

00:30:52,150 --> 00:30:55,390
that's how I programmed it imagine this

00:30:54,100 --> 00:30:57,190
was your live system

00:30:55,390 --> 00:30:59,830
and you've got one of this pie chart for

00:30:57,190 --> 00:31:01,330
all of your kind of API endpoints you'll

00:30:59,830 --> 00:31:04,929
be able to see proportionately which one

00:31:01,330 --> 00:31:09,610
is giving you the most trouble it's

00:31:04,929 --> 00:31:12,610
going to add another one now and we'll

00:31:09,610 --> 00:31:15,490
do HTTP status code distribution so what

00:31:12,610 --> 00:31:18,610
portion of your requests are 200 versus

00:31:15,490 --> 00:31:26,200
500 504 so we're going to use a terms

00:31:18,610 --> 00:31:28,210
pie chart again okay and this time I'm

00:31:26,200 --> 00:31:31,770
going to use the response field which is

00:31:28,210 --> 00:31:34,410
where we store the HTTP response codes

00:31:31,770 --> 00:31:37,240
I'm going to use a pie chart again and

00:31:34,410 --> 00:31:38,980
this time when I'm picking a query and

00:31:37,240 --> 00:31:43,330
that this data represents I'm going to

00:31:38,980 --> 00:31:45,640
pick all of the access logs available so

00:31:43,330 --> 00:31:47,740
if I select that now I've got this

00:31:45,640 --> 00:31:49,919
colorful graph which shows me there most

00:31:47,740 --> 00:31:52,059
of my requests at 2:01

00:31:49,919 --> 00:31:54,010
some are to hundreds and then there's a

00:31:52,059 --> 00:31:55,570
small kind of smattering of errors there

00:31:54,010 --> 00:31:57,460
now in a minute I'm going to show you

00:31:55,570 --> 00:31:58,660
how to refine that to just one endpoint

00:31:57,460 --> 00:32:01,059
and that's when it gets really

00:31:58,660 --> 00:32:02,890
interesting because for for just one

00:32:01,059 --> 00:32:05,710
endpoint it's great to see what kind of

00:32:02,890 --> 00:32:10,030
behavior is exhibiting on a HTTP level

00:32:05,710 --> 00:32:14,290
but before I do that let's add another

00:32:10,030 --> 00:32:17,110
panel so cabaÃ±a can also show time

00:32:14,290 --> 00:32:21,130
series data so if I choose a histogram

00:32:17,110 --> 00:32:27,730
let's look at our accounts over time so

00:32:21,130 --> 00:32:30,220
I'll just call this one errors and I'll

00:32:27,730 --> 00:32:31,840
choose to make it lines and I'll reduce

00:32:30,220 --> 00:32:34,600
the line width just to make it a bit

00:32:31,840 --> 00:32:37,000
more pretty so I'm going to select a

00:32:34,600 --> 00:32:38,530
query tip that this graph will represent

00:32:37,000 --> 00:32:44,380
and it's going to represent the error

00:32:38,530 --> 00:32:47,350
rate okay so that's I've set the time

00:32:44,380 --> 00:32:48,730
filter to 12 hours so you'll see all of

00:32:47,350 --> 00:32:51,520
the data that I graphed when I was

00:32:48,730 --> 00:32:53,710
playing with this earlier on but just to

00:32:51,520 --> 00:32:56,440
make this look a bit nicer let's kind of

00:32:53,710 --> 00:33:01,570
make that span the whole call and the

00:32:56,440 --> 00:33:05,260
whole chart so now you've got errors

00:33:01,570 --> 00:33:07,929
over time and now I'm going to add

00:33:05,260 --> 00:33:09,270
another one which will represent latency

00:33:07,929 --> 00:33:11,540
because we've got the response too

00:33:09,270 --> 00:33:18,450
so we could graph that over time as well

00:33:11,540 --> 00:33:21,540
so I choose a histogram show latency and

00:33:18,450 --> 00:33:26,250
choose it to chu'lak choose lines again

00:33:21,540 --> 00:33:31,350
line width set it to something nice and

00:33:26,250 --> 00:33:33,570
then you choose that okay now the

00:33:31,350 --> 00:33:35,970
important change between this this graph

00:33:33,570 --> 00:33:37,200
and the last one is that the chart value

00:33:35,970 --> 00:33:39,210
I'm going to pick this time is going to

00:33:37,200 --> 00:33:43,020
be mean instead of count so I'm no

00:33:39,210 --> 00:33:44,760
longer counting the instances of

00:33:43,020 --> 00:33:47,550
something occurring I'm plotting the

00:33:44,760 --> 00:33:55,220
average of those values so for that I

00:33:47,550 --> 00:34:01,620
have to specify a response time like so

00:33:55,220 --> 00:34:04,740
for all requests and I'll make it pretty

00:34:01,620 --> 00:34:07,050
again by expanding it so that it fills

00:34:04,740 --> 00:34:08,130
up the whole screen so now if you look

00:34:07,050 --> 00:34:11,550
what we've got we've got some error

00:34:08,130 --> 00:34:16,740
distribution HTTP status codes errors

00:34:11,550 --> 00:34:19,620
and latency so that's all interesting

00:34:16,740 --> 00:34:21,210
information but let's make it more

00:34:19,620 --> 00:34:23,250
interesting I'm going to fire up Jaime

00:34:21,210 --> 00:34:25,710
to now and this is a script which

00:34:23,250 --> 00:34:29,010
basically makes loads and loads of calls

00:34:25,710 --> 00:34:31,110
to that endpoint so I've been looking in

00:34:29,010 --> 00:34:33,120
the past at the moment and kind of

00:34:31,110 --> 00:34:36,090
twelve hours ago but now if I set this

00:34:33,120 --> 00:34:39,120
graph to be the last five minutes and I

00:34:36,090 --> 00:34:41,550
set it to auto refresh so jmeter is

00:34:39,120 --> 00:34:43,950
hopefully generating a lot of traffic

00:34:41,550 --> 00:34:46,850
which should appear on these graphs

00:34:43,950 --> 00:34:46,850
surely

00:34:50,770 --> 00:35:02,320
hmm there you go oh thanks jmeter okay

00:34:58,630 --> 00:35:03,640
so this I've only just started Jamie -

00:35:02,320 --> 00:35:04,540
but if we left this running for five

00:35:03,640 --> 00:35:06,220
minutes

00:35:04,540 --> 00:35:07,870
that line would kind of go all the way

00:35:06,220 --> 00:35:09,630
across the page and if you left this

00:35:07,870 --> 00:35:12,010
running in your office you'd then have

00:35:09,630 --> 00:35:14,380
real error rates coming from your

00:35:12,010 --> 00:35:16,860
production server real latency rates and

00:35:14,380 --> 00:35:19,180
you can just leave it there refreshing

00:35:16,860 --> 00:35:21,010
and a technique that we've used in the

00:35:19,180 --> 00:35:22,510
past is to set up a number of these

00:35:21,010 --> 00:35:25,270
dashboards which represent different

00:35:22,510 --> 00:35:28,930
areas of the system and then Chrome has

00:35:25,270 --> 00:35:31,150
got a rotator tab rotator tool so you

00:35:28,930 --> 00:35:37,120
can just set Chrome to rotate through

00:35:31,150 --> 00:35:40,570
each one of these graphs so handy couple

00:35:37,120 --> 00:35:43,300
more things to show you so HTTP status

00:35:40,570 --> 00:35:44,770
codes isn't very useful at the moment

00:35:43,300 --> 00:35:46,600
because it's showing us the makeup of

00:35:44,770 --> 00:35:48,910
all the codes over the whole

00:35:46,600 --> 00:35:51,640
infrastructure if we create a new query

00:35:48,910 --> 00:35:56,230
and this time we'll go for type of

00:35:51,640 --> 00:36:03,120
Records nginx access logs and we'll do

00:35:56,230 --> 00:36:06,010
request is for floppy floppy endpoint

00:36:03,120 --> 00:36:08,650
okay so we've now got a query which is

00:36:06,010 --> 00:36:09,910
in blue here which represents just the

00:36:08,650 --> 00:36:14,080
request which went to a particular

00:36:09,910 --> 00:36:18,430
endpoint and now if I configure that

00:36:14,080 --> 00:36:22,210
chart and tell it to use the new query

00:36:18,430 --> 00:36:23,890
which is this fluffy one I now get a

00:36:22,210 --> 00:36:25,900
much more interesting and relevant graph

00:36:23,890 --> 00:36:28,510
so here we've got a graph which is

00:36:25,900 --> 00:36:30,310
showing the distribution of status codes

00:36:28,510 --> 00:36:32,200
for a particular endpoint in the last

00:36:30,310 --> 00:36:34,810
five minutes so you can see there's a

00:36:32,200 --> 00:36:36,580
nice healthy chunk of to hundreds but

00:36:34,810 --> 00:36:38,380
there's also a bit of a fat slice of

00:36:36,580 --> 00:36:40,000
five hundreds as well so that would be

00:36:38,380 --> 00:36:44,920
ringing alarm bells for the development

00:36:40,000 --> 00:36:46,900
team you can also change the colors here

00:36:44,920 --> 00:36:49,560
so if I this errors one for example

00:36:46,900 --> 00:36:51,580
could be red and if I close that

00:36:49,560 --> 00:36:55,210
aesthetically now that looks a lot more

00:36:51,580 --> 00:36:57,910
error like and then you can pin these

00:36:55,210 --> 00:37:00,580
queries as well so pinning them makes

00:36:57,910 --> 00:37:02,530
them smaller and and then you can also

00:37:00,580 --> 00:37:05,460
hide the query bar as well so it makes

00:37:02,530 --> 00:37:08,290
it really dashboard like

00:37:05,460 --> 00:37:11,580
and there's one last kind of graph that

00:37:08,290 --> 00:37:11,580
I will show you which is quite useful

00:37:13,090 --> 00:37:20,040
it's called sparklines apparently this

00:37:15,280 --> 00:37:24,310
one is this one is experimental

00:37:20,040 --> 00:37:27,430
so sparklines show you a graph with no

00:37:24,310 --> 00:37:31,150
axes and it's updated over time so it

00:37:27,430 --> 00:37:39,850
shows you I'm the shape of data and so

00:37:31,150 --> 00:37:44,470
let's select this floppy my doing let's

00:37:39,850 --> 00:37:47,170
do it all for the moment okay so this

00:37:44,470 --> 00:37:49,780
Sparx thing and so what I haven't done

00:37:47,170 --> 00:37:51,910
there is select an appropriate query but

00:37:49,780 --> 00:37:53,620
it it will work on any query that we've

00:37:51,910 --> 00:37:55,930
done so far so this one's particularly

00:37:53,620 --> 00:38:00,130
useful for response time data so you can

00:37:55,930 --> 00:38:02,470
have all of your API endpoints in the

00:38:00,130 --> 00:38:05,320
query box up there I'm showing response

00:38:02,470 --> 00:38:08,110
time and graph it in a little box like

00:38:05,320 --> 00:38:11,590
that and you can then kind of label them

00:38:08,110 --> 00:38:14,590
as well so if I and if I pick something

00:38:11,590 --> 00:38:16,210
like all here press close and it that

00:38:14,590 --> 00:38:21,160
makes its way onto the diagram as well

00:38:16,210 --> 00:38:29,730
it's quite useful so that is the live

00:38:21,160 --> 00:38:34,290
demo pretty please nothing exploded okay

00:38:29,730 --> 00:38:39,070
live demo over let's get back to logging

00:38:34,290 --> 00:38:41,920
these were just in case it broke we

00:38:39,070 --> 00:38:45,310
don't have to use those today so what I

00:38:41,920 --> 00:38:48,100
showed you there was and the output of

00:38:45,310 --> 00:38:50,380
that process basically reading from

00:38:48,100 --> 00:38:52,450
elasticsearch log stash comes into its

00:38:50,380 --> 00:38:54,520
own as a layer of getting events into

00:38:52,450 --> 00:38:58,300
elasticsearch so how does it achieve

00:38:54,520 --> 00:39:01,960
that so I've spoken a little bit about

00:38:58,300 --> 00:39:04,210
the log stash forwarder this is what it

00:39:01,960 --> 00:39:05,920
looks like to configure until recently

00:39:04,210 --> 00:39:09,000
you could get install it now you have to

00:39:05,920 --> 00:39:11,920
compile it from source don't know why

00:39:09,000 --> 00:39:15,610
you just specify a server end point for

00:39:11,920 --> 00:39:17,829
your a lot log slash instance a time out

00:39:15,610 --> 00:39:21,160
you have to send all of the

00:39:17,829 --> 00:39:24,029
log entries over SSL and then you've got

00:39:21,160 --> 00:39:27,190
an array of files or groups of files

00:39:24,029 --> 00:39:30,279
that the forwarder will then tail and

00:39:27,190 --> 00:39:32,499
send back to logs - the important thing

00:39:30,279 --> 00:39:35,559
in this diagram or in this config file

00:39:32,499 --> 00:39:37,809
is this field section here and I'm

00:39:35,559 --> 00:39:39,640
adding a type field and the type field

00:39:37,809 --> 00:39:42,009
is really important in logs - because it

00:39:39,640 --> 00:39:46,569
lets us differentiate between different

00:39:42,009 --> 00:39:48,930
types of logs and that enginex - access

00:39:46,569 --> 00:39:51,400
is what I was just using to kind of

00:39:48,930 --> 00:39:53,859
narrow down all the data to just the

00:39:51,400 --> 00:39:59,380
nginx access logs so remember to put

00:39:53,859 --> 00:40:01,779
that in there now logstash process

00:39:59,380 --> 00:40:03,249
processing itself this is on the log

00:40:01,779 --> 00:40:07,499
stash machine rather than on the web

00:40:03,249 --> 00:40:11,650
machine has got a input a filter and

00:40:07,499 --> 00:40:13,269
input filter and out section the input

00:40:11,650 --> 00:40:15,039
section is really simple it's just

00:40:13,269 --> 00:40:18,430
opening up a port for us to throw all

00:40:15,039 --> 00:40:20,769
that data towards together with some SSL

00:40:18,430 --> 00:40:22,930
config but at this point you could also

00:40:20,769 --> 00:40:25,479
specify something like Redis and it will

00:40:22,930 --> 00:40:30,400
use Redis lists and Redis cues to kind

00:40:25,479 --> 00:40:33,309
of ingest from a Redis cluster and then

00:40:30,400 --> 00:40:34,900
you have a filter section this is

00:40:33,309 --> 00:40:38,019
probably the most important section of

00:40:34,900 --> 00:40:40,539
the log stash config because it allows

00:40:38,019 --> 00:40:43,449
you to say that if the type field is

00:40:40,539 --> 00:40:46,479
equal to a certain type then do this

00:40:43,449 --> 00:40:49,049
list of things and in this case this

00:40:46,479 --> 00:40:50,920
list of things contains a grok filter

00:40:49,049 --> 00:40:52,779
and I'm going to talk about this in

00:40:50,920 --> 00:40:54,880
detail in a minute but for the moment

00:40:52,779 --> 00:40:57,160
think of it as a way of passing or

00:40:54,880 --> 00:40:58,839
everything into tokens and putting those

00:40:57,160 --> 00:41:02,319
tokens into a document to send to

00:40:58,839 --> 00:41:03,670
elasticsearch so that's really important

00:41:02,319 --> 00:41:06,969
you'll find yourself doing this all the

00:41:03,670 --> 00:41:09,309
time and then this date action here is

00:41:06,969 --> 00:41:11,589
also very important because by default

00:41:09,309 --> 00:41:14,109
log stash will take information in and

00:41:11,589 --> 00:41:16,719
it will timestamp it with the data that

00:41:14,109 --> 00:41:17,739
at the day time it was ingested which is

00:41:16,719 --> 00:41:21,219
no good for us because if we're

00:41:17,739 --> 00:41:23,079
ingesting access logs we want to know

00:41:21,219 --> 00:41:25,299
the time the server requests and

00:41:23,079 --> 00:41:29,349
accepted the request not the time that

00:41:25,299 --> 00:41:31,359
log stash ingested it it's even more

00:41:29,349 --> 00:41:32,120
important if you're going to backfill

00:41:31,359 --> 00:41:35,300
all of your

00:41:32,120 --> 00:41:38,000
or log data from your existing nginx

00:41:35,300 --> 00:41:39,530
instances into this because otherwise it

00:41:38,000 --> 00:41:43,700
will look like all that traffic happened

00:41:39,530 --> 00:41:46,130
like today and then there's an output

00:41:43,700 --> 00:41:47,300
section so the output section in this

00:41:46,130 --> 00:41:49,790
case is really simple it's just

00:41:47,300 --> 00:41:53,060
elasticsearch logstash takes care of

00:41:49,790 --> 00:41:56,930
creating indexes qivana takes care of

00:41:53,060 --> 00:41:58,280
reading those indexes kind of following

00:41:56,930 --> 00:41:59,990
a naming convention that they both

00:41:58,280 --> 00:42:02,360
agreed on so that's the only line for

00:41:59,990 --> 00:42:05,660
output equally here though you could do

00:42:02,360 --> 00:42:09,830
a if type equals and then your define

00:42:05,660 --> 00:42:16,010
type then send send the log entry to

00:42:09,830 --> 00:42:17,890
stats D or send it to graphite so now

00:42:16,010 --> 00:42:20,960
I'm going to talk about grokking

00:42:17,890 --> 00:42:25,730
grokking is probably the most important

00:42:20,960 --> 00:42:27,530
concept for log stash you have a grok

00:42:25,730 --> 00:42:29,960
filter at the top and then it matches

00:42:27,530 --> 00:42:31,430
the contents of a field that the log

00:42:29,960 --> 00:42:34,670
stash forwarder will always send

00:42:31,430 --> 00:42:37,130
something with a message and then in

00:42:34,670 --> 00:42:40,700
that section you've got a token and the

00:42:37,130 --> 00:42:42,980
token there is actually a combination of

00:42:40,700 --> 00:42:44,870
other tokens grouped together for your

00:42:42,980 --> 00:42:46,610
convenience but each one of those

00:42:44,870 --> 00:42:49,880
smaller tokens maps to a regular

00:42:46,610 --> 00:42:53,030
expression the idea being that you can

00:42:49,880 --> 00:42:57,530
put together common tokens to pass a

00:42:53,030 --> 00:42:59,570
regular log files unholy if you have

00:42:57,530 --> 00:43:01,580
like mysql or if you have rabbit

00:42:59,570 --> 00:43:03,860
somebody will have already invented the

00:43:01,580 --> 00:43:06,670
set of tokens you need to pass those

00:43:03,860 --> 00:43:11,180
logs but here is an example underneath

00:43:06,670 --> 00:43:12,770
of an irregular log format and the set

00:43:11,180 --> 00:43:15,200
of tokens that you would need to put in

00:43:12,770 --> 00:43:17,030
those inverted commas to pass that so no

00:43:15,200 --> 00:43:21,230
regular expressions actually we're just

00:43:17,030 --> 00:43:25,340
looking for an IP address a word a UI

00:43:21,230 --> 00:43:27,710
per am number and a number and here

00:43:25,340 --> 00:43:30,140
client method requests by its duration

00:43:27,710 --> 00:43:35,630
those are labels to what that data will

00:43:30,140 --> 00:43:38,270
end up as in elasticsearch so earlier on

00:43:35,630 --> 00:43:40,400
I was saying I added a number of fields

00:43:38,270 --> 00:43:42,410
to the standard nginx

00:43:40,400 --> 00:43:45,080
log format which uses the combined

00:43:42,410 --> 00:43:45,500
Apache log format all I had to do to

00:43:45,080 --> 00:43:48,980
make that

00:43:45,500 --> 00:43:51,440
work was to quote this token and then

00:43:48,980 --> 00:43:53,480
two of these tokens because it's just

00:43:51,440 --> 00:43:55,730
the normal format plus two additional

00:43:53,480 --> 00:43:57,920
numbers which I asked nginx to drop in

00:43:55,730 --> 00:44:00,680
the log file so you can see how you can

00:43:57,920 --> 00:44:04,340
quickly build up a really great kind of

00:44:00,680 --> 00:44:07,100
passing passing config file without much

00:44:04,340 --> 00:44:09,050
effort that said you can't see it

00:44:07,100 --> 00:44:10,370
because it's falling off the screen but

00:44:09,050 --> 00:44:13,220
it is really difficult to get your head

00:44:10,370 --> 00:44:16,220
round to start with so this croc debug

00:44:13,220 --> 00:44:18,860
Heroku opcom lets you put some log data

00:44:16,220 --> 00:44:20,750
in a field at the top and then tokens

00:44:18,860 --> 00:44:22,850
underneath and it will show you what the

00:44:20,750 --> 00:44:25,760
result would be if it was passed by croc

00:44:22,850 --> 00:44:27,500
so that's useful if you if you're new

00:44:25,760 --> 00:44:30,410
and then underneath that there's a link

00:44:27,500 --> 00:44:33,890
to a definition file for all of those

00:44:30,410 --> 00:44:35,240
grok patterns which is useful if you

00:44:33,890 --> 00:44:41,090
want to know what's available first time

00:44:35,240 --> 00:44:43,040
around so coming towards the end of the

00:44:41,090 --> 00:44:45,530
talk we now know hopefully how to do

00:44:43,040 --> 00:44:47,600
some logging in PHP how to ingest all

00:44:45,530 --> 00:44:49,640
that into log stash so here are some

00:44:47,600 --> 00:44:53,210
logging ideas that we've used in the

00:44:49,640 --> 00:44:56,210
past having the idea of a release marker

00:44:53,210 --> 00:44:59,600
is really good so if you think of

00:44:56,210 --> 00:45:02,000
something which can put an event into a

00:44:59,600 --> 00:45:04,070
log every time a release occurs then you

00:45:02,000 --> 00:45:06,650
can create graphs which happen over time

00:45:04,070 --> 00:45:08,960
like the error and the latency and have

00:45:06,650 --> 00:45:12,190
a vertical line when your release

00:45:08,960 --> 00:45:14,660
occurred so you can see on those graphs

00:45:12,190 --> 00:45:16,610
before the release and after the release

00:45:14,660 --> 00:45:18,350
and you can make quick judgments about

00:45:16,610 --> 00:45:19,910
whether that was a success or a failure

00:45:18,350 --> 00:45:24,470
whether you roll back or whether you

00:45:19,910 --> 00:45:27,140
stay error rates of various applications

00:45:24,470 --> 00:45:29,720
over time so I showed you every race

00:45:27,140 --> 00:45:31,940
there of the web server but imagine if

00:45:29,720 --> 00:45:34,120
you've got microservices architecture or

00:45:31,940 --> 00:45:36,380
you've got n tiered architecture and

00:45:34,120 --> 00:45:38,060
make sure you're displaying all of the

00:45:36,380 --> 00:45:40,280
error rates all the way through the tier

00:45:38,060 --> 00:45:42,290
and then that's much better for fault

00:45:40,280 --> 00:45:45,110
isolation than trying to figure out by

00:45:42,290 --> 00:45:49,100
going to the web log Web Access log and

00:45:45,110 --> 00:45:51,680
then work back yourself and one thing I

00:45:49,100 --> 00:45:53,870
didn't cover in the previous example

00:45:51,680 --> 00:45:56,690
because it's a little hard to do there

00:45:53,870 --> 00:45:59,360
are plenty of sites that show you how to

00:45:56,690 --> 00:46:01,880
do this is latency in the various

00:45:59,360 --> 00:46:04,940
Gentiles of your applications here so

00:46:01,880 --> 00:46:07,460
really want to know what is latency in

00:46:04,940 --> 00:46:10,430
the 95th percentile to give you a kind

00:46:07,460 --> 00:46:13,450
of worst case scenario what a customer's

00:46:10,430 --> 00:46:17,330
experiencing in that 5% of your

00:46:13,450 --> 00:46:19,580
experience so yeah if you're interested

00:46:17,330 --> 00:46:22,390
in that I can show you the links to

00:46:19,580 --> 00:46:26,090
where that appears in the documentation

00:46:22,390 --> 00:46:28,160
I'm so looking at HTTP responses is also

00:46:26,090 --> 00:46:29,150
really interesting so I showed

00:46:28,160 --> 00:46:31,910
500-series

00:46:29,150 --> 00:46:34,910
a lot here but equally as interesting is

00:46:31,910 --> 00:46:38,090
the 400 series so things like 401 not

00:46:34,910 --> 00:46:40,670
authorized and 403 not allowed they can

00:46:38,090 --> 00:46:42,800
be signs of kind of nefarious access to

00:46:40,670 --> 00:46:46,730
your system so having some graphs which

00:46:42,800 --> 00:46:48,470
show them is a good idea um one thing

00:46:46,730 --> 00:46:50,960
I've not done myself I've always wanted

00:46:48,470 --> 00:46:53,570
to devote the time to is auto get blame

00:46:50,960 --> 00:46:55,820
for production errors so if you've got a

00:46:53,570 --> 00:46:58,940
system which is which knows where the

00:46:55,820 --> 00:47:01,160
error was in the PHP code base and can

00:46:58,940 --> 00:47:02,560
do a quick call to get to say who's the

00:47:01,160 --> 00:47:05,150
person that last touched that

00:47:02,560 --> 00:47:08,530
now ring them and say why is this

00:47:05,150 --> 00:47:12,410
happening I think would be really good

00:47:08,530 --> 00:47:14,570
in the example code base I also tail the

00:47:12,410 --> 00:47:16,970
off and sis logs so while they're not

00:47:14,570 --> 00:47:19,250
immediately interesting to your

00:47:16,970 --> 00:47:21,440
application they are kind of symptomatic

00:47:19,250 --> 00:47:23,240
of your environment so tailing syslog

00:47:21,440 --> 00:47:25,070
and figuring out what's problem could be

00:47:23,240 --> 00:47:27,830
there telling authority figure out

00:47:25,070 --> 00:47:32,360
whether someone's messing with you good

00:47:27,830 --> 00:47:35,240
ideas so at this point I'm going to say

00:47:32,360 --> 00:47:38,600
go forth and do lots of logging but

00:47:35,240 --> 00:47:41,930
there are some gotchas along the way log

00:47:38,600 --> 00:47:43,460
rotation so I got caught out when I was

00:47:41,930 --> 00:47:45,230
doing this a while ago because I was

00:47:43,460 --> 00:47:47,000
creating a bunch of log files but not in

00:47:45,230 --> 00:47:49,820
the log directory so they didn't get

00:47:47,000 --> 00:47:53,090
picked up by log rotation so I ended up

00:47:49,820 --> 00:47:55,010
producing huge files which are really

00:47:53,090 --> 00:47:56,930
difficult to kind of manipulate later on

00:47:55,010 --> 00:48:00,470
so remember

00:47:56,930 --> 00:48:03,170
log rotation also beware of running out

00:48:00,470 --> 00:48:04,850
of space so if you're a company which

00:48:03,170 --> 00:48:07,430
hasn't done this type of logging before

00:48:04,850 --> 00:48:09,320
and then you put kind of thousands of

00:48:07,430 --> 00:48:11,390
these events in your system you're going

00:48:09,320 --> 00:48:13,260
to start generating huge log files so

00:48:11,390 --> 00:48:16,860
have a procedure in place for kind

00:48:13,260 --> 00:48:20,280
of making sure that rotated out deleted

00:48:16,860 --> 00:48:22,950
when they've got into logs - on one

00:48:20,280 --> 00:48:27,630
particular problem we had when I worked

00:48:22,950 --> 00:48:29,700
for a large betting company was the the

00:48:27,630 --> 00:48:31,980
logging process logged lots and lots of

00:48:29,700 --> 00:48:34,650
things to disk but if you're if you're

00:48:31,980 --> 00:48:37,110
on NFS or if you're on a shared folder

00:48:34,650 --> 00:48:40,980
system on VirtualBox something like that

00:48:37,110 --> 00:48:42,510
and I open sysm great so if you're doing

00:48:40,980 --> 00:48:45,270
thousands of these every time you get a

00:48:42,510 --> 00:48:50,010
request then you can expect IO to be a

00:48:45,270 --> 00:48:51,930
problem so does anybody have any

00:48:50,010 --> 00:48:55,880
questions about anything I've mentioned

00:48:51,930 --> 00:48:55,880
um and is there a mic

00:49:15,330 --> 00:49:23,170
okay first question hi hello

00:49:20,140 --> 00:49:24,700
I um I have a question about IO mm-hmm

00:49:23,170 --> 00:49:26,710
in a setup that's not a production

00:49:24,700 --> 00:49:29,530
environment would it be possible to use

00:49:26,710 --> 00:49:33,040
this but on the web servers not right to

00:49:29,530 --> 00:49:34,510
file at all but sends lots - yeah so

00:49:33,040 --> 00:49:37,840
there are there are other options other

00:49:34,510 --> 00:49:40,359
than using the file shipper which is the

00:49:37,840 --> 00:49:45,970
standard configuration so any of the

00:49:40,359 --> 00:49:48,820
kind of em cues RabbitMQ or 0 mq you can

00:49:45,970 --> 00:49:51,369
do it via syslog so syslog can send logs

00:49:48,820 --> 00:49:53,920
by UDP without writing into a file

00:49:51,369 --> 00:49:57,310
there's literally I don't know whether I

00:49:53,920 --> 00:49:59,230
can just show you on the log stash docks

00:49:57,310 --> 00:50:01,990
just to give you a scale of when I talk

00:49:59,230 --> 00:50:05,440
about it as a bus ins and outs and

00:50:01,990 --> 00:50:09,910
filters there's just so much there so on

00:50:05,440 --> 00:50:12,220
the input section file exec what's that

00:50:09,910 --> 00:50:16,420
I'm a lumberjack was why I showed you

00:50:12,220 --> 00:50:17,890
before log4j Redis s3 standard in

00:50:16,420 --> 00:50:20,380
there's just so many different ways you

00:50:17,890 --> 00:50:24,369
can get data in and so many different

00:50:20,380 --> 00:50:27,190
ways you can push it out sure any other

00:50:24,369 --> 00:50:32,410
questions I'm so there's a gentleman in

00:50:27,190 --> 00:50:35,320
the white shirt here very good talk

00:50:32,410 --> 00:50:37,240
thank you thank you you talked about at

00:50:35,320 --> 00:50:39,580
the end about running out of space and I

00:50:37,240 --> 00:50:42,940
think you meant on the application

00:50:39,580 --> 00:50:46,030
servers yes presumably you don't have

00:50:42,940 --> 00:50:48,100
infinite storage in in elastics elastic

00:50:46,030 --> 00:50:51,760
search is it and so what's your strategy

00:50:48,100 --> 00:50:54,310
for for kind of pruning this big

00:50:51,760 --> 00:50:56,410
mountain of logs yep that's quite a good

00:50:54,310 --> 00:50:57,700
question and I think I've just in the

00:50:56,410 --> 00:50:59,650
rush to get to the end just skimmed over

00:50:57,700 --> 00:51:04,119
that completely so thanks for asking um

00:50:59,650 --> 00:51:07,390
logstash will create a index for every

00:51:04,119 --> 00:51:09,910
day so one tactic to quickly get rid of

00:51:07,390 --> 00:51:12,850
a lot of old log data that you might not

00:51:09,910 --> 00:51:18,780
want would be to delete the indexes

00:51:12,850 --> 00:51:22,630
create a year ago you can also kind of

00:51:18,780 --> 00:51:24,880
you can also store the data in s3 and

00:51:22,630 --> 00:51:26,920
then bring it back into elastic search

00:51:24,880 --> 00:51:28,540
at some point in the future so maybe you

00:51:26,920 --> 00:51:30,850
want a log resolution of

00:51:28,540 --> 00:51:33,010
every event in the excuse me every event

00:51:30,850 --> 00:51:34,990
in the system for a year but then you

00:51:33,010 --> 00:51:37,810
want to kind of store all that old data

00:51:34,990 --> 00:51:40,900
in s3 so if anything ever you need to

00:51:37,810 --> 00:51:42,670
kind of do a bigger pattern analysis

00:51:40,900 --> 00:51:44,830
over say two years you just bring the

00:51:42,670 --> 00:51:46,330
data back from that storage but that's

00:51:44,830 --> 00:51:48,750
not something logstash can help you with

00:51:46,330 --> 00:51:51,120
unfortunately that's kind of an OPS task

00:51:48,750 --> 00:52:00,520
thank you

00:51:51,120 --> 00:52:02,650
question over here yeah thanks for the

00:52:00,520 --> 00:52:04,990
talk again I was just wondering about

00:52:02,650 --> 00:52:06,820
logging on all of the front end so

00:52:04,990 --> 00:52:09,010
you've got your JavaScript code written

00:52:06,820 --> 00:52:12,010
and you want to start logging that and

00:52:09,010 --> 00:52:14,830
maybe the response time that the end

00:52:12,010 --> 00:52:16,570
customer sees from what is still loading

00:52:14,830 --> 00:52:18,580
on the JavaScript all the resources and

00:52:16,570 --> 00:52:20,980
assets and whether you had that in place

00:52:18,580 --> 00:52:22,900
where you are and yeah whatever you like

00:52:20,980 --> 00:52:25,300
I've kind of caveats there I earn around

00:52:22,900 --> 00:52:27,040
that kind of process so I'm gonna have

00:52:25,300 --> 00:52:29,410
to say I've got not much knowledge of

00:52:27,040 --> 00:52:31,660
that area I'm so server side I like all

00:52:29,410 --> 00:52:34,660
my websites to be rendered in Jason and

00:52:31,660 --> 00:52:37,000
nothing else but I appreciate there is a

00:52:34,660 --> 00:52:38,470
market for that kind of logging I'm but

00:52:37,000 --> 00:52:47,350
I don't know what it is I'm sorry okay

00:52:38,470 --> 00:52:50,560
Cheers can you use log stash to send out

00:52:47,350 --> 00:52:52,630
email alerts on say say you get a number

00:52:50,560 --> 00:52:54,910
of errors or something long yes I think

00:52:52,630 --> 00:52:57,850
you can I'm one of the kind of outputs

00:52:54,910 --> 00:52:59,860
here is email and whether or not you'd

00:52:57,850 --> 00:53:03,210
want to is a different matter so I guess

00:52:59,860 --> 00:53:06,130
you could write the config file and that

00:53:03,210 --> 00:53:07,660
the config file that would do that but

00:53:06,130 --> 00:53:12,340
what equally you could just send an

00:53:07,660 --> 00:53:15,190
event to any kind of ops phone call

00:53:12,340 --> 00:53:20,140
system and have that I've had them do it

00:53:15,190 --> 00:53:23,290
and so yes I would probably given a

00:53:20,140 --> 00:53:25,930
number of events and occur that gets

00:53:23,290 --> 00:53:27,850
sent via log stash to your ops alerting

00:53:25,930 --> 00:53:31,000
system the thresholds and the logic

00:53:27,850 --> 00:53:33,240
could be in there rather than in log

00:53:31,000 --> 00:53:33,240
stash

00:53:44,490 --> 00:53:49,650
hey I'm assuming let's group everything

00:53:48,240 --> 00:53:51,660
you just did in the demo as one

00:53:49,650 --> 00:53:53,010
deployment and you spent ages making

00:53:51,660 --> 00:53:54,180
your dashboard and getting all of the

00:53:53,010 --> 00:53:56,250
rafts and everything looks brilliant

00:53:54,180 --> 00:53:58,830
could you export those dashboard

00:53:56,250 --> 00:54:00,869
configurations to move to other yep

00:53:58,830 --> 00:54:04,740
so the log status yep the log stash

00:54:00,869 --> 00:54:06,839
itself you can save um and what is

00:54:04,740 --> 00:54:09,420
actually how I think is adjacent file

00:54:06,839 --> 00:54:12,000
and you can even version control that so

00:54:09,420 --> 00:54:13,650
if you've got a couple of developers

00:54:12,000 --> 00:54:16,290
working on it or if you want to deploy

00:54:13,650 --> 00:54:18,570
it automatically to new instances they

00:54:16,290 --> 00:54:29,609
can work that kind of thing yeah it's

00:54:18,570 --> 00:54:32,250
version control and deployable thanks I

00:54:29,609 --> 00:54:34,950
have a question if for instance you're

00:54:32,250 --> 00:54:39,000
logging errors from application is there

00:54:34,950 --> 00:54:41,540
any way for for this stack to aggregate

00:54:39,000 --> 00:54:44,750
those errors for instance I will I have

00:54:41,540 --> 00:54:47,300
three kinds of errors and one sense

00:54:44,750 --> 00:54:51,210
million million of error messages and

00:54:47,300 --> 00:54:53,280
other are less so so I'd like to have

00:54:51,210 --> 00:54:56,250
three lines that are up to generate

00:54:53,280 --> 00:54:58,800
generate errors hmm so that is quite an

00:54:56,250 --> 00:55:00,690
advanced use case and I haven't done it

00:54:58,800 --> 00:55:02,880
personally but I think it's probably

00:55:00,690 --> 00:55:06,630
possible because of the aggregation

00:55:02,880 --> 00:55:08,609
features of elasticsearch so I guess

00:55:06,630 --> 00:55:10,349
Cubano is a front-end I don't know if we

00:55:08,609 --> 00:55:12,390
can do it but definitely if you were

00:55:10,349 --> 00:55:14,460
written elastic search query you could

00:55:12,390 --> 00:55:17,010
get that data so I suspect there's a way

00:55:14,460 --> 00:55:22,619
to put the two together but yeah I'm not

00:55:17,010 --> 00:55:24,800
sure if Cubana can do it thanks anybody

00:55:22,619 --> 00:55:24,800
else

00:55:25,819 --> 00:55:35,630
I just there this gentleman here in the

00:55:29,449 --> 00:55:39,309
front row hi just a good question to get

00:55:35,630 --> 00:55:41,809
an idea of scale so how many

00:55:39,309 --> 00:55:43,729
applications servers or services you

00:55:41,809 --> 00:55:45,979
need to have to start thinking of the

00:55:43,729 --> 00:55:49,430
cluster solution yeah like how much

00:55:45,979 --> 00:55:54,769
single server solution can homes so I've

00:55:49,430 --> 00:55:58,160
had a single server receiving logs from

00:55:54,769 --> 00:56:01,309
a hundred instances and that seemed to

00:55:58,160 --> 00:56:04,640
perform well for me however I've not

00:56:01,309 --> 00:56:06,969
really done that scale in production so

00:56:04,640 --> 00:56:09,499
for a test deployment I've done that and

00:56:06,969 --> 00:56:11,029
in terms of what we do at Sainsbury's

00:56:09,499 --> 00:56:15,709
we're probably looking at log stash

00:56:11,029 --> 00:56:18,650
across about 10 to 20 instances so I

00:56:15,709 --> 00:56:21,140
think I think you can scale and the kind

00:56:18,650 --> 00:56:25,039
of log collecting element of log stash

00:56:21,140 --> 00:56:28,309
by using Redis cluster and also the kind

00:56:25,039 --> 00:56:32,359
of storage aspect using elastic search

00:56:28,309 --> 00:56:36,229
clusters and you can have multiple log

00:56:32,359 --> 00:56:38,119
stash instances and ship kind of logs to

00:56:36,229 --> 00:56:39,559
different ones and they'll pour both

00:56:38,119 --> 00:56:41,809
into the because they're both doing this

00:56:39,559 --> 00:56:44,209
the same kind of processing and they're

00:56:41,809 --> 00:56:45,829
putting it into the same data source so

00:56:44,209 --> 00:56:48,670
I think the whole ELQ stack is

00:56:45,829 --> 00:56:48,670
horizontally scalable

00:56:52,560 --> 00:57:03,270
any more questions just a quick

00:56:59,910 --> 00:57:05,370
practical thing in terms of monologue

00:57:03,270 --> 00:57:09,030
what would you use on a production

00:57:05,370 --> 00:57:11,310
server for pushing logs out because the

00:57:09,030 --> 00:57:13,020
one thing that I would want to really

00:57:11,310 --> 00:57:15,270
avoid is any sort of i/o from logging

00:57:13,020 --> 00:57:17,520
hence I would never write to disk yep so

00:57:15,270 --> 00:57:20,430
how do you you know push the the logs

00:57:17,520 --> 00:57:25,920
out in some other way and wondering yeah

00:57:20,430 --> 00:57:27,870
we use so the so I've I personally

00:57:25,920 --> 00:57:28,440
always write to a file using the fingers

00:57:27,870 --> 00:57:31,230
crossed

00:57:28,440 --> 00:57:34,560
method so that I only get say one or two

00:57:31,230 --> 00:57:37,020
log lines per request and this is a

00:57:34,560 --> 00:57:40,560
guess but given that the stream handler

00:57:37,020 --> 00:57:42,690
and can write to any stream not just not

00:57:40,560 --> 00:57:44,640
just file streams and given the

00:57:42,690 --> 00:57:48,270
community behind monologue I suspect

00:57:44,640 --> 00:57:59,000
you'll be able to log to nq's or via UDP

00:57:48,270 --> 00:57:59,000
or similar lady over here

00:58:02,750 --> 00:58:08,280
yeah hi what he said about locks - it's

00:58:06,000 --> 00:58:10,800
sounds quite similar to what Splunk does

00:58:08,280 --> 00:58:13,170
so what's the difference or why is it

00:58:10,800 --> 00:58:15,540
better why is it worsened yeah right to

00:58:13,170 --> 00:58:17,130
use lock starts but not spawn so that is

00:58:15,540 --> 00:58:19,650
a really good question I've used spelunk

00:58:17,130 --> 00:58:22,980
and I think it is really really good and

00:58:19,650 --> 00:58:27,060
in the beginning it's quite cheap and so

00:58:22,980 --> 00:58:28,800
the problem is in its like I suppose

00:58:27,060 --> 00:58:29,430
it's like Splunk is like a bit like a

00:58:28,800 --> 00:58:31,860
drug dealer

00:58:29,430 --> 00:58:34,740
they'll keep selling you drugs in terms

00:58:31,860 --> 00:58:37,619
of data storage and your need to consume

00:58:34,740 --> 00:58:39,360
those drugs will increase and and you

00:58:37,619 --> 00:58:41,220
become kind of dependent on them and

00:58:39,360 --> 00:58:43,950
that price increases with the more data

00:58:41,220 --> 00:58:45,270
you need to store and that is true of

00:58:43,950 --> 00:58:47,430
log stash because obviously you need

00:58:45,270 --> 00:58:50,490
more infrastructure to support a bigger

00:58:47,430 --> 00:58:52,860
logging operation but I think it's

00:58:50,490 --> 00:58:57,570
cheaper and yeah that's the reason why I

00:58:52,860 --> 00:59:00,770
use it man the reason is it's a cheaper

00:58:57,570 --> 00:59:00,770
yeah for me

00:59:06,150 --> 00:59:11,490
I was just wondering where the log stash

00:59:08,910 --> 00:59:14,099
will actually pick up abnormal behavior

00:59:11,490 --> 00:59:16,140
from trends whether you can use that to

00:59:14,099 --> 00:59:19,230
alert yourself with yeah so one thing

00:59:16,140 --> 00:59:22,770
that I didn't show is the trends graph

00:59:19,230 --> 00:59:24,660
here so you can set up a query which

00:59:22,770 --> 00:59:28,589
would highlight a normal anomalous

00:59:24,660 --> 00:59:31,680
behavior say number of decline credit

00:59:28,589 --> 00:59:33,720
card transactions and you can show in a

00:59:31,680 --> 00:59:36,690
kind of stock tick away percentage up

00:59:33,720 --> 00:59:40,829
and down over a particular time so if

00:59:36,690 --> 00:59:42,450
you had a kind of separate dashboard to

00:59:40,829 --> 00:59:44,220
this which just had the stock ticker

00:59:42,450 --> 00:59:46,410
metrics on you'd wrote you'd written

00:59:44,220 --> 00:59:48,630
queries which surface that anomalous

00:59:46,410 --> 00:59:51,390
behavior you could very easily then see

00:59:48,630 --> 00:59:55,470
kind of green for red for going up green

00:59:51,390 --> 00:59:56,880
for going down so on the Cubana side

00:59:55,470 --> 01:00:07,589
that that's probably what I'd recommend

00:59:56,880 --> 01:00:08,910
there okay thank you anyone else okay I

01:00:07,589 --> 01:00:11,720
think that's it thank you very much for

01:00:08,910 --> 01:00:11,720

YouTube URL: https://www.youtube.com/watch?v=9dni1D4QQOk


