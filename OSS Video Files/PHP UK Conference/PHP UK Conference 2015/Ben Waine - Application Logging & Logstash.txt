Title: Ben Waine - Application Logging & Logstash
Publication date: 2015-04-15
Playlist: PHP UK Conference 2015
Description: 
	Modern web applications are complex, have many layers and usually integrate many technologies. A well structured application log is invaluable for debugging your application in development and monitoring it in production. All this being said it’s amazing how many applications don't have an application log or have a poor, inconsistent one which is little use to anyone! This session takes a brief look at the basics of logging (libraries and tools) and moves on to look at how to plan an effective logging strategy for your application. Questions like: What to log, logging levels, how much to log and how long to keep historical logs are all addressed. In the second half of the session we consider logs as a stream of events and how we can use Logstash and Kibana to surface a wealth of interesting information about our applications.
Captions: 
	00:00:05,470 --> 00:00:10,850
so my name is Ben wayna I've worked with

00:00:08,120 --> 00:00:13,160
PHP for about five years I'm currently a

00:00:10,850 --> 00:00:14,470
software engineer at Sainsbury's and

00:00:13,160 --> 00:00:16,310
this is the plug bit of the talk

00:00:14,470 --> 00:00:18,140
Sainsbury's are currently building out a

00:00:16,310 --> 00:00:20,480
whole new team we're using technologies

00:00:18,140 --> 00:00:23,840
like log stash so we'd love to hear from

00:00:20,480 --> 00:00:25,910
you at this our store I also

00:00:23,840 --> 00:00:27,410
occasionally dabble in DevOps which the

00:00:25,910 --> 00:00:31,849
amusement of the people we actually pay

00:00:27,410 --> 00:00:33,860
to do that job at Sainsbury's um this is

00:00:31,849 --> 00:00:35,720
the joined in link to the talk so if

00:00:33,860 --> 00:00:38,570
you've got any feedback at the end or in

00:00:35,720 --> 00:00:42,649
the middle and please do send some

00:00:38,570 --> 00:00:45,859
feedback my way so without further ado

00:00:42,649 --> 00:00:48,350
moving on to logging when I talk about

00:00:45,859 --> 00:00:49,460
logging and logging your application I'm

00:00:48,350 --> 00:00:51,769
actually talking about two different

00:00:49,460 --> 00:00:54,320
types of logging log stash is going to

00:00:51,769 --> 00:00:56,179
help us with both of those and the first

00:00:54,320 --> 00:00:58,570
one is a quick win really because it's

00:00:56,179 --> 00:01:01,519
just the system logs which all of your

00:00:58,570 --> 00:01:05,600
technology choices generates so for

00:01:01,519 --> 00:01:08,330
example MySQL slow query log Apache and

00:01:05,600 --> 00:01:09,650
nginx access logs and for information

00:01:08,330 --> 00:01:12,890
about what type of requests are coming

00:01:09,650 --> 00:01:16,250
in also you've got PHP fpm where a lot

00:01:12,890 --> 00:01:17,720
of errors creep in and then all of the

00:01:16,250 --> 00:01:20,060
other pieces of your infrastructure will

00:01:17,720 --> 00:01:21,950
generate logs and you should be

00:01:20,060 --> 00:01:24,020
capturing those logs in order to solve

00:01:21,950 --> 00:01:26,690
problems and also to look at trends in

00:01:24,020 --> 00:01:29,030
your infrastructure so these bits come

00:01:26,690 --> 00:01:31,610
for free and the next bit takes a little

00:01:29,030 --> 00:01:34,880
bit more effort to implement which is

00:01:31,610 --> 00:01:36,860
the application log a while ago I did a

00:01:34,880 --> 00:01:39,290
talk about application logging and I was

00:01:36,860 --> 00:01:41,630
surprised by how many people don't have

00:01:39,290 --> 00:01:44,630
a log for their application it's really

00:01:41,630 --> 00:01:45,950
useful in terms of debugging and so how

00:01:44,630 --> 00:01:49,100
do you get information about your

00:01:45,950 --> 00:01:51,320
application at runtime when you can't

00:01:49,100 --> 00:01:53,510
just fire up a debugging session but

00:01:51,320 --> 00:01:55,850
it's also really useful for looking at

00:01:53,510 --> 00:02:00,290
trends of your of your application over

00:01:55,850 --> 00:02:02,600
time so what should we be putting in an

00:02:00,290 --> 00:02:03,710
application log the first thing the

00:02:02,600 --> 00:02:05,690
first and most important thing is

00:02:03,710 --> 00:02:07,610
debugging information so at some point

00:02:05,690 --> 00:02:09,649
it will be four o'clock in the morning

00:02:07,610 --> 00:02:11,269
some of your servers will be on fire and

00:02:09,649 --> 00:02:13,850
you'll be trying to figure out what the

00:02:11,269 --> 00:02:15,920
hell's going on so you should be logging

00:02:13,850 --> 00:02:17,010
errors and these normally are things

00:02:15,920 --> 00:02:19,049
like connection

00:02:17,010 --> 00:02:21,510
to a remote server which isn't there

00:02:19,049 --> 00:02:23,849
anymore that could cause an error on

00:02:21,510 --> 00:02:25,860
core business exceptions so perhaps a

00:02:23,849 --> 00:02:28,530
situation has arisen in your app that

00:02:25,860 --> 00:02:30,239
you didn't think to handle those should

00:02:28,530 --> 00:02:32,010
be in your log and also resource

00:02:30,239 --> 00:02:33,959
exhaustion how many times have we logged

00:02:32,010 --> 00:02:36,209
into a server and looked at the log to

00:02:33,959 --> 00:02:37,500
find out and they tried to allocate some

00:02:36,209 --> 00:02:39,840
memory that it couldn't because it run

00:02:37,500 --> 00:02:42,599
out or that you exceeded the number of

00:02:39,840 --> 00:02:43,799
file descriptors available so really

00:02:42,599 --> 00:02:47,909
important information that you want to

00:02:43,799 --> 00:02:51,930
that you want to log i'm narrative

00:02:47,909 --> 00:02:53,190
information i'm so information about the

00:02:51,930 --> 00:02:55,049
kind of method calls that have been made

00:02:53,190 --> 00:02:58,709
in your system how long it takes to

00:02:55,049 --> 00:03:00,480
execute a particular method all really

00:02:58,709 --> 00:03:02,879
useful at runtime when you're trying to

00:03:00,480 --> 00:03:04,290
figure out what's going on and also

00:03:02,879 --> 00:03:05,430
event triggers as well so if you're

00:03:04,290 --> 00:03:07,650
triggering events with an event

00:03:05,430 --> 00:03:11,220
dispatcher it's really easy to hang

00:03:07,650 --> 00:03:14,099
logging information off of that and then

00:03:11,220 --> 00:03:16,650
finally this one tends to get overlooked

00:03:14,099 --> 00:03:19,409
i'm but logging business events in a

00:03:16,650 --> 00:03:22,079
separate log next to your application is

00:03:19,409 --> 00:03:23,519
a really worthwhile so Facebook and Etsy

00:03:22,079 --> 00:03:26,519
are kind of pioneers of this technique

00:03:23,519 --> 00:03:29,790
they log these events like purchases

00:03:26,519 --> 00:03:31,019
logins registrations unsubscribes things

00:03:29,790 --> 00:03:33,180
they were important to the business and

00:03:31,019 --> 00:03:35,099
they logged them they count them and

00:03:33,180 --> 00:03:37,199
then they display that information to

00:03:35,099 --> 00:03:39,510
the business also really useful because

00:03:37,199 --> 00:03:41,849
after a deploy if any of these kind of

00:03:39,510 --> 00:03:43,260
soft business e metrics go up or down

00:03:41,849 --> 00:03:46,799
you know you've either done a really

00:03:43,260 --> 00:03:48,329
good or a really bad job and so I'm

00:03:46,799 --> 00:03:52,560
going to demonstrate some of those in

00:03:48,329 --> 00:03:55,199
action shortly so we've all been in this

00:03:52,560 --> 00:03:59,010
situation servers are on fire and we've

00:03:55,199 --> 00:04:01,230
logged into various boxes the web server

00:03:59,010 --> 00:04:03,690
at the top they're tailing the nginx

00:04:01,230 --> 00:04:05,099
access log and perhaps telling your own

00:04:03,690 --> 00:04:07,739
log to try and figure out what's going

00:04:05,099 --> 00:04:09,299
on problem is endure through login to

00:04:07,739 --> 00:04:12,629
another server and after that another

00:04:09,299 --> 00:04:14,669
one trying to identify this problem and

00:04:12,629 --> 00:04:16,620
the point of this story is that this

00:04:14,669 --> 00:04:19,079
isn't really feasible you might be able

00:04:16,620 --> 00:04:20,519
to solve this problem once when it's

00:04:19,079 --> 00:04:22,470
actually happening to you wouldn't it be

00:04:20,519 --> 00:04:25,349
great to kind of preempt this problem I

00:04:22,470 --> 00:04:28,020
have one place to go when times are hard

00:04:25,349 --> 00:04:30,000
and the servers are on fire and also

00:04:28,020 --> 00:04:34,890
look at trends over time

00:04:30,000 --> 00:04:38,550
for this log information now the answer

00:04:34,890 --> 00:04:41,280
to that I think is is the lq stack which

00:04:38,550 --> 00:04:42,960
consists of elasticsearch log stash and

00:04:41,280 --> 00:04:44,760
Cabana those are the three parts that

00:04:42,960 --> 00:04:46,350
I'm going to discuss today there are

00:04:44,760 --> 00:04:48,750
some kind of paid for alternatives to

00:04:46,350 --> 00:04:50,370
this but at Sainsbury's we've had really

00:04:48,750 --> 00:04:53,580
good resorts just using these open

00:04:50,370 --> 00:04:54,000
source free tools so what are these

00:04:53,580 --> 00:04:57,180
things

00:04:54,000 --> 00:04:59,370
I'm elasticsearch is a open source

00:04:57,180 --> 00:05:02,730
search engine you can basically pump it

00:04:59,370 --> 00:05:04,680
full of data and then do clever searches

00:05:02,730 --> 00:05:08,370
and aggregations on that data at a later

00:05:04,680 --> 00:05:09,390
point lasting searchers originally just

00:05:08,370 --> 00:05:11,760
a piece of software but it's now a

00:05:09,390 --> 00:05:14,040
company that was formed around that and

00:05:11,760 --> 00:05:17,000
they owned these other two technologies

00:05:14,040 --> 00:05:20,160
or sponsor the project these other two

00:05:17,000 --> 00:05:22,140
Tech's logstash

00:05:20,160 --> 00:05:24,480
sits in in the middle of this setup you

00:05:22,140 --> 00:05:26,730
can think of logstash as a bus because

00:05:24,480 --> 00:05:29,460
it has a range of inputs some middleware

00:05:26,730 --> 00:05:30,450
and a range of outputs so in the example

00:05:29,460 --> 00:05:33,720
I'm going to show you today I'm going to

00:05:30,450 --> 00:05:36,510
show you a simple tool which tails log

00:05:33,720 --> 00:05:37,530
files and gets it into log stash but

00:05:36,510 --> 00:05:41,520
there's a range of other inputs like

00:05:37,530 --> 00:05:43,320
Redis is another one the various M

00:05:41,520 --> 00:05:44,610
queues so if you if you've got an

00:05:43,320 --> 00:05:46,470
interesting setup and you want to get

00:05:44,610 --> 00:05:49,970
data into log stash go to the website

00:05:46,470 --> 00:05:52,380
there's so many options and then on the

00:05:49,970 --> 00:05:53,760
output side of log stash you've got

00:05:52,380 --> 00:05:55,620
things like elastic search which is

00:05:53,760 --> 00:05:59,100
pretty much the default but you've also

00:05:55,620 --> 00:06:02,370
got things like stats D graphite the M

00:05:59,100 --> 00:06:03,300
cues like 0 mq rabbitmq and then in the

00:06:02,370 --> 00:06:06,360
middle of that you've got a

00:06:03,300 --> 00:06:09,570
transformation layer which can filter

00:06:06,360 --> 00:06:10,860
that data tokenize it and do interesting

00:06:09,570 --> 00:06:13,290
things which I'll demonstrate in a

00:06:10,860 --> 00:06:17,160
minute so think of it as a way to get

00:06:13,290 --> 00:06:18,419
data into various outputs and then on

00:06:17,160 --> 00:06:22,020
the end of this tool chain we've got

00:06:18,419 --> 00:06:24,360
Cabana and Cabana is a way to quickly

00:06:22,020 --> 00:06:27,290
interrogate elasticsearch for log data

00:06:24,360 --> 00:06:31,140
we also allows you to build dashboards

00:06:27,290 --> 00:06:33,300
again that's in the demo in a moment and

00:06:31,140 --> 00:06:38,220
that's that's the end result this is the

00:06:33,300 --> 00:06:40,530
kind of Holy Grail really surfacing log

00:06:38,220 --> 00:06:42,720
data in a way that means something to

00:06:40,530 --> 00:06:43,920
the business so the titles of these logs

00:06:42,720 --> 00:06:47,280
are things like load

00:06:43,920 --> 00:06:49,410
by request a title here is total bytes

00:06:47,280 --> 00:06:51,360
per second by extension so you could

00:06:49,410 --> 00:06:54,750
imagine that being a critical dashboard

00:06:51,360 --> 00:06:56,340
for somebody that runs a CDN and that's

00:06:54,750 --> 00:06:58,020
what we're going to build about halfway

00:06:56,340 --> 00:07:01,080
through this presentation but before

00:06:58,020 --> 00:07:03,360
getting to that point and we need some

00:07:01,080 --> 00:07:05,850
way of generating these logs and so I'm

00:07:03,360 --> 00:07:07,830
going to go back to first principles and

00:07:05,850 --> 00:07:13,140
talk about how we capture logs in a PHP

00:07:07,830 --> 00:07:14,760
application so if a while ago I did that

00:07:13,140 --> 00:07:16,530
presentation where I showed you lots of

00:07:14,760 --> 00:07:18,420
different types of ways of logging data

00:07:16,530 --> 00:07:20,460
for anything from you know file put

00:07:18,420 --> 00:07:22,470
contents into a file various different

00:07:20,460 --> 00:07:24,150
logging libraries but in my old age I've

00:07:22,470 --> 00:07:26,910
become a lot more opinionated and I

00:07:24,150 --> 00:07:28,410
think monologue is such a mature product

00:07:26,910 --> 00:07:29,880
it's such a pleasure to use I've been

00:07:28,410 --> 00:07:32,100
using for about three years it's the

00:07:29,880 --> 00:07:33,680
go-to logger for symphony I highly

00:07:32,100 --> 00:07:36,180
recommend you go and have a look at it

00:07:33,680 --> 00:07:39,210
and all the examples I'm gonna show her

00:07:36,180 --> 00:07:41,160
in monologue so here's what I'm going to

00:07:39,210 --> 00:07:44,040
show and the basics of monologue and

00:07:41,160 --> 00:07:45,090
some important features that make it

00:07:44,040 --> 00:07:48,360
compatible with logstash

00:07:45,090 --> 00:07:54,780
and a little code snippet on logging

00:07:48,360 --> 00:07:57,630
business events so this is a all the

00:07:54,780 --> 00:07:59,490
code which is required to bootstrap a

00:07:57,630 --> 00:08:01,860
monologue logger which is the tool you

00:07:59,490 --> 00:08:03,210
use that you get logs into a log file so

00:08:01,860 --> 00:08:04,410
there's about eight lines they're not

00:08:03,210 --> 00:08:07,050
really normally just sits in the

00:08:04,410 --> 00:08:08,640
bootstrap section of your application so

00:08:07,050 --> 00:08:12,750
I'm going to go through these make them

00:08:08,640 --> 00:08:14,220
a little bit bigger and so the first

00:08:12,750 --> 00:08:16,770
thing that I do in this bootstrap

00:08:14,220 --> 00:08:17,850
process is to pick a log level and I'm

00:08:16,770 --> 00:08:19,230
going to go through the various log

00:08:17,850 --> 00:08:20,550
levels in a minute but at the moment you

00:08:19,230 --> 00:08:23,220
can think about it as a way of choosing

00:08:20,550 --> 00:08:24,810
how verbose your log should be and it

00:08:23,220 --> 00:08:26,430
ranges from really verbose I want to

00:08:24,810 --> 00:08:30,150
know everything about my application to

00:08:26,430 --> 00:08:31,800
only surface the most critical events so

00:08:30,150 --> 00:08:34,830
why am i doing that and using an

00:08:31,800 --> 00:08:39,330
environment variable rather than using

00:08:34,830 --> 00:08:41,010
config or hard coding that value using

00:08:39,330 --> 00:08:43,260
an environment variable lets you log

00:08:41,010 --> 00:08:44,880
into a server maybe it's a problem

00:08:43,260 --> 00:08:46,260
server in your infrastructure and just

00:08:44,880 --> 00:08:48,990
change the environment just that one

00:08:46,260 --> 00:08:50,850
server is running in so it allows you to

00:08:48,990 --> 00:08:52,410
switch that level down to debug but

00:08:50,850 --> 00:08:53,760
without affecting the rest of the web

00:08:52,410 --> 00:08:55,290
tier for example if you're logging into

00:08:53,760 --> 00:08:56,790
the web tier so that's been really

00:08:55,290 --> 00:08:57,270
useful for us when we're debugging

00:08:56,790 --> 00:09:01,950
problems

00:08:57,270 --> 00:09:03,780
specific machines and then in the next

00:09:01,950 --> 00:09:06,810
line I'm just setting a sensible default

00:09:03,780 --> 00:09:10,260
for that level and then creating a new

00:09:06,810 --> 00:09:12,480
logger which is the kind of object of

00:09:10,260 --> 00:09:14,490
interest here once you have a logger

00:09:12,480 --> 00:09:18,420
you can call various methods to write

00:09:14,490 --> 00:09:21,840
some logs so in the next section here

00:09:18,420 --> 00:09:24,210
we've got a stream handler um and a

00:09:21,840 --> 00:09:25,950
fingers-crossed handler so handlers are

00:09:24,210 --> 00:09:28,170
really pointers to some kind of output

00:09:25,950 --> 00:09:30,990
so this stream handler at the top here

00:09:28,170 --> 00:09:33,180
it points to a file and the second

00:09:30,990 --> 00:09:36,120
argument is a log level which have

00:09:33,180 --> 00:09:39,000
hard-coded so why the hard-coded it that

00:09:36,120 --> 00:09:41,280
goes against what I just said so the

00:09:39,000 --> 00:09:43,110
output here would be any any kind of log

00:09:41,280 --> 00:09:45,000
entry would go into this file there's no

00:09:43,110 --> 00:09:48,150
filtering at all and the reason I

00:09:45,000 --> 00:09:49,910
hard-coded it there is because this next

00:09:48,150 --> 00:09:54,530
handler the fingers-crossed handler

00:09:49,910 --> 00:09:57,870
wraps another handler and will only let

00:09:54,530 --> 00:09:59,880
log messages through to it if a log

00:09:57,870 --> 00:10:01,980
entry of a certain level comes along so

00:09:59,880 --> 00:10:04,230
to put that in an example imagine you

00:10:01,980 --> 00:10:06,750
have a request where you logged ten

00:10:04,230 --> 00:10:07,470
debug lines in a row normally you're not

00:10:06,750 --> 00:10:09,980
gonna want to see that information

00:10:07,470 --> 00:10:12,510
because that request was a success

00:10:09,980 --> 00:10:15,060
storing all that information you know

00:10:12,510 --> 00:10:17,010
why would you bother but if a critical

00:10:15,060 --> 00:10:19,320
event happened at event number 11

00:10:17,010 --> 00:10:21,240
suddenly all those are the ten log lines

00:10:19,320 --> 00:10:24,060
become really relevant so fingers

00:10:21,240 --> 00:10:26,010
crossed enables us to capture all the

00:10:24,060 --> 00:10:29,730
data but only when we need it so it's a

00:10:26,010 --> 00:10:33,210
really useful construct and then you

00:10:29,730 --> 00:10:35,580
push that handler onto the app log that

00:10:33,210 --> 00:10:39,780
we just created and then you're ready to

00:10:35,580 --> 00:10:41,820
go so then you have this log object here

00:10:39,780 --> 00:10:44,280
with methods like debug to our debug log

00:10:41,820 --> 00:10:47,040
or emergency to write an emergency log

00:10:44,280 --> 00:10:48,680
and you can then pass that around your

00:10:47,040 --> 00:10:57,150
system in whatever way you see fit so

00:10:48,680 --> 00:11:00,720
dependency injection or similar okay so

00:10:57,150 --> 00:11:03,390
that's the basics of creating a logger

00:11:00,720 --> 00:11:05,190
so that's useful but it's much more

00:11:03,390 --> 00:11:08,910
useful if we follow these next two

00:11:05,190 --> 00:11:11,040
approaches so the first one of those is

00:11:08,910 --> 00:11:13,290
tagging and it allows you to

00:11:11,040 --> 00:11:16,140
metadata to every single log line that

00:11:13,290 --> 00:11:20,460
goes through one of the loggers so

00:11:16,140 --> 00:11:23,820
metadata could be an IP address some geo

00:11:20,460 --> 00:11:25,710
data the date and time allows you to tag

00:11:23,820 --> 00:11:27,330
all of those with with that data which

00:11:25,710 --> 00:11:32,580
is useful later on further down the

00:11:27,330 --> 00:11:34,680
chain and then there's a formatter so

00:11:32,580 --> 00:11:36,720
formatter and so many different log

00:11:34,680 --> 00:11:40,040
formats it's like syslog former maybe

00:11:36,720 --> 00:11:42,240
you want to out log your logs as Jason

00:11:40,040 --> 00:11:44,610
so rather than trying to write that

00:11:42,240 --> 00:11:47,430
logic itself you just attach a formatter

00:11:44,610 --> 00:11:55,110
and to do it for you so let's take a

00:11:47,430 --> 00:11:57,840
look at these in detail so I'm creating

00:11:55,110 --> 00:12:00,930
a logger as in the previous example and

00:11:57,840 --> 00:12:03,630
a handler as in the previous example but

00:12:00,930 --> 00:12:05,640
this time I am setting a formatter on

00:12:03,630 --> 00:12:06,270
that handler which is a log stash

00:12:05,640 --> 00:12:09,300
formatter

00:12:06,270 --> 00:12:11,700
it's a pre-written piece of code which

00:12:09,300 --> 00:12:14,070
will format all of our logs in Jason and

00:12:11,700 --> 00:12:16,590
attach an important tag and to that

00:12:14,070 --> 00:12:19,170
Jason and so that when we tail it and

00:12:16,590 --> 00:12:20,910
send it to log stash it's in the right

00:12:19,170 --> 00:12:24,900
format reduces the work we have to do

00:12:20,910 --> 00:12:28,080
later on and then um push the handler

00:12:24,900 --> 00:12:33,150
onto the log object as in the previous

00:12:28,080 --> 00:12:35,870
example so this is an example of tagging

00:12:33,150 --> 00:12:39,780
so this is a pattern that I often follow

00:12:35,870 --> 00:12:42,600
when when logging so say you are

00:12:39,780 --> 00:12:44,340
generating 10 log lines in a request but

00:12:42,600 --> 00:12:46,470
you're actually accepting lots of

00:12:44,340 --> 00:12:48,690
requests at the same time so in your log

00:12:46,470 --> 00:12:51,690
file all these log entries get jumbled

00:12:48,690 --> 00:12:53,400
up and what's really useful is say we're

00:12:51,690 --> 00:12:55,770
doing some load testing and one or two

00:12:53,400 --> 00:12:57,630
requests errored you want to be able to

00:12:55,770 --> 00:13:02,250
identify all of the log lines from that

00:12:57,630 --> 00:13:04,380
particular request that errored so I've

00:13:02,250 --> 00:13:07,800
got varnish at the front of this setup

00:13:04,380 --> 00:13:09,420
varnish drops and ID in the HTTP headers

00:13:07,800 --> 00:13:11,880
before it sends it over to nginx or

00:13:09,420 --> 00:13:15,060
Apache and then here I'm grabbing that

00:13:11,880 --> 00:13:17,460
ID feeding it into a task a tag

00:13:15,060 --> 00:13:20,790
processor and pushing that processor

00:13:17,460 --> 00:13:23,850
onto the log object so now I've got a UD

00:13:20,790 --> 00:13:24,730
unique ID which links all of my logs to

00:13:23,850 --> 00:13:26,920
a particular request

00:13:24,730 --> 00:13:31,360
very useful I'm going to demonstrate

00:13:26,920 --> 00:13:34,360
that later on so log levels have been

00:13:31,360 --> 00:13:37,150
talking about them in general but they

00:13:34,360 --> 00:13:40,240
do have a specific specification so in

00:13:37,150 --> 00:13:43,680
RFC five four to four which is one of

00:13:40,240 --> 00:13:49,030
the more interesting rfcs it describes

00:13:43,680 --> 00:13:50,740
it describes a scheme for logging or lot

00:13:49,030 --> 00:13:52,930
of levels at the bottom of that scheme

00:13:50,740 --> 00:13:54,250
is debug and you're not normally going

00:13:52,930 --> 00:13:57,070
to want to see these in production but

00:13:54,250 --> 00:13:58,210
it's useful in development or it's

00:13:57,070 --> 00:14:00,430
useful if something's going wrong and

00:13:58,210 --> 00:14:05,590
you need some really granular data in

00:14:00,430 --> 00:14:07,360
the middle of that scale is error and so

00:14:05,590 --> 00:14:09,340
an error could still be kind of isolated

00:14:07,360 --> 00:14:11,980
to a specific request it might be an

00:14:09,340 --> 00:14:13,660
uncaught exception something like that

00:14:11,980 --> 00:14:15,310
or it could be symptomatic of a wider

00:14:13,660 --> 00:14:18,010
problem which is affecting more things

00:14:15,310 --> 00:14:20,230
but anything from error upwards like

00:14:18,010 --> 00:14:22,500
critical critical conditions alert

00:14:20,230 --> 00:14:25,210
action must be taken immediately an

00:14:22,500 --> 00:14:26,830
emergency system is unstable it's these

00:14:25,210 --> 00:14:29,260
kind of messages which should be waking

00:14:26,830 --> 00:14:31,300
up your ops team and sounding alarms and

00:14:29,260 --> 00:14:33,580
then on the side they're represented by

00:14:31,300 --> 00:14:36,850
codes which are normally in constants in

00:14:33,580 --> 00:14:39,610
the various log libraries but with it

00:14:36,850 --> 00:14:43,480
being a PHP talk it would be remiss of

00:14:39,610 --> 00:14:46,120
me not to mention PRS oh three the PHP

00:14:43,480 --> 00:14:48,340
logging interface standard so they

00:14:46,120 --> 00:14:51,790
define the same set of conditions

00:14:48,340 --> 00:14:54,340
however they use phrases instead of kind

00:14:51,790 --> 00:14:55,750
of codes and a monologue will support

00:14:54,340 --> 00:14:57,880
both of these things depending on what

00:14:55,750 --> 00:14:59,530
you want to do and I do think it's a bit

00:14:57,880 --> 00:15:02,320
a bit kind of like this we already

00:14:59,530 --> 00:15:06,910
specified some really useful codes why

00:15:02,320 --> 00:15:10,720
specify another set so a last kind of

00:15:06,910 --> 00:15:12,430
thing on logging in PHP now earlier on I

00:15:10,720 --> 00:15:15,850
mentioned it was really important to log

00:15:12,430 --> 00:15:18,700
business events so if you're using the

00:15:15,850 --> 00:15:20,470
pattern of an event dispatcher and a

00:15:18,700 --> 00:15:22,390
kind of event-driven system you're

00:15:20,470 --> 00:15:24,940
probably doing things like a

00:15:22,390 --> 00:15:27,070
pre-registration and a post registration

00:15:24,940 --> 00:15:30,160
hook and you can hook various things

00:15:27,070 --> 00:15:32,710
into that process and if you were to

00:15:30,160 --> 00:15:33,910
create a separate business logger using

00:15:32,710 --> 00:15:37,390
the same kind of style that we've just

00:15:33,910 --> 00:15:38,380
demonstrated you could hang a log file

00:15:37,390 --> 00:15:39,820
off of that so that

00:15:38,380 --> 00:15:42,040
every time we receive a registration

00:15:39,820 --> 00:15:44,020
event it logs it in that file that a

00:15:42,040 --> 00:15:45,580
customer's being registered later on we

00:15:44,020 --> 00:15:47,770
can push that into log stash and show

00:15:45,580 --> 00:15:49,450
some interesting information about real

00:15:47,770 --> 00:15:51,850
time business events in our system

00:15:49,450 --> 00:15:54,220
things like registrations per minute or

00:15:51,850 --> 00:15:57,730
amount of money spent in the last hour

00:15:54,220 --> 00:16:03,850
that kind of thing and then at the

00:15:57,730 --> 00:16:06,790
bottom just dispatch that event so that

00:16:03,850 --> 00:16:09,510
is logging in PHP or an opinionated view

00:16:06,790 --> 00:16:13,390
on logging in PHP how does that fit into

00:16:09,510 --> 00:16:14,800
the log stash or the ELQ stack so they

00:16:13,390 --> 00:16:18,850
say this is an infrastructure diagram

00:16:14,800 --> 00:16:21,340
with a web server at the bottom and the

00:16:18,850 --> 00:16:23,590
web server is running the log stash

00:16:21,340 --> 00:16:27,010
forwarding agent and that is a tool

00:16:23,590 --> 00:16:29,680
which will tail files on your system and

00:16:27,010 --> 00:16:32,200
send those files send those lines from

00:16:29,680 --> 00:16:35,130
those files to log stash which is the

00:16:32,200 --> 00:16:37,750
next thing in the chain log stash will

00:16:35,130 --> 00:16:40,920
do some kind of filtering process on

00:16:37,750 --> 00:16:43,600
them and then send them to elasticsearch

00:16:40,920 --> 00:16:45,310
and back at the kind of front end of the

00:16:43,600 --> 00:16:48,960
infrastructure you've got key abana

00:16:45,310 --> 00:16:52,150
which is a front-end to elasticsearch

00:16:48,960 --> 00:16:53,890
I've done a example code base which I'm

00:16:52,150 --> 00:16:55,690
going to show you in a minute and in the

00:16:53,890 --> 00:16:58,240
example code base log stash

00:16:55,690 --> 00:16:59,980
elasticsearch and cabana are all on one

00:16:58,240 --> 00:17:02,140
box so if you have a small

00:16:59,980 --> 00:17:04,560
infrastructure you can kind of pack all

00:17:02,140 --> 00:17:07,750
of this onto one machine cost-effective

00:17:04,560 --> 00:17:09,250
but if you've got a big infrastructure

00:17:07,750 --> 00:17:10,870
you're likely going to be generating a

00:17:09,250 --> 00:17:13,780
lot of logs so you might need something

00:17:10,870 --> 00:17:16,110
a little more complicated so there are a

00:17:13,780 --> 00:17:19,960
number of different shipping models in

00:17:16,110 --> 00:17:22,480
log stash but one of them is Redis so

00:17:19,960 --> 00:17:25,290
you could have n number of web servers

00:17:22,480 --> 00:17:27,790
or ship shipping logs to a Redis cluster

00:17:25,290 --> 00:17:30,970
log stash reads from that Redis cluster

00:17:27,790 --> 00:17:35,410
and writes to a cluster of elasticsearch

00:17:30,970 --> 00:17:40,210
instances which are read by multiple

00:17:35,410 --> 00:17:42,430
Cabana front ends so how you want to set

00:17:40,210 --> 00:17:44,140
up that is set up that infrastructure is

00:17:42,430 --> 00:17:45,520
up to you so if you're generating a lot

00:17:44,140 --> 00:17:48,700
of logs probably go for something like

00:17:45,520 --> 00:17:52,430
that if you're just kind of testing ELQ

00:17:48,700 --> 00:17:55,470
then maybe set it up on one instance

00:17:52,430 --> 00:17:57,690
so there's a demo coming up but let me

00:17:55,470 --> 00:17:59,490
just tell you about joined in here if

00:17:57,690 --> 00:18:02,010
you if you're feeling warm things about

00:17:59,490 --> 00:18:05,010
this presentation already why wait until

00:18:02,010 --> 00:18:09,960
the end to to post some feedback why not

00:18:05,010 --> 00:18:13,580
do it now so it looks nice demo um all

00:18:09,960 --> 00:18:17,370
of the code for this demo is on github

00:18:13,580 --> 00:18:18,690
application logging with log stash and

00:18:17,370 --> 00:18:23,340
I'm going to take a walk through that

00:18:18,690 --> 00:18:27,260
codebase now okay

00:18:23,340 --> 00:18:34,670
so let's increase the size a little bit

00:18:27,260 --> 00:18:42,150
and how's that for size a bit bigger or

00:18:34,670 --> 00:18:45,150
okay oh yeah sure so URL is a live

00:18:42,150 --> 00:18:47,280
software slash application - logging

00:18:45,150 --> 00:18:49,410
with logs - that's a good point so you

00:18:47,280 --> 00:18:55,170
can read along with the code now if you

00:18:49,410 --> 00:18:57,180
go to that website okay um

00:18:55,170 --> 00:19:01,770
so the first thing I want to say is that

00:18:57,180 --> 00:19:03,870
the code base is vagrant ice-t-- so

00:19:01,770 --> 00:19:06,060
there's a vagrant file which will do two

00:19:03,870 --> 00:19:08,160
VMs so if you want to try this out one

00:19:06,060 --> 00:19:11,160
of those VMs is the elk stack the other

00:19:08,160 --> 00:19:12,990
one has a test web application this is a

00:19:11,160 --> 00:19:15,030
particularly interesting file which is

00:19:12,990 --> 00:19:17,100
provision - log stash in the build

00:19:15,030 --> 00:19:19,770
directory and that's basically a shell

00:19:17,100 --> 00:19:22,710
script which will set up log stash from

00:19:19,770 --> 00:19:26,220
first principles so all the Java log

00:19:22,710 --> 00:19:28,320
stash elasticsearch it will set

00:19:26,220 --> 00:19:30,840
everything up including nginx which is

00:19:28,320 --> 00:19:32,490
used to proxy elastic search so if

00:19:30,840 --> 00:19:33,360
you're interested in how it's all how

00:19:32,490 --> 00:19:36,480
it's all set up

00:19:33,360 --> 00:19:37,920
then that's the file to go to and then

00:19:36,480 --> 00:19:40,410
also there's a provision web which

00:19:37,920 --> 00:19:42,150
provisions the web server the most

00:19:40,410 --> 00:19:44,220
interesting thing here is that it's also

00:19:42,150 --> 00:19:46,350
setting up the log stash forwarder

00:19:44,220 --> 00:19:51,210
so those are the two moving parts if you

00:19:46,350 --> 00:19:53,490
want to join in later on so in order to

00:19:51,210 --> 00:19:55,560
demonstrate a log a logging tool I need

00:19:53,490 --> 00:19:57,210
some actual logs so I'll take you

00:19:55,560 --> 00:19:59,910
through what this little tiny

00:19:57,210 --> 00:20:02,670
application does and the first thing it

00:19:59,910 --> 00:20:04,590
does is on the kind of slash root it

00:20:02,670 --> 00:20:05,070
returns a string saying hello I'm

00:20:04,590 --> 00:20:07,050
generate

00:20:05,070 --> 00:20:09,560
logs as we speak that's never going to

00:20:07,050 --> 00:20:12,510
fail that's gonna show us some good logs

00:20:09,560 --> 00:20:15,570
and then you'll see here if I just

00:20:12,510 --> 00:20:16,860
highlight this section this is the code

00:20:15,570 --> 00:20:18,810
that I showed you on the slides earlier

00:20:16,860 --> 00:20:21,390
on so this is bootstrapping an instance

00:20:18,810 --> 00:20:25,170
of monologue game you're working with

00:20:21,390 --> 00:20:25,800
the tag processor and also the log stash

00:20:25,170 --> 00:20:29,730
formatter

00:20:25,800 --> 00:20:32,070
and then after that there is this funky

00:20:29,730 --> 00:20:35,570
piece of code which defies all kind of

00:20:32,070 --> 00:20:39,930
testing it will randomly return either

00:20:35,570 --> 00:20:43,650
404 not found a 500 with loads of logs

00:20:39,930 --> 00:20:46,260
and a really latent response so this one

00:20:43,650 --> 00:20:48,780
is sleeping so there's three seconds

00:20:46,260 --> 00:20:52,740
worth of latency here a normal response

00:20:48,780 --> 00:20:53,910
and a bad gateway response so hopefully

00:20:52,740 --> 00:20:57,900
it provides us with a little bit of

00:20:53,910 --> 00:21:00,210
variety to our logs and then just while

00:20:57,900 --> 00:21:02,760
I'm here if you want to play it play

00:21:00,210 --> 00:21:04,340
with this after there is an atty Ted

00:21:02,760 --> 00:21:07,500
example of the fingers-crossed

00:21:04,340 --> 00:21:09,980
demo showing you how to set that up and

00:21:07,500 --> 00:21:13,800
then underneath there is an annotated

00:21:09,980 --> 00:21:15,660
example of logging with business events

00:21:13,800 --> 00:21:23,610
but they're not too important for this

00:21:15,660 --> 00:21:26,550
demo so without further ado okay so as

00:21:23,610 --> 00:21:28,650
you know so this is when you open Cubana

00:21:26,550 --> 00:21:30,870
this is what you get a screen with some

00:21:28,650 --> 00:21:32,550
text on and the first thing you want to

00:21:30,870 --> 00:21:34,890
do is go to the log stash dashboard

00:21:32,550 --> 00:21:38,220
which is a template and log stash

00:21:34,890 --> 00:21:43,130
implementation and then what you get on

00:21:38,220 --> 00:21:47,580
here is a histogram full of log entries

00:21:43,130 --> 00:21:50,270
over time and then you get a list of all

00:21:47,580 --> 00:21:52,380
the log entries that have been captured

00:21:50,270 --> 00:21:57,030
so let's have a look at how this might

00:21:52,380 --> 00:21:59,400
and kind of affect the front-end so here

00:21:57,030 --> 00:22:01,920
is the front-end of the website so it

00:21:59,400 --> 00:22:03,690
says hello I'm gathering logs so if I

00:22:01,920 --> 00:22:05,880
just request this quite a lot what I'm

00:22:03,690 --> 00:22:10,430
hoping to see when I go back to logs -

00:22:05,880 --> 00:22:10,430
as some nginx access logs appear there

00:22:12,500 --> 00:22:18,960
okay the last five minutes so you can

00:22:15,210 --> 00:22:20,309
see I was that that last view was on

00:22:18,960 --> 00:22:21,990
six-hour view so you could see I was

00:22:20,309 --> 00:22:23,429
playing with it earlier on but right

00:22:21,990 --> 00:22:26,210
over in the right-hand corner we've now

00:22:23,429 --> 00:22:30,120
got some new events in the 5-minute view

00:22:26,210 --> 00:22:32,610
and if I kind of drag and drop drop if I

00:22:30,120 --> 00:22:35,610
drag to the right and highlight that and

00:22:32,610 --> 00:22:38,309
it will reset the time to be something a

00:22:35,610 --> 00:22:41,039
bit more granular so let's zoom in a

00:22:38,309 --> 00:22:42,240
little bit and see what what the logs

00:22:41,039 --> 00:22:43,460
actually look like when they've reached

00:22:42,240 --> 00:22:46,500
logstash

00:22:43,460 --> 00:22:48,600
so this is a list of all those home

00:22:46,500 --> 00:22:50,669
pages that I just viewed you can click

00:22:48,600 --> 00:22:54,809
on an item here and I'll just increase

00:22:50,669 --> 00:22:57,120
the size there again I'm and when you

00:22:54,809 --> 00:23:00,149
actually expanded a log item you can see

00:22:57,120 --> 00:23:04,919
things like agent or number of bytes in

00:23:00,149 --> 00:23:07,500
the request the client IP the file host

00:23:04,919 --> 00:23:10,110
basically everything that is in the

00:23:07,500 --> 00:23:11,159
original message here but it's been

00:23:10,110 --> 00:23:15,690
tokenized

00:23:11,159 --> 00:23:19,169
and it's been given meaning by logs - so

00:23:15,690 --> 00:23:22,140
that is useful as a way to quickly

00:23:19,169 --> 00:23:23,730
interrogate logs but one of the other

00:23:22,140 --> 00:23:25,770
goals that I saw at the start of this

00:23:23,730 --> 00:23:28,289
would be able to isolate faults as they

00:23:25,770 --> 00:23:30,390
happened so for that we need a really

00:23:28,289 --> 00:23:33,480
ropey endpoint which is going to give us

00:23:30,390 --> 00:23:36,870
some errors which is this flat the

00:23:33,480 --> 00:23:38,970
endpoint and so I can kind of flick

00:23:36,870 --> 00:23:42,210
through this it's going to give me a

00:23:38,970 --> 00:23:44,850
slow response now not found something

00:23:42,210 --> 00:23:50,850
terrible has happened so let's get the

00:23:44,850 --> 00:23:54,870
console open like so so the console

00:23:50,850 --> 00:23:56,370
shows us here which you can't really see

00:23:54,870 --> 00:23:59,669
there and it doesn't magnify with the

00:23:56,370 --> 00:24:02,039
rest of the screen but it shows us the

00:23:59,669 --> 00:24:04,740
error code here and if we click in it we

00:24:02,039 --> 00:24:07,830
can also see the varnish header because

00:24:04,740 --> 00:24:10,200
varnish returns that unique ID to the

00:24:07,830 --> 00:24:12,330
client now armed with that kind of

00:24:10,200 --> 00:24:14,730
unique ID we can then go back to logs -

00:24:12,330 --> 00:24:16,950
and pull back all of the events that

00:24:14,730 --> 00:24:19,350
happened in the system the correspond to

00:24:16,950 --> 00:24:21,210
that ID so it's getting more interesting

00:24:19,350 --> 00:24:23,760
one to do that with something terrible

00:24:21,210 --> 00:24:26,870
has happened generates lots of logs so

00:24:23,760 --> 00:24:31,370
let's click through and then I'll copy

00:24:26,870 --> 00:24:37,520
copy this ID over here

00:24:31,370 --> 00:24:45,690
you know if I do a search for request ID

00:24:37,520 --> 00:24:47,810
equals a reset the time okay so you can

00:24:45,690 --> 00:24:50,970
see now that I've done a search for the

00:24:47,810 --> 00:24:53,460
request ID in the top corner and that

00:24:50,970 --> 00:24:56,790
corresponds to exactly one event here

00:24:53,460 --> 00:25:03,150
which is a number of events because I've

00:24:56,790 --> 00:25:05,940
been clicking through and that request

00:25:03,150 --> 00:25:10,020
ID is present here

00:25:05,940 --> 00:25:13,560
so it's come through in the it's come

00:25:10,020 --> 00:25:16,440
through as a token here in the log

00:25:13,560 --> 00:25:19,920
response so if I want to then do a

00:25:16,440 --> 00:25:22,590
search for everything that contains a

00:25:19,920 --> 00:25:25,200
reference to that number then I can see

00:25:22,590 --> 00:25:28,890
now that I've got one two three four

00:25:25,200 --> 00:25:30,720
five different logs and so just to give

00:25:28,890 --> 00:25:32,370
you a sample of one of those logs this

00:25:30,720 --> 00:25:34,830
one is from the actual application log

00:25:32,370 --> 00:25:38,490
and it says it's caused an error oh god

00:25:34,830 --> 00:25:40,890
no in your application now I'll probably

00:25:38,490 --> 00:25:46,290
mean something like the database is

00:25:40,890 --> 00:25:48,180
timed out and so on and so that is an

00:25:46,290 --> 00:25:49,740
easy way to isolate a particular error

00:25:48,180 --> 00:25:53,160
providing you can get hold of the

00:25:49,740 --> 00:25:54,990
varnish ID header but it would be good

00:25:53,160 --> 00:25:58,440
if we could surface kind of more general

00:25:54,990 --> 00:26:00,450
errors so let's do a response which is

00:25:58,440 --> 00:26:02,610
an alias to the response code

00:26:00,450 --> 00:26:06,660
let's do response greater than or equal

00:26:02,610 --> 00:26:10,260
to 500 and then let's set the time to be

00:26:06,660 --> 00:26:12,030
something like at the last kind of 12

00:26:10,260 --> 00:26:13,770
hours so I was playing with this this

00:26:12,030 --> 00:26:16,260
morning and generated some error

00:26:13,770 --> 00:26:17,550
responses so if you were from an

00:26:16,260 --> 00:26:19,110
infrastructure perspective if you were

00:26:17,550 --> 00:26:21,390
kind of surveying what's happened in the

00:26:19,110 --> 00:26:24,480
last 6 hours you could look for all the

00:26:21,390 --> 00:26:28,980
500 responses and they're all listed

00:26:24,480 --> 00:26:31,080
here you could click in and then you

00:26:28,980 --> 00:26:33,630
could go and find that request ID field

00:26:31,080 --> 00:26:35,220
for one of the 500 responses pull back

00:26:33,630 --> 00:26:36,810
all the logs that relate to it and

00:26:35,220 --> 00:26:38,820
really start doing some in-depth

00:26:36,810 --> 00:26:42,540
analysis as to what what's going on in

00:26:38,820 --> 00:26:44,330
your system and another one worth

00:26:42,540 --> 00:26:52,910
showing is

00:26:44,330 --> 00:26:57,710
response response time greater than one

00:26:52,910 --> 00:26:59,330
yeah so in the demo code I show this in

00:26:57,710 --> 00:27:01,130
the github repository I've added some

00:26:59,330 --> 00:27:03,110
custom headers one of those is upstream

00:27:01,130 --> 00:27:07,880
response time so that tells you how long

00:27:03,110 --> 00:27:10,070
PHP is spending with your code so this

00:27:07,880 --> 00:27:12,380
this graph shows anything greater than

00:27:10,070 --> 00:27:19,730
one second worth of time is spent being

00:27:12,380 --> 00:27:23,030
spent there okay so that's all very well

00:27:19,730 --> 00:27:25,760
for solving problems and kind of

00:27:23,030 --> 00:27:27,260
debugging but one of cabañas real

00:27:25,760 --> 00:27:29,840
strengths is that it lets you build

00:27:27,260 --> 00:27:31,929
dashboards with historical data that you

00:27:29,840 --> 00:27:35,570
can of harvest from your log files and

00:27:31,929 --> 00:27:38,090
so let's give that a go so I'm going to

00:27:35,570 --> 00:27:42,740
hide the other two the graph and the

00:27:38,090 --> 00:27:46,100
events now all the all the kind of

00:27:42,740 --> 00:27:48,230
graphs and panels are powered by these

00:27:46,100 --> 00:27:50,299
queries at the top so we have to think

00:27:48,230 --> 00:27:51,679
of some queries and that will give us

00:27:50,299 --> 00:27:56,330
some meaningful data which we can then

00:27:51,679 --> 00:27:58,039
graph before I do that I'll just add

00:27:56,330 --> 00:28:01,940
some additional rows here so that we can

00:27:58,039 --> 00:28:05,179
add some more graphs so configure the

00:28:01,940 --> 00:28:08,090
dashboard and then let's create some new

00:28:05,179 --> 00:28:09,770
rows and so you can create as many of

00:28:08,090 --> 00:28:14,900
these as you want you can create them

00:28:09,770 --> 00:28:18,440
big small and grow that and I will just

00:28:14,900 --> 00:28:22,070
move the existing rows down and so there

00:28:18,440 --> 00:28:24,770
are new rows are on top if I save that

00:28:22,070 --> 00:28:26,450
now we've got some blank space there

00:28:24,770 --> 00:28:29,210
that we can hopefully fill with some

00:28:26,450 --> 00:28:30,980
interesting data so the first thing I'm

00:28:29,210 --> 00:28:33,049
going to do is specify a type of logs

00:28:30,980 --> 00:28:38,200
that I'm interested in so I'm going to

00:28:33,049 --> 00:28:38,200
do nginx access if I spell that right

00:28:45,240 --> 00:28:50,520
okay yes did spell that right so that's

00:28:47,920 --> 00:28:53,440
going to show us all of the access logs

00:28:50,520 --> 00:28:55,960
now let's also look for all of the

00:28:53,440 --> 00:28:58,690
access logs but only the lines that

00:28:55,960 --> 00:29:03,820
errored so if I do type engine X is

00:28:58,690 --> 00:29:06,250
access and the response is greater than

00:29:03,820 --> 00:29:09,220
or equal to 500 so that's the error

00:29:06,250 --> 00:29:11,140
series of status codes and then

00:29:09,220 --> 00:29:12,610
underneath in this graph which I like to

00:29:11,140 --> 00:29:13,810
keep open just to make sure that there

00:29:12,610 --> 00:29:17,050
are actually events which match my

00:29:13,810 --> 00:29:19,110
queries you can see success and then

00:29:17,050 --> 00:29:26,410
also on top of that stacked up our

00:29:19,110 --> 00:29:28,600
errors so let's add a pie chart so you

00:29:26,410 --> 00:29:31,390
can once you've got rows you then add

00:29:28,600 --> 00:29:35,740
columns and columns stack up left to

00:29:31,390 --> 00:29:37,900
right so this this panel type is called

00:29:35,740 --> 00:29:40,750
terms and terms is actually an elastic

00:29:37,900 --> 00:29:42,760
search and kind of name for a process

00:29:40,750 --> 00:29:46,090
for anything that you can book it so if

00:29:42,760 --> 00:29:48,220
you imagine HTTP status codes they're

00:29:46,090 --> 00:29:50,200
quite small in number and unique so you

00:29:48,220 --> 00:29:53,440
could have buckets of them and then put

00:29:50,200 --> 00:29:55,890
that into a pie chart equally endpoints

00:29:53,440 --> 00:29:59,200
on your infrastructure so like slash

00:29:55,890 --> 00:30:00,880
slash flappy slash register they're also

00:29:59,200 --> 00:30:04,810
quite small in number so you could make

00:30:00,880 --> 00:30:07,470
pie charts built on those so let's do

00:30:04,810 --> 00:30:12,070
the first one as an error distribution

00:30:07,470 --> 00:30:13,780
so I'm going to show all of the show pie

00:30:12,070 --> 00:30:23,110
chart of where all of the errors are

00:30:13,780 --> 00:30:24,700
occurring in the system ok the field

00:30:23,110 --> 00:30:27,490
that I'm going to do the terms on is

00:30:24,700 --> 00:30:29,590
request which maps to the string at the

00:30:27,490 --> 00:30:31,210
end of the URL and I'm going to select a

00:30:29,590 --> 00:30:36,520
specific query to do and I'm going to

00:30:31,210 --> 00:30:38,920
pick the error responses one thing that

00:30:36,520 --> 00:30:43,210
I forgot to do is to make sure that it

00:30:38,920 --> 00:30:45,280
was a pie chart ok ok so this is

00:30:43,210 --> 00:30:47,950
interesting I mean we've got a really

00:30:45,280 --> 00:30:50,410
trivial example here but all of the

00:30:47,950 --> 00:30:52,150
errors are from the slash floppy

00:30:50,410 --> 00:30:54,100
endpoint which is to be expected because

00:30:52,150 --> 00:30:55,390
that's how I programmed it imagine this

00:30:54,100 --> 00:30:57,190
was your live system

00:30:55,390 --> 00:30:59,830
and you've got one of this pie chart for

00:30:57,190 --> 00:31:01,330
all of your kind of API endpoints you'll

00:30:59,830 --> 00:31:04,929
be able to see proportionately which one

00:31:01,330 --> 00:31:09,610
is giving you the most trouble it's

00:31:04,929 --> 00:31:12,610
going to add another one now and we'll

00:31:09,610 --> 00:31:15,490
do HTTP status code distribution so what

00:31:12,610 --> 00:31:18,610
portion of your requests are 200 versus

00:31:15,490 --> 00:31:26,200
500 504 so we're going to use a terms

00:31:18,610 --> 00:31:28,210
pie chart again okay and this time I'm

00:31:26,200 --> 00:31:31,770
going to use the response field which is

00:31:28,210 --> 00:31:34,410
where we store the HTTP response codes

00:31:31,770 --> 00:31:37,240
I'm going to use a pie chart again and

00:31:34,410 --> 00:31:38,980
this time when I'm picking a query and

00:31:37,240 --> 00:31:43,330
that this data represents I'm going to

00:31:38,980 --> 00:31:45,640
pick all of the access logs available so

00:31:43,330 --> 00:31:47,740
if I select that now I've got this

00:31:45,640 --> 00:31:49,919
colorful graph which shows me there most

00:31:47,740 --> 00:31:52,059
of my requests at 2:01

00:31:49,919 --> 00:31:54,010
some are to hundreds and then there's a

00:31:52,059 --> 00:31:55,570
small kind of smattering of errors there

00:31:54,010 --> 00:31:57,460
now in a minute I'm going to show you

00:31:55,570 --> 00:31:58,660
how to refine that to just one endpoint

00:31:57,460 --> 00:32:01,059
and that's when it gets really

00:31:58,660 --> 00:32:02,890
interesting because for for just one

00:32:01,059 --> 00:32:05,710
endpoint it's great to see what kind of

00:32:02,890 --> 00:32:10,030
behavior is exhibiting on a HTTP level

00:32:05,710 --> 00:32:14,290
but before I do that let's add another

00:32:10,030 --> 00:32:17,110
panel so cabaña can also show time

00:32:14,290 --> 00:32:21,130
series data so if I choose a histogram

00:32:17,110 --> 00:32:27,730
let's look at our accounts over time so

00:32:21,130 --> 00:32:30,220
I'll just call this one errors and I'll

00:32:27,730 --> 00:32:31,840
choose to make it lines and I'll reduce

00:32:30,220 --> 00:32:34,600
the line width just to make it a bit

00:32:31,840 --> 00:32:37,000
more pretty so I'm going to select a

00:32:34,600 --> 00:32:38,530
query tip that this graph will represent

00:32:37,000 --> 00:32:44,380
and it's going to represent the error

00:32:38,530 --> 00:32:47,350
rate okay so that's I've set the time

00:32:44,380 --> 00:32:48,730
filter to 12 hours so you'll see all of

00:32:47,350 --> 00:32:51,520
the data that I graphed when I was

00:32:48,730 --> 00:32:53,710
playing with this earlier on but just to

00:32:51,520 --> 00:32:56,440
make this look a bit nicer let's kind of

00:32:53,710 --> 00:33:01,570
make that span the whole call and the

00:32:56,440 --> 00:33:05,260
whole chart so now you've got errors

00:33:01,570 --> 00:33:07,929
over time and now I'm going to add

00:33:05,260 --> 00:33:09,270
another one which will represent latency

00:33:07,929 --> 00:33:11,540
because we've got the response too

00:33:09,270 --> 00:33:18,450
so we could graph that over time as well

00:33:11,540 --> 00:33:21,540
so I choose a histogram show latency and

00:33:18,450 --> 00:33:26,250
choose it to chu'lak choose lines again

00:33:21,540 --> 00:33:31,350
line width set it to something nice and

00:33:26,250 --> 00:33:33,570
then you choose that okay now the

00:33:31,350 --> 00:33:35,970
important change between this this graph

00:33:33,570 --> 00:33:37,200
and the last one is that the chart value

00:33:35,970 --> 00:33:39,210
I'm going to pick this time is going to

00:33:37,200 --> 00:33:43,020
be mean instead of count so I'm no

00:33:39,210 --> 00:33:44,760
longer counting the instances of

00:33:43,020 --> 00:33:47,550
something occurring I'm plotting the

00:33:44,760 --> 00:33:55,220
average of those values so for that I

00:33:47,550 --> 00:34:01,620
have to specify a response time like so

00:33:55,220 --> 00:34:04,740
for all requests and I'll make it pretty

00:34:01,620 --> 00:34:07,050
again by expanding it so that it fills

00:34:04,740 --> 00:34:08,130
up the whole screen so now if you look

00:34:07,050 --> 00:34:11,550
what we've got we've got some error

00:34:08,130 --> 00:34:16,740
distribution HTTP status codes errors

00:34:11,550 --> 00:34:19,620
and latency so that's all interesting

00:34:16,740 --> 00:34:21,210
information but let's make it more

00:34:19,620 --> 00:34:23,250
interesting I'm going to fire up Jaime

00:34:21,210 --> 00:34:25,710
to now and this is a script which

00:34:23,250 --> 00:34:29,010
basically makes loads and loads of calls

00:34:25,710 --> 00:34:31,110
to that endpoint so I've been looking in

00:34:29,010 --> 00:34:33,120
the past at the moment and kind of

00:34:31,110 --> 00:34:36,090
twelve hours ago but now if I set this

00:34:33,120 --> 00:34:39,120
graph to be the last five minutes and I

00:34:36,090 --> 00:34:41,550
set it to auto refresh so jmeter is

00:34:39,120 --> 00:34:43,950
hopefully generating a lot of traffic

00:34:41,550 --> 00:34:46,850
which should appear on these graphs

00:34:43,950 --> 00:34:46,850
surely

00:34:50,770 --> 00:35:02,320
hmm there you go oh thanks jmeter okay

00:34:58,630 --> 00:35:03,640
so this I've only just started Jamie -

00:35:02,320 --> 00:35:04,540
but if we left this running for five

00:35:03,640 --> 00:35:06,220
minutes

00:35:04,540 --> 00:35:07,870
that line would kind of go all the way

00:35:06,220 --> 00:35:09,630
across the page and if you left this

00:35:07,870 --> 00:35:12,010
running in your office you'd then have

00:35:09,630 --> 00:35:14,380
real error rates coming from your

00:35:12,010 --> 00:35:16,860
production server real latency rates and

00:35:14,380 --> 00:35:19,180
you can just leave it there refreshing

00:35:16,860 --> 00:35:21,010
and a technique that we've used in the

00:35:19,180 --> 00:35:22,510
past is to set up a number of these

00:35:21,010 --> 00:35:25,270
dashboards which represent different

00:35:22,510 --> 00:35:28,930
areas of the system and then Chrome has

00:35:25,270 --> 00:35:31,150
got a rotator tab rotator tool so you

00:35:28,930 --> 00:35:37,120
can just set Chrome to rotate through

00:35:31,150 --> 00:35:40,570
each one of these graphs so handy couple

00:35:37,120 --> 00:35:43,300
more things to show you so HTTP status

00:35:40,570 --> 00:35:44,770
codes isn't very useful at the moment

00:35:43,300 --> 00:35:46,600
because it's showing us the makeup of

00:35:44,770 --> 00:35:48,910
all the codes over the whole

00:35:46,600 --> 00:35:51,640
infrastructure if we create a new query

00:35:48,910 --> 00:35:56,230
and this time we'll go for type of

00:35:51,640 --> 00:36:03,120
Records nginx access logs and we'll do

00:35:56,230 --> 00:36:06,010
request is for floppy floppy endpoint

00:36:03,120 --> 00:36:08,650
okay so we've now got a query which is

00:36:06,010 --> 00:36:09,910
in blue here which represents just the

00:36:08,650 --> 00:36:14,080
request which went to a particular

00:36:09,910 --> 00:36:18,430
endpoint and now if I configure that

00:36:14,080 --> 00:36:22,210
chart and tell it to use the new query

00:36:18,430 --> 00:36:23,890
which is this fluffy one I now get a

00:36:22,210 --> 00:36:25,900
much more interesting and relevant graph

00:36:23,890 --> 00:36:28,510
so here we've got a graph which is

00:36:25,900 --> 00:36:30,310
showing the distribution of status codes

00:36:28,510 --> 00:36:32,200
for a particular endpoint in the last

00:36:30,310 --> 00:36:34,810
five minutes so you can see there's a

00:36:32,200 --> 00:36:36,580
nice healthy chunk of to hundreds but

00:36:34,810 --> 00:36:38,380
there's also a bit of a fat slice of

00:36:36,580 --> 00:36:40,000
five hundreds as well so that would be

00:36:38,380 --> 00:36:44,920
ringing alarm bells for the development

00:36:40,000 --> 00:36:46,900
team you can also change the colors here

00:36:44,920 --> 00:36:49,560
so if I this errors one for example

00:36:46,900 --> 00:36:51,580
could be red and if I close that

00:36:49,560 --> 00:36:55,210
aesthetically now that looks a lot more

00:36:51,580 --> 00:36:57,910
error like and then you can pin these

00:36:55,210 --> 00:37:00,580
queries as well so pinning them makes

00:36:57,910 --> 00:37:02,530
them smaller and and then you can also

00:37:00,580 --> 00:37:05,460
hide the query bar as well so it makes

00:37:02,530 --> 00:37:08,290
it really dashboard like

00:37:05,460 --> 00:37:11,580
and there's one last kind of graph that

00:37:08,290 --> 00:37:11,580
I will show you which is quite useful

00:37:13,090 --> 00:37:20,040
it's called sparklines apparently this

00:37:15,280 --> 00:37:24,310
one is this one is experimental

00:37:20,040 --> 00:37:27,430
so sparklines show you a graph with no

00:37:24,310 --> 00:37:31,150
axes and it's updated over time so it

00:37:27,430 --> 00:37:39,850
shows you I'm the shape of data and so

00:37:31,150 --> 00:37:44,470
let's select this floppy my doing let's

00:37:39,850 --> 00:37:47,170
do it all for the moment okay so this

00:37:44,470 --> 00:37:49,780
Sparx thing and so what I haven't done

00:37:47,170 --> 00:37:51,910
there is select an appropriate query but

00:37:49,780 --> 00:37:53,620
it it will work on any query that we've

00:37:51,910 --> 00:37:55,930
done so far so this one's particularly

00:37:53,620 --> 00:38:00,130
useful for response time data so you can

00:37:55,930 --> 00:38:02,470
have all of your API endpoints in the

00:38:00,130 --> 00:38:05,320
query box up there I'm showing response

00:38:02,470 --> 00:38:08,110
time and graph it in a little box like

00:38:05,320 --> 00:38:11,590
that and you can then kind of label them

00:38:08,110 --> 00:38:14,590
as well so if I and if I pick something

00:38:11,590 --> 00:38:16,210
like all here press close and it that

00:38:14,590 --> 00:38:21,160
makes its way onto the diagram as well

00:38:16,210 --> 00:38:29,730
it's quite useful so that is the live

00:38:21,160 --> 00:38:34,290
demo pretty please nothing exploded okay

00:38:29,730 --> 00:38:39,070
live demo over let's get back to logging

00:38:34,290 --> 00:38:41,920
these were just in case it broke we

00:38:39,070 --> 00:38:45,310
don't have to use those today so what I

00:38:41,920 --> 00:38:48,100
showed you there was and the output of

00:38:45,310 --> 00:38:50,380
that process basically reading from

00:38:48,100 --> 00:38:52,450
elasticsearch log stash comes into its

00:38:50,380 --> 00:38:54,520
own as a layer of getting events into

00:38:52,450 --> 00:38:58,300
elasticsearch so how does it achieve

00:38:54,520 --> 00:39:01,960
that so I've spoken a little bit about

00:38:58,300 --> 00:39:04,210
the log stash forwarder this is what it

00:39:01,960 --> 00:39:05,920
looks like to configure until recently

00:39:04,210 --> 00:39:09,000
you could get install it now you have to

00:39:05,920 --> 00:39:11,920
compile it from source don't know why

00:39:09,000 --> 00:39:15,610
you just specify a server end point for

00:39:11,920 --> 00:39:17,829
your a lot log slash instance a time out

00:39:15,610 --> 00:39:21,160
you have to send all of the

00:39:17,829 --> 00:39:24,029
log entries over SSL and then you've got

00:39:21,160 --> 00:39:27,190
an array of files or groups of files

00:39:24,029 --> 00:39:30,279
that the forwarder will then tail and

00:39:27,190 --> 00:39:32,499
send back to logs - the important thing

00:39:30,279 --> 00:39:35,559
in this diagram or in this config file

00:39:32,499 --> 00:39:37,809
is this field section here and I'm

00:39:35,559 --> 00:39:39,640
adding a type field and the type field

00:39:37,809 --> 00:39:42,009
is really important in logs - because it

00:39:39,640 --> 00:39:46,569
lets us differentiate between different

00:39:42,009 --> 00:39:48,930
types of logs and that enginex - access

00:39:46,569 --> 00:39:51,400
is what I was just using to kind of

00:39:48,930 --> 00:39:53,859
narrow down all the data to just the

00:39:51,400 --> 00:39:59,380
nginx access logs so remember to put

00:39:53,859 --> 00:40:01,779
that in there now logstash process

00:39:59,380 --> 00:40:03,249
processing itself this is on the log

00:40:01,779 --> 00:40:07,499
stash machine rather than on the web

00:40:03,249 --> 00:40:11,650
machine has got a input a filter and

00:40:07,499 --> 00:40:13,269
input filter and out section the input

00:40:11,650 --> 00:40:15,039
section is really simple it's just

00:40:13,269 --> 00:40:18,430
opening up a port for us to throw all

00:40:15,039 --> 00:40:20,769
that data towards together with some SSL

00:40:18,430 --> 00:40:22,930
config but at this point you could also

00:40:20,769 --> 00:40:25,479
specify something like Redis and it will

00:40:22,930 --> 00:40:30,400
use Redis lists and Redis cues to kind

00:40:25,479 --> 00:40:33,309
of ingest from a Redis cluster and then

00:40:30,400 --> 00:40:34,900
you have a filter section this is

00:40:33,309 --> 00:40:38,019
probably the most important section of

00:40:34,900 --> 00:40:40,539
the log stash config because it allows

00:40:38,019 --> 00:40:43,449
you to say that if the type field is

00:40:40,539 --> 00:40:46,479
equal to a certain type then do this

00:40:43,449 --> 00:40:49,049
list of things and in this case this

00:40:46,479 --> 00:40:50,920
list of things contains a grok filter

00:40:49,049 --> 00:40:52,779
and I'm going to talk about this in

00:40:50,920 --> 00:40:54,880
detail in a minute but for the moment

00:40:52,779 --> 00:40:57,160
think of it as a way of passing or

00:40:54,880 --> 00:40:58,839
everything into tokens and putting those

00:40:57,160 --> 00:41:02,319
tokens into a document to send to

00:40:58,839 --> 00:41:03,670
elasticsearch so that's really important

00:41:02,319 --> 00:41:06,969
you'll find yourself doing this all the

00:41:03,670 --> 00:41:09,309
time and then this date action here is

00:41:06,969 --> 00:41:11,589
also very important because by default

00:41:09,309 --> 00:41:14,109
log stash will take information in and

00:41:11,589 --> 00:41:16,719
it will timestamp it with the data that

00:41:14,109 --> 00:41:17,739
at the day time it was ingested which is

00:41:16,719 --> 00:41:21,219
no good for us because if we're

00:41:17,739 --> 00:41:23,079
ingesting access logs we want to know

00:41:21,219 --> 00:41:25,299
the time the server requests and

00:41:23,079 --> 00:41:29,349
accepted the request not the time that

00:41:25,299 --> 00:41:31,359
log stash ingested it it's even more

00:41:29,349 --> 00:41:32,120
important if you're going to backfill

00:41:31,359 --> 00:41:35,300
all of your

00:41:32,120 --> 00:41:38,000
or log data from your existing nginx

00:41:35,300 --> 00:41:39,530
instances into this because otherwise it

00:41:38,000 --> 00:41:43,700
will look like all that traffic happened

00:41:39,530 --> 00:41:46,130
like today and then there's an output

00:41:43,700 --> 00:41:47,300
section so the output section in this

00:41:46,130 --> 00:41:49,790
case is really simple it's just

00:41:47,300 --> 00:41:53,060
elasticsearch logstash takes care of

00:41:49,790 --> 00:41:56,930
creating indexes qivana takes care of

00:41:53,060 --> 00:41:58,280
reading those indexes kind of following

00:41:56,930 --> 00:41:59,990
a naming convention that they both

00:41:58,280 --> 00:42:02,360
agreed on so that's the only line for

00:41:59,990 --> 00:42:05,660
output equally here though you could do

00:42:02,360 --> 00:42:09,830
a if type equals and then your define

00:42:05,660 --> 00:42:16,010
type then send send the log entry to

00:42:09,830 --> 00:42:17,890
stats D or send it to graphite so now

00:42:16,010 --> 00:42:20,960
I'm going to talk about grokking

00:42:17,890 --> 00:42:25,730
grokking is probably the most important

00:42:20,960 --> 00:42:27,530
concept for log stash you have a grok

00:42:25,730 --> 00:42:29,960
filter at the top and then it matches

00:42:27,530 --> 00:42:31,430
the contents of a field that the log

00:42:29,960 --> 00:42:34,670
stash forwarder will always send

00:42:31,430 --> 00:42:37,130
something with a message and then in

00:42:34,670 --> 00:42:40,700
that section you've got a token and the

00:42:37,130 --> 00:42:42,980
token there is actually a combination of

00:42:40,700 --> 00:42:44,870
other tokens grouped together for your

00:42:42,980 --> 00:42:46,610
convenience but each one of those

00:42:44,870 --> 00:42:49,880
smaller tokens maps to a regular

00:42:46,610 --> 00:42:53,030
expression the idea being that you can

00:42:49,880 --> 00:42:57,530
put together common tokens to pass a

00:42:53,030 --> 00:42:59,570
regular log files unholy if you have

00:42:57,530 --> 00:43:01,580
like mysql or if you have rabbit

00:42:59,570 --> 00:43:03,860
somebody will have already invented the

00:43:01,580 --> 00:43:06,670
set of tokens you need to pass those

00:43:03,860 --> 00:43:11,180
logs but here is an example underneath

00:43:06,670 --> 00:43:12,770
of an irregular log format and the set

00:43:11,180 --> 00:43:15,200
of tokens that you would need to put in

00:43:12,770 --> 00:43:17,030
those inverted commas to pass that so no

00:43:15,200 --> 00:43:21,230
regular expressions actually we're just

00:43:17,030 --> 00:43:25,340
looking for an IP address a word a UI

00:43:21,230 --> 00:43:27,710
per am number and a number and here

00:43:25,340 --> 00:43:30,140
client method requests by its duration

00:43:27,710 --> 00:43:35,630
those are labels to what that data will

00:43:30,140 --> 00:43:38,270
end up as in elasticsearch so earlier on

00:43:35,630 --> 00:43:40,400
I was saying I added a number of fields

00:43:38,270 --> 00:43:42,410
to the standard nginx

00:43:40,400 --> 00:43:45,080
log format which uses the combined

00:43:42,410 --> 00:43:45,500
Apache log format all I had to do to

00:43:45,080 --> 00:43:48,980
make that

00:43:45,500 --> 00:43:51,440
work was to quote this token and then

00:43:48,980 --> 00:43:53,480
two of these tokens because it's just

00:43:51,440 --> 00:43:55,730
the normal format plus two additional

00:43:53,480 --> 00:43:57,920
numbers which I asked nginx to drop in

00:43:55,730 --> 00:44:00,680
the log file so you can see how you can

00:43:57,920 --> 00:44:04,340
quickly build up a really great kind of

00:44:00,680 --> 00:44:07,100
passing passing config file without much

00:44:04,340 --> 00:44:09,050
effort that said you can't see it

00:44:07,100 --> 00:44:10,370
because it's falling off the screen but

00:44:09,050 --> 00:44:13,220
it is really difficult to get your head

00:44:10,370 --> 00:44:16,220
round to start with so this croc debug

00:44:13,220 --> 00:44:18,860
Heroku opcom lets you put some log data

00:44:16,220 --> 00:44:20,750
in a field at the top and then tokens

00:44:18,860 --> 00:44:22,850
underneath and it will show you what the

00:44:20,750 --> 00:44:25,760
result would be if it was passed by croc

00:44:22,850 --> 00:44:27,500
so that's useful if you if you're new

00:44:25,760 --> 00:44:30,410
and then underneath that there's a link

00:44:27,500 --> 00:44:33,890
to a definition file for all of those

00:44:30,410 --> 00:44:35,240
grok patterns which is useful if you

00:44:33,890 --> 00:44:41,090
want to know what's available first time

00:44:35,240 --> 00:44:43,040
around so coming towards the end of the

00:44:41,090 --> 00:44:45,530
talk we now know hopefully how to do

00:44:43,040 --> 00:44:47,600
some logging in PHP how to ingest all

00:44:45,530 --> 00:44:49,640
that into log stash so here are some

00:44:47,600 --> 00:44:53,210
logging ideas that we've used in the

00:44:49,640 --> 00:44:56,210
past having the idea of a release marker

00:44:53,210 --> 00:44:59,600
is really good so if you think of

00:44:56,210 --> 00:45:02,000
something which can put an event into a

00:44:59,600 --> 00:45:04,070
log every time a release occurs then you

00:45:02,000 --> 00:45:06,650
can create graphs which happen over time

00:45:04,070 --> 00:45:08,960
like the error and the latency and have

00:45:06,650 --> 00:45:12,190
a vertical line when your release

00:45:08,960 --> 00:45:14,660
occurred so you can see on those graphs

00:45:12,190 --> 00:45:16,610
before the release and after the release

00:45:14,660 --> 00:45:18,350
and you can make quick judgments about

00:45:16,610 --> 00:45:19,910
whether that was a success or a failure

00:45:18,350 --> 00:45:24,470
whether you roll back or whether you

00:45:19,910 --> 00:45:27,140
stay error rates of various applications

00:45:24,470 --> 00:45:29,720
over time so I showed you every race

00:45:27,140 --> 00:45:31,940
there of the web server but imagine if

00:45:29,720 --> 00:45:34,120
you've got microservices architecture or

00:45:31,940 --> 00:45:36,380
you've got n tiered architecture and

00:45:34,120 --> 00:45:38,060
make sure you're displaying all of the

00:45:36,380 --> 00:45:40,280
error rates all the way through the tier

00:45:38,060 --> 00:45:42,290
and then that's much better for fault

00:45:40,280 --> 00:45:45,110
isolation than trying to figure out by

00:45:42,290 --> 00:45:49,100
going to the web log Web Access log and

00:45:45,110 --> 00:45:51,680
then work back yourself and one thing I

00:45:49,100 --> 00:45:53,870
didn't cover in the previous example

00:45:51,680 --> 00:45:56,690
because it's a little hard to do there

00:45:53,870 --> 00:45:59,360
are plenty of sites that show you how to

00:45:56,690 --> 00:46:01,880
do this is latency in the various

00:45:59,360 --> 00:46:04,940
Gentiles of your applications here so

00:46:01,880 --> 00:46:07,460
really want to know what is latency in

00:46:04,940 --> 00:46:10,430
the 95th percentile to give you a kind

00:46:07,460 --> 00:46:13,450
of worst case scenario what a customer's

00:46:10,430 --> 00:46:17,330
experiencing in that 5% of your

00:46:13,450 --> 00:46:19,580
experience so yeah if you're interested

00:46:17,330 --> 00:46:22,390
in that I can show you the links to

00:46:19,580 --> 00:46:26,090
where that appears in the documentation

00:46:22,390 --> 00:46:28,160
I'm so looking at HTTP responses is also

00:46:26,090 --> 00:46:29,150
really interesting so I showed

00:46:28,160 --> 00:46:31,910
500-series

00:46:29,150 --> 00:46:34,910
a lot here but equally as interesting is

00:46:31,910 --> 00:46:38,090
the 400 series so things like 401 not

00:46:34,910 --> 00:46:40,670
authorized and 403 not allowed they can

00:46:38,090 --> 00:46:42,800
be signs of kind of nefarious access to

00:46:40,670 --> 00:46:46,730
your system so having some graphs which

00:46:42,800 --> 00:46:48,470
show them is a good idea um one thing

00:46:46,730 --> 00:46:50,960
I've not done myself I've always wanted

00:46:48,470 --> 00:46:53,570
to devote the time to is auto get blame

00:46:50,960 --> 00:46:55,820
for production errors so if you've got a

00:46:53,570 --> 00:46:58,940
system which is which knows where the

00:46:55,820 --> 00:47:01,160
error was in the PHP code base and can

00:46:58,940 --> 00:47:02,560
do a quick call to get to say who's the

00:47:01,160 --> 00:47:05,150
person that last touched that

00:47:02,560 --> 00:47:08,530
now ring them and say why is this

00:47:05,150 --> 00:47:12,410
happening I think would be really good

00:47:08,530 --> 00:47:14,570
in the example code base I also tail the

00:47:12,410 --> 00:47:16,970
off and sis logs so while they're not

00:47:14,570 --> 00:47:19,250
immediately interesting to your

00:47:16,970 --> 00:47:21,440
application they are kind of symptomatic

00:47:19,250 --> 00:47:23,240
of your environment so tailing syslog

00:47:21,440 --> 00:47:25,070
and figuring out what's problem could be

00:47:23,240 --> 00:47:27,830
there telling authority figure out

00:47:25,070 --> 00:47:32,360
whether someone's messing with you good

00:47:27,830 --> 00:47:35,240
ideas so at this point I'm going to say

00:47:32,360 --> 00:47:38,600
go forth and do lots of logging but

00:47:35,240 --> 00:47:41,930
there are some gotchas along the way log

00:47:38,600 --> 00:47:43,460
rotation so I got caught out when I was

00:47:41,930 --> 00:47:45,230
doing this a while ago because I was

00:47:43,460 --> 00:47:47,000
creating a bunch of log files but not in

00:47:45,230 --> 00:47:49,820
the log directory so they didn't get

00:47:47,000 --> 00:47:53,090
picked up by log rotation so I ended up

00:47:49,820 --> 00:47:55,010
producing huge files which are really

00:47:53,090 --> 00:47:56,930
difficult to kind of manipulate later on

00:47:55,010 --> 00:48:00,470
so remember

00:47:56,930 --> 00:48:03,170
log rotation also beware of running out

00:48:00,470 --> 00:48:04,850
of space so if you're a company which

00:48:03,170 --> 00:48:07,430
hasn't done this type of logging before

00:48:04,850 --> 00:48:09,320
and then you put kind of thousands of

00:48:07,430 --> 00:48:11,390
these events in your system you're going

00:48:09,320 --> 00:48:13,260
to start generating huge log files so

00:48:11,390 --> 00:48:16,860
have a procedure in place for kind

00:48:13,260 --> 00:48:20,280
of making sure that rotated out deleted

00:48:16,860 --> 00:48:22,950
when they've got into logs - on one

00:48:20,280 --> 00:48:27,630
particular problem we had when I worked

00:48:22,950 --> 00:48:29,700
for a large betting company was the the

00:48:27,630 --> 00:48:31,980
logging process logged lots and lots of

00:48:29,700 --> 00:48:34,650
things to disk but if you're if you're

00:48:31,980 --> 00:48:37,110
on NFS or if you're on a shared folder

00:48:34,650 --> 00:48:40,980
system on VirtualBox something like that

00:48:37,110 --> 00:48:42,510
and I open sysm great so if you're doing

00:48:40,980 --> 00:48:45,270
thousands of these every time you get a

00:48:42,510 --> 00:48:50,010
request then you can expect IO to be a

00:48:45,270 --> 00:48:51,930
problem so does anybody have any

00:48:50,010 --> 00:48:55,880
questions about anything I've mentioned

00:48:51,930 --> 00:48:55,880
um and is there a mic

00:49:15,330 --> 00:49:23,170
okay first question hi hello

00:49:20,140 --> 00:49:24,700
I um I have a question about IO mm-hmm

00:49:23,170 --> 00:49:26,710
in a setup that's not a production

00:49:24,700 --> 00:49:29,530
environment would it be possible to use

00:49:26,710 --> 00:49:33,040
this but on the web servers not right to

00:49:29,530 --> 00:49:34,510
file at all but sends lots - yeah so

00:49:33,040 --> 00:49:37,840
there are there are other options other

00:49:34,510 --> 00:49:40,359
than using the file shipper which is the

00:49:37,840 --> 00:49:45,970
standard configuration so any of the

00:49:40,359 --> 00:49:48,820
kind of em cues RabbitMQ or 0 mq you can

00:49:45,970 --> 00:49:51,369
do it via syslog so syslog can send logs

00:49:48,820 --> 00:49:53,920
by UDP without writing into a file

00:49:51,369 --> 00:49:57,310
there's literally I don't know whether I

00:49:53,920 --> 00:49:59,230
can just show you on the log stash docks

00:49:57,310 --> 00:50:01,990
just to give you a scale of when I talk

00:49:59,230 --> 00:50:05,440
about it as a bus ins and outs and

00:50:01,990 --> 00:50:09,910
filters there's just so much there so on

00:50:05,440 --> 00:50:12,220
the input section file exec what's that

00:50:09,910 --> 00:50:16,420
I'm a lumberjack was why I showed you

00:50:12,220 --> 00:50:17,890
before log4j Redis s3 standard in

00:50:16,420 --> 00:50:20,380
there's just so many different ways you

00:50:17,890 --> 00:50:24,369
can get data in and so many different

00:50:20,380 --> 00:50:27,190
ways you can push it out sure any other

00:50:24,369 --> 00:50:32,410
questions I'm so there's a gentleman in

00:50:27,190 --> 00:50:35,320
the white shirt here very good talk

00:50:32,410 --> 00:50:37,240
thank you thank you you talked about at

00:50:35,320 --> 00:50:39,580
the end about running out of space and I

00:50:37,240 --> 00:50:42,940
think you meant on the application

00:50:39,580 --> 00:50:46,030
servers yes presumably you don't have

00:50:42,940 --> 00:50:48,100
infinite storage in in elastics elastic

00:50:46,030 --> 00:50:51,760
search is it and so what's your strategy

00:50:48,100 --> 00:50:54,310
for for kind of pruning this big

00:50:51,760 --> 00:50:56,410
mountain of logs yep that's quite a good

00:50:54,310 --> 00:50:57,700
question and I think I've just in the

00:50:56,410 --> 00:50:59,650
rush to get to the end just skimmed over

00:50:57,700 --> 00:51:04,119
that completely so thanks for asking um

00:50:59,650 --> 00:51:07,390
logstash will create a index for every

00:51:04,119 --> 00:51:09,910
day so one tactic to quickly get rid of

00:51:07,390 --> 00:51:12,850
a lot of old log data that you might not

00:51:09,910 --> 00:51:18,780
want would be to delete the indexes

00:51:12,850 --> 00:51:22,630
create a year ago you can also kind of

00:51:18,780 --> 00:51:24,880
you can also store the data in s3 and

00:51:22,630 --> 00:51:26,920
then bring it back into elastic search

00:51:24,880 --> 00:51:28,540
at some point in the future so maybe you

00:51:26,920 --> 00:51:30,850
want a log resolution of

00:51:28,540 --> 00:51:33,010
every event in the excuse me every event

00:51:30,850 --> 00:51:34,990
in the system for a year but then you

00:51:33,010 --> 00:51:37,810
want to kind of store all that old data

00:51:34,990 --> 00:51:40,900
in s3 so if anything ever you need to

00:51:37,810 --> 00:51:42,670
kind of do a bigger pattern analysis

00:51:40,900 --> 00:51:44,830
over say two years you just bring the

00:51:42,670 --> 00:51:46,330
data back from that storage but that's

00:51:44,830 --> 00:51:48,750
not something logstash can help you with

00:51:46,330 --> 00:51:51,120
unfortunately that's kind of an OPS task

00:51:48,750 --> 00:52:00,520
thank you

00:51:51,120 --> 00:52:02,650
question over here yeah thanks for the

00:52:00,520 --> 00:52:04,990
talk again I was just wondering about

00:52:02,650 --> 00:52:06,820
logging on all of the front end so

00:52:04,990 --> 00:52:09,010
you've got your JavaScript code written

00:52:06,820 --> 00:52:12,010
and you want to start logging that and

00:52:09,010 --> 00:52:14,830
maybe the response time that the end

00:52:12,010 --> 00:52:16,570
customer sees from what is still loading

00:52:14,830 --> 00:52:18,580
on the JavaScript all the resources and

00:52:16,570 --> 00:52:20,980
assets and whether you had that in place

00:52:18,580 --> 00:52:22,900
where you are and yeah whatever you like

00:52:20,980 --> 00:52:25,300
I've kind of caveats there I earn around

00:52:22,900 --> 00:52:27,040
that kind of process so I'm gonna have

00:52:25,300 --> 00:52:29,410
to say I've got not much knowledge of

00:52:27,040 --> 00:52:31,660
that area I'm so server side I like all

00:52:29,410 --> 00:52:34,660
my websites to be rendered in Jason and

00:52:31,660 --> 00:52:37,000
nothing else but I appreciate there is a

00:52:34,660 --> 00:52:38,470
market for that kind of logging I'm but

00:52:37,000 --> 00:52:47,350
I don't know what it is I'm sorry okay

00:52:38,470 --> 00:52:50,560
Cheers can you use log stash to send out

00:52:47,350 --> 00:52:52,630
email alerts on say say you get a number

00:52:50,560 --> 00:52:54,910
of errors or something long yes I think

00:52:52,630 --> 00:52:57,850
you can I'm one of the kind of outputs

00:52:54,910 --> 00:52:59,860
here is email and whether or not you'd

00:52:57,850 --> 00:53:03,210
want to is a different matter so I guess

00:52:59,860 --> 00:53:06,130
you could write the config file and that

00:53:03,210 --> 00:53:07,660
the config file that would do that but

00:53:06,130 --> 00:53:12,340
what equally you could just send an

00:53:07,660 --> 00:53:15,190
event to any kind of ops phone call

00:53:12,340 --> 00:53:20,140
system and have that I've had them do it

00:53:15,190 --> 00:53:23,290
and so yes I would probably given a

00:53:20,140 --> 00:53:25,930
number of events and occur that gets

00:53:23,290 --> 00:53:27,850
sent via log stash to your ops alerting

00:53:25,930 --> 00:53:31,000
system the thresholds and the logic

00:53:27,850 --> 00:53:33,240
could be in there rather than in log

00:53:31,000 --> 00:53:33,240
stash

00:53:44,490 --> 00:53:49,650
hey I'm assuming let's group everything

00:53:48,240 --> 00:53:51,660
you just did in the demo as one

00:53:49,650 --> 00:53:53,010
deployment and you spent ages making

00:53:51,660 --> 00:53:54,180
your dashboard and getting all of the

00:53:53,010 --> 00:53:56,250
rafts and everything looks brilliant

00:53:54,180 --> 00:53:58,830
could you export those dashboard

00:53:56,250 --> 00:54:00,869
configurations to move to other yep

00:53:58,830 --> 00:54:04,740
so the log status yep the log stash

00:54:00,869 --> 00:54:06,839
itself you can save um and what is

00:54:04,740 --> 00:54:09,420
actually how I think is adjacent file

00:54:06,839 --> 00:54:12,000
and you can even version control that so

00:54:09,420 --> 00:54:13,650
if you've got a couple of developers

00:54:12,000 --> 00:54:16,290
working on it or if you want to deploy

00:54:13,650 --> 00:54:18,570
it automatically to new instances they

00:54:16,290 --> 00:54:29,609
can work that kind of thing yeah it's

00:54:18,570 --> 00:54:32,250
version control and deployable thanks I

00:54:29,609 --> 00:54:34,950
have a question if for instance you're

00:54:32,250 --> 00:54:39,000
logging errors from application is there

00:54:34,950 --> 00:54:41,540
any way for for this stack to aggregate

00:54:39,000 --> 00:54:44,750
those errors for instance I will I have

00:54:41,540 --> 00:54:47,300
three kinds of errors and one sense

00:54:44,750 --> 00:54:51,210
million million of error messages and

00:54:47,300 --> 00:54:53,280
other are less so so I'd like to have

00:54:51,210 --> 00:54:56,250
three lines that are up to generate

00:54:53,280 --> 00:54:58,800
generate errors hmm so that is quite an

00:54:56,250 --> 00:55:00,690
advanced use case and I haven't done it

00:54:58,800 --> 00:55:02,880
personally but I think it's probably

00:55:00,690 --> 00:55:06,630
possible because of the aggregation

00:55:02,880 --> 00:55:08,609
features of elasticsearch so I guess

00:55:06,630 --> 00:55:10,349
Cubano is a front-end I don't know if we

00:55:08,609 --> 00:55:12,390
can do it but definitely if you were

00:55:10,349 --> 00:55:14,460
written elastic search query you could

00:55:12,390 --> 00:55:17,010
get that data so I suspect there's a way

00:55:14,460 --> 00:55:22,619
to put the two together but yeah I'm not

00:55:17,010 --> 00:55:24,800
sure if Cubana can do it thanks anybody

00:55:22,619 --> 00:55:24,800
else

00:55:25,819 --> 00:55:35,630
I just there this gentleman here in the

00:55:29,449 --> 00:55:39,309
front row hi just a good question to get

00:55:35,630 --> 00:55:41,809
an idea of scale so how many

00:55:39,309 --> 00:55:43,729
applications servers or services you

00:55:41,809 --> 00:55:45,979
need to have to start thinking of the

00:55:43,729 --> 00:55:49,430
cluster solution yeah like how much

00:55:45,979 --> 00:55:54,769
single server solution can homes so I've

00:55:49,430 --> 00:55:58,160
had a single server receiving logs from

00:55:54,769 --> 00:56:01,309
a hundred instances and that seemed to

00:55:58,160 --> 00:56:04,640
perform well for me however I've not

00:56:01,309 --> 00:56:06,969
really done that scale in production so

00:56:04,640 --> 00:56:09,499
for a test deployment I've done that and

00:56:06,969 --> 00:56:11,029
in terms of what we do at Sainsbury's

00:56:09,499 --> 00:56:15,709
we're probably looking at log stash

00:56:11,029 --> 00:56:18,650
across about 10 to 20 instances so I

00:56:15,709 --> 00:56:21,140
think I think you can scale and the kind

00:56:18,650 --> 00:56:25,039
of log collecting element of log stash

00:56:21,140 --> 00:56:28,309
by using Redis cluster and also the kind

00:56:25,039 --> 00:56:32,359
of storage aspect using elastic search

00:56:28,309 --> 00:56:36,229
clusters and you can have multiple log

00:56:32,359 --> 00:56:38,119
stash instances and ship kind of logs to

00:56:36,229 --> 00:56:39,559
different ones and they'll pour both

00:56:38,119 --> 00:56:41,809
into the because they're both doing this

00:56:39,559 --> 00:56:44,209
the same kind of processing and they're

00:56:41,809 --> 00:56:45,829
putting it into the same data source so

00:56:44,209 --> 00:56:48,670
I think the whole ELQ stack is

00:56:45,829 --> 00:56:48,670
horizontally scalable

00:56:52,560 --> 00:57:03,270
any more questions just a quick

00:56:59,910 --> 00:57:05,370
practical thing in terms of monologue

00:57:03,270 --> 00:57:09,030
what would you use on a production

00:57:05,370 --> 00:57:11,310
server for pushing logs out because the

00:57:09,030 --> 00:57:13,020
one thing that I would want to really

00:57:11,310 --> 00:57:15,270
avoid is any sort of i/o from logging

00:57:13,020 --> 00:57:17,520
hence I would never write to disk yep so

00:57:15,270 --> 00:57:20,430
how do you you know push the the logs

00:57:17,520 --> 00:57:25,920
out in some other way and wondering yeah

00:57:20,430 --> 00:57:27,870
we use so the so I've I personally

00:57:25,920 --> 00:57:28,440
always write to a file using the fingers

00:57:27,870 --> 00:57:31,230
crossed

00:57:28,440 --> 00:57:34,560
method so that I only get say one or two

00:57:31,230 --> 00:57:37,020
log lines per request and this is a

00:57:34,560 --> 00:57:40,560
guess but given that the stream handler

00:57:37,020 --> 00:57:42,690
and can write to any stream not just not

00:57:40,560 --> 00:57:44,640
just file streams and given the

00:57:42,690 --> 00:57:48,270
community behind monologue I suspect

00:57:44,640 --> 00:57:59,000
you'll be able to log to nq's or via UDP

00:57:48,270 --> 00:57:59,000
or similar lady over here

00:58:02,750 --> 00:58:08,280
yeah hi what he said about locks - it's

00:58:06,000 --> 00:58:10,800
sounds quite similar to what Splunk does

00:58:08,280 --> 00:58:13,170
so what's the difference or why is it

00:58:10,800 --> 00:58:15,540
better why is it worsened yeah right to

00:58:13,170 --> 00:58:17,130
use lock starts but not spawn so that is

00:58:15,540 --> 00:58:19,650
a really good question I've used spelunk

00:58:17,130 --> 00:58:22,980
and I think it is really really good and

00:58:19,650 --> 00:58:27,060
in the beginning it's quite cheap and so

00:58:22,980 --> 00:58:28,800
the problem is in its like I suppose

00:58:27,060 --> 00:58:29,430
it's like Splunk is like a bit like a

00:58:28,800 --> 00:58:31,860
drug dealer

00:58:29,430 --> 00:58:34,740
they'll keep selling you drugs in terms

00:58:31,860 --> 00:58:37,619
of data storage and your need to consume

00:58:34,740 --> 00:58:39,360
those drugs will increase and and you

00:58:37,619 --> 00:58:41,220
become kind of dependent on them and

00:58:39,360 --> 00:58:43,950
that price increases with the more data

00:58:41,220 --> 00:58:45,270
you need to store and that is true of

00:58:43,950 --> 00:58:47,430
log stash because obviously you need

00:58:45,270 --> 00:58:50,490
more infrastructure to support a bigger

00:58:47,430 --> 00:58:52,860
logging operation but I think it's

00:58:50,490 --> 00:58:57,570
cheaper and yeah that's the reason why I

00:58:52,860 --> 00:59:00,770
use it man the reason is it's a cheaper

00:58:57,570 --> 00:59:00,770
yeah for me

00:59:06,150 --> 00:59:11,490
I was just wondering where the log stash

00:59:08,910 --> 00:59:14,099
will actually pick up abnormal behavior

00:59:11,490 --> 00:59:16,140
from trends whether you can use that to

00:59:14,099 --> 00:59:19,230
alert yourself with yeah so one thing

00:59:16,140 --> 00:59:22,770
that I didn't show is the trends graph

00:59:19,230 --> 00:59:24,660
here so you can set up a query which

00:59:22,770 --> 00:59:28,589
would highlight a normal anomalous

00:59:24,660 --> 00:59:31,680
behavior say number of decline credit

00:59:28,589 --> 00:59:33,720
card transactions and you can show in a

00:59:31,680 --> 00:59:36,690
kind of stock tick away percentage up

00:59:33,720 --> 00:59:40,829
and down over a particular time so if

00:59:36,690 --> 00:59:42,450
you had a kind of separate dashboard to

00:59:40,829 --> 00:59:44,220
this which just had the stock ticker

00:59:42,450 --> 00:59:46,410
metrics on you'd wrote you'd written

00:59:44,220 --> 00:59:48,630
queries which surface that anomalous

00:59:46,410 --> 00:59:51,390
behavior you could very easily then see

00:59:48,630 --> 00:59:55,470
kind of green for red for going up green

00:59:51,390 --> 00:59:56,880
for going down so on the Cubana side

00:59:55,470 --> 01:00:07,589
that that's probably what I'd recommend

00:59:56,880 --> 01:00:08,910
there okay thank you anyone else okay I

01:00:07,589 --> 01:00:11,720
think that's it thank you very much for

01:00:08,910 --> 01:00:11,720

YouTube URL: https://www.youtube.com/watch?v=9dni1D4QQOk


