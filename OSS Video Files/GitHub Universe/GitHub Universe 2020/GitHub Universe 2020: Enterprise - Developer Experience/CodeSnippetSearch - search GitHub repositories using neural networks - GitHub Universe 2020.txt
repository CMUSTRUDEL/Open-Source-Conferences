Title: CodeSnippetSearch - search GitHub repositories using neural networks - GitHub Universe 2020
Publication date: 2020-12-11
Playlist: GitHub Universe 2020: Enterprise - Developer Experience
Description: 
	Presented by 
Rok Novosel, Software Engineer, CodeSnippetSearch.net
Hamel Husain, Staff Machine Learning Engineer, GitHub

For more from GitHub Universe 2020, visit https://githubuniverse.com

In this talk, Rok Novosel and Hamel Husain will present CodeSnippetSearch, a web application extension that allows you to search GitHub repositories using natural language queries and code itself. It uses PyTorch to power deep neural networks which embed natural language queries and code snippets into vectors. Using nearest neighbor search, it can determine which code snippets are most similar to a given query or another code snippet. What's more, a tool like CodeSnippetSearch can allow you to easily explore unfamiliar code focusing on the semantics without getting bogged down in the syntax, which is especially useful when onboarding a new developer onto a project.

As always, feel free to leave us a comment below and don't forget to subscribe: http://bit.ly/subgithub

Thanks!

Connect with us.
Facebook: http://fb.com/github
Twitter: http://twitter.com/github
LinkedIn: http://linkedin.com/company/github

About GitHub
GitHub is the best place to share code with friends, co-workers, classmates, and complete strangers. Millions of people use GitHub to build amazing things together. For more info, go to http://github.com
Captions: 
	00:00:01,240 --> 00:00:06,690
[Music]

00:00:14,480 --> 00:00:17,039
hi

00:00:14,799 --> 00:00:18,240
i'm hamil hussein i'm a staff machine

00:00:17,039 --> 00:00:21,279
learning engineer

00:00:18,240 --> 00:00:24,880
at github and i'm joined by rock

00:00:21,279 --> 00:00:26,320
who is a engineer at source graph

00:00:24,880 --> 00:00:28,560
and today we're going to be talking

00:00:26,320 --> 00:00:30,560
about co-snippet search but first

00:00:28,560 --> 00:00:32,399
i want to talk about code search net

00:00:30,560 --> 00:00:34,320
which is what powers code snippets

00:00:32,399 --> 00:00:37,600
search and a number of other projects

00:00:34,320 --> 00:00:37,600
as a way of introduction

00:00:40,640 --> 00:00:44,960
so first i want to talk about what code

00:00:44,160 --> 00:00:48,079
search

00:00:44,960 --> 00:00:49,840
net is so code searchnet is a project

00:00:48,079 --> 00:00:53,360
released by github

00:00:49,840 --> 00:00:56,079
a set of data and models that are meant

00:00:53,360 --> 00:00:57,920
meant for researchers to use for various

00:00:56,079 --> 00:00:58,480
artificial intelligence machine learning

00:00:57,920 --> 00:01:01,600
research

00:00:58,480 --> 00:01:04,320
tasks now

00:01:01,600 --> 00:01:06,000
in popular culture you might think of

00:01:04,320 --> 00:01:10,080
artificial intelligence

00:01:06,000 --> 00:01:11,920
in in these ways

00:01:10,080 --> 00:01:15,280
so things that people are familiar with

00:01:11,920 --> 00:01:19,119
are self-driving cars

00:01:15,280 --> 00:01:20,960
winning games like alphago um

00:01:19,119 --> 00:01:22,960
mundane things like text message

00:01:20,960 --> 00:01:25,759
completion when you're on your phone

00:01:22,960 --> 00:01:27,119
and even other things such as winning

00:01:25,759 --> 00:01:28,479
jeopardy

00:01:27,119 --> 00:01:30,960
these are all things that have been

00:01:28,479 --> 00:01:33,439
popularized in in the media as a

00:01:30,960 --> 00:01:35,280
lately uh as far as things that

00:01:33,439 --> 00:01:36,560
artificial intelligence can do

00:01:35,280 --> 00:01:38,720
however there's another side to

00:01:36,560 --> 00:01:40,320
artificial intelligence

00:01:38,720 --> 00:01:42,799
there's a side of it that can help

00:01:40,320 --> 00:01:45,200
everyday people do their jobs

00:01:42,799 --> 00:01:47,920
and especially skilled labor so what do

00:01:45,200 --> 00:01:47,920
i mean by that

00:01:48,079 --> 00:01:52,880
so for example radiologists we see

00:01:51,200 --> 00:01:54,640
artificial intelligence helping

00:01:52,880 --> 00:01:58,479
radiologists

00:01:54,640 --> 00:02:00,719
zone in on potential areas where

00:01:58,479 --> 00:02:02,799
there might be a disease or a diagnosis

00:02:00,719 --> 00:02:06,079
that needs to be made

00:02:02,799 --> 00:02:07,840
oops i went to the wrong side and

00:02:06,079 --> 00:02:09,520
you see here that there's a heat map

00:02:07,840 --> 00:02:10,959
where the artificial intelligence

00:02:09,520 --> 00:02:12,800
algorithm has highlighted for the

00:02:10,959 --> 00:02:15,120
radiologists points of interest that

00:02:12,800 --> 00:02:16,560
they might want to pay more attention to

00:02:15,120 --> 00:02:18,160
and what this does it doesn't replace

00:02:16,560 --> 00:02:19,040
the radiologist it augments the

00:02:18,160 --> 00:02:20,640
radiologists

00:02:19,040 --> 00:02:23,840
and it helps make them more effective at

00:02:20,640 --> 00:02:23,840
what they're doing

00:02:23,920 --> 00:02:30,319
similarly we see artificial intelligence

00:02:27,440 --> 00:02:32,080
augmenting writers so this is a very

00:02:30,319 --> 00:02:34,959
popular tool called grammarly

00:02:32,080 --> 00:02:35,360
you may have used it and not only does

00:02:34,959 --> 00:02:37,200
it

00:02:35,360 --> 00:02:38,800
help you with spell checking but it

00:02:37,200 --> 00:02:41,920
helps you with

00:02:38,800 --> 00:02:45,040
more nuanced type of things like

00:02:41,920 --> 00:02:47,599
engagement and

00:02:45,040 --> 00:02:48,640
uh grammar and things that are a little

00:02:47,599 --> 00:02:53,840
bit harder to do

00:02:48,640 --> 00:02:53,840
with non-ai approaches

00:02:54,319 --> 00:02:58,239
we also see artificial intelligence

00:02:56,800 --> 00:03:00,879
augmenting lawyers

00:02:58,239 --> 00:03:02,840
so there's a lot of tools there's a

00:03:00,879 --> 00:03:06,400
growing set of tools out there

00:03:02,840 --> 00:03:10,000
that oops this is the wrong slide

00:03:06,400 --> 00:03:13,519
um sorry we have artificial intelligence

00:03:10,000 --> 00:03:15,840
augmenting lawyers so

00:03:13,519 --> 00:03:18,480
uh we see that harvard intelligence is

00:03:15,840 --> 00:03:21,599
allowing lawyers to go through

00:03:18,480 --> 00:03:23,040
lots of documents and cut out the

00:03:21,599 --> 00:03:25,680
mundane work of

00:03:23,040 --> 00:03:26,480
sifting through lots and lots of data

00:03:25,680 --> 00:03:28,560
manually

00:03:26,480 --> 00:03:29,760
and helping lawyers focus on more high

00:03:28,560 --> 00:03:33,760
value tasks

00:03:29,760 --> 00:03:36,400
that they are trained for

00:03:33,760 --> 00:03:37,519
and we also see that artificial

00:03:36,400 --> 00:03:40,640
intelligence is helping in

00:03:37,519 --> 00:03:42,959
basic science so helping with things

00:03:40,640 --> 00:03:44,959
like drug discovery

00:03:42,959 --> 00:03:46,159
helping researchers scientists find

00:03:44,959 --> 00:03:49,599
molecules and

00:03:46,159 --> 00:03:52,640
compounds that might help in

00:03:49,599 --> 00:03:55,280
things like having

00:03:52,640 --> 00:03:56,799
creating therapeutics or medicines for

00:03:55,280 --> 00:03:58,560
various different diseases

00:03:56,799 --> 00:04:01,840
and being able to find those a lot

00:03:58,560 --> 00:04:01,840
faster than ever before

00:04:04,000 --> 00:04:08,560
we also see ai augmenting things like

00:04:06,799 --> 00:04:09,200
artists so even creative professions are

00:04:08,560 --> 00:04:12,720
being

00:04:09,200 --> 00:04:14,799
aided by ai which is something that

00:04:12,720 --> 00:04:16,079
folks may have not anticipated ai helps

00:04:14,799 --> 00:04:18,639
with but it does

00:04:16,079 --> 00:04:20,639
and here there's a technique called uh

00:04:18,639 --> 00:04:22,240
generative adversarial networks that

00:04:20,639 --> 00:04:25,440
help

00:04:22,240 --> 00:04:26,880
uh artists even create interesting

00:04:25,440 --> 00:04:28,639
pieces

00:04:26,880 --> 00:04:30,639
that some of which have turned out to be

00:04:28,639 --> 00:04:33,120
very popular

00:04:30,639 --> 00:04:34,160
but this begs the question what about

00:04:33,120 --> 00:04:37,120
programmers

00:04:34,160 --> 00:04:38,639
and that can artificial intelligence aid

00:04:37,120 --> 00:04:42,080
programmers

00:04:38,639 --> 00:04:44,080
and how and that's a question that we

00:04:42,080 --> 00:04:45,919
have been interested in at github as

00:04:44,080 --> 00:04:49,280
researchers

00:04:45,919 --> 00:04:50,880
now one key facet of

00:04:49,280 --> 00:04:53,280
artificial intelligence in machine

00:04:50,880 --> 00:04:55,440
learning is it's very data hungry

00:04:53,280 --> 00:04:58,160
you need lots of data you need lots of

00:04:55,440 --> 00:05:00,479
examples to show an algorithm

00:04:58,160 --> 00:05:01,759
or training algorithm on how to

00:05:00,479 --> 00:05:04,800
understand a domain

00:05:01,759 --> 00:05:08,400
like programming so this is why

00:05:04,800 --> 00:05:11,759
github released code search net

00:05:08,400 --> 00:05:14,800
code search net is a large corpus of

00:05:11,759 --> 00:05:15,840
of data and it's essentially code and

00:05:14,800 --> 00:05:17,919
comment pairs

00:05:15,840 --> 00:05:19,759
there's two million code in comment

00:05:17,919 --> 00:05:20,880
pairs pulled from various github

00:05:19,759 --> 00:05:24,000
repositories

00:05:20,880 --> 00:05:26,320
cleaned and curated and

00:05:24,000 --> 00:05:27,759
um what that what that is is a treasure

00:05:26,320 --> 00:05:31,440
trove of data

00:05:27,759 --> 00:05:32,880
for researchers to then use for various

00:05:31,440 --> 00:05:35,520
artificial intelligence and machine

00:05:32,880 --> 00:05:35,520
learning tests

00:05:35,759 --> 00:05:40,720
so well not only is it the data but

00:05:39,199 --> 00:05:42,320
there's also some other things i should

00:05:40,720 --> 00:05:43,199
talk about in a moment but this gives

00:05:42,320 --> 00:05:46,639
you a breakdown

00:05:43,199 --> 00:05:48,479
various languages have been represented

00:05:46,639 --> 00:05:51,600
in this data

00:05:48,479 --> 00:05:53,039
go java javascript python ruby so on and

00:05:51,600 --> 00:05:54,560
so forth

00:05:53,039 --> 00:05:56,960
and we can see there's a lot of

00:05:54,560 --> 00:05:59,520
diversity in here for researchers to

00:05:56,960 --> 00:05:59,520
dive into

00:06:00,160 --> 00:06:03,680
so in addition to the data we also

00:06:02,319 --> 00:06:06,400
provided benchmarks

00:06:03,680 --> 00:06:07,280
and reference models and one task that

00:06:06,400 --> 00:06:10,479
we propose

00:06:07,280 --> 00:06:13,759
is search so given a natural language

00:06:10,479 --> 00:06:16,319
description of some kind of code

00:06:13,759 --> 00:06:18,080
the task is that we propose is can you

00:06:16,319 --> 00:06:21,280
find code that does that

00:06:18,080 --> 00:06:23,440
and to do that in a very semantic way so

00:06:21,280 --> 00:06:25,680
not keyword search so you can imagine

00:06:23,440 --> 00:06:26,560
this case that is illustrated on this

00:06:25,680 --> 00:06:28,800
slide

00:06:26,560 --> 00:06:30,720
let's say you're trying to find code

00:06:28,800 --> 00:06:32,000
that pings a rest api and returns

00:06:30,720 --> 00:06:34,000
results but suppose

00:06:32,000 --> 00:06:35,440
the code doesn't contain any of those

00:06:34,000 --> 00:06:37,199
keywords at all

00:06:35,440 --> 00:06:39,440
the code doesn't contain the words ping

00:06:37,199 --> 00:06:40,960
or rest or api

00:06:39,440 --> 00:06:43,360
so how would you find that code if you

00:06:40,960 --> 00:06:45,039
didn't know if you're not familiar with

00:06:43,360 --> 00:06:48,240
the syntax you're just new to a project

00:06:45,039 --> 00:06:51,039
so what this is meant to illustrate is

00:06:48,240 --> 00:06:53,039
ai can help with tasks like this

00:06:51,039 --> 00:06:54,639
semantic search of code

00:06:53,039 --> 00:06:56,080
and this is one of the references

00:06:54,639 --> 00:06:59,120
provided by codes

00:06:56,080 --> 00:07:00,880
code search net so code search net is a

00:06:59,120 --> 00:07:02,960
competition

00:07:00,880 --> 00:07:04,720
where people entered and competed on

00:07:02,960 --> 00:07:07,199
this task

00:07:04,720 --> 00:07:08,639
but it's also a data set and that data

00:07:07,199 --> 00:07:10,160
set you can find more information on

00:07:08,639 --> 00:07:12,319
github at github

00:07:10,160 --> 00:07:14,479
codesearchnet where the reference models

00:07:12,319 --> 00:07:16,000
and the data set are hosted

00:07:14,479 --> 00:07:17,759
but i don't want to spend too much time

00:07:16,000 --> 00:07:19,520
on that um

00:07:17,759 --> 00:07:21,440
but i want to i really want to hand it

00:07:19,520 --> 00:07:22,880
off to rock who is

00:07:21,440 --> 00:07:24,479
one of our top participants in

00:07:22,880 --> 00:07:27,919
codesearchnet who

00:07:24,479 --> 00:07:30,639
in my opinion made the most interesting

00:07:27,919 --> 00:07:32,080
thing out of searching it and i want to

00:07:30,639 --> 00:07:34,240
head it over to him so he can tell you

00:07:32,080 --> 00:07:34,240
more

00:07:34,319 --> 00:07:38,080
um thank you i'll give it to you

00:07:39,919 --> 00:07:44,639
thank you hamil um hi my name is rook

00:07:43,120 --> 00:07:45,599
and today i would like to present code

00:07:44,639 --> 00:07:48,080
snippet search

00:07:45,599 --> 00:07:49,759
it is a web application and a web

00:07:48,080 --> 00:07:51,360
extension that allows you to search

00:07:49,759 --> 00:07:54,160
github repositories

00:07:51,360 --> 00:07:56,400
and uh guitar repositories using natural

00:07:54,160 --> 00:07:59,840
language queries and code itself

00:07:56,400 --> 00:07:59,840
so next slide

00:08:03,120 --> 00:08:05,919
hamill next slide

00:08:06,479 --> 00:08:10,400
um so code search for me at least is one

00:08:08,879 --> 00:08:11,280
of the most important tools during

00:08:10,400 --> 00:08:12,960
coding

00:08:11,280 --> 00:08:14,960
i noticed that searching and navigating

00:08:12,960 --> 00:08:16,319
through code heavily outweighs the

00:08:14,960 --> 00:08:18,000
actual coding so

00:08:16,319 --> 00:08:19,520
when i'm already familiar with the code

00:08:18,000 --> 00:08:21,120
base i find that

00:08:19,520 --> 00:08:23,039
exact search is what they need for

00:08:21,120 --> 00:08:25,120
example regular expressions

00:08:23,039 --> 00:08:26,639
and case and sensitive search that is

00:08:25,120 --> 00:08:27,599
because i'm already familiar with the

00:08:26,639 --> 00:08:29,599
naming scheme

00:08:27,599 --> 00:08:31,360
and i can reasonably predict how to

00:08:29,599 --> 00:08:34,000
formulate a search query

00:08:31,360 --> 00:08:35,120
and every id and code editor is already

00:08:34,000 --> 00:08:36,719
equipped with it

00:08:35,120 --> 00:08:38,320
but when i'm not familiar with the code

00:08:36,719 --> 00:08:39,120
base or even a new folder within an

00:08:38,320 --> 00:08:40,959
existing large

00:08:39,120 --> 00:08:42,479
project it makes searching more

00:08:40,959 --> 00:08:43,120
difficult because it's more trial and

00:08:42,479 --> 00:08:44,880
error

00:08:43,120 --> 00:08:46,560
a tool like code snippet search would

00:08:44,880 --> 00:08:48,240
allow me to easily explore

00:08:46,560 --> 00:08:50,399
unfamiliar code focusing on the

00:08:48,240 --> 00:08:51,440
semantics without getting bogged down in

00:08:50,399 --> 00:08:53,360
the syntax

00:08:51,440 --> 00:08:55,279
for example this is especially useful

00:08:53,360 --> 00:08:57,040
when onboarding a new developer onto a

00:08:55,279 --> 00:08:57,600
project because it can be a significant

00:08:57,040 --> 00:09:00,320
boost

00:08:57,600 --> 00:09:01,279
to their productivity outside of a work

00:09:00,320 --> 00:09:03,040
environment we

00:09:01,279 --> 00:09:04,480
encounter unfamiliar code in the form of

00:09:03,040 --> 00:09:06,160
github repositories

00:09:04,480 --> 00:09:08,720
semantic search tools would provide a

00:09:06,160 --> 00:09:10,480
faster way for users to find answers

00:09:08,720 --> 00:09:12,480
to their issues directly in the code

00:09:10,480 --> 00:09:15,200
consequently it would lessen the burden

00:09:12,480 --> 00:09:18,560
on maintainers to provide these answers

00:09:15,200 --> 00:09:20,399
next slide please to give a short

00:09:18,560 --> 00:09:21,920
example let's say i'm a new developer at

00:09:20,399 --> 00:09:24,080
a company and i get a ticket to

00:09:21,920 --> 00:09:26,399
implement an api endpoint that fetches

00:09:24,080 --> 00:09:28,399
products and their shipping information

00:09:26,399 --> 00:09:30,000
additionally i want to implement as much

00:09:28,399 --> 00:09:31,680
as possible on my own

00:09:30,000 --> 00:09:33,040
traditionally i would start searching

00:09:31,680 --> 00:09:34,880
and going through all classes and

00:09:33,040 --> 00:09:36,959
functions that mention products or

00:09:34,880 --> 00:09:38,720
shipping depending on the size of the

00:09:36,959 --> 00:09:39,519
code base that may be a lot of code to

00:09:38,720 --> 00:09:41,200
go through

00:09:39,519 --> 00:09:42,640
there really is no good way to

00:09:41,200 --> 00:09:43,440
automatically narrow down the search

00:09:42,640 --> 00:09:44,959
results

00:09:43,440 --> 00:09:47,200
until you get more familiar with the

00:09:44,959 --> 00:09:49,040
code base and you only get more familiar

00:09:47,200 --> 00:09:51,120
if you work with it for long enough

00:09:49,040 --> 00:09:53,120
so this leads to bootstrap problem that

00:09:51,120 --> 00:09:55,279
all developers eventually overcome

00:09:53,120 --> 00:09:57,120
to shorten the bootstrap time i'd rather

00:09:55,279 --> 00:09:58,880
enter a few general queries

00:09:57,120 --> 00:10:00,320
that automatically surface the best

00:09:58,880 --> 00:10:02,560
possible results

00:10:00,320 --> 00:10:04,560
so in our case here i will enter get all

00:10:02,560 --> 00:10:07,519
products get shipping info for product

00:10:04,560 --> 00:10:08,320
and paginate api response i would expect

00:10:07,519 --> 00:10:10,399
to get product

00:10:08,320 --> 00:10:11,839
and shipping info classes for example if

00:10:10,399 --> 00:10:13,839
we're programming in an

00:10:11,839 --> 00:10:15,760
object-oriented language and some

00:10:13,839 --> 00:10:16,959
functions dealing with sql and

00:10:15,760 --> 00:10:18,000
retrieving the entities from the

00:10:16,959 --> 00:10:19,600
database

00:10:18,000 --> 00:10:21,120
i may be a bit stretching with this

00:10:19,600 --> 00:10:22,720
example but hopefully you get the

00:10:21,120 --> 00:10:25,839
overall point

00:10:22,720 --> 00:10:26,480
next slide please so let's move on to

00:10:25,839 --> 00:10:28,959
the demo

00:10:26,480 --> 00:10:30,160
or rather some screenshots of the demo

00:10:28,959 --> 00:10:31,880
first we'll check out the web

00:10:30,160 --> 00:10:33,279
application located at

00:10:31,880 --> 00:10:34,640
codesnippetsearch.net

00:10:33,279 --> 00:10:36,160
on the first page we see a list of

00:10:34,640 --> 00:10:37,279
supported repositories with their

00:10:36,160 --> 00:10:39,519
descriptions and

00:10:37,279 --> 00:10:41,360
these supported languages now we need to

00:10:39,519 --> 00:10:43,279
select the repository to search and

00:10:41,360 --> 00:10:44,000
since django powers the back end of this

00:10:43,279 --> 00:10:46,399
project

00:10:44,000 --> 00:10:49,200
it seems like a good place to start next

00:10:46,399 --> 00:10:49,200
next slide please

00:10:49,279 --> 00:10:53,440
um so let's say we wanted to know how to

00:10:51,279 --> 00:10:55,440
get a database table name in django

00:10:53,440 --> 00:10:56,880
well we get back a list of code snippets

00:10:55,440 --> 00:10:57,760
each one containing the link to the

00:10:56,880 --> 00:10:59,920
github file

00:10:57,760 --> 00:11:02,079
match reading similar code snippets and

00:10:59,920 --> 00:11:03,600
syntax highlighted code

00:11:02,079 --> 00:11:05,200
everything except the match rating and

00:11:03,600 --> 00:11:06,880
the similar code snippets link should be

00:11:05,200 --> 00:11:08,800
pretty self-explanatory

00:11:06,880 --> 00:11:11,440
we'll go over the similar code snippets

00:11:08,800 --> 00:11:13,040
later on the match rating is calculated

00:11:11,440 --> 00:11:14,320
from the vector distance between the

00:11:13,040 --> 00:11:16,079
query and the snippet

00:11:14,320 --> 00:11:17,760
so higher the match rating the closer

00:11:16,079 --> 00:11:19,760
the query and the snippet

00:11:17,760 --> 00:11:21,920
this is a little foreshadowing into how

00:11:19,760 --> 00:11:23,680
code snippet search works under the hood

00:11:21,920 --> 00:11:25,360
in this case we didn't get the exact

00:11:23,680 --> 00:11:27,040
answer we were searching for

00:11:25,360 --> 00:11:28,320
but looking at the first snippet i think

00:11:27,040 --> 00:11:30,000
we could make it work with a bit of

00:11:28,320 --> 00:11:33,120
tweaking

00:11:30,000 --> 00:11:34,640
next slide please to showcase the

00:11:33,120 --> 00:11:36,000
similar code snippets option

00:11:34,640 --> 00:11:38,800
let's say you wanted to know how to

00:11:36,000 --> 00:11:40,640
convert a time zone to a local date time

00:11:38,800 --> 00:11:42,399
by clicking on the similar code snippets

00:11:40,640 --> 00:11:43,760
link code snippet search will look for

00:11:42,399 --> 00:11:46,160
other code snippets

00:11:43,760 --> 00:11:47,519
that best match the selected one as you

00:11:46,160 --> 00:11:49,760
can anticipate we'll get

00:11:47,519 --> 00:11:51,440
a lot of time zone converting functions

00:11:49,760 --> 00:11:54,880
this is great for exploring the code

00:11:51,440 --> 00:11:58,320
base and similar functionality

00:11:54,880 --> 00:12:00,240
next slide please okay so

00:11:58,320 --> 00:12:01,760
let's move on to the web extension web

00:12:00,240 --> 00:12:03,120
extension can be used directly on the

00:12:01,760 --> 00:12:05,040
github repository

00:12:03,120 --> 00:12:06,639
i'll be using the python repository as

00:12:05,040 --> 00:12:08,160
an example because i'm using it to

00:12:06,639 --> 00:12:09,519
implement the neural networks within

00:12:08,160 --> 00:12:11,360
code snippet search

00:12:09,519 --> 00:12:12,639
the web extension is implemented in the

00:12:11,360 --> 00:12:14,560
form of a sidebar

00:12:12,639 --> 00:12:16,639
that is opened by pressing alt shift and

00:12:14,560 --> 00:12:18,639
s there you can search by entering a

00:12:16,639 --> 00:12:21,279
query or a code snippet

00:12:18,639 --> 00:12:21,279
next slide

00:12:23,440 --> 00:12:26,959
so if i was interested in one

00:12:25,040 --> 00:12:29,040
dimensional convolution in pytorch

00:12:26,959 --> 00:12:30,720
i can enter the query in the sidebar the

00:12:29,040 --> 00:12:33,120
search results have the same format

00:12:30,720 --> 00:12:35,920
as an as on the web application but they

00:12:33,120 --> 00:12:39,760
are more accessible through the sidebar

00:12:35,920 --> 00:12:41,680
next slide please a convenient way

00:12:39,760 --> 00:12:43,600
of searching by code is selecting a code

00:12:41,680 --> 00:12:45,120
snippet right clicking and selecting the

00:12:43,600 --> 00:12:46,720
search by code option

00:12:45,120 --> 00:12:48,720
with the search by code option you can

00:12:46,720 --> 00:12:50,160
search using any arbitrary code snippet

00:12:48,720 --> 00:12:52,160
and the most similar snippets from the

00:12:50,160 --> 00:12:54,320
repository will be returned

00:12:52,160 --> 00:12:56,480
this could be repurposed into detecting

00:12:54,320 --> 00:12:58,720
duplicate code in pull requests as an

00:12:56,480 --> 00:13:01,519
automated check or even as an offline

00:12:58,720 --> 00:13:04,720
tool to help us while developing

00:13:01,519 --> 00:13:06,880
next slide please the main data source

00:13:04,720 --> 00:13:09,360
and inspiration for code snippet search

00:13:06,880 --> 00:13:10,800
is of course the code search net project

00:13:09,360 --> 00:13:12,480
it provides various baseline

00:13:10,800 --> 00:13:13,839
implementations of neural code search

00:13:12,480 --> 00:13:15,760
and tensorflow

00:13:13,839 --> 00:13:17,040
they range from simpler ones like back

00:13:15,760 --> 00:13:18,839
of words to the ones using

00:13:17,040 --> 00:13:21,040
state-of-the-art techniques like

00:13:18,839 --> 00:13:22,880
self-attention my implementation was

00:13:21,040 --> 00:13:24,399
inspired by their neural bag awards

00:13:22,880 --> 00:13:25,839
baseline implementation

00:13:24,399 --> 00:13:28,079
because it was performing the best

00:13:25,839 --> 00:13:29,920
overall initially i had written code

00:13:28,079 --> 00:13:32,320
snippets searching keras and was able to

00:13:29,920 --> 00:13:34,000
search through the code search at corpus

00:13:32,320 --> 00:13:35,680
the cairo's models became really

00:13:34,000 --> 00:13:37,120
unwieldy to work with due to the

00:13:35,680 --> 00:13:37,920
specific structure of the neural

00:13:37,120 --> 00:13:39,600
networks

00:13:37,920 --> 00:13:41,120
they also did not place the play nice

00:13:39,600 --> 00:13:42,000
when developing the app together with

00:13:41,120 --> 00:13:44,000
django

00:13:42,000 --> 00:13:45,680
so i had to look for alternatives i

00:13:44,000 --> 00:13:47,199
decided to switch to pytorch when i

00:13:45,680 --> 00:13:48,639
wanted to add support for searching

00:13:47,199 --> 00:13:50,240
github repositories

00:13:48,639 --> 00:13:52,000
because the model code became really

00:13:50,240 --> 00:13:52,959
simplified and there were no issues when

00:13:52,000 --> 00:13:55,920
developing

00:13:52,959 --> 00:13:57,120
and deploying the django app next slide

00:13:55,920 --> 00:13:58,959
please

00:13:57,120 --> 00:14:00,399
so code snippet search works by using

00:13:58,959 --> 00:14:02,240
joint embeddings of code

00:14:00,399 --> 00:14:04,880
and queries to implement a neural search

00:14:02,240 --> 00:14:06,800
system the training objective is to map

00:14:04,880 --> 00:14:08,880
code and corresponding queries onto

00:14:06,800 --> 00:14:10,399
vectors that are similar to each other

00:14:08,880 --> 00:14:12,000
there are multiple ways to measure

00:14:10,399 --> 00:14:13,760
similarity between vectors

00:14:12,000 --> 00:14:16,160
but i am using cosine similarity

00:14:13,760 --> 00:14:18,079
throughout with a trained model i can

00:14:16,160 --> 00:14:19,760
embed a natural language query

00:14:18,079 --> 00:14:21,600
and then yield and then use nearest

00:14:19,760 --> 00:14:23,440
neighbor search to return a set of

00:14:21,600 --> 00:14:25,120
closest code snippets

00:14:23,440 --> 00:14:27,120
during training i use function dot

00:14:25,120 --> 00:14:28,480
strings as approximations of natural

00:14:27,120 --> 00:14:32,079
language queries

00:14:28,480 --> 00:14:34,480
next slide please so here

00:14:32,079 --> 00:14:36,639
here is just a simple visual

00:14:34,480 --> 00:14:39,600
representations of these visual

00:14:36,639 --> 00:14:40,560
of these um embeddings that we're we're

00:14:39,600 --> 00:14:43,519
trying to

00:14:40,560 --> 00:14:45,839
learn by training the models next slide

00:14:43,519 --> 00:14:45,839
please

00:14:46,160 --> 00:14:49,279
and in this image i have the entire

00:14:47,680 --> 00:14:51,920
model split up into layers

00:14:49,279 --> 00:14:54,000
first is the input layer one input is

00:14:51,920 --> 00:14:55,040
four queries and one input each for

00:14:54,000 --> 00:14:56,880
languages

00:14:55,040 --> 00:14:59,360
inputs are forwarded into embedding

00:14:56,880 --> 00:15:01,199
layers with jump out applied afterward

00:14:59,360 --> 00:15:03,519
the magic happens in the encoding and

00:15:01,199 --> 00:15:05,199
pulling layer here i take the simplest

00:15:03,519 --> 00:15:07,199
but in this case the most effective way

00:15:05,199 --> 00:15:08,000
of encoding the tokens using a weighted

00:15:07,199 --> 00:15:10,000
mean

00:15:08,000 --> 00:15:12,160
the weights i use for weighting are

00:15:10,000 --> 00:15:13,680
learned in a separate strainable layer

00:15:12,160 --> 00:15:16,000
since we split up the languages at the

00:15:13,680 --> 00:15:18,000
input we have to concatenate them back

00:15:16,000 --> 00:15:19,199
in the same order as they appear in the

00:15:18,000 --> 00:15:21,040
queries

00:15:19,199 --> 00:15:23,279
finally i normalize the rows in both

00:15:21,040 --> 00:15:26,079
matrices and multiply them to get

00:15:23,279 --> 00:15:27,680
cosine similarities the last function

00:15:26,079 --> 00:15:29,680
can be alternatively explained as

00:15:27,680 --> 00:15:30,560
maximizing the similarities between the

00:15:29,680 --> 00:15:33,040
corresponding

00:15:30,560 --> 00:15:34,959
coded query pairs while minimizing the

00:15:33,040 --> 00:15:36,399
similarities between non-corresponding

00:15:34,959 --> 00:15:38,240
pairs

00:15:36,399 --> 00:15:40,240
okay and how is the actual searching

00:15:38,240 --> 00:15:42,240
performed i take all of the code

00:15:40,240 --> 00:15:43,759
snippets i encode them using the trained

00:15:42,240 --> 00:15:45,120
model and store them in a nearest

00:15:43,759 --> 00:15:46,639
neighbor's search index

00:15:45,120 --> 00:15:48,480
when i want to look up the nearest code

00:15:46,639 --> 00:15:50,320
snippets using a query

00:15:48,480 --> 00:15:53,199
i first encode the query and then look

00:15:50,320 --> 00:15:54,959
for the nearest code snippet

00:15:53,199 --> 00:15:56,959
and here we have the training procedure

00:15:54,959 --> 00:15:58,639
which is split into two major parts

00:15:56,959 --> 00:16:00,160
training the base language models and

00:15:58,639 --> 00:16:02,639
then training the repository language

00:16:00,160 --> 00:16:04,959
models with the help of the base models

00:16:02,639 --> 00:16:06,720
both contain roughly the same steps

00:16:04,959 --> 00:16:08,480
first the code and query sequences

00:16:06,720 --> 00:16:09,759
are pre-processed for example the tokens

00:16:08,480 --> 00:16:12,320
are converted to lower case

00:16:09,759 --> 00:16:14,160
and invalid tokens are filtered out then

00:16:12,320 --> 00:16:16,000
we prepare the vocabularies for code and

00:16:14,160 --> 00:16:17,279
query tokens and we only keep the most

00:16:16,000 --> 00:16:19,120
frequent ones

00:16:17,279 --> 00:16:20,880
with the help of the vocabularies we

00:16:19,120 --> 00:16:22,320
encode the tokens into integers

00:16:20,880 --> 00:16:24,560
that can then be used in the neural

00:16:22,320 --> 00:16:26,800
network embeddings all that is left is

00:16:24,560 --> 00:16:28,639
to train the model and save it

00:16:26,800 --> 00:16:30,800
i train the base language models using

00:16:28,639 --> 00:16:33,040
the code search net corpus first

00:16:30,800 --> 00:16:34,560
the main goal of the base language model

00:16:33,040 --> 00:16:36,480
is to train the wording code token

00:16:34,560 --> 00:16:38,079
embeddings that can then be transferred

00:16:36,480 --> 00:16:39,920
to repository models

00:16:38,079 --> 00:16:41,600
i'm hoping that the base language model

00:16:39,920 --> 00:16:43,519
learns general language features that

00:16:41,600 --> 00:16:44,560
can then be fine-tuned by the repository

00:16:43,519 --> 00:16:46,560
data

00:16:44,560 --> 00:16:48,000
with repositories i was on my own to

00:16:46,560 --> 00:16:50,320
extract the corpus

00:16:48,000 --> 00:16:52,240
i'm using github's treeseater to find

00:16:50,320 --> 00:16:54,240
functions and then tokenize them

00:16:52,240 --> 00:16:55,279
repository models have the exact same

00:16:54,240 --> 00:16:57,199
structure i just

00:16:55,279 --> 00:16:59,199
initialized all embedding layers with

00:16:57,199 --> 00:17:00,639
base language embeddings

00:16:59,199 --> 00:17:02,320
i also want to mention that there are of

00:17:00,639 --> 00:17:04,240
course plenty of warnings attached to

00:17:02,320 --> 00:17:05,600
the code snippet search model

00:17:04,240 --> 00:17:07,600
you need a reasonably large

00:17:05,600 --> 00:17:08,480
well-documented repository to train the

00:17:07,600 --> 00:17:11,039
model

00:17:08,480 --> 00:17:12,640
and even then it only works on functions

00:17:11,039 --> 00:17:14,559
so the first next step would be to

00:17:12,640 --> 00:17:16,400
figure out how to work on smaller chunks

00:17:14,559 --> 00:17:18,400
of code for example a couple of lines of

00:17:16,400 --> 00:17:20,240
code with a comment above

00:17:18,400 --> 00:17:22,079
from a model perspective i would like to

00:17:20,240 --> 00:17:23,439
experiment with the tree based models

00:17:22,079 --> 00:17:25,039
that could potentially capture the

00:17:23,439 --> 00:17:26,240
information hidden in the exchange

00:17:25,039 --> 00:17:27,679
syntax tree

00:17:26,240 --> 00:17:29,360
but for that i would need to gather a

00:17:27,679 --> 00:17:32,480
lot more samples

00:17:29,360 --> 00:17:34,720
next slide please

00:17:32,480 --> 00:17:36,400
and i saved a small anecdote for the end

00:17:34,720 --> 00:17:38,400
which highlights why reinventing the

00:17:36,400 --> 00:17:39,440
wheel is sometimes good and perhaps even

00:17:38,400 --> 00:17:41,200
necessary

00:17:39,440 --> 00:17:42,720
as software engineers we have thought

00:17:41,200 --> 00:17:44,720
that we're thought that we should avoid

00:17:42,720 --> 00:17:47,120
reinventing as much as possible

00:17:44,720 --> 00:17:48,799
and just use the already proven solution

00:17:47,120 --> 00:17:50,400
this is generally good advice that

00:17:48,799 --> 00:17:51,039
prevents us spending too much time

00:17:50,400 --> 00:17:53,280
developing

00:17:51,039 --> 00:17:54,799
unnecessary things but if you're looking

00:17:53,280 --> 00:17:56,559
to learn deepen your knowledge

00:17:54,799 --> 00:17:58,480
and have fun while doing it then

00:17:56,559 --> 00:17:59,280
rewinding the wheel is a great place to

00:17:58,480 --> 00:18:00,880
start

00:17:59,280 --> 00:18:02,480
that was the main source of motivation

00:18:00,880 --> 00:18:04,080
for me and why i wanted to join the

00:18:02,480 --> 00:18:05,840
codesearchnet challenge

00:18:04,080 --> 00:18:07,679
of course i could have taken the already

00:18:05,840 --> 00:18:08,799
implemented baseline models and tinker

00:18:07,679 --> 00:18:10,559
with them

00:18:08,799 --> 00:18:12,320
my goal was to learn newer networks in

00:18:10,559 --> 00:18:14,400
depth and actually understand the

00:18:12,320 --> 00:18:16,559
mechanics behind them

00:18:14,400 --> 00:18:18,720
and my re-implementation efforts led me

00:18:16,559 --> 00:18:19,600
to discover a small bug in the region in

00:18:18,720 --> 00:18:22,080
the original

00:18:19,600 --> 00:18:24,000
baseline pipeline that was causing a big

00:18:22,080 --> 00:18:26,000
problem with the evaluation

00:18:24,000 --> 00:18:27,919
as i've mentioned i re-implemented the

00:18:26,000 --> 00:18:29,760
code search and baseline models

00:18:27,919 --> 00:18:31,840
including the evaluation part

00:18:29,760 --> 00:18:33,840
i used scikit's nearest neighbor search

00:18:31,840 --> 00:18:35,120
class instead of hanoi index which is a

00:18:33,840 --> 00:18:37,919
nearest neighbor library

00:18:35,120 --> 00:18:39,840
used by codeserchnet i wasn't expecting

00:18:37,919 --> 00:18:41,600
a good result on the challenge since i

00:18:39,840 --> 00:18:43,440
wasn't doing anything special

00:18:41,600 --> 00:18:45,039
but it turned out i was at the top of

00:18:43,440 --> 00:18:47,440
the leaderboard almost doubling the

00:18:45,039 --> 00:18:49,760
previous best evaluation score

00:18:47,440 --> 00:18:51,600
this puzzled me and the organizers as

00:18:49,760 --> 00:18:53,919
well so i did some digging

00:18:51,600 --> 00:18:54,880
since i re-implemented the models twice

00:18:53,919 --> 00:18:56,960
once in keras

00:18:54,880 --> 00:18:58,720
and once in pi torch i was fairly

00:18:56,960 --> 00:19:00,640
confident that my models were close

00:18:58,720 --> 00:19:02,559
enough to the baseline models

00:19:00,640 --> 00:19:04,080
the only thing different in my pipeline

00:19:02,559 --> 00:19:06,160
was the evaluation

00:19:04,080 --> 00:19:07,679
i swapped out the sidekick version i had

00:19:06,160 --> 00:19:09,919
been using for searching with

00:19:07,679 --> 00:19:11,919
annoy index and my score instantly

00:19:09,919 --> 00:19:13,600
dropped it turned out the baseline code

00:19:11,919 --> 00:19:15,280
search at the variation code

00:19:13,600 --> 00:19:17,440
didn't update the size of the annoy

00:19:15,280 --> 00:19:18,880
index and the final search index was too

00:19:17,440 --> 00:19:20,799
small to capture the necessary

00:19:18,880 --> 00:19:22,880
information

00:19:20,799 --> 00:19:24,960
that meant it was returning terrible

00:19:22,880 --> 00:19:26,720
results so the lesson is

00:19:24,960 --> 00:19:28,320
always benchmark the index size if

00:19:26,720 --> 00:19:30,320
you're using annuity index

00:19:28,320 --> 00:19:31,840
and don't be afraid to reinvent the

00:19:30,320 --> 00:19:33,679
wheel you just might find yourself

00:19:31,840 --> 00:19:35,280
giving a talk at github universe a

00:19:33,679 --> 00:19:39,840
couple of months later

00:19:35,280 --> 00:19:39,840

YouTube URL: https://www.youtube.com/watch?v=erHImBut71E


