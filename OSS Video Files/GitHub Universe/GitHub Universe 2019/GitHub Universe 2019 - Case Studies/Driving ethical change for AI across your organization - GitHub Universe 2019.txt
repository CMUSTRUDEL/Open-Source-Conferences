Title: Driving ethical change for AI across your organization - GitHub Universe 2019
Publication date: 2019-12-14
Playlist: GitHub Universe 2019 - Case Studies
Description: 
	Presented by Andrea Gallego, CTO at BCG GAMMA Global

As AI becomes more prevalent in our workplaces, it is imperative that solutions are deployed ethically. Organizations need to be cognizant of how they collect and use data, the products they leverage, and the biases that could applied. Most importantly, we need to make sure those who develop algorithms are as diverse as the societies they serve, and that they have the ability to harness the interests and needs of their users. Andrea Gallego, CTO of BCG GAMMA, will explain how GitHub is used by GAMMA’s software and data engineers, data scientists, and analysts to check that unique perspectives and insights are curated and nurtured, and collaboration is tracked and supported across the organization—making GAMMA’s AI solutions more ethical, inclusive, and comprehensive. This talk will help you answer the question: How can we make sure the intelligence we gain from innovations is created and managed ethically?

About GitHub Universe:
GitHub Universe is a two-day conference dedicated to the creativity and curiosity of the largest software community in the world. Sessions cover topics from team culture to open source software across industries and technologies.

For more information on GitHub Universe, check the website:
https://githubuniverse.com
Captions: 
	00:00:00,580 --> 00:00:14,179
[Music]

00:00:17,120 --> 00:00:21,320
all right everyone how you doing towards

00:00:19,700 --> 00:00:26,119
the end of the day here so thanks for

00:00:21,320 --> 00:00:28,460
showing up so my name is uh I'll talk

00:00:26,119 --> 00:00:31,400
about myself before I go right into this

00:00:28,460 --> 00:00:33,260
but so my name is Andrea Gagne go I'm a

00:00:31,400 --> 00:00:36,020
chief technology officer at the Boston

00:00:33,260 --> 00:00:38,540
Consulting Group and within that group I

00:00:36,020 --> 00:00:41,449
I work for gamma which is our analytics

00:00:38,540 --> 00:00:44,510
and artificial intelligence unit

00:00:41,449 --> 00:00:46,129
prior to BCG I was at McKinsey and a

00:00:44,510 --> 00:00:47,890
couple of other consulting firms and I

00:00:46,129 --> 00:00:50,030
have a background in economics and

00:00:47,890 --> 00:00:52,640
engineering which is why you'll see the

00:00:50,030 --> 00:00:56,140
crux of my talk is the intersection of

00:00:52,640 --> 00:00:59,120
those two disciplines and a few others

00:00:56,140 --> 00:01:02,710
so we're gonna talk about something that

00:00:59,120 --> 00:01:05,720
I coined called symbiotic evolution and

00:01:02,710 --> 00:01:08,149
building socially acceptable AI and the

00:01:05,720 --> 00:01:09,530
reason why this is important at github

00:01:08,149 --> 00:01:12,649
and why we're here today talking about

00:01:09,530 --> 00:01:16,789
this is because you will see towards the

00:01:12,649 --> 00:01:19,340
end of my talk that I think the over 50

00:01:16,789 --> 00:01:21,770
million developers at github and that

00:01:19,340 --> 00:01:25,340
use github have the opportunity to

00:01:21,770 --> 00:01:27,860
either make a place for ethical AI or

00:01:25,340 --> 00:01:29,990
just let it go and and have us continue

00:01:27,860 --> 00:01:35,090
to evolve as we are and we'll see what

00:01:29,990 --> 00:01:38,479
that means all right so we're gonna talk

00:01:35,090 --> 00:01:41,500
about humans and machines so humans are

00:01:38,479 --> 00:01:45,849
irrational and machines are rational

00:01:41,500 --> 00:01:49,009
humans are clumsy and machines are exact

00:01:45,849 --> 00:01:52,539
humans are thoughtful and machines are

00:01:49,009 --> 00:01:58,220
calculated humans are original and

00:01:52,539 --> 00:02:02,420
machines are repetitive but humans and

00:01:58,220 --> 00:02:04,849
machines are both biased so something's

00:02:02,420 --> 00:02:07,310
wrong there right you would think that

00:02:04,849 --> 00:02:11,660
the other side would say machines are

00:02:07,310 --> 00:02:14,800
not biased but as we all know we

00:02:11,660 --> 00:02:18,280
inevitably as human beings are biased in

00:02:14,800 --> 00:02:21,290
everything we do and every walk we take

00:02:18,280 --> 00:02:23,989
every thought we make we are built and

00:02:21,290 --> 00:02:26,480
bred by our social construct by our

00:02:23,989 --> 00:02:29,390
culture by our society and by our

00:02:26,480 --> 00:02:29,940
anthropology and so waking up tomorrow

00:02:29,390 --> 00:02:32,840
more

00:02:29,940 --> 00:02:35,520
and say we're gonna build unbiased AI is

00:02:32,840 --> 00:02:38,480
simply not what in my opinion I believe

00:02:35,520 --> 00:02:38,480
is the right answer

00:02:39,320 --> 00:02:46,170
we've son this song before we used to

00:02:43,170 --> 00:02:50,400
have currency s paper and we introduced

00:02:46,170 --> 00:02:52,680
credit and we said Oh amazing we're now

00:02:50,400 --> 00:02:53,460
gonna offer the world credit it's gonna

00:02:52,680 --> 00:02:56,550
be amazing

00:02:53,460 --> 00:02:59,550
and part of it was we went from people

00:02:56,550 --> 00:03:02,280
living in rural areas to living in urban

00:02:59,550 --> 00:03:04,530
areas we went from people making smaller

00:03:02,280 --> 00:03:09,810
incomes to making more incomes we sped

00:03:04,530 --> 00:03:15,360
up the economy but as we know credit has

00:03:09,810 --> 00:03:20,310
also harmed us in numerous ways and it

00:03:15,360 --> 00:03:22,880
continues to do so today let's see if I

00:03:20,310 --> 00:03:22,880
can get my

00:03:29,349 --> 00:03:37,989
so here's an article from vice 20:19

00:03:33,840 --> 00:03:42,370
about now how algorithms are making

00:03:37,989 --> 00:03:44,859
credit more biased there are about six

00:03:42,370 --> 00:03:47,799
other articles like this on how the

00:03:44,859 --> 00:03:51,849
credit system is still fraught with bias

00:03:47,799 --> 00:03:53,950
but wait we have AI right and it's a

00:03:51,849 --> 00:03:57,219
machine that's calculated so why is it

00:03:53,950 --> 00:04:02,219
creating bias because it's using data

00:03:57,219 --> 00:04:02,219
from history that is biased

00:04:07,800 --> 00:04:19,590
give me a second here all right so we

00:04:16,170 --> 00:04:23,940
are building AI that reinforces a world

00:04:19,590 --> 00:04:26,550
we live in today not the world we want

00:04:23,940 --> 00:04:28,650
tomorrow so we all know I won't go into

00:04:26,550 --> 00:04:31,050
politics it's a heated topic but the

00:04:28,650 --> 00:04:32,790
world we are in today it's not exactly a

00:04:31,050 --> 00:04:35,100
world we want our children and our

00:04:32,790 --> 00:04:38,160
grandchildren to live in and we are

00:04:35,100 --> 00:04:39,930
still evolving as a society right but we

00:04:38,160 --> 00:04:41,850
kind of know what we want to do we kind

00:04:39,930 --> 00:04:45,750
of know how we want to evolve Society

00:04:41,850 --> 00:04:47,760
but we need to symbiotically build our

00:04:45,750 --> 00:04:50,070
machines as we evolve we're not going to

00:04:47,760 --> 00:04:52,410
wake up tomorrow and have zero bias but

00:04:50,070 --> 00:04:53,850
we are building our machines to do so so

00:04:52,410 --> 00:04:56,040
the question I asked the group in the

00:04:53,850 --> 00:04:58,830
room is if a machine was completely

00:04:56,040 --> 00:05:01,980
rational tomorrow and had to live in a

00:04:58,830 --> 00:05:08,040
world with billions of irrational humans

00:05:01,980 --> 00:05:10,530
what would happen maybe but I don't want

00:05:08,040 --> 00:05:14,910
to be that gloom and doom autonomous AI

00:05:10,530 --> 00:05:17,010
doesn't understand humanity we can talk

00:05:14,910 --> 00:05:19,500
about Tesla and we can talk about cars

00:05:17,010 --> 00:05:21,140
and and all that sort of stuff right but

00:05:19,500 --> 00:05:23,820
we all know that this is simply the case

00:05:21,140 --> 00:05:28,110
the machines simply don't understand how

00:05:23,820 --> 00:05:31,530
we how we react to things but there's a

00:05:28,110 --> 00:05:36,270
positive to this as engineers and as

00:05:31,530 --> 00:05:38,310
developers we have a kind of we need to

00:05:36,270 --> 00:05:39,900
do this right so we need to talk to

00:05:38,310 --> 00:05:41,250
economists we need to talk to

00:05:39,900 --> 00:05:44,330
anthropologists and we need to

00:05:41,250 --> 00:05:47,370
understand how what we are building

00:05:44,330 --> 00:05:50,910
impacts the way we think the way we work

00:05:47,370 --> 00:05:53,040
the way we act the way we react and how

00:05:50,910 --> 00:05:55,250
the data were putting into all of our

00:05:53,040 --> 00:05:58,410
systems and into all of our code is

00:05:55,250 --> 00:06:00,480
representative of society today now some

00:05:58,410 --> 00:06:02,610
people will tell you remove all the bias

00:06:00,480 --> 00:06:05,220
from the data it's not what I'm saying

00:06:02,610 --> 00:06:07,370
I'm saying understand how the bias

00:06:05,220 --> 00:06:09,630
you're bringing into that data is

00:06:07,370 --> 00:06:11,190
affecting the system you're building and

00:06:09,630 --> 00:06:14,070
how much you might need to pull that

00:06:11,190 --> 00:06:15,540
back little by little and start using

00:06:14,070 --> 00:06:18,150
the engineering and the wonderful

00:06:15,540 --> 00:06:20,810
products we are building to educate a

00:06:18,150 --> 00:06:28,900
better society and not force it into

00:06:20,810 --> 00:06:31,610
submission so your role as creators is

00:06:28,900 --> 00:06:34,250
to slowly and progressively build

00:06:31,610 --> 00:06:38,630
systems that understand us as humans and

00:06:34,250 --> 00:06:41,090
don't fight with us we can't fight it

00:06:38,630 --> 00:06:42,830
overnight right and the thing I like to

00:06:41,090 --> 00:06:44,930
use here it made me I was thinking about

00:06:42,830 --> 00:06:47,380
this the other day is the Constitution

00:06:44,930 --> 00:06:50,240
right the Constitution was created

00:06:47,380 --> 00:06:51,770
hundreds and hundreds of years ago but

00:06:50,240 --> 00:06:54,229
when they wrote it they said well we're

00:06:51,770 --> 00:06:55,850
not gonna be around for long and we kind

00:06:54,229 --> 00:07:00,560
of want this thing to survive for like

00:06:55,850 --> 00:07:03,970
you know 10 20 30 years maybe thousands

00:07:00,560 --> 00:07:07,370
of years so how do we build something

00:07:03,970 --> 00:07:09,500
that goes way beyond something we could

00:07:07,370 --> 00:07:11,870
ever think of but in a way that makes it

00:07:09,500 --> 00:07:14,450
flexible for 5 years from now for it to

00:07:11,870 --> 00:07:18,050
work 10 years from now 20 years from now

00:07:14,450 --> 00:07:20,389
30 years from now I have yet to see a

00:07:18,050 --> 00:07:23,150
thought process like this I've seen a

00:07:20,389 --> 00:07:25,910
lot of people say everything is unbiased

00:07:23,150 --> 00:07:26,510
everything is biased remove ai ai is in

00:07:25,910 --> 00:07:28,820
danger

00:07:26,510 --> 00:07:31,250
ai is not good AI is great the world's

00:07:28,820 --> 00:07:34,430
gonna you know more jobs all the jobs I

00:07:31,250 --> 00:07:37,340
mean all sorts of weird you know extra

00:07:34,430 --> 00:07:39,260
extremes and what I'm asking for is to

00:07:37,340 --> 00:07:40,940
find a happy middle because what will

00:07:39,260 --> 00:07:43,160
eventually happen which is what happens

00:07:40,940 --> 00:07:45,169
with a lot of technologies is people get

00:07:43,160 --> 00:07:47,990
a bit of a nausea to the buzzword right

00:07:45,169 --> 00:07:49,930
ten years from now ai is gonna be I mean

00:07:47,990 --> 00:07:53,810
what was it was a great was it not great

00:07:49,930 --> 00:07:55,760
caused a bit of a mess look at credit

00:07:53,810 --> 00:07:57,740
right so I think we have a real

00:07:55,760 --> 00:08:00,260
opportunity we're at an incredible

00:07:57,740 --> 00:08:02,479
inflection point right now going into a

00:08:00,260 --> 00:08:04,490
new decade where we can really start to

00:08:02,479 --> 00:08:06,650
think about how we as an entire society

00:08:04,490 --> 00:08:09,440
and community come together and build

00:08:06,650 --> 00:08:11,270
thoughtful true artificial intelligence

00:08:09,440 --> 00:08:15,110
applications not what people think is

00:08:11,270 --> 00:08:20,840
artificial intelligence and and come as

00:08:15,110 --> 00:08:24,260
a community together so I'm leaving some

00:08:20,840 --> 00:08:28,669
room open for Q&A but what I want to

00:08:24,260 --> 00:08:31,610
leave you guys with is there are over 50

00:08:28,669 --> 00:08:33,349
million people write writing code and

00:08:31,610 --> 00:08:34,969
those are the ones we know at github

00:08:33,349 --> 00:08:38,659
this is something we

00:08:34,969 --> 00:08:40,759
very carefully of at BCG join us in

00:08:38,659 --> 00:08:44,720
fighting the AI status quo right don't

00:08:40,759 --> 00:08:46,850
follow the the streams of folks that are

00:08:44,720 --> 00:08:49,490
out there but really take a deep deep

00:08:46,850 --> 00:08:51,920
thought into what it is that we're

00:08:49,490 --> 00:08:54,500
building and how we are going to live

00:08:51,920 --> 00:08:56,660
with it in a society slowly and

00:08:54,500 --> 00:08:57,980
progressively and evolve that way and

00:08:56,660 --> 00:09:01,519
not just try to change everything

00:08:57,980 --> 00:09:03,970
overnight so I'll open it up for Q&A and

00:09:01,519 --> 00:09:03,970
leave there

00:09:11,910 --> 00:09:16,889
knowing it's a hot topic I know yeah go

00:09:14,800 --> 00:09:16,889
ahead

00:09:18,779 --> 00:09:23,900
hello

00:09:20,940 --> 00:09:26,840
so if there are 15 million developers

00:09:23,900 --> 00:09:30,290
terror against corporations that have

00:09:26,840 --> 00:09:33,420
infinity amount of resources hadith

00:09:30,290 --> 00:09:36,360
imagine we are going to compete against

00:09:33,420 --> 00:09:38,400
these companies that's an excellent

00:09:36,360 --> 00:09:40,620
question so I'm I'm in a corporation

00:09:38,400 --> 00:09:42,270
right there are developers in the room

00:09:40,620 --> 00:09:48,120
here that belong to the to the same

00:09:42,270 --> 00:09:49,440
corporation at the end of the day if we

00:09:48,120 --> 00:09:51,830
don't do this the right way those

00:09:49,440 --> 00:09:55,110
corporations are gonna lose ton of money

00:09:51,830 --> 00:09:58,800
simple fact right they're gonna make a

00:09:55,110 --> 00:10:00,630
ton of bad decisions right so if you

00:09:58,800 --> 00:10:02,220
think about an automotive company right

00:10:00,630 --> 00:10:04,170
if an automotive company does not

00:10:02,220 --> 00:10:06,630
understand what autonomous AI is gonna

00:10:04,170 --> 00:10:08,430
do or how its gonna disrupt their space

00:10:06,630 --> 00:10:10,290
in a very thoughtful way they're gonna

00:10:08,430 --> 00:10:13,080
lose a ton of money so you're you're

00:10:10,290 --> 00:10:16,290
right and they're actually I'll take a

00:10:13,080 --> 00:10:17,610
step back they're also in a lot of legal

00:10:16,290 --> 00:10:19,260
risk so I don't know if you guys saw the

00:10:17,610 --> 00:10:23,100
UnitedHealthcare story who's seen that

00:10:19,260 --> 00:10:26,460
one yet yeah okay so they just got

00:10:23,100 --> 00:10:32,010
they're getting heavily heavily sued for

00:10:26,460 --> 00:10:34,470
bias and an algorithm right so now whose

00:10:32,010 --> 00:10:35,970
fault really was it was it the engineers

00:10:34,470 --> 00:10:38,040
fault writing the algorithm who knows

00:10:35,970 --> 00:10:39,690
right but the idea that's why I think

00:10:38,040 --> 00:10:42,000
we're at a point where people will

00:10:39,690 --> 00:10:45,960
listen because this is stuff that sea

00:10:42,000 --> 00:10:47,400
levels are are curious about afraid

00:10:45,960 --> 00:10:49,050
about right what is this going to do

00:10:47,400 --> 00:10:50,670
fundamentally to their business I think

00:10:49,050 --> 00:10:52,380
there's a moment right now where people

00:10:50,670 --> 00:10:57,770
will actually listen to the technical

00:10:52,380 --> 00:10:57,770
community yeah

00:10:59,029 --> 00:11:04,260
hey actually part of the united

00:11:02,700 --> 00:11:06,839
corporation so i know what you're

00:11:04,260 --> 00:11:09,930
talking about my question to you is is

00:11:06,839 --> 00:11:11,880
there some sort of a manifesto kind of

00:11:09,930 --> 00:11:16,130
mentality that we need to go through as

00:11:11,880 --> 00:11:20,459
a collective to have this basically push

00:11:16,130 --> 00:11:22,589
the regulations and laws everybody knows

00:11:20,459 --> 00:11:26,579
that right it's out in the open that DB

00:11:22,589 --> 00:11:28,680
are the laws are not at all in update

00:11:26,579 --> 00:11:31,139
with ready exactly with the technology

00:11:28,680 --> 00:11:33,510
but what kind of manifesto could a

00:11:31,139 --> 00:11:37,019
community create that is working day and

00:11:33,510 --> 00:11:38,970
night with the AI learning stuff along

00:11:37,019 --> 00:11:41,490
with pushing this change which is super

00:11:38,970 --> 00:11:45,300
super impactful to the end customer and

00:11:41,490 --> 00:11:47,130
and in everyday life yeah so do you guys

00:11:45,300 --> 00:11:50,279
all know about the open AI initiative

00:11:47,130 --> 00:11:54,089
right it's good it's not a manifesto I

00:11:50,279 --> 00:11:56,880
think you are spot on there is I mean I

00:11:54,089 --> 00:11:59,040
think you talk to Microsoft right that

00:11:56,880 --> 00:12:01,769
now owns github right you talk to Amazon

00:11:59,040 --> 00:12:03,959
I think they're looking for this isn't

00:12:01,769 --> 00:12:05,940
an easy manifesto to write right this is

00:12:03,959 --> 00:12:07,980
what I mean I think yes it has to be the

00:12:05,940 --> 00:12:10,560
community but it cannot be a community

00:12:07,980 --> 00:12:12,779
of just engineers right economists know

00:12:10,560 --> 00:12:15,390
this inside and out they know exactly

00:12:12,779 --> 00:12:18,360
what caused the market crash right they

00:12:15,390 --> 00:12:20,490
know what impact credit has they know

00:12:18,360 --> 00:12:20,730
what impacts everything else in the

00:12:20,490 --> 00:12:22,199
world

00:12:20,730 --> 00:12:23,610
anthropologist understand how we

00:12:22,199 --> 00:12:25,740
communicate with each other and how we

00:12:23,610 --> 00:12:27,990
why putting a laptop in front of us

00:12:25,740 --> 00:12:30,360
changes the way we we interact as humans

00:12:27,990 --> 00:12:32,490
right so I do think we have to write a

00:12:30,360 --> 00:12:34,470
manifesto I do think we need to do it

00:12:32,490 --> 00:12:36,779
soon I do think we have to do it as a

00:12:34,470 --> 00:12:39,240
collective I'm gonna try to spur some

00:12:36,779 --> 00:12:44,040
interest right I think I need help from

00:12:39,240 --> 00:12:46,290
a lot of other people I I struggle to

00:12:44,040 --> 00:12:48,660
find another way we're we're five years

00:12:46,290 --> 00:12:52,199
from now and have a I mean have a bunch

00:12:48,660 --> 00:12:54,269
of algorithms just making decisions that

00:12:52,199 --> 00:12:56,370
we assume are better decisions we can't

00:12:54,269 --> 00:13:02,750
backtrack in that sort of scenario but

00:12:56,370 --> 00:13:02,750
yeah so long story short I think so yeah

00:13:07,010 --> 00:13:12,300
all right well think about it

00:13:10,260 --> 00:13:14,610
think long and hard about the stuff that

00:13:12,300 --> 00:13:16,950
you're coding and then how and how the

00:13:14,610 --> 00:13:18,630
world is changing and and hopefully

00:13:16,950 --> 00:13:20,580
you'll see me tweeting and instagramming

00:13:18,630 --> 00:13:22,380
and all that jazz so hopefully you'll

00:13:20,580 --> 00:13:25,070
you'll join up with the effort thank you

00:13:22,380 --> 00:13:25,070
very much everyone

00:13:25,620 --> 00:13:54,049

YouTube URL: https://www.youtube.com/watch?v=oFjffeGcbV4


