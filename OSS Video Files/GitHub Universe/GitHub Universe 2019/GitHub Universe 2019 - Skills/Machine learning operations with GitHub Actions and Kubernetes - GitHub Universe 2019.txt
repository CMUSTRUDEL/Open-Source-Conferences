Title: Machine learning operations with GitHub Actions and Kubernetes - GitHub Universe 2019
Publication date: 2019-12-13
Playlist: GitHub Universe 2019 - Skills
Description: 
	Presented by:
Hamel Husain, Staff Machine Learning Engineer at GitHub
Jeremy Lewi, Software Engineer at Google

From automating mundane tasks to reducing inefficiencies in developers’ workflows, machine learning has the potential to scale your team’s results like never before. However, the practice of deploying machine learning for enterprises is relatively new. In this talk, Hamel and Jeremy will demonstrate how GitHub Actions and Kubernetes can be used to orchestrate machine learning workflows in new ways that increase transparency and reliability of these applications. By borrowing best practices and technologies from DevOps, they’ll help you learn how to deploy machine learning solutions with confidence.

About GitHub Universe:
GitHub Universe is a two-day conference dedicated to the creativity and curiosity of the largest software community in the world. Sessions cover topics from team culture to open source software across industries and technologies.

For more information on GitHub Universe, check the website:
https://githubuniverse.com
Captions: 
	00:00:00,620 --> 00:00:14,210
[Music]

00:00:17,949 --> 00:00:23,119
for coming out today to hear us talk

00:00:20,750 --> 00:00:25,849
about mo ops using github actions and

00:00:23,119 --> 00:00:28,399
kubernetes my name is Jeremy levy I'm a

00:00:25,849 --> 00:00:30,410
tech lead at Google I work on coop flow

00:00:28,399 --> 00:00:32,480
which is an open source kubernetes

00:00:30,410 --> 00:00:34,760
native platform for machine learning and

00:00:32,480 --> 00:00:36,860
I'll let my co-speaker introduce himself

00:00:34,760 --> 00:00:40,519
hi my name is Hamill I'm a machine

00:00:36,860 --> 00:00:42,710
learning engineer i github alright so

00:00:40,519 --> 00:00:45,230
let's get started so when most people

00:00:42,710 --> 00:00:47,540
hear about AI what they think about is a

00:00:45,230 --> 00:00:50,089
lot of very high profile in very

00:00:47,540 --> 00:00:52,519
exciting success stories in terms of AI

00:00:50,089 --> 00:00:55,670
so you have self-driving cars like way

00:00:52,519 --> 00:00:57,619
mo you have alphago in which a computer

00:00:55,670 --> 00:00:59,659
program beat the world's best go player

00:00:57,619 --> 00:01:03,170
you have everyday applications like

00:00:59,659 --> 00:01:05,119
google translate but when when ml

00:01:03,170 --> 00:01:07,159
engineers and data scientists for in the

00:01:05,119 --> 00:01:09,850
field think about AI they have a very

00:01:07,159 --> 00:01:13,189
different experience and life story so

00:01:09,850 --> 00:01:14,840
this Twitter quote has been receiving a

00:01:13,189 --> 00:01:16,759
lot of attention in the ML infra and

00:01:14,840 --> 00:01:19,250
data science community because it really

00:01:16,759 --> 00:01:21,770
sums up pretty nicely the sentiment and

00:01:19,250 --> 00:01:25,100
feeling of a data scientist on a day to

00:01:21,770 --> 00:01:26,900
day basis and the idea is basically data

00:01:25,100 --> 00:01:29,420
scientists are able to rapidly and

00:01:26,900 --> 00:01:32,530
quickly develop a model and prove that

00:01:29,420 --> 00:01:35,360
this model is able to improve some

00:01:32,530 --> 00:01:37,010
important business metric and then they

00:01:35,360 --> 00:01:39,110
would try to get that into production or

00:01:37,010 --> 00:01:40,970
into some application and that's where

00:01:39,110 --> 00:01:42,500
they still run into roadblocks so they

00:01:40,970 --> 00:01:44,900
end up spending months trying to get

00:01:42,500 --> 00:01:48,440
this into production and oftentimes they

00:01:44,900 --> 00:01:50,600
fail and what we've learned at Google is

00:01:48,440 --> 00:01:53,299
that one of the reasons that these

00:01:50,600 --> 00:01:55,070
systems is so difficult to get ml into

00:01:53,299 --> 00:01:57,020
production is that when you're building

00:01:55,070 --> 00:01:59,540
an ml application you're actually

00:01:57,020 --> 00:02:01,760
building a complex distributed system so

00:01:59,540 --> 00:02:03,860
this is a famous diagram from a paper

00:02:01,760 --> 00:02:06,590
that Google published called the hidden

00:02:03,860 --> 00:02:08,149
technical debt in ml systems and the key

00:02:06,590 --> 00:02:11,450
idea is that most people tend to think

00:02:08,149 --> 00:02:13,370
about ml as being about complex machine

00:02:11,450 --> 00:02:15,860
learning algorithms and difficult math

00:02:13,370 --> 00:02:18,620
but in reality this ends up being only a

00:02:15,860 --> 00:02:21,080
small portion of your ml application or

00:02:18,620 --> 00:02:23,470
system in reality what you have is a lot

00:02:21,080 --> 00:02:25,640
of systems and infrastructure that

00:02:23,470 --> 00:02:28,370
surrounds that it is necessary to

00:02:25,640 --> 00:02:29,650
support that ml algorithm so for example

00:02:28,370 --> 00:02:31,730
you need

00:02:29,650 --> 00:02:34,430
infrastructure to collect data and

00:02:31,730 --> 00:02:36,319
pre-process it extract features you need

00:02:34,430 --> 00:02:38,060
to clean the data and make sure that

00:02:36,319 --> 00:02:40,190
it's valid and you can use it in

00:02:38,060 --> 00:02:42,230
production you have to manage all the

00:02:40,190 --> 00:02:44,300
resources like to compute in order to do

00:02:42,230 --> 00:02:45,709
both the training and the survey once

00:02:44,300 --> 00:02:47,600
you get your model into production you

00:02:45,709 --> 00:02:50,690
have all of the usual ops challenges

00:02:47,600 --> 00:02:53,090
monitoring logging auditing etc and so

00:02:50,690 --> 00:02:54,980
in reality most of your time and energy

00:02:53,090 --> 00:02:56,690
goes into to all these surrounding

00:02:54,980 --> 00:02:58,489
systems and infrastructure and that's

00:02:56,690 --> 00:02:59,810
what makes getting an imbalance of

00:02:58,489 --> 00:03:03,050
production so difficult and so

00:02:59,810 --> 00:03:04,850
challenging so one of the things that

00:03:03,050 --> 00:03:07,040
we've learned at Google dealing with

00:03:04,850 --> 00:03:10,430
complex distributed systems is that if

00:03:07,040 --> 00:03:11,900
you deploy more often this often reduces

00:03:10,430 --> 00:03:14,390
the likelihood that problems will occur

00:03:11,900 --> 00:03:16,489
and there also reduces the severity of

00:03:14,390 --> 00:03:18,380
the problems when they do happen so this

00:03:16,489 --> 00:03:21,110
has been backed up by research so

00:03:18,380 --> 00:03:22,700
there's a DevOps research report called

00:03:21,110 --> 00:03:25,610
the Dora report that comes out every

00:03:22,700 --> 00:03:27,500
year and in 2019 the data basically

00:03:25,610 --> 00:03:29,750
confirmed this conclusion that if you

00:03:27,500 --> 00:03:32,300
deploy more often you will all have less

00:03:29,750 --> 00:03:35,120
problems and this is a key principle of

00:03:32,300 --> 00:03:36,799
Google I sorry's so the quote on the

00:03:35,120 --> 00:03:38,810
bottom right is from a paper that Google

00:03:36,799 --> 00:03:41,540
SR is published and it's basically about

00:03:38,810 --> 00:03:43,010
how important getting to automation that

00:03:41,540 --> 00:03:45,799
allows you to do things like push on

00:03:43,010 --> 00:03:49,370
green is in order to maintain some of

00:03:45,799 --> 00:03:51,920
the most complex systems at Google so

00:03:49,370 --> 00:03:53,660
for traditional applications continuous

00:03:51,920 --> 00:03:56,630
integration and continuous deployment

00:03:53,660 --> 00:03:58,010
has a very rich ecosystem so we have

00:03:56,630 --> 00:04:00,049
this cycle where we want to do

00:03:58,010 --> 00:04:01,850
development you know create make a

00:04:00,049 --> 00:04:04,340
change to our application like our web

00:04:01,850 --> 00:04:06,860
application push that to our source repo

00:04:04,340 --> 00:04:09,470
then build it test it and deploy it and

00:04:06,860 --> 00:04:11,959
so we have a very rich tool chain for

00:04:09,470 --> 00:04:14,870
supporting that so if for example we can

00:04:11,959 --> 00:04:17,359
develop our code in vs code we have that

00:04:14,870 --> 00:04:19,160
lots of integrations in vs code to do

00:04:17,359 --> 00:04:22,729
things like automatically build our code

00:04:19,160 --> 00:04:25,850
using a tool chain like maven or basil

00:04:22,729 --> 00:04:28,130
we then have integrations for gluing the

00:04:25,850 --> 00:04:30,350
steps together so you have plug-ins 4 vs

00:04:28,130 --> 00:04:32,060
code for things like git and finally you

00:04:30,350 --> 00:04:34,630
have off-the-shelf platforms like

00:04:32,060 --> 00:04:38,390
Jenkins that can put this all together

00:04:34,630 --> 00:04:40,430
for ML C ICD is similar but it's

00:04:38,390 --> 00:04:41,990
difference and these differences create

00:04:40,430 --> 00:04:43,220
challenges which are preventing us from

00:04:41,990 --> 00:04:46,190
having having

00:04:43,220 --> 00:04:48,410
rich toolchain as of yet to help us to

00:04:46,190 --> 00:04:51,410
see ICD from machine learning so as an

00:04:48,410 --> 00:04:54,290
example with with with with ml we have

00:04:51,410 --> 00:04:56,120
new tools as an example a lot of data

00:04:54,290 --> 00:04:58,040
scientists and ml engineers are using

00:04:56,120 --> 00:04:59,920
Jupiter which is a note booking

00:04:58,040 --> 00:05:01,820
environment to develop their models

00:04:59,920 --> 00:05:03,950
because it provides really rich

00:05:01,820 --> 00:05:06,710
visualizations and is really great for

00:05:03,950 --> 00:05:09,080
interactive data analysis but this is

00:05:06,710 --> 00:05:10,760
these toolchain right look Jupiter lacks

00:05:09,080 --> 00:05:12,800
some of the integrations that we expect

00:05:10,760 --> 00:05:14,780
from our existing tool train so as an

00:05:12,800 --> 00:05:16,430
example we don't really have great tools

00:05:14,780 --> 00:05:19,100
with Jupiter to automatically build

00:05:16,430 --> 00:05:21,350
artifacts like containers that we would

00:05:19,100 --> 00:05:23,750
typically want to put into our CI CD

00:05:21,350 --> 00:05:26,150
pipeline to automate the deployment and

00:05:23,750 --> 00:05:28,670
getting them into production similarly

00:05:26,150 --> 00:05:30,680
when we comes to testing models we have

00:05:28,670 --> 00:05:32,630
new challenges so when it comes to

00:05:30,680 --> 00:05:34,070
traditional web applications we have a

00:05:32,630 --> 00:05:36,080
pretty good handle on how we go about

00:05:34,070 --> 00:05:38,690
deploying that application and then

00:05:36,080 --> 00:05:39,890
testing it by sending an RPC to make

00:05:38,690 --> 00:05:41,990
sure that we get the desired response

00:05:39,890 --> 00:05:43,490
and if we get the desired response we

00:05:41,990 --> 00:05:45,920
know that the code is good and we can

00:05:43,490 --> 00:05:47,570
push it into production but evaluating a

00:05:45,920 --> 00:05:49,400
model is a little bit different because

00:05:47,570 --> 00:05:52,310
what we care about is whether that model

00:05:49,400 --> 00:05:54,230
is actually improving ma the performance

00:05:52,310 --> 00:05:56,360
and quality of the predictions and that

00:05:54,230 --> 00:05:57,620
can be a lot harder to evaluate the

00:05:56,360 --> 00:05:59,480
first thing you have to do is you have

00:05:57,620 --> 00:06:00,980
to train the model and that can take a

00:05:59,480 --> 00:06:03,260
lot of time and it can also be

00:06:00,980 --> 00:06:05,690
computationally expensive the other

00:06:03,260 --> 00:06:07,850
problem is that often times when you

00:06:05,690 --> 00:06:10,010
have a model like ranked a ranking model

00:06:07,850 --> 00:06:11,750
or a recommendation model it can be

00:06:10,010 --> 00:06:13,940
difficult to evaluate the performance of

00:06:11,750 --> 00:06:16,700
that model offline you may have to

00:06:13,940 --> 00:06:18,530
deploy that model into production test

00:06:16,700 --> 00:06:20,390
it on a subset of traffic and see

00:06:18,530 --> 00:06:22,340
whether it's actually driving more

00:06:20,390 --> 00:06:26,390
traffic to see whether it's improving

00:06:22,340 --> 00:06:28,460
quality so because of all these

00:06:26,390 --> 00:06:32,270
challenges we don't really have a really

00:06:28,460 --> 00:06:34,160
great tool chain yet for doing CI cd44

00:06:32,270 --> 00:06:36,890
ml and as a result this lack of

00:06:34,160 --> 00:06:38,270
automation causes problems and Hamill's

00:06:36,890 --> 00:06:39,919
going to walk us through an example to

00:06:38,270 --> 00:06:45,260
illustrate how the lack of automation

00:06:39,919 --> 00:06:47,570
causes problems thanks Jeremy so I just

00:06:45,260 --> 00:06:50,750
want to show you an example a concrete

00:06:47,570 --> 00:06:53,330
example of what happens because of this

00:06:50,750 --> 00:06:56,060
lack of DevOps for machine learning or

00:06:53,330 --> 00:06:56,639
more concretely CI C D for machine

00:06:56,060 --> 00:07:00,419
learning

00:06:56,639 --> 00:07:04,520
this is a real PR from the coup flow

00:07:00,419 --> 00:07:07,830
project and this is really indicative of

00:07:04,520 --> 00:07:11,520
the PR that you will see on any machine

00:07:07,830 --> 00:07:14,240
learning project it's very typical and

00:07:11,520 --> 00:07:16,830
this specific person opened the PR and

00:07:14,240 --> 00:07:20,310
they said they're changing some

00:07:16,830 --> 00:07:23,009
parameters of a model so changing the

00:07:20,310 --> 00:07:24,569
size of a default the size of a hidden

00:07:23,009 --> 00:07:27,539
layer changing the learning rate

00:07:24,569 --> 00:07:28,469
strategy that took the type of model so

00:07:27,539 --> 00:07:32,550
on and so forth

00:07:28,469 --> 00:07:34,560
and later on in this PR someone makes a

00:07:32,550 --> 00:07:36,870
comment and says that these new

00:07:34,560 --> 00:07:39,419
parameters and things you changed that

00:07:36,870 --> 00:07:41,039
increase the performance of the model

00:07:39,419 --> 00:07:44,699
and so in this case precision and recall

00:07:41,039 --> 00:07:50,580
and the original author responds and

00:07:44,699 --> 00:07:53,849
says yes I think so and the PR gets

00:07:50,580 --> 00:07:56,009
merged so this is really problematic for

00:07:53,849 --> 00:07:57,870
a traditional software kind of

00:07:56,009 --> 00:08:02,190
development lifecycle we don't want to

00:07:57,870 --> 00:08:06,719
guess whether a code should be merged or

00:08:02,190 --> 00:08:09,060
not so you might typically see on github

00:08:06,719 --> 00:08:11,789
when using a CI CD you have integration

00:08:09,060 --> 00:08:12,360
test smoke tests a unit test whatever

00:08:11,789 --> 00:08:14,639
they might be

00:08:12,360 --> 00:08:17,699
and you have these robust systems that

00:08:14,639 --> 00:08:20,159
help you monitor that so you don't have

00:08:17,699 --> 00:08:24,800
to guess like I just showed you in that

00:08:20,159 --> 00:08:27,810
PR and so we want to bring that same

00:08:24,800 --> 00:08:30,419
paradigm to machine learning so that you

00:08:27,810 --> 00:08:32,909
can take the guesswork out and then

00:08:30,419 --> 00:08:34,979
there's a lot of other things so you

00:08:32,909 --> 00:08:36,690
want to be able to block PRS from being

00:08:34,979 --> 00:08:38,880
merged in production for example if the

00:08:36,690 --> 00:08:40,890
test didn't pass or if the test is not

00:08:38,880 --> 00:08:45,329
even run which we also want to bring

00:08:40,890 --> 00:08:48,209
into machine learning so what can we do

00:08:45,329 --> 00:08:51,149
about this so I'm going to show you a

00:08:48,209 --> 00:08:54,690
demo of how we can use github actions

00:08:51,149 --> 00:08:57,230
and coop flow which is a cloud native

00:08:54,690 --> 00:09:00,180
machine learning infrastructure project

00:08:57,230 --> 00:09:03,720
to to orchestrate a machine learning CI

00:09:00,180 --> 00:09:05,959
CD workflow and but I'm gonna show you

00:09:03,720 --> 00:09:08,830
is we're gonna start with opening up er

00:09:05,959 --> 00:09:10,830
then we're gonna build the environment

00:09:08,830 --> 00:09:14,440
we're gonna train the model on

00:09:10,830 --> 00:09:16,570
kubernetes using coop flow and we're

00:09:14,440 --> 00:09:19,750
gonna collect metrics about the models

00:09:16,570 --> 00:09:22,209
performance and then we will ultimately

00:09:19,750 --> 00:09:24,850
deploy that model and then along the way

00:09:22,209 --> 00:09:27,399
we will report back status of all of

00:09:24,850 --> 00:09:29,470
these steps into the PR so that it's

00:09:27,399 --> 00:09:35,560
transparent and it's easy to understand

00:09:29,470 --> 00:09:41,170
the the end result so I'm gonna go ahead

00:09:35,560 --> 00:09:46,839
and show a demo so what I have here is a

00:09:41,170 --> 00:09:49,839
PR example and in this PR I'm changing

00:09:46,839 --> 00:09:51,370
some parameters of a model so if you

00:09:49,839 --> 00:09:53,110
look at this file I'm changing I'm

00:09:51,370 --> 00:09:55,480
increasing the size of an embedding

00:09:53,110 --> 00:09:57,399
later from 50 to 85 the details are not

00:09:55,480 --> 00:10:01,690
important but the idea is I'm changing

00:09:57,399 --> 00:10:03,519
something about this model and for

00:10:01,690 --> 00:10:05,860
example one thing that you may want to

00:10:03,519 --> 00:10:08,230
do differently from traditional CI CD is

00:10:05,860 --> 00:10:10,480
you might not want to trigger a full run

00:10:08,230 --> 00:10:13,209
of your model on every push because

00:10:10,480 --> 00:10:15,610
machine learning can be very compute

00:10:13,209 --> 00:10:18,880
intensive and time intensive you might

00:10:15,610 --> 00:10:20,620
want a human to give you give the final

00:10:18,880 --> 00:10:25,470
word on when they're ready to run the

00:10:20,620 --> 00:10:28,540
full test so using github actions we can

00:10:25,470 --> 00:10:32,050
orchestrate something like a chat ops so

00:10:28,540 --> 00:10:35,079
I'm gonna issue a command here run full

00:10:32,050 --> 00:10:38,110
test slash run full test make a comment

00:10:35,079 --> 00:10:41,620
and that will trigger Gift of actions to

00:10:38,110 --> 00:10:43,990
start this process so what you'll see

00:10:41,620 --> 00:10:46,480
next is we'll start building the

00:10:43,990 --> 00:10:49,690
environment so I'm gonna click on the

00:10:46,480 --> 00:10:52,089
actions tab and then you'll see this

00:10:49,690 --> 00:10:53,949
workflow and the first thing we're doing

00:10:52,089 --> 00:10:57,699
is we're building the containers that

00:10:53,949 --> 00:11:00,220
contain the this code in this PR and

00:10:57,699 --> 00:11:01,660
we're gonna use that to then run the

00:11:00,220 --> 00:11:04,810
machine learning workflow on the

00:11:01,660 --> 00:11:06,100
infrastructure of our choice so in this

00:11:04,810 --> 00:11:07,870
case we're building the containers

00:11:06,100 --> 00:11:12,000
they're being pushed to the docker

00:11:07,870 --> 00:11:16,570
registry that we have and then we're

00:11:12,000 --> 00:11:18,610
building the kubernetes spec that will

00:11:16,570 --> 00:11:20,960
then deploy emitting this machine

00:11:18,610 --> 00:11:26,240
learning pipeline

00:11:20,960 --> 00:11:28,040
to Kufa low to get run and so if you

00:11:26,240 --> 00:11:30,890
notice the the context for this is

00:11:28,040 --> 00:11:33,260
dropped into the PR so actions build a

00:11:30,890 --> 00:11:35,300
container and it submitted the job to

00:11:33,260 --> 00:11:38,000
coop flow to run this machine learning

00:11:35,300 --> 00:11:40,100
pipeline I'm not going to go into a lot

00:11:38,000 --> 00:11:42,290
of detail right now about how that works

00:11:40,100 --> 00:11:46,690
Jeremy is gonna tell you more about coop

00:11:42,290 --> 00:11:50,540
flow but this is this is sort of a

00:11:46,690 --> 00:11:53,420
machine learning workflow system that

00:11:50,540 --> 00:11:57,529
runs on coop flow again this is

00:11:53,420 --> 00:11:59,720
available from the PR and as you notice

00:11:57,529 --> 00:12:02,360
we created a checker on so this yellow

00:11:59,720 --> 00:12:06,320
dot here is going to stay open until the

00:12:02,360 --> 00:12:08,589
end of the machine learning job so

00:12:06,320 --> 00:12:13,760
eventually this job will keep running

00:12:08,589 --> 00:12:15,649
the the training will complete and it'll

00:12:13,760 --> 00:12:17,180
clear that check run now keep in mind

00:12:15,649 --> 00:12:19,370
this is running on the infrastructure of

00:12:17,180 --> 00:12:22,040
your choice this was in this specific

00:12:19,370 --> 00:12:23,360
example this is running on gke but this

00:12:22,040 --> 00:12:25,100
can run anywhere this go-around on cloud

00:12:23,360 --> 00:12:26,839
any cloud of your choice on any

00:12:25,100 --> 00:12:29,089
infrastructure of your choice and when

00:12:26,839 --> 00:12:30,890
the model is complete when the training

00:12:29,089 --> 00:12:34,160
is complete it'll clear that Chuck run

00:12:30,890 --> 00:12:36,080
so now that check run is complete and

00:12:34,160 --> 00:12:37,580
you might notice there's some more

00:12:36,080 --> 00:12:40,279
information that has been dropped into

00:12:37,580 --> 00:12:42,890
the PR so while the model was training

00:12:40,279 --> 00:12:44,779
we logged a lot of statistics to an

00:12:42,890 --> 00:12:47,300
experiment tracking system in this case

00:12:44,779 --> 00:12:49,310
we're using weights and biases and that

00:12:47,300 --> 00:12:51,110
experiment tracking system captures lots

00:12:49,310 --> 00:12:54,260
of information about model performance

00:12:51,110 --> 00:12:57,230
and other system metrics that you might

00:12:54,260 --> 00:13:00,230
want to see what happens is when the

00:12:57,230 --> 00:13:03,050
model is done it queries that and brings

00:13:00,230 --> 00:13:04,760
into the PR so this is what weights and

00:13:03,050 --> 00:13:07,760
biases looks like you can use any

00:13:04,760 --> 00:13:10,370
experiment tracking system you want but

00:13:07,760 --> 00:13:12,709
just to give you an idea this is a

00:13:10,370 --> 00:13:14,089
system where you can log very detailed

00:13:12,709 --> 00:13:17,899
metrics about your machine learning

00:13:14,089 --> 00:13:19,010
models training run things like a lot of

00:13:17,899 --> 00:13:22,390
things that are locked to standard out

00:13:19,010 --> 00:13:26,899
and you see here various model artifacts

00:13:22,390 --> 00:13:29,000
so again this is dropped into the PR so

00:13:26,899 --> 00:13:32,379
all runs that correspond to this Shaw

00:13:29,000 --> 00:13:37,149
that this PR is run on

00:13:32,379 --> 00:13:38,829
is dropped into this PR and it's dropped

00:13:37,149 --> 00:13:40,509
in as markdown and you can customize

00:13:38,829 --> 00:13:43,930
this actually all of these steps are

00:13:40,509 --> 00:13:45,970
available as actions that you can use on

00:13:43,930 --> 00:13:49,300
the marketplace which I'll show you how

00:13:45,970 --> 00:13:52,360
to get to a little bit later and you can

00:13:49,300 --> 00:13:54,790
verify that this run is not stale that

00:13:52,360 --> 00:13:58,180
corresponds to the the latest Shah on

00:13:54,790 --> 00:13:59,709
the head of this branch and so you might

00:13:58,180 --> 00:14:01,509
take this information and you might

00:13:59,709 --> 00:14:03,910
it'll allow you to have a discussion

00:14:01,509 --> 00:14:07,300
about the models performance and it

00:14:03,910 --> 00:14:09,790
allows you to have a transparent record

00:14:07,300 --> 00:14:12,129
of everything that's related to this

00:14:09,790 --> 00:14:14,800
model in this PR and really takes out a

00:14:12,129 --> 00:14:18,519
lot of the guesswork that you otherwise

00:14:14,800 --> 00:14:20,199
might have so finally you might want to

00:14:18,519 --> 00:14:22,120
deploy this model so we can use chat ops

00:14:20,199 --> 00:14:25,240
again so you see I'm going to take this

00:14:22,120 --> 00:14:31,769
run ID and then use chat ops to deploy

00:14:25,240 --> 00:14:35,529
this model and this specific demo is

00:14:31,769 --> 00:14:37,480
linked with the github deployment API so

00:14:35,529 --> 00:14:40,990
we can report status on how this model

00:14:37,480 --> 00:14:42,759
is deployed now using this demo this is

00:14:40,990 --> 00:14:46,990
being deployed to Google cloud functions

00:14:42,759 --> 00:14:48,370
which is a lambda like service and this

00:14:46,990 --> 00:14:49,930
is just to drive home the fact that you

00:14:48,370 --> 00:14:51,670
can have various heterogeneous

00:14:49,930 --> 00:14:54,220
deployments using actions because

00:14:51,670 --> 00:14:56,350
actions can allow you to string together

00:14:54,220 --> 00:15:00,069
whatever infrastructure that you want

00:14:56,350 --> 00:15:04,029
and then you see that this has been

00:15:00,069 --> 00:15:07,660
reported back to the PRS so let me go

00:15:04,029 --> 00:15:10,360
back a little bit so this shows that

00:15:07,660 --> 00:15:12,399
model being deployed and then when it's

00:15:10,360 --> 00:15:14,350
done being deployed it will report back

00:15:12,399 --> 00:15:17,800
success or failure of deployment into

00:15:14,350 --> 00:15:20,290
the PR so that you can go and see if

00:15:17,800 --> 00:15:22,480
this code was deployed and when it was

00:15:20,290 --> 00:15:27,639
deployed and you can have a record of it

00:15:22,480 --> 00:15:30,970
all within the PR and this is just the

00:15:27,639 --> 00:15:32,559
dashboard of the this particular model

00:15:30,970 --> 00:15:34,839
being deployed to Google Cloud functions

00:15:32,559 --> 00:15:37,300
so this is really exciting we showed

00:15:34,839 --> 00:15:38,860
here in this example kind of an

00:15:37,300 --> 00:15:41,410
end-to-end workflow where you have

00:15:38,860 --> 00:15:43,389
machine learning CI CD model is running

00:15:41,410 --> 00:15:45,970
on infrastructure of your choice in this

00:15:43,389 --> 00:15:47,860
case we're using GPUs

00:15:45,970 --> 00:15:49,240
on the machine learning infrastructure

00:15:47,860 --> 00:15:51,130
of your choice and you can compose

00:15:49,240 --> 00:15:54,310
various github actions to achieve that

00:15:51,130 --> 00:15:57,850
and that's really new and exciting that

00:15:54,310 --> 00:16:00,430
is gonna allow more transparency for

00:15:57,850 --> 00:16:06,640
people working on machine learning and

00:16:00,430 --> 00:16:08,529
that are using github so you can see the

00:16:06,640 --> 00:16:13,390
example of this code by going to this

00:16:08,529 --> 00:16:17,950
link bitly ml - ops if you want to kind

00:16:13,390 --> 00:16:20,170
of dig in more so I'll give it back to

00:16:17,950 --> 00:16:22,470
Jeremy so he can talk about coup flow

00:16:20,170 --> 00:16:24,790
and drilling into that a little bit more

00:16:22,470 --> 00:16:27,310
Thank You Hemel so now we're gonna talk

00:16:24,790 --> 00:16:29,800
about how we go about building that CI

00:16:27,310 --> 00:16:32,680
CD workflow that it Hamel just demoed by

00:16:29,800 --> 00:16:34,209
using coop flow and github actions so

00:16:32,680 --> 00:16:37,300
let's just let's start off by talking

00:16:34,209 --> 00:16:39,250
about what we need to do ml for CI CD

00:16:37,300 --> 00:16:41,950
for ML there's really four things we

00:16:39,250 --> 00:16:43,899
need we need tools and applications that

00:16:41,950 --> 00:16:45,459
allow us to develop our models that

00:16:43,899 --> 00:16:47,290
includes editing the code as well as

00:16:45,459 --> 00:16:49,149
training our models we also need

00:16:47,290 --> 00:16:50,800
applications that we can use to deploy

00:16:49,149 --> 00:16:53,050
our models and create you know serving

00:16:50,800 --> 00:16:55,480
endpoints that we can hit with HTTP or G

00:16:53,050 --> 00:16:57,190
RPC requests the second thing that we

00:16:55,480 --> 00:16:59,080
need is we need to the third thing is

00:16:57,190 --> 00:17:00,990
tools for automation right so we need

00:16:59,080 --> 00:17:03,850
that PR workflow where we can

00:17:00,990 --> 00:17:05,740
automatically trigger those training and

00:17:03,850 --> 00:17:08,290
deployment steps like camel showed in

00:17:05,740 --> 00:17:09,670
response to actions by the user and so

00:17:08,290 --> 00:17:11,800
for that we're going to be using github

00:17:09,670 --> 00:17:13,750
actions and then finally what they'll

00:17:11,800 --> 00:17:15,880
need is we often want to have a manual

00:17:13,750 --> 00:17:18,250
review process where humans decide

00:17:15,880 --> 00:17:21,130
whether a model is actually improving

00:17:18,250 --> 00:17:22,900
performance and therefore decide to push

00:17:21,130 --> 00:17:24,790
it into production and for that we're

00:17:22,900 --> 00:17:28,120
going to use github and the standard

00:17:24,790 --> 00:17:30,309
review and approval process so let's

00:17:28,120 --> 00:17:33,580
start with coop flows so what we started

00:17:30,309 --> 00:17:35,770
to see about two years ago with an ml

00:17:33,580 --> 00:17:37,660
space was that companies that were

00:17:35,770 --> 00:17:38,920
really successful with machine learning

00:17:37,660 --> 00:17:40,300
and we're really on the cutting edge we

00:17:38,920 --> 00:17:41,860
started talking a lot about the

00:17:40,300 --> 00:17:43,870
platforms they were building in

00:17:41,860 --> 00:17:45,670
particular to solve this problem about

00:17:43,870 --> 00:17:47,980
how to continuously integrate and deploy

00:17:45,670 --> 00:17:49,420
models into production so Google was one

00:17:47,980 --> 00:17:51,309
of the first and we started talking

00:17:49,420 --> 00:17:52,510
about tf-x which was our internal

00:17:51,309 --> 00:17:54,400
machine learning system which we've

00:17:52,510 --> 00:17:56,380
started to open-source and one of the

00:17:54,400 --> 00:17:58,450
key problems that tf-x was designed to

00:17:56,380 --> 00:18:00,850
solve was continually we reach

00:17:58,450 --> 00:18:02,260
your models as new data arrived in

00:18:00,850 --> 00:18:04,570
pushing those models into production

00:18:02,260 --> 00:18:06,610
there are other companies also started

00:18:04,570 --> 00:18:08,950
talking a lot about the platform's they

00:18:06,610 --> 00:18:10,540
were building lift was one of the big

00:18:08,950 --> 00:18:12,340
ones they talked about a platform called

00:18:10,540 --> 00:18:14,350
lift learn which enables their data

00:18:12,340 --> 00:18:16,120
scientists to rapidly develop models

00:18:14,350 --> 00:18:17,980
using jupiter notebooks and then push

00:18:16,120 --> 00:18:20,260
those into production so a lot of other

00:18:17,980 --> 00:18:21,400
companies like Bloomberg stripe it

00:18:20,260 --> 00:18:22,990
Airbnb

00:18:21,400 --> 00:18:24,310
have been talking about the platform's

00:18:22,990 --> 00:18:25,990
they've been building and how they've

00:18:24,310 --> 00:18:28,960
been building these platforms on top of

00:18:25,990 --> 00:18:31,270
open source applications like 10 0x tree

00:18:28,960 --> 00:18:33,670
boost and onyx and then also on top of

00:18:31,270 --> 00:18:35,860
kubernetes so we decided to start

00:18:33,670 --> 00:18:37,660
building coop flow as an open source

00:18:35,860 --> 00:18:40,180
kubernetes native platform permission

00:18:37,660 --> 00:18:43,240
learning so that all the companies that

00:18:40,180 --> 00:18:45,490
wanted to use and needed an ml platform

00:18:43,240 --> 00:18:47,260
but didn't have the resources to build

00:18:45,490 --> 00:18:49,780
their own platform could also get the

00:18:47,260 --> 00:18:51,250
advantages and begin to use ml

00:18:49,780 --> 00:18:54,040
successfully just like these bigger

00:18:51,250 --> 00:18:55,870
companies so our mission with coop flow

00:18:54,040 --> 00:18:57,910
is to make it easy for everyone to

00:18:55,870 --> 00:18:59,230
develop deploy and manage portable

00:18:57,910 --> 00:19:02,320
distributed machine learning on

00:18:59,230 --> 00:19:04,630
kubernetes we are an entirely open

00:19:02,320 --> 00:19:06,760
source project we have an incredible

00:19:04,630 --> 00:19:08,320
amount of momentum so we have over 30

00:19:06,760 --> 00:19:10,390
companies that are contributing and

00:19:08,320 --> 00:19:13,360
building on top of coop flow and we have

00:19:10,390 --> 00:19:15,550
an amazing number of PRS and unique

00:19:13,360 --> 00:19:18,280
authors every month so I think we're up

00:19:15,550 --> 00:19:23,290
to over 80 unique PR contributors every

00:19:18,280 --> 00:19:25,780
every 28 days or so so getting back to

00:19:23,290 --> 00:19:27,780
the CI CD workflow the first step is

00:19:25,780 --> 00:19:29,800
developing your models with coop flow is

00:19:27,780 --> 00:19:31,470
developing your models and we're going

00:19:29,800 --> 00:19:35,200
to show you how cupola makes that easy

00:19:31,470 --> 00:19:37,450
so as we said before jupiter is becoming

00:19:35,200 --> 00:19:39,790
one of the standard tools in the data

00:19:37,450 --> 00:19:42,220
scientist toolbox we had a panel a

00:19:39,790 --> 00:19:43,930
couple years ago if the cupola Summit

00:19:42,220 --> 00:19:45,730
and we asked a bunch of data scientists

00:19:43,930 --> 00:19:47,740
what is the tool you most can't live

00:19:45,730 --> 00:19:48,310
without and they all said notebooks in

00:19:47,740 --> 00:19:49,960
Jupiter

00:19:48,310 --> 00:19:53,590
so with coop flow we've made it super

00:19:49,960 --> 00:19:55,330
easy to spin up notebooks on on

00:19:53,590 --> 00:19:57,730
kubernetes we provide you this very

00:19:55,330 --> 00:19:59,890
simple web UI where you can launch a new

00:19:57,730 --> 00:20:01,990
notebook and then begin to use that to

00:19:59,890 --> 00:20:04,090
develop your model but as we showed

00:20:01,990 --> 00:20:06,160
before one of the big challenges with

00:20:04,090 --> 00:20:08,050
Jupiter notebooks is how do you go about

00:20:06,160 --> 00:20:10,660
converting those notebooks into

00:20:08,050 --> 00:20:11,830
artifacts like dr. containers that you

00:20:10,660 --> 00:20:14,679
can begin to in degree

00:20:11,830 --> 00:20:16,809
into a CI CD workflow and so for that

00:20:14,679 --> 00:20:19,480
income flow we've developed fairing and

00:20:16,809 --> 00:20:22,480
fairing as an SDK that makes it easy to

00:20:19,480 --> 00:20:25,809
convert notebooks into docker containers

00:20:22,480 --> 00:20:28,230
and so the idea is that fairing is

00:20:25,809 --> 00:20:30,519
divided into three steps you have

00:20:28,230 --> 00:20:32,559
preprocessors and what the preprocessor

00:20:30,519 --> 00:20:34,630
does is it basically takes your jupiter

00:20:32,559 --> 00:20:36,880
notebook which is really just a json

00:20:34,630 --> 00:20:39,010
file and converts that to a python file

00:20:36,880 --> 00:20:40,899
and it basically does that just by sort

00:20:39,010 --> 00:20:42,460
of stripping out the markdown and then

00:20:40,899 --> 00:20:45,039
allow you to tell it using various

00:20:42,460 --> 00:20:47,289
filters and other kinds of syntax what

00:20:45,039 --> 00:20:48,429
sells to include and what sells to

00:20:47,289 --> 00:20:50,350
remove so you can remove your

00:20:48,429 --> 00:20:52,090
visualizations or some of the other

00:20:50,350 --> 00:20:54,340
cells that you don't need in your

00:20:52,090 --> 00:20:56,950
production artifacts and once you've

00:20:54,340 --> 00:20:59,950
produced that Python file Farren

00:20:56,950 --> 00:21:02,919
provides a builder which makes it easy

00:20:59,950 --> 00:21:05,230
to build the docker container from that

00:21:02,919 --> 00:21:07,240
Python file and so it can do things like

00:21:05,230 --> 00:21:09,669
generate a pitfall of requirements file

00:21:07,240 --> 00:21:13,360
to install your Python dependencies then

00:21:09,669 --> 00:21:14,679
finally once you have that container we

00:21:13,360 --> 00:21:17,559
actually need to deploy that on

00:21:14,679 --> 00:21:19,240
kubernetes and so for that Ferenc

00:21:17,559 --> 00:21:22,000
provides deployers which allow you to

00:21:19,240 --> 00:21:24,130
easily create kubernetes resources using

00:21:22,000 --> 00:21:26,470
that docker container that you've built

00:21:24,130 --> 00:21:28,720
and so the the idea here is that by

00:21:26,470 --> 00:21:30,340
building this sdk we've provided the

00:21:28,720 --> 00:21:31,720
integration for you so you can integrate

00:21:30,340 --> 00:21:35,380
your development step with the

00:21:31,720 --> 00:21:37,059
downstream steps so coop flow provides a

00:21:35,380 --> 00:21:39,399
variety of ways that you can actually

00:21:37,059 --> 00:21:42,250
execute these ml workloads on kubernetes

00:21:39,399 --> 00:21:44,230
and that's what these different types of

00:21:42,250 --> 00:21:47,500
applications can be used as the deployer

00:21:44,230 --> 00:21:49,090
is with Ferenc so the first one is you

00:21:47,500 --> 00:21:51,490
typically want to train your model and

00:21:49,090 --> 00:21:54,100
so for that kubernetes provides a bunch

00:21:51,490 --> 00:21:56,110
of different applications called custom

00:21:54,100 --> 00:21:57,760
resources that make it really easy to

00:21:56,110 --> 00:22:00,340
train your model using different

00:21:57,760 --> 00:22:03,070
frameworks like tensor flow or pi torch

00:22:00,340 --> 00:22:04,570
or extra boost and the idea here is that

00:22:03,070 --> 00:22:06,700
we give you a very simple declarative

00:22:04,570 --> 00:22:09,789
spec so you can give us a very simple

00:22:06,700 --> 00:22:11,590
yeah mph specification that focuses on

00:22:09,789 --> 00:22:13,630
the high-level information that you as a

00:22:11,590 --> 00:22:15,370
data scientist want to provide so in

00:22:13,630 --> 00:22:17,260
particular you just have to specify the

00:22:15,370 --> 00:22:18,789
image which is the docker container that

00:22:17,260 --> 00:22:20,559
contains your code and then you would

00:22:18,789 --> 00:22:22,840
have to specify the number of workers

00:22:20,559 --> 00:22:25,299
that you want to use if you want to run

00:22:22,840 --> 00:22:27,549
your kit your job just

00:22:25,299 --> 00:22:30,820
so you can take advantage of training

00:22:27,549 --> 00:22:32,559
large models in parallel and so this

00:22:30,820 --> 00:22:34,509
simplifies the management process

00:22:32,559 --> 00:22:37,570
because the data scientists no longer

00:22:34,509 --> 00:22:39,429
has to manage individual resources or

00:22:37,570 --> 00:22:41,559
multiple containers they just have to

00:22:39,429 --> 00:22:44,950
spend create this simple kubernetes

00:22:41,559 --> 00:22:46,600
specification another big challenge in

00:22:44,950 --> 00:22:49,029
machine learning is hyper parameter

00:22:46,600 --> 00:22:50,350
tuning a lot of times machine learning

00:22:49,029 --> 00:22:52,749
models or deep learning models have

00:22:50,350 --> 00:22:55,299
hyper parameters that the only way to

00:22:52,749 --> 00:22:56,919
train them is to try models with a bunch

00:22:55,299 --> 00:22:59,139
of different values and see which one

00:22:56,919 --> 00:23:00,730
performs best so a simple example with

00:22:59,139 --> 00:23:02,830
deep learning is you often have the

00:23:00,730 --> 00:23:04,509
number of different layers to use in

00:23:02,830 --> 00:23:06,909
your model and the only way to see which

00:23:04,509 --> 00:23:09,970
what the desire best value is is to try

00:23:06,909 --> 00:23:11,919
a bunch of different values so with the

00:23:09,970 --> 00:23:13,869
hyper parameter tuning support and cube

00:23:11,919 --> 00:23:15,730
flow you can again just create a very

00:23:13,869 --> 00:23:18,519
simple declarative specification in

00:23:15,730 --> 00:23:19,840
which you specify what algorithm you

00:23:18,519 --> 00:23:22,299
want to use to tune your hyper

00:23:19,840 --> 00:23:26,379
parameters and then what the hemp prop

00:23:22,299 --> 00:23:28,210
parameters are and then the the

00:23:26,379 --> 00:23:30,369
application will take care of launching

00:23:28,210 --> 00:23:32,679
multiple training jobs waiting for those

00:23:30,369 --> 00:23:34,649
jobs to finish collect the metrics and

00:23:32,679 --> 00:23:36,850
then report back to you which model

00:23:34,649 --> 00:23:40,330
performs the best and what those desired

00:23:36,850 --> 00:23:42,519
parameters are finally you actually have

00:23:40,330 --> 00:23:45,159
to deploy the model and so for that we

00:23:42,519 --> 00:23:47,590
provide k-f serving and so km serving as

00:23:45,159 --> 00:23:49,989
an application where again one can

00:23:47,590 --> 00:23:53,049
provide a simple declarative spec where

00:23:49,989 --> 00:23:55,419
all one has to provide is the URI of

00:23:53,049 --> 00:23:58,059
your model so this could be a protocol

00:23:55,419 --> 00:24:00,159
buffer outputted by Tenzer flow and then

00:23:58,059 --> 00:24:02,830
under the hood KF serving takes care of

00:24:00,159 --> 00:24:04,269
all of the complex DevOps tasks or ml

00:24:02,830 --> 00:24:05,529
ops tasks that you would need to

00:24:04,269 --> 00:24:07,480
actually deploy this model into

00:24:05,529 --> 00:24:10,359
production so as an example of some of

00:24:07,480 --> 00:24:13,210
the complex DevOps challenges that KF

00:24:10,359 --> 00:24:14,919
serving handles for you is KF serving

00:24:13,210 --> 00:24:18,279
will automatically auto scale the number

00:24:14,919 --> 00:24:19,929
of instances your model has deployed so

00:24:18,279 --> 00:24:21,700
if you have a lot of requests coming in

00:24:19,929 --> 00:24:23,590
and you need to spin up more capacity

00:24:21,700 --> 00:24:26,289
it'll automatically add more containers

00:24:23,590 --> 00:24:28,600
and as the number of requests decrease

00:24:26,289 --> 00:24:30,279
it'll reduce the number of containers

00:24:28,600 --> 00:24:33,399
you have deployed so you're not wasting

00:24:30,279 --> 00:24:36,519
resources or money on compute you don't

00:24:33,399 --> 00:24:38,139
need it can also handle complex DevOps

00:24:36,519 --> 00:24:38,890
challenges like you know rolling out

00:24:38,139 --> 00:24:40,330
changes in

00:24:38,890 --> 00:24:43,510
doing things like Canaries and rollouts

00:24:40,330 --> 00:24:45,190
on the machine learning side KF surgeon

00:24:43,510 --> 00:24:48,220
provides a lot of advanced functionality

00:24:45,190 --> 00:24:50,080
as an example we provide model servers

00:24:48,220 --> 00:24:52,450
that can easily take a Python function

00:24:50,080 --> 00:24:54,640
that does inference and then wraps that

00:24:52,450 --> 00:24:56,410
with an RPC arrest server that

00:24:54,640 --> 00:24:59,170
integrates with things like sto to

00:24:56,410 --> 00:25:00,820
provide monitoring and metrics that you

00:24:59,170 --> 00:25:02,860
need if you're actually going to deploy

00:25:00,820 --> 00:25:04,810
this into production it can also handle

00:25:02,860 --> 00:25:06,940
things like inference graphs where you

00:25:04,810 --> 00:25:08,350
deploy two types of bottles such as an

00:25:06,940 --> 00:25:11,560
extra boost model and a deep learning

00:25:08,350 --> 00:25:13,270
model and then combines those results

00:25:11,560 --> 00:25:15,400
from both of those models to generate

00:25:13,270 --> 00:25:18,970
the final predictions and then finally

00:25:15,400 --> 00:25:20,140
can handle things like a be testing so

00:25:18,970 --> 00:25:23,380
once we have these different

00:25:20,140 --> 00:25:24,880
applications we actually have to tie

00:25:23,380 --> 00:25:26,860
them together into the complete workflow

00:25:24,880 --> 00:25:29,140
and that's where github and github

00:25:26,860 --> 00:25:30,760
actions comes into play and so to

00:25:29,140 --> 00:25:36,700
describe that piece of it I'm gonna hand

00:25:30,760 --> 00:25:39,640
it back over to Hamill thanks Jeremy so

00:25:36,700 --> 00:25:41,260
just to kind of circle back and just in

00:25:39,640 --> 00:25:44,530
case anybody doesn't know what github

00:25:41,260 --> 00:25:48,250
actions is yet probably everybody does

00:25:44,530 --> 00:25:50,260
at this point in the conference you can

00:25:48,250 --> 00:25:52,000
github actions makes a lot of this stuff

00:25:50,260 --> 00:25:54,370
very easily so in just in case you

00:25:52,000 --> 00:25:57,070
haven't seen get up actions you specify

00:25:54,370 --> 00:25:59,820
with the llamó file and the dot github

00:25:57,070 --> 00:26:03,940
slash workflow of directory of your repo

00:25:59,820 --> 00:26:06,400
you can trigger it on any github event

00:26:03,940 --> 00:26:08,920
and then you can also use other

00:26:06,400 --> 00:26:11,680
predefined workflows from other people

00:26:08,920 --> 00:26:14,950
without rewriting all the code so it

00:26:11,680 --> 00:26:17,590
it's very modular and composable in that

00:26:14,950 --> 00:26:19,200
sense and that makes it really good for

00:26:17,590 --> 00:26:22,150
machine learning ops because you can

00:26:19,200 --> 00:26:24,850
inherit other people's parts of other

00:26:22,150 --> 00:26:27,760
people's workflow without kind of

00:26:24,850 --> 00:26:29,470
redoing it rebuilding it and there's a

00:26:27,760 --> 00:26:31,810
large library of existing actions

00:26:29,470 --> 00:26:35,320
including some actions for machine

00:26:31,810 --> 00:26:38,290
learning ops so you can you can find

00:26:35,320 --> 00:26:40,930
those on the marketplace you can search

00:26:38,290 --> 00:26:42,760
for those and also they come up when

00:26:40,930 --> 00:26:45,790
you're building actions as well let me

00:26:42,760 --> 00:26:48,910
show you screen with that and if you go

00:26:45,790 --> 00:26:51,460
to the bitly link bitly /ml ops with the

00:26:48,910 --> 00:26:52,210
link i shared earlier you'll see a list

00:26:51,460 --> 00:26:55,510
of

00:26:52,210 --> 00:26:58,890
actions that can help facilitate ML ops

00:26:55,510 --> 00:27:00,970
to get you started the great part is

00:26:58,890 --> 00:27:02,890
again this is these are all very

00:27:00,970 --> 00:27:04,540
composable so let's say you don't want

00:27:02,890 --> 00:27:07,150
to use argo or you don't even want to

00:27:04,540 --> 00:27:08,560
use weights and biases you can try it

00:27:07,150 --> 00:27:11,410
you can find another action that does

00:27:08,560 --> 00:27:16,780
that and compose your ml workflow in a

00:27:11,410 --> 00:27:19,600
different way and adding actions to your

00:27:16,780 --> 00:27:22,540
repo again that's done by specifying the

00:27:19,600 --> 00:27:24,880
llamo file and the workflows directory

00:27:22,540 --> 00:27:27,370
your dot github workflows directory and

00:27:24,880 --> 00:27:30,370
again you can search the marketplace and

00:27:27,370 --> 00:27:34,390
find actions that that are relevant to

00:27:30,370 --> 00:27:36,580
you so so that makes it makes this

00:27:34,390 --> 00:27:39,070
problem very tractable of kind of

00:27:36,580 --> 00:27:44,560
connecting your code to your various

00:27:39,070 --> 00:27:48,130
workflows ok and just to kind of wrap

00:27:44,560 --> 00:27:50,650
things up and kind of discuss what we

00:27:48,130 --> 00:27:53,550
talked about so again we talked about

00:27:50,650 --> 00:27:57,640
how we started from a pull request and

00:27:53,550 --> 00:28:00,610
then we build the environment from the

00:27:57,640 --> 00:28:03,850
code in that pull request specifically

00:28:00,610 --> 00:28:05,530
docker containers and then showed a way

00:28:03,850 --> 00:28:09,160
how we can orchestrate training of those

00:28:05,530 --> 00:28:11,440
models with github actions and reporting

00:28:09,160 --> 00:28:13,000
the status back to the PR and also at

00:28:11,440 --> 00:28:15,850
the same time collecting metrics about

00:28:13,000 --> 00:28:17,980
your model and reporting that back to

00:28:15,850 --> 00:28:20,620
the PR and then ultimately deploying the

00:28:17,980 --> 00:28:24,720
model and again up actions is a great

00:28:20,620 --> 00:28:24,720
way to orchestrate all of these things

00:28:25,800 --> 00:28:31,330
that's that's all we have for this talk

00:28:28,690 --> 00:28:33,730
but here are some resources that you

00:28:31,330 --> 00:28:37,650
might be interested in and we're also

00:28:33,730 --> 00:28:37,650
open to questions if you have any

00:28:38,160 --> 00:29:09,460
[Applause]

00:28:40,640 --> 00:29:09,460

YouTube URL: https://www.youtube.com/watch?v=Ll50l3fsoYs


