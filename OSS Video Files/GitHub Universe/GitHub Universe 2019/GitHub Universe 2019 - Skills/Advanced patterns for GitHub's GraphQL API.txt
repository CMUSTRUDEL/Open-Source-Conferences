Title: Advanced patterns for GitHub's GraphQL API
Publication date: 2019-12-13
Playlist: GitHub Universe 2019 - Skills
Description: 
	Presented by Rea Loretta, CEO at Toast

The GitHub API is a key part of accelerating workflows at scale. This session will leave you with tactical tips for how to paginate effectively, create and plan queries, use tech-preview features, and manage costs learned from years of practice and iteration at Toast and beyond.

About GitHub Universe:
GitHub Universe is a two-day conference dedicated to the creativity and curiosity of the largest software community in the world. Sessions cover topics from team culture to open source software across industries and technologies.

For more information on GitHub Universe, check the website:
https://githubuniverse.com
Captions: 
	00:00:00,580 --> 00:00:14,179
[Music]

00:00:17,260 --> 00:00:24,029
hi everyone welcome to advanced patterns

00:00:20,590 --> 00:00:27,160
for a github graph QL API I'm RIA

00:00:24,029 --> 00:00:29,500
co-founder and CEO of toast and I'm

00:00:27,160 --> 00:00:31,930
super thrilled to be here in front of

00:00:29,500 --> 00:00:33,660
all of you and that all of you want to

00:00:31,930 --> 00:00:36,820
take your grass fuel to the next level

00:00:33,660 --> 00:00:39,730
and I really hope that you'll enjoy this

00:00:36,820 --> 00:00:42,160
talk as much as I did making it

00:00:39,730 --> 00:00:45,120
and definitely tweet me if you have any

00:00:42,160 --> 00:00:48,970
questions I make sure to get back to you

00:00:45,120 --> 00:00:51,010
okay so I wanted to start off with some

00:00:48,970 --> 00:00:53,500
pre talk hype I know it's mid-afternoon

00:00:51,010 --> 00:00:56,770
and it's probably tiring you're probably

00:00:53,500 --> 00:00:59,050
craving in that pod so work with me I

00:00:56,770 --> 00:01:00,489
need some energy in the room here are

00:00:59,050 --> 00:01:02,559
some pictures of people who listen to

00:01:00,489 --> 00:01:05,560
this talk you'll become a master of

00:01:02,559 --> 00:01:07,840
graph QL you will manipulate code with

00:01:05,560 --> 00:01:09,729
just your mind you will put out

00:01:07,840 --> 00:01:13,920
production fires with a snap of your

00:01:09,729 --> 00:01:15,850
fingers and you will be a 10x engineer

00:01:13,920 --> 00:01:17,440
but you probably still won't understand

00:01:15,850 --> 00:01:20,410
ma nuts but that's ok

00:01:17,440 --> 00:01:21,960
it's another talk alright let's bring it

00:01:20,410 --> 00:01:25,800
back

00:01:21,960 --> 00:01:27,540
on a somewhat more serious note we know

00:01:25,800 --> 00:01:30,150
a lot of people would like better access

00:01:27,540 --> 00:01:32,880
to the wealth of data that getup has and

00:01:30,150 --> 00:01:36,150
it is after all a hub of all your team

00:01:32,880 --> 00:01:37,830
activity so my goal with this talk is to

00:01:36,150 --> 00:01:39,810
enable you to build your own data

00:01:37,830 --> 00:01:42,570
fetcher for github using the graphical

00:01:39,810 --> 00:01:44,550
API and then you can go crazy retrieve

00:01:42,570 --> 00:01:48,060
massive amounts of data and you know

00:01:44,550 --> 00:01:50,729
it's all good and also with open source

00:01:48,060 --> 00:01:53,910
our code into a reusable starter kit so

00:01:50,729 --> 00:01:56,550
everyone can play and I believe data

00:01:53,910 --> 00:01:59,759
should be freely accessible for everyone

00:01:56,550 --> 00:02:02,450
who is curious to learn so that's why

00:01:59,759 --> 00:02:05,090
it's really really important to me um

00:02:02,450 --> 00:02:07,290
disclaimer though don't use it for evil

00:02:05,090 --> 00:02:09,600
it's not a real license don't look it up

00:02:07,290 --> 00:02:11,820
I just want us to stay true to the

00:02:09,600 --> 00:02:14,470
Archimedean oath which is like the

00:02:11,820 --> 00:02:17,800
Hippocratic oath but for engineers

00:02:14,470 --> 00:02:19,510
so like other talks today have mentioned

00:02:17,800 --> 00:02:20,950
data is pretty amazing and does cool

00:02:19,510 --> 00:02:24,430
things like you can use it to identify

00:02:20,950 --> 00:02:27,370
bottlenecks and unblock your team this

00:02:24,430 --> 00:02:28,990
is good you should be doing this but

00:02:27,370 --> 00:02:31,360
data can also be taken out of context

00:02:28,990 --> 00:02:34,510
and used to make up arbitrary metrics to

00:02:31,360 --> 00:02:35,530
track engineer performance this is bad

00:02:34,510 --> 00:02:38,710
very bad

00:02:35,530 --> 00:02:40,630
and the consequences for misusing the

00:02:38,710 --> 00:02:43,300
content of this talk and open source

00:02:40,630 --> 00:02:46,890
library is I will personally be very

00:02:43,300 --> 00:02:50,110
disappointed in you let that sink in

00:02:46,890 --> 00:02:52,660
okay so now that we squared that bit

00:02:50,110 --> 00:02:54,700
away I want to provide context about

00:02:52,660 --> 00:02:59,140
myself and how I got interested in graph

00:02:54,700 --> 00:03:01,480
QL for starters toasts integrates with

00:02:59,140 --> 00:03:04,630
github and slack and notifies engineers

00:03:01,480 --> 00:03:06,730
when to unblock teammates and as you

00:03:04,630 --> 00:03:08,230
probably all know github has a lot of

00:03:06,730 --> 00:03:10,750
activities the bigger your team is the

00:03:08,230 --> 00:03:12,460
more activity so it's very important for

00:03:10,750 --> 00:03:14,680
us to not simply just pass everything

00:03:12,460 --> 00:03:17,290
through and because I would just create

00:03:14,680 --> 00:03:19,540
distractions for everyone so toast

00:03:17,290 --> 00:03:21,130
filters through this noise and delivers

00:03:19,540 --> 00:03:23,890
relevant notifications to the person

00:03:21,130 --> 00:03:26,430
needed for unblocking the team and we

00:03:23,890 --> 00:03:28,720
started out as this zero setup

00:03:26,430 --> 00:03:30,430
notification bot for individual

00:03:28,720 --> 00:03:32,230
contributors and since then we've

00:03:30,430 --> 00:03:34,209
evolved to empower the entire review

00:03:32,230 --> 00:03:36,850
process by allowing teammates to respond

00:03:34,209 --> 00:03:38,440
directly in slack and this past year

00:03:36,850 --> 00:03:40,300
we've just we've learned so much and

00:03:38,440 --> 00:03:42,520
we're continuously growing to meet

00:03:40,300 --> 00:03:44,769
demands of the team level and I'm sure

00:03:42,520 --> 00:03:49,150
you've all been in healthy teams and

00:03:44,769 --> 00:03:51,250
unhealthy teams me too as such a huge

00:03:49,150 --> 00:03:53,050
part of our ongoing work is learning the

00:03:51,250 --> 00:03:57,489
behaviors and patterns in these teams

00:03:53,050 --> 00:04:00,580
and alerting them on anomalies before

00:03:57,489 --> 00:04:03,580
they snowball and get out of control so

00:04:00,580 --> 00:04:05,500
to do this we learn from a variety of

00:04:03,580 --> 00:04:08,200
Oryx with different workflows and habits

00:04:05,500 --> 00:04:10,870
and we've needed to build out a robust

00:04:08,200 --> 00:04:12,310
analytics pipeline for this so in this

00:04:10,870 --> 00:04:13,690
talk I'll share some of the technical

00:04:12,310 --> 00:04:16,350
challenges that we face along the way

00:04:13,690 --> 00:04:19,269
and personally I have a hard time

00:04:16,350 --> 00:04:22,270
following super abstract talks so I

00:04:19,269 --> 00:04:24,450
thought it'd be nice to and fun to learn

00:04:22,270 --> 00:04:27,370
through building something meaningful

00:04:24,450 --> 00:04:30,180
so let's define the problem and the

00:04:27,370 --> 00:04:32,170
scope what data should we pull first

00:04:30,180 --> 00:04:34,300
arguably one of the most interesting

00:04:32,170 --> 00:04:36,130
aspects of github is code collaboration

00:04:34,300 --> 00:04:38,890
where all the learnings and drama

00:04:36,130 --> 00:04:40,900
happens so here's a familiar story we

00:04:38,890 --> 00:04:43,060
write some code we send it off for

00:04:40,900 --> 00:04:45,760
review it lands in the reviewers inbox

00:04:43,060 --> 00:04:47,170
and after some time they get to it if

00:04:45,760 --> 00:04:47,590
we're lucky they give us a meaningful

00:04:47,170 --> 00:04:51,010
feedback

00:04:47,590 --> 00:04:52,930
plus bonus knits and we get the review

00:04:51,010 --> 00:04:56,620
back in oh boy we're excited to make all

00:04:52,930 --> 00:04:58,240
these changes and grow as engineers and

00:04:56,620 --> 00:05:02,010
maybe we go through a few more rounds of

00:04:58,240 --> 00:05:04,770
commits re reviews more changes and

00:05:02,010 --> 00:05:07,600
eventually ends up like that right but

00:05:04,770 --> 00:05:09,820
in all seriousness pull request history

00:05:07,600 --> 00:05:11,830
tells a fascinating story and we can

00:05:09,820 --> 00:05:13,810
learn a lot from the timeline events the

00:05:11,830 --> 00:05:17,320
people involved and how quickly concerns

00:05:13,810 --> 00:05:20,680
were raised and resolved so this leads

00:05:17,320 --> 00:05:22,660
to fascinating insights now I'm not

00:05:20,680 --> 00:05:26,140
saying that you're going to be able to

00:05:22,660 --> 00:05:27,340
infer all of this magically but you can

00:05:26,140 --> 00:05:29,260
at least start looking through some data

00:05:27,340 --> 00:05:30,850
and then you can detect potential

00:05:29,260 --> 00:05:32,800
triggers that lead to a high-stress

00:05:30,850 --> 00:05:34,390
working environment or even get deeper

00:05:32,800 --> 00:05:36,490
insights like what is the best time of

00:05:34,390 --> 00:05:38,770
day to ask for a review or learn that

00:05:36,490 --> 00:05:40,450
large PRS are three times less likely to

00:05:38,770 --> 00:05:42,280
be approved within the first 24 hours

00:05:40,450 --> 00:05:44,230
with this you can encourage best

00:05:42,280 --> 00:05:47,950
practices and healthy habits for your

00:05:44,230 --> 00:05:50,220
team so let's look at our data shape at

00:05:47,950 --> 00:05:54,790
the root there's our organization and

00:05:50,220 --> 00:05:57,670
some repos and each repository has four

00:05:54,790 --> 00:06:00,250
requests y'all know this each BOE

00:05:57,670 --> 00:06:03,040
request has lists associated with with

00:06:00,250 --> 00:06:05,830
associated entities so such as timeline

00:06:03,040 --> 00:06:07,150
events reviews commits etc all of those

00:06:05,830 --> 00:06:09,460
these are the ones we thought were

00:06:07,150 --> 00:06:11,170
interesting so this is a good match for

00:06:09,460 --> 00:06:12,930
a graph he'll write in the shape of a

00:06:11,170 --> 00:06:15,540
graph

00:06:12,930 --> 00:06:16,919
we're at a talk about graph QL so

00:06:15,540 --> 00:06:19,080
naturally you're all here to learn about

00:06:16,919 --> 00:06:21,530
this but I still want to take some time

00:06:19,080 --> 00:06:24,720
and point out benefits of using Dracul

00:06:21,530 --> 00:06:27,090
so for one we can pull much more data

00:06:24,720 --> 00:06:30,810
with your round trips so this entire

00:06:27,090 --> 00:06:32,759
process is more efficient secondly some

00:06:30,810 --> 00:06:34,740
data is not easily accessible via the

00:06:32,759 --> 00:06:38,160
REST API but we can grab it with a graph

00:06:34,740 --> 00:06:40,680
Y API and as previously mentioned the

00:06:38,160 --> 00:06:43,110
graphical data model Maps conveniently

00:06:40,680 --> 00:06:47,130
to our mental model of the data shape so

00:06:43,110 --> 00:06:49,530
it's easier to reason about on top of

00:06:47,130 --> 00:06:52,050
this github graph QL API is one of the

00:06:49,530 --> 00:06:53,760
better graphical API eyes out there and

00:06:52,050 --> 00:06:56,220
they've really tried in my opinion to

00:06:53,760 --> 00:06:58,889
provide a good developer experience so

00:06:56,220 --> 00:07:00,900
that's in itself a good reason to try it

00:06:58,889 --> 00:07:03,090
out but if you're not really into any of

00:07:00,900 --> 00:07:05,880
these benefits then this talk is

00:07:03,090 --> 00:07:06,870
probably not very useful for you but I

00:07:05,880 --> 00:07:10,669
don't know maybe you're here for the

00:07:06,870 --> 00:07:10,669
doodles I'll be ok with that

00:07:10,880 --> 00:07:16,440
ok so we got our data data shape let's

00:07:13,470 --> 00:07:18,449
look at the schema next so the schema

00:07:16,440 --> 00:07:21,000
will describe our graph nodes and their

00:07:18,449 --> 00:07:23,310
relationships to each other and the good

00:07:21,000 --> 00:07:25,919
news is github scrath ql schema is

00:07:23,310 --> 00:07:28,560
structured exactly like we want so can

00:07:25,919 --> 00:07:31,139
we just fetch all the data it's not that

00:07:28,560 --> 00:07:32,970
simple so according to our schema we

00:07:31,139 --> 00:07:35,370
want to pull all the poll requests for

00:07:32,970 --> 00:07:37,830
the org if we want to do that we want to

00:07:35,370 --> 00:07:40,500
first pull all the repos and then all

00:07:37,830 --> 00:07:43,800
the associated PRS so a very simple

00:07:40,500 --> 00:07:47,039
query would look like this notice we

00:07:43,800 --> 00:07:49,770
specify the organization by name then

00:07:47,039 --> 00:07:52,470
the repositories with the argument first

00:07:49,770 --> 00:07:54,900
100 and this is our page size upper

00:07:52,470 --> 00:07:57,510
limit which specifies fetching only the

00:07:54,900 --> 00:08:00,090
first 100 repos which is fine for some

00:07:57,510 --> 00:08:01,870
orgs and we also request the first 100

00:08:00,090 --> 00:08:04,690
PRS for this repo

00:08:01,870 --> 00:08:09,160
and the repos I have more than 100 PRS

00:08:04,690 --> 00:08:11,590
are out of luck for now so coming back

00:08:09,160 --> 00:08:13,990
to our query there's an obvious problem

00:08:11,590 --> 00:08:17,169
with this since we're not fetching all

00:08:13,990 --> 00:08:19,389
the data we can get the first 100 but

00:08:17,169 --> 00:08:20,260
the volume of data that we want may be

00:08:19,389 --> 00:08:22,780
much larger

00:08:20,260 --> 00:08:25,479
how large well it's not uncommon to have

00:08:22,780 --> 00:08:26,530
thousands of PRS in one repo or more and

00:08:25,479 --> 00:08:28,020
for analytics we need to fetch

00:08:26,530 --> 00:08:31,590
everything to get that complete picture

00:08:28,020 --> 00:08:31,590
how do we tackle this

00:08:32,280 --> 00:08:36,719
so you probably all know that API s and

00:08:35,070 --> 00:08:39,240
database has solved this by allowing us

00:08:36,719 --> 00:08:42,659
to paginate and there are various types

00:08:39,240 --> 00:08:44,880
of pagination supported by graph QL but

00:08:42,659 --> 00:08:47,600
the most flexible one is cursor based

00:08:44,880 --> 00:08:49,920
pagination so here's a quick refresher

00:08:47,600 --> 00:08:52,620
let's represent the pull request we want

00:08:49,920 --> 00:08:53,640
to fetch with these squares and number

00:08:52,620 --> 00:08:56,960
them for convenience

00:08:53,640 --> 00:08:56,960
now let's line them up

00:08:58,240 --> 00:09:02,740
and as I mentioned previously that the

00:09:00,040 --> 00:09:04,840
max page size is a hundred so for the

00:09:02,740 --> 00:09:06,460
purposes of illustration let's just

00:09:04,840 --> 00:09:08,440
pretend we're gonna pull three at a time

00:09:06,460 --> 00:09:10,450
arbitrarily choose that page size of

00:09:08,440 --> 00:09:13,090
three so these will be the first three

00:09:10,450 --> 00:09:15,850
pour request that we fetch taking a look

00:09:13,090 --> 00:09:18,390
at our updated query we just update that

00:09:15,850 --> 00:09:20,800
number to three and also notice that we

00:09:18,390 --> 00:09:25,210
pull our specific repo by name to

00:09:20,800 --> 00:09:27,640
simplify our query so to start

00:09:25,210 --> 00:09:30,310
paginating we need to specify not only

00:09:27,640 --> 00:09:33,220
page size which is 3 but we also need a

00:09:30,310 --> 00:09:37,930
cursor at this point we don't yet have a

00:09:33,220 --> 00:09:40,520
cursor so let's leave that as null our

00:09:37,930 --> 00:09:43,200
return would look like this

00:09:40,520 --> 00:09:44,910
but we still want a cursor to get the

00:09:43,200 --> 00:09:46,620
cursor as part of our return we need to

00:09:44,910 --> 00:09:50,250
update our query to pull an object

00:09:46,620 --> 00:09:53,820
called page info that's what it is right

00:09:50,250 --> 00:09:57,750
there now we get a cursor along with

00:09:53,820 --> 00:09:59,580
poor requests and our result set nice so

00:09:57,750 --> 00:10:03,240
for context the results would look like

00:09:59,580 --> 00:10:05,910
this our cursor is just an opaque

00:10:03,240 --> 00:10:07,500
pointer it's called end cursor because

00:10:05,910 --> 00:10:10,530
it points to the last item in the data

00:10:07,500 --> 00:10:12,360
set we just fetched so that's what it

00:10:10,530 --> 00:10:15,540
looks like now it's pointing at PR

00:10:12,360 --> 00:10:19,230
number three in this case so for the

00:10:15,540 --> 00:10:21,780
second page we just repeat simple enough

00:10:19,230 --> 00:10:23,910
page slide is page size is still three

00:10:21,780 --> 00:10:26,730
and we provide the cursor that we got

00:10:23,910 --> 00:10:30,210
from page one and we use the after

00:10:26,730 --> 00:10:34,950
parameter this passing the cursor here's

00:10:30,210 --> 00:10:37,260
how our query looks like now so note the

00:10:34,950 --> 00:10:39,420
highlight the dollar sign this case a

00:10:37,260 --> 00:10:41,100
notes the query at the cursor variable

00:10:39,420 --> 00:10:43,410
that has to be passed into the query

00:10:41,100 --> 00:10:45,420
separately your implementation will

00:10:43,410 --> 00:10:47,070
depend entirely on your language and

00:10:45,420 --> 00:10:51,960
library of choice but as well it looks

00:10:47,070 --> 00:10:54,750
like for us as a result this query would

00:10:51,960 --> 00:10:59,280
fetch PRS four five and six and the new

00:10:54,750 --> 00:11:02,520
cursor which points to PR six now

00:10:59,280 --> 00:11:06,390
finally we can fetch our last two PRS by

00:11:02,520 --> 00:11:08,280
repeating this process cool like this so

00:11:06,390 --> 00:11:11,010
we notice notice that we're using our

00:11:08,280 --> 00:11:12,660
cursor from step two right there so it's

00:11:11,010 --> 00:11:14,430
pretty straightforward this whole path

00:11:12,660 --> 00:11:18,930
and now our cursor points a PR eight

00:11:14,430 --> 00:11:20,880
right this is our result how do we know

00:11:18,930 --> 00:11:22,230
we're at the end because we don't want

00:11:20,880 --> 00:11:25,080
to just keep fetching forever how do we

00:11:22,230 --> 00:11:26,820
know we need to stop fetching so in our

00:11:25,080 --> 00:11:29,460
return the list of PRS is contained

00:11:26,820 --> 00:11:32,220
within notes this notes object and the

00:11:29,460 --> 00:11:33,780
cursor is actually living in page info

00:11:32,220 --> 00:11:36,870
like we know already with a full name

00:11:33,780 --> 00:11:39,450
and cursor page info also has another

00:11:36,870 --> 00:11:41,520
property called has next page which

00:11:39,450 --> 00:11:43,050
behaves exactly as it sounds this

00:11:41,520 --> 00:11:45,720
property is true when there are more

00:11:43,050 --> 00:11:47,070
pages and we can keep fetching it's

00:11:45,720 --> 00:11:49,770
false when we fetched everything

00:11:47,070 --> 00:11:52,160
straight forward the updated final query

00:11:49,770 --> 00:11:54,949
would look like this

00:11:52,160 --> 00:11:58,279
right now we can paginate with cursors

00:11:54,949 --> 00:12:02,660
yay so I know what you're thinking

00:11:58,279 --> 00:12:04,759
you're probably like ria hi this is get

00:12:02,660 --> 00:12:06,259
up universe and you're on stage with

00:12:04,759 --> 00:12:08,899
your little microphone talking about

00:12:06,259 --> 00:12:12,980
pagination like it's some new cool thing

00:12:08,899 --> 00:12:18,980
what am i a baby developer nah nah man

00:12:12,980 --> 00:12:22,100
here I'm here for the advanced stuff so

00:12:18,980 --> 00:12:25,519
hold on yes it sounds absurdly simple so

00:12:22,100 --> 00:12:28,100
far but what makes it hard like why is

00:12:25,519 --> 00:12:30,649
this a basis of a talk at all well for

00:12:28,100 --> 00:12:33,740
one pagination alone is not enough to

00:12:30,649 --> 00:12:35,689
pull all the data we need pagination is

00:12:33,740 --> 00:12:38,089
a big part but there are pitfalls with

00:12:35,689 --> 00:12:40,339
nested queries and graph QL is all about

00:12:38,089 --> 00:12:43,910
nested queries we can't just padge me on

00:12:40,339 --> 00:12:45,410
any query we want why well meet this

00:12:43,910 --> 00:12:47,389
little guy from earlier you may have

00:12:45,410 --> 00:12:48,279
noticed him terrorizing my engineers in

00:12:47,389 --> 00:12:52,310
the opening slides

00:12:48,279 --> 00:12:54,769
he's a gotcha sore this little monster

00:12:52,310 --> 00:12:56,389
lives in every codebase architecture

00:12:54,769 --> 00:12:59,420
complex system in the nooks and crannies

00:12:56,389 --> 00:13:02,750
ready to jump out and delight you with

00:12:59,420 --> 00:13:05,089
all the gotchas he can find that's not

00:13:02,750 --> 00:13:07,490
terrible he's actually kind of cute to

00:13:05,089 --> 00:13:09,889
me but what can surprise you is that

00:13:07,490 --> 00:13:13,189
these gotchas can turn into much bigger

00:13:09,889 --> 00:13:16,069
blockers which can't be resolved without

00:13:13,189 --> 00:13:18,230
some serious consideration so let's take

00:13:16,069 --> 00:13:21,949
a look at the gotchas that are bigger

00:13:18,230 --> 00:13:24,800
than they seem so one of the biggest

00:13:21,949 --> 00:13:27,350
strengths of graph QL also presents one

00:13:24,800 --> 00:13:29,120
of the biggest technical challenges how

00:13:27,350 --> 00:13:31,189
do we pageant effectively when there are

00:13:29,120 --> 00:13:34,220
nodes on each nested level that we care

00:13:31,189 --> 00:13:36,230
about do we manage multiple cursors and

00:13:34,220 --> 00:13:38,950
pagin across every layer what's an

00:13:36,230 --> 00:13:41,350
effective strategy for that

00:13:38,950 --> 00:13:45,730
a range of other gotchas built on this

00:13:41,350 --> 00:13:47,860
one so if you were thinking of just

00:13:45,730 --> 00:13:49,420
making a massive API call you'll be hit

00:13:47,860 --> 00:13:52,600
with the nodes limit pretty quickly that

00:13:49,420 --> 00:13:53,740
exists on every query fortunately this

00:13:52,600 --> 00:13:55,360
is something that you can easily

00:13:53,740 --> 00:14:00,250
calculate yourself before you run your

00:13:55,360 --> 00:14:01,899
query recall this query regardless of

00:14:00,250 --> 00:14:03,699
how many nodes actually exist on each

00:14:01,899 --> 00:14:06,699
layer the nodes limit looks at the

00:14:03,699 --> 00:14:10,449
projected total so a hundred times 100

00:14:06,699 --> 00:14:12,430
gives us 10,000 total nodes what if we

00:14:10,449 --> 00:14:16,329
decide to fetch comments with our pull

00:14:12,430 --> 00:14:19,240
requests my calculation yields 1 million

00:14:16,329 --> 00:14:21,699
total nodes that's a lot github notes

00:14:19,240 --> 00:14:24,010
limit is 500,000 which is already very

00:14:21,699 --> 00:14:28,660
generous and if we run our query will

00:14:24,010 --> 00:14:30,310
see the following error ok and the good

00:14:28,660 --> 00:14:31,630
part is github is very explicit about

00:14:30,310 --> 00:14:33,550
what went wrong this is a nice

00:14:31,630 --> 00:14:36,459
descriptive error message but just very

00:14:33,550 --> 00:14:38,740
useful and let's keep this in mind and

00:14:36,459 --> 00:14:41,050
be careful with fetching nested data as

00:14:38,740 --> 00:14:45,160
it's pretty easy to hit the limit on

00:14:41,050 --> 00:14:47,230
tarnex gotcha so we know that graph QL

00:14:45,160 --> 00:14:49,690
has a power to pull grass of objects in

00:14:47,230 --> 00:14:51,670
one roundtrip but like everything in

00:14:49,690 --> 00:14:54,160
life queries also have a cost associated

00:14:51,670 --> 00:14:56,680
to them it's like the price that github

00:14:54,160 --> 00:14:58,420
has to pay to fulfill your query and if

00:14:56,680 --> 00:14:59,610
the price is too high they're refused to

00:14:58,420 --> 00:15:01,810
pay it

00:14:59,610 --> 00:15:03,300
unlike the notes limit cost is not

00:15:01,810 --> 00:15:05,709
something you can compute for yourself

00:15:03,300 --> 00:15:08,079
it's something that github will compute

00:15:05,709 --> 00:15:10,329
for us but through trial and error we

00:15:08,079 --> 00:15:13,329
can build some intuition around this so

00:15:10,329 --> 00:15:14,980
here's how that we can check this is a

00:15:13,329 --> 00:15:18,100
special type that we can add at the top

00:15:14,980 --> 00:15:19,990
level called rate limit here we ask for

00:15:18,100 --> 00:15:22,360
two properties cost and remaining and

00:15:19,990 --> 00:15:25,480
remaining is like how much of our query

00:15:22,360 --> 00:15:27,610
allowance is left quick note this is the

00:15:25,480 --> 00:15:32,019
primary way that github imposes rate

00:15:27,610 --> 00:15:34,000
limits on graph QL requests where rest

00:15:32,019 --> 00:15:36,100
api is will have a limit of 5,000

00:15:34,000 --> 00:15:38,709
requests per hour graph QL api's will

00:15:36,100 --> 00:15:40,990
have a hourly cost limit of 5,000 and

00:15:38,709 --> 00:15:44,980
the simplest queries my experience

00:15:40,990 --> 00:15:46,569
usually cost one for comparison so

00:15:44,980 --> 00:15:50,170
notice like I'm in commented out

00:15:46,569 --> 00:15:52,510
comments which we're still pulling 10

00:15:50,170 --> 00:15:57,850
and notes at maths right now this is

00:15:52,510 --> 00:16:01,389
still a lot yes how much this costs it's

00:15:57,850 --> 00:16:03,880
one that's a bit unexpected we're

00:16:01,389 --> 00:16:05,980
fetching a lot of data right so let's

00:16:03,880 --> 00:16:07,930
add comments back in and go ahead and

00:16:05,980 --> 00:16:12,639
make a guess in your minds

00:16:07,930 --> 00:16:15,220
how does cost change yeah that's a huge

00:16:12,639 --> 00:16:18,820
cost difference that's a huge cost

00:16:15,220 --> 00:16:20,829
difference with a query this costly you

00:16:18,820 --> 00:16:21,310
won't be able to exceed 50 requests per

00:16:20,829 --> 00:16:25,480
hour

00:16:21,310 --> 00:16:27,730
so be wary and there are many things

00:16:25,480 --> 00:16:29,829
that can influence cost the biggest one

00:16:27,730 --> 00:16:32,490
in my experience is projected amount of

00:16:29,829 --> 00:16:36,010
nested entities in this case it's almost

00:16:32,490 --> 00:16:38,380
500k so use this as an indicator for

00:16:36,010 --> 00:16:41,260
potential costs but do experiment it

00:16:38,380 --> 00:16:44,350
will vary from time from different

00:16:41,260 --> 00:16:46,600
experiences we will discuss strategies

00:16:44,350 --> 00:16:50,440
for minimizing costs but for now let's

00:16:46,600 --> 00:16:52,750
just look at the next gotcha so now that

00:16:50,440 --> 00:16:54,760
we satisfy notes limits and cost

00:16:52,750 --> 00:16:57,670
constraints the final test is the actual

00:16:54,760 --> 00:16:58,959
runtime performance of this query it's

00:16:57,670 --> 00:17:01,959
not something that even github can

00:16:58,959 --> 00:17:04,150
protect predict ahead of time and

00:17:01,959 --> 00:17:05,589
sometimes a perfectly fine query that

00:17:04,150 --> 00:17:07,660
doesn't look like much would

00:17:05,589 --> 00:17:10,390
consistently timeout and the timeout is

00:17:07,660 --> 00:17:11,970
fairly aggressive if a query takes more

00:17:10,390 --> 00:17:15,910
than 10 seconds it will fail

00:17:11,970 --> 00:17:18,400
unfortunately so this query only costs 6

00:17:15,910 --> 00:17:22,630
points but 9 times out of 10 it would

00:17:18,400 --> 00:17:24,280
timeout with this error in our

00:17:22,630 --> 00:17:27,160
experience this happens when a query

00:17:24,280 --> 00:17:30,490
actually returns a lot of data so here

00:17:27,160 --> 00:17:32,320
it's attempting to return about 30,000

00:17:30,490 --> 00:17:34,960
lines of JSON which is not that much

00:17:32,320 --> 00:17:37,510
only about 12 kilobytes gzipped

00:17:34,960 --> 00:17:41,520
but it's a lot for github to compute in

00:17:37,510 --> 00:17:44,650
a single API call that's not pre cached

00:17:41,520 --> 00:17:46,600
so note that also cost is just an

00:17:44,650 --> 00:17:48,760
estimate of how heavy the query is

00:17:46,600 --> 00:17:50,590
forget observers and even low-cost

00:17:48,760 --> 00:17:53,830
queries could timeout this makes

00:17:50,590 --> 00:17:55,030
designing our strategy complex because

00:17:53,830 --> 00:17:58,650
we don't know upfront which exact

00:17:55,030 --> 00:17:58,650
queries would timeout ahead

00:17:58,840 --> 00:18:04,250
so these gotchas are were the opponents

00:18:01,880 --> 00:18:05,690
on their own but in a teamfight they can

00:18:04,250 --> 00:18:07,850
be overwhelming and present quite the

00:18:05,690 --> 00:18:10,610
challenge so let's take a look at the

00:18:07,850 --> 00:18:12,650
whole request lifecycle in context we

00:18:10,610 --> 00:18:15,200
write our graph QL query that pulls NASA

00:18:12,650 --> 00:18:16,580
data send it to github servers they

00:18:15,200 --> 00:18:20,720
check that our queries but note below

00:18:16,580 --> 00:18:22,429
the notes limit okay that passes then

00:18:20,720 --> 00:18:24,260
they compute the cost and add it to our

00:18:22,429 --> 00:18:28,159
current running costs and check if we're

00:18:24,260 --> 00:18:30,559
still below our hourly quota stone the

00:18:28,159 --> 00:18:33,010
green cold they finally run the query

00:18:30,559 --> 00:18:37,250
and if it completes under 10 seconds

00:18:33,010 --> 00:18:39,500
then we finally get the result back now

00:18:37,250 --> 00:18:40,909
it's worth noting notes limit is

00:18:39,500 --> 00:18:43,220
something we can compute ourselves as

00:18:40,909 --> 00:18:44,899
mentioned cost of something get up

00:18:43,220 --> 00:18:48,200
computes but we can experiment to find

00:18:44,899 --> 00:18:50,360
out more in timeouts is I don't know

00:18:48,200 --> 00:18:52,730
just something not we or github can

00:18:50,360 --> 00:18:55,100
calculate and predict sometimes retries

00:18:52,730 --> 00:18:56,510
help but most often it means our query

00:18:55,100 --> 00:18:59,090
is just too heavy and needs to be broken

00:18:56,510 --> 00:19:01,820
down and the simplest way to do this is

00:18:59,090 --> 00:19:03,559
to decrease the max page size and send

00:19:01,820 --> 00:19:07,340
several lighter requests instead of one

00:19:03,559 --> 00:19:09,169
heavy request so I hope this gives you a

00:19:07,340 --> 00:19:11,210
good understanding of the query

00:19:09,169 --> 00:19:13,340
lifecycle and knowing how this process

00:19:11,210 --> 00:19:14,779
works and help you navigate errors that

00:19:13,340 --> 00:19:16,220
you will undoubtedly get when you

00:19:14,779 --> 00:19:18,799
actually start getting your hands dirty

00:19:16,220 --> 00:19:22,820
so you can always refer back to this

00:19:18,799 --> 00:19:24,110
when the time comes now that I've

00:19:22,820 --> 00:19:26,890
convinced you of the sufficiently

00:19:24,110 --> 00:19:30,289
challenging problem how do we solve it

00:19:26,890 --> 00:19:34,460
so recall to answer this question looks

00:19:30,289 --> 00:19:38,090
to call our data model any entities on

00:19:34,460 --> 00:19:42,110
PRS are three levels deep let's try to

00:19:38,090 --> 00:19:44,990
query for comments in this case so

00:19:42,110 --> 00:19:46,909
notice we picked 40 as the upper limit

00:19:44,990 --> 00:19:48,740
for PRS just a satisfying notes limit

00:19:46,909 --> 00:19:50,750
could have been 100 while we limit

00:19:48,740 --> 00:19:52,669
comments to 40 it's fine doesn't really

00:19:50,750 --> 00:19:54,880
matter much for this example what does

00:19:52,669 --> 00:19:58,669
matter is that this query Unisa paginate

00:19:54,880 --> 00:20:01,039
on an s identity PRS in this case and

00:19:58,669 --> 00:20:02,960
it's very common to have more than 40

00:20:01,039 --> 00:20:04,880
PRS in a repo and if your company has

00:20:02,960 --> 00:20:06,789
more than 100 repos that complicates

00:20:04,880 --> 00:20:09,230
things but even if we can handle that

00:20:06,789 --> 00:20:11,510
some PRS are more than 100 comments

00:20:09,230 --> 00:20:15,470
that's even more nesting so we need a

00:20:11,510 --> 00:20:18,620
better strategy the strategy that we

00:20:15,470 --> 00:20:20,059
came up with is double paths in cases

00:20:18,620 --> 00:20:22,100
when we need to pull a lot of data that

00:20:20,059 --> 00:20:25,820
requires pagination on multiple levels

00:20:22,100 --> 00:20:27,470
we can do it in multiple passes so the

00:20:25,820 --> 00:20:29,899
core idea behind double pass is that we

00:20:27,470 --> 00:20:32,149
can first query to collect the IDS of

00:20:29,899 --> 00:20:34,909
our top-level entity repos in this case

00:20:32,149 --> 00:20:38,289
and then run a query for each top-level

00:20:34,909 --> 00:20:41,330
entity and paginate on them separately

00:20:38,289 --> 00:20:44,269
so this query would fetch IDs and names

00:20:41,330 --> 00:20:46,340
of first hundred repos now with IDs and

00:20:44,269 --> 00:20:48,500
names we can create a new query where we

00:20:46,340 --> 00:20:51,409
provide one idea at a time and paginate

00:20:48,500 --> 00:20:53,720
on purpose simple enough but how do we

00:20:51,409 --> 00:20:56,330
do this typically graphical API is have

00:20:53,720 --> 00:20:58,159
a rigid schema for this to work we need

00:20:56,330 --> 00:21:01,159
to find a way to construct a query that

00:20:58,159 --> 00:21:05,630
fetches repositories by ID or similar

00:21:01,159 --> 00:21:08,330
alternative this is from the API schema

00:21:05,630 --> 00:21:11,330
documentation so we can either paginate

00:21:08,330 --> 00:21:13,880
over repos and apply various filters or

00:21:11,330 --> 00:21:17,090
we can just pull one repo at a time the

00:21:13,880 --> 00:21:18,740
latter seems handy so let's use that now

00:21:17,090 --> 00:21:21,110
we can run one query for each repository

00:21:18,740 --> 00:21:23,539
and then paginate on the nested entity

00:21:21,110 --> 00:21:25,669
poor request where do you learn how to

00:21:23,539 --> 00:21:27,860
do this previously and this approach

00:21:25,669 --> 00:21:29,720
works for sure but it's kind of slow and

00:21:27,860 --> 00:21:32,899
it doesn't fully leverage the power of

00:21:29,720 --> 00:21:36,049
github Braccio api let's think about how

00:21:32,899 --> 00:21:38,000
we can optimize us further if we look

00:21:36,049 --> 00:21:39,769
closely at a typical organization we

00:21:38,000 --> 00:21:41,720
noticed that the total number of PRS in

00:21:39,769 --> 00:21:43,760
repos is very non-uniform in

00:21:41,720 --> 00:21:46,039
distribution and it's common to have

00:21:43,760 --> 00:21:49,429
repos with thousands of peers and repos

00:21:46,039 --> 00:21:51,049
with zero to ten we can plan our queries

00:21:49,429 --> 00:21:54,380
better by taking into account the total

00:21:51,049 --> 00:21:55,820
number of PRS and repos and if we can

00:21:54,380 --> 00:21:57,980
get this number without fat fetching all

00:21:55,820 --> 00:22:01,159
the PRS that's great and it turns out we

00:21:57,980 --> 00:22:03,139
can so let's modify our query this query

00:22:01,159 --> 00:22:06,139
will fetch three repos and a total

00:22:03,139 --> 00:22:07,970
counts of PRS they have the total count

00:22:06,139 --> 00:22:10,309
is accessible for every connection in

00:22:07,970 --> 00:22:12,260
this graph so for any collection of

00:22:10,309 --> 00:22:13,880
nodes we can get their total count

00:22:12,260 --> 00:22:15,400
without pulling all of the notes this is

00:22:13,880 --> 00:22:19,240
very useful

00:22:15,400 --> 00:22:20,920
this is what we return now let's see how

00:22:19,240 --> 00:22:25,300
the PR account fits into our double path

00:22:20,920 --> 00:22:27,820
strategy okay so first we run a query to

00:22:25,300 --> 00:22:28,590
get all repo IDs and how many PRS they

00:22:27,820 --> 00:22:30,820
contain

00:22:28,590 --> 00:22:34,030
to illustrate session the result let's

00:22:30,820 --> 00:22:37,330
use an adorable puppy we separate out

00:22:34,030 --> 00:22:38,680
repos that have fewer than 100 PRS save

00:22:37,330 --> 00:22:40,720
it for batch fetching so we don't have

00:22:38,680 --> 00:22:44,050
to fetch each separately this reduces a

00:22:40,720 --> 00:22:46,210
total number of requests and if the repo

00:22:44,050 --> 00:22:48,400
has more than 100 PRS it goes on the

00:22:46,210 --> 00:22:52,510
right and we fetch one repo at a time

00:22:48,400 --> 00:22:55,000
paginate on pour requests and then when

00:22:52,510 --> 00:22:57,340
we're done we can merge the results on

00:22:55,000 --> 00:23:00,580
those two strategies and end up with a

00:22:57,340 --> 00:23:03,670
lot of puppies so straightforward enough

00:23:00,580 --> 00:23:05,980
let's see it with some data here's our

00:23:03,670 --> 00:23:07,330
repos with their accounts I've arranged

00:23:05,980 --> 00:23:09,010
them an increasing count so we can

00:23:07,330 --> 00:23:12,480
quickly sort through them into two

00:23:09,010 --> 00:23:14,800
groups and we're batching these ones

00:23:12,480 --> 00:23:17,230
into groups where the totals still less

00:23:14,800 --> 00:23:18,640
than a hundred it doesn't really matter

00:23:17,230 --> 00:23:22,090
how we group these as long as we keep

00:23:18,640 --> 00:23:23,680
below the max of 100 the bottom line is

00:23:22,090 --> 00:23:26,410
that now instead of four queries bring

00:23:23,680 --> 00:23:30,280
you only to make two for large data sets

00:23:26,410 --> 00:23:32,350
this is a lot of savings on this side we

00:23:30,280 --> 00:23:33,180
fetch one by one it's pretty

00:23:32,350 --> 00:23:35,320
self-explanatory

00:23:33,180 --> 00:23:37,030
using the method weird you describe for

00:23:35,320 --> 00:23:40,990
paginating large amounts of nests and

00:23:37,030 --> 00:23:42,970
entities we merge this together and we

00:23:40,990 --> 00:23:47,410
have all of our peers that we set out to

00:23:42,970 --> 00:23:50,860
fetch right nice there's still one piece

00:23:47,410 --> 00:23:54,850
to flush out let's deep dive into this

00:23:50,860 --> 00:23:57,160
part so the repos that don't have a lot

00:23:54,850 --> 00:23:59,560
of PRS we need to construct a query that

00:23:57,160 --> 00:24:02,500
pulls a subset of repos and they're PRS

00:23:59,560 --> 00:24:05,380
how do we do this specifically how do we

00:24:02,500 --> 00:24:07,090
batch we need to find a way to construct

00:24:05,380 --> 00:24:09,910
a query that fetches multiple

00:24:07,090 --> 00:24:12,430
repositories by ID seen this

00:24:09,910 --> 00:24:14,350
documentation before it does not allow

00:24:12,430 --> 00:24:17,440
us to pull data by multiple IDs or by

00:24:14,350 --> 00:24:19,440
multiple names what if we just combine

00:24:17,440 --> 00:24:21,580
multiple repository types into one query

00:24:19,440 --> 00:24:22,220
ok let's be less confusing with some

00:24:21,580 --> 00:24:24,480
examples

00:24:22,220 --> 00:24:27,359
one approach would be to use graphical

00:24:24,480 --> 00:24:28,730
aliases to fetch several repositories by

00:24:27,359 --> 00:24:31,470
name

00:24:28,730 --> 00:24:33,659
notice these repo back-end and repo web

00:24:31,470 --> 00:24:35,639
labels these are aliases which is just a

00:24:33,659 --> 00:24:38,249
way to name your data to avoid conflicts

00:24:35,639 --> 00:24:40,559
in the resulting data set this would

00:24:38,249 --> 00:24:42,149
work but it's a lot more hands-on we

00:24:40,559 --> 00:24:44,009
would have to figure out alias names and

00:24:42,149 --> 00:24:45,600
how to stitch this together this will

00:24:44,009 --> 00:24:47,909
either result in a heavier query or

00:24:45,600 --> 00:24:49,499
require us to use partials so we're not

00:24:47,909 --> 00:24:52,230
duplicating our sub queries for each

00:24:49,499 --> 00:24:53,669
repo and most importantly this approach

00:24:52,230 --> 00:24:56,970
only works because there is a type that

00:24:53,669 --> 00:24:58,350
allows us to fetch repo by name what

00:24:56,970 --> 00:25:00,179
about all the other entities we might

00:24:58,350 --> 00:25:02,519
need to pull like this it doesn't seem

00:25:00,179 --> 00:25:06,359
like a solution that is robust and can

00:25:02,519 --> 00:25:08,039
be applied generically again fortunately

00:25:06,359 --> 00:25:10,019
for us github already thought about use

00:25:08,039 --> 00:25:13,259
cases like this and it's one of the only

00:25:10,019 --> 00:25:15,029
graph QL API as I did but it really

00:25:13,259 --> 00:25:17,429
should be a best practice because it's

00:25:15,029 --> 00:25:19,350
pretty great so what github did is add a

00:25:17,429 --> 00:25:22,710
special type on the top level called

00:25:19,350 --> 00:25:25,559
notes type this allows us to query any

00:25:22,710 --> 00:25:28,470
item by ID and it's like a global key

00:25:25,559 --> 00:25:31,830
value store so for example consider this

00:25:28,470 --> 00:25:35,389
query notice top-level that accepts an

00:25:31,830 --> 00:25:38,700
array of IDs and returns given entities

00:25:35,389 --> 00:25:40,679
like type but because it's a generic

00:25:38,700 --> 00:25:43,230
type we can't access any of the

00:25:40,679 --> 00:25:45,779
repository type properties we can only

00:25:43,230 --> 00:25:47,580
get ID and type name fields so in order

00:25:45,779 --> 00:25:51,149
to solve this we need to use inline

00:25:47,580 --> 00:25:54,179
fragments it's like a type cast for

00:25:51,149 --> 00:25:56,639
graph QL and we can now rewrite our

00:25:54,179 --> 00:25:58,859
query reporting Li that's what the

00:25:56,639 --> 00:26:00,929
syntax looks like this allows us to

00:25:58,859 --> 00:26:02,789
access properties on a specific type and

00:26:00,929 --> 00:26:05,010
it's a very useful technique we can use

00:26:02,789 --> 00:26:07,680
it to fetch pretty much anything

00:26:05,010 --> 00:26:10,320
we can just keep adding IDs the list

00:26:07,680 --> 00:26:12,360
simple and easy so when we batch we'll

00:26:10,320 --> 00:26:14,520
just feed all the IDS from each batch

00:26:12,360 --> 00:26:17,310
into this parameter and fetch all those

00:26:14,520 --> 00:26:19,800
entities at once so you might be

00:26:17,310 --> 00:26:22,470
thinking hey this type thing looks like

00:26:19,800 --> 00:26:24,260
cheating can I just provide a million

00:26:22,470 --> 00:26:26,370
IDs and fetch everything in one query

00:26:24,260 --> 00:26:28,920
you are right

00:26:26,370 --> 00:26:30,570
normal pagination rules don't apply to

00:26:28,920 --> 00:26:32,850
this type it's not bound by the upper

00:26:30,570 --> 00:26:33,990
limit of a hundred items but in our

00:26:32,850 --> 00:26:36,180
experience it's easy to construct

00:26:33,990 --> 00:26:38,070
queries then that lead to timeouts and

00:26:36,180 --> 00:26:40,410
it really depends on how heavy the

00:26:38,070 --> 00:26:42,660
objects are in our experience the

00:26:40,410 --> 00:26:45,960
reasonable limit would be between 30 and

00:26:42,660 --> 00:26:48,420
80 I use at once the proquest with a lot

00:26:45,960 --> 00:26:49,490
more nested properties fetched 50 is a

00:26:48,420 --> 00:26:51,750
good middle ground

00:26:49,490 --> 00:26:54,990
these should provide some context for

00:26:51,750 --> 00:26:56,700
your experimentation all right so this

00:26:54,990 --> 00:26:58,260
all sounds good but let's make sure we

00:26:56,700 --> 00:27:01,170
covered and accounted for all the

00:26:58,260 --> 00:27:04,170
gotchas that we discussed before these

00:27:01,170 --> 00:27:06,090
guys so the nested data problem is

00:27:04,170 --> 00:27:07,890
solved by double pass by definition

00:27:06,090 --> 00:27:09,690
although depending on what you're hoping

00:27:07,890 --> 00:27:13,650
to do you mind a triple pass or dribble

00:27:09,690 --> 00:27:15,330
pass more passes nodes limits with this

00:27:13,650 --> 00:27:17,730
is simple well we just make sure we

00:27:15,330 --> 00:27:21,240
don't exceed the notes limit during

00:27:17,730 --> 00:27:23,130
query composition we discussed

00:27:21,240 --> 00:27:26,490
optimizing for cost reduction let's make

00:27:23,130 --> 00:27:29,160
sure we handle this let's consider this

00:27:26,490 --> 00:27:31,680
query to fetch reviews and comments for

00:27:29,160 --> 00:27:35,160
every pull request if we run it as is

00:27:31,680 --> 00:27:38,040
the cost is gonna be the following which

00:27:35,160 --> 00:27:40,110
is a lot we can cheat a little what do

00:27:38,040 --> 00:27:42,270
we know about this data if we think

00:27:40,110 --> 00:27:43,950
about this query well it's very rare

00:27:42,270 --> 00:27:47,040
when a poor ol class will have more than

00:27:43,950 --> 00:27:49,530
100 reviews or more than 40 comments per

00:27:47,040 --> 00:27:52,320
review so intuitively it just seems

00:27:49,530 --> 00:27:55,470
abnormal so I would say it's safe to

00:27:52,320 --> 00:27:57,960
decrease those page sizes let's assume

00:27:55,470 --> 00:28:00,420
there are no more than 20 reviews for

00:27:57,960 --> 00:28:02,340
starters run our update of query and

00:28:00,420 --> 00:28:04,950
look at that the cost went down by a

00:28:02,340 --> 00:28:07,040
factor of five optimizations like this

00:28:04,950 --> 00:28:09,870
require knowing your data of course

00:28:07,040 --> 00:28:12,330
intuition and experiments but it can

00:28:09,870 --> 00:28:14,309
make a drastic difference so definitely

00:28:12,330 --> 00:28:17,440
make sure you play around with this

00:28:14,309 --> 00:28:19,509
we at toast analyzed data from a bunch

00:28:17,440 --> 00:28:21,190
of companies and pick sensible defaults

00:28:19,509 --> 00:28:24,279
so if you're planning to use our library

00:28:21,190 --> 00:28:27,099
that's all covered there lastly we have

00:28:24,279 --> 00:28:29,710
timeouts this is purely trial and error

00:28:27,099 --> 00:28:31,239
and running queries in production what

00:28:29,710 --> 00:28:33,460
we learned is that the easiest strategy

00:28:31,239 --> 00:28:35,200
to reduce page size is at the cost of

00:28:33,460 --> 00:28:37,629
making more requests or the easiest

00:28:35,200 --> 00:28:40,299
strategy is to reduce pay sites we can

00:28:37,629 --> 00:28:42,279
reduce page sizes from like a hundred

00:28:40,299 --> 00:28:45,159
250 and double the number of requests

00:28:42,279 --> 00:28:47,320
but in general seems like 50 s a good

00:28:45,159 --> 00:28:51,639
default to set as a pagination limit for

00:28:47,320 --> 00:28:54,729
heavy queries and the last bit I want to

00:28:51,639 --> 00:28:57,580
mention is a schema preview the github

00:28:54,729 --> 00:29:00,219
Raphael API is constantly evolving and

00:28:57,580 --> 00:29:02,799
new features become available often for

00:29:00,219 --> 00:29:06,519
example things like is my pull request a

00:29:02,799 --> 00:29:08,529
draft and are my CI checks passing these

00:29:06,519 --> 00:29:10,179
are still in the preview mode so to

00:29:08,529 --> 00:29:12,279
access these you would need to execute

00:29:10,179 --> 00:29:13,719
your queries while sending HTTP accept

00:29:12,279 --> 00:29:15,940
headers it would look something like

00:29:13,719 --> 00:29:22,119
this this is a header I use for draft

00:29:15,940 --> 00:29:23,739
pull requests and cool that's it that

00:29:22,119 --> 00:29:25,419
was a lot of stuff a lot of learnings

00:29:23,739 --> 00:29:26,769
that we just went through give yourself

00:29:25,419 --> 00:29:30,219
a pat on the back if you're still with

00:29:26,769 --> 00:29:32,830
me high-five and go build your own

00:29:30,219 --> 00:29:34,450
github data fetcher here's the link to

00:29:32,830 --> 00:29:35,950
the open source library it's a

00:29:34,450 --> 00:29:37,659
work-in-progress and currently contains

00:29:35,950 --> 00:29:39,969
starter code you can use it as a one-off

00:29:37,659 --> 00:29:42,639
script to pull data for your org and get

00:29:39,969 --> 00:29:45,549
it as a JSON file we'll be adding to it

00:29:42,639 --> 00:29:47,669
more as we build more so and you know

00:29:45,549 --> 00:29:51,729
feel free to send in some poor requests

00:29:47,669 --> 00:29:53,649
and if you're like RIA where's the

00:29:51,729 --> 00:29:55,210
advanced stuff and this talk is

00:29:53,649 --> 00:29:57,369
apparently still way too baby level for

00:29:55,210 --> 00:29:59,139
you please please please PLEASE say hi

00:29:57,369 --> 00:30:00,549
to me after I'd love to get your

00:29:59,139 --> 00:30:03,729
expertise on stuff that we haven't

00:30:00,549 --> 00:30:07,210
solved yet and thanks and you can send

00:30:03,729 --> 00:30:09,249
me homework reading on Twitter yeah so

00:30:07,210 --> 00:30:09,639
lastly this talk was a lot of fun to put

00:30:09,249 --> 00:30:11,349
together

00:30:09,639 --> 00:30:12,860
thanks for coming and thanks for

00:30:11,349 --> 00:30:17,850
listening to me ramble

00:30:12,860 --> 00:30:46,170
[Applause]

00:30:17,850 --> 00:30:46,170

YouTube URL: https://www.youtube.com/watch?v=i5pIszu9MeM


