Title: State of the Source 2020: Introduction to Ethics for Computer People
Publication date: 2020-09-23
Playlist: State of the Source Summit 2020
Description: 
	Introduction to Ethics for Computer People
Molly de Blanc

Everyone should take an ethics course! This is not an ethics course, but instead a whirlwind tour of ethical concepts and questions, all of which will be related back to computers and open source.

Thank you to our Video Recordings Sponsor eng@salesforce!
Full Program: https://eventyay.com/e/8fa7fd14/schedule
Captions: 
	00:00:04,720 --> 00:00:07,440
hi i'm molly de blanc and i'm here to talk

00:00:07,440 --> 00:00:13,519
with you a little bit about ethics um so let's make a start

00:00:13,759 --> 00:00:17,359
uh i am the strategic initiatives manager of the gnome foundation

00:00:17,359 --> 00:00:24,160
um so i send lots of emails um and i talk with people and

00:00:24,160 --> 00:00:28,640
organizations about partnerships and ways that we can

00:00:28,640 --> 00:00:34,320
do awesome things together um in addition to that uh on the

00:00:34,320 --> 00:00:42,399
free software front um i'm a devel deviant developer i'm a debian developer

00:00:42,399 --> 00:00:44,719
uh where i do work on the community team

00:00:44,719 --> 00:00:49,200
and i used to part the outreach team uh and uh you might know me from one of

00:00:49,200 --> 00:00:53,520
my greatest hits as a former board member and president

00:00:53,520 --> 00:00:58,160
of the open source initiative um in addition to that i have some other

00:00:58,160 --> 00:01:00,960
credentials uh which i'm talking about right now

00:01:00,960 --> 00:01:05,920
um uh the the real reason that i'm talking to you

00:01:05,920 --> 00:01:08,159
or the reason i can justify talking to you

00:01:08,159 --> 00:01:12,880
um is that i'm a graduate student in bioethics at new york university or nyu

00:01:12,880 --> 00:01:17,040
um i studied philosophy of science as an undergraduate

00:01:17,040 --> 00:01:22,960
so i have a background and current experience in looking at philosophy

00:01:22,960 --> 00:01:27,360
specifically from the perspective of ethics and applied ethics

00:01:27,360 --> 00:01:31,680
um looking at philosophy from a perspective of science and technology

00:01:31,680 --> 00:01:38,000
um i spend a lot of time i have spent and i continue to spend a

00:01:38,000 --> 00:01:42,799
lot of time thinking about ethics and technology

00:01:42,799 --> 00:01:48,320
digital ethics and techno ethics and the role technology has in our lives

00:01:48,320 --> 00:01:54,079
and these intersections between technology and society and rights um

00:01:54,079 --> 00:01:57,439
uh this has led me to do a few different things one of them is uh

00:01:57,439 --> 00:02:01,119
along with karen sandler i wrote the principles of digital autonomy

00:02:01,119 --> 00:02:04,479
which you can read at techautonomy.org um if you

00:02:04,479 --> 00:02:08,800
want to check out some other cool stuff um

00:02:08,800 --> 00:02:11,920
and in general i've done a lot of writing and thinking and reading

00:02:11,920 --> 00:02:16,720
around these issues um so

00:02:19,280 --> 00:02:27,040
um so normally before we embark on a discussion in philosophy or ethics

00:02:27,040 --> 00:02:33,760
uh we like to define our terms um so that we all know we're on the same

00:02:33,760 --> 00:02:38,080
page we understand each other uh and because

00:02:38,080 --> 00:02:40,959
we're going to be using lots of words that

00:02:40,959 --> 00:02:46,160
have very specific meanings um when you're talking about ethics

00:02:46,160 --> 00:02:49,519
especially but i think this is true for a lot of philosophy

00:02:49,519 --> 00:02:55,840
you're talking about things people have intuitions about right we

00:02:55,840 --> 00:03:02,560
all have a sense of what's right and what's wrong um and

00:03:02,560 --> 00:03:06,239
whether that sense is something that we view as like this

00:03:06,239 --> 00:03:08,400
is what society thinks is right and wrong

00:03:08,400 --> 00:03:12,400
this is what i think is right and wrong you know we have these

00:03:12,400 --> 00:03:16,800
these ways that we feel about it um and the ways we feel might be very

00:03:16,800 --> 00:03:21,760
different from the things we think uh and the things we think uh might be

00:03:21,760 --> 00:03:25,760
are usually informed by education and society

00:03:25,760 --> 00:03:30,879
so when we're talking about when we're asking questions about ethics

00:03:30,879 --> 00:03:34,879
you know we have different intuitions that are based on like

00:03:34,879 --> 00:03:39,200
these very nuanced understandings of the world that we have as individuals

00:03:39,200 --> 00:03:46,080
um so it's good to define your terms uh my best friend in college uh studied

00:03:46,080 --> 00:03:49,360
politics and philosophy um and we used to get into these

00:03:49,360 --> 00:03:56,159
riotous loud fights um where we would just like be seething at

00:03:56,159 --> 00:03:58,560
each other uh and it would turn out that usually we

00:03:58,560 --> 00:04:04,080
agreed actually but we were uh approaching the conversation

00:04:04,080 --> 00:04:08,239
uh thinking words meant different things and or different words meant to you know

00:04:08,239 --> 00:04:15,439
with there was this like like linguistic issue this issue of definition um

00:04:15,439 --> 00:04:20,799
so uh but i'm not gonna do that right now uh what

00:04:20,799 --> 00:04:25,840
i do want to define though is ethics um so for the sake of this

00:04:25,840 --> 00:04:30,880
conversation ethics uh are ethics are a

00:04:30,880 --> 00:04:37,600
set of rules or principles or values that we use to define

00:04:37,600 --> 00:04:43,120
that we use to express morality so similarly to how the open source

00:04:43,120 --> 00:04:47,759
definition is a set of rules that express

00:04:47,759 --> 00:04:51,280
like whether something provides software freedom ethics

00:04:51,280 --> 00:04:56,320
is uh also a set of rules um so we're going to think about ethics and

00:04:56,320 --> 00:04:59,360
ethical principles

00:04:59,919 --> 00:05:08,800
um so why why bioethics matters to techno ethics um

00:05:09,039 --> 00:05:11,919
when i first met people in my program and we were talking about what we were

00:05:11,919 --> 00:05:14,720
interested in there was a lot of people like there was

00:05:14,720 --> 00:05:20,560
a very strong interest in technology and artificial intelligence um

00:05:20,560 --> 00:05:24,560
uh and and in general in this like space of

00:05:24,560 --> 00:05:32,400
technology and humanity um so i think there's

00:05:32,400 --> 00:05:35,440
there's in general a strong connection um

00:05:35,440 --> 00:05:38,000
if we want to think a little bit differently and say like why do

00:05:38,000 --> 00:05:42,400
bioethics matter to technoethics is like you know the

00:05:42,400 --> 00:05:47,919
first organ transplant that i know about

00:05:47,919 --> 00:05:51,199
happened in 1905 it was a cornea transplant

00:05:51,199 --> 00:05:57,840
um and then the first use of the term robot was in 1920 um

00:05:57,840 --> 00:06:01,520
in Rossumovi Univerzální Roboti R.U.R. um

00:06:01,520 --> 00:06:06,240
so i would say that bioethics has been around

00:06:06,240 --> 00:06:13,840
a little bit longer um than techno ethics or ethics of technology

00:06:13,840 --> 00:06:17,199
in the sense that we're actually so let's define a quick term

00:06:17,199 --> 00:06:20,720
mostly when i say technology right now i'm gonna be talking about computers

00:06:20,720 --> 00:06:27,360
um uh so when we talk about computer computing technology like that's that's

00:06:27,360 --> 00:06:35,759
a more recent conversational invention than uh medicine um

00:06:35,759 --> 00:06:38,800
and medicine is a lot older than the first organ transplant

00:06:38,800 --> 00:06:46,840
um uh so i would say that i think bioethics are a

00:06:46,840 --> 00:06:51,599
more well-tread path in that people just been thinking

00:06:51,599 --> 00:06:54,080
and talking about it for longer than they've been thinking

00:06:54,080 --> 00:07:00,319
and talking about computers um but also like bioethics is is very

00:07:00,319 --> 00:07:04,000
there's a lot of theory and philosophy but there's also a lot of principles

00:07:04,000 --> 00:07:10,479
um it's a very principled field where people are interested in applications

00:07:10,479 --> 00:07:17,919
um and how things impact the world um additionally

00:07:17,919 --> 00:07:22,080
and i think this is more to my point why bioethics matter

00:07:22,080 --> 00:07:26,319
um is that

00:07:26,960 --> 00:07:33,759
in my learning about bioethics uh the base principles are that we want to

00:07:33,759 --> 00:07:39,280
respect people's autonomy um we want to unders which which means

00:07:39,280 --> 00:07:43,199
that we want to respect them as self-governing

00:07:43,199 --> 00:07:47,680
individuals or groups like we we think that people

00:07:47,680 --> 00:07:51,680
have the right to decide what happens to them and what happens

00:07:51,680 --> 00:07:55,919
about them um and like

00:07:55,919 --> 00:08:02,319
this is super true for technology like it's it's true around around buyout like

00:08:02,319 --> 00:08:05,840
like medical matters and bioethics like includes medicine and

00:08:05,840 --> 00:08:12,319
um research and public health and at nyu uh environmental ethics is also there so

00:08:12,319 --> 00:08:15,680
there's like a bunch of stuff that like fits under this little umbrella

00:08:15,680 --> 00:08:20,960
um uh but a lot of it comes down to the same basic

00:08:20,960 --> 00:08:26,080
issues that i think tie into that i think are the same basic issues

00:08:26,080 --> 00:08:29,759
when we're thinking about technology right

00:08:30,400 --> 00:08:34,880
we're thinking about who has ownership in a situation who

00:08:34,880 --> 00:08:39,279
who is the person who has rights what are those rights

00:08:39,279 --> 00:08:45,200
how do we like protect and empower people to make decisions about

00:08:45,200 --> 00:08:52,839
themselves how do we respect their autonomy um so i think it's a good

00:08:52,839 --> 00:08:55,519
discussion now we're going to talk about the seven

00:08:55,519 --> 00:09:01,360
principles of public health um this is

00:09:01,360 --> 00:09:08,880
one approach of many um to making

00:09:08,880 --> 00:09:15,519
ethical decisions making moral decisions um in a public health context a lot of

00:09:15,519 --> 00:09:18,720
it relies heavily on medicine um but i'm gonna do my best to

00:09:18,720 --> 00:09:25,120
contextualize it as well for technology um so non-maleficence you

00:09:25,120 --> 00:09:30,880
might know about this as do no harm um so

00:09:30,880 --> 00:09:34,080
philosophy is really talking about pain uh

00:09:34,080 --> 00:09:38,399
they've decided pain is like the universal bad that everyone can agree is

00:09:38,399 --> 00:09:41,920
terrible um and and so like your first principle

00:09:41,920 --> 00:09:44,640
is to not not actively unintentionally cause pain

00:09:44,640 --> 00:09:49,040
not in the lipsense don't don't don't do harm don't be bad um it's

00:09:49,040 --> 00:09:52,720
not even don't be bad it's like don't hurt people don't do bad things to

00:09:52,720 --> 00:09:56,640
people um so if you want to like look at this in

00:09:56,640 --> 00:10:01,279
the context of technology uh we can i mean there's so many ways we

00:10:01,279 --> 00:10:05,360
can look at it because like we can talk about this from the

00:10:05,360 --> 00:10:08,560
perspective of design we can talk about it from the perspective

00:10:08,560 --> 00:10:13,839
of policy or um functionality like there's so many ways

00:10:13,839 --> 00:10:18,800
we can talk about it right so we can say don't do harm means

00:10:18,800 --> 00:10:22,880
something like uh in the do you know harm license

00:10:22,880 --> 00:10:31,040
uh where people are discussing things like you know cannot be used for

00:10:31,040 --> 00:10:35,519
like weapons i mean i don't you know like you can't use something for weapons

00:10:35,519 --> 00:10:40,320
um but we could also think about it as meaning things like

00:10:40,320 --> 00:10:48,399
like cannot be produced with uh slave labor

00:10:48,399 --> 00:10:52,720
right like plenty of resources inside of computers i mean plenty i don't i don't

00:10:52,720 --> 00:10:56,640
actually know how many um but uh i know that resources used in

00:10:56,640 --> 00:11:01,200
technology uh sometimes can come from a slave labor

00:11:01,200 --> 00:11:08,000
um in mines uh and that's uh or at least they have historically um

00:11:08,000 --> 00:11:12,160
uh and that's pretty bad um so we're gonna say that's a bad thing and that's

00:11:12,160 --> 00:11:18,640
that's maleficent um and we don't want that so like

00:11:18,640 --> 00:11:23,279
not doing that kind of stuff um taking this as a principle

00:11:23,279 --> 00:11:26,720
for which we want to apply to our own work

00:11:26,720 --> 00:11:34,560
is not easy um there's there's this idea of reducing

00:11:34,560 --> 00:11:38,079
this reduction like taking taking a point and kind of

00:11:38,079 --> 00:11:41,920
digging down deeper and deeper to its to its basic state

00:11:41,920 --> 00:11:45,600
and you can do that with technology too you can think about okay so i'm writing

00:11:45,600 --> 00:11:48,240
software so we could take software and we could

00:11:48,240 --> 00:11:50,959
turn it into hardware and we could turn hardware into the base

00:11:50,959 --> 00:11:53,680
components we can turn the base components into mining

00:11:53,680 --> 00:12:00,320
um or we could think about software and break it down to like

00:12:00,320 --> 00:12:03,839
electricity and how electricity is moving in computers

00:12:03,839 --> 00:12:10,240
um so it's a little it's not quite tangential

00:12:10,240 --> 00:12:14,480
i'm almost being tangential um my point is that we can we can think

00:12:14,480 --> 00:12:20,079
about what doing harm is on a lot of levels uh and strive

00:12:20,079 --> 00:12:26,000
to not do harm so on the other side of doing no harm uh

00:12:26,000 --> 00:12:29,120
is is doing good right beneficence we want to

00:12:29,120 --> 00:12:33,120
actively do good we want to produce things that are better

00:12:33,120 --> 00:12:36,560
um we want to make things nice we don't just want to not cause pain we want to

00:12:36,560 --> 00:12:41,920
relieve pain right we want to uh you know so that so

00:12:41,920 --> 00:12:45,200
so this could mean we want to create technology that helps people

00:12:45,200 --> 00:12:50,880
um or helps the world and that can mean lots of different

00:12:50,880 --> 00:12:53,279
things right it could it could mean you're

00:12:53,279 --> 00:12:58,160
making systems to be more efficient so they're less demanding of electricity

00:12:58,160 --> 00:13:05,360
uh and power it could be that you're developing software to like

00:13:05,360 --> 00:13:12,720
disperse funds um across places where like people don't have

00:13:12,720 --> 00:13:17,040
standard bank like what we think of in the us at least is like banking

00:13:17,040 --> 00:13:23,680
standard banking procedures um or like how

00:13:23,680 --> 00:13:28,160
like or how to educate people technology is used in so many ways to do good

00:13:28,160 --> 00:13:35,040
um uh so we we can ask ourselves

00:13:35,040 --> 00:13:39,120
regardless of what we're producing is the question of like

00:13:39,120 --> 00:13:42,800
is this doing good how am i helping it to do good and what what is the good

00:13:42,800 --> 00:13:47,360
being done right um and that that can mean a lot of

00:13:47,360 --> 00:13:51,519
things and i think there's space to have a conversation about how

00:13:51,519 --> 00:13:55,120
technologies that don't seem to obviously be doing good

00:13:55,120 --> 00:13:59,519
are still being benefit beneficial

00:13:59,680 --> 00:14:05,519
um health maximization uh so when you're having these like this is a good

00:14:05,519 --> 00:14:09,199
third point after the first two um because the first two are like

00:14:09,199 --> 00:14:13,519
actually pretty theoretical um so health maximalization especially

00:14:13,519 --> 00:14:18,560
relevant uh in public health um because it's about

00:14:18,560 --> 00:14:25,519
uh like actually like helping people's health so it's not just like let's

00:14:25,519 --> 00:14:30,160
do good on this high level but like let's save some lives

00:14:30,160 --> 00:14:34,000
let's like help people have good teeth because that will

00:14:34,000 --> 00:14:38,560
matter um you know so so in these cases we in like

00:14:38,560 --> 00:14:43,279
in in health maximalization like applied maximalization like you can look at it

00:14:43,279 --> 00:14:47,199
in just the same way um uh where we replace health with

00:14:47,199 --> 00:14:53,600
something like like okay let's talk about um

00:14:53,600 --> 00:14:57,920
like energy efficiency or environmental impact like let's pick a very specific

00:14:57,920 --> 00:15:02,240
thing and and maximize that um how do we

00:15:02,240 --> 00:15:09,120
how do we do the best we can uh on a particular front

00:15:09,120 --> 00:15:12,720
right um and part of that is is how can we do the best we can with the resources

00:15:12,720 --> 00:15:16,880
we have um efficiency uh which is is the next

00:15:16,880 --> 00:15:21,360
principle um we don't

00:15:21,360 --> 00:15:27,680
have unlimited resources um which is unfortunate uh and we don't

00:15:27,680 --> 00:15:30,000
have

00:15:30,399 --> 00:15:35,920
the power to do whatever we want with the things we have we don't have

00:15:35,920 --> 00:15:41,279
the like the social power or the the structural power um

00:15:41,279 --> 00:15:44,720
i think efficiency is a really interesting thing to talk about

00:15:44,720 --> 00:15:48,399
uh in terms of technology because things like hardware

00:15:48,399 --> 00:15:52,000
like hardware is limited your time is limited

00:15:52,000 --> 00:15:57,279
your abilities are limited um but if you want to talk about software

00:15:57,279 --> 00:16:00,160
we have to think about it a little bit differently because software

00:16:00,160 --> 00:16:03,600
once it exists isn't limited right it it's it's

00:16:03,600 --> 00:16:08,639
it's ephemeral uh there is no physical you can put it on a disk but like

00:16:08,639 --> 00:16:16,720
what what is i can't touch software um like i can touch a computer um so

00:16:16,720 --> 00:16:20,800
we need to to change how we think about efficiency and resources

00:16:20,800 --> 00:16:24,399
um especially in the context of software and in general in the context of

00:16:24,399 --> 00:16:29,519
computing technology um and also

00:16:29,519 --> 00:16:35,120
you we have a lot of resources available to us

00:16:35,120 --> 00:16:40,639
like there are physical limits um but monetary limits are another

00:16:40,639 --> 00:16:45,519
interesting conversation to have that we won't get into right now

00:16:47,440 --> 00:16:51,440
um respect for autonomy i mentioned autonomy earlier right it's this idea

00:16:51,440 --> 00:16:57,040
that we have control

00:16:57,040 --> 00:17:02,160
over ourselves um and

00:17:02,240 --> 00:17:07,120
kind of on the other side of having control over ourselves

00:17:07,120 --> 00:17:13,679
is not controlling other people so to provide a very

00:17:13,679 --> 00:17:17,600
visceral example right uh we all have the right not to

00:17:17,600 --> 00:17:20,720
get raped right so so my bodily autonomy my bodily

00:17:20,720 --> 00:17:23,360
autonomy which is like what is frequently talked about in

00:17:23,360 --> 00:17:26,799
public health are in medicine and bioethics in general

00:17:26,799 --> 00:17:30,880
um is something other people don't have the right

00:17:30,880 --> 00:17:38,880
to people don't have the right to affect our bodies

00:17:38,880 --> 00:17:42,000
without our consent without us saying yes

00:17:42,000 --> 00:17:46,640
that is actually okay like i understand what you mean you are allowed to perform

00:17:46,640 --> 00:17:49,600
surgery on me you are allowed to do whatever like you know whatever you're

00:17:49,600 --> 00:17:55,840
agreeing on um so our digital autonomy is actually

00:17:55,840 --> 00:18:02,400
like the same um i mean as i mentioned earlier like karen and i

00:18:02,400 --> 00:18:05,200
talk about this in the principles of digital autonomy

00:18:05,200 --> 00:18:09,280
um but like we have the right to define what happens

00:18:09,280 --> 00:18:11,520
to

00:18:12,960 --> 00:18:17,760
what happens with respect to how we interact with technology

00:18:17,760 --> 00:18:23,280
um and with the cell like the versions of ourselves and our works

00:18:23,280 --> 00:18:28,160
that we create and produce

00:18:28,160 --> 00:18:35,919
and and store on technology um so respecting autonomy

00:18:35,919 --> 00:18:41,600
is like like all these there's so many ways we can have this conversation

00:18:41,600 --> 00:18:45,919
right we can talk about the autonomy of users um and like building things to

00:18:45,919 --> 00:18:49,520
respect the economy of users we can talk about the economy of developers

00:18:49,520 --> 00:18:54,840
we can talk about how developers

00:18:54,840 --> 00:18:58,320
have right to the things that they make right like

00:18:58,320 --> 00:19:02,720
you can say well there shouldn't be clas uh contributor licensing agreements

00:19:02,720 --> 00:19:07,039
um that require developers to sign away the code

00:19:07,039 --> 00:19:10,160
like the rights to the code that they wrote um

00:19:10,160 --> 00:19:14,559
because that code is theirs and belongs to them

00:19:14,559 --> 00:19:18,960
um you can say that you shouldn't force someone

00:19:18,960 --> 00:19:23,520
uh you can't like it is it is it is immoral it is unethical it is it is

00:19:23,520 --> 00:19:27,600
wrong to force someone to produce a certain uh

00:19:27,600 --> 00:19:33,840
type of technology um uh and like when we say force someone we

00:19:33,840 --> 00:19:38,080
actually also we don't just mean like hold a gun to someone said but but we

00:19:38,080 --> 00:19:41,280
mean like coerce them right like say oh well if

00:19:41,280 --> 00:19:43,280
you don't do this you're going to be fired

00:19:43,280 --> 00:19:48,960
um so respecting people's autonomy

00:19:49,360 --> 00:19:53,919
is i mean for me this is this for me personally this is the foundational

00:19:53,919 --> 00:19:58,080
point um of applied ethics uh

00:19:58,080 --> 00:20:05,280
and it's something we really need to think about um and and believe in

00:20:05,280 --> 00:20:09,440
and encompass and bring into our work um both the work that we do for ourselves

00:20:09,440 --> 00:20:15,600
for ourselves um uh and for others

00:20:17,600 --> 00:20:24,159
uh justice okay justice justice means a lot of things in a sense

00:20:24,159 --> 00:20:27,120
talking about justice as a point is also defining justice

00:20:27,120 --> 00:20:31,600
um so we're going to talk about justice in the context of

00:20:31,600 --> 00:20:37,840
equal consideration um and and what that means is we're not

00:20:37,840 --> 00:20:45,440
saying we have the same needs right

00:20:45,440 --> 00:20:49,360
we're not saying we have the same abilities we're saying that we

00:20:49,360 --> 00:20:54,799
have we deserve to be considered and in a sense accommodated

00:20:54,799 --> 00:21:02,080
equally um so like one way this plays out um one way this

00:21:02,080 --> 00:21:05,200
this plays out and like the concept of justice plays out

00:21:05,200 --> 00:21:08,559
and like a workplace is you can't not hire someone

00:21:08,559 --> 00:21:16,000
because they have a disability right um it might mean that a company needs to

00:21:16,000 --> 00:21:19,360
create different types of accommodations um

00:21:19,360 --> 00:21:24,480
for the person uh uh and adapt adapt to the environment that person is

00:21:24,480 --> 00:21:30,159
in um to meet their needs but but like

00:21:30,159 --> 00:21:33,679
we're going to consider the essential value of

00:21:33,679 --> 00:21:42,159
any two people the same um in medicine uh of like a a contemporary

00:21:42,159 --> 00:21:46,559
example um is the idea of ventilators um

00:21:46,559 --> 00:21:50,559
like with uh covid patients who need to go on ventilators

00:21:50,559 --> 00:21:54,559
um and then assigning who gets your limited supply of ventilators

00:21:54,559 --> 00:22:02,960
um and it's it's unjust to make determinations and say like well

00:22:02,960 --> 00:22:07,039
this person has led a good life and this person has not let a good life

00:22:07,039 --> 00:22:10,400
so we're gonna give it to the person who has led a good life um because you're

00:22:10,400 --> 00:22:15,600
not considering the value of these people as inherent

00:22:16,559 --> 00:22:23,679
okay proportionality um proportionality is about thinking

00:22:23,679 --> 00:22:28,720
on the balance between the individual and the

00:22:28,720 --> 00:22:34,159
collective um so

00:22:34,159 --> 00:22:40,480
we want and like and and you know like so much of this is

00:22:40,480 --> 00:22:44,240
built on uh thinking about each individual person

00:22:44,240 --> 00:22:47,120
each individual user patient technologist like each

00:22:47,120 --> 00:22:52,960
individual um and it's important and necessary it's

00:22:52,960 --> 00:22:57,200
important and then it's even necessary to still contextualize individuals as

00:22:57,200 --> 00:23:02,240
part of the whole um and that sometimes

00:23:02,240 --> 00:23:07,120
to to i think of this almost as more of an acknowledgement

00:23:07,120 --> 00:23:12,559
that sometimes an individual can't get everything they

00:23:12,559 --> 00:23:17,679
need um because of the whole

00:23:17,679 --> 00:23:20,960
and that doesn't mean we shouldn't try right like

00:23:20,960 --> 00:23:24,000
you should still try you should still try to help people you should try

00:23:24,000 --> 00:23:27,440
to maximize their good you should try to not harm them

00:23:27,440 --> 00:23:32,480
um but like it doesn't always work out that way

00:23:32,480 --> 00:23:38,960
um uh and occasionally like i mean not

00:23:38,960 --> 00:23:43,919
occasionally like all the time things happen for the sake of the collective

00:23:45,440 --> 00:23:52,640
okay so after all that um i when i conceived of this talk um

00:23:52,640 --> 00:23:57,520
i thought about it as i thought what would be great would be to have a

00:23:57,520 --> 00:24:01,520
discussion um and to like find this as a discussion

00:24:01,520 --> 00:24:04,559
uh in part because there are so many more things

00:24:04,559 --> 00:24:10,320
that i could have said during the first section of this talk um and so

00:24:10,320 --> 00:24:15,520
many more things that i want to say even um but me lecturing at you

00:24:15,520 --> 00:24:19,440
about ethics uh even even just like such a small slither of that

00:24:19,440 --> 00:24:25,919
sliver of ethics for an hour um might be fun for me but i assume would

00:24:25,919 --> 00:24:28,799
be less fun for you and and more more so i assume would be less

00:24:28,799 --> 00:24:32,799
educational than us participating in a conversation and

00:24:32,799 --> 00:24:36,240
allowing ourselves to try out ideas and see how they feel for us

00:24:36,240 --> 00:24:41,120
um you know i think being playing just playing devil's advocate on

00:24:41,120 --> 00:24:43,760
the internet is like kind of a jerk thing to do

00:24:43,760 --> 00:24:48,159
um but doing it in a safe philosophical conversation

00:24:48,159 --> 00:24:53,760
is like arguably an imperative um you know this is the thing that sorry

00:24:53,760 --> 00:24:57,919
this is totally changing it's related um a soviet soviet

00:24:57,919 --> 00:25:02,480
scientists used to investigate um things that people had

00:25:02,480 --> 00:25:05,120
already felt disproven just in case that it turned

00:25:05,120 --> 00:25:08,400
out that they weren't and because it challenged them to

00:25:08,400 --> 00:25:12,480
understand science better um and they kind of think of that as the

00:25:12,480 --> 00:25:16,799
same right like like intellectual conversation in the

00:25:16,799 --> 00:25:21,520
right context is about having a chance to try out

00:25:21,520 --> 00:25:26,840
ideas but also to better understand ideas so

00:25:26,840 --> 00:25:30,960
anyway i i i have some wonderful friends um who

00:25:30,960 --> 00:25:34,159
very generously offered to participate in a conversation

00:25:34,159 --> 00:25:38,960
um about autonomous drones uh which i will get into in a moment

00:25:38,960 --> 00:25:45,200
um and uh hopefully you'll have a chance to join us in some capacity

00:25:45,200 --> 00:25:48,000
so thanks

00:25:51,039 --> 00:25:54,240
so now we're here with deb nicholson and karen sandler

00:25:54,240 --> 00:25:57,840
who are both super awesome you might know them from some of their greatest

00:25:57,840 --> 00:26:00,559
hits like the open source initiative and the

00:26:00,559 --> 00:26:07,279
software freedom conservancy um and they graciously uh offered to

00:26:07,279 --> 00:26:10,720
kind of take the place of audience members um

00:26:10,720 --> 00:26:14,000
or the audience members i guess uh since y'all can't

00:26:14,000 --> 00:26:17,600
participate in this conversation right now um

00:26:17,600 --> 00:26:22,559
so the the the question i'm going to ask is if an autonomous drone

00:26:22,559 --> 00:26:28,240
kills someone who is responsible and and i have like lots of little

00:26:28,240 --> 00:26:33,760
details i can fill in um if they matter because i find that

00:26:33,760 --> 00:26:39,120
they matter a lot to me and my intuitions

00:26:39,120 --> 00:26:42,720
can you give us an example of how they matter to you like like give us like far

00:26:42,720 --> 00:26:47,760
ends of the perspective sure um i think

00:26:47,760 --> 00:26:50,080
if

00:26:51,120 --> 00:26:54,960
so so if it's like a hobbyist builds a drone

00:26:54,960 --> 00:26:58,400
and sends them out then i'm like oh the hobbyist is definitely responsible

00:26:58,400 --> 00:27:05,760
for what happened um but if it's say uh a non-profit is trying to build a

00:27:05,760 --> 00:27:10,320
drone to do medical care and

00:27:10,320 --> 00:27:15,840
it kills someone and be like oh well i think the nonprofit is responsible

00:27:15,840 --> 00:27:18,880
but like i feel a little differently about it

00:27:18,880 --> 00:27:22,000
like i think i think it's it's a better way to put it is

00:27:22,000 --> 00:27:26,240
with that particular example i think people are responsible

00:27:26,240 --> 00:27:35,279
still but my anger is different maybe or my

00:27:35,279 --> 00:27:43,440
not add too many details right now too i mean i do think we i think our impulse

00:27:43,440 --> 00:27:47,520
at least in the us is that someone should be responsible and then it makes

00:27:47,520 --> 00:27:50,080
us feel like we can it's actually a very programmery

00:27:50,080 --> 00:27:54,240
impulse then we can find the link in the chain where stuff

00:27:54,240 --> 00:27:57,760
went downhill and then stop it and make sure

00:27:57,760 --> 00:28:01,440
it doesn't happen again of course if the point of the drone is

00:28:01,440 --> 00:28:06,240
to just kill lots of children then i'm not i'm not sure that a minor

00:28:06,240 --> 00:28:10,880
tweet to the process is the right remedy

00:28:11,520 --> 00:28:15,360
it's interesting to me because by adding the non-profit to it you've like brought

00:28:15,360 --> 00:28:20,480
in this like legal construct of like corporate liability um and

00:28:20,480 --> 00:28:23,440
like which i always find it really fascinating that there are different

00:28:23,440 --> 00:28:26,640
legal frameworks for dealing with this responsibility and

00:28:26,640 --> 00:28:30,159
like there's a lot of like legal concepts that are maybe

00:28:30,159 --> 00:28:36,240
like have their origins in these or spring for out from these ethical

00:28:36,240 --> 00:28:40,080
analyses and so like things like there's this concept called like piercing the

00:28:40,080 --> 00:28:44,240
corporate veil which is to say like if somebody acted

00:28:44,240 --> 00:28:47,600
like extremely negligently or or with willful willfully

00:28:47,600 --> 00:28:51,360
like really i'm trying to try not to use legal terms but like if somebody acted

00:28:51,360 --> 00:28:55,279
you know like intentionally then they should totally be responsible

00:28:55,279 --> 00:28:59,039
but then there's this legal concept of like the organization if it's doing that

00:28:59,039 --> 00:29:02,399
great mission that you refer to there should be some shield if they're

00:29:02,399 --> 00:29:06,399
rescuing you know lots of people who would have otherwise had you

00:29:06,399 --> 00:29:11,840
know died or had really horrible outcomes yeah all right

00:29:11,840 --> 00:29:15,520
it's also really easy i think in a in a corporate

00:29:15,520 --> 00:29:21,679
of any type corporation setting for the responsibility to be really distributed

00:29:21,679 --> 00:29:25,120
and for one person to just be like oh i just pressed the button and another

00:29:25,120 --> 00:29:28,080
person like well i was supposed to go and check make

00:29:28,080 --> 00:29:31,919
sure no one was out there but i was on break because someone else said they

00:29:31,919 --> 00:29:34,320
were gonna help and you know so then it gets really

00:29:34,320 --> 00:29:38,399
complicated like well which person didn't do the one

00:29:38,399 --> 00:29:43,039
tiny piece that they were supposed to do and then then you kind of have to zoom

00:29:43,039 --> 00:29:46,080
out and it's like why why are you like setting stuff off that

00:29:46,080 --> 00:29:50,960
could kill people without like a lot more like backups

00:29:50,960 --> 00:29:57,360
and um you know like uh contingency plans um

00:29:57,360 --> 00:30:02,000
so so one thing is um something i think is really interesting

00:30:02,000 --> 00:30:06,799
and this is like a general intuition people have about like ethics

00:30:06,799 --> 00:30:11,279
and morals is if someone does something that they know

00:30:11,279 --> 00:30:14,559
will have a good effect but that they don't care about the good

00:30:14,559 --> 00:30:18,320
effect then they like don't get points for doing

00:30:18,320 --> 00:30:21,679
good but if they do something that they know will

00:30:21,679 --> 00:30:24,799
have a bad effect but don't intend the bad effect they get the points for being

00:30:24,799 --> 00:30:30,000
bad um so like we have a tendency to not

00:30:30,000 --> 00:30:33,919
like our our intuition is that like people aren't responsible for doing good

00:30:33,919 --> 00:30:39,919
but they are responsible for doing bad um and then the other thing is so when i

00:30:39,919 --> 00:30:43,600
was thinking about this like more to the framing is is it's a

00:30:43,600 --> 00:30:48,000
large corporation that has designed the drone

00:30:48,000 --> 00:30:53,600
um so it and when i say large corporation i mean there's like a board

00:30:53,600 --> 00:30:59,120
there are like multiple people with chief in their title and many

00:30:59,120 --> 00:31:01,120
departments like the engineering department

00:31:01,120 --> 00:31:06,960
and the hr department um a legal department so there are like

00:31:06,960 --> 00:31:14,720
many actors like human actors now involved in this um and the drone

00:31:14,720 --> 00:31:18,640
wasn't like in i at least want to try this with the idea that the drone wasn't

00:31:18,640 --> 00:31:21,440
designed to kill people but it crashed into a kid

00:31:21,440 --> 00:31:28,240
and killed the kid that's sad i know it's sad i'm sorry it's fictional

00:31:28,240 --> 00:31:30,960
though right yeah it's fictional this didn't many

00:31:30,960 --> 00:31:33,840
terrible things have happened but this is not one of them because you start to

00:31:33,840 --> 00:31:36,960
feel differently if you assign that child a name and a story

00:31:36,960 --> 00:31:39,760
even though it shouldn't make a difference it starts to feel different

00:31:39,760 --> 00:31:42,000
right

00:31:44,880 --> 00:31:48,480
is it i mean and the other thing is this is it

00:31:48,480 --> 00:31:51,519
i guess it and then i kind of want to know like is it a like

00:31:51,519 --> 00:31:56,559
like freak weird accident there was like a terrible story about like a brick that

00:31:56,559 --> 00:32:00,080
killed a child and it was but it was like such a

00:32:00,080 --> 00:32:03,679
one in a zillion chance that that brick would have

00:32:03,679 --> 00:32:09,919
fallen the way that it fell you know uh like i guess we talking like a giant

00:32:09,919 --> 00:32:12,880
like car sized drone or are we talking like a little

00:32:12,880 --> 00:32:16,480
buzz thing that just kind of hit a vein you know i guess

00:32:16,480 --> 00:32:19,679
it's like do you have any reasonable expectation that the thing you built

00:32:19,679 --> 00:32:24,080
could have resulted in deaths was there like a big storm that geared

00:32:24,080 --> 00:32:28,240
it off course and no no big storm uh

00:32:28,240 --> 00:32:31,279
let's say the weather is good and they test they like

00:32:31,279 --> 00:32:36,000
they brought it to a test site and they thought the test site was gonna be

00:32:36,000 --> 00:32:42,240
okay um and the drone just like failed uh and

00:32:42,240 --> 00:32:47,440
it's not that it's like you know maybe some some size

00:32:47,440 --> 00:32:51,519
that if crashing into someone would cause some serious damage

00:32:51,519 --> 00:32:54,799
um i picked a kid just because it seemed sadder

00:32:54,799 --> 00:32:58,159
but if you want to you could be an adult or dinosaur

00:32:58,159 --> 00:33:01,360
that makes you feel better

00:33:01,679 --> 00:33:05,440
i like your alternate dinosaur extinction theory

00:33:05,440 --> 00:33:12,000
um time traveling drones whoops um i don't know

00:33:12,000 --> 00:33:15,279
then i have all these questions about like was the

00:33:15,279 --> 00:33:19,760
site clearly labeled no trespassing and there's like and i you know i

00:33:19,760 --> 00:33:24,000
grew up near like a lot of like army and navy installations and

00:33:24,000 --> 00:33:27,679
and there's definitely a difference between like oh we actually just don't

00:33:27,679 --> 00:33:30,880
want you to mess up the grass because it looks real nice here kind of signs

00:33:30,880 --> 00:33:35,600
and there's like no don't go here live ammunition is being used

00:33:35,600 --> 00:33:39,279
you know like because they know that kids like sort of ignore those

00:33:39,279 --> 00:33:43,919
soft like don't step on the grass no trespassing but like if you say like

00:33:43,919 --> 00:33:46,720
we're you we use live ammunition in here so please

00:33:46,720 --> 00:33:50,000
stay out because you might die it's i don't know

00:33:50,000 --> 00:33:54,080
and then is the kid old enough to read i don't yeah there's so many things

00:33:54,080 --> 00:33:57,519
and who is watching that kid like where was that kid's

00:33:57,519 --> 00:34:03,679
caregiver and like you know i i'm all for kids having the ability to

00:34:03,679 --> 00:34:06,640
move about freely but but at the same time like if there's

00:34:06,640 --> 00:34:11,119
this drone testing site yeah and maybe not before they're old

00:34:11,119 --> 00:34:15,760
enough to read like signs that warn them about stuff i find

00:34:15,760 --> 00:34:19,599
it interesting that

00:34:19,599 --> 00:34:26,639
at least where you are right now is like putting responsibility on the kid

00:34:27,440 --> 00:34:30,639
i well i think we're we're not doing that because we're like

00:34:30,639 --> 00:34:34,320
you know like signing are there steps are there steps

00:34:34,320 --> 00:34:38,240
like did people take reasonable steps to avoid anything bad happening to anyone

00:34:38,240 --> 00:34:41,359
and like why we're just trying to figure out

00:34:41,359 --> 00:34:43,440
like what i think is really happening is we

00:34:43,440 --> 00:34:45,200
don't want kids to get killed and we're like

00:34:45,200 --> 00:34:49,119
how do we prevent this like what needed to happen

00:34:49,119 --> 00:34:52,720
to avoid this which is i don't know if that's really like a productive

00:34:52,720 --> 00:34:56,079
thing i don't think that anyone is blaming the kid yeah

00:34:56,079 --> 00:35:02,640
okay yeah i think it's more like oh and i do think especially with minors

00:35:02,640 --> 00:35:06,640
like there is it is like the adult world's

00:35:06,640 --> 00:35:09,839
responsibility it's like if you have a pool in your backyard you have to have a

00:35:09,839 --> 00:35:12,240
fence around your yard you can't just like

00:35:12,240 --> 00:35:17,280
let people stumble into the pool and so there's like a recognition that if you

00:35:17,280 --> 00:35:22,079
have a dangerous thing that you it's sort of uh incumbent on

00:35:22,079 --> 00:35:27,200
the owner of the dangerous item to keep people that are not so savvy

00:35:27,200 --> 00:35:30,240
out of that item

00:35:30,960 --> 00:35:34,320
yeah a drone is totally an attractive nuisance like like

00:35:34,320 --> 00:35:38,000
if you've seen little kids see drones it's like the there's like

00:35:38,000 --> 00:35:41,839
the instinct is either to well to run away or to run towards it like

00:35:41,839 --> 00:35:46,000
one or the other because it's like this amazing thing

00:35:46,000 --> 00:35:51,200
um i guess like but like i guess you're right that details matter a lot because

00:35:51,200 --> 00:35:53,280
we're just trying to get at the heart of like

00:35:53,280 --> 00:35:57,520
you know what what went wrong because you need to understand what went wrong

00:35:57,520 --> 00:36:01,040
before you can assign blame if it's appropriate to

00:36:01,040 --> 00:36:03,760
assign blame

00:36:04,640 --> 00:36:09,200
yeah so i guess i mean i'm not i guess i wonder

00:36:09,200 --> 00:36:13,839
what do you like how much do you think that matters

00:36:13,839 --> 00:36:18,079
how much do you think it matters if like a kid wandered into a testing site

00:36:18,079 --> 00:36:22,240
or if the because it's an autonomous drone right so maybe

00:36:22,240 --> 00:36:25,440
there was something weird that happened within the drone and

00:36:25,440 --> 00:36:28,800
instead it went and crashed into a playground that was like on the other

00:36:28,800 --> 00:36:31,920
side of the testing site

00:36:32,480 --> 00:36:38,480
that's a poorly placed playground but or a poorly placed testing site yeah

00:36:38,480 --> 00:36:44,640
but something is poor there the actual regular philosophy examples

00:36:44,640 --> 00:36:47,520
are so dumb

00:36:48,000 --> 00:36:54,240
who even rides charlie's these days um so i guess like getting to the

00:36:54,240 --> 00:36:56,960
software part of it molly like do we think that this

00:36:56,960 --> 00:37:00,400
uh drone should have been able to recognize

00:37:00,400 --> 00:37:06,400
human forms and avoid them and if then is it like is it some

00:37:06,400 --> 00:37:09,040
software developers like for not getting us to do enough

00:37:09,040 --> 00:37:13,599
captchas of like you know push all the squares that have

00:37:13,599 --> 00:37:17,920
children in them yeah or more reasonable or like you know

00:37:17,920 --> 00:37:21,599
very easily to understand like did they include diverse pictures of

00:37:21,599 --> 00:37:24,960
you know of of of different kinds of different kids from

00:37:24,960 --> 00:37:28,240
you know who had different appearances in that training set

00:37:28,240 --> 00:37:31,839
like what if it's a kid in a wheelchair

00:37:34,839 --> 00:37:38,079
oh i mean you know i i think about this

00:37:38,079 --> 00:37:41,599
often because in free software licenses there's those big warranty

00:37:41,599 --> 00:37:45,200
you know disclaimers of warranties and um and things like that and

00:37:45,200 --> 00:37:50,079
um you know i i i think that you can go down a road where you make it seem like

00:37:50,079 --> 00:37:53,839
the situation is a lot more dire and worse for free software than it is for

00:37:53,839 --> 00:37:58,720
other um for proprietary software and i think that that's kind of false and so

00:37:58,720 --> 00:38:02,320
that really makes me nervous

00:38:03,040 --> 00:38:07,760
yeah then they can just put in biases that you can't look at

00:38:07,760 --> 00:38:12,800
yeah and i think that to be fair i think that the projects that are working on

00:38:12,800 --> 00:38:15,920
drone software in the free software space at least from what i can tell

00:38:15,920 --> 00:38:20,160
have been really careful about thinking through some of the liability issues and

00:38:20,160 --> 00:38:23,680
work hard to make sure that their software is used

00:38:23,680 --> 00:38:26,800
with responsibility

00:38:28,320 --> 00:38:34,640
i am so now that you brought up hey that's really interesting with me

00:38:34,640 --> 00:38:40,160
now that deb brought up software and developers it's like what i

00:38:40,160 --> 00:38:45,599
wonder about it is like so so there's blame right so

00:38:45,599 --> 00:38:48,640
there's like financial blame criminal blame and like

00:38:48,640 --> 00:38:51,440
just carrying the weight of having killed someone

00:38:51,440 --> 00:38:54,560
um and those are like three different types of punishment

00:38:54,560 --> 00:39:00,160
um but so like do you want to blame the board for

00:39:00,160 --> 00:39:07,119
approving the program and the budget do you want to blame

00:39:07,119 --> 00:39:11,200
i mean i mean maybe we're not at this point in a conversation

00:39:11,200 --> 00:39:14,560
but like i could see like oh you could blame

00:39:14,560 --> 00:39:19,839
the legal team for approve like for not doing something or you

00:39:19,839 --> 00:39:23,680
could blame the hr team for hiring the people who did this

00:39:23,680 --> 00:39:27,520
um i mean i don't think that's the best example but

00:39:27,520 --> 00:39:31,760
i guess it sort of depends on how much knowledge each person is given within

00:39:31,760 --> 00:39:35,359
their role like if so i mean and it depends on what kind of

00:39:35,359 --> 00:39:39,680
board you have like if it's like hey we're going to do a project a that

00:39:39,680 --> 00:39:42,800
did really well last year and uh attracted a

00:39:42,800 --> 00:39:47,599
lot of investments does everyone want to continue project a like a board that

00:39:47,599 --> 00:39:50,000
stamps that i mean it's you know they should

00:39:50,000 --> 00:39:52,320
probably ask some questions but if it's not

00:39:52,320 --> 00:39:55,440
you know it also depends on whether they're told like

00:39:55,440 --> 00:39:59,520
oh and if everyone remembers we carry a lot of extra insurance on project a

00:39:59,520 --> 00:40:04,000
because it's super dangerous or has the potential for harm

00:40:04,000 --> 00:40:06,960
yeah i mean i think procedures matter a lot in this context like and this is

00:40:06,960 --> 00:40:10,319
again back to your point initially molly about how the details do

00:40:10,319 --> 00:40:13,760
make a big difference um like i do know that like

00:40:13,760 --> 00:40:16,880
you know at conservancy for example when we're asked to do things like

00:40:16,880 --> 00:40:23,040
hire folks to do uh child care at conferences we have a much

00:40:23,040 --> 00:40:26,640
higher level of scrutiny for how we handle those contracts than how we

00:40:26,640 --> 00:40:30,000
handle other contracts because there's a responsibility inherent in

00:40:30,000 --> 00:40:34,079
that work and like i think some of it comes back to like what was

00:40:34,079 --> 00:40:37,200
the goal of like what what was the scope of the

00:40:37,200 --> 00:40:40,720
of the work that was being done by the you know the team that worked on

00:40:40,720 --> 00:40:43,200
this drone like what were they trying to do and did they

00:40:43,200 --> 00:40:46,480
spec out what you know like what could go wrong

00:40:46,480 --> 00:40:49,839
in this way and like before you can i don't know

00:40:49,839 --> 00:40:55,280
i i i promise you that if a child was killed by a drone every single one of

00:40:55,280 --> 00:41:00,079
those people that you named would feel really really bad about it

00:41:00,880 --> 00:41:04,240
that's true and would work hard to avoid that you know

00:41:04,240 --> 00:41:08,560
that possibly happening or if it happened ever again

00:41:08,560 --> 00:41:12,560
it would probably cause some people to quit their drone company job

00:41:12,580 --> 00:41:16,640
we would hope that but uh having worked in an environmental law

00:41:16,640 --> 00:41:21,920
like um you know uh i worked on a case where we were suing a company that

00:41:21,920 --> 00:41:24,480
was essentially causing cancer through

00:41:24,480 --> 00:41:27,440
pollution and they fought tooth and nail because

00:41:27,440 --> 00:41:30,000
it was really really expensive for them to not

00:41:30,000 --> 00:41:36,640
pollute that part of louisiana um and i'm not sure how they sleep at night

00:41:36,640 --> 00:41:42,400
or rationalize it if they just have better insomnia aids than the rest of us

00:41:42,400 --> 00:41:46,079
but um i don't i sadly don't think

00:41:46,079 --> 00:41:48,880
that's always the case like i think people

00:41:48,880 --> 00:41:53,920
some most i hope most people behave and react the way that you just described

00:41:53,920 --> 00:41:59,599
karen but i think that some people are capable of distancing themselves in

00:41:59,599 --> 00:42:03,280
a way that allows them to say like well that was

00:42:03,280 --> 00:42:06,319
just a fluke or you know they shouldn't have been living

00:42:06,319 --> 00:42:10,720
there in the shadow of our plant or you know letting their kids run off

00:42:10,720 --> 00:42:15,839
all unsupervised and somehow shifting that blame because the

00:42:15,839 --> 00:42:19,040
like corporations still do cause people's

00:42:19,040 --> 00:42:21,359
deaths

00:42:24,560 --> 00:42:27,760
well on that note which i think is a great note

00:42:27,760 --> 00:42:35,680
uh that was 15 minutes of your time so thanks molly

00:42:36,000 --> 00:42:39,680
i'm really sad now but it was still a nice conversation

00:42:39,680 --> 00:42:43,440
yeah i hope you weren't looking for an uplifting like kind of

00:42:43,440 --> 00:42:47,200
fun good place sort of romp in the ethics

00:42:47,200 --> 00:42:51,440
right now so there's a question do you think there will come a time

00:42:51,440 --> 00:42:54,880
when the osi will approve a license that includes ethical requirements

00:42:54,880 --> 00:42:59,119
can one be written that satisfies the current osd is a time to rethink the osd

00:42:59,119 --> 00:43:02,720
this is a an interesting question especially as

00:43:02,720 --> 00:43:08,319
uh someone who used to be on the board of the osi and the president of the osi

00:43:08,319 --> 00:43:16,960
i would say that so i'll start by saying i can't

00:43:16,960 --> 00:43:22,480
conceive of a license that would meet the osd but still contain

00:43:22,480 --> 00:43:28,400
ethical considerations um uh

00:43:29,280 --> 00:43:33,839
but uh while i am a great thinker on many things i am not

00:43:33,839 --> 00:43:38,160
a great legal thinker uh and i am not a great licensing thinker

00:43:38,160 --> 00:43:41,599
um so maybe i just don't have enough

00:43:41,599 --> 00:43:46,800
imagination in that space um

00:43:46,800 --> 00:43:49,520
i don't

00:43:49,920 --> 00:43:55,520
think i love open source but i don't think it's the answer to every problem

00:43:55,520 --> 00:43:59,680
in the world um and so

00:43:59,680 --> 00:44:06,319
if you're looking to find a way to talk about licenses that have these

00:44:06,319 --> 00:44:11,839
these these uh restrictions around use like talking about them in some other

00:44:11,839 --> 00:44:14,880
context other than open source like there's nothing wrong with that right

00:44:14,880 --> 00:44:18,640
not everything has to be open um or open source

00:44:18,640 --> 00:44:24,800
uh so like i think this is a difficult conversation to have and kind of the the

00:44:24,800 --> 00:44:27,040
question of whether it's time to rethink the osd is

00:44:27,040 --> 00:44:31,839
is i don't think we should rethink the osd i think the osd is fine as it is

00:44:31,839 --> 00:44:37,920
um but i think there is space to consider other things like i'll make a

00:44:37,920 --> 00:44:40,640
shameless pitch here to say that you know karen sandler and i've been

00:44:40,640 --> 00:44:43,359
doing a lot of thinking and talking about digital autonomy over the past

00:44:43,359 --> 00:44:47,280
well we've been thinking about it for maybe a year now um and we've been doing

00:44:47,280 --> 00:44:50,000
some writing over the past few months and we've started talking about

00:44:50,000 --> 00:44:54,400
it um and so like i'll make a shameless pitch to say you should

00:44:54,400 --> 00:45:00,000
look into that because you know we think in terms of digital autonomy we think

00:45:00,000 --> 00:45:03,599
open source and free software are part of the picture but not the

00:45:03,599 --> 00:45:06,640
entirety of the picture

00:45:07,040 --> 00:45:10,079
so that's the question

00:45:10,800 --> 00:45:14,240
oh now there's some chat stuff oh i got a like

00:45:14,240 --> 00:45:17,280
these are some cool things in the chat um

00:45:17,280 --> 00:45:21,680
does anyone have any other questions i'd i'd love to answer questions and if

00:45:21,680 --> 00:45:24,880
people would like to

00:45:24,960 --> 00:45:30,640
try to bring the conversation from the video about autonomous drones

00:45:30,640 --> 00:45:34,720
live this is like the only time you're ever gonna take me uh get me requesting

00:45:34,720 --> 00:45:41,119
comments and not questions um so this is your chance to talk at me

00:45:41,119 --> 00:45:48,160
and have me listen um so i'll so so some things i'll just talk

00:45:48,160 --> 00:45:49,920
for a little bit until somebody interrupts me

00:45:49,920 --> 00:45:53,839
um so one of the things i didn't address in this

00:45:53,839 --> 00:45:59,040
talk is um you know i talked about these these seven principles of public health

00:45:59,040 --> 00:46:03,440
ethics and these principles of ethics um and

00:46:03,440 --> 00:46:07,280
but there are there i think this is a really big field it turns

00:46:07,280 --> 00:46:13,520
out uh you can get a whole degree in it um and still not cover everything

00:46:13,520 --> 00:46:20,720
uh so some of the main ways we talk about ethics is we divide them into

00:46:20,720 --> 00:46:23,599
different categories and those categories include things like

00:46:23,599 --> 00:46:28,240
consequentialism so looking at the results of actions and like judging

00:46:28,240 --> 00:46:31,359
whether an action is a good action or a bad action because of the results

00:46:31,359 --> 00:46:37,440
um there are things like like deontology which is oh more questions in the shared

00:46:37,440 --> 00:46:40,800
notes cool um well there's deontology and value ethics

00:46:40,800 --> 00:46:42,880
and i'll answer your other questions first

00:46:42,880 --> 00:46:49,440
and then ramble more about that um uh someone is still typing here's a

00:46:49,440 --> 00:46:52,960
follow-up question from mccoy smith if licenses aren't the best mechanism to

00:46:52,960 --> 00:46:56,560
address ethics and software or more broadly technical creations of

00:46:56,560 --> 00:47:01,680
individuals what is um so

00:47:01,680 --> 00:47:07,839
i think collective organizing is probably the best tool we have

00:47:07,839 --> 00:47:14,079
um and i say collective organizing um

00:47:15,520 --> 00:47:19,359
let's scratch it let's restart the sentence um

00:47:19,359 --> 00:47:22,839
i think it's a really great tool because it

00:47:22,839 --> 00:47:26,720
enables people to make the decisions about what they're doing and what

00:47:26,720 --> 00:47:28,720
they're creating what's permissible and what isn't

00:47:28,720 --> 00:47:35,200
permissible when we look at history we see successful movements

00:47:35,200 --> 00:47:39,119
we see success in movements come out of groups of people coming together to make

00:47:39,119 --> 00:47:43,440
something happen rather than just relying on like legal

00:47:43,440 --> 00:47:46,480
hacks and the laws themselves to make those things happen

00:47:46,480 --> 00:47:52,319
um so i would like to see more people refusing to do work that they think is

00:47:52,319 --> 00:47:57,520
unethical i would like to see more people um you know

00:47:57,520 --> 00:48:01,200
educating themselves and educating each other to

00:48:01,200 --> 00:48:05,280
uh like of these issues so that they are in places to make these sorts of

00:48:05,280 --> 00:48:10,559
decisions um uh

00:48:10,559 --> 00:48:13,760
yeah so that's that's i think like the first

00:48:13,760 --> 00:48:17,599
that's like i think a very important thing um and i have

00:48:17,599 --> 00:48:21,839
like fantasy plans about this too that i'm happy to talk about later that i

00:48:21,839 --> 00:48:27,119
won't talk about right now um a question next question

00:48:27,119 --> 00:48:30,720
you and karen have discussed how the idea of digital autonomy affects people

00:48:30,720 --> 00:48:33,520
who want licenses to be open but also really

00:48:33,520 --> 00:48:37,599
loathe that folks who use tech who use tech we build for

00:48:37,599 --> 00:48:39,680
evil things putting kids in cages human

00:48:39,680 --> 00:48:43,599
trafficking how can things be allowed to be permissible

00:48:43,599 --> 00:48:49,839
but with a caveat of not using software for ill

00:48:52,079 --> 00:48:59,839
that's a really good question that i don't have an immediate

00:48:59,839 --> 00:49:03,200
firm answer to um but i think a lot of that

00:49:03,200 --> 00:49:07,920
does so i think there i think there's like a lot of different

00:49:07,920 --> 00:49:13,040
stuff going on in the question of like

00:49:13,040 --> 00:49:18,240
i'm gonna say ethics and technology but really what i mean is like

00:49:18,240 --> 00:49:21,839
software and technology being used for bad and like

00:49:21,839 --> 00:49:30,160
the licensing aspect of it um because like let's say

00:49:30,160 --> 00:49:34,319
from a practical standpoint i just think it's frankly unlikely that if we adopt

00:49:34,319 --> 00:49:37,359
that if like we create do no harm licenses like

00:49:37,359 --> 00:49:40,559
that the people who need to be adopting them will be adopting them

00:49:40,559 --> 00:49:45,280
um i think that companies have shown that they are quite happy

00:49:45,280 --> 00:49:50,000
to pay uh for like large companies are happy to use free code but they're also

00:49:50,000 --> 00:49:55,040
happy to pay for it when they need to um that happened i think

00:49:55,040 --> 00:50:02,000
with amazon not too long ago um for example

00:50:02,000 --> 00:50:08,240
i think that um like i was saying with mccoy's

00:50:08,240 --> 00:50:11,200
question i think this is really similar which is just like

00:50:11,200 --> 00:50:14,400
we need to be taking active stances we need to be

00:50:14,400 --> 00:50:20,480
taking taking we need to be actively engaging we need to be taking

00:50:20,480 --> 00:50:24,000
on roles as activists we need to be taking on the

00:50:24,000 --> 00:50:29,760
roles of social justice your class here um we need to

00:50:29,760 --> 00:50:33,520
be fighting the good fight in all sorts of ways and that like

00:50:33,520 --> 00:50:37,040
software definitely enables abuse and software enables terrible terrible

00:50:37,040 --> 00:50:39,920
things but by simply closing the software we're

00:50:39,920 --> 00:50:42,160
not going to be solving those problems like

00:50:42,160 --> 00:50:46,400
the things that bring about that evil are still there

00:50:46,400 --> 00:50:49,839
um and i'll get to some more questions and then get

00:50:49,839 --> 00:50:54,000
i'm happy to rant more about that later um is failing to deal with an unexpected

00:50:54,000 --> 00:50:58,480
circumstance a bug or more to the point how weird does the odd event have to be

00:50:58,480 --> 00:51:00,720
before we can shrug it off our shoulders and say

00:51:00,720 --> 00:51:04,640
well no one could have foreseen dot dot and i'm going to

00:51:04,640 --> 00:51:08,480
and i'm assuming that an autonomous system is not being regarded as

00:51:08,480 --> 00:51:12,480
intelligent uh i i have this joke where i say i think

00:51:12,480 --> 00:51:17,440
artificial intelligence is boring um but uh that's a different

00:51:17,440 --> 00:51:22,480
conversation too um i think that

00:51:22,480 --> 00:51:29,040
okay so a person crashes a car um and we're like well

00:51:29,040 --> 00:51:35,520
you know a deer ran out into the street and then the person crashed a car and

00:51:35,520 --> 00:51:38,640
hit another person and another person died we were like

00:51:38,640 --> 00:51:41,280
well no one could have kept that from

00:51:41,280 --> 00:51:46,319
happening um can't stop a deer uh but if it's a an

00:51:46,319 --> 00:51:51,520
autonomous vehicle and then suddenly like the same thing happens

00:51:51,520 --> 00:51:54,720
you know we have a very different conversation on our hands

00:51:54,720 --> 00:52:02,000
um something that i meant to mention uh in the recording is that um one of the

00:52:02,000 --> 00:52:06,800
prevailing theories is that people should there should always be a person

00:52:06,800 --> 00:52:12,800
watching an autonomous like creation moving um and like

00:52:12,800 --> 00:52:19,119
monitoring what it's doing um because like the fact is people aren't

00:52:19,119 --> 00:52:21,520
perfect so we have accidents to make mistakes

00:52:21,520 --> 00:52:24,880
and it's very reasonable that that computers and machines are also

00:52:24,880 --> 00:52:29,040
going to be making mistakes especially as we put them off on their own and say

00:52:29,040 --> 00:52:35,359
you know do the thing um uh but the blame but

00:52:35,359 --> 00:52:40,000
blamers really responsibilities it's a really different consideration

00:52:40,000 --> 00:52:43,359
um because responsibility is dispersed but also

00:52:43,359 --> 00:52:46,480
because responsibility is legislated right

00:52:46,480 --> 00:52:52,240
um so one of the things with autonomous vehicles is like people have just

00:52:52,240 --> 00:52:55,920
calculated how many people like how many deaths are okay in the

00:52:55,920 --> 00:53:00,079
testing of autonomous cars self-driving cars and

00:53:00,079 --> 00:53:02,960
someone put a number on like what an acceptable

00:53:02,960 --> 00:53:05,920
loss is uh and and like these things have

00:53:05,920 --> 00:53:08,240
happened or are happening from legislative

00:53:08,240 --> 00:53:14,880
standpoints um uh and that's that's something that like

00:53:14,880 --> 00:53:18,640
really hits people's intuitions and is like for me

00:53:18,640 --> 00:53:22,000
something i find kind of disgusting um and i could

00:53:22,000 --> 00:53:24,319
i could talk through why and come up with an answer as to why i find it

00:53:24,319 --> 00:53:29,839
disgusting um but i think that's like

00:53:30,000 --> 00:53:34,079
that's an interesting question and and part of that

00:53:34,079 --> 00:53:37,359
comes part of my responses come from like

00:53:37,359 --> 00:53:43,839
my particular value sets

00:53:49,839 --> 00:53:51,920

YouTube URL: https://www.youtube.com/watch?v=MZIkaaHd2_Y


