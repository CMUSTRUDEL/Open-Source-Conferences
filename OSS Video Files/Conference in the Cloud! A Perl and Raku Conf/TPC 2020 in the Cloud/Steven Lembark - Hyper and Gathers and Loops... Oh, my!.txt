Title: Steven Lembark - Hyper and Gathers and Loops... Oh, my!
Publication date: 2020-06-25
Playlist: TPC 2020 in the Cloud
Description: 
	Raku provides a variety of mechanisms for graceful threaded processing. Queues are nice, handle all of the bookkeeping, and have a fair bit of overhead. Lazy gathers require a bit more planning but can be quite a bit faster with race() or hyper() to make them happen. This talk looks at examples of each, describing the syntax for using them and showing the performance tradeoffs and some ways to benchmark them in your own environment.
Captions: 
	00:00:00,030 --> 00:00:03,689
okay I have four o'clock right now I

00:00:02,220 --> 00:00:09,269
have four o'clock right now if you want

00:00:03,689 --> 00:00:10,860
to start okay so hi I'm Steve I I'm

00:00:09,269 --> 00:00:17,430
gonna be describing a few things I've

00:00:10,860 --> 00:00:20,480
done to process data in parallel and how

00:00:17,430 --> 00:00:23,130
Rick who makes it a lot easier now so

00:00:20,480 --> 00:00:25,380
the case I'm gonna be showing you

00:00:23,130 --> 00:00:28,710
there's a file called NR GZ that's

00:00:25,380 --> 00:00:32,910
published from an NCBI and it's supposed

00:00:28,710 --> 00:00:35,460
to be a list of non-redundant amino acid

00:00:32,910 --> 00:00:38,100
sequences for everything known so

00:00:35,460 --> 00:00:40,260
instead of showing you sequences by

00:00:38,100 --> 00:00:42,149
thing it shows you all the things that

00:00:40,260 --> 00:00:44,510
have one sequence in it and the

00:00:42,149 --> 00:00:49,410
sequencers is supposed to be unique so

00:00:44,510 --> 00:00:53,960
it's it's about a 123 eighth of a

00:00:49,410 --> 00:00:56,640
terabyte with a lot of sequences in it

00:00:53,960 --> 00:01:01,590
the sequences themselves some of them

00:00:56,640 --> 00:01:05,970
are tiny great cat I've got a peanut

00:01:01,590 --> 00:01:08,700
gallery here and just to give you an

00:01:05,970 --> 00:01:12,750
idea the size just catting it can take

00:01:08,700 --> 00:01:15,600
an hour and change so the question is is

00:01:12,750 --> 00:01:17,909
it really unique ncbi doesn't always

00:01:15,600 --> 00:01:19,619
have perfect quality control and there

00:01:17,909 --> 00:01:21,720
are times when they will duplicate some

00:01:19,619 --> 00:01:24,000
of the sequences and if you're loading

00:01:21,720 --> 00:01:27,600
all of this into a database and it's

00:01:24,000 --> 00:01:30,119
wrong it's a real hassle so doing the QA

00:01:27,600 --> 00:01:31,710
on it is just sort of a hobby of mine

00:01:30,119 --> 00:01:34,560
these guys have learned a dislike email

00:01:31,710 --> 00:01:37,619
from me but the problem is you've got

00:01:34,560 --> 00:01:39,119
that much data that's that variable how

00:01:37,619 --> 00:01:43,970
do you actually validate the uniqueness

00:01:39,119 --> 00:01:46,890
I mean it starts as blast format which

00:01:43,970 --> 00:01:48,360
looks like greater than followed by a

00:01:46,890 --> 00:01:51,240
header line and the header line could be

00:01:48,360 --> 00:01:53,610
a megabyte as long as it ends in the

00:01:51,240 --> 00:01:56,340
first new line then it gets sequences

00:01:53,610 --> 00:01:59,430
and another header end of file ends the

00:01:56,340 --> 00:02:05,009
last sequence so there's no real marker

00:01:59,430 --> 00:02:07,770
for end to compare all of these I got to

00:02:05,009 --> 00:02:11,039
filter out the headings I then extract

00:02:07,770 --> 00:02:13,800
the sequences if I wanted to compare

00:02:11,039 --> 00:02:15,960
them yeah that's only this many pairs

00:02:13,800 --> 00:02:17,940
for an upper triangular and at a

00:02:15,960 --> 00:02:20,610
kilohertz you know I'll call it a

00:02:17,940 --> 00:02:22,290
million years that might not be fast

00:02:20,610 --> 00:02:24,630
enough to keep up with the release cycle

00:02:22,290 --> 00:02:27,360
from NCBI so I had to figure out a

00:02:24,630 --> 00:02:28,650
better way one people one thing

00:02:27,360 --> 00:02:30,510
everyone's thinking yeah that's easy you

00:02:28,650 --> 00:02:32,940
don't compare all the sequences Esha

00:02:30,510 --> 00:02:36,450
them and all the sh asel work it out and

00:02:32,940 --> 00:02:39,180
the problem is that sha-512 with salt

00:02:36,450 --> 00:02:41,490
using multiple stacked as hundreds of

00:02:39,180 --> 00:02:43,560
collisions the problem is the data is so

00:02:41,490 --> 00:02:46,290
low entropy because you're using an

00:02:43,560 --> 00:02:48,810
entire byte to store 20 items there is

00:02:46,290 --> 00:02:51,690
so much similarity in it that it doesn't

00:02:48,810 --> 00:02:54,270
really work very well for digesting

00:02:51,690 --> 00:02:55,740
there's no way to just use digest and

00:02:54,270 --> 00:02:58,740
compare them so what I end up having to

00:02:55,740 --> 00:03:02,670
do is extract sequences and I generate

00:02:58,740 --> 00:03:06,000
two files one is God lengths and digests

00:03:02,670 --> 00:03:10,350
and then I've got a record number with

00:03:06,000 --> 00:03:12,930
the sequence I can sort merge than the

00:03:10,350 --> 00:03:15,870
length numbers find the duplicates in

00:03:12,930 --> 00:03:17,160
there which is a small enough pool that

00:03:15,870 --> 00:03:19,739
I can then go back and compare the

00:03:17,160 --> 00:03:23,520
sequences so this is what I'm actually

00:03:19,739 --> 00:03:25,320
doing now I'm reading this stuff the

00:03:23,520 --> 00:03:27,930
input stream because it's a variable

00:03:25,320 --> 00:03:30,989
sized record there's no good way to do

00:03:27,930 --> 00:03:32,940
that in parallel what I end up doing is

00:03:30,989 --> 00:03:35,459
extracting the sequences and then

00:03:32,940 --> 00:03:39,540
processing them in parallel and passing

00:03:35,459 --> 00:03:42,239
them off to worker threads and everyone

00:03:39,540 --> 00:03:44,459
here is used thread pools right I I

00:03:42,239 --> 00:03:46,590
guess we can't all raise our hand but

00:03:44,459 --> 00:03:49,380
who really enjoys fork execs you know

00:03:46,590 --> 00:03:51,239
you you get to four and if it's not if

00:03:49,380 --> 00:03:52,739
it isn't and then you got the process

00:03:51,239 --> 00:03:54,180
communication you can exchange the data

00:03:52,739 --> 00:03:56,910
you got to deal with the data types

00:03:54,180 --> 00:03:58,200
you've got to deal with all of the sum

00:03:56,910 --> 00:03:59,459
of force to make sure that everything

00:03:58,200 --> 00:04:01,200
gets locked you got to put it all back

00:03:59,459 --> 00:04:06,959
in order you got to spit it out to get

00:04:01,200 --> 00:04:11,970
it aside nobody wants this and riku is

00:04:06,959 --> 00:04:14,040
very good at ending your pain so the

00:04:11,970 --> 00:04:17,489
first thing is we've all written

00:04:14,040 --> 00:04:18,870
something that starts at zero increments

00:04:17,489 --> 00:04:21,150
and every time you go through a loop you

00:04:18,870 --> 00:04:23,640
print the count and you've always said

00:04:21,150 --> 00:04:24,630
gee I just maybe it's locked up I don't

00:04:23,640 --> 00:04:26,479
I haven't seen an account in a while

00:04:24,630 --> 00:04:28,590
everyone wants that little in

00:04:26,479 --> 00:04:31,860
gratification of seeing the count show

00:04:28,590 --> 00:04:33,600
up on the screen now I could just add a

00:04:31,860 --> 00:04:37,020
loop sleep five seconds and show me the

00:04:33,600 --> 00:04:38,190
count but that doesn't do much for my

00:04:37,020 --> 00:04:41,729
program because then all it's going to

00:04:38,190 --> 00:04:43,380
do is report every five seconds this is

00:04:41,729 --> 00:04:45,419
as much as it takes to start a new

00:04:43,380 --> 00:04:50,370
thread looping sleeping every five

00:04:45,419 --> 00:04:53,280
seconds and showing me the count so Riku

00:04:50,370 --> 00:04:56,220
makes this a lot easier we're done here

00:04:53,280 --> 00:04:58,860
I mean this this is the core of eight if

00:04:56,220 --> 00:05:00,960
I want to have a generic writer thread

00:04:58,860 --> 00:05:04,259
we've all had again the fork exec that

00:05:00,960 --> 00:05:06,630
opens standard in for gzip and writes

00:05:04,259 --> 00:05:10,520
out a bunch of gzipped output to a file

00:05:06,630 --> 00:05:18,780
I can do that here with a proc async and

00:05:10,520 --> 00:05:21,630
I take gzip this here runs a gzip I can

00:05:18,780 --> 00:05:24,330
open an output file here's where I bind

00:05:21,630 --> 00:05:30,630
the standard out and then I started no

00:05:24,330 --> 00:05:33,090
Forks no execs no funny logic if I want

00:05:30,630 --> 00:05:34,740
to read from input this is what I

00:05:33,090 --> 00:05:36,449
usually end up doing because most of the

00:05:34,740 --> 00:05:40,349
time when you're reading input you're

00:05:36,449 --> 00:05:42,870
gonna read it in a block so by up with

00:05:40,349 --> 00:05:46,740
run that's going to create a thread

00:05:42,870 --> 00:05:49,440
running an external program which then

00:05:46,740 --> 00:05:52,560
the colon out says I want to capture its

00:05:49,440 --> 00:05:56,789
standard output the dot out returns me

00:05:52,560 --> 00:06:00,690
that file handle which I then map on to

00:05:56,789 --> 00:06:03,139
input so now I've got an input file

00:06:00,690 --> 00:06:06,599
handle that's available inside the block

00:06:03,139 --> 00:06:09,750
the neat thing about width is it works

00:06:06,599 --> 00:06:14,039
sort of like else but it's examines

00:06:09,750 --> 00:06:18,719
undef so as I map this I can have an

00:06:14,039 --> 00:06:20,849
else that says this is why I failed so

00:06:18,719 --> 00:06:24,090
it's it the logic is pretty clean I'll

00:06:20,849 --> 00:06:26,909
show a complete program later so I've

00:06:24,090 --> 00:06:29,159
got to read the sequences now the easy

00:06:26,909 --> 00:06:32,969
way to do it would have been realign

00:06:29,159 --> 00:06:37,589
with an end of record equal to a greater

00:06:32,969 --> 00:06:40,169
than and unfortunately that's broken and

00:06:37,589 --> 00:06:44,340
rate I get this line reads after

00:06:40,169 --> 00:06:48,090
22:17 egg every time so I had a convert

00:06:44,340 --> 00:06:54,120
to fixed size reads to do that

00:06:48,090 --> 00:06:57,749
I do a read into a buffer and what I

00:06:54,120 --> 00:06:59,099
keep doing is looking for I'm treating

00:06:57,749 --> 00:07:03,240
these greater than as if they were

00:06:59,099 --> 00:07:06,300
terminators I find a chunk or I find no

00:07:03,240 --> 00:07:10,460
input at that point each time I read one

00:07:06,300 --> 00:07:14,520
of these a read returns in in Roku a a

00:07:10,460 --> 00:07:16,710
blob you have to decode it to make it a

00:07:14,520 --> 00:07:18,409
string thankfully these are ASCII so

00:07:16,710 --> 00:07:21,180
there's almost no processing on them

00:07:18,409 --> 00:07:25,469
there's a balance and threading between

00:07:21,180 --> 00:07:27,659
the size of what I do and its speed and

00:07:25,469 --> 00:07:30,419
the number of threads if what I do takes

00:07:27,659 --> 00:07:32,629
too long threads sit idle if it takes

00:07:30,419 --> 00:07:36,110
too little time I can't feed them enough

00:07:32,629 --> 00:07:39,330
so one Meg reads with about a 16 Meg

00:07:36,110 --> 00:07:42,360
chunk of data turned out to be right to

00:07:39,330 --> 00:07:45,360
this ready the last thing I had to deal

00:07:42,360 --> 00:07:47,129
with on this end was the fact that my

00:07:45,360 --> 00:07:49,469
individual chunks don't end on records

00:07:47,129 --> 00:07:51,870
so I've read all this stuff in I know

00:07:49,469 --> 00:07:55,889
there's a greater than I've got to store

00:07:51,870 --> 00:07:59,430
it for reuse so this is the core of what

00:07:55,889 --> 00:08:03,330
I have that is going to be threaded for

00:07:59,430 --> 00:08:06,689
reading now I have to process the jokes

00:08:03,330 --> 00:08:08,580
and what approach is a channel which

00:08:06,689 --> 00:08:10,409
you've got a bunch of subscribers

00:08:08,580 --> 00:08:11,759
writing a bunch of subscribers reading

00:08:10,409 --> 00:08:14,699
everything goes through it everything's

00:08:11,759 --> 00:08:17,520
kept in order it's beautiful there's a

00:08:14,699 --> 00:08:20,490
really nice description of it the three

00:08:17,520 --> 00:08:21,919
CUDA or dock site is excellent the

00:08:20,490 --> 00:08:24,629
people that have worked on this thing

00:08:21,919 --> 00:08:25,460
they deserve a beer from all of us it's

00:08:24,629 --> 00:08:27,270
wonderful

00:08:25,460 --> 00:08:29,310
problem is it doesn't work it's way too

00:08:27,270 --> 00:08:32,339
slow for what I'm doing it's really

00:08:29,310 --> 00:08:34,800
designed to keep a lot of things in sync

00:08:32,339 --> 00:08:36,779
with multiple people going in multiple

00:08:34,800 --> 00:08:40,289
processes going out and it does too much

00:08:36,779 --> 00:08:42,779
bookkeeping what I found is dr. Seuss I

00:08:40,289 --> 00:08:45,540
went back to the first grade so I've got

00:08:42,779 --> 00:08:47,279
to gather in a take to do the reading in

00:08:45,540 --> 00:08:49,620
a lazy loop with a map and a race

00:08:47,279 --> 00:08:53,670
everyone turned their mics on you just

00:08:49,620 --> 00:08:57,260
let's say together and take them the

00:08:53,670 --> 00:09:01,770
so the trick here is I gather intake

00:08:57,260 --> 00:09:03,900
replace the fount everyone's done this

00:09:01,770 --> 00:09:05,640
you can you create an array and you push

00:09:03,900 --> 00:09:08,000
onto it and you push another one every

00:09:05,640 --> 00:09:12,300
day you push it and then you go and you

00:09:08,000 --> 00:09:14,280
gather handles a block and whatever

00:09:12,300 --> 00:09:17,580
comes whatever is taken inside of the

00:09:14,280 --> 00:09:18,960
block the gather returns one thing ten

00:09:17,580 --> 00:09:22,020
things a hundred things gather just

00:09:18,960 --> 00:09:23,790
deals with it so all of that variable

00:09:22,020 --> 00:09:29,540
and is it in scope and out of scope and

00:09:23,790 --> 00:09:32,730
when it's just vanished so a simple

00:09:29,540 --> 00:09:35,370
Handler to read all of these chunks that

00:09:32,730 --> 00:09:40,260
I just shown you is here I have a gather

00:09:35,370 --> 00:09:43,440
of a loop each time I have a chunk or

00:09:40,260 --> 00:09:46,590
I'm finished that exits the loop for

00:09:43,440 --> 00:09:49,170
each one of these I take it now again

00:09:46,590 --> 00:09:51,390
inside of this block I could modify

00:09:49,170 --> 00:09:52,920
things I can have five different takes I

00:09:51,390 --> 00:09:55,320
can have a switch from Hell deciding

00:09:52,920 --> 00:09:57,600
what to take but any take that's called

00:09:55,320 --> 00:10:01,560
in this block is output by that gather

00:09:57,600 --> 00:10:04,230
which the problem is I don't want all of

00:10:01,560 --> 00:10:06,810
it a lot of piece of it I do only need

00:10:04,230 --> 00:10:12,350
one of these at a time so I can make

00:10:06,810 --> 00:10:15,480
that gather lazy and a lazy gather is

00:10:12,350 --> 00:10:18,090
generates nothing when I assign it all

00:10:15,480 --> 00:10:20,130
it is is a promise that the next time

00:10:18,090 --> 00:10:22,920
you ask for something out of that array

00:10:20,130 --> 00:10:27,690
the gather is going to be called to

00:10:22,920 --> 00:10:30,660
populate it so what happens is gab now

00:10:27,690 --> 00:10:33,540
my gather is called on demand and it

00:10:30,660 --> 00:10:37,050
feeds that array with whatever I want to

00:10:33,540 --> 00:10:43,470
take out of it but until you ask for it

00:10:37,050 --> 00:10:46,290
the array doesn't exist so now I can

00:10:43,470 --> 00:10:48,690
take this this lazy array that it didn't

00:10:46,290 --> 00:10:50,340
fill up any memory yet and I can put it

00:10:48,690 --> 00:10:51,780
into a map which thankfully these you

00:10:50,340 --> 00:10:52,790
don't have to stand on the ceiling to

00:10:51,780 --> 00:10:56,400
read maps anymore

00:10:52,790 --> 00:11:01,020
so I'm pushing all the content to this

00:10:56,400 --> 00:11:05,130
array through the map the thing is I

00:11:01,020 --> 00:11:07,170
want to do it in parallel all of the

00:11:05,130 --> 00:11:09,089
bookkeeping for handle

00:11:07,170 --> 00:11:10,799
dispatching threads putting things out

00:11:09,089 --> 00:11:14,429
getting them back getting them in order

00:11:10,799 --> 00:11:17,910
tracking everything is in hyper hyper

00:11:14,429 --> 00:11:20,309
modifies the map not the array so

00:11:17,910 --> 00:11:24,089
basically hyper sucks up the map and

00:11:20,309 --> 00:11:27,379
says I'm gonna run this in way threads

00:11:24,089 --> 00:11:30,720
parallel that's this degree down here

00:11:27,379 --> 00:11:32,669
the batch says well dollar underscore

00:11:30,720 --> 00:11:33,899
you might want one of them you might

00:11:32,669 --> 00:11:35,669
want to accumulate ten dollar

00:11:33,899 --> 00:11:39,749
underscores and pass all of them as a

00:11:35,669 --> 00:11:42,660
list into processed junk in my case one

00:11:39,749 --> 00:11:45,749
at a time is fine but I I can choose the

00:11:42,660 --> 00:11:49,230
degree of parallelization that it has at

00:11:45,749 --> 00:11:51,899
that point the hyper and Map thread

00:11:49,230 --> 00:11:53,790
things for me notice you haven't seen a

00:11:51,899 --> 00:11:56,069
fork you haven't seen a semaphore you

00:11:53,790 --> 00:11:58,410
haven't seen blocking you haven't seen

00:11:56,069 --> 00:11:59,759
anything about who's doing what in any

00:11:58,410 --> 00:12:02,399
order it just happens

00:11:59,759 --> 00:12:04,980
the problem is hyper waste a lot of time

00:12:02,399 --> 00:12:07,290
because the size of the blocks that I'm

00:12:04,980 --> 00:12:09,089
dealing with very enormous like the

00:12:07,290 --> 00:12:11,129
header is some of the short sequences

00:12:09,089 --> 00:12:14,399
are used in a million different species

00:12:11,129 --> 00:12:16,649
the long legs might be used by ten so

00:12:14,399 --> 00:12:18,779
the amount of data in one chunk varies

00:12:16,649 --> 00:12:20,819
so much that if I wait to get them all

00:12:18,779 --> 00:12:23,249
done in order I'm wasting a lot of time

00:12:20,819 --> 00:12:25,230
I don't care about the order so I can

00:12:23,249 --> 00:12:28,199
replace the hyper with this race and

00:12:25,230 --> 00:12:31,529
what race will do is the first time it

00:12:28,199 --> 00:12:35,220
sees any thread complete it will feed it

00:12:31,529 --> 00:12:38,160
with a new dollar underscore so as often

00:12:35,220 --> 00:12:40,589
as threads are completing I'm going back

00:12:38,160 --> 00:12:42,720
into that lazy gather I'm having it do

00:12:40,589 --> 00:12:45,660
one more loop it's doing whatever it

00:12:42,720 --> 00:12:47,459
takes it has and I'm pushing that out if

00:12:45,660 --> 00:12:50,519
I take five things I don't have to run

00:12:47,459 --> 00:12:53,639
the loop as often but I'm only going

00:12:50,519 --> 00:12:57,169
into that loop when this race thinks a

00:12:53,639 --> 00:13:02,459
thread needs a new dollar underscore now

00:12:57,169 --> 00:13:04,829
the chunked input again is where the

00:13:02,459 --> 00:13:06,689
laziness comes in the laziness isn't in

00:13:04,829 --> 00:13:09,029
the map and it's not really in the

00:13:06,689 --> 00:13:11,789
gather you can think of the laziness

00:13:09,029 --> 00:13:14,189
attached to this thing so dollar

00:13:11,789 --> 00:13:20,910
underscore is coming out of chunk input

00:13:14,189 --> 00:13:22,980
on demand a sink is a void

00:13:20,910 --> 00:13:25,680
having to deal with the output of the

00:13:22,980 --> 00:13:29,040
map so what happens if you think there's

00:13:25,680 --> 00:13:31,080
you know a bad habit some people hadn't

00:13:29,040 --> 00:13:33,330
profile he's using a map to iterate a

00:13:31,080 --> 00:13:34,860
list instead of a for loop because then

00:13:33,330 --> 00:13:38,280
you you have to deal with all the

00:13:34,860 --> 00:13:40,650
expected output map is the same way the

00:13:38,280 --> 00:13:41,220
only reason I'm using a map here is that

00:13:40,650 --> 00:13:43,650
race

00:13:41,220 --> 00:13:46,230
I want the race to paralyze my map for

00:13:43,650 --> 00:13:48,870
me by putting a sink in here I tell it

00:13:46,230 --> 00:13:51,780
ignored any output from processed John

00:13:48,870 --> 00:13:53,400
so the map is returning nothing at which

00:13:51,780 --> 00:13:57,870
point I don't have to buffer output I'm

00:13:53,400 --> 00:13:59,820
never gonna read in fact you think about

00:13:57,870 --> 00:14:01,320
it chunked inputs gonna be populated by

00:13:59,820 --> 00:14:04,620
this gathering I'm going to be filling

00:14:01,320 --> 00:14:08,610
in an array that has an eighth of a

00:14:04,620 --> 00:14:10,890
terabyte worth of a bazillion why I'm

00:14:08,610 --> 00:14:13,140
never gonna use them as soon as they're

00:14:10,890 --> 00:14:16,290
red they're they're done so another

00:14:13,140 --> 00:14:19,470
trick is by using backslash chunked

00:14:16,290 --> 00:14:23,640
input instead of the array I dodged the

00:14:19,470 --> 00:14:26,640
container at that point I have a sigil

00:14:23,640 --> 00:14:28,830
ax syndicate there is you see down below

00:14:26,640 --> 00:14:30,570
chunked input doesn't have an @ sign in

00:14:28,830 --> 00:14:34,170
front of it anymore what I've done is

00:14:30,570 --> 00:14:38,130
create a container list object that is

00:14:34,170 --> 00:14:42,240
my my laziness so I've got this promise

00:14:38,130 --> 00:14:44,850
to produce lazy output from the gather

00:14:42,240 --> 00:14:48,360
without having to store it combine that

00:14:44,850 --> 00:14:50,910
with sync and I have no extra storage

00:14:48,360 --> 00:14:53,490
for this so I'm getting away from this

00:14:50,910 --> 00:14:55,650
with no overhead pretty much the race

00:14:53,490 --> 00:14:57,750
does all my parallelization the map

00:14:55,650 --> 00:15:02,910
reads the stuff and chunk in put deals

00:14:57,750 --> 00:15:07,950
with staging the data so this is all it

00:15:02,910 --> 00:15:10,800
takes to have a single stream processing

00:15:07,950 --> 00:15:16,800
in way parallel threads in regu for

00:15:10,800 --> 00:15:19,140
output one nice thing about when I go

00:15:16,800 --> 00:15:23,040
into the the processor there's a dollar

00:15:19,140 --> 00:15:24,660
thread that the start wijl are the

00:15:23,040 --> 00:15:27,990
things that are created automatically

00:15:24,660 --> 00:15:31,320
for you by Riku percent star env is the

00:15:27,990 --> 00:15:34,170
environment every thread has its own

00:15:31,320 --> 00:15:34,410
thread object which gives me an ID which

00:15:34,170 --> 00:15:36,060
is

00:15:34,410 --> 00:15:38,959
nice if you want to generate output

00:15:36,060 --> 00:15:41,699
files per thread or have file handles or

00:15:38,959 --> 00:15:44,850
any data that you know each thread wants

00:15:41,699 --> 00:15:46,170
to keep track of on its own so what I

00:15:44,850 --> 00:15:52,160
can do what I do here

00:15:46,170 --> 00:15:55,290
the thing that's dispatched by the map

00:15:52,160 --> 00:15:58,439
calls thread to ID and it stores a file

00:15:55,290 --> 00:15:59,910
handle for each thread and that seems to

00:15:58,439 --> 00:16:02,399
be some system overhead I don't have to

00:15:59,910 --> 00:16:05,399
open new files each time I can just keep

00:16:02,399 --> 00:16:08,730
reusing them by scoping the files array

00:16:05,399 --> 00:16:10,920
within the the end point whenever the

00:16:08,730 --> 00:16:14,519
that block is exited all the files are

00:16:10,920 --> 00:16:16,649
is closed so there's another thing

00:16:14,519 --> 00:16:20,279
that's built into recuva called atomic

00:16:16,649 --> 00:16:21,990
int which is amazing this thing when you

00:16:20,279 --> 00:16:23,610
use it it glows in the dark

00:16:21,990 --> 00:16:24,959
if you got a bunch of these going you

00:16:23,610 --> 00:16:26,970
look inside the computer you can see

00:16:24,959 --> 00:16:29,189
this funny glow if no one believes me

00:16:26,970 --> 00:16:34,040
okay the great thing about it really is

00:16:29,189 --> 00:16:37,019
the it's implemented in assembly so that

00:16:34,040 --> 00:16:39,060
operations on it don't require external

00:16:37,019 --> 00:16:41,790
synchronization they synchronize

00:16:39,060 --> 00:16:44,939
themselves so this operation atomic ink

00:16:41,790 --> 00:16:46,889
fetch it can be run in all of my threads

00:16:44,939 --> 00:16:49,170
at once and they won't step on each

00:16:46,889 --> 00:16:51,300
other before I showed you how to write

00:16:49,170 --> 00:16:54,149
that little watchdog loop this start

00:16:51,300 --> 00:16:56,069
loop all it does is print out the number

00:16:54,149 --> 00:16:58,439
of sequences that have been processed as

00:16:56,069 --> 00:17:00,060
they're being written out very low

00:16:58,439 --> 00:17:01,529
overhead but it gives me the instant

00:17:00,060 --> 00:17:03,389
gratification of knowing that it all

00:17:01,529 --> 00:17:06,449
isn't locked up waiting for something

00:17:03,389 --> 00:17:08,130
and it the whole thing just takes a

00:17:06,449 --> 00:17:10,049
couple of lines of code to do safely

00:17:08,130 --> 00:17:12,480
again without some affordance without

00:17:10,049 --> 00:17:14,970
walking that atomic ink fetch

00:17:12,480 --> 00:17:17,280
if eighteen of these threads call it at

00:17:14,970 --> 00:17:20,069
the same instant because it's an atomic

00:17:17,280 --> 00:17:21,419
operation and at the hardware level they

00:17:20,069 --> 00:17:28,169
don't step on each other they just do

00:17:21,419 --> 00:17:31,590
successive increments so the point is to

00:17:28,169 --> 00:17:36,150
get this done really doesn't take very

00:17:31,590 --> 00:17:39,539
much the gathering take acquire data the

00:17:36,150 --> 00:17:43,200
lazy loop iterates the input and by

00:17:39,539 --> 00:17:48,300
using the lazy with the gather allows me

00:17:43,200 --> 00:17:50,100
to stage the the the data in the stream

00:17:48,300 --> 00:17:53,130
and the race allows me to paralyze the

00:17:50,100 --> 00:17:55,590
map in one step I didn't have to create

00:17:53,130 --> 00:17:57,450
a threat you didn't see any logic in

00:17:55,590 --> 00:17:58,770
here for creating a threat operating on

00:17:57,450 --> 00:18:01,290
thread stopping one starting one

00:17:58,770 --> 00:18:02,850
synchronizing one testing whether the

00:18:01,290 --> 00:18:05,040
output was in the right or wrong order

00:18:02,850 --> 00:18:06,720
detecting whether they exited there were

00:18:05,040 --> 00:18:09,980
there were no catch blocks no try blocks

00:18:06,720 --> 00:18:13,650
no thread didn't read things properly

00:18:09,980 --> 00:18:14,730
values it all just happened which is the

00:18:13,650 --> 00:18:16,380
beautiful thing about raikou but the

00:18:14,730 --> 00:18:19,560
important stuff really is there under

00:18:16,380 --> 00:18:26,550
the hood here are some really good

00:18:19,560 --> 00:18:30,530
examples of documentation and what I was

00:18:26,550 --> 00:18:34,980
going to do now if people aren't asleep

00:18:30,530 --> 00:18:38,370
he is go over here and if I've done this

00:18:34,980 --> 00:18:39,600
correctly you can actually see a copy of

00:18:38,370 --> 00:18:41,760
this running and I can warn you there's

00:18:39,600 --> 00:18:43,770
one minor problem in Riku there are our

00:18:41,760 --> 00:18:48,350
memory leaks this thing started out at

00:18:43,770 --> 00:18:52,320
2gig not long ago so the complete code

00:18:48,350 --> 00:18:55,040
for processing this is right here the

00:18:52,320 --> 00:18:59,910
interesting thing about it that with run

00:18:55,040 --> 00:19:02,010
goes from here to there that even with

00:18:59,910 --> 00:19:04,890
Berkeley braces and comments it's 120

00:19:02,010 --> 00:19:07,470
lines there are a couple of atomic in

00:19:04,890 --> 00:19:11,370
store keeping track of things at the

00:19:07,470 --> 00:19:14,070
start I loop one neat thing I don't know

00:19:11,370 --> 00:19:18,270
if people have seen also if I have a

00:19:14,070 --> 00:19:20,580
quote and I have curly braces inside of

00:19:18,270 --> 00:19:22,500
it that indicates code that's being run

00:19:20,580 --> 00:19:24,510
inside of the quote so instead of having

00:19:22,500 --> 00:19:26,790
to have a bunch of variables or a printf

00:19:24,510 --> 00:19:29,280
I can just stick curly braces and do the

00:19:26,790 --> 00:19:32,220
division and integer eyes it right there

00:19:29,280 --> 00:19:34,770
in the quotes it you can abuse it but it

00:19:32,220 --> 00:19:36,450
actually it's nice for output the reader

00:19:34,770 --> 00:19:39,210
chunk is right here this is the thing I

00:19:36,450 --> 00:19:45,090
showed you I until I can find something

00:19:39,210 --> 00:19:45,840
keep going I up a number of chairs every

00:19:45,090 --> 00:19:49,040
so often

00:19:45,840 --> 00:19:53,280
I dumped some status information and

00:19:49,040 --> 00:19:56,310
then store the next processing a chunk

00:19:53,280 --> 00:19:59,760
really isn't very long I read it I keep

00:19:56,310 --> 00:20:02,340
walking down it finding the index down

00:19:59,760 --> 00:20:04,950
here of a new line and a greater than

00:20:02,340 --> 00:20:07,230
and process what's in between here is

00:20:04,950 --> 00:20:10,200
the entire code two parallel eyes

00:20:07,230 --> 00:20:16,740
everything above this it's a lazy gather

00:20:10,200 --> 00:20:23,669
and a race with a map when it actually

00:20:16,740 --> 00:20:25,409
runs and it takes a while to shut up and

00:20:23,669 --> 00:20:26,520
running this for a while now it takes a

00:20:25,409 --> 00:20:27,529
while to shut down because there enough

00:20:26,520 --> 00:20:32,130
threads that you have to be tied up

00:20:27,529 --> 00:20:36,140
quite when I said before there's the

00:20:32,130 --> 00:20:38,940
trick of using an array to store

00:20:36,140 --> 00:20:40,529
individual file handles you'll see here

00:20:38,940 --> 00:20:41,909
in some output if I'm hoping people can

00:20:40,529 --> 00:20:44,070
see this on the screen someone warned me

00:20:41,909 --> 00:20:46,890
if it's too small but you'll see all

00:20:44,070 --> 00:20:51,990
these opens and you'll see it's opening

00:20:46,890 --> 00:20:53,909
9 ABC these are the thread numbers that

00:20:51,990 --> 00:20:55,470
Moore is using what that tells me is

00:20:53,909 --> 00:20:57,929
there were probably about eight threads

00:20:55,470 --> 00:21:00,690
out there doing something before it

00:20:57,929 --> 00:21:02,429
actually got into that race and you'll

00:21:00,690 --> 00:21:06,929
see there's five chunks nine chunks

00:21:02,429 --> 00:21:09,750
thirteen in the sec will get to the

00:21:06,929 --> 00:21:14,190
point where it's opened forty-two chunks

00:21:09,750 --> 00:21:16,289
the 42 threads will all have opened a

00:21:14,190 --> 00:21:19,590
file handle and we're just gonna see the

00:21:16,289 --> 00:21:27,360
watchdog running and we're just about

00:21:19,590 --> 00:21:32,850
there and you can see here this is this

00:21:27,360 --> 00:21:34,770
this is that status report and now we're

00:21:32,850 --> 00:21:38,789
done opening new files every time it

00:21:34,770 --> 00:21:41,070
gets a new chunk all of these things are

00:21:38,789 --> 00:21:46,470
running out here you see there's only 37

00:21:41,070 --> 00:21:47,880
chunks and I can tell they're all

00:21:46,470 --> 00:21:50,399
running because the fans have gone up

00:21:47,880 --> 00:21:53,909
about two octaves but now you see 40

00:21:50,399 --> 00:21:57,299
chunks 43 chunks this is that 5 second

00:21:53,909 --> 00:22:00,690
timer catching those atomic variables

00:21:57,299 --> 00:22:02,460
they've been incremented but I'm not

00:22:00,690 --> 00:22:06,750
opening new file handles because the

00:22:02,460 --> 00:22:09,659
arrays already populated if you look at

00:22:06,750 --> 00:22:12,960
this thing running you can see it's 4.8

00:22:09,659 --> 00:22:13,890
gig right now and here's the Gotcha 5.5

00:22:12,960 --> 00:22:16,760
this

00:22:13,890 --> 00:22:18,990
keep growing the entire time it runs

00:22:16,760 --> 00:22:22,590
about the time it's finished I mean

00:22:18,990 --> 00:22:24,960
right now XZ cats and the 80% 85% that's

00:22:22,590 --> 00:22:28,440
pretty good with just having to pipe

00:22:24,960 --> 00:22:33,810
output this will actually get to around

00:22:28,440 --> 00:22:37,170
200 gig and it will exhaust about half

00:22:33,810 --> 00:22:39,240
of my swab but fortunately I got a

00:22:37,170 --> 00:22:41,160
memory that I can get away with it the

00:22:39,240 --> 00:22:42,420
warning is if you're doing this you've

00:22:41,160 --> 00:22:44,760
got to be careful because there still

00:22:42,420 --> 00:22:46,650
are some memory leaks in regu if anyone

00:22:44,760 --> 00:22:48,270
anywhere knows anyone trying to fix

00:22:46,650 --> 00:22:50,760
those memory leaks I'll be more than

00:22:48,270 --> 00:22:53,760
happy to to try anything here to fix

00:22:50,760 --> 00:22:56,790
them and we're up to 78 chunks I've gone

00:22:53,760 --> 00:22:59,130
through a million sequences now I get

00:22:56,790 --> 00:23:03,750
that well five second gratification at

00:22:59,130 --> 00:23:10,380
least and you can see here these are the

00:23:03,750 --> 00:23:12,810
files that's creating and the files at

00:23:10,380 --> 00:23:14,940
the beginning the earliest threads have

00:23:12,810 --> 00:23:16,890
been run through a few times the later

00:23:14,940 --> 00:23:19,050
ones haven't had a chance to catch up as

00:23:16,890 --> 00:23:20,820
much and I've gone through about two

00:23:19,050 --> 00:23:24,090
million rows so they're about two

00:23:20,820 --> 00:23:25,920
million sequences in here two million

00:23:24,090 --> 00:23:32,820
seven hundred thousand so it chews

00:23:25,920 --> 00:23:34,020
through pretty quickly and you can see

00:23:32,820 --> 00:23:35,690
here again we've gone through two

00:23:34,020 --> 00:23:41,690
million one hundred and nine chunks

00:23:35,690 --> 00:23:45,240
again these lines here are this watchdog

00:23:41,690 --> 00:23:48,180
that was just a start loop display the

00:23:45,240 --> 00:23:49,380
contents of these atomic intz and they

00:23:48,180 --> 00:23:53,100
actually kind of do glow in the dark

00:23:49,380 --> 00:23:54,450
it's fun and that was about it I wanted

00:23:53,100 --> 00:23:56,310
to try and show people the code if

00:23:54,450 --> 00:23:57,480
anyone had questions about it or what it

00:23:56,310 --> 00:23:59,730
looks like when it's running

00:23:57,480 --> 00:24:02,600
we obviously have time holy cow it's a

00:23:59,730 --> 00:24:04,950
lot faster than I ever thought I'd be I

00:24:02,600 --> 00:24:06,720
don't know if anyone wants to ask

00:24:04,950 --> 00:24:10,590
questions or I've just managed to put

00:24:06,720 --> 00:24:19,070
you to sleep somewhere here is my talk

00:24:10,590 --> 00:24:19,070
right there we go and that was it

00:24:19,670 --> 00:24:24,590
oh yeah question that would a lot faster

00:24:22,670 --> 00:24:27,549
than I thought I can barely hear you I

00:24:24,590 --> 00:24:30,350
don't know that's my fault of yours uh

00:24:27,549 --> 00:24:32,960
can you hear me yeah no I can hear you

00:24:30,350 --> 00:24:37,460
better yeah okay I I'm trying to use

00:24:32,960 --> 00:24:42,020
supply the class supply yeah and is it

00:24:37,460 --> 00:24:44,809
like this no it's the exact opposite

00:24:42,020 --> 00:24:49,400
you've with the supply and the channel

00:24:44,809 --> 00:24:52,220
approach you there is a bookkeeper in

00:24:49,400 --> 00:24:55,970
the middle and you have things writing

00:24:52,220 --> 00:24:57,950
into a channel and the supply takes care

00:24:55,970 --> 00:24:59,510
of putting them in order and and making

00:24:57,950 --> 00:25:01,100
sure that everyone gets everything once

00:24:59,510 --> 00:25:02,720
and if somebody doesn't get it a tree

00:25:01,100 --> 00:25:05,990
transmits it to the next one it's a

00:25:02,720 --> 00:25:10,990
really nice thing okay but it's a lot

00:25:05,990 --> 00:25:14,450
slower if you've got a single thread

00:25:10,990 --> 00:25:18,200
reading the content feeding it out to a

00:25:14,450 --> 00:25:21,950
pool of workers that process it the lazy

00:25:18,200 --> 00:25:26,540
loop with a gather and take and a race

00:25:21,950 --> 00:25:29,840
or or a hyper with a map is less it is

00:25:26,540 --> 00:25:31,220
less overhead and runs a lot faster and

00:25:29,840 --> 00:25:33,770
chronically I don't think it's any more

00:25:31,220 --> 00:25:36,950
code when you use the supplies you end

00:25:33,770 --> 00:25:40,549
up having to have all of the try and

00:25:36,950 --> 00:25:42,290
catch four end of supply you know there

00:25:40,549 --> 00:25:47,000
there's no more data left and all the

00:25:42,290 --> 00:25:51,169
rest of it and by the time you're right

00:25:47,000 --> 00:25:55,010
done writing all of the the input and

00:25:51,169 --> 00:25:56,690
output catch try organization I actually

00:25:55,010 --> 00:25:59,210
thought it looked cleaner the way I did

00:25:56,690 --> 00:26:01,250
it uh-huh but then I've got one

00:25:59,210 --> 00:26:05,059
advantage I've got only one thread

00:26:01,250 --> 00:26:07,490
reading the content if I'm in if I got

00:26:05,059 --> 00:26:08,990
nginx supplying 14 web servers in the

00:26:07,490 --> 00:26:09,950
backend of this is a worker pool of

00:26:08,990 --> 00:26:12,500
things that look things up in the

00:26:09,950 --> 00:26:14,690
database and feed it back to them this

00:26:12,500 --> 00:26:17,240
wouldn't work this is about crunching

00:26:14,690 --> 00:26:26,000
through data quickly if you see the

00:26:17,240 --> 00:26:28,590
connection the supplies actually there

00:26:26,000 --> 00:26:38,700
they are variables

00:26:28,590 --> 00:26:42,360
I liked it because it looked very clean

00:26:38,700 --> 00:26:44,880
to me yeah it's and it's this is what

00:26:42,360 --> 00:26:49,760
you have you got the supply and you emit

00:26:44,880 --> 00:26:54,080
something and you have a tap method that

00:26:49,760 --> 00:27:00,600
has these things on it and it just works

00:26:54,080 --> 00:27:04,830
yeah it is nice and the I guess it the

00:27:00,600 --> 00:27:07,620
only problem I found for it is that the

00:27:04,830 --> 00:27:09,659
amount of overhead that makes it simple

00:27:07,620 --> 00:27:15,090
and clean to use because it does so much

00:27:09,659 --> 00:27:17,929
bookkeeping ended up leaving the whole

00:27:15,090 --> 00:27:19,830
thing too slow for what I was doing okay

00:27:17,929 --> 00:27:21,929
continuing the stuff I've done it's

00:27:19,830 --> 00:27:24,899
about crunching a whole lot of data

00:27:21,929 --> 00:27:31,649
it's not about elegantly dealing with

00:27:24,899 --> 00:27:32,970
events as they arrive but is this the

00:27:31,649 --> 00:27:39,840
place you were looking to see about it

00:27:32,970 --> 00:27:50,460
before yes yes there is a if you look in

00:27:39,840 --> 00:27:54,630
the language tutorials concurrency have

00:27:50,460 --> 00:27:56,070
you read this yet no that is the one

00:27:54,630 --> 00:27:57,450
that's in my talk I actually gave

00:27:56,070 --> 00:28:00,860
somebody something useful today

00:27:57,450 --> 00:28:06,570
okay this shows you different approaches

00:28:00,860 --> 00:28:11,220
aha including the promise including and

00:28:06,570 --> 00:28:13,770
I tried just about all of these and this

00:28:11,220 --> 00:28:17,490
is supplies an asynchronous data

00:28:13,770 --> 00:28:19,730
streaming mechanism a supply if you use

00:28:17,490 --> 00:28:19,730
it

00:28:30,030 --> 00:28:35,200
look sort of like this you yeah here it

00:28:33,070 --> 00:28:40,450
is so you have something like this

00:28:35,200 --> 00:28:44,580
you've got a loop that catches closed or

00:28:40,450 --> 00:28:44,580
other errors and then doesn't receive

00:28:46,440 --> 00:28:58,960
and then I can take a channel and if I

00:28:52,630 --> 00:29:01,690
have this new channel I can create a

00:28:58,960 --> 00:29:07,690
number of processes that write this

00:29:01,690 --> 00:29:09,490
stuff and then send something to eat I

00:29:07,690 --> 00:29:13,120
can do a channel to send and because

00:29:09,490 --> 00:29:15,460
each of these guys has attached to the

00:29:13,120 --> 00:29:18,400
channel what I do is send on it each one

00:29:15,460 --> 00:29:21,940
of them will get it they won't all it'll

00:29:18,400 --> 00:29:25,990
be sent to each one of them that's cute

00:29:21,940 --> 00:29:28,180
because I could have 14 things taking

00:29:25,990 --> 00:29:30,940
input I can have a bunch of these things

00:29:28,180 --> 00:29:32,410
on the output and I could be feeding all

00:29:30,940 --> 00:29:34,000
kinds of stuff from all kinds of places

00:29:32,410 --> 00:29:36,730
into whatever back-end thing is

00:29:34,000 --> 00:29:39,220
available because I have the channel

00:29:36,730 --> 00:29:42,190
anyone can do a send on it it doesn't

00:29:39,220 --> 00:29:47,860
need to be just one one at a time and

00:29:42,190 --> 00:29:49,570
the supplies look like that like I said

00:29:47,860 --> 00:29:50,760
the problem for me was they just weren't

00:29:49,570 --> 00:29:55,810
fast enough

00:29:50,760 --> 00:29:59,620
okay thanks did that help at all oh yeah

00:29:55,810 --> 00:30:04,630
I'll read up on this stuff now the yeah

00:29:59,620 --> 00:30:08,790
if you go back in the slides there's at

00:30:04,630 --> 00:30:08,790
the end here was the low level stuff but

00:30:09,600 --> 00:30:22,300
right come on their language concurrency

00:30:18,420 --> 00:30:22,980
uh-huh once I put this to just look that

00:30:22,300 --> 00:30:25,780
up

00:30:22,980 --> 00:30:29,020
Rinku dot org is is the place to go look

00:30:25,780 --> 00:30:31,510
at those tutorials yeah the the

00:30:29,020 --> 00:30:35,280
tutorials are really nice reading the

00:30:31,510 --> 00:30:39,100
people who dealt with the

00:30:35,280 --> 00:30:40,780
dockstar a coup org did an amazing job I

00:30:39,100 --> 00:30:42,370
mean I I don't I'm sure they didn't

00:30:40,780 --> 00:30:44,169
write every one of the tutorials but

00:30:42,370 --> 00:30:47,200
collecting them putting them in place

00:30:44,169 --> 00:30:53,530
making sure they're clickable readable

00:30:47,200 --> 00:31:00,490
complete it's wonderful so I would start

00:30:53,530 --> 00:31:02,620
there anyone else awake they don't have

00:31:00,490 --> 00:31:07,350
a heartbeat there's two people three

00:31:02,620 --> 00:31:07,350
people with questions in the list

00:31:08,160 --> 00:31:15,429
there's well there's a chat and there's

00:31:10,240 --> 00:31:18,010
also a participant list I can see on the

00:31:15,429 --> 00:31:19,600
news I'm not any tiny I click the middle

00:31:18,010 --> 00:31:21,910
what I see me I click the bigger one I

00:31:19,600 --> 00:31:24,580
see even more of me yes no there should

00:31:21,910 --> 00:31:27,970
be at the bottom there should be lot of

00:31:24,580 --> 00:31:31,600
I see when I do the one with the four

00:31:27,970 --> 00:31:35,290
things on it all I see is well guys just

00:31:31,600 --> 00:31:37,600
unmute yourself and scream I have hi

00:31:35,290 --> 00:31:39,520
this is Bruce first my daughter's

00:31:37,600 --> 00:31:44,080
sleeping for the record

00:31:39,520 --> 00:31:48,580
we've got 54 people Oh God and and the

00:31:44,080 --> 00:31:52,530
question the question is earlier you

00:31:48,580 --> 00:31:56,799
were saying the failure of using an on

00:31:52,530 --> 00:32:00,790
new line as a delimiter that happened at

00:31:56,799 --> 00:32:04,000
about did you say 70 gig or 70 mag-7

00:32:00,790 --> 00:32:07,480
teabag attend to between 20 and 100 mega

00:32:04,000 --> 00:32:09,760
every time if you look at matter of fact

00:32:07,480 --> 00:32:14,710
if you look on github there's a bug I

00:32:09,760 --> 00:32:16,960
put in one of the people there found an

00:32:14,710 --> 00:32:21,760
alternate data source that would blow up

00:32:16,960 --> 00:32:25,330
equally well and it blows up reliably

00:32:21,760 --> 00:32:28,830
and if the tens of megabytes range doing

00:32:25,330 --> 00:32:35,440
a read and F basically the F read

00:32:28,830 --> 00:32:39,220
something between F read and mmmm the F

00:32:35,440 --> 00:32:40,840
read into the main memory into decode

00:32:39,220 --> 00:32:45,920
into whatever your buffer is supposed to

00:32:40,840 --> 00:32:48,840
be it blows up and gets an unlined read

00:32:45,920 --> 00:32:51,570
thank you

00:32:48,840 --> 00:32:55,830
if you want I could find you that if you

00:32:51,570 --> 00:32:59,640
go and do in to rake ooh github and just

00:32:55,830 --> 00:33:02,130
look me up yeah a couple of thugs that's

00:32:59,640 --> 00:33:02,610
one of them if I can't find it I'll good

00:33:02,130 --> 00:33:09,230
job

00:33:02,610 --> 00:33:12,060
feel free anyone else

00:33:09,230 --> 00:33:14,220
sure so I want to know what the result

00:33:12,060 --> 00:33:17,010
was did you did you find duplicates what

00:33:14,220 --> 00:33:20,070
what kinds of yeah they're always well

00:33:17,010 --> 00:33:26,970
every every time what ends up happening

00:33:20,070 --> 00:33:28,770
to this stuff is I can show you here so

00:33:26,970 --> 00:33:30,360
we've got these one thing I might

00:33:28,770 --> 00:33:34,560
actually give up doing is having one

00:33:30,360 --> 00:33:36,870
file handle per thread and actually have

00:33:34,560 --> 00:33:39,510
one file handle for chunk just open them

00:33:36,870 --> 00:33:41,220
as I go through and the reason there is

00:33:39,510 --> 00:33:49,430
that sorting those small files is much

00:33:41,220 --> 00:33:53,310
faster why what I end up having to do is

00:33:49,430 --> 00:33:59,760
sort these I do a I do I merge these by

00:33:53,310 --> 00:34:02,190
length and store basically either in the

00:33:59,760 --> 00:34:04,470
nested hash from hell or an array by

00:34:02,190 --> 00:34:08,120
length with the nested hash from hell

00:34:04,470 --> 00:34:13,500
and at that point I can store counts and

00:34:08,120 --> 00:34:16,410
after I'm done with all of it you you

00:34:13,500 --> 00:34:18,660
dump out any I have all of the digests

00:34:16,410 --> 00:34:21,540
that came up more than once I have to go

00:34:18,660 --> 00:34:23,730
back to the record numbers and then I go

00:34:21,540 --> 00:34:25,680
back to the sequences for those record

00:34:23,730 --> 00:34:27,270
numbers and I compare those as complete

00:34:25,680 --> 00:34:32,280
strings because they have the same

00:34:27,270 --> 00:34:33,780
length on the same digests and even then

00:34:32,280 --> 00:34:35,610
there will be multiple collisions that

00:34:33,780 --> 00:34:38,820
are unique but the last time I ran this

00:34:35,610 --> 00:34:41,550
I think I found two collisions and the

00:34:38,820 --> 00:34:43,880
response when you reported that oh we'll

00:34:41,550 --> 00:34:43,880
look anyway

00:34:44,330 --> 00:34:50,850
the the people and I I think they don't

00:34:47,400 --> 00:34:52,410
like getting email for me I its whoever

00:34:50,850 --> 00:34:53,400
deals with it they always get that I

00:34:52,410 --> 00:34:55,860
always get the same response

00:34:53,400 --> 00:34:57,390
oh no it's unique and I say but wait if

00:34:55,860 --> 00:34:58,950
you look at this one record number this

00:34:57,390 --> 00:34:59,310
record number that begins with this

00:34:58,950 --> 00:35:00,960
begin

00:34:59,310 --> 00:35:02,370
so that here are the two sequences

00:35:00,960 --> 00:35:08,640
you'll notice that are identical

00:35:02,370 --> 00:35:10,860
oh the response is always oops and you

00:35:08,640 --> 00:35:13,290
know I I think they do all of this by

00:35:10,860 --> 00:35:15,000
selecting out of a large enough database

00:35:13,290 --> 00:35:16,160
I don't know what procedure they use for

00:35:15,000 --> 00:35:18,990
it

00:35:16,160 --> 00:35:20,550
the problem you get into is if you try

00:35:18,990 --> 00:35:23,520
to even do something as simple as

00:35:20,550 --> 00:35:28,470
shoving this up Postgres just the raw

00:35:23,520 --> 00:35:32,400
the raw database is so huge that you can

00:35:28,470 --> 00:35:34,920
get errors so processing this is the

00:35:32,400 --> 00:35:42,690
fact that they can process it is pretty

00:35:34,920 --> 00:35:55,550
damn amazing but yeah I usually do find

00:35:42,690 --> 00:36:01,020
some dudes was this useful to people a

00:35:55,550 --> 00:36:03,750
question sure yeah I'm the one one of

00:36:01,020 --> 00:36:06,660
the complaints about rocku is its speed

00:36:03,750 --> 00:36:09,960
but you're using it for a data intensive

00:36:06,660 --> 00:36:12,000
purposes of is that is there nothing I

00:36:09,960 --> 00:36:14,970
mean is there no other program that is

00:36:12,000 --> 00:36:17,640
fast that I can use pro/5 and pro/5

00:36:14,970 --> 00:36:25,620
might be a little faster even if i hack

00:36:17,640 --> 00:36:27,750
this in see the so basically what I'm

00:36:25,620 --> 00:36:31,770
saying is you know not cool it wouldn't

00:36:27,750 --> 00:36:34,830
be much faster Saku is still far as fast

00:36:31,770 --> 00:36:37,320
as anything else it's it's pretty

00:36:34,830 --> 00:36:41,220
reasonable well my usual measure is this

00:36:37,320 --> 00:36:45,000
if I look at XZ cat which is down around

00:36:41,220 --> 00:36:50,940
63% right now you can see this has grown

00:36:45,000 --> 00:36:53,610
to 28 gig by the way the if I wrote this

00:36:50,940 --> 00:36:58,590
the version of this that I have running

00:36:53,610 --> 00:37:00,030
in Perl 5 is slower because it doesn't

00:36:58,590 --> 00:37:04,050
handle the threading is gratefully it

00:37:00,030 --> 00:37:09,570
uses Forks and IPC and data pipes for

00:37:04,050 --> 00:37:12,630
all of it so doing this in Perl 5 is a

00:37:09,570 --> 00:37:15,540
lot of work and you spend

00:37:12,630 --> 00:37:18,120
enough time synchronizing things that

00:37:15,540 --> 00:37:22,230
that eats into the if I did this in C

00:37:18,120 --> 00:37:25,140
and threaded it if you look at it doing

00:37:22,230 --> 00:37:29,880
it the code do an index read a buffer

00:37:25,140 --> 00:37:32,190
append a buffer is not complicated but

00:37:29,880 --> 00:37:35,580
the amount of work I would do in C to

00:37:32,190 --> 00:37:38,190
synchronize all the threads and create

00:37:35,580 --> 00:37:40,110
them and walk down the pool and find the

00:37:38,190 --> 00:37:42,810
next one that's available and feed the

00:37:40,110 --> 00:37:45,660
data into that thread and make sure you

00:37:42,810 --> 00:37:49,920
don't lock the buffer copy the buffer

00:37:45,660 --> 00:37:54,230
unlock the buffer start the thread all

00:37:49,920 --> 00:38:00,180
of that overhead is there no matter what

00:37:54,230 --> 00:38:03,320
and the IPC if I run X Z cat and I have

00:38:00,180 --> 00:38:07,860
the riku program reading just reading I

00:38:03,320 --> 00:38:10,530
can get X Z cat up to 90 something

00:38:07,860 --> 00:38:12,750
percent if I type X Z cat into cat

00:38:10,530 --> 00:38:16,590
greater than dev null it doesn't get

00:38:12,750 --> 00:38:19,800
much above 90 95 percent because of IPC

00:38:16,590 --> 00:38:21,720
overhead I mean you're feeding things

00:38:19,800 --> 00:38:23,340
through a pipe that's a kernel call the

00:38:21,720 --> 00:38:25,050
kernel has to buffer it and then turn

00:38:23,340 --> 00:38:28,730
off one process and turn on another and

00:38:25,050 --> 00:38:31,290
and fill some buffers and file handles

00:38:28,730 --> 00:38:33,840
but you know I think there probably

00:38:31,290 --> 00:38:36,240
might be something out there faster than

00:38:33,840 --> 00:38:38,390
red ku that I could do it in but frankly

00:38:36,240 --> 00:38:44,210
by the time I was done writing it

00:38:38,390 --> 00:38:48,540
compared to you know something this much

00:38:44,210 --> 00:38:51,050
which one would you rather write I mean

00:38:48,540 --> 00:38:56,220
I can process the input of this thing

00:38:51,050 --> 00:39:00,060
with one lazy gather right here one lazy

00:38:56,220 --> 00:39:05,420
gather on a loop is everything it takes

00:39:00,060 --> 00:39:05,420
me to stage the data for anyway input

00:39:06,530 --> 00:39:13,350
one race with a map is all it takes me

00:39:10,560 --> 00:39:16,910
to get this done anyway parallel with

00:39:13,350 --> 00:39:21,980
all the thread management done for me

00:39:16,910 --> 00:39:26,490
throw in the backslash throw in the sink

00:39:21,980 --> 00:39:30,369
you know that's as much code as it takes

00:39:26,490 --> 00:39:37,030
two parallel process an arbitrary reader

00:39:30,369 --> 00:39:39,100
and in arbitrary prop handlers so maybe

00:39:37,030 --> 00:39:41,140
I could do this in C but how many how

00:39:39,100 --> 00:39:50,110
much time would I spend writing it let

00:39:41,140 --> 00:39:52,780
alone debugging it and like I said I if

00:39:50,110 --> 00:40:01,690
you watch this now it's it does a pretty

00:39:52,780 --> 00:40:03,490
good job you know XE cat is moving since

00:40:01,690 --> 00:40:08,200
I kicked this off a few minutes ago

00:40:03,490 --> 00:40:12,070
we've done 17 million sequences 500 and

00:40:08,200 --> 00:40:13,570
so we're up to you know 9.2 gigabytes of

00:40:12,070 --> 00:40:17,190
data that have been processed through it

00:40:13,570 --> 00:40:20,410
generating a digest for each and

00:40:17,190 --> 00:40:28,270
spitting the lines out the files that

00:40:20,410 --> 00:40:29,619
are running here okay I mean there's a

00:40:28,270 --> 00:40:31,119
fair amount of where each each one of

00:40:29,619 --> 00:40:34,660
these has got half a million records in

00:40:31,119 --> 00:40:36,970
it and for each one of those I had to

00:40:34,660 --> 00:40:39,070
come I had to read it split out the

00:40:36,970 --> 00:40:41,920
string strip out the new lines computed

00:40:39,070 --> 00:40:46,720
digest and then spit one copy of it out

00:40:41,920 --> 00:40:50,560
here and spit this the the other one the

00:40:46,720 --> 00:40:52,900
spit the the sequence out to another

00:40:50,560 --> 00:40:57,160
file now the great thing about atomic

00:40:52,900 --> 00:41:01,119
intz is that all 42 threads are using

00:40:57,160 --> 00:41:04,030
the same dollar variable to increment

00:41:01,119 --> 00:41:06,609
their their thread counter excuse me the

00:41:04,030 --> 00:41:08,170
the sequence counter and I don't have to

00:41:06,609 --> 00:41:09,760
write any of the synchronization code

00:41:08,170 --> 00:41:17,770
for it because it's all been an assembly

00:41:09,760 --> 00:41:20,730
as atomic process so no I think I don't

00:41:17,770 --> 00:41:25,030
know if if I were going to try and run

00:41:20,730 --> 00:41:31,060
the you know the Nielsen system on on

00:41:25,030 --> 00:41:33,280
rake who quite yet but much like Perl I

00:41:31,060 --> 00:41:35,830
think its capacity for processing data

00:41:33,280 --> 00:41:38,280
may be a bit underrated it's gotten

00:41:35,830 --> 00:41:38,280
faster

00:41:39,780 --> 00:41:45,010
looks like Vadim might have a question I

00:41:42,580 --> 00:41:46,150
don't know if I pronounced that yes

00:41:45,010 --> 00:41:49,420
William Kirk

00:41:46,150 --> 00:41:51,280
not a question but rather you know Steve

00:41:49,420 --> 00:41:52,810
I think you have a little bit of a

00:41:51,280 --> 00:41:56,590
problem

00:41:52,810 --> 00:42:00,130
you rely on sir they need to store files

00:41:56,590 --> 00:42:02,110
as far as I remember it's not reliable

00:42:00,130 --> 00:42:05,050
unfortunately I have big news for you

00:42:02,110 --> 00:42:05,740
shred ID is not guaranteed to stay the

00:42:05,050 --> 00:42:09,720
same

00:42:05,740 --> 00:42:13,270
for a chunk of code because over time

00:42:09,720 --> 00:42:16,240
after some weight in operations like you

00:42:13,270 --> 00:42:19,530
know going hasn't it the file handle

00:42:16,240 --> 00:42:23,890
isn't specific to the chunk of code the

00:42:19,530 --> 00:42:25,690
thread ID but for anybody else who

00:42:23,890 --> 00:42:28,390
doesn't know the specific so keep in

00:42:25,690 --> 00:42:31,440
mind that Rocko actually manages threads

00:42:28,390 --> 00:42:33,520
there at any time your code could be

00:42:31,440 --> 00:42:35,950
relocated from one thread to another

00:42:33,520 --> 00:42:37,990
soul strategy might arrange inside your

00:42:35,950 --> 00:42:41,800
code at some point but I said you

00:42:37,990 --> 00:42:44,050
wouldn't have well the thread ID would

00:42:41,800 --> 00:42:49,960
change during one proc during one

00:42:44,050 --> 00:42:50,260
iteration of the loop ah no I don't

00:42:49,960 --> 00:42:52,900
think so

00:42:50,260 --> 00:42:56,350
it has that kind of operation which

00:42:52,900 --> 00:42:59,350
actually Jesus thread resources like

00:42:56,350 --> 00:43:01,270
again like a weight or maybe read from

00:42:59,350 --> 00:43:09,850
channel I don't to my balls and pills I

00:43:01,270 --> 00:43:14,080
mean I'm not looking at it what I was

00:43:09,850 --> 00:43:18,420
told and if it's wrong that's good to

00:43:14,080 --> 00:43:24,100
know now before I shoot my feet off is

00:43:18,420 --> 00:43:26,290
right up here there we go

00:43:24,100 --> 00:43:29,560
so what happens is I go into process

00:43:26,290 --> 00:43:32,320
chunk I know that this thread ID is

00:43:29,560 --> 00:43:36,340
supposed to be stable through one

00:43:32,320 --> 00:43:41,740
iteration the thread ID shouldn't be

00:43:36,340 --> 00:43:42,160
changing hmm through one model ration

00:43:41,740 --> 00:43:45,640
yes

00:43:42,160 --> 00:43:50,110
no nowhere it's about you so the thread

00:43:45,640 --> 00:43:50,990
ID here all I'm saying is for this one

00:43:50,110 --> 00:43:55,850
iteration

00:43:50,990 --> 00:43:58,520
of the code I'm gonna be in one thread

00:43:55,850 --> 00:44:02,750
until the until that call from map

00:43:58,520 --> 00:44:05,300
returns the next time I enter this

00:44:02,750 --> 00:44:09,530
whatever the thread ID is there will

00:44:05,300 --> 00:44:12,140
never be two threads with the same ID so

00:44:09,530 --> 00:44:16,060
I grab some file handles I odd put

00:44:12,140 --> 00:44:16,060
everything for the chunk and I return to

00:44:17,320 --> 00:44:24,980
the best of my knowledge if I'm using

00:44:20,480 --> 00:44:27,710
race once I've dispatched that block

00:44:24,980 --> 00:44:31,070
into a thread the thread won't change

00:44:27,710 --> 00:44:33,890
until the block returns because then

00:44:31,070 --> 00:44:36,770
then it's idle now I can there's no

00:44:33,890 --> 00:44:39,230
telling which will be the next thread

00:44:36,770 --> 00:44:41,180
that gets called with the next chunk I'm

00:44:39,230 --> 00:44:43,220
perfectly comfortable with that because

00:44:41,180 --> 00:44:46,160
whenever it comes in here it'll figure

00:44:43,220 --> 00:44:48,380
out which thread it is it'll find the

00:44:46,160 --> 00:44:51,710
file handles for that specific thread

00:44:48,380 --> 00:44:53,990
and it'll output to them until it exits

00:44:51,710 --> 00:44:58,280
and returns back in and the thread is

00:44:53,990 --> 00:45:03,920
idle I think that's supposed to be the

00:44:58,280 --> 00:45:06,710
way it works or me if I'm wrong unless

00:45:03,920 --> 00:45:10,880
you do something atomic encouraged which

00:45:06,710 --> 00:45:14,360
might hold your thread on hold like if

00:45:10,880 --> 00:45:18,890
inside atomic encourage you do any

00:45:14,360 --> 00:45:22,370
reading from the file there could be

00:45:18,890 --> 00:45:25,550
some waiting for data synchronicity and

00:45:22,370 --> 00:45:28,130
at that point it is possible that it

00:45:25,550 --> 00:45:29,990
would happen but on the other hand when

00:45:28,130 --> 00:45:32,200
you enter this block you most likely

00:45:29,990 --> 00:45:36,500
will have unique ID and because you've

00:45:32,200 --> 00:45:39,560
stored you know you priest or it then

00:45:36,500 --> 00:45:41,420
it's it's okay everybody I assign it at

00:45:39,560 --> 00:45:44,930
the top of the block and I keep using it

00:45:41,420 --> 00:45:49,130
throughout well the other side of it is

00:45:44,930 --> 00:45:52,010
the the lazy gather is called in map

00:45:49,130 --> 00:45:54,380
once to produce a chunk of data and

00:45:52,010 --> 00:45:57,380
there's only one chunk that gets passed

00:45:54,380 --> 00:46:00,080
into here I don't do any reading inside

00:45:57,380 --> 00:46:03,080
of the block the only blocking operation

00:46:00,080 --> 00:46:04,530
I perform are these save and they're

00:46:03,080 --> 00:46:09,000
actually putting out some pretty small

00:46:04,530 --> 00:46:12,090
Johnson data so there these to say Xin

00:46:09,000 --> 00:46:15,450
to the to file handles that happens some

00:46:12,090 --> 00:46:17,670
number of times I can't tell but the

00:46:15,450 --> 00:46:20,430
read is done in the gather because

00:46:17,670 --> 00:46:21,960
that's where that take grabs the chunk

00:46:20,430 --> 00:46:23,970
that was read the next chunk that's been

00:46:21,960 --> 00:46:27,630
read out of out of the the input stream

00:46:23,970 --> 00:46:29,760
a little under five minutes to go and

00:46:27,630 --> 00:46:38,700
you do have one more hand up sure

00:46:29,760 --> 00:46:41,600
whoever it is just talk yeah everybody

00:46:38,700 --> 00:46:44,340
hear me stiva thumbs great talk

00:46:41,600 --> 00:46:46,830
I've got a couple of questions

00:46:44,340 --> 00:46:51,720
I missed unfortunately the first minute

00:46:46,830 --> 00:46:53,190
or two and I'm just wondering no that's

00:46:51,720 --> 00:46:54,720
fine I just wonder if this is a bit of

00:46:53,190 --> 00:46:58,260
an artificial test you're doing because

00:46:54,720 --> 00:47:01,740
you don't seem to be testing based upon

00:46:58,260 --> 00:47:06,090
organism or file them or anything to

00:47:01,740 --> 00:47:09,540
that the NRG z file is supposed to be a

00:47:06,090 --> 00:47:13,770
unique list okay all the amino acid

00:47:09,540 --> 00:47:15,750
sequences known to NCBI the blast header

00:47:13,770 --> 00:47:18,210
line for each of those is a list of

00:47:15,750 --> 00:47:22,080
every organism that is known to exhibit

00:47:18,210 --> 00:47:24,690
that sequence okay so um are you doing

00:47:22,080 --> 00:47:26,310
are you doing a first pass to pull out

00:47:24,690 --> 00:47:27,780
everything under a particular organism

00:47:26,310 --> 00:47:31,500
and then check out all those I don't

00:47:27,780 --> 00:47:36,480
care my own question is is the non

00:47:31,500 --> 00:47:43,050
redundant file redundant okay I look at

00:47:36,480 --> 00:47:48,030
all 128 million sequences in that file

00:47:43,050 --> 00:47:50,130
or any of the sequences repeated well

00:47:48,030 --> 00:47:54,600
you're very ambitious person if you want

00:47:50,130 --> 00:47:56,340
to do that I mean and I think there are

00:47:54,600 --> 00:47:59,940
other ways there are other ways you

00:47:56,340 --> 00:48:01,920
might want to approach it but I mean if

00:47:59,940 --> 00:48:03,630
that's if that's your test I thought you

00:48:01,920 --> 00:48:07,650
know this beautiful I put it in the

00:48:03,630 --> 00:48:10,190
comment section here I'm sorry I'm doing

00:48:07,650 --> 00:48:13,440
it for the hell I'm doing okay it's it's

00:48:10,190 --> 00:48:16,470
it's a non-trivial computing problem to

00:48:13,440 --> 00:48:17,930
try and determine the quality control is

00:48:16,470 --> 00:48:23,200
is that much

00:48:17,930 --> 00:48:26,350
data in that cruddy of a format unique

00:48:23,200 --> 00:48:30,980
that's it most of the time when I run it

00:48:26,350 --> 00:48:32,180
I do get dupes and I send them back I'll

00:48:30,980 --> 00:48:35,390
send something back to the nice people

00:48:32,180 --> 00:48:37,910
at NCBI with I read the file published

00:48:35,390 --> 00:48:40,730
on this date and sequences number x and

00:48:37,910 --> 00:48:41,870
y and z were duplicated here's the

00:48:40,730 --> 00:48:43,160
sequence for this one here's the

00:48:41,870 --> 00:48:44,690
sequence for that one here's the first

00:48:43,160 --> 00:48:49,700
two organisms there here's the first two

00:48:44,690 --> 00:48:53,690
organisms there and they'll usually say

00:48:49,700 --> 00:48:55,790
loops are bad and they twiddle whatever

00:48:53,690 --> 00:48:58,130
bits in their database and I usually

00:48:55,790 --> 00:49:00,410
don't see in the same sequences that are

00:48:58,130 --> 00:49:02,210
that are duplicated but it's different

00:49:00,410 --> 00:49:04,250
sequences that are duplicated each time

00:49:02,210 --> 00:49:07,880
I think what's happening is there some

00:49:04,250 --> 00:49:12,530
of these sequences they're selecting

00:49:07,880 --> 00:49:17,590
them by organism and some of the time

00:49:12,530 --> 00:49:20,810
the sequences in the database are are

00:49:17,590 --> 00:49:22,900
are not unique but they're given

00:49:20,810 --> 00:49:25,730
separate identifiers x'b I'm absolutely

00:49:22,900 --> 00:49:27,260
or they're going the viewing is gonna go

00:49:25,730 --> 00:49:30,560
back and find out which of their

00:49:27,260 --> 00:49:31,940
identifiers are incorrect they're coming

00:49:30,560 --> 00:49:33,080
from a different genome Center and

00:49:31,940 --> 00:49:34,670
they're just aggregating them together

00:49:33,080 --> 00:49:38,020
they're all getting thrown in together

00:49:34,670 --> 00:49:40,820
so yeah that's the the quality control D

00:49:38,020 --> 00:49:43,460
this is a layer of quality control that

00:49:40,820 --> 00:49:46,670
if every time I find this if you're

00:49:43,460 --> 00:49:49,190
trying to do this by organism I'm saving

00:49:46,670 --> 00:49:50,990
you from not realizing that sixteen

00:49:49,190 --> 00:49:55,340
dozen organisms really do share a

00:49:50,990 --> 00:49:59,660
sequence because they've accidentally

00:49:55,340 --> 00:50:02,120
split two sets of organisms onto

00:49:59,660 --> 00:50:05,270
different editions of an identical

00:50:02,120 --> 00:50:06,470
sequence that's where I got involved in

00:50:05,270 --> 00:50:08,510
this was I was working for a

00:50:06,470 --> 00:50:13,190
bioinformatics company and that was our

00:50:08,510 --> 00:50:15,980
job it was giving people it was for

00:50:13,190 --> 00:50:17,510
disease discovery and people would look

00:50:15,980 --> 00:50:18,920
in the database and say I want to find

00:50:17,510 --> 00:50:25,340
everything everywhere that has this

00:50:18,920 --> 00:50:28,040
amino acid and if they if the NCBI data

00:50:25,340 --> 00:50:30,390
it has dupes in it and you look at the

00:50:28,040 --> 00:50:31,980
wrong amino acid you're gonna find half

00:50:30,390 --> 00:50:33,960
because half of them would be associated

00:50:31,980 --> 00:50:36,029
with one entry and half with the other

00:50:33,960 --> 00:50:40,039
and they really should all be collapsed

00:50:36,029 --> 00:50:42,539
into one and it is a non-trivial problem

00:50:40,039 --> 00:50:44,430
and if you look at the number of entries

00:50:42,539 --> 00:50:48,319
that it takes here doing it in

00:50:44,430 --> 00:50:50,700
reasonable time isn't easy

00:50:48,319 --> 00:50:54,359
and I had a solution that worked in Perl

00:50:50,700 --> 00:50:56,579
5 which was similar to this one but it

00:50:54,359 --> 00:51:00,690
was a lot more work to program and make

00:50:56,579 --> 00:51:03,990
it work properly and the interesting

00:51:00,690 --> 00:51:06,210
thing to me and riku is I could get the

00:51:03,990 --> 00:51:10,430
same amount of threading and everything

00:51:06,210 --> 00:51:12,960
else done with a hell of a lot less code

00:51:10,430 --> 00:51:14,819
so are you just doing this on a on a

00:51:12,960 --> 00:51:17,430
desktop or a workstation have you tried

00:51:14,819 --> 00:51:19,829
something like AWS or anything to that

00:51:17,430 --> 00:51:23,069
effect I've got a well what I've got in

00:51:19,829 --> 00:51:26,690
the next room screaming at me is a four

00:51:23,069 --> 00:51:31,769
cpu Super Micro server with 48 48

00:51:26,690 --> 00:51:36,109
corazon 8 and 180 128 gig of core that's

00:51:31,769 --> 00:51:39,240
why I can get that's why the fact that

00:51:36,109 --> 00:51:43,289
if I didn't have where is it there's top

00:51:39,240 --> 00:51:46,289
if I didn't have that much memory this

00:51:43,289 --> 00:51:51,089
thing being at 40 36 gig resident would

00:51:46,289 --> 00:51:53,220
be causing me pain okay but it doesn't

00:51:51,089 --> 00:51:56,700
even bother me until it gets up to 100

00:51:53,220 --> 00:51:58,890
gig of resident and I've got 128 gig of

00:51:56,700 --> 00:52:00,480
swap and from what I've seen once the

00:51:58,890 --> 00:52:01,890
stuff gets swapped it never gets touched

00:52:00,480 --> 00:52:04,589
again which is why I think it's a memory

00:52:01,890 --> 00:52:09,180
leak and more orang rated because

00:52:04,589 --> 00:52:13,440
something somewhere is just not freeing

00:52:09,180 --> 00:52:19,230
something but I've got so much memory I

00:52:13,440 --> 00:52:21,299
can get away with it now if I were

00:52:19,230 --> 00:52:24,089
processing this data for real-time

00:52:21,299 --> 00:52:26,069
telemetry at NASA to try and keep people

00:52:24,089 --> 00:52:29,240
from dying on the space station I

00:52:26,069 --> 00:52:31,289
probably wouldn't be doing it in rayker

00:52:29,240 --> 00:52:33,790
but neither are they so no one's gonna

00:52:31,289 --> 00:52:36,500
hurt me for it

00:52:33,790 --> 00:52:39,730
but for this kind of a problem it seems

00:52:36,500 --> 00:52:42,710
it's a nice I think it's a nice solution

00:52:39,730 --> 00:52:45,500
because it's only this much code to make

00:52:42,710 --> 00:52:49,820
it happen I mean there is it perfectly

00:52:45,500 --> 00:52:53,540
generic single reader multi-threaded

00:52:49,820 --> 00:52:56,560
writer processor handler for anyone

00:52:53,540 --> 00:53:00,620
anywhere that's as much code as it takes

00:52:56,560 --> 00:53:03,080
if you tried to write this in C++ you

00:53:00,620 --> 00:53:12,110
could spend the next six years debugging

00:53:03,080 --> 00:53:13,970
the threading so like I said you know I

00:53:12,110 --> 00:53:15,290
started with the subscriber model I

00:53:13,970 --> 00:53:24,170
tried the subscriber as I try the

00:53:15,290 --> 00:53:27,140
channels and then I got someone was kind

00:53:24,170 --> 00:53:30,470
enough to show me how how lazy gathers

00:53:27,140 --> 00:53:34,040
in loops work and and from there I just

00:53:30,470 --> 00:53:35,990
went down this rabbit hole but this page

00:53:34,040 --> 00:53:38,390
right here is a perfectly this is

00:53:35,990 --> 00:53:41,570
completely generic anything whatever

00:53:38,390 --> 00:53:45,830
your next chunk of data is and whatever

00:53:41,570 --> 00:53:47,270
your process a chunk of data is this is

00:53:45,830 --> 00:53:50,050
the framework that will do them in

00:53:47,270 --> 00:53:50,050

YouTube URL: https://www.youtube.com/watch?v=rgCk5w2o-GY


