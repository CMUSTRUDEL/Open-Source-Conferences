Title: Shawn Moore (‎sartak‎) - DTrace War Stories
Publication date: 2014-06-24
Playlist: YAPC::NA 2014
Description: 
	Several stories of how DTrace saved someone's day when *nothing else* could.

You too can wield this power and be the envy of your community.
Captions: 
	00:00:02,389 --> 00:00:06,200
load faster YouTube

00:00:09,730 --> 00:00:18,550
alright we have 30 seconds and my

00:00:14,270 --> 00:00:18,550
computer is synced with ntp John

00:00:22,920 --> 00:00:40,800
no I could perform some illusions for

00:00:34,180 --> 00:00:47,800
you all right I'm gonna get started

00:00:40,800 --> 00:00:51,250
hello hi good morning it's 330 good

00:00:47,800 --> 00:00:54,280
morning so I'm Sean Moore I work for

00:00:51,250 --> 00:00:58,330
infinity interactive and I'm here to

00:00:54,280 --> 00:01:01,269
talk about DTrace so I freaking love

00:00:58,330 --> 00:01:06,630
printf debugging right you guys do too

00:01:01,269 --> 00:01:08,740
can I get an amen okay a couple of you

00:01:06,630 --> 00:01:11,170
sometimes I feel like any problem could

00:01:08,740 --> 00:01:13,510
be solved with thoughtfully placed print

00:01:11,170 --> 00:01:15,520
up debugging I've never been a big fan

00:01:13,510 --> 00:01:17,710
of debuggers like gdb or the pearl

00:01:15,520 --> 00:01:20,880
debugger I feel like they kind of get in

00:01:17,710 --> 00:01:26,380
my way more than they help but printf

00:01:20,880 --> 00:01:28,710
has it got my back so when i'm debugging

00:01:26,380 --> 00:01:32,410
a problem this is my seven step process

00:01:28,710 --> 00:01:33,610
for fixing it so i stopped the program

00:01:32,410 --> 00:01:35,860
that I think really hard about what

00:01:33,610 --> 00:01:38,200
happened and I try to make a guess as to

00:01:35,860 --> 00:01:40,960
what's going wrong so I add some print

00:01:38,200 --> 00:01:44,620
statements and then I started out the

00:01:40,960 --> 00:01:47,760
program calls the problem to happen

00:01:44,620 --> 00:01:49,930
again see what comes out and then

00:01:47,760 --> 00:01:56,530
usually takes a few stout a few

00:01:49,930 --> 00:01:58,210
iterations of this loop but few feedback

00:01:56,530 --> 00:02:01,090
loops I always get there in the end

00:01:58,210 --> 00:02:02,350
though but this only applies to like

00:02:01,090 --> 00:02:06,510
ninety-nine percent of the problems you

00:02:02,350 --> 00:02:09,970
run into there is still one percent

00:02:06,510 --> 00:02:12,280
exact numbers here of problems that this

00:02:09,970 --> 00:02:13,659
doesn't actually work for so I want to

00:02:12,280 --> 00:02:15,580
highlight two of these steps which are

00:02:13,659 --> 00:02:18,640
stopped the program and restart the

00:02:15,580 --> 00:02:20,470
program that's not always an option what

00:02:18,640 --> 00:02:23,080
if the problem is running on some

00:02:20,470 --> 00:02:25,590
critical production server and you can't

00:02:23,080 --> 00:02:29,829
just stop the server change the code

00:02:25,590 --> 00:02:31,599
push it deploy it and then restart your

00:02:29,829 --> 00:02:33,180
system ins who are already pretty grumpy

00:02:31,599 --> 00:02:36,629
and overworked are probably going to get

00:02:33,180 --> 00:02:38,310
yo and then I'm going to take a break to

00:02:36,629 --> 00:02:48,060
eat a piece of cookie while you're in

00:02:38,310 --> 00:02:50,670
done I'm not seeing any nods okay John

00:02:48,060 --> 00:02:53,040
you got me back i also want to talk

00:02:50,670 --> 00:02:55,530
about this problem what happens when the

00:02:53,040 --> 00:02:58,280
problem actually takes days or extremely

00:02:55,530 --> 00:03:00,389
unusual circumstances to actually appear

00:02:58,280 --> 00:03:03,030
sometimes it's not always trivial to

00:03:00,389 --> 00:03:05,790
directly replicate a problem but you'll

00:03:03,030 --> 00:03:11,340
know when you see it and for these two

00:03:05,790 --> 00:03:13,739
situations I usually turn to DTrace so

00:03:11,340 --> 00:03:17,250
here's the one line some is a 5-11

00:03:13,739 --> 00:03:19,139
summary of de tres sorry papa it's an

00:03:17,250 --> 00:03:21,030
instrumentation tool so it tells you

00:03:19,139 --> 00:03:23,510
about instra interesting events in your

00:03:21,030 --> 00:03:25,319
system and you can use those for both

00:03:23,510 --> 00:03:29,519
profiling to improve your code and

00:03:25,319 --> 00:03:32,219
debugging its scripted scripting a bowl

00:03:29,519 --> 00:03:34,379
which town which is how you tell DTrace

00:03:32,219 --> 00:03:36,389
what events are interesting to you it's

00:03:34,379 --> 00:03:37,919
full stack meaning you can instrument

00:03:36,389 --> 00:03:41,609
everything from your pearl application

00:03:37,919 --> 00:03:45,449
down to the pro guts itself to engine x2

00:03:41,609 --> 00:03:49,439
apache into your kernel to the device

00:03:45,449 --> 00:03:51,120
drivers and you can instrument each

00:03:49,439 --> 00:03:53,909
section separately or you can kind of

00:03:51,120 --> 00:03:57,720
take them in the gestalt flowing how was

00:03:53,909 --> 00:03:59,129
up so you can kind of correlate how

00:03:57,720 --> 00:04:01,530
you're Pro application in your database

00:03:59,129 --> 00:04:04,739
are working together to pound the hard

00:04:01,530 --> 00:04:06,720
drive its runtime meaning you can

00:04:04,739 --> 00:04:09,329
instrument programs as they run without

00:04:06,720 --> 00:04:12,000
any modification or special compile

00:04:09,329 --> 00:04:14,549
flags or special runtime flags or even

00:04:12,000 --> 00:04:16,849
restarting the program you can hook into

00:04:14,549 --> 00:04:19,590
a running program and debug it there and

00:04:16,849 --> 00:04:21,180
finally its production safe meaning you

00:04:19,590 --> 00:04:25,610
never have to worry about it crashing

00:04:21,180 --> 00:04:30,780
your system or even kind of causing

00:04:25,610 --> 00:04:32,520
slowdowns it's very low overhead so the

00:04:30,780 --> 00:04:35,580
way I kind of think about dtrace is like

00:04:32,520 --> 00:04:37,650
the best last resort it's never the

00:04:35,580 --> 00:04:40,590
first to lure each for usually that's

00:04:37,650 --> 00:04:41,550
Google you know if you get an error

00:04:40,590 --> 00:04:43,800
message just google the error message

00:04:41,550 --> 00:04:45,060
stack overflow is probably going to

00:04:43,800 --> 00:04:46,920
explain what went wrong and how to fix

00:04:45,060 --> 00:04:48,300
our

00:04:46,920 --> 00:04:51,390
but when I exhaust all the tools like

00:04:48,300 --> 00:04:56,550
nyt prof and Charles proxy and even

00:04:51,390 --> 00:04:58,440
printf I reach for DTrace and as good as

00:04:56,550 --> 00:04:59,970
d traces its not going to fix this one

00:04:58,440 --> 00:05:03,870
you still have to thank very hard about

00:04:59,970 --> 00:05:05,880
your problem so I want to spend the rest

00:05:03,870 --> 00:05:08,310
of the talk talking about specific

00:05:05,880 --> 00:05:10,020
problems I've run into and solved with

00:05:08,310 --> 00:05:12,270
DTrace to kind of give you an idea of

00:05:10,020 --> 00:05:17,000
what it can do and where it might fit

00:05:12,270 --> 00:05:17,000
into your life so here are the problems

00:05:18,050 --> 00:05:25,320
okay thank you for coming alright the

00:05:22,890 --> 00:05:27,890
first problem was we were having a test

00:05:25,320 --> 00:05:30,960
feel because of internationalization and

00:05:27,890 --> 00:05:33,770
I run my system in Japanese and this is

00:05:30,960 --> 00:05:36,150
what i was seeing out of my test results

00:05:33,770 --> 00:05:38,400
that actually says public key not found

00:05:36,150 --> 00:05:41,070
in Japanese so that much is right but

00:05:38,400 --> 00:05:43,220
obviously string matching is not going

00:05:41,070 --> 00:05:45,420
to understand that that's the same thing

00:05:43,220 --> 00:05:46,920
and it's not that I was the only

00:05:45,420 --> 00:05:48,570
developer using a language other than

00:05:46,920 --> 00:05:51,300
English we had a rushing on the project

00:05:48,570 --> 00:05:53,970
too so where I internationalisation was

00:05:51,300 --> 00:05:57,090
still working it was just failing oddly

00:05:53,970 --> 00:05:59,930
from me and in particular this string

00:05:57,090 --> 00:06:02,790
doesn't actually occur in the code base

00:05:59,930 --> 00:06:04,230
so that was a little bit weird it looked

00:06:02,790 --> 00:06:07,500
like the translation was being provided

00:06:04,230 --> 00:06:11,520
by a GP g so that really narrowed down

00:06:07,500 --> 00:06:12,780
where the problem could be but the the

00:06:11,520 --> 00:06:15,330
russian guy wasn't seeing this kind of

00:06:12,780 --> 00:06:18,870
problem so the real question was how

00:06:15,330 --> 00:06:21,450
come gpg new to use japanese i almost

00:06:18,870 --> 00:06:24,870
said nihongo but pull it back a little

00:06:21,450 --> 00:06:26,970
bit so my first thought was maybe some

00:06:24,870 --> 00:06:29,630
kind of system call that gpg was making

00:06:26,970 --> 00:06:33,090
to figure out what language that she use

00:06:29,630 --> 00:06:34,560
because the the project we're working on

00:06:33,090 --> 00:06:37,050
actually controlled what environment

00:06:34,560 --> 00:06:38,520
variables that gpg got and obviously the

00:06:37,050 --> 00:06:40,080
environment variables on my system would

00:06:38,520 --> 00:06:41,910
be the same as on the russian guy system

00:06:40,080 --> 00:06:45,060
so that wasn't the problem so I assumed

00:06:41,910 --> 00:06:47,070
as a first pass and my piece s calls so

00:06:45,060 --> 00:06:50,670
what would I do I instrumented gpg

00:06:47,070 --> 00:06:52,320
syscalls and it turns out that gpg was

00:06:50,670 --> 00:06:55,920
opening up a file in my library

00:06:52,320 --> 00:06:57,770
directory it was a plist file so it was

00:06:55,920 --> 00:07:00,210
very obvious that it was an OS 10 issue

00:06:57,770 --> 00:07:02,699
the other guy was using Linux

00:07:00,210 --> 00:07:05,460
was the problem and then that led me to

00:07:02,699 --> 00:07:07,610
figure out that okay so if we pass the

00:07:05,460 --> 00:07:10,470
environment variable LC all as a blank

00:07:07,610 --> 00:07:12,270
gpg will inspect the system to figure

00:07:10,470 --> 00:07:13,770
out what it should be using if we

00:07:12,270 --> 00:07:16,199
specifically pass Japanese of course it

00:07:13,770 --> 00:07:17,940
will give you Japanese but if you ask

00:07:16,199 --> 00:07:21,150
for English specifically it'll give you

00:07:17,940 --> 00:07:27,240
English so that's the fix that we made

00:07:21,150 --> 00:07:30,210
and that stopped dtrace sorry that's

00:07:27,240 --> 00:07:32,849
stopped gpg from trying to give me

00:07:30,210 --> 00:07:35,009
Japanese so I know what some of your

00:07:32,849 --> 00:07:39,500
thinking you call dtrace the best last

00:07:35,009 --> 00:07:39,500
resort why didn't you just use estrace

00:07:40,820 --> 00:07:47,580
well actually I did so there's a program

00:07:44,460 --> 00:07:52,190
called de trois which is a descendant of

00:07:47,580 --> 00:07:56,639
trust which is kind of the Solaris OS 10

00:07:52,190 --> 00:07:59,460
estrace program it actually uses dtrace

00:07:56,639 --> 00:08:02,000
under the hood to tell you about all the

00:07:59,460 --> 00:08:04,590
system calls that process is making and

00:08:02,000 --> 00:08:07,800
one way that this is so much better than

00:08:04,590 --> 00:08:10,020
estrace is I told it tell me all this

00:08:07,800 --> 00:08:11,970
his calls that any gpg process is making

00:08:10,020 --> 00:08:16,020
so I didn't have to specifically stop

00:08:11,970 --> 00:08:18,030
the process pull out its pit and inspect

00:08:16,020 --> 00:08:20,219
that particular process individually I

00:08:18,030 --> 00:08:22,830
could just say hey whenever gpg does

00:08:20,219 --> 00:08:25,380
something let me know which meant I

00:08:22,830 --> 00:08:26,909
didn't have to stop gpg as it was

00:08:25,380 --> 00:08:30,630
launching which might have been too late

00:08:26,909 --> 00:08:32,370
or even edit a single line of code and

00:08:30,630 --> 00:08:34,950
when I was reading the estrace man page

00:08:32,370 --> 00:08:37,610
i noticed this little tidbit that's a

00:08:34,950 --> 00:08:37,610
hell of a rebase

00:08:39,460 --> 00:08:48,010
nobody appreciates that in 1991 this

00:08:45,270 --> 00:08:50,260
fork was from like a year ago whereas

00:08:48,010 --> 00:08:52,450
today you want to be like on the right

00:08:50,260 --> 00:08:56,560
commit I don't know I thought that was

00:08:52,450 --> 00:08:59,910
cool all right next problem we are

00:08:56,560 --> 00:09:02,830
writing a small static file server for a

00:08:59,910 --> 00:09:04,210
as part of a bigger project and the

00:09:02,830 --> 00:09:06,670
server was pretty much just a thin

00:09:04,210 --> 00:09:08,340
wrapper around plaque app static so just

00:09:06,670 --> 00:09:11,590
loading files from desk and serving them

00:09:08,340 --> 00:09:13,270
but we noticed it was really slow like

00:09:11,590 --> 00:09:16,960
really slow like five seconds for a

00:09:13,270 --> 00:09:19,870
single request and this happened both

00:09:16,960 --> 00:09:21,130
from a web browser and from curl and

00:09:19,870 --> 00:09:23,700
this was actually happening on a few

00:09:21,130 --> 00:09:28,150
machines including mine and you know

00:09:23,700 --> 00:09:31,240
this is obviously unusable slow so i

00:09:28,150 --> 00:09:33,630
tried tools like nyt prof and IO top to

00:09:31,240 --> 00:09:36,100
figure out where this is getting hung up

00:09:33,630 --> 00:09:37,690
but not really making any progress

00:09:36,100 --> 00:09:41,920
everything looks super fast from pros

00:09:37,690 --> 00:09:44,350
perspective and I used estrace to try to

00:09:41,920 --> 00:09:46,150
figure out what was going on so I got

00:09:44,350 --> 00:09:51,100
pages and pages of em advised system

00:09:46,150 --> 00:09:52,330
calls which is not really useful so I

00:09:51,100 --> 00:09:54,630
instrumented curl that figure I was

00:09:52,330 --> 00:09:58,150
going on wrong this is what it was doing

00:09:54,630 --> 00:10:00,610
it's just playing with memory so what

00:09:58,150 --> 00:10:06,610
can we do now well obviously you profile

00:10:00,610 --> 00:10:07,600
the colonel so nice thing about DTrace

00:10:06,610 --> 00:10:09,370
you don't have to restart your computer

00:10:07,600 --> 00:10:11,980
you don't have to recompile your kernel

00:10:09,370 --> 00:10:14,680
to profile the colonel you just turn on

00:10:11,980 --> 00:10:17,080
dtrace and it starts going so my tactic

00:10:14,680 --> 00:10:19,960
was every thousand one millisecond

00:10:17,080 --> 00:10:23,530
microseconds snapshot what the kernel

00:10:19,960 --> 00:10:25,600
stack trace was and then hopefully we'll

00:10:23,530 --> 00:10:28,870
see why it's taking five seconds to load

00:10:25,600 --> 00:10:32,020
this page and we ran this and it turns

00:10:28,870 --> 00:10:33,490
out it was idling for five seconds well

00:10:32,020 --> 00:10:36,340
that's not really useful either I can't

00:10:33,490 --> 00:10:39,760
really act on that information so new

00:10:36,340 --> 00:10:42,760
plan so it seems like something was

00:10:39,760 --> 00:10:44,980
waiting for something to happen so the

00:10:42,760 --> 00:10:47,470
fact that every request was taking just

00:10:44,980 --> 00:10:48,640
a hair over five seconds was suggesting

00:10:47,470 --> 00:10:50,980
to me that there's like a five second

00:10:48,640 --> 00:10:52,960
time on somewhere that was kicking in so

00:10:50,980 --> 00:10:55,000
my next tactic was

00:10:52,960 --> 00:10:56,980
soon as curl becomes blocked as soon as

00:10:55,000 --> 00:10:58,930
it leaves the CPU or something capture

00:10:56,980 --> 00:11:00,580
its stack trace and then maybe that'll

00:10:58,930 --> 00:11:05,350
tell us where it's being is spending its

00:11:00,580 --> 00:11:07,840
time and the first the heaviest track

00:11:05,350 --> 00:11:11,220
trace was using dlopen which is just for

00:11:07,840 --> 00:11:13,570
loading dynamic libraries like dll's so

00:11:11,220 --> 00:11:14,920
that's not slow because every curl on

00:11:13,570 --> 00:11:17,080
vacation would be slow if this was slow

00:11:14,920 --> 00:11:19,360
and then the next stack trace was

00:11:17,080 --> 00:11:22,030
actually very interesting you can see a

00:11:19,360 --> 00:11:23,770
lot of references to DNS here and in

00:11:22,030 --> 00:11:25,960
particular since I called mdns which I

00:11:23,770 --> 00:11:27,160
wasn't familiar with but now that we

00:11:25,960 --> 00:11:29,140
have this new piece of information we

00:11:27,160 --> 00:11:32,050
can kind of restart our debug cycle and

00:11:29,140 --> 00:11:35,350
go to google and use google for mdns and

00:11:32,050 --> 00:11:39,850
you get this multicast DNS and the most

00:11:35,350 --> 00:11:42,610
important thing is it sends an IP

00:11:39,850 --> 00:11:48,370
multicast query message which times out

00:11:42,610 --> 00:11:51,090
in five seconds so it actually occurs

00:11:48,370 --> 00:11:55,720
whenever using the dot local domain name

00:11:51,090 --> 00:11:58,990
TLD so our fix was first of all stop

00:11:55,720 --> 00:12:01,990
using that domain name so pinging

00:11:58,990 --> 00:12:04,600
localhost directly was super fast and if

00:12:01,990 --> 00:12:07,440
we rename the dot local to CDN that made

00:12:04,600 --> 00:12:13,030
it really fast too so we figured out

00:12:07,440 --> 00:12:15,640
yeah we figured out by instrumenting our

00:12:13,030 --> 00:12:19,050
kernel and everything that dealt local

00:12:15,640 --> 00:12:23,740
is not something you want to use lately

00:12:19,050 --> 00:12:25,420
another one line fix caused by dtrace so

00:12:23,740 --> 00:12:28,840
is that a hackathon a few years ago and

00:12:25,420 --> 00:12:30,900
someone called out is there a regular

00:12:28,840 --> 00:12:35,910
expression master in the room I

00:12:30,900 --> 00:12:35,910
volunteered to help oops

00:12:36,170 --> 00:12:40,800
this is what they were seeing so they're

00:12:39,000 --> 00:12:42,030
running the test suite and it looked

00:12:40,800 --> 00:12:43,410
like a regular expression was being

00:12:42,030 --> 00:12:46,280
recompiled every time it was matched

00:12:43,410 --> 00:12:48,900
which was happening like 300,000 times

00:12:46,280 --> 00:12:52,200
which all those compilations add up to a

00:12:48,900 --> 00:12:55,080
long time you can see that it spent two

00:12:52,200 --> 00:12:57,590
seconds making calls to core red comp

00:12:55,080 --> 00:13:01,530
which compiles a regular expression um

00:12:57,590 --> 00:13:03,060
so nyt prof caught this immediately but

00:13:01,530 --> 00:13:06,810
then where do from there you know that

00:13:03,060 --> 00:13:09,240
red comp is taking a long time what do

00:13:06,810 --> 00:13:11,730
you do so I'm sure most of you know this

00:13:09,240 --> 00:13:13,770
if you use QR that will compile a

00:13:11,730 --> 00:13:15,090
regular expression once and then it

00:13:13,770 --> 00:13:19,310
won't have to recompile it every time

00:13:15,090 --> 00:13:27,240
you match but this code was using QR

00:13:19,310 --> 00:13:37,590
that's interesting what now what would

00:13:27,240 --> 00:13:39,600
you do we use the trace yeah email

00:13:37,590 --> 00:13:42,120
Abigail so we wanted more detail than

00:13:39,600 --> 00:13:44,880
nyt prof could give NYC for office for

00:13:42,120 --> 00:13:46,770
profiling Perl code it has some

00:13:44,880 --> 00:13:48,450
knowledge of what's going on in the Perl

00:13:46,770 --> 00:13:49,560
interpreter but we needed a deeper

00:13:48,450 --> 00:13:51,720
understanding what's going on in the

00:13:49,560 --> 00:13:54,990
Perl interpreter so what we did was we

00:13:51,720 --> 00:13:57,510
were taking pearls own see stack trace

00:13:54,990 --> 00:13:59,370
every time this red comp was happening

00:13:57,510 --> 00:14:01,140
to try to figure I was going on this

00:13:59,370 --> 00:14:03,570
didn't really pan out we didn't really

00:14:01,140 --> 00:14:04,650
get anything useful out of that part of

00:14:03,570 --> 00:14:06,720
it is because I don't have a good

00:14:04,650 --> 00:14:08,630
understanding of the pearl internals so

00:14:06,720 --> 00:14:10,230
I'm sure that someone with like an

00:14:08,630 --> 00:14:13,170
encyclopedic knowledge of everything

00:14:10,230 --> 00:14:16,530
going on inside pro would just like see

00:14:13,170 --> 00:14:18,060
what was going wrong but I'm just a guy

00:14:16,530 --> 00:14:22,050
that doesn't know what's going on inside

00:14:18,060 --> 00:14:26,760
pearl I'm just a pro programmer so what

00:14:22,050 --> 00:14:28,800
i did was i timed how long each red comp

00:14:26,760 --> 00:14:30,860
was taking individually and then i

00:14:28,800 --> 00:14:34,620
charted it now that's an interesting

00:14:30,860 --> 00:14:35,820
distribution so it turns out that so

00:14:34,620 --> 00:14:38,640
what this chart is doing is it's

00:14:35,820 --> 00:14:41,220
bucketing based on how long each red com

00:14:38,640 --> 00:14:42,810
took it tells you how many red comps

00:14:41,220 --> 00:14:44,700
took that long so it's kind of giving

00:14:42,810 --> 00:14:47,700
you a histogram of how long each rug

00:14:44,700 --> 00:14:48,660
comp takes and as soon as I as soon as

00:14:47,700 --> 00:14:50,459
DTrace printed that

00:14:48,660 --> 00:14:53,699
the person I was helping was like oh I

00:14:50,459 --> 00:14:55,199
get it okay so what happened was one of

00:14:53,699 --> 00:14:57,930
these four regular expressions was doing

00:14:55,199 --> 00:15:00,000
something interesting and that

00:14:57,930 --> 00:15:02,190
immediately flagged the prop the problem

00:15:00,000 --> 00:15:04,769
was one of these regular expressions so

00:15:02,190 --> 00:15:08,579
the change that fix this two-second

00:15:04,769 --> 00:15:11,819
delay was adding ? colon to this regular

00:15:08,579 --> 00:15:16,980
expression how would you find out

00:15:11,819 --> 00:15:19,980
without DTrace or something like it rev

00:15:16,980 --> 00:15:21,420
nyt prof among many other things he

00:15:19,980 --> 00:15:25,439
likes DTrace just as much as i do and

00:15:21,420 --> 00:15:28,769
i'm sure that he would really appreciate

00:15:25,439 --> 00:15:31,110
this kind of feature in nyt prof. this

00:15:28,769 --> 00:15:33,750
histogram feature because averages lose

00:15:31,110 --> 00:15:37,980
so much information I owe him a patch

00:15:33,750 --> 00:15:40,889
I'm sorry Tim if you're here alright

00:15:37,980 --> 00:15:42,930
last one so I was approached by this

00:15:40,889 --> 00:15:45,240
gentleman who had a problem he knew that

00:15:42,930 --> 00:15:46,500
he wanted to use DTrace to solve it but

00:15:45,240 --> 00:15:49,410
he hadn't really learned it yet so he

00:15:46,500 --> 00:15:53,009
asked for my help for sake of protecting

00:15:49,410 --> 00:15:54,389
innocent I'll call him our pumpkin well

00:15:53,009 --> 00:16:00,990
that's kind of obvious let's go with

00:15:54,389 --> 00:16:03,360
ricardo p alright so Ricardo had a few

00:16:00,990 --> 00:16:04,649
files that he was monitoring and this is

00:16:03,360 --> 00:16:08,069
what you would expect to happen the

00:16:04,649 --> 00:16:09,380
contents like gradually increment so the

00:16:08,069 --> 00:16:11,970
first time you check the file is one

00:16:09,380 --> 00:16:13,470
checked it again it was one checked

00:16:11,970 --> 00:16:15,660
again moved to two checked it again

00:16:13,470 --> 00:16:17,279
three etc this is what you kind of

00:16:15,660 --> 00:16:19,889
expect that happen this is what actually

00:16:17,279 --> 00:16:23,579
happens so as he was monitoring the file

00:16:19,889 --> 00:16:25,470
what kind of revert back in history so

00:16:23,579 --> 00:16:28,850
this was obviously causing some problems

00:16:25,470 --> 00:16:31,800
it was involved with some important work

00:16:28,850 --> 00:16:34,649
process so he needed to get to the

00:16:31,800 --> 00:16:36,600
bottom of it so like any good programmer

00:16:34,649 --> 00:16:39,779
he used like a thousand different tools

00:16:36,600 --> 00:16:43,980
to figure out what was going wrong and

00:16:39,779 --> 00:16:45,629
he ended up at dtrace so the question

00:16:43,980 --> 00:16:51,689
you wanted to know was which processes

00:16:45,629 --> 00:16:55,829
open or rename this file so you know we

00:16:51,689 --> 00:16:58,829
use dtrace to figure this out Oh tres

00:16:55,829 --> 00:17:01,939
script that tells you every time

00:16:58,829 --> 00:17:03,499
something touches this bar file

00:17:01,939 --> 00:17:05,990
obviously if you right to food we get

00:17:03,499 --> 00:17:08,990
new output if we write to bar we get

00:17:05,990 --> 00:17:10,699
output on so first is my shell opening

00:17:08,990 --> 00:17:13,490
the file so that's zsh on the right and

00:17:10,699 --> 00:17:14,959
then you see that md worker is looking

00:17:13,490 --> 00:17:16,879
at it and that's actually part of

00:17:14,959 --> 00:17:18,799
spotlight on OS 10 in the search engine

00:17:16,879 --> 00:17:21,350
so it's kind of cool to see that it

00:17:18,799 --> 00:17:22,730
actually updates real time and then we

00:17:21,350 --> 00:17:26,449
tried move and that kind of did some

00:17:22,730 --> 00:17:29,480
stuff and then when we actually ran this

00:17:26,449 --> 00:17:38,570
program on Rick server here's what we

00:17:29,480 --> 00:17:44,000
were seeing this guy what the hell we

00:17:38,570 --> 00:17:45,740
found it in addition to possibly a free

00:17:44,000 --> 00:17:49,610
PO box account for life I also earned a

00:17:45,740 --> 00:17:55,340
dollar bill so dtrace is helping to

00:17:49,610 --> 00:17:57,500
support my family all right so use a

00:17:55,340 --> 00:17:59,090
bunch of tools to try to solve these

00:17:57,500 --> 00:18:02,750
problems but none of them worked but

00:17:59,090 --> 00:18:04,250
dtrace ultimately did I hope that this

00:18:02,750 --> 00:18:06,350
kind of inspires you to learn a little

00:18:04,250 --> 00:18:12,669
bit more about dtrace maybe see how it

00:18:06,350 --> 00:18:16,610
can fit into your debugging tool chain

00:18:12,669 --> 00:18:18,500
so some resources there's detroit's book

00:18:16,610 --> 00:18:20,480
which actually brought with me it's

00:18:18,500 --> 00:18:26,570
pretty heavy but you know it's it's

00:18:20,480 --> 00:18:29,539
really useful there's a DTrace man page

00:18:26,570 --> 00:18:31,490
specifically for pearl that you should

00:18:29,539 --> 00:18:34,940
have already if you have a modern

00:18:31,490 --> 00:18:36,110
version of pearl and then there's some

00:18:34,940 --> 00:18:39,080
training and stuff and there's a

00:18:36,110 --> 00:18:41,120
conference related to DTrace so there's

00:18:39,080 --> 00:18:46,070
a lot of stuff here and i encourage you

00:18:41,120 --> 00:18:48,440
to check it out before i go it turns out

00:18:46,070 --> 00:18:50,840
DTrace is not just a development tool it

00:18:48,440 --> 00:18:52,940
describes itself is safe for production

00:18:50,840 --> 00:18:54,350
so there's no real reason we can't use

00:18:52,940 --> 00:18:58,309
it as a fundamental building block in

00:18:54,350 --> 00:18:59,750
our applications so years ago I worked

00:18:58,309 --> 00:19:01,490
on a net hack bought with a couple

00:18:59,750 --> 00:19:04,940
people you might know like Stefano rear

00:19:01,490 --> 00:19:06,780
who went on to write Nietzsche Jessie

00:19:04,940 --> 00:19:11,970
lures who I don't know

00:19:06,780 --> 00:19:16,440
I hear us now so we had this netic

00:19:11,970 --> 00:19:19,260
bought is this playing and as putting

00:19:16,440 --> 00:19:22,140
that hack it's actually playing that

00:19:19,260 --> 00:19:27,120
hack at about one maybe two three turns

00:19:22,140 --> 00:19:28,830
per second we spent a long time trying

00:19:27,120 --> 00:19:32,070
to optimize this to figure out what was

00:19:28,830 --> 00:19:33,630
going wrong and we kind of couldn't

00:19:32,070 --> 00:19:35,160
figure it out we just kind of chalked it

00:19:33,630 --> 00:19:37,770
up to Pearl being slow and moose being

00:19:35,160 --> 00:19:40,370
slow maybe I'll moose being slow I don't

00:19:37,770 --> 00:19:42,780
want to like that fire any more lies

00:19:40,370 --> 00:19:44,070
just like twenty five thousand lines of

00:19:42,780 --> 00:19:46,050
pearl so we didn't want to just like get

00:19:44,070 --> 00:19:50,280
rid of pearl and righted and see or rust

00:19:46,050 --> 00:19:54,030
which didn't exist at the time so what

00:19:50,280 --> 00:19:55,410
we did was we had this weird code to

00:19:54,030 --> 00:19:57,240
kind of interact directly with nethack

00:19:55,410 --> 00:20:00,870
to get his output and to produce an

00:19:57,240 --> 00:20:03,210
input for it it was basically like using

00:20:00,870 --> 00:20:06,540
like a sleep to wait for net act to give

00:20:03,210 --> 00:20:07,710
us output but then I realized I could

00:20:06,540 --> 00:20:09,930
teach this net knock bought to use

00:20:07,710 --> 00:20:13,020
DTrace to tell it when the heck is to

00:20:09,930 --> 00:20:15,930
have it tell the bot when nethack is

00:20:13,020 --> 00:20:21,660
ready for input so this is how fast it

00:20:15,930 --> 00:20:22,950
is now it's like 10 times faster it went

00:20:21,660 --> 00:20:26,690
from like boring to watch to

00:20:22,950 --> 00:20:29,640
significantly faster than human players

00:20:26,690 --> 00:20:31,430
it obviously if you're writing like an

00:20:29,640 --> 00:20:33,090
AI this is a huge boon to like

00:20:31,430 --> 00:20:35,970
productivity in debugging and

00:20:33,090 --> 00:20:39,480
experimentation and excitements games

00:20:35,970 --> 00:20:41,310
finish in minutes rather than ours so

00:20:39,480 --> 00:20:44,460
that was pretty cool to use dtrace for

00:20:41,310 --> 00:20:46,560
that it has not ascended it's not even

00:20:44,460 --> 00:20:48,120
close this is what the code looks like

00:20:46,560 --> 00:20:52,410
if that looks interesting to you come

00:20:48,120 --> 00:20:54,120
see me it's actually really cool to

00:20:52,410 --> 00:20:55,320
watch this stuff around he like puts on

00:20:54,120 --> 00:20:56,850
a ring of invisibility to fight monsters

00:20:55,320 --> 00:20:58,500
and it takes off when he's done because

00:20:56,850 --> 00:21:01,870
you don't be invisible when you don't

00:20:58,500 --> 00:21:04,120
have to let that be a lesson to you

00:21:01,870 --> 00:21:12,730
alright thank you very much for your

00:21:04,120 --> 00:21:19,620
attention unfortunately I'm over time

00:21:12,730 --> 00:21:19,620
but I'll take one quick question mr.

00:21:31,530 --> 00:21:37,000
Rosso you could deploy Solaris next

00:21:35,290 --> 00:21:38,620
question no there's actually a

00:21:37,000 --> 00:21:41,230
relatively recent tool that came out

00:21:38,620 --> 00:21:44,290
called sis dig that kind of gives you

00:21:41,230 --> 00:21:46,059
some of the same benefits as dtrace for

00:21:44,290 --> 00:21:47,170
linux so I definitely check that out

00:21:46,059 --> 00:21:55,230
unfortunately don't know too much about

00:21:47,170 --> 00:21:55,230
it freebsd yep all right thank you

00:21:55,530 --> 00:21:57,590

YouTube URL: https://www.youtube.com/watch?v=P88qXvU2RUA


