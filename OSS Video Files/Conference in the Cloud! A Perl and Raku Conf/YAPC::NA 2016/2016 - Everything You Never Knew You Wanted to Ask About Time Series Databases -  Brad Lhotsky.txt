Title: 2016 - Everything You Never Knew You Wanted to Ask About Time Series Databases -  Brad Lhotsky
Publication date: 2016-06-27
Playlist: YAPC::NA 2016
Description: 
	
Captions: 
	00:00:00,060 --> 00:00:04,799
so I work a Craigslist I that's still

00:00:03,030 --> 00:00:06,930
sinking in I just started in October

00:00:04,799 --> 00:00:08,309
it's a pretty awesome place to work

00:00:06,930 --> 00:00:10,050
I love being able to come to a

00:00:08,309 --> 00:00:12,030
conference like this and tell you that I

00:00:10,050 --> 00:00:16,020
work for a company that puts people

00:00:12,030 --> 00:00:17,730
before money it's an amazing place lots

00:00:16,020 --> 00:00:19,130
of really cool people doing lots of cool

00:00:17,730 --> 00:00:21,390
stuff

00:00:19,130 --> 00:00:23,580
there's tons of challenges tons of

00:00:21,390 --> 00:00:26,310
technical challenges and it's a small

00:00:23,580 --> 00:00:28,490
company of only 50 people total so it

00:00:26,310 --> 00:00:31,740
feels more like a family than a company

00:00:28,490 --> 00:00:33,780
please check out the positions we have

00:00:31,740 --> 00:00:36,120
open even if we don't have something

00:00:33,780 --> 00:00:39,840
that matches your skill set try to find

00:00:36,120 --> 00:00:43,469
the closest thing we write positions but

00:00:39,840 --> 00:00:46,680
we hire good people I strongly encourage

00:00:43,469 --> 00:00:47,789
you to consider it the only downside is

00:00:46,680 --> 00:00:49,379
it's in the Bay Area

00:00:47,789 --> 00:00:51,570
you know Starr must be willing to

00:00:49,379 --> 00:00:53,520
relocate to San Francisco

00:00:51,570 --> 00:00:56,219
that was a huge negative for me but I

00:00:53,520 --> 00:00:59,430
can tell you it was totally worth it in

00:00:56,219 --> 00:01:01,680
the long run so today I want to talk to

00:00:59,430 --> 00:01:03,840
you a little bit about time series

00:01:01,680 --> 00:01:05,430
databases how many people are using time

00:01:03,840 --> 00:01:07,260
series databases in their infrastructure

00:01:05,430 --> 00:01:11,970
right now and how many people understand

00:01:07,260 --> 00:01:14,270
how they work yeah cool so some of this

00:01:11,970 --> 00:01:17,280
is going to be repetitive to this people

00:01:14,270 --> 00:01:18,750
redundant even but I hopefully there's

00:01:17,280 --> 00:01:19,979
some stuff you can learn here for

00:01:18,750 --> 00:01:22,110
everyone and if you're not using time

00:01:19,979 --> 00:01:25,259
series databases hopefully you start

00:01:22,110 --> 00:01:27,630
using them after this talk but first we

00:01:25,259 --> 00:01:30,000
need to know why I'm arriving here at

00:01:27,630 --> 00:01:33,390
this destination why I'm taking you down

00:01:30,000 --> 00:01:36,170
this road things started like this in

00:01:33,390 --> 00:01:38,790
the 90s has anyone seen this page before

00:01:36,170 --> 00:01:40,200
at a Perl conference it's likely almost

00:01:38,790 --> 00:01:41,310
everyone in the room has seen one of

00:01:40,200 --> 00:01:44,430
these this is mrtg

00:01:41,310 --> 00:01:46,049
and I I don't know if you guys were had

00:01:44,430 --> 00:01:49,710
that experience of the first deployment

00:01:46,049 --> 00:01:52,079
of mrtg in a company but it was it was

00:01:49,710 --> 00:01:54,299
amazing all of a sudden you had graphs

00:01:52,079 --> 00:01:56,280
you could see stuff everyone was excited

00:01:54,299 --> 00:01:58,409
prior to this it was Big Brother little

00:01:56,280 --> 00:02:00,420
like submarine alerts going off it's

00:01:58,409 --> 00:02:02,189
little blinking red lights but now we

00:02:00,420 --> 00:02:05,549
could actually see why those lights were

00:02:02,189 --> 00:02:08,509
blinking red um it was so successful

00:02:05,549 --> 00:02:11,300
that Tobias actually broke off the

00:02:08,509 --> 00:02:13,340
graphing graph storage and generation

00:02:11,300 --> 00:02:14,710
engine into rrdtool because other people

00:02:13,340 --> 00:02:17,300
wanted to do other things with it and

00:02:14,710 --> 00:02:20,120
this is where I spent most of my time

00:02:17,300 --> 00:02:23,150
writing cool stuff in the early 2000s

00:02:20,120 --> 00:02:26,360
was in rrdtool itself and it was great

00:02:23,150 --> 00:02:28,040
except for I had to I had to know what I

00:02:26,360 --> 00:02:29,630
was going to store when I was gonna

00:02:28,040 --> 00:02:31,580
store it how I was gonna store it how he

00:02:29,630 --> 00:02:38,300
was gonna graph it before I actually

00:02:31,580 --> 00:02:40,390
created the data source and that how

00:02:38,300 --> 00:02:43,870
many people know reverse polish notation

00:02:40,390 --> 00:02:48,620
put your hand down if you learned it

00:02:43,870 --> 00:02:51,110
outside of RD tool so most of the people

00:02:48,620 --> 00:02:52,550
learned it because of our D tool so

00:02:51,110 --> 00:02:54,890
where are we now with time-series

00:02:52,550 --> 00:02:57,020
databases there's a few open source

00:02:54,890 --> 00:02:58,760
solutions that are available and you

00:02:57,020 --> 00:03:00,290
know everyone has these cool like

00:02:58,760 --> 00:03:04,850
software as-a-service

00:03:00,290 --> 00:03:07,190
there's now ma-maa mas which is

00:03:04,850 --> 00:03:08,930
monitoring as a service so there's even

00:03:07,190 --> 00:03:10,640
hosted solutions that are available you

00:03:08,930 --> 00:03:13,430
installed our agent it sends your data

00:03:10,640 --> 00:03:14,510
into their systems there's some benefits

00:03:13,430 --> 00:03:15,590
to that you don't have to worry about

00:03:14,510 --> 00:03:16,760
monitoring your monitoring

00:03:15,590 --> 00:03:18,410
infrastructure because they monitor

00:03:16,760 --> 00:03:21,170
their monitoring infrastructure for you

00:03:18,410 --> 00:03:22,670
and but you do lose a little bit of

00:03:21,170 --> 00:03:25,459
control over the data that you're

00:03:22,670 --> 00:03:26,660
sending and if you're paranoid like you

00:03:25,459 --> 00:03:28,190
happen to work in a security industry

00:03:26,660 --> 00:03:30,020
you start to think about what people

00:03:28,190 --> 00:03:32,320
could glean from the monitoring data and

00:03:30,020 --> 00:03:39,230
that might freak you out a little bit so

00:03:32,320 --> 00:03:40,730
what do I use I use graphite if I'm good

00:03:39,230 --> 00:03:41,930
friends with Jason Dixon who's one of

00:03:40,730 --> 00:03:45,770
the project maintainers and if I don't

00:03:41,930 --> 00:03:48,380
use it he'll hurt me another option is

00:03:45,770 --> 00:03:51,170
in flux DB this is a newcomer to the

00:03:48,380 --> 00:03:54,980
scene it's really easy to send metrics

00:03:51,170 --> 00:03:59,980
there's this concept called metrics 2.0

00:03:54,980 --> 00:04:02,209
which is about it came out of graphite

00:03:59,980 --> 00:04:04,280
and we'll look at this in a little bit

00:04:02,209 --> 00:04:05,720
but the dotted path notation wasn't

00:04:04,280 --> 00:04:08,269
enough context for the monitoring

00:04:05,720 --> 00:04:10,790
metrics so key values needed to be added

00:04:08,269 --> 00:04:13,430
to the metrics as well and so metrics

00:04:10,790 --> 00:04:15,980
2.0 supports a key value system for

00:04:13,430 --> 00:04:17,900
storing metrics and influx database has

00:04:15,980 --> 00:04:20,539
that built in so it's it supports

00:04:17,900 --> 00:04:22,550
metrics 2.0 right out of the box and the

00:04:20,539 --> 00:04:23,960
interface is like sequel so if you know

00:04:22,550 --> 00:04:25,100
sequel you can get metrics

00:04:23,960 --> 00:04:27,289
you can combine metrics you can

00:04:25,100 --> 00:04:30,770
aggregate them which sounds really great

00:04:27,289 --> 00:04:31,880
the downside is that in flux DB doesn't

00:04:30,770 --> 00:04:33,830
read scale very well

00:04:31,880 --> 00:04:35,330
it writes scales better than graphite so

00:04:33,830 --> 00:04:36,590
if you want to send a ton of data to it

00:04:35,330 --> 00:04:39,800
you can do that but if you want to get

00:04:36,590 --> 00:04:42,349
that data back out then it's not quite

00:04:39,800 --> 00:04:44,750
as good at that you will not be able to

00:04:42,349 --> 00:04:47,120
do one-to-one right to read with with in

00:04:44,750 --> 00:04:49,520
flux DB at the time that I at the time

00:04:47,120 --> 00:04:51,110
that I was involved in the evaluation of

00:04:49,520 --> 00:04:52,370
this it is giving better they are

00:04:51,110 --> 00:04:54,169
learning and they are making

00:04:52,370 --> 00:04:57,560
improvements but it's still not as good

00:04:54,169 --> 00:05:00,259
read performance as graphite is then

00:04:57,560 --> 00:05:02,419
there's open TS d be open TS DB again

00:05:00,259 --> 00:05:05,870
supports metrics 2.0 it's easy to get

00:05:02,419 --> 00:05:07,430
metrics in it has an HBase back-end so

00:05:05,870 --> 00:05:10,099
you don't need to roll up any metrics

00:05:07,430 --> 00:05:11,449
you one of the things with with graphite

00:05:10,099 --> 00:05:13,280
which we'll talk about as metrics need

00:05:11,449 --> 00:05:15,979
to you aggregate they need to roll up at

00:05:13,280 --> 00:05:17,270
certain boundaries with open TS DB you

00:05:15,979 --> 00:05:19,220
can just forfeit that all because

00:05:17,270 --> 00:05:21,590
everything's in Hadoop anyways it's big

00:05:19,220 --> 00:05:23,659
data again the downside is that the

00:05:21,590 --> 00:05:26,180
reality is compromised for that large

00:05:23,659 --> 00:05:28,610
storage option so if you need to do lots

00:05:26,180 --> 00:05:30,610
of reads against your your time series

00:05:28,610 --> 00:05:33,259
database this may not be the greatest

00:05:30,610 --> 00:05:35,659
solution and also some people would put

00:05:33,259 --> 00:05:38,240
in HBase back-end in the con it's up to

00:05:35,659 --> 00:05:40,520
you it's your organization your mouth

00:05:38,240 --> 00:05:42,169
may bury and that's ok use what makes

00:05:40,520 --> 00:05:44,449
sense those are the options I'm going to

00:05:42,169 --> 00:05:45,889
talk to you today about graphite because

00:05:44,449 --> 00:05:49,009
that's what I have the most experience

00:05:45,889 --> 00:05:50,539
with and I have yet to come across an

00:05:49,009 --> 00:05:53,270
instance where it hasn't solved my

00:05:50,539 --> 00:05:55,699
problems I use it because it's an open

00:05:53,270 --> 00:05:58,789
source there's a ton of projects in the

00:05:55,699 --> 00:06:00,800
that interface with graphite you

00:05:58,789 --> 00:06:02,590
probably don't need to write something

00:06:00,800 --> 00:06:05,419
because something's already out there

00:06:02,590 --> 00:06:06,919
it's scalable so we can start with a

00:06:05,419 --> 00:06:09,620
docker instance we can start with the

00:06:06,919 --> 00:06:12,409
VMware image of graphite and then we can

00:06:09,620 --> 00:06:14,419
scale that instance of graphite up into

00:06:12,409 --> 00:06:17,780
something that can handle millions and

00:06:14,419 --> 00:06:20,960
millions of metric per second it's easy

00:06:17,780 --> 00:06:23,719
to use for me as an administrator for

00:06:20,960 --> 00:06:25,940
you as a developer for a product owner

00:06:23,719 --> 00:06:27,680
who wants to find out what's going on in

00:06:25,940 --> 00:06:29,160
the system it's very easy for them to

00:06:27,680 --> 00:06:33,180
use as well and this is

00:06:29,160 --> 00:06:36,030
important thing it's composable the guys

00:06:33,180 --> 00:06:37,800
who started graphite kind of decided to

00:06:36,030 --> 00:06:40,500
you apply the UNIX philosophy to

00:06:37,800 --> 00:06:43,260
monitoring its do one thing well and

00:06:40,500 --> 00:06:44,850
allow interoperability with other tools

00:06:43,260 --> 00:06:47,850
so part of the reason there's such a

00:06:44,850 --> 00:06:50,310
large ecosystem of tools in the graphite

00:06:47,850 --> 00:06:52,260
world is because they came at it from

00:06:50,310 --> 00:06:54,330
we're going to provide an API and that

00:06:52,260 --> 00:06:56,190
API is going to be consistent and every

00:06:54,330 --> 00:06:58,170
point in the system is going to be

00:06:56,190 --> 00:07:00,150
interchangeable so if you don't like the

00:06:58,170 --> 00:07:01,770
way one piece of the infrastructure

00:07:00,150 --> 00:07:05,190
works you can pull it out so long as you

00:07:01,770 --> 00:07:08,760
maintain the API in and the the data

00:07:05,190 --> 00:07:11,730
points out and because of that it's just

00:07:08,760 --> 00:07:13,290
you can do anything with it you're not

00:07:11,730 --> 00:07:15,240
locked in like you are with something

00:07:13,290 --> 00:07:17,640
like cacti where you have a

00:07:15,240 --> 00:07:20,150
configuration it's a black box with

00:07:17,640 --> 00:07:23,060
graphite your data is is free to flow

00:07:20,150 --> 00:07:25,560
however you'd like to do it it's a

00:07:23,060 --> 00:07:27,000
Monsieur do if you're developing an

00:07:25,560 --> 00:07:30,150
open-source project you need to think

00:07:27,000 --> 00:07:31,860
about how can I exchange my data freely

00:07:30,150 --> 00:07:33,900
with other people because that's really

00:07:31,860 --> 00:07:36,570
gonna that's really what some given

00:07:33,900 --> 00:07:38,310
graphite its power and of course it's

00:07:36,570 --> 00:07:41,670
fun I love looking at crafts

00:07:38,310 --> 00:07:44,040
I love mutating graphs and and finding

00:07:41,670 --> 00:07:46,110
trends and and being able to actually

00:07:44,040 --> 00:07:48,360
have data to come to someone and say

00:07:46,110 --> 00:07:50,669
here's why your process failed here's

00:07:48,360 --> 00:07:53,010
why this this was slow here's why this

00:07:50,669 --> 00:07:56,490
is succeeding let's let's take a look at

00:07:53,010 --> 00:08:01,140
it it's a lot of fun so what is time

00:07:56,490 --> 00:08:04,380
series data time series is a measurement

00:08:01,140 --> 00:08:07,490
at fixed intervals you can't have two

00:08:04,380 --> 00:08:11,460
measurements for the same data point and

00:08:07,490 --> 00:08:13,680
in in graphite specifically if you have

00:08:11,460 --> 00:08:15,900
a bucket of time and you have two

00:08:13,680 --> 00:08:17,520
readings the last one that gets to the

00:08:15,900 --> 00:08:20,010
storage engine is the one that wins it

00:08:17,520 --> 00:08:23,430
may not be the most recent it could be

00:08:20,010 --> 00:08:25,020
one that arrives out of order but the

00:08:23,430 --> 00:08:27,630
last one to that bucket wins it

00:08:25,020 --> 00:08:29,190
overwrites the the buckets and again the

00:08:27,630 --> 00:08:31,340
important thing to think about here is

00:08:29,190 --> 00:08:34,400
these are things that are measured at at

00:08:31,340 --> 00:08:37,800
consistent intervals right so I'm

00:08:34,400 --> 00:08:40,920
taking this counter at this every minute

00:08:37,800 --> 00:08:42,840
every every 10 seconds if there's going

00:08:40,920 --> 00:08:45,210
to be a long period of time where

00:08:42,840 --> 00:08:47,280
there's not going to be an event that's

00:08:45,210 --> 00:08:50,640
not really the place for a time series

00:08:47,280 --> 00:08:52,320
database because that reading unless

00:08:50,640 --> 00:08:54,090
it's important right so the only

00:08:52,320 --> 00:08:56,100
exception to this I that I can think of

00:08:54,090 --> 00:08:59,850
offhand is like error rate on an

00:08:56,100 --> 00:09:02,730
interface unlike a NIC where you expect

00:08:59,850 --> 00:09:04,530
that counter to remain at zero except

00:09:02,730 --> 00:09:07,080
when it goes up and then then you really

00:09:04,530 --> 00:09:09,450
want to know about it but for the most

00:09:07,080 --> 00:09:11,370
part if you're if you have like an alert

00:09:09,450 --> 00:09:13,800
and in Nagios that is not time series

00:09:11,370 --> 00:09:16,440
data maybe the number of alerts per day

00:09:13,800 --> 00:09:18,720
is time series data so you have to think

00:09:16,440 --> 00:09:20,520
about how you what your data actually is

00:09:18,720 --> 00:09:23,310
don't just stuff everything into

00:09:20,520 --> 00:09:26,430
graphite but stuff everything into

00:09:23,310 --> 00:09:28,560
graphite so if you've worked with

00:09:26,430 --> 00:09:30,600
rrdtool you may be familiar with the

00:09:28,560 --> 00:09:34,620
concept of gages and counters graphite

00:09:30,600 --> 00:09:36,840
doesn't care it's up to you to know what

00:09:34,620 --> 00:09:38,520
you're storing and how to retrieve it so

00:09:36,840 --> 00:09:39,960
the storage and retrieval are two

00:09:38,520 --> 00:09:41,250
separate parts so it doesn't matter if

00:09:39,960 --> 00:09:43,230
it's a gauge or a counter you give it a

00:09:41,250 --> 00:09:47,010
value it stores it that's it

00:09:43,230 --> 00:09:50,130
it's simple gauges encounters don't use

00:09:47,010 --> 00:09:52,620
gauges gauges will hide problems so if

00:09:50,130 --> 00:09:56,010
you're looking you know interface bits

00:09:52,620 --> 00:09:58,170
per second as a gauge and you measure it

00:09:56,010 --> 00:10:00,090
and then in between the minute that you

00:09:58,170 --> 00:10:01,980
measure it you have a huge spike and

00:10:00,090 --> 00:10:04,350
then it drops back down again

00:10:01,980 --> 00:10:06,060
you just lost that data or with a

00:10:04,350 --> 00:10:08,090
counter you may not be able to find the

00:10:06,060 --> 00:10:10,320
exact microsecond that that particular

00:10:08,090 --> 00:10:12,150
spike occurred but you will be able to

00:10:10,320 --> 00:10:14,610
see that there is a spike in that minute

00:10:12,150 --> 00:10:16,380
interval and then if you have problems

00:10:14,610 --> 00:10:18,180
with with microburst and stuff like that

00:10:16,380 --> 00:10:19,560
you can adjust your your time series

00:10:18,180 --> 00:10:22,080
retention so that you can pull more

00:10:19,560 --> 00:10:24,090
frequently so favored counters over

00:10:22,080 --> 00:10:29,100
gauges

00:10:24,090 --> 00:10:31,410
how does graphite work I already kind of

00:10:29,100 --> 00:10:33,660
alluded to this but we have a dotted dot

00:10:31,410 --> 00:10:35,990
separated namespace here you can see

00:10:33,660 --> 00:10:40,260
this is what a metric name looks like

00:10:35,990 --> 00:10:42,030
you might have what I've adopted more

00:10:40,260 --> 00:10:44,640
recently is using the collector name as

00:10:42,030 --> 00:10:47,490
the first piece followed by the data

00:10:44,640 --> 00:10:50,640
center followed by the poster role or

00:10:47,490 --> 00:10:52,950
the role or build of the server followed

00:10:50,640 --> 00:10:56,400
by the host name the type of metric and

00:10:52,950 --> 00:10:57,840
then the metric identifier the metrics

00:10:56,400 --> 00:11:00,180
are all created automatically on the

00:10:57,840 --> 00:11:01,650
first time they're they're stored so you

00:11:00,180 --> 00:11:03,960
don't need to pre declare these the

00:11:01,650 --> 00:11:05,460
second that you write them to the to the

00:11:03,960 --> 00:11:07,380
graphite instance they're created if

00:11:05,460 --> 00:11:10,590
they don't exist already all the date

00:11:07,380 --> 00:11:12,210
all the storage is pre-allocated so once

00:11:10,590 --> 00:11:14,640
you create the metric there's no need

00:11:12,210 --> 00:11:17,040
for the disk to really go through a lot

00:11:14,640 --> 00:11:18,840
of intensive i/o to add additional

00:11:17,040 --> 00:11:21,240
things on every the storage is

00:11:18,840 --> 00:11:25,380
completely allocated there's multiple

00:11:21,240 --> 00:11:28,890
storage engines you can try series or

00:11:25,380 --> 00:11:30,840
signing they're nice they have a little

00:11:28,890 --> 00:11:33,000
bit of additional you know

00:11:30,840 --> 00:11:35,250
sharding and and replication features

00:11:33,000 --> 00:11:37,620
built in but for most purposes the

00:11:35,250 --> 00:11:40,020
whisper flat file database datastore is

00:11:37,620 --> 00:11:42,080
more than sufficient and actually you

00:11:40,020 --> 00:11:47,120
can get a lot of scale out of whisper

00:11:42,080 --> 00:11:47,120
just by sharding the data using relays

00:11:47,570 --> 00:11:54,390
for queries you can ask for a metric by

00:11:51,840 --> 00:11:58,260
name you can ask for all the metrics of

00:11:54,390 --> 00:12:02,670
a particular thing using stars you can

00:11:58,260 --> 00:12:06,150
then combine mutate or select find

00:12:02,670 --> 00:12:08,130
fine-tune your pickings using functions

00:12:06,150 --> 00:12:09,330
the nice thing about graphite and the

00:12:08,130 --> 00:12:12,360
most powerful thing and one of the

00:12:09,330 --> 00:12:14,370
reasons why if you I think if you're

00:12:12,360 --> 00:12:16,380
doing monitoring right you have a higher

00:12:14,370 --> 00:12:18,510
read than write load on your monitoring

00:12:16,380 --> 00:12:21,240
system is because you can return this

00:12:18,510 --> 00:12:23,460
data as graphs so you can look at it as

00:12:21,240 --> 00:12:24,810
a human but I don't really want to watch

00:12:23,460 --> 00:12:27,600
graphs all day

00:12:24,810 --> 00:12:30,180
I want to write code and so I can use

00:12:27,600 --> 00:12:33,840
the JSON output to actually use that

00:12:30,180 --> 00:12:35,730
data to make intelligent decisions

00:12:33,840 --> 00:12:38,690
I like Computers monitor computers

00:12:35,730 --> 00:12:43,670
because they're better at it than I am

00:12:38,690 --> 00:12:43,670
so there's a few components go ahead

00:12:52,140 --> 00:12:57,990
okay so the question is that the last

00:12:55,200 --> 00:12:59,640
value wins in a collision with a metric

00:12:57,990 --> 00:13:01,560
name is there a way to configure that so

00:12:59,640 --> 00:13:05,490
there's a different set of rules for

00:13:01,560 --> 00:13:09,480
that and the answer is no with an

00:13:05,490 --> 00:13:12,630
asterisk it depends you can you can get

00:13:09,480 --> 00:13:14,399
very clever we'll talk about this maybe

00:13:12,630 --> 00:13:16,500
I hope I think it's in here but there's

00:13:14,399 --> 00:13:18,690
there's ways that you can relay metrics

00:13:16,500 --> 00:13:21,209
through two aggregators so you could

00:13:18,690 --> 00:13:24,329
potentially say you know okay for this

00:13:21,209 --> 00:13:26,730
metric I want to if it has this name I

00:13:24,329 --> 00:13:28,440
want to count that I want to sum these I

00:13:26,730 --> 00:13:30,149
want to average them I want the median

00:13:28,440 --> 00:13:32,220
I'd like the standard deviation the

00:13:30,149 --> 00:13:34,920
variance you can do all of those types

00:13:32,220 --> 00:13:37,410
of things in an aggregator so you can

00:13:34,920 --> 00:13:39,690
provide some wrappers around that but by

00:13:37,410 --> 00:13:41,310
default when it gets to the back end and

00:13:39,690 --> 00:13:44,250
storage back end if you say there's one

00:13:41,310 --> 00:13:45,870
point per minute and you have a metric

00:13:44,250 --> 00:13:47,760
coming in a finalized metric that's

00:13:45,870 --> 00:13:49,260
through your aggregators that's every

00:13:47,760 --> 00:13:50,790
ten seconds for some reason because

00:13:49,260 --> 00:13:55,170
there's a mismatch between the storage

00:13:50,790 --> 00:13:58,019
engine and the generation the last ten

00:13:55,170 --> 00:14:00,839
second update to arrive at the store not

00:13:58,019 --> 00:14:03,329
the last one sequentially by epoch but

00:14:00,839 --> 00:14:05,880
the last one to arrive at the store will

00:14:03,329 --> 00:14:08,180
win and if you're doing replication and

00:14:05,880 --> 00:14:10,769
the metrics arrive out of order and

00:14:08,180 --> 00:14:12,930
they're different values you can end up

00:14:10,769 --> 00:14:15,209
with two nodes in the same cluster that

00:14:12,930 --> 00:14:17,670
have two different values because the

00:14:15,209 --> 00:14:19,320
last value was different because this

00:14:17,670 --> 00:14:23,160
one received it out of order this one

00:14:19,320 --> 00:14:25,709
received it in order so it's not super

00:14:23,160 --> 00:14:28,140
precise in that regard you can you can

00:14:25,709 --> 00:14:31,020
take some precautions using relays and

00:14:28,140 --> 00:14:35,170
aggregators to prevent that

00:14:31,020 --> 00:14:38,050
so there's a there's a few components of

00:14:35,170 --> 00:14:39,310
graphite carbon is for routing and

00:14:38,050 --> 00:14:41,290
storing metrics there's a few diamonds

00:14:39,310 --> 00:14:44,520
in there there's a carbon cache carbon

00:14:41,290 --> 00:14:47,190
relay and carbon aggregator Damon I

00:14:44,520 --> 00:14:50,050
think I'm going to get into those but

00:14:47,190 --> 00:14:52,600
carbon is is the backend that allows you

00:14:50,050 --> 00:14:54,460
to that the graphite web frontend

00:14:52,600 --> 00:14:58,270
actually queries to get the metrics and

00:14:54,460 --> 00:15:01,720
you send the metrics to and it's it's

00:14:58,270 --> 00:15:04,450
called carbon cache because it uses in

00:15:01,720 --> 00:15:06,190
in-memory caching under the assumption

00:15:04,450 --> 00:15:07,960
that the most recent metrics are the

00:15:06,190 --> 00:15:11,560
ones you care about the most

00:15:07,960 --> 00:15:13,630
so not only does it service storing the

00:15:11,560 --> 00:15:15,580
metrics but it keeps those metrics in

00:15:13,630 --> 00:15:17,080
memory for a little while so that as you

00:15:15,580 --> 00:15:18,430
query them with a graphite web it

00:15:17,080 --> 00:15:20,130
doesn't have to read them from disk

00:15:18,430 --> 00:15:25,080
which is kind of nice

00:15:20,130 --> 00:15:27,850
whisper is the flat storage file

00:15:25,080 --> 00:15:29,680
mechanism behind carbon so when carbon

00:15:27,850 --> 00:15:34,060
writes came right to Ceres syenite or

00:15:29,680 --> 00:15:36,550
whisper and whisper is the default it's

00:15:34,060 --> 00:15:40,510
kind of like already it's you know WSP

00:15:36,550 --> 00:15:42,100
is like a dot rrd so how does this scale

00:15:40,510 --> 00:15:45,910
because that's always a big question

00:15:42,100 --> 00:15:48,220
and the thing is if it's scales really

00:15:45,910 --> 00:15:49,840
well Jason Dixon is writing this this

00:15:48,220 --> 00:15:51,310
book monitoring with graphite I think

00:15:49,840 --> 00:15:54,790
almost all the chapters are done now

00:15:51,310 --> 00:15:56,620
it's up on safari on O'Reilly and you

00:15:54,790 --> 00:15:58,150
can if you buy the pre-order you can

00:15:56,620 --> 00:16:01,600
actually read it online it's it's

00:15:58,150 --> 00:16:03,970
amazing I can't do justice to this topic

00:16:01,600 --> 00:16:06,550
in the length of this talk there's a

00:16:03,970 --> 00:16:09,340
whole chapter on on scaling graphite in

00:16:06,550 --> 00:16:12,160
this book it's amazing do stuff on SSDs

00:16:09,340 --> 00:16:14,530
because I mean there's there's that and

00:16:12,160 --> 00:16:18,390
yeah they do fail because there is a

00:16:14,530 --> 00:16:20,890
high read and write load on these boxes

00:16:18,390 --> 00:16:22,150
when I was working for booking some of

00:16:20,890 --> 00:16:24,610
the guys at booking wrote some

00:16:22,150 --> 00:16:26,500
components that drop into the graphite

00:16:24,610 --> 00:16:27,670
infrastructure because like I said every

00:16:26,500 --> 00:16:30,010
piece of the infrastructure is pretty

00:16:27,670 --> 00:16:32,650
much you can just like have a TCP relay

00:16:30,010 --> 00:16:34,900
that talks from 2003 to 2013 and you can

00:16:32,650 --> 00:16:37,480
do whatever you want in the middle so

00:16:34,900 --> 00:16:38,710
these these are some tools I recommend

00:16:37,480 --> 00:16:40,370
checking out if you're especially

00:16:38,710 --> 00:16:42,589
interested in getting high

00:16:40,370 --> 00:16:44,630
performance from graphite these things

00:16:42,589 --> 00:16:46,820
solve a lot of issues that you might run

00:16:44,630 --> 00:16:51,110
into if you have very high read loads

00:16:46,820 --> 00:16:53,240
which means you're doing it right so I

00:16:51,110 --> 00:16:55,610
get all ask us now that like influx is

00:16:53,240 --> 00:16:59,620
taking off like in the Twitterverse

00:16:55,610 --> 00:17:02,420
would I deploy graphite today absolutely

00:16:59,620 --> 00:17:04,790
influx is still too new and and still

00:17:02,420 --> 00:17:06,620
has a lot of they're they're still

00:17:04,790 --> 00:17:07,880
learning a lot so they have a lot of

00:17:06,620 --> 00:17:10,250
really good groundwork that they're

00:17:07,880 --> 00:17:14,920
putting down but for production stuff

00:17:10,250 --> 00:17:18,170
that I rely on I would use graphite so

00:17:14,920 --> 00:17:21,050
graphite stores data in whisper files on

00:17:18,170 --> 00:17:23,059
every file is at every dotted notation

00:17:21,050 --> 00:17:24,770
that we just looked at is a file is a

00:17:23,059 --> 00:17:27,260
directory or file and disk so it looks

00:17:24,770 --> 00:17:32,809
like this right so if you have sis data

00:17:27,260 --> 00:17:34,250
center zone host metric you know it's

00:17:32,809 --> 00:17:35,900
pretty simple and that's how the

00:17:34,250 --> 00:17:37,429
searches work they're just glob searches

00:17:35,900 --> 00:17:39,650
in Python so it searches through the

00:17:37,429 --> 00:17:41,540
file system so anything that you would

00:17:39,650 --> 00:17:43,550
use to search for a file on the phone

00:17:41,540 --> 00:17:47,960
disk you can use in graphite to resolve

00:17:43,550 --> 00:17:49,340
a metric as data ages it becomes less

00:17:47,960 --> 00:17:51,440
important and that's it becomes less

00:17:49,340 --> 00:17:53,720
important generally we just discard some

00:17:51,440 --> 00:17:55,610
of that data so what the default is is

00:17:53,720 --> 00:17:58,040
basically just to take an average of all

00:17:55,610 --> 00:17:59,900
those buckets and take like five of

00:17:58,040 --> 00:18:02,150
those buckets and put them into one one

00:17:59,900 --> 00:18:04,340
with an average as we do that though we

00:18:02,150 --> 00:18:06,620
lose granularity and this is an

00:18:04,340 --> 00:18:08,030
important concept to understand you can

00:18:06,620 --> 00:18:09,710
you can change this because you can

00:18:08,030 --> 00:18:11,120
change aggregation methods you can

00:18:09,710 --> 00:18:13,520
duplicate metrics and then change the

00:18:11,120 --> 00:18:16,130
aggregation method met methods that

00:18:13,520 --> 00:18:17,750
those duplicated metrics use so you can

00:18:16,130 --> 00:18:21,470
preserve things like the average the

00:18:17,750 --> 00:18:24,010
median the min the max and and or the

00:18:21,470 --> 00:18:24,010
last value

00:18:26,290 --> 00:18:31,750
this is the storage schema which defines

00:18:30,010 --> 00:18:35,710
the retention periods for your metrics

00:18:31,750 --> 00:18:38,500
the default is 60 seconds for 14 well

00:18:35,710 --> 00:18:41,860
actually the default is 60 seconds for

00:18:38,500 --> 00:18:43,270
one day so I like having that as the

00:18:41,860 --> 00:18:45,040
default because if somebody creates a

00:18:43,270 --> 00:18:46,840
new metric path in the graphite

00:18:45,040 --> 00:18:48,670
infrastructure they can't fill up all

00:18:46,840 --> 00:18:51,550
the disks on all of the graphite servers

00:18:48,670 --> 00:18:53,230
which has happened I've yeah

00:18:51,550 --> 00:18:54,400
you know like somebody gets a great idea

00:18:53,230 --> 00:19:00,280
that they want to count the number of

00:18:54,400 --> 00:19:03,370
requests from each IP address there's

00:19:00,280 --> 00:19:06,430
only 255 in each each bucket that's not

00:19:03,370 --> 00:19:08,260
really not that big so yeah we're not

00:19:06,430 --> 00:19:10,660
good at exponential math and so I like

00:19:08,260 --> 00:19:13,270
having that so that it's a cutoff point

00:19:10,660 --> 00:19:15,460
but this is actually what would I run

00:19:13,270 --> 00:19:17,950
with now in production is basically 60

00:19:15,460 --> 00:19:20,650
seconds for 14 days and then 30 minutes

00:19:17,950 --> 00:19:22,720
for two years is the default you might

00:19:20,650 --> 00:19:27,310
want to like have a different like TMP

00:19:22,720 --> 00:19:28,510
that has a smaller retention period and

00:19:27,310 --> 00:19:31,330
you can have in many of these roll-ups

00:19:28,510 --> 00:19:33,970
as you'd like and and you can configure

00:19:31,330 --> 00:19:35,590
them however you'd like

00:19:33,970 --> 00:19:37,540
then there's the aggregators and what

00:19:35,590 --> 00:19:40,240
the aggregators do is they perform how

00:19:37,540 --> 00:19:42,700
do how are those boundaries handled so

00:19:40,240 --> 00:19:43,750
the default is this is the graphite

00:19:42,700 --> 00:19:45,580
default and if you haven't touched

00:19:43,750 --> 00:19:48,130
anything you don't have a storage

00:19:45,580 --> 00:19:50,440
aggregation com this is what happens is

00:19:48,130 --> 00:19:52,960
you need to have at least 50% of the

00:19:50,440 --> 00:19:54,550
data in that bucket in those series of

00:19:52,960 --> 00:19:59,170
buckets to perform the roll-up and it

00:19:54,550 --> 00:20:02,170
uses the average to roll it up so

00:19:59,170 --> 00:20:03,070
x-files factor it's not fear factor for

00:20:02,170 --> 00:20:07,330
aliens

00:20:03,070 --> 00:20:09,280
it is a float between 0 & 1 and that is

00:20:07,330 --> 00:20:11,740
the number of non null data points that

00:20:09,280 --> 00:20:16,180
must be present for an aggregation to

00:20:11,740 --> 00:20:18,280
not roll up to null so how much you

00:20:16,180 --> 00:20:20,650
tolerate nulls in your data stream

00:20:18,280 --> 00:20:22,990
because if you have nulls in your data

00:20:20,650 --> 00:20:25,240
stream then your average on one bucket

00:20:22,990 --> 00:20:27,160
may only contain three metrics and the

00:20:25,240 --> 00:20:29,950
next bucket may contain five metrics and

00:20:27,160 --> 00:20:31,390
now your average is not exactly what you

00:20:29,950 --> 00:20:34,780
expect it to be

00:20:31,390 --> 00:20:36,550
so I you have to be conscious of this

00:20:34,780 --> 00:20:38,890
and maybe your tolerance is higher for

00:20:36,550 --> 00:20:40,810
nulls or maybe it's lower and you want

00:20:38,890 --> 00:20:44,530
to have everything there before you

00:20:40,810 --> 00:20:48,670
perform a rollup the aggregator

00:20:44,530 --> 00:20:51,640
functions our average min max sum and

00:20:48,670 --> 00:20:53,590
last so if you have a count if you have

00:20:51,640 --> 00:20:56,140
a counter object that you're storing

00:20:53,590 --> 00:20:57,970
data on you might want to use last as

00:20:56,140 --> 00:20:59,800
the aggregation method because that

00:20:57,970 --> 00:21:02,650
makes more sense than average for a

00:20:59,800 --> 00:21:04,270
counter if you have and something that

00:21:02,650 --> 00:21:07,480
you're cat like an event that's

00:21:04,270 --> 00:21:08,740
occurring you might want to use some to

00:21:07,480 --> 00:21:10,390
count the number of events that have

00:21:08,740 --> 00:21:12,130
occurred in that time period so when you

00:21:10,390 --> 00:21:13,720
get a five-minute roll-up you're

00:21:12,130 --> 00:21:16,240
counting all of the events that occurred

00:21:13,720 --> 00:21:17,310
in that 5-minute point does that make

00:21:16,240 --> 00:21:20,650
sense

00:21:17,310 --> 00:21:22,810
and then min max an average you can use

00:21:20,650 --> 00:21:30,040
like if you what I usually do is I'll

00:21:22,810 --> 00:21:32,620
have an entry in in here that says dot

00:21:30,040 --> 00:21:34,540
star underscore min and then I'll change

00:21:32,620 --> 00:21:39,400
the aggregation method to min for that

00:21:34,540 --> 00:21:43,120
bucket or max the same thing you might

00:21:39,400 --> 00:21:48,280
come across this on occasion and there

00:21:43,120 --> 00:21:50,110
be dragons here so this is somebody got

00:21:48,280 --> 00:21:53,500
an interesting idea to store all the

00:21:50,110 --> 00:21:57,070
Nagios alerts on a minute li basis by

00:21:53,500 --> 00:22:00,130
host in graphite which is just wasting a

00:21:57,070 --> 00:22:01,840
ton of space and because most of the

00:22:00,130 --> 00:22:04,000
time there's no data for a host in

00:22:01,840 --> 00:22:06,250
Nagios the x-files factor was set to

00:22:04,000 --> 00:22:07,930
zero which meant always aggregate

00:22:06,250 --> 00:22:10,630
regardless of how many things are there

00:22:07,930 --> 00:22:12,430
and aggregate on some this may not be

00:22:10,630 --> 00:22:15,160
the right place for that particular data

00:22:12,430 --> 00:22:17,080
if you see something like this and again

00:22:15,160 --> 00:22:18,730
remember as you roll data up across

00:22:17,080 --> 00:22:22,200
retention boundaries you lose

00:22:18,730 --> 00:22:24,760
granularity it's pretty common sense but

00:22:22,200 --> 00:22:26,890
you know there's a one minute I've heard

00:22:24,760 --> 00:22:29,530
there's one minute and we have a a high

00:22:26,890 --> 00:22:31,930
of 80 after five minutes that's dropped

00:22:29,530 --> 00:22:32,940
to 28 and then after 25 minutes it's

00:22:31,930 --> 00:22:35,370
down to 9 points

00:22:32,940 --> 00:22:40,410
six so you will lose granularity as you

00:22:35,370 --> 00:22:43,490
go across retention bed boundaries so

00:22:40,410 --> 00:22:45,900
how do I use it how do I get data there

00:22:43,490 --> 00:22:48,000
that's it that's all you have to do

00:22:45,900 --> 00:22:50,970
wherever graphite is just send it the

00:22:48,000 --> 00:22:52,650
odd notation send it the value and send

00:22:50,970 --> 00:22:54,870
it at the epoch of when that that event

00:22:52,650 --> 00:22:56,760
was generated and if it doesn't exist

00:22:54,870 --> 00:22:58,980
its created if it does exist it's

00:22:56,760 --> 00:23:03,000
updated it's really simple like that

00:22:58,980 --> 00:23:06,450
beats the crap out of our ernie tool the

00:23:03,000 --> 00:23:07,650
important thing to is I don't even like

00:23:06,450 --> 00:23:09,570
telling people this but epoch is

00:23:07,650 --> 00:23:11,220
optional and if it's not specified it

00:23:09,570 --> 00:23:13,230
will use the epoch when it gets to a

00:23:11,220 --> 00:23:15,090
storage engine which may or may not be

00:23:13,230 --> 00:23:17,820
when the actual event was generated so

00:23:15,090 --> 00:23:21,500
always send an epoch with your data and

00:23:17,820 --> 00:23:21,500
if you can enforce that in a relay

00:23:22,520 --> 00:23:27,420
there's a lot of ways to get a lot of

00:23:25,020 --> 00:23:32,730
libraries like search CDN for graphite

00:23:27,420 --> 00:23:34,290
there's like six or seven modules but

00:23:32,730 --> 00:23:37,740
you heard time series database and you

00:23:34,290 --> 00:23:39,510
came here for pretty pictures so here's

00:23:37,740 --> 00:23:42,480
an ugly picture oh wow that does not

00:23:39,510 --> 00:23:44,160
look good at all I tried to use the

00:23:42,480 --> 00:23:46,380
Craigslist purple but it didn't work out

00:23:44,160 --> 00:23:48,840
so anyways across the top in this in

00:23:46,380 --> 00:23:51,480
this composer window which is the middle

00:23:48,840 --> 00:23:52,710
window here you have a refresh button if

00:23:51,480 --> 00:23:54,540
you click that button it will refresh

00:23:52,710 --> 00:23:56,220
the data in the graph the next one over

00:23:54,540 --> 00:24:00,300
is absolute time so if you want to say

00:23:56,220 --> 00:24:03,540
between you know 2016 6th of June at

00:24:00,300 --> 00:24:05,580
10:00 p.m. and 8:00 p.m. you can use

00:24:03,540 --> 00:24:07,980
that to select an absolute time the next

00:24:05,580 --> 00:24:10,500
button over is is a relative time so if

00:24:07,980 --> 00:24:13,200
you miss a last 24 hours you can use

00:24:10,500 --> 00:24:15,930
that button to do it the one with the

00:24:13,200 --> 00:24:19,200
bar and the arrow up this is awesome

00:24:15,930 --> 00:24:22,680
sauce right here so if somebody pastes a

00:24:19,200 --> 00:24:24,750
graphite URL into a channel at work you

00:24:22,680 --> 00:24:27,090
can copy that URL click that button

00:24:24,750 --> 00:24:28,590
paste that URL in there and boom you

00:24:27,090 --> 00:24:31,080
have that graph in the composer window

00:24:28,590 --> 00:24:32,760
which means you can manipulate it so one

00:24:31,080 --> 00:24:35,970
of the things that I like to do is go

00:24:32,760 --> 00:24:38,040
heywhy min equals zero because people

00:24:35,970 --> 00:24:40,080
like to skew

00:24:38,040 --> 00:24:42,630
correlation by not having a

00:24:40,080 --> 00:24:46,649
to zero because graphite will render

00:24:42,630 --> 00:24:49,169
this this graph to fit the window so

00:24:46,649 --> 00:24:51,299
it's going to try to constrain whatever

00:24:49,169 --> 00:24:55,019
metric you have to the size of the

00:24:51,299 --> 00:24:56,370
window so the the y-axis is going to be

00:24:55,019 --> 00:24:58,799
wherever it needs to be to make the

00:24:56,370 --> 00:25:00,269
prettiest graph basically some most of

00:24:58,799 --> 00:25:01,409
the time you need to set Y min equal to

00:25:00,269 --> 00:25:05,190
zero and then all of a sudden your

00:25:01,409 --> 00:25:08,159
correlation disappears the next one over

00:25:05,190 --> 00:25:10,590
is a short URL generator so if you want

00:25:08,159 --> 00:25:13,169
to paste a URL into a channel you can

00:25:10,590 --> 00:25:14,639
generate a short URL and and get to it

00:25:13,169 --> 00:25:17,490
there and it doesn't mess up in

00:25:14,639 --> 00:25:20,519
somebody's jabber client or IRC client

00:25:17,490 --> 00:25:23,970
on the left side you have a metrics

00:25:20,519 --> 00:25:27,330
browser I use that occasionally there's

00:25:23,970 --> 00:25:29,309
also a user graphs but this this is my

00:25:27,330 --> 00:25:30,600
this is data exploration right here this

00:25:29,309 --> 00:25:33,750
is really the power of graphite it's

00:25:30,600 --> 00:25:35,250
horribly ugly and you don't realize how

00:25:33,750 --> 00:25:37,409
powerful this is until you start playing

00:25:35,250 --> 00:25:39,750
with it this is the main reason why

00:25:37,409 --> 00:25:42,029
graphite beats almost anything out there

00:25:39,750 --> 00:25:46,470
in the graphing space is because from

00:25:42,029 --> 00:25:48,809
here I can explore my data and there's

00:25:46,470 --> 00:25:51,870
no constraints on what I can do

00:25:48,809 --> 00:25:54,360
there's no presumptions about how I want

00:25:51,870 --> 00:25:57,179
to see the data I can control everything

00:25:54,360 --> 00:26:00,440
about the graph and the data that I'm

00:25:57,179 --> 00:26:00,440
retrieving here yes

00:26:03,320 --> 00:26:07,870
the question is does it have facilities

00:26:05,780 --> 00:26:11,180
for correlating to time series databases

00:26:07,870 --> 00:26:13,580
no it'll only use what this graphite web

00:26:11,180 --> 00:26:15,800
server is set up to use as data sources

00:26:13,580 --> 00:26:18,170
now if you have a data source that can

00:26:15,800 --> 00:26:20,390
speak graphite you can set it up in the

00:26:18,170 --> 00:26:22,400
graphite web configuration to also be a

00:26:20,390 --> 00:26:23,900
peer and those metrics will be merged

00:26:22,400 --> 00:26:26,780
and you'll be able to see all of the

00:26:23,900 --> 00:26:28,700
metrics in here so if you have like

00:26:26,780 --> 00:26:30,170
multiple clusters of graphite boxes you

00:26:28,700 --> 00:26:31,850
can include those clusters in the

00:26:30,170 --> 00:26:34,310
configuration and all of the data will

00:26:31,850 --> 00:26:40,340
be merged into the metrics that are

00:26:34,310 --> 00:26:42,560
displayed this bothers the crap out of

00:26:40,340 --> 00:26:44,890
me there's an autocomplete feature

00:26:42,560 --> 00:26:48,770
that's built in you can't disable it and

00:26:44,890 --> 00:26:50,840
it has a tendency to like if you hit

00:26:48,770 --> 00:26:52,460
enter right now what you think would

00:26:50,840 --> 00:26:53,660
happen doesn't happen and that's

00:26:52,460 --> 00:26:57,170
terrible

00:26:53,660 --> 00:27:00,950
hit escape and then enter that's minor

00:26:57,170 --> 00:27:04,700
quirk so let's take a look at why this

00:27:00,950 --> 00:27:08,030
is so awesome so for instance I just

00:27:04,700 --> 00:27:10,670
want to see the number of log entries

00:27:08,030 --> 00:27:13,040
that have been indexed the the total

00:27:10,670 --> 00:27:16,070
number and and all the different nodes

00:27:13,040 --> 00:27:18,260
so here I just put a star there where

00:27:16,070 --> 00:27:20,510
the node and identifier would be and I

00:27:18,260 --> 00:27:22,070
have all of the all of the data on one

00:27:20,510 --> 00:27:26,290
graph that's it that's all I had to

00:27:22,070 --> 00:27:28,700
query if I want to I can select which

00:27:26,290 --> 00:27:30,470
log processors I want to actually

00:27:28,700 --> 00:27:31,790
include so I can use like I said

00:27:30,470 --> 00:27:34,520
anything that you can use in the shell

00:27:31,790 --> 00:27:37,190
to generate a file name you can use in a

00:27:34,520 --> 00:27:38,720
metric to expand so here I'm using a

00:27:37,190 --> 00:27:43,820
character class to expand so now I only

00:27:38,720 --> 00:27:45,350
have two i I can use the curly braces so

00:27:43,820 --> 00:27:47,390
I can be very specific and say just

00:27:45,350 --> 00:27:51,920
these guys here use commas and curly

00:27:47,390 --> 00:27:53,750
braces so now I have just these two like

00:27:51,920 --> 00:27:55,280
you see the metric names you don't need

00:27:53,750 --> 00:27:56,810
to read them but they're pretty long

00:27:55,280 --> 00:27:59,240
down there and I like things to be

00:27:56,810 --> 00:28:00,770
pretty so you can do things like change

00:27:59,240 --> 00:28:02,690
the way that they're displayed in the

00:28:00,770 --> 00:28:04,910
graph to say alias these by node and

00:28:02,690 --> 00:28:07,730
then you give the zero based index node

00:28:04,910 --> 00:28:10,250
in the dotted notation and now your your

00:28:07,730 --> 00:28:11,730
labels are pretty pretty labels are

00:28:10,250 --> 00:28:13,810
better than ugly labels

00:28:11,730 --> 00:28:19,240
you can combine all your metrics

00:28:13,810 --> 00:28:20,980
together using some series maybe you

00:28:19,240 --> 00:28:24,850
have two wildcards and you'd like to

00:28:20,980 --> 00:28:28,600
combine them so here we're combining on

00:28:24,850 --> 00:28:31,510
the third element or fourth element

00:28:28,600 --> 00:28:33,610
because it's zero based so we end up

00:28:31,510 --> 00:28:38,050
totaling all of the node identifiers x'

00:28:33,610 --> 00:28:41,860
and the class so here we have all of the

00:28:38,050 --> 00:28:45,190
the ones that resulted in errors dropped

00:28:41,860 --> 00:28:48,610
ignore bad from all the hosts so we can

00:28:45,190 --> 00:28:53,860
combine data like that we can average

00:28:48,610 --> 00:28:55,420
the series same thing we can average

00:28:53,860 --> 00:28:58,150
series with wildcards so we can do the

00:28:55,420 --> 00:29:01,570
same thing where we're looking at these

00:28:58,150 --> 00:29:09,850
total and ignore bits on average across

00:29:01,570 --> 00:29:12,370
the entire cluster of things or I have

00:29:09,850 --> 00:29:14,410
the slide twice there we go and so you

00:29:12,370 --> 00:29:16,570
can do percentile of the data this may

00:29:14,410 --> 00:29:19,180
not do what you're expecting it to do so

00:29:16,570 --> 00:29:22,240
n percentile will generate the 95th

00:29:19,180 --> 00:29:27,610
percentile as a constant line for every

00:29:22,240 --> 00:29:29,230
data point in in your selection so here

00:29:27,610 --> 00:29:33,430
we have constant lines at the 95th

00:29:29,230 --> 00:29:35,530
percentile for the time range across now

00:29:33,430 --> 00:29:38,680
if I what I probably wanted to see is

00:29:35,530 --> 00:29:39,880
the 95th percentile at each interval and

00:29:38,680 --> 00:29:42,580
that's a different function called

00:29:39,880 --> 00:29:45,040
percentile of series so now we have the

00:29:42,580 --> 00:29:48,370
percent the 95th percentile at each time

00:29:45,040 --> 00:29:52,480
point that's being graphed which can be

00:29:48,370 --> 00:29:53,590
helpful we can select metrics so maybe I

00:29:52,480 --> 00:29:56,500
want to see the ones with the highest

00:29:53,590 --> 00:29:59,020
highest standard deviation so I can ask

00:29:56,500 --> 00:30:00,820
for the most deviant - this is an

00:29:59,020 --> 00:30:02,410
instance where I'm like not super

00:30:00,820 --> 00:30:03,850
thrilled about graphite because this is

00:30:02,410 --> 00:30:05,980
kind of like the needle haystack issue

00:30:03,850 --> 00:30:07,330
in PHP where you have to have the

00:30:05,980 --> 00:30:09,460
function reference available to know

00:30:07,330 --> 00:30:12,460
whether the two comes first or after the

00:30:09,460 --> 00:30:14,680
thing but that this function is really

00:30:12,460 --> 00:30:16,590
useful for finding really weird stuff in

00:30:14,680 --> 00:30:18,940
your data

00:30:16,590 --> 00:30:21,820
you can select the highest current

00:30:18,940 --> 00:30:24,400
available and here the two comes at the

00:30:21,820 --> 00:30:26,320
end the highest two current you can

00:30:24,400 --> 00:30:28,360
select a highest average so this is

00:30:26,320 --> 00:30:31,360
across the graph the the highest to

00:30:28,360 --> 00:30:33,340
average here and we can start

00:30:31,360 --> 00:30:34,990
transforming metrics so generally when I

00:30:33,340 --> 00:30:37,020
start to see patterns like this it

00:30:34,990 --> 00:30:39,580
usually means that I'm storing a counter

00:30:37,020 --> 00:30:42,460
and that counter is gonna increase and

00:30:39,580 --> 00:30:44,380
it's never going to reset until the

00:30:42,460 --> 00:30:47,800
service is restarted or it hits an

00:30:44,380 --> 00:30:50,530
overflow point right so this this is

00:30:47,800 --> 00:30:53,050
helpful to be able to see these types of

00:30:50,530 --> 00:30:54,610
things occur but usually what humans are

00:30:53,050 --> 00:30:56,380
looking for is actually the the

00:30:54,610 --> 00:30:58,960
derivative of this the change at the

00:30:56,380 --> 00:31:01,570
time so when I start to see these step

00:30:58,960 --> 00:31:06,580
patterns or these constant lines with a

00:31:01,570 --> 00:31:10,570
with a constant slope then I start to

00:31:06,580 --> 00:31:13,360
look at the non- derivative function in

00:31:10,570 --> 00:31:14,770
the the or weight this is my Y minimum

00:31:13,360 --> 00:31:17,500
so yeah

00:31:14,770 --> 00:31:20,740
if you if you look at the graph here at

00:31:17,500 --> 00:31:23,770
the top graph you'll see an example of

00:31:20,740 --> 00:31:27,180
what I was talking about the minimum

00:31:23,770 --> 00:31:29,920
there is not zero it's not even close

00:31:27,180 --> 00:31:31,390
and that's because it's just basically

00:31:29,920 --> 00:31:33,100
saying I want to fit this graph in this

00:31:31,390 --> 00:31:34,960
point so here's my X and my Y that I

00:31:33,100 --> 00:31:37,270
need to fit the graph and make it pretty

00:31:34,960 --> 00:31:38,680
so this is one of the most common

00:31:37,270 --> 00:31:43,060
transformations I do on the graph that

00:31:38,680 --> 00:31:46,360
someone pastes into an IRC channel is y

00:31:43,060 --> 00:31:48,070
minimum equals zero and then now that

00:31:46,360 --> 00:31:50,020
looks a little bit different it's the

00:31:48,070 --> 00:31:55,090
same graph but with Y minimum set to

00:31:50,020 --> 00:31:57,550
zero so using non negative derivative we

00:31:55,090 --> 00:32:00,310
can then actually graph the change and

00:31:57,550 --> 00:32:02,530
over time there's also integral so if

00:32:00,310 --> 00:32:04,810
you have a derivative you know a change

00:32:02,530 --> 00:32:06,160
over time value you can actually

00:32:04,810 --> 00:32:09,540
transform it back into the interval and

00:32:06,160 --> 00:32:09,540
see the slope of the line if you'd like

00:32:10,530 --> 00:32:15,850
you can actually select data points so

00:32:13,300 --> 00:32:17,740
here we're moving removing any data

00:32:15,850 --> 00:32:20,770
points that are above the 95th

00:32:17,740 --> 00:32:22,660
percentile and this actually removes

00:32:20,770 --> 00:32:24,760
them from the graph so they become null

00:32:22,660 --> 00:32:25,870
but then they don't cause the graph to

00:32:24,760 --> 00:32:31,090
see you

00:32:25,870 --> 00:32:33,700
but they are null we can scale them per

00:32:31,090 --> 00:32:37,780
second we can scale them per minute we

00:32:33,700 --> 00:32:40,300
can scale them however we'd like to I

00:32:37,780 --> 00:32:42,970
did find a bug with this that I'm still

00:32:40,300 --> 00:32:44,830
writing the bug report for which is

00:32:42,970 --> 00:32:49,680
scale two seconds over a retention

00:32:44,830 --> 00:32:52,210
boundary does not calculate properly but

00:32:49,680 --> 00:32:56,550
it's still it's still useful to have

00:32:52,210 --> 00:32:56,550
especially for for current data

00:33:00,460 --> 00:33:05,529
so here this is one thing that I did a

00:33:04,240 --> 00:33:08,679
lot was trying to figure out why

00:33:05,529 --> 00:33:10,690
elasticsearch was broken and most of the

00:33:08,679 --> 00:33:13,840
time it had to do with garbage

00:33:10,690 --> 00:33:17,529
collection so first I started out with

00:33:13,840 --> 00:33:20,200
the garbage collection in milliseconds

00:33:17,529 --> 00:33:22,299
and there's this these spikes of lines

00:33:20,200 --> 00:33:25,840
now really the problem really only

00:33:22,299 --> 00:33:29,289
creeps up at certain points it's usually

00:33:25,840 --> 00:33:30,999
when it's above 30 milliseconds so if I

00:33:29,289 --> 00:33:35,769
wanted to what I could do is I could

00:33:30,999 --> 00:33:38,889
push that line down to minus 30 seconds

00:33:35,769 --> 00:33:43,409
right so I basically offset at minus 30

00:33:38,889 --> 00:33:47,379
and what happens in your mind is that

00:33:43,409 --> 00:33:50,169
you see one but then I draw this

00:33:47,379 --> 00:33:52,539
infinite so anything that's above zero

00:33:50,169 --> 00:33:54,100
gets drawn as an infinite value and I

00:33:52,539 --> 00:33:55,720
see the second one which was there in

00:33:54,100 --> 00:33:58,480
the first one but our brain doesn't

00:33:55,720 --> 00:34:00,190
really catch it so if you want to look

00:33:58,480 --> 00:34:01,929
at event you can transform your time

00:34:00,190 --> 00:34:04,960
series data essentially into event data

00:34:01,929 --> 00:34:11,559
based on thresholds using features like

00:34:04,960 --> 00:34:14,379
this we can compare metrics here we're

00:34:11,559 --> 00:34:18,069
comparing the amount of stuff that's

00:34:14,379 --> 00:34:21,790
been logged based on now against a time

00:34:18,069 --> 00:34:25,149
shifted value seven days back so you can

00:34:21,790 --> 00:34:28,750
see the green line is going to be last

00:34:25,149 --> 00:34:31,089
week and the blue line is today so

00:34:28,750 --> 00:34:34,240
that's kind of neat you can kind of see

00:34:31,089 --> 00:34:37,210
what's going on in there you can take

00:34:34,240 --> 00:34:39,639
the difference here I draw a constant

00:34:37,210 --> 00:34:43,000
red line at zero and then the difference

00:34:39,639 --> 00:34:44,609
between from now to last week so I can

00:34:43,000 --> 00:34:49,299
see when I'm above and when I'm below

00:34:44,609 --> 00:34:52,389
last week's values which is kind of neat

00:34:49,299 --> 00:34:54,819
and we can do forecasting this is hope

00:34:52,389 --> 00:34:57,609
winters forecasting is built right into

00:34:54,819 --> 00:35:02,020
graphite so we can draw on confidence

00:34:57,609 --> 00:35:04,329
bands directly on the graph and places

00:35:02,020 --> 00:35:08,170
where the red line would break outside

00:35:04,329 --> 00:35:11,020
of those grey lines is actually would be

00:35:08,170 --> 00:35:12,720
considered an anomalous event something

00:35:11,020 --> 00:35:17,430
abnormal has happened

00:35:12,720 --> 00:35:19,410
here and you can read the the function

00:35:17,430 --> 00:35:21,270
reference there's you can configure the

00:35:19,410 --> 00:35:23,280
the whole winners parameters as well in

00:35:21,270 --> 00:35:25,500
the graph I usually tend to use the

00:35:23,280 --> 00:35:33,830
defaults when I'm using them but you can

00:35:25,500 --> 00:35:37,080
change some of the parameters there so

00:35:33,830 --> 00:35:39,030
what I found was a correlation not

00:35:37,080 --> 00:35:40,710
causation between the number of

00:35:39,030 --> 00:35:43,380
documents in the elasticsearch cluster

00:35:40,710 --> 00:35:47,130
and the amount of time that was spent in

00:35:43,380 --> 00:35:50,010
garbage collection so this this graph

00:35:47,130 --> 00:35:54,180
kind of helped me understand like where

00:35:50,010 --> 00:35:56,730
the threshold was for the number of

00:35:54,180 --> 00:35:58,619
documents on this graph doesn't show it

00:35:56,730 --> 00:36:01,530
I should have I forgot to capture that

00:35:58,619 --> 00:36:05,040
but there was a point at which when when

00:36:01,530 --> 00:36:08,730
the Green Line went up to high the heap

00:36:05,040 --> 00:36:10,099
memory used was very very high it has to

00:36:08,730 --> 00:36:16,590
do with how elasticsearch stores

00:36:10,099 --> 00:36:19,530
references to documents in memory so

00:36:16,590 --> 00:36:21,570
here you know now we can find what I

00:36:19,530 --> 00:36:23,700
what I'm doing here is any time the

00:36:21,570 --> 00:36:26,220
garbage collection is greater than 250

00:36:23,700 --> 00:36:28,790
milliseconds in the cluster that's if

00:36:26,220 --> 00:36:31,619
two of them happen near the same time

00:36:28,790 --> 00:36:33,330
that can cause issues so here we have

00:36:31,619 --> 00:36:35,520
each one of these lines represents a

00:36:33,330 --> 00:36:37,859
each color represents a particular node

00:36:35,520 --> 00:36:40,890
in the cluster and each time that that

00:36:37,859 --> 00:36:42,690
cluster node hit 250 milliseconds are

00:36:40,890 --> 00:36:44,250
higher of garbage collection we have a

00:36:42,690 --> 00:36:47,609
vertical line again

00:36:44,250 --> 00:36:50,040
this isn't terribly useful as a graph

00:36:47,609 --> 00:36:51,690
per se but if we overlay other

00:36:50,040 --> 00:36:54,300
statistics on top of this we now have

00:36:51,690 --> 00:36:57,420
event data on the graph and this is not

00:36:54,300 --> 00:37:01,220
an event system so I can look at these

00:36:57,420 --> 00:37:04,640
events using code rather than graphs and

00:37:01,220 --> 00:37:07,619
find out what else is going on there

00:37:04,640 --> 00:37:11,130
all right so do I even dashboard like

00:37:07,619 --> 00:37:12,690
these are graphs but that's great so I

00:37:11,130 --> 00:37:15,690
highly recommend graph on Oh

00:37:12,690 --> 00:37:18,210
graph on it used to be so the origin of

00:37:15,690 --> 00:37:20,790
the name comes from kibana is anyone

00:37:18,210 --> 00:37:23,700
heard of Caban on it use kibana yeah it

00:37:20,790 --> 00:37:25,500
started out as kind of a toy it was kind

00:37:23,700 --> 00:37:28,110
of cool it made pretty graphs

00:37:25,500 --> 00:37:30,540
now it is evolving into a full

00:37:28,110 --> 00:37:32,310
functioning monitoring platform graph

00:37:30,540 --> 00:37:34,590
online now supports multiple database

00:37:32,310 --> 00:37:36,870
multiple time series database back ends

00:37:34,590 --> 00:37:39,570
on the same graph so if you want to you

00:37:36,870 --> 00:37:43,560
can graph elasticsearch graphite pump

00:37:39,570 --> 00:37:45,630
and DSC be Prometheus influx and if your

00:37:43,560 --> 00:37:48,360
application supports a simple four call

00:37:45,630 --> 00:37:51,930
API your own applications metrics

00:37:48,360 --> 00:37:53,520
directly on the same graph there's also

00:37:51,930 --> 00:37:56,630
some really cool features I'm gonna try

00:37:53,520 --> 00:38:02,850
to break out of the fourth wall here

00:37:56,630 --> 00:38:05,850
where's that yeah over here so you have

00:38:02,850 --> 00:38:07,410
full control over the graph styling you

00:38:05,850 --> 00:38:09,090
can create dashboards with any number of

00:38:07,410 --> 00:38:11,370
graphs on them

00:38:09,090 --> 00:38:13,290
it supports graphite elasticsearch are

00:38:11,370 --> 00:38:14,730
the two big ones but it supports a lot

00:38:13,290 --> 00:38:16,020
of other data sources and there's a

00:38:14,730 --> 00:38:17,730
plug-in infrastructure which supports

00:38:16,020 --> 00:38:20,780
even more here's where we start getting

00:38:17,730 --> 00:38:22,980
into some craziness template variables

00:38:20,780 --> 00:38:25,350
graph fauna allows you to make an

00:38:22,980 --> 00:38:28,260
elastic search query or a graphite

00:38:25,350 --> 00:38:30,330
metrics search and then from the results

00:38:28,260 --> 00:38:33,060
of that search generate a variable that

00:38:30,330 --> 00:38:39,680
you can then use in your graphs and

00:38:33,060 --> 00:38:43,670
dashboards so in in this example here

00:38:39,680 --> 00:38:47,310
you can see that they've created a

00:38:43,670 --> 00:38:51,000
variable and selected as multis they can

00:38:47,310 --> 00:38:52,800
tick these boxes and add or remove from

00:38:51,000 --> 00:38:55,860
that graph in real time

00:38:52,800 --> 00:38:56,880
which is pretty pretty amazing I was

00:38:55,860 --> 00:39:00,690
actually working on project to do

00:38:56,880 --> 00:39:04,020
exactly this because it's so useful you

00:39:00,690 --> 00:39:06,480
can then say on a graph or a panel for

00:39:04,020 --> 00:39:09,570
every one of these things that the user

00:39:06,480 --> 00:39:10,920
selects repeat this row or panel so you

00:39:09,570 --> 00:39:13,320
can have multiple hosts or multiple

00:39:10,920 --> 00:39:15,180
builds multiple locations on the same

00:39:13,320 --> 00:39:16,530
dashboard if the user selects three

00:39:15,180 --> 00:39:18,510
locations they can see all three at once

00:39:16,530 --> 00:39:22,050
they only select one they just see one

00:39:18,510 --> 00:39:25,260
it's very powerful and simply it's

00:39:22,050 --> 00:39:28,860
amazing the the dashboard sharing

00:39:25,260 --> 00:39:31,950
feature is what why hasn't this existed

00:39:28,860 --> 00:39:34,080
on there's a remote sharing feature

00:39:31,950 --> 00:39:36,660
using the the guys that developed this

00:39:34,080 --> 00:39:37,819
as rain tank dot IO I don't recommend

00:39:36,660 --> 00:39:39,109
using that because again you

00:39:37,819 --> 00:39:41,150
storing your metrics in someone else's

00:39:39,109 --> 00:39:42,709
cloud but there is a local snapshot

00:39:41,150 --> 00:39:45,199
feature available which stores in your

00:39:42,709 --> 00:39:46,609
database back-end and what this does is

00:39:45,199 --> 00:39:49,479
whatever graph you're looking at when

00:39:46,609 --> 00:39:51,699
you click Share and snapshot dashboard

00:39:49,479 --> 00:39:54,650
everything that you see on the dashboard

00:39:51,699 --> 00:39:56,739
all the metrics all the values all the

00:39:54,650 --> 00:39:58,640
graphs all their parameters are

00:39:56,739 --> 00:40:01,400
snapshotted and stored in the database

00:39:58,640 --> 00:40:03,170
and then you can go back and look at

00:40:01,400 --> 00:40:06,079
these snapshots so if you have an outage

00:40:03,170 --> 00:40:08,329
event you can snapshot the dashboard and

00:40:06,079 --> 00:40:11,059
document it and it will exist for

00:40:08,329 --> 00:40:12,349
Prosperity you can set the expire period

00:40:11,059 --> 00:40:14,119
automatically or you can set it the

00:40:12,349 --> 00:40:15,739
default is never and then you can always

00:40:14,119 --> 00:40:17,989
go back to that dashboard and look and

00:40:15,739 --> 00:40:20,029
see what's happening for for like post

00:40:17,989 --> 00:40:21,469
mortems it is amazing

00:40:20,029 --> 00:40:23,359
and it's also really helpful because

00:40:21,469 --> 00:40:25,489
then you know regardless of who's

00:40:23,359 --> 00:40:27,229
looking at your dashboard when they pull

00:40:25,489 --> 00:40:29,329
it up they see exactly what you see so

00:40:27,229 --> 00:40:31,130
there's no like weird you know rendering

00:40:29,329 --> 00:40:33,440
issues or any of that stuff

00:40:31,130 --> 00:40:35,390
or maybe the data's been rolled up and

00:40:33,440 --> 00:40:37,910
now the data doesn't look right anymore

00:40:35,390 --> 00:40:42,160
this stores the data as it is all the

00:40:37,910 --> 00:40:44,390
data in the local snapshot it is amazing

00:40:42,160 --> 00:40:46,369
it allows you to authenticate you can

00:40:44,390 --> 00:40:48,140
have multiple organizations so each

00:40:46,369 --> 00:40:49,640
organization can have their own think of

00:40:48,140 --> 00:40:51,199
it Morris departments have their own

00:40:49,640 --> 00:40:53,749
dashboards create their own dashboards

00:40:51,199 --> 00:40:56,180
share their own dashboards and then

00:40:53,749 --> 00:40:58,309
there's annotations and this is this was

00:40:56,180 --> 00:41:00,469
really cool what I was able to what I

00:40:58,309 --> 00:41:03,829
did with this just as an example to

00:41:00,469 --> 00:41:05,569
demonstrate its power was I I used on we

00:41:03,829 --> 00:41:08,299
had error error logging exception

00:41:05,569 --> 00:41:11,299
logging going into elasticsearch I was

00:41:08,299 --> 00:41:14,269
able to create those as annotations on

00:41:11,299 --> 00:41:16,190
the graph and map that against page

00:41:14,269 --> 00:41:18,469
views so you can see where an error

00:41:16,190 --> 00:41:20,930
occurred and where errors occurred and

00:41:18,469 --> 00:41:23,539
where what effect that had on the graph

00:41:20,930 --> 00:41:25,910
which is really cool I don't know if

00:41:23,539 --> 00:41:27,229
they have really good visuals here

00:41:25,910 --> 00:41:29,150
because they're using the dark theme but

00:41:27,229 --> 00:41:32,059
you can see these red lines here with

00:41:29,150 --> 00:41:33,619
these tick boxes down here and the nice

00:41:32,059 --> 00:41:37,849
thing about this is if your document is

00:41:33,619 --> 00:41:40,609
in elasticsearch you can tell graph ahna

00:41:37,849 --> 00:41:43,759
use this field to display this

00:41:40,609 --> 00:41:45,199
annotation this is the title field and

00:41:43,759 --> 00:41:45,600
this is the description field of this

00:41:45,199 --> 00:41:49,020
and

00:41:45,600 --> 00:41:50,940
so you can have your when you click on

00:41:49,020 --> 00:41:53,790
this you can actually see the event data

00:41:50,940 --> 00:41:55,950
from elasticsearch that's associated

00:41:53,790 --> 00:41:59,730
with that event it's it's amazing like

00:41:55,950 --> 00:42:02,010
really it's it's transformative um also

00:41:59,730 --> 00:42:06,690
supports theming so you can have a light

00:42:02,010 --> 00:42:09,630
theme if if you'd like to its pluggable

00:42:06,690 --> 00:42:10,830
like I said you can write your own you

00:42:09,630 --> 00:42:13,410
can write your own panels your own

00:42:10,830 --> 00:42:16,710
graphs you can write your own data

00:42:13,410 --> 00:42:18,510
sources so the world is your oyster you

00:42:16,710 --> 00:42:21,180
can start incorporating your data into

00:42:18,510 --> 00:42:22,650
it the the time range selection is

00:42:21,180 --> 00:42:25,710
awesome and there's an there's a number

00:42:22,650 --> 00:42:27,630
of like apps and stuff at graph on

00:42:25,710 --> 00:42:31,980
annette is an overview of all the

00:42:27,630 --> 00:42:33,750
plugins that are supported so you can

00:42:31,980 --> 00:42:36,810
add a clock and if you'd like to a world

00:42:33,750 --> 00:42:38,700
clock it integrates with zabbix with

00:42:36,810 --> 00:42:40,680
boats and there's a pie chart if you

00:42:38,700 --> 00:42:45,180
hate people and you want to make them

00:42:40,680 --> 00:42:47,460
hate you it integrates with open and the

00:42:45,180 --> 00:42:49,920
mess I just saw this stage monitor got

00:42:47,460 --> 00:42:51,500
added recently which if you do a lot of

00:42:49,920 --> 00:42:53,790
job if you have support a lot of Java

00:42:51,500 --> 00:42:55,290
frameworks and they use stage monitors

00:42:53,790 --> 00:42:58,650
one of the it's one of the gmx

00:42:55,290 --> 00:43:00,150
monitoring plugins for for java you can

00:42:58,650 --> 00:43:02,550
now natively pull data from a stage

00:43:00,150 --> 00:43:04,320
monitor box the percona

00:43:02,550 --> 00:43:07,260
Pro kinda actually supports like a

00:43:04,320 --> 00:43:09,480
performance gathering system I think the

00:43:07,260 --> 00:43:10,860
backend is prometheus but they have an

00:43:09,480 --> 00:43:12,870
app that they've built which is an

00:43:10,860 --> 00:43:14,190
overview oh that's the nice thing it's

00:43:12,870 --> 00:43:16,320
like you can take a whole series of

00:43:14,190 --> 00:43:20,610
dashboards and create them as an app and

00:43:16,320 --> 00:43:22,230
then upload that or distribute that so

00:43:20,610 --> 00:43:24,330
that you can actually have like a

00:43:22,230 --> 00:43:26,880
representative sample of what monitoring

00:43:24,330 --> 00:43:30,060
your application looks like and install

00:43:26,880 --> 00:43:32,810
it reinstall it and then again you know

00:43:30,060 --> 00:43:35,680
here's the elasticsearch plugin oops I

00:43:32,810 --> 00:43:39,940
hit

00:43:35,680 --> 00:43:43,780
stuff um and really the kind of cool one

00:43:39,940 --> 00:43:47,880
where to go down here simple Jason this

00:43:43,780 --> 00:43:47,880
is awesome so basically what you do is

00:43:48,000 --> 00:43:54,520
now you're not gonna support that oh

00:43:50,590 --> 00:43:55,960
here we go there's a few API calls that

00:43:54,520 --> 00:43:57,580
you have to support with the simple JSON

00:43:55,960 --> 00:43:59,080
plug-in but that will instantly add you

00:43:57,580 --> 00:44:01,030
the ability to do templating and

00:43:59,080 --> 00:44:03,430
annotations and also pool time-series

00:44:01,030 --> 00:44:05,800
data so if you have you know your custom

00:44:03,430 --> 00:44:08,020
app or your Apache stats page that you'd

00:44:05,800 --> 00:44:15,070
like to to integrate with it you can do

00:44:08,020 --> 00:44:22,780
that instantly with with Griffin ax over

00:44:15,070 --> 00:44:24,490
here there's also a graph Explorer it's

00:44:22,780 --> 00:44:26,290
not it doesn't have close menu oh the

00:44:24,490 --> 00:44:28,210
other thing is on the future pipeline

00:44:26,290 --> 00:44:30,400
for graph Anna is alerting so you can

00:44:28,210 --> 00:44:32,140
drag and say this time period is what I

00:44:30,400 --> 00:44:32,650
want to learn on if you see this happen

00:44:32,140 --> 00:44:34,120
again

00:44:32,650 --> 00:44:36,010
send me an alert which is pretty

00:44:34,120 --> 00:44:37,720
powerful and also support static that

00:44:36,010 --> 00:44:40,180
are holding that's going to probably be

00:44:37,720 --> 00:44:43,150
in the 3-1 release so it'll be like a

00:44:40,180 --> 00:44:45,700
one stop monitoring thing others graph

00:44:43,150 --> 00:44:47,740
Explorer which is similar it's it

00:44:45,700 --> 00:44:50,590
supports the metrics 2.0 like key value

00:44:47,740 --> 00:44:53,050
stuff there's cubism which is kind of

00:44:50,590 --> 00:44:54,970
cool I'm what I'm gonna be working on

00:44:53,050 --> 00:44:56,590
soon as a cubism plugin for graph on o

00:44:54,970 --> 00:44:59,710
so that I can get cubism graphs and

00:44:56,590 --> 00:45:03,720
graph fauna because I need that and

00:44:59,710 --> 00:45:03,720
that's pretty much thank you

00:45:07,940 --> 00:45:14,670
we we have we have time for questions I

00:45:11,820 --> 00:45:19,430
can I'm gonna steal Sawyer's idea like I

00:45:14,670 --> 00:45:21,770
can do questions bonus material or both

00:45:19,430 --> 00:45:25,800
does anyone have a preference he wants

00:45:21,770 --> 00:45:29,130
bonus material yes bonus material bonus

00:45:25,800 --> 00:45:32,630
material are just questions I'm not as

00:45:29,130 --> 00:45:34,650
good at this as Sawyer is how about both

00:45:32,630 --> 00:45:37,350
all right we'll try both

00:45:34,650 --> 00:45:40,310
so bonus material real quick how do I

00:45:37,350 --> 00:45:43,470
know how I'm doing in in time series

00:45:40,310 --> 00:45:48,120
always compare the data to the same day

00:45:43,470 --> 00:45:49,740
of the week here is a graph of this is

00:45:48,120 --> 00:45:52,380
actually very easy to do in graph on ax

00:45:49,740 --> 00:45:54,630
this is the past week this is today all

00:45:52,380 --> 00:45:57,480
the way back to seven days ago and

00:45:54,630 --> 00:45:59,940
there's one line that fits really good

00:45:57,480 --> 00:46:01,260
and the rest of them they come close

00:45:59,940 --> 00:46:03,890
some of them come close but they're not

00:46:01,260 --> 00:46:05,940
exact always compared to the same day

00:46:03,890 --> 00:46:10,980
anyone have any guesses as to what

00:46:05,940 --> 00:46:13,370
breaks this rule yep national holidays

00:46:10,980 --> 00:46:15,920
generally tend to look like Sundays so

00:46:13,370 --> 00:46:20,540
that's kind of the the gist of this

00:46:15,920 --> 00:46:20,540
again thank you questions

00:46:35,020 --> 00:46:40,670
true-true um so the comment was that the

00:46:38,840 --> 00:46:42,470
uploading terrain tank is really useful

00:46:40,670 --> 00:46:45,620
when you're working with with external

00:46:42,470 --> 00:46:47,150
vendors and you want to share this is

00:46:45,620 --> 00:46:48,830
where your stuff broke here's the graph

00:46:47,150 --> 00:46:50,390
that I'm looking at because then you can

00:46:48,830 --> 00:46:53,170
share that link it's public and it's

00:46:50,390 --> 00:46:56,170
it's automatic yes that's that's correct

00:46:53,170 --> 00:46:56,170
yes

00:47:03,770 --> 00:47:07,640
my my app

00:47:12,320 --> 00:47:14,950
yeah

00:47:17,109 --> 00:47:24,260
okay yeah okay um so I the question was

00:47:21,829 --> 00:47:25,400
do I feed any other data other than just

00:47:24,260 --> 00:47:27,560
pulling data from things like

00:47:25,400 --> 00:47:28,670
elasticsearch and system metrics into

00:47:27,560 --> 00:47:32,210
graphite and the answer is absolutely

00:47:28,670 --> 00:47:34,760
any time that I can any the you may be

00:47:32,210 --> 00:47:36,470
familiar with Etsy they did a lot of

00:47:34,760 --> 00:47:39,079
this this work they have a program

00:47:36,470 --> 00:47:39,770
called stats D there's a number of like

00:47:39,079 --> 00:47:42,230
C based

00:47:39,770 --> 00:47:44,900
stats d Damon so you don't have to worry

00:47:42,230 --> 00:47:46,520
about node that supports that C I highly

00:47:44,900 --> 00:47:49,280
recommend checking out stats D stats

00:47:46,520 --> 00:47:50,839
site is what I'm using for my stats D

00:47:49,280 --> 00:47:52,970
implementation it's all written in C and

00:47:50,839 --> 00:47:57,619
packages nicely on Linux free and

00:47:52,970 --> 00:47:59,660
freebsd on a tease philosophy which I

00:47:57,619 --> 00:48:01,520
highly agree with is if it moves graph

00:47:59,660 --> 00:48:04,510
it if it doesn't move graph it anyways

00:48:01,520 --> 00:48:07,550
because if it moves then you're screwed

00:48:04,510 --> 00:48:10,099
so if you're doing anything if you're

00:48:07,550 --> 00:48:12,050
writing a subroutine and it's public

00:48:10,099 --> 00:48:14,270
facing your users are gonna hit it

00:48:12,050 --> 00:48:16,160
why aren't you timing it with stats T

00:48:14,270 --> 00:48:17,900
you can sample so you can say I only

00:48:16,160 --> 00:48:20,839
want to sample this once every every

00:48:17,900 --> 00:48:23,569
hundred times but please just tell me

00:48:20,839 --> 00:48:25,970
how long it takes it's insanely useful

00:48:23,569 --> 00:48:28,310
when you hit debugging and you want to

00:48:25,970 --> 00:48:30,890
know where's my app dying well yeah sure

00:48:28,310 --> 00:48:34,130
you can do flame graphs and MIT Prof but

00:48:30,890 --> 00:48:37,369
if you have a corpus of data of real

00:48:34,130 --> 00:48:39,380
life and you can see how that evolved

00:48:37,369 --> 00:48:41,869
over time you can pinpoint something

00:48:39,380 --> 00:48:43,550
changed here what changed here and it

00:48:41,869 --> 00:48:46,849
takes troubleshooting to a whole new

00:48:43,550 --> 00:48:49,010
level stats D yeah and stat site is a

00:48:46,849 --> 00:48:51,319
really great implementation of stats D

00:48:49,010 --> 00:48:52,460
and C and like I said then you don't

00:48:51,319 --> 00:48:54,710
have to worry about all the nodejs

00:48:52,460 --> 00:48:58,280
packaging stuff which is not usually

00:48:54,710 --> 00:49:01,900
easy to do when things like CentOS any

00:48:58,280 --> 00:49:01,900
other questions is over here

00:49:03,690 --> 00:49:08,790
when you recording so that when you

00:49:06,280 --> 00:49:08,790
recording

00:49:10,790 --> 00:49:14,190
okay so the question is this is a

00:49:12,750 --> 00:49:16,110
challenge to find the balance when

00:49:14,190 --> 00:49:19,010
you're using aggregates to get the right

00:49:16,110 --> 00:49:21,420
data out of the system and I would say

00:49:19,010 --> 00:49:23,580
there's a lot of there's a lot of like

00:49:21,420 --> 00:49:25,830
blog posts on this about where people

00:49:23,580 --> 00:49:27,420
ran into issues and I know we all think

00:49:25,830 --> 00:49:29,220
that we're snowflakes but we all do the

00:49:27,420 --> 00:49:30,840
same stuff and we all have the same

00:49:29,220 --> 00:49:33,330
problems and someone has already run

00:49:30,840 --> 00:49:35,070
into that issue before so generally like

00:49:33,330 --> 00:49:37,020
Google what you're what you're

00:49:35,070 --> 00:49:40,590
collecting and you'll usually find like

00:49:37,020 --> 00:49:42,510
an aggregate philosophy and like for

00:49:40,590 --> 00:49:45,150
instance like with stats D the names of

00:49:42,510 --> 00:49:47,070
the metrics are consistent so when it

00:49:45,150 --> 00:49:48,840
writes a sum it has underscore some in

00:49:47,070 --> 00:49:50,580
the name if it writes a min you know

00:49:48,840 --> 00:49:52,650
these things are consistent and so you

00:49:50,580 --> 00:49:54,420
can actually just go and I did because I

00:49:52,650 --> 00:49:55,950
was like I can't think of every single

00:49:54,420 --> 00:49:57,810
permutation off the top of my head

00:49:55,950 --> 00:49:59,790
someone has already got it just up on

00:49:57,810 --> 00:50:02,760
github that says you know here's how you

00:49:59,790 --> 00:50:05,340
would do stats d aggregators and usually

00:50:02,760 --> 00:50:07,740
the defaults are okay so you have to be

00:50:05,340 --> 00:50:10,640
cognizant to the roll-ups you know your

00:50:07,740 --> 00:50:15,540
this is giving you this is like a

00:50:10,640 --> 00:50:18,390
intuition it's not going to you don't

00:50:15,540 --> 00:50:21,960
use this to build people with right so

00:50:18,390 --> 00:50:24,210
keep that in mind and that that is all

00:50:21,960 --> 00:50:27,690
the time we have come talk to me I can

00:50:24,210 --> 00:50:30,470
talk about this for days on end if you

00:50:27,690 --> 00:50:30,470

YouTube URL: https://www.youtube.com/watch?v=GgBh8XzI6no


