Title: 2016 - big data on little linux - Daniel Sterling
Publication date: 2016-06-27
Playlist: YAPC::NA 2016
Description: 
	
Captions: 
	00:00:00,170 --> 00:00:05,490
welcome to big data on little Linux I'm

00:00:03,929 --> 00:00:07,589
going to go a little bit fast because I

00:00:05,490 --> 00:00:11,460
have a number of slides I want to get

00:00:07,589 --> 00:00:14,160
through so I'm a HPC high performance

00:00:11,460 --> 00:00:15,870
computing admin at a genomics lab we do

00:00:14,160 --> 00:00:18,230
operations we don't shy away from it we

00:00:15,870 --> 00:00:20,550
write applications that run on servers

00:00:18,230 --> 00:00:22,650
cloud is somebody else's a server we

00:00:20,550 --> 00:00:25,320
don't we don't run on other people's

00:00:22,650 --> 00:00:28,590
servers that's not us and we are working

00:00:25,320 --> 00:00:30,720
to fight cancer so you know this is

00:00:28,590 --> 00:00:33,570
really cool this is not something that

00:00:30,720 --> 00:00:35,940
we did this is from open source of The

00:00:33,570 --> 00:00:38,010
Economist website but you can see this

00:00:35,940 --> 00:00:39,629
is the kind of work that we do we figure

00:00:38,010 --> 00:00:42,030
out treatments that will help people

00:00:39,629 --> 00:00:43,770
actually keep living this is pretty cool

00:00:42,030 --> 00:00:45,090
stuff that's not why you're here you're

00:00:43,770 --> 00:00:47,670
here cuz I was gonna talk about Big Data

00:00:45,090 --> 00:00:50,460
and I've got some more stories and I'm

00:00:47,670 --> 00:00:51,960
gonna get to them so you know you can

00:00:50,460 --> 00:00:54,030
buy servers with a lot of RAM nowadays

00:00:51,960 --> 00:00:56,460
and you know if you've got a lot of RAM

00:00:54,030 --> 00:00:58,440
just process your data in RAM you don't

00:00:56,460 --> 00:01:00,899
need any special big data tools to do

00:00:58,440 --> 00:01:02,460
that right well you know you still have

00:01:00,899 --> 00:01:03,930
to get the data in and out of RAM you

00:01:02,460 --> 00:01:05,880
still got to process it you still gotta

00:01:03,930 --> 00:01:08,100
store it so what's your storage back-end

00:01:05,880 --> 00:01:10,920
how do you actually get the data onto

00:01:08,100 --> 00:01:12,900
your processing nodes to use you know

00:01:10,920 --> 00:01:15,390
Windows File Sharing to use NFS

00:01:12,900 --> 00:01:16,680
I scuzzy once you've got the processing

00:01:15,390 --> 00:01:18,600
how do you scale your processing to use

00:01:16,680 --> 00:01:21,270
threads to use distributed memory shared

00:01:18,600 --> 00:01:23,729
memory you know what kind of HPC tools

00:01:21,270 --> 00:01:26,220
do you use in our case we keep it pretty

00:01:23,729 --> 00:01:28,470
simple our data usually does fit in a

00:01:26,220 --> 00:01:31,950
memory the instruments will run they'll

00:01:28,470 --> 00:01:33,930
generate data per run you know one to

00:01:31,950 --> 00:01:35,070
four days several terabytes of data we

00:01:33,930 --> 00:01:36,990
have a lot of instruments so we can

00:01:35,070 --> 00:01:40,619
generate a lot of data several terabytes

00:01:36,990 --> 00:01:42,810
a week adds it pretty quick per sample

00:01:40,619 --> 00:01:44,820
our analysis scripts are pretty simple

00:01:42,810 --> 00:01:47,189
we're not doing a whole lot of complex

00:01:44,820 --> 00:01:48,360
analysis on the compute side for Big

00:01:47,189 --> 00:01:50,520
Data we're not using Hadoop or anything

00:01:48,360 --> 00:01:53,579
crazy we're just running regular tools

00:01:50,520 --> 00:01:54,899
on large data sets so we want to scale

00:01:53,579 --> 00:01:57,719
our computers we want to buy computers

00:01:54,899 --> 00:02:00,119
lots of cores lots of memory let's just

00:01:57,719 --> 00:02:03,090
run samples faster more samples per

00:02:00,119 --> 00:02:04,649
system so to scale out we just buy more

00:02:03,090 --> 00:02:06,570
servers pretty simple from the

00:02:04,649 --> 00:02:09,149
processing side I'm going to talk to you

00:02:06,570 --> 00:02:11,160
about the storage side more than that so

00:02:09,149 --> 00:02:13,200
for the storage side again fairly simple

00:02:11,160 --> 00:02:15,090
you can buy expensive enterprise

00:02:13,200 --> 00:02:17,489
systems and if you have the money go

00:02:15,090 --> 00:02:20,489
ahead and do that by your big ice funds

00:02:17,489 --> 00:02:22,019
by your big net apps you know spend that

00:02:20,489 --> 00:02:24,930
spend that money spend you know give

00:02:22,019 --> 00:02:26,640
give EMC a million dollars or two if you

00:02:24,930 --> 00:02:27,750
want to save some money in health care

00:02:26,640 --> 00:02:30,750
we will actually want to save some money

00:02:27,750 --> 00:02:34,650
can you imagine just run Linux so how

00:02:30,750 --> 00:02:35,700
hard can it be all right so what's the

00:02:34,650 --> 00:02:39,330
worst thing any of you have ever seen

00:02:35,700 --> 00:02:40,830
your computer do catch fire that's a

00:02:39,330 --> 00:02:43,440
pretty bad one I've seen some bad things

00:02:40,830 --> 00:02:44,940
people all right I lied to you this is

00:02:43,440 --> 00:02:47,459
talk is actually everything is broken

00:02:44,940 --> 00:02:50,700
and needs replaced upgraded or patched I

00:02:47,459 --> 00:02:53,069
found bugs so you don't have to so the

00:02:50,700 --> 00:02:54,890
hardware is broken pretty simple you

00:02:53,069 --> 00:02:57,870
know you can swap out new hardware

00:02:54,890 --> 00:02:59,010
troubleshooting hardware is a little bit

00:02:57,870 --> 00:03:00,390
easier than troubleshooting software

00:02:59,010 --> 00:03:02,760
because it's physical you go and replace

00:03:00,390 --> 00:03:05,549
parts you can see the parts you can look

00:03:02,760 --> 00:03:07,799
at them the biggest thing that fails is

00:03:05,549 --> 00:03:09,390
memory and memory does fail you will

00:03:07,799 --> 00:03:11,640
corrupt data you will corrupt file

00:03:09,390 --> 00:03:12,239
systems I've seen it it's not it's not

00:03:11,640 --> 00:03:14,340
theoretical

00:03:12,239 --> 00:03:18,060
so definitely burn in your systems and

00:03:14,340 --> 00:03:19,290
buy supported systems you don't just buy

00:03:18,060 --> 00:03:21,060
the first thing that comes off of the

00:03:19,290 --> 00:03:22,200
boat from Asia if you do that you'll

00:03:21,060 --> 00:03:23,700
have a lot of systems that are really

00:03:22,200 --> 00:03:25,709
cheap and you'll have a lot of failures

00:03:23,700 --> 00:03:27,780
so spend the extra money it's not that

00:03:25,709 --> 00:03:30,209
much more get some brand-name components

00:03:27,780 --> 00:03:33,859
get some vendor support or enjoy dealing

00:03:30,209 --> 00:03:39,329
with tons of bad systems talk more about

00:03:33,859 --> 00:03:42,450
redundancy later okay so you've got your

00:03:39,329 --> 00:03:43,920
HPC system set up your running NFS you

00:03:42,450 --> 00:03:46,650
your users are happy they're using their

00:03:43,920 --> 00:03:48,750
data and all of a sudden you come in

00:03:46,650 --> 00:03:51,000
Monday morning all the servers are off

00:03:48,750 --> 00:03:52,920
hire all the servers off because they

00:03:51,000 --> 00:03:54,799
all hit emergency shutdown at 105 see

00:03:52,920 --> 00:03:57,090
when you use the service they get hot

00:03:54,799 --> 00:03:58,709
solution number one upgrade your

00:03:57,090 --> 00:03:59,340
firmware upgrade your bias turn on your

00:03:58,709 --> 00:04:02,280
fans

00:03:59,340 --> 00:04:03,750
you've got servers $5,000 worth of CPUs

00:04:02,280 --> 00:04:06,780
and a server you were about $20 with the

00:04:03,750 --> 00:04:08,609
fans know turn your fans on 100% run

00:04:06,780 --> 00:04:10,859
them all the time get your fans going

00:04:08,609 --> 00:04:13,019
call your systems down they'll run a lot

00:04:10,859 --> 00:04:14,669
faster they might run twice as fast this

00:04:13,019 --> 00:04:15,750
is a big deal do you bite your CPUs

00:04:14,669 --> 00:04:17,880
might be throttling you don't even know

00:04:15,750 --> 00:04:20,609
it you might run twice as fast as my

00:04:17,880 --> 00:04:22,409
turn on your fans all right so your

00:04:20,609 --> 00:04:24,419
systems aren't crashing anymore now

00:04:22,409 --> 00:04:25,389
you've got an offense going your users

00:04:24,419 --> 00:04:27,909
come to you

00:04:25,389 --> 00:04:29,650
Pass is broken what's broken is that the

00:04:27,909 --> 00:04:34,270
server is that the client on Linux why

00:04:29,650 --> 00:04:36,729
not both all right so on Linux you know

00:04:34,270 --> 00:04:37,779
the Linux NFS implementation god bless

00:04:36,729 --> 00:04:39,999
them but it's probably not the best

00:04:37,779 --> 00:04:41,680
implementation that has ever occurred in

00:04:39,999 --> 00:04:44,650
the state of you know in the world for

00:04:41,680 --> 00:04:46,120
NFS implementations the caching logic is

00:04:44,650 --> 00:04:48,879
complicated confusing and probably

00:04:46,120 --> 00:04:51,159
broken the next we'll try to use one TCP

00:04:48,879 --> 00:04:53,259
connection for everything - everyone's

00:04:51,159 --> 00:04:55,779
detriment the RPC system does not

00:04:53,259 --> 00:04:58,150
multiplex very well by default and for

00:04:55,779 --> 00:04:59,590
no good reason way back in the day they

00:04:58,150 --> 00:05:01,180
said we're gonna put the Linux and FS

00:04:59,590 --> 00:05:03,009
server and the kernel why not it's fast

00:05:01,180 --> 00:05:05,080
you know but think about it do we do

00:05:03,009 --> 00:05:06,759
association the kernel no do we do HTTP

00:05:05,080 --> 00:05:08,860
in the kernel no why are we doing an

00:05:06,759 --> 00:05:10,180
offense in the kernel really for no good

00:05:08,860 --> 00:05:11,319
reason and pretty much they discovered

00:05:10,180 --> 00:05:11,860
this as soon as they did it but it was

00:05:11,319 --> 00:05:13,599
too late

00:05:11,860 --> 00:05:16,449
cloud forward and enough festus doing

00:05:13,599 --> 00:05:19,479
the kernel to this day so gonna be fixed

00:05:16,449 --> 00:05:22,029
but not yet all right

00:05:19,479 --> 00:05:24,400
but Linux FS being horrible it's not

00:05:22,029 --> 00:05:27,849
really the fault of the people working

00:05:24,400 --> 00:05:30,669
on Linux NFS the problem here is that

00:05:27,849 --> 00:05:33,250
all software is broken how do you fix

00:05:30,669 --> 00:05:36,159
software you test it you upgrade it how

00:05:33,250 --> 00:05:38,650
do you test the kernel where NFS is you

00:05:36,159 --> 00:05:41,500
don't the Linux automated testing is

00:05:38,650 --> 00:05:43,899
essentially just does it boot all right

00:05:41,500 --> 00:05:46,509
ship it out see if anybody reports bugs

00:05:43,899 --> 00:05:48,699
if nobody uses the software if nobody's

00:05:46,509 --> 00:05:51,639
reporting bugs the code is buggy the

00:05:48,699 --> 00:05:54,219
code is buggy Linux is surprisingly

00:05:51,639 --> 00:05:55,930
buggy how hard is it to write C code I

00:05:54,219 --> 00:06:03,610
mean come on people know how many slaves

00:05:55,930 --> 00:06:05,979
to the joke yeah C C is not writing C

00:06:03,610 --> 00:06:08,020
code is not actually simple to do bug

00:06:05,979 --> 00:06:09,219
free so if you I told you to buy

00:06:08,020 --> 00:06:11,020
hardware support the next thing I'm

00:06:09,219 --> 00:06:12,699
gonna tell you to do is buy software to

00:06:11,020 --> 00:06:14,080
support or if you don't buy small for

00:06:12,699 --> 00:06:15,279
support at least use the same stuff that

00:06:14,080 --> 00:06:17,379
people who are paying for software

00:06:15,279 --> 00:06:20,919
support are using so that means in this

00:06:17,379 --> 00:06:23,139
case that means in this case you know

00:06:20,919 --> 00:06:25,930
run your Red Hat kernels run your kernel

00:06:23,139 --> 00:06:28,979
or long term support kernels do not run

00:06:25,930 --> 00:06:28,979
do to stock kernels

00:06:30,870 --> 00:06:37,240
because they're full of bugs so how does

00:06:35,800 --> 00:06:38,530
the Linux kernel management we're gonna

00:06:37,240 --> 00:06:40,510
get back to NFS we're gonna take a

00:06:38,530 --> 00:06:43,000
detour how does the Linux kernel deal

00:06:40,510 --> 00:06:44,800
with memory well not very well it turns

00:06:43,000 --> 00:06:46,990
out and this is a surprise to a lot of

00:06:44,800 --> 00:06:48,460
Linux kernel maintainer x' what do you

00:06:46,990 --> 00:06:50,140
mean I wrote all of this code to handle

00:06:48,460 --> 00:06:51,160
all these out of memory conditions and

00:06:50,140 --> 00:06:53,680
you're telling me it never actually

00:06:51,160 --> 00:06:55,900
gives a run are you kidding me no Linux

00:06:53,680 --> 00:06:57,580
will hang forever if it if it gets into

00:06:55,900 --> 00:06:59,770
an in kernel low memory situation and

00:06:57,580 --> 00:07:01,330
that's the best-case in the worst case

00:06:59,770 --> 00:07:03,550
it runs out of memory it corrupts your

00:07:01,330 --> 00:07:05,530
data I've seen it happen I think's what

00:07:03,550 --> 00:07:07,660
we use all of its in kernel memory thing

00:07:05,530 --> 00:07:08,890
and that hits bugs because codes weren't

00:07:07,660 --> 00:07:09,760
over it over try to allocate memory it

00:07:08,890 --> 00:07:13,720
doesn't happen very often

00:07:09,760 --> 00:07:17,680
it's a bug your data is corrupted all

00:07:13,720 --> 00:07:20,050
right so the reason NFS is actually

00:07:17,680 --> 00:07:22,090
broken is because you've run out of

00:07:20,050 --> 00:07:25,180
memory and you'll see the symptoms of

00:07:22,090 --> 00:07:27,550
this if your server runs slowly for no

00:07:25,180 --> 00:07:28,780
reason you're looking at table it's not

00:07:27,550 --> 00:07:30,730
doing anything why is it running so

00:07:28,780 --> 00:07:32,110
slowly you've got out of memory errors

00:07:30,730 --> 00:07:34,120
and logs but there's plenty of free

00:07:32,110 --> 00:07:36,490
memory maybe your filesystem scrubbed

00:07:34,120 --> 00:07:38,560
what the heck happened what happened is

00:07:36,490 --> 00:07:40,390
your 10 gig Nick ran your kernel out of

00:07:38,560 --> 00:07:42,190
memory combined with your excess

00:07:40,390 --> 00:07:43,990
filesystem allocating in tons of memory

00:07:42,190 --> 00:07:47,070
your kernel memory so give it more

00:07:43,990 --> 00:07:49,600
memory upgrade your kernel step 1 and

00:07:47,070 --> 00:07:50,740
increase some tuneable x' slides are on

00:07:49,600 --> 00:07:53,350
the web's you can go look at these two

00:07:50,740 --> 00:07:55,330
numbers later all right

00:07:53,350 --> 00:07:56,860
now your system a little bit more stable

00:07:55,330 --> 00:07:58,900
but your users are still coming to you

00:07:56,860 --> 00:08:01,150
and if this is still slow and if this is

00:07:58,900 --> 00:08:03,700
still broken well what's going on here

00:08:01,150 --> 00:08:05,470
well you've got high load on the clients

00:08:03,700 --> 00:08:07,050
now the server is doing ok but the

00:08:05,470 --> 00:08:11,290
clients a high load and they're crashing

00:08:07,050 --> 00:08:14,200
all right so the issue here is all

00:08:11,290 --> 00:08:16,090
software is buggy this the NFS client

00:08:14,200 --> 00:08:18,730
software is buggy there's a particular

00:08:16,090 --> 00:08:20,200
tunable that everybody knows to set even

00:08:18,730 --> 00:08:22,330
though the documentation says you don't

00:08:20,200 --> 00:08:26,020
have to set it when you don't set that

00:08:22,330 --> 00:08:27,820
tunable then your graphs look kind of

00:08:26,020 --> 00:08:29,650
insane you see the load is spiking up to

00:08:27,820 --> 00:08:31,810
100 when the server is essentially idle

00:08:29,650 --> 00:08:33,640
set your tunable your severe load

00:08:31,810 --> 00:08:36,040
doesn't spike anymore your servers don't

00:08:33,640 --> 00:08:39,250
crash it more all right you have to set

00:08:36,040 --> 00:08:40,960
this tunable before the NFS amounts get

00:08:39,250 --> 00:08:43,900
mounted so you got to put it in modprobe

00:08:40,960 --> 00:08:45,220
and this is just three lines

00:08:43,900 --> 00:08:46,800
on one side in this talk but it's

00:08:45,220 --> 00:08:50,290
probably the most important three lines

00:08:46,800 --> 00:08:52,570
use multiple IPS on the server if you've

00:08:50,290 --> 00:08:54,970
got a server and it's only got one IP

00:08:52,570 --> 00:08:57,280
all of your connections from each client

00:08:54,970 --> 00:08:59,380
are going over one TCP connection which

00:08:57,280 --> 00:09:00,580
is so it makes sense multiplexer

00:08:59,380 --> 00:09:02,800
connections I don't know why the ladies

00:09:00,580 --> 00:09:04,420
plant doesn't do this by default but you

00:09:02,800 --> 00:09:07,240
can trick it into doing it just give

00:09:04,420 --> 00:09:11,170
your server a whole bunch of IPs and set

00:09:07,240 --> 00:09:12,790
up a whole bunch of directory mounts in

00:09:11,170 --> 00:09:14,830
your FS tab and now you've got multiple

00:09:12,790 --> 00:09:16,930
IP connections to your server and it

00:09:14,830 --> 00:09:18,040
will run a lot faster because you don't

00:09:16,930 --> 00:09:21,430
have people stepping all over each other

00:09:18,040 --> 00:09:23,110
all right so now your reads and writes

00:09:21,430 --> 00:09:25,420
are going mostly okay but your client

00:09:23,110 --> 00:09:28,180
your users are still saying well heavy

00:09:25,420 --> 00:09:30,280
heavy write loads cause issues so what's

00:09:28,180 --> 00:09:32,080
happened here is on the server side

00:09:30,280 --> 00:09:33,400
you've filled up your whole servers

00:09:32,080 --> 00:09:35,260
memory as much you know you got a server

00:09:33,400 --> 00:09:37,330
with a huge amount of RAM and Linux is

00:09:35,260 --> 00:09:38,400
fill it all up with with dirty data with

00:09:37,330 --> 00:09:41,050
data that needs to be written to disk

00:09:38,400 --> 00:09:42,910
and at some point limits gonna say hold

00:09:41,050 --> 00:09:44,350
on this is too much data in memory I

00:09:42,910 --> 00:09:46,240
need to start writing this to disk and

00:09:44,350 --> 00:09:49,540
at that point things become synchronous

00:09:46,240 --> 00:09:52,000
they go really slowly and if you also

00:09:49,540 --> 00:09:53,080
been Riis at the same time then things

00:09:52,000 --> 00:09:55,030
are really slowly

00:09:53,080 --> 00:09:58,240
so tell Linux not to store so much

00:09:55,030 --> 00:10:00,160
memory in the page cache and if you

00:09:58,240 --> 00:10:02,530
really care about latency I've done this

00:10:00,160 --> 00:10:04,540
just think constantly I mean why not

00:10:02,530 --> 00:10:06,640
just literally just think all the time

00:10:04,540 --> 00:10:10,570
and this does work it slows everything

00:10:06,640 --> 00:10:12,250
down but it works all right so now your

00:10:10,570 --> 00:10:13,870
users would come in and say well alright

00:10:12,250 --> 00:10:15,070
NFS is still acting weird I don't know

00:10:13,870 --> 00:10:17,550
what's going on I got these weird

00:10:15,070 --> 00:10:20,980
application airs I can't figure it out

00:10:17,550 --> 00:10:22,630
well so the users have tried to you know

00:10:20,980 --> 00:10:24,880
it's got NFS they're trying to you know

00:10:22,630 --> 00:10:26,590
hammer this this round peg in a square

00:10:24,880 --> 00:10:28,180
hole using I have us for everything

00:10:26,590 --> 00:10:31,510
we're gonna just store everything in a

00:10:28,180 --> 00:10:33,550
file on NFS so now because of that and

00:10:31,510 --> 00:10:35,500
because of NFS caching your application

00:10:33,550 --> 00:10:36,970
has an invisible distributed cache on

00:10:35,500 --> 00:10:40,180
every single server that you can't query

00:10:36,970 --> 00:10:41,310
control invalidate you can't see a bad

00:10:40,180 --> 00:10:45,160
idea

00:10:41,310 --> 00:10:46,210
fix that upgrade your kernel because if

00:10:45,160 --> 00:10:48,420
you don't upgrade your kernel and you

00:10:46,210 --> 00:10:52,210
turn off caching your clients will crash

00:10:48,420 --> 00:10:53,680
that the kernel has some bugs and will

00:10:52,210 --> 00:10:55,750
just basically drive up load and crash

00:10:53,680 --> 00:10:57,970
the server if it's got to be constantly

00:10:55,750 --> 00:11:01,300
hitting the server forever

00:10:57,970 --> 00:11:02,290
don't use the no AC no attribute caching

00:11:01,300 --> 00:11:04,450
option you would think it would make

00:11:02,290 --> 00:11:06,100
sense turn on no after group caching but

00:11:04,450 --> 00:11:08,020
actually also turns on synchronous i/o

00:11:06,100 --> 00:11:09,820
which slows everything down so you want

00:11:08,020 --> 00:11:10,570
to use some separate options AC timeout

00:11:09,820 --> 00:11:13,090
equals zero

00:11:10,570 --> 00:11:15,370
turn off your negative lookup cache all

00:11:13,090 --> 00:11:18,000
right I'm so done with NFS there's other

00:11:15,370 --> 00:11:19,330
bugs I didn't cover upgrade your kernel

00:11:18,000 --> 00:11:21,430
all right

00:11:19,330 --> 00:11:23,410
next problem you used XT for for big

00:11:21,430 --> 00:11:25,390
data and now your data is corrupted and

00:11:23,410 --> 00:11:28,000
you can't even when the file system

00:11:25,390 --> 00:11:31,270
check is you going to memory so use X of

00:11:28,000 --> 00:11:32,740
S or ZFS but that's another talk XT for

00:11:31,270 --> 00:11:35,050
was written by some good people but it

00:11:32,740 --> 00:11:38,020
was never met for big data X if s was

00:11:35,050 --> 00:11:40,240
written and maintained to deal with huge

00:11:38,020 --> 00:11:41,830
file systems it's gonna you know it's

00:11:40,240 --> 00:11:46,420
got another 5-10 years life left in it

00:11:41,830 --> 00:11:50,260
so use X of s all right now we're going

00:11:46,420 --> 00:11:52,270
to talk about Linux swap you you're

00:11:50,260 --> 00:11:53,350
running your application you actually

00:11:52,270 --> 00:11:55,330
can run it now because you've got all

00:11:53,350 --> 00:11:57,190
your NFS bugs fixed now you've run out

00:11:55,330 --> 00:12:00,370
of memory Linux is running so slowly

00:11:57,190 --> 00:12:00,880
alright so why does Linux handle swap so

00:12:00,370 --> 00:12:03,580
poorly

00:12:00,880 --> 00:12:05,290
well because Linux doesn't lock the UI

00:12:03,580 --> 00:12:06,910
in memory other applications or other

00:12:05,290 --> 00:12:09,070
operating systems rather might lock the

00:12:06,910 --> 00:12:12,640
UI in memory and more importantly other

00:12:09,070 --> 00:12:14,890
operating systems enforce room in the

00:12:12,640 --> 00:12:17,170
page file for every byte in memory and

00:12:14,890 --> 00:12:20,590
this is important Linux does not do this

00:12:17,170 --> 00:12:22,900
on Windows if you've got 16 gigs of ram

00:12:20,590 --> 00:12:24,760
you have to have a 16 gig page file if

00:12:22,900 --> 00:12:28,830
you don't have that you can't allocate

00:12:24,760 --> 00:12:31,300
memory that means that at any point

00:12:28,830 --> 00:12:33,010
windows can swap out everything in

00:12:31,300 --> 00:12:34,720
memory and replace it with everything on

00:12:33,010 --> 00:12:36,220
disk if you've got an application that

00:12:34,720 --> 00:12:38,350
needs to be swapped out it can do that

00:12:36,220 --> 00:12:40,450
immediately the next can't because it's

00:12:38,350 --> 00:12:42,850
used both the RAM and the page file for

00:12:40,450 --> 00:12:44,560
all allocations so Linux is gonna have

00:12:42,850 --> 00:12:45,700
only a very small amount of RAM to swap

00:12:44,560 --> 00:12:47,500
things in and out and it will never

00:12:45,700 --> 00:12:49,660
finish I'm sure all of you have seen the

00:12:47,500 --> 00:12:51,520
Linux whopping bug the reason this

00:12:49,660 --> 00:12:53,890
doesn't work well is because nobody

00:12:51,520 --> 00:12:57,070
actually uses Linux swamp you just run

00:12:53,890 --> 00:13:01,660
with a disable just turn it off so HPC

00:12:57,070 --> 00:13:03,310
no swap alright so you actually do want

00:13:01,660 --> 00:13:05,740
to use swap there are a few cases where

00:13:03,310 --> 00:13:07,690
swap makes some sense and you're using

00:13:05,740 --> 00:13:08,980
it and your application wasn't swapping

00:13:07,690 --> 00:13:10,930
heavily but your system stopped working

00:13:08,980 --> 00:13:12,399
anyway and you're using

00:13:10,930 --> 00:13:13,600
one you didn't pirated cars in your

00:13:12,399 --> 00:13:17,800
service because you wanted to save some

00:13:13,600 --> 00:13:21,339
money don't do that but you did so Linux

00:13:17,800 --> 00:13:23,529
is gonna crash because there's a bug it

00:13:21,339 --> 00:13:26,440
does affect a recent kernels upgrade

00:13:23,529 --> 00:13:29,649
your kernel and/or use SWA files instead

00:13:26,440 --> 00:13:31,690
of swap partitions all right another

00:13:29,649 --> 00:13:33,630
issue service unresponsive because

00:13:31,690 --> 00:13:36,610
you're doing a lot of stuff in memory

00:13:33,630 --> 00:13:38,200
your application hasn't you know spent

00:13:36,610 --> 00:13:39,820
the time to become a database so it's

00:13:38,200 --> 00:13:42,430
just allocating memory willy-nilly and

00:13:39,820 --> 00:13:45,220
you have huge pages enabled in your in

00:13:42,430 --> 00:13:47,529
your kernel all right now transparent

00:13:45,220 --> 00:13:49,240
huge pages is this great idea that

00:13:47,529 --> 00:13:51,070
somebody had that just totally

00:13:49,240 --> 00:13:53,230
complicates the Linux kernel to no end

00:13:51,070 --> 00:13:54,820
the Linux kernel for huge pages to work

00:13:53,230 --> 00:13:56,740
has to constantly be compacting and

00:13:54,820 --> 00:13:57,940
defragging memory to make enough room to

00:13:56,740 --> 00:14:00,730
be able to allocate these huge pages

00:13:57,940 --> 00:14:02,380
it's 1999 again except instead of double

00:14:00,730 --> 00:14:04,540
spacing and defragging or drives Linux

00:14:02,380 --> 00:14:08,649
is defrag in our memory and obviously

00:14:04,540 --> 00:14:10,839
it's buggy so just turn it off never

00:14:08,649 --> 00:14:13,120
into a huge pages enabled disables all

00:14:10,839 --> 00:14:15,180
the compaction logic do huge pages in

00:14:13,120 --> 00:14:17,560
your application if it makes sense

00:14:15,180 --> 00:14:19,779
next problem you reading a huge file

00:14:17,560 --> 00:14:22,000
you're thrashing disk constantly but

00:14:19,779 --> 00:14:24,490
you've got tons of RAM the Linux should

00:14:22,000 --> 00:14:28,209
be caching this the page cache exists to

00:14:24,490 --> 00:14:29,350
read all the data in cache it and you

00:14:28,209 --> 00:14:32,050
don't have to read disk anymore why

00:14:29,350 --> 00:14:35,020
isn't this happening well it's because

00:14:32,050 --> 00:14:36,400
heavy writes are pushing any any writes

00:14:35,020 --> 00:14:38,440
not even heavy writes rights are pushing

00:14:36,400 --> 00:14:41,020
the read data out of memory this the

00:14:38,440 --> 00:14:42,940
kernel doesn't care it upgrades are

00:14:41,020 --> 00:14:44,529
going to help this issue because it's

00:14:42,940 --> 00:14:46,510
gonna prioritize writes going to the

00:14:44,529 --> 00:14:48,370
cache over reads so it's not going to

00:14:46,510 --> 00:14:49,660
try to cache your memory all your files

00:14:48,370 --> 00:14:52,120
so you have to do the same tricks the

00:14:49,660 --> 00:14:54,160
database app uses you've got to lock the

00:14:52,120 --> 00:14:55,630
memory into into Ram which you can do

00:14:54,160 --> 00:14:58,779
pretty easily but you have to know to do

00:14:55,630 --> 00:15:00,579
it all right you did run out of space

00:14:58,779 --> 00:15:01,959
because your application actually works

00:15:00,579 --> 00:15:03,880
now you've been reading and writing

00:15:01,959 --> 00:15:06,610
you're having fun now you've run out of

00:15:03,880 --> 00:15:08,740
space what do you delete running a disk

00:15:06,610 --> 00:15:10,060
usage report on a system on a file

00:15:08,740 --> 00:15:11,230
system with hundreds of terabytes this

00:15:10,060 --> 00:15:13,540
data is going to take some time it's

00:15:11,230 --> 00:15:16,450
gonna take days potentially so Corona

00:15:13,540 --> 00:15:18,220
disk usage report add locking add file

00:15:16,450 --> 00:15:20,620
rotation so that you have the old files

00:15:18,220 --> 00:15:24,980
even if you ran out of space

00:15:20,620 --> 00:15:27,470
so lots of problems potentially with big

00:15:24,980 --> 00:15:29,149
data you know I haven't gone over the

00:15:27,470 --> 00:15:30,410
compute side that's a whole other issue

00:15:29,149 --> 00:15:32,600
going over the storage a little bit with

00:15:30,410 --> 00:15:35,389
you I'm going to talk about the future

00:15:32,600 --> 00:15:38,149
of the neck storage very briefly so like

00:15:35,389 --> 00:15:41,630
I said by itself Linux is not really a

00:15:38,149 --> 00:15:43,279
good big storage system it's Linux does

00:15:41,630 --> 00:15:45,440
not have the tools you need to be able

00:15:43,279 --> 00:15:46,490
to do big storage well so again if you

00:15:45,440 --> 00:15:49,459
can if you have the money

00:15:46,490 --> 00:15:51,740
don't use Linux or Pig for big storage

00:15:49,459 --> 00:15:54,740
by an enterprise storage system if you

00:15:51,740 --> 00:15:56,600
can enterprise storage systems will do

00:15:54,740 --> 00:15:58,850
things like be able to allocate every

00:15:56,600 --> 00:16:00,709
single disk individually right so Linux

00:15:58,850 --> 00:16:02,000
if you've got a raid card it can't see

00:16:00,709 --> 00:16:04,310
every disk it just sees what the raid

00:16:02,000 --> 00:16:06,170
card gives it big storage systems you

00:16:04,310 --> 00:16:08,449
can configure the amount of redundancy

00:16:06,170 --> 00:16:10,190
you have so you can say I want an entire

00:16:08,449 --> 00:16:12,800
node to be able to go offline or I want

00:16:10,190 --> 00:16:14,839
you know five disks to be able offline

00:16:12,800 --> 00:16:16,790
you can configure how much overhead you

00:16:14,839 --> 00:16:18,529
have you know a third storage overhead

00:16:16,790 --> 00:16:22,250
30% storage overhead you can figure that

00:16:18,529 --> 00:16:24,529
overhead if you want to use Linux and

00:16:22,250 --> 00:16:26,800
you can buy good block storage which

00:16:24,529 --> 00:16:28,220
means-- means a good raid card and

00:16:26,800 --> 00:16:30,350
direct-attached storage

00:16:28,220 --> 00:16:33,699
plug that into your server you know fill

00:16:30,350 --> 00:16:33,699
it up with disks you're good to go

00:16:33,970 --> 00:16:39,260
in the future there are some things

00:16:37,100 --> 00:16:42,410
butter FS maybe you know there's

00:16:39,260 --> 00:16:45,079
Oracle's working on it staff Red Hat's

00:16:42,410 --> 00:16:46,550
working on that ZFS on Linux perhaps or

00:16:45,079 --> 00:16:50,089
just switch to BSD but that's another

00:16:46,550 --> 00:16:51,290
talk right now can provide good block

00:16:50,089 --> 00:16:52,910
storage so the difference you know

00:16:51,290 --> 00:16:55,550
really quick between block storage and

00:16:52,910 --> 00:16:59,380
object storage block storage is just

00:16:55,550 --> 00:17:01,790
your the kernel sees it all is one big

00:16:59,380 --> 00:17:03,350
contiguous storage space and it has to

00:17:01,790 --> 00:17:05,350
put a file system on top of that object

00:17:03,350 --> 00:17:07,280
storage you're storing retrieving

00:17:05,350 --> 00:17:09,559
objects you're storing retrieving files

00:17:07,280 --> 00:17:11,870
so you've already got the file system

00:17:09,559 --> 00:17:14,360
there Ceph right now can do block

00:17:11,870 --> 00:17:15,709
storage and you so to use that you

00:17:14,360 --> 00:17:17,179
either have to be using you know some

00:17:15,709 --> 00:17:18,589
kind of virtualization system where it's

00:17:17,179 --> 00:17:20,689
expecting block storage because it's

00:17:18,589 --> 00:17:22,490
just assigning that to a VM or you've

00:17:20,689 --> 00:17:23,809
got to put exit fast on it and you're

00:17:22,490 --> 00:17:25,160
not nobody's going to support you people

00:17:23,809 --> 00:17:26,990
are doing it but nobody's going to

00:17:25,160 --> 00:17:29,419
support you if you put extra vests on

00:17:26,990 --> 00:17:31,970
SEF and then serve that over NFS it

00:17:29,419 --> 00:17:35,230
might work if you spend a lot of time

00:17:31,970 --> 00:17:37,970
it out but nobody will support you but

00:17:35,230 --> 00:17:39,409
Saif is working on the object level

00:17:37,970 --> 00:17:40,970
storage as well so Saif has block

00:17:39,409 --> 00:17:42,409
storage down and they're working on the

00:17:40,970 --> 00:17:45,110
object level storage in the form of the

00:17:42,409 --> 00:17:46,700
Saif file system probably still five to

00:17:45,110 --> 00:17:48,200
ten years out but when this hits it's

00:17:46,700 --> 00:17:49,970
going to be pretty nifty because then

00:17:48,200 --> 00:17:51,830
Linux will have essentially an

00:17:49,970 --> 00:17:54,350
enterprise level storage system built

00:17:51,830 --> 00:17:55,850
into it it'll be able to address each

00:17:54,350 --> 00:17:57,919
drive separately it'll be able to

00:17:55,850 --> 00:17:59,120
configure the redundancy levels the

00:17:57,919 --> 00:18:01,220
erase your coding all that kind of stuff

00:17:59,120 --> 00:18:03,049
so this is this is pretty nifty when

00:18:01,220 --> 00:18:04,700
this finally gets going then

00:18:03,049 --> 00:18:07,429
the enterprise storage people might be

00:18:04,700 --> 00:18:11,570
in some trouble and of course the Linux

00:18:07,429 --> 00:18:13,340
NFS internal server is needs to be put

00:18:11,570 --> 00:18:15,440
down and it will be there's a project

00:18:13,340 --> 00:18:17,210
called Ganesha NFS which is ready now

00:18:15,440 --> 00:18:19,809
for some workloads it's getting heavily

00:18:17,210 --> 00:18:24,169
worked by Red Hat so this this will be

00:18:19,809 --> 00:18:26,419
going against suit in cluster so for now

00:18:24,169 --> 00:18:28,010
if you want to do our heads down if you

00:18:26,419 --> 00:18:30,140
want to do big data storage on Linux use

00:18:28,010 --> 00:18:32,299
Gluster bluster is really simple

00:18:30,140 --> 00:18:33,559
what Gluster does is it takes your

00:18:32,299 --> 00:18:35,690
existing block level storage your

00:18:33,559 --> 00:18:38,059
existing raid cards and drives you put

00:18:35,690 --> 00:18:39,950
XFS on it and in the cluster system what

00:18:38,059 --> 00:18:42,320
that adds is it duplicates the data at

00:18:39,950 --> 00:18:44,690
the file level across multiple servers

00:18:42,320 --> 00:18:46,549
and file systems it's really simple and

00:18:44,690 --> 00:18:49,309
it works you know pretty simple concepts

00:18:46,549 --> 00:18:50,600
it's stable so to scale all that out to

00:18:49,309 --> 00:18:52,610
get better storage you just have to buy

00:18:50,600 --> 00:18:57,020
better disks better raid cars that kind

00:18:52,610 --> 00:18:59,059
of stuff okay and then just shout out to

00:18:57,020 --> 00:19:05,179
Red Hat they do a great job so give them

00:18:59,059 --> 00:19:06,919
money alright so so I told you buy

00:19:05,179 --> 00:19:08,780
supported hardware running your hardware

00:19:06,919 --> 00:19:10,100
so you find your memory errors before

00:19:08,780 --> 00:19:12,200
your application doesn't corrupt data

00:19:10,100 --> 00:19:13,820
upgrade you're biased or on your fans

00:19:12,200 --> 00:19:18,200
upgrade your kernel upgrade your Perl

00:19:13,820 --> 00:19:19,970
the app see Perl use NFS wisely apply

00:19:18,200 --> 00:19:22,280
the NFS tricks in the talk

00:19:19,970 --> 00:19:24,230
don't use swap use EXO fests lower your

00:19:22,280 --> 00:19:28,929
caching to enables wait for Steph to

00:19:24,230 --> 00:19:28,929
become stable and that's it thank you

00:19:40,040 --> 00:19:53,360
so for Hadoop how'd it make some sense

00:19:44,920 --> 00:19:54,740
it's it's more if you're yeah so so so

00:19:53,360 --> 00:19:57,860
if you if you structure your application

00:19:54,740 --> 00:19:59,330
around an existing big data system like

00:19:57,860 --> 00:20:01,850
I do or something like that Cassandra or

00:19:59,330 --> 00:20:03,500
whatever then great fine you know that

00:20:01,850 --> 00:20:04,400
there's no problem with that you've got

00:20:03,500 --> 00:20:05,780
to make sure that you're you're

00:20:04,400 --> 00:20:07,940
maintaining it properly but other than

00:20:05,780 --> 00:20:09,440
that it works fine what you have to be

00:20:07,940 --> 00:20:10,670
aware of that is that you have to

00:20:09,440 --> 00:20:12,770
structure your application that way you

00:20:10,670 --> 00:20:14,450
can't just take a bunch of tools that

00:20:12,770 --> 00:20:16,670
you know bioinformatics have written and

00:20:14,450 --> 00:20:19,130
throw them on to a server and and then

00:20:16,670 --> 00:20:20,960
change them to use Hadoop doesn't work

00:20:19,130 --> 00:20:23,300
so in our case we really have to just

00:20:20,960 --> 00:20:25,220
have big servers and a lot of data that

00:20:23,300 --> 00:20:26,840
can be accessed over the network because

00:20:25,220 --> 00:20:28,580
we're just we're just running simple

00:20:26,840 --> 00:20:29,990
Linux applications well not simple we're

00:20:28,580 --> 00:20:31,850
just running plain Linux applications

00:20:29,990 --> 00:20:34,870
against it's huge data sets so it

00:20:31,850 --> 00:20:34,870
doesn't fit our applications

00:20:39,870 --> 00:20:42,500
yeah

00:20:48,390 --> 00:20:51,500
yeah that might work

00:20:56,830 --> 00:21:01,680
well people I haven't used BSD

00:20:58,870 --> 00:21:05,860
extensively people say BSD is better I

00:21:01,680 --> 00:21:09,250
try it and tell me so yeah I'm getting

00:21:05,860 --> 00:21:11,550
told we're done so I come by ask me for

00:21:09,250 --> 00:21:11,550

YouTube URL: https://www.youtube.com/watch?v=QG9FIFKDKOU


