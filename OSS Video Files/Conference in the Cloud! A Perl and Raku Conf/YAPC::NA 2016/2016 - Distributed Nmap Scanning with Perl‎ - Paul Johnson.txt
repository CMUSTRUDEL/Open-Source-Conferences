Title: 2016 - Distributed Nmap Scanning with Perlâ€Ž - Paul Johnson
Publication date: 2016-06-27
Playlist: YAPC::NA 2016
Description: 
	
Captions: 
	00:00:00,560 --> 00:00:06,089
good morning my name is Paul Johnson I'm

00:00:03,449 --> 00:00:11,040
presenting distribute and map scanning

00:00:06,089 --> 00:00:13,259
with pearl so I'm basking it on Twitter

00:00:11,040 --> 00:00:16,230
and that's my github URL where I'll put

00:00:13,259 --> 00:00:17,369
this talk later um a little about a

00:00:16,230 --> 00:00:19,410
little bit about me I'm a professional

00:00:17,369 --> 00:00:21,240
employed security type person I've been

00:00:19,410 --> 00:00:24,990
coding in Pearl on and off for many

00:00:21,240 --> 00:00:27,840
years my previous employer where this

00:00:24,990 --> 00:00:28,949
all started is redacted and my current

00:00:27,840 --> 00:00:32,910
employer where I'm doing this again is

00:00:28,949 --> 00:00:34,800
also redacted so talk is presented by me

00:00:32,910 --> 00:00:36,930
as an individual no employer client

00:00:34,800 --> 00:00:40,290
mouthful freighting the content it also

00:00:36,930 --> 00:00:43,140
predates a shmoocon talk from last year

00:00:40,290 --> 00:00:46,500
collaborative scanning with minions this

00:00:43,140 --> 00:00:48,840
all started back in 2014 or so or 2013

00:00:46,500 --> 00:00:51,120
I've the initial part that I was not

00:00:48,840 --> 00:00:53,879
involved in so in the beginning before I

00:00:51,120 --> 00:00:56,460
was involved we had centrally located

00:00:53,879 --> 00:01:01,289
hosts doing a large mm scans a large

00:00:56,460 --> 00:01:05,250
network over 700,000 live IPS scans were

00:01:01,289 --> 00:01:07,530
launched in / 16 blocks and the overall

00:01:05,250 --> 00:01:09,900
scanning took three plus weeks to

00:01:07,530 --> 00:01:13,020
complete which is a long time because

00:01:09,900 --> 00:01:14,280
they were actually running 24 7 the

00:01:13,020 --> 00:01:17,100
codebase was already in Pearl when I

00:01:14,280 --> 00:01:19,409
showed up which was very nice the

00:01:17,100 --> 00:01:22,650
problem I got presented like day one was

00:01:19,409 --> 00:01:24,570
how to scan better to issues to address

00:01:22,650 --> 00:01:26,159
you know it's improve the speed and scan

00:01:24,570 --> 00:01:28,710
and we're going to scale up for about

00:01:26,159 --> 00:01:31,439
eight scanners to 200 scanners deployed

00:01:28,710 --> 00:01:34,170
throughout the network that's the end of

00:01:31,439 --> 00:01:36,150
the formal requirements I was given the

00:01:34,170 --> 00:01:37,829
small team included myself the original

00:01:36,150 --> 00:01:39,299
tool author who I was able to rely on a

00:01:37,829 --> 00:01:42,240
little bit and then somebody else to add

00:01:39,299 --> 00:01:43,979
extra time that we used most of the work

00:01:42,240 --> 00:01:47,220
was done by me because I was personally

00:01:43,979 --> 00:01:49,200
paid to do it so my idea was great a

00:01:47,220 --> 00:01:51,180
scanning botnet first thing don't call

00:01:49,200 --> 00:01:52,710
it a botnet management not going to like

00:01:51,180 --> 00:01:53,850
it even if their security people because

00:01:52,710 --> 00:01:55,799
they're going to tell their bosses and

00:01:53,850 --> 00:02:00,479
their bosses and so call it a

00:01:55,799 --> 00:02:01,860
distributed scanning solution second we

00:02:00,479 --> 00:02:03,329
needed to reduce the impact of any

00:02:01,860 --> 00:02:04,799
single scan slowing down the whole

00:02:03,329 --> 00:02:07,860
network progress that's why it was

00:02:04,799 --> 00:02:10,590
taking so long those / 16 blocks it was

00:02:07,860 --> 00:02:12,900
getting bogged down in an individual IP

00:02:10,590 --> 00:02:15,220
space to scan it so how

00:02:12,900 --> 00:02:17,409
third the existing communication

00:02:15,220 --> 00:02:19,209
structure was used utilized for

00:02:17,409 --> 00:02:23,830
assistant encrypted ssl tunnels

00:02:19,209 --> 00:02:25,590
basically to all those eight hosts so

00:02:23,830 --> 00:02:27,819
you had to maintain open connections

00:02:25,590 --> 00:02:30,790
fourth there was no real audit trail

00:02:27,819 --> 00:02:32,739
given the size of the skins and you

00:02:30,790 --> 00:02:34,360
wouldn't be able to tell until after the

00:02:32,739 --> 00:02:37,510
scandals all over who is scanning what

00:02:34,360 --> 00:02:40,599
went so first how to reduce the impact

00:02:37,510 --> 00:02:44,110
of slow scans my solution was chopped it

00:02:40,599 --> 00:02:47,470
up from / 62 / twenty-fours the

00:02:44,110 --> 00:02:54,160
advantage / 24 is only 256 addresses as

00:02:47,470 --> 00:02:57,129
opposed to 64k for a / 8 or it 64 16

00:02:54,160 --> 00:03:00,909
million for the entire / 8 and 62 fourth

00:02:57,129 --> 00:03:02,349
K for a / 16 that they were scanning it

00:03:00,909 --> 00:03:03,819
also meant that if we had to rescan any

00:03:02,349 --> 00:03:05,769
individual network segment it should go

00:03:03,819 --> 00:03:07,900
a lot faster and we could potentially

00:03:05,769 --> 00:03:09,159
better identify those parts of the

00:03:07,900 --> 00:03:12,190
network where the scans were getting

00:03:09,159 --> 00:03:15,370
bogged down for whatever reason so this

00:03:12,190 --> 00:03:18,250
is what I did you know just to split up

00:03:15,370 --> 00:03:19,780
those scans so I since I was the person

00:03:18,250 --> 00:03:22,000
in putting the stuff I got to decide

00:03:19,780 --> 00:03:23,620
that I was doing cider cider was also

00:03:22,000 --> 00:03:25,510
much better because when people give you

00:03:23,620 --> 00:03:29,530
my experience was when people gave you

00:03:25,510 --> 00:03:32,079
lists of ips to scan and ranges zeros we

00:03:29,530 --> 00:03:34,209
get in there extra periods it was just a

00:03:32,079 --> 00:03:40,359
mess so I got to do this so i just

00:03:34,209 --> 00:03:43,389
decided slider not great code but

00:03:40,359 --> 00:03:45,400
workable on the communication for the

00:03:43,389 --> 00:03:47,590
agents like i said was originally SSL

00:03:45,400 --> 00:03:49,510
persistent communications it's fine for

00:03:47,590 --> 00:03:50,799
eight hosts but when you have 200 you

00:03:49,510 --> 00:03:54,639
can have the single host you know

00:03:50,799 --> 00:03:56,139
maintaining 200 encrypted tunnels also

00:03:54,639 --> 00:03:58,900
dropped communications between those

00:03:56,139 --> 00:04:01,900
hosts would cause you know scans to

00:03:58,900 --> 00:04:05,769
terminate and we're also using you know

00:04:01,900 --> 00:04:08,400
hard coded authentication credentials

00:04:05,769 --> 00:04:08,400
for with that

00:04:08,750 --> 00:04:12,959
how to improve in scale communications

00:04:10,859 --> 00:04:14,340
my thought was we already have ssh on

00:04:12,959 --> 00:04:16,530
all these boxes why not just leverage

00:04:14,340 --> 00:04:18,750
that the advantage that's a charity

00:04:16,530 --> 00:04:21,269
there we can also wrap our sink inside

00:04:18,750 --> 00:04:23,789
or inside that ssh connection and

00:04:21,269 --> 00:04:26,220
smartly sink back results so instead of

00:04:23,789 --> 00:04:27,660
like knowing we've already synced back

00:04:26,220 --> 00:04:30,000
these results we do tell our sink to

00:04:27,660 --> 00:04:31,710
sync all the results back and it's smart

00:04:30,000 --> 00:04:32,729
enough to do it for it we could also

00:04:31,710 --> 00:04:35,610
leverage the key authentication

00:04:32,729 --> 00:04:36,900
available in ssh to do all that like i

00:04:35,610 --> 00:04:38,970
said and then we're just going to

00:04:36,900 --> 00:04:41,039
connect to the remote agents in the

00:04:38,970 --> 00:04:42,990
background launch a scan and come back

00:04:41,039 --> 00:04:46,229
and check on progress later so drop

00:04:42,990 --> 00:04:49,500
connections no problems and it means

00:04:46,229 --> 00:04:53,099
really going to have a few sessions open

00:04:49,500 --> 00:04:57,300
at the same time it's real simple just

00:04:53,099 --> 00:04:59,849
using openssh our net Oh Pizza SSH we

00:04:57,300 --> 00:05:01,590
could you know configure individual key

00:04:59,849 --> 00:05:04,669
files and users for individual hosts if

00:05:01,590 --> 00:05:08,310
we wanted to we did not but we could

00:05:04,669 --> 00:05:10,710
audit trails doing security testing and

00:05:08,310 --> 00:05:12,870
audit trails are important because any

00:05:10,710 --> 00:05:15,270
time any time something goes wrong it's

00:05:12,870 --> 00:05:18,659
your fault and too unless you can prove

00:05:15,270 --> 00:05:20,520
otherwise so I mean literally if a hard

00:05:18,659 --> 00:05:22,500
drive fails while you're scanning it was

00:05:20,520 --> 00:05:26,190
the scans fault because of course it was

00:05:22,500 --> 00:05:27,270
um so and like I said before we didn't

00:05:26,190 --> 00:05:31,289
have a really good audit trail of

00:05:27,270 --> 00:05:33,840
knowing what was scanning when so my

00:05:31,289 --> 00:05:35,729
idea was set up it but most of the back

00:05:33,840 --> 00:05:37,409
end of this is all database no setup ok

00:05:35,729 --> 00:05:39,419
today's as an audit trail or database

00:05:37,409 --> 00:05:42,840
tables another trail we track the

00:05:39,419 --> 00:05:43,949
current status of the scan track when a

00:05:42,840 --> 00:05:45,930
scan on the network launched and

00:05:43,949 --> 00:05:47,520
completed track the pit of a skin so if

00:05:45,930 --> 00:05:50,460
you go ahead do anything to it we can do

00:05:47,520 --> 00:05:53,070
it quickly we check the completeness of

00:05:50,460 --> 00:05:54,300
the scan is it running is it 20 protect

00:05:53,070 --> 00:05:56,490
complete Pig percent complete hunter

00:05:54,300 --> 00:05:59,720
person complete and capture a snapshot

00:05:56,490 --> 00:06:05,550
of the hosts it found when it completes

00:05:59,720 --> 00:06:07,409
simple table um just status key to keep

00:06:05,550 --> 00:06:10,710
it there know what agents running what

00:06:07,409 --> 00:06:11,639
targets running against scan overall

00:06:10,710 --> 00:06:14,849
scan because you can create multiple

00:06:11,639 --> 00:06:17,490
scans and court on paid the status the

00:06:14,849 --> 00:06:19,740
progress when it was launched when it

00:06:17,490 --> 00:06:22,529
finished and how many hosts it's mostly

00:06:19,740 --> 00:06:24,120
int launched and finished our just you

00:06:22,529 --> 00:06:27,719
know unix time stamps because they're

00:06:24,120 --> 00:06:29,729
easier to sort um status was just an

00:06:27,719 --> 00:06:30,960
arbitrary status code which I sometimes

00:06:29,729 --> 00:06:33,659
regret doing because I would look at the

00:06:30,960 --> 00:06:38,880
table and go what was status code eight

00:06:33,659 --> 00:06:42,029
mean again so we made all these changes

00:06:38,880 --> 00:06:45,840
we did it and we launched the scan we

00:06:42,029 --> 00:06:48,419
deployed custom and map code and the

00:06:45,840 --> 00:06:49,650
users or the scanning agent and their

00:06:48,419 --> 00:06:52,560
keys and everything like that using just

00:06:49,650 --> 00:06:54,360
a lil rpm that we created and we did

00:06:52,560 --> 00:06:56,760
that because we're a red hat shop and we

00:06:54,360 --> 00:06:58,610
leveraged the satellite servers to do

00:06:56,760 --> 00:07:02,159
that so we did it launched the scan

00:06:58,610 --> 00:07:03,930
first thing commanding control launching

00:07:02,159 --> 00:07:06,389
all these scans which really slutting

00:07:03,930 --> 00:07:08,400
slow to keep up with all the scanning it

00:07:06,389 --> 00:07:12,960
was looping as I'll show you in a second

00:07:08,400 --> 00:07:15,599
um and it just couldn't keep up um so my

00:07:12,960 --> 00:07:17,400
idea was oh let's spawn threads threads

00:07:15,599 --> 00:07:19,169
are great to connect to the agents in

00:07:17,400 --> 00:07:21,210
Los the scans problem is I don't know

00:07:19,169 --> 00:07:23,070
how to do that the time so here's the

00:07:21,210 --> 00:07:25,229
original lube you know while there were

00:07:23,070 --> 00:07:28,889
targets to scan it would go through the

00:07:25,229 --> 00:07:30,930
agents and launch scans and then the

00:07:28,889 --> 00:07:34,500
launch cans would you know that actual

00:07:30,930 --> 00:07:38,669
SSH capture took a few seconds but it

00:07:34,500 --> 00:07:40,169
adds up so while everything's still

00:07:38,669 --> 00:07:42,570
running because you want to continue to

00:07:40,169 --> 00:07:45,960
me forward progress i did a crash course

00:07:42,570 --> 00:07:48,930
in perl and threads well how to use

00:07:45,960 --> 00:07:50,789
threads in perl quickly determine that

00:07:48,930 --> 00:07:52,889
spawning detached threads to launch x

00:07:50,789 --> 00:07:55,020
cans per agent was the quickest and

00:07:52,889 --> 00:07:56,669
cleanest solution now i could have done

00:07:55,020 --> 00:07:58,680
you know parallel processing him fork

00:07:56,669 --> 00:08:00,630
them off in the background but i had

00:07:58,680 --> 00:08:02,699
illusions that i would someday go back

00:08:00,630 --> 00:08:05,909
to this and have the threads communicate

00:08:02,699 --> 00:08:07,409
back in i haven't actually done that for

00:08:05,909 --> 00:08:09,120
that but I did learn enough about

00:08:07,409 --> 00:08:11,940
threads to use threads and communication

00:08:09,120 --> 00:08:13,349
later on for something different managed

00:08:11,940 --> 00:08:14,729
to actually implement the changes into

00:08:13,349 --> 00:08:18,180
production within 24 hours in scam

00:08:14,729 --> 00:08:23,200
launched the new logic as you see is

00:08:18,180 --> 00:08:27,020
just instead of doing

00:08:23,200 --> 00:08:29,210
as a stage captor just spawn a thread to

00:08:27,020 --> 00:08:30,800
do the exact same thing it took a little

00:08:29,210 --> 00:08:33,140
bit of trial and error to figure out

00:08:30,800 --> 00:08:34,310
what size the stack needed to be because

00:08:33,140 --> 00:08:36,320
when I started off in the default one

00:08:34,310 --> 00:08:38,270
you just kept crashing so it was like

00:08:36,320 --> 00:08:41,510
figure out what was going wrong and

00:08:38,270 --> 00:08:43,820
debug we actually completed the scan in

00:08:41,510 --> 00:08:45,830
under 12 days which is a lot of hard

00:08:43,820 --> 00:08:47,810
percent improvement scan results were

00:08:45,830 --> 00:08:49,700
good and we found hosts with live corpse

00:08:47,810 --> 00:08:51,380
and stuff like that and management was

00:08:49,700 --> 00:08:55,040
happy we didn't break anything that they

00:08:51,380 --> 00:08:57,350
weren't aware of I self identified areas

00:08:55,040 --> 00:08:59,270
to improve including making the scanning

00:08:57,350 --> 00:09:00,380
smarter so we have these two hundred

00:08:59,270 --> 00:09:03,080
agents scattered throughout the network

00:09:00,380 --> 00:09:05,360
why not figure out what's near them to

00:09:03,080 --> 00:09:06,860
scan closer so we did that and we later

00:09:05,360 --> 00:09:10,760
double performance again because there's

00:09:06,860 --> 00:09:12,320
a lot less Network latency we weren't

00:09:10,760 --> 00:09:13,760
blogging down like you know sort of

00:09:12,320 --> 00:09:17,810
rocdur routers in the core or anything

00:09:13,760 --> 00:09:20,750
like that we also what else I determined

00:09:17,810 --> 00:09:22,580
was that cnc or the kind of control was

00:09:20,750 --> 00:09:24,380
not very smart about the load on the

00:09:22,580 --> 00:09:27,080
agents so it just kept launching you

00:09:24,380 --> 00:09:28,640
know making sure there was like 10 scans

00:09:27,080 --> 00:09:31,190
what I Nagant any time no matter what

00:09:28,640 --> 00:09:32,900
the system load was but that sometimes

00:09:31,190 --> 00:09:34,940
didn't improve performance so throttle

00:09:32,900 --> 00:09:38,570
new scanning if the load was over X

00:09:34,940 --> 00:09:40,190
amount we also had scans that hung you

00:09:38,570 --> 00:09:41,870
know this was actually part of the root

00:09:40,190 --> 00:09:43,490
cause of why those other scans would

00:09:41,870 --> 00:09:46,490
take so long as you know individual

00:09:43,490 --> 00:09:48,560
networks would hang up scans so for

00:09:46,490 --> 00:09:50,780
actively like check back and if it's

00:09:48,560 --> 00:09:53,420
been running too long kill it and start

00:09:50,780 --> 00:09:55,310
it again um we build on the knowledge

00:09:53,420 --> 00:09:57,440
base we were accumulating about like

00:09:55,310 --> 00:10:01,700
what hosts were out there how many we

00:09:57,440 --> 00:10:03,140
should expect on a given on a given

00:10:01,700 --> 00:10:04,550
Network or anything like that so that we

00:10:03,140 --> 00:10:06,050
found a hundred hosts on their last time

00:10:04,550 --> 00:10:07,370
we scan we were seeing two now what's

00:10:06,050 --> 00:10:10,310
wrong let's go back and check that out

00:10:07,370 --> 00:10:12,140
later add quality control checks to

00:10:10,310 --> 00:10:14,800
verify the XML results we were getting

00:10:12,140 --> 00:10:18,320
back from nmap because what I found was

00:10:14,800 --> 00:10:20,150
an map when you are launching lots of in

00:10:18,320 --> 00:10:22,640
map scans at the same time well

00:10:20,150 --> 00:10:24,830
sometimes there's a pointer in the a map

00:10:22,640 --> 00:10:27,140
code that will sometimes write that XML

00:10:24,830 --> 00:10:29,330
out non-sequential a so you'll get the

00:10:27,140 --> 00:10:31,160
end tags in the middle of the XML it'll

00:10:29,330 --> 00:10:34,570
be complete but just not on the right

00:10:31,160 --> 00:10:37,520
order so the SML is effectively

00:10:34,570 --> 00:10:41,840
so did that later so you you'd have to

00:10:37,520 --> 00:10:46,100
like sink back results and verify the

00:10:41,840 --> 00:10:47,690
XML as the scan is running management

00:10:46,100 --> 00:10:48,950
directed changes because after we did

00:10:47,690 --> 00:10:51,410
this they're like oh can you do

00:10:48,950 --> 00:10:52,790
different things the ability to stop

00:10:51,410 --> 00:10:54,380
this the end in the middle and resume

00:10:52,790 --> 00:10:55,610
later so you know if there's a

00:10:54,380 --> 00:10:57,710
maintenance window that you don't want

00:10:55,610 --> 00:10:59,060
to do or you don't want to be affecting

00:10:57,710 --> 00:11:01,010
or anything like that so this was

00:10:59,060 --> 00:11:02,840
actually kind of easy with the audit

00:11:01,010 --> 00:11:04,940
table because you just you kill the

00:11:02,840 --> 00:11:06,650
scans and you went into the audit table

00:11:04,940 --> 00:11:08,300
and figure it out what was it like

00:11:06,650 --> 00:11:11,480
running at the time and just recue them

00:11:08,300 --> 00:11:14,050
in so minor tweaks and we did that fine

00:11:11,480 --> 00:11:16,250
that canteen handy because later on

00:11:14,050 --> 00:11:18,080
based on the second one management

00:11:16,250 --> 00:11:19,790
wanted us to kill scans um they

00:11:18,080 --> 00:11:21,110
arbitrarily decided us we wanted they

00:11:19,790 --> 00:11:23,740
wanted to exclude networks and or

00:11:21,110 --> 00:11:26,060
individual the individual IP addresses

00:11:23,740 --> 00:11:27,770
there were fragile hosts and then this

00:11:26,060 --> 00:11:32,060
network that would fall over in

00:11:27,770 --> 00:11:35,030
different ways when you touch them some

00:11:32,060 --> 00:11:38,180
they just went offline others it caused

00:11:35,030 --> 00:11:42,290
a dls event yep 2016 there's still

00:11:38,180 --> 00:11:45,860
fragile Hudson networks they also wanted

00:11:42,290 --> 00:11:47,420
to exclude network devices that we were

00:11:45,860 --> 00:11:49,070
scanning that where was actually you

00:11:47,420 --> 00:11:51,290
know triggering IDs alerts to a third

00:11:49,070 --> 00:11:53,270
party vendor because we were scanning

00:11:51,290 --> 00:11:56,960
through the network but these were like

00:11:53,270 --> 00:11:59,450
DMZ toasts and somebody got a flood of

00:11:56,960 --> 00:12:04,100
you know emails going why are you what

00:11:59,450 --> 00:12:06,440
are you doing stop it now so we

00:12:04,100 --> 00:12:08,090
implemented this to prevent scans well

00:12:06,440 --> 00:12:10,010
we implement San exclude file that's

00:12:08,090 --> 00:12:13,040
dynamically added to the scanning string

00:12:10,010 --> 00:12:14,870
as you do it and as a fail-safe if that

00:12:13,040 --> 00:12:17,630
exclude file is not there on the host it

00:12:14,870 --> 00:12:19,190
just won't scan the and that was

00:12:17,630 --> 00:12:20,780
actually ridiculously simple because you

00:12:19,190 --> 00:12:23,060
just add the hidden to the end map

00:12:20,780 --> 00:12:24,110
command spring and if they met commands

00:12:23,060 --> 00:12:27,920
stirring doesn't find the exclude file

00:12:24,110 --> 00:12:31,340
it just doesn't work personal lessons

00:12:27,920 --> 00:12:32,690
learned be flexible because management

00:12:31,340 --> 00:12:36,410
will ask you for things you never

00:12:32,690 --> 00:12:38,710
expected and things they didn't know

00:12:36,410 --> 00:12:38,710
they wanted

00:12:39,779 --> 00:12:46,480
so and audit trails so I was once a day

00:12:43,690 --> 00:12:49,509
to meeting with my boss and we were

00:12:46,480 --> 00:12:51,160
scanning and he got an email going where

00:12:49,509 --> 00:12:53,170
they say we're scanning and we're

00:12:51,160 --> 00:12:54,819
breaking things I've said okay fine tell

00:12:53,170 --> 00:12:57,250
me what I peas you know we're breaking I

00:12:54,819 --> 00:12:59,410
can tell you we are or not so he came

00:12:57,250 --> 00:13:01,269
back and said these things at this time

00:12:59,410 --> 00:13:03,310
we're being broken I looked that up I'm

00:13:01,269 --> 00:13:04,990
not a trail ends nope we weren't even

00:13:03,310 --> 00:13:11,019
touching that we were nowhere near there

00:13:04,990 --> 00:13:12,459
so he was very happy about that so just

00:13:11,019 --> 00:13:14,079
some shameless plugs for some other

00:13:12,459 --> 00:13:18,490
talks I've watched about threading

00:13:14,079 --> 00:13:23,459
forking and parallel processing and any

00:13:18,490 --> 00:13:23,459
questions go back to the shins plugs

00:13:27,899 --> 00:13:35,370
here by using trace Matt a traceroute

00:13:32,560 --> 00:13:37,930
information to kind of figure it out and

00:13:35,370 --> 00:13:38,980
actually for the what were we started

00:13:37,930 --> 00:13:40,000
this it was little bit easier because

00:13:38,980 --> 00:13:42,790
the networking people had better

00:13:40,000 --> 00:13:44,470
diagrams where I am now it's like a

00:13:42,790 --> 00:13:45,430
black hole so I'm really you know

00:13:44,470 --> 00:13:50,199
searching through trace right

00:13:45,430 --> 00:13:51,819
information um cell so one of the things

00:13:50,199 --> 00:13:53,980
you can add to the end map string is do

00:13:51,819 --> 00:13:56,139
a traceroute as it scans so it's

00:13:53,980 --> 00:13:57,730
capturing that information in the XML

00:13:56,139 --> 00:13:59,500
when it comes back and then you parts

00:13:57,730 --> 00:14:01,000
that all and you capture it and then

00:13:59,500 --> 00:14:02,500
what I've done for my current employer

00:14:01,000 --> 00:14:04,300
is I took all that information and the

00:14:02,500 --> 00:14:06,730
stock should a bit and put it to get so

00:14:04,300 --> 00:14:08,019
you get this huge you know cloud of

00:14:06,730 --> 00:14:10,300
stuff and you can start figuring out

00:14:08,019 --> 00:14:13,750
where like some key you know network

00:14:10,300 --> 00:14:17,579
hosts are and start from there but it's

00:14:13,750 --> 00:14:17,579
I'm found a good automated way to do it

00:14:18,819 --> 00:14:22,829
any other questions

00:14:26,019 --> 00:14:31,250
ok

00:14:27,870 --> 00:14:31,250

YouTube URL: https://www.youtube.com/watch?v=UcPw2DEzXoE


