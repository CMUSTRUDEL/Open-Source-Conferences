Title: Brad Lhotsky Lessons from High Velocity Loggingâ€Ž
Publication date: 2015-06-16
Playlist: YAPC::NA 2015
Description: 
	
Captions: 
	00:00:00,000 --> 00:00:04,620
I need to acknowledge that on this

00:00:02,550 --> 00:00:06,480
wasn't the work of one person there are

00:00:04,620 --> 00:00:08,010
a lot of amazing people that I have the

00:00:06,480 --> 00:00:12,750
privilege of working with who

00:00:08,010 --> 00:00:16,230
contributed ideas complaints code pull

00:00:12,750 --> 00:00:19,470
requests tickets late-night phone calls

00:00:16,230 --> 00:00:21,020
to make some of the the things that

00:00:19,470 --> 00:00:26,820
we're going to learn about here possible

00:00:21,020 --> 00:00:28,170
so these people are really great on what

00:00:26,820 --> 00:00:29,849
are we going to do today we're going to

00:00:28,170 --> 00:00:32,610
talk a little bit about the lessons that

00:00:29,849 --> 00:00:34,020
I've learned from various stages of

00:00:32,610 --> 00:00:35,969
logging so there's four major components

00:00:34,020 --> 00:00:37,410
to the logging infrastructure it's

00:00:35,969 --> 00:00:39,210
shipping the logs parsing the log

00:00:37,410 --> 00:00:40,530
storing the logs and then doing

00:00:39,210 --> 00:00:42,540
something with them a lot of people

00:00:40,530 --> 00:00:44,160
forget that last step don't forget that

00:00:42,540 --> 00:00:45,450
last step because there's a lot of

00:00:44,160 --> 00:00:50,129
really cool stuff that you can do with

00:00:45,450 --> 00:00:52,170
your log data that said there are a few

00:00:50,129 --> 00:00:53,219
assumptions that I'm making here and

00:00:52,170 --> 00:00:55,590
these may not line up with your

00:00:53,219 --> 00:00:59,699
environments and again these are my

00:00:55,590 --> 00:01:01,410
lessons for you as tales of caution they

00:00:59,699 --> 00:01:04,290
may not fit exactly with what you're

00:01:01,410 --> 00:01:06,600
going to do we had our Business

00:01:04,290 --> 00:01:09,299
Intelligence Platform already covered by

00:01:06,600 --> 00:01:11,100
our application logging so I was free to

00:01:09,299 --> 00:01:13,490
experiment and to break the system and

00:01:11,100 --> 00:01:16,650
so I did frequently several times a day

00:01:13,490 --> 00:01:18,840
sometimes several times a minute just to

00:01:16,650 --> 00:01:21,330
see what was possible and what we could

00:01:18,840 --> 00:01:24,750
do our entire infrastructure is Linux

00:01:21,330 --> 00:01:26,189
servers yours may not be that's usually

00:01:24,750 --> 00:01:27,710
okay a lot of the stuff that I'm talking

00:01:26,189 --> 00:01:31,350
going to talk about will work

00:01:27,710 --> 00:01:34,619
cross-platform with open source or free

00:01:31,350 --> 00:01:36,509
components anyways and this this was

00:01:34,619 --> 00:01:38,520
done during my work as a security

00:01:36,509 --> 00:01:41,579
engineer at booking.com so a lot of this

00:01:38,520 --> 00:01:43,380
came from the mindset of a security guy

00:01:41,579 --> 00:01:46,170
trying to do incident response and

00:01:43,380 --> 00:01:49,170
forensic analysis of a whole bunch of

00:01:46,170 --> 00:01:51,240
servers all at once so that's that's

00:01:49,170 --> 00:01:55,860
where this project started and it's gone

00:01:51,240 --> 00:01:56,880
a lot of different ways so the first

00:01:55,860 --> 00:01:58,259
thing we want to talk about is how to

00:01:56,880 --> 00:02:01,649
get the logs around the infrastructure

00:01:58,259 --> 00:02:03,450
on and of course at web scale and that

00:02:01,649 --> 00:02:07,740
was a transition transitions or web

00:02:03,450 --> 00:02:10,020
scale um you may disagree with this but

00:02:07,740 --> 00:02:12,390
we're using syslog as a transport

00:02:10,020 --> 00:02:13,170
mechanism for our system and access log

00:02:12,390 --> 00:02:15,959
data

00:02:13,170 --> 00:02:18,360
booking.com why syslog well it was

00:02:15,959 --> 00:02:20,459
already there all the Simmons know about

00:02:18,360 --> 00:02:23,760
it most developers are aware of how to

00:02:20,459 --> 00:02:26,430
use it on we were using syslog-ng please

00:02:23,760 --> 00:02:27,900
don't do that use our syslog and would

00:02:26,430 --> 00:02:29,700
always remember if you are looking at

00:02:27,900 --> 00:02:32,310
this as an application-level logging

00:02:29,700 --> 00:02:34,800
platform that syslog calls are blocking

00:02:32,310 --> 00:02:37,260
so if you have multiple threads or

00:02:34,800 --> 00:02:39,180
multiple children that are all writing

00:02:37,260 --> 00:02:41,370
to the same logging socket there will be

00:02:39,180 --> 00:02:44,220
some contention and that will cause

00:02:41,370 --> 00:02:46,380
dramatic impact to to your platform a

00:02:44,220 --> 00:02:48,959
classic example is are there any Linux

00:02:46,380 --> 00:02:51,780
system ends in the room has anyone ever

00:02:48,959 --> 00:02:53,340
enabled iptables with the logging thing

00:02:51,780 --> 00:02:55,290
and then watch the network performance

00:02:53,340 --> 00:02:58,680
of the box drop off that's because

00:02:55,290 --> 00:03:01,709
you're competing for right access on on

00:02:58,680 --> 00:03:04,500
dev vlog so keep that in mind there are

00:03:01,709 --> 00:03:14,340
ways around this to make syslog a lot

00:03:04,500 --> 00:03:18,209
faster but keep that in mind yes yes yes

00:03:14,340 --> 00:03:20,190
it will yes um so syslog-ng one of the

00:03:18,209 --> 00:03:21,959
things that I was told as to why we

00:03:20,190 --> 00:03:24,989
selected syslog-ng this was before I got

00:03:21,959 --> 00:03:28,170
there is that it's pretty and that is

00:03:24,989 --> 00:03:30,750
pretty elegant someone that isn't a

00:03:28,170 --> 00:03:33,390
assistant in can probably read this and

00:03:30,750 --> 00:03:35,430
understand the author's intentions and

00:03:33,390 --> 00:03:37,950
that's the key here you're reading the

00:03:35,430 --> 00:03:40,829
intentions of the author there there's

00:03:37,950 --> 00:03:43,680
some subtle bugs here I'll point a few

00:03:40,829 --> 00:03:44,970
of them out first off anything that

00:03:43,680 --> 00:03:47,400
doesn't match our filter is silently

00:03:44,970 --> 00:03:48,780
discarded there's no log entry for the

00:03:47,400 --> 00:03:51,209
rest of the stuff that fell through the

00:03:48,780 --> 00:03:54,269
cracks right so that that data is gone

00:03:51,209 --> 00:03:57,150
and will never know about it again has

00:03:54,269 --> 00:03:59,579
anyone seen follow frequency on a file

00:03:57,150 --> 00:04:02,269
entry in syslog-ng has anyone seen that

00:03:59,579 --> 00:04:05,130
before that that's basically syslog-ng

00:04:02,269 --> 00:04:07,230
by default will use epoll or I notify to

00:04:05,130 --> 00:04:09,420
find out if a file has more content on

00:04:07,230 --> 00:04:12,269
it to then forward that through the the

00:04:09,420 --> 00:04:14,160
chain you usually see a follow frequency

00:04:12,269 --> 00:04:16,019
when someone enables out on something

00:04:14,160 --> 00:04:17,850
like an access log that runs through a

00:04:16,019 --> 00:04:19,919
lot of data and then causes a high cpu

00:04:17,850 --> 00:04:22,109
load because tailing that one log file

00:04:19,919 --> 00:04:23,610
is taking up all of the resources for

00:04:22,109 --> 00:04:25,879
that one thread in

00:04:23,610 --> 00:04:28,710
ng so they add a fellow frequency one

00:04:25,879 --> 00:04:29,879
which reads that access log just once a

00:04:28,710 --> 00:04:33,719
second and checks to see if there's new

00:04:29,879 --> 00:04:36,840
data subtle bug it only reads log fetch

00:04:33,719 --> 00:04:38,520
limit lines per second who wants to take

00:04:36,840 --> 00:04:42,259
a guess as to what the default is on

00:04:38,520 --> 00:04:46,530
that particular configuration parameter

00:04:42,259 --> 00:04:48,750
yes it's 10 so in this case we said oh

00:04:46,530 --> 00:04:51,150
my god the performance of tailing us log

00:04:48,750 --> 00:04:53,520
file is too much we can't handle it so

00:04:51,150 --> 00:04:55,530
we're going to follow it just once a

00:04:53,520 --> 00:04:58,050
second and now we've limited our access

00:04:55,530 --> 00:04:59,699
logs to 10 messages per second which may

00:04:58,050 --> 00:05:05,520
not be what you intended this bug

00:04:59,699 --> 00:05:07,830
existed at booking for six years but

00:05:05,520 --> 00:05:09,060
like I said we have application logging

00:05:07,830 --> 00:05:10,560
enabled that we do all of our business

00:05:09,060 --> 00:05:12,919
intelligence off up so the only people

00:05:10,560 --> 00:05:16,560
who are suffering or the SIS admin's

00:05:12,919 --> 00:05:17,729
okay so i said rsyslog is better so why

00:05:16,560 --> 00:05:20,009
don't we do something complicated with

00:05:17,729 --> 00:05:21,509
rsyslog like if the remote servers down

00:05:20,009 --> 00:05:23,370
let's cue everything locally owned disk

00:05:21,509 --> 00:05:25,199
that way we're not stressing any

00:05:23,370 --> 00:05:26,909
intermediary endpoint all of our clients

00:05:25,199 --> 00:05:29,039
are queuing their own messages on disk

00:05:26,909 --> 00:05:31,529
which we should be able to maintain for

00:05:29,039 --> 00:05:33,210
a few minutes a few hours or in most

00:05:31,529 --> 00:05:35,219
cases a few days without causing any

00:05:33,210 --> 00:05:38,550
problems let's send all that data over

00:05:35,219 --> 00:05:44,789
TCP encrypted with peer validation of

00:05:38,550 --> 00:05:46,529
the SS of the TLS certificates and

00:05:44,789 --> 00:05:48,930
that's that's it it's certainly not as

00:05:46,529 --> 00:05:51,629
beautiful as the syslog ng configuration

00:05:48,930 --> 00:05:54,210
is but with this you have pure validated

00:05:51,629 --> 00:05:57,449
certificates you have TCP transport and

00:05:54,210 --> 00:05:59,189
a known disk you in case the remote

00:05:57,449 --> 00:06:03,210
server goes away so you won't lose any

00:05:59,189 --> 00:06:06,360
data additionally rsyslog supports a

00:06:03,210 --> 00:06:07,889
number of outputs so you can do

00:06:06,360 --> 00:06:10,289
traditional things like forwarding data

00:06:07,889 --> 00:06:13,860
around sending data to file through

00:06:10,289 --> 00:06:15,599
sockets on out to databases directly

00:06:13,860 --> 00:06:17,219
which means if you just are looking at

00:06:15,599 --> 00:06:18,960
the simple data that you want to get out

00:06:17,219 --> 00:06:21,539
of syslog you can do that very easily

00:06:18,960 --> 00:06:23,460
without adding any other components into

00:06:21,539 --> 00:06:24,779
your system it's already there and then

00:06:23,460 --> 00:06:28,169
you can also send it out to something

00:06:24,779 --> 00:06:30,270
like elasticsearch MongoDB Kafka or HDFS

00:06:28,169 --> 00:06:34,469
natively from rsyslog which is kind of

00:06:30,270 --> 00:06:35,969
nice um and this is even cooler if your

00:06:34,469 --> 00:06:36,680
mail administrator comes up to you and

00:06:35,969 --> 00:06:38,389
says

00:06:36,680 --> 00:06:40,729
we want to find a way to stress test our

00:06:38,389 --> 00:06:50,690
smtp infrastructure you can go yes we

00:06:40,729 --> 00:06:52,789
can okay I will but I hear especially a

00:06:50,690 --> 00:06:54,380
lot of meetups with all the fancy new

00:06:52,789 --> 00:06:56,840
technology said syslog isn't web-scale

00:06:54,380 --> 00:06:58,850
and it's not it's not a pub sub Q if

00:06:56,840 --> 00:07:00,380
you're interested in working with a pub

00:06:58,850 --> 00:07:01,940
sub Q for your application logs I

00:07:00,380 --> 00:07:04,970
recommend taking a look at something

00:07:01,940 --> 00:07:07,009
like Apache Kafka it's nice it supports

00:07:04,970 --> 00:07:09,050
down time when your consumers so that

00:07:07,009 --> 00:07:11,210
you can pick up right where you left off

00:07:09,050 --> 00:07:13,370
of your consumers died and it feeds

00:07:11,210 --> 00:07:15,500
directly into Apache spark so your data

00:07:13,370 --> 00:07:17,539
analysts are able to natively work with

00:07:15,500 --> 00:07:19,340
your logging data as soon as you enable

00:07:17,539 --> 00:07:20,509
the pipeline which is nice if you're

00:07:19,340 --> 00:07:21,710
looking for something lightweight that's

00:07:20,509 --> 00:07:24,550
going to get your data from one point to

00:07:21,710 --> 00:07:26,840
another take a look at nano message

00:07:24,550 --> 00:07:29,810
beyond that there's there's zeromq

00:07:26,840 --> 00:07:31,520
rabbitmq activemq anything with mq and

00:07:29,810 --> 00:07:33,680
the name is really cool and a lot of

00:07:31,520 --> 00:07:36,139
devs are convinced that Redis is a queue

00:07:33,680 --> 00:07:38,919
though it's not i would recommend not

00:07:36,139 --> 00:07:40,940
using that for something high volume

00:07:38,919 --> 00:07:43,310
it's also not a logging pipeline

00:07:40,940 --> 00:07:45,770
Facebook design scribe and there's the

00:07:43,310 --> 00:07:48,740
Apache flume project for application

00:07:45,770 --> 00:07:51,130
pipelining of log data if you're looking

00:07:48,740 --> 00:07:54,169
for something very formalized of very

00:07:51,130 --> 00:07:56,570
particular about the fields and and and

00:07:54,169 --> 00:07:57,860
the ability to enforce those things you

00:07:56,570 --> 00:07:59,330
might want to look at flume inscribe

00:07:57,860 --> 00:08:02,870
that didn't really work too well for us

00:07:59,330 --> 00:08:04,220
and it's not a full logging platform so

00:08:02,870 --> 00:08:05,750
all the work that I'm going to talk to

00:08:04,220 --> 00:08:07,250
you about today is stuff that we did

00:08:05,750 --> 00:08:09,830
before these projects were mature enough

00:08:07,250 --> 00:08:12,169
to be able to handle data at scale oh or

00:08:09,830 --> 00:08:14,360
before they even existed so if I were to

00:08:12,169 --> 00:08:16,759
start this today the first place i would

00:08:14,360 --> 00:08:19,729
start would be looking at hecka which is

00:08:16,759 --> 00:08:22,099
a full logging platform that addresses a

00:08:19,729 --> 00:08:24,889
number of the problems that I had when I

00:08:22,099 --> 00:08:26,720
was evaluating things like log stash if

00:08:24,889 --> 00:08:28,370
hecka didn't fit your infrastructure for

00:08:26,720 --> 00:08:29,840
some reason because it is go and maybe

00:08:28,370 --> 00:08:31,250
you don't have experienced deploying go

00:08:29,840 --> 00:08:33,320
projects and you don't want to add that

00:08:31,250 --> 00:08:36,110
to your infrastructure affluent d is an

00:08:33,320 --> 00:08:37,969
excellent alternative to something like

00:08:36,110 --> 00:08:39,680
hecka though it doesn't quite deal with

00:08:37,969 --> 00:08:42,349
all of the issues that hecka does it

00:08:39,680 --> 00:08:45,050
does do a really great job I put log

00:08:42,349 --> 00:08:47,540
stash up here because I think we owe log

00:08:45,050 --> 00:08:50,000
stash a lot of credit for really making

00:08:47,540 --> 00:08:52,430
this space interesting

00:08:50,000 --> 00:08:55,610
fun and something that we can all get

00:08:52,430 --> 00:08:57,050
behind however I had no luck scaling

00:08:55,610 --> 00:09:00,440
logstash beyond a few hundred machines

00:08:57,050 --> 00:09:02,900
and the talk that the guys that designed

00:09:00,440 --> 00:09:05,090
hecka gave on why they designed hecka

00:09:02,900 --> 00:09:06,380
confirms that things haven't gotten much

00:09:05,090 --> 00:09:09,470
better in a few years that I've been

00:09:06,380 --> 00:09:12,560
working on this project so log stash I

00:09:09,470 --> 00:09:14,960
lots of credit tongue love the work that

00:09:12,560 --> 00:09:17,830
Jordan sisal did on log stash it just it

00:09:14,960 --> 00:09:20,630
could not keep up with our log volumes

00:09:17,830 --> 00:09:22,580
so I would actually argue that syslog is

00:09:20,630 --> 00:09:24,590
web-scale first off it's boring

00:09:22,580 --> 00:09:26,750
technology and we need more boring

00:09:24,590 --> 00:09:29,810
technology we need to simplify the

00:09:26,750 --> 00:09:33,020
endpoints this gives your endpoints more

00:09:29,810 --> 00:09:35,240
CPU more resources to be able to serve

00:09:33,020 --> 00:09:37,580
your customers that's the most important

00:09:35,240 --> 00:09:39,320
thing if you can solve a problem without

00:09:37,580 --> 00:09:41,330
burdening the endpoints and you make

00:09:39,320 --> 00:09:43,490
your application faster you've done your

00:09:41,330 --> 00:09:45,650
job as a sysadmin the other nice thing

00:09:43,490 --> 00:09:47,780
about syslog is that it's easy to

00:09:45,650 --> 00:09:49,790
understand um there are tons of

00:09:47,780 --> 00:09:52,220
resources available online right now for

00:09:49,790 --> 00:09:56,570
configuring syslog us whether syslog-ng

00:09:52,220 --> 00:09:58,250
or rsyslog and anyone from sysadmin to a

00:09:56,570 --> 00:10:00,350
death can understand these documents and

00:09:58,250 --> 00:10:02,060
get things working get things moving

00:10:00,350 --> 00:10:04,880
through the through the logging pipeline

00:10:02,060 --> 00:10:08,930
without burdening your endpoints and

00:10:04,880 --> 00:10:12,740
I've heard pearl isn't wet scale and I

00:10:08,930 --> 00:10:14,360
disagree I built this all in well we as

00:10:12,740 --> 00:10:16,850
a team built this all together and pearl

00:10:14,360 --> 00:10:19,520
with PO as a back-end and it

00:10:16,850 --> 00:10:21,110
outperformed blog stash just out of the

00:10:19,520 --> 00:10:23,050
gate after a day's worth of work I was

00:10:21,110 --> 00:10:25,310
able to outperform log stash in

00:10:23,050 --> 00:10:28,490
consuming a logging stream and

00:10:25,310 --> 00:10:30,290
tokenizing a ton of data right now we're

00:10:28,490 --> 00:10:32,480
doing sixty thousand messages per second

00:10:30,290 --> 00:10:34,220
on each processor node and and that's

00:10:32,480 --> 00:10:36,710
only constrained by elasticsearch

00:10:34,220 --> 00:10:38,270
indexing speed so I have plenty of extra

00:10:36,710 --> 00:10:40,940
resources available to do even more

00:10:38,270 --> 00:10:42,440
message processing and I'm just stuck at

00:10:40,940 --> 00:10:46,430
the number of nodes I have in the

00:10:42,440 --> 00:10:48,320
elastic search cluster yeah we're there

00:10:46,430 --> 00:10:52,420
any questions when any of that before I

00:10:48,320 --> 00:10:52,420
go forward yes

00:10:56,540 --> 00:11:03,210
um I would say if you're looking at the

00:11:01,530 --> 00:11:05,460
access and application logs of anything

00:11:03,210 --> 00:11:07,740
more than a few hundred servers you're

00:11:05,460 --> 00:11:10,110
into the area of you need to think about

00:11:07,740 --> 00:11:13,200
how to scale things and how you're going

00:11:10,110 --> 00:11:15,000
to divide and conquer that problem a few

00:11:13,200 --> 00:11:17,220
hundred servers you can probably do this

00:11:15,000 --> 00:11:20,400
all of this off of a single instance log

00:11:17,220 --> 00:11:22,860
stash box going into you know a three

00:11:20,400 --> 00:11:24,900
node elasticsearch cluster beyond a few

00:11:22,860 --> 00:11:26,460
hundred servers I would say maybe 500

00:11:24,900 --> 00:11:27,780
servers into the thousands you're going

00:11:26,460 --> 00:11:31,410
to have to start adding a little bit of

00:11:27,780 --> 00:11:33,180
complexity I'm so this is a pro

00:11:31,410 --> 00:11:35,580
conference and so I want to talk a

00:11:33,180 --> 00:11:38,190
little bit about parsing log data um if

00:11:35,580 --> 00:11:41,130
we're using syslog we get some stuff for

00:11:38,190 --> 00:11:42,390
free we know when the log happened

00:11:41,130 --> 00:11:45,210
because the timestamp is part of the

00:11:42,390 --> 00:11:46,920
syslog RFC we know which server sent the

00:11:45,210 --> 00:11:49,140
log always use fully qualified domain

00:11:46,920 --> 00:11:51,300
names when your sis logging there's no

00:11:49,140 --> 00:11:54,390
excuse to you short host names we'll

00:11:51,300 --> 00:11:56,760
talk about why in a met in a minute you

00:11:54,390 --> 00:11:58,890
also get the log level and the facility

00:11:56,760 --> 00:12:00,390
so you get whether it was a debug

00:11:58,890 --> 00:12:02,520
through emergency and then you also get

00:12:00,390 --> 00:12:04,920
the the arbitrary facility that that you

00:12:02,520 --> 00:12:06,330
decided to attach to your log um and

00:12:04,920 --> 00:12:08,100
then you get the program that generated

00:12:06,330 --> 00:12:11,010
the logs sometimes most of the times

00:12:08,100 --> 00:12:15,570
depending on the vendor and the content

00:12:11,010 --> 00:12:17,730
of the message so when I when I started

00:12:15,570 --> 00:12:18,990
down the project of looking at logging

00:12:17,730 --> 00:12:21,270
stuff and this was back in two thousand

00:12:18,990 --> 00:12:22,410
seven before i joined booking.com i was

00:12:21,270 --> 00:12:24,480
looking for something that could parse a

00:12:22,410 --> 00:12:26,940
syslog message and give me something

00:12:24,480 --> 00:12:28,920
back and i didn't find anything that was

00:12:26,940 --> 00:12:31,530
very performant on the Sipan so i

00:12:28,920 --> 00:12:33,060
created my own as is the case with

00:12:31,530 --> 00:12:36,240
everything you always create your own

00:12:33,060 --> 00:12:38,280
and the objectives were to create order

00:12:36,240 --> 00:12:39,900
from chaos to take this freeform text

00:12:38,280 --> 00:12:41,820
that's coming in via syslog and

00:12:39,900 --> 00:12:43,920
transform it into a document into it

00:12:41,820 --> 00:12:46,290
into a hash associative array

00:12:43,920 --> 00:12:48,060
whatever you want to call it I wanted it

00:12:46,290 --> 00:12:50,070
to be fast I wanted to be the fastest

00:12:48,060 --> 00:12:52,980
parser own Sipan because I wanted to

00:12:50,070 --> 00:12:56,000
handle high volumes of data and I wanted

00:12:52,980 --> 00:12:59,790
to be able to support vendors that don't

00:12:56,000 --> 00:13:01,410
follow standards um and it was an

00:12:59,790 --> 00:13:04,420
exploration in how much I could tolerate

00:13:01,410 --> 00:13:06,730
from vendors as well which was a hand

00:13:04,420 --> 00:13:09,230
um with what is interesting though is

00:13:06,730 --> 00:13:12,500
cisco actually includes a second date

00:13:09,230 --> 00:13:14,810
stamp in the syslog message after the

00:13:12,500 --> 00:13:17,570
host but before the program and that

00:13:14,810 --> 00:13:19,760
date format is usually ISO 8601 except

00:13:17,570 --> 00:13:21,890
for when it's not and the original

00:13:19,760 --> 00:13:23,570
timestamp is still there and they

00:13:21,890 --> 00:13:25,100
include an extra character at the end of

00:13:23,570 --> 00:13:27,260
the date which will tell you whether or

00:13:25,100 --> 00:13:29,450
not that device is configured to use ntp

00:13:27,260 --> 00:13:32,180
and whether or not the ntp has been able

00:13:29,450 --> 00:13:34,820
to detect successfully update in the

00:13:32,180 --> 00:13:36,440
past few seconds and parse this log line

00:13:34,820 --> 00:13:38,330
understands that and can actually tell

00:13:36,440 --> 00:13:39,950
you if your sis logging if you're

00:13:38,330 --> 00:13:42,140
parsing Cisco data whether or not your

00:13:39,950 --> 00:13:44,240
devices are configured to use ntp and

00:13:42,140 --> 00:13:47,000
whether the ntp has been updated

00:13:44,240 --> 00:13:49,070
properly so there's a lot of people have

00:13:47,000 --> 00:13:51,050
contributed by sending me log messages

00:13:49,070 --> 00:13:52,910
that it that parses logline didn't work

00:13:51,050 --> 00:13:55,430
on so thank you for that because i was

00:13:52,910 --> 00:13:57,860
able to make them work so basically we

00:13:55,430 --> 00:13:59,840
get something like this and it goes into

00:13:57,860 --> 00:14:02,150
something much larger the cool thing

00:13:59,840 --> 00:14:04,970
here is that that wonderful date stamp

00:14:02,150 --> 00:14:07,340
that you get from syslog has been

00:14:04,970 --> 00:14:09,740
transformed into an ID 80 iso 8601

00:14:07,340 --> 00:14:11,330
format which you can then use in a whole

00:14:09,740 --> 00:14:13,160
bunch of other things and transform

00:14:11,330 --> 00:14:15,590
rather easily you'll also notice that it

00:14:13,160 --> 00:14:21,470
got bigger there's a lot more crap here

00:14:15,590 --> 00:14:22,760
and that's okay I don't mind that so

00:14:21,470 --> 00:14:25,490
some of the things that I learned from

00:14:22,760 --> 00:14:26,840
writing this module were well vendors

00:14:25,490 --> 00:14:29,210
never comply with standards they always

00:14:26,840 --> 00:14:31,570
create their own always use cons fast

00:14:29,210 --> 00:14:34,520
never read only p.m. thank you Sawyer

00:14:31,570 --> 00:14:36,950
good rag exes are really fast actually

00:14:34,520 --> 00:14:39,650
faster than BNF parsers in Perl right

00:14:36,950 --> 00:14:41,450
now and untested regular expressions are

00:14:39,650 --> 00:14:43,720
dangerously slow hey how many people

00:14:41,450 --> 00:14:46,280
write regular expressions in this room

00:14:43,720 --> 00:14:47,870
cool how many of you actually test those

00:14:46,280 --> 00:14:50,360
regular expressions to see how fast they

00:14:47,870 --> 00:14:52,580
are cool you guys are doing it right

00:14:50,360 --> 00:14:54,140
I've been doing it wrong for a long time

00:14:52,580 --> 00:14:55,970
this was a project that taught me how to

00:14:54,140 --> 00:14:57,440
do things right um the other thing I

00:14:55,970 --> 00:15:00,170
learned is that pearl doc per Ola

00:14:57,440 --> 00:15:03,680
realize to you sometimes one particular

00:15:00,170 --> 00:15:05,600
instance is the /o does anyone know what

00:15:03,680 --> 00:15:07,190
/ 0 is as a modifier at the end of a

00:15:05,600 --> 00:15:10,330
regular expression can someone tell me

00:15:07,190 --> 00:15:13,640
what it's supposed to do I'm serious

00:15:10,330 --> 00:15:16,030
yeah okay so that's one one one

00:15:13,640 --> 00:15:16,030
definition

00:15:18,070 --> 00:15:22,670
okay that's another definition that I've

00:15:20,480 --> 00:15:24,950
seen and I've also heard and the actual

00:15:22,670 --> 00:15:27,260
I think the current pearl doc pearl re

00:15:24,950 --> 00:15:32,780
just says will introduce bugs into your

00:15:27,260 --> 00:15:34,940
code what which is interesting because

00:15:32,780 --> 00:15:37,670
um I don't know if you guys have used

00:15:34,940 --> 00:15:39,710
dating Conway's weird XP common um but I

00:15:37,670 --> 00:15:41,360
adopted this type of let me define all

00:15:39,710 --> 00:15:42,800
my regular expressions once and then

00:15:41,360 --> 00:15:45,080
write my code and then when I actually

00:15:42,800 --> 00:15:47,570
write the regular expressions I will use

00:15:45,080 --> 00:15:49,160
the named regular expression from the

00:15:47,570 --> 00:15:51,290
regular expression hash so that someone

00:15:49,160 --> 00:15:52,610
reading my code can understand what I'm

00:15:51,290 --> 00:15:54,320
attempting to match their and if they're

00:15:52,610 --> 00:15:55,760
really interested in the nasty guts of

00:15:54,320 --> 00:15:57,470
what I'm trying to match they can go

00:15:55,760 --> 00:15:59,180
back up to the top of the file and see

00:15:57,470 --> 00:16:00,800
the regular expressions however there's

00:15:59,180 --> 00:16:03,350
a little bit of a problem with this so

00:16:00,800 --> 00:16:06,160
here are four identical regular

00:16:03,350 --> 00:16:09,680
expressions running through a loop of

00:16:06,160 --> 00:16:11,930
50,000 and the test data I have a few

00:16:09,680 --> 00:16:13,790
gist's or in github that demonstrate

00:16:11,930 --> 00:16:16,340
this using different variations on all

00:16:13,790 --> 00:16:18,500
kinds of things but there's two million

00:16:16,340 --> 00:16:22,610
entries in each one of these and what's

00:16:18,500 --> 00:16:24,620
really surprising to me is that there's

00:16:22,610 --> 00:16:29,330
a two times performance gain simply by

00:16:24,620 --> 00:16:31,460
adding the / 02 the end of this so using

00:16:29,330 --> 00:16:33,110
this reg ex table thing that kind of

00:16:31,460 --> 00:16:34,970
became common and made my code more

00:16:33,110 --> 00:16:36,830
readable is actually really bad for

00:16:34,970 --> 00:16:39,490
performance and again this is what it

00:16:36,830 --> 00:16:41,990
says in the current pearl dr. REE about

00:16:39,490 --> 00:16:45,440
/o actually in 520 it's not even

00:16:41,990 --> 00:16:47,060
president 522 in the priority the reason

00:16:45,440 --> 00:16:49,070
that you can introduce bugs is because

00:16:47,060 --> 00:16:51,440
this top example is perfectly okay

00:16:49,070 --> 00:16:53,390
because as a gentleman's specified back

00:16:51,440 --> 00:16:56,030
here it only interpolates the variable

00:16:53,390 --> 00:16:57,530
once so for the entire execution of the

00:16:56,030 --> 00:16:59,270
program if that regular expression is

00:16:57,530 --> 00:17:02,330
not going to change this optimization

00:16:59,270 --> 00:17:05,270
works and gets you near to a static

00:17:02,330 --> 00:17:07,730
regular expression performance as its

00:17:05,270 --> 00:17:09,680
supposed to do without /o according to

00:17:07,730 --> 00:17:12,980
everyone that I've talked to that's

00:17:09,680 --> 00:17:15,380
touched the reg ex code however if you

00:17:12,980 --> 00:17:16,699
were to do this particular example what

00:17:15,380 --> 00:17:19,339
you end up with is the first time

00:17:16,699 --> 00:17:20,959
through sets that regular expression for

00:17:19,339 --> 00:17:22,730
the entire execution of your program so

00:17:20,959 --> 00:17:25,730
if you're using variable regular

00:17:22,730 --> 00:17:28,459
expressions in a lookup table then / 0

00:17:25,730 --> 00:17:29,510
is dangerous and will introduce bugs

00:17:28,459 --> 00:17:30,049
into your code but i think this

00:17:29,510 --> 00:17:31,820
distinction

00:17:30,049 --> 00:17:33,440
is worth pointing out and worth noting

00:17:31,820 --> 00:17:37,879
especially if we're going to be doing

00:17:33,440 --> 00:17:41,389
stuff at high speed okay so back to logs

00:17:37,879 --> 00:17:46,639
as documents um does anyone recognize

00:17:41,389 --> 00:17:48,320
this this format it's a little bit yeah

00:17:46,639 --> 00:17:51,460
it's it's a patchy access log format

00:17:48,320 --> 00:17:54,139
does anyone see what's going on here

00:17:51,460 --> 00:17:56,600
yeah someone's trying to inject on our

00:17:54,139 --> 00:17:58,249
website to a file it doesn't exist

00:17:56,600 --> 00:18:00,049
actually they're trying to inject PHP

00:17:58,249 --> 00:18:02,059
into our site we don't have any PHP on

00:18:00,049 --> 00:18:03,320
our front end so that that's an

00:18:02,059 --> 00:18:05,779
indicator for me that there's something

00:18:03,320 --> 00:18:07,909
going sideways here but at the top you

00:18:05,779 --> 00:18:10,759
have the message elements that are

00:18:07,909 --> 00:18:13,519
coming directly from syslog itself on

00:18:10,759 --> 00:18:15,109
the deve facility and priority are

00:18:13,519 --> 00:18:18,230
encoded in the integer at the beginning

00:18:15,109 --> 00:18:20,779
the dates date format in an incredibly

00:18:18,230 --> 00:18:22,389
usable date format especially useful for

00:18:20,779 --> 00:18:24,470
writing tests when you're trying to

00:18:22,389 --> 00:18:26,359
distribute this because there's no year

00:18:24,470 --> 00:18:28,249
so every time the Year rolls over all my

00:18:26,359 --> 00:18:30,350
tests fail so I have to hard code the

00:18:28,249 --> 00:18:32,600
the year on par syslog lines test suite

00:18:30,350 --> 00:18:35,720
and then the fully qualified domain name

00:18:32,600 --> 00:18:37,999
that we have here and and the program

00:18:35,720 --> 00:18:40,369
which we override to be the V host that

00:18:37,999 --> 00:18:41,840
is the determination point and then we

00:18:40,369 --> 00:18:43,580
have a bunch of content this is all just

00:18:41,840 --> 00:18:44,929
content and this is kind of the

00:18:43,580 --> 00:18:46,850
breakdown that you would get out of

00:18:44,929 --> 00:18:50,480
running this data through parsis logline

00:18:46,850 --> 00:18:52,700
those are the only distinctions um we

00:18:50,480 --> 00:18:54,109
can do a little bit better than that so

00:18:52,700 --> 00:18:56,989
if we're using fully qualified domain

00:18:54,109 --> 00:18:59,210
names for our logging entries and in

00:18:56,989 --> 00:19:01,100
syslog I don't know about you but for us

00:18:59,210 --> 00:19:02,600
we encode information in our domain

00:19:01,100 --> 00:19:04,879
names because they're supposed to make

00:19:02,600 --> 00:19:07,070
it more useful for humans so things like

00:19:04,879 --> 00:19:09,950
data center ID and the environment or

00:19:07,070 --> 00:19:11,269
zone that are present in our domain name

00:19:09,950 --> 00:19:13,190
so whether it's production whether its

00:19:11,269 --> 00:19:14,480
development whether it's another zone

00:19:13,190 --> 00:19:16,789
are present in the fully qualified

00:19:14,480 --> 00:19:19,549
domain names as well as the data center

00:19:16,789 --> 00:19:22,039
ID and then again well well known log

00:19:19,549 --> 00:19:23,629
formats are easily tokenized so how many

00:19:22,039 --> 00:19:27,499
people have written an Apache access log

00:19:23,629 --> 00:19:29,749
parser yeah so there's a ton of them you

00:19:27,499 --> 00:19:31,519
can you can easily reuse code to chunk

00:19:29,749 --> 00:19:33,619
that up or you can do something really

00:19:31,519 --> 00:19:35,389
simple and stupid which is awesome just

00:19:33,619 --> 00:19:36,859
look for a curly brace in the syslog

00:19:35,389 --> 00:19:38,480
message and if you see a curly brace

00:19:36,859 --> 00:19:40,220
substring from that point until the end

00:19:38,480 --> 00:19:42,740
curly brace or actually to the end of

00:19:40,220 --> 00:19:43,539
this the line is what I do and if it

00:19:42,740 --> 00:19:46,179
decodes

00:19:43,539 --> 00:19:48,129
then you have a tokenized representation

00:19:46,179 --> 00:19:50,200
of that field already ready and somebody

00:19:48,129 --> 00:19:52,239
can start indexing data directly into

00:19:50,200 --> 00:19:54,999
elasticsearch however they'd like simply

00:19:52,239 --> 00:19:56,679
by sending a syslog message so these are

00:19:54,999 --> 00:19:58,239
some things that you should do all of

00:19:56,679 --> 00:19:59,889
that comes from the log message itself

00:19:58,239 --> 00:20:01,119
here are some things you shouldn't I

00:19:59,889 --> 00:20:02,409
don't like to tell people not to do

00:20:01,119 --> 00:20:04,269
things because there are trade-offs that

00:20:02,409 --> 00:20:07,059
you're making here when you do some of

00:20:04,269 --> 00:20:09,159
these things so for me they don't make

00:20:07,059 --> 00:20:10,090
sense but for you they might be okay but

00:20:09,159 --> 00:20:12,460
you need to know what you're getting

00:20:10,090 --> 00:20:13,840
yourself into so you shouldn't write bad

00:20:12,460 --> 00:20:15,580
regular expressions and that goes

00:20:13,840 --> 00:20:16,840
without saying but you should test those

00:20:15,580 --> 00:20:19,269
regular expressions you should be able

00:20:16,840 --> 00:20:21,820
to turn them on and off and run through

00:20:19,269 --> 00:20:24,220
a benchmarking suite and see did I have

00:20:21,820 --> 00:20:26,139
a statistically significant impact on

00:20:24,220 --> 00:20:27,909
the performance of the program being off

00:20:26,139 --> 00:20:29,639
by one or two percent is not

00:20:27,909 --> 00:20:33,190
statistically significant in the case of

00:20:29,639 --> 00:20:34,779
benchmarking usually so did I decrease

00:20:33,190 --> 00:20:36,820
the performance by thirty percent okay

00:20:34,779 --> 00:20:38,109
baby that's a bad regular expression the

00:20:36,820 --> 00:20:39,789
other thing we found is that calls to

00:20:38,109 --> 00:20:41,139
external sources are incredibly

00:20:39,789 --> 00:20:43,869
expensive especially when you're talking

00:20:41,139 --> 00:20:46,479
about 60,000 to 120,000 log messages per

00:20:43,869 --> 00:20:49,179
second if you're a CP API introduces

00:20:46,479 --> 00:20:51,190
even 100 millisecond lag time into each

00:20:49,179 --> 00:20:52,539
message you're suddenly not able to

00:20:51,190 --> 00:20:55,389
scale the infrastructure anymore and you

00:20:52,539 --> 00:20:57,220
have to multiply your processing farm by

00:20:55,389 --> 00:20:59,440
hundred times some of these things are

00:20:57,220 --> 00:21:01,059
better served at the lookup time right

00:20:59,440 --> 00:21:03,309
so once I have the data in there and I

00:21:01,059 --> 00:21:05,590
want to go back and fetch that data as a

00:21:03,309 --> 00:21:06,909
consumer of it then maybe I can perform

00:21:05,590 --> 00:21:08,470
those things because waiting there's

00:21:06,909 --> 00:21:10,720
extra few seconds isn't going to be a

00:21:08,470 --> 00:21:14,649
big deal for me as a consumer but it

00:21:10,720 --> 00:21:15,940
will be a huge deal for the computers so

00:21:14,649 --> 00:21:17,859
I don't expect you to be able to read

00:21:15,940 --> 00:21:20,429
this but you can this is an example of

00:21:17,859 --> 00:21:23,080
that that previous Apache access log

00:21:20,429 --> 00:21:26,019
after so this is what it was before and

00:21:23,080 --> 00:21:28,029
it got bigger here the main things I

00:21:26,019 --> 00:21:30,220
wanted to point out or that we've

00:21:28,029 --> 00:21:31,899
basically split this into kind of key

00:21:30,220 --> 00:21:33,340
values and we've looked at things that

00:21:31,899 --> 00:21:34,989
are interesting we've taken them out of

00:21:33,340 --> 00:21:36,999
the log stream and made them key value

00:21:34,989 --> 00:21:40,179
pairs so that we can easily reference

00:21:36,999 --> 00:21:44,789
that data later and do it cheaply in our

00:21:40,179 --> 00:21:49,090
data store so that that's kind of cool

00:21:44,789 --> 00:21:51,789
but we can do more these are access logs

00:21:49,090 --> 00:21:53,559
after all so we have URI is available to

00:21:51,789 --> 00:21:56,739
us we know which hosts terminated those

00:21:53,559 --> 00:21:57,940
points we know the full uri string so we

00:21:56,739 --> 00:22:01,480
can actually evaluate

00:21:57,940 --> 00:22:03,789
the the request-uri and break out

00:22:01,480 --> 00:22:06,279
everything that is the path to the file

00:22:03,789 --> 00:22:08,769
so now I can look at pivoting on

00:22:06,279 --> 00:22:10,929
individual pages rather than pages and

00:22:08,769 --> 00:22:13,299
their parameters then I can introspect

00:22:10,929 --> 00:22:15,009
into those parameters take a look at the

00:22:13,299 --> 00:22:16,899
ones that are interesting don't make the

00:22:15,009 --> 00:22:18,399
mistake that I made because you'll learn

00:22:16,899 --> 00:22:20,799
a lot of stuff about the way your site

00:22:18,399 --> 00:22:21,909
works by doing this at first I just said

00:22:20,799 --> 00:22:23,259
okay I'm going to take all the request

00:22:21,909 --> 00:22:26,860
parameters and I'm going to store them

00:22:23,259 --> 00:22:28,600
in a hash in elastic search and i'm just

00:22:26,860 --> 00:22:31,149
going to cram it in there and everything

00:22:28,600 --> 00:22:34,509
will work and did and then i realized

00:22:31,149 --> 00:22:37,149
that we have dynamic keys for our uri

00:22:34,509 --> 00:22:41,950
parameters so i had over two and a half

00:22:37,149 --> 00:22:43,509
million unique keys in in in storage

00:22:41,950 --> 00:22:44,740
which causes massive problems with

00:22:43,509 --> 00:22:46,840
elastic search you don't have two and a

00:22:44,740 --> 00:22:50,590
half million unique keys values are fine

00:22:46,840 --> 00:22:52,269
keys not so much so extract things that

00:22:50,590 --> 00:22:53,409
are interesting for us interesting

00:22:52,269 --> 00:22:56,409
especially from a security perspective

00:22:53,409 --> 00:22:58,870
things like Hotel ID a transaction

00:22:56,409 --> 00:23:00,279
number user identifier ziff those things

00:22:58,870 --> 00:23:02,230
are present we like to pull them out of

00:23:00,279 --> 00:23:04,450
the message stream and put them in as

00:23:02,230 --> 00:23:06,340
their own key value pair in the document

00:23:04,450 --> 00:23:08,919
so that we can pivot and search when

00:23:06,340 --> 00:23:10,149
those unique pairings the other thing we

00:23:08,919 --> 00:23:13,419
did was you saw the sequel injection

00:23:10,149 --> 00:23:17,259
that that was attempted we look at the

00:23:13,419 --> 00:23:21,039
URI string both unenroll and then we

00:23:17,259 --> 00:23:23,100
attempt to your ID code that fool you

00:23:21,039 --> 00:23:25,269
are I string to see if there's any

00:23:23,100 --> 00:23:28,269
common words that we associate with

00:23:25,269 --> 00:23:30,279
attacks and then we also needed geoip

00:23:28,269 --> 00:23:33,580
data so we did that locally from a local

00:23:30,279 --> 00:23:36,159
database not through a web api to save

00:23:33,580 --> 00:23:39,370
some time so what we did was we added

00:23:36,159 --> 00:23:40,600
stuff like this into the document so

00:23:39,370 --> 00:23:44,649
here you can see there's some scoring

00:23:40,600 --> 00:23:48,250
information about how likely this is to

00:23:44,649 --> 00:23:50,500
be some sort of an attack in this case

00:23:48,250 --> 00:23:52,990
you actually get i decided to tokenize

00:23:50,500 --> 00:23:55,539
each individual thing that triggered my

00:23:52,990 --> 00:23:57,779
attack scoring algorithm so that we

00:23:55,539 --> 00:24:00,129
could evaluate the effectiveness of our

00:23:57,779 --> 00:24:02,409
algorithms we were building and then we

00:24:00,129 --> 00:24:04,509
also had the refer so we decided to

00:24:02,409 --> 00:24:05,769
tokenize that and we ran it through the

00:24:04,509 --> 00:24:07,210
same thing that you would run the uri

00:24:05,769 --> 00:24:08,950
encoding through and we pulled out all

00:24:07,210 --> 00:24:10,480
the interesting data out of the refer so

00:24:08,950 --> 00:24:11,200
we get the domain in the file so that we

00:24:10,480 --> 00:24:13,450
can

00:24:11,200 --> 00:24:16,049
at we can pivot on refers which was

00:24:13,450 --> 00:24:18,820
interesting from a security perspective

00:24:16,049 --> 00:24:21,279
so the lessons that I learned from this

00:24:18,820 --> 00:24:23,350
where test the performance of the system

00:24:21,279 --> 00:24:25,690
its key to be able to disable and enable

00:24:23,350 --> 00:24:28,360
new features in your parsing stream to

00:24:25,690 --> 00:24:30,490
understand the impact it has at ed scale

00:24:28,360 --> 00:24:32,649
so you can see that you can keep up with

00:24:30,490 --> 00:24:34,149
the data if you can't test it you

00:24:32,649 --> 00:24:35,649
shouldn't be introducing it because you

00:24:34,149 --> 00:24:38,169
could break something that someone has

00:24:35,649 --> 00:24:40,090
come to rely on I'll work with the data

00:24:38,169 --> 00:24:43,240
you have the NSA is really good at this

00:24:40,090 --> 00:24:46,179
and so can you so there's a lot of data

00:24:43,240 --> 00:24:47,769
in the log stream that you can just take

00:24:46,179 --> 00:24:49,090
advantage of and it's cheap to take

00:24:47,769 --> 00:24:51,100
advantage of what you already have

00:24:49,090 --> 00:24:52,600
rather than going well you know here's

00:24:51,100 --> 00:24:54,549
the short host name when we call out to

00:24:52,600 --> 00:24:56,320
my server inventory system to get the

00:24:54,549 --> 00:24:57,820
rest of the information about it no just

00:24:56,320 --> 00:24:59,740
block the fully qualified domain and

00:24:57,820 --> 00:25:02,380
then chunk it from there and stash that

00:24:59,740 --> 00:25:03,820
you're good and then tokenize the data

00:25:02,380 --> 00:25:07,450
that you're interested in into a set of

00:25:03,820 --> 00:25:11,080
well-defined fields any questions about

00:25:07,450 --> 00:25:15,519
that stuff because now we're getting

00:25:11,080 --> 00:25:18,789
into ELQ um everyone know what ELQ is

00:25:15,519 --> 00:25:22,000
other than this it's the elastic search

00:25:18,789 --> 00:25:23,830
log stash Cabana ecosystem it

00:25:22,000 --> 00:25:27,039
elasticsearch was originally started as

00:25:23,830 --> 00:25:30,159
a distributed free text search engine

00:25:27,039 --> 00:25:32,080
and it did not perform quite as well as

00:25:30,159 --> 00:25:34,570
something like Apache Solr for that use

00:25:32,080 --> 00:25:36,370
case but it did scale very is easily so

00:25:34,570 --> 00:25:37,480
it was adapted by people like me who

00:25:36,370 --> 00:25:41,470
decided to break it and use it for

00:25:37,480 --> 00:25:44,740
logging and elastico acquired log stash

00:25:41,470 --> 00:25:51,090
and cabana and now they have a platform

00:25:44,740 --> 00:25:53,649
for logging and we've broken it a lot so

00:25:51,090 --> 00:25:57,250
these are things that we actually tested

00:25:53,649 --> 00:25:59,309
and broke some of this stuff is advice

00:25:57,250 --> 00:26:01,809
that you will get from reading blogs

00:25:59,309 --> 00:26:05,049
never content with just taking something

00:26:01,809 --> 00:26:06,700
off of a blog I decided to tweak it and

00:26:05,049 --> 00:26:08,950
a few of my coworkers help me test a few

00:26:06,700 --> 00:26:10,330
variations of this you'll hear don't go

00:26:08,950 --> 00:26:11,740
above a 30 gig heap you're not going to

00:26:10,330 --> 00:26:13,840
increase any of your you're not going to

00:26:11,740 --> 00:26:17,080
increase performance confirmed that is

00:26:13,840 --> 00:26:19,600
true we ran a machine with a 60 gig heap

00:26:17,080 --> 00:26:22,659
and we compared it to that of another

00:26:19,600 --> 00:26:24,970
system running to 30 gig heaps and the 2

00:26:22,659 --> 00:26:26,980
30 gig heaps performed twice as good

00:26:24,970 --> 00:26:29,110
as the 160 gig heap so there was no

00:26:26,980 --> 00:26:31,630
performance gain by going over this 30

00:26:29,110 --> 00:26:33,280
gig limit that said leave about

00:26:31,630 --> 00:26:34,630
twenty-five percent of the memory

00:26:33,280 --> 00:26:36,640
available for the file system cache

00:26:34,630 --> 00:26:38,860
elasticsearch there's a lot of i/o and

00:26:36,640 --> 00:26:41,110
having that cash available and hot is

00:26:38,860 --> 00:26:42,700
good if you're using how many are using

00:26:41,110 --> 00:26:45,429
elastic search today in their production

00:26:42,700 --> 00:26:47,970
infrastructure have you guys read the

00:26:45,429 --> 00:26:49,809
call me maybe papers on elastic search

00:26:47,970 --> 00:26:52,059
because they're very interesting they

00:26:49,809 --> 00:26:54,280
talk about the cap performance of

00:26:52,059 --> 00:26:57,669
elastic search so there there's some

00:26:54,280 --> 00:27:00,250
things that you may be surprising um for

00:26:57,669 --> 00:27:01,840
instance in some cases you may have as

00:27:00,250 --> 00:27:03,039
low as ninety six percent of the data

00:27:01,840 --> 00:27:05,820
that you're writing to elasticsearch

00:27:03,039 --> 00:27:08,380
actually ending up on disk in some cases

00:27:05,820 --> 00:27:10,120
and then on the other end when you

00:27:08,380 --> 00:27:12,789
perform a search you may only be hitting

00:27:10,120 --> 00:27:14,440
a certain much smaller percentage of the

00:27:12,789 --> 00:27:16,390
data that you have available these are

00:27:14,440 --> 00:27:18,159
things you need to know so that you can

00:27:16,390 --> 00:27:21,370
make those assumptions and make the

00:27:18,159 --> 00:27:23,950
right calls with stuff um separate your

00:27:21,370 --> 00:27:25,419
master and data nodes just this is this

00:27:23,950 --> 00:27:26,770
is good your data node should never be

00:27:25,419 --> 00:27:28,330
able to elect themselves master of the

00:27:26,770 --> 00:27:30,669
cluster it just works better for

00:27:28,330 --> 00:27:32,200
consensus in the elastic search and then

00:27:30,669 --> 00:27:34,659
namespace index is based on your

00:27:32,200 --> 00:27:36,429
retention needs so it's very expensive

00:27:34,659 --> 00:27:39,580
to delete a document and elastic search

00:27:36,429 --> 00:27:42,100
it's cheap drop in index so that's how

00:27:39,580 --> 00:27:44,890
you should think about designing indexes

00:27:42,100 --> 00:27:46,270
um so in order for you to understand the

00:27:44,890 --> 00:27:47,980
next few things I'm going to have to

00:27:46,270 --> 00:27:50,350
talk to you a little bit about charting

00:27:47,980 --> 00:27:51,909
and the shards and elastic search and

00:27:50,350 --> 00:27:54,429
index is made up of a number of shards

00:27:51,909 --> 00:27:56,710
and data is automatically shard to base

00:27:54,429 --> 00:27:58,419
it automatically shorted based on idea

00:27:56,710 --> 00:28:01,419
unless you specify a different algorithm

00:27:58,419 --> 00:28:03,340
for allocating data but spoiler alert

00:28:01,419 --> 00:28:07,000
most people don't it's sharded by

00:28:03,340 --> 00:28:08,260
automatic uuid generation and so what

00:28:07,000 --> 00:28:09,840
elasticsearch is doing is actually

00:28:08,260 --> 00:28:12,669
providing a real-time wrapper around

00:28:09,840 --> 00:28:14,380
leucine and it does that through this

00:28:12,669 --> 00:28:16,150
this blue piece here called the

00:28:14,380 --> 00:28:21,970
transaction log which is a pre leucine

00:28:16,150 --> 00:28:24,309
real text search pre buffer of data and

00:28:21,970 --> 00:28:27,940
that every once in a while is flushed

00:28:24,309 --> 00:28:29,590
out to leucine into a segment now when a

00:28:27,940 --> 00:28:32,230
segment gets to a certain size it can no

00:28:29,590 --> 00:28:34,960
longer be written to so a new segment is

00:28:32,230 --> 00:28:37,030
spawned and the next transaction log

00:28:34,960 --> 00:28:38,290
will flush to that segment so what you

00:28:37,030 --> 00:28:39,850
end up with is you have

00:28:38,290 --> 00:28:41,740
series of shards and inside of those

00:28:39,850 --> 00:28:43,990
charged you actually have Micro shards

00:28:41,740 --> 00:28:46,420
which are called leucine segments which

00:28:43,990 --> 00:28:49,170
can be a variable size and can be

00:28:46,420 --> 00:28:53,440
completely can duplicate a lot of data

00:28:49,170 --> 00:28:56,200
so this this one shard maybe let's say

00:28:53,440 --> 00:28:58,630
50 gigs of data contained in these

00:28:56,200 --> 00:29:00,550
segments you can run a command to

00:28:58,630 --> 00:29:02,770
optimize those shards which runs what's

00:29:00,550 --> 00:29:05,500
called the merge to merge the segments

00:29:02,770 --> 00:29:07,030
together so that each shard contains a

00:29:05,500 --> 00:29:09,250
fixed number of segments and you can

00:29:07,030 --> 00:29:11,650
specify how many you'd like the default

00:29:09,250 --> 00:29:13,330
is to but most people will set that to

00:29:11,650 --> 00:29:15,100
one and then you end up with something

00:29:13,330 --> 00:29:18,070
like this where you have ones we've seen

00:29:15,100 --> 00:29:20,290
segments fewer segments mean less

00:29:18,070 --> 00:29:23,410
mapping and less reducing and more web

00:29:20,290 --> 00:29:25,480
scale but the problem is that this is a

00:29:23,410 --> 00:29:27,670
very expensive operation and it's done

00:29:25,480 --> 00:29:30,190
is performed mostly in memory so if

00:29:27,670 --> 00:29:32,110
you're going to take a bunch of let's

00:29:30,190 --> 00:29:34,720
say 50 gigs of data and merge it

00:29:32,110 --> 00:29:37,720
together you're doing that at almost ten

00:29:34,720 --> 00:29:40,750
to thirty percent of the disks usage in

00:29:37,720 --> 00:29:42,880
the merge operation so if you're doing a

00:29:40,750 --> 00:29:45,550
lot of logging you cannot afford to

00:29:42,880 --> 00:29:48,090
perform segment maintenance and optimize

00:29:45,550 --> 00:29:49,930
on the nice thing about segment

00:29:48,090 --> 00:29:51,880
optimization though is that the search

00:29:49,930 --> 00:29:55,300
performance is dramatically increased

00:29:51,880 --> 00:29:58,000
when you're down to a single segment and

00:29:55,300 --> 00:30:00,310
so is disk space so a 50 gigs segment

00:29:58,000 --> 00:30:02,110
may migrate may optimize down to

00:30:00,310 --> 00:30:04,810
something like 30 gigs or sometimes

00:30:02,110 --> 00:30:06,760
you'll even get 20 a fool like fifty

00:30:04,810 --> 00:30:10,570
percent compression off of merging

00:30:06,760 --> 00:30:13,450
segments together so here are some

00:30:10,570 --> 00:30:16,450
settings that we use for elastic search

00:30:13,450 --> 00:30:18,070
the process the first one is is

00:30:16,450 --> 00:30:22,660
disabling the memory mapping of the

00:30:18,070 --> 00:30:27,310
shards and using the built-in java neo

00:30:22,660 --> 00:30:30,760
FS a memory map or shared concurrent

00:30:27,310 --> 00:30:34,060
read library the default is to use about

00:30:30,760 --> 00:30:35,890
both memory mapped and a near OFS as

00:30:34,060 --> 00:30:37,480
dependent but we found that elastic

00:30:35,890 --> 00:30:40,180
search is really bad at predicting its

00:30:37,480 --> 00:30:42,070
future memory usage so anything that's

00:30:40,180 --> 00:30:44,590
going to use memory that isn't necessary

00:30:42,070 --> 00:30:47,050
you should disable the second thing is

00:30:44,590 --> 00:30:49,210
the Refresh interval on this is going to

00:30:47,050 --> 00:30:51,350
be the amount of time that a document

00:30:49,210 --> 00:30:54,590
arriving in the indexing

00:30:51,350 --> 00:30:56,480
is held before it is flushed to disk so

00:30:54,590 --> 00:31:00,260
for us we've said we can tolerate 30

00:30:56,480 --> 00:31:02,419
seconds of data in memory so if we lose

00:31:00,260 --> 00:31:06,020
that node we lose 30 seconds the data

00:31:02,419 --> 00:31:07,730
you can tweak this but we're trying to

00:31:06,020 --> 00:31:10,240
get more control over when these

00:31:07,730 --> 00:31:12,679
operations occur so that there's more on

00:31:10,240 --> 00:31:14,990
there more visible and that failures

00:31:12,679 --> 00:31:17,960
don't cascade the second thing is I was

00:31:14,990 --> 00:31:20,990
talking about merging earlier merges

00:31:17,960 --> 00:31:22,340
occur whether you like it or not you

00:31:20,990 --> 00:31:24,230
can't really control that but what you

00:31:22,340 --> 00:31:26,120
can do is tell elasticsearch I only use

00:31:24,230 --> 00:31:28,280
one thread for merges so this will

00:31:26,120 --> 00:31:29,929
prevent you from having on like eight

00:31:28,280 --> 00:31:31,970
concurrent merges at one time competing

00:31:29,929 --> 00:31:35,000
for 30 gigs of memory which is really

00:31:31,970 --> 00:31:36,289
only probably 12 gigs of memory after

00:31:35,000 --> 00:31:39,440
all the housekeeping that elasticsearch

00:31:36,289 --> 00:31:41,990
is doing so we set that down to 11

00:31:39,440 --> 00:31:43,280
thread to prevent that and then the

00:31:41,990 --> 00:31:45,440
other thing is a transaction log I

00:31:43,280 --> 00:31:46,940
talked about this in a previous slide it

00:31:45,440 --> 00:31:48,919
was at blue box it's the pre-listing

00:31:46,940 --> 00:31:50,299
segment we're saying it can get up to 1

00:31:48,919 --> 00:31:52,340
gig large before it needs to be flushed

00:31:50,299 --> 00:31:54,740
out to a leucine segment and then you

00:31:52,340 --> 00:31:55,940
can configure cache sizes of the newer

00:31:54,740 --> 00:31:57,650
versions of elastic search are much

00:31:55,940 --> 00:31:59,960
better at managing their cache sizes

00:31:57,650 --> 00:32:01,820
automatically we had to manually

00:31:59,960 --> 00:32:05,480
configure this quite extensively on

00:32:01,820 --> 00:32:06,919
earlier versions of elastic search this

00:32:05,480 --> 00:32:09,470
is what our index layouts look like um

00:32:06,919 --> 00:32:12,159
so this is for our access log data we've

00:32:09,470 --> 00:32:16,010
split it by data center tag this is

00:32:12,159 --> 00:32:17,990
artifacts from Alaska 20 19 where you

00:32:16,010 --> 00:32:20,659
could not tribe nodes so if you wanted

00:32:17,990 --> 00:32:22,669
to have data two clusters that

00:32:20,659 --> 00:32:24,230
communicated as one living in two

00:32:22,669 --> 00:32:25,820
separate data centers you have to do a

00:32:24,230 --> 00:32:27,950
trick like this and what it says is

00:32:25,820 --> 00:32:30,590
essentially any index that starts with

00:32:27,950 --> 00:32:32,360
this data center tag the shards can only

00:32:30,590 --> 00:32:35,360
be allocated inside of this data center

00:32:32,360 --> 00:32:38,570
so if I try to create one of these these

00:32:35,360 --> 00:32:41,059
a mess for indexes the shards will all

00:32:38,570 --> 00:32:43,909
be allocated to the nodes in AMS for and

00:32:41,059 --> 00:32:46,130
if the AMS for data center can't handle

00:32:43,909 --> 00:32:48,080
those shards then that index isn't

00:32:46,130 --> 00:32:50,030
created it's not migrated automatically

00:32:48,080 --> 00:32:51,409
to another data center this prevents any

00:32:50,030 --> 00:32:53,450
type of communication between the data

00:32:51,409 --> 00:32:56,150
centers and there is one one thing that

00:32:53,450 --> 00:32:58,039
this does one compromise we're making

00:32:56,150 --> 00:33:00,290
here which may not be okay for you but

00:32:58,039 --> 00:33:01,669
is ok for us if the link between the

00:33:00,290 --> 00:33:04,100
data centers goes down we lose

00:33:01,669 --> 00:33:04,640
visibility between two that logging data

00:33:04,100 --> 00:33:06,650
that's in

00:33:04,640 --> 00:33:07,850
the other side of the cluster but for us

00:33:06,650 --> 00:33:09,590
we're like then we have bigger problems

00:33:07,850 --> 00:33:11,300
anyways we're not going to want to look

00:33:09,590 --> 00:33:14,930
at cabana dashboards we know that we're

00:33:11,300 --> 00:33:17,690
screwed so let's let's not replicate

00:33:14,930 --> 00:33:19,880
that data back and forth some things you

00:33:17,690 --> 00:33:21,470
can do to optimize on the indexes I'm

00:33:19,880 --> 00:33:23,300
split them into functional namespaces

00:33:21,470 --> 00:33:25,070
already mentioned this based on

00:33:23,300 --> 00:33:26,720
retention periods is the most easy way

00:33:25,070 --> 00:33:28,760
like I'm going to need this data I'm

00:33:26,720 --> 00:33:31,100
going to want it for six months okay

00:33:28,760 --> 00:33:32,870
cool I only need this data first 30 days

00:33:31,100 --> 00:33:35,750
cool then there's are two separate index

00:33:32,870 --> 00:33:39,020
names that that way you can delete them

00:33:35,750 --> 00:33:40,610
very simply I'm use dynamic mappings

00:33:39,020 --> 00:33:42,380
these are schemas for your scheme Ellis

00:33:40,610 --> 00:33:44,300
data I'm sorry to admit there is no

00:33:42,380 --> 00:33:46,670
panacea of schema list data if your data

00:33:44,300 --> 00:33:48,890
is schema-less elasticsearch will

00:33:46,670 --> 00:33:50,330
attempt to do some black magic which

00:33:48,890 --> 00:33:52,550
means that the first document that go

00:33:50,330 --> 00:33:55,550
comes in with a new field it will guess

00:33:52,550 --> 00:33:57,170
the type and from that point one every

00:33:55,550 --> 00:33:58,670
other document must meet that type

00:33:57,170 --> 00:34:01,880
constraint for that field or that

00:33:58,670 --> 00:34:03,950
document is discarded so use dynamic

00:34:01,880 --> 00:34:06,290
mappings to tell it what you expect

00:34:03,950 --> 00:34:07,760
fields to be weather and if you expect

00:34:06,290 --> 00:34:09,770
them to just be strings you can tell it

00:34:07,760 --> 00:34:11,929
it's just a string I don't care there's

00:34:09,770 --> 00:34:13,429
an auto expand replicas feature I'm even

00:34:11,929 --> 00:34:15,110
sorry to have to mention it don't ever

00:34:13,429 --> 00:34:18,649
look for it don't look it up don't use

00:34:15,110 --> 00:34:21,169
it it's it's bad and of course fewer

00:34:18,649 --> 00:34:23,750
replicas mean better write performance

00:34:21,169 --> 00:34:26,480
because Rev rights are actually

00:34:23,750 --> 00:34:29,360
replicated without any type of binary

00:34:26,480 --> 00:34:31,310
optimization so rights are passed to all

00:34:29,360 --> 00:34:33,649
the nodes that need those rights and

00:34:31,310 --> 00:34:36,919
those binary operations are performed on

00:34:33,649 --> 00:34:39,919
each node individually elasticsearch

00:34:36,919 --> 00:34:42,560
cheats okay um its source as much stuff

00:34:39,919 --> 00:34:44,659
about your data in memory as possible so

00:34:42,560 --> 00:34:46,040
here what I learned and this is a really

00:34:44,659 --> 00:34:47,690
interesting because you to know this but

00:34:46,040 --> 00:34:49,520
every document inside of your instance

00:34:47,690 --> 00:34:51,560
uses eight bytes of memory so if you

00:34:49,520 --> 00:34:53,690
have two billion documents on a node

00:34:51,560 --> 00:34:55,429
you're using a significant amount of

00:34:53,690 --> 00:34:58,160
memory just to have those documents

00:34:55,429 --> 00:34:59,990
there why because the idea of the

00:34:58,160 --> 00:35:02,300
document the timestamp of the document

00:34:59,990 --> 00:35:04,670
and the shard the location one disk of

00:35:02,300 --> 00:35:07,010
the document needs to be indexed so that

00:35:04,670 --> 00:35:09,320
your data can be retrieved fast so

00:35:07,010 --> 00:35:11,990
elasticsearch just happily keeps it in

00:35:09,320 --> 00:35:13,910
memory for you um analyzing fields is

00:35:11,990 --> 00:35:17,240
expensive if anyone's work with leucine

00:35:13,910 --> 00:35:18,320
anyone leucine analyzing stuff you can

00:35:17,240 --> 00:35:20,870
do all kinds of really cool

00:35:18,320 --> 00:35:22,940
stuff with things right I don't ever

00:35:20,870 --> 00:35:24,590
need that with logs my blogs talked to

00:35:22,940 --> 00:35:26,090
me in their native language I just need

00:35:24,590 --> 00:35:28,730
the representations of those so all of

00:35:26,090 --> 00:35:30,470
my fields are stored unanalyzed in

00:35:28,730 --> 00:35:32,090
elasticsearch except for one and that is

00:35:30,470 --> 00:35:35,030
the full message string which we

00:35:32,090 --> 00:35:37,670
tokenize just based on white space so

00:35:35,030 --> 00:35:39,380
that we can we can search it um it's

00:35:37,670 --> 00:35:40,730
really important and I already mention

00:35:39,380 --> 00:35:42,260
this and again it's another one of those

00:35:40,730 --> 00:35:45,920
things that needs to be driven home

00:35:42,260 --> 00:35:48,470
agree on a name field set for your

00:35:45,920 --> 00:35:50,840
documents okay if you want some guidance

00:35:48,470 --> 00:35:53,330
there's um there's about there's six of

00:35:50,840 --> 00:35:55,430
them right there there's about 40 or 50

00:35:53,330 --> 00:35:58,550
of them and the best thing to do is just

00:35:55,430 --> 00:35:59,450
simply invent your own you're going to

00:35:58,550 --> 00:36:01,250
need to monitor and maintain

00:35:59,450 --> 00:36:03,560
elasticsearch it doesn't do this by

00:36:01,250 --> 00:36:06,110
itself delete old indexes that you're

00:36:03,560 --> 00:36:08,690
not using anymore degrade replicas so

00:36:06,110 --> 00:36:10,160
basically what we say is over time the

00:36:08,690 --> 00:36:12,050
index has become less important to us so

00:36:10,160 --> 00:36:13,670
we can tolerate more failure so we

00:36:12,050 --> 00:36:17,000
degrade the number of replicas in our

00:36:13,670 --> 00:36:18,740
indexes as time moves forward that that

00:36:17,000 --> 00:36:20,720
frees up documents right so every

00:36:18,740 --> 00:36:22,340
replica is also counting against your

00:36:20,720 --> 00:36:26,120
document count in that eight bytes of

00:36:22,340 --> 00:36:27,650
memory so we age those out clothes on

00:36:26,120 --> 00:36:29,150
used indexes if you're only searching

00:36:27,650 --> 00:36:30,650
back a week if not you per cent of your

00:36:29,150 --> 00:36:32,300
searches are over just a week of data

00:36:30,650 --> 00:36:34,430
then maybe just close everything that's

00:36:32,300 --> 00:36:36,380
30 days or older opening and index is a

00:36:34,430 --> 00:36:38,330
very cheap operation it takes a second

00:36:36,380 --> 00:36:41,150
to open an index in our cluster and we

00:36:38,330 --> 00:36:43,370
have thousands of shards so you could do

00:36:41,150 --> 00:36:45,530
this very easily I'm disabled bloom

00:36:43,370 --> 00:36:47,690
filters on indexes that are no longer

00:36:45,530 --> 00:36:49,160
being written to they consume a lot of

00:36:47,690 --> 00:36:51,230
memory just to tell elasticsearch would

00:36:49,160 --> 00:36:53,180
already exist which fields exist and the

00:36:51,230 --> 00:36:56,240
cardinality of those fields so it knows

00:36:53,180 --> 00:36:58,820
how to optimize queries in for writing

00:36:56,240 --> 00:37:00,680
so you can disable that once you're done

00:36:58,820 --> 00:37:02,870
writing to it and then monitor key

00:37:00,680 --> 00:37:04,280
metrics you don't have to do this

00:37:02,870 --> 00:37:06,380
yourself we've written a series of

00:37:04,280 --> 00:37:09,170
utilities to handle all of these things

00:37:06,380 --> 00:37:11,720
based on the the way that we lay out

00:37:09,170 --> 00:37:13,280
indexes but these also work with any any

00:37:11,720 --> 00:37:16,010
format of indexes whether it's daily

00:37:13,280 --> 00:37:19,490
hourly monthly weekly whatever you're

00:37:16,010 --> 00:37:20,840
going to use the script here is

00:37:19,490 --> 00:37:23,330
basically saying close anything that's

00:37:20,840 --> 00:37:25,520
90 days or older that looks like a log

00:37:23,330 --> 00:37:28,310
stash index delete anything that's a

00:37:25,520 --> 00:37:32,090
hundred and 880 days or older and then

00:37:28,310 --> 00:37:35,030
set the conform to zero

00:37:32,090 --> 00:37:37,430
replicas after seven days with a max

00:37:35,030 --> 00:37:40,280
number of replicas of two so over time

00:37:37,430 --> 00:37:41,930
the the replicas are degraded and at

00:37:40,280 --> 00:37:43,850
seven days we're down two replicas men

00:37:41,930 --> 00:37:45,380
which is zero so we only have the

00:37:43,850 --> 00:37:50,450
primaries available for searching after

00:37:45,380 --> 00:37:52,310
seven days um we also like to optimize

00:37:50,450 --> 00:37:54,620
stuff for programming and making things

00:37:52,310 --> 00:37:56,780
easy for programmers to do so we have a

00:37:54,620 --> 00:37:58,760
way to control aliases that link up to

00:37:56,780 --> 00:38:00,590
the indexes so here the first one just

00:37:58,760 --> 00:38:02,450
basically takes you can see the pattern

00:38:00,590 --> 00:38:04,070
the star at the end at the beginning

00:38:02,450 --> 00:38:07,880
here that's to match anything that comes

00:38:04,070 --> 00:38:11,060
before so if it's a mess for lhr for and

00:38:07,880 --> 00:38:13,130
then alias is it too logstash date so

00:38:11,060 --> 00:38:15,440
now when you search logstash date you'll

00:38:13,130 --> 00:38:16,850
search every index all the data centers

00:38:15,440 --> 00:38:18,290
that are available and currently

00:38:16,850 --> 00:38:20,870
reporting that they have an alias

00:38:18,290 --> 00:38:24,320
available for that then we also do a

00:38:20,870 --> 00:38:27,020
relative so that we can say I've go all

00:38:24,320 --> 00:38:29,660
the way back seven days till today and

00:38:27,020 --> 00:38:30,980
then tag it as log stash weekly so we

00:38:29,660 --> 00:38:32,810
can write scripts against the weekly

00:38:30,980 --> 00:38:35,150
indexes and we never have to compute

00:38:32,810 --> 00:38:37,430
what the weekly index actually is in the

00:38:35,150 --> 00:38:39,560
script it makes it just it's a lazy hack

00:38:37,430 --> 00:38:42,260
for something that we can do very easily

00:38:39,560 --> 00:38:44,750
so basically what that looks like is you

00:38:42,260 --> 00:38:46,990
have these indexes they both have the

00:38:44,750 --> 00:38:49,970
same alias so when you search access

00:38:46,990 --> 00:38:52,700
2014 10 28 which this generated this a

00:38:49,970 --> 00:38:56,140
while ago um you will receive all the

00:38:52,700 --> 00:38:59,450
results from both of those indexes

00:38:56,140 --> 00:39:01,640
elasticsearch has a monitoring suite

00:38:59,450 --> 00:39:03,830
called marvel it uses elasticsearch to

00:39:01,640 --> 00:39:05,030
monitor elasticsearch doing problem with

00:39:03,830 --> 00:39:06,890
that is if your elastic search isn't

00:39:05,030 --> 00:39:08,180
working you're monitoring also isn't

00:39:06,890 --> 00:39:10,160
working and that's not really that

00:39:08,180 --> 00:39:11,480
helpful so to get around this what they

00:39:10,160 --> 00:39:13,130
should recommend is to build a separate

00:39:11,480 --> 00:39:15,920
cluster to monitor your elastic search

00:39:13,130 --> 00:39:17,360
clusters with Marvel I prefer just using

00:39:15,920 --> 00:39:19,520
something like graphite because a lot of

00:39:17,360 --> 00:39:21,500
this is just time series data anyways so

00:39:19,520 --> 00:39:23,600
we wrote a script that basically just

00:39:21,500 --> 00:39:26,360
scans the the full elastic search stats

00:39:23,600 --> 00:39:28,700
API and and dumps all of that data into

00:39:26,360 --> 00:39:31,970
graphite for you so you this is all you

00:39:28,700 --> 00:39:36,920
have to run if and you have data going

00:39:31,970 --> 00:39:40,190
into graphite okay ten minutes to do

00:39:36,920 --> 00:39:41,810
this using logs on this is that last

00:39:40,190 --> 00:39:43,160
part that I said is really important

00:39:41,810 --> 00:39:45,560
because now you got everything there

00:39:43,160 --> 00:39:45,800
you're happy how do you how do you start

00:39:45,560 --> 00:39:47,810
using

00:39:45,800 --> 00:39:49,310
data so if you haven't seen component

00:39:47,810 --> 00:39:53,060
before Alyssa's cabana here's an

00:39:49,310 --> 00:39:56,240
overview of our web logs the others

00:39:53,060 --> 00:39:59,510
break down by the V host so which sites

00:39:56,240 --> 00:40:00,980
and then the access by country and then

00:39:59,510 --> 00:40:02,840
over there you see that web attack score

00:40:00,980 --> 00:40:04,820
which gives us an indication if someone

00:40:02,840 --> 00:40:07,100
scanning us there's an overview of the

00:40:04,820 --> 00:40:08,360
web attacks that occurred and I

00:40:07,100 --> 00:40:10,520
intentionally didn't blur those ip's

00:40:08,360 --> 00:40:13,220
because those guys aren't nice so I'm

00:40:10,520 --> 00:40:14,450
not going to be nice on the top the top

00:40:13,220 --> 00:40:16,000
is the number of incidents that are

00:40:14,450 --> 00:40:18,620
occurring so these are requests with

00:40:16,000 --> 00:40:22,430
suspicious-looking characters in them

00:40:18,620 --> 00:40:24,470
and then the bottom one is the total

00:40:22,430 --> 00:40:26,210
score of all of those requests over time

00:40:24,470 --> 00:40:28,130
which is why those who look a little bit

00:40:26,210 --> 00:40:31,850
different over here you have the vhosts

00:40:28,130 --> 00:40:33,800
that are being attacked and then the

00:40:31,850 --> 00:40:35,810
source IP by the top score by the the

00:40:33,800 --> 00:40:38,020
cumulative score and then countries and

00:40:35,810 --> 00:40:40,550
you can see the u.s. is really malicious

00:40:38,020 --> 00:40:41,810
and then at the bottom there's a table

00:40:40,550 --> 00:40:44,570
where you can actually investigate all

00:40:41,810 --> 00:40:46,550
these little things we've been trying to

00:40:44,570 --> 00:40:49,550
convince devs not to log devs and system

00:40:46,550 --> 00:40:52,670
ins not to log in to end points so we

00:40:49,550 --> 00:40:56,600
built a siege wall of shame on so the

00:40:52,670 --> 00:40:58,250
top the top is by user name right so you

00:40:56,600 --> 00:41:01,910
see that yellow line right there that is

00:40:58,250 --> 00:41:04,190
a single user using sudo in a while true

00:41:01,910 --> 00:41:07,670
loop okay and this is actually very

00:41:04,190 --> 00:41:09,560
common in our infrastructure and the

00:41:07,670 --> 00:41:11,330
nice thing about a page like this is

00:41:09,560 --> 00:41:13,340
that I can talk to someone like the CIO

00:41:11,330 --> 00:41:15,770
and pull this page up and show him

00:41:13,340 --> 00:41:17,750
here's how people are using abusing our

00:41:15,770 --> 00:41:20,270
systems do you think that this is okay

00:41:17,750 --> 00:41:22,070
and then he can go no I don't think it's

00:41:20,270 --> 00:41:23,540
okay and then when he says don't do this

00:41:22,070 --> 00:41:25,580
it comes with a little bit more weight

00:41:23,540 --> 00:41:26,750
than when I say don't do this and the

00:41:25,580 --> 00:41:28,790
other thing you'll notice is that that

00:41:26,750 --> 00:41:31,280
yellow line is probably representative

00:41:28,790 --> 00:41:32,870
of lsm I which you probably can't seeks

00:41:31,280 --> 00:41:35,000
us a little bit blurred but basically

00:41:32,870 --> 00:41:37,760
this this dev is monitoring the network

00:41:35,000 --> 00:41:39,500
performance using lsi the side effect

00:41:37,760 --> 00:41:41,780
the the Heisenberg principle here is

00:41:39,500 --> 00:41:43,910
that monitoring the network performance

00:41:41,780 --> 00:41:45,320
using Ellis of I actually affects

00:41:43,910 --> 00:41:46,310
negatively affects the network

00:41:45,320 --> 00:41:47,930
performance of the device that you're

00:41:46,310 --> 00:41:50,510
monitoring because it has to read those

00:41:47,930 --> 00:41:52,430
statistics and pause what it's doing so

00:41:50,510 --> 00:41:56,270
you end up with you know dev actually

00:41:52,430 --> 00:41:57,830
hurting a production system when I was

00:41:56,270 --> 00:41:59,670
putting the other dashboards to show you

00:41:57,830 --> 00:42:01,829
i came across stuff that i didn't

00:41:59,670 --> 00:42:03,329
no existed someone found this and

00:42:01,829 --> 00:42:06,599
started making dashboards to figure out

00:42:03,329 --> 00:42:10,040
who's abusing graphite so this is this

00:42:06,599 --> 00:42:12,180
is these are graphs of of different

00:42:10,040 --> 00:42:13,349
different queries that are taking more

00:42:12,180 --> 00:42:16,200
than one and a half second for the

00:42:13,349 --> 00:42:19,230
render API to return and then down below

00:42:16,200 --> 00:42:20,609
he's got the host names that are making

00:42:19,230 --> 00:42:22,349
those calls so that he can track those

00:42:20,609 --> 00:42:25,559
people down and smack them with a ruler

00:42:22,349 --> 00:42:28,349
I mean another dev decided he wanted to

00:42:25,559 --> 00:42:29,579
see exactly what the slowest pages were

00:42:28,349 --> 00:42:31,440
using the information is coming from

00:42:29,579 --> 00:42:34,200
engine X and X gives us an upstream in

00:42:31,440 --> 00:42:38,849
and request milliseconds per page so

00:42:34,200 --> 00:42:41,880
here a group did by role so you see

00:42:38,849 --> 00:42:44,700
double I think there's like our www site

00:42:41,880 --> 00:42:46,349
is one of the lines and then our admin

00:42:44,700 --> 00:42:48,299
site which is for hotels is another line

00:42:46,349 --> 00:42:49,530
and then a few other internal sites i

00:42:48,299 --> 00:42:50,940
don't remember exactly which ones they

00:42:49,530 --> 00:42:53,010
were and I blurred them out for for

00:42:50,940 --> 00:42:56,579
safety and then you can actually get a

00:42:53,010 --> 00:42:57,720
breakdown of those pages over here where

00:42:56,579 --> 00:42:59,970
it's all blurred out because I'm not

00:42:57,720 --> 00:43:01,500
disclosing that information is the the

00:42:59,970 --> 00:43:02,910
pages that were really slow to load and

00:43:01,500 --> 00:43:04,680
then on the other side is actually the

00:43:02,910 --> 00:43:06,780
source IP country so you can find really

00:43:04,680 --> 00:43:07,829
interesting bugs like for instance um we

00:43:06,780 --> 00:43:09,750
had a bug in the right to left

00:43:07,829 --> 00:43:11,490
processing and that took longer to

00:43:09,750 --> 00:43:13,109
display and so countries that use

00:43:11,490 --> 00:43:16,079
right-to-left languages showed up in

00:43:13,109 --> 00:43:17,930
this page as taking longer than you know

00:43:16,079 --> 00:43:20,670
a significant amount of time longer than

00:43:17,930 --> 00:43:22,680
comparable left right languages so

00:43:20,670 --> 00:43:25,349
that's kind of neat but I'm a

00:43:22,680 --> 00:43:28,950
command-line junkie um and I really

00:43:25,349 --> 00:43:31,230
liked will Stevenson's talk yesterday on

00:43:28,950 --> 00:43:34,530
machine learning and he talked about how

00:43:31,230 --> 00:43:37,020
um this is an exploratory process it's

00:43:34,530 --> 00:43:38,460
not there's not like a an algorithm you

00:43:37,020 --> 00:43:40,650
apply you need to know what you're

00:43:38,460 --> 00:43:42,210
trying to do before you can do it you

00:43:40,650 --> 00:43:45,000
can't just have something learn to do

00:43:42,210 --> 00:43:47,400
what you're doing I'm so part of our

00:43:45,000 --> 00:43:49,020
investigations we need to be able to do

00:43:47,400 --> 00:43:50,790
things that we couldn't easily do in

00:43:49,020 --> 00:43:52,890
cabana and we need to them fast and we

00:43:50,790 --> 00:43:55,460
need to pipe them different places so we

00:43:52,890 --> 00:43:57,869
created this utility in it's also in a

00:43:55,460 --> 00:43:59,849
circle a search utilities called es r

00:43:57,869 --> 00:44:01,980
sub PL which allows you to do searches

00:43:59,849 --> 00:44:03,660
this is leucine query syntax up top

00:44:01,980 --> 00:44:07,410
there's actually a bug in this query

00:44:03,660 --> 00:44:09,270
syntax on and the and is lowercase so if

00:44:07,410 --> 00:44:11,309
you do and lowercase in the lucene

00:44:09,270 --> 00:44:13,480
queries syntax you basically are doing

00:44:11,309 --> 00:44:15,820
in or across all of your things with

00:44:13,480 --> 00:44:16,690
and included as one of the or statements

00:44:15,820 --> 00:44:18,490
and you don't get back what you're

00:44:16,690 --> 00:44:20,920
expecting so what I do is I actually

00:44:18,490 --> 00:44:22,810
parse the the query that you're sending

00:44:20,920 --> 00:44:24,400
and look for things that are common

00:44:22,810 --> 00:44:27,210
mistakes like lower case and lower case

00:44:24,400 --> 00:44:30,609
or lowercase not and translate them into

00:44:27,210 --> 00:44:32,440
their their equivalent syntax and so

00:44:30,609 --> 00:44:35,710
here you can see that this is just

00:44:32,440 --> 00:44:40,330
showing us some 404s on the site with

00:44:35,710 --> 00:44:42,580
the source IP and the actual file on you

00:44:40,330 --> 00:44:45,130
can then facet or aggregate that data

00:44:42,580 --> 00:44:48,100
very easily you just say you know show

00:44:45,130 --> 00:44:50,590
me the top source IPS from dub dub dub

00:44:48,100 --> 00:44:52,630
booking calm and you get this data back

00:44:50,590 --> 00:44:54,070
this works across multiple days so the

00:44:52,630 --> 00:44:56,350
way that I designed es searches it works

00:44:54,070 --> 00:44:57,910
over a time period of days so it'll

00:44:56,350 --> 00:44:59,350
perform a search across one day and then

00:44:57,910 --> 00:45:02,350
it will go to the next day and then to

00:44:59,350 --> 00:45:03,940
the next day so what you see is a

00:45:02,350 --> 00:45:05,470
duplicated thing here because it's the

00:45:03,940 --> 00:45:06,940
only going across one day but you get an

00:45:05,470 --> 00:45:10,240
aggregate at the bottom of all of the

00:45:06,940 --> 00:45:14,560
data that you got back not super

00:45:10,240 --> 00:45:16,690
interesting those are all Google BOTS so

00:45:14,560 --> 00:45:18,010
then with the aggregations that they

00:45:16,690 --> 00:45:19,869
added in elasticsearch I want to be able

00:45:18,010 --> 00:45:22,390
to do cool stuff like show me the top

00:45:19,869 --> 00:45:24,460
file by the number of distinct source IP

00:45:22,390 --> 00:45:25,960
countries that have accessed it and of

00:45:24,460 --> 00:45:28,600
course it is what you would expect it to

00:45:25,960 --> 00:45:31,390
be it's / is their most commonly visited

00:45:28,600 --> 00:45:32,560
file across all the country codes but

00:45:31,390 --> 00:45:34,210
then you can see something interesting

00:45:32,560 --> 00:45:35,859
happens here there's a there's

00:45:34,210 --> 00:45:37,660
decoration of about three country codes

00:45:35,859 --> 00:45:40,090
from the first one to the second one and

00:45:37,660 --> 00:45:42,730
what that actually is is the the match

00:45:40,090 --> 00:45:46,150
mine geoip tag one of them is a one and

00:45:42,730 --> 00:45:48,609
A one means anonymous proxy and in this

00:45:46,150 --> 00:45:50,530
case these are BOTS that are not making

00:45:48,609 --> 00:45:52,420
the asynchronous JavaScript requests

00:45:50,530 --> 00:45:54,190
that the rest of the site the rest of

00:45:52,420 --> 00:45:55,869
the normal users would be making so a

00:45:54,190 --> 00:45:58,750
one is one of the representations that

00:45:55,869 --> 00:46:00,880
is not that is in the first group but

00:45:58,750 --> 00:46:03,940
not in the second group so there's kind

00:46:00,880 --> 00:46:06,700
of like weird stuff in there and that's

00:46:03,940 --> 00:46:09,100
cool but um I want to be able to teach a

00:46:06,700 --> 00:46:11,530
nice things together and do on you know

00:46:09,100 --> 00:46:14,260
one query leads to the next query leads

00:46:11,530 --> 00:46:16,270
to the next query and eventually it's 12

00:46:14,260 --> 00:46:17,470
hours later and you haven't slept and

00:46:16,270 --> 00:46:18,910
everything is awesome you have

00:46:17,470 --> 00:46:22,359
discovered the secret to the universe

00:46:18,910 --> 00:46:24,580
I'm so here um I built another utility

00:46:22,359 --> 00:46:26,500
which Apple a search utilities users

00:46:24,580 --> 00:46:26,830
called CLI helpers to handle input and

00:46:26,500 --> 00:46:29,650
output

00:46:26,830 --> 00:46:31,570
to the terminal and one of the options I

00:46:29,650 --> 00:46:33,250
CLI helpers allows you to do is specify

00:46:31,570 --> 00:46:36,250
a data file and then anytime you call

00:46:33,250 --> 00:46:37,840
output you can tag that line is data and

00:46:36,250 --> 00:46:39,580
that line will go out to the data file

00:46:37,840 --> 00:46:41,650
in addition to going out to wherever

00:46:39,580 --> 00:46:43,150
else it's supposed to so this does what

00:46:41,650 --> 00:46:45,370
you would expect it to do it looks at

00:46:43,150 --> 00:46:47,920
the top source IP s by attack score the

00:46:45,370 --> 00:46:49,360
output comes to the screen as normal but

00:46:47,920 --> 00:46:52,090
it's also written to that top attackers

00:46:49,360 --> 00:46:55,270
at DAT file so then in an xquery what i

00:46:52,090 --> 00:46:57,550
do is i say a source IP get it from top

00:46:55,270 --> 00:47:00,040
attackers that dat the last column of

00:46:57,550 --> 00:47:03,010
that file so any file that ends in CSV

00:47:00,040 --> 00:47:04,420
txt or dat and I'm turning this into a

00:47:03,010 --> 00:47:09,040
plug-in system so you could easily do

00:47:04,420 --> 00:47:11,020
xls in here as well convert that into a

00:47:09,040 --> 00:47:14,560
terms query and use that as my search

00:47:11,020 --> 00:47:16,720
criteria and then show these fields and

00:47:14,560 --> 00:47:19,210
and sort by that attack score so what

00:47:16,720 --> 00:47:20,170
you end up with is these documents here

00:47:19,210 --> 00:47:21,370
and I've highlighted some of these

00:47:20,170 --> 00:47:23,680
fields just so that you can see there

00:47:21,370 --> 00:47:25,090
are three documents here but then the

00:47:23,680 --> 00:47:27,640
terms query is automatically generated

00:47:25,090 --> 00:47:29,950
from the contents of the file so now I

00:47:27,640 --> 00:47:34,230
can take this query I'll put it to a

00:47:29,950 --> 00:47:34,230
data file and continue my investigation

00:47:35,640 --> 00:47:38,970

YouTube URL: https://www.youtube.com/watch?v=6gXxBgGEv_I


