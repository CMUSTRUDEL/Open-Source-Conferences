Title: John McDonald HPCI manage cluster cloud computing
Publication date: 2015-06-16
Playlist: YAPC::NA 2015
Description: 
	
Captions: 
	00:00:00,000 --> 00:00:07,379
okay I'm John McDonald we've already

00:00:02,970 --> 00:00:10,950
moved forward and I'm talking about HPC

00:00:07,379 --> 00:00:15,269
I which is a new module in Sipan for

00:00:10,950 --> 00:00:17,210
high-performance computing and I'll give

00:00:15,269 --> 00:00:22,160
a bit of an introduction go over

00:00:17,210 --> 00:00:26,760
components how you use it status future

00:00:22,160 --> 00:00:29,570
so when you write large programs they

00:00:26,760 --> 00:00:32,099
tend to run on many computers and

00:00:29,570 --> 00:00:36,350
there's a lot of different ways of doing

00:00:32,099 --> 00:00:40,320
it there's clusters clouds

00:00:36,350 --> 00:00:41,820
multiprocessors and for hpc I i call

00:00:40,320 --> 00:00:45,570
them all a cluster just because that's

00:00:41,820 --> 00:00:47,610
what I started with the only reason a

00:00:45,570 --> 00:00:50,489
program that wants to run on many

00:00:47,610 --> 00:00:54,180
computers worries too much but the

00:00:50,489 --> 00:00:57,449
cluster is to get the job done I want to

00:00:54,180 --> 00:01:01,590
run all of these jobs and distribute

00:00:57,449 --> 00:01:04,860
them please each cluster type though has

00:01:01,590 --> 00:01:08,130
its own interface software they're all

00:01:04,860 --> 00:01:11,220
different so if you write your program

00:01:08,130 --> 00:01:12,960
to run on one you can't move it to a

00:01:11,220 --> 00:01:17,369
different kind of kind of distributed

00:01:12,960 --> 00:01:19,799
system very easily now clusters are

00:01:17,369 --> 00:01:22,140
defined more than anything just by that

00:01:19,799 --> 00:01:24,689
interface software under the hood

00:01:22,140 --> 00:01:27,330
there's they're usually Linux or

00:01:24,689 --> 00:01:31,470
sometimes BSD or sometimes other things

00:01:27,330 --> 00:01:36,450
but to a large extent you just call the

00:01:31,470 --> 00:01:38,790
interface and do things so what HPC I is

00:01:36,450 --> 00:01:41,640
to fight is set up to do is to provide

00:01:38,790 --> 00:01:44,549
an abstract interface that you can use

00:01:41,640 --> 00:01:48,420
for any kind of cluster you write your

00:01:44,549 --> 00:01:54,090
program once to act on this type on the

00:01:48,420 --> 00:01:57,540
HPC I cluster and one of the parameters

00:01:54,090 --> 00:02:02,159
is what kind of cluster do i really want

00:01:57,540 --> 00:02:05,369
to use this time but you can change the

00:02:02,159 --> 00:02:09,000
parameter to go on a different kind of

00:02:05,369 --> 00:02:10,440
cluster now you can edit the program

00:02:09,000 --> 00:02:12,290
texture you can make it a command line

00:02:10,440 --> 00:02:16,320
parameters or whatever

00:02:12,290 --> 00:02:19,290
now moving to a different cluster well

00:02:16,320 --> 00:02:21,630
if you're even in a big organization

00:02:19,290 --> 00:02:24,690
you've got a cluster and it's broken and

00:02:21,630 --> 00:02:26,070
you can't wait until it gets fixed you

00:02:24,690 --> 00:02:28,770
might want to run your program somewhere

00:02:26,070 --> 00:02:30,060
else if it's a research environment at

00:02:28,770 --> 00:02:32,490
where you're trying to duplicate

00:02:30,060 --> 00:02:34,170
somebody else's results your lab might

00:02:32,490 --> 00:02:37,050
have a different kind of clustering

00:02:34,170 --> 00:02:42,470
system than theirs there's lots of

00:02:37,050 --> 00:02:46,260
reasons for it now underneath hpc I

00:02:42,470 --> 00:02:51,180
there's HP CD high performance computing

00:02:46,260 --> 00:02:53,180
drivers if anybody's use dbi DVD the

00:02:51,180 --> 00:02:58,640
resemblance in name is not accidental

00:02:53,180 --> 00:03:03,720
it's the same general idea so the driver

00:02:58,640 --> 00:03:06,090
translates the abstract HPC I interface

00:03:03,720 --> 00:03:12,960
into the specific one for that

00:03:06,090 --> 00:03:14,640
particular kind of cluster and for every

00:03:12,960 --> 00:03:20,310
kind of cluster you want to use you need

00:03:14,640 --> 00:03:22,050
us up a separate driver so the base the

00:03:20,310 --> 00:03:25,380
general components of the abstract

00:03:22,050 --> 00:03:30,180
interface I'm going to go through each

00:03:25,380 --> 00:03:32,220
of these in turn the top-level thing is

00:03:30,180 --> 00:03:33,870
a group that you create the group

00:03:32,220 --> 00:03:37,290
calling it what kind of cluster it's

00:03:33,870 --> 00:03:41,760
running on it manages the whole process

00:03:37,290 --> 00:03:45,240
of setting up things to be run remotely

00:03:41,760 --> 00:03:47,520
any dependencies where these people

00:03:45,240 --> 00:03:51,810
these steps have to go before these ones

00:03:47,520 --> 00:03:53,040
that sort of thing so that there's the

00:03:51,810 --> 00:03:55,800
scheduling in terms of those

00:03:53,040 --> 00:03:58,140
dependencies detecting when individual

00:03:55,800 --> 00:04:00,060
things run deciding whether something

00:03:58,140 --> 00:04:02,520
can be run again because it failed in a

00:04:00,060 --> 00:04:08,480
way that can be corrected collecting

00:04:02,520 --> 00:04:10,920
status and so on a stage is a

00:04:08,480 --> 00:04:13,470
computation to be run on a remote

00:04:10,920 --> 00:04:15,750
computer so it's a the entire

00:04:13,470 --> 00:04:19,980
computation that goes to one remote

00:04:15,750 --> 00:04:23,160
computer and so it needs a command to be

00:04:19,980 --> 00:04:24,900
run there's lots of wrapping stuff we'll

00:04:23,160 --> 00:04:28,740
get into that more later

00:04:24,900 --> 00:04:30,360
a run is usually the same as a stage but

00:04:28,740 --> 00:04:38,370
if you get retries there could be more

00:04:30,360 --> 00:04:42,990
than one run for a stage so there's a

00:04:38,370 --> 00:04:45,479
logger the if you pass one to hpc I

00:04:42,990 --> 00:04:49,410
it'll use it otherwise it'll create its

00:04:45,479 --> 00:04:52,080
own and this is logging everything in

00:04:49,410 --> 00:04:54,110
the top level system it's not all the

00:04:52,080 --> 00:04:56,400
remote jobs themselves it's just

00:04:54,110 --> 00:04:59,370
creating and getting status back and

00:04:56,400 --> 00:05:02,460
retries from those jobs unless you

00:04:59,370 --> 00:05:04,139
provide your own lager and have the code

00:05:02,460 --> 00:05:07,260
in the remote jobs connect to it

00:05:04,139 --> 00:05:13,620
yourself it's not one log for the whole

00:05:07,260 --> 00:05:16,380
thing resource requirements now most

00:05:13,620 --> 00:05:19,070
kinds of clusters have part of their

00:05:16,380 --> 00:05:22,590
interface being something that says

00:05:19,070 --> 00:05:26,190
these this job needs this much memory it

00:05:22,590 --> 00:05:29,250
runs for this length of time and it uses

00:05:26,190 --> 00:05:32,849
that to schedule things to decide which

00:05:29,250 --> 00:05:37,740
which systems it runs on that sort of

00:05:32,849 --> 00:05:40,380
stuff the naming and the the just the

00:05:37,740 --> 00:05:42,240
resources that you have to specify that

00:05:40,380 --> 00:05:46,949
you can specify is going to be clustered

00:05:42,240 --> 00:05:50,610
specific there's a bit of translation of

00:05:46,949 --> 00:05:55,260
naming done by HPC I so we have standard

00:05:50,610 --> 00:05:58,949
names for some things with a resource

00:05:55,260 --> 00:06:02,340
failing there's two kinds a soft failure

00:05:58,949 --> 00:06:04,110
it'll send a signal to the job warning

00:06:02,340 --> 00:06:08,760
it that it's gone over the soft limit

00:06:04,110 --> 00:06:10,860
heart failure it gets killed not all

00:06:08,760 --> 00:06:17,190
clusters will support either or both of

00:06:10,860 --> 00:06:20,639
those when you send a job off to another

00:06:17,190 --> 00:06:22,500
computer there's often environmental

00:06:20,639 --> 00:06:25,949
things that you want to wrap around it

00:06:22,500 --> 00:06:29,849
so both the command-line environment

00:06:25,949 --> 00:06:35,009
search has possibly files to pass on and

00:06:29,849 --> 00:06:39,059
all sorts of things like that the

00:06:35,009 --> 00:06:43,619
the directory layout that hpc I provides

00:06:39,059 --> 00:06:45,619
by default it can be overwritten in many

00:06:43,619 --> 00:06:48,089
ways you'll see some of that a bit later

00:06:45,619 --> 00:06:52,110
but there's a top-level directory that

00:06:48,089 --> 00:06:54,300
probably exists space dear immediately

00:06:52,110 --> 00:06:56,639
under it the group directory gets

00:06:54,300 --> 00:07:01,080
created with a the group name in a

00:06:56,639 --> 00:07:03,119
timestamp inside there the log that for

00:07:01,080 --> 00:07:05,759
the job and separate directories for

00:07:03,119 --> 00:07:08,849
each stage which get the name of the

00:07:05,759 --> 00:07:11,759
stage within the stage directory there's

00:07:08,849 --> 00:07:14,550
the script and a separate directory for

00:07:11,759 --> 00:07:16,949
each run and a symlink that points back

00:07:14,550 --> 00:07:18,419
to the last run because usually that's

00:07:16,949 --> 00:07:21,719
the one you care about if there were

00:07:18,419 --> 00:07:23,789
retries the one that happened last

00:07:21,719 --> 00:07:26,580
either failed in a way that couldn't be

00:07:23,789 --> 00:07:28,499
retried or it succeeded so that's when

00:07:26,580 --> 00:07:34,289
you're looking at within a run director

00:07:28,499 --> 00:07:41,520
you get students today so dynamics of

00:07:34,289 --> 00:07:43,409
using it to create a group and the you

00:07:41,520 --> 00:07:46,979
have to tell it what kind of cluster it

00:07:43,409 --> 00:07:50,129
is the name is optional there's a bunch

00:07:46,979 --> 00:07:51,899
of parameters now there's a bit of

00:07:50,129 --> 00:07:55,469
social engineering here the default name

00:07:51,899 --> 00:07:58,169
is default group name which is big and

00:07:55,469 --> 00:08:00,300
clunky it encourages people to choose a

00:07:58,169 --> 00:08:02,550
name but if you've got a small-scale

00:08:00,300 --> 00:08:07,339
kind of job where you're not running

00:08:02,550 --> 00:08:10,349
huge numbers of different HPC I process

00:08:07,339 --> 00:08:14,269
collections then having as a default

00:08:10,349 --> 00:08:18,389
name is easy enough max concurrent is

00:08:14,269 --> 00:08:21,029
managed by HPC I itself it limits the

00:08:18,389 --> 00:08:26,039
number of stages that are runs that are

00:08:21,029 --> 00:08:29,339
executing at the same time there are

00:08:26,039 --> 00:08:33,089
often similar sorts of things within the

00:08:29,339 --> 00:08:34,589
cluster interfaith the the cluster

00:08:33,089 --> 00:08:37,289
interface software so you don't

00:08:34,589 --> 00:08:39,750
necessarily need it you can set the base

00:08:37,289 --> 00:08:41,159
directory or group directory you can set

00:08:39,750 --> 00:08:43,699
blogging parameters environment

00:08:41,159 --> 00:08:43,699
parameters

00:08:44,720 --> 00:08:51,319
if you login parameters either you can

00:08:48,230 --> 00:08:54,050
provide a lug or you can say control

00:08:51,319 --> 00:08:56,959
where the log go is put either a full

00:08:54,050 --> 00:09:03,189
path or override the default directory

00:08:56,959 --> 00:09:08,120
or path file name okay create an object

00:09:03,189 --> 00:09:11,029
now you you have to have a name and you

00:09:08,120 --> 00:09:13,009
have to have a command we've got a

00:09:11,029 --> 00:09:15,620
couple of convenience methods for

00:09:13,009 --> 00:09:17,149
creating commands so you don't

00:09:15,620 --> 00:09:20,600
necessarily have to have it as part of

00:09:17,149 --> 00:09:23,839
the the actual creation but it has to be

00:09:20,600 --> 00:09:28,550
done fairly soon thereafter these set

00:09:23,839 --> 00:09:32,269
commands they translate things like take

00:09:28,550 --> 00:09:35,079
a list value and turn it into dash dash

00:09:32,269 --> 00:09:38,029
list a dash dash list d dash let's see

00:09:35,079 --> 00:09:42,769
but for something like Python it knows

00:09:38,029 --> 00:09:47,660
that it's dash dash last a b c and it

00:09:42,769 --> 00:09:49,910
adds in the interpreter name you can set

00:09:47,660 --> 00:09:52,490
the directory it explicitly if you don't

00:09:49,910 --> 00:09:56,209
want the default resource requires

00:09:52,490 --> 00:09:59,000
resources required as a hash which is

00:09:56,209 --> 00:10:03,709
often going to be specific to the type

00:09:59,000 --> 00:10:07,910
of cluster retry resources required if

00:10:03,709 --> 00:10:12,339
the if the driver is capable of

00:10:07,910 --> 00:10:17,059
detecting that a resource was exceeded

00:10:12,339 --> 00:10:19,459
then if you provide resource retry

00:10:17,059 --> 00:10:21,559
resources required it's a list of

00:10:19,459 --> 00:10:23,149
alternate values to use it just takes

00:10:21,559 --> 00:10:26,569
the next bigot next one that's bigger

00:10:23,149 --> 00:10:30,220
than the last one for a retry that's

00:10:26,569 --> 00:10:34,610
only supported right now for memory size

00:10:30,220 --> 00:10:36,649
failure action if stage fails do you

00:10:34,610 --> 00:10:39,529
stop running any stages that depend on

00:10:36,649 --> 00:10:41,629
it stop running anything else or ignore

00:10:39,529 --> 00:10:44,319
it this is a job that always gives up

00:10:41,629 --> 00:10:51,050
bad status even though it succeeded and

00:10:44,319 --> 00:10:54,860
environment passing you can copy things

00:10:51,050 --> 00:10:58,360
out of the environment it can be done

00:10:54,860 --> 00:11:03,040
either in the group or in the stage

00:10:58,360 --> 00:11:06,500
you and I think I've got the list yeah

00:11:03,040 --> 00:11:10,100
you can select which ones to retain or

00:11:06,500 --> 00:11:11,540
select which ones to remove usually

00:11:10,100 --> 00:11:14,000
you'll do one or the other although if

00:11:11,540 --> 00:11:16,970
you use a pattern then you might combine

00:11:14,000 --> 00:11:22,640
them you can have a list an additional

00:11:16,970 --> 00:11:25,450
set that you add explicitly and whatever

00:11:22,640 --> 00:11:30,410
you end up with if there was anything

00:11:25,450 --> 00:11:36,020
then it'll get put into the script that

00:11:30,410 --> 00:11:38,180
gets run if not nothing was provided

00:11:36,020 --> 00:11:43,510
anywhere then there's no nothing put

00:11:38,180 --> 00:11:46,190
into the script dependencies we've got

00:11:43,510 --> 00:11:48,940
prerequisites dependence those are two

00:11:46,190 --> 00:11:51,110
sides of a double of an arrow a

00:11:48,940 --> 00:11:54,170
prerequisite has to run first the

00:11:51,110 --> 00:11:57,830
dependent has to run later often you'll

00:11:54,170 --> 00:12:00,020
have a few stages that are building

00:11:57,830 --> 00:12:03,770
files that get used by a whole bunch of

00:12:00,020 --> 00:12:05,510
others so preparation things whole bunch

00:12:03,770 --> 00:12:11,089
of processing things maybe you'll have a

00:12:05,510 --> 00:12:15,080
group of report steps afterwards and the

00:12:11,089 --> 00:12:17,540
EDD depth is can be specified as

00:12:15,080 --> 00:12:20,870
one-to-one one-to-many many-to-one

00:12:17,540 --> 00:12:24,230
many-to-many you just give a list and

00:12:20,870 --> 00:12:26,089
you actually use the stage name rather

00:12:24,230 --> 00:12:27,920
than the object references normally you

00:12:26,089 --> 00:12:33,020
don't even bother keeping the stage

00:12:27,920 --> 00:12:37,370
object references and then the final

00:12:33,020 --> 00:12:39,890
thing is execute and that's a single

00:12:37,370 --> 00:12:42,560
atomic thing all of this it will not

00:12:39,890 --> 00:12:44,589
finish until all of the stages that are

00:12:42,560 --> 00:12:47,450
going to be run have completed and

00:12:44,589 --> 00:12:52,520
returns a structure that has the status

00:12:47,450 --> 00:12:55,370
and the status is a hash by stage name

00:12:52,520 --> 00:12:58,190
and for each one of them there's an

00:12:55,370 --> 00:13:01,940
array by run number and then there's a

00:12:58,190 --> 00:13:05,300
hash at the bottom the bottom level hash

00:13:01,940 --> 00:13:08,720
always has exit status and it can have

00:13:05,300 --> 00:13:11,730
anything else that the the specific

00:13:08,720 --> 00:13:17,140
cluster reports on

00:13:11,730 --> 00:13:21,310
the current status it there's hpc I

00:13:17,140 --> 00:13:26,680
itself and HP CD s GE are available in

00:13:21,310 --> 00:13:28,420
Sipan ste which is son great engine is

00:13:26,680 --> 00:13:31,210
the kind of cluster we have in my labs

00:13:28,420 --> 00:13:35,020
so that's the one we develop first we've

00:13:31,210 --> 00:13:38,380
been running it for six months now when

00:13:35,020 --> 00:13:41,050
various incarnations there's also a

00:13:38,380 --> 00:13:44,940
driver called HP CD uni that's part of

00:13:41,050 --> 00:13:48,820
hpc I and it just does a fork for

00:13:44,940 --> 00:13:52,920
creating a a state stage runs it on the

00:13:48,820 --> 00:13:55,780
same system that's important for testing

00:13:52,920 --> 00:13:58,150
when you download something you might

00:13:55,780 --> 00:14:00,040
not be on one of the systems in your lab

00:13:58,150 --> 00:14:04,750
that has access to the cluster or

00:14:00,040 --> 00:14:07,690
whatever or you might require

00:14:04,750 --> 00:14:09,310
authorization that sort of thing and

00:14:07,690 --> 00:14:13,150
that hasn't been set up for that on the

00:14:09,310 --> 00:14:15,700
the base tests the other thing you can

00:14:13,150 --> 00:14:18,130
use with uni is just a fallback you know

00:14:15,700 --> 00:14:20,920
the cluster is broken and you don't have

00:14:18,130 --> 00:14:22,930
a second kind run it with uni and so it

00:14:20,920 --> 00:14:25,000
takes a long time on your system but at

00:14:22,930 --> 00:14:29,620
least its operating as opposed to not

00:14:25,000 --> 00:14:33,190
doing sge as I say has been pretty well

00:14:29,620 --> 00:14:35,580
tested at our lab however I got a couple

00:14:33,190 --> 00:14:38,140
of co-op students writing tests like

00:14:35,580 --> 00:14:40,630
about two weeks ago and they're coming

00:14:38,140 --> 00:14:42,700
up with all sorts of things so I'm going

00:14:40,630 --> 00:14:47,350
to have a new release shortly that

00:14:42,700 --> 00:14:51,040
covers the ones they've found now the

00:14:47,350 --> 00:14:55,030
future main thing I need at the moment

00:14:51,040 --> 00:14:58,870
is a community as I say we've got one

00:14:55,030 --> 00:15:00,760
kind of cluster in our lab I don't have

00:14:58,870 --> 00:15:05,080
access to others to build drivers for it

00:15:00,760 --> 00:15:07,540
so to make this maximally useful I need

00:15:05,080 --> 00:15:09,430
drivers for all kinds of clusters I'm

00:15:07,540 --> 00:15:12,280
actually going to be getting access to a

00:15:09,430 --> 00:15:13,630
couple of other kinds which my boss says

00:15:12,280 --> 00:15:16,440
he's going to arrange and I haven't

00:15:13,630 --> 00:15:20,620
figured out what they are or wind

00:15:16,440 --> 00:15:22,210
there's so I'd like to help people if

00:15:20,620 --> 00:15:23,260
they want to write drivers for clusters

00:15:22,210 --> 00:15:28,320
that I don't have ax

00:15:23,260 --> 00:15:32,080
to or you know additions to hpc I itself

00:15:28,320 --> 00:15:35,140
as new drivers come along who's probably

00:15:32,080 --> 00:15:38,290
holds in the the top-level definition

00:15:35,140 --> 00:15:39,940
that need to be covered I know of a

00:15:38,290 --> 00:15:46,390
couple that I'm planning to put in

00:15:39,940 --> 00:15:49,000
already cloud clouds as opposed to

00:15:46,390 --> 00:15:50,230
clusters the moment at the moment

00:15:49,000 --> 00:15:52,180
everything assumes that you've got a

00:15:50,230 --> 00:15:54,700
shared file system and so when you run

00:15:52,180 --> 00:15:56,350
your job it just goes to the same

00:15:54,700 --> 00:15:58,300
directory and runs this job that was

00:15:56,350 --> 00:16:00,820
already created and its output files are

00:15:58,300 --> 00:16:05,980
available to whatever state is later on

00:16:00,820 --> 00:16:07,990
need them with clouds there's probably

00:16:05,980 --> 00:16:10,960
going to be bundling of files that have

00:16:07,990 --> 00:16:13,420
to go along with the stage results that

00:16:10,960 --> 00:16:17,850
come back there's probably going to be

00:16:13,420 --> 00:16:21,490
extra parameters for authorization and

00:16:17,850 --> 00:16:28,600
maybe even choosing subsets of nodes and

00:16:21,490 --> 00:16:30,670
things like that and we've got some

00:16:28,600 --> 00:16:36,340
resources I've got a mailing list set up

00:16:30,670 --> 00:16:39,190
and those you can bug reports can go

00:16:36,340 --> 00:16:42,880
through either the RT system on c pan or

00:16:39,190 --> 00:16:45,610
directly to boutros lab software and if

00:16:42,880 --> 00:16:47,770
anybody needs to talk to me that's my my

00:16:45,610 --> 00:16:59,610
mail address I also get all the Boutrous

00:16:47,770 --> 00:16:59,610
lab soft work so any questions yes

00:17:01,170 --> 00:17:23,939
ah well if just fork and exact running a

00:17:21,780 --> 00:17:27,030
job is going to spread things across the

00:17:23,939 --> 00:17:35,340
different cores I I think that actually

00:17:27,030 --> 00:17:38,460
works fairly well so that's where that's

00:17:35,340 --> 00:17:40,710
where i put in the max concurrent so

00:17:38,460 --> 00:17:44,240
that's the top level thing that hpc I

00:17:40,710 --> 00:17:49,260
manages it's not the driver itself

00:17:44,240 --> 00:17:51,780
camera can't read that but ok times up

00:17:49,260 --> 00:17:56,450
so anybody has further questions come

00:17:51,780 --> 00:17:56,450

YouTube URL: https://www.youtube.com/watch?v=fb7XZj__Pqg


