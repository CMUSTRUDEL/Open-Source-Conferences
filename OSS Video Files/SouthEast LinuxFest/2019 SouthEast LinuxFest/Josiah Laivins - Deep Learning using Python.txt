Title: Josiah Laivins - Deep Learning using Python
Publication date: 2019-06-19
Playlist: 2019 SouthEast LinuxFest
Description: 
	SouthEast Linux Fest 2019
In this talk, I will do a crash course in Python deep learning (DL). We will cover tabular (csv, spreadsheet), image, and forecasting DL. The key take away I want to give you is an idea on what tools are out there, and where you might look to accomplish some goal using DL. I will quickly cover tools like Tensorboard, Jupyter, Matplotlib, and Pycharm for visuals. I will be using Pytorch, and explain the difference between that and Tensorflow.
Captions: 
	00:04:55,120 --> 00:05:08,030
that oh wow yeah yeah okay so like I

00:05:04,760 --> 00:05:12,170
said and for the recording I'm just hi

00:05:08,030 --> 00:05:14,030
Elevens um so so yeah so I wanted to go

00:05:12,170 --> 00:05:17,420
and actually hopefully do a crash course

00:05:14,030 --> 00:05:19,730
on mostly what is out there and deep

00:05:17,420 --> 00:05:22,370
learning what you can use it for and how

00:05:19,730 --> 00:05:23,450
you would get started and hopefully like

00:05:22,370 --> 00:05:24,980
there's two different routes that I

00:05:23,450 --> 00:05:28,190
think that you could go you can go the

00:05:24,980 --> 00:05:30,590
deep low level math route and then you

00:05:28,190 --> 00:05:32,240
can go the more high level there's a lot

00:05:30,590 --> 00:05:35,960
more easier framework so if you don't

00:05:32,240 --> 00:05:38,330
have a very solid math background you

00:05:35,960 --> 00:05:41,390
can still like go full out and do really

00:05:38,330 --> 00:05:42,950
really crazy stuff with this stuff so so

00:05:41,390 --> 00:05:44,960
hopefully holy that's it Jenna listen

00:05:42,950 --> 00:05:47,480
say you that you get and I'll do a lot

00:05:44,960 --> 00:05:50,660
of links and the logit Oriels that uh or

00:05:47,480 --> 00:05:53,030
link to a tutorial they can look at okay

00:05:50,660 --> 00:05:54,320
so I'm gonna talk about what is deep

00:05:53,030 --> 00:05:57,850
learning some of the tools that are used

00:05:54,320 --> 00:06:00,590
for it right now and then some examples

00:05:57,850 --> 00:06:05,590
there are also a lot of applications for

00:06:00,590 --> 00:06:05,590
these so I'll go through a few of those

00:06:05,890 --> 00:06:12,860
okay so deep learning it is a subset of

00:06:10,490 --> 00:06:14,930
machine learning and that is a subset of

00:06:12,860 --> 00:06:16,580
artificial intelligence so artificial

00:06:14,930 --> 00:06:18,710
intelligence has been around for a very

00:06:16,580 --> 00:06:20,210
long time and odds are if you've done

00:06:18,710 --> 00:06:22,040
anything computer science base you've

00:06:20,210 --> 00:06:23,750
ever either worked with something with

00:06:22,040 --> 00:06:26,090
artificial intelligence or you've

00:06:23,750 --> 00:06:27,920
actually done some simple written some

00:06:26,090 --> 00:06:32,360
simple programs that do some form of

00:06:27,920 --> 00:06:34,370
artificial intelligence to me I mean

00:06:32,360 --> 00:06:37,250
artificial intelligence could be just a

00:06:34,370 --> 00:06:41,720
lot of is statements so if this happens

00:06:37,250 --> 00:06:44,030
do this otherwise do that so it could be

00:06:41,720 --> 00:06:46,460
that or you're doing this really crazy

00:06:44,030 --> 00:06:48,920
deep level deep learning stuff with

00:06:46,460 --> 00:06:50,900
neural nets and all that all everything

00:06:48,920 --> 00:06:53,090
in between that is artificial

00:06:50,900 --> 00:06:54,590
intelligence you're you're trying to get

00:06:53,090 --> 00:06:56,860
a computer to do something somewhat

00:06:54,590 --> 00:06:56,860
smart

00:07:02,039 --> 00:07:08,560
yeah so it was really hard for me to

00:07:06,099 --> 00:07:09,699
think about the difference between like

00:07:08,560 --> 00:07:11,229
artificial intelligence and machine

00:07:09,699 --> 00:07:13,330
learning

00:07:11,229 --> 00:07:16,770
mostly with artificial intelligence that

00:07:13,330 --> 00:07:19,509
involves searching for a solution or

00:07:16,770 --> 00:07:20,830
doing some kind of like brute force game

00:07:19,509 --> 00:07:23,530
playing like if you're playing chess I

00:07:20,830 --> 00:07:25,689
might just go and like find the best

00:07:23,530 --> 00:07:27,550
number of moves that go on like well out

00:07:25,689 --> 00:07:30,819
- again - somehow winning State or

00:07:27,550 --> 00:07:32,139
something like that for me artificial

00:07:30,819 --> 00:07:34,509
intelligence is just doing lots of

00:07:32,139 --> 00:07:35,979
searches so whether you're doing a

00:07:34,509 --> 00:07:38,199
database search and you're going and

00:07:35,979 --> 00:07:40,150
trying to find them at the fastest way

00:07:38,199 --> 00:07:42,939
possible or the most intelligent way

00:07:40,150 --> 00:07:44,680
possible that that's probably the most

00:07:42,939 --> 00:07:48,779
immediate thing that comes to mind when

00:07:44,680 --> 00:07:48,779
we talk about artificial intelligence

00:07:48,960 --> 00:07:56,289
let me see more specifically machine

00:07:54,340 --> 00:07:57,189
learning so machine learning is really

00:07:56,289 --> 00:08:00,909
exciting

00:07:57,189 --> 00:08:04,539
odds are that if you all have emails an

00:08:00,909 --> 00:08:06,759
easy example is right now Google and a

00:08:04,539 --> 00:08:08,169
lot of these companies they have spam

00:08:06,759 --> 00:08:10,270
filters and how do you think go and

00:08:08,169 --> 00:08:12,969
figure out what what is spam and what

00:08:10,270 --> 00:08:14,409
isn't spam so you're your two

00:08:12,969 --> 00:08:16,750
alternatives is that if you're a

00:08:14,409 --> 00:08:18,940
software engineer if you're doing

00:08:16,750 --> 00:08:20,589
artificial intelligence approach you're

00:08:18,940 --> 00:08:23,650
going to design some super complicated

00:08:20,589 --> 00:08:26,529
algorithm that's going to look for like

00:08:23,650 --> 00:08:29,710
free or you're gonna measure how many

00:08:26,529 --> 00:08:33,279
like fancy graphics are in the email or

00:08:29,710 --> 00:08:36,399
if it's if the email saying urgent or is

00:08:33,279 --> 00:08:40,029
from like some kind you are l artificial

00:08:36,399 --> 00:08:43,630
intelligence you might go in hard code

00:08:40,029 --> 00:08:46,540
all that and then in a month you're

00:08:43,630 --> 00:08:49,810
going to throw that all away and recode

00:08:46,540 --> 00:08:52,570
it because all the people sending spam

00:08:49,810 --> 00:08:54,640
emails have now changed what their spam

00:08:52,570 --> 00:08:56,500
emails look like and you're gonna do

00:08:54,640 --> 00:08:59,800
that constantly over and over and over

00:08:56,500 --> 00:09:02,380
again which is kind of ridiculous so it

00:08:59,800 --> 00:09:04,870
would be better for the computer just to

00:09:02,380 --> 00:09:06,820
figure out what does spam so you don't

00:09:04,870 --> 00:09:08,380
have to do any of that coating so

00:09:06,820 --> 00:09:11,649
Michigan learning goes and figures that

00:09:08,380 --> 00:09:13,839
out it'll go and figure out whether this

00:09:11,649 --> 00:09:15,310
email might be spam or this email and it

00:09:13,839 --> 00:09:17,830
might look at a giant group

00:09:15,310 --> 00:09:21,870
of emails and if they're spam I try to

00:09:17,830 --> 00:09:21,870
try to find commonalities between them

00:09:25,230 --> 00:09:30,970
yeah so machine learning is mostly

00:09:27,940 --> 00:09:33,760
involved in finding patterns in large

00:09:30,970 --> 00:09:36,310
amount of data clustering data and then

00:09:33,760 --> 00:09:38,230
trying to make some kind of inference so

00:09:36,310 --> 00:09:41,100
some kind of like classification or

00:09:38,230 --> 00:09:41,100
something like that

00:09:42,839 --> 00:09:48,070
finally deep learning is very recent

00:09:45,400 --> 00:09:49,390
deep learning involves involves neural

00:09:48,070 --> 00:09:51,010
nets they're usually very very deep

00:09:49,390 --> 00:09:53,800
they're supposed to go and stimulate the

00:09:51,010 --> 00:09:55,800
brain so the more layers you have more

00:09:53,800 --> 00:09:58,990
complex representations you can actually

00:09:55,800 --> 00:10:06,910
see you know go into a little more

00:09:58,990 --> 00:10:09,339
detail with that yeah so this is just

00:10:06,910 --> 00:10:11,020
some some of the differences between

00:10:09,339 --> 00:10:13,270
them also machine learning is doing a

00:10:11,020 --> 00:10:21,550
like probability and also reinforcement

00:10:13,270 --> 00:10:23,080
learning and stuff like that so so yeah

00:10:21,550 --> 00:10:26,260
so you have everything from clustering

00:10:23,080 --> 00:10:27,850
so k-means clustering you might have a

00:10:26,260 --> 00:10:29,530
lot of data points and the goal is that

00:10:27,850 --> 00:10:33,660
you're going to go and try to find what

00:10:29,530 --> 00:10:33,660
things belong to one group automatically

00:10:35,670 --> 00:10:40,690
probabilistic like Bayesian inference is

00:10:37,990 --> 00:10:41,800
going to be looking at what is the was

00:10:40,690 --> 00:10:43,210
the probability that maybe a

00:10:41,800 --> 00:10:48,580
cause-and-effect happened or something

00:10:43,210 --> 00:10:52,210
like that and SVM's are a very smart way

00:10:48,580 --> 00:10:54,010
they go and use some data they keep some

00:10:52,210 --> 00:10:56,730
of the data to go and do some kind of

00:10:54,010 --> 00:10:58,900
like binary binary classification so

00:10:56,730 --> 00:11:00,940
differentiating between like a dog or a

00:10:58,900 --> 00:11:04,350
cat so as vm's will go and do something

00:11:00,940 --> 00:11:07,660
like that yeah

00:11:04,350 --> 00:11:09,839
support vector machines support vector

00:11:07,660 --> 00:11:13,779
machines you want to go and Google that

00:11:09,839 --> 00:11:16,270
they're very very cool so from what I've

00:11:13,779 --> 00:11:18,250
seen they do they do binary they'll go

00:11:16,270 --> 00:11:20,260
and split things in half but there's a

00:11:18,250 --> 00:11:22,870
lot of tricks that you can do where you

00:11:20,260 --> 00:11:28,140
might stack svms so you can actually do

00:11:22,870 --> 00:11:28,140
a lot of different classifications yeah

00:11:30,270 --> 00:11:44,890
artificial intelligence yeah yeah yeah

00:11:41,020 --> 00:11:46,120
and the general goal is so the one to me

00:11:44,890 --> 00:11:48,190
the biggest difference between machine

00:11:46,120 --> 00:11:51,640
learning and artificial intelligence is

00:11:48,190 --> 00:11:56,520
that machine learning is trying to go

00:11:51,640 --> 00:11:58,090
and use existing data to go and

00:11:56,520 --> 00:12:00,820
accomplish something that you would

00:11:58,090 --> 00:12:03,220
otherwise have to really manually I'm

00:12:00,820 --> 00:12:04,720
talking about like if statements or

00:12:03,220 --> 00:12:09,100
you're doing some kind of like search

00:12:04,720 --> 00:12:10,690
trees or something like that so so yeah

00:12:09,100 --> 00:12:13,180
so the goal of machine learning is that

00:12:10,690 --> 00:12:16,030
you don't have to do as much coding in

00:12:13,180 --> 00:12:17,980
fact the biggest goal is that you build

00:12:16,030 --> 00:12:21,700
your data set you don't train change

00:12:17,980 --> 00:12:24,520
your program and in deep learning takes

00:12:21,700 --> 00:12:26,320
that to the extreme so at some point you

00:12:24,520 --> 00:12:27,790
only have your model you don't change it

00:12:26,320 --> 00:12:33,100
for a year you're just changing the data

00:12:27,790 --> 00:12:35,560
set so so yeah symbolic algorithms they

00:12:33,100 --> 00:12:39,370
just seem I feel like they're probably

00:12:35,560 --> 00:12:40,750
the more most intuitive ones they're

00:12:39,370 --> 00:12:41,890
taking in some kind of like logical

00:12:40,750 --> 00:12:43,210
relationships like if you're talking

00:12:41,890 --> 00:12:44,650
about like grandparents and

00:12:43,210 --> 00:12:48,940
relationships between them and children

00:12:44,650 --> 00:12:50,440
or something like that reinforcement

00:12:48,940 --> 00:12:54,120
learning so this is what I'm actually

00:12:50,440 --> 00:12:57,100
studying right now you go and teach a

00:12:54,120 --> 00:12:58,350
algorithm to do something with just

00:12:57,100 --> 00:13:01,600
reward and Punishment

00:12:58,350 --> 00:13:04,510
so something here if it goes and fails

00:13:01,600 --> 00:13:05,860
like if the pole tipped over okay it

00:13:04,510 --> 00:13:08,140
gets punished but then the longer keeps

00:13:05,860 --> 00:13:14,680
pull at the mortal word it gets and so

00:13:08,140 --> 00:13:17,500
that just learns that automatically okay

00:13:14,680 --> 00:13:19,300
and then we're at deep learning and

00:13:17,500 --> 00:13:20,500
neural nets so something is deep

00:13:19,300 --> 00:13:23,200
learning if you're having multiple

00:13:20,500 --> 00:13:26,880
layers and that allows you to do like a

00:13:23,200 --> 00:13:30,030
lot more complex representations yeah

00:13:26,880 --> 00:13:30,030
yeah yeah

00:13:30,890 --> 00:13:41,470
oh did I have okay I have them yeah

00:13:39,860 --> 00:13:45,260
you're talking about genetic algorithms

00:13:41,470 --> 00:13:47,480
so so not really but they're very

00:13:45,260 --> 00:13:50,210
closely tied like that Li you can keep

00:13:47,480 --> 00:13:53,810
them separate but yeah they're they're

00:13:50,210 --> 00:13:55,250
actually pretty closely tied genetic

00:13:53,810 --> 00:13:58,010
algorithms are used for a lot more than

00:13:55,250 --> 00:14:00,020
just that I was kind of I wasn't really

00:13:58,010 --> 00:14:02,270
sure whether put genetic algorithms and

00:14:00,020 --> 00:14:04,910
machine learning but genetic algorithms

00:14:02,270 --> 00:14:06,200
is a form of searching either for a

00:14:04,910 --> 00:14:10,130
solution or something like that that

00:14:06,200 --> 00:14:12,320
involves splitting children up into or

00:14:10,130 --> 00:14:14,300
taking parents making children but

00:14:12,320 --> 00:14:18,230
randomly mutating the children and then

00:14:14,300 --> 00:14:21,470
doing random combinations of them Tamia

00:14:18,230 --> 00:14:23,090
was a form of searching so so yeah but

00:14:21,470 --> 00:14:29,210
they're definitely they're definitely

00:14:23,090 --> 00:14:30,950
like um combined a lot yeah so neural

00:14:29,210 --> 00:14:32,870
nets so it's not there's no such thing

00:14:30,950 --> 00:14:35,420
as like a single neural net or a single

00:14:32,870 --> 00:14:37,220
type of neural net there's a lot of ways

00:14:35,420 --> 00:14:41,690
they can go and take these neurons and

00:14:37,220 --> 00:14:43,790
like stack them together once again the

00:14:41,690 --> 00:14:56,500
main thing is that these are supposed to

00:14:43,790 --> 00:14:56,500
go and model how the brain works yeah

00:14:56,700 --> 00:15:03,510
so the oranges the hidden layers and

00:14:59,460 --> 00:15:05,850
then the green is your input and then

00:15:03,510 --> 00:15:07,200
you have your output layers so it's a

00:15:05,850 --> 00:15:10,710
picture that I have here is an example

00:15:07,200 --> 00:15:12,270
of a fully connected one so in the end

00:15:10,710 --> 00:15:14,250
put layer each of those neurons each

00:15:12,270 --> 00:15:16,170
each of those circles is connected to

00:15:14,250 --> 00:15:19,080
all of the neurons in the next one and

00:15:16,170 --> 00:15:20,730
so all of each one of those lines is

00:15:19,080 --> 00:15:25,380
some kind of toggle or some kind of

00:15:20,730 --> 00:15:27,300
parameter between them and that can be

00:15:25,380 --> 00:15:28,440
inefficient so one of you one of the

00:15:27,300 --> 00:15:29,790
people's goals is that you're gonna

00:15:28,440 --> 00:15:32,430
reduce the number of those black lines

00:15:29,790 --> 00:15:36,390
that you have in the neural net so that

00:15:32,430 --> 00:15:39,240
your actual like back propagation it

00:15:36,390 --> 00:15:42,540
works properly and I'll talk I'll talk

00:15:39,240 --> 00:15:44,280
more in detail later but there's a lot

00:15:42,540 --> 00:15:46,590
of different implementations that is a

00:15:44,280 --> 00:15:49,350
linear fully connected neural net and

00:15:46,590 --> 00:15:51,720
you could have heared there three of

00:15:49,350 --> 00:15:54,780
these or you could have 50 of them just

00:15:51,720 --> 00:15:56,880
all stack in a giant line but then

00:15:54,780 --> 00:15:58,920
there's also a convolutional Arnon scans

00:15:56,880 --> 00:16:02,580
and I have examples for these like

00:15:58,920 --> 00:16:06,630
practical examples at the bottom DQ ends

00:16:02,580 --> 00:16:08,310
DT D DPG actor critic

00:16:06,630 --> 00:16:10,470
those are reinforcement and neural nets

00:16:08,310 --> 00:16:14,790
so they are the neural nets they use for

00:16:10,470 --> 00:16:16,470
like teaching like at combining actions

00:16:14,790 --> 00:16:19,410
and correlating them with rewards and

00:16:16,470 --> 00:16:22,460
punishments and we'll go through a bunch

00:16:19,410 --> 00:16:22,460
of examples of those

00:16:32,779 --> 00:16:39,209
yeah yeah so yeah that's the general

00:16:36,600 --> 00:16:42,870
idea so it's supposed to be modeled off

00:16:39,209 --> 00:16:44,820
of this so I think and I don't know how

00:16:42,870 --> 00:16:47,670
I feel about someone's correct me

00:16:44,820 --> 00:16:49,529
so dendrites are the inputs and then

00:16:47,670 --> 00:16:51,450
when those inputs get high enough like

00:16:49,529 --> 00:16:53,640
combined together and get high enough a

00:16:51,450 --> 00:16:56,579
neuron will go and decide to actually

00:16:53,640 --> 00:16:59,640
fire and then it'll actually fire its

00:16:56,579 --> 00:17:03,089
signal out the I think it's boat ons I

00:16:59,640 --> 00:17:04,500
don't know I say it but well yeah it's

00:17:03,089 --> 00:17:07,230
collecting a bunch of inputs and when

00:17:04,500 --> 00:17:09,600
they reach a threshold it fires and then

00:17:07,230 --> 00:17:11,819
what you do is you have hundreds of

00:17:09,600 --> 00:17:14,370
these correlate together and multiple

00:17:11,819 --> 00:17:16,980
layers and what that actually allows you

00:17:14,370 --> 00:17:18,299
to do this is rip single one of these is

00:17:16,980 --> 00:17:22,040
very simple but if you have a lot of

00:17:18,299 --> 00:17:24,120
these they can do amazingly complicated

00:17:22,040 --> 00:17:28,740
actions or find very complicated

00:17:24,120 --> 00:17:31,049
patterns and so this is the exact same

00:17:28,740 --> 00:17:33,660
thing as the biological picture that I

00:17:31,049 --> 00:17:37,530
showed but in more of a mathematical

00:17:33,660 --> 00:17:39,809
term so each of these X's they could be

00:17:37,530 --> 00:17:43,169
anything the easiest example would be

00:17:39,809 --> 00:17:45,210
like image pixels so if you have a bunch

00:17:43,169 --> 00:17:51,270
of image pixels and then they're

00:17:45,210 --> 00:17:56,390
multiplied by some weights I'm wondering

00:17:51,270 --> 00:17:59,220
how I can explain that better yeah so

00:17:56,390 --> 00:18:00,630
the x's are could be like image pixels

00:17:59,220 --> 00:18:02,520
or something so you're gonna go and try

00:18:00,630 --> 00:18:06,630
to predict whether maybe a picture is a

00:18:02,520 --> 00:18:10,890
dog or a cat the weights are how

00:18:06,630 --> 00:18:14,820
important that pixel or that value is to

00:18:10,890 --> 00:18:16,679
that neuro line so so that neuron like

00:18:14,820 --> 00:18:17,940
maybe the outer outer weights might be

00:18:16,679 --> 00:18:19,650
lower but then the middle ones might be

00:18:17,940 --> 00:18:21,480
higher who knows

00:18:19,650 --> 00:18:23,010
and actually you won't really have a

00:18:21,480 --> 00:18:24,900
whole lot of control over what those

00:18:23,010 --> 00:18:27,809
weights are but that's the main

00:18:24,900 --> 00:18:30,570
challenge for your own that's in general

00:18:27,809 --> 00:18:32,010
so when you're doing something called

00:18:30,570 --> 00:18:33,929
back propagation that's going to go and

00:18:32,010 --> 00:18:35,549
toggle those weights until your neural

00:18:33,929 --> 00:18:38,549
net actually improves and whatever

00:18:35,549 --> 00:18:40,320
performance or accuracy that's doing in

00:18:38,549 --> 00:18:42,510
that so it sums them together and does

00:18:40,320 --> 00:18:45,179
some kind of activation the activation

00:18:42,510 --> 00:18:46,350
function decides based on all these

00:18:45,179 --> 00:18:48,929
inputs we've

00:18:46,350 --> 00:18:52,640
clump of these together when do I decide

00:18:48,929 --> 00:18:56,820
to actually fire off some kind of signal

00:18:52,640 --> 00:18:58,559
so so that's so we have that there but

00:18:56,820 --> 00:19:00,000
we'll also go into that more detail

00:18:58,559 --> 00:19:04,230
actually like when we're actually

00:19:00,000 --> 00:19:06,870
implementing this it's also really

00:19:04,230 --> 00:19:08,100
complicated like I can't show my speaker

00:19:06,870 --> 00:19:14,880
notes on here so I'm using my phone

00:19:08,100 --> 00:19:16,530
action right now so how hard yeah so

00:19:14,880 --> 00:19:18,210
this is just general summary so a neuron

00:19:16,530 --> 00:19:23,270
has inputs weights and some kind of

00:19:18,210 --> 00:19:25,710
activation back propagation is the main

00:19:23,270 --> 00:19:27,150
main problem with neural nets that

00:19:25,710 --> 00:19:30,299
people are trying to solve or make

00:19:27,150 --> 00:19:34,470
better so there's a lot of strategies

00:19:30,299 --> 00:19:35,850
with this that's going to go and toggle

00:19:34,470 --> 00:19:39,480
them to go and improve some have

00:19:35,850 --> 00:19:40,830
accuracy terminology when you see

00:19:39,480 --> 00:19:44,549
something like model parameters those

00:19:40,830 --> 00:19:45,990
are the weights loss is just how much

00:19:44,549 --> 00:19:49,350
the neural net needs to change its

00:19:45,990 --> 00:19:53,159
wastes it also says how wrong the model

00:19:49,350 --> 00:19:54,960
is normalization so when you're going

00:19:53,159 --> 00:19:56,400
and feeding your data into it into a

00:19:54,960 --> 00:19:59,669
neural net you have to make sure that

00:19:56,400 --> 00:20:00,900
there's 0 to 1 and the hidden layers are

00:19:59,669 --> 00:20:04,500
the ones that aren't connected to your

00:20:00,900 --> 00:20:07,980
outputs or your inputs yeah and we'll go

00:20:04,500 --> 00:20:10,830
into more detail with that so so when

00:20:07,980 --> 00:20:13,320
the quickest things that you can do is

00:20:10,830 --> 00:20:15,809
if you go to playground tensorflow you

00:20:13,320 --> 00:20:17,309
can actually look at on your own net in

00:20:15,809 --> 00:20:19,890
action and you can actually toggle the

00:20:17,309 --> 00:20:21,690
neurons right now it's doing a binary

00:20:19,890 --> 00:20:25,679
classification like it's trying to

00:20:21,690 --> 00:20:29,429
classify classify the the blue dots with

00:20:25,679 --> 00:20:31,620
the orange dots and so you can go and

00:20:29,429 --> 00:20:37,710
change change how many layers you have

00:20:31,620 --> 00:20:39,000
how big the layers are and normally the

00:20:37,710 --> 00:20:40,980
more layers you have the more

00:20:39,000 --> 00:20:42,840
complicated features you can have so

00:20:40,980 --> 00:20:47,280
like something like this if you add more

00:20:42,840 --> 00:20:55,200
layers and maybe like more neurons then

00:20:47,280 --> 00:20:57,450
it'll separate them yeah ok so we talked

00:20:55,200 --> 00:20:59,250
about all that but right now it's kind

00:20:57,450 --> 00:21:01,409
of high level it's kind of

00:20:59,250 --> 00:21:02,610
and maybe you don't really maybe you

00:21:01,409 --> 00:21:04,380
don't have a really solid under

00:21:02,610 --> 00:21:06,809
understand what's actually happening so

00:21:04,380 --> 00:21:14,190
then we'll go into the actual tools and

00:21:06,809 --> 00:21:16,110
the actual code for this okay so I want

00:21:14,190 --> 00:21:18,030
to go in really quickly gauge okay so

00:21:16,110 --> 00:21:22,049
first off like who all is familiar with

00:21:18,030 --> 00:21:24,090
just machine learning in general okay

00:21:22,049 --> 00:21:27,980
so not very many um who all has worked

00:21:24,090 --> 00:21:31,620
with Python okay cool cool

00:21:27,980 --> 00:21:34,200
okay so so if you're already familiar

00:21:31,620 --> 00:21:36,960
with Python then this should be pretty

00:21:34,200 --> 00:21:39,450
uh pretty obvious to you on how amazing

00:21:36,960 --> 00:21:42,480
it is like I've I've had a lot of fun

00:21:39,450 --> 00:21:43,830
with it so in recent years it's gained

00:21:42,480 --> 00:21:44,789
in a lot of popularity and mostly

00:21:43,830 --> 00:21:50,400
because of the libraries that are

00:21:44,789 --> 00:21:53,220
available to it one of the package

00:21:50,400 --> 00:21:55,140
managers that uses anaconda I also use a

00:21:53,220 --> 00:21:56,730
more stripped-down version of mini

00:21:55,140 --> 00:21:59,039
Kannada because I like to install the

00:21:56,730 --> 00:22:01,440
packages that I only the packages that I

00:21:59,039 --> 00:22:02,880
want but anaconda will go and just if

00:22:01,440 --> 00:22:04,650
you download it it'll just have

00:22:02,880 --> 00:22:11,130
everything for you so you don't have to

00:22:04,650 --> 00:22:12,780
download anything new in theory some of

00:22:11,130 --> 00:22:16,470
the basic libraries so there's this

00:22:12,780 --> 00:22:18,960
library called numpy and so if you've

00:22:16,470 --> 00:22:21,470
done them you program programming before

00:22:18,960 --> 00:22:24,450
you know about arrays and 2d arrays so

00:22:21,470 --> 00:22:27,299
numpy is basically the same thing except

00:22:24,450 --> 00:22:30,179
it does it lets you do very very

00:22:27,299 --> 00:22:33,059
complicated operations on them which is

00:22:30,179 --> 00:22:39,630
necessary for just me reinforcement

00:22:33,059 --> 00:22:42,299
learning in general pandas

00:22:39,630 --> 00:22:44,429
okay so pandas you don't you don't even

00:22:42,299 --> 00:22:46,200
need to do it for machine learning so

00:22:44,429 --> 00:22:49,559
pandas you can have literally an entire

00:22:46,200 --> 00:22:51,390
CSV as a variable and like you take a

00:22:49,559 --> 00:22:56,340
CSV shove it into a variable and do

00:22:51,390 --> 00:22:57,960
operations on it in memory so so I

00:22:56,340 --> 00:23:00,270
usually try to think about like whether

00:22:57,960 --> 00:23:03,630
I need to use Excel or Google sheets

00:23:00,270 --> 00:23:05,400
versus using pandas so this is an

00:23:03,630 --> 00:23:07,140
example of actually reading in what is

00:23:05,400 --> 00:23:09,360
called a data frame and so the data

00:23:07,140 --> 00:23:12,820
frame has all your columns all the all

00:23:09,360 --> 00:23:15,519
the values in each column

00:23:12,820 --> 00:23:17,109
so this is extremely useful in machine

00:23:15,519 --> 00:23:27,369
learning but also in data science in

00:23:17,109 --> 00:23:30,549
general ok so these are some really

00:23:27,369 --> 00:23:33,070
really popular data science libraries

00:23:30,549 --> 00:23:37,749
that are available tense flow is

00:23:33,070 --> 00:23:41,070
currently developed by Google pi torches

00:23:37,749 --> 00:23:45,519
develop my facebook then that fast AI is

00:23:41,070 --> 00:23:47,229
built on top of pi torch but fast de-ice

00:23:45,519 --> 00:23:50,379
goal is if you don't have a background

00:23:47,229 --> 00:23:52,029
in math you should be able to do a fast

00:23:50,379 --> 00:23:55,899
AI and actually toss up some very

00:23:52,029 --> 00:23:59,320
complicated models without having to

00:23:55,899 --> 00:24:01,989
have really detailed knowledge of of

00:23:59,320 --> 00:24:03,309
math or like matrix operations but

00:24:01,989 --> 00:24:05,259
tensor flow and PI torch they're a

00:24:03,309 --> 00:24:09,609
little bit more middle low-level

00:24:05,259 --> 00:24:12,039
libraries which I work a lot with PI

00:24:09,609 --> 00:24:14,200
torch I started off with tensor flow but

00:24:12,039 --> 00:24:16,200
I had a hard time with it it's

00:24:14,200 --> 00:24:20,369
documentation is really challenging

00:24:16,200 --> 00:24:20,369
there's some other big differences also

00:24:20,639 --> 00:24:28,869
so so some common features of these

00:24:27,429 --> 00:24:30,549
frameworks so they're all using

00:24:28,869 --> 00:24:34,349
something called a tensor and all that

00:24:30,549 --> 00:24:38,499
tensor is is just a 2d matrix that's a

00:24:34,349 --> 00:24:40,479
rectangle um I stripped this this off of

00:24:38,499 --> 00:24:45,820
tensor flow website describing what a

00:24:40,479 --> 00:24:48,779
tensor is I don't I don't really that

00:24:45,820 --> 00:24:53,979
doesn't really help me definition wise

00:24:48,779 --> 00:24:57,219
so just a better definition of what a

00:24:53,979 --> 00:25:00,759
tensor is is just a tensor is just a

00:24:57,219 --> 00:25:02,289
matrix that is rectangular so at least

00:25:00,759 --> 00:25:04,149
in machine learning that's what it is so

00:25:02,289 --> 00:25:05,979
if you go until like a physicist or

00:25:04,149 --> 00:25:08,469
someone who has a physics background or

00:25:05,979 --> 00:25:09,999
engineering background I might harshly

00:25:08,469 --> 00:25:13,269
disagree with you on what a tensor is

00:25:09,999 --> 00:25:15,999
but in machine learning it's just a

00:25:13,269 --> 00:25:19,839
array that is right that is rectangular

00:25:15,999 --> 00:25:21,909
or or cubic or something like that so

00:25:19,839 --> 00:25:25,149
you can see here so all the way at the

00:25:21,909 --> 00:25:26,140
bottom both at the bottom and also the

00:25:25,149 --> 00:25:28,720
middle numpy

00:25:26,140 --> 00:25:30,250
so those are what you call like in just

00:25:28,720 --> 00:25:33,490
computer science just ragged arrays

00:25:30,250 --> 00:25:36,730
they're just like eat like row zero

00:25:33,490 --> 00:25:41,020
might be just to row three might be five

00:25:36,730 --> 00:25:43,500
row four might be just eighteen like

00:25:41,020 --> 00:25:46,470
size 18

00:25:43,500 --> 00:25:48,670
so if ya if you have an array that has

00:25:46,470 --> 00:25:50,950
inside a different lists of different

00:25:48,670 --> 00:25:52,900
lengths it just becomes a ragged ray and

00:25:50,950 --> 00:25:54,640
you can't do basically if you have a

00:25:52,900 --> 00:25:57,730
ragged ray you can't do basic matrix

00:25:54,640 --> 00:25:59,260
operations on it then so they usually

00:25:57,730 --> 00:26:02,200
have to be they have to be rectangular

00:25:59,260 --> 00:26:05,860
or cubic or whatever up and dimension

00:26:02,200 --> 00:26:07,450
that they are so so that's all that

00:26:05,860 --> 00:26:10,240
tensor is so when you see tensor

00:26:07,450 --> 00:26:12,280
that's it took me about it took me about

00:26:10,240 --> 00:26:17,950
eight months to realize what it was I

00:26:12,280 --> 00:26:20,530
just ignored it so so yeah and also with

00:26:17,950 --> 00:26:25,500
these libraries like like pi torch and

00:26:20,530 --> 00:26:27,669
tensor flow at the bottom is PI torch

00:26:25,500 --> 00:26:31,299
tints flow in pipe or tensors they

00:26:27,669 --> 00:26:33,910
actually look a lot like numpy arrays or

00:26:31,299 --> 00:26:36,610
like just regular lists and they usually

00:26:33,910 --> 00:26:38,830
ask similar functions the biggest

00:26:36,610 --> 00:26:41,020
difference between tensors and umpire

00:26:38,830 --> 00:26:44,380
raises that these frameworks will go and

00:26:41,020 --> 00:26:46,750
automatically actually decide whether or

00:26:44,380 --> 00:26:49,660
not this operation should be executed on

00:26:46,750 --> 00:26:52,419
GPU or CPU and then also what order to

00:26:49,660 --> 00:26:55,120
go and execute those those operations so

00:26:52,419 --> 00:26:57,520
a lot of this is being automated in fact

00:26:55,120 --> 00:27:01,110
like a lot of Pi torch and tensor flow

00:26:57,520 --> 00:27:03,549
one of the biggest powers for them is

00:27:01,110 --> 00:27:05,470
they'll automatically figure out what

00:27:03,549 --> 00:27:06,940
what operations can be X U at the same

00:27:05,470 --> 00:27:13,570
time so you don't have to think about

00:27:06,940 --> 00:27:15,580
that at all yeah so that's an example of

00:27:13,570 --> 00:27:17,020
like the actual in actual graph so this

00:27:15,580 --> 00:27:21,669
is a very very complicated model that's

00:27:17,020 --> 00:27:22,600
being visualized but what this allows

00:27:21,669 --> 00:27:23,980
tensorflow

00:27:22,600 --> 00:27:27,070
so this is an example of a tensor flow

00:27:23,980 --> 00:27:29,350
model tensile flow can automatically

00:27:27,070 --> 00:27:30,790
decide which one of those can be execute

00:27:29,350 --> 00:27:34,059
at the same time just should be

00:27:30,790 --> 00:27:38,230
distribute it across which GPUs at the

00:27:34,059 --> 00:27:40,210
same time and can you even be escalate

00:27:38,230 --> 00:27:42,940
up to entire cluster so you can just

00:27:40,210 --> 00:27:44,470
distributed across larger numbers of

00:27:42,940 --> 00:27:46,179
different machines and it'll just figure

00:27:44,470 --> 00:27:49,029
it out on its own

00:27:46,179 --> 00:27:51,100
so that's like when I think about PI

00:27:49,029 --> 00:27:53,470
torts and tensor flow they really talk

00:27:51,100 --> 00:27:58,860
about their deep learning libraries but

00:27:53,470 --> 00:28:01,059
they're really parallelization libraries

00:27:58,860 --> 00:28:03,100
they're really just deciding on what

00:28:01,059 --> 00:28:05,379
what order to go and do these operations

00:28:03,100 --> 00:28:06,580
and the reason why this is important is

00:28:05,379 --> 00:28:08,730
because some of these models can take

00:28:06,580 --> 00:28:11,679
weeks actually to Train so if you have

00:28:08,730 --> 00:28:13,749
if you have something like someone like

00:28:11,679 --> 00:28:16,629
Google or Facebook trying to train on

00:28:13,749 --> 00:28:18,429
faces or something like that

00:28:16,629 --> 00:28:20,289
they need to be able to go and utilize

00:28:18,429 --> 00:28:21,789
their computing resources the best that

00:28:20,289 --> 00:28:24,999
they can and so doing something like

00:28:21,789 --> 00:28:26,649
this makes it super easy and they made

00:28:24,999 --> 00:28:31,659
this open-source so all of us can go and

00:28:26,649 --> 00:28:33,749
leverage this okay so I have some

00:28:31,659 --> 00:28:36,369
examples of some Jupiter notebooks

00:28:33,749 --> 00:28:39,190
we'll see how far we get with them I'm

00:28:36,369 --> 00:28:40,629
probably only gonna do the first one I'm

00:28:39,190 --> 00:28:42,429
gonna probably only do them a gem now

00:28:40,629 --> 00:28:45,399
this one's on this one's partially done

00:28:42,429 --> 00:28:47,289
and airily I don't have time to finish

00:28:45,399 --> 00:28:52,480
that one but this one just shows it can

00:28:47,289 --> 00:28:56,529
go and learn a sine wave so just switch

00:28:52,480 --> 00:29:00,940
over to that this is an image one right

00:28:56,529 --> 00:29:02,889
okay okay so there's this website called

00:29:00,940 --> 00:29:06,159
kaggle so if you're in data science at

00:29:02,889 --> 00:29:07,960
all you know that caracal has live data

00:29:06,159 --> 00:29:10,629
science but if you're just getting into

00:29:07,960 --> 00:29:12,700
it I would definitely go to Kaggle look

00:29:10,629 --> 00:29:15,519
at they have huge numbers of datasets

00:29:12,700 --> 00:29:19,899
there's one day is that the head that

00:29:15,519 --> 00:29:23,470
had data collected from Catan games on

00:29:19,899 --> 00:29:24,519
like what like who won or lost and so

00:29:23,470 --> 00:29:26,649
you can train a machine learning model

00:29:24,519 --> 00:29:28,299
to go and predict what is the what is

00:29:26,649 --> 00:29:31,659
the best move or something like that if

00:29:28,299 --> 00:29:33,190
you want to kind of crazy umm but you

00:29:31,659 --> 00:29:36,190
can get everything from stocks to image

00:29:33,190 --> 00:29:38,499
datasets all that stuff what we're gonna

00:29:36,190 --> 00:29:42,789
get is something called an amnesty to

00:29:38,499 --> 00:29:43,869
set I don't remember why it's called M

00:29:42,789 --> 00:29:46,330
Ness I don't know what it stands for

00:29:43,869 --> 00:29:48,490
it's a it's an image data set that has a

00:29:46,330 --> 00:29:50,350
lot of pictures of numbers and so your

00:29:48,490 --> 00:29:53,379
goal is to go and train your neural net

00:29:50,350 --> 00:29:53,960
to actually recognize and classify

00:29:53,379 --> 00:29:58,369
numbers

00:29:53,960 --> 00:30:01,039
so I kind of walked through how to strip

00:29:58,369 --> 00:30:10,129
them out but you're gonna go put them in

00:30:01,039 --> 00:30:11,419
to see us these CSVs and yeah and so

00:30:10,129 --> 00:30:16,879
then we go and put them into your data

00:30:11,419 --> 00:30:19,639
frame so they generally will looks like

00:30:16,879 --> 00:30:21,619
this so the first the first column in

00:30:19,639 --> 00:30:23,809
the data frame is labeled five and as

00:30:21,619 --> 00:30:26,659
all the actual like Y labels for your

00:30:23,809 --> 00:30:31,309
whether that picture is a four or five

00:30:26,659 --> 00:30:36,379
or six or seven all the other columns

00:30:31,309 --> 00:30:39,980
are the pixels it's kind of complicated

00:30:36,379 --> 00:30:44,389
but I'm I'll probably just breeze

00:30:39,980 --> 00:30:46,639
through this and this also so I use I

00:30:44,389 --> 00:30:48,289
use a pycharm IDE and what it lets you

00:30:46,639 --> 00:30:51,200
do is you can actually go and peek into

00:30:48,289 --> 00:30:54,830
a data frame and so this is actually

00:30:51,200 --> 00:30:56,840
what it looks like so these are your so

00:30:54,830 --> 00:30:58,879
yeah so it's just its PyCharm I think

00:30:56,840 --> 00:31:05,450
they have a community they have a

00:30:58,879 --> 00:31:07,519
community IDE but so this is so this

00:31:05,450 --> 00:31:10,850
column is labeled five these are the

00:31:07,519 --> 00:31:16,629
actual labels so like Row one has a

00:31:10,850 --> 00:31:21,110
picture of a four and it's kind of ugly

00:31:16,629 --> 00:31:25,669
because we don't see a picture here just

00:31:21,110 --> 00:31:30,200
see a bunch of zeros but actually if you

00:31:25,669 --> 00:31:33,259
go and rearrange them if you go and

00:31:30,200 --> 00:31:38,860
follow all that we're doing where do I

00:31:33,259 --> 00:31:38,860
have it I thought I showed

00:31:39,790 --> 00:31:50,390
okay so we do it there so you have to go

00:31:45,470 --> 00:31:56,150
and wrap the pixels around so each row

00:31:50,390 --> 00:31:58,250
is has 785 columns we go up here so

00:31:56,150 --> 00:32:00,740
there are 785 columns that these going

00:31:58,250 --> 00:32:02,780
all the way to the right the first

00:32:00,740 --> 00:32:05,360
column is five so the rest of the

00:32:02,780 --> 00:32:11,570
columns are just just talking about

00:32:05,360 --> 00:32:14,120
image pixels and and so like the image

00:32:11,570 --> 00:32:16,940
pixels there are 784 image pixels and

00:32:14,120 --> 00:32:18,500
you can go and reshape some 28 by 28 and

00:32:16,940 --> 00:32:22,610
you actually get whatever the image

00:32:18,500 --> 00:32:25,370
looks like so in this data set the

00:32:22,610 --> 00:32:27,980
leftmost column has the has the actual

00:32:25,370 --> 00:32:30,050
label and then everything and then for

00:32:27,980 --> 00:32:32,720
each row everything after is just a flat

00:32:30,050 --> 00:32:33,620
image and so your goal so if you want to

00:32:32,720 --> 00:32:37,100
go and actually see what that image

00:32:33,620 --> 00:32:40,160
looks like you just call the reshape the

00:32:37,100 --> 00:32:43,160
reshape method and as long as you know

00:32:40,160 --> 00:32:46,430
what the original image is shape is you

00:32:43,160 --> 00:32:48,680
shouldn't have any issues 784 can be

00:32:46,430 --> 00:32:53,930
wrapped into a 28 by 28 image and that's

00:32:48,680 --> 00:32:57,020
why we're able to see this yeah what I'm

00:32:53,930 --> 00:32:59,360
a little bit mixed about this because

00:32:57,020 --> 00:33:02,330
the reshape method and stuff like that

00:32:59,360 --> 00:33:04,430
was kind of mind-blowing so if you don't

00:33:02,330 --> 00:33:07,160
get it right now that's completely fine

00:33:04,430 --> 00:33:09,580
that took me a while to figure out yeah

00:33:07,160 --> 00:33:12,290
it was pretty mind-blowing to me but

00:33:09,580 --> 00:33:14,360
hopefully I mean you can go on you can

00:33:12,290 --> 00:33:15,770
go and look at this on on my github if

00:33:14,360 --> 00:33:20,480
your compute confused by something I

00:33:15,770 --> 00:33:22,370
definitely do an issue or PR but so we

00:33:20,480 --> 00:33:24,560
have that but we're gonna just be feet

00:33:22,370 --> 00:33:27,020
we're just gonna feed in a flat image so

00:33:24,560 --> 00:33:28,490
if you have your own custom data set and

00:33:27,020 --> 00:33:30,440
you're just doing a regular linear

00:33:28,490 --> 00:33:32,540
neural net just make the image really

00:33:30,440 --> 00:33:35,780
really flat just plan out all the

00:33:32,540 --> 00:33:38,510
picture pixels numpy makes this easy you

00:33:35,780 --> 00:33:41,690
just called numpy the numpy array dot

00:33:38,510 --> 00:33:46,910
flatten and it'll just flatten the image

00:33:41,690 --> 00:33:49,570
all the way out yeah so I talked about

00:33:46,910 --> 00:33:49,570
more of that

00:33:51,290 --> 00:34:05,130
okay so this yeah yeah yep

00:34:02,040 --> 00:34:06,930
yeah so if you have a pic picture you're

00:34:05,130 --> 00:34:10,340
just gonna take first row shove it that

00:34:06,930 --> 00:34:12,270
way the next row shove it that way just

00:34:10,340 --> 00:34:14,190
flatten them out so it's literally

00:34:12,270 --> 00:34:15,990
flattened and numpy makes this easy so

00:34:14,190 --> 00:34:17,970
you don't have to manually do that you

00:34:15,990 --> 00:34:19,950
just call dot flatten and it'll just

00:34:17,970 --> 00:34:22,260
land them out and then you could just do

00:34:19,950 --> 00:34:24,330
reshape and where the the images

00:34:22,260 --> 00:34:26,580
original dimensions were it'll just

00:34:24,330 --> 00:34:29,070
reshape them back it's kind of

00:34:26,580 --> 00:34:31,350
mind-blowing it's it's like it took me a

00:34:29,070 --> 00:34:32,909
while to get comfortable with it and

00:34:31,350 --> 00:34:34,320
there's actually a lot of different ways

00:34:32,909 --> 00:34:38,909
they could just flatten images and

00:34:34,320 --> 00:34:41,940
reshape them so so if you go and make

00:34:38,909 --> 00:34:44,540
them flat then what you're doing then is

00:34:41,940 --> 00:34:48,450
similar to if you just had a regular CSV

00:34:44,540 --> 00:34:50,970
so if you had a CSV of like like if you

00:34:48,450 --> 00:34:54,780
wanted if you if you had a CSV filled

00:34:50,970 --> 00:34:57,030
with students study hours and that

00:34:54,780 --> 00:34:59,930
number of hours that they slept and

00:34:57,030 --> 00:35:02,190
you're trying to predict their average

00:34:59,930 --> 00:35:09,360
grade point average or something like

00:35:02,190 --> 00:35:12,450
that then instead of 784 you would just

00:35:09,360 --> 00:35:14,070
be feeding into you'd be feeding in two

00:35:12,450 --> 00:35:15,450
features which is their the number of

00:35:14,070 --> 00:35:17,850
hours they studied in the number of

00:35:15,450 --> 00:35:20,310
hours that they slept and then your

00:35:17,850 --> 00:35:26,850
label is just what their grade point

00:35:20,310 --> 00:35:36,930
average is yeah so we go and load all

00:35:26,850 --> 00:35:38,640
that thing I do I do 200 okay yeah so

00:35:36,930 --> 00:35:44,880
this is whether our our neural net looks

00:35:38,640 --> 00:35:52,580
like so the X dot shape so right now

00:35:44,880 --> 00:35:54,690
we're pulling in pulling in 200 rows

00:35:52,580 --> 00:35:57,260
we're gonna go and pretend that we only

00:35:54,690 --> 00:36:02,060
pulled in five just so this makes sense

00:35:57,260 --> 00:36:05,869
so if you go go and pull in five then

00:36:02,060 --> 00:36:09,480
excess shape is going to be 5 and 784

00:36:05,869 --> 00:36:14,550
meaning you have 5 rows and then each

00:36:09,480 --> 00:36:18,089
row there are 784 pixels so we have only

00:36:14,550 --> 00:36:28,460
we have 5 images and then lie shape oh

00:36:18,089 --> 00:36:35,270
yeah I blew past this oh yeah okay yeah

00:36:28,460 --> 00:36:38,250
go and okay so we have picked we have

00:36:35,270 --> 00:36:41,099
now for each row we have a bunch of

00:36:38,250 --> 00:36:43,859
pixels but our Y is going to be some

00:36:41,099 --> 00:36:46,050
kind of label so it's going to be some

00:36:43,859 --> 00:36:51,030
where it's going to be either a1 a0 a1

00:36:46,050 --> 00:36:53,730
a2 all the way up to 9 now neural Nets

00:36:51,030 --> 00:36:56,750
need to have their data from a scale of

00:36:53,730 --> 00:37:00,240
0 to 1 and so you have two alternatives

00:36:56,750 --> 00:37:02,310
to normalize your data pixels are really

00:37:00,240 --> 00:37:07,560
easy to normalize all of them are always

00:37:02,310 --> 00:37:07,830
on a scale of 0 to 255 which I do do I

00:37:07,560 --> 00:37:10,890
do

00:37:07,830 --> 00:37:13,410
yeah I do down here so like X is just a

00:37:10,890 --> 00:37:15,930
list of pick images and you just divide

00:37:13,410 --> 00:37:20,570
them by 255 and now they're a scale of 0

00:37:15,930 --> 00:37:27,890
to 1 so images are very easy to do that

00:37:20,570 --> 00:37:29,880
regular tabular data is a lot harder but

00:37:27,890 --> 00:37:34,020
for labels you have to do something

00:37:29,880 --> 00:37:43,580
called one hotting so I don't know how

00:37:34,020 --> 00:37:43,580
to explain this but try my best

00:37:53,460 --> 00:38:02,400
oh I can try my best so so why is has is

00:37:59,220 --> 00:38:05,280
going to be one of ten labels it's going

00:38:02,400 --> 00:38:07,309
to be zero to nine so we have ten

00:38:05,280 --> 00:38:11,339
classes that we want to go and predict

00:38:07,309 --> 00:38:14,880
so we're gonna go and make some kind of

00:38:11,339 --> 00:38:19,790
array that is ten classes wide this

00:38:14,880 --> 00:38:19,790
should be five actually I'll go and just

00:38:20,420 --> 00:38:35,910
yeah we'll just run that yeah so it's

00:38:31,440 --> 00:38:38,369
five cool so why is going to have so we

00:38:35,910 --> 00:38:42,740
have five pictures and then why is gonna

00:38:38,369 --> 00:38:45,930
have be either one of ten classes and

00:38:42,740 --> 00:38:51,300
when hotting just means that we're gonna

00:38:45,930 --> 00:38:56,220
put ones wherever Y is so this is also

00:38:51,300 --> 00:38:58,559
kind of complicated so we translate a Y

00:38:56,220 --> 00:39:00,510
from just being like full numbers to

00:38:58,559 --> 00:39:06,990
just as yours or ones you're just making

00:39:00,510 --> 00:39:08,940
a binary so for example if actually the

00:39:06,990 --> 00:39:11,569
first row Y so the first the first

00:39:08,940 --> 00:39:14,400
picture is actually a picture of a zero

00:39:11,569 --> 00:39:21,500
so you're going to put a 1 in index 0

00:39:14,400 --> 00:39:21,500
and then picture - that's a picture of a

00:39:22,099 --> 00:39:29,490
think this picture of a for ya Row 2 is

00:39:25,920 --> 00:39:34,859
a picture of a 4 so we're gonna put a

00:39:29,490 --> 00:39:37,380
number at index 4 and then Row 3 is a

00:39:34,859 --> 00:39:42,210
picture of a 1 so you're gonna put a 1

00:39:37,380 --> 00:39:46,589
at index 1 so this is just a strategy to

00:39:42,210 --> 00:39:48,869
make your data in a range from 0 to 1 I

00:39:46,589 --> 00:39:52,680
don't know if that makes sense but

00:39:48,869 --> 00:39:54,299
that's that's one of that's one of a few

00:39:52,680 --> 00:39:56,280
ways to go in action normalize your data

00:39:54,299 --> 00:40:01,670
and it's pretty common for actual like

00:39:56,280 --> 00:40:01,670
class based image classification so

00:40:08,840 --> 00:40:14,420
I don't know does anyone have any

00:40:10,610 --> 00:40:17,420
questions about this yeah yeah

00:40:14,420 --> 00:40:21,380
so we've were we're still we're still

00:40:17,420 --> 00:40:23,030
doing basically where we're taking our

00:40:21,380 --> 00:40:25,820
data and cleaning it before even feeding

00:40:23,030 --> 00:40:29,900
it into a model so we haven't done any

00:40:25,820 --> 00:40:36,760
deep learning yet yeah yeah so we're

00:40:29,900 --> 00:40:47,240
preparing this for our model yeah yeah

00:40:36,760 --> 00:40:49,190
yeah yeah yeah so you can't just fee if

00:40:47,240 --> 00:40:50,930
you feed in the okay what if one of

00:40:49,190 --> 00:40:53,930
these numbers was a picture of ten

00:40:50,930 --> 00:40:56,180
billion right you don't want to feed the

00:40:53,930 --> 00:40:58,190
literal value of ten billion into your

00:40:56,180 --> 00:41:02,150
neural net it will just throw it

00:40:58,190 --> 00:41:04,040
completely off so if you normalize them

00:41:02,150 --> 00:41:07,610
from zero to one it keeps it really

00:41:04,040 --> 00:41:09,980
really nice it trains a lot better um I

00:41:07,610 --> 00:41:11,120
don't know how else to explain it like

00:41:09,980 --> 00:41:13,160
if you're going and actually training

00:41:11,120 --> 00:41:15,260
your own own model and you're wondering

00:41:13,160 --> 00:41:17,320
why it's not working I would go and

00:41:15,260 --> 00:41:19,880
check whether they're normalized or not

00:41:17,320 --> 00:41:22,910
because if they're not you're gonna have

00:41:19,880 --> 00:41:24,290
very very weird results like I might

00:41:22,910 --> 00:41:24,620
work sometimes and then sometimes it

00:41:24,290 --> 00:41:26,150
won't

00:41:24,620 --> 00:41:29,870
it'll be really really weird so your

00:41:26,150 --> 00:41:31,370
goal both your X and your Y the data

00:41:29,870 --> 00:41:33,230
you're feeding in and the data you're

00:41:31,370 --> 00:41:37,640
comparing against they all need to be

00:41:33,230 --> 00:41:39,770
zero to one so this is one strategy that

00:41:37,640 --> 00:41:42,110
you can use to make Y from zero to one

00:41:39,770 --> 00:41:44,740
and this is really common this isn't

00:41:42,110 --> 00:41:44,740
something weird

00:41:53,680 --> 00:42:10,420
oh um oh I could feed him I could feed

00:42:07,510 --> 00:42:12,190
him more like you could feed on like 20

00:42:10,420 --> 00:42:14,079
or 30 I think originally I was just

00:42:12,190 --> 00:42:16,359
doing uh I was doing five just because

00:42:14,079 --> 00:42:19,539
it's easier for you to see because if

00:42:16,359 --> 00:42:23,049
you do 200 like 200 different images of

00:42:19,539 --> 00:42:24,520
numbers then there's just gonna be a

00:42:23,049 --> 00:42:34,539
massive cluster that you're not gonna be

00:42:24,520 --> 00:42:36,190
able to read so yeah like these are yeah

00:42:34,539 --> 00:42:37,869
and I'll I probably should actually just

00:42:36,190 --> 00:42:39,250
jump to that just so it's a little bit

00:42:37,869 --> 00:42:42,339
easier I'm gonna just go ahead and run

00:42:39,250 --> 00:42:43,480
on this all again just you can see it's

00:42:42,339 --> 00:42:46,289
like yeah I probably should

00:42:43,480 --> 00:42:46,289
I just show

00:42:54,660 --> 00:42:59,759
okay so like this was this was this is

00:42:57,990 --> 00:43:02,309
an example of what they all look like so

00:42:59,759 --> 00:43:04,890
yeah so you might have 200 images but

00:43:02,309 --> 00:43:10,170
they're 200 images of different numbers

00:43:04,890 --> 00:43:16,170
and how they're written from 0 to 9 so

00:43:10,170 --> 00:43:17,609
this is what the data set looks like so

00:43:16,170 --> 00:43:21,960
like this is what our model actually

00:43:17,609 --> 00:43:26,009
predicting what they are it is pretty

00:43:21,960 --> 00:43:28,049
good here but then further down these

00:43:26,009 --> 00:43:31,529
are images that hasn't seen before it

00:43:28,049 --> 00:43:33,750
still does an okay job it should get one

00:43:31,529 --> 00:43:35,579
of these wrong oh yeah yeah so like for

00:43:33,750 --> 00:43:38,309
example it gets this one wrong so

00:43:35,579 --> 00:43:45,539
apparently that's actually an 8 but I

00:43:38,309 --> 00:43:49,440
think said it's a 3 yeah yeah so yeah

00:43:45,539 --> 00:43:51,329
yeah so like that's one of the that's

00:43:49,440 --> 00:43:53,609
one of those things where it got it

00:43:51,329 --> 00:43:57,119
wrong but it's also like even as a human

00:43:53,609 --> 00:44:00,170
it's kind of confusing so it kind of

00:43:57,119 --> 00:44:00,170
makes sense why I would get it wrong

00:44:09,680 --> 00:44:16,680
yeah yeah probably yeah so one thing

00:44:14,910 --> 00:44:18,870
that we could do with this is we could

00:44:16,680 --> 00:44:20,510
do everything from image augmentation we

00:44:18,870 --> 00:44:22,560
can make our model a little bit deeper

00:44:20,510 --> 00:44:24,900
we could also give it more examples

00:44:22,560 --> 00:44:27,240
there are 60,000 pictures in this data

00:44:24,900 --> 00:44:30,060
set so you could feed an else you can

00:44:27,240 --> 00:44:32,640
feed in 60,000 of them but I don't want

00:44:30,060 --> 00:44:36,900
to wait too long so that's gonna be on

00:44:32,640 --> 00:44:40,080
your own time also the other issue is

00:44:36,900 --> 00:44:45,780
that we're so yeah so we have that and

00:44:40,080 --> 00:44:47,730
what our model actually looks like like

00:44:45,780 --> 00:44:52,380
this is what our model looks like so

00:44:47,730 --> 00:44:53,820
that's our neural nut it's pretty

00:44:52,380 --> 00:44:55,410
straightforward if you're gonna do

00:44:53,820 --> 00:44:57,750
60,000 you'll realize that you'll run

00:44:55,410 --> 00:45:00,510
into an issue it works on small data

00:44:57,750 --> 00:45:02,270
sets but when you get into much larger

00:45:00,510 --> 00:45:06,030
ones and much more complicated

00:45:02,270 --> 00:45:09,660
representations it'll just go kind of

00:45:06,030 --> 00:45:16,200
slow I'm not going to actually try on

00:45:09,660 --> 00:45:19,500
all 60,000 though so so for example so

00:45:16,200 --> 00:45:22,890
originally we were doing five pictures

00:45:19,500 --> 00:45:26,810
right so five pictures they could be any

00:45:22,890 --> 00:45:31,770
of the numbers zero to nine one of those

00:45:26,810 --> 00:45:36,540
so shape index one this is it's

00:45:31,770 --> 00:45:38,820
basically putting an 784 so you put 784

00:45:36,540 --> 00:45:44,940
here that is our input size and then

00:45:38,820 --> 00:45:51,330
this is our hidden layer size so 128 and

00:45:44,940 --> 00:45:56,790
then outputting 16 so so I think that

00:45:51,330 --> 00:45:59,970
this one has 128 neurons yeah yeah this

00:45:56,790 --> 00:46:01,350
one has 128 neurons and this one has 16

00:45:59,970 --> 00:46:03,890
neurons and then you're getting some

00:46:01,350 --> 00:46:03,890
kind of output

00:46:06,620 --> 00:46:23,700
no that's fine yeah yeah so when you

00:46:13,080 --> 00:46:27,660
make it flat like so that translates to

00:46:23,700 --> 00:46:31,440
28 by 28 yeah so like what I showed

00:46:27,660 --> 00:46:35,490
earlier yeah so when you reshape it yeah

00:46:31,440 --> 00:46:38,160
when you do that reshape 28 by 28 that's

00:46:35,490 --> 00:46:41,160
what it's 784 which means if you made

00:46:38,160 --> 00:46:42,900
this 785 you're not gonna be able it'll

00:46:41,160 --> 00:46:45,350
crack it like if you try doing reshape

00:46:42,900 --> 00:46:49,380
it'll crash but they can't it can't

00:46:45,350 --> 00:46:50,970
contort that part of the part of the

00:46:49,380 --> 00:46:52,530
mind-blowing thing about the reshape

00:46:50,970 --> 00:47:06,720
method is kind of it was really

00:46:52,530 --> 00:47:16,020
confusing for me so um you have 200 the

00:47:06,720 --> 00:47:19,200
actual size yeah yeah yeah so that's the

00:47:16,020 --> 00:47:23,250
other confusing thing with linear layers

00:47:19,200 --> 00:47:25,440
it does not care if like you have a

00:47:23,250 --> 00:47:27,300
pixel little if you have if you have a

00:47:25,440 --> 00:47:33,450
bunch of pixels and like that are close

00:47:27,300 --> 00:47:38,220
to each other like oh that's a good

00:47:33,450 --> 00:47:40,710
example it's like a 4 so if you have

00:47:38,220 --> 00:47:43,950
like a pixels on the left here that lit

00:47:40,710 --> 00:47:47,670
up linear layer is not smart enough to

00:47:43,950 --> 00:47:51,450
care about pixels that are close to each

00:47:47,670 --> 00:47:54,210
other like yeah it does not care so

00:47:51,450 --> 00:47:57,000
that's why when we flatten it it doesn't

00:47:54,210 --> 00:47:59,730
make a difference because a linear layer

00:47:57,000 --> 00:48:01,500
just really it doesn't care if they're

00:47:59,730 --> 00:48:04,830
close to each other cuz it will just

00:48:01,500 --> 00:48:07,410
figure it out on its own anyways it's

00:48:04,830 --> 00:48:08,880
kind of if you're confused

00:48:07,410 --> 00:48:10,770
that's normal because that would

00:48:08,880 --> 00:48:12,810
confuses me if you actually look at the

00:48:10,770 --> 00:48:15,240
neurons when they're activated it's just

00:48:12,810 --> 00:48:17,760
no it looks like noise you won't see in

00:48:15,240 --> 00:48:19,550
a for inside that neural net like you

00:48:17,760 --> 00:48:22,010
won't see that representation

00:48:19,550 --> 00:48:24,260
because it's just the neurons do not

00:48:22,010 --> 00:48:29,450
care where they are it's just toggling

00:48:24,260 --> 00:48:31,550
them to go and reduce whatever whatever

00:48:29,450 --> 00:48:50,830
loss it has with like it's incorrect

00:48:31,550 --> 00:49:08,000
predictions yeah yeah yeah yeah yeah

00:48:50,830 --> 00:49:11,180
yeah yeah yeah yeah yeah yeah yeah it's

00:49:08,000 --> 00:49:13,070
almost it's basically magic which is one

00:49:11,180 --> 00:49:18,680
reason why one of the challenges with

00:49:13,070 --> 00:49:24,440
deep learning is when a no that's fine

00:49:18,680 --> 00:49:27,620
the the biggest problem with oh I was

00:49:24,440 --> 00:49:30,520
gonna say magic makes sense also the

00:49:27,620 --> 00:49:33,080
problem with deep learning is that

00:49:30,520 --> 00:49:36,530
neural nets are called black boxes for a

00:49:33,080 --> 00:49:39,740
reason if your neural net is not working

00:49:36,530 --> 00:49:42,440
properly or it's massively screwing up

00:49:39,740 --> 00:49:45,590
it's extremely hard for you to figure

00:49:42,440 --> 00:49:46,670
out what's wrong with it because if you

00:49:45,590 --> 00:49:48,110
actually look at the weights it just

00:49:46,670 --> 00:49:49,790
looks like noise I should have I should

00:49:48,110 --> 00:49:51,800
have visualized it but I haven't I

00:49:49,790 --> 00:49:53,660
didn't do that here it would just look

00:49:51,800 --> 00:49:57,140
like random white noise if you try to

00:49:53,660 --> 00:50:03,770
get the image out directly out of it but

00:49:57,140 --> 00:50:06,350
there there is a something called a back

00:50:03,770 --> 00:50:09,470
up here so instead of linear layer so

00:50:06,350 --> 00:50:11,540
maybe you want to care about the shape

00:50:09,470 --> 00:50:13,550
of the image or whether a pixel is next

00:50:11,540 --> 00:50:15,410
to another pixel like that's more

00:50:13,550 --> 00:50:17,390
intuitive to you and it's intuitive to

00:50:15,410 --> 00:50:19,940
me right so it's kind of ridiculous that

00:50:17,390 --> 00:50:23,060
a linear layer really doesn't care about

00:50:19,940 --> 00:50:25,990
the locations of your items so there's

00:50:23,060 --> 00:50:28,490
something called a convolutional

00:50:25,990 --> 00:50:30,830
convolutional net so instead of linear

00:50:28,490 --> 00:50:32,330
you you would do something called comm

00:50:30,830 --> 00:50:35,300
2d

00:50:32,330 --> 00:50:38,510
and what that is instead of a flat image

00:50:35,300 --> 00:50:40,850
you'll actually feed in a 28 by 28 image

00:50:38,510 --> 00:50:44,840
and set into it and it's called a calm

00:50:40,850 --> 00:50:46,730
2d layer and that if you actually like

00:50:44,840 --> 00:50:49,790
rip it out it might look something like

00:50:46,730 --> 00:50:51,620
the number you're looking for is no

00:50:49,790 --> 00:50:53,780
guarantee that will look like whatever

00:50:51,620 --> 00:50:57,520
you're looking at but it might look kind

00:50:53,780 --> 00:50:59,810
of something like it like people use a

00:50:57,520 --> 00:51:01,670
convolutional neural nets extensively

00:50:59,810 --> 00:51:05,480
and like finding faces and stuff like

00:51:01,670 --> 00:51:07,520
that and if you open up a convolutional

00:51:05,480 --> 00:51:09,200
net that's been trained on faces you

00:51:07,520 --> 00:51:12,740
might see something about like eyes and

00:51:09,200 --> 00:51:14,240
nose positions but once again it's not

00:51:12,740 --> 00:51:18,560
gonna look like what you want it to be

00:51:14,240 --> 00:51:19,820
it's going to be very very weird it's

00:51:18,560 --> 00:51:22,760
really cool to actually see them like

00:51:19,820 --> 00:51:24,650
see their activations but right now that

00:51:22,760 --> 00:51:26,390
I just did a linear cuz this is probably

00:51:24,650 --> 00:52:01,370
easiest neural net you could possibly

00:51:26,390 --> 00:52:10,540
put together yeah yeah it could be seen

00:52:01,370 --> 00:52:12,560
something like that yeah yeah yeah it's

00:52:10,540 --> 00:52:14,540
yeah the whole idea is that you don't

00:52:12,560 --> 00:52:15,950
really know what it's doing at least

00:52:14,540 --> 00:52:22,690
like what's linear layer and one of the

00:52:15,950 --> 00:52:22,690
things that you can do is yeah yeah

00:52:26,990 --> 00:52:32,000
so the actual like neural-net on how it

00:52:29,790 --> 00:52:34,740
like figures out what a number is is

00:52:32,000 --> 00:52:35,610
done by itself in your like for

00:52:34,740 --> 00:52:37,080
something like this you're not gonna be

00:52:35,610 --> 00:52:38,610
able figure it out like figure out like

00:52:37,080 --> 00:52:42,330
how it went about doing it like

00:52:38,610 --> 00:52:43,470
debugging it is very very hard if

00:52:42,330 --> 00:52:44,910
there's a lot of strategies that you can

00:52:43,470 --> 00:52:50,390
do but it's going to be like very weird

00:52:44,910 --> 00:52:53,550
strategies one of the that's fine the

00:52:50,390 --> 00:52:55,260
the one of the analogies that you can

00:52:53,550 --> 00:52:57,390
use for neural nets is that they're self

00:52:55,260 --> 00:52:59,280
programming programs and so it's just

00:52:57,390 --> 00:53:00,840
programming itself it's it's those

00:52:59,280 --> 00:53:04,590
weights that we talked about its

00:53:00,840 --> 00:53:08,340
toggling those weights and however it

00:53:04,590 --> 00:53:11,930
toggles it that's that's only something

00:53:08,340 --> 00:53:14,370
that it knows so we don't really know

00:53:11,930 --> 00:53:15,630
but you could use better layers so

00:53:14,370 --> 00:53:18,780
you're talking about like why don't you

00:53:15,630 --> 00:53:21,390
push in a regular 2d image if you want

00:53:18,780 --> 00:53:23,580
to do that I would look into comm 2d

00:53:21,390 --> 00:53:26,180
neural nets they're not too complicated

00:53:23,580 --> 00:53:30,630
the only problem with two-dimensional

00:53:26,180 --> 00:53:33,330
convolutional layers is that whatever

00:53:30,630 --> 00:53:35,070
your input is and what you define they

00:53:33,330 --> 00:53:38,580
have a very fixed whatever they're gonna

00:53:35,070 --> 00:53:42,540
out but like this neural net I could

00:53:38,580 --> 00:53:44,460
change this to just like 30 and 30 it's

00:53:42,540 --> 00:53:46,910
not gonna make a difference like it's

00:53:44,460 --> 00:53:50,070
not gonna crash anything cuz it's just

00:53:46,910 --> 00:53:54,770
it's just each neuron is just connected

00:53:50,070 --> 00:53:57,240
to every other neuron but a con that

00:53:54,770 --> 00:53:59,070
based on what you input and some of your

00:53:57,240 --> 00:54:00,600
parameters it's gonna have some kind of

00:53:59,070 --> 00:54:05,550
like output so it's gonna be a little

00:54:00,600 --> 00:54:07,080
bit more complicated shape wise so so

00:54:05,550 --> 00:54:10,140
yeah and then generally like in general

00:54:07,080 --> 00:54:14,100
this is what it looks like so our output

00:54:10,140 --> 00:54:18,320
layer he's gonna output it's gonna

00:54:14,100 --> 00:54:21,390
output some kind of like one by ten

00:54:18,320 --> 00:54:24,330
neurons so you have ten classes it's

00:54:21,390 --> 00:54:26,280
gonna output it's gonna four it's ten

00:54:24,330 --> 00:54:28,860
classes it's gonna give a hot the

00:54:26,280 --> 00:54:32,310
highest value for the class that it

00:54:28,860 --> 00:54:35,520
thinks is the image and so you just get

00:54:32,310 --> 00:54:35,950
the index of that of that image let me

00:54:35,520 --> 00:54:40,390
see where

00:54:35,950 --> 00:54:42,430
oh yeah I didn't go go into actually

00:54:40,390 --> 00:54:48,940
what the training is that I'm running

00:54:42,430 --> 00:54:52,210
I'm running low on time yeah so this

00:54:48,940 --> 00:54:53,440
this is this is just the strategy that

00:54:52,210 --> 00:54:54,910
you go about training it I tried to

00:54:53,440 --> 00:54:58,480
comment as much as I can you can go to

00:54:54,910 --> 00:55:01,170
my github for this an epoch is just a

00:54:58,480 --> 00:55:04,420
full it went through the entire dataset

00:55:01,170 --> 00:55:07,480
tested itself try to change some of its

00:55:04,420 --> 00:55:09,099
weights so I think we do about 200

00:55:07,480 --> 00:55:11,890
epochs so it does it 200 times it goes

00:55:09,099 --> 00:55:14,829
through the entire dataset and then it

00:55:11,890 --> 00:55:20,079
tries to make some changes that makes it

00:55:14,829 --> 00:55:22,000
do better on that data set I guess I

00:55:20,079 --> 00:55:23,079
don't have to do batches if your dataset

00:55:22,000 --> 00:55:25,770
is really big you want to do it in

00:55:23,079 --> 00:55:31,089
pieces so matches kind of do it that way

00:55:25,770 --> 00:55:32,829
um yeah you can talk to me after after

00:55:31,089 --> 00:55:35,160
that but this is up there and you can go

00:55:32,829 --> 00:55:39,010
and talk to me about if you wanted to

00:55:35,160 --> 00:55:42,849
keep going with this so yeah this is a

00:55:39,010 --> 00:55:48,760
lot of analysis I'm using matplotlib for

00:55:42,849 --> 00:55:50,440
a lot of visualizing one of the most

00:55:48,760 --> 00:55:52,799
important things is you want to feed it

00:55:50,440 --> 00:55:55,780
if you're testing how good it is you

00:55:52,799 --> 00:55:59,680
want to test it on images that it hasn't

00:55:55,780 --> 00:56:04,869
seen so we use so I split the data set

00:55:59,680 --> 00:56:10,329
up into test X and test Y so right now

00:56:04,869 --> 00:56:12,839
I'm getting 84 84 accuracy here and for

00:56:10,329 --> 00:56:16,030
its training set I'm embedding 100 so

00:56:12,839 --> 00:56:18,609
most of the time the images that you

00:56:16,030 --> 00:56:19,960
trained it on it should do really well

00:56:18,609 --> 00:56:24,160
accuracy wise because these are pictures

00:56:19,960 --> 00:56:27,130
it's always seen but the the actressy

00:56:24,160 --> 00:56:29,559
that you really care about are the

00:56:27,130 --> 00:56:31,540
pictures that hasn't seen ever so the

00:56:29,559 --> 00:56:32,410
ones that you haven't trained on those

00:56:31,540 --> 00:56:34,900
are the ones that count

00:56:32,410 --> 00:56:37,410
so this accuracy score matters way more

00:56:34,900 --> 00:56:40,030
than the training accuracy score

00:56:37,410 --> 00:56:42,369
training accuracy score if that's not

00:56:40,030 --> 00:56:43,750
close to a hundred you have a bug in

00:56:42,369 --> 00:56:46,359
your model or something like you have

00:56:43,750 --> 00:56:47,349
something seriously wrong with it but

00:56:46,359 --> 00:56:49,210
you when you're trying to brag to people

00:56:47,349 --> 00:56:52,180
how good your model is I mean

00:56:49,210 --> 00:56:56,770
to test your your test score how good

00:56:52,180 --> 00:56:58,180
does it do in the real world yeah so

00:56:56,770 --> 00:57:01,030
there's a lot of strategies they can use

00:56:58,180 --> 00:57:02,500
there's things called drop out which are

00:57:01,030 --> 00:57:03,910
modules you can use different layers

00:57:02,500 --> 00:57:06,370
different activations different loss

00:57:03,910 --> 00:57:08,740
functions image augmentation is where

00:57:06,370 --> 00:57:10,600
you purposely mess with image like you

00:57:08,740 --> 00:57:14,350
move the seven around flip it upside

00:57:10,600 --> 00:57:16,840
down maybe you'll add like some noise to

00:57:14,350 --> 00:57:18,100
it so it makes it harder to see and if

00:57:16,840 --> 00:57:21,870
you do that it makes it actually do

00:57:18,100 --> 00:57:21,870
better on images that hasn't seen before

00:57:25,380 --> 00:57:29,590
yeah I had another one about like

00:57:27,970 --> 00:57:34,030
estimating sine waves it's kind of cool

00:57:29,590 --> 00:57:35,440
but but like our neural net based on

00:57:34,030 --> 00:57:37,270
like this and put data was able to

00:57:35,440 --> 00:57:39,310
actually like predict the rest of the

00:57:37,270 --> 00:57:41,080
sine wave movements so you don't need to

00:57:39,310 --> 00:57:44,020
just do image classification you can do

00:57:41,080 --> 00:57:46,600
like actual it can actually become a

00:57:44,020 --> 00:57:48,580
sine function or a cosine function or

00:57:46,600 --> 00:57:50,380
some kind of some kind of continuous

00:57:48,580 --> 00:57:52,990
function if you want it to but that's a

00:57:50,380 --> 00:57:55,210
that's for forecasting also I didn't

00:57:52,990 --> 00:57:57,690
fully finish this because it's chained

00:57:55,210 --> 00:57:57,690
really slow

00:58:02,270 --> 00:58:07,670
yeah so what are all the things that you

00:58:05,450 --> 00:58:09,560
can do with a neural net so we did the

00:58:07,670 --> 00:58:11,510
image class of classifiers this is very

00:58:09,560 --> 00:58:13,990
very recent so that neural net is

00:58:11,510 --> 00:58:16,369
obviously way more complicated than ours

00:58:13,990 --> 00:58:18,470
but this one is predicting a thousand

00:58:16,369 --> 00:58:20,810
classes so it's predicting everything

00:58:18,470 --> 00:58:23,210
from airplanes to cars it's just one

00:58:20,810 --> 00:58:25,369
model so it's going to decide whether a

00:58:23,210 --> 00:58:29,510
picture is of a car or a person or an

00:58:25,369 --> 00:58:32,230
airplane so a thousand classes it's

00:58:29,510 --> 00:58:35,300
gonna be a very very big dataset

00:58:32,230 --> 00:58:36,619
um object detect detectors so maybe you

00:58:35,300 --> 00:58:39,380
want to classify but you also want to

00:58:36,619 --> 00:58:42,560
know where that object is it's not as

00:58:39,380 --> 00:58:44,890
straightforward as you would hope like

00:58:42,560 --> 00:58:49,010
you were talking about actually the

00:58:44,890 --> 00:58:51,260
normally neural nets don't care like the

00:58:49,010 --> 00:58:52,670
positions of pixels and so that makes it

00:58:51,260 --> 00:58:54,920
hard to figure out where that thing is

00:58:52,670 --> 00:58:56,690
cuz it'll just look at an image say oh

00:58:54,920 --> 00:58:58,760
there's dog in there but you can't

00:58:56,690 --> 00:59:01,130
actually figure out like what pixels are

00:58:58,760 --> 00:59:03,680
actually mattering to it and so these

00:59:01,130 --> 00:59:05,300
models actually go and try to change

00:59:03,680 --> 00:59:06,760
that so it's actually getting a better

00:59:05,300 --> 00:59:09,830
idea

00:59:06,760 --> 00:59:12,920
that involves images with actual like

00:59:09,830 --> 00:59:15,320
humans true bounding boxes on the where

00:59:12,920 --> 00:59:17,869
the object is so these datasets are

00:59:15,320 --> 00:59:19,220
harder to get but they they are popular

00:59:17,869 --> 00:59:22,580
and out there so if you want to know

00:59:19,220 --> 00:59:29,090
where something is or look into those

00:59:22,580 --> 00:59:35,359
kinds of models and datasets this is

00:59:29,090 --> 00:59:39,020
very recent 2019 so there's this there's

00:59:35,359 --> 00:59:40,850
this company called open AI they made

00:59:39,020 --> 00:59:45,230
this a neural net model that is able to

00:59:40,850 --> 00:59:47,330
make fake news articles fake stories I

00:59:45,230 --> 00:59:50,170
can write poems other people have taken

00:59:47,330 --> 00:59:53,240
it and then trained it to write jokes

00:59:50,170 --> 00:59:56,420
and this is an example of what it output

00:59:53,240 --> 00:59:58,550
so the top is like the human it's the X

00:59:56,420 --> 01:00:01,430
that we were feeding on so instead of an

00:59:58,550 --> 01:00:04,940
image we're feeding it text and it's Y

01:00:01,430 --> 01:00:06,200
was the output and it's an entire story

01:00:04,940 --> 01:00:10,070
about some scientist discovering

01:00:06,200 --> 01:00:12,710
unicorns I guess but you can download

01:00:10,070 --> 01:00:15,140
this right now so this is 2019 this is

01:00:12,710 --> 01:00:16,109
very reason so we have a neuron that ii

01:00:15,140 --> 01:00:20,599
now right

01:00:16,109 --> 01:00:22,950
very convincing context based stories

01:00:20,599 --> 01:00:25,019
and their I think they use something

01:00:22,950 --> 01:00:26,579
called a transformer neural net they're

01:00:25,019 --> 01:00:29,190
all using neurons but there's different

01:00:26,579 --> 01:00:32,539
ways that you can shuffle the layers to

01:00:29,190 --> 01:00:32,539
go and make it do very different things

01:00:32,630 --> 01:00:38,009
autoencoders so like like JPEG image

01:00:36,150 --> 01:00:40,979
compression something like that you use

01:00:38,009 --> 01:00:45,719
a neural net to also compress data which

01:00:40,979 --> 01:00:47,160
is actually mind blowing and so like

01:00:45,719 --> 01:00:48,420
like a human being like if you show a

01:00:47,160 --> 01:00:50,759
human being like a picture of a dog

01:00:48,420 --> 01:00:53,430
normally a human being like our brains

01:00:50,759 --> 01:00:55,469
will go and that picture of a dog boil

01:00:53,430 --> 01:00:57,630
it down in just something way simpler

01:00:55,469 --> 01:00:59,369
and so this is an example of a neural

01:00:57,630 --> 01:01:02,239
net boiling something down into

01:00:59,369 --> 01:01:02,239

YouTube URL: https://www.youtube.com/watch?v=2xA8bxAOSzo


