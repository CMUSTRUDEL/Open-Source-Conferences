Title: 2012 SouthEast LinuxFest - ODBC Track - Josh Berkus - Data Warehousing 101
Publication date: 2013-04-08
Playlist: 2012 SouthEast LinuxFest - ODBC - Speaker Track
Description: 
	2012 SouthEast LinuxFest
Open Database Camp Speaker Track
Josh Berkus
Data Warehousing 101
Captions: 
	00:00:00,000 --> 00:00:05,370
the following presentation was recorded

00:00:02,580 --> 00:00:08,130
the 2012 southeast linux fest in

00:00:05,370 --> 00:00:10,469
charlotte north carolina it is licensed

00:00:08,130 --> 00:00:12,179
under a creative commons license for

00:00:10,469 --> 00:00:17,670
more information about the southeast

00:00:12,179 --> 00:00:19,529
linux fest visit WWDC linux pc org the

00:00:17,670 --> 00:00:22,410
southeast linux fest would like to thank

00:00:19,529 --> 00:00:25,189
the following diamond sponsors in 2012

00:00:22,410 --> 00:00:29,279
for helping make these videos possible

00:00:25,189 --> 00:00:31,920
everybody i'm here to talk about Dida

00:00:29,279 --> 00:00:33,600
warehousing or everything that you never

00:00:31,920 --> 00:00:37,260
wanted to know about big databases but

00:00:33,600 --> 00:00:40,800
we're forced to find out anyway so for a

00:00:37,260 --> 00:00:43,950
quick question how many people here do

00:00:40,800 --> 00:00:46,079
manage a database or databases a fair

00:00:43,950 --> 00:00:51,449
amount how many of you have more than a

00:00:46,079 --> 00:00:55,350
terabyte of data okay more than 100

00:00:51,449 --> 00:00:58,500
terabytes anyone there we go so how much

00:00:55,350 --> 00:01:03,660
are we talking about okay that's pretty

00:00:58,500 --> 00:01:04,769
good it's a lot of data the okay so I'm

00:01:03,660 --> 00:01:06,710
going to talk about a few things in this

00:01:04,769 --> 00:01:09,180
talk this is sort of a general overview

00:01:06,710 --> 00:01:11,549
I'm going to talk about some of the

00:01:09,180 --> 00:01:13,229
concepts of data warehousing some

00:01:11,549 --> 00:01:16,170
general data warehousing techniques and

00:01:13,229 --> 00:01:20,130
different database technologies that you

00:01:16,170 --> 00:01:23,189
can use for data warehousing because of

00:01:20,130 --> 00:01:24,780
the one hour time slot I'm not going to

00:01:23,189 --> 00:01:26,930
talk about hardware because that could

00:01:24,780 --> 00:01:29,670
easily be a whole hour in and of itself

00:01:26,930 --> 00:01:31,409
and I'm not really gonna go into

00:01:29,670 --> 00:01:32,640
analytics and reporting tools because

00:01:31,409 --> 00:01:33,780
again there's a lot of them out there

00:01:32,640 --> 00:01:36,600
and I just don't really have time to

00:01:33,780 --> 00:01:38,189
give a lot of coverage but before we get

00:01:36,600 --> 00:01:39,509
started I'm actually also on the core

00:01:38,189 --> 00:01:41,729
team of the postman school project may

00:01:39,509 --> 00:01:44,670
just did want to mention that version

00:01:41,729 --> 00:01:47,520
9.2 is currently in beta our next

00:01:44,670 --> 00:01:50,460
version and among other features for the

00:01:47,520 --> 00:01:53,100
people who are doing large databases we

00:01:50,460 --> 00:01:55,649
now support index only scams it's been a

00:01:53,100 --> 00:01:57,329
big deal on large databases so anyway

00:01:55,649 --> 00:01:58,770
that that's happening now but really

00:01:57,329 --> 00:02:01,680
it's not what I came to talk about what

00:01:58,770 --> 00:02:03,780
I came to talk about is a lot of people

00:02:01,680 --> 00:02:05,969
that I know have found themselves

00:02:03,780 --> 00:02:09,800
increasingly dealing with a lot more

00:02:05,969 --> 00:02:12,330
data than they ever expected to have and

00:02:09,800 --> 00:02:14,010
they're sort of playing catch-up in

00:02:12,330 --> 00:02:16,260
terms of fig

00:02:14,010 --> 00:02:19,860
during out what to do with this massive

00:02:16,260 --> 00:02:21,540
amount of data that they have how to

00:02:19,860 --> 00:02:24,920
store it how to retrieve it how to get

00:02:21,540 --> 00:02:27,750
some sort of useful stuff out of it and

00:02:24,920 --> 00:02:28,920
because of you know because of everybody

00:02:27,750 --> 00:02:31,590
doing this we've gotten like this new

00:02:28,920 --> 00:02:32,940
buzzword right big dad right everybody's

00:02:31,590 --> 00:02:34,650
talking about Big Data everybody's got a

00:02:32,940 --> 00:02:36,510
big data now you know gotta get Hadoop

00:02:34,650 --> 00:02:37,500
because we've got big data you know and

00:02:36,510 --> 00:02:39,599
we've got this and that sort of thing

00:02:37,500 --> 00:02:43,500
it's all new right and nobody ever had

00:02:39,599 --> 00:02:45,200
big data before now right you know it's

00:02:43,500 --> 00:02:50,040
only happened in the last five years

00:02:45,200 --> 00:02:51,540
well not really it's right one of the

00:02:50,040 --> 00:02:53,639
various word associated with large

00:02:51,540 --> 00:02:55,530
databases is the term data warehousing

00:02:53,639 --> 00:03:01,109
term data warehousing was excellent in

00:02:55,530 --> 00:03:03,720
1970 this guy Bill Imam right and and

00:03:01,109 --> 00:03:05,670
bill came up with data warehousing and a

00:03:03,720 --> 00:03:07,409
lot of this sort of science and thinking

00:03:05,670 --> 00:03:10,019
behind how to handle large amounts of

00:03:07,409 --> 00:03:13,290
data that is still being implemented in

00:03:10,019 --> 00:03:14,519
solutions today so the first thing that

00:03:13,290 --> 00:03:16,139
you should do if you suddenly find

00:03:14,519 --> 00:03:19,079
yourself in possession of terabytes of

00:03:16,139 --> 00:03:23,190
data is actually do some research well

00:03:19,079 --> 00:03:25,680
starting with his presentation and I you

00:03:23,190 --> 00:03:27,060
know aunt because there's already a lot

00:03:25,680 --> 00:03:28,769
of knowledge is already 40 years of

00:03:27,060 --> 00:03:31,980
knowledge out there about how to deal

00:03:28,769 --> 00:03:36,109
with data that is at the limits of what

00:03:31,980 --> 00:03:39,239
your computer can store in process so

00:03:36,109 --> 00:03:41,099
but let me a little more specific here

00:03:39,239 --> 00:03:43,709
because I specifically said this is data

00:03:41,099 --> 00:03:45,690
warehousing 101 so I said what is what

00:03:43,709 --> 00:03:46,980
is a data warehouse you know how am I

00:03:45,690 --> 00:03:48,480
using that term what does it mean

00:03:46,980 --> 00:03:54,060
exactly a lot of people think well data

00:03:48,480 --> 00:03:56,129
warehouse is big data yes and no because

00:03:54,060 --> 00:04:00,540
there's actually a couple of different

00:03:56,129 --> 00:04:02,310
kinds of large databases and data

00:04:00,540 --> 00:04:05,340
warehouse the term data warehouse does

00:04:02,310 --> 00:04:06,269
not apply to all of them the trim data

00:04:05,340 --> 00:04:08,639
warehouse and what I'm going to be

00:04:06,269 --> 00:04:11,790
talking about apply specifically to the

00:04:08,639 --> 00:04:15,599
kind of database system the kind of data

00:04:11,790 --> 00:04:18,659
system where you have some kind of

00:04:15,599 --> 00:04:21,959
processes business processes data

00:04:18,659 --> 00:04:25,870
collection instruments whatever that are

00:04:21,959 --> 00:04:29,760
accumulating data and that data is

00:04:25,870 --> 00:04:33,280
being sent into a storage location and

00:04:29,760 --> 00:04:36,360
what the owner of the data wants to get

00:04:33,280 --> 00:04:39,280
out of it is some sort of knowledge

00:04:36,360 --> 00:04:40,840
about what's in the data and one of

00:04:39,280 --> 00:04:44,050
those about databases is that this

00:04:40,840 --> 00:04:46,180
accumulation process of data just keeps

00:04:44,050 --> 00:04:49,060
going on continuously and the data just

00:04:46,180 --> 00:04:50,949
keeps getting bigger and bigger and

00:04:49,060 --> 00:04:52,210
that's your data warehouse that's why

00:04:50,949 --> 00:04:53,650
they use the term warehousing because

00:04:52,210 --> 00:04:57,310
they're thinking about storage and the

00:04:53,650 --> 00:04:59,229
analogy for moving stuff so but the term

00:04:57,310 --> 00:05:03,280
big data is used to apply to a couple of

00:04:59,229 --> 00:05:06,699
different things whoo well that's very

00:05:03,280 --> 00:05:10,680
interesting okay my apologies that

00:05:06,699 --> 00:05:13,750
happened in like since I open this file

00:05:10,680 --> 00:05:17,710
so a big data refers to a couple

00:05:13,750 --> 00:05:21,039
different things because part of it

00:05:17,710 --> 00:05:22,479
refers to things like like I've been

00:05:21,039 --> 00:05:24,430
talking about you know you've got the

00:05:22,479 --> 00:05:25,780
large data warehouse the other term the

00:05:24,430 --> 00:05:27,580
other way that people use big data to

00:05:25,780 --> 00:05:30,070
refer to is these databases like

00:05:27,580 --> 00:05:33,610
Amazon's transactional database for when

00:05:30,070 --> 00:05:35,080
you order stuff or the Facebook database

00:05:33,610 --> 00:05:37,270
for all the stuff and in their case

00:05:35,080 --> 00:05:39,849
they're talking about a database that's

00:05:37,270 --> 00:05:41,530
designed to support thousands to

00:05:39,849 --> 00:05:43,389
millions of concurrent users each of

00:05:41,530 --> 00:05:45,130
whom is doing very small units of work

00:05:43,389 --> 00:05:47,800
with the system they're retrieving their

00:05:45,130 --> 00:05:50,800
personal profile they're there you know

00:05:47,800 --> 00:05:52,510
ordering one product you know and and

00:05:50,800 --> 00:05:53,950
that is a form of big data but it is not

00:05:52,510 --> 00:05:56,910
a data warehouse what I'm talking about

00:05:53,950 --> 00:06:00,310
for the data warehouse is you have large

00:05:56,910 --> 00:06:01,960
databases generally your data gets in by

00:06:00,310 --> 00:06:04,810
batch import generally you're storing

00:06:01,960 --> 00:06:06,099
years of data your queries are often

00:06:04,810 --> 00:06:07,570
generated by large reports so you

00:06:06,099 --> 00:06:10,270
generally have a very small number of

00:06:07,570 --> 00:06:12,250
users but each user is using an enormous

00:06:10,270 --> 00:06:14,949
amount of resources is combing through

00:06:12,250 --> 00:06:19,419
you know gigabytes terabytes of data in

00:06:14,949 --> 00:06:20,770
order to find what they want a better

00:06:19,419 --> 00:06:22,660
way you can summarize it particularly

00:06:20,770 --> 00:06:27,460
since would not having that layout

00:06:22,660 --> 00:06:29,860
problem is you're sort of transactional

00:06:27,460 --> 00:06:32,169
big data is big data for many concurrent

00:06:29,860 --> 00:06:35,800
requests for small amounts of data each

00:06:32,169 --> 00:06:38,860
and your data warehouse is big data for

00:06:35,800 --> 00:06:39,430
low concurrency requests for very large

00:06:38,860 --> 00:06:42,460
amounts

00:06:39,430 --> 00:06:47,820
data a piece so we're talking about the

00:06:42,460 --> 00:06:52,600
second now the lot of other terms used

00:06:47,820 --> 00:06:56,200
for I data warehousing so go over some

00:06:52,600 --> 00:06:58,690
of those terms because they get talked

00:06:56,200 --> 00:07:01,240
about and actually I see these terms as

00:06:58,690 --> 00:07:05,020
basically sort of your sub categories of

00:07:01,240 --> 00:07:06,460
data warehousing these used a variety of

00:07:05,020 --> 00:07:08,740
ways but the thing this is your sort of

00:07:06,460 --> 00:07:12,330
three three different types of data

00:07:08,740 --> 00:07:16,780
warehousing the first one is archiving

00:07:12,330 --> 00:07:19,480
and this has become increasingly popular

00:07:16,780 --> 00:07:22,450
later the idea of the archive data

00:07:19,480 --> 00:07:24,550
warehouse is let's put away all of our

00:07:22,450 --> 00:07:28,900
data somewhere in case we might need it

00:07:24,550 --> 00:07:31,180
someday this has become increasingly

00:07:28,900 --> 00:07:33,190
popular lately thanks to government

00:07:31,180 --> 00:07:35,440
regulations that require retention of

00:07:33,190 --> 00:07:38,290
data for a certain period for that

00:07:35,440 --> 00:07:41,620
reason I often call this Warren data is

00:07:38,290 --> 00:07:44,140
in right once we'd never because the

00:07:41,620 --> 00:07:45,760
business doesn't actually intend to do

00:07:44,140 --> 00:07:49,870
anything with the data they are required

00:07:45,760 --> 00:07:53,020
to keep it by law and they need to store

00:07:49,870 --> 00:07:54,280
it somewhere and your main concern with

00:07:53,020 --> 00:07:56,710
when constructing one of these archives

00:07:54,280 --> 00:07:59,920
is how to store it efficiently and

00:07:56,710 --> 00:08:02,260
inexpensively not what you can get out

00:07:59,920 --> 00:08:04,120
of it or how quickly because you don't

00:08:02,260 --> 00:08:07,050
necessarily have any expectations of

00:08:04,120 --> 00:08:10,450
trying to get anything out of it at all

00:08:07,050 --> 00:08:12,700
your second type of data your second

00:08:10,450 --> 00:08:15,250
type of data warehouse isn't known as

00:08:12,700 --> 00:08:18,280
data mining and the way the data mining

00:08:15,250 --> 00:08:21,100
is starts like this gee our web servers

00:08:18,280 --> 00:08:22,570
are generating a lot of data I'm sure

00:08:21,100 --> 00:08:24,550
there's something useful about our

00:08:22,570 --> 00:08:28,090
customers in there but I don't know what

00:08:24,550 --> 00:08:32,050
and so then you start at a data mining

00:08:28,090 --> 00:08:33,370
operation and that's where you store a

00:08:32,050 --> 00:08:34,570
whole bunch of data and it's generally

00:08:33,370 --> 00:08:36,790
it's a lot of data terabytes to

00:08:34,570 --> 00:08:38,590
petabytes of data because it's being

00:08:36,790 --> 00:08:41,020
generated by instruments that have a

00:08:38,590 --> 00:08:43,570
whole lot of data it's often what's

00:08:41,020 --> 00:08:45,160
known as semi-structured data as in you

00:08:43,570 --> 00:08:46,780
know certain things about the data but

00:08:45,160 --> 00:08:48,670
there's other things that are just blobs

00:08:46,780 --> 00:08:50,110
they're just text strings binary things

00:08:48,670 --> 00:08:50,620
that you don't really know what's in

00:08:50,110 --> 00:08:54,640
them when you

00:08:50,620 --> 00:08:56,350
receive them the data is generally

00:08:54,640 --> 00:08:57,910
produced as a side effect almost always

00:08:56,350 --> 00:08:59,440
produces a side effect of something else

00:08:57,910 --> 00:09:01,990
you know its web blogs its

00:08:59,440 --> 00:09:03,490
instrumentation it's like one of these

00:09:01,990 --> 00:09:07,779
that I'm working on currently its data

00:09:03,490 --> 00:09:10,330
mining is telemetry for power generating

00:09:07,779 --> 00:09:12,490
windmills you know and the main purpose

00:09:10,330 --> 00:09:14,730
of the telemetry is to warn you when the

00:09:12,490 --> 00:09:17,560
windmills about to lock up or burn out

00:09:14,730 --> 00:09:19,150
but now they're accumulating the data

00:09:17,560 --> 00:09:21,220
because they want to actually figure out

00:09:19,150 --> 00:09:25,480
some things about the windmills

00:09:21,220 --> 00:09:27,700
themselves from it and because you

00:09:25,480 --> 00:09:29,080
generally don't know a lot about the

00:09:27,700 --> 00:09:31,630
structure of the data when you're

00:09:29,080 --> 00:09:33,430
receiving in in advance in order to get

00:09:31,630 --> 00:09:36,040
useful information out of it you have to

00:09:33,430 --> 00:09:39,430
do a lot of CPU intensive processing a

00:09:36,040 --> 00:09:42,490
lot of summarizing and searching for

00:09:39,430 --> 00:09:44,350
Strings and mathematical comparisons and

00:09:42,490 --> 00:09:47,440
operations so if you do a lot of CPU

00:09:44,350 --> 00:09:49,839
intensive processing and for that we as

00:09:47,440 --> 00:09:51,400
in a lot of what you're optimizing in

00:09:49,839 --> 00:09:53,740
data mining you're looking at a certain

00:09:51,400 --> 00:09:56,170
amount in storage but one of your main

00:09:53,740 --> 00:09:58,600
goals and data mining is to have some

00:09:56,170 --> 00:10:01,570
platform where you can do a large amount

00:09:58,600 --> 00:10:03,910
of CPU intensive processing across as

00:10:01,570 --> 00:10:07,510
many as much of your resources as is

00:10:03,910 --> 00:10:10,690
possible to process your data our third

00:10:07,510 --> 00:10:14,320
type of data warehousing goes under a

00:10:10,690 --> 00:10:16,900
rather dismaying number of names the

00:10:14,320 --> 00:10:21,160
three most common ones that get used are

00:10:16,900 --> 00:10:23,290
all acronyms bi DSS and OLAP which stand

00:10:21,160 --> 00:10:25,000
for business intelligence decision

00:10:23,290 --> 00:10:26,980
report online and the other one is

00:10:25,000 --> 00:10:29,110
online analytical processing it also

00:10:26,980 --> 00:10:33,220
goes into the innominate the non acronym

00:10:29,110 --> 00:10:34,959
name analytics and what this really is

00:10:33,220 --> 00:10:39,700
all referring to is this sort of thing

00:10:34,959 --> 00:10:41,350
is you've got a bunch of data you've got

00:10:39,700 --> 00:10:45,550
a bunch of business data and you want to

00:10:41,350 --> 00:10:48,730
produce some nice charts and interactive

00:10:45,550 --> 00:10:52,150
visualizations of data in order to

00:10:48,730 --> 00:10:57,459
support management decisions about what

00:10:52,150 --> 00:10:59,709
you're going to do next so generally

00:10:57,459 --> 00:11:01,480
when you're building analytics platform

00:10:59,709 --> 00:11:02,890
you have data where the structure and

00:11:01,480 --> 00:11:03,310
the contents of the data are very well

00:11:02,890 --> 00:11:05,860
understood

00:11:03,310 --> 00:11:07,540
good you know maybe things like point of

00:11:05,860 --> 00:11:09,279
sale records or survey records or

00:11:07,540 --> 00:11:10,960
whatever will you know you've got a set

00:11:09,279 --> 00:11:12,870
of fields or whatever you know what kind

00:11:10,960 --> 00:11:15,330
of data is going to be in each field

00:11:12,870 --> 00:11:18,610
most of your data can be reduced to

00:11:15,330 --> 00:11:22,960
numbers categories geography and

00:11:18,610 --> 00:11:23,980
taxonomy structures and your primary

00:11:22,960 --> 00:11:26,200
concern when you get into one of these

00:11:23,980 --> 00:11:28,029
bi things is you're looking at a

00:11:26,200 --> 00:11:30,279
platform that allows you to do a lot of

00:11:28,029 --> 00:11:32,490
advanced and clever indexing because

00:11:30,279 --> 00:11:36,810
that indexing is going to support having

00:11:32,490 --> 00:11:40,510
speedy visualizations that allow you to

00:11:36,810 --> 00:11:41,890
visualize and work with the data so

00:11:40,510 --> 00:11:43,360
there are three basic types of data

00:11:41,890 --> 00:11:45,220
warehousing now one of the things that

00:11:43,360 --> 00:11:46,930
can make your life really complicated is

00:11:45,220 --> 00:11:48,640
in a real world situation you're

00:11:46,930 --> 00:11:52,900
frequently trying to implement more than

00:11:48,640 --> 00:11:56,160
one type at once sometimes you're trying

00:11:52,900 --> 00:11:58,180
to implement all three at once and

00:11:56,160 --> 00:12:00,040
sometimes that means adopting a hybrid

00:11:58,180 --> 00:12:01,240
solution in some businesses I work with

00:12:00,040 --> 00:12:03,070
that's actually meant having more than

00:12:01,240 --> 00:12:05,710
one data warehouse for the same

00:12:03,070 --> 00:12:07,720
essential body of data you know one in

00:12:05,710 --> 00:12:09,339
which they're doing the the regulatory

00:12:07,720 --> 00:12:10,780
archiving and storing ten years of data

00:12:09,339 --> 00:12:12,580
another one in which they're only

00:12:10,780 --> 00:12:14,620
storing one year of data but they're

00:12:12,580 --> 00:12:16,300
mining it and then another one where

00:12:14,620 --> 00:12:17,620
they're taking all of the mind data that

00:12:16,300 --> 00:12:21,550
is now highly structured and putting it

00:12:17,620 --> 00:12:24,280
in analytics platform so but those are

00:12:21,550 --> 00:12:26,050
three sort of basic activities so I may

00:12:24,280 --> 00:12:28,960
get some other terminology out of the

00:12:26,050 --> 00:12:30,460
way again going back to Bill going back

00:12:28,960 --> 00:12:32,860
to 1970 and that sort of thing because

00:12:30,460 --> 00:12:35,620
the stuff still applies even if you're

00:12:32,860 --> 00:12:40,960
using some sort of modern search based

00:12:35,620 --> 00:12:47,410
non-relational platform one of those is

00:12:40,960 --> 00:12:49,690
dimension the you know could I ask you a

00:12:47,410 --> 00:12:54,100
favor in this text right there could you

00:12:49,690 --> 00:12:56,050
give me a glass of water Thanks one of

00:12:54,100 --> 00:12:58,209
those is dimension and you get this

00:12:56,050 --> 00:13:00,430
thing called dimension and I've seen

00:12:58,209 --> 00:13:03,010
this term misapplied a lot to a lot of

00:13:00,430 --> 00:13:04,720
different sorts of things that are not

00:13:03,010 --> 00:13:08,589
dimensions so let me explain basically

00:13:04,720 --> 00:13:11,080
dimension is in in a lot of data

00:13:08,589 --> 00:13:14,380
warehouses a lot of data warehouses

00:13:11,080 --> 00:13:15,370
essentially have one gigantic table or

00:13:14,380 --> 00:13:19,600
data

00:13:15,370 --> 00:13:21,550
base or population of files and there's

00:13:19,600 --> 00:13:25,660
one gigantic table is known as a fact

00:13:21,550 --> 00:13:27,600
table that that is your actual data that

00:13:25,660 --> 00:13:30,790
you're interested in investigating now

00:13:27,600 --> 00:13:33,130
this data can be summarized in

00:13:30,790 --> 00:13:35,860
understood in different ways like for

00:13:33,130 --> 00:13:38,560
example you'll often have various

00:13:35,860 --> 00:13:40,240
categories like you know like say we're

00:13:38,560 --> 00:13:44,320
dealing with sales data and you've got

00:13:40,240 --> 00:13:45,520
it organized by customer so you have you

00:13:44,320 --> 00:13:48,100
know you've got your customer idea

00:13:45,520 --> 00:13:50,050
what's even more useful and much more of

00:13:48,100 --> 00:13:52,600
a dimension is you might have something

00:13:50,050 --> 00:13:55,870
like a taxonomy a whole hierarchy of

00:13:52,600 --> 00:13:56,950
categories category subcategory detail

00:13:55,870 --> 00:13:58,390
category that sort of thing that

00:13:56,950 --> 00:14:02,080
eventually point to the specific

00:13:58,390 --> 00:14:04,060
category in the fact team or you know

00:14:02,080 --> 00:14:05,770
you might have something else that helps

00:14:04,060 --> 00:14:09,040
you visualize the data like geographic

00:14:05,770 --> 00:14:11,589
information which is wonderful useful

00:14:09,040 --> 00:14:12,880
dimension you know refers to information

00:14:11,589 --> 00:14:14,470
that comes from outside the database

00:14:12,880 --> 00:14:15,940
which is the geography of the world but

00:14:14,470 --> 00:14:18,100
it's still stuff that you want to know

00:14:15,940 --> 00:14:21,450
you know what we're my or my sales like

00:14:18,100 --> 00:14:26,529
how many hits that I get from Australia

00:14:21,450 --> 00:14:29,589
so so these are your dimensions these

00:14:26,529 --> 00:14:31,570
these sort of canonical that information

00:14:29,589 --> 00:14:34,570
that doesn't change very much versus the

00:14:31,570 --> 00:14:36,160
main data that's actually coming in some

00:14:34,570 --> 00:14:39,070
more examples of dimensions you know

00:14:36,160 --> 00:14:42,490
localization region product

00:14:39,070 --> 00:14:45,550
categorization URLs transaction types

00:14:42,490 --> 00:14:46,810
account hierarchy IP addresses this is

00:14:45,550 --> 00:14:50,560
what I want with a lot in the Mozilla

00:14:46,810 --> 00:14:53,560
stuff OS version build and in a

00:14:50,560 --> 00:14:55,029
well-formed data warehouse dimensions

00:14:53,560 --> 00:14:57,670
are often going to be structured

00:14:55,029 --> 00:15:00,490
hierarchically you know like this like

00:14:57,670 --> 00:15:02,410
OS version build for example because

00:15:00,490 --> 00:15:03,610
then you can summarize by OS youth and

00:15:02,410 --> 00:15:06,310
summarized by OS and version you can

00:15:03,610 --> 00:15:07,990
summarize by build there's some synonyms

00:15:06,310 --> 00:15:10,600
for the word dimension particularly for

00:15:07,990 --> 00:15:12,820
newer technologies one of the ones that

00:15:10,600 --> 00:15:15,850
shows up in the lucene based stuff that

00:15:12,820 --> 00:15:17,220
will discuss later facet here a lot and

00:15:15,850 --> 00:15:21,209
that's basically means dimension

00:15:17,220 --> 00:15:24,010
taxonomy for all those category trees

00:15:21,209 --> 00:15:26,020
for systems that have index organized

00:15:24,010 --> 00:15:28,540
data you'll often hear dimensions

00:15:26,020 --> 00:15:31,420
referred to as secondary indexes

00:15:28,540 --> 00:15:33,190
or sometimes this views now that's those

00:15:31,420 --> 00:15:35,050
last two are confusing because those

00:15:33,190 --> 00:15:37,210
terms are also used to mean other things

00:15:35,050 --> 00:15:40,060
that are not dimensions other bits of

00:15:37,210 --> 00:15:42,070
database technology so one of the

00:15:40,060 --> 00:15:44,170
difficult things in comprehending the

00:15:42,070 --> 00:15:46,270
data warehousing world is that the same

00:15:44,170 --> 00:15:49,240
terms are used to mean different things

00:15:46,270 --> 00:15:52,420
depending on which vendor which open

00:15:49,240 --> 00:15:55,060
source project which domain of data

00:15:52,420 --> 00:15:56,050
warehousing you're talking to another

00:15:55,060 --> 00:15:58,180
term that actually I want to talk about

00:15:56,050 --> 00:15:59,740
because it comes up and because it's

00:15:58,180 --> 00:16:03,490
important in data warehouse design is

00:15:59,740 --> 00:16:06,250
this term called is is another common

00:16:03,490 --> 00:16:07,750
term you run across so the one of the

00:16:06,250 --> 00:16:10,840
common things that you'll have in a very

00:16:07,750 --> 00:16:12,640
simple data warehouse which actually is

00:16:10,840 --> 00:16:14,620
numerically very common you know you

00:16:12,640 --> 00:16:16,090
have like say your web logs so your

00:16:14,620 --> 00:16:18,070
purpose your data warehouse is to mind

00:16:16,090 --> 00:16:20,200
your web blogs you have your web logs

00:16:18,070 --> 00:16:23,740
and that is your fact table this is

00:16:20,200 --> 00:16:25,180
gigantic series of digested web logs and

00:16:23,740 --> 00:16:27,010
then that has various dimensions are

00:16:25,180 --> 00:16:29,770
taxonomies and geographic dimensions and

00:16:27,010 --> 00:16:31,240
that sort of thing around that and the

00:16:29,770 --> 00:16:33,600
term for this using the data warehousing

00:16:31,240 --> 00:16:35,890
world is it's called a star schema

00:16:33,600 --> 00:16:38,200
because this thing sort of forms a star

00:16:35,890 --> 00:16:43,330
you know with the fact table at the

00:16:38,200 --> 00:16:44,800
center and the and when I explain

00:16:43,330 --> 00:16:45,970
systems later on you'll understand all

00:16:44,800 --> 00:16:47,530
these important distinction now there is

00:16:45,970 --> 00:16:50,500
a different kind of data warehouses a

00:16:47,530 --> 00:16:54,280
little bit more complex because for some

00:16:50,500 --> 00:16:55,930
highly structured data for some large

00:16:54,280 --> 00:16:58,110
business operations and stuff you'll

00:16:55,930 --> 00:17:01,990
actually have more than one fact table

00:16:58,110 --> 00:17:03,880
you know so like this might be purchases

00:17:01,990 --> 00:17:05,380
and this might be inventory movement and

00:17:03,880 --> 00:17:08,350
this might be you know for like an ERP

00:17:05,380 --> 00:17:10,210
system right and this might be orders

00:17:08,350 --> 00:17:11,650
you know and these are all obviously

00:17:10,210 --> 00:17:14,650
closely related but they're not the same

00:17:11,650 --> 00:17:19,720
thing they're different fact tables or

00:17:14,650 --> 00:17:23,140
if you're doing analysis of like we're

00:17:19,720 --> 00:17:24,370
doing analysis of I you know windmills

00:17:23,140 --> 00:17:26,380
and that sort of thing this might be

00:17:24,370 --> 00:17:27,940
weather conditions which record you know

00:17:26,380 --> 00:17:29,620
by minute and this might be windmill

00:17:27,940 --> 00:17:32,260
behavior and this byte might be

00:17:29,620 --> 00:17:34,180
maintenance activities so again related

00:17:32,260 --> 00:17:36,130
stuff but each one of them forming an

00:17:34,180 --> 00:17:37,960
enormous separate fact table which then

00:17:36,130 --> 00:17:41,200
relate to different dimensions within

00:17:37,960 --> 00:17:42,190
the database and this more complex kind

00:17:41,200 --> 00:17:44,350
of data warehouse is

00:17:42,190 --> 00:17:46,570
it's known as a snowflake schema you can

00:17:44,350 --> 00:17:48,010
sort of see this the snowflake pattern

00:17:46,570 --> 00:17:49,510
right because it doesn't doesn't have a

00:17:48,010 --> 00:17:53,110
single fact table in the center and has

00:17:49,510 --> 00:17:54,460
more of a ring of them so that's the

00:17:53,110 --> 00:17:55,870
snowflake schema so if you know you hear

00:17:54,460 --> 00:17:57,520
those terms being used in the future

00:17:55,870 --> 00:17:58,630
you'll know what star schema and

00:17:57,520 --> 00:18:00,610
snowflake schema are referring to

00:17:58,630 --> 00:18:02,590
because I'm going to use them late in

00:18:00,610 --> 00:18:04,990
this presentation so here's another term

00:18:02,590 --> 00:18:07,630
this is actually this is what started

00:18:04,990 --> 00:18:09,610
this presentation because I went into a

00:18:07,630 --> 00:18:12,060
client where they were about to do

00:18:09,610 --> 00:18:14,740
something with 20 terabytes of data and

00:18:12,060 --> 00:18:18,960
I said okay so let's take a look at your

00:18:14,740 --> 00:18:22,660
ETL processes and they're like what CTL

00:18:18,960 --> 00:18:24,040
and I was extract transform I was tell

00:18:22,660 --> 00:18:26,470
you this is what's he tail and the thing

00:18:24,040 --> 00:18:29,050
is it was startling to me because the

00:18:26,470 --> 00:18:30,970
entire purpose of this company was to

00:18:29,050 --> 00:18:31,960
deal with terabytes of data it wasn't

00:18:30,970 --> 00:18:33,850
something they were doing as a side

00:18:31,960 --> 00:18:36,820
effect this was their business and they

00:18:33,850 --> 00:18:39,220
hadn't heard this term before and etl

00:18:36,820 --> 00:18:41,560
stands for extract transform load and

00:18:39,220 --> 00:18:44,020
simply put this is how you get your data

00:18:41,560 --> 00:18:46,710
from wherever it comes from into

00:18:44,020 --> 00:18:52,720
whatever your database solution is

00:18:46,710 --> 00:18:54,760
preferably somewhat cleaned up the so

00:18:52,720 --> 00:18:56,710
you know you could be going from say a

00:18:54,760 --> 00:18:58,330
whole bunch of web logs to a web

00:18:56,710 --> 00:18:59,740
analytics DB a whole bunch of

00:18:58,330 --> 00:19:01,090
point-of-sale files to a financial

00:18:59,740 --> 00:19:03,340
reporting database you could be going

00:19:01,090 --> 00:19:05,800
from your transaction processing

00:19:03,340 --> 00:19:07,510
database server which can be emitting

00:19:05,800 --> 00:19:12,340
changes and going directly to 10 years

00:19:07,510 --> 00:19:14,550
of data history is also occasionally

00:19:12,340 --> 00:19:17,800
called by data warehousing geeks elt

00:19:14,550 --> 00:19:19,780
because of the popularity of if you have

00:19:17,800 --> 00:19:21,730
a relational or clustered database

00:19:19,780 --> 00:19:23,470
system of doing your transformations

00:19:21,730 --> 00:19:26,080
inside the database so what you do is

00:19:23,470 --> 00:19:28,480
extract load transform instead of

00:19:26,080 --> 00:19:29,740
extract transform load but they

00:19:28,480 --> 00:19:32,260
basically mean the same thing it's how

00:19:29,740 --> 00:19:35,020
do we get data from here to there so you

00:19:32,260 --> 00:19:37,450
want to get it from your source you want

00:19:35,020 --> 00:19:39,070
to clean up a bunch of garbage in the

00:19:37,450 --> 00:19:42,570
data because any large volume of data

00:19:39,070 --> 00:19:49,240
you can have a bunch of garbage right

00:19:42,570 --> 00:19:51,580
the you want to identify attributes in

00:19:49,240 --> 00:19:53,110
dimensions and mark them somehow

00:19:51,580 --> 00:19:54,010
depending your database system like if

00:19:53,110 --> 00:19:55,810
it's going to a relational database

00:19:54,010 --> 00:19:58,300
system you want to separate attributes

00:19:55,810 --> 00:19:59,620
them in fields you want to separate out

00:19:58,300 --> 00:20:01,470
of the dimensions and put them to look

00:19:59,620 --> 00:20:04,660
up tables you want to deduplicate

00:20:01,470 --> 00:20:06,100
duplicate information because if you

00:20:04,660 --> 00:20:07,870
haven't looked for duplicate information

00:20:06,100 --> 00:20:12,700
in your database I guarantee you that

00:20:07,870 --> 00:20:15,100
you have some and you want to possibly

00:20:12,700 --> 00:20:18,820
calculate materialized views and update

00:20:15,100 --> 00:20:21,760
indexes depending on your design and so

00:20:18,820 --> 00:20:23,140
you do all that fortunately there's a

00:20:21,760 --> 00:20:28,750
lot of good tools to help you with us I

00:20:23,140 --> 00:20:30,490
mean I have the I such as I mean for

00:20:28,750 --> 00:20:31,810
example these these are three the top

00:20:30,490 --> 00:20:43,300
ones so there's talent which is an

00:20:31,810 --> 00:20:44,860
independent open source yeah yeah the

00:20:43,300 --> 00:20:50,020
scheme is not going to do anything for

00:20:44,860 --> 00:20:51,220
you code has to do something yeah and

00:20:50,020 --> 00:20:53,950
sometimes they'll have duplicates and

00:20:51,220 --> 00:20:55,510
and you need to do something now for

00:20:53,950 --> 00:20:56,680
using a relational database it'll give

00:20:55,510 --> 00:20:58,390
you an error if it encounters a

00:20:56,680 --> 00:20:59,560
duplicate but then you have to do

00:20:58,390 --> 00:21:01,750
something when you receive that error

00:20:59,560 --> 00:21:03,370
the database is not going to do anything

00:21:01,750 --> 00:21:05,170
other than air out refuse to accept the

00:21:03,370 --> 00:21:07,390
data and that's not generally a

00:21:05,170 --> 00:21:13,540
satisfactory solution for for any data

00:21:07,390 --> 00:21:15,910
warehouse yes but that's part of the elt

00:21:13,540 --> 00:21:17,740
that's part of the ETL that's something

00:21:15,910 --> 00:21:21,130
you want to do as part of the ETL you

00:21:17,740 --> 00:21:24,490
don't want to store highly duplicate

00:21:21,130 --> 00:21:26,350
data full of garbage as your main

00:21:24,490 --> 00:21:27,970
referential data warehouse because then

00:21:26,350 --> 00:21:29,800
every time you make a request of that

00:21:27,970 --> 00:21:31,090
native warehouse it requires a whole

00:21:29,800 --> 00:21:32,890
bunch of extra processing that you

00:21:31,090 --> 00:21:37,840
should have done when you imported the

00:21:32,890 --> 00:21:39,580
data in the first place it's a it's a

00:21:37,840 --> 00:21:40,780
question of it's a decision of how much

00:21:39,580 --> 00:21:42,190
time do you want to spend when you're

00:21:40,780 --> 00:21:43,180
importing the data versus how much time

00:21:42,190 --> 00:21:48,010
you want to spend when you're trying to

00:21:43,180 --> 00:21:51,100
get stuff out of the data so for some of

00:21:48,010 --> 00:21:53,500
the ETL tools I you know talent

00:21:51,100 --> 00:21:55,810
independent open source you know java

00:21:53,500 --> 00:21:58,630
based thing kettle was an independent

00:21:55,810 --> 00:22:00,220
project got take got taken over by

00:21:58,630 --> 00:22:04,420
panoho and it's not part of the pen oh

00:22:00,220 --> 00:22:05,860
ho sweet and in informatica as an

00:22:04,420 --> 00:22:09,790
example of an old school

00:22:05,860 --> 00:22:11,500
Bowl commercial basically ETL platform

00:22:09,790 --> 00:22:15,400
they call it data lifetime management

00:22:11,500 --> 00:22:16,600
but it's basically just it CTL + + you

00:22:15,400 --> 00:22:18,549
know and then they give you these nice

00:22:16,600 --> 00:22:20,830
sort of graphical things that allow you

00:22:18,549 --> 00:22:22,480
to do sort of formulas and diagrams and

00:22:20,830 --> 00:22:23,920
that sort of thing of this is where my

00:22:22,480 --> 00:22:25,780
dad is going from here to there and

00:22:23,920 --> 00:22:30,280
apply this operation and insert this

00:22:25,780 --> 00:22:34,840
other thing I think this is a Talon

00:22:30,280 --> 00:22:39,010
screen yeah the but this is still the

00:22:34,840 --> 00:22:40,600
most popular method of ETL you know and

00:22:39,010 --> 00:22:42,190
most of the sites that are run across in

00:22:40,600 --> 00:22:47,080
most of the places even very large

00:22:42,190 --> 00:22:50,770
enterprises most dtl takes the form of

00:22:47,080 --> 00:22:52,929
ad hoc Perl Python Java scripts even

00:22:50,770 --> 00:22:55,480
shell scripts that that do various

00:22:52,929 --> 00:22:59,230
transformations of data a few tips

00:22:55,480 --> 00:23:02,610
undoing ETL always think about the

00:22:59,230 --> 00:23:04,750
volume that you're dealing with so every

00:23:02,610 --> 00:23:06,990
operation that you do should be thought

00:23:04,750 --> 00:23:09,669
of in terms of bulk processing or

00:23:06,990 --> 00:23:12,429
parallel processing you should never be

00:23:09,669 --> 00:23:16,210
thinking in terms of a single row or a

00:23:12,429 --> 00:23:17,590
single document because if you iterating

00:23:16,210 --> 00:23:19,390
over a single document works fine when

00:23:17,590 --> 00:23:21,940
you're testing it when you have a

00:23:19,390 --> 00:23:25,090
billion documents coming in a day or an

00:23:21,940 --> 00:23:26,410
hour it doesn't really work software

00:23:25,090 --> 00:23:29,890
gets much more efficient when you can

00:23:26,410 --> 00:23:32,020
actually do things in batches the other

00:23:29,890 --> 00:23:33,640
thing is you should think of terms that

00:23:32,020 --> 00:23:35,200
inserting into permanent storage

00:23:33,640 --> 00:23:36,640
inserting into your long-term data

00:23:35,200 --> 00:23:39,250
warehouse should be the very last step

00:23:36,640 --> 00:23:40,299
of your transformation now in relational

00:23:39,250 --> 00:23:42,010
databases that often means you can

00:23:40,299 --> 00:23:43,570
insert into a bunch of temporary tables

00:23:42,010 --> 00:23:44,860
and then do transformations those

00:23:43,570 --> 00:23:47,950
temporary tables and insert them in the

00:23:44,860 --> 00:23:49,809
final fact table in a non-relational you

00:23:47,950 --> 00:23:52,390
know operation you might have a separate

00:23:49,809 --> 00:23:54,160
server that handles the partly digested

00:23:52,390 --> 00:23:58,390
data and then you insert it into your

00:23:54,160 --> 00:23:59,530
flannel server no updates now one of the

00:23:58,390 --> 00:24:02,110
things that actually has changed

00:23:59,530 --> 00:24:03,730
relatively recently how to handle ETL on

00:24:02,110 --> 00:24:06,460
a lot of the sites that I work with is

00:24:03,730 --> 00:24:09,610
that we've started to move over to Ewing

00:24:06,460 --> 00:24:12,429
using queuing software for handling a

00:24:09,610 --> 00:24:15,130
lot of the incoming gap because it

00:24:12,429 --> 00:24:17,800
allows us to do continuous processing of

00:24:15,130 --> 00:24:18,740
incoming data rather than doing it and

00:24:17,800 --> 00:24:21,110
say staggered

00:24:18,740 --> 00:24:22,970
hourly matches and allows us to keep up

00:24:21,110 --> 00:24:25,070
with a lot more the demand now for users

00:24:22,970 --> 00:24:26,809
wanting to actually see analytics from

00:24:25,070 --> 00:24:32,240
data that was generated only a few

00:24:26,809 --> 00:24:33,770
minutes ago so the other big thing is a

00:24:32,240 --> 00:24:52,070
lot of the questions that people say

00:24:33,770 --> 00:24:54,020
yeah yeah oh you mean for yeah so so

00:24:52,070 --> 00:24:56,090
state state information and that sort of

00:24:54,020 --> 00:24:59,929
thing yeah those those are challenging

00:24:56,090 --> 00:25:01,730
for that reason the I haven't seen in a

00:24:59,929 --> 00:25:04,070
really satisfactory solution for those

00:25:01,730 --> 00:25:06,500
because you're either doing so he's

00:25:04,070 --> 00:25:08,270
talking about time-based databases we

00:25:06,500 --> 00:25:11,150
have records that represent a point in

00:25:08,270 --> 00:25:12,710
time for particular entity and so the

00:25:11,150 --> 00:25:15,080
point in time has to be terminated at

00:25:12,710 --> 00:25:16,790
some point and the termination basically

00:25:15,080 --> 00:25:18,950
means doing an update to a record or a

00:25:16,790 --> 00:25:20,270
document which is really expensive when

00:25:18,950 --> 00:25:27,500
you think about doing it to millions of

00:25:20,270 --> 00:25:28,850
them so the end I haven't actually seen

00:25:27,500 --> 00:25:33,830
what I ever felt was a really elegant

00:25:28,850 --> 00:25:35,750
solution to that problem the because the

00:25:33,830 --> 00:25:36,980
alternative is to keep the beginning and

00:25:35,750 --> 00:25:39,290
end information somewhere else so you

00:25:36,980 --> 00:25:41,300
can do it append only but then the cost

00:25:39,290 --> 00:25:43,190
of calculating which records are current

00:25:41,300 --> 00:25:48,290
of a particular point of time goes up

00:25:43,190 --> 00:25:51,490
tremendously so you know I it's it ends

00:25:48,290 --> 00:25:51,490
up being awkward on one end or the other

00:25:52,540 --> 00:25:56,210
so a lot of people ask what kind of

00:25:54,710 --> 00:25:57,320
database should i use for this there's

00:25:56,210 --> 00:25:59,210
the other reason why i wanted to give

00:25:57,320 --> 00:26:01,400
this talk is that the other thing that I

00:25:59,210 --> 00:26:03,650
encounter a lot is that people have

00:26:01,400 --> 00:26:05,630
heard of one particular database that

00:26:03,650 --> 00:26:07,490
handles big data and therefore they are

00:26:05,630 --> 00:26:09,770
convinced that they have to use if they

00:26:07,490 --> 00:26:13,880
have a lot of data they have to use that

00:26:09,770 --> 00:26:16,010
particular data system and sometimes

00:26:13,880 --> 00:26:17,809
that's fine but sometimes that system is

00:26:16,010 --> 00:26:19,100
really inappropriate for what they want

00:26:17,809 --> 00:26:21,260
to use it for and they go through a lot

00:26:19,100 --> 00:26:24,230
of pain so I want to actually go over a

00:26:21,260 --> 00:26:26,390
survey of the different types of data

00:26:24,230 --> 00:26:29,549
warehousing systems data warehousing

00:26:26,390 --> 00:26:32,580
database systems there are

00:26:29,549 --> 00:26:36,749
and what they're good for did bad for in

00:26:32,580 --> 00:26:39,720
general so here's our five types one is

00:26:36,749 --> 00:26:42,659
your standard relational databases the

00:26:39,720 --> 00:26:44,610
second is MPP relational databases will

00:26:42,659 --> 00:26:49,499
explain what that means in a minute the

00:26:44,610 --> 00:26:51,509
third is columnstore databases fourth is

00:26:49,499 --> 00:26:53,580
MapReduce supporting systems and the

00:26:51,509 --> 00:26:54,539
fifth is enterprise search these are

00:26:53,580 --> 00:26:56,340
actually in kind of a rough

00:26:54,539 --> 00:26:59,700
chronological order in the order of

00:26:56,340 --> 00:27:01,200
which they were invented so our first

00:26:59,700 --> 00:27:03,409
thing is standard relational databases

00:27:01,200 --> 00:27:06,090
and so here I'm talking about postgresql

00:27:03,409 --> 00:27:10,980
mysql oracle sequel server that sort of

00:27:06,090 --> 00:27:12,539
thing and the nice thing about standard

00:27:10,980 --> 00:27:15,119
relational databases are kind of

00:27:12,539 --> 00:27:17,340
designed to do everything they don't

00:27:15,119 --> 00:27:21,869
necessarily do everything well but they

00:27:17,340 --> 00:27:23,519
do do everything so they become actually

00:27:21,869 --> 00:27:26,489
sort of an all-purpose solution if your

00:27:23,519 --> 00:27:28,200
data is not particularly large and these

00:27:26,489 --> 00:27:30,210
days given the speed of hardware not

00:27:28,200 --> 00:27:33,090
particularly large tends to be more in

00:27:30,210 --> 00:27:36,269
the realm of a couple of terabytes you

00:27:33,090 --> 00:27:38,279
know so it's you know the sort of

00:27:36,269 --> 00:27:40,230
adequate tall task it easy to use they

00:27:38,279 --> 00:27:43,499
have low resource requirements they're

00:27:40,230 --> 00:27:45,840
well supported by all kinds of different

00:27:43,499 --> 00:27:47,129
software they're very familiar you know

00:27:45,840 --> 00:27:51,389
it's just that you don't necessarily

00:27:47,129 --> 00:27:54,690
want to throw your 790 gigabytes on post

00:27:51,389 --> 00:27:55,859
quiz or MySQL because it's not really

00:27:54,690 --> 00:27:59,399
going to handle it well I mean actually

00:27:55,859 --> 00:28:01,649
this is this is a quietly oversimplified

00:27:59,399 --> 00:28:03,269
sizing chart but it gives you an idea of

00:28:01,649 --> 00:28:06,629
what I'm talking about so like you know

00:28:03,269 --> 00:28:09,480
you've got half a terabyte stigma of

00:28:06,629 --> 00:28:11,129
mysql is fine post-crisis depending on

00:28:09,480 --> 00:28:14,759
your use case may be good for up to

00:28:11,129 --> 00:28:17,009
about eight terabytes you know or so but

00:28:14,759 --> 00:28:19,470
you know once you start getting north of

00:28:17,009 --> 00:28:21,570
a few terabytes don't you start getting

00:28:19,470 --> 00:28:22,980
in like the five terabyte range you're

00:28:21,570 --> 00:28:24,539
going to really it's time to start

00:28:22,980 --> 00:28:26,399
thinking it with with current Hardware

00:28:24,539 --> 00:28:28,529
it's time to start thinking about using

00:28:26,399 --> 00:28:31,710
a data system that is specifically

00:28:28,529 --> 00:28:34,350
designed for large data what are the

00:28:31,710 --> 00:28:36,090
ones that I'm going to talk about so

00:28:34,350 --> 00:28:38,100
speaking of data systems that are

00:28:36,090 --> 00:28:41,470
designed for large data let's talk about

00:28:38,100 --> 00:28:43,710
our first one historically which is MPP

00:28:41,470 --> 00:28:47,020
another acronym NP p stands for

00:28:43,710 --> 00:28:49,690
massively parallel processing horrible

00:28:47,020 --> 00:28:52,299
marketing term but it's stuck and what

00:28:49,690 --> 00:28:55,510
this specifically refers to is these are

00:28:52,299 --> 00:28:57,100
basically relational databases but the

00:28:55,510 --> 00:29:01,390
query engines are designed not just

00:28:57,100 --> 00:29:03,070
execute queries across on a single

00:29:01,390 --> 00:29:04,390
processor across multiple processors on

00:29:03,070 --> 00:29:08,100
a machine but in fact across multiple

00:29:04,390 --> 00:29:10,659
machines this is actually one of our

00:29:08,100 --> 00:29:12,210
marketing query diagrams from Greenplum

00:29:10,659 --> 00:29:15,070
back from when I worked at green plum

00:29:12,210 --> 00:29:17,200
and you know and they're talking about

00:29:15,070 --> 00:29:18,730
this is that what happens is parts of

00:29:17,200 --> 00:29:20,890
the query get executed on each server

00:29:18,730 --> 00:29:23,530
and then the different servers exchange

00:29:20,890 --> 00:29:25,000
data between each other and there's

00:29:23,530 --> 00:29:27,400
actually a bunch of solutions for this

00:29:25,000 --> 00:29:32,980
because it is the oldest type for

00:29:27,400 --> 00:29:35,110
example Teradata greenplum I Netezza db2

00:29:32,980 --> 00:29:40,120
has a parallel processing addition that

00:29:35,110 --> 00:29:42,460
is basically an MPB database now and the

00:29:40,120 --> 00:29:44,980
you know and there's a couple of others

00:29:42,460 --> 00:29:46,270
that are not represented here and not

00:29:44,980 --> 00:29:51,429
really remember what they are but they

00:29:46,270 --> 00:29:54,940
give the idea so now what NPP is

00:29:51,429 --> 00:29:57,730
generally good for is they're good for

00:29:54,940 --> 00:30:02,049
that CPU intensive data mining among

00:29:57,730 --> 00:30:05,049
other things I they support complex

00:30:02,049 --> 00:30:09,580
CoreLogic they support custom functions

00:30:05,049 --> 00:30:13,390
and complex calculations they support

00:30:09,580 --> 00:30:14,530
reasonably sized data I mean my general

00:30:13,390 --> 00:30:15,789
experience is when you start to have

00:30:14,530 --> 00:30:17,980
problems with them is somewhere in the

00:30:15,789 --> 00:30:19,980
hundreds of terabytes depending on how

00:30:17,980 --> 00:30:21,940
much you're willing to spend on hardware

00:30:19,980 --> 00:30:23,940
generally people actually tend to use

00:30:21,940 --> 00:30:26,200
MPP systems these days on smaller

00:30:23,940 --> 00:30:28,360
systems for cost reasons because the

00:30:26,200 --> 00:30:30,490
thing is that all of the production

00:30:28,360 --> 00:30:32,799
quality MPP systems I know of our

00:30:30,490 --> 00:30:36,039
proprietary and they all have fairly

00:30:32,799 --> 00:30:38,380
expensive licensing you know on the

00:30:36,039 --> 00:30:39,730
order of like I think greenplum

00:30:38,380 --> 00:30:41,830
licensors there by the terabyte and

00:30:39,730 --> 00:30:44,679
they're like ten or fifteen thousand

00:30:41,830 --> 00:30:50,919
dollars per terabyte which really tends

00:30:44,679 --> 00:30:53,320
to add up the but you know for your sort

00:30:50,919 --> 00:30:54,429
of I you know for your sort of data

00:30:53,320 --> 00:30:56,049
mining operation

00:30:54,429 --> 00:30:58,570
can be really excellent particularly if

00:30:56,049 --> 00:31:00,759
you have a mining staff who are familiar

00:30:58,570 --> 00:31:03,279
or who use tools that are designed for

00:31:00,759 --> 00:31:05,259
relational databases also the other

00:31:03,279 --> 00:31:07,509
thing is because of the oldest types a

00:31:05,259 --> 00:31:08,889
lot of the vendors who do these light

00:31:07,509 --> 00:31:10,539
green Policastro data have been

00:31:08,889 --> 00:31:12,789
hybridizing them with some of the types

00:31:10,539 --> 00:31:13,779
we see later on adding features from

00:31:12,789 --> 00:31:15,309
some of the other types of the data

00:31:13,779 --> 00:31:18,220
warehouses so you can get some of the

00:31:15,309 --> 00:31:20,580
functionality of both other types of

00:31:18,220 --> 00:31:22,779
data warehouses including column stores

00:31:20,580 --> 00:31:25,570
so a lot of people ask you know what's a

00:31:22,779 --> 00:31:28,960
column store here are these referred to

00:31:25,570 --> 00:31:30,669
sometimes I sort of simply is that it's

00:31:28,960 --> 00:31:33,639
the sort of 90-degree pivot will you

00:31:30,669 --> 00:31:39,779
take a relational database again and you

00:31:33,639 --> 00:31:39,779
take everything you take like this is

00:31:39,929 --> 00:31:44,460
this is your standard relational

00:31:41,980 --> 00:31:47,559
database is known as what's a roast or

00:31:44,460 --> 00:31:49,299
so data is stored in rows you know and

00:31:47,559 --> 00:31:50,619
then groups of rows go on a data page

00:31:49,299 --> 00:31:52,570
and they get stored in a table and that

00:31:50,619 --> 00:31:54,820
sort of thing and that means that any

00:31:52,570 --> 00:31:56,499
time you're querying a row any time

00:31:54,820 --> 00:31:59,590
you're looking for an entire row that's

00:31:56,499 --> 00:32:02,200
a relatively fast operation the problem

00:31:59,590 --> 00:32:04,690
is that what if you want only one column

00:32:02,200 --> 00:32:07,059
over millions of rows or billions of

00:32:04,690 --> 00:32:08,980
rows you have to actually read the whole

00:32:07,059 --> 00:32:09,909
rest of the row for all of those rows I

00:32:08,980 --> 00:32:11,860
mean someone who can get it from an

00:32:09,909 --> 00:32:13,299
index or something like that but if

00:32:11,860 --> 00:32:15,369
we're going to the heap you have to

00:32:13,299 --> 00:32:18,490
actually read every row so you've got

00:32:15,369 --> 00:32:20,169
like say you know what they did for

00:32:18,490 --> 00:32:24,340
column stores and this was introduced in

00:32:20,169 --> 00:32:26,409
two thousand was they turned that 90

00:32:24,340 --> 00:32:29,649
degrees and so the data is actually

00:32:26,409 --> 00:32:31,720
stored in columns and the database

00:32:29,649 --> 00:32:33,940
maintains pointers as to which of those

00:32:31,720 --> 00:32:35,919
columns go together as a row and that

00:32:33,940 --> 00:32:38,379
means that any time you're looking up an

00:32:35,919 --> 00:32:41,350
individual column it's very fast because

00:32:38,379 --> 00:32:43,240
those records are all contiguous all

00:32:41,350 --> 00:32:45,070
contiguous but if you have to actually

00:32:43,240 --> 00:32:47,019
join a lot of rows together it's slow

00:32:45,070 --> 00:32:53,919
because that information is is not

00:32:47,019 --> 00:32:55,299
contiguous oh so you know it's basically

00:32:53,919 --> 00:32:57,909
it's a sort of inverse right it's

00:32:55,299 --> 00:33:00,129
imagine if if you're used to indexes and

00:32:57,909 --> 00:33:01,690
tables in a relational database it's

00:33:00,129 --> 00:33:04,419
like your indexes became your data on

00:33:01,690 --> 00:33:07,340
your data became your indexes there's a

00:33:04,419 --> 00:33:11,040
number of tools for these

00:33:07,340 --> 00:33:13,380
vertica was the first commercialized one

00:33:11,040 --> 00:33:16,740
and people often consider it the most

00:33:13,380 --> 00:33:19,200
production quality one pair Excel info

00:33:16,740 --> 00:33:22,650
bright as a bicycle table types

00:33:19,200 --> 00:33:24,360
columnstore lucid DB independent column

00:33:22,650 --> 00:33:26,210
store database pretty good when a TV and

00:33:24,360 --> 00:33:29,520
know anything about it is a column store

00:33:26,210 --> 00:33:31,500
so column stores are good for

00:33:29,520 --> 00:33:35,940
aggregations and transformations of

00:33:31,500 --> 00:33:37,950
highly structured data because they do

00:33:35,940 --> 00:33:40,049
you have to everything has to be

00:33:37,950 --> 00:33:41,520
organized into columns and the data and

00:33:40,049 --> 00:33:43,710
the columns has to be remarkably similar

00:33:41,520 --> 00:33:46,710
free to each other and for coasters to

00:33:43,710 --> 00:33:49,260
really win they do things like run

00:33:46,710 --> 00:33:50,640
length compression on the values which

00:33:49,260 --> 00:33:53,370
means that you have to have sort of

00:33:50,640 --> 00:33:55,770
defined range of values so that makes

00:33:53,370 --> 00:33:58,169
them really good for business

00:33:55,770 --> 00:34:00,570
intelligence analytics graphing that

00:33:58,169 --> 00:34:04,140
sort of thing it makes them not very

00:34:00,570 --> 00:34:06,240
good for data mining now column stores

00:34:04,140 --> 00:34:07,770
if you have highly structured data comp

00:34:06,240 --> 00:34:10,050
stores can also be good for archiving

00:34:07,770 --> 00:34:11,609
because pretty much all of the serious

00:34:10,050 --> 00:34:14,760
column stores have compression built in

00:34:11,609 --> 00:34:15,929
and so that saves you on storage even if

00:34:14,760 --> 00:34:19,919
it is a lot of features you don't really

00:34:15,929 --> 00:34:21,929
need for that also they're generally

00:34:19,919 --> 00:34:23,220
they're very slow to write to because of

00:34:21,929 --> 00:34:25,169
the reorganization of the data that

00:34:23,220 --> 00:34:28,109
takes place when you insert new data and

00:34:25,169 --> 00:34:30,179
frequently updating existing data or

00:34:28,109 --> 00:34:32,369
deleting data is actually impossible you

00:34:30,179 --> 00:34:34,679
have to take the existing data and we

00:34:32,369 --> 00:34:36,929
build it into a new table and then

00:34:34,679 --> 00:34:41,369
delete the previous table so imagine

00:34:36,929 --> 00:34:43,770
it's fairly expensive so one of the

00:34:41,369 --> 00:34:46,800
things to us in roughly 2000 although it

00:34:43,770 --> 00:34:49,250
was a while before it showed any real

00:34:46,800 --> 00:34:54,060
use in the field and that was this

00:34:49,250 --> 00:34:55,800
technique called MapReduce what you hear

00:34:54,060 --> 00:34:57,600
about a lot MapReduce is basically it's

00:34:55,800 --> 00:35:01,440
an idea it's a technique it's sort of an

00:34:57,600 --> 00:35:06,600
algorithm for doing distributed

00:35:01,440 --> 00:35:08,820
processing of any kind really where you

00:35:06,600 --> 00:35:11,190
divide you do functional programming and

00:35:08,820 --> 00:35:13,530
you divide all of your functions into a

00:35:11,190 --> 00:35:15,210
map section or reduce section and so use

00:35:13,530 --> 00:35:16,530
the map to gather all of your data in

00:35:15,210 --> 00:35:17,710
the reduced section to actually process

00:35:16,530 --> 00:35:21,339
the data and

00:35:17,710 --> 00:35:23,650
it on to the next stage and this is

00:35:21,339 --> 00:35:26,980
generally best done within a MapReduce

00:35:23,650 --> 00:35:30,220
framework a tool that actually supports

00:35:26,980 --> 00:35:32,339
doing running MapReduce code across a

00:35:30,220 --> 00:35:34,390
large number of servers the most popular

00:35:32,339 --> 00:35:37,420
overwhelmingly the most popular these is

00:35:34,390 --> 00:35:40,089
Hadoop people use Hadoop for I mean to

00:35:37,420 --> 00:35:43,260
the point where Hadoop is really sort of

00:35:40,089 --> 00:35:46,450
your open source solution for MapReduce

00:35:43,260 --> 00:35:48,130
couchdb actually does have some support

00:35:46,450 --> 00:35:49,950
from MapReduce operations it's not

00:35:48,130 --> 00:35:52,180
really suitable for data warehousing

00:35:49,950 --> 00:35:53,950
because they have limitations and

00:35:52,180 --> 00:35:56,619
they're not very good at storing large

00:35:53,950 --> 00:35:58,510
amounts of data but for fairly small

00:35:56,619 --> 00:36:04,349
amounts of highly unstructured data can

00:35:58,510 --> 00:36:04,349
be useful aster is a hybrid MapReduce

00:36:04,500 --> 00:36:08,470
MapReduce relational database based on

00:36:06,670 --> 00:36:11,020
post grows as is one I forgot to put on

00:36:08,470 --> 00:36:15,210
here which is called adapt which is a

00:36:11,020 --> 00:36:18,280
hybrid MapReduce relational database now

00:36:15,210 --> 00:36:20,800
is an example a one of your textbook

00:36:18,280 --> 00:36:24,369
examples a very simple MapReduce

00:36:20,800 --> 00:36:28,030
algorithm that basically just says if if

00:36:24,369 --> 00:36:30,130
we meet this criteria then go ahead and

00:36:28,030 --> 00:36:32,740
admit the value and return to the

00:36:30,130 --> 00:36:34,750
function key the one of the big problems

00:36:32,740 --> 00:36:36,490
with MapReduce is compared to some of

00:36:34,750 --> 00:36:38,740
the earlier types we looked at like MPP

00:36:36,490 --> 00:36:41,260
which is actually sort of your main

00:36:38,740 --> 00:36:43,599
contrast because both of these are very

00:36:41,260 --> 00:36:45,220
good for data mining is that it's

00:36:43,599 --> 00:36:46,930
basically taking something that the

00:36:45,220 --> 00:36:48,550
system used to do for you and putting it

00:36:46,930 --> 00:36:50,109
on you as the programmer could develop

00:36:48,550 --> 00:36:52,300
the algorithm for how to find the data

00:36:50,109 --> 00:36:54,460
and that's both a disadvantage and

00:36:52,300 --> 00:36:56,830
advantage it's an advantage in that

00:36:54,460 --> 00:36:58,750
you're not limited to thoughts of

00:36:56,830 --> 00:37:00,430
standard relational or tabular data you

00:36:58,750 --> 00:37:02,470
can run MapReduce systems / libraries

00:37:00,430 --> 00:37:04,240
and images you can run MapReduce systems

00:37:02,470 --> 00:37:06,040
/ bioinformatics you'll run you can do

00:37:04,240 --> 00:37:07,990
any any kind of processing you can write

00:37:06,040 --> 00:37:14,040
as code can be reduced to a MapReduce

00:37:07,990 --> 00:37:17,170
algorithm the disadvantage is that your

00:37:14,040 --> 00:37:19,210
requests to the MapReduce system tend to

00:37:17,170 --> 00:37:20,710
get really complicated and tend to be a

00:37:19,210 --> 00:37:22,750
lot of code and a lot of maintenance and

00:37:20,710 --> 00:37:25,450
you have to hire very smart people to

00:37:22,750 --> 00:37:29,770
write them for you MapReduce is not a

00:37:25,450 --> 00:37:31,579
point-and-click operation so you're sort

00:37:29,770 --> 00:37:34,430
of contrast here is map

00:37:31,579 --> 00:37:35,900
versus MPP now tremendous advantage open

00:37:34,430 --> 00:37:38,209
sort there's a lot open source and free

00:37:35,900 --> 00:37:42,589
implementations and therefore that

00:37:38,209 --> 00:37:45,589
drives the costs way down also you can

00:37:42,589 --> 00:37:47,690
do there's there's no theoretical limit

00:37:45,589 --> 00:37:50,209
on the amount of data that you could

00:37:47,690 --> 00:37:53,109
pump into a MapReduce system and so that

00:37:50,209 --> 00:37:55,579
tends to go into the petabyte department

00:37:53,109 --> 00:37:57,259
you you know but on the other hand you

00:37:55,579 --> 00:38:00,259
can writing routines by hand as opposed

00:37:57,259 --> 00:38:01,640
to using an advanced query planner you

00:38:00,259 --> 00:38:04,759
you are the query planner and a

00:38:01,640 --> 00:38:07,489
MapReduce routine you know MapReduce

00:38:04,759 --> 00:38:08,749
tends to be very resource inefficient in

00:38:07,489 --> 00:38:10,539
terms of the amount of resources

00:38:08,749 --> 00:38:13,489
required to do the same amount of work

00:38:10,539 --> 00:38:15,229
you know but it's a generic solution as

00:38:13,489 --> 00:38:18,529
opposed to a specific sort of relational

00:38:15,229 --> 00:38:20,660
database solution also the nice thing

00:38:18,529 --> 00:38:23,989
about it is because it is inefficient

00:38:20,660 --> 00:38:26,390
and highly distributed you can use cheap

00:38:23,989 --> 00:38:27,829
software cloud hosting and that sort of

00:38:26,390 --> 00:38:29,420
thing where's you can't really do that

00:38:27,829 --> 00:38:32,559
with an MPP platform that's really

00:38:29,420 --> 00:38:35,690
expecting good very consistent hardware

00:38:32,559 --> 00:38:37,339
so so this you know the sort of decision

00:38:35,690 --> 00:38:39,140
making to go through and deciding which

00:38:37,339 --> 00:38:41,660
which option makes more sense for say a

00:38:39,140 --> 00:38:43,729
data mining which both plat both types

00:38:41,660 --> 00:38:46,400
of data warehouse is a very good at so

00:38:43,729 --> 00:38:48,319
this has been enterprise search is our

00:38:46,400 --> 00:38:51,499
most recent entrant into the data

00:38:48,319 --> 00:38:53,390
warehousing arena and Enterprise Search

00:38:51,499 --> 00:38:56,180
actually started out with what it sounds

00:38:53,390 --> 00:38:59,680
like with search engines patike

00:38:56,180 --> 00:39:03,049
specifically with the lucene project and

00:38:59,680 --> 00:39:04,729
what people realized was that they

00:39:03,049 --> 00:39:07,670
didn't have to use with scene just to do

00:39:04,729 --> 00:39:09,949
search on their live website you could

00:39:07,670 --> 00:39:11,479
actually store a large amount of text on

00:39:09,949 --> 00:39:13,849
your servers and use the scene to do

00:39:11,479 --> 00:39:18,380
search internally and so people a couple

00:39:13,849 --> 00:39:19,969
of different forked projects I decided

00:39:18,380 --> 00:39:22,630
to do this in different ways one is the

00:39:19,969 --> 00:39:26,930
Apache Solr the alone is elastic search

00:39:22,630 --> 00:39:29,839
and and they've added a whole bunch of

00:39:26,930 --> 00:39:31,819
features to these to the point where

00:39:29,839 --> 00:39:34,279
they are effectively data warehousing

00:39:31,819 --> 00:39:36,289
databases and they can actually solve

00:39:34,279 --> 00:39:38,079
some of the same problems you would have

00:39:36,289 --> 00:39:40,430
solved with other types of databases

00:39:38,079 --> 00:39:43,910
particularly cases that are oriented

00:39:40,430 --> 00:39:45,080
towards locating and filtering small

00:39:43,910 --> 00:39:50,360
amounts of data from the very

00:39:45,080 --> 00:39:52,970
large data population so we're really

00:39:50,360 --> 00:39:55,280
wins is you have a large amount of data

00:39:52,970 --> 00:39:59,240
most of which you know very little about

00:39:55,280 --> 00:40:03,440
the structure so it might be memos it

00:39:59,240 --> 00:40:06,020
might be web pages it might be HTML

00:40:03,440 --> 00:40:07,880
documents that might be JSON documents

00:40:06,020 --> 00:40:10,640
whatever you know so you know very

00:40:07,880 --> 00:40:14,120
little about what's actually in them and

00:40:10,640 --> 00:40:15,980
you need to flame things in so and you

00:40:14,120 --> 00:40:18,530
can do both sort of lightweight data

00:40:15,980 --> 00:40:21,230
mining and support bi and analytic

00:40:18,530 --> 00:40:22,850
systems they also have an interesting

00:40:21,230 --> 00:40:24,410
thing that is regarded as either a

00:40:22,850 --> 00:40:26,330
feature or a Miss feature depending on

00:40:24,410 --> 00:40:28,760
what you want which is the ability to

00:40:26,330 --> 00:40:32,780
return approximate results in order to

00:40:28,760 --> 00:40:34,670
return results faster the and of course

00:40:32,780 --> 00:40:37,550
they come built in with a lot of special

00:40:34,670 --> 00:40:38,900
features for web-based app so in a lot

00:40:37,550 --> 00:40:41,690
of ways if you're sort of looking at

00:40:38,900 --> 00:40:43,930
this you say you know you're sort of

00:40:41,690 --> 00:40:47,450
division is you might be looking at

00:40:43,930 --> 00:40:49,880
elasticsearch versus column store as a

00:40:47,450 --> 00:40:50,900
contrast in those things because

00:40:49,880 --> 00:40:52,190
particularly this was actually where I

00:40:50,900 --> 00:40:56,360
was specifically looking at it in one

00:40:52,190 --> 00:40:59,150
project because of the support for what

00:40:56,360 --> 00:41:00,530
they added to enterprise search both in

00:40:59,150 --> 00:41:02,630
solar and elastic search to support with

00:41:00,530 --> 00:41:04,490
isn't called faceting and fascinating

00:41:02,630 --> 00:41:07,330
our materialized indexes and you can use

00:41:04,490 --> 00:41:09,590
the materialized indexes to do basically

00:41:07,330 --> 00:41:12,800
column level information out of your

00:41:09,590 --> 00:41:16,250
data so you have to update both of them

00:41:12,800 --> 00:41:18,050
by batch load enterprise search is

00:41:16,250 --> 00:41:20,750
better for semi-structured data c-stores

00:41:18,050 --> 00:41:22,430
better for fully normalized data so far

00:41:20,750 --> 00:41:24,470
all the search implementations have been

00:41:22,430 --> 00:41:28,280
uncompressed that could that could be

00:41:24,470 --> 00:41:30,560
changed generally with this you know if

00:41:28,280 --> 00:41:31,550
you need to scale it massively for like

00:41:30,560 --> 00:41:34,040
elasticsearch you're going to do

00:41:31,550 --> 00:41:36,260
sharding for columnstore you're going to

00:41:34,040 --> 00:41:39,320
do parallel query approximate results

00:41:36,260 --> 00:41:41,210
exact results now there's still some

00:41:39,320 --> 00:41:42,920
stuff to be implemented in enterprise

00:41:41,210 --> 00:41:45,080
search to really make it sort of data

00:41:42,920 --> 00:41:47,720
warehousing more of a data warehousing

00:41:45,080 --> 00:41:49,490
contender than it is now for example you

00:41:47,720 --> 00:41:50,570
can really only do us I said this was

00:41:49,490 --> 00:41:52,550
going to come up again you can really

00:41:50,570 --> 00:41:54,590
only do a star schema you can do we only

00:41:52,550 --> 00:41:57,110
have one table and in any kind of

00:41:54,590 --> 00:41:58,660
enterprise search implementation it

00:41:57,110 --> 00:42:01,810
doesn't have the concept of having diff

00:41:58,660 --> 00:42:04,990
divisions among your data and currently

00:42:01,810 --> 00:42:07,600
for both elasticsearch and solar you

00:42:04,990 --> 00:42:08,740
can't combine indexes and it doesn't

00:42:07,600 --> 00:42:11,170
really have any concept of data

00:42:08,740 --> 00:42:13,480
aggregation so if you really want to do

00:42:11,170 --> 00:42:15,370
full-on bi stuff you have to use

00:42:13,480 --> 00:42:17,770
Enterprise Search to generate your data

00:42:15,370 --> 00:42:19,390
and then feed it to another system such

00:42:17,770 --> 00:42:22,060
as a relational database or MapReduce

00:42:19,390 --> 00:42:26,760
system in order to do your aggregation

00:42:22,060 --> 00:42:29,620
for you to represent the analytics the

00:42:26,760 --> 00:42:31,330
but you know for people with really

00:42:29,620 --> 00:42:33,370
massive amounts of data it can be worth

00:42:31,330 --> 00:42:34,330
going to that work because one of the

00:42:33,370 --> 00:42:35,980
things that the column stirs don't

00:42:34,330 --> 00:42:38,140
really that none of the existing column

00:42:35,980 --> 00:42:40,150
stores really have is any support for

00:42:38,140 --> 00:42:42,580
having large-scale clustered databases

00:42:40,150 --> 00:42:44,650
so for really efficient columnstore

00:42:42,580 --> 00:42:49,030
operation you're limited to what you can

00:42:44,650 --> 00:42:51,580
put on a single server so few other

00:42:49,030 --> 00:42:55,390
concepts that I want to go over see

00:42:51,580 --> 00:42:56,530
we've got about 15 minutes left so I'm

00:42:55,390 --> 00:42:57,580
going to actually breeze through these

00:42:56,530 --> 00:43:00,220
so that will still have some time for

00:42:57,580 --> 00:43:01,960
questions because I got over the main

00:43:00,220 --> 00:43:03,280
things I wanted to cover some of the

00:43:01,960 --> 00:43:04,630
other things so I work in the relational

00:43:03,280 --> 00:43:06,250
database world that work specifically on

00:43:04,630 --> 00:43:07,750
postcards and I wanted to actually

00:43:06,250 --> 00:43:09,520
address a couple of other things that

00:43:07,750 --> 00:43:12,010
are really useful data warehousing that

00:43:09,520 --> 00:43:15,760
people frequently don't know about at

00:43:12,010 --> 00:43:16,810
all one of this is windowing queries how

00:43:15,760 --> 00:43:20,350
many people here know what a window

00:43:16,810 --> 00:43:21,670
where he is yeah see and one other

00:43:20,350 --> 00:43:24,280
person is not raising his hand in the

00:43:21,670 --> 00:43:28,570
back the because I know he does what win

00:43:24,280 --> 00:43:29,650
do queries so one of the things because

00:43:28,570 --> 00:43:31,300
this is actually a concept that could

00:43:29,650 --> 00:43:37,480
translate to non-relational databases it

00:43:31,300 --> 00:43:39,400
just hasn't yet the so um the idea of a

00:43:37,480 --> 00:43:41,590
conventional aggregate is you have your

00:43:39,400 --> 00:43:44,770
bulk data here and it can condense it

00:43:41,590 --> 00:43:46,480
down to a handful you know of this so

00:43:44,770 --> 00:43:48,670
like these are say the sums of a bunch

00:43:46,480 --> 00:43:51,820
of values right so running the sum and

00:43:48,670 --> 00:43:56,230
we do that the idea of a windowing query

00:43:51,820 --> 00:43:59,110
is that instead of just condensing the

00:43:56,230 --> 00:44:01,330
aggregate is you can get back both the

00:43:59,110 --> 00:44:04,600
values and you can run aggregate

00:44:01,330 --> 00:44:07,000
operations over a window of the other

00:44:04,600 --> 00:44:09,460
data and this way you can say for

00:44:07,000 --> 00:44:11,720
example compare the value for an

00:44:09,460 --> 00:44:14,480
individual row to the aggregate value

00:44:11,720 --> 00:44:16,460
so say for example I want to see all of

00:44:14,480 --> 00:44:19,790
the rows that are more than ten percent

00:44:16,460 --> 00:44:21,230
above the average for the set that's an

00:44:19,790 --> 00:44:22,670
operation doing all the time in

00:44:21,230 --> 00:44:24,859
analytics that sort of thing I want to

00:44:22,670 --> 00:44:27,440
see everybody who's who's in the top 10%

00:44:24,859 --> 00:44:29,020
for the set that's the sort of operation

00:44:27,440 --> 00:44:33,710
doing all the time and that requires a

00:44:29,020 --> 00:44:35,450
windowing operation now sequel has a

00:44:33,710 --> 00:44:38,480
whole syntax for when doing operations

00:44:35,450 --> 00:44:40,220
so say for example we had this event

00:44:38,480 --> 00:44:43,310
table this is a good example this could

00:44:40,220 --> 00:44:44,780
be machines turning on and off this

00:44:43,310 --> 00:44:48,020
could be I actually did this basically

00:44:44,780 --> 00:44:49,609
for database connections wherever you

00:44:48,020 --> 00:44:51,050
have you have an event got some

00:44:49,609 --> 00:44:53,060
surrogate key for the event thing of a

00:44:51,050 --> 00:44:54,440
type of event and then you have a start

00:44:53,060 --> 00:44:57,290
of the event and you have how long the

00:44:54,440 --> 00:45:00,530
event went on right and what I want to

00:44:57,290 --> 00:45:02,840
know is say for example at what second

00:45:00,530 --> 00:45:06,740
were the most concurrent events going on

00:45:02,840 --> 00:45:10,460
and how many events were they and so

00:45:06,740 --> 00:45:14,930
then you can actually do a sort of pile

00:45:10,460 --> 00:45:17,510
up here where you stretch out your

00:45:14,930 --> 00:45:21,650
series of events into a group of ones

00:45:17,510 --> 00:45:24,230
and negative ones and then everything

00:45:21,650 --> 00:45:26,900
adds one as it starts and subtracts one

00:45:24,230 --> 00:45:30,280
as it ends and then you get your

00:45:26,900 --> 00:45:33,560
windowing phrase up here where it says

00:45:30,280 --> 00:45:37,130
order by the start and then select the

00:45:33,560 --> 00:45:41,420
sum and then I want to actually know the

00:45:37,130 --> 00:45:42,920
maximum concurrent ones how you can look

00:45:41,420 --> 00:45:44,450
at this later this is actually on the

00:45:42,920 --> 00:45:46,430
this presentation is on laying a look at

00:45:44,450 --> 00:45:48,680
it later that's the basic idea and a lot

00:45:46,430 --> 00:45:50,570
of the concept here is in SQL is it

00:45:48,680 --> 00:45:53,300
takes the concept of stream processing

00:45:50,570 --> 00:45:54,290
for evident people know said the command

00:45:53,300 --> 00:45:56,330
line utility and that sort of thing

00:45:54,290 --> 00:45:58,430
takes the process of stream processing

00:45:56,330 --> 00:45:59,990
to SQL and it turns out when you're

00:45:58,430 --> 00:46:01,250
dealing with really large volumes of

00:45:59,990 --> 00:46:04,520
data stream processing is a very

00:46:01,250 --> 00:46:06,440
powerful concept because writing a bunch

00:46:04,520 --> 00:46:07,790
of data then retrieving it and doing an

00:46:06,440 --> 00:46:09,170
operation to it and then writing it

00:46:07,790 --> 00:46:10,760
again and then retrieving it during the

00:46:09,170 --> 00:46:13,730
operation do again is really inefficient

00:46:10,760 --> 00:46:16,609
it would be much better to pump the data

00:46:13,730 --> 00:46:17,930
through a process and do all of the

00:46:16,609 --> 00:46:22,550
different things you needed to do to it

00:46:17,930 --> 00:46:24,740
in series before writing it and within

00:46:22,550 --> 00:46:25,310
the SQL world window enquiries can

00:46:24,740 --> 00:46:27,050
actually

00:46:25,310 --> 00:46:28,970
view that and allow you to replace

00:46:27,050 --> 00:46:30,800
literally pages of application code a

00:46:28,970 --> 00:46:32,510
lot of cases one of the examples that I

00:46:30,800 --> 00:46:36,140
have on a blog or something is where I

00:46:32,510 --> 00:46:37,970
actually did the rankings for an entire

00:46:36,140 --> 00:46:42,860
fantasy football league using a single

00:46:37,970 --> 00:46:44,510
window inquiry so now another concept

00:46:42,860 --> 00:46:45,980
and this one does translate to both

00:46:44,510 --> 00:46:47,690
relational anomaly tional databases

00:46:45,980 --> 00:46:48,890
materialized views something you should

00:46:47,690 --> 00:46:51,020
actually understand the basic idea of

00:46:48,890 --> 00:46:53,750
materialized views is query results as a

00:46:51,020 --> 00:46:57,700
table that is I'm going to take the

00:46:53,750 --> 00:47:02,600
results of a query request and store it

00:46:57,700 --> 00:47:04,130
as a and put it on storage so that when

00:47:02,600 --> 00:47:06,110
I want to run that request in the future

00:47:04,130 --> 00:47:08,870
I can get the answers a lot back a lot

00:47:06,110 --> 00:47:10,430
faster and where this really works is

00:47:08,870 --> 00:47:11,960
where the underlying query is going to

00:47:10,430 --> 00:47:13,130
be expensive either because it acts as a

00:47:11,960 --> 00:47:15,860
whole lot of data because it does a

00:47:13,130 --> 00:47:17,360
whole lot of calculation and where it's

00:47:15,860 --> 00:47:19,640
going to be frequently referenced we're

00:47:17,360 --> 00:47:21,410
going to want that information a lot now

00:47:19,640 --> 00:47:24,490
you're not necessarily going to build a

00:47:21,410 --> 00:47:26,720
material query a materialized view as

00:47:24,490 --> 00:47:28,670
exactly the query that you want for the

00:47:26,720 --> 00:47:30,260
end user often it'll be sort of part of

00:47:28,670 --> 00:47:31,850
it you know it will be something that

00:47:30,260 --> 00:47:34,250
was going to be in the from and where

00:47:31,850 --> 00:47:36,890
clauses or something that's a

00:47:34,250 --> 00:47:39,440
pre-processing thing like right before

00:47:36,890 --> 00:47:43,130
you run the group by you know and that

00:47:39,440 --> 00:47:44,660
sort of thing and you might manually or

00:47:43,130 --> 00:47:47,690
automatically update it it really

00:47:44,660 --> 00:47:49,850
depends on the product and what sort of

00:47:47,690 --> 00:47:50,780
support that you have for the

00:47:49,850 --> 00:47:52,280
materialized either but that's the

00:47:50,780 --> 00:47:53,780
essential concept there's a lot of

00:47:52,280 --> 00:47:55,490
tooling that goes with codes but the

00:47:53,780 --> 00:47:57,080
central concept is I'm going to take a

00:47:55,490 --> 00:47:59,450
query or part of a query I was going to

00:47:57,080 --> 00:48:01,730
run anyway and run it ahead of time and

00:47:59,450 --> 00:48:03,320
store the results somewhere permanent so

00:48:01,730 --> 00:48:07,250
that I don't have to do that at user

00:48:03,320 --> 00:48:09,920
request time now this also translates

00:48:07,250 --> 00:48:12,830
into non-relational databases a couple

00:48:09,920 --> 00:48:15,620
of examples here couchdb has this

00:48:12,830 --> 00:48:17,480
concept of views and basically what you

00:48:15,620 --> 00:48:20,360
do is you run a MapReduce operation in

00:48:17,480 --> 00:48:22,660
CouchDB which is expensive and then you

00:48:20,360 --> 00:48:26,270
store that MapReduce operation as a view

00:48:22,660 --> 00:48:28,910
and that's actually updated at request

00:48:26,270 --> 00:48:31,220
time which I had some issues with but

00:48:28,910 --> 00:48:33,170
well talk about that later it's updated

00:48:31,220 --> 00:48:35,090
request time but it's actually stored as

00:48:33,170 --> 00:48:37,190
a separate structure within the CouchDB

00:48:35,090 --> 00:48:38,630
database so that if you need to run that

00:48:37,190 --> 00:48:41,210
MapReduce operation again you can get

00:48:38,630 --> 00:48:43,369
the answers back a lot faster rather

00:48:41,210 --> 00:48:46,880
than having to wait for them for that

00:48:43,369 --> 00:48:48,440
matter what they said the enterprise

00:48:46,880 --> 00:48:51,829
search things have this concept called

00:48:48,440 --> 00:48:53,599
facets and the facets are currently they

00:48:51,829 --> 00:48:54,920
need a lot of expansion to make them

00:48:53,599 --> 00:48:57,710
actually a lot more usable the fastest

00:48:54,920 --> 00:48:59,509
currently basically you can form a

00:48:57,710 --> 00:49:01,730
request for certain data and then store

00:48:59,509 --> 00:49:04,549
that as a facet and then when you need

00:49:01,730 --> 00:49:06,049
to actually run that as part of your

00:49:04,549 --> 00:49:09,859
search again you can get it back from

00:49:06,049 --> 00:49:14,779
the FAFSA and these get updated at data

00:49:09,859 --> 00:49:17,690
at data change time so now regardless of

00:49:14,779 --> 00:49:20,119
relational non-relational for really

00:49:17,690 --> 00:49:23,450
large databases there are different ways

00:49:20,119 --> 00:49:25,220
that you can update them the best way to

00:49:23,450 --> 00:49:27,019
do it is if you have some form of

00:49:25,220 --> 00:49:30,289
bachelor process or stream load process

00:49:27,019 --> 00:49:32,839
is that you want to update the map views

00:49:30,289 --> 00:49:36,410
as the data comes in that is the most

00:49:32,839 --> 00:49:39,319
efficient way to deal with it a pretty

00:49:36,410 --> 00:49:42,349
much regardless of that now in some

00:49:39,319 --> 00:49:44,630
cases that's not practical to do and you

00:49:42,349 --> 00:49:46,579
have to do something like updated

00:49:44,630 --> 00:49:48,289
according to a clock or calendar so you

00:49:46,579 --> 00:49:50,900
a time-based table you might update it

00:49:48,289 --> 00:49:53,599
every hour you know in impact in the

00:49:50,900 --> 00:49:56,660
creative Matt view there are some

00:49:53,599 --> 00:49:58,309
systems where it actually checks the map

00:49:56,660 --> 00:50:00,500
view for updates when you run data

00:49:58,309 --> 00:50:03,309
requests like couch TV and then updates

00:50:00,500 --> 00:50:06,259
that when you want a data request ah

00:50:03,309 --> 00:50:08,420
that tends to not be such a good idea

00:50:06,259 --> 00:50:09,920
because the thing is that in a really

00:50:08,420 --> 00:50:12,230
large database those data requests take

00:50:09,920 --> 00:50:13,460
a really long time to run and so you

00:50:12,230 --> 00:50:17,089
don't want to use your waiting for them

00:50:13,460 --> 00:50:19,759
dynamically right the worst for any

00:50:17,089 --> 00:50:23,420
really large database is to have a

00:50:19,759 --> 00:50:25,160
trigger or some analogous concept that

00:50:23,420 --> 00:50:28,309
automatically executes when certain

00:50:25,160 --> 00:50:31,390
things are updated because it's almost

00:50:28,309 --> 00:50:34,940
certain that any time you need to update

00:50:31,390 --> 00:50:36,259
some part of the map view based on data

00:50:34,940 --> 00:50:39,559
change and you're going to need to

00:50:36,259 --> 00:50:42,009
update millions of rows in the map view

00:50:39,559 --> 00:50:44,390
or maybe rebuild the entire map view and

00:50:42,009 --> 00:50:46,069
low level automatically triggered

00:50:44,390 --> 00:50:48,289
functionality in the database like in

00:50:46,069 --> 00:50:50,390
SQL trigger does not have the discretion

00:50:48,289 --> 00:50:51,210
it does stuff row at a time and row at a

00:50:50,390 --> 00:50:52,800
time

00:50:51,210 --> 00:50:57,839
time you have a billion rows is a bad

00:50:52,800 --> 00:51:00,119
idea other tips format views that you

00:50:57,839 --> 00:51:03,869
should be small a tenth to a quarter of

00:51:00,119 --> 00:51:06,720
ram you know at most if if your mat

00:51:03,869 --> 00:51:07,800
views are much larger than that IH know

00:51:06,720 --> 00:51:09,660
if you met these are much larger than

00:51:07,800 --> 00:51:11,010
that then you're not really then you're

00:51:09,660 --> 00:51:13,200
basically going to be reading them from

00:51:11,010 --> 00:51:15,210
disk anyway there are some cases I mean

00:51:13,200 --> 00:51:16,140
when your data is really huge you know

00:51:15,210 --> 00:51:17,880
what you're dealing with a petabyte of

00:51:16,140 --> 00:51:19,920
data than a mat v that's on you know

00:51:17,880 --> 00:51:22,920
that's a mat view that's 200 gigabytes

00:51:19,920 --> 00:51:24,240
is actually seriously advantageous but

00:51:22,920 --> 00:51:25,920
if you're getting in the circumstance

00:51:24,240 --> 00:51:27,510
where the map view is a quarter the size

00:51:25,920 --> 00:51:28,920
of the raw data you're making the mat

00:51:27,510 --> 00:51:33,270
view / then you've done something wrong

00:51:28,920 --> 00:51:36,000
usually Matt views and general should

00:51:33,270 --> 00:51:39,869
support several different query types or

00:51:36,000 --> 00:51:42,240
one really important one also looking at

00:51:39,869 --> 00:51:45,000
Matt views like the base tables look at

00:51:42,240 --> 00:51:47,820
ways that you can append and truncate

00:51:45,000 --> 00:51:48,900
not update the mat views because again

00:51:47,820 --> 00:51:50,820
you have to think about you doing these

00:51:48,900 --> 00:51:53,520
operations over millions or billions of

00:51:50,820 --> 00:51:55,250
rows or documents and so you never want

00:51:53,520 --> 00:51:57,030
to do that on a search and replace

00:51:55,250 --> 00:51:59,520
algorithm because those are always

00:51:57,030 --> 00:52:00,510
really slow and then also if you're

00:51:59,520 --> 00:52:01,950
dealing with system that supports

00:52:00,510 --> 00:52:04,920
indexes of course you want to index the

00:52:01,950 --> 00:52:06,089
map you like crazy unless you're doing

00:52:04,920 --> 00:52:10,740
with system where the mat you basically

00:52:06,089 --> 00:52:12,300
is the index like enterprise search so

00:52:10,740 --> 00:52:14,010
briefly we're running out of time I'm

00:52:12,300 --> 00:52:15,330
gonna go briefly another trim right here

00:52:14,010 --> 00:52:18,300
is OLAP because I brought it up before

00:52:15,330 --> 00:52:19,560
on a local processing this was invented

00:52:18,300 --> 00:52:23,250
by a company that was acquired by

00:52:19,560 --> 00:52:24,780
microsoft in the mid-90s at least as far

00:52:23,250 --> 00:52:28,940
as I know they invented at personal to

00:52:24,780 --> 00:52:31,530
commercializing and the idea of OLAP is

00:52:28,940 --> 00:52:34,410
there's this concept no web called cubes

00:52:31,530 --> 00:52:35,670
which is not strictly accurate the ID of

00:52:34,410 --> 00:52:37,920
the cubes is remember I talked about a

00:52:35,670 --> 00:52:39,450
concept of dimensions right the idea is

00:52:37,920 --> 00:52:41,099
that you actually take your dimensions

00:52:39,450 --> 00:52:44,520
for your data and you create a

00:52:41,099 --> 00:52:46,080
multi-dimensional virtual space where

00:52:44,520 --> 00:52:47,700
each of those dimensions is a spatial

00:52:46,080 --> 00:52:49,740
dimension and then you can navigate

00:52:47,700 --> 00:52:51,660
around it you can zoom in and out and

00:52:49,740 --> 00:52:53,790
you can navigate around the different

00:52:51,660 --> 00:52:55,589
aspects of the dimensions and sort of

00:52:53,790 --> 00:52:58,109
drill down into different things this is

00:52:55,589 --> 00:53:01,830
a output for Mondrian which is the main

00:52:58,109 --> 00:53:03,960
open source of lap tool you know

00:53:01,830 --> 00:53:06,660
drilling down to sort of different

00:53:03,960 --> 00:53:08,970
portions of of your sort of aspects of

00:53:06,660 --> 00:53:11,010
your data it's a visualization technique

00:53:08,970 --> 00:53:12,540
it tends to be a cpu and particularly

00:53:11,010 --> 00:53:14,580
very Ram intensive for one thing I've

00:53:12,540 --> 00:53:17,700
yet to see an OLAP engine that could

00:53:14,580 --> 00:53:20,130
handle spilling to disk and so you tend

00:53:17,700 --> 00:53:21,990
to on really largely left database as

00:53:20,130 --> 00:53:26,040
you can find yourself maxing out your

00:53:21,990 --> 00:53:28,530
budget for ram the it works really well

00:53:26,040 --> 00:53:29,700
together with column stores because it's

00:53:28,530 --> 00:53:33,150
a lot of the same sort of structural

00:53:29,700 --> 00:53:35,730
information so we got a few minutes for

00:53:33,150 --> 00:53:38,160
questions it's my contact information oh

00:53:35,730 --> 00:53:45,270
by the way don't forget post was 9.2 in

00:53:38,160 --> 00:53:58,070
beta currently so questions somebody

00:53:45,270 --> 00:53:58,070
must have some questions yeah yeah yeah

00:54:11,639 --> 00:54:16,179
right well I mean if that it doesn't

00:54:14,469 --> 00:54:19,869
exist you're not going to you're not

00:54:16,179 --> 00:54:22,749
going to get it through the ETL but even

00:54:19,869 --> 00:54:24,759
when you're going from 0 right to repeat

00:54:22,749 --> 00:54:26,409
the question yet so his question was he

00:54:24,759 --> 00:54:29,380
understands ETL when you're coming from

00:54:26,409 --> 00:54:30,729
sort of raw data form like you know Word

00:54:29,380 --> 00:54:32,469
documents or whatever but he's saying

00:54:30,729 --> 00:54:33,669
what sort of transformation you need to

00:54:32,469 --> 00:54:35,529
do when it's coming from another

00:54:33,669 --> 00:54:37,959
database system where it's already

00:54:35,529 --> 00:54:41,499
presumably normalized well the second

00:54:37,959 --> 00:54:43,539
thing is the thing is that the database

00:54:41,499 --> 00:54:45,659
structure that you're using the schema

00:54:43,539 --> 00:54:48,849
the structure of the tables the indexes

00:54:45,659 --> 00:54:50,829
that sort of thing for transaction

00:54:48,849 --> 00:54:53,349
processing is frequently different than

00:54:50,829 --> 00:54:56,380
the one you want for the same data when

00:54:53,349 --> 00:54:58,659
you are mining it or doing analytics

00:54:56,380 --> 00:55:00,609
over it at the very least you're going

00:54:58,659 --> 00:55:01,929
on a different set of indexes frequently

00:55:00,609 --> 00:55:03,429
you want different table structures

00:55:01,929 --> 00:55:06,909
sometimes you can use a different

00:55:03,429 --> 00:55:07,959
database system for the warehouses

00:55:06,909 --> 00:55:10,089
you're using for the transaction

00:55:07,959 --> 00:55:12,219
processing and so there's going to need

00:55:10,089 --> 00:55:15,089
to be some work done some transformation

00:55:12,219 --> 00:55:18,459
to get the data from one part to another

00:55:15,089 --> 00:55:19,869
the you know and also you might be

00:55:18,459 --> 00:55:21,489
updating a bunch of Matt views and

00:55:19,869 --> 00:55:25,839
indexes and doing other cleanup and that

00:55:21,489 --> 00:55:28,149
sort of thing so other questions you've

00:55:25,839 --> 00:55:30,779
got 800 terabytes of data surely you

00:55:28,149 --> 00:55:30,779
must have questions

00:55:42,380 --> 00:56:07,800
story that story really yes that's very

00:55:48,180 --> 00:56:09,780
impressive yeah the yeah the I'm the

00:56:07,800 --> 00:56:11,369
first thing you want to do this a ring

00:56:09,780 --> 00:56:13,050
up the Aster Data folks and see if they

00:56:11,369 --> 00:56:15,690
can somehow graft asked her on top of

00:56:13,050 --> 00:56:18,180
existing post post databases let be my

00:56:15,690 --> 00:56:19,589
first thought honestly because you

00:56:18,180 --> 00:56:21,329
really need something on top of all of

00:56:19,589 --> 00:56:24,930
those databases there allows you to pull

00:56:21,329 --> 00:56:26,790
data from all of them and and nothing in

00:56:24,930 --> 00:56:28,050
vanilla mainstream postgres is going to

00:56:26,790 --> 00:56:33,839
allow you to handle that situation

00:56:28,050 --> 00:56:35,040
easily yeah the you know plus the other

00:56:33,839 --> 00:56:36,630
problem the other problem by the way

00:56:35,040 --> 00:56:38,570
they didn't mention early this is here's

00:56:36,630 --> 00:56:41,339
here's the other thing that becomes a

00:56:38,570 --> 00:56:42,420
really painful reality when you start

00:56:41,339 --> 00:56:44,579
dealing with really large volumes of

00:56:42,420 --> 00:56:48,630
data which is the inability to do

00:56:44,579 --> 00:56:51,630
upgrades of any kind because you simply

00:56:48,630 --> 00:56:54,150
can't anything any kind of upgrading

00:56:51,630 --> 00:56:56,849
that requires the data rewrite is just

00:56:54,150 --> 00:56:58,530
completely out of the question and

00:56:56,849 --> 00:57:00,990
particularly this for postman's 8.2 is

00:56:58,530 --> 00:57:03,810
to get anywhere beyond 8.2 you have to

00:57:00,990 --> 00:57:08,640
rewrite the all of the data because we

00:57:03,810 --> 00:57:10,290
change the file format in 8.3 so the you

00:57:08,640 --> 00:57:11,190
know so you're basically not operating

00:57:10,290 --> 00:57:15,089
and i wouldn't talk to you about

00:57:11,190 --> 00:57:17,190
upgrading the so and you weren't into

00:57:15,089 --> 00:57:18,810
that with other things i mean for

00:57:17,190 --> 00:57:19,950
example i worked on greenplum for a

00:57:18,810 --> 00:57:23,599
while with moon poem we had to really

00:57:19,950 --> 00:57:25,920
work on not making a lot of changes to

00:57:23,599 --> 00:57:27,720
any change the file format and that sort

00:57:25,920 --> 00:57:29,970
of thing so we have time for one more

00:57:27,720 --> 00:57:32,240
question if anybody else is another

00:57:29,970 --> 00:57:32,240
question

00:57:36,900 --> 00:57:42,210
no okay so we'll actually finish on time

00:57:39,280 --> 00:57:42,210
here so thank you very much

00:57:53,140 --> 00:57:55,200
you

00:57:58,380 --> 00:58:03,460
when we created asterisk over a decade

00:58:01,120 --> 00:58:05,410
ago we could not have imagined that

00:58:03,460 --> 00:58:07,630
asterisk would not only become the most

00:58:05,410 --> 00:58:09,760
widely adopted open source communication

00:58:07,630 --> 00:58:11,800
software on the planet but that it would

00:58:09,760 --> 00:58:14,320
impact the entire industry in the way

00:58:11,800 --> 00:58:16,300
that it has today asterisk has found its

00:58:14,320 --> 00:58:18,940
way in the more than 170 countries and

00:58:16,300 --> 00:58:20,890
virtually every fortune 1000 company the

00:58:18,940 --> 00:58:22,960
success of asterisk has enabled a

00:58:20,890 --> 00:58:24,370
transition of power from the hands of

00:58:22,960 --> 00:58:26,650
the traditional proprietary phone

00:58:24,370 --> 00:58:29,050
vendors into the hands of the users and

00:58:26,650 --> 00:58:31,120
administrators of phone systems using

00:58:29,050 --> 00:58:32,770
this power our customers have traded all

00:58:31,120 --> 00:58:34,870
sorts of business changing applications

00:58:32,770 --> 00:58:36,670
from small office phone systems to

00:58:34,870 --> 00:58:38,770
mission-critical call centers the

00:58:36,670 --> 00:58:40,570
international carrier networks in fact

00:58:38,770 --> 00:58:42,040
there's even an entire country those

00:58:40,570 --> 00:58:44,290
communications infrastructure runs on

00:58:42,040 --> 00:58:46,450
esters the gym has always been about

00:58:44,290 --> 00:58:48,160
creating technology that expands

00:58:46,450 --> 00:58:50,320
communications capabilities in ways that

00:58:48,160 --> 00:58:51,520
we could never have imagined and that's

00:58:50,320 --> 00:58:53,920
part of what's game-changing about

00:58:51,520 --> 00:58:56,620
Digium today we're doing it again this

00:58:53,920 --> 00:58:58,690
time by introducing a new family of HD

00:58:56,620 --> 00:59:00,820
IP phones this extends control of the

00:58:58,690 --> 00:59:02,560
user all the way to the desktop the

00:59:00,820 --> 00:59:04,270
launch of these new products represents

00:59:02,560 --> 00:59:06,370
the next phase indigenous history of

00:59:04,270 --> 00:59:08,800
innovation these are the first and only

00:59:06,370 --> 00:59:10,480
IP phones designed to fully leverage the

00:59:08,800 --> 00:59:12,040
power of esters when we first discussed

00:59:10,480 --> 00:59:14,080
our expectations for building a family

00:59:12,040 --> 00:59:16,240
of phones for use with asterisk our

00:59:14,080 --> 00:59:17,980
requirements were pretty simple we asked

00:59:16,240 --> 00:59:19,600
the team to build the phones such that

00:59:17,980 --> 00:59:21,820
they were easy to install integrate

00:59:19,600 --> 00:59:23,650
provision and use I think you'll soon

00:59:21,820 --> 00:59:26,200
agree our engineers have delivered on

00:59:23,650 --> 00:59:27,850
that goal user feedback is validating

00:59:26,200 --> 00:59:29,950
that when it comes to operation with

00:59:27,850 --> 00:59:32,380
Astra space systems including our own

00:59:29,950 --> 00:59:34,960
Switchvox based product these are the

00:59:32,380 --> 00:59:36,280
easiest to use best integrated most

00:59:34,960 --> 00:59:38,860
interoperable products on the market

00:59:36,280 --> 00:59:40,830
today the Digium family phones will

00:59:38,860 --> 00:59:42,910
initially include three IP des hommes

00:59:40,830 --> 00:59:44,860
uniquely designed to complement any

00:59:42,910 --> 00:59:46,720
asterisk or switch box based solution

00:59:44,860 --> 00:59:49,390
these phones are different for a number

00:59:46,720 --> 00:59:52,030
of reasons first there is clue sively

00:59:49,390 --> 00:59:53,470
designed for use with esters secondly

00:59:52,030 --> 00:59:55,080
we've made it really easy to

00:59:53,470 --> 00:59:57,460
autodiscover and provision the phones

00:59:55,080 --> 00:59:59,170
next we've made it easy for the phones

00:59:57,460 --> 01:00:01,100
to access information inside of

00:59:59,170 --> 01:00:03,100
asterisks allowing tight coupling

01:00:01,100 --> 01:00:05,090
between an application and the phone

01:00:03,100 --> 01:00:07,250
additionally we've created an

01:00:05,090 --> 01:00:09,350
applications engineer that allows users

01:00:07,250 --> 01:00:12,590
and developers to create and run their

01:00:09,350 --> 01:00:14,360
own apps on the phone and finally we've

01:00:12,590 --> 01:00:16,310
done all of this at a very compelling

01:00:14,360 --> 01:00:17,990
price point at Digium we're always

01:00:16,310 --> 01:00:20,090
thinking of ways to give our customers

01:00:17,990 --> 01:00:22,610
the best value in business phone systems

01:00:20,090 --> 01:00:24,230
and also give them the power to create

01:00:22,610 --> 01:00:26,360
their own solutions or any

01:00:24,230 --> 01:00:28,100
communications challenge well continue

01:00:26,360 --> 01:00:29,510
to push the boundaries not only to make

01:00:28,100 --> 01:00:31,580
Astra's cooler faster and more

01:00:29,510 --> 01:00:33,200
technologically feature-rich but to make

01:00:31,580 --> 01:00:35,660
asterisk and what communications even

01:00:33,200 --> 01:00:39,370
easier and together we'll change the way

01:00:35,660 --> 01:00:39,370
the world communicates again

01:00:50,089 --> 01:00:57,239
prospects I every way this is the way to

01:00:53,549 --> 01:00:59,190
better utilize all your resources and it

01:00:57,239 --> 01:01:02,249
makes managing all your resources pretty

01:00:59,190 --> 01:01:07,109
easy all of the innovation is happening

01:01:02,249 --> 01:01:10,440
in open source the collaborative nature

01:01:07,109 --> 01:01:13,140
and of the you know of the community and

01:01:10,440 --> 01:01:15,029
the speed at which these are these you

01:01:13,140 --> 01:01:17,130
know these these deficiencies these bugs

01:01:15,029 --> 01:01:19,710
are getting discovered and then fixed is

01:01:17,130 --> 01:01:21,180
it like that really shows the power of

01:01:19,710 --> 01:01:23,910
the you know of the open source

01:01:21,180 --> 01:01:26,089
community it is global and it's

01:01:23,910 --> 01:01:29,609
definitely because of the users

01:01:26,089 --> 01:01:35,609
community people are extremely friendly

01:01:29,609 --> 01:01:37,319
and always ready to help if you go on

01:01:35,609 --> 01:01:39,539
tire see any day you'll see these guys

01:01:37,319 --> 01:01:42,150
helping each other out and they're all

01:01:39,539 --> 01:01:43,859
doing it like in a selfless manner the

01:01:42,150 --> 01:01:46,849
product is transparent for everyone

01:01:43,859 --> 01:01:49,799
everyone can look at the code base

01:01:46,849 --> 01:01:51,509
everyone can see how close that is is

01:01:49,799 --> 01:01:57,390
being built nothing nothing is

01:01:51,509 --> 01:02:00,029
proprietary everything is open in many

01:01:57,390 --> 01:02:04,229
ways it's absolutely vital to the the

01:02:00,029 --> 01:02:08,009
unborn health cloudstack the most

01:02:04,229 --> 01:02:11,880
exciting event in recent memory for me

01:02:08,009 --> 01:02:14,549
was our first developer boot camp

01:02:11,880 --> 01:02:17,190
and our call gave people I gave me two

01:02:14,549 --> 01:02:21,150
weeks notice to come attend I was

01:02:17,190 --> 01:02:25,349
expecting 25 or 30 people so we ended up

01:02:21,150 --> 01:02:28,170
with 87 people and had to go get more

01:02:25,349 --> 01:02:30,690
chairs in the room twice everything

01:02:28,170 --> 01:02:34,049
within cloud computing is commodity and

01:02:30,690 --> 01:02:37,019
is open source and so I don't think that

01:02:34,049 --> 01:02:38,789
you will you'll see anywhere where open

01:02:37,019 --> 01:02:41,819
source is not pervasive in cloud

01:02:38,789 --> 01:02:44,369
computing and so i think it's i think

01:02:41,819 --> 01:02:45,869
it's an assumption i think when you talk

01:02:44,369 --> 01:02:47,009
about cloud computing you're really

01:02:45,869 --> 01:02:52,559
talking about open source cloud

01:02:47,009 --> 01:02:55,589
computing cloud sac is a robust solution

01:02:52,559 --> 01:02:57,720
for large deployments you'll have dozens

01:02:55,589 --> 01:03:01,980
of data centers and thousands of servers

01:02:57,720 --> 01:03:04,650
in each data centers these hardware is

01:03:01,980 --> 01:03:08,160
going to fail and CloudStack is designed

01:03:04,650 --> 01:03:10,859
to handle number one that mass scale

01:03:08,160 --> 01:03:14,039
number two it's designed to handle the

01:03:10,859 --> 01:03:16,890
failure that inevitably happens in large

01:03:14,039 --> 01:03:21,089
deployments started working on cobb deck

01:03:16,890 --> 01:03:24,390
over four years ago and it was the

01:03:21,089 --> 01:03:26,130
original set of people working on it had

01:03:24,390 --> 01:03:30,240
a background of delivering software

01:03:26,130 --> 01:03:34,470
telcos and service providers lots of QA

01:03:30,240 --> 01:03:38,099
lots of users actually using it high

01:03:34,470 --> 01:03:41,190
availability is the key feature multiple

01:03:38,099 --> 01:03:43,349
hypervisors support different network

01:03:41,190 --> 01:03:46,349
models we can pick up whatever suits you

01:03:43,349 --> 01:03:48,119
better while step management server can

01:03:46,349 --> 01:03:51,809
be deployed in different physical

01:03:48,119 --> 01:03:53,490
machines it definitely has a huge

01:03:51,809 --> 01:03:58,500
footprint it's being deployed everywhere

01:03:53,490 --> 01:04:01,049
there's a major movie studio that they

01:03:58,500 --> 01:04:04,470
were using cloudstack they were using it

01:04:01,049 --> 01:04:06,660
to transmit video and I thought that was

01:04:04,470 --> 01:04:07,980
terribly fascinating what I found more

01:04:06,660 --> 01:04:11,000
fascinating is what they did during

01:04:07,980 --> 01:04:13,760
lunch where they would spin up

01:04:11,000 --> 01:04:14,930
you know 50 or 60 game servers then as

01:04:13,760 --> 01:04:16,910
soon as lunch was over they would

01:04:14,930 --> 01:04:20,960
destroy all the instances and go back to

01:04:16,910 --> 01:04:22,370
doing real work CloudStack is vast it

01:04:20,960 --> 01:04:23,900
touches so many different aspects and

01:04:22,370 --> 01:04:26,120
there's no one person that's kind of

01:04:23,900 --> 01:04:29,600
like a master of all those realms I

01:04:26,120 --> 01:04:32,270
think cloudstack as a project is going

01:04:29,600 --> 01:04:36,070
to be one of the leaders simply because

01:04:32,270 --> 01:04:42,500
it's some of the most feature fallen and

01:04:36,070 --> 01:04:45,970
and robust platforms out they were out

01:04:42,500 --> 01:04:45,970

YouTube URL: https://www.youtube.com/watch?v=b8UrFHtBMws


