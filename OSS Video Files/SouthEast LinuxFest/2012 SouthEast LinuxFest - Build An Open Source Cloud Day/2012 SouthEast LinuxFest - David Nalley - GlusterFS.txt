Title: 2012 SouthEast LinuxFest - David Nalley - GlusterFS
Publication date: 2014-01-09
Playlist: 2012 SouthEast LinuxFest - Build An Open Source Cloud Day
Description: 
	2012 SouthEast LinuxFest
David Nalley
Build An Open Source Cloud Day
GlusterFS
Captions: 
	00:00:00,000 --> 00:00:05,279
the following presentation was recorded

00:00:02,490 --> 00:00:08,040
the 2012 southeast linux fest in

00:00:05,279 --> 00:00:10,410
charlotte north carolina it is licensed

00:00:08,040 --> 00:00:12,090
under a creative commons license for

00:00:10,410 --> 00:00:16,859
more information about the southeast

00:00:12,090 --> 00:00:19,230
linux fest visit www.lend expense org

00:00:16,859 --> 00:00:21,320
the southeast linux fest would like to

00:00:19,230 --> 00:00:23,279
thank the following diamond sponsors in

00:00:21,320 --> 00:00:26,519
2012 for helping make these videos

00:00:23,279 --> 00:00:28,830
possible we'll start with a little

00:00:26,519 --> 00:00:30,689
history I don't know I've also got some

00:00:28,830 --> 00:00:34,290
deep dive slides that I'm happy to run

00:00:30,689 --> 00:00:36,510
into as well historically I have been a

00:00:34,290 --> 00:00:40,379
cluster user although I am a

00:00:36,510 --> 00:00:41,879
comparatively lightweight user because

00:00:40,379 --> 00:00:44,910
now I no longer manage hundreds of

00:00:41,879 --> 00:00:48,710
machines I manage tens of machines at

00:00:44,910 --> 00:00:50,640
best and it's not my full-time job so

00:00:48,710 --> 00:00:56,460
keep that in mind when you hear

00:00:50,640 --> 00:00:58,500
something from me so for those of you

00:00:56,460 --> 00:01:01,649
coming in my name is John Mark Walker I

00:00:58,500 --> 00:01:05,460
work for red hat and please find me

00:01:01,649 --> 00:01:09,030
offensive and complain so the big idea

00:01:05,460 --> 00:01:14,430
behind cluster is effectively having

00:01:09,030 --> 00:01:17,220
commodity scale-out storage so if you've

00:01:14,430 --> 00:01:22,320
been around enterprise storage you know

00:01:17,220 --> 00:01:26,009
that effectively its sands are you'll

00:01:22,320 --> 00:01:27,750
have a filer an azz filer that will

00:01:26,009 --> 00:01:32,759
provide you massive amounts of storage

00:01:27,750 --> 00:01:36,000
and it's also very painful to increase

00:01:32,759 --> 00:01:37,860
that storage as you grow typically it's

00:01:36,000 --> 00:01:40,439
going to get more and more expensive to

00:01:37,860 --> 00:01:43,110
maintain support you're going to have to

00:01:40,439 --> 00:01:45,810
do for cliff up upgrades when you reach

00:01:43,110 --> 00:01:47,700
certain capacity levels both from an IO

00:01:45,810 --> 00:01:52,710
perspective as well as a complete

00:01:47,700 --> 00:01:55,890
capacity perspective and so this was

00:01:52,710 --> 00:02:00,240
originally born out of a project that

00:01:55,890 --> 00:02:03,540
someone had 44 what we would probably

00:02:00,240 --> 00:02:07,469
call grid computing a research

00:02:03,540 --> 00:02:11,280
institution in South America needed to

00:02:07,469 --> 00:02:13,180
be able to to do some things and the

00:02:11,280 --> 00:02:14,890
folks behind Gluster

00:02:13,180 --> 00:02:17,069
not want to do storage because storage

00:02:14,890 --> 00:02:19,629
is painful and everybody hates doing it

00:02:17,069 --> 00:02:22,959
and people have these expectations

00:02:19,629 --> 00:02:26,620
around not losing files and not losing

00:02:22,959 --> 00:02:31,659
data that are far more exacting than

00:02:26,620 --> 00:02:33,010
typical software writing will allow so

00:02:31,659 --> 00:02:35,859
they didn't want to do that they were

00:02:33,010 --> 00:02:38,019
looking at some of the existing

00:02:35,859 --> 00:02:42,400
implementations things like luster and

00:02:38,019 --> 00:02:46,769
gfs and all of them had a number of

00:02:42,400 --> 00:02:49,359
limitations and also just didn't scale

00:02:46,769 --> 00:02:51,310
there's a lot of marketing fluff in here

00:02:49,359 --> 00:02:54,970
and I will be happy to tell you when

00:02:51,310 --> 00:02:57,849
it's marketing fluff and we'll skip

00:02:54,970 --> 00:03:00,669
those slides really quickly so the idea

00:02:57,849 --> 00:03:03,189
behind scale-out are distributed storage

00:03:00,669 --> 00:03:04,989
is that effectively you can add

00:03:03,189 --> 00:03:07,780
additional bricks to your storage

00:03:04,989 --> 00:03:10,870
building at any time so you may start

00:03:07,780 --> 00:03:13,030
off with 15 servers that are providing

00:03:10,870 --> 00:03:15,609
storage and you can add a machine to it

00:03:13,030 --> 00:03:18,190
and you're adding incrementally and that

00:03:15,609 --> 00:03:21,459
is giving you both additional IO as well

00:03:18,190 --> 00:03:23,379
as additional capacity and this allows

00:03:21,459 --> 00:03:25,569
you to scale up in a sustainable way

00:03:23,379 --> 00:03:27,030
without having to do for cliff upgrades

00:03:25,569 --> 00:03:32,229
like you would have to do with

00:03:27,030 --> 00:03:35,079
enterprise storage options this has been

00:03:32,229 --> 00:03:38,439
driven and probably accelerated a lot

00:03:35,079 --> 00:03:41,009
because of this idea of cloud computing

00:03:38,439 --> 00:03:45,340
and the entire commoditization of

00:03:41,009 --> 00:03:47,109
everything that's happened so when you

00:03:45,340 --> 00:03:51,370
know it costs you eight cents an hour to

00:03:47,109 --> 00:03:54,699
run a virtual machine paying twenty

00:03:51,370 --> 00:03:58,060
thousand dollars just for a memory

00:03:54,699 --> 00:04:03,819
upgrade on your honest and Fowler is

00:03:58,060 --> 00:04:06,099
sounds crazy in in that same context so

00:04:03,819 --> 00:04:09,400
this is effectively providing a

00:04:06,099 --> 00:04:13,959
commodity you can use x86 boxes or arm

00:04:09,400 --> 00:04:17,590
boxes for that matter to provide to

00:04:13,959 --> 00:04:21,519
provide their disks as available to

00:04:17,590 --> 00:04:22,990
for this distributed storage so this is

00:04:21,519 --> 00:04:26,680
marketing fluff they want to show you

00:04:22,990 --> 00:04:28,210
how how they used to be this is more

00:04:26,680 --> 00:04:30,190
marketing fluff that shows you a little

00:04:28,210 --> 00:04:32,530
bit about some of the places that have

00:04:30,190 --> 00:04:36,600
deployed it it's marginally interesting

00:04:32,530 --> 00:04:40,720
but you know it doesn't really add value

00:04:36,600 --> 00:04:43,360
this is this is some of the larger users

00:04:40,720 --> 00:04:47,860
of clustered that are using this in

00:04:43,360 --> 00:04:50,260
production right now the big group is

00:04:47,860 --> 00:04:53,260
Pandora so if you use pandora to listen

00:04:50,260 --> 00:04:55,210
to music all of that music is stored on

00:04:53,260 --> 00:04:57,690
Gluster and they are one of the larger

00:04:55,210 --> 00:05:00,130
implementations of bluster out there

00:04:57,690 --> 00:05:03,060
we'll talk about them a little more in

00:05:00,130 --> 00:05:07,560
depth simply because they provide a

00:05:03,060 --> 00:05:11,229
great example of the ideal workload for

00:05:07,560 --> 00:05:15,220
for gloucester interestingly enough

00:05:11,229 --> 00:05:18,039
patio is a company that was just

00:05:15,220 --> 00:05:19,720
acquired by the company I work for and I

00:05:18,039 --> 00:05:25,810
didn't know that they were a glossary

00:05:19,720 --> 00:05:27,729
user till recently so the the real

00:05:25,810 --> 00:05:31,300
problem is is that you know we have

00:05:27,729 --> 00:05:33,370
typically bought we typically bought

00:05:31,300 --> 00:05:37,060
storage and we've expected it to last us

00:05:33,370 --> 00:05:38,770
three or so years maybe five years if

00:05:37,060 --> 00:05:44,380
we're if we're buying really high-end

00:05:38,770 --> 00:05:48,419
stuff that's expandable that's not

00:05:44,380 --> 00:05:51,070
keeping up with the rate of data growth

00:05:48,419 --> 00:05:53,950
the annual rate of data growth is

00:05:51,070 --> 00:05:58,380
seventy-four percent according to I

00:05:53,950 --> 00:06:02,440
think this is Gartner that said this and

00:05:58,380 --> 00:06:04,510
that is that is very hard to predict and

00:06:02,440 --> 00:06:06,910
very hard to keep up with when you're

00:06:04,510 --> 00:06:12,280
making huge capital investments five

00:06:06,910 --> 00:06:15,669
years in advance so when you consider

00:06:12,280 --> 00:06:20,110
that we are expanding the rate the rate

00:06:15,669 --> 00:06:23,830
of growth as well it it's very hard to

00:06:20,110 --> 00:06:26,260
do things when when most of those are

00:06:23,830 --> 00:06:29,260
very painful upgrades or plateau

00:06:26,260 --> 00:06:30,789
upgrades you can you can continue

00:06:29,260 --> 00:06:31,390
expanding with this shelves until you

00:06:30,789 --> 00:06:36,220
reach a certain

00:06:31,390 --> 00:06:40,330
point and by the way this is a complete

00:06:36,220 --> 00:06:42,310
marketing fluff side so what do you want

00:06:40,330 --> 00:06:45,990
to use gluster for because gluster is

00:06:42,310 --> 00:06:49,330
not a solution to every storage problem

00:06:45,990 --> 00:06:52,620
gluster will allow you to mainly store

00:06:49,330 --> 00:06:55,780
unstructured data so think about

00:06:52,620 --> 00:07:00,370
documents images in the case of Pandora

00:06:55,780 --> 00:07:05,160
they use it for audio files that is an

00:07:00,370 --> 00:07:09,520
ideal use case it's it's data that's

00:07:05,160 --> 00:07:11,730
reasonably large but not but not

00:07:09,520 --> 00:07:15,610
terribly structured it's not like a

00:07:11,730 --> 00:07:17,860
database file I hear that people are

00:07:15,610 --> 00:07:20,220
using gluster for database really a bad

00:07:17,860 --> 00:07:24,460
use case for that unless you're doing

00:07:20,220 --> 00:07:27,580
using something like HDFS and and doing

00:07:24,460 --> 00:07:31,420
Hadoop so Gloucester just recently had

00:07:27,580 --> 00:07:34,000
its 33 release in which they brought out

00:07:31,420 --> 00:07:38,650
a number of things around big data so

00:07:34,000 --> 00:07:41,740
they brought out HDFS compatibility so

00:07:38,650 --> 00:07:45,850
if you are into big data and your you

00:07:41,740 --> 00:07:49,810
want to use an a dupe like file system

00:07:45,850 --> 00:07:52,270
you can use cluster to do that you can

00:07:49,810 --> 00:07:55,080
also do object storage so how many of

00:07:52,270 --> 00:07:58,690
you are familiar with s3 from amazon

00:07:55,080 --> 00:08:01,300
awesome lots of people so amazon allows

00:07:58,690 --> 00:08:03,750
you to essentially instantiate the

00:08:01,300 --> 00:08:08,200
existence of an object which is a file

00:08:03,750 --> 00:08:09,550
most of the time and and you can't do

00:08:08,200 --> 00:08:12,160
things like modify the file without

00:08:09,550 --> 00:08:13,600
writing the entire file back out so if

00:08:12,160 --> 00:08:16,090
you're doing something that changes

00:08:13,600 --> 00:08:18,130
frequently it's very inefficient what it

00:08:16,090 --> 00:08:21,940
does allow you to do though is it allows

00:08:18,130 --> 00:08:25,750
very simple access to those to those

00:08:21,940 --> 00:08:30,430
objects very simple to push the objects

00:08:25,750 --> 00:08:32,650
up to write them and so gluster will

00:08:30,430 --> 00:08:36,640
provide you the ability to have that

00:08:32,650 --> 00:08:41,830
object store and to use it actually uses

00:08:36,640 --> 00:08:44,740
the OpenStack Swift API to to expose

00:08:41,830 --> 00:08:45,300
those objects and expose that object

00:08:44,740 --> 00:08:49,260
store

00:08:45,300 --> 00:08:53,140
and the nice thing about gluster is that

00:08:49,260 --> 00:08:56,980
you know while you might access your

00:08:53,140 --> 00:09:00,030
video files mounting the file system as

00:08:56,980 --> 00:09:05,020
you would a traditional file system you

00:09:00,030 --> 00:09:07,780
you would not you would also have access

00:09:05,020 --> 00:09:09,760
to the same object same files through an

00:09:07,780 --> 00:09:14,830
object store so they call that unified

00:09:09,760 --> 00:09:16,780
file access and so essentially the same

00:09:14,830 --> 00:09:19,060
program the same repository of

00:09:16,780 --> 00:09:22,180
information you can access that as an

00:09:19,060 --> 00:09:30,040
object store as file you can do it with

00:09:22,180 --> 00:09:32,860
HDFS etc I confused anyone so far this

00:09:30,040 --> 00:09:34,780
is a pretty marketing heavy slide things

00:09:32,860 --> 00:09:39,460
should be simple things really rarely

00:09:34,780 --> 00:09:41,650
are so Gloucester's unified we talked

00:09:39,460 --> 00:09:45,970
about unified a bit it's distributed so

00:09:41,650 --> 00:09:49,570
effectively it distributes the bits that

00:09:45,970 --> 00:09:53,170
are your files across the number of

00:09:49,570 --> 00:09:54,790
nodes and it will continue splitting

00:09:53,170 --> 00:10:00,700
those up and figuring out where those

00:09:54,790 --> 00:10:01,750
are to reassemble them and and of course

00:10:00,700 --> 00:10:08,860
that means that you can also specify

00:10:01,750 --> 00:10:10,660
some redundancy and there as well so so

00:10:08,860 --> 00:10:13,830
there's some controversy around how

00:10:10,660 --> 00:10:17,680
Gluster does things the cluster folks

00:10:13,830 --> 00:10:20,370
were originally and if you look at the

00:10:17,680 --> 00:10:25,120
one of the names on the title slide a B

00:10:20,370 --> 00:10:28,180
was agony heard developer and canoe herd

00:10:25,120 --> 00:10:30,550
has a microkernel architecture we're

00:10:28,180 --> 00:10:32,260
very very little is actually in the

00:10:30,550 --> 00:10:36,430
kernel and everything else is in user

00:10:32,260 --> 00:10:41,560
space there is a significant chunk of

00:10:36,430 --> 00:10:48,820
folks who who think that user space file

00:10:41,560 --> 00:10:52,300
systems cannot be performant and even

00:10:48,820 --> 00:10:56,080
lennis says that user space file systems

00:10:52,300 --> 00:10:58,710
are real are realistic for anything but

00:10:56,080 --> 00:11:03,060
toys those people are just misguided

00:10:58,710 --> 00:11:06,120
so and that's that's honestly true right

00:11:03,060 --> 00:11:08,340
so the vast majority of of user space

00:11:06,120 --> 00:11:10,710
file systems that you would mount with

00:11:08,340 --> 00:11:15,570
things like fuse you cannot get decent

00:11:10,710 --> 00:11:17,520
performance on some of the the reasons

00:11:15,570 --> 00:11:19,860
that gluster still did this and still

00:11:17,520 --> 00:11:21,900
are able to maintain high performance is

00:11:19,860 --> 00:11:25,220
that they had a deep understanding of

00:11:21,900 --> 00:11:27,750
how to do things from this microkernel

00:11:25,220 --> 00:11:30,840
architecture perspective and that

00:11:27,750 --> 00:11:32,790
allowed them to that allowed them to

00:11:30,840 --> 00:11:38,280
focus on performance and tune it very

00:11:32,790 --> 00:11:41,760
heavily and you know gluster will expose

00:11:38,280 --> 00:11:45,060
NFS so it has a effectively its own NFS

00:11:41,760 --> 00:11:48,780
implementation that you can mount that

00:11:45,060 --> 00:11:52,020
is still slower than their fuse their

00:11:48,780 --> 00:11:57,590
fuse client so if you mount it be a fuse

00:11:52,020 --> 00:12:01,080
you actually get better performance so

00:11:57,590 --> 00:12:04,500
you can see specifying some of these

00:12:01,080 --> 00:12:08,850
some of the gloucester volumes and

00:12:04,500 --> 00:12:13,470
turning on different different features

00:12:08,850 --> 00:12:17,970
including performance options etc so

00:12:13,470 --> 00:12:19,800
this is a quick show of how at least how

00:12:17,970 --> 00:12:24,330
originally this was done it's pretty

00:12:19,800 --> 00:12:26,280
rudimentary so from an evolution you

00:12:24,330 --> 00:12:29,820
used to have to do handcrafted

00:12:26,280 --> 00:12:32,760
definition files for your volumes it was

00:12:29,820 --> 00:12:36,090
really simple the store the performance

00:12:32,760 --> 00:12:42,150
was faster than tape and people thought

00:12:36,090 --> 00:12:44,220
that was good and I suppose it is so you

00:12:42,150 --> 00:12:46,110
know they built this originally for a

00:12:44,220 --> 00:12:50,000
giant compute cluster in South America

00:12:46,110 --> 00:12:50,000
who need process large amounts of data

00:12:50,090 --> 00:12:56,070
what we would typically consider HDFS

00:12:52,950 --> 00:13:01,410
for these days except Hadoop didn't

00:12:56,070 --> 00:13:04,290
exist at the time so they ended up

00:13:01,410 --> 00:13:05,880
seeing that even that use case was was

00:13:04,290 --> 00:13:09,330
somewhat limited and people were using

00:13:05,880 --> 00:13:11,600
Gluster for unstructured data and I

00:13:09,330 --> 00:13:17,240
think that still remains the

00:13:11,600 --> 00:13:23,380
the still remains the the prime use case

00:13:17,240 --> 00:13:29,060
so gluster 30 and 31 came about in 2010

00:13:23,380 --> 00:13:31,759
they they essentially added the ability

00:13:29,060 --> 00:13:38,000
to make it elastic so you can remove and

00:13:31,759 --> 00:13:40,910
add volumes with gluster they provided

00:13:38,000 --> 00:13:42,920
their own NFS implementation this is

00:13:40,910 --> 00:13:47,300
really when people started taking notice

00:13:42,920 --> 00:13:49,310
of bluster and you can see some of these

00:13:47,300 --> 00:13:51,470
things so if you're adding a brick

00:13:49,310 --> 00:13:55,940
you're essentially adding a host that's

00:13:51,470 --> 00:13:58,639
going to be on a on a volume you can

00:13:55,940 --> 00:14:00,440
have it rebalance an entire volume so

00:13:58,639 --> 00:14:02,329
your volume would be spread across in

00:14:00,440 --> 00:14:04,699
number of nodes you could have it

00:14:02,329 --> 00:14:08,120
rebalance and of course you can do that

00:14:04,699 --> 00:14:10,069
oh you can script all of that so that

00:14:08,120 --> 00:14:15,050
it's not it's not something you have to

00:14:10,069 --> 00:14:16,730
do manually just going back to this real

00:14:15,050 --> 00:14:19,550
quickly you can create volumes and you

00:14:16,730 --> 00:14:22,040
can see how many stripes that volume is

00:14:19,550 --> 00:14:24,829
going to be on how many replicas there

00:14:22,040 --> 00:14:28,370
needs to be of each piece of data so

00:14:24,829 --> 00:14:33,490
that gives you some some options there

00:14:28,370 --> 00:14:37,339
and you can of course you can of course

00:14:33,490 --> 00:14:39,380
set stripes and replicas up to whatever

00:14:37,339 --> 00:14:46,880
you feel comfortable trusting your data

00:14:39,380 --> 00:14:50,990
to so in three two we Gluster introduced

00:14:46,880 --> 00:14:53,600
geo replication it's a sink that's

00:14:50,990 --> 00:14:57,800
getting dramatically better with 33 but

00:14:53,600 --> 00:14:59,509
32 g or replication did come in but that

00:14:57,800 --> 00:15:02,420
meant that if you changed five terabytes

00:14:59,509 --> 00:15:05,029
in one day then you have a huge five

00:15:02,420 --> 00:15:08,149
terabyte change as opposed to that was

00:15:05,029 --> 00:15:10,100
10 megabytes at a time so you are

00:15:08,149 --> 00:15:12,829
effectively saying point in time go

00:15:10,100 --> 00:15:15,290
ahead and replicate this elsewhere and

00:15:12,829 --> 00:15:18,680
you could do that and they did that

00:15:15,290 --> 00:15:23,839
simply because replicating in real time

00:15:18,680 --> 00:15:24,980
across a LAN just doesn't does not does

00:15:23,839 --> 00:15:28,550
not work

00:15:24,980 --> 00:15:33,190
readily that's getting somewhat better

00:15:28,550 --> 00:15:33,190
it's getting a lot more granular in 33

00:15:34,180 --> 00:15:41,930
so here's what we used to see right we

00:15:39,860 --> 00:15:45,380
used to see fibre channel fibre channel

00:15:41,930 --> 00:15:47,449
over ethernet started coming in recently

00:15:45,380 --> 00:15:52,550
I scuzzy predates fibre channel over

00:15:47,449 --> 00:15:54,290
ethernet a bit but s3 came along

00:15:52,550 --> 00:15:59,050
completely changed that object storage

00:15:54,290 --> 00:16:02,120
push everything up in down via HTTP and

00:15:59,050 --> 00:16:05,690
it's a it's a lot simpler you don't have

00:16:02,120 --> 00:16:08,630
to have complex you don't have to have

00:16:05,690 --> 00:16:14,000
complex modules you can have anything

00:16:08,630 --> 00:16:18,380
access your objects we also used to have

00:16:14,000 --> 00:16:22,600
appliance based stuff so I can't tell

00:16:18,380 --> 00:16:25,790
you how many proprietary sans storage

00:16:22,600 --> 00:16:30,500
pieces that I've actually had that we're

00:16:25,790 --> 00:16:32,899
based on things like vxworks or are some

00:16:30,500 --> 00:16:34,339
proprietary unix and of course

00:16:32,899 --> 00:16:36,050
completely locked down so you can do

00:16:34,339 --> 00:16:39,949
anything that they didn't want you to do

00:16:36,050 --> 00:16:42,410
and that was all tied into licensing and

00:16:39,949 --> 00:16:45,560
so application and and glutes are

00:16:42,410 --> 00:16:47,240
certainly not the only not the only

00:16:45,560 --> 00:16:49,790
application in this space there's a

00:16:47,240 --> 00:16:55,459
number of others each having their own

00:16:49,790 --> 00:16:58,220
little niche but you know things are

00:16:55,459 --> 00:16:59,510
things are dramatically especially if

00:16:58,220 --> 00:17:01,130
you look at some of the people who are

00:16:59,510 --> 00:17:05,030
doing distributed computing

00:17:01,130 --> 00:17:07,429
architectures already there are very few

00:17:05,030 --> 00:17:11,390
of them who are calling up EMC our net

00:17:07,429 --> 00:17:15,079
app and saying hey I've got this 10,000

00:17:11,390 --> 00:17:18,860
node distributed architecture I'm going

00:17:15,079 --> 00:17:20,750
to need some storage for that first of

00:17:18,860 --> 00:17:24,819
all the 10,000 client licenses would

00:17:20,750 --> 00:17:24,819
price them out of business I think but

00:17:24,850 --> 00:17:30,140
so things are certainly moving I think

00:17:27,890 --> 00:17:32,960
in a distributed storage function this

00:17:30,140 --> 00:17:35,659
is a marketing fluff slide it doesn't

00:17:32,960 --> 00:17:37,309
tell you anything important and so when

00:17:35,659 --> 00:17:38,390
we talk about this scale out or

00:17:37,309 --> 00:17:41,510
distributed

00:17:38,390 --> 00:17:43,280
infrastructure some of the early people

00:17:41,510 --> 00:17:44,990
who are doing this right word Google and

00:17:43,280 --> 00:17:49,730
Facebook Google certainly before

00:17:44,990 --> 00:17:53,180
Facebook but google has this idea and

00:17:49,730 --> 00:17:55,100
not just with storage but with you know

00:17:53,180 --> 00:17:57,310
essentially everything they do that they

00:17:55,100 --> 00:18:01,160
don't care about the individual nodes

00:17:57,310 --> 00:18:03,380
they are building in redundancy and the

00:18:01,160 --> 00:18:05,780
same should be true of storage and so

00:18:03,380 --> 00:18:08,120
this is this distributed storage is just

00:18:05,780 --> 00:18:10,330
a natural evolution of the rest of this

00:18:08,120 --> 00:18:17,470
scale out or distributed architecture

00:18:10,330 --> 00:18:19,480
and that is that particular slide let me

00:18:17,470 --> 00:18:24,790
anybody want to ask questions while I

00:18:19,480 --> 00:18:24,790
switch slide decks go ahead

00:18:33,379 --> 00:18:42,499
yes yeah no no there's typically not a

00:18:37,499 --> 00:18:46,499
back in sand it there's typically only

00:18:42,499 --> 00:18:49,079
local disc on each nude and and it is

00:18:46,499 --> 00:18:52,739
managing the local storage and so it's

00:18:49,079 --> 00:18:57,329
it's very commodity you're using SAS or

00:18:52,739 --> 00:18:59,279
SATA disks typically and and the nodes

00:18:57,329 --> 00:19:11,579
are replicating the data among all of

00:18:59,279 --> 00:19:13,049
those there are there are production the

00:19:11,579 --> 00:19:16,409
question was what is the typical size

00:19:13,049 --> 00:19:20,249
and there are so my my instances of

00:19:16,409 --> 00:19:21,869
gluster or in the couple terabyte range

00:19:20,249 --> 00:19:28,759
and I would consider myself very very

00:19:21,869 --> 00:19:30,989
small there are a number of gluster

00:19:28,759 --> 00:19:37,739
implementation that are in the tens of

00:19:30,989 --> 00:19:42,359
petabytes so oh no not on each typically

00:19:37,739 --> 00:19:44,279
each node is going to be the low end is

00:19:42,359 --> 00:19:46,349
probably hundreds of gigabytes in the

00:19:44,279 --> 00:19:50,869
high end is is probably tens of

00:19:46,349 --> 00:19:50,869
terabytes yes sir

00:20:06,530 --> 00:20:13,380
um so so you're talking like the old

00:20:10,230 --> 00:20:17,310
eight an open AFS model where everybody

00:20:13,380 --> 00:20:19,140
toss their yeah you can assuming that

00:20:17,310 --> 00:20:20,820
you haven't given your users too much

00:20:19,140 --> 00:20:23,370
power you can probably do that with no

00:20:20,820 --> 00:20:26,400
problem the things that would worry me

00:20:23,370 --> 00:20:29,250
our users are prone to turn their

00:20:26,400 --> 00:20:32,040
machines off and you wouldn't want to

00:20:29,250 --> 00:20:39,270
have enough replication to do that well

00:20:32,040 --> 00:20:40,800
and clusters also assuming a for for a

00:20:39,270 --> 00:20:43,350
given volume that it's all going to be

00:20:40,800 --> 00:20:47,160
on the same network so that's an

00:20:43,350 --> 00:20:48,780
architectural issue potentially because

00:20:47,160 --> 00:20:51,020
you might not have everybody on the same

00:20:48,780 --> 00:20:51,020
VLAN

00:20:58,280 --> 00:21:08,100
so open AFS architectural e is a little

00:21:02,310 --> 00:21:10,910
different first of all it's the the

00:21:08,100 --> 00:21:13,350
clients are kernel-based right and

00:21:10,910 --> 00:21:15,000
opening FS is not in the kernel and so

00:21:13,350 --> 00:21:18,620
it's it's something that you typically

00:21:15,000 --> 00:21:22,040
compile yourself which is painful

00:21:18,620 --> 00:21:24,750
gluster makes out a little easier well

00:21:22,040 --> 00:21:26,280
you know it's not painful if you're

00:21:24,750 --> 00:21:27,540
doing it for a thousand machines it's

00:21:26,280 --> 00:21:29,580
very painful if you've got to have

00:21:27,540 --> 00:21:32,250
support on those machines because then

00:21:29,580 --> 00:21:34,670
you just modified your kernel and you

00:21:32,250 --> 00:21:37,950
call up your friendly red hat or Susa

00:21:34,670 --> 00:21:41,130
support rep and he goes oh you have a

00:21:37,950 --> 00:21:42,480
tainted colonel are oh you have a kernel

00:21:41,130 --> 00:21:45,390
that is not the colonel we shipped you

00:21:42,480 --> 00:21:49,920
please repeat this problem when you're

00:21:45,390 --> 00:21:52,050
using the right colonel so it's not

00:21:49,920 --> 00:21:53,820
technically difficult it's it's

00:21:52,050 --> 00:21:58,020
challenging for places that care about

00:21:53,820 --> 00:22:01,950
having support and it's also burdensome

00:21:58,020 --> 00:22:04,550
because every every kernel update means

00:22:01,950 --> 00:22:07,290
you have to go and recompile your curl

00:22:04,550 --> 00:22:10,740
it also means that you potentially need

00:22:07,290 --> 00:22:13,590
to keep those colonel those kernel

00:22:10,740 --> 00:22:16,290
updates and sync and especially across a

00:22:13,590 --> 00:22:19,110
volume and it's a lot easier to have a

00:22:16,290 --> 00:22:20,850
client package that is completely

00:22:19,110 --> 00:22:24,840
standalone and effectively a leaf

00:22:20,850 --> 00:22:27,480
package it's it's a lot easier to keep

00:22:24,840 --> 00:22:31,080
that all on the same version across your

00:22:27,480 --> 00:22:35,460
enterprise then you know I've got rail

00:22:31,080 --> 00:22:37,650
62 here and rails 63 is coming out and I

00:22:35,460 --> 00:22:42,810
want to upgrade to rail 63 on these

00:22:37,650 --> 00:22:45,030
machines oh no different kernel got to

00:22:42,810 --> 00:22:46,380
make sure that it all works so I think

00:22:45,030 --> 00:22:51,540
it's I think it's actually a little

00:22:46,380 --> 00:22:54,270
easier so there's also some gluster

00:22:51,540 --> 00:22:55,920
doesn't have this a lot of distributed

00:22:54,270 --> 00:23:00,150
file system say yes we're going to

00:22:55,920 --> 00:23:03,120
distribute out the data but we're not

00:23:00,150 --> 00:23:05,490
going to we have essentially these nodes

00:23:03,120 --> 00:23:09,060
that track metadata like file names and

00:23:05,490 --> 00:23:10,260
attributes of the file and so those

00:23:09,060 --> 00:23:12,240
become the

00:23:10,260 --> 00:23:14,460
choke points are sometimes a single

00:23:12,240 --> 00:23:16,320
point of failure for those distributed

00:23:14,460 --> 00:23:19,800
file systems there are still some that

00:23:16,320 --> 00:23:26,820
do that not not open AFS in particular

00:23:19,800 --> 00:23:28,860
but there is a very large distributed

00:23:26,820 --> 00:23:31,830
file system project that still gets a

00:23:28,860 --> 00:23:36,000
ton of press that if the metadata server

00:23:31,830 --> 00:23:38,610
goes down it you essentially lose access

00:23:36,000 --> 00:23:44,310
and it's a it's not designed to be

00:23:38,610 --> 00:23:47,310
replicated there's also some there's

00:23:44,310 --> 00:23:48,840
also some issues around when you do have

00:23:47,310 --> 00:23:52,890
metadata servers even if they are

00:23:48,840 --> 00:23:54,300
replicable to track the number of

00:23:52,890 --> 00:23:56,580
objects is the number of objects

00:23:54,300 --> 00:23:59,120
increases if that actually goes to disk

00:23:56,580 --> 00:24:03,150
you start losing tons of performance

00:23:59,120 --> 00:24:05,190
because you then are querying where is

00:24:03,150 --> 00:24:06,810
the file and we have to go to disk to

00:24:05,190 --> 00:24:09,330
find out where the file is and oh yeah

00:24:06,810 --> 00:24:10,860
it's not here it's over there and so

00:24:09,330 --> 00:24:13,890
your latency starts increasing

00:24:10,860 --> 00:24:16,100
tremendously with Gluster the

00:24:13,890 --> 00:24:19,800
architecture is that there is no master

00:24:16,100 --> 00:24:24,120
we will talk about quorum and a bit but

00:24:19,800 --> 00:24:29,880
but essentially you can it doesn't

00:24:24,120 --> 00:24:31,740
matter if you have the bazillion files

00:24:29,880 --> 00:24:33,930
you don't have to have those all on a

00:24:31,740 --> 00:24:35,250
single metadata server so kind of

00:24:33,930 --> 00:24:36,630
removing at least one of those

00:24:35,250 --> 00:24:42,560
checkpoints or adding some additional

00:24:36,630 --> 00:24:42,560
latency yes sir

00:25:07,580 --> 00:25:12,860
I think it could I think it depends upon

00:25:11,420 --> 00:25:15,770
what workloads you're wanting to put on

00:25:12,860 --> 00:25:20,960
that there are a lot of workloads that I

00:25:15,770 --> 00:25:23,810
think cluster is poorly suited for until

00:25:20,960 --> 00:25:25,760
33 I would have said putting VM images

00:25:23,810 --> 00:25:27,940
on cluster was a bad idea but I think

00:25:25,760 --> 00:25:30,920
that they have dramatically improved

00:25:27,940 --> 00:25:33,260
some of the granularity behind that so

00:25:30,920 --> 00:25:35,300
that that performance is increasing I

00:25:33,260 --> 00:25:39,980
would not for instance put large

00:25:35,300 --> 00:25:41,390
databases on bluster in it people have

00:25:39,980 --> 00:25:43,730
done it it will not perform as you

00:25:41,390 --> 00:25:46,460
expect it to and it that is not the

00:25:43,730 --> 00:25:50,840
solution that you're looking for if on

00:25:46,460 --> 00:25:54,260
the other hand you have you know 500,000

00:25:50,840 --> 00:25:57,830
video files maybe that works for you if

00:25:54,260 --> 00:26:01,820
if you're starting to do if you need a

00:25:57,830 --> 00:26:04,250
Hadoop like file system perhaps cluster

00:26:01,820 --> 00:26:08,390
gives you some options that straight

00:26:04,250 --> 00:26:11,810
hdfs does not so I think it depends upon

00:26:08,390 --> 00:26:14,240
workload I think that most people don't

00:26:11,810 --> 00:26:16,010
realize the amount of actual local

00:26:14,240 --> 00:26:19,310
direct-attached storage they have that

00:26:16,010 --> 00:26:21,650
they could start leveraging for this at

00:26:19,310 --> 00:26:24,020
the same time I would not go out and

00:26:21,650 --> 00:26:25,100
deploy clustered just because I wanted

00:26:24,020 --> 00:26:26,720
to do the Leicester I would actually

00:26:25,100 --> 00:26:30,160
have the problem to solve that cluster

00:26:26,720 --> 00:26:33,280
would do would solve for me in advance

00:26:30,160 --> 00:26:33,280
yes sir

00:26:43,480 --> 00:26:50,500
I would not personally knew I don't know

00:26:48,940 --> 00:26:54,550
what the official gluster line on that

00:26:50,500 --> 00:26:55,600
is because I I don't know so but I that

00:26:54,550 --> 00:27:02,160
would not be a work like that I

00:26:55,600 --> 00:27:02,160
personally with it on it yes

00:27:15,380 --> 00:27:25,290
probably not to the degree that you're

00:27:17,430 --> 00:27:28,110
expecting it so yes but now any other

00:27:25,290 --> 00:27:31,470
questions or will we will keep marching

00:27:28,110 --> 00:27:34,620
on alright so we will not talk about

00:27:31,470 --> 00:27:37,740
this so there have been some replication

00:27:34,620 --> 00:27:39,870
improvements essentially and they talk

00:27:37,740 --> 00:27:41,880
about granular locking but the entire

00:27:39,870 --> 00:27:50,010
process of replication has been made

00:27:41,880 --> 00:27:54,810
more granular in 33 and so that that

00:27:50,010 --> 00:27:57,930
helps certainly with geo replication but

00:27:54,810 --> 00:28:01,050
also the the local replication has been

00:27:57,930 --> 00:28:03,030
enhanced significantly hdfs

00:28:01,050 --> 00:28:06,390
compatibility is something else that's

00:28:03,030 --> 00:28:08,910
new to 33 unified file and object

00:28:06,390 --> 00:28:10,560
storage has been around since 32 but

00:28:08,910 --> 00:28:17,220
nobody talked about it for some reason

00:28:10,560 --> 00:28:20,040
and has really matured in 33 marketing

00:28:17,220 --> 00:28:26,250
fluff you've seen that before seen that

00:28:20,040 --> 00:28:28,620
before ah so things that aren't on here

00:28:26,250 --> 00:28:30,390
before Gluster was acquired by Red Hat

00:28:28,620 --> 00:28:32,610
some of the interesting things that were

00:28:30,390 --> 00:28:37,790
done by the community were multi-tenant

00:28:32,610 --> 00:28:40,680
encrypted file system a top cluster so

00:28:37,790 --> 00:28:45,480
gluster has this concept of translators

00:28:40,680 --> 00:28:48,810
and a guy named Jeff Darcy wrote a

00:28:45,480 --> 00:28:50,280
translator and he's an amazing

00:28:48,810 --> 00:28:51,900
individual for doing this because there

00:28:50,280 --> 00:28:55,500
was essentially no documentation other

00:28:51,900 --> 00:28:57,870
than the source code and he went through

00:28:55,500 --> 00:28:59,970
and created translator that said you

00:28:57,870 --> 00:29:01,710
know what this is a massive file store

00:28:59,970 --> 00:29:03,630
and I don't trust anybody else using it

00:29:01,710 --> 00:29:05,280
I need this to be multi-tenant so

00:29:03,630 --> 00:29:06,990
everything's isolated from everyone else

00:29:05,280 --> 00:29:10,080
and it needs to have an option for

00:29:06,990 --> 00:29:11,820
encrypting it all so that even if

00:29:10,080 --> 00:29:15,020
someone else got to hold of it it would

00:29:11,820 --> 00:29:18,990
be useless so he came up with hecka FS

00:29:15,020 --> 00:29:21,540
which is layered a top cluster and it's

00:29:18,990 --> 00:29:23,610
certainly worth a look I think it's the

00:29:21,540 --> 00:29:30,180
current plan is that it's going to be

00:29:23,610 --> 00:29:32,580
merged into into gluster Jeff was a was

00:29:30,180 --> 00:29:36,090
a storage guy at Red Hat and before

00:29:32,580 --> 00:29:37,590
gluster was acquired he was he was one

00:29:36,090 --> 00:29:41,040
of the big external community

00:29:37,590 --> 00:29:44,160
contributors and and did some really

00:29:41,040 --> 00:29:47,490
awesome things with hecka FS and that's

00:29:44,160 --> 00:29:48,930
HEK FS and it's worth taking a look at

00:29:47,490 --> 00:29:52,890
if you've got a multi-tenant environment

00:29:48,930 --> 00:29:58,500
or you need heavy isolated encryption on

00:29:52,890 --> 00:30:01,410
a distributed file system today anything

00:29:58,500 --> 00:30:03,150
else there that's so one of the

00:30:01,410 --> 00:30:05,820
community led features was of course geo

00:30:03,150 --> 00:30:10,620
replication and that's really matured a

00:30:05,820 --> 00:30:12,660
lot in 2012 so in 2011 it was

00:30:10,620 --> 00:30:14,820
effectively a replicated distributed

00:30:12,660 --> 00:30:18,750
file system and it was nothing more than

00:30:14,820 --> 00:30:24,660
scale-out nas which was okay place to

00:30:18,750 --> 00:30:28,020
start but you can start seeing some of

00:30:24,660 --> 00:30:29,880
the gaps object storage became something

00:30:28,020 --> 00:30:32,430
people wanted and not just wanted but

00:30:29,880 --> 00:30:36,900
we're consuming very heavily if you look

00:30:32,430 --> 00:30:41,760
at the growth in storage just of s3 it's

00:30:36,900 --> 00:30:44,070
staggering and I honestly think that it

00:30:41,760 --> 00:30:48,060
scares the traditional storage vendors

00:30:44,070 --> 00:30:51,420
quite a bit enough that we've seen some

00:30:48,060 --> 00:30:54,660
proprietary companies like Kareem go get

00:30:51,420 --> 00:30:56,310
into the object storage space and that's

00:30:54,660 --> 00:31:04,070
effectively the only thing they sell is

00:30:56,310 --> 00:31:06,930
object storage appliances big data I

00:31:04,070 --> 00:31:09,140
don't know that big data started in 2011

00:31:06,930 --> 00:31:13,890
but Big Data really took off in

00:31:09,140 --> 00:31:18,210
2010-2011 gluster didn't have any Hadoop

00:31:13,890 --> 00:31:20,280
our MapReduce capabilities they

00:31:18,210 --> 00:31:26,250
certainly couldn't do any kind of

00:31:20,280 --> 00:31:28,770
structured database not well anyway so

00:31:26,250 --> 00:31:32,310
here are some of the problems people had

00:31:28,770 --> 00:31:34,620
when they were putting VM images up it's

00:31:32,310 --> 00:31:35,700
really difficult to self-heal cluster or

00:31:34,620 --> 00:31:38,850
it was

00:31:35,700 --> 00:31:40,830
back in 2011 rebalancing it so you know

00:31:38,850 --> 00:31:43,740
not everything was coming from a single

00:31:40,830 --> 00:31:47,700
node especially if you were adding nodes

00:31:43,740 --> 00:31:50,700
for additional capacity and of course it

00:31:47,700 --> 00:31:52,409
didn't do small files very well and I

00:31:50,700 --> 00:31:57,149
will argue that it still does not do

00:31:52,409 --> 00:32:01,529
small files very well so as they were

00:31:57,149 --> 00:32:04,019
developing Gluster essentially they

00:32:01,529 --> 00:32:06,510
started trying to fill these gaps they

00:32:04,019 --> 00:32:08,789
focused very heavily on replication and

00:32:06,510 --> 00:32:11,279
especially making all of that far more

00:32:08,789 --> 00:32:13,740
granular so instead of updating an

00:32:11,279 --> 00:32:17,010
entire file replicating an entire file

00:32:13,740 --> 00:32:22,769
let's just update the blocks that were

00:32:17,010 --> 00:32:26,250
updated self-healing used to be just

00:32:22,769 --> 00:32:29,669
healing in other words you had to go and

00:32:26,250 --> 00:32:34,679
tell it to heal itself that's no longer

00:32:29,669 --> 00:32:39,480
a problem one of the problems that you

00:32:34,679 --> 00:32:41,519
typically had though was if there if you

00:32:39,480 --> 00:32:44,159
encountered a problem and you ran into a

00:32:41,519 --> 00:32:45,840
split brain issue so you have these

00:32:44,159 --> 00:32:47,700
distributed systems and they become

00:32:45,840 --> 00:32:51,690
disconnected and you have half on once

00:32:47,700 --> 00:32:53,460
I'm half on the other and you've got

00:32:51,690 --> 00:32:55,769
multiple people still accessing both

00:32:53,460 --> 00:32:58,409
sides which one's the latest revision

00:32:55,769 --> 00:33:01,049
and how do you know because they're

00:32:58,409 --> 00:33:03,120
going to be updating each other they're

00:33:01,049 --> 00:33:05,460
going to say oh that's a new version 3

00:33:03,120 --> 00:33:06,809
and the other side says oh this is the

00:33:05,460 --> 00:33:09,649
new version 3 which one's the real

00:33:06,809 --> 00:33:12,210
version 3 in which one's version for

00:33:09,649 --> 00:33:13,919
that was a real problem so they

00:33:12,210 --> 00:33:17,760
introduced this concept of quorum

00:33:13,919 --> 00:33:19,500
meaning to to consider yourself

00:33:17,760 --> 00:33:22,049
authoritative you must have a minimum

00:33:19,500 --> 00:33:24,679
number of nodes and that minimum number

00:33:22,049 --> 00:33:27,179
of nodes is something you can specify

00:33:24,679 --> 00:33:31,919
and they add a synchronous translator

00:33:27,179 --> 00:33:33,240
API because async was a sink was

00:33:31,919 --> 00:33:35,519
probably right from a storage

00:33:33,240 --> 00:33:38,309
perspective to you know wait until it's

00:33:35,519 --> 00:33:40,440
written out but was annoying because a

00:33:38,309 --> 00:33:45,769
lot of those things weren't just storage

00:33:40,440 --> 00:33:45,769
and IO going in and out so

00:33:45,799 --> 00:33:58,010
this granular locking it's block by

00:33:50,220 --> 00:34:00,210
block locking now and if a node goes out

00:33:58,010 --> 00:34:02,399
effectively it's going to lock its own

00:34:00,210 --> 00:34:04,980
and it will compare it with what was the

00:34:02,399 --> 00:34:07,380
live version until it's compared all the

00:34:04,980 --> 00:34:10,139
blocks and otherwise those will be

00:34:07,380 --> 00:34:13,560
locked and this actually forms some of

00:34:10,139 --> 00:34:19,770
the basis for some of the better better

00:34:13,560 --> 00:34:21,389
replication so self-healing is performed

00:34:19,770 --> 00:34:24,540
server-to-server there's no need for you

00:34:21,389 --> 00:34:28,530
to get involved when a node comes online

00:34:24,540 --> 00:34:30,419
or a node recovers it's actually going

00:34:28,530 --> 00:34:33,210
to query its peers to find out the

00:34:30,419 --> 00:34:37,050
status of things to find out if it's

00:34:33,210 --> 00:34:39,929
even valid to to come back in and it

00:34:37,050 --> 00:34:42,540
will automatically say oh I'm supposed

00:34:39,929 --> 00:34:44,669
to have will say that these are really

00:34:42,540 --> 00:34:48,510
blocks I'm supposed to have blocks 1 2 &

00:34:44,669 --> 00:34:51,839
3 let me get them from a from one of the

00:34:48,510 --> 00:34:57,630
good replicas that the quorum agrees is

00:34:51,839 --> 00:35:01,859
good so we talked about this real

00:34:57,630 --> 00:35:04,740
briefly split brain and and the love of

00:35:01,859 --> 00:35:06,540
the Quorum enforcement has gone into

00:35:04,740 --> 00:35:08,700
this to stop this because this was a big

00:35:06,540 --> 00:35:11,730
problem that they would see especially

00:35:08,700 --> 00:35:15,000
as there were larger and larger

00:35:11,730 --> 00:35:16,859
deployments of luster it became more

00:35:15,000 --> 00:35:20,970
more common for there to be these

00:35:16,859 --> 00:35:26,369
conflicts in which which version or

00:35:20,970 --> 00:35:27,960
iteration was the correct one and this

00:35:26,369 --> 00:35:34,820
is marketing fluff that we've already

00:35:27,960 --> 00:35:37,950
talked about and so this is briefly

00:35:34,820 --> 00:35:41,849
demonstrating the if you don't have

00:35:37,950 --> 00:35:44,369
quorum essentially the Gluster Damon

00:35:41,849 --> 00:35:47,910
will stop allowing you to have rights to

00:35:44,369 --> 00:35:52,440
those bricks and if you do have for them

00:35:47,910 --> 00:35:54,500
you may continue writing so let's talk

00:35:52,440 --> 00:35:54,500
about

00:35:57,529 --> 00:36:01,589
the synchronous translators allow you to

00:35:59,940 --> 00:36:06,960
do things that aren't necessarily i/o

00:36:01,589 --> 00:36:09,769
bound r io sensitive but nobody cares

00:36:06,960 --> 00:36:11,700
about that or I don't care about that

00:36:09,769 --> 00:36:15,200
and that's because I'm not running a

00:36:11,700 --> 00:36:19,559
translator unified file and object

00:36:15,200 --> 00:36:24,480
access so you can have the same the same

00:36:19,559 --> 00:36:26,099
file which of course it's going to be

00:36:24,480 --> 00:36:29,339
our volume which will have directories

00:36:26,099 --> 00:36:32,789
and will have files there and both an

00:36:29,339 --> 00:36:34,829
object storage client and or a NFS or

00:36:32,789 --> 00:36:37,079
Gluster mount can access the same thing

00:36:34,829 --> 00:36:39,420
and that gives you multiple routes to it

00:36:37,079 --> 00:36:41,730
so you're really stupid applications

00:36:39,420 --> 00:36:44,970
that you don't want to have to assume

00:36:41,730 --> 00:36:47,190
that there's a fuse client on or you

00:36:44,970 --> 00:36:49,619
don't want to have to mount an NFS mount

00:36:47,190 --> 00:36:54,749
or maybe you can't because it's over the

00:36:49,619 --> 00:36:57,440
wam can still get access to a file using

00:36:54,749 --> 00:36:57,440
object storage

00:37:00,650 --> 00:37:10,400
so HDFS compatibility essentially gives

00:37:06,920 --> 00:37:13,369
you the ability to run MapReduce jobs on

00:37:10,400 --> 00:37:16,789
gluster and gives you more of that

00:37:13,369 --> 00:37:23,660
unstructured data capability to Hadoop

00:37:16,789 --> 00:37:26,569
and and then you know so your Hadoop

00:37:23,660 --> 00:37:38,440
server is processing against your your

00:37:26,569 --> 00:37:40,460
cluster volumes I think cluster clusters

00:37:38,440 --> 00:37:48,770
structure is actually a little more

00:37:40,460 --> 00:37:50,029
redundant than HDFS is HDFS I think I'm

00:37:48,770 --> 00:37:51,740
not saying that they do this but I think

00:37:50,029 --> 00:37:56,510
that the assumptions that they make or

00:37:51,740 --> 00:37:59,270
that things will work well and that if

00:37:56,510 --> 00:38:02,260
there's a failure you can start all over

00:37:59,270 --> 00:38:04,990
again and be running the same analysis

00:38:02,260 --> 00:38:07,369
which is which is fine for some

00:38:04,990 --> 00:38:09,859
implementations think gluster actually

00:38:07,369 --> 00:38:13,460
provides a little more redundancy in

00:38:09,859 --> 00:38:15,529
that set up because HDFS if things start

00:38:13,460 --> 00:38:18,940
breaking apart it starts failing very

00:38:15,529 --> 00:38:24,170
rapidly and this has a little bit better

00:38:18,940 --> 00:38:25,460
capability to keep up all right so again

00:38:24,170 --> 00:38:27,500
if you wish to complain about this

00:38:25,460 --> 00:38:29,539
presentation John market redhat com is

00:38:27,500 --> 00:38:32,119
where you can email the person to

00:38:29,539 --> 00:38:36,400
complain that any questions I can answer

00:38:32,119 --> 00:38:36,400
about cluster yes

00:38:42,720 --> 00:38:49,930
so it has no inherent knowledge of

00:38:45,730 --> 00:38:56,170
things like d Duke and it really doesn't

00:38:49,930 --> 00:39:05,020
have a concept of it really doesn't have

00:38:56,170 --> 00:39:07,050
a concept of a that it shouldn't that

00:39:05,020 --> 00:39:12,730
should be keeping the same exact files

00:39:07,050 --> 00:39:15,580
as a single object it it is dumb in that

00:39:12,730 --> 00:39:16,930
particular method in it it just assumes

00:39:15,580 --> 00:39:20,130
that you know what you're doing and that

00:39:16,930 --> 00:39:22,480
you're putting data the right data and

00:39:20,130 --> 00:39:23,350
that you're not copying 50 files of

00:39:22,480 --> 00:39:27,690
itself because it will of course

00:39:23,350 --> 00:39:36,910
distribute and replicate all 50 copies

00:39:27,690 --> 00:39:39,820
yes so I have heard of people doing ZFS

00:39:36,910 --> 00:39:42,730
and gluster and particularly but they're

00:39:39,820 --> 00:39:46,000
running ZFS a top clusters of

00:39:42,730 --> 00:39:53,830
Gloucester's is to take advantage of

00:39:46,000 --> 00:39:54,970
snapshots because but I haven't I don't

00:39:53,830 --> 00:39:58,810
know if people are doing the opposite

00:39:54,970 --> 00:40:01,900
way or if that would work even certainly

00:39:58,810 --> 00:40:03,790
you could use ZFS as the as the back end

00:40:01,900 --> 00:40:06,220
but I don't know that it would

00:40:03,790 --> 00:40:09,070
necessarily help you because it's going

00:40:06,220 --> 00:40:11,619
to be chunks of files that are written

00:40:09,070 --> 00:40:15,100
to ZFS so I don't know that there's a

00:40:11,619 --> 00:40:18,060
benefit there I don't know that there's

00:40:15,100 --> 00:40:21,060
a benefit there 22 running ZFS under

00:40:18,060 --> 00:40:21,060
cluster

00:40:25,039 --> 00:40:30,829
really I can't believe red hat says that

00:40:35,630 --> 00:40:43,199
wow they used to say XFS and technically

00:40:41,249 --> 00:40:47,099
they'll do anything that has extended

00:40:43,199 --> 00:40:49,469
attributes I find that very interesting

00:40:47,099 --> 00:40:58,400
that they would suggest ZFS especially

00:40:49,469 --> 00:41:06,179
since they can't then sell that right

00:40:58,400 --> 00:41:08,369
yeah i I don't I don't know that I

00:41:06,179 --> 00:41:10,849
understand the reason for part of using

00:41:08,369 --> 00:41:10,849
ZFS

00:41:17,219 --> 00:41:28,359
you're getting same capabilities on your

00:41:20,049 --> 00:41:33,849
direct attached storage right the the

00:41:28,359 --> 00:41:35,859
concern that I have with ZFS is solaris

00:41:33,849 --> 00:41:42,369
isn't necessarily dead but open solaris

00:41:35,859 --> 00:41:46,059
certainly is and freebsd abut freebsd is

00:41:42,369 --> 00:41:51,930
actually using a several versions back

00:41:46,059 --> 00:41:51,930
of ZFS they're not using the latest and

00:41:52,680 --> 00:41:56,769
okay i was going to say i can't i can't

00:41:54,940 --> 00:41:58,239
believe that they want to do ZFS but

00:41:56,769 --> 00:42:00,509
that makes no sense on the Red Hat's

00:41:58,239 --> 00:42:00,509
part

00:42:08,750 --> 00:42:15,110
yeah so so I actually had this argument

00:42:11,510 --> 00:42:17,570
with him and i told them probably

00:42:15,110 --> 00:42:21,290
actually it was at the last self I had a

00:42:17,570 --> 00:42:24,560
call with with a B and John Mark and I

00:42:21,290 --> 00:42:26,960
said so I see the recommended way is

00:42:24,560 --> 00:42:30,380
fuse and I was a Gloucester noob at the

00:42:26,960 --> 00:42:32,450
time and I said that makes no sense

00:42:30,380 --> 00:42:33,530
that's just horrendous capability when

00:42:32,450 --> 00:42:36,050
are you guys going to get it in the

00:42:33,530 --> 00:42:38,030
kernel and a be laughed at me and said

00:42:36,050 --> 00:42:41,060
have you actually tried it and I said

00:42:38,030 --> 00:42:44,260
well you know I've tried it I don't know

00:42:41,060 --> 00:42:48,950
that I've done any performance tests and

00:42:44,260 --> 00:42:51,680
clusters is never going to be an

00:42:48,950 --> 00:42:53,810
incredible speed demon it will be

00:42:51,680 --> 00:42:59,420
impressive for for the size that you

00:42:53,810 --> 00:43:01,730
push it too but you know when I actually

00:42:59,420 --> 00:43:04,190
got around to investigating the fuse

00:43:01,730 --> 00:43:07,130
client gave surprisingly good

00:43:04,190 --> 00:43:09,110
performance especially for what it is

00:43:07,130 --> 00:43:12,740
which is a distributed file system it

00:43:09,110 --> 00:43:13,880
was it was beating it was feeding some

00:43:12,740 --> 00:43:17,750
of the other things that I've been

00:43:13,880 --> 00:43:19,400
looking at like luster forward with

00:43:17,750 --> 00:43:23,860
given workloads it was doing a much

00:43:19,400 --> 00:43:27,080
better job so looking at the fuse client

00:43:23,860 --> 00:43:30,020
ignore your prejudices for a bit and try

00:43:27,080 --> 00:43:35,750
it and see if it will deliver what you

00:43:30,020 --> 00:43:37,670
want I was pleasantly surprised sir all

00:43:35,750 --> 00:43:42,520
right well I will gladly shut up if no

00:43:37,670 --> 00:43:42,520
one else has questions good for

00:43:43,540 --> 00:43:55,790
say it one more time yeah so it'll it'll

00:43:50,750 --> 00:43:57,620
handle sports files it's not going to do

00:43:55,790 --> 00:43:59,420
it any faster because it's looking at

00:43:57,620 --> 00:44:01,430
the block level rather than considering

00:43:59,420 --> 00:44:05,420
oh this is a bunch of zeros that I can

00:44:01,430 --> 00:44:13,040
compress so it will handle them as if

00:44:05,420 --> 00:44:16,010
it's not a sparse file right all right

00:44:13,040 --> 00:44:17,870
all right well I appreciate your

00:44:16,010 --> 00:44:20,630
attention I apologize that I am NOT John

00:44:17,870 --> 00:44:23,110
Mark for you feel free to complain to

00:44:20,630 --> 00:44:23,110
him there

00:44:35,640 --> 00:44:40,420
when we created asterisk over a decade

00:44:38,350 --> 00:44:42,670
ago we could not have imagined that

00:44:40,420 --> 00:44:44,860
asterisk would not only become the most

00:44:42,670 --> 00:44:46,990
widely adopted open source communication

00:44:44,860 --> 00:44:49,030
software on the planet but that it would

00:44:46,990 --> 00:44:51,550
impact the entire industry in the way

00:44:49,030 --> 00:44:53,530
that it has today asterisk has found its

00:44:51,550 --> 00:44:56,170
way into more than 170 countries and

00:44:53,530 --> 00:44:58,120
virtually every fortune 1000 company the

00:44:56,170 --> 00:45:00,190
success of asterisk has enabled a

00:44:58,120 --> 00:45:01,630
transition of power from the hands of

00:45:00,190 --> 00:45:03,910
the traditional proprietary phone

00:45:01,630 --> 00:45:06,280
vendors into the hands of the users and

00:45:03,910 --> 00:45:08,230
administrators of phone systems using

00:45:06,280 --> 00:45:09,460
this power our customers have created

00:45:08,230 --> 00:45:11,410
all sorts of business changing

00:45:09,460 --> 00:45:13,240
applications from small office phone

00:45:11,410 --> 00:45:15,760
systems to mission-critical call centres

00:45:13,240 --> 00:45:17,380
the international carrier networks in

00:45:15,760 --> 00:45:19,150
fact there's even an entire country

00:45:17,380 --> 00:45:21,550
those communications of a structure runs

00:45:19,150 --> 00:45:23,680
on esters the gym has always been about

00:45:21,550 --> 00:45:25,390
creating technology that expands

00:45:23,680 --> 00:45:27,550
communications capabilities in ways that

00:45:25,390 --> 00:45:28,750
we could never have imagined and that's

00:45:27,550 --> 00:45:31,480
part of what's game-changing about

00:45:28,750 --> 00:45:33,850
Digium today we're doing it again this

00:45:31,480 --> 00:45:35,950
time by introducing a new family of HD

00:45:33,850 --> 00:45:38,050
IP phones that extends control of the

00:45:35,950 --> 00:45:39,820
user all the way to the desktop the

00:45:38,050 --> 00:45:41,530
launch of these new products represents

00:45:39,820 --> 00:45:43,630
the next phase indigenous history of

00:45:41,530 --> 00:45:46,030
innovation these are the first and only

00:45:43,630 --> 00:45:47,740
IP phones designed to fully leverage the

00:45:46,030 --> 00:45:49,270
power of estrus when we first discussed

00:45:47,740 --> 00:45:51,310
our expectations for building a family

00:45:49,270 --> 00:45:53,470
of phones for use with asterisk our

00:45:51,310 --> 00:45:55,210
requirements were pretty simple we asked

00:45:53,470 --> 00:45:56,860
the team to build the phones such that

00:45:55,210 --> 00:45:59,050
they were easy to install integrate

00:45:56,860 --> 00:46:00,880
provision and use I think you'll soon

00:45:59,050 --> 00:46:03,460
agree our engineers have delivered on

00:46:00,880 --> 00:46:05,080
that goal user feedback is validating

00:46:03,460 --> 00:46:07,210
that when it comes to operation with

00:46:05,080 --> 00:46:09,640
Pastor space systems including our own

00:46:07,210 --> 00:46:12,190
Switchvox based product these are the

00:46:09,640 --> 00:46:13,510
easiest to use best integrated most

00:46:12,190 --> 00:46:16,090
interoperable products on the market

00:46:13,510 --> 00:46:18,090
today the digitally phones will

00:46:16,090 --> 00:46:20,140
initially include three IP des hommes

00:46:18,090 --> 00:46:22,090
uniquely designed to complement any

00:46:20,140 --> 00:46:23,950
asterisks or Switchvox based solution

00:46:22,090 --> 00:46:26,620
these phones are different for a number

00:46:23,950 --> 00:46:29,290
of reasons first there is clue sively

00:46:26,620 --> 00:46:30,700
designed for use with esters secondly

00:46:29,290 --> 00:46:32,310
we've made it really easy to

00:46:30,700 --> 00:46:34,720
autodiscover and provision the phones

00:46:32,310 --> 00:46:36,400
next we've made it easy for the phones

00:46:34,720 --> 00:46:38,350
to access information inside of

00:46:36,400 --> 00:46:40,350
asterisks allowing tight coupling

00:46:38,350 --> 00:46:42,340
between an application and the phone

00:46:40,350 --> 00:46:44,470
additionally we've created an

00:46:42,340 --> 00:46:46,600
applications engine that allows users

00:46:44,470 --> 00:46:49,840
and developers to create and run their

00:46:46,600 --> 00:46:51,610
own apps on the phone and finally we've

00:46:49,840 --> 00:46:53,560
done all of this at a very compelling

00:46:51,610 --> 00:46:55,240
price point at Digium we're always

00:46:53,560 --> 00:46:57,340
thinking of ways to give our customers

00:46:55,240 --> 00:46:59,920
the best value in business phone systems

00:46:57,340 --> 00:47:01,570
and also give them the power to create

00:46:59,920 --> 00:47:03,610
their own solutions or eating

00:47:01,570 --> 00:47:05,320
communications challenge will continue

00:47:03,610 --> 00:47:06,760
to push the boundaries not only to make

00:47:05,320 --> 00:47:08,830
asterisk cooler faster and more

00:47:06,760 --> 00:47:10,450
technologically feature-rich but to make

00:47:08,830 --> 00:47:12,910
asterisk and what communications even

00:47:10,450 --> 00:47:16,620
easier and together we'll change the way

00:47:12,910 --> 00:47:16,620
the world communicates again

00:47:27,330 --> 00:47:34,480
prospects I every way this is the way to

00:47:30,790 --> 00:47:36,430
better utilize all your resources and it

00:47:34,480 --> 00:47:39,490
makes managing all your resources pretty

00:47:36,430 --> 00:47:44,350
easy all of the innovation is happening

00:47:39,490 --> 00:47:47,710
in open source the collaborative nature

00:47:44,350 --> 00:47:50,380
and of the you know of the community and

00:47:47,710 --> 00:47:52,420
the speed at which these are these you

00:47:50,380 --> 00:47:54,580
know these deficiencies these bugs are

00:47:52,420 --> 00:47:56,950
getting discovered and then fixed is it

00:47:54,580 --> 00:47:58,420
I think that really shows the power of

00:47:56,950 --> 00:48:01,150
the you know of the open source

00:47:58,420 --> 00:48:03,330
community it is global and it's

00:48:01,150 --> 00:48:06,850
definitely because of the users

00:48:03,330 --> 00:48:12,730
community people are extremely friendly

00:48:06,850 --> 00:48:14,560
and always ready to help if you go an

00:48:12,730 --> 00:48:16,780
entire see any day you'll see these guys

00:48:14,560 --> 00:48:19,390
helping each other out and they're all

00:48:16,780 --> 00:48:21,100
doing it like in a selfless manner the

00:48:19,390 --> 00:48:24,090
product is transparent for everyone

00:48:21,100 --> 00:48:26,740
everyone can look at the code base

00:48:24,090 --> 00:48:28,750
everyone can see how growth dark is

00:48:26,740 --> 00:48:34,630
being built nothing nothing is

00:48:28,750 --> 00:48:37,270
proprietary everything is open in many

00:48:34,630 --> 00:48:41,470
ways it's absolutely vital to the the

00:48:37,270 --> 00:48:45,250
ongoing health cloudstack the most

00:48:41,470 --> 00:48:49,080
exciting event in recent memory for me

00:48:45,250 --> 00:48:51,780
was our first developer boot camp

00:48:49,080 --> 00:48:54,450
and our call gave people I gave you two

00:48:51,780 --> 00:48:58,410
weeks notice to come attend I was

00:48:54,450 --> 00:49:02,640
expecting 25 or 30 people so we ended up

00:48:58,410 --> 00:49:05,400
with 87 people and had to go get board

00:49:02,640 --> 00:49:07,950
chairs in the room twice everything

00:49:05,400 --> 00:49:11,190
within cloud computing is commodity and

00:49:07,950 --> 00:49:14,250
is open source and so I don't think that

00:49:11,190 --> 00:49:16,020
you will you'll see anywhere where open

00:49:14,250 --> 00:49:19,050
source is not pervasive in cloud

00:49:16,020 --> 00:49:21,600
computing and so i think it's i think

00:49:19,050 --> 00:49:23,100
it's an assumption i think when you talk

00:49:21,600 --> 00:49:24,240
about cloud computing you're really

00:49:23,100 --> 00:49:29,790
talking about open source cloud

00:49:24,240 --> 00:49:32,820
computing cloud zac is a robust solution

00:49:29,790 --> 00:49:34,950
for large deployments you'll have dozens

00:49:32,820 --> 00:49:39,210
of data centers and thousands of servers

00:49:34,950 --> 00:49:41,910
in each data centers these hardware is

00:49:39,210 --> 00:49:45,390
going to fail and CloudStack is designed

00:49:41,910 --> 00:49:48,090
to handle number one that mass scale

00:49:45,390 --> 00:49:51,270
number two it's designed to handle the

00:49:48,090 --> 00:49:53,820
failure that inevitably happens in large

00:49:51,270 --> 00:49:58,320
deployments started working on cloud

00:49:53,820 --> 00:50:01,620
tech over four years ago and it was the

00:49:58,320 --> 00:50:03,630
original set of people working on it had

00:50:01,620 --> 00:50:07,470
a background of delivering software to

00:50:03,630 --> 00:50:11,730
telcos and service providers lots of QA

00:50:07,470 --> 00:50:15,330
lots of users actually using it high

00:50:11,730 --> 00:50:18,420
availability is a key feature multiple

00:50:15,330 --> 00:50:20,430
hypervisors support different network

00:50:18,420 --> 00:50:23,100
models you can pick up whatever suits

00:50:20,430 --> 00:50:25,380
you better while step management server

00:50:23,100 --> 00:50:29,070
can be deployed in different physical

00:50:25,380 --> 00:50:30,720
machines it definitely has a huge

00:50:29,070 --> 00:50:35,700
footprint it's being deployed everywhere

00:50:30,720 --> 00:50:38,280
there's a major movie studio that they

00:50:35,700 --> 00:50:41,580
were using cloudstack they were using it

00:50:38,280 --> 00:50:43,560
to transcode video and i thought that

00:50:41,580 --> 00:50:45,210
was terribly fascinating what i found

00:50:43,560 --> 00:50:48,480
more fascinating is what they did during

00:50:45,210 --> 00:50:51,270
lunch where they would spin up you know

00:50:48,480 --> 00:50:52,430
50 or 60 game servers then as soon as

00:50:51,270 --> 00:50:54,260
lunch was over they would just

00:50:52,430 --> 00:50:58,400
all the instances and go back to doing

00:50:54,260 --> 00:51:00,020
real work CloudStack is vast it touches

00:50:58,400 --> 00:51:02,090
so many different aspects and there's no

00:51:00,020 --> 00:51:04,490
one person that's kind of like a master

00:51:02,090 --> 00:51:08,600
of all those realms I think clouds back

00:51:04,490 --> 00:51:10,820
as a project is going to be one of the

00:51:08,600 --> 00:51:14,870
leaders simply because it's some of the

00:51:10,820 --> 00:51:20,300
most feature fallen and and robust

00:51:14,870 --> 00:51:23,230
platforms out they were Adam senior

00:51:20,300 --> 00:51:23,230
living through the clouds dag

00:51:32,710 --> 00:51:34,770
you

00:51:36,059 --> 00:51:38,119
you

00:52:06,799 --> 00:52:08,859

YouTube URL: https://www.youtube.com/watch?v=kI3b8dgln7Q


