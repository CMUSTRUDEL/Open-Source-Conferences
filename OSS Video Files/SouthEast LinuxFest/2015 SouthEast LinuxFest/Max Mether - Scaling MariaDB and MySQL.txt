Title: Max Mether - Scaling MariaDB and MySQL
Publication date: 2017-01-25
Playlist: 2015 SouthEast LinuxFest
Description: 
	
Captions: 
	00:22:32,930 --> 00:22:36,920
if you do if you do right on all nodes

00:22:35,270 --> 00:22:40,550
at which point that it's kind of stopped

00:22:36,920 --> 00:22:41,930
being beneficial and from the Benjamins

00:22:40,550 --> 00:22:44,030
I've seen in somewhere between five and

00:22:41,930 --> 00:22:46,160
eight nodes depending on what type of

00:22:44,030 --> 00:22:47,690
transactions you have where you don't

00:22:46,160 --> 00:22:51,320
gain anything anymore by adding more

00:22:47,690 --> 00:22:54,350
nodes but typically i mean we have a lot

00:22:51,320 --> 00:22:55,970
of customers using galloran and eighty

00:22:54,350 --> 00:22:58,420
percent or ninety percent are using

00:22:55,970 --> 00:23:01,010
three notes they don't use anymore

00:22:58,420 --> 00:23:03,170
because that's already good enough but

00:23:01,010 --> 00:23:07,820
you can add notes and you get some some

00:23:03,170 --> 00:23:09,320
benefit from that and same as with

00:23:07,820 --> 00:23:12,860
replication though you need some kind of

00:23:09,320 --> 00:23:14,810
load balancer because because well to

00:23:12,860 --> 00:23:16,790
hide to hide the underlying the cluster

00:23:14,810 --> 00:23:17,840
cluster nodes from from your

00:23:16,790 --> 00:23:20,330
applications that the application

00:23:17,840 --> 00:23:22,580
doesn't have to be aware of which nodes

00:23:20,330 --> 00:23:24,050
are there which nodes are up to date and

00:23:22,580 --> 00:23:25,880
what if you're not crashes and so forth

00:23:24,050 --> 00:23:33,050
so you need some kind of load balancer

00:23:25,880 --> 00:23:35,210
for it any questions about good air well

00:23:33,050 --> 00:23:38,150
this is going well everyone is no one

00:23:35,210 --> 00:23:42,530
has any questions I must be do I must be

00:23:38,150 --> 00:23:44,690
doing a really good job so if you look

00:23:42,530 --> 00:23:50,420
at load balancers then for both both

00:23:44,690 --> 00:23:52,250
color and and replication there's in the

00:23:50,420 --> 00:23:54,140
main reason as I said for load balancer

00:23:52,250 --> 00:23:56,960
is so that the application does not have

00:23:54,140 --> 00:23:59,060
to include any kind of logic for for the

00:23:56,960 --> 00:24:01,870
cluster there's a couple of examples

00:23:59,060 --> 00:24:04,340
here a che proxies and open source one

00:24:01,870 --> 00:24:09,740
got a load balancer is specific to

00:24:04,340 --> 00:24:11,510
calera the JDBC client or it's actually

00:24:09,740 --> 00:24:15,250
called connector the JDBC connector

00:24:11,510 --> 00:24:18,620
actually has built-in load balancing

00:24:15,250 --> 00:24:20,120
very basic load balancing but still you

00:24:18,620 --> 00:24:22,190
can bet you give it a list of servers

00:24:20,120 --> 00:24:24,200
and it assumes the first server is the

00:24:22,190 --> 00:24:26,000
master the others are slaves and it will

00:24:24,200 --> 00:24:28,430
distribute your traffic according to

00:24:26,000 --> 00:24:30,440
that then you can use my escrow fabric

00:24:28,430 --> 00:24:33,140
with also a lot of sharding which I'm a

00:24:30,440 --> 00:24:35,360
bit more complicated and then we have

00:24:33,140 --> 00:24:41,660
our own mac scale which i will just

00:24:35,360 --> 00:24:46,200
mention in a few slides here so

00:24:41,660 --> 00:24:49,020
so a load balancer in general needs to

00:24:46,200 --> 00:24:50,940
be able to obviously monitor your nodes

00:24:49,020 --> 00:24:54,800
so if you know it goes down the load

00:24:50,940 --> 00:24:59,400
balancer doesn't send queries do it and

00:24:54,800 --> 00:25:05,190
some proxies that are generic depending

00:24:59,400 --> 00:25:07,200
on your your what type of technology

00:25:05,190 --> 00:25:08,640
using behind a generic little balancer

00:25:07,200 --> 00:25:11,130
might not be enough for example H a

00:25:08,640 --> 00:25:13,500
proxy when we started using it we use it

00:25:11,130 --> 00:25:15,840
for galera clusters but the problem was

00:25:13,500 --> 00:25:18,780
that it was only checking whether myszka

00:25:15,840 --> 00:25:20,820
servers up or down and you know got our

00:25:18,780 --> 00:25:22,740
cluster at my school I'm ready to be

00:25:20,820 --> 00:25:25,050
server can be up but not not

00:25:22,740 --> 00:25:27,510
synchronized so you shouldn't send

00:25:25,050 --> 00:25:30,360
anything i mean i mean the the server

00:25:27,510 --> 00:25:32,100
will refuse any any kind of queries so

00:25:30,360 --> 00:25:34,200
you so you could they will fail anyway

00:25:32,100 --> 00:25:35,850
but you waste a lot of you get a lot of

00:25:34,200 --> 00:25:37,170
aborted transactions just because you

00:25:35,850 --> 00:25:40,790
send queries to server that's actually

00:25:37,170 --> 00:25:43,260
not there so you're you need to have a

00:25:40,790 --> 00:25:45,600
monitoring that's actually applied to

00:25:43,260 --> 00:25:47,520
your underlying technology right so you

00:25:45,600 --> 00:25:49,800
need to be aware of ok how does gallery

00:25:47,520 --> 00:25:51,600
work when is a glare note synchronized

00:25:49,800 --> 00:25:53,940
and so forth and if you use your

00:25:51,600 --> 00:25:56,070
application you typically want to be

00:25:53,940 --> 00:25:59,430
aware of the slave lag as well because

00:25:56,070 --> 00:26:01,380
you want you know if a slave is lagging

00:25:59,430 --> 00:26:03,420
a day you probably don't want to read

00:26:01,380 --> 00:26:05,460
from that slave well I mean maybe you do

00:26:03,420 --> 00:26:07,590
but but depending on your application

00:26:05,460 --> 00:26:10,320
you might have requirements on which

00:26:07,590 --> 00:26:14,010
slaves can be read from which cannot do

00:26:10,320 --> 00:26:15,780
the things like this and I mean those

00:26:14,010 --> 00:26:19,440
are some of the things we've built into

00:26:15,780 --> 00:26:22,470
Mac scale we obviously no no believe it

00:26:19,440 --> 00:26:26,820
quite a bit so we have we have the this

00:26:22,470 --> 00:26:28,410
dis knowledge in house so I don't want

00:26:26,820 --> 00:26:30,930
to talk too much about macs care but but

00:26:28,410 --> 00:26:34,670
just a few use cases so to give an idea

00:26:30,930 --> 00:26:39,390
so max scale also has a read/write split

00:26:34,670 --> 00:26:42,150
router built in which means that if you

00:26:39,390 --> 00:26:45,150
stand the replication you have a master

00:26:42,150 --> 00:26:47,580
in have slaves max scale allows you to

00:26:45,150 --> 00:26:49,920
just to just connect the mac scale and

00:26:47,580 --> 00:26:51,240
based on the queries coming it will

00:26:49,920 --> 00:26:54,270
decide whether to send this to the

00:26:51,240 --> 00:26:55,500
master or to the slaves alright so it's

00:26:54,270 --> 00:27:02,340
a rewrite and

00:26:55,500 --> 00:27:03,960
which is very useful oh it's again

00:27:02,340 --> 00:27:06,060
depending on your transaction obvious if

00:27:03,960 --> 00:27:08,520
you have if you have large transactions

00:27:06,060 --> 00:27:10,050
that have have both writes and reads

00:27:08,520 --> 00:27:12,990
they will all go to the master because

00:27:10,050 --> 00:27:14,550
it it won't be able to tell in advance

00:27:12,990 --> 00:27:17,580
whether this is going to be a read or

00:27:14,550 --> 00:27:19,470
write so it will if the only work in

00:27:17,580 --> 00:27:24,030
some cases it won't work but it won't

00:27:19,470 --> 00:27:26,220
work in all cases and basically the max

00:27:24,030 --> 00:27:28,290
care proxy it looks like a mysql server

00:27:26,220 --> 00:27:30,330
marie DB server so your application

00:27:28,290 --> 00:27:33,090
things or your clients they think

00:27:30,330 --> 00:27:34,350
they're connecting to tumor UDP or mysql

00:27:33,090 --> 00:27:36,360
but they're actually connecting to max

00:27:34,350 --> 00:27:39,810
scale and max scale then decides it gets

00:27:36,360 --> 00:27:41,430
the queries from from the application

00:27:39,810 --> 00:27:43,830
and then they decide ok what do I do

00:27:41,430 --> 00:27:45,360
with these queries in this case using

00:27:43,830 --> 00:27:47,040
the read/write split router it will

00:27:45,360 --> 00:27:49,980
check ok is this a read query right

00:27:47,040 --> 00:27:52,650
query or a query where I don't really

00:27:49,980 --> 00:27:54,510
know what it is and and for write

00:27:52,650 --> 00:27:56,190
queries and I don't know one it will

00:27:54,510 --> 00:27:58,830
send them to the master and for the rest

00:27:56,190 --> 00:28:01,340
if to do a load balancing on the slaves

00:27:58,830 --> 00:28:03,600
and here max scale also has the

00:28:01,340 --> 00:28:05,250
capability you can give it rules ok if a

00:28:03,600 --> 00:28:06,690
slave lag is more than this the slavery

00:28:05,250 --> 00:28:08,730
should not be used and things like that

00:28:06,690 --> 00:28:11,820
then you can say ok I want to load

00:28:08,730 --> 00:28:14,520
balance based on connections on those

00:28:11,820 --> 00:28:21,330
slaves or something like that so you can

00:28:14,520 --> 00:28:22,800
choose that max can also be used color

00:28:21,330 --> 00:28:27,300
because we have a built-in galera

00:28:22,800 --> 00:28:29,640
monitor and again you can actually use

00:28:27,300 --> 00:28:31,530
the read write rewrite split router as

00:28:29,640 --> 00:28:33,900
well but you can also a la Mexico to

00:28:31,530 --> 00:28:38,370
just do round-robin across the gallery

00:28:33,900 --> 00:28:41,550
node which is typically the the best way

00:28:38,370 --> 00:28:43,380
from a load balancing perspective in

00:28:41,550 --> 00:28:47,910
which case basically you connect to max

00:28:43,380 --> 00:28:50,610
scale it looks like 11 Marie DB server

00:28:47,910 --> 00:28:52,440
but max k will actually redirect this to

00:28:50,610 --> 00:28:55,020
one of the active service in the gallery

00:28:52,440 --> 00:28:57,330
cluster again one of the main benefits

00:28:55,020 --> 00:28:58,890
is that max scale takes care of

00:28:57,330 --> 00:29:00,900
monitoring whether all the nodes are

00:28:58,890 --> 00:29:03,210
there so your application don't have to

00:29:00,900 --> 00:29:04,860
worry about hey do I have three nodes to

00:29:03,210 --> 00:29:06,480
have four nodes which one is running and

00:29:04,860 --> 00:29:09,420
so forth because max gothic scared of

00:29:06,480 --> 00:29:13,290
that so the transpire is a huge benefit

00:29:09,420 --> 00:29:14,430
and you can use the reader even if using

00:29:13,290 --> 00:29:17,790
gallery you can actually use the

00:29:14,430 --> 00:29:20,340
read/write spectra router with max scale

00:29:17,790 --> 00:29:24,590
as well now what would the benefit be of

00:29:20,340 --> 00:29:24,590
doing that instead of the previous one

00:29:31,160 --> 00:29:35,700
exactly the answer is that way you don't

00:29:33,720 --> 00:29:39,690
have to worry about potential conflicts

00:29:35,700 --> 00:29:41,880
so you can sit up galera in a normal

00:29:39,690 --> 00:29:44,670
cluster but you send all your rights to

00:29:41,880 --> 00:29:47,070
one node this way it that it perhaps

00:29:44,670 --> 00:29:48,930
doesn't scale rights as much but you

00:29:47,070 --> 00:29:51,870
will never have any conflicts because

00:29:48,930 --> 00:29:53,850
because all rights are going to one node

00:29:51,870 --> 00:29:55,410
so they cannot be any conflict so you

00:29:53,850 --> 00:30:05,160
never have a voltage transaction due to

00:29:55,410 --> 00:30:07,080
conflicts so how does it handle local

00:30:05,160 --> 00:30:09,420
deadlock conflict well the same way as

00:30:07,080 --> 00:30:11,550
inhibitors it's it's all in a DB and

00:30:09,420 --> 00:30:14,280
inner DB has a deadlock conflict the

00:30:11,550 --> 00:30:17,400
texture built in so if doesn't if there

00:30:14,280 --> 00:30:19,080
isn't a deadlock in one server unity we

00:30:17,400 --> 00:30:21,750
will will detect it and it will abort

00:30:19,080 --> 00:30:23,190
the latest statement basically and it's

00:30:21,750 --> 00:30:25,320
the same way so it's all based on in

00:30:23,190 --> 00:30:26,820
Italy because it's all if all rights go

00:30:25,320 --> 00:30:28,320
to one machine everything is local so

00:30:26,820 --> 00:30:33,300
it's done the same way as if you only

00:30:28,320 --> 00:30:35,000
had one machine so that's kind of the

00:30:33,300 --> 00:30:38,130
benefit of using something like this and

00:30:35,000 --> 00:30:39,480
again I am in governor there is no real

00:30:38,130 --> 00:30:41,280
master so it doesn't really matter which

00:30:39,480 --> 00:30:43,380
service the master and if that server

00:30:41,280 --> 00:30:45,380
fails some other one will become the

00:30:43,380 --> 00:30:48,390
master this is something that's actually

00:30:45,380 --> 00:30:50,220
max scale will do there's no master

00:30:48,390 --> 00:30:52,650
definition inside girl rides purely from

00:30:50,220 --> 00:30:55,370
the proxy respecting the master master

00:30:52,650 --> 00:30:55,370
definition

00:31:07,580 --> 00:31:12,220
oh you mean can you decide with like

00:31:10,280 --> 00:31:16,040
have a preference order for the right I

00:31:12,220 --> 00:31:18,140
actually don't know do you know Daniel

00:31:16,040 --> 00:31:22,370
no I actually don't know if you can do

00:31:18,140 --> 00:31:24,650
that my guess is that my guess is yes

00:31:22,370 --> 00:31:26,090
but I am NOT trying to ensure it

00:31:24,650 --> 00:31:32,560
shouldn't be very hard to code if it's

00:31:26,090 --> 00:31:32,560
not there yet so yeah

00:31:34,980 --> 00:31:40,620
no I mean so even if it doesn't have

00:31:38,970 --> 00:31:43,200
that it will automatically fail I worked

00:31:40,620 --> 00:31:45,570
another node it's just that can you can

00:31:43,200 --> 00:31:48,390
you have an order of preference or not

00:31:45,570 --> 00:31:50,010
but even if you can't basically will

00:31:48,390 --> 00:31:52,530
choose one node if that node fails it

00:31:50,010 --> 00:31:55,679
will just use the next one so that works

00:31:52,530 --> 00:31:57,480
for sure and in general with a better

00:31:55,679 --> 00:31:59,340
cluster because all machines will do the

00:31:57,480 --> 00:32:01,260
same thing you want your nodes to

00:31:59,340 --> 00:32:05,820
basically have exactly the same hardware

00:32:01,260 --> 00:32:07,679
anyway the cluster will be as as fast as

00:32:05,820 --> 00:32:09,570
your slowest slowest machine so there's

00:32:07,679 --> 00:32:11,610
no point having one that's faster than

00:32:09,570 --> 00:32:13,559
the others in a galera cluster in

00:32:11,610 --> 00:32:15,270
replication you can do things like like

00:32:13,559 --> 00:32:18,030
we had to find slave and things like

00:32:15,270 --> 00:32:19,500
that but where it might might make sense

00:32:18,030 --> 00:32:22,169
to have one machine faster than others

00:32:19,500 --> 00:32:28,320
but in galera they're all equal so they

00:32:22,169 --> 00:32:33,750
should have have the same hardware any

00:32:28,320 --> 00:32:35,880
other questions about galera or mac

00:32:33,750 --> 00:32:38,070
scale for that matter all right then we

00:32:35,880 --> 00:32:43,470
get to the most fun part which is

00:32:38,070 --> 00:32:45,380
charting so I mean what is charting is

00:32:43,470 --> 00:32:47,490
basically means that you're spitting up

00:32:45,380 --> 00:32:50,100
something in different pieces and

00:32:47,490 --> 00:32:54,380
they're independent in this case we're

00:32:50,100 --> 00:32:58,230
swimming up the database in some way

00:32:54,380 --> 00:32:59,549
across multiple instances and now you

00:32:58,230 --> 00:33:01,260
know there's different ways of doing

00:32:59,549 --> 00:33:02,730
showing you can do schema sharding which

00:33:01,260 --> 00:33:06,270
basically means that you have different

00:33:02,730 --> 00:33:09,120
schemas on different machines so it's

00:33:06,270 --> 00:33:11,429
very much a logical division you can

00:33:09,120 --> 00:33:13,950
have some parts of your application and

00:33:11,429 --> 00:33:15,090
in this data in this server or this

00:33:13,950 --> 00:33:20,610
server cluster and then you have other

00:33:15,090 --> 00:33:23,549
parts in that in another server and it

00:33:20,610 --> 00:33:24,660
can be be something that's done on the

00:33:23,549 --> 00:33:25,950
application level so that the

00:33:24,660 --> 00:33:28,380
application is actually aware of this

00:33:25,950 --> 00:33:31,830
split and it knows it okay if I'm doing

00:33:28,380 --> 00:33:33,870
a like a purchase I should go to these

00:33:31,830 --> 00:33:35,700
databases if I'm doing some kind of

00:33:33,870 --> 00:33:37,740
analytics I go to these database so it's

00:33:35,700 --> 00:33:40,980
a sharding that's very much built into

00:33:37,740 --> 00:33:43,860
your application logic but you can also

00:33:40,980 --> 00:33:46,679
have it in a lower level you can well

00:33:43,860 --> 00:33:47,820
actually max scale has a built-in schema

00:33:46,679 --> 00:33:48,720
base charting so you can actually use

00:33:47,820 --> 00:33:50,640
max scale for

00:33:48,720 --> 00:33:52,620
having separation on the schema level

00:33:50,640 --> 00:33:55,950
see you everyone connects to max scale

00:33:52,620 --> 00:33:58,620
and the application is not aware that

00:33:55,950 --> 00:34:00,419
actually if you if you query that scheme

00:33:58,620 --> 00:34:02,340
way to go to different server than if

00:34:00,419 --> 00:34:04,740
you query that schema it's done by by

00:34:02,340 --> 00:34:08,159
Mac by the proxy so the application is

00:34:04,740 --> 00:34:09,720
not aware of the Charlotte but then

00:34:08,159 --> 00:34:12,030
sharding can also be done on a lower

00:34:09,720 --> 00:34:14,190
level because this of course only works

00:34:12,030 --> 00:34:16,290
if you have multiple schemas and you

00:34:14,190 --> 00:34:18,330
have a fairly even distribution between

00:34:16,290 --> 00:34:21,810
lower between the schemas but if you

00:34:18,330 --> 00:34:24,720
have one table where you have a huge

00:34:21,810 --> 00:34:28,230
load that doesn't help the only way to

00:34:24,720 --> 00:34:30,090
to get past that is to use key base

00:34:28,230 --> 00:34:32,659
charting so basically split your table

00:34:30,090 --> 00:34:35,340
in two different pieces based on on

00:34:32,659 --> 00:34:36,720
primary key values and then these

00:34:35,340 --> 00:34:43,530
different pieces will be on different

00:34:36,720 --> 00:34:44,790
machines right and how you implement

00:34:43,530 --> 00:34:46,919
charting so you can do it in the

00:34:44,790 --> 00:34:48,480
application independently of whether

00:34:46,919 --> 00:34:51,050
it's key based or schema is charred and

00:34:48,480 --> 00:34:53,129
you can do it in the application level

00:34:51,050 --> 00:34:54,690
again the problem it is that your

00:34:53,129 --> 00:34:56,250
application has to be aware of this land

00:34:54,690 --> 00:34:58,440
if you make changes you have to change

00:34:56,250 --> 00:35:00,270
the application and so forth you can do

00:34:58,440 --> 00:35:02,820
it in in through the connectors for

00:35:00,270 --> 00:35:07,320
example using MySQL fabric you can do it

00:35:02,820 --> 00:35:11,730
through the proxy or there are storage

00:35:07,320 --> 00:35:16,020
engines that allow charting as well or

00:35:11,730 --> 00:35:18,960
have built-in Charlie for example ND b

00:35:16,020 --> 00:35:20,640
which is mysql cluster has built-in

00:35:18,960 --> 00:35:22,170
sharding that automatically shot in the

00:35:20,640 --> 00:35:24,720
storage engine called scaly b which is

00:35:22,170 --> 00:35:26,220
built in charting and then there's a

00:35:24,720 --> 00:35:31,890
sojourning called spider which basically

00:35:26,220 --> 00:35:35,070
is is charting I'm going to talk a bit

00:35:31,890 --> 00:35:40,589
about spider here when later on just

00:35:35,070 --> 00:35:42,119
because it's a so simple so well I don't

00:35:40,589 --> 00:35:44,460
have any advantages with sharding your

00:35:42,119 --> 00:35:48,599
own have disadvantages but the advantage

00:35:44,460 --> 00:35:49,830
is the advantage with charting is of

00:35:48,599 --> 00:35:52,500
course well you need to do it because

00:35:49,830 --> 00:35:53,910
you're you're you can't and reserve one

00:35:52,500 --> 00:35:57,780
machine or not enough right so you have

00:35:53,910 --> 00:35:59,130
to speed up somehow and by using

00:35:57,780 --> 00:36:00,750
multiple machines you can then do

00:35:59,130 --> 00:36:02,670
certain things faster the problem is

00:36:00,750 --> 00:36:04,830
that that because you split

00:36:02,670 --> 00:36:06,990
your data you will have an encrypt

00:36:04,830 --> 00:36:08,940
increased complexity of SQL well

00:36:06,990 --> 00:36:13,290
especially if you have key best charting

00:36:08,940 --> 00:36:17,520
right because if using key best charting

00:36:13,290 --> 00:36:19,920
this means that the speed or your

00:36:17,520 --> 00:36:21,270
performance or operate operations will

00:36:19,920 --> 00:36:25,290
be handled differently how do you do a

00:36:21,270 --> 00:36:28,230
full table scan from from a Charlotte

00:36:25,290 --> 00:36:29,490
table how do you do a normal index scan

00:36:28,230 --> 00:36:31,730
from Charlotte table and things like

00:36:29,490 --> 00:36:33,810
that everything becomes more complex and

00:36:31,730 --> 00:36:35,310
depending on where your shard you might

00:36:33,810 --> 00:36:36,540
not be able to do it in one state when

00:36:35,310 --> 00:36:38,580
you might have to do multiple statements

00:36:36,540 --> 00:36:41,460
to get the same result as you would

00:36:38,580 --> 00:36:43,500
earlier get with one statement depending

00:36:41,460 --> 00:36:45,030
on where your shard it's obviously more

00:36:43,500 --> 00:36:48,900
complex to manage because you have

00:36:45,030 --> 00:36:50,250
multiple pieces so so for example if you

00:36:48,900 --> 00:36:51,780
have one server you need some a chair

00:36:50,250 --> 00:36:54,270
you sit up an H a solution for that

00:36:51,780 --> 00:36:56,790
server well if you have ten you any one

00:36:54,270 --> 00:37:00,810
of these 10 pieces failing and your your

00:36:56,790 --> 00:37:03,210
your clusters out so you have to set

00:37:00,810 --> 00:37:06,000
that up so failure failure becomes more

00:37:03,210 --> 00:37:08,250
complex what if one piece fails and it

00:37:06,000 --> 00:37:10,170
fails overshoot the others be available

00:37:08,250 --> 00:37:12,090
during the failover should they not then

00:37:10,170 --> 00:37:14,250
things of that so there's more things to

00:37:12,090 --> 00:37:17,310
think about for for that I'm the same

00:37:14,250 --> 00:37:20,310
with backups again you have 10 machines

00:37:17,310 --> 00:37:23,520
you take a backup well how do you

00:37:20,310 --> 00:37:25,170
synchronize the back up taking a backup

00:37:23,520 --> 00:37:27,120
from 11 minus glamour you be sure is

00:37:25,170 --> 00:37:28,500
easy there's lots of tools for taking

00:37:27,120 --> 00:37:30,690
one back up and having it synchronized

00:37:28,500 --> 00:37:32,280
but doing it across 10 is not so easy

00:37:30,690 --> 00:37:34,710
anymore because how do you synchronize

00:37:32,280 --> 00:37:36,330
the query so that they're consistent

00:37:34,710 --> 00:37:39,510
across all up in charge so you have to

00:37:36,330 --> 00:37:41,430
think about that as well so there's all

00:37:39,510 --> 00:37:43,590
of disadvantages which is charting but

00:37:41,430 --> 00:37:48,360
but if you have to do you have to do it

00:37:43,590 --> 00:37:49,830
right so I already mentioned this

00:37:48,360 --> 00:37:52,740
basically you can shot on different

00:37:49,830 --> 00:37:55,050
different levels so if you shot on the

00:37:52,740 --> 00:37:56,700
application or on the or in the

00:37:55,050 --> 00:37:59,130
connector it basically means that the

00:37:56,700 --> 00:38:00,960
application will directly connect to the

00:37:59,130 --> 00:38:04,260
different machines or diff in charge and

00:38:00,960 --> 00:38:08,310
here the application has to be aware of

00:38:04,260 --> 00:38:14,020
how you shot or something and if you do

00:38:08,310 --> 00:38:16,150
it on a proxy level then

00:38:14,020 --> 00:38:17,650
well then the applications have to be

00:38:16,150 --> 00:38:21,570
aware of the sharding which makes a lot

00:38:17,650 --> 00:38:25,270
easier for the application layer and

00:38:21,570 --> 00:38:26,800
then you can do it through a storage

00:38:25,270 --> 00:38:29,050
engine which basically means well you

00:38:26,800 --> 00:38:31,000
don't even need a proxy in that case

00:38:29,050 --> 00:38:32,530
because because you connect to a

00:38:31,000 --> 00:38:34,630
database server and the shortening is

00:38:32,530 --> 00:38:37,690
happening underneath the hood in the

00:38:34,630 --> 00:38:42,850
storage layer or in the or in some other

00:38:37,690 --> 00:38:43,930
layer so as an example of shoddy i'm

00:38:42,850 --> 00:38:45,880
going to talk i'm going to talk a bit

00:38:43,930 --> 00:38:48,580
about spider which is uh has anyone

00:38:45,880 --> 00:38:53,370
heard about spider is it him i use

00:38:48,580 --> 00:38:57,190
spider now okay so spider is is is a

00:38:53,370 --> 00:38:59,860
storage engine in murrieta v it so it

00:38:57,190 --> 00:39:03,640
looks like it stores data but it doesn't

00:38:59,860 --> 00:39:05,230
actually store data it's actually the

00:39:03,640 --> 00:39:09,280
basic principle of spiders spiders

00:39:05,230 --> 00:39:13,260
extremely simple basically you create a

00:39:09,280 --> 00:39:16,360
spider table with a partitioning scheme

00:39:13,260 --> 00:39:19,870
who has used partitioning in mysql

00:39:16,360 --> 00:39:22,660
amarilla baby okay half so basically in

00:39:19,870 --> 00:39:25,030
mysql DB you can say partition by and

00:39:22,660 --> 00:39:28,270
you have you can choose list

00:39:25,030 --> 00:39:29,740
partitioning so we define range we

00:39:28,270 --> 00:39:31,360
define a list of values and they will go

00:39:29,740 --> 00:39:32,980
into one and another list of values that

00:39:31,360 --> 00:39:34,510
will go into second you can arrange

00:39:32,980 --> 00:39:38,320
partitioning where you have ranges of

00:39:34,510 --> 00:39:40,960
values for example dates you can put all

00:39:38,320 --> 00:39:42,490
the rows for with the specific date

00:39:40,960 --> 00:39:45,160
range to go into one partition and

00:39:42,490 --> 00:39:47,110
another date range and so forth you find

00:39:45,160 --> 00:39:48,910
ranges and then you have hash

00:39:47,110 --> 00:39:52,540
partitioning and key partitioning which

00:39:48,910 --> 00:39:54,160
is basically saying that well you don't

00:39:52,540 --> 00:39:57,820
really care how its divided as long as

00:39:54,160 --> 00:39:59,920
it's divided right so what spider does

00:39:57,820 --> 00:40:02,980
is it is basically uses your

00:39:59,920 --> 00:40:04,870
partitioning scheme but instead of with

00:40:02,980 --> 00:40:06,820
Noah partitioning basically you

00:40:04,870 --> 00:40:08,920
partition your table but all the tables

00:40:06,820 --> 00:40:11,290
are still locally but spider does is

00:40:08,920 --> 00:40:14,140
that it actually instead of having

00:40:11,290 --> 00:40:17,200
storing the data locally it creates

00:40:14,140 --> 00:40:19,030
remote connections to other more unity

00:40:17,200 --> 00:40:22,240
servers so you don't actually store

00:40:19,030 --> 00:40:26,050
anything locally you store it on remote

00:40:22,240 --> 00:40:29,070
servers so basically the starting for

00:40:26,050 --> 00:40:29,070
you in the

00:40:30,630 --> 00:40:36,130
so that's kind of the really basic thing

00:40:33,550 --> 00:40:37,990
idea behind spider but it becomes more

00:40:36,130 --> 00:40:40,000
complex because of course it has to

00:40:37,990 --> 00:40:44,290
handle transactions now this is easy

00:40:40,000 --> 00:40:45,460
because we support X a transaction so we

00:40:44,290 --> 00:40:47,290
can have distributed transactions

00:40:45,460 --> 00:40:49,240
through XA and that's also how spider

00:40:47,290 --> 00:40:52,360
does it so when you have a transaction

00:40:49,240 --> 00:40:54,940
into its interaction transaction on all

00:40:52,360 --> 00:40:56,140
of the nodes introduction that spans all

00:40:54,940 --> 00:40:57,460
the nodes and when the deduction is done

00:40:56,140 --> 00:41:00,460
it could be committed on all the nodes

00:40:57,460 --> 00:41:02,260
and we have XA transactions that

00:41:00,460 --> 00:41:10,930
supports supports this so it's kind of

00:41:02,260 --> 00:41:15,130
built in 222 Murray dbm mysql so if you

00:41:10,930 --> 00:41:18,070
look at a spider table basically no data

00:41:15,130 --> 00:41:21,850
is stored locally instead is stored on

00:41:18,070 --> 00:41:25,120
the shards and you can because the

00:41:21,850 --> 00:41:26,770
shards our money DB servers this means

00:41:25,120 --> 00:41:32,920
that you can access the data through the

00:41:26,770 --> 00:41:34,690
spider server or directly through to

00:41:32,920 --> 00:41:40,000
these remote servers or Charlotte

00:41:34,690 --> 00:41:42,040
servers and this is also easy for this

00:41:40,000 --> 00:41:43,900
is this is also a good example for

00:41:42,040 --> 00:41:48,030
highlighting kind of the potential

00:41:43,900 --> 00:41:48,030
problems with with key best shouting so

00:41:48,390 --> 00:41:53,020
typically you have one or two tables

00:41:51,520 --> 00:41:54,790
that are huge you need to shard and the

00:41:53,020 --> 00:41:59,350
other tables might be smaller so you can

00:41:54,790 --> 00:42:01,270
actually maintain them as non spider

00:41:59,350 --> 00:42:05,410
tables right so the small tables are on

00:42:01,270 --> 00:42:07,300
the spider server right now you want to

00:42:05,410 --> 00:42:08,710
do a key-value look up you want you have

00:42:07,300 --> 00:42:10,960
a you have a primary key value and you

00:42:08,710 --> 00:42:12,760
want to get it from this table this you

00:42:10,960 --> 00:42:14,590
know you go to the spider node despite

00:42:12,760 --> 00:42:16,720
or node through the partitioning knows

00:42:14,590 --> 00:42:19,990
where to get it from and you get it it's

00:42:16,720 --> 00:42:21,970
fast it's quick it will typically be a

00:42:19,990 --> 00:42:24,820
lot faster than having one machine with

00:42:21,970 --> 00:42:26,860
all the data because on the load on the

00:42:24,820 --> 00:42:28,270
smaller machines on the shards they

00:42:26,860 --> 00:42:29,950
everything will fit in memories there

00:42:28,270 --> 00:42:32,320
will be a faster operation right so key

00:42:29,950 --> 00:42:35,860
value lookups are great for Kiki was

00:42:32,320 --> 00:42:38,230
charting now the problem is what if you

00:42:35,860 --> 00:42:40,330
have a where clause that spans a range

00:42:38,230 --> 00:42:42,400
of values

00:42:40,330 --> 00:42:45,190
and that's where key best shouting is

00:42:42,400 --> 00:42:47,380
not so great anymore because because

00:42:45,190 --> 00:42:49,660
there is no way of knowing which shard

00:42:47,380 --> 00:42:51,640
will have the data you basically have to

00:42:49,660 --> 00:42:54,150
send this query to all of the Sharks and

00:42:51,640 --> 00:42:56,920
all the shots send their data back and

00:42:54,150 --> 00:42:58,810
then you will you create your result set

00:42:56,920 --> 00:43:04,630
and you give it back okay that's still

00:42:58,810 --> 00:43:08,470
okay but what about joints so you want

00:43:04,630 --> 00:43:11,740
to join your your shot at table with an

00:43:08,470 --> 00:43:14,530
uncharted table now this is where we

00:43:11,740 --> 00:43:17,980
have we have a problem so traditionally

00:43:14,530 --> 00:43:20,440
in in mysql and really be there used to

00:43:17,980 --> 00:43:25,470
be only one way to do do joints called

00:43:20,440 --> 00:43:28,270
nested loop joints which is basically

00:43:25,470 --> 00:43:30,790
nested for loops right so you take the

00:43:28,270 --> 00:43:32,770
other table start with the first row

00:43:30,790 --> 00:43:34,660
then you go the next table next stable

00:43:32,770 --> 00:43:37,330
and then you go back and do it again so

00:43:34,660 --> 00:43:39,610
you go through like a for loop like like

00:43:37,330 --> 00:43:44,100
nested for loops finding the rows for a

00:43:39,610 --> 00:43:46,990
John and this works fine if you're

00:43:44,100 --> 00:43:48,790
everything is in local memory but if

00:43:46,990 --> 00:43:51,040
this is if one of the table to spider

00:43:48,790 --> 00:43:54,190
table every time you fetch a row you

00:43:51,040 --> 00:43:55,630
have to go out on a network right so if

00:43:54,190 --> 00:43:59,680
you do a nested loop join where you have

00:43:55,630 --> 00:44:01,840
to get a row like 500 * you basically

00:43:59,680 --> 00:44:03,970
have to go under network 500 times and

00:44:01,840 --> 00:44:05,890
then you know a million times you

00:44:03,970 --> 00:44:09,580
basically kill your query because

00:44:05,890 --> 00:44:13,660
because of latency right so doing joins

00:44:09,580 --> 00:44:16,930
uh with Charlotte tables is not great

00:44:13,660 --> 00:44:19,300
this is you know many of the new SQL

00:44:16,930 --> 00:44:22,000
solutions they shot everything but you

00:44:19,300 --> 00:44:23,200
typically can't do joins and this is the

00:44:22,000 --> 00:44:24,400
reason because it's actually a hard

00:44:23,200 --> 00:44:30,730
problem you can't really there's no good

00:44:24,400 --> 00:44:33,280
solution for you and spider actually has

00:44:30,730 --> 00:44:36,160
I mean we have added some optimizations

00:44:33,280 --> 00:44:37,750
in it we have a batch key joins you can

00:44:36,160 --> 00:44:40,150
do bache ki access instead of doing one

00:44:37,750 --> 00:44:41,830
row at a time you do a range of rows at

00:44:40,150 --> 00:44:43,360
a time which makes the bit faster so you

00:44:41,830 --> 00:44:44,800
don't have to take every single row at

00:44:43,360 --> 00:44:48,100
one at a time but you take a bunch of

00:44:44,800 --> 00:44:49,780
rows but spider itself actually offers

00:44:48,100 --> 00:44:53,890
functionalities for getting around this

00:44:49,780 --> 00:44:55,510
so for example if you have

00:44:53,890 --> 00:44:57,310
a smaller table that you want to join

00:44:55,510 --> 00:44:59,890
with and you also store the same small

00:44:57,310 --> 00:45:02,680
temple on every single shard spider has

00:44:59,890 --> 00:45:05,230
allows you to basically create the joint

00:45:02,680 --> 00:45:08,320
do the joins locally and send the result

00:45:05,230 --> 00:45:11,230
sets so basically the joint operation is

00:45:08,320 --> 00:45:13,420
done under on the shards instead of on

00:45:11,230 --> 00:45:16,210
the spider node and that way you only

00:45:13,420 --> 00:45:17,890
have to do one network up the operation

00:45:16,210 --> 00:45:20,080
is done the operation is actually spread

00:45:17,890 --> 00:45:21,820
out everyone does the joint operation

00:45:20,080 --> 00:45:25,450
and then the only thing despite a node

00:45:21,820 --> 00:45:29,050
does is it gathers together the result

00:45:25,450 --> 00:45:31,750
set so that's why even joints can can be

00:45:29,050 --> 00:45:33,520
actually quite fast spider but this is

00:45:31,750 --> 00:45:35,350
not something that you have to use

00:45:33,520 --> 00:45:38,880
specific functions for this it's not

00:45:35,350 --> 00:45:41,860
built in so if you just use the normal

00:45:38,880 --> 00:45:44,680
Murray DB I mean if you just write a

00:45:41,860 --> 00:45:48,400
query the optimizer is not aware of of

00:45:44,680 --> 00:45:49,960
of the optimizer is not spider aware so

00:45:48,400 --> 00:45:51,610
it won't be able to do this for you it

00:45:49,960 --> 00:45:53,140
doesn't know that you might have local

00:45:51,610 --> 00:45:55,510
tables on the other nodes and so forth

00:45:53,140 --> 00:45:58,870
but spider has built-in functionality

00:45:55,510 --> 00:46:03,400
for it all right some other cool things

00:45:58,870 --> 00:46:05,290
with spider is that so the link table

00:46:03,400 --> 00:46:07,060
can't have any engine right so the

00:46:05,290 --> 00:46:10,960
remote tables so you create your shards

00:46:07,060 --> 00:46:13,420
and the spider table is user spider as

00:46:10,960 --> 00:46:14,650
the engine but on the shards you can

00:46:13,420 --> 00:46:15,970
actually use any engine and you can

00:46:14,650 --> 00:46:18,160
actually use different engine so you can

00:46:15,970 --> 00:46:19,720
have in your debut for some and miles

00:46:18,160 --> 00:46:22,360
and presume okay I don't you know there

00:46:19,720 --> 00:46:24,520
might be a good reason for that you can

00:46:22,360 --> 00:46:28,180
also use partitioning on remote mode

00:46:24,520 --> 00:46:29,680
tables and remote the remote servers are

00:46:28,180 --> 00:46:32,130
not spider where they don't they don't

00:46:29,680 --> 00:46:34,750
actually know that they're just a shard

00:46:32,130 --> 00:46:39,360
it they're just a server and they will

00:46:34,750 --> 00:46:41,950
happen to be accessed by by spider and

00:46:39,360 --> 00:46:44,920
you can have multiple spider notes for

00:46:41,950 --> 00:46:47,440
the same table so so if you look at this

00:46:44,920 --> 00:46:50,290
this image here I could basically have

00:46:47,440 --> 00:46:52,270
another spider note here it just has to

00:46:50,290 --> 00:46:55,830
have the same definition of the tables

00:46:52,270 --> 00:46:58,990
connected to the same notes so basically

00:46:55,830 --> 00:47:00,490
well the main use case is available if

00:46:58,990 --> 00:47:01,660
the spider knows go down suddenly you

00:47:00,490 --> 00:47:03,850
don't can't access anything but if you

00:47:01,660 --> 00:47:04,960
have two of them well it doesn't matter

00:47:03,850 --> 00:47:07,350
which way you go because they don't

00:47:04,960 --> 00:47:09,000
actually store much data

00:47:07,350 --> 00:47:10,410
didn't store anything for the spider

00:47:09,000 --> 00:47:13,260
tables they might start something for

00:47:10,410 --> 00:47:21,270
other tables if you have none not none

00:47:13,260 --> 00:47:25,500
sharted tables so here's an example of

00:47:21,270 --> 00:47:27,420
how you do it so on the spider node you

00:47:25,500 --> 00:47:34,590
create a table with engine spider and

00:47:27,420 --> 00:47:38,790
then you basically define how to connect

00:47:34,590 --> 00:47:40,170
it to the periodic tables in here my

00:47:38,790 --> 00:47:42,210
example is not that great actually

00:47:40,170 --> 00:47:44,520
because I only have one I don't have one

00:47:42,210 --> 00:47:46,170
node here I should have a connection to

00:47:44,520 --> 00:47:48,570
multiple nodes so I must have missed

00:47:46,170 --> 00:47:55,740
something I put this together but basic

00:47:48,570 --> 00:47:57,840
you would have a yeah well actually okay

00:47:55,740 --> 00:48:00,690
okay have two examples sorry I confused

00:47:57,840 --> 00:48:04,100
myself I have two example here this is a

00:48:00,690 --> 00:48:06,360
much better example so here I have a

00:48:04,100 --> 00:48:08,760
spider example with chartings here I

00:48:06,360 --> 00:48:10,380
create a table on the spider node i have

00:48:08,760 --> 00:48:13,470
a partitioning scheme and then for each

00:48:10,380 --> 00:48:15,480
partition i say okay where is this shard

00:48:13,470 --> 00:48:17,460
located because it won't it won't below

00:48:15,480 --> 00:48:21,990
call so basically for each partition it

00:48:17,460 --> 00:48:23,400
gives which server to connect right so

00:48:21,990 --> 00:48:27,540
here we have a table with three

00:48:23,400 --> 00:48:28,800
partitions and then we created this

00:48:27,540 --> 00:48:30,540
fight or node we obviously have to

00:48:28,800 --> 00:48:33,870
create we have to create the tables on

00:48:30,540 --> 00:48:36,210
all of the shard nodes and then we can

00:48:33,870 --> 00:48:40,590
insert or select queries from the spider

00:48:36,210 --> 00:48:42,180
node and well here I insert the three

00:48:40,590 --> 00:48:43,710
nodes and then if you look on each know

00:48:42,180 --> 00:48:47,130
you will nobody will see that each node

00:48:43,710 --> 00:48:54,030
will only have one of the rows on this

00:48:47,130 --> 00:48:57,720
one is table there so a few words about

00:48:54,030 --> 00:49:02,220
performance with spider so reading it

00:48:57,720 --> 00:49:03,900
generally faster except for joints where

00:49:02,220 --> 00:49:05,280
it can be slower because of of the

00:49:03,900 --> 00:49:08,010
problems i just mentioned and right

00:49:05,280 --> 00:49:09,570
right thing which is typically one of

00:49:08,010 --> 00:49:12,420
the main reasons for using spider is

00:49:09,570 --> 00:49:14,130
insert speed is generally a lot faster

00:49:12,420 --> 00:49:16,800
because you only insert to one of the

00:49:14,130 --> 00:49:19,170
sharks so you can do lots of insert in

00:49:16,800 --> 00:49:21,390
parallel right because if you have 10

00:49:19,170 --> 00:49:23,220
nodes well you can write

00:49:21,390 --> 00:49:25,740
10 shards you can write on all 10

00:49:23,220 --> 00:49:29,400
machines at the same time so you kind of

00:49:25,740 --> 00:49:30,950
gain gain huge increase in right speed

00:49:29,400 --> 00:49:32,880
that's typically the main reason

00:49:30,950 --> 00:49:42,240
typically the main reason for using

00:49:32,880 --> 00:49:46,980
spider is this and here's just a summary

00:49:42,240 --> 00:49:49,620
of of what I went through here so

00:49:46,980 --> 00:49:52,260
replication is good for read scaling and

00:49:49,620 --> 00:49:55,140
gallery is mainly for read scaling but

00:49:52,260 --> 00:49:56,820
gives a che as well and and also the

00:49:55,140 --> 00:50:00,090
fact that the nodes are synchronized

00:49:56,820 --> 00:50:03,060
which means that you don't worry about

00:50:00,090 --> 00:50:06,870
slay like and then sharding which which

00:50:03,060 --> 00:50:08,730
I would say should only be used if you

00:50:06,870 --> 00:50:10,380
really have to but but depending on on

00:50:08,730 --> 00:50:12,560
how you use it so I mean obviously I

00:50:10,380 --> 00:50:14,490
mentioned spider here because because

00:50:12,560 --> 00:50:19,110
well because it's easy and I actually

00:50:14,490 --> 00:50:21,060
like spider but but also because uh you

00:50:19,110 --> 00:50:23,190
know it's a it's an interesting example

00:50:21,060 --> 00:50:25,380
because it's under on the database

00:50:23,190 --> 00:50:28,230
server but you can there are there are

00:50:25,380 --> 00:50:30,960
charting proxies out there way that goes

00:50:28,230 --> 00:50:34,590
in front of your database servers as I

00:50:30,960 --> 00:50:36,120
said so so like like like Mexico can do

00:50:34,590 --> 00:50:39,660
schema base charting but you can get key

00:50:36,120 --> 00:50:43,280
best sharting as well there's like a

00:50:39,660 --> 00:50:50,700
scale basin and some others as well

00:50:43,280 --> 00:50:57,270
alright any questions on this innovation

00:50:50,700 --> 00:50:58,770
sharing when you're on the Mac scale ok

00:50:57,270 --> 00:51:02,250
the question I have there so when you're

00:50:58,770 --> 00:51:03,720
scaling that out can you does it

00:51:02,250 --> 00:51:05,130
automatically take care of all those

00:51:03,720 --> 00:51:07,200
different shards as is pushing it down

00:51:05,130 --> 00:51:10,080
like let's say if I'm doing a right to a

00:51:07,200 --> 00:51:11,550
table that's coming down through it's

00:51:10,080 --> 00:51:14,280
going through my charts and then it

00:51:11,550 --> 00:51:16,680
let's say I have one of those notes goes

00:51:14,280 --> 00:51:18,510
down there how long does it like this is

00:51:16,680 --> 00:51:21,270
going to propagate back up I guess my

00:51:18,510 --> 00:51:23,910
question and like to say hey there's a

00:51:21,270 --> 00:51:25,680
problem here yeah so like I'm just

00:51:23,910 --> 00:51:28,410
interested on that part of that the

00:51:25,680 --> 00:51:32,310
proxy software ok so you see I said oh I

00:51:28,410 --> 00:51:33,810
mean max scale because it it's aware of

00:51:32,310 --> 00:51:36,140
it depends on what's you so typically

00:51:33,810 --> 00:51:39,420
when you do the shouting like this

00:51:36,140 --> 00:51:42,420
you have to have some kind of a che for

00:51:39,420 --> 00:51:44,910
each shot right I'm max scale of course

00:51:42,420 --> 00:51:47,130
is aware of gutter and replications if

00:51:44,910 --> 00:51:50,210
using galera or standard replication for

00:51:47,130 --> 00:51:52,530
its shard there is no issue because it's

00:51:50,210 --> 00:51:54,480
same way we saw a good eye works with if

00:51:52,530 --> 00:51:56,130
you only have one shard like you only

00:51:54,480 --> 00:51:57,420
have one rep master slave or you only

00:51:56,130 --> 00:51:59,310
have one color of class to max that

00:51:57,420 --> 00:52:01,109
works so the skimmer is charting its

00:51:59,310 --> 00:52:02,849
basic the same thing except you have

00:52:01,109 --> 00:52:05,910
multiple of these so you can monitor all

00:52:02,849 --> 00:52:08,940
of the machines if the master fails to

00:52:05,910 --> 00:52:12,150
start using the the new master and so

00:52:08,940 --> 00:52:13,829
forth so basically it would work out of

00:52:12,150 --> 00:52:16,349
the box but you have to say it but but

00:52:13,829 --> 00:52:18,060
it doesn't create h.j for you so you

00:52:16,349 --> 00:52:19,410
have to still set up day so you have to

00:52:18,060 --> 00:52:20,970
make sure that you are using galera for

00:52:19,410 --> 00:52:27,380
its char dorm over whelming whatever you

00:52:20,970 --> 00:52:27,380
are using so yeah yeah

00:52:30,960 --> 00:52:34,600
alright so the question was how does max

00:52:32,980 --> 00:52:37,630
Keller where schemas are so basically

00:52:34,600 --> 00:52:39,850
you define that so so max Geller you

00:52:37,630 --> 00:52:42,390
basically define how to out the shorter

00:52:39,850 --> 00:52:42,390
different schemas

00:52:57,100 --> 00:53:01,470
right so so if in scheme is shouting the

00:52:59,410 --> 00:53:04,600
question is how do you deal with with

00:53:01,470 --> 00:53:06,760
like if you have different loads for

00:53:04,600 --> 00:53:09,280
different schemas so I mean that's kind

00:53:06,760 --> 00:53:11,410
of where schema based shouting doesn't

00:53:09,280 --> 00:53:13,030
work so well anymore is when you have if

00:53:11,410 --> 00:53:16,690
you have one scheme or one table that's

00:53:13,030 --> 00:53:18,460
that's getting all the traffic so I mean

00:53:16,690 --> 00:53:20,290
your example for example if you have

00:53:18,460 --> 00:53:21,970
like one schema per customer some

00:53:20,290 --> 00:53:24,430
customers have much more traffic than

00:53:21,970 --> 00:53:27,250
others then you can put multiple schemas

00:53:24,430 --> 00:53:29,770
in the same cluster and then separate

00:53:27,250 --> 00:53:31,600
out the one that has the most traffic

00:53:29,770 --> 00:53:33,070
and that's its own class cluster and

00:53:31,600 --> 00:53:34,720
then you have ten on the other question

00:53:33,070 --> 00:53:37,570
or something so that would be a way to

00:53:34,720 --> 00:53:39,220
get around it but it might not work it

00:53:37,570 --> 00:53:41,490
depends on how different the loads are

00:53:39,220 --> 00:53:41,490
right

00:53:57,710 --> 00:54:03,950
I'm not sure Daniel dunno give can you

00:54:02,300 --> 00:54:05,630
send me the question by email and i'll

00:54:03,950 --> 00:54:08,180
i'll make sure to get an answer because

00:54:05,630 --> 00:54:13,520
i'm not sure actually i actually haven't

00:54:08,180 --> 00:54:16,490
used the schema by sharing myself I I

00:54:13,520 --> 00:54:17,660
think running out of time so so feel

00:54:16,490 --> 00:54:19,250
free to send me if you have any

00:54:17,660 --> 00:54:22,780
questions well grab me after these are

00:54:19,250 --> 00:54:22,780
sent me neva thanks

00:54:30,380 --> 00:54:34,430
you

00:54:32,369 --> 00:54:34,430
you

00:55:13,610 --> 00:55:17,940
your customers rely on your website or

00:55:16,440 --> 00:55:20,310
application if it's slower

00:55:17,940 --> 00:55:23,250
non-responsive it infuriates your users

00:55:20,310 --> 00:55:25,050
and costs you money keeping your

00:55:23,250 --> 00:55:28,590
business critical systems humming along

00:55:25,050 --> 00:55:30,870
requires insight into what they're doing

00:55:28,590 --> 00:55:32,850
your system metrics tells stories

00:55:30,870 --> 00:55:34,860
stories that can reveal performance

00:55:32,850 --> 00:55:37,170
bottlenecks resource limitations and

00:55:34,860 --> 00:55:38,760
other problems but how do you keep an

00:55:37,170 --> 00:55:41,400
eye on all of your systems performance

00:55:38,760 --> 00:55:44,430
metrics in real-time and record this

00:55:41,400 --> 00:55:46,410
data 4-liter analysis enter long view

00:55:44,430 --> 00:55:48,180
the new way to see what's really going

00:55:46,410 --> 00:55:50,340
on under the hood the long view

00:55:48,180 --> 00:55:52,410
dashboard lets you visualize the status

00:55:50,340 --> 00:55:54,870
of all your systems providing you with a

00:55:52,410 --> 00:55:58,260
bird's-eye view of your entire fleet you

00:55:54,870 --> 00:56:00,840
can sort by cpu memory swap processes

00:55:58,260 --> 00:56:02,640
load and network usage click a specific

00:56:00,840 --> 00:56:04,980
system to access its individual

00:56:02,640 --> 00:56:07,520
dashboard then click and drag to zoom in

00:56:04,980 --> 00:56:09,900
on chokepoints and get more detail

00:56:07,520 --> 00:56:11,580
comprehensive network data including

00:56:09,900 --> 00:56:13,890
inbound and outbound traffic is

00:56:11,580 --> 00:56:15,480
available on the network tab and disk

00:56:13,890 --> 00:56:17,370
rights and free space on the disk

00:56:15,480 --> 00:56:20,220
stabbed while the process Explorer

00:56:17,370 --> 00:56:22,740
displays usage statistics for individual

00:56:20,220 --> 00:56:24,960
processes the system info tab shows

00:56:22,740 --> 00:56:27,150
listening services active connections

00:56:24,960 --> 00:56:29,310
and available updates adding long view

00:56:27,150 --> 00:56:30,990
to a system is easy just click the

00:56:29,310 --> 00:56:33,150
button copy the one line installation

00:56:30,990 --> 00:56:35,580
command then run the command on your

00:56:33,150 --> 00:56:37,290
linux system to complete the process the

00:56:35,580 --> 00:56:39,450
agent will begin collecting data and

00:56:37,290 --> 00:56:42,990
sending it to longview then the graph

00:56:39,450 --> 00:56:45,240
start rolling use long view to gain

00:56:42,990 --> 00:56:50,000
visibility into your servers so when

00:56:45,240 --> 00:56:50,000
your website or app heats up it stays up

00:58:07,310 --> 00:58:11,630
Citrix XenServer gives you everything

00:58:09,440 --> 00:58:14,630
you need to integrate manage and

00:58:11,630 --> 00:58:16,760
automate a virtual data center all on an

00:58:14,630 --> 00:58:19,160
enterprise-class cloud proven virtual

00:58:16,760 --> 00:58:21,890
platform and at a third of the cost of

00:58:19,160 --> 00:58:23,600
other solutions but why even bother with

00:58:21,890 --> 00:58:25,760
virtualizing your server infrastructure

00:58:23,600 --> 00:58:27,740
in the first place well let's say you

00:58:25,760 --> 00:58:30,020
have a traditional one server to one

00:58:27,740 --> 00:58:32,030
application architecture but you're

00:58:30,020 --> 00:58:34,580
running out of resources and performance

00:58:32,030 --> 00:58:36,910
is suffering once you order new server

00:58:34,580 --> 00:58:39,800
hardware you'll wait for delivery

00:58:36,910 --> 00:58:42,680
configure it install your business

00:58:39,800 --> 00:58:45,440
application stage and test the server

00:58:42,680 --> 00:58:47,660
and finally add it to your production

00:58:45,440 --> 00:58:49,820
farm if you've been through this process

00:58:47,660 --> 00:58:52,370
before you know it can take weeks or

00:58:49,820 --> 00:58:54,200
even months you also know it's a

00:58:52,370 --> 00:58:56,150
manually intensive process that will

00:58:54,200 --> 00:58:59,480
burden your team every time you outgrow

00:58:56,150 --> 00:59:01,250
your current setup with a virtual server

00:58:59,480 --> 00:59:04,040
solution you could accomplish all of

00:59:01,250 --> 00:59:06,620
that in less than half a day server

00:59:04,040 --> 00:59:08,420
virtualization software separates the OS

00:59:06,620 --> 00:59:10,850
and application from the underlying

00:59:08,420 --> 00:59:13,010
server hardware and with multiple

00:59:10,850 --> 00:59:14,720
virtual machines on a single server you

00:59:13,010 --> 00:59:17,690
can use each of them to run different

00:59:14,720 --> 00:59:19,490
os's and applications this makes it

00:59:17,690 --> 00:59:21,260
possible to move your virtual machines

00:59:19,490 --> 00:59:23,330
from one piece of hardware to another

00:59:21,260 --> 00:59:25,610
whenever you want to maximize

00:59:23,330 --> 00:59:27,560
utilization simplify maintenance or

00:59:25,610 --> 00:59:29,870
recover from a hardware failure and

00:59:27,560 --> 00:59:32,810
without slowing down your applications

00:59:29,870 --> 00:59:35,870
or users clearly server virtualization

00:59:32,810 --> 00:59:38,870
provides big benefits and Citrix

00:59:35,870 --> 00:59:41,120
XenServer provides even more since it's

00:59:38,870 --> 00:59:42,950
built on an open platform xenserver

00:59:41,120 --> 00:59:45,260
plays well with your existing hardware

00:59:42,950 --> 00:59:47,540
storage systems and IT management

00:59:45,260 --> 00:59:50,270
software as well as with the industry's

00:59:47,540 --> 00:59:52,310
leading cloud service providers best of

00:59:50,270 --> 00:59:54,020
all you can get started by downloading a

00:59:52,310 --> 00:59:57,290
fully functional production-ready

00:59:54,020 --> 00:59:59,510
version of xenserver for free after a

00:59:57,290 --> 01:00:01,610
10-minute installation process you'll

00:59:59,510 --> 01:00:03,650
see how easy it is to start virtualizing

01:00:01,610 --> 01:00:06,080
your workloads and automating your IT

01:00:03,650 --> 01:00:07,700
management processes and when you're

01:00:06,080 --> 01:00:09,590
ready for a richer set of management

01:00:07,700 --> 01:00:12,110
tools just upgrade to one of the premium

01:00:09,590 --> 01:00:13,940
editions of xenserver so whether you're

01:00:12,110 --> 01:00:15,860
interested in virtualizing servers for

01:00:13,940 --> 01:00:18,170
the first time expanding your server

01:00:15,860 --> 01:00:20,690
virtualization footprint or moving

01:00:18,170 --> 01:00:21,270
server workloads to the cloud download

01:00:20,690 --> 01:00:23,580
and install

01:00:21,270 --> 01:00:26,610
xenserver today and see how it can help

01:00:23,580 --> 01:00:31,970
you simplify your IT environment citrix

01:00:26,610 --> 01:00:31,970
xenserver do more don't spend more

01:01:12,910 --> 01:01:14,970

YouTube URL: https://www.youtube.com/watch?v=jxeilvav7uA


