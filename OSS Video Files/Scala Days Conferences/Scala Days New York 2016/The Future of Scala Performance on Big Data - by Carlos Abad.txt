Title: The Future of Scala Performance on Big Data - by Carlos Abad
Publication date: 2016-06-17
Playlist: Scala Days New York 2016
Description: 
	This talk was recorded at Scala Days New York, 2016. Follow along on Twitter @scaladays and on the website for more information http://scaladays.org/.

Abstract:
Scala is the fastest expanding language in the datacenter, big data, and cloud computing. However the lack of standardized Big Data Scala benchmarks has hindered extended performance improvements related to the datacenter. In this talk we will present a newly developed Scala Big Data Benchmark based on learnings from TCPx-BB, SparkBench, and genomic (GATK/ADAM) workloads. We'll also introduce the most impacting big data-related performance improvements for Scala developed by our group.
Captions: 
	00:00:05,050 --> 00:00:14,870
so the first question many people ask me

00:00:08,179 --> 00:00:16,700
is Intel the scholar Intel scholar they

00:00:14,870 --> 00:00:18,800
don't they don't get it and no one gets

00:00:16,700 --> 00:00:21,830
it guess that it's actually because

00:00:18,800 --> 00:00:24,320
Intel is they see interest a hyper

00:00:21,830 --> 00:00:26,780
company but Intel is not happy company

00:00:24,320 --> 00:00:29,780
anymore Intel does not do only CPUs

00:00:26,780 --> 00:00:31,930
anymore we got beyond that like 10 or 15

00:00:29,780 --> 00:00:35,539
years ago although we have not

00:00:31,930 --> 00:00:40,460
advertised that so much so can you ask

00:00:35,539 --> 00:00:43,520
you me all right cool so so Intel does

00:00:40,460 --> 00:00:45,800
things like SSDs network accelerators

00:00:43,520 --> 00:00:49,210
and for the longest time have been

00:00:45,800 --> 00:00:52,880
trained to optimize languages and and

00:00:49,210 --> 00:00:54,710
all our systems to run faster on top of

00:00:52,880 --> 00:00:59,420
our hardware like jello have been ten

00:00:54,710 --> 00:01:03,230
years optimizing Java Python no Jas and

00:00:59,420 --> 00:01:06,950
now we last year we started working on a

00:01:03,230 --> 00:01:08,360
scholar and I have to say that all the

00:01:06,950 --> 00:01:09,950
work that we're going to see today is

00:01:08,360 --> 00:01:12,860
going to be open source through github

00:01:09,950 --> 00:01:14,210
so I don't think this is an effort to

00:01:12,860 --> 00:01:15,799
tell you anything we want to sell

00:01:14,210 --> 00:01:17,690
anything here we actually want to show

00:01:15,799 --> 00:01:19,760
the community what we have done and we

00:01:17,690 --> 00:01:22,610
want to give back we want and we would

00:01:19,760 --> 00:01:23,840
like your input as well so everything to

00:01:22,610 --> 00:01:26,570
do are going to see here is gonna be

00:01:23,840 --> 00:01:30,369
open source through github and the

00:01:26,570 --> 00:01:32,570
presentation obviously will be online so

00:01:30,369 --> 00:01:34,250
so what is the agenda for today with

00:01:32,570 --> 00:01:38,080
what are we going to see today so today

00:01:34,250 --> 00:01:40,280
we're going to see we're going to show a

00:01:38,080 --> 00:01:44,060
scallop bench where we have been working

00:01:40,280 --> 00:01:47,890
on based on Big Data workloads like Adam

00:01:44,060 --> 00:01:51,470
like a spark and we are also going to

00:01:47,890 --> 00:01:53,210
show us a case where we have optimized

00:01:51,470 --> 00:01:55,430
scholar of the libraries of a scholar

00:01:53,210 --> 00:01:58,490
actually to work faster we want whole

00:01:55,430 --> 00:02:00,820
benchmarks and also to work faster with

00:01:58,490 --> 00:02:03,110
one of those big data workloads and

00:02:00,820 --> 00:02:05,840
actually I'm going to show how you

00:02:03,110 --> 00:02:07,970
anyone's you can do it once we put it

00:02:05,840 --> 00:02:10,009
that this online a couple weeks you can

00:02:07,970 --> 00:02:11,980
go to github and extend it you can

00:02:10,009 --> 00:02:15,250
create new benchmarks micro benchmarks

00:02:11,980 --> 00:02:17,230
it will it will join the main threat and

00:02:15,250 --> 00:02:19,569
it will be

00:02:17,230 --> 00:02:21,670
families can be a we think that this is

00:02:19,569 --> 00:02:26,260
will be a very good starting point to

00:02:21,670 --> 00:02:29,200
create a benchmark suite in Scala that

00:02:26,260 --> 00:02:30,700
this is big data-related I think this we

00:02:29,200 --> 00:02:33,580
see that this talk will be interested

00:02:30,700 --> 00:02:35,440
for not only people who optimize the

00:02:33,580 --> 00:02:41,110
scholar or our language developers of

00:02:35,440 --> 00:02:42,959
escala but also for scalps developers

00:02:41,110 --> 00:02:46,720
big data app developers who uses scholar

00:02:42,959 --> 00:02:49,120
so you guys know if you're for example

00:02:46,720 --> 00:02:52,450
door and the features you want to use is

00:02:49,120 --> 00:02:53,980
actually a a bottleneck in Scala or it's

00:02:52,450 --> 00:02:55,959
actually in your program so that

00:02:53,980 --> 00:02:57,700
information can be useful when you are

00:02:55,959 --> 00:03:00,780
trying to optimize your servers or your

00:02:57,700 --> 00:03:03,970
service or micro service on your cluster

00:03:00,780 --> 00:03:05,950
so I am a performance engineer

00:03:03,970 --> 00:03:08,140
datacenter performance engineer and I

00:03:05,950 --> 00:03:10,660
think at Intel for five years and many

00:03:08,140 --> 00:03:16,120
people ask me why I'm not a manager yet

00:03:10,660 --> 00:03:19,030
I like my job so I'm how so in this time

00:03:16,120 --> 00:03:22,209
in the last year I we started looking at

00:03:19,030 --> 00:03:24,849
escala performance and the scala

00:03:22,209 --> 00:03:27,609
performance one of the few things that

00:03:24,849 --> 00:03:29,500
we we look at it was like where are the

00:03:27,609 --> 00:03:31,470
benchmarks like we have a scholar bench

00:03:29,500 --> 00:03:36,000
for example but the Scala bench is

00:03:31,470 --> 00:03:39,010
somehow out of the updated and the and

00:03:36,000 --> 00:03:43,090
they work notes are not ready to big

00:03:39,010 --> 00:03:46,870
data at all so optimizing force

00:03:43,090 --> 00:03:48,400
calamansi will not help truly optimizing

00:03:46,870 --> 00:03:52,299
for big data work looks like a spark

00:03:48,400 --> 00:03:55,560
like Adam like genomic and it's park

00:03:52,299 --> 00:03:57,849
bench for example running a sperm bank

00:03:55,560 --> 00:03:59,530
doesn't really expert village is focused

00:03:57,849 --> 00:04:02,079
on the spark is a it's a its eventual

00:03:59,530 --> 00:04:04,720
first part it will help optimizing for

00:04:02,079 --> 00:04:07,000
that it will help inspire directly but

00:04:04,720 --> 00:04:09,099
they would not help ask Allah so and

00:04:07,000 --> 00:04:11,500
then we found a bunch of micro

00:04:09,099 --> 00:04:13,299
benchmarks online but so with a lot of

00:04:11,500 --> 00:04:16,539
limitations they were not actually the

00:04:13,299 --> 00:04:18,789
scripts if I'm a big data workloads so

00:04:16,539 --> 00:04:21,240
the main problem when you the main

00:04:18,789 --> 00:04:23,680
problem when you want to run your own

00:04:21,240 --> 00:04:28,120
benchmark or mega benchmark in a scholar

00:04:23,680 --> 00:04:30,190
or big data benchmark is do you need two

00:04:28,120 --> 00:04:30,700
things the first thing you need full

00:04:30,190 --> 00:04:33,130
configure

00:04:30,700 --> 00:04:36,190
for a passion apache spark installation

00:04:33,130 --> 00:04:38,380
and the second most important is your

00:04:36,190 --> 00:04:41,620
own cluster so this is really really

00:04:38,380 --> 00:04:44,710
expensive so we thought what about if we

00:04:41,620 --> 00:04:46,930
extract those from using those big data

00:04:44,710 --> 00:04:49,030
workloads we extract that in those

00:04:46,930 --> 00:04:51,100
bottlenecks those pain points and we

00:04:49,030 --> 00:04:55,000
create a light way escala benchmark

00:04:51,100 --> 00:04:59,280
sweet sweet and that way it's much

00:04:55,000 --> 00:05:03,340
easier to use this benjamin street to

00:04:59,280 --> 00:05:05,140
get profiling data from that bottleneck

00:05:03,340 --> 00:05:08,020
or where know we are optimizing the

00:05:05,140 --> 00:05:10,150
bottleneck than to run an entire stack

00:05:08,020 --> 00:05:15,460
and to run an entire workload that will

00:05:10,150 --> 00:05:21,460
take hours so this way you will not need

00:05:15,460 --> 00:05:23,320
an entire cluster to accelerate to get

00:05:21,460 --> 00:05:26,830
performance information or profiling

00:05:23,320 --> 00:05:30,370
information from a big data a bottleneck

00:05:26,830 --> 00:05:32,380
bug in it so you will not need a class

00:05:30,370 --> 00:05:34,960
there anymore or an entire spark

00:05:32,380 --> 00:05:37,570
installation we have extracted all that

00:05:34,960 --> 00:05:40,930
information and convinced that all a

00:05:37,570 --> 00:05:43,420
benchmark suite and the best are sweet

00:05:40,930 --> 00:05:45,220
the code itself looks a bit strange but

00:05:43,420 --> 00:05:48,460
that's because we have tried to get the

00:05:45,220 --> 00:05:50,530
data patterns the algorithms exactly and

00:05:48,460 --> 00:05:53,230
the usage of the office scala libraries

00:05:50,530 --> 00:05:54,520
as they are from big data so this is the

00:05:53,230 --> 00:05:55,840
minimal expression that you need to

00:05:54,520 --> 00:05:57,070
actually express the same bottlenecks

00:05:55,840 --> 00:06:03,310
and you can run this in your own machine

00:05:57,070 --> 00:06:07,090
and moreover it has things like so this

00:06:03,310 --> 00:06:08,860
allow will allow you to optimize way to

00:06:07,090 --> 00:06:11,590
run this benchmark in minutes instead of

00:06:08,860 --> 00:06:13,240
hours like before so this is the main

00:06:11,590 --> 00:06:16,630
difference and this is actually a great

00:06:13,240 --> 00:06:20,080
advantage compared with before it also

00:06:16,630 --> 00:06:21,430
has features like it will give you throw

00:06:20,080 --> 00:06:24,250
put information garbage collection

00:06:21,430 --> 00:06:27,220
information memory memory location track

00:06:24,250 --> 00:06:29,830
synchronizations did the just-in-time

00:06:27,220 --> 00:06:33,160
compiler dr just-in-time compiler JVMs

00:06:29,830 --> 00:06:34,930
holistics all-in-one benchmark suite

00:06:33,160 --> 00:06:38,620
that these are actually easily

00:06:34,930 --> 00:06:43,000
expandable so this is actually how we

00:06:38,620 --> 00:06:43,960
created all or benchmarks as part of the

00:06:43,000 --> 00:06:47,080
street so

00:06:43,960 --> 00:06:49,240
so the first thing that we did was to

00:06:47,080 --> 00:06:51,100
run big little workloads in a spark

00:06:49,240 --> 00:06:54,520
cluster that it has been between eighty

00:06:51,100 --> 00:06:57,550
and ninety percent optimized and the way

00:06:54,520 --> 00:07:03,099
we did this is that we my team at Intel

00:06:57,550 --> 00:07:04,810
we focus on optimizing data centers and

00:07:03,099 --> 00:07:06,639
we have all that experience ten years of

00:07:04,810 --> 00:07:08,199
experience of dimension data centers and

00:07:06,639 --> 00:07:10,120
we created a very optimized at the

00:07:08,199 --> 00:07:12,430
center eighty or ninety percent of fully

00:07:10,120 --> 00:07:14,110
optimized why 80 or 90 not hundred

00:07:12,430 --> 00:07:18,639
mainly because depending on the workload

00:07:14,110 --> 00:07:20,889
your optimum point moves right so we

00:07:18,639 --> 00:07:22,630
grabbed a midpoint at where all our big

00:07:20,889 --> 00:07:25,949
data workloads are pretty much optimized

00:07:22,630 --> 00:07:29,289
ninety percent or eighty percent and

00:07:25,949 --> 00:07:31,270
this is the first part right when we are

00:07:29,289 --> 00:07:32,620
trying to do any performance work we

00:07:31,270 --> 00:07:34,900
need to eliminate all the bottlenecks

00:07:32,620 --> 00:07:37,720
because of a spark itself because of the

00:07:34,900 --> 00:07:41,500
JVM they did compiler the garbage

00:07:37,720 --> 00:07:44,590
collection the Linux kernel SSDs network

00:07:41,500 --> 00:07:47,500
connection server configuration all that

00:07:44,590 --> 00:07:49,360
stuff fully optimized so modern acts are

00:07:47,500 --> 00:07:50,979
not because of that but the next has to

00:07:49,360 --> 00:07:53,530
go because purely because of the SCADA

00:07:50,979 --> 00:07:56,259
or peel or ask our libraries or the

00:07:53,530 --> 00:07:58,150
Scala compiler so all those places where

00:07:56,259 --> 00:08:01,080
the photonics could be are off the table

00:07:58,150 --> 00:08:04,060
right away and we ran on Big Data

00:08:01,080 --> 00:08:08,500
workload in this case big bench for

00:08:04,060 --> 00:08:11,680
example TCP X big bench and we extract a

00:08:08,500 --> 00:08:14,650
bunch of profiling data from running

00:08:11,680 --> 00:08:16,659
that in on workload in or cluster for

00:08:14,650 --> 00:08:18,490
several hours we started profiling

00:08:16,659 --> 00:08:20,139
information and from that profile using

00:08:18,490 --> 00:08:22,960
the profile information we actually

00:08:20,139 --> 00:08:25,120
create a benchmark and that benchmark

00:08:22,960 --> 00:08:27,820
will run four minutes so we had a

00:08:25,120 --> 00:08:30,190
cluster running for day sometimes days

00:08:27,820 --> 00:08:32,440
we extract that profiling information we

00:08:30,190 --> 00:08:34,539
create this benchmark that you only need

00:08:32,440 --> 00:08:37,149
to run we only need to run four minutes

00:08:34,539 --> 00:08:40,120
to obtain the same bottlenecks and we

00:08:37,149 --> 00:08:42,039
include that in your suite and we

00:08:40,120 --> 00:08:46,540
actually repeated this process over and

00:08:42,039 --> 00:08:48,339
over to extract more benchmarks for we

00:08:46,540 --> 00:08:52,480
usually have one benchmark for each

00:08:48,339 --> 00:08:54,630
bottleneck that we found and four will

00:08:52,480 --> 00:08:57,769
be this process as well for each

00:08:54,630 --> 00:09:00,309
workload for example gatk

00:08:57,769 --> 00:09:04,579
is a dynamic and acid tool kicked

00:09:00,309 --> 00:09:06,589
developed by MIT a spark bench as well

00:09:04,579 --> 00:09:10,939
we run we obtain a bunch of information

00:09:06,589 --> 00:09:14,299
from there and Adam genomics as well so

00:09:10,939 --> 00:09:18,470
we found our both an extra nada genomics

00:09:14,299 --> 00:09:21,319
and these are some of the bottlenecks we

00:09:18,470 --> 00:09:26,149
found like for example the middle

00:09:21,319 --> 00:09:30,069
hashmap the middle vault reset the array

00:09:26,149 --> 00:09:31,759
buffer we found them to be some kind of

00:09:30,069 --> 00:09:34,279
bottleneck in certain situations

00:09:31,759 --> 00:09:35,929
especially for the data patterns that we

00:09:34,279 --> 00:09:38,480
are seeing in bit data that's it that's

00:09:35,929 --> 00:09:41,449
also the main problem when we write a

00:09:38,480 --> 00:09:42,920
micro benchmark any well performance

00:09:41,449 --> 00:09:45,920
Nene right the mega venture or anyone

00:09:42,920 --> 00:09:48,949
write your own micro benchmark is the

00:09:45,920 --> 00:09:53,269
input data you're using related to the

00:09:48,949 --> 00:09:56,029
big data or not it's really really one

00:09:53,269 --> 00:09:58,459
input way to genomic big data analysis

00:09:56,029 --> 00:10:00,529
or not and this is exactly how we found

00:09:58,459 --> 00:10:03,110
this model next because with the right

00:10:00,529 --> 00:10:06,290
input you can boil this out and we're

00:10:03,110 --> 00:10:08,179
going to show an example later in the

00:10:06,290 --> 00:10:10,899
next slide in this is like this is this

00:10:08,179 --> 00:10:14,779
how it works for each one of these

00:10:10,899 --> 00:10:19,040
benchmarks we grab our driver that we

00:10:14,779 --> 00:10:22,459
develop grabs the h1 and branch it am i

00:10:19,040 --> 00:10:24,230
running it it actually it runs in

00:10:22,459 --> 00:10:26,480
minutes instead of hours and it also

00:10:24,230 --> 00:10:28,189
allows time-based profiling let me

00:10:26,480 --> 00:10:30,939
explain time-based profiling a little

00:10:28,189 --> 00:10:33,829
bit more so when we run any benchmarks

00:10:30,939 --> 00:10:36,290
usually we say oh I want to run this out

00:10:33,829 --> 00:10:38,420
no do this array update operation for

00:10:36,290 --> 00:10:40,459
like ten thousand times right let me

00:10:38,420 --> 00:10:42,139
tell you something in Scala you are

00:10:40,459 --> 00:10:45,379
running on top of a JVM the Java Virtual

00:10:42,139 --> 00:10:47,389
Machine and a just-in-time compiler it

00:10:45,379 --> 00:10:51,189
takes time to for those thing to warm up

00:10:47,389 --> 00:10:55,339
at the same with with the cash they ask

00:10:51,189 --> 00:10:57,559
click caches and and your CPU cache is

00:10:55,339 --> 00:10:59,899
all the system takes time to warm up it

00:10:57,559 --> 00:11:02,509
does is not enough with with running

00:10:59,899 --> 00:11:04,819
10,000 times array update for the

00:11:02,509 --> 00:11:08,720
function array update for example for

00:11:04,819 --> 00:11:10,930
10,000 times because that can be 1.1

00:11:08,720 --> 00:11:13,210
microseconds in your cluster and

00:11:10,930 --> 00:11:15,910
dress that cannot be warm enough meaning

00:11:13,210 --> 00:11:17,950
more you will see more compilations in

00:11:15,910 --> 00:11:19,660
the future done by your just-in-time

00:11:17,950 --> 00:11:22,210
compiler you will see more garbage

00:11:19,660 --> 00:11:23,950
collection getting getting done that

00:11:22,210 --> 00:11:26,080
useful but there will be a point in time

00:11:23,950 --> 00:11:28,540
when all those things will establish and

00:11:26,080 --> 00:11:30,490
it will be a stable point and that's

00:11:28,540 --> 00:11:34,899
where we get all our measurements that's

00:11:30,490 --> 00:11:37,839
why this benchmark suite has a warm-up

00:11:34,899 --> 00:11:39,670
sequence embedded in it so you don't

00:11:37,839 --> 00:11:42,010
need to worry about that anymore you are

00:11:39,670 --> 00:11:44,500
going to see how this workload will

00:11:42,010 --> 00:11:45,670
perform in your trash their low run then

00:11:44,500 --> 00:11:47,320
you can set the main thing right when

00:11:45,670 --> 00:11:49,209
you do you're running you're in your

00:11:47,320 --> 00:11:51,430
laptop and you put it on your cluster

00:11:49,209 --> 00:11:54,580
it's not gonna work the same why because

00:11:51,430 --> 00:11:58,510
because you say when you see in your in

00:11:54,580 --> 00:12:01,920
your pc or your order that laptop or

00:11:58,510 --> 00:12:05,230
desktop is known as stable work

00:12:01,920 --> 00:12:07,570
performance reducing your cluster

00:12:05,230 --> 00:12:09,220
long-running workloads long-running

00:12:07,570 --> 00:12:12,010
performance and this is exactly what

00:12:09,220 --> 00:12:14,370
this benchmark suite will give to

00:12:12,010 --> 00:12:18,700
provide to you that type of data and

00:12:14,370 --> 00:12:22,300
also it it does dynamic input scaling so

00:12:18,700 --> 00:12:26,020
it will it will dynamically right change

00:12:22,300 --> 00:12:28,029
the size of the input so do you know how

00:12:26,020 --> 00:12:29,709
is the scalability of the original you

00:12:28,029 --> 00:12:36,610
have been developing or this calorie

00:12:29,709 --> 00:12:38,440
deficit I'm a scholar feature and well

00:12:36,610 --> 00:12:40,209
we also get more information like thread

00:12:38,440 --> 00:12:41,550
synchronization call stack as i said

00:12:40,209 --> 00:12:44,920
before memory education statistics

00:12:41,550 --> 00:12:46,600
throat put and java file recorder you

00:12:44,920 --> 00:12:48,040
don't even need to worry about how it

00:12:46,600 --> 00:12:49,779
works you just get the data that will

00:12:48,040 --> 00:12:54,100
click and boom a bunch of information

00:12:49,779 --> 00:12:56,850
appears it also allows us to compare

00:12:54,100 --> 00:13:00,760
different Java and Scala versions and

00:12:56,850 --> 00:13:03,490
that means that sometimes you it's very

00:13:00,760 --> 00:13:05,500
hard to know which java version shall I

00:13:03,490 --> 00:13:07,329
use with switches calibration shall I

00:13:05,500 --> 00:13:09,579
use because believe me using the latest

00:13:07,329 --> 00:13:11,140
and greatest calibration generation can

00:13:09,579 --> 00:13:13,089
be counterproductive in terms of

00:13:11,140 --> 00:13:15,640
performance sometimes depending on what

00:13:13,089 --> 00:13:19,750
you are doing depending what feature you

00:13:15,640 --> 00:13:22,720
are using and this is one of example

00:13:19,750 --> 00:13:24,830
output it also gives besides it's

00:13:22,720 --> 00:13:26,360
basically right now it's a very poor

00:13:24,830 --> 00:13:28,040
Excel spreadsheet with a bunch of data

00:13:26,360 --> 00:13:33,200
we're still working on that this is like

00:13:28,040 --> 00:13:35,780
the beta release also a flight recorder

00:13:33,200 --> 00:13:39,410
a file which contains most of the data

00:13:35,780 --> 00:13:40,970
and it allows as I said before it allows

00:13:39,410 --> 00:13:43,550
to get the throat with four different

00:13:40,970 --> 00:13:48,500
sizes scaling the sizes for you no need

00:13:43,550 --> 00:13:50,600
to touch anything it also will perform

00:13:48,500 --> 00:13:52,790
different permutations of different Java

00:13:50,600 --> 00:13:54,680
versions and differing scala versions so

00:13:52,790 --> 00:13:56,780
do you know exactly how this feature

00:13:54,680 --> 00:13:58,820
will perform with different JVMs and

00:13:56,780 --> 00:14:01,880
different JVM configurations like for

00:13:58,820 --> 00:14:06,770
example hip size and different Excalibur

00:14:01,880 --> 00:14:07,730
shoes and the best thing is that the

00:14:06,770 --> 00:14:11,840
only thing you need to do is to click a

00:14:07,730 --> 00:14:13,580
button and wait 30 minutes and this is

00:14:11,840 --> 00:14:18,530
one example what we have successfully

00:14:13,580 --> 00:14:20,330
use this technique to actually to

00:14:18,530 --> 00:14:24,320
actually improve something in Scala

00:14:20,330 --> 00:14:26,660
finally so the thinner we we have we

00:14:24,320 --> 00:14:30,560
have improved and we have not yet

00:14:26,660 --> 00:14:33,440
released the code is the middle hashmap

00:14:30,560 --> 00:14:35,570
we found that with a certain pattern the

00:14:33,440 --> 00:14:37,280
middle hashmap in Scala is very

00:14:35,570 --> 00:14:39,620
interesting because it performs very

00:14:37,280 --> 00:14:42,260
well but under a certain part input

00:14:39,620 --> 00:14:44,600
pattern when we are inputting data into

00:14:42,260 --> 00:14:46,880
the hashmap a under a certain input

00:14:44,600 --> 00:14:49,970
pattern this hash map actually becomes

00:14:46,880 --> 00:14:53,870
overloaded and its performance changes

00:14:49,970 --> 00:14:57,530
from Big O one too big o n basically a

00:14:53,870 --> 00:15:00,200
linked list performance and we found out

00:14:57,530 --> 00:15:02,780
that we need to update the the best way

00:15:00,200 --> 00:15:07,060
to go with this performance issue has to

00:15:02,780 --> 00:15:10,340
update the middle hashmap in using a

00:15:07,060 --> 00:15:13,190
opportunistic redback tree and basically

00:15:10,340 --> 00:15:16,100
what we do is that we use rabbit trix

00:15:13,190 --> 00:15:17,930
trees inside the hashmap

00:15:16,100 --> 00:15:23,900
opportunistically whenever we see a good

00:15:17,930 --> 00:15:25,550
situation to use them and and and that

00:15:23,900 --> 00:15:28,900
changed about the performance of the

00:15:25,550 --> 00:15:31,280
worst case scenario from om 20 log N and

00:15:28,900 --> 00:15:33,980
believe me folks this increases the

00:15:31,280 --> 00:15:36,680
performance of Alan genomics even though

00:15:33,980 --> 00:15:38,690
it doesn't look related but all the way

00:15:36,680 --> 00:15:40,010
these changes are going all

00:15:38,690 --> 00:15:42,230
way to a stack where the point where

00:15:40,010 --> 00:15:47,180
Alan genomics work faster thanks to this

00:15:42,230 --> 00:15:49,820
change this is example benchmark that I

00:15:47,180 --> 00:15:51,700
wrote this is an example to see how do

00:15:49,820 --> 00:15:58,820
you can create your own brains work

00:15:51,700 --> 00:16:02,420
easily and the only the only requirement

00:15:58,820 --> 00:16:04,460
is basically needing to extend this

00:16:02,420 --> 00:16:07,580
trait that we create we get a trade with

00:16:04,460 --> 00:16:10,670
two empty methods that need to be

00:16:07,580 --> 00:16:14,060
defined those two methods the warm-up

00:16:10,670 --> 00:16:17,000
and the run basically the warm-up is

00:16:14,060 --> 00:16:20,210
just too it's a warm-up sequence that

00:16:17,000 --> 00:16:22,580
the the driver will run and if we run a

00:16:20,210 --> 00:16:26,660
sequence that the driver will run with

00:16:22,580 --> 00:16:29,270
different sizes and why we did it this

00:16:26,660 --> 00:16:31,220
way is actually too high complexity so

00:16:29,270 --> 00:16:32,780
the driver is actually very complex and

00:16:31,220 --> 00:16:34,610
your benchmark can be as complex as you

00:16:32,780 --> 00:16:36,020
need but the interface is as simple as

00:16:34,610 --> 00:16:37,970
possible it's just two methods and

00:16:36,020 --> 00:16:40,700
that's the only thing that with actually

00:16:37,970 --> 00:16:42,920
one argument with one argument each so

00:16:40,700 --> 00:16:45,620
the driver only understands those two

00:16:42,920 --> 00:16:49,370
methods and if at all the complexities

00:16:45,620 --> 00:16:51,800
hidden from both sides so that's the

00:16:49,370 --> 00:16:53,710
beauty of it you can create your

00:16:51,800 --> 00:16:57,650
benchmark as complex as you want inside

00:16:53,710 --> 00:17:01,910
but you got you have you hide all that

00:16:57,650 --> 00:17:05,990
complexity away from the driver so this

00:17:01,910 --> 00:17:08,410
is just the conclusions we build a scale

00:17:05,990 --> 00:17:13,390
of interest rate based on Big Data

00:17:08,410 --> 00:17:17,810
workload and that allows us actually to

00:17:13,390 --> 00:17:20,810
simplify to simplify the optimizing

00:17:17,810 --> 00:17:24,470
escala because if i would have to run

00:17:20,810 --> 00:17:30,230
this benchmark over and over in my data

00:17:24,470 --> 00:17:33,650
frame main in my in my a data center

00:17:30,230 --> 00:17:35,930
that we had a lintel it will take hours

00:17:33,650 --> 00:17:38,450
days so everything I do I small change

00:17:35,930 --> 00:17:40,700
it takes me days that's grace if I get

00:17:38,450 --> 00:17:45,070
wanna get quick iterative development

00:17:40,700 --> 00:17:47,780
right so by using this performance

00:17:45,070 --> 00:17:51,340
benchmark we can do the same thing in

00:17:47,780 --> 00:17:54,820
minute and we want or

00:17:51,340 --> 00:17:57,340
or idea is to keep updating this this

00:17:54,820 --> 00:17:59,140
rental suite right now with more and

00:17:57,340 --> 00:18:02,860
more bad words as we see more and more

00:17:59,140 --> 00:18:06,220
bottlenecks in the future and and we

00:18:02,860 --> 00:18:09,990
actually would like to encourage the

00:18:06,220 --> 00:18:09,990
community you guys to actually

00:18:10,980 --> 00:18:15,790
contribute be part of the of the effort

00:18:14,680 --> 00:18:20,620
because I think this is that this is a

00:18:15,790 --> 00:18:23,830
game-changing effort nowadays it's a way

00:18:20,620 --> 00:18:26,260
that for the first time we have

00:18:23,830 --> 00:18:30,100
benchmarks Big Data benchmarks for a

00:18:26,260 --> 00:18:31,860
scholar and for the first time this is

00:18:30,100 --> 00:18:35,590
going to be a stable and maintain

00:18:31,860 --> 00:18:42,400
hopefully properly and it will empower

00:18:35,590 --> 00:18:44,620
the community so this is some links from

00:18:42,400 --> 00:18:47,500
the benchmarks at the big data workloads

00:18:44,620 --> 00:18:49,840
we we have used and this is my contact

00:18:47,500 --> 00:18:54,060
information and we thought without

00:18:49,840 --> 00:18:54,060
anything else thank you folks

00:19:18,930 --> 00:19:29,820
that's right so the work knows where we

00:19:21,390 --> 00:19:33,140
are so no we don't so I spike itself so

00:19:29,820 --> 00:19:37,590
right now all workers Runaways park and

00:19:33,140 --> 00:19:40,200
and but we do we have an entire stack

00:19:37,590 --> 00:19:42,270
right we have a JVM we have a linux

00:19:40,200 --> 00:19:44,460
kernel we have a JVM we have a cluster

00:19:42,270 --> 00:19:46,860
manager we have a spark manager we have

00:19:44,460 --> 00:19:49,980
we have a scholar with in the middle and

00:19:46,860 --> 00:19:54,660
I think I guys up to certain stan is

00:19:49,980 --> 00:19:57,480
part of a spark as well so that's the

00:19:54,660 --> 00:20:02,150
stud stack we use unless that answer

00:19:57,480 --> 00:20:02,150
your question or not okay

00:20:22,510 --> 00:20:27,640
so it's actually more than expanding a

00:20:25,660 --> 00:20:30,550
long time right so maybe the question

00:20:27,640 --> 00:20:33,160
could be rephrased as how did the AC

00:20:30,550 --> 00:20:35,950
termine what is actually need need to be

00:20:33,160 --> 00:20:37,330
though in a benchmark right and once you

00:20:35,950 --> 00:20:42,460
have the benchmark what do you do right

00:20:37,330 --> 00:20:43,990
with a benchmark right so um the way we

00:20:42,460 --> 00:20:47,020
determine official venture I said about

00:20:43,990 --> 00:20:49,240
how long it takes for how percentage of

00:20:47,020 --> 00:20:50,740
time we spend on that point that's

00:20:49,240 --> 00:20:52,860
actually one of the measurements but

00:20:50,740 --> 00:20:56,080
there are other measurements right like

00:20:52,860 --> 00:20:58,810
for example cola stacks eating if you

00:20:56,080 --> 00:21:00,610
don't even if a function even if a

00:20:58,810 --> 00:21:02,500
function testing expect even if you

00:21:00,610 --> 00:21:04,330
don't expect out of time in a function

00:21:02,500 --> 00:21:05,860
that doesn't mean that function stop

00:21:04,330 --> 00:21:08,830
blogging you because you made you might

00:21:05,860 --> 00:21:12,220
be waiting for the function to come back

00:21:08,830 --> 00:21:14,710
to you in a synchronous system unless it

00:21:12,220 --> 00:21:16,720
makes sense like you're waiting for a

00:21:14,710 --> 00:21:19,090
resource to get free dinner in a data

00:21:16,720 --> 00:21:21,070
center it's not a function is a natural

00:21:19,090 --> 00:21:23,770
resource it's like for example SSD hard

00:21:21,070 --> 00:21:25,600
drive that is that's not going to show

00:21:23,770 --> 00:21:27,520
up in your in your hot methods let's say

00:21:25,600 --> 00:21:31,330
in your in your profiling information

00:21:27,520 --> 00:21:33,070
per se right b-but you are actually blog

00:21:31,330 --> 00:21:35,650
in your system because of the system

00:21:33,070 --> 00:21:39,930
information linux kernel calls right

00:21:35,650 --> 00:21:44,950
they need to get optimized those we also

00:21:39,930 --> 00:21:47,880
see those things also a a CPU cache

00:21:44,950 --> 00:21:53,230
misses it's like it looks like very very

00:21:47,880 --> 00:21:57,430
low level but if if our CPU cache misses

00:21:53,230 --> 00:21:59,320
I know this for some people may sound

00:21:57,430 --> 00:22:03,610
like like black magic but we also look

00:21:59,320 --> 00:22:06,400
at CPU cache misses and five percent

00:22:03,610 --> 00:22:07,630
more cache misses in a cpu means twenty

00:22:06,400 --> 00:22:10,180
percent of your performance is gone

00:22:07,630 --> 00:22:13,660
that's because your access the way you

00:22:10,180 --> 00:22:15,970
access your data is the way you access

00:22:13,660 --> 00:22:17,680
your data your money your the cluster

00:22:15,970 --> 00:22:20,710
man I cannot keep up with you accessing

00:22:17,680 --> 00:22:21,670
your data all over your cluster so that

00:22:20,710 --> 00:22:24,640
type of thing for that type of

00:22:21,670 --> 00:22:27,430
information in first tells us where they

00:22:24,640 --> 00:22:30,330
they say hotspot is how to spot I don't

00:22:27,430 --> 00:22:32,770
mean a JVM I mean by a spot where we

00:22:30,330 --> 00:22:34,360
take out the resources time is one

00:22:32,770 --> 00:22:35,170
resource but there are other resources

00:22:34,360 --> 00:22:37,810
right

00:22:35,170 --> 00:22:45,100
so that that that answer the question

00:22:37,810 --> 00:22:48,490
and from there we extract benchmarks do

00:22:45,100 --> 00:22:51,850
we use tungsten I think the workloads we

00:22:48,490 --> 00:22:55,300
are using right now they partially you

00:22:51,850 --> 00:22:58,950
Stan Stan but I'm not really sure to

00:22:55,300 --> 00:23:02,020
what extent they I know let me think

00:22:58,950 --> 00:23:03,850
then tungsten itself right is power of

00:23:02,020 --> 00:23:06,130
the office park itself it's all the data

00:23:03,850 --> 00:23:13,020
switch like turn on that then turn on

00:23:06,130 --> 00:23:13,020
Thurston and well

00:23:32,960 --> 00:23:40,289
that's that is totally true in theory

00:23:37,169 --> 00:23:42,750
the in theory tungsten should help with

00:23:40,289 --> 00:23:45,210
cash wishes for example he actually cash

00:23:42,750 --> 00:23:47,160
machine cache misses was an example

00:23:45,210 --> 00:23:51,000
right it was not like the centric point

00:23:47,160 --> 00:23:54,390
was just an example but talking about

00:23:51,000 --> 00:23:56,340
tungsten I know it also depends on the

00:23:54,390 --> 00:23:58,410
workload I am Not sure and we need to

00:23:56,340 --> 00:24:01,289
come back to you maybe you can email me

00:23:58,410 --> 00:24:03,630
and I can come back to you if or

00:24:01,289 --> 00:24:05,309
bursting of a spark has Dunston and if

00:24:03,630 --> 00:24:07,710
the workers are we ran on top of a spark

00:24:05,309 --> 00:24:09,539
they have Dunston they use Dunston or

00:24:07,710 --> 00:24:11,549
not I believe some of them do some of

00:24:09,539 --> 00:24:13,260
them do not but i am not sure which one

00:24:11,549 --> 00:24:24,059
so i would need to actually check that

00:24:13,260 --> 00:24:28,320
and come back to you sorry say again so

00:24:24,059 --> 00:24:30,450
we are working where where the benchmark

00:24:28,320 --> 00:24:33,179
we are using some of the used data frame

00:24:30,450 --> 00:24:36,570
someone used some of them used our dd's

00:24:33,179 --> 00:24:38,520
so it's not it's not it's not it's not

00:24:36,570 --> 00:24:40,440
that we are benchmarking is parted

00:24:38,520 --> 00:24:43,140
surface and we are using workloads of

00:24:40,440 --> 00:24:44,850
workloads on top of a spark that they

00:24:43,140 --> 00:24:48,179
some of them use are these two buttons

00:24:44,850 --> 00:24:49,860
use data frames so it's not that we we

00:24:48,179 --> 00:24:51,990
we created those workloads is that the

00:24:49,860 --> 00:24:54,059
workloads already out there people are

00:24:51,990 --> 00:24:57,510
using them with that and we just grab

00:24:54,059 --> 00:24:59,909
them and run them so it's not that we're

00:24:57,510 --> 00:25:02,400
profiling artists only or data frames

00:24:59,909 --> 00:25:04,620
only or tungsten only in terms of inside

00:25:02,400 --> 00:25:06,840
the spark no no no is that we are at a

00:25:04,620 --> 00:25:08,549
higher level running workloads that are

00:25:06,840 --> 00:25:10,470
big data and those workloads sometimes

00:25:08,549 --> 00:25:13,230
they choose to use data frame sometimes

00:25:10,470 --> 00:25:16,289
they don't use they just use simply

00:25:13,230 --> 00:25:19,260
spark does it make sense and the unit

00:25:16,289 --> 00:25:20,909
and they use each each work look it may

00:25:19,260 --> 00:25:22,679
require different version of a spark so

00:25:20,909 --> 00:25:25,530
textin might not be available so that's

00:25:22,679 --> 00:25:30,679
why i'm in doubt after what the stand

00:25:25,530 --> 00:25:30,679
testing is enabled in those workloads

00:25:42,940 --> 00:25:51,790
so right now they yeah so the question

00:25:48,010 --> 00:25:53,380
was if is there any user interface to

00:25:51,790 --> 00:25:58,810
capture all this performance data and

00:25:53,380 --> 00:26:01,480
show it to a user so to be honest right

00:25:58,810 --> 00:26:03,550
now the user interface is quite poor we

00:26:01,480 --> 00:26:06,070
have worked more on the engine type of

00:26:03,550 --> 00:26:11,620
site and it's actually just generating

00:26:06,070 --> 00:26:13,930
an excel sheet and and a Java flight

00:26:11,620 --> 00:26:16,180
recorder file and the jal flight

00:26:13,930 --> 00:26:18,220
recorder provides a lot of information

00:26:16,180 --> 00:26:19,930
and you can actually just double click

00:26:18,220 --> 00:26:22,420
on that if you have Java installed in

00:26:19,930 --> 00:26:24,340
your in your PC or desktop or Mac or

00:26:22,420 --> 00:26:26,170
whatever I will actually open up the

00:26:24,340 --> 00:26:27,940
very nice interface of jellified

00:26:26,170 --> 00:26:29,440
recorder and that will give you like

00:26:27,940 --> 00:26:31,120
fifty percent of information and the

00:26:29,440 --> 00:26:35,200
other 50 / saying is an expression that

00:26:31,120 --> 00:26:36,850
is easily available readable so we are

00:26:35,200 --> 00:26:41,290
working on that but we have not put all

00:26:36,850 --> 00:26:47,280
our efforts on that that's true no

00:26:41,290 --> 00:26:47,280
question there should dance

00:27:16,509 --> 00:27:20,789
I'm not really sure I follow the

00:27:18,429 --> 00:27:20,789
question

00:27:36,790 --> 00:27:43,390
so yeah so I think now it makes sense

00:27:40,280 --> 00:27:45,320
thank you so I guess the question was

00:27:43,390 --> 00:27:47,090
since you are we're running such a

00:27:45,320 --> 00:27:49,430
variety of workloads how can we

00:27:47,090 --> 00:27:52,370
determine what is a hot spot because all

00:27:49,430 --> 00:27:54,110
the workers will average out well the

00:27:52,370 --> 00:27:56,270
way we get obtained or prevailing

00:27:54,110 --> 00:27:58,370
information it's actually a own

00:27:56,270 --> 00:28:04,280
individual basis and all individual

00:27:58,370 --> 00:28:06,320
cases so I believe it or not the hot

00:28:04,280 --> 00:28:08,840
spots and where they do not aware it out

00:28:06,320 --> 00:28:10,850
and we take each individual case and a

00:28:08,840 --> 00:28:12,470
sub slsc as a pain point or shibaura a

00:28:10,850 --> 00:28:15,880
benchmark I want to create a bench work

00:28:12,470 --> 00:28:19,990
so for example and we said that Adam

00:28:15,880 --> 00:28:24,410
Adam genomics under certain conditions

00:28:19,990 --> 00:28:28,480
uses the meat of a hashmap and we

00:28:24,410 --> 00:28:30,679
optimize that lesson doesn't mean that

00:28:28,480 --> 00:28:32,059
is that it said actually something that

00:28:30,679 --> 00:28:33,320
we are not going to see at all in other

00:28:32,059 --> 00:28:36,410
benchmarks that's true that's what it

00:28:33,320 --> 00:28:39,080
true but that's why that's one only one

00:28:36,410 --> 00:28:42,980
beta is in inside a huge suite right

00:28:39,080 --> 00:28:45,679
it's just one pain point that we want to

00:28:42,980 --> 00:28:49,100
highlight by doing that it's not it's

00:28:45,679 --> 00:28:50,750
not in all the beds water we have in the

00:28:49,100 --> 00:28:55,120
street it's only one bench bar inside

00:28:50,750 --> 00:28:59,210
the suite right so each benchmark is

00:28:55,120 --> 00:29:03,080
it's in charge the same church or it was

00:28:59,210 --> 00:29:07,400
created with highlighting highlighting

00:29:03,080 --> 00:29:08,960
one pain point in mind right so does it

00:29:07,400 --> 00:29:16,960
make sense listen that does not answer

00:29:08,960 --> 00:29:16,960
your question next

00:29:20,020 --> 00:29:30,570
I guess that's all well thank you so

00:29:23,590 --> 00:29:30,570

YouTube URL: https://www.youtube.com/watch?v=sKQ1NNVU3uA


