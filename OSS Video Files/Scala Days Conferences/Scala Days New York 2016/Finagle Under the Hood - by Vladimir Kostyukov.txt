Title: Finagle Under the Hood - by Vladimir Kostyukov
Publication date: 2016-06-17
Playlist: Scala Days New York 2016
Description: 
	This talk was recorded at Scala Days New York, 2016. Follow along on Twitter @scaladays and on the website for more information http://scaladays.org/.

Abstract:
Finagle is an extensible RPC system for JVM written in Scala that is used and developed at Twitter. It promotes a programming model where servers and clients might be viewed as functions, which are both easy to compose and reason about. Although, there is quite a complex logic (connection pooling, load balancing, circuit breaking and so on) hidden behind those functions. In this talk, we'll dig into Finagle internals and carefully walk through each of its modules to see what happens when we a) send a request to a Finagle client b) receive a request by a Finagle server.
Captions: 
	00:00:02,750 --> 00:00:11,309
okay hello everyone and welcome to this

00:00:08,309 --> 00:00:15,389
finagle talk my name is vladimir

00:00:11,309 --> 00:00:20,520
Korsakov and today i'm going to give you

00:00:15,389 --> 00:00:23,520
a brief overview of what finagle is but

00:00:20,520 --> 00:00:25,290
that's just an overview and I hope it

00:00:23,520 --> 00:00:27,600
should be enough for you to figure out

00:00:25,290 --> 00:00:30,450
if an angle is a good fit for your

00:00:27,600 --> 00:00:33,890
infrastructure if it solves that kind of

00:00:30,450 --> 00:00:39,300
problems you trying to do right now and

00:00:33,890 --> 00:00:43,350
yeah that's basically so who the hell am

00:00:39,300 --> 00:00:45,930
I so my name is Vladimir I said and I've

00:00:43,350 --> 00:00:48,900
been working on finagle for almost two

00:00:45,930 --> 00:00:51,090
years already and on the finagle team at

00:00:48,900 --> 00:00:53,640
tier and before that I was doing some

00:00:51,090 --> 00:00:57,170
kind of low-level stuff working with

00:00:53,640 --> 00:01:00,720
compilers and VMs at Intel and then

00:00:57,170 --> 00:01:02,190
something happened to me in 2013 and I

00:01:00,720 --> 00:01:05,939
started playing with functional

00:01:02,190 --> 00:01:08,069
programming in Scala and now I finally

00:01:05,939 --> 00:01:09,840
do that full time and I'm actually the

00:01:08,069 --> 00:01:12,270
person who's responsible for that boring

00:01:09,840 --> 00:01:14,700
tweets from Aetna gallant or so you

00:01:12,270 --> 00:01:18,240
should feel free to blame me for that

00:01:14,700 --> 00:01:22,619
also alright so um what he Neagle is

00:01:18,240 --> 00:01:24,389
anyway an Engel is an RPC system that is

00:01:22,619 --> 00:01:27,509
built and used in production at Twitter

00:01:24,389 --> 00:01:30,479
and many other companies outside here as

00:01:27,509 --> 00:01:33,869
well and it's written in Java it works

00:01:30,479 --> 00:01:35,639
on JVM and so it's written in scala and

00:01:33,869 --> 00:01:37,439
it works on JVM but mostly usable from

00:01:35,639 --> 00:01:40,409
Java as well and we have a bunch of

00:01:37,439 --> 00:01:43,049
services from teams internally that

00:01:40,409 --> 00:01:46,009
brand services in Java quite successful

00:01:43,049 --> 00:01:49,499
in finagle and um and we have this

00:01:46,009 --> 00:01:51,869
robobird logo for finagle so which it

00:01:49,499 --> 00:01:53,700
wasn't named after birth but we have to

00:01:51,869 --> 00:01:56,990
make things equal so we have this logo

00:01:53,700 --> 00:02:00,959
that looks like a bird but it's robot

00:01:56,990 --> 00:02:02,909
anyways when it comes to describing what

00:02:00,959 --> 00:02:05,849
finagle is I really like this Alexi too

00:02:02,909 --> 00:02:08,490
is from the most recent finagle con and

00:02:05,849 --> 00:02:10,890
it says the key problem is finagle

00:02:08,490 --> 00:02:12,720
adoption that has solves tons of problem

00:02:10,890 --> 00:02:14,340
you know nothing about until it's too

00:02:12,720 --> 00:02:15,630
late and this is really interesting

00:02:14,340 --> 00:02:17,910
because

00:02:15,630 --> 00:02:21,590
finagle actually solves tons of problems

00:02:17,910 --> 00:02:24,180
people don't know know nothing about and

00:02:21,590 --> 00:02:26,430
this is really really really exciting

00:02:24,180 --> 00:02:28,440
part about finagle that usually just

00:02:26,430 --> 00:02:30,210
works and you know nothing about what's

00:02:28,440 --> 00:02:31,620
happening underneath it just solved

00:02:30,210 --> 00:02:32,430
problem for you like every day every

00:02:31,620 --> 00:02:37,290
second

00:02:32,430 --> 00:02:39,120
that's what finagle does anyways what

00:02:37,290 --> 00:02:42,690
your Nagle is today is being around

00:02:39,120 --> 00:02:46,080
quite a long time since 2010 and this

00:02:42,690 --> 00:02:48,710
four point five thousand github stars

00:02:46,080 --> 00:02:51,570
it makes top 7s color project there and

00:02:48,710 --> 00:02:55,170
we've got fifteen protocols implemented

00:02:51,570 --> 00:02:57,570
already slightly more including our own

00:02:55,170 --> 00:03:01,440
protocol called max which stands for

00:02:57,570 --> 00:03:04,260
multiplexing and it's basically if you

00:03:01,440 --> 00:03:06,750
view that you can view that as a subset

00:03:04,260 --> 00:03:09,930
of HTTP two because it happened way

00:03:06,750 --> 00:03:12,390
before HTTP 2 and so it has control

00:03:09,930 --> 00:03:14,670
messages like low-level control messages

00:03:12,390 --> 00:03:16,620
for request cancellation section life is

00:03:14,670 --> 00:03:18,660
infection stuff like that that you can

00:03:16,620 --> 00:03:21,690
successfully utilize from the finagle

00:03:18,660 --> 00:03:26,550
side and use it quite quite efficiently

00:03:21,690 --> 00:03:28,350
and this is adopted quite well inside to

00:03:26,550 --> 00:03:31,770
your add a bunch of different companies

00:03:28,350 --> 00:03:36,090
outside the company who run services

00:03:31,770 --> 00:03:39,570
using thrift marks and http2 and 94

00:03:36,090 --> 00:03:41,430
migration is on track and moving really

00:03:39,570 --> 00:03:43,500
fast I know we've been promising that

00:03:41,430 --> 00:03:45,930
metaphor you talk here for about two

00:03:43,500 --> 00:03:47,790
years right now but we are finally

00:03:45,930 --> 00:03:52,320
getting there and this is really

00:03:47,790 --> 00:03:54,240
exciting and the last part is it's

00:03:52,320 --> 00:03:56,610
important to understand what finagle is

00:03:54,240 --> 00:03:58,230
in terms of it's not the application

00:03:56,610 --> 00:03:59,940
server it's not the application

00:03:58,230 --> 00:04:02,010
framework it doesn't answer your

00:03:59,940 --> 00:04:03,840
question how you how you would write

00:04:02,010 --> 00:04:06,270
your services it doesn't sort of

00:04:03,840 --> 00:04:08,160
question how you how you how you would

00:04:06,270 --> 00:04:10,740
communicate between them and this is

00:04:08,160 --> 00:04:12,240
really important because of that there

00:04:10,740 --> 00:04:13,560
are lots of questions finagle doesn't

00:04:12,240 --> 00:04:15,660
answer like for example how do you

00:04:13,560 --> 00:04:17,580
define as controller how they do chase

00:04:15,660 --> 00:04:20,760
and how they do log in and stuff like

00:04:17,580 --> 00:04:22,560
that and this is why it has so many like

00:04:20,760 --> 00:04:24,960
child projects build and tell of that

00:04:22,560 --> 00:04:28,580
and to mention if you specifically

00:04:24,960 --> 00:04:29,900
designed for HTTP is finit sure

00:04:28,580 --> 00:04:33,110
Finch and Twitter server that you can

00:04:29,900 --> 00:04:34,729
actually like pull up and build stuff

00:04:33,110 --> 00:04:36,250
like already it has everything you need

00:04:34,729 --> 00:04:39,319
for that

00:04:36,250 --> 00:04:41,680
so what finagle is that - we've got a

00:04:39,319 --> 00:04:44,090
team called core system libraries

00:04:41,680 --> 00:04:46,430
they're about telling your ears so far

00:04:44,090 --> 00:04:48,650
in the team and we maintain libraries

00:04:46,430 --> 00:04:50,689
that powers to distribute in

00:04:48,650 --> 00:04:53,120
infrastructure so we've got finagle

00:04:50,689 --> 00:04:55,419
utility or server Scrooge fenÃªtre as

00:04:53,120 --> 00:04:57,949
well a bunch of metrics libraries and

00:04:55,419 --> 00:05:00,199
we've got internal version of funiculars

00:04:57,949 --> 00:05:02,300
and coal rotation which means that we

00:05:00,199 --> 00:05:04,610
use to provide support for internal

00:05:02,300 --> 00:05:06,349
teams during this production issues and

00:05:04,610 --> 00:05:08,120
NATO and the last part is really

00:05:06,349 --> 00:05:10,580
interesting so we've got one repo in

00:05:08,120 --> 00:05:13,639
wanna build which means we always run

00:05:10,580 --> 00:05:15,289
finagle from source from master and what

00:05:13,639 --> 00:05:19,009
it means for you as an open source

00:05:15,289 --> 00:05:20,990
customers is that when we do an open

00:05:19,009 --> 00:05:22,699
source release we always roll out the

00:05:20,990 --> 00:05:24,400
code that have been stress tested in

00:05:22,699 --> 00:05:26,990
production for about several months and

00:05:24,400 --> 00:05:29,090
thousands of services and this is really

00:05:26,990 --> 00:05:34,789
a good deal I guess for for you as a

00:05:29,090 --> 00:05:37,250
customer and so what any goal is outside

00:05:34,789 --> 00:05:39,889
to err is really interesting as well

00:05:37,250 --> 00:05:42,199
because so many companies and teams

00:05:39,889 --> 00:05:44,889
trying to clone that functionality that

00:05:42,199 --> 00:05:48,469
we've built and this is a tweet from

00:05:44,889 --> 00:05:51,469
about a new fork written in C++ from

00:05:48,469 --> 00:05:54,520
from Facebook and I also aware of one

00:05:51,469 --> 00:05:56,900
clone from Dropbox written in rust and I

00:05:54,520 --> 00:05:58,460
personally don't know about Java eight

00:05:56,900 --> 00:06:01,069
clone but I wouldn't be surprised if

00:05:58,460 --> 00:06:03,830
that one exists already so people

00:06:01,069 --> 00:06:06,800
actually trying to copy that in 2016

00:06:03,830 --> 00:06:08,270
technology that was built in 2010 and I

00:06:06,800 --> 00:06:10,069
think it's pretty interesting because it

00:06:08,270 --> 00:06:13,849
means that we're still doing something

00:06:10,069 --> 00:06:17,509
right right because before we start

00:06:13,849 --> 00:06:19,159
talking about services and clients that

00:06:17,509 --> 00:06:23,000
kind of abstractions let's talk about

00:06:19,159 --> 00:06:25,039
the big picture up as well so if an

00:06:23,000 --> 00:06:28,639
englis design busy simple idea in mind

00:06:25,039 --> 00:06:30,289
that your server is a function and the

00:06:28,639 --> 00:06:32,569
function from requests to future

00:06:30,289 --> 00:06:35,419
response and the most so in a medium

00:06:32,569 --> 00:06:37,639
that you can you can talk to a server

00:06:35,419 --> 00:06:39,409
using that function by just calling it

00:06:37,639 --> 00:06:40,610
and you can define your server by

00:06:39,409 --> 00:06:43,400
implementing that

00:06:40,610 --> 00:06:45,680
and the interesting part here is that

00:06:43,400 --> 00:06:48,560
those react and web type Rams

00:06:45,680 --> 00:06:51,620
it means that the server is obstructed

00:06:48,560 --> 00:06:54,520
over the protocol is used by means that

00:06:51,620 --> 00:06:57,199
you can define most of the genetic

00:06:54,520 --> 00:06:59,539
functionality using just this

00:06:57,199 --> 00:07:01,129
abstraction like we try slow down and

00:06:59,539 --> 00:07:03,229
stuff like that and you can share it

00:07:01,129 --> 00:07:05,120
across different protocol and

00:07:03,229 --> 00:07:08,479
implementation and this is exactly what

00:07:05,120 --> 00:07:10,669
we do in neo and so this picture

00:07:08,479 --> 00:07:13,129
basically it's how finagle looks like

00:07:10,669 --> 00:07:15,849
internally so we've got this stack thing

00:07:13,129 --> 00:07:18,469
on the Left which is a finagle stack and

00:07:15,849 --> 00:07:20,960
maytee pipeline on the right if you know

00:07:18,469 --> 00:07:23,479
what Nettie's is are your library for

00:07:20,960 --> 00:07:24,740
JVM and we've got transferred in between

00:07:23,479 --> 00:07:27,349
that sort of like glues them together

00:07:24,740 --> 00:07:30,319
and it's really interesting because on

00:07:27,349 --> 00:07:32,569
the left side there is like a functional

00:07:30,319 --> 00:07:34,550
service-oriented abstraction which is

00:07:32,569 --> 00:07:36,979
built by composition and on the right

00:07:34,550 --> 00:07:39,830
side there is like event based low-level

00:07:36,979 --> 00:07:41,240
high-performance data pipeline and the

00:07:39,830 --> 00:07:45,020
transferor serve like glues them

00:07:41,240 --> 00:07:47,120
together yeah and in this talk will

00:07:45,020 --> 00:07:49,580
mostly be focusing on the left part on

00:07:47,120 --> 00:07:53,120
this tag part and the stack is basically

00:07:49,580 --> 00:07:55,370
a collection of genetic models that you

00:07:53,120 --> 00:07:57,199
can fold over and build your clients or

00:07:55,370 --> 00:07:59,120
servers basically you can fold over

00:07:57,199 --> 00:08:01,940
anything that is known how to compose

00:07:59,120 --> 00:08:03,589
and we know how to compose functions so

00:08:01,940 --> 00:08:06,229
we basically know how to compose those

00:08:03,589 --> 00:08:08,839
services we've seen before so in in fact

00:08:06,229 --> 00:08:11,000
what we do is we put those services

00:08:08,839 --> 00:08:13,129
inside this tag and then fold over it

00:08:11,000 --> 00:08:15,020
and we've got a client we can call or

00:08:13,129 --> 00:08:18,529
basically we can we can get a server

00:08:15,020 --> 00:08:20,659
that we can serve traffic through that's

00:08:18,529 --> 00:08:23,060
really interesting functionality and

00:08:20,659 --> 00:08:26,180
insights that every every single piece

00:08:23,060 --> 00:08:29,210
inside stack every single like genetic

00:08:26,180 --> 00:08:31,639
model sort of implements stand alarm

00:08:29,210 --> 00:08:33,620
like functionality for example retries

00:08:31,639 --> 00:08:35,419
is a standalone stack model for example

00:08:33,620 --> 00:08:38,779
load balancer is a standalone stack

00:08:35,419 --> 00:08:41,180
model and so on this form and um that

00:08:38,779 --> 00:08:42,800
picture is exactly how she looks like

00:08:41,180 --> 00:08:45,140
but clients are slightly complicated

00:08:42,800 --> 00:08:47,390
because they do so much work on top of

00:08:45,140 --> 00:08:49,010
just sending the request to make sure it

00:08:47,390 --> 00:08:52,520
will succeed in the least possible time

00:08:49,010 --> 00:08:54,770
so they actually form some kind of tree

00:08:52,520 --> 00:08:56,690
of stacks with two branching points

00:08:54,770 --> 00:08:58,960
so we've got law bouncer in the very

00:08:56,690 --> 00:09:01,340
first that distribute traffic across

00:08:58,960 --> 00:09:03,290
several notes where your software is

00:09:01,340 --> 00:09:05,320
deployed to and we've got connection

00:09:03,290 --> 00:09:07,820
pooling that maintains a pool of

00:09:05,320 --> 00:09:09,530
connections tags and each tag look

00:09:07,820 --> 00:09:11,270
exactly like we've seen on a previous

00:09:09,530 --> 00:09:15,320
slide that terminates with native I plan

00:09:11,270 --> 00:09:16,940
and that forms that tree of stacks forms

00:09:15,320 --> 00:09:19,640
a client sort of like slightly

00:09:16,940 --> 00:09:23,810
complicated picture but still still

00:09:19,640 --> 00:09:25,700
quite useful to understand and before we

00:09:23,810 --> 00:09:28,070
start talking about clients let's talk

00:09:25,700 --> 00:09:31,130
about what they share in common and

00:09:28,070 --> 00:09:32,540
missing an interesting topic on the

00:09:31,130 --> 00:09:35,000
problem is how you would configure

00:09:32,540 --> 00:09:36,910
clients and servers and finagle and

00:09:35,000 --> 00:09:39,440
we've been thinking about that a lot and

00:09:36,910 --> 00:09:41,510
try to solve the problem three different

00:09:39,440 --> 00:09:45,140
times and I hope the finally succeed

00:09:41,510 --> 00:09:47,030
here and so White's wiki is because the

00:09:45,140 --> 00:09:50,360
configuration API should be simple

00:09:47,030 --> 00:09:53,090
enough and easy to use as well because

00:09:50,360 --> 00:09:55,520
you want to be you want to sure that

00:09:53,090 --> 00:09:57,440
common things is easy to do but uncommon

00:09:55,520 --> 00:10:00,440
things as possible to do and what we

00:09:57,440 --> 00:10:02,540
have today works quite well so

00:10:00,440 --> 00:10:04,910
configuration is always cold and finagle

00:10:02,540 --> 00:10:07,220
um it's not current like flags not

00:10:04,910 --> 00:10:09,380
config files it means that it always

00:10:07,220 --> 00:10:12,110
type checks by your compiler and it

00:10:09,380 --> 00:10:14,030
autocompletes by your ID and it's really

00:10:12,110 --> 00:10:16,310
useful there is the conventional API

00:10:14,030 --> 00:10:18,650
pattern for finding an entry point into

00:10:16,310 --> 00:10:21,500
your API so basically you type something

00:10:18,650 --> 00:10:23,900
like HTTP dot client dot this and you

00:10:21,500 --> 00:10:26,360
see what ID suggests you what kind of

00:10:23,900 --> 00:10:28,370
params you can alright on that client or

00:10:26,360 --> 00:10:33,140
server and what you can do that

00:10:28,370 --> 00:10:35,060
basically and what we have to do is we

00:10:33,140 --> 00:10:37,670
try to provide slightly different api

00:10:35,060 --> 00:10:39,590
for configuring what's known like expert

00:10:37,670 --> 00:10:41,630
level params the thing is not really

00:10:39,590 --> 00:10:43,310
safe - all right today but we want to

00:10:41,630 --> 00:10:45,650
expose them because we want to

00:10:43,310 --> 00:10:47,990
experiment with them and there is a

00:10:45,650 --> 00:10:50,570
slightly different API for ease and it's

00:10:47,990 --> 00:10:52,700
not so user friendly it's slightly not

00:10:50,570 --> 00:10:55,010
so discoverable which means that when

00:10:52,700 --> 00:10:56,990
you use that you don't have you're not

00:10:55,010 --> 00:10:59,240
having a good time which is basically

00:10:56,990 --> 00:11:01,010
the kind of nice message we can send to

00:10:59,240 --> 00:11:02,300
user if you're not having a good time

00:11:01,010 --> 00:11:04,310
you're probably doing something wrong

00:11:02,300 --> 00:11:05,930
and dangerous and the new API is

00:11:04,310 --> 00:11:07,820
actually like one hard piece in Java

00:11:05,930 --> 00:11:08,410
compatible so you can totally use it

00:11:07,820 --> 00:11:11,170
from Java

00:11:08,410 --> 00:11:14,500
it's like the same kind of God all right

00:11:11,170 --> 00:11:16,750
finally I'm servers and finagle and

00:11:14,500 --> 00:11:19,029
service up very very simple and finagle

00:11:16,750 --> 00:11:21,459
they like really simple they optimized

00:11:19,029 --> 00:11:22,720
for high throughput and they don't do

00:11:21,459 --> 00:11:24,430
anything extra

00:11:22,720 --> 00:11:26,500
almost anything extra on top of just

00:11:24,430 --> 00:11:27,759
handing the in common requests and that

00:11:26,500 --> 00:11:29,379
kind of makes a lot of sense because

00:11:27,759 --> 00:11:31,569
when you're serving you've got a request

00:11:29,379 --> 00:11:33,519
there is nothing to think about you just

00:11:31,569 --> 00:11:35,230
handle it and move along right but

00:11:33,519 --> 00:11:39,100
there's slightly complicated logic

00:11:35,230 --> 00:11:41,649
inside server to to accommodate that and

00:11:39,100 --> 00:11:45,189
basically by default everything will

00:11:41,649 --> 00:11:47,199
server does three simple things they do

00:11:45,189 --> 00:11:50,259
metrics and racing for you so it's easy

00:11:47,199 --> 00:11:53,470
to develop to to debug production issues

00:11:50,259 --> 00:11:55,389
and monitor your calm hands it enforces

00:11:53,470 --> 00:11:58,689
really simple request handle time

00:11:55,389 --> 00:12:01,509
timeout and also maintains we request

00:11:58,689 --> 00:12:03,040
concurrency limit and that's basically

00:12:01,509 --> 00:12:05,379
it there is nothing much

00:12:03,040 --> 00:12:06,790
server left to do because it's just like

00:12:05,379 --> 00:12:09,850
straightforward you've got a request you

00:12:06,790 --> 00:12:11,680
handle that so here's how you can all

00:12:09,850 --> 00:12:14,740
write the concurrent submit on on your

00:12:11,680 --> 00:12:18,009
server you can say that I won't handle

00:12:14,740 --> 00:12:20,529
it at max 10 requests at once and there

00:12:18,009 --> 00:12:22,959
is like zero waiters allowed for my

00:12:20,529 --> 00:12:25,810
server and there's like a super

00:12:22,959 --> 00:12:27,579
straightforward API to do that and the

00:12:25,810 --> 00:12:29,470
concurrency limits and everything on top

00:12:27,579 --> 00:12:31,360
of that 10 requests will be rejected by

00:12:29,470 --> 00:12:33,670
a server and hopefully will be retried

00:12:31,360 --> 00:12:35,860
by a client that talks to you but we

00:12:33,670 --> 00:12:37,870
don't know that for sure but anyways we

00:12:35,860 --> 00:12:41,050
just reject them and the concurrency

00:12:37,870 --> 00:12:43,089
limit is one of the forms of admission

00:12:41,050 --> 00:12:45,550
control with support for servers and

00:12:43,089 --> 00:12:47,889
finagle and when I say admission control

00:12:45,550 --> 00:12:50,259
I mean some kind of feedback control

00:12:47,889 --> 00:12:52,750
from the underlying system that you can

00:12:50,259 --> 00:12:54,309
use to figure out if you have enough

00:12:52,750 --> 00:12:56,769
resources to handle the incoming

00:12:54,309 --> 00:12:58,389
requests and if you don't have them you

00:12:56,769 --> 00:13:01,920
just reject that and this is really

00:12:58,389 --> 00:13:05,259
useful for servers to not like overwhelm

00:13:01,920 --> 00:13:07,360
theirselves and for example another form

00:13:05,259 --> 00:13:10,000
of admission control we are experiencing

00:13:07,360 --> 00:13:11,620
right now is and it's not included in

00:13:10,000 --> 00:13:14,379
our open source for because it's highly

00:13:11,620 --> 00:13:17,370
tied to the infrastructure when we where

00:13:14,379 --> 00:13:19,449
we run our services which is messes and

00:13:17,370 --> 00:13:21,189
so the admission control is called

00:13:19,449 --> 00:13:21,699
throttling based admission control and

00:13:21,189 --> 00:13:24,569
by

00:13:21,699 --> 00:13:26,619
name is kind of like known what it do

00:13:24,569 --> 00:13:28,720
basically what it's trying to do it

00:13:26,619 --> 00:13:31,569
monitors messes Patrick's and try to

00:13:28,720 --> 00:13:33,279
predict the time when when replicas when

00:13:31,569 --> 00:13:35,230
your server is about to get throttled by

00:13:33,279 --> 00:13:37,389
by messes and it start rejecting

00:13:35,230 --> 00:13:39,220
requests to prevent that basically it

00:13:37,389 --> 00:13:42,100
like it's really really useful to

00:13:39,220 --> 00:13:44,109
protect yourself from half from huge

00:13:42,100 --> 00:13:45,730
spikes on traffic and instead of like

00:13:44,109 --> 00:13:48,519
slowing down one higher percent of

00:13:45,730 --> 00:13:50,679
requests you just reject 25% of them and

00:13:48,519 --> 00:13:53,230
keep operating normally this is really

00:13:50,679 --> 00:13:54,970
really nice feature because sometimes it

00:13:53,230 --> 00:13:57,129
even helps you to stay within the Ethel

00:13:54,970 --> 00:13:59,169
O's so you don't have to for example

00:13:57,129 --> 00:14:01,239
with throttling based admission control

00:13:59,169 --> 00:14:04,179
the worst case scenario you can get is

00:14:01,239 --> 00:14:07,149
you if you disabled you can get the

00:14:04,179 --> 00:14:09,249
entire cluster restarted because every

00:14:07,149 --> 00:14:11,230
single note there bilbies will be

00:14:09,249 --> 00:14:12,879
troubled but we start on based admission

00:14:11,230 --> 00:14:14,739
control you just sort of like reject

00:14:12,879 --> 00:14:16,749
some amount of requests some kind of

00:14:14,739 --> 00:14:18,669
failures you've sent to clients but you

00:14:16,749 --> 00:14:21,160
keep operating normally and you keep

00:14:18,669 --> 00:14:22,839
serving traffic at least 75% of traffic

00:14:21,160 --> 00:14:25,269
you do that and this is really really

00:14:22,839 --> 00:14:28,600
useful thing and another interesting

00:14:25,269 --> 00:14:30,309
part I wanted to mention is it's not

00:14:28,600 --> 00:14:31,809
really like a mission control is

00:14:30,309 --> 00:14:34,029
something something slightly different

00:14:31,809 --> 00:14:37,059
but we've been also experimenting with

00:14:34,029 --> 00:14:39,249
that and it's called GC avoidance so the

00:14:37,059 --> 00:14:40,809
thing is you sort of like do the same

00:14:39,249 --> 00:14:43,419
thing but instead of monitoring Meadows

00:14:40,809 --> 00:14:45,579
metrics you monitor your GC activities

00:14:43,419 --> 00:14:48,609
and when you can predict that you know

00:14:45,579 --> 00:14:50,649
it is about to get to go for launch easy

00:14:48,609 --> 00:14:52,689
pause you can start rejecting traffic to

00:14:50,649 --> 00:14:55,269
prevent that or you can just say your

00:14:52,689 --> 00:14:57,339
servers saying like hey I want to spend

00:14:55,269 --> 00:14:59,379
some time doing GC you can you know can

00:14:57,339 --> 00:15:01,209
you not send some requests to me for

00:14:59,379 --> 00:15:03,609
that time and it's really useful it's

00:15:01,209 --> 00:15:05,369
not enabled by default for us but we're

00:15:03,609 --> 00:15:07,749
trying to experiment with that as well

00:15:05,369 --> 00:15:10,629
and if you want to talk about that more

00:15:07,749 --> 00:15:12,339
we can chat about afterwards I can I can

00:15:10,629 --> 00:15:15,220
teach you how to enable that

00:15:12,339 --> 00:15:17,199
so what else the request amount is

00:15:15,220 --> 00:15:19,239
really a simple thing you just say how

00:15:17,199 --> 00:15:21,850
much time do you want to spend handling

00:15:19,239 --> 00:15:23,589
the request and if you want unable to if

00:15:21,850 --> 00:15:25,899
you wasn't able to handle that in 42

00:15:23,589 --> 00:15:29,019
seconds you just timeout that which is

00:15:25,899 --> 00:15:31,239
really interesting part we're at clients

00:15:29,019 --> 00:15:33,189
clients are really really smart they

00:15:31,239 --> 00:15:34,980
like much more complicated than servers

00:15:33,189 --> 00:15:37,440
then even if even if

00:15:34,980 --> 00:15:39,240
compared to death servers that can sort

00:15:37,440 --> 00:15:40,769
of monitor masses Patrick's and do stuff

00:15:39,240 --> 00:15:43,860
like that it's still really stupid

00:15:40,769 --> 00:15:46,889
compared to clients so and clients do as

00:15:43,860 --> 00:15:48,690
much as possible to make sure that your

00:15:46,889 --> 00:15:49,740
request will succeed in the least

00:15:48,690 --> 00:15:51,750
possible time

00:15:49,740 --> 00:15:54,240
so they maximize success rate and

00:15:51,750 --> 00:15:55,740
minimize latency but servers optimized

00:15:54,240 --> 00:15:57,839
for high throughput so there's not so

00:15:55,740 --> 00:16:01,050
much we can do and then clients we can

00:15:57,839 --> 00:16:04,220
do a lot to make sure that success rate

00:16:01,050 --> 00:16:07,010
is maximum and latency is minimum and

00:16:04,220 --> 00:16:10,170
this is basically the list of features

00:16:07,010 --> 00:16:12,510
client supports we won't be able to talk

00:16:10,170 --> 00:16:15,449
about everything here but we will try to

00:16:12,510 --> 00:16:17,730
cover like highlighted parts and before

00:16:15,449 --> 00:16:19,350
doing that let's just go through every

00:16:17,730 --> 00:16:22,920
item here and see if we understand why

00:16:19,350 --> 00:16:24,899
we actually need that so the retry model

00:16:22,920 --> 00:16:26,760
is actually pretty straightforward you

00:16:24,899 --> 00:16:29,190
want to be able to retry failed requests

00:16:26,760 --> 00:16:31,980
to make sure that success success rate

00:16:29,190 --> 00:16:33,779
is is high and to do that it's actually

00:16:31,980 --> 00:16:36,329
really tricky problem because you have

00:16:33,779 --> 00:16:38,279
to answer a bunch of tricky tricky

00:16:36,329 --> 00:16:40,889
questions for example how do you save

00:16:38,279 --> 00:16:43,889
request failed or how do I say if it's

00:16:40,889 --> 00:16:45,779
safe to retry or for example if you

00:16:43,889 --> 00:16:47,910
already do try it once the question is

00:16:45,779 --> 00:16:49,470
should I keep retrying or should I stop

00:16:47,910 --> 00:16:51,480
so these just drop it and stuff like

00:16:49,470 --> 00:16:54,360
that this is really tricky problem and

00:16:51,480 --> 00:16:56,339
retries are enabled by default and every

00:16:54,360 --> 00:16:59,130
finical client you usually shouldn't

00:16:56,339 --> 00:17:01,860
care about that much what else we have

00:16:59,130 --> 00:17:03,510
to be able to help services like hate

00:17:01,860 --> 00:17:06,209
each other so there is a built in

00:17:03,510 --> 00:17:08,819
service discovery and naming tool inside

00:17:06,209 --> 00:17:10,679
they go inside finagle clients so

00:17:08,819 --> 00:17:12,089
basically you get that for free and if

00:17:10,679 --> 00:17:14,130
you want to get more you want to know

00:17:12,089 --> 00:17:16,410
more about that I highly recommend

00:17:14,130 --> 00:17:18,419
watching viruses talk from the most

00:17:16,410 --> 00:17:20,339
recent finagle con it's called RPC redux

00:17:18,419 --> 00:17:23,250
and it covers naming and service

00:17:20,339 --> 00:17:25,410
discovery part really well so we want to

00:17:23,250 --> 00:17:27,809
make sure we can override timeouts and

00:17:25,410 --> 00:17:30,990
expiration inside the clients so we can

00:17:27,809 --> 00:17:33,030
sort of like put the bounds into our

00:17:30,990 --> 00:17:35,220
distributed system into companies in

00:17:33,030 --> 00:17:37,049
distributed system we also make sure the

00:17:35,220 --> 00:17:38,549
most interesting part we also want to

00:17:37,049 --> 00:17:41,700
make sure that we can distribute traffic

00:17:38,549 --> 00:17:44,309
across a number of instances where our

00:17:41,700 --> 00:17:46,669
software is deployed and load balancers

00:17:44,309 --> 00:17:48,750
solved that inside every finagle client

00:17:46,669 --> 00:17:50,940
what we also can do we

00:17:48,750 --> 00:17:52,560
the rate-limiting and that's actually

00:17:50,940 --> 00:17:54,720
really depends on what you want to try

00:17:52,560 --> 00:17:57,180
to do for example you can talk to some

00:17:54,720 --> 00:17:58,500
external API that rate limits you you

00:17:57,180 --> 00:17:59,880
can make sure you if an Eagle client

00:17:58,500 --> 00:18:01,890
doesn't exceed that you can configure

00:17:59,880 --> 00:18:04,970
that and make sure you still keep

00:18:01,890 --> 00:18:07,140
getting the traffic to through that API

00:18:04,970 --> 00:18:08,820
you can do connection pooling it's

00:18:07,140 --> 00:18:11,430
enabled by default for every single

00:18:08,820 --> 00:18:13,410
finagle client except for the marks one

00:18:11,430 --> 00:18:16,730
because it doesn't actually make sense

00:18:13,410 --> 00:18:19,080
to have that for multiplexing protocols

00:18:16,730 --> 00:18:22,910
you can do circuit breaker and failure

00:18:19,080 --> 00:18:25,230
detection for free and finagle does that

00:18:22,910 --> 00:18:28,710
so why we need that is we want to make

00:18:25,230 --> 00:18:31,470
sure we keep we want to make sure we

00:18:28,710 --> 00:18:33,660
take her about unreliable replicas knows

00:18:31,470 --> 00:18:35,430
inside your cluster so we want to

00:18:33,660 --> 00:18:38,130
exclude them from the load balancer set

00:18:35,430 --> 00:18:40,590
so we don't set traffic onto that nodes

00:18:38,130 --> 00:18:42,930
it does metrics and tracing for you for

00:18:40,590 --> 00:18:44,670
free in the same way servers does so you

00:18:42,930 --> 00:18:46,530
can monitor some kind of like success

00:18:44,670 --> 00:18:48,240
rate how many requests you've got was

00:18:46,530 --> 00:18:51,090
delete and see or regress and stuff like

00:18:48,240 --> 00:18:54,510
that it's really really useful and the

00:18:51,090 --> 00:18:56,130
last two is really tricky questions here

00:18:54,510 --> 00:18:58,500
and most of the people don't care about

00:18:56,130 --> 00:19:02,430
them but we do care and if a niggle does

00:18:58,500 --> 00:19:04,950
support that and so interrupts why we

00:19:02,430 --> 00:19:07,890
need interrupts we need to make sure

00:19:04,950 --> 00:19:10,050
that we do not do some useless work and

00:19:07,890 --> 00:19:13,200
we want to make sure we have support for

00:19:10,050 --> 00:19:15,360
canceling requests so and so the problem

00:19:13,200 --> 00:19:17,520
is you might think why would they cancel

00:19:15,360 --> 00:19:19,980
request why would I do that I just send

00:19:17,520 --> 00:19:22,290
it crap I want that handled but um

00:19:19,980 --> 00:19:24,180
that's actually happened implicitly for

00:19:22,290 --> 00:19:26,100
you for example let's imagine you have a

00:19:24,180 --> 00:19:28,110
client and you set up a timeout for that

00:19:26,100 --> 00:19:30,180
and after given amount of time it

00:19:28,110 --> 00:19:32,580
expires and what finagle does it

00:19:30,180 --> 00:19:34,350
interrupts the future associated with

00:19:32,580 --> 00:19:36,720
that request and it gets propagated

00:19:34,350 --> 00:19:39,240
across the service boundaries and if you

00:19:36,720 --> 00:19:41,340
interrupt the request you just send and

00:19:39,240 --> 00:19:43,200
it actually really really depends on the

00:19:41,340 --> 00:19:44,940
protocol you use and today it will check

00:19:43,200 --> 00:19:47,580
care about that so the worst case

00:19:44,940 --> 00:19:48,810
scenario for HTTP for example there is

00:19:47,580 --> 00:19:51,570
nothing we can do just tap the

00:19:48,810 --> 00:19:54,060
connection but for some more advanced

00:19:51,570 --> 00:19:56,310
protocol like Marx we can actually try

00:19:54,060 --> 00:19:58,980
to cancel that we can send a low-level

00:19:56,310 --> 00:20:00,630
signal to a server saying that hey I'm

00:19:58,980 --> 00:20:01,920
no longer interested in the result of

00:20:00,630 --> 00:20:03,540
that request I just haven't

00:20:01,920 --> 00:20:05,850
you so you should feel free to drop that

00:20:03,540 --> 00:20:08,640
and we do that today and we do that for

00:20:05,850 --> 00:20:10,500
four marks and you if you use marks to

00:20:08,640 --> 00:20:13,980
three parts you should get that for free

00:20:10,500 --> 00:20:16,950
and the contacts propagation is kind of

00:20:13,980 --> 00:20:18,860
similar thing and the idea is you want

00:20:16,950 --> 00:20:21,210
to make sure that you can attach some

00:20:18,860 --> 00:20:22,920
additional information to your request

00:20:21,210 --> 00:20:25,290
like Trey said you like client I did

00:20:22,920 --> 00:20:26,970
request ID anything you want to you want

00:20:25,290 --> 00:20:29,220
to do and basically if an Eagle takes

00:20:26,970 --> 00:20:31,770
care about that it gets she relies

00:20:29,220 --> 00:20:33,900
serialized and this relized depending on

00:20:31,770 --> 00:20:36,210
the protocol use and sort of like

00:20:33,900 --> 00:20:37,890
replicates over the wire in between

00:20:36,210 --> 00:20:39,720
different services and you can sort of

00:20:37,890 --> 00:20:42,300
pull up that information anytime you

00:20:39,720 --> 00:20:43,920
want and you'll get that explicitly for

00:20:42,300 --> 00:20:47,790
your request and this is really is

00:20:43,920 --> 00:20:49,830
useful for tracing for example and um so

00:20:47,790 --> 00:20:52,950
the last two is actually really good

00:20:49,830 --> 00:20:55,080
reason for us to still like our futures

00:20:52,950 --> 00:20:56,640
better because they support that and

00:20:55,080 --> 00:21:01,170
scholar futures doesn't actually do that

00:20:56,640 --> 00:21:03,570
for you all right so um before we start

00:21:01,170 --> 00:21:06,720
about talking in deep details about

00:21:03,570 --> 00:21:09,480
legal client models we can talk about

00:21:06,720 --> 00:21:13,200
this interesting problem in this poll

00:21:09,480 --> 00:21:16,980
and the problem is how come HTTP 500

00:21:13,200 --> 00:21:20,580
considered success and finagle so who

00:21:16,980 --> 00:21:22,920
thinks it's a bug nobody thinks it's a

00:21:20,580 --> 00:21:25,860
bug okay that's actually right because

00:21:22,920 --> 00:21:27,870
it's not a bug the idea is as you

00:21:25,860 --> 00:21:30,060
remember if an Eagle operates on layer 5

00:21:27,870 --> 00:21:32,430
of the OSI mode so it knows nothing

00:21:30,060 --> 00:21:36,000
about the protocol application is used

00:21:32,430 --> 00:21:38,640
by and basically when it sees an HTTP

00:21:36,000 --> 00:21:41,160
request B response 500 it sees the

00:21:38,640 --> 00:21:43,290
correctly structured HTTP message and it

00:21:41,160 --> 00:21:45,840
looks like success to finagle and that's

00:21:43,290 --> 00:21:48,000
actually pretty big deal because first

00:21:45,840 --> 00:21:49,440
of all our matrix messed up because

00:21:48,000 --> 00:21:51,330
success rate looks like one higher

00:21:49,440 --> 00:21:54,140
percent but we are serving failures

00:21:51,330 --> 00:21:56,790
basically and the second part is

00:21:54,140 --> 00:21:59,370
Finnegan load balancers go crazy about

00:21:56,790 --> 00:22:01,590
that because if it's your respond really

00:21:59,370 --> 00:22:03,990
quickly and response looks like success

00:22:01,590 --> 00:22:05,580
load balancer things well it might be

00:22:03,990 --> 00:22:06,990
good idea to send more traffic to that

00:22:05,580 --> 00:22:09,420
replica because it's doing really great

00:22:06,990 --> 00:22:12,330
job and in fact not because it's serving

00:22:09,420 --> 00:22:14,490
failures and to solve that it's been a

00:22:12,330 --> 00:22:15,260
problem quite a lot of time and to solve

00:22:14,490 --> 00:22:17,870
that we have

00:22:15,260 --> 00:22:21,380
really new exciting future calls it's

00:22:17,870 --> 00:22:24,170
called response classifiers so it means

00:22:21,380 --> 00:22:25,970
that you can actually plug in into

00:22:24,170 --> 00:22:29,330
finagle client some sort of like a

00:22:25,970 --> 00:22:31,040
partial function saying saying what kind

00:22:29,330 --> 00:22:32,720
of request should be treated as success

00:22:31,040 --> 00:22:34,820
what kind of request should be treated

00:22:32,720 --> 00:22:37,370
as failures and you can teach vinegar

00:22:34,820 --> 00:22:39,320
how to how to behave on that kind of

00:22:37,370 --> 00:22:41,690
request and this is really useful for

00:22:39,320 --> 00:22:43,040
example here you can define response

00:22:41,690 --> 00:22:45,590
classifier that treats

00:22:43,040 --> 00:22:47,930
five-four-three service unavailable as a

00:22:45,590 --> 00:22:50,450
non-viable failure so which means that

00:22:47,930 --> 00:22:53,240
when you've got that response from from

00:22:50,450 --> 00:22:56,180
a server the circuit breaker will kick

00:22:53,240 --> 00:22:58,310
in and sort of like disabled that

00:22:56,180 --> 00:23:00,260
replica for some time to make sure that

00:22:58,310 --> 00:23:02,180
gets turned almost stayed before before

00:23:00,260 --> 00:23:03,580
you can retry that request and it's

00:23:02,180 --> 00:23:08,450
really useful

00:23:03,580 --> 00:23:11,270
right so finally how do we do retries so

00:23:08,450 --> 00:23:13,400
basically um you've got retry model on

00:23:11,270 --> 00:23:15,410
the waited by the top of the stack so

00:23:13,400 --> 00:23:17,660
you can retry everything underneath and

00:23:15,410 --> 00:23:19,190
basically if for example you can retry

00:23:17,660 --> 00:23:21,860
failures from circuit breakers from

00:23:19,190 --> 00:23:23,690
timeouts from a load balancer and that

00:23:21,860 --> 00:23:26,080
makes a lot of sense if you happen to

00:23:23,690 --> 00:23:29,030
pick a replica that rejected the request

00:23:26,080 --> 00:23:30,890
by some reason we don't know which for

00:23:29,030 --> 00:23:32,960
example it might be going to for

00:23:30,890 --> 00:23:33,590
admission control methods traveling or

00:23:32,960 --> 00:23:35,480
something like that

00:23:33,590 --> 00:23:36,800
and if you reject the request it makes a

00:23:35,480 --> 00:23:39,140
lot of sense to retry that in a

00:23:36,800 --> 00:23:40,700
different replica and retries actually

00:23:39,140 --> 00:23:42,950
do that for you

00:23:40,700 --> 00:23:45,140
and by default finagle bridge highs only

00:23:42,950 --> 00:23:47,330
when it's absolutely safe to do when

00:23:45,140 --> 00:23:50,080
it's absolutely known that your request

00:23:47,330 --> 00:23:53,030
wasn't reason to wire yet for example or

00:23:50,080 --> 00:23:54,650
when remote server reject that so you

00:23:53,030 --> 00:23:55,220
know that if you retry that it's going

00:23:54,650 --> 00:24:00,110
to be fine

00:23:55,220 --> 00:24:03,050
nothing is going to be break and it uses

00:24:00,110 --> 00:24:05,120
some interesting interesting techniques

00:24:03,050 --> 00:24:07,370
to make sure you prevent Wheatridge

00:24:05,120 --> 00:24:10,040
storms basically which means we trying

00:24:07,370 --> 00:24:12,380
too much and there's this abstraction

00:24:10,040 --> 00:24:14,720
called retry bucket sorry retry budget

00:24:12,380 --> 00:24:15,860
which is basically a leaky talking

00:24:14,720 --> 00:24:18,650
bucket and it ties

00:24:15,860 --> 00:24:21,020
total number of requests to number of

00:24:18,650 --> 00:24:22,910
retries you have so you can have some

00:24:21,020 --> 00:24:25,010
sort of like manageable upper bounds and

00:24:22,910 --> 00:24:26,630
total number of retries you have and you

00:24:25,010 --> 00:24:27,980
will be a good citizen in your system

00:24:26,630 --> 00:24:30,290
because you don't

00:24:27,980 --> 00:24:32,929
want overwhelm undermine underlying

00:24:30,290 --> 00:24:35,510
services because of retries and also

00:24:32,929 --> 00:24:37,400
what I also do for you is there is a

00:24:35,510 --> 00:24:39,290
back half policy you can all right and

00:24:37,400 --> 00:24:41,900
it basically answers the question how

00:24:39,290 --> 00:24:43,610
long to wait between retries like shrimp

00:24:41,900 --> 00:24:45,860
you try immediately should I wait some

00:24:43,610 --> 00:24:48,230
time and the back off policy does it for

00:24:45,860 --> 00:24:50,809
you and so both those abstraction are

00:24:48,230 --> 00:24:52,850
quite useful to mitigate we drive storms

00:24:50,809 --> 00:24:55,790
and that's that's what we use today and

00:24:52,850 --> 00:24:57,980
that's really really helpful for example

00:24:55,790 --> 00:25:00,350
you can define retry budget in a way

00:24:57,980 --> 00:25:03,350
like this he's saying I want to make

00:25:00,350 --> 00:25:06,320
sure that I will try no more than 10% of

00:25:03,350 --> 00:25:09,110
total number of requests and on top of

00:25:06,320 --> 00:25:13,010
five retries per second to accommodate

00:25:09,110 --> 00:25:14,960
clients with low RPS and and the nice

00:25:13,010 --> 00:25:17,120
thing about that as I said you can

00:25:14,960 --> 00:25:19,280
actually share that instance with for

00:25:17,120 --> 00:25:20,809
example different finagle components and

00:25:19,280 --> 00:25:23,510
make sure that don't cause with dry

00:25:20,809 --> 00:25:26,419
storms for example you can build your

00:25:23,510 --> 00:25:28,520
own retry filter that retries failures

00:25:26,419 --> 00:25:30,410
from your domain application and you can

00:25:28,520 --> 00:25:33,200
use the same instance of the retry

00:25:30,410 --> 00:25:34,850
badges there it's straight safe and you

00:25:33,200 --> 00:25:36,410
will get the manageable upper bound of

00:25:34,850 --> 00:25:38,990
total number of retries so you should

00:25:36,410 --> 00:25:42,440
feel safe about your client not be

00:25:38,990 --> 00:25:45,950
trying too much it's really useful so

00:25:42,440 --> 00:25:48,350
the back half policy is defines the

00:25:45,950 --> 00:25:50,540
behavior of the retry client saying how

00:25:48,350 --> 00:25:52,370
long to wait between retry attempts and

00:25:50,540 --> 00:25:54,500
basically a stream of duration which is

00:25:52,370 --> 00:25:56,540
really nice because you can plug in your

00:25:54,500 --> 00:25:59,270
own thing really easily because we know

00:25:56,540 --> 00:26:01,669
how to work with streams and sculler

00:25:59,270 --> 00:26:04,130
collections stuff like that and we also

00:26:01,669 --> 00:26:07,450
provide a bunch of predefined policies

00:26:04,130 --> 00:26:11,890
as well and the most useful of them is

00:26:07,450 --> 00:26:14,960
the one that jittered so the idea is

00:26:11,890 --> 00:26:16,820
quite often you have a bunch of clients

00:26:14,960 --> 00:26:19,669
talking to a single server and that's

00:26:16,820 --> 00:26:21,740
erm I behave might perform really poorly

00:26:19,669 --> 00:26:23,270
and the high contention and you want to

00:26:21,740 --> 00:26:25,460
make sure that clients started at the

00:26:23,270 --> 00:26:27,320
same time if they go on a chart if

00:26:25,460 --> 00:26:29,299
you're going to retry they should

00:26:27,320 --> 00:26:31,070
shouldn't retry at the same time so we

00:26:29,299 --> 00:26:32,540
want to reduce contention between them

00:26:31,070 --> 00:26:35,000
and we want to make sure they don't

00:26:32,540 --> 00:26:36,799
compete with each other so to do that we

00:26:35,000 --> 00:26:40,310
just add some sort of like randomized

00:26:36,799 --> 00:26:43,430
factored into any into every retry in

00:26:40,310 --> 00:26:45,110
any duration in the back half so we want

00:26:43,430 --> 00:26:47,690
to make sure that clients do not compete

00:26:45,110 --> 00:26:49,940
with each other and jitter back offs do

00:26:47,690 --> 00:26:52,970
that for you and you can use any of the

00:26:49,940 --> 00:26:54,530
predefined policies here and the most

00:26:52,970 --> 00:26:56,840
interesting part is that finagle does

00:26:54,530 --> 00:27:00,350
that for free for most of its components

00:26:56,840 --> 00:27:02,960
but not for what not for for rich high

00:27:00,350 --> 00:27:04,730
policies and for retries you can

00:27:02,960 --> 00:27:06,260
override that using saying something

00:27:04,730 --> 00:27:09,290
like I want to use exponential jitter

00:27:06,260 --> 00:27:11,300
starting at two seconds and ending at 32

00:27:09,290 --> 00:27:13,640
seconds and what it means in practice

00:27:11,300 --> 00:27:16,100
you will be refined at random of two

00:27:13,640 --> 00:27:16,790
seconds random of four seconds and so on

00:27:16,100 --> 00:27:19,400
and so forth

00:27:16,790 --> 00:27:21,230
up to 32 seconds and you will make sure

00:27:19,400 --> 00:27:24,050
that your clients do not compete with

00:27:21,230 --> 00:27:28,850
each other how much we try to to single

00:27:24,050 --> 00:27:31,100
server right so now that we know how to

00:27:28,850 --> 00:27:34,790
retry we want to make sure that there is

00:27:31,100 --> 00:27:37,520
something we can retry - and one of the

00:27:34,790 --> 00:27:39,860
ways to do that is - all right some time

00:27:37,520 --> 00:27:41,240
outs in your client and this is there is

00:27:39,860 --> 00:27:42,830
a hell of a ground timeout so you can

00:27:41,240 --> 00:27:44,540
all right in finagle you can all write

00:27:42,830 --> 00:27:47,120
TCP timeout you can all right session

00:27:44,540 --> 00:27:49,430
acquisition timeout session max lifetime

00:27:47,120 --> 00:27:52,760
section max idle time and stuff like

00:27:49,430 --> 00:27:56,840
that and this really useful and none of

00:27:52,760 --> 00:27:58,580
that is all written by default finagle

00:27:56,840 --> 00:28:02,090
is trying to not speculate on

00:27:58,580 --> 00:28:03,290
application level values so it doesn't

00:28:02,090 --> 00:28:05,600
overwrite anything like that

00:28:03,290 --> 00:28:07,790
there is no retry sorry there is no

00:28:05,600 --> 00:28:10,300
request timeout by default there is no

00:28:07,790 --> 00:28:13,250
concurrency limit by default there is no

00:28:10,300 --> 00:28:15,470
any kind of session timeout by default

00:28:13,250 --> 00:28:17,780
you have to be explicit about that and

00:28:15,470 --> 00:28:19,940
it should be tied to your domain to your

00:28:17,780 --> 00:28:22,450
application and things like that finagle

00:28:19,940 --> 00:28:25,760
is not speculating on those values

00:28:22,450 --> 00:28:27,680
finally the most interesting part of

00:28:25,760 --> 00:28:31,360
balancing Nagle which I think the most

00:28:27,680 --> 00:28:34,280
advanced thing and infinite go at all so

00:28:31,360 --> 00:28:37,100
first of all to understand how this

00:28:34,280 --> 00:28:39,770
thing works and finial is that the

00:28:37,100 --> 00:28:41,960
important part here is there is a

00:28:39,770 --> 00:28:44,870
deep-seated assumption inside finagle it

00:28:41,960 --> 00:28:47,330
was designed above this idea that your

00:28:44,870 --> 00:28:49,640
server set is basically a replica set

00:28:47,330 --> 00:28:51,800
which means every single note in your

00:28:49,640 --> 00:28:53,730
certain you in your server set is

00:28:51,800 --> 00:28:55,200
actually equivalent from the point

00:28:53,730 --> 00:28:57,540
view of your application so you can

00:28:55,200 --> 00:28:59,280
totally retry any kind of requests

00:28:57,540 --> 00:29:00,900
you've got on to any different replicas

00:28:59,280 --> 00:29:03,990
you have and you should feel free to

00:29:00,900 --> 00:29:06,000
distribute traffic uniformly and do not

00:29:03,990 --> 00:29:07,830
tie to any kind of replica on your

00:29:06,000 --> 00:29:09,570
server set and finagle comes with a

00:29:07,830 --> 00:29:12,840
bunch of load balancing options

00:29:09,570 --> 00:29:14,970
different and it might be viewed as a

00:29:12,840 --> 00:29:17,250
combination of two different components

00:29:14,970 --> 00:29:20,100
we've got flawed distributor and load

00:29:17,250 --> 00:29:22,020
factor basically what we do is we

00:29:20,100 --> 00:29:24,600
distribute traffic across some subset of

00:29:22,020 --> 00:29:26,220
nodes and then keep the one for which

00:29:24,600 --> 00:29:28,710
the Lord metric is minimal

00:29:26,220 --> 00:29:31,740
that's what general load balancer does

00:29:28,710 --> 00:29:34,410
for for nato and in order to understand

00:29:31,740 --> 00:29:36,360
what kind of load balancers we have an

00:29:34,410 --> 00:29:38,160
eagle right now I think it makes a lot

00:29:36,360 --> 00:29:40,980
of sense to have a look at the evolution

00:29:38,160 --> 00:29:42,840
of load balancer and finagle and in the

00:29:40,980 --> 00:29:45,330
early days we have this very simple

00:29:42,840 --> 00:29:47,580
thing it was called hip low down sir and

00:29:45,330 --> 00:29:50,310
it's built on top of min-heap that

00:29:47,580 --> 00:29:52,680
maintains number of outstanding requests

00:29:50,310 --> 00:29:54,750
per each node in your cluster and it's

00:29:52,680 --> 00:29:56,100
really simple and straightforward and so

00:29:54,750 --> 00:29:57,810
when you make a request you just pick a

00:29:56,100 --> 00:30:00,360
root because it's minimal value and then

00:29:57,810 --> 00:30:00,930
you rebuild the entire hip and it works

00:30:00,360 --> 00:30:02,820
really well

00:30:00,930 --> 00:30:04,950
and it worked really well for us for

00:30:02,820 --> 00:30:06,840
some time and at some point we figure

00:30:04,950 --> 00:30:09,180
out a couple of drawbacks that solution

00:30:06,840 --> 00:30:11,580
had first of all when you think about

00:30:09,180 --> 00:30:13,710
load bouncer and the load balancer state

00:30:11,580 --> 00:30:15,690
is a highly contended resource it should

00:30:13,710 --> 00:30:17,700
be really fast and it should scale scale

00:30:15,690 --> 00:30:19,890
really really well because you have to

00:30:17,700 --> 00:30:21,570
update that on every single request and

00:30:19,890 --> 00:30:23,970
you might get like million of those

00:30:21,570 --> 00:30:26,790
requests every second and heap is really

00:30:23,970 --> 00:30:28,950
really fast on one hand but you can take

00:30:26,790 --> 00:30:31,410
the minimum element in constant time but

00:30:28,950 --> 00:30:33,480
every other operation takes logarithmic

00:30:31,410 --> 00:30:36,330
time to perform and this is a big deal

00:30:33,480 --> 00:30:38,670
because you are not able to implement

00:30:36,330 --> 00:30:40,950
any kind of sophisticated load metric on

00:30:38,670 --> 00:30:42,540
top of that because it would be it would

00:30:40,950 --> 00:30:45,450
just sacrifice the performance of the

00:30:42,540 --> 00:30:47,010
hip then this large service app and

00:30:45,450 --> 00:30:48,960
there is like a several thousands of

00:30:47,010 --> 00:30:50,550
nodes in your hip it might be a big deal

00:30:48,960 --> 00:30:52,910
and that's what you've noticed before

00:30:50,550 --> 00:30:55,620
and the next thing we try to build

00:30:52,910 --> 00:30:58,050
instead of the heap balancer is called a

00:30:55,620 --> 00:30:59,490
power of two choices it's basically

00:30:58,050 --> 00:31:02,970
really really nice and brilliant

00:30:59,490 --> 00:31:04,920
mathematical idea and it's the load

00:31:02,970 --> 00:31:06,750
balancer algorithm basically picks two

00:31:04,920 --> 00:31:07,520
nodes from the server's head like

00:31:06,750 --> 00:31:09,410
randomly

00:31:07,520 --> 00:31:11,179
and pigs Dilys loaded one and by

00:31:09,410 --> 00:31:13,160
repeatedly applying this strategy on

00:31:11,179 --> 00:31:15,320
your server set you will get manageable

00:31:13,160 --> 00:31:17,120
upper bound on flawed for each note

00:31:15,320 --> 00:31:19,309
which is exactly what we need when we

00:31:17,120 --> 00:31:21,080
want to do load balancer and load

00:31:19,309 --> 00:31:23,570
balancing and we want to make sure that

00:31:21,080 --> 00:31:26,059
every single note in your cluster will

00:31:23,570 --> 00:31:29,600
get a uniform sort of like uniform lot

00:31:26,059 --> 00:31:31,670
and busey works really well because it

00:31:29,600 --> 00:31:33,200
scales really nice it's just the rate

00:31:31,670 --> 00:31:35,440
you have to update you can do that in

00:31:33,200 --> 00:31:38,240
constant time and then load to build

00:31:35,440 --> 00:31:40,070
some kind of sophisticated load matrix

00:31:38,240 --> 00:31:44,270
now just least load it but something

00:31:40,070 --> 00:31:46,400
more interesting and the most advanced

00:31:44,270 --> 00:31:49,130
load metric we have today is called the

00:31:46,400 --> 00:31:51,110
WMA which stands for exponentially

00:31:49,130 --> 00:31:53,540
weighted moving average and basically

00:31:51,110 --> 00:31:55,120
for every single note in your cluster it

00:31:53,540 --> 00:31:57,440
keeps track of the round-trip latency

00:31:55,120 --> 00:32:00,140
weighted by the number of outstanding

00:31:57,440 --> 00:32:02,660
put outstanding requests for each note

00:32:00,140 --> 00:32:04,940
and this is sort of like smart because

00:32:02,660 --> 00:32:07,250
being on layer 5 of the OSI model it

00:32:04,940 --> 00:32:10,460
kind of makes sense to take advantage of

00:32:07,250 --> 00:32:12,530
all four RPC latency and RPC cue dev and

00:32:10,460 --> 00:32:14,540
the this metric does it for you and

00:32:12,530 --> 00:32:17,059
which means it's really sensitive to

00:32:14,540 --> 00:32:19,370
latency spikes and that makes it really

00:32:17,059 --> 00:32:22,130
really nice to if you want to protect

00:32:19,370 --> 00:32:24,710
yourself from long G supporters or JVM

00:32:22,130 --> 00:32:26,690
bar mobs because if you happen to be a

00:32:24,710 --> 00:32:29,420
metric ER so if you happen to be a

00:32:26,690 --> 00:32:31,460
replica that just went for laundry see

00:32:29,420 --> 00:32:33,440
pause that metric will reflect that

00:32:31,460 --> 00:32:35,690
immediately and your load balancer will

00:32:33,440 --> 00:32:37,429
avoid that note for some time giving it

00:32:35,690 --> 00:32:40,340
a chance actually to finish you see and

00:32:37,429 --> 00:32:42,530
the same thing happens if you're if you

00:32:40,340 --> 00:32:44,780
just start in servers and JVM needs some

00:32:42,530 --> 00:32:47,720
time to warm up and that that metric

00:32:44,780 --> 00:32:49,490
helps prevent it's like huge load on

00:32:47,720 --> 00:32:51,500
that may trigger so huge load on that

00:32:49,490 --> 00:32:55,190
replica giving it some time to actually

00:32:51,500 --> 00:32:56,630
finish those and so let's look at the

00:32:55,190 --> 00:32:59,390
comparison of those three different

00:32:56,630 --> 00:33:00,920
options we've seen before that's really

00:32:59,390 --> 00:33:03,020
interesting so we've got round Roman

00:33:00,920 --> 00:33:06,890
which is like two based algorithm error

00:33:03,020 --> 00:33:11,330
and we've got Peter C++ least loaded and

00:33:06,890 --> 00:33:13,370
peter c plus a WMA and here we run a

00:33:11,330 --> 00:33:15,830
simulation when one client talks to

00:33:13,370 --> 00:33:18,350
eleven different servers and each server

00:33:15,830 --> 00:33:20,240
replace a latency profile captured from

00:33:18,350 --> 00:33:21,440
from a production system with no

00:33:20,240 --> 00:33:23,690
significant spikes

00:33:21,440 --> 00:33:25,850
but at some point we fix the latency of

00:33:23,690 --> 00:33:30,230
one of the servers to two seconds

00:33:25,850 --> 00:33:32,260
simulating laundry see balls and we want

00:33:30,230 --> 00:33:35,060
to make sure that we want to measure

00:33:32,260 --> 00:33:37,130
sort of like top-line latency from that

00:33:35,060 --> 00:33:39,140
client and we use different while load

00:33:37,130 --> 00:33:43,340
balancing strategies and what we see

00:33:39,140 --> 00:33:45,880
here is so round-robin behaves really

00:33:43,340 --> 00:33:48,650
really bad it goes to more than one

00:33:45,880 --> 00:33:51,650
one-and-a-half seconds at p99

00:33:48,650 --> 00:33:54,440
and least loaded doing much better until

00:33:51,650 --> 00:33:56,150
doing much better in mitigating latency

00:33:54,440 --> 00:33:59,860
spikes until p99

00:33:56,150 --> 00:34:02,570
and ewm a doing this great job in

00:33:59,860 --> 00:34:04,820
maintaining low latency until ninety

00:34:02,570 --> 00:34:06,110
nine and nine percent which is which is

00:34:04,820 --> 00:34:09,080
and there is a huge difference between

00:34:06,110 --> 00:34:12,410
those three and for example in usually

00:34:09,080 --> 00:34:14,750
we tie latency with failures with

00:34:12,410 --> 00:34:16,190
timeouts in distributed systems and if

00:34:14,750 --> 00:34:18,320
you say timeout is going to be one

00:34:16,190 --> 00:34:19,880
second for that client we will get

00:34:18,320 --> 00:34:21,740
different success rate for those three

00:34:19,880 --> 00:34:24,710
options and with round-robin you will

00:34:21,740 --> 00:34:27,380
get success rate 95% with least loaded

00:34:24,710 --> 00:34:30,020
it's going to be 99 and this WMA is

00:34:27,380 --> 00:34:32,210
going to be 99.9 and there is a huge

00:34:30,020 --> 00:34:33,980
difference between those because this

00:34:32,210 --> 00:34:37,280
first two you were likely to get page

00:34:33,980 --> 00:34:40,640
probably at night and with WMA you will

00:34:37,280 --> 00:34:43,520
be sleeping like a baby that's that's a

00:34:40,640 --> 00:34:45,290
good selling point actually yeah that's

00:34:43,520 --> 00:34:47,990
really interesting and if you think that

00:34:45,290 --> 00:34:49,880
a WMA is the most advanced load balancer

00:34:47,990 --> 00:34:51,919
you think you've seen so far you're

00:34:49,880 --> 00:34:53,480
probably wrong because we've got

00:34:51,919 --> 00:34:56,179
something really really cool inside

00:34:53,480 --> 00:34:59,510
finagle and it's called aperture so um

00:34:56,179 --> 00:35:01,880
and what what it does is it's trying to

00:34:59,510 --> 00:35:04,820
solve the problem of large server sets

00:35:01,880 --> 00:35:07,100
large clusters and what happens in

00:35:04,820 --> 00:35:10,100
practice you may have one client talking

00:35:07,100 --> 00:35:12,410
to several thousands of servers and

00:35:10,100 --> 00:35:14,570
which usually result in several

00:35:12,410 --> 00:35:17,480
thousands of open connections and so

00:35:14,570 --> 00:35:20,450
then low concurrency level per each node

00:35:17,480 --> 00:35:22,490
in you in your load balancer and that's

00:35:20,450 --> 00:35:25,100
actually quite a big problem so why is

00:35:22,490 --> 00:35:28,100
the problem is why number of connections

00:35:25,100 --> 00:35:30,290
murder well that's easy it's just waste

00:35:28,100 --> 00:35:32,270
of resources and it usually comes with

00:35:30,290 --> 00:35:34,010
the cost of longtail latency because of

00:35:32,270 --> 00:35:36,020
high number of connections

00:35:34,010 --> 00:35:38,720
Bushman which usually takes much longer

00:35:36,020 --> 00:35:41,120
than just send a request and white

00:35:38,720 --> 00:35:43,160
concurrency murder and it's actually

00:35:41,120 --> 00:35:45,710
really interesting because in order to

00:35:43,160 --> 00:35:47,630
take an advantage of any law balance

00:35:45,710 --> 00:35:47,930
your metric we need some numbers to work

00:35:47,630 --> 00:35:50,090
with

00:35:47,930 --> 00:35:51,920
we need something and when concurrency

00:35:50,090 --> 00:35:54,050
is low there is everything is zero

00:35:51,920 --> 00:35:55,940
number of outstanding requests per

00:35:54,050 --> 00:35:57,770
repair of each node is zero there is

00:35:55,940 --> 00:36:00,350
nothing we can do to advise load

00:35:57,770 --> 00:36:03,290
balancer how to how to make sure that

00:36:00,350 --> 00:36:05,390
you you made the right choice and sort

00:36:03,290 --> 00:36:09,230
of to solve that we are trying to view

00:36:05,390 --> 00:36:11,540
the cluster set the replica set you have

00:36:09,230 --> 00:36:14,390
like a huge replica set from small

00:36:11,540 --> 00:36:16,460
through the small window called aperture

00:36:14,390 --> 00:36:19,010
and in that window you can actually

00:36:16,460 --> 00:36:21,650
apply any kind of load balancer you have

00:36:19,010 --> 00:36:24,320
to the integral you can apply to see UWM

00:36:21,650 --> 00:36:27,320
a you get and get the best latency

00:36:24,320 --> 00:36:30,620
profile error and this actually works

00:36:27,320 --> 00:36:32,720
pretty well and it means you've got less

00:36:30,620 --> 00:36:34,910
connection for from client less

00:36:32,720 --> 00:36:37,430
connections from server you get better

00:36:34,910 --> 00:36:41,030
latency profile you get better tail

00:36:37,430 --> 00:36:42,950
latency especially and what it does it

00:36:41,030 --> 00:36:45,860
uses some kind of feedback controller

00:36:42,950 --> 00:36:48,230
from the underlying system and it tries

00:36:45,860 --> 00:36:50,000
to adjust your aperture size to make

00:36:48,230 --> 00:36:52,220
sure that every node inside your

00:36:50,000 --> 00:36:55,250
aperture will be getting constant fold

00:36:52,220 --> 00:36:56,930
between the boundaries you set in the

00:36:55,250 --> 00:36:59,210
very beginning and this is really nice

00:36:56,930 --> 00:37:01,490
because it keeps every single replica

00:36:59,210 --> 00:37:03,380
inside your aperture in the warm state

00:37:01,490 --> 00:37:07,400
so everything is up everything is right

00:37:03,380 --> 00:37:09,530
and it works really really great here's

00:37:07,400 --> 00:37:11,570
so I perch is not default today it's

00:37:09,530 --> 00:37:14,570
sort of like still experimental but you

00:37:11,570 --> 00:37:17,450
can try to enable that in your servers

00:37:14,570 --> 00:37:20,030
and what you can do for example you can

00:37:17,450 --> 00:37:22,970
say I want to make sure that I start

00:37:20,030 --> 00:37:25,430
with aperture size 10 which means 10

00:37:22,970 --> 00:37:27,290
nodes in your cluster set and I want to

00:37:25,430 --> 00:37:30,050
make sure that every single note inside

00:37:27,290 --> 00:37:32,990
my replica set is getting constantly

00:37:30,050 --> 00:37:35,420
getting load between 1 and 2 concurrent

00:37:32,990 --> 00:37:37,760
requests and the aperture will try to

00:37:35,420 --> 00:37:39,830
adjust your window starting from 10 it

00:37:37,760 --> 00:37:41,840
might shrink it my widen that to make

00:37:39,830 --> 00:37:43,940
sure that every single node is getting

00:37:41,840 --> 00:37:45,210
that kind of plot there it is really

00:37:43,940 --> 00:37:49,859
interesting

00:37:45,210 --> 00:37:52,079
the dynamic approach finally so now that

00:37:49,859 --> 00:37:53,970
we know how load bouncers works you want

00:37:52,079 --> 00:37:56,190
to make sure that we exclude some bad

00:37:53,970 --> 00:37:59,670
replicas from your server set and we

00:37:56,190 --> 00:38:02,280
don't set traffic to unreliable nodes on

00:37:59,670 --> 00:38:04,349
your cluster and then we'll use this

00:38:02,280 --> 00:38:06,420
circuit breaker for that and there is a

00:38:04,349 --> 00:38:09,390
layer underneath a load balancer and

00:38:06,420 --> 00:38:11,040
they can disable some of the nodes from

00:38:09,390 --> 00:38:13,200
the sort of like exclude some of the

00:38:11,040 --> 00:38:15,210
notes from replicas set so you don't so

00:38:13,200 --> 00:38:18,260
you don't send traffic to them and we've

00:38:15,210 --> 00:38:20,819
got three of them implemented right now

00:38:18,260 --> 00:38:22,619
the this simplest and the most

00:38:20,819 --> 00:38:25,470
straightforward one is fail-fast which

00:38:22,619 --> 00:38:28,109
basically disabled your session if you

00:38:25,470 --> 00:38:29,790
fail to do TCP connect and try to retry

00:38:28,109 --> 00:38:32,460
this pconnect of the given amount of

00:38:29,790 --> 00:38:35,430
time you can overwrite the back off

00:38:32,460 --> 00:38:37,410
policy for that as well the next one is

00:38:35,430 --> 00:38:39,150
called failure chrome is actually really

00:38:37,410 --> 00:38:41,339
really sparking and the most advanced

00:38:39,150 --> 00:38:43,380
circuit breaker we have today and it has

00:38:41,339 --> 00:38:45,990
like pluggable policies we will see

00:38:43,380 --> 00:38:48,180
later how to configure that and the last

00:38:45,990 --> 00:38:49,859
one is called threshold failure detector

00:38:48,180 --> 00:38:51,960
which is basically a pink based fail

00:38:49,859 --> 00:38:54,869
detector and we support that for

00:38:51,960 --> 00:38:56,670
protocols that support low-level session

00:38:54,869 --> 00:38:59,579
lightness detection signals like mocks

00:38:56,670 --> 00:39:01,319
or HTTP - will support that and the idea

00:38:59,579 --> 00:39:03,869
is really simple you just send a ping

00:39:01,319 --> 00:39:05,819
and measure the latency which takes to

00:39:03,869 --> 00:39:09,990
get a punk from from your from your

00:39:05,819 --> 00:39:11,790
server and you try to to answer your

00:39:09,990 --> 00:39:14,280
question whether or not that latency

00:39:11,790 --> 00:39:16,410
good or bad and if it's bad you just say

00:39:14,280 --> 00:39:18,329
that load that node is unreliable I'm

00:39:16,410 --> 00:39:20,490
going to exclude that from the replica

00:39:18,329 --> 00:39:23,760
set for some time and then I'm going to

00:39:20,490 --> 00:39:26,549
try back and everything is reliable here

00:39:23,760 --> 00:39:28,980
so circuit breakers the junk don't just

00:39:26,549 --> 00:39:31,410
exclude something like for unlimited

00:39:28,980 --> 00:39:33,390
time they try to retry after given

00:39:31,410 --> 00:39:36,480
amount of time so we can go back to

00:39:33,390 --> 00:39:39,569
normal state if if it's possible to do

00:39:36,480 --> 00:39:43,799
and so let's talk about failure curl

00:39:39,569 --> 00:39:45,569
just for a bit so this is the most

00:39:43,799 --> 00:39:47,160
advanced circuit breaker we have and in

00:39:45,569 --> 00:39:50,190
a way that it supports pluggable

00:39:47,160 --> 00:39:53,130
policies you can plug it in to and teach

00:39:50,190 --> 00:39:55,349
it how to how to treat your responses

00:39:53,130 --> 00:39:57,690
and how to and whether and when to

00:39:55,349 --> 00:39:59,040
disable your your session basically and

00:39:57,690 --> 00:40:00,870
right now we have two

00:39:59,040 --> 00:40:03,300
different policies available so the

00:40:00,870 --> 00:40:05,640
first one is success rate based so you

00:40:03,300 --> 00:40:07,440
basically can say what kind of what what

00:40:05,640 --> 00:40:09,270
success rate you expect for them from

00:40:07,440 --> 00:40:11,880
every single node and if it goes below

00:40:09,270 --> 00:40:13,680
that value it will be the replica little

00:40:11,880 --> 00:40:16,650
bit will be disabled will be excluded

00:40:13,680 --> 00:40:18,960
for some time you can also say how many

00:40:16,650 --> 00:40:21,270
consecutive failures you can get from a

00:40:18,960 --> 00:40:23,010
replica and if it's greater than some

00:40:21,270 --> 00:40:26,670
number you can disable that as well and

00:40:23,010 --> 00:40:29,700
by default every by default it uses the

00:40:26,670 --> 00:40:31,470
policy when it's see five failures by

00:40:29,700 --> 00:40:34,920
the same node if you'll disable that and

00:40:31,470 --> 00:40:37,350
goes to do to back off to rename all

00:40:34,920 --> 00:40:39,480
that again and try again and for example

00:40:37,350 --> 00:40:41,070
you can all write that and say I want to

00:40:39,480 --> 00:40:43,740
make sure that success rate is going to

00:40:41,070 --> 00:40:46,620
be at least 95% and if it's less than

00:40:43,740 --> 00:40:48,810
that value the node for every particular

00:40:46,620 --> 00:40:50,760
node so it keeps track of success rate

00:40:48,810 --> 00:40:52,410
for every node in your cluster it will

00:40:50,760 --> 00:40:54,450
be disabled from it will be excluded

00:40:52,410 --> 00:40:56,910
from the server set you'll have balance

00:40:54,450 --> 00:41:01,680
balance over there is really useful

00:40:56,910 --> 00:41:04,710
thing actually so what else

00:41:01,680 --> 00:41:07,350
let's see finagle is actually really

00:41:04,710 --> 00:41:09,210
really rich in terms of ecosystem you've

00:41:07,350 --> 00:41:11,670
got so many different libraries that do

00:41:09,210 --> 00:41:13,800
so many different things for example you

00:41:11,670 --> 00:41:17,580
can do tracing wheel Zipkin which is

00:41:13,800 --> 00:41:19,560
enabled by default you can do HTTP admin

00:41:17,580 --> 00:41:23,160
interface and matrix collect metrics see

00:41:19,560 --> 00:41:25,320
some charts do some profiling see like

00:41:23,160 --> 00:41:27,930
flame grass and stuff like that using

00:41:25,320 --> 00:41:30,240
tor server you can use library coffee

00:41:27,930 --> 00:41:33,030
not sure which is like sort of like HTTP

00:41:30,240 --> 00:41:38,010
framework you can use to build rest

00:41:33,030 --> 00:41:39,810
services on top of Nagle and or even

00:41:38,010 --> 00:41:41,820
there even three services as well and

00:41:39,810 --> 00:41:43,920
you can use something like Finch or

00:41:41,820 --> 00:41:46,710
Cersei type level libraries to build

00:41:43,920 --> 00:41:48,540
sort of like purely functional endpoints

00:41:46,710 --> 00:41:51,540
on top of finagle that will be served

00:41:48,540 --> 00:41:54,690
inside the finagle ecosystem or there is

00:41:51,540 --> 00:41:56,430
a new library called further bat which

00:41:54,690 --> 00:41:58,950
means and you can use that to build sort

00:41:56,430 --> 00:42:02,040
of like type level HTTP clients use a

00:41:58,950 --> 00:42:05,670
shapeless user C as well and make sure

00:42:02,040 --> 00:42:08,040
that you actually doing sort of like

00:42:05,670 --> 00:42:10,500
HTTP REST API calls in a type safe

00:42:08,040 --> 00:42:12,630
manner and there is many many more on on

00:42:10,500 --> 00:42:12,900
the github page so you should feel free

00:42:12,630 --> 00:42:14,970
to

00:42:12,900 --> 00:42:17,880
to go there and check out what an eagle

00:42:14,970 --> 00:42:20,130
can do for you and I hope that was quite

00:42:17,880 --> 00:42:22,380
useful and that's the github page for

00:42:20,130 --> 00:42:25,200
finagle feel free to give it a start

00:42:22,380 --> 00:42:27,870
file a ticket send a few requests we we

00:42:25,200 --> 00:42:28,290
are totally welcoming those thank you

00:42:27,870 --> 00:42:34,770
much

00:42:28,290 --> 00:42:36,990
that's all if you have if you have any

00:42:34,770 --> 00:42:39,990
questions or if you need an Engel

00:42:36,990 --> 00:42:42,120
t-shirt they have some but so but

00:42:39,990 --> 00:42:45,060
first-come first-serve I get just queue

00:42:42,120 --> 00:42:50,510
of them so any any questions so far

00:42:45,060 --> 00:42:50,510
oh yeah sure go ahead

00:42:51,410 --> 00:42:56,370
yeah powers to powers to choices yeah so

00:42:54,750 --> 00:42:57,840
there is a that's really interesting

00:42:56,370 --> 00:43:01,110
there is a computer science paper

00:42:57,840 --> 00:43:03,330
written in 2001 and it says that we are

00:43:01,110 --> 00:43:05,190
trying to utilize some randomized

00:43:03,330 --> 00:43:07,740
algorithm for load balancer and

00:43:05,190 --> 00:43:09,270
basically the idea is if you have a

00:43:07,740 --> 00:43:11,460
large server set you can pick two of

00:43:09,270 --> 00:43:13,530
node of the nodes and then pick the list

00:43:11,460 --> 00:43:16,050
loaded one and if you repeatedly apply

00:43:13,530 --> 00:43:17,760
that like serial several times you will

00:43:16,050 --> 00:43:20,340
get manageable upper bound on your load

00:43:17,760 --> 00:43:21,930
so you won't get a situation when one of

00:43:20,340 --> 00:43:23,850
the node getting like much more traffic

00:43:21,930 --> 00:43:26,060
there is an upper bound on that which is

00:43:23,850 --> 00:43:28,650
actually a great if most logarithm of an

00:43:26,060 --> 00:43:30,810
original and there is like actually um

00:43:28,650 --> 00:43:33,570
sort of a proof for that so it's like

00:43:30,810 --> 00:43:35,040
mathematically stable model and it's

00:43:33,570 --> 00:43:36,780
really useful because you can do that in

00:43:35,040 --> 00:43:42,320
constant time that's why it's so

00:43:36,780 --> 00:43:44,850
powerful power of two choices yeah you

00:43:42,320 --> 00:43:47,600
yeah you just can even you can google

00:43:44,850 --> 00:43:52,970
that same like power of two choices yeah

00:43:47,600 --> 00:43:52,970
this one I really just

00:43:56,750 --> 00:44:02,610
very large data sets like like let's say

00:43:59,460 --> 00:44:07,590
5 10 gigabytes of data or like streaming

00:44:02,610 --> 00:44:12,240
mentions right so you said finagle a

00:44:07,590 --> 00:44:14,340
should be related in them right exactly

00:44:12,240 --> 00:44:16,140
so if an angle is like protocol agnostic

00:44:14,340 --> 00:44:18,090
which means you can build anything you

00:44:16,140 --> 00:44:19,650
want on top of that and we have a bunch

00:44:18,090 --> 00:44:22,290
of different protocols implemented

00:44:19,650 --> 00:44:26,250
already and if you want to do streaming

00:44:22,290 --> 00:44:28,170
for example for like large payloads you

00:44:26,250 --> 00:44:30,830
can totally use HTTP finagle that

00:44:28,170 --> 00:44:33,570
support that you can you can stream HTTP

00:44:30,830 --> 00:44:35,190
chunks you can also use something like

00:44:33,570 --> 00:44:37,950
thrift marks and there is a quite

00:44:35,190 --> 00:44:40,200
exciting recent future feature we did is

00:44:37,950 --> 00:44:42,870
basically you can stream age you can

00:44:40,200 --> 00:44:45,860
stream 3 flex pay award through the max

00:44:42,870 --> 00:44:48,690
connection yeah so basically it's drift

00:44:45,860 --> 00:44:50,520
just regular treif but it works on top

00:44:48,690 --> 00:44:53,100
of that protocol I was describing it's

00:44:50,520 --> 00:44:55,050
called Max and doesn't use like it's

00:44:53,100 --> 00:44:58,020
like a multiplexing protocol it uses one

00:44:55,050 --> 00:45:00,060
connection per end point per per per

00:44:58,020 --> 00:45:01,770
server and you're in your cluster and

00:45:00,060 --> 00:45:03,690
it's like really really powerful in

00:45:01,770 --> 00:45:05,940
terms of like streaming data it can

00:45:03,690 --> 00:45:09,480
ditch hunk your payload and send it like

00:45:05,940 --> 00:45:14,280
in in small chunks if you have a large

00:45:09,480 --> 00:45:16,700
one for example yeah anything else yeah

00:45:14,280 --> 00:45:16,700
go ahead

00:45:18,110 --> 00:45:25,140
this client work with Scala yes I don't

00:45:22,380 --> 00:45:27,090
think so we've never tried to enable

00:45:25,140 --> 00:45:29,130
that I guess there is something you

00:45:27,090 --> 00:45:32,340
should do with your SBT bills right to

00:45:29,130 --> 00:45:33,990
make sure it works yeah we didn't do

00:45:32,340 --> 00:45:35,490
that but we welcome pull requests if you

00:45:33,990 --> 00:45:41,270
want to do that if you want to try

00:45:35,490 --> 00:45:43,680
should feel free yeah anything else I

00:45:41,270 --> 00:45:45,150
guess that's it so feel free to reach

00:45:43,680 --> 00:45:48,010
out if you have any extra questions

00:45:45,150 --> 00:45:50,070
totally happy to chat thank you

00:45:48,010 --> 00:45:50,070

YouTube URL: https://www.youtube.com/watch?v=kfs-dtbG0kY


