Title: Scala: The Unpredicted Lingua Franca for Data Science - by Andy Petrella & Dean Wampler
Publication date: 2016-06-17
Playlist: Scala Days New York 2016
Description: 
	This talk was recorded at Scala Days New York, 2016. Follow along on Twitter @scaladays and on the website for more information http://scaladays.org/.

Abstract:
It was true that, until pretty recently, the language of choice to manipulate and to make sense out of the data for Data Scientists was mainly one of Python, R or Matlab. This lead to split in the communities and duplication of efforts in languages offering a similar set functionnaiity.
Although, it was foreseen that Julia (for instance) could gather parts of these communities, an unexpected event happened: the amount of available data and the distributed technologies to handle them. Distributed technologies raised out of the blue by data engineer and most of them are using a convenient and easy to deploy platform, the JVM.
 
In this talk, we’ll show how the Data Scientists are now part of an heterogeneous team that has to face many problems and have to work towards a global solution together. This is including a new responsibility to be productive and agile in order to have their work integrated into the platform. This is why technologies like Apache Spark is so important nowadays and is gaining this traction from different communities. And even though some binding are available to legacy languages there, all the creativity in new ways to analyse the data has to be done in scala. So that, the second part of this talk will introduce and summarize all the new methodologies and scientific advances in machine learning done Scala as the main language, rather than others. We’ll demonstrate that all using the right tooling for Data Scientists which is enabling interactivity, live reactivity, charting capabilities and robustness in Scala, something there were still missing from the legacy languages. Hence, the examples will be provided and shown in a fully productive and reproducible environment combining the Spark Notebook and Docker.
Captions: 
	00:00:04,600 --> 00:00:10,490
so we are here to show why actually we

00:00:08,360 --> 00:00:14,750
claim that Scala can be the next

00:00:10,490 --> 00:00:16,250
distributed data science language so

00:00:14,750 --> 00:00:18,980
this code this talk will be given by

00:00:16,250 --> 00:00:22,100
myself and the patroller over here the

00:00:18,980 --> 00:00:24,020
monkey with Dean one player over here

00:00:22,100 --> 00:00:26,859
from night band and if you want to

00:00:24,020 --> 00:00:30,710
present yourself with the Hunger Games

00:00:26,859 --> 00:00:32,660
presidents nobody I mean he's there just

00:00:30,710 --> 00:00:40,760
for you the white band is only the day

00:00:32,660 --> 00:00:43,850
job actually I'm right um so yes this

00:00:40,760 --> 00:00:47,299
will data science why distribute this

00:00:43,850 --> 00:00:50,030
data science today is actually the new

00:00:47,299 --> 00:00:52,789
interpretation of Big Data what I meant

00:00:50,030 --> 00:00:55,010
by that is that big data until recently

00:00:52,789 --> 00:00:59,120
was was considered to be only this video

00:00:55,010 --> 00:01:00,619
computing at some point in time and only

00:00:59,120 --> 00:01:04,070
it distributed computing doesn't explain

00:01:00,619 --> 00:01:06,470
why big data is now pretty hot and this

00:01:04,070 --> 00:01:08,510
is also something that I want to explain

00:01:06,470 --> 00:01:10,820
today why Big Data became hot at some

00:01:08,510 --> 00:01:16,840
point and what are the different key

00:01:10,820 --> 00:01:21,020
that we need to have in mind so well so

00:01:16,840 --> 00:01:22,850
now when I to say that this will

00:01:21,020 --> 00:01:26,300
computing is pretty interesting the Big

00:01:22,850 --> 00:01:27,950
Data it doesn't explain why it's very

00:01:26,300 --> 00:01:30,680
important now however this ability

00:01:27,950 --> 00:01:33,500
computing is very whole in some point in

00:01:30,680 --> 00:01:36,170
time because it was using Academy for I

00:01:33,500 --> 00:01:39,920
mean maybe 30 years right so why now

00:01:36,170 --> 00:01:41,870
it's pretty hot I would like you to

00:01:39,920 --> 00:01:43,700
think about that for 30 seconds for

00:01:41,870 --> 00:01:47,150
instance so why did you feel completing

00:01:43,700 --> 00:01:51,409
became at some point Big Data it existed

00:01:47,150 --> 00:01:57,230
and it became big data why distant

00:01:51,409 --> 00:02:01,070
transition happened in the IT world just

00:01:57,230 --> 00:02:05,110
give it a thought for 15 seconds now if

00:02:01,070 --> 00:02:05,110
you like how this transition happened

00:02:09,190 --> 00:02:14,290
so big data is actually could be

00:02:11,770 --> 00:02:17,440
computing not just I mean it's important

00:02:14,290 --> 00:02:20,080
right so the thing is that business it's

00:02:17,440 --> 00:02:22,840
all about business okay so if we think

00:02:20,080 --> 00:02:24,760
about Big Data we think Google right

00:02:22,840 --> 00:02:27,220
but what Google wanted to do initially

00:02:24,760 --> 00:02:30,490
they wanted to business business with

00:02:27,220 --> 00:02:33,280
ads okay and in order to achieve that

00:02:30,490 --> 00:02:35,560
they had to you know be very good in

00:02:33,280 --> 00:02:37,450
performance its accuracy and so on with

00:02:35,560 --> 00:02:40,540
data science in order to be very

00:02:37,450 --> 00:02:42,220
accurate they had to - you know harvest

00:02:40,540 --> 00:02:45,130
the whole information accessible in the

00:02:42,220 --> 00:02:46,780
in the for free which is the web at that

00:02:45,130 --> 00:02:48,820
time so they want to aggregate

00:02:46,780 --> 00:02:50,560
everything to compute things on that and

00:02:48,820 --> 00:02:52,330
then finally discover information in

00:02:50,560 --> 00:02:54,790
there that they can use in order to make

00:02:52,330 --> 00:02:56,920
money out of it okay so they had to to

00:02:54,790 --> 00:03:00,040
process the whole data but why didn't

00:02:56,920 --> 00:03:03,310
they use HPC then you know I prefer his

00:03:00,040 --> 00:03:05,590
computer to do that why did they invent

00:03:03,310 --> 00:03:08,080
it stuff in order to achieve these

00:03:05,590 --> 00:03:10,870
performances actually they did already

00:03:08,080 --> 00:03:14,640
invent it this different theories like

00:03:10,870 --> 00:03:17,290
MapReduce and and Google file system I

00:03:14,640 --> 00:03:20,320
mean that they really didn't really

00:03:17,290 --> 00:03:22,540
invented that because actually what they

00:03:20,320 --> 00:03:26,020
did is that they harmonize the way

00:03:22,540 --> 00:03:28,090
scientists were working with HPC when

00:03:26,020 --> 00:03:30,040
you work with HPC that means that you

00:03:28,090 --> 00:03:31,120
had a big data file then you had to

00:03:30,040 --> 00:03:33,160
split it manually

00:03:31,120 --> 00:03:36,370
then you had to launch processing on

00:03:33,160 --> 00:03:39,640
your cluster of CPU and GPUs and then

00:03:36,370 --> 00:03:42,040
aggregate them manually with basket and

00:03:39,640 --> 00:03:44,350
so on and so forth right so you had to

00:03:42,040 --> 00:03:46,900
do that as the scientist at that time so

00:03:44,350 --> 00:03:49,360
what they wanted to do is to harmonize

00:03:46,900 --> 00:03:53,350
the way you want to work with your

00:03:49,360 --> 00:03:55,930
distributing cluster okay and why did

00:03:53,350 --> 00:03:58,840
they were so popularized because now it

00:03:55,930 --> 00:04:01,120
was simple to do it okay so with this

00:03:58,840 --> 00:04:04,420
immunization it was simple but not only

00:04:01,120 --> 00:04:06,459
simple it was also resilient so this is

00:04:04,420 --> 00:04:08,920
what they added to the to the system the

00:04:06,459 --> 00:04:11,500
resiliency because you know basket are

00:04:08,920 --> 00:04:14,110
not really resilient so they had this

00:04:11,500 --> 00:04:16,239
resiliency to the system with HDFS with

00:04:14,110 --> 00:04:18,820
the Google alpha system and with

00:04:16,239 --> 00:04:20,760
MapReduce and they had to introduce that

00:04:18,820 --> 00:04:23,520
for business again big

00:04:20,760 --> 00:04:25,560
there had to be productive they don't

00:04:23,520 --> 00:04:27,660
want to have scientists restarting the

00:04:25,560 --> 00:04:30,030
full job when it felt or having

00:04:27,660 --> 00:04:31,710
scientists conflicting their jobs on the

00:04:30,030 --> 00:04:33,930
cluster so they wanted to be very

00:04:31,710 --> 00:04:36,960
efficient on that so they do that's why

00:04:33,930 --> 00:04:40,320
they are so they thought about that so

00:04:36,960 --> 00:04:42,120
this is mainly about business so this is

00:04:40,320 --> 00:04:44,450
computing big and big data because of

00:04:42,120 --> 00:04:46,620
business but nothing really changed

00:04:44,450 --> 00:04:51,960
unless the organization that brought

00:04:46,620 --> 00:04:54,510
back brought by by Google so then Hadoop

00:04:51,960 --> 00:04:55,980
ok Hadoop is what for me it's an

00:04:54,510 --> 00:04:58,080
enterprise ready open source

00:04:55,980 --> 00:04:59,550
implementation of these first papers

00:04:58,080 --> 00:05:01,260
right so there are other papers from

00:04:59,550 --> 00:05:04,170
Google that were very interesting to

00:05:01,260 --> 00:05:10,200
check like pre girl or or different

00:05:04,170 --> 00:05:12,000
orders however about Hadoop itself it's

00:05:10,200 --> 00:05:14,970
pretty arresting because it's an opera

00:05:12,000 --> 00:05:18,600
mentation but it's based on the JVM and

00:05:14,970 --> 00:05:21,000
JVM is crucial here why because it

00:05:18,600 --> 00:05:24,000
became something that that can be

00:05:21,000 --> 00:05:27,120
deployed directly in a enterprise

00:05:24,000 --> 00:05:29,190
platform so you can go to a department

00:05:27,120 --> 00:05:31,680
and install Hadoop because it's based on

00:05:29,190 --> 00:05:34,320
the JVM and the system absinthe and so

00:05:31,680 --> 00:05:36,840
on were pretty used to deploy jetty and

00:05:34,320 --> 00:05:39,120
things if you go to a Defense Department

00:05:36,840 --> 00:05:41,010
they have JVM running all over the

00:05:39,120 --> 00:05:43,410
places and and so they can integrate

00:05:41,010 --> 00:05:45,650
Hadoop easily and then you can start

00:05:43,410 --> 00:05:49,320
computing stuff on your whole data set

00:05:45,650 --> 00:05:52,170
however Hadoop you know that has a very

00:05:49,320 --> 00:05:54,620
tough mobile implementation are pretty

00:05:52,170 --> 00:05:57,090
hard so the interfaces and clunky and

00:05:54,620 --> 00:05:59,400
also it's something very important is

00:05:57,090 --> 00:06:01,710
that the first time for the first result

00:05:59,400 --> 00:06:04,040
is very long right the installation is

00:06:01,710 --> 00:06:09,030
not easy especially in a single machine

00:06:04,040 --> 00:06:10,950
and yeah so if you want to write simple

00:06:09,030 --> 00:06:13,020
a simple word count you have to build

00:06:10,950 --> 00:06:16,470
the package and so on so forth right so

00:06:13,020 --> 00:06:18,150
this is why DRI for the business wasn't

00:06:16,470 --> 00:06:20,790
clear and the programmers didn't

00:06:18,150 --> 00:06:23,310
invested a lot of time in there I mean

00:06:20,790 --> 00:06:27,420
the mainstream implement developers like

00:06:23,310 --> 00:06:29,850
yeah majority of the developers be so

00:06:27,420 --> 00:06:32,070
because of these points however spark

00:06:29,850 --> 00:06:33,249
came into the party and a lot of people

00:06:32,070 --> 00:06:35,139
was claiming yeah

00:06:33,249 --> 00:06:37,919
you know spark is one other time faster

00:06:35,139 --> 00:06:39,159
than the Hadoop and I say who cares

00:06:37,919 --> 00:06:42,189
right

00:06:39,159 --> 00:06:44,709
I mean regarding the importance of spark

00:06:42,189 --> 00:06:47,139
in D in the ecosystem this is not this

00:06:44,709 --> 00:06:49,569
factor that was very important for us

00:06:47,139 --> 00:06:51,639
actually what was really interesting

00:06:49,569 --> 00:06:55,089
this park is that they added a new level

00:06:51,639 --> 00:06:56,319
of simplicity on top of Hadoop for

00:06:55,089 --> 00:06:58,839
instance or the spirit computing

00:06:56,319 --> 00:07:00,729
framework that means that now in order

00:06:58,839 --> 00:07:03,099
to have the first result with spark

00:07:00,729 --> 00:07:06,399
it takes a few minutes while with Hadoop

00:07:03,099 --> 00:07:09,219
it took a lot of a lot of time so spark

00:07:06,399 --> 00:07:11,319
gays gay gaining popularity because you

00:07:09,219 --> 00:07:16,119
can install it very easily so it justit

00:07:11,319 --> 00:07:18,519
are you can run the rattle in order to

00:07:16,119 --> 00:07:22,569
do quick experimentations and a

00:07:18,519 --> 00:07:25,479
what-what count is like two lines okay

00:07:22,569 --> 00:07:29,349
so that means that the access to the

00:07:25,479 --> 00:07:33,009
first result is very very fast and also

00:07:29,349 --> 00:07:35,409
the API that you use are very simple and

00:07:33,009 --> 00:07:37,479
they are very looking like the aps that

00:07:35,409 --> 00:07:41,289
you are used to use when you do python

00:07:37,479 --> 00:07:42,729
are Ruby or JavaScript and even Java 8

00:07:41,289 --> 00:07:44,889
now you know this kind of chaining

00:07:42,729 --> 00:07:49,029
functions of I you know high order

00:07:44,889 --> 00:07:51,969
functions so the wanna that factor is

00:07:49,029 --> 00:07:56,559
important but this is no not wise perl

00:07:51,969 --> 00:07:59,409
get this popularity well okay so now

00:07:56,559 --> 00:08:03,879
let's say the Sparky spark added this

00:07:59,409 --> 00:08:07,209
simplicity into the into the party for

00:08:03,879 --> 00:08:09,219
bit data computing that means that some

00:08:07,209 --> 00:08:11,439
engine genies most probably got bored

00:08:09,219 --> 00:08:13,779
because they thought how they think

00:08:11,439 --> 00:08:15,579
still that now it's easy to do this

00:08:13,779 --> 00:08:17,199
video computing thanks to spark so they

00:08:15,579 --> 00:08:19,319
say ok so now I can do this bit

00:08:17,199 --> 00:08:22,629
computing so what can I do next

00:08:19,319 --> 00:08:25,239
well they look around and they say ok so

00:08:22,629 --> 00:08:30,309
we can do data science now so I want to

00:08:25,239 --> 00:08:32,379
become a data scientist well which is

00:08:30,309 --> 00:08:36,180
not always the good move of course but

00:08:32,379 --> 00:08:39,360
anyway the thing is that they are no

00:08:36,180 --> 00:08:42,009
getting back to where Google started

00:08:39,360 --> 00:08:44,259
okay so Google studied this in these

00:08:42,009 --> 00:08:44,829
different theories in order to do the

00:08:44,259 --> 00:08:46,960
science

00:08:44,829 --> 00:08:48,700
and now after maybe 10 years

00:08:46,960 --> 00:08:50,380
we got back to this point and we say now

00:08:48,700 --> 00:08:53,320
yeah so now we can we want to move

00:08:50,380 --> 00:08:55,060
forward I don't mean that at the time of

00:08:53,320 --> 00:08:57,370
Hadoop we didn't do any data science of

00:08:55,060 --> 00:09:00,220
course Mahad what was there very good

00:08:57,370 --> 00:09:02,350
very performant however it had it

00:09:00,220 --> 00:09:04,870
suffered the same problem as Hadoop the

00:09:02,350 --> 00:09:07,900
interface were cranky the implementation

00:09:04,870 --> 00:09:11,980
was hard and and the setup was also

00:09:07,900 --> 00:09:14,560
pretty pretty tough so the thing is that

00:09:11,980 --> 00:09:16,660
now the IT guys are turning back to the

00:09:14,560 --> 00:09:18,250
business and say ok so what do you want

00:09:16,660 --> 00:09:22,960
to do now that we can compute stuff

00:09:18,250 --> 00:09:25,510
easily and originally the sad thing is

00:09:22,960 --> 00:09:27,940
that they quickly made wrong choices

00:09:25,510 --> 00:09:30,610
because business is de copper from IT

00:09:27,940 --> 00:09:35,200
generally and this is something that we

00:09:30,610 --> 00:09:37,990
will cover after what Dean will cover

00:09:35,200 --> 00:09:45,640
now about why Scala is a good language

00:09:37,990 --> 00:09:47,740
now so actually you can get the stuff

00:09:45,640 --> 00:09:50,770
I'm about to demo at this github link

00:09:47,740 --> 00:09:53,050
data fellas is Andy's company Scala for

00:09:50,770 --> 00:09:55,900
data science and there's also a link to

00:09:53,050 --> 00:09:57,850
the slides there so I'm actually using

00:09:55,900 --> 00:09:59,860
something called the SPARC notebook that

00:09:57,850 --> 00:10:03,540
Andy started it's if you've ever worked

00:09:59,860 --> 00:10:06,670
with a notebook environment like ipython

00:10:03,540 --> 00:10:08,890
Mathematica a lot of the data science

00:10:06,670 --> 00:10:11,530
tools that build these notebooks that

00:10:08,890 --> 00:10:13,420
are sort of a you know a computer

00:10:11,530 --> 00:10:16,960
version of the old lab notebooks where

00:10:13,420 --> 00:10:18,880
you can put in formulas draw graphs put

00:10:16,960 --> 00:10:20,170
in notes and so forth I think this is a

00:10:18,880 --> 00:10:22,930
great way to work with the repple

00:10:20,170 --> 00:10:25,450
actually and it's it's if you've ever

00:10:22,930 --> 00:10:28,210
used like the the worksheets in Scala

00:10:25,450 --> 00:10:30,430
IDE or IntelliJ it's kind of similar you

00:10:28,210 --> 00:10:32,980
can go back and edit code rerun it you

00:10:30,430 --> 00:10:34,660
can embed like I said commentary you can

00:10:32,980 --> 00:10:36,220
do graphs of results which unfortunately

00:10:34,660 --> 00:10:38,020
don't have an example of that but that's

00:10:36,220 --> 00:10:39,700
another nice thing if you have data and

00:10:38,020 --> 00:10:42,100
you want to see what it looks like you

00:10:39,700 --> 00:10:43,510
can go ahead and graph it but what I

00:10:42,100 --> 00:10:45,760
want to do actually is walk through some

00:10:43,510 --> 00:10:47,380
bullet points of the things about Scala

00:10:45,760 --> 00:10:50,080
that make it great for doing data

00:10:47,380 --> 00:10:52,120
science I won't cover all of them

00:10:50,080 --> 00:10:54,010
there's actually like a secondary list

00:10:52,120 --> 00:10:56,500
that I won't cover for time some of the

00:10:54,010 --> 00:10:58,750
things in that list are actually really

00:10:56,500 --> 00:11:00,430
valuable for general Scala use but

00:10:58,750 --> 00:11:02,860
actually less important

00:11:00,430 --> 00:11:05,200
in spark use like for example recursion

00:11:02,860 --> 00:11:07,630
and then the tail call optimization in

00:11:05,200 --> 00:11:09,130
Scala actually completely useless when

00:11:07,630 --> 00:11:11,650
you're writing spark code you don't need

00:11:09,130 --> 00:11:15,400
it but it's obviously very nice for

00:11:11,650 --> 00:11:18,400
regular spark regular Scala code so

00:11:15,400 --> 00:11:21,040
anyway these cells that I'm evaluating

00:11:18,400 --> 00:11:25,350
by hitting shift return are basically

00:11:21,040 --> 00:11:28,180
marked down so this is my commentary and

00:11:25,350 --> 00:11:31,120
you know as far as spark goes that

00:11:28,180 --> 00:11:33,130
supports Python or Java and Scala and

00:11:31,120 --> 00:11:35,080
most of data scientists come from like a

00:11:33,130 --> 00:11:37,840
statistics background or whatever and so

00:11:35,080 --> 00:11:39,580
they tend to use Python an AR but

00:11:37,840 --> 00:11:40,570
there's actually some nice advantages of

00:11:39,580 --> 00:11:43,510
using Scala

00:11:40,570 --> 00:11:45,670
instead one of the first advantages is

00:11:43,510 --> 00:11:47,350
the nice balance between object-oriented

00:11:45,670 --> 00:11:48,940
and functional programming so in the

00:11:47,350 --> 00:11:51,070
impression of the choir here but all the

00:11:48,940 --> 00:11:53,350
benefits of functional programming are

00:11:51,070 --> 00:11:54,970
here like immutable values you knows no

00:11:53,350 --> 00:11:56,290
side-effects and all that stuff I won't

00:11:54,970 --> 00:12:00,010
even read this list because you all know

00:11:56,290 --> 00:12:01,390
this list but you might take this list

00:12:00,010 --> 00:12:03,040
and take it back to your colleagues

00:12:01,390 --> 00:12:04,720
they're worse insisting they should do

00:12:03,040 --> 00:12:07,390
Java with spark which I think it's a

00:12:04,720 --> 00:12:09,850
stupidest idea ever well maybe not the

00:12:07,390 --> 00:12:13,570
ever but pretty it doesn't make any

00:12:09,850 --> 00:12:15,670
sense but only other hand we use objects

00:12:13,570 --> 00:12:17,410
for encapsulation your modularity and

00:12:15,670 --> 00:12:19,630
all that stuff so that's it's it's a

00:12:17,410 --> 00:12:20,950
nice we can all argue about the balance

00:12:19,630 --> 00:12:22,480
I mean this is one of the things our

00:12:20,950 --> 00:12:24,160
community loves to do is argue about

00:12:22,480 --> 00:12:26,530
whether Scala is pure enough and all

00:12:24,160 --> 00:12:29,620
that stuff but pragmatically it works

00:12:26,530 --> 00:12:31,150
really well I also have cells in here

00:12:29,620 --> 00:12:33,700
where I compare all right what about

00:12:31,150 --> 00:12:35,860
Python is it does have some functional

00:12:33,700 --> 00:12:38,050
features maybe not as well thought out

00:12:35,860 --> 00:12:39,730
or is more of a math language so it

00:12:38,050 --> 00:12:43,870
doesn't really have the same concepts

00:12:39,730 --> 00:12:45,850
and so forth in Java except for the fact

00:12:43,870 --> 00:12:47,530
they finally added lambdas which is a it

00:12:45,850 --> 00:12:49,150
was a huge step forward there's a whole

00:12:47,530 --> 00:12:52,600
bunch of stuff still missing that we'll

00:12:49,150 --> 00:12:54,130
talk about as we go on here so actually

00:12:52,600 --> 00:12:55,930
let's look at some examples to see how

00:12:54,130 --> 00:12:58,060
concise things can be so I'm going to

00:12:55,930 --> 00:12:59,650
write a helper function to compute it

00:12:58,060 --> 00:13:01,170
for numbers prime I stole this from

00:12:59,650 --> 00:13:03,880
Wikipedia it's not even a very good

00:13:01,170 --> 00:13:06,250
function but it's just here as a helper

00:13:03,880 --> 00:13:08,320
this is actually the first code so when

00:13:06,250 --> 00:13:10,960
I hit return here it's going to compile

00:13:08,320 --> 00:13:13,240
that code this green bar is here because

00:13:10,960 --> 00:13:14,089
they've been running spark jobs it's

00:13:13,240 --> 00:13:15,529
like a status

00:13:14,089 --> 00:13:18,290
at the moment it's just blocking the

00:13:15,529 --> 00:13:20,290
output but not being very helpful but

00:13:18,290 --> 00:13:23,209
all I did was define a function is prime

00:13:20,290 --> 00:13:27,199
now I can use that so now I've got some

00:13:23,209 --> 00:13:29,240
more you know commentary marked down

00:13:27,199 --> 00:13:31,160
here I'm commenting on you know with the

00:13:29,240 --> 00:13:32,629
virtues of the previous function as far

00:13:31,160 --> 00:13:35,300
as you know like no side effects no

00:13:32,629 --> 00:13:37,639
that's sort of stuff but we can also

00:13:35,300 --> 00:13:41,300
write regular Scala code so here just as

00:13:37,639 --> 00:13:43,279
an example I'm going to find all the

00:13:41,300 --> 00:13:45,139
primes between one and a hundred and you

00:13:43,279 --> 00:13:47,480
group them over whether they're prime or

00:13:45,139 --> 00:13:49,759
not and then print out the results print

00:13:47,480 --> 00:13:51,410
out the number of primes versus non

00:13:49,759 --> 00:13:53,149
Prime's and it turns out curiously

00:13:51,410 --> 00:13:56,569
enough there's actually almost exactly

00:13:53,149 --> 00:13:58,699
three to one of non Prime's to primes

00:13:56,569 --> 00:14:01,100
so not a terribly exciting result

00:13:58,699 --> 00:14:03,350
slightly interesting just the how close

00:14:01,100 --> 00:14:05,870
it is to being exactly three to one but

00:14:03,350 --> 00:14:07,819
nevertheless code that hopefully this is

00:14:05,870 --> 00:14:09,230
very recognizable to all of you the

00:14:07,819 --> 00:14:11,149
thing is in a minute I'm going to take

00:14:09,230 --> 00:14:13,699
the third of the three lines that end

00:14:11,149 --> 00:14:16,300
this expression and and do them on the

00:14:13,699 --> 00:14:18,350
spark side without any modifications and

00:14:16,300 --> 00:14:21,110
this is one of the great things that

00:14:18,350 --> 00:14:22,879
sparked the spark program recognized is

00:14:21,110 --> 00:14:25,519
that you know this collection API is

00:14:22,879 --> 00:14:27,679
actually really powerful for data so why

00:14:25,519 --> 00:14:29,540
don't we just use it and maybe adapted

00:14:27,679 --> 00:14:31,730
in ways that we have - like it's lazy

00:14:29,540 --> 00:14:33,649
versus eager and things like that but

00:14:31,730 --> 00:14:35,389
here's a spark program that basically

00:14:33,649 --> 00:14:37,970
does the same thing I'm going to take a

00:14:35,389 --> 00:14:39,949
range I'm gonna paralyze it which means

00:14:37,970 --> 00:14:41,929
turn it into sparks internal data

00:14:39,949 --> 00:14:44,360
structure called a resilient distributed

00:14:41,929 --> 00:14:46,279
data set and in the next three lines or

00:14:44,360 --> 00:14:48,769
exactly what we just had to you know do

00:14:46,279 --> 00:14:51,769
this calculation because it's a lazy

00:14:48,769 --> 00:14:53,959
collection I have to force it to return

00:14:51,769 --> 00:14:56,329
the results and that's what this collect

00:14:53,959 --> 00:14:58,610
method does it actually makes it copies

00:14:56,329 --> 00:15:00,800
it from the RTD back to a Java rather

00:14:58,610 --> 00:15:02,990
Scala collection and that will let

00:15:00,800 --> 00:15:05,179
materialize it but otherwise it won't

00:15:02,990 --> 00:15:07,009
you can there's a little bit of output

00:15:05,179 --> 00:15:08,809
here but there's the result same thing

00:15:07,009 --> 00:15:12,679
that we expected to get notice it comes

00:15:08,809 --> 00:15:14,120
back as an array so we can take what we

00:15:12,679 --> 00:15:16,279
know about you know functional

00:15:14,120 --> 00:15:21,199
programming or Scala in particular and

00:15:16,279 --> 00:15:22,759
actually use it straight away there's a

00:15:21,199 --> 00:15:24,529
bunch of extensions they've added

00:15:22,759 --> 00:15:27,260
there's ways in which they've taken that

00:15:24,529 --> 00:15:29,330
API and made it work in Python and

00:15:27,260 --> 00:15:31,130
are in Java so it's a little alien to

00:15:29,330 --> 00:15:34,460
those language communities but it feels

00:15:31,130 --> 00:15:37,040
right at home for us another important

00:15:34,460 --> 00:15:38,570
benefit over Java in particularly so for

00:15:37,040 --> 00:15:40,280
the other language is is the fact that

00:15:38,570 --> 00:15:41,720
we have an interpreter that we can work

00:15:40,280 --> 00:15:44,150
with them that I'm using right here

00:15:41,720 --> 00:15:46,730
under the hood this is obviously not

00:15:44,150 --> 00:15:48,470
news for us but from people from the

00:15:46,730 --> 00:15:50,360
Java world this is so valuable that I

00:15:48,470 --> 00:15:51,980
can do stuff interactively like this

00:15:50,360 --> 00:15:54,800
whether I'm using a notebook or

00:15:51,980 --> 00:15:56,750
something else use tuples all the time

00:15:54,800 --> 00:15:58,520
this is one of those little minor things

00:15:56,750 --> 00:16:00,770
that we kind of take for granted but a

00:15:58,520 --> 00:16:02,570
lot of code looks like this when you're

00:16:00,770 --> 00:16:04,820
working with rdd's at the bottom here

00:16:02,570 --> 00:16:06,650
let me scroll this up a bit where you're

00:16:04,820 --> 00:16:09,050
gonna treat tuples is like your record

00:16:06,650 --> 00:16:10,820
type and it's just very convenient to be

00:16:09,050 --> 00:16:12,290
able to manipulate the elements on the

00:16:10,820 --> 00:16:14,000
tupple to you know do pattern matching

00:16:12,290 --> 00:16:16,940
which I'll come to in a second is

00:16:14,000 --> 00:16:19,070
another advantage so this becomes really

00:16:16,940 --> 00:16:20,900
useful Python has something similar but

00:16:19,070 --> 00:16:23,030
not quite as nice and job of course

00:16:20,900 --> 00:16:25,940
doesn't have it at all so you end up

00:16:23,030 --> 00:16:27,890
writing crap like this which is you know

00:16:25,940 --> 00:16:30,140
this thing that spark had to add because

00:16:27,890 --> 00:16:31,910
Java doesn't have any sort of tupple

00:16:30,140 --> 00:16:34,700
type so they made up their own immutable

00:16:31,910 --> 00:16:37,310
pair type two element tuples key values

00:16:34,700 --> 00:16:38,780
are the most common tuples who use and

00:16:37,310 --> 00:16:41,060
this is how you would do it in the you

00:16:38,780 --> 00:16:44,810
know the spark Java API which is you

00:16:41,060 --> 00:16:46,370
know just noise but of course pattern

00:16:44,810 --> 00:16:48,500
matching is something else that's sort

00:16:46,370 --> 00:16:50,630
of dual to tuples in the sense that it's

00:16:48,500 --> 00:16:52,820
you know I can put stuff in records now

00:16:50,630 --> 00:16:54,920
I need to rip it apart and there's no

00:16:52,820 --> 00:16:57,080
better way to do it and doing pattern

00:16:54,920 --> 00:16:59,240
matching so here it's the same program

00:16:57,080 --> 00:17:01,250
but now I'm using like useful names for

00:16:59,240 --> 00:17:03,110
the fields I'm throwing away things I

00:17:01,250 --> 00:17:07,910
don't care about all just by using

00:17:03,110 --> 00:17:09,290
pattern matching and then I describe

00:17:07,910 --> 00:17:11,320
what's going on so there's a lot of

00:17:09,290 --> 00:17:13,880
details in the notebook here that are

00:17:11,320 --> 00:17:15,350
familiar to us but maybe less familiar

00:17:13,880 --> 00:17:17,300
to people coming from other languages

00:17:15,350 --> 00:17:18,829
and then the fact that we can do pattern

00:17:17,300 --> 00:17:20,720
matching at art you know arbitrarily

00:17:18,829 --> 00:17:22,610
deep levels here I just did an

00:17:20,720 --> 00:17:27,070
assignment where I ripped apart a nested

00:17:22,610 --> 00:17:27,070
tupple into its constituents

00:17:27,089 --> 00:17:30,860
case classes are an interesting beast

00:17:28,950 --> 00:17:33,450
they also are really good for

00:17:30,860 --> 00:17:35,340
representing records as well when you

00:17:33,450 --> 00:17:37,440
want a name and not just a position like

00:17:35,340 --> 00:17:39,299
you would have with a tupple what's

00:17:37,440 --> 00:17:41,700
interesting about case classes though is

00:17:39,299 --> 00:17:44,640
that we're moving in spark towards very

00:17:41,700 --> 00:17:46,350
very optimized in-memory representations

00:17:44,640 --> 00:17:48,780
of data which I'll show you an example

00:17:46,350 --> 00:17:50,490
of in a second and in a way you really

00:17:48,780 --> 00:17:52,140
don't want to instantiate a case class

00:17:50,490 --> 00:17:54,330
for every record if you have a billion

00:17:52,140 --> 00:17:56,220
of them or something but it is a great

00:17:54,330 --> 00:17:57,960
way to think about the schema and only

00:17:56,220 --> 00:17:59,730
sort of move away from something that's

00:17:57,960 --> 00:18:03,450
a little bit more record oriented not

00:17:59,730 --> 00:18:05,190
like data structure oriented later on if

00:18:03,450 --> 00:18:06,780
for performance but for right now it's a

00:18:05,190 --> 00:18:09,360
great way to think about like a person

00:18:06,780 --> 00:18:11,340
type or something and spark integrates

00:18:09,360 --> 00:18:15,059
very nicely with this to let you use

00:18:11,340 --> 00:18:16,860
these as record types I describe what

00:18:15,059 --> 00:18:19,070
the case keyword does that's obviously

00:18:16,860 --> 00:18:21,840
something you guys know but here I'm

00:18:19,070 --> 00:18:25,049
sort of synthesized as an example where

00:18:21,840 --> 00:18:29,309
I created two person objects with

00:18:25,049 --> 00:18:30,840
made-up ages for Andy and myself maybe

00:18:29,309 --> 00:18:33,600
you're 29 I don't know I'm definitely

00:18:30,840 --> 00:18:35,309
not 39 and then just did once again

00:18:33,600 --> 00:18:38,730
doing pattern matching on it to tease it

00:18:35,309 --> 00:18:40,500
apart and actually putting out tuples

00:18:38,730 --> 00:18:42,659
again just as an example of how you can

00:18:40,500 --> 00:18:45,840
manipulate the data very easily to

00:18:42,659 --> 00:18:48,659
convert it to the format you want type

00:18:45,840 --> 00:18:50,250
inference this this makes yeah we all

00:18:48,659 --> 00:18:52,110
know this but boy there's nothing like

00:18:50,250 --> 00:18:53,730
having the types when you want them but

00:18:52,110 --> 00:18:56,429
having them disappear in the background

00:18:53,730 --> 00:18:58,020
when you don't so you can infer type so

00:18:56,429 --> 00:19:00,600
obviously this is a trivial example of

00:18:58,020 --> 00:19:02,520
inferring integers but even things like

00:19:00,600 --> 00:19:04,049
just having to tell you what it inferred

00:19:02,520 --> 00:19:06,030
so you don't have to know exactly what

00:19:04,049 --> 00:19:07,710
this in particular method returns but

00:19:06,030 --> 00:19:10,230
just by seeing what it returns now I

00:19:07,710 --> 00:19:11,970
know that oh I just got a RTD and it

00:19:10,230 --> 00:19:15,809
came out with this data structure for

00:19:11,970 --> 00:19:17,730
the records so we just don't have to put

00:19:15,809 --> 00:19:19,320
in a lot of the usual type annotations

00:19:17,730 --> 00:19:21,390
we have with Java but we get these

00:19:19,320 --> 00:19:23,580
really useful messages about what we

00:19:21,390 --> 00:19:26,220
just did which is fantastic when you're

00:19:23,580 --> 00:19:28,710
learning this API whenever I've used the

00:19:26,220 --> 00:19:30,900
Python spark API I inevitably find

00:19:28,710 --> 00:19:32,309
myself printing the first few records so

00:19:30,900 --> 00:19:34,320
I know what I ended up with because I'm

00:19:32,309 --> 00:19:35,880
not sure you know there's no information

00:19:34,320 --> 00:19:36,840
from Python that tells me what I just

00:19:35,880 --> 00:19:39,539
created

00:19:36,840 --> 00:19:44,490
so I really like that balance of type

00:19:39,539 --> 00:19:46,409
safety as well as expressiveness a

00:19:44,490 --> 00:19:48,600
couple a couple last points and then

00:19:46,409 --> 00:19:49,860
we'll switch back over to Andy as we all

00:19:48,600 --> 00:19:52,409
know it's a great tool for building

00:19:49,860 --> 00:19:54,480
domain-specific languages and a specific

00:19:52,409 --> 00:19:57,179
example in SPARC is the way they have

00:19:54,480 --> 00:20:00,990
supported the notion of doing sequel

00:19:57,179 --> 00:20:02,999
like queries this is a wrap of so you

00:20:00,990 --> 00:20:05,490
could call it an API on top of the core

00:20:02,999 --> 00:20:07,230
called the data frames API and it's

00:20:05,490 --> 00:20:09,360
managed by this thing called a sequel

00:20:07,230 --> 00:20:11,340
context so here I'm just doing some

00:20:09,360 --> 00:20:14,789
imports and setting up some stuff that I

00:20:11,340 --> 00:20:16,950
need but and also I'm going to load some

00:20:14,789 --> 00:20:18,389
data here let me see and this is all set

00:20:16,950 --> 00:20:20,399
up so we've got this environment

00:20:18,389 --> 00:20:24,029
variable that we defined when I started

00:20:20,399 --> 00:20:26,399
this thing and yeah what am I doing here

00:20:24,029 --> 00:20:28,440
I'm getting the current path actually it

00:20:26,399 --> 00:20:30,360
turns I don't need that line anymore but

00:20:28,440 --> 00:20:32,519
here's some data that's like it turns

00:20:30,360 --> 00:20:34,409
out you can download all a data set for

00:20:32,519 --> 00:20:36,629
all of the airline public airline

00:20:34,409 --> 00:20:39,509
flights and commercial aviation for the

00:20:36,629 --> 00:20:41,070
last 15 or 20 years it's rather

00:20:39,509 --> 00:20:43,710
interesting to play with if you fly a

00:20:41,070 --> 00:20:48,119
lot like I do but this particular one is

00:20:43,710 --> 00:20:50,190
a JSON file that has all data about all

00:20:48,119 --> 00:20:52,950
the airports in the country and this is

00:20:50,190 --> 00:20:55,289
convenient API here I'm using the sequel

00:20:52,950 --> 00:20:58,019
context so just read Jace and it parses

00:20:55,289 --> 00:21:00,210
it it infers the schema you can see that

00:20:58,019 --> 00:21:01,889
it's printing the schema below here as a

00:21:00,210 --> 00:21:03,779
result so that's really nice it does

00:21:01,889 --> 00:21:06,029
this with other file formats like parque

00:21:03,779 --> 00:21:08,580
as well those of you who worked in

00:21:06,029 --> 00:21:10,230
Hadoop know what parque is caching tells

00:21:08,580 --> 00:21:12,210
SPARC keep this in memory because I'm

00:21:10,230 --> 00:21:13,710
gonna keep reusing this data and then

00:21:12,210 --> 00:21:15,269
there's this nice show routine that

00:21:13,710 --> 00:21:16,769
prints out something that looks like if

00:21:15,269 --> 00:21:18,869
you've ever used a database

00:21:16,769 --> 00:21:21,119
you know interpreter you get this sort

00:21:18,869 --> 00:21:23,669
of table format when you look at the

00:21:21,119 --> 00:21:25,619
first in this case 20 records so here's

00:21:23,669 --> 00:21:28,860
the first 20 airports that showed up in

00:21:25,619 --> 00:21:30,090
that list but now we can write sequel

00:21:28,860 --> 00:21:32,100
like queries you can either you can

00:21:30,090 --> 00:21:33,869
either embed a sequel query in a string

00:21:32,100 --> 00:21:35,220
which is we all know is bad because it's

00:21:33,869 --> 00:21:37,049
string ly typed right at least that's

00:21:35,220 --> 00:21:39,210
what we were told in computer science

00:21:37,049 --> 00:21:40,980
school or whatever or we can do this

00:21:39,210 --> 00:21:42,389
sort of DSL it's kind of the point of

00:21:40,980 --> 00:21:44,759
this section is that we can write

00:21:42,389 --> 00:21:46,379
expressive DSL is pretty nicely than

00:21:44,759 --> 00:21:49,019
encapsulate our domain knowledge and

00:21:46,379 --> 00:21:49,710
this is very close to a Python DSL that

00:21:49,019 --> 00:21:52,440
was in spray

00:21:49,710 --> 00:21:54,330
this API but here I'm just doing a group

00:21:52,440 --> 00:21:56,640
by over the state and country for these

00:21:54,330 --> 00:21:58,529
airports and then basically ordering by

00:21:56,640 --> 00:22:01,500
descending count how many airports are

00:21:58,529 --> 00:22:04,799
in each state basically and it turns out

00:22:01,500 --> 00:22:06,390
Alaska has most of them I think because

00:22:04,799 --> 00:22:09,059
it has lots of little tiny airports

00:22:06,390 --> 00:22:12,570
actually did it sort miss me

00:22:09,059 --> 00:22:14,460
oh I scrolled up too far but um yeah

00:22:12,570 --> 00:22:17,039
didn't actually run that one yet here we

00:22:14,460 --> 00:22:19,080
go so it showed me the schema that's

00:22:17,039 --> 00:22:21,690
this print schema thing and here's okay

00:22:19,080 --> 00:22:23,370
here it is so Alaska has quite a lot of

00:22:21,690 --> 00:22:25,020
little airports scattered out in the

00:22:23,370 --> 00:22:30,059
backcountry in Texas has a lot

00:22:25,020 --> 00:22:34,440
California and so forth so we can do

00:22:30,059 --> 00:22:36,630
sequel queries and what am I doing here

00:22:34,440 --> 00:22:37,830
I just just sorry ran it again I guess

00:22:36,630 --> 00:22:39,600
alright then there's some comments about

00:22:37,830 --> 00:22:41,399
Python and so forth all right I'm gonna

00:22:39,600 --> 00:22:42,840
skip the rest of these like I said

00:22:41,399 --> 00:22:45,029
there's some things that we really love

00:22:42,840 --> 00:22:46,909
about Scala that are not as important in

00:22:45,029 --> 00:22:49,919
spark but are generally useful like

00:22:46,909 --> 00:22:52,020
recursion you don't really use it much

00:22:49,919 --> 00:22:53,880
in spark but a lot of these other little

00:22:52,020 --> 00:22:56,070
features like no semicolons string

00:22:53,880 --> 00:22:57,840
interpolation or things that we all know

00:22:56,070 --> 00:23:00,779
and love that just make our lives a lot

00:22:57,840 --> 00:23:04,520
better so with that I'll go back into

00:23:00,779 --> 00:23:08,970
presentation mode here and let Andy

00:23:04,520 --> 00:23:10,770
finish up cool so yeah this this

00:23:08,970 --> 00:23:13,140
notebook is awesome because I mean we

00:23:10,770 --> 00:23:14,610
can take it go to the data scientist and

00:23:13,140 --> 00:23:17,010
show all the great stuff that they can

00:23:14,610 --> 00:23:19,710
use mascara and actually if you look at

00:23:17,010 --> 00:23:22,020
it it's not so far from that for from

00:23:19,710 --> 00:23:23,640
Python there is no lambda everywhere

00:23:22,020 --> 00:23:26,340
that's the good part of it I would say

00:23:23,640 --> 00:23:27,960
yeah this is lambda Hal right and

00:23:26,340 --> 00:23:32,100
JavaScript you have the callback hell in

00:23:27,960 --> 00:23:34,980
Python you have the lambda hell so yeah

00:23:32,100 --> 00:23:37,890
I recommend everybody to go to this to

00:23:34,980 --> 00:23:41,429
this github repository to check it out

00:23:37,890 --> 00:23:43,590
test it and to to make recommendation PR

00:23:41,429 --> 00:23:46,649
whatever I mean this is a good tool for

00:23:43,590 --> 00:23:48,600
us to demonstrate that Scala is actually

00:23:46,649 --> 00:23:51,000
a good language to go for with data

00:23:48,600 --> 00:23:54,360
science and especially with distributed

00:23:51,000 --> 00:23:55,890
data science so now the scanner is the

00:23:54,360 --> 00:24:00,570
distributed data science so if you

00:23:55,890 --> 00:24:03,860
to be very harsh well so Dana science is

00:24:00,570 --> 00:24:09,300
actually what brings real added value

00:24:03,860 --> 00:24:12,090
for the data right that means that we

00:24:09,300 --> 00:24:15,780
can now create models that can work with

00:24:12,090 --> 00:24:19,140
distributed data and so we can run

00:24:15,780 --> 00:24:21,810
algorithms on whatever size of data we

00:24:19,140 --> 00:24:23,760
have for speed or or yeah or variability

00:24:21,810 --> 00:24:27,540
you know this I don't know how many of

00:24:23,760 --> 00:24:30,860
these we are now and the fact is

00:24:27,540 --> 00:24:33,570
actually Dana scientist not new

00:24:30,860 --> 00:24:36,030
distributed computing wasn't new and

00:24:33,570 --> 00:24:38,040
Dana sense neither right actually the

00:24:36,030 --> 00:24:40,920
last science is something which is used

00:24:38,040 --> 00:24:42,900
for edges and there are even businesses

00:24:40,920 --> 00:24:45,840
that are completely relying on data

00:24:42,900 --> 00:24:47,120
science to be profitable if you think

00:24:45,840 --> 00:24:49,770
about insurance

00:24:47,120 --> 00:24:51,900
what is insurance they don't have a

00:24:49,770 --> 00:24:54,390
hardware they don't have software they

00:24:51,900 --> 00:24:56,310
just have your data and then make money

00:24:54,390 --> 00:24:57,630
out of it okay this is the whole the

00:24:56,310 --> 00:24:59,340
only thing that they have they don't

00:24:57,630 --> 00:25:02,580
even take your money to deal with that

00:24:59,340 --> 00:25:04,650
they just need your money and maybe help

00:25:02,580 --> 00:25:07,040
you when you have a problem so based on

00:25:04,650 --> 00:25:11,190
your data they can make money because

00:25:07,040 --> 00:25:13,440
your yourse care about the future right

00:25:11,190 --> 00:25:16,550
so in order to do that they have to

00:25:13,440 --> 00:25:19,380
analyze your data efficiently accurately

00:25:16,550 --> 00:25:22,920
otherwise they might screw themselves

00:25:19,380 --> 00:25:25,290
right so the data sign is not new so

00:25:22,920 --> 00:25:31,200
what's the difference now between big

00:25:25,290 --> 00:25:34,950
data of yesterday and today there is one

00:25:31,200 --> 00:25:38,610
thing that I I think is that actually

00:25:34,950 --> 00:25:42,840
now the ownership of big data being data

00:25:38,610 --> 00:25:47,100
science now is is moving from IT teams

00:25:42,840 --> 00:25:50,400
or IT hands to data science teams ok so

00:25:47,100 --> 00:25:53,910
this is what happens now this is pretty

00:25:50,400 --> 00:25:56,010
sad I would say and the thing is since

00:25:53,910 --> 00:25:58,320
this is this ownership is moving and

00:25:56,010 --> 00:26:00,330
with these tools this you know

00:25:58,320 --> 00:26:03,660
distributed computing framework

00:26:00,330 --> 00:26:06,050
distributing clusters and so on the Dana

00:26:03,660 --> 00:26:08,120
senses are feeling a little bit

00:26:06,050 --> 00:26:10,340
miss kaswell so they they put them

00:26:08,120 --> 00:26:12,170
selling the defensive position saying

00:26:10,340 --> 00:26:14,120
yeah actually we were running this

00:26:12,170 --> 00:26:16,550
insurance company for years so we don't

00:26:14,120 --> 00:26:18,650
really need this stuff okay so we were

00:26:16,550 --> 00:26:22,970
already accurate right so otherwise how

00:26:18,650 --> 00:26:24,620
could we be profitable since then so

00:26:22,970 --> 00:26:25,940
they put them on the defensive so what

00:26:24,620 --> 00:26:28,280
they do is generally they say okay so

00:26:25,940 --> 00:26:30,560
now I want to have proofs of evidence

00:26:28,280 --> 00:26:33,590
that learning these technologies and

00:26:30,560 --> 00:26:36,680
systems are worth right hey sue is what

00:26:33,590 --> 00:26:39,050
yeah so that means that they're entered

00:26:36,680 --> 00:26:41,450
this exploration phase and they will hit

00:26:39,050 --> 00:26:44,540
two barriers merely the first barrier

00:26:41,450 --> 00:26:46,580
will be the to chain okay the to chain

00:26:44,540 --> 00:26:48,260
is involving a lot of different things

00:26:46,580 --> 00:26:52,960
like Sandra Park

00:26:48,260 --> 00:26:56,750
hdfs Park a we just heard about it and

00:26:52,960 --> 00:26:58,610
and also creating programs that you

00:26:56,750 --> 00:27:00,440
deploy somewhere that you ask a

00:26:58,610 --> 00:27:02,660
scheduler to run and things like these

00:27:00,440 --> 00:27:04,160
and then you go to the console in seven

00:27:02,660 --> 00:27:05,990
machines in order to see what's going

00:27:04,160 --> 00:27:07,850
around and then you have sterilization

00:27:05,990 --> 00:27:09,290
problems and and so on and so forth

00:27:07,850 --> 00:27:11,420
right so this is the tool chain that

00:27:09,290 --> 00:27:13,940
they have in their hands now so this is

00:27:11,420 --> 00:27:16,400
upsetting for them where they can just

00:27:13,940 --> 00:27:18,920
start a Python console create a sample

00:27:16,400 --> 00:27:21,140
of the data I don't know how and then

00:27:18,920 --> 00:27:22,520
running a model that they you know they

00:27:21,140 --> 00:27:24,140
can import the package from Mahara

00:27:22,520 --> 00:27:28,160
Python so this is the first barrier that

00:27:24,140 --> 00:27:30,560
they hit the second barrier so of course

00:27:28,160 --> 00:27:34,220
now if you go to Python or whatever so

00:27:30,560 --> 00:27:36,440
they have point-and-click UI so drag and

00:27:34,220 --> 00:27:40,250
drop with boxes with coding it or

00:27:36,440 --> 00:27:43,370
whatever and this is what they like the

00:27:40,250 --> 00:27:45,290
second barrier is the language so Python

00:27:43,370 --> 00:27:48,530
are this these are the language that are

00:27:45,290 --> 00:27:52,460
used to use and they want to keep their

00:27:48,530 --> 00:27:54,380
Abbot's so when we say okay so maybe

00:27:52,460 --> 00:27:56,990
it's not the right way to go you know

00:27:54,380 --> 00:28:01,640
spar for instance is implemented in

00:27:56,990 --> 00:28:06,080
scala and maybe it's a good it's a good

00:28:01,640 --> 00:28:10,030
move to use Cala either the problem is

00:28:06,080 --> 00:28:13,010
that if dues borrow those barriers are

00:28:10,030 --> 00:28:14,870
influencing the toolkit that you want to

00:28:13,010 --> 00:28:17,060
use in your team then you will hit a

00:28:14,870 --> 00:28:20,449
productive program so they say that if

00:28:17,060 --> 00:28:22,909
you have if they have to use Canada 1b

00:28:20,449 --> 00:28:24,949
what I says that if they use they don't

00:28:22,909 --> 00:28:28,539
use scanner the whole team won't be

00:28:24,949 --> 00:28:30,649
predictive okay

00:28:28,539 --> 00:28:32,629
so why do I say that

00:28:30,649 --> 00:28:36,079
so maybe I need to define now what is a

00:28:32,629 --> 00:28:38,119
team then if you think about spark

00:28:36,079 --> 00:28:42,409
how do Cassandra Kefka and so on and so

00:28:38,119 --> 00:28:44,659
forth how do those frameworks or our

00:28:42,409 --> 00:28:50,089
tools are actually implemented on the

00:28:44,659 --> 00:28:53,359
JVM okay and the thing is that if you

00:28:50,089 --> 00:28:54,979
has no the dissent is to work on these

00:28:53,359 --> 00:28:56,779
things they will say that they have a

00:28:54,979 --> 00:28:59,239
product C program right so the pretty

00:28:56,779 --> 00:29:01,789
people will drop down because they they

00:28:59,239 --> 00:29:06,709
want to have the right rivalries to use

00:29:01,789 --> 00:29:08,239
them so and and if they want to use if

00:29:06,709 --> 00:29:09,739
they have to use JDM they will say to

00:29:08,239 --> 00:29:11,690
other things that we say yeah but my

00:29:09,739 --> 00:29:14,829
models aren't implemented in Java or

00:29:11,690 --> 00:29:18,739
scanner so I cannot use I don't know

00:29:14,829 --> 00:29:20,599
easily a random forest or in a random

00:29:18,739 --> 00:29:23,479
forest is like the Silver Bullet for

00:29:20,599 --> 00:29:26,539
them for most of them which is most of

00:29:23,479 --> 00:29:30,609
the time wrong anyway and also again the

00:29:26,539 --> 00:29:34,069
tooling are not very

00:29:30,609 --> 00:29:36,199
I cannot work smoothly with the current

00:29:34,069 --> 00:29:39,139
tooling I don't want to use IntelliJ I

00:29:36,199 --> 00:29:40,909
don't want to use eclipse right I want

00:29:39,139 --> 00:29:43,699
to use a notebook for instance right

00:29:40,909 --> 00:29:47,299
like the one that you just see so the

00:29:43,699 --> 00:29:50,239
thing is that again so if we introduce

00:29:47,299 --> 00:29:54,109
this this different concept into our

00:29:50,239 --> 00:29:57,319
team then we will hit a predictive

00:29:54,109 --> 00:30:01,369
program so what is a team a team is a

00:29:57,319 --> 00:30:04,609
mix of data scientists and engineers so

00:30:01,369 --> 00:30:07,009
if you remember like maybe three or four

00:30:04,609 --> 00:30:09,169
years ago so we integrated the C's ups

00:30:07,009 --> 00:30:10,999
into the IT engineer esteem right

00:30:09,169 --> 00:30:14,149
because it makes sense to have those

00:30:10,999 --> 00:30:16,879
guys or to working all together into the

00:30:14,149 --> 00:30:18,649
same direction so actually what I would

00:30:16,879 --> 00:30:21,289
say is that Dana scientists have to be

00:30:18,649 --> 00:30:24,529
part of the same of the same team

00:30:21,289 --> 00:30:26,839
why because at that time they have only

00:30:24,529 --> 00:30:30,079
one goal is that to create one data

00:30:26,839 --> 00:30:32,449
product altogether otherwise they might

00:30:30,079 --> 00:30:33,059
you know go in one direction each of

00:30:32,449 --> 00:30:36,629
them and

00:30:33,059 --> 00:30:40,289
you have to always you know solve fights

00:30:36,629 --> 00:30:41,999
internally and thing like this so what I

00:30:40,289 --> 00:30:45,269
said that actually a Dana scientists

00:30:41,999 --> 00:30:47,999
nowadays is its work is to produce two

00:30:45,269 --> 00:30:49,740
things a job that has to run in a

00:30:47,999 --> 00:30:52,019
distributed computing framer because it

00:30:49,740 --> 00:30:53,789
uses dispute data so this is one thing

00:30:52,019 --> 00:30:56,460
and the other thing that he has to

00:30:53,789 --> 00:30:59,330
produce is he has to produce is a

00:30:56,460 --> 00:31:03,509
service that has to be consumed by Mercy

00:30:59,330 --> 00:31:05,549
applications so just returning a PDF

00:31:03,509 --> 00:31:07,470
with a few plots and a few tables it's

00:31:05,549 --> 00:31:09,840
not you know it's not worth any more I

00:31:07,470 --> 00:31:12,629
mean it doesn't help anymore because the

00:31:09,840 --> 00:31:15,210
data is moving too fast the the

00:31:12,629 --> 00:31:17,340
information evolves as well too fast it

00:31:15,210 --> 00:31:19,320
changes only we need to be agile and

00:31:17,340 --> 00:31:21,149
reactive with the way we are dealing

00:31:19,320 --> 00:31:24,389
with the data so that means that we need

00:31:21,149 --> 00:31:26,220
to deploy services that can that are

00:31:24,389 --> 00:31:30,509
enabling predictive function but also

00:31:26,220 --> 00:31:33,269
that are giving a an interface a layer a

00:31:30,509 --> 00:31:34,919
view layer onto the data which were the

00:31:33,269 --> 00:31:36,509
sources the output and the intermediate

00:31:34,919 --> 00:31:38,639
steps so this is what they have to

00:31:36,509 --> 00:31:43,710
produce that mean that they they should

00:31:38,639 --> 00:31:45,179
be able to write spray code I mean they

00:31:43,710 --> 00:31:47,220
come from Python they are they have to

00:31:45,179 --> 00:31:49,230
write spray code even us I mean say that

00:31:47,220 --> 00:31:52,559
sometimes it's a bit hard to to write

00:31:49,230 --> 00:31:54,600
this code and I mean that's why we they

00:31:52,559 --> 00:31:56,909
need to be you know in very close

00:31:54,600 --> 00:31:58,289
relationship with somebody that can

00:31:56,909 --> 00:32:00,210
write this kind of work and that can

00:31:58,289 --> 00:32:11,129
deploy this job into production

00:32:00,210 --> 00:32:13,409
otherwise they will just fight so well

00:32:11,129 --> 00:32:16,980
yeah actually so the team is it has to

00:32:13,409 --> 00:32:20,429
be you know everybody working on the

00:32:16,980 --> 00:32:22,799
same thing otherwise if we have people

00:32:20,429 --> 00:32:26,070
working on Python one side let's say so

00:32:22,799 --> 00:32:30,659
I have this script is pretty produces

00:32:26,070 --> 00:32:32,369
this feature this feature tables it's

00:32:30,659 --> 00:32:34,259
really neat and then I can predict that

00:32:32,369 --> 00:32:37,200
onto that so it's a pretty good break as

00:32:34,259 --> 00:32:38,429
Python script so now please deploy it so

00:32:37,200 --> 00:32:40,230
the guys we said yeah but I don't know

00:32:38,429 --> 00:32:41,789
how to deploy that I don't even know

00:32:40,230 --> 00:32:43,080
what kind of output you I don't know

00:32:41,789 --> 00:32:43,370
what is the scheme I don't know what is

00:32:43,080 --> 00:32:45,410
it

00:32:43,370 --> 00:32:46,730
and so on so forth so they we say we

00:32:45,410 --> 00:32:48,530
will have to rewrite that

00:32:46,730 --> 00:32:52,910
you know into something that we can

00:32:48,530 --> 00:32:54,680
deploy okay so I mean rewriting is

00:32:52,910 --> 00:32:57,770
always a bad thing and also you know

00:32:54,680 --> 00:33:00,140
most of the models aren't still

00:32:57,770 --> 00:33:01,880
available on the JVM anyway but we can

00:33:00,140 --> 00:33:04,400
say now we have but you know is parked

00:33:01,880 --> 00:33:06,950
we have PI spark we have spark our we

00:33:04,400 --> 00:33:08,900
have data frame and when I say is that

00:33:06,950 --> 00:33:12,470
yeah okay but they are just late api's

00:33:08,900 --> 00:33:15,080
they are very limited and and in some

00:33:12,470 --> 00:33:18,530
sense they're not even not Enterprise

00:33:15,080 --> 00:33:23,180
ready yet why because these are layers

00:33:18,530 --> 00:33:25,430
there are layers onto the rail engine

00:33:23,180 --> 00:33:27,470
which is color what does that mean it

00:33:25,430 --> 00:33:31,030
means that if you want to be able to

00:33:27,470 --> 00:33:34,190
access the last the last cutting etching

00:33:31,030 --> 00:33:36,050
models implemented in Scala you have to

00:33:34,190 --> 00:33:38,600
use Cal is very inspiring you have to

00:33:36,050 --> 00:33:41,840
use scatter Y because this is where they

00:33:38,600 --> 00:33:44,330
are strengthening the the models they

00:33:41,840 --> 00:33:46,220
are starting at the API is they are you

00:33:44,330 --> 00:33:48,650
know full proofing the code and so on so

00:33:46,220 --> 00:33:52,070
forth so only when it's done the exposed

00:33:48,650 --> 00:33:53,480
is API in Python and are okay so the

00:33:52,070 --> 00:33:55,640
real engine is scanner the other are

00:33:53,480 --> 00:33:57,620
just API with sometimes and most of the

00:33:55,640 --> 00:33:59,870
time right I would say services or

00:33:57,620 --> 00:34:02,420
layers connected to them that can break

00:33:59,870 --> 00:34:05,270
and for sure it breaks all the time you

00:34:02,420 --> 00:34:06,680
know so you would say okay but Dana from

00:34:05,270 --> 00:34:09,910
is a little bit different because then

00:34:06,680 --> 00:34:14,180
you have this kind of DSL so it's a DSL

00:34:09,910 --> 00:34:16,490
that is ionizing the way you can work

00:34:14,180 --> 00:34:19,190
with the engine behind but it's talking

00:34:16,490 --> 00:34:21,830
a DSL right so it opens some doors but

00:34:19,190 --> 00:34:25,000
it closes a lot of doors when you do

00:34:21,830 --> 00:34:27,410
machine learning or or you know

00:34:25,000 --> 00:34:30,410
optimization you have to go a little bit

00:34:27,410 --> 00:34:32,840
deeper and how to do it in SPARC is by

00:34:30,410 --> 00:34:33,470
creating two things either you create

00:34:32,840 --> 00:34:36,170
UDF's

00:34:33,470 --> 00:34:39,680
or you use map partitions I don't know

00:34:36,170 --> 00:34:41,390
who knows map partitions okay so my

00:34:39,680 --> 00:34:44,390
partitions is just a function that we

00:34:41,390 --> 00:34:47,900
run not only on one item but on a bunch

00:34:44,390 --> 00:34:50,900
of item and this bunch of item is a data

00:34:47,900 --> 00:34:53,330
partition okay that means that you know

00:34:50,900 --> 00:34:54,860
you can do local optimization by using

00:34:53,330 --> 00:34:56,060
array for instance where you can

00:34:54,860 --> 00:34:58,370
aggregate

00:34:56,060 --> 00:35:00,080
some computation and then you can pass

00:34:58,370 --> 00:35:02,480
it to the next steps so this is an

00:35:00,080 --> 00:35:04,550
optimization which is regularly done in

00:35:02,480 --> 00:35:07,310
machine learning but if you do that in

00:35:04,550 --> 00:35:09,470
Python are then you will suffer two

00:35:07,310 --> 00:35:12,290
things first you will have to get the

00:35:09,470 --> 00:35:13,730
data from the JVM and gene behind so

00:35:12,290 --> 00:35:15,650
there is something to do is you have to

00:35:13,730 --> 00:35:17,300
sterilize them then you have to read

00:35:15,650 --> 00:35:19,940
them transpose them into Python or

00:35:17,300 --> 00:35:23,600
whatever and then you have to execute a

00:35:19,940 --> 00:35:25,250
function that will run onto the JVM on

00:35:23,600 --> 00:35:29,510
the our environment so you will suffer

00:35:25,250 --> 00:35:31,550
for from a perform you will suffer the

00:35:29,510 --> 00:35:33,350
performances of these systems so that

00:35:31,550 --> 00:35:35,390
means that you are stuck by using what

00:35:33,350 --> 00:35:38,300
has been defined so far so you are kind

00:35:35,390 --> 00:35:39,920
of a monkey first X that's what I said

00:35:38,300 --> 00:35:41,900
so if you want to stick with that then

00:35:39,920 --> 00:35:43,970
there is a high chance that you won't be

00:35:41,900 --> 00:35:45,770
able to do exactly what you want to do

00:35:43,970 --> 00:35:51,890
with the exact performance you want to

00:35:45,770 --> 00:35:54,290
do now so what is missing and the JVM

00:35:51,890 --> 00:35:56,990
and Scala specifically in order to have

00:35:54,290 --> 00:35:59,090
those guys happy I would say that the

00:35:56,990 --> 00:36:02,330
first thing is to hollow them to be able

00:35:59,090 --> 00:36:04,510
to interactively work on the data and

00:36:02,330 --> 00:36:07,070
this is why we have these notebooks

00:36:04,510 --> 00:36:13,120
however the notebooks needs to be you

00:36:07,070 --> 00:36:15,350
know already you know they there must be

00:36:13,120 --> 00:36:16,940
enterprise ready I will say that so you

00:36:15,350 --> 00:36:19,460
can deploy it on the current

00:36:16,940 --> 00:36:22,310
infrastructure with no pain and also

00:36:19,460 --> 00:36:23,900
they should be able to run the code that

00:36:22,310 --> 00:36:26,090
has been created in the notebook

00:36:23,900 --> 00:36:28,520
directly on to the cluster otherwise you

00:36:26,090 --> 00:36:30,050
still have this transition between the

00:36:28,520 --> 00:36:32,360
notebook which is a simple page with

00:36:30,050 --> 00:36:34,070
markdown and code to a rare project and

00:36:32,360 --> 00:36:39,050
run in production so this is one thing

00:36:34,070 --> 00:36:41,060
that we need to enable for them yeah so

00:36:39,050 --> 00:36:44,510
the other things is the models of course

00:36:41,060 --> 00:36:48,020
so - are has a long history and we have

00:36:44,510 --> 00:36:49,400
also julia which is a late comer where

00:36:48,020 --> 00:36:51,830
there are a lot of model have been

00:36:49,400 --> 00:36:54,590
implemented the thing is that now we

00:36:51,830 --> 00:36:57,470
should be able to implement them pretty

00:36:54,590 --> 00:36:59,480
easily onto the JVM of course if we have

00:36:57,470 --> 00:37:02,570
to implement everything is going to be

00:36:59,480 --> 00:37:05,180
to take quite some time but we should at

00:37:02,570 --> 00:37:07,460
first produce at least on the you know

00:37:05,180 --> 00:37:08,359
the few models that are very interesting

00:37:07,460 --> 00:37:10,310
for

00:37:08,359 --> 00:37:13,099
for what we need to do you know if you

00:37:10,310 --> 00:37:15,470
want to do business or whatever so if

00:37:13,099 --> 00:37:18,619
there may be hundreds of models that

00:37:15,470 --> 00:37:20,660
very worth implementing right now so you

00:37:18,619 --> 00:37:22,430
shouldn't take too much time to to cover

00:37:20,660 --> 00:37:25,369
that and actually ma how did a great job

00:37:22,430 --> 00:37:27,230
because it rich pretty much is level but

00:37:25,369 --> 00:37:33,290
then you have to stick with huddle for

00:37:27,230 --> 00:37:36,080
this one right so and however what we

00:37:33,290 --> 00:37:39,349
can do now in the JVM and Scala and what

00:37:36,080 --> 00:37:41,060
they they are not ready yet on Python

00:37:39,349 --> 00:37:43,480
and our work for instance is that we

00:37:41,060 --> 00:37:46,760
start now creating models this directly

00:37:43,480 --> 00:37:48,470
distributable you know they are still

00:37:46,760 --> 00:37:50,150
working on the local machine when they

00:37:48,470 --> 00:37:52,040
create models because they want to stick

00:37:50,150 --> 00:37:53,990
with Python are so they don't have this

00:37:52,040 --> 00:37:57,109
capability to optimize locally so they

00:37:53,990 --> 00:37:59,930
still doing jobs on on local systems so

00:37:57,109 --> 00:38:03,380
they are inventing system to create very

00:37:59,930 --> 00:38:06,680
good sample or they are optimizing at

00:38:03,380 --> 00:38:09,560
the very low level watching photo or or

00:38:06,680 --> 00:38:11,839
C++ code but I mean this is still local

00:38:09,560 --> 00:38:13,369
optimization so when we can do with the

00:38:11,839 --> 00:38:15,640
Scala with the API that we have in mind

00:38:13,369 --> 00:38:19,040
and with databases like Cassandra and

00:38:15,640 --> 00:38:23,750
streaming like Kafka we can go very very

00:38:19,040 --> 00:38:25,790
far that they can go and the thing is

00:38:23,750 --> 00:38:28,160
that there are a lot of initiative that

00:38:25,790 --> 00:38:30,830
we should follow and we should come with

00:38:28,160 --> 00:38:33,440
others so I would say that I've shot

00:38:30,830 --> 00:38:35,810
listed a few one there that I'm used to

00:38:33,440 --> 00:38:37,790
there is D planning for J which is a

00:38:35,810 --> 00:38:40,160
very great tool but to to run deep

00:38:37,790 --> 00:38:42,440
learning algorithms on to the JVM but

00:38:40,160 --> 00:38:45,080
also on spark for instance and tomorrow

00:38:42,440 --> 00:38:46,609
I will give a talk with sky mine with

00:38:45,080 --> 00:38:51,550
which is the company on top of the

00:38:46,609 --> 00:38:55,310
effigy how to run an LP using the our 4G

00:38:51,550 --> 00:38:56,230
op tml is a very good project at

00:38:55,310 --> 00:38:59,720
Stanford

00:38:56,230 --> 00:39:02,570
it's a machine learning DSN let's say in

00:38:59,720 --> 00:39:05,660
Scala they have implemented quite a

00:39:02,570 --> 00:39:07,820
bunch of models already there I'm

00:39:05,660 --> 00:39:10,940
working with the Paris innovative Paris

00:39:07,820 --> 00:39:17,089
13 on different algorithms G stream

00:39:10,940 --> 00:39:19,910
which is a groaner al stream model min

00:39:17,089 --> 00:39:22,900
shift with LSH self-organizing map I'm

00:39:19,910 --> 00:39:26,210
actually those models are able to

00:39:22,900 --> 00:39:28,369
discover new cluster on streaming data

00:39:26,210 --> 00:39:30,170
you know it's not like k-means you know

00:39:28,369 --> 00:39:31,730
when we think about cluster and

00:39:30,170 --> 00:39:34,220
supervised we think about k-means and

00:39:31,730 --> 00:39:35,660
then you have to fix k actually here we

00:39:34,220 --> 00:39:38,900
don't have to fix k anymore so this

00:39:35,660 --> 00:39:40,910
hyper parameter can be evolving with the

00:39:38,900 --> 00:39:42,710
time of course we have ml leap from

00:39:40,910 --> 00:39:44,359
spark and we have all the friends on

00:39:42,710 --> 00:39:46,640
spec but gadget that you need to check

00:39:44,359 --> 00:39:49,190
the spy notebook is there for the for

00:39:46,640 --> 00:39:52,040
the interactive sessions and figaro this

00:39:49,190 --> 00:39:54,320
is one of my favorite figure i'm a graph

00:39:52,040 --> 00:39:56,150
guy and figure is the mix of machine

00:39:54,320 --> 00:39:58,670
learning and graph because it's a

00:39:56,150 --> 00:40:01,339
probabilistic programming language and

00:39:58,670 --> 00:40:03,530
you can do very fancy stuff i mean this

00:40:01,339 --> 00:40:05,780
is what's checking we're not there yet

00:40:03,530 --> 00:40:08,060
because the way to optimize this kind of

00:40:05,780 --> 00:40:11,089
model is still tough it still takes a

00:40:08,060 --> 00:40:13,490
lot of computation time but I mean this

00:40:11,089 --> 00:40:16,430
is one area when where I would like to

00:40:13,490 --> 00:40:19,520
work for the coming years I mean it's a

00:40:16,430 --> 00:40:21,230
good mix of craft computing theory and

00:40:19,520 --> 00:40:22,790
machine learning and what I've

00:40:21,230 --> 00:40:24,950
discovered recently is that there are

00:40:22,790 --> 00:40:28,520
many universities around the world are

00:40:24,950 --> 00:40:29,720
starting teaching Scala directly to

00:40:28,520 --> 00:40:32,900
their data scientist

00:40:29,720 --> 00:40:34,520
why because if they think that their

00:40:32,900 --> 00:40:36,320
students have to have a great job

00:40:34,520 --> 00:40:39,770
afterwards not only creating great

00:40:36,320 --> 00:40:42,109
papers what is hot now is to create new

00:40:39,770 --> 00:40:44,660
models on distributed data systems right

00:40:42,109 --> 00:40:46,150
and as I said creating new models means

00:40:44,660 --> 00:40:48,589
that you have to write that in scanner

00:40:46,150 --> 00:40:50,599
so if they have to understand how the

00:40:48,589 --> 00:40:52,280
current models have been implemented

00:40:50,599 --> 00:40:54,830
they have to look into the scanner code

00:40:52,280 --> 00:40:56,510
if they want to create new models they

00:40:54,830 --> 00:40:58,609
have to write the code into scale I

00:40:56,510 --> 00:41:00,530
don't mean that they are writing very

00:40:58,609 --> 00:41:02,810
beautiful scanner code but at least it's

00:41:00,530 --> 00:41:04,220
Kara it can be predicted easily if you

00:41:02,810 --> 00:41:07,040
need to if you want to check the de

00:41:04,220 --> 00:41:10,070
Parys code they're the three projects

00:41:07,040 --> 00:41:11,839
the code in Scala is is not that bad but

00:41:10,070 --> 00:41:13,700
it's not so good to I mean there are a

00:41:11,839 --> 00:41:16,099
few things that we we see that they are

00:41:13,700 --> 00:41:18,560
coming from part in their heart but this

00:41:16,099 --> 00:41:20,000
is all it has to to help them I mean

00:41:18,560 --> 00:41:21,999
they are very clever they they move very

00:41:20,000 --> 00:41:24,129
fast as well so there is no

00:41:21,999 --> 00:41:25,869
problem with that so this is exciting

00:41:24,129 --> 00:41:27,429
moments for us and this is why this

00:41:25,869 --> 00:41:30,399
notebook that I've been written by Dean

00:41:27,429 --> 00:41:32,379
as to has to be you know reused time to

00:41:30,399 --> 00:41:36,729
time in order to see you to show you

00:41:32,379 --> 00:41:39,179
know you should you should move into the

00:41:36,729 --> 00:41:39,179

YouTube URL: https://www.youtube.com/watch?v=3_oV25nZz8I


