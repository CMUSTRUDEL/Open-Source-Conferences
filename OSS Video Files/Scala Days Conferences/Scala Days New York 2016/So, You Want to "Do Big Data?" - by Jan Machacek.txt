Title: So, You Want to "Do Big Data?" - by Jan Machacek
Publication date: 2016-06-17
Playlist: Scala Days New York 2016
Description: 
	This talk was recorded at Scala Days New York, 2016. Follow along on Twitter @scaladays and on the website for more information http://scaladays.org/.

Abstract:
Jan's talk will give overview of the major components in a ML connected fitness system built on the Lightbend platform. Jan will then show the most exciting aspects of the Akka and Spark code: The Akka cluster and CQRS to handle the volume of users; the Spark code to compute the models for the users. Naturally, the talk will include the infrastructure code for continuous deployment and monitoring of the entire system. As usual, expect a nerve-wracking live demos! Jan closes with lessons learnt in maintaining and evolving the system, helping you avoid the typical pitfalls in Akka & Spark systems, and hopefully motivating you to "have a go" at designing and implementing similarly complex systems yourselves!
Captions: 
	00:00:00,000 --> 00:00:08,309
what we're going to do today and in this

00:00:03,750 --> 00:00:11,820
talk is start with a demo and then it

00:00:08,309 --> 00:00:14,759
will be a long winded discussion of two

00:00:11,820 --> 00:00:16,500
years of learning essentially what have

00:00:14,759 --> 00:00:19,529
we found how are we constructed this

00:00:16,500 --> 00:00:24,529
application and what might you do to

00:00:19,529 --> 00:00:24,529
improve it see if I can improve this

00:00:24,590 --> 00:00:32,219
very carefully okay how about now

00:00:30,349 --> 00:00:37,170
perfect all right all right all right

00:00:32,219 --> 00:00:42,300
okay so let's do a daring demo so I have

00:00:37,170 --> 00:00:45,360
a couple of things one I have to so

00:00:42,300 --> 00:00:48,600
these are ibeacons find location they

00:00:45,360 --> 00:00:53,039
will help our classification and I have

00:00:48,600 --> 00:00:55,800
a couple of sensors so an Apple watch

00:00:53,039 --> 00:00:57,600
with some private api's I know that's

00:00:55,800 --> 00:01:01,680
naughty and Apple wouldn't approve but

00:00:57,600 --> 00:01:05,280
we needed it and I have a pebble so that

00:01:01,680 --> 00:01:08,400
gives more real-time information we are

00:01:05,280 --> 00:01:10,920
able to write code that actually sends

00:01:08,400 --> 00:01:14,729
the data immediately well we are we have

00:01:10,920 --> 00:01:16,770
to buffer for one second so we record

00:01:14,729 --> 00:01:19,530
once and second of accelerometer and

00:01:16,770 --> 00:01:25,020
gyroscope data and that's what gets sent

00:01:19,530 --> 00:01:29,360
to the mobile application all right so

00:01:25,020 --> 00:01:32,549
now that this is all nearly set up okay

00:01:29,360 --> 00:01:35,250
so we're ready to go this is the

00:01:32,549 --> 00:01:37,259
connection is this is like a remote

00:01:35,250 --> 00:01:42,890
connection let's see how that works no

00:01:37,259 --> 00:01:47,670
too bad not too shabby let me start it

00:01:42,890 --> 00:01:49,799
okay so what we're gonna do is see what

00:01:47,670 --> 00:01:52,290
it does and then we'll talk about how

00:01:49,799 --> 00:01:54,750
it's built and how we've gone about

00:01:52,290 --> 00:01:59,299
building it and what were the mistakes

00:01:54,750 --> 00:02:02,759
and how it went ready all right let's go

00:01:59,299 --> 00:02:07,020
so it's nice and simple right we run the

00:02:02,759 --> 00:02:09,509
app and it does exercise / physiotherapy

00:02:07,020 --> 00:02:11,370
so we went through a number of

00:02:09,509 --> 00:02:12,690
iterations particularly when it came to

00:02:11,370 --> 00:02:14,610
sensors

00:02:12,690 --> 00:02:18,000
you know what can we get in real time

00:02:14,610 --> 00:02:21,030
and what else can we get from the

00:02:18,000 --> 00:02:23,670
support in the gym so we have a

00:02:21,030 --> 00:02:27,960
statistical model of what the user does

00:02:23,670 --> 00:02:30,030
so if I wander down to a location where

00:02:27,960 --> 00:02:32,790
i can start exercise and grab a weight

00:02:30,030 --> 00:02:35,340
imaginary wait do a setup movement as

00:02:32,790 --> 00:02:37,110
soon as I do that this detected that i'm

00:02:35,340 --> 00:02:40,890
holding a weight imaginary weight very

00:02:37,110 --> 00:02:42,930
heavy I can start exercising and all

00:02:40,890 --> 00:02:47,220
that it does is record all the

00:02:42,930 --> 00:02:49,320
accelerometer traces and provide you

00:02:47,220 --> 00:02:52,650
know the user some feedback of what to

00:02:49,320 --> 00:02:55,230
do what wait to get and how many

00:02:52,650 --> 00:02:57,390
repetitions to do I have to stop moving

00:02:55,230 --> 00:02:59,880
and then we'll say okay here's what

00:02:57,390 --> 00:03:02,400
you've done suppose it was 15 kilos you

00:02:59,880 --> 00:03:04,050
must have been right what I also got was

00:03:02,400 --> 00:03:06,690
immediate confirmation I'm there at this

00:03:04,050 --> 00:03:08,700
right I'm gonna wander over here so the

00:03:06,690 --> 00:03:10,080
beacon are now away from the beacon I'm

00:03:08,700 --> 00:03:13,140
going to walk over to this one right

00:03:10,080 --> 00:03:16,020
let's do something a bit bit heavier so

00:03:13,140 --> 00:03:18,570
it should pick it up any minute now come

00:03:16,020 --> 00:03:21,030
on there we go right so it picks up that

00:03:18,570 --> 00:03:23,670
and it really wants me to do this so

00:03:21,030 --> 00:03:25,530
let's get set up right there might be a

00:03:23,670 --> 00:03:29,100
delay let's get going all right I'm

00:03:25,530 --> 00:03:32,880
ready should be getting a boat ready to

00:03:29,100 --> 00:03:35,070
do it and if we go right one and so on

00:03:32,880 --> 00:03:37,950
count with me Rises the ridiculous I

00:03:35,070 --> 00:03:40,200
feel completely completely ridiculous

00:03:37,950 --> 00:03:42,360
but it feels great why this is so light

00:03:40,200 --> 00:03:45,030
if you exercise in the gym with the real

00:03:42,360 --> 00:03:51,239
deal you'll find this completely magical

00:03:45,030 --> 00:03:54,959
all right nearly done okay last one all

00:03:51,239 --> 00:03:57,650
right and done so again opportunity to

00:03:54,959 --> 00:04:00,530
provide enough I'm glad that this worked

00:03:57,650 --> 00:04:02,970
let's not push my luck any further so

00:04:00,530 --> 00:04:04,620
that's what it does it's nice and simple

00:04:02,970 --> 00:04:07,050
right I didn't have to touch anything

00:04:04,620 --> 00:04:12,150
all i have to do was walk around an

00:04:07,050 --> 00:04:14,310
exercise how easy so totally about a

00:04:12,150 --> 00:04:16,950
statistical model of user exercise of

00:04:14,310 --> 00:04:19,560
the user behavior humans are beautifully

00:04:16,950 --> 00:04:22,109
predictable animals right so when

00:04:19,560 --> 00:04:24,990
someone exercises they usually do the

00:04:22,109 --> 00:04:26,100
same thing all the time so it's really

00:04:24,990 --> 00:04:28,440
easy to build

00:04:26,100 --> 00:04:31,410
transition model from the different

00:04:28,440 --> 00:04:33,420
exercises so when I stand start an

00:04:31,410 --> 00:04:36,390
exercise session that targets I don't

00:04:33,420 --> 00:04:38,100
know arms and legs and then H knows that

00:04:36,390 --> 00:04:41,960
I typically start with exercise eighth

00:04:38,100 --> 00:04:45,720
and that I 15 times move over to be and

00:04:41,960 --> 00:04:48,570
from be three times stay at B and one

00:04:45,720 --> 00:04:51,060
time go to today and so on so that is

00:04:48,570 --> 00:04:52,200
the initial cut of predictions you know

00:04:51,060 --> 00:04:55,110
what am I going to do this is a

00:04:52,200 --> 00:04:58,080
suggestion do this next well that's

00:04:55,110 --> 00:05:00,570
sometimes not sufficient so I have this

00:04:58,080 --> 00:05:03,930
least two ibeacons this fine location so

00:05:00,570 --> 00:05:05,610
when I go over here it is alright so you

00:05:03,930 --> 00:05:07,890
are likely you could have been doing a b

00:05:05,610 --> 00:05:10,110
c and d but now that I'm standing here

00:05:07,890 --> 00:05:12,840
it says okay we're going to be doing

00:05:10,110 --> 00:05:15,330
just a or C there might be some other

00:05:12,840 --> 00:05:17,070
A's and some other seas but you know it

00:05:15,330 --> 00:05:19,890
gets filtered down depending on what

00:05:17,070 --> 00:05:21,660
kind of exercise I'm about to do okay

00:05:19,890 --> 00:05:24,330
well that's kind of good right so

00:05:21,660 --> 00:05:26,070
suppose I now have two options of what

00:05:24,330 --> 00:05:29,700
to do when I was here right I had two

00:05:26,070 --> 00:05:33,750
options and then finally I have the

00:05:29,700 --> 00:05:35,370
sensor data so there are a load of

00:05:33,750 --> 00:05:37,920
papers that you can download that talk

00:05:35,370 --> 00:05:40,830
about classification of exercise and

00:05:37,920 --> 00:05:42,990
they usually require long time windows

00:05:40,830 --> 00:05:45,480
so you're looking at five seconds maybe

00:05:42,990 --> 00:05:48,510
eight seconds worth of data and that's

00:05:45,480 --> 00:05:50,220
completely insufficient when the

00:05:48,510 --> 00:05:52,860
feedback has to be immediate right so I

00:05:50,220 --> 00:05:55,740
had to do this and the thing had to say

00:05:52,860 --> 00:05:57,600
get ready now I couldn't be doing eight

00:05:55,740 --> 00:06:01,050
seconds worth of movement and then Freud

00:05:57,600 --> 00:06:02,370
say oh I see you're now exercising no

00:06:01,050 --> 00:06:05,670
I've started eight seconds ago just

00:06:02,370 --> 00:06:08,700
because they sit all broken so the

00:06:05,670 --> 00:06:09,810
combination of these three factors if

00:06:08,700 --> 00:06:11,400
you think about it writes the

00:06:09,810 --> 00:06:12,930
combination of typical statistical

00:06:11,400 --> 00:06:16,620
movement of the user what does the user

00:06:12,930 --> 00:06:19,710
do typically plus fine location plus as

00:06:16,620 --> 00:06:21,150
many senses you can get on someone so I

00:06:19,710 --> 00:06:24,180
started with two so hard right

00:06:21,150 --> 00:06:27,180
accelerometer accelerometer gyroscope we

00:06:24,180 --> 00:06:30,450
have other experiments in fact i'll talk

00:06:27,180 --> 00:06:33,540
about this fully wired human and a fully

00:06:30,450 --> 00:06:36,150
wired human has in fact i'll tell you

00:06:33,540 --> 00:06:39,390
later it's very exciting all right so

00:06:36,150 --> 00:06:39,840
how do we get there it was a long road i

00:06:39,390 --> 00:06:44,580
start

00:06:39,840 --> 00:06:47,880
at scala exchange two years ago and I

00:06:44,580 --> 00:06:49,979
had a secure acs application in anaka so

00:06:47,880 --> 00:06:51,990
i had all the user profiles at cluster

00:06:49,979 --> 00:06:54,479
shouting I had a vent sourcing Martin

00:06:51,990 --> 00:06:57,750
talked a lot about you know using that

00:06:54,479 --> 00:07:00,210
treating all the events as streams it

00:06:57,750 --> 00:07:02,490
had rest api for user profiles it had a

00:07:00,210 --> 00:07:06,389
mobile application that all worked

00:07:02,490 --> 00:07:08,790
reasonably well and it did this

00:07:06,389 --> 00:07:12,150
real-time ingestion of accelerometer

00:07:08,790 --> 00:07:14,610
data ha see now that's that's where

00:07:12,150 --> 00:07:16,380
everything failed but anyway you know

00:07:14,610 --> 00:07:17,700
all the pro singles on the server with

00:07:16,380 --> 00:07:20,610
the Apple push notification to the app

00:07:17,700 --> 00:07:24,870
so what worked well clearly CTR SES

00:07:20,610 --> 00:07:27,150
anagha perfect rest api not a problem we

00:07:24,870 --> 00:07:30,120
had swift and Swift said any guesses

00:07:27,150 --> 00:07:33,270
what is swift Zed now come on it's

00:07:30,120 --> 00:07:34,860
calles a drive but in Swift we had said

00:07:33,270 --> 00:07:37,110
in sedate I ingestion and decoding with

00:07:34,860 --> 00:07:40,860
s codec which was really quite cool so

00:07:37,110 --> 00:07:43,020
these guys and big endian 13 bits signed

00:07:40,860 --> 00:07:46,410
integers we need to decode them in some

00:07:43,020 --> 00:07:48,389
sensible way s codec was fantastic there

00:07:46,410 --> 00:07:53,210
was no actual classification everything

00:07:48,389 --> 00:07:56,490
else was cool though so typical ARCA

00:07:53,210 --> 00:07:59,880
cluster shutting setup so backed by

00:07:56,490 --> 00:08:02,430
Cassandra journal so to do this user

00:07:59,880 --> 00:08:04,590
profile and use a profile processor all

00:08:02,430 --> 00:08:07,080
I had to do was create a cluster chard

00:08:04,590 --> 00:08:09,300
for the for the shouted actor and I've

00:08:07,080 --> 00:08:12,060
got to care of distributing it across

00:08:09,300 --> 00:08:14,940
all the nodes it took care of in all the

00:08:12,060 --> 00:08:18,210
details of routing messages to it if I

00:08:14,940 --> 00:08:20,340
needed an actor that was just local on

00:08:18,210 --> 00:08:23,610
an actor system it was even easier right

00:08:20,340 --> 00:08:27,419
not a problem this sort of thing right

00:08:23,610 --> 00:08:28,950
api is every all work and then that

00:08:27,419 --> 00:08:32,010
might be an exaggeration we should all

00:08:28,950 --> 00:08:34,680
have written a service in spray or a

00:08:32,010 --> 00:08:38,969
HTTP even if you've done Ruby on Rails

00:08:34,680 --> 00:08:43,110
this is readable all right so this is a

00:08:38,969 --> 00:08:46,050
service that handles some HTTP requests

00:08:43,110 --> 00:08:49,290
it uses the user exercises and user

00:08:46,050 --> 00:08:51,709
exercises view as actors and you know

00:08:49,290 --> 00:08:53,700
this is actually a really pretty simple

00:08:51,709 --> 00:08:56,520
anything that starts with X

00:08:53,700 --> 00:08:59,790
I followed by an identify for a user and

00:08:56,520 --> 00:09:02,190
identify for session when there's a get

00:08:59,790 --> 00:09:04,950
request we complete it by asking the

00:09:02,190 --> 00:09:08,820
user exercises view for a response to

00:09:04,950 --> 00:09:11,970
user get exercises readable right not

00:09:08,820 --> 00:09:13,380
nothing too bad on a put we handle this

00:09:11,970 --> 00:09:16,110
bit vector now this was the ingestion

00:09:13,380 --> 00:09:19,830
right so the mobile phone every time it

00:09:16,110 --> 00:09:21,450
had a it tried to bother but it tried to

00:09:19,830 --> 00:09:24,090
send all the data from the sensors

00:09:21,450 --> 00:09:26,790
immediately for processing it encoded

00:09:24,090 --> 00:09:29,250
them as bit vectors this is from s codec

00:09:26,790 --> 00:09:31,680
and all we had to do in this user

00:09:29,250 --> 00:09:34,860
exercise this processor which was a

00:09:31,680 --> 00:09:37,260
persistent actor and it's a login all

00:09:34,860 --> 00:09:40,260
that to do is decode it now in here you

00:09:37,260 --> 00:09:43,590
can also see how we typically go about

00:09:40,260 --> 00:09:47,190
switching the state right so we start by

00:09:43,590 --> 00:09:50,010
not exercising so the receive behavior

00:09:47,190 --> 00:09:51,300
by default is not exercising right so

00:09:50,010 --> 00:09:53,370
presumably someone would have to press

00:09:51,300 --> 00:09:56,820
the buttons as I'm exercising now right

00:09:53,370 --> 00:09:59,040
so it would go from not exercising kind

00:09:56,820 --> 00:10:01,680
of behavior to this exercising behavior

00:09:59,040 --> 00:10:04,920
and when we had this put request that

00:10:01,680 --> 00:10:08,070
received all the data we will decode all

00:10:04,920 --> 00:10:10,080
and on left on error which LOL you you

00:10:08,070 --> 00:10:11,940
said to me an HTTP four hundred this is

00:10:10,080 --> 00:10:15,480
some stuff that I don't know to decode

00:10:11,940 --> 00:10:18,540
and on HDD on success right so if we

00:10:15,480 --> 00:10:20,160
decoded all these bits we sent it to

00:10:18,540 --> 00:10:22,590
these exercise classifiers and

00:10:20,160 --> 00:10:26,640
presumably they spit out the actual

00:10:22,590 --> 00:10:28,470
exercise all right so when I talked

00:10:26,640 --> 00:10:30,690
about these thirteen bit things this

00:10:28,470 --> 00:10:33,690
this is what happens on pebble I it's a

00:10:30,690 --> 00:10:36,210
tiny machine you have like 18 kilobytes

00:10:33,690 --> 00:10:38,970
for your code and memory kilobytes you

00:10:36,210 --> 00:10:40,860
don't have a lot of space so what we

00:10:38,970 --> 00:10:42,860
have to do well this is how we decode it

00:10:40,860 --> 00:10:46,530
right so we first decode the header

00:10:42,860 --> 00:10:48,510
magic gives us an idea of em D&S little

00:10:46,530 --> 00:10:51,090
big endian little endian can look at the

00:10:48,510 --> 00:10:54,990
order of bytes and then we can decode

00:10:51,090 --> 00:10:57,630
the rest of the staff now 13 13 13 39

00:10:54,990 --> 00:11:00,930
bits plus 1 padding so we have 5 bytes

00:10:57,630 --> 00:11:03,300
per sample this is just enough to

00:11:00,930 --> 00:11:05,230
squeeze through reliably through a

00:11:03,300 --> 00:11:08,200
bluetooth low energy connection

00:11:05,230 --> 00:11:10,990
so that's kind of cool right and then

00:11:08,200 --> 00:11:12,820
the data this this bid vector got sent

00:11:10,990 --> 00:11:17,890
to exercise classifier this is what we

00:11:12,820 --> 00:11:19,600
had at scala exchange that right now is

00:11:17,890 --> 00:11:26,980
kind of cool right I'll do it again i'll

00:11:19,600 --> 00:11:28,990
show you in full horror we had that Oh a

00:11:26,980 --> 00:11:31,120
timeout do you want to hear a TCP junk

00:11:28,990 --> 00:11:35,730
anyway don't do this right this is a

00:11:31,120 --> 00:11:38,230
terrible idea okay so we fixed that so

00:11:35,730 --> 00:11:40,810
we need at least we thought let's do

00:11:38,230 --> 00:11:41,920
this actual classification so we did

00:11:40,810 --> 00:11:44,830
horrible things I me that were they were

00:11:41,920 --> 00:11:47,320
cool but if you're thinking about doing

00:11:44,830 --> 00:11:50,590
these things don't do them so we hand

00:11:47,320 --> 00:11:52,930
road and svm we wrote some our code and

00:11:50,590 --> 00:11:55,030
thought this is it we did manual feature

00:11:52,930 --> 00:11:58,000
extraction we wrote hand-rolled svm we

00:11:55,030 --> 00:12:01,090
used radio kernels and that was kind of

00:11:58,000 --> 00:12:03,190
cool we used breeze for the older linear

00:12:01,090 --> 00:12:06,850
algebra and that worked beautifully we

00:12:03,190 --> 00:12:11,020
even had some dynamic logic linear with

00:12:06,850 --> 00:12:13,930
time and we added in an smt solver so

00:12:11,020 --> 00:12:16,960
the idea was that an exercise is a

00:12:13,930 --> 00:12:19,690
sequence of basic movements and they

00:12:16,960 --> 00:12:22,600
follow in some bounded time so what we

00:12:19,690 --> 00:12:25,240
would say is you know a squat is this

00:12:22,600 --> 00:12:27,340
movement followed by at some point in

00:12:25,240 --> 00:12:31,020
the future this movement and if that

00:12:27,340 --> 00:12:33,220
times out then it's clearly not a squat

00:12:31,020 --> 00:12:35,170
yeah that bad turns out not to work

00:12:33,220 --> 00:12:39,880
really because of the lag of user

00:12:35,170 --> 00:12:42,310
interface and the bad things were the

00:12:39,880 --> 00:12:45,070
jni connection to an smt s over which

00:12:42,310 --> 00:12:47,320
forced us to use jdk 1.7 in a particular

00:12:45,070 --> 00:12:50,200
version of ubuntu so it was all pretty

00:12:47,320 --> 00:12:54,670
terrible but it was cool right on we had

00:12:50,200 --> 00:12:58,060
working dynamic logic over finite traces

00:12:54,670 --> 00:13:01,120
in in eraqus tree we had vagrant and

00:12:58,060 --> 00:13:03,180
puppet as an automated infrastructure

00:13:01,120 --> 00:13:06,100
build that was kind of cool

00:13:03,180 --> 00:13:07,840
unfortunately we had to use the CVC for

00:13:06,100 --> 00:13:10,870
development packages and we had to

00:13:07,840 --> 00:13:12,310
depend on jdk 1.7 and the only thing we

00:13:10,870 --> 00:13:14,440
could classify this is the cool stuff

00:13:12,310 --> 00:13:17,790
right and all of this glorious set up

00:13:14,440 --> 00:13:20,109
the only thing we could detect was this

00:13:17,790 --> 00:13:22,660
but but it was beautiful right radio

00:13:20,109 --> 00:13:25,359
kernels and all that and we had no

00:13:22,660 --> 00:13:26,799
proper training and test data sets so

00:13:25,359 --> 00:13:28,809
see where I'm getting towards what is

00:13:26,799 --> 00:13:33,850
kind of difficult so we had that's right

00:13:28,809 --> 00:13:35,290
it and as VM we didn't actually just use

00:13:33,850 --> 00:13:37,929
the Taylor expansion of the radio

00:13:35,290 --> 00:13:40,179
account which was sufficient enough to

00:13:37,929 --> 00:13:42,850
do the prediction and then we wrote this

00:13:40,179 --> 00:13:45,970
kind of stuff because we didn't know any

00:13:42,850 --> 00:13:47,529
better did this worked but really we

00:13:45,970 --> 00:13:51,639
could do better then we tried this with

00:13:47,529 --> 00:13:53,410
this complicated dynamic logic over

00:13:51,639 --> 00:13:56,019
finite races so we would split these

00:13:53,410 --> 00:13:57,939
stains we'd have a query evaluator which

00:13:56,019 --> 00:14:00,850
returns future of queries these would be

00:13:57,939 --> 00:14:02,799
the dynamic logic queries that we would

00:14:00,850 --> 00:14:06,540
run in a nucca stream so we ended up

00:14:02,799 --> 00:14:09,939
with this a couple of flows so the main

00:14:06,540 --> 00:14:11,980
flow is a thing that goes from sensor

00:14:09,939 --> 00:14:14,019
net value which identifies elements at a

00:14:11,980 --> 00:14:16,179
different points of a human body through

00:14:14,019 --> 00:14:18,850
an evaluator to a decider so this is how

00:14:16,179 --> 00:14:22,809
it all flowed added was all beautifully

00:14:18,850 --> 00:14:25,029
it was a terrible idea it looked cool

00:14:22,809 --> 00:14:26,799
and this is what I talked about its

00:14:25,029 --> 00:14:28,869
holidays in San Francisco's if I wants

00:14:26,799 --> 00:14:31,029
to have a look at what i thought back

00:14:28,869 --> 00:14:35,949
then would have worked have a look at

00:14:31,029 --> 00:14:38,199
the talk but it didn't we had the main

00:14:35,949 --> 00:14:40,089
workflow at least you know the only

00:14:38,199 --> 00:14:42,429
thing that we could classify hair was a

00:14:40,089 --> 00:14:45,399
tap and we even wrote this beautiful

00:14:42,429 --> 00:14:47,169
interface to an smt solar and then

00:14:45,399 --> 00:14:50,470
unfortunately we had to do this sort of

00:14:47,169 --> 00:14:52,989
stuff it was pretty terrible right and

00:14:50,470 --> 00:14:55,209
yet if you want to do dynamic logic this

00:14:52,989 --> 00:14:58,540
is the thing to do CVC for actually

00:14:55,209 --> 00:15:01,809
works and the code that we have here you

00:14:58,540 --> 00:15:03,910
can write dynamic linear dynamic logic

00:15:01,809 --> 00:15:07,720
or finite traces so if you need it for

00:15:03,910 --> 00:15:08,980
your app definitely grab it so oh he

00:15:07,720 --> 00:15:11,019
thought on one page that that's a

00:15:08,980 --> 00:15:13,539
terrible idea let's move all the

00:15:11,019 --> 00:15:15,489
computation to the mobile app just like

00:15:13,539 --> 00:15:17,259
you've seen here all right so everything

00:15:15,489 --> 00:15:22,480
got evaluated on the phone I didn't need

00:15:17,259 --> 00:15:24,249
to do anything on the server except we

00:15:22,480 --> 00:15:26,110
thought we have a CTRs application

00:15:24,249 --> 00:15:29,319
that's kind of cool we have all the

00:15:26,110 --> 00:15:30,499
events in the journal how about we use

00:15:29,319 --> 00:15:32,929
all these events in

00:15:30,499 --> 00:15:35,149
as input for this Barbra job surely we

00:15:32,929 --> 00:15:37,399
have not odd not just the user

00:15:35,149 --> 00:15:42,159
registration and profile setting we have

00:15:37,399 --> 00:15:45,259
the sensor data okay okay well we tried

00:15:42,159 --> 00:15:46,939
we had distributed computation we had a

00:15:45,259 --> 00:15:48,829
cluster of spark machines we had 20

00:15:46,939 --> 00:15:52,549
megabytes of data right so i was there

00:15:48,829 --> 00:15:54,889
was a bit of a bummer we had very very

00:15:52,549 --> 00:15:57,289
naive model evaluation and testing by

00:15:54,889 --> 00:16:00,939
that i mean we just computed something

00:15:57,289 --> 00:16:03,409
and well that's what we had right tada

00:16:00,939 --> 00:16:06,349
there was no way to keep historical

00:16:03,409 --> 00:16:09,229
rolling best model so if we computed if

00:16:06,349 --> 00:16:11,119
we fitted a particular model we didn't

00:16:09,229 --> 00:16:12,619
remember anything with just whatever we

00:16:11,119 --> 00:16:14,869
computed on the day is what God

00:16:12,619 --> 00:16:16,669
delivered to the mobile application so

00:16:14,869 --> 00:16:19,789
that was pretty terrible and we treated

00:16:16,669 --> 00:16:22,429
all the users the same remember well we

00:16:19,789 --> 00:16:26,869
had 20 megabytes of data so we had one

00:16:22,429 --> 00:16:29,419
user okay so that wasn't great model

00:16:26,869 --> 00:16:33,409
storage and delivery well I say

00:16:29,419 --> 00:16:35,989
rudimentary we had files files on one

00:16:33,409 --> 00:16:39,619
machine that's how bad it was so

00:16:35,989 --> 00:16:41,919
terrible ideas our use of occasional as

00:16:39,619 --> 00:16:45,529
source of data for the spark jobs

00:16:41,919 --> 00:16:47,419
because you tie yourself down to if you

00:16:45,529 --> 00:16:50,029
write a message Anaka obviously it ends

00:16:47,419 --> 00:16:53,059
up as a blob in this journal you can

00:16:50,029 --> 00:16:55,399
wire in a bit better serializers which

00:16:53,059 --> 00:16:58,099
is what we ended up doing but it's still

00:16:55,399 --> 00:16:59,869
not good second point I think it is

00:16:58,099 --> 00:17:03,079
crucial right 20 megabytes is in big

00:16:59,869 --> 00:17:06,559
data 20 gigabytes is in big data 20

00:17:03,079 --> 00:17:08,299
terabytes she probably is but that's

00:17:06,559 --> 00:17:10,909
when you should get start getting

00:17:08,299 --> 00:17:14,449
interested in it everything else will

00:17:10,909 --> 00:17:19,279
fit in memory that's it why why bother

00:17:14,449 --> 00:17:21,470
and we learned that production machine

00:17:19,279 --> 00:17:25,129
learning models need really a reliable

00:17:21,470 --> 00:17:26,600
way of model evaluation of model storage

00:17:25,129 --> 00:17:29,149
of test data and training data

00:17:26,600 --> 00:17:32,629
management without it we just we were

00:17:29,149 --> 00:17:34,279
guessing and ultimately these these

00:17:32,629 --> 00:17:36,619
models are the gold dust right so it's

00:17:34,279 --> 00:17:38,149
so the files the models that get

00:17:36,619 --> 00:17:40,460
delivered to the mobile app it's still

00:17:38,149 --> 00:17:41,620
roughly 10 megabytes or floating point

00:17:40,460 --> 00:17:43,840
numbers

00:17:41,620 --> 00:17:45,790
and it's been like that for a year

00:17:43,840 --> 00:17:49,270
except now we have better 10 megabytes

00:17:45,790 --> 00:17:51,850
and having the right 10 megabytes is is

00:17:49,270 --> 00:17:53,440
the gold dust all right so i talked

00:17:51,850 --> 00:17:55,360
about serialization so even if you're

00:17:53,440 --> 00:17:58,960
not doing these horrors that i talked

00:17:55,360 --> 00:18:00,850
about he should in a sikh urs and event

00:17:58,960 --> 00:18:03,850
sourcing use something a bit better than

00:18:00,850 --> 00:18:06,820
java.io.serializable so we ended up with

00:18:03,850 --> 00:18:10,690
creo a particular fiddly version of creo

00:18:06,820 --> 00:18:14,650
which ran really well with spark so we

00:18:10,690 --> 00:18:17,620
were on 212 in the main active system

00:18:14,650 --> 00:18:19,390
and we were on 211 and spark and we just

00:18:17,620 --> 00:18:23,230
needed to make it all connect so that

00:18:19,390 --> 00:18:25,900
worked and then we did this which is

00:18:23,230 --> 00:18:28,929
really horrific right so we saw all the

00:18:25,900 --> 00:18:32,650
journal in the spark job and then we

00:18:28,929 --> 00:18:35,380
only matched on some events say this one

00:18:32,650 --> 00:18:40,090
user exercise process of persistence and

00:18:35,380 --> 00:18:42,450
we got the exercise session which

00:18:40,090 --> 00:18:44,920
allowed us to grab the samples that

00:18:42,450 --> 00:18:47,650
samples with labels and we did some

00:18:44,920 --> 00:18:49,559
processing of that that's a terrible

00:18:47,650 --> 00:18:53,350
idea because you're doing so much work

00:18:49,559 --> 00:18:56,410
it's much better to just store the data

00:18:53,350 --> 00:18:58,090
in in the right form so i'll call this a

00:18:56,410 --> 00:18:59,620
deferred success it was all a deferred

00:18:58,090 --> 00:19:02,290
success because we explored some

00:18:59,620 --> 00:19:05,440
fantastic topics and we learned many

00:19:02,290 --> 00:19:07,720
many many things so the pattern that you

00:19:05,440 --> 00:19:09,670
can see on this slide really is that the

00:19:07,720 --> 00:19:11,080
mechanics works you know acha works the

00:19:09,670 --> 00:19:14,080
types of stack really works secure acs

00:19:11,080 --> 00:19:16,960
unica brilliant reston akka and spray

00:19:14,080 --> 00:19:21,670
and arc htp no problem hacks with swift

00:19:16,960 --> 00:19:24,309
and Swift said yeah fine sensor data

00:19:21,670 --> 00:19:26,650
ingestion decoding with s codec all good

00:19:24,309 --> 00:19:28,450
right it all worked the actual

00:19:26,650 --> 00:19:30,550
classification was a big tough right so

00:19:28,450 --> 00:19:33,010
that didn't work so enough success for a

00:19:30,550 --> 00:19:36,520
big machine learning project so we went

00:19:33,010 --> 00:19:38,350
on right and we go to working dynamic

00:19:36,520 --> 00:19:41,530
logic in Anaka stream so acha stream

00:19:38,350 --> 00:19:43,300
worked and dynamic logic work not a

00:19:41,530 --> 00:19:46,150
surprise right everything else was

00:19:43,300 --> 00:19:49,210
pretty terrible and then we go to this

00:19:46,150 --> 00:19:52,390
final stage where well it was all

00:19:49,210 --> 00:19:54,450
terrible right so the mechanics that the

00:19:52,390 --> 00:19:56,010
building books are right except

00:19:54,450 --> 00:19:59,190
what is important is to use them in the

00:19:56,010 --> 00:20:02,070
right way and this really isn't well as

00:19:59,190 --> 00:20:04,260
Google suggests right I'm frightened

00:20:02,070 --> 00:20:08,160
about the third one right so are we

00:20:04,260 --> 00:20:13,890
really down to self aware it is right

00:20:08,160 --> 00:20:16,560
but tell me what anywho so this is what

00:20:13,890 --> 00:20:18,000
we have now this is what we think now is

00:20:16,560 --> 00:20:20,760
the right way of doing it it turns out

00:20:18,000 --> 00:20:23,010
that this this works as you've seen so

00:20:20,760 --> 00:20:25,530
we have the same old stuff right we have

00:20:23,010 --> 00:20:27,540
sensors we have mobile app the mobile

00:20:25,530 --> 00:20:29,910
app now does all the processing so it

00:20:27,540 --> 00:20:32,460
receives all the other models all the

00:20:29,910 --> 00:20:34,770
architectures for the models builds this

00:20:32,460 --> 00:20:38,600
this tiny Markov chain of insertion

00:20:34,770 --> 00:20:41,400
prediction that makes small incision

00:20:38,600 --> 00:20:44,430
linear predictions because users will

00:20:41,400 --> 00:20:46,800
generally do tiny changes in one session

00:20:44,430 --> 00:20:49,170
but between sessions they'll make bigger

00:20:46,800 --> 00:20:51,150
ones which we can compute in the rest of

00:20:49,170 --> 00:20:55,440
the system we have an accra cluster that

00:20:51,150 --> 00:20:57,870
holds the user profiles and it handles

00:20:55,440 --> 00:20:59,280
the ingestion of the sensor data so when

00:20:57,870 --> 00:21:01,290
the session ends we submit the whole

00:20:59,280 --> 00:21:04,110
block with with the labels and all the

00:21:01,290 --> 00:21:07,050
sensor data it's a ctrl system so we

00:21:04,110 --> 00:21:09,360
have a journal with messages ins with

00:21:07,050 --> 00:21:13,550
events and snapshots that works really

00:21:09,360 --> 00:21:16,350
really well we also write the same

00:21:13,550 --> 00:21:17,760
events the sensory data we expand we

00:21:16,350 --> 00:21:22,350
write them into properly structured

00:21:17,760 --> 00:21:24,270
Cassandra tables not just a blob now we

00:21:22,350 --> 00:21:27,120
read we cluster the users right so now

00:21:24,270 --> 00:21:29,520
that we have a few more users we need to

00:21:27,120 --> 00:21:31,710
close to them because you know I'm

00:21:29,520 --> 00:21:33,360
getting old and some young people

00:21:31,710 --> 00:21:35,730
exercise in a completely different way

00:21:33,360 --> 00:21:38,820
and older people than me not many but

00:21:35,730 --> 00:21:41,310
but they exercise in different way still

00:21:38,820 --> 00:21:43,200
so we have at the minute we have 10

00:21:41,310 --> 00:21:45,570
clusters of users and at that sufficient

00:21:43,200 --> 00:21:49,650
you know by age group by sex by self

00:21:45,570 --> 00:21:52,610
reported exercise proficiency I suppose

00:21:49,650 --> 00:21:55,110
and then we have a whole bunch of

00:21:52,610 --> 00:21:57,090
machine learning tasks so they actually

00:21:55,110 --> 00:22:00,450
fit these setup movements that I've done

00:21:57,090 --> 00:22:01,920
these crucial first second movements we

00:22:00,450 --> 00:22:04,440
need to train on those and we use the

00:22:01,920 --> 00:22:08,220
labels to do that the model parameters

00:22:04,440 --> 00:22:11,190
end up again in a database and they get

00:22:08,220 --> 00:22:13,289
red or bacon notified that we have a new

00:22:11,190 --> 00:22:15,510
model to our acha cluster and that gets

00:22:13,289 --> 00:22:16,620
pushed over to the apples content

00:22:15,510 --> 00:22:18,390
delivery system because we don't

00:22:16,620 --> 00:22:23,100
actually like to send these 10 megabytes

00:22:18,390 --> 00:22:25,830
it costs too much money now I am

00:22:23,100 --> 00:22:27,870
frightened that many people will just

00:22:25,830 --> 00:22:31,730
use these apps with no concern to

00:22:27,870 --> 00:22:36,450
privacy I mean who here uses say strava

00:22:31,730 --> 00:22:37,679
there we go are you not worried you know

00:22:36,450 --> 00:22:40,110
there are people who know we can just

00:22:37,679 --> 00:22:42,120
look and these are this is even worse

00:22:40,110 --> 00:22:44,580
this is you know really biometric data

00:22:42,120 --> 00:22:46,500
if we have a fully wired human this

00:22:44,580 --> 00:22:48,480
sends a lot of information it's possible

00:22:46,500 --> 00:22:51,240
to find out lots of things so we use

00:22:48,480 --> 00:22:53,429
unstable identifies throughout now for

00:22:51,240 --> 00:22:56,730
physiotherapy our unstable identifies

00:22:53,429 --> 00:23:01,080
are unstable for just one session so you

00:22:56,730 --> 00:23:05,880
can't really connect the behavior to one

00:23:01,080 --> 00:23:09,000
specific user so do that um technologies

00:23:05,880 --> 00:23:10,799
usual lot right iOS on the mobile and

00:23:09,000 --> 00:23:13,950
Swift we have our we have Cassandra we

00:23:10,799 --> 00:23:15,270
have spark we have key RS um mainly for

00:23:13,950 --> 00:23:18,750
experiments right because we can use

00:23:15,270 --> 00:23:21,059
that as an abstraction over either

00:23:18,750 --> 00:23:23,429
theano or tensorflow so it tends to flow

00:23:21,059 --> 00:23:26,419
in the real big application and locally

00:23:23,429 --> 00:23:28,890
I have a tiny little sec set of docket

00:23:26,419 --> 00:23:31,380
containers and we can train locally on

00:23:28,890 --> 00:23:33,630
piano and we have missiles to run all of

00:23:31,380 --> 00:23:36,150
this on AWS because I want to save money

00:23:33,630 --> 00:23:37,799
on to squeeze as much power out of it as

00:23:36,150 --> 00:23:41,010
possible and I don't want to worry about

00:23:37,799 --> 00:23:43,799
things so I got you know about acha this

00:23:41,010 --> 00:23:45,539
is not difficult to reactive service for

00:23:43,799 --> 00:23:48,600
user profiles see Carausius

00:23:45,539 --> 00:23:50,280
implementation uses Cassandra for the

00:23:48,600 --> 00:23:53,429
events and the journals but it also

00:23:50,280 --> 00:23:56,580
writes properly structured data to

00:23:53,429 --> 00:23:58,260
Cassandra and it then reads and the

00:23:56,580 --> 00:23:59,820
model parameters that we've computed and

00:23:58,260 --> 00:24:01,799
it delivers them to Apple which then

00:23:59,820 --> 00:24:05,700
delivers them to the mobile applications

00:24:01,799 --> 00:24:08,309
and we have spark right now I talked

00:24:05,700 --> 00:24:12,179
about this fully wired human there

00:24:08,309 --> 00:24:15,360
aren't that many so we between our what

00:24:12,179 --> 00:24:16,710
could be about 200 years but keep in

00:24:15,360 --> 00:24:19,289
between these two hundred years I know

00:24:16,710 --> 00:24:21,270
it's going to be on YouTube hey hope so

00:24:19,289 --> 00:24:23,250
we have some smart clothes that

00:24:21,270 --> 00:24:25,500
have some really really cool sensors who

00:24:23,250 --> 00:24:28,470
have strain gauges along every major

00:24:25,500 --> 00:24:30,750
major muscle group and so we have a few

00:24:28,470 --> 00:24:33,450
people who are what we call fully wired

00:24:30,750 --> 00:24:35,580
and so these people can generate an

00:24:33,450 --> 00:24:38,640
awful lot of data to give you an idea

00:24:35,580 --> 00:24:42,960
one sensor generates about one megabyte

00:24:38,640 --> 00:24:44,850
per hour of exercise so four sensors now

00:24:42,960 --> 00:24:48,180
what right so if I'm fully wired human

00:24:44,850 --> 00:24:50,130
with now four sensors then if I train a

00:24:48,180 --> 00:24:52,110
model that uses all these four sensors

00:24:50,130 --> 00:24:54,390
they will be usable only for other fully

00:24:52,110 --> 00:24:56,160
wired humans well I just sort of want to

00:24:54,390 --> 00:24:57,690
use every bit of that right so I need to

00:24:56,160 --> 00:25:00,180
split it in two combinations if I have

00:24:57,690 --> 00:25:02,610
these two sensors I need to have a model

00:25:00,180 --> 00:25:04,320
that does both that does one on the left

00:25:02,610 --> 00:25:06,360
wrist and one on the right wrist if I

00:25:04,320 --> 00:25:09,810
have more sensors I need to you know

00:25:06,360 --> 00:25:12,210
make all these combinations and then you

00:25:09,810 --> 00:25:14,880
know we use spark for the typical what i

00:25:12,210 --> 00:25:18,210
would call Big Data tasks now they are

00:25:14,880 --> 00:25:19,530
really interesting so we might and this

00:25:18,210 --> 00:25:21,930
is real might have submitted a proposal

00:25:19,530 --> 00:25:24,450
research proposal for a big EU project

00:25:21,930 --> 00:25:25,760
we might actually have injured people

00:25:24,450 --> 00:25:28,200
which is kind of I mean it's not cool

00:25:25,760 --> 00:25:31,080
but it's useful right we will actually

00:25:28,200 --> 00:25:33,770
be able to do a good job so what what I

00:25:31,080 --> 00:25:36,050
did this sort of pattern of behavior

00:25:33,770 --> 00:25:39,390
transition from one exercise to the next

00:25:36,050 --> 00:25:44,280
what we can now answer is what is the

00:25:39,390 --> 00:25:47,670
most what is the best most efficient set

00:25:44,280 --> 00:25:52,080
of transitions to do gain I don't know

00:25:47,670 --> 00:25:53,670
muscle mass gain or a fat loss but for

00:25:52,080 --> 00:25:55,620
this horizons project we will be able to

00:25:53,670 --> 00:25:57,270
answer questions like what is the best

00:25:55,620 --> 00:26:00,270
sequence of exercises to improve

00:25:57,270 --> 00:26:04,170
mobility to recover from injury so that

00:26:00,270 --> 00:26:05,880
will be really really exciting oh my we

00:26:04,170 --> 00:26:08,550
have a mistake that that seems to have

00:26:05,880 --> 00:26:11,460
crept in that's like a terrible idea

00:26:08,550 --> 00:26:15,960
right imagine if you have fully wired

00:26:11,460 --> 00:26:17,940
human 2020 census so we would compute

00:26:15,960 --> 00:26:19,580
all these combinations and write them to

00:26:17,940 --> 00:26:23,220
Cassandra we would have so much data

00:26:19,580 --> 00:26:25,080
well this actually belongs in the

00:26:23,220 --> 00:26:27,120
machine learning job right this is the

00:26:25,080 --> 00:26:29,430
thing that prepares the data set so what

00:26:27,120 --> 00:26:32,220
we have in the spa job is we don't touch

00:26:29,430 --> 00:26:34,800
the sensor data we cluster them into

00:26:32,220 --> 00:26:37,650
groups by profile data

00:26:34,800 --> 00:26:40,880
but then we leave the rest we do the

00:26:37,650 --> 00:26:44,700
machine learning and model fitting

00:26:40,880 --> 00:26:47,640
entirely in our cluster of now Python

00:26:44,700 --> 00:26:49,740
jobs in chaos which gives us an

00:26:47,640 --> 00:26:52,410
abstraction over at the theater or tense

00:26:49,740 --> 00:26:54,660
flow so we do sensor data preprocessing

00:26:52,410 --> 00:26:57,090
we expand the sensor data into all the

00:26:54,660 --> 00:26:59,400
combinations that we have so just this

00:26:57,090 --> 00:27:01,890
one just this one both whatever else I'm

00:26:59,400 --> 00:27:03,600
wearing and then we have training and

00:27:01,890 --> 00:27:06,720
evaluation programs this again shouldn't

00:27:03,600 --> 00:27:08,280
be too unfamiliar for anyone who's doing

00:27:06,720 --> 00:27:10,560
or trying to do machine learning so we

00:27:08,280 --> 00:27:12,330
have a program that trains that selects

00:27:10,560 --> 00:27:14,460
a subset of data calls it the training

00:27:12,330 --> 00:27:16,260
data set we have the test data set you

00:27:14,460 --> 00:27:20,280
fit some model then you evaluate its

00:27:16,260 --> 00:27:23,880
performance and you need to write the

00:27:20,280 --> 00:27:25,800
best model out so what we do is keep 10

00:27:23,880 --> 00:27:27,360
best models including their

00:27:25,800 --> 00:27:28,890
architectures including the activation

00:27:27,360 --> 00:27:31,730
functions the shape on the network and

00:27:28,890 --> 00:27:35,190
the parameters and every time we run we

00:27:31,730 --> 00:27:37,980
select the 10 best ones and sometimes we

00:27:35,190 --> 00:27:40,470
randomly mutate them so we might flip

00:27:37,980 --> 00:27:42,390
activation function you just we actually

00:27:40,470 --> 00:27:44,250
have no systematic way of doing it at

00:27:42,390 --> 00:27:47,100
the moment we just do a random mutation

00:27:44,250 --> 00:27:48,620
but we remember the 10 best best ones

00:27:47,100 --> 00:27:52,590
and that seems to do a really good job

00:27:48,620 --> 00:27:56,700
so you know this is right down to python

00:27:52,590 --> 00:27:58,980
so we load the bottle on here we have

00:27:56,700 --> 00:28:01,320
some no hard coded network it turns out

00:27:58,980 --> 00:28:06,300
that three layers will will do the job

00:28:01,320 --> 00:28:08,040
right load the data and construct in

00:28:06,300 --> 00:28:11,700
this case or just a multi-layered

00:28:08,040 --> 00:28:13,380
perceptron and fit it compile it fitted

00:28:11,700 --> 00:28:16,680
depending on which back-end you use this

00:28:13,380 --> 00:28:19,200
will end up as a tensorflow job or as as

00:28:16,680 --> 00:28:21,840
a piano job on this machine and we're

00:28:19,200 --> 00:28:24,480
good to go right so again just to give

00:28:21,840 --> 00:28:26,070
you an idea of what it looks like here

00:28:24,480 --> 00:28:28,920
are three exercises this is

00:28:26,070 --> 00:28:31,620
accelerometer XYZ traces and the big

00:28:28,920 --> 00:28:36,330
blue areas these are the areas of labels

00:28:31,620 --> 00:28:38,660
now if we you know train on all of these

00:28:36,330 --> 00:28:40,860
blocks that's that's too much and

00:28:38,660 --> 00:28:42,360
besides it means that we have to submit

00:28:40,860 --> 00:28:45,630
all these samples well that's not gonna

00:28:42,360 --> 00:28:47,490
work we need and this is difficult to

00:28:45,630 --> 00:28:48,110
see so I've highlighted them we need

00:28:47,490 --> 00:28:51,049
just a

00:28:48,110 --> 00:28:53,120
second the setup movement right so what

00:28:51,049 --> 00:28:55,490
we have as a combination of we know

00:28:53,120 --> 00:28:57,650
where we are geolocation so we know what

00:28:55,490 --> 00:29:00,170
the user might be able to do so we can

00:28:57,650 --> 00:29:02,360
pinpoint it down to a particular gym or

00:29:00,170 --> 00:29:04,940
a particular Hospital we know fine

00:29:02,360 --> 00:29:06,950
location inside a building so we know

00:29:04,940 --> 00:29:09,650
really if I'm standing here there are

00:29:06,950 --> 00:29:11,660
only so many things I can do and I also

00:29:09,650 --> 00:29:13,700
know the sequence of exercises that I

00:29:11,660 --> 00:29:15,920
like to do them that I like to do in a

00:29:13,700 --> 00:29:18,170
particular session so that really really

00:29:15,920 --> 00:29:20,120
really narrows it down and then i have

00:29:18,170 --> 00:29:20,960
this confirmation of oh now the sensors

00:29:20,120 --> 00:29:23,240
are telling you that i've done a

00:29:20,960 --> 00:29:26,419
particular movement surely this must now

00:29:23,240 --> 00:29:28,850
be such and such exercise so this is how

00:29:26,419 --> 00:29:32,660
we train it you know the usual

00:29:28,850 --> 00:29:35,500
evaluation we get down to know pretty

00:29:32,660 --> 00:29:37,669
bloody good loss and reasonable accuracy

00:29:35,500 --> 00:29:40,490
you know what let's see it I have

00:29:37,669 --> 00:29:41,900
another daring demo of course so I have

00:29:40,490 --> 00:29:47,020
a couple of other docker containers

00:29:41,900 --> 00:29:49,820
around and take my word for it let's lip

00:29:47,020 --> 00:29:56,120
right through that I have final one to

00:29:49,820 --> 00:29:57,410
run I know make far so we have make file

00:29:56,120 --> 00:30:01,490
which calls the docker file which caused

00:29:57,410 --> 00:30:05,150
Arthur whole bunch of others ok so there

00:30:01,490 --> 00:30:08,960
we have it um all the other containers

00:30:05,150 --> 00:30:12,169
have started up I trust so here is what

00:30:08,960 --> 00:30:16,220
it looks like all right up to you bad so

00:30:12,169 --> 00:30:18,470
let's just run all the cells so we can

00:30:16,220 --> 00:30:22,520
see how we load the data so this is the

00:30:18,470 --> 00:30:24,740
data that we've submitted from Cassandra

00:30:22,520 --> 00:30:26,690
where are we there we go right so these

00:30:24,740 --> 00:30:28,610
are the samples I've shown you so we

00:30:26,690 --> 00:30:35,000
just plot them we then select just the

00:30:28,610 --> 00:30:36,770
first second for our labeling we append

00:30:35,000 --> 00:30:40,970
and merge all their friends our world

00:30:36,770 --> 00:30:43,040
will have to wait for a bit now some of

00:30:40,970 --> 00:30:44,960
you who might have done some spark will

00:30:43,040 --> 00:30:49,669
look at this and there should be

00:30:44,960 --> 00:30:51,740
something suspicious any any any takers

00:30:49,669 --> 00:30:54,850
so Python work all right we can we can

00:30:51,740 --> 00:30:57,350
use Python in it's about notebooks

00:30:54,850 --> 00:30:59,990
anything else suspicious so localhost

00:30:57,350 --> 00:31:04,309
8888 sounds reasonable

00:30:59,990 --> 00:31:06,830
but the logo is wrong this is not a

00:31:04,309 --> 00:31:09,260
spark notebook this is actual Jupiter

00:31:06,830 --> 00:31:11,600
notebook where the connection spark so

00:31:09,260 --> 00:31:14,990
there's a there's a docker container

00:31:11,600 --> 00:31:16,880
that does that and there was a little

00:31:14,990 --> 00:31:18,920
bit of a problem that could be right

00:31:16,880 --> 00:31:20,750
down to me in using sparknote bach

00:31:18,920 --> 00:31:22,670
particularly with the a no particularly

00:31:20,750 --> 00:31:26,210
with tensorflow on GPUs that just

00:31:22,670 --> 00:31:28,670
crashed all the time so I ended up just

00:31:26,210 --> 00:31:31,280
using Jupiter with connection to spark

00:31:28,670 --> 00:31:32,510
and so if you're doing experiments with

00:31:31,280 --> 00:31:35,150
spark notebook if you're then running

00:31:32,510 --> 00:31:38,630
your native Python code you're probably

00:31:35,150 --> 00:31:41,090
cool so here we are still doing it it's

00:31:38,630 --> 00:31:43,760
it's making noise so so I'm sure it's

00:31:41,090 --> 00:31:46,520
it's actually working and then we'll see

00:31:43,760 --> 00:31:49,820
you can you know wait for this to train

00:31:46,520 --> 00:31:51,380
and to fit old mess ugh you know what

00:31:49,820 --> 00:31:55,850
I'll just keep it running and go back to

00:31:51,380 --> 00:31:58,490
my slides we'll come back to it alright

00:31:55,850 --> 00:32:01,700
so daring demo is in progress machines

00:31:58,490 --> 00:32:05,050
are working um so check that one out

00:32:01,700 --> 00:32:07,280
that's actually quite helpful alright so

00:32:05,050 --> 00:32:10,250
so now that we had all this coded that

00:32:07,280 --> 00:32:11,929
that was kind of great and it ran on

00:32:10,250 --> 00:32:13,220
this machine and he ran on a couple of

00:32:11,929 --> 00:32:16,520
other machines that I had in my office

00:32:13,220 --> 00:32:19,370
but no we needed to run it as a service

00:32:16,520 --> 00:32:22,910
for someone to use so this was in the

00:32:19,370 --> 00:32:26,090
days before open-source DCOs so we ended

00:32:22,910 --> 00:32:27,980
up on plainly sauce with a couple of

00:32:26,090 --> 00:32:30,110
frameworks ramp running a couple of

00:32:27,980 --> 00:32:32,000
tasks so marathon for the wrong long

00:32:30,110 --> 00:32:34,610
running tasks long-running ones I mean

00:32:32,000 --> 00:32:37,640
Cassandra cluster I mean the vehicle

00:32:34,610 --> 00:32:40,100
cluster I also mean the spark jobs and

00:32:37,640 --> 00:32:45,830
finally the machine learning jobs that

00:32:40,100 --> 00:32:47,720
you know the Python tasks what we wanted

00:32:45,830 --> 00:32:50,600
in our runtime environment is something

00:32:47,720 --> 00:32:53,030
simple something that would be easy to

00:32:50,600 --> 00:32:56,360
manage easy to orchestrate easy to work

00:32:53,030 --> 00:32:59,059
with so if i have my machine that one i

00:32:56,360 --> 00:33:02,500
want to be able to use it in a very

00:32:59,059 --> 00:33:04,790
convenient way support for many polygons

00:33:02,500 --> 00:33:06,920
polyglot languages that's just terrible

00:33:04,790 --> 00:33:08,410
i'll edit it out before I give the

00:33:06,920 --> 00:33:12,220
slides out this is

00:33:08,410 --> 00:33:14,140
I don't that that thing anyway polyglot

00:33:12,220 --> 00:33:15,880
frameworks and components we needed cost

00:33:14,140 --> 00:33:17,620
effective resource utilization so if I

00:33:15,880 --> 00:33:19,060
have a whole bunch of Amazon machines I

00:33:17,620 --> 00:33:21,220
need to make sure I squeeze as much out

00:33:19,060 --> 00:33:24,270
of them as possible so we ended up with

00:33:21,220 --> 00:33:27,490
docker which allowed us to deploy this

00:33:24,270 --> 00:33:29,530
reliably and consistently so I can build

00:33:27,490 --> 00:33:33,550
a doctor machine published to a registry

00:33:29,530 --> 00:33:34,840
and really be happy with it really quick

00:33:33,550 --> 00:33:37,390
startup you've seen it if the demon

00:33:34,840 --> 00:33:40,180
we're running it would be all good it

00:33:37,390 --> 00:33:41,920
was pretty simple developer and friendly

00:33:40,180 --> 00:33:43,780
workflow ultimately right so we have a

00:33:41,920 --> 00:33:45,670
dev ops team which could build all sorts

00:33:43,780 --> 00:33:48,760
of complicated stuff all the way down to

00:33:45,670 --> 00:33:51,310
these vagrant boxes but I didn't want to

00:33:48,760 --> 00:33:52,810
do that I'd much rather that you know

00:33:51,310 --> 00:33:57,190
the actual software team two people

00:33:52,810 --> 00:33:59,080
could do it um so we did this ride

00:33:57,190 --> 00:34:00,160
authorized this Cassandra environments

00:33:59,080 --> 00:34:02,830
if I have a cluster of Cassandra

00:34:00,160 --> 00:34:05,770
machines and on this macbook pro well

00:34:02,830 --> 00:34:07,210
you know we need to give it as little

00:34:05,770 --> 00:34:11,669
memory as possible so that turned out to

00:34:07,210 --> 00:34:14,230
be okay you know tiny heap no cash is

00:34:11,669 --> 00:34:16,870
nothing at all so if I could run it on

00:34:14,230 --> 00:34:19,840
my machine for testing in production

00:34:16,870 --> 00:34:23,530
that was slightly different it needed

00:34:19,840 --> 00:34:25,600
more parameters Nate medial the the

00:34:23,530 --> 00:34:27,580
caches in the data needed to be mounted

00:34:25,600 --> 00:34:29,620
on specific volumes so that's that i

00:34:27,580 --> 00:34:31,960
could have immutable containers rather

00:34:29,620 --> 00:34:33,310
than worrying about newt ability doc

00:34:31,960 --> 00:34:36,190
raising our cluster is actually really

00:34:33,310 --> 00:34:38,800
easy all right so doc a plug-in with us

00:34:36,190 --> 00:34:41,470
BTW assembly we ended up using make a

00:34:38,800 --> 00:34:45,340
fat jar exposed the port and we're done

00:34:41,470 --> 00:34:47,169
with it for development there's a

00:34:45,340 --> 00:34:50,500
setting that you can say oh here it is

00:34:47,169 --> 00:34:52,020
right but it says don't cash the image

00:34:50,500 --> 00:34:57,610
that you create so this is part of our

00:34:52,020 --> 00:34:59,380
hood SBT so just specify the dependent

00:34:57,610 --> 00:35:03,400
news my type in as BTW docker it feels

00:34:59,380 --> 00:35:06,700
first makes this fat jar grab the java

00:35:03,400 --> 00:35:09,370
image that has jdk and all the different

00:35:06,700 --> 00:35:11,440
components expose port 8080 which is

00:35:09,370 --> 00:35:14,380
where the rest api sit and i'm done with

00:35:11,440 --> 00:35:16,000
it give it some name and what i said is

00:35:14,380 --> 00:35:19,000
you know for development you might not

00:35:16,000 --> 00:35:19,950
want to cache the generated generated

00:35:19,000 --> 00:35:22,410
image

00:35:19,950 --> 00:35:25,589
okay oh yes here we are Bloc ros

00:35:22,410 --> 00:35:27,630
production environment for Cassandra so

00:35:25,589 --> 00:35:31,140
immutable images right so the data

00:35:27,630 --> 00:35:34,140
commit log and safe caches go on mounts

00:35:31,140 --> 00:35:35,579
they don't go into the the container run

00:35:34,140 --> 00:35:38,280
Cassandra in the foreground right i mean

00:35:35,579 --> 00:35:40,890
i've done it so many times brenda the

00:35:38,280 --> 00:35:42,900
darker process and it just dies american

00:35:40,890 --> 00:35:44,640
goes i'm gonna start a new one oh that

00:35:42,900 --> 00:35:47,839
died i'm gonna start a new one so don't

00:35:44,640 --> 00:35:51,329
forget minus f that turns out to be

00:35:47,839 --> 00:35:53,070
bitter it's easy to spot right then we

00:35:51,329 --> 00:35:55,230
had me sauce which this is the official

00:35:53,070 --> 00:35:57,210
line and distributed systems colonel

00:35:55,230 --> 00:35:58,950
tens of thousands of nodes we don't have

00:35:57,210 --> 00:36:01,530
tens of thousands of nodes for the

00:35:58,950 --> 00:36:04,710
record we have 20 but still you know I

00:36:01,530 --> 00:36:06,270
don't want to manage them nice support

00:36:04,710 --> 00:36:08,520
for dr. containers which was really

00:36:06,270 --> 00:36:11,190
important so this is the again the

00:36:08,520 --> 00:36:13,890
official architecture what we do is map

00:36:11,190 --> 00:36:15,450
frameworks that describe the tasks that

00:36:13,890 --> 00:36:17,400
run in them so we have a framework for

00:36:15,450 --> 00:36:19,020
Cassandra we have a framework for the

00:36:17,400 --> 00:36:21,690
spark we have a framework for the for

00:36:19,020 --> 00:36:26,069
the ml jobs framework cracker tasks the

00:36:21,690 --> 00:36:30,810
individual things that run and finally

00:36:26,069 --> 00:36:35,400
and this is nearly the end I you know I

00:36:30,810 --> 00:36:37,650
this was my / cake solutions money so we

00:36:35,400 --> 00:36:39,420
didn't really want to spend money at all

00:36:37,650 --> 00:36:41,010
if possible which is why computation

00:36:39,420 --> 00:36:43,230
happens on user devices you might have

00:36:41,010 --> 00:36:45,270
gone to the scholar j/s talks where the

00:36:43,230 --> 00:36:47,339
guys keep saying the same things I don't

00:36:45,270 --> 00:36:49,619
pay for it the browser these devices

00:36:47,339 --> 00:36:52,410
that users pay for that he uses feed

00:36:49,619 --> 00:36:55,290
they can run a lot of it so we do that

00:36:52,410 --> 00:36:57,119
too we use spot instances where ever

00:36:55,290 --> 00:37:00,140
possible so for all the spark jobs they

00:36:57,119 --> 00:37:02,940
all run on spot instances all the

00:37:00,140 --> 00:37:04,619
machine learning model fitting tasks

00:37:02,940 --> 00:37:07,380
they all run on spot instances if they

00:37:04,619 --> 00:37:10,140
die they die but they they cost half the

00:37:07,380 --> 00:37:12,030
money which is kind of cool and in nice

00:37:10,140 --> 00:37:14,940
awesome marathon I can actually support

00:37:12,030 --> 00:37:17,640
a scenario where I'm running and then

00:37:14,940 --> 00:37:19,560
amazon says nope are you know we're

00:37:17,640 --> 00:37:21,060
gonna take these 10 machines away from

00:37:19,560 --> 00:37:24,000
you because they're now too expensive

00:37:21,060 --> 00:37:26,640
you paid fifty cents per hour we're now

00:37:24,000 --> 00:37:31,140
charging them at 60 they're gone we're

00:37:26,640 --> 00:37:32,610
ok we can can recover zookeeper day yeah

00:37:31,140 --> 00:37:34,110
don't run zookeeper on spot

00:37:32,610 --> 00:37:37,560
instances that would be a terrible idea

00:37:34,110 --> 00:37:39,270
and perhaps don't run your database on

00:37:37,560 --> 00:37:41,850
spot instances that also would be a

00:37:39,270 --> 00:37:44,520
terrible idea but nearly everything else

00:37:41,850 --> 00:37:49,020
we run on spot instances and that all

00:37:44,520 --> 00:37:50,910
works really well so where to next so

00:37:49,020 --> 00:37:53,310
what's gonna happen in the next couple

00:37:50,910 --> 00:37:55,710
of months fingers crossed right we'll

00:37:53,310 --> 00:37:57,810
have our big research project funded we

00:37:55,710 --> 00:38:00,240
have four institutions working on it a

00:37:57,810 --> 00:38:03,050
couple of other companies in Europe and

00:38:00,240 --> 00:38:07,650
that will allow us to really push for

00:38:03,050 --> 00:38:10,050
fully wired human smart wearables the

00:38:07,650 --> 00:38:14,600
t-shirts so it looks like a battery of

00:38:10,050 --> 00:38:17,430
t-shirt and you know shorts I suppose

00:38:14,600 --> 00:38:19,110
they're not too expensive they are

00:38:17,430 --> 00:38:21,270
cheaper than the fashionable ones that

00:38:19,110 --> 00:38:24,210
you could buy they come with strain

00:38:21,270 --> 00:38:25,950
gauges and sort of conductive fabrics

00:38:24,210 --> 00:38:30,090
and then we have a bluetooth module

00:38:25,950 --> 00:38:32,850
that's that's about this big right that

00:38:30,090 --> 00:38:34,950
fits into a pocket and that sends us all

00:38:32,850 --> 00:38:38,130
the data that we have so in addition to

00:38:34,950 --> 00:38:40,740
50 Hertz from these consumer sensors we

00:38:38,130 --> 00:38:43,380
have 10 hertz samples for every major

00:38:40,740 --> 00:38:44,940
movement and if you look at measure the

00:38:43,380 --> 00:38:48,750
performance of our classifiers if you

00:38:44,940 --> 00:38:52,800
these are very good but then we measured

00:38:48,750 --> 00:38:55,860
f1 our fully wired human and we were

00:38:52,800 --> 00:38:58,170
consistent for getting 120 so we were

00:38:55,860 --> 00:39:00,240
very very very pleased now we still

00:38:58,170 --> 00:39:02,400
don't have a notion of quality of

00:39:00,240 --> 00:39:03,600
exercise we don't know what it is so we

00:39:02,400 --> 00:39:05,970
can tell you you've done this movement

00:39:03,600 --> 00:39:09,330
we can tell you what was we can't tell

00:39:05,970 --> 00:39:10,860
you whether it was good so that's a lot

00:39:09,330 --> 00:39:13,170
of work that we're going to have to do

00:39:10,860 --> 00:39:15,630
it will end up with physiotherapy injury

00:39:13,170 --> 00:39:20,280
prevention smart wearables all this good

00:39:15,630 --> 00:39:22,350
stuff so the official line is that we

00:39:20,280 --> 00:39:24,990
are a cake solutions can play a you know

00:39:22,350 --> 00:39:27,750
very very very very experienced scholar

00:39:24,990 --> 00:39:29,550
program as we are based here and in

00:39:27,750 --> 00:39:31,650
London and in Manchester we're hiring so

00:39:29,550 --> 00:39:33,390
if anyone wants to do this sort of crazy

00:39:31,650 --> 00:39:35,510
stuff the stuff that Martin talked about

00:39:33,390 --> 00:39:39,900
other stuff that better talked about

00:39:35,510 --> 00:39:41,920
talk to us um and that's it thank you

00:39:39,900 --> 00:39:51,520
for attention any questions

00:39:41,920 --> 00:39:55,420
I think this oh yes good good good

00:39:51,520 --> 00:39:58,720
question it finished of course it did

00:39:55,420 --> 00:40:01,869
right so thank you for that so we did it

00:39:58,720 --> 00:40:05,740
we fitted three layers with hot boys

00:40:01,869 --> 00:40:09,549
hands and we trained what was it 30

00:40:05,740 --> 00:40:11,770
epochs and we go down to 0.03 loss and

00:40:09,549 --> 00:40:13,869
point for two accuracy so that suit

00:40:11,770 --> 00:40:15,880
sounds really bad right but these are

00:40:13,869 --> 00:40:18,069
many different exercises what you need

00:40:15,880 --> 00:40:20,710
to do the actual recognition is the

00:40:18,069 --> 00:40:22,900
external context so you know what the

00:40:20,710 --> 00:40:24,430
user is likely to do you know where the

00:40:22,900 --> 00:40:27,220
user is standing so that allows you to

00:40:24,430 --> 00:40:29,380
weed out all these other options and if

00:40:27,220 --> 00:40:32,079
you think about there are exercises that

00:40:29,380 --> 00:40:34,720
we can't recognize so if you do this and

00:40:32,079 --> 00:40:36,339
you do some sort of kicking well we're

00:40:34,720 --> 00:40:38,740
stuck right because you know these don't

00:40:36,339 --> 00:40:40,930
move so it's where most sensors will

00:40:38,740 --> 00:40:42,460
help no one really wants to wear

00:40:40,930 --> 00:40:45,490
anything on their ankles for some reason

00:40:42,460 --> 00:40:53,500
so you know we have to give them the

00:40:45,490 --> 00:40:55,180
smart clothes sure so they end up as raw

00:40:53,500 --> 00:40:57,579
values of floating point numbers so we

00:40:55,180 --> 00:40:59,619
persist the weights and biases we know

00:40:57,579 --> 00:41:01,809
the layout of memory and we dump them

00:40:59,619 --> 00:41:03,250
into just an array of floating point

00:41:01,809 --> 00:41:06,430
numbers who will run on single precision

00:41:03,250 --> 00:41:09,339
and we then shift the models back to iOS

00:41:06,430 --> 00:41:11,740
and we can evaluate them on that using

00:41:09,339 --> 00:41:12,880
the the DSP and the vector units so all

00:41:11,740 --> 00:41:15,369
the computation that happens on the

00:41:12,880 --> 00:41:17,890
phone is actually done using vectorized

00:41:15,369 --> 00:41:21,819
operations which is really cool we're

00:41:17,890 --> 00:41:24,660
end up as about 5% cpu usage even when

00:41:21,819 --> 00:41:24,660
the data is coming in

00:41:29,020 --> 00:41:32,950
so it's the same oh hang on I'll repeat

00:41:31,150 --> 00:41:34,990
the question I'm research so what do we

00:41:32,950 --> 00:41:38,350
use for Spock instance provisioning so

00:41:34,990 --> 00:41:40,900
these are all marathon jobs on me sauce

00:41:38,350 --> 00:41:42,490
so the spark jobs take you know some

00:41:40,900 --> 00:41:44,230
time to use we use marathon also for

00:41:42,490 --> 00:41:47,080
scheduling so we run at a particular

00:41:44,230 --> 00:41:49,540
time in the day when typically the AWS

00:41:47,080 --> 00:41:53,170
instance costs are lowest so we just say

00:41:49,540 --> 00:41:56,140
make so many tasks of this kind the

00:41:53,170 --> 00:42:00,010
spark jobs take about 20-25 minutes to

00:41:56,140 --> 00:42:03,570
run so we say to marathon run this and

00:42:00,010 --> 00:42:03,570
then kill the jobs when they're finished

00:42:06,390 --> 00:42:10,540
all right well I'll take stunned silence

00:42:08,980 --> 00:42:14,160
as a thank you and thank you for

00:42:10,540 --> 00:42:14,160

YouTube URL: https://www.youtube.com/watch?v=HUN_BQQ3pVo


