Title: Spark 2.0 - by Matei Zaharia
Publication date: 2016-06-17
Playlist: Scala Days New York 2016
Description: 
	This talk was recorded at Scala Days New York, 2016. Follow along on Twitter @scaladays and on the website for more information http://scaladays.org/.

Abstract:
Apache Spark is one of the largest Scala projects, as well as the most active open source project in big data processing. It was one of the first systems to provide a functional API in Scala and automatically distribute the work over clusters, simplifying distributed programming. In this keynote, I'll talk about how Spark's API has evolved since the first version, and in particular about new APIs in the soon to be released Spark 2.0. The largest additions have been to make the API more declarative, allowing richer automatic optimizations, and to provide a stronger link between Scala data types and a binary data format, enabling memory- and CPU-efficient processing. These ideas may be relevant to other high-performance libraries in Scala.
Captions: 
	00:00:03,770 --> 00:00:08,849
Thanks thanks a lot Martin and thanks

00:00:06,240 --> 00:00:12,570
everyone for waking up early and and

00:00:08,849 --> 00:00:14,849
showing up to my talk so I'm excited to

00:00:12,570 --> 00:00:17,580
talk today about spark 2.0 and and more

00:00:14,849 --> 00:00:21,000
generally I'm going to talk about sort

00:00:17,580 --> 00:00:23,430
of the development of api's in spark as

00:00:21,000 --> 00:00:25,769
we learned from you know from the first

00:00:23,430 --> 00:00:27,720
version of it so you know as I'll talk

00:00:25,769 --> 00:00:30,210
about spark started out with this kind

00:00:27,720 --> 00:00:32,669
of crazy idea of just taking the

00:00:30,210 --> 00:00:34,500
dysfunctional language Scala and making

00:00:32,669 --> 00:00:36,329
all the functions on on a cluster and

00:00:34,500 --> 00:00:38,910
making some of the distribution

00:00:36,329 --> 00:00:40,379
transparent in that way and I'll talk

00:00:38,910 --> 00:00:42,629
about you know what worked well with

00:00:40,379 --> 00:00:45,239
that and and what didn't and some of the

00:00:42,629 --> 00:00:48,210
new things we've been doing in it so

00:00:45,239 --> 00:00:50,550
what is Apache spark it's it's a

00:00:48,210 --> 00:00:53,160
general-purpose processing engine for

00:00:50,550 --> 00:00:55,350
clusters so by general I mean it tries

00:00:53,160 --> 00:00:58,170
to support a wide variety of workloads

00:00:55,350 --> 00:01:00,840
and it does this by basically extending

00:00:58,170 --> 00:01:02,820
Google's MapReduce model to support more

00:01:00,840 --> 00:01:05,369
types of applications in particular it

00:01:02,820 --> 00:01:07,170
adds these these primitives for data

00:01:05,369 --> 00:01:09,390
sharing that let it do it ahead of

00:01:07,170 --> 00:01:11,250
things and streaming and all kinds of

00:01:09,390 --> 00:01:14,430
things I need to maintain state as they

00:01:11,250 --> 00:01:15,900
are and the engine provides you know all

00:01:14,430 --> 00:01:18,840
the things you'd expect in this kind of

00:01:15,900 --> 00:01:22,009
engine distribution of work scheduling

00:01:18,840 --> 00:01:24,630
load balancing fault tolerance and so on

00:01:22,009 --> 00:01:26,700
increasingly SPARC is also not just an

00:01:24,630 --> 00:01:28,710
engine but a collection of api's and

00:01:26,700 --> 00:01:31,829
libraries that are all designed to work

00:01:28,710 --> 00:01:33,540
well together so it has the core sort of

00:01:31,829 --> 00:01:35,579
functional API you can use it in

00:01:33,540 --> 00:01:37,890
different languages including of course

00:01:35,579 --> 00:01:41,490
Scala which is what it started out with

00:01:37,890 --> 00:01:44,490
and it also has these four standard

00:01:41,490 --> 00:01:46,560
libraries on top for sequel streaming

00:01:44,490 --> 00:01:49,560
machine learning and graph processing

00:01:46,560 --> 00:01:51,060
and you know these are all designed to

00:01:49,560 --> 00:01:54,479
work well together so you can combine

00:01:51,060 --> 00:01:56,820
them in a program and spark has going

00:01:54,479 --> 00:01:58,290
into a very you know pretty large open

00:01:56,820 --> 00:02:00,930
source community this is a picture of

00:01:58,290 --> 00:02:03,210
just the meetups of it around the world

00:02:00,930 --> 00:02:04,320
that I you know that are active like

00:02:03,210 --> 00:02:06,780
right now

00:02:04,320 --> 00:02:09,000
and if you look at just the commit

00:02:06,780 --> 00:02:11,520
activity on the project patches per

00:02:09,000 --> 00:02:13,830
month you know her bugs open things like

00:02:11,520 --> 00:02:15,780
that it's it's the most

00:02:13,830 --> 00:02:17,670
active of the open source Big Data

00:02:15,780 --> 00:02:21,180
projects right now it's been that way

00:02:17,670 --> 00:02:23,610
for probably like around two years and

00:02:21,180 --> 00:02:27,150
it's used in a variety of places we

00:02:23,610 --> 00:02:29,250
counted over a thousand deployments of

00:02:27,150 --> 00:02:31,590
it over a thousand organizations the

00:02:29,250 --> 00:02:35,190
largest cluster that people have talked

00:02:31,590 --> 00:02:36,840
about publicly is around 8,000 notes so

00:02:35,190 --> 00:02:39,660
these are some of the companies that use

00:02:36,840 --> 00:02:42,000
it okay so what am I going to have in

00:02:39,660 --> 00:02:43,890
this stock I'm going to start with kind

00:02:42,000 --> 00:02:46,290
of the original goals for the project

00:02:43,890 --> 00:02:48,960
the original vision and then how did the

00:02:46,290 --> 00:02:51,480
vision hold up and then I'll talk about

00:02:48,960 --> 00:02:54,630
probably sort of the most significant

00:02:51,480 --> 00:02:57,030
change or extension of SPARC api's which

00:02:54,630 --> 00:02:59,490
is structured api's there are keys to

00:02:57,030 --> 00:03:01,760
new api is new and the last year called

00:02:59,490 --> 00:03:04,320
data frames and datasets that try to

00:03:01,760 --> 00:03:06,480
address some challenges in the original

00:03:04,320 --> 00:03:08,730
one I'll talk about optimizing

00:03:06,480 --> 00:03:10,830
underneath the structure is API some

00:03:08,730 --> 00:03:12,420
cool things you can do there and then

00:03:10,830 --> 00:03:15,040
about some of the next steps in this

00:03:12,420 --> 00:03:20,439
direction in spark together

00:03:15,040 --> 00:03:22,569
okay cool so so so what's the original

00:03:20,439 --> 00:03:25,180
spark vision so basically we have two

00:03:22,569 --> 00:03:27,760
goals for the project first goal is we

00:03:25,180 --> 00:03:29,500
wanted to have a unified engine for big

00:03:27,760 --> 00:03:32,019
data processing that supports the

00:03:29,500 --> 00:03:34,000
different types of workloads so when you

00:03:32,019 --> 00:03:36,220
do big data processing you're going to

00:03:34,000 --> 00:03:39,040
run a whole bunch of batch jobs but a

00:03:36,220 --> 00:03:40,840
lot of it is also interactive exploiting

00:03:39,040 --> 00:03:42,700
the data a lot of it is iterative

00:03:40,840 --> 00:03:45,099
algorithms that go over the data many

00:03:42,700 --> 00:03:46,870
times and eventually if something works

00:03:45,099 --> 00:03:51,700
well maybe you want to turn it into a

00:03:46,870 --> 00:03:53,650
streaming sort of continuous job and you

00:03:51,700 --> 00:03:55,750
know before spark a lot of these

00:03:53,650 --> 00:03:57,970
required separate engines basically to

00:03:55,750 --> 00:04:00,549
build which was you know very very

00:03:57,970 --> 00:04:03,129
time-consuming for for developers so we

00:04:00,549 --> 00:04:05,109
wanted a unified engine the second goal

00:04:03,129 --> 00:04:08,260
we had is that we wanted this concise

00:04:05,109 --> 00:04:10,150
language integrated API that looks kind

00:04:08,260 --> 00:04:12,099
of like working with local collections

00:04:10,150 --> 00:04:14,980
of data and we did this through this

00:04:12,099 --> 00:04:16,500
functional programming API in Scala Java

00:04:14,980 --> 00:04:19,840
and Python

00:04:16,500 --> 00:04:23,620
okay so just what's the motive the

00:04:19,840 --> 00:04:26,560
motivation so when you look at cluster

00:04:23,620 --> 00:04:28,419
computing engines the first one was

00:04:26,560 --> 00:04:30,280
MapReduce and actually a big deal about

00:04:28,419 --> 00:04:32,979
MapReduce if you go back and read the

00:04:30,280 --> 00:04:35,680
presentations on it or the papers or and

00:04:32,979 --> 00:04:37,900
so on is how general it was Google said

00:04:35,680 --> 00:04:39,430
hey we built this thing once and then we

00:04:37,900 --> 00:04:41,400
were able to run all these different

00:04:39,430 --> 00:04:44,910
applications on top of it you know

00:04:41,400 --> 00:04:47,229
reporting every night web indexing

00:04:44,910 --> 00:04:50,410
machine learning all this kind of stuff

00:04:47,229 --> 00:04:53,020
so generality was super important but

00:04:50,410 --> 00:04:55,090
after this came out there were a whole

00:04:53,020 --> 00:04:56,889
bunch of new workloads that acquired

00:04:55,090 --> 00:04:59,650
slightly different things from the

00:04:56,889 --> 00:05:01,690
engine and people ended up building a

00:04:59,650 --> 00:05:04,479
whole bunch of specialized systems so

00:05:01,690 --> 00:05:06,580
even in Google they built per a goal

00:05:04,479 --> 00:05:08,680
which is a graph processing system or

00:05:06,580 --> 00:05:11,020
Dremel which is an interactive sequel

00:05:08,680 --> 00:05:13,270
one and outside Google mostly in the

00:05:11,020 --> 00:05:15,250
Apache open-source community there were

00:05:13,270 --> 00:05:18,010
all these other systems like giraffe and

00:05:15,250 --> 00:05:19,889
an Impala and storm that that are

00:05:18,010 --> 00:05:22,570
specialized for different workloads and

00:05:19,889 --> 00:05:24,070
these you know having these is great

00:05:22,570 --> 00:05:26,950
because you can do more workloads but

00:05:24,070 --> 00:05:28,420
it's hard to manage tune and deploy all

00:05:26,950 --> 00:05:29,860
of this all

00:05:28,420 --> 00:05:31,840
you know all the hard work if you've

00:05:29,860 --> 00:05:33,820
ever done it of managing something like

00:05:31,840 --> 00:05:35,680
Hadoop now you get to do it over and

00:05:33,820 --> 00:05:38,350
over again for each each of these

00:05:35,680 --> 00:05:40,780
applications and even more than that

00:05:38,350 --> 00:05:43,300
even if you can install them and stuff

00:05:40,780 --> 00:05:45,610
it's hard to combine them into pipelines

00:05:43,300 --> 00:05:47,320
so a real application will usually use a

00:05:45,610 --> 00:05:49,660
mix of these things you don't just get

00:05:47,320 --> 00:05:51,250
like a graph and it to you or a bunch of

00:05:49,660 --> 00:05:54,130
vectors for machine learning you have a

00:05:51,250 --> 00:05:56,560
pipeline that transforms data and maybe

00:05:54,130 --> 00:05:58,030
run several different algorithms and now

00:05:56,560 --> 00:05:59,800
there are different engines with their

00:05:58,030 --> 00:06:03,070
own programming models and data formats

00:05:59,800 --> 00:06:06,700
and so on so the question was can we go

00:06:03,070 --> 00:06:09,700
back to a more general engine and still

00:06:06,700 --> 00:06:11,650
capture these workloads and that's what

00:06:09,700 --> 00:06:14,710
we wanted to do with with spark to

00:06:11,650 --> 00:06:17,320
design a unified engine for these okay

00:06:14,710 --> 00:06:19,600
so that's the unification part for the

00:06:17,320 --> 00:06:22,660
concise API the motivation is pretty

00:06:19,600 --> 00:06:24,700
simple a lot of data analysis is is very

00:06:22,660 --> 00:06:26,200
exploratory or interactive we don't want

00:06:24,700 --> 00:06:29,380
to spend a lot of time writing these

00:06:26,200 --> 00:06:31,450
jobs so spark introduced this this API

00:06:29,380 --> 00:06:34,630
called resilient distributed data sets

00:06:31,450 --> 00:06:36,190
or rdd's which are just distributed

00:06:34,630 --> 00:06:38,470
collections if you work with scholar

00:06:36,190 --> 00:06:40,870
collections you already know a lot of

00:06:38,470 --> 00:06:42,610
what it takes to work with rdd's so

00:06:40,870 --> 00:06:48,250
they're distributed collections with a

00:06:42,610 --> 00:06:51,480
simple sort of functional API so as a

00:06:48,250 --> 00:06:54,160
really simple example in spark you can

00:06:51,480 --> 00:06:56,590
create something like a text file which

00:06:54,160 --> 00:06:57,970
is an RDD of strings it's a distributed

00:06:56,590 --> 00:07:00,880
collection of strings and some

00:06:57,970 --> 00:07:02,830
distributed file system you can do

00:07:00,880 --> 00:07:04,450
things like maps on it so here we're

00:07:02,830 --> 00:07:06,100
going to turn it into a bunch of point

00:07:04,450 --> 00:07:08,410
objects now we get a collection of

00:07:06,100 --> 00:07:11,440
points you can do filters you can do

00:07:08,410 --> 00:07:14,290
counts you know group wise joins all

00:07:11,440 --> 00:07:16,690
kinds of operations like that and the

00:07:14,290 --> 00:07:19,540
API I guess I should say the API is also

00:07:16,690 --> 00:07:22,330
lazily evaluated so it doesn't run these

00:07:19,540 --> 00:07:24,360
things until you call a special

00:07:22,330 --> 00:07:27,550
operation that produces an output and

00:07:24,360 --> 00:07:29,290
that means that it has some chance to to

00:07:27,550 --> 00:07:31,750
optimize the job based on what you're

00:07:29,290 --> 00:07:33,850
doing at the end so in this case like

00:07:31,750 --> 00:07:36,250
you know when we're doing a map and an

00:07:33,850 --> 00:07:38,080
account it's never gonna save these

00:07:36,250 --> 00:07:39,730
intermediate results it just counts the

00:07:38,080 --> 00:07:41,710
ones that pass the filter so it does a

00:07:39,730 --> 00:07:44,590
little bit of optimization

00:07:41,710 --> 00:07:48,009
underneath that as well okay so that's

00:07:44,590 --> 00:07:49,629
kind of the basic spark so how did the

00:07:48,009 --> 00:07:52,090
vision actually hold up you know are

00:07:49,629 --> 00:07:55,120
people using it in the way we kind of

00:07:52,090 --> 00:07:57,940
intended generally it worked it worked

00:07:55,120 --> 00:08:00,310
pretty well users really liked this

00:07:57,940 --> 00:08:02,620
unification aspect most users used a mix

00:08:00,310 --> 00:08:04,150
of these libraries and do applications

00:08:02,620 --> 00:08:07,300
that combine different things which is

00:08:04,150 --> 00:08:09,759
really really cool to see the functional

00:08:07,300 --> 00:08:13,030
API and basically that just building at

00:08:09,759 --> 00:08:15,400
around Scala functions and objects did

00:08:13,030 --> 00:08:18,520
cause some challenges and these are the

00:08:15,400 --> 00:08:20,860
ones that I'm going to talk about now so

00:08:18,520 --> 00:08:22,479
on the unification side here's you know

00:08:20,860 --> 00:08:24,729
here's a guy picture of the library is

00:08:22,479 --> 00:08:26,440
built on spark we were able we built

00:08:24,729 --> 00:08:28,599
these all kind of after the core engine

00:08:26,440 --> 00:08:30,430
came out and we were able to capture a

00:08:28,599 --> 00:08:32,740
whole bunch of workloads you know sequel

00:08:30,430 --> 00:08:35,349
query streaming machine learning and

00:08:32,740 --> 00:08:38,320
graphs and to run them on the same API

00:08:35,349 --> 00:08:40,269
involving rdd's and together they

00:08:38,320 --> 00:08:42,760
actually they're currently the largest

00:08:40,269 --> 00:08:45,790
sort of integrated standard library for

00:08:42,760 --> 00:08:49,089
Big Data and when you look at how people

00:08:45,790 --> 00:08:51,870
use them people actually use a good

00:08:49,089 --> 00:08:55,510
mixture of these things so this is a

00:08:51,870 --> 00:08:57,250
graph from a survey we did last year so

00:08:55,510 --> 00:08:59,890
you can see each of these libraries on

00:08:57,250 --> 00:09:03,820
top of the Coe API is used by at least

00:08:59,890 --> 00:09:07,209
50% of the organizations in this survey

00:09:03,820 --> 00:09:09,360
and also 75% of them use more than one

00:09:07,209 --> 00:09:12,459
component so basically two of the

00:09:09,360 --> 00:09:13,899
libraries together and we're excited to

00:09:12,459 --> 00:09:15,760
see this because this is you know one of

00:09:13,899 --> 00:09:18,779
the things we wanted to enable is to let

00:09:15,760 --> 00:09:21,160
you mix together these processing types

00:09:18,779 --> 00:09:23,589
the applications people use them for

00:09:21,160 --> 00:09:25,290
also interesting there's a whole there's

00:09:23,589 --> 00:09:28,029
a whole range of those as well which is

00:09:25,290 --> 00:09:29,500
which is nice to see because you know we

00:09:28,029 --> 00:09:32,260
we wanted to have something pretty

00:09:29,500 --> 00:09:33,370
general so the most common things are

00:09:32,260 --> 00:09:35,140
data warehousing and business

00:09:33,370 --> 00:09:37,720
intelligence which means basically

00:09:35,140 --> 00:09:40,149
computing reports you know kind of

00:09:37,720 --> 00:09:42,670
simple like statistical aggregations or

00:09:40,149 --> 00:09:44,440
data cubes or things like that but we

00:09:42,670 --> 00:09:47,709
also see things like recommendation

00:09:44,440 --> 00:09:49,810
engines log processing user-facing

00:09:47,709 --> 00:09:52,570
services so this is an application where

00:09:49,810 --> 00:09:54,140
like a user is interacting with a UI and

00:09:52,570 --> 00:09:57,680
it's running spark on the back

00:09:54,140 --> 00:09:59,330
as they interact with it and Argus this

00:09:57,680 --> 00:10:02,900
is misspelled a fraud detection and

00:09:59,330 --> 00:10:06,590
security so so these are also some of

00:10:02,900 --> 00:10:07,880
the things people are doing okay so

00:10:06,590 --> 00:10:11,180
that's that's kind of the unification

00:10:07,880 --> 00:10:13,250
side so what about the kind of the

00:10:11,180 --> 00:10:15,350
challenge the functional API not nothing

00:10:13,250 --> 00:10:17,740
wrong here by the way with the idea of

00:10:15,350 --> 00:10:21,080
functional programming just kind of the

00:10:17,740 --> 00:10:23,180
level of visibility we have into in the

00:10:21,080 --> 00:10:24,860
user programs is the problem so

00:10:23,180 --> 00:10:26,570
basically you know the API looks high

00:10:24,860 --> 00:10:29,180
level like I showed you this code where

00:10:26,570 --> 00:10:31,610
we're just starting map on a distributed

00:10:29,180 --> 00:10:33,440
file and you know we automatically send

00:10:31,610 --> 00:10:35,450
the work there to all the nodes and

00:10:33,440 --> 00:10:37,910
split it up across the cluster and stuff

00:10:35,450 --> 00:10:40,700
but it actually hides a lot of the

00:10:37,910 --> 00:10:42,740
semantics of the computation from the

00:10:40,700 --> 00:10:44,570
engine and that makes it hard for the

00:10:42,740 --> 00:10:45,860
engine to automatically optimize stuff

00:10:44,570 --> 00:10:48,800
and it means the programmer you know

00:10:45,860 --> 00:10:52,220
needs to needs to be pretty careful to

00:10:48,800 --> 00:10:54,230
get it on efficiently so the functions

00:10:52,220 --> 00:10:56,450
that we see you know like the lambda

00:10:54,230 --> 00:10:58,580
function I pass to a map is just an

00:10:56,450 --> 00:11:00,470
arbitrary block of Java code it's really

00:10:58,580 --> 00:11:03,050
hard to figure out what it's trying to

00:11:00,470 --> 00:11:05,030
do and the data is also stored in

00:11:03,050 --> 00:11:07,310
arbitrary Java objects so there's not a

00:11:05,030 --> 00:11:09,950
lot of them to optimize how it's stored

00:11:07,310 --> 00:11:12,320
and how it's sent across the network and

00:11:09,950 --> 00:11:15,940
kind of the end result of this is users

00:11:12,320 --> 00:11:20,740
can mix the api's and in suboptimal ways

00:11:15,940 --> 00:11:23,120
so as a little kind of quiz on that

00:11:20,740 --> 00:11:25,460
basically I'm curious if there people

00:11:23,120 --> 00:11:28,880
can guess which of the API calls and

00:11:25,460 --> 00:11:30,650
spark causes the most sort of support

00:11:28,880 --> 00:11:32,690
tickets are like questions on the

00:11:30,650 --> 00:11:34,310
mailing list you know some of these are

00:11:32,690 --> 00:11:39,110
used more often than others but I'm

00:11:34,310 --> 00:11:42,770
wondering people have some idea wait

00:11:39,110 --> 00:11:44,510
right join that's a good guess yeah

00:11:42,770 --> 00:11:48,980
join is definitely tricky but it's

00:11:44,510 --> 00:11:50,690
actually not done that happen so other

00:11:48,980 --> 00:11:52,520
ones yeah so someone said so it's

00:11:50,690 --> 00:11:55,600
actually yeah it's it's actually grouped

00:11:52,520 --> 00:11:58,090
by key this is a innocuous seeming

00:11:55,600 --> 00:12:00,320
operation that's unfortunately pretty

00:11:58,090 --> 00:12:02,810
pretty tricky just because of the way

00:12:00,320 --> 00:12:05,120
it's set up so so this is actually the

00:12:02,810 --> 00:12:07,130
one that causes the most problems and

00:12:05,120 --> 00:12:08,040
the reason why is because basically

00:12:07,130 --> 00:12:10,410
because group

00:12:08,040 --> 00:12:12,690
like he set up to it or a sequence

00:12:10,410 --> 00:12:14,970
because that's kind of how you were

00:12:12,690 --> 00:12:16,800
representing we thought it was most

00:12:14,970 --> 00:12:19,529
natural to represent groups it's Carla

00:12:16,800 --> 00:12:21,870
basically so here's an example of a

00:12:19,529 --> 00:12:25,050
problem this is just gonna be a use of

00:12:21,870 --> 00:12:27,600
word count basically so we have a file

00:12:25,050 --> 00:12:29,610
say it's a bunch of words and we map it

00:12:27,600 --> 00:12:31,889
to you know fair is word comma one

00:12:29,610 --> 00:12:34,259
that's that's that's fine

00:12:31,889 --> 00:12:38,130
and then we group them by key and then

00:12:34,259 --> 00:12:40,769
for each list of ones we compute its sum

00:12:38,130 --> 00:12:43,050
so that looks like you know like a

00:12:40,769 --> 00:12:45,420
reasonable way to do word count maybe

00:12:43,050 --> 00:12:47,279
and if you were working with a small

00:12:45,420 --> 00:12:49,649
data set like you know a thousand words

00:12:47,279 --> 00:12:52,829
a million words on your laptop that's

00:12:49,649 --> 00:12:55,440
okay but the problem is the API of these

00:12:52,829 --> 00:12:57,990
things so the API of group by key is

00:12:55,440 --> 00:13:00,930
that it returns you know an RDD

00:12:57,990 --> 00:13:03,149
containing sequences of of the value

00:13:00,930 --> 00:13:06,569
like sequence of int in this case so you

00:13:03,149 --> 00:13:08,430
have so that means all the ones that we

00:13:06,569 --> 00:13:10,290
sent across the network even though you

00:13:08,430 --> 00:13:12,240
know we're just trying to add them at

00:13:10,290 --> 00:13:15,660
the end first they get put into this

00:13:12,240 --> 00:13:17,910
giant vector of ones which needs to be

00:13:15,660 --> 00:13:20,130
sitting in memory because the you know

00:13:17,910 --> 00:13:21,750
the API is like we're just going to give

00:13:20,130 --> 00:13:24,360
you a sequence of these and then you're

00:13:21,750 --> 00:13:26,010
gonna run weird arbitrary java code on

00:13:24,360 --> 00:13:27,779
it and we don't know what it's doing so

00:13:26,010 --> 00:13:30,720
like it better actually be a sequence

00:13:27,779 --> 00:13:32,819
and then later well as soon as we did

00:13:30,720 --> 00:13:35,160
all the work to create a giant sequence

00:13:32,819 --> 00:13:36,750
of ones you're just going to sum them so

00:13:35,160 --> 00:13:38,430
you know looking at this it would have

00:13:36,750 --> 00:13:40,560
been really nice if we somehow knew that

00:13:38,430 --> 00:13:42,420
you're going to sum them and and not

00:13:40,560 --> 00:13:45,149
have to actually materialize these guys

00:13:42,420 --> 00:13:47,519
in memory now the solution and in SPARC

00:13:45,149 --> 00:13:49,110
is of course you don't use group by key

00:13:47,519 --> 00:13:51,480
here there's a different operation

00:13:49,110 --> 00:13:55,050
called reduced by key but for users

00:13:51,480 --> 00:13:57,149
especially you know new users or people

00:13:55,050 --> 00:13:58,589
you know not not from a big data

00:13:57,149 --> 00:14:01,589
background it can be pretty confusing

00:13:58,589 --> 00:14:03,899
because the operations look pretty

00:14:01,589 --> 00:14:06,240
innocuous also maybe a used to group by

00:14:03,899 --> 00:14:07,740
from a database and all of a sudden the

00:14:06,240 --> 00:14:09,839
combination of them isn't great and

00:14:07,740 --> 00:14:11,670
there's not much we can do in the engine

00:14:09,839 --> 00:14:13,740
because this guy's signature is to

00:14:11,670 --> 00:14:16,500
return a sequence and/or an iterable

00:14:13,740 --> 00:14:18,329
actually I should say but even with that

00:14:16,500 --> 00:14:20,100
you know we don't know what you're going

00:14:18,329 --> 00:14:21,780
to do later like you might be doing some

00:14:20,100 --> 00:14:23,790
but you might also be doing some

00:14:21,780 --> 00:14:26,880
some totally we think that that requires

00:14:23,790 --> 00:14:29,040
a real sequence okay so this is this is

00:14:26,880 --> 00:14:31,830
what I mean by kind of the

00:14:29,040 --> 00:14:34,530
object-oriented functional API being a

00:14:31,830 --> 00:14:36,960
problem and then a second problem with

00:14:34,530 --> 00:14:39,420
it is that you know the API is in terms

00:14:36,960 --> 00:14:41,130
of Java objects and although we can play

00:14:39,420 --> 00:14:43,200
tricks like when we send them over the

00:14:41,130 --> 00:14:45,630
network and or even when we store them

00:14:43,200 --> 00:14:46,890
in memory we can serialize them most of

00:14:45,630 --> 00:14:49,470
the time when you're working with Java

00:14:46,890 --> 00:14:51,870
objects they just have a lot of space

00:14:49,470 --> 00:14:53,940
overhead and for an in-memory computing

00:14:51,870 --> 00:14:57,570
framework in particular that that can be

00:14:53,940 --> 00:14:58,680
a problem so even if you have a simple

00:14:57,570 --> 00:15:01,170
object in Java

00:14:58,680 --> 00:15:03,990
like here we've got a user class with a

00:15:01,170 --> 00:15:07,080
name and a bunch of ins a bunch of

00:15:03,990 --> 00:15:09,510
friend friend IDs if you look in the

00:15:07,080 --> 00:15:11,670
object most of most of the memory

00:15:09,510 --> 00:15:14,070
allocated for it is actually kind of

00:15:11,670 --> 00:15:16,170
overhead so each Java object has a

00:15:14,070 --> 00:15:18,390
header that's usually around 16 bytes

00:15:16,170 --> 00:15:20,550
and then in this case the user is going

00:15:18,390 --> 00:15:23,160
to have pointers to a string and an int

00:15:20,550 --> 00:15:25,740
array each of those will have a header

00:15:23,160 --> 00:15:28,470
and then the internet' has its lengths

00:15:25,740 --> 00:15:31,290
and then it has a bunch of data and then

00:15:28,470 --> 00:15:33,720
the string itself is an object with a

00:15:31,290 --> 00:15:35,280
pointer to a char array and then the

00:15:33,720 --> 00:15:36,720
char array you know also has a length

00:15:35,280 --> 00:15:39,570
and a bunch of data and maybe some

00:15:36,720 --> 00:15:42,450
padding to make it you know kind of

00:15:39,570 --> 00:15:44,250
aligned to a reasonable size so when you

00:15:42,450 --> 00:15:46,589
look at it probably like two-thirds of

00:15:44,250 --> 00:15:49,560
the space in here is kind of wasted and

00:15:46,589 --> 00:15:51,540
and this is this is typical and a lot of

00:15:49,560 --> 00:15:53,850
common data structures again it's fine

00:15:51,540 --> 00:15:55,830
when you work with like a few thousand

00:15:53,850 --> 00:15:57,750
of them in your laptop but it's not that

00:15:55,830 --> 00:16:00,839
great when you're trying to work with

00:15:57,750 --> 00:16:02,550
lots of them okay so basically these

00:16:00,839 --> 00:16:04,680
were kind of the two problems but we did

00:16:02,550 --> 00:16:06,540
I mean overall people do like the kind

00:16:04,680 --> 00:16:09,630
of language integrated API so we don't

00:16:06,540 --> 00:16:13,710
want to step away from that either so

00:16:09,630 --> 00:16:15,660
how are we solving this so basically to

00:16:13,710 --> 00:16:18,660
address these problems we we introduce

00:16:15,660 --> 00:16:20,580
two new API Xin spark data frames and

00:16:18,660 --> 00:16:22,860
data sets that are structured API so

00:16:20,580 --> 00:16:24,720
I'll talk about that but they put a few

00:16:22,860 --> 00:16:26,550
more restrictions on the data so that

00:16:24,720 --> 00:16:28,890
you can understand its structure and

00:16:26,550 --> 00:16:31,080
optimize for it and these API is

00:16:28,890 --> 00:16:33,450
complement the original RDD what

00:16:31,080 --> 00:16:35,430
actually they're implemented on top of

00:16:33,450 --> 00:16:36,630
it and they

00:16:35,430 --> 00:16:38,910
see they're just kind of higher-level

00:16:36,630 --> 00:16:43,050
api's but you can easily interoperate

00:16:38,910 --> 00:16:45,720
with code using rdd's as well okay so

00:16:43,050 --> 00:16:48,300
what is structured why are we doing this

00:16:45,720 --> 00:16:50,640
so structure you know basically it means

00:16:48,300 --> 00:16:51,180
that to construct or arranged according

00:16:50,640 --> 00:16:53,610
to a plan

00:16:51,180 --> 00:16:54,990
to give a pattern or organization so

00:16:53,610 --> 00:16:57,510
that's what we're trying to do to the

00:16:54,990 --> 00:16:59,130
data as well as to a lot of the

00:16:57,510 --> 00:17:02,160
computations you put in it so that we

00:16:59,130 --> 00:17:04,250
can understand what they're doing why do

00:17:02,160 --> 00:17:07,380
we want to do this by definition

00:17:04,250 --> 00:17:09,390
structure will obviously limit what can

00:17:07,380 --> 00:17:12,870
be expressed because it it makes things

00:17:09,390 --> 00:17:14,579
fit a particular plan but in practice we

00:17:12,870 --> 00:17:16,250
found that you know even using the

00:17:14,579 --> 00:17:19,199
structured API is you can actually

00:17:16,250 --> 00:17:21,540
capture a lot of the computations people

00:17:19,199 --> 00:17:23,699
really want to do on a cluster and using

00:17:21,540 --> 00:17:28,110
features in Scala for example such as

00:17:23,699 --> 00:17:29,760
case classes and you know implicit and

00:17:28,110 --> 00:17:32,130
things like that it's very lightweight

00:17:29,760 --> 00:17:34,470
for the user the user as well so it's

00:17:32,130 --> 00:17:36,650
pretty pretty easy to just use these the

00:17:34,470 --> 00:17:39,600
same way that the functional API worked

00:17:36,650 --> 00:17:42,900
and basically limiting the space of what

00:17:39,600 --> 00:17:44,730
you can Express enables optimizations

00:17:42,900 --> 00:17:47,040
I'm gonna spend a whole bunch of time

00:17:44,730 --> 00:17:51,480
talking about optimizations underneath

00:17:47,040 --> 00:17:53,100
the structured API okay so data frames

00:17:51,480 --> 00:17:55,890
and data sets are language integrated

00:17:53,100 --> 00:17:57,809
API is for structured data they actually

00:17:55,890 --> 00:17:59,550
built on the spark sequel engine and

00:17:57,809 --> 00:18:01,290
spark sequel appear if you're not

00:17:59,550 --> 00:18:04,290
familiar with it it's essentially kind

00:18:01,290 --> 00:18:06,360
of analytical database engine so it does

00:18:04,290 --> 00:18:09,600
a lot of the optimizations you'd see in

00:18:06,360 --> 00:18:11,550
an analytical database such as thorough

00:18:09,600 --> 00:18:13,470
data or Vertica or something like that

00:18:11,550 --> 00:18:15,929
for example it does column-oriented

00:18:13,470 --> 00:18:17,880
storage and processing it does quite

00:18:15,929 --> 00:18:21,929
planning you know trying to optimize

00:18:17,880 --> 00:18:24,480
query plans and and so on so basically

00:18:21,929 --> 00:18:26,580
these these api is benefit from the same

00:18:24,480 --> 00:18:30,120
kind of optimizations you'd get in a

00:18:26,580 --> 00:18:31,410
high-performance optimized database but

00:18:30,120 --> 00:18:33,690
instead of writing sequel you know

00:18:31,410 --> 00:18:36,960
you're writing scala or Python or one of

00:18:33,690 --> 00:18:40,890
Sparx languages data frames are

00:18:36,960 --> 00:18:42,870
dynamically typed API and based I did

00:18:40,890 --> 00:18:45,090
there are common API for data science

00:18:42,870 --> 00:18:46,980
and things like R and Python and data

00:18:45,090 --> 00:18:48,669
sets are statically typed and that looks

00:18:46,980 --> 00:18:50,890
a lot like rdd's

00:18:48,669 --> 00:18:52,600
so I'll talk about both of them and all

00:18:50,890 --> 00:18:54,429
these components are pretty new sparks

00:18:52,600 --> 00:18:56,769
equal itself came out around two years

00:18:54,429 --> 00:18:58,809
ago and data frames and data sets came

00:18:56,769 --> 00:19:01,059
out last year but all they're all things

00:18:58,809 --> 00:19:04,690
you can use and in the latest spark

00:19:01,059 --> 00:19:07,090
releases so how do these things execute

00:19:04,690 --> 00:19:10,240
so basically at a high level we're going

00:19:07,090 --> 00:19:12,789
to start with query or like a

00:19:10,240 --> 00:19:15,220
computation could be sequel data frames

00:19:12,789 --> 00:19:17,289
or data sets we don't really carry all

00:19:15,220 --> 00:19:19,990
of these get turned into a logical plan

00:19:17,289 --> 00:19:22,600
which is a tree of operators you know

00:19:19,990 --> 00:19:24,250
kind of database operators and then

00:19:22,600 --> 00:19:25,899
we're going to optimize them we're gonna

00:19:24,250 --> 00:19:28,080
take in information about the data

00:19:25,899 --> 00:19:30,549
catalog as well of what's out there and

00:19:28,080 --> 00:19:33,070
create a physical plan that we can

00:19:30,549 --> 00:19:36,100
actually run and then we're going to use

00:19:33,070 --> 00:19:38,200
a code generation actually to generate

00:19:36,100 --> 00:19:40,029
just a program using rdd's that

00:19:38,200 --> 00:19:42,130
specialize for sort of the data types

00:19:40,029 --> 00:19:45,490
and things that you're using in your

00:19:42,130 --> 00:19:47,919
computation and finally a final piece of

00:19:45,490 --> 00:19:51,820
this sort of spark sequel and data set

00:19:47,919 --> 00:19:54,730
ecosystem is the data source API so we

00:19:51,820 --> 00:19:57,490
basically all these API scan work with a

00:19:54,730 --> 00:20:00,399
wide variety of data sources like Hadoop

00:19:57,490 --> 00:20:02,769
file system or Cassandra or JSON or

00:20:00,399 --> 00:20:04,899
stuff like that and there's a common way

00:20:02,769 --> 00:20:07,710
to plug those in as well which I which

00:20:04,899 --> 00:20:07,710
I'll talk about in a bit

00:20:08,840 --> 00:20:14,549
that's that's basically the picture very

00:20:12,510 --> 00:20:18,600
similar to kind of query planning in a

00:20:14,549 --> 00:20:22,980
database okay so what is the data frame

00:20:18,600 --> 00:20:25,260
API so data frames hold the hose which

00:20:22,980 --> 00:20:28,350
are you know the just kind of generic

00:20:25,260 --> 00:20:30,210
containers with an own schema so all the

00:20:28,350 --> 00:20:32,400
rows in it in one data frame have the

00:20:30,210 --> 00:20:34,710
same scheme at the same fields and they

00:20:32,400 --> 00:20:36,539
are for relational operations which

00:20:34,710 --> 00:20:38,400
means sort of database operations to a

00:20:36,539 --> 00:20:40,500
DSL

00:20:38,400 --> 00:20:43,230
so basically through embedded sort of

00:20:40,500 --> 00:20:45,840
DSL so here's kind of how you use them

00:20:43,230 --> 00:20:48,480
so you could create for example or hive

00:20:45,840 --> 00:20:52,470
context which is something used to read

00:20:48,480 --> 00:20:54,000
data from Apache hive and using you know

00:20:52,470 --> 00:20:57,090
you could use just sequel on it or you

00:20:54,000 --> 00:20:59,820
could do hive context table users but

00:20:57,090 --> 00:21:01,289
basically you can create a data frame

00:20:59,820 --> 00:21:04,409
that X you know

00:21:01,289 --> 00:21:05,909
expresses some some subset of the data

00:21:04,409 --> 00:21:07,679
in there

00:21:05,909 --> 00:21:10,169
and then you can do transformations on

00:21:07,679 --> 00:21:13,080
it so for example here I'm picking the

00:21:10,169 --> 00:21:15,419
the users that live in Massachusetts and

00:21:13,080 --> 00:21:17,640
this is kind of the DSL part so there's

00:21:15,419 --> 00:21:20,760
a small set of operators you can use to

00:21:17,640 --> 00:21:23,100
create these expressions and instead of

00:21:20,760 --> 00:21:24,659
getting back just a scholar function

00:21:23,100 --> 00:21:27,539
which is you know a bunch of byte code

00:21:24,659 --> 00:21:30,059
we actually get back an abstract syntax

00:21:27,539 --> 00:21:31,860
tree for the expression so we see you're

00:21:30,059 --> 00:21:33,840
looking for a field called state and

00:21:31,860 --> 00:21:35,909
users and you're checking if it's equal

00:21:33,840 --> 00:21:37,590
to Massachusetts so now we can actually

00:21:35,909 --> 00:21:39,659
understand what's happening in your

00:21:37,590 --> 00:21:43,890
expression and maybe do some more

00:21:39,659 --> 00:21:46,110
interesting optimizations underneath and

00:21:43,890 --> 00:21:48,030
then you can do you know standard stuff

00:21:46,110 --> 00:21:50,750
like in SPARC you can count these users

00:21:48,030 --> 00:21:53,150
you can group them and average them

00:21:50,750 --> 00:21:55,950
notice that here again instead of

00:21:53,150 --> 00:21:59,010
instead of just having some weird

00:21:55,950 --> 00:22:01,740
function after we have an explicit API

00:21:59,010 --> 00:22:03,870
to do average so so we don't run into

00:22:01,740 --> 00:22:05,940
that problem of figuring out what you're

00:22:03,870 --> 00:22:08,610
trying to do with the list if you use

00:22:05,940 --> 00:22:11,130
this API and if you don't like using the

00:22:08,610 --> 00:22:14,070
the the DSL to do these things you can

00:22:11,130 --> 00:22:16,440
also always view them again as Scala

00:22:14,070 --> 00:22:18,720
objects so in this case you know I'm

00:22:16,440 --> 00:22:20,789
going to do a map and those are these

00:22:18,720 --> 00:22:21,240
containers of arbitrary types so I have

00:22:20,789 --> 00:22:23,820
to

00:22:21,240 --> 00:22:26,309
the fields of them as you know using the

00:22:23,820 --> 00:22:28,350
special gutter methods so so you know I

00:22:26,309 --> 00:22:30,299
can get the first element and turn it to

00:22:28,350 --> 00:22:32,880
uppercase sorry or do my complicated

00:22:30,299 --> 00:22:34,980
scholar stuff but the nice thing is that

00:22:32,880 --> 00:22:37,110
a lot of the common stuff you do can be

00:22:34,980 --> 00:22:39,080
done in the DSL and then the engine

00:22:37,110 --> 00:22:41,779
understands it quite a bit better so

00:22:39,080 --> 00:22:45,450
that's that's kind of what it looks like

00:22:41,779 --> 00:22:47,789
why did we choose this particular API

00:22:45,450 --> 00:22:50,850
data frames are actually really popular

00:22:47,789 --> 00:22:53,429
concept for data science in languages

00:22:50,850 --> 00:22:55,380
like our hand Python in Python the

00:22:53,429 --> 00:22:57,390
interface to them it's called pandas and

00:22:55,380 --> 00:22:59,820
so we said okay this is something people

00:22:57,390 --> 00:23:02,370
already know we actually wanted to cover

00:22:59,820 --> 00:23:05,100
other languages anyway so let's give

00:23:02,370 --> 00:23:07,200
them a abstraction they already know but

00:23:05,100 --> 00:23:09,270
interestingly spark I think is the first

00:23:07,200 --> 00:23:11,279
data frame API to make a declarative

00:23:09,270 --> 00:23:14,730
which means to do optimization

00:23:11,279 --> 00:23:16,950
underneath so in in Python and in in our

00:23:14,730 --> 00:23:19,559
if you use data frames each of these

00:23:16,950 --> 00:23:21,390
steps is evaluated eagerly so there's no

00:23:19,559 --> 00:23:23,850
chance to do optimization like when you

00:23:21,390 --> 00:23:25,679
do this it actually creates a table with

00:23:23,850 --> 00:23:28,799
all the users in Massachusetts and then

00:23:25,679 --> 00:23:30,390
it counts them and and so on or when you

00:23:28,799 --> 00:23:32,190
do this it actually has to group them

00:23:30,390 --> 00:23:35,190
and the sequences for send and average

00:23:32,190 --> 00:23:37,860
them in sparks API it doesn't compute

00:23:35,190 --> 00:23:39,600
them until you force the result like

00:23:37,860 --> 00:23:42,270
until you you print it or save it

00:23:39,600 --> 00:23:46,080
somewhere so it can actually optimize it

00:23:42,270 --> 00:23:48,149
you know in a declarative way okay so

00:23:46,080 --> 00:23:50,399
that's kind of why we're you know why we

00:23:48,149 --> 00:23:52,320
chose to do this and then the other nice

00:23:50,399 --> 00:23:55,080
thing about doing this is it integrates

00:23:52,320 --> 00:23:56,570
well with other data science libraries

00:23:55,080 --> 00:23:58,860
and it also gives us a common

00:23:56,570 --> 00:24:01,169
interchange format between the different

00:23:58,860 --> 00:24:03,980
languages that spark supports so in

00:24:01,169 --> 00:24:06,450
spark itself the ML library for example

00:24:03,980 --> 00:24:08,820
machine learning takes data frames as

00:24:06,450 --> 00:24:11,309
input and output now so you know

00:24:08,820 --> 00:24:14,010
anything whether it's all Python or

00:24:11,309 --> 00:24:16,470
Scala can call into ml lib and just pass

00:24:14,010 --> 00:24:18,289
in a data frame and in Python and are

00:24:16,470 --> 00:24:20,399
you can map these to the sort of local

00:24:18,289 --> 00:24:22,559
single node data frames in those

00:24:20,399 --> 00:24:24,330
languages and then use the libraries

00:24:22,559 --> 00:24:27,740
that work on those so it's really easy

00:24:24,330 --> 00:24:27,740
to integrate it with these things

00:24:27,960 --> 00:24:34,840
okay all right and so what do what do

00:24:32,200 --> 00:24:36,640
the structured API is enabled so the

00:24:34,840 --> 00:24:39,280
first thing is more compact data a

00:24:36,640 --> 00:24:42,280
presentation we know the schema of the

00:24:39,280 --> 00:24:44,230
element it's a bunch you know when you

00:24:42,280 --> 00:24:46,150
specify the schema it's a bunch of data

00:24:44,230 --> 00:24:48,610
types that we understand like strings

00:24:46,150 --> 00:24:50,380
and integers and stuff and so we can

00:24:48,610 --> 00:24:52,270
decide how to store them we don't have

00:24:50,380 --> 00:24:54,580
to deal with like oh is this a class

00:24:52,270 --> 00:24:56,410
that has lots of subclasses or weird

00:24:54,580 --> 00:24:59,800
pointers to itself or stuff like that

00:24:56,410 --> 00:25:02,260
and in particular we use a column

00:24:59,800 --> 00:25:04,510
oriented compressed format whenever we

00:25:02,260 --> 00:25:06,420
store these things in memory when we

00:25:04,510 --> 00:25:09,340
process them we still turn them into

00:25:06,420 --> 00:25:12,610
into individual objects although that's

00:25:09,340 --> 00:25:16,000
likely to change actually in like two

00:25:12,610 --> 00:25:18,010
you know after 2.0 and the second thing

00:25:16,000 --> 00:25:19,810
that enables is optimization across the

00:25:18,010 --> 00:25:22,000
operators we can look in the operators

00:25:19,810 --> 00:25:23,710
see the expressions they're doing and do

00:25:22,000 --> 00:25:25,690
things like join the ordering or

00:25:23,710 --> 00:25:27,640
predicate push down you know moving the

00:25:25,690 --> 00:25:29,410
filter down the tree to do it as early

00:25:27,640 --> 00:25:31,510
as possible and so on

00:25:29,410 --> 00:25:32,770
same things you'd get in a database and

00:25:31,510 --> 00:25:34,540
then the final thing we get is a

00:25:32,770 --> 00:25:37,240
one-time code generation where we can

00:25:34,540 --> 00:25:39,010
generate code that's tailored and

00:25:37,240 --> 00:25:41,440
specialized for the computation you're

00:25:39,010 --> 00:25:43,600
doing right now and not have to deal

00:25:41,440 --> 00:25:45,340
with generics and innovators and and and

00:25:43,600 --> 00:25:48,460
all these kind of virtual function calls

00:25:45,340 --> 00:25:50,380
you get otherwise so in terms of

00:25:48,460 --> 00:25:52,390
performance here's like some you know

00:25:50,380 --> 00:25:55,150
one really simple kind of word count

00:25:52,390 --> 00:25:57,220
like benchmark we ordered in Python and

00:25:55,150 --> 00:25:58,900
Scala using the Arditi api and you know

00:25:57,220 --> 00:26:01,630
it's actually there's like a difference

00:25:58,900 --> 00:26:04,180
you know python is is is definitely

00:26:01,630 --> 00:26:06,370
slower and then we hold it with data

00:26:04,180 --> 00:26:08,410
frames and with data frames this was

00:26:06,370 --> 00:26:10,720
slightly faster even than the Scala code

00:26:08,410 --> 00:26:12,510
basically due to being specialized for

00:26:10,720 --> 00:26:15,610
this computation due to having fewer

00:26:12,510 --> 00:26:17,230
kind of virtual calls and also you get

00:26:15,610 --> 00:26:19,300
the same performance in in every

00:26:17,230 --> 00:26:20,770
language that maps to it because it's

00:26:19,300 --> 00:26:23,710
the same kind of query applied and

00:26:20,770 --> 00:26:25,270
execution engine underneath so I'll talk

00:26:23,710 --> 00:26:27,040
I'll talk a little more about

00:26:25,270 --> 00:26:29,940
performance later but basically this is

00:26:27,040 --> 00:26:33,510
the kind of stuff we can do with it

00:26:29,940 --> 00:26:35,130
okay so that was data frames now if

00:26:33,510 --> 00:26:37,590
you're especially you know if you're a

00:26:35,130 --> 00:26:40,230
scholar developer you might you know be

00:26:37,590 --> 00:26:42,750
kind of unsettled by this this loosely

00:26:40,230 --> 00:26:45,120
type nature of that just like get string

00:26:42,750 --> 00:26:47,010
0 and all that stuff that's that seems

00:26:45,120 --> 00:26:49,350
dangerous that might you know if it

00:26:47,010 --> 00:26:50,400
compiles it by not on so that's pretty

00:26:49,350 --> 00:26:53,310
scary

00:26:50,400 --> 00:26:56,280
but luckily you can also do this as a

00:26:53,310 --> 00:26:59,730
typed API so the data set API is is

00:26:56,280 --> 00:27:01,650
trying to make it type basically with

00:26:59,730 --> 00:27:04,530
datasets you're going to have a data set

00:27:01,650 --> 00:27:06,870
of objects of a given class and whenever

00:27:04,530 --> 00:27:08,340
you look at them in Scala were going to

00:27:06,870 --> 00:27:10,170
actually create objects of that class

00:27:08,340 --> 00:27:13,740
and everything you write will be type

00:27:10,170 --> 00:27:16,050
safe and you know but underneath a lot

00:27:13,740 --> 00:27:17,820
of the operations can still use the same

00:27:16,050 --> 00:27:21,510
data format and some of the same

00:27:17,820 --> 00:27:23,610
optimizations okay so how does that work

00:27:21,510 --> 00:27:25,950
so you know let's just have a bunch of

00:27:23,610 --> 00:27:28,650
case classes it actually in Scala it

00:27:25,950 --> 00:27:31,830
works primarily on on case classes in

00:27:28,650 --> 00:27:33,420
Java it works on Java beans and so you

00:27:31,830 --> 00:27:35,580
know you have some case classes with

00:27:33,420 --> 00:27:37,800
some fields here we've got users and

00:27:35,580 --> 00:27:39,960
messages and messages each container

00:27:37,800 --> 00:27:43,770
user as well so it can be some nested

00:27:39,960 --> 00:27:45,720
sort of data so first thing you can do

00:27:43,770 --> 00:27:48,120
is you can read a data frame this is on

00:27:45,720 --> 00:27:50,940
type so here we're reading a JSON file

00:27:48,120 --> 00:27:52,980
and actually sparks equal looks at the

00:27:50,940 --> 00:27:54,960
JSON and and for is a schema for it and

00:27:52,980 --> 00:27:57,150
hopefully you know in this case we know

00:27:54,960 --> 00:28:00,390
that schema is going to match messages

00:27:57,150 --> 00:28:03,120
and users and then you can do data frame

00:28:00,390 --> 00:28:06,540
dot as message and get back a data set

00:28:03,120 --> 00:28:09,180
of message and if you know if if the

00:28:06,540 --> 00:28:10,940
JSON has the right fields we're going to

00:28:09,180 --> 00:28:13,590
view this as a bunch of message object

00:28:10,940 --> 00:28:16,830
and now that you've got messages you can

00:28:13,590 --> 00:28:19,170
do nice type safe operations that act on

00:28:16,830 --> 00:28:21,840
the scala types so for example you can

00:28:19,170 --> 00:28:23,610
filter out you know messages whose texts

00:28:21,840 --> 00:28:25,170
contain spark and you've just got a

00:28:23,610 --> 00:28:27,180
message object and you look at its

00:28:25,170 --> 00:28:29,850
fields this gives you a new data set of

00:28:27,180 --> 00:28:31,560
message you can pull out the user from

00:28:29,850 --> 00:28:33,780
each message this gives you a data set

00:28:31,560 --> 00:28:37,440
of user and as long as you're using case

00:28:33,780 --> 00:28:39,770
classes and here basically the API kind

00:28:37,440 --> 00:28:42,760
of works and you get back new data sets

00:28:39,770 --> 00:28:44,530
so that's what that looks like

00:28:42,760 --> 00:28:48,820
and if you want you can also use the

00:28:44,530 --> 00:28:50,890
sort of less safe data frame message as

00:28:48,820 --> 00:28:53,500
well where you just pass in field names

00:28:50,890 --> 00:28:55,480
and stuff like that so for example you

00:28:53,500 --> 00:28:58,270
know let's do group by user that name

00:28:55,480 --> 00:29:01,150
pass that in and count this is actually

00:28:58,270 --> 00:29:02,290
so you know if you didn't want to do

00:29:01,150 --> 00:29:04,360
this you could also pass in a function

00:29:02,290 --> 00:29:06,400
like you could group by you know

00:29:04,360 --> 00:29:08,020
underscore that user dot name if you

00:29:06,400 --> 00:29:10,929
wanted to do that instead but you can

00:29:08,020 --> 00:29:13,090
also go back to this one and again you

00:29:10,929 --> 00:29:14,590
get back you know it keeps track of the

00:29:13,090 --> 00:29:16,750
scholar types in this case you know you

00:29:14,590 --> 00:29:19,210
don't have names for them anymore but

00:29:16,750 --> 00:29:21,490
it's just the tuple of string in it so

00:29:19,210 --> 00:29:24,070
that's basically how the API works so

00:29:21,490 --> 00:29:26,710
you get a lot of you know a lot of the

00:29:24,070 --> 00:29:28,860
properties of the previous one but you

00:29:26,710 --> 00:29:33,010
have types okay

00:29:28,860 --> 00:29:35,950
so just a few details inside it to let

00:29:33,010 --> 00:29:39,580
you plug in types and you know to make

00:29:35,950 --> 00:29:42,010
it actually work data sets each data set

00:29:39,580 --> 00:29:44,230
comes with an implicit encoder object

00:29:42,010 --> 00:29:47,049
for its type key so if you've got a data

00:29:44,230 --> 00:29:49,840
set T and an encoder of T you can build

00:29:47,049 --> 00:29:52,090
a data frame and that map's back and

00:29:49,840 --> 00:29:53,890
forth between Scala objects and sequel

00:29:52,090 --> 00:29:56,980
rules which are these generic containers

00:29:53,890 --> 00:29:59,080
that I talked about earlier so you need

00:29:56,980 --> 00:30:02,740
that kind of object we create them

00:29:59,080 --> 00:30:04,870
automatically for case classes and it's

00:30:02,740 --> 00:30:06,490
not yet a public API to add your own but

00:30:04,870 --> 00:30:08,380
it will probably be soon we're just

00:30:06,490 --> 00:30:10,780
trying to iron out the details you know

00:30:08,380 --> 00:30:13,090
to use it for a while before committing

00:30:10,780 --> 00:30:14,380
to a public API for these things because

00:30:13,090 --> 00:30:16,120
it's important for these things to be

00:30:14,380 --> 00:30:18,580
high-performance so we want to play with

00:30:16,120 --> 00:30:20,890
a few different things and basically the

00:30:18,580 --> 00:30:22,960
encoder API itself is like Oh given your

00:30:20,890 --> 00:30:25,510
object and or think for like writing

00:30:22,960 --> 00:30:28,150
data and 200 like you know go ahead and

00:30:25,510 --> 00:30:30,429
hide it that's what it looks like the

00:30:28,150 --> 00:30:32,559
data set methods take either closures or

00:30:30,429 --> 00:30:35,230
expressions in the DSL I described

00:30:32,559 --> 00:30:36,820
before and then a final thing we want to

00:30:35,230 --> 00:30:38,860
do that that will be cool to do it's

00:30:36,820 --> 00:30:41,679
it's not you know we haven't done it yet

00:30:38,860 --> 00:30:43,929
is for some of the closures we want to

00:30:41,679 --> 00:30:46,270
understand their semantics even if you

00:30:43,929 --> 00:30:47,980
pass in a closure and that's possible

00:30:46,270 --> 00:30:51,040
now in Scala using features such as

00:30:47,980 --> 00:30:54,010
macros and meta programming so for

00:30:51,040 --> 00:30:56,409
example when you pass in like this kind

00:30:54,010 --> 00:30:59,019
of function here m dot user

00:30:56,409 --> 00:31:00,549
we should be able you know we should be

00:30:59,019 --> 00:31:02,470
able to know that you're just pulling

00:31:00,549 --> 00:31:04,509
out the user field and we don't need to

00:31:02,470 --> 00:31:06,190
initialize like all the other fields in

00:31:04,509 --> 00:31:08,110
the message right if this was an

00:31:06,190 --> 00:31:10,480
arbitrary function we'd have to read all

00:31:08,110 --> 00:31:11,980
the fields from the message and populate

00:31:10,480 --> 00:31:14,169
it and then you only pull out the user

00:31:11,980 --> 00:31:16,120
and we wasted time pulling out the

00:31:14,169 --> 00:31:18,190
string and stuff but if we could analyze

00:31:16,120 --> 00:31:19,899
this function we could just create a

00:31:18,190 --> 00:31:23,049
message object where you know only the

00:31:19,899 --> 00:31:24,429
user field is set for example so that's

00:31:23,049 --> 00:31:27,340
something we want to do in the future

00:31:24,429 --> 00:31:29,259
there are some some you know some at

00:31:27,340 --> 00:31:31,539
least kind of research prototype type

00:31:29,259 --> 00:31:33,549
systems that do some of this and we want

00:31:31,539 --> 00:31:35,379
to see if we can do it sort of for

00:31:33,549 --> 00:31:38,110
healing like sort of real user programs

00:31:35,379 --> 00:31:39,730
but we're not using that yet so there's

00:31:38,110 --> 00:31:42,190
still like a small kind of performance

00:31:39,730 --> 00:31:45,190
straight up if you use the DSL we get

00:31:42,190 --> 00:31:47,590
slightly better performance okay so

00:31:45,190 --> 00:31:49,720
that's how datasets work and so long

00:31:47,590 --> 00:31:51,909
term rdd's will remain kind of the

00:31:49,720 --> 00:31:53,950
low-level API in spark if you know what

00:31:51,909 --> 00:31:56,169
you're doing you have Java objects and

00:31:53,950 --> 00:31:58,299
you control the layout and their memory

00:31:56,169 --> 00:32:00,970
usage and so on you can you know you can

00:31:58,299 --> 00:32:03,490
use it and a lot of existing libraries

00:32:00,970 --> 00:32:06,330
use it data frames and data sets are

00:32:03,490 --> 00:32:09,399
higher level API with more semantics and

00:32:06,330 --> 00:32:11,289
optimizations and our new libraries the

00:32:09,399 --> 00:32:13,779
standard libraries will increasingly use

00:32:11,289 --> 00:32:16,149
these as the interchange format one

00:32:13,779 --> 00:32:17,860
really good reason for that is also to

00:32:16,149 --> 00:32:20,740
make it easy to pass data into these

00:32:17,860 --> 00:32:22,509
libraries from Python and R and so on so

00:32:20,740 --> 00:32:24,250
we're already doing this with ml liban

00:32:22,509 --> 00:32:26,590
with this thing called graph frames

00:32:24,250 --> 00:32:29,409
which is a new graph processing API

00:32:26,590 --> 00:32:30,580
based on data frames and it's always you

00:32:29,409 --> 00:32:32,139
know if you actually work with this it's

00:32:30,580 --> 00:32:35,409
super easy to convert back and forth

00:32:32,139 --> 00:32:37,480
between the two as well okay so that's

00:32:35,409 --> 00:32:42,220
kind of that's kind of how we're trying

00:32:37,480 --> 00:32:44,769
to to tackle these problems okay so then

00:32:42,220 --> 00:32:48,070
next thing I want to talk about is some

00:32:44,769 --> 00:32:50,110
optimizations that these api is enable

00:32:48,070 --> 00:32:51,879
you know just to show what we're getting

00:32:50,110 --> 00:32:55,090
out of them you know more than just

00:32:51,879 --> 00:32:56,799
putting up one graph once and I'm going

00:32:55,090 --> 00:32:59,830
to talk about two of these one is the

00:32:56,799 --> 00:33:03,480
data sources for reading stuff from

00:32:59,830 --> 00:33:05,679
external systems and one is CPU and

00:33:03,480 --> 00:33:07,100
performance optimizations - code

00:33:05,679 --> 00:33:10,280
generation

00:33:07,100 --> 00:33:12,690
okay so data sources are common

00:33:10,280 --> 00:33:15,030
interface for both data sets and data

00:33:12,690 --> 00:33:17,910
frames and sequel actually the access

00:33:15,030 --> 00:33:18,810
external storage and the really cool

00:33:17,910 --> 00:33:20,640
thing about it

00:33:18,810 --> 00:33:22,320
I showed them a bit with JSON before but

00:33:20,640 --> 00:33:24,240
the really cool thing is if you write a

00:33:22,320 --> 00:33:26,790
spark application and you know you have

00:33:24,240 --> 00:33:28,920
your own pipes inside it you can migrate

00:33:26,790 --> 00:33:32,160
it across very different storage systems

00:33:28,920 --> 00:33:34,110
like hive or Cassandra or JSON files or

00:33:32,160 --> 00:33:36,420
Avvo you know you can start with a JSON

00:33:34,110 --> 00:33:38,340
file on your laptop build your thing and

00:33:36,420 --> 00:33:40,920
then run it in Cassandra and then if you

00:33:38,340 --> 00:33:43,110
have the same field names and schema the

00:33:40,920 --> 00:33:44,430
same program will work so it's only that

00:33:43,110 --> 00:33:47,240
line at the beginning of the program

00:33:44,430 --> 00:33:49,920
that loads the data that has to change

00:33:47,240 --> 00:33:52,260
the other thing that's cool about these

00:33:49,920 --> 00:33:54,300
is because we have the structured

00:33:52,260 --> 00:33:56,790
semantics after we know which fields

00:33:54,300 --> 00:33:59,130
you're accessing and you know which

00:33:56,790 --> 00:34:01,890
parts of the data you use we can also

00:33:59,130 --> 00:34:03,990
push down part of the Quai into data

00:34:01,890 --> 00:34:05,760
sources which is not really possible in

00:34:03,990 --> 00:34:08,460
the original spark so in the original

00:34:05,760 --> 00:34:10,260
spark if you use the Cassandra API to

00:34:08,460 --> 00:34:12,570
eat you know a table from Cassandra

00:34:10,260 --> 00:34:14,129
you're gonna get all the fields in it or

00:34:12,570 --> 00:34:16,260
you have to jump through some hoops to

00:34:14,129 --> 00:34:18,990
tell it to only read some fields but

00:34:16,260 --> 00:34:21,480
here if you use data frames or data sets

00:34:18,990 --> 00:34:23,370
and we see you know your class only has

00:34:21,480 --> 00:34:24,929
two of the fields in it we're only going

00:34:23,370 --> 00:34:28,410
to read those two fields and we don't

00:34:24,929 --> 00:34:32,280
need to do a lot of extra wasted work

00:34:28,410 --> 00:34:34,170
so internally data sources can implement

00:34:32,280 --> 00:34:36,150
these different interfaces based on how

00:34:34,170 --> 00:34:38,970
smart they are how much computation they

00:34:36,150 --> 00:34:41,070
can do so some data sources just do a

00:34:38,970 --> 00:34:42,870
table scan all they can do is weed the

00:34:41,070 --> 00:34:45,300
whole data you can just like jump in the

00:34:42,870 --> 00:34:48,750
middle of atom pull stuff out so CSV and

00:34:45,300 --> 00:34:50,820
JSON are examples some of them support a

00:34:48,750 --> 00:34:53,640
poun scan which is just to read specific

00:34:50,820 --> 00:34:55,830
columns these are columnar things like

00:34:53,640 --> 00:34:58,500
Cassandra and HBase and some of them

00:34:55,830 --> 00:35:00,870
also support pushing filters into the

00:34:58,500 --> 00:35:02,520
data source itself and doing those you

00:35:00,870 --> 00:35:04,590
know maybe somewhere else not having to

00:35:02,520 --> 00:35:06,600
send back everything over the network so

00:35:04,590 --> 00:35:08,940
things like JDBC when you connect to a

00:35:06,600 --> 00:35:11,160
database and parquet and hive which

00:35:08,940 --> 00:35:15,150
support indexing and and pulling out

00:35:11,160 --> 00:35:16,890
just some parts of the data so just as a

00:35:15,150 --> 00:35:18,990
simple example I'm going to show this in

00:35:16,890 --> 00:35:21,600
sequel you know you could have a data

00:35:18,990 --> 00:35:23,850
source for JSON and you could do stuff

00:35:21,600 --> 00:35:26,700
like this like select user that ID and

00:35:23,850 --> 00:35:28,560
text and it's gonna read the whole JSON

00:35:26,700 --> 00:35:31,320
file there's no nice way to parse JSON

00:35:28,560 --> 00:35:34,050
kind of incrementally you could have

00:35:31,320 --> 00:35:36,480
something like JDBC where you select you

00:35:34,050 --> 00:35:39,270
know here you have these users and you

00:35:36,480 --> 00:35:42,240
wanna select age for users where

00:35:39,270 --> 00:35:45,450
language equals English and you can also

00:35:42,240 --> 00:35:47,040
have some combinations of them and in in

00:35:45,450 --> 00:35:49,650
the same system combine these two

00:35:47,040 --> 00:35:52,710
different data sources and when you do

00:35:49,650 --> 00:35:56,640
have these my sequel things because my

00:35:52,710 --> 00:35:58,140
sequel supports filter pushdown spark is

00:35:56,640 --> 00:36:00,480
gonna actually is gonna eat the whole

00:35:58,140 --> 00:36:02,880
JSON file but then it's going to push

00:36:00,480 --> 00:36:05,280
part of the claim into my sequel so it

00:36:02,880 --> 00:36:08,100
will tell my sequel here I only want the

00:36:05,280 --> 00:36:10,950
idea and age for things where language

00:36:08,100 --> 00:36:12,480
equals English and and pull those out

00:36:10,950 --> 00:36:14,760
and so you don't have to read back all

00:36:12,480 --> 00:36:17,280
the data so this is a pretty powerful

00:36:14,760 --> 00:36:19,020
feature because big data is you know

00:36:17,280 --> 00:36:21,930
it's expensive to move around and and

00:36:19,020 --> 00:36:23,850
this lets you save a lot of the work of

00:36:21,930 --> 00:36:26,070
actually loading you know the data you

00:36:23,850 --> 00:36:29,010
don't care about so this is kind of the

00:36:26,070 --> 00:36:31,040
data source API and it benefits from

00:36:29,010 --> 00:36:36,000
from the structure stuff shown before

00:36:31,040 --> 00:36:38,780
okay and then the second thing that that

00:36:36,000 --> 00:36:41,849
we're trying to do under this API is is

00:36:38,780 --> 00:36:43,680
CPU and memory optimization

00:36:41,849 --> 00:36:46,829
and that has a lot to do with basically

00:36:43,680 --> 00:36:50,099
hardware trends so when spark kind of

00:36:46,829 --> 00:36:53,069
began in in 2010 a lot of big data

00:36:50,099 --> 00:36:56,430
systems were bottlenecked on Io so back

00:36:53,069 --> 00:36:58,829
then storage was mostly on hard disks

00:36:56,430 --> 00:37:01,019
and you know one disk has 50 megabytes

00:36:58,829 --> 00:37:02,880
per second bandwidth you might you you

00:37:01,019 --> 00:37:05,339
would probably have like a dozen disks

00:37:02,880 --> 00:37:07,949
in a server but still it's it's it's

00:37:05,339 --> 00:37:09,449
fairly limited and networks and data

00:37:07,949 --> 00:37:12,299
centers were pretty small maybe you've

00:37:09,449 --> 00:37:15,269
got one gigabit per second bandwidth and

00:37:12,299 --> 00:37:17,160
CPUs were around three gigahertz if you

00:37:15,269 --> 00:37:18,509
look now at the common configuration

00:37:17,160 --> 00:37:22,380
like if you just launched a bunch of

00:37:18,509 --> 00:37:25,289
nodes on ec2 a lot of data especially

00:37:22,380 --> 00:37:28,229
the recent kind of hard data is placed

00:37:25,289 --> 00:37:30,390
on SSDs and this can be a lot faster

00:37:28,229 --> 00:37:33,900
than even one SSD can be around ten

00:37:30,390 --> 00:37:35,849
times faster than a disk networks are

00:37:33,900 --> 00:37:37,469
pretty much everywhere and data centers

00:37:35,849 --> 00:37:39,839
they've jumped to ten gigabits and

00:37:37,469 --> 00:37:42,359
there's also 40 gigabit you know that

00:37:39,839 --> 00:37:44,190
will become pretty standard soon but

00:37:42,359 --> 00:37:45,719
CPUs unfortunately are still around

00:37:44,190 --> 00:37:48,359
three gigahertz and sure there are a

00:37:45,719 --> 00:37:50,609
couple more cores but not enough to make

00:37:48,359 --> 00:37:52,979
up for the difference compared to five

00:37:50,609 --> 00:37:55,949
years ago so basically these things

00:37:52,979 --> 00:37:58,829
improved by a factor of ten the CPU

00:37:55,949 --> 00:38:01,170
didn't so all of a sudden you know back

00:37:58,829 --> 00:38:04,349
in 2010 the name of the game was just a

00:38:01,170 --> 00:38:07,319
void IO definitely avoid Network IO

00:38:04,349 --> 00:38:09,479
whenever possible you know maximize data

00:38:07,319 --> 00:38:11,729
locality group data together to make it

00:38:09,479 --> 00:38:14,099
fast and try to avoid this scale which

00:38:11,729 --> 00:38:17,099
we did to caching hot data and memory

00:38:14,099 --> 00:38:19,009
but now even if you're going you know

00:38:17,099 --> 00:38:22,650
with with just basic data right off your

00:38:19,009 --> 00:38:24,989
SSD you need to be really efficient and

00:38:22,650 --> 00:38:28,349
in the cpu as well in order to get the

00:38:24,989 --> 00:38:31,829
maximum performance so we have a set of

00:38:28,349 --> 00:38:33,539
different optimizations that try to do

00:38:31,829 --> 00:38:37,559
this which together are called project

00:38:33,539 --> 00:38:40,319
tungsten and basically they do see main

00:38:37,559 --> 00:38:41,930
things runtime code generation the

00:38:40,319 --> 00:38:44,849
specialized code I talked about

00:38:41,930 --> 00:38:46,799
exploiting cache locality and of heap

00:38:44,849 --> 00:38:49,519
memory management basically managing

00:38:46,799 --> 00:38:52,450
binary data instead of using Java

00:38:49,519 --> 00:38:57,970
objects and sort of their overhead

00:38:52,450 --> 00:39:01,450
so tungsten basically encodes encodes

00:38:57,970 --> 00:39:02,860
rows of data you know supporting in you

00:39:01,450 --> 00:39:05,770
know in the kind of schemas that

00:39:02,860 --> 00:39:07,750
that spark can express as binary data in

00:39:05,770 --> 00:39:09,760
a much more compact way than Java

00:39:07,750 --> 00:39:11,710
objects obviously it's less generic than

00:39:09,760 --> 00:39:13,390
Java objects because they don't have

00:39:11,710 --> 00:39:15,010
lots of pointers to each other and stuff

00:39:13,390 --> 00:39:17,440
like that but it's pretty good if you

00:39:15,010 --> 00:39:20,560
have structured data so if you have you

00:39:17,440 --> 00:39:23,560
know a ho of elements like a number and

00:39:20,560 --> 00:39:25,420
two strings it's gonna encode them into

00:39:23,560 --> 00:39:27,520
a you know pretty pretty small object

00:39:25,420 --> 00:39:30,220
not tiny but but much easier to work

00:39:27,520 --> 00:39:32,890
with which has like a bit set for which

00:39:30,220 --> 00:39:36,570
field is null and then integers are just

00:39:32,890 --> 00:39:39,940
you know in binary form and then strings

00:39:36,570 --> 00:39:42,820
are basically in this kind of binary

00:39:39,940 --> 00:39:43,330
data section at the end and they're back

00:39:42,820 --> 00:39:45,100
together

00:39:43,330 --> 00:39:49,210
they're all kind of back next to each

00:39:45,100 --> 00:39:51,850
other with no pointers and in a lot of a

00:39:49,210 --> 00:39:54,580
lot of workloads this basically leads to

00:39:51,850 --> 00:39:56,920
a lot of save save space which is

00:39:54,580 --> 00:39:58,870
important both for like cache data that

00:39:56,920 --> 00:40:00,550
lives long term anyone for short-term

00:39:58,870 --> 00:40:03,100
data you don't want to be allocating

00:40:00,550 --> 00:40:05,830
lots of memory to you know to group data

00:40:03,100 --> 00:40:08,020
or stuff like that so this is like one

00:40:05,830 --> 00:40:11,040
you know one really simple kind of

00:40:08,020 --> 00:40:13,720
benchmark that you know that shows that

00:40:11,040 --> 00:40:16,510
okay so this is this is the first part

00:40:13,720 --> 00:40:18,910
is the binary format the second part is

00:40:16,510 --> 00:40:21,160
one time code generation so we start

00:40:18,910 --> 00:40:25,420
with an expression like in the in the

00:40:21,160 --> 00:40:27,580
DSL or in sequel and then we in

00:40:25,420 --> 00:40:29,170
internally we view this as an AST

00:40:27,580 --> 00:40:31,390
there's this thing called catalyst which

00:40:29,170 --> 00:40:34,840
is spark sequence query optimizer that

00:40:31,390 --> 00:40:38,260
can capture these things and then

00:40:34,840 --> 00:40:39,850
finally we generate bytecode or you know

00:40:38,260 --> 00:40:43,170
actually we generate Java code and then

00:40:39,850 --> 00:40:47,680
compile it that that works with that and

00:40:43,170 --> 00:40:49,620
one of the things with this is because

00:40:47,680 --> 00:40:53,890
we're storing the object in this binary

00:40:49,620 --> 00:40:56,140
format we actually have the code work

00:40:53,890 --> 00:40:58,540
directly on binary data with pointers so

00:40:56,140 --> 00:41:02,800
we use this platform that get int which

00:40:58,540 --> 00:41:05,200
is a JVM like intrinsic to to work with

00:41:02,800 --> 00:41:08,380
pointers it's part of sun dot miss Quran

00:41:05,200 --> 00:41:11,020
safe so this is kind of you know

00:41:08,380 --> 00:41:13,150
basically this allows the code to work

00:41:11,020 --> 00:41:15,310
with data whether it's in the Java heap

00:41:13,150 --> 00:41:17,350
or outside of it so we can just allocate

00:41:15,310 --> 00:41:19,750
a bunch of memory outside the heap and

00:41:17,350 --> 00:41:21,640
work with it and get similar performance

00:41:19,750 --> 00:41:23,830
to what you did if you did you know you

00:41:21,640 --> 00:41:25,450
know like malloc and C and then you you

00:41:23,830 --> 00:41:26,740
had a bunch of structs in there and try

00:41:25,450 --> 00:41:29,680
to eat fields from them

00:41:26,740 --> 00:41:33,040
now it may seem like a little scary to

00:41:29,680 --> 00:41:34,660
be doing this stuff in the JVM but one

00:41:33,040 --> 00:41:36,910
of the nice things with with SPARC is

00:41:34,660 --> 00:41:39,820
that data you know has a very

00:41:36,910 --> 00:41:41,530
well-defined lifecycle it's pretty clear

00:41:39,820 --> 00:41:43,510
like when you put something in the cache

00:41:41,530 --> 00:41:45,610
when you remove it and you can't really

00:41:43,510 --> 00:41:48,280
have pointers to it outside each time we

00:41:45,610 --> 00:41:50,290
pull out on object we we copy the fields

00:41:48,280 --> 00:41:52,240
into a Java object so it's pretty easy

00:41:50,290 --> 00:41:54,850
to manage if this was something with

00:41:52,240 --> 00:41:56,380
lots of data structures with pointers

00:41:54,850 --> 00:41:58,330
and chains and stuff like that it would

00:41:56,380 --> 00:42:00,190
be a lot harder but here you know it's

00:41:58,330 --> 00:42:02,680
it's not too hard to manage a bunch of

00:42:00,190 --> 00:42:05,020
memory outside the heap to do this and

00:42:02,680 --> 00:42:06,580
you got really good performance you got

00:42:05,020 --> 00:42:08,530
similar performance to these things that

00:42:06,580 --> 00:42:11,350
were it and you know from the ground up

00:42:08,530 --> 00:42:14,680
to do that so that's that basically how

00:42:11,350 --> 00:42:16,660
this part works so the long term vision

00:42:14,680 --> 00:42:18,760
for this is basically we have a bunch of

00:42:16,660 --> 00:42:21,250
language phonons they all create a

00:42:18,760 --> 00:42:24,340
logical plan this part is mostly in

00:42:21,250 --> 00:42:26,770
there today it works for all of them and

00:42:24,340 --> 00:42:29,050
then we can map it to different backends

00:42:26,770 --> 00:42:31,060
and right now we're still executing the

00:42:29,050 --> 00:42:33,400
code in the JVM but we're also

00:42:31,060 --> 00:42:35,020
interested in doing stuff like LLVM so

00:42:33,400 --> 00:42:39,070
basically generating assembly code

00:42:35,020 --> 00:42:41,530
instead or vector instructions or even

00:42:39,070 --> 00:42:44,170
for using other types of memory like 3d

00:42:41,530 --> 00:42:46,120
crosspoint or memory mapping and SSD

00:42:44,170 --> 00:42:48,010
directly or stuff like that so that we

00:42:46,120 --> 00:42:50,440
can work with new types of hardware and

00:42:48,010 --> 00:42:52,530
it's pretty important because basically

00:42:50,440 --> 00:42:55,090
the future of hardware is gonna be these

00:42:52,530 --> 00:42:57,250
you know these new storage devices and

00:42:55,090 --> 00:42:59,260
these new kind of computing units that

00:42:57,250 --> 00:43:00,970
the JVM doesn't run really well on it

00:42:59,260 --> 00:43:03,580
and we don't we don't want to lose like

00:43:00,970 --> 00:43:06,550
the really nice things of working in

00:43:03,580 --> 00:43:08,800
Scala and Java in the user API so we

00:43:06,550 --> 00:43:11,619
need to figure out a way to map it onto

00:43:08,800 --> 00:43:14,810
these pieces of hardware

00:43:11,619 --> 00:43:17,359
so that's that bye okay and then the

00:43:14,810 --> 00:43:18,980
final part I'll talk about very briefly

00:43:17,359 --> 00:43:21,950
is kind of some of the next steps

00:43:18,980 --> 00:43:24,260
happening with this in spark 2.0 so a

00:43:21,950 --> 00:43:25,609
spark to Rado is the is the next major

00:43:24,260 --> 00:43:28,160
release of spark it will come out

00:43:25,609 --> 00:43:31,339
probably around the end of May so it's

00:43:28,160 --> 00:43:34,790
coming up pretty soon and it's got a lot

00:43:31,339 --> 00:43:37,490
of a lot of exciting features in it just

00:43:34,790 --> 00:43:40,040
to give you a sense of what it means the

00:43:37,490 --> 00:43:42,470
sort of spark versioning scheme so in

00:43:40,040 --> 00:43:45,380
spark we just use semantic versioning so

00:43:42,470 --> 00:43:48,380
major version which pseudo is means that

00:43:45,380 --> 00:43:51,050
it may change API is minor version which

00:43:48,380 --> 00:43:53,119
most of our releases have been so far it

00:43:51,050 --> 00:43:55,820
can add api's and features but it can't

00:43:53,119 --> 00:43:57,740
break backwards compatibility actually

00:43:55,820 --> 00:44:00,430
we try to enforce binary compatibility

00:43:57,740 --> 00:44:03,530
so you don't even have to recompile your

00:44:00,430 --> 00:44:05,180
your application and patch versions can

00:44:03,530 --> 00:44:07,940
only have bug fixes and they definitely

00:44:05,180 --> 00:44:11,450
can't break binary compatibility either

00:44:07,940 --> 00:44:14,720
so 2.0 is sort of a chance to clean up

00:44:11,450 --> 00:44:17,420
and think about some of the major API X

00:44:14,720 --> 00:44:20,630
now in reality we we really hate

00:44:17,420 --> 00:44:22,520
breaking API so anyone you know who's

00:44:20,630 --> 00:44:25,130
who's worked on the project early on

00:44:22,520 --> 00:44:27,170
knows that people such as myself we're

00:44:25,130 --> 00:44:29,300
extremely stubborn about not breaking

00:44:27,170 --> 00:44:32,119
API is because the worst thing as a

00:44:29,300 --> 00:44:34,220
developer is you update some-some

00:44:32,119 --> 00:44:36,349
library that is not your main type of

00:44:34,220 --> 00:44:38,119
work and all of a sudden you have to you

00:44:36,349 --> 00:44:40,220
know work for like a few hours to just

00:44:38,119 --> 00:44:41,990
fix everything and if you don't update

00:44:40,220 --> 00:44:43,790
it you don't get bug fixes if you do

00:44:41,990 --> 00:44:45,950
update it you get all this junk that you

00:44:43,790 --> 00:44:49,040
didn't want to work with so we we really

00:44:45,950 --> 00:44:51,619
try not to break these api's and inspark

00:44:49,040 --> 00:44:53,660
- although we haven't we haven't done a

00:44:51,619 --> 00:44:55,339
lot of API breaking the the most thing

00:44:53,660 --> 00:44:57,410
we've done is we've tried to remove some

00:44:55,339 --> 00:44:59,900
dependencies that were causing conflicts

00:44:57,410 --> 00:45:02,839
so like one kind of interesting story is

00:44:59,900 --> 00:45:04,940
we use guava to implement the option

00:45:02,839 --> 00:45:07,730
type in Java because we have some API

00:45:04,940 --> 00:45:10,250
involving options and option seems like

00:45:07,730 --> 00:45:13,730
a really simple class but somehow guava

00:45:10,250 --> 00:45:15,830
book api compatibility for option in one

00:45:13,730 --> 00:45:18,560
of its minor releases so that's that's

00:45:15,830 --> 00:45:20,510
definitely unfortunate because we were

00:45:18,560 --> 00:45:23,060
stuck with our public api with some

00:45:20,510 --> 00:45:25,270
guava 1.5 or whatever old version of it

00:45:23,060 --> 00:45:27,810
is so we've done a few things like

00:45:25,270 --> 00:45:30,820
but mostly things will work as before

00:45:27,810 --> 00:45:33,280
some may require recompiling because we

00:45:30,820 --> 00:45:35,050
did change some of the type stuff in

00:45:33,280 --> 00:45:38,250
Scala some of the type magic to make

00:45:35,050 --> 00:45:41,500
them to make a few things more uniform

00:45:38,250 --> 00:45:43,840
okay and there are a whole bunch of big

00:45:41,500 --> 00:45:45,640
features into dotto there's a lot more

00:45:43,840 --> 00:45:47,500
of tungsten actually a lot of the stuff

00:45:45,640 --> 00:45:50,410
I described like the code generation is

00:45:47,500 --> 00:45:52,450
improved quite a bit there's a machine

00:45:50,410 --> 00:45:54,460
learning model export and then the thing

00:45:52,450 --> 00:45:57,010
I want to talk about is a new structured

00:45:54,460 --> 00:45:59,410
API called structured streaming which

00:45:57,010 --> 00:46:01,360
gives us the same kind of optimization I

00:45:59,410 --> 00:46:04,750
showed with data frames and data sets

00:46:01,360 --> 00:46:06,640
but on streaming computation and I just

00:46:04,750 --> 00:46:09,640
have a couple of slides on it I don't

00:46:06,640 --> 00:46:12,520
I'm not gonna spend a super long time so

00:46:09,640 --> 00:46:14,590
structured streaming is actually you

00:46:12,520 --> 00:46:16,720
know an attempt to do in fact a bit more

00:46:14,590 --> 00:46:18,400
than streaming it comes from seeing what

00:46:16,720 --> 00:46:20,560
people did with spark streaming and with

00:46:18,400 --> 00:46:22,720
sort of other streaming engines so

00:46:20,560 --> 00:46:24,100
real-time processing is obviously you

00:46:22,720 --> 00:46:26,650
know pretty important for a lot of

00:46:24,100 --> 00:46:29,050
applications but what we found when

00:46:26,650 --> 00:46:31,330
people deploy spark streaming or other

00:46:29,050 --> 00:46:33,520
systems is no one is just doing

00:46:31,330 --> 00:46:35,380
streaming no one just says yeah I have a

00:46:33,520 --> 00:46:37,450
stream of objects and I just want to add

00:46:35,380 --> 00:46:40,900
a map over it and yeah I'm gonna get

00:46:37,450 --> 00:46:43,180
another stream back in fact they usually

00:46:40,900 --> 00:46:45,610
combine it with batch or interactive

00:46:43,180 --> 00:46:48,250
ways so for example you might have a

00:46:45,610 --> 00:46:50,080
stream that tracks state and updates it

00:46:48,250 --> 00:46:52,450
in a database maybe it puts it in a my

00:46:50,080 --> 00:46:55,180
sequel database or in some weird you

00:46:52,450 --> 00:46:57,760
know in in Redis or some some other some

00:46:55,180 --> 00:46:59,260
some fast in memory database and the

00:46:57,760 --> 00:47:01,090
whole point of doing the stream is

00:46:59,260 --> 00:47:03,040
because you have a web application and

00:47:01,090 --> 00:47:05,050
fun that's going to run sequel queries

00:47:03,040 --> 00:47:06,610
on it so that's a really common use case

00:47:05,050 --> 00:47:09,070
but that's not streaming it's not

00:47:06,610 --> 00:47:11,650
supported well by by many streaming

00:47:09,070 --> 00:47:13,960
engines or another example is you train

00:47:11,650 --> 00:47:15,820
a machine learning model offline which

00:47:13,960 --> 00:47:18,790
is a bad job and then you update it or

00:47:15,820 --> 00:47:20,440
you apply it to it stream so spark is

00:47:18,790 --> 00:47:22,030
very well suited to do this because the

00:47:20,440 --> 00:47:23,800
engine can actually support all these

00:47:22,030 --> 00:47:26,560
workloads and we just want to make sure

00:47:23,800 --> 00:47:30,610
the API is let you easily mix them as

00:47:26,560 --> 00:47:32,980
well so a structured streaming is a high

00:47:30,610 --> 00:47:36,130
level streaming API built on data frames

00:47:32,980 --> 00:47:37,990
and data sets it's got a bunch of you

00:47:36,130 --> 00:47:38,980
know features that make streaming easy

00:47:37,990 --> 00:47:41,109
to work with like

00:47:38,980 --> 00:47:43,660
Vence time supports or your data can

00:47:41,109 --> 00:47:46,480
come in in any order and we deal with

00:47:43,660 --> 00:47:49,480
life data and update results windowing

00:47:46,480 --> 00:47:51,400
session ization and data sources and

00:47:49,480 --> 00:47:53,650
syncs that are pretty powerful similar

00:47:51,400 --> 00:47:56,050
to the ones I showed before and it's

00:47:53,650 --> 00:47:57,490
optimized using tungsten but it's also

00:47:56,050 --> 00:47:59,830
designed to support batch and

00:47:57,490 --> 00:48:02,170
interactive ways so for example you can

00:47:59,830 --> 00:48:04,600
aggregate data using a stream and then

00:48:02,170 --> 00:48:06,850
serve it out of memory using JDBC and

00:48:04,600 --> 00:48:09,190
plug in your web application to spark

00:48:06,850 --> 00:48:11,770
and just have a single consistent thing

00:48:09,190 --> 00:48:12,820
know managing two weird systems that you

00:48:11,770 --> 00:48:14,109
know don't know how to talk to each

00:48:12,820 --> 00:48:16,119
other very well

00:48:14,109 --> 00:48:18,070
you can change the quays at runtime if

00:48:16,119 --> 00:48:20,140
you have a new way you can build and

00:48:18,070 --> 00:48:22,630
apply ml models and we're working to

00:48:20,140 --> 00:48:25,180
make all of Sparks libraries tie in with

00:48:22,630 --> 00:48:26,830
this to let you combine them so we're

00:48:25,180 --> 00:48:28,540
calling this you know we wanted to give

00:48:26,830 --> 00:48:29,770
it a name because it's not just

00:48:28,540 --> 00:48:31,510
streaming we're calling these

00:48:29,770 --> 00:48:33,790
applications that combine the mode

00:48:31,510 --> 00:48:36,640
continuous application so it's an entire

00:48:33,790 --> 00:48:40,119
end-to-end application not just a map on

00:48:36,640 --> 00:48:41,830
it on a stream so just as an example of

00:48:40,119 --> 00:48:43,660
like things you can do with this you

00:48:41,830 --> 00:48:46,150
know very common use case for streaming

00:48:43,660 --> 00:48:48,040
today is to do ETL or extract transform

00:48:46,150 --> 00:48:50,530
and load so maybe you start with a

00:48:48,040 --> 00:48:52,540
message bus like Kafka and then you do

00:48:50,530 --> 00:48:55,000
ETL and you put stuff in a database or

00:48:52,540 --> 00:48:57,369
another common thing is reporting you

00:48:55,000 --> 00:48:59,890
know you compute some aggregates and

00:48:57,369 --> 00:49:01,330
then you push it to some applications so

00:48:59,890 --> 00:49:03,369
we're going to support these two things

00:49:01,330 --> 00:49:06,910
these are kind of things that are you

00:49:03,369 --> 00:49:08,590
know pretty doable today another thing

00:49:06,910 --> 00:49:10,630
you may want to do though is that hard

00:49:08,590 --> 00:49:12,340
ways and in structured streaming you can

00:49:10,630 --> 00:49:14,950
just connect to the system using

00:49:12,340 --> 00:49:16,930
something like JDBC and just run queries

00:49:14,950 --> 00:49:18,550
on the state and the stream so this is

00:49:16,930 --> 00:49:20,920
another type of processing you know

00:49:18,550 --> 00:49:22,690
previously you would have had to find

00:49:20,920 --> 00:49:24,820
some kind of database and figure out how

00:49:22,690 --> 00:49:27,520
to consistently update it and and do

00:49:24,820 --> 00:49:29,590
these quays and another similar thing is

00:49:27,520 --> 00:49:31,450
a machine learning model so this is kind

00:49:29,590 --> 00:49:33,340
of a closed loop where like as you learn

00:49:31,450 --> 00:49:35,650
stuff you feed it back into the stream

00:49:33,340 --> 00:49:38,200
and you need to keep iterating from the

00:49:35,650 --> 00:49:40,270
previous one so basically the goal is to

00:49:38,200 --> 00:49:43,380
support these end-to-end application so

00:49:40,270 --> 00:49:46,450
you write these things in a single API

00:49:43,380 --> 00:49:48,250
so I'll give a really brief introduction

00:49:46,450 --> 00:49:53,350
to the streaming structured streaming

00:49:48,250 --> 00:49:57,970
API we thought a bunch about how to how

00:49:53,350 --> 00:50:01,540
to create this API and basically we

00:49:57,970 --> 00:50:04,150
decided that actually you already know

00:50:01,540 --> 00:50:06,760
it you don't need to create a new API so

00:50:04,150 --> 00:50:10,090
the actually the model in the system has

00:50:06,760 --> 00:50:11,890
been that the sort of the simplest way

00:50:10,090 --> 00:50:13,900
to think about streaming analytics is

00:50:11,890 --> 00:50:15,940
not having to think about the streaming

00:50:13,900 --> 00:50:18,100
aspect itself how to actually do stuff

00:50:15,940 --> 00:50:20,670
incrementally just tell me what you want

00:50:18,100 --> 00:50:23,890
to get at the end and will compute an

00:50:20,670 --> 00:50:26,020
incremental plan to do that computation

00:50:23,890 --> 00:50:27,880
so basically all you have to do is

00:50:26,020 --> 00:50:29,290
describe the final is out you want you

00:50:27,880 --> 00:50:31,420
don't have to say like oh yeah when you

00:50:29,290 --> 00:50:33,160
get a record update this state over

00:50:31,420 --> 00:50:36,400
there and send a message to that guy and

00:50:33,160 --> 00:50:38,890
do something else so and the API is

00:50:36,400 --> 00:50:41,770
based on data frames basically in spark

00:50:38,890 --> 00:50:44,320
1x we had the static fixed size data

00:50:41,770 --> 00:50:46,810
frames that were a table into todo we

00:50:44,320 --> 00:50:49,270
just represent streams as infinite data

00:50:46,810 --> 00:50:51,550
frames essentially so host keep getting

00:50:49,270 --> 00:50:54,670
added at the end and you have the same

00:50:51,550 --> 00:50:58,270
data frame API to specify what you want

00:50:54,670 --> 00:51:00,490
to get out of it and basically you know

00:50:58,270 --> 00:51:02,290
you you you you tell us what you want at

00:51:00,490 --> 00:51:04,690
the end and we do it incrementally as

00:51:02,290 --> 00:51:07,450
the data set goes because new records

00:51:04,690 --> 00:51:10,840
have been added into it ok so just

00:51:07,450 --> 00:51:13,600
really short example on it here's like a

00:51:10,840 --> 00:51:15,190
simple batch job using data frames

00:51:13,600 --> 00:51:17,590
there's no streaming here this is how

00:51:15,190 --> 00:51:20,410
you do a batch job so here we were going

00:51:17,590 --> 00:51:22,720
to eat you know a JSON file and do a

00:51:20,410 --> 00:51:25,090
group by and you know figure out the

00:51:22,720 --> 00:51:27,430
average lace latency by user ID an hour

00:51:25,090 --> 00:51:29,290
and then we're going to write it to by

00:51:27,430 --> 00:51:32,290
sequel this is the data source you know

00:51:29,290 --> 00:51:35,290
writing API so that's a batch job

00:51:32,290 --> 00:51:37,330
and we can run this once on a file if

00:51:35,290 --> 00:51:40,119
you want to do streaming instead we just

00:51:37,330 --> 00:51:42,190
change a couple of words in here so we

00:51:40,119 --> 00:51:44,080
say oh instead of a fixed file let's

00:51:42,190 --> 00:51:46,510
stream stuff on my directory so every

00:51:44,080 --> 00:51:48,340
time a file is uploaded into that s3

00:51:46,510 --> 00:51:51,190
bucket we're going to update our result

00:51:48,340 --> 00:51:54,070
so we just changed that for each stream

00:51:51,190 --> 00:51:56,290
and here we're going to say ok let's

00:51:54,070 --> 00:51:57,310
start a stream to update my sequel with

00:51:56,290 --> 00:51:59,440
these results

00:51:57,310 --> 00:52:01,720
and as a results get ahead as a file

00:51:59,440 --> 00:52:04,180
gets read from SD were going to hunt a

00:52:01,720 --> 00:52:06,640
transaction on my sequel to update the

00:52:04,180 --> 00:52:08,440
hoes that change so even if we get data

00:52:06,640 --> 00:52:10,900
that's late or things like that like

00:52:08,440 --> 00:52:12,700
some record for a user ID comes in three

00:52:10,900 --> 00:52:15,190
hours ago we can actually update the

00:52:12,700 --> 00:52:17,380
table and the result is the same as if

00:52:15,190 --> 00:52:19,150
you had just done the batch job on all

00:52:17,380 --> 00:52:21,580
the latest data so far so if you

00:52:19,150 --> 00:52:25,090
understand what the batch job does you

00:52:21,580 --> 00:52:27,400
understand what the result of this is so

00:52:25,090 --> 00:52:28,960
the way it executes is logically you

00:52:27,400 --> 00:52:31,540
give us a data frame and you do

00:52:28,960 --> 00:52:33,700
operations as if you had all the data so

00:52:31,540 --> 00:52:35,440
far like it is if you had a giant table

00:52:33,700 --> 00:52:37,810
with all the data and you tell us what

00:52:35,440 --> 00:52:39,520
you want to compute we look at the plan

00:52:37,810 --> 00:52:41,800
we say okay this is a thing we know how

00:52:39,520 --> 00:52:43,660
to do incremental E and then we actually

00:52:41,800 --> 00:52:46,420
run it incrementally in a streaming

00:52:43,660 --> 00:52:48,130
fashion so we track state and we we

00:52:46,420 --> 00:52:51,880
actually execute it incrementally and

00:52:48,130 --> 00:52:53,530
save the result and basically to do this

00:52:51,880 --> 00:52:56,110
transformation we just we basically

00:52:53,530 --> 00:52:59,020
transform the batch query plan that we

00:52:56,110 --> 00:53:00,790
had into a continuous one and we use a

00:52:59,020 --> 00:53:02,710
lot of information about the structure

00:53:00,790 --> 00:53:04,540
of the data and of the operators to know

00:53:02,710 --> 00:53:06,730
that or you're doing a group by this

00:53:04,540 --> 00:53:09,010
field you know that means we need to

00:53:06,730 --> 00:53:11,320
maintain such-and-such stage or you're

00:53:09,010 --> 00:53:13,750
only reading these columns and so on so

00:53:11,320 --> 00:53:15,670
for example in in the one I showed the

00:53:13,750 --> 00:53:18,280
continuous plan will be you know we're

00:53:15,670 --> 00:53:20,200
going to scan for new files in SD

00:53:18,280 --> 00:53:21,850
when files come in we're going to do a

00:53:20,200 --> 00:53:23,920
stage pull advocate so we're going to

00:53:21,850 --> 00:53:25,900
have some state on the side with the

00:53:23,920 --> 00:53:28,180
count so far and we're going to update

00:53:25,900 --> 00:53:30,010
them based on the new user IDs and

00:53:28,180 --> 00:53:31,630
records that came in and then we're

00:53:30,010 --> 00:53:34,600
going to update my sequel each time a

00:53:31,630 --> 00:53:37,120
new data comes in obviously we can't do

00:53:34,600 --> 00:53:38,980
this for every computation some of them

00:53:37,120 --> 00:53:40,540
can't be done incrementally but then

00:53:38,980 --> 00:53:42,910
we'll tell you about that so for the

00:53:40,540 --> 00:53:44,650
ones that can be done incrementally will

00:53:42,910 --> 00:53:47,140
do it will automatically incremental

00:53:44,650 --> 00:53:48,820
eyes them for you okay

00:53:47,140 --> 00:53:51,940
so that's really a brief overview of

00:53:48,820 --> 00:53:54,400
structured streaming spark 2.0 we'll

00:53:51,940 --> 00:53:57,280
have the first version with a basic API

00:53:54,400 --> 00:53:59,470
focused on ETL and in spark 2.1 we're

00:53:57,280 --> 00:54:01,270
going to add a bunch of other sources

00:53:59,470 --> 00:54:03,370
and syncs and more complex kind of

00:54:01,270 --> 00:54:05,530
aggregations and and things like machine

00:54:03,370 --> 00:54:07,990
learning so it's under heavy development

00:54:05,530 --> 00:54:11,130
but you know the first one will be out

00:54:07,990 --> 00:54:13,810
in hopefully unless

00:54:11,130 --> 00:54:16,150
okay and then so in conclusion basically

00:54:13,810 --> 00:54:18,130
getting high performance for big data is

00:54:16,150 --> 00:54:20,860
increasingly hard it requires these

00:54:18,130 --> 00:54:23,350
low-level CPU and memory optimizations

00:54:20,860 --> 00:54:26,050
in SPARC we're doing that while keeping

00:54:23,350 --> 00:54:29,050
the nice aspects of a Scala API to this

00:54:26,050 --> 00:54:30,910
type safe DSL and you know we're still

00:54:29,050 --> 00:54:32,320
exploring some new ground here but we're

00:54:30,910 --> 00:54:34,780
pretty happy so far I mean these are

00:54:32,320 --> 00:54:36,400
used by real users and they basically

00:54:34,780 --> 00:54:38,590
work you know the stuff that can be

00:54:36,400 --> 00:54:41,050
improved a bit but it's pretty cool that

00:54:38,590 --> 00:54:42,880
you're able to do this in Scala and I

00:54:41,050 --> 00:54:44,940
think future things coming into Scala

00:54:42,880 --> 00:54:47,560
such as new meta programming features

00:54:44,940 --> 00:54:49,750
will in fact make this even easier and

00:54:47,560 --> 00:54:52,030
I'm super excited about you know the

00:54:49,750 --> 00:54:55,840
direction Scala is moving to facilitate

00:54:52,030 --> 00:54:58,900
really powerful API is like this and one

00:54:55,840 --> 00:55:01,690
final thing if you want to learn SPARC

00:54:58,900 --> 00:55:04,270
our company database gives you a free

00:55:01,690 --> 00:55:06,100
online environment with a mini spark

00:55:04,270 --> 00:55:08,860
cluster called community edition and we

00:55:06,100 --> 00:55:10,540
have a whole bunch of video tutorials

00:55:08,860 --> 00:55:12,190
and interactive ones you can just type

00:55:10,540 --> 00:55:16,290
in and do so I encourage you to check

00:55:12,190 --> 00:55:16,290
that out online Thanks

00:55:24,380 --> 00:55:29,609
yeah good question yeah how do you do

00:55:27,479 --> 00:55:32,279
when doing an esteem so so when doing

00:55:29,609 --> 00:55:36,450
for us is is just a group by so

00:55:32,279 --> 00:55:37,769
basically like here I cooked by some

00:55:36,450 --> 00:55:40,589
fields and the data when you do

00:55:37,769 --> 00:55:43,440
windowing you say you know window it's a

00:55:40,589 --> 00:55:45,690
by the our field and have 10 hours or

00:55:43,440 --> 00:55:48,269
something like that and you get keys

00:55:45,690 --> 00:55:51,660
which are the window ID and values which

00:55:48,269 --> 00:55:55,079
are well basically the ego in that in

00:55:51,660 --> 00:55:56,789
that window so so windowing is basically

00:55:55,079 --> 00:55:59,369
a group by is like it's kind of like

00:55:56,789 --> 00:56:01,859
doing group by some function of our

00:55:59,369 --> 00:56:07,499
where the function can put each one into

00:56:01,859 --> 00:56:08,969
many many time windows yeah the api's

00:56:07,499 --> 00:56:10,859
for this are online actually I should

00:56:08,969 --> 00:56:12,989
have had a link but if you google for

00:56:10,859 --> 00:56:15,029
spark structured streaming there's an

00:56:12,989 --> 00:56:19,940
API doc that describes this so I should

00:56:15,029 --> 00:56:19,940
have had a pointer to that thank you

00:56:23,150 --> 00:56:29,960
yeah good question so the existing ones

00:56:27,329 --> 00:56:33,299
don't all work yet they need a bit of of

00:56:29,960 --> 00:56:36,329
having to make them work here so I think

00:56:33,299 --> 00:56:39,569
the first release will likely not have

00:56:36,329 --> 00:56:42,450
kafka but then 2.1 will or we might even

00:56:39,569 --> 00:56:44,369
release by we I mean database might be

00:56:42,450 --> 00:56:46,650
released after as an external package

00:56:44,369 --> 00:56:48,900
even earlier on while we're developing

00:56:46,650 --> 00:56:52,289
it we just didn't have time to finalize

00:56:48,900 --> 00:56:54,479
it for the for the release deadline but

00:56:52,289 --> 00:56:58,200
we definitely intend to support all of

00:56:54,479 --> 00:57:00,049
these all of these things or just it has

00:56:58,200 --> 00:57:05,009
to be done kind of step by step yeah

00:57:00,049 --> 00:57:07,769
back here yeah I'm curious kin would it

00:57:05,009 --> 00:57:09,900
be possible to get to 11 binaries on the

00:57:07,769 --> 00:57:10,769
on the website oh yeah they have to

00:57:09,900 --> 00:57:12,269
build it ourselves

00:57:10,769 --> 00:57:13,950
yeah that's a really good question yeah

00:57:12,269 --> 00:57:16,349
we actually were going to do that by

00:57:13,950 --> 00:57:18,029
default starting and todaro in spark

00:57:16,349 --> 00:57:20,999
tirado we'll put those as the default

00:57:18,029 --> 00:57:22,440
and there might make sense to put

00:57:20,999 --> 00:57:25,200
binaries for all of them i think it's

00:57:22,440 --> 00:57:27,839
mostly an issue of like basically of CD

00:57:25,200 --> 00:57:30,359
and kind of space that's that's

00:57:27,839 --> 00:57:31,680
expensive but I think it's a it's a

00:57:30,359 --> 00:57:32,700
really good point I think we should we

00:57:31,680 --> 00:57:51,089
should be doing that

00:57:32,700 --> 00:57:53,339
yeah one question oh good question

00:57:51,089 --> 00:57:55,200
plugging in the reactive streams yeah we

00:57:53,339 --> 00:57:58,109
haven't looked into that too much but I

00:57:55,200 --> 00:58:00,210
think that's a really good idea and we

00:57:58,109 --> 00:58:02,490
are designing it to have both sources

00:58:00,210 --> 00:58:04,829
and sinks that our pluggable and that

00:58:02,490 --> 00:58:06,599
support interesting semantics like when

00:58:04,829 --> 00:58:08,549
you have late records we should be able

00:58:06,599 --> 00:58:11,099
to tell or think that stuff is late and

00:58:08,549 --> 00:58:13,079
you should have some way to know what to

00:58:11,099 --> 00:58:14,970
deal with that so we haven't looked at

00:58:13,079 --> 00:58:16,880
it yet but I think it's it would

00:58:14,970 --> 00:58:21,589
definitely be interesting to look at

00:58:16,880 --> 00:58:21,589

YouTube URL: https://www.youtube.com/watch?v=RUTeY4E2MoQ


