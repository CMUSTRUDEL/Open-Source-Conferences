Title: Perfect Scalability - by Michael Nash
Publication date: 2016-06-17
Playlist: Scala Days New York 2016
Description: 
	This talk was recorded at Scala Days New York, 2016. Follow along on Twitter @scaladays and on the website for more information http://scaladays.org/.

Abstract: 
One of the great benefits of the Lightbend ecosystem is its ability to empower great scalability, and it is often considered for this very reason.Indeed, if employed correctly, Scala, Akka and the rest can make it possible to much more easily write systems that scale up (and out) to extremes. However, not every design is able to scale, and the approach and architecture can sharply limit this option, no matter how good the tools. What does ""perfect scalability"" look like? Is there even such a thing? Can systems be designed to scale virtually without limit? Where does the pattern break down, and what can you do about it?

In this session, we will examine the practical architectural constraints on systems intended to scale up in near-linear fashion. It turns out that having the ability to scale to extreme load is more about what not to do than it is about what to do. Why do some system scale up and others don't? How can you take a design that has problems and refactor to a design that has fewer limits? Our teams encounter these problems every day, and we keep track of what works and what doesn't in the real world. What have we seen in the field? What proven real-world design and architecture approaches can we use to help ensure our systems can scale, and what should we avoid? How can we leverage the power of the Lightbend platform to architect such systems? Which parts for which kinds of problems? How does Akka and the actor model fit these patterns? How does Spark, Kafka, Akka streaming?

Highly scalable systems often put together tools such as Docker, Ansible, Salt, Mesos, ConductR with techniques such as microservices, monitoring, and continious delivery. Automation of the deploy pipeline and deep performance monitoring are essential parts of highly scalable systems - we will look at examples of such systems, and which of the tools in the stack they employ, and why. In this talk, we'll examine both the practices and the pitfalls, and what combinations we have used to scale not only within single clusters, but across data centers for continent-spanning applications where required.We will use use-cases all the way from massive IoT scale wearable devices all the way to high finance, and find the commonalities in the solutions that don't limit their own expansion. If your organization needs to take your scalability to significantly higher levels, this talk will be where you need to start.
Captions: 
	00:00:05,609 --> 00:00:10,990
so what are we going to talk about the

00:00:09,280 --> 00:00:12,000
the title of the talk gives it away a

00:00:10,990 --> 00:00:14,770
little bit I'm going to talk about

00:00:12,000 --> 00:00:17,380
scalability as different from

00:00:14,770 --> 00:00:19,029
performance what it means how you get

00:00:17,380 --> 00:00:21,039
there and more importantly how you don't

00:00:19,029 --> 00:00:23,080
get there some of the things that block

00:00:21,039 --> 00:00:24,550
it how it relates to performance because

00:00:23,080 --> 00:00:27,669
it's a pretty close cousin but not the

00:00:24,550 --> 00:00:30,730
same and what characterizes a scalable

00:00:27,669 --> 00:00:33,640
architecture and a scalable design there

00:00:30,730 --> 00:00:36,010
again related but not the same thing and

00:00:33,640 --> 00:00:37,660
what's this perfect scalability we will

00:00:36,010 --> 00:00:40,450
speak of perfection today it sounds very

00:00:37,660 --> 00:00:42,970
zen but how do I get that you know how

00:00:40,450 --> 00:00:44,800
do I aim for that point where I can in

00:00:42,970 --> 00:00:46,150
fact keep scaling to the level that I

00:00:44,800 --> 00:00:47,590
need because everyone's trying to

00:00:46,150 --> 00:00:50,560
achieve a higher level of scalability

00:00:47,590 --> 00:00:53,680
often so let's start by defining the

00:00:50,560 --> 00:00:55,360
term scalability is not the same as

00:00:53,680 --> 00:00:57,670
performance like I said there are close

00:00:55,360 --> 00:01:00,730
cousins and I would characterize an

00:00:57,670 --> 00:01:03,010
increase in performance as saying ok I

00:01:00,730 --> 00:01:04,839
can take the same load I was running

00:01:03,010 --> 00:01:07,509
before but I can now do it faster

00:01:04,839 --> 00:01:09,579
alright so in other words I can handle

00:01:07,509 --> 00:01:10,590
the same number of requests in a shorter

00:01:09,579 --> 00:01:13,299
amount of time

00:01:10,590 --> 00:01:15,689
adding hardware which is adding

00:01:13,299 --> 00:01:18,280
resources in general usually Hardware

00:01:15,689 --> 00:01:20,079
increases the speed of handling requests

00:01:18,280 --> 00:01:22,439
but it doesn't necessarily say the total

00:01:20,079 --> 00:01:24,609
volume of requests I can handle went up

00:01:22,439 --> 00:01:26,799
that's actually scalability so

00:01:24,609 --> 00:01:29,890
scalability is the ability to handle a

00:01:26,799 --> 00:01:31,450
larger load without failure not

00:01:29,890 --> 00:01:33,819
necessarily up the same performance so

00:01:31,450 --> 00:01:34,420
performance may in fact decrease but it

00:01:33,819 --> 00:01:35,649
didn't break

00:01:34,420 --> 00:01:38,130
you can now handle that load and

00:01:35,649 --> 00:01:42,399
requests are still being handled at a

00:01:38,130 --> 00:01:44,770
reasonable non error rate of response so

00:01:42,399 --> 00:01:46,270
I can't I can handle more requests in

00:01:44,770 --> 00:01:49,179
the same amount of time which sounds

00:01:46,270 --> 00:01:52,149
surprisingly like performance white and

00:01:49,179 --> 00:01:54,850
adding hardware increases my ability to

00:01:52,149 --> 00:01:56,679
handle more requests so I get more load

00:01:54,850 --> 00:01:59,950
I don't just handle the same load faster

00:01:56,679 --> 00:02:01,689
this is easier with pictures so a system

00:01:59,950 --> 00:02:04,719
under load write the world's simplest

00:02:01,689 --> 00:02:07,450
diagram increasing load goes in this

00:02:04,719 --> 00:02:10,119
direction the response time so the time

00:02:07,450 --> 00:02:12,489
it takes us to handle every request goes

00:02:10,119 --> 00:02:14,920
in this direction this is a typical

00:02:12,489 --> 00:02:15,700
system somewhere there's going to be a

00:02:14,920 --> 00:02:17,920
curve like

00:02:15,700 --> 00:02:19,599
sometimes even a little sharper which is

00:02:17,920 --> 00:02:22,750
where we've reached the limit of our

00:02:19,599 --> 00:02:24,760
ability to scale now the normal reaction

00:02:22,750 --> 00:02:28,540
is a throw more hardware at it either

00:02:24,760 --> 00:02:30,459
scale vertically by adding more capacity

00:02:28,540 --> 00:02:31,959
to the existing machine or scale

00:02:30,459 --> 00:02:34,270
horizontally by adding more machines

00:02:31,959 --> 00:02:35,970
great let's see the effect of that and

00:02:34,270 --> 00:02:39,640
see again the difference between

00:02:35,970 --> 00:02:42,550
scalability and performance so let's say

00:02:39,640 --> 00:02:45,400
we increase performance what that means

00:02:42,550 --> 00:02:47,110
is that the whole curve moves down it

00:02:45,400 --> 00:02:49,989
doesn't necessarily mean we change the

00:02:47,110 --> 00:02:51,940
shape of the curve so our response time

00:02:49,989 --> 00:02:53,680
is better so every response is coming

00:02:51,940 --> 00:02:56,110
back in a shorter period of time right

00:02:53,680 --> 00:02:58,810
performance is up but there's still this

00:02:56,110 --> 00:03:00,670
dogleg down here and that's the one we

00:02:58,810 --> 00:03:03,069
would worry about that's a point we'll

00:03:00,670 --> 00:03:05,170
talk about later but what's happening is

00:03:03,069 --> 00:03:06,970
that's our limit of scalability we

00:03:05,170 --> 00:03:08,799
haven't necessarily changed that by

00:03:06,970 --> 00:03:10,900
increasing performance often we will

00:03:08,799 --> 00:03:12,610
often will not get exactly the same

00:03:10,900 --> 00:03:14,380
shape curve we'll move it a little bit

00:03:12,610 --> 00:03:16,930
but there will still be a point like

00:03:14,380 --> 00:03:20,200
this somewhere in our somewhere in our

00:03:16,930 --> 00:03:22,000
breadth increasing scalability on the

00:03:20,200 --> 00:03:24,489
other hand is a different animal what

00:03:22,000 --> 00:03:26,890
that means without necessarily affecting

00:03:24,489 --> 00:03:29,350
performance is that I have now moved my

00:03:26,890 --> 00:03:31,120
line to the right I didn't necessarily

00:03:29,350 --> 00:03:32,769
bring it down although often you will a

00:03:31,120 --> 00:03:35,350
little bit and often you'll change the

00:03:32,769 --> 00:03:37,750
shape of it somewhat as well but if i

00:03:35,350 --> 00:03:39,549
strictly increased scalability i will

00:03:37,750 --> 00:03:41,590
have moved my line to the right without

00:03:39,549 --> 00:03:44,530
necessarily moving it down so I'm not

00:03:41,590 --> 00:03:46,930
actually responding to requests any

00:03:44,530 --> 00:03:48,760
faster than I was before but a larger

00:03:46,930 --> 00:03:51,489
volume of requests on the overall system

00:03:48,760 --> 00:03:53,260
is no longer met with either an error or

00:03:51,489 --> 00:03:55,090
a very rapidly increasing response time

00:03:53,260 --> 00:03:56,920
usually followed by an error you know

00:03:55,090 --> 00:03:59,799
that's typically what lives at the top

00:03:56,920 --> 00:04:02,140
of that curve is a 500 or some other

00:03:59,799 --> 00:04:03,640
manner of nasty error so that's what I

00:04:02,140 --> 00:04:05,680
mean by the difference between

00:04:03,640 --> 00:04:08,739
performance and scalability close

00:04:05,680 --> 00:04:10,900
cousins not exactly identical so I

00:04:08,739 --> 00:04:13,690
talked in the title about perfect

00:04:10,900 --> 00:04:15,700
scalability would not be nice so what

00:04:13,690 --> 00:04:16,959
would that look like if we had it what

00:04:15,700 --> 00:04:18,579
would it look like if we were to achieve

00:04:16,959 --> 00:04:19,419
it so that we know what the goal is that

00:04:18,579 --> 00:04:21,700
we're talking about

00:04:19,419 --> 00:04:24,610
well obviously we'd be able to handle

00:04:21,700 --> 00:04:27,760
higher and higher load by increasing

00:04:24,610 --> 00:04:28,889
resources not just on the same box that

00:04:27,760 --> 00:04:31,199
would be a magic computer

00:04:28,889 --> 00:04:32,999
quantum computer maybe but that's later

00:04:31,199 --> 00:04:35,610
if the riddle so really what we're

00:04:32,999 --> 00:04:38,219
saying is perfect scalability would be

00:04:35,610 --> 00:04:40,620
characterized by a linear relationship

00:04:38,219 --> 00:04:42,360
between adding resources and being able

00:04:40,620 --> 00:04:43,889
to handle more load so to give you an

00:04:42,360 --> 00:04:46,560
example if I can handle a thousand

00:04:43,889 --> 00:04:49,169
requests on one node and I can handle

00:04:46,560 --> 00:04:51,539
exactly 2,000 requests if I have two

00:04:49,169 --> 00:04:53,819
nodes well so far I have perfectly

00:04:51,539 --> 00:04:56,879
linear scalability that's what I mean by

00:04:53,819 --> 00:04:58,169
perfect scalability now there are many

00:04:56,879 --> 00:05:00,029
many things that interfere with that

00:04:58,169 --> 00:05:01,740
obviously right all of us would like to

00:05:00,029 --> 00:05:04,050
say fine just get some more instances

00:05:01,740 --> 00:05:06,479
now if you want to handle more requests

00:05:04,050 --> 00:05:08,400
on each node that sometimes it's a

00:05:06,479 --> 00:05:10,800
problem for performance or a vertical

00:05:08,400 --> 00:05:13,080
scalability on that single node but

00:05:10,800 --> 00:05:14,490
often when we go to scale nowadays we're

00:05:13,080 --> 00:05:16,439
talking about horizontal scalability

00:05:14,490 --> 00:05:19,740
where we're actually actually adding

00:05:16,439 --> 00:05:21,810
more Hardware resources to the system so

00:05:19,740 --> 00:05:24,419
perfect scalability would look like this

00:05:21,810 --> 00:05:26,339
the curve would perhaps increase over

00:05:24,419 --> 00:05:28,259
the first a while but eventually as we

00:05:26,339 --> 00:05:30,779
start adding more resources it would

00:05:28,259 --> 00:05:33,719
either be flat or possibly even decrease

00:05:30,779 --> 00:05:35,759
just a little but effectively no dogleg

00:05:33,719 --> 00:05:37,289
that's the big difference is it no

00:05:35,759 --> 00:05:40,409
matter how far we go across in this

00:05:37,289 --> 00:05:43,080
graph if we never find the dogleg that

00:05:40,409 --> 00:05:45,360
we've achieved perfect scalability don't

00:05:43,080 --> 00:05:47,399
see this a whole lot to say you know

00:05:45,360 --> 00:05:49,050
doesn't happen every day but of course

00:05:47,399 --> 00:05:52,620
that's the ideal that we aim for right

00:05:49,050 --> 00:05:55,110
how much how close to perfection you

00:05:52,620 --> 00:05:56,729
need to get for your system depends on

00:05:55,110 --> 00:05:58,469
what load you need to handle obviously

00:05:56,729 --> 00:06:00,659
you would like to say your predicted

00:05:58,469 --> 00:06:02,789
load is here and the dogleg is way off

00:06:00,659 --> 00:06:04,639
there in the corner somewhere right

00:06:02,789 --> 00:06:06,659
you've got that nice buffer zone so your

00:06:04,639 --> 00:06:09,689
scalability may not be perfect but it's

00:06:06,659 --> 00:06:11,250
sufficient but if you aim for perfect

00:06:09,689 --> 00:06:12,659
scalability in terms of your design

00:06:11,250 --> 00:06:14,279
principles you'll always grow up in a

00:06:12,659 --> 00:06:15,779
few places and you'll end up with a

00:06:14,279 --> 00:06:17,460
curve that you can live with that's

00:06:15,779 --> 00:06:19,050
really the reality of projects that

00:06:17,460 --> 00:06:21,210
we've seen out in the field over the

00:06:19,050 --> 00:06:23,610
years working with scalloped act of the

00:06:21,210 --> 00:06:25,949
whole ecosystem so what would a

00:06:23,610 --> 00:06:28,379
perfectly scalable system look like I

00:06:25,949 --> 00:06:30,330
would what would what would one look

00:06:28,379 --> 00:06:32,370
like in terms of the nuts and bolts of

00:06:30,330 --> 00:06:34,740
an actual system right let's say we add

00:06:32,370 --> 00:06:38,129
two integers very sophisticated right

00:06:34,740 --> 00:06:41,910
we've all written one of these I'm an

00:06:38,129 --> 00:06:43,470
HTTP API arbitrarily could be anything

00:06:41,910 --> 00:06:46,230
two numbers passed as a parameter on the

00:06:43,470 --> 00:06:47,520
URL and the result is in the response it

00:06:46,230 --> 00:06:50,520
sounds like it should scale pretty well

00:06:47,520 --> 00:06:52,560
right there's not a lot of things that

00:06:50,520 --> 00:06:54,510
would prevent that scaling if we fired

00:06:52,560 --> 00:06:57,690
up a second node well we should be able

00:06:54,510 --> 00:06:59,540
to do twice as many requests now that we

00:06:57,690 --> 00:07:02,430
have two nodes and so on and so on

00:06:59,540 --> 00:07:03,570
so why does this system scale what's the

00:07:02,430 --> 00:07:05,610
question we started asking ourselves

00:07:03,570 --> 00:07:07,200
because over the years my team and

00:07:05,610 --> 00:07:09,300
myself have started thinking you know

00:07:07,200 --> 00:07:11,130
what are the characteristics when we win

00:07:09,300 --> 00:07:12,540
and what are the characteristics when we

00:07:11,130 --> 00:07:13,800
lose and then what are the theoretical

00:07:12,540 --> 00:07:16,440
underpinnings and what should we avoid

00:07:13,800 --> 00:07:17,430
what you'll be in for so one of the

00:07:16,440 --> 00:07:18,990
things that's beautiful about that

00:07:17,430 --> 00:07:21,600
incredibly simple system I just

00:07:18,990 --> 00:07:22,950
described is there's no state right

00:07:21,600 --> 00:07:25,740
there's nothing holding on I'm not

00:07:22,950 --> 00:07:27,060
saying accumulate the brand total of all

00:07:25,740 --> 00:07:30,450
of the two integers that I've added

00:07:27,060 --> 00:07:33,540
anywhere so there's no retained value at

00:07:30,450 --> 00:07:35,640
all maybe not too practical but again we

00:07:33,540 --> 00:07:37,950
were looking at a simplistic example we

00:07:35,640 --> 00:07:39,780
share nothing so if I've got five nodes

00:07:37,950 --> 00:07:41,490
all doing this computation they have no

00:07:39,780 --> 00:07:42,870
idea each other exists right they're

00:07:41,490 --> 00:07:43,950
completely independent from each other

00:07:42,870 --> 00:07:46,080
there's nothing being shared between

00:07:43,950 --> 00:07:47,280
them all of the computations are

00:07:46,080 --> 00:07:49,230
independent which is another way of

00:07:47,280 --> 00:07:51,630
saying we share nothing so in other

00:07:49,230 --> 00:07:53,310
words this adding of two and two has

00:07:51,630 --> 00:07:55,230
nothing to do with this adding of two

00:07:53,310 --> 00:07:57,120
and two one does not rely on the other

00:07:55,230 --> 00:07:59,850
so we don't share state we don't have a

00:07:57,120 --> 00:08:02,190
dependency we don't communicate sounds

00:07:59,850 --> 00:08:03,830
pretty darn scalable right I can't

00:08:02,190 --> 00:08:06,780
imagine any reason that wouldn't scale

00:08:03,830 --> 00:08:09,840
it's a little flawed this is a Barak at

00:08:06,780 --> 00:08:11,970
large bond really badly wrong if all of

00:08:09,840 --> 00:08:14,910
those nodes weirdly enough or on let's

00:08:11,970 --> 00:08:16,320
save the same HTTP namespace somewhere

00:08:14,910 --> 00:08:17,850
in there there's a load bouncer and

00:08:16,320 --> 00:08:19,410
somewhere in there at a certain level of

00:08:17,850 --> 00:08:21,750
scale that low bouncer is gonna start to

00:08:19,410 --> 00:08:23,970
get really warm and a fans going to kick

00:08:21,750 --> 00:08:26,490
on and eventually we may in fact find a

00:08:23,970 --> 00:08:27,900
dogleg in that curve way down I mean you

00:08:26,490 --> 00:08:30,030
know low balances are pretty good and

00:08:27,900 --> 00:08:31,560
elastic load balancing is good so I

00:08:30,030 --> 00:08:34,320
would characterize that as its

00:08:31,560 --> 00:08:39,960
scalability being very good but not

00:08:34,320 --> 00:08:41,640
perfect even that simpler system why why

00:08:39,960 --> 00:08:43,890
would that scalability not be perfect

00:08:41,640 --> 00:08:46,500
well we share something we share

00:08:43,890 --> 00:08:48,390
something very lightly and that's that

00:08:46,500 --> 00:08:49,860
namespace but somewhere along the line

00:08:48,390 --> 00:08:50,250
maybe there's a load balancer that we

00:08:49,860 --> 00:08:52,020
share

00:08:50,250 --> 00:08:53,880
eventually that load balancer will hit

00:08:52,020 --> 00:08:55,320
its limit even if we have a cluster of

00:08:53,880 --> 00:08:57,750
load balancers we will

00:08:55,320 --> 00:08:59,850
we hit that limit so what could we maybe

00:08:57,750 --> 00:09:01,710
do about it well we would break it up

00:08:59,850 --> 00:09:03,750
let's say into multiple namespaces and

00:09:01,710 --> 00:09:05,100
maybe make a smart client I'm just

00:09:03,750 --> 00:09:06,870
throwing this out as one way to do it

00:09:05,100 --> 00:09:08,160
yes that's a wrap up with duct tape and

00:09:06,870 --> 00:09:09,800
that looks like the gas station in the

00:09:08,160 --> 00:09:14,130
background what could possibly go wrong

00:09:09,800 --> 00:09:16,470
chip it's go to production so why would

00:09:14,130 --> 00:09:18,570
this help why we're doing this to our

00:09:16,470 --> 00:09:20,490
crazy little theoretical system what

00:09:18,570 --> 00:09:22,380
what have we what's the characteristic

00:09:20,490 --> 00:09:23,460
we've just modified because that's what

00:09:22,380 --> 00:09:25,770
we've been trying to do over the years

00:09:23,460 --> 00:09:27,990
working with systems like this is what

00:09:25,770 --> 00:09:29,610
just worked why would that be a good

00:09:27,990 --> 00:09:31,890
thing to do how does it get us closer to

00:09:29,610 --> 00:09:32,460
perfect scalability well unlike in real

00:09:31,890 --> 00:09:35,580
life

00:09:32,460 --> 00:09:38,370
the answer is we shared less and we got

00:09:35,580 --> 00:09:40,380
closer to perfection so we took the tiny

00:09:38,370 --> 00:09:43,260
little bit that we were sharing on that

00:09:40,380 --> 00:09:45,420
load balancer theoretically and we tried

00:09:43,260 --> 00:09:48,000
to eliminate it so we actually reduced

00:09:45,420 --> 00:09:49,050
our sharing this is how we came up with

00:09:48,000 --> 00:09:51,090
a lot of what I'm going to talk about

00:09:49,050 --> 00:09:53,220
today is we looked at real-world systems

00:09:51,090 --> 00:09:55,110
we looked at things we did to increase

00:09:53,220 --> 00:09:56,580
their scalability every now and then

00:09:55,110 --> 00:09:57,750
bumping their performance as well and

00:09:56,580 --> 00:09:59,580
going well that doesn't help that much

00:09:57,750 --> 00:10:02,970
because all that did was move the curve

00:09:59,580 --> 00:10:05,250
not push the dogleg out and we tried to

00:10:02,970 --> 00:10:07,170
think about what we've actually modified

00:10:05,250 --> 00:10:08,610
what principle did we just change and

00:10:07,170 --> 00:10:10,560
that's what I want to give you today is

00:10:08,610 --> 00:10:12,060
some of that history some of the things

00:10:10,560 --> 00:10:14,450
that we've discovered make sense

00:10:12,060 --> 00:10:16,950
everybody everybody still with me okay

00:10:14,450 --> 00:10:19,920
can't actually see most of you so if you

00:10:16,950 --> 00:10:21,090
doze off um so I think it was Stephen

00:10:19,920 --> 00:10:22,710
Hawking who first said every time you

00:10:21,090 --> 00:10:25,290
introduce a formula to your presentation

00:10:22,710 --> 00:10:27,450
or your book in his case 50% of the

00:10:25,290 --> 00:10:28,710
readership goes away so I think 3/4 of

00:10:27,450 --> 00:10:32,160
you will have to leave at some point

00:10:28,710 --> 00:10:33,570
because it's to formula 1 is something

00:10:32,160 --> 00:10:36,270
I'm sure we've all heard of Amdahl's law

00:10:33,570 --> 00:10:37,800
and the idea is simply saying that the

00:10:36,270 --> 00:10:40,050
percentage of the time you spend in a

00:10:37,800 --> 00:10:40,950
parallel will parallelizable portion of

00:10:40,050 --> 00:10:42,390
your application

00:10:40,950 --> 00:10:44,850
Spock check doesn't know what that means

00:10:42,390 --> 00:10:46,710
by the way and the portion of your time

00:10:44,850 --> 00:10:48,420
you spend in the purely sequential

00:10:46,710 --> 00:10:52,740
portion of an application he was talking

00:10:48,420 --> 00:10:55,110
about on a single processor 100% minus

00:10:52,740 --> 00:10:57,480
that parallelizable time is the upper

00:10:55,110 --> 00:10:59,400
bound on how much parallelism or

00:10:57,480 --> 00:11:00,600
distribution can help so you could have

00:10:59,400 --> 00:11:03,690
a perfectly parallel eye system

00:11:00,600 --> 00:11:05,370
distributed all to pieces and that's

00:11:03,690 --> 00:11:06,570
your upper bound because if there's some

00:11:05,370 --> 00:11:08,450
portion of your system that still

00:11:06,570 --> 00:11:10,970
requires secuence

00:11:08,450 --> 00:11:14,510
operation you can't ever optimize beyond

00:11:10,970 --> 00:11:17,030
it so now again he was talking at the

00:11:14,510 --> 00:11:18,740
time about a single processor and in a

00:11:17,030 --> 00:11:21,850
single computing you know the typical

00:11:18,740 --> 00:11:24,590
von Neumann architecture obviously we've

00:11:21,850 --> 00:11:25,900
mostly gone beyond that now there's an

00:11:24,590 --> 00:11:28,520
honorable mention of de Stefan's

00:11:25,900 --> 00:11:30,470
counter-argument to this but the bottom

00:11:28,520 --> 00:11:32,180
line is if only 50% of your algorithm is

00:11:30,470 --> 00:11:33,890
parallelizable then the absolute best

00:11:32,180 --> 00:11:35,960
then anything can do in terms of

00:11:33,890 --> 00:11:38,690
distribution for you is 50% and that's

00:11:35,960 --> 00:11:40,040
if everything we're perfect now it's not

00:11:38,690 --> 00:11:43,040
even that good the news is actually

00:11:40,040 --> 00:11:45,380
worse than this windsors law which takes

00:11:43,040 --> 00:11:46,940
a little bit further and I won't again I

00:11:45,380 --> 00:11:48,740
won't bore you with the with the actual

00:11:46,940 --> 00:11:51,260
formula but taking it and take it on

00:11:48,740 --> 00:11:55,250
good authority that the K in this case

00:11:51,260 --> 00:11:57,620
is the coherency delay so in other words

00:11:55,250 --> 00:11:59,060
the time for different elements of your

00:11:57,620 --> 00:12:01,250
system different nodes in the

00:11:59,060 --> 00:12:03,410
distributed system to synchronize on a

00:12:01,250 --> 00:12:05,050
piece of data is that coherency delay

00:12:03,410 --> 00:12:09,260
before we all arrive at the same answer

00:12:05,050 --> 00:12:10,760
so having a higher K is worse in this

00:12:09,260 --> 00:12:12,910
formula if you actually do the math and

00:12:10,760 --> 00:12:15,260
again we were really talking about here

00:12:12,910 --> 00:12:17,690
parallelizable systems on a single CP on

00:12:15,260 --> 00:12:22,010
a single node on a single CPU multiple

00:12:17,690 --> 00:12:24,260
cpus on one chip essentially the higher

00:12:22,010 --> 00:12:25,610
the cave the higher the communication in

00:12:24,260 --> 00:12:28,730
this case it's actually not with the K

00:12:25,610 --> 00:12:31,790
stands for the worst our potential

00:12:28,730 --> 00:12:33,410
upside of scalability is so of course

00:12:31,790 --> 00:12:35,690
you can work all of these formulas and

00:12:33,410 --> 00:12:36,860
say ok what is the what would perfect

00:12:35,690 --> 00:12:39,620
scalability look like well perfect

00:12:36,860 --> 00:12:42,290
scalability would look like the ability

00:12:39,620 --> 00:12:44,840
to paralyze all of our operations 100%

00:12:42,290 --> 00:12:46,430
of our operations then 100 minus the

00:12:44,840 --> 00:12:49,130
parallelizable portion is zero and

00:12:46,430 --> 00:12:52,700
that's the absolute worst case that we

00:12:49,130 --> 00:12:54,830
think we could have and the minimum k k

00:12:52,700 --> 00:12:56,150
even zero which means there is no

00:12:54,830 --> 00:12:57,830
coherency well how do you achieve no

00:12:56,150 --> 00:12:59,750
coherency you achieve no sharing no

00:12:57,830 --> 00:13:03,710
sharing of data if there's no contention

00:12:59,750 --> 00:13:06,110
or sharing then both of these laws start

00:13:03,710 --> 00:13:08,710
to not apply and those relate to perfect

00:13:06,110 --> 00:13:12,710
scalability I promise no more formulas

00:13:08,710 --> 00:13:15,800
okay so if we refer to that dogleg we

00:13:12,710 --> 00:13:19,730
saw in the curve as a wall in our

00:13:15,800 --> 00:13:21,620
scalability it is very common to reach a

00:13:19,730 --> 00:13:22,550
point where you hit that wall the way

00:13:21,620 --> 00:13:24,440
you know you hit that

00:13:22,550 --> 00:13:26,660
you start throwing additional nodes at

00:13:24,440 --> 00:13:28,550
your scalable system and your

00:13:26,660 --> 00:13:29,839
performance or your overall ability to

00:13:28,550 --> 00:13:32,870
handle load different from your

00:13:29,839 --> 00:13:35,240
performance goes down not up a little

00:13:32,870 --> 00:13:37,070
bit typically what that means is that

00:13:35,240 --> 00:13:39,170
you've actually hit Gunter's law in the

00:13:37,070 --> 00:13:40,610
sense that in a distributed sense which

00:13:39,170 --> 00:13:42,560
is really an extension of what he had in

00:13:40,610 --> 00:13:45,920
mind but in that distributed sense where

00:13:42,560 --> 00:13:48,440
the overall necessity to synchronize

00:13:45,920 --> 00:13:50,779
between nodes in any fashion has started

00:13:48,440 --> 00:13:52,430
to exceed the ability of those new nodes

00:13:50,779 --> 00:13:55,100
to add more performance to your system

00:13:52,430 --> 00:13:56,870
so you've actually gone backwards via

00:13:55,100 --> 00:13:58,880
the overhead of synchronizing those

00:13:56,870 --> 00:14:00,620
nodes is somehow greater than the

00:13:58,880 --> 00:14:01,760
difference of adding one more node that

00:14:00,620 --> 00:14:03,589
is truly the point of diminishing

00:14:01,760 --> 00:14:05,149
returns because at that point you can

00:14:03,589 --> 00:14:06,980
put as many more nodes as you want you

00:14:05,149 --> 00:14:10,190
actually cannot increase your ability to

00:14:06,980 --> 00:14:13,190
handle load on that overall system now

00:14:10,190 --> 00:14:15,440
the wall could be closer than that in

00:14:13,190 --> 00:14:19,459
the sense that if your algorithm in the

00:14:15,440 --> 00:14:21,050
design of your system does not follow a

00:14:19,459 --> 00:14:22,490
certain number of rules which are more

00:14:21,050 --> 00:14:24,829
about avoiding things than including

00:14:22,490 --> 00:14:26,540
things then the wall will be where that

00:14:24,829 --> 00:14:28,310
dogleg is in the curve because the

00:14:26,540 --> 00:14:31,490
dogleg in the curve will be that point

00:14:28,310 --> 00:14:33,260
of increasing response time and often

00:14:31,490 --> 00:14:35,270
it's darn near vertical at some point

00:14:33,260 --> 00:14:37,520
you'll get a few more requests and

00:14:35,270 --> 00:14:40,040
suddenly your system starts to 500 and

00:14:37,520 --> 00:14:42,290
requests are starting to fail that dog

00:14:40,040 --> 00:14:44,540
late in the curve is what we call is

00:14:42,290 --> 00:14:46,370
what I'm referring to as a wall whether

00:14:44,540 --> 00:14:47,779
it's caused by that coherency delay

00:14:46,370 --> 00:14:49,459
which is probably further down or

00:14:47,779 --> 00:14:51,140
whether it's caused much more commonly

00:14:49,459 --> 00:14:52,640
by having an algorithm that you can't

00:14:51,140 --> 00:14:55,209
fully distribute one way or another you

00:14:52,640 --> 00:14:58,130
run into it the good news is that

00:14:55,209 --> 00:14:59,839
quoting Edison here we haven't failed to

00:14:58,130 --> 00:15:01,370
discover perfect scalability but we know

00:14:59,839 --> 00:15:03,350
a great many more than ten thousand ways

00:15:01,370 --> 00:15:05,300
that it won't work so we know a lot

00:15:03,350 --> 00:15:07,070
about we started looking at the problem

00:15:05,300 --> 00:15:09,199
in terms of negative space we went okay

00:15:07,070 --> 00:15:11,029
what are the things about a solution

00:15:09,199 --> 00:15:12,680
that would prevent it from being able to

00:15:11,029 --> 00:15:14,570
scale perfectly and we know a lot about

00:15:12,680 --> 00:15:17,390
that so here I can tell you a great deal

00:15:14,570 --> 00:15:19,610
about failure and hopefully that'll been

00:15:17,390 --> 00:15:21,160
formed and what we found is that by and

00:15:19,610 --> 00:15:23,600
large from a practical point of view

00:15:21,160 --> 00:15:25,459
avoiding these enemies of scalability

00:15:23,600 --> 00:15:28,339
I'll call them is what gets you around

00:15:25,459 --> 00:15:31,399
the wall as opposed to simply moving the

00:15:28,339 --> 00:15:32,600
wall it's not that hard to move the wall

00:15:31,399 --> 00:15:34,100
out a little bit you improve your

00:15:32,600 --> 00:15:35,750
algorithm you prove to reduce

00:15:34,100 --> 00:15:36,390
communication time you get faster

00:15:35,750 --> 00:15:38,310
process

00:15:36,390 --> 00:15:40,200
you can keep kind of shoving that dogleg

00:15:38,310 --> 00:15:42,390
in the curve further and further to the

00:15:40,200 --> 00:15:44,100
right can't make it go away until you

00:15:42,390 --> 00:15:45,540
fundamentally change the algorithms and

00:15:44,100 --> 00:15:48,089
the style and the architecture of your

00:15:45,540 --> 00:15:50,579
application and even though it's pretty

00:15:48,089 --> 00:15:52,829
tricky so what are some of these enemies

00:15:50,579 --> 00:15:56,160
that I've talked about suspenseful music

00:15:52,829 --> 00:15:58,079
here contention is the obvious one

00:15:56,160 --> 00:15:59,519
so contending over resources we'll talk

00:15:58,079 --> 00:15:59,760
about that we'll talk about how to avoid

00:15:59,519 --> 00:16:02,250
it

00:15:59,760 --> 00:16:04,380
shared state shared state leads to

00:16:02,250 --> 00:16:08,970
contention sounds like something in the

00:16:04,380 --> 00:16:11,550
Jedi Creed sharing of any data of any

00:16:08,970 --> 00:16:13,470
state of any information moving messages

00:16:11,550 --> 00:16:16,320
around the network these are all enemies

00:16:13,470 --> 00:16:18,390
of scalability strangely and there are

00:16:16,320 --> 00:16:21,000
ways to minimize some of these things

00:16:18,390 --> 00:16:22,410
ordering in sequence is one of the most

00:16:21,000 --> 00:16:25,470
insidious ones and one of the ones we

00:16:22,410 --> 00:16:26,910
see most common and it turns out that by

00:16:25,470 --> 00:16:28,339
shifting your algorithm and shifting

00:16:26,910 --> 00:16:31,040
your design you can usually avoid

00:16:28,339 --> 00:16:34,079
ordering in sequence requirements

00:16:31,040 --> 00:16:35,850
communication and I will talk about that

00:16:34,079 --> 00:16:38,339
in some more detail and then of course

00:16:35,850 --> 00:16:40,230
failure things going wrong with the

00:16:38,339 --> 00:16:42,510
overall system naturally limit the

00:16:40,230 --> 00:16:45,990
ability to scale and we'll talk about

00:16:42,510 --> 00:16:48,029
that too so avoiding contention

00:16:45,990 --> 00:16:51,500
contention fighting over a resource in

00:16:48,029 --> 00:16:54,060
this case of all contention comes from

00:16:51,500 --> 00:16:55,290
parallelism or distribution and when you

00:16:54,060 --> 00:16:57,000
think about a distribution there's just

00:16:55,290 --> 00:16:58,440
parallelism in the large we're going

00:16:57,000 --> 00:17:01,279
across multiple machines rather than

00:16:58,440 --> 00:17:04,230
multiple cores and a single CPU and

00:17:01,279 --> 00:17:06,299
shared state so you can paralyze all you

00:17:04,230 --> 00:17:08,309
want and you will never have contention

00:17:06,299 --> 00:17:10,230
the problem comes when you start to

00:17:08,309 --> 00:17:11,730
share anything as soon as there's some

00:17:10,230 --> 00:17:13,829
piece of state or piece of information

00:17:11,730 --> 00:17:15,990
or data that needs to be shared by more

00:17:13,829 --> 00:17:17,579
than one process in the system you end

00:17:15,990 --> 00:17:20,400
up with contention you end up with a

00:17:17,579 --> 00:17:22,350
scalability problem so no share

00:17:20,400 --> 00:17:24,179
resources avoids it well be great if we

00:17:22,350 --> 00:17:25,980
could design algorithms like that that's

00:17:24,179 --> 00:17:27,900
the challenge so the hard part is not

00:17:25,980 --> 00:17:30,570
necessarily to completely eliminate

00:17:27,900 --> 00:17:34,290
state but to minimize that state which

00:17:30,570 --> 00:17:36,870
is shared so how do we do that the

00:17:34,290 --> 00:17:39,090
easiest way and the way that sometimes

00:17:36,870 --> 00:17:40,470
is not obvious and we almost have to

00:17:39,090 --> 00:17:41,580
force ourselves to do it if we've come

00:17:40,470 --> 00:17:43,980
from a little bit of the relational

00:17:41,580 --> 00:17:46,350
database world is to get each dog their

00:17:43,980 --> 00:17:48,300
own stick basically do you normalize

00:17:46,350 --> 00:17:49,710
you'd be normalize data to think about

00:17:48,300 --> 00:17:51,929
it in the data base sense what

00:17:49,710 --> 00:17:54,480
happens is you end up with duplicate

00:17:51,929 --> 00:17:57,240
information existing within your overall

00:17:54,480 --> 00:17:58,830
system that feels unnatural at first but

00:17:57,240 --> 00:18:00,299
in distributed systems operating at

00:17:58,830 --> 00:18:02,610
scale it's actually completely normal

00:18:00,299 --> 00:18:04,260
and okay and again you have to keep

00:18:02,610 --> 00:18:06,270
repeating that to yourself sometimes

00:18:04,260 --> 00:18:08,880
when you come from the old world but

00:18:06,270 --> 00:18:09,809
effectively if individual processes in

00:18:08,880 --> 00:18:11,460
parts of your system

00:18:09,809 --> 00:18:12,929
have their own copy of data well then

00:18:11,460 --> 00:18:14,820
they're not sharing data now how does

00:18:12,929 --> 00:18:16,289
that copy get updated etc these are all

00:18:14,820 --> 00:18:20,130
good questions that you have to answer

00:18:16,289 --> 00:18:22,590
eventually so avoiding shared state is

00:18:20,130 --> 00:18:25,140
probably one of the most important these

00:18:22,590 --> 00:18:28,110
are maybe vaguely in order of importance

00:18:25,140 --> 00:18:29,909
in certainly in terms of how often we

00:18:28,110 --> 00:18:31,799
see the problem and the problem appears

00:18:29,909 --> 00:18:34,860
to be helped by doing these things I've

00:18:31,799 --> 00:18:38,340
tried to order these enemies in that in

00:18:34,860 --> 00:18:39,690
that order so state is great nothing

00:18:38,340 --> 00:18:41,760
wrong with state we're not saying that

00:18:39,690 --> 00:18:43,470
each of your individual nodes at each of

00:18:41,760 --> 00:18:45,210
individual processes needs to be

00:18:43,470 --> 00:18:46,830
completely stateless statelessness is

00:18:45,210 --> 00:18:48,029
all well and good but an awful lot of

00:18:46,830 --> 00:18:50,039
things can't be done with statelessness

00:18:48,029 --> 00:18:53,340
you actually do need state that's fine

00:18:50,039 --> 00:18:55,230
but state must be private and private to

00:18:53,340 --> 00:18:58,289
what and private to where we'll talk a

00:18:55,230 --> 00:19:00,480
little bit about more so by broadcasting

00:18:58,289 --> 00:19:02,070
deltas to state which are usually

00:19:00,480 --> 00:19:03,390
considered events something that's

00:19:02,070 --> 00:19:05,039
happened in the past so on commands

00:19:03,390 --> 00:19:06,270
something happens in the future and

00:19:05,039 --> 00:19:09,210
event something that's happened in the

00:19:06,270 --> 00:19:11,309
past by broadcasting the deltas to state

00:19:09,210 --> 00:19:13,830
in some fashion whether you're using a

00:19:11,309 --> 00:19:15,149
cadet or replication or you're using an

00:19:13,830 --> 00:19:18,179
event sourcing pattern of command

00:19:15,149 --> 00:19:19,890
sourcing pattern you can then have

00:19:18,179 --> 00:19:22,080
multiple copies of state which will

00:19:19,890 --> 00:19:23,250
eventually converge so eg the basis of

00:19:22,080 --> 00:19:26,130
eventual consistency

00:19:23,250 --> 00:19:28,080
private state to within one element of

00:19:26,130 --> 00:19:29,789
your system often one node of your

00:19:28,080 --> 00:19:31,350
system may be one micro service of your

00:19:29,789 --> 00:19:33,419
system can then be updated from these

00:19:31,350 --> 00:19:35,940
deltas after the fact so yes I am

00:19:33,419 --> 00:19:37,529
talking about eventual consistency but

00:19:35,940 --> 00:19:40,320
if avoid some of those possibilities now

00:19:37,529 --> 00:19:43,200
here's a counter problem we also want to

00:19:40,320 --> 00:19:44,520
limit communication so as part of your

00:19:43,200 --> 00:19:46,440
design patterns when you're considering

00:19:44,520 --> 00:19:50,130
high scalability look at all

00:19:46,440 --> 00:19:53,760
communication as a cost in other words I

00:19:50,130 --> 00:19:55,470
have to have a really good reason to put

00:19:53,760 --> 00:19:57,690
up with communication between my notes

00:19:55,470 --> 00:20:00,299
my perfect scalability would be nobody

00:19:57,690 --> 00:20:01,200
talks everyone does not communicate with

00:20:00,299 --> 00:20:03,210
anyone else they're completely

00:20:01,200 --> 00:20:05,130
independent probably can't do that

00:20:03,210 --> 00:20:07,679
but every time I add communication to my

00:20:05,130 --> 00:20:09,179
system I am reducing my ability to scale

00:20:07,679 --> 00:20:13,309
and I'm bringing the wall a little bit

00:20:09,179 --> 00:20:16,440
closer so point-to-point communications

00:20:13,309 --> 00:20:18,929
particularly must be seen as a form of

00:20:16,440 --> 00:20:21,480
coupling and coupling there's another

00:20:18,929 --> 00:20:23,880
one of these enemies take into account

00:20:21,480 --> 00:20:25,230
locality when considering that cost so

00:20:23,880 --> 00:20:27,149
in other words all communication is not

00:20:25,230 --> 00:20:29,789
created equal if I communicate with

00:20:27,149 --> 00:20:31,679
another process on my local node that's

00:20:29,789 --> 00:20:33,029
generally a lot less expensive than

00:20:31,679 --> 00:20:35,399
communicating with the process on

00:20:33,029 --> 00:20:37,289
another node obviously if I communicate

00:20:35,399 --> 00:20:39,120
with another node in another data center

00:20:37,289 --> 00:20:41,250
ok I really need to think about that one

00:20:39,120 --> 00:20:43,350
the volume of communication that I can

00:20:41,250 --> 00:20:45,659
handle without severely compromising

00:20:43,350 --> 00:20:47,909
scalability goes down rapidly with the

00:20:45,659 --> 00:20:51,779
distance both logically and physically

00:20:47,909 --> 00:20:54,980
from the originator ok so communication

00:20:51,779 --> 00:20:57,299
is a cost ordering is a huge cost

00:20:54,980 --> 00:20:59,940
because ordering actually leads to

00:20:57,299 --> 00:21:01,500
shared state in a certain sense you must

00:20:59,940 --> 00:21:05,700
have shared state in order to preserve

00:21:01,500 --> 00:21:07,890
or in order to pervert preserve order or

00:21:05,700 --> 00:21:10,020
English for that matter

00:21:07,890 --> 00:21:12,870
it leads to contention and it severely

00:21:10,020 --> 00:21:15,210
limits scalability so in the developer

00:21:12,870 --> 00:21:16,590
sense stay commutative it should not

00:21:15,210 --> 00:21:19,500
matter in the order on which things are

00:21:16,590 --> 00:21:22,289
applied and we'll talk about how to do

00:21:19,500 --> 00:21:23,520
that some of you may already guess now

00:21:22,289 --> 00:21:25,140
what's interesting is we talked about

00:21:23,520 --> 00:21:26,730
failure and we said of course failure is

00:21:25,140 --> 00:21:28,440
a problem in a distributed system in a

00:21:26,730 --> 00:21:31,110
highly scalable system chances are

00:21:28,440 --> 00:21:32,970
highly scalable system is distributed so

00:21:31,110 --> 00:21:36,840
a distributed system is actually much

00:21:32,970 --> 00:21:38,610
more likely to have something fail it is

00:21:36,840 --> 00:21:41,279
on the other hand much more likely that

00:21:38,610 --> 00:21:42,779
the system will not fail so the system

00:21:41,279 --> 00:21:45,419
as a whole and its ability to handle

00:21:42,779 --> 00:21:47,360
load at all must not fail or obviously

00:21:45,419 --> 00:21:49,770
we have severely limited our scalability

00:21:47,360 --> 00:21:51,210
possibly to zero if all the lights go

00:21:49,770 --> 00:21:54,690
out and the whole thing goes down that's

00:21:51,210 --> 00:21:57,450
a problem now what's interesting is the

00:21:54,690 --> 00:21:59,580
less we share the less likely system

00:21:57,450 --> 00:22:01,260
failure is so a whole series of

00:21:59,580 --> 00:22:02,909
completely independent well you know

00:22:01,260 --> 00:22:04,980
people know this even from power

00:22:02,909 --> 00:22:06,630
supplies if you're on the same plug and

00:22:04,980 --> 00:22:07,529
the lights go out well chances are

00:22:06,630 --> 00:22:09,090
they're both going to go down at the

00:22:07,529 --> 00:22:12,000
same time you're own completely separate

00:22:09,090 --> 00:22:14,490
independent power supplies same thing is

00:22:12,000 --> 00:22:16,980
true of data if your source of data is

00:22:14,490 --> 00:22:19,169
independent and not necessarily

00:22:16,980 --> 00:22:21,419
dependent on another service which may

00:22:19,169 --> 00:22:23,130
or may not fail and you have as few of

00:22:21,419 --> 00:22:25,100
those connections as possible a few of

00:22:23,130 --> 00:22:27,360
those dependencies possible than the

00:22:25,100 --> 00:22:30,059
probability of that one element failing

00:22:27,360 --> 00:22:32,220
those let goes down and therefore the

00:22:30,059 --> 00:22:35,190
probability of your overall system being

00:22:32,220 --> 00:22:37,710
able to survive goes up so again sharing

00:22:35,190 --> 00:22:41,970
is a problem like the dogs with the

00:22:37,710 --> 00:22:45,179
state the presents itself as an

00:22:41,970 --> 00:22:47,400
avoidance of linear time so again this

00:22:45,179 --> 00:22:49,080
is an algorithmic design pattern this is

00:22:47,400 --> 00:22:51,570
really now down to design not so much

00:22:49,080 --> 00:22:53,520
architecture where you use things such

00:22:51,570 --> 00:22:56,070
as finite state machines single use

00:22:53,520 --> 00:22:58,500
actors there's many different types of

00:22:56,070 --> 00:23:00,720
patterns and things to avoid linear

00:22:58,500 --> 00:23:03,360
processing where you have a whole series

00:23:00,720 --> 00:23:04,799
of steps which must be processed one

00:23:03,360 --> 00:23:09,150
after another well when you think about

00:23:04,799 --> 00:23:11,549
it that's a source of sequence there is

00:23:09,150 --> 00:23:13,200
sequence in your in your processing even

00:23:11,549 --> 00:23:14,429
if there isn't sequencing your data you

00:23:13,200 --> 00:23:16,440
don't have things that you say okay I

00:23:14,429 --> 00:23:19,020
have to process that's one two three but

00:23:16,440 --> 00:23:22,290
I do have to finish these functions one

00:23:19,020 --> 00:23:26,429
at a time if you can avoid that you are

00:23:22,290 --> 00:23:29,030
again avoiding the linear nature that is

00:23:26,429 --> 00:23:31,290
actually an enemy of scalability

00:23:29,030 --> 00:23:33,750
communications especially between

00:23:31,290 --> 00:23:36,299
services must be asynchronous and non

00:23:33,750 --> 00:23:37,710
blocking and this is also linear time so

00:23:36,299 --> 00:23:39,390
if I have a service that calls another

00:23:37,710 --> 00:23:41,549
service waits for the reply and then go

00:23:39,390 --> 00:23:42,840
well my absolute limit of scalability is

00:23:41,549 --> 00:23:45,900
whatever the scalability of that other

00:23:42,840 --> 00:23:48,000
services plus some constant because I

00:23:45,900 --> 00:23:49,380
take time to process whereas if I could

00:23:48,000 --> 00:23:50,820
do a fire-and-forget message and

00:23:49,380 --> 00:23:52,740
eventually this comes back and then I

00:23:50,820 --> 00:23:54,780
send on the message in the meantime I've

00:23:52,740 --> 00:23:56,669
handled twenty other requests I'm a lot

00:23:54,780 --> 00:23:59,790
more scalable than in the first scenario

00:23:56,669 --> 00:24:01,860
so communications between services have

00:23:59,790 --> 00:24:04,710
to be async and non blocking that sounds

00:24:01,860 --> 00:24:06,299
easy you know when you say it fast it's

00:24:04,710 --> 00:24:08,520
actually very difficult and it's one of

00:24:06,299 --> 00:24:09,690
the most common problems we see is that

00:24:08,520 --> 00:24:11,790
when you actually think about it

00:24:09,690 --> 00:24:14,040
services are in fact temporally coupled

00:24:11,790 --> 00:24:15,510
this has to finish before that this has

00:24:14,040 --> 00:24:17,280
to actually send an answer back to that

00:24:15,510 --> 00:24:18,630
you may as well combine those into a

00:24:17,280 --> 00:24:21,090
single bounded context and into a

00:24:18,630 --> 00:24:22,890
service they'll probably be faster so

00:24:21,090 --> 00:24:25,590
that's a case where micro services have

00:24:22,890 --> 00:24:27,150
actually been taken too far or further

00:24:25,590 --> 00:24:28,530
than is valuable from a scalability

00:24:27,150 --> 00:24:29,850
point of view in any case it might be

00:24:28,530 --> 00:24:33,929
another good reason for separating

00:24:29,850 --> 00:24:38,370
but scalability isn't it so think about

00:24:33,929 --> 00:24:40,650
design for perfect scalability as a

00:24:38,370 --> 00:24:44,190
series of principles and a series of

00:24:40,650 --> 00:24:47,010
things to avoid and build your services

00:24:44,190 --> 00:24:49,799
around adherence to those principles

00:24:47,010 --> 00:24:51,360
first so for example if I go all right

00:24:49,799 --> 00:24:52,679
I'm going to build this in such a

00:24:51,360 --> 00:24:54,570
fashion that I need communication

00:24:52,679 --> 00:24:56,309
between these two things well ask

00:24:54,570 --> 00:24:58,500
yourself okay what's the cost of that

00:24:56,309 --> 00:24:59,850
communication is there another way can I

00:24:58,500 --> 00:25:01,770
have the data between those two

00:24:59,850 --> 00:25:03,929
separated can they both derive the

00:25:01,770 --> 00:25:05,460
information that they require from some

00:25:03,929 --> 00:25:07,860
separate source rather than going to

00:25:05,460 --> 00:25:10,980
each other because cost is the thing

00:25:07,860 --> 00:25:13,320
that sorry communication is a thing that

00:25:10,980 --> 00:25:14,429
cost me in my design so you kind of do

00:25:13,320 --> 00:25:16,380
the balance sheet in your head when

00:25:14,429 --> 00:25:17,700
you're looking at a design like this in

00:25:16,380 --> 00:25:19,409
many cases you'll need a different

00:25:17,700 --> 00:25:20,850
algorithm the algorithm that you

00:25:19,409 --> 00:25:22,860
originally sketched out to actually

00:25:20,850 --> 00:25:24,900
solve your problem in your domain you'll

00:25:22,860 --> 00:25:26,640
need to go well and on that requires too

00:25:24,900 --> 00:25:27,720
much communication or that means that

00:25:26,640 --> 00:25:29,580
these steps all have to happen

00:25:27,720 --> 00:25:31,860
sequentially what if we did it like this

00:25:29,580 --> 00:25:33,510
then we could avoid those enemies of

00:25:31,860 --> 00:25:36,809
scalability you'll end up with a better

00:25:33,510 --> 00:25:39,059
design tuning can't fix a bad design

00:25:36,809 --> 00:25:41,400
this and that again that sounds obvious

00:25:39,059 --> 00:25:43,470
when you say it fast we have helped many

00:25:41,400 --> 00:25:45,720
a client try to you know tune a bad

00:25:43,470 --> 00:25:47,669
design to the point where that's it the

00:25:45,720 --> 00:25:49,200
wall is here the wall isn't moving any

00:25:47,669 --> 00:25:51,960
further that's the limit of the

00:25:49,200 --> 00:25:54,530
tunability so tunability is not the

00:25:51,960 --> 00:25:57,480
answer to scalability problems define it

00:25:54,530 --> 00:25:59,549
so a scalable architecture taking it a

00:25:57,480 --> 00:26:00,450
scale-up and and going okay now let's

00:25:59,549 --> 00:26:02,070
think about the plumbing and the

00:26:00,450 --> 00:26:04,289
interconnect between service and so on

00:26:02,070 --> 00:26:05,880
obviously avoid single points of

00:26:04,289 --> 00:26:08,059
contention which by the way also avoid

00:26:05,880 --> 00:26:10,530
single points of failure generally

00:26:08,059 --> 00:26:12,030
nothing shared and we'll talk about what

00:26:10,530 --> 00:26:13,400
the worst possible thing to share is and

00:26:12,030 --> 00:26:16,770
then we see it all the time in a while

00:26:13,400 --> 00:26:18,150
no state would be great often you have

00:26:16,770 --> 00:26:19,140
to violate some of these rules but if

00:26:18,150 --> 00:26:20,909
you violate them as little as possible

00:26:19,140 --> 00:26:23,370
you end up with a more scalable system

00:26:20,909 --> 00:26:24,809
no sequence sequence is a killer

00:26:23,370 --> 00:26:27,450
it actually has several of these

00:26:24,809 --> 00:26:30,450
disadvantages all built into one and no

00:26:27,450 --> 00:26:32,309
synchronous persistence of course we all

00:26:30,450 --> 00:26:34,530
know you know talking to disk in any

00:26:32,309 --> 00:26:36,900
fashion or you know even other

00:26:34,530 --> 00:26:39,330
persistent stores is many times slower

00:26:36,900 --> 00:26:40,799
than not except of course when the

00:26:39,330 --> 00:26:42,280
network is actually a bottleneck which

00:26:40,799 --> 00:26:46,220
eventually you get to

00:26:42,280 --> 00:26:47,840
so scalability goes both ways and this

00:26:46,220 --> 00:26:49,610
is often also forgotten and this is

00:26:47,840 --> 00:26:52,250
actually a very common problem we see is

00:26:49,610 --> 00:26:55,400
that clients will want a system that

00:26:52,250 --> 00:26:57,950
will scale to handle some huge load but

00:26:55,400 --> 00:26:59,540
what they don't anticipate is how can

00:26:57,950 --> 00:27:02,090
you rapidly successfully without

00:26:59,540 --> 00:27:04,270
dropping requests scale back down again

00:27:02,090 --> 00:27:06,890
when the load is starting to reduce

00:27:04,270 --> 00:27:09,200
oftentimes you'll be able to predict in

00:27:06,890 --> 00:27:11,150
a particular scenario that load will be

00:27:09,200 --> 00:27:13,280
encountered on the following timeframe

00:27:11,150 --> 00:27:15,740
or you'll have a season or you'll have

00:27:13,280 --> 00:27:17,270
you know everybody comes at 11:55 every

00:27:15,740 --> 00:27:19,550
morning you have all of your requests

00:27:17,270 --> 00:27:21,170
great you can scale your system up how

00:27:19,550 --> 00:27:23,360
do you go about scaling back down again

00:27:21,170 --> 00:27:25,309
and that's often a pure cost avoidance

00:27:23,360 --> 00:27:26,870
problem because of course if you can

00:27:25,309 --> 00:27:28,429
scale up and you can handle all the load

00:27:26,870 --> 00:27:30,260
in the world with your thousand nodes

00:27:28,429 --> 00:27:32,210
well thousand nodes on Amazon it's gonna

00:27:30,260 --> 00:27:34,520
cost you a couple of bucks so how do you

00:27:32,210 --> 00:27:36,140
turn that back down again is actually a

00:27:34,520 --> 00:27:37,370
problem you have to think about it's

00:27:36,140 --> 00:27:38,720
it's not obvious that's one of the

00:27:37,370 --> 00:27:41,450
reasons that one of the key attributes

00:27:38,720 --> 00:27:44,170
of micro services is mobility I need to

00:27:41,450 --> 00:27:46,370
be able to move them around so

00:27:44,170 --> 00:27:49,460
responding to load should of course also

00:27:46,370 --> 00:27:51,380
allow our reduction not just up so what

00:27:49,460 --> 00:27:53,030
what's it like at the high end of that

00:27:51,380 --> 00:27:56,390
scale so we've done a few systems that

00:27:53,030 --> 00:27:58,640
are very large the ultra large scale

00:27:56,390 --> 00:28:00,559
systems have a few characteristics and

00:27:58,640 --> 00:28:02,420
again these are characteristics that if

00:28:00,559 --> 00:28:05,090
you think about them now when you're

00:28:02,420 --> 00:28:06,260
trying to scale you don't have to try to

00:28:05,090 --> 00:28:08,600
bolt them in afterwards because

00:28:06,260 --> 00:28:12,620
retrofitting things for scalability is

00:28:08,600 --> 00:28:15,679
often a challenge until we say bordering

00:28:12,620 --> 00:28:17,960
on the impossible let's say so they tend

00:28:15,679 --> 00:28:20,990
to be systems of systems so they tend to

00:28:17,960 --> 00:28:23,990
be a series of independent operating

00:28:20,990 --> 00:28:25,160
systems within an overall system that

00:28:23,990 --> 00:28:26,929
are not necessarily coupled to each

00:28:25,160 --> 00:28:28,520
other other than very very loosely if at

00:28:26,929 --> 00:28:30,350
all they share some of the

00:28:28,520 --> 00:28:32,270
characteristics of organic ecosystems

00:28:30,350 --> 00:28:33,320
interestingly enough and you'll see what

00:28:32,270 --> 00:28:35,929
I mean by that when we talk about

00:28:33,320 --> 00:28:37,490
failure they handle conflicting

00:28:35,929 --> 00:28:38,600
requirements so there are actually two

00:28:37,490 --> 00:28:40,460
or three different ways of doing

00:28:38,600 --> 00:28:42,500
something within some of these are ultra

00:28:40,460 --> 00:28:45,230
large-scale systems they evolve

00:28:42,500 --> 00:28:48,110
continuously while in operation I often

00:28:45,230 --> 00:28:49,610
use this analogy with my team I say you

00:28:48,110 --> 00:28:51,530
know the airplanes in flight we've got a

00:28:49,610 --> 00:28:54,980
full passenger load I'm going to replace

00:28:51,530 --> 00:28:57,499
the wing now this is the the this the

00:28:54,980 --> 00:28:59,450
the impression you get when trying to

00:28:57,499 --> 00:29:02,779
modify an existing running system and

00:28:59,450 --> 00:29:06,080
it's possible but it's easier if you

00:29:02,779 --> 00:29:08,059
follow some of these things so they're

00:29:06,080 --> 00:29:09,679
often internally inconsistent and this

00:29:08,059 --> 00:29:11,330
is part of getting each dog their own

00:29:09,679 --> 00:29:14,330
stick a little bit you have situations

00:29:11,330 --> 00:29:15,889
where system a and system B will in fact

00:29:14,330 --> 00:29:17,720
disagree about what the customers

00:29:15,889 --> 00:29:19,820
balance is and that might actually be

00:29:17,720 --> 00:29:21,980
okay it's certainly a lot more scalable

00:29:19,820 --> 00:29:23,389
and we'll talk about later balancing the

00:29:21,980 --> 00:29:26,619
need for scalability against the need

00:29:23,389 --> 00:29:29,899
for consistency and all of these things

00:29:26,619 --> 00:29:32,480
users affect overall emergent behavior

00:29:29,899 --> 00:29:34,369
so highly scalable systems and

00:29:32,480 --> 00:29:36,289
particularly ultra large scale systems

00:29:34,369 --> 00:29:37,789
are extremely difficult to predict ahead

00:29:36,289 --> 00:29:40,850
of time so we're going to talk about

00:29:37,789 --> 00:29:43,220
monitoring and user load and user

00:29:40,850 --> 00:29:45,470
behavior have a significant effect on

00:29:43,220 --> 00:29:47,119
where the load is coming on your system

00:29:45,470 --> 00:29:49,369
and it's very difficult to anticipate

00:29:47,119 --> 00:29:52,820
that ahead of time so the reaction is

00:29:49,369 --> 00:29:55,549
let's have the ability to modify our

00:29:52,820 --> 00:29:57,679
system on the fly such that we now need

00:29:55,549 --> 00:29:59,869
50 more instances of that service okay

00:29:57,679 --> 00:30:01,519
if that's possible and we can scale that

00:29:59,869 --> 00:30:04,369
independently of all other pieces of our

00:30:01,519 --> 00:30:05,929
system we can adapt to a shape to the

00:30:04,369 --> 00:30:07,909
load that we did not anticipate and

00:30:05,929 --> 00:30:11,419
that's extremely common in ultra

00:30:07,909 --> 00:30:14,450
large-scale systems failure is the norm

00:30:11,419 --> 00:30:15,859
at any given moment it is likely some

00:30:14,450 --> 00:30:18,830
piece of the system will be down and

00:30:15,859 --> 00:30:20,480
that's okay just again with an organic

00:30:18,830 --> 00:30:21,769
system every single cell in your body is

00:30:20,480 --> 00:30:23,869
not necessarily operating at peak

00:30:21,769 --> 00:30:25,879
efficiency 24 hours a day and that's

00:30:23,869 --> 00:30:28,039
okay some of them are dying all right

00:30:25,879 --> 00:30:31,669
the overall system however is not

00:30:28,039 --> 00:30:34,549
affected you're still up so obviously

00:30:31,669 --> 00:30:36,350
one shape of that is spike load

00:30:34,549 --> 00:30:37,389
sometimes you get a whole lot more users

00:30:36,350 --> 00:30:40,460
and sometimes they're really unhappy

00:30:37,389 --> 00:30:43,789
something you were expecting scalability

00:30:40,460 --> 00:30:45,470
means you can add resources not that you

00:30:43,789 --> 00:30:47,720
already have so there's nothing actually

00:30:45,470 --> 00:30:50,720
inherent about scalability that says I

00:30:47,720 --> 00:30:52,730
can handle spike load I might be have an

00:30:50,720 --> 00:30:55,609
incredibly scalable system I can add

00:30:52,730 --> 00:30:56,419
integers as fast as anything but now all

00:30:55,609 --> 00:30:58,730
of a sudden there's a whole lot more

00:30:56,419 --> 00:31:00,590
people trying to add integers than I

00:30:58,730 --> 00:31:01,879
expected I still go down I've got an

00:31:00,590 --> 00:31:03,950
incredibly scalable system but that

00:31:01,879 --> 00:31:05,809
doesn't mean I have scale I me still

00:31:03,950 --> 00:31:07,549
need the ability to add those resources

00:31:05,809 --> 00:31:08,260
when load starts to go up again the

00:31:07,549 --> 00:31:10,210
importance of MA

00:31:08,260 --> 00:31:11,860
which we'll talk about later so even

00:31:10,210 --> 00:31:13,990
it's basically the moral is even a

00:31:11,860 --> 00:31:16,690
perfectly scalable system can be taken

00:31:13,990 --> 00:31:18,400
down by spiked load so some of the

00:31:16,690 --> 00:31:20,590
specific techniques we've seen to try to

00:31:18,400 --> 00:31:22,690
handle that are you put shock absorbers

00:31:20,590 --> 00:31:24,220
in your system so to speak we frequently

00:31:22,690 --> 00:31:26,890
see Apache Kafka in this role for

00:31:24,220 --> 00:31:28,420
instance command sourcing is one way to

00:31:26,890 --> 00:31:30,640
go about this so if you differentiate a

00:31:28,420 --> 00:31:32,650
command which is a request for something

00:31:30,640 --> 00:31:34,660
which could fail that is supposed to

00:31:32,650 --> 00:31:35,830
happen in the future from an event which

00:31:34,660 --> 00:31:38,020
is the recording of something that's

00:31:35,830 --> 00:31:39,850
already happened it's in the past then

00:31:38,020 --> 00:31:41,020
command sourcing is essentially like

00:31:39,850 --> 00:31:42,700
events or City except the other way

00:31:41,020 --> 00:31:44,680
around the original command of the

00:31:42,700 --> 00:31:46,570
system is actually what you persist then

00:31:44,680 --> 00:31:49,150
you handle it after you've persisted it

00:31:46,570 --> 00:31:50,800
that way if spike load starts to happen

00:31:49,150 --> 00:31:52,420
you can decouple those two temporarily

00:31:50,800 --> 00:31:55,630
and say great we're gonna record

00:31:52,420 --> 00:31:56,680
commands as fast as they come in what

00:31:55,630 --> 00:31:59,380
we're going to process them a little

00:31:56,680 --> 00:32:03,520
later so you're then the shock absorber

00:31:59,380 --> 00:32:04,990
returns to normal size so if we just say

00:32:03,520 --> 00:32:06,910
what's on the slider if we persist our

00:32:04,990 --> 00:32:09,070
commands initially and handle them

00:32:06,910 --> 00:32:13,150
asynchronously we have a much better

00:32:09,070 --> 00:32:14,800
ability to handle spike load and to give

00:32:13,150 --> 00:32:16,390
us an opportunity to go oh look spike

00:32:14,800 --> 00:32:17,770
load is happening great let's ramp up

00:32:16,390 --> 00:32:19,930
other systems whether it's a human

00:32:17,770 --> 00:32:23,140
watching a graph and flipping switches

00:32:19,930 --> 00:32:25,390
to bring up new nodes or an elastic load

00:32:23,140 --> 00:32:27,760
balancing system detecting that load and

00:32:25,390 --> 00:32:29,710
bringing up new nodes it's always alive

00:32:27,760 --> 00:32:31,960
there's you know unless you're trying to

00:32:29,710 --> 00:32:33,910
predict it by saying ok the rate of

00:32:31,960 --> 00:32:36,010
increase is X there's still a bit of a

00:32:33,910 --> 00:32:37,150
lag bringing up those new nodes so

00:32:36,010 --> 00:32:39,810
although you have a system that can

00:32:37,150 --> 00:32:42,610
scale it won't necessarily be scale and

00:32:39,810 --> 00:32:44,230
then the opposite is also true just

00:32:42,610 --> 00:32:46,540
again like organic systems and ultra

00:32:44,230 --> 00:32:50,260
large-scale systems degrading gracefully

00:32:46,540 --> 00:32:52,060
is very important because if there are

00:32:50,260 --> 00:32:54,730
and again all of these attributes tie

00:32:52,060 --> 00:32:56,410
together so the less you share the more

00:32:54,730 --> 00:32:59,320
the probability is you will in fact be

00:32:56,410 --> 00:33:00,580
able to degrade gracefully I love this

00:32:59,320 --> 00:33:01,960
quote it's actually actually a comedian

00:33:00,580 --> 00:33:03,280
not a computer scientist that came up

00:33:01,960 --> 00:33:04,720
with an escalator can never break it

00:33:03,280 --> 00:33:07,540
just become stairs we have a demo of

00:33:04,720 --> 00:33:10,210
that of the lobby as it turns out you

00:33:07,540 --> 00:33:12,370
see real world I didn't arrange that so

00:33:10,210 --> 00:33:14,560
basically the idea is portions of the

00:33:12,370 --> 00:33:16,150
system should be able to fail without

00:33:14,560 --> 00:33:18,490
affecting other portions that's actually

00:33:16,150 --> 00:33:19,750
a good test switch some services off to

00:33:18,490 --> 00:33:21,260
the other services still work great

00:33:19,750 --> 00:33:23,179
you're not couple you know

00:33:21,260 --> 00:33:25,340
if the coupling is there if those

00:33:23,179 --> 00:33:27,169
services simply keep going with slightly

00:33:25,340 --> 00:33:28,669
out of date data but they keep going

00:33:27,169 --> 00:33:30,169
then you have a much more resilient

00:33:28,669 --> 00:33:33,650
system that can in fact do very

00:33:30,169 --> 00:33:36,200
gracefully it again is sometimes really

00:33:33,650 --> 00:33:38,360
hard to design this ahead of time it's

00:33:36,200 --> 00:33:39,890
very hard to predict and it is a design

00:33:38,360 --> 00:33:42,410
issue not a tuning issue

00:33:39,890 --> 00:33:44,809
you can't tune place for that

00:33:42,410 --> 00:33:47,179
degradation your system has to be built

00:33:44,809 --> 00:33:48,740
to do that so all of this probably

00:33:47,179 --> 00:33:50,210
points us a little bit in the direction

00:33:48,740 --> 00:33:51,770
of micro-services I'm not going to try

00:33:50,210 --> 00:33:54,200
to duplicate other excellent thoughts on

00:33:51,770 --> 00:33:57,559
that but micro-services do come with a

00:33:54,200 --> 00:33:59,120
cost there's added complexity they're

00:33:57,559 --> 00:34:00,770
more difficult to deploy you know this

00:33:59,120 --> 00:34:01,970
is this is going this is making the

00:34:00,770 --> 00:34:03,410
rounds and a lot of different circles

00:34:01,970 --> 00:34:05,600
that people are realizing hey this is

00:34:03,410 --> 00:34:07,190
not free this is not a universally

00:34:05,600 --> 00:34:09,169
cheaper and easier way to build systems

00:34:07,190 --> 00:34:10,550
but they're worth it from a scalability

00:34:09,169 --> 00:34:11,810
point of view and that's what we're

00:34:10,550 --> 00:34:13,550
limiting ourselves to in this talk

00:34:11,810 --> 00:34:15,649
though they have other advantages if

00:34:13,550 --> 00:34:17,600
they provide some of the things we

00:34:15,649 --> 00:34:19,129
talked about here that is a sync

00:34:17,600 --> 00:34:21,590
communication both of the services were

00:34:19,129 --> 00:34:23,330
required and again minimize it more it's

00:34:21,590 --> 00:34:26,169
a cost keep it as low as possible

00:34:23,330 --> 00:34:28,550
note the word async I repeat myself

00:34:26,169 --> 00:34:30,080
isolation which is how we get the

00:34:28,550 --> 00:34:32,149
ability to have some pieces of the

00:34:30,080 --> 00:34:35,179
system fail in the overall system keep

00:34:32,149 --> 00:34:36,590
going and a single responsibility now

00:34:35,179 --> 00:34:38,869
the single responsibility principle

00:34:36,590 --> 00:34:41,270
comes way back from the object-oriented

00:34:38,869 --> 00:34:43,250
days applies to micro services and it's

00:34:41,270 --> 00:34:44,990
a good design principle to just keep

00:34:43,250 --> 00:34:47,330
things clean but there's an important

00:34:44,990 --> 00:34:49,690
element that relates specifically to

00:34:47,330 --> 00:34:51,740
scalability about that and that is

00:34:49,690 --> 00:34:53,570
individual functions can then be tuned

00:34:51,740 --> 00:34:55,700
independently so let's say for some

00:34:53,570 --> 00:34:58,369
reason a whole bunch of people start

00:34:55,700 --> 00:34:59,810
logging in but not doing anything else

00:34:58,369 --> 00:35:01,700
in your system well you probably need

00:34:59,810 --> 00:35:03,320
more instances of the login service but

00:35:01,700 --> 00:35:05,960
if the login service is also the one

00:35:03,320 --> 00:35:08,510
that does five other things well then

00:35:05,960 --> 00:35:10,369
it's hard to say I need just more login

00:35:08,510 --> 00:35:12,350
I need more capacity around that

00:35:10,369 --> 00:35:13,970
function around that particular piece of

00:35:12,350 --> 00:35:16,160
my system but not other pieces

00:35:13,970 --> 00:35:18,800
necessarily you can't change the shape

00:35:16,160 --> 00:35:20,510
of your capacity handling easily unless

00:35:18,800 --> 00:35:22,100
each element of your system does only

00:35:20,510 --> 00:35:23,780
one thing then you can just say great

00:35:22,100 --> 00:35:26,420
give me more of those and you can

00:35:23,780 --> 00:35:28,250
increase capacity that way so that's the

00:35:26,420 --> 00:35:31,310
attribute of micro services that

00:35:28,250 --> 00:35:33,619
specifically relates to that and most

00:35:31,310 --> 00:35:34,850
importantly has its own data so micro

00:35:33,619 --> 00:35:36,770
service should hazards

00:35:34,850 --> 00:35:39,080
should have its own data which is not

00:35:36,770 --> 00:35:41,720
shared with other services talk about

00:35:39,080 --> 00:35:44,150
them again in another minute simple is

00:35:41,720 --> 00:35:46,360
good so if your systems diagram starting

00:35:44,150 --> 00:35:50,150
to look like this you're not gonna feel

00:35:46,360 --> 00:35:51,320
sorry you know and and it's this is this

00:35:50,150 --> 00:35:53,300
is a joke obviously

00:35:51,320 --> 00:35:55,100
however I've seen system diagrams that

00:35:53,300 --> 00:35:56,420
look worse than this they didn't have

00:35:55,100 --> 00:35:57,800
the loo moving balls they really needed

00:35:56,420 --> 00:35:59,600
to for the message passing you know I

00:35:57,800 --> 00:36:01,220
thought it really looked like a a badly

00:35:59,600 --> 00:36:04,550
done acted job

00:36:01,220 --> 00:36:06,110
however complexity doesn't scale so on

00:36:04,550 --> 00:36:07,970
top of all of the other things I'm

00:36:06,110 --> 00:36:10,220
telling you to avoid I am telling you to

00:36:07,970 --> 00:36:12,800
steer towards simplicity and avoid

00:36:10,220 --> 00:36:14,290
complexity because actually a lot of the

00:36:12,800 --> 00:36:16,790
principles we're talking about our

00:36:14,290 --> 00:36:19,490
initial principles first principles that

00:36:16,790 --> 00:36:20,740
actually encourage simplicity so simple

00:36:19,490 --> 00:36:23,960
patterns applied consistently

00:36:20,740 --> 00:36:28,030
consistently way easier to scale than a

00:36:23,960 --> 00:36:31,190
complex pattern coordinating on time

00:36:28,030 --> 00:36:33,950
which is incredibly common really easy

00:36:31,190 --> 00:36:35,900
to fall into mentally doesn't work at

00:36:33,950 --> 00:36:37,310
all in a distributed system right it is

00:36:35,900 --> 00:36:38,810
physically an impossibility

00:36:37,310 --> 00:36:41,840
I think Wade's going to talk about that

00:36:38,810 --> 00:36:43,880
at one point and so the worst kind of

00:36:41,840 --> 00:36:46,310
coordination you can have in a system is

00:36:43,880 --> 00:36:48,020
temporal I love this postman with two

00:36:46,310 --> 00:36:50,030
watches never knows what time it is is

00:36:48,020 --> 00:36:52,040
an approximation o'clock time

00:36:50,030 --> 00:36:54,290
specifically cannot be used for ordering

00:36:52,040 --> 00:36:56,180
so one ordering is bad o'clock time to

00:36:54,290 --> 00:36:58,850
do it is even worse and we see the two

00:36:56,180 --> 00:37:02,270
of them combined often persistence

00:36:58,850 --> 00:37:03,980
doesn't have to be a bottleneck but it

00:37:02,270 --> 00:37:05,870
is again a cost that you have to think

00:37:03,980 --> 00:37:06,670
about minimizing command sourcing is one

00:37:05,870 --> 00:37:09,320
way to do that

00:37:06,670 --> 00:37:11,330
idem potency is another way to do it the

00:37:09,320 --> 00:37:14,540
file system strangely is often the

00:37:11,330 --> 00:37:16,250
fastest choice databases seldom are we

00:37:14,540 --> 00:37:18,350
often however see Cassandra Postgres

00:37:16,250 --> 00:37:20,030
HDFS in this space but sometimes we see

00:37:18,350 --> 00:37:22,520
things like CAPTA which do in fact

00:37:20,030 --> 00:37:24,230
generally persist to the file system

00:37:22,520 --> 00:37:27,230
streaming is your friend talking to the

00:37:24,230 --> 00:37:29,720
file system a way to pick the right spot

00:37:27,230 --> 00:37:31,190
to persist without persisting any more

00:37:29,720 --> 00:37:32,870
than necessary because it is such a high

00:37:31,190 --> 00:37:35,150
cost is to think about where you need to

00:37:32,870 --> 00:37:37,430
recover this is our favorite metric is

00:37:35,150 --> 00:37:40,030
to say okay if I had to pick that

00:37:37,430 --> 00:37:41,810
request back up and do it again and

00:37:40,030 --> 00:37:43,940
idempotency is how I can do it again

00:37:41,810 --> 00:37:46,250
where would I need to do that at what

00:37:43,940 --> 00:37:47,660
point in the system could I do I need to

00:37:46,250 --> 00:37:49,490
recover now could I

00:37:47,660 --> 00:37:51,170
what point in the system must I recover

00:37:49,490 --> 00:37:53,210
from where there's a failure I can pick

00:37:51,170 --> 00:37:55,069
it back up and my favorite spot for that

00:37:53,210 --> 00:37:56,839
is at the very beginning so command

00:37:55,069 --> 00:38:00,140
sourcing when I get the initial request

00:37:56,839 --> 00:38:01,460
persist that great done now the system

00:38:00,140 --> 00:38:02,960
cannot fail because I could always pick

00:38:01,460 --> 00:38:03,440
that back up out of persistence and do

00:38:02,960 --> 00:38:06,890
it again

00:38:03,440 --> 00:38:08,329
now if I can do it again that means that

00:38:06,890 --> 00:38:12,079
I have idempotent we'll talk about that

00:38:08,329 --> 00:38:14,359
later the most egregious pattern that we

00:38:12,079 --> 00:38:17,119
see that it leads to many of the others

00:38:14,359 --> 00:38:18,109
and brings many of them together is to

00:38:17,119 --> 00:38:19,940
not keep your hands out of other

00:38:18,109 --> 00:38:21,950
people's databases essentially it's the

00:38:19,940 --> 00:38:24,470
way we put it is the worst thing to

00:38:21,950 --> 00:38:26,089
share is a database if your services

00:38:24,470 --> 00:38:27,559
share a database this is also a great

00:38:26,089 --> 00:38:29,869
quote then your database is your

00:38:27,559 --> 00:38:31,309
monolith so if you've done microservices

00:38:29,869 --> 00:38:33,170
but you share a database all you've done

00:38:31,309 --> 00:38:36,079
Smoove the monolith it's now in that box

00:38:33,170 --> 00:38:37,039
over there marked database services have

00:38:36,079 --> 00:38:38,839
to keep their midst out of other

00:38:37,039 --> 00:38:41,000
people's databases they have to have

00:38:38,839 --> 00:38:43,039
their own independent source of private

00:38:41,000 --> 00:38:44,720
state so therefore their own independent

00:38:43,039 --> 00:38:46,339
database storage mechanism doesn't

00:38:44,720 --> 00:38:48,799
matter what that mechanism is you don't

00:38:46,339 --> 00:38:50,599
share it with anybody else so see rdt is

00:38:48,799 --> 00:38:52,309
an event sourcing are ways to share that

00:38:50,599 --> 00:38:54,109
state we talked about them before where

00:38:52,309 --> 00:38:55,250
you broadcast a delta to the state and

00:38:54,109 --> 00:38:57,170
say hey anybody that cares

00:38:55,250 --> 00:38:59,559
this just changed if you need to update

00:38:57,170 --> 00:39:03,529
your local data store knock yourself out

00:38:59,559 --> 00:39:05,210
distributed transactions are you know a

00:39:03,529 --> 00:39:07,430
sea monster is a good way of putting it

00:39:05,210 --> 00:39:08,869
lots of arms they go everywhere and suck

00:39:07,430 --> 00:39:11,180
everything down with them they are

00:39:08,869 --> 00:39:12,890
actually the exact opposite of the

00:39:11,180 --> 00:39:14,630
philosophical approach we find works the

00:39:12,890 --> 00:39:16,460
best in other words that's actually

00:39:14,630 --> 00:39:17,990
solving the problem but solving it by

00:39:16,460 --> 00:39:20,480
bowing in exactly the wrong direction

00:39:17,990 --> 00:39:22,670
by attempting to impose a global now in

00:39:20,480 --> 00:39:25,039
some sense in some portion across the

00:39:22,670 --> 00:39:26,750
system so if you find yourself thinking

00:39:25,039 --> 00:39:28,460
hmm I'm gonna need more transactions

00:39:26,750 --> 00:39:30,559
here or more than one service is going

00:39:28,460 --> 00:39:32,089
to need a transaction you probably need

00:39:30,559 --> 00:39:36,890
to back up and look at that design again

00:39:32,089 --> 00:39:39,799
for persistence so idempotency is how we

00:39:36,890 --> 00:39:41,210
can recover only at the spot we need to

00:39:39,799 --> 00:39:43,160
recover what I mean by that is let's say

00:39:41,210 --> 00:39:44,660
we command source so every time we get a

00:39:43,160 --> 00:39:46,430
request into our system the first thing

00:39:44,660 --> 00:39:48,650
we do is save it then all of the other

00:39:46,430 --> 00:39:50,569
processing in our system could be

00:39:48,650 --> 00:39:52,400
repeated if of course the system goes

00:39:50,569 --> 00:39:53,839
down halfway through processing and we

00:39:52,400 --> 00:39:55,309
pick that back up out of persistent

00:39:53,839 --> 00:39:57,770
storage see hey it hasn't been checked

00:39:55,309 --> 00:39:59,839
off when done it yet do it again if our

00:39:57,770 --> 00:40:01,490
system is okay with that then we've

00:39:59,839 --> 00:40:04,280
achieved idempotency and

00:40:01,490 --> 00:40:07,310
we can persist less so scalability is

00:40:04,280 --> 00:40:08,960
improved by having idempotency right so

00:40:07,310 --> 00:40:10,910
it's a third order relationship but

00:40:08,960 --> 00:40:12,860
basically unimportance he says we don't

00:40:10,910 --> 00:40:14,150
need to persist as much because we could

00:40:12,860 --> 00:40:15,650
just recover the whole thing again by

00:40:14,150 --> 00:40:17,750
replaying the command in its entirety

00:40:15,650 --> 00:40:19,700
and my system is okay with that

00:40:17,750 --> 00:40:21,500
so idempotency is important in terms of

00:40:19,700 --> 00:40:23,510
scalability and that's why I was saying

00:40:21,500 --> 00:40:25,310
persist only where you need to and where

00:40:23,510 --> 00:40:28,010
you need to is where do I need to

00:40:25,310 --> 00:40:30,920
recover am i doing okay

00:40:28,010 --> 00:40:32,690
few more minutes consistency helps so

00:40:30,920 --> 00:40:34,730
the idea of domain driven design and

00:40:32,690 --> 00:40:36,320
consistency again I encourage you to go

00:40:34,730 --> 00:40:38,570
to Wade stop where he's going to explain

00:40:36,320 --> 00:40:40,160
what's in this picture however focus

00:40:38,570 --> 00:40:42,650
focusing on the domain and not the

00:40:40,160 --> 00:40:46,160
plumbing is really the core of DDD and

00:40:42,650 --> 00:40:47,600
onion and a consistent approach helps

00:40:46,160 --> 00:40:49,640
ensure that you're actually following

00:40:47,600 --> 00:40:51,440
these principles so if you have a way of

00:40:49,640 --> 00:40:53,630
splitting up your system that is always

00:40:51,440 --> 00:40:55,550
maintained consistently in your codebase

00:40:53,630 --> 00:40:57,260
you're ahead of the game as opposed to

00:40:55,550 --> 00:40:58,369
every service being different then

00:40:57,260 --> 00:40:59,960
you're going to have to look for those

00:40:58,369 --> 00:41:02,330
enemies of scalability in a different

00:40:59,960 --> 00:41:03,770
place every time otherwise you can get

00:41:02,330 --> 00:41:05,360
some good patterns going at certain

00:41:03,770 --> 00:41:07,300
layers of the architecture and be

00:41:05,360 --> 00:41:10,240
consistent about it by encapsulating

00:41:07,300 --> 00:41:12,340
bounded context in a single service or a

00:41:10,240 --> 00:41:14,660
tightly knit group of services

00:41:12,340 --> 00:41:15,470
communication is therefore limited so in

00:41:14,660 --> 00:41:17,480
other words everything to do with

00:41:15,470 --> 00:41:19,400
customers is in this service right

00:41:17,480 --> 00:41:20,780
therefore customer information doesn't

00:41:19,400 --> 00:41:23,060
need to be communicated to anybody else

00:41:20,780 --> 00:41:25,190
I own the customer state I keep it

00:41:23,060 --> 00:41:27,670
private I keep an internal that's a DDD

00:41:25,190 --> 00:41:30,260
concept applied to scalability

00:41:27,670 --> 00:41:32,480
now obviously scalability is not always

00:41:30,260 --> 00:41:35,030
the only concern we have you know we

00:41:32,480 --> 00:41:37,700
need to go fast we need velocity we have

00:41:35,030 --> 00:41:41,180
lots of features so scalability and

00:41:37,700 --> 00:41:43,160
simplicity must be balanced against cost

00:41:41,180 --> 00:41:44,930
in every case how much do I need to

00:41:43,160 --> 00:41:47,270
scale how far do I need to push the wall

00:41:44,930 --> 00:41:49,010
it's a good consideration but I always

00:41:47,270 --> 00:41:50,210
give yourself some buffer space and if

00:41:49,010 --> 00:41:51,350
you follow some of these design

00:41:50,210 --> 00:41:52,730
principles even though you don't

00:41:51,350 --> 00:41:55,460
necessarily need them in your

00:41:52,730 --> 00:41:56,960
scalability level now then you avoid the

00:41:55,460 --> 00:41:58,700
syndrome of that we've seen in some

00:41:56,960 --> 00:42:01,369
clients where success is their worst

00:41:58,700 --> 00:42:04,130
enemy you know our site is taking off oh

00:42:01,369 --> 00:42:05,420
no our site is taking off you know we're

00:42:04,130 --> 00:42:07,040
starting to get all sorts of traffic

00:42:05,420 --> 00:42:08,750
that's great marketing think it's

00:42:07,040 --> 00:42:09,859
wonderful and the tech guys are ready to

00:42:08,750 --> 00:42:11,840
pull their hair up because they're not

00:42:09,859 --> 00:42:14,030
ready for that scalability trying to

00:42:11,840 --> 00:42:15,510
trying to retool your system to handle

00:42:14,030 --> 00:42:19,980
scalability at the MoMA

00:42:15,510 --> 00:42:22,410
success is far too late so now quickly

00:42:19,980 --> 00:42:24,720
to talk about monitoring trust your

00:42:22,410 --> 00:42:26,430
design but at the same time verified by

00:42:24,720 --> 00:42:28,140
definition a distributed system is

00:42:26,430 --> 00:42:29,340
non-deterministic therefore I don't

00:42:28,140 --> 00:42:31,500
actually know what it's going to do

00:42:29,340 --> 00:42:32,790
under load the more complex that system

00:42:31,500 --> 00:42:34,410
gets and the more pieces of it there are

00:42:32,790 --> 00:42:37,050
less I know what it's going to do under

00:42:34,410 --> 00:42:39,090
load so how do i how do I predict I

00:42:37,050 --> 00:42:40,440
don't I follow I find out what it's

00:42:39,090 --> 00:42:43,160
going to do under load by monitoring

00:42:40,440 --> 00:42:46,350
what you monitor how much you monitor

00:42:43,160 --> 00:42:48,180
load tests or necessity monitoring can

00:42:46,350 --> 00:42:50,460
tell you things that design and tests

00:42:48,180 --> 00:42:53,850
static tests in a isolated environment

00:42:50,460 --> 00:42:55,170
can't tell you the log is not enough to

00:42:53,850 --> 00:42:57,990
paraphrase double-oh-seven

00:42:55,170 --> 00:42:59,460
log aggregation is necessary that's

00:42:57,990 --> 00:43:00,690
great you can see what's going on in

00:42:59,460 --> 00:43:02,460
logging logging of courses you have

00:43:00,690 --> 00:43:05,310
another cost and you have to watch for

00:43:02,460 --> 00:43:06,560
contention VM and node monitoring is

00:43:05,310 --> 00:43:08,880
great

00:43:06,560 --> 00:43:10,230
throughput in response time monitoring

00:43:08,880 --> 00:43:12,210
is also handy if you're using an

00:43:10,230 --> 00:43:14,670
accurate system monitoring time in

00:43:12,210 --> 00:43:16,020
mailbox and mailbox size is probably one

00:43:14,670 --> 00:43:17,670
of the stats you're most interested in

00:43:16,020 --> 00:43:19,590
because that's showing you the shock

00:43:17,670 --> 00:43:21,810
absorbers in the small in your system

00:43:19,590 --> 00:43:23,400
but be careful how you monitor it's also

00:43:21,810 --> 00:43:25,890
possible to build monitoring in your

00:43:23,400 --> 00:43:28,530
system and slow it down so we've seen

00:43:25,890 --> 00:43:30,360
techniques such as push monitoring where

00:43:28,530 --> 00:43:32,460
you're actually sending statistics to a

00:43:30,360 --> 00:43:35,280
centralized server over UDP or something

00:43:32,460 --> 00:43:36,540
stats D and so on we've also seen pull

00:43:35,280 --> 00:43:37,890
monitoring where you connect to each

00:43:36,540 --> 00:43:40,410
node and pull statistics that are

00:43:37,890 --> 00:43:42,600
aggregated internal to the node via jmx

00:43:40,410 --> 00:43:43,680
both of those have downsides and you

00:43:42,600 --> 00:43:45,300
have to carefully balance those

00:43:43,680 --> 00:43:47,610
downsides keep thinking about

00:43:45,300 --> 00:43:48,800
communication as a cost and that's

00:43:47,610 --> 00:43:51,960
communication

00:43:48,800 --> 00:43:53,790
DevOps matters a lot in highly scalable

00:43:51,960 --> 00:43:55,590
systems and a perfectly scalable system

00:43:53,790 --> 00:43:57,390
you have to be able to do a rolling

00:43:55,590 --> 00:43:59,850
upgrade for example you have to be able

00:43:57,390 --> 00:44:01,320
to have your system again in consistency

00:43:59,850 --> 00:44:03,780
you have to have some nodes if your

00:44:01,320 --> 00:44:05,610
system are on version 1 other nodes of

00:44:03,780 --> 00:44:07,620
your system are on version 1.5 and

00:44:05,610 --> 00:44:09,120
everything's still ok because if you

00:44:07,620 --> 00:44:10,920
can't upgrade your system then by

00:44:09,120 --> 00:44:12,630
definition you can't scale it did you

00:44:10,920 --> 00:44:14,520
actually have to turn it off in order to

00:44:12,630 --> 00:44:16,620
or turn some pieces of it off in order

00:44:14,520 --> 00:44:18,420
to roll out a new version so you have to

00:44:16,620 --> 00:44:19,860
think about versioning of messages you

00:44:18,420 --> 00:44:22,320
have to think about a rolling restart

00:44:19,860 --> 00:44:24,330
and so on all of that is greatly

00:44:22,320 --> 00:44:26,820
facilitated by modern DevOps too so

00:44:24,330 --> 00:44:28,650
things such as meso Sandy costs and

00:44:26,820 --> 00:44:30,809
conductor and so on

00:44:28,650 --> 00:44:32,520
Oh palak that's all I have time to say

00:44:30,809 --> 00:44:34,520
on that is that that's essential you

00:44:32,520 --> 00:44:37,140
can't overlook it it is the most

00:44:34,520 --> 00:44:38,549
commonly overlooked section when we talk

00:44:37,140 --> 00:44:40,500
about scalable systems everybody's all

00:44:38,549 --> 00:44:41,700
excited about the design ah this is

00:44:40,500 --> 00:44:44,010
going to be amazing it's going to scale

00:44:41,700 --> 00:44:45,839
like crazy how do we deploy it everybody

00:44:44,010 --> 00:44:48,660
stops and looked at each other well

00:44:45,839 --> 00:44:52,770
so ask that question good and early and

00:44:48,660 --> 00:44:55,440
deploy as a process of day one okay

00:44:52,770 --> 00:44:58,049
nearly at the end so perfect scalability

00:44:55,440 --> 00:44:59,940
is in fact achievable it not with every

00:44:58,049 --> 00:45:02,430
design there are some designs that will

00:44:59,940 --> 00:45:04,770
not scale no matter what you apply to

00:45:02,430 --> 00:45:06,750
them in terms of resources if you avoid

00:45:04,770 --> 00:45:08,609
those enemies of scalability we were

00:45:06,750 --> 00:45:10,260
talking about you're better off than if

00:45:08,609 --> 00:45:11,430
you don't those are the rules of thumb

00:45:10,260 --> 00:45:13,230
that I can give you that we've found

00:45:11,430 --> 00:45:15,150
over the years that make a big

00:45:13,230 --> 00:45:16,470
difference and almost an order of

00:45:15,150 --> 00:45:19,529
importance keeping out of other people's

00:45:16,470 --> 00:45:21,960
databases maybe being number one find

00:45:19,529 --> 00:45:24,299
the patterns of the design of design

00:45:21,960 --> 00:45:26,970
that don't use those enemies so for

00:45:24,299 --> 00:45:27,900
example CQRS event sourcing these are

00:45:26,970 --> 00:45:29,640
some of the things that allow you to

00:45:27,900 --> 00:45:31,109
avoid some of those enemies that's why

00:45:29,640 --> 00:45:33,599
they work it's because they're staying

00:45:31,109 --> 00:45:35,160
away from these things so you almost set

00:45:33,599 --> 00:45:37,890
up to design your system in a negative

00:45:35,160 --> 00:45:39,450
sense by going what should I not do okay

00:45:37,890 --> 00:45:42,510
great anything that's not in that bucket

00:45:39,450 --> 00:45:44,069
chances are that's all right monitor and

00:45:42,510 --> 00:45:46,260
adjust monitoring is critical and

00:45:44,069 --> 00:45:48,329
embracing handle failure it's going to

00:45:46,260 --> 00:45:49,770
fail that's okay as long as it doesn't

00:45:48,329 --> 00:45:53,880
take the whole system down you still all

00:45:49,770 --> 00:45:56,099
right okay time for a couple of

00:45:53,880 --> 00:45:59,059
questions anyone have that expression

00:45:56,099 --> 00:46:03,869
I know make sure you see any tilted head

00:45:59,059 --> 00:46:06,619
it sums it up no it's done silence oh

00:46:03,869 --> 00:46:06,619
wait

00:46:10,060 --> 00:46:16,010
very funny I'm writing a book this way

00:46:12,500 --> 00:46:18,410
not about this interestingly enough no

00:46:16,010 --> 00:46:31,760
and you can't make me get rid of that

00:46:18,410 --> 00:46:32,599
good anybody else that's an excellent

00:46:31,760 --> 00:46:33,920
question

00:46:32,599 --> 00:46:36,040
there's a lot of detail behind so the

00:46:33,920 --> 00:46:38,690
question was if we persist a request

00:46:36,040 --> 00:46:42,230
that what do I reply one thing I could

00:46:38,690 --> 00:46:44,839
reply is okay got the request and then

00:46:42,230 --> 00:46:46,490
have the client or reply with let's say

00:46:44,839 --> 00:46:47,960
a UUID that I've generated in a

00:46:46,490 --> 00:46:50,450
distributed fashion no sequential

00:46:47,960 --> 00:46:52,580
numbers a hue idea I've generated to say

00:46:50,450 --> 00:46:54,200
ask me in a little while for the answer

00:46:52,580 --> 00:46:56,390
if the client is smart enough to say

00:46:54,200 --> 00:46:58,670
okay now I need the reply great come

00:46:56,390 --> 00:47:00,380
back so avoiding that synchronous loop

00:46:58,670 --> 00:47:02,210
of communications quite possible even

00:47:00,380 --> 00:47:03,950
with HTTP and of course there's always

00:47:02,210 --> 00:47:05,420
WebSockets to where you can keep the

00:47:03,950 --> 00:47:06,950
socket open but that has its own

00:47:05,420 --> 00:47:09,500
problems one of the favorite answers

00:47:06,950 --> 00:47:11,810
I've seen is ticket to 204 which has no

00:47:09,500 --> 00:47:13,970
content so I don't have an answer for

00:47:11,810 --> 00:47:16,369
you I got it I heard you thank you and

00:47:13,970 --> 00:47:18,080
then later you can say okay now what

00:47:16,369 --> 00:47:19,460
what's the answer you know that's a

00:47:18,080 --> 00:47:23,170
separate request from the client that's

00:47:19,460 --> 00:47:23,170
one favorite pattern there's many others

00:47:29,740 --> 00:47:36,049
plans to do rowdy right sharding now is

00:47:34,369 --> 00:47:39,230
gonna move it into application levels

00:47:36,049 --> 00:47:42,109
and especially which is we're just

00:47:39,230 --> 00:47:45,309
introducing it now into our system what

00:47:42,109 --> 00:47:48,589
are your thoughts about that as far as

00:47:45,309 --> 00:47:50,420
charting is a cost don't do it unless

00:47:48,589 --> 00:47:52,280
you have to make somebody back you into

00:47:50,420 --> 00:47:54,260
a corner before you pick that choice

00:47:52,280 --> 00:47:55,760
there is for example it's one of the

00:47:54,260 --> 00:47:57,920
reasons that cluster sharding requires

00:47:55,760 --> 00:48:00,260
active persistence is that there needs

00:47:57,920 --> 00:48:01,609
to be state stored in order to do it so

00:48:00,260 --> 00:48:03,140
you're actually adding a significant

00:48:01,609 --> 00:48:04,849
amount of cost and the reason you're

00:48:03,140 --> 00:48:08,359
doing it that compromise you're making

00:48:04,849 --> 00:48:10,369
is for consistency so if I could avoid

00:48:08,359 --> 00:48:12,619
consistency which sounds very strange

00:48:10,369 --> 00:48:14,960
please do so you're way better in terms

00:48:12,619 --> 00:48:16,220
of scalability so that's not more than

00:48:14,960 --> 00:48:19,010
my consistent system

00:48:16,220 --> 00:48:21,619
what's that virtual exactly yeah the

00:48:19,010 --> 00:48:23,359
more the larger eventually can be the

00:48:21,619 --> 00:48:24,829
more scalable you can be to be honest

00:48:23,359 --> 00:48:26,210
you know the larger the answer of what's

00:48:24,829 --> 00:48:27,770
eventually mean you know is it

00:48:26,210 --> 00:48:29,750
milliseconds you need to be pretty

00:48:27,770 --> 00:48:32,510
consistent pretty close to consistent if

00:48:29,750 --> 00:48:34,400
it's minutes okay I can scale pretty far

00:48:32,510 --> 00:48:36,650
then so that's the price you're paying

00:48:34,400 --> 00:48:38,329
basically so I always say our cluster

00:48:36,650 --> 00:48:40,369
and there's a great answer to a problem

00:48:38,329 --> 00:48:42,619
I hope you don't have now that cluster

00:48:40,369 --> 00:48:44,030
after cluster sharding yeah I can Buster

00:48:42,619 --> 00:48:45,200
actually is also a communication

00:48:44,030 --> 00:48:47,089
overhead would you have to be careful

00:48:45,200 --> 00:48:48,799
about right the boss of protocol isn't

00:48:47,089 --> 00:48:53,859
free either and eventually it will scale

00:48:48,799 --> 00:48:53,859
out well Stan

00:48:55,450 --> 00:49:01,410
like you earlier you mentioned that you

00:48:58,119 --> 00:49:06,430
should avoid sharing the database yes

00:49:01,410 --> 00:49:08,500
share some techniques yes absolutely

00:49:06,430 --> 00:49:09,940
it's actually the worst thing you can do

00:49:08,500 --> 00:49:12,700
for your scalability because like I said

00:49:09,940 --> 00:49:14,230
if your micro-services share your

00:49:12,700 --> 00:49:17,140
database if your database is your

00:49:14,230 --> 00:49:18,849
database is your monolith and the way to

00:49:17,140 --> 00:49:20,680
avoid that is essentially denormalized

00:49:18,849 --> 00:49:22,869
every service can have its own

00:49:20,680 --> 00:49:24,220
persistent store if it needs one it's

00:49:22,869 --> 00:49:26,290
the first question can I avoid one

00:49:24,220 --> 00:49:27,609
altogether great I went so I don't need

00:49:26,290 --> 00:49:30,160
to store anything that's a great service

00:49:27,609 --> 00:49:32,380
often I need to store something store it

00:49:30,160 --> 00:49:35,079
independently store it locally and store

00:49:32,380 --> 00:49:38,079
it isolated don't share it with anybody

00:49:35,079 --> 00:49:39,819
else and update my local state based on

00:49:38,079 --> 00:49:42,010
let's say event sourcing that's one way

00:49:39,819 --> 00:49:43,599
to have an event bus it says okay the

00:49:42,010 --> 00:49:46,000
customer just bought something therefore

00:49:43,599 --> 00:49:48,460
you know customer inventory just gets

00:49:46,000 --> 00:49:49,660
adjusted based on an event that ripples

00:49:48,460 --> 00:49:51,910
through the system those two systems

00:49:49,660 --> 00:49:53,950
don't even know each other exist I just

00:49:51,910 --> 00:49:55,900
I am in an event with the I'm just sayin

00:49:53,950 --> 00:49:57,700
pattern and then somebody picks it up

00:49:55,900 --> 00:49:59,859
and says I need to decrement the amount

00:49:57,700 --> 00:50:01,599
of of X available if those two could be

00:49:59,859 --> 00:50:03,910
completely independent you're better off

00:50:01,599 --> 00:50:05,260
you can't always do it you know it's

00:50:03,910 --> 00:50:07,240
again one of these trade offs but if you

00:50:05,260 --> 00:50:10,900
can that's the most common technique to

00:50:07,240 --> 00:50:14,079
do it it's CQRS and event sourcing that

00:50:10,900 --> 00:50:15,520
answer your question okay anybody else I

00:50:14,079 --> 00:50:16,660
can't actually see you back there so

00:50:15,520 --> 00:50:19,119
somebody's raising their hand in the

00:50:16,660 --> 00:50:20,530
back I won't build it up okay I think

00:50:19,119 --> 00:50:23,819
we're out of time in any case so thank

00:50:20,530 --> 00:50:23,819

YouTube URL: https://www.youtube.com/watch?v=ROQE4XrovP8


