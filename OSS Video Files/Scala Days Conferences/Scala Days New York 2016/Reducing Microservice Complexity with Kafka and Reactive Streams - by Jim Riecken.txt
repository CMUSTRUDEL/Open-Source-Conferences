Title: Reducing Microservice Complexity with Kafka and Reactive Streams - by Jim Riecken
Publication date: 2016-06-17
Playlist: Scala Days New York 2016
Description: 
	This talk was recorded at Scala Days New York, 2016. Follow along on Twitter @scaladays and on the website for more information http://scaladays.org/.

Abstract:
Transitioning from a monolithic application to a set of microservices can help increase performance and scalability, but it can also drastically increase complexity. Layers of inter-service network calls for add latency and an increasing risk of failure where previously only local function calls existed. In this talk, I'll speak about how to tame this complexity using Apache Kafka and Reactive Streams to:
- Extract non-critical processing from the critical path of your application to reduce request latency
- Provide back-pressure to handle both slow and fast producers/consumers
- Maintain high availability, high performance, and reliable messaging
- Evolve message payloads while maintaining backwards and forwards compatibility.
Captions: 
	00:00:00,930 --> 00:00:06,220
so my name is Jim Rican I'm a senior

00:00:04,450 --> 00:00:08,710
developer at HootSuite in Vancouver I

00:00:06,220 --> 00:00:10,570
work on our platform team we're

00:00:08,710 --> 00:00:13,030
responsible for our micro services

00:00:10,570 --> 00:00:14,500
architecture and a lot of our back-end

00:00:13,030 --> 00:00:18,699
services and infrastructure that powers

00:00:14,500 --> 00:00:20,769
the our application HootSuite is one of

00:00:18,699 --> 00:00:22,300
the most widely used platforms for

00:00:20,769 --> 00:00:24,749
managing social media that's used by

00:00:22,300 --> 00:00:27,789
over 10 million people across the world

00:00:24,749 --> 00:00:29,919
we have a web-based dashboard product as

00:00:27,789 --> 00:00:32,859
well as some mobile apps and you can

00:00:29,919 --> 00:00:36,460
send messages scheduled messages see

00:00:32,859 --> 00:00:38,020
messages and interact with them for the

00:00:36,460 --> 00:00:39,969
past couple years we've been working on

00:00:38,020 --> 00:00:42,280
transitioning from a massive PHP

00:00:39,969 --> 00:00:44,199
application to a set of Scalla based

00:00:42,280 --> 00:00:48,039
micro services to have a scale or

00:00:44,199 --> 00:00:50,649
application and our organization so

00:00:48,039 --> 00:00:52,059
today I want to talk about a bit about

00:00:50,649 --> 00:00:54,579
the transition from a monolithic

00:00:52,059 --> 00:00:56,469
application to a distributed system of

00:00:54,579 --> 00:00:58,719
micro services and some of the ways the

00:00:56,469 --> 00:01:01,839
complexity and sources of failure can be

00:00:58,719 --> 00:01:03,789
introduced when that happens I also want

00:01:01,839 --> 00:01:05,619
to talk about asynchronous messaging in

00:01:03,789 --> 00:01:07,420
specific publish/subscribe pattern and

00:01:05,619 --> 00:01:10,810
how it can help reduce some of that pain

00:01:07,420 --> 00:01:13,360
I want to show how Kafka which is an

00:01:10,810 --> 00:01:15,270
asynchronous pub/sub measuring system is

00:01:13,360 --> 00:01:17,560
a great foundation to do that and

00:01:15,270 --> 00:01:19,479
finally I'll talk about reactive streams

00:01:17,560 --> 00:01:21,250
and akka streams and how they can be

00:01:19,479 --> 00:01:26,079
used to easily work with Kafka in your

00:01:21,250 --> 00:01:29,380
Scalla code I'm not going to talk about

00:01:26,079 --> 00:01:33,090
how to set up a catholic cluster or give

00:01:29,380 --> 00:01:35,290
an in-depth tutorial on data streams i

00:01:33,090 --> 00:01:36,939
want to start off by showing some of the

00:01:35,290 --> 00:01:39,189
types of complexity and sources of

00:01:36,939 --> 00:01:40,659
errors that are often encountered when

00:01:39,189 --> 00:01:44,020
breaking up a monolith into a set of

00:01:40,659 --> 00:01:45,460
small maker services so most

00:01:44,020 --> 00:01:47,860
applications start off as a monolith

00:01:45,460 --> 00:01:49,509
what do I mean by a monolith so it's

00:01:47,860 --> 00:01:51,729
usually a single codebase that the

00:01:49,509 --> 00:01:53,229
entire development team works on is

00:01:51,729 --> 00:01:55,060
usually a single source of truth or a

00:01:53,229 --> 00:01:57,609
single database in a single deploy

00:01:55,060 --> 00:01:59,710
process sort of one build pipeline one

00:01:57,609 --> 00:02:01,840
places where changes are waiting to be

00:01:59,710 --> 00:02:03,729
deployed this is usually great for a

00:02:01,840 --> 00:02:05,469
small team there's not a lot of moving

00:02:03,729 --> 00:02:07,509
parts it's really easy to iterate and

00:02:05,469 --> 00:02:08,979
make atomic changes to the entire code

00:02:07,509 --> 00:02:10,750
base if you want to change an API you

00:02:08,979 --> 00:02:11,840
can just search and find all the usages

00:02:10,750 --> 00:02:14,220
of it

00:02:11,840 --> 00:02:16,800
and everything's a local metric Holloway

00:02:14,220 --> 00:02:19,730
or call to a central database everything

00:02:16,800 --> 00:02:22,080
sort of stays in sync real easily

00:02:19,730 --> 00:02:23,970
however a lot of the benefits of

00:02:22,080 --> 00:02:25,709
monoliths become drawbacks as the size

00:02:23,970 --> 00:02:26,970
of the development team increases and

00:02:25,709 --> 00:02:29,430
the application becomes larger and

00:02:26,970 --> 00:02:32,520
larger the efficiency server drops off

00:02:29,430 --> 00:02:33,959
after that initial spike as the

00:02:32,520 --> 00:02:35,850
application gets larger and larger the

00:02:33,959 --> 00:02:38,100
build times get longer and longer tests

00:02:35,850 --> 00:02:40,530
take longer and longer and one developer

00:02:38,100 --> 00:02:42,959
can break the building cause the entire

00:02:40,530 --> 00:02:44,580
team to have to wait there also needs to

00:02:42,959 --> 00:02:46,050
be more coordination between development

00:02:44,580 --> 00:02:48,239
teams to ensure that the application can

00:02:46,050 --> 00:02:49,530
always be deployed for example some

00:02:48,239 --> 00:02:51,780
teams may need to wait until Liars

00:02:49,530 --> 00:02:53,690
others finish their functionality before

00:02:51,780 --> 00:02:55,739
they can actually deploy their code

00:02:53,690 --> 00:02:57,750
things like feature flagging can help

00:02:55,739 --> 00:03:00,450
with this but if you get a lot of flags

00:02:57,750 --> 00:03:02,190
can become really complicated also the

00:03:00,450 --> 00:03:04,110
entire application needs to be scaled up

00:03:02,190 --> 00:03:06,390
because of small highly used feature

00:03:04,110 --> 00:03:09,060
often can take a lot more resources and

00:03:06,390 --> 00:03:11,489
money it can also be hard or impossible

00:03:09,060 --> 00:03:13,110
to try out new technologies or new data

00:03:11,489 --> 00:03:17,850
stores because your application is this

00:03:13,110 --> 00:03:19,320
sort of mono mono with one obvious

00:03:17,850 --> 00:03:21,120
solution is to store to start pulling

00:03:19,320 --> 00:03:22,920
independent small services out of the

00:03:21,120 --> 00:03:24,810
monolith you sort of decrease the size

00:03:22,920 --> 00:03:26,250
of the monolith and and you allow the

00:03:24,810 --> 00:03:29,299
teams to work separately from each other

00:03:26,250 --> 00:03:31,170
and communicating over API contracts

00:03:29,299 --> 00:03:33,000
eventually you get enough services

00:03:31,170 --> 00:03:34,980
pulled out that the monolith is kind of

00:03:33,000 --> 00:03:36,989
gone it sort of becomes a facade where

00:03:34,980 --> 00:03:38,250
it just calls the backend services and

00:03:36,989 --> 00:03:41,670
aggregates the data that the client

00:03:38,250 --> 00:03:43,320
needs and by micro services I mean a

00:03:41,670 --> 00:03:45,360
small service that's as has an

00:03:43,320 --> 00:03:46,650
independent codebase it's deployed

00:03:45,360 --> 00:03:49,170
independently of others

00:03:46,650 --> 00:03:51,299
he owns its own data it kind of only

00:03:49,170 --> 00:03:52,920
does one thing and it's owned by a

00:03:51,299 --> 00:03:54,720
single small team that's empowered to

00:03:52,920 --> 00:03:57,239
make changes whenever they need to to

00:03:54,720 --> 00:03:59,370
the application and it communicates with

00:03:57,239 --> 00:04:01,290
the other services via well-defined API

00:03:59,370 --> 00:04:05,160
it could be rest over HTTP could be

00:04:01,290 --> 00:04:07,590
thrift could be G RPC micro services

00:04:05,160 --> 00:04:09,420
come with a lot of benefits since

00:04:07,590 --> 00:04:10,829
they're small and encapsulate one thing

00:04:09,420 --> 00:04:13,079
the build times are short the test times

00:04:10,829 --> 00:04:14,730
are short each service can be deployed

00:04:13,079 --> 00:04:17,130
and scaled independently of the other

00:04:14,730 --> 00:04:19,410
services it can save a lot of money as

00:04:17,130 --> 00:04:21,180
compared to with the monolith each

00:04:19,410 --> 00:04:23,430
service can use whatever technology is

00:04:21,180 --> 00:04:25,110
appropriate for it for example an

00:04:23,430 --> 00:04:25,720
analytic service might have a lot

00:04:25,110 --> 00:04:27,070
different data

00:04:25,720 --> 00:04:30,010
requirements and a user management

00:04:27,070 --> 00:04:32,530
service now it's also easy to create new

00:04:30,010 --> 00:04:34,350
services because they're small it can

00:04:32,530 --> 00:04:36,580
help spur innovation in the organization

00:04:34,350 --> 00:04:38,920
also each service has a clear owner

00:04:36,580 --> 00:04:41,110
who's responsible for all aspects of the

00:04:38,920 --> 00:04:43,270
service with a monolith there's not

00:04:41,110 --> 00:04:44,710
often clear owners and you can sort of

00:04:43,270 --> 00:04:49,990
sometimes get one person that gets

00:04:44,710 --> 00:04:52,390
overburdened with all that work micro

00:04:49,990 --> 00:04:53,800
services aren't a so devote though with

00:04:52,390 --> 00:04:55,240
the introduction of services the

00:04:53,800 --> 00:04:58,620
application now becomes a distributed

00:04:55,240 --> 00:05:01,060
system you can network calls everywhere

00:04:58,620 --> 00:05:03,160
get latency all the calls between

00:05:01,060 --> 00:05:05,800
services or remote which adds additional

00:05:03,160 --> 00:05:07,600
latency there's a lot higher chance of

00:05:05,800 --> 00:05:09,340
failure so any service or network

00:05:07,600 --> 00:05:10,750
component can fail at any time the

00:05:09,340 --> 00:05:13,030
system has to be architected with this

00:05:10,750 --> 00:05:15,220
in mind and the failure of a downstream

00:05:13,030 --> 00:05:17,140
components say this very last box can

00:05:15,220 --> 00:05:21,490
cause cascading failure to cause failure

00:05:17,140 --> 00:05:23,560
all the way up to the top level also the

00:05:21,490 --> 00:05:25,300
uptime of the entire service now becomes

00:05:23,560 --> 00:05:27,610
the combined uptime of all the services

00:05:25,300 --> 00:05:29,770
in the critical path for example save

00:05:27,610 --> 00:05:32,650
you for services that each have 99.9%

00:05:29,770 --> 00:05:34,810
uptime which is pretty good the overall

00:05:32,650 --> 00:05:36,790
uptime of this whole chain is only 99.5

00:05:34,810 --> 00:05:37,990
percent which is not that good it's sort

00:05:36,790 --> 00:05:39,430
of the difference between like half an

00:05:37,990 --> 00:05:43,570
hour of downtime versus three and a half

00:05:39,430 --> 00:05:45,340
hours of downtime in a month you also

00:05:43,570 --> 00:05:46,660
need a lot more coordination since

00:05:45,340 --> 00:05:48,400
there's no longer a single source of

00:05:46,660 --> 00:05:50,979
truth it becomes necessary for

00:05:48,400 --> 00:05:52,390
coordination between these services for

00:05:50,979 --> 00:05:54,190
example when a user's deleted from the

00:05:52,390 --> 00:05:55,750
system you might need to clean up all

00:05:54,190 --> 00:05:57,790
their information you don't really want

00:05:55,750 --> 00:05:59,500
to be doing this synchronously or having

00:05:57,790 --> 00:06:00,400
the user management system need to know

00:05:59,500 --> 00:06:03,220
about every other part of the

00:06:00,400 --> 00:06:05,830
application you also need more

00:06:03,220 --> 00:06:07,570
coordination between teams cross-cutting

00:06:05,830 --> 00:06:10,540
changes can't be made atomically any

00:06:07,570 --> 00:06:11,919
more API changes need to be carefully

00:06:10,540 --> 00:06:13,330
designed so that they're backwards

00:06:11,919 --> 00:06:14,710
compatible or the API needs to be

00:06:13,330 --> 00:06:18,490
version and it can slow down the speed

00:06:14,710 --> 00:06:20,080
of changes and if you're not careful the

00:06:18,490 --> 00:06:21,610
overall system can end up tightly

00:06:20,080 --> 00:06:23,740
coupled and starting to look like

00:06:21,610 --> 00:06:25,180
spaghetti and having each request pass

00:06:23,740 --> 00:06:27,040
through tons and tons of other services

00:06:25,180 --> 00:06:30,040
similar table monolith can sort of

00:06:27,040 --> 00:06:31,870
become a little tangle together but

00:06:30,040 --> 00:06:34,120
usually not all these calls necessarily

00:06:31,870 --> 00:06:36,070
need to be part of the critical path to

00:06:34,120 --> 00:06:37,539
complete the user's operation for

00:06:36,070 --> 00:06:38,979
example like deleting a user you don't

00:06:37,539 --> 00:06:39,830
really need to delete all their data

00:06:38,979 --> 00:06:41,900
synchronous

00:06:39,830 --> 00:06:42,860
also sending a message to a social

00:06:41,900 --> 00:06:44,360
network it doesn't really need to

00:06:42,860 --> 00:06:48,350
archive a message or perform any

00:06:44,360 --> 00:06:50,360
analytics on it so a good way to reduce

00:06:48,350 --> 00:06:52,040
the complexity and lower the chance of

00:06:50,360 --> 00:06:53,180
failure of the overall request is sort

00:06:52,040 --> 00:06:55,460
of by pulling out all the non-essential

00:06:53,180 --> 00:06:57,669
API calls from being synchronous and

00:06:55,460 --> 00:06:59,990
blocking or synchronous and non blocking

00:06:57,669 --> 00:07:02,330
for the request to be asynchronous so

00:06:59,990 --> 00:07:06,200
there's fewer paths fewer calls in the

00:07:02,330 --> 00:07:07,639
path it becomes more reliable rather

00:07:06,200 --> 00:07:10,220
than having everything being done over

00:07:07,639 --> 00:07:11,840
API calls we can simply send the

00:07:10,220 --> 00:07:13,850
messages to an asynchronous message bus

00:07:11,840 --> 00:07:15,680
service can fire-and-forget and message

00:07:13,850 --> 00:07:17,479
into the bus any number of other

00:07:15,680 --> 00:07:19,340
services cannot consume this information

00:07:17,479 --> 00:07:23,479
at their own pace outside of the scope

00:07:19,340 --> 00:07:25,490
of the users request so obvious

00:07:23,479 --> 00:07:26,530
stability reasons aside why would you

00:07:25,490 --> 00:07:28,729
want to do this

00:07:26,530 --> 00:07:30,530
it has decoupling it completely

00:07:28,729 --> 00:07:32,510
decouples the producers from the

00:07:30,530 --> 00:07:34,190
consumers of the information for example

00:07:32,510 --> 00:07:35,960
the service that needs to process tweets

00:07:34,190 --> 00:07:37,550
from the Twitter firehose doesn't really

00:07:35,960 --> 00:07:40,160
need to know about the analytic system

00:07:37,550 --> 00:07:41,360
or a push notification system the

00:07:40,160 --> 00:07:42,889
point-to-point messaging becomes

00:07:41,360 --> 00:07:46,010
publish/subscribe one to one becomes

00:07:42,889 --> 00:07:48,350
one-to-many there's less coordination

00:07:46,010 --> 00:07:50,570
aside from agreeing on the message

00:07:48,350 --> 00:07:52,490
format this little no coordination that

00:07:50,570 --> 00:07:55,550
has to happen between a producer of

00:07:52,490 --> 00:07:56,750
information and a consumer of it once

00:07:55,550 --> 00:07:58,789
something's producing data to the

00:07:56,750 --> 00:08:00,289
message bus adding additional consumers

00:07:58,789 --> 00:08:01,940
to that information doesn't affect the

00:08:00,289 --> 00:08:04,460
producing service at all so it's really

00:08:01,940 --> 00:08:06,080
easy it can also help scale your

00:08:04,460 --> 00:08:08,030
organization as there's less

00:08:06,080 --> 00:08:09,590
back-and-forth necessary between teams a

00:08:08,030 --> 00:08:10,880
team doesn't even need to know who's

00:08:09,590 --> 00:08:12,470
consuming their messages as long as

00:08:10,880 --> 00:08:17,660
they're publishing sort of the format of

00:08:12,470 --> 00:08:19,070
the message so what kind of requirements

00:08:17,660 --> 00:08:21,530
would be want in make synchronous

00:08:19,070 --> 00:08:23,210
messaging solution it needs to have

00:08:21,530 --> 00:08:25,490
well-defined delivery semantics for

00:08:23,210 --> 00:08:27,830
example it needs to know if it's at

00:08:25,490 --> 00:08:29,479
least once or at most ones or exactly

00:08:27,830 --> 00:08:31,220
once because with at least once you have

00:08:29,479 --> 00:08:33,289
to deal with duplicates with the most

00:08:31,220 --> 00:08:35,539
once or with it most once you have to

00:08:33,289 --> 00:08:38,390
deal with lost messages and exactly once

00:08:35,539 --> 00:08:41,270
is really hard to do you want it to be

00:08:38,390 --> 00:08:43,390
high throughput we don't want to block

00:08:41,270 --> 00:08:45,860
colors we want the messages to send fast

00:08:43,390 --> 00:08:47,720
we want to be highly available we don't

00:08:45,860 --> 00:08:49,990
want a part of the messaging system that

00:08:47,720 --> 00:08:52,360
breaks to stop it from working

00:08:49,990 --> 00:08:53,500
we also want to have durability but we

00:08:52,360 --> 00:08:55,270
don't want a failure of part of the

00:08:53,500 --> 00:08:59,140
system or the entire system to cause

00:08:55,270 --> 00:09:00,400
data loss we want it to be scalable we

00:08:59,140 --> 00:09:03,340
want to be able to grow the capacity of

00:09:00,400 --> 00:09:04,990
the system over time and really

00:09:03,340 --> 00:09:07,720
important we want that pressure we want

00:09:04,990 --> 00:09:09,250
to be able to have a slow consumer to be

00:09:07,720 --> 00:09:12,550
able to handle data coming from a fast

00:09:09,250 --> 00:09:15,130
producer there's a lot of different

00:09:12,550 --> 00:09:16,630
messaging systems out there and the one

00:09:15,130 --> 00:09:20,650
that I found is really good it's called

00:09:16,630 --> 00:09:23,590
Kafka so what is Kafka some of you may

00:09:20,650 --> 00:09:25,420
have heard of it before I use it it's a

00:09:23,590 --> 00:09:27,780
distributed partitioned replicated

00:09:25,420 --> 00:09:29,920
commit log service that's a lot of words

00:09:27,780 --> 00:09:32,860
but sort of like commit log think of how

00:09:29,920 --> 00:09:35,440
a database works with it's right ahead

00:09:32,860 --> 00:09:38,010
log is append only it's a linear reads

00:09:35,440 --> 00:09:41,260
and writes which is really fast

00:09:38,010 --> 00:09:44,050
it has pub/sub messaging functionality

00:09:41,260 --> 00:09:45,880
one-to-many messaging it was created by

00:09:44,050 --> 00:09:48,100
LinkedIn but now it's a top-level Apache

00:09:45,880 --> 00:09:49,630
open-source project and it has a company

00:09:48,100 --> 00:09:53,590
that manages it I think it's called

00:09:49,630 --> 00:09:55,090
confluent so a catholic cluster

00:09:53,590 --> 00:09:57,850
maintains feeds and messages called

00:09:55,090 --> 00:09:59,860
topics entities that send messages into

00:09:57,850 --> 00:10:01,150
topics are called producers and the

00:09:59,860 --> 00:10:05,500
entities that read the messages are

00:10:01,150 --> 00:10:07,090
called consumers caf-co topics are sort

00:10:05,500 --> 00:10:09,040
of a logical grouping of one or more

00:10:07,090 --> 00:10:10,480
individual commit logs called partitions

00:10:09,040 --> 00:10:13,510
for example this topic has three

00:10:10,480 --> 00:10:14,890
partitions and each partition is an

00:10:13,510 --> 00:10:17,200
ordered immutable sequence of messages

00:10:14,890 --> 00:10:19,660
this means that if the topic has many

00:10:17,200 --> 00:10:20,950
partitions there's no total order of the

00:10:19,660 --> 00:10:23,530
messages there's only an order per

00:10:20,950 --> 00:10:25,390
partition the messages are continually

00:10:23,530 --> 00:10:27,870
added to the end of the log and each

00:10:25,390 --> 00:10:30,250
message has a numeric position offset

00:10:27,870 --> 00:10:31,990
the messages are persisted in the log

00:10:30,250 --> 00:10:34,390
for a configured amount of time whether

00:10:31,990 --> 00:10:36,400
they've been consumed or not typical

00:10:34,390 --> 00:10:38,620
retention periods are usually around an

00:10:36,400 --> 00:10:40,300
hour to a week depending on the volume

00:10:38,620 --> 00:10:43,120
of data I think the cat could default is

00:10:40,300 --> 00:10:44,860
two weeks we use this a to tweet for two

00:10:43,120 --> 00:10:46,210
weeks except for a few situations where

00:10:44,860 --> 00:10:48,010
we lower the retention to be smaller

00:10:46,210 --> 00:10:50,530
because we have a very high volume topic

00:10:48,010 --> 00:10:51,970
and usually you start with just a few

00:10:50,530 --> 00:10:56,350
partitions and then you can add them as

00:10:51,970 --> 00:10:58,180
more parallelism is necessary producers

00:10:56,350 --> 00:11:00,520
as their name implies send messages to

00:10:58,180 --> 00:11:02,650
partitions and topics they're

00:11:00,520 --> 00:11:03,710
responsible for choosing which partition

00:11:02,650 --> 00:11:05,970
to send the message to

00:11:03,710 --> 00:11:07,980
so there's a couple different strategies

00:11:05,970 --> 00:11:09,780
you can choose messages can be round

00:11:07,980 --> 00:11:11,490
robins over all the partitions which

00:11:09,780 --> 00:11:14,160
letter puts an even amount of load on

00:11:11,490 --> 00:11:16,140
each partition which is kind of fair the

00:11:14,160 --> 00:11:17,550
producer can also consistently hash the

00:11:16,140 --> 00:11:19,740
messages based on a message key you

00:11:17,550 --> 00:11:21,450
provide and this ensures that as long as

00:11:19,740 --> 00:11:23,640
a message has the same key it will

00:11:21,450 --> 00:11:25,110
always go to the same partition so you

00:11:23,640 --> 00:11:27,030
can't if you can't get a total order

00:11:25,110 --> 00:11:29,040
over the whole topic you can at least

00:11:27,030 --> 00:11:32,460
get an order for the individual message

00:11:29,040 --> 00:11:34,050
the individual keys this can cause an

00:11:32,460 --> 00:11:35,460
unbalance in messages sometimes if one

00:11:34,050 --> 00:11:37,350
message key is busier than the others

00:11:35,460 --> 00:11:40,560
say you have a really really popular

00:11:37,350 --> 00:11:44,670
user on your system its messages will

00:11:40,560 --> 00:11:46,680
always get hash to the same partition so

00:11:44,670 --> 00:11:48,810
consumers as opposed to many other

00:11:46,680 --> 00:11:50,550
messaging systems backcap good consumers

00:11:48,810 --> 00:11:53,280
pull data from the purchases of a topic

00:11:50,550 --> 00:11:54,960
it's not pushed to them and all they

00:11:53,280 --> 00:11:57,180
track is their position in each

00:11:54,960 --> 00:11:58,530
partition this makes them extremely

00:11:57,180 --> 00:12:03,060
cheap they just have to store a couple

00:11:58,530 --> 00:12:05,580
numbers consumers are tagged with the

00:12:03,060 --> 00:12:07,800
group ID which groups all consumers with

00:12:05,580 --> 00:12:10,530
the same ID into a logical group and

00:12:07,800 --> 00:12:13,290
each partition is assigned to only one

00:12:10,530 --> 00:12:15,210
consumer in a consumer group so if all

00:12:13,290 --> 00:12:16,560
consumers have the same group ID you

00:12:15,210 --> 00:12:17,970
start to get a queueing message model

00:12:16,560 --> 00:12:20,940
where the message will be sort of

00:12:17,970 --> 00:12:22,500
round-robin between all the entities if

00:12:20,940 --> 00:12:24,030
they all have different group IDs you

00:12:22,500 --> 00:12:26,820
sort of get a pub sub like topics where

00:12:24,030 --> 00:12:28,380
everything gets all the messages the

00:12:26,820 --> 00:12:30,570
number of partitions in a topic should

00:12:28,380 --> 00:12:33,030
be more large enough so that they're at

00:12:30,570 --> 00:12:35,100
least as many top partitions as there

00:12:33,030 --> 00:12:36,330
are consumers in the group otherwise

00:12:35,100 --> 00:12:37,500
some consumers will be sitting there

00:12:36,330 --> 00:12:39,900
doing nothing until one of the other

00:12:37,500 --> 00:12:42,060
consumer stops so for example in group 2

00:12:39,900 --> 00:12:44,010
here consumer six is doing nothing

00:12:42,060 --> 00:12:46,350
because there's four consumers in that

00:12:44,010 --> 00:12:48,540
group but only three partitions but for

00:12:46,350 --> 00:12:50,610
example if consumer three died consumer

00:12:48,540 --> 00:12:53,760
six would get assigned the partition

00:12:50,610 --> 00:12:55,890
that it had at boot sweet we typically

00:12:53,760 --> 00:12:57,990
have a single consumer group per service

00:12:55,890 --> 00:12:59,490
and then we scale up and down the

00:12:57,990 --> 00:13:02,520
service knowing that each message will

00:12:59,490 --> 00:13:07,530
only be delivered once by one of the

00:13:02,520 --> 00:13:09,000
server workers in that group so now that

00:13:07,530 --> 00:13:10,740
I sort of describe the main components

00:13:09,000 --> 00:13:13,410
of kafka doesn't meet those requirements

00:13:10,740 --> 00:13:16,789
that I talked about a bit earlier for

00:13:13,410 --> 00:13:20,339
our good messaging system

00:13:16,789 --> 00:13:21,749
cap is really fast it can handle

00:13:20,339 --> 00:13:23,339
hundreds of megabytes per second of

00:13:21,749 --> 00:13:26,219
reads and writes over thousands of

00:13:23,339 --> 00:13:28,079
comparing clients it uses immutable log

00:13:26,219 --> 00:13:30,539
files so it's like an O one operation to

00:13:28,079 --> 00:13:32,309
upend it it also uses on the central

00:13:30,539 --> 00:13:33,899
system call which lets it send data

00:13:32,309 --> 00:13:35,339
directly from the operating system page

00:13:33,899 --> 00:13:36,749
cache to the network card it never

00:13:35,339 --> 00:13:38,549
actually goes into the application

00:13:36,749 --> 00:13:40,289
memory and eases the same data

00:13:38,549 --> 00:13:42,899
structures for in memory and on disk so

00:13:40,289 --> 00:13:44,759
that's really efficient iso at the

00:13:42,899 --> 00:13:46,619
high-end linkedin published a blog post

00:13:44,759 --> 00:13:50,189
last year where they said they'd send

00:13:46,619 --> 00:13:53,189
800 billion messages per day 100 like 18

00:13:50,189 --> 00:13:55,769
million messages per second at peak and

00:13:53,189 --> 00:13:57,299
175 terabytes of data per day over a

00:13:55,769 --> 00:13:59,759
thousand servers so it scales really

00:13:57,299 --> 00:14:01,169
well at HootSuite we pushed hundreds of

00:13:59,759 --> 00:14:04,949
gigabytes per day so we're not really at

00:14:01,169 --> 00:14:07,679
that scale yet taffeta deals really well

00:14:04,949 --> 00:14:10,139
with failure of its components for the

00:14:07,679 --> 00:14:12,539
brokers all messages are persistent on

00:14:10,139 --> 00:14:14,369
disk so if a broker crashes it just

00:14:12,539 --> 00:14:15,689
comes back up with no loss of data and

00:14:14,369 --> 00:14:19,169
it sort of syncs with the other brokers

00:14:15,689 --> 00:14:20,939
with what it missed the partitions of a

00:14:19,169 --> 00:14:23,549
topic are replicated across the cluster

00:14:20,939 --> 00:14:25,319
the number of replicas that a topic has

00:14:23,549 --> 00:14:27,629
is configurable but it should usually

00:14:25,319 --> 00:14:29,819
always be at least two if the broker

00:14:27,629 --> 00:14:31,319
crashes one of the other nodes will take

00:14:29,819 --> 00:14:33,179
over as a leader for the partition and

00:14:31,319 --> 00:14:34,529
sort of the producers and consumers will

00:14:33,179 --> 00:14:38,309
switch over to using that new server

00:14:34,529 --> 00:14:40,319
automatically for the consumers if they

00:14:38,309 --> 00:14:41,759
stop working they just start where they

00:14:40,319 --> 00:14:44,549
stopped off because they store that

00:14:41,759 --> 00:14:46,289
position in the log if the consumer is

00:14:44,549 --> 00:14:48,089
part of a group the partitions that it

00:14:46,289 --> 00:14:50,059
had will get reassigned to another

00:14:48,089 --> 00:14:53,339
consumer in the group that's still alive

00:14:50,059 --> 00:14:56,459
and producers producers can be

00:14:53,339 --> 00:14:58,349
configured to retry on failure and this

00:14:56,459 --> 00:15:00,359
changes it sort of in at least once

00:14:58,349 --> 00:15:02,399
messaging semantics where it's sending

00:15:00,359 --> 00:15:05,489
duplicates but you get you make sure the

00:15:02,399 --> 00:15:07,439
message gets sent a hoot sweet we've had

00:15:05,489 --> 00:15:09,479
brokers fail with the zero effect on the

00:15:07,439 --> 00:15:10,709
producers and consumers the producer

00:15:09,479 --> 00:15:12,799
some necks have to retry sending a

00:15:10,709 --> 00:15:16,289
message but the consumers just basically

00:15:12,799 --> 00:15:18,839
change to use the new broker that takes

00:15:16,289 --> 00:15:20,129
takes charge of the topics this is also

00:15:18,839 --> 00:15:22,109
really good when we deploy new code

00:15:20,129 --> 00:15:23,999
because the rolling restarts of a

00:15:22,109 --> 00:15:26,039
consumer results in no loss messages

00:15:23,999 --> 00:15:29,629
because the consumers started to start

00:15:26,039 --> 00:15:29,629
where they left off when they restart

00:15:30,290 --> 00:15:34,310
it's really easy to scale Kafka you can

00:15:32,630 --> 00:15:36,920
see in those numbers from LinkedIn it

00:15:34,310 --> 00:15:38,360
skills do very large sizes additional

00:15:36,920 --> 00:15:41,300
capacity can be added to the cluster

00:15:38,360 --> 00:15:42,620
runtime with zero downtime you just need

00:15:41,300 --> 00:15:44,000
to add more servers and then you can

00:15:42,620 --> 00:15:46,000
assign partitions on to those new

00:15:44,000 --> 00:15:47,750
servers with a command line tool and

00:15:46,000 --> 00:15:50,260
effectively adding more servers just

00:15:47,750 --> 00:15:54,110
adds more disk space to the cluster and

00:15:50,260 --> 00:15:55,730
since partitions can be stored topic or

00:15:54,110 --> 00:15:57,200
stored on different servers the size of

00:15:55,730 --> 00:15:59,090
its single topic can actually be larger

00:15:57,200 --> 00:16:02,360
than any single server can hold which is

00:15:59,090 --> 00:16:03,920
kind of nice and you can add more

00:16:02,360 --> 00:16:05,570
partitions at one time to get more

00:16:03,920 --> 00:16:07,670
parallelism so you have more consumers

00:16:05,570 --> 00:16:12,350
you can just add more partitions and

00:16:07,670 --> 00:16:14,210
they'll be able to consume Kathy also

00:16:12,350 --> 00:16:17,120
helps introduce backpressure into your

00:16:14,210 --> 00:16:19,430
system disk is really cheap so it can

00:16:17,120 --> 00:16:23,180
allow an almost unlimited data sink for

00:16:19,430 --> 00:16:24,500
messages the per topic retention period

00:16:23,180 --> 00:16:26,390
sort of acts as a service level

00:16:24,500 --> 00:16:28,280
agreement for the consumers the consumer

00:16:26,390 --> 00:16:30,550
has to pull the messages within that

00:16:28,280 --> 00:16:33,080
period of time to avoid losing anything

00:16:30,550 --> 00:16:34,420
but if if it does during that period of

00:16:33,080 --> 00:16:36,650
time it won't lose anything

00:16:34,420 --> 00:16:38,900
this way it's sort of almost impossible

00:16:36,650 --> 00:16:41,690
for a fast producer to overload a slow

00:16:38,900 --> 00:16:43,100
consumer and this helps enable the

00:16:41,690 --> 00:16:45,920
creation of real-time as well as batch

00:16:43,100 --> 00:16:47,840
processing that works effectively so you

00:16:45,920 --> 00:16:49,880
can have a process that runs once a day

00:16:47,840 --> 00:16:51,140
it just gets all the messages that

00:16:49,880 --> 00:16:52,700
happen during that day as long as the

00:16:51,140 --> 00:16:57,080
retention period on the topic is longer

00:16:52,700 --> 00:16:58,790
than that I want to make a quick aside

00:16:57,080 --> 00:17:01,820
about the data format and messages that

00:16:58,790 --> 00:17:02,960
you would send into Kafka sort of in

00:17:01,820 --> 00:17:05,089
order for different parts of your system

00:17:02,960 --> 00:17:06,860
to take full advantage of the message

00:17:05,089 --> 00:17:08,750
bus they need to know what is in the

00:17:06,860 --> 00:17:12,160
message and be able to sort of react to

00:17:08,750 --> 00:17:15,260
changes in the message without exploding

00:17:12,160 --> 00:17:18,230
calf it only deals with byte arrays in

00:17:15,260 --> 00:17:20,600
its data format the messages are byte

00:17:18,230 --> 00:17:21,920
arrays the keys or byte arrays you need

00:17:20,600 --> 00:17:23,180
to choose a message format that can be

00:17:21,920 --> 00:17:26,000
understood by both producers and

00:17:23,180 --> 00:17:27,470
consumers it's even more important if

00:17:26,000 --> 00:17:28,640
you're sort of in a polyglot environment

00:17:27,470 --> 00:17:30,920
where you're dealing with multiple

00:17:28,640 --> 00:17:32,600
languages where message is produced in

00:17:30,920 --> 00:17:35,330
one language need to be consumed by

00:17:32,600 --> 00:17:37,460
consumers in another language so there

00:17:35,330 --> 00:17:40,970
are a few options sort of that naive

00:17:37,460 --> 00:17:42,410
option would be to use serialization but

00:17:40,970 --> 00:17:42,700
serialization has a lot of issues with

00:17:42,410 --> 00:17:45,050
it

00:17:42,700 --> 00:17:46,910
including being ridiculously easy to

00:17:45,050 --> 00:17:49,430
create backwards incompatible changes

00:17:46,910 --> 00:17:52,970
even minor language versions introduced

00:17:49,430 --> 00:17:55,190
non binary compatible changes can use a

00:17:52,970 --> 00:17:55,610
text format like JSON which is a good

00:17:55,190 --> 00:17:57,770
choice

00:17:55,610 --> 00:18:01,280
everything in the universe can read and

00:17:57,770 --> 00:18:03,050
write a song without too much effort the

00:18:01,280 --> 00:18:04,760
only issue is that it's kind of large

00:18:03,050 --> 00:18:08,240
and relatively slow to parts compared to

00:18:04,760 --> 00:18:09,530
a binary format at HootSuite we found

00:18:08,240 --> 00:18:12,650
the best format for us is to use a

00:18:09,530 --> 00:18:15,080
protocol buffers which are a binary data

00:18:12,650 --> 00:18:16,550
format that was developed at Google it's

00:18:15,080 --> 00:18:19,430
really fast it serializing and

00:18:16,550 --> 00:18:21,650
deserializing data it uses an interface

00:18:19,430 --> 00:18:22,970
description language to describe the

00:18:21,650 --> 00:18:26,210
message format which you can then

00:18:22,970 --> 00:18:28,490
compile into code that runs in a many

00:18:26,210 --> 00:18:30,530
languages it's also easy to make

00:18:28,490 --> 00:18:32,120
backwards compatible changes if you

00:18:30,530 --> 00:18:36,440
follow just a few simple rules that they

00:18:32,120 --> 00:18:37,970
outline on their documentation there's

00:18:36,440 --> 00:18:39,980
one caveat with the protocol buffers

00:18:37,970 --> 00:18:41,840
though is that they're untyped when

00:18:39,980 --> 00:18:43,490
they're serialized so you need to sort

00:18:41,840 --> 00:18:45,260
of pass type information along with your

00:18:43,490 --> 00:18:47,830
messages so that the consumers know

00:18:45,260 --> 00:18:50,870
which protobuf to use to deserialize it

00:18:47,830 --> 00:18:53,180
in our case we sort of associated with

00:18:50,870 --> 00:18:55,190
each message type append that UUID to

00:18:53,180 --> 00:18:57,530
our messages and then use a mapping that

00:18:55,190 --> 00:18:58,940
we generate in our source code that

00:18:57,530 --> 00:19:02,830
map's between the two and lets us

00:18:58,940 --> 00:19:02,830
automatically deserialize the messages

00:19:02,890 --> 00:19:07,610
so i want to switch gears a little bit

00:19:05,030 --> 00:19:09,320
now so far i've talked about the

00:19:07,610 --> 00:19:11,480
problems that can arise with migration

00:19:09,320 --> 00:19:12,860
for monoliths and microservices and sort

00:19:11,480 --> 00:19:15,350
of how a message bus and calf get in

00:19:12,860 --> 00:19:17,810
specific can help with that that sort of

00:19:15,350 --> 00:19:19,400
the macro-level now I want to sort of

00:19:17,810 --> 00:19:20,990
focus more on the micro level actually

00:19:19,400 --> 00:19:24,020
interacting with streams of data from

00:19:20,990 --> 00:19:25,340
Kefka and if you're not careful code

00:19:24,020 --> 00:19:26,720
within your single service can come

00:19:25,340 --> 00:19:28,370
become complicated and prone to

00:19:26,720 --> 00:19:30,890
unbounded memory usage when dealing with

00:19:28,370 --> 00:19:32,690
streaming data so I want to talk about

00:19:30,890 --> 00:19:34,840
reactive streams akka streams and how

00:19:32,690 --> 00:19:40,040
they can be used to simply and safely

00:19:34,840 --> 00:19:42,530
interact with Kafka so reactive streams

00:19:40,040 --> 00:19:45,230
isn't it an initiative initiative for

00:19:42,530 --> 00:19:46,610
providing standard asynchronous stream

00:19:45,230 --> 00:19:48,350
processing with non blocking back

00:19:46,610 --> 00:19:50,150
pressure and that non blocking back

00:19:48,350 --> 00:19:52,040
pressure is really key

00:19:50,150 --> 00:19:53,630
many stream processing systems don't

00:19:52,040 --> 00:19:55,370
allow a consumer to push back on a

00:19:53,630 --> 00:19:56,830
producer that's producing too quickly

00:19:55,370 --> 00:19:58,700
for it to

00:19:56,830 --> 00:20:00,919
reactive streams have an explicit

00:19:58,700 --> 00:20:03,080
mechanism for subscribers the signal

00:20:00,919 --> 00:20:05,289
demand to publishers and the publisher

00:20:03,080 --> 00:20:07,669
will not send more data than is demanded

00:20:05,289 --> 00:20:10,309
but reactive streams is fairly low-level

00:20:07,669 --> 00:20:11,600
and meant for library authors to use for

00:20:10,309 --> 00:20:15,559
implementing more developer

00:20:11,600 --> 00:20:17,450
developer-friendly api's this is

00:20:15,559 --> 00:20:20,049
basically the entire reactive streams

00:20:17,450 --> 00:20:23,120
API it's three to three classes there's

00:20:20,049 --> 00:20:25,370
subscribers that can subscribe to

00:20:23,120 --> 00:20:28,100
publishers who then send them a

00:20:25,370 --> 00:20:31,220
subscription that they can use to

00:20:28,100 --> 00:20:33,590
request messages from the publisher the

00:20:31,220 --> 00:20:36,500
publisher calls on next every time

00:20:33,590 --> 00:20:39,549
there's a piece of data it can call that

00:20:36,500 --> 00:20:41,659
indefinitely if the stream is infinite

00:20:39,549 --> 00:20:43,580
then it calls on complete when it's

00:20:41,659 --> 00:20:46,850
finished or on fail on error if it

00:20:43,580 --> 00:20:48,919
actually failed at some point the

00:20:46,850 --> 00:20:52,580
subscriber can I use the subscription to

00:20:48,919 --> 00:20:53,389
request data as you can see this it's

00:20:52,580 --> 00:20:55,519
really low-level

00:20:53,389 --> 00:20:58,970
I'm using it on its own would be kind of

00:20:55,519 --> 00:21:00,169
painful so instead I'm going to spend

00:20:58,970 --> 00:21:02,899
the rest of this presentation talking

00:21:00,169 --> 00:21:04,789
about akka streams which is

00:21:02,899 --> 00:21:08,000
interoperable with the reactive streams

00:21:04,789 --> 00:21:10,490
API and it uses it internally but eases

00:21:08,000 --> 00:21:15,649
actors and a nice API on top of that to

00:21:10,490 --> 00:21:17,269
implement the stream processing as I

00:21:15,649 --> 00:21:19,159
said akka streams is a library built on

00:21:17,269 --> 00:21:21,259
top of back actors and reactive streams

00:21:19,159 --> 00:21:23,000
and it provides a way to process

00:21:21,259 --> 00:21:25,250
sequences of elements using bounded

00:21:23,000 --> 00:21:26,659
buffer space so this is different than

00:21:25,250 --> 00:21:28,850
the typical actor model that you have

00:21:26,659 --> 00:21:31,039
enacted which often processes using

00:21:28,850 --> 00:21:32,840
unbounded buffers that can explode your

00:21:31,039 --> 00:21:35,179
heap or it allows down to buffers but

00:21:32,840 --> 00:21:37,100
will drop messages PACA stream doesn't

00:21:35,179 --> 00:21:40,100
allow buffers to grow unbounded and it

00:21:37,100 --> 00:21:42,230
doesn't drop messages also unlike plain

00:21:40,100 --> 00:21:44,629
actors actor streams are strong PACA

00:21:42,230 --> 00:21:45,889
streams are strongly typed so you don't

00:21:44,629 --> 00:21:49,330
have to deal with receive block that

00:21:45,889 --> 00:21:49,330
takes the universe and returns nothing

00:21:50,169 --> 00:21:54,740
akka streams have some core concepts

00:21:52,669 --> 00:21:57,850
that are used to construct topologies

00:21:54,740 --> 00:22:00,409
for stream processing there's a source

00:21:57,850 --> 00:22:02,539
which is a processing stage with exactly

00:22:00,409 --> 00:22:06,350
one output and no inputs that emits

00:22:02,539 --> 00:22:09,330
elements downstream when they're ready

00:22:06,350 --> 00:22:11,160
there's a sink which is the opposite

00:22:09,330 --> 00:22:13,860
it's a processing stage with exactly one

00:22:11,160 --> 00:22:16,500
input it accepts elements from upstream

00:22:13,860 --> 00:22:18,210
and may influence that pressure on the

00:22:16,500 --> 00:22:20,820
upstream so they can say I'm busy stop

00:22:18,210 --> 00:22:22,470
sending me things then there's a flow

00:22:20,820 --> 00:22:25,500
which is the combination of both of them

00:22:22,470 --> 00:22:29,370
it has exactly one input one output and

00:22:25,500 --> 00:22:30,960
it sort of just transforms data and

00:22:29,370 --> 00:22:32,790
propagates back pressure up if it gets

00:22:30,960 --> 00:22:34,460
back pressure from its output it sends

00:22:32,790 --> 00:22:36,990
it up to the input

00:22:34,460 --> 00:22:39,390
there are also some Junction stages that

00:22:36,990 --> 00:22:40,290
though that do fan out in fan Yin for

00:22:39,390 --> 00:22:42,780
fan out think it's something like

00:22:40,290 --> 00:22:44,550
broadcasting or round robin and for fan

00:22:42,780 --> 00:22:49,320
in think of like merging or concatenate

00:22:44,550 --> 00:22:51,540
extremes once a stream topology has no

00:22:49,320 --> 00:22:53,310
exposed outputs or inputs it's what's

00:22:51,540 --> 00:22:59,220
called a runnable graph and can actually

00:22:53,310 --> 00:23:01,830
be run and do things processing stages

00:22:59,220 --> 00:23:05,100
are immutable they can be reused it can

00:23:01,830 --> 00:23:06,900
post together just like functions so

00:23:05,100 --> 00:23:10,230
anything that has the shape of a source

00:23:06,900 --> 00:23:12,450
is a source anything that has a shape of

00:23:10,230 --> 00:23:14,220
a sink is a sink and anything that has a

00:23:12,450 --> 00:23:16,350
shape of a flow is a flow no matter how

00:23:14,220 --> 00:23:19,140
complicated it is inside so it's really

00:23:16,350 --> 00:23:20,820
nice for building abstractions around

00:23:19,140 --> 00:23:23,250
complicated stream processing and still

00:23:20,820 --> 00:23:24,840
have a simple building block it's sort

00:23:23,250 --> 00:23:26,730
of simple to have similar to how in a

00:23:24,840 --> 00:23:28,650
cow you have the props object which is

00:23:26,730 --> 00:23:32,240
an immutable print for blueprint for

00:23:28,650 --> 00:23:32,240
constructing the actual actor instances

00:23:34,340 --> 00:23:41,850
so when act the actual graph is run its

00:23:40,260 --> 00:23:43,830
materialized and sorted this is

00:23:41,850 --> 00:23:45,330
analogous to turning on the tap in a

00:23:43,830 --> 00:23:48,030
real pipeline and seeing the water start

00:23:45,330 --> 00:23:51,030
flowing through actors will be created

00:23:48,030 --> 00:23:52,710
to handle the processing stages files or

00:23:51,030 --> 00:23:55,650
sockets or other resources can be opened

00:23:52,710 --> 00:23:56,970
or connections to kapha in our case when

00:23:55,650 --> 00:24:00,990
the stream completes these resources

00:23:56,970 --> 00:24:02,790
will be released automatically each

00:24:00,990 --> 00:24:05,160
graph stage can optionally provide a

00:24:02,790 --> 00:24:06,480
value once it's materialized that can

00:24:05,160 --> 00:24:07,860
help you interact with other parts of

00:24:06,480 --> 00:24:11,070
your system that aren't necessarily

00:24:07,860 --> 00:24:14,040
using active streams for example a

00:24:11,070 --> 00:24:15,990
source can materialize to an actor that

00:24:14,040 --> 00:24:18,720
when sent messages will emit them

00:24:15,990 --> 00:24:19,530
downstream into the stream or a promise

00:24:18,720 --> 00:24:22,800
that

00:24:19,530 --> 00:24:24,530
once completed will omit that item

00:24:22,800 --> 00:24:27,780
downstream and and complete the stream

00:24:24,530 --> 00:24:31,290
or it can also interoperable Interop

00:24:27,780 --> 00:24:33,560
with a reactive stream subscriber it can

00:24:31,290 --> 00:24:36,270
also materialize signal value and

00:24:33,560 --> 00:24:37,980
similarly syncs can materialize to an

00:24:36,270 --> 00:24:39,930
actor that upstream messages will be

00:24:37,980 --> 00:24:41,310
sent to and this actor can actually

00:24:39,930 --> 00:24:44,430
influence back pressure on the rest of

00:24:41,310 --> 00:24:46,650
the stream or it can be a future that

00:24:44,430 --> 00:24:48,360
contains a computed result assuming that

00:24:46,650 --> 00:24:50,490
the stream completes and it is kind of

00:24:48,360 --> 00:24:51,900
nice you can you could hide the fact

00:24:50,490 --> 00:24:53,340
that you're using a cue streams from an

00:24:51,900 --> 00:24:55,620
api consumer you get up an API that

00:24:53,340 --> 00:24:57,090
returns a future and then inside that

00:24:55,620 --> 00:24:58,980
API method you can actually create a

00:24:57,090 --> 00:25:00,930
crazy stream topology and run it and

00:24:58,980 --> 00:25:04,260
just return that simple future results

00:25:00,930 --> 00:25:05,790
here consumer also it can be a reactive

00:25:04,260 --> 00:25:10,320
streams producer which allows

00:25:05,790 --> 00:25:12,630
interoperability you might be thinking

00:25:10,320 --> 00:25:15,720
active streams looks great but can I

00:25:12,630 --> 00:25:17,370
easily hook it up with Kafka and yes

00:25:15,720 --> 00:25:19,470
there's a library called reactive cafe

00:25:17,370 --> 00:25:21,570
that does this

00:25:19,470 --> 00:25:24,030
it's a naka streams wrapper around the

00:25:21,570 --> 00:25:26,040
cap point nine java api it's under a

00:25:24,030 --> 00:25:27,630
little bit of flux and development right

00:25:26,040 --> 00:25:29,430
now it was originally written I think by

00:25:27,630 --> 00:25:32,310
software ml but it's moved to be an

00:25:29,430 --> 00:25:36,030
active project now and it provides

00:25:32,310 --> 00:25:38,850
cascade consumer source and a cat

00:25:36,030 --> 00:25:40,740
co-producer sync it might seem kind of

00:25:38,850 --> 00:25:43,230
backwards having a consumer via source

00:25:40,740 --> 00:25:44,430
and a producer being a sync but it kind

00:25:43,230 --> 00:25:46,800
of makes sense once you think about it a

00:25:44,430 --> 00:25:48,450
bit the consumers polling messages from

00:25:46,800 --> 00:25:51,330
Kafka and then emitting them into the

00:25:48,450 --> 00:25:52,950
stream I mean the producers receiving

00:25:51,330 --> 00:25:57,960
messages from the stream and then

00:25:52,950 --> 00:25:59,910
sending them to Kafka so the reactive

00:25:57,960 --> 00:26:03,420
captive producer is a sync that can send

00:25:59,910 --> 00:26:05,940
messages to Kafka topics it can also be

00:26:03,420 --> 00:26:07,860
a flow that emits a message to Kafka and

00:26:05,940 --> 00:26:09,300
then emits a message downstream when the

00:26:07,860 --> 00:26:11,250
message is successfully produced to

00:26:09,300 --> 00:26:13,260
Kafka and this is kind of useful for

00:26:11,250 --> 00:26:15,000
logging when a message sends properly or

00:26:13,260 --> 00:26:17,850
doing some other processing once that

00:26:15,000 --> 00:26:19,230
happens another nice thing is that the

00:26:17,850 --> 00:26:22,440
Kafka connections are automatically

00:26:19,230 --> 00:26:24,510
managed when the sync is connected the

00:26:22,440 --> 00:26:26,130
cafe connection will be made and when

00:26:24,510 --> 00:26:29,540
the stream completes the connection to

00:26:26,130 --> 00:26:29,540
caf-co will be automatically closed

00:26:30,170 --> 00:26:35,400
similarly the reactive calf

00:26:32,970 --> 00:26:36,900
humor is a source that pulls messages

00:26:35,400 --> 00:26:40,710
from one or more caf-co topics and

00:26:36,900 --> 00:26:42,870
amidst them downstream it's able to auto

00:26:40,710 --> 00:26:46,020
commit messages which provides in at

00:26:42,870 --> 00:26:47,850
most once message semantics or can also

00:26:46,020 --> 00:26:51,390
pass an offset object down through the

00:26:47,850 --> 00:26:53,789
stream that can then be used later to

00:26:51,390 --> 00:26:55,110
actually commit that offset back to

00:26:53,789 --> 00:26:56,789
Khafre and say I've consumed this

00:26:55,110 --> 00:26:58,559
message after the message that the

00:26:56,789 --> 00:27:00,000
processing is completed and it sort of

00:26:58,559 --> 00:27:01,770
gives the at least once consumption

00:27:00,000 --> 00:27:05,340
semantics so you can choose depending on

00:27:01,770 --> 00:27:07,380
how your application works it receives

00:27:05,340 --> 00:27:09,090
that pressure from downstream so if it

00:27:07,380 --> 00:27:10,860
receives that back pressure it just

00:27:09,090 --> 00:27:12,419
simply stops consuming from Kafka that's

00:27:10,860 --> 00:27:13,950
the nice thing about CAF gives is a pull

00:27:12,419 --> 00:27:17,820
base if you don't want more messages you

00:27:13,950 --> 00:27:20,700
just don't pull them also when the

00:27:17,820 --> 00:27:22,860
stream is run it materializes into a

00:27:20,700 --> 00:27:25,580
control object that can be used to stop

00:27:22,860 --> 00:27:27,659
the consumer and finish the stream and

00:27:25,580 --> 00:27:29,970
like the producer all the caf-co

00:27:27,659 --> 00:27:31,770
resources are automatically managed when

00:27:29,970 --> 00:27:33,690
the stream starts it connects the Kafka

00:27:31,770 --> 00:27:41,460
when the control objects used to stop

00:27:33,690 --> 00:27:44,250
the stream it disconnects from Kafka so

00:27:41,460 --> 00:27:46,770
this is what a pretty simple producer

00:27:44,250 --> 00:27:48,840
example would look like I'm eating an

00:27:46,770 --> 00:27:50,700
actor system and an actor materializer

00:27:48,840 --> 00:27:53,070
and that's what a caduceus actually

00:27:50,700 --> 00:27:55,580
construct the actors and do all sorts of

00:27:53,070 --> 00:27:57,990
magic when it actually runs you stream

00:27:55,580 --> 00:28:00,570
you need producer settings so here we're

00:27:57,990 --> 00:28:02,070
using a key that's a byte array a body

00:28:00,570 --> 00:28:03,809
for the message that's a string and

00:28:02,070 --> 00:28:07,350
telling it to send it to the kaepa

00:28:03,809 --> 00:28:08,580
server running on localhost finally we

00:28:07,350 --> 00:28:10,710
create a source that's emitting the

00:28:08,580 --> 00:28:12,000
numbers 1 to 100 we're mapping it and

00:28:10,710 --> 00:28:14,400
changing it into a string that just says

00:28:12,000 --> 00:28:16,289
message 1 message two then we create

00:28:14,400 --> 00:28:19,289
this producer record which is a cafe

00:28:16,289 --> 00:28:21,539
data structure that says I'm sending a

00:28:19,289 --> 00:28:23,610
message that has a key of a byte array a

00:28:21,539 --> 00:28:25,289
body of string I'm sending it to the

00:28:23,610 --> 00:28:26,990
topic called lower and I'm just sending

00:28:25,289 --> 00:28:29,760
the message I'm not giving it a key and

00:28:26,990 --> 00:28:32,760
then we're seeing I send it to a

00:28:29,760 --> 00:28:34,320
producer a plain sync and this is a sync

00:28:32,760 --> 00:28:36,419
that just sends messages to cap it with

00:28:34,320 --> 00:28:38,039
those producer settings and then when

00:28:36,419 --> 00:28:39,000
you run it it will take those messages

00:28:38,039 --> 00:28:40,710
from 1 to 100

00:28:39,000 --> 00:28:43,789
and send them to calculate and then

00:28:40,710 --> 00:28:43,789
close the connection to come

00:28:44,429 --> 00:28:50,049
similarly the consumer you need the same

00:28:46,840 --> 00:28:52,540
actor system and materializer the

00:28:50,049 --> 00:28:54,309
consumer settings are similar in this

00:28:52,540 --> 00:28:56,799
case you have a DC realizer instead of

00:28:54,309 --> 00:28:58,480
serializer we're also telling it the set

00:28:56,799 --> 00:29:00,760
of topics to listen to in this case was

00:28:58,480 --> 00:29:02,350
listening to a set called lower we're

00:29:00,760 --> 00:29:04,120
telling it which catheter would connect

00:29:02,350 --> 00:29:05,350
to and they were telling it a consumer

00:29:04,120 --> 00:29:07,390
group ID which I talked about earlier

00:29:05,350 --> 00:29:11,320
which allows you to have a logical set

00:29:07,390 --> 00:29:13,179
of consumers here we're creating an at

00:29:11,320 --> 00:29:14,950
most one source which is automatically

00:29:13,179 --> 00:29:17,110
committing the offsets once it gets

00:29:14,950 --> 00:29:18,820
messages we're grabbing the value out of

00:29:17,110 --> 00:29:20,530
the record and then we're just sending

00:29:18,820 --> 00:29:22,840
it to a sync that just prints out the

00:29:20,530 --> 00:29:24,160
value so this will basically listen to

00:29:22,840 --> 00:29:26,890
calculate and print out values from the

00:29:24,160 --> 00:29:29,020
messages against and run it and that

00:29:26,890 --> 00:29:31,390
gives this control object and then later

00:29:29,020 --> 00:29:32,950
on whenever we're in our application we

00:29:31,390 --> 00:29:36,850
can call stop on it which will stop

00:29:32,950 --> 00:29:39,490
consuming and complete the stream you

00:29:36,850 --> 00:29:42,280
can also combine the two I emitted the

00:29:39,490 --> 00:29:44,049
boilerplate on this one but here we're

00:29:42,280 --> 00:29:46,330
using a different kind of source a

00:29:44,049 --> 00:29:48,190
committable source which passes an

00:29:46,330 --> 00:29:51,760
offset object down the stream that can

00:29:48,190 --> 00:29:53,799
be used later to commit the offset then

00:29:51,760 --> 00:29:55,720
we sort of are taking the value making

00:29:53,799 --> 00:29:57,520
it uppercase constructing that same

00:29:55,720 --> 00:29:59,100
producer record sending it to a

00:29:57,520 --> 00:30:01,240
different topic this time called upper

00:29:59,100 --> 00:30:03,309
and then we're passing that committable

00:30:01,240 --> 00:30:06,070
offset around that's what comes from the

00:30:03,309 --> 00:30:07,780
committable source and that's sent

00:30:06,070 --> 00:30:09,850
downstream to this committable sync and

00:30:07,780 --> 00:30:12,100
so what happens here is the producer

00:30:09,850 --> 00:30:13,780
produces the message and only once it's

00:30:12,100 --> 00:30:15,640
successfully produced the message will

00:30:13,780 --> 00:30:17,230
it actually commit that offset back to

00:30:15,640 --> 00:30:21,010
calc and say I'm done producing this

00:30:17,230 --> 00:30:23,080
message so that's really nice if you

00:30:21,010 --> 00:30:24,309
fail in the middle it'll just when you

00:30:23,080 --> 00:30:25,330
reconnect it'll start reading the

00:30:24,309 --> 00:30:29,320
messages from where it left off

00:30:25,330 --> 00:30:31,390
reprocess them and eventually commit so

00:30:29,320 --> 00:30:33,400
what does this give us in about five or

00:30:31,390 --> 00:30:35,950
six lines of code we have at least ones

00:30:33,400 --> 00:30:37,900
delivery we have full back pressure in

00:30:35,950 --> 00:30:41,070
here if for some reason committing

00:30:37,900 --> 00:30:43,419
sending Kafka becomes slow it'll stop

00:30:41,070 --> 00:30:45,910
consuming messages from the other topic

00:30:43,419 --> 00:30:47,290
and it's pretty easy to read we did a

00:30:45,910 --> 00:30:49,360
bunch of stuff here we connected to a

00:30:47,290 --> 00:30:51,220
caf-co topic we transform the data and

00:30:49,360 --> 00:30:52,809
we sent it to another topic just in a

00:30:51,220 --> 00:30:54,580
few lines of code

00:30:52,809 --> 00:30:56,590
it's a super simple example but it

00:30:54,580 --> 00:30:57,360
illustrates how easy it is to process

00:30:56,590 --> 00:31:01,260
data from Caffe

00:30:57,360 --> 00:31:03,990
with Pakistan's so now I'm going to

00:31:01,260 --> 00:31:07,830
attempt the demo gods and try to show

00:31:03,990 --> 00:31:10,920
you this running so on my laptop I'm

00:31:07,830 --> 00:31:12,870
running a cafe broker I have two topics

00:31:10,920 --> 00:31:14,760
called lower and upper and I'm running

00:31:12,870 --> 00:31:16,140
that I'm gonna run that example from the

00:31:14,760 --> 00:31:18,330
last couple slides where it consumes

00:31:16,140 --> 00:31:22,250
from one topic upper cases the value and

00:31:18,330 --> 00:31:22,250
produces it to the other topic

00:31:22,320 --> 00:31:30,660
let's see I'll start the consumers this

00:31:27,299 --> 00:31:31,799
is this is consuming up the lower case

00:31:30,660 --> 00:31:33,510
topic so we'll see the messages

00:31:31,799 --> 00:31:35,520
initially come in here lower case and

00:31:33,510 --> 00:31:38,160
this is consuming from the upper case

00:31:35,520 --> 00:31:40,710
topic where it will show the messages as

00:31:38,160 --> 00:31:42,860
they're processed by a customs and upper

00:31:40,710 --> 00:31:42,860
case

00:31:53,129 --> 00:31:58,059
so I'm going to use an ammonite which is

00:31:55,809 --> 00:31:59,080
a repli'd is a little bit nicer than the

00:31:58,059 --> 00:32:01,990
built-in scale AREPO

00:31:59,080 --> 00:32:03,759
it has syntax highlighting and some

00:32:01,990 --> 00:32:07,809
other nice stuff you can load libraries

00:32:03,759 --> 00:32:12,210
from Ivy so here I'm loading akka akka

00:32:07,809 --> 00:32:12,210
akka streams and then reactive Kafka

00:32:19,790 --> 00:32:25,940
now I'm going to load just some imports

00:32:22,310 --> 00:32:28,070
that we need from all the libraries it's

00:32:25,940 --> 00:32:32,710
sort of importing this galley DSL from a

00:32:28,070 --> 00:32:32,710
constrains and CAF and reactive Kafka

00:32:34,750 --> 00:32:45,500
now I'm going to create the active

00:32:38,030 --> 00:32:48,560
system and the materializer create the

00:32:45,500 --> 00:32:53,660
producer settings that we had before in

00:32:48,560 --> 00:32:56,600
the slides connecting to the broker

00:32:53,660 --> 00:33:00,140
running on my computer and the consumer

00:32:56,600 --> 00:33:04,130
settings I'm going to tell it to listen

00:33:00,140 --> 00:33:05,690
to the topic called lower and connect to

00:33:04,130 --> 00:33:11,210
my local Kafka instance and given the

00:33:05,690 --> 00:33:15,140
group ID service 1 now I'm going to

00:33:11,210 --> 00:33:17,230
actually create the consumer and start

00:33:15,140 --> 00:33:17,230
it

00:33:21,010 --> 00:33:24,640
so here you can see it's basically code

00:33:23,380 --> 00:33:27,640
from the slide I'm returning that

00:33:24,640 --> 00:33:29,860
control object after running the stream

00:33:27,640 --> 00:33:32,350
here so this is actually connecting to

00:33:29,860 --> 00:33:37,870
Kafka now I have that control object

00:33:32,350 --> 00:33:41,290
that I can use later to stop now I'm

00:33:37,870 --> 00:33:43,090
going to produce some messages I'm going

00:33:41,290 --> 00:33:44,730
to add an evil thread dot sleep in here

00:33:43,090 --> 00:33:47,890
so that I can quickly move to the other

00:33:44,730 --> 00:33:50,590
screen so here I'm just going to omit

00:33:47,890 --> 00:33:54,150
from 1 to 100 emit message 1 to 100 to

00:33:50,590 --> 00:33:57,520
the lower topic so let's hope this works

00:33:54,150 --> 00:33:58,929
so this will send it so we should see on

00:33:57,520 --> 00:34:01,090
the left hand side the messages come in

00:33:58,929 --> 00:34:02,679
on the lower topic and then them start

00:34:01,090 --> 00:34:05,410
being processed immediately on the other

00:34:02,679 --> 00:34:07,720
side to the upper case and so that

00:34:05,410 --> 00:34:08,080
happened pretty quickly and we can do it

00:34:07,720 --> 00:34:10,179
again

00:34:08,080 --> 00:34:15,149
I'm going to show you what happens when

00:34:10,179 --> 00:34:15,149
you if I stop the consumer

00:34:20,440 --> 00:34:25,710
so it's not running anymore so what

00:34:22,210 --> 00:34:25,710
happens if I produce some more messages

00:34:31,620 --> 00:34:35,520
so I just produced a bunch of messages

00:34:33,240 --> 00:34:38,000
you can see them here I'm going to

00:34:35,520 --> 00:34:38,000
change the

00:34:46,830 --> 00:34:50,780
submit some different messages as though

00:34:54,840 --> 00:34:57,540
so you can see this Galatea's messages

00:34:56,520 --> 00:34:59,850
they're not on the other side because

00:34:57,540 --> 00:35:07,940
the consumer is not running but if I

00:34:59,850 --> 00:35:07,940
started again oops

00:35:14,480 --> 00:35:17,750
you can see that it immediately started

00:35:16,250 --> 00:35:20,380
processing the messages again and got

00:35:17,750 --> 00:35:20,380
all the messages

00:35:27,970 --> 00:35:40,660
so to wrap up microservices have a lot

00:35:37,450 --> 00:35:42,640
of advantages they're trade-offs that

00:35:40,660 --> 00:35:43,780
you have to make with the increasing

00:35:42,640 --> 00:35:46,869
chance of failure and increase

00:35:43,780 --> 00:35:48,520
complexity asynchronous messaging can

00:35:46,869 --> 00:35:50,200
help reduce this complexity and caf-co

00:35:48,520 --> 00:35:51,190
is a really good option for for doing

00:35:50,200 --> 00:35:53,410
this

00:35:51,190 --> 00:35:55,089
and finally akka streams and reactive

00:35:53,410 --> 00:35:57,040
caf-co make reliably processing data

00:35:55,089 --> 00:36:00,540
from Kafka with full back pressure in

00:35:57,040 --> 00:36:03,849
your actual services really simple and

00:36:00,540 --> 00:36:06,069
so this might be what an architecture of

00:36:03,849 --> 00:36:07,630
your system using extreme Zacapa might

00:36:06,069 --> 00:36:09,640
look like you have services that are

00:36:07,630 --> 00:36:12,069
basically running akka streams inside

00:36:09,640 --> 00:36:16,690
producing the Kafka and then consuming

00:36:12,069 --> 00:36:26,090
using echo streams in other services and

00:36:16,690 --> 00:36:30,010
that's all I have thanks any questions

00:36:26,090 --> 00:36:30,010
I guess you

00:36:37,220 --> 00:36:41,260
so the question was whether cap can

00:36:39,470 --> 00:36:44,750
enforce any sequential message streams

00:36:41,260 --> 00:36:47,990
as I talked about earlier

00:36:44,750 --> 00:36:51,230
Kafka only allows ordering within a

00:36:47,990 --> 00:36:52,940
given partition and so if you are really

00:36:51,230 --> 00:36:54,319
really care about ordering you can

00:36:52,940 --> 00:36:56,270
create a topic that has one partition

00:36:54,319 --> 00:36:58,640
and you will get a total order within

00:36:56,270 --> 00:37:01,730
that topic every message you put in will

00:36:58,640 --> 00:37:05,810
be in the order that they arrived if

00:37:01,730 --> 00:37:08,240
you're a little more if you want to be

00:37:05,810 --> 00:37:10,310
able to have more partitions you give

00:37:08,240 --> 00:37:11,750
your message as a partition key so say

00:37:10,310 --> 00:37:13,280
like a user ID or something and then all

00:37:11,750 --> 00:37:15,230
messages for that user will always go to

00:37:13,280 --> 00:37:20,089
the same partition so you can have an

00:37:15,230 --> 00:37:27,349
ordering at least in that part more

00:37:20,089 --> 00:37:31,160
questions yeah back there um I drift I

00:37:27,349 --> 00:37:33,829
think is a our PC format isn't it

00:37:31,160 --> 00:37:35,720
I haven't had a lot of experience with

00:37:33,829 --> 00:37:38,960
drift you can use other things uh I

00:37:35,720 --> 00:37:41,119
think another popular format is Avro

00:37:38,960 --> 00:37:42,800
but basically anything that's binary is

00:37:41,119 --> 00:37:44,839
probably a better idea than using JSON

00:37:42,800 --> 00:37:46,640
just because it's smaller takes up less

00:37:44,839 --> 00:37:49,690
space in the coptic cluster and is

00:37:46,640 --> 00:37:56,619
really fast to serialize and deserialize

00:37:49,690 --> 00:37:56,619
yeah any other questions yes back there

00:38:02,110 --> 00:38:06,100
yeah it's there's I think some of the

00:38:04,510 --> 00:38:08,080
methods take an implicit actor system

00:38:06,100 --> 00:38:10,390
that you're not seeing in the actual

00:38:08,080 --> 00:38:11,440
examples and one of the nice things that

00:38:10,390 --> 00:38:13,600
you don't actually have to deal with the

00:38:11,440 --> 00:38:15,610
actors you just can write code that does

00:38:13,600 --> 00:38:17,290
flat maps maps group eyes stuff like

00:38:15,610 --> 00:38:18,730
that on the streams and akka handles

00:38:17,290 --> 00:38:20,680
creating all the actors and doing all

00:38:18,730 --> 00:38:26,760
the back pressure and stuff like that

00:38:20,680 --> 00:38:26,760
for you which is really nice yeah

00:38:30,220 --> 00:38:35,500
um I guess anyway you would monitor

00:38:33,970 --> 00:38:37,839
other actors that actually does create

00:38:35,500 --> 00:38:40,210
actors and you can you can find them and

00:38:37,839 --> 00:38:43,200
find their names and do whatever other

00:38:40,210 --> 00:38:46,329
kind of actor monitoring you would do I

00:38:43,200 --> 00:38:48,670
haven't actually done a lot of work with

00:38:46,329 --> 00:38:51,250
this yet I've just been playing around

00:38:48,670 --> 00:38:53,530
with it a lot but I'm sure there are

00:38:51,250 --> 00:38:54,849
ways to monitor it at least for Cathy

00:38:53,530 --> 00:38:56,619
itself there are tools for monitoring

00:38:54,849 --> 00:38:58,270
the topics and you can tell how many

00:38:56,619 --> 00:38:59,799
messages are being produced how many are

00:38:58,270 --> 00:39:01,150
being consumed whether there's any

00:38:59,799 --> 00:39:02,589
consumers that are lagging behind and

00:39:01,150 --> 00:39:03,910
stuff like that so that there's a lot of

00:39:02,589 --> 00:39:09,930
tooling around that but I've used a lot

00:39:03,910 --> 00:39:09,930
and it works really well yeah

00:39:15,970 --> 00:39:22,480
um yeah I think akka streams basically

00:39:19,030 --> 00:39:24,640
is good for inside a service you can

00:39:22,480 --> 00:39:26,849
have a stream that materializes to an

00:39:24,640 --> 00:39:29,710
actor and so you could have the stream

00:39:26,849 --> 00:39:31,030
process send messages that actor and

00:39:29,710 --> 00:39:33,520
then that actor could send messages

00:39:31,030 --> 00:39:35,799
somewhere else in your cluster and send

00:39:33,520 --> 00:39:37,390
it say to another actor that is in front

00:39:35,799 --> 00:39:41,680
of another stream so you could you could

00:39:37,390 --> 00:39:43,000
do it that way yeah you would have to

00:39:41,680 --> 00:39:45,359
deal with the back pressure yourself in

00:39:43,000 --> 00:39:53,049
that case passing it over the cluster I

00:39:45,359 --> 00:39:54,880
believe you could do that too you could

00:39:53,049 --> 00:39:57,400
use caffeine between the services to get

00:39:54,880 --> 00:40:04,020
that back pressure there depending on

00:39:57,400 --> 00:40:04,020
whether what your requirements are yeah

00:40:07,860 --> 00:40:11,940
yeah capita point 9 added a whole bunch

00:40:10,680 --> 00:40:14,610
of security features so it added

00:40:11,940 --> 00:40:17,310
authentication and authorization and so

00:40:14,610 --> 00:40:19,800
there are ssl certificates it can use

00:40:17,310 --> 00:40:21,540
Kerberos to authenticate users and

00:40:19,800 --> 00:40:23,850
that's a CL so you can say this user is

00:40:21,540 --> 00:40:26,430
only allowed to write to this topic or

00:40:23,850 --> 00:40:28,080
read from this topic you can do quota

00:40:26,430 --> 00:40:30,210
base limits on topics too so they can

00:40:28,080 --> 00:40:32,540
only produce or consume so fast from the

00:40:30,210 --> 00:40:32,540

YouTube URL: https://www.youtube.com/watch?v=k_Y5ieFHGbs


