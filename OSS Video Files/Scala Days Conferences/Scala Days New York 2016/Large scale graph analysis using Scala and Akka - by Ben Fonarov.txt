Title: Large scale graph analysis using Scala and Akka - by Ben Fonarov
Publication date: 2016-06-17
Playlist: Scala Days New York 2016
Description: 
	This talk was recorded at Scala Days New York, 2016. Follow along on Twitter @scaladays and on the website for more information http://scaladays.org/.

Abstract:
In this talk we will go over the architecture and implementation of a large-scale graph analysis engine using Scala, Akka and Hbase. Starting from the needs that led to this type of processing, weâ€™ll go through the different implementation considerations, jump into an implementation that includes a query parser and an akka cluster to do the processing, and then see how we can produce a stream output of the results. This talk could benefit anyone that is interested in Scala, Akka, Big Data, stream processing and of course graphs.
Captions: 
	00:00:01,040 --> 00:00:05,549
we're talking today about large-scale

00:00:03,449 --> 00:00:08,970
graph analysis specifically with alkyne

00:00:05,549 --> 00:00:10,980
and Scala just a little intro then been

00:00:08,970 --> 00:00:13,440
coding for more than 10 years always

00:00:10,980 --> 00:00:15,510
full stack been working with graphs for

00:00:13,440 --> 00:00:17,669
about five years now and I work in the

00:00:15,510 --> 00:00:20,849
Big Data space for Capital One we

00:00:17,669 --> 00:00:22,589
basically build data products a little

00:00:20,849 --> 00:00:23,970
bit about what I hope you can get out of

00:00:22,589 --> 00:00:26,130
this talk so we're gonna start a little

00:00:23,970 --> 00:00:28,199
bit about the need why we actually built

00:00:26,130 --> 00:00:29,819
the graph at Capital One some of the

00:00:28,199 --> 00:00:31,980
requirements that we had and then go

00:00:29,819 --> 00:00:33,629
through two different architectures of

00:00:31,980 --> 00:00:34,890
how we thought about implementing this

00:00:33,629 --> 00:00:36,390
this would be sort of a naive

00:00:34,890 --> 00:00:38,519
implementation and not anything that

00:00:36,390 --> 00:00:39,989
would be robust in production just as an

00:00:38,519 --> 00:00:42,239
example of what we've been thinking

00:00:39,989 --> 00:00:43,620
through and what we've built and then a

00:00:42,239 --> 00:00:45,269
couple of thoughts about what else can

00:00:43,620 --> 00:00:46,859
be done and - in order to improve those

00:00:45,269 --> 00:00:50,820
things so optimizations as well as

00:00:46,859 --> 00:00:52,440
what's next after that for us so

00:00:50,820 --> 00:00:54,750
starting with the need it's actually a

00:00:52,440 --> 00:00:56,309
very simple ask and it's surprisingly

00:00:54,750 --> 00:00:59,129
surprisingly difficult to add an

00:00:56,309 --> 00:01:01,469
enterprise the need is can get me some

00:00:59,129 --> 00:01:03,359
data now we're gonna talk a little bit

00:01:01,469 --> 00:01:05,820
about why that's difficult in a second

00:01:03,359 --> 00:01:07,619
but first a bit of a couple of the

00:01:05,820 --> 00:01:09,119
premises about getting people some data

00:01:07,619 --> 00:01:11,070
well they want the data to be already

00:01:09,119 --> 00:01:12,600
integrated because it turned out that at

00:01:11,070 --> 00:01:14,670
least at the company that I work for

00:01:12,600 --> 00:01:17,490
people were spending an ornament amount

00:01:14,670 --> 00:01:19,470
of time just joining tables again and

00:01:17,490 --> 00:01:21,720
again and again second thing is that

00:01:19,470 --> 00:01:23,430
they want to get the results fast right

00:01:21,720 --> 00:01:25,259
so they don't want to sit there and wait

00:01:23,430 --> 00:01:27,149
for a query to finish processing and

00:01:25,259 --> 00:01:30,899
then get the data they want to see the

00:01:27,149 --> 00:01:32,729
data that they want immediately thing

00:01:30,899 --> 00:01:34,170
after that is that they think in tables

00:01:32,729 --> 00:01:37,170
they don't necessarily think in graphs

00:01:34,170 --> 00:01:38,880
and some people do think in graphs but

00:01:37,170 --> 00:01:40,290
that's not a majority people most people

00:01:38,880 --> 00:01:42,530
think in tables and they want the

00:01:40,290 --> 00:01:44,759
results in a tape in a tabular format

00:01:42,530 --> 00:01:47,189
and it should be usable with their own

00:01:44,759 --> 00:01:50,100
two tools so you can parts you know you

00:01:47,189 --> 00:01:51,840
can plug a spark into it or h2o or if

00:01:50,100 --> 00:01:53,399
you want to do something else you can do

00:01:51,840 --> 00:01:55,860
that with the data and last but not

00:01:53,399 --> 00:01:58,109
least they wanted to think about the

00:01:55,860 --> 00:02:00,689
data or they wanted to use the data in

00:01:58,109 --> 00:02:02,549
the way that they think about so Capital

00:02:00,689 --> 00:02:04,170
One is a bank we have accounts we have

00:02:02,549 --> 00:02:06,899
customers we have transactions we have

00:02:04,170 --> 00:02:10,259
statements why not have the data be

00:02:06,899 --> 00:02:11,480
represented in the same way so that was

00:02:10,259 --> 00:02:13,819
the need

00:02:11,480 --> 00:02:17,420
and the problem is that with a lot of

00:02:13,819 --> 00:02:20,060
data there's a lot of problems we have a

00:02:17,420 --> 00:02:22,370
lot of data right we have a lot of ETL

00:02:20,060 --> 00:02:24,769
that produces more data and it's not in

00:02:22,370 --> 00:02:26,659
one place we have multiple data

00:02:24,769 --> 00:02:29,330
warehouses we have a data Lake we have

00:02:26,659 --> 00:02:31,629
all these source systems or pieces in

00:02:29,330 --> 00:02:34,370
the pie that basically generate data and

00:02:31,629 --> 00:02:36,560
there's a data overload at the end of

00:02:34,370 --> 00:02:38,540
the day and so what you want to do is

00:02:36,560 --> 00:02:41,629
you want to get a situation that doesn't

00:02:38,540 --> 00:02:44,480
look like this think you want it to get

00:02:41,629 --> 00:02:46,400
a little bit more like this but how do

00:02:44,480 --> 00:02:49,849
you get that and we'll talk a little bit

00:02:46,400 --> 00:02:53,060
about that in a second but what we did

00:02:49,849 --> 00:02:54,829
is we decided to build a graph in this

00:02:53,060 --> 00:03:00,170
case a distributed time series graph

00:02:54,829 --> 00:03:01,760
with Scala akka and HBase and actually

00:03:00,170 --> 00:03:05,480
that's our little logo for it work it's

00:03:01,760 --> 00:03:08,690
called athena and it's like a database

00:03:05,480 --> 00:03:11,660
but it does a bit more so first off you

00:03:08,690 --> 00:03:14,180
get to design the graph schema or the

00:03:11,660 --> 00:03:15,889
model if you will in a visual interface

00:03:14,180 --> 00:03:17,870
it's an ontology it's basically saying

00:03:15,889 --> 00:03:19,489
that what is of the graph so what are

00:03:17,870 --> 00:03:20,569
the things in the graph in our example

00:03:19,489 --> 00:03:22,519
there are customers there are

00:03:20,569 --> 00:03:24,109
transactions there's accounts there's

00:03:22,519 --> 00:03:25,609
things like that

00:03:24,109 --> 00:03:27,560
once you do that it actually

00:03:25,609 --> 00:03:29,989
automatically knows how to connect to

00:03:27,560 --> 00:03:31,190
different sources grab the data bring it

00:03:29,989 --> 00:03:33,859
into the graph and make it available

00:03:31,190 --> 00:03:35,180
available to you under that sort of

00:03:33,859 --> 00:03:37,430
business language that we just talked

00:03:35,180 --> 00:03:40,069
about you can run large-scale

00:03:37,430 --> 00:03:42,079
computation on it and it stores the data

00:03:40,069 --> 00:03:43,940
in time series that deduplicate the data

00:03:42,079 --> 00:03:46,669
and provides you an API so you can use

00:03:43,940 --> 00:03:49,190
it whether you know through programmatic

00:03:46,669 --> 00:03:51,049
interface or a REST API or whatever way

00:03:49,190 --> 00:03:53,269
you want to use the data you can get the

00:03:51,049 --> 00:03:56,959
data which is the original of what we

00:03:53,269 --> 00:03:59,900
want it to do so why why a graph well a

00:03:56,959 --> 00:04:03,500
graph is already integrated by nature so

00:03:59,900 --> 00:04:06,440
we get the joins out of the way if we

00:04:03,500 --> 00:04:09,049
give labels to nodes then we've already

00:04:06,440 --> 00:04:11,209
made the second step towards a business

00:04:09,049 --> 00:04:13,459
language so now a node that has a label

00:04:11,209 --> 00:04:15,729
customer we know that all customer

00:04:13,459 --> 00:04:18,349
information is aggregated on that node

00:04:15,729 --> 00:04:20,390
it can easily evolve over time there's

00:04:18,349 --> 00:04:23,570
no schema and it's relatively easy to

00:04:20,390 --> 00:04:25,970
migrate a graph and it's efficient query

00:04:23,570 --> 00:04:28,340
so those are some of the reasons why we

00:04:25,970 --> 00:04:29,330
decided to go towards a graph and if you

00:04:28,340 --> 00:04:30,950
want to think about it there's an

00:04:29,330 --> 00:04:33,020
enterprise term for this it's a semantic

00:04:30,950 --> 00:04:34,640
data Lake right so it's one step forward

00:04:33,020 --> 00:04:36,890
from a data Lake it's basically getting

00:04:34,640 --> 00:04:40,790
the data and to some sort of semantics

00:04:36,890 --> 00:04:43,280
or ontology base let's dive in because

00:04:40,790 --> 00:04:45,020
that's sort of a preface now we want to

00:04:43,280 --> 00:04:48,620
go into code and actual implementation

00:04:45,020 --> 00:04:50,420
and things like that so we're gonna look

00:04:48,620 --> 00:04:54,200
specifically at pattern matching on

00:04:50,420 --> 00:04:56,180
graphs today and this is sort of a query

00:04:54,200 --> 00:04:58,760
language that we've created internally

00:04:56,180 --> 00:05:00,860
just for the sake of doing pattern

00:04:58,760 --> 00:05:03,290
matching the idea is pretty simple you

00:05:00,860 --> 00:05:04,820
start with a node label so that in our

00:05:03,290 --> 00:05:07,010
case could be a customer or an account

00:05:04,820 --> 00:05:08,840
and then you say the properties that you

00:05:07,010 --> 00:05:11,150
care about and those are within square

00:05:08,840 --> 00:05:14,390
brackets and you have some sort of comma

00:05:11,150 --> 00:05:16,010
a dot represents an edge so some sort of

00:05:14,390 --> 00:05:17,600
relationship between that node and

00:05:16,010 --> 00:05:19,100
another node and then you have another

00:05:17,600 --> 00:05:20,720
loan node label now that could be the

00:05:19,100 --> 00:05:22,510
same as the first node label but there

00:05:20,720 --> 00:05:25,250
would have to be an edge between them

00:05:22,510 --> 00:05:26,600
once you do that if you wanted to

00:05:25,250 --> 00:05:29,840
illustrate it you basically get these

00:05:26,600 --> 00:05:32,210
multiple steps so pattern matching and

00:05:29,840 --> 00:05:33,920
graphs is really about breaking down the

00:05:32,210 --> 00:05:35,330
pattern into multiple steps that as

00:05:33,920 --> 00:05:37,040
you're traversing the graph you're going

00:05:35,330 --> 00:05:39,500
to discover these different things that

00:05:37,040 --> 00:05:40,700
fit that pattern and if they do fit that

00:05:39,500 --> 00:05:42,920
pattern then you take the next step

00:05:40,700 --> 00:05:45,860
until you finally reach the end and you

00:05:42,920 --> 00:05:47,390
say I've found a result that looks very

00:05:45,860 --> 00:05:49,850
much like the pattern I was looking for

00:05:47,390 --> 00:05:55,040
let me proceed because this is an actual

00:05:49,850 --> 00:05:56,270
valid result the data model in HBase I

00:05:55,040 --> 00:05:58,370
don't know how many people here have

00:05:56,270 --> 00:06:03,500
used HBase Jovan's

00:05:58,370 --> 00:06:05,960
very few okay so it's a sparse table

00:06:03,500 --> 00:06:08,330
matrix I'm not showing everything that

00:06:05,960 --> 00:06:10,610
actually exists within within an HBase

00:06:08,330 --> 00:06:12,590
table and there are things in here for

00:06:10,610 --> 00:06:15,380
instance there's where there's a value

00:06:12,590 --> 00:06:17,240
then there's also a timestamp which is

00:06:15,380 --> 00:06:19,490
also used to version and there's some

00:06:17,240 --> 00:06:20,930
labels so that you can do things like

00:06:19,490 --> 00:06:22,520
security around it and so on

00:06:20,930 --> 00:06:25,340
but if you if you wanted to think about

00:06:22,520 --> 00:06:27,350
it then it's a no sequel store and you

00:06:25,340 --> 00:06:30,410
have this row key and you get to define

00:06:27,350 --> 00:06:32,510
column families and in the in a column

00:06:30,410 --> 00:06:34,790
family you can have as many columns as

00:06:32,510 --> 00:06:36,919
you want so there's those are called

00:06:34,790 --> 00:06:37,400
qualifiers so our data model essentially

00:06:36,919 --> 00:06:39,800
takes

00:06:37,400 --> 00:06:42,560
every node in the graph and says you

00:06:39,800 --> 00:06:44,660
have to column families properties

00:06:42,560 --> 00:06:46,940
column family and an edges column family

00:06:44,660 --> 00:06:48,889
under properties will store all the

00:06:46,940 --> 00:06:50,750
values or the key values that relate to

00:06:48,889 --> 00:06:54,550
you and under the edges we're going to

00:06:50,750 --> 00:06:54,550
store all the edges that you linked to

00:06:54,759 --> 00:07:00,919
we also do this as a table for every

00:06:57,949 --> 00:07:03,470
label in the graph so if you wanted to

00:07:00,919 --> 00:07:04,970
think about it if I defined in the

00:07:03,470 --> 00:07:07,009
ontology where in this visual

00:07:04,970 --> 00:07:09,020
organization that I that I described

00:07:07,009 --> 00:07:10,520
before if I was talking about customers

00:07:09,020 --> 00:07:12,740
and accounts and so on we would have a

00:07:10,520 --> 00:07:14,479
table for each and every one of those so

00:07:12,740 --> 00:07:17,360
now what you get is per table all the

00:07:14,479 --> 00:07:20,780
row keys only refer to things that exist

00:07:17,360 --> 00:07:22,370
within that label that's going to be

00:07:20,780 --> 00:07:25,729
important for what we're going to show

00:07:22,370 --> 00:07:27,440
next which is actual different types of

00:07:25,729 --> 00:07:32,180
consumption architectures so this is the

00:07:27,440 --> 00:07:34,130
first one and what's nice about it is

00:07:32,180 --> 00:07:36,560
that essentially you get to distribute

00:07:34,130 --> 00:07:39,320
your processing of the graph very nicely

00:07:36,560 --> 00:07:41,570
with akka and Scala so we start by

00:07:39,320 --> 00:07:43,880
parsing the query which is again a

00:07:41,570 --> 00:07:46,250
pattern match we build an ast out of it

00:07:43,880 --> 00:07:49,280
and then what we do is essentially have

00:07:46,250 --> 00:07:53,449
a queue or a list of vertices that need

00:07:49,280 --> 00:07:55,490
to be visited by the akka actors and as

00:07:53,449 --> 00:07:58,520
they're going through this queue or the

00:07:55,490 --> 00:08:00,199
list of vertices to visit they're

00:07:58,520 --> 00:08:02,659
emitting results once they've found a

00:08:00,199 --> 00:08:05,060
pattern now we're gonna dive into the

00:08:02,659 --> 00:08:08,330
code in a second but this is vertex

00:08:05,060 --> 00:08:11,090
based that's not new prego does that

00:08:08,330 --> 00:08:13,699
think like a vertex it's message passing

00:08:11,090 --> 00:08:16,010
obviously because it's akka there's a

00:08:13,699 --> 00:08:18,289
streaming data flow paradigm we use

00:08:16,010 --> 00:08:20,599
Kafka as a mediator mediator for results

00:08:18,289 --> 00:08:22,820
because what we wanted to do is if you

00:08:20,599 --> 00:08:26,240
want to start emitting results in a

00:08:22,820 --> 00:08:28,430
stream from a graph processing then you

00:08:26,240 --> 00:08:31,310
need some way to capture that and people

00:08:28,430 --> 00:08:33,020
might want to replay that result set so

00:08:31,310 --> 00:08:35,750
we decided to use Kafka as a thing that

00:08:33,020 --> 00:08:38,209
captures those results the underlying

00:08:35,750 --> 00:08:39,890
algorithm is depth-first search and if

00:08:38,209 --> 00:08:42,050
you remember we talked about getting

00:08:39,890 --> 00:08:43,370
results fast that's one of the reasons

00:08:42,050 --> 00:08:45,020
because if we did breadth-first search

00:08:43,370 --> 00:08:46,640
then we would have to wait until we've

00:08:45,020 --> 00:08:48,920
covered the whole breadth of the graph

00:08:46,640 --> 00:08:51,290
until we produce results depth-first

00:08:48,920 --> 00:08:53,360
search allows us to get oh I have a

00:08:51,290 --> 00:08:56,750
pattern match I'm gonna admit the

00:08:53,360 --> 00:08:58,850
results immediately and the query that

00:08:56,750 --> 00:08:59,650
we're gonna look at is something along

00:08:58,850 --> 00:09:03,290
that line

00:08:59,650 --> 00:09:05,720
so count and then get the account ID and

00:09:03,290 --> 00:09:07,400
then match to some transaction that

00:09:05,720 --> 00:09:10,880
belongs to that account and then get to

00:09:07,400 --> 00:09:14,300
get the transaction amount for that so

00:09:10,880 --> 00:09:16,730
starting from the actor system we start

00:09:14,300 --> 00:09:19,280
off by basically creating we take a

00:09:16,730 --> 00:09:23,540
query and we split it up into the

00:09:19,280 --> 00:09:25,760
different steps that that query goes

00:09:23,540 --> 00:09:27,950
into for each one of those steps we

00:09:25,760 --> 00:09:30,350
basically create a pool of actors using

00:09:27,950 --> 00:09:33,200
a round-robin a round-robin pool for

00:09:30,350 --> 00:09:35,090
acha and store it within a hashmap that

00:09:33,200 --> 00:09:36,530
basically stores the reference to all of

00:09:35,090 --> 00:09:40,310
those actors or to in this case the

00:09:36,530 --> 00:09:43,010
router itself once we do that we

00:09:40,310 --> 00:09:44,780
basically start with a scan on the HBase

00:09:43,010 --> 00:09:48,320
table that is the first step of the

00:09:44,780 --> 00:09:50,240
graph okay so in this step we're

00:09:48,320 --> 00:09:53,090
basically saying get the current step

00:09:50,240 --> 00:09:55,610
where we are on query and then we're

00:09:53,090 --> 00:09:57,410
going to scan that table and as we're

00:09:55,610 --> 00:10:01,610
getting different nodes from the table

00:09:57,410 --> 00:10:04,640
we're gonna send those nodes to our pool

00:10:01,610 --> 00:10:06,710
of actors that belong to that step with

00:10:04,640 --> 00:10:08,810
the query and the query iterator which

00:10:06,710 --> 00:10:10,490
essentially provides some sort of path

00:10:08,810 --> 00:10:12,080
iterator and if you can think about it

00:10:10,490 --> 00:10:13,940
there are different steps in our query

00:10:12,080 --> 00:10:16,100
so you can move forward and you can move

00:10:13,940 --> 00:10:18,080
backwards and what that does is

00:10:16,100 --> 00:10:21,740
essentially starts the processing of the

00:10:18,080 --> 00:10:24,440
graph itself so very much like the top

00:10:21,740 --> 00:10:25,880
left here you have a queue or list of

00:10:24,440 --> 00:10:27,950
vertices that you need to visit those

00:10:25,880 --> 00:10:29,270
are generated by the first scan and then

00:10:27,950 --> 00:10:30,950
you have different actors that in a

00:10:29,270 --> 00:10:32,450
round-robin are receiving those vertices

00:10:30,950 --> 00:10:35,870
and are going to do something with them

00:10:32,450 --> 00:10:40,010
and we're gonna take a look at that so

00:10:35,870 --> 00:10:42,290
this is the node actor okay and what it

00:10:40,010 --> 00:10:45,230
does is basically receives a vertex and

00:10:42,290 --> 00:10:47,810
if it has some data that it needs to

00:10:45,230 --> 00:10:49,160
store that or if it has already some

00:10:47,810 --> 00:10:51,140
data then it's just going to continue

00:10:49,160 --> 00:10:53,030
getting that data and if it doesn't then

00:10:51,140 --> 00:10:55,280
it's going to create some sort of hash

00:10:53,030 --> 00:10:57,050
map to hold the data that you might want

00:10:55,280 --> 00:10:58,730
so all those properties that you asked

00:10:57,050 --> 00:11:01,450
for it when you were asking to do the

00:10:58,730 --> 00:11:03,900
traversal it's then going to say is

00:11:01,450 --> 00:11:06,930
there a next step to the pattern

00:11:03,900 --> 00:11:08,490
if there is a next step then let me

00:11:06,930 --> 00:11:11,430
create and let me go through the edges

00:11:08,490 --> 00:11:13,890
of that node and Amit Amit those

00:11:11,430 --> 00:11:16,230
vertices to the next set of workers and

00:11:13,890 --> 00:11:18,450
if there isn't then I've reached the end

00:11:16,230 --> 00:11:21,510
of the pattern and now I can omit this

00:11:18,450 --> 00:11:23,790
result right so depth-first search I

00:11:21,510 --> 00:11:25,830
started with a node if there are

00:11:23,790 --> 00:11:29,190
additional edges that match this pattern

00:11:25,830 --> 00:11:30,870
I'm going to proceed if there aren't I'm

00:11:29,190 --> 00:11:33,930
just going to return a result because

00:11:30,870 --> 00:11:38,940
I'm obviously part of the result set if

00:11:33,930 --> 00:11:41,130
I've received a vertex so when it sends

00:11:38,940 --> 00:11:44,250
the vertex back to the sender it's

00:11:41,130 --> 00:11:47,430
actually sending it over here to the

00:11:44,250 --> 00:11:49,529
original master or you know master node

00:11:47,430 --> 00:11:51,750
where it's basically getting the vertex

00:11:49,529 --> 00:11:54,690
and it's taking these step workers and

00:11:51,750 --> 00:11:56,760
it's gonna send to the next step to the

00:11:54,690 --> 00:11:58,710
next pool of actors that manage that

00:11:56,760 --> 00:12:02,190
stuff it's going to send them those set

00:11:58,710 --> 00:12:06,750
of vertices so if I were to try and

00:12:02,190 --> 00:12:09,089
visualize this we get a query we scan

00:12:06,750 --> 00:12:10,680
the first table once we've scanned the

00:12:09,089 --> 00:12:12,510
first table we generate basically an

00:12:10,680 --> 00:12:15,750
edge list or a vertice list that we need

00:12:12,510 --> 00:12:17,790
to visit we have groups of actors or

00:12:15,750 --> 00:12:20,310
pools of actors that basically take this

00:12:17,790 --> 00:12:22,500
queue and get it and as they are

00:12:20,310 --> 00:12:24,660
progressing they're basically moving

00:12:22,500 --> 00:12:26,760
from one step to the other until finally

00:12:24,660 --> 00:12:32,330
they say oh I'm at the end of my path

00:12:26,760 --> 00:12:35,880
let's push this result to Kafka so

00:12:32,330 --> 00:12:41,430
initial tests of this are roughly 8,000

00:12:35,880 --> 00:12:44,100
results a second for up to three hop

00:12:41,430 --> 00:12:46,890
queries meaning that if I did a pattern

00:12:44,100 --> 00:12:48,750
match I would start with node label one

00:12:46,890 --> 00:12:51,839
I would go to node label two I would go

00:12:48,750 --> 00:12:54,029
to node node label three with that three

00:12:51,839 --> 00:12:56,430
hops that's what we've managed to see as

00:12:54,029 --> 00:12:57,870
initial tests but that's highly

00:12:56,430 --> 00:12:59,550
dependent on HBase so some of the

00:12:57,870 --> 00:13:02,250
benefits of using HBase is that it's

00:12:59,550 --> 00:13:04,100
HDFS under the hood meaning that we

00:13:02,250 --> 00:13:07,950
already have a resilient distributed

00:13:04,100 --> 00:13:11,160
store of data HBase is also distributed

00:13:07,950 --> 00:13:13,950
it it can optimize itself to load things

00:13:11,160 --> 00:13:16,260
into memory as needed but the problem

00:13:13,950 --> 00:13:17,760
with this is that HBase becomes a

00:13:16,260 --> 00:13:20,220
bottleneck

00:13:17,760 --> 00:13:22,020
because of the design wherever every

00:13:20,220 --> 00:13:24,540
node that we visit we need to go and get

00:13:22,020 --> 00:13:26,640
some information from HBase this is

00:13:24,540 --> 00:13:29,160
going to be a bottleneck for us

00:13:26,640 --> 00:13:31,650
in addition messages flow through the

00:13:29,160 --> 00:13:34,890
master as we saw right in the actor we

00:13:31,650 --> 00:13:37,020
actually sent the vert the next vertex

00:13:34,890 --> 00:13:40,740
to visit to the sender which is the

00:13:37,020 --> 00:13:42,270
master now Anaka actors themselves are

00:13:40,740 --> 00:13:44,490
not optimized for routing messages

00:13:42,270 --> 00:13:46,560
that's why we have routers they've been

00:13:44,490 --> 00:13:48,300
built specifically to do that so it's

00:13:46,560 --> 00:13:49,920
not a wise idea to send everything

00:13:48,300 --> 00:13:52,350
through the master but because we wanted

00:13:49,920 --> 00:13:54,810
to segment the workers based on the

00:13:52,350 --> 00:13:56,430
steps we had to do that right the master

00:13:54,810 --> 00:13:58,410
itself was the one that contained all

00:13:56,430 --> 00:14:01,290
these different groups of actors that

00:13:58,410 --> 00:14:03,360
need it to do that so what can we do to

00:14:01,290 --> 00:14:05,160
to improve this a little bit well we can

00:14:03,360 --> 00:14:08,160
reduce the number of gets to HBase we

00:14:05,160 --> 00:14:09,480
can instead of saying for every for

00:14:08,160 --> 00:14:11,339
every edge we're gonna see we're gonna

00:14:09,480 --> 00:14:13,920
do one get we're gonna group all the

00:14:11,339 --> 00:14:16,470
edges per node that we visited and then

00:14:13,920 --> 00:14:18,750
make one get for all of those things we

00:14:16,470 --> 00:14:21,480
can also do the alternative and do a

00:14:18,750 --> 00:14:23,220
scan but then that introduces some

00:14:21,480 --> 00:14:25,890
latency because we need to wait until

00:14:23,220 --> 00:14:30,570
we've had quite a few nodes to visit

00:14:25,890 --> 00:14:32,160
until we initiate the scan one thing you

00:14:30,570 --> 00:14:34,380
can also do is of course lift things

00:14:32,160 --> 00:14:36,600
into Aden to memory within HBase and

00:14:34,380 --> 00:14:38,459
that will improve performance and you

00:14:36,600 --> 00:14:40,350
can say okay I'm gonna use Kafka not

00:14:38,459 --> 00:14:42,510
only as a place to get the results from

00:14:40,350 --> 00:14:44,580
but I'm gonna use it to cache so I can

00:14:42,510 --> 00:14:46,260
give it some some sort of life span

00:14:44,580 --> 00:14:48,029
where I'm gonna keep the data in there

00:14:46,260 --> 00:14:49,470
and every time that somebody hits the

00:14:48,029 --> 00:14:52,410
same query again I'm just going to

00:14:49,470 --> 00:14:54,330
replay them the same topic and that will

00:14:52,410 --> 00:14:56,459
allow me to not have to traverse this

00:14:54,330 --> 00:14:58,230
again

00:14:56,459 --> 00:15:00,060
we can also add workers to the steps

00:14:58,230 --> 00:15:01,770
dynamically if need be if we're seeing

00:15:00,060 --> 00:15:05,190
that they're they're getting too many

00:15:01,770 --> 00:15:08,550
messages to process from a reliability

00:15:05,190 --> 00:15:11,250
perspective akka does not does not

00:15:08,550 --> 00:15:14,250
actually support it implements at most

00:15:11,250 --> 00:15:16,290
once out of the box for delivery which

00:15:14,250 --> 00:15:17,610
means that for net splits or faulty

00:15:16,290 --> 00:15:20,660
nodes or things like that

00:15:17,610 --> 00:15:22,890
you're gonna lose results right I'm

00:15:20,660 --> 00:15:25,230
traversing the graph and I get to one

00:15:22,890 --> 00:15:27,720
actor and that actor wants to send edges

00:15:25,230 --> 00:15:30,720
to the next actor and all of a sudden it

00:15:27,720 --> 00:15:32,579
can't reach it so what happens to that

00:15:30,720 --> 00:15:34,230
to that message does it just go away am

00:15:32,579 --> 00:15:36,060
I not going to get that result that's

00:15:34,230 --> 00:15:38,370
gonna be a major problem well luckily

00:15:36,060 --> 00:15:39,930
there are many implementations of at

00:15:38,370 --> 00:15:42,779
least once delivery and exactly once

00:15:39,930 --> 00:15:44,550
delivery with Anaka so we can go and

00:15:42,779 --> 00:15:46,769
grab that and implement it but that

00:15:44,550 --> 00:15:51,810
would be an addition to what akka

00:15:46,769 --> 00:15:55,459
implements natively so we know that the

00:15:51,810 --> 00:15:58,379
major problem with this one is that it's

00:15:55,459 --> 00:16:00,420
it's bottleneck as HBase you have to

00:15:58,379 --> 00:16:03,269
continuously go get the information from

00:16:00,420 --> 00:16:08,189
HBase so what if we just lifted the

00:16:03,269 --> 00:16:11,189
entire thing into memory this one is

00:16:08,189 --> 00:16:14,069
relatively similar but it's different in

00:16:11,189 --> 00:16:17,310
the way that instead of going through a

00:16:14,069 --> 00:16:19,920
queue of getting all these getting all

00:16:17,310 --> 00:16:23,069
these this data from HBase we actually

00:16:19,920 --> 00:16:24,600
have the entire graph in memory the

00:16:23,069 --> 00:16:27,029
messages that are being passed whereas

00:16:24,600 --> 00:16:28,889
before we were passing vertices so this

00:16:27,029 --> 00:16:31,800
is the next vertex vertex free to visit

00:16:28,889 --> 00:16:33,839
the one that we have now is edge based

00:16:31,800 --> 00:16:35,610
so I'm going to send you an edge and

00:16:33,839 --> 00:16:38,430
then you're gonna do some processing

00:16:35,610 --> 00:16:41,579
based on the edge and it's it's a

00:16:38,430 --> 00:16:46,740
distributed in memory graph at the end

00:16:41,579 --> 00:16:49,620
of the day okay so how do you actually

00:16:46,740 --> 00:16:52,410
make it reasonable to store a graph in

00:16:49,620 --> 00:16:55,050
memory you can't obviously fit it in one

00:16:52,410 --> 00:16:57,269
machine if you have a lot of data and if

00:16:55,050 --> 00:16:58,980
you consistently just loaded the same

00:16:57,269 --> 00:17:01,589
data again and again and again you had

00:16:58,980 --> 00:17:03,589
to partition it in some way well luckily

00:17:01,589 --> 00:17:06,809
akka pulls through because it has a

00:17:03,589 --> 00:17:08,429
consistent hashing router and what's

00:17:06,809 --> 00:17:11,850
nice about consistent hashing is it

00:17:08,429 --> 00:17:14,819
basically says if you add more actors or

00:17:11,850 --> 00:17:16,850
more buckets or places where I can place

00:17:14,819 --> 00:17:20,309
this data I'm not going to have to move

00:17:16,850 --> 00:17:23,039
every single key in the hash table I'm

00:17:20,309 --> 00:17:25,439
only gonna have to move K over N which

00:17:23,039 --> 00:17:27,480
is essentially the number of keys versus

00:17:25,439 --> 00:17:31,289
the number of ones that were the the set

00:17:27,480 --> 00:17:33,150
of keys that were changed and that's on

00:17:31,289 --> 00:17:34,950
average but that means that we can

00:17:33,150 --> 00:17:37,350
efficiently or relatively efficiently

00:17:34,950 --> 00:17:39,059
start to load this entire graph into

00:17:37,350 --> 00:17:41,730
memory by using this consistent hashing

00:17:39,059 --> 00:17:44,610
where if you imagine these buckets being

00:17:41,730 --> 00:17:46,470
servers or actors each one of those act

00:17:44,610 --> 00:17:49,620
is holding a bunch of those nodes in

00:17:46,470 --> 00:17:51,120
memory but if it wants to then if now

00:17:49,620 --> 00:17:53,040
I'm going to try and find this other

00:17:51,120 --> 00:17:55,500
node I'm just using this consistent

00:17:53,040 --> 00:17:57,450
hashing actor to go to the next step and

00:17:55,500 --> 00:18:00,990
it knows where to go to because of that

00:17:57,450 --> 00:18:02,790
consistent hashing and of course this

00:18:00,990 --> 00:18:04,920
router could easily be a cluster way

00:18:02,790 --> 00:18:10,200
around because that's natively

00:18:04,920 --> 00:18:12,299
implemented within maka so if you think

00:18:10,200 --> 00:18:15,000
about it the consistent hashing pool is

00:18:12,299 --> 00:18:16,799
sort of the graph address book it knows

00:18:15,000 --> 00:18:18,630
or through that algorithm we know

00:18:16,799 --> 00:18:20,160
exactly where the nodes are stored on

00:18:18,630 --> 00:18:22,590
different actors across the cluster

00:18:20,160 --> 00:18:25,290
across whatever servers you may have

00:18:22,590 --> 00:18:26,820
nodes on the graph have a location it's

00:18:25,290 --> 00:18:27,840
a very simplistic one it's just a bunch

00:18:26,820 --> 00:18:30,480
of strings that are concatenated

00:18:27,840 --> 00:18:33,090
together and the next thing that you'll

00:18:30,480 --> 00:18:36,030
see here is actually that you just have

00:18:33,090 --> 00:18:38,390
one one router in this case I gave it

00:18:36,030 --> 00:18:41,220
ten actors in a consistent hashing pool

00:18:38,390 --> 00:18:42,900
it's called the graph and we implement

00:18:41,220 --> 00:18:43,470
this thing and it basically changes

00:18:42,900 --> 00:18:45,809
everything

00:18:43,470 --> 00:18:48,150
now we have everything in memory and we

00:18:45,809 --> 00:18:50,790
can start to traverse the graph in

00:18:48,150 --> 00:18:53,640
memory as well but the same idea and ply

00:18:50,790 --> 00:18:56,250
applies there's still data coming into

00:18:53,640 --> 00:18:58,260
HBase so there might be new nodes that

00:18:56,250 --> 00:19:01,890
I'm not aware of so I have to start by

00:18:58,260 --> 00:19:03,690
scanning that table right if if the

00:19:01,890 --> 00:19:05,850
graph is populated and everything is in

00:19:03,690 --> 00:19:07,770
memory but data has changed in the

00:19:05,850 --> 00:19:09,210
underlying data store which is HBase I

00:19:07,770 --> 00:19:11,940
still need to start from the initial

00:19:09,210 --> 00:19:13,860
scan of that first table when I'm doing

00:19:11,940 --> 00:19:18,299
my traversal otherwise I might miss new

00:19:13,860 --> 00:19:22,169
data that has come in the actor node

00:19:18,299 --> 00:19:23,520
itself only gets two changes well the

00:19:22,169 --> 00:19:26,400
first thing is that we implement a very

00:19:23,520 --> 00:19:28,260
simplistic cache that says first let me

00:19:26,400 --> 00:19:30,090
check if I have this in memory if I

00:19:28,260 --> 00:19:32,580
don't then let me go to HBase and get

00:19:30,090 --> 00:19:34,440
that data and the second thing is that

00:19:32,580 --> 00:19:37,830
instead of sending it through some sort

00:19:34,440 --> 00:19:39,990
of master actor that contains that that

00:19:37,830 --> 00:19:42,440
has to route everything I'm actually

00:19:39,990 --> 00:19:45,450
sending it directly to the router itself

00:19:42,440 --> 00:19:49,620
which depending on how much you've used

00:19:45,450 --> 00:19:51,690
akka routers in your own work you might

00:19:49,620 --> 00:19:54,270
know that actor that akka routers

00:19:51,690 --> 00:19:56,040
actually don't don't don't do the

00:19:54,270 --> 00:19:57,500
routing themselves they actually hand

00:19:56,040 --> 00:19:59,540
over their code

00:19:57,500 --> 00:20:01,490
to the different actors that are going

00:19:59,540 --> 00:20:03,260
to implement them so what you get from

00:20:01,490 --> 00:20:05,390
that is essentially that these nodes are

00:20:03,260 --> 00:20:08,570
talking to each other they don't have to

00:20:05,390 --> 00:20:09,920
go to hops to talk to the next node that

00:20:08,570 --> 00:20:12,380
they need to talk to they just talk

00:20:09,920 --> 00:20:15,710
immediately to one another using this

00:20:12,380 --> 00:20:17,900
consistent hashing algorithm so if I

00:20:15,710 --> 00:20:20,990
were to try and visualize this we get

00:20:17,900 --> 00:20:23,180
the query we scan the first table we see

00:20:20,990 --> 00:20:25,640
what matches those labels within the

00:20:23,180 --> 00:20:27,020
nodes and then essentially all of the

00:20:25,640 --> 00:20:30,290
actors in our system are just going to

00:20:27,020 --> 00:20:33,350
talk to one another and these vectors

00:20:30,290 --> 00:20:35,690
are gonna just continue to pass through

00:20:33,350 --> 00:20:38,360
until one of them says yes I've I've

00:20:35,690 --> 00:20:39,830
gotten to the end of a result and now I

00:20:38,360 --> 00:20:43,490
have a result and I'm just gonna admit

00:20:39,830 --> 00:20:46,490
it so this is really cool because you

00:20:43,490 --> 00:20:48,680
can't you so far whether you use neo4j

00:20:46,490 --> 00:20:49,760
or other things those are not

00:20:48,680 --> 00:20:51,560
necessarily distributed and you

00:20:49,760 --> 00:20:55,040
definitely don't have in-memory graph

00:20:51,560 --> 00:20:56,750
processing that is relatively efficient

00:20:55,040 --> 00:20:59,060
when you're trying to add notes to the

00:20:56,750 --> 00:21:01,400
graph or if you know one of the server's

00:20:59,060 --> 00:21:03,200
shuts down or if another server spins up

00:21:01,400 --> 00:21:05,350
they don't have to migrate the graph

00:21:03,200 --> 00:21:08,030
completely in order to fit that new

00:21:05,350 --> 00:21:09,860
those new servers you can just do for

00:21:08,030 --> 00:21:13,520
the portion that has changed within the

00:21:09,860 --> 00:21:15,950
consistent hashing mechanism of course

00:21:13,520 --> 00:21:18,590
there's still some problems in here but

00:21:15,950 --> 00:21:20,570
those can be relatively I mean there's

00:21:18,590 --> 00:21:23,360
definitely ways to to go forward with

00:21:20,570 --> 00:21:26,830
that so initial tests for the three hops

00:21:23,360 --> 00:21:29,870
show us roughly 75,000 results a second

00:21:26,830 --> 00:21:31,820
and that's after the first load into

00:21:29,870 --> 00:21:34,220
memory so the way that this works is

00:21:31,820 --> 00:21:35,990
essentially if you go back to here again

00:21:34,220 --> 00:21:38,180
if I don't have the vertex in memory

00:21:35,990 --> 00:21:41,840
then I'm just going to go and get it

00:21:38,180 --> 00:21:44,780
from HBase so once I have everything in

00:21:41,840 --> 00:21:46,880
memory then it's just much faster so

00:21:44,780 --> 00:21:49,820
this this number is basically after

00:21:46,880 --> 00:21:51,860
we've loaded everything into memory the

00:21:49,820 --> 00:21:53,840
messages go directly between actors

00:21:51,860 --> 00:21:57,290
which means that it's much more scalable

00:21:53,840 --> 00:22:00,470
and performant and we only use HBase

00:21:57,290 --> 00:22:02,540
when the node is non memory or an actor

00:22:00,470 --> 00:22:04,610
dies and now you need to have a new

00:22:02,540 --> 00:22:07,070
actor that represents that set of the

00:22:04,610 --> 00:22:09,590
hash bucket then you'd need to load from

00:22:07,070 --> 00:22:10,820
memory again the reliability is still

00:22:09,590 --> 00:22:13,309
the same as the first

00:22:10,820 --> 00:22:17,000
architecture because we only have at

00:22:13,309 --> 00:22:18,710
most once delivery but you can get to

00:22:17,000 --> 00:22:21,110
that exactly once delivery that we

00:22:18,710 --> 00:22:23,090
talked about earlier and there's one

00:22:21,110 --> 00:22:25,009
question here well what about if I just

00:22:23,090 --> 00:22:28,789
loaded the entire graph into memory

00:22:25,009 --> 00:22:34,279
would that be nicer so I'm actually

00:22:28,789 --> 00:22:38,230
going to show a quick demo welcome to

00:22:34,279 --> 00:22:41,450
akka graph and I'm going to load a

00:22:38,230 --> 00:22:43,490
database into memory what it's going to

00:22:41,450 --> 00:22:46,309
do now so this is an actually slightly

00:22:43,490 --> 00:22:48,679
more improved version than what I showed

00:22:46,309 --> 00:22:51,080
in terms of the code so it does load

00:22:48,679 --> 00:22:53,090
everything into memory up front and it

00:22:51,080 --> 00:22:55,960
does that by scanning the HBase tables

00:22:53,090 --> 00:23:04,009
and then just putting them into memory

00:22:55,960 --> 00:23:06,379
sorry one second there's quite a lot of

00:23:04,009 --> 00:23:12,559
data in there so I need to give it just

00:23:06,379 --> 00:23:14,570
a little bit more memory how much data

00:23:12,559 --> 00:23:21,169
in here I'll tell you in a second it's

00:23:14,570 --> 00:23:23,590
gonna show sorry I can hear oh yeah

00:23:21,169 --> 00:23:23,590
larger

00:23:28,820 --> 00:23:37,110
so remember that each of these each of

00:23:34,170 --> 00:23:39,390
these nodes has properties in them so

00:23:37,110 --> 00:23:40,620
it's not only storing the reference to

00:23:39,390 --> 00:23:42,150
the node itself in memory but it's

00:23:40,620 --> 00:23:44,460
actually storing the data of those nodes

00:23:42,150 --> 00:23:46,140
in this one we have roughly seven

00:23:44,460 --> 00:23:51,330
hundred and twenty-seven thousand nodes

00:23:46,140 --> 00:23:53,010
and 1.2 million edges and what we can do

00:23:51,330 --> 00:23:55,500
is we can start by a simple query right

00:23:53,010 --> 00:23:58,590
we can say let me get this account and

00:23:55,500 --> 00:24:01,560
the account ID and I'm gonna get the

00:23:58,590 --> 00:24:05,540
transaction and the transaction amount

00:24:01,560 --> 00:24:07,530
and obviously less efficient than

00:24:05,540 --> 00:24:10,770
pushing it into Kafka but I'm just

00:24:07,530 --> 00:24:12,210
printing this into screen and these are

00:24:10,770 --> 00:24:14,730
all results of things that have

00:24:12,210 --> 00:24:16,500
basically been produced from that query

00:24:14,730 --> 00:24:20,250
on the graph where all these nodes are

00:24:16,500 --> 00:24:21,810
just talking to each other so if I

00:24:20,250 --> 00:24:25,740
wanted to just show everything that's in

00:24:21,810 --> 00:24:33,900
the graph and of course this is fake

00:24:25,740 --> 00:24:36,960
data well many more fields than what I

00:24:33,900 --> 00:24:40,230
just showed right but this is just

00:24:36,960 --> 00:24:41,760
producing those results and allows us to

00:24:40,230 --> 00:24:44,160
see sort of the power where we can

00:24:41,760 --> 00:24:45,690
easily match this or we can easily do

00:24:44,160 --> 00:24:48,150
pattern matching and get all this data

00:24:45,690 --> 00:24:49,620
back and this entire graph is in memory

00:24:48,150 --> 00:24:52,560
and of course I can make it distribute

00:24:49,620 --> 00:24:54,660
it I can make it cluster and I can

00:24:52,560 --> 00:24:58,410
manage the idea of a node dying or a

00:24:54,660 --> 00:25:03,900
node coming back up you might have

00:24:58,410 --> 00:25:06,770
noticed that there's a question mark of

00:25:03,900 --> 00:25:10,050
when does it actually end processing and

00:25:06,770 --> 00:25:11,880
we it's really relatively easy for us as

00:25:10,050 --> 00:25:14,610
humans to notice that because no more

00:25:11,880 --> 00:25:16,380
data is being printed to screen but for

00:25:14,610 --> 00:25:18,660
a machine it's a little bit more

00:25:16,380 --> 00:25:21,090
difficult so a couple of ideas there

00:25:18,660 --> 00:25:23,760
well we could count the number of nodes

00:25:21,090 --> 00:25:25,710
that we intend to visit and check that

00:25:23,760 --> 00:25:27,630
against the number of nodes that we have

00:25:25,710 --> 00:25:30,690
visited and we could also do that per

00:25:27,630 --> 00:25:32,460
step but that's not that great in a

00:25:30,690 --> 00:25:34,020
non-deterministic system we could have a

00:25:32,460 --> 00:25:36,600
race condition of some sort where

00:25:34,020 --> 00:25:38,640
something actually gets the count up and

00:25:36,600 --> 00:25:41,210
it's it matches the number of things and

00:25:38,640 --> 00:25:43,590
we think oh this is the end of the query

00:25:41,210 --> 00:25:45,690
but if we actually go towards

00:25:43,590 --> 00:25:47,970
implementing exactly once delivery then

00:25:45,690 --> 00:25:51,150
we can actually get a distributed task

00:25:47,970 --> 00:25:53,760
list of the nose notes to visit in the

00:25:51,150 --> 00:25:56,190
memory within within the actors

00:25:53,760 --> 00:25:59,100
themselves so if I'm an actor and I'm

00:25:56,190 --> 00:26:01,409
trying to think okay well I've sent

00:25:59,100 --> 00:26:03,510
these edges to this to this other actor

00:26:01,409 --> 00:26:06,090
and I'm waiting for it to respond with

00:26:03,510 --> 00:26:07,919
the list of edges that it's visited I

00:26:06,090 --> 00:26:10,650
can hold that in memory and until I've

00:26:07,919 --> 00:26:13,710
resolved all of my tasks that I've given

00:26:10,650 --> 00:26:17,549
out I'm not going to finish now that

00:26:13,710 --> 00:26:18,870
might sound like ask with innaka but

00:26:17,549 --> 00:26:20,970
there are some problems with

00:26:18,870 --> 00:26:22,620
implementing it that way there are

00:26:20,970 --> 00:26:25,260
better ways where we can just do this

00:26:22,620 --> 00:26:28,080
with message passing with tell instead

00:26:25,260 --> 00:26:29,669
of ask and a get better performance so

00:26:28,080 --> 00:26:32,820
long as you're managing this task list

00:26:29,669 --> 00:26:34,230
and it doesn't require that you have to

00:26:32,820 --> 00:26:35,970
hold the entire graph in memory of

00:26:34,230 --> 00:26:39,770
course it's just the task that you sent

00:26:35,970 --> 00:26:44,190
out and actually quite nicely we can see

00:26:39,770 --> 00:26:46,020
in here so I've given 50 actors to this

00:26:44,190 --> 00:26:49,380
graph each one I have each one of them

00:26:46,020 --> 00:26:53,240
has a set of load to them so based on

00:26:49,380 --> 00:26:56,610
the distributed hash this one has 16,000

00:26:53,240 --> 00:26:58,679
this number of queries or messages that

00:26:56,610 --> 00:27:03,390
pass through it and this is a number of

00:26:58,679 --> 00:27:05,520
results that it produced and we can use

00:27:03,390 --> 00:27:12,059
that information to further optimize so

00:27:05,520 --> 00:27:12,840
we'll talk about that in a second okay

00:27:12,059 --> 00:27:15,690
so what next

00:27:12,840 --> 00:27:19,260
well how about getting rid of HBase

00:27:15,690 --> 00:27:20,880
altogether right we're using HBase right

00:27:19,260 --> 00:27:24,870
now to store the data we do need a

00:27:20,880 --> 00:27:26,850
persistent store but do we have to rely

00:27:24,870 --> 00:27:28,500
on it being the first thing that

00:27:26,850 --> 00:27:30,899
actually gets the data or could we just

00:27:28,500 --> 00:27:34,679
push all data first to the Aqua graph

00:27:30,899 --> 00:27:36,179
and then have that persist well why

00:27:34,679 --> 00:27:38,580
don't you mutate the graph in memory

00:27:36,179 --> 00:27:40,529
first maybe implement the wall right

00:27:38,580 --> 00:27:43,710
ahead log that's very similar to what

00:27:40,529 --> 00:27:45,289
it's what Cassandra and HBase do maybe

00:27:43,710 --> 00:27:47,940
we can use a cup persistence for this

00:27:45,289 --> 00:27:49,230
maybe we use something like HBase or

00:27:47,940 --> 00:27:51,090
Cassandra or something else but just to

00:27:49,230 --> 00:27:54,210
store the actor State

00:27:51,090 --> 00:27:58,350
not necessarily to store the data aside

00:27:54,210 --> 00:27:59,730
from that state another interesting

00:27:58,350 --> 00:28:02,250
thing that we could do is you looked at

00:27:59,730 --> 00:28:03,780
some of those statistics about adding or

00:28:02,250 --> 00:28:06,150
the number of messages that flowed

00:28:03,780 --> 00:28:07,410
through or the number of nodes in the

00:28:06,150 --> 00:28:09,540
graph that I'm actually holding as a

00:28:07,410 --> 00:28:11,610
single actor what we can optimize based

00:28:09,540 --> 00:28:13,800
on that I could say I don't want to hold

00:28:11,610 --> 00:28:16,500
in a single actor more than X number of

00:28:13,800 --> 00:28:20,580
nodes so if I find that let me add

00:28:16,500 --> 00:28:22,140
another route T to the kosis and there's

00:28:20,580 --> 00:28:27,480
many more options for what we can do

00:28:22,140 --> 00:28:29,160
here so to finish up my talk before we

00:28:27,480 --> 00:28:32,370
go into sort of the question and answer

00:28:29,160 --> 00:28:33,750
model we are hiring we build data

00:28:32,370 --> 00:28:35,400
products if you're interested in things

00:28:33,750 --> 00:28:38,250
like this without go Scala and play it's

00:28:35,400 --> 00:28:41,910
a shameless plug but here we are we are

00:28:38,250 --> 00:28:43,740
hiring people and now for questions and

00:28:41,910 --> 00:28:48,810
answers I think we have roughly 10

00:28:43,740 --> 00:28:55,080
minutes left yeah sorry I couldn't hear

00:28:48,810 --> 00:29:04,310
you so it will be it's roughly 200

00:28:55,080 --> 00:29:04,310
billion nodes yeah

00:29:10,720 --> 00:29:13,320
yeah

00:29:14,870 --> 00:29:22,519
yeah so the question is actually one

00:29:17,570 --> 00:29:25,669
that I guessed we should address so why

00:29:22,519 --> 00:29:28,629
not tinker pop why not gremlin why not

00:29:25,669 --> 00:29:31,249
graphics why not all these things well I

00:29:28,629 --> 00:29:32,600
think that gremlin has a DSL for

00:29:31,249 --> 00:29:34,970
traversing graphs is actually a great

00:29:32,600 --> 00:29:37,610
DSL under the hood though it's

00:29:34,970 --> 00:29:40,129
relatively tied to tinker pop as a stack

00:29:37,610 --> 00:29:41,809
which doesn't include time series and it

00:29:40,129 --> 00:29:43,789
doesn't do streaming data flow it does

00:29:41,809 --> 00:29:45,980
do data flow but it doesn't do streaming

00:29:43,789 --> 00:29:47,869
data flow in the sense that I can't just

00:29:45,980 --> 00:29:49,970
start to emit results immediately they

00:29:47,869 --> 00:29:53,210
have to be batched up and then and then

00:29:49,970 --> 00:29:55,759
deliver the results so will we ever go

00:29:53,210 --> 00:29:57,860
into a DSL that's like gremlin or like

00:29:55,759 --> 00:29:58,730
cypher or anything else yeah I think I

00:29:57,860 --> 00:30:02,629
think that's definitely a possibility

00:29:58,730 --> 00:30:04,850
but we haven't implemented that yet for

00:30:02,629 --> 00:30:07,369
from a graph processing frameworks I

00:30:04,850 --> 00:30:08,509
mean you can run PageRank and and

00:30:07,369 --> 00:30:10,220
connected components and all these

00:30:08,509 --> 00:30:13,279
things but remember that our premise was

00:30:10,220 --> 00:30:14,840
let me get the data to the people first

00:30:13,279 --> 00:30:15,919
and and then they can do whatever they

00:30:14,840 --> 00:30:17,659
want with it

00:30:15,919 --> 00:30:19,700
eventually we will implement these graph

00:30:17,659 --> 00:30:21,619
these graph algorithms but first let's

00:30:19,700 --> 00:30:23,179
pattern match and get people the the

00:30:21,619 --> 00:30:30,009
actual data that fits what they were

00:30:23,179 --> 00:30:30,009
looking for other questions yeah

00:30:41,780 --> 00:30:45,890
the connector is lonely

00:30:46,120 --> 00:30:52,700
at the same time then do you keep them a

00:30:49,490 --> 00:30:55,010
mapper accurate to say I have ten active

00:30:52,700 --> 00:30:56,840
queries and then each of them I am going

00:30:55,010 --> 00:31:01,460
through five address for example so I'm

00:30:56,840 --> 00:31:03,790
keeping yeah that's a good question

00:31:01,460 --> 00:31:08,890
let's see if it's in here

00:31:03,790 --> 00:31:11,990
yeah this is just a very you know

00:31:08,890 --> 00:31:14,740
illustrative example but at the master

00:31:11,990 --> 00:31:18,230
at the master or the query engine actor

00:31:14,740 --> 00:31:20,210
we would store the query jobs each query

00:31:18,230 --> 00:31:23,840
job essentially creates a query object

00:31:20,210 --> 00:31:27,320
that has an ID that query object when

00:31:23,840 --> 00:31:28,820
it's getting past the vertices in here

00:31:27,320 --> 00:31:30,290
could actually and that's not

00:31:28,820 --> 00:31:32,480
implemented on the screen but I'm just

00:31:30,290 --> 00:31:34,880
showing it in theory that query object

00:31:32,480 --> 00:31:37,580
could be stored in some sort of task

00:31:34,880 --> 00:31:39,770
list so per query job here's my task

00:31:37,580 --> 00:31:42,110
list and then I'm waiting for all these

00:31:39,770 --> 00:31:44,660
tasks to be finished once it's finished

00:31:42,110 --> 00:31:46,730
I'm going to say I'm done with all my

00:31:44,660 --> 00:31:49,130
tests for this query job and then the

00:31:46,730 --> 00:31:51,890
master knows okay well it's it's

00:31:49,130 --> 00:31:54,740
resolved the entire query I can now say

00:31:51,890 --> 00:31:57,530
this query job is done I'm still going

00:31:54,740 --> 00:31:59,180
to produce the results in a stream but

00:31:57,530 --> 00:32:00,740
I'm at least going to check whether or

00:31:59,180 --> 00:32:02,720
not it's it's completed its entire

00:32:00,740 --> 00:32:04,990
traversal does that make sense make

00:32:02,720 --> 00:32:04,990
sense

00:32:08,590 --> 00:32:13,090
you sell you you some video to two

00:32:10,750 --> 00:32:16,090
different ology so what took do you use

00:32:13,090 --> 00:32:17,039
and is that our ontology or what kind of

00:32:16,090 --> 00:32:21,520
ontology is it

00:32:17,039 --> 00:32:25,179
yeah so we built our own just internally

00:32:21,520 --> 00:32:27,429
using d3 and a bunch of other things

00:32:25,179 --> 00:32:29,980
right now it doesn't produce an owl but

00:32:27,429 --> 00:32:31,900
we want to move to that some of the

00:32:29,980 --> 00:32:33,909
reason is just because vowels don't

00:32:31,900 --> 00:32:35,740
inherently connect to the data source

00:32:33,909 --> 00:32:37,900
that we want to pull the data from so

00:32:35,740 --> 00:32:40,179
the logic within Athena which is what

00:32:37,900 --> 00:32:42,460
I've been talking about is that you want

00:32:40,179 --> 00:32:44,740
to connect you want to build the

00:32:42,460 --> 00:32:46,330
ontology using fields that actually

00:32:44,740 --> 00:32:49,539
exist out there in the different data

00:32:46,330 --> 00:32:51,370
sets so will eventually move towards owl

00:32:49,539 --> 00:32:53,140
but we don't have that ability yet

00:32:51,370 --> 00:32:57,690
and then there was another person right

00:32:53,140 --> 00:33:04,750
there do you run this on your own

00:32:57,690 --> 00:33:07,179
infrastructure yeah so ec2 is where

00:33:04,750 --> 00:33:11,649
we're on we've actually migrated a lot

00:33:07,179 --> 00:33:18,570
of things into ec2 recently other

00:33:11,649 --> 00:33:18,570

YouTube URL: https://www.youtube.com/watch?v=WMlt4EB5gUA


