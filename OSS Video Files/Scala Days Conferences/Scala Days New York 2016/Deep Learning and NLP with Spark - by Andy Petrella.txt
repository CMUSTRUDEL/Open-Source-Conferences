Title: Deep Learning and NLP with Spark - by Andy Petrella
Publication date: 2016-06-17
Playlist: Scala Days New York 2016
Description: 
	This talk was recorded at Scala Days New York, 2016. Follow along on Twitter @scaladays and on the website for more information http://scaladays.org/.

Abstract:
Deep Learning is taking data science by storm. Unfortunately, most existing solutions aren’t particularly scalable.In this talk, Andy will show how to easily implement a Spark­ ready version one of the most complex deep learning models, the Long Short­ Term Memory (LSTM) neural network, widely used in the hardest natural language processing and understanding problems, such as automatic summarization, machine translation, question answering and discourse. He also shows an LSTM demo with interactive, real­time visualizations using the Spark Notebook and Spark Streaming.
Captions: 
	00:00:01,139 --> 00:00:07,990
okay hi everyone my name is Susan your

00:00:05,170 --> 00:00:10,420
ally and I work at a startup called sky

00:00:07,990 --> 00:00:13,420
mind in San Francisco

00:00:10,420 --> 00:00:15,509
I hope sky mine rings a bell I hope you

00:00:13,420 --> 00:00:19,510
guys have heard of Skynet and Terminator

00:00:15,509 --> 00:00:22,210
no Terminator fans in the audience okay

00:00:19,510 --> 00:00:26,199
well if it rings the village should and

00:00:22,210 --> 00:00:28,000
if you think about what Skynet is and

00:00:26,199 --> 00:00:30,460
you think about our name it shouldn't

00:00:28,000 --> 00:00:32,949
tell you along with the very giant

00:00:30,460 --> 00:00:37,239
heading over here what we do this is

00:00:32,949 --> 00:00:39,460
Andy and annual introduce himself so I

00:00:37,239 --> 00:00:42,309
want gonna start up that far in Belgium

00:00:39,460 --> 00:00:46,719
you know control beers chocolate and so

00:00:42,309 --> 00:00:48,399
on solar startup is an idea around it's

00:00:46,719 --> 00:00:51,579
by notebook and making you know data

00:00:48,399 --> 00:00:54,670
scientist predictive and in enterprise

00:00:51,579 --> 00:00:57,910
environment and yeah so we're teaming up

00:00:54,670 --> 00:01:00,280
with sky mine to to do some NLP using

00:00:57,910 --> 00:01:03,129
deepening 4G and this is the topic of

00:01:00,280 --> 00:01:06,640
this talk so I will leave Susan

00:01:03,129 --> 00:01:08,710
introducing all the concepts behind it

00:01:06,640 --> 00:01:10,630
and then I will come afterwards with an

00:01:08,710 --> 00:01:12,880
example in that book that won't be able

00:01:10,630 --> 00:01:16,509
to run because I mean it would take time

00:01:12,880 --> 00:01:21,640
of course but I will at least show it

00:01:16,509 --> 00:01:22,810
can be done okay okay great so um what

00:01:21,640 --> 00:01:24,880
do you guys think of when you think

00:01:22,810 --> 00:01:27,549
about machine learning other than

00:01:24,880 --> 00:01:29,799
terminator what do you think we can do

00:01:27,549 --> 00:01:33,340
with machine learning but where do you

00:01:29,799 --> 00:01:34,360
think we are no show of hands nope no

00:01:33,340 --> 00:01:42,960
thoughts on the matter

00:01:34,360 --> 00:01:46,439
yeah go ahead yes that's true it's true

00:01:42,960 --> 00:01:50,140
well I put the picture of course yeah

00:01:46,439 --> 00:01:52,119
off we go yeah um I put a picture of the

00:01:50,140 --> 00:01:54,520
cat up there because I'm sure you guys

00:01:52,119 --> 00:01:59,170
heard and then use a few a little while

00:01:54,520 --> 00:02:02,610
back how you fed a neural net images

00:01:59,170 --> 00:02:05,130
from YouTube and it eventually learn

00:02:02,610 --> 00:02:06,450
the cat looks like that is about where

00:02:05,130 --> 00:02:09,860
we are in terms of machine learning

00:02:06,450 --> 00:02:13,010
nowhere close to terminator territory

00:02:09,860 --> 00:02:16,140
despite that machine learning is is

00:02:13,010 --> 00:02:18,090
present everywhere if you watch netflix

00:02:16,140 --> 00:02:22,050
your recommendations on netflix are

00:02:18,090 --> 00:02:25,410
powered by machine learning if you think

00:02:22,050 --> 00:02:26,970
about the canonical example of OCR which

00:02:25,410 --> 00:02:30,570
stands for optical character recognition

00:02:26,970 --> 00:02:32,610
the postal office uses it this so the

00:02:30,570 --> 00:02:34,910
idea is that machine learning has been

00:02:32,610 --> 00:02:39,990
around for a very long time it's

00:02:34,910 --> 00:02:43,440
probably a new way to think about some

00:02:39,990 --> 00:02:46,410
very old concepts so I'm here to talk

00:02:43,440 --> 00:02:48,900
about what deep learning is and what

00:02:46,410 --> 00:02:50,760
neural nets are so how many guys in the

00:02:48,900 --> 00:02:54,750
audience have heard about neural nets

00:02:50,760 --> 00:02:57,570
are deep learning before great how many

00:02:54,750 --> 00:03:01,970
of you guys have launched near on that

00:02:57,570 --> 00:03:05,820
Center production or oh wow that's great

00:03:01,970 --> 00:03:07,800
well just briefly going over what a

00:03:05,820 --> 00:03:09,510
basic neural net is for the rest of the

00:03:07,800 --> 00:03:12,090
people in the audience that are not

00:03:09,510 --> 00:03:14,310
familiar with what on your own that is I

00:03:12,090 --> 00:03:15,750
you know I being an engineer I like to

00:03:14,310 --> 00:03:17,850
describe everything in terms of gates

00:03:15,750 --> 00:03:21,030
and logical functions so I would say

00:03:17,850 --> 00:03:23,489
that a perceptron is really just a fancy

00:03:21,030 --> 00:03:25,709
word for a NAND gate so what is the big

00:03:23,489 --> 00:03:27,269
deal about a a new kind of NAND gate

00:03:25,709 --> 00:03:30,840
right especially one that's been around

00:03:27,269 --> 00:03:33,540
for so long and the answer to that is

00:03:30,840 --> 00:03:35,640
that you don't have to layout the gates

00:03:33,540 --> 00:03:38,640
to get the function you want you can

00:03:35,640 --> 00:03:41,070
find a way to get these gates to

00:03:38,640 --> 00:03:42,840
self-organize or learn the connections

00:03:41,070 --> 00:03:45,989
to each other to give you the output

00:03:42,840 --> 00:03:48,239
that you want given its input so if I

00:03:45,989 --> 00:03:50,610
had to rephrase I would probably say

00:03:48,239 --> 00:03:52,110
something like you want a learning

00:03:50,610 --> 00:03:54,900
algorithm where you show it a ton of

00:03:52,110 --> 00:03:57,120
examples that are your input and it sees

00:03:54,900 --> 00:04:00,000
and it sees the output and it has a way

00:03:57,120 --> 00:04:01,769
of learning or predicting what the

00:04:00,000 --> 00:04:05,400
output should be when it's given an

00:04:01,769 --> 00:04:07,769
input it's not seen before so so those

00:04:05,400 --> 00:04:10,130
those black dots and gray dots those are

00:04:07,769 --> 00:04:12,390
perceptrons this is very simple

00:04:10,130 --> 00:04:14,340
multi-layer network

00:04:12,390 --> 00:04:16,920
are the inputs the outputs and those are

00:04:14,340 --> 00:04:18,959
the weights in the connections and

00:04:16,920 --> 00:04:21,120
that's really all that's there to it so

00:04:18,959 --> 00:04:23,360
why is all this at the forefront now and

00:04:21,120 --> 00:04:27,240
why is all this making such big news

00:04:23,360 --> 00:04:29,580
part of it is because we've had progress

00:04:27,240 --> 00:04:32,130
in you know 20 years ago with

00:04:29,580 --> 00:04:34,380
backpropagation well I think it's far

00:04:32,130 --> 00:04:36,750
more than 20 at this point but back

00:04:34,380 --> 00:04:40,110
propagation and we've found an algorithm

00:04:36,750 --> 00:04:42,840
to get the network to learn and not only

00:04:40,110 --> 00:04:46,169
that we now have the compute power to go

00:04:42,840 --> 00:04:48,960
after these huge problems and another

00:04:46,169 --> 00:04:51,150
reason is that there's an explosion of

00:04:48,960 --> 00:04:53,610
data right there's so much data and a

00:04:51,150 --> 00:04:55,890
lot of it is unstructured right there's

00:04:53,610 --> 00:04:59,970
you know whether it's pixels in a

00:04:55,890 --> 00:05:02,669
photograph or text in some chat that you

00:04:59,970 --> 00:05:04,919
have or your emails all this is like

00:05:02,669 --> 00:05:06,750
highly unstructured data that's just and

00:05:04,919 --> 00:05:08,250
it just keeps exploding right and that's

00:05:06,750 --> 00:05:09,900
something that neural nets are really

00:05:08,250 --> 00:05:13,410
really good at they're good at finding

00:05:09,900 --> 00:05:17,280
patterns and good at dealing with this

00:05:13,410 --> 00:05:20,550
kind of voluminous unstructured data and

00:05:17,280 --> 00:05:23,610
and so so that is why at this point in

00:05:20,550 --> 00:05:27,090
time there's just so much research and

00:05:23,610 --> 00:05:29,370
so much momentum behind these techniques

00:05:27,090 --> 00:05:33,750
so I'm gonna pause at this point and ask

00:05:29,370 --> 00:05:36,750
if there are any questions not at this

00:05:33,750 --> 00:05:37,710
point great so now okay this is this is

00:05:36,750 --> 00:05:40,650
the company I work for

00:05:37,710 --> 00:05:43,830
it's called scary mind we're a small

00:05:40,650 --> 00:05:44,490
team ten of us now we were eight less

00:05:43,830 --> 00:05:49,890
than a month ago

00:05:44,490 --> 00:05:52,260
so growing rapidly and what we do is we

00:05:49,890 --> 00:05:54,390
provide okay so the way I would like to

00:05:52,260 --> 00:05:56,490
explain it is we're the cloud we want to

00:05:54,390 --> 00:06:00,570
be or we are the cloud era for deep

00:05:56,490 --> 00:06:03,150
learning so we are a base on the JVM and

00:06:00,570 --> 00:06:07,950
I don't need to convince this particular

00:06:03,150 --> 00:06:11,940
group of people to invest in the JVM so

00:06:07,950 --> 00:06:14,400
we want to be the go-to for enterprise

00:06:11,940 --> 00:06:18,780
when they want to implement a deep

00:06:14,400 --> 00:06:21,120
learning algorithm on their data and and

00:06:18,780 --> 00:06:22,960
that has access to their ETL pipelines

00:06:21,120 --> 00:06:26,080
so

00:06:22,960 --> 00:06:31,240
it's all open source everything all code

00:06:26,080 --> 00:06:33,009
is open source and we also have because

00:06:31,240 --> 00:06:35,169
it's the JVM we also have support for

00:06:33,009 --> 00:06:37,599
Scala we have some scholar wrappers

00:06:35,169 --> 00:06:40,270
around some of our libraries we would

00:06:37,599 --> 00:06:42,759
very much appreciate contributions from

00:06:40,270 --> 00:06:44,800
you guys I can go go through some of

00:06:42,759 --> 00:06:47,409
those things at the very end but first

00:06:44,800 --> 00:06:50,199
let's just talk about what the

00:06:47,409 --> 00:06:54,189
challenges possibly could be if you

00:06:50,199 --> 00:06:59,919
wanted to do scientific computing on the

00:06:54,189 --> 00:07:06,399
JVM it's not an easy task

00:06:59,919 --> 00:07:09,369
right and on why is that you can't you

00:07:06,399 --> 00:07:12,809
cannot only have to the the main thing

00:07:09,369 --> 00:07:15,369
I'm getting at is you know C++ and

00:07:12,809 --> 00:07:17,919
Fortran have been able to take advantage

00:07:15,369 --> 00:07:20,769
of sending instructions to send you a

00:07:17,919 --> 00:07:23,679
single instruction multiple data with

00:07:20,769 --> 00:07:26,289
compiler support but Java can't write so

00:07:23,679 --> 00:07:29,860
that's one problem so you're not really

00:07:26,289 --> 00:07:32,709
exploiting the CPU as much as you can

00:07:29,860 --> 00:07:36,669
the second problem is that even on a

00:07:32,709 --> 00:07:39,039
64-bit JVM your array indexing only goes

00:07:36,669 --> 00:07:42,059
up to 32 bits so you're severely

00:07:39,039 --> 00:07:48,669
restricted in terms of memory right so

00:07:42,059 --> 00:07:52,599
this recent effort by our team to write

00:07:48,669 --> 00:07:56,909
a all C++ backhand that is plug and play

00:07:52,599 --> 00:08:01,139
for the framework Soulstice issue and

00:07:56,909 --> 00:08:04,360
because we have openmp and cuda

00:08:01,139 --> 00:08:08,529
directors in there we can exploit the

00:08:04,360 --> 00:08:11,169
actual hardware on a chip basis so we

00:08:08,529 --> 00:08:13,300
can support GPUs and CPUs and things

00:08:11,169 --> 00:08:16,119
will work super super fast and of course

00:08:13,300 --> 00:08:18,849
like this also means that we've extended

00:08:16,119 --> 00:08:22,869
our array indexing from 32 bits to 64

00:08:18,849 --> 00:08:26,319
bits right so so so that is essentially

00:08:22,869 --> 00:08:29,319
what what our stack looks like we have a

00:08:26,319 --> 00:08:32,800
c++ underbelly and then we have a linear

00:08:29,319 --> 00:08:33,969
algebra library called nd4 jrn d-4s that

00:08:32,800 --> 00:08:36,399
sits on top of it

00:08:33,969 --> 00:08:38,680
and then we have DL for J which is our

00:08:36,399 --> 00:08:40,870
deep learning library and the idea is to

00:08:38,680 --> 00:08:43,060
have everything be plug play kind of

00:08:40,870 --> 00:08:44,529
like the idea being you can you know

00:08:43,060 --> 00:08:46,420
swap something out and it should not

00:08:44,529 --> 00:08:47,589
interfere with anything else and I think

00:08:46,420 --> 00:08:50,230
you guys are all very familiar with the

00:08:47,589 --> 00:08:52,540
concept so now to talk a little bit

00:08:50,230 --> 00:08:54,910
about machine learning I didn't really

00:08:52,540 --> 00:08:57,610
touch pum touch upon how you go about

00:08:54,910 --> 00:08:59,649
training a network there's this

00:08:57,610 --> 00:09:02,170
algorithm called back propagation I'm

00:08:59,649 --> 00:09:03,730
sure a lot of you who have raised your

00:09:02,170 --> 00:09:05,769
hands when I said neural nets so you

00:09:03,730 --> 00:09:10,480
guys are familiar it boils down to the

00:09:05,769 --> 00:09:12,959
chain rule and this is a version and you

00:09:10,480 --> 00:09:15,670
use something like a gradient descent to

00:09:12,959 --> 00:09:18,670
or many there are many optimization

00:09:15,670 --> 00:09:21,399
algorithms that you can use so this

00:09:18,670 --> 00:09:25,569
essentially shows how you would scale

00:09:21,399 --> 00:09:29,170
your learning from one machine to a

00:09:25,569 --> 00:09:31,990
cluster and the idea is a lot like this

00:09:29,170 --> 00:09:36,009
which is you know it might look like

00:09:31,990 --> 00:09:37,629
MapReduce but the idea is that you it's

00:09:36,009 --> 00:09:40,180
a lot like data parallelism you take

00:09:37,629 --> 00:09:42,160
your data which might be massive and you

00:09:40,180 --> 00:09:44,949
just put it up into different chunks and

00:09:42,160 --> 00:09:48,279
you send it out to all the all the

00:09:44,949 --> 00:09:51,009
machines on your cluster and then so

00:09:48,279 --> 00:09:52,480
each each machine has its own copy of

00:09:51,009 --> 00:09:54,370
the model that you're trying to build

00:09:52,480 --> 00:09:56,290
when I say model what I mean is there

00:09:54,370 --> 00:09:58,180
are hyper parameters and parameters and

00:09:56,290 --> 00:10:01,240
all the stuff but each model has a copy

00:09:58,180 --> 00:10:04,360
of this and then each model on each

00:10:01,240 --> 00:10:06,430
machine learns whatever the weak update

00:10:04,360 --> 00:10:08,829
should be on the network for that

00:10:06,430 --> 00:10:12,399
particular batch or clock our chunk of

00:10:08,829 --> 00:10:16,829
data and then it sends the updates back

00:10:12,399 --> 00:10:18,970
to the master and then they're basically

00:10:16,829 --> 00:10:21,879
averaged and then send them back out

00:10:18,970 --> 00:10:23,680
again to the machines to learn on a new

00:10:21,879 --> 00:10:27,550
data set and this is they're iteratively

00:10:23,680 --> 00:10:29,410
and therefore you're utilizing all you

00:10:27,550 --> 00:10:32,379
utilizing all the CPU power that you

00:10:29,410 --> 00:10:35,680
have at your disposal to train so that's

00:10:32,379 --> 00:10:37,350
essentially what DL for J is capable of

00:10:35,680 --> 00:10:41,139
doing which is that you know with our

00:10:37,350 --> 00:10:42,780
C++ native compute layer we're really

00:10:41,139 --> 00:10:46,080
able to take advantage of

00:10:42,780 --> 00:10:50,580
hardware and we're able to scale out

00:10:46,080 --> 00:10:51,990
pretty easily with this model okay so

00:10:50,580 --> 00:10:55,950
now we're gonna talk a little bit about

00:10:51,990 --> 00:10:57,990
NLP I think we're okay on time so that's

00:10:55,950 --> 00:11:00,200
great so mo P stands for natural

00:10:57,990 --> 00:11:04,650
language processing and NLP is hard

00:11:00,200 --> 00:11:07,010
really hard and the reason is obviously

00:11:04,650 --> 00:11:09,270
because text in language has a lot of

00:11:07,010 --> 00:11:12,270
nuances and a lot of subtlety and

00:11:09,270 --> 00:11:14,400
getting a machine to learn all of this

00:11:12,270 --> 00:11:17,850
vast and complicated subtlety and nuance

00:11:14,400 --> 00:11:21,080
is no small feat right so what you will

00:11:17,850 --> 00:11:24,000
see is that in in the research a lot of

00:11:21,080 --> 00:11:26,730
the accuracy for something like

00:11:24,000 --> 00:11:28,770
sentiment analysis has kind of her have

00:11:26,730 --> 00:11:30,960
kind of been at a pace of like 1%

00:11:28,770 --> 00:11:33,690
increase every year and suddenly there

00:11:30,960 --> 00:11:35,550
was this huge explosive increase of 5%

00:11:33,690 --> 00:11:36,990
and that was thanks to some research

00:11:35,550 --> 00:11:39,120
that came out of Stanford and it was

00:11:36,990 --> 00:11:41,100
based on this particular architecture

00:11:39,120 --> 00:11:43,560
neural nets that I'm gonna talk about

00:11:41,100 --> 00:11:47,870
which is known as recursive neural nets

00:11:43,560 --> 00:11:50,040
so it's just another example of how how

00:11:47,870 --> 00:11:53,220
neural nets have come to the forefront

00:11:50,040 --> 00:11:54,990
at this time thanks to the amount of

00:11:53,220 --> 00:11:57,060
data that's available and that'll be

00:11:54,990 --> 00:12:01,890
very clear when I go through some more

00:11:57,060 --> 00:12:03,990
slides and so so I showed you guys an

00:12:01,890 --> 00:12:06,300
example of a feed-forward Network rate

00:12:03,990 --> 00:12:09,210
so that was just very simple input a

00:12:06,300 --> 00:12:11,160
hidden layer and output and and a bunch

00:12:09,210 --> 00:12:13,830
of connections and denoting the weights

00:12:11,160 --> 00:12:18,300
right so one of the restrictions for

00:12:13,830 --> 00:12:21,690
that kind of model what would one of the

00:12:18,300 --> 00:12:23,520
restrictions be the idea is that you can

00:12:21,690 --> 00:12:24,870
feed it one example and they don't know

00:12:23,520 --> 00:12:27,630
their example and then another example

00:12:24,870 --> 00:12:29,310
in no particular order and it should

00:12:27,630 --> 00:12:31,830
give you the same results right it has

00:12:29,310 --> 00:12:35,880
like no sense of order it has no sense

00:12:31,830 --> 00:12:38,100
of our time or steps or in a sequence of

00:12:35,880 --> 00:12:40,680
steps or anything like that right so if

00:12:38,100 --> 00:12:42,720
you wanted to train anything on a neural

00:12:40,680 --> 00:12:45,450
net that involved something like a time

00:12:42,720 --> 00:12:47,730
series or anything that essentially had

00:12:45,450 --> 00:12:48,600
a sense of order to it like sentence

00:12:47,730 --> 00:12:51,779
inherently

00:12:48,600 --> 00:12:54,120
a sense of order like the word the words

00:12:51,779 --> 00:12:56,610
that are surrounding us and given word

00:12:54,120 --> 00:13:00,389
has relevance they're not all saying

00:12:56,610 --> 00:13:03,029
right so you can't if you can't deal

00:13:00,389 --> 00:13:06,509
with those kind of complexities and

00:13:03,029 --> 00:13:09,449
nuances with with a simple

00:13:06,509 --> 00:13:10,889
straightforward MLP but you can with

00:13:09,449 --> 00:13:13,709
something known as a recurrent here on

00:13:10,889 --> 00:13:15,750
that and the idea of a recurrent neural

00:13:13,709 --> 00:13:17,579
net and one word would be feedback it's

00:13:15,750 --> 00:13:21,779
a Fiat there's a feedback loop and that

00:13:17,579 --> 00:13:24,899
is what is indicated by the the the

00:13:21,779 --> 00:13:29,519
arrow that feeds back from the output

00:13:24,899 --> 00:13:31,949
layer to the hidden layer and so when

00:13:29,519 --> 00:13:34,259
you think in terms of a feedback loop I

00:13:31,949 --> 00:13:37,980
hope you guys think in terms of memory

00:13:34,259 --> 00:13:40,110
right if you can feed back if you feed

00:13:37,980 --> 00:13:43,949
back a lot of things for example I'm

00:13:40,110 --> 00:13:45,810
thinking specifically of like of an SRAM

00:13:43,949 --> 00:13:49,139
cell because I was a hardware engineer

00:13:45,810 --> 00:13:50,970
never mind that you have you know costly

00:13:49,139 --> 00:13:53,009
you have inverters back to back which

00:13:50,970 --> 00:13:55,470
preserve the state of the cell whether

00:13:53,009 --> 00:13:57,360
it's a 1 or a 0 so whenever you have a

00:13:55,470 --> 00:14:00,089
feedback loop you have memory right so

00:13:57,360 --> 00:14:02,910
that is that is that is what makes the

00:14:00,089 --> 00:14:04,980
recurrent neural net so special and so

00:14:02,910 --> 00:14:09,000
powerful is that it has this ability to

00:14:04,980 --> 00:14:11,459
remember things and that means that you

00:14:09,000 --> 00:14:12,509
can apply it to a different problem said

00:14:11,459 --> 00:14:14,339
that you couldn't before

00:14:12,509 --> 00:14:18,899
but that neural nets weren't applicable

00:14:14,339 --> 00:14:21,750
for before right so again like a lot of

00:14:18,899 --> 00:14:24,509
things a neural net this is not a idea

00:14:21,750 --> 00:14:27,709
that just came out it's in fact an idea

00:14:24,509 --> 00:14:32,100
that's been around for a while all right

00:14:27,709 --> 00:14:36,000
so so so then what was the issue well

00:14:32,100 --> 00:14:37,889
part of the issues was that if you

00:14:36,000 --> 00:14:40,680
looked into the glutes of back

00:14:37,889 --> 00:14:42,569
propagation you'll see which is the

00:14:40,680 --> 00:14:44,670
chain rule which again boils down to

00:14:42,569 --> 00:14:46,139
multiplying things by multiplying things

00:14:44,670 --> 00:14:48,420
and multiplying these and multiplying

00:14:46,139 --> 00:14:52,050
things right so the weight matrix gets

00:14:48,420 --> 00:14:55,470
multiplied by the gradient and the if

00:14:52,050 --> 00:14:59,439
the weights are either really small and

00:14:55,470 --> 00:15:02,259
you have to you have many time steps

00:14:59,439 --> 00:15:04,959
this is just one feedback but imagine if

00:15:02,259 --> 00:15:06,849
you had to do something over many time

00:15:04,959 --> 00:15:08,410
steps and then you gets multiplied over

00:15:06,849 --> 00:15:10,449
and over and over again and essentially

00:15:08,410 --> 00:15:12,519
what you where you end up with is a

00:15:10,449 --> 00:15:15,189
problem where your gradient either

00:15:12,519 --> 00:15:17,529
explodes because you multiplied a large

00:15:15,189 --> 00:15:19,389
number several times over and it's

00:15:17,529 --> 00:15:21,189
exposed or you mill quite a small number

00:15:19,389 --> 00:15:22,929
several times over and now it's just

00:15:21,189 --> 00:15:25,419
like nothing so it's called the

00:15:22,929 --> 00:15:28,689
exploding or vanishing gradient problem

00:15:25,419 --> 00:15:31,119
so so that was that was the problem with

00:15:28,689 --> 00:15:33,189
this very simple configuration and that

00:15:31,119 --> 00:15:34,869
was one of the reasons why recurrent

00:15:33,189 --> 00:15:36,459
neural nets couldn't be applied to these

00:15:34,869 --> 00:15:40,569
problems even though it was around in

00:15:36,459 --> 00:15:42,909
theory so so then some other research

00:15:40,569 --> 00:15:46,029
came up fairly recently with the

00:15:42,909 --> 00:15:49,449
solution for it and and with the drainer

00:15:46,029 --> 00:15:51,819
was a tweak to the original idea and and

00:15:49,449 --> 00:15:54,129
the tweak was to explicitly add some

00:15:51,819 --> 00:15:56,589
memory with some control around it and

00:15:54,129 --> 00:15:58,809
to make all of this learner ball so

00:15:56,589 --> 00:16:01,329
there is an implicate and appliqued in a

00:15:58,809 --> 00:16:03,669
forget gate and the parameters that

00:16:01,329 --> 00:16:06,339
control the in forget the implicit and

00:16:03,669 --> 00:16:10,419
the output gate all can be learned so

00:16:06,339 --> 00:16:12,939
that now the cell is able to remember

00:16:10,419 --> 00:16:15,669
things sort of further further

00:16:12,939 --> 00:16:17,139
fatherfather back in memory that it

00:16:15,669 --> 00:16:19,600
couldn't earlier because of the

00:16:17,139 --> 00:16:24,179
exploding revenge and gradient problem

00:16:19,600 --> 00:16:27,970
so with this in place a lot of very cool

00:16:24,179 --> 00:16:31,329
applications have been have been

00:16:27,970 --> 00:16:34,959
developed with these kind of networks

00:16:31,329 --> 00:16:36,789
with recurrent neural nets have any of

00:16:34,959 --> 00:16:39,039
you guys seen these before I just would

00:16:36,789 --> 00:16:42,129
like to know how and you guys seen any

00:16:39,039 --> 00:16:47,019
of the examples okay one person okay

00:16:42,129 --> 00:16:51,279
great so okay - Wow you guys not my

00:16:47,019 --> 00:16:53,529
research but anyway so you know the

00:16:51,279 --> 00:16:55,119
point is that with your essentially

00:16:53,529 --> 00:16:57,579
going from one sequence to another

00:16:55,119 --> 00:16:59,859
sequence and you're really only limited

00:16:57,579 --> 00:17:03,999
by your imagination because you could

00:16:59,859 --> 00:17:06,759
think about ain't many many things in

00:17:03,999 --> 00:17:08,679
terms of sequences right I may be an

00:17:06,759 --> 00:17:10,380
obvious example which is a very hard

00:17:08,679 --> 00:17:12,209
example is machine translation

00:17:10,380 --> 00:17:14,459
right to go from like English to French

00:17:12,209 --> 00:17:17,550
right so that would be the many-to-many

00:17:14,459 --> 00:17:21,360
example up here you go from English to

00:17:17,550 --> 00:17:22,860
French you could have many do one

00:17:21,360 --> 00:17:25,440
example which is the one that we're

00:17:22,860 --> 00:17:27,930
going to show you guys which is this way

00:17:25,440 --> 00:17:29,670
you take where the example that we're

00:17:27,930 --> 00:17:32,040
that we're going to show you is where

00:17:29,670 --> 00:17:34,230
you take a set of GU view reviews from

00:17:32,040 --> 00:17:36,360
imbd and they're classified as either

00:17:34,230 --> 00:17:39,270
positive sentiment or negative sentiment

00:17:36,360 --> 00:17:40,800
and so that's a many to one you feed in

00:17:39,270 --> 00:17:42,930
a review and you get out something that

00:17:40,800 --> 00:17:46,080
says this is either positive or this is

00:17:42,930 --> 00:17:48,510
negative and then once and many actually

00:17:46,080 --> 00:17:49,740
it's a very interesting use case I don't

00:17:48,510 --> 00:17:51,150
know if you guys have might not have

00:17:49,740 --> 00:17:53,550
seen this but there was some research

00:17:51,150 --> 00:17:54,060
out of Google and I'm describing the

00:17:53,550 --> 00:17:57,480
image

00:17:54,060 --> 00:17:59,970
it showed a sterol with some pizza on

00:17:57,480 --> 00:18:01,920
top of the stove and you fed it to this

00:17:59,970 --> 00:18:03,690
network this image so the image is the

00:18:01,920 --> 00:18:06,000
single input and the output was a

00:18:03,690 --> 00:18:07,350
caption and the caption was three slices

00:18:06,000 --> 00:18:09,480
something along the lines of three

00:18:07,350 --> 00:18:12,630
slices of pizza sitting on top of a

00:18:09,480 --> 00:18:16,170
white stove that's pretty incredible

00:18:12,630 --> 00:18:18,120
considering it's a machine our engines

00:18:16,170 --> 00:18:20,910
are particularly good at that Saarinen's

00:18:18,120 --> 00:18:23,580
are very good at taking in an image and

00:18:20,910 --> 00:18:25,730
I'm giving it a caption and it almost

00:18:23,580 --> 00:18:28,980
seems magical how that can happen

00:18:25,730 --> 00:18:32,700
so the one-to-one really is just me

00:18:28,980 --> 00:18:35,280
showing how you can't really with just

00:18:32,700 --> 00:18:37,170
just a regular MLP Network you know it's

00:18:35,280 --> 00:18:38,850
just you have it's pretty static you

00:18:37,170 --> 00:18:40,200
just have one input and then bunch of

00:18:38,850 --> 00:18:42,270
layers or whatever and then you have an

00:18:40,200 --> 00:18:44,880
output rate so I'm just contrasting that

00:18:42,270 --> 00:18:49,530
with all these other cases so kind of

00:18:44,880 --> 00:18:51,690
glazed over a lot of stuff we're talking

00:18:49,530 --> 00:18:54,600
about worms right how are we going to

00:18:51,690 --> 00:18:56,070
feed our words into your network how

00:18:54,600 --> 00:18:58,250
many you think is the same word to buck

00:18:56,070 --> 00:19:01,380
before great

00:18:58,250 --> 00:19:03,390
it's a neat party verdict then kind of

00:19:01,380 --> 00:19:05,280
gets old when you heard it like the for

00:19:03,390 --> 00:19:08,730
the thousands time but it's still worth

00:19:05,280 --> 00:19:11,070
putting up there which is there is this

00:19:08,730 --> 00:19:13,360
concept called word Tyvek which is a

00:19:11,070 --> 00:19:18,970
very shallow neuron that

00:19:13,360 --> 00:19:22,660
that was trained on a massive corpus

00:19:18,970 --> 00:19:25,750
which is one the case that we're using

00:19:22,660 --> 00:19:31,330
here is the google news corpus which has

00:19:25,750 --> 00:19:35,260
I believe 13 million unique words and it

00:19:31,330 --> 00:19:37,960
has 100 billion words in in the corpus

00:19:35,260 --> 00:19:42,040
style that is what this net was trained

00:19:37,960 --> 00:19:44,170
on and the idea was so it had to the

00:19:42,040 --> 00:19:45,790
paper describes two separate approaches

00:19:44,170 --> 00:19:49,150
I'm just going to describe the one that

00:19:45,790 --> 00:19:53,290
was that the paper recommends which is

00:19:49,150 --> 00:19:56,650
you are approaching a word given when

00:19:53,290 --> 00:19:58,060
you have a word and in a sentence there

00:19:56,650 --> 00:19:59,650
are the words on the left and the words

00:19:58,060 --> 00:20:02,200
on the right what you're trying to

00:19:59,650 --> 00:20:04,240
predict is the probability of seeing the

00:20:02,200 --> 00:20:06,610
neighboring words to the left and the

00:20:04,240 --> 00:20:09,870
right given the Center word and you take

00:20:06,610 --> 00:20:12,220
and you and you train a network on

00:20:09,870 --> 00:20:14,440
maximizing these probabilities like the

00:20:12,220 --> 00:20:16,660
maximum likelihood estimation and you

00:20:14,440 --> 00:20:19,150
come up with a representation for each

00:20:16,660 --> 00:20:22,600
word and each word you can also think

00:20:19,150 --> 00:20:26,590
about this as each word gets converted

00:20:22,600 --> 00:20:29,380
based on its neighbors to a 300

00:20:26,590 --> 00:20:34,300
dimensional space represented by a

00:20:29,380 --> 00:20:36,760
vector that has 300 separate axes or you

00:20:34,300 --> 00:20:38,320
know however you want 300 long column

00:20:36,760 --> 00:20:40,840
vector however you would like to

00:20:38,320 --> 00:20:42,580
represent it so that is the idea it's it

00:20:40,840 --> 00:20:47,470
was transformed from this large

00:20:42,580 --> 00:20:52,900
dimension you know 13 million words down

00:20:47,470 --> 00:20:57,370
to 300 long factors and that is pretty

00:20:52,900 --> 00:21:00,390
incredible right and what we found are

00:20:57,370 --> 00:21:03,730
like what they found was that these

00:21:00,390 --> 00:21:06,730
these embeddings or these vectors are

00:21:03,730 --> 00:21:09,550
very useful at representing meaning

00:21:06,730 --> 00:21:11,560
which is why you end up with something

00:21:09,550 --> 00:21:13,990
that looks like this which is a king -

00:21:11,560 --> 00:21:17,950
men plus women gives you the vector for

00:21:13,990 --> 00:21:21,040
queen and then as you can see the the

00:21:17,950 --> 00:21:23,110
vectors from women to ant is

00:21:21,040 --> 00:21:25,400
approximately in the same direction is

00:21:23,110 --> 00:21:28,120
Man 2 on call so it

00:21:25,400 --> 00:21:30,680
and then there's a lot of a lot of

00:21:28,120 --> 00:21:33,160
information that gets embedded in this

00:21:30,680 --> 00:21:37,100
vector space and it's really incredible

00:21:33,160 --> 00:21:41,990
and I can take questions about it later

00:21:37,100 --> 00:21:44,900
so this is another another you know quit

00:21:41,990 --> 00:21:47,210
snippet of how the embeddings capture

00:21:44,900 --> 00:21:50,630
the relationship between countries and

00:21:47,210 --> 00:21:54,260
cities and how you know most of these

00:21:50,630 --> 00:21:56,180
are parallel so that is oops sorry so

00:21:54,260 --> 00:21:58,280
that is what you used to feed into your

00:21:56,180 --> 00:22:01,040
network you when you are training your

00:21:58,280 --> 00:22:04,610
recurrent neuron that on a sentence you

00:22:01,040 --> 00:22:07,670
the sequence is your sentences and the

00:22:04,610 --> 00:22:10,370
time steps or each time step is a word

00:22:07,670 --> 00:22:15,710
and each world is represented by one of

00:22:10,370 --> 00:22:18,080
these vectors and yeah and another for

00:22:15,710 --> 00:22:20,059
an application that I was talking about

00:22:18,080 --> 00:22:22,040
that I think is like really incredible

00:22:20,059 --> 00:22:24,620
this is from arc and Jake our party's

00:22:22,040 --> 00:22:26,929
website his website is really great he

00:22:24,620 --> 00:22:29,120
has a lot of very clear clean

00:22:26,929 --> 00:22:30,679
explanations I'd recommend you guys to

00:22:29,120 --> 00:22:33,020
check it out if you're interested is

00:22:30,679 --> 00:22:37,210
this is this is auto-generated

00:22:33,020 --> 00:22:42,140
Shakespeare it's crazy

00:22:37,210 --> 00:22:46,610
the an RNN was given the entire corpus

00:22:42,140 --> 00:22:48,440
for Shakespeare and trained on it arm

00:22:46,610 --> 00:22:50,030
sequence prediction so you give it you

00:22:48,440 --> 00:22:52,520
give it a sentence and then you shift

00:22:50,030 --> 00:22:54,710
the sentence over and then that would be

00:22:52,520 --> 00:22:57,620
the output so and then you turn it on

00:22:54,710 --> 00:23:00,230
everything you have and you feed it like

00:22:57,620 --> 00:23:02,240
you see it with something initially and

00:23:00,230 --> 00:23:04,850
then it just Auto generates the rest of

00:23:02,240 --> 00:23:08,600
the time steps and this is yeah this is

00:23:04,850 --> 00:23:11,960
you know Shakespeare that was generated

00:23:08,600 --> 00:23:16,309
by his example that he trained on the

00:23:11,960 --> 00:23:19,700
entire corpus so so now we're back to

00:23:16,309 --> 00:23:21,320
mine example which is you know it's a

00:23:19,700 --> 00:23:25,190
very it's almost like a canonical

00:23:21,320 --> 00:23:27,050
example the imbd dataset classifying the

00:23:25,190 --> 00:23:29,870
sentiment for this and this is just a

00:23:27,050 --> 00:23:31,640
repeating what I was describing to you

00:23:29,870 --> 00:23:32,290
guys earlier which is your reviewers

00:23:31,640 --> 00:23:35,080
percent

00:23:32,290 --> 00:23:37,180
says they're strung together with as

00:23:35,080 --> 00:23:40,150
vectors each word is a vector and then

00:23:37,180 --> 00:23:42,400
your sentiment is positive or negative

00:23:40,150 --> 00:23:48,760
reviews so now Andy will talk a little

00:23:42,400 --> 00:23:53,830
bit about thank you it was pretty clear

00:23:48,760 --> 00:23:55,720
right I thought I'd seen right so a

00:23:53,830 --> 00:23:59,290
little word about us before before I get

00:23:55,720 --> 00:24:02,530
study so I don't know how many of you

00:23:59,290 --> 00:24:04,810
know the spar notebook who knows about

00:24:02,530 --> 00:24:06,370
this ponderable not that much great

00:24:04,810 --> 00:24:08,250
that's that's cool for me because then I

00:24:06,370 --> 00:24:11,530
can expose myself a lot more

00:24:08,250 --> 00:24:13,240
so this my notebook is is the only note

00:24:11,530 --> 00:24:16,560
Buddha I can find around which is fully

00:24:13,240 --> 00:24:19,540
Scala and allows you to to do some in

00:24:16,560 --> 00:24:22,690
interactive sessions with mainly spark

00:24:19,540 --> 00:24:25,870
of course as the name is stated but also

00:24:22,690 --> 00:24:27,580
you can play with Cassandra Kafka there

00:24:25,870 --> 00:24:30,670
are many many examples in the in the

00:24:27,580 --> 00:24:32,440
codes in the source code organizing

00:24:30,670 --> 00:24:34,240
notebooks of course that allows you to

00:24:32,440 --> 00:24:38,230
do whatever you want also machine

00:24:34,240 --> 00:24:39,700
learning examples including genomics if

00:24:38,230 --> 00:24:43,000
you like genomics for instance we have a

00:24:39,700 --> 00:24:44,620
few examples doing so and the idea of

00:24:43,000 --> 00:24:48,790
the notebook is that it allows data

00:24:44,620 --> 00:24:50,590
scientists to be pretty close to the

00:24:48,790 --> 00:24:55,150
runtime because they can use Park and

00:24:50,590 --> 00:24:56,830
whatever tool like Kafka but also they

00:24:55,150 --> 00:24:59,950
can be very effective right so they can

00:24:56,830 --> 00:25:01,960
run code tests rewrite retest and song

00:24:59,950 --> 00:25:07,030
without having to rerun the full example

00:25:01,960 --> 00:25:08,530
for a full project with tests right

00:25:07,030 --> 00:25:10,810
which takes a lot of time because you

00:25:08,530 --> 00:25:13,270
know when you try to a model it takes

00:25:10,810 --> 00:25:14,860
maybe 20 maybe an hour 20 minutes maybe

00:25:13,270 --> 00:25:16,480
an hour and then you don't want to

00:25:14,860 --> 00:25:18,220
return all the time so you want to

00:25:16,480 --> 00:25:20,620
explore a little bit before returning

00:25:18,220 --> 00:25:22,300
the algorithm or the model so it allows

00:25:20,620 --> 00:25:23,950
them to be very efficient and these

00:25:22,300 --> 00:25:25,570
structures has a webpage simply with

00:25:23,950 --> 00:25:27,520
boxes where you type code you click

00:25:25,570 --> 00:25:29,590
enter and what's going on behind it that

00:25:27,520 --> 00:25:31,960
is sense code over the WebSocket the

00:25:29,590 --> 00:25:34,630
code is executed into a new JVM which is

00:25:31,960 --> 00:25:37,330
spawned for each notebook and which is

00:25:34,630 --> 00:25:39,100
actually mainly containing a rebel and

00:25:37,330 --> 00:25:40,259
the rapid execute the code and returns

00:25:39,100 --> 00:25:42,749
of the

00:25:40,259 --> 00:25:46,409
the results the idea is that there is

00:25:42,749 --> 00:25:48,269
also a I mean since it's a Scala is

00:25:46,409 --> 00:25:51,710
pretty reactive that means that you can

00:25:48,269 --> 00:25:54,389
very easily drop some Scala domain

00:25:51,710 --> 00:25:56,249
classes instances and then it will

00:25:54,389 --> 00:25:58,350
automatically plots over it

00:25:56,249 --> 00:26:00,269
so without asking for anything or you

00:25:58,350 --> 00:26:02,519
can also ask for a few charts if you

00:26:00,269 --> 00:26:05,309
like specifically but it will plot for

00:26:02,519 --> 00:26:07,409
you whatever data that you have asked

00:26:05,309 --> 00:26:09,720
for has a table at par chart lines or

00:26:07,409 --> 00:26:11,489
whatever and you can add to these plots

00:26:09,720 --> 00:26:13,859
dynamically thanks to WebSockets

00:26:11,489 --> 00:26:19,350
connections it's based on play by the

00:26:13,859 --> 00:26:22,019
way and interface was based on Jupiter's

00:26:19,350 --> 00:26:25,289
one but I will refer to it quite a bit

00:26:22,019 --> 00:26:28,080
in order to for it to look like a little

00:26:25,289 --> 00:26:31,289
bit more like our studio I'm a old user

00:26:28,080 --> 00:26:35,129
of our I kind of like our studio and I

00:26:31,289 --> 00:26:36,509
wanted it to be looking like it so then

00:26:35,129 --> 00:26:38,549
I fellas is a company that I've created

00:26:36,509 --> 00:26:40,019
with my buddy exhibited Juarez in

00:26:38,549 --> 00:26:43,080
Belgium still but we are currently

00:26:40,019 --> 00:26:45,840
creating a another startup here in in in

00:26:43,080 --> 00:26:48,480
the US we've been recently accepted into

00:26:45,840 --> 00:26:51,720
the alchemists accelerator which is the

00:26:48,480 --> 00:26:55,519
Y Combinator for b2b / Enterprise if you

00:26:51,720 --> 00:26:58,470
like what we do essentially is that we

00:26:55,519 --> 00:27:01,350
enable data driven businesses so we do a

00:26:58,470 --> 00:27:03,419
lot of work in insurance in media and in

00:27:01,350 --> 00:27:05,669
genomics and we all know the data sent

00:27:03,419 --> 00:27:06,929
is to be very efficient not only because

00:27:05,669 --> 00:27:08,549
they have the notebook but from the

00:27:06,929 --> 00:27:11,639
notebook they can generate a lot of

00:27:08,549 --> 00:27:15,960
things including jobs and avro web

00:27:11,639 --> 00:27:18,600
services or whatever and why I'm here is

00:27:15,960 --> 00:27:21,179
because also we do a lot of NLP on what

00:27:18,600 --> 00:27:23,399
on the notebooks so the notebooks for us

00:27:21,179 --> 00:27:25,470
is an input and we do a lot of work

00:27:23,399 --> 00:27:28,440
around the node the notebook in order to

00:27:25,470 --> 00:27:30,389
detect inputs outputs the content of the

00:27:28,440 --> 00:27:32,580
notebook will link input and and put

00:27:30,389 --> 00:27:35,489
together we are constructed context

00:27:32,580 --> 00:27:38,190
around the inputs and and the processes

00:27:35,489 --> 00:27:42,090
that means that we can predict what's

00:27:38,190 --> 00:27:44,009
what a data scientist want to do and we

00:27:42,090 --> 00:27:46,169
can also do some recommendation like a

00:27:44,009 --> 00:27:48,029
you may want to use this data set that

00:27:46,169 --> 00:27:49,289
has the structure because we know the

00:27:48,029 --> 00:27:51,749
structure because we use a ver and

00:27:49,289 --> 00:27:54,330
Parker all the time and maybe you can

00:27:51,749 --> 00:27:56,130
consume the web server in order to check

00:27:54,330 --> 00:27:57,420
if you have the right feeling with this

00:27:56,130 --> 00:27:59,310
data and then you can connect directly

00:27:57,420 --> 00:28:00,720
with Cassandra because we provide the

00:27:59,310 --> 00:28:02,910
right ship snippet to do it

00:28:00,720 --> 00:28:05,520
thanks to Marshall learning including

00:28:02,910 --> 00:28:07,290
deep learning you can do that and

00:28:05,520 --> 00:28:08,900
because of course the descent is no what

00:28:07,290 --> 00:28:11,310
it does when it deploy to production

00:28:08,900 --> 00:28:13,050
right so if it deploys to production

00:28:11,310 --> 00:28:16,230
that means for us that it has a pretty

00:28:13,050 --> 00:28:19,470
good accuracy and it has a right a high

00:28:16,230 --> 00:28:21,090
confidence on the on the job that it

00:28:19,470 --> 00:28:23,250
just deployed in production so it

00:28:21,090 --> 00:28:26,490
consumes data I consume CPUs and so on

00:28:23,250 --> 00:28:28,320
so I hope is pretty confident so the

00:28:26,490 --> 00:28:30,990
idea of the notebook is in charge

00:28:28,320 --> 00:28:35,820
essentially as I said a web page that we

00:28:30,990 --> 00:28:39,480
can see so much actually it had a better

00:28:35,820 --> 00:28:51,300
resolution before actually maybe I can

00:28:39,480 --> 00:28:54,420
check display right so a notebook is

00:28:51,300 --> 00:28:58,040
this kind of page the thing is that this

00:28:54,420 --> 00:29:00,420
notebook will be published open source

00:28:58,040 --> 00:29:05,700
collaboratively with sky mind and we

00:29:00,420 --> 00:29:07,620
will provide the decatur prepare soon so

00:29:05,700 --> 00:29:11,580
you can contact us or we will publish it

00:29:07,620 --> 00:29:14,310
a new way on our Twitter feeds and what

00:29:11,580 --> 00:29:17,160
we do here so we use a a cluster that

00:29:14,310 --> 00:29:22,620
has been created by canonical because we

00:29:17,160 --> 00:29:28,530
have a project with canonical IBM Nvidia

00:29:22,620 --> 00:29:33,420
and sky mine and sky man yes in order to

00:29:28,530 --> 00:29:35,340
do deep learning of services there are

00:29:33,420 --> 00:29:37,620
failures and services so this is a

00:29:35,340 --> 00:29:40,260
message cluster essentially with a few

00:29:37,620 --> 00:29:43,980
with a few machines we have I think up

00:29:40,260 --> 00:29:47,580
to only 34 CPU right now but we have

00:29:43,980 --> 00:29:50,370
also GPUs that we we can serve right to

00:29:47,580 --> 00:29:51,960
the SPARC executors so which is pretty

00:29:50,370 --> 00:29:55,500
neat we have a few memory but it's

00:29:51,960 --> 00:29:57,090
enough for us to complete our tasks and

00:29:55,500 --> 00:30:00,480
then we can do a lot of things including

00:29:57,090 --> 00:30:03,510
as I said service detection so the idea

00:30:00,480 --> 00:30:05,760
here has susan said is that we can take

00:30:03,510 --> 00:30:06,580
the word to vex so it's a pretty trained

00:30:05,760 --> 00:30:08,980
well to back

00:30:06,580 --> 00:30:13,299
model by Google on the Google News and

00:30:08,980 --> 00:30:18,899
data and we will try to predict so did

00:30:13,299 --> 00:30:21,730
the code we'll try to predict reviews

00:30:18,899 --> 00:30:23,620
IMDB reviews actually if it's a good

00:30:21,730 --> 00:30:25,480
review for the movie on a bad review for

00:30:23,620 --> 00:30:27,429
the movie so I will go through the

00:30:25,480 --> 00:30:29,620
notebook as I said I can't really

00:30:27,429 --> 00:30:31,149
execute it for certain number of reasons

00:30:29,620 --> 00:30:35,440
but the thing is that we are polishing

00:30:31,149 --> 00:30:37,299
it and I will show you one of the main

00:30:35,440 --> 00:30:39,399
reason why I cannot execute it right now

00:30:37,299 --> 00:30:40,870
because this is how you did you declare

00:30:39,399 --> 00:30:42,669
dependencies in a notebook so you

00:30:40,870 --> 00:30:47,980
declare before ends and then just get it

00:30:42,669 --> 00:30:51,100
gets included into into both the driver

00:30:47,980 --> 00:30:53,880
and executors and so it looks like SBT

00:30:51,100 --> 00:30:57,630
okay so we have included a bunch of

00:30:53,880 --> 00:31:00,309
dependencies and they all ends by this

00:30:57,630 --> 00:31:03,730
snapshot so the code is stabilizing

00:31:00,309 --> 00:31:05,620
right now and we should release this in

00:31:03,730 --> 00:31:09,460
a few weeks right so it's going to be

00:31:05,620 --> 00:31:11,760
pretty clean and pretty good so the

00:31:09,460 --> 00:31:14,230
interesting thing as well is that we use

00:31:11,760 --> 00:31:16,809
native implementation so that means that

00:31:14,230 --> 00:31:19,960
behind the scene we can connect to Java

00:31:16,809 --> 00:31:25,059
see you through Java CCP and so forth to

00:31:19,960 --> 00:31:28,169
tulip and and d4 J as well too to the

00:31:25,059 --> 00:31:34,990
hard work efficiently so this is very

00:31:28,169 --> 00:31:37,049
very impressive well so the notebook

00:31:34,990 --> 00:31:40,389
that you will be able to access soon is

00:31:37,049 --> 00:31:43,539
starting with the classical Java Scala

00:31:40,389 --> 00:31:45,909
code a bunch of imports but you can see

00:31:43,539 --> 00:31:48,490
that besides the Apache Commons we are

00:31:45,909 --> 00:31:51,460
importing a lot of things to deal with

00:31:48,490 --> 00:31:55,840
datasets but also from the newer linear

00:31:51,460 --> 00:31:57,429
neural network packages the optimizer in

00:31:55,840 --> 00:32:00,760
order to be able to collect information

00:31:57,429 --> 00:32:04,720
where the training is going on and as I

00:32:00,760 --> 00:32:07,480
said so we also have the ng D indeed for

00:32:04,720 --> 00:32:10,090
J a binder is not R to be able to use ng

00:32:07,480 --> 00:32:13,090
arrays and the arrays are essentially

00:32:10,090 --> 00:32:16,210
the portage of ng array from nihai numpy

00:32:13,090 --> 00:32:17,980
on java this is a great work that susan

00:32:16,210 --> 00:32:19,860
is also doing quite a lot these days

00:32:17,980 --> 00:32:22,529
right

00:32:19,860 --> 00:32:25,710
and just so you know there is I don't

00:32:22,529 --> 00:32:28,919
know if he's here but there is a portage

00:32:25,710 --> 00:32:31,919
a layer on top on D for J which is

00:32:28,919 --> 00:32:33,419
called nd for s and this is essentially

00:32:31,919 --> 00:32:40,019
a skylight P I that allows you to

00:32:33,419 --> 00:32:44,549
directly use the CPU power beyond the

00:32:40,019 --> 00:32:49,289
hood so why we have defined all this

00:32:44,549 --> 00:32:52,789
resource there's these packages so we

00:32:49,289 --> 00:32:57,210
can start by collecting the Google News

00:32:52,789 --> 00:32:59,760
or to vac and this is available on s3 so

00:32:57,210 --> 00:33:02,760
you can access to it pretty easily so

00:32:59,760 --> 00:33:06,750
it's a 1.5 gig file that you have to

00:33:02,760 --> 00:33:09,059
unzip it at some point so I created a

00:33:06,750 --> 00:33:12,059
small fire here that I will collect

00:33:09,059 --> 00:33:14,159
where I will collect the the file

00:33:12,059 --> 00:33:15,059
locally so I won't be able one have to

00:33:14,159 --> 00:33:16,950
do it all the time

00:33:15,059 --> 00:33:19,230
this is something that you have to do as

00:33:16,950 --> 00:33:21,149
a scientist you don't want to always go

00:33:19,230 --> 00:33:24,779
to the to the web to collect your file

00:33:21,149 --> 00:33:26,880
but you will locate it locally you will

00:33:24,779 --> 00:33:28,740
pass it locally but also it is a good

00:33:26,880 --> 00:33:31,049
thing about a notebook is that at least

00:33:28,740 --> 00:33:33,600
the original file is in the notebook

00:33:31,049 --> 00:33:36,539
right it's not like yeah you can use my

00:33:33,600 --> 00:33:38,429
my my notebook or whatever and then he

00:33:36,539 --> 00:33:40,529
uses a local path that you don't know

00:33:38,429 --> 00:33:41,610
from where it came right so you're stuck

00:33:40,529 --> 00:33:43,409
because you don't have the data you

00:33:41,610 --> 00:33:45,510
don't know how to collect it at least

00:33:43,409 --> 00:33:47,690
here because it's in the code you can do

00:33:45,510 --> 00:33:51,179
it pretty easily

00:33:47,690 --> 00:33:55,460
so when you have been zip it using the

00:33:51,179 --> 00:33:58,139
DL 4j API you can load it right so and

00:33:55,460 --> 00:34:00,809
this is pretty easy so this is how you

00:33:58,139 --> 00:34:04,230
can load can load the word to vac like

00:34:00,809 --> 00:34:07,289
vector or to vex array in memory so that

00:34:04,230 --> 00:34:09,899
means that you is going to read the four

00:34:07,289 --> 00:34:14,419
gigs and expanded its four gigs file I

00:34:09,899 --> 00:34:16,589
put it in memory so we have four gigs of

00:34:14,419 --> 00:34:18,599
information that we can access so this

00:34:16,589 --> 00:34:21,839
is essentially a lookup table a word and

00:34:18,599 --> 00:34:25,349
then a vector right it has been trained

00:34:21,839 --> 00:34:27,179
already so if you use Park that the the

00:34:25,349 --> 00:34:29,339
SPARC API you can train your model by

00:34:27,179 --> 00:34:31,710
the way so you can check in ml later you

00:34:29,339 --> 00:34:32,820
can train your model you can feed an RDD

00:34:31,710 --> 00:34:35,400
of

00:34:32,820 --> 00:34:40,530
strings and then is gonna learn how to

00:34:35,400 --> 00:34:42,630
structure the data in a 300 space by the

00:34:40,530 --> 00:34:45,270
way if you look if you think about this

00:34:42,630 --> 00:34:47,250
space you can also you saw that these

00:34:45,270 --> 00:34:48,900
lines were pretty parallel right that

00:34:47,250 --> 00:34:51,930
means that instantly they are part of

00:34:48,900 --> 00:34:55,290
the same theme so that means that you

00:34:51,930 --> 00:34:57,840
know if you look at the dimensions if

00:34:55,290 --> 00:35:00,240
you remove some dimensions and so on so

00:34:57,840 --> 00:35:02,370
you can look at the data has new

00:35:00,240 --> 00:35:04,200
clusters as themes so and the way you

00:35:02,370 --> 00:35:06,030
will look at it you will switch from one

00:35:04,200 --> 00:35:09,300
theme to the other this is essentially

00:35:06,030 --> 00:35:12,240
how you can think about these 300

00:35:09,300 --> 00:35:14,880
dimensions if you like so the idea that

00:35:12,240 --> 00:35:16,440
we can learn easily a mobile atom

00:35:14,880 --> 00:35:19,170
learning model so what I'm gonna do is

00:35:16,440 --> 00:35:21,900
that we were trying the two layers model

00:35:19,170 --> 00:35:28,350
the first one is gonna be the lsdm one

00:35:21,900 --> 00:35:29,910
with 200 input sorry cells and the last

00:35:28,350 --> 00:35:32,340
one which is the output is going to be a

00:35:29,910 --> 00:35:33,870
tool to sense one of course because we

00:35:32,340 --> 00:35:35,910
will say if it's a positive or negative

00:35:33,870 --> 00:35:38,490
one so we have only two outputs as

00:35:35,910 --> 00:35:40,290
classification so the size of the

00:35:38,490 --> 00:35:44,280
batches that we're going to use for

00:35:40,290 --> 00:35:46,590
further training is 15 so we pass the

00:35:44,280 --> 00:35:48,000
vector size which is 200 because this is

00:35:46,590 --> 00:35:52,980
a number of dimensions that we use

00:35:48,000 --> 00:35:54,750
number if a pocket for the SGD then this

00:35:52,980 --> 00:36:00,210
is something from from notebook that

00:35:54,750 --> 00:36:02,670
allows you to plot to plot connections

00:36:00,210 --> 00:36:05,760
essentially and the idea is that I go

00:36:02,670 --> 00:36:08,670
from minus six to six by quarter and

00:36:05,760 --> 00:36:11,370
then I plot the soft science of Max and

00:36:08,670 --> 00:36:13,860
in order to do that I just call line

00:36:11,370 --> 00:36:16,110
chart and then sizes and I give a size

00:36:13,860 --> 00:36:20,340
to the plot and the idea is that row

00:36:16,110 --> 00:36:24,690
plus plus is a no is a is able to call

00:36:20,340 --> 00:36:27,750
to concatenate to two widgets and they

00:36:24,690 --> 00:36:31,140
will be part one behind the other below

00:36:27,750 --> 00:36:31,860
the other row can actually stack them in

00:36:31,140 --> 00:36:33,360
a row

00:36:31,860 --> 00:36:35,010
there is also column layout and so on

00:36:33,360 --> 00:36:37,680
and so forth but anyway so this is

00:36:35,010 --> 00:36:40,410
actually how it soft sang is looking

00:36:37,680 --> 00:36:43,440
like and the softmax is looking like so

00:36:40,410 --> 00:36:46,150
you can see more or less how we

00:36:43,440 --> 00:36:47,589
structure in the softmax is is looking

00:36:46,150 --> 00:36:51,730
something you know that can that's

00:36:47,589 --> 00:36:55,930
separate pretty confidently to class to

00:36:51,730 --> 00:36:59,109
classes right so this is how you can

00:36:55,930 --> 00:37:01,210
create the lsdm layer now so the idea is

00:36:59,109 --> 00:37:04,240
that you can just use the Builder so to

00:37:01,210 --> 00:37:05,950
be honest actually the the title is in

00:37:04,240 --> 00:37:07,359
scanner but this is a good thing with

00:37:05,950 --> 00:37:09,579
scanner is that we can use the Java API

00:37:07,359 --> 00:37:10,900
so this thing hasn't been ported with

00:37:09,579 --> 00:37:13,029
the Java

00:37:10,900 --> 00:37:14,680
you know convention but actually it's

00:37:13,029 --> 00:37:18,520
pretty neat as well because at least it

00:37:14,680 --> 00:37:21,670
uses the Builder API so pattern sorry so

00:37:18,520 --> 00:37:23,950
the idea is that we put we pass the

00:37:21,670 --> 00:37:26,799
vector size with the which is 300 of

00:37:23,950 --> 00:37:30,299
course because we're gonna put points in

00:37:26,799 --> 00:37:33,849
the word to backspace the outputs is 200

00:37:30,299 --> 00:37:36,670
so we want 200 steps right so it's more

00:37:33,849 --> 00:37:38,349
or less if scientist sentences if you

00:37:36,670 --> 00:37:41,619
think like that to progress of 200

00:37:38,349 --> 00:37:43,869
whether we can try to train on and then

00:37:41,619 --> 00:37:50,549
the activations I is I shouldn't be for

00:37:43,869 --> 00:37:53,440
is a soft sign then we will apply a

00:37:50,549 --> 00:37:57,640
simple logistic regression onto unto

00:37:53,440 --> 00:37:59,410
these input Eden inputs inputs array and

00:37:57,640 --> 00:38:01,779
then in order to do that we can

00:37:59,410 --> 00:38:04,029
constrict this output layer that will be

00:38:01,779 --> 00:38:07,890
pluggable into the error and network and

00:38:04,029 --> 00:38:10,329
to do so we use the activation softmax

00:38:07,890 --> 00:38:15,640
with the way address function is this

00:38:10,329 --> 00:38:19,150
emissions mi6 and and of course since we

00:38:15,640 --> 00:38:22,510
had 200 input Eden cells then we have

00:38:19,150 --> 00:38:27,190
200 inputs now and it comes out as a 2

00:38:22,510 --> 00:38:29,559
output result so the good thing is that

00:38:27,190 --> 00:38:31,779
we you you will be able to go through

00:38:29,559 --> 00:38:34,510
these notebooks at home and being able

00:38:31,779 --> 00:38:35,980
to replay it at will right so so I'm not

00:38:34,510 --> 00:38:39,059
going to do it because it might take a

00:38:35,980 --> 00:38:44,770
lot of time to do so and I don't have it

00:38:39,059 --> 00:38:47,470
anyway so now we can create this model

00:38:44,770 --> 00:38:50,740
right so in order to do so we have to

00:38:47,470 --> 00:38:52,299
pass how we we will optimize which

00:38:50,740 --> 00:38:54,010
graden we're gonna use we're going to

00:38:52,299 --> 00:38:58,319
use the third key stick right on with

00:38:54,010 --> 00:38:59,410
one iteration for each batch the data is

00:38:58,319 --> 00:39:02,559
the

00:38:59,410 --> 00:39:04,150
as prob that means that we use the

00:39:02,559 --> 00:39:07,599
remains square of the differences of the

00:39:04,150 --> 00:39:10,000
grunion blah blah blah and we

00:39:07,599 --> 00:39:13,960
initialized with Xavier method so it's

00:39:10,000 --> 00:39:18,900
pretty important to to note it's also

00:39:13,960 --> 00:39:21,849
about vanishing we use organization we

00:39:18,900 --> 00:39:24,910
normalize the gradient so they don't

00:39:21,849 --> 00:39:28,210
they don't get too large or too too too

00:39:24,910 --> 00:39:32,549
small so we can cap it to one so this is

00:39:28,210 --> 00:39:36,089
the cap learning rate is a magic number

00:39:32,549 --> 00:39:38,770
yeah you know I put parameters you know

00:39:36,089 --> 00:39:42,400
so we know that this one works pretty

00:39:38,770 --> 00:39:46,660
well right how we know it so this is a

00:39:42,400 --> 00:39:49,839
secret sauce it takes a few minutes to

00:39:46,660 --> 00:39:52,510
know of course I'm kidding

00:39:49,839 --> 00:39:55,299
then we are adding the two layers that

00:39:52,510 --> 00:39:58,480
we want to add into our recurrent

00:39:55,299 --> 00:40:00,700
network and then we use some I can't

00:39:58,480 --> 00:40:02,950
remember exactly what it does but I mean

00:40:00,700 --> 00:40:05,440
just I think it's speeding up the thing

00:40:02,950 --> 00:40:07,480
initially and then we of course has from

00:40:05,440 --> 00:40:11,289
back propagation to optimize it

00:40:07,480 --> 00:40:13,059
efficiently and finally we initialize

00:40:11,289 --> 00:40:15,309
what is called a multi-layer network

00:40:13,059 --> 00:40:18,010
with this configuration now we need to

00:40:15,309 --> 00:40:21,539
specify it right just occupy it this

00:40:18,010 --> 00:40:25,390
this model we just have to import this

00:40:21,539 --> 00:40:28,210
class we can say ok I want to average

00:40:25,390 --> 00:40:31,480
each iteration right so every time I get

00:40:28,210 --> 00:40:34,569
the result from the executors I want to

00:40:31,480 --> 00:40:36,309
average inand I want to propagate to the

00:40:34,569 --> 00:40:39,190
executors once again and then return

00:40:36,309 --> 00:40:41,440
over there and then we can I capsulate

00:40:39,190 --> 00:40:43,000
the network that we just created with

00:40:41,440 --> 00:40:46,740
the spark context and then we got this

00:40:43,000 --> 00:40:52,420
spark there forge a virtual layer thing

00:40:46,740 --> 00:40:56,289
about the data so there is two things

00:40:52,420 --> 00:40:58,930
that we need to do is that we have to

00:40:56,289 --> 00:41:01,359
take the data on the web we have ten

00:40:58,930 --> 00:41:04,720
pack it to to look at the structure and

00:41:01,359 --> 00:41:07,079
also we have to put it some somehow in

00:41:04,720 --> 00:41:10,089
spark right in order to be able to train

00:41:07,079 --> 00:41:12,490
the model on spark so the idea is that

00:41:10,089 --> 00:41:13,090
we we should be able to train using the

00:41:12,490 --> 00:41:14,860
data

00:41:13,090 --> 00:41:17,050
are distributed and not locally

00:41:14,860 --> 00:41:19,690
available so what we're going to do is

00:41:17,050 --> 00:41:22,000
that we take this information that has

00:41:19,690 --> 00:41:25,030
been created by Stanford that we'll see

00:41:22,000 --> 00:41:28,840
later and it has the structure so we

00:41:25,030 --> 00:41:31,030
have 200 and 500 reviews in the positive

00:41:28,840 --> 00:41:34,060
reviews in the train set and the same

00:41:31,030 --> 00:41:35,500
amount of negative reviews in a train

00:41:34,060 --> 00:41:39,940
set as well and we have the same amount

00:41:35,500 --> 00:41:42,040
in the in the test set so the data said

00:41:39,940 --> 00:41:45,580
that we are going to create our three

00:41:42,040 --> 00:41:49,240
dimensional so we have 10 reviews per

00:41:45,580 --> 00:41:51,730
data set we have 300 of course I

00:41:49,240 --> 00:41:54,660
mentions I wonder that is the number of

00:41:51,730 --> 00:41:58,630
words that we will use per review right

00:41:54,660 --> 00:42:00,670
it's not 100 random words is the one of

00:41:58,630 --> 00:42:03,700
that first words but the ones that are

00:42:00,670 --> 00:42:05,380
of course in the world to vac space

00:42:03,700 --> 00:42:07,620
otherwise that would be completely

00:42:05,380 --> 00:42:07,620
useless

00:42:08,700 --> 00:42:19,200
so how I've seen here a very large block

00:42:14,170 --> 00:42:22,930
of code which is taking the data and and

00:42:19,200 --> 00:42:24,880
creating data sets so I'm going to go

00:42:22,930 --> 00:42:26,920
through it the most important part is

00:42:24,880 --> 00:42:29,530
how the heck and I take a token I

00:42:26,920 --> 00:42:33,490
convert it into the word to vex space

00:42:29,530 --> 00:42:35,920
and and family create a vector that I

00:42:33,490 --> 00:42:37,690
can use in order to supervise my

00:42:35,920 --> 00:42:41,110
training so in order to do that so you

00:42:37,690 --> 00:42:45,010
use the walk to vac object and we we get

00:42:41,110 --> 00:42:48,700
the vector out of this token out of the

00:42:45,010 --> 00:42:51,250
way to effect space by using this method

00:42:48,700 --> 00:42:55,000
onto the token right so we get a vector

00:42:51,250 --> 00:42:58,870
which is an array essentially and what

00:42:55,000 --> 00:43:01,230
we do is that we say okay so I want to

00:42:58,870 --> 00:43:04,270
create a three dimension

00:43:01,230 --> 00:43:06,760
where the which is located in at the

00:43:04,270 --> 00:43:09,670
Revue index so the current reviewing the

00:43:06,760 --> 00:43:14,470
zero to nine and then we put all

00:43:09,670 --> 00:43:19,240
information which is the determinant

00:43:14,470 --> 00:43:21,340
that information and then we expose it

00:43:19,240 --> 00:43:24,609
at the token index so actually this is

00:43:21,340 --> 00:43:27,920
the index of the world

00:43:24,609 --> 00:43:29,510
right so now about the data so it's a

00:43:27,920 --> 00:43:33,890
data which is available on the Stanford

00:43:29,510 --> 00:43:35,840
website this is actually a project that

00:43:33,890 --> 00:43:39,800
has been running there and they took all

00:43:35,840 --> 00:43:43,400
the NB MDB reviews and they started

00:43:39,800 --> 00:43:46,100
classifying it so in order to do it so I

00:43:43,400 --> 00:43:48,830
get the data and enter it and then I put

00:43:46,100 --> 00:43:52,580
it locally and then I can create this

00:43:48,830 --> 00:43:54,619
iterator over the data that will take a

00:43:52,580 --> 00:43:56,630
review convert it into the war to vex

00:43:54,619 --> 00:44:00,680
space created data set around it and

00:43:56,630 --> 00:44:02,619
then pass it to me so this is the ugly

00:44:00,680 --> 00:44:08,420
part this is the thing that we have to

00:44:02,619 --> 00:44:10,580
change before we open source it right

00:44:08,420 --> 00:44:13,310
now so everything is in memory so and in

00:44:10,580 --> 00:44:16,340
order to put it into into the executors

00:44:13,310 --> 00:44:17,720
we load it we load everything into the

00:44:16,340 --> 00:44:20,780
main memory right now and then we

00:44:17,720 --> 00:44:23,480
paralyze it so this is very ugly but

00:44:20,780 --> 00:44:26,570
just just a matter of a few lines of

00:44:23,480 --> 00:44:28,220
code order to put the files remotely but

00:44:26,570 --> 00:44:30,920
in order to do that so we need to do

00:44:28,220 --> 00:44:34,490
something which is not so easy then now

00:44:30,920 --> 00:44:37,609
is to put the word - vac model onto each

00:44:34,490 --> 00:44:39,890
executor of the spark cluster that means

00:44:37,609 --> 00:44:42,680
that we need to broadcast it and since

00:44:39,890 --> 00:44:44,780
it's a for gigs thing so it may take a

00:44:42,680 --> 00:44:48,500
little bit of time to do it so we need

00:44:44,780 --> 00:44:50,810
to see how we can speed up these dis

00:44:48,500 --> 00:44:53,150
operations but yeah we're getting close

00:44:50,810 --> 00:44:56,840
to that anyway so we collect information

00:44:53,150 --> 00:44:59,090
we paralyze it in order to train it so

00:44:56,840 --> 00:45:01,910
we just have to ask ok fit data set and

00:44:59,090 --> 00:45:04,480
we provide your DD and and we will use

00:45:01,910 --> 00:45:07,670
you know this process super step

00:45:04,480 --> 00:45:12,080
operations in order to train local

00:45:07,670 --> 00:45:15,440
neural network propagate and aggregate

00:45:12,080 --> 00:45:20,590
the the weight onto the driver and then

00:45:15,440 --> 00:45:23,150
it will iterate for for the new step

00:45:20,590 --> 00:45:24,760
doing the prediction is easy as well so

00:45:23,150 --> 00:45:27,619
we also create an ER DD

00:45:24,760 --> 00:45:32,180
and then so we have access to the data

00:45:27,619 --> 00:45:34,880
on the cluster and for each data set

00:45:32,180 --> 00:45:36,770
into the cluster so we can take out the

00:45:34,880 --> 00:45:39,710
features and we

00:45:36,770 --> 00:45:41,150
can has the train network to predict on

00:45:39,710 --> 00:45:43,069
to the features and we have the

00:45:41,150 --> 00:45:45,200
predictions which will be negative

00:45:43,069 --> 00:45:48,260
positive negative positive and we can

00:45:45,200 --> 00:45:50,450
check the accuracy in the next version

00:45:48,260 --> 00:45:53,089
the new version that we will also

00:45:50,450 --> 00:45:55,970
deliver we will also have some plots

00:45:53,089 --> 00:45:57,800
that I will be showing the accuracy the

00:45:55,970 --> 00:45:59,440
prediction the propagation so we there

00:45:57,800 --> 00:46:01,970
are a lot of things that we can

00:45:59,440 --> 00:46:05,359
listening to while turning the idiocracy

00:46:01,970 --> 00:46:07,579
so that means that we can only stop you

00:46:05,359 --> 00:46:10,160
know the training if we see that it's

00:46:07,579 --> 00:46:12,380
going you know in the wild we can ask

00:46:10,160 --> 00:46:13,730
the notebook to okay stop the model are

00:46:12,380 --> 00:46:15,319
going to chance a little bit my

00:46:13,730 --> 00:46:18,440
parameter because I'm losing my time

00:46:15,319 --> 00:46:21,079
right now so yeah this is reaching the

00:46:18,440 --> 00:46:23,630
end so it's not too long a notebook I

00:46:21,079 --> 00:46:29,270
would say because there is a lot of of

00:46:23,630 --> 00:46:31,160
explanation and on propose and yeah I

00:46:29,270 --> 00:46:36,470
mean you can you can do very tricky

00:46:31,160 --> 00:46:39,859
things like understanding wireless

00:46:36,470 --> 00:46:41,930
review and as Susan said NLP is art and

00:46:39,859 --> 00:46:46,010
actually even the sentence NLP is heart

00:46:41,930 --> 00:47:01,640
is hard to interpret right so that's

00:46:46,010 --> 00:47:06,650
pretty much it now right yeah yeah

00:47:01,640 --> 00:47:08,510
we're we're on the JVM that really

00:47:06,650 --> 00:47:10,670
matters for a lot of people who are in

00:47:08,510 --> 00:47:14,630
enterprise when they think in terms of

00:47:10,670 --> 00:47:17,400
you know connecting to their ETL or

00:47:14,630 --> 00:47:21,870
whatever they'd rather work with some

00:47:17,400 --> 00:47:23,490
the GBM so yeah the thing is that's it's

00:47:21,870 --> 00:47:26,880
very cool to have the model which is

00:47:23,490 --> 00:47:28,470
trying right but then then you have to

00:47:26,880 --> 00:47:31,200
interact with many other things right

00:47:28,470 --> 00:47:33,660
because now you have your reviews okay

00:47:31,200 --> 00:47:36,090
so are you gonna ask your business to go

00:47:33,660 --> 00:47:38,490
to whatever Scala code to connect it and

00:47:36,090 --> 00:47:40,590
to make his review using the predict

00:47:38,490 --> 00:47:42,990
function no right so you want to do

00:47:40,590 --> 00:47:45,120
something which is pretty classic is

00:47:42,990 --> 00:47:47,310
creating a web service into it and then

00:47:45,120 --> 00:47:49,740
you know serving this information right

00:47:47,310 --> 00:47:51,240
that means that if you start doing it in

00:47:49,740 --> 00:47:53,220
Python that there is high chance that

00:47:51,240 --> 00:47:54,690
you won't be scalable right if you can

00:47:53,220 --> 00:47:56,520
start doing with Python then you will

00:47:54,690 --> 00:47:58,980
have to introduce a new layer into

00:47:56,520 --> 00:48:01,220
infrastructure and and if you think

00:47:58,980 --> 00:48:04,320
about enterprise which is my concern

00:48:01,220 --> 00:48:06,300
then there is 99 percent of the people

00:48:04,320 --> 00:48:09,630
that will just say no I won't deploy any

00:48:06,300 --> 00:48:11,220
Python in my infrastructure I can't go

00:48:09,630 --> 00:48:13,110
to any department and say yeah I have

00:48:11,220 --> 00:48:15,530
some smart Python script it's pretty

00:48:13,110 --> 00:48:18,180
cool can you deploy it they will just go

00:48:15,530 --> 00:48:19,590
that will yeah they will just say no and

00:48:18,180 --> 00:48:21,270
then you can go away and then rewrite

00:48:19,590 --> 00:48:23,160
anything and if you leave your data

00:48:21,270 --> 00:48:24,240
scientist write everything in Python and

00:48:23,160 --> 00:48:27,540
then you have to rewrite everything

00:48:24,240 --> 00:48:29,940
right so tons of doesn't throw for me

00:48:27,540 --> 00:48:32,100
the toy yeah and also the fact that we

00:48:29,940 --> 00:48:34,860
have a native compute layer that

00:48:32,100 --> 00:48:36,690
optimizes our on the basis of the chip

00:48:34,860 --> 00:48:40,530
that you're running on is no small feat

00:48:36,690 --> 00:48:42,360
you know so I mentioned nd 4j so just to

00:48:40,530 --> 00:48:45,330
be clear it's a linear algebra library

00:48:42,360 --> 00:48:47,880
we have support for nd erase a lot of

00:48:45,330 --> 00:48:51,210
languages who have gotten this right but

00:48:47,880 --> 00:48:53,430
not really in the Java world or really

00:48:51,210 --> 00:48:56,790
in you know like for example breeze I

00:48:53,430 --> 00:48:58,590
think only supports for D I think or 5 D

00:48:56,790 --> 00:49:02,100
I'm not sure and I don't know about CUDA

00:48:58,590 --> 00:49:04,290
support for breeze either yeah so you

00:49:02,100 --> 00:49:06,390
know that's no small feat to get the

00:49:04,290 --> 00:49:09,030
stack to work with us in + + underbelly

00:49:06,390 --> 00:49:12,990
and a lot of that is with java cpp which

00:49:09,030 --> 00:49:15,840
is a really great a great API that was

00:49:12,990 --> 00:49:17,730
written by one of our engineers that

00:49:15,840 --> 00:49:19,770
basically hooks into the j and i based

00:49:17,730 --> 00:49:22,160
on the syntax and

00:49:19,770 --> 00:49:25,410
and semantics of the language

00:49:22,160 --> 00:49:29,640
similarities so I encourage you guys to

00:49:25,410 --> 00:49:32,270
look it up if you're interested any

00:49:29,640 --> 00:49:32,270
other questions

00:49:33,080 --> 00:49:38,060
okay cool okay great thanks for being

00:49:35,849 --> 00:49:38,060

YouTube URL: https://www.youtube.com/watch?v=kHgttIwWcaE


